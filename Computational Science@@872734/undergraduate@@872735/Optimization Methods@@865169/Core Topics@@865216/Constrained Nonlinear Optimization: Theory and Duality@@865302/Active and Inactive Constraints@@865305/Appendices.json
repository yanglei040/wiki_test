{"hands_on_practices": [{"introduction": "The most intuitive way to grasp why a constraint becomes active is through a geometric lens. This exercise asks you to find the point within a feasible half-space that is closest to an ideal, but infeasible, target point. By recognizing this optimization problem as a geometric projection, you will see precisely why the solution must lie on the boundary, and you will use the Karush-Kuhn-Tucker (KKT) conditions to formalize this intuition and compute the associated Lagrange multiplier [@problem_id:3094283].", "problem": "Consider the constrained optimization problem in the $2$-dimensional Euclidean space: minimize the strictly convex quadratic objective $f(x_1,x_2)=(x_1-1)^2+(x_2-1)^2$ subject to the affine inequality constraint $x_1+x_2\\ge 3$. Starting from the foundational facts that the unconstrained minimizer of a differentiable convex function is characterized by a zero gradient, and that the unique minimizer of the squared Euclidean distance to a point over a closed convex set is its Euclidean projection, justify from first principles why the sum constraint becomes active at the constrained minimizer. Then, using the stationarity and complementary slackness conditions from the Karush–Kuhn–Tucker (KKT) optimality framework for a single inequality constraint $g(x_1,x_2)\\le 0$, compute the value of the optimal Lagrange multiplier associated with the active constraint. Your final answer must be the single numerical value of this multiplier. No rounding is required.", "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- Objective function to minimize: $f(x_1, x_2) = (x_1-1)^2 + (x_2-1)^2$. This is a strictly convex quadratic function in $\\mathbb{R}^2$.\n- Constraint: $x_1 + x_2 \\ge 3$. This is an affine inequality constraint.\n- Foundational facts for justification:\n    1. The unconstrained minimizer of a differentiable convex function is characterized by a zero gradient.\n    2. The unique minimizer of the squared Euclidean distance to a point over a closed convex set is its Euclidean projection.\n- Task:\n    1. Justify from first principles why the constraint is active at the constrained minimizer.\n    2. Use the Karush–Kuhn–Tucker (KKT) conditions to compute the optimal Lagrange multiplier.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a standard quadratic programming (QP) problem, a fundamental topic in convex optimization. The objective function represents the squared Euclidean distance from a point, and the constraint defines a closed half-plane. These are well-established mathematical constructs. The problem is scientifically and mathematically sound.\n- **Well-Posed:** The objective function $f(x_1, x_2)$ is strictly convex, and the feasible region defined by the linear inequality $x_1 + x_2 \\ge 3$ is a closed and convex set. The minimization of a strictly convex function over a non-empty, closed, convex set has a unique solution. The problem is well-posed.\n- **Objective:** The problem is stated using precise mathematical language, with no ambiguity or subjective elements.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe problem is to solve the following constrained optimization problem:\n$$ \\text{minimize} \\quad f(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 1)^2 $$\n$$ \\text{subject to} \\quad x_1 + x_2 \\ge 3 $$\n\nThe solution is presented in two parts as requested.\n\n**Part 1: Justification for the Active Constraint**\n\nThe objective function $f(x_1, x_2)$ can be interpreted as the squared Euclidean distance between a variable point $x = (x_1, x_2)$ and a fixed point $p = (1, 1)$. Minimizing $f(x)$ is therefore equivalent to finding the point $x$ in the feasible region that is closest to $p$.\n\nFirst, let us find the unconstrained minimizer of $f(x_1, x_2)$. According to the provided principle, the minimizer of a differentiable convex function is located where the gradient is zero. The gradient of $f(x)$ is:\n$$ \\nabla f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2(x_1 - 1) \\\\ 2(x_2 - 1) \\end{pmatrix} $$\nSetting the gradient to the zero vector gives:\n$$ 2(x_1 - 1) = 0 \\implies x_1 = 1 $$\n$$ 2(x_2 - 1) = 0 \\implies x_2 = 1 $$\nThe unconstrained minimizer is the point $x_{unc} = (1, 1)$, which is the point $p$ itself.\n\nNext, we check if this unconstrained minimizer lies within the feasible region defined by the constraint $x_1 + x_2 \\ge 3$. Substituting the coordinates of $x_{unc}$ into the inequality:\n$$ 1 + 1 = 2 $$\nSince $2  3$, the condition $x_1 + x_2 \\ge 3$ is not satisfied. The unconstrained minimizer $x_{unc}$ is not a feasible point.\n\nThe feasible set is $S = \\{ (x_1, x_2) \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge 3 \\}$. This set is a closed half-plane, which is a closed convex set. The problem is to find the point in $S$ that minimizes the squared Euclidean distance to the point $p = (1, 1)$. Based on the second provided principle, this solution is the Euclidean projection of $p$ onto the set $S$.\n\nSince the point $p = (1, 1)$ lies outside the closed convex set $S$, its projection onto $S$ must lie on the boundary of $S$. The boundary of the feasible set $S$ is the line defined by the equation $x_1 + x_2 = 3$.\nTherefore, the constrained minimizer, let us call it $x^* = (x_1^*, x_2^*)$, must satisfy the equality:\n$$ x_1^* + x_2^* = 3 $$\nWhen an inequality constraint holds with equality at a specific point, it is said to be an **active constraint** at that point. Thus, we have justified from first principles that the constraint $x_1 + x_2 \\ge 3$ must be active at the solution.\n\n**Part 2: Computation of the Lagrange Multiplier using KKT Conditions**\n\nTo apply the KKT framework, we first write the problem in the standard form:\n$$ \\text{minimize} \\quad f(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 1)^2 $$\n$$ \\text{subject to} \\quad g(x_1, x_2) = 3 - x_1 - x_2 \\le 0 $$\nThe Lagrangian function $L(x_1, x_2, \\lambda)$ is defined as:\n$$ L(x_1, x_2, \\lambda) = f(x_1, x_2) + \\lambda g(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 1)^2 + \\lambda(3 - x_1 - x_2) $$\nwhere $\\lambda$ is the Lagrange multiplier associated with the constraint $g(x_1, x_2) \\le 0$.\n\nThe Karush–Kuhn–Tucker (KKT) conditions for an optimal solution $(x_1^*, x_2^*)$ with multiplier $\\lambda^*$ are:\n1.  **Stationarity:** $\\nabla_x L(x_1^*, x_2^*, \\lambda^*) = 0$\n2.  **Primal Feasibility:** $g(x_1^*, x_2^*) \\le 0$\n3.  **Dual Feasibility:** $\\lambda^* \\ge 0$\n4.  **Complementary Slackness:** $\\lambda^* g(x_1^*, x_2^*) = 0$\n\nLet's apply these conditions. The stationarity condition requires the partial derivatives of the Lagrangian with respect to $x_1$ and $x_2$ to be zero:\n$$ \\frac{\\partial L}{\\partial x_1} = 2(x_1 - 1) - \\lambda = 0 \\implies 2x_1 - 2 = \\lambda $$\n$$ \\frac{\\partial L}{\\partial x_2} = 2(x_2 - 1) - \\lambda = 0 \\implies 2x_2 - 2 = \\lambda $$\nFrom these two equations, we deduce that $2x_1 - 2 = 2x_2 - 2$, which simplifies to $x_1 = x_2$.\n\nThe complementary slackness condition $\\lambda (3 - x_1 - x_2) = 0$ implies that either $\\lambda = 0$ or $3 - x_1 - x_2 = 0$.\nIf $\\lambda = 0$, the stationarity conditions would give $x_1=1$ and $x_2=1$. As established in Part 1, this point $(1, 1)$ is not feasible because it violates the primal feasibility condition $3 - x_1 - x_2 \\le 0$ (since $3-1-1 = 1 > 0$).\nTherefore, we must have $\\lambda \\ne 0$. Due to dual feasibility ($\\lambda \\ge 0$), it must be that $\\lambda > 0$. From complementary slackness, this forces $g(x_1, x_2) = 0$, which is $3 - x_1 - x_2 = 0$, or $x_1 + x_2 = 3$. This confirms our earlier finding that the constraint is active.\n\nWe now have a system of two linear equations for $x_1$ and $x_2$:\n1. $x_1 = x_2$\n2. $x_1 + x_2 = 3$\n\nSubstituting the first equation into the second gives $x_1 + x_1 = 3$, which leads to $2x_1 = 3$, so $x_1 = 1.5$. Since $x_1 = x_2$, we also have $x_2 = 1.5$. The optimal point is $x^* = (1.5, 1.5)$.\n\nFinally, we compute the value of the optimal Lagrange multiplier $\\lambda^*$ using the stationarity condition:\n$$ \\lambda^* = 2x_1^* - 2 = 2(1.5) - 2 = 3 - 2 = 1 $$\nThis value satisfies dual feasibility, as $\\lambda^* = 1 \\ge 0$. All KKT conditions are satisfied. The value of the optimal Lagrange multiplier is $1$.", "answer": "$$\\boxed{1}$$", "id": "3094283"}, {"introduction": "Real-world optimization problems rarely involve just one restriction. This practice challenges you to handle a feasible region defined by multiple intersecting linear constraints, forming a cone. The key insight you will develop is that the identity of the active constraints depends critically on the location of the unconstrained minimizer [@problem_id:3094227]. Solving this problem requires a systematic case analysis using the KKT conditions, resulting in an elegant piecewise function that maps any unconstrained optimum to its unique feasible counterpart.", "problem": "Consider the convex optimization problem of minimizing the quadratic objective $f(x,y)=(x-1)^{2}+(y+3)^{2}$ subject to the linear inequality constraints $y\\ge x$ and $x\\ge 0$. The unconstrained minimizer of $f$ is the point where its gradient vanishes. Treat the constrained minimizer as the Euclidean projection of the unconstrained minimizer onto the feasible set, and derive the solution using the Karush-Kuhn-Tucker (KKT) conditions. Starting from the core definitions of convexity, Euclidean projection onto a closed convex set, and the KKT optimality system for convex quadratic problems with linear inequality constraints, determine the exact conditions under which the line constraint $y=x$ is active at the solution. Generalize the objective to $f_{a,b}(x,y)=(x-a)^{2}+(y-b)^{2}$ with unconstrained minimizer $(a,b)\\in\\mathbb{R}^{2}$ and derive a closed-form expression for the constrained minimizer $(x^{\\star},y^{\\star})$ as a function of $(a,b)$, expressed as a single analytic piecewise formula that encompasses all cases of active and inactive constraints. Your final answer should be the explicit piecewise expression for $(x^{\\star},y^{\\star})$ in terms of $(a,b)$. No rounding is required and no units are involved. Additionally, explain qualitatively how $(x^{\\star},y^{\\star})$ changes as $(a,b)$ crosses the boundary of the feasible region and identify precisely when $y=x$ is active.", "solution": "The user-provided problem is a convex optimization task. I will first validate its formal structure and scientific soundness before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Specific Objective Function**: $f(x,y)=(x-1)^{2}+(y+3)^{2}$\n- **Generalized Objective Function**: $f_{a,b}(x,y)=(x-a)^{2}+(y-b)^{2}$\n- **Unconstrained Minimizer (Generalized)**: $(a,b)\\in\\mathbb{R}^{2}$\n- **Constraints**: $y\\ge x$ and $x\\ge 0$\n- **Methodology**: The problem is to be interpreted as a Euclidean projection onto the feasible set, solved using the Karush-Kuhn-Tucker (KKT) conditions.\n- **Required Outputs**:\n    1. The exact conditions under which the constraint $y=x$ is active at the solution for the specific case.\n    2. A closed-form, single analytic piecewise formula for the constrained minimizer $(x^{\\star},y^{\\star})$ as a function of $(a,b)$.\n    3. A qualitative explanation of how $(x^{\\star},y^{\\star})$ changes as $(a,b)$ crosses the boundaries of the feasible region.\n    4. A precise identification of when $y=x$ is active in the general case.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is a classic quadratic programming (QP) problem. The objective function $f_{a,b}(x,y)$ is strictly convex for any $(a,b) \\in \\mathbb{R}^2$, as its Hessian matrix is $2I$, which is positive definite. The feasible set defined by the linear inequalities $y \\ge x$ and $x \\ge 0$ is a closed convex set. The minimization of a strictly convex function over a non-empty, closed, convex set is a well-posed problem with a unique solution. The interpretation of this solution as the Euclidean projection of the unconstrained minimizer $(a,b)$ onto the feasible set is a fundamental and correct result in optimization theory. The problem is scientifically and mathematically sound.\n- **Well-Posed**: A unique, stable, and meaningful solution $(x^\\star, y^\\star)$ exists for any given $(a,b) \\in \\mathbb{R}^2$.\n- **Objective**: The problem is stated using precise, unambiguous mathematical language.\n- **Other Flaws**: The problem is self-contained, its constraints are consistent, and it is a standard, non-trivial problem in optimization methods, directly related to the topic of active and inactive constraints.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed with a full derivation.\n\n### Solution Derivation\n\nThe problem is to find the point $(x^\\star, y^\\star)$ in the feasible set $S = \\{ (x,y) \\in \\mathbb{R}^2 \\mid y \\ge x, x \\ge 0 \\}$ that minimizes the squared Euclidean distance to a given point $(a,b)$. This is equivalent to finding the Euclidean projection of $(a,b)$ onto $S$.\n\nWe formulate this as a constrained optimization problem:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad  f(x,y) = (x-a)^2 + (y-b)^2 \\\\\n\\text{subject to} \\quad  g_1(x,y) = x - y \\le 0 \\\\\n g_2(x,y) = -x \\le 0\n\\end{aligned}\n$$\n\nThe Karush-Kuhn-Tucker (KKT) conditions for a solution $(x^\\star, y^\\star)$ with Lagrange multipliers $\\mu_1, \\mu_2$ are:\n\n1.  **Stationarity**: $\\nabla f(x^\\star, y^\\star) + \\mu_1 \\nabla g_1(x^\\star, y^\\star) + \\mu_2 \\nabla g_2(x^\\star, y^\\star) = 0$\n    $2(x^\\star - a) + \\mu_1(1) + \\mu_2(-1) = 0$\n    $2(y^\\star - b) + \\mu_1(-1) + \\mu_2(0) = 0$\n2.  **Primal Feasibility**:\n    $x^\\star - y^\\star \\le 0$\n    $-x^\\star \\le 0$\n3.  **Dual Feasibility**:\n    $\\mu_1 \\ge 0$\n    $\\mu_2 \\ge 0$\n4.  **Complementary Slackness**:\n    $\\mu_1 (x^\\star - y^\\star) = 0$\n    $\\mu_2 (-x^\\star) = 0$\n\nFrom the stationarity equations, we can express the multipliers in terms of the solution variables:\nFrom the second equation: $\\mu_1 = 2(y^\\star - b)$.\nFrom the first equation: $\\mu_2 = 2(x^\\star - a) + \\mu_1 = 2(x^\\star - a) + 2(y^\\star - b) = 2(x^\\star + y^\\star - a - b)$.\n\nThe KKT system can be solved by considering four cases based on the complementary slackness conditions.\n\n**Case 1: No constraints are active ($\\mu_1 = 0, \\mu_2 = 0$).**\nThe stationarity equations give $x^\\star = a$ and $y^\\star = b$.\nFor this to be a valid solution, it must be primal feasible: $a-b \\le 0 \\implies b \\ge a$ and $-a \\le 0 \\implies a \\ge 0$.\nSo, if $(a,b)$ is in the feasible region $S$, it is its own projection.\nRegion: $\\{ (a,b) \\mid a \\ge 0 \\text{ and } b \\ge a \\}$. Solution: $(x^\\star, y^\\star) = (a,b)$.\n\n**Case 2: $g_1$ is active ($x^\\star=y^\\star$), $g_2$ is inactive ($x^\\star > 0 \\implies \\mu_2=0$).**\nWith $x^\\star=y^\\star$ and $\\mu_2=0$, stationarity becomes:\n$2(x^\\star - a) + \\mu_1 = 0$\n$2(x^\\star - b) - \\mu_1 = 0$\nAdding these gives $4x^\\star - 2a - 2b = 0$, so $x^\\star = \\frac{a+b}{2}$. Thus, $(x^\\star, y^\\star) = (\\frac{a+b}{2}, \\frac{a+b}{2})$.\nConditions for validity:\n- Primal feasibility: $x^\\star > 0 \\implies a+b > 0$.\n- Dual feasibility: $\\mu_1 = 2(x^\\star - b) = a+b-2b = a-b \\ge 0 \\implies a \\ge b$.\nCombining these with non-strict inequalities for the boundary cases, we define the region.\nRegion: $\\{ (a,b) \\mid a \\ge b \\text{ and } a+b \\ge 0 \\}$. Solution: $(x^\\star, y^\\star) = (\\frac{a+b}{2}, \\frac{a+b}{2})$. This corresponds to projection onto the boundary line $y=x$.\n\n**Case 3: $g_2$ is active ($x^\\star=0$), $g_1$ is inactive ($y^\\star > x^\\star \\implies \\mu_1=0$).**\nWith $x^\\star=0$ and $\\mu_1=0$, stationarity becomes:\n$2(-a) - \\mu_2 = 0 \\implies \\mu_2 = -2a$.\n$2(y^\\star - b) = 0 \\implies y^\\star=b$.\nSolution is $(x^\\star, y^\\star) = (0,b)$.\nConditions for validity:\n- Primal feasibility: $y^\\star > x^\\star \\implies b > 0$.\n- Dual feasibility: $\\mu_2 = -2a \\ge 0 \\implies a \\le 0$.\nRegion: $\\{ (a,b) \\mid a \\le 0 \\text{ and } b \\ge 0 \\}$. Solution: $(x^\\star, y^\\star) = (0,b)$. This corresponds to projection onto the boundary line $x=0$.\n\n**Case 4: Both $g_1$ and $g_2$ are active ($x^\\star=y^\\star$ and $x^\\star=0$).**\nThis immediately gives the solution $(x^\\star, y^\\star) = (0,0)$.\nConditions for validity come from dual feasibility ($\\mu_1 \\ge 0, \\mu_2 \\ge 0$):\n$\\mu_1 = 2(y^\\star - b) = -2b \\ge 0 \\implies b \\le 0$.\n$\\mu_2 = 2(x^\\star + y^\\star - a - b) = -2(a+b) \\ge 0 \\implies a+b \\le 0$.\nRegion: $\\{ (a,b) \\mid b \\le 0 \\text{ and } a+b \\le 0 \\}$. Solution: $(x^\\star, y^\\star) = (0,0)$. This corresponds to projection onto the vertex $(0,0)$.\n\nThese four closed regions cover the entire $\\mathbb{R}^2$ plane for $(a,b)$. On their boundaries, the solution formulas are consistent, ensuring the projection is a continuous function of $(a,b)$.\n\n### Analysis of Active Constraints and Qualitative Behavior\n\n**When is the constraint $y=x$ active?**\nThe constraint $y=x$ is active at the solution $(x^\\star, y^\\star)$ when $x^\\star = y^\\star$. Analyzing our four cases:\n1.  Solution $(a,b)$: Active if $a=b$. This occurs on the boundary of the feasible set, $\\{ (a,b) \\mid a \\ge 0, b=a \\}$.\n2.  Solution $(\\frac{a+b}{2}, \\frac{a+b}{2})$: Always active by definition. This holds for the region $\\{ (a,b) \\mid a \\ge b, a+b \\ge 0 \\}$.\n3.  Solution $(0,b)$: Active if $b=0$. This occurs for $\\{ (a,b) \\mid a \\le 0, b=0 \\}$.\n4.  Solution $(0,0)$: Always active by definition. This holds for the region $\\{ (a,b) \\mid b \\le 0, a+b \\le 0 \\}$.\n\nCombining these, the constraint $y=x$ is active if and only if $(a,b)$ belongs to the union of the regions from Case 2 and Case 4:\n$\\{ (a,b) \\mid (a \\ge b \\text{ and } a+b \\ge 0) \\lor (b \\le 0 \\text{ and } a+b \\le 0) \\}$.\n\nFor the specific initial problem with $f(x,y)=(x-1)^2+(y+3)^2$, we have $(a,b)=(1,-3)$. This point falls into Case 4 since $b=-3 \\le 0$ and $a+b = 1-3=-2 \\le 0$. The solution is $(x^\\star, y^\\star)=(0,0)$. At this solution, $x^\\star=y^\\star=0$, so the constraint $y=x$ is active.\n\n**Qualitative Behavior of the Solution**\nThe solution $(x^\\star, y^\\star)$ is the projection of the unconstrained minimizer $(a,b)$ onto the feasible set $S$.\n- When $(a,b)$ is inside the feasible set $S$ (Region 1), the solution is simply $(a,b)$.\n- As $(a,b)$ crosses a boundary of $S$, the solution $(x^\\star, y^\\star)$ \"sticks\" to that boundary.\n- If $(a,b)$ crosses the boundary $b=a$ into Region 2, the solution becomes its projection onto the line $y=x$, which is $(\\frac{a+b}{2}, \\frac{a+b}{2})$. The solution point slides along the line $y=x$.\n- If $(a,b)$ crosses the boundary $a=0$ into Region 3, the solution becomes its projection onto the $y$-axis, which is $(0,b)$. The solution point slides along the line $x=0$.\n- If $(a,b)$ is in Region 4, which is geometrically \"behind\" the vertex of the feasible cone, the projection is always the vertex itself. Thus, for any $(a,b)$ in this region, the solution is fixed at $(0,0)$.\nThe function mapping $(a,b)$ to $(x^\\star, y^\\star)$ is continuous across all of $\\mathbb{R}^2$, but its derivatives are discontinuous across the boundaries of the four regions.\n\nThe closed-form piecewise expression for the constrained minimizer $(x^\\star, y^\\star)$ is the final answer.", "answer": "$$\n\\boxed{\n(x^\\star, y^\\star) = \n\\begin{cases} \n(a, b)  \\text{if } a \\ge 0 \\text{ and } b \\ge a \\\\\n\\left( \\frac{a+b}{2}, \\frac{a+b}{2} \\right)  \\text{if } a \\ge b \\text{ and } a+b \\ge 0 \\\\\n(0, b)  \\text{if } a \\le 0 \\text{ and } b \\ge 0 \\\\\n(0, 0)  \\text{if } b \\le 0 \\text{ and } a+b \\le 0\n\\end{cases}\n}\n$$", "id": "3094227"}, {"introduction": "Identifying an active constraint is just the beginning; understanding its consequences for optimization algorithms is the next critical step. In this exercise, you will track the solution of a quadratic program as a parameter changes, pinpointing the exact moment the solution path hits a boundary and the active set changes [@problem_id:3094249]. You will then compute the projected Hessian, which measures the objective function's curvature along the active constraint's boundary, a fundamental quantity that guides the steps taken by many advanced optimization methods.", "problem": "Consider the bound-constrained quadratic program in two dimensions\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\;\\; \\frac{1}{2} x^{\\top} Q x + c(t)^{\\top} x\n$$\nsubject to the box constraints\n$$\nl \\le x \\le u,\n$$\nwhere the Hessian $Q$ is symmetric positive definite, the lower and upper bounds are $l = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$ and $u = \\begin{pmatrix} 2 \\\\ \\frac{9}{11} \\end{pmatrix}$, and the linear term varies affinely with a parameter $t \\ge 0$ as\n$$\nc(t) = c_{0} + t d, \\quad c_{0} = \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix}, \\quad d = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}.\n$$\nLet\n$$\nQ = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}.\n$$\nStarting from fundamental facts in optimization methods and the definition of active and inactive constraints, determine the earliest nonnegative parameter value $t^{\\star}$ at which the constrained minimizer’s active set changes from having no active constraints to having at least one active bound. Then, immediately after $t^{\\star}$ (for $t > t^{\\star}$ sufficiently small), identify the active bound and compute the scalar projected Hessian obtained by restricting $Q$ to the nullspace of the gradients of the active constraints.\n\nYour final answer must be a single two-entry row matrix containing, in order, the exact value of $t^{\\star}$ and the exact scalar value of the projected Hessian immediately after $t^{\\star}$. No rounding is required. Express the final answer with no units.", "solution": "The user wants to solve a parametric quadratic programming problem.\n\n### **Problem Validation**\n\n**Step 1: Extract Givens**\nThe problem is to solve the quadratic program:\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\;\\; f(x,t) = \\frac{1}{2} x^{\\top} Q x + c(t)^{\\top} x\n$$\nsubject to the box constraints $l \\le x \\le u$.\nThe given data are:\n- Hessian matrix: $Q = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}$\n- Linear term: $c(t) = c_{0} + t d$, with $c_{0} = \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix}$ and $d = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}$ for $t \\ge 0$.\n- Lower bounds: $l = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$\n- Upper bounds: $u = \\begin{pmatrix} 2 \\\\ \\frac{9}{11} \\end{pmatrix}$\nThese constraints can be written as: $0 \\le x_1 \\le 2$ and $-1 \\le x_2 \\le \\frac{9}{11}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded. It is a standard parametric quadratic program from the field of mathematical optimization. The Hessian matrix $Q$ is symmetric. To check if it is positive definite, we compute its principal minors. The first principal minor is $\\det(Q_{11}) = 4 > 0$. The second principal minor is the determinant of $Q$, $\\det(Q) = (4)(3) - (1)(1) = 11 > 0$. Since all principal minors are positive, $Q$ is symmetric positive definite. This guarantees that the objective function $f(x,t)$ is strictly convex for any fixed $t$, ensuring a unique minimizer for the constrained problem. The feasible region is a closed and bounded set (a box), which is also convex. The problem is well-posed, as a unique solution $x(t)$ exists for each $t \\ge 0$. The problem asks for the first value of $t$ where the solution trajectory hits the boundary, which is a well-defined question. All terms are defined mathematically, and the problem is objective and self-contained.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed to the solution.\n\n### **Solution**\n\nThe problem starts with no active constraints. In this case, the minimizer of the constrained problem is the same as the unconstrained minimizer, provided it lies strictly within the feasible region. The unconstrained minimizer, which we denote as $x_u(t)$, is found by setting the gradient of the objective function to zero.\n\nThe gradient of $f(x,t)$ with respect to $x$ is:\n$$\n\\nabla_x f(x,t) = Qx + c(t)\n$$\nSetting the gradient to zero, we find the unconstrained minimizer:\n$$\nQx_u(t) + c(t) = 0 \\implies x_u(t) = -Q^{-1} c(t)\n$$\nFirst, we compute the inverse of $Q$:\n$$\nQ^{-1} = \\frac{1}{\\det(Q)} \\begin{pmatrix} 3  -1 \\\\ -1  4 \\end{pmatrix} = \\frac{1}{11} \\begin{pmatrix} 3  -1 \\\\ -1  4 \\end{pmatrix}\n$$\nNow, we express $x_u(t)$ as a function of the parameter $t$:\n$$\nx_u(t) = -Q^{-1} (c_0 + td) = -Q^{-1} c_0 - t(Q^{-1}d)\n$$\nWe compute the terms $Q^{-1}c_0$ and $Q^{-1}d$:\n$$\nQ^{-1}c_0 = \\frac{1}{11} \\begin{pmatrix} 3  -1 \\\\ -1  4 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} = \\frac{1}{11} \\begin{pmatrix} -6 \\\\ 2 \\end{pmatrix}\n$$\n$$\nQ^{-1}d = \\frac{1}{11} \\begin{pmatrix} 3  -1 \\\\ -1  4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix} = \\frac{1}{11} \\begin{pmatrix} 3(1) + (-1)(-3) \\\\ -1(1) + 4(-3) \\end{pmatrix} = \\frac{1}{11} \\begin{pmatrix} 6 \\\\ -13 \\end{pmatrix}\n$$\nSubstituting these back into the expression for $x_u(t)$:\n$$\nx_u(t) = -\\frac{1}{11} \\begin{pmatrix} -6 \\\\ 2 \\end{pmatrix} - t \\frac{1}{11} \\begin{pmatrix} 6 \\\\ -13 \\end{pmatrix} = \\frac{1}{11} \\begin{pmatrix} 6 - 6t \\\\ -2 + 13t \\end{pmatrix}\n$$\nThis represents the trajectory of the minimizer as long as no constraints are active. Let's verify the problem's premise that for $t=0$, the solution is in the interior.\n$$\nx(0) = \\frac{1}{11} \\begin{pmatrix} 6 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 6/11 \\\\ -2/11 \\end{pmatrix}\n$$\nWe check this against the bounds:\n$0 \\le x_1(0) \\le 2 \\implies 0  6/11  2$, which is true.\n$-1 \\le x_2(0) \\le 9/11 \\implies -1  -2/11  9/11$, which is also true.\nSo, at $t=0$, the minimizer is indeed in the strict interior of the feasible set.\n\nNext, we find the smallest $t \\ge 0$ for which $x_u(t)$ touches one of the boundaries of the box $[0, 2] \\times [-1, 9/11]$. This occurs when one of the following four conditions is met:\n1.  $x_1(t) = 0 \\implies \\frac{1}{11}(6 - 6t) = 0 \\implies 6 - 6t = 0 \\implies t = 1$.\n2.  $x_1(t) = 2 \\implies \\frac{1}{11}(6 - 6t) = 2 \\implies 6 - 6t = 22 \\implies -6t = 16 \\implies t = -8/3$. This is negative and thus not the first non-negative time.\n3.  $x_2(t) = -1 \\implies \\frac{1}{11}(-2 + 13t) = -1 \\implies -2 + 13t = -11 \\implies 13t = -9 \\implies t = -9/13$. This is also negative.\n4.  $x_2(t) = 9/11 \\implies \\frac{1}{11}(-2 + 13t) = \\frac{9}{11} \\implies -2 + 13t = 9 \\implies 13t = 11 \\implies t = 11/13$.\n\nThe positive values of $t$ at which a constraint becomes active are $t=1$ and $t=11/13$. The earliest (smallest) of these is $t^{\\star} = 11/13$.\n\nAt $t^{\\star} = 11/13$, the constraint $x_2 = 9/11$ becomes active. For $t > t^{\\star}$ and sufficiently small, this constraint will remain active. The active set immediately after $t^{\\star}$ contains the single constraint $x_2 = u_2 = 9/11$. This can be written as $x_2 - 9/11 = 0$.\n\nThe projected Hessian is obtained by restricting the Hessian $Q$ to the nullspace of the gradients of the active constraints. The gradient of the active constraint function $g(x) = x_2 - 9/11$ is:\n$$\n\\nabla g(x) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThe matrix of active constraint gradients is $A = \\begin{pmatrix} 0  1 \\end{pmatrix}$. The nullspace of $A$ is the set of vectors $z \\in \\mathbb{R}^2$ such that $Az = 0$.\n$$\n\\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix} = z_2 = 0\n$$\nThe nullspace consists of all vectors of the form $\\begin{pmatrix} z_1 \\\\ 0 \\end{pmatrix}$. A basis for this one-dimensional nullspace is given by the single vector:\n$$\nZ = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nThe projected Hessian, $H_p$, is a scalar in this case and is computed as $Z^{\\top}QZ$.\n$$\nH_p = Z^{\\top}QZ = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\n\\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}\n$$\n$$\nH_p = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix} = 4\n$$\nThe scalar projected Hessian is $4$.\n\nThe two required values are $t^{\\star} = 11/13$ and the projected Hessian, which is $4$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{11}{13}  4\n\\end{pmatrix}\n}\n$$", "id": "3094249"}]}