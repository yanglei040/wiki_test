{"hands_on_practices": [{"introduction": "To begin, we tackle a foundational problem that illustrates the core mechanics of the Lagrangian method. This exercise involves minimizing a simple quadratic cost function subject to a linear equality constraint, a scenario often encountered in resource allocation. By working through this problem, you will practice the essential steps of constructing the Lagrangian, deriving the first-order necessary conditions, and solving the resulting system of equations to find the optimal solution and the associated Lagrange multiplier [@problem_id:2216747].", "problem": "Consider the problem of allocating a fixed, normalized resource between two independent processes. Let $x$ and $y$ represent the fraction of the resource allocated to the first and second process, respectively, such that $x \\geq 0$ and $y \\geq 0$. The total resource must be fully utilized, which imposes the constraint that the allocations must sum to one. The cost associated with this allocation is given by the function $f(x,y) = \\alpha x^2 + \\beta y^2$, where $\\alpha$ and $\\beta$ are given positive real constants representing the cost coefficients for each process.\n\nYour task is to find the optimal allocation $(x^*, y^*)$ that minimizes the total cost, subject to the full utilization constraint. You must also find the value of the Lagrange multiplier, $\\lambda^*$, at this optimal point.\n\nProvide the triplet of values $(x^*, y^*, \\lambda^*)$ in terms of the constants $\\alpha$ and $\\beta$.", "solution": "The problem is to minimize the objective function $f(x,y) = \\alpha x^2 + \\beta y^2$ subject to the equality constraint $g(x,y) = x+y-1=0$. We can solve this using the method of Lagrange multipliers.\n\nFirst, we define the Lagrangian function $\\mathcal{L}(x, y, \\lambda)$, which is given by:\n$$\n\\mathcal{L}(x, y, \\lambda) = f(x,y) - \\lambda g(x,y)\n$$\nSubstituting the given functions, we get:\n$$\n\\mathcal{L}(x, y, \\lambda) = \\alpha x^2 + \\beta y^2 - \\lambda(x+y-1)\n$$\nTo find the stationary points of the Lagrangian, which correspond to the constrained extrema of the objective function, we must find the points where the gradient of $\\mathcal{L}$ is zero. This gives us a system of three equations, obtained by taking the partial derivatives of $\\mathcal{L}$ with respect to $x$, $y$, and $\\lambda$ and setting them to zero. These are the first-order necessary conditions for optimality.\n\n1.  The partial derivative with respect to $x$:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x} = 2\\alpha x - \\lambda = 0\n    $$\n    From this equation, we can express $x$ in terms of $\\lambda$:\n    $$\n    x = \\frac{\\lambda}{2\\alpha}\n    $$\n\n2.  The partial derivative with respect to $y$:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial y} = 2\\beta y - \\lambda = 0\n    $$\n    Similarly, we express $y$ in terms of $\\lambda$:\n    $$\n    y = \\frac{\\lambda}{2\\beta}\n    $$\n\n3.  The partial derivative with respect to $\\lambda$:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -(x+y-1) = 0\n    $$\n    This simply recovers our original constraint equation:\n    $$\n    x+y=1\n    $$\n\nNow, we have a system of three linear equations for the three unknowns $x$, $y$, and $\\lambda$. We can solve this system by substituting the expressions for $x$ and $y$ from the first two equations into the third equation.\n\nSubstituting $x = \\frac{\\lambda}{2\\alpha}$ and $y = \\frac{\\lambda}{2\\beta}$ into $x+y=1$:\n$$\n\\frac{\\lambda}{2\\alpha} + \\frac{\\lambda}{2\\beta} = 1\n$$\nFactor out $\\lambda$ from the left-hand side:\n$$\n\\lambda \\left( \\frac{1}{2\\alpha} + \\frac{1}{2\\beta} \\right) = 1\n$$\nTo solve for $\\lambda$, we first find a common denominator for the terms in the parentheses:\n$$\n\\lambda \\left( \\frac{\\beta + \\alpha}{2\\alpha\\beta} \\right) = 1\n$$\nNow, we can isolate $\\lambda$ to find its optimal value, $\\lambda^*$:\n$$\n\\lambda^* = \\frac{2\\alpha\\beta}{\\alpha+\\beta}\n$$\nWith the value of $\\lambda^*$ found, we can now find the optimal allocations $x^*$ and $y^*$ by substituting $\\lambda^*$ back into the expressions for $x$ and $y$.\n\nFor $x^*$:\n$$\nx^* = \\frac{\\lambda^*}{2\\alpha} = \\frac{1}{2\\alpha} \\left( \\frac{2\\alpha\\beta}{\\alpha+\\beta} \\right) = \\frac{\\beta}{\\alpha+\\beta}\n$$\n\nFor $y^*$:\n$$\ny^* = \\frac{\\lambda^*}{2\\beta} = \\frac{1}{2\\beta} \\left( \\frac{2\\alpha\\beta}{\\alpha+\\beta} \\right) = \\frac{\\alpha}{\\alpha+\\beta}\n$$\nSince $\\alpha$ and $\\beta$ are positive constants, it follows that $x^*  0$ and $y^*  0$, satisfying the non-negativity conditions mentioned in the problem. The triplet of optimal values is therefore $(x^*, y^*, \\lambda^*) = \\left(\\frac{\\beta}{\\alpha+\\beta}, \\frac{\\alpha}{\\alpha+\\beta}, \\frac{2\\alpha\\beta}{\\alpha+\\beta}\\right)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\beta}{\\alpha+\\beta}  \\frac{\\alpha}{\\alpha+\\beta}  \\frac{2\\alpha\\beta}{\\alpha+\\beta} \\end{pmatrix}}\n$$", "id": "2216747"}, {"introduction": "Next, we shift from an algebraic setup to a more geometric one, reinforcing the concepts from a different perspective. This practice asks you to find the point on a hyperbola that is closest to the origin, which demonstrates how to apply Lagrange multipliers to non-linear constraints. This exercise is valuable for building intuition, as the solution corresponds to the point where the constraint curve is tangent to a level set of the function being optimized [@problem_id:2216754].", "problem": "In a materials science laboratory, researchers are developing a new composite material by combining two substances, Substance X and Substance Y. Let $x$ and $y$ represent the positive, dimensionless quantities of Substance X and Substance Y used in the mixture, respectively. Due to the chemical kinetics of the synthesis process, the quantities must satisfy the strict relationship $xy = 18$. The structural integrity of the final composite is found to be optimal when a quality metric, $Q(x,y) = x^2 + y^2$, is minimized. Your task is to determine the exact quantities of Substance X and Substance Y that should be used to achieve this optimal structural integrity.\n\nPresent your answer as a pair of values $(x, y)$.", "solution": "We are asked to minimize the function $Q(x,y) = x^{2} + y^{2}$ subject to the constraint $xy = 18$ with $x0$ and $y0$. This is a constrained optimization problem, which we solve using Lagrange multipliers.\n\nDefine $f(x,y) = x^{2} + y^{2}$ and the constraint function $g(x,y) = xy - 18 = 0$. The method of Lagrange multipliers sets the gradients to be proportional, $\\nabla f(x,y) = \\lambda \\nabla g(x,y)$, which yields the system of equations:\n$$\n\\frac{\\partial f}{\\partial x} = 2x = \\lambda \\frac{\\partial g}{\\partial x} = \\lambda y\n$$\n$$\n\\frac{\\partial f}{\\partial y} = 2y = \\lambda \\frac{\\partial g}{\\partial y} = \\lambda x\n$$\ntogether with the constraint:\n$$\nxy = 18\n$$\nFrom $2x = \\lambda y$ we obtain $\\lambda = \\frac{2x}{y}$, and from $2y = \\lambda x$ we obtain $\\lambda = \\frac{2y}{x}$. Equating these expressions for $\\lambda$ gives:\n$$\n\\frac{2x}{y} = \\frac{2y}{x} \\implies x^{2} = y^{2}\n$$\nSince $x0$ and $y0$, it follows that $x = y$. Substituting into the constraint $xy = 18$ yields:\n$$\nx^{2} = 18 \\implies x = \\sqrt{18}\n$$\nThus, $x = y = \\sqrt{18}$. To confirm this is a minimum, note that for positive $x,y$ with fixed product $xy=18$, the inequality $(x-y)^{2} \\geq 0$ implies $x^{2} + y^{2} \\geq 2xy = 36$, with equality if and only if $x=y$. The found point achieves this global minimum.\n\nTherefore, the exact quantities are $x = \\sqrt{18} = 3\\sqrt{2}$ and $y = \\sqrt{18} = 3\\sqrt{2}$.", "answer": "$$\\boxed{\\begin{pmatrix} 3\\sqrt{2}  3\\sqrt{2} \\end{pmatrix}}$$", "id": "2216754"}, {"introduction": "Finally, we advance to a more complex and realistic scenario involving inequality constraints. Many real-world optimization problems are defined by limits and boundaries, such as budgets that cannot be exceeded or quantities that must be non-negative. This requires extending the Lagrangian framework to the Karush-Kuhn-Tucker (KKT) conditions, which are introduced in this problem [@problem_id:3192411]. This practice is essential for learning to identify which constraints are active at the optimum and how to interpret the multipliers, a key skill for tackling advanced optimization problems.", "problem": "Consider the convex quadratic program with two decision variables $x_1$ and $x_2$:\nminimize $f(x_1,x_2) = (x_1 - 3.5)^2 + (x_2 - 1.5)^2$ subject to the four inequality constraints\n$g_1(x_1,x_2) = x_1 + x_2 - 3 \\leq 0$, $g_2(x_1,x_2) = x_1 - 2 \\leq 0$, $g_3(x_1,x_2) = -x_1 \\leq 0$, and $g_4(x_1,x_2) = -x_2 \\leq 0$.\nFormulate the Lagrangian function using nonnegative multipliers $\\lambda_1$, $\\lambda_2$, $\\lambda_3$, and $\\lambda_4$ associated with $g_1$, $g_2$, $g_3$, and $g_4$, respectively. Using first principles for convex optimization, derive the necessary optimality conditions and solve for the optimal primal variables $(x_1^{\\star}, x_2^{\\star})$ and optimal multipliers $(\\lambda_1^{\\star}, \\lambda_2^{\\star}, \\lambda_3^{\\star}, \\lambda_4^{\\star})$. Identify which constraints bind by interpreting the sign and magnitude of the optimal multipliers and verify your identification using complementary slackness. Express your final answer as a single row vector $\\begin{pmatrix} x_1^{\\star}  x_2^{\\star}  \\lambda_1^{\\star}  \\lambda_2^{\\star}  \\lambda_3^{\\star}  \\lambda_4^{\\star} \\end{pmatrix}$.", "solution": "This is a convex quadratic programming problem. The objective function is strictly convex, and the constraints are linear, defining a convex feasible region. For such a convex optimization problem, the Karush-Kuhn-Tucker (KKT) conditions are both necessary and sufficient for optimality.\n\nFirst, we formulate the Lagrangian function $L(x_1, x_2, \\boldsymbol{\\lambda})$:\n$$L(x_1, x_2, \\boldsymbol{\\lambda}) = (x_1 - 3.5)^2 + (x_2 - 1.5)^2 + \\lambda_1(x_1 + x_2 - 3) + \\lambda_2(x_1 - 2) + \\lambda_3(-x_1) + \\lambda_4(-x_2)$$\nwhere $\\boldsymbol{\\lambda} = (\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4)$ is the vector of non-negative Lagrange multipliers.\n\nThe KKT conditions consist of stationarity, primal feasibility, dual feasibility, and complementary slackness.\n1.  **Stationarity**: The gradient of the Lagrangian with respect to the primal variables must be zero.\n    $$ \\frac{\\partial L}{\\partial x_1} = 2(x_1 - 3.5) + \\lambda_1 + \\lambda_2 - \\lambda_3 = 0 $$\n    $$ \\frac{\\partial L}{\\partial x_2} = 2(x_2 - 1.5) + \\lambda_1 - \\lambda_4 = 0 $$\n2.  **Primal Feasibility**: The constraints must be satisfied: $g_i(\\mathbf{x}) \\le 0$ for $i=1,..,4$.\n3.  **Dual Feasibility**: The multipliers must be non-negative: $\\lambda_i \\ge 0$ for $i=1,..,4$.\n4.  **Complementary Slackness**: The product of each multiplier and its constraint must be zero: $\\lambda_i g_i(\\mathbf{x}) = 0$ for $i=1,..,4$.\n\nThe unconstrained minimum of the objective function is at $(3.5, 1.5)$. This point violates constraints $g_1(x_1,x_2) \\le 0$ and $g_2(x_1,x_2) \\le 0$, indicating the optimum must lie on the boundary of the feasible region. Let's test the hypothesis that constraints $g_1$ and $g_2$ are active (binding) at the optimum.\n\nIf $g_1$ and $g_2$ are active, we have the system:\n$$ x_1 + x_2 - 3 = 0 $$\n$$ x_1 - 2 = 0 $$\nSolving this yields the candidate optimal point $(x_1^{\\star}, x_2^{\\star}) = (2, 1)$.\n\nAt this point, constraints $g_3(2,1)=-2  0$ and $g_4(2,1)=-1  0$ are inactive. By complementary slackness ($\\lambda_3 g_3 = 0, \\lambda_4 g_4 = 0$), their corresponding multipliers must be zero: $\\lambda_3^{\\star} = 0$ and $\\lambda_4^{\\star} = 0$.\n\nWe use the stationarity conditions to solve for the remaining multipliers, $\\lambda_1$ and $\\lambda_2$. Substituting $x_1=2, x_2=1, \\lambda_3=0, \\lambda_4=0$:\n$$ 2(2 - 3.5) + \\lambda_1 + \\lambda_2 - 0 = 0 \\implies -3 + \\lambda_1 + \\lambda_2 = 0 $$\n$$ 2(1 - 1.5) + \\lambda_1 - 0 = 0 \\implies -1 + \\lambda_1 = 0 $$\nFrom the second equation, we find $\\lambda_1^{\\star} = 1$. Substituting this into the first equation gives $-3 + 1 + \\lambda_2 = 0$, which yields $\\lambda_2^{\\star} = 2$.\n\nThe candidate solution is $(x_1^{\\star}, x_2^{\\star})=(2, 1)$ and $(\\lambda_1^{\\star}, \\lambda_2^{\\star}, \\lambda_3^{\\star}, \\lambda_4^{\\star})=(1, 2, 0, 0)$. This solution satisfies all KKT conditions:\n- **Primal Feasibility**: The point $(2,1)$ satisfies all four inequality constraints.\n- **Dual Feasibility**: All multipliers $(1, 2, 0, 0)$ are non-negative.\n- **Stationarity and Complementary Slackness**: These are satisfied by construction.\n\nSince all KKT conditions are satisfied for this convex problem, the solution is the unique global optimum. The positive multipliers $\\lambda_1^{\\star}=1$ and $\\lambda_2^{\\star}=2$ confirm that constraints $g_1$ and $g_2$ are binding. The zero multipliers $\\lambda_3^{\\star}=0$ and $\\lambda_4^{\\star}=0$ confirm that $g_3$ and $g_4$ are not binding, which is consistent with $g_3(2,1)  0$ and $g_4(2,1)  0$.\n\nThe final vector of optimal primal and dual variables is $(2, 1, 1, 2, 0, 0)$.", "answer": "$$ \\boxed{ \\begin{pmatrix} 2  1  1  2  0  0 \\end{pmatrix} } $$", "id": "3192411"}]}