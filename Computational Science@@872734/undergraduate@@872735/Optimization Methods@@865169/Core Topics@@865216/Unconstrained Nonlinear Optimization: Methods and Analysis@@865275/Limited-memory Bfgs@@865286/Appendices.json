{"hands_on_practices": [{"introduction": "The L-BFGS algorithm constructs its memory-efficient Hessian approximation using information gathered from recent steps. This practice focuses on computing the fundamental building blocks of this history: the displacement vectors $s_k$ and the gradient difference vectors $y_k$. Mastering this initial step is crucial for understanding how the algorithm learns about the curvature of the objective function. [@problem_id:2184596]", "problem": "You are analyzing the behavior of the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm, a popular quasi-Newton method for unconstrained optimization. The algorithm builds an approximation of the inverse Hessian matrix by storing the $m$ most recent pairs of vectors $(s_k, y_k)$. Here, $x_k$ is the iterate at step $k$, $s_k = x_{k+1} - x_k$ is the displacement vector, and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ is the change in the gradient vector for some objective function $f(x)$.\n\nConsider the optimization of the two-dimensional convex quadratic function $f(x) = f(x_1, x_2) = (x_1 - 2)^2 + 3(x_2 + 1)^2$. An optimization routine has produced the following sequence of three iterates (position vectors):\n$$\nx_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad x_1 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}, \\quad x_2 = \\begin{pmatrix} 2 \\\\ -1.5 \\end{pmatrix}\n$$\nCalculate the two pairs of history vectors, $(s_0, y_0)$ and $(s_1, y_1)$, that an L-BFGS algorithm would store based on this sequence of iterates.\n\nExpress your answer as a single $2 \\times 4$ matrix where the columns represent the vectors $s_0$, $y_0$, $s_1$, and $y_1$ in that specific order. Use fractions for any non-integer values.", "solution": "The goal is to compute the displacement vectors $s_k = x_{k+1} - x_k$ and the gradient difference vectors $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ for $k=0$ and $k=1$.\n\nFirst, we need to find the gradient of the objective function $f(x_1, x_2) = (x_1 - 2)^2 + 3(x_2 + 1)^2$. The partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial x_1} = 2(x_1 - 2)\n$$\n$$\n\\frac{\\partial f}{\\partial x_2} = 3 \\cdot 2(x_2 + 1) = 6(x_2 + 1)\n$$\nSo, the gradient vector is:\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} 2(x_1 - 2) \\\\ 6(x_2 + 1) \\end{pmatrix}\n$$\n\nNext, we evaluate the gradient at each of the given iterates $x_0$, $x_1$, and $x_2$. Let's denote these gradients as $g_0$, $g_1$, and $g_2$.\n\nFor $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$:\n$$\ng_0 = \\nabla f(0, 0) = \\begin{pmatrix} 2(0 - 2) \\\\ 6(0 + 1) \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ 6 \\end{pmatrix}\n$$\n\nFor $x_1 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$:\n$$\ng_1 = \\nabla f(1, -2) = \\begin{pmatrix} 2(1 - 2) \\\\ 6(-2 + 1) \\end{pmatrix} = \\begin{pmatrix} 2(-1) \\\\ 6(-1) \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix}\n$$\n\nFor $x_2 = \\begin{pmatrix} 2 \\\\ -1.5 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{3}{2} \\end{pmatrix}$:\n$$\ng_2 = \\nabla f(2, -1.5) = \\begin{pmatrix} 2(2 - 2) \\\\ 6(-1.5 + 1) \\end{pmatrix} = \\begin{pmatrix} 2(0) \\\\ 6(-0.5) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}\n$$\n\nNow we can compute the displacement vectors $s_0$ and $s_1$.\n\nFor $k=0$:\n$$\ns_0 = x_1 - x_0 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\n$$\n\nFor $k=1$:\n$$\ns_1 = x_2 - x_1 = \\begin{pmatrix} 2 \\\\ -1.5 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 2 - 1 \\\\ -1.5 - (-2) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0.5 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}\n$$\n\nNext, we compute the gradient difference vectors $y_0$ and $y_1$.\n\nFor $k=0$:\n$$\ny_0 = g_1 - g_0 = \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix} - \\begin{pmatrix} -4 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} -2 - (-4) \\\\ -6 - 6 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -12 \\end{pmatrix}\n$$\n\nFor $k=1$:\n$$\ny_1 = g_2 - g_1 = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix} = \\begin{pmatrix} 0 - (-2) \\\\ -3 - (-6) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\n\nFinally, we assemble the results into a $2 \\times 4$ matrix where the columns are $s_0, y_0, s_1, y_1$.\n$$\ns_0 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}, \\quad y_0 = \\begin{pmatrix} 2 \\\\ -12 \\end{pmatrix}, \\quad s_1 = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}, \\quad y_1 = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\nThe resulting matrix is:\n$$\n\\begin{pmatrix} 1  2  1  2 \\\\ -2  -12  \\frac{1}{2}  3 \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  2  1  2 \\\\ -2  -12  \\frac{1}{2}  3 \\end{pmatrix}}\n$$", "id": "2184596"}, {"introduction": "With the history pairs $(s_k, y_k)$ in hand, L-BFGS computes a search direction without ever forming an explicit inverse Hessian matrix. This exercise takes you into the engine room of the algorithm to perform the celebrated two-loop recursion. Tracing this computation reveals the source of L-BFGS's efficiency and power in solving large-scale problems. [@problem_id:2184578]", "problem": "The Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm is a popular quasi-Newton method for unconstrained optimization. In each iteration $k$, the algorithm computes a search direction $p_k$ by applying an approximation of the inverse Hessian matrix to the negative of the current gradient, $g_k = \\nabla f(x_k)$. This approximation is constructed implicitly using a limited history of the $m$ most recent steps.\n\nThe history is stored as pairs of vectors $(s_i, y_i)$ for $i=k-m, \\dots, k-1$, where $s_i = x_{i+1} - x_i$ is the change in position and $y_i = g_{i+1} - g_i$ is the change in the gradient. The search direction $p_k$ is then found by a procedure known as the L-BFGS two-loop recursion.\n\nConsider an L-BFGS update at step $k$ with a memory size of $m=2$. The relevant data available from previous steps are:\n- Current gradient: $g_k = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$\n- History from step $k-1$: $s_{k-1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $y_{k-1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n- History from step $k-2$: $s_{k-2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $y_{k-2} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}$\n\nYour task is to compute the search direction vector $p_k$ for this iteration. Express your answer as a $2 \\times 1$ column vector with exact rational components.", "solution": "The L-BFGS search direction $p_k$ is computed by approximating the product $-H_k g_k$, where $H_k$ is the inverse Hessian approximation. This is achieved efficiently using the two-loop recursion algorithm. We are given $m=2$, the gradient $g_k$, and the history vectors $(s_{k-1}, y_{k-1})$ and $(s_{k-2}, y_{k-2})$.\n\nThe algorithm is as follows:\n\n1.  Initialize a vector $q$ with the current gradient:\n    $q = g_k = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$.\n\n2.  **First Loop (backward pass):** This loop iterates from $i = k-1$ down to $i = k-m$. In our case, $i$ goes from $k-1$ to $k-2$.\n    We first pre-calculate the scalars $\\rho_i = \\frac{1}{y_i^T s_i}$.\n    For $i = k-1$:\n    $y_{k-1}^T s_{k-1} = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (1)(1) + (1)(0) = 1$.\n    So, $\\rho_{k-1} = \\frac{1}{1} = 1$.\n\n    For $i = k-2$:\n    $y_{k-2}^T s_{k-2} = \\begin{pmatrix} -1  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (-1)(0) + (2)(1) = 2$.\n    So, $\\rho_{k-2} = \\frac{1}{2}$.\n\n    Now, we perform the loop updates. We will also store the computed $\\alpha_i$ values, as they are needed in the second loop.\n    -   **For $i = k-1$**:\n        $\\alpha_{k-1} = \\rho_{k-1} s_{k-1}^T q = (1) \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = (1)((1)(1) + (0)(-2)) = 1$.\n        $q \\leftarrow q - \\alpha_{k-1} y_{k-1} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} - (1) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1-1 \\\\ -2-1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}$.\n\n    -   **For $i = k-2$**:\n        $\\alpha_{k-2} = \\rho_{k-2} s_{k-2}^T q = \\left(\\frac{1}{2}\\right) \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = \\left(\\frac{1}{2}\\right)((0)(0) + (1)(-3)) = -\\frac{3}{2}$.\n        $q \\leftarrow q - \\alpha_{k-2} y_{k-2} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\left(-\\frac{3}{2}\\right) \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} + \\begin{pmatrix} -\\frac{3}{2} \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 - \\frac{3}{2} \\\\ -3 + 3 \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{2} \\\\ 0 \\end{pmatrix}$.\n\n3.  **Initial Hessian Scaling:** The initial inverse Hessian approximation $H_k^0$ is a diagonal matrix $\\gamma_k I$, where $\\gamma_k = \\frac{s_{k-1}^T y_{k-1}}{y_{k-1}^T y_{k-1}}$. We initialize our result vector $r$ by multiplying this scaled identity matrix with the current $q$.\n    $s_{k-1}^T y_{k-1} = (1)(1) + (0)(1) = 1$.\n    $y_{k-1}^T y_{k-1} = (1)^2 + (1)^2 = 2$.\n    $\\gamma_k = \\frac{1}{2}$.\n    $r = \\gamma_k q = \\frac{1}{2} \\begin{pmatrix} -\\frac{3}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix}$.\n\n4.  **Second Loop (forward pass):** This loop iterates from $i = k-m$ up to $i = k-1$. In our case, $i$ goes from $k-2$ to $k-1$.\n    -   **For $i = k-2$**:\n        $\\beta = \\rho_{k-2} y_{k-2}^T r = \\left(\\frac{1}{2}\\right) \\begin{pmatrix} -1  2 \\end{pmatrix} \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} = \\left(\\frac{1}{2}\\right)((-1)(-\\frac{3}{4}) + (2)(0)) = \\frac{3}{8}$.\n        $r \\leftarrow r + s_{k-2} (\\alpha_{k-2} - \\beta) = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\left(-\\frac{3}{2} - \\frac{3}{8}\\right) = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\left(-\\frac{12}{8} - \\frac{3}{8}\\right) = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -\\frac{15}{8} \\end{pmatrix} = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix}$.\n\n    -   **For $i = k-1$**:\n        $\\beta = \\rho_{k-1} y_{k-1}^T r = (1) \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} = (1)(-\\frac{6}{8}) + (1)(-\\frac{15}{8}) = -\\frac{21}{8}$.\n        $r \\leftarrow r + s_{k-1} (\\alpha_{k-1} - \\beta) = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\left(1 - \\left(-\\frac{21}{8}\\right)\\right) = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\left(\\frac{8}{8} + \\frac{21}{8}\\right) = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} + \\begin{pmatrix} \\frac{29}{8} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{23}{8} \\\\ -\\frac{15}{8} \\end{pmatrix}$.\n\n5.  The final result of the two-loop recursion is the vector $r = H_k g_k$. The search direction is $p_k = -r$.\n    $p_k = - \\begin{pmatrix} \\frac{23}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} = \\begin{pmatrix} -\\frac{23}{8} \\\\ \\frac{15}{8} \\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix} -\\frac{23}{8} \\\\ \\frac{15}{8} \\end{pmatrix}}$$", "id": "2184578"}, {"introduction": "After exploring the mechanics of L-BFGS, it is insightful to understand its behavior on a larger scale. This exercise challenges you to think qualitatively about the search path the algorithm takes on a difficult, ill-conditioned problem. Developing this intuition helps to appreciate why L-BFGS is often a superior choice compared to both simpler and more complex alternatives. [@problem_id:2184592]", "problem": "An optimization problem involves minimizing a function $f: \\mathbb{R}^n \\to \\mathbb{R}$. The function is a strongly convex quadratic, but it is also ill-conditioned. This means its level sets are highly elongated hyper-ellipsoids. The function minimum is at the origin, $\\mathbf{x}^* = \\mathbf{0}$.\n\nWe will use the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm to find the minimum. L-BFGS is a quasi-Newton method that approximates the inverse of the Hessian matrix using a limited number of the most recent updates to the position and gradient. The number of past updates stored is determined by a memory parameter, $m$. For this problem, we consider an L-BFGS implementation with a very small memory parameter, for example, $m=2$. The algorithm is initiated at a starting point $\\mathbf{x}_0$ that does not lie on any of the principal axes of the hyper-ellipsoids.\n\nWhich of the following statements provides the best qualitative description of the search path, $\\{\\mathbf{x}_k\\}$, generated by the L-BFGS algorithm under these conditions?\n\nA. The search path will consist of a small number of straight-line segments that point directly towards the minimum, reaching it in at most $n$ steps.\n\nB. The search path will be nearly identical to the path taken by the steepest descent method, exhibiting a characteristic slow \"zigzagging\" convergence pattern.\n\nC. The search path will initially be curved, but after approximately $n$ iterations, it will straighten out and point directly towards the minimum, exhibiting superlinear convergence.\n\nD. The search path will exhibit a repeating pattern of behavior. In each cycle of roughly $m$ steps, the path will become progressively more aligned with the optimal direction, followed by a reversion to a less optimal, more gradient-like direction, resulting in a trajectory that is generally better than steepest descent but not as direct as a full quasi-Newton method.", "solution": "Let the strongly convex quadratic be written as\n$$\nf(x)=\\frac{1}{2}x^{T}Qx,\\quad Q=Q^{T}\\succ 0,\\quad \\mathbf{x}^{*}=\\mathbf{0},\\quad g(x)=\\nabla f(x)=Qx.\n$$\nIll-conditioning means the eigenvalues of $Q$ satisfy $\\lambda_{\\max}/\\lambda_{\\min}\\gg 1$, so level sets are elongated hyper-ellipsoids. Starting from $\\mathbf{x}_{0}$ not aligned with principal axes ensures the gradient $g_{0}$ has components in multiple eigen-directions.\n\nSteepest descent uses $p_{k}^{\\mathrm{SD}}=-g_{k}$ with exact line search $\\alpha_{k}=\\frac{g_{k}^{T}g_{k}}{g_{k}^{T}Qg_{k}}$, leading to a characteristic zigzag that is slow when $Q$ is ill-conditioned, since the contraction factor is governed by $\\frac{\\kappa(Q)-1}{\\kappa(Q)+1}$.\n\nQuasi-Newton methods compute $p_{k}=-H_{k}g_{k}$, where $H_{k}\\approx Q^{-1}$. Full-memory BFGS maintains all past curvature information and, on a quadratic with exact line searches, produces $Q$-conjugate directions equivalent to conjugate gradients, achieving finite termination in at most $n$ steps and superlinear convergence.\n\nL-BFGS with memory parameter $m$ stores only the last $m$ curvature pairs\n$$\ns_{i}=x_{i+1}-x_{i},\\quad y_{i}=g_{i+1}-g_{i}=Qs_{i},\\quad i\\in\\{k-m,\\dots,k-1\\},\n$$\nand forms the direction via the two-loop recursion:\n1) Set $q=g_{k}$; for $i=k-1,\\dots,k-m$, compute $\\rho_{i}=\\frac{1}{y_{i}^{T}s_{i}}$, $\\alpha_{i}=\\rho_{i}s_{i}^{T}q$, $q\\leftarrow q-\\alpha_{i}y_{i}$.\n2) Initialize $H_{k}^{(0)}=\\gamma_{k}I$ with $\\gamma_{k}=\\frac{s_{k-1}^{T}y_{k-1}}{y_{k-1}^{T}y_{k-1}}$, set $r=H_{k}^{(0)}q$.\n3) For $i=k-m,\\dots,k-1$, compute $\\beta=\\rho_{i}y_{i}^{T}r$, $r\\leftarrow r+s_{i}(\\alpha_{i}-\\beta)$.\nThen $p_{k}=-r$.\n\nFrom these equations, $H_{k}$ satisfies the secant conditions $H_{k}y_{i}=s_{i}$ only for the stored $m$ pairs; information beyond the last $m$ steps is discarded. Consequently, $H_{k}$ is accurate in the subspace spanned by $\\{s_{k-m},\\dots,s_{k-1}\\}$ and defaults to the scaled identity $H_{k}^{(0)}$ outside that subspace. With $m=2$, only a two-dimensional slice of curvature is well represented at any iteration.\n\nOn an ill-conditioned quadratic, as iterations proceed:\n- Over the next $m$ steps, the stored pairs adapt $H_{k}$ to locally capture principal curvature directions encountered, so $p_{k}$ becomes better aligned with $Q^{-1}g_{k}$ (a preconditioned direction), improving over steepest descent.\n- When a new pair is added, the oldest is dropped; curvature information in directions not represented by the last $m$ steps is lost, and the search direction partially reverts toward the scaled gradient due to $H_{k}^{(0)}=\\gamma_{k}I$.\n\nThis creates a repeating cycle with period on the order of $m$: progressive alignment and improvement within the span of recent steps, followed by degradation as older information is discarded. Therefore:\n- Statement A is false because the finite-step, direct-to-minimum behavior is associated with conjugate gradients or full-memory BFGS on quadratics, not L-BFGS with $m=2$.\n- Statement B is too pessimistic; L-BFGS uses curvature information to precondition and is generally better than steepest descent, not nearly identical to it.\n- Statement C requires accumulating about $n$ independent curvature directions to construct a near-exact inverse Hessian and achieve superlinear, nearly straight convergence; with $m=2$, L-BFGS cannot retain that information.\n\nThe qualitative behavior described above matches Statement D: a repeating pattern over cycles of roughly $m$ steps, alternately improving alignment and then partially reverting, yielding a trajectory better than steepest descent but less direct than a full quasi-Newton method.", "answer": "$$\\boxed{D}$$", "id": "2184592"}]}