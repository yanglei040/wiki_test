{"hands_on_practices": [{"introduction": "The Barzilai-Borwein method's power often comes from a counter-intuitive idea: sometimes, taking a step that isn't locally optimal can lead to a much better global convergence rate. This first exercise [@problem_id:2162618] provides a concrete look at this phenomenon by comparing a single step of the classic steepest descent method with the first BB step on an ill-conditioned quadratic problem. You will calculate the BB step size and uncover the geometric reasoning behind why \"overstepping\" the minimum along the current gradient direction can break the inefficient zig-zagging pattern of steepest descent, leading to faster overall convergence.", "problem": "Consider the problem of minimizing the quadratic objective function $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T A \\mathbf{x}$, where $\\mathbf{x} = (x_1, x_2)^T \\in \\mathbb{R}^2$. This function represents a simplified cost function in a system identification task, where the goal is to find the optimal parameter vector $\\mathbf{x}$ that minimizes the error. The matrix $A$ is given by:\n$$A = \\begin{pmatrix} 100  0 \\\\ 0  1 \\end{pmatrix}$$\nThe high condition number of this matrix indicates that the two parameters have vastly different sensitivities, making optimization challenging for standard methods.\n\nWe will use a gradient-based iterative method to find the minimum, starting from the initial point $\\mathbf{x}_0 = (1, 10)^T$. The general update rule is $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)$.\n\nFor the first iteration (from $k=0$ to $k=1$), a single step of the steepest descent method is performed using an exact line search. The step size $\\alpha_0$ is chosen to be the one that exactly minimizes $f(\\mathbf{x}_0 - \\alpha \\nabla f(\\mathbf{x}_0))$.\n\nFor all subsequent iterations ($k \\geq 1$), we switch to the Barzilai-Borwein (BB) method. One of the common step-size strategies for the Barzilai-Borwein method is defined as:\n$$\\alpha_k = \\frac{s_{k-1}^T s_{k-1}}{s_{k-1}^T y_{k-1}}$$\nwhere $s_{k-1} = \\mathbf{x}_k - \\mathbf{x}_{k-1}$ is the displacement vector from the previous step, and $y_{k-1} = \\nabla f(\\mathbf{x}_k) - \\nabla f(\\mathbf{x}_{k-1})$ is the change in the gradient.\n\nYou are tasked with two objectives:\n1.  Calculate the value of the first Barzilai-Borwein step size, $\\alpha_1$. Round your numerical answer to four significant figures.\n2.  The BB method is known to often outperform the steepest descent method on ill-conditioned quadratics, despite its non-monotonic behavior (i.e., the function value $f(\\mathbf{x}_k)$ may not decrease at every step). Select the statement below that best provides the geometric intuition for this accelerated convergence.\n\n(A) The BB method guarantees a monotonic decrease in the magnitude of the gradient at each step, forcing it to zero more rapidly.\n\n(B) By using a step size from the previous iteration's information, the method ensures that successive gradient vectors are perfectly collinear, eliminating zig-zagging behavior.\n\n(C) The BB step size is typically larger than what an exact line search would yield for the current direction. This \"overstepping\" breaks the strict orthogonality of consecutive gradients seen in steepest descent, producing a new search direction that is better aligned with the long-axis of the cost function's elliptical level sets.\n\n(D) The method computes a step size that is always the average of the reciprocals of the largest and smallest eigenvalues of the Hessian matrix $A$, providing a balanced move in all eigendirections.\n\n(E) The BB step size is always smaller than the steepest descent step size, which makes the algorithm more stable and prevents the divergence that can occur with large step sizes in ill-conditioned problems.\n\nProvide your answers for Part 1 and Part 2 as a two-element row matrix, with the numerical answer for $\\alpha_1$ as the first element and the capital letter corresponding to the correct choice for Part 2 as the second element.", "solution": "We are minimizing the quadratic $f(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^{T}A\\mathbf{x}$ with $A=\\mathrm{diag}(100,1)$. For a symmetric matrix $A$, the gradient is $\\nabla f(\\mathbf{x})=A\\mathbf{x}$.\n\nStarting from $\\mathbf{x}_{0}=\\begin{pmatrix}1\\\\10\\end{pmatrix}$, the steepest descent direction at $k=0$ is $-\\nabla f(\\mathbf{x}_{0})$. Compute the gradient:\n$$\n\\mathbf{g}_{0}=\\nabla f(\\mathbf{x}_{0})=A\\mathbf{x}_{0}=\\begin{pmatrix}100\\\\10\\end{pmatrix}.\n$$\nWith exact line search along $-\\mathbf{g}_{0}$, the optimal step size is the standard quadratic formula\n$$\n\\alpha_{0}=\\frac{\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}}{\\mathbf{g}_{0}^{T}A\\mathbf{g}_{0}}.\n$$\nCompute the required inner products:\n$$\n\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}=100^{2}+10^{2}=10100,\\qquad\nA\\mathbf{g}_{0}=\\begin{pmatrix}10000\\\\10\\end{pmatrix},\\qquad\n\\mathbf{g}_{0}^{T}A\\mathbf{g}_{0}=100\\cdot 10000+10\\cdot 10=1000100.\n$$\nHence\n$$\n\\alpha_{0}=\\frac{10100}{1000100}=\\frac{101}{10001}.\n$$\nUpdate to $\\mathbf{x}_{1}$:\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}-\\alpha_{0}\\mathbf{g}_{0}\n=\\begin{pmatrix}1\\\\10\\end{pmatrix}-\\frac{101}{10001}\\begin{pmatrix}100\\\\10\\end{pmatrix}\n=\\begin{pmatrix}1-\\frac{10100}{10001}\\\\ 10-\\frac{1010}{10001}\\end{pmatrix}\n=\\begin{pmatrix}-\\frac{99}{10001}\\\\ \\frac{99000}{10001}\\end{pmatrix}.\n$$\n\nFor the first Barzilai-Borwein step ($k=1$), define\n$$\n\\mathbf{s}_{0}=\\mathbf{x}_{1}-\\mathbf{x}_{0}=-\\alpha_{0}\\mathbf{g}_{0},\\qquad\n\\mathbf{y}_{0}=\\nabla f(\\mathbf{x}_{1})-\\nabla f(\\mathbf{x}_{0})=A\\mathbf{x}_{1}-A\\mathbf{x}_{0}=A(\\mathbf{x}_{1}-\\mathbf{x}_{0})=A\\mathbf{s}_{0}.\n$$\nThe BB step size prescribed is\n$$\n\\alpha_{1}=\\frac{\\mathbf{s}_{0}^{T}\\mathbf{s}_{0}}{\\mathbf{s}_{0}^{T}\\mathbf{y}_{0}}\n=\\frac{(-\\alpha_{0}\\mathbf{g}_{0})^{T}(-\\alpha_{0}\\mathbf{g}_{0})}{(-\\alpha_{0}\\mathbf{g}_{0})^{T}(A(-\\alpha_{0}\\mathbf{g}_{0}))}\n=\\frac{\\alpha_{0}^{2}\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}}{\\alpha_{0}^{2}\\mathbf{g}_{0}^{T}A\\mathbf{g}_{0}}\n=\\frac{\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}}{\\mathbf{g}_{0}^{T}A\\mathbf{g}_{0}}\n=\\alpha_{0}.\n$$\nTherefore,\n$$\n\\alpha_{1}=\\frac{101}{10001}\\approx 0.0100999\\ldots,\n$$\nwhich rounded to four significant figures is $0.01010$.\n\nFor the geometric intuition, the steepest descent method with exact line search produces gradients that are orthogonal at consecutive iterates on quadratics, which induces a zig-zag along the narrow valley for ill-conditioned problems. The Barzilai-Borwein method selects a step size using secant information that is typically larger than the exact line-search step for the current steepest-descent direction, thereby breaking the strict orthogonality of consecutive gradients and yielding a search direction better aligned with the long axis of the elliptical level sets, accelerating convergence on ill-conditioned quadratics. This corresponds to option (C).", "answer": "$$\\boxed{\\begin{pmatrix} 0.01010  C \\end{pmatrix}}$$", "id": "2162618"}, {"introduction": "Having built an intuition for the BB method, we now dig deeper into its mathematical foundation as a secant method. This practice [@problem_id:3100582] challenges you to derive the exact condition under which the first BB step size, $\\alpha_{k}^{\\text{BB1}}$, becomes identical to the step size from an exact line search, $\\alpha_{k}^{\\text{ELS}}$, for a quadratic objective. By working through a specific numerical example, you will see how this condition relates to the eigenvectors of the Hessian matrix, revealing how the BB method implicitly captures and utilizes curvature information.", "problem": "Consider unconstrained minimization of a strictly convex quadratic objective $f(x) = \\frac{1}{2} x^{\\top} Q x - b^{\\top} x$, where $Q \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and $b \\in \\mathbb{R}^{n}$. Let the gradient at iteration $k$ be $g_{k} = \\nabla f(x_{k})$. Define the displacement $s_{k} = x_{k} - x_{k-1}$ and the gradient difference $y_{k} = g_{k} - g_{k-1}$. A gradient descent step uses $x_{k+1} = x_{k} - \\alpha_{k} g_{k}$ with a step size $\\alpha_{k}  0$. The first Barzilai–Borwein (BB1) step size is the scalar chosen to approximate an inverse Hessian along $s_{k}$ and $y_{k}$.\n\nStarting from the fundamental definitions of the quadratic model, its gradient, and exact line search (minimization along the negative gradient direction), derive a mathematical condition involving $Q$ and the gradients $g_{k-1}$ and $g_{k}$ under which the BB1 step computed at iteration $k$ equals the exact line-search step size used at the same iteration for quadratic objectives.\n\nThen verify this condition with a concrete construction in $\\mathbb{R}^{2}$: take\n$$\nQ = \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix}, \\quad \\text{choose any } x_{k-1} \\in \\mathbb{R}^{2} \\text{ such that } g_{k-1} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}, \\quad \\alpha_{k-1} = \\frac{1}{5},\n$$\nset $x_{k} = x_{k-1} - \\alpha_{k-1} g_{k-1}$, and define $b = Q x_{k-1} - g_{k-1}$ so that the specified $g_{k-1}$ is consistent with the quadratic model. Under this construction, compute the common step size value $\\alpha$ that both BB1 and exact line search produce at iteration $k$. Express your final answer as an exact value (no rounding).", "solution": "**Part 1: Derivation of the Condition**\n\nThe exact line-search step size at iteration $k$, $\\alpha_k^{\\text{ELS}}$, is the value that minimizes $\\phi(\\alpha) = f(x_k - \\alpha g_k)$. For a quadratic objective, this is given by:\n$$ \\alpha_k^{\\text{ELS}} = \\frac{g_k^{\\top} g_k}{g_k^{\\top} Q g_k} $$\nThe first Barzilai-Borwein (BB1) step size at iteration $k$ is:\n$$ \\alpha_k^{\\text{BB1}} = \\frac{s_k^{\\top} s_k}{s_k^{\\top} y_k} $$\nwhere $s_k = x_k - x_{k-1}$ and $y_k = g_k - g_{k-1}$. For a quadratic, $y_k = Q s_k$. The step from $x_{k-1}$ to $x_k$ is a gradient step, $s_k = -\\alpha_{k-1} g_{k-1}$. Substituting these into the BB1 formula yields:\n$$ \\alpha_k^{\\text{BB1}} = \\frac{(-\\alpha_{k-1} g_{k-1})^{\\top} (-\\alpha_{k-1} g_{k-1})}{(-\\alpha_{k-1} g_{k-1})^{\\top} Q (-\\alpha_{k-1} g_{k-1})} = \\frac{g_{k-1}^{\\top} g_{k-1}}{g_{k-1}^{\\top} Q g_{k-1}} $$\nTherefore, the condition for the two step sizes to be equal, $\\alpha_k^{\\text{BB1}} = \\alpha_k^{\\text{ELS}}$, is:\n$$ \\frac{g_{k-1}^{\\top} g_{k-1}}{g_{k-1}^{\\top} Q g_{k-1}} = \\frac{g_k^{\\top} g_k}{g_k^{\\top} Q g_k} $$\n\n**Part 2: Verification and Computation**\n\nGiven the values $Q = \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix}$, $g_{k-1} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$, and $\\alpha_{k-1} = \\frac{1}{5}$.\n\nFirst, we compute the BB1 step size at iteration $k$:\n$$ \\alpha_k^{\\text{BB1}} = \\frac{g_{k-1}^{\\top} g_{k-1}}{g_{k-1}^{\\top} Q g_{k-1}} = \\frac{0^2+2^2}{\\begin{pmatrix} 0  2 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}} = \\frac{4}{16} = \\frac{1}{4} $$\nNext, we compute the gradient $g_k$ using the update rule $g_k = (I - \\alpha_{k-1} Q) g_{k-1}$:\n$$ g_k = \\left(\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix}\\right) \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4/5  0 \\\\ 0  1/5 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2/5 \\end{pmatrix} $$\nFinally, we compute the exact line-search step size at iteration $k$:\n$$ \\alpha_k^{\\text{ELS}} = \\frac{g_k^{\\top} g_k}{g_k^{\\top} Q g_k} = \\frac{(2/5)^2}{\\begin{pmatrix} 0  2/5 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2/5 \\end{pmatrix}} = \\frac{4/25}{16/25} = \\frac{4}{16} = \\frac{1}{4} $$\nThe two step sizes are equal. The common value is $\\frac{1}{4}$.", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "3100582"}, {"introduction": "A powerful theoretical algorithm must also be numerically robust to be useful in practice. This final exercise [@problem_id:3100631] confronts a common challenge in implementing BB methods: the potential for numerical instability when dealing with functions that have highly varied curvature in different directions. You will analyze a scenario where the second BB step size, $\\alpha_{k}^{\\text{BB2}}$, can become pathologically large and evaluate several common safeguarding strategies, learning to distinguish between ad-hoc fixes and principled, robust implementation techniques.", "problem": "Consider the gradient method applied to a smooth quadratic objective in $3$ dimensions, defined by $f(x) = \\tfrac{1}{2} x^{\\top} H x$ with a symmetric positive definite Hessian $H \\in \\mathbb{R}^{3 \\times 3}$. Let $H = \\operatorname{diag}(10, 10, 10^{-8})$, and suppose at iteration $k-1$ the point is $x_{k-1} = (1, 1, 1)^{\\top}$ and the next iterate is taken along the third coordinate direction: $x_{k} = x_{k-1} + \\delta e_{3} = (1, 1, 1 + \\delta)^{\\top}$ for some $\\delta \\in \\mathbb{R}$, where $e_{3}$ is the third standard basis vector. Define the standard gradient-difference and step-difference quantities used in gradient-based methods: $s_{k-1} = x_{k} - x_{k-1}$ and $y_{k-1} = \\nabla f(x_{k}) - \\nabla f(x_{k-1})$. Assume a Barzilai–Borwein (BB) gradient method that periodically computes the second Barzilai–Borwein step size (BB2), which is built from the secant idea using $s_{k-1}$ and $y_{k-1}$, and recall that gradients may be nonzero even if gradient differences are small.\n\nPart $\\mathrm{I}$ (construction): Using the fundamental relation for quadratic objectives, $\\nabla f(x) = H x$, show that for $\\delta$ of order $1$ the quantity $y_{k-1}^{\\top} y_{k-1}$ can be arbitrarily close to machine precision while the individual gradients $\\nabla f(x_{k-1})$ and $\\nabla f(x_{k})$ are nonzero and of moderately large magnitude. Explain the role of anisotropic curvature (eigenvalues of $H$) in producing this effect.\n\nPart $\\mathrm{II}$ (safeguards): The BB2 step size depends critically on $y_{k-1}^{\\top} y_{k-1}$. In the scenario above, selecting BB2 without safeguards may lead to numerical instability. Which of the following strategies is the most appropriate to ensure stable iteration while preserving the secant-motivated scaling and providing a principled fallback to the first Barzilai–Borwein step size (BB1)?\n\nA. Impose bounds on the BB2 step size by clipping it into an interval $[\\alpha_{\\min}, \\alpha_{\\max}]$ with $\\alpha_{\\min}  0$ and $\\alpha_{\\max}  \\infty$; additionally, if $y_{k-1}^{\\top} y_{k-1} \\le \\varepsilon$ for a small threshold $\\varepsilon  0$, bypass BB2 and compute the BB1 step size instead, using it only if $s_{k-1}^{\\top} y_{k-1}  0$; otherwise, reuse the previous step size or a safe default.\n\nB. Replace $y_{k-1}^{\\top} y_{k-1}$ by $y_{k-1}^{\\top} y_{k-1} + \\varepsilon$ with a small $\\varepsilon  0$ in the BB2 formula and proceed without any bounding or fallback, since the regularization alone prevents division by zero.\n\nC. Normalize the gradient difference before forming BB2 by setting $\\tilde{y}_{k-1} = y_{k-1} / \\|y_{k-1}\\|_{2}$ and then use $\\tilde{y}_{k-1}$ in place of $y_{k-1}$ to avoid small denominators; this ensures the denominator is exactly $1$.\n\nD. Rely on a monotone backtracking line search after computing BB2, which will reject any unstable step lengths; no bounding or fallback is needed because the line search alone enforces stability.\n\nSelect the single best option.", "solution": "The problem is scientifically valid and highlights a key numerical challenge in implementing BB methods on ill-conditioned problems.\n\n**Part I: Construction and Analysis**\n\nThe gradient of the quadratic objective is $\\nabla f(x) = Hx$.\n1.  **Gradients**: The gradients at the two points are $\\nabla f(x_{k-1}) = H x_{k-1} = (10, 10, 10^{-8})^{\\top}$ and $\\nabla f(x_{k}) = H x_{k} = (10, 10, 10^{-8}(1+\\delta))^{\\top}$. For $\\delta \\sim O(1)$, the norms of both gradients are $\\approx\\sqrt{200}$, which is a moderately large magnitude.\n\n2.  **Step and Gradient Differences**: The step difference is $s_{k-1} = x_{k} - x_{k-1} = (0, 0, \\delta)^{\\top}$. The gradient difference is $y_{k-1} = \\nabla f(x_{k}) - \\nabla f(x_{k-1}) = (0, 0, \\delta \\cdot 10^{-8})^{\\top}$.\n\n3.  **Demonstration**: The denominator for the BB2 step is $y_{k-1}^{\\top} y_{k-1} = \\|y_{k-1}\\|_2^2 = (\\delta \\cdot 10^{-8})^2 = \\delta^2 \\cdot 10^{-16}$. This value is on the order of machine precision, confirming the scenario.\n\n**Explanation**: This effect is caused by **anisotropic curvature**. The step $s_{k-1}$ is taken purely in the direction of minimum curvature (eigenvalue $\\lambda_3=10^{-8}$). The resulting gradient change, $y_{k-1}=Hs_{k-1}$, is scaled by this tiny eigenvalue, making it minuscule, even though the gradient itself is large due to components in high-curvature directions.\n\n**Part II: Safeguards for the BB2 Step Size**\n\nThe BB2 step size, $\\alpha_k^{\\text{BB2}} = \\frac{s_{k-1}^{\\top} y_{k-1}}{y_{k-1}^{\\top} y_{k-1}}$, becomes $\\frac{\\delta^2 \\cdot 10^{-8}}{\\delta^2 \\cdot 10^{-16}} = 10^8$, a pathologically large value. The most appropriate safeguarding strategy is **A**.\n\n*   **Option A** describes a comprehensive, multi-layered defense that is standard practice. It proactively detects the failure mode (small denominator), provides a principled fallback (to BB1), includes an essential curvature check ($s_{k-1}^{\\top} y_{k-1} > 0$), and uses bounding as a final safeguard. This preserves the secant motivation of the BB method while ensuring stability.\n*   **Option B** is an ad-hoc fix that alters the BB formula.\n*   **Option C** fundamentally breaks the method by discarding crucial curvature information.\n*   **Option D** is inappropriate as it relies on a *monotone* line search for an inherently non-monotone method and is a reactive, inefficient fix.\n\nTherefore, strategy A is the most principled and robust choice.", "answer": "$$\\boxed{A}$$", "id": "3100631"}]}