{"hands_on_practices": [{"introduction": "Understanding a powerful algorithm often begins by examining its behavior in an idealized setting. This first exercise explores the special case of minimizing a quadratic function whose Hessian matrix is a multiple of the identity, $A = cI$. This scenario corresponds to a perfectly spherical objective function, and as you will demonstrate, it allows the Conjugate Gradient method to find the exact minimum in a single step, revealing a direct connection to the simpler method of steepest descent [@problem_id:2211314].", "problem": "Consider the minimization of the multivariate quadratic function $f(\\mathbf{x})$ defined as:\n$$f(\\mathbf{x}) = \\frac{1}{2} c \\mathbf{x}^T \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}$$\nwhere $\\mathbf{x}$ and $\\mathbf{b}$ are column vectors in $\\mathbb{R}^n$ for some integer $n \\ge 1$, and $c$ is a positive real-valued constant.\n\nStarting from an arbitrary initial point $\\mathbf{x}_0$, for which the gradient of $f$ is non-zero, a single iteration of the conjugate gradient (CG) method is performed to find the next point, $\\mathbf{x}_1$.\n\nDetermine the expression for $\\mathbf{x}_1$. Express your answer in terms of the constant $c$ and the vectors $\\mathbf{b}$ and $\\mathbf{x}_0$.", "solution": "We consider the quadratic function $f(\\mathbf{x})=\\frac{1}{2}c\\,\\mathbf{x}^{T}\\mathbf{x}-\\mathbf{b}^{T}\\mathbf{x}$ with $c0$. Its gradient and Hessian are\n$$\n\\nabla f(\\mathbf{x})=c\\,\\mathbf{x}-\\mathbf{b}, \\qquad \\nabla^{2}f(\\mathbf{x})=c\\,\\mathbf{I},\n$$\nso minimizing $f$ is equivalent to solving the linear system\n$$\n\\mathbf{A}\\mathbf{x}=\\mathbf{b}, \\quad \\text{with} \\quad \\mathbf{A}=c\\,\\mathbf{I},\n$$\nwhich is symmetric positive definite.\n\nIn the conjugate gradient method, starting from $\\mathbf{x}_{0}$ with nonzero gradient, define the residual and search direction at the first iteration as\n$$\n\\mathbf{r}_{0}=\\mathbf{b}-\\mathbf{A}\\mathbf{x}_{0}=\\mathbf{b}-c\\,\\mathbf{x}_{0}, \\qquad \\mathbf{p}_{0}=\\mathbf{r}_{0}.\n$$\nThe step size $\\alpha_{0}$ is given by\n$$\n\\alpha_{0}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{\\mathbf{p}_{0}^{T}\\mathbf{A}\\mathbf{p}_{0}}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{\\mathbf{r}_{0}^{T}(c\\,\\mathbf{I})\\mathbf{r}_{0}}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{c\\,\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}=\\frac{1}{c}.\n$$\nThus, the updated point after one CG iteration is\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}+\\alpha_{0}\\mathbf{p}_{0}=\\mathbf{x}_{0}+\\frac{1}{c}\\left(\\mathbf{b}-c\\,\\mathbf{x}_{0}\\right)=\\frac{1}{c}\\,\\mathbf{b}.\n$$\nTherefore, a single CG step reaches the exact minimizer since $\\mathbf{A}=c\\,\\mathbf{I}$ has a single eigenvalue.", "answer": "$$\\boxed{\\frac{1}{c}\\,\\mathbf{b}}$$", "id": "2211314"}, {"introduction": "Having explored an ideal case, we now dive into the complete mechanics of the Conjugate Gradient algorithm. This practice guides you through a manual, step-by-step calculation for a 3-dimensional problem, building each iterate from first principles. By computing the residuals, search directions, and step sizes yourself, you will gain a concrete understanding of how the core concepts of $A$-conjugacy and residual orthogonality work together to guarantee convergence in at most $n$ steps [@problem_id:3111640].", "problem": "Consider the unconstrained quadratic minimization problem with objective function $f(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$, where $A \\in \\mathbb{R}^{3 \\times 3}$ is symmetric positive definite (SPD) with distinct eigenvalues. Let\n$$\nA = \\begin{pmatrix}\n2  0  0 \\\\\n0  3  0 \\\\\n0  0  5\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n1 \\\\ 1 \\\\ 1\n\\end{pmatrix}, \\quad\nx_{0} = \\begin{pmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\nNote that $A$ is SPD and has distinct eigenvalues $2$, $3$, and $5$. Apply the Conjugate Gradient (CG) method to minimize $f(x)$, starting at $x_{0}$, by constructing search directions that are pairwise $A$-conjugate and by performing at each step an exact line search along the current search direction. Work from first principles as follows:\n- Use the definition of $f(x)$ and the Euclidean inner product to derive the step size at each iteration by minimizing $f(x_{k} + \\alpha p_{k})$ with respect to $\\alpha$.\n- Impose $A$-conjugacy of successive directions to determine the recurrence for the new search direction, and justify the orthogonality relations that arise between residuals and search directions.\n- Carry out exactly $3$ Conjugate Gradient iterations $(k = 0, 1, 2)$ by hand, computing $x_{1}$, $x_{2}$, and $x_{3}$, and verify termination by showing the residual at $x_{3}$ is the zero vector.\n\nWhat is the exact value of $f(x_{3})$? Provide your answer as a single simplified fraction. No rounding is required.", "solution": "The problem is to minimize the quadratic function $f(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$ using the Conjugate Gradient (CG) method. The gradient of this function is $\\nabla f(x) = A x - b$. The minimum is achieved when $\\nabla f(x) = 0$, which corresponds to solving the linear system $A x = b$. The residual at an iterate $x_k$ is defined as $r_k = b - A x_k = -\\nabla f(x_k)$. The CG method generates a sequence of iterates $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is the search direction and $\\alpha_k$ is the step size.\n\nFirst, we derive the step size $\\alpha_k$ by performing an exact line search. We minimize $\\phi(\\alpha) = f(x_k + \\alpha p_k)$ with respect to $\\alpha$:\n$$\n\\phi(\\alpha) = \\frac{1}{2} (x_k + \\alpha p_k)^{\\top} A (x_k + \\alpha p_k) - b^{\\top} (x_k + \\alpha p_k)\n$$\nExpanding this expression and using the symmetry of $A$ ($A=A^{\\top}$), we get:\n$$\n\\phi(\\alpha) = \\frac{1}{2} x_k^{\\top} A x_k - b^{\\top} x_k + \\alpha (p_k^{\\top} A x_k - p_k^{\\top} b) + \\frac{1}{2} \\alpha^2 p_k^{\\top} A p_k\n$$\nTo find the minimum, we set the derivative with respect to $\\alpha$ to zero:\n$$\n\\frac{d\\phi}{d\\alpha} = p_k^{\\top} A x_k - p_k^{\\top} b + \\alpha p_k^{\\top} A p_k = p_k^{\\top} (A x_k - b) + \\alpha p_k^{\\top} A p_k = -p_k^{\\top} r_k + \\alpha p_k^{\\top} A p_k = 0\n$$\nSolving for $\\alpha$, we get the step size:\n$$\n\\alpha_k = \\frac{p_k^{\\top} r_k}{p_k^{\\top} A p_k}\n$$\n\nNext, we determine the recurrence for the search direction $p_k$. The initial search direction is the steepest descent direction, $p_0 = r_0$. For subsequent steps, the new search direction $p_{k+1}$ is constructed to be a linear combination of the new residual $r_{k+1}$ and the previous search direction $p_k$:\n$$\np_{k+1} = r_{k+1} + \\beta_k p_k\n$$\nThe coefficient $\\beta_k$ is chosen to enforce $A$-conjugacy between successive search directions, i.e., $p_{k+1}^{\\top} A p_k = 0$.\n$$\n(r_{k+1} + \\beta_k p_k)^{\\top} A p_k = 0 \\implies r_{k+1}^{\\top} A p_k + \\beta_k p_k^{\\top} A p_k = 0\n$$\nThis gives the Hestenes-Stiefel formula for $\\beta_k$:\n$$\n\\beta_k = -\\frac{r_{k+1}^{\\top} A p_k}{p_k^{\\top} A p_k}\n$$\nThe residual is updated as $r_{k+1} = r_k - \\alpha_k A p_k$, from which we get $A p_k = \\frac{1}{\\alpha_k} (r_k - r_{k+1})$. Substituting this into the numerator for $\\beta_k$:\n$$\nr_{k+1}^{\\top} A p_k = \\frac{1}{\\alpha_k} r_{k+1}^{\\top} (r_k - r_{k+1}) = -\\frac{1}{\\alpha_k} r_{k+1}^{\\top} r_{k+1}\n$$\nHere we have used the fact that successive residuals are orthogonal, $r_{k+1}^{\\top} r_k = 0$. Also, the search direction update $p_k = r_k + \\beta_{k-1} p_{k-1}$ and the orthogonality of residuals $r_k^{\\top} r_j=0$ for $j  k$ implies $p_k^{\\top} r_k = (r_k + \\beta_{k-1} p_{k-1})^{\\top} r_k = r_k^{\\top} r_k$. This simplifies the step size formula to $\\alpha_k = \\frac{r_k^{\\top} r_k}{p_k^{\\top} A p_k}$.\nSubstituting this into the expression for $\\beta_k$:\n$$\n\\beta_k = -\\frac{- (r_{k+1}^{\\top} r_{k+1}) / \\alpha_k}{p_k^{\\top} A p_k} = \\frac{r_{k+1}^{\\top} r_{k+1}}{\\alpha_k (p_k^{\\top} A p_k)} = \\frac{r_{k+1}^{\\top} r_{k+1}}{(\\frac{r_k^{\\top} r_k}{p_k^{\\top} A p_k}) (p_k^{\\top} A p_k)} = \\frac{r_{k+1}^{\\top} r_{k+1}}{r_k^{\\top} r_k}\n$$\nThis is the Fletcher-Reeves formula for $\\beta_k$, which we will use for the calculations.\n\nWe are given $A = \\begin{pmatrix} 2  0  0 \\\\ 0  3  0 \\\\ 0  0  5 \\end{pmatrix}$, $b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$, and $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\n**Iteration $k=0$:**\n1.  Initial residual: $r_0 = b - A x_0 = b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n2.  Initial search direction: $p_0 = r_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n3.  Compute $r_0^{\\top} r_0 = 1^2 + 1^2 + 1^2 = 3$.\n4.  Compute $A p_0 = \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix}$.\n5.  Compute $p_0^{\\top} A p_0 = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix} = 2+3+5 = 10$.\n6.  Step size: $\\alpha_0 = \\frac{r_0^{\\top} r_0}{p_0^{\\top} A p_0} = \\frac{3}{10}$.\n7.  Update solution: $x_1 = x_0 + \\alpha_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{3}{10} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3/10 \\\\ 3/10 \\\\ 3/10 \\end{pmatrix}$.\n8.  Update residual: $r_1 = r_0 - \\alpha_0 A p_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{3}{10} \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 1 - 6/10 \\\\ 1 - 9/10 \\\\ 1 - 15/10 \\end{pmatrix} = \\begin{pmatrix} 4/10 \\\\ 1/10 \\\\ -5/10 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix}$.\n\n**Iteration $k=1$:**\n1.  Compute $r_1^{\\top} r_1 = (\\frac{1}{10})^2 (4^2 + 1^2 + (-5)^2) = \\frac{1}{100}(16+1+25) = \\frac{42}{100} = \\frac{21}{50}$.\n2.  Compute $\\beta_0 = \\frac{r_1^{\\top} r_1}{r_0^{\\top} r_0} = \\frac{21/50}{3} = \\frac{7}{50}$.\n3.  Update search direction: $p_1 = r_1 + \\beta_0 p_0 = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix} + \\frac{7}{50} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{50} \\begin{pmatrix} 20 \\\\ 5 \\\\ -25 \\end{pmatrix} + \\frac{1}{50} \\begin{pmatrix} 7 \\\\ 7 \\\\ 7 \\end{pmatrix} = \\frac{1}{50} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix}$.\n4.  Compute $A p_1 = \\frac{1}{50} \\begin{pmatrix} 2  0  0 \\\\ 0  3  0 \\\\ 0  0  5 \\end{pmatrix} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{50} \\begin{pmatrix} 54 \\\\ 36 \\\\ -90 \\end{pmatrix}$.\n5.  Compute $p_1^{\\top} A p_1 = \\frac{1}{50} \\begin{pmatrix} 27  12  -18 \\end{pmatrix} \\frac{1}{50} \\begin{pmatrix} 54 \\\\ 36 \\\\ -90 \\end{pmatrix} = \\frac{1}{2500}(27 \\cdot 54 + 12 \\cdot 36 - 18 \\cdot (-90)) = \\frac{1458+432+1620}{2500} = \\frac{3510}{2500} = \\frac{351}{250}$.\n6.  Step size: $\\alpha_1 = \\frac{r_1^{\\top} r_1}{p_1^{\\top} A p_1} = \\frac{21/50}{351/250} = \\frac{21}{50} \\cdot \\frac{250}{351} = \\frac{21 \\cdot 5}{351} = \\frac{105}{351} = \\frac{35}{117}$.\n7.  Update solution: $x_2 = x_1 + \\alpha_1 p_1 = \\begin{pmatrix} 3/10 \\\\ 3/10 \\\\ 3/10 \\end{pmatrix} + \\frac{35}{117} \\frac{1}{50} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{10}\\begin{pmatrix} 3 \\\\ 3 \\\\ 3 \\end{pmatrix} + \\frac{7}{1170} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{117}{1170} \\begin{pmatrix} 3 \\\\ 3 \\\\ 3 \\end{pmatrix} + \\frac{1}{1170} \\begin{pmatrix} 189 \\\\ 84 \\\\ -126 \\end{pmatrix} = \\frac{1}{1170} \\begin{pmatrix} 351+189 \\\\ 351+84 \\\\ 351-126 \\end{pmatrix} = \\frac{1}{1170} \\begin{pmatrix} 540 \\\\ 435 \\\\ 225 \\end{pmatrix} = \\frac{15}{1170} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix}$.\n8.  Update residual: $r_2 = r_1 - \\alpha_1 A p_1 = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix} - \\frac{35}{117} \\frac{1}{50} \\begin{pmatrix} 54 \\\\ 36 \\\\ -90 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix} - \\frac{7}{65} \\begin{pmatrix} 3 \\\\ 2 \\\\ -5 \\end{pmatrix} = \\frac{1}{130} \\begin{pmatrix} 52-42 \\\\ 13-28 \\\\ -65+70 \\end{pmatrix} = \\frac{1}{130} \\begin{pmatrix} 10 \\\\ -15 \\\\ 5 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix}$.\n\n**Iteration $k=2$:**\n1.  Compute $r_2^{\\top} r_2 = (\\frac{1}{26})^2 (2^2 + (-3)^2 + 1^2) = \\frac{1}{676}(4+9+1) = \\frac{14}{676} = \\frac{7}{338}$.\n2.  Compute $\\beta_1 = \\frac{r_2^{\\top} r_2}{r_1^{\\top} r_1} = \\frac{7/338}{21/50} = \\frac{7}{338} \\cdot \\frac{50}{21} = \\frac{1}{338} \\frac{50}{3} = \\frac{25}{507}$.\n3.  Update search direction: $p_2 = r_2 + \\beta_1 p_1 = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} + \\frac{25}{507} \\frac{1}{50} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} + \\frac{1}{1014} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{39}{1014} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} + \\frac{1}{1014} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{1014} \\begin{pmatrix} 78+27 \\\\ -117+12 \\\\ 39-18 \\end{pmatrix} = \\frac{1}{1014} \\begin{pmatrix} 105 \\\\ -105 \\\\ 21 \\end{pmatrix} = \\frac{21}{1014} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{7}{338} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix}$.\n4.  Compute $A p_2 = \\frac{7}{338} \\begin{pmatrix} 2  0  0 \\\\ 0  3  0 \\\\ 0  0  5 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{7}{338} \\begin{pmatrix} 10 \\\\ -15 \\\\ 5 \\end{pmatrix} = \\frac{35}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix}$.\n5.  Compute $p_2^{\\top} A p_2 = \\frac{7}{338} \\begin{pmatrix} 5  -5  1 \\end{pmatrix} \\frac{35}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\frac{245}{338^2}(10+15+1) = \\frac{245 \\cdot 26}{338^2} = \\frac{245}{338 \\cdot 13} = \\frac{245}{4394}$.\n6.  Step size: $\\alpha_2 = \\frac{r_2^{\\top} r_2}{p_2^{\\top} A p_2} = \\frac{7/338}{245/4394} = \\frac{7}{338} \\frac{4394}{245} = \\frac{7 \\cdot 13}{245} = \\frac{91}{245} = \\frac{13}{35}$.\n7.  Update solution: $x_3 = x_2 + \\alpha_2 p_2 = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{13}{35} \\frac{7}{338} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{1}{5 \\cdot 26} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{1}{130} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{5}{390} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{3}{390} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{1}{390} \\begin{pmatrix} 180+15 \\\\ 145-15 \\\\ 75+3 \\end{pmatrix} = \\frac{1}{390} \\begin{pmatrix} 195 \\\\ 130 \\\\ 78 \\end{pmatrix} = \\begin{pmatrix} 195/390 \\\\ 130/390 \\\\ 78/390 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/3 \\\\ 1/5 \\end{pmatrix}$.\n8.  Update residual: $r_3 = r_2 - \\alpha_2 A p_2 = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} - \\frac{13}{35} \\frac{35}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} - \\frac{13}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} - \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nThe residual $r_3$ is the zero vector, which verifies that the method has converged to the exact solution in $3$ iterations, as expected for a $3 \\times 3$ system.\nThe final solution is $x_3 = x^* = A^{-1} b = \\begin{pmatrix} 1/2 \\\\ 1/3 \\\\ 1/5 \\end{pmatrix}$.\n\nWe are asked to compute $f(x_3)$.\n$$\nf(x_3) = \\frac{1}{2} x_3^{\\top} A x_3 - b^{\\top} x_3\n$$\nSince $r_3 = b - Ax_3 = 0$, we have $Ax_3 = b$. Substituting this into the expression for $f(x_3)$:\n$$\nf(x_3) = \\frac{1}{2} x_3^{\\top} b - b^{\\top} x_3 = -\\frac{1}{2} b^{\\top} x_3\n$$\nNow we compute the value:\n$$\nb^{\\top} x_3 = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/3 \\\\ 1/5 \\end{pmatrix} = \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{5} = \\frac{15}{30} + \\frac{10}{30} + \\frac{6}{30} = \\frac{31}{30}\n$$\nTherefore, the minimum value of the function is:\n$$\nf(x_3) = -\\frac{1}{2} \\left( \\frac{31}{30} \\right) = -\\frac{31}{60}\n$$", "answer": "$$\\boxed{-\\frac{31}{60}}$$", "id": "3111640"}, {"introduction": "In practical applications, especially for large-scale systems, running the CG algorithm for the full $n$ iterations is often computationally prohibitive, so we stop when the solution is \"good enough\". This coding exercise addresses the practical question of how to measure convergence by comparing two widely used stopping criteria: one based on the residual ($r_k = b - A x_k$) and another on the true error ($e_k = x_k - x^\\star$). By testing these criteria on problems with different conditioning, you will gain valuable insight into their respective behaviors and learn why choosing the right stopping condition is crucial for a robust implementation [@problem_id:3111692].", "problem": "Consider the unconstrained minimization of a strictly convex quadratic function $f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x$ where $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and $b \\in \\mathbb{R}^n$. The unique minimizer $x^\\star$ satisfies the first-order optimality condition $A x^\\star = b$. The Conjugate Gradient (CG) method iteratively produces approximations $x_k$ that minimize $f(x)$ over growing Krylov subspaces. Two widely used stopping criteria for the CG iterates are based on the following quantities:\n\n- The residual $r_k = b - A x_k$, and the relative residual norm $\\|r_k\\|_2 / \\|b\\|_2$.\n- The error $e_k = x_k - x^\\star$, and the energy norm $\\|e_k\\|_A = \\sqrt{e_k^\\top A e_k}$.\n\nYour task is to:\n1. Implement the Conjugate Gradient (CG) method for symmetric positive definite $A$ and compute the iterate sequence $\\{x_k\\}$ starting from $x_0 = 0$.\n2. For each iterate $x_k$, compute the residual $r_k$ and the error $e_k$ using the exact solution $x^\\star = A^{-1} b$. Use these to evaluate $\\|r_k\\|_2 / \\|b\\|_2$ and $\\|e_k\\|_A$ at each iteration $k$.\n3. For given tolerances $\\tau_{\\mathrm{res}}$ and $\\tau_{\\mathrm{en}}$, define the stopping iterations\n   $$k_{\\mathrm{res}} = \\min\\{k \\ge 0 : \\|r_k\\|_2 / \\|b\\|_2 \\le \\tau_{\\mathrm{res}}\\}, \\quad\n   k_{\\mathrm{en}} = \\min\\{k \\ge 0 : \\|e_k\\|_A \\le \\tau_{\\mathrm{en}}\\}.$$\n   If a criterion is not met within the prescribed iteration limit $k \\in \\{0,1,\\dots,n\\}$, set the corresponding stopping iteration to $n$.\n4. Report, for each test case, the pair $(k_{\\mathrm{res}}, k_{\\mathrm{en}})$ and whether they differ. If $\\|b\\|_2 = 0$, define the relative residual at $k=0$ to be $0$ and treat the problem as solved with $x^\\star = 0$.\n\nUse the following test suite with explicit matrices, vectors, and tolerances. All matrices are diagonal and all entries are real numbers:\n\n- Case $1$ (happy path, well-conditioned):\n  - $A = \\mathrm{diag}(1, 2, 3, 4, 5)$,\n  - $b = [1, 0.5, -1, 2, 0.25]^\\top$,\n  - $\\tau_{\\mathrm{res}} = 10^{-8}$, $\\tau_{\\mathrm{en}} = 10^{-8}$.\n\n- Case $2$ (significant eigenvalue spread, ill-conditioned):\n  - $A = \\mathrm{diag}(10^{-6}, 10^{-4}, 10^{-2}, 1, 10)$,\n  - $b = [1, 1, 1, 1, 1]^\\top$,\n  - $\\tau_{\\mathrm{res}} = 10^{-8}$, $\\tau_{\\mathrm{en}} = 10^{-8}$.\n\n- Case $3$ (boundary condition, zero right-hand side):\n  - $A = \\mathrm{diag}(2, 3, 4)$,\n  - $b = [0, 0, 0]^\\top$,\n  - $\\tau_{\\mathrm{res}} = 10^{-8}$, $\\tau_{\\mathrm{en}} = 10^{-8}$.\n\n- Case $4$ (different practical thresholds to provoke divergence of criteria):\n  - $A = \\mathrm{diag}(10^{-4}, 1, 100)$,\n  - $b = [1, 1, 1]^\\top$,\n  - $\\tau_{\\mathrm{res}} = 10^{-10}$, $\\tau_{\\mathrm{en}} = 10^{-4}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a bracketed list of the form $[k_{\\mathrm{res}},k_{\\mathrm{en}},\\mathrm{diff}]$ where $\\mathrm{diff}$ is a boolean indicating whether $k_{\\mathrm{res}} \\ne k_{\\mathrm{en}}$. There must be no spaces anywhere in the output. For example, if there were two cases the output format would be $[[k_{\\mathrm{res}}^{(1)},k_{\\mathrm{en}}^{(1)},\\mathrm{diff}^{(1)}],[k_{\\mathrm{res}}^{(2)},k_{\\mathrm{en}}^{(2)},\\mathrm{diff}^{(2)}]]$.", "solution": "The user has provided a problem that requires the implementation and analysis of the Conjugate Gradient (CG) method for minimizing a strictly convex quadratic function. The core of the task is to compare two common stopping criteria: one based on the relative residual norm and the other on the error's energy norm.\n\n### Problem Formulation\n\nThe problem is to find the minimizer $x^\\star$ of the unconstrained quadratic objective function:\n$$ f(x) = \\frac{1}{2} x^\\top A x - b^\\top x $$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite (SPD) matrix and $b \\in \\mathbb{R}^n$. The unique minimizer $x^\\star$ is the solution to the linear system of equations derived from the first-order optimality condition $\\nabla f(x) = A x - b = 0$:\n$$ A x^\\star = b $$\n\nThe Conjugate Gradient method is an iterative algorithm designed to solve such systems efficiently. It generates a sequence of approximations $\\{x_k\\}$ that provably converges to $x^\\star$ in at most $n$ iterations in exact arithmetic. The problem specifies an initial guess of $x_0 = 0$.\n\n### The Conjugate Gradient Algorithm\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization ($k=0$):**\n    *   Set the initial guess: $x_0 = 0$.\n    *   Compute the initial residual: $r_0 = b - A x_0 = b$.\n    *   Set the initial search direction: $p_0 = r_0$.\n\n2.  **Iterative Refinement:** For $k = 0, 1, 2, \\dots, n-1$:\n    *   Compute the matrix-vector product $A p_k$.\n    *   Calculate the optimal step size $\\alpha_k$ along the direction $p_k$:\n        $$ \\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k} $$\n    *   Update the solution iterate:\n        $$ x_{k+1} = x_k + \\alpha_k p_k $$\n    *   Update the residual using a computationally stable formula:\n        $$ r_{k+1} = r_k - \\alpha_k A p_k $$\n    *   Compute the factor $\\beta_k$ to construct the next A-orthogonal search direction:\n        $$ \\beta_k = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k} $$\n    *   Update the search direction:\n        $$ p_{k+1} = r_{k+1} + \\beta_k p_k $$\n    \nAt each iteration $k$, the iterate $x_k$ minimizes the quadratic function $f(x)$ over the affine subspace $x_0 + \\mathcal{K}_k(A, r_0)$, where $\\mathcal{K}_k(A, r_0) = \\mathrm{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$ is the $k$-th Krylov subspace.\n\n### Monitored Quantities and Stopping Criteria\n\nThe performance of the algorithm is monitored using two key metrics at each iteration $k \\in \\{0, 1, \\dots, n\\}$:\n\n1.  **Relative Residual Norm:** The residual $r_k = b - A x_k$ measures how well the current iterate $x_k$ satisfies the target equation. The relative residual norm is defined as $\\|r_k\\|_2 / \\|b\\|_2$, where $\\| \\cdot \\|_2$ denotes the Euclidean norm. For the special case where $b=0$, the problem specifies that this ratio is $0$ at $k=0$.\n\n2.  **Energy Norm of the Error:** The error is $e_k = x_k - x^\\star$. The A-norm, or energy norm, of the error is defined as $\\|e_k\\|_A = \\sqrt{e_k^\\top A e_k}$. The CG method is fundamentally designed to minimize this quantity at each step; that is, $x_k$ is the solution to $\\min_{x \\in x_0 + \\mathcal{K}_k(A, r_0)} \\|x-x^\\star\\|_A$.\n\nBased on these quantities and given tolerances $\\tau_{\\mathrm{res}}$ and $\\tau_{\\mathrm{en}}$, the stopping iterations are defined as the first iteration $k$ where each respective criterion is met:\n$$ k_{\\mathrm{res}} = \\min\\{k \\ge 0 : \\frac{\\|r_k\\|_2}{\\|b\\|_2} \\le \\tau_{\\mathrm{res}}\\} $$\n$$ k_{\\mathrm{en}} = \\min\\{k \\ge 0 : \\|e_k\\|_A \\le \\tau_{\\mathrm{en}}\\} $$\nIf a criterion is not satisfied for any $k \\in \\{0, 1, \\dots, n\\}$, the corresponding stopping iteration is set to $n$.\n\n### Implementation Details\n\nFor each test case, the implementation proceeds as follows:\n- The exact solution $x^\\star$ is computed first to enable calculation of the error $e_k$. Since all matrices $A$ are diagonal, $x^\\star_i = b_i / A_{ii}$.\n- The stopping iterations $k_{\\mathrm{res}}$ and $k_{\\mathrm{en}}$ are initialized to $n$.\n- The conditions are checked at $k=0$ with the initial values $x_0=0$, $r_0=b$, and $e_0=-x^\\star$. If a criterion is met, the corresponding stopping iteration is set to $0$.\n- The CG algorithm is then executed for $k = 1, \\dots, n$ iterations. At each step, the two criteria are checked. If a criterion is met for the first time at iteration $k$, the corresponding stopping iteration is updated to $k$.\n- The loop can terminate early if both stopping iterations have been found.\n- The special case of $b=0$ is handled as defined: $x^\\star = 0$, leading to $r_0=0$ and $e_0=0$. Therefore, both criteria are met at $k=0$, yielding $(k_{\\mathrm{res}}, k_{\\mathrm{en}}) = (0, 0)$.\n- The product of a diagonal matrix $A$ with a vector $p$ is computed efficiently via element-wise multiplication of the diagonal entries of $A$ and the elements of $p$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_cg_case(A_diag: np.ndarray, b: np.ndarray, tau_res: float, tau_en: float):\n    \"\"\"\n    Runs the Conjugate Gradient method for a single test case.\n\n    Args:\n        A_diag: The diagonal entries of the matrix A.\n        b: The right-hand side vector b.\n        tau_res: The tolerance for the relative residual norm.\n        tau_en: The tolerance for the energy norm of the error.\n\n    Returns:\n        A tuple (k_res, k_en) with the stopping iterations.\n    \"\"\"\n    n = len(b)\n    A = np.diag(A_diag)\n    b_norm = np.linalg.norm(b, 2)\n\n    # Special case from problem description: b = 0\n    if b_norm == 0:\n        # x_star = 0. x_0 = 0.\n        # r_0 = b - A*x_0 = 0. rel_res is defined as 0.\n        # e_0 = x_0 - x_star = 0. en_norm is 0.\n        # Both criteria are met at k=0.\n        return 0, 0\n\n    x_star = b / A_diag\n    \n    # Initialize stopping iterations to n as per problem spec\n    k_res = n\n    k_en = n\n\n    # --- Check criteria at iteration k=0 ---\n    x = np.zeros(n)\n    r = b - A @ x  # At k=0, x_0 = 0, so r_0 = b\n\n    rel_res = np.linalg.norm(r, 2) / b_norm\n    e = x - x_star  # e_0 = -x_star\n    en_norm = np.sqrt(e.T @ A @ e)\n\n    if rel_res = tau_res:\n        k_res = 0\n    if en_norm = tau_en:\n        k_en = 0\n    \n    if k_res != n and k_en != n:\n        return k_res, k_en\n        \n    # --- CG Iterations from k=1 to n ---\n    p = r.copy()\n    rs_old = r.T @ r\n\n    for k in range(1, n + 1):\n        # Efficient matrix-vector product for diagonal A\n        Ap = A_diag * p\n        \n        alpha = rs_old / (p.T @ Ap)\n        \n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        # Calculate stopping criteria quantities for iterate k\n        rel_res = np.linalg.norm(r, 2) / b_norm\n        e = x - x_star\n        en_norm = np.sqrt(e.T @ A @ e)\n        \n        if k_res == n and rel_res = tau_res:\n            k_res = k\n        if k_en == n and en_norm = tau_en:\n            k_en = k\n\n        # Early exit if both stopping iterations have been found\n        if k_res != n and k_en != n:\n            break\n            \n        # Check for numerical convergence to prevent division by small rs_old\n        rs_new = r.T @ r\n        if np.sqrt(rs_new)  1e-15:\n            # If residual is numerically zero, solution is exact.\n            # Both criteria will be met if tolerances are not zero.\n            if k_res == n: k_res = k\n            if k_en == n: k_en = k\n            break\n\n        # Update for next CG step\n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return k_res, k_en\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the CG analysis for each, printing the formatted result.\n    \"\"\"\n    test_cases = [\n        # Case 1: Well-conditioned\n        (np.array([1.0, 2.0, 3.0, 4.0, 5.0]), np.array([1.0, 0.5, -1.0, 2.0, 0.25]), 1e-8, 1e-8),\n        # Case 2: Ill-conditioned\n        (np.array([1e-6, 1e-4, 1e-2, 1.0, 10.0]), np.array([1.0, 1.0, 1.0, 1.0, 1.0]), 1e-8, 1e-8),\n        # Case 3: Zero right-hand side\n        (np.array([2.0, 3.0, 4.0]), np.array([0.0, 0.0, 0.0]), 1e-8, 1e-8),\n        # Case 4: Different tolerances\n        (np.array([1e-4, 1.0, 100.0]), np.array([1.0, 1.0, 1.0]), 1e-10, 1e-4),\n    ]\n\n    results_for_print = []\n    for case in test_cases:\n        A_diag, b, tau_res, tau_en = case\n        k_res, k_en = run_cg_case(A_diag, b, tau_res, tau_en)\n        diff = k_res != k_en\n        # Format string manually to avoid spaces, per problem spec\n        results_for_print.append(f\"[{k_res},{k_en},{str(diff).lower()}]\")\n\n    # Final print statement in the exact required format with no spaces.\n    print(f\"[{','.join(results_for_print)}]\")\n\nsolve()\n```", "id": "3111692"}]}