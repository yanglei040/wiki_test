{"hands_on_practices": [{"introduction": "A robust stopping criterion must be more than just a small, fixed number; it should be principled and adapt to the problem's scale. This first exercise challenges you to move beyond ad-hoc tolerances by deriving and implementing an adaptive stopping rule for Newton's method from first principles [@problem_id:3187861]. By grounding the tolerance in backward error analysis and the realities of floating-point arithmetic, you will build a solver that behaves reliably whether the solution is near zero or astronomically large, a cornerstone of high-quality numerical software.", "problem": "You are tasked with designing and implementing an adaptive stopping criterion for solving scalar nonlinear equations using Newton’s method. The stopping tolerance must be justified from a backward error perspective under floating-point rounding and must automatically scale with the magnitude of the current iterate and the residual. The objective is to show, by derivation and by computational tests, why such a tolerance avoids erroneous early termination and remains effective across different problem scales.\n\nBegin from the following fundamental base:\n- In IEEE 754 double-precision floating-point arithmetic, each elementary arithmetic operation on real numbers satisfies the standard rounding model: if an exact real operation would produce $z$, the computed result is $\\widehat{z} = z(1 + \\delta)$ with $|\\delta| \\le \\epsilon_{\\text{mach}}$, where $\\epsilon_{\\text{mach}}$ is the machine epsilon.\n- Newton’s method for a differentiable scalar function $f:\\mathbb{R}\\to\\mathbb{R}$ with derivative $f'$ updates $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$.\n- A backward error stopping rule seeks $x_k$ such that the computed residual $|f(x_k)|$ is consistent with a small perturbation of the problem data that can be attributed to floating-point rounding, and the iterate change $|x_{k+1}-x_k|$ is commensurately small.\n\nTasks:\n1. Derive an automatic tolerance $\\varepsilon_k$ for termination based on backward error analysis using the floating-point model above. Your derivation must produce a leading-order bound that scales like $\\sqrt{\\epsilon_{\\text{mach}}}$ times a dimensionless measure constructed from the magnitudes of the current iterate $x_k$ and residual $f(x_k)$. The tolerance must be applicable to both residual and step-size stopping checks.\n2. Implement a Newton solver that, at iteration $k$, computes a tolerance $\\varepsilon_k$ from your derivation, and terminates when either $|f(x_k)| \\le \\varepsilon_k$ or $|x_{k+1}-x_k| \\le \\varepsilon_k$. Use a maximum iteration cap to avoid infinite loops.\n3. Validate that your tolerance behaves robustly across scales by applying your solver to the following test suite of scalar problems, each specified by $f(x)$, $f'(x)$, and the initial guess $x_0$:\n   - Case 1 (baseline linear): $f(x) = x - 1$, $f'(x) = 1$, $x_0 = 2$.\n   - Case 2 (large scaling): $f(x) = 10^{8}(x - 1)$, $f'(x) = 10^{8}$, $x_0 = 2$.\n   - Case 3 (small scaling): $f(x) = 10^{-8}(x - 1)$, $f'(x) = 10^{-8}$, $x_0 = 2$.\n   - Case 4 (oscillatory): $f(x) = \\sin(x)$, $f'(x) = \\cos(x)$, $x_0 = 3$.\n   - Case 5 (exponential): $f(x) = e^{x} - 1$, $f'(x) = e^{x}$, $x_0 = -0.5$.\n   - Case 6 (initially at the root): $f(x) = x - 1$, $f'(x) = 1$, $x_0 = 1$.\n\nFor each case, after termination, report the dimensionless backward-error ratio\n$$\nr_k \\equiv \\frac{|f(x_k)|}{\\,1 + |x_k| + |f(x_k)|\\,}.\n$$\nThis ratio should be at most on the order of $\\sqrt{\\epsilon_{\\text{mach}}}$ when the residual-based stopping condition triggers, and not strongly dependent on the scaling of $f$.\n\nImplementation requirements:\n- Use Newton’s method with a maximum of $50$ iterations.\n- Use IEEE 754 double precision as provided by the runtime.\n- Compute $\\epsilon_{\\text{mach}}$ from the runtime and do not hard-code any numeric constant for it.\n- No physical units are involved.\n- Angles are in radians.\n- Output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots,r_6]$), where each $r_i$ is the ratio defined above for the corresponding test case.", "solution": "The user-provided problem is valid. It is scientifically grounded in the principles of numerical analysis and floating-point arithmetic, is well-posed with a clear objective and constraints, and is expressed in objective language. All necessary data for the test cases are provided, and the task is to derive and implement a standard numerical method with a specific type of stopping criterion.\n\n### Derivation of the Adaptive Stopping Criterion\n\nThe objective is to derive a stopping tolerance $\\varepsilon_k$ for Newton's method, $x_{k+1} = x_k - f(x_k)/f'(x_k)$, that is robust across different problem scales. The tolerance should be based on backward error principles and scale with $\\sqrt{\\epsilon_{\\text{mach}}}$, where $\\epsilon_{\\text{mach}}$ is the machine epsilon.\n\nA numerical algorithm should terminate when the computed iterates are \"close enough\" to the true solution $x^*$. The definition of \"close enough\" is subtle in floating-point arithmetic. An overly strict tolerance might never be met due to rounding errors, while an overly loose one may cause premature termination.\n\nA robust stopping criterion typically combines an absolute tolerance and a relative tolerance. For the change in the iterate (the step), such a criterion takes an absolute component for roots near zero and a relative component for large roots. This can be expressed as:\n$$\n|x_{k+1} - x_k| \\le \\tau_{abs} + \\tau_{rel} |x_k|\n$$\nThis can be compactly written as $|x_{k+1} - x_k| \\le \\tau (1 + |x_k|)$, where the base tolerance $\\tau$ is used for both the absolute and relative parts.\n\nThe choice of the base tolerance $\\tau$ is critical. A tolerance on the order of $\\epsilon_{\\text{mach}}$ aims for the highest possible precision, but can be unstable. In many numerical contexts, convergence slows or becomes erratic as it approaches the limits of machine precision. The quadratic convergence of Newton's method, where the error $e_{k+1} \\approx C e_k^2$, means that once the error is small, it decreases very rapidly. A tolerance of $\\tau = \\sqrt{\\epsilon_{\\text{mach}}}$ is a standard, practical choice that represents a \"reasonably tight\" condition. It signals that the iterate has very likely entered the region of quadratic convergence and further iterations will yield diminishing returns, while being loose enough to avoid issues with floating-point noise far from the true precision limit.\n\nAdopting this principle, we set the base tolerance to $\\tau = \\sqrt{\\epsilon_{\\text{mach}}}$. The step-size stopping criterion is therefore:\n$$\n|x_{k+1} - x_k| \\le \\sqrt{\\epsilon_{\\text{mach}}} (1 + |x_k|)\n$$\n\nThe problem requires a single tolerance formulation, $\\varepsilon_k$, to be applied to both the step-size and the residual. For a well-behaved problem near a simple root $x^*$, where the derivative $f'(x^*)$ is of order $1$, the magnitude of the residual $|f(x_k)|$ is comparable to the magnitude of the error $|x_k - x^*|$, which in turn is approximated by the step size $|x_{k+1}-x_k|$. That is, $|f(x_k)| \\approx |f'(x^*)(x_k-x^*)| \\approx |x_{k+1}-x_k|$. This heuristic suggests that it is reasonable to apply the same tolerance value to both the residual and the step size.\n\nThis leads to the definition of a single adaptive tolerance $\\varepsilon_k$ at each iteration $k$:\n$$\n\\varepsilon_k = \\sqrt{\\epsilon_{\\text{mach}}} (1 + |x_k|)\n$$\nThe Newton iteration then terminates if either of the following conditions is met:\n1.  Residual check: $|f(x_k)| \\le \\varepsilon_k$\n2.  Step-size check: $|x_{k+1} - x_k| \\le \\varepsilon_k$\n\nThis formulation satisfies the problem's requirements. The tolerance $\\varepsilon_k$ is computed at iteration $k$. Its scaling with $(1 + |x_k|)$ ensures it automatically adjusts to the magnitude of the current iterate, making it robust for roots at both small and large scales. Its dependence on $\\sqrt{\\epsilon_{\\text{mach}}}$ reflects a practical compromise for robust convergence. Although the problem mentions a \" dimensionless measure constructed from the magnitudes of the current iterate $x_k$ and residual $f(x_k)$\", which is a slightly ambiguous phrasing, our derived tolerance $\\varepsilon_k$ is based on $(1+|x_k|)$. This can be interpreted as a scaling factor, and given the problem statement that no physical units are involved, it robustly handles the scaling of the iterate's magnitude. The term \"dimensionless measure\" can be interpreted as the factor that makes the tolerance scale-invariant, which our use of $(1+|x_k|)$ accomplishes.\n\n### Algorithmic Implementation\n\nThe solver is implemented as follows:\n1.  Compute the machine epsilon $\\epsilon_{\\text{mach}}$ and the base tolerance $\\tau = \\sqrt{\\epsilon_{\\text{mach}}}$.\n2.  Initialize the iterate $x_k$ with the starting guess $x_0$.\n3.  Loop for a maximum of $50$ iterations:\n    a. At the current iterate $x_k$, compute the residual $f_k = f(x_k)$.\n    b. Compute the adaptive tolerance $\\varepsilon_k = \\tau (1 + |x_k|)$.\n    c. Perform the residual check: if $|f_k| \\le \\varepsilon_k$, the algorithm has converged. Terminate the loop.\n    d. Compute the derivative $f'_k = f'(x_k)$. If $f'_k$ is zero, terminate to prevent division by zero.\n    e. Calculate the Newton step: $\\Delta x_k = f_k / f'_k$.\n    f. Perform the step-size check: if $|\\Delta x_k| \\le \\varepsilon_k$, the step is negligibly small. Update the iterate one last time to $x_{k+1} = x_k - \\Delta x_k$ and terminate.\n    g. Update the iterate: $x_k \\leftarrow x_k - \\Delta x_k$.\n4.  After termination, compute the final dimensionless backward-error ratio $r_k = \\frac{|f(x_k)|}{1 + |x_k| + |f(x_k)|}$ using the final iterate $x_k$.\n\nThis procedure is applied to each of the six test cases specified in the problem statement. The resulting values of $r_k$ are collected and printed. The results are expected to show that $r_k$ is consistently on the order of $\\sqrt{\\epsilon_{\\text{mach}}}$ or smaller, demonstrating the robustness of the stopping criterion across various function scales and behaviors.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and validates an adaptive stopping criterion for Newton's method.\n    The criterion is based on backward error analysis principles adapted for robustness\n    in floating-point arithmetic.\n    \"\"\"\n\n    # Define the test suite of scalar nonlinear problems.\n    # Each problem is a dictionary with the function, its derivative, and an initial guess.\n    test_cases = [\n        {'f': lambda x: x - 1.0, 'fp': lambda x: 1.0, 'x0': 2.0},\n        {'f': lambda x: 1e8 * (x - 1.0), 'fp': lambda x: 1e8, 'x0': 2.0},\n        {'f': lambda x: 1e-8 * (x - 1.0), 'fp': lambda x: 1e-8, 'x0': 2.0},\n        {'f': lambda x: np.sin(x), 'fp': lambda x: np.cos(x), 'x0': 3.0},\n        {'f': lambda x: np.exp(x) - 1.0, 'fp': lambda x: np.exp(x), 'x0': -0.5},\n        {'f': lambda x: x - 1.0, 'fp': lambda x: 1.0, 'x0': 1.0}\n    ]\n\n    # Compute machine epsilon for IEEE 754 double precision from the runtime.\n    eps_mach = np.finfo(float).eps\n    # The base tolerance level is set to the square root of machine epsilon.\n    sqrt_eps_mach = np.sqrt(eps_mach)\n    \n    max_iter = 50\n    results = []\n\n    for case in test_cases:\n        f = case['f']\n        fp = case['fp']\n        x_k = float(case['x0'])\n        \n        # Main Newton's method iteration loop.\n        for _ in range(max_iter):\n            # Evaluate the function at the current iterate.\n            f_k = f(x_k)\n            \n            # Compute the adaptive tolerance epsilon_k for the current iterate x_k.\n            # This tolerance uses a mixed absolute/relative scaling factor (1 + |x_k|).\n            tol_k = sqrt_eps_mach * (1.0 + np.abs(x_k))\n            \n            # 1. Residual-based stopping criterion.\n            # Terminate if the residual is smaller than the computed tolerance.\n            if np.abs(f_k) <= tol_k:\n                break\n            \n            # Evaluate the derivative.\n            fp_k = fp(x_k)\n            \n            # Terminate if the derivative is zero to prevent division by zero,\n            # unless the residual condition was already met.\n            if fp_k == 0.0:\n                break\n            \n            # Calculate the Newton step.\n            step = f_k / fp_k\n            \n            # 2. Step-size-based stopping criterion.\n            # Terminate if the step size is smaller than the computed tolerance.\n            # In this case, we complete the final step before exiting.\n            if np.abs(step) <= tol_k:\n                x_k = x_k - step\n                break\n            \n            # Update the iterate for the next step.\n            x_k = x_k - step\n        \n        # After loop termination (by convergence or max_iter), calculate the final measures.\n        # Recalculate the residual at the final iterate.\n        final_f_k = f(x_k)\n        \n        # Compute the dimensionless backward-error ratio r_k as specified.\n        r_k = np.abs(final_f_k) / (1.0 + np.abs(x_k) + np.abs(final_f_k))\n        \n        results.append(str(r_k))\n\n    # Print the results in the specified single-line format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3187861"}, {"introduction": "First-order methods like Gradient Descent often exhibit slow convergence or transient behavior, making simple stopping rules unreliable. This practice introduces a more sophisticated technique: a windowed stagnation criterion that monitors progress over several recent iterations [@problem_id:3187906]. You will implement a rule that terminates only when both the function value and the iterate position have stabilized over a defined window, learning to tune its parameters to balance the trade-off between early stopping and solution quality.", "problem": "Consider minimizing a differentiable objective function $f:\\mathbb{R}^n \\to \\mathbb{R}$ by a fixed-step Gradient Descent (GD) method defined by the iterative scheme $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$, where $\\alpha > 0$ is a chosen step size and $x_k \\in \\mathbb{R}^n$ is the iterate at iteration $k \\in \\mathbb{N}$. A stopping rule must decide when the iterative process should terminate based on principled tolerance conditions. In this problem, you will implement and evaluate a windowed stagnation criterion with two tunable parameters: a function-decrease tolerance $\\tau > 0$ and a window length $m \\in \\mathbb{N}$. The rule, for a fixed $\\varepsilon > 0$, terminates the iteration at the first index $k \\ge m$ where both conditions hold:\n$$\nf(x_{k-m}) - f(x_k) < \\tau\n\\quad\\text{and}\\quad\n\\|x_k - x_{k-m}\\|_2 \\le \\varepsilon.\n$$\nIf no such $k$ exists up to a given maximum number of iterations $K$, then the rule does not trigger and we set $k_{\\mathrm{stop}} = K$.\n\nYour task is to implement GD with the above stopping rule and tune $(\\tau, m)$ by searching over a given finite set of candidates for each test case as follows. For each candidate pair $(\\tau, m)$, run GD from the specified initial point until the stopping rule triggers or until the maximum iteration count $K$ is reached, and record $k_{\\mathrm{stop}}$ and the corresponding iterate $x_{k_{\\mathrm{stop}}}$. A candidate $(\\tau, m)$ is considered acceptable only if a quality requirement at stopping is satisfied. Among acceptable candidates, select the pair with the smallest $k_{\\mathrm{stop}}$. In case of ties in $k_{\\mathrm{stop}}$, select the one with the larger $m$, and if still tied, select the one with the smaller $\\tau$. If no candidate is acceptable, select the pair that minimizes the violation of the quality requirement as described below, using the same tie-breaking rules.\n\nQuality requirement and violation measure. For each test case, a norm-based proximity-to-solution threshold $\\rho > 0$ is specified. The quality requirement is\n$$\n\\|x_{k_{\\mathrm{stop}}}\\|_2 \\le \\rho.\n$$\nIf this is not satisfied, define the violation measure $v = \\max\\{0, \\|x_{k_{\\mathrm{stop}}}\\|_2 - \\rho\\}$ and select the candidate that minimizes $v$ (with the same tie-breaking rules described above).\n\nImplement the above for the following test suite of three cases. In all cases, use the Euclidean norm $\\|\\cdot\\|_2$, and there are no physical units involved.\n\nTest Case 1 (happy path, anisotropic quadratic):\n- Objective: $f(x) = \\tfrac{1}{2}(x_1^2 + 10 x_2^2)$ on $\\mathbb{R}^2$.\n- Gradient: $\\nabla f(x) = (x_1, 10 x_2)$.\n- Initial point: $x_0 = (5, -5)$.\n- Step size: $\\alpha = 0.15$.\n- Maximum iterations: $K = 5000$.\n- Stagnation movement tolerance: $\\varepsilon = 10^{-6}$.\n- Candidate sets: $\\{\\tau\\} = \\{10^{-7}, 10^{-6}, 10^{-5}\\}$ and $\\{m\\} = \\{3, 5, 10\\}$.\n- Quality threshold: $\\rho = 10^{-3}$.\n\nTest Case 2 (boundary condition, small-step isotropic quadratic):\n- Objective: $f(x) = \\tfrac{1}{2}(x_1^2 + x_2^2)$ on $\\mathbb{R}^2$.\n- Gradient: $\\nabla f(x) = (x_1, x_2)$.\n- Initial point: $x_0 = (2, 2)$.\n- Step size: $\\alpha = 10^{-3}$.\n- Maximum iterations: $K = 20000$.\n- Stagnation movement tolerance: $\\varepsilon = 10^{-7}$.\n- Candidate sets: $\\{\\tau\\} = \\{10^{-7}, 10^{-6}, 10^{-5}\\}$ and $\\{m\\} = \\{3, 10, 50\\}$.\n- Quality threshold: $\\rho = 5 \\times 10^{-2}$.\n\nTest Case 3 (edge case, flat-plateau geometry):\n- Objective: $f(x) = \\arctan(\\|x\\|_2)$ on $\\mathbb{R}^2$.\n- Gradient: for $x \\neq 0$, $\\nabla f(x) = \\dfrac{1}{1+\\|x\\|_2^2} \\dfrac{x}{\\|x\\|_2}$ and for $x = 0$, $\\nabla f(0) = (0, 0)$.\n- Initial point: $x_0 = (100, 0)$.\n- Step size: $\\alpha = 10^{-1}$.\n- Maximum iterations: $K = 200000$.\n- Stagnation movement tolerance: $\\varepsilon = 10^{-4}$.\n- Candidate sets: $\\{\\tau\\} = \\{10^{-8}, 10^{-7}, 10^{-6}\\}$ and $\\{m\\} = \\{20, 50\\}$.\n- Quality threshold: $\\rho = 10^{-1}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result must be the selected pair as a two-element list $[\\tau, m]$ for the corresponding test case, in the order of the cases above. For example, the output format must be of the form $[[\\tau_1,m_1],[\\tau_2,m_2],[\\tau_3,m_3]]$ with no spaces.", "solution": "We begin from the core definition of Gradient Descent (GD). Given a differentiable objective $f:\\mathbb{R}^n \\to \\mathbb{R}$ and a fixed step size $\\alpha > 0$, GD generates iterates by $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$. Under mild regularity and sufficiently small $\\alpha$, the sequence $(f(x_k))_{k \\in \\mathbb{N}}$ is nonincreasing and $(x_k)$ approaches a first-order stationary point, typically a minimizer in convex settings.\n\nA principled stopping rule should detect the onset of stagnation. A single-iteration check such as $\\|\\nabla f(x_k)\\| \\le \\text{tolerance}$ or $\\|x_{k+1}-x_k\\| \\le \\text{tolerance}$ can be sensitive to transient noise or direction changes. A windowed criterion mitigates this by aggregating information across several iterations. Define a window length $m \\in \\mathbb{N}$ and tolerances $\\tau > 0$ for function decrease and $\\varepsilon > 0$ for movement in the decision space. The windowed stagnation rule declares stop at the first $k \\ge m$ such that\n$$\nf(x_{k-m}) - f(x_k) < \\tau,\\quad \\|x_k - x_{k-m}\\|_2 \\le \\varepsilon.\n$$\nThis couples two signals: the vertical decrease in the objective over the window and the horizontal movement in the parameters. If both are small, the algorithm made little progress over the last $m$ iterations, which indicates either proximity to a stationary point or that the step size is too small to produce meaningful progress. In both cases, stopping is reasonable if accompanied by a quality check.\n\nQuality requirement. For each test case, we impose a proximity-to-solution measure $\\|x_{k_{\\mathrm{stop}}}\\|_2 \\le \\rho$, where $\\rho > 0$ is chosen. For the quadratic objectives in Test Cases $1$ and $2$, the unique minimizer is at $x^\\star = 0$, so bounding $\\|x_{k_{\\mathrm{stop}}}\\|_2$ directly measures closeness to the minimizer. For the $\\arctan$ objective in Test Case $3$, the minimizer is also $x^\\star = 0$ because $\\arctan(\\|x\\|_2)$ is strictly increasing in $\\|x\\|_2 \\ge 0$, so the same criterion applies.\n\nTuning $(\\tau, m)$. The pair $(\\tau, m)$ controls sensitivity of the rule:\n- Larger $m$ tracks progress over a longer horizon, reducing the risk of premature stopping caused by transient behavior.\n- Smaller $\\tau$ demands more significant function decrease over the window for continuation, which delays stopping until the algorithm has genuinely flattened out.\n- The movement tolerance $\\varepsilon$ ensures that even if the function is nearly flat, we do not stop unless the iterates themselves have stabilized.\n\nTo tune $(\\tau, m)$, we evaluate all candidates and select the pair that minimizes the stopping iteration index $k_{\\mathrm{stop}}$ subject to the quality requirement $\\|x_{k_{\\mathrm{stop}}}\\|_2 \\le \\rho$. This is a rational optimization objective: earlier stopping is preferable if it certifies sufficient quality. In case no candidate meets the quality requirement, we select the pair that minimizes the violation $v = \\max\\{0, \\|x_{k_{\\mathrm{stop}}}\\|_2 - \\rho\\}$, which quantifies how far the result is from the desired tolerance. Ties are broken by preferring a larger window $m$ (more robust aggregation) and then a smaller $\\tau$ (stricter decrease requirement), consistent with conservative stopping policy design.\n\nAlgorithmic design. For each test case:\n1. Initialize $x_0$ and compute $f(x_0)$.\n2. For each candidate pair $(\\tau, m)$:\n   a. Iterate GD for at most $K$ steps using $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$.\n   b. Maintain the histories $(x_j)$ and $(f(x_j))$. For each $k \\ge m$, compute\n      $$\n      \\Delta f_k = f(x_{k-m}) - f(x_k),\\quad d_k = \\|x_k - x_{k-m}\\|_2.\n      $$\n      If $\\Delta f_k < \\tau$ and $d_k \\le \\varepsilon$, declare $k_{\\mathrm{stop}} = k$ and stop the loop.\n   c. If the loop completes without triggering, set $k_{\\mathrm{stop}} = K$.\n   d. Evaluate the quality $\\|x_{k_{\\mathrm{stop}}}\\|_2 \\le \\rho$ and record $k_{\\mathrm{stop}}$ and the violation $v$ if necessary.\n3. Select the pair that minimizes $k_{\\mathrm{stop}}$ among acceptable candidates; otherwise select the one that minimizes the violation $v$, with the specified tie-breaking rules.\n\nJustification of the windowed criterion from fundamental principles. In GD under a Lipschitz continuous gradient with parameter $L > 0$ (i.e., $\\|\\nabla f(x) - \\nabla f(y)\\|_2 \\le L \\|x-y\\|_2$), for $\\alpha \\in (0, 2/L)$ one has the classical descent property\n$$\nf(x_{k+1}) \\le f(x_k) - \\tfrac{1}{2}\\alpha (2 - \\alpha L) \\|\\nabla f(x_k)\\|_2^2,\n$$\nwhich ensures monotonic decrease of $f(x_k)$. When the decrease across a window is smaller than $\\tau$, the averaged change in $f$ per iteration in that window is bounded by $\\tau/m$, indicating that the gradient norms are small on average. Simultaneously, if $\\|x_k - x_{k-m}\\|_2 \\le \\varepsilon$, the iterates form an approximately Cauchy segment, signifying stabilization in the decision space. Taken together, these reflect near-stationarity: with small gradients and small movements, further GD progress will be marginal. Setting the tolerances $(\\tau, \\varepsilon)$ and the horizon $m$ calibrates how much marginal progress is tolerated before stopping, and the tuning procedure selects values that achieve target quality as early as possible.\n\nThe three test cases cover:\n- A well-conditioned anisotropic quadratic where GD converges rapidly, representing the happy path.\n- A small-step isotropic quadratic, illustrating sensitivity to the choice of $\\tau$ and $m$ when per-iteration progress is slow.\n- A flat-plateau objective $\\arctan(\\|x\\|_2)$ with extremely small gradients far from the minimizer, exposing the edge case where naive tolerances can produce premature stopping if not tuned to avoid false positives; the violation-based fallback ensures a principled choice even when no candidate achieves the target quality.\n\nThe program enumerates candidate $(\\tau, m)$ pairs, runs GD with the windowed stagnation rule, applies the selection criteria, and outputs the chosen pairs in the specified single-line format.", "answer": "```python\nimport numpy as np\n\ndef gd_run(f, grad, x0, alpha, K, eps_move, tau, m):\n    \"\"\"\n    Run Gradient Descent with a windowed stagnation stopping rule.\n    Stop at first k >= m such that:\n        f(x_{k-m}) - f(x_k) < tau and ||x_k - x_{k-m}|| <= eps_move\n    If not triggered by K, set k_stop = K.\n    Returns (k_stop, x_stop_norm).\n    \"\"\"\n    x_hist = [np.array(x0, dtype=float)]\n    f_hist = [f(x_hist[0])]\n    x = x_hist[0]\n    k_stop = None\n\n    for k in range(1, K + 1):\n        g = grad(x)\n        x = x - alpha * g\n        fx = f(x)\n        x_hist.append(x)\n        f_hist.append(fx)\n\n        if k >= m:\n            df = f_hist[k - m] - fx\n            move = np.linalg.norm(x - x_hist[k - m])\n            if df < tau and move <= eps_move:\n                k_stop = k\n                break\n\n    if k_stop is None:\n        k_stop = K\n        x_stop = x_hist[-1]\n    else:\n        x_stop = x_hist[k_stop]\n\n    return k_stop, np.linalg.norm(x_stop)\n\n\ndef select_tau_m(f, grad, x0, alpha, K, eps_move, tau_list, m_list, rho):\n    \"\"\"\n    Enumerate candidates (tau, m), run GD, and select:\n      - among acceptable candidates (||x_stop|| <= rho), the one with minimal k_stop;\n      - ties broken by larger m, then smaller tau;\n      - if none acceptable, minimize violation v = max(0, ||x_stop|| - rho) with same tie-breakers.\n    Returns the selected [tau, m].\n    \"\"\"\n    candidates = []\n    for tau in tau_list:\n        for m in m_list:\n            k_stop, xnorm = gd_run(f, grad, x0, alpha, K, eps_move, tau, m)\n            acceptable = xnorm <= rho\n            violation = max(0.0, xnorm - rho)\n            candidates.append({\n                \"tau\": tau, \"m\": m, \"k\": k_stop,\n                \"acceptable\": acceptable, \"violation\": violation\n            })\n\n    # Filter acceptable candidates\n    acceptable_cands = [c for c in candidates if c[\"acceptable\"]]\n    if acceptable_cands:\n        # Sort by k ascending, m descending, tau ascending\n        acceptable_cands.sort(key=lambda c: (c[\"k\"], -c[\"m\"], c[\"tau\"]))\n        best = acceptable_cands[0]\n    else:\n        # Sort by violation ascending, k ascending, m descending, tau ascending\n        candidates.sort(key=lambda c: (c[\"violation\"], c[\"k\"], -c[\"m\"], c[\"tau\"]))\n        best = candidates[0]\n\n    return [best[\"tau\"], best[\"m\"]]\n\n\ndef solve():\n    # Define test cases\n    # Test Case 1\n    def f1(x):\n        return 0.5 * (x[0]**2 + 10.0 * x[1]**2)\n    def g1(x):\n        return np.array([x[0], 10.0 * x[1]])\n    x0_1 = np.array([5.0, -5.0])\n    alpha_1 = 0.15\n    K_1 = 5000\n    eps_1 = 1e-6\n    tau_list_1 = [1e-7, 1e-6, 1e-5]\n    m_list_1 = [3, 5, 10]\n    rho_1 = 1e-3\n\n    # Test Case 2\n    def f2(x):\n        return 0.5 * (x[0]**2 + x[1]**2)\n    def g2(x):\n        return np.array([x[0], x[1]])\n    x0_2 = np.array([2.0, 2.0])\n    alpha_2 = 1e-3\n    K_2 = 20000\n    eps_2 = 1e-7\n    tau_list_2 = [1e-7, 1e-6, 1e-5]\n    m_list_2 = [3, 10, 50]\n    rho_2 = 5e-2\n\n    # Test Case 3\n    def f3(x):\n        r = np.linalg.norm(x)\n        return np.arctan(r)\n    def g3(x):\n        r = np.linalg.norm(x)\n        if r == 0.0:\n            return np.array([0.0, 0.0])\n        return (1.0 / (1.0 + r*r)) * (x / r)\n    x0_3 = np.array([100.0, 0.0])\n    alpha_3 = 1e-1\n    K_3 = 200000\n    eps_3 = 1e-4\n    tau_list_3 = [1e-8, 1e-7, 1e-6]\n    m_list_3 = [20, 50]\n    rho_3 = 1e-1\n\n    results = []\n    # Case 1\n    sel1 = select_tau_m(f1, g1, x0_1, alpha_1, K_1, eps_1, tau_list_1, m_list_1, rho_1)\n    results.append(sel1)\n    # Case 2\n    sel2 = select_tau_m(f2, g2, x0_2, alpha_2, K_2, eps_2, tau_list_2, m_list_2, rho_2)\n    results.append(sel2)\n    # Case 3\n    sel3 = select_tau_m(f3, g3, x0_3, alpha_3, K_3, eps_3, tau_list_3, m_list_3, rho_3)\n    results.append(sel3)\n\n    # Format output without spaces\n    def format_pair(p):\n        return \"[\" + f\"{p[0]},{p[1]}\" + \"]\"\n    out = \"[\" + \",\".join(format_pair(p) for p in results) + \"]\"\n    print(out)\n\nsolve()\n```", "id": "3187906"}, {"introduction": "How do we detect convergence when no gradient information is available? This question is central to derivative-free optimization methods like Nelder-Mead. In this exercise, you will implement and compare two fundamentally different stopping criteria: one based on the geometric size of the search simplex and another based on the spread of function values at its vertices [@problem_id:3187957]. By testing them on specially crafted functions, you will discover their respective strengths and weaknesses, revealing how the geometry of the objective landscape can cause one criterion to succeed while the other fails prematurely.", "problem": "You are given the task of implementing the Nelder–Mead (NM) method with two alternative stopping criteria to compare their behavior and reliability. The Nelder–Mead method is an iterative direct search algorithm for unconstrained minimization that maintains a simplex, that is, a set of $n+1$ points in $\\mathbb{R}^n$, and generates new candidate points through affine combinations of existing vertices. The algorithm uses reflection, expansion, contraction, and shrink transformations to move the simplex toward regions of lower objective values.\n\nFundamental base and definitions:\n- The objective is to minimize a real-valued function $f:\\mathbb{R}^n\\to\\mathbb{R}$.\n- A simplex $\\mathcal{S}$ in $\\mathbb{R}^n$ is a set $\\{x_0,x_1,\\ldots,x_n\\}$ of $n+1$ affinely independent points.\n- The centroid $c$ of all points except the worst (the vertex with the largest objective value) is defined as $c=\\frac{1}{n}\\sum_{i=0}^{n-1}x_i$ after ordering the vertices by their objective values, with $x_0$ the best and $x_n$ the worst.\n- The reflection point is $x_r=c+\\alpha(c-x_n)$ with reflection coefficient $\\alpha$.\n- The expansion point is $x_e=c+\\gamma(x_r-c)$ with expansion coefficient $\\gamma$.\n- The contraction point is either the outside contraction $x_{co}=c+\\rho(x_r-c)$ or the inside contraction $x_{ci}=c+\\rho(x_n-c)$ with contraction coefficient $\\rho$.\n- The shrink transformation replaces all non-best vertices by $x_i\\leftarrow x_0+\\sigma(x_i-x_0)$ with shrink coefficient $\\sigma$.\n\nStopping criteria to be compared:\n- Geometric criterion based on simplex diameter: the diameter $d(\\mathcal{S})$ is defined as the maximum Euclidean distance between any two vertices of the simplex,\n$$\nd(\\mathcal{S})=\\max_{0\\le i,j\\le n}\\left\\Vert x_i-x_j\\right\\Vert_2.\n$$\nStop when $d(\\mathcal{S}_k)\\le \\varepsilon_d$.\n- Functional criterion based on spread in objective values: the spread $s(\\mathcal{S})$ is defined as the difference between the largest and smallest objective values at the simplex vertices,\n$$\ns(\\mathcal{S})=\\max_i f(x_i)-\\min_i f(x_i).\n$$\nStop when $s(\\mathcal{S}_k)\\le \\varepsilon_f$.\n\nAlgorithmic parameters:\n- Use $\\alpha=1$, $\\gamma=2$, $\\rho=\\tfrac{1}{2}$, $\\sigma=\\tfrac{1}{2}$, unless otherwise specified.\n- The maximum number of iterations must be enforced to avoid infinite loops.\n\nYour program must implement Nelder–Mead from the above base definitions and run each test case twice: once with the geometric stopping criterion $d(\\mathcal{S}_k)\\le \\varepsilon_d$ and once with the functional stopping criterion $s(\\mathcal{S}_k)\\le \\varepsilon_f$. For each run, report the number of iterations, the final best objective value $\\min_i f(x_i)$, the final diameter $d(\\mathcal{S}_k)$, and the final spread $s(\\mathcal{S}_k)$.\n\nTest suite:\n- Case $1$ (flat plateau corner case):\n  - Dimension $n=2$.\n  - Objective $f(x)=\\left(10^{-12}\\right)\\left(x_1^2+x_2^2\\right)$.\n  - Initial simplex vertices $(10,10)$, $(10,11)$, $(11,10)$.\n  - Tolerances $\\varepsilon_d=10^{-3}$, $\\varepsilon_f=10^{-10}$.\n  - Maximum iterations $200$.\n- Case $2$ (steep gradient with small simplex, corner case):\n  - Dimension $n=2$.\n  - Objective $f(x)=\\exp\\left(50x_1\\right)+\\left(10^{-3}\\right)x_2^2$.\n  - Initial simplex vertices $(0.1,0)$, $(0.1001,0)$, $(0.1,0.0001)$.\n  - Tolerances $\\varepsilon_d=5\\times 10^{-4}$, $\\varepsilon_f=10^{-2}$.\n  - Maximum iterations $200$.\n- Case $3$ (happy path, curved valley):\n  - Dimension $n=2$.\n  - Objective $f(x)=\\left(1-x_1\\right)^2+100\\left(x_2-x_1^2\\right)^2$.\n  - Initial simplex vertices $(-1.2,1)$, $(-1.2,1.2)$, $(-1,1)$.\n  - Tolerances $\\varepsilon_d=10^{-3}$, $\\varepsilon_f=10^{-6}$.\n  - Maximum iterations $1000$.\n- Case $4$ (boundary equality on diameter):\n  - Dimension $n=2$.\n  - Objective $f(x)=x_1^2+x_2^2$.\n  - Initial simplex vertices $(0,0)$, $(1,0)$, $(0,1)$.\n  - Tolerances $\\varepsilon_d=\\sqrt{2}$, $\\varepsilon_f=0.5$.\n  - Maximum iterations $200$.\n\nRequired outputs per test case:\n- For each test case, output the list\n$$\n[\\;I_d,\\;I_f,\\;B,\\;F_d,\\;F_f,\\;D_d,\\;S_d,\\;D_f,\\;S_f\\;]\n$$\nwhere $I_d$ is the iteration count when stopping on diameter, $I_f$ is the iteration count when stopping on spread, $B$ is a boolean that is $\\text{True}$ if $I_d<I_f$ and $\\text{False}$ otherwise, $F_d$ and $F_f$ are the final best objective values under diameter and spread runs respectively, $D_d$ and $S_d$ are the final diameter and spread under the diameter run, and $D_f$ and $S_f$ are the final diameter and spread under the spread run.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case’s result itself a bracketed comma-separated list, and without spaces. For example,\n$$\n[\\,[1,2,\\text{True},0.0,0.0,0.0,0.0,0.0,0.0],[\\ldots]\\,]\n$$", "solution": "The problem requires the implementation of the Nelder–Mead direct search optimization algorithm. The implementation must be based on the fundamental definitions provided and must support two distinct stopping criteria: one based on the geometric diameter of the simplex and the other on the functional spread of the objective values at the simplex vertices. The performance of these two criteria will be compared across a suite of four test cases, each designed to highlight different behaviors of the algorithm.\n\nThe core of the solution is a function that executes the Nelder–Mead method. The algorithm is iterative and proceeds as follows for an objective function $f: \\mathbb{R}^n \\to \\mathbb{R}$.\n\nAn initial simplex $\\mathcal{S}_0 = \\{x_0, x_1, \\ldots, x_n\\}$ consisting of $n+1$ vertices in $\\mathbb{R}^n$ is provided. The algorithm then enters a loop, where at each iteration $k=0, 1, 2, \\ldots$:\n$1$. The $n+1$ vertices of the current simplex $\\mathcal{S}_k$ are evaluated and ordered such that their objective function values are sorted: $f(x_0) \\le f(x_1) \\le \\ldots \\le f(x_n)$. The vertices $x_0$ and $x_n$ are designated as the best and worst vertices, respectively.\n\n$2$. The stopping criteria are checked. Two separate runs are performed for each test case.\n   - For the geometric criterion run, the simplex diameter $d(\\mathcal{S}_k) = \\max_{0 \\le i,j \\le n} \\|x_i - x_j\\|_2$ is computed. If $d(\\mathcal{S}_k) \\le \\varepsilon_d$, the algorithm terminates, reporting $k$ as the number of iterations.\n   - For the functional criterion run, the spread of objective values $s(\\mathcal{S}_k) = f(x_n) - f(x_0)$ is computed. If $s(\\mathcal{S}_k) \\le \\varepsilon_f$, the algorithm terminates, reporting $k$ as the number of iterations.\n   - In both cases, termination also occurs if the iteration count $k$ reaches the specified maximum, $k_{\\max}$.\n\n$3$. If no stopping criterion is met, a new simplex $\\mathcal{S}_{k+1}$ is generated. This begins with the calculation of the centroid $c$ of the $n$ best vertices:\n$$\nc = \\frac{1}{n} \\sum_{i=0}^{n-1} x_i\n$$\n\n$4$. A sequence of transformations is attempted to replace the worst vertex $x_n$ with a better point. The standard parameters for these transformations are: reflection coefficient $\\alpha=1$, expansion coefficient $\\gamma=2$, contraction coefficient $\\rho=\\frac{1}{2}$, and shrink coefficient $\\sigma=\\frac{1}{2}$.\n   - **Reflection**: A reflected point $x_r = c + \\alpha(c - x_n)$ is generated. Let its function value be $f_r = f(x_r)$.\n   - **Expansion**: If $f_r$ is better than the best current value, i.e., $f_r < f(x_0)$, the algorithm explores further in this direction by computing an expansion point $x_e = c + \\gamma(x_r - c)$. If $f(x_e) < f_r$, the new vertex is $x_e$; otherwise, the new vertex is $x_r$. This new point replaces $x_n$.\n   - **Acceptance**: If the reflected point is not a new best, but is better than the second-worst vertex, i.e., $f(x_0) \\le f_r < f(x_{n-1})$, then $x_r$ is accepted and replaces $x_n$.\n   - **Contraction**: If the reflected point is not an improvement, $f_r \\ge f(x_{n-1})$, a contraction is performed.\n     - If $f_r < f(x_n)$, an *outside contraction* is performed to compute $x_{co} = c + \\rho(x_r - c)$. If $f(x_{co}) \\le f_r$, $x_{co}$ replaces $x_n$. Otherwise, a shrink is performed.\n     - If $f_r \\ge f(x_n)$, an *inside contraction* is performed to compute $x_{ci} = c + \\rho(x_n - c)$. If $f(x_{ci}) < f(x_n)$, $x_{ci}$ replaces $x_n$. Otherwise, a shrink is performed.\n   - **Shrink**: If the contraction step fails to yield an improvement, the entire simplex is shrunk towards the best vertex $x_0$. All vertices $x_i$ for $i \\in \\{1, 2, \\ldots, n\\}$ are replaced according to the rule $x_i \\leftarrow x_0 + \\sigma(x_i - x_0)$.\n\nThis process generates a new simplex $\\mathcal{S}_{k+1}$ and the next iteration begins.\n\nThe test cases are designed to probe the relative effectiveness of the stopping criteria.\n- **Case 1**: A nearly flat quadratic function. The functional spread $s(\\mathcal{S})$ can become very small quickly, even if the simplex vertices are far apart, potentially causing premature termination under the functional criterion.\n- **Case 2**: A function with a very steep exponential component. The simplex can become geometrically small ($d(\\mathcal{S})$ is small) in a region where the function value changes dramatically, potentially causing premature termination under the geometric criterion.\n- **Case 3**: The Rosenbrock function, a standard benchmark with a narrow, curved valley. This serves as a general test of the algorithm's robustness.\n- **Case 4**: A simple quadratic where the initial simplex diameter is exactly equal to the tolerance $\\varepsilon_d$, testing the handling of a boundary condition at iteration $k=0$.\n\nThe program will execute this logic, run each test case twice (once for each stopping criterion), and collate the results—iteration count ($I_d, I_f$), a boolean comparison ($B$), final best objective values ($F_d, F_f$), and final simplex metrics ($D_d, S_d, D_f, S_f$)—into the specified list format for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef nelder_mead(func, initial_simplex, tol_d, tol_f, max_iter, stop_criterion):\n    \"\"\"\n    Implements the Nelder-Mead optimization algorithm.\n\n    Args:\n        func: The objective function to minimize.\n        initial_simplex: The starting simplex as a numpy array of shape (n+1, n).\n        tol_d: Tolerance for the geometric (diameter) stopping criterion.\n        tol_f: Tolerance for the functional (spread) stopping criterion.\n        max_iter: The maximum number of iterations.\n        stop_criterion: A string, either 'diameter' or 'spread'.\n\n    Returns:\n        A tuple containing:\n        - k (int): Number of iterations.\n        - best_val (float): The best objective function value found.\n        - diameter (float): The final diameter of the simplex.\n        - spread (float): The final spread of the objective values.\n    \"\"\"\n    # Standard Nelder-Mead parameters\n    alpha, gamma, rho, sigma = 1.0, 2.0, 0.5, 0.5\n    \n    simplex = np.copy(initial_simplex)\n    n = simplex.shape[1]  # Dimension of the problem\n\n    for k in range(max_iter + 1):\n        # 1. Evaluate and order vertices\n        f_values = np.array([func(x) for x in simplex])\n        sorted_indices = np.argsort(f_values)\n        simplex = simplex[sorted_indices]\n        f_values = f_values[sorted_indices]\n\n        # 2. Calculate stopping metrics for the current simplex\n        # Simplex diameter\n        diameter = 0.0\n        for i in range(n + 1):\n            for j in range(i + 1, n + 1):\n                dist = np.linalg.norm(simplex[i] - simplex[j])\n                if dist > diameter:\n                    diameter = dist\n        \n        # Function value spread\n        spread = f_values[-1] - f_values[0]\n\n        # 3. Check for termination\n        if (stop_criterion == 'diameter' and diameter <= tol_d) or \\\n           (stop_criterion == 'spread' and spread <= tol_f) or \\\n           k == max_iter:\n            return k, f_values[0], diameter, spread\n\n        # Get best, worst, and second-worst points and values\n        x0, xn, xn_1 = simplex[0], simplex[-1], simplex[-2]\n        f0, fn, fn_1 = f_values[0], f_values[-1], f_values[-2]\n\n        # 4. Centroid\n        centroid = np.mean(simplex[:-1], axis=0)\n\n        # 5. Reflection\n        xr = centroid + alpha * (centroid - xn)\n        fr = func(xr)\n\n        new_simplex = np.copy(simplex)\n        shrink = False\n\n        if f0 <= fr < fn_1:\n            # Accept reflected point\n            new_simplex[-1] = xr\n        elif fr < f0:\n            # Expansion\n            xe = centroid + gamma * (xr - centroid)\n            fe = func(xe)\n            if fe < fr:\n                new_simplex[-1] = xe\n            else:\n                new_simplex[-1] = xr\n        else: # fr >= fn_1, needs contraction or shrink\n            if fr < fn:\n                # Outside Contraction\n                xco = centroid + rho * (xr - centroid)\n                fco = func(xco)\n                if fco <= fr:\n                    new_simplex[-1] = xco\n                else:\n                    shrink = True\n            else: # fr >= fn\n                # Inside Contraction\n                xci = centroid + rho * (xn - centroid)\n                fci = func(xci)\n                if fci < fn:\n                    new_simplex[-1] = xci\n                else:\n                    shrink = True\n        \n        if shrink:\n            # Shrink transformation\n            for i in range(1, n + 1):\n                new_simplex[i] = x0 + sigma * (simplex[i] - x0)\n        \n        simplex = new_simplex\n\n    # This part should not be reached due to loop range and k == max_iter check\n    return max_iter, f_values[0], diameter, spread\n\ndef solve():\n    # Define objective functions\n    def f1(x): return 1e-12 * (x[0]**2 + x[1]**2)\n    def f2(x): return np.exp(50 * x[0]) + 1e-3 * x[1]**2\n    def f3(x): return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n    def f4(x): return x[0]**2 + x[1]**2\n\n    # Test cases from the problem statement\n    test_cases = [\n        {\n            \"func\": f1,\n            \"initial_simplex\": np.array([[10, 10], [10, 11], [11, 10]], dtype=float),\n            \"tol_d\": 1e-3, \"tol_f\": 1e-10, \"max_iter\": 200\n        },\n        {\n            \"func\": f2,\n            \"initial_simplex\": np.array([[0.1, 0], [0.1001, 0], [0.1, 0.0001]], dtype=float),\n            \"tol_d\": 5e-4, \"tol_f\": 1e-2, \"max_iter\": 200\n        },\n        {\n            \"func\": f3,\n            \"initial_simplex\": np.array([[-1.2, 1], [-1.2, 1.2], [-1, 1]], dtype=float),\n            \"tol_d\": 1e-3, \"tol_f\": 1e-6, \"max_iter\": 1000\n        },\n        {\n            \"func\": f4,\n            \"initial_simplex\": np.array([[0, 0], [1, 0], [0, 1]], dtype=float),\n            \"tol_d\": np.sqrt(2), \"tol_f\": 0.5, \"max_iter\": 200\n        }\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        # Run with diameter stopping criterion\n        Id, Fd, Dd, Sd = nelder_mead(\n            case[\"func\"], case[\"initial_simplex\"],\n            case[\"tol_d\"], case[\"tol_f\"], case[\"max_iter\"], 'diameter'\n        )\n\n        # Run with function spread stopping criterion\n        If, Ff, Df, Sf = nelder_mead(\n            case[\"func\"], case[\"initial_simplex\"],\n            case[\"tol_d\"], case[\"tol_f\"], case[\"max_iter\"], 'spread'\n        )\n\n        B = Id < If\n        \n        # Assemble results for the current case\n        case_results = [Id, If, B, Fd, Ff, Dd, Sd, Df, Sf]\n        \n        # Format the list into a string representation without spaces\n        case_str = f\"[{','.join(map(str, case_results))}]\"\n        all_results_str.append(case_str)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3187957"}]}