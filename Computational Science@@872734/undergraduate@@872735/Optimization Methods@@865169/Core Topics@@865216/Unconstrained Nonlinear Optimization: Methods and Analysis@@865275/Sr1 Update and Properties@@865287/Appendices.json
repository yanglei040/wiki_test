{"hands_on_practices": [{"introduction": "To build intuition, it's often best to start simple. This exercise reduces the SR1 update to a one-dimensional setting, revealing its fundamental connection to the secant method for approximating curvature. By applying it to a quadratic function, you will observe the powerful 'finite termination' property, where SR1 computes the exact Hessian in a single step, a key theoretical underpinning of its effectiveness. [@problem_id:3184208]", "problem": "Consider a one-dimensional unconstrained smooth minimization problem for a twice continuously differentiable objective function $f:\\mathbb{R}\\to\\mathbb{R}$ with gradient $g(x)=\\nabla f(x)$. In a Quasi-Newton (QN) method, a scalar curvature approximation $B_k$ to the Hessian $\\nabla^2 f(x_k)$ is maintained, and the step $s_k$ is defined by the linear model $B_k s_k = -g(x_k)$ with the new iterate $x_{k+1}=x_k+s_k$. The Symmetric Rank-One (SR1) update constructs $B_{k+1}$ to satisfy the QN secant condition $B_{k+1}s_k = y_k$, where $y_k := g(x_{k+1}) - g(x_k)$, using a rank-one correction to $B_k$ and requiring the update to be well-defined in the one-dimensional case whenever $y_k - B_k s_k \\neq 0$.\n\n- Task 1 (analytical): Starting only from the QN secant condition $B_{k+1}s_k = y_k$ and the fact that SR1 makes a rank-one change to $B_k$ in one dimension, derive the expression for $B_{k+1}$ in terms of $y_k$ and $s_k$ when $y_k - B_k s_k \\neq 0$. Then, using the structure of a quadratic function $f(x) = \\frac{1}{2} a x^2 + b x + c$ with constant curvature $a > 0$, explain why the one-dimensional SR1 update attains the exact Newton curvature after a single step in this setting, and show how this implies that the subsequent QN step coincides with the Newton step for this quadratic.\n\n- Task 2 (piecewise linear-quadratic illustration): Consider the piecewise quadratic objective\n$$\nf(x) = \n\\begin{cases}\n\\frac{1}{2}\\cdot 3\\, x^2 - 1\\cdot x,  x \\leq 0, \\\\\n\\frac{1}{2}\\cdot 1\\, x^2 - 1\\cdot x,  x > 0,\n\\end{cases}\n$$\nwhich is continuous at $x=0$. Let $x_0 = -\\frac{1}{4}$ and $B_0 = 2$. Perform one QN step to get $x_1 = x_0 + s_0$ from $B_0 s_0 = -g(x_0)$, then compute $y_0 = g(x_1) - g(x_0)$ and use your result from Task 1 to obtain $B_1$. Finally, take one more QN step from $x_1$ using $B_1$ to obtain $x_2 = x_1 + s_1$ with $B_1 s_1 = -g(x_1)$. Provide the value of $x_2$ rounded to five significant figures. No physical units are involved; report the number as a pure real value.", "solution": "The user has provided a two-part problem concerning the Symmetric Rank-One (SR1) update for a one-dimensional Quasi-Newton (QN) optimization method.\n\nFirst, the problem's validity is assessed.\n- **Step 1: Extract Givens**\n  - Problem context: One-dimensional unconstrained smooth minimization of a twice continuously differentiable objective function $f:\\mathbb{R}\\to\\mathbb{R}$.\n  - Gradient: $g(x) = \\nabla f(x)$.\n  - QN step: $x_{k+1} = x_k + s_k$, where $B_k s_k = -g(x_k)$. $B_k$ is a scalar approximation to the Hessian $\\nabla^2 f(x_k)$.\n  - Secant condition: $B_{k+1}s_k = y_k$, where $y_k = g(x_{k+1}) - g(x_k)$.\n  - SR1 property: $B_{k+1}$ is formed by a symmetric rank-one correction to $B_k$.\n  - 1D well-definedness condition: The update is well-defined whenever $y_k - B_k s_k \\neq 0$.\n  - **Task 1**: Derive the 1D SR1 update for $B_{k+1}$. Analyze its behavior for a quadratic function $f(x) = \\frac{1}{2} a x^2 + b x + c$ ($a > 0$), showing it computes the exact curvature in one step and that the subsequent QN step is a Newton step.\n  - **Task 2**: A numerical illustration with $f(x) = \\begin{cases} \\frac{1}{2}\\cdot 3\\, x^2 - 1\\cdot x,  x \\leq 0, \\\\ \\frac{1}{2}\\cdot 1\\, x^2 - 1\\cdot x,  x > 0. \\end{cases}$\n  - Initial conditions for Task 2: $x_0 = -\\frac{1}{4}$ and $B_0 = 2$.\n  - Required output for Task 2: Value of the second iterate $x_2$, rounded to five significant figures.\n\n- **Step 2: Validate Using Extracted Givens**\n  - The problem is scientifically grounded in the field of numerical optimization.\n  - The problem is well-posed, with all necessary information provided for both tasks.\n  - There is a minor inconsistency: the introduction specifies a twice continuously differentiable function, while Task 2 uses a function that is only once continuously differentiable (the second derivative is discontinuous at $x=0$). However, the mechanics of the QN steps and the SR1 update only require the existence of the gradient, which is continuous for the given piecewise function. Therefore, all calculations are well-defined and can be performed as requested. This inconsistency does not invalidate the problem but rather serves to test the application of the algorithm in a non-ideal but tractable scenario.\n  \n- **Step 3: Verdict and Action**\n  - The problem is deemed valid. A full solution will be provided.\n\n### Task 1: Analytical Derivation and Properties\n\nThe SR1 update requires that the new Hessian approximation, $B_{k+1}$, is obtained by a symmetric rank-one correction to the previous approximation, $B_k$. In a one-dimensional setting, $B_k$ and $B_{k+1}$ are scalars. A rank-one correction matrix is also a scalar. Let us denote this correction by $c_k$. Thus, the update has the form:\n$$B_{k+1} = B_k + c_k$$\nThe symmetry requirement is trivially satisfied as scalars are symmetric.\n\nThis new approximation must satisfy the secant condition:\n$$B_{k+1} s_k = y_k$$\nwhere $s_k = x_{k+1} - x_k$ and $y_k = g(x_{k+1}) - g(x_k)$.\n\nSubstituting the update form into the secant condition gives:\n$$(B_k + c_k) s_k = y_k$$\n$$B_k s_k + c_k s_k = y_k$$\nWe can solve for the correction term $c_k$. Assuming we are not at a stationary point, $g(x_k) \\neq 0$, which implies $s_k \\neq 0$ (unless $B_k=0$, a degenerate case). We can therefore divide by $s_k$:\n$$c_k = \\frac{y_k - B_k s_k}{s_k}$$\nThe problem states that the update is well-defined whenever $y_k - B_k s_k \\neq 0$. This condition ensures that the correction $c_k$ is non-zero, meaning the approximation is meaningfully updated.\n\nSubstituting the expression for $c_k$ back into the update formula for $B_{k+1}$:\n$$B_{k+1} = B_k + \\frac{y_k - B_k s_k}{s_k} = B_k + \\frac{y_k}{s_k} - \\frac{B_k s_k}{s_k} = B_k + \\frac{y_k}{s_k} - B_k$$\nThis simplifies to:\n$$B_{k+1} = \\frac{y_k}{s_k}$$\nThis expression is the one-dimensional SR1 update formula. It is equivalent to the secant method's approximation of the second derivative.\n\nNow, consider the quadratic objective function $f(x) = \\frac{1}{2} a x^2 + b x + c$, with constant curvature $a > 0$.\nThe gradient is $g(x) = a x + b$.\nThe exact Hessian is $\\nabla^2 f(x) = a$ for all $x$.\n\nLet's analyze one step of the QN method starting from an arbitrary iterate $x_k$ and an arbitrary initial Hessian approximation $B_k$. The step is $x_{k+1} = x_k + s_k$. The gradient difference is:\n$$y_k = g(x_{k+1}) - g(x_k) = (a x_{k+1} + b) - (a x_k + b) = a (x_{k+1} - x_k) = a s_k$$\nNow, we compute the updated Hessian approximation $B_{k+1}$ using the derived SR1 formula:\n$$B_{k+1} = \\frac{y_k}{s_k} = \\frac{a s_k}{s_k} = a$$\nThis demonstrates that after a single step on a quadratic function, the 1D SR1 update yields the exact Hessian (curvature), $a$, regardless of the initial approximation $B_k$.\n\nThe subsequent QN step, $s_{k+1}$, is determined by the equation $B_{k+1} s_{k+1} = -g(x_{k+1})$. Since we have shown that $B_{k+1} = a$, this equation becomes:\n$$a s_{k+1} = -g(x_{k+1})$$\nA Newton step, $p_{k+1}$, is defined by the equation $\\nabla^2 f(x_{k+1}) p_{k+1} = -g(x_{k+1})$. For the quadratic function, $\\nabla^2 f(x_{k+1}) = a$. The Newton step equation is:\n$$a p_{k+1} = -g(x_{k+1})$$\nComparing the two equations, we see that $s_{k+1} = p_{k+1}$. Therefore, the QN step following the SR1 update is identical to the Newton step. For a quadratic function, the Newton step from any point $x$ converges to the minimizer in a single iteration.\n\n### Task 2: Piecewise Linear-Quadratic Illustration\n\nWe are given the objective function:\n$$f(x) = \n\\begin{cases}\n\\frac{3}{2} x^2 - x,  x \\leq 0, \\\\\n\\frac{1}{2} x^2 - x,  x > 0,\n\\end{cases}\n$$\nThe gradient is:\n$$g(x) = f'(x) = \n\\begin{cases}\n3x - 1,  x \\leq 0, \\\\\nx - 1,  x > 0.\n\\end{cases}\n$$\nThe initial conditions are $x_0 = -\\frac{1}{4}$ and $B_0 = 2$.\n\n**Step 1: Compute $x_1$**\nFirst, we compute the gradient at $x_0$. Since $x_0 = -0.25 \\leq 0$, we use the first case for $g(x)$:\n$$g(x_0) = g(-0.25) = 3(-0.25) - 1 = -0.75 - 1 = -1.75$$\nThe first QN step $s_0$ is found from $B_0 s_0 = -g(x_0)$:\n$$2 s_0 = -(-1.75) = 1.75$$\n$$s_0 = \\frac{1.75}{2} = 0.875$$\nThe next iterate $x_1$ is:\n$$x_1 = x_0 + s_0 = -0.25 + 0.875 = 0.625$$\n\n**Step 2: Compute $B_1$**\nTo compute the updated Hessian approximation $B_1$, we use the formula $B_1 = y_0/s_0$. We first need to compute $y_0 = g(x_1) - g(x_0)$.\nWe have $x_1 = 0.625 > 0$, so we use the second case for $g(x)$:\n$$g(x_1) = g(0.625) = 0.625 - 1 = -0.375$$\nNow we compute $y_0$:\n$$y_0 = g(x_1) - g(x_0) = -0.375 - (-1.75) = 1.375$$\nWe can now find $B_1$:\n$$B_1 = \\frac{y_0}{s_0} = \\frac{1.375}{0.875} = \\frac{11/8}{7/8} = \\frac{11}{7}$$\n\n**Step 3: Compute $x_2$**\nThe second QN step $s_1$ is found from $B_1 s_1 = -g(x_1)$:\n$$\\frac{11}{7} s_1 = -(-0.375) = 0.375 = \\frac{3}{8}$$\nSolving for $s_1$:\n$$s_1 = \\frac{3}{8} \\cdot \\frac{7}{11} = \\frac{21}{88}$$\nThe final iterate $x_2$ is:\n$$x_2 = x_1 + s_1 = 0.625 + \\frac{21}{88} = \\frac{5}{8} + \\frac{21}{88} = \\frac{5 \\cdot 11}{88} + \\frac{21}{88} = \\frac{55+21}{88} = \\frac{76}{88} = \\frac{19}{22}$$\nTo provide the numerical answer, we compute the decimal value of $x_2$:\n$$x_2 = \\frac{19}{22} \\approx 0.86363636...$$\nRounding to five significant figures gives:\n$$x_2 \\approx 0.86364$$", "answer": "$$\n\\boxed{0.86364}\n$$", "id": "3184208"}, {"introduction": "A key advantage of the SR1 method over more common updates like BFGS is its ability to handle non-convexity. This practice guides you through an experiment focused on a saddle point, demonstrating how SR1 can generate an indefinite Hessian approximation. You will see firsthand how this allows the optimizer to identify and exploit negative curvature directions to escape the saddle, a task where methods that enforce positive definiteness often struggle. [@problem_id:3184299]", "problem": "Consider the quadratic objective function defined for $x \\in \\mathbb{R}^2$ by\n$$\nf(x) = x_1^2 - x_2^2 + \\alpha x_1 x_2,\n$$\nwhere $\\alpha \\in \\mathbb{R}$ is a parameter. Let $H \\in \\mathbb{R}^{2 \\times 2}$ denote the symmetric Hessian matrix of $f$, and let $g(x) = \\nabla f(x)$ denote the gradient. The point $x = 0$ is a saddle point. In the context of quasi-Newton methods, two standard Hessian approximation updates are Symmetric Rank-One (SR1) and Broyden-Fletcher-Goldfarb-Shanno (BFGS). The SR1 update allows indefinite approximations, while the BFGS update is commonly used with a curvature condition that enforces positive definiteness.\n\nStarting from the following fundamental base:\n- For a twice continuously differentiable quadratic objective of the form $f(x) = \\tfrac{1}{2} x^\\top H x$ with $H$ symmetric, the gradient is $g(x) = H x$ and the Hessian is the constant matrix $H$.\n- A saddle point of a twice differentiable function is characterized locally by an indefinite Hessian, meaning $H$ has both positive and negative eigenvalues.\n- Quasi-Newton updates construct a sequence of symmetric matrices $\\{B_k\\}$ that aim to approximate the true Hessian $H$ by enforcing the secant condition $B_{k+1} s_k \\approx y_k$, where $s_k = x_{k+1} - x_k$ and $y_k = g(x_{k+1}) - g(x_k)$.\n\nTasks:\n1. Construct $H$ and $g(x)$ for the given $f(x)$ starting from the fundamental base above, and determine the eigenvalues and eigenvectors of $H$. Use these to identify the negative-curvature eigendirection $v_{-}$ associated with the smallest (most negative) eigenvalue $\\lambda_{-}$ of $H$.\n2. Explain, using first principles and the secant condition, why enforcing positive definiteness in the approximate Hessian may suppress detection of negative curvature near a saddle, and why allowing indefiniteness (as SR1 does) can encode the negative curvature direction into the approximation.\n3. Implement a single-update comparison experiment as follows:\n   - Initialize the approximate Hessian with $B_0 = I$, the identity matrix.\n   - For a given $\\alpha$ and a given starting point $x^{(0)} \\in \\mathbb{R}^2$, form a small step $s = h \\, v_{-}$ with $h = 0.1$, where $v_{-}$ is the unit eigenvector of $H$ corresponding to $\\lambda_{-}$. Set $x^{(1)} = x^{(0)} + s$ and compute $y = g(x^{(1)}) - g(x^{(0)})$. For a quadratic $f$, this simplifies to $y = H s$.\n   - Apply the SR1 update to obtain $B_{\\mathrm{SR1}}$ from $B_0$ and the pair $(s,y)$.\n   - Apply the BFGS update to obtain $B_{\\mathrm{BFGS}}$ from $B_0$ and the pair $(s,y)$, but enforce the standard curvature condition by skipping the update entirely when $s^\\top y \\le 0$ so that $B_{\\mathrm{BFGS}} = B_0$ in that case.\n   - For each $B$ in $\\{B_{\\mathrm{SR1}}, B_{\\mathrm{BFGS}}\\}$, compute its eigen-decomposition and extract its smallest eigenvalue and corresponding unit eigenvector. Determine whether $B$ captures negative curvature (i.e., has a negative smallest eigenvalue) and whether its smallest-eigenvector aligns with the true negative-curvature eigendirection $v_{-}$, quantified by the absolute cosine $|\\langle v_{-}, v_B \\rangle|$ between the unit vectors. Use a threshold of $0.9$ for declaring a strong alignment.\n4. Using the above procedure, evaluate the following test suite of parameter settings:\n   - Test case $1$: $\\alpha = 0$, $x^{(0)} = (0.2, -0.05)$.\n   - Test case $2$: $\\alpha = 1$, $x^{(0)} = (-0.1, 0.15)$.\n   - Test case $3$: $\\alpha = 3$, $x^{(0)} = (0.05, 0.06)$.\n   The scalar step size in the construction of $s$ is $h = 0.1$ in all cases.\n5. For each test case, produce a boolean result that is true if and only if $B_{\\mathrm{SR1}}$ both (i) has a negative smallest eigenvalue and (ii) its smallest-eigenvector has absolute cosine with $v_{-}$ at least $0.9$, while simultaneously $B_{\\mathrm{BFGS}}$ fails to have any negative eigenvalue. This boolean is intended to quantify the advantage of SR1’s allowance of indefiniteness for encoding the correct escape direction near the saddle.\n   \nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the three test cases in order, for example: $[{\\tt result1},{\\tt result2},{\\tt result3}]$, where each ${\\tt resulti}$ is either ${\\tt True}$ or ${\\tt False}$.\n- No physical units are involved; all quantities are dimensionless real numbers.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of numerical optimization and multivariable calculus, is well-posed with a clear objective and sufficient data, and uses precise, objective language. We may therefore proceed with a solution.\n\nThe core of the problem is to compare the behavior of the Symmetric Rank-One (SR1) and Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton updates in the vicinity of a saddle point. The analysis hinges on how each method's Hessian approximation, $B$, evolves when a step is taken along a direction of negative curvature.\n\n### Task 1: System Characterization (Gradient, Hessian, and Eigendecomposition)\n\nThe given objective function is $f(x) = x_1^2 - x_2^2 + \\alpha x_1 x_2$ for $x = (x_1, x_2)^\\top \\in \\mathbb{R}^2$.\n\nFirst, we compute the gradient vector $g(x) = \\nabla f(x)$:\n$$\ng(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 + \\alpha x_2 \\\\ -2x_2 + \\alpha x_1 \\end{pmatrix}\n$$\nThis can be written in matrix form as $g(x) = Hx$, where the matrix $H$ is:\n$$\nH = \\begin{pmatrix} 2  \\alpha \\\\ \\alpha  -2 \\end{pmatrix}\n$$\nThe Hessian of $f(x)$ is the matrix of second partial derivatives, $\\nabla^2 f(x)$, which is indeed the constant, symmetric matrix $H$. The problem states that for a quadratic function of the form $f(x) = \\frac{1}{2} x^\\top H x$, the gradient is $g(x) = Hx$. Our function satisfies this structure.\n\nNext, we find the eigenvalues of $H$ by solving the characteristic equation $\\det(H - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 2-\\lambda  \\alpha \\\\ \\alpha  -2-\\lambda \\end{pmatrix} = (2-\\lambda)(-2-\\lambda) - \\alpha^2 = -(4-\\lambda^2) - \\alpha^2 = \\lambda^2 - 4 - \\alpha^2 = 0\n$$\nThis gives $\\lambda^2 = 4 + \\alpha^2$, so the eigenvalues are $\\lambda = \\pm\\sqrt{4 + \\alpha^2}$.\nAs $\\sqrt{4+\\alpha^2} > 0$ for any real $\\alpha$, $H$ has one positive eigenvalue, $\\lambda_+ = \\sqrt{4+\\alpha^2}$, and one negative eigenvalue, $\\lambda_- = -\\sqrt{4+\\alpha^2}$. This confirms that the Hessian is indefinite and the origin $x=0$ (where $g(0)=0$) is a saddle point.\n\nThe negative-curvature eigendirection $v_{-}$ is the eigenvector corresponding to the smallest (most negative) eigenvalue $\\lambda_{-}$. We solve the system $(H - \\lambda_{-}I)v_{-} = 0$:\n$$\n\\begin{pmatrix} 2 - \\lambda_{-}  \\alpha \\\\ \\alpha  -2 - \\lambda_{-} \\end{pmatrix} v_{-} = \\begin{pmatrix} 2 + \\sqrt{4+\\alpha^2}  \\alpha \\\\ \\alpha  -2 + \\sqrt{4+\\alpha^2} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first row, we get $(2 + \\sqrt{4+\\alpha^2})v_1 + \\alpha v_2 = 0$. An unnormalized eigenvector is thus proportional to $\\begin{pmatrix} \\alpha \\\\ -(2+\\sqrt{4+\\alpha^2}) \\end{pmatrix}$. The vector $v_{-}$ required by the problem is the unit-norm version of this vector.\n\n### Task 2: Theoretical Underpinnings of SR1 and BFGS Updates\n\nThe secant condition, $B_{k+1}s_k = y_k$, is central to quasi-Newton methods. Here, $s_k = x_{k+1} - x_k$ and $y_k = g_{k+1} - g_k$. For a quadratic function with Hessian $H$, we have $y_k = H s_k$.\n\n**BFGS Update and Positive Definiteness:**\nThe standard BFGS update formula is:\n$$\nB_{k+1} = B_k - \\frac{B_k s_k s_k^\\top B_k}{s_k^\\top B_k s_k} + \\frac{y_k y_k^\\top}{y_k^\\top s_k}\n$$\nA key property of this update is that if $B_k$ is positive definite and the curvature condition $y_k^\\top s_k > 0$ holds, then $B_{k+1}$ is also positive definite. The curvature condition $y_k^\\top s_k = s_k^\\top H s_k$ checks if the function's curvature along the direction $s_k$ is positive. Near a saddle point, it is possible to choose a step $s_k$ such that $s_k^\\top H s_k \\le 0$. This corresponds to moving along a direction of non-positive curvature. As specified, optimization algorithms using BFGS often enforce positive definiteness by skipping the update when $y_k^\\top s_k \\le 0$. If we start with $B_0 = I$ (which is positive definite) and only perform updates when $y_k^\\top s_k > 0$, the entire sequence of Hessian approximations $\\{B_k\\}$ remains positive definite. A positive definite matrix can only have positive eigenvalues. Consequently, such a strategy prevents the Hessian approximation from ever capturing the negative curvature (i.e., negative eigenvalues) of the true Hessian $H$ near a saddle point.\n\n**SR1 Update and Indefiniteness:**\nThe SR1 update formula is:\n$$\nB_{k+1} = B_k + \\frac{(y_k - B_k s_k)(y_k - B_k s_k)^\\top}{(y_k - B_k s_k)^\\top s_k}\n$$\nUnlike BFGS, the denominator $(y_k - B_k s_k)^\\top s_k$ in the SR1 formula can be positive, negative, or zero. If the denominator is negative, the rank-one term subtracted from $B_k$ can introduce negative eigenvalues, allowing $B_{k+1}$ to become indefinite. This is crucial for modeling saddle points.\n\nConsider the specific step proposed in the problem: $s = h v_{-}$, where $v_{-}$ is the negative-curvature eigendirection. For this step, $y = H s = H(h v_{-}) = h (H v_{-}) = h (\\lambda_{-} v_{-}) = \\lambda_{-} s$.\nLet's analyze the SR1 update starting from $B_0 = I$:\nThe numerator term is $(y - B_0 s)(y - B_0 s)^\\top = (\\lambda_{-} s - s)(\\lambda_{-} s - s)^\\top = (\\lambda_{-}-1)^2 s s^\\top$.\nThe denominator is $(y - B_0 s)^\\top s = (\\lambda_{-} s - s)^\\top s = (\\lambda_{-} - 1)s^\\top s = (\\lambda_{-} - 1)\\|s\\|^2$.\nSince $\\lambda_{-} = -\\sqrt{4+\\alpha^2} \\le -2$, the denominator is strictly negative.\nThe update is:\n$$\nB_{\\mathrm{SR1}} = I + \\frac{(\\lambda_{-}-1)^2 s s^\\top}{(\\lambda_{-} - 1)\\|s\\|^2} = I + \\frac{\\lambda_{-}-1}{\\|s\\|^2} s s^\\top\n$$\nLet's test the action of $B_{\\mathrm{SR1}}$ on the step vector $s$:\n$$\nB_{\\mathrm{SR1}} s = \\left( I + \\frac{\\lambda_{-}-1}{\\|s\\|^2} s s^\\top \\right) s = s + \\frac{\\lambda_{-}-1}{\\|s\\|^2} s (s^\\top s) = s + (\\lambda_{-} - 1)s = \\lambda_{-} s\n$$\nThis shows that $s$ (and thus $v_{-}$) is an eigenvector of the updated matrix $B_{\\mathrm{SR1}}$ with the eigenvalue $\\lambda_{-}$. Since $\\lambda_{-}$ is negative, the SR1 update has successfully incorporated the negative curvature of the true Hessian $H$ into its approximation. Any vector $w$ orthogonal to $s$ is an eigenvector with eigenvalue $1$, since $s^\\top w=0$ implies $B_{\\mathrm{SR1}}w = (I + \\dots)w = Iw = w$. Thus, $B_{\\mathrm{SR1}}$ has eigenvalues $\\lambda_{-}$ and $1$, correctly capturing the negative curvature.\n\n### Tasks 3-5: Implementation and Evaluation\n\nThe logic described above is implemented for the three specified test cases. For each case, we construct $H$, find its negative eigendirection $v_{-}$, and compute the step $s$ and gradient difference $y$. We then apply the SR1 and BFGS update rules.\n\nFor the BFGS update, the crucial quantity is the curvature $s^\\top y$. In our setup, $s^\\top y = s^\\top (\\lambda_{-} s) = \\lambda_{-} \\|s\\|^2$. Since $\\lambda_{-}  0$ and $\\|s\\|^2 > 0$, the curvature is always negative. Therefore, the BFGS update is skipped in all test cases, and $B_{\\mathrm{BFGS}}$ remains the identity matrix $I$, which has no negative eigenvalues.\n\nFor the SR1 update, as shown analytically, the resulting matrix $B_{\\mathrm{SR1}}$ will have $v_{-}$ as an eigenvector with eigenvalue $\\lambda_{-}$. This means $B_{\\mathrm{SR1}}$ will have a negative eigenvalue, and its corresponding eigenvector will be perfectly aligned with the true negative-curvature direction $v_{-}$.\n\nConsequently, for all test cases, the condition for the boolean result will be met:\n1. $B_{\\mathrm{SR1}}$ has a negative smallest eigenvalue ($\\lambda_{-}$).\n2. The corresponding eigenvector $v_{B,SR1}$ is $v_{-}$, so the absolute cosine $|\\langle v_{-}, v_{B,SR1} \\rangle|$ is $1$, which is $\\ge 0.9$.\n3. $B_{\\mathrm{BFGS}}$ remains $I$ and has no negative eigenvalues.\n\nThe final boolean result will be true for all three test cases. The provided Python code executes this procedure numerically.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a single-step comparison of SR1 and BFGS updates near a saddle point\n    for a given quadratic function.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, x0_tuple)\n        (0.0, (0.2, -0.05)),\n        (1.0, (-0.1, 0.15)),\n        (3.0, (0.05, 0.06)),\n    ]\n\n    results = []\n    \n    # Parameters for the experiment\n    h = 0.1\n    alignment_threshold = 0.9\n\n    for alpha, x0_tuple in test_cases:\n        x0 = np.array(x0_tuple) # x0 is not used in the calculations as per problem logic\n        \n        # 1. Construct H and find its negative-curvature eigendirection\n        H = np.array([\n            [2.0, alpha],\n            [alpha, -2.0]\n        ])\n        \n        # np.linalg.eigh returns eigenvalues in ascending order for symmetric matrices\n        eigenvalues, eigenvectors = np.linalg.eigh(H)\n        lambda_minus = eigenvalues[0]\n        v_minus = eigenvectors[:, 0]\n\n        # 3. Form the step s and gradient difference y\n        B0 = np.eye(2)\n        s = h * v_minus\n        \n        # For a quadratic function f(x) = 1/2 * x^T*H*x, y = g(x1)-g(x0) = Hx1-Hx0 = H(x1-x0) = Hs\n        y = H @ s\n        \n        # --- SR1 Update ---\n        y_minus_B0s = y - B0 @ s\n        den_sr1 = (y_minus_B0s).T @ s\n        # As shown in the analysis, den_sr1 is non-zero for this problem setup.\n        B_sr1 = B0 + np.outer(y_minus_B0s, y_minus_B0s) / den_sr1\n\n        # --- BFGS Update (with curvature check) ---\n        sTy = s.T @ y\n        if sTy > 0:\n            # This branch is not taken in this specific problem, as s^T*y will be negative.\n            B0s = B0 @ s\n            sTB0s = s.T @ B0s\n            B_bfgs = B0 - np.outer(B0s, B0s) / sTB0s + np.outer(y, y) / sTy\n        else:\n            # Curvature condition s^T*y = 0 is met, so we skip the BFGS update.\n            B_bfgs = B0\n\n            \n        # 4. Analyze the resulting Hessian approximations\n        \n        # SR1 Analysis\n        eigvals_sr1, eigvecs_sr1 = np.linalg.eigh(B_sr1)\n        sr1_min_eigval = eigvals_sr1[0]\n        v_B_sr1 = eigvecs_sr1[:, 0]\n        \n        sr1_has_neg_curv = sr1_min_eigval  0\n        # The dot product of two unit vectors is the cosine of the angle between them.\n        sr1_is_aligned = abs(v_minus.T @ v_B_sr1) >= alignment_threshold\n        sr1_condition_met = sr1_has_neg_curv and sr1_is_aligned\n\n        # BFGS Analysis\n        eigvals_bfgs, _ = np.linalg.eigh(B_bfgs)\n        bfgs_min_eigval = eigvals_bfgs[0]\n        bfgs_has_neg_curv = bfgs_min_eigval  0\n\n        # 5. Determine the final boolean result for the test case\n        # The result is True if SR1 succeeds and BFGS fails to capture negative curvature.\n        final_result = sr1_condition_met and not bfgs_has_neg_curv\n        results.append(final_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3184299"}, {"introduction": "The flexibility of the SR1 update comes with a numerical caveat: the update formula can become unstable if its denominator, $(y_k - B_k s_k)^T s_k$, is close to zero. This exercise explores this critical practical issue, often called the 'denominator breakdown,' and the common strategy of skipping the update to maintain robustness. By engineering scenarios that trigger this breakdown, you will gain a deeper understanding of when this safeguard is crucial and how it impacts the algorithm's performance. [@problem_id:3184285]", "problem": "Consider unconstrained minimization of a twice continuously differentiable function $f:\\mathbb{R}^n\\to\\mathbb{R}$ using a Quasi-Newton method with the Symmetric Rank-One (SR1) update and a Strong Wolfe line search. Let $x_k\\in\\mathbb{R}^n$ denote the iterate, $g_k=\\nabla f(x_k)$ the gradient, $B_k$ a symmetric approximation of the Hessian, $p_k=-B_k^{-1}g_k$ the search direction, $s_k=\\alpha_k p_k$ the step, and $y_k=g_{k+1}-g_k$ the gradient difference. The SR1 update is constructed from the defining secant condition $B_{k+1}s_k\\approx y_k$ and a minimal-change principle in a suitable norm, but it is known to be numerically unsafe when the scalar denominator $(y_k - B_k s_k)^\\top s_k$ is near zero; in such cases implementations skip the SR1 update and retain $B_{k+1}=B_k$. A Strong Wolfe line search selects $\\alpha_k0$ that satisfies both the sufficient decrease condition $f(x_k+\\alpha_k p_k)\\le f(x_k)+c_1\\alpha_k g_k^\\top p_k$ and the curvature condition $\\lvert\\nabla f(x_k+\\alpha_k p_k)^\\top p_k\\rvert\\le c_2\\lvert g_k^\\top p_k\\rvert$, where $c_1\\in(0,1)$ and $c_2\\in(c_1,1)$ are fixed constants.\n\nYour task is to explore, design, and test sequences $(s_k,y_k)$ that cause frequent SR1 skipping by making $(y_k - B_k s_k)^\\top s_k\\approx 0$, and to study the convergence behavior under a Strong Wolfe line search. Base your reasoning and implementation on the following fundamental components only: the definition of the gradient $g(x)=\\nabla f(x)$, the secant condition $B_{k+1}s_k\\approx y_k$, and the Strong Wolfe line search conditions. Do not use or state any closed-form SR1 update expression in the problem statement; derive and implement it in the solution based on the principles above.\n\nImplement a program that:\n- Constructs and minimizes three test functions using an SR1-based Quasi-Newton method with a Strong Wolfe line search, a denominator-breakdown skip criterion, and a fixed gradient-norm convergence tolerance. In each test, count the number of skipped SR1 updates caused by near-zero denominators.\n- Uses the skip criterion $\\lvert(y_k - B_k s_k)^\\top s_k\\rvert \\le \\delta \\lVert s_k\\rVert \\,\\lVert y_k - B_k s_k\\rVert$ for a given threshold $\\delta0$.\n- Stops when $\\lVert g_k\\rVert \\le \\varepsilon$ or when a maximum iteration count is reached.\n- Reports, for each test, the integer skip count, a boolean indicating whether convergence was achieved, and the final gradient norm as a float.\n\nTest suite specification:\n1. Quadratic with exact curvature match (forced perpetual skipping, happy path):\n   - Dimension $n=2$.\n   - Hessian $H=\\operatorname{diag}(1,1)$.\n   - Objective $f(x)=\\tfrac{1}{2}x^\\top H x$.\n   - Initial point $x_0=\\begin{bmatrix}10\\\\-7\\end{bmatrix}$.\n   - Initial matrix $B_0=H$.\n   - Strong Wolfe parameters $c_1=10^{-4}$, $c_2=0.9$.\n   - Skip threshold $\\delta=10^{-8}$.\n   - Convergence tolerance $\\varepsilon=10^{-12}$.\n   - Maximum iterations $N_{\\max}=50$.\n   This case forces $(y_k-B_k s_k)^\\top s_k=0$ exactly for all iterations due to $y_k=H s_k$ and $B_k=H$, causing perpetual skipping while still allowing rapid convergence.\n\n2. Quadratic with engineered first-step breakdown (boundary condition):\n   - Dimension $n=2$.\n   - Hessian $H=\\operatorname{diag}(2,1)$.\n   - Objective $f(x)=\\tfrac{1}{2}x^\\top H x$.\n   - Initial matrix $B_0=\\operatorname{diag}(3,0.5)$.\n   - Choose a first step direction $s_0=\\begin{bmatrix}1\\\\\\sqrt{2}\\end{bmatrix}$ satisfying $s_0^\\top(H-B_0)s_0=0$. Set the initial point $x_0=-H^{-1}B_0 s_0=\\begin{bmatrix}-\\tfrac{3}{2}\\\\-\\tfrac{\\sqrt{2}}{2}\\end{bmatrix}$ so that the algorithm’s first search direction equals $s_0$ when $\\alpha_0=1$; this enforces $(y_0-B_0 s_0)^\\top s_0=0$ exactly on the first update.\n   - Strong Wolfe parameters $c_1=10^{-4}$, $c_2=0.9$.\n   - Skip threshold $\\delta=10^{-8}$.\n   - Convergence tolerance $\\varepsilon=10^{-12}$.\n   - Maximum iterations $N_{\\max}=50$.\n\n3. Perturbed quadratic (nonlinear curvature, near-breakdown behavior):\n   - Dimension $n=2$.\n   - Base Hessian $H=\\operatorname{diag}(2,1)$.\n   - Objective $f(x)=\\tfrac{1}{2}x^\\top H x + \\tfrac{\\eta}{4}\\sum_{i=1}^2 x_i^4$, with small perturbation parameter $\\eta=10^{-3}$.\n   - Initial point $x_0=\\begin{bmatrix}1\\\\-1.5\\end{bmatrix}$.\n   - Initial matrix $B_0=H$.\n   - Strong Wolfe parameters $c_1=10^{-4}$, $c_2=0.9$.\n   - Skip threshold $\\delta=10^{-8}$.\n   - Convergence tolerance $\\varepsilon=10^{-10}$.\n   - Maximum iterations $N_{\\max}=50$.\n   This case typically exhibits $(y_k-B_k s_k)^\\top s_k$ small but not exactly zero for many steps because the quartic perturbation makes the Hessian position-dependent, so the SR1 skip may trigger intermittently depending on the chosen $\\delta$ and the local geometry.\n\nOutput format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of the form $[\\text{skips},\\text{converged},\\text{final\\_grad\\_norm}]$.\n- For example, a valid overall output with three tests is $[[2,True,0.0001],[1,False,0.0123],[10,True,1.0e-08]]$.\n\nAll mathematical quantities in this problem statement are expressed in LaTeX. Angles and physical units are not applicable here. Answers are integers, booleans, and floats as specified. The test suite spans a happy path (perpetual skipping with exact curvature), a boundary-engineered breakdown on the first step, and a nonlinear edge case with near-breakdown behavior under Strong Wolfe line search.", "solution": "The problem requires the design and implementation of a Quasi-Newton optimization algorithm utilizing the Symmetric Rank-One (SR1) update formula, safeguarded against numerical instability, and coupled with a Strong Wolfe line search. The task is to analyze the algorithm's behavior on three specifically constructed test cases designed to probe the SR1 update's skipping mechanism.\n\nThe solution proceeds in three stages: first, the derivation of the SR1 update from fundamental principles; second, the detailed design of the iterative algorithm; and third, an analysis of the specified test cases.\n\n### 1. Derivation of the SR1 Update Formula\n\nThe SR1 update modifies the current Hessian approximation $B_k$ to a new approximation $B_{k+1}$ that satisfies the secant condition. The secant condition is a finite-difference approximation of the defining property of the true Hessian, $\\nabla^2 f(x) s \\approx \\nabla f(x+s) - \\nabla f(x)$. For the step from $x_k$ to $x_{k+1}$, where $s_k = x_{k+1} - x_k$ and $y_k = g_{k+1} - g_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$, the secant condition for the new Hessian approximation $B_{k+1}$ is:\n$$\nB_{k+1} s_k = y_k\n$$\nThe SR1 formula is derived by positing that the update is of the simplest non-trivial form: a symmetric rank-one correction to the previous matrix $B_k$. That is,\n$$\nB_{k+1} = B_k + \\sigma u u^\\top\n$$\nfor some scalar $\\sigma \\in \\mathbb{R}$ and vector $u \\in \\mathbb{R}^n$. The term $\\sigma u u^\\top$ is a symmetric rank-one matrix.\n\nTo determine $\\sigma$ and $u$, we enforce the secant condition on $B_{k+1}$:\n$$\n(B_k + \\sigma u u^\\top) s_k = y_k\n$$\nRearranging the terms yields:\n$$\nB_k s_k + \\sigma u (u^\\top s_k) = y_k\n$$\n$$\n\\sigma (u^\\top s_k) u = y_k - B_k s_k\n$$\nThis equation reveals that the vector $u$ must be proportional to the vector $y_k - B_k s_k$. A convenient choice is to set $u = y_k - B_k s_k$. Let this vector be denoted by $v_k = y_k - B_k s_k$. If $v_k=0$, it implies that the previous matrix $B_k$ already satisfies the secant condition along the direction $s_k$. In this case, no update is needed, so we can set $B_{k+1}=B_k$.\n\nIf $v_k \\neq 0$, substituting $u = v_k$ into the equation gives:\n$$\n\\sigma (v_k^\\top s_k) v_k = v_k\n$$\nSince $v_k \\neq 0$, we can divide by the vector $v_k$ to obtain a scalar equation:\n$$\n\\sigma (v_k^\\top s_k) = 1\n$$\nIf the scalar denominator $(v_k^\\top s_k) = (y_k - B_k s_k)^\\top s_k$ is non-zero, we can solve for $\\sigma$:\n$$\n\\sigma = \\frac{1}{(y_k - B_k s_k)^\\top s_k}\n$$\nSubstituting $\\sigma$ and $u$ back into the update form yields the SR1 update formula:\n$$\nB_{k+1} = B_k + \\frac{(y_k - B_k s_k)(y_k - B_k s_k)^\\top}{(y_k - B_k s_k)^\\top s_k}\n$$\nThis formula is undefined if the denominator $(y_k - B_k s_k)^\\top s_k = 0$. In practice, it is numerically unstable if this denominator is close to zero.\n\n### 2. Algorithm Design: Quasi-Newton Method with SR1 and Strong Wolfe Line Search\n\nThe core of the implementation is a generic Quasi-Newton iterative procedure.\n\n**Algorithm: SR1 Quasi-Newton Method**\n1.  **Initialization:** Given an initial point $x_0$, an initial symmetric Hessian approximation $B_0$ (often the identity matrix), a convergence tolerance $\\varepsilon > 0$, and a maximum number of iterations $N_{\\max}$. Set $k=0$, and compute the initial gradient $g_0 = \\nabla f(x_0)$. Initialize a skip counter `skips = 0`.\n\n2.  **Iteration:** For $k = 0, 1, 2, \\dots, N_{\\max}-1$:\n    a.  **Convergence Check:** If $\\lVert g_k \\rVert \\le \\varepsilon$, terminate and report convergence.\n    b.  **Compute Search Direction:** Solve the linear system $B_k p_k = -g_k$ for the search direction $p_k$. This is a defining step of Newton-type methods.\n    c.  **Line Search:** Find a step length $\\alpha_k > 0$ that satisfies the Strong Wolfe conditions:\n        i.  Sufficient Decrease: $f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top p_k$\n        ii. Curvature Condition: $\\lvert g(x_k + \\alpha_k p_k)^\\top p_k \\rvert \\le c_2 \\lvert g_k^\\top p_k \\rvert$\n        for constants $0  c_1  c_2  1$. This search ensures that the step makes sufficient progress in reducing the objective function and that the step is not excessively short or long. A robust line search routine is employed for this purpose. If no such $\\alpha_k$ is found, the optimization terminates.\n    d.  **Update State:** Calculate the step $s_k = \\alpha_k p_k$ and the new iterate $x_{k+1} = x_k + s_k$. Compute the new gradient $g_{k+1} = \\nabla f(x_{k+1})$ and the gradient difference $y_k = g_{k+1} - g_k$.\n    e.  **Update Hessian Approximation ($B_{k+1}$):**\n        i.  Calculate the vectors $v_k = y_k - B_k s_k$ and the denominator $d_k = v_k^\\top s_k$.\n        ii. **Skip Condition:** Check if the update is numerically safe. Using the provided criterion, if $\\lvert d_k \\rvert \\le \\delta \\lVert s_k \\rVert \\lVert v_k \\rVert$ for a small threshold $\\delta > 0$, the update is deemed unsafe. In this case, skip the update by setting $B_{k+1} = B_k$ and increment the skip counter.\n        iii. **SR1 Update:** Otherwise, apply the SR1 formula: $B_{k+1} = B_k + \\frac{v_k v_k^\\top}{d_k}$.\n\n3.  **Termination:** If the loop completes due to reaching $N_{\\max}$, report non-convergence. The final state includes the skip count, convergence status, and the final gradient norm.\n\n### 3. Analysis of Test Cases\n\nThe three test cases are designed to systematically evaluate the behavior of the SR1 update and its skip logic under different curvature conditions.\n\n1.  **Quadratic with Exact Curvature Match:** For the quadratic function $f(x) = \\frac{1}{2}x^\\top H x$, the gradient is $g(x) = Hx$. The gradient difference is $y_k = g_{k+1} - g_k = Hx_{k+1} - Hx_k = H(x_{k+1} - x_k) = H s_k$. The initial Hessian approximation is set to the true Hessian, $B_0 = H$. In the first iteration, $v_0 = y_0 - B_0 s_0 = H s_0 - H s_0 = 0$. The denominator is thus $(y_0 - B_0 s_0)^\\top s_0 = 0$. The skip condition $\\lvert 0 \\rvert \\le \\delta \\lVert s_0 \\rVert \\lVert 0 \\rVert$, which simplifies to $0 \\le 0$, is met. The update is skipped, and $B_1 = B_0 = H$. By induction, if $B_k=H$, the subsequent update will also be skipped, leading to $B_{k+1}=H$. Thus, skipping occurs at every iteration. For this specific problem, the Newton step with the exact Hessian, $p_k = -H^{-1}(Hx_k) = -x_k$, with $\\alpha_k=1$ finds the minimum at $x=0$ in a single step. Therefore, we expect one iteration and one skipped update.\n\n2.  **Quadratic with Engineered First-Step Breakdown:** This case constructs a scenario where the SR1 denominator is exactly zero at the first step ($k=0$) despite $B_0 \\neq H$. The initial conditions are engineered so that $p_0$ is a specific vector $s_0$ and the line search is likely to choose $\\alpha_0=1$. The denominator is $(y_0 - B_0 s_0)^\\top s_0$. Since $f(x)$ is quadratic with Hessian $H$, $y_0 = H s_0$. The denominator becomes $(H s_0 - B_0 s_0)^\\top s_0 = s_0^\\top(H-B_0)s_0$. The problem provides $s_0$, $H$, and $B_0$ such that this quantity is precisely zero. This forces a skip at $k=0$. After this step, $B_1=B_0$, and the subsequent iterations proceed with this sub-optimal Hessian approximation, potentially converging without further skips unless the geometry triggers another near-zero denominator.\n\n3.  **Perturbed Quadratic:** This function, $f(x)=\\tfrac{1}{2}x^\\top H x + \\tfrac{\\eta}{4}\\sum_{i} x_i^4$, is no longer purely quadratic. Its Hessian, $\\nabla^2 f(x) = H + 3\\eta \\cdot \\operatorname{diag}(x_1^2, x_2^2, \\dots)$, is now dependent on the position $x$. The initial approximation $B_0=H$ is exact only at $x=0$. At other points, $B_k$ will lag behind the true Hessian. The term $y_k$ can be approximated by the mean value theorem as $y_k = \\bar{H}_k s_k$, where $\\bar{H}_k = \\int_0^1 \\nabla^2 f(x_k + \\tau s_k) d\\tau$ is the averaged Hessian along the step $s_k$. The denominator becomes $((\\bar{H}_k - B_k)s_k)^\\top s_k$. Due to the non-linearity, $\\bar{H}_k - B_k$ will typically be non-zero, but for small steps or when $B_k$ is a good approximation, this difference can be small. Consequently, the denominator can become small relative to the norms of the vectors, triggering the skip condition intermittently as the iterates explore regions of changing curvature. This test case models a more realistic scenario where near-breakdowns occur due to the evolving geometry of the problem.\n\nThe implementation will now follow, adhering to this theoretical framework.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import line_search\n\ndef quasi_newton_sr1(func, grad, x0, B0, c1, c2, delta, epsilon, max_iter):\n    \"\"\"\n    Performs unconstrained optimization using a Quasi-Newton method with\n    the SR1 update, a Strong Wolfe line search, and a skip condition.\n    \"\"\"\n    k = 0\n    skips = 0\n    converged = False\n    \n    x_k = np.copy(x0)\n    B_k = np.copy(B0)\n    g_k = grad(x_k)\n\n    for k in range(max_iter):\n        g_norm = np.linalg.norm(g_k)\n        if g_norm = epsilon:\n            converged = True\n            break\n        \n        try:\n            # Step 1: Compute search direction\n            p_k = np.linalg.solve(B_k, -g_k)\n        except np.linalg.LinAlgError:\n            # Failed to compute search direction, terminate\n            break\n\n        # Ensure it's a descent direction (should be if Bk is PD)\n        if g_k.T @ p_k >= 0:\n            break\n\n        # Step 2: Strong Wolfe Line Search\n        f_k = func(x_k)\n        # Using scipy's line search implementation\n        alpha_k, _, _, _, _, g_kp1 = line_search(\n            func, grad, x_k, p_k, gfk=g_k, old_fval=f_k, c1=c1, c2=c2, maxiter=50\n        )\n\n        if alpha_k is None:\n            # Line search failed to find a suitable step length\n            break\n\n        # Step 3: Update state\n        s_k = alpha_k * p_k\n        x_kp1 = x_k + s_k\n        # g_kp1 is returned by line_search\n        y_k = g_kp1 - g_k\n\n        # Step 4: Update Hessian approximation (SR1)\n        v = y_k - B_k @ s_k\n        s_norm = np.linalg.norm(s_k)\n        \n        if s_norm == 0:\n            # Zero step, no progress, terminate\n            break\n            \n        v_norm = np.linalg.norm(v)\n        denom = v.T @ s_k\n\n        # Check skip condition\n        if np.abs(denom) = delta * s_norm * v_norm:\n            B_kp1 = B_k # Skip update\n            skips += 1\n        else:\n            B_kp1 = B_k + np.outer(v, v) / denom\n\n        # Prepare for next iteration\n        x_k = x_kp1\n        g_k = g_kp1\n        B_k = B_kp1\n        \n    final_grad_norm = np.linalg.norm(g_k)\n    if not converged:\n        converged = final_grad_norm = epsilon\n\n    return [skips, converged, final_grad_norm]\n\ndef solve():\n    \"\"\"\n    Sets up and runs the three test cases for the SR1 Quasi-Newton method.\n    \"\"\"\n    test_cases = []\n\n    # Test Case 1: Quadratic with exact curvature match\n    H1 = np.diag([1.0, 1.0])\n    f1 = lambda x: 0.5 * x.T @ H1 @ x\n    g1 = lambda x: H1 @ x\n    x0_1 = np.array([10.0, -7.0])\n    B0_1 = np.copy(H1)\n    params1 = {\n        'func': f1, 'grad': g1, 'x0': x0_1, 'B0': B0_1,\n        'c1': 1e-4, 'c2': 0.9, 'delta': 1e-8,\n        'epsilon': 1e-12, 'max_iter': 50\n    }\n    test_cases.append(params1)\n\n    # Test Case 2: Quadratic with engineered first-step breakdown\n    H2 = np.diag([2.0, 1.0])\n    f2 = lambda x: 0.5 * x.T @ H2 @ x\n    g2 = lambda x: H2 @ x\n    B0_2 = np.diag([3.0, 0.5])\n    s0_2 = np.array([1.0, np.sqrt(2.0)])\n    # x0 is engineered: x0 = -inv(H) * B0 * s0\n    x0_2 = -np.linalg.inv(H2) @ B0_2 @ s0_2\n    params2 = {\n        'func': f2, 'grad': g2, 'x0': x0_2, 'B0': B0_2,\n        'c1': 1e-4, 'c2': 0.9, 'delta': 1e-8,\n        'epsilon': 1e-12, 'max_iter': 50\n    }\n    test_cases.append(params2)\n\n    # Test Case 3: Perturbed quadratic\n    H3 = np.diag([2.0, 1.0])\n    eta3 = 1e-3\n    f3 = lambda x: 0.5 * x.T @ H3 @ x + (eta3 / 4.0) * (x[0]**4 + x[1]**4)\n    g3 = lambda x: H3 @ x + eta3 * np.array([x[0]**3, x[1]**3])\n    x0_3 = np.array([1.0, -1.5])\n    B0_3 = np.copy(H3)\n    params3 = {\n        'func': f3, 'grad': g3, 'x0': x0_3, 'B0': B0_3,\n        'c1': 1e-4, 'c2': 0.9, 'delta': 1e-8,\n        'epsilon': 1e-10, 'max_iter': 50\n    }\n    test_cases.append(params3)\n\n    results = []\n    for params in test_cases:\n        result = quasi_newton_sr1(\n            params['func'], params['grad'], params['x0'], params['B0'],\n            params['c1'], params['c2'], params['delta'],\n            params['epsilon'], params['max_iter']\n        )\n        results.append(result)\n\n    final_output = []\n    for res in results:\n        final_output.append(f\"[{res[0]},{'True' if res[1] else 'False'},{res[2]}]\")\n\n    print(f\"[{','.join(final_output)}]\")\n\n\nsolve()\n\n```", "id": "3184285"}]}