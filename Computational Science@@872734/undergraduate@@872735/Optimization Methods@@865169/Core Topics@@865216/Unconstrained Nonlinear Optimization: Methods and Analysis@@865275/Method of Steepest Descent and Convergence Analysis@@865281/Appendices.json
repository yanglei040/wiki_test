{"hands_on_practices": [{"introduction": "Theory provides bounds on convergence, but seeing them in action provides a much deeper intuition. This first practice invites you to implement the method of steepest descent for a simple but important class of functions—convex quadratics. By systematically varying the problem's condition number, $\\kappa$, and the desired accuracy, you will empirically verify the relationship between these factors and the number of iterations required, bringing the theoretical convergence bounds to life through computation.", "problem": "You are to implement the method of steepest descent with exact line search on a strictly convex quadratic objective and empirically examine how many iterations are needed to reduce the optimization error by a prescribed factor. The study should validate that the required number of iterations scales logarithmically with the requested error reduction factor. Work with a quadratic objective that has a known eigenstructure so that its condition number is precisely controlled.\n\nUse the following fundamental base:\n- A strictly convex quadratic objective function $f(\\mathbf{x})$ has the form $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} \\mathbf{A} \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}$, where $\\mathbf{A}$ is real, symmetric, and positive definite. Symmetric Positive Definite (SPD) means $\\mathbf{A} = \\mathbf{A}^{\\top}$ and $\\mathbf{z}^{\\top}\\mathbf{A}\\mathbf{z} > 0$ for all nonzero $\\mathbf{z}$.\n- The unique minimizer $\\mathbf{x}^{\\star}$ satisfies $\\mathbf{A}\\mathbf{x}^{\\star} = \\mathbf{b}$.\n- The method of steepest descent generates iterates $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)$, where $\\alpha_k$ is chosen by exact one-dimensional minimization along the negative gradient direction at $\\mathbf{x}_k$.\n\nYour task:\n- For each test case below, build the SPD matrix $\\mathbf{A}$ with the specified eigenvalues by taking $\\mathbf{A}$ diagonal with those eigenvalues, choose the minimizer as $\\mathbf{x}^{\\star}$ with components $x^{\\star}_i = i$ for $i = 1, \\dots, n$, set $\\mathbf{b} = \\mathbf{A}\\mathbf{x}^{\\star}$, and initialize $\\mathbf{x}_0 = \\mathbf{0}$.\n- Implement steepest descent with exact line search from $\\mathbf{x}_0$. Let $f^{\\star} = f(\\mathbf{x}^{\\star})$ and define the initial error $E_0 = f(\\mathbf{x}_0) - f^{\\star}$.\n- For a given target exponent $p$, run the iterations until the first index $k$ such that $f(\\mathbf{x}_k) - f^{\\star} \\leq E_0 \\cdot 10^{-p}$. Record this $k$ as the required number of iterations for that test case. All angles, if any, are to be interpreted in radians, but no angles are used in this problem.\n- The goal is to observe how this $k$ depends on $p$ and the spectrum of $\\mathbf{A}$, thereby validating that the required iterations grow proportionally to $\\log(10^p) = p \\log(10)$ for fixed $\\mathbf{A}$.\n\nTest suite:\n- Case $1$: dimension $n = 6$; eigenvalues: three entries equal to $1$ and three entries equal to $9$ (condition number $\\kappa = 9$); target exponent $p = 1$.\n- Case $2$: dimension $n = 6$; eigenvalues: three entries equal to $1$ and three entries equal to $9$ (condition number $\\kappa = 9$); target exponent $p = 3$.\n- Case $3$: dimension $n = 5$; eigenvalues: all entries equal to $7$ (condition number $\\kappa = 1$); target exponent $p = 5$.\n- Case $4$: dimension $n = 8$; eigenvalues: four entries equal to $1$ and four entries equal to $100$ (condition number $\\kappa = 100$); target exponent $p = 2$.\n- Case $5$: dimension $n = 10$; eigenvalues: five entries equal to $1$ and five entries equal to $1000$ (condition number $\\kappa = 1000$); target exponent $p = 3$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the required iteration counts (integers) for the five test cases in the order specified above. For example, the output should look like $[k_1,k_2,k_3,k_4,k_5]$, where each $k_i$ is the computed integer number of iterations for case $i$.", "solution": "The problem statement has been analyzed and is deemed valid. It is a well-posed, scientifically grounded, and objective problem from the field of numerical optimization. All necessary data and definitions for a unique and meaningful solution are provided.\n\nThe core task is to implement the method of steepest descent with exact line search for a specific class of quadratic objective functions and to determine the number of iterations required to achieve a prescribed reduction in the optimization error.\n\nThe objective function is a strictly convex quadratic of the form:\n$$\nf(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^{\\top} \\mathbf{A} \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}\n$$\nwhere $\\mathbf{A}$ is a real, symmetric positive-definite (SPD) matrix of size $n \\times n$, and $\\mathbf{x}, \\mathbf{b} \\in \\mathbb{R}^n$.\n\nThe gradient of this function is given by:\n$$\n\\nabla f(\\mathbf{x}) = \\mathbf{A} \\mathbf{x} - \\mathbf{b}\n$$\nThe function $f(\\mathbf{x})$ has a unique global minimizer, denoted by $\\mathbf{x}^{\\star}$, which is the solution to the linear system $\\nabla f(\\mathbf{x}^{\\star}) = \\mathbf{0}$, or:\n$$\n\\mathbf{A}\\mathbf{x}^{\\star} = \\mathbf{b}\n$$\nThe minimum value of the function is $f^{\\star} = f(\\mathbf{x}^{\\star})$. Substituting $\\mathbf{b} = \\mathbf{A}\\mathbf{x}^{\\star}$ into the expression for $f^{\\star}$ yields:\n$$\nf^{\\star} = \\frac{1}{2} (\\mathbf{x}^{\\star})^{\\top} \\mathbf{A} \\mathbf{x}^{\\star} - (\\mathbf{A}\\mathbf{x}^{\\star})^{\\top} \\mathbf{x}^{\\star} = \\frac{1}{2} (\\mathbf{x}^{\\star})^{\\top} \\mathbf{A} \\mathbf{x}^{\\star} - (\\mathbf{x}^{\\star})^{\\top} \\mathbf{A}^{\\top} \\mathbf{x}^{\\star}\n$$\nSince $\\mathbf{A}$ is symmetric ($\\mathbf{A} = \\mathbf{A}^{\\top}$), this simplifies to:\n$$\nf^{\\star} = -\\frac{1}{2} (\\mathbf{x}^{\\star})^{\\top} \\mathbf{A} \\mathbf{x}^{\\star} = -\\frac{1}{2} (\\mathbf{x}^{\\star})^{\\top} \\mathbf{b}\n$$\nThe method of steepest descent is an iterative algorithm that generates a sequence of points $\\{\\mathbf{x}_k\\}$ starting from an initial guess $\\mathbf{x}_0$. The update rule is:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k\n$$\nwhere $\\mathbf{p}_k$ is the direction of steepest descent, defined as the negative of the gradient at $\\mathbf{x}_k$:\n$$\n\\mathbf{p}_k = - \\nabla f(\\mathbf{x}_k) = -(\\mathbf{A}\\mathbf{x}_k - \\mathbf{b})\n$$\nLet's denote the gradient at step $k$ as $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$. The update rule becomes:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{g}_k\n$$\nThe step size $\\alpha_k$ is chosen to minimize the function $f$ along the ray starting from $\\mathbf{x}_k$ in the direction $\\mathbf{p}_k$. This is known as exact line search. We seek to find $\\alpha_k > 0$ that minimizes the one-dimensional function $\\phi(\\alpha) = f(\\mathbf{x}_k - \\alpha \\mathbf{g}_k)$. The minimum is found by setting the derivative with respect to $\\alpha$ to zero:\n$$\n\\frac{d\\phi}{d\\alpha} = \\nabla f(\\mathbf{x}_k - \\alpha \\mathbf{g}_k)^{\\top} \\cdot (-\\mathbf{g}_k) = 0\n$$\nSubstituting the expression for the gradient:\n$$\n(\\mathbf{A}(\\mathbf{x}_k - \\alpha \\mathbf{g}_k) - \\mathbf{b})^{\\top} (-\\mathbf{g}_k) = 0\n$$\n$$\n((\\mathbf{A}\\mathbf{x}_k - \\mathbf{b}) - \\alpha \\mathbf{A} \\mathbf{g}_k)^{\\top} \\mathbf{g}_k = 0\n$$\nRecognizing that $\\mathbf{g}_k = \\mathbf{A}\\mathbf{x}_k - \\mathbf{b}$:\n$$\n(\\mathbf{g}_k - \\alpha \\mathbf{A} \\mathbf{g}_k)^{\\top} \\mathbf{g}_k = 0\n$$\n$$\n\\mathbf{g}_k^{\\top}\\mathbf{g}_k - \\alpha \\mathbf{g}_k^{\\top} \\mathbf{A} \\mathbf{g}_k = 0\n$$\nSolving for $\\alpha$, we obtain the optimal step size for the quadratic case:\n$$\n\\alpha_k = \\frac{\\mathbf{g}_k^{\\top}\\mathbf{g}_k}{\\mathbf{g}_k^{\\top}\\mathbf{A}\\mathbf{g}_k}\n$$\nThe implementation will proceed as follows for each test case.\n$1$. The matrix $\\mathbf{A}$ is constructed as a diagonal matrix with the specified eigenvalues.\n$2$. The minimizer $\\mathbf{x}^{\\star}$ is defined with components $x^{\\star}_i = i$ for $i \\in \\{1, \\dots, n\\}$.\n$3$. The vector $\\mathbf{b}$ is computed from $\\mathbf{b} = \\mathbf{A}\\mathbf{x}^{\\star}$.\n$4$. The initial point is $\\mathbf{x}_0 = \\mathbf{0}$.\n$5$. The minimum function value $f^{\\star}$ is calculated as $f^{\\star} = -0.5 \\cdot (\\mathbf{x}^{\\star})^{\\top}\\mathbf{b}$.\n$6$. The initial error is $E_0 = f(\\mathbf{x}_0) - f^{\\star}$. Since $\\mathbf{x}_0 = \\mathbf{0}$, $f(\\mathbf{x}_0) = 0$, so $E_0 = -f^{\\star}$.\n$7$. The target error value is $E_{target} = E_0 \\cdot 10^{-p}$, where $p$ is the given target exponent.\n$8$. The algorithm iterates, starting with $k=0$ and $\\mathbf{x}_k = \\mathbf{x}_0$. In each iteration, a new point $\\mathbf{x}_{k+1}$ is computed. The loop terminates when the first index $k$ is found such that $f(\\mathbf{x}_k) - f^{\\star} \\leq E_{target}$. This value of $k$ is the result for the test case.\n\nA special case arises when the condition number $\\kappa(\\mathbf{A}) = \\lambda_{\\max}/\\lambda_{\\min} = 1$. This occurs when all eigenvalues are equal, i.e., $\\mathbf{A} = c\\mathbf{I}$ for some constant $c > 0$. In this scenario, the steepest descent method is guaranteed to converge in a single iteration. This is because the level sets of $f(\\mathbf{x})$ are hyperspheres, and the gradient at any point $\\mathbf{x}_k$ points directly towards the center (the minimizer $\\mathbf{x}^{\\star}$). This will be observed in Case $3$.", "answer": "```python\nimport numpy as np\n\ndef run_steepest_descent(dimension, eigenvalues, p_exponent):\n    \"\"\"\n    Implements the method of steepest descent to find the number of iterations\n    required to reduce the optimization error by a factor of 10**(-p).\n\n    Args:\n        dimension (int): The dimension of the vector space, n.\n        eigenvalues (list): A list of eigenvalues for the matrix A.\n        p_exponent (int): The target exponent for error reduction.\n\n    Returns:\n        int: The number of iterations k.\n    \"\"\"\n    n = dimension\n    p = p_exponent\n\n    # 1. Build the SPD matrix A\n    A = np.diag(eigenvalues)\n\n    # 2. Choose the minimizer x_star and compute b\n    x_star = np.arange(1, n + 1)\n    b = A @ x_star\n\n    # 3. Compute the minimum function value f_star\n    f_star = -0.5 * x_star.T @ b\n\n    # 4. Initialize steepest descent\n    x_k = np.zeros(n)\n    k = 0\n\n    # 5. Calculate initial error and target error\n    f_at_x0 = 0.5 * x_k.T @ A @ x_k - b.T @ x_k  # This will be 0\n    E_0 = f_at_x0 - f_star\n    \n    # Handle the trivial case where initial error is zero or negative.\n    if E_0 <= 0:\n        return 0\n        \n    target_error_value = E_0 * (10**(-p))\n    \n    # 6. Check if initial point already meets the criterion\n    # This check is implicitly handled by the loop structure.\n    # The loop will not execute if the condition is already met, returning k=0.\n    \n    # 7. Iteration loop\n    while True:\n        # Check current error before performing an iteration step\n        f_at_xk = 0.5 * x_k.T @ A @ x_k - b.T @ x_k\n        current_error = f_at_xk - f_star\n        if current_error <= target_error_value:\n            break\n\n        # Calculate gradient\n        g_k = A @ x_k - b\n\n        # If gradient is zero, we are at the minimum.\n        if np.allclose(g_k, 0):\n            break\n\n        # Calculate optimal step size alpha_k\n        gkTg = g_k.T @ g_k\n        gkT_A_gk = g_k.T @ A @ g_k\n        alpha_k = gkTg / gkT_A_gk\n\n        # Update x_k\n        x_k = x_k - alpha_k * g_k\n        \n        # Increment iteration counter\n        k += 1\n\n    return k\n\ndef solve():\n    \"\"\"\n    Defines and runs the test cases, printing the results in the required format.\n    \"\"\"\n    test_cases = [\n        # Case 1: n=6, kappa=9, p=1\n        {\"dimension\": 6, \"eigenvalues\": [1, 1, 1, 9, 9, 9], \"p_exponent\": 1},\n        # Case 2: n=6, kappa=9, p=3\n        {\"dimension\": 6, \"eigenvalues\": [1, 1, 1, 9, 9, 9], \"p_exponent\": 3},\n        # Case 3: n=5, kappa=1, p=5\n        {\"dimension\": 5, \"eigenvalues\": [7, 7, 7, 7, 7], \"p_exponent\": 5},\n        # Case 4: n=8, kappa=100, p=2\n        {\"dimension\": 8, \"eigenvalues\": [1, 1, 1, 1, 100, 100, 100, 100], \"p_exponent\": 2},\n        # Case 5: n=10, kappa=1000, p=3\n        {\"dimension\": 10, \"eigenvalues\": [1]*5 + [1000]*5, \"p_exponent\": 3},\n    ]\n\n    results = []\n    for case in test_cases:\n        k = run_steepest_descent(**case)\n        results.append(k)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3149696"}, {"introduction": "The steepest descent algorithm, while intuitive, can exhibit surprisingly slow, oscillating \"zig-zag\" behavior on ill-conditioned problems. This exercise challenges you to move from empirical observation to theoretical derivation, dissecting the geometry of a single step to understand exactly why this happens. By analyzing a two-dimensional quadratic function, you will derive the famous worst-case convergence rate from first principles, revealing its direct dependence on the condition number and the orientation of the iterates relative to the function's contours.", "problem": "Consider the unconstrained minimization of the strictly convex quadratic objective function $f(x) = \\tfrac{1}{2} x^{\\top} A x$ in $\\mathbb{R}^{2}$, where $A$ is a symmetric positive definite (SPD) matrix with eigenvalues $\\mu$ and $L$ satisfying $0 < \\mu < L$. Let the eigenstructure of $A$ be rotated by the coordinate rotation $R(\\theta)$ so that $A(\\theta) = R(\\theta) \\,\\mathrm{diag}(\\mu, L)\\, R(\\theta)^{\\top}$, where $R(\\theta) = \\begin{pmatrix}\\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta\\end{pmatrix}$. An optimization algorithm applies Steepest Descent (SD) with exact line search, starting from the initial point $x_{0} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$.\n\nFrom first principles and definitions (gradient of a quadratic, exact line search, and properties of orthogonal rotations), derive an analytic expression for the one-step effective convergence rate, defined as the ratio $r(\\theta) = \\dfrac{f(x_{1})}{f(x_{0})}$, as a function of $\\theta$, $\\mu$, and $L$. Interpret the “zig-zag effect” as the orientation $\\theta$ that maximizes $r(\\theta)$ over $\\theta \\in [0, \\pi/2]$ for fixed $\\mu$ and $L$. Using your expression, determine the maximal value $r_{\\max}$ of $r(\\theta)$ and express $r_{\\max}$ solely in terms of the condition number $\\kappa = \\dfrac{L}{\\mu}$. Provide your final answer as a single closed-form analytical expression for $r_{\\max}(\\kappa)$.", "solution": "The user wants to analyze the one-step convergence rate of the Steepest Descent (SD) method for a specific 2D quadratic optimization problem.\n\n### Step 1: Problem Formulation and Setup\n\nThe objective function is $f(x) = \\frac{1}{2} x^{\\top} A x$ for $x \\in \\mathbb{R}^{2}$. The matrix $A$ is given by its eigendecomposition, rotated by an angle $\\theta$:\n$$\nA = A(\\theta) = R(\\theta) D R(\\theta)^{\\top}\n$$\nwhere $D = \\mathrm{diag}(\\mu, L)$ with $0 < \\mu < L$, and $R(\\theta)$ is the rotation matrix:\n$$\nR(\\theta) = \\begin{pmatrix}\\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta\\end{pmatrix}\n$$\nThe SD algorithm starts at $x_{0} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ and the next iterate is $x_{1} = x_{0} - \\alpha_{0} \\nabla f(x_{0})$. The step size $\\alpha_{0}$ is determined by an exact line search.\n\nThe convergence rate for this step is defined as $r(\\theta) = \\frac{f(x_{1})}{f(x_{0})}$.\n\nTo simplify the calculations, we perform a change of coordinates. Let $y = R(\\theta)^{\\top} x$, so $x = R(\\theta) y$. The objective function in the new coordinates becomes:\n$$\nf(x) = \\tilde{f}(y) = \\frac{1}{2} (R(\\theta)y)^{\\top} (R(\\theta) D R(\\theta)^{\\top}) (R(\\theta)y) = \\frac{1}{2} y^{\\top} R(\\theta)^{\\top} R(\\theta) D R(\\theta)^{\\top} R(\\theta) y\n$$\nSince $R(\\theta)$ is an orthogonal matrix, $R(\\theta)^{\\top}R(\\theta) = I$.\n$$\n\\tilde{f}(y) = \\frac{1}{2} y^{\\top} D y = \\frac{1}{2} (\\mu y_{1}^{2} + L y_{2}^{2})\n$$\nThe initial point $x_{0}$ is transformed to $y_{0}$:\n$$\ny_{0} = R(\\theta)^{\\top} x_{0} = \\begin{pmatrix}\\cos\\theta & \\sin\\theta \\\\ -\\sin\\theta & \\cos\\theta\\end{pmatrix} \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}\\cos\\theta \\\\ -\\sin\\theta\\end{pmatrix}\n$$\nThe gradient in the original coordinates is $\\nabla_{x} f(x) = Ax$. The gradient in the transformed coordinates is $\\nabla_{y} \\tilde{f}(y) = Dy$. The relationship between the gradients is $\\nabla_{x}f(x) = R(\\theta)\\nabla_{y}\\tilde{f}(y)$.\n\nThe SD update in the $y$ coordinates is $y_{1} = y_{0} - \\alpha_{0} D y_{0}$. This is because $R(\\theta)^{\\top} \\nabla_{x} f(x_0) = R(\\theta)^{\\top} A x_0 = R(\\theta)^{\\top} (R(\\theta) D R(\\theta)^{\\top}) (R(\\theta) y_0) = D y_0$. The step size $\\alpha_{0}$ is the same in both coordinate systems as it is a scalar minimizing the same function value.\n\n### Step 2: Derivation of the Convergence Rate $r(\\theta)$\n\nFirst, we express the value of the function at the initial point $x_{0}$ (or $y_{0}$):\n$$\nf(x_{0}) = \\tilde{f}(y_{0}) = \\frac{1}{2} y_{0}^{\\top} D y_{0} = \\frac{1}{2} \\left[ \\mu (\\cos\\theta)^{2} + L (-\\sin\\theta)^{2} \\right] = \\frac{1}{2} (\\mu \\cos^{2}\\theta + L \\sin^{2}\\theta)\n$$\nFor an exact line search minimizing $f(x_{0} - \\alpha g_{0})$ where $g_{0} = \\nabla f(x_{0})$, the optimal step size $\\alpha_{0}$ for a quadratic function is given by:\n$$\n\\alpha_{0} = \\frac{g_{0}^{\\top} g_{0}}{g_{0}^{\\top} A g_{0}}\n$$\nWe express these terms using the transformed coordinates. Let $\\tilde{g}_{0} = \\nabla_{y}\\tilde{f}(y_{0}) = Dy_0$. Then $g_{0} = R(\\theta)\\tilde{g}_{0}$.\n$$\ng_{0}^{\\top} g_{0} = (R(\\theta)\\tilde{g}_{0})^{\\top} (R(\\theta)\\tilde{g}_{0}) = \\tilde{g}_{0}^{\\top} \\tilde{g}_{0} = (Dy_{0})^{\\top} (Dy_{0}) = y_{0}^{\\top} D^{2} y_{0}\n$$\n$$\ng_{0}^{\\top} A g_{0} = (R(\\theta)\\tilde{g}_{0})^{\\top} A (R(\\theta)\\tilde{g}_{0}) = \\tilde{g}_{0}^{\\top} R(\\theta)^{\\top} A R(\\theta) \\tilde{g}_{0} = \\tilde{g}_{0}^{\\top} D \\tilde{g}_{0} = (Dy_{0})^{\\top} D (Dy_{0}) = y_{0}^{\\top} D^{3} y_{0}\n$$\nThe terms evaluate to:\n$$\ny_{0}^{\\top} D^{2} y_{0} = \\mu^{2} \\cos^{2}\\theta + L^{2} \\sin^{2}\\theta\n$$\n$$\ny_{0}^{\\top} D^{3} y_{0} = \\mu^{3} \\cos^{2}\\theta + L^{3} \\sin^{2}\\theta\n$$\nSo the step size is $\\alpha_{0} = \\frac{\\mu^{2} \\cos^{2}\\theta + L^{2} \\sin^{2}\\theta}{\\mu^{3} \\cos^{2}\\theta + L^{3} \\sin^{2}\\theta}$.\n\nThe function value at the next iterate $x_{1}$ is:\n$$\nf(x_{1}) = f(x_{0}) - \\frac{1}{2} \\frac{(g_0^{\\top} g_0)^2}{g_0^{\\top} A g_0} = f(x_0) - \\frac{1}{2} \\frac{(y_0^{\\top} D^2 y_0)^2}{y_0^{\\top} D^3 y_0}\n$$\nThe one-step convergence rate $r(\\theta)$ is:\n$$\nr(\\theta) = \\frac{f(x_{1})}{f(x_{0})} = 1 - \\frac{(y_{0}^{\\top} D^{2} y_{0})^{2}}{2 f(x_{0}) (y_{0}^{\\top} D^{3} y_{0})} = 1 - \\frac{(y_{0}^{\\top} D^{2} y_{0})^{2}}{(y_{0}^{\\top} D y_{0})(y_{0}^{\\top} D^{3} y_{0})}\n$$\nSubstituting the expressions in terms of $\\theta$:\n$$\nr(\\theta) = 1 - \\frac{(\\mu^{2} \\cos^{2}\\theta + L^{2} \\sin^{2}\\theta)^{2}}{(\\mu \\cos^{2}\\theta + L \\sin^{2}\\theta)(\\mu^{3} \\cos^{2}\\theta + L^{3} \\sin^{2}\\theta)}\n$$\nTo simplify, we find a common denominator and combine the terms:\n$$\nr(\\theta) = \\frac{(\\mu \\cos^{2}\\theta + L \\sin^{2}\\theta)(\\mu^{3} \\cos^{2}\\theta + L^{3} \\sin^{2}\\theta) - (\\mu^{2} \\cos^{2}\\theta + L^{2} \\sin^{2}\\theta)^{2}}{(\\mu \\cos^{2}\\theta + L \\sin^{2}\\theta)(\\mu^{3} \\cos^{2}\\theta + L^{3} \\sin^{2}\\theta)}\n$$\nLet's expand the numerator. Let $c^{2} = \\cos^2\\theta$ and $s^{2} = \\sin^2\\theta$.\nNumerator $= (\\mu c^2 + L s^2)(\\mu^3 c^2 + L^3 s^2) - (\\mu^2 c^2 + L^2 s^2)^2$\n$= (\\mu^4 c^4 + \\mu L^3 c^2 s^2 + L\\mu^3 c^2 s^2 + L^4 s^4) - (\\mu^4 c^4 + 2\\mu^2 L^2 c^2 s^2 + L^4 s^4)$\n$= (\\mu L^3 + L\\mu^3 - 2\\mu^2 L^2) c^2 s^2 = \\mu L (L^2 + \\mu^2 - 2\\mu L) c^2 s^2 = \\mu L (L-\\mu)^2 c^2 s^2$\nSo, the analytic expression for the convergence rate is:\n$$\nr(\\theta) = \\frac{\\mu L (L-\\mu)^{2} \\cos^{2}\\theta \\sin^{2}\\theta}{(\\mu \\cos^{2}\\theta + L \\sin^{2}\\theta)(\\mu^{3} \\cos^{2}\\theta + L^{3} \\sin^{2}\\theta)}\n$$\n\n### Step 3: Maximization of the Rate $r(\\theta)$\n\nThe \"zig-zag effect\" is most pronounced when the convergence is slowest, which corresponds to maximizing $r(\\theta)$. We aim to find $\\max_{\\theta \\in [0, \\pi/2]} r(\\theta)$.\nLet $t = \\tan^{2}\\theta$. For $\\theta \\in [0, \\pi/2]$, $t \\in [0, \\infty)$. We use the identities $\\cos^{2}\\theta = \\frac{1}{1+t}$ and $\\sin^{2}\\theta = \\frac{t}{1+t}$.\nSubstituting these into the expression for $r(\\theta)$ yields a function of $t$:\n$$\nr(t) = \\frac{\\mu L (L-\\mu)^2 \\frac{t}{(1+t)^2}}{(\\mu\\frac{1}{1+t} + L\\frac{t}{1+t}) (\\mu^3\\frac{1}{1+t} + L^3\\frac{t}{1+t})} = \\frac{\\mu L (L-\\mu)^2 t}{(\\mu + Lt)(\\mu^3 + L^3 t)}\n$$\nTo find the maximum, we can differentiate with respect to $t$ and set the derivative to zero. Let's focus on maximizing the core part $G(t) = \\frac{t}{(\\mu + Lt)(\\mu^3 + L^3 t)}$.\nThe denominator is $P(t) = (\\mu+Lt)(\\mu^3+L^3 t) = L^4 t^2 + (\\mu L^3 + L\\mu^3)t + \\mu^4$.\nUsing the quotient rule, $G'(t) = \\frac{P(t) \\cdot 1 - t \\cdot P'(t)}{[P(t)]^2}$. We set the numerator to zero:\n$P(t) - t P'(t) = (L^4 t^2 + (\\mu L^3 + L\\mu^3)t + \\mu^4) - t(2L^4 t + \\mu L^3 + L\\mu^3) = 0$\n$L^4 t^2 + (\\mu L^3 + L\\mu^3)t + \\mu^4 - 2L^4 t^2 - (\\mu L^3 + L\\mu^3)t = 0$\n$\\mu^4 - L^4 t^2 = 0 \\implies t^2 = \\frac{\\mu^4}{L^4}$\nSince $t = \\tan^2\\theta \\ge 0$, we take the positive root: $t = \\frac{\\mu^2}{L^2}$.\nThis value of $t$ maximizes $r(t)$.\n\n### Step 4: Calculation of the Maximal Rate $r_{\\max}$\n\nWe substitute $t = \\frac{\\mu^2}{L^2}$ back into the expression for $r(t)$:\n$$\nr_{\\max} = r\\left(t=\\frac{\\mu^2}{L^2}\\right) = \\frac{\\mu L (L-\\mu)^2 (\\mu^2/L^2)}{(\\mu + L(\\mu^2/L^2))(\\mu^3 + L^3(\\mu^2/L^2))}\n$$\nNumerator: $\\mu L (L-\\mu)^2 \\frac{\\mu^2}{L^2} = \\frac{\\mu^3}{L} (L-\\mu)^2$.\nDenominator:\nThe first factor is $\\mu + L\\frac{\\mu^2}{L^2} = \\mu + \\frac{\\mu^2}{L} = \\frac{\\mu L + \\mu^2}{L} = \\frac{\\mu(L+\\mu)}{L}$.\nThe second factor is $\\mu^3 + L^3\\frac{\\mu^2}{L^2} = \\mu^3 + \\mu^2 L = \\mu^2(\\mu+L)$.\nThe product is $\\frac{\\mu(L+\\mu)}{L} \\cdot \\mu^2(\\mu+L) = \\frac{\\mu^3(L+\\mu)^2}{L}$.\nThus, the maximal rate is:\n$$\nr_{\\max} = \\frac{\\frac{\\mu^3}{L} (L-\\mu)^2}{\\frac{\\mu^3(L+\\mu)^2}{L}} = \\frac{(L-\\mu)^2}{(L+\\mu)^2} = \\left(\\frac{L-\\mu}{L+\\mu}\\right)^2\n$$\n\n### Step 5: Expression in terms of Condition Number $\\kappa$\n\nThe condition number of matrix $A$ is $\\kappa = \\frac{L}{\\mu}$. We express $r_{\\max}$ in terms of $\\kappa$:\n$$\nr_{\\max} = \\left(\\frac{L-\\mu}{L+\\mu}\\right)^2 = \\left(\\frac{(L/\\mu) - 1}{(L/\\mu) + 1}\\right)^2 = \\left(\\frac{\\kappa-1}{\\kappa+1}\\right)^2\n$$\nThis is the well-known worst-case convergence rate for the steepest descent method on a quadratic function, derived here from first principles for a specific initial condition that generates the worst case.", "answer": "$$\n\\boxed{\\left(\\frac{\\kappa-1}{\\kappa+1}\\right)^2}\n$$", "id": "3149756"}, {"introduction": "While convex problems guarantee that a gradient of zero signifies a minimum, the world of non-convex optimization is more treacherous, populated by saddle points that can trap algorithms. This practice moves beyond the well-behaved quadratic landscape to explore the behavior of steepest descent on a function with a saddle point, a scenario common in modern machine learning. You will demonstrate how easily the method can be fooled and then implement a simple yet powerful technique—injecting random noise—to help the algorithm escape the saddle and continue its search for a true local minimum.", "problem": "You are to construct and analyze a concrete instance of the method of steepest descent in the setting of a smooth nonconvex function that possesses a saddle point. Start from the foundational definitions: the gradient of a differentiable function, the Hessian matrix, and the method of steepest descent under the Euclidean inner product. Using these, implement a program that executes steepest descent on a specific smooth nonconvex function, demonstrates convergence to a saddle point for certain initializations, and then shows how small random perturbations can enable escape from the saddle.\n\nTasks to perform:\n- Define the function $f : \\mathbb{R}^2 \\to \\mathbb{R}$ by\n$$\nf(x,y) = x^2 - y^2 + y^4\n$$,\nwhich is smooth and nonconvex. The steepest descent direction with respect to the Euclidean inner product is the negative gradient $-\\nabla f(x,y)$. Construct the gradient mapping $g(x,y) = \\nabla f(x,y)$.\n- Use the fundamental definitions to justify that $(0,0)$ is a saddle point of $f$ by verifying $g(0,0) = (0,0)$ and that the Hessian $H(0,0)$ is indefinite.\n- Implement steepest descent with a fixed step size $\\alpha$:\n$$\n\\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix}\n=\n\\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix}\n-\n\\alpha \\, g(x_k,y_k)\n$$,\nand a stopping rule based on the gradient norm becoming small or after a maximum number of iterations. Additionally, implement an optional perturbation mechanism: when the current iterate is close to the saddle (within a prescribed radius) and the gradient norm is small, inject a single small random perturbation (drawn from a zero-mean normal distribution with a given standard deviation) to the current point, then continue steepest descent from the perturbed point.\n- Demonstrate that for initializations on the $x$-axis (i.e., $y_0 = 0$), steepest descent converges to the saddle $(0,0)$; whereas for initializations off the $x$-axis (i.e., $y_0 \\neq 0$), steepest descent converges to a local minimum (one of $(0,1/\\sqrt{2})$ or $(0,-1/\\sqrt{2})$). Then demonstrate that adding a small random perturbation near the saddle can enable escape and subsequent convergence to a local minimum.\n\nProgram requirements:\n- Implement the above in a single, self-contained program that takes no input and uses the specified test suite below.\n- For each test case, run steepest descent with the parameters given, and return a boolean indicating whether the algorithm converged to the saddle $(0,0)$ (defined as the final Euclidean norm of the iterate being less than a specified tolerance). The four test cases are:\n    1. Initial point $(x_0,y_0) = (1.0,0.0)$, step size $\\alpha = 0.3$, maximum iterations $200$, no perturbation. This tests convergence to the saddle along the stable manifold.\n    2. Initial point $(x_0,y_0) = (1.0,0.01)$, step size $\\alpha = 0.3$, maximum iterations $400$, no perturbation. This tests escape from the saddle to a local minimum when starting off the saddle’s stable manifold.\n    3. Initial point $(x_0,y_0) = (1.0,0.0)$, step size $\\alpha = 0.3$, maximum iterations $400$, with a single small perturbation allowed when near the saddle. Use perturbation threshold on the gradient norm $10^{-12}$, saddle proximity radius $10^{-3}$, perturbation standard deviation $10^{-3}$, and a fixed random seed $12345$. This tests escape from the saddle via perturbation even when starting on the stable manifold.\n    4. Initial point $(x_0,y_0) = (0.0,0.0)$, step size $\\alpha = 0.3$, maximum iterations $400$, with a single small perturbation allowed using the same thresholds and random seed as in case $3$. This tests escape from the saddle when starting exactly at the saddle.\n\nOutput specification:\n- For each test case, output a boolean indicating whether the final iterate is within the saddle tolerance $10^{-8}$ of $(0,0)$; that is, output $\\mathrm{True}$ if $\\sqrt{x^2+y^2} < 10^{-8}$ at termination, and $\\mathrm{False}$ otherwise.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$).\n- No physical units, angle units, or percentages are involved in this problem; all quantities are dimensionless real numbers.\n\nThe program must adhere to the specified execution environment and libraries.", "solution": "The problem statement has been validated and is deemed a well-posed, scientifically grounded problem in numerical optimization. We will proceed with a solution.\n\nThe core of the problem is to analyze the behavior of the method of steepest descent on the smooth, nonconvex function $f: \\mathbb{R}^2 \\to \\mathbb{R}$ defined as\n$$\nf(x,y) = x^2 - y^2 + y^4\n$$\nThis function serves as a model for understanding how optimization algorithms navigate landscapes with saddle points.\n\nFirst, we establish the necessary analytical tools. The steepest descent direction is given by the negative of the gradient. The gradient mapping, denoted by $g(x,y) = \\nabla f(x,y)$, is calculated from the partial derivatives of $f(x,y)$:\n$$\n\\frac{\\partial f}{\\partial x} = 2x\n$$\n$$\n\\frac{\\partial f}{\\partial y} = -2y + 4y^3\n$$\nThus, the gradient vector is:\n$$\ng(x,y) = \\nabla f(x,y) = \\begin{pmatrix} 2x \\\\ 4y^3 - 2y \\end{pmatrix}\n$$\n\nNext, we verify that the point $(0,0)$ is a saddle point. A point is a critical point if the gradient vanishes there. Evaluating the gradient at $(0,0)$:\n$$\ng(0,0) = \\begin{pmatrix} 2(0) \\\\ 4(0)^3 - 2(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis confirms that $(0,0)$ is a critical point. To classify this point, we analyze the Hessian matrix, $H(x,y)$, which is the matrix of second-order partial derivatives:\n$$\nH(x,y) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial y \\partial x} \\\\ \\frac{\\partial^2 f}{\\partial x \\partial y} & \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 12y^2 - 2 \\end{pmatrix}\n$$\nEvaluating the Hessian at the critical point $(0,0)$:\n$$\nH(0,0) = \\begin{pmatrix} 2 & 0 \\\\ 0 & 12(0)^2 - 2 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}\n$$\nThe eigenvalues of $H(0,0)$ are the diagonal entries, $\\lambda_1 = 2$ and $\\lambda_2 = -2$. Since one eigenvalue is positive and the other is negative, the Hessian matrix is indefinite at $(0,0)$. A critical point with an indefinite Hessian is, by definition, a saddle point. The eigenvector corresponding to the positive eigenvalue $\\lambda_1=2$ is $(1,0)$, spanning the direction of upward curvature (the unstable subspace for a maximization problem, stable for minimization). The eigenvector for $\\lambda_2=-2$ is $(0,1)$, spanning the direction of downward curvature (the unstable subspace for minimization).\n\nThe steepest descent algorithm with a fixed step size $\\alpha > 0$ generates a sequence of iterates $(x_k, y_k)$ according to the rule:\n$$\n\\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix} = \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix} - \\alpha \\, g(x_k,y_k)\n$$\nSubstituting the components of the gradient, we get the specific update rules:\n$$\nx_{k+1} = x_k - \\alpha (2x_k) = (1 - 2\\alpha)x_k\n$$\n$$\ny_{k+1} = y_k - \\alpha (4y_k^3 - 2y_k) = (1 + 2\\alpha)y_k - 4\\alpha y_k^3\n$$\nThe behavior of this dynamical system depends critically on the initial point $(x_0, y_0)$.\n\nIf we initialize on the $x$-axis, i.e., $y_0 = 0$, the second update rule implies $y_1 = (1+2\\alpha)(0) - 4\\alpha(0)^3 = 0$. By induction, if $y_k=0$, then $y_{k+1}=0$. The optimization is thus confined to the $x$-axis, and the dynamics reduce to $x_{k+1} = (1 - 2\\alpha)x_k$. For convergence to $x=0$, we require $|1 - 2\\alpha| < 1$, which holds for $0 < \\alpha < 1$. The specified step size $\\alpha=0.3$ satisfies this condition. Therefore, starting with $y_0=0$, the iterates converge to the saddle point $(0,0)$. This corresponds to moving along the stable manifold of the saddle point for the gradient flow.\n\nIf we initialize with $y_0 \\neq 0$, the dynamics are different. Near the saddle point where $y_k$ is very small, the cubic term $y_k^3$ is negligible compared to the linear term. The update for $y$ can be approximated by $y_{k+1} \\approx (1 + 2\\alpha)y_k$. Since $\\alpha > 0$, the factor $(1+2\\alpha)$ is greater than $1$. This means that any small, non-zero $y$-component will be amplified, pushing the iterate away from the $x$-axis. This is movement along the unstable manifold. The iterate escapes the vicinity of the saddle point and eventually converges to one of the function's local minima, which are located at $(0, \\pm 1/\\sqrt{2})$.\n\nThe implementation will consist of a function executing this iterative process for a given number of steps. The perturbation mechanism is designed to demonstrate that even if the algorithm is on a path to the saddle (e.g., initialized exactly on the stable manifold), a small random nudge is sufficient to push it into the region of instability and lead to escape. The perturbation is applied once when the iterate is very close to the saddle (Euclidean norm below a radius threshold) and the dynamics have slowed down (gradient norm below a threshold).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases for steepest descent\n    on a nonconvex function and report convergence to the saddle point.\n    \"\"\"\n\n    def run_steepest_descent(x0, y0, alpha, max_iter, use_perturbation, pert_params=None):\n        \"\"\"\n        Executes the steepest descent algorithm for a single test case.\n\n        Args:\n            x0 (float): Initial x-coordinate.\n            y0 (float): Initial y-coordinate.\n            alpha (float): The fixed step size.\n            max_iter (int): The maximum number of iterations.\n            use_perturbation (bool): Flag to enable/disable the perturbation mechanism.\n            pert_params (dict, optional): Parameters for the perturbation.\n\n        Returns:\n            bool: True if the final iterate is within the saddle tolerance, False otherwise.\n        \"\"\"\n\n        def grad_f(p):\n            \"\"\"Computes the gradient of f(x,y) = x^2 - y^2 + y^4.\"\"\"\n            x, y = p\n            gx = 2.0 * x\n            gy = 4.0 * y**3 - 2.0 * y\n            return np.array([gx, gy])\n\n        rng = None\n        if use_perturbation and pert_params:\n            rng = np.random.default_rng(pert_params['seed'])\n\n        p = np.array([float(x0), float(y0)])\n        perturbation_applied = False\n\n        for _ in range(max_iter):\n            # Check for and apply perturbation if conditions are met\n            if use_perturbation and not perturbation_applied and pert_params:\n                grad_norm = np.linalg.norm(grad_f(p))\n                saddle_dist = np.linalg.norm(p)\n\n                if (grad_norm < pert_params['grad_norm_thresh'] and\n                        saddle_dist < pert_params['saddle_radius']):\n                    \n                    perturbation = rng.normal(loc=0.0, scale=pert_params['std_dev'], size=2)\n                    p += perturbation\n                    perturbation_applied = True\n            \n            # Calculate gradient and perform update step\n            grad = grad_f(p)\n            p -= alpha * grad\n\n        final_norm = np.linalg.norm(p)\n        saddle_tolerance = 1e-8\n        \n        return final_norm < saddle_tolerance\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Start on stable manifold, no perturbation. Expect convergence to saddle.\n        {'x0': 1.0, 'y0': 0.0, 'alpha': 0.3, 'max_iter': 200, 'use_perturbation': False},\n        \n        # Case 2: Start near stable manifold, no perturbation. Expect escape.\n        {'x0': 1.0, 'y0': 0.01, 'alpha': 0.3, 'max_iter': 400, 'use_perturbation': False},\n\n        # Case 3: Start on stable manifold, with perturbation. Expect escape.\n        {'x0': 1.0, 'y0': 0.0, 'alpha': 0.3, 'max_iter': 400, 'use_perturbation': True,\n         'pert_params': {'grad_norm_thresh': 1e-12, 'saddle_radius': 1e-3, 'std_dev': 1e-3, 'seed': 12345}},\n        \n        # Case 4: Start at the saddle, with perturbation. Expect escape.\n        {'x0': 0.0, 'y0': 0.0, 'alpha': 0.3, 'max_iter': 400, 'use_perturbation': True,\n         'pert_params': {'grad_norm_thresh': 1e-12, 'saddle_radius': 1e-3, 'std_dev': 1e-3, 'seed': 12345}}\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_steepest_descent(**params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # str(True) -> 'True', str(False) -> 'False'\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3149710"}]}