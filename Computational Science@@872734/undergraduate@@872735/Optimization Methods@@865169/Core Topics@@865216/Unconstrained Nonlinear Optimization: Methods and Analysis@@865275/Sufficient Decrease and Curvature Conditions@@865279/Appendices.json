{"hands_on_practices": [{"introduction": "The very foundation of line search methods is the idea of moving \"downhill\" from a given point. This exercise provides a crucial sanity check by exploring what happens when we attempt to apply the sufficient decrease condition along an ascent direction. By working through this simple quadratic example, you will develop a concrete understanding of why the requirement that a search direction be a descent direction (i.e., $\\nabla f(x)^{\\top}p \\lt 0$) is not just a theoretical nicety, but a practical necessity for the Armijo condition to be meaningful [@problem_id:3189981].", "problem": "Consider the twice continuously differentiable function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by $f(x)=\\tfrac{1}{2}\\|x\\|^{2}$, evaluated at the point $x_{0}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$, and the search direction $p=\\begin{pmatrix}1\\\\0\\end{pmatrix}$. The sufficient decrease (Armijo) condition stipulates that for a given constant $c_{1}\\in(0,1)$, an acceptable step length $\\alpha0$ must satisfy\n$$\nf(x_{0}+\\alpha p)\\le f(x_{0})+c_{1}\\,\\alpha\\,\\nabla f(x_{0})^{\\top}p.\n$$\nUsing only the definition of the Armijo condition and basic calculus, analyze the function $f$ along the line $x(\\alpha)=x_{0}+\\alpha p$ and the role of curvature along $p$, quantified by $p^{\\top}\\nabla^{2}f(x_{0})p$. Show that along this direction $p$ the curvature is nonnegative and that $p$ is not a descent direction. Then, compute the threshold value\n$$\nc_{1}^{\\star}=\\inf_{\\alpha0}\\frac{f(x_{0}+\\alpha p)-f(x_{0})}{\\alpha\\,\\nabla f(x_{0})^{\\top}p},\n$$\nwhich is the minimal constant that would, in principle, make the Armijo inequality viable for some $\\alpha0$. Conclude whether any $\\alpha0$ can satisfy the Armijo condition for any $c_{1}\\in(0,1)$ in this setting, and report the single numerical value of $c_{1}^{\\star}$. Your final answer must be a single real number with no units. No rounding is required.", "solution": "The problem is valid as it is scientifically grounded in the principles of numerical optimization, well-posed with a clear objective, and provides all necessary information for a unique solution.\n\nFirst, we establish the properties of the function $f(x)=\\frac{1}{2}\\|x\\|^{2}$ at the point $x_{0}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$. The function is $f(x_1, x_2) = \\frac{1}{2}(x_1^2 + x_2^2)$.\nThe gradient of $f$ is given by $\\nabla f(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x$.\nThe Hessian matrix of $f$ is $\\nabla^{2}f(x) = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I$, where $I$ is the $2 \\times 2$ identity matrix. The Hessian is constant for all $x \\in \\mathbb{R}^2$.\n\nAt the specific point $x_{0}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$, we have:\nThe function value is $f(x_{0}) = \\frac{1}{2}(1^2 + 0^2) = \\frac{1}{2}$.\nThe gradient is $\\nabla f(x_{0}) = x_{0} = \\begin{pmatrix}1\\\\0\\end{pmatrix}$.\nThe Hessian is $\\nabla^{2}f(x_{0}) = I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$.\n\nThe problem asks us to consider the search direction $p=\\begin{pmatrix}1\\\\0\\end{pmatrix}$. A direction $p$ is a descent direction if it satisfies $\\nabla f(x_{0})^{\\top}p  0$. Let's compute this quantity:\n$$\n\\nabla f(x_{0})^{\\top}p = \\begin{pmatrix}1  0\\end{pmatrix} \\begin{pmatrix}1\\\\0\\end{pmatrix} = (1)(1) + (0)(0) = 1.\n$$\nSince $\\nabla f(x_{0})^{\\top}p = 1  0$, the direction $p$ is not a descent direction. It is a direction of ascent, meaning the function value initially increases along this direction from $x_{0}$.\n\nNext, we analyze the curvature of $f$ at $x_{0}$ along the direction $p$. The curvature is given by the quadratic form $p^{\\top}\\nabla^{2}f(x_{0})p$.\n$$\np^{\\top}\\nabla^{2}f(x_{0})p = \\begin{pmatrix}1  0\\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix}1\\\\0\\end{pmatrix} = \\begin{pmatrix}1  0\\end{pmatrix} \\begin{pmatrix}1\\\\0\\end{pmatrix} = 1.\n$$\nThe curvature is $1$, which is a positive value, thus confirming that the curvature is nonnegative.\n\nNow, we compute the threshold value $c_{1}^{\\star}$, defined as:\n$$\nc_{1}^{\\star}=\\inf_{\\alpha0}\\frac{f(x_{0}+\\alpha p)-f(x_{0})}{\\alpha\\,\\nabla f(x_{0})^{\\top}p}.\n$$\nLet's evaluate the terms in this expression. The point along the search direction is $x(\\alpha) = x_{0}+\\alpha p = \\begin{pmatrix}1\\\\0\\end{pmatrix} + \\alpha\\begin{pmatrix}1\\\\0\\end{pmatrix} = \\begin{pmatrix}1+\\alpha\\\\0\\end{pmatrix}$.\nThe function value at this point is:\n$$\nf(x_{0}+\\alpha p) = \\frac{1}{2}\\|x_{0}+\\alpha p\\|^2 = \\frac{1}{2}((1+\\alpha)^2 + 0^2) = \\frac{1}{2}(1+\\alpha)^2 = \\frac{1}{2}(1+2\\alpha+\\alpha^2).\n$$\nThe numerator of the fraction defining $c_{1}^{\\star}$ is:\n$$\nf(x_{0}+\\alpha p)-f(x_{0}) = \\frac{1}{2}(1+2\\alpha+\\alpha^2) - \\frac{1}{2} = \\alpha + \\frac{1}{2}\\alpha^2.\n$$\nThe denominator is:\n$$\n\\alpha\\,\\nabla f(x_{0})^{\\top}p = \\alpha(1) = \\alpha.\n$$\nSubstituting these into the expression for $c_{1}^{\\star}$, we get the ratio inside the infimum:\n$$\n\\frac{\\alpha + \\frac{1}{2}\\alpha^2}{\\alpha}.\n$$\nSince we are considering $\\alpha  0$, we can simplify this fraction by dividing the numerator and denominator by $\\alpha$:\n$$\n\\frac{\\alpha(1 + \\frac{1}{2}\\alpha)}{\\alpha} = 1 + \\frac{1}{2}\\alpha.\n$$\nNow we must find the infimum of this expression over all $\\alpha  0$:\n$$\nc_{1}^{\\star} = \\inf_{\\alpha0} \\left(1 + \\frac{1}{2}\\alpha\\right).\n$$\nThe function $g(\\alpha) = 1 + \\frac{1}{2}\\alpha$ is a strictly increasing function of $\\alpha$. Its infimum on the interval $(0, \\infty)$ is the limit as $\\alpha$ approaches $0$ from the right:\n$$\nc_{1}^{\\star} = \\lim_{\\alpha \\to 0^+} \\left(1 + \\frac{1}{2}\\alpha\\right) = 1.\n$$\nThe threshold value is $c_{1}^{\\star}=1$.\n\nFinally, we must conclude whether any $\\alpha  0$ can satisfy the Armijo condition for any $c_{1} \\in (0,1)$. The Armijo condition is:\n$$\nf(x_{0}+\\alpha p) \\le f(x_{0})+c_{1}\\,\\alpha\\,\\nabla f(x_{0})^{\\top}p.\n$$\nFor $\\alpha  0$ and $\\nabla f(x_0)^\\top p \\ne 0$, we can rearrange this to:\n$$\n\\frac{f(x_{0}+\\alpha p)-f(x_{0})}{\\alpha\\,\\nabla f(x_{0})^{\\top}p} \\le c_1.\n$$\nWe have already calculated the left-hand side to be $1 + \\frac{1}{2}\\alpha$. So the condition becomes:\n$$\n1 + \\frac{1}{2}\\alpha \\le c_1.\n$$\nWe are given that $\\alpha  0$, which implies $\\frac{1}{2}\\alpha  0$, and therefore $1 + \\frac{1}{2}\\alpha  1$.\nThe Armijo condition requires the parameter $c_1$ to be in the interval $(0,1)$, meaning $c_1  1$.\nThe inequality $1 + \\frac{1}{2}\\alpha \\le c_1$ thus requires a number strictly greater than $1$ to be less than or equal to a number strictly less than $1$. This is a contradiction. Therefore, there is no step length $\\alpha  0$ that can satisfy the Armijo condition for any $c_1 \\in (0,1)$. This is a direct consequence of $p$ not being a descent direction. The requested numerical value is $c_{1}^{\\star}$.", "answer": "$$\\boxed{1}$$", "id": "3189981"}, {"introduction": "Having established the need for a descent direction, a natural question arises: can we find a step size that is *guaranteed* to work? This practice delves into the theoretical underpinnings of step-size selection by connecting the Armijo condition to the global curvature of the function, as captured by its Lipschitz constant $L$. You will derive a worst-case step size that ensures sufficient decrease, providing insight into the trade-off between theoretical guarantees and practical performance [@problem_id:3189999].", "problem": "Let $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ be continuously differentiable with an $L$-Lipschitz continuous gradient, meaning there exists $L0$ such that $\\|\\nabla f(x)-\\nabla f(y)\\|\\le L\\|x-y\\|$ for all $x,y\\in\\mathbb{R}^{n}$. Consider a single steepest descent step $x^{+}=x-\\alpha \\nabla f(x)$ with step size $\\alpha0$. The Armijo sufficient decrease condition with parameter $c_{1}\\in(0,1)$ requires that along a descent direction $d$ one has $f(x+\\alpha d)\\le f(x)+c_{1}\\alpha \\nabla f(x)^{\\top}d$. Assume the direction is the steepest descent direction $d=-\\nabla f(x)$ and recall that $L$-Lipschitz continuity of $\\nabla f$ implies the so-called descent lemma. \n\nUsing only these base facts, derive an explicit upper bound on $\\alpha$ that guarantees the Armijo sufficient decrease condition holds for the steepest descent step. Then, specialize this bound to produce a single choice of $\\alpha$ depending only on $L$ that guarantees the Armijo condition for every $c_{1}\\in(0,\\tfrac{1}{2}]$, irrespective of $x$ and $\\nabla f(x)$. Express your final answer as a closed-form analytic expression in terms of $L$. Finally, explain briefly whether this fixed choice is conservative relative to what might be admissible by the Armijo condition in practice, and relate your explanation to the role of curvature conditions such as the Wolfe and Goldstein conditions. No numerical rounding is required for the final answer.", "solution": "The problem asks for the derivation of an upper bound on the step size $\\alpha$ for the steepest descent method that guarantees the Armijo sufficient decrease condition, given a function $f$ with an $L$-Lipschitz continuous gradient. Then, it asks for a specific fixed step size based on this bound and a brief discussion of its practical implications.\n\nFirst, we formalize the given information.\nThe function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ is continuously differentiable.\nIts gradient, $\\nabla f$, is $L$-Lipschitz continuous, which means there exists a constant $L0$ such that for all $x, y \\in \\mathbb{R}^{n}$:\n$$\n\\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x-y\\|\n$$\nThis property implies the Descent Lemma (also known as the quadratic upper bound lemma), which states:\n$$\nf(y) \\le f(x) + \\nabla f(x)^{\\top}(y-x) + \\frac{L}{2} \\|y-x\\|^2\n$$\nThe optimization step is a steepest descent step:\n$$\nx^{+} = x - \\alpha \\nabla f(x)\n$$\nwhere $\\alpha  0$ is the step size. This corresponds to a step $\\alpha d$ in the direction $d = -\\nabla f(x)$.\n\nThe Armijo sufficient decrease condition for a step size $\\alpha$ along a descent direction $d$ is:\n$$\nf(x+\\alpha d) \\le f(x) + c_1 \\alpha \\nabla f(x)^{\\top} d\n$$\nwhere $c_1 \\in (0, 1)$. Substituting the steepest descent direction $d = -\\nabla f(x)$ into this condition gives:\n$$\nf(x - \\alpha \\nabla f(x)) \\le f(x) + c_1 \\alpha \\nabla f(x)^{\\top} (-\\nabla f(x))\n$$\n$$\nf(x - \\alpha \\nabla f(x)) \\le f(x) - c_1 \\alpha \\|\\nabla f(x)\\|^2\n$$\nOur goal is to find an upper bound on $\\alpha$ that guarantees this inequality holds. We use the Descent Lemma. Let $y = x^{+} = x - \\alpha \\nabla f(x)$. Applying the lemma:\n$$\nf(x - \\alpha \\nabla f(x)) \\le f(x) + \\nabla f(x)^{\\top}((x - \\alpha \\nabla f(x)) - x) + \\frac{L}{2} \\|(x - \\alpha \\nabla f(x)) - x\\|^2\n$$\n$$\nf(x - \\alpha \\nabla f(x)) \\le f(x) + \\nabla f(x)^{\\top}(-\\alpha \\nabla f(x)) + \\frac{L}{2} \\|-\\alpha \\nabla f(x)\\|^2\n$$\n$$\nf(x - \\alpha \\nabla f(x)) \\le f(x) - \\alpha \\|\\nabla f(x)\\|^2 + \\frac{L\\alpha^2}{2} \\|\\nabla f(x)\\|^2\n$$\nThis inequality provides an upper bound on the value of $f$ at the next iterate. The Armijo condition will be satisfied if this upper bound is less than or equal to the right-hand side of the Armijo inequality. That is, we require:\n$$\nf(x) - \\alpha \\|\\nabla f(x)\\|^2 + \\frac{L\\alpha^2}{2} \\|\\nabla f(x)\\|^2 \\le f(x) - c_1 \\alpha \\|\\nabla f(x)\\|^2\n$$\nAssuming we are not at a stationary point (i.e., $\\nabla f(x) \\neq 0$), we can subtract $f(x)$ from both sides and then divide by the positive scalar $\\|\\nabla f(x)\\|^2$:\n$$\n-\\alpha + \\frac{L\\alpha^2}{2} \\le -c_1 \\alpha\n$$\nSince $\\alpha  0$ by definition, we can divide by $\\alpha$:\n$$\n-1 + \\frac{L\\alpha}{2} \\le -c_1\n$$\nNow, we solve for $\\alpha$:\n$$\n\\frac{L\\alpha}{2} \\le 1 - c_1\n$$\n$$\n\\alpha \\le \\frac{2(1-c_1)}{L}\n$$\nThis is the explicit upper bound on $\\alpha$ that guarantees the Armijo condition holds for a given $c_1 \\in (0, 1)$.\n\nNext, we must specialize this bound to find a single choice of $\\alpha$, depending only on $L$, that guarantees the condition for every $c_1 \\in (0, \\frac{1}{2}]$. To ensure the inequality $\\alpha \\le \\frac{2(1-c_1)}{L}$ holds for all $c_1$ in this interval, $\\alpha$ must be less than or equal to the minimum value of the expression on the right-hand side over this interval. Let $g(c_1) = \\frac{2(1-c_1)}{L}$. The derivative with respect to $c_1$ is $g'(c_1) = -\\frac{2}{L}$, which is negative since $L0$. Therefore, $g(c_1)$ is a decreasing function of $c_1$. The minimum value of $g(c_1)$ on the interval $(0, \\frac{1}{2}]$ will occur at the largest value of $c_1$, which is $c_1 = \\frac{1}{2}$.\nThe minimum value of the upper bound is:\n$$\n\\min_{c_1 \\in (0, 1/2]} \\frac{2(1-c_1)}{L} = \\frac{2(1 - \\frac{1}{2})}{L} = \\frac{2(\\frac{1}{2})}{L} = \\frac{1}{L}\n$$\nTherefore, any $\\alpha$ such that $0  \\alpha \\le \\frac{1}{L}$ will satisfy the Armijo condition for any choice of $c_1 \\in (0, \\frac{1}{2}]$. The problem asks for a single choice of $\\alpha$; the most general and least restrictive fixed choice that works for any $f$ (with an $L$-Lipschitz gradient) and any $x$ is the upper limit of this range, $\\alpha = \\frac{1}{L}$.\n\nFinally, we are asked to comment on whether this fixed choice is conservative. The step size $\\alpha = \\frac{1}{L}$ is indeed very conservative. This value is a worst-case guarantee derived from the global Lipschitz constant $L$, which reflects the maximum curvature of the function $f$ over its entire domain. In many regions, the local curvature may be much smaller than $L$, meaning that much larger step sizes would be admissible under the Armijo condition and would lead to faster convergence. Practical line search methods, such as backtracking, exploit this by starting with a larger trial step size and iteratively reducing it until the Armijo condition is met, thereby adapting the step size to the local properties of the function at each iteration.\n\nThis relates to the role of curvature conditions, such as the Wolfe or Goldstein conditions. The Armijo condition (a sufficient decrease condition) on its own only provides an upper bound on acceptable step sizes. It does not prevent the step size $\\alpha$ from being excessively small. An algorithm could satisfy the Armijo condition with a tiny $\\alpha$ that makes negligible progress towards the minimum. To ensure meaningful progress, the Armijo condition is typically paired with a curvature condition. The second Wolfe condition, $\\nabla f(x+\\alpha d)^{\\top}d \\ge c_2 \\nabla f(x)^{\\top}d$ for $c_2 \\in (c_1, 1)$, enforces that the slope at the new point is less steep than the initial slope, which effectively places a lower bound on the acceptable step size and rules out pathologically small steps. The Goldstein conditions similarly bracket the acceptable step sizes from above and below. Thus, while our fixed step $\\alpha=\\frac{1}{L}$ guarantees decrease, it may be an overly cautious step that a more sophisticated line search employing curvature conditions would reject in favor of a larger, more productive one.", "answer": "$$\n\\boxed{\\frac{1}{L}}\n$$", "id": "3189999"}, {"introduction": "Theoretical step-size guarantees are often too conservative for practical use, leading to slow convergence. Real-world optimizers rely on adaptive line search procedures that combine sufficient decrease with curvature conditions. This hands-on coding exercise challenges you to implement the strong Wolfe conditions, diagnose a common failure mode where a poor parameter choice causes the algorithm to stagnate, and then engineer a robust solution using an auto-tuning strategy. This practice bridges the gap between theory and implementation, revealing the nuances of building efficient and reliable optimization algorithms [@problem_id:3143424].", "problem": "Consider a twice continuously differentiable function $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ and a descent direction $p\\in\\mathbb{R}^n$ at a point $x\\in\\mathbb{R}^n$. A line search chooses a step size $\\alpha0$ to reduce $f$ along the line $x+\\alpha p$. The inexact line search known as the strong Wolfe conditions consists of two inequalities parameterized by constants $c_1$ and $c_2$ with $0c_1c_21$: the sufficient decrease (Armijo) condition $f(x+\\alpha p)\\le f(x)+c_1\\,\\alpha\\,\\nabla f(x)^\\top p$, and the curvature condition $\\lvert \\nabla f(x+\\alpha p)^\\top p\\rvert \\le c_2\\,\\lvert \\nabla f(x)^\\top p\\rvert$. Many practical implementations perform backtracking of $\\alpha$ by multiplying by a factor in $(0,1)$ until the conditions are satisfied. However, if $c_2$ is poorly chosen and the algorithm only ever decreases $\\alpha$, the curvature condition can force step sizes to stagnate at a minimum threshold.\n\nYour tasks:\n\n1. Use the fundamental definitions of differentiability and the gradient, together with the strong Wolfe sufficient decrease and curvature conditions, to implement a backtracking line search that tests both conditions. The backtracking must start from an initial $\\alpha_00$ and multiply by a factor $\\beta\\in(0,1)$ each time the conditions are not satisfied. To guarantee termination, if $\\alpha$ drops below a prescribed minimum $\\alpha_{\\min}0$, the routine must return $\\alpha_{\\min}$.\n\n2. Construct a concrete example by choosing a positive definite quadratic function $f(x)=\\tfrac{1}{2}\\,x^\\top Q x$ with $Q\\in\\mathbb{R}^{2\\times 2}$ symmetric positive definite (so that $\\nabla f(x)=Qx$), an initial point $x_0\\in\\mathbb{R}^2$, and the steepest descent direction $p_0=-\\nabla f(x_0)$. This setting makes all relevant quantities computable from first principles and ensures scientific realism. In this setting, define $Q=\\mathrm{diag}(100,1)$ and $x_0=(1,1)^\\top$. Use the constants $c_1=10^{-4}$, $\\alpha_0=1$, $\\beta=0.5$, and $\\alpha_{\\min}=10^{-6}$.\n\n3. Demonstrate with this example that a poor choice of $c_2$ causes step sizes to stagnate under pure backtracking. Specifically, show numerically that for small $c_2$ values the algorithm returns $\\alpha_{\\min}$ because backtracking cannot satisfy the curvature condition while only decreasing $\\alpha$.\n\n4. Propose and implement an auto-tuning strategy for $c_2$ based on the observed curvature ratio $r(\\alpha)=\\lvert \\nabla f(x+\\alpha p)^\\top p\\rvert / \\lvert \\nabla f(x)^\\top p\\rvert$. When the sufficient decrease condition is satisfied but the curvature condition fails, update $c_2$ to a relaxed value $c_2^{\\mathrm{new}}$ that depends on the observed ratio. Use the rule $c_2^{\\mathrm{new}}=\\min\\{0.99,\\max\\{c_1+10^{-12},\\,r(\\alpha)+0.05\\}\\}$, then re-check the curvature condition at the current $\\alpha$ with this updated $c_2$. This simple auto-tuning maintains $c_2\\in(c_1,1)$ and uses the observed curvature ratio to avoid stagnation.\n\n5. For reference, also compute the exact line search step size for the quadratic function along the steepest descent direction, which follows from minimizing $f(x+\\alpha p)$ over $\\alpha$ using the definitions of $f$, $\\nabla f$, and the fact that $Q$ is symmetric positive definite.\n\nImplement all computations in a single program and evaluate the following test suite of parameter values, each producing a single floating-point output equal to the accepted step size:\n- Test case 1 (happy path): $c_2=0.9$, with no auto-tuning.\n- Test case 2 (stagnation): $c_2=0.1$, with no auto-tuning.\n- Test case 3 (auto-tuned recovery): $c_2=0.1$, with auto-tuning enabled.\n- Test case 4 (edge, more severe): $c_2=0.001$, with no auto-tuning.\n- Test case 5 (redundant tuning): $c_2=0.9$, with auto-tuning enabled.\n- Test case 6 (baseline exact): compute the exact line search step size for the quadratic $f$ along $p_0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\dots]$). No physical units, angles, or percentages are involved in this problem; all outputs are raw real numbers.", "solution": "The problem requires the implementation and analysis of a backtracking line search algorithm based on the strong Wolfe conditions, demonstrating a common failure mode and a strategy to correct it. The solution is developed based on the fundamental principles of multivariate calculus and numerical optimization.\n\n### Principles of Line Search and the Strong Wolfe Conditions\n\nIn iterative optimization methods like gradient descent or quasi-Newton methods, an update step is taken from the current point $x$ along a descent direction $p$ (i.e., a direction where the function $f$ initially decreases, $\\nabla f(x)^\\top p  0$). The new point is $x_{\\text{new}} = x + \\alpha p$, where $\\alpha  0$ is the step size. The goal of a line search is to find a suitable $\\alpha$ that provides a sufficient reduction in $f$.\n\nThe **strong Wolfe conditions** provide a standard set of criteria for an acceptable step size $\\alpha$. They consist of two inequalities:\n1.  **Sufficient Decrease (Armijo) Condition**: This ensures that the step size $\\alpha$ achieves a tangible decrease in the function value, proportional to the step size and the directional derivative.\n    $$f(x+\\alpha p)\\le f(x)+c_1\\,\\alpha\\,\\nabla f(x)^\\top p$$\n    Here, $c_1 \\in (0, 1)$ is a constant, typically small (e.g., $10^{-4}$). Since $p$ is a descent direction, $\\nabla f(x)^\\top p  0$, so the right-hand side forms an upper bound on $f(x+\\alpha p)$ that is stricter than just $f(x+\\alpha p)  f(x)$.\n\n2.  **Curvature Condition**: This ensures that the step is not too short. It requires that the slope of the function at the new point $x+\\alpha p$ along the direction $p$ is less steep than at the original point $x$, thereby ensuring progress. The strong Wolfe version is:\n    $$\\lvert \\nabla f(x+\\alpha p)^\\top p\\rvert \\le c_2\\,\\lvert \\nabla f(x)^\\top p\\rvert$$\n    Here, $c_2 \\in (c_1, 1)$ is a constant. This condition prevents the algorithm from taking excessively small steps into regions where the gradient is still large.\n\n### The Quadratic Model Problem\n\nTo analyze the algorithm, we use a standard test function: a positive definite quadratic form, $f(x)=\\tfrac{1}{2}\\,x^\\top Q x$. For such a function, the gradient is linear: $\\nabla f(x) = Qx$. This analytical simplicity allows for exact computation of all relevant quantities.\nThe specific parameters are:\n-   $Q = \\mathrm{diag}(100, 1)$, a $2 \\times 2$ symmetric positive definite matrix.\n-   $x_0 = (1, 1)^\\top$.\n-   The direction is the steepest descent direction, $p_0 = -\\nabla f(x_0) = -Qx_0$.\n\nLet's compute the initial values:\n-   $\\nabla f(x_0) = Qx_0 = \\begin{pmatrix} 100  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 100 \\\\ 1 \\end{pmatrix}$.\n-   $p_0 = -\\nabla f(x_0) = \\begin{pmatrix} -100 \\\\ -1 \\end{pmatrix}$.\n-   The directional derivative at $x_0$ is $\\nabla f(x_0)^\\top p_0 = \\begin{pmatrix} 100  1 \\end{pmatrix} \\begin{pmatrix} -100 \\\\ -1 \\end{pmatrix} = -10000 - 1 = -10001$.\n\nThe gradient at a new point $x_0+\\alpha p_0$ is $\\nabla f(x_0+\\alpha p_0) = Q(x_0+\\alpha p_0)$. The directional derivative at this new point is $\\nabla f(x_0+\\alpha p_0)^\\top p_0 = (Q(x_0+\\alpha p_0))^\\top p_0 = (Qx_0 + \\alpha Qp_0)^\\top p_0 = \\nabla f(x_0)^\\top p_0 + \\alpha(Qp_0)^\\top p_0$.\nUsing $p_0 = -Qx_0$, this simplifies to $\\nabla f(x_0)^\\top p_0 - \\alpha(Q(Qx_0))^\\top p_0 = \\nabla f(x_0)^\\top p_0 - \\alpha(Q^2x_0)^\\top p_0$. The expression $\\nabla f(x_0)^\\top p_0 + \\alpha p_0^\\top Q p_0$ is more direct.\nNumerically:\n$p_0^\\top Q p_0 = \\begin{pmatrix} -100  -1 \\end{pmatrix} \\begin{pmatrix} 100  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} -100 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -10000  -1 \\end{pmatrix} \\begin{pmatrix} -100 \\\\ -1 \\end{pmatrix} = 1000000 + 1 = 1000001$.\nSo, $\\nabla f(x_0+\\alpha p_0)^\\top p_0 = -10001 + \\alpha (1000001)$.\n\n### Exact Line Search (Baseline)\n\nFor a quadratic function, the exact step size $\\alpha_{\\text{exact}}$ that minimizes $\\phi(\\alpha) = f(x_0 + \\alpha p_0)$ can be found by setting $\\frac{d\\phi}{d\\alpha}=0$.\n$\\frac{d\\phi}{d\\alpha} = \\nabla f(x_0+\\alpha p_0)^\\top p_0 = 0$.\nUsing the expression above: $-10001 + \\alpha_{\\text{exact}} (1000001) = 0$.\nThis yields $\\alpha_{\\text{exact}} = \\frac{10001}{1000001} \\approx 0.01$. This value is the theoretical optimum and serves as our reference.\n\n### Backtracking Algorithm and Stagnation\n\nA backtracking line search starts with an initial guess $\\alpha_0$ and iteratively shrinks it by a factor $\\beta \\in (0, 1)$ until the Wolfe conditions are met. The algorithm terminates if $\\alpha$ becomes smaller than a threshold $\\alpha_{\\min}$.\n\nThe curvature condition is $|\\nabla f(x_0+\\alpha p_0)^\\top p_0| \\le c_2 |\\nabla f(x_0)^\\top p_0|$, which translates to:\n$$|-10001 + \\alpha (1000001)| \\le c_2 |-10001|$$\n$$|1 - \\alpha \\frac{1000001}{10001}| \\le c_2 \\implies |1 - \\alpha/\\alpha_{\\text{exact}}| \\le c_2$$\nThis inequality defines an interval of acceptable $\\alpha$ values around $\\alpha_{\\text{exact}}$:\n$$1 - c_2 \\le \\alpha/\\alpha_{\\text{exact}} \\le 1 + c_2$$\n$$\\alpha \\in [(1-c_2)\\alpha_{\\text{exact}}, (1+c_2)\\alpha_{\\text{exact}}]$$\n\nWith pure backtracking starting from $\\alpha_0=1$ and $\\beta=0.5$, the sequence of trial step sizes is $1, 0.5, 0.25, ..., \\alpha_k = (\\frac{1}{2})^k$. If a chosen $c_2$ is too small, the interval of acceptable $\\alpha$ values may become so narrow that no trial step $\\alpha_k$ falls into it.\nFor example, with $c_2=0.1$ and $\\alpha_{\\text{exact}} \\approx 0.01$, the acceptable interval is approximately $[0.009, 0.011]$. The backtracking sequence includes $\\alpha=0.015625$ (too high) and the next step is $\\alpha=0.0078125$ (too low). Backtracking \"jumps over\" the valid region. As $\\alpha \\to 0$, we have $|1-\\alpha/\\alpha_{\\text{exact}}| \\to 1$. The curvature condition requires $1 \\le c_2$, which is impossible since $c_2  1$. Thus, once $\\alpha$ is small enough, the curvature condition can never be satisfied. The algorithm will continue backtracking until $\\alpha  \\alpha_{\\min}$, causing stagnation. This is what Test Cases 2 and 4 demonstrate.\n\n### Auto-Tuning Strategy for Recovery\n\nTo prevent this stagnation, an adaptive strategy for $c_2$ is introduced. If the sufficient decrease condition holds but the curvature condition fails, it means the step is in a \"good\" region but the curvature constraint is too strict. The auto-tuning mechanism relaxes $c_2$ based on the observed curvature ratio $r(\\alpha) = |\\nabla f(x+\\alpha p)^\\top p| / |\\nabla f(x)^\\top p|$.\nThe update rule is:\n$$c_2^{\\mathrm{new}} = \\min\\{0.99, \\max\\{c_1+10^{-12}, r(\\alpha)+0.05\\}\\}$$\nThe new $c_2$ is guaranteed to be in $(c_1, 1)$ and is slightly larger than the currently observed ratio $r(\\alpha)$. When the curvature condition is re-checked using $c_2^{\\mathrm{new}}$, the test $r(\\alpha) \\le c_2^{\\mathrm{new}}$ is highly likely to pass, because $r(\\alpha) \\le r(\\alpha)+0.05$. This allows the algorithm to accept the current step $\\alpha$ and proceed, effectively overcoming the stagnation. Test Case 3 demonstrates this recovery. Test Case 5 shows that this mechanism does not interfere when not needed.\n\nThe following Python program implements this entire logic, computes the step sizes for all test cases, and calculates the exact step size for comparison.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a backtracking line search with strong Wolfe conditions,\n    demonstrating stagnation due to a poor c2 choice and recovery via auto-tuning.\n    \"\"\"\n    \n    # 2. Construct the concrete example\n    Q = np.diag([100.0, 1.0])\n    x0 = np.array([1.0, 1.0])\n    \n    c1 = 1e-4\n    alpha0 = 1.0\n    beta = 0.5\n    alpha_min = 1e-6\n\n    # Define the function and its gradient\n    def f(x):\n        return 0.5 * x.T @ Q @ x\n\n    def grad_f(x):\n        return Q @ x\n\n    # Calculate initial direction and constant terms\n    p0 = -grad_f(x0)\n    f_x0 = f(x0)\n    grad_f_x0_T_p0 = grad_f(x0).T @ p0\n    abs_grad_p_term = np.abs(grad_f_x0_T_p0)\n\n    # 1. Implement backtracking line search with strong Wolfe conditions\n    def backtracking_wolfe(c2_initial, auto_tune):\n        \"\"\"\n        Performs backtracking line search to find a step size alpha\n        satisfying the strong Wolfe conditions.\n        \"\"\"\n        alpha = alpha0\n        c2 = c2_initial\n\n        while alpha = alpha_min:\n            # Test sufficient decrease (Armijo) condition\n            armijo_cond_satisfied = f(x0 + alpha * p0) = f_x0 + c1 * alpha * grad_f_x0_T_p0\n            \n            if armijo_cond_satisfied:\n                # Test curvature condition\n                grad_f_x_alpha_p_T_p = grad_f(x0 + alpha * p0).T @ p0\n                abs_grad_p_alpha_term = np.abs(grad_f_x_alpha_p_T_p)\n                \n                curvature_cond_satisfied = abs_grad_p_alpha_term = c2 * abs_grad_p_term\n                \n                if curvature_cond_satisfied:\n                    return alpha # Both conditions met\n\n                # 4. Implement auto-tuning for c2\n                elif auto_tune:\n                    r_alpha = abs_grad_p_alpha_term / abs_grad_p_term\n                    c2_new = min(0.99, max(c1 + 1e-12, r_alpha + 0.05))\n                    \n                    # Re-check with the relaxed c2\n                    if abs_grad_p_alpha_term = c2_new * abs_grad_p_term:\n                        return alpha # Recovered with new c2\n\n            # If conditions fail, backtrack\n            alpha *= beta\n\n        # If loop finishes, alpha has dropped below alpha_min\n        return alpha_min\n\n    # 5. Compute the exact line search step size\n    def exact_line_search():\n        \"\"\"\n        Computes the exact optimal step size for the quadratic function.\n        \"\"\"\n        grad_fx0 = grad_f(x0)\n        # Using the formula alpha = (g'g) / (g'Hg) for steepest descent on quadratics\n        # where g = grad_f(x0) and H = Q here.\n        # This is equivalent to alpha = - (p'g) / (p'Hp) with p=-g\n        numerator = grad_fx0.T @ grad_fx0\n        denominator = grad_fx0.T @ Q @ grad_fx0\n        return numerator / denominator\n\n    # 6. Evaluate all test cases\n    test_cases = [\n        # (c2, auto_tune_enabled)\n        (0.9, False),    # Case 1: Happy path\n        (0.1, False),    # Case 2: Stagnation\n        (0.1, True),     # Case 3: Auto-tuned recovery\n        (0.001, False),  # Case 4: Severe stagnation\n        (0.9, True),     # Case 5: Redundant tuning\n    ]\n\n    results = []\n    for c2_val, tune_flag in test_cases:\n        result = backtracking_wolfe(c2_val, tune_flag)\n        results.append(result)\n\n    # Add the exact line search result\n    results.append(exact_line_search())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3143424"}]}