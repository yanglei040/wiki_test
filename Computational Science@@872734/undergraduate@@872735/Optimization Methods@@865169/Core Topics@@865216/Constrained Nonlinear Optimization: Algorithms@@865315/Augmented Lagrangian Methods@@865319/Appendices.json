{"hands_on_practices": [{"introduction": "To begin our hands-on exploration of the Augmented Lagrangian Method (ALM), we will walk through a single, complete iteration of the algorithm. This foundational exercise [@problem_id:2208360] simplifies the problem to a single dimension, allowing us to focus entirely on the two core mechanics: solving the unconstrained primal subproblem and then applying the first-order multiplier update rule. Mastering this fundamental loop is the first step toward understanding and implementing ALM for more complex problems.", "problem": "Consider the constrained optimization problem of minimizing the objective function $f(x) = x^2$ subject to the equality constraint $h(x) = x - 3 = 0$.\n\nThe augmented Lagrangian method is an iterative algorithm for solving such problems. The augmented Lagrangian function is defined as:\n$$L_A(x, \\lambda; \\rho) = f(x) + \\lambda h(x) + \\frac{\\rho}{2} [h(x)]^2$$\nwhere $\\lambda$ is the Lagrange multiplier estimate and $\\rho > 0$ is the penalty parameter.\n\nA single iteration of the method, starting from an estimate $\\lambda_k$, consists of two main steps:\n1.  Find the next iterate $x_{k+1}$ by solving the unconstrained minimization problem:\n    $$x_{k+1} = \\arg\\min_{x} L_A(x, \\lambda_k; \\rho)$$\n2.  Update the Lagrange multiplier using the formula:\n    $$\\lambda_{k+1} = \\lambda_k + \\rho h(x_{k+1})$$\n\nPerform one full iteration of the augmented Lagrangian method starting with an initial multiplier estimate $\\lambda_0 = 1$ and using a penalty parameter $\\rho = 2$. Determine the resulting values for the new iterate $x_1$ and the updated multiplier $\\lambda_1$.\n\nExpress your answer as a row matrix $\\begin{pmatrix} x_1 & \\lambda_1 \\end{pmatrix}$ using exact fractions.", "solution": "We are given $f(x) = x^2$ and $h(x) = x - 3$. The augmented Lagrangian is defined consistent with the main text as:\n$$L_{A}(x,\\lambda;\\rho) = f(x) + \\lambda h(x) + \\frac{\\rho}{2}[h(x)]^{2}$$\nWith the initial values $\\lambda_{0} = 1$ and $\\rho = 2$, the function for the first subproblem is:\n$$L_{A}(x,1;2) = x^{2} + 1\\cdot(x-3) + \\frac{2}{2}(x-3)^{2} = x^{2} + x - 3 + (x^2 - 6x + 9) = 2x^2 - 5x + 6$$\nTo find the next iterate $x_1$, we solve the unconstrained minimization problem $x_{1} = \\arg\\min_{x} L_{A}(x,1;2)$. We set the derivative to zero:\n$$\\frac{d}{dx}L_{A}(x,1;2) = 4x - 5$$\n$$4x - 5 = 0 \\implies x_{1} = \\frac{5}{4}$$\nThe second derivative is $4 > 0$, so $x_{1} = \\frac{5}{4}$ is the unique minimizer.\n\nNext, we update the multiplier using the corresponding update rule $\\lambda_{1} = \\lambda_{0} + \\rho h(x_{1})$. First, we find the constraint violation:\n$$h(x_{1}) = \\frac{5}{4} - 3 = -\\frac{7}{4}$$\nNow we perform the multiplier update:\n$$\\lambda_{1} = 1 + 2\\left(-\\frac{7}{4}\\right) = 1 - \\frac{7}{2} = -\\frac{5}{2}$$\nThus, the resulting values are $x_{1} = \\frac{5}{4}$ and $\\lambda_{1} = -\\frac{5}{2}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{5}{4} & -\\frac{5}{2} \\end{pmatrix}}$$", "id": "2208360"}, {"introduction": "Having familiarized ourselves with the basic iteration, we now turn our attention to the heart of the method: the primal minimization subproblem. In this practice [@problem_id:2208379], we will solve for the minimizer of the augmented Lagrangian in a two-dimensional space, leaving the solution in terms of the penalty parameter $\\rho$. This exercise is designed to build intuition for how the penalty term works to enforce constraints, effectively pulling the subproblem's solution closer to the feasible region as $\\rho$ increases.", "problem": "Consider the optimization problem of minimizing the function $f(x_1, x_2) = x_1^2 + x_2^2$ subject to the linear equality constraint $x_1 + x_2 - 2 = 0$.\n\nThis problem can be addressed using the Augmented Lagrangian Method. For a general optimization problem with objective function $f(x)$ and an equality constraint $h(x) = 0$, the augmented Lagrangian is defined as:\n$$L_\\rho(x, \\lambda) = f(x) + \\lambda h(x) + \\frac{\\rho}{2}[h(x)]^2$$\nwhere $\\lambda$ is the estimate of the Lagrange multiplier and $\\rho > 0$ is a positive penalty parameter.\n\nThe method involves a sequence of unconstrained minimization subproblems. Starting with an initial multiplier estimate $\\lambda_0$, one finds the vector $x^{(1)}$ that minimizes $L_\\rho(x, \\lambda_0)$.\n\nYour task is to solve this first subproblem. Given an initial Lagrange multiplier estimate $\\lambda_0 = 0$, find the vector $x^{(1)} = (x_1^{(1)}, x_2^{(1)})$ that minimizes the corresponding augmented Lagrangian. Express your answer as a row vector in terms of the penalty parameter $\\rho$.", "solution": "We are given $f(x_{1}, x_{2}) = x_{1}^{2} + x_{2}^{2}$ and the equality constraint $h(x) = x_{1} + x_{2} - 2 = 0$. The augmented Lagrangian with parameter $\\rho > 0$ is\n$$\nL_{\\rho}(x, \\lambda) = f(x) + \\lambda h(x) + \\frac{\\rho}{2}\\left[h(x)\\right]^{2}.\n$$\nWith the initial multiplier estimate $\\lambda_{0} = 0$, the first subproblem is the unconstrained minimization of\n$$\nL_{\\rho}(x, 0) = x_{1}^{2} + x_{2}^{2} + \\frac{\\rho}{2}\\left(x_{1} + x_{2} - 2\\right)^{2}.\n$$\nTo find its minimizer, set the gradient to zero. Let $s = x_{1} + x_{2} - 2$. Then\n$$\n\\frac{\\partial L_{\\rho}}{\\partial x_{1}} = 2x_{1} + \\rho s = 2x_{1} + \\rho(x_{1} + x_{2} - 2) = 0,\n$$\n$$\n\\frac{\\partial L_{\\rho}}{\\partial x_{2}} = 2x_{2} + \\rho s = 2x_{2} + \\rho(x_{1} + x_{2} - 2) = 0.\n$$\nThese yield the linear system\n$$\n(2+\\rho)x_{1} + \\rho x_{2} = 2\\rho, \\qquad \\rho x_{1} + (2+\\rho)x_{2} = 2\\rho.\n$$\nBy symmetry, the solution satisfies $x_{1} = x_{2} = t$. Substituting gives\n$$\n2t + \\rho(2t - 2) = 0 \\;\\;\\Longrightarrow\\;\\; (2 + 2\\rho)t = 2\\rho \\;\\;\\Longrightarrow\\;\\; t = \\frac{\\rho}{1+\\rho}.\n$$\nHence $x_{1}^{(1)} = x_{2}^{(1)} = \\frac{\\rho}{1+\\rho}$. The Hessian of $L_{\\rho}(x,0)$ is $2I + \\rho\\begin{pmatrix}1 & 1 \\\\ 1 & 1\\end{pmatrix}$, which is positive definite for $\\rho > 0$, so this stationary point is the unique global minimizer.\n\nTherefore,\n$$\nx^{(1)} = \\begin{pmatrix} \\frac{\\rho}{1+\\rho} & \\frac{\\rho}{1+\\rho} \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{\\rho}{1+\\rho} & \\frac{\\rho}{1+\\rho}\\end{pmatrix}}$$", "id": "2208379"}, {"introduction": "Augmented Lagrangian Methods truly shine when applied to complex, nonconvex problems where multiple local solutions may exist. This advanced practice [@problem_id:3099666] challenges you to implement the full Method of Multipliers to explore such a scenario, where the feasible set consists of two distinct points. By combining theoretical analysis with a coding implementation, you will discover how the initial choice of the Lagrange multiplier can act as a powerful tool to guide the algorithm, steering it toward a specific desired local optimum.", "problem": "Construct a complete, runnable program that implements the method of multipliers (also known as augmented Lagrangian methods) for a small nonconvex equality-constrained optimization problem, and uses it to demonstrate how different initial Lagrange multipliers can steer the algorithm to different local Karush–Kuhn–Tucker (KKT) points.\n\nYou must use the following setup.\n\n- Decision variable: $x \\in \\mathbb{R}^2$ with components $x = (x_1, x_2)$.\n- Objective function: $f(x) = \\tfrac{1}{2}(x_1^2 + x_2^2) + \\alpha x_1$, where $\\alpha = 0.02$.\n- Equality constraints $h_1(x) = 0$ and $h_2(x) = 0$ defined by:\n  - $h_1(x) = x_1^2 + x_2^2 - 1$,\n  - $h_2(x) = x_2 - m x_1$ with $m = 0.3$.\n\nFundamental base and definitions to use:\n\n- The method of multipliers is based on the augmented Lagrangian function for equality constraints, defined for multipliers $\\lambda = (\\lambda_1,\\lambda_2)$ and penalty parameter $\\rho > 0$ by\n  $$\n  \\mathcal{L}_\\rho(x,\\lambda) \\;=\\; f(x) \\;+\\; \\sum_{i=1}^2 \\lambda_i h_i(x) \\;+\\; \\frac{\\rho}{2} \\sum_{i=1}^2 h_i(x)^2.\n  $$\n- The method of multipliers generates sequences $\\{x^k\\}$ and $\\{\\lambda^k\\}$ via:\n  1. $x^{k+1}$ approximately minimizes $x \\mapsto \\mathcal{L}_\\rho(x,\\lambda^k)$,\n  2. $\\lambda^{k+1} = \\lambda^k + \\rho \\, h(x^{k+1})$, where $h(x) = (h_1(x),h_2(x))$.\n- Karush–Kuhn–Tucker (KKT) conditions for equality constraints are: there exists $\\lambda^\\star$ such that\n  $$\n  \\nabla f(x^\\star) + \\sum_{i=1}^2 \\lambda_i^\\star \\nabla h_i(x^\\star) = 0,\\quad h_1(x^\\star)=0,\\quad h_2(x^\\star)=0.\n  $$\n\nProblem tasks:\n\n1) Using only the fundamental base and definitions stated above, analyze the feasible set $\\{x \\in \\mathbb{R}^2 \\mid h_1(x)=0,\\, h_2(x)=0\\}$ to identify its structure. Show that the constraints intersect in exactly two points, and give these points explicitly in terms of $m$ and numerically for $m = 0.3$.\n\n2) Using the KKT conditions, argue that both intersection points are KKT points for the equality-constrained problem with the given $f$, and write down the linear system that determines the associated multipliers $(\\lambda_1^\\star,\\lambda_2^\\star)$ at each point. You do not need to compute the numeric values of the multipliers, but you must justify that finite solutions exist.\n\n3) Explain, from first principles, why the initial value of the second multiplier $\\lambda_2^0$ can bias the descent direction of the inner minimization $x \\mapsto \\mathcal{L}_\\rho(x,\\lambda^k)$ near a symmetric initial point $x^0=(0,0)$, thereby guiding the method of multipliers toward one or the other feasible KKT point. Your reasoning must start from the gradient formula for $\\mathcal{L}_\\rho$ and properties of $h_2$.\n\n4) Implement the method of multipliers with a fixed penalty parameter $\\rho$ and an inner unconstrained minimization of $\\mathcal{L}_\\rho(\\cdot,\\lambda^k)$ at each outer iteration using a standard smooth unconstrained solver. Use the gradients of $f$, $h_1$, and $h_2$ to provide the exact gradient of $\\mathcal{L}_\\rho$ to the solver. Use the stopping criterion that the constraint residual norm $\\|h(x^{k+1})\\|_2$ is less than $10^{-8}$ or a maximum number of outer iterations is reached. Use initial point $x^0=(0,0)$ and fixed $\\rho$.\n\n5) Test suite. Run your implementation for the following four test cases, with $x^0=(0,0)$ and $\\rho=40$:\n   - Case A (happy path, strong positive bias): $\\lambda^0 = (0, +3)$.\n   - Case B (happy path, strong negative bias): $\\lambda^0 = (0, -3)$.\n   - Case C (neutral multiplier baseline): $\\lambda^0 = (0, 0)$.\n   - Case D (edge condition: bias in the first multiplier only): $\\lambda^0 = (+5, 0)$.\n\nFor each case, after convergence, identify which KKT point the algorithm reached by the sign of the first coordinate $x_1^\\star$: return $+1$ if $x_1^\\star \\ge 0$ and $-1$ if $x_1^\\star < 0$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each result is the integer $+1$ or $-1$ for the corresponding test case in the order A, B, C, D.", "solution": "The problem statement is a valid exercise in numerical optimization. It is scientifically grounded, well-posed, objective, and contains all necessary information to proceed with a solution. We will address the problem tasks sequentially.\n\nThe problem is to solve the equality-constrained optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^2} f(x) \\quad \\text{subject to} \\quad h_1(x)=0, \\; h_2(x)=0,\n$$\nwhere the decision variable is $x=(x_1, x_2)$, the objective function is $f(x) = \\frac{1}{2}(x_1^2 + x_2^2) + \\alpha x_1$ with $\\alpha = 0.02$, and the constraints are $h_1(x) = x_1^2 + x_2^2 - 1$ and $h_2(x) = x_2 - m x_1$ with $m = 0.3$. The solution will be found using the method of multipliers.\n\n**1. Analysis of the Feasible Set**\n\nThe feasible set is the collection of all points $x \\in \\mathbb{R}^2$ that satisfy both constraint equations simultaneously:\n$$\n\\begin{cases}\nh_1(x) = x_1^2 + x_2^2 - 1 = 0 \\\\\nh_2(x) = x_2 - m x_1 = 0\n\\end{cases}\n$$\nThe first equation, $x_1^2 + x_2^2 = 1$, describes the unit circle centered at the origin. The second equation, $x_2 = m x_1$, describes a line passing through the origin with slope $m$. The feasible set is the intersection of this circle and this line.\n\nTo find the intersection points, we substitute the expression for $x_2$ from the second equation into the first:\n$$\nx_1^2 + (m x_1)^2 = 1\n$$\nFactoring out $x_1^2$ yields:\n$$\nx_1^2 (1 + m^2) = 1\n$$\nSolving for $x_1^2$, we get $x_1^2 = \\frac{1}{1 + m^2}$, which gives two solutions for $x_1$:\n$$\nx_1 = \\pm \\frac{1}{\\sqrt{1+m^2}}\n$$\nWe find the corresponding $x_2$ coordinate for each solution using $x_2 = m x_1$:\n$$\nx_2 = \\pm \\frac{m}{\\sqrt{1+m^2}}\n$$\nThus, there are exactly two feasible points, which we denote $x_A^\\star$ and $x_B^\\star$:\n$$\nx_A^\\star = \\left( \\frac{1}{\\sqrt{1+m^2}}, \\frac{m}{\\sqrt{1+m^2}} \\right) \\quad \\text{and} \\quad x_B^\\star = \\left( -\\frac{1}{\\sqrt{1+m^2}}, -\\frac{m}{\\sqrt{1+m^2}} \\right)\n$$\nFor the given value $m = 0.3$, we have $1+m^2 = 1+(0.3)^2 = 1.09$. The numerical values of the two feasible points are:\n$$\nx_A^\\star \\approx \\left( \\frac{1}{\\sqrt{1.09}}, \\frac{0.3}{\\sqrt{1.09}} \\right) \\approx (0.957826, 0.287348)\n$$\n$$\nx_B^\\star \\approx \\left( -\\frac{1}{\\sqrt{1.09}}, -\\frac{0.3}{\\sqrt{1.09}} \\right) \\approx (-0.957826, -0.287348)\n$$\n\n**2. Karush–Kuhn–Tucker (KKT) Point Verification**\n\nFor an equality-constrained problem, a point $x^\\star$ is a KKT point if it is feasible and there exist Lagrange multipliers $\\lambda^\\star = (\\lambda_1^\\star, \\lambda_2^\\star)$ such that the gradient of the Lagrangian is zero:\n$$\n\\nabla f(x^\\star) + \\sum_{i=1}^2 \\lambda_i^\\star \\nabla h_i(x^\\star) = 0\n$$\nWe have already shown that $x_A^\\star$ and $x_B^\\star$ are feasible. We now compute the necessary gradients:\n- $\\nabla f(x) = \\begin{pmatrix} x_1 + \\alpha \\\\ x_2 \\end{pmatrix}$\n- $\\nabla h_1(x) = \\begin{pmatrix} 2x_1 \\\\ 2x_2 \\end{pmatrix}$\n- $\\nabla h_2(x) = \\begin{pmatrix} -m \\\\ 1 \\end{pmatrix}$\n\nThe KKT stationarity condition can be written as a linear system for the multipliers $\\lambda^\\star$:\n$$\n\\nabla h_1(x^\\star) \\lambda_1^\\star + \\nabla h_2(x^\\star) \\lambda_2^\\star = - \\nabla f(x^\\star)\n$$\nIn matrix form, this is $J_h(x^\\star)^T \\lambda^\\star = -\\nabla f(x^\\star)$, where $J_h(x^\\star)$ is the Jacobian of the constraint functions:\n$$\n\\begin{pmatrix} 2x_1^\\star & -m \\\\ 2x_2^\\star & 1 \\end{pmatrix} \\begin{pmatrix} \\lambda_1^\\star \\\\ \\lambda_2^\\star \\end{pmatrix} = -\\begin{pmatrix} x_1^\\star + \\alpha \\\\ x_2^\\star \\end{pmatrix}\n$$\nA unique solution for $(\\lambda_1^\\star, \\lambda_2^\\star)$ exists if the matrix on the left is invertible, which is true if its determinant is non-zero. The determinant is:\n$$\n\\det\\begin{pmatrix} 2x_1^\\star & -m \\\\ 2x_2^\\star & 1 \\end{pmatrix} = (2x_1^\\star)(1) - (-m)(2x_2^\\star) = 2x_1^\\star + 2mx_2^\\star\n$$\nAt any feasible point, we know $x_2^\\star = m x_1^\\star$. Substituting this into the determinant expression:\n$$\n2x_1^\\star + 2m(m x_1^\\star) = 2x_1^\\star (1+m^2)\n$$\nFrom part 1, we know $x_1^\\star = \\pm 1/\\sqrt{1+m^2}$. Since $m=0.3$, $1+m^2 \\neq 0$, and thus $x_1^\\star \\neq 0$ for both feasible points $x_A^\\star$ and $x_B^\\star$. Therefore, the determinant $2x_1^\\star(1+m^2)$ is non-zero at both points. This guarantees that for each of the two feasible points, there exists a unique pair of Lagrange multipliers satisfying the KKT conditions. Consequently, both $x_A^\\star$ and $x_B^\\star$ are KKT points.\n\n**3. Bias from Initial Multipliers**\n\nThe method of multipliers iteratively solves an unconstrained subproblem of the form $\\min_x \\mathcal{L}_\\rho(x, \\lambda^k)$. The initial behavior of the algorithm is determined by the search direction for the first subproblem, starting from $x^0=(0,0)$ with an initial multiplier vector $\\lambda^0=(\\lambda_1^0, \\lambda_2^0)$. This direction is the negative gradient of the augmented Lagrangian, $-\\nabla_x \\mathcal{L}_\\rho(x^0, \\lambda^0)$.\n\nThe gradient of the augmented Lagrangian $\\mathcal{L}_\\rho(x, \\lambda) = f(x) + \\lambda^T h(x) + \\frac{\\rho}{2} \\|h(x)\\|_2^2$ is:\n$$\n\\nabla_x \\mathcal{L}_\\rho(x, \\lambda) = \\nabla f(x) + \\sum_{i=1}^2 \\lambda_i \\nabla h_i(x) + \\rho \\sum_{i=1}^2 h_i(x) \\nabla h_i(x)\n$$\nWe evaluate the components at the initial point $x^0 = (0,0)$:\n- $\\nabla f(x^0) = (\\alpha, 0)^T = (0.02, 0)^T$\n- $h_1(x^0) = 0^2 + 0^2 - 1 = -1$\n- $h_2(x^0) = 0 - m(0) = 0$\n- $\\nabla h_1(x^0) = (2(0), 2(0))^T = (0, 0)^T$\n- $\\nabla h_2(x^0) = (-m, 1)^T = (-0.3, 1)^T$\n\nSubstituting these into the gradient formula:\n$$\n\\nabla_x \\mathcal{L}_\\rho(x^0, \\lambda^0) = \\nabla f(x^0) + \\lambda_1^0 \\nabla h_1(x^0) + \\lambda_2^0 \\nabla h_2(x^0) + \\rho h_1(x^0) \\nabla h_1(x^0) + \\rho h_2(x^0) \\nabla h_2(x^0)\n$$\n$$\n\\nabla_x \\mathcal{L}_\\rho(x^0, \\lambda^0) = \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix} + \\lambda_1^0 \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\lambda_2^0 \\begin{pmatrix} -m \\\\ 1 \\end{pmatrix} + \\rho(-1) \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\rho(0) \\begin{pmatrix} -m \\\\ 1 \\end{pmatrix}\n$$\nThe terms involving $\\nabla h_1(x^0)$ are zero. Simplifying, we obtain:\n$$\n\\nabla_x \\mathcal{L}_\\rho(x^0, \\lambda^0) = \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix} + \\lambda_2^0 \\begin{pmatrix} -m \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\alpha - m\\lambda_2^0 \\\\ \\lambda_2^0 \\end{pmatrix}\n$$\nThe initial search direction is $d^0 = -\\nabla_x \\mathcal{L}_\\rho(x^0, \\lambda^0) = (m\\lambda_2^0 - \\alpha, -\\lambda_2^0)^T$. The two KKT points lie in opposite half-planes defined by $x_1=0$. The sign of the first component of $d^0$, which is $m\\lambda_2^0 - \\alpha$, determines the initial movement along the $x_1$-axis.\n- If $\\lambda_2^0$ is a large positive number (e.g., $\\lambda_2^0 = 3$), the first component of the direction, $m\\lambda_2^0 - \\alpha = 0.3 \\times 3 - 0.02 = 0.88$, is positive. The algorithm initially moves towards positive $x_1$, biasing it toward the KKT point $x_A^\\star$ where $x_1 > 0$.\n- If $\\lambda_2^0$ is a large negative number (e.g., $\\lambda_2^0 = -3$), the first component, $m\\lambda_2^0 - \\alpha = 0.3 \\times (-3) - 0.02 = -0.92$, is negative. The algorithm initially moves towards negative $x_1$, biasing it toward the KKT point $x_B^\\star$ where $x_1  0$.\n- If $\\lambda_2^0 = 0$, the first component is simply $-\\alpha = -0.02$, which is negative. This is because the objective function $f(x)$ itself is biased toward negative $x_1$. This biases the search towards $x_B^\\star$. Notably, the initial multiplier $\\lambda_1^0$ has no influence on the initial direction because $\\nabla h_1(x^0) = 0$.\n\nThis analysis demonstrates that the initial choice of $\\lambda_2^0$ directly controls the initial search direction from the origin, thereby steering the algorithm to one of the two distinct local solutions.\n\n**4. Algorithm Implementation and Testing**\n\nThe method of multipliers is implemented as an outer loop that updates the Lagrange multipliers $\\lambda^k$. Inside this loop, an unconstrained minimization subproblem for $x^{k+1}$ is solved using a quasi-Newton method (`BFGS` from `scipy.optimize.minimize`), which is well-suited for smooth, unconstrained problems. We provide the solver with the exact analytical gradient of the augmented Lagrangian, $\\nabla_x \\mathcal{L}_\\rho(x, \\lambda^k)$, for efficiency and accuracy. The outer loop begins with the specified initial values $x^0=(0,0)$ and $\\lambda^0$ for each test case, and a fixed penalty parameter $\\rho=40$. The loop terminates when the constraint violation, measured by $\\|h(x^{k+1})\\|_2$, falls below a tolerance of $10^{-8}$. The final converged point $x^\\star$ is then classified based on the sign of its first component, $x_1^\\star$. The code implementing this logic is provided in the final answer.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Implements the method of multipliers for a nonconvex equality-constrained problem\n    and demonstrates how different initial Lagrange multipliers steer the algorithm\n    to different local KKT points.\n    \"\"\"\n\n    # --- Problem Definition ---\n    ALPHA = 0.02\n    M = 0.3\n    \n    def f(x):\n        \"\"\"Objective function.\"\"\"\n        return 0.5 * (x[0]**2 + x[1]**2) + ALPHA * x[0]\n\n    def grad_f(x):\n        \"\"\"Gradient of the objective function.\"\"\"\n        return np.array([x[0] + ALPHA, x[1]])\n\n    def h(x):\n        \"\"\"Constraint functions vector h(x).\"\"\"\n        h1 = x[0]**2 + x[1]**2 - 1.0\n        h2 = x[1] - M * x[0]\n        return np.array([h1, h2])\n\n    def grad_h(x):\n        \"\"\"Jacobian of the constraint functions, returned as a list of gradients.\"\"\"\n        grad_h1 = np.array([2.0 * x[0], 2.0 * x[1]])\n        grad_h2 = np.array([-M, 1.0])\n        return [grad_h1, grad_h2]\n\n    # --- Augmented Lagrangian Method Implementation ---\n    def augmented_lagrangian_solver(x0, lambda0, rho, max_outer_iter=100, tol=1e-8):\n        \"\"\"\n        Solves the constrained optimization problem using the method of multipliers.\n\n        Args:\n            x0 (np.ndarray): Initial guess for the decision variables x.\n            lambda0 (np.ndarray): Initial guess for the Lagrange multipliers.\n            rho (float): Penalty parameter.\n            max_outer_iter (int): Maximum number of outer loop iterations.\n            tol (float): Tolerance for constraint violation norm.\n\n        Returns:\n            np.ndarray: The solution vector x.\n        \"\"\"\n        x_k = np.copy(x0)\n        lambda_k = np.copy(lambda0)\n\n        for k in range(max_outer_iter):\n            # Define the augmented Lagrangian and its gradient for the current lambda_k\n            def L_rho(x):\n                h_x = h(x)\n                return f(x) + np.dot(lambda_k, h_x) + (rho / 2.0) * np.dot(h_x, h_x)\n\n            def grad_L_rho(x):\n                h_x = h(x)\n                grads_h_x = grad_h(x)\n                \n                # Gradient of penalty term section\n                penalty_grad_term = np.zeros(2)\n                for i in range(2):\n                    penalty_grad_term += h_x[i] * grads_h_x[i]\n                \n                # Gradient of lambda term section\n                lambda_grad_term = np.zeros(2)\n                for i in range(2):\n                    lambda_grad_term += lambda_k[i] * grads_h_x[i]\n\n                return grad_f(x) + lambda_grad_term + rho * penalty_grad_term\n\n            # Solve the unconstrained subproblem\n            # Start the minimization from the previous iterate x_k\n            res = minimize(L_rho, x_k, method='BFGS', jac=grad_L_rho)\n            x_k_plus_1 = res.x\n\n            # Check for convergence\n            h_next = h(x_k_plus_1)\n            constraint_residual = np.linalg.norm(h_next)\n            \n            if constraint_residual  tol:\n                return x_k_plus_1\n            \n            # Update multipliers\n            lambda_k = lambda_k + rho * h_next\n            \n            # Update x for the next iteration\n            x_k = x_k_plus_1\n            \n        return x_k\n\n    # --- Test Suite ---\n    x0 = np.array([0.0, 0.0])\n    rho = 40.0\n\n    test_cases = [\n        # Case A: Strong positive bias\n        np.array([0.0, 3.0]),\n        # Case B: Strong negative bias\n        np.array([0.0, -3.0]),\n        # Case C: Neutral multiplier baseline\n        np.array([0.0, 0.0]),\n        # Case D: Bias in the first multiplier only\n        np.array([5.0, 0.0]),\n    ]\n\n    results = []\n    for lambda0_case in test_cases:\n        x_star = augmented_lagrangian_solver(x0, lambda0_case, rho)\n        \n        # Identify which KKT point was reached\n        if x_star[0] >= 0:\n            results.append(1)\n        else:\n            results.append(-1)\n            \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3099666"}]}