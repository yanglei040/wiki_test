{"hands_on_practices": [{"introduction": "Mastering any algorithm begins with understanding its core loop. This first practice problem provides a step-by-step walkthrough of a single, standard iteration of the Wolfe method, framed as a \"debugging checklist\" that solidifies the essential calculations [@problem_id:3198861]. You will practice constructing the null-space basis $\\mathbf{Z}$ for the active-constraint set, solving the reduced system to find a search direction, and determining how far you can move along that direction before hitting a new constraint.", "problem": "Consider the convex quadratic program with objective function $f(\\mathbf{x}) = \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{Q}\\,\\mathbf{x} + \\mathbf{c}^{\\top}\\mathbf{x}$ and linear inequality constraints $\\mathbf{A}\\mathbf{x} \\le \\mathbf{b}$, where\n$$\n\\mathbf{Q} = \\begin{pmatrix}\n4  2 \\\\\n2  2\n\\end{pmatrix},\\quad\n\\mathbf{c} = \\begin{pmatrix}\n-2 \\\\\n-6\n\\end{pmatrix},\\quad\n\\mathbf{A} = \\begin{pmatrix}\n1  0 \\\\\n1  1 \\\\\n-1  2\n\\end{pmatrix},\\quad\n\\mathbf{b} = \\begin{pmatrix}\n2 \\\\\n2 \\\\\n2\n\\end{pmatrix}.\n$$\nYou will apply the “debugging” checklist for Wolfe’s method for quadratic programming, starting from the feasible point $\\mathbf{x}^{(0)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ with the initial working set $\\mathcal{W}^{(0)}$ consisting of the single active constraint given by the second row of $\\mathbf{A}$, so that the working-set equality is $\\mathbf{a}_{2}^{\\top}\\mathbf{x} = b_{2}$.\n\nFollow these steps, justifying each from first principles appropriate to optimization methods: verify primal feasibility $\\mathbf{A}\\mathbf{x}^{(0)} \\le \\mathbf{b}$ and compute residuals $r_{i} = b_{i} - \\mathbf{a}_{i}^{\\top}\\mathbf{x}^{(0)}$, construct a null-space basis $\\mathbf{Z}$ for the active constraints so that $\\mathbf{A}_{\\mathcal{W}^{(0)}}\\mathbf{Z} = \\mathbf{0}$, form and solve the reduced system determining the search direction $\\mathbf{p} = \\mathbf{Z}\\mathbf{d}$ on the current face of the feasible region, and then compute the intersection step length $\\alpha$ at which the next constraint becomes active along $\\mathbf{x}(\\alpha) = \\mathbf{x}^{(0)} + \\alpha\\,\\mathbf{p}$.\n\nYour task is to carry out these steps for the given data and report the exact value of the intersection step length $\\alpha$ at which the next constraint enters the working set. Express your final answer as an exact fraction. No rounding is required.", "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and objective.\n\n### Step 1: Extract Givens\nThe given data for the convex quadratic program are:\n-   Objective function: $f(\\mathbf{x}) = \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{Q}\\,\\mathbf{x} + \\mathbf{c}^{\\top}\\mathbf{x}$\n-   Hessian matrix of the quadratic term: $\\mathbf{Q} = \\begin{pmatrix} 4  2 \\\\ 2  2 \\end{pmatrix}$\n-   Linear term vector: $\\mathbf{c} = \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix}$\n-   Constraint matrix: $\\mathbf{A} = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ -1  2 \\end{pmatrix}$\n-   Constraint bounds vector: $\\mathbf{b} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix}$\n-   Constraint type: $\\mathbf{A}\\mathbf{x} \\le \\mathbf{b}$\n-   Initial feasible point: $\\mathbf{x}^{(0)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n-   Initial working set: $\\mathcal{W}^{(0)}$, consisting of the single active constraint from the second row of $\\mathbf{A}$, so that the working-set equality is $\\mathbf{a}_{2}^{\\top}\\mathbf{x} = b_{2}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Soundness**: The problem describes a standard quadratic program (QP). To ensure convexity, the Hessian matrix $\\mathbf{Q}$ must be positive semi-definite. The principal minors of $\\mathbf{Q}$ are $\\det(Q_{11}) = 4  0$ and $\\det(\\mathbf{Q}) = 4 \\times 2 - 2 \\times 2 = 8 - 4 = 4  0$. Since all leading principal minors are positive, $\\mathbf{Q}$ is positive definite, which is a stronger condition. A QP with a positive definite Hessian is strictly convex, guaranteeing a unique minimum. The problem is thus mathematically sound.\n2.  **Well-Posed**: The problem is well-posed. It provides all necessary data and a clear set of instructions for the first iteration of an active-set method. The objective and constraints are clearly defined.\n3.  **Objective**: The language is precise and unbiased.\n4.  **Completeness**: All required matrices, vectors, and the initial state are provided.\nThe problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution\nThe task is to perform one iteration of an active-set method for the given quadratic program, starting from the point $\\mathbf{x}^{(0)}$ with the specified working set $\\mathcal{W}^{(0)}$.\n\n**1. Verify Primal Feasibility and Compute Residuals**\nFirst, we verify that the starting point $\\mathbf{x}^{(0)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ is feasible by checking if it satisfies $\\mathbf{A}\\mathbf{x}^{(0)} \\le \\mathbf{b}$.\n$$\n\\mathbf{A}\\mathbf{x}^{(0)} = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ -1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 1 \\\\ 1 \\cdot 1 + 1 \\cdot 1 \\\\ -1 \\cdot 1 + 2 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}\n$$\nWe compare this with $\\mathbf{b} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix}$.\n-   Constraint 1: $1 \\le 2$ (Satisfied, inactive)\n-   Constraint 2: $2 \\le 2$ (Satisfied, active)\n-   Constraint 3: $1 \\le 2$ (Satisfied, inactive)\nThe point $\\mathbf{x}^{(0)}$ is feasible. The second constraint is active, which is consistent with the problem statement that $\\mathcal{W}^{(0)} = \\{2\\}$.\n\nThe residuals $r_i = b_i - \\mathbf{a}_{i}^{\\top}\\mathbf{x}^{(0)}$ are:\n-   $r_1 = b_1 - \\mathbf{a}_{1}^{\\top}\\mathbf{x}^{(0)} = 2 - 1 = 1$\n-   $r_2 = b_2 - \\mathbf{a}_{2}^{\\top}\\mathbf{x}^{(0)} = 2 - 2 = 0$\n-   $r_3 = b_3 - \\mathbf{a}_{3}^{\\top}\\mathbf{x}^{(0)} = 2 - 1 = 1$\n\n**2. Construct Null-Space Basis $\\mathbf{Z}$**\nThe working set $\\mathcal{W}^{(0)} = \\{2\\}$ defines the active constraint matrix $\\mathbf{A}_{\\mathcal{W}^{(0)}}$. This is simply the second row of $\\mathbf{A}$:\n$$\n\\mathbf{A}_{\\mathcal{W}^{(0)}} = \\mathbf{a}_{2}^{\\top} = \\begin{pmatrix} 1  1 \\end{pmatrix}\n$$\nThe null-space basis $\\mathbf{Z}$ is a matrix whose columns span the null space of $\\mathbf{A}_{\\mathcal{W}^{(0)}}$. We need to find vectors $\\mathbf{z}$ such that $\\mathbf{A}_{\\mathcal{W}^{(0)}} \\mathbf{z} = \\mathbf{0}$.\n$$\n\\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix} = z_1 + z_2 = 0\n$$\nThis implies $z_1 = -z_2$. The null space is one-dimensional. A basis for this null space can be chosen as:\n$$\n\\mathbf{Z} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}\n$$\n\n**3. Form and Solve the Reduced System**\nThe search direction $\\mathbf{p}$ lies in the null space of the active constraints, so we write $\\mathbf{p} = \\mathbf{Z}\\mathbf{d}$. The vector $\\mathbf{d}$ is found by solving the reduced system. First, we compute the gradient of the objective function at $\\mathbf{x}^{(0)}$:\n$$\n\\mathbf{g} = \\mathbf{g}(\\mathbf{x}^{(0)}) = \\mathbf{Q}\\mathbf{x}^{(0)} + \\mathbf{c} = \\begin{pmatrix} 4  2 \\\\ 2  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ -2 \\end{pmatrix}\n$$\nThe reduced system is $(\\mathbf{Z}^{\\top}\\mathbf{Q}\\mathbf{Z})\\mathbf{d} = -\\mathbf{Z}^{\\top}\\mathbf{g}$. We compute the components:\n-   The reduced Hessian:\n$$\n\\mathbf{Z}^{\\top}\\mathbf{Q}\\mathbf{Z} = \\begin{pmatrix} -1  1 \\end{pmatrix} \\begin{pmatrix} 4  2 \\\\ 2  2 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1  1 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} = 2\n$$\n-   The reduced gradient:\n$$\n\\mathbf{Z}^{\\top}\\mathbf{g} = \\begin{pmatrix} -1  1 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -2 \\end{pmatrix} = -4 - 2 = -6\n$$\nThe reduced system is a scalar equation in this case:\n$$\n2\\mathbf{d} = -(-6) = 6\n$$\nSolving for $\\mathbf{d}$ gives $\\mathbf{d} = 3$. The search direction $\\mathbf{p}$ is then:\n$$\n\\mathbf{p} = \\mathbf{Z}\\mathbf{d} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} (3) = \\begin{pmatrix} -3 \\\\ 3 \\end{pmatrix}\n$$\n\n**4. Compute the Intersection Step Length $\\alpha$**\nThe next point is given by $\\mathbf{x}(\\alpha) = \\mathbf{x}^{(0)} + \\alpha\\mathbf{p}$. We need to find the maximum step length $\\alpha  0$ that maintains feasibility for the inactive constraints ($i=1$ and $i=3$). A constraint $i$ remains feasible if $\\mathbf{a}_{i}^{\\top}\\mathbf{x}(\\alpha) \\le b_i$.\n$$\n\\mathbf{a}_{i}^{\\top}(\\mathbf{x}^{(0)} + \\alpha\\mathbf{p}) \\le b_i \\implies \\mathbf{a}_{i}^{\\top}\\mathbf{x}^{(0)} + \\alpha(\\mathbf{a}_{i}^{\\top}\\mathbf{p}) \\le b_i \\implies \\alpha(\\mathbf{a}_{i}^{\\top}\\mathbf{p}) \\le b_i - \\mathbf{a}_{i}^{\\top}\\mathbf{x}^{(0)}\n$$\nWe only need to consider constraints for which $\\mathbf{a}_{i}^{\\top}\\mathbf{p}  0$, as these are the ones that can become active as $\\alpha$ increases.\n\n-   For inactive constraint $i=1$:\n$$\n\\mathbf{a}_{1}^{\\top}\\mathbf{p} = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} -3 \\\\ 3 \\end{pmatrix} = -3\n$$\nSince $\\mathbf{a}_{1}^{\\top}\\mathbf{p}  0$, this constraint will not block the step for $\\alpha  0$.\n\n-   For inactive constraint $i=3$:\n$$\n\\mathbf{a}_{3}^{\\top}\\mathbf{p} = \\begin{pmatrix} -1  2 \\end{pmatrix} \\begin{pmatrix} -3 \\\\ 3 \\end{pmatrix} = (-1)(-3) + (2)(3) = 3 + 6 = 9\n$$\nSince $\\mathbf{a}_{3}^{\\top}\\mathbf{p}  0$, this constraint limits the step length. The maximum $\\alpha$ is found when the inequality becomes an equality:\n$$\n\\alpha = \\frac{b_3 - \\mathbf{a}_{3}^{\\top}\\mathbf{x}^{(0)}}{\\mathbf{a}_{3}^{\\top}\\mathbf{p}} = \\frac{r_3}{\\mathbf{a}_{3}^{\\top}\\mathbf{p}} = \\frac{1}{9}\n$$\nThe step to the minimizer along the current search direction is $\\alpha=1$. The intersection step length is the step at which a new constraint becomes active. This is the minimum of the positive values calculated, which in this case is only $\\frac{1}{9}$.\nThus, the step length $\\alpha$ at which the next constraint (constraint 3) enters the working set is $\\frac{1}{9}$.", "answer": "$$\n\\boxed{\\frac{1}{9}}\n$$", "id": "3198861"}, {"introduction": "Real-world optimization problems often present challenges that go beyond the textbook case, such as a rank-deficient working set where active constraints become linearly dependent. This exercise demonstrates how to diagnose and resolve this situation by pruning the working set until the active constraints are linearly independent, ensuring the underlying Karush-Kuhn-Tucker (KKT) system is well-defined and solvable [@problem_id:3198856]. This is a crucial skill for understanding the theoretical underpinnings and practical implementation of active-set methods.", "problem": "Consider the Quadratic Programming (QP) problem of minimizing the quadratic objective subject to linear inequality constraints, in the setting of Wolfe's method (active-set method). The objective function is\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\;\\; \\frac{1}{2}\\, x^{\\top} H x + c^{\\top} x,\n$$\nwhere\n$$\nH = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix}, \\qquad c = \\begin{pmatrix} -4 \\\\ -2 \\end{pmatrix}.\n$$\nThe inequality constraints are collected as\n$$\nA x \\ge b, \\quad \\text{with} \\quad\nA = \\begin{pmatrix}\n1  1 \\\\\n1  -1 \\\\\n2  2\n\\end{pmatrix}, \\qquad\nb = \\begin{pmatrix}\n3 \\\\\n1 \\\\\n6\n\\end{pmatrix}.\n$$\nSuppose that, at a feasible iterate $x^{(k)}$, Wolfe's method currently uses the working set $W = \\{1,2,3\\}$, so that the active-constraint matrix and right-hand side are\n$$\nA_{W} = \\begin{pmatrix}\n1  1 \\\\\n1  -1 \\\\\n2  2\n\\end{pmatrix}, \\qquad b_{W} = \\begin{pmatrix} 3 \\\\ 1 \\\\ 6 \\end{pmatrix}.\n$$\nStarting from the fundamental definitions of rank and the Karush–Kuhn–Tucker (KKT) first-order conditions for equality-constrained subproblems, do the following:\n\n1. Detect whether $A_{W}$ is rank-deficient. If it is, prune constraints from $W$ until the remaining working set satisfies $\\operatorname{rank}(A_{W}) = |W|$, justifying your choice using linear dependence among the rows.\n\n2. Using the pruned working set, form and solve the equality-constrained KKT system to obtain the unique minimizer of the corresponding equality-constrained subproblem.\n\nWhat is the exact value of the objective function at this minimizer? Provide the exact value; do not round.", "solution": "The problem is valid as it is self-contained, scientifically grounded in the theory of quadratic programming, and unambiguously posed.\n\nWe are asked to analyze a specific state in Wolfe's method for a given quadratic programming (QP) problem. The objective function to be minimized is $f(x) = \\frac{1}{2} x^{\\top} H x + c^{\\top} x$, where $x \\in \\mathbb{R}^{2}$, and the matrices are given as:\n$$\nH = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix}, \\qquad c = \\begin{pmatrix} -4 \\\\ -2 \\end{pmatrix}\n$$\nThe linear inequality constraints are $A x \\ge b$, with:\n$$\nA = \\begin{pmatrix}\n1  1 \\\\\n1  -1 \\\\\n2  2\n\\end{pmatrix}, \\qquad\nb = \\begin{pmatrix}\n3 \\\\\n1 \\\\\n6\n\\end{pmatrix}\n$$\nAt the current iterate, the working set of active constraints is given as $W = \\{1, 2, 3\\}$. The corresponding active-constraint matrix and right-hand side vector are:\n$$\nA_{W} = \\begin{pmatrix}\n1  1 \\\\\n1  -1 \\\\\n2  2\n\\end{pmatrix}, \\qquad b_{W} = \\begin{pmatrix} 3 \\\\ 1 \\\\ 6 \\end{pmatrix}\n$$\nThe number of constraints in the working set is $|W|=3$.\n\n**Part 1: Rank-Deficiency and Pruning of the Working Set**\n\nFirst, we must determine if the matrix of active constraints, $A_{W}$, is rank-deficient. In the context of active-set methods, this typically means checking if the rows of $A_{W}$ are linearly independent. The condition required for the KKT subproblem to be well-defined with a unique Lagrange multiplier vector is that the rows of the active-constraint matrix are linearly independent. This is equivalent to the columns of $A_W^\\top$ being linearly independent, which implies that the number of active constraints cannot exceed the number of variables, i.e., $|W| \\le n$. When $|W|  n$, the rows are necessarily linearly dependent. Here, $|W|=3$ and $n=2$, so the rows must be linearly dependent.\n\nLet the rows of $A_W$ be denoted by $r_1, r_2, r_3$:\n$$\nr_1 = \\begin{pmatrix} 1  1 \\end{pmatrix} \\\\\nr_2 = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\\\\nr_3 = \\begin{pmatrix} 2  2 \\end{pmatrix}\n$$\nWe can observe a linear dependency between these rows. Specifically, row $r_3$ is a scalar multiple of row $r_1$:\n$$\nr_3 = 2 \\cdot r_1 \\quad \\text{since} \\quad \\begin{pmatrix} 2  2 \\end{pmatrix} = 2 \\cdot \\begin{pmatrix} 1  1 \\end{pmatrix}\n$$\nThis confirms that the rows of $A_W$ are linearly dependent. The rank of $A_W$ is the number of linearly independent rows. Since $r_1$ and $r_2$ are not parallel (i.e., not scalar multiples of each other), they are linearly independent. Thus, the rank of $A_W$ is $2$.\nWe have $\\operatorname{rank}(A_{W}) = 2$ and $|W|=3$. The condition $\\operatorname{rank}(A_{W}) = |W|$ is not satisfied.\n\nTo resolve this, we must prune the working set $W$ by removing constraints until the rows of the new active-constraint matrix are linearly independent. The linear dependency is $r_3 - 2r_1 = 0$, which involves constraints $1$ and $3$. Removing either constraint $1$ or $3$ will break this specific dependency. Let's remove constraint $3$ from the working set $W$. The new, pruned working set is $W' = \\{1, 2\\}$.\n\nThe corresponding active-constraint matrix and right-hand side are:\n$$\nA_{W'} = \\begin{pmatrix}\n1  1 \\\\\n1  -1\n\\end{pmatrix}, \\qquad b_{W'} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n$$\nThe number of constraints in this new working set is $|W'| = 2$. The rows of $A_{W'}$ are $r_1 = \\begin{pmatrix} 1  1 \\end{pmatrix}$ and $r_2 = \\begin{pmatrix} 1  -1 \\end{pmatrix}$. To check for linear independence, we compute the determinant of $A_{W'}$:\n$$\n\\det(A_{W'}) = (1)(-1) - (1)(1) = -1 - 1 = -2\n$$\nSince $\\det(A_{W'}) \\neq 0$, the rows are linearly independent. Therefore, $\\operatorname{rank}(A_{W'}) = 2$.\nThe condition $\\operatorname{rank}(A_{W'}) = |W'|$ is now satisfied, as $2=2$.\n\n**Part 2: Solving the Equality-Constrained KKT Subproblem**\n\nWith the pruned working set $W'$, we now solve the equality-constrained subproblem:\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\;\\; \\frac{1}{2}\\, x^{\\top} H x + c^{\\top} x \\quad \\text{subject to} \\quad A_{W'} x = b_{W'}\n$$\nThe Karush–Kuhn–Tucker (KKT) conditions for this subproblem state that there must exist a vector of Lagrange multipliers $\\lambda \\in \\mathbb{R}^{2}$ such that:\n$$\n\\nabla_x (\\frac{1}{2}\\, x^{\\top} H x + c^{\\top} x + \\lambda^{\\top} (b_{W'} - A_{W'}x) )= 0\n$$\n$$\nA_{W'} x = b_{W'}\n$$\nThe first condition simplifies to $H x + c - A_{W'}^{\\top} \\lambda = 0$, or $H x + A_{W'}^{\\top} \\lambda = -c$. Together, the KKT conditions form a system of linear equations:\n$$\n\\begin{pmatrix} H  A_{W'}^{\\top} \\\\ A_{W'}  0 \\end{pmatrix}\n\\begin{pmatrix} x \\\\ \\lambda \\end{pmatrix}\n=\n\\begin{pmatrix} -c \\\\ b_{W'} \\end{pmatrix}\n$$\nThe relevant part for finding the minimizer $x$ is the set of equality constraints themselves:\n$$\nA_{W'} x = b_{W'} \\implies \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n$$\nThis represents the system of equations:\n1. $x_1 + x_2 = 3$\n2. $x_1 - x_2 = 1$\n\nAdding the two equations yields $2x_1 = 4$, so $x_1 = 2$.\nSubstituting $x_1=2$ into the first equation gives $2 + x_2 = 3$, so $x_2 = 1$.\nThe unique minimizer of the equality-constrained subproblem is $x = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$.\n\n**Final Calculation: Objective Function Value**\n\nThe final step is to calculate the value of the objective function $f(x)$ at this minimizer $x = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$.\nThe objective function is $f(x) = \\frac{1}{2} x^{\\top} H x + c^{\\top} x$.\nLet's compute each term:\nThe quadratic term is:\n$$\n\\frac{1}{2} x^{\\top} H x = \\frac{1}{2} \\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 4  2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\frac{1}{2} ((4)(2) + (2)(1)) = \\frac{1}{2} (8 + 2) = \\frac{1}{2} (10) = 5\n$$\nThe linear term is:\n$$\nc^{\\top} x = \\begin{pmatrix} -4  -2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = (-4)(2) + (-2)(1) = -8 - 2 = -10\n$$\nThe total value of the objective function is the sum of these two terms:\n$$\nf\\left(\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\right) = 5 + (-10) = -5\n$$\nThe exact value of the objective function at this minimizer is $-5$.", "answer": "$$ \\boxed{-5} $$", "id": "3198856"}, {"introduction": "An algorithm's path to the optimum is not always simple, and \"degenerate\" steps can occur where the iterate hits multiple constraints simultaneously. This problem explores such a scenario, requiring the use of a tie-breaking rule to update the working set [@problem_id:3198924]. More importantly, it provides a deeper insight into the nature of Lagrange multipliers, revealing that their values at a potential solution are unique and determined by the KKT conditions, not by the path taken to get there.", "problem": "Consider the quadratic programming (QP) problem of minimizing the convex quadratic objective\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\, x^{\\top} Q x \\;+\\; c^{\\top} x\n$$\nwith $Q \\in \\mathbb{R}^{2 \\times 2}$ positive definite and $c \\in \\mathbb{R}^{2}$, subject to two linear inequality constraints\n$$\na_{1}^{\\top} x \\;\\le\\; b_{1}, \\qquad a_{2}^{\\top} x \\;\\le\\; b_{2}.\n$$\nLet the data be\n$$\nQ \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\qquad c \\;=\\; \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}, \\qquad a_{1} \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\qquad a_{2} \\;=\\; \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\qquad b_{1} \\;=\\; 1, \\qquad b_{2} \\;=\\; 1.\n$$\nYou will apply Wolfe’s method for quadratic programming (an active-set method) starting from the feasible point $x^{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ with an empty working set $W^{0} = \\varnothing$. The working set $W$ stores the indices of constraints currently treated as equalities.\n\nTasks:\n1. Using the equality-constrained subproblem that defines the search direction in Wolfe’s method, derive the search direction $p^{0}$ at $x^{0}$ and the step length $\\alpha^{\\star}$ determined by feasibility along the ray $x(\\alpha) = x^{0} + \\alpha p^{0}$.\n2. Show that two constraints become active simultaneously at the same $\\alpha^{\\star}$ and decide, using the lexicographic rule “choose the smallest index first,” which constraint should be added to the working set $W$ at that step.\n3. Denote by $x^{1} = x^{0} + \\alpha^{\\star} p^{0}$ the point reached by the step. At $x^{1}$, treat both constraints as active (that is, $a_{1}^{\\top} x^{1} = b_{1}$ and $a_{2}^{\\top} x^{1} = b_{2}$), and use the Karush–Kuhn–Tucker (KKT) conditions to compute the Lagrange multipliers $\\lambda^{(1)}$ and $\\lambda^{(2)}$ corresponding to the two constraints under the following two tie-breaking orders:\n   - Order A: add constraint $1$ first, then $2$.\n   - Order B: add constraint $2$ first, then $1$.\n4. Report the Euclidean norm of the difference between the two multiplier vectors obtained under Order A and Order B.\n\nYour final answer must be a single real number equal to the Euclidean norm of $\\lambda^{(1)} - \\lambda^{(2)}$. No rounding is required.", "solution": "The user wants me to solve a quadratic programming (QP) problem by applying steps related to Wolfe's method and analyzing the Lagrange multipliers at a specific point.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\nThe quadratic programming problem is to minimize the objective function:\n$$\nf(x) = \\frac{1}{2} x^{\\top} Q x + c^{\\top} x\n$$\nsubject to the linear inequality constraints:\n$$\na_{1}^{\\top} x \\le b_{1} \\\\\na_{2}^{\\top} x \\le b_{2}\n$$\nThe data provided are:\n-   Matrix $Q = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$\n-   Vector $c = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$\n-   Constraint vector $a_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and scalar $b_{1} = 1$\n-   Constraint vector $a_{2} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ and scalar $b_{2} = 1$\n-   Initial feasible point $x^{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n-   Initial working set $W^{0} = \\varnothing$\n-   The matrix $Q$ is specified as positive definite.\n-   A tie-breaking rule for degeneracy: \"choose the smallest index first\".\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is a standard convex quadratic programming problem, a fundamental topic in mathematical optimization. The method mentioned, Wolfe's method, is a well-established active-set algorithm for solving such problems. All provided data and definitions are mathematically sound.\n-   **Well-Posed**: The objective function is strictly convex because $Q$ is positive definite, and the feasible region defined by linear inequalities is convex. Therefore, a unique global minimum exists. The initial point $x^0 = (0,0)^{\\top}$ is feasible, as $a_1^{\\top}x^0 = 0 \\le 1$ and $a_2^{\\top}x^0 = 0 \\le 1$. The tasks are specific and lead to a computable result. The language in Task 3 concerning \"tie-breaking orders\" for multiplier calculation could be perceived as ambiguous, but it can be resolved through rigorous application of the definition of Lagrange multipliers from the Karush–Kuhn–Tucker (KKT) conditions. The problem is self-contained and consistent.\n-   **Objective**: The problem is formulated in precise, objective mathematical terms.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. I will now proceed with a complete, reasoned solution.\n\n### Solution\n\nThe objective function can be written explicitly as $f(x_1, x_2) = \\frac{1}{2}(x_1^2 + x_2^2) - x_1$. The constraints are $x_1 + x_2 \\le 1$ and $x_1 - x_2 \\le 1$. We start at $x^0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ with an empty working set $W^0 = \\varnothing$.\n\n**Task 1: Search direction $p^0$ and step length $\\alpha^{\\star}$**\n\nAt iteration $k=0$, we are at $x^0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ with working set $W^0 = \\varnothing$. The gradient of the objective function at $x^0$ is:\n$$\ng_0 = \\nabla f(x^0) = Q x^0 + c = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\n$$\nThe search direction $p^0$ is the solution to the equality-constrained subproblem. Since $W^0$ is empty, there are no equality constraints on $p^0$. The subproblem is to minimize $\\frac{1}{2}(p^0)^{\\top}Q p^0 + g_0^{\\top}p^0$. The solution is found by setting the gradient with respect to $p^0$ to zero:\n$$\nQ p^0 + g_0 = 0 \\implies p^0 = -Q^{-1} g_0\n$$\nSince $Q=I$ (the identity matrix), $Q^{-1}=I$. Thus:\n$$\np^0 = -g_0 = -\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nNext, we determine the maximum step length $\\alpha$ along the ray $x(\\alpha) = x^0 + \\alpha p^0$ that maintains feasibility. The point along the ray is $x(\\alpha) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\alpha \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix}$.\nWe must satisfy the constraints not in the working set, i.e., constraints $1$ and $2$.\nFor constraint $1$: $a_1^{\\top} x(\\alpha) \\le b_1 \\implies \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix} \\le 1 \\implies \\alpha \\le 1$.\nFor constraint $2$: $a_2^{\\top} x(\\alpha) \\le b_2 \\implies \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix} \\le 1 \\implies \\alpha \\le 1$.\nThe maximum step $\\alpha_{max}$ that can be taken before violating a constraint is the smallest positive $\\alpha$ that makes a constraint active. Here, both constraints become active at $\\alpha=1$. So, $\\alpha_{max} = 1$.\nThe step length $\\alpha^\\star$ in Wolfe's method is the smaller of $1$ (full step to the minimum of the subproblem) and $\\alpha_{max}$.\n$$\n\\alpha^{\\star} = \\min(1, \\alpha_{max}) = \\min(1, 1) = 1\n$$\n\n**Task 2: Degeneracy and tie-breaking**\n\nThe new point is $x^1 = x^0 + \\alpha^{\\star}p^0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + 1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nAt this point, we check the constraints:\n-   Constraint 1: $a_1^{\\top}x^1 = \\begin{pmatrix} 1  1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$. Since $b_1=1$, this constraint is active.\n-   Constraint 2: $a_2^{\\top}x^1 = \\begin{pmatrix} 1  -1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$. Since $b_2=1$, this constraint is also active.\nSince two constraints become active simultaneously at $\\alpha^{\\star}=1$, this is a degenerate step. The problem specifies a lexicographic tie-breaking rule: \"choose the smallest index first.\" Between indices $1$ and $2$, the smallest is $1$. Therefore, constraint $1$ is added to the working set. The new working set for the next iteration of Wolfe's method would be $W^1 = \\{1\\}$.\n\n**Task 3  4: Multiplier calculation and norm of difference**\n\nThis task asks for a separate analysis at the point $x^1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. We are instructed to \"treat both constraints as active\", meaning we consider the working set $W = \\{1, 2\\}$. We must compute the Lagrange multipliers based on the KKT conditions for this situation.\n\nThe KKT stationarity condition at a point $x$ with an active set $W$ is:\n$$\n\\nabla f(x) + \\sum_{i \\in W} \\lambda_i a_i = 0\n$$\nwhere $\\lambda_i$ are the Lagrange multipliers. We need to compute the gradient of $f$ at $x^1$:\n$$\ng_1 = \\nabla f(x^1) = Q x^1 + c = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nWith $W=\\{1,2\\}$, the stationarity condition is:\n$$\ng_1 + \\lambda_1 a_1 + \\lambda_2 a_2 = 0\n$$\nSubstituting the values of $g_1$, $a_1$, and $a_2$:\n$$\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\lambda_1 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\lambda_2 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives a system of linear equations for $\\lambda_1$ and $\\lambda_2$:\n$$\n\\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} \\lambda_1 \\\\ \\lambda_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe matrix $\\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix}$ has a determinant of $(1)(-1) - (1)(1) = -2$, so it is invertible. The only solution to this homogeneous system is the trivial solution:\n$$\n\\begin{pmatrix} \\lambda_1 \\\\ \\lambda_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe problem asks to compute two multiplier vectors, $\\lambda^{(1)}$ and $\\lambda^{(2)}$, under two different \"tie-breaking orders\". However, the Lagrange multipliers at a vertex defined by a specific set of active constraints are unique. They are determined by the KKT system above, which has a unique solution. The phrasing \"Order A: add constraint 1 first, then 2\" and \"Order B: add constraint 2 first, then 1\" does not alter the set of active constraints $\\{1, 2\\}$ being considered, nor does it change the defining KKT equations. Therefore, the resulting multiplier vector is the same in both cases.\n$$\n\\lambda^{(1)} = \\lambda^{(2)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe difference between these two vectors is:\n$$\n\\lambda^{(1)} - \\lambda^{(2)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe Euclidean norm of this difference is:\n$$\n\\| \\lambda^{(1)} - \\lambda^{(2)} \\|_2 = \\sqrt{0^2 + 0^2} = 0\n$$\nThe point $x^1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ is the unconstrained global minimizer of $f(x)$, as its gradient is zero there. Since this point is also feasible, it is the solution to the constrained problem. The KKT conditions must be satisfied with all multipliers equal to zero. The complex phrasing of the problem regarding \"tie-breaking orders\" for multiplier calculation at this specific vertex serves only to test the fundamental understanding that the multipliers are unique.", "answer": "$$ \\boxed{0} $$", "id": "3198924"}]}