{"hands_on_practices": [{"introduction": "This first exercise provides a foundational walkthrough of a single, decisive iteration in an active-set method. By starting from a feasible point and calculating the search direction and step length, you will see precisely how the algorithm moves towards the optimum until blocked by a constraint boundary. This problem [@problem_id:3094761] is specially designed to land at a unique type of solution where strict complementarity fails, introducing the important concept of degeneracy.", "problem": "Consider the Quadratic Programming (QP) problem in two variables with objective function $f(x)=\\tfrac{1}{2} x^{\\top} Q x$ and linear inequality constraints $A x \\le b$, where $Q \\in \\mathbb{R}^{2 \\times 2}$, $A \\in \\mathbb{R}^{2 \\times 2}$, and $b \\in \\mathbb{R}^{2}$. Let $Q=I_{2}$, $A=-I_{2}$, and $b=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$. The feasible region is therefore $\\{x \\in \\mathbb{R}^{2} : x \\ge 0\\}$, and the unconstrained minimizer of $f$ is $x=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$.\n\nYou will run one iteration of an active-set method starting from the feasible point $x^{0}=\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$ with the initial working set $W^{0}=\\varnothing$. In the equality-constrained subproblem defined by $W^{0}$, compute the search direction $p^{0}$ that minimizes $f$ subject to the constraints in $W^{0}$. Then, apply the standard feasibility-maintaining step acceptance along the ray $x(\\alpha)=x^{0}+\\alpha p^{0}$ to determine the largest accepted step length $\\alpha^{\\star}$ such that $A x(\\alpha) \\le b$ holds. After taking the accepted step to $x^{1}=x^{0}+\\alpha^{\\star} p^{0}$ and updating the working set by including all constraints that become active at $x^{1}$, compute the Lagrange multipliers associated with the active constraints at $x^{1}$ using the optimality conditions for constrained quadratic minimization.\n\nUsing only fundamental definitions of feasibility, stationarity, and complementarity, and the logic of the active-set method, determine the accepted step length $\\alpha^{\\star}$ and the multipliers $\\lambda_{1}^{\\star}$ and $\\lambda_{2}^{\\star}$ associated with the two constraints at the resulting point. Express your final answer as a row matrix with entries $\\alpha^{\\star}$, $\\lambda_{1}^{\\star}$, $\\lambda_{2}^{\\star}$. No rounding is required.", "solution": "The problem is a Quadratic Programming (QP) problem defined as:\n$$\n\\text{minimize} \\quad f(x) = \\frac{1}{2} x^{\\top} Q x \\\\\n\\text{subject to} \\quad A x \\le b\n$$\nwhere $x \\in \\mathbb{R}^{2}$. The given parameters are the Hessian matrix $Q = I_{2} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$, the constraint matrix $A = -I_{2} = \\begin{pmatrix} -1  0 \\\\ 0  -1 \\end{pmatrix}$, and the vector $b = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nThe objective function can be written as $f(x_1, x_2) = \\frac{1}{2} (x_1^2 + x_2^2)$. The gradient of the objective function is $\\nabla f(x) = Qx = I_{2}x = x$.\nThe constraints $A x \\le b$ are equivalent to $-I_{2}x \\le 0$, which simplifies to $-x \\le 0$, or $x \\ge 0$. These can be written as two individual linear inequality constraints:\n$c_1(x): -x_1 \\le 0$\n$c_2(x): -x_2 \\le 0$\nThe rows of the matrix $A$ are $a_1^{\\top} = \\begin{pmatrix} -1  0 \\end{pmatrix}$ and $a_2^{\\top} = \\begin{pmatrix} 0  -1 \\end{pmatrix}$.\n\nWe start one iteration of an active-set method from the feasible point $x^{0}=\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$ with the initial working set $W^{0}=\\varnothing$. At $x^0$, we have $c_1(x^0) = -1  0$ and $c_2(x^0) = -1  0$, confirming that no constraints are active and the choice of $W^0$ is consistent.\n\nFirst, we compute the search direction $p^0$ by solving the equality-constrained subproblem defined by the current working set $W^0$. We seek a step $p$ that minimizes $f(x^0+p)$ subject to the constraints in $W^0$ being active, i.e., $a_i^{\\top} p = 0$ for all $i \\in W^0$.\nSince $W^0 = \\varnothing$, there are no constraints on $p$. The subproblem is an unconstrained minimization of $f(x^0+p) = \\frac{1}{2}(x^0+p)^{\\top}Q(x^0+p)$. The objective function in terms of $p$ is $\\frac{1}{2}p^{\\top}Qp + (Qx^0)^{\\top}p + \\text{const}$.\nThe optimal step $p^0$ is found by setting the gradient with respect to $p$ to zero:\n$$\n\\nabla_p f(x^0+p) \\Big|_{p=p^0} = Q p^0 + Q x^0 = 0\n$$\nThis gives the linear system $Q p^0 = -Q x^0$. Substituting the given matrices $Q=I_2$:\n$$\nI_2 p^0 = -I_2 x^0 \\implies p^0 = -x^0\n$$\nWith $x^0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, the search direction is $p^0 = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$.\n\nNext, we determine the maximum step length $\\alpha^{\\star}$ that maintains feasibility. The new point is $x(\\alpha) = x^0 + \\alpha p^0$. We require $x(\\alpha)$ to satisfy all constraints not in the working set, i.e., $a_i^{\\top}(x^0 + \\alpha p^0) \\le b_i$ for $i \\in \\{1, 2\\}$. Since $b_i=0$ for both constraints, this is $a_i^{\\top}x^0 + \\alpha a_i^{\\top}p^0 \\le 0$.\nFor a constraint $i$ to become active, we must move towards its boundary. This occurs if $a_i^{\\top} p^0  0$. The step length at which constraint $i$ becomes active is given by $\\alpha_i = \\frac{b_i-a_i^{\\top}x^0}{a_i^{\\top}p^0} = \\frac{-a_i^{\\top}x^0}{a_i^{\\top}p^0}$.\n\nFor constraint $1$: $a_1^{\\top} = \\begin{pmatrix} -1  0 \\end{pmatrix}$.\n$a_1^{\\top}p^0 = \\begin{pmatrix} -1  0 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix} = 1  0$.\nThe step length to hit this constraint boundary is $\\alpha_1 = \\frac{-a_1^{\\top}x^0}{a_1^{\\top}p^0} = \\frac{-(-1)}{1} = 1$.\n\nFor constraint $2$: $a_2^{\\top} = \\begin{pmatrix} 0  -1 \\end{pmatrix}$.\n$a_2^{\\top}p^0 = \\begin{pmatrix} 0  -1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix} = 1  0$.\nThe step length to hit this constraint boundary is $\\alpha_2 = \\frac{-a_2^{\\top}x^0}{a_2^{\\top}p^0} = \\frac{-(-1)}{1} = 1$.\n\nThe maximum step length before violating any constraint is $\\alpha_{max} = \\min\\{\\alpha_1, \\alpha_2\\} = \\min\\{1, 1\\} = 1$.\nThe step to the minimizer of the unconstrained subproblem is of length $1$, since $x^0 + 1 \\cdot p^0 = x^0 - x^0 = 0$.\nThe accepted step length $\\alpha^{\\star}$ is the smaller of the step to the subproblem's minimizer and the maximum feasible step: $\\alpha^{\\star} = \\min(1, \\alpha_{max}) = \\min(1, 1) = 1$.\n\nThe new iterate is $x^1 = x^0 + \\alpha^{\\star} p^0$:\n$$\nx^1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + 1 \\cdot \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nAt $x^1$, both constraints are active:\n$c_1(x^1) = -x_1^1 = 0$\n$c_2(x^1) = -x_2^1 = 0$\nSince the step was limited by both constraints $1$ and $2$, the new working set is updated to include their indices: $W^1 = \\{1, 2\\}$.\n\nFinally, we compute the Lagrange multipliers $\\lambda^{\\star} = \\begin{pmatrix} \\lambda_1^{\\star} \\\\ \\lambda_2^{\\star} \\end{pmatrix}$ associated with the active constraints at $x^1$. The first-order optimality (stationarity) condition for the original QP states that at a constrained minimizer, the gradient of the objective function must be a linear combination of the gradients of the active constraints:\n$$\n\\nabla f(x^1) + \\sum_{i \\in W^1} \\lambda_i^{\\star} \\nabla c_i (x^1) = 0\n$$\nThe gradient of the objective at $x^1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ is $\\nabla f(x^1) = x^1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe gradients of the constraints are constant vectors: $\\nabla c_1(x) = a_1 = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$ and $\\nabla c_2(x) = a_2 = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$.\nThe stationarity condition becomes:\n$$\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\lambda_1^{\\star} \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} + \\lambda_2^{\\star} \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis is equivalent to the linear system:\n$$\n\\begin{pmatrix} -1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} \\lambda_1^{\\star} \\\\ \\lambda_2^{\\star} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis system has the unique solution $\\lambda_1^{\\star} = 0$ and $\\lambda_2^{\\star} = 0$.\n\nThe results of the single iteration are: the accepted step length $\\alpha^{\\star} = 1$, and the Lagrange multipliers $\\lambda_1^{\\star} = 0$ and $\\lambda_2^{\\star} = 0$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  0  0 \\end{pmatrix}}\n$$", "id": "3094761"}, {"introduction": "Before we can optimize, we need a valid starting point, which is not always obvious. This practice focuses on \"Phase I\" of the active-set method, a preliminary procedure used to find a feasible solution when one is not given. You will work through a scenario [@problem_id:3094700] with a deliberately infeasible initial setup and apply the active-set logic to systematically reduce constraint violations until a feasible point is found, preparing you for the main optimization phase.", "problem": "Consider the following Quadratic Programming (QP) problem in two variables $x \\in \\mathbb{R}^{2}$:\nMinimize the quadratic objective\n$$\nq(x) \\;=\\; \\frac{1}{2}\\,x^{\\top}H\\,x \\;+\\; c^{\\top}x,\n$$\nwhere $H \\in \\mathbb{R}^{2 \\times 2}$ and $c \\in \\mathbb{R}^{2}$ are given by\n$$\nH \\;=\\; \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix}, \n\\qquad\nc \\;=\\; \\begin{pmatrix} -2 \\\\ -3 \\end{pmatrix},\n$$\nsubject to the polyhedral inequality constraints $A x \\leq b$ specified by\n$$\n\\begin{aligned}\n\\text{Constraint } C_{1}:  -x_{1} \\;\\leq\\; 0 \\quad\\text{(equivalently $x_{1} \\geq 0$)}, \\\\\n\\text{Constraint } C_{2}:  x_{1} \\;\\leq\\; 2, \\\\\n\\text{Constraint } C_{3}:  x_{1} \\;\\leq\\; 1, \\\\\n\\text{Constraint } C_{4}:  -x_{2} \\;\\leq\\; 0 \\quad\\text{(equivalently $x_{2} \\geq 0$)}, \\\\\n\\text{Constraint } C_{5}:  x_{2} \\;\\leq\\; 3.\n\\end{aligned}\n$$\nThe feasible set for these constraints is nonempty.\n\nYou will run the feasibility Phase I of the active-set method for Quadratic Programming (QP) with the initial working set (the set of currently enforced active inequalities treated as equalities) chosen as\n$$\nW_{0} \\;=\\; \\{\\, C_{1},\\, C_{2},\\, C_{3},\\, C_{4} \\,\\},\n$$\nwhich is infeasible because it requires $x_{1}$ to simultaneously satisfy $x_{1} = 0$, $x_{1} = 2$, and $x_{1} = 1$, together with $x_{2} = 0$.\n\nFor Phase I, use the standard least-squares auxiliary subproblem that reduces infeasibility of the current working set by solving, at iteration $k$, the problem\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\;\\frac{1}{2}\\,\\|A_{W_{k}}\\,x \\;-\\; b_{W_{k}}\\|^{2},\n$$\nwhere $A_{W_{k}}$ and $b_{W_{k}}$ collect the rows of $A$ and entries of $b$ corresponding to the constraints in $W_{k}$, interpreted as equalities $A_{W_{k}}x = b_{W_{k}}$ at the end of the step. Use the standard active-set swap rule based on Lagrange multiplier signs: if any multiplier associated with an active inequality in $W_{k}$ is negative, remove from $W_{k}$ the constraint with the most negative multiplier; otherwise, if no such negative multiplier exists, proceed to add a blocking constraint that becomes active.\n\nCarry out Phase I until a feasible point for all five inequalities is found, documenting the sequence of working sets and the least-squares solutions. Your final answer must be the total number of swaps performed during Phase I. Provide your answer as a single integer. No rounding is required.", "solution": "The Phase I procedure is executed to find a feasible point, starting from the infeasible working set $W_0 = \\{C_1, C_2, C_3, C_4\\}$. A \"swap\" occurs when a constraint is removed from the working set due to a negative multiplier. At each step, we solve the least-squares subproblem $\\min \\frac{1}{2}\\|A_{W_k}x - b_{W_k}\\|^2$, calculate the multipliers (which for this problem are the residuals $\\lambda = A_{W_k}x^{(k)} - b_{W_k}$), and remove the constraint with the most negative multiplier.\n\n**Iteration $k=0$:**\nThe initial working set is $W_0 = \\{C_1, C_2, C_3, C_4\\}$. The subproblem matrices are:\n$$\nA_{W_0} = \\begin{pmatrix} -1  0 \\\\ 1  0 \\\\ 1  0 \\\\ 0  -1 \\end{pmatrix}, \\quad b_{W_0} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThe solution to the normal equations $A_{W_0}^{\\top}A_{W_0} x^{(0)} = A_{W_0}^{\\top}b_{W_0}$ is $x^{(0)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe multipliers are the residuals $\\lambda^{(0)} = A_{W_0}x^{(0)} - b_{W_0} = \\begin{pmatrix} -1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe most negative multiplier is $-1$, corresponding to both $C_1$ and $C_2$. Breaking the tie by choosing the constraint with the smallest index, we remove $C_1$. This is the first swap. The new working set is $W_1 = \\{C_2, C_3, C_4\\}$.\n\n**Iteration $k=1$:**\nThe working set is $W_1 = \\{C_2, C_3, C_4\\}$. The subproblem matrices are:\n$$\nA_{W_1} = \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 0  -1 \\end{pmatrix}, \\quad b_{W_1} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThe solution to the normal equations is $x^{(1)} = \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix}$.\nThe multipliers are the residuals $\\lambda^{(1)} = A_{W_1}x^{(1)} - b_{W_1} = \\begin{pmatrix} -0.5 \\\\ 0.5 \\\\ 0 \\end{pmatrix}$.\nThe only negative multiplier is $\\lambda_2 = -0.5$. We remove $C_2$. This is the second swap. The new working set is $W_2 = \\{C_3, C_4\\}$.\n\n**Iteration $k=2$:**\nThe working set is $W_2 = \\{C_3, C_4\\}$. The subproblem matrices are:\n$$\nA_{W_2} = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}, \\quad b_{W_2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nThe solution is $x^{(2)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe multipliers are the residuals $\\lambda^{(2)} = A_{W_2}x^{(2)} - b_{W_2} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThere are no negative multipliers. We check if the point $x^{(2)} = (1, 0)$ is feasible for all original constraints:\n- $C_1: 1 \\ge 0$ (Satisfied)\n- $C_2: 1 \\le 2$ (Satisfied)\n- $C_3: 1 \\le 1$ (Satisfied)\n- $C_4: 0 \\ge 0$ (Satisfied)\n- $C_5: 0 \\le 3$ (Satisfied)\nThe point is feasible, so Phase I terminates.\n\nA total of 2 swaps were performed.", "answer": "$$\\boxed{2}$$", "id": "3094700"}, {"introduction": "Optimization algorithms are not just theoretical constructs; they must be implemented as robust computer programs. This practice [@problem_id:3094695] delves into the numerical realities of active-set methods, particularly how they handle the degenerate cases first seen in our initial exercise. You will analyze how the choice of numerical tolerances for checking multipliers and constraint satisfaction can affect the algorithm's ability to correctly identify the active set, bridging the gap between abstract theory and practical implementation.", "problem": "Consider the convex Quadratic Programming (QP) problem with decision variable $x \\in \\mathbb{R}^2$ defined by the objective and inequality constraints\n$$\n\\min_{x \\in \\mathbb{R}^2} \\;\\; \\tfrac{1}{2} x^\\top H x + c^\\top x\n$$\nsubject to\n$$\ng_1(x) = -x_1 \\le 0, \\quad g_2(x) = -x_2 \\le 0, \\quad g_3(x) = s - a^\\top x \\le 0,\n$$\nwhere $H \\in \\mathbb{R}^{2 \\times 2}$ is the identity matrix $I_2$, $c \\in \\mathbb{R}^2$ is the zero vector, $a = (1,1)^\\top$, and $s \\in \\mathbb{R}$ is a scalar parameter. The feasible set is the intersection of the nonnegativity orthant and the half-space defined by $a^\\top x \\ge s$.\n\nFundamental base to use:\n- Convexity: $H = I_2$ is positive definite, so the problem is strictly convex and has a unique minimizer.\n- Karush-Kuhn-Tucker (KKT) conditions: For a convex QP with inequality constraints, optimality is characterized by primal feasibility $g_i(x^\\star) \\le 0$, dual feasibility $\\mu_i^\\star \\ge 0$, stationarity $H x^\\star + c + \\sum_{i=1}^3 \\mu_i^\\star \\nabla g_i(x^\\star) = 0$, and complementarity $\\mu_i^\\star g_i(x^\\star) = 0$ for each constraint $i$.\n\nYour tasks:\n1) Derive, from the fundamental base above and first principles, the optimal solution $x^\\star$ and the corresponding Lagrange multipliers $\\mu^\\star \\in \\mathbb{R}^3$ for the problem as a function of the scalar parameter $s$. Do not assume or quote pre-packaged projection formulas; instead, use stationarity, primal feasibility, dual feasibility, and complementarity to determine $x^\\star$ and $\\mu^\\star$.\n\n2) Define the true active set at the solution as\n$$\n\\mathcal{A}^\\star = \\{ i \\in \\{1,2,3\\} : g_i(x^\\star) = 0 \\}.\n$$\nDefine two stopping criteria that an active-set method might use to decide activeness:\n- Multiplier-based identification under tolerance $\\tau_\\mu$: declare constraint $i$ active if $\\mu_i^\\star \\ge \\tau_\\mu$.\n- Slack-based identification under tolerance $\\tau_{\\text{slack}}$: declare constraint $i$ active if $|g_i(x^\\star)| \\le \\tau_{\\text{slack}}$.\n\n3) For each test case, quantify false identification under a given pair of tolerances by the integer\n$$\nM = \\left|\\mathcal{A}^\\star \\setminus \\widehat{\\mathcal{A}}_\\mu\\right| + \\left|\\widehat{\\mathcal{A}}_s \\setminus \\mathcal{A}^\\star\\right|,\n$$\nwhere $\\widehat{\\mathcal{A}}_\\mu = \\{ i : \\mu_i^\\star \\ge \\tau_\\mu \\}$ and $\\widehat{\\mathcal{A}}_s = \\{ i : |g_i(x^\\star)| \\le \\tau_{\\text{slack}} \\}$. The first term counts false negatives from the multiplier-based rule; the second term counts false positives from the slack-based rule.\n\n4) Implement a program that, for each $s$ in the test suite below, computes $x^\\star$, $g_i(x^\\star)$, $\\mu_i^\\star$, and then $M$ for both a strict tolerance pair and a loose (inaccurate) tolerance pair:\n- Strict tolerances: $\\tilde{\\tau}_\\mu = 10^{-12}$ and $\\tilde{\\tau}_{\\text{slack}} = 10^{-12}$.\n- Loose tolerances: $\\tau_\\mu = 10^{-3}$ and $\\tau_{\\text{slack}} = 10^{-3}$.\n\nTest suite (values of $s$):\n- Happy path: $s = 0.5$.\n- Boundary/degenerate: $s = 0$.\n- Nearly zero positive: $s = 10^{-8}$.\n- Near-threshold positive: $s = 10^{-3}$.\n- Nearly zero negative: $s = -10^{-8}$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For the five test cases above in the given order, output the flattened list $[M_{\\text{strict}}(s_1), M_{\\text{loose}}(s_1), \\dots, M_{\\text{strict}}(s_5), M_{\\text{loose}}(s_5)]$.\n- Each $M$ must be an integer.", "solution": "## Analytical Solution via KKT Conditions\nThe problem is to find the point in the feasible set that is closest to the origin:\n$$ \\min_{x_1, x_2} \\;\\; \\frac{1}{2} (x_1^2 + x_2^2) $$\nsubject to\n$$ x_1 \\ge 0, \\quad x_2 \\ge 0, \\quad x_1 + x_2 \\ge s $$\nThe Lagrangian $L(x, \\mu)$ for the problem with constraints $g_i(x) \\le 0$ is:\n$$ L(x, \\mu) = \\frac{1}{2}(x_1^2 + x_2^2) + \\mu_1(-x_1) + \\mu_2(-x_2) + \\mu_3(s - x_1 - x_2) $$\nThe KKT conditions for an optimal solution $(x^\\star, \\mu^\\star)$ are derived from stationarity, primal feasibility, dual feasibility, and complementary slackness. We analyze the solution by considering cases for the parameter $s$.\n\n**Case 1: $s  0$**\nThe feasible region does not contain the unconstrained minimizer (the origin). By analyzing the KKT conditions, we find that the solution must lie on the boundary $x_1+x_2=s$. Due to symmetry, $x_1^\\star = x_2^\\star$. This leads to the solution:\n- $x^\\star = (s/2, s/2)^\\top$\n- $\\mu^\\star = (0, 0, s/2)^\\top$\n- $g(x^\\star) = (-s/2, -s/2, 0)^\\top$\n- The true active set is $\\mathcal{A}^\\star = \\{3\\}$.\n\n**Case 2: $s \\le 0$**\nThe constraint $x_1+x_2 \\ge s$ is redundant for any non-negative $x_1, x_2$. The problem reduces to finding the point in the first quadrant closest to the origin, which is the origin itself.\n- $x^\\star = (0,0)^\\top$\n- $\\mu^\\star = (0,0,0)^\\top$\n- $g(x^\\star) = (0, 0, s)^\\top$\nThe true active set depends on the value of $s$:\n- If $s  0$, then $g_3(x^\\star) = s  0$. Only constraints 1 and 2 are active: $\\mathcal{A}^\\star = \\{1, 2\\}$.\n- If $s = 0$, then $g_3(x^\\star) = 0$. All three constraints are active: $\\mathcal{A}^\\star = \\{1, 2, 3\\}$. This is a degenerate case where active constraints have zero multipliers.\n\n## Active Set Identification and Mismatch Metric\nThe problem defines two criteria for identifying the active set based on numerical tolerances:\n- Multiplier-based: $\\widehat{\\mathcal{A}}_\\mu = \\{ i : \\mu_i^\\star \\ge \\tau_\\mu \\}$\n- Slack-based: $\\widehat{\\mathcal{A}}_s = \\{ i : |g_i(x^\\star)| \\le \\tau_{\\text{slack}} \\}$\n\nA mismatch metric quantifies identification errors:\n$M = \\left|\\mathcal{A}^\\star \\setminus \\widehat{\\mathcal{A}}_\\mu\\right| + \\left|\\widehat{\\mathcal{A}}_s \\setminus \\mathcal{A}^\\star\\right|$\nwhere the first term counts false negatives by the multiplier rule (true active constraints missed) and the second term counts false positives by the slack rule (inactive constraints misidentified as active).\n\n## Evaluation for Test Cases\nWe apply the derived solution to calculate $M$ for each test case.\n\n**For $s=0$ (Degenerate Case):**\n- Solution: $x^\\star=(0,0)^\\top$, $\\mu^\\star=(0,0,0)^\\top$, $g(x^\\star)=(0,0,0)^\\top$.\n- True active set: $\\mathcal{A}^\\star=\\{1,2,3\\}$.\n- For any positive tolerance $\\tau_\\mu$, the multiplier-based rule fails completely because all multipliers are zero, so $\\widehat{\\mathcal{A}}_\\mu = \\emptyset$. This yields 3 false negatives.\n- For any positive tolerance $\\tau_{\\text{slack}}$, the slack-based rule correctly identifies all active constraints because all slacks are exactly zero, so $\\widehat{\\mathcal{A}}_s = \\{1,2,3\\}$. This yields 0 false positives.\n- The total mismatch is $M = 3 + 0 = 3$ for both strict and loose tolerances.\n\n**For $s=10^{-8}$ (Nearly Degenerate Positive Case):**\n- Solution: $s  0$, so $x^\\star=(0.5\\times 10^{-8}, 0.5\\times 10^{-8})^\\top$, $\\mu^\\star=(0, 0, 0.5\\times 10^{-8})^\\top$, $g(x^\\star)=(-0.5\\times 10^{-8}, -0.5\\times 10^{-8}, 0)^\\top$.\n- True active set: $\\mathcal{A}^\\star=\\{3\\}$.\n- With **loose tolerances** ($\\tau_\\mu = 10^{-3}, \\tau_{\\text{slack}} = 10^{-3}$):\n  - $\\mu_3^\\star = 0.5\\times 10^{-8}  10^{-3}$, so the multiplier rule misses the active constraint: $\\widehat{\\mathcal{A}}_\\mu = \\emptyset$. (1 false negative)\n  - $|g_1|=|g_2|=0.5\\times 10^{-8} \\le 10^{-3}$, so the slack rule misidentifies the inactive constraints as active: $\\widehat{\\mathcal{A}}_s = \\{1,2,3\\}$. (2 false positives)\n  - The total mismatch is $M = 1 + 2 = 3$.\n- With **strict tolerances**, the values are resolved correctly, and $M=0$.\n\nThe provided Python code in the next section implements this analysis for all test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the QP problem for different values of s, and computes the \n    active set identification mismatch metric M for strict and loose tolerances.\n    \"\"\"\n    \n    # Define test cases and tolerances\n    test_cases = [0.5, 0.0, 1e-8, 1e-3, -1e-8]\n    strict_tolerances = (1e-12, 1e-12) # (tau_mu, tau_slack)\n    loose_tolerances = (1e-3, 1e-3)   # (tau_mu, tau_slack)\n\n    results = []\n\n    def compute_mismatch(s: float, tau_mu: float, tau_slack: float) - int:\n        \"\"\"\n        Computes the mismatch metric M for a given s and tolerance pair.\n        \"\"\"\n        # 1. Determine the analytical solution (x_star, mu_star, g_star)\n        if s  0:\n            x_star = np.array([s/2, s/2])\n            mu_star = np.array([0, 0, s/2])\n            g_star = np.array([-x_star[0], -x_star[1], s - np.sum(x_star)])\n            A_star = {3}\n        elif s == 0:\n            x_star = np.array([0, 0])\n            mu_star = np.array([0, 0, 0])\n            g_star = np.array([0, 0, 0])\n            A_star = {1, 2, 3}\n        else:  # s  0\n            x_star = np.array([0, 0])\n            mu_star = np.array([0, 0, 0])\n            g_star = np.array([0, 0, s])\n            A_star = {1, 2}\n            \n        # 2. Identify active sets based on the two criteria\n        A_hat_mu = {i + 1 for i, m in enumerate(mu_star) if m = tau_mu}\n        A_hat_s = {i + 1 for i, g in enumerate(g_star) if abs(g) = tau_slack}\n        \n        # 3. Compute the mismatch metric M\n        # False negatives from multiplier rule\n        false_negatives = len(A_star.difference(A_hat_mu))\n        # False positives from slack rule\n        false_positives = len(A_hat_s.difference(A_star))\n        \n        M = false_negatives + false_positives\n        return M\n\n    for s_val in test_cases:\n        # Calculate M for strict tolerances\n        M_strict = compute_mismatch(s_val, strict_tolerances[0], strict_tolerances[1])\n        results.append(M_strict)\n        \n        # Calculate M for loose tolerances\n        M_loose = compute_mismatch(s_val, loose_tolerances[0], loose_tolerances[1])\n        results.append(M_loose)\n\n    # Print the final output in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3094695"}]}