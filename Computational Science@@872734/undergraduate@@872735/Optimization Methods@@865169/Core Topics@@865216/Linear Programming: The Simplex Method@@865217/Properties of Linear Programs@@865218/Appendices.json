{"hands_on_practices": [{"introduction": "This practice explores the fundamental connection between the geometry of the feasible region and the objective function. By considering a simple cube, you will determine which objective vectors $c$ make a given vertex optimal, leading to the powerful concept of a normal cone. This exercise provides a geometric intuition for optimality conditions that is essential for understanding more complex optimization problems [@problem_id:3165494].", "problem": "Consider the Linear Program (LP) in dimension $3$ given by maximizing the linear functional $c^{\\top} x$ over a cube:\nmaximize $c^{\\top} x$ subject to $0 \\leq x_i \\leq 1$ for $i \\in \\{1,2,3\\}$, where $c \\in \\mathbb{R}^3$ is a given cost vector and $x \\in \\mathbb{R}^3$ is the decision variable. Let $P \\subset \\mathbb{R}^3$ denote the feasible set, which is the cube $[0,1]^3$. Use only core definitions from linear programming and convex analysis, including the definition of a polyhedron, a vertex, and the normal cone $N_P(v)$ at a point $v \\in P$ defined by\n$$\nN_P(v) \\;=\\; \\{\\, c \\in \\mathbb{R}^3 \\;:\\; c^{\\top}(x - v) \\leq 0 \\text{ for all } x \\in P \\,\\}.\n$$\nPerform the following steps.\n\n(a) Express the cube constraints in matrix form $A x \\leq b$, specifying $A \\in \\mathbb{R}^{6 \\times 3}$ and $b \\in \\mathbb{R}^6$ explicitly.\n\n(b) For a vertex $v \\in \\{0,1\\}^3$, use the definition of the normal cone to characterize the set of cost vectors $c \\in \\mathbb{R}^3$ for which $v$ is an optimal solution of the LP. State the condition on the signs of the components of $c$ in terms of the components of $v$.\n\n(c) Describe the normal cones $N_P(v)$ geometrically for all $8$ vertices $v \\in \\{0,1\\}^3$ by identifying which orthant of $\\mathbb{R}^3$ each $N_P(v)$ equals.\n\n(d) Assume now that the cost vector $c$ is random with a continuous, spherically symmetric distribution in $\\mathbb{R}^3$, meaning its probability density function can be written as $f(c) = g(\\|c\\|)$ for some integrable function $g : [0,\\infty) \\to \\mathbb{R}_+$, and that $c$ has no atoms (so $\\mathbb{P}(c_i = 0) = 0$ for all $i$). What is the exact probability that the optimal solution of the LP is $x^{\\star} = (1,1,1)$? Express your final answer as a reduced fraction. Do not round.", "solution": "The problem is subjected to validation and is deemed valid as it is scientifically grounded, well-posed, and objective. It is a standard problem in the field of linear programming and convex analysis.\n\n(a) Express the cube constraints in matrix form $A x \\leq b$.\n\nThe feasible set $P$ is the cube $[0,1]^3$, defined by the constraints $0 \\leq x_i \\leq 1$ for each component $i \\in \\{1, 2, 3\\}$ of the vector $x = (x_1, x_2, x_3)^{\\top}$. These constraints can be separated into two groups of three inequalities each.\n\nThe upper bounds are:\n$x_1 \\leq 1$\n$x_2 \\leq 1$\n$x_3 \\leq 1$\n\nThe lower bounds are $x_i \\ge 0$, which are equivalent to $-x_i \\leq 0$:\n$-x_1 \\leq 0$\n$-x_2 \\leq 0$\n$-x_3 \\leq 0$\n\nTo express these $6$ inequalities in the matrix form $A x \\leq b$, where $A \\in \\mathbb{R}^{6 \\times 3}$, $x \\in \\mathbb{R}^3$, and $b \\in \\mathbb{R}^6$, we construct the matrix $A$ and vector $b$ row by row.\n\nThe first three rows correspond to the upper bounds:\n$$\n\\begin{pmatrix} 1  0  0 \\end{pmatrix} x \\leq 1 \\\\\n\\begin{pmatrix} 0  1  0 \\end{pmatrix} x \\leq 1 \\\\\n\\begin{pmatrix} 0  0  1 \\end{pmatrix} x \\leq 1\n$$\nThis block can be represented by the $3 \\times 3$ identity matrix $I_3$.\n\nThe next three rows correspond to the lower bounds:\n$$\n\\begin{pmatrix} -1  0  0 \\end{pmatrix} x \\leq 0 \\\\\n\\begin{pmatrix} 0  -1  0 \\end{pmatrix} x \\leq 0 \\\\\n\\begin{pmatrix} 0  0  -1 \\end{pmatrix} x \\leq 0\n$$\nThis block can be represented by the negative of the identity matrix, $-I_3$.\n\nCombining these, the matrix $A \\in \\mathbb{R}^{6 \\times 3}$ is formed by stacking $I_3$ on top of $-I_3$, and the vector $b \\in \\mathbb{R}^6$ is formed by stacking a vector of ones on top of a vector of zeros.\n\nExplicitly, the matrix $A$ is:\n$$\nA = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1 \\\\\n-1  0  0 \\\\\n0  -1  0 \\\\\n0  0  -1\n\\end{pmatrix}\n$$\nAnd the vector $b$ is:\n$$\nb = \\begin{pmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix}\n$$\n\n(b) For a vertex $v \\in \\{0,1\\}^3$, characterize the set of cost vectors $c$ for which $v$ is an optimal solution.\n\nA feasible point $v \\in P$ is an optimal solution to the problem of maximizing $c^{\\top}x$ over $P$ if and only if $c^{\\top}v \\ge c^{\\top}x$ for all $x \\in P$. This is equivalent to the condition $c^{\\top}(x - v) \\leq 0$ for all $x \\in P$. The set of cost vectors $c$ that satisfy this condition is precisely the normal cone to $P$ at $v$, denoted $N_P(v)$. The problem asks us to characterize this set.\n\nLet $v = (v_1, v_2, v_3)^{\\top}$ be a vertex of the cube, so $v_i \\in \\{0, 1\\}$ for $i \\in \\{1, 2, 3\\}$. We need to find the conditions on $c = (c_1, c_2, c_3)^{\\top}$ such that $\\sum_{i=1}^3 c_i (x_i - v_i) \\leq 0$ for all $x \\in [0,1]^3$.\n\nTo find necessary conditions, we can select specific points $x \\in P$. Consider a point $x$ which differs from $v$ only in the $i$-th component. Let $x_j = v_j$ for $j \\neq i$, and let $x_i$ be free to vary in $[0,1]$. For the inequality to hold, we must have $c_i(x_i - v_i) \\leq 0$ for all $x_i \\in [0,1]$.\n\nWe analyze this for the two possible values of $v_i$:\nCase 1: $v_i = 0$. The condition is $c_i(x_i - 0) \\leq 0$, i.e., $c_i x_i \\leq 0$, for all $x_i \\in [0,1]$. Since we can choose $x_i=1$, we must have $c_1 \\cdot 1 \\le 0$, which implies $c_i \\leq 0$. If $c_i \\leq 0$, then for any non-negative $x_i$, the product $c_i x_i$ is non-positive, so the condition is satisfied.\n\nCase 2: $v_i = 1$. The condition is $c_i(x_i - 1) \\leq 0$ for all $x_i \\in [0,1]$. Since we can choose $x_i=0$, we must have $c_i(0-1) \\le 0$, which implies $-c_i \\leq 0$, or $c_i \\geq 0$. If $c_i \\geq 0$, then for any $x_i \\in [0,1]$, the term $(x_i - 1)$ is non-positive, so the product $c_i(x_i-1)$ is non-positive. The condition is satisfied.\n\nThese conditions on each component $c_i$ are also sufficient. If the sign of each $c_i$ is determined as above, then for any $x \\in [0,1]^3$, each term $c_i(x_i - v_i)$ in the sum $\\sum_{i=1}^3 c_i(x_i - v_i)$ is non-positive. The sum of non-positive terms is itself non-positive.\n\nTherefore, a vertex $v \\in \\{0,1\\}^3$ is an optimal solution for a cost vector $c$ if and only if for each $i \\in \\{1,2,3\\}$:\n- $c_i \\geq 0$ if $v_i = 1$\n- $c_i \\leq 0$ if $v_i = 0$\n\nThis can be stated compactly as $c_i(2v_i - 1) \\geq 0$ for each $i \\in \\{1,2,3\\}$.\n\n(c) Describe the normal cones $N_P(v)$ geometrically for all $8$ vertices.\n\nThe normal cone $N_P(v)$ is the set of vectors $c$ satisfying the conditions derived in part (b). These conditions constrain the sign of each component of $c$, which geographically defines one of the $2^3 = 8$ orthants of $\\mathbb{R}^3$. The orthants are the regions of space defined by the signs of the coordinates.\n\nThe correspondence between the $8$ vertices $v$ and the orthants that constitute their normal cones $N_P(v)$ is as follows:\n- For $v = (0,0,0)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\leq 0, c_2 \\leq 0, c_3 \\leq 0 \\}$ (the non-positive orthant).\n- For $v = (1,0,0)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\geq 0, c_2 \\leq 0, c_3 \\leq 0 \\}$.\n- For $v = (0,1,0)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\leq 0, c_2 \\geq 0, c_3 \\leq 0 \\}$.\n- For $v = (0,0,1)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\leq 0, c_2 \\leq 0, c_3 \\geq 0 \\}$.\n- For $v = (1,1,0)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\geq 0, c_2 \\geq 0, c_3 \\leq 0 \\}$.\n- For $v = (1,0,1)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\geq 0, c_2 \\leq 0, c_3 \\geq 0 \\}$.\n- For $v = (0,1,1)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\leq 0, c_2 \\geq 0, c_3 \\geq 0 \\}$.\n- For $v = (1,1,1)$: $N_P(v) = \\{ c \\in \\mathbb{R}^3 : c_1 \\geq 0, c_2 \\geq 0, c_3 \\geq 0 \\}$ (the non-negative orthant).\n\nThese $8$ normal cones are the 8 closed orthants of $\\mathbb{R}^3$. They partition the entire space, i.e., $\\bigcup_{v \\in \\{0,1\\}^3} N_P(v) = \\mathbb{R}^3$. Their interiors are disjoint.\n\n(d) Calculate the probability that the optimal solution is $x^{\\star} = (1,1,1)$.\n\nThe optimal solution is $x^{\\star} = (1,1,1)$ if and only if the cost vector $c$ belongs to the normal cone $N_P((1,1,1))$. From part (c), this is the non-negative orthant:\n$$\nN_P((1,1,1)) = \\{ c \\in \\mathbb{R}^3 : c_1 \\geq 0, c_2 \\geq 0, c_3 \\geq 0 \\}\n$$\nWe need to find the probability $\\mathbb{P}(c \\in N_P((1,1,1)))$, which is $\\mathbb{P}(c_1 \\geq 0, c_2 \\geq 0, c_3 \\geq 0)$.\n\nThe cost vector $c$ is drawn from a continuous, spherically symmetric distribution. A distribution is spherically symmetric if its probability density function $f(c)$ depends only on the norm of $c$, i.e., $f(c) = g(\\|c\\|)$. This property implies that the probability measure is invariant under rotations and reflections.\n\nThe three coordinate planes ($c_1=0$, $c_2=0$, $c_3=0$) divide $\\mathbb{R}^3$ into 8 disjoint (except for the boundaries) orthants. The problem specifies that the distribution is continuous and $\\mathbb{P}(c_i=0)=0$. This means the probability of $c$ lying on any of these boundary planes is zero, so we can ignore these boundaries in our probability calculation.\n\nLet $p_k$ be the probability that $c$ falls into the $k$-th orthant, for $k=1, \\dots, 8$. The spherical symmetry of the distribution means that the probability density is the same at any two points that are equidistant from the origin. All 8 orthants are geometrically congruent; any orthant can be mapped to any other by a sequence of reflections across the coordinate planes. For example, reflecting across the $c_1=0$ plane maps the non-negative orthant to the orthant where $c_1 \\leq 0, c_2 \\geq 0, c_3 \\geq 0$. Since reflections preserve the norm $\\|c\\|$, the probability density function is invariant under these transformations.\n\nConsequently, the probability of the random vector $c$ falling into any of the 8 orthants is the same. Let this common probability be $p$.\n$$\np = \\mathbb{P}(c \\in O_k) \\quad \\text{for each orthant } O_k.\n$$\nThe union of the 8 orthants is the entire space $\\mathbb{R}^3$. Therefore, the sum of their probabilities must be 1:\n$$\n\\sum_{k=1}^8 \\mathbb{P}(c \\in O_k) = 1\n$$\n$$\n8 p = 1\n$$\nThis yields $p = \\frac{1}{8}$.\n\nThe event that the optimal solution is $x^{\\star}=(1,1,1)$ corresponds to the event that $c$ is in the non-negative orthant. The probability of this event is $p$.\nTherefore, the exact probability that the optimal solution is $x^{\\star} = (1,1,1)$ is $\\frac{1}{8}$.", "answer": "$$\\boxed{\\frac{1}{8}}$$", "id": "3165494"}, {"introduction": "Linear programs often model real-world scenarios where resources are limited. This exercise delves into sensitivity analysis, addressing the practical question of how the optimal value changes when a constraint is slightly relaxed. You will calculate the \"shadow price\" of a constraint by leveraging the principles of duality and complementary slackness, revealing the profound economic interpretation of dual variables [@problem_id:3165476].", "problem": "Consider the following Linear Program (LP), defined in terms of a matrix of coefficients, a right-hand-side vector, and a cost vector. Let the primal LP be\n$$\n\\begin{aligned}\n\\text{maximize} \\quad  c^{\\top} x \\\\\n\\text{subject to} \\quad  A x \\le b, \\\\\n x \\ge 0,\n\\end{aligned}\n$$\nwhere\n$$\nA = \\begin{pmatrix}\n1  1 \\\\\n2  1 \\\\\n1  3\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n4 \\\\\n6 \\\\\n9\n\\end{pmatrix}, \\quad\nc = \\begin{pmatrix}\n3 \\\\\n2\n\\end{pmatrix},\n$$\nand all inequalities are componentwise. For this LP, define the optimal value function $v(b)$ to be the maximum objective value attained at optimality for the right-hand-side vector $b$.\n\nNow consider relaxing a single constraint by increasing its right-hand side. Specifically, for the index $i = 2$, replace the constraint $a_{2}^{\\top} x \\le b_{2}$ by $a_{2}^{\\top} x \\le b_{2} + \\delta$, where $\\delta \\ge 0$ and $a_{2}^{\\top}$ denotes the second row of $A$. Let $b(\\delta) = b + \\delta e_{2}$, where $e_{2}$ is the second standard basis vector in $\\mathbb{R}^{3}$, and let $v(b(\\delta))$ denote the optimal value of the LP with the perturbed right-hand side.\n\nUsing only the core definitions of Linear Programming duality, strong duality, complementary slackness, and the Lagrangian, determine the right-hand derivative at $\\delta = 0$ of the optimal value with respect to this relaxation, that is, compute\n$$\n\\lim_{\\delta \\to 0^{+}} \\frac{v(b(\\delta)) - v(b)}{\\delta}.\n$$\nExpress your final answer as a single real number. No rounding is required.", "solution": "The problem asks for the right-hand derivative of the optimal value function of a given linear program (LP) with respect to a perturbation in the right-hand side of the second constraint. The problem is to be solved using fundamental principles of linear programming theory.\n\n### Step 1: Problem Validation\n\nFirst, the problem statement is validated.\n\n**Extracted Givens:**\n- Primal LP formulation:\n  $$\n  \\begin{aligned}\n  \\text{maximize} \\quad  c^{\\top} x \\\\\n  \\text{subject to} \\quad  A x \\le b, \\\\\n   x \\ge 0,\n  \\end{aligned}\n  $$\n- Coefficients:\n  $$\n  A = \\begin{pmatrix}\n  1  1 \\\\\n  2  1 \\\\\n  1  3\n  \\end{pmatrix}, \\quad\n  b = \\begin{pmatrix}\n  4 \\\\\n  6 \\\\\n  9\n  \\end{pmatrix}, \\quad\n  c = \\begin{pmatrix}\n  3 \\\\\n  2\n  \\end{pmatrix}\n  $$\n- Optimal value function: $v(b)$.\n- Perturbation: The second constraint $a_{2}^{\\top} x \\le b_{2}$ is replaced by $a_{2}^{\\top} x \\le b_{2} + \\delta$, where $\\delta \\ge 0$. This corresponds to a new right-hand-side vector $b(\\delta) = b + \\delta e_{2}$.\n- Target quantity:\n  $$\n  \\lim_{\\delta \\to 0^{+}} \\frac{v(b(\\delta)) - v(b)}{\\delta}\n  $$\n\n**Validation Verdict:**\nThe problem is a standard exercise in sensitivity analysis for linear programming. It is scientifically grounded in established mathematical theory, well-posed with a clear and unique objective, and objectively stated. The data is complete and consistent. The problem is therefore deemed **valid**.\n\n### Step 2: Solution Derivation\n\nThe analysis will proceed in four parts: (1) formulating the dual LP, (2) solving the primal LP to find the optimal solution $x^*$ and value $v(b)$, (3) using complementary slackness to find the optimal dual solution $y^*$, and (4) analyzing the perturbed problem to find the required derivative.\n\n**1. The Primal and Dual LPs**\n\nThe primal LP (P) is given by:\n$$\n\\begin{array}{ll}\n\\text{maximize}    3x_1 + 2x_2 \\\\\n\\text{subject to}  x_1 + x_2 \\le 4 \\\\\n                   2x_1 + x_2 \\le 6 \\\\\n                   x_1 + 3x_2 \\le 9 \\\\\n                   x_1, x_2 \\ge 0\n\\end{array}\n$$\nThe dual LP (D) is formulated as:\n$$\n\\begin{array}{ll}\n\\text{minimize}    4y_1 + 6y_2 + 9y_3 \\\\\n\\text{subject to}  y_1 + 2y_2 + y_3 \\ge 3 \\\\\n                   y_1 + y_2 + 3y_3 \\ge 2 \\\\\n                   y_1, y_2, y_3 \\ge 0\n\\end{array}\n$$\n\n**2. Solving the Primal LP**\n\nThe feasible region of the primal LP is a convex polygon in the first quadrant of the $x_1$-$x_2$ plane. The maximum value of the linear objective function must occur at one of the vertices of this region. We identify the vertices by finding the intersection points of the constraint boundaries.\nThe vertices of the feasible region are:\n- $(0, 0)$: Intersection of $x_1=0$ and $x_2=0$. Objective value: $3(0) + 2(0) = 0$.\n- $(3, 0)$: Intersection of $2x_1 + x_2=6$ and $x_2=0$. Objective value: $3(3) + 2(0) = 9$.\n- $(0, 3)$: Intersection of $x_1 + 3x_2=9$ and $x_1=0$. Objective value: $3(0) + 2(3) = 6$.\n- $(2, 2)$: Intersection of $x_1 + x_2=4$ and $2x_1 + x_2=6$. Objective value: $3(2) + 2(2) = 10$.\n- $(1.5, 2.5)$: Intersection of $x_1 + x_2=4$ and $x_1 + 3x_2=9$. Objective value: $3(1.5) + 2(2.5) = 4.5 + 5 = 9.5$.\n\nComparing the objective values, the maximum is $10$, which occurs at the vertex $(2, 2)$. Thus, the optimal primal solution is $x^* = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$, and the optimal value of the unperturbed LP is $v(b) = c^{\\top}x^* = 10$.\n\n**3. Finding the Optimal Dual Solution**\n\nWe use the complementary slackness conditions to find the optimal dual solution $y^*$. These conditions state that for optimal solutions $x^*$ and $y^*$:\n- $y_i^* (b_i - a_i^{\\top}x^*) = 0$ for each primal constraint $i=1, 2, 3$.\n- $x_j^* ((A^{\\top}y^*)_j - c_j) = 0$ for each primal variable $j=1, 2$.\n\nAt the optimal primal solution $x^* = (2, 2)^{\\top}$:\n- Constraint $1$: $x_1^* + x_2^* = 2 + 2 = 4$. The constraint is binding (active).\n- Constraint $2$: $2x_1^* + x_2^* = 2(2) + 2 = 6$. The constraint is binding.\n- Constraint $3$: $x_1^* + 3x_2^* = 2 + 3(2) = 8  9$. The constraint is slack (inactive).\n\nFrom the first set of complementary slackness conditions, since the third primal constraint is slack, the corresponding dual variable must be zero: $y_3^*=0$.\n\nThe primal variables are both non-zero, $x_1^* = 2  0$ and $x_2^* = 2  0$. From the second set of conditions, this implies that the corresponding dual constraints must be binding:\n- Dual Constraint $1$: $y_1^* + 2y_2^* + y_3^* = 3$.\n- Dual Constraint $2$: $y_1^* + y_2^* + 3y_3^* = 2$.\n\nSubstituting $y_3^*=0$ into this system gives:\n1. $y_1^* + 2y_2^* = 3$\n2. $y_1^* + y_2^* = 2$\n\nSubtracting the second equation from the first yields $y_2^* = 1$. Substituting $y_2^*=1$ into the second equation gives $y_1^* + 1 = 2$, so $y_1^* = 1$.\nThe optimal dual solution is $y^* = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\n\nTo verify, we check the dual objective value: $b^{\\top}y^* = 4(1) + 6(1) + 9(0) = 10$. This matches the primal optimal value $v(b)$, as expected from the Strong Duality Theorem.\n\n**4. Analyzing the Perturbed Problem**\n\nWe are asked to compute the limit $\\lim_{\\delta \\to 0^{+}} \\frac{v(b(\\delta)) - v(b)}{\\delta}$.\nThe optimal value of the perturbed LP, $v(b(\\delta))$, is given by the optimal value of its dual:\n$$\nv(b(\\delta)) = \\min_{y \\in Y} (b(\\delta))^{\\top} y\n$$\nwhere $Y = \\{y \\in \\mathbb{R}^3 \\mid A^{\\top}y \\ge c, y \\ge 0\\}$ is the dual feasible set. The set $Y$ is independent of $b$ and $\\delta$.\nSubstituting $b(\\delta) = b + \\delta e_2$:\n$$\nv(b(\\delta)) = \\min_{y \\in Y} (b + \\delta e_2)^{\\top} y = \\min_{y \\in Y} (b^{\\top}y + \\delta y_2)\n$$\nLet $Y_{opt}(b) \\subseteq Y$ be the set of optimal solutions to the unperturbed dual problem. By definition, for any $y \\in Y_{opt}(b)$, we have $b^{\\top}y = v(b)$.\nFor any $y' \\notin Y_{opt}(b)$, we have $b^{\\top}y'  v(b)$.\nFor a sufficiently small $\\delta  0$, the term $\\delta y_2$ is a small perturbation to the objective function. An optimal solution to the perturbed problem must be sought from the set $Y_{opt}(b)$, because for any $y' \\notin Y_{opt}(b)$, the gap $b^{\\top}y' - v(b)  0$ will dominate the $\\delta$-term.\nThus, for small $\\delta  0$:\n$$\nv(b(\\delta)) = \\min_{y \\in Y_{opt}(b)} (b^{\\top}y + \\delta y_2) = \\min_{y \\in Y_{opt}(b)} (v(b) + \\delta y_2)\n$$\n$$\nv(b(\\delta)) = v(b) + \\delta \\left( \\min_{y \\in Y_{opt}(b)} y_2 \\right)\n$$\nRearranging and taking the limit:\n$$\n\\lim_{\\delta \\to 0^{+}} \\frac{v(b(\\delta)) - v(b)}{\\delta} = \\min_{y \\in Y_{opt}(b)} y_2\n$$\nIn our case, the system of equations for the dual variables yielded a unique solution, $y^* = (1, 1, 0)^{\\top}$. Therefore, the set of optimal dual solutions is a singleton: $Y_{opt}(b) = \\{y^*\\}$.\nThe expression for the derivative simplifies to:\n$$\n\\lim_{\\delta \\to 0^{+}} \\frac{v(b(\\delta)) - v(b)}{\\delta} = y_2^*\n$$\nFrom our calculation in part (3), we have $y_2^*=1$.\nThe required right-hand derivative is $1$. This value represents the shadow price of the second constraint, indicating the rate at which the optimal objective value increases as the right-hand side of this constraint is relaxed.", "answer": "$$\n\\boxed{1}\n$$", "id": "3165476"}, {"introduction": "Not all linear programs have a solution; some are infeasible due to conflicting constraints. This practice provides tools for diagnosing infeasibility, guiding you to identify the core conflict as an Irreducible Infeasible Subsystem (IIS). You will then use Farkas' Lemma to generate a formal certificate of infeasibility and explore how to minimally relax the system to restore feasibility, a crucial skill in practical model formulation [@problem_id:3165483].", "problem": "Consider the following feasibility Linear Program (LP) in variables $x \\in \\mathbb{R}^{2}$:\n- Constraints:\n  1) $x_{1} \\geq 1$\n  2) $x_{2} \\geq 1$\n  3) $x_{1} + x_{2} \\leq 1$\n  4) $x_{1} \\leq 2$\n  5) $x_{2} \\leq 2$\n\nYour tasks are:\n- Using only core definitions of feasibility for linear inequalities and the Alternative Theorem for linear inequalities (Farkas' Lemma), determine a subset of constraints that forms an Irreducible Infeasible Subsystem (IIS), where Irreducible Infeasible Subsystem (IIS) means a set of constraints that is infeasible but becomes feasible if any single constraint from the set is removed.\n- Identify a nonnegative vector of multipliers (a dual ray) that certifies infeasibility of that IIS by combining the inequalities into a contradiction of the form $0 \\leq \\text{negative}$, after placing all constraints into the unified form $A x \\leq b$.\n- Suppose you are allowed to minimally relax only the right-hand sides of the three IIS constraints to restore feasibility. Specifically, replace $x_{1} \\geq 1$, $x_{2} \\geq 1$, and $x_{1} + x_{2} \\leq 1$ by $x_{1} \\geq 1 - \\rho_{1}$, $x_{2} \\geq 1 - \\rho_{2}$, and $x_{1} + x_{2} \\leq 1 + \\rho_{3}$ with $\\rho_{1}, \\rho_{2}, \\rho_{3} \\geq 0$. Formulate the minimal-relaxation problem that finds the smallest total relaxation magnitude $s^{\\star} = \\rho_{1} + \\rho_{2} + \\rho_{3}$ such that the relaxed system is feasible, and then compute the exact value of $s^{\\star}$.\n\nReport only the value of $s^{\\star}$ as your final answer. No rounding is required.", "solution": "The problem asks for a three-part analysis of a given system of linear inequalities. First, we must identify an Irreducible Infeasible Subsystem (IIS). Second, we must provide a dual certificate of this infeasibility using Farkas' Lemma. Third, we must formulate and solve a problem to find the minimum total relaxation of the right-hand sides of the IIS constraints to restore feasibility.\n\nThe given system of linear inequalities in variables $x = (x_1, x_2) \\in \\mathbb{R}^{2}$ is:\n1) $x_{1} \\geq 1$\n2) $x_{2} \\geq 1$\n3) $x_{1} + x_{2} \\leq 1$\n4) $x_{1} \\leq 2$\n5) $x_{2} \\leq 2$\n\nPart 1: Identification of the Irreducible Infeasible Subsystem (IIS)\n\nAn IIS is a subset of constraints that is infeasible, but from which the removal of any single constraint renders the subset feasible.\nBy inspection, constraints ($1$), ($2$), and ($3$) appear to be in conflict. Let's analyze this subset.\nFrom constraint ($1$), we have $x_{1} \\geq 1$.\nFrom constraint ($2$), we have $x_{2} \\geq 1$.\nSumming these two inequalities yields $x_{1} + x_{2} \\geq 1 + 1 = 2$.\nHowever, constraint ($3$) states that $x_{1} + x_{2} \\leq 1$.\nThe combination of these three constraints leads to the contradiction $2 \\leq x_{1} + x_{2} \\leq 1$, which is impossible. Therefore, the subsystem consisting of constraints ($1$), ($2$), and ($3$) is infeasible.\n\nTo confirm this is an *irreducible* subsystem, we must show that removing any one of these three constraints makes the remaining system feasible.\n- Remove constraint ($1$): The system is $x_{2} \\geq 1$ and $x_{1} + x_{2} \\leq 1$. This implies $x_1 \\leq 1 - x_2$. Since $x_2 \\geq 1$, we have $1-x_2 \\leq 0$, so $x_1 \\leq 0$. A feasible point satisfying $x_2 \\geq 1$ and $x_1 \\leq 0$ is, for instance, $(x_{1}, x_{2}) = (0, 1)$. This point satisfies $x_2=1 \\geq 1$ and $x_1+x_2 = 0+1=1 \\leq 1$. The system is feasible.\n- Remove constraint ($2$): The system is $x_{1} \\geq 1$ and $x_{1} + x_{2} \\leq 1$. Symmetrically, this implies $x_2 \\leq 1 - x_1$. Since $x_1 \\geq 1$, we have $1-x_1 \\leq 0$, so $x_2 \\leq 0$. A feasible point is $(x_{1}, x_{2}) = (1, 0)$. This system is feasible.\n- Remove constraint ($3$): The system is $x_{1} \\geq 1$ and $x_{2} \\geq 1$. A feasible point is $(x_{1}, x_{2}) = (1, 1)$. This system is feasible.\n\nSince the subsystem $\\{1, 2, 3\\}$ is infeasible and removing any single constraint restores feasibility, it is an Irreducible Infeasible Subsystem (IIS).\n\nPart 2: Farkas' Lemma Certificate of Infeasibility\n\nFarkas' Lemma, in a form for linear inequalities, states that a system $Ax \\leq b$ is infeasible if and only if there exists a vector of multipliers $y \\geq 0$ such that $y^{T}A = 0$ and $y^{T}b  0$.\nFirst, we write the IIS constraints in the standard form $Ax \\leq b$:\n1) $x_{1} \\geq 1 \\implies -x_{1} \\leq -1$\n2) $x_{2} \\geq 1 \\implies -x_{2} \\leq -1$\n3) $x_{1} + x_{2} \\leq 1$\n\nThis gives the matrix representation:\n$$\nA = \\begin{pmatrix} -1  0 \\\\ 0  -1 \\\\ 1  1 \\end{pmatrix}, \\quad x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}, \\quad b = \\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\end{pmatrix}\n$$\nWe seek a vector $y = (y_1, y_2, y_3)^T$ with $y_1, y_2, y_3 \\geq 0$ satisfying the conditions of the lemma.\nThe condition $y^{T}A = 0$ translates to:\n$$\n\\begin{pmatrix} y_{1}  y_{2}  y_{3} \\end{pmatrix} \\begin{pmatrix} -1  0 \\\\ 0  -1 \\\\ 1  1 \\end{pmatrix} = \\begin{pmatrix} -y_{1} + y_{3}  -y_{2} + y_{3} \\end{pmatrix} = \\begin{pmatrix} 0  0 \\end{pmatrix}\n$$\nThis gives the system of equations:\n$-y_{1} + y_{3} = 0 \\implies y_{1} = y_{3}$\n$-y_{2} + y_{3} = 0 \\implies y_{2} = y_{3}$\nThus, we must have $y_{1} = y_{2} = y_{3}$. Since $y \\geq 0$ and we need a non-trivial certificate, let's choose the simplest positive multipliers, $y_{1} = y_{2} = y_{3} = 1$. So, a valid vector of multipliers (a generator for the dual ray) is $y = (1, 1, 1)^T$.\n\nNow we check the second condition, $y^{T}b  0$:\n$$\ny^{T}b = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\end{pmatrix} = (1)(-1) + (1)(-1) + (1)(1) = -1 - 1 + 1 = -1\n$$\nSince $-1  0$, the condition is satisfied. The vector $y = (1, 1, 1)^T$ is a valid set of Farkas multipliers that certifies the infeasibility of the IIS. Multiplying the inequalities in $Ax \\leq b$ by the corresponding components of $y$ and summing them gives the manifest contradiction $0 \\leq -1$.\n\nPart 3: Minimal Relaxation Problem\n\nWe are asked to relax the RHS of the IIS constraints to restore feasibility. The relaxed system is:\n1) $x_{1} \\geq 1 - \\rho_{1}$\n2) $x_{2} \\geq 1 - \\rho_{2}$\n3) $x_{1} + x_{2} \\leq 1 + \\rho_{3}$\nwhere $\\rho_{1}, \\rho_{2}, \\rho_{3} \\geq 0$. We wish to minimize the total relaxation $s = \\rho_{1} + \\rho_{2} + \\rho_{3}$.\n\nIn the standard form $Ax \\leq b'$, the relaxed right-hand-side vector $b'$ is:\n$$\nb' = \\begin{pmatrix} -1 + \\rho_{1} \\\\ -1 + \\rho_{2} \\\\ 1 + \\rho_{3} \\end{pmatrix}\n$$\nFor the relaxed system $Ax \\leq b'$ to be feasible, the Alternative Theorem (Farkas' Lemma) requires that for every $y \\geq 0$ with $y^T A = 0$, we must have $y^T b' \\geq 0$.\nFrom Part 2, the vectors $y$ satisfying these conditions are of the form $y = (k, k, k)^T$ for any $k \\geq 0$. To satisfy the feasibility condition for all such $y$, we only need to check it for the generator of the ray, $y = (1, 1, 1)^T$.\nThe condition is $y^T b' \\geq 0$:\n$$\n\\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} -1 + \\rho_{1} \\\\ -1 + \\rho_{2} \\\\ 1 + \\rho_{3} \\end{pmatrix} \\geq 0\n$$\n$$\n(-1 + \\rho_{1}) + (-1 + \\rho_{2}) + (1 + \\rho_{3}) \\geq 0\n$$\n$$\n-1 + \\rho_{1} + \\rho_{2} + \\rho_{3} \\geq 0\n$$\nThis simplifies to $\\rho_{1} + \\rho_{2} + \\rho_{3} \\geq 1$.\nThe minimal relaxation problem is thus to find the minimum of $s = \\rho_{1} + \\rho_{2} + \\rho_{3}$ subject to the constraints $\\rho_{1} + \\rho_{2} + \\rho_{3} \\geq 1$ and $\\rho_{1}, \\rho_{2}, \\rho_{3} \\geq 0$.\nThe objective function is $s$ itself, and the primary constraint is $s \\geq 1$. Therefore, the minimum possible value for $s$ is $1$.\nThe minimal total relaxation magnitude is $s^{\\star} = 1$. This can be achieved, for example, by setting $\\rho_3 = 1$ and $\\rho_1 = \\rho_2 = 0$, which makes the system $x_1 \\geq 1, x_2 \\geq 1, x_1+x_2 \\leq 2$ feasible at the point $(1,1)$.", "answer": "$$\\boxed{1}$$", "id": "3165483"}]}