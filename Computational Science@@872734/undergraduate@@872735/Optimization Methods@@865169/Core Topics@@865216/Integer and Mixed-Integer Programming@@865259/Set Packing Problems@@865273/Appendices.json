{"hands_on_practices": [{"introduction": "The set packing problem provides a powerful framework for resource allocation and scheduling. In this exercise, you will model a university exam scheduling scenario where resources like invigilators and rooms are limited, translating these constraints into the canonical set packing form $A x \\le \\mathbf{1}$. By solving a series of small, targeted instances, you will gain hands-on experience with how resource conflicts dictate feasible solutions and how to navigate the trade-offs between objective value and other metrics like cardinality [@problem_id:3181311].", "problem": "You are asked to model and solve a small, self-contained family of university exam scheduling instances using the framework of the set packing problem. The fundamental base is the core definition of set packing: given a universe of ground items and a family of subsets, select a subfamily of pairwise disjoint subsets that maximizes a given objective. In this setting, the ground items are resources (invigilators and rooms), and each subset represents one possible exam option that occupies a specific set of resources. The principle of disjointness is enforced using a binary incidence matrix and linear inequalities.\n\nLet the universe of resources be denoted by the finite set $U$, and let the index set of exam options be $S = \\{1,2,\\dots,m\\}$. For each option $s \\in S$, let $R(s) \\subseteq U$ denote the set of resources occupied by option $s$. Define an incidence matrix $A \\in \\{0,1\\}^{|U|\\times m}$ by\n$$\nA_{u,s} = \\begin{cases}\n1 & \\text{if } u \\in R(s),\\\\\n0 & \\text{otherwise.}\n\\end{cases}\n$$\nLet $x \\in \\{0,1\\}^m$ be the decision vector indicating which options are chosen, and let $w \\in \\mathbb{R}_+^m$ be a vector of nonnegative fairness weights. The weighted set packing model is\n$$\n\\max_{x \\in \\{0,1\\}^m} \\quad \\sum_{s=1}^m w_s x_s \\quad \\text{subject to} \\quad A x \\le \\mathbf{1},\n$$\nwhere $\\mathbf{1}$ is the all-ones vector in $\\mathbb{R}^{|U|}$ and the inequality is taken componentwise. The constraint $A x \\le \\mathbf{1}$ ensures that no resource $u \\in U$ is used by more than one selected option, i.e., selected options are pairwise resource-disjoint. The fairness weights $w_s$ encode the relative desirability of selecting each option.\n\nYour program must, for each test instance below, compute the optimal objective value $\\sum_{s=1}^m w_s x_s$ over all $x$ satisfying $A x \\le \\mathbf{1}$ and $x \\in \\{0,1\\}^m$. In case of ties in the optimal objective value, break ties by preferring solutions with larger cardinality $\\sum_{s=1}^m x_s$. If there is still a tie, prefer the lexicographically smallest index set of selected options.\n\nTest Suite. Use exactly the following four instances. For each instance, an explicit universe $U$, the family of resource-occupying sets $\\{R(s)\\}_{s=1}^m$, and the fairness weights $w$ are provided. The resource labels are purely symbolic; treat equality of labels as the identity of resources.\n\n- Instance $1$ (happy path with meaningful overlap across rooms and invigilators):\n  - Universe $U^{(1)} = \\{I_1, I_2, I_3, R_{101}, R_{102}\\}$.\n  - Options and required resources:\n    - $s=1$: $R^{(1)}(1) = \\{I_1, I_2, R_{101}\\}$, weight $w^{(1)}_1 = 8.0$.\n    - $s=2$: $R^{(1)}(2) = \\{I_2, I_3, R_{101}\\}$, weight $w^{(1)}_2 = 7.5$.\n    - $s=3$: $R^{(1)}(3) = \\{I_1, I_3, R_{102}\\}$, weight $w^{(1)}_3 = 6.0$.\n    - $s=4$: $R^{(1)}(4) = \\{I_2, R_{102}\\}$, weight $w^{(1)}_4 = 4.5$.\n    - $s=5$: $R^{(1)}(5) = \\{I_3, R_{102}\\}$, weight $w^{(1)}_5 = 5.0$.\n    - $s=6$: $R^{(1)}(6) = \\{I_1, R_{101}\\}$, weight $w^{(1)}_6 = 4.0$.\n- Instance $2$ (boundary case: high fairness options share invigilators, allowing at most one selection):\n  - Universe $U^{(2)} = \\{I_1, I_2, R_{101}, R_{102}, R_{103}\\}$.\n  - Options and required resources:\n    - $s=1$: $R^{(2)}(1) = \\{I_1, I_2, R_{101}\\}$, weight $w^{(2)}_1 = 9.0$.\n    - $s=2$: $R^{(2)}(2) = \\{I_1, I_2, R_{102}\\}$, weight $w^{(2)}_2 = 9.5$.\n    - $s=3$: $R^{(2)}(3) = \\{I_1, I_2, R_{103}\\}$, weight $w^{(2)}_3 = 9.2$.\n- Instance $3$ (edge case: all options are mutually disjoint, selecting all is feasible):\n  - Universe $U^{(3)} = \\{I_1, I_2, I_3, I_4, R_{101}, R_{102}, R_{103}, R_{104}\\}$.\n  - Options and required resources:\n    - $s=1$: $R^{(3)}(1) = \\{I_1, R_{101}\\}$, weight $w^{(3)}_1 = 2.5$.\n    - $s=2$: $R^{(3)}(2) = \\{I_2, R_{102}\\}$, weight $w^{(3)}_2 = 2.5$.\n    - $s=3$: $R^{(3)}(3) = \\{I_3, R_{103}\\}$, weight $w^{(3)}_3 = 2.5$.\n    - $s=4$: $R^{(3)}(4) = \\{I_4, R_{104}\\}$, weight $w^{(3)}_4 = 2.5$.\n- Instance $4$ (tie case illustrating feasibility versus optimality tension and tie-breaking by cardinality):\n  - Universe $U^{(4)} = \\{I_1, I_2, I_3, R_{101}, R_{102}, R_{103}\\}$.\n  - Options and required resources:\n    - $s=1$: $R^{(4)}(1) = \\{I_1, R_{101}\\}$, weight $w^{(4)}_1 = 4.0$.\n    - $s=2$: $R^{(4)}(2) = \\{I_2, R_{102}\\}$, weight $w^{(4)}_2 = 4.0$.\n    - $s=3$: $R^{(4)}(3) = \\{I_1, I_2, R_{103}\\}$, weight $w^{(4)}_3 = 8.0$.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each instance $k \\in \\{1,2,3,4\\}$, output a two-element list containing the optimal objective value and the number of selected options, in that order. The final output must therefore have the form\n$$\n\\big[ [\\text{opt}^{(1)}, \\text{card}^{(1)}], [\\text{opt}^{(2)}, \\text{card}^{(2)}], [\\text{opt}^{(3)}, \\text{card}^{(3)}], [\\text{opt}^{(4)}, \\text{card}^{(4)}] \\big],\n$$\nwith no spaces in the printed line. No physical units or angle units are involved; all numerical results are dimensionless real numbers and integers as appropriate.", "solution": "The problem requires solving a series of weighted set packing instances, a classic problem in combinatorial optimization. The problem is formally defined as an integer linear program:\n$$\n\\max_{x \\in \\{0,1\\}^m} \\quad \\sum_{s=1}^m w_s x_s \\quad \\text{subject to} \\quad A x \\le \\mathbf{1}\n$$\nwhere $x$ is a binary decision vector indicating which of the $m$ options are selected, $w$ is a vector of non-negative weights, and the constraint $A x \\le \\mathbf{1}$ ensures that no resource is used more than once. The matrix $A$ is an incidence matrix where $A_{u,s} = 1$ if resource $u$ is used by option $s$, and $A_{u,s} = 0$ otherwise. The inequality $A x \\le \\mathbf{1}$ is equivalent to stating that for any two selected options $s_i$ and $s_j$ (where $x_i=1$ and $x_j=1$), their respective resource sets $R(s_i)$ and $R(s_j)$ must be disjoint, i.e., $R(s_i) \\cap R(s_j) = \\emptyset$.\n\nThe task is to find a feasible solution $x$ that maximizes the objective function $\\sum w_s x_s$. Since the decision vector $x$ is in $\\{0,1\\}^m$, the set of all possible choices of $x$ is finite, with size $2^m$. For the given instances, the number of options $m$ is small ($m \\le 6$), which makes an exhaustive search of all $2^m$ possible subsets of options computationally feasible.\n\nThe solution algorithm proceeds by iterating through every possible subset of the available options for each instance. For each subset, we perform the following steps:\n$1$. **Feasibility Check**: A subset of options is feasible if and only if all pairs of options within the subset are mutually disjoint in their resource requirements. This is checked by verifying that for any two distinct options $s_i$ and $s_j$ in the subset, the intersection of their resource sets, $R(s_i) \\cap R(s_j)$, is the empty set.\n\n$2$. **Evaluation**: If a subset is found to be feasible, its objective value is calculated by summing the weights $w_s$ of all options $s$ in the subset. The cardinality is simply the number of options in the subset.\n\n$3$. **Selection of Optimal Solution**: The algorithm maintains a record of the best solution found so far. A new feasible solution replaces the current best solution if it is superior according to the specified hierarchical tie-breaking rules:\n    a. The new solution has a strictly greater objective value $\\sum w_s x_s$.\n    b. If the objective values are equal, the new solution has a strictly greater cardinality $\\sum x_s$.\n    c. If both objective value and cardinality are equal, the new solution's set of option indices is lexicographically smaller than the current best.\n\nThis procedure guarantees finding the unique optimal solution as defined by the problem statement. We apply this algorithm to each of the four instances.\n\n**Instance 1**: $m=6$, so there are $2^6=64$ subsets to check. After examining all feasible subsets, the one maximizing the objective is $\\{s=1, s=5\\}$.\n- Resource sets: $R^{(1)}(1) = \\{I_1, I_2, R_{101}\\}$ and $R^{(1)}(5) = \\{I_3, R_{102}\\}$. Their intersection is empty, so the choice is feasible.\n- Objective value: $w^{(1)}_1 + w^{(1)}_5 = 8.0 + 5.0 = 13.0$.\n- Cardinality: $2$.\nThis yields a higher objective value than any other feasible subset. For instance, the next best feasible packing by value is $\\{s=5, s=6\\}$ with value $9.5$. Thus, the optimal value is $13.0$ with a cardinality of $2$.\n\n**Instance 2**: $m=3$. The options are $s=1, s=2, s=3$. All options share resources $\\{I_1, I_2\\}$.\n- $R^{(2)}(1) \\cap R^{(2)}(2) = \\{I_1, I_2\\} \\ne \\emptyset$.\n- $R^{(2)}(1) \\cap R^{(2)}(3) = \\{I_1, I_2\\} \\ne \\emptyset$.\n- $R^{(2)}(2) \\cap R^{(2)}(3) = \\{I_1, I_2\\} \\ne \\emptyset$.\nTherefore, no two options can be selected simultaneously. Feasible solutions can contain at most one option. The optimal solution is to select the single option with the highest weight, which is $s=2$ with $w^{(2)}_2 = 9.5$.\n- Optimal value: $9.5$.\n- Cardinality: $1$.\n\n**Instance 3**: $m=4$. The resource sets for the four options are all pairwise disjoint.\n- $R^{(3)}(i) \\cap R^{(3)}(j) = \\emptyset$ for all $i \\ne j$.\nThis means any subset of options is feasible. Since all weights $w_s = 2.5$ are positive, the optimal solution is to select all four options.\n- Objective value: $w^{(3)}_1+w^{(3)}_2+w^{(3)}_3+w^{(3)}_4 = 4 \\times 2.5 = 10.0$.\n- Cardinality: $4$.\n\n**Instance 4**: $m=3$. The resource sets are $R^{(4)}(1)=\\{I_1, R_{101}\\}$, $R^{(4)}(2)=\\{I_2, R_{102}\\}$, and $R^{(4)}(3)=\\{I_1, I_2, R_{103}\\}$.\n- Conflicts: $R^{(4)}(1) \\cap R^{(4)}(3) = \\{I_1\\}$, $R^{(4)}(2) \\cap R^{(4)}(3) = \\{I_2\\}$.\n- Feasible pair: $R^{(4)}(1) \\cap R^{(4)}(2) = \\emptyset$.\nThe feasible subsets (packings) are $\\{s=1\\}$, $\\{s=2\\}$, $\\{s=3\\}$, and $\\{s=1, s=2\\}$. Let's evaluate them:\n- $\\{s=1\\}$: value = $4.0$, cardinality = $1$.\n- $\\{s=2\\}$: value = $4.0$, cardinality = $1$.\n- $\\{s=3\\}$: value = $8.0$, cardinality = $1$.\n- $\\{s=1, s=2\\}$: value = $w^{(4)}_1 + w^{(4)}_2 = 4.0 + 4.0 = 8.0$, cardinality = $2$.\nThe maximum objective value is $8.0$, achieved by both $\\{s=3\\}$ and $\\{s=1, s=2\\}$. According to the tie-breaking rule, we prefer the one with the larger cardinality. The set $\\{s=1, s=2\\}$ has cardinality $2$, while $\\{s=3\\}$ has cardinality $1$.\n- Optimal value: $8.0$.\n- Cardinality: $2$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Models and solves the set packing instances described in the problem.\n    \"\"\"\n    \n    test_cases = [\n        # Instance 1\n        {\n            \"U\": {\"I_1\", \"I_2\", \"I_3\", \"R_101\", \"R_102\"},\n            \"options\": [\n                (8.0, {\"I_1\", \"I_2\", \"R_101\"}),\n                (7.5, {\"I_2\", \"I_3\", \"R_101\"}),\n                (6.0, {\"I_1\", \"I_3\", \"R_102\"}),\n                (4.5, {\"I_2\", \"R_102\"}),\n                (5.0, {\"I_3\", \"R_102\"}),\n                (4.0, {\"I_1\", \"R_101\"}),\n            ]\n        },\n        # Instance 2\n        {\n            \"U\": {\"I_1\", \"I_2\", \"R_101\", \"R_102\", \"R_103\"},\n            \"options\": [\n                (9.0, {\"I_1\", \"I_2\", \"R_101\"}),\n                (9.5, {\"I_1\", \"I_2\", \"R_102\"}),\n                (9.2, {\"I_1\", \"I_2\", \"R_103\"}),\n            ]\n        },\n        # Instance 3\n        {\n            \"U\": {\"I_1\", \"I_2\", \"I_3\", \"I_4\", \"R_101\", \"R_102\", \"R_103\", \"R_104\"},\n            \"options\": [\n                (2.5, {\"I_1\", \"R_101\"}),\n                (2.5, {\"I_2\", \"R_102\"}),\n                (2.5, {\"I_3\", \"R_103\"}),\n                (2.5, {\"I_4\", \"R_104\"}),\n            ]\n        },\n        # Instance 4\n        {\n            \"U\": {\"I_1\", \"I_2\", \"I_3\", \"R_101\", \"R_102\", \"R_103\"},\n            \"options\": [\n                (4.0, {\"I_1\", \"R_101\"}),\n                (4.0, {\"I_2\", \"R_102\"}),\n                (8.0, {\"I_1\", \"I_2\", \"R_103\"}),\n            ]\n        }\n    ]\n\n    def solve_instance(options):\n        \"\"\"\n        Solves a single weighted set packing instance by exhaustive search.\n        \n        Args:\n            options: A list of tuples, where each tuple contains the weight (float)\n                     and the set of required resources (set of strings).\n\n        Returns:\n            A list containing the optimal objective value and the cardinality of the\n            optimal set.\n        \"\"\"\n        num_options = len(options)\n        \n        # Best solution found so far, using (objective, cardinality, index_set)\n        # We initialize objective to a value lower than any possible sum of non-negative weights.\n        best_solution = (-1.0, -1, [])\n\n        # Iterate through all 2^m subsets of options using a bitmask\n        for i in range(1  num_options):\n            current_options_indices = []\n            for j in range(num_options):\n                if (i >> j)  1:\n                    current_options_indices.append(j)\n\n            # Check feasibility of the current subset\n            is_feasible = True\n            current_obj_val = 0.0\n            used_resources = set()\n\n            for idx in current_options_indices:\n                weight, resources = options[idx]\n                if not resources.isdisjoint(used_resources):\n                    is_feasible = False\n                    break\n                used_resources.update(resources)\n                current_obj_val += weight\n\n            if is_feasible:\n                current_cardinality = len(current_options_indices)\n                # Option indices are 1-based for lexicographical comparison\n                current_index_set = [idx + 1 for idx in current_options_indices]\n\n                # Tie-breaking logic\n                # 1. Higher objective value is better\n                if current_obj_val > best_solution[0]:\n                    best_solution = (current_obj_val, current_cardinality, current_index_set)\n                elif current_obj_val == best_solution[0]:\n                    # 2. Higher cardinality is better\n                    if current_cardinality > best_solution[1]:\n                        best_solution = (current_obj_val, current_cardinality, current_index_set)\n                    elif current_cardinality == best_solution[1]:\n                        # 3. Lexicographically smaller index set is better\n                        if current_index_set  best_solution[2]:\n                            best_solution = (current_obj_val, current_cardinality, current_index_set)\n        \n        return [best_solution[0], best_solution[1]]\n\n    results = []\n    for case in test_cases:\n        result = solve_instance(case[\"options\"])\n        results.append(result)\n\n    # Format the final output string to be exactly as specified, with no spaces.\n    list_of_strings = [f\"[{val},{card}]\" for val, card in results]\n    final_output_string = f\"[{','.join(list_of_strings)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "3181311"}, {"introduction": "While finding the exact optimal solution to a set packing problem is computationally hard, simple heuristics are often used in practice. This exercise challenges you to analyze the performance of a common greedy-by-weight algorithm by designing an adversarial instance where it performs poorly. This practice is fundamental to the theory of approximation algorithms, as it requires you to rigorously quantify the gap between the heuristic solution and the true optimum, revealing the worst-case performance guarantees of the algorithm [@problem_id:3181347].", "problem": "Consider the weighted set packing problem: given a finite ground set $U$ and a family $\\mathcal{F} \\subseteq 2^{U}$ of subsets with nonnegative weights $\\{w(S)\\}_{S \\in \\mathcal{F}}$, the goal is to select a subfamily $\\mathcal{P} \\subseteq \\mathcal{F}$ of pairwise-disjoint sets that maximizes $\\sum_{S \\in \\mathcal{P}} w(S)$. A simple greedy-by-weight algorithm iteratively selects a remaining set $S \\in \\mathcal{F}$ of maximum weight and deletes all sets that intersect $S$, repeating until no sets remain.\n\nDesign and analyze an adversarial instance, restricted to sets of sizes $2$ and $3$, that causes the greedy-by-weight algorithm to select a suboptimal solution. Your instance should be composed of $m \\in \\mathbb{N}$ identical gadgets, each gadget $i \\in \\{1, \\dots, m\\}$ built on six distinct elements $\\{a_{i}, b_{i}, x_{i}, y_{i}, u_{i}, v_{i}\\}$ with three sets:\n- a $2$-set $S_{i}^{(2)} = \\{a_{i}, b_{i}\\}$ of weight $w(S_{i}^{(2)}) = w + \\varepsilon$,\n- two disjoint $3$-sets $S_{i,1}^{(3)} = \\{a_{i}, x_{i}, y_{i}\\}$ and $S_{i,2}^{(3)} = \\{b_{i}, u_{i}, v_{i}\\}$, each of weight $w(S_{i,1}^{(3)}) = w(S_{i,2}^{(3)}) = w$,\nwhere $w  0$ and $\\varepsilon \\in (0, w)$, and gadgets are disjoint across different indices $i$.\n\nStarting from the core definitions of set packing and the greedy-by-weight procedure, rigorously justify that the greedy-by-weight algorithm applied to your instance selects the $m$ sets $\\{S_{i}^{(2)}\\}_{i=1}^{m}$ and that an optimal solution selects the $2m$ sets $\\{S_{i,1}^{(3)}, S_{i,2}^{(3)}\\}_{i=1}^{m}$. Then compute the approximation ratio\n$$\\rho(m, w, \\varepsilon) = \\frac{\\text{total weight of greedy solution}}{\\text{total weight of optimal solution}}$$\nas a function of $m$, $w$, and $\\varepsilon$, and take the limit as $\\varepsilon \\to 0^{+}$ to obtain the limiting worst-case value induced by this adversarial construction. Finally, interpret how this limiting value compares to what one would expect when generalizing the same construction idea to $k$-set packing, where each set has size at most $k$.\n\nReport the limiting worst-case approximation ratio as a single exact number. No rounding is required. No physical units are involved.", "solution": "The problem asks for an analysis of a specific adversarial instance for the weighted set packing problem under a greedy-by-weight heuristic. The instance is composed of $m \\in \\mathbb{N}$ identical and disjoint gadgets.\n\nFirst, let us formalize the problem instance. The ground set is $U = \\bigcup_{i=1}^{m} U_{i}$, where each $U_i = \\{a_{i}, b_{i}, x_{i}, y_{i}, u_{i}, v_{i}\\}$ is the set of elements for gadget $i$. Since the gadgets are disjoint, we have $|U| = 6m$. The family of sets is $\\mathcal{F} = \\bigcup_{i=1}^{m} \\mathcal{F}_i$, where $\\mathcal{F}_i = \\{S_{i}^{(2)}, S_{i,1}^{(3)}, S_{i,2}^{(3)}\\}$. The sets and their weights are given as:\n- $S_{i}^{(2)} = \\{a_{i}, b_{i}\\}$, with weight $w(S_{i}^{(2)}) = w + \\varepsilon$.\n- $S_{i,1}^{(3)} = \\{a_{i}, x_{i}, y_{i}\\}$, with weight $w(S_{i,1}^{(3)}) = w$.\n- $S_{i,2}^{(3)} = \\{b_{i}, u_{i}, v_{i}\\}$, with weight $w(S_{i,2}^{(3)}) = w$.\n\nThe parameters are constrained by $w  0$ and $\\varepsilon \\in (0, w)$.\n\nWe begin by analyzing the behavior of the greedy-by-weight algorithm on this instance $\\mathcal{F}$. The algorithm iteratively selects a set with the maximum weight among all currently available sets.\nAt the initial step, the set of available sets is $\\mathcal{F}$. The weights of the sets in $\\mathcal{F}$ are either $w$ or $w+\\varepsilon$. Since $\\varepsilon  0$, the maximum weight is $w+\\varepsilon$. This weight is associated with the $m$ sets of the form $S_{i}^{(2)}$.\n\nThe algorithm will select one of these maximum-weight sets. Let us assume, without loss of generality, that it selects $S_{1}^{(2)}$ in the first iteration. The weight of this set is $w(S_{1}^{(2)}) = w + \\varepsilon$. Upon selecting $S_{1}^{(2)}$, the algorithm removes $S_{1}^{(2)}$ itself and all other sets in $\\mathcal{F}$ that have a non-empty intersection with $S_{1}^{(2)} = \\{a_1, b_1\\}$. Let us identify these sets:\n- $S_{1,1}^{(3)} = \\{a_1, x_1, y_1\\}$ intersects $S_{1}^{(2)}$ because $a_1 \\in S_{1}^{(2)} \\cap S_{1,1}^{(3)}$.\n- $S_{1,2}^{(3)} = \\{b_1, u_1, v_1\\}$ intersects $S_{1}^{(2)}$ because $b_1 \\in S_{1}^{(2)} \\cap S_{1,2}^{(3)}$.\n\nBy construction, sets from different gadgets $i \\neq j$ are disjoint. Therefore, for $j \\neq 1$, $S_{j}^{(2)}$, $S_{j,1}^{(3)}$, and $S_{j,2}^{(3)}$ do not intersect $S_{1}^{(2)}$.\nAfter the first iteration, the set of remaining available sets is $\\mathcal{F}' = \\bigcup_{i=2}^{m} \\mathcal{F}_i$. The problem is now reduced to the same initial structure but with $m-1$ gadgets.\n\nBy induction, the greedy algorithm will proceed to select $S_{2}^{(2)}$, then $S_{3}^{(2)}$, and so on, until $S_{m}^{(2)}$. At each step $i$, selecting $S_{i}^{(2)}$ causes the removal of $S_{i,1}^{(3)}$ and $S_{i,2}^{(3)}$. The final solution produced by the greedy algorithm is the set of pairwise-disjoint sets $\\mathcal{P}_{G} = \\{S_{1}^{(2)}, S_{2}^{(2)}, \\dots, S_{m}^{(2)}\\}$.\nThe total weight of this greedy solution, $W_G$, is the sum of the weights of the selected sets:\n$$W_G = \\sum_{i=1}^{m} w(S_{i}^{(2)}) = \\sum_{i=1}^{m} (w + \\varepsilon) = m(w + \\varepsilon)$$\n\nNext, we must determine the optimal solution. An optimal solution is a subfamily $\\mathcal{P}_{OPT} \\subseteq \\mathcal{F}$ of pairwise-disjoint sets that maximizes total weight. Since the gadgets are disjoint, the overall optimization problem decouples into $m$ independent subproblems, one for each gadget. The total optimal weight is $m$ times the optimal weight for a single gadget.\nLet's analyze a single-gadget subproblem on $\\mathcal{F}_i$. The sets are $S_{i}^{(2)}$, $S_{i,1}^{(3)}$, and $S_{i,2}^{(3)}$. We seek a maximum-weight packing from these three sets.\nThe possible maximal packings for gadget $i$ are:\n$1$. $\\{S_{i}^{(2)}\\}$: We cannot include $S_{i,1}^{(3)}$ or $S_{i,2}^{(3)}$ as they both intersect $S_{i}^{(2)}$. The weight of this packing is $w(S_{i}^{(2)}) = w + \\varepsilon$.\n$2$. $\\{S_{i,1}^{(3)}, S_{i,2}^{(3)}\\}$: These two sets are disjoint, as $S_{i,1}^{(3)} \\cap S_{i,2}^{(3)} = \\{a_{i}, x_{i}, y_{i}\\} \\cap \\{b_{i}, u_{i}, v_{i}\\} = \\emptyset$. The weight of this packing is $w(S_{i,1}^{(3)}) + w(S_{i,2}^{(3)}) = w + w = 2w$.\n\nTo find the optimal solution for the gadget, we compare the weights of these two packings. We are given that $\\varepsilon \\in (0, w)$, which implies $\\varepsilon  w$. Therefore, $w + \\varepsilon  w + w = 2w$.\nThe optimal packing for a single gadget $i$ is $\\{S_{i,1}^{(3)}, S_{i,2}^{(3)}\\}$ with a total weight of $2w$.\nThe overall optimal solution is the union of the optimal solutions for each gadget:\n$\\mathcal{P}_{OPT} = \\bigcup_{i=1}^{m} \\{S_{i,1}^{(3)}, S_{i,2}^{(3)}\\}$.\nThe total weight of the optimal solution, $W_{OPT}$, is:\n$$W_{OPT} = \\sum_{i=1}^{m} (w(S_{i,1}^{(3)}) + w(S_{i,2}^{(3)})) = \\sum_{i=1}^{m} (w + w) = 2mw$$\n\nNow, we can compute the approximation ratio $\\rho(m, w, \\varepsilon)$ as the ratio of the greedy solution's weight to the optimal solution's weight:\n$$\\rho(m, w, \\varepsilon) = \\frac{W_G}{W_{OPT}} = \\frac{m(w + \\varepsilon)}{2mw} = \\frac{w + \\varepsilon}{2w} = \\frac{1}{2} + \\frac{\\varepsilon}{2w}$$\nAs a check, we note that the ratio is independent of $m$, as expected from the repetitive structure of the instance.\n\nThe problem requires the limiting worst-case value of this ratio as $\\varepsilon \\to 0^{+}$.\n$$\\lim_{\\varepsilon \\to 0^{+}} \\rho(m, w, \\varepsilon) = \\lim_{\\varepsilon \\to 0^{+}} \\left(\\frac{1}{2} + \\frac{\\varepsilon}{2w}\\right)$$\nSince $w  0$ is a fixed positive constant, the term $\\frac{\\varepsilon}{2w}$ approaches $0$ as $\\varepsilon \\to 0^{+}$.\n$$\\lim_{\\varepsilon \\to 0^{+}} \\rho(m, w, \\varepsilon) = \\frac{1}{2} + 0 = \\frac{1}{2}$$\n\nFinally, we interpret this result. The instance is an example of $k$-set packing where the maximum set size is $k=3$. The greedy-by-weight algorithm is tempted by a set $S_i^{(2)}$ of size $2$ with a slightly higher weight $w+\\varepsilon$. Selecting this \"bait\" set prevents the selection of two disjoint sets, $S_{i,1}^{(3)}$ and $S_{i,2}^{(3)}$, each of size $3$ and weight $w$. The optimal solution for the gadget consists of these two sets, with total weight $2w$. The limiting ratio of greedy-to-optimal weight is $w/(2w) = 1/2$.\nThis construction is a canonical example used to establish lower bounds on the approximation ratio of greedy algorithms. For a generalized $k$-set packing problem (maximum set size $k$), a similar gadget can be constructed. One would use a \"bait\" set of size $k-1$ with weight $w+\\varepsilon$ that intersects $k-1$ disjoint sets of size $k$, each with weight $w$. The greedy algorithm would choose the bait set for a total weight of $w+\\varepsilon$. The optimal solution would consist of the $k-1$ larger sets, for a total weight of $(k-1)w$. The resulting approximation ratio would approach $\\frac{w}{(k-1)w} = \\frac{1}{k-1}$ as $\\varepsilon \\to 0^{+}$.\nIn our problem, $k=3$, so the generalized formula predicts a worst-case ratio of $\\frac{1}{3-1} = \\frac{1}{2}$. Our calculated limiting value matches this expectation perfectly.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3181347"}, {"introduction": "Many real-world set packing applications, like the cutting stock problem, involve an enormous number of potential sets, making the corresponding linear program too large to solve directly. This advanced practice introduces column generation, a technique to solve such massive LPs by generating variables (columns) only as they are needed. You will derive the pricing subproblem using LP duality—discovering its structure as a classic knapsack problem—and implement the full algorithm to solve the LP relaxation efficiently [@problem_id:3181317].", "problem": "Consider a cutting stock variant cast as a set packing model. There is a finite set of items indexed by $j \\in \\{1,\\dots,n\\}$, each item $j$ has an integer length $l_j \\in \\mathbb{Z}_{\\ge 0}$ and a profit $c_j \\in \\mathbb{R}$. A single resource (a bin or stock roll) has a fixed integer capacity $W \\in \\mathbb{Z}_{\\ge 0}$. A pattern $s$ is any subset of items $S \\subseteq \\{1,\\dots,n\\}$ whose total length respects the capacity, that is, $\\sum_{j \\in S} l_j \\le W$. Let $v_s = \\sum_{j \\in S} c_j$ denote the profit of pattern $s$. The set packing relaxation introduces one decision variable $x_s \\ge 0$ for each feasible pattern $s$, and enforces that no item appears in more than one selected pattern by the constraints $\\sum_{s: j \\in s} x_s \\le 1$ for every item $j$. The objective is to maximize the total profit $\\sum_s v_s x_s$.\n\nStarting only from the fundamental definition of the set packing relaxation and the standard linear programming duality, derive why the column generation pricing subproblem for this model is equivalent to a $0$-$1$ knapsack problem on the items. Specifically, show how the reduced cost of a pattern is constructed from the dual variables of the current restricted master problem, and why the pattern with maximum positive reduced cost can be found by selecting a subset of items whose total length does not exceed $W$ to maximize an adjusted sum of item values.\n\nThen, implement a column generation algorithm that:\n- Initializes with no patterns in the restricted master problem.\n- Iteratively solves the dual of the current restricted master problem to obtain dual variables for the item constraints.\n- Solves the pricing subproblem as a $0$-$1$ knapsack to find a single new pattern with strictly positive reduced cost. If none exists, terminates.\n- After termination, solves the final restricted master problem (the primal form) to obtain the optimal objective value of the relaxation.\n\nYour implementation must adhere to the following requirements:\n- Treat all lengths $l_j$ and capacity $W$ as integers, and solve the pricing subproblem using a dynamic programming algorithm for the $0$-$1$ knapsack.\n- Solve linear programs using a standard method compliant with linear programming duality.\n- Use a stopping tolerance for reduced cost of $\\varepsilon = 10^{-9}$.\n- When the restricted master has no columns, define the dual variables as the zero vector and proceed to pricing.\n\nTest Suite:\nProvide outputs for the following three instances, stated in purely mathematical terms:\n1. Case A: $W = 10$, item lengths $[2,3,5,7]$, item profits $[6,5,9,8]$.\n2. Case B: $W = 9$, item lengths $[2,3,4]$, item profits $[5,5,5]$.\n3. Case C: $W = 10$, item lengths $[11,2,9]$, item profits $[100,4,7]$.\n\nAnswer Specification:\n- For each test case, return the optimal objective value of the final restricted master problem (linear relaxation), rounded to six decimals.\n- Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, for example, `[`v_A`, `v_B`, `v_C`]`, where `v_A`, `v_B`, and `v_C` are the rounded objective values corresponding to Cases A, B, and C, respectively.", "solution": "The task is to derive the pricing subproblem for a set packing formulation of a cutting stock problem and then implement a column generation algorithm to solve its linear relaxation.\n\n### Derivation of the Pricing Subproblem\n\nLet $\\mathcal{P}$ denote the set of all feasible patterns. A pattern $s$ is a subset of items, which can be represented by a binary vector $a_s \\in \\{0, 1\\}^n$, where the $j$-th component $(a_s)_j = 1$ if item $j$ is in pattern $s$, and $0$ otherwise. The feasibility condition for a pattern is $\\sum_{j=1}^n l_j (a_s)_j \\le W$. The profit of pattern $s$ is $v_s = \\sum_{j=1}^n c_j (a_s)_j$.\n\nThe full linear programming relaxation, also known as the Master Problem (MP), is formulated as follows:\n$$\n\\begin{align*}\n\\text{(Primal MP)} \\quad \\max \\quad  \\sum_{s \\in \\mathcal{P}} v_s x_s \\\\\n\\text{s.t.} \\quad  \\sum_{s \\in \\mathcal{P}} (a_s)_j x_s \\le 1, \\quad \\forall j \\in \\{1, \\dots, n\\} \\\\\n x_s \\ge 0, \\quad \\forall s \\in \\mathcal{P}\n\\end{align*}\n$$\nThe constraint $\\sum_{s \\in \\mathcal{P}} (a_s)_j x_s \\le 1$ ensures that the total fraction of patterns containing item $j$ does not exceed $1$, effectively meaning that item $j$ is used at most once across all selected patterns.\n\nThe number of feasible patterns $|\\mathcal{P}|$ can be astronomically large, making it impossible to solve the MP directly by enumerating all variables $x_s$. Column generation is a method to solve such LPs by working with a small, manageable subset of columns (patterns) at a time. This subset of patterns, denoted $\\mathcal{P}' \\subset \\mathcal{P}$, defines the Restricted Master Problem (RMP).\n\nThe Primal RMP is:\n$$\n\\begin{align*}\n\\text{(Primal RMP)} \\quad \\max \\quad  \\sum_{s \\in \\mathcal{P}'} v_s x_s \\\\\n\\text{s.t.} \\quad  \\sum_{s \\in \\mathcal{P}'} (a_s)_j x_s \\le 1, \\quad \\forall j \\in \\{1, \\dots, n\\} \\\\\n x_s \\ge 0, \\quad \\forall s \\in \\mathcal{P}'\n\\end{align*}\n$$\nBy standard linear programming duality, we can formulate the dual of the RMP. Let $\\pi_j \\ge 0$ be the dual variable associated with the constraint for item $j$.\n\nThe Dual RMP is:\n$$\n\\begin{align*}\n\\text{(Dual RMP)} \\quad \\min \\quad  \\sum_{j=1}^n \\pi_j \\\\\n\\text{s.t.} \\quad  \\sum_{j=1}^n (a_s)_j \\pi_j \\ge v_s, \\quad \\forall s \\in \\mathcal{P}' \\\\\n \\pi_j \\ge 0, \\quad \\forall j \\in \\{1, \\dots, n\\}\n\\end{align*}\n$$\nAfter solving the current RMP (typically its dual form, to obtain the dual variables $\\pi_j$), we must determine if the current solution is optimal for the full MP. According to the criteria of the simplex method, the current solution is optimal if and only if all non-basic variables have non-positive reduced costs. In the context of column generation, the non-basic variables are the $x_s$ for all patterns $s \\in \\mathcal{P} \\setminus \\mathcal{P}'$.\n\nThe reduced cost, $\\bar{c}_s$, of a variable $x_s$ is given by its objective function coefficient minus the cost of its resource consumption, where the resource costs are given by the optimal dual variables $\\pi^*_j$ of the current RMP. For a pattern $s$, the resource consumption is encoded by the column $a_s$.\n\nThe reduced cost of a pattern $s$ is:\n$$\n\\bar{c}_s = v_s - \\sum_{j=1}^n (a_s)_j \\pi_j^*\n$$\nTo find a variable that could potentially improve the objective function, we must search for a pattern $s$ with a strictly positive reduced cost, $\\bar{c}_s  0$. The column generation algorithm's pricing subproblem is the task of finding the pattern with the maximum possible reduced cost:\n$$\n\\max_{s \\in \\mathcal{P}} \\bar{c}_s = \\max_{s \\in \\mathcal{P}} \\left( v_s - \\sum_{j=1}^n (a_s)_j \\pi_j^* \\right)\n$$\nSubstituting the definition of the pattern profit $v_s = \\sum_{j=1}^n c_j (a_s)_j$:\n$$\n\\max_{s \\in \\mathcal{P}} \\left( \\sum_{j=1}^n c_j (a_s)_j - \\sum_{j=1}^n \\pi_j^* (a_s)_j \\right) = \\max_{s \\in \\mathcal{P}} \\sum_{j=1}^n (c_j - \\pi_j^*) (a_s)_j\n$$\nThe maximization is over all feasible patterns $s \\in \\mathcal{P}$, which are defined by the constraint $\\sum_{j=1}^n l_j (a_s)_j \\le W$. To operationalize this search, we introduce binary decision variables $y_j \\in \\{0, 1\\}$ for each item $j$, where $y_j=1$ signifies that item $j$ is included in the pattern we are constructing. The pattern vector $a_s$ is thus replaced by the decision vector $y$. The pricing subproblem can now be formally stated as:\n$$\n\\begin{align*}\n\\text{(Pricing Subproblem)} \\quad \\max \\quad  \\sum_{j=1}^n (c_j - \\pi_j^*) y_j \\\\\n\\text{s.t.} \\quad  \\sum_{j=1}^n l_j y_j \\le W \\\\\n y_j \\in \\{0, 1\\}, \\quad \\forall j \\in \\{1, \\dots, n\\}\n\\end{align*}\n$$\nThis is precisely the definition of the $0$-$1$ knapsack problem. For this knapsack problem:\n-   The \"items\" to place in the knapsack are the original items $j \\in \\{1, \\dots, n\\}$.\n-   The \"weight\" of each knapsack item $j$ is its length $l_j$.\n-   The \"capacity\" of the knapsack is $W$.\n-   The \"value\" of each knapsack item $j$ is its profit adjusted by the current dual price, $c_j - \\pi_j^*$.\n\nIf the optimal objective value of this knapsack problem is greater than $0$ (or a small tolerance $\\varepsilon  0$ for numerical stability), the pattern defined by the optimal $y^*$ vector is a column with positive reduced cost and is added to the set $\\mathcal{P}'$. The RMP is then re-solved. If the optimal value of the knapsack problem is less than or equal to $0$, no column with a positive reduced cost exists, implying that the current RMP solution is also optimal for the full Master Problem. The algorithm terminates.\n\nSince the problem specifies that the lengths $l_j$ and capacity $W$ are integers, this $0$-$1$ knapsack problem can be efficiently solved using dynamic programming.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef knapsack_dp(capacity, weights, values):\n    \"\"\"\n    Solves the 0-1 knapsack problem using dynamic programming.\n    This version is designed to handle potentially negative values.\n\n    Args:\n        capacity (int): The knapsack capacity.\n        weights (list or np.array): The weights of the items.\n        values (list or np.array): The values of the items.\n\n    Returns:\n        tuple: A tuple containing:\n            - float: The maximum value of the knapsack.\n            - list: A list of indices of the items included in the optimal solution.\n    \"\"\"\n    n = len(weights)\n    # The DP table can be initialized with 0, as not picking any item gives a value of 0.\n    # We are maximizing, so any negative values will be correctly handled.\n    dp = np.zeros((n + 1, capacity + 1))\n\n    for i in range(1, n + 1):\n        weight = weights[i-1]\n        value = values[i-1]\n        for w in range(capacity + 1):\n            # Option 1: Don't include item i-1.\n            dp[i, w] = dp[i-1, w]\n            # Option 2: Include item i-1 if it fits.\n            if w >= weight:\n                dp[i, w] = max(dp[i, w], value + dp[i-1, w - weight])\n\n    max_value = dp[n, capacity]\n\n    # Backtracking to find the items in the optimal set\n    items_included = []\n    w = capacity\n    for i in range(n, 0, -1):\n        # A simple and robust way to backtrack is to check if the value changed\n        # by excluding the current item. If it does, the item must have been\n        # included. Ties are broken by not including the item.\n        if not np.isclose(dp[i, w], dp[i-1, w]):\n            items_included.append(i-1)\n            w -= weights[i-1]\n\n    return max_value, items_included\n\ndef solve_set_packing_cg(W, l, c, tolerance=1e-9, max_iter=100):\n    \"\"\"\n    Solves the set packing relaxation using column generation.\n\n    Args:\n        W (int): The capacity of the resource.\n        l (list): The lengths of the items.\n        c (list): The profits of the items.\n        tolerance (float): The tolerance for checking if reduced cost is positive.\n        max_iter (int): A failsafe limit on iterations.\n\n    Returns:\n        float: The optimal objective value of the LP relaxation.\n    \"\"\"\n    n = len(l)\n    l_np = np.array(l, dtype=int)\n    c_np = np.array(c, dtype=float)\n\n    patterns = []  # List to store pattern vectors (as numpy arrays)\n\n    for _ in range(max_iter):\n        # Step 1: Solve the Dual of the Restricted Master Problem\n        if not patterns:\n            # Per problem statement, if RMP has no columns, duals are the zero vector.\n            pi = np.zeros(n, dtype=float)\n        else:\n            # Construct and solve the Dual RMP\n            num_patterns = len(patterns)\n            pattern_matrix = np.array(patterns) # Shape: (num_patterns, n)\n            pattern_profits = np.array([np.dot(p, c_np) for p in patterns])\n            \n            # Dual RMP: min 1^T * pi, s.t. A^T * pi >= v, pi >= 0\n            # `linprog` form: min c^T * x, s.t. A_ub * x = b_ub\n            # So, -A^T * pi = -v\n            c_dual = np.ones(n)\n            A_ub_dual = -pattern_matrix\n            b_ub_dual = -pattern_profits\n\n            # The 'highs' method is robust for LPs.\n            res_dual = linprog(c_dual, A_ub=A_ub_dual, b_ub=b_ub_dual, bounds=(0, None), method='highs')\n            \n            if not res_dual.success:\n                # This should not happen in a well-behaved column generation.\n                # It might indicate numerical instability or an issue with the problem.\n                # We can assume for this problem that the dual is always feasible and bounded.\n                # If it happens, we stop and report the current best solution.\n                break \n\n            pi = res_dual.x\n\n        # Step 2: Solve the Pricing Subproblem (0-1 Knapsack)\n        knapsack_values = c_np - pi\n        \n        # We only need to consider items with non-negative weights for the DP.\n        # The problem states l_j are non-negative, but 0-weight items with\n        # positive value could be problematic, but 0-1 knapsack handles them.\n        # Here all lengths are positive integers or greater than capacity.\n        max_reduced_cost, new_pattern_items = knapsack_dp(W, l_np, knapsack_values)\n\n        # Step 3: Check for Termination\n        if max_reduced_cost = tolerance:\n            break\n\n        # Step 4: Add New Pattern\n        new_pattern = np.zeros(n, dtype=int)\n        new_pattern[new_pattern_items] = 1\n        \n        # Avoid adding a duplicate pattern to prevent cycling (though unlikely with 'highs')\n        if not any(np.array_equal(new_pattern, p) for p in patterns):\n            patterns.append(new_pattern)\n        else: # If a duplicate is generated, something may be stalled. Stop.\n            break\n\n    # Step 5: Solve the final Primal Restricted Master Problem\n    if not patterns:\n        return 0.0\n\n    num_patterns = len(patterns)\n    pattern_matrix_T = np.array(patterns).T # Shape: (n, num_patterns)\n    pattern_profits = np.array([np.dot(p, c_np) for p in patterns])\n\n    # Primal RMP: max v^T * x, s.t. A * x = 1, x >= 0\n    # `linprog` form: min -v^T*x\n    c_primal = -pattern_profits\n    A_ub_primal = pattern_matrix_T\n    b_ub_primal = np.ones(n)\n    \n    res_primal = linprog(c_primal, A_ub=A_ub_primal, b_ub=b_ub_primal, bounds=(0, None), method='highs')\n\n    if res_primal.success:\n        return -res_primal.fun\n    else:\n        # Should not fail if the process was successful.\n        return 0.0\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case A: W = 10, item lengths [2,3,5,7], item profits [6,5,9,8].\n        {\"W\": 10, \"l\": [2, 3, 5, 7], \"c\": [6, 5, 9, 8]},\n        # Case B: W = 9, item lengths [2,3,4], item profits [5,5,5].\n        {\"W\": 9, \"l\": [2, 3, 4], \"c\": [5, 5, 5]},\n        # Case C: W = 10, item lengths [11,2,9], item profits [100,4,7].\n        {\"W\": 10, \"l\": [11, 2, 9], \"c\": [100, 4, 7]},\n    ]\n\n    results = []\n    for case in test_cases:\n        W, l, c = case[\"W\"], case[\"l\"], case[\"c\"]\n        optimal_value = solve_set_packing_cg(W, l, c)\n        results.append(optimal_value)\n\n    # Format output to six decimal places as specified.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3181317"}]}