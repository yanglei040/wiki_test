{"hands_on_practices": [{"introduction": "This first exercise takes you to the mathematical core of the trust-region subproblem. By applying the Karush-Kuhn-Tucker (KKT) conditions, you will derive and solve the foundational 'secular equation' for a simple but illustrative case. This practice provides crucial insight into the relationship between the geometric trust-region radius, $\\Delta$, and the algebraic Lagrange multiplier, $\\lambda$, which underpins both the theory and implementation of advanced trust-region algorithms. [@problem_id:3193674]", "problem": "Let $m(\\mathbf{s}) = g^{\\top} \\mathbf{s} + \\frac{1}{2} \\mathbf{s}^{\\top} Q \\mathbf{s}$ be the quadratic model for a twice continuously differentiable objective function at a given iterate, where $\\mathbf{s} \\in \\mathbb{R}^{2}$ is the step, $g \\in \\mathbb{R}^{2}$ is the gradient vector, and $Q \\in \\mathbb{R}^{2 \\times 2}$ is a symmetric positive definite matrix. The trust-region subproblem seeks the step that minimizes $m(\\mathbf{s})$ subject to the Euclidean norm constraint $\\|\\mathbf{s}\\| \\leq \\Delta$, where $\\Delta > 0$ is the trust-region radius.\n\nConsider the specific case where $Q = 3 I_{2}$ and $g = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$, with $I_{2}$ the $2 \\times 2$ identity matrix. Treat $\\Delta$ as a positive parameter, and assume that the minimizer lies on the boundary of the trust region for sufficiently small $\\Delta$.\n\nStarting solely from the first-order optimality conditions for constrained minimization (Karush-Kuhn-Tucker (KKT) conditions), derive the secular equation that determines the Lagrange multiplier $\\lambda \\geq 0$ associated with the trust-region constraint, solve this equation exactly for $\\lambda$ in terms of $\\Delta$, and then obtain the exact trust-region step $\\mathbf{s}^{\\star}(\\Delta)$. Finally, determine the sensitivity of the Lagrange multiplier with respect to the trust-region radius by computing $\\frac{d\\lambda}{d\\Delta}$ evaluated at $\\Delta = 1$.\n\nYour final answer must be the single real number value of $\\frac{d\\lambda}{d\\Delta}$ at $\\Delta = 1$. No rounding is required.", "solution": "The problem is to find the minimizer of a quadratic model $m(\\mathbf{s})$ subject to a trust-region constraint. The optimization problem is stated as:\n$$ \\min_{\\mathbf{s} \\in \\mathbb{R}^{2}} m(\\mathbf{s}) = g^{\\top} \\mathbf{s} + \\frac{1}{2} \\mathbf{s}^{\\top} Q \\mathbf{s} $$\n$$ \\text{subject to} \\quad \\|\\mathbf{s}\\| \\leq \\Delta $$\nwhere the Euclidean norm is used. We are given the specific values $Q = 3 I_{2}$ and $g = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$. The problem also specifies that the solution, denoted $\\mathbf{s}^{\\star}$, lies on the boundary of the feasible set, meaning $\\|\\mathbf{s}^{\\star}\\| = \\Delta$.\n\nTo solve this constrained optimization problem, we utilize the Karush-Kuhn-Tucker (KKT) conditions. The constraint can be written as $c(\\mathbf{s}) = \\frac{1}{2}(\\mathbf{s}^{\\top}\\mathbf{s} - \\Delta^2) \\leq 0$. The Lagrangian function $\\mathcal{L}(\\mathbf{s}, \\lambda)$ is:\n$$ \\mathcal{L}(\\mathbf{s}, \\lambda) = m(\\mathbf{s}) + \\lambda c(\\mathbf{s}) = g^{\\top} \\mathbf{s} + \\frac{1}{2} \\mathbf{s}^{\\top} Q \\mathbf{s} + \\frac{\\lambda}{2} (\\mathbf{s}^{\\top}\\mathbf{s} - \\Delta^2) $$\nwhere $\\lambda$ is the Lagrange multiplier associated with the norm constraint.\n\nThe first-order necessary conditions for optimality (KKT conditions) are:\n1.  **Stationarity:** The gradient of the Lagrangian with respect to $\\mathbf{s}$ must be the zero vector: $\\nabla_{\\mathbf{s}} \\mathcal{L}(\\mathbf{s}^{\\star}, \\lambda) = \\mathbf{0}$.\n2.  **Primal Feasibility:** The solution must satisfy the constraint: $(\\mathbf{s}^{\\star})^{\\top}\\mathbf{s}^{\\star} - \\Delta^2 \\leq 0$.\n3.  **Dual Feasibility:** The Lagrange multiplier must be non-negative: $\\lambda \\geq 0$.\n4.  **Complementary Slackness:** $\\lambda ((\\mathbf{s}^{\\star})^{\\top}\\mathbf{s}^{\\star} - \\Delta^2) = 0$.\n\nFirst, we compute the gradient of the Lagrangian:\n$$ \\nabla_{\\mathbf{s}} \\mathcal{L}(\\mathbf{s}, \\lambda) = g + Q\\mathbf{s} + \\lambda\\mathbf{s} = (Q + \\lambda I)\\mathbf{s} + g $$\nThe stationarity condition is therefore:\n$$ (Q + \\lambda I)\\mathbf{s}^{\\star} = -g $$\n\nWe are given that the solution lies on the boundary, so $\\|\\mathbf{s}^{\\star}\\| = \\Delta$. This implies that the constraint is active, i.e., $(\\mathbf{s}^{\\star})^{\\top}\\mathbf{s}^{\\star} - \\Delta^2 = 0$. By the complementary slackness condition, this allows for $\\lambda > 0$. The condition that the solution is on the boundary for a convex problem implies $\\lambda$ will be strictly positive.\n\nSubstituting the given matrix $Q = 3 I_{2}$ into the stationarity equation:\n$$ (3I_{2} + \\lambda I_{2})\\mathbf{s}^{\\star} = -g $$\n$$ (3 + \\lambda)\\mathbf{s}^{\\star} = -g $$\nSince $Q$ is positive definite (its eigenvalues are both $3$), and $\\lambda \\geq 0$, the matrix $Q + \\lambda I_2$ is symmetric and positive definite (its eigenvalues are $3+\\lambda > 0$). Thus, it is invertible. We can solve for $\\mathbf{s}^{\\star}$:\n$$ \\mathbf{s}^{\\star} = -\\frac{1}{3+\\lambda} g $$\n\nNow, we enforce the boundary condition $\\|\\mathbf{s}^{\\star}\\| = \\Delta$:\n$$ \\|\\mathbf{s}^{\\star}\\| = \\left\\| -\\frac{1}{3+\\lambda} g \\right\\| = \\frac{1}{3+\\lambda} \\|g\\| $$\nThis leads to the secular equation, which relates $\\lambda$ to $\\Delta$:\n$$ \\frac{\\|g\\|}{3+\\lambda} = \\Delta $$\nWe are given $g = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$. The Euclidean norm of $g$ is:\n$$ \\|g\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5 $$\nSubstituting this value into the secular equation gives:\n$$ \\frac{5}{3+\\lambda} = \\Delta $$\nThis equation determines the value of the Lagrange multiplier $\\lambda$ for a given trust-region radius $\\Delta$.\n\nThe next step is to solve this equation for $\\lambda$ as a function of $\\Delta$:\n$$ 5 = \\Delta (3+\\lambda) $$\n$$ \\frac{5}{\\Delta} = 3+\\lambda $$\n$$ \\lambda(\\Delta) = \\frac{5}{\\Delta} - 3 $$\nThis expression is valid for $\\lambda \\geq 0$, which corresponds to $\\frac{5}{\\Delta} - 3 \\geq 0$, or $\\Delta \\leq \\frac{5}{3}$. The problem considers $\\Delta=1$, which satisfies this condition.\n\nUsing this result, we can find the exact trust-region step $\\mathbf{s}^{\\star}(\\Delta)$:\n$$ 3 + \\lambda(\\Delta) = 3 + \\left(\\frac{5}{\\Delta} - 3\\right) = \\frac{5}{\\Delta} $$\nSo,\n$$ \\mathbf{s}^{\\star}(\\Delta) = -\\frac{1}{3+\\lambda(\\Delta)} g = -\\frac{1}{5/\\Delta} g = -\\frac{\\Delta}{5} g = -\\frac{\\Delta}{5} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} $$\n\nFinally, we need to compute the sensitivity of the Lagrange multiplier with respect to the trust-region radius, $\\frac{d\\lambda}{d\\Delta}$, evaluated at $\\Delta = 1$. We have the expression for $\\lambda(\\Delta)$:\n$$ \\lambda(\\Delta) = 5\\Delta^{-1} - 3 $$\nWe differentiate $\\lambda(\\Delta)$ with respect to $\\Delta$:\n$$ \\frac{d\\lambda}{d\\Delta} = \\frac{d}{d\\Delta} (5\\Delta^{-1} - 3) = -5\\Delta^{-2} = -\\frac{5}{\\Delta^2} $$\nEvaluating this derivative at $\\Delta = 1$:\n$$ \\left. \\frac{d\\lambda}{d\\Delta} \\right|_{\\Delta=1} = -\\frac{5}{1^2} = -5 $$\nThe sensitivity of the Lagrange multiplier with respect to the trust-region radius at $\\Delta=1$ is $-5$.", "answer": "$$\n\\boxed{-5}\n$$", "id": "3193674"}, {"introduction": "Having explored the analytical solution, we now investigate the central motivation for trust-region methods: the fact that our quadratic model is only an approximation. This practice challenges you to quantify how well the model predicts progress by calculating the difference between the predicted and actual reduction in a function with tunable non-convexity. Through this hands-on analysis, you will see precisely why we must adaptively control the 'region of trust' to ensure reliable optimization. [@problem_id:3193707]", "problem": "Consider a one-dimensional trust-region method for unconstrained minimization. At an iterate $x_k$, a twice continuously differentiable function $f$ is approximated by its second-order Taylor polynomial, called the quadratic model, over a trust-region defined by a radius $\\Delta$. The quadratic model $m(s)$ about $x_k$ is constructed using the gradient $g$ and the Hessian $B$ of $f$ at $x_k$. The trust-region subproblem seeks a step $s$ that reduces the model subject to a bound on the step length. The actual reduction is measured by the change of the true function value, while the predicted reduction is measured by the decrease in the quadratic model. The discrepancy between actual and predicted reductions quantifies how well the quadratic model captures the true function behavior within the trust-region.\n\nStarting from these fundamental definitions, consider the following specific setup:\n\n- The family of functions is a parameterized quartic polynomial given by $f_{\\alpha}(x) = \\frac{1}{4}x^4 - \\frac{\\alpha}{2} x^2 + \\beta x$, where $\\alpha$ controls nonconvexity and $\\beta$ is a fixed small linear perturbation.\n- Use the fixed perturbation $\\beta = 0.1$.\n- Use the fixed iterate $x_k = 0.5$.\n- In one dimension, the quadratic model at $x_k$ can be expressed as $m(s) = f_{\\alpha}(x_k) + g_{\\alpha} s + \\frac{1}{2} B_{\\alpha} s^2$, where $g_{\\alpha} = f_{\\alpha}'(x_k)$ and $B_{\\alpha} = f_{\\alpha}''(x_k)$.\n- Define the trust-region subproblem in one dimension as minimizing $m(s)$ subject to the constraint $|s| \\le \\Delta$.\n\nFor the one-dimensional case, the steepest descent direction is the negative gradient direction. The step used in this problem should be the minimizer of $m(s)$ along the steepest descent direction subject to the trust-region bound, namely the step $s(\\Delta)$ on the ray in the direction $-g_{\\alpha}$ that minimizes $m(s)$ while satisfying $|s| \\le \\Delta$.\n\nYour tasks are:\n\n- For each choice of $\\alpha$, construct $f_{\\alpha}$ and compute the gradient $g_{\\alpha}$ and Hessian $B_{\\alpha}$ at $x_k$.\n- Determine the step $s(\\Delta)$ that minimizes the quadratic model $m(s)$ along the steepest descent direction with the constraint $|s| \\le \\Delta$.\n- Compute the actual reduction $AR(\\Delta) = f_{\\alpha}(x_k) - f_{\\alpha}(x_k + s(\\Delta))$ and the predicted reduction $PR(\\Delta) = m(0) - m(s(\\Delta))$.\n- Compute the difference $D(\\Delta) = AR(\\Delta) - PR(\\Delta)$ for each test case.\n\nTest Suite Specification:\n\n- Use the parameter set of nonconvexity levels $\\alpha \\in \\{0.0, 0.5, 0.75, 1.0, 2.0\\}$.\n- Use the trust-region radii $\\Delta \\in \\{0.01, 0.1, 0.3, 0.5\\}$.\n- The fixed iterate is $x_k = 0.5$.\n- The fixed perturbation is $\\beta = 0.1$.\n\nDesign for Coverage:\n\n- The set of $\\alpha$ values probes regimes from convex near $x_k$ to strongly nonconvex near $x_k$.\n- The set of $\\Delta$ values probes small radii (model locally accurate), boundary-limited steps, and moderately large radii.\n- Include the borderline case where the curvature at $x_k$ is zero for $\\alpha = 0.75$.\n\nFinal Output Specification:\n\n- Your program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets. Each inner list corresponds to one value of $\\alpha$ in ascending order, and contains the values of $D(\\Delta)$ for $\\Delta$ in ascending order. For example, the output format should look like $[[d_{0,0},d_{0,1},d_{0,2},d_{0,3}],[d_{1,0},\\dots],\\dots]$, where $d_{i,j}$ is the difference for the $i$-th $\\alpha$ and $j$-th $\\Delta$.\n- Each $d_{i,j}$ must be a real number (a float), expressed without any physical units, and printed directly as a numeric literal.\n\nNo user input is required; the program must be self-contained and compute the specified values using the given test suite. The angle unit is not applicable. No physical units or percentages are involved in this problem. Ensure scientific consistency and use only the definitions and well-tested formulas that follow from the second-order Taylor model within trust-region methods.", "solution": "The user has requested a solution to a problem concerning trust-region optimization methods. The task involves calculating the difference between the actual and predicted reduction of a model function for a given family of one-dimensional quartic polynomials.\n\n### Step 1: Preliminary Analysis and Formulation\n\nThe problem is centered around the function $f_{\\alpha}(x) = \\frac{1}{4}x^4 - \\frac{\\alpha}{2} x^2 + \\beta x$ with fixed parameters $x_k=0.5$ and $\\beta=0.1$. The core of the problem is to determine the step $s(\\Delta)$ that minimizes a quadratic model $m(s)$ and then use this step to compute the quantity $D(\\Delta) = AR(\\Delta) - PR(\\Delta)$.\n\nFirst, we establish the key components of the quadratic model, $m(s) = f_{\\alpha}(x_k) + g_{\\alpha} s + \\frac{1}{2} B_{\\alpha} s^2$. The gradient $g_{\\alpha}$ and Hessian $B_{\\alpha}$ at the iterate $x_k$ are the first and second derivatives of $f_{\\alpha}(x)$ evaluated at $x_k$.\n\nThe derivatives of $f_{\\alpha}(x)$ are:\n$f'_{\\alpha}(x) = x^3 - \\alpha x + \\beta$\n$f''_{\\alpha}(x) = 3x^2 - \\alpha$\n$f'''_{\\alpha}(x) = 6x$\n$f''''_{\\alpha}(x) = 6$\n\nEvaluating these at the fixed iterate $x_k = 0.5$ with $\\beta = 0.1$:\nThe gradient is $g_{\\alpha} = f'_{\\alpha}(0.5) = (0.5)^3 - \\alpha(0.5) + 0.1 = 0.125 - 0.5\\alpha + 0.1 = 0.225 - 0.5\\alpha$.\nThe Hessian (or curvature in 1D) is $B_{\\alpha} = f''_{\\alpha}(0.5) = 3(0.5)^2 - \\alpha = 0.75 - \\alpha$.\n\n### Step 2: Simplifying the Target Quantity $D(\\Delta)$\n\nThe target quantity is the difference between the actual reduction, $AR(\\Delta)$, and the predicted reduction, $PR(\\Delta)$.\n$AR(\\Delta) = f_{\\alpha}(x_k) - f_{\\alpha}(x_k + s(\\Delta))$\n$PR(\\Delta) = m(0) - m(s(\\Delta)) = f_{\\alpha}(x_k) - m(s(\\Delta))$\n\nTherefore, the difference is:\n$D(\\Delta) = AR(\\Delta) - PR(\\Delta) = (f_{\\alpha}(x_k) - f_{\\alpha}(x_k + s(\\Delta))) - (f_{\\alpha}(x_k) - m(s(\\Delta)))$\n$D(\\Delta) = m(s(\\Delta)) - f_{\\alpha}(x_k + s(\\Delta))$\n\nThis quantity represents the error between the quadratic model and the true function at the point $x_k + s(\\Delta)$. We can express this error using the higher-order terms of the Taylor expansion of $f_{\\alpha}$ around $x_k$. Since $f_{\\alpha}$ is a quartic polynomial, its Taylor series is finite and exact:\n$f_{\\alpha}(x_k+s) = f_{\\alpha}(x_k) + f'_{\\alpha}(x_k)s + \\frac{1}{2}f''_{\\alpha}(x_k)s^2 + \\frac{1}{6}f'''_{\\alpha}(x_k)s^3 + \\frac{1}{24}f''''_{\\alpha}(x_k)s^4$\nBy definition, $m(s) = f_{\\alpha}(x_k) + f'_{\\alpha}(x_k)s + \\frac{1}{2}f''_{\\alpha}(x_k)s^2$.\nSo, $f_{\\alpha}(x_k+s) = m(s) + \\frac{1}{6}f'''_{\\alpha}(x_k)s^3 + \\frac{1}{24}f''''_{\\alpha}(x_k)s^4$.\n\nSubstituting $f'''_{\\alpha}(0.5) = 6(0.5) = 3$ and $f''''_{\\alpha}(0.5) = 6$:\n$f_{\\alpha}(0.5+s) = m(s) + \\frac{1}{6}(3)s^3 + \\frac{1}{24}(6)s^4 = m(s) + \\frac{1}{2}s^3 + \\frac{1}{4}s^4$.\n\nThus, for any step $s$, the difference is:\n$m(s) - f_{\\alpha}(0.5+s) = -(\\frac{1}{2}s^3 + \\frac{1}{4}s^4)$.\nThis means $D(\\Delta) = -\\frac{1}{2} s(\\Delta)^3 - \\frac{1}{4} s(\\Delta)^4$, which significantly simplifies the calculation. The problem reduces to finding the step $s(\\Delta)$.\n\n### Step 3: Determining the Step $s(\\Delta)$\n\nThe step $s(\\Delta)$ is defined as the minimizer of the model $m(s)$ along the steepest descent direction, $-g_{\\alpha}$, subject to the trust-region constraint $|s| \\le \\Delta$.\nLet the step be of the form $s = \\tau \\cdot \\operatorname{dir}$, where $\\tau \\ge 0$ is the step length and $\\operatorname{dir} = -\\frac{g_{\\alpha}}{|g_{\\alpha}|} = -\\operatorname{sgn}(g_{\\alpha})$ is the unit direction. The constraint becomes $\\tau \\le \\Delta$.\n\nWe substitute $s = -\\tau \\operatorname{sgn}(g_{\\alpha})$ into the model. Since $g_{\\alpha} s = -\\tau |g_{\\alpha}|$, the model becomes a function of $\\tau$:\n$m(\\tau) = f_{\\alpha}(x_k) - |g_{\\alpha}|\\tau + \\frac{1}{2} B_{\\alpha} \\tau^2$.\n\nWe need to minimize this quadratic function of $\\tau$ over the interval $[0, \\Delta]$. We analyze two cases based on the sign of the curvature $B_{\\alpha}$.\n\n**Case 1: Convex Model ($B_{\\alpha} > 0$)**\nThe model is a convex parabola opening upwards. The unconstrained minimum is found by setting the derivative with respect to $\\tau$ to zero:\n$m'(\\tau) = -|g_{\\alpha}| + B_{\\alpha}\\tau = 0 \\implies \\tau_{unc} = \\frac{|g_{\\alpha}|}{B_{\\alpha}}$.\nThe minimizer within the trust region is found by projecting this point onto the feasible interval $[0, \\Delta]$:\n$\\tau_{opt} = \\min(\\tau_{unc}, \\Delta) = \\min\\left(\\frac{|g_{\\alpha}|}{B_{\\alpha}}, \\Delta\\right)$.\n\n**Case 2: Concave or Linear Model ($B_{\\alpha} \\le 0$)**\nThe function $m(\\tau)$ is either a downward-opening parabola or a line with a negative slope (since $g_{\\alpha}$ is non-zero for all test cases). In either scenario, $m(\\tau)$ is a decreasing function on $[0, \\infty)$. To minimize $m(\\tau)$ on $[0, \\Delta]$, we must choose the largest possible value for $\\tau$.\n$\\tau_{opt} = \\Delta$.\n\nOnce $\\tau_{opt}$ is determined, the step is calculated as $s(\\Delta) = \\tau_{opt} \\cdot (-\\operatorname{sgn}(g_{\\alpha}))$.\n\n### Step 4: Final Calculation Algorithm\n\nFor each pair of $(\\alpha, \\Delta)$ from the specified test suites, we perform the following calculations:\n1.  Compute $g_{\\alpha} = 0.225 - 0.5\\alpha$.\n2.  Compute $B_{\\alpha} = 0.75 - \\alpha$.\n3.  Determine the optimal step length $\\tau_{opt}$:\n    - If $B_{\\alpha} > 0$, calculate $\\tau_{opt} = \\min(|g_{\\alpha}|/B_{\\alpha}, \\Delta)$.\n    - If $B_{\\alpha} \\le 0$, set $\\tau_{opt} = \\Delta$.\n4.  Compute the step $s(\\Delta) = \\tau_{opt} \\cdot (-\\operatorname{sgn}(g_{\\alpha}))$. Note that for $g_{\\alpha}=0$, this would correctly yield $s=0$.\n5.  Compute the difference $D(\\Delta) = -0.5 s(\\Delta)^3 - 0.25 s(\\Delta)^4$.\n6.  Collect the results $D(\\Delta)$ into a list for the current $\\alpha$.\n7.  After iterating through all $\\Delta$ values, append this list to the final list of lists.\nThis procedure is then implemented for all specified $\\alpha$ values to generate the final output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the trust-region subproblem for a parameterized quartic polynomial\n    and computes the difference between actual and predicted reduction.\n    \"\"\"\n    \n    # Test Suite Specification\n    alphas = [0.0, 0.5, 0.75, 1.0, 2.0]\n    deltas = [0.01, 0.1, 0.3, 0.5]\n    \n    # Fixed parameters\n    x_k = 0.5\n    beta = 0.1\n    \n    # This list will hold lists of results, one for each alpha.\n    all_results = []\n    \n    for alpha in alphas:\n        # This list will hold the results for the current alpha over all deltas.\n        results_for_alpha = []\n        \n        # Step 1: Compute gradient g_alpha and Hessian B_alpha at x_k\n        # g_alpha = f'(x_k) = x_k^3 - alpha*x_k + beta\n        g_alpha = x_k**3 - alpha * x_k + beta\n        # B_alpha = f''(x_k) = 3*x_k^2 - alpha\n        B_alpha = 3 * x_k**2 - alpha\n        \n        for delta in deltas:\n            # Step 2: Determine the optimal step s(delta)\n            \n            # The minimization is along the steepest descent direction -g_alpha.\n            # We solve for the step length tau_opt along this direction.\n            # The model along this direction is m(tau) = c - |g_alpha|*tau + 0.5*B_alpha*tau^2.\n            \n            tau_opt = 0.0\n\n            if B_alpha > 0:\n                # Convex case: The unconstrained minimizer is at tau_unc = |g_alpha| / B_alpha.\n                # The constrained solution is the minimum of this and the trust-region radius.\n                if g_alpha != 0:\n                    tau_unc = abs(g_alpha) / B_alpha\n                    tau_opt = min(tau_unc, delta)\n                else:\n                    # If gradient is zero, step is zero.\n                    tau_opt = 0.0\n\n            else: # B_alpha = 0\n                # Concave or linear case: The model decreases along the ray.\n                # The minimum is at the trust-region boundary.\n                tau_opt = delta\n                \n            # Compute the step vector s. The direction is -g_alpha.\n            # The sign of the step is -sign(g_alpha).\n            # np.sign(0) is 0, correctly handling the g_alpha=0 case.\n            step_s = tau_opt * (-np.sign(g_alpha))\n            \n            # Step 3: Compute the difference D(delta) = AR - PR = m(s) - f(x_k + s)\n            # As derived, D(delta) = -0.5 * s^3 - 0.25 * s^4.\n            diff_D = -0.5 * step_s**3 - 0.25 * step_s**4\n            results_for_alpha.append(diff_D)\n            \n        all_results.append(results_for_alpha)\n\n    # Final Output Specification\n    # Format: [[d_0,0, d_0,1, ...], [d_1,0, ...], ...]\n    # Using a list comprehension to format numbers without trailing '.0'\n    formatted_results = [\n        \"[\" + \",\".join(map(str, inner_list)) + \"]\"\n        for inner_list in all_results\n    ]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3193707"}, {"introduction": "Solving the trust-region subproblem exactly can be too slow for large-scale applications. This final exercise bridges the gap between theory and practice by having you implement and compare two powerful iterative algorithms: a dogleg-style method and the truncated Conjugate Gradient (CG) method. By coding these strategies and testing them in various scenarios, including those with challenging curvature, you will gain a practical understanding of how robust optimization steps are computed efficiently. [@problem_id:3193710]", "problem": "You are tasked with implementing and comparing two trust-region step strategies for minimizing a smooth nonlinear objective. At a given point $x \\in \\mathbb{R}^n$, consider a twice continuously differentiable function $f(x)$ and the quadratic model based on the second-order Taylor expansion around $x$:\n$$\nm(s) = f(x) + \\nabla f(x)^\\top s + \\tfrac{1}{2} s^\\top \\nabla^2 f(x) s,\n$$\nwhere $s \\in \\mathbb{R}^n$ is the step, $\\nabla f(x)$ is the gradient, and $\\nabla^2 f(x)$ is the Hessian. The trust-region framework constrains the step via the ball $\\|s\\| \\le \\Delta$, where $\\Delta > 0$ is the trust-region radius.\n\nStarting from the foundational definitions above and without invoking any shortcut formulas, you must derive and implement the following two strategies:\n\n- Two-stage model-based step:\n  1. Compute a Cauchy-type point by minimizing the quadratic model $m(s)$ along the negative gradient direction within the trust region constraint $\\|s\\| \\le \\Delta$.\n  2. Apply a quadratic correction by moving along the polygonal dogleg path from the Cauchy-type point toward the Newton step (the solution of $\\nabla^2 f(x) s = -\\nabla f(x)$ if it exists), truncated to satisfy $\\|s\\| \\le \\Delta$. The final two-stage step is the resulting dogleg step.\n- Direct quadratic model step:\n  Solve the trust-region subproblem for the quadratic model directly, that is, find $s$ that minimizes $m(s)$ subject to $\\|s\\| \\le \\Delta$, by using truncated Conjugate Gradient (CG) from first principles: start from $s=0$, use the steepest-descent direction, detect negative curvature, and handle boundary truncation when necessary.\n\nFor both strategies, use the trust-region acceptance ratio that quantifies the agreement between the model and the actual objective:\n$$\n\\rho = \\frac{f(x) - f(x + s)}{m(0) - m(s)}.\n$$\nA step is considered accepted if $\\rho \\ge \\eta$ and the predicted reduction $m(0) - m(s)$ is strictly positive; otherwise, it is rejected.\n\nImplement both strategies for the objective\n$$\nf(x) = \\tfrac{1}{2} x^\\top A x + \\alpha \\left( \\sin(x_1) + \\sin(x_2) \\right) + b^\\top x,\n$$\nwith gradient and Hessian\n$$\n\\nabla f(x) = A x + \\alpha \\begin{bmatrix}\\cos(x_1) \\\\ \\cos(x_2)\\end{bmatrix} + b,\\quad\n\\nabla^2 f(x) = A - \\alpha \\begin{bmatrix}\\sin(x_1)  0 \\\\ 0  \\sin(x_2)\\end{bmatrix}.\n$$\nYou must implement the entire logic from these base definitions. Do not use any function or formula that has not been derived from these foundations.\n\nUse the following four test cases, each specified by the tuple $(A, \\alpha, b, x, \\Delta, \\eta)$:\n\n1. Happy path:\n   - $A = \\begin{bmatrix}3  1 \\\\ 1  2\\end{bmatrix}$, $\\alpha = 0.2$, $b = \\begin{bmatrix}-0.5 \\\\ 0.3\\end{bmatrix}$, $x = \\begin{bmatrix}0.8 \\\\ -0.5\\end{bmatrix}$, $\\Delta = 0.5$, $\\eta = 0.1$.\n2. Small trust region:\n   - $A = \\begin{bmatrix}4  0 \\\\ 0  1\\end{bmatrix}$, $\\alpha = 0.1$, $b = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, $x = \\begin{bmatrix}2.0 \\\\ 2.0\\end{bmatrix}$, $\\Delta = 0.1$, $\\eta = 0.2$.\n3. Indefinite curvature:\n   - $A = \\begin{bmatrix}-1  0 \\\\ 0  3\\end{bmatrix}$, $\\alpha = 0.5$, $b = \\begin{bmatrix}0.2 \\\\ -0.1\\end{bmatrix}$, $x = \\begin{bmatrix}0.2 \\\\ 2.0\\end{bmatrix}$, $\\Delta = 0.7$, $\\eta = 0.05$.\n4. Nearly flat gradient:\n   - $A = \\begin{bmatrix}0.01  0 \\\\ 0  0.02\\end{bmatrix}$, $\\alpha = 0.01$, $b = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, $x = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, $\\Delta = 0.05$, $\\eta = 0.5$.\n\nFor each case, compute whether the two-stage step is accepted and whether the direct quadratic step is accepted, using the acceptance criteria described. Then, produce a single integer outcome per test case defined as:\n- $2$ if both strategies yield accepted steps,\n- $1$ if only the two-stage step is accepted,\n- $-1$ if only the direct quadratic step is accepted,\n- $0$ if neither strategy yields an accepted step.\n\nYour program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4]$. All angles must be in radians, and the numeric outputs are unitless integers.", "solution": "The problem requires the implementation and comparison of two distinct strategies for computing a trial step in a trust-region optimization framework. We are given a specific objective function $f(x)$, its gradient $\\nabla f(x)$, and its Hessian $\\nabla^2 f(x)$. At a point $x$, we construct a quadratic model of the function:\n$$\nm(s) = f(x) + g^\\top s + \\tfrac{1}{2} s^\\top H s\n$$\nwhere $s$ is the step, $g = \\nabla f(x)$ is the gradient, and $H = \\nabla^2 f(x)$ is the Hessian at $x$. The step $s$ is constrained to lie within a trust region, defined by the inequality $\\|s\\| \\le \\Delta$ for a given radius $\\Delta > 0$. The norm $\\|\\cdot\\|$ is the standard Euclidean norm.\n\nAfter computing a step $s$ using either strategy, its quality is assessed using the ratio $\\rho$ of actual reduction to predicted reduction:\n$$\n\\rho = \\frac{f(x) - f(x+s)}{m(0) - m(s)}\n$$\nThe predicted reduction is given by $pred(s) = m(0) - m(s) = -g^\\top s - \\tfrac{1}{2}s^\\top H s$. A step is deemed \"accepted\" if the predicted reduction is strictly positive, $pred(s) > 0$, and the ratio satisfies $\\rho \\ge \\eta$ for a given threshold $\\eta$.\n\nThe problem is well-posed, scientifically grounded in numerical optimization theory, and provides all necessary information. We proceed to derive and implement the two strategies from the provided definitions.\n\n### Strategy 1: Two-Stage Model-Based (Dogleg-like) Step\n\nThis strategy is described as a two-stage process. First, a Cauchy-type point is computed. Second, a correction is applied by moving from this point towards the Newton step.\n\n**Stage 1: Cauchy Point, $s^C$**\nThe Cauchy point is found by minimizing the quadratic model $m(s)$ along the steepest descent direction, $d = -g$, within the trust region. Let the step be $s(\\tau) = \\tau d = -\\tau g$ for $\\tau \\ge 0$. The trust-region constraint $\\|s(\\tau)\\| \\le \\Delta$ translates to $\\tau\\|g\\| \\le \\Delta$, or $\\tau \\in [0, \\Delta/\\|g\\|]$.\nWe substitute $s(\\tau)$ into the model's change, $m(s) - m(0)$:\n$$\nm(-\\tau g) - m(0) = -\\tau g^\\top g + \\tfrac{1}{2} \\tau^2 g^\\top H g\n$$\nThis is a one-dimensional quadratic in $\\tau$. Let $q(\\tau) = m(-\\tau g)$. To find the minimum:\n1.  If $g^\\top H g \\le 0$, the quadratic does not increase (or is concave down) along the ray. The minimum within the allowed interval for $\\tau$ is thus at the boundary. We take the largest possible step: $\\tau_C = \\Delta / \\|g\\|$.\n2.  If $g^\\top H g > 0$, the unconstrained minimizer of the quadratic is found by setting its derivative to zero: $q'(\\tau) = -g^\\top g + \\tau (g^\\top H g) = 0$, which yields $\\tau^* = \\frac{g^\\top g}{g^\\top H g}$. We must respect the trust-region boundary, so the optimal $\\tau$ is $\\tau_C = \\min(\\tau^*, \\Delta / \\|g\\|)$.\n\nIn both cases, the Cauchy point is $s^C = -\\tau_C g$. This step guarantees a decrease in the model value (unless $g=0$).\n\n**Stage 2: Correction towards Newton Point, $s^N$**\nThe Newton step, $s^N$, is the unconstrained minimizer of the quadratic model (if $H$ is positive definite) and is found by solving the linear system $H s = -g$. We compute $s^N$ by attempting to solve this system. If the Hessian $H$ is singular, a unique solution may not exist, and we take the Cauchy point $s^C$ as the final step.\n\nIf $s^N$ is successfully computed, we follow the problem's description to apply a correction: \"moving along the polygonal dogleg path from the Cauchy-type point toward the Newton step, truncated to satisfy $\\|s\\| \\le \\Delta$.\" We interpret this as defining a path along the line segment connecting $s^C$ and $s^N$. Let a point on this segment be $s_{path}(\\beta) = s^C + \\beta(s^N - s^C)$ for $\\beta \\in [0, 1]$. The final step $s$ is this path vector, but it must satisfy $\\|s\\| \\le \\Delta$. This implies we find the largest $\\beta \\in [0, 1]$ such that $\\|s^C + \\beta(s^N-s^C)\\| \\le \\Delta$. This is equivalent to solving the quadratic inequality $\\|v\\|^2\\beta^2 + 2(s^C \\cdot v)\\beta + (\\|s^C\\|^2 - \\Delta^2) \\le 0$, where $v = s^N - s^C$. Since $\\|s^C\\| \\le \\Delta$, the constant term is non-positive, guaranteeing a real solution for $\\beta$. We solve for the positive root of the corresponding equation, let's call it $\\beta_{root}$. The final step is then $s = s^C + \\min(1, \\beta_{root})(s^N - s^C)$.\n\n### Strategy 2: Direct Quadratic Model Step via Truncated Conjugate Gradient (CG)\n\nThis strategy, also known as the Steihaug-Toint method, directly approximates the solution to the trust-region subproblem $\\min_s \\{m(s) : \\|s\\| \\le \\Delta\\}$ using an iterative conjugate gradient approach. The standard CG algorithm is adapted to handle the trust-region constraint and potential negative curvature of the Hessian.\n\nThe algorithm starts with an initial guess $s_0 = 0$. The corresponding residual is $r_0 = \\nabla m(s_0) = g + H s_0 = g$. The initial search direction is the steepest descent direction, $p_0 = -r_0 = -g$.\n\nAt each iteration $k$, the algorithm performs the following checks:\n1.  **Negative Curvature Detection**: It checks the curvature along the current search direction, $p_k$. If $p_k^\\top H p_k \\le 0$, the model $m(s)$ is not bounded below by a positive definite quadratic along this direction. The current step $s_k$ is then extended along $p_k$ until it hits the trust-region boundary. This requires finding $\\tau > 0$ such that $\\|s_k + \\tau p_k\\| = \\Delta$ by solving a quadratic equation. The resulting step $s_k + \\tau p_k$ is the final solution, and the algorithm terminates.\n2.  **Step Calculation**: If the curvature is positive, the optimal step length along $p_k$ that minimizes the model in the subspace is $\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top H p_k}$.\n3.  **Boundary Truncation**: A new candidate step is $s_{k+1}' = s_k + \\alpha_k p_k$. If $\\|s_{k+1}'\\| \\ge \\Delta$, the step has exited the trust region. The final solution must lie on the boundary. We find $\\tau > 0$ such that $\\|s_k + \\tau p_k\\| = \\Delta$ (again, a quadratic solve) and terminate with the step $s_k + \\tau p_k$.\n4.  **Iteration Update**: If the algorithm has not terminated, the step is updated to $s_{k+1} = s_{k+1}'$. The residual is updated as $r_{k+1} = r_k + \\alpha_k H p_k$. A new conjugate search direction is computed: $p_{k+1} = -r_{k+1} + \\beta_{k+1} p_k$, where $\\beta_{k+1} = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k}$.\n\nThe process continues until termination or for a maximum of $n$ iterations (the dimension of the space), as CG is guaranteed to find the exact minimizer of a quadratic in at most $n$ steps in exact arithmetic.\n\nBoth strategies are implemented from these first principles for the provided objective function and test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n\n    test_cases = [\n        (np.array([[3, 1], [1, 2]]), 0.2, np.array([-0.5, 0.3]), np.array([0.8, -0.5]), 0.5, 0.1),\n        (np.array([[4, 0], [0, 1]]), 0.1, np.array([0, 0]), np.array([2.0, 2.0]), 0.1, 0.2),\n        (np.array([[-1, 0], [0, 3]]), 0.5, np.array([0.2, -0.1]), np.array([0.2, 2.0]), 0.7, 0.05),\n        (np.array([[0.01, 0], [0, 0.02]]), 0.01, np.array([0, 0]), np.array([0, 0]), 0.05, 0.5),\n    ]\n\n    results = []\n    for A, alpha, b, x, delta, eta in test_cases:\n        # Define objective function and its derivatives\n        def f(vec, A_mat, a_val, b_vec):\n            return 0.5 * vec.T @ A_mat @ vec + a_val * (np.sin(vec[0]) + np.sin(vec[1])) + b_vec.T @ vec\n\n        def grad_f(vec, A_mat, a_val, b_vec):\n            return A_mat @ vec + a_val * np.array([np.cos(vec[0]), np.cos(vec[1])]) + b_vec\n\n        def hess_f(vec, A_mat, a_val):\n            return A_mat - a_val * np.diag([np.sin(vec[0]), np.sin(vec[1])])\n\n        # Calculate values at current point x\n        f_val = f(x, A, alpha, b)\n        g = grad_f(x, A, alpha, b)\n        H = hess_f(x, A, alpha)\n\n        # Common helper functions\n        def predicted_reduction(s, g_vec, H_mat):\n            return -np.dot(g_vec, s) - 0.5 * np.dot(s, H_mat @ s)\n\n        def solve_boundary_intersection(s_vec, p_vec, delta_val):\n            a = np.dot(p_vec, p_vec)\n            b = 2 * np.dot(s_vec, p_vec)\n            c = np.dot(s_vec, s_vec) - delta_val**2\n            if a  1e-12: # p is nearly zero vector, no unique intersection\n                return np.inf\n\n            discriminant = b**2 - 4 * a * c\n            if discriminant  0:\n                return np.inf # No real intersection\n            \n            # Since s is inside the trust region, c = 0, guaranteeing at least one non-negative root.\n            # We want the positive root that moves us forward along p.\n            tau = (-b + np.sqrt(discriminant)) / (2 * a)\n            return tau\n\n        # --- Strategy 1: Two-Stage (Dogleg-like) Step ---\n        def two_stage_step(g_vec, H_mat, delta_val):\n            g_norm = np.linalg.norm(g_vec)\n            if g_norm  1e-12:\n                return np.zeros_like(g_vec)\n\n            # Stage 1: Cauchy Point\n            d = -g_vec\n            gHg = d.T @ H_mat @ d\n            if gHg = 0:\n                tau = delta_val / g_norm\n            else:\n                tau = min((g_norm**2) / gHg, delta_val / g_norm)\n            s_c = tau * d\n\n            # Stage 2: Correction towards Newton Point\n            try:\n                s_n = np.linalg.solve(H_mat, -g_vec)\n            except np.linalg.LinAlgError:\n                return s_c # Hessian is singular, return Cauchy point\n            \n            v = s_n - s_c\n            beta_root = solve_boundary_intersection(s_c, v, delta_val)\n            \n            beta = min(1.0, beta_root)\n            return s_c + beta * v\n\n        # --- Strategy 2: Direct Quadratic (Truncated CG) Step ---\n        def direct_quadratic_step(g_vec, H_mat, delta_val):\n            s = np.zeros_like(g_vec)\n            r = g_vec\n            p = -g_vec\n            \n            g_norm = np.linalg.norm(g_vec)\n            if g_norm  1e-12:\n                return s\n\n            for j in range(len(g_vec)): # Max n iterations for R^n\n                pHp = p.T @ H_mat @ p\n                if pHp = 0: # Negative curvature detected\n                    tau = solve_boundary_intersection(s, p, delta_val)\n                    return s + tau * p\n                \n                alpha_k = np.dot(r, r) / pHp\n                s_next = s + alpha_k * p\n\n                if np.linalg.norm(s_next) >= delta_val:\n                    tau = solve_boundary_intersection(s, p, delta_val)\n                    return s + tau * p\n                \n                s = s_next\n                r_next = r + alpha_k * (H_mat @ p)\n\n                if np.linalg.norm(r_next)  1e-9 * g_norm:\n                    break\n                \n                beta = np.dot(r_next, r_next) / np.dot(r, r)\n                p = -r_next + beta * p\n                r = r_next\n            return s\n        \n        # --- Evaluate both strategies ---\n        accepted1 = False\n        s1 = two_stage_step(g, H, delta)\n        pred1 = predicted_reduction(s1, g, H)\n        if pred1 > 1e-12:\n            ared1 = f_val - f(x + s1, A, alpha, b)\n            rho1 = ared1 / pred1\n            if rho1 >= eta:\n                accepted1 = True\n        \n        accepted2 = False\n        s2 = direct_quadratic_step(g, H, delta)\n        pred2 = predicted_reduction(s2, g, H)\n        if pred2 > 1e-12:\n            ared2 = f_val - f(x + s2, A, alpha, b)\n            rho2 = ared2 / pred2\n            if rho2 >= eta:\n                accepted2 = True\n\n        # Determine outcome\n        if accepted1 and accepted2:\n            outcome = 2\n        elif accepted1 and not accepted2:\n            outcome = 1\n        elif not accepted1 and accepted2:\n            outcome = -1\n        else:\n            outcome = 0\n        results.append(outcome)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3193710"}]}