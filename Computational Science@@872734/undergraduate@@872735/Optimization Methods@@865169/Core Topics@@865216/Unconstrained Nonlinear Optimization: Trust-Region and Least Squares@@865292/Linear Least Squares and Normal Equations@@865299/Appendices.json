{"hands_on_practices": [{"introduction": "The most fundamental application of linear least squares is finding the best-fit line for a set of experimental data. This exercise guides you through a classic scenario: calibrating a sensor. By translating data points into a matrix system and solving the normal equations, you will determine the optimal linear model and quantify the goodness of fit by calculating the residual error. [@problem_id:2218047]", "problem": "An engineer is calibrating a novel thermal sensor. The sensor's output voltage, $V$, is assumed to be a linear function of the ambient temperature, $T$. The relationship is modeled by the equation $V(T) = c_0 + c_1 T$, where $c_0$ and $c_1$ are the calibration constants to be determined. To find these constants, four measurements are taken in a controlled environment:\n\n*   At a temperature of $T=10$ degrees Celsius, the measured voltage is $V=2.6$ volts.\n*   At a temperature of $T=20$ degrees Celsius, the measured voltage is $V=3.4$ volts.\n*   At a temperature of $T=30$ degrees Celsius, the measured voltage is $V=4.7$ volts.\n*   At a temperature of $T=40$ degrees Celsius, the measured voltage is $V=5.4$ volts.\n\nThe parameters $c_0$ and $c_1$ are to be determined such that the sum of the squared differences between the measured voltages and the voltages predicted by the linear model is minimized. Let the resulting best-fit line be $\\hat{V}(T) = \\hat{c}_0 + \\hat{c}_1 T$.\n\nYour task is to calculate the Euclidean norm of the residual vector, where the components of the residual vector are the differences between the individually measured voltages and the corresponding voltages predicted by this best-fit line.\n\nExpress your final answer in volts, rounded to three significant figures.", "solution": "We model the voltage as a linear function of temperature, $V(T)=c_{0}+c_{1}T$, and determine $(\\hat{c}_{0},\\hat{c}_{1})$ by least squares using the four measurements $(T_{i},V_{i})=(10,2.6),(20,3.4),(30,4.7),(40,5.4)$. Let the design matrix be $A=\\begin{pmatrix}1 & 10 \\\\ 1 & 20 \\\\ 1 & 30 \\\\ 1 & 40\\end{pmatrix}$ and the observation vector be $\\boldsymbol{V}=\\begin{pmatrix}2.6 \\\\ 3.4 \\\\ 4.7 \\\\ 5.4\\end{pmatrix}$. The least-squares estimate satisfies\n$$\n\\begin{pmatrix}\\hat{c}_{0} \\\\ \\hat{c}_{1}\\end{pmatrix}=(A^{T}A)^{-1}A^{T}\\boldsymbol{V},\n$$\nequivalently the normal equations\n$$\n\\begin{pmatrix}n & \\sum T_{i} \\\\ \\sum T_{i} & \\sum T_{i}^{2}\\end{pmatrix}\\begin{pmatrix}\\hat{c}_{0} \\\\ \\hat{c}_{1}\\end{pmatrix}=\\begin{pmatrix}\\sum V_{i} \\\\ \\sum T_{i}V_{i}\\end{pmatrix}.\n$$\nCompute the sums: $n=4$, $\\sum T_{i}=10+20+30+40=100$, $\\sum T_{i}^{2}=10^{2}+20^{2}+30^{2}+40^{2}=3000$, $\\sum V_{i}=2.6+3.4+4.7+5.4=16.1$, and $\\sum T_{i}V_{i}=10\\cdot 2.6+20\\cdot 3.4+30\\cdot 4.7+40\\cdot 5.4=451$. Thus we solve\n$$\n\\begin{pmatrix}4 & 100 \\\\ 100 & 3000\\end{pmatrix}\\begin{pmatrix}\\hat{c}_{0} \\\\ \\hat{c}_{1}\\end{pmatrix}=\\begin{pmatrix}16.1 \\\\ 451\\end{pmatrix}.\n$$\nThe determinant is $4\\cdot 3000-100\\cdot 100=2000$, so\n$$\n\\hat{c}_{0}=\\frac{3000\\cdot 16.1-100\\cdot 451}{2000}=1.6,\\quad \\hat{c}_{1}=\\frac{-100\\cdot 16.1+4\\cdot 451}{2000}=0.097.\n$$\nHence the best-fit line is $\\hat{V}(T)=1.6+0.097\\,T$.\n\nCompute the residuals $r_{i}=V_{i}-\\hat{V}(T_{i})$ at the four temperatures:\n$$\n\\hat{V}(10)=1.6+0.097\\cdot 10=2.57,\\quad r_{1}=2.6-2.57=0.03,\n$$\n$$\n\\hat{V}(20)=1.6+0.097\\cdot 20=3.54,\\quad r_{2}=3.4-3.54=-0.14,\n$$\n$$\n\\hat{V}(30)=1.6+0.097\\cdot 30=4.51,\\quad r_{3}=4.7-4.51=0.19,\n$$\n$$\n\\hat{V}(40)=1.6+0.097\\cdot 40=5.48,\\quad r_{4}=5.4-5.48=-0.08.\n$$\nThe Euclidean norm of the residual vector $\\boldsymbol{r}$ is\n$$\n\\|\\boldsymbol{r}\\|_{2}=\\sqrt{\\sum_{i=1}^{4}r_{i}^{2}}=\\sqrt{(0.03)^{2}+(-0.14)^{2}+(0.19)^{2}+(-0.08)^{2}}=\\sqrt{0.063}.\n$$\nEvaluating the square root and rounding to three significant figures gives\n$$\n\\|\\boldsymbol{r}\\|_{2}\\approx 0.251.\n$$\nThis value is in volts because each residual is a voltage difference.", "answer": "$$\\boxed{0.251}$$", "id": "2218047"}, {"introduction": "The power of the normal equations extends beyond simple line fitting to models with multiple variables. This practice demonstrates how to fit a plane to a set of 3D data points, a common task in fields like materials science and geography. You will construct the design matrix for a multivariable linear model and solve for the coefficients that define the best-fit plane. [@problem_id:2218050]", "problem": "A materials scientist is studying the thermal properties of a newly developed alloy. They place a rectangular plate of this material in a controlled environment and measure the temperature $T$ at four different locations $(x, y)$ on the plate's surface. The measurements are assumed to be in a consistent, but unspecified, system of units for length and temperature. The scientist wishes to find the best linear model for the temperature distribution across the plate, which is of the form $T(x,y) = c_1 x + c_2 y + c_3$.\n\nThe four data points $(x, y, T)$ collected are:\n- (1, 1, 3.5)\n- (2, -1, 1.2)\n- (-1, 2, 4.0)\n- (0, -2, -0.5)\n\nUsing the method of linear least squares, determine the coefficients $c_1, c_2,$ and $c_3$ for the best-fit plane. Provide the values for $c_1, c_2,$ and $c_3$ in this specific order. Your final answers should be rounded to three significant figures.", "solution": "We seek the least-squares fit of the model $T(x,y)=c_{1}x+c_{2}y+c_{3}$ to the four data points $(x_{i},y_{i},T_{i})$. Let\n$$\nA=\\begin{pmatrix}\n1 & 1 & 1\\\\\n2 & -1 & 1\\\\\n-1 & 2 & 1\\\\\n0 & -2 & 1\n\\end{pmatrix},\\quad\n\\mathbf{c}=\\begin{pmatrix}c_{1}\\\\ c_{2}\\\\ c_{3}\\end{pmatrix},\\quad\n\\mathbf{b}=\\begin{pmatrix}3.5\\\\ 1.2\\\\ 4.0\\\\ -0.5\\end{pmatrix}.\n$$\nThe least-squares solution satisfies the normal equations $A^{\\top}A\\,\\mathbf{c}=A^{\\top}\\mathbf{b}$. Compute the components needed:\n$$\n\\sum x_{i}=2,\\quad \\sum y_{i}=0,\\quad \\sum x_{i}^{2}=6,\\quad \\sum y_{i}^{2}=10,\\quad \\sum x_{i}y_{i}=-3,\n$$\n$$\n\\sum x_{i}T_{i}=1.9,\\quad \\sum y_{i}T_{i}=11.3,\\quad \\sum T_{i}=8.2.\n$$\nThus\n$$\nA^{\\top}A=\\begin{pmatrix}6 & -3 & 2\\\\ -3 & 10 & 0\\\\ 2 & 0 & 4\\end{pmatrix},\\quad\nA^{\\top}\\mathbf{b}=\\begin{pmatrix}1.9\\\\ 11.3\\\\ 8.2\\end{pmatrix}.\n$$\nThis yields the linear system\n$$\n\\begin{cases}\n6c_{1}-3c_{2}+2c_{3}=1.9,\\\\\n-3c_{1}+10c_{2}=11.3,\\\\\n2c_{1}+4c_{3}=8.2.\n\\end{cases}\n$$\nFrom the third equation, $c_{1}+2c_{3}=4.1$, so $c_{1}=4.1-2c_{3}$. Substitute into the second equation to get $-3(4.1-2c_{3})+10c_{2}=11.3$, which simplifies to $5c_{2}+3c_{3}=11.8$. Substitute $c_{1}=4.1-2c_{3}$ into the first equation to obtain $3c_{2}+10c_{3}=22.7$. Solving\n$$\n\\begin{cases}\n5c_{2}+3c_{3}=11.8,\\\\\n3c_{2}+10c_{3}=22.7\n\\end{cases}\n$$\ngives\n$$\nc_{2}=\\frac{11.8\\cdot 10-3\\cdot 22.7}{41}=\\frac{49.9}{41}=\\frac{499}{410},\\quad\nc_{3}=\\frac{5\\cdot 22.7-3\\cdot 11.8}{41}=\\frac{78.1}{41}=\\frac{781}{410}.\n$$\nThen\n$$\nc_{1}=4.1-2c_{3}=\\frac{41}{10}-2\\cdot\\frac{781}{410}=\\frac{1681-1562}{410}=\\frac{119}{410}.\n$$\nThus the exact coefficients are\n$$\nc_{1}=\\frac{119}{410},\\quad c_{2}=\\frac{499}{410},\\quad c_{3}=\\frac{781}{410}.\n$$\nRounded to three significant figures:\n$$\nc_{1}\\approx 0.290,\\quad c_{2}\\approx 1.22,\\quad c_{3}\\approx 1.90.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}0.290 & 1.22 & 1.90\\end{pmatrix}}$$", "id": "2218050"}, {"introduction": "While the normal equations provide an elegant theoretical solution, their practical implementation can suffer from numerical instability, especially with ill-conditioned matrices. This advanced practice challenges you to go beyond pencil-and-paper calculations by implementing the normal equations solver in code. By testing your implementation on a notoriously ill-conditioned Hilbert matrix, you will gain a firsthand understanding of the method's limitations and appreciate why more stable techniques like QR factorization are often preferred in professional software. [@problem_id:3257364]", "problem": "Implement a complete program that, from first principles, derives and uses the normal-equations approach to solve a linear least squares problem and contrasts its numerical behavior on well-conditioned and ill-conditioned instances constructed from Hilbert matrices. Start from the fundamental formulation of linear least squares: given a matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$ and a right-hand side $b \\in \\mathbb{R}^m$, define the objective function $f(x) = \\tfrac{1}{2}\\lVert A x - b \\rVert_2^2$ for $x \\in \\mathbb{R}^n$. Use calculus on $f(x)$ to obtain the stationarity condition for a minimizer and implement a solver that enforces this stationarity condition using only basic linear algebra operations available in the specified runtime. Do not use any black-box least squares routine. Then, implement a numerically stable reference solver based on an orthogonal-triangular (QR) factorization (QR) to obtain a baseline solution. Explain and demonstrate how ill-conditioning in $A$ degrades the normal-equations method, particularly on Hilbert matrices whose entries are given by $H_{ij} = \\tfrac{1}{i + j - 1}$ for integers $i \\ge 1$, $j \\ge 1$.\n\nFollow these requirements.\n\n- Derivation basis: Begin from $f(x) = \\tfrac{1}{2}\\lVert A x - b \\rVert_2^2$ and the rule that the gradient of $\\tfrac{1}{2}\\lVert r(x) \\rVert_2^2$ is $J(x)^\\top r(x)$ where $J(x)$ is the Jacobian of $r(x)$. Specialize this to the linear residual $r(x) = A x - b$ and derive the stationarity condition that your solver must satisfy. Do not include any pre-derived \"shortcut\" formulas in the problem statement itself.\n- Numerical stability discussion to inform your design: In floating-point arithmetic, error growth depends on the condition number of $A$ with respect to the $2$-norm, $\\kappa_2(A)$. Recall that forming $A^\\top A$ can square the $2$-norm condition number, which amplifies roundoff compared to a QR-based approach that applies orthogonal transformations.\n- Implementation requirements:\n  - Implement a function that solves linear least squares via the normal-equations approach derived above. Solve the resulting symmetric system using a direct linear solver rather than an explicit inverse.\n  - Implement a reference solver using a thin QR factorization $A = Q R$ with $Q \\in \\mathbb{R}^{m \\times n}$ having orthonormal columns and $R \\in \\mathbb{R}^{n \\times n}$ upper triangular, then solve $R x = Q^\\top b$.\n  - Implement a function to construct a rectangular Hilbert matrix $H \\in \\mathbb{R}^{m \\times n}$ with entries $H_{ij} = \\tfrac{1}{i + j - 1}$.\n  - For each test case below, construct $b$ by $b = A x_{\\mathrm{true}} + \\varepsilon$ where $\\varepsilon \\in \\mathbb{R}^m$ is a fixed, tiny, deterministic perturbation with entries $\\varepsilon_i = \\alpha \\cdot (-1)^i$ for a specified scalar $\\alpha$. This ensures a controlled, nonzero residual for assessing sensitivity.\n- Error metrics and required output per test case:\n  - Compute the forward relative error for a solver output $x$ as $e(x) = \\dfrac{\\lVert x - x_{\\mathrm{true}} \\rVert_2}{\\lVert x_{\\mathrm{true}} \\rVert_2}$.\n  - For each test case, compute the ratio $r = \\dfrac{e(x_{\\mathrm{NE}})}{e(x_{\\mathrm{QR}})}$, where $x_{\\mathrm{NE}}$ is the normal-equations solution and $x_{\\mathrm{QR}}$ is the QR-based solution.\n  - Your program must output a single line containing a list of these ratios for all test cases, in order, formatted as a comma-separated list enclosed in square brackets, for example: $[r_1,r_2,r_3,r_4]$. Each $r_k$ must be output as a floating-point number.\n\nTest suite. Implement exactly the following four test cases to exercise different conditioning regimes and edge cases:\n\n- Test case $1$ (well-conditioned, small, consistent up to tiny perturbation):\n  - $A_1 \\in \\mathbb{R}^{3 \\times 2}$ with rows $[1,0]$, $[0,1]$, $[1,1]$.\n  - $x_{\\mathrm{true},1} = [1,-1]^\\top$.\n  - $\\alpha = 10^{-12}$ and $\\varepsilon_i = \\alpha \\cdot (-1)^i$ for $i \\in \\{1,2,3\\}$, so $b_1 = A_1 x_{\\mathrm{true},1} + \\varepsilon$.\n- Test case $2$ (Hilbert, moderate dimensions, ill-conditioned):\n  - $A_2 = H \\in \\mathbb{R}^{10 \\times 5}$ with $H_{ij} = \\tfrac{1}{i + j - 1}$ for $i \\in \\{1,\\dots,10\\}$, $j \\in \\{1,\\dots,5\\}$.\n  - $x_{\\mathrm{true},2} \\in \\mathbb{R}^5$ with entries $x_{\\mathrm{true},2} = [1,-1,1,-1,1]^\\top$.\n  - $\\alpha = 10^{-12}$ and $\\varepsilon_i = \\alpha \\cdot (-1)^i$ for $i \\in \\{1,\\dots,10\\}$, so $b_2 = A_2 x_{\\mathrm{true},2} + \\varepsilon$.\n- Test case $3$ (Hilbert, larger dimensions, more ill-conditioned):\n  - $A_3 = H \\in \\mathbb{R}^{24 \\times 12}$ with $H_{ij} = \\tfrac{1}{i + j - 1}$ for $i \\in \\{1,\\dots,24\\}$, $j \\in \\{1,\\dots,12\\}$.\n  - $x_{\\mathrm{true},3} \\in \\mathbb{R}^{12}$ with entries $x_{\\mathrm{true},3}(j) = 1$ if $j$ is odd and $x_{\\mathrm{true},3}(j) = -1$ if $j$ is even, for $j \\in \\{1,\\dots,12\\}$.\n  - $\\alpha = 10^{-12}$ and $\\varepsilon_i = \\alpha \\cdot (-1)^i$ for $i \\in \\{1,\\dots,24\\}$, so $b_3 = A_3 x_{\\mathrm{true},3} + \\varepsilon$.\n- Test case $4$ (Hilbert, edge case with one column):\n  - $A_4 = H \\in \\mathbb{R}^{10 \\times 1}$ with $H_{i1} = \\tfrac{1}{i}$ for $i \\in \\{1,\\dots,10\\}$.\n  - $x_{\\mathrm{true},4} = [1]$.\n  - $\\alpha = 10^{-12}$ and $\\varepsilon_i = \\alpha \\cdot (-1)^i$ for $i \\in \\{1,\\dots,10\\}$, so $b_4 = A_4 x_{\\mathrm{true},4} + \\varepsilon$.\n\nProgram output format specification:\n\n- Your program must produce exactly one line on standard output containing the list $[r_1,r_2,r_3,r_4]$, where each $r_k$ is the ratio defined above for test case $k$. Do not print any additional text or lines. Angles and physical units are not involved in this problem, so no unit conversions are required.", "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded exercise in numerical linear algebra. It is self-contained, with all necessary data, constants, and definitions provided. The objectives are clear, and the required analysis rests on established principles of numerical analysis and matrix computations.\n\nThe task is to solve the linear least squares problem, which seeks to find a vector $x \\in \\mathbb{R}^n$ that minimizes the objective function $f(x) = \\frac{1}{2}\\lVert A x - b \\rVert_2^2$ for a given matrix $A \\in \\mathbb{R}^{m \\times n}$ (with $m \\ge n$) and vector $b \\in \\mathbb{R}^m$. We will derive the normal equations from first principles, implement a solver based on them, contrast its numerical stability with a more robust QR-based method, and test both on a suite of problems including ill-conditioned Hilbert matrices.\n\n**1. Derivation of the Normal Equations**\n\nThe solution to the linear least squares problem is the vector $x$ that minimizes the objective function $f(x)$. Since $f(x)$ is a differentiable and convex function, its minimizer can be found by locating a point where the gradient is zero. The problem specifies using the general rule for the gradient of a squared norm of a vector function: the gradient of $g(x) = \\frac{1}{2}\\lVert r(x) \\rVert_2^2$ is $\\nabla g(x) = J(x)^\\top r(x)$, where $J(x)$ is the Jacobian matrix of the residual function $r(x)$.\n\nFor the linear least squares problem, the residual function is $r(x) = Ax - b$. This function is linear in $x$. To find its Jacobian, we consider how a small perturbation $\\delta x$ in $x$ affects $r(x)$:\n$$r(x + \\delta x) = A(x + \\delta x) - b = (Ax - b) + A \\delta x = r(x) + A \\delta x$$\nThe Jacobian $J(x)$ is the matrix that linearly approximates the change in $r(x)$, i.e., $r(x + \\delta x) \\approx r(x) + J(x) \\delta x$. By direct comparison, the Jacobian of $r(x) = Ax - b$ is the constant matrix $J(x) = A$.\n\nNow, we apply the provided gradient rule to our objective function $f(x)$:\n$$\\nabla f(x) = J(x)^\\top r(x) = A^\\top (Ax - b)$$\nA necessary condition for $x$ to be a minimizer is that the gradient at $x$ must be the zero vector:\n$$\\nabla f(x) = 0$$\nThis leads to the stationarity condition:\n$$A^\\top (Ax - b) = 0$$\nRearranging this equation yields the system of **normal equations**:\n$$A^\\top A x = A^\\top b$$\nThis is a square system of $n$ linear equations in $n$ unknowns. The matrix $C = A^\\top A$ is symmetric. If the columns of $A$ are linearly independent (i.e., $A$ has full column rank, which is typical for overdetermined least squares problems), then $A^\\top A$ is also positive definite and thus invertible, guaranteeing a unique solution for $x$.\n\n**2. Numerical Stability and Solution Methods**\n\n**Normal Equations Method:**\nThis method consists of two steps:\n1.  Form the matrix $A^\\top A$ and the vector $A^\\top b$.\n2.  Solve the resulting $n \\times n$ linear system $(A^\\top A) x = (A^\\top b)$ for $x$.\n\nThe primary numerical drawback of this method relates to the condition number of the matrix $A^\\top A$. The $2$-norm condition number of a matrix $M$, denoted $\\kappa_2(M)$, measures the sensitivity of the solution of a system $My=d$ to perturbations in $M$ and $d$. For the normal equations, the condition number of the system is $\\kappa_2(A^\\top A)$. It can be shown that $\\kappa_2(A^\\top A) = (\\kappa_2(A))^2$.\n\nIf $A$ is ill-conditioned, meaning $\\kappa_2(A)$ is large, then $\\kappa_2(A^\\top A)$ will be much larger. For example, if $\\kappa_2(A) = 10^4$, then $\\kappa_2(A^\\top A) = 10^8$. Solving a system with such a large condition number can lead to significant amplification of roundoff errors present in floating-point arithmetic, resulting in a computed solution $x_{\\mathrm{NE}}$ that is far from the true minimizer.\n\n**QR Factorization Method:**\nA more numerically stable approach avoids the explicit formation of $A^\\top A$. This can be achieved using an orthogonal-triangular (QR) factorization of $A$. For $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$ and full column rank, we can compute a \"thin\" QR factorization:\n$$A = QR$$\nwhere $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns ($Q^\\top Q = I_n$, where $I_n$ is the $n \\times n$ identity matrix), and $R \\in \\mathbb{R}^{n \\times n}$ is an upper triangular matrix with positive diagonal entries.\n\nSubstituting this into the objective function:\n$$\\lVert Ax - b \\rVert_2^2 = \\lVert QRx - b \\rVert_2^2$$\nWe can multiply the residual vector $QRx - b$ by any orthogonal matrix without changing its $2$-norm. Although $Q$ is not square, we can leverage the property $Q^\\top Q = I_n$. The minimizer $x$ of $\\lVert Ax-b \\rVert_2$ is the solution to the normal equations $A^\\top A x = A^\\top b$. Substituting $A=QR$:\n$$(QR)^\\top (QR) x = (QR)^\\top b$$\n$$R^\\top Q^\\top Q R x = R^\\top Q^\\top b$$\n$$R^\\top I_n R x = R^\\top Q^\\top b$$\n$$R^\\top R x = R^\\top Q^\\top b$$\nSince $R$ is invertible (as $A$ has full rank), $R^\\top$ is also invertible. We can left-multiply by $(R^\\top)^{-1}$ to obtain:\n$$Rx = Q^\\top b$$\nThis is an upper triangular system for $x$, which can be solved efficiently and accurately using back substitution. The condition number of this system is $\\kappa_2(R)$. It can be shown that $\\kappa_2(R) = \\kappa_2(A)$. Thus, the QR method solves a system with the same condition number as the original matrix $A$, avoiding the squaring effect seen in the normal equations. This makes the QR method significantly more robust against roundoff errors when $A$ is ill-conditioned.\n\n**3. Implementation and Testing Strategy**\n\nWe will implement three core functions:\n- `construct_hilbert(m, n)`: Creates an $m \\times n$ Hilbert matrix $H$ with entries $H_{ij} = 1/(i+j-1)$, for $1$-based indices $i, j$.\n- `solve_normal_equations(A, b)`: Solves the least squares problem by forming and solving $A^\\top A x = A^\\top b$ using a standard direct solver like `numpy.linalg.solve`.\n- `solve_qr(A, b)`: Solves the least squares problem by computing the thin QR factorization of $A$ and solving $Rx = Q^\\top b$. `numpy.linalg.qr` with `mode='reduced'` will be used for the factorization.\n\nThe program will iterate through the four specified test cases. For each case, it will:\n1.  Construct the matrix $A$ (either a custom matrix or a Hilbert matrix).\n2.  Define the true solution $x_{\\mathrm{true}}$.\n3.  Construct the right-hand side vector $b = A x_{\\mathrm{true}} + \\varepsilon$, where $\\varepsilon$ is a small, deterministic perturbation vector.\n4.  Compute the solution using both the normal equations method ($x_{\\mathrm{NE}}$) and the QR method ($x_{\\mathrm{QR}}$).\n5.  Calculate the forward relative error for each solution: $e(x) = \\lVert x - x_{\\mathrm{true}} \\rVert_2 / \\lVert x_{\\mathrm{true}} \\rVert_2$.\n6.  Compute the ratio of these errors, $r = e(x_{\\mathrm{NE}}) / e(x_{\\mathrm{QR}})$.\n\nThe final output will be a list of these ratios, which will demonstrate the degradation in accuracy of the normal equations method as the condition number of $A$ increases, especially for the Hilbert matrix test cases. For well-conditioned problems (Case $1$) and the special single-column case (Case $4$), the ratio is expected to be close to $1$, indicating comparable accuracy. For ill-conditioned Hilbert matrices (Cases $2$ and $3$), the ratio is expected to be significantly larger than $1$, highlighting the superior numerical stability of the QR-based approach.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef construct_hilbert(m, n):\n    \"\"\"\n    Constructs an m x n Hilbert matrix.\n    The entry (i, j) is 1 / (i + j - 1), using 1-based indexing.\n    \"\"\"\n    i = np.arange(1, m + 1).reshape(m, 1)\n    j = np.arange(1, n + 1).reshape(1, n)\n    return 1.0 / (i + j - 1)\n\ndef solve_normal_equations(A, b):\n    \"\"\"\n    Solves the linear least squares problem min ||Ax - b||_2\n    using the normal equations A.T*A*x = A.T*b.\n    \"\"\"\n    AtA = A.T @ A\n    Atb = A.T @ b\n    x_ne = np.linalg.solve(AtA, Atb)\n    return x_ne\n\ndef solve_qr(A, b):\n    \"\"\"\n    Solves the linear least squares problem min ||Ax - b||_2\n    using a thin QR factorization A = QR, solving Rx = Q.T*b.\n    \"\"\"\n    Q, R = np.linalg.qr(A, mode='reduced')\n    Qtb = Q.T @ b\n    x_qr = np.linalg.solve(R, Qtb)\n    return x_qr\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the required output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases_spec = [\n        # Test case 1: well-conditioned, small\n        {\n            'm': 3, 'n': 2, 'alpha': 1e-12,\n            'A_def': lambda m, n: np.array([[1., 0.], [0., 1.], [1., 1.]]),\n            'xtrue_def': lambda n: np.array([1., -1.])\n        },\n        # Test case 2: Hilbert, moderate dimensions, ill-conditioned\n        {\n            'm': 10, 'n': 5, 'alpha': 1e-12,\n            'A_def': construct_hilbert,\n            'xtrue_def': lambda n: np.array([1. if (i % 2) == 0 else -1. for i in range(n)])\n        },\n        # Test case 3: Hilbert, larger dimensions, more ill-conditioned\n        {\n            'm': 24, 'n': 12, 'alpha': 1e-12,\n            'A_def': construct_hilbert,\n            'xtrue_def': lambda n: np.array([1. if (i % 2) == 0 else -1. for i in range(n)])\n        },\n        # Test case 4: Hilbert, edge case with one column\n        {\n            'm': 10, 'n': 1, 'alpha': 1e-12,\n            'A_def': construct_hilbert,\n            'xtrue_def': lambda n: np.array([1.])\n        }\n    ]\n\n    ratios = []\n\n    for case in test_cases_spec:\n        m, n, alpha = case['m'], case['n'], case['alpha']\n        \n        # 1. Construct matrix A\n        A = case['A_def'](m, n)\n        \n        # 2. Construct true solution xtrue\n        xtrue = case['xtrue_def'](n)\n        \n        # 3. Construct the right-hand side b = A*xtrue + epsilon\n        # Perturbation epsilon_i = alpha * (-1)^i for i = 1, ..., m\n        epsilon = alpha * ((-1.0)**np.arange(1, m + 1))\n        b = A @ xtrue + epsilon\n        \n        # 4. Solve using both methods\n        x_ne = solve_normal_equations(A, b)\n        x_qr = solve_qr(A, b)\n        \n        # 5. Compute forward relative errors\n        norm_xtrue = np.linalg.norm(xtrue)\n        \n        err_ne = np.linalg.norm(x_ne - xtrue) / norm_xtrue\n        err_qr = np.linalg.norm(x_qr - xtrue) / norm_xtrue\n        \n        # 6. Compute and store the ratio of errors\n        # Handle the unlikely case of err_qr being zero to avoid division by zero\n        if err_qr == 0.0:\n            if err_ne == 0.0:\n                ratio = 1.0\n            else:\n                ratio = float('inf')\n        else:\n            ratio = err_ne / err_qr\n            \n        ratios.append(ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, ratios))}]\")\n\nsolve()\n```", "id": "3257364"}]}