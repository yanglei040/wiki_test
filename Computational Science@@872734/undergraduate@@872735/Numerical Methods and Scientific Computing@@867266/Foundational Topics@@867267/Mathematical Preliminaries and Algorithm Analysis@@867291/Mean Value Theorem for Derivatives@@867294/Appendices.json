{"hands_on_practices": [{"introduction": "The Mean Value Theorem guarantees the existence of a point $c$ with a special property, but it doesn't tell us how to find it. This practice transforms the theorem's abstract statement into a concrete computational task. By reformulating the MVT equation $f'(c) = \\frac{f(b) - f(a)}{b-a}$ as a root-finding problem, you will develop a robust numerical algorithm to locate all such values of $c$ for any given polynomial [@problem_id:3251031].", "problem": "Design and implement a complete, runnable program that, for a given real polynomial $f$ and a closed interval $[a,b]$ with $a<b$, numerically finds all values $c$ in the open interval $(a,b)$ that satisfy the Mean Value Theorem (MVT) for derivatives (Mean Value Theorem (MVT): there exists at least one point where the instantaneous rate of change equals the average rate of change over $[a,b]$). Your program must solve this by constructing and solving a root-finding problem derived from first principles of differentiation and average rate of change, and it must robustly handle numerical issues such as complex roots produced by polynomial root solvers, endpoint exclusion, and spurious near-real roots.\n\nUse only real arithmetic for the final answers. For a polynomial $f(x)$ given by coefficients in descending powers (highest degree first), proceed as follows:\n- Compute the derivative polynomial $f'(x)$ from the coefficients of $f(x)$ using the definition of the derivative for polynomials.\n- Compute the average rate of change $s$ on $[a,b]$ using the difference quotient $s = \\dfrac{f(b)-f(a)}{b-a}$.\n- Form the polynomial $g(x) = f'(x) - s$ and find all of its roots numerically.\n- From the numerical roots of $g(x)$, keep only those that are real within tolerance, lie strictly inside $(a,b)$ (endpoints excluded), and satisfy $|f'(x)-s|$ within a residual tolerance.\n- Deduplicate numerically indistinguishable roots.\n\nAlgorithmic and numerical requirements:\n- Let the imaginary-part tolerance be $\\tau_{\\mathrm{imag}} = 10^{-9}$; accept a root $z$ only if $|\\operatorname{Im}(z)| \\le \\tau_{\\mathrm{imag}}$.\n- Let the residual tolerance be $\\tau_{\\mathrm{res}} = 10^{-8}$; accept a real candidate $x$ only if $|f'(x)-s| \\le \\tau_{\\mathrm{res}}$.\n- Let the interval guard be $\\tau_{\\mathrm{int}} = 10^{-12}$; accept only $x$ with $a+\\tau_{\\mathrm{int}} \\le x \\le b-\\tau_{\\mathrm{int}}$.\n- When multiple accepted roots are within $\\tau_{\\mathrm{merge}} = 10^{-8}$ of each other, merge them into a single representative by sorting and keeping the first.\n- If $f$ is affine on $[a,b]$ (equivalently $f'(x)$ is constant and equals $s$), there are infinitely many valid $c$; return the empty list for that case.\n\nInput and data model:\n- Hard-code the test suite below directly in your program; no user input is required.\n- Each test case is a tuple $(\\text{coeffs}, a, b)$ where $\\text{coeffs}$ is a list of real numbers giving the polynomial coefficients from highest degree to constant term.\n\nTest suite to implement and solve:\n- Case $1$: $f(x) = x^3 - 3x$ on $[a,b] = [-2,2]$. Coefficients $\\,[1,0,-3,0]$.\n- Case $2$: $f(x) = x^4$ on $[a,b] = [-1,2]$. Coefficients $\\,[1,0,0,0,0]$.\n- Case $3$: $f(x) = x^4 - 2x^2$ on $[a,b] = [-1,1]$. Coefficients $\\,[1,0,-2,0,0]$.\n- Case $4$: $f(x) = x^5 - 5x^3 + 4x$ on $[a,b] = [-2,2]$. Coefficients $\\,[1,0,-5,0,4,0]$.\n- Case $5$: $f(x) = x^2$ on $[a,b] = [0,10^{-8}]$. Coefficients $\\,[1,0,0]$.\n\nOutput specification:\n- For each test case, output the sorted list (ascending) of all accepted values of $c$ found in $(a,b)$.\n- Represent each real number using fixed-point notation with exactly $12$ digits after the decimal point.\n- Aggregate the lists for all test cases into a single line, formatted as a comma-separated list enclosed in square brackets. For example, a valid format with two cases is $[[c_{1,1},c_{1,2}],[c_{2,1}]]$ (no spaces required).\n- Your program must produce exactly one line of output, containing the lists corresponding to the five cases above, in the stated order.\n\nNotes:\n- Express all numerical answers without physical units.\n- All angles, if any appeared, would be in radians, but none are used here.\n- The final output must be a single line as specified; do not print any extra text or diagnostics.", "solution": "The problem requires finding all values $c$ in an open interval $(a,b)$ that satisfy the conclusion of the Mean Value Theorem (MVT) for a given real polynomial $f(x)$. The MVT states that for a function $f$ that is continuous on the closed interval $[a,b]$ and differentiable on the open interval $(a,b)$, there must exist at least one number $c \\in (a,b)$ such that the instantaneous rate of change at $c$ equals the average rate of change over $[a,b]$. Since any polynomial function is continuous and differentiable on the entire real line, the MVT is always applicable. The theorem's conclusion is expressed by the equation:\n$$\nf'(c) = \\frac{f(b) - f(a)}{b-a}\n$$\nTo find all such values of $c$, we can transform this into a root-finding problem. Let $s$ represent the constant average rate of change (the slope of the secant line connecting the endpoints of the graph of $f$ over the interval):\n$$\ns = \\frac{f(b) - f(a)}{b-a}\n$$\nThe values of $c$ we seek are the solutions to the equation $f'(c) = s$ that lie strictly within the interval $(a,b)$. This is equivalent to finding the roots of a new polynomial, $g(x)$, defined as:\n$$\ng(x) = f'(x) - s\n$$\nThe algorithmic procedure to solve this problem is as follows:\n\n1.  **Polynomial Representation and Differentiation**: A real polynomial of degree $n$, given by $f(x) = p_{n}x^{n} + p_{n-1}x^{n-1} + \\dots + p_{1}x + p_{0}$, is represented by its vector of coefficients $[p_{n}, p_{n-1}, \\dots, p_{0}]$. Its derivative, $f'(x)$, is a polynomial of degree $n-1$:\n    $$\n    f'(x) = n p_{n}x^{n-1} + (n-1)p_{n-1}x^{n-2} + \\dots + p_{1}\n    $$\n    The coefficients of $f'(x)$ are algorithmically computed from the coefficients of $f(x)$.\n\n2.  **Formation of the Root-Finding Equation**: The scalar value $s$ is first calculated using its definition. With $a<b$ guaranteed, the denominator $b-a$ is always positive. We then construct the polynomial $g(x) = f'(x) - s$ by subtracting $s$ from the constant term of $f'(x)$.\n\n3.  **Handling the Affine Case**: A special case arises if $f'(x)$ is a constant function that is equal to $s$ over the interval. This implies that $g(x) = f'(x) - s$ is the zero polynomial, meaning all of its coefficients are zero. This situation occurs if $f(x)$ is an affine function (degree at most $1$) and leads to infinitely many solutions for $c$ in $(a,b)$. As specified, our procedure identifies this case by checking if all coefficients of $g(x)$ are numerically zero and returns an empty list.\n\n4.  **Numerical Root Finding**: We find all complex roots of the polynomial equation $g(x)=0$. Standard numerical libraries provide robust methods, such as computing the eigenvalues of the companion matrix associated with the polynomial, to find all roots simultaneously.\n\n5.  **Filtering and Selection of Roots**: The set of computed roots, which may be complex and contain numerical errors, is subjected to a rigorous filtering process:\n    *   **Reality Check**: A root $z = x+iy$ is accepted as a real-valued candidate if its imaginary part is sufficiently close to zero, specifically $|\\operatorname{Im}(z)| \\le \\tau_{\\mathrm{imag}}$, where the tolerance is given as $\\tau_{\\mathrm{imag}} = 10^{-9}$. Only the real part, $x$, of such roots is retained.\n    *   **Interval Confinement**: A real candidate $x$ must lie strictly within the open interval $(a,b)$. To avoid issues with floating-point precision at the endpoints, this is enforced using a small guard band: $a + \\tau_{\\mathrm{int}} \\le x \\le b - \\tau_{\\mathrm{int}}$, where $\\tau_{\\mathrm{int}} = 10^{-12}$.\n    *   **Residual Verification**: As a final check on the numerical accuracy of a candidate root $x$, we verify that it satisfies the original MVT equation within a small residual. The condition is $|f'(x) - s| \\le \\tau_{\\mathrm{res}}$, with the tolerance $\\tau_{\\mathrm{res}} = 10^{-8}$.\n\n6.  **Deduplication and Finalization**: The filtered set of valid roots is first sorted in ascending order. It is possible that multiple computed roots are numerical approximations of a single true root. To consolidate these, we merge any roots that are numerically close. A sorted sequence of roots is iterated through; a root is added to the final list only if its distance from the previously added root (representing the last cluster) exceeds a merging tolerance, $\\tau_{\\mathrm{merge}} = 10^{-8}$. This \"sort and keep the first\" strategy ensures each distinct solution is represented uniquely.\n\nThis integrated approach, combining foundational calculus principles with robust numerical computation and filtering, provides a complete and reliable method for solving the stated problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_mvt_case(coeffs, a, b):\n    \"\"\"\n    Finds all values c in (a,b) that satisfy the Mean Value Theorem for a polynomial.\n\n    Args:\n        coeffs (list of float): Coefficients of the polynomial f(x) in descending order.\n        a (float): The start of the interval [a,b].\n        b (float): The end of the interval [a,b].\n\n    Returns:\n        list of float: Sorted list of unique values of c satisfying the MVT.\n    \"\"\"\n    # Numerical tolerances from the problem statement\n    T_IMAG = 1e-9\n    T_RES = 1e-8\n    T_INT = 1e-12\n    T_MERGE = 1e-8\n\n    # 1. Represent f(x) as a polynomial object and compute its derivative f'(x)\n    f_poly = np.poly1d(coeffs)\n    f_prime_poly = f_poly.deriv()\n\n    # 2. Compute the average rate of change s on [a,b]\n    if b - a == 0:\n        return []  # Should not happen given a < b, but for robustness.\n    s = (f_poly(b) - f_poly(a)) / (b - a)\n\n    # 3. Form the polynomial g(x) = f'(x) - s\n    g_poly = f_prime_poly - s\n\n    # 4. Handle the special case of infinitely many solutions\n    # This occurs if g(x) is the zero polynomial (f'(x) is constant and equals s).\n    if np.allclose(g_poly.coeffs, 0):\n        return []\n\n    # 5. Find all complex roots of g(x) = 0\n    roots = np.roots(g_poly.coeffs)\n\n    # 6. Filter the roots based on the specified criteria\n    # 6.1. Keep only roots that are effectively real\n    real_candidates = [r.real for r in roots if abs(r.imag) <= T_IMAG]\n\n    # 6.2. Keep only roots strictly within the interval (a, b) with a guard\n    interval_candidates = [x for x in real_candidates if (a + T_INT) <= x <= (b - T_INT)]\n\n    # 6.3. Keep only roots that satisfy the residual tolerance\n    accepted_roots = [x for x in interval_candidates if abs(f_prime_poly(x) - s) <= T_RES]\n    \n    # 7. Sort and deduplicate the final list of roots\n    accepted_roots.sort()\n\n    if not accepted_roots:\n        return []\n\n    # Merge numerically close roots by keeping the first of each cluster\n    deduplicated_roots = [accepted_roots[0]]\n    last_kept_root = accepted_roots[0]\n    for i in range(1, len(accepted_roots)):\n        if abs(accepted_roots[i] - last_kept_root) > T_MERGE:\n            deduplicated_roots.append(accepted_roots[i])\n            last_kept_root = accepted_roots[i]\n\n    return deduplicated_roots\n\ndef solve():\n    \"\"\"\n    Main driver function to run the test suite and print the formatted output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ([1, 0, -3, 0], -2.0, 2.0),\n        ([1, 0, 0, 0, 0], -1.0, 2.0),\n        ([1, 0, -2, 0, 0], -1.0, 1.0),\n        ([1, 0, -5, 0, 4, 0], -2.0, 2.0),\n        ([1, 0, 0], 0.0, 1e-8),\n    ]\n\n    all_results_formatted = []\n    for case in test_cases:\n        coeffs, a, b = case\n        c_values = solve_mvt_case(coeffs, a, b)\n        \n        # Format numbers to fixed-point with 12 decimal places\n        formatted_c_values = [f\"{c:.12f}\" for c in c_values]\n        \n        # Format the list of numbers as a string \"[num1,num2,...]\"\n        result_string = f\"[{','.join(formatted_c_values)}]\"\n        all_results_formatted.append(result_string)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_formatted)}]\")\n\nsolve()\n```", "id": "3251031"}, {"introduction": "A deep understanding of a theorem includes knowing where it does not apply. This exercise challenges the direct generalization of Rolle's Theorem, a special case of the MVT, to functions with complex or vector values. You will implement a numerical test to investigate this limitation and discover why the structure of the real number line is so crucial to the theorem's traditional proof [@problem_id:3251052].", "problem": "You will examine a natural but incorrect generalization of a classical real-variable result to functions taking values in higher-dimensional normed spaces. Work from the following fundamental base: the definition of the derivative as the limit of difference quotients, the definition of a uniform partition of an interval, and the Extreme Value Theorem for real-valued continuous functions on compact intervals. Do not assume any extension of these real-variable facts to complex- or vector-valued functions without justification.\n\nTask. Consider the following hypothesis for functions on a real interval: If a function $f$ is continuous on $[a,b]$, differentiable on $(a,b)$, and satisfies $f(a)=f(b)$, then there exists some $c\\in(a,b)$ with $f'(c)=0$. Your objective is to design a numerical test that, given a function $f:[a,b]\\to Y$ where $Y$ is either $\\mathbb{R}$, $\\mathbb{C}$, or $\\mathbb{R}^m$ with the Euclidean norm, attempts to determine whether there exists $c\\in(a,b)$ with $\\lVert f'(c)\\rVert\\le \\varepsilon$, for a prescribed tolerance $\\varepsilon>0$.\n\nAlgorithm specification. Implement the following procedure for each test case:\n- Construct a uniform grid $t_k=a+k h$ for $k=0,1,\\dots,N-1$ with $h=(b-a)/(N-1)$ and integer $N\\ge 3$.\n- Approximate the derivative on the interior nodes using the centered difference\n$$D_h f(t_k)=\\frac{f(t_{k+1})-f(t_{k-1})}{2h},\\quad k=1,2,\\dots,N-2.$$\n- Compute the Euclidean norm $\\lVert D_h f(t_k)\\rVert$; for $Y=\\mathbb{R}$ or $Y=\\mathbb{C}$ this is the absolute value. Decide the predicate\n$$\\min_{1\\le k\\le N-2} \\lVert D_h f(t_k)\\rVert \\le \\varepsilon,$$\nwhich numerically asserts the existence of a point $c\\in(a,b)$ where the derivative is (approximately) zero. Return a boolean for each test case.\n\nAngle unit. When trigonometric functions are used, interpret angles in radians. Do not use degrees.\n\nTest suite. Apply your procedure to the following four functions and intervals, each with its own grid size $N$ and tolerance $\\varepsilon$:\n- Case A (complex-valued, periodic endpoints): $f(t)=e^{i t}$ on $[0,2\\pi]$, with $N=4001$ and $\\varepsilon=10^{-3}$. Angles are in radians.\n- Case B (vector-valued, equal endpoints): $f(t)=(t^2-1,\\;t^3-t)$ on $[-1,1]$, with $N=2001$ and $\\varepsilon=10^{-3}$.\n- Case C (real-valued baseline, equal endpoints): $f(t)=\\cos t$ on $[0,2\\pi]$, with $N=20001$ and $\\varepsilon=10^{-3}$. Angles are in radians.\n- Case D (constant complex-valued, trivial): $f(t)=2+3i$ on $[0,1]$, with $N=11$ and $\\varepsilon=10^{-3}$.\n\nRequired final output format. Your program should produce a single line of output containing the four boolean results, in the order A, B, C, D, as a comma-separated list enclosed in square brackets; for example, the output must be of the form\n[True,False,True,False]\nwith no additional text.", "solution": "The problem statement is critically examined and found to be valid. It is a well-posed, self-contained problem in numerical analysis that is scientifically grounded and free of ambiguity. The task is to implement a specific numerical algorithm to test a generalization of Rolle's theorem for vector-valued functions, using provided test cases.\n\n### Theoretical Foundation\n\nRolle's Theorem is a fundamental result in real analysis. It states that for any real-valued function $f:[a,b] \\to \\mathbb{R}$ that is continuous on the closed interval $[a,b]$, differentiable on the open interval $(a,b)$, and satisfies the condition $f(a)=f(b)$, there must exist at least one point $c \\in (a,b)$ where the derivative is zero, i.e., $f'(c)=0$. The proof relies on the Extreme Value Theorem, which guarantees that $f$ attains a global maximum and minimum on $[a,b]$. If both extrema are at the endpoints, then $f$ must be a constant function, making its derivative zero everywhere in $(a,b)$. Otherwise, at least one extremum occurs at an interior point $c \\in (a,b)$, and by Fermat's theorem on stationary points, the derivative at this point must be zero, $f'(c)=0$.\n\nThe problem requires an investigation into whether this theorem generalizes to functions with a vector-valued codomain, such as $f:[a,b] \\to Y$ where $Y$ is $\\mathbb{R}^m$ or $\\mathbb{C}$. The direct generalization fails. The concept of an ordered maximum or minimum does not apply to vector spaces of dimension greater than one, so the proof strategy for the real-valued case breaks down. A function can satisfy the endpoint condition $f(a)=f(b)$ without its derivative vector ever being the zero vector in $(a,b)$. The classic counterexample, provided as Test Case A, is the function $f(t)=e^{it}$ on the interval $[0, 2\\pi]$. This function satisfies $f(0)=e^{i0}=1$ and $f(2\\pi)=e^{i2\\pi}=1$. However, its derivative is $f'(t)=ie^{it}$, and the norm of the derivative is $\\lVert f'(t) \\rVert = |ie^{it}| = |i| \\cdot |e^{it}| = 1 \\cdot 1 = 1$. The derivative's norm is constant and never zero.\n\n### Numerical Algorithm\n\nThe specified numerical procedure is designed to empirically test for the existence of a point $c$ where the norm of the derivative $\\lVert f'(c) \\rVert$ is close to zero. This is done by approximating the derivative at a finite number of points and checking if its norm falls below a prescribed tolerance $\\varepsilon > 0$.\n\n1.  **Discretization**: The domain interval $[a,b]$ is discretized into a uniform grid of $N$ points, denoted by $t_k = a + k h$ for $k \\in \\{0, 1, \\dots, N-1\\}$, where the step size is $h = (b-a)/(N-1)$.\n\n2.  **Derivative Approximation**: The derivative $f'(t_k)$ at each interior grid point $t_k$ (for $k \\in \\{1, \\dots, N-2\\}$) is approximated by the second-order accurate centered difference formula:\n    $$D_h f(t_k) = \\frac{f(t_{k+1}) - f(t_{k-1})}{2h}$$\n\n3.  **Norm Calculation**: The magnitude of each derivative approximation vector $D_h f(t_k)$ is computed. The problem specifies the Euclidean norm for $\\mathbb{R}^m$ and the absolute value for $\\mathbb{R}$ and $\\mathbb{C}$. Note that the absolute value of a complex number $z=x+iy$ is $|z|=\\sqrt{x^2+y^2}$, which is precisely the Euclidean norm of the corresponding vector $(x,y)$ in $\\mathbb{R}^2$.\n\n4.  **Decision Criterion**: The algorithm finds the minimum norm among all computed derivative approximations, $\\min_{1 \\le k \\le N-2} \\lVert D_h f(t_k) \\rVert$. The final output is the boolean result of the comparison:\n    $$\\min_{1 \\le k \\le N-2} \\lVert D_h f(t_k) \\rVert \\le \\varepsilon$$\n\n### Analysis of Test Cases\n\n**Case A**: Function $f(t)=e^{i t}$ on $[0,2\\pi]$, with $N=4001$ and $\\varepsilon=10^{-3}$.\nThe function satisfies $f(0) = f(2\\pi) = 1$. The step size is $h = (2\\pi)/(4000) = \\pi/2000$. The numerical derivative is $D_h f(t_k) = \\frac{e^{i(t_k+h)} - e^{i(t_k-h)}}{2h} = e^{it_k}\\frac{e^{ih}-e^{-ih}}{2h} = i e^{it_k} \\frac{\\sin(h)}{h}$.\nThe norm is therefore constant for all interior points: $\\lVert D_h f(t_k) \\rVert = |i e^{it_k} \\frac{\\sin(h)}{h}| = \\frac{\\sin(h)}{h}$. As $h \\to 0$, $\\sin(h)/h \\to 1$. For $h=\\pi/2000$, the value is $\\frac{\\sin(\\pi/2000)}{\\pi/2000} \\approx 1 - (\\pi/2000)^2/6 \\approx 0.99999959$, which is significantly greater than $\\varepsilon=10^{-3}$. The test will return `False`.\n\n**Case B**: Function $f(t)=(t^2-1,\\;t^3-t)$ on $[-1,1]$, with $N=2001$ and $\\varepsilon=10^{-3}$.\nThe function satisfies $f(-1) = (0,0)$ and $f(1)=(0,0)$. The analytical derivative is $f'(t)=(2t, 3t^2-1)$. This derivative is never the zero vector, as the first component is zero only at $t=0$, where the second component is $-1$. The squared norm of the derivative is $\\lVert f'(t) \\rVert^2 = (2t)^2 + (3t^2-1)^2 = 9t^4 - 2t^2 + 1$. Its minimum value on $[-1,1]$ occurs at $t=\\pm 1/3$, where $\\lVert f'(t) \\rVert^2 = 8/9$. The minimum norm is thus $\\sqrt{8/9} \\approx 0.9428$. Since the centered difference is a good approximation, the minimum of the norms of the numerical derivatives will be close to this value, which is much larger than $\\varepsilon=10^{-3}$. The test will return `False`.\n\n**Case C**: Function $f(t)=\\cos t$ on $[0,2\\pi]$, with $N=20001$ and $\\varepsilon=10^{-3}$.\nThis is a real-valued function with $f(0)=f(2\\pi)=1$, so Rolle's Theorem applies. The derivative $f'(t)=-\\sin t$ is zero at $t=\\pi \\in (0,2\\pi)$. The step size is $h=2\\pi/20000=\\pi/10000$. The grid point $t_{10000} = 10000 \\cdot h = \\pi$. Therefore, the algorithm will compute the numerical derivative exactly at the point where the analytical derivative is zero. The centered difference at $t=\\pi$ is $D_h f(\\pi) = \\frac{\\cos(\\pi+h)-\\cos(\\pi-h)}{2h} = \\frac{-\\cos h - (-\\cos h)}{2h} = 0$. The minimum norm will be $0$. Since $0 \\le \\varepsilon=10^{-3}$, the test will return `True`.\n\n**Case D**: Function $f(t)=2+3i$ on $[0,1]$, with $N=11$ and $\\varepsilon=10^{-3}$.\nThis is a constant function. Its derivative is identically zero, $f'(t)=0$ for all $t$. The numerical centered difference $D_h f(t_k) = \\frac{(2+3i)-(2+3i)}{2h}=0$ for all interior points $t_k$. The minimum norm will be $0$. Since $0 \\le \\varepsilon=10^{-3}$, the test will return `True`.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a numerical test for a generalization of Rolle's theorem\n    and applies it to four specific cases.\n    \"\"\"\n\n    # Define the four test functions as specified in the problem.\n    # Case A: Complex-valued function f(t) = exp(i*t)\n    f_A = lambda t: np.exp(1j * t)\n    \n    # Case B: Vector-valued function f(t) = (t^2-1, t^3-t)\n    # The lambda function is constructed to return an array of shape (len(t), 2).\n    f_B = lambda t: np.array([t**2 - 1, t**3 - t]).T\n    \n    # Case C: Real-valued function f(t) = cos(t)\n    f_C = lambda t: np.cos(t)\n    \n    # Case D: Constant complex-valued function f(t) = 2+3i\n    f_D = lambda t: (2 + 3j) * np.ones_like(t)\n\n    # Define the parameters for each test case.\n    test_cases = [\n        {'f': f_A, 'a': 0.0, 'b': 2 * np.pi, 'N': 4001, 'eps': 1e-3, 'is_vector': False},\n        {'f': f_B, 'a': -1.0, 'b': 1.0, 'N': 2001, 'eps': 1e-3, 'is_vector': True},\n        {'f': f_C, 'a': 0.0, 'b': 2 * np.pi, 'N': 20001, 'eps': 1e-3, 'is_vector': False},\n        {'f': f_D, 'a': 0.0, 'b': 1.0, 'N': 11, 'eps': 1e-3, 'is_vector': False}\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Extract parameters for the current case.\n        f = case['f']\n        a, b, N = case['a'], case['b'], case['N']\n        eps = case['eps']\n        is_vector = case['is_vector']\n\n        # Construct the uniform grid t_k = a + k*h for k = 0, ..., N-1.\n        t = np.linspace(a, b, N)\n        h = (b - a) / (N - 1)\n        \n        # Evaluate the function at all grid points.\n        f_vals = f(t)\n        \n        # Approximate the derivative on the interior nodes (k=1, ..., N-2)\n        # using the centered difference formula: D_h f(t_k) = (f(t_{k+1}) - f(t_{k-1})) / (2h).\n        # f_vals[2:] corresponds to f(t_{k+1}) for k in {1, ..., N-2}.\n        # f_vals[:-2] corresponds to f(t_{k-1}) for k in {1, ..., N-2}.\n        D_f = (f_vals[2:] - f_vals[:-2]) / (2 * h)\n        \n        # Compute the norm of the derivative approximations.\n        if is_vector:\n            # For f: R -> R^m, D_f is an (N-2, m) array.\n            # np.linalg.norm with axis=1 computes the Euclidean norm for each row vector.\n            norms = np.linalg.norm(D_f, axis=1)\n        else:\n            # For f: R -> R or f: R -> C, np.abs computes the absolute value or complex modulus.\n            norms = np.abs(D_f)\n            \n        # Find the minimum norm among all interior points.\n        min_norm = np.min(norms)\n        \n        # Apply the predicate: min ||D_h f(t_k)|| <= epsilon.\n        results.append(min_norm <= eps)\n\n    # Format the output as a comma-separated list of booleans in brackets.\n    # The default str() for a boolean (e.g., str(True)) is 'True', which matches the example format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3251052"}, {"introduction": "The Mean Value Theorem is not just a theoretical curiosity; it is a fundamental tool for analyzing the accuracy of numerical algorithms. In this practice, you will use Taylor's theorem, a powerful extension of the MVT, to derive the truncation error for common finite difference formulas. You will then write code to numerically verify these theoretical error estimates, connecting abstract calculus concepts to the practical performance of numerical differentiation [@problem_id:3251126].", "problem": "You are asked to connect the Mean Value Theorem (MVT) for derivatives to the truncation error of finite difference formulas, and to numerically verify the resulting order of accuracy. Begin from core definitions in calculus: continuity, differentiability, the Fundamental Theorem of Calculus, and the Mean Value Theorem for derivatives. The Mean Value Theorem (MVT) states that for a function $f$ that is differentiable on an interval $(a,b)$ and continuous on $[a,b]$, there exists $\\xi \\in (a,b)$ such that $f(b) - f(a) = (b-a) f'(\\xi)$. Use this principle to reason about error terms.\n\nTask A (derivation): Using the Mean Value Theorem (MVT) and its consequences, derive the leading-order truncation error for the following finite difference formulas that approximate the derivative $f'(x)$:\n- Forward difference (one-sided): $\\displaystyle D^{+}_h f(x) = \\frac{f(x+h) - f(x)}{h}$.\n- Central difference (two-sided): $\\displaystyle D^{0}_h f(x) = \\frac{f(x+h) - f(x-h)}{2h}$.\n\nYour derivation must start from the foundational base of the Mean Value Theorem (MVT) and differentiability, without invoking unproven shortcut formulas. Use assumptions that are standard and realistic:\n- For the forward difference, assume $f$ is twice continuously differentiable on an interval containing $[x, x+h]$.\n- For the central difference, assume $f$ is three times continuously differentiable on an interval containing $[x-h, x+h]$.\n\nExplain how the MVT implies the existence of intermediate points giving the leading error behavior and conclude the expected order of accuracy as a function of $h$.\n\nTask B (numerical verification): Implement a program to compute approximate derivatives using the specified finite difference formulas, evaluate absolute errors against the exact derivative $f'(x)$, and estimate the observed order $p$ by fitting a straight line to the data $\\left(\\log h_i, \\log \\text{error}_i\\right)$ for a sequence of step sizes $\\{h_i\\}$ using least squares. The slope of this line is the measured order $p$. Use natural logarithms. If any error is numerically zero for a given $h_i$, exclude that $h_i$ from the fit.\n\nAngle specification: For any trigonometric function, angles must be in radians.\n\nTest Suite:\n- Case $1$ (happy path, one-sided boundary formula): $f(x) = e^x$, $x_0 = 0.2$, scheme $D^{+}_h$, $h$ values $\\{0.1, 0.05, 0.025, 0.0125, 0.00625\\}$. Report the measured order $p$ (expected approximately $1$).\n- Case $2$ (happy path, two-sided interior formula): $f(x) = \\sin(x)$, $x_0 = 1.0$ (radians), scheme $D^{0}_h$, $h$ values $\\{0.1, 0.05, 0.025, 0.0125, 0.00625\\}$. Report the measured order $p$ (expected approximately $2$).\n- Case $3$ (edge case illustrating floating-point round-off domination): $f(x) = \\cos(x)$, $x_0 = 1.0$ (radians), scheme $D^{0}_h$, $h$ values $\\{10^{-8}, 5\\times 10^{-9}, 2.5\\times 10^{-9}, 1.25\\times 10^{-9}\\}$. Report the measured order $p$. In this regime, subtractive cancellation and finite precision with machine epsilon $\\epsilon \\approx 2.22\\times 10^{-16}$ can dominate, so $p$ may deviate significantly from the theoretical truncation order.\n\nYour program must compute the measured order $p$ for each case and produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, with each $p$ rounded to three decimal places (for example, `[1.001,1.998,-0.012]`). No additional text should be printed.\n\nAll computations are dimensionless; no physical units are involved. Angles are in radians. The only libraries permitted are the Python standard library and NumPy.", "solution": "The task is to derive the truncation error for the forward and central finite difference formulas using the Mean Value Theorem (MVT) for derivatives and to numerically verify the order of accuracy. The MVT states that for a function $f$ that is differentiable on an interval $(a,b)$ and continuous on $[a,b]$, there exists a point $\\xi \\in (a,b)$ such that $f(b) - f(a) = (b-a) f'(\\xi)$. A direct consequence of this theorem, obtained through repeated application, is Taylor's theorem with the Lagrange form of the remainder. We will use this consequence to perform the derivations.\n\n### Task A: Derivation of Truncation Errors\n\nThe truncation error $E(x,h)$ for a finite difference approximation $D_h f(x)$ of $f'(x)$ is defined as the difference between the exact derivative and its approximation: $E(x,h) = f'(x) - D_h f(x)$. The order of accuracy $p$ is the exponent in the leading term of the error, such that $|E(x,h)| = O(h^p)$.\n\n#### 1. Forward Difference Formula\n\nThe forward difference formula is given by $D^{+}_h f(x) = \\frac{f(x+h) - f(x)}{h}$. We assume the function $f$ is twice continuously differentiable, denoted $f \\in C^2$, on an interval containing $[x, x+h]$.\n\nBy Taylor's theorem with Lagrange remainder, we can expand $f(x+h)$ around the point $x$. For a function in $C^2$, this gives:\n$$\nf(x+h) = f(x) + f'(x)h + \\frac{f''(\\xi_1)}{2!}h^2\n$$\nfor some intermediate point $\\xi_1 \\in (x, x+h)$. This formulation is a direct result of applying the MVT (specifically, Rolle's Theorem, a special case of MVT) to an auxiliary function constructed to analyze the remainder term of the Taylor polynomial.\n\nRearranging the Taylor expansion to isolate the terms in the forward difference formula, we get:\n$$\nf(x+h) - f(x) = f'(x)h + \\frac{f''(\\xi_1)}{2}h^2\n$$\nDividing by $h$ (where $h \\neq 0$):\n$$\n\\frac{f(x+h) - f(x)}{h} = f'(x) + \\frac{f''(\\xi_1)}{2}h\n$$\nThe term on the left is the forward difference approximation, $D^{+}_h f(x)$. So, we have:\n$$\nD^{+}_h f(x) = f'(x) + \\frac{f''(\\xi_1)}{2}h\n$$\nThe truncation error is therefore:\n$$\nE(x,h) = f'(x) - D^{+}_h f(x) = - \\frac{f''(\\xi_1)}{2}h\n$$\nThe magnitude of the error is $|E(x,h)| = \\left| \\frac{f''(\\xi_1)}{2} \\right| |h|$. Since $f \\in C^2$, the second derivative $f''$ is continuous and bounded on $[x, x+h]$. As $h \\to 0$, $\\xi_1 \\to x$, and $f''(\\xi_1) \\to f''(x)$. Provided $f''(x) \\neq 0$, the error is directly proportional to $h$. Thus, the forward difference formula is said to have a leading-order error of $O(h)$, and its order of accuracy is $p=1$.\n\n#### 2. Central Difference Formula\n\nThe central difference formula is given by $D^{0}_h f(x) = \\frac{f(x+h) - f(x-h)}{2h}$. We assume the function $f$ is three times continuously differentiable, $f \\in C^3$, on an interval containing $[x-h, x+h]$.\n\nWe again use Taylor's theorem with Lagrange remainder, this time expanding to a higher order. We write expansions for both $f(x+h)$ and $f(x-h)$ around the point $x$:\n$$\nf(x+h) = f(x) + f'(x)h + \\frac{f''(x)}{2!}h^2 + \\frac{f'''(\\xi_1)}{3!}h^3 \\quad \\text{for some } \\xi_1 \\in (x, x+h)\n$$\n$$\nf(x-h) = f(x) + f'(x)(-h) + \\frac{f''(x)}{2!}(-h)^2 + \\frac{f'''(\\xi_2)}{3!}(-h)^3 \\quad \\text{for some } \\xi_2 \\in (x-h, x)\n$$\nSimplifying the second expansion:\n$$\nf(x-h) = f(x) - f'(x)h + \\frac{f''(x)}{2}h^2 - \\frac{f'''(\\xi_2)}{6}h^3\n$$\nTo form the central difference, we subtract the second expansion from the first:\n$$\nf(x+h) - f(x-h) = (f(x)-f(x)) + (f'(x)h - (-f'(x)h)) + \\left(\\frac{f''(x)}{2}h^2 - \\frac{f''(x)}{2}h^2\\right) + \\left(\\frac{f'''(\\xi_1)}{6}h^3 - \\left(-\\frac{f'''(\\xi_2)}{6}h^3\\right)\\right)\n$$\nThe terms involving $f(x)$ and $f''(x)$ cancel out, leaving:\n$$\nf(x+h) - f(x-h) = 2f'(x)h + \\frac{h^3}{6} (f'''(\\xi_1) + f'''(\\xi_2))\n$$\nDividing by $2h$ (where $h \\neq 0$):\n$$\n\\frac{f(x+h) - f(x-h)}{2h} = f'(x) + \\frac{h^2}{12} (f'''(\\xi_1) + f'''(\\xi_2))\n$$\nThe left side is the central difference approximation, $D^{0}_h f(x)$. The truncation error is:\n$$\nE(x,h) = f'(x) - D^{0}_h f(x) = -\\frac{h^2}{12} (f'''(\\xi_1) + f'''(\\xi_2))\n$$\nSince $f \\in C^3$, the third derivative $f'''$ is continuous on the interval $[x-h, x+h]$, which contains both $\\xi_1$ and $\\xi_2$. By the Intermediate Value Theorem, there exists a point $\\xi \\in (\\xi_2, \\xi_1) \\subset (x-h, x+h)$ such that $f'''(\\xi) = \\frac{f'''(\\xi_1) + f'''(\\xi_2)}{2}$. We can thus simplify the error expression:\n$$\nE(x,h) = -\\frac{h^2}{6} f'''(\\xi)\n$$\nThe magnitude of the error is $|E(x,h)| = \\left| \\frac{f'''(\\xi)}{6} \\right| h^2$. As $h \\to 0$, $\\xi \\to x$, and $f'''(\\xi) \\to f'''(x)$. Provided $f'''(x) \\neq 0$, the error is proportional to $h^2$. Thus, the central difference formula has a leading-order error of $O(h^2)$, and its order of accuracy is $p=2$.\n\n### Task B: Numerical Verification\n\nThe theoretical relationship between the absolute error $|E|$ and the step size $h$ for a method of order $p$ is $|E| \\approx C h^p$ for some constant $C$ and sufficiently small $h$. Taking the natural logarithm of both sides gives a linear relationship:\n$$\n\\ln(|E|) \\approx \\ln(C) + p \\ln(h)\n$$\nThis equation is of the form $y = c + mx$, where $y = \\ln(|E|)$, $x = \\ln(h)$, the slope $m$ is the order of accuracy $p$, and the intercept $c$ is $\\ln(C)$. By computing the error for a sequence of decreasing step sizes $\\{h_i\\}$, we can generate a set of data points $(\\ln(h_i), \\ln(|E_i|))$. Applying a linear least-squares regression to these points allows for a numerical estimation of the slope $p$, which is the observed order of accuracy. The following program implements this procedure for the specified test cases. For extremely small values of $h$, this relationship breaks down as floating-point round-off error, which scales as $O(h^{-1})$, becomes dominant over the truncation error.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the observed order of accuracy for finite difference schemes.\n    \"\"\"\n    \n    # Define test cases as a list of dictionaries.\n    test_cases = [\n        {\n            \"f\": lambda x: np.exp(x),\n            \"f_prime\": lambda x: np.exp(x),\n            \"x0\": 0.2,\n            \"scheme\": \"D+\",\n            \"h_values\": np.array([0.1, 0.05, 0.025, 0.0125, 0.00625])\n        },\n        {\n            \"f\": lambda x: np.sin(x),\n            \"f_prime\": lambda x: np.cos(x),\n            \"x0\": 1.0,\n            \"scheme\": \"D0\",\n            \"h_values\": np.array([0.1, 0.05, 0.025, 0.0125, 0.00625])\n        },\n        {\n            \"f\": lambda x: np.cos(x),\n            \"f_prime\": lambda x: -np.sin(x),\n            \"x0\": 1.0,\n            \"scheme\": \"D0\",\n            \"h_values\": np.array([1e-8, 5e-9, 2.5e-9, 1.25e-9])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        f = case[\"f\"]\n        f_prime = case[\"f_prime\"]\n        x0 = case[\"x0\"]\n        scheme = case[\"scheme\"]\n        h_values = case[\"h_values\"]\n        \n        log_h = []\n        log_error = []\n        \n        exact_derivative = f_prime(x0)\n        \n        for h in h_values:\n            if scheme == \"D+\":\n                # Forward difference formula\n                approx_derivative = (f(x0 + h) - f(x0)) / h\n            elif scheme == \"D0\":\n                # Central difference formula\n                approx_derivative = (f(x0 + h) - f(x0 - h)) / (2 * h)\n            else:\n                # Should not happen with given test cases\n                raise ValueError(\"Unknown scheme\")\n            \n            # Calculate absolute error\n            error = np.abs(exact_derivative - approx_derivative)\n            \n            # Exclude data points where error is numerically zero\n            if error > 0:\n                log_h.append(np.log(h))\n                log_error.append(np.log(error))\n\n        # Perform linear least-squares fit to find the order of accuracy (the slope)\n        if len(log_h) > 1:\n            log_h_np = np.array(log_h)\n            log_error_np = np.array(log_error)\n            \n            # We are fitting log_error = p * log_h + log_C\n            # This is a linear system Ax = b, where x = [p, log_C]^T\n            A = np.vstack([log_h_np, np.ones(len(log_h_np))]).T\n            p, _ = np.linalg.lstsq(A, log_error_np, rcond=None)[0]\n            results.append(p)\n        else:\n            # Handle cases with insufficient data for a fit\n            results.append(np.nan)\n\n    # Format output according to the problem specification\n    # e.g., [1.001,1.998,-0.012]\n    output_str = f\"[{','.join([f'{p:.3f}' for p in results])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3251126"}]}