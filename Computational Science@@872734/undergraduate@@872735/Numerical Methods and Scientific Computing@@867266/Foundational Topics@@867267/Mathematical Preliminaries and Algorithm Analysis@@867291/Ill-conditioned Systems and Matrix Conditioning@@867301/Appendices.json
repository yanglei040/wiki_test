{"hands_on_practices": [{"introduction": "The condition number of a matrix can seem like an abstract quantity, but it has a powerful geometric meaning. This exercise invites you to explore this connection by constructing a highly ill-conditioned matrix and observing its effect on the unit circle. By relating the singular values to the semi-axes of the resulting ellipse, you will develop a tangible intuition for how an ill-conditioned matrix dramatically stretches space in some directions while compressing it in others. [@problem_id:3240924]", "problem": "You are asked to construct and analyze a linear transformation that is severely ill-conditioned. Starting from the definitions of the matrix two-norm, singular values, and condition number, perform the following:\n\n- Construct an explicit real $2 \\times 2$ matrix $A$ such that the condition number with respect to the matrix two-norm satisfies $\\kappa_{2}(A) = 10^{8}$ and $\\det(A) = 1$.\n- Using only those definitions and basic properties of orthogonal transformations, establish that the image of the unit circle under $A$ is an ellipse. Identify the semiaxes of this ellipse.\n- Define the eccentricity $e$ of the resulting ellipse using its geometric definition and derive an expression for $e$ in terms of the semiaxes.\n- Evaluate $e$ for your constructed matrix and provide the final value as a single exact analytic expression.\n\nYour final answer must be a single real-valued expression. No rounding is required and no units apply.", "solution": "The problem requires the construction and analysis of a severely ill-conditioned $2 \\times 2$ real matrix $A$. The analysis involves its geometric action on the unit circle and the calculation of the resulting ellipse's eccentricity.\n\nFirst, we must construct the matrix $A$ satisfying the given conditions. Let the singular values of the $2 \\times 2$ matrix $A$ be $\\sigma_1$ and $\\sigma_2$, with the convention $\\sigma_1 \\ge \\sigma_2 > 0$ for an invertible matrix.\n\nThe condition number with respect to the matrix two-norm, $\\kappa_2(A)$, is defined as the ratio of the largest to the smallest singular value:\n$$ \\kappa_2(A) = \\frac{\\sigma_1}{\\sigma_2} $$\nWe are given that $\\kappa_2(A) = 10^8$. Thus, we have the equation:\n$$ \\frac{\\sigma_1}{\\sigma_2} = 10^8 $$\nThe determinant of $A$, $\\det(A)$, is related to the singular values by $|\\det(A)| = \\sigma_1 \\sigma_2$. We are given $\\det(A) = 1$, which implies $\\sigma_1 \\sigma_2 = 1$.\n\nWe now have a system of two equations for the two unknown singular values $\\sigma_1$ and $\\sigma_2$:\n1. $\\sigma_1 = 10^8 \\sigma_2$\n2. $\\sigma_1 \\sigma_2 = 1$\n\nSubstituting the first equation into the second gives:\n$$ (10^8 \\sigma_2) \\sigma_2 = 1 \\implies \\sigma_2^2 = 10^{-8} $$\nSince singular values are non-negative, we take the positive square root:\n$$ \\sigma_2 = \\sqrt{10^{-8}} = 10^{-4} $$\nNow, we find $\\sigma_1$ using the first equation:\n$$ \\sigma_1 = 10^8 \\sigma_2 = 10^8 \\cdot 10^{-4} = 10^4 $$\nThe required singular values are $\\sigma_1 = 10^4$ and $\\sigma_2 = 10^{-4}$.\n\nTo construct an explicit matrix $A$, we can use the singular value decomposition (SVD), which states that any real matrix $A$ can be written as $A = U\\Sigma V^T$, where $U$ and $V$ are orthogonal matrices and $\\Sigma$ is a diagonal matrix containing the singular values. For our $2 \\times 2$ case:\n$$ \\Sigma = \\begin{pmatrix} \\sigma_1 & 0 \\\\ 0 & \\sigma_2 \\end{pmatrix} = \\begin{pmatrix} 10^4 & 0 \\\\ 0 & 10^{-4} \\end{pmatrix} $$\nThe simplest construction for $A$ is to choose $U$ and $V$ to be the identity matrix, $I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$, which is an orthogonal matrix. This gives:\n$$ A = I \\Sigma I^T = \\Sigma = \\begin{pmatrix} 10^4 & 0 \\\\ 0 & 10^{-4} \\end{pmatrix} $$\nThis matrix is real, $2 \\times 2$, and satisfies $\\kappa_2(A) = \\frac{10^4}{10^{-4}} = 10^8$ and $\\det(A) = (10^4)(10^{-4}) = 1$.\n\nNext, we establish that the image of the unit circle under $A$ is an ellipse. The unit circle is the set of vectors $\\mathbf{x} \\in \\mathbb{R}^2$ such that $\\|\\mathbf{x}\\|_2 = 1$. The image is the set of vectors $\\mathbf{y} = A\\mathbf{x}$. Using the SVD, $\\mathbf{y} = U\\Sigma V^T \\mathbf{x}$.\nLet $\\mathbf{z} = V^T \\mathbf{x}$. Since $V^T$ is an orthogonal transformation, it preserves the Euclidean norm, so $\\|\\mathbf{z}\\|_2 = \\|V^T \\mathbf{x}\\|_2 = \\|\\mathbf{x}\\|_2 = 1$. As $\\mathbf{x}$ traces the unit circle, so does $\\mathbf{z}$.\nLet $\\mathbf{z}$ be represented parametrically as $\\mathbf{z} = \\begin{pmatrix} \\cos(\\theta) \\\\ \\sin(\\theta) \\end{pmatrix}$ for $\\theta \\in [0, 2\\pi)$.\nThe vector $\\mathbf{w} = \\Sigma \\mathbf{z}$ is then:\n$$ \\mathbf{w} = \\begin{pmatrix} \\sigma_1 & 0 \\\\ 0 & \\sigma_2 \\end{pmatrix} \\begin{pmatrix} \\cos(\\theta) \\\\ \\sin(\\theta) \\end{pmatrix} = \\begin{pmatrix} \\sigma_1\\cos(\\theta) \\\\ \\sigma_2\\sin(\\theta) \\end{pmatrix} $$\nThe components of $\\mathbf{w}$, $w_1 = \\sigma_1\\cos(\\theta)$ and $w_2 = \\sigma_2\\sin(\\theta)$, satisfy the equation of an ellipse:\n$$ \\left(\\frac{w_1}{\\sigma_1}\\right)^2 + \\left(\\frac{w_2}{\\sigma_2}\\right)^2 = \\cos^2(\\theta) + \\sin^2(\\theta) = 1 $$\nThis ellipse is centered at the origin with semiaxes of lengths $\\sigma_1$ and $\\sigma_2$ aligned with the coordinate axes.\nThe final image vector is $\\mathbf{y} = U\\mathbf{w}$. The matrix $U$ represents an orthogonal transformation (a rotation or reflection), which maps the ellipse traced by $\\mathbf{w}$ to another ellipse. This transformation preserves the lengths of the semiaxes.\nTherefore, the image of the unit circle under $A$ is an ellipse whose semiaxes have lengths equal to the singular values of $A$. The semi-major axis is $a = \\sigma_1 = 10^4$ and the semi-minor axis is $b = \\sigma_2 = 10^{-4}$.\n\nNow we derive an expression for the eccentricity $e$. For an ellipse with semi-major axis $a$ and semi-minor axis $b$, the distance from the center to each focus, denoted $c$, is given by the relation $a^2 = b^2 + c^2$. This gives $c = \\sqrt{a^2 - b^2}$. The geometric definition of eccentricity $e$ is the ratio of the center-to-focus distance to the semi-major axis:\n$$ e = \\frac{c}{a} = \\frac{\\sqrt{a^2 - b^2}}{a} $$\nWe can rewrite this as:\n$$ e = \\sqrt{\\frac{a^2 - b^2}{a^2}} = \\sqrt{1 - \\left(\\frac{b}{a}\\right)^2} $$\n\nFinally, we evaluate $e$ for our specific ellipse. The semiaxes are $a = 10^4$ and $b = 10^{-4}$. Substituting these values into the expression for eccentricity:\n$$ e = \\sqrt{1 - \\left(\\frac{10^{-4}}{10^4}\\right)^2} $$\n$$ e = \\sqrt{1 - (10^{-8})^2} $$\n$$ e = \\sqrt{1 - 10^{-16}} $$\nThis is the exact analytic expression for the eccentricity of the ellipse.", "answer": "$$\\boxed{\\sqrt{1 - 10^{-16}}}$$", "id": "3240924"}, {"introduction": "Ill-conditioning is not just a theoretical concern; it has profound consequences for solving linear systems in the presence of noise or rounding errors. This practice problem guides you through a controlled experiment to witness this sensitivity firsthand. By constructing a specific perturbation to the right-hand side vector $b$ and using the singular value decomposition (SVD) to track its effect on the solution $x$, you will quantify the massive amplification of error that an ill-conditioned matrix can cause. [@problem_id:3240884]", "problem": "Consider a real matrix $A \\in \\mathbb{R}^{3 \\times 3}$ with singular value decomposition (SVD) $A = Q \\Sigma V^{\\top}$, where $Q$ and $V$ are orthogonal and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\sigma_{3})$ with known singular values $\\sigma_{1} = 6$, $\\sigma_{2} = 2$, and $\\sigma_{3} = 10^{-12}$. Let the left singular vectors be the columns of $Q$, denoted $u_{1}$, $u_{2}$, and $u_{3}$, respectively, with $u_{3}$ corresponding to the smallest singular value $\\sigma_{3}$. You will construct a right-hand side vector $b$ that is nearly orthogonal to $u_{3}$ and analyze the sensitivity of the solution $x = A^{\\dagger} b$, where $A^{\\dagger}$ denotes the Moore–Penrose pseudoinverse.\n\nDefine $b_{0} = u_{1} + u_{2}$ and $b = b_{0} + \\varepsilon u_{3}$ with $\\varepsilon = 10^{-9}$. Let $x_{0} = A^{\\dagger} b_{0}$ and $x = A^{\\dagger} b$. Using only foundational definitions (orthonormality of singular vectors, linearity, and the definition of the pseudoinverse via the SVD), derive the amplification factor of the solution with respect to the perturbation along $u_{3}$:\n$$\ng \\equiv \\frac{\\|x - x_{0}\\|_{2}}{\\|b - b_{0}\\|_{2}},\n$$\nwhere $\\|\\cdot\\|_{2}$ denotes the Euclidean norm. Provide the final value of $g$ as a pure number. No units are required. Do not use any shortcut formulas beyond the fundamental definitions. The final answer must be a single real number. No rounding is required.", "solution": "The problem asks for the amplification factor $g$, defined as the ratio of the norm of the change in the solution to the norm of the change in the right-hand side vector.\n$$\ng \\equiv \\frac{\\|x - x_{0}\\|_{2}}{\\|b - b_{0}\\|_{2}}\n$$\nThe variables are defined as follows:\n- Matrix $A \\in \\mathbb{R}^{3 \\times 3}$ with SVD $A = Q \\Sigma V^{\\top}$.\n- $Q = [u_1, u_2, u_3]$ and $V = [v_1, v_2, v_3]$ are orthogonal matrices, with $\\{u_i\\}$ and $\\{v_i\\}$ being the sets of left and right singular vectors, respectively.\n- $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\sigma_{3})$, with singular values $\\sigma_{1} = 6$, $\\sigma_{2} = 2$, and $\\sigma_{3} = 10^{-12}$.\n- $b_{0} = u_{1} + u_{2}$.\n- $b = b_{0} + \\varepsilon u_{3}$, with $\\varepsilon = 10^{-9}$.\n- $x_{0} = A^{\\dagger} b_{0}$ and $x = A^{\\dagger} b$, where $A^{\\dagger}$ is the Moore-Penrose pseudoinverse.\n\nFirst, we define the Moore-Penrose pseudoinverse $A^{\\dagger}$ using the SVD of $A$:\n$$\nA^{\\dagger} = V \\Sigma^{\\dagger} Q^{\\top}\n$$\nSince all given singular values $\\sigma_i$ are non-zero, the matrix $\\Sigma^{\\dagger}$ is the diagonal matrix of the reciprocals of the singular values:\n$$\n\\Sigma^{\\dagger} = \\operatorname{diag}(\\sigma_{1}^{-1}, \\sigma_{2}^{-1}, \\sigma_{3}^{-1})\n$$\n\nNext, we analyze the numerator of the expression for $g$, which is $\\|x - x_{0}\\|_{2}$.\nThe difference between the solutions $x$ and $x_0$ is:\n$$\nx - x_{0} = A^{\\dagger} b - A^{\\dagger} b_{0}\n$$\nBy the linearity of the matrix-vector multiplication, we can write:\n$$\nx - x_{0} = A^{\\dagger} (b - b_{0})\n$$\nThe difference between the right-hand side vectors $b$ and $b_0$ is:\n$$\nb - b_{0} = (b_{0} + \\varepsilon u_{3}) - b_{0} = \\varepsilon u_{3}\n$$\nSubstituting this into the expression for the difference in solutions:\n$$\nx - x_{0} = A^{\\dagger} (\\varepsilon u_{3}) = \\varepsilon (A^{\\dagger} u_{3})\n$$\nNow, we compute the action of $A^{\\dagger}$ on the vector $u_3$ using its SVD-based definition:\n$$\nA^{\\dagger} u_{3} = (V \\Sigma^{\\dagger} Q^{\\top}) u_{3} = V \\Sigma^{\\dagger} (Q^{\\top} u_{3})\n$$\nThe matrix $Q$ is orthogonal, so its columns $\\{u_1, u_2, u_3\\}$ form an orthonormal basis. The matrix $Q^{\\top}$ has rows $u_1^{\\top}$, $u_2^{\\top}$, and $u_3^{\\top}$. The product $Q^{\\top} u_{3}$ is a column vector whose $i$-th component is $u_{i}^{\\top} u_{3}$. Due to orthonormality, $u_{i}^{\\top} u_{j} = \\delta_{ij}$ (the Kronecker delta). Therefore:\n$$\nQ^{\\top} u_{3} = \\begin{pmatrix} u_{1}^{\\top} u_{3} \\\\ u_{2}^{\\top} u_{3} \\\\ u_{3}^{\\top} u_{3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nSubstituting this result back into the expression for $A^{\\dagger} u_{3}$:\n$$\nA^{\\dagger} u_{3} = V \\Sigma^{\\dagger} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = V \\begin{pmatrix} \\sigma_{1}^{-1} & 0 & 0 \\\\ 0 & \\sigma_{2}^{-1} & 0 \\\\ 0 & 0 & \\sigma_{3}^{-1} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = V \\begin{pmatrix} 0 \\\\ 0 \\\\ \\sigma_{3}^{-1} \\end{pmatrix}\n$$\nSince $V$ is the matrix whose columns are the right singular vectors, $V = [v_1, v_2, v_3]$, the final product is:\n$$\nA^{\\dagger} u_{3} = [v_1, v_2, v_3] \\begin{pmatrix} 0 \\\\ 0 \\\\ \\sigma_{3}^{-1} \\end{pmatrix} = 0 \\cdot v_{1} + 0 \\cdot v_{2} + \\sigma_{3}^{-1} \\cdot v_{3} = \\sigma_{3}^{-1} v_{3}\n$$\nNow we can write the final expression for the difference in solutions:\n$$\nx - x_{0} = \\varepsilon (\\sigma_{3}^{-1} v_{3})\n$$\nWe compute the Euclidean norm of this vector for the numerator of $g$:\n$$\n\\|x - x_{0}\\|_{2} = \\|\\varepsilon \\sigma_{3}^{-1} v_{3}\\|_{2} = |\\varepsilon \\sigma_{3}^{-1}| \\|v_{3}\\|_{2}\n$$\nThe vector $v_3$ is a column of the orthogonal matrix $V$, so it is a unit vector, meaning $\\|v_{3}\\|_{2} = 1$. Given that $\\varepsilon = 10^{-9} > 0$ and $\\sigma_{3} = 10^{-12} > 0$, the absolute value can be dropped:\n$$\n\\|x - x_{0}\\|_{2} = \\varepsilon \\sigma_{3}^{-1}\n$$\nNow we analyze the denominator of $g$, which is $\\|b - b_{0}\\|_{2}$.\nWe already found that $b - b_{0} = \\varepsilon u_{3}$. The norm is:\n$$\n\\|b - b_{0}\\|_{2} = \\|\\varepsilon u_{3}\\|_{2} = |\\varepsilon| \\|u_{3}\\|_{2}\n$$\nSimilarly, $u_3$ is a column of the orthogonal matrix $Q$, so it is a unit vector, $\\|u_{3}\\|_{2} = 1$. With $\\varepsilon > 0$, we have:\n$$\n\\|b - b_{0}\\|_{2} = \\varepsilon\n$$\nFinally, we compute the amplification factor $g$ by taking the ratio of the two norms:\n$$\ng = \\frac{\\|x - x_{0}\\|_{2}}{\\|b - b_{0}\\|_{2}} = \\frac{\\varepsilon \\sigma_{3}^{-1}}{\\varepsilon} = \\sigma_{3}^{-1}\n$$\nSubstituting the given value for $\\sigma_{3}$:\n$$\ng = (10^{-12})^{-1} = 10^{12}\n$$\nThe result shows that a small perturbation in the direction of the left singular vector corresponding to the smallest singular value is amplified in the solution by a factor equal to the reciprocal of that singular value. This demonstrates the characteristic sensitivity of ill-conditioned systems.", "answer": "$$\\boxed{10^{12}}$$", "id": "3240884"}, {"introduction": "In practical applications like curve fitting, the choice of basis functions is critical for numerical stability. This hands-on coding exercise challenges you to diagnose and then cure an ill-conditioned least squares problem. You will first see how a naive choice of basis functions leads to a design matrix with a very high condition number, and then you will implement a basis redesign using orthogonal polynomials to dramatically improve the conditioning and stability of the solution. [@problem_id:3240835]", "problem": "Consider the ordinary least squares curve fitting of a scalar response $y$ to a set of basis functions $\\{\\phi_j(x)\\}_{j=0}^{m-1}$ evaluated on samples $\\{x_i\\}_{i=0}^{n-1}$. Let the design matrix $A \\in \\mathbb{R}^{n \\times m}$ have entries $A_{ij} = \\phi_j(x_i)$, and let the least squares normal equations be $A^\\top A \\beta = A^\\top y$ with solution coefficients $\\beta \\in \\mathbb{R}^m$ when the columns of $A$ are linearly independent. A matrix with highly correlated columns is susceptible to numerical instability during least squares, which can be diagnosed via the matrix conditioning. Starting from the core definition of the induced matrix norm in the vector $2$-norm and the Moore–Penrose pseudoinverse, devise and implement a numerically stable algorithm to evaluate the $2$-norm condition number $\\kappa_2(A)$ for a rectangular design matrix with full column rank, using a derivation grounded in the Singular Value Decomposition (SVD) without assuming any shortcut formulas. Then, redesign the basis to reduce column correlation by mapping the sample domain to $[-1,1]$ and using Legendre polynomials of matching maximal degree as the new basis, and evaluate the impact on $\\kappa_2$.\n\nYour program must construct the following test suite, compute the improvement factor for each case defined as the ratio $r = \\kappa_2(A_{\\text{orig}})/\\kappa_2(A_{\\text{redesign}})$, and output the three improvement factors as a single list on one line.\n\nTest suite:\n- Case $1$ (boundary, near-collinearity): $n = 50$, samples $x_i = 1 + \\left(\\frac{i}{n-1} - \\frac{1}{2}\\right)\\cdot 2\\times 10^{-3}$ for $i = 0,1,\\dots,n-1$. Original basis functions $\\phi_0(x) = 1$, $\\phi_1(x) = x$, $\\phi_2(x) = x + \\varepsilon$ with $\\varepsilon = 10^{-6}$. Redesigned basis: map $x$ affinely to $t \\in [-1,1]$ using the sample minimum and maximum, and use Legendre polynomials $P_k(t)$ for $k = 0,1,2$.\n- Case $2$ (happy path, moderately correlated monomials): $n = 100$, samples $x_i = \\frac{i}{n-1}$ for $i = 0,1,\\dots,n-1$. Original basis functions $\\phi_k(x) = x^k$ for $k = 0,1,\\dots,6$. Redesigned basis: map $x$ to $t = 2x - 1$ and use Legendre polynomials $P_k(t)$ for $k = 0,1,\\dots,6$.\n- Case $3$ (edge case, already near-orthogonal): $n = 80$, samples $x_i = -1 + \\frac{2i}{n-1}$ for $i = 0,1,\\dots,n-1$. Original basis functions are Legendre polynomials $P_k(x)$ for $k = 0,1,\\dots,6$. Redesigned basis: map $x$ affinely to $t \\in [-1,1]$ using the sample minimum and maximum and use Legendre polynomials $P_k(t)$ for $k = 0,1,\\dots,6$.\n\nImplementation details required:\n- Construct each design matrix $A$ by evaluating the stated basis functions at the stated sample points, placing the values for $\\phi_j$ in column $j$.\n- Implement the computation of $\\kappa_2(A)$ through a derivation from the definition of the induced $2$-norm and the Moore–Penrose pseudoinverse, leading to an SVD-based algorithm. Do not assume any closed-form conditioning shortcuts in the problem statement; your solution must justify the algorithmic steps from first principles.\n- Implement the Legendre polynomials $P_k(t)$ for integer degree $k \\ge 0$ using their three-term recurrence on $[-1,1]$; do not call any external special function libraries.\n\nFinal output format:\n- Your program should produce a single line of output containing the three improvement factors as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3]$), where each $r_i$ is a decimal float corresponding to Case $i$ in the order listed above.", "solution": "The problem asks for a derivation and implementation of a method to compute the $2$-norm condition number, $\\kappa_2(A)$, of a rectangular design matrix $A$ and to evaluate the improvement in conditioning achieved by redesigning the basis functions. The derivation must be grounded in the singular value decomposition (SVD) and start from first principles.\n\n### Part 1: Derivation of the $2$-Norm Condition Number via SVD\n\nThe problem specifies that the derivation must start from the core definitions of the induced $2$-norm and the Moore-Penrose pseudoinverse.\n\n**1. The Induced $2$-Norm**\n\nFor a matrix $A \\in \\mathbb{R}^{n \\times m}$, the induced $2$-norm, denoted $\\|A\\|_2$, is defined as the maximum amplification of the Euclidean norm of a vector when multiplied by $A$:\n$$\n\\|A\\|_2 = \\max_{\\vec{v} \\in \\mathbb{R}^m, \\vec{v} \\neq \\vec{0}} \\frac{\\|A\\vec{v}\\|_2}{\\|\\vec{v}\\|_2}\n$$\nThe quantity being maximized, the Rayleigh quotient, can be squared:\n$$\n\\frac{\\|A\\vec{v}\\|_2^2}{\\|\\vec{v}\\|_2^2} = \\frac{(A\\vec{v})^\\top (A\\vec{v})}{\\vec{v}^\\top \\vec{v}} = \\frac{\\vec{v}^\\top A^\\top A \\vec{v}}{\\vec{v}^\\top \\vec{v}}\n$$\nThe matrix $A^\\top A$ is a symmetric positive semi-definite matrix of size $m \\times m$. The maximum value of this Rayleigh quotient is the largest eigenvalue of $A^\\top A$, denoted $\\lambda_{\\max}(A^\\top A)$. Therefore,\n$$\n\\|A\\|_2^2 = \\lambda_{\\max}(A^\\top A) \\implies \\|A\\|_2 = \\sqrt{\\lambda_{\\max}(A^\\top A)}\n$$\n\n**2. Singular Value Decomposition (SVD)**\n\nAny matrix $A \\in \\mathbb{R}^{n \\times m}$ has a Singular Value Decomposition (SVD) of the form:\n$$\nA = U \\Sigma V^\\top\n$$\nwhere $U \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix ($U^\\top U = I_n$), $V \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix ($V^\\top V = I_m$), and $\\Sigma \\in \\mathbb{R}^{n \\times m}$ is a rectangular diagonal matrix containing the singular values $\\sigma_i$ of $A$. The singular values are non-negative and are typically ordered such that $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$, where $r$ is the rank of $A$. For this problem, we are given that $A$ has full column rank, so $r=m$ and all $m$ singular values are positive.\n\nLet's relate the eigenvalues of $A^\\top A$ to the singular values of $A$.\n$$\nA^\\top A = (U \\Sigma V^\\top)^\\top (U \\Sigma V^\\top) = (V \\Sigma^\\top U^\\top) (U \\Sigma V^\\top)\n$$\nSince $U^\\top U = I_n$, this simplifies to:\n$$\nA^\\top A = V \\Sigma^\\top \\Sigma V^\\top\n$$\nThis is the eigendecomposition of $A^\\top A$. The columns of $V$ are the eigenvectors of $A^\\top A$, and the diagonal entries of the $m \\times m$ matrix $\\Sigma^\\top \\Sigma$ are the corresponding eigenvalues. The matrix $\\Sigma^\\top \\Sigma$ is diagonal with entries $\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_m^2$.\nThus, the eigenvalues of $A^\\top A$ are the squares of the singular values of $A$: $\\lambda_i(A^\\top A) = \\sigma_i^2$.\n\nFrom this, we find the $2$-norm of $A$ in terms of its singular values:\n$$\n\\|A\\|_2 = \\sqrt{\\lambda_{\\max}(A^\\top A)} = \\sqrt{\\sigma_1^2} = \\sigma_1 = \\sigma_{\\max}(A)\n$$\n\n**3. The Moore-Penrose Pseudoinverse**\n\nFor a rectangular matrix $A$, the condition number is defined in terms of its pseudoinverse, $A^+$. The Moore-Penrose pseudoinverse $A^+$ is the unique matrix that satisfies four specific criteria. For a matrix $A \\in \\mathbb{R}^{n \\times m}$ with $n \\ge m$ and full column rank (as specified in the problem), the pseudoinverse has a simple and explicit form:\n$$\nA^+ = (A^\\top A)^{-1} A^\\top\n$$\nThe SVD of the pseudoinverse $A^+$ can be derived from the SVD of $A$. It is given by $A^+ = V \\Sigma^+ U^\\top$, where $\\Sigma^+$ is the pseudoinverse of $\\Sigma$. For a tall matrix $A$ with full column rank, $\\Sigma$ is an $n \\times m$ matrix with its top $m \\times m$ block being a diagonal matrix $D = \\text{diag}(\\sigma_1, \\dots, \\sigma_m)$ and the rest zeros. Its pseudoinverse $\\Sigma^+$ is an $m \\times n$ matrix where the left $m \\times m$ block is $D^{-1} = \\text{diag}(1/\\sigma_1, \\dots, 1/\\sigma_m)$ and the rest zeros.\n\nThe singular values of $A^+$ are the reciprocals of the non-zero singular values of $A$. Thus, the singular values of $A^+$ are $1/\\sigma_1, 1/\\sigma_2, \\dots, 1/\\sigma_m$.\n\n**4. The Condition Number**\n\nThe $2$-norm condition number of $A$ is defined as:\n$$\n\\kappa_2(A) = \\|A\\|_2 \\|A^+\\|_2\n$$\nUsing the relationship between the $2$-norm and the largest singular value, we have:\n$$\n\\|A\\|_2 = \\sigma_{\\max}(A) = \\sigma_1\n$$\n$$\n\\|A^+\\|_2 = \\sigma_{\\max}(A^+) = \\frac{1}{\\sigma_{\\min}(A)} = \\frac{1}{\\sigma_m}\n$$\nHere, $\\sigma_{\\min}(A)$ refers to the smallest non-zero singular value of $A$. Since $A$ has full column rank, all its $m$ singular values are non-zero, so $\\sigma_{\\min}(A) = \\sigma_m > 0$.\n\nCombining these results yields the desired expression for the condition number:\n$$\n\\kappa_2(A) = \\sigma_{\\max}(A) \\cdot \\frac{1}{\\sigma_{\\min}(A)} = \\frac{\\sigma_1}{\\sigma_m}\n$$\nThis derivation provides a stable algorithm: compute the SVD of $A$, find the largest and smallest singular values, and take their ratio. This approach avoids forming the potentially more ill-conditioned matrix $A^\\top A$.\n\n### Part 2: Basis Redesign and Legendre Polynomials\n\nIll-conditioning in least squares problems often arises from high correlation between the columns of the design matrix $A$. A common cause is the use of monomial basis functions $\\{\\phi_k(x) = x^k\\}$ over an interval that is not centered at $0$. To mitigate this, one can use a basis of orthogonal polynomials.\n\n**1. Affine Transformation**\n\nTo use orthogonal polynomials defined on a canonical interval, such as $[-1, 1]$, the sample domain $[x_{\\min}, x_{\\max}]$ must be mapped to this interval. An affine transformation $t = f(x) = ax+b$ can be used. We require $f(x_{\\min}) = -1$ and $f(x_{\\max}) = 1$. Solving this system of two linear equations for $a$ and $b$ yields:\n$$\na = \\frac{2}{x_{\\max} - x_{\\min}} \\quad \\text{and} \\quad b = - \\frac{x_{\\max} + x_{\\min}}{x_{\\max} - x_{\\min}}\n$$\nThe transformation is thus:\n$$\nt_i = \\frac{2(x_i - x_{\\min})}{x_{\\max} - x_{\\min}} - 1\n$$\nFor Case $2$, $x \\in [0, 1]$, which gives $t = \\frac{2(x - 0)}{1 - 0} - 1 = 2x - 1$. For Case $3$, $x \\in [-1, 1]$, which gives $t = \\frac{2(x - (-1))}{1 - (-1)} - 1 = x$.\n\n**2. Legendre Polynomials**\n\nLegendre polynomials, $P_k(t)$, are a sequence of orthogonal polynomials on the interval $[-1, 1]$ with respect to the weight function $w(t)=1$. They are generated using the three-term recurrence relation (Bonnet's recurrence formula):\n$$\nP_0(t) = 1\n$$\n$$\nP_1(t) = t\n$$\n$$\n(k+1)P_{k+1}(t) = (2k+1)tP_k(t) - kP_{k-1}(t) \\quad \\text{for } k \\ge 1\n$$\nUsing these polynomials as the basis functions $\\phi_k(t) = P_k(t)$ for the transformed variable $t$ results in columns of the design matrix that are nearly orthogonal, thereby significantly reducing the condition number. The improvement is quantified by the ratio $r = \\kappa_2(A_{\\text{orig}}) / \\kappa_2(A_{\\text{redesign}})$.\n\n### Part 3: Execution Plan for Test Cases\n\nThe algorithm proceeds as follows for each case:\n1.  Generate the sample points $x_i$ for $i=0, \\dots, n-1$.\n2.  Construct the original design matrix $A_{\\text{orig}} \\in \\mathbb{R}^{n \\times m}$ by evaluating the specified original basis functions at each $x_i$.\n3.  Compute the singular values of $A_{\\text{orig}}$ using an SVD routine. Calculate $\\kappa_2(A_{\\text{orig}}) = \\sigma_{\\max}/\\sigma_{\\min}$.\n4.  Apply the affine transformation to map the $x_i$ points to $t_i \\in [-1, 1]$.\n5.  Construct the redesigned design matrix $A_{\\text{redesign}} \\in \\mathbb{R}^{n \\times m}$ by evaluating the Legendre polynomials $P_k(t_i)$ up to the required degree.\n6.  Compute the singular values of $A_{\\text{redesign}}$ and calculate $\\kappa_2(A_{\\text{redesign}})$.\n7.  Compute the improvement factor $r = \\kappa_2(A_{\\text{orig}}) / \\kappa_2(A_{\\text{redesign}})$.\nThe results for the three cases are then reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef legendre_poly_values(max_k: int, t_values: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes Legendre polynomial values P_k(t) for k=0 to max_k.\n\n    Args:\n        max_k: The maximum degree of the polynomial.\n        t_values: A 1D numpy array of points t to evaluate the polynomials at.\n\n    Returns:\n        A numpy array of shape (len(t_values), max_k + 1), where column j\n        contains the values of P_j(t).\n    \"\"\"\n    n_samples = len(t_values)\n    # The matrix p_matrix will store P_k(t) in column k.\n    p_matrix = np.zeros((n_samples, max_k + 1))\n\n    # Base case P_0(t) = 1\n    if max_k >= 0:\n        p_matrix[:, 0] = 1.0\n\n    # Base case P_1(t) = t\n    if max_k >= 1:\n        p_matrix[:, 1] = t_values\n\n    # Recurrence relation for k >= 1:\n    # (k+1)P_{k+1}(t) = (2k+1)t*P_k(t) - k*P_{k-1}(t)\n    for k in range(1, max_k):\n        # This calculates P_{k+1}(t) and stores it in column k+1.\n        pk_plus_1 = ((2 * k + 1) * t_values * p_matrix[:, k] - k * p_matrix[:, k - 1]) / (k + 1)\n        p_matrix[:, k + 1] = pk_plus_1\n        \n    return p_matrix\n\ndef compute_condition_number(A: np.ndarray) -> float:\n    \"\"\"\n    Computes the 2-norm condition number of a matrix A using SVD.\n    kappa_2(A) = sigma_max / sigma_min.\n    \"\"\"\n    try:\n        singular_values = np.linalg.svd(A, compute_uv=False)\n        # Assuming A has full column rank, the smallest singular value is non-zero.\n        # Handle potential floating point underflow for sigma_min.\n        if singular_values[-1] < np.finfo(float).eps:\n            return np.inf\n        return singular_values[0] / singular_values[-1]\n    except np.linalg.LinAlgError:\n        return np.inf\n\ndef solve():\n    \"\"\"\n    Main function to run test suite and compute improvement factors.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (boundary, near-collinearity)\n        {\n            \"n\": 50,\n            \"m\": 3,\n            \"x_gen\": lambda n: 1.0 + (np.arange(n, dtype=float) / (n - 1) - 0.5) * 2e-3,\n            \"orig_basis_builder\": lambda x, m: np.column_stack([np.ones_like(x), x, x + 1e-6]),\n            \"redesign_max_k\": 2\n        },\n        # Case 2 (happy path, moderately correlated monomials)\n        {\n            \"n\": 100,\n            \"m\": 7,\n            \"x_gen\": lambda n: np.arange(n, dtype=float) / (n - 1),\n            \"orig_basis_builder\": lambda x, m: np.vander(x, m, increasing=True),\n            \"redesign_max_k\": 6\n        },\n        # Case 3 (edge case, already near-orthogonal)\n        {\n            \"n\": 80,\n            \"m\": 7,\n            \"x_gen\": lambda n: -1.0 + 2.0 * np.arange(n, dtype=float) / (n - 1),\n            \"orig_basis_builder\": lambda x, m: legendre_poly_values(m - 1, x),\n            \"redesign_max_k\": 6\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        n = case[\"n\"]\n        m = case[\"m\"]\n        \n        # 1. Generate sample points\n        x_i = case[\"x_gen\"](n)\n\n        # 2. Construct and analyze original design matrix\n        A_orig = case[\"orig_basis_builder\"](x_i, m)\n        kappa_orig = compute_condition_number(A_orig)\n\n        # 3. Transform sample points for redesign\n        x_min, x_max = np.min(x_i), np.max(x_i)\n        # Affine map x to t in [-1, 1]\n        # Avoid division by zero if x_min == x_max\n        if x_max > x_min:\n            t_i = 2.0 * (x_i - x_min) / (x_max - x_min) - 1.0\n        else: # all points are the same\n            t_i = np.zeros_like(x_i)\n\n        # 4. Construct and analyze redesigned design matrix\n        max_k = case[\"redesign_max_k\"]\n        A_redesign = legendre_poly_values(max_k, t_i)\n        kappa_redesign = compute_condition_number(A_redesign)\n        \n        # 5. Compute improvement factor\n        if kappa_redesign > np.finfo(float).eps:\n            improvement_factor = kappa_orig / kappa_redesign\n        else:\n            # If redesigned kappa is near zero, improvement is effectively infinite.\n            improvement_factor = np.inf\n            \n        results.append(improvement_factor)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3240835"}]}