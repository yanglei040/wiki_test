{"hands_on_practices": [{"introduction": "The Intermediate Value Theorem's most direct application in numerical analysis is to confirm the existence of a solution within a given interval. By evaluating a continuous function at the endpoints of an interval, we can search for a change in sign, which guarantees a root lies between them. This exercise [@problem_id:2215829] provides practice in this fundamental \"bracketing\" technique, which is the essential first step in many root-finding algorithms.", "problem": "Consider the polynomial function $p(x) = x^5 - 5x + 3$. One of the fundamental tasks in numerical analysis is to locate the roots of such functions. For the polynomial $p(x)$, which one of the following intervals of length 1 is guaranteed to contain at least one real root?\n\nA. $[-1, 0]$\n\nB. $[0, 1]$\n\nC. $[-3, -2]$\n\nD. $[2, 3]$", "solution": "We want an interval of length 1 that is guaranteed to contain a real root of the continuous function $p(x)=x^{5}-5x+3$. By the Intermediate Value Theorem, if $p(a)$ and $p(b)$ have opposite signs for the endpoints $a$ and $b$ of an interval, then there exists at least one $c$ in $(a,b)$ such that $p(c)=0$.\n\nEvaluate $p(x)$ at the endpoints of each candidate interval:\n- For $[-1,0]$: \n$$p(-1)=(-1)^{5}-5(-1)+3=-1+5+3=7>0,\\quad p(0)=3>0.$$\nNo sign change, so not guaranteed.\n\n- For $[0,1]$:\n$$p(0)=3>0,\\quad p(1)=1^{5}-5\\cdot 1+3=1-5+3=-10.$$\nThere is a sign change, so by the Intermediate Value Theorem there is at least one root in $(0,1)$, hence in $[0,1]$.\n\n- For $[-3,-2]$:\n$$p(-3)=(-3)^{5}-5(-3)+3=-243+15+3=-2250,\\quad p(-2)=(-2)^{5}-5(-2)+3=-32+10+3=-190.$$\nNo sign change, so not guaranteed.\n\n- For $[2,3]$:\n$$p(2)=2^{5}-5\\cdot 2+3=32-10+3=25>0,\\quad p(3)=3^{5}-5\\cdot 3+3=243-15+3=231>0.$$\nNo sign change, so not guaranteed.\n\nTherefore, only the interval $[0,1]$ is guaranteed to contain at least one real root.", "answer": "$$\\boxed{B}$$", "id": "2215829"}, {"introduction": "While the IVT guarantees that *at least* one root exists in an interval with a sign change, it doesn't, by itself, tell us the exact number of roots. To gain a more complete picture, we can combine the IVT with concepts from differential calculus. This practice problem [@problem_id:558414] demonstrates how to use a function's derivative to identify intervals of monotonicity, a powerful method that allows us to precisely count the total number of real roots.", "problem": "Consider the polynomial equation:\n\n$$\nx^5 - 5x + 5 = 0.\n$$\n\nUsing the Intermediate Value Theorem and an analysis of the monotonicity of the polynomial, determine the number of distinct real roots of the equation. Justify your answer by identifying intervals where the Intermediate Value Theorem guarantees a root and by examining the derivative to determine intervals of increase and decrease.", "solution": "1. Define $f(x)=x^5-5x+5$. Then  \n   $$\n   f'(x)=5x^4-5=5(x^4-1).\n   $$\n2. Critical points from $f'(x)=0$ are $x^4=1$, i.e.\\ $x=\\pm1$.  \n3. Sign of $f'$:\n   - For $x-1$: $x^41\\implies f'0$ (increasing).  \n   - For $-1x1$: $x^41\\implies f'0$ (decreasing).  \n   - For $x1$: $x^41\\implies f'0$ (increasing).  \n4. End behavior:  \n   $$\n   \\lim_{x\\to-\\infty}f(x)=-\\infty,\\qquad \\lim_{x\\to\\infty}f(x)=\\infty.\n   $$\n5. Values at critical points:  \n   $$\n   f(-1)=(-1)^5-5(-1)+5=-1+5+5=90,\\quad f(1)=1-5+5=10.\n   $$\n6. By the Intermediate Value Theorem:\n   - On $(-\\infty,-1)$: Since $\\lim_{x \\to -\\infty}f(x) = -\\infty$ and $f(-1)=9$, the function changes sign and is strictly monotonic, so there is exactly one root.  \n   - On $(-1,1)$: Since $f(1)=1$ is the minimum value on this interval and is positive, there are no roots.\n   - On $(1,\\infty)$: Since $f(1)=1$ is the minimum value for this interval and the function is increasing, there are no roots.\n\nTherefore there is exactly one distinct real root.", "answer": "$$\\boxed{1}$$", "id": "558414"}, {"introduction": "The principles of the Intermediate Value Theorem extend far beyond textbook examples, forming the theoretical backbone of sophisticated numerical algorithms. This advanced exercise [@problem_id:3243050] illustrates how the IVT underpins line search methods in optimization, guaranteeing that a step length satisfying a crucial performance condition can be found. While this problem uses a specific hypothetical model for demonstration, the underlying principle—using the IVT to reliably bracket a valid parameter—is a cornerstone of modern scientific computing.", "problem": "Consider a continuously differentiable function $f:\\mathbb{R}^n\\to\\mathbb{R}$ and a fixed point $x\\in\\mathbb{R}^n$. For a search direction $p\\in\\mathbb{R}^n$ and a step length $\\alpha\\ge 0$, define the univariate function $\\phi(\\alpha)=f(x+\\alpha p)$. The sufficient decrease condition (Armijo condition) requires that for a chosen constant $c\\in(0,1)$ the inequality $f(x+\\alpha p)\\le f(x)+c\\,\\alpha\\,\\nabla f(x)\\cdot p$ holds. Consider the auxiliary function $s(\\alpha)=\\phi(\\alpha)-f(x)-c\\,\\alpha\\,\\nabla f(x)\\cdot p$, which is continuous in $\\alpha$ whenever $f$ is continuously differentiable. Assume the descent condition $\\nabla f(x)\\cdot p0$ and note that $s(0)=0$.\n\nYour tasks are the following:\n- Derive, from the fundamental facts of continuity and differentiability together with the Intermediate Value Theorem (IVT), that there exists $\\alpha0$ such that $s(\\alpha)\\le 0$.\n- Propose and implement a bracketed search procedure for $\\alpha$ that relies on the sign of $s(\\alpha)$:\n  1. Establish a lower bound $\\alpha_{\\mathrm{lo}}0$ such that $s(\\alpha_{\\mathrm{lo}})0$ by shrinking an initial step length $\\alpha_{\\mathrm{init}}0$ until a negative value of $s$ is found. This relies on continuity near $0$ and the descent condition.\n  2. Establish an upper bound $\\alpha_{\\mathrm{hi}}\\alpha_{\\mathrm{lo}}$ such that $s(\\alpha_{\\mathrm{hi}})0$ by expanding the step length (for example, by multiplying by a factor $\\gamma1$) until a positive value of $s$ is found. Argue that for a broad class of functions, including those in the provided test suite, $s(\\alpha)$ eventually becomes positive as $\\alpha$ grows, which ensures a sign change from negative to positive, and thus a bracket exists by the Intermediate Value Theorem.\n  3. Use bisection on $[\\alpha_{\\mathrm{lo}},\\alpha_{\\mathrm{hi}}]$ to locate an $\\alpha_\\star$ such that $s(\\alpha_\\star)\\le 0$ and the interval width is below a prescribed tolerance. Return $\\alpha_\\star$ as the selected step length.\n- Implement this method as a complete, runnable program that computes $\\alpha_\\star$ for each test case in the suite below. No physical units are involved; all quantities are unitless.\n\nDefinitions and constraints to use:\n- $\\phi(\\alpha)=f(x+\\alpha p)$.\n- $s(\\alpha)=\\phi(\\alpha)-f(x)-c\\,\\alpha\\,\\nabla f(x)\\cdot p$.\n- $\\nabla f(x)\\cdot p0$ (descent condition).\n- $c\\in(0,1)$.\n- $\\alpha_{\\mathrm{init}}0$ and expansion factor $\\gamma1$.\n\nTest suite:\n- Case $1$ (quadratic model):\n  - Dimension $n=2$.\n  - $Q=\\begin{bmatrix}3  0.5\\\\ 0.5  1.5\\end{bmatrix}$, $b=\\begin{bmatrix}-1\\\\ 2\\end{bmatrix}$.\n  - $f(x)=\\tfrac{1}{2}x^\\top Qx+b^\\top x$, $\\nabla f(x)=Qx+b$.\n  - $x=\\begin{bmatrix}1\\\\ -1\\end{bmatrix}$, $p=-\\nabla f(x)$.\n  - $c=0.2$, $\\alpha_{\\mathrm{init}}=1.0$, $\\gamma=2.0$, shrink factor $0.5$, maximum expansions $30$, maximum bisection iterations $60$, tolerance $10^{-12}$.\n- Case $2$ (exponential plus quadratic):\n  - Dimension $n=2$.\n  - $f(x)=e^{x_1}+(x_2-1)^2$, $\\nabla f(x)=\\begin{bmatrix}e^{x_1}\\\\ 2(x_2-1)\\end{bmatrix}$.\n  - $x=\\begin{bmatrix}0.5\\\\ 0\\end{bmatrix}$, $p=-\\nabla f(x)$.\n  - $c=0.1$, $\\alpha_{\\mathrm{init}}=1.0$, $\\gamma=2.0$, shrink factor $0.5$, maximum expansions $30$, maximum bisection iterations $60$, tolerance $10^{-12}$.\n- Case $3$ (logarithmic plus quartic):\n  - Dimension $n=2$.\n  - $f(x)=\\ln(1+x_1^2)+\\tfrac{1}{4}x_2^4$, $\\nabla f(x)=\\begin{bmatrix}\\dfrac{2x_1}{1+x_1^2}\\\\ x_2^3\\end{bmatrix}$.\n  - $x=\\begin{bmatrix}0.1\\\\ -0.5\\end{bmatrix}$, $p=-\\nabla f(x)$.\n  - $c=0.5$, $\\alpha_{\\mathrm{init}}=1.0$, $\\gamma=2.0$, shrink factor $0.5$, maximum expansions $30$, maximum bisection iterations $60$, tolerance $10^{-12}$.\n\nYour program must:\n- Implement the bracketed search described above and compute a step length $\\alpha_\\star$ that satisfies $s(\\alpha_\\star)\\le 0$ for each test case.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\\texttt{[result1,result2,result3]}$, where each $resulti$ is the computed float $\\alpha_\\star$ for case $i$ in the order presented.\n\nNo external input is permitted; all parameters are to be hard-coded from the test suite above.", "solution": "#### Theoretical Justification\nThe problem requires finding a step length $\\alpha$ that satisfies the Armijo condition, $f(x+\\alpha p)\\le f(x)+c\\,\\alpha\\,\\nabla f(x)\\cdot p$. This can be analyzed using the auxiliary function $s(\\alpha)=\\phi(\\alpha)-f(x)-c\\,\\alpha\\,\\nabla f(x)\\cdot p$, where $\\phi(\\alpha) = f(x+\\alpha p)$. Since $f$ is continuously differentiable, $s(\\alpha)$ is a continuous function.\n\nFirst, we establish that a valid step length $\\alpha > 0$ must exist. We note that $s(0) = f(x) - f(x) - 0 = 0$. The derivative of $s(\\alpha)$ at $\\alpha=0$ is given by $s'(0) = \\phi'(0) - c\\,\\nabla f(x)\\cdot p$. Using the chain rule, $\\phi'(\\alpha) = \\nabla f(x+\\alpha p) \\cdot p$, so $\\phi'(0) = \\nabla f(x)\\cdot p$. This gives:\n$$s'(0) = \\nabla f(x)\\cdot p - c\\,\\nabla f(x)\\cdot p = (1-c)\\nabla f(x)\\cdot p$$\nGiven that we are using a descent direction ($\\nabla f(x)\\cdot p  0$) and $c \\in (0,1)$ (so $1-c > 0$), we have $s'(0)  0$. Since $s(0)=0$ and the function is initially decreasing, there must exist a small interval $(0, \\alpha_{\\max})$ where $s(\\alpha)  0$. This guarantees the existence of a step length satisfying the Armijo condition.\n\n#### Algorithm Description\nThe implemented algorithm is a three-phase bracketed line search:\n1.  **Find Lower Bound:** Start with $\\alpha = \\alpha_{\\mathrm{init}}$. If $s(\\alpha) > 0$, repeatedly shrink $\\alpha$ by a `shrink_factor` until $s(\\alpha) \\le 0$. The theoretical justification above guarantees this will succeed for $\\alpha$ close enough to zero. This value becomes the lower bound, $\\alpha_{\\mathrm{lo}}$.\n2.  **Find Upper Bound:** Starting from $\\alpha_{\\mathrm{lo}}$, repeatedly expand the step length by multiplying with a factor $\\gamma > 1$ until a value $\\alpha_{\\mathrm{hi}}$ is found such that $s(\\alpha_{\\mathrm{hi}}) > 0$. This phase succeeds because for the given test functions, $f(x+\\alpha p)$ grows faster than the linear term $-c\\alpha\\nabla f(x)\\cdot p$, ensuring $s(\\alpha)$ eventually becomes positive. We now have a bracket $[\\alpha_{\\mathrm{lo}}, \\alpha_{\\mathrm{hi}}]$ where $s(\\alpha_{\\mathrm{lo}}) \\le 0$ and $s(\\alpha_{\\mathrm{hi}}) > 0$.\n3.  **Bisection:** By the Intermediate Value Theorem, since $s(\\alpha)$ is continuous and changes sign over $[\\alpha_{\\mathrm{lo}}, \\alpha_{\\mathrm{hi}}]$, there must be a root of $s(\\alpha)=0$ in this interval. We use a bisection method to narrow this bracket. In each step, we test the midpoint $\\alpha_{\\mathrm{mid}}$. If $s(\\alpha_{\\mathrm{mid}}) \\le 0$, the step is valid, so we set $\\alpha_{\\mathrm{lo}} = \\alpha_{\\mathrm{mid}}$ to search for a potentially larger valid step. If $s(\\alpha_{\\mathrm{mid}}) > 0$, the step is invalid, so we set $\\alpha_{\\mathrm{hi}} = \\alpha_{\\mathrm{mid}}$. This continues until the interval width is below the tolerance. The final step length $\\alpha_\\star$ is the last valid lower bound found.\n\n#### Implementation Details\nThe provided Python code implements this algorithm. For each test case, it:\n1. Defines the objective function `f` and its gradient `grad_f`.\n2. Sets the initial point `x` and calculates the steepest descent direction `p = -grad_f(x)`.\n3. Calls the `find_step_length` function, which executes the three-phase search described above with the specified parameters (`c`, `alpha_init`, `gamma`, etc.).\n4. The final, computed step length for each case is collected and printed in the required format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef find_step_length(f, grad_f, x, p, c, alpha_init, gamma, shrink_factor, max_expansions, max_bisections, tol):\n    \"\"\"\n    Implements a bracketed line search to find a step length alpha satisfying the Armijo condition.\n    \"\"\"\n    # Pre-calculate constant terms for the auxiliary function s(alpha)\n    f_x = f(x)\n    grad_f_p = np.dot(grad_f(x), p)\n\n    def s(alpha):\n        return f(x + alpha * p) - f_x - c * alpha * grad_f_p\n\n    # --- Phase 1: Shrink to find a lower bound alpha_lo ---\n    # Find alpha_lo > 0 such that s(alpha_lo) = 0\n    alpha_lo = alpha_init\n    # A safety break to prevent infinite loops in case of unforeseen issues\n    for _ in range(50): \n        if s(alpha_lo) = 0:\n            break\n        alpha_lo *= shrink_factor\n    else:\n        # This should not be reached if the theory holds and precision is sufficient\n        return np.nan # Indicate failure to find lower bound\n\n    # --- Phase 2: Expand to find an upper bound alpha_hi ---\n    # Find alpha_hi > alpha_lo such that s(alpha_hi) > 0\n    alpha_hi = alpha_lo\n    # If alpha_init already satisfied the condition, we start expanding from there.\n    # Otherwise, alpha_lo is a shrunken version, and we need to find a point where s > 0.\n    # If s(alpha_lo) is already positive (e.g. if alpha_init was tiny),\n    # this might be tricky, but the shrink phase guarantees s(alpha_lo) = 0.\n    # We expand from alpha_lo to find the other side of the bracket.\n    for _ in range(max_expansions):\n        if s(alpha_hi) > 0:\n            break\n        alpha_hi *= gamma\n    else:\n        # This indicates the function might not be bounded below in direction p,\n        # or the expansion was not large enough.\n        # For the given problems, this should not occur.\n        return np.nan # Indicate failure to find upper bound\n\n    # --- Phase 3: Bisection ---\n    # Refine the bracketing interval [alpha_lo, alpha_hi]\n    # At this point we have a bracket where s(alpha_lo) = 0 and s(alpha_hi) > 0\n    a = alpha_lo\n    b = alpha_hi\n\n    for _ in range(max_bisections):\n        if (b - a)  tol:\n            break\n        \n        alpha_mid = (a + b) / 2.0\n        \n        if s(alpha_mid) > 0:\n            b = alpha_mid  # Midpoint is too large, update upper bound\n        else:\n            a = alpha_mid  # Midpoint is acceptable, try for a larger step\n    \n    # The final alpha_star should satisfy s(alpha_star) = 0.\n    # By our logic, 'a' always stores an acceptable step.\n    return a\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (quadratic model)\n        {\n            \"f\": lambda x, Q, b: 0.5 * x.T @ Q @ x + b.T @ x,\n            \"grad_f\": lambda x, Q, b: Q @ x + b,\n            \"Q\": np.array([[3, 0.5], [0.5, 1.5]]),\n            \"b\": np.array([-1, 2]),\n            \"x\": np.array([1, -1]),\n            \"c\": 0.2,\n            \"alpha_init\": 1.0,\n        },\n        # Case 2 (exponential plus quadratic)\n        {\n            \"f\": lambda x: np.exp(x[0]) + (x[1] - 1)**2,\n            \"grad_f\": lambda x: np.array([np.exp(x[0]), 2 * (x[1] - 1)]),\n            \"x\": np.array([0.5, 0]),\n            \"c\": 0.1,\n            \"alpha_init\": 1.0,\n        },\n        # Case 3 (logarithmic plus quartic)\n        {\n            \"f\": lambda x: np.log(1 + x[0]**2) + 0.25 * x[1]**4,\n            \"grad_f\": lambda x: np.array([2 * x[0] / (1 + x[0]**2), x[1]**3]),\n            \"x\": np.array([0.1, -0.5]),\n            \"c\": 0.5,\n            \"alpha_init\": 1.0,\n        },\n    ]\n\n    # Shared algorithm parameters\n    params = {\n        \"gamma\": 2.0,\n        \"shrink_factor\": 0.5,\n        \"max_expansions\": 30,\n        \"max_bisections\": 60,\n        \"tol\": 1e-12,\n    }\n\n    results = []\n    \n    # Case 1\n    case1 = test_cases[0]\n    f1 = lambda x: case1[\"f\"](x, case1[\"Q\"], case1[\"b\"])\n    grad_f1 = lambda x: case1[\"grad_f\"](x, case1[\"Q\"], case1[\"b\"])\n    x1 = case1[\"x\"]\n    p1 = -grad_f1(x1)\n    alpha1 = find_step_length(f1, grad_f1, x1, p1, case1[\"c\"], case1[\"alpha_init\"], **params)\n    results.append(alpha1)\n\n    # Case 2\n    case2 = test_cases[1]\n    x2 = case2[\"x\"]\n    p2 = -case2[\"grad_f\"](x2)\n    alpha2 = find_step_length(case2[\"f\"], case2[\"grad_f\"], x2, p2, case2[\"c\"], case2[\"alpha_init\"], **params)\n    results.append(alpha2)\n\n    # Case 3\n    case3 = test_cases[2]\n    x3 = case3[\"x\"]\n    p3 = -case3[\"grad_f\"](x3)\n    alpha3 = find_step_length(case3[\"f\"], case3[\"grad_f\"], x3, p3, case3[\"c\"], case3[\"alpha_init\"], **params)\n    results.append(alpha3)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3243050"}]}