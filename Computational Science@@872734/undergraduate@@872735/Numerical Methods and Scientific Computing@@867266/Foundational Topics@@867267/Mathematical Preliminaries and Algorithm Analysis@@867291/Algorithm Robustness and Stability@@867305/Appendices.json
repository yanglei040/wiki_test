{"hands_on_practices": [{"introduction": "The Fibonacci sequence is a classic introductory example in programming, but it also offers a profound lesson in numerical stability. While an iterative implementation and a naive recursive one are mathematically identical in exact arithmetic, their computational structures are vastly different. This practice [@problem_id:3205171] lets you explore firsthand how the explosive growth in the number of operations in the recursive method leads to a catastrophic accumulation of rounding errors, demonstrating why algorithmic choice is critical for robust results.", "problem": "You are asked to study algorithm robustness and stability by comparing how rounding errors propagate in two implementations of the Fibonacci sequence under a controlled rounding model. Use the following fundamental base to frame your analysis and implementation.\n\n- Fibonacci numbers are defined by the recurrence $F_0 = 0$, $F_1 = 1$, and $F_n = F_{n-1} + F_{n-2}$ for $n \\ge 2$.\n- Adopt a standard floating-point rounding model for addition: each addition is rounded to a finite precision by an operator $\\mathrm{fl}_t(\\cdot)$ that rounds any real number to $t$ significant decimal digits using rounding-to-nearest with ties-to-even. In analysis form, one may write $\\mathrm{fl}_t(a+b) = (a+b)(1+\\delta)$ with $|\\delta| \\le u$, where $u$ (the unit roundoff) depends on $t$.\n\nYour task is to implement two algorithms that both use the same rounding operator $\\mathrm{fl}_t(\\cdot)$ after each addition:\n\n1) Iterative implementation:\n- Initialize $x_0 = \\mathrm{fl}_t(0)$, $x_1 = \\mathrm{fl}_t(1)$.\n- For $k = 2, 3, \\dots, n$, set $x_k = \\mathrm{fl}_t(x_{k-1} + x_{k-2})$.\n- Return $x_n$.\n\n2) Naive recursive implementation:\n- Define a function `fib_rec(n)` by\n  - If $n = 0$, return $\\mathrm{fl}_t(0)$.\n  - If $n = 1$, return $\\mathrm{fl}_t(1)$.\n  - Otherwise, return `fl_t(fib_rec(n-1) + fib_rec(n-2))`.\n\nFor a given pair $(n,t)$, compute:\n- The exact value $F_n$ as an integer using the recurrence without rounding.\n- The iteratively rounded value $X_n^{\\mathrm{iter}}$ and the recursively rounded value $X_n^{\\mathrm{rec}}$.\n- The relative errors (expressed as decimals, not percentages and without any unit):\n  - $r_{\\mathrm{iter}} = \\dfrac{|X_n^{\\mathrm{iter}} - F_n|}{|F_n|}$,\n  - $r_{\\mathrm{rec}} = \\dfrac{|X_n^{\\mathrm{rec}} - F_n|}{|F_n|}$,\n  - and their difference $s = r_{\\mathrm{rec}} - r_{\\mathrm{iter}}$.\n\nTest Suite:\nUse the following parameter pairs $(n,t)$:\n- Case A: $(n,t) = (1, 3)$,\n- Case B: $(n,t) = (12, 4)$,\n- Case C: $(n,t) = (20, 6)$,\n- Case D: $(n,t) = (24, 6)$,\n- Case E: $(n,t) = (24, 3)$.\n\nRequired final output format:\n- Your program must produce a single line of output containing a comma-separated flat list enclosed in square brackets.\n- The list must contain, in order, for each test case, the triple $[r_{\\mathrm{iter}}, r_{\\mathrm{rec}}, s]$ concatenated across all cases into one flat list:\n  - $[r_{\\mathrm{iter}}^{(A)}, r_{\\mathrm{rec}}^{(A)}, s^{(A)}, r_{\\mathrm{iter}}^{(B)}, r_{\\mathrm{rec}}^{(B)}, s^{(B)}, \\dots]$.\n- Express each number as a decimal using scientific or fixed notation with exactly $12$ significant digits.\n- There are no physical units in the answer.\n\nScientific realism and constraints:\n- You must implement $\\mathrm{fl}_t(\\cdot)$ exactly as rounding to $t$ significant decimal digits with rounding-to-nearest ties-to-even.\n- Angles are not involved; no angle units are required.\n- Ensure that the recursive implementation is the naive one described above (no memoization or dynamic programming), so that it recomputes subproblems and thus exhibits its own rounding error propagation characteristics.\n\nThe goal is to empirically compare the robustness and stability of the two methods under the same rounding model. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,...]\").", "solution": "The problem statement has been critically validated and is deemed valid. It is a well-posed problem in the field of numerical analysis, specifically concerning the study of algorithm stability and the propagation of rounding errors. The problem is scientifically grounded, internally consistent, and contains all necessary information to proceed with a solution.\n\nThe core task is to compare the numerical stability of two algorithms for computing the Fibonacci sequence under a specified rounding model. The stability of a numerical algorithm is determined by how it propagates input errors and errors introduced by floating-point arithmetic (rounding errors). A stable algorithm will not unduly magnify these errors.\n\nThe fundamental difference between the two proposed algorithms lies in the number of rounding operations performed.\n1.  The iterative algorithm computes $F_n$ with approximately $n-1$ additions, and thus $n-1$ rounding operations. The number of roundings grows linearly with $n$.\n2.  The naive recursive algorithm, which recomputes subproblems, performs a number of additions (and roundings) equal to $F_{n+1}-1$. This number grows exponentially with $n$, providing a classic example of an inefficient and numerically unstable approach for this problem. The massive number of rounding operations is expected to lead to a much larger accumulation of error compared to the iterative method.\n\nThe implementation will proceed in several steps: defining the rounding model, implementing the three required Fibonacci functions (exact, iterative, recursive), and then calculating the specified error metrics for each test case.\n\n**1. Rounding Model `fl_t(·)`**\n\nThe problem specifies a rounding operator $\\mathrm{fl}_t(\\cdot)$ that rounds a real number to $t$ significant decimal digits using the \"round-to-nearest, ties-to-even\" rule. The most robust way to implement this in Python is to use the `decimal` module from the standard library. This module is designed for decimal floating-point arithmetic and provides precise control over precision and rounding modes.\n\nThe function `fl_t(x, t)` will be implemented as follows:\n- Set the precision of the `decimal` context to $t$.\n- Set the rounding mode of the context to `ROUND_HALF_EVEN`, which corresponds to \"round-to-nearest, ties-to-even\".\n- Convert the input number $x$ (which is a standard binary float) to a `Decimal` object.\n- Apply the context's rounding rules to this `Decimal` object. The unary plus operator (`+`) on a `Decimal` object is a canonical way to do this.\n- Convert the resulting `Decimal` object back to a standard float for use in subsequent calculations.\n\n$$ \\mathrm{fl}_t(x) = \\text{float}(\\text{round}_{\\text{prec}=t, \\text{mode}=\\text{HALF\\_EVEN}}(\\text{Decimal}(x))) $$\n\n**2. Fibonacci Algorithm Implementations**\n\nThree functions are required to compute the Fibonacci numbers.\n\n- **Exact Value $F_n$**: A simple iterative algorithm using Python's arbitrary-precision integers will be used to compute the exact value $F_n$. This serves as the ground truth for error calculations.\n    - $a, b \\leftarrow 0, 1$\n    - For $k$ from $2$ to $n$: $a, b \\leftarrow b, a+b$\n    - Return $b$.\n\n- **Iterative Rounded Value $X_n^{\\mathrm{iter}}$**: This algorithm follows the same iterative structure as the exact one but applies the rounding operator $\\mathrm{fl}_t(\\cdot)$ after each addition.\n    - $x_0 \\leftarrow \\mathrm{fl}_t(0, t)$, $x_1 \\leftarrow \\mathrm{fl}_t(1, t)$\n    - For $k$ from $2$ to $n$:\n        - $s \\leftarrow x_{k-1} + x_{k-2}$\n        - $x_k \\leftarrow \\mathrm{fl}_t(s, t)$\n        - Update stored values: $x_{k-2} \\leftarrow x_{k-1}$, $x_{k-1} \\leftarrow x_k$\n    - Return $x_n$.\n\n- **Naive Recursive Rounded Value $X_n^{\\mathrm{rec}}$**: This algorithm is a direct translation of the mathematical recurrence, with rounding applied at each step. As per the problem statement, it must be \"naive,\" meaning it recomputes values for subproblems, thus exhibiting its characteristic error propagation.\n    - $\\text{fib\\_rec}(k, t)$:\n        - If $k=0$, return $\\mathrm{fl}_t(0, t)$.\n        - If $k=1$, return $\\mathrm{fl}_t(1, t)$.\n        - Otherwise, return $\\mathrm{fl}_t(\\text{fib\\_rec}(k-1, t) + \\text{fib\\_rec}(k-2, t), t)$.\n\n**3. Error Calculation**\n\nFor each test pair $(n, t)$, we compute the following quantities:\n- The exact value $F_n$.\n- The iterative rounded result $X_n^{\\mathrm{iter}}$.\n- The recursive rounded result $X_n^{\\mathrm{rec}}$.\n\nThe relative errors are then calculated. Since the test cases have $n \\ge 1$, the exact value $F_n$ is always non-zero, so division by zero is not a concern.\n- Iterative relative error: $r_{\\mathrm{iter}} = \\dfrac{|X_n^{\\mathrm{iter}} - F_n|}{|F_n|}$\n- Recursive relative error: $r_{\\mathrm{rec}} = \\dfrac{|X_n^{\\mathrm{rec}} - F_n|}{|F_n|}$\n- Difference in errors: $s = r_{\\mathrm{rec}} - r_{\\mathrm{iter}}$\n\nThese three values, for each test case, are formatted to $12$ significant digits using scientific notation and concatenated into a single list for the final output. The format string `\"{:.11e}\"` is used to achieve this, providing $1$ digit before the decimal point and $11$ after, for a total of $12$ significant digits.\n\n**4. Execution and Analysis**\n\nThe main program iterates through the provided test suite. For each $(n, t)$ pair, it invokes the three Fibonacci functions, computes the errors $r_{\\mathrm{iter}}$, $r_{\\mathrm{rec}}$, and $s$, formats the results, and appends them to a list. Finally, it joins the list elements with commas and prints them inside square brackets, adhering to the specified output format. The results are expected to show that $r_{\\mathrm{rec}}$ is significantly larger than $r_{\\mathrm{iter}}$, especially for larger $n$ or smaller $t$, demonstrating the superior numerical stability of the iterative algorithm.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext, ROUND_HALF_EVEN\nimport sys\n\n# Increase recursion limit for larger n in the naive recursive implementation.\nsys.setrecursionlimit(2000)\n\ndef solve():\n    \"\"\"\n    Solves the Fibonacci rounding error comparison problem.\n    \"\"\"\n\n    def fl_t(x: float, t: int) - float:\n        \"\"\"\n        Rounds a number x to t significant decimal digits using\n        rounding-to-nearest with ties-to-even.\n        \"\"\"\n        if x == 0.0:\n            return 0.0\n        \n        # Set the context for decimal arithmetic\n        getcontext().prec = t\n        getcontext().rounding = ROUND_HALF_EVEN\n        \n        # Convert to Decimal, apply rounding, and convert back to float\n        # The unary '+' applies the precision and rounding from the context.\n        rounded_decimal = +Decimal(x)\n        \n        return float(rounded_decimal)\n\n    def fib_exact(n: int) - int:\n        \"\"\"\n        Computes the exact n-th Fibonacci number using integer arithmetic.\n        \"\"\"\n        if n = 1:\n            return n\n        a, b = 0, 1\n        for _ in range(2, n + 1):\n            a, b = b, a + b\n        return b\n\n    def fib_iterative(n: int, t: int) - float:\n        \"\"\"\n        Computes the n-th Fibonacci number using an iterative approach\n        with rounding after each addition.\n        \"\"\"\n        if n == 0:\n            return fl_t(0.0, t)\n        if n == 1:\n            return fl_t(1.0, t)\n        \n        x0 = fl_t(0.0, t)\n        x1 = fl_t(1.0, t)\n        \n        for _ in range(2, n + 1):\n            s = x1 + x0\n            xk = fl_t(s, t)\n            x0, x1 = x1, xk\n            \n        return x1\n\n    def fib_recursive(n: int, t: int) - float:\n        \"\"\"\n        Computes the n-th Fibonacci number using a naive recursive approach\n        with rounding after each addition.\n        \"\"\"\n        if n == 0:\n            return fl_t(0.0, t)\n        if n == 1:\n            return fl_t(1.0, t)\n        \n        # Recursive calls are made, and the sum is rounded.\n        fib_nm1 = fib_recursive(n - 1, t)\n        fib_nm2 = fib_recursive(n - 2, t)\n        \n        return fl_t(fib_nm1 + fib_nm2, t)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1, 3),   # Case A\n        (12, 4),  # Case B\n        (20, 6),  # Case C\n        (24, 6),  # Case D\n        (24, 3),  # Case E\n    ]\n\n    results = []\n    for n, t in test_cases:\n        # Calculate the exact value.\n        fn_exact = fib_exact(n)\n        \n        # Calculate the value using the iterative algorithm with rounding.\n        xn_iter = fib_iterative(n, t)\n        \n        # Calculate the value using the naive recursive algorithm with rounding.\n        xn_rec = fib_recursive(n, t)\n        \n        # Calculate relative errors. Denominator fn_exact is  0 for all n = 1.\n        if fn_exact != 0:\n            r_iter = abs(xn_iter - fn_exact) / abs(fn_exact)\n            r_rec = abs(xn_rec - fn_exact) / abs(fn_exact)\n        else: # This case is not hit by the provided test suite.\n            r_iter = 0.0 if xn_iter == 0.0 else float('inf')\n            r_rec = 0.0 if xn_rec == 0.0 else float('inf')\n            \n        s = r_rec - r_iter\n        \n        # Format results to 12 significant digits in scientific notation.\n        # Format specifier \".11e\" means 11 digits after the decimal point,\n        # plus one before, totaling 12 significant digits.\n        results.append(f\"{r_iter:.11e}\")\n        results.append(f\"{r_rec:.11e}\")\n        results.append(f\"{s:.11e}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3205171"}, {"introduction": "Moving from simple recurrences to data analysis, even fundamental statistical measures like the arithmetic and geometric mean are not immune to numerical pitfalls. When dealing with very large datasets or numbers close to the limits of machine precision, naive implementations can fail spectacularly due to accumulated rounding errors or underflow. This exercise [@problem_id:3205191] challenges you to implement and compare these naive methods against robust alternatives, such as Kahan summation and logarithmic transformations, to build a practical toolkit for stable statistical computing.", "problem": "You will investigate the numerical robustness and stability of computing the arithmetic mean and the geometric mean for sets of positive real numbers whose magnitudes are very close to the machine epsilon of double-precision floating-point arithmetic. Use the standard rounding model for floating-point arithmetic as the foundation: for every elementary operation on real numbers $x$ and $y$, the computed floating-point result $\\mathrm{fl}(x \\,\\mathrm{op}\\, y)$ satisfies $\\mathrm{fl}(x \\,\\mathrm{op}\\, y) = (x \\,\\mathrm{op}\\, y)(1 + \\delta)$ with $|\\delta| \\le u$, where $u$ is the unit roundoff (half of machine epsilon). In double precision following the Institute of Electrical and Electronics Engineers (IEEE) $754$ standard, the machine epsilon is approximately $2^{-52}$ and the unit roundoff $u$ is approximately $2^{-53}$. \n\nThe arithmetic mean of positive numbers $x_1,\\dots,x_n$ is defined as $(x_1 + \\cdots + x_n)/n$. The geometric mean is defined as $(x_1 \\cdots x_n)^{1/n}$. You will assess two computational strategies for each mean: a straightforward approach and a numerically robust alternative justified from first principles (for example, by mitigating error accumulation or avoiding underflow via a transformation that reduces dynamic range). You will quantify robustness by computing the relative error of each strategy with respect to a high-accuracy baseline computed using a summation method that substantially reduces rounding error for arithmetic sums and a transformation that avoids underflow for products.\n\nUse the following definitions. If $a_{\\text{approx}}$ is an approximation to a positive reference value $a_{\\text{ref}}$, then the relative error is $|a_{\\text{approx}} - a_{\\text{ref}}| / a_{\\text{ref}}$. For the arithmetic mean baseline, compute the sum using a high-accuracy summation routine provided by the language standard library if available, then divide by $n$. For the geometric mean baseline, compute the sum of natural logarithms using a high-accuracy summation routine, divide by $n$, and then exponentiate.\n\nYour program must implement, for each test case:\n- A straightforward arithmetic mean and a numerically robust arithmetic mean, each compared to the arithmetic baseline via relative error.\n- A straightforward geometric mean and a numerically robust geometric mean, each compared to the geometric baseline via relative error.\n\nAll inputs are pure numbers without physical units. All angles, if any appear, must be in radians, although no angles are required here.\n\nTest suite. Let $\\varepsilon$ denote the machine epsilon of IEEE $754$ double precision. Construct the following four deterministic test cases of positive numbers:\n- Case $1$ (happy path, small $n$): $n = 5$, with $x_i = \\varepsilon\\,(1 + i \\cdot 10^{-12})$ for $i = 0,1,2,3,4$.\n- Case $2$ (product underflow stress): $n = 300$, with $x_i = \\varepsilon$ for $i = 1,2,\\dots,300$.\n- Case $3$ (large $n$, mild heterogeneity): $n = 20000$, with $x_i = \\varepsilon$ for odd $i$ and $x_i = 1.5\\,\\varepsilon$ for even $i$.\n- Case $4$ (very large $n$, tiny deterministic perturbations): $n = 100000$, with $x_i = \\varepsilon\\left(1 + \\left((i \\bmod 10) - 5\\right)\\cdot 10^{-16}\\right)$ for $i = 0,1,2,\\dots,99999$.\n\nAll $x_i$ are strictly positive in these definitions.\n\nFor each case, compute the following four quantities:\n- Relative error of the straightforward arithmetic mean versus its baseline.\n- Relative error of the robust arithmetic mean versus its baseline.\n- Relative error of the straightforward geometric mean versus its baseline.\n- Relative error of the robust geometric mean versus its baseline.\n\nFinal output format. Your program should produce a single line of output that is a comma-separated list enclosed in square brackets, containing one entry per test case, in order. Each entry must itself be a list of four floating-point numbers in the order specified above. For example, the output must have the form\n\"[ [a11,a12,a13,a14],[a21,a22,a23,a24],[a31,a32,a33,a34],[a41,a42,a43,a44] ]\"\nwith no additional text before or after this single line.", "solution": "The problem requires an investigation into the numerical robustness of computing the arithmetic and geometric means of sets of positive numbers that are close in magnitude to machine epsilon. The analysis is performed by comparing straightforward computational methods with more robust alternatives, quantifying the performance using relative error against a high-accuracy baseline.\n\nThe foundation of this analysis is the standard model of floating-point arithmetic, where for real numbers $x$ and $y$ and an elementary operation $\\mathrm{op}$, the computed result $\\mathrm{fl}(x \\,\\mathrm{op}\\, y)$ is given by $\\mathrm{fl}(x \\,\\mathrm{op}\\, y) = (x \\,\\mathrm{op}\\, y)(1 + \\delta)$, where $|\\delta| \\le u$. The term $u$ represents the unit roundoff, which for IEEE $754$ double precision is $u \\approx 2^{-53}$. The machine epsilon, $\\varepsilon$, is twice the unit roundoff, $\\varepsilon = 2u \\approx 2^{-52}$.\n\n**I. Arithmetic Mean**\n\nThe arithmetic mean (AM) of a set of $n$ numbers $\\{x_1, x_2, \\dots, x_n\\}$ is defined as $A = \\frac{1}{n} \\sum_{i=1}^{n} x_i$.\n\n**1. Straightforward Algorithm for AM**\n\nThe most direct method is to first compute the sum $S = \\sum_{i=1}^{n} x_i$ using a simple loop and then divide by $n$. In floating-point arithmetic, this sum is computed iteratively as $\\hat{S}_k = \\mathrm{fl}(\\hat{S}_{k-1} + x_k)$, where $\\hat{S}_0 = 0$. Each addition can introduce a rounding error. When summing a large number of values, these small errors can accumulate to a significant total error in the final sum. This is especially problematic when adding numbers of widely different magnitudes, but even with numbers of similar magnitude as in this problem, the accumulated error over a long sequence (e.g., $n=100000$) can degrade accuracy. This method is implemented using `numpy.sum()`.\n\n**2. Robust Algorithm for AM: Kahan Summation**\n\nTo mitigate the accumulation of rounding errors, a compensated summation algorithm, specifically the Kahan summation algorithm, is a robust alternative. This algorithm maintains a running compensation variable, $c$, which captures the low-order bits lost in each addition. The value of $c$ is then incorporated into the next step of the summation, effectively correcting the accumulated sum.\n\nThe Kahan summation algorithm proceeds as follows for a sequence of numbers $X = \\{x_1, \\dots, x_n\\}$:\n1. Initialize sum $s = 0.0$ and compensation $c = 0.0$.\n2. For each $x_i$ in $X$:\n   a. $y = x_i - c$\n   b. $t = s + y$\n   c. $c = (t - s) - y$\n   d. $s = t$\n3. Return $s$.\n\nThe key step is $c = (t - s) - y$. In exact arithmetic, this would be $c = ((s+y)-s) - y = 0$. However, in floating-point arithmetic, $t = \\mathrm{fl}(s+y)$ might lose the low-order bits of $y$ if $s$ is much larger than $y$. The term $(t-s)$ recovers the high-order part of $y$ that was successfully added to $s$, so $(t-s)-y$ isolates the negative of the part of $y$ that was lost. This lost part is stored in $c$ and subtracted from the next term $x_{i+1}$, reintroducing the lost precision into the calculation. The robust mean is then calculated as $A_{\\text{robust}} = \\text{KahanSum}(X) / n$.\n\n**3. Baseline for AM**\n\nThe reference value for the arithmetic mean, $A_{\\text{ref}}$, is computed using a high-accuracy summation routine as specified. Python's `math.fsum()` provides such a routine. It uses an algorithm by Shewchuk that tracks multiple partial sums to maintain high precision, effectively eliminating round-off error for all but the most pathological inputs. The baseline is thus $A_{\\text{ref}} = \\mathrm{math.fsum}(X) / n$.\n\n**II. Geometric Mean**\n\nThe geometric mean (GM) of a set of positive numbers $\\{x_1, x_2, \\dots, x_n\\}$ is defined as $G = (\\prod_{i=1}^{n} x_i)^{1/n}$.\n\n**1. Straightforward Algorithm for GM**\n\nThe direct implementation is to compute the product $P = \\prod_{i=1}^{n} x_i$ and then take the $n$-th root, $G_{\\text{sf}} = P^{1/n}$. This method is highly susceptible to numerical underflow or overflow. The test cases involve numbers $x_i$ on the order of machine epsilon, $\\varepsilon \\approx 2.22 \\times 10^{-16}$. The product of $n$ such numbers is approximately $\\varepsilon^n$. For even moderate $n$, like $n=300$ in Case $2$, this value becomes astronomically small ($\\approx (10^{-16})^{300} = 10^{-4800}$), far below the minimum representable positive double-precision number (which is approximately $5 \\times 10^{-324}$). The computed product underflows to $0.0$, leading to a completely incorrect geometric mean of $0.0$.\n\n**2. Robust Algorithm for GM: Logarithmic Transformation**\n\nA numerically stable approach avoids the large product by using logarithms. Based on the identity $\\ln(G) = \\frac{1}{n}\\sum_{i=1}^{n}\\ln(x_i)$, the geometric mean can be computed as:\n$G_{\\text{robust}} = \\exp\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\ln(x_i)\\right)$.\nThis transformation converts the product into a sum. For $x_i \\approx \\varepsilon$, the values of $\\ln(x_i)$ are moderate negative numbers (around $\\ln(2.22 \\times 10^{-16}) \\approx -36.0$). Summing these values is numerically stable and avoids the underflow/overflow issues inherent in direct multiplication. The sum of logarithms is computed using the standard `numpy.sum()`.\n\n**3. Baseline for GM**\n\nTo create the most accurate reference value, $G_{\\text{ref}}$, we combine the robust logarithmic transformation with the high-accuracy `math.fsum()` for the summation part:\n$G_{\\text{ref}} = \\exp\\left(\\frac{\\mathrm{math.fsum}(\\{\\ln(x_1), \\dots, \\ln(x_n)\\})}{n}\\right)$.\nThis serves as the \"true\" value against which the straightforward and robust methods are compared.\n\n**III. Error Quantification**\n\nThe robustness of each method is quantified by its relative error with respect to the corresponding baseline value. For an approximation $a_{\\text{approx}}$ and a reference $a_{\\text{ref}}$, the relative error is computed as $\\frac{|a_{\\text{approx}} - a_{\\text{ref}}|}{a_{\\text{ref}}}$. Since all $x_i$ are strictly positive, all means will be positive, and the reference values will be non-zero.\n\nThe program will execute this analysis for each of the four specified test cases, which are designed to probe different aspects of numerical stability, including the effects of large $n$ and potential underflow.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef kahan_sum(arr: np.ndarray) - float:\n    \"\"\"\n    Computes the sum of an array of floats using Kahan's summation algorithm\n    to minimize numerical error.\n    \"\"\"\n    s = 0.0\n    c = 0.0  # A running compensation for lost low-order bits.\n    for x in arr:\n        y = x - c\n        t = s + y\n        # Algebraically, c should be 0.\n        # But in floating point, it recovers the lost low-order part of y.\n        c = (t - s) - y\n        s = t\n    return s\n\ndef relative_error(approx_val: float, ref_val: float) - float:\n    \"\"\"\n    Computes the relative error of an approximation.\n    \"\"\"\n    if ref_val == 0:\n        # Avoid division by zero. If ref is 0, any non-zero approx is infinite error.\n        # If both are 0, error is 0.\n        return 0.0 if approx_val == 0.0 else np.inf\n    return np.abs(approx_val - ref_val) / np.abs(ref_val)\n\ndef generate_test_cases():\n    \"\"\"\n    Generates the four deterministic test cases as specified in the problem.\n    \"\"\"\n    eps = np.finfo(np.float64).eps\n\n    # Case 1 (happy path, small n)\n    n1 = 5\n    x1 = np.array([eps * (1.0 + i * 1e-12) for i in range(n1)], dtype=np.float64)\n\n    # Case 2 (product underflow stress)\n    n2 = 300\n    x2 = np.full(n2, eps, dtype=np.float64)\n\n    # Case 3 (large n, mild heterogeneity)\n    n3 = 20000\n    x3 = np.full(n3, eps, dtype=np.float64)\n    x3[1::2] = 1.5 * eps  # even indices in 1-based counting\n\n    # Case 4 (very large n, tiny deterministic perturbations)\n    n4 = 100000\n    i = np.arange(n4)\n    perturbations = ((i % 10) - 5) * 1e-16\n    x4 = eps * (1.0 + perturbations)\n\n    return [(n1, x1), (n2, x2), (n3, x3), (n4, x4)]\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    test_cases = generate_test_cases()\n    all_results = []\n\n    for n, x in test_cases:\n        case_results = []\n\n        # --- Arithmetic Mean Analysis ---\n        # Baseline AM (high-accuracy sum)\n        am_ref = math.fsum(x) / n\n        \n        # Straightforward AM\n        am_sf = np.sum(x) / n\n        \n        # Robust AM (Kahan sum)\n        am_robust = kahan_sum(x) / n\n\n        # Relative errors for AM\n        err_am_sf = relative_error(am_sf, am_ref)\n        err_am_robust = relative_error(am_robust, am_ref)\n        case_results.extend([err_am_sf, err_am_robust])\n\n        # --- Geometric Mean Analysis ---\n        log_x = np.log(x)\n        \n        # Baseline GM (log-transform with high-accuracy sum)\n        gm_ref = np.exp(math.fsum(log_x) / n)\n        \n        # Straightforward GM (direct product)\n        # This is expected to underflow for large n\n        with np.errstate(under='ignore'): # Suppress underflow warnings for clean output\n            gm_sf = np.prod(x)**(1.0/n)\n        \n        # Robust GM (log-transform with standard sum)\n        gm_robust = np.exp(np.sum(log_x) / n)\n\n        # Relative errors for GM\n        err_gm_sf = relative_error(gm_sf, gm_ref)\n        err_gm_robust = relative_error(gm_robust, gm_ref)\n        case_results.extend([err_gm_sf, err_gm_robust])\n        \n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified.\n    results_str_parts = []\n    for row in all_results:\n        # Use repr() for high-precision floating point string representation\n        # which is suitable for this numerical context.\n        row_str = \",\".join(map(repr, row))\n        results_str_parts.append(f\"[{row_str}]\")\n    \n    final_output = f\"[{','.join(results_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3205191"}, {"introduction": "The determinant of a matrix is a fundamental concept in linear algebra, but its textbook definition via cofactor expansion is a classic example of an algorithm that is computationally infeasible and numerically unstable for all but the smallest matrices. In contrast, modern scientific computing relies on matrix factorizations, like the LU decomposition, which are both efficient and robust. This practice [@problem_id:3205186] provides a stark, empirical comparison, revealing why the stable, polynomial-time LU-based method is universally preferred over the factorial-time, error-prone cofactor expansion.", "problem": "You are given the task of assessing the numerical stability and robustness of two algorithms for computing the determinant of a square matrix: cofactor (Laplace) expansion and the product of factors from a Lower-Upper (LU) decomposition with partial pivoting. The fundamental base for all reasoning in this task is the standard floating-point arithmetic model and the well-established properties of Gaussian elimination with partial pivoting.\n\nAssume the floating-point arithmetic follows the model $fl(x \\ \\mathrm{op} \\ y) = (x \\ \\mathrm{op} \\ y)(1 + \\delta)$ with $|\\delta| \\leq \\varepsilon$, where $\\varepsilon$ is the machine roundoff unit, and $\\mathrm{op} \\in \\{+, -, \\times, /\\}$. Assume real double-precision arithmetic throughout the computational procedures. The cofactor expansion computes the determinant by recursively expanding along the first row using minors and alternating signs. The LU decomposition computes a decomposition $P A = L U$, where $P$ is a permutation matrix recording row interchanges, $L$ is unit lower triangular, and $U$ is upper triangular, and then evaluates the determinant as the signed product of the diagonal entries of $U$.\n\nYour program must:\n- Implement the determinant via cofactor expansion in floating-point arithmetic.\n- Implement the determinant via LU decomposition with partial pivoting in floating-point arithmetic, with the determinant given by the product of the diagonal entries of $U$ times the sign associated with the permutation.\n- Compute a reference determinant exactly using the fraction-free Bareiss algorithm in rational arithmetic (based on Python exact rational numbers), to serve as ground truth.\n\nFor each test case matrix $A$, let $d_{\\mathrm{exact}}$ denote the exact determinant computed in rational arithmetic. Let $d_{\\mathrm{cofactor}}$ and $d_{\\mathrm{LU}}$ denote the determinants computed in floating-point arithmetic via cofactor expansion and LU decomposition, respectively. For matrices with $d_{\\mathrm{exact}} \\neq 0$, define the relative forward error for a method as\n$$\ne = \\frac{|d_{\\mathrm{approx}} - d_{\\mathrm{exact}}|}{|d_{\\mathrm{exact}}|}.\n$$\nFor matrices with $d_{\\mathrm{exact}} = 0$, define the absolute forward error as\n$$\ne = |d_{\\mathrm{approx}}|.\n$$\nFor each test case, report three values: the error of cofactor expansion, the error of LU, and a boolean indicating whether LU has strictly smaller error than cofactor expansion.\n\nUse the following test suite of matrices, encoded exactly as specified:\n\n- Case $1$ (well-conditioned integer $3 \\times 3$):\n  $$\n  A_1 = \\begin{pmatrix}\n  2  -3  1 \\\\\n  2  0  -1 \\\\\n  1  4  5\n  \\end{pmatrix}.\n  $$\n- Case $2$ (Hilbert $4 \\times 4$, rational and ill-conditioned):\n  $$\n  A_2(i,j) = \\frac{1}{i + j - 1}, \\quad 1 \\leq i,j \\leq 4.\n  $$\n- Case $3$ (Vandermonde $5 \\times 5$ with closely spaced rational nodes):\n  Nodes $x_1 = \\frac{10000}{10000}$, $x_2 = \\frac{10001}{10000}$, $x_3 = \\frac{10002}{10000}$, $x_4 = \\frac{10003}{10000}$, $x_5 = \\frac{10004}{10000}$, and\n  $$\n  A_3(i,j) = x_i^{j-1}, \\quad 1 \\leq i \\leq 5, \\ 1 \\leq j \\leq 5.\n  $$\n- Case $4$ (singular $4 \\times 4$ with a row multiple):\n  $$\n  A_4 = \\begin{pmatrix}\n  1  2  3  4 \\\\\n  2  4  6  8 \\\\\n  1  -1  0  1 \\\\\n  0  0  0  1\n  \\end{pmatrix}.\n  $$\n- Case $5$ (large-scale integer $4 \\times 4$):\n  $$\n  A_5 = \\begin{pmatrix}\n  123456789  987654321  564738291  192837465 \\\\\n  111111111  222222222  333333333  444444444 \\\\\n  135791357  246802468  112233445  556677889 \\\\\n  100000000  100000001  99999999  123456789\n  \\end{pmatrix}.\n  $$\n\nAngle units are not applicable. No physical units are involved. All outputs must be numerical and unitless. For each case, output a list [$e_{\\mathrm{cofactor}}, e_{\\mathrm{LU}}, b$], where $e_{\\mathrm{cofactor}}$ and $e_{\\mathrm{LU}}$ are real numbers (floating-point), and $b$ is a boolean. For cases with $d_{\\mathrm{exact}} \\neq 0$ report the relative errors; for cases with $d_{\\mathrm{exact}} = 0$ report the absolute errors. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \n$$\n[[$e_{1,\\mathrm{cofactor}}, e_{1,\\mathrm{LU}}, b_1], \\ldots, [$e_{5,\\mathrm{cofactor}}, e_{5,\\mathrm{LU}}, b_5]\\,].\n$$", "solution": "The problem requires an assessment of the numerical stability and robustness of two algorithms for computing the determinant of a square matrix: cofactor expansion and LU decomposition with partial pivoting. The assessment will be performed by comparing their results against an exact value computed using rational arithmetic.\n\nThe core principle distinguishing these algorithms lies in their computational complexity and susceptibility to roundoff errors in standard floating-point arithmetic, which is modeled by $fl(x \\ \\mathrm{op} \\ y) = (x \\ \\mathrm{op} \\ y)(1 + \\delta)$ with $|\\delta| \\leq \\varepsilon$ (machine epsilon).\n\n### Method 1: Cofactor (Laplace) Expansion\n\nThe determinant of an $n \\times n$ matrix $A$ can be defined recursively by expanding along a row or column. Expanding along the first row gives:\n$$\n\\det(A) = \\sum_{j=1}^{n} (-1)^{1+j} a_{1j} \\det(M_{1j})\n$$\nwhere $a_{1j}$ is the element in the first row and $j$-th column, and $M_{1j}$ is the $(n-1) \\times (n-1)$ submatrix (minor) obtained by removing row $1$ and column $j$ from $A$. The base case is a $1 \\times 1$ matrix, for which $\\det([a]) = a$.\n\n**Computational Cost and Stability:**\nThe number of multiplications required for this method follows the recurrence $T(n) = n T(n-1) + n$, which leads to a computational complexity of $O(n!)$. For even moderately sized matrices (e.g., $n  20$), this is computationally intractable. More critically for numerical stability, the formula involves a sum of $n$ terms. These terms can be large and have alternating signs, creating a high potential for **catastrophic cancellation**—the subtraction of nearly equal floating-point numbers, leading to a drastic loss of significant digits. The vast number of arithmetic operations also leads to the accumulation of roundoff errors at each step.\n\n### Method 2: LU Decomposition with Partial Pivoting\n\nThis method is based on the factorization of a matrix $A$ into a product of a lower triangular matrix $L$, an upper triangular matrix $U$, and a permutation matrix $P$ such that $PA = LU$. The determinant of $A$ is then derived from the properties of determinants:\n$$\n\\det(P) \\det(A) = \\det(L) \\det(U)\n$$\n$$\n\\det(A) = \\det(P)^{-1} \\det(L) \\det(U)\n$$\nIn this decomposition:\n- $L$ is a unit lower triangular matrix, so $\\det(L) = 1$.\n- $U$ is an upper triangular matrix, so its determinant is the product of its diagonal entries: $\\det(U) = \\prod_{i=1}^{n} u_{ii}$.\n- $P$ is a permutation matrix that represents row interchanges. Its determinant is $\\det(P) = (-1)^s$, where $s$ is the number of row swaps performed. Thus, $\\det(P)^{-1} = \\det(P) = (-1)^s$.\n\nCombining these, the determinant is:\n$$\n\\det(A) = (-1)^s \\prod_{i=1}^{n} u_{ii}\n$$\n**Computational Cost and Stability:**\nThe LU factorization algorithm has a complexity of $O(n^3)$, which is vastly more efficient than cofactor expansion. The key to its numerical stability is **partial pivoting**. At each step of the elimination, the algorithm chooses the entry with the largest absolute value in the current column as the pivot. This ensures that the multipliers used in the elimination satisfy $|l_{ij}| \\leq 1$, which helps to bound the growth of elements in the matrix $U$ during the procedure. Controlling this element growth is crucial for mitigating roundoff error accumulation. The final determinant is computed via a single product of $n$ numbers, a much more stable operation than the large summation required by cofactor expansion. Gaussian elimination with partial pivoting is the standard, stable method for this class of problems.\n\n### Method 3: Exact Rational Arithmetic\n\nTo provide an incontrovertible baseline, $d_{\\mathrm{exact}}$, we compute the determinant using exact rational arithmetic. The problem statement mentions the Bareiss algorithm, which is an elegant method for fraction-free determinant computation over an integral domain. A more direct approach for matrices with rational entries, and one which aligns with the spirit of using \"Python exact rational numbers\", is to perform Gaussian elimination with partial pivoting directly on matrix elements represented as `fractions.Fraction` objects. This avoids all floating-point precision issues, yielding a mathematically exact result. The determinant is computed as the signed product of the diagonal elements of the resulting upper triangular matrix of rational numbers.\n\n### Error Analysis\n\nThe forward error of the floating-point approximations ($d_{\\mathrm{approx}} \\in \\{d_{\\mathrm{cofactor}}, d_{\\mathrm{LU}}\\}$) is quantified relative to the exact determinant $d_{\\mathrm{exact}}$.\n- For non-singular matrices ($d_{\\mathrm{exact}} \\neq 0$), the relative forward error is used:\n$$\ne = \\frac{|d_{\\mathrm{approx}} - d_{\\mathrm{exact}}|}{|d_{\\mathrm{exact}}|}\n$$\n- For singular matrices ($d_{\\mathrm{exact}} = 0$), the absolute forward error is used:\n$$\ne = |d_{\\mathrm{approx}}|\n$$\nThis allows for a fair comparison of the accuracy of the two floating-point methods across a test suite of matrices designed to challenge their stability, including well-conditioned, ill-conditioned, and singular cases. The boolean value $b$ indicating whether $e_{\\mathrm{LU}}  e_{\\mathrm{cofactor}}$ will systematically show the superiority of the LU-based method.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom fractions import Fraction\nimport sys\n\n# Increase recursion limit for cofactor expansion on larger matrices if needed, though 5x5 is fine.\nsys.setrecursionlimit(2000)\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing and comparing three determinant algorithms.\n    \"\"\"\n\n    def det_cofactor(A: np.ndarray) - float:\n        \"\"\"\n        Computes the determinant using cofactor expansion.\n        Implemented recursively.\n        \"\"\"\n        n = A.shape[0]\n        if n == 1:\n            return A[0, 0]\n\n        total = 0.0\n        for j in range(n):\n            minor = np.delete(np.delete(A, 0, axis=0), j, axis=1)\n            sign = 1.0 if j % 2 == 0 else -1.0\n            total += sign * A[0, j] * det_cofactor(minor)\n        return total\n\n    def det_lu_pivot(A: np.ndarray) - float:\n        \"\"\"\n        Computes the determinant using LU decomposition with partial pivoting.\n        \"\"\"\n        n = A.shape[0]\n        U = A.copy().astype(np.float64)\n        num_swaps = 0\n\n        for k in range(n):\n            # Find pivot in column k\n            pivot_row = k + np.argmax(np.abs(U[k:, k]))\n            \n            # Check for singularity\n            if U[pivot_row, k] == 0.0:\n                return 0.0\n\n            if pivot_row != k:\n                # Swap rows\n                U[[k, pivot_row]] = U[[pivot_row, k]]\n                num_swaps += 1\n\n            # Elimination\n            for i in range(k + 1, n):\n                factor = U[i, k] / U[k, k]\n                U[i, k:] -= factor * U[k, k:]\n        \n        det = (-1.0)**num_swaps\n        for i in range(n):\n            det *= U[i, i]\n            \n        return det\n        \n    def det_exact_rational(matrix_data) - Fraction:\n        \"\"\"\n        Computes the exact determinant using Gaussian elimination with Fraction objects.\n        \"\"\"\n        # Ensure the matrix is made of Fraction objects\n        M = [[Fraction(x) for x in row] for row in matrix_data]\n        n = len(M)\n        if n == 0:\n            return Fraction(1)\n        \n        sign = Fraction(1)\n\n        for k in range(n):\n            # Find pivot\n            pivot_row_idx = k\n            max_val = abs(M[k][k])\n            for i in range(k + 1, n):\n                if abs(M[i][k])  max_val:\n                    max_val = abs(M[i][k])\n                    pivot_row_idx = i\n\n            if max_val == 0:\n                return Fraction(0)  # Singular matrix\n\n            # Swap rows if necessary\n            if pivot_row_idx != k:\n                M[k], M[pivot_row_idx] = M[pivot_row_idx], M[k]\n                sign *= -1\n\n            # Elimination\n            pivot_val = M[k][k]\n            for i in range(k + 1, n):\n                if M[i][k] != 0:\n                    factor = M[i][k] / pivot_val\n                    for j in range(k, n):\n                        M[i][j] -= factor * M[k][j]\n\n        # Calculate determinant as the product of the diagonal\n        det = sign\n        for i in range(n):\n            det *= M[i][i]\n        return det\n\n    # Define the test cases\n    A1 = [[2, -3, 1], [2, 0, -1], [1, 4, 5]]\n    \n    A2 = [[Fraction(1, i + j - 1) for j in range(1, 5)] for i in range(1, 5)]\n\n    nodes3 = [Fraction(10000 + i, 10000) for i in range(5)]\n    A3 = [[node ** (j - 1) for j in range(1, 6)] for node in nodes3]\n\n    A4 = [[1, 2, 3, 4], [2, 4, 6, 8], [1, -1, 0, 1], [0, 0, 0, 1]]\n    \n    A5 = [[123456789, 987654321, 564738291, 192837465],\n          [111111111, 222222222, 333333333, 444444444],\n          [135791357, 246802468, 112233445, 556677889],\n          [100000000, 100000001, 99999999, 123456789]]\n\n    test_cases_rational = [A1, A2, A3, A4, A5]\n    \n    results = []\n    \n    for A_rational in test_cases_rational:\n        # Convert rational matrix to numpy float array for FP algorithms\n        A_float = np.array([[float(x) for x in row] for row in A_rational])\n\n        # Compute determinants\n        d_exact = det_exact_rational(A_rational)\n        d_cofactor = det_cofactor(A_float)\n        d_lu = det_lu_pivot(A_float)\n\n        # Calculate errors\n        if d_exact != 0:\n            # Relative error\n            d_exact_float = float(d_exact)\n            e_cofactor = abs(d_cofactor - d_exact_float) / abs(d_exact_float)\n            e_lu = abs(d_lu - d_exact_float) / abs(d_exact_float)\n        else:\n            # Absolute error\n            e_cofactor = abs(d_cofactor)\n            e_lu = abs(d_lu)\n            \n        is_lu_better = e_lu  e_cofactor\n        \n        results.append(f\"[{e_cofactor},{e_lu},{str(is_lu_better).lower()}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3205186"}]}