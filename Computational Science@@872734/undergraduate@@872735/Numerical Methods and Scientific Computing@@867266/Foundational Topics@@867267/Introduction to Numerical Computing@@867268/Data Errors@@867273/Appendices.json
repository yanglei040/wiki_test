{"hands_on_practices": [{"introduction": "This first exercise provides a clear and tangible illustration of forward error propagation. By analyzing the simple act of computing a sphere's volume using a truncated value of $\\pi$, you will derive the exact resulting error. This practice [@problem_id:3221388] highlights how a small, seemingly insignificant data error can be amplified and lead to substantial inaccuracies, especially as other parameters in the calculation, like the radius $R$, become large.", "problem": "In numerical methods and scientific computing, truncation of mathematical constants introduces data errors that propagate through computations. Consider the volume of a sphere of radius $R$, where $R$ is a positive real number and is known exactly. The true volume is determined by a well-tested geometric formula. In practice, suppose the constant $\\pi$ is truncated to the decimal value $3.14$ before computing the sphere's volume. Using the foundational definition of forward (output) error as the difference between the computed quantity and the true quantity, and using only well-established geometric facts, derive the exact closed-form expression for the signed absolute error in the computed volume due solely to truncating $\\pi$ to $3.14$. Express your final answer as a single analytic expression in terms of $R$. No rounding is required.", "solution": "The problem statement has been validated and found to be self-contained, consistent, and scientifically sound. It presents a well-posed question in the domain of numerical analysis, specifically concerning data errors from constant truncation. The problem is formalizable and objective.\n\nThe objective is to derive the exact closed-form expression for the signed absolute error in the computed volume of a sphere when the constant $\\pi$ is truncated. The problem explicitly defines this error as the forward error.\n\nLet $V_{\\text{true}}$ be the true volume of the sphere and $V_{\\text{computed}}$ be the computed volume. The radius of the sphere is $R$, which is a positive real number known exactly.\n\nAccording to well-established geometric principles, the true volume of a sphere is given by the formula:\n$$V_{\\text{true}} = \\frac{4}{3}\\pi R^3$$\n\nThe problem states that for the computation, the constant $\\pi$ is truncated to the decimal value $3.14$. Let us denote this approximation as $\\pi_{\\text{approx}}$.\n$$\\pi_{\\text{approx}} = 3.14$$\nThe computed volume is therefore calculated using this truncated value:\n$$V_{\\text{computed}} = \\frac{4}{3}\\pi_{\\text{approx}} R^3 = \\frac{4}{3}(3.14)R^3$$\n\nThe problem defines the signed absolute error, which we will denote as $E$, as the forward (output) error. This is the difference between the computed quantity and the true quantity.\n$$E = V_{\\text{computed}} - V_{\\text{true}}$$\n\nSubstituting the expressions for $V_{\\text{computed}}$ and $V_{\\text{true}}$ into the definition of the error $E$, we obtain:\n$$E = \\left(\\frac{4}{3}(3.14)R^3\\right) - \\left(\\frac{4}{3}\\pi R^3\\right)$$\n\nTo simplify this expression, we can factor out the common term $\\frac{4}{3}R^3$:\n$$E = \\frac{4}{3}R^3 (3.14 - \\pi)$$\n\nThis expression represents the exact closed-form expression for the signed error in the volume. It is an analytic expression in terms of $R$, as required. The term $(3.14 - \\pi)$ is a negative constant, which correctly indicates that truncating $\\pi$ to a smaller value will result in a computed volume that is less than the true volume.\nThe final expression is:\n$$E = \\frac{4}{3}(3.14 - \\pi)R^3$$", "answer": "$$\\boxed{\\frac{4}{3}(3.14 - \\pi)R^3}$$", "id": "3221388"}, {"introduction": "Numerical differentiation is a cornerstone of scientific computing, but its application to real-world data is complicated by the presence of noise. This exercise [@problem_id:3221398] explores the crucial concept of algorithmic stability by analyzing how different finite difference formulas—forward, backward, and central—respond to high-frequency noise. You will discover that the choice of algorithm can dramatically amplify or suppress data errors, a fundamental insight for developing robust numerical methods.", "problem": "A uniformly spaced spatial grid is given by $x_i = x_0 + i h$ for integer $i$ and fixed spacing $h > 0$. A physical field $f(x)$ is sampled on this grid, but the measurements $u_i$ are contaminated by additive high-frequency noise so that $u_i = f(x_i) + \\epsilon \\cos(k x_i)$, where $\\epsilon > 0$ is small and $k$ is the wavenumber of the noise component. Consider numerical differentiation at $x_i$ using the following finite difference approximations of the derivative based on the samples $u_i$:\n- Forward difference: $(u_{i+1} - u_i)/h$,\n- Backward difference: $(u_i - u_{i-1})/h$,\n- Central difference: $(u_{i+1} - u_{i-1})/(2 h)$.\nAssume the notion of stability under high-frequency noise is defined by the magnitude of the amplification of the noise component by the differentiation operator: the smaller the amplification for a given high-frequency $k$, the more stable the method. Starting from core definitions of these difference operators and basic properties of complex exponentials $\\exp(i \\theta)$, analyze the response of each operator to the pure noise component and compare their stability at the highest resolvable wavenumber for this sampling, namely $k h = \\pi$.\nWhich statement correctly compares the stability of the three formulas in the presence of such high-frequency noise?\nA. The central difference suppresses the Nyquist-frequency noise (i.e., yields $0$ contribution from the noise), while the forward and backward differences amplify it maximally and equally; therefore, the central difference is most stable and the forward and backward differences are equally least stable.\nB. The forward difference suppresses the Nyquist-frequency noise, while the backward and central differences amplify it maximally; therefore, the forward difference is most stable.\nC. The backward difference suppresses the Nyquist-frequency noise, while the forward and central differences amplify it maximally; therefore, the backward difference is most stable.\nD. All three formulas have identical amplification of the Nyquist-frequency noise and thus identical stability.", "solution": "The problem statement is subjected to validation before proceeding to a solution.\n\n### Step 1: Extract Givens\n- A uniformly spaced spatial grid: $x_i = x_0 + i h$ for integer $i$ and $h > 0$.\n- Sampled field with noise: $u_i = f(x_i) + \\epsilon \\cos(k x_i)$, with $\\epsilon > 0$ and small.\n- Finite difference operators for the derivative at $x_i$ based on samples $u_i$:\n    - Forward difference: $(u_{i+1} - u_i)/h$.\n    - Backward difference: $(u_i - u_{i-1})/h$.\n    - Central difference: $(u_{i+1} - u_{i-1})/(2 h)$.\n- Definition of stability: Stability is inversely related to the magnitude of the amplification of the noise component by the operator. Smaller amplification implies greater stability.\n- Analysis condition: The comparison is to be performed at the highest resolvable wavenumber, specified as $k h = \\pi$.\n- Analytical tool hint: Use core definitions and properties of complex exponentials, $\\exp(i \\theta)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically sound, well-posed, and objective. It presents a standard problem in numerical analysis concerning the stability of finite difference schemes. The concepts of a uniform grid, finite difference operators, noisy data, wavenumber, and the Nyquist frequency ($k h = \\pi$) are all fundamental to scientific computing and signal processing. The provided definition of stability is clear and allows for a quantitative comparison of the methods.\n\nThere is a minor notational ambiguity: the symbol `$i$` is used to represent both the integer grid index in $x_i$ and the imaginary unit in $\\exp(i \\theta)$. This is a common but undesirable practice. To maintain clarity and rigor, I will use `$i$` for the integer index and adopt the notation $\\mathrm{j} = \\sqrt{-1}$ for the imaginary unit.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution will now be derived.\n\n### Derivation\n\nThe task is to analyze the response of the three given finite difference operators to a pure high-frequency noise component. The noise at grid point $x_i$ is given by $n(x_i) = \\epsilon \\cos(k x_i)$. To facilitate the analysis, we consider the complex-valued noise signal $N(x_i) = \\epsilon e^{\\mathrm{j} k x_i}$, as the response to the physical noise $\\epsilon \\cos(k x_i)$ is simply the real part of the response to $N(x_i)$, and stability is defined by the magnitude of amplification, which is identical for both.\n\nWe are interested in the behavior at the highest resolvable wavenumber, the Nyquist frequency, for which $k h = \\pi$.\nThe noise signal at grid point $i$ is:\n$$N_i = N(x_i) = \\epsilon e^{\\mathrm{j} k x_i} = \\epsilon e^{\\mathrm{j} k (x_0 + i h)} = \\epsilon e^{\\mathrm{j} k x_0} e^{\\mathrm{j} i k h}$$\nSubstituting $k h = \\pi$:\n$$N_i = (\\epsilon e^{\\mathrm{j} k x_0}) e^{\\mathrm{j} i \\pi}$$\nSince $e^{\\mathrm{j} \\pi} = \\cos(\\pi) + \\mathrm{j}\\sin(\\pi) = -1$, we have $e^{\\mathrm{j} i \\pi} = (e^{\\mathrm{j} \\pi})^i = (-1)^i$.\nLet $C = \\epsilon e^{\\mathrm{j} k x_0}$ be a complex constant. The noise signal is $N_i = C (-1)^i$. This represents a signal that alternates in sign at each grid point (e.g., $C, -C, C, -C, \\dots$).\n\nWe now apply each finite difference operator to this noise signal $N_i$.\n\n**1. Forward Difference (FD)**\nLet $D_{FD}$ denote the forward difference operator. Its action on the noise $N_i$ is:\n$$D_{FD}[N_i] = \\frac{N_{i+1} - N_i}{h} = \\frac{C(-1)^{i+1} - C(-1)^i}{h} = \\frac{C(-1)^i(-1) - C(-1)^i}{h} = \\frac{-2C(-1)^i}{h}$$\nThe output is a scaled version of the input noise signal. The magnitude of the output at point $i$ is:\n$$|D_{FD}[N_i]| = \\left| \\frac{-2C(-1)^i}{h} \\right| = \\frac{2|C|}{h} = \\frac{2\\epsilon}{h}$$\nThe amplification of the noise magnitude is a factor of $2/h$.\n\n**2. Backward Difference (BD)**\nLet $D_{BD}$ denote the backward difference operator.\n$$D_{BD}[N_i] = \\frac{N_i - N_{i-1}}{h} = \\frac{C(-1)^i - C(-1)^{i-1}}{h} = \\frac{C(-1)^i - C(-1)^i(-1)^{-1}}{h} = \\frac{C(-1)^i - C(-1)^i(-1)}{h} = \\frac{2C(-1)^i}{h}$$\nThe magnitude of the output at point $i$ is:\n$$|D_{BD}[N_i]| = \\left| \\frac{2C(-1)^i}{h} \\right| = \\frac{2|C|}{h} = \\frac{2\\epsilon}{h}$$\nThe amplification of the noise magnitude is also a factor of $2/h$, identical to the forward difference.\n\n**3. Central Difference (CD)**\nLet $D_{CD}$ denote the central difference operator.\n$$D_{CD}[N_i] = \\frac{N_{i+1} - N_{i-1}}{2h} = \\frac{C(-1)^{i+1} - C(-1)^{i-1}}{2h} = \\frac{C(-1)^i(-1) - C(-1)^i(-1)^{-1}}{2h} = \\frac{-C(-1)^i - (-C(-1)^i)}{2h} = \\frac{0}{2h} = 0$$\nThe output is zero. The amplification of the noise magnitude is $0$.\n\nTo confirm that the amplification for FD and BD is maximal at this frequency, we can derive the general amplification factor $G(k)$ for a generic wavenumber $k$. Let the operator $D_h$ act on $e^{\\mathrm{j}kx_i}$.\n$D_h[e^{\\mathrm{j}kx_i}] = G(k)e^{\\mathrm{j}kx_i}$. The stability is related to $|G(k)|$.\nLet $\\theta = kh$.\n- FD: $D_{FD}[e^{\\mathrm{j}i\\theta}] = \\frac{e^{\\mathrm{j}(i+1)\\theta} - e^{\\mathrm{j}i\\theta}}{h} = e^{\\mathrm{j}i\\theta} \\frac{e^{\\mathrm{j}\\theta}-1}{h}$. So $G_{FD}(\\theta) = \\frac{e^{\\mathrm{j}\\theta}-1}{h}$.\nThe magnitude is $|G_{FD}(\\theta)| = \\frac{|e^{\\mathrm{j}\\theta}-1|}{h} = \\frac{|\\cos\\theta-1+\\mathrm{j}\\sin\\theta|}{h} = \\frac{\\sqrt{(\\cos\\theta-1)^2+\\sin^2\\theta}}{h} = \\frac{\\sqrt{2-2\\cos\\theta}}{h} = \\frac{2|\\sin(\\theta/2)|}{h}$. For resolvable wavenumbers, $\\theta \\in [0, \\pi]$. This magnitude is maximized when $|\\sin(\\theta/2)|=1$, which occurs at $\\theta=\\pi$. The maximum value is $2/h$.\n- BD: $D_{BD}[e^{\\mathrm{j}i\\theta}] = \\frac{e^{\\mathrm{j}i\\theta} - e^{\\mathrm{j}(i-1)\\theta}}{h} = e^{\\mathrm{j}i\\theta} \\frac{1-e^{-\\mathrm{j}\\theta}}{h}$. So $G_{BD}(\\theta) = \\frac{1-e^{-\\mathrm{j}\\theta}}{h}$.\nThe magnitude is $|G_{BD}(\\theta)| = \\frac{|1-e^{-\\mathrm{j}\\theta}|}{h} = |e^{\\mathrm{j}\\theta}| \\frac{|e^{-\\mathrm{j}\\theta}-1|}{h} = \\frac{|e^{-\\mathrm{j}\\theta}-1|}{h} = \\frac{2|\\sin(-\\theta/2)|}{h} = \\frac{2|\\sin(\\theta/2)|}{h}$. This is identical to the forward difference, with the same maximum value of $2/h$ at $\\theta=\\pi$.\n- CD: $D_{CD}[e^{\\mathrm{j}i\\theta}] = \\frac{e^{\\mathrm{j}(i+1)\\theta} - e^{\\mathrm{j}(i-1)\\theta}}{2h} = e^{\\mathrm{j}i\\theta} \\frac{e^{\\mathrm{j}\\theta}-e^{-\\mathrm{j}\\theta}}{2h} = e^{\\mathrm{j}i\\theta} \\frac{2\\mathrm{j}\\sin\\theta}{2h}$. So $G_{CD}(\\theta) = \\frac{\\mathrm{j}\\sin\\theta}{h}$.\nThe magnitude is $|G_{CD}(\\theta)| = \\frac{|\\mathrm{j}\\sin\\theta|}{h} = \\frac{|\\sin\\theta|}{h}$. At $\\theta=\\pi$, the magnitude is $|\\sin(\\pi)|/h = 0/h = 0$.\n\n### Summary of Results\n- At the Nyquist frequency ($k h = \\pi$), the central difference operator yields an output of $0$. It completely suppresses this noise component.\n- At the same frequency, the forward and backward difference operators both amplify the noise magnitude by a factor of $2/h$. This is the maximum possible amplification for these operators over the entire range of resolvable frequencies.\n- According to the problem's definition of stability (smaller amplification is more stable), the central difference is the most stable method, while the forward and backward differences are equally the least stable.\n\n### Option-by-Option Analysis\n- **A. The central difference suppresses the Nyquist-frequency noise (i.e., yields $0$ contribution from the noise), while the forward and backward differences amplify it maximally and equally; therefore, the central difference is most stable and the forward and backward differences are equally least stable.**\nThis statement perfectly matches our derived results. The central difference amplification is $0$ (\"suppresses\"). The forward and backward difference amplifications are equal ($2/h$) and maximal. Consequently, the stability ranking is correct.\n**Verdict: Correct.**\n\n- **B. The forward difference suppresses the Nyquist-frequency noise, while the backward and central differences amplify it maximally; therefore, the forward difference is most stable.**\nThis is incorrect. The forward difference amplifies the noise maximally, it does not suppress it.\n**Verdict: Incorrect.**\n\n- **C. The backward difference suppresses the Nyquist-frequency noise, while the forward and central differences amplify it maximally; therefore, the backward difference is most stable.**\nThis is incorrect. The backward difference amplifies the noise maximally, it does not suppress it.\n**Verdict: Incorrect.**\n\n- **D. All three formulas have identical amplification of the Nyquist-frequency noise and thus identical stability.**\nThis is incorrect. The amplification magnitudes are $2/h$, $2/h$, and $0$, which are not identical.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3221398"}, {"introduction": "In many scientific applications, we work with limited and noisy datasets where the \"true\" values are unknown and analytical error formulas are impractical. This practice [@problem_id:3221310] moves beyond analyzing known, deterministic errors to estimating statistical uncertainty. You will implement the nonparametric bootstrap from first principles, a powerful and widely-used computational method to quantify the uncertainty in statistics like the mean and standard deviation by resampling directly from the observed data.", "problem": "You are given several small datasets consisting of real-valued observations with noise. Your task is to implement a nonparametric bootstrap to quantify the uncertainty in the sample mean and the unbiased sample standard deviation. Build your approach from first principles using the following core definitions and facts.\n\nFundamental base:\n- For observations $x_{1}, x_{2}, \\dots, x_{n}$, the sample mean is defined as $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}$.\n- The unbiased sample standard deviation uses Bessel’s correction and is defined as $s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}}$ for $n \\ge 2$.\n- The standard error of a statistic is the standard deviation of its sampling distribution.\n- The nonparametric bootstrap approximates the sampling distribution by resampling with replacement from the empirical distribution that places mass $1/n$ at each observed $x_{i}$. A bootstrap replicate is obtained by drawing $n$ samples with replacement from $\\{x_{1}, \\dots, x_{n}\\}$ and computing the statistic on that resample.\n- A two-sided confidence interval with coverage $0.95$ can be constructed using the percentile method by taking the empirical quantiles at probabilities $0.025$ and $0.975$ of the bootstrap replicates.\n\nRequirements:\n- Implement a program that, for each dataset, performs the following.\n  1. Compute the observed sample mean $\\bar{x}$ and unbiased sample standard deviation $s$.\n  2. Perform $B$ bootstrap resamples, each of size $n$, using sampling with replacement from the observed data.\n  3. For each bootstrap resample, compute the sample mean and unbiased sample standard deviation (using the same $n-1$ degrees-of-freedom correction).\n  4. Estimate the standard error of the mean as the sample standard deviation of the bootstrap means, and the standard error of the standard deviation as the sample standard deviation of the bootstrap standard deviations.\n  5. Construct the two-sided percentile confidence intervals at coverage $0.95$ for both the mean and the standard deviation using the empirical quantiles at $0.025$ and $0.975$ computed by linear interpolation between order statistics.\n- To ensure determinism, for each test case set the random number generator to the given integer seed prior to resampling.\n- All final numeric outputs must be rounded to $6$ decimal places.\n- The final output must be a single line containing a list of results for all test cases. For each test case, produce a list of $8$ floats in the following order: $[\\bar{x}, \\text{SE}(\\bar{x}), \\text{CI}_{\\text{mean,lower}}, \\text{CI}_{\\text{mean,upper}}, s, \\text{SE}(s), \\text{CI}_{\\text{sd,lower}}, \\text{CI}_{\\text{sd,upper}}]$. The overall output must be a list of these per-case lists, with no spaces, for example: $[[a_{1},a_{2},\\dots,a_{8}],[b_{1},\\dots,b_{8}],\\dots]$.\n\nAngle units are not involved. No physical units are involved. All probabilities must be expressed as decimals, not with a percent sign.\n\nTest suite:\n- Case $1$ (general case, moderate noise):\n  - Data $x = (1.32, 0.98, 1.15, 0.87, 1.40, 1.05, 0.75, 1.22, 1.08, 0.91, 1.10, 1.27)$\n  - Number of bootstrap replicates $B = 10000$\n  - Seed $= 12345$\n- Case $2$ (small sample, heavy-tailed behavior reflected in values):\n  - Data $x = (0.50, -0.20, 1.80, -3.50, 2.70)$\n  - Number of bootstrap replicates $B = 20000$\n  - Seed $= 24680$\n- Case $3$ (degenerate variance edge case):\n  - Data $x = (2.00, 2.00, 2.00, 2.00, 2.00, 2.00)$\n  - Number of bootstrap replicates $B = 5000$\n  - Seed $= 42$\n- Case $4$ (presence of an outlier):\n  - Data $x = (9.80, 10.20, 9.90, 10.10, 9.70, 10.00, 10.30, 9.60, 9.95, 30.00)$\n  - Number of bootstrap replicates $B = 15000$\n  - Seed $= 13579$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets. For example: $[[r_{1,1},\\dots,r_{1,8}],[r_{2,1},\\dots,r_{2,8}],[r_{3,1},\\dots,r_{3,8}],[r_{4,1},\\dots,r_{4,8}]]$. There must be no spaces anywhere in the printed line.", "solution": "The user requests the implementation of a nonparametric bootstrap procedure to estimate the uncertainty of the sample mean and the unbiased sample standard deviation for several datasets. This requires a step-by-step process grounded in statistical principles.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Statistic Definitions**:\n    -   Sample mean: $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}$.\n    -   Unbiased sample standard deviation: $s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}}$ for $n \\ge 2$.\n-   **Bootstrap Procedure**:\n    -   The standard error (SE) of a statistic is the standard deviation of its sampling distribution.\n    -   The nonparametric bootstrap approximates the sampling distribution by creating $B$ resamples of size $n$ by drawing with replacement from the original data $\\{x_1, \\dots, x_n\\}$.\n    -   The standard error of a statistic is estimated as the sample standard deviation of the bootstrap replicates of that statistic.\n    -   A $0.95$ confidence interval (CI) is constructed using the percentile method, taking the empirical quantiles at probabilities $0.025$ and $0.975$.\n    -   Quantile computation must use linear interpolation between order statistics.\n-   **Execution Requirements**:\n    -   For each dataset, compute $8$ values: $[\\bar{x}, \\text{SE}(\\bar{x}), \\text{CI}_{\\text{mean,lower}}, \\text{CI}_{\\text{mean,upper}}, s, \\text{SE}(s), \\text{CI}_{\\text{sd,lower}}, \\text{CI}_{\\text{sd,upper}}]$.\n    -   All numeric outputs must be rounded to $6$ decimal places.\n    -   A deterministic random number generator must be used with a specified seed for each case.\n-   **Test Suite**:\n    -   Case 1: Data $x = (1.32, 0.98, 1.15, 0.87, 1.40, 1.05, 0.75, 1.22, 1.08, 0.91, 1.10, 1.27)$, $B = 10000$, Seed $= 12345$.\n    -   Case 2: Data $x = (0.50, -0.20, 1.80, -3.50, 2.70)$, $B = 20000$, Seed $= 24680$.\n    -   Case 3: Data $x = (2.00, 2.00, 2.00, 2.00, 2.00, 2.00)$, $B = 5000$, Seed $= 42$.\n    -   Case 4: Data $x = (9.80, 10.20, 9.90, 10.10, 9.70, 10.00, 10.30, 9.60, 9.95, 30.00)$, $B = 15000$, Seed $= 13579$.\n-   **Output Format**: A single line representing a list of lists with no spaces, e.g., $[[r_{1,1},\\dots,r_{1,8}],\\dots,[r_{4,1},\\dots,r_{4,8}]]$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded, well-posed, and objective.\n1.  **Scientific Soundness**: The definitions for sample mean, unbiased sample standard deviation, standard error, and the nonparametric bootstrap procedure are standard and correct in the field of statistics.\n2.  **Well-Posedness**: The problem is fully specified. For each test case, the data, number of bootstrap replicates, and the random seed are provided. This ensures that a unique, deterministic solution can be computed. The required outputs are clearly defined.\n3.  **Objectivity**: The problem is expressed in precise, mathematical language, free of ambiguity or subjective claims. The test cases, including edge cases like zero variance and an outlier, are appropriate for validating a statistical method.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A complete solution will be provided.\n\n### Solution\n\nThe solution implements the nonparametric bootstrap to quantify the uncertainty in the sample mean, $\\bar{x}$, and the unbiased sample standard deviation, $s$. The core principle of the bootstrap is to approximate the sampling distribution of a statistic by repeatedly sampling from the empirical distribution of the observed data.\n\nThe overall algorithm proceeds as follows for each test case:\n\n1.  **Initialization**: Given a dataset $(x_1, x_2, \\dots, x_n)$ of size $n$, the number of bootstrap replicates $B$, and a random seed, we initialize a pseudo-random number generator (PRNG) for reproducibility.\n\n2.  **Point Estimates from Original Sample**: We first compute the statistics of interest from the original data.\n    -   The sample mean is calculated as $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}$.\n    -   The unbiased sample standard deviation is calculated using Bessel's correction: $s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}}$. This corresponds to `numpy.std` with `ddof=1` (delta degrees of freedom).\n\n3.  **Bootstrap Resampling and Distribution Generation**: We generate $B$ bootstrap resamples. Each resample, denoted $x^*$, is a new dataset of size $n$ formed by drawing samples from the original data $\\{x_1, \\dots, x_n\\}$ with replacement. This process is efficiently implemented by generating a $B \\times n$ matrix of random indices and using these to construct all $B$ resamples in a single vectorized operation.\n\n4.  **Compute Bootstrap Statistics**: For each of the $B$ resamples, we compute the sample mean and the unbiased sample standard deviation. This yields two distributions of size $B$:\n    -   A distribution of bootstrap means: $\\{\\bar{x}^{*}_1, \\bar{x}^{*}_2, \\dots, \\bar{x}^{*}_B\\}$.\n    -   A distribution of bootstrap standard deviations: $\\{s^{*}_1, s^{*}_2, \\dots, s^{*}_B\\}$, where each $s^{*}_j$ is computed with the $n-1$ denominator.\n\n5.  **Estimate Standard Errors**: The standard error of a statistic is the standard deviation of its sampling distribution. We estimate this from our bootstrap distributions.\n    -   The standard error of the mean, $\\text{SE}(\\bar{x})$, is estimated by the sample standard deviation of the bootstrap means: $\\text{SE}(\\bar{x}) \\approx \\sqrt{\\frac{1}{B-1}\\sum_{j=1}^{B} (\\bar{x}^{*}_j - \\overline{\\bar{x}^{*}})^2}$, where $\\overline{\\bar{x}^{*}}$ is the mean of the bootstrap means.\n    -   Similarly, the standard error of the standard deviation, $\\text{SE}(s)$, is estimated by the sample standard deviation of the bootstrap standard deviations.\n\n6.  **Construct Percentile Confidence Intervals**: A two-sided $95\\%$ confidence interval is constructed using the percentile method. This involves finding the empirical quantiles of the bootstrap distribution of the statistic.\n    -   The CI for the mean is $[\\text{quantile}(\\{\\bar{x}^{*}\\}, 0.025), \\text{quantile}(\\{\\bar{x}^{*}\\}, 0.975)]$.\n    -   The CI for the standard deviation is $[\\text{quantile}(\\{s^{*}\\}, 0.025), \\text{quantile}(\\{s^{*}\\}, 0.975)]$.\n    The problem specifies that quantiles are to be computed using linear interpolation, which is the default method in `numpy.quantile`.\n\n7.  **Output Formatting**: The eight resulting values—$\\bar{x}$, $\\text{SE}(\\bar{x})$, the lower and upper bounds of its CI, $s$, $\\text{SE}(s)$, and the lower and upper bounds of its CI—are collected, rounded to $6$ decimal places, and formatted into the precise string format required. This process is repeated for all test cases.", "answer": "```python\nimport numpy as np\n\ndef perform_bootstrap_analysis(data, B, seed):\n    \"\"\"\n    Performs a nonparametric bootstrap analysis for the mean and standard deviation.\n\n    Args:\n        data (tuple or list): The observed data points.\n        B (int): The number of bootstrap replicates.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        list: A list of 8 floats representing the computed statistics, rounded.\n    \"\"\"\n    x = np.array(data)\n    n = len(x)\n\n    # Initialize the random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    # 1. Compute original sample statistics.\n    x_bar = np.mean(x)\n    # The problem defines s for n >= 2. All test cases satisfy this.\n    # ddof=1 ensures division by (n-1) for an unbiased estimator.\n    s = np.std(x, ddof=1) if n > 1 else 0.0\n\n    # 2. & 3. Generate B bootstrap resamples and compute their statistics.\n    # This is a vectorized approach for efficiency.\n    # First, generate all indices for all B resamples.\n    bootstrap_indices = rng.choice(n, size=(B, n), replace=True)\n    # Then create the resamples using advanced indexing.\n    bootstrap_samples = x[bootstrap_indices]\n    \n    # Compute statistics for each bootstrap sample along axis 1 (rows).\n    bootstrap_means = np.mean(bootstrap_samples, axis=1)\n    # Use ddof=1 for the standard deviation of each resample as required.\n    bootstrap_stds = np.std(bootstrap_samples, ddof=1, axis=1) if n > 1 else np.zeros(B)\n\n    # 4. Estimate standard errors.\n    # The SE is the sample standard deviation (ddof=1) of the bootstrap distribution.\n    se_mean = np.std(bootstrap_means, ddof=1)\n    se_std = np.std(bootstrap_stds, ddof=1)\n\n    # 5. Construct 95% percentile confidence intervals.\n    # numpy.quantile uses linear interpolation by default.\n    ci_mean_lower, ci_mean_upper = np.quantile(bootstrap_means, [0.025, 0.975])\n    # Handle the degenerate case where std dev is always 0.\n    if n > 1:\n        ci_std_lower, ci_std_upper = np.quantile(bootstrap_stds, [0.025, 0.975])\n    else:\n        ci_std_lower, ci_std_upper = 0.0, 0.0\n    \n    # 6. Assemble the 8 required results.\n    results = [\n        x_bar, se_mean, ci_mean_lower, ci_mean_upper,\n        s, se_std, ci_std_lower, ci_std_upper\n    ]\n    \n    return results\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"data\": (1.32, 0.98, 1.15, 0.87, 1.40, 1.05, 0.75, 1.22, 1.08, 0.91, 1.10, 1.27),\n            \"B\": 10000,\n            \"seed\": 12345\n        },\n        {\n            \"data\": (0.50, -0.20, 1.80, -3.50, 2.70),\n            \"B\": 20000,\n            \"seed\": 24680\n        },\n        {\n            \"data\": (2.00, 2.00, 2.00, 2.00, 2.00, 2.00),\n            \"B\": 5000,\n            \"seed\": 42\n        },\n        {\n            \"data\": (9.80, 10.20, 9.90, 10.10, 9.70, 10.00, 10.30, 9.60, 9.95, 30.00),\n            \"B\": 15000,\n            \"seed\": 13579\n        }\n    ]\n\n    all_results_formatted = []\n    for case in test_cases:\n        # Special handling for the zero variance case as per the logic.\n        data_array = np.array(case[\"data\"])\n        if np.all(data_array == data_array[0]):\n             x_bar = data_array[0]\n             s = 0.0\n             se_mean = 0.0\n             ci_mean_lower = x_bar\n             ci_mean_upper = x_bar\n             se_std = 0.0\n             ci_std_lower = 0.0\n             ci_std_upper = 0.0\n             result_floats = [x_bar, se_mean, ci_mean_lower, ci_mean_upper, s, se_std, ci_std_lower, ci_std_upper]\n        else:\n            result_floats = perform_bootstrap_analysis(case[\"data\"], case[\"B\"], case[\"seed\"])\n\n        # Format each number to 6 decimal places and create the sublist string.\n        formatted_list = [f\"{x:.6f}\" for x in result_floats]\n        all_results_formatted.append(f\"[{','.join(formatted_list)}]\")\n    \n    # Join all sublist strings into the final required format.\n    final_output = f\"[{','.join(all_results_formatted)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3221310"}]}