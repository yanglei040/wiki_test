{"hands_on_practices": [{"introduction": "Understanding the abstract criteria for a well-posed problem becomes much clearer with a concrete example. This first exercise provides a direct, hands-on look at what happens when Hadamard's third criterion—continuous dependence of the solution on the data—fails. By solving a simple $2 \\times 2$ linear system, you will see how a tiny, almost insignificant perturbation in the input data can cause a dramatic and disproportionately large change in the solution, a phenomenon known as ill-conditioning. This practice is fundamental to appreciating why ill-posed problems are so challenging in scientific computing, where measurement errors are a fact of life.", "problem": "In many scientific and engineering applications, we encounter systems of linear equations of the form $Ax = b$, where we need to find the vector $x$ given a matrix $A$ and a vector $b$. The vector $b$ often represents measurements, which are subject to small errors. An ill-conditioned system is one where small errors in $b$ can lead to large errors in the solution $x$.\n\nConsider the following ill-conditioned linear system:\n$$\nA = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1.001 \\end{pmatrix}\n$$\nLet's first consider the case where the measurement vector is exactly:\n$$\nb_{orig} = \\begin{pmatrix} 2 \\\\ 2.001 \\end{pmatrix}\n$$\nAnd the corresponding solution vector is $x_{orig}$, satisfying $A x_{orig} = b_{orig}$.\n\nNow, suppose a small measurement error occurs, leading to a new, perturbed measurement vector:\n$$\nb_{pert} = \\begin{pmatrix} 2 \\\\ 2.002 \\end{pmatrix}\n$$\nThe new solution vector is $x_{pert}$, which satisfies $A x_{pert} = b_{pert}$.\n\nYour task is to quantify the effect of this small perturbation in the measurement vector. Calculate the magnitude of the change in the solution vector, which is given by the Euclidean norm of the difference, $\\| x_{pert} - x_{orig} \\|_2$. Round your final answer to three significant figures.", "solution": "We solve both linear systems using $x = A^{-1} b$. For a $2 \\times 2$ matrix $A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is $A^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$. Here,\n$$\nA = \\begin{pmatrix} 1 & 1 \\\\ 1 & \\frac{1001}{1000} \\end{pmatrix},\n\\quad \\det(A) = 1 \\cdot \\frac{1001}{1000} - 1 \\cdot 1 = \\frac{1}{1000}.\n$$\nThus,\n$$\nA^{-1} = 1000 \\begin{pmatrix} \\frac{1001}{1000} & -1 \\\\ -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1001 & -1000 \\\\ -1000 & 1000 \\end{pmatrix}.\n$$\nFor $b_{orig} = \\begin{pmatrix} 2 \\\\ \\frac{2001}{1000} \\end{pmatrix}$,\n$$\nx_{orig} = A^{-1} b_{orig} = \\begin{pmatrix} 1001 & -1000 \\\\ -1000 & 1000 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ \\frac{2001}{1000} \\end{pmatrix}\n= \\begin{pmatrix} 1001 \\cdot 2 - 1000 \\cdot \\frac{2001}{1000} \\\\ -1000 \\cdot 2 + 1000 \\cdot \\frac{2001}{1000} \\end{pmatrix}\n= \\begin{pmatrix} 2002 - 2001 \\\\ -2000 + 2001 \\end{pmatrix}\n= \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nFor $b_{pert} = \\begin{pmatrix} 2 \\\\ \\frac{2002}{1000} \\end{pmatrix}$,\n$$\nx_{pert} = A^{-1} b_{pert} = \\begin{pmatrix} 1001 & -1000 \\\\ -1000 & 1000 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ \\frac{2002}{1000} \\end{pmatrix}\n= \\begin{pmatrix} 1001 \\cdot 2 - 1000 \\cdot \\frac{2002}{1000} \\\\ -1000 \\cdot 2 + 1000 \\cdot \\frac{2002}{1000} \\end{pmatrix}\n= \\begin{pmatrix} 2002 - 2002 \\\\ -2000 + 2002 \\end{pmatrix}\n= \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}.\n$$\nThe change in the solution is\n$$\nx_{pert} - x_{orig} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix},\n$$\nand its Euclidean norm is\n$$\n\\|x_{pert} - x_{orig}\\|_{2} = \\sqrt{(-1)^{2} + 1^{2}} = \\sqrt{2}.\n$$\nRounding $\\sqrt{2}$ to three significant figures gives $1.41$.", "answer": "$$\\boxed{1.41}$$", "id": "2197153"}, {"introduction": "We now move from a discrete linear system to a common problem in data analysis: estimating the derivative of a function from noisy samples. While differentiation is a basic operation in calculus, its numerical counterpart is a classic example of an ill-posed problem because it inherently amplifies high-frequency noise. This practice guides you through framing numerical differentiation as an inverse problem and demonstrates why a naive approach fails. You will then implement a practical solution through regularization, using a smoothing filter to tame the noise and exploring the fundamental trade-off between bias (smoothing error) and variance (noise amplification) to find an optimal filter width [@problem_id:3286744].", "problem": "You are given the task of estimating the derivative $f'(x_0)$ from uniformly sampled and noisy observations $y_i = f(x_i) + \\epsilon_i$ on a grid $x_i = x_0 + i \\,\\Delta x$, where the noise $\\epsilon_i$ are independent and identically distributed with zero mean and variance $\\sigma^2$. Using the definitions of well-posedness due to Jacques Hadamard (existence, uniqueness, and continuous dependence on data), frame numerical differentiation as an inverse problem and show why it is ill-posed by demonstrating that high-frequency noise is amplified by differentiation. Then, to stabilize the estimation, consider the following two-step linear estimator for $f'(x_0)$:\n- First smooth the data with a symmetric boxcar (uniform) kernel of width $h$ (so the smoothed function is $f * K_h$, with $K_h$ even and normalized).\n- Then apply the central difference formula to the smoothed signal with step $\\Delta x$.\n\nStarting only from the following foundational facts:\n- The derivative of a convolution satisfies $(f * K_h)' = f' * K_h$.\n- A symmetric, normalized boxcar kernel has second moment $\\mu_2(K_h) = h^2/12$.\n- A moving average of $m \\approx h/\\Delta x$ independent samples reduces variance by a factor of $m$, and differencing two independent samples adds variances.\nderive the leading-order mean squared error at a point,\n$$\n\\operatorname{MSE}(h) \\approx \\text{bias}^2(h) + \\text{var}(h),\n$$\nwhere the bias is governed by the local third derivative $f^{(3)}(x_0)$ and the variance is governed by $\\sigma^2$, $\\Delta x$, and $h$. Show that minimizing this asymptotic expression leads to an optimal width\n$$\nh_\\star \\;=\\; \\left(\\frac{72\\,\\sigma^2}{\\Delta x\\,\\big(f^{(3)}(x_0)\\big)^2}\\right)^{1/5}.\n$$\nIn practice, $h$ cannot be smaller than $\\Delta x$ and should not exceed a user-chosen maximum $H_{\\max}$ determined by domain or trend-scale considerations. Also, when $\\lvert f^{(3)}(x_0)\\rvert$ is extremely small, the bias is negligible and the optimal width should saturate at $H_{\\max}$.\n\nYour program must implement the mapping\n$$\n(\\Delta x,\\;\\sigma,\\;H_{\\max},\\;x_0,\\;f)\\;\\longmapsto\\; h,\n$$\nusing the rule:\n- If $\\lvert f^{(3)}(x_0)\\rvert \\le \\tau$ for a small threshold $\\tau$, set $h = H_{\\max}$.\n- Otherwise compute $h_\\star = \\left(\\frac{72\\,\\sigma^2}{\\Delta x\\,[f^{(3)}(x_0)]^2}\\right)^{1/5}$ and clip to $[\\Delta x,\\;H_{\\max}]$.\n\nAngles are to be interpreted in radians. There are no physical units in this problem. For numerical reporting, express each $h$ as a decimal rounded to six digits after the decimal point.\n\nTest suite. For each case below, compute $h$ using the rule above. Each function $f$ is smooth and its third derivative $f^{(3)}$ is provided analytically:\n- Case A: $f(x) = \\sin(x)$, so $f^{(3)}(x) = -\\cos(x)$. Parameters: $x_0 = 1.0$, $\\Delta x = 0.01$, $\\sigma = 0.02$, $H_{\\max} = 2.0$.\n- Case B: $f(x) = e^{x}$, so $f^{(3)}(x) = e^{x}$. Parameters: $x_0 = 0.0$, $\\Delta x = 0.005$, $\\sigma = 0.05$, $H_{\\max} = 2.0$.\n- Case C: $f(x) = x^3$, so $f^{(3)}(x) = 6$. Parameters: $x_0 = 0.2$, $\\Delta x = 0.1$, $\\sigma = 0.0001$, $H_{\\max} = 2.0$.\n- Case D: $f(x) = \\cos(5x)$, so $f^{(3)}(x) = 125\\,\\sin(5x)$. Parameters: $x_0 = 0.0$, $\\Delta x = 0.01$, $\\sigma = 0.1$, $H_{\\max} = 1.0$.\n\nFinal output format. Your program should produce a single line of output containing a comma-separated list of the four computed $h$ values, in the order A, B, C, D, rounded to six decimal places, and enclosed in square brackets, for example, $[h_A,h_B,h_C,h_D]$.", "solution": "### The Ill-Posed Nature of Numerical Differentiation\n\nThe problem of finding the derivative $f'$ of a function $f$ from a set of data points can be framed as an inverse problem. Let $D$ be the differentiation operator, such that $D(f) = f'$. Our goal is to recover $f'$ given noisy data $y = f + \\epsilon$. Applying the operator $D$ to the data yields $D(y) = D(f) + D(\\epsilon) = f' + \\epsilon'$. The crux of the problem lies in the behavior of the noise term $D(\\epsilon) = \\epsilon'$.\n\nAccording to Jacques Hadamard, a problem is well-posed if a solution exists, is unique, and depends continuously on the input data. Numerical differentiation violates the third criterion. To see this, consider a high-frequency noise component in the data, of the form $\\epsilon(x) = A \\sin(\\omega x)$ for some small amplitude $A$ and large frequency $\\omega$. The magnitude of this perturbation to the function $f$ is at most $A$. However, the perturbation to the derivative $f'$ is $\\epsilon'(x) = A \\omega \\cos(\\omega x)$, which has a magnitude of $A\\omega$. By choosing a sufficiently large frequency $\\omega$, the error in the derivative, $A\\omega$, can be made arbitrarily large, even for an infinitesimally small error $A$ in the data.\n\nThis shows that the solution (the derivative) does not depend continuously on the input data. Small, high-frequency perturbations in the input can be amplified to arbitrarily large perturbations in the output. Therefore, numerical differentiation is an ill-posed problem. To obtain a stable and meaningful solution, a regularization technique is required, which typically involves smoothing the data before differentiation.\n\n### Analysis of the Regularized Estimator\n\nThe proposed estimator regularizes the problem by first smoothing the data and then applying a finite difference formula. The two steps are:\n1.  Smooth the noisy data $y$ using a symmetric boxcar (uniform) kernel $K_h$ of width $h$. The smoothed signal is $\\tilde{y} = y * K_h$.\n2.  Apply the central difference formula with step size $\\Delta x$ to the smoothed signal $\\tilde{y}$ to approximate the derivative at $x_0$.\n\nThe estimator for $f'(x_0)$ is thus given by:\n$$\n\\hat{f}'(x_0) = \\frac{\\tilde{y}(x_0 + \\Delta x) - \\tilde{y}(x_0 - \\Delta x)}{2 \\Delta x} = \\frac{(y * K_h)(x_0 + \\Delta x) - (y * K_h)(x_0 - \\Delta x)}{2 \\Delta x}\n$$\nThe performance of this estimator is characterized by its Mean Squared Error ($\\operatorname{MSE}$), which can be decomposed into the sum of the squared bias and the variance:\n$$\n\\operatorname{MSE}(h) = \\text{bias}^2(h) + \\text{var}(h) = \\left( E[\\hat{f}'(x_0)] - f'(x_0) \\right)^2 + \\operatorname{Var}(\\hat{f}'(x_0))\n$$\n\n#### Bias Derivation\nThe bias is the systematic error of the estimator. Since the noise $\\epsilon_i$ has zero mean ($E[\\epsilon_i]=0$), the expected value of the estimator is found by applying it to the noise-free function $f(x)$:\n$$\nE[\\hat{f}'(x_0)] = \\frac{(f * K_h)(x_0 + \\Delta x) - (f * K_h)(x_0 - \\Delta x)}{2 \\Delta x}\n$$\nThis is the central difference approximation of the derivative of the function $g(x) = (f * K_h)(x)$ at $x=x_0$. For a sufficiently small $\\Delta x$, this expression is a good approximation of $g'(x_0)$. Using the given fact that $(f * K_h)' = f' * K_h$, we have $g'(x_0) = (f' * K_h)(x_0)$. The error introduced by the central difference formula itself is of order $O((\\Delta x)^2)$ and is considered of higher order compared to the smoothing bias, so for the leading-order bias we can approximate $E[\\hat{f}'(x_0)] \\approx (f' * K_h)(x_0)$.\n\nThe bias is then the difference between the expected smoothed derivative and the true derivative:\n$$\n\\text{bias}(h) = E[\\hat{f}'(x_0)] - f'(x_0) \\approx (f' * K_h)(x_0) - f'(x_0)\n$$\nTo analyze this, we expand $f'$ in a Taylor series around $x_0$ inside the convolution integral:\n$$\n(f' * K_h)(x_0) = \\int_{-\\infty}^{\\infty} f'(x_0 - u) K_h(u) du \\approx \\int_{-\\infty}^{\\infty} \\left[ f'(x_0) - u f''(x_0) + \\frac{u^2}{2} f^{(3)}(x_0) \\right] K_h(u) du\n$$\nSince the kernel $K_h$ is normalized ($\\int K_h(u) du = 1$) and symmetric ($\\int u K_h(u) du = 0$), this simplifies to:\n$$\n(f' * K_h)(x_0) \\approx f'(x_0) \\int K_h(u) du - f''(x_0) \\int u K_h(u) du + \\frac{f^{(3)}(x_0)}{2} \\int u^2 K_h(u) du\n$$\n$$\n(f' * K_h)(x_0) \\approx f'(x_0) + \\frac{f^{(3)}(x_0)}{2} \\mu_2(K_h)\n$$\nwhere $\\mu_2(K_h)$ is the second moment of the kernel. Using the provided fact $\\mu_2(K_h) = h^2/12$, the leading-order bias is:\n$$\n\\text{bias}(h) \\approx \\frac{h^2}{24} f^{(3)}(x_0)\n$$\nThe squared bias is therefore:\n$$\n\\text{bias}^2(h) \\approx \\frac{h^4}{576} \\left(f^{(3)}(x_0)\\right)^2\n$$\n\n#### Variance Derivation\nThe variance of the estimator captures the effect of the random noise $\\epsilon_i$.\n$$\n\\operatorname{Var}(\\hat{f}'(x_0)) = \\operatorname{Var}\\left( \\frac{(\\epsilon * K_h)(x_0 + \\Delta x) - (\\epsilon * K_h)(x_0 - \\Delta x)}{2 \\Delta x} \\right)\n$$\nLet $\\tilde{\\epsilon}(x) = (\\epsilon * K_h)(x)$ be the smoothed noise. The variance is:\n$$\n\\operatorname{Var}(\\hat{f}'(x_0)) = \\frac{1}{4(\\Delta x)^2} \\operatorname{Var}\\left( \\tilde{\\epsilon}(x_0 + \\Delta x) - \\tilde{\\epsilon}(x_0 - \\Delta x) \\right)\n$$\nUsing the provided hint, smoothing with a moving average over $m \\approx h/\\Delta x$ samples reduces the variance of the noise from $\\sigma^2$ to $\\sigma^2/m = \\sigma^2 \\Delta x / h$.\nThus, $\\operatorname{Var}(\\tilde{\\epsilon}(x)) \\approx \\sigma^2 \\Delta x / h$.\nThe second hint states that differencing two independent samples adds their variances. For the purpose of a leading-order analysis, we assume the smoothed noise values at $x_0+\\Delta x$ and $x_0-\\Delta x$ are approximately independent, which is reasonable if $h \\gg \\Delta x$. The variance of their difference is then the sum of their variances:\n$$\n\\operatorname{Var}\\left( \\tilde{\\epsilon}(x_0 + \\Delta x) - \\tilde{\\epsilon}(x_0 - \\Delta x) \\right) \\approx \\operatorname{Var}(\\tilde{\\epsilon}(x_0+\\Delta x)) + \\operatorname{Var}(\\tilde{\\epsilon}(x_0-\\Delta x)) \\approx 2 \\frac{\\sigma^2 \\Delta x}{h}\n$$\nSubstituting this back into the expression for the variance of the estimator:\n$$\n\\operatorname{var}(h) \\approx \\frac{1}{4(\\Delta x)^2} \\left( \\frac{2 \\sigma^2 \\Delta x}{h} \\right) = \\frac{\\sigma^2}{2 h \\Delta x}\n$$\n\n#### MSE Minimization and Optimal Width\nCombining the squared bias and variance gives the asymptotic Mean Squared Error:\n$$\n\\operatorname{MSE}(h) \\approx \\frac{h^4}{576} \\left(f^{(3)}(x_0)\\right)^2 + \\frac{\\sigma^2}{2 h \\Delta x}\n$$\nTo find the optimal kernel width $h_\\star$ that minimizes this $\\operatorname{MSE}$, we differentiate with respect to $h$ and set the result to zero:\n$$\n\\frac{d}{dh}\\operatorname{MSE}(h) = \\frac{4h^3}{576} \\left(f^{(3)}(x_0)\\right)^2 - \\frac{\\sigma^2}{2 h^2 \\Delta x} = 0\n$$\n$$\n\\frac{h^3}{144} \\left(f^{(3)}(x_0)\\right)^2 = \\frac{\\sigma^2}{2 h^2 \\Delta x}\n$$\nSolving for $h^5$:\n$$\nh^5 = \\frac{144 \\sigma^2}{2 \\Delta x \\left(f^{(3)}(x_0)\\right)^2} = \\frac{72 \\sigma^2}{\\Delta x \\left(f^{(3)}(x_0)\\right)^2}\n$$\nThis gives the optimal width $h_\\star$:\n$$\nh_\\star = \\left(\\frac{72\\,\\sigma^2}{\\Delta x\\,\\big(f^{(3)}(x_0)\\big)^2}\\right)^{1/5}\n$$\nThis derivation confirms the formula provided in the problem statement. The algorithm for implementation will compute this value, handling the case where $f^{(3)}(x_0)$ is near zero and ensuring the resulting $h$ is within the physical and practical bounds $[\\Delta x, H_{\\max}]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the optimal smoothing width h for numerical differentiation\n    based on a bias-variance trade-off analysis.\n    \"\"\"\n    \n    # A small threshold to handle cases where the third derivative is zero or very close to it.\n    # This prevents division by zero and implements the saturation logic described.\n    tau = 1e-9\n\n    # Define the test cases from the problem statement.\n    # Each case is a dictionary for clarity.\n    test_cases = [\n        # Case A\n        {\n            \"f_name\": \"sin(x)\",\n            \"f3\": lambda x: -np.cos(x),\n            \"x0\": 1.0,\n            \"dx\": 0.01,\n            \"sigma\": 0.02,\n            \"Hmax\": 2.0,\n        },\n        # Case B\n        {\n            \"f_name\": \"exp(x)\",\n            \"f3\": lambda x: np.exp(x),\n            \"x0\": 0.0,\n            \"dx\": 0.005,\n            \"sigma\": 0.05,\n            \"Hmax\": 2.0,\n        },\n        # Case C\n        {\n            \"f_name\": \"x^3\",\n            \"f3\": lambda x: 6.0,\n            \"x0\": 0.2,\n            \"dx\": 0.1,\n            \"sigma\": 0.0001,\n            \"Hmax\": 2.0,\n        },\n        # Case D\n        {\n            \"f_name\": \"cos(5x)\",\n            \"f3\": lambda x: 125 * np.sin(5 * x),\n            \"x0\": 0.0,\n            \"dx\": 0.01,\n            \"sigma\": 0.1,\n            \"Hmax\": 1.0,\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        dx = case[\"dx\"]\n        sigma = case[\"sigma\"]\n        Hmax = case[\"Hmax\"]\n        \n        # Calculate the third derivative at x0\n        f3_val = case[\"f3\"](case[\"x0\"])\n        \n        # Apply the rule specified in the problem statement\n        if abs(f3_val) <= tau:\n            # If the third derivative is negligible, bias is negligible.\n            # We choose the maximum smoothing to minimize variance.\n            h = Hmax\n        else:\n            # Calculate the optimal width h_star based on the derived formula\n            numerator = 72 * sigma**2\n            denominator = dx * f3_val**2\n            h_star = (numerator / denominator)**(1/5)\n            \n            # Clip the result to the valid range [dx, Hmax]\n            # h cannot be smaller than the grid spacing, and should not exceed Hmax.\n            h = np.clip(h_star, dx, Hmax)\n            \n        results.append(h)\n\n    # Final print statement in the exact required format.\n    # Each h value is formatted to six decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3286744"}, {"introduction": "Building on the concept of regularization, this final practice explores a more powerful and widely used technique: the smoothing spline. Instead of simply interpolating noisy data points, which leads to erratic results, a smoothing spline finds a function that balances fidelity to the data with a penalty on its \"roughness,\" typically its second derivative. Your task is to implement this method and, more importantly, to quantify how the smoothing parameter $\\lambda$ controls this trade-off [@problem_id:3286808]. By computing diagnostics like spectral noise amplification and mean squared error, you will gain a deeper, quantitative understanding of how regularization restores well-posedness to an otherwise unstable problem.", "problem": "You are asked to make precise, quantify, and computationally demonstrate the role of the smoothing parameter in a discrete analogue of the classical smoothing spline, and to compare it against raw interpolation when the data are corrupted by additive noise. Work in one dimension on a uniform grid and express all angles in radians.\n\nConsider a uniform grid with $N$ points on $[0,1]$, defined by $x_i = i h$ for $i \\in \\{0,1,\\dots,N-1\\}$ with spacing $h = 1/(N-1)$. Let $f^{\\star}(x) = \\sin(2\\pi x) + \\tfrac{1}{2}\\cos(5\\pi x)$ be the unknown ground truth signal and suppose we observe noisy samples $y_i = f^{\\star}(x_i) + \\epsilon_i$, where $\\epsilon_i$ are independent zero-mean Gaussian perturbations with standard deviation $\\sigma$. Use a deterministic pseudorandom sequence by seeding the generator with $0$ and drawing a length-$N$ vector, so that a single fixed noise realization is used for each $(N,\\sigma)$ pair.\n\nDefine the discrete analogue of the smoothing spline estimator $f_{\\lambda}$ as the vector in $\\mathbb{R}^N$ that minimizes the penalized least-squares functional\n$$\nJ_{\\lambda}(f) = \\sum_{i=0}^{N-1} (f_i - y_i)^2 + \\lambda \\lVert D_2 f \\rVert_2^2,\n$$\nwhere $D_2$ is the standard second-difference operator approximating the second derivative with central differences on the interior nodes. Concretely, for each interior index $i \\in \\{1,2,\\dots,N-2\\}$,\n$$\n(D_2 f)_{i-1} = \\frac{f_{i-1} - 2 f_{i} + f_{i+1}}{h^2},\n$$\nso that $D_2 \\in \\mathbb{R}^{(N-2)\\times N}$ and $L = D_2^{\\top} D_2 \\in \\mathbb{R}^{N\\times N}$. Show that the minimizer satisfies the normal equations\n$$\n(I + \\lambda L) f_{\\lambda} = y,\n$$\nwhere $I \\in \\mathbb{R}^{N\\times N}$ is the identity matrix and $y \\in \\mathbb{R}^N$ stacks the $y_i$. Interpret the case $\\lambda = 0$ as the raw interpolation at the grid nodes (an exact fit $f_0 = y$ to the noisy data). For $\\lambda &gt; 0$, define the linear smoothing operator $S_{\\lambda} = (I + \\lambda L)^{-1}$ so that $f_{\\lambda} = S_{\\lambda} y$.\n\nYour computational task is to quantify how the smoothing parameter $\\lambda$ controls stability with respect to noise in the sense of Hadamard well-posedness. In particular, examine stability of estimated curvature via the operator $D_2 S_{\\lambda}$. For each test case specified below, compute three scalar diagnostics:\n\n- The spectral noise amplification for curvature, given by the induced $2$-norm\n$$\na_{\\mathrm{spec}}(\\lambda) = \\lVert D_2 S_{\\lambda} \\rVert_2,\n$$\nwhich measures the worst-case amplification of noise in the estimated second derivative.\n\n- The empirical curvature amplification on the fixed noise realization,\n$$\na_{\\mathrm{emp}}(\\lambda) = \\frac{\\lVert D_2 S_{\\lambda} \\epsilon \\rVert_2}{\\lVert \\epsilon \\rVert_2},\n$$\nwhere $\\epsilon$ is the deterministic noise vector used to form $y$.\n\n- The mean squared error of the reconstructed signal,\n$$\n\\mathrm{MSE}(\\lambda) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\bigl(f_{\\lambda,i} - f^{\\star}(x_i)\\bigr)^2.\n$$\n\nUse the following test suite of parameter triples $(N,\\lambda,\\sigma)$, with all angles in radians:\n\n- Case $1$: $(N,\\lambda,\\sigma) = (50, 0, 0.05)$.\n- Case $2$: $(N,\\lambda,\\sigma) = (50, 0.01, 0.05)$.\n- Case $3$: $(N,\\lambda,\\sigma) = (50, 0.1, 0.05)$.\n- Case $4$: $(N,\\lambda,\\sigma) = (50, 1.0, 0.05)$.\n- Case $5$: $(N,\\lambda,\\sigma) = (200, 0, 0.05)$.\n- Case $6$: $(N,\\lambda,\\sigma) = (200, 0.1, 0.05)$.\n\nYour program must, for each case, construct $x_i$, form $y_i = f^{\\star}(x_i) + \\epsilon_i$ using a single fixed noise vector drawn from a Gaussian distribution with standard deviation $\\sigma$ and pseudorandom generator seeded with $0$, build $D_2$, compute $S_{\\lambda}$, and then compute the three diagnostics $(a_{\\mathrm{spec}}, a_{\\mathrm{emp}}, \\mathrm{MSE})$. All computations are dimensionless. Report each of the three diagnostics as a float rounded to six decimal places.\n\nFinal output format: Your program should produce a single line containing a list of length six, one entry per test case. Each entry must itself be a three-element list in the order $[a_{\\mathrm{spec}}, a_{\\mathrm{emp}}, \\mathrm{MSE}]$. For example, the output must look like\n$[\\,[a_{\\mathrm{spec}}^{(1)},a_{\\mathrm{emp}}^{(1)},\\mathrm{MSE}^{(1)}],\\dots,[a_{\\mathrm{spec}}^{(6)},a_{\\mathrm{emp}}^{(6)},\\mathrm{MSE}^{(6)}]\\,]$\nwith every float rounded to six decimals.", "solution": "The core of the problem lies in minimizing the penalized least-squares functional for a vector $f \\in \\mathbb{R}^N$:\n$$\nJ_{\\lambda}(f) = \\sum_{i=0}^{N-1} (f_i - y_i)^2 + \\lambda \\lVert D_2 f \\rVert_2^2\n$$\nwhere $y \\in \\mathbb{R}^N$ is the vector of noisy observations, $\\lambda \\geq 0$ is the smoothing parameter, and $D_2 \\in \\mathbb{R}^{(N-2)\\times N}$ is the second-difference matrix operator.\n\nFirst, we derive the normal equations for the minimizer $f_{\\lambda}$. We can write the functional in matrix-vector notation:\n$$\nJ_{\\lambda}(f) = (f - y)^{\\top}(f - y) + \\lambda (D_2 f)^{\\top}(D_2 f)\n$$\nExpanding the terms, we get:\n$$\nJ_{\\lambda}(f) = f^{\\top}f - 2y^{\\top}f + y^{\\top}y + \\lambda f^{\\top}D_2^{\\top}D_2 f\n$$\nTo find the minimum, we compute the gradient of this quadratic form with respect to $f$ and set it to zero. Let $L = D_2^{\\top}D_2$. The matrix $L$ is symmetric.\n$$\n\\nabla_f J_{\\lambda}(f) = 2f - 2y + 2\\lambda L f\n$$\nSetting the gradient to zero gives the necessary condition for a minimum:\n$$\n2f_{\\lambda} - 2y + 2\\lambda L f_{\\lambda} = 0\n$$\n$$\n(I + \\lambda L) f_{\\lambda} = y\n$$\nThis confirms the normal equations provided in the problem statement. The matrix $(I + \\lambda L)$ is symmetric and positive definite for $\\lambda > 0$ (since $I$ is positive definite and $L$ is positive semi-definite), guaranteeing a unique solution $f_{\\lambda}$. For the case $\\lambda = 0$, the equation becomes $I f_0 = y$, which means $f_0 = y$. This corresponds to a raw interpolation of the noisy data, as no smoothing is applied. The solution is thus given by $f_{\\lambda} = (I + \\lambda L)^{-1} y$. We define the linear smoothing operator as $S_{\\lambda} = (I + \\lambda L)^{-1}$.\n\nThe computational procedure for each test case $(N, \\lambda, \\sigma)$ is as follows:\n\n1.  **Grid and Signal Generation**: A uniform grid of $N$ points, $x_i = i h$ for $i \\in \\{0, 1, \\dots, N-1\\}$, is created on the interval $[0, 1]$, with spacing $h = 1/(N-1)$. The ground truth signal vector $f^{\\star} \\in \\mathbb{R}^N$ is computed, with components $f^{\\star}_i = \\sin(2\\pi x_i) + \\frac{1}{2}\\cos(5\\pi x_i)$.\n\n2.  **Noise Generation**: A single deterministic noise vector $\\epsilon \\in \\mathbb{R}^N$ is generated for each unique $(N, \\sigma)$ pair. This is accomplished by seeding a pseudorandom number generator with $0$ and drawing $N$ samples from a Gaussian distribution with mean $0$ and standard deviation $\\sigma$. The noisy observations are then synthesized as $y = f^{\\star} + \\epsilon$.\n\n3.  **Operator Construction**: The second-difference matrix $D_2 \\in \\mathbb{R}^{(N-2) \\times N}$ is constructed. The problem states $(D_2 f)_{i-1} = (f_{i-1} - 2f_i + f_{i+1})/h^2$ for $i \\in \\{1, \\dots, N-2\\}$. This means row $j$ (where $j=i-1$) of the operator acts on columns $j$, $j+1$, and $j+2$. The penalty matrix $L \\in \\mathbb{R}^{N \\times N}$ is then computed as $L = D_2^{\\top}D_2$.\n\n4.  **Smoother Calculation**: The smoothing operator $S_{\\lambda} \\in \\mathbb{R}^{N \\times N}$ is calculated. If $\\lambda = 0$, $S_0$ is the identity matrix $I$. If $\\lambda > 0$, $S_{\\lambda}$ is computed by inverting the matrix $(I + \\lambda L)$.\n\n5.  **Diagnostic Computation**: Three diagnostic metrics are computed using the generated operators and vectors:\n    -   The spectral noise amplification, $a_{\\mathrm{spec}}(\\lambda) = \\lVert D_2 S_{\\lambda} \\rVert_2$, is the induced $2$-norm (or spectral norm) of the matrix $D_2 S_{\\lambda}$. It quantifies the worst-case amplification of input perturbations in the estimated curvature.\n    -   The empirical curvature amplification, $a_{\\mathrm{emp}}(\\lambda) = \\frac{\\lVert D_2 S_{\\lambda} \\epsilon \\rVert_2}{\\lVert \\epsilon \\rVert_2}$, measures the amplification factor specifically for the generated noise realization $\\epsilon$.\n    -   The Mean Squared Error, $\\mathrm{MSE}(\\lambda) = \\frac{1}{N} \\lVert f_{\\lambda} - f^{\\star} \\rVert_2^2$, quantifies the accuracy of the reconstructed signal $f_{\\lambda} = S_{\\lambda}y$ against the ground truth $f^{\\star}$.\n\nThis entire procedure is executed for each of the six specified test cases. The case $\\lambda=0$ demonstrates the ill-posed nature of differentiating noisy data, characterized by large values of $a_{\\mathrm{spec}}$ and $a_{\\mathrm{emp}}$. The cases with $\\lambda > 0$ illustrate the regularizing effect of the smoothing spline, which stabilizes the differentiation process (reducing the amplification factors) at the cost of introducing a bias in the signal estimate, a trade-off reflected in the MSE.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes diagnostics for a discrete smoothing spline problem across several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, lambda, sigma)\n        (50, 0, 0.05),\n        (50, 0.01, 0.05),\n        (50, 0.1, 0.05),\n        (50, 1.0, 0.05),\n        (200, 0, 0.05),\n        (200, 0.1, 0.05),\n    ]\n\n    all_results = []\n    \n    # Cache for noise vectors to ensure the same realization is used for each (N, sigma) pair.\n    noise_cache = {}\n\n    for N, lam, sigma in test_cases:\n        # 1. Grid and Signal Generation\n        h = 1.0 / (N - 1)\n        x = np.linspace(0, 1, N)\n        f_star = np.sin(2 * np.pi * x) + 0.5 * np.cos(5 * np.pi * x)\n\n        # 2. Noise Generation (with caching)\n        if (N, sigma) in noise_cache:\n            epsilon = noise_cache[(N, sigma)]\n        else:\n            np.random.seed(0)\n            epsilon = sigma * np.random.randn(N)\n            noise_cache[(N, sigma)] = epsilon\n\n        # 3. Noisy Data\n        y = f_star + epsilon\n\n        # 4. Operator Construction\n        # D2 is (N-2) x N\n        D2 = np.zeros((N - 2, N))\n        for i in range(N - 2):\n            D2[i, i:i+3] = [1.0, -2.0, 1.0]\n        D2 /= (h**2)\n        \n        I = np.identity(N)\n\n        # 5. Smoother Calculation\n        if lam == 0:\n            S_lambda = I\n        else:\n            L = D2.T @ D2\n            # The matrix to invert is I + lambda * L\n            matrix_to_invert = I + lam * L\n            S_lambda = np.linalg.inv(matrix_to_invert)\n\n        # 6. Diagnostic Computation\n        \n        # Spectral noise amplification\n        a_spec = np.linalg.norm(D2 @ S_lambda, 2)\n\n        # Empirical curvature amplification\n        # Use the pre-computed noise vector epsilon\n        norm_epsilon = np.linalg.norm(epsilon, 2)\n        # Handle the theoretical case of zero noise\n        if norm_epsilon > 0:\n            a_emp = np.linalg.norm(D2 @ S_lambda @ epsilon, 2) / norm_epsilon\n        else:\n            a_emp = 0.0\n\n        # Mean Squared Error\n        f_lambda = S_lambda @ y\n        mse = np.mean((f_lambda - f_star)**2)\n\n        # Append formatted results for the current case\n        all_results.append([\n            f\"{a_spec:.6f}\",\n            f\"{a_emp:.6f}\",\n            f\"{mse:.6f}\"\n        ])\n\n    # Format the final output string as a list of lists.\n    output_str = \"[\" + \",\".join([f\"[{','.join(res)}]\" for res in all_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3286808"}]}