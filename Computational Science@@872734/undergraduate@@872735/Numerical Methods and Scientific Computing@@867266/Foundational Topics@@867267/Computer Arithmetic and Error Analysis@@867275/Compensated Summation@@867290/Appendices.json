{"hands_on_practices": [{"introduction": "Before diving into code, let's build our intuition with a conceptual exercise. This problem uses a carefully chosen, hypothetical sequence of numbers to reveal the hidden pitfalls of standard floating-point arithmetic, such as catastrophic cancellation and swamping. By tracing the calculations by hand, you will see not only why a naive sum can fail spectacularly but also discover a scenario where even the standard Kahan compensated summation algorithm reaches its limits [@problem_id:3214532].", "problem": "Consider summing three real numbers in the Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) binary64 format, using rounding to nearest, ties to even. Let the sequence be $a_{1} = 1$, $a_{2} = 10^{100}$, and $a_{3} = -10^{100}$. The exact sum in real arithmetic is $a_{1} + a_{2} + a_{3}$. In floating-point arithmetic, define naive summation to be left-to-right accumulation of a running total $s$ by the recurrence $s \\leftarrow \\operatorname{fl}(s + a_{k})$, where $\\operatorname{fl}(\\cdot)$ denotes one correctly rounded floating-point operation in binary64. Define Kahan compensated summation to be the variant that maintains a compensation variable $c$ and updates according to $y \\leftarrow a_{k} - c$, $t \\leftarrow \\operatorname{fl}(s + y)$, $c \\leftarrow \\operatorname{fl}(\\operatorname{fl}(t - s) - y)$, $s \\leftarrow t$.\n\nUsing only the foundational definition of floating-point rounding to nearest in binary64 and the concept of unit in the last place, analyze the effect of rounding on the partial sums for naive accumulation in the orders $(a_{1}, a_{2}, a_{3})$ and $(a_{2}, a_{3}, a_{1})$, and for Kahan compensated summation in the order $(a_{1}, a_{2}, a_{3})$. Then, determine the final value returned by Kahan compensated summation for the order $(a_{1}, a_{2}, a_{3})$. No rounding of the reported answer is required, and no physical units apply. Provide your answer as a single real number.", "solution": "The user wants to find the result of summing a sequence of three numbers, $a_{1} = 1$, $a_{2} = 10^{100}$, and $a_{3} = -10^{100}$, using the Kahan compensated summation algorithm in IEEE $754$ binary64 arithmetic.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- **Numbers:** $a_{1} = 1$, $a_{2} = 10^{100}$, $a_{3} = -10^{100}$.\n- **Arithmetic:** IEEE $754$ binary64 format, rounding to nearest, ties to even.\n- **Floating-point operation:** $\\operatorname{fl}(\\cdot)$ denotes one correctly rounded floating-point operation.\n- **Naive Summation:** $s \\leftarrow \\operatorname{fl}(s + a_{k})$ with $s$ initialized to $0$.\n- **Kahan Compensated Summation:** Initialize $s=0, c=0$. For each $a_k$:\n  $y \\leftarrow a_{k} - c$\n  $t \\leftarrow \\operatorname{fl}(s + y)$\n  $c \\leftarrow \\operatorname{fl}(\\operatorname{fl}(t - s) - y)$\n  $s \\leftarrow t$\n  The problem description for the update to $y$ does not explicitly use $\\operatorname{fl}(\\cdot)$, but in the context of implementing numerical algorithms, this assignment implies a floating-point operation. Thus, we interpret it as $y \\leftarrow \\operatorname{fl}(a_k - c)$.\n- **Task:** Analyze naive summation for orders $(a_1, a_2, a_3)$ and $(a_2, a_3, a_1)$, and determine the final value from Kahan summation for the order $(a_1, a_2, a_3)$. The exact sum is $1 + 10^{100} + (-10^{100}) = 1$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is firmly rooted in numerical analysis and the specifics of floating-point arithmetic as defined by the IEEE $754$ standard. It is a classic example used to illustrate the properties of summation algorithms.\n- **Well-Posed:** The problem is well-defined. The inputs, arithmetic rules, and algorithms are specified, leading to a unique, deterministic result.\n- **Objective:** The problem is stated in precise, formal language, free of ambiguity or subjective claims.\n\nThe problem is deemed valid as it adheres to all criteria. It is a sound and formal question within the domain of scientific computing.\n\n**Solution**\n\nFirst, let's establish the floating-point representations of the input numbers in binary64 format. The binary64 format has a $53$-bit significand precision (including the implicit leading bit).\n- $a_1 = 1$: This number is exactly representable in binary64 as $1.0 \\times 2^0$. Let's denote its representation as $\\hat{a}_1 = 1$.\n- $a_2 = 10^{100}$: This number is not exactly representable. It must be rounded to the nearest binary64 number. Let's denote this representation as $\\hat{a}_2 = \\operatorname{fl}(10^{100})$. The value of $\\hat{a}_2$ is very large. We can estimate its magnitude: $\\log_2(10^{100}) = 100 \\log_2(10) \\approx 100 \\times 3.3219 \\approx 332.19$. So, $\\hat{a}_2$ is approximately $m \\times 2^{332}$ for some significand $m \\in [1, 2)$.\n- $a_3 = -10^{100}$: Its representation is $\\hat{a}_3 = \\operatorname{fl}(-10^{100})$. Due to the symmetry of rounding to nearest, $\\hat{a}_3 = -\\operatorname{fl}(10^{100}) = -\\hat{a}_2$.\n\nA key concept in this problem is **swamping**, which occurs when adding or subtracting two floating-point numbers of widely different magnitudes. If $|x| \\gg |y|$, the result of $\\operatorname{fl}(x+y)$ may be equal to $x$, effectively \"swamping\" $y$. This happens when $y$ is smaller than the spacing of representable numbers around $x$. The unit in the last place, $\\mathrm{ulp}(x)$, for $x = m \\times 2^E$ ($1\\le m < 2$), is $2^{E-52}$. For $\\hat{a}_2 \\approx 2^{332}$, the ulp is approximately $2^{332-52} = 2^{280}$, a number immensely larger than $1$. Therefore, adding $1$ to $\\hat{a}_2$ will have no effect on its floating-point representation.\n\n**Analysis of Naive Summation**\n- **Order $(a_1, a_2, a_3)$:**\n  1. $s_0 = 0$.\n  2. $s_1 = \\operatorname{fl}(s_0 + a_1) = \\operatorname{fl}(0 + 1) = 1$.\n  3. $s_2 = \\operatorname{fl}(s_1 + a_2) = \\operatorname{fl}(1 + 10^{100}) = \\operatorname{fl}(1 + \\hat{a}_2) = \\hat{a}_2$ (due to swamping).\n  4. $s_3 = \\operatorname{fl}(s_2 + a_3) = \\operatorname{fl}(\\hat{a}_2 + (-\\hat{a}_2)) = 0$.\n  The result is $0$.\n\n- **Order $(a_2, a_3, a_1)$:**\n  1. $s_0 = 0$.\n  2. $s_1 = \\operatorname{fl}(s_0 + a_2) = \\operatorname{fl}(0 + 10^{100}) = \\hat{a}_2$.\n  3. $s_2 = \\operatorname{fl}(s_1 + a_3) = \\operatorname{fl}(\\hat{a}_2 + (-\\hat{a}_2)) = 0$.\n  4. $s_3 = \\operatorname{fl}(s_2 + a_1) = \\operatorname{fl}(0 + 1) = 1$.\n  The result is $1$, which is the correct sum. This illustrates the importance of summation order.\n\n**Analysis of Kahan Compensated Summation for Order $(a_1, a_2, a_3)$**\nWe will trace the state variables $s$ (sum) and $c$ (compensation) through each step of the algorithm.\n\n**Initialization:**\n- $s = 0$\n- $c = 0$\n\n**Step 1: Add $a_1 = 1$**\n- $y \\leftarrow \\operatorname{fl}(a_1 - c) = \\operatorname{fl}(1 - 0) = 1$.\n- $t \\leftarrow \\operatorname{fl}(s + y) = \\operatorname{fl}(0 + 1) = 1$.\n- $c \\leftarrow \\operatorname{fl}(\\operatorname{fl}(t - s) - y) = \\operatorname{fl}(\\operatorname{fl}(1 - 0) - 1) = \\operatorname{fl}(1 - 1) = 0$.\n- $s \\leftarrow t = 1$.\n**State after Step 1:** $s = 1$, $c = 0$. The sum is correct and there is no compensation needed.\n\n**Step 2: Add $a_2 = 10^{100}$**\n- $y \\leftarrow \\operatorname{fl}(a_2 - c) = \\operatorname{fl}(10^{100} - 0) = \\hat{a}_2$.\n- $t \\leftarrow \\operatorname{fl}(s + y) = \\operatorname{fl}(1 + \\hat{a}_2)$. As established, the magnitude of $\\hat{a}_2$ is so large that adding $1$ is lost due to swamping. The result is rounded to $\\hat{a}_2$. So, $t = \\hat{a}_2$.\n- $c \\leftarrow \\operatorname{fl}(\\operatorname{fl}(t - s) - y)$. We have $t=\\hat{a}_2$, $s=1$, and $y=\\hat{a}_2$.\n  - Inner operation: $\\operatorname{fl}(t - s) = \\operatorname{fl}(\\hat{a}_2 - 1)$. Again, due to swamping, subtracting $1$ has no effect on the floating-point result. So, $\\operatorname{fl}(\\hat{a}_2 - 1) = \\hat{a}_2$.\n  - Outer operation: $c \\leftarrow \\operatorname{fl}(\\hat{a}_2 - y) = \\operatorname{fl}(\\hat{a}_2 - \\hat{a}_2) = 0$.\n- $s \\leftarrow t = \\hat{a}_2$.\n**State after Step 2:** $s = \\hat{a}_2$, $c = 0$. The Kahan algorithm has failed to capture the lost term $1$ in the compensation variable $c$. This failure occurs because the condition $|s| \\ge |y|$ for the correct calculation of the error term is violated.\n\n**Step 3: Add $a_3 = -10^{100}$**\n- $y \\leftarrow \\operatorname{fl}(a_3 - c) = \\operatorname{fl}(-10^{100} - 0) = \\hat{a}_3 = -\\hat{a}_2$.\n- $t \\leftarrow \\operatorname{fl}(s + y) = \\operatorname{fl}(\\hat{a}_2 + (-\\hat{a}_2)) = 0$. This subtraction is exact as floating-point numbers have exact negatives.\n- $c \\leftarrow \\operatorname{fl}(\\operatorname{fl}(t - s) - y) = \\operatorname{fl}(\\operatorname{fl}(0 - \\hat{a}_2) - (-\\hat{a}_2))$.\n  - Inner operation: $\\operatorname{fl}(0 - \\hat{a}_2) = -\\hat{a}_2$ (exact).\n  - Outer operation: $c \\leftarrow \\operatorname{fl}((-\\hat{a}_2) - (-\\hat{a}_2)) = \\operatorname{fl}(-\\hat{a}_2 + \\hat{a}_2) = 0$.\n- $s \\leftarrow t = 0$.\n**Final State:** $s = 0$, $c = 0$.\n\nThe final value returned by the Kahan summation algorithm, which is the final value of $s$, is $0$. This example demonstrates a scenario where the standard Kahan algorithm fails to improve upon naive summation due to swamping that occurs when a running sum is much smaller than the term being added.", "answer": "$$\\boxed{0}$$", "id": "3214532"}, {"introduction": "Now, let's translate theory into practice by implementing and testing the Kahan summation algorithm. This exercise tasks you with summing the alternating harmonic series, which converges to the natural logarithm of 2, providing a concrete 'ground truth' for comparison. You will quantify the accuracy improvement of compensated summation and also explore how the order of operations—summing forwards versus backwards—provides another crucial tool for managing numerical error [@problem_id:3214610].", "problem": "You are asked to study numerical summation of a conditionally convergent series and quantify the impact of compensation on floating-point rounding errors. Consider the alternating harmonic series defined by\n$$\nS = \\sum_{k=1}^{\\infty} (-1)^{k+1}\\frac{1}{k}.\n$$\nIt is a well-tested fact that the Taylor series for the natural logarithm satisfies\n$$\n\\ln(1+x) = \\sum_{k=1}^{\\infty} (-1)^{k+1}\\frac{x^k}{k}\n$$\nfor $|x| \\leq 1$ with $x \\neq -1$, implying that\n$$\n\\sum_{k=1}^{\\infty} (-1)^{k+1}\\frac{1}{k} = \\ln(2).\n$$\nThe alternating series test further guarantees that for the partial sum\n$$\nS_N = \\sum_{k=1}^{N} (-1)^{k+1}\\frac{1}{k},\n$$\nthe truncation error $|S - S_N|$ is bounded above by the magnitude of the next term:\n$$\n|S - S_N| \\leq \\frac{1}{N+1}.\n$$\n\nFrom the perspective of numerical computation, let floating-point arithmetic follow the standard rounding model under the Institute of Electrical and Electronics Engineers (IEEE) $754$ double precision: for addition and subtraction,\n$$\n\\operatorname{fl}(a \\oplus b) = (a \\oplus b)(1 + \\delta), \\quad |\\delta| \\leq \\varepsilon_{\\text{mach}},\n$$\nwhere $\\varepsilon_{\\text{mach}}$ is the machine epsilon and $\\operatorname{fl}(\\cdot)$ denotes the floating-point result. In long summations, many low-order bits can be lost when adding numbers of vastly different magnitudes or opposite signs, causing accumulated rounding errors. A principled mitigation is compensated summation, which maintains a running compensation for the low-order part lost to rounding at each step and feeds it back in the next addition.\n\nYour tasks are:\n\n- Implement two numerical summation procedures for computing $S_N$:\n  1. A naive forward summation that accumulates the terms in the specified order without compensation.\n  2. A compensated summation that uses a one-term compensation variable, updating it at each addition to capture and re-introduce the low-order part lost to rounding.\n\n- Support two accumulation orders:\n  - Forward order: iterate $k = 1,2,\\dots,N$.\n  - Reverse order: iterate $k = N,N-1,\\dots,1$.\n\n- For each partial sum, compute:\n  1. The absolute error $E_{\\text{naive}} = |\\operatorname{fl}(S_N^{\\text{naive}}) - \\ln(2)|$,\n  2. The absolute error $E_{\\text{comp}} = |\\operatorname{fl}(S_N^{\\text{comp}}) - \\ln(2)|$,\n  3. The normalized error $E_{\\text{naive}} / \\left(\\frac{1}{N+1}\\right)$,\n  4. The normalized error $E_{\\text{comp}} / \\left(\\frac{1}{N+1}\\right)$,\n  5. A boolean flag indicating whether compensation improved accuracy, i.e., $E_{\\text{comp}} \\leq E_{\\text{naive}}$.\n\nDesign and implement your program so that it uses double precision ($\\text{IEEE } 754$ double) arithmetic (Python’s built-in $float$) and produces results for the following test suite:\n- Case $1$: $N = 1$, forward order.\n- Case $2$: $N = 10$, forward order.\n- Case $3$: $N = 1000$, forward order.\n- Case $4$: $N = 200000$, forward order.\n- Case $5$: $N = 200000$, reverse order.\n\nThe test suite covers:\n- A boundary case ($N = 1$) where few terms are summed.\n- A small case ($N = 10$) illustrating early convergence behavior.\n- A moderate case ($N = 1000$) where truncation error is small but rounding begins to matter.\n- A large case ($N = 200000$) where rounding errors can accumulate.\n- Sensitivity to summation order at large $N$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, emit exactly five items in the following order:\n$$\n[E_{\\text{naive}}, E_{\\text{comp}}, \\; E_{\\text{naive}}/(1/(N+1)), \\; E_{\\text{comp}}/(1/(N+1)), \\; (E_{\\text{comp}} \\leq E_{\\text{naive}})]\n$$\nConcatenate the results of all five test cases into a single flat list. For example, the output format must be\n$$\n[\\text{case1\\_naive\\_err},\\text{case1\\_comp\\_err},\\text{case1\\_naive\\_norm},\\text{case1\\_comp\\_norm},\\text{case1\\_improved},\\dots,\\text{case5\\_improved}]\n$$\nwith no extra characters or lines. No physical units or angles are involved in this problem, so no unit conversions are required. All numeric values should be printed in standard decimal notation.", "solution": "The problem requires a numerical investigation into the effects of compensated summation on the accuracy of computing the partial sum of the alternating harmonic series, $S_N = \\sum_{k=1}^{N} (-1)^{k+1}\\frac{1}{k}$. The exact value of the infinite series is known to be $S = \\ln(2)$. The total error in a numerical computation of $S_N$, which is $|\\operatorname{fl}(S_N) - S|$, is the sum of two components: the truncation error, $|S_N - S|$, and the rounding error, $|\\operatorname{fl}(S_N) - S_N|$. The problem provides an upper bound for the truncation error, $|S - S_N| \\leq \\frac{1}{N+1}$. Our objective is to implement and compare two summation algorithms, naive and compensated, and evaluate their performance under forward and reverse summation orders, thereby quantifying the impact of the rounding error component. All computations are performed using IEEE $754$ double-precision floating-point arithmetic.\n\nA naive summation algorithm computes the partial sum $S_N$ by sequentially adding terms in a given order. For a sequence of terms $\\{x_k\\}_{k=1}^N$, the sum $S_N = \\sum_{k=1}^N x_k$ is computed via the recurrence $s_k = s_{k-1} + x_k$, with $s_0=0$. In floating-point arithmetic, this becomes $\\operatorname{fl}(s_k) = \\operatorname{fl}(\\operatorname{fl}(s_{k-1}) + x_k)$. According to the standard model, $\\operatorname{fl}(a+b) = (a+b)(1+\\delta)$ where $|\\delta| \\leq \\varepsilon_{\\text{mach}}$. At each step, a small rounding error is introduced. When adding a term $x_k$ to a running sum $s_{k-1}$, if $|s_{k-1}| \\gg |x_k|$, the lower-order bits of $x_k$ are shifted out of the mantissa and lost. This loss of precision accumulates over many operations, leading to a potentially significant rounding error in the final result.\n\nTo mitigate this accumulation of rounding error, we employ compensated summation, specifically the Kahan summation algorithm. This method introduces a compensation variable, $c$, which tracks the error introduced at each addition. Let $s$ be the running sum and $c$ be the compensation, both initialized to $0$. To add a new term $x$, the algorithm proceeds as follows:\n$1$. A corrected term $y$ is computed by subtracting the previous error from the new term: $y = x - c$.\n$2$. The corrected term is added to the sum: $t = s + y$. Due to finite precision, this operation may be inexact.\n$3$. The new error introduced in the previous step is calculated as $c_{\\text{new}} = (t - s) - y$. In exact arithmetic, this would be zero. However, in floating-point arithmetic, $(t - s)$ recovers the high-order part of $y$ that was successfully added to $s$, so subtracting $y$ from it yields the negative of the low-order part of $y$ that was lost.\n$4$. The sum is updated to $s = t$, and the new compensation term $c$ is set to $c_{\\text{new}}$ for the next iteration.\nThis procedure effectively \"carries\" the lost precision from one step to the next, reintroducing it into the summation and dramatically reducing the final accumulated rounding error.\n\nThe order of summation can also affect the accuracy of the naive method. For a series with terms of decreasing magnitude, such as the alternating harmonic series, there are two natural orders:\n- **Forward order**: Summing from $k=1$ to $N$. The terms $x_k = (-1)^{k+1}/k$ decrease in magnitude. The running sum $s_k$ quickly approaches its final value, while the terms being added become progressively smaller. This leads to the classic scenario where $|s_k| \\gg |x_{k+1}|$, causing significant rounding errors.\n- **Reverse order**: Summing from $k=N$ to $1$. This strategy adds the smallest-magnitude terms first. The running sum remains small initially, which allows subsequent additions of small terms to be performed with higher relative accuracy. This heuristic often improves the accuracy of naive summation.\n\nFor the specified test cases, we anticipate the following behavior. For small $N$ (e.g., $N=1, 10$), truncation error dominates, and the difference between summation methods will be negligible. For large $N$ (e.g., $N=200000$), rounding error becomes significant. We expect the error of the naive forward sum, $E_{\\text{naive}}$, to be considerably larger than the truncation error bound. In contrast, the compensated sum error, $E_{\\text{comp}}$, should remain close to the truncation error bound, demonstrating the algorithm's effectiveness. We also expect the naive reverse sum to be more accurate than the naive forward sum. The normalized error, $E / (\\frac{1}{N+1})$, provides a measure of how much the total error exceeds the theoretical truncation error bound; a value close to $1$ indicates that rounding error is well-controlled.\n\nThe analysis proceeds by implementing these algorithms and applying them to the given test suite. For each case, we calculate the absolute errors $E_{\\text{naive}}$ and $E_{\\text{comp}}$ with respect to the high-precision value of $\\ln(2)$, the normalized errors, and a boolean flag indicating if compensation improved the result.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef naive_sum(N, order):\n    \"\"\"\n    Computes the partial sum of the alternating harmonic series using naive summation.\n\n    Args:\n        N (int): The number of terms to sum.\n        order (str): The order of summation, 'forward' or 'reverse'.\n\n    Returns:\n        float: The computed sum.\n    \"\"\"\n    s = 0.0\n    k_range = range(1, N + 1) if order == 'forward' else range(N, 0, -1)\n    \n    for k in k_range:\n        term = ((-1.0)**(k + 1)) / k\n        s += term\n        \n    return s\n\ndef compensated_sum(N, order):\n    \"\"\"\n    Computes the partial sum of the alternating harmonic series using\n    Kahan compensated summation.\n\n    Args:\n        N (int): The number of terms to sum.\n        order (str): The order of summation, 'forward' or 'reverse'.\n\n    Returns:\n        float: The computed sum.\n    \"\"\"\n    s = 0.0  # The running sum\n    c = 0.0  # A running compensation for the lost low-order bits\n    k_range = range(1, N + 1) if order == 'forward' else range(N, 0, -1)\n    \n    for k in k_range:\n        term = ((-1.0)**(k + 1)) / k\n        y = term - c\n        t = s + y\n        # (t - s) is the high-order part of y that was successfully added.\n        # (t - s) - y isolates the low-order part of y that was lost.\n        # c stores the negative of this lost part.\n        c = (t - s) - y\n        s = t\n        \n    return s\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1, 'forward'),\n        (10, 'forward'),\n        (1000, 'forward'),\n        (200000, 'forward'),\n        (200000, 'reverse'),\n    ]\n\n    # Use numpy's high-precision value for ln(2) as the \"exact\" value.\n    ln2 = np.log(2.0)\n\n    # List to store all results in a flat structure.\n    all_results = []\n\n    for N, order in test_cases:\n        # 1. Compute sums\n        s_naive = naive_sum(N, order)\n        s_comp = compensated_sum(N, order)\n\n        # 2. Compute absolute errors\n        err_naive = abs(s_naive - ln2)\n        err_comp = abs(s_comp - ln2)\n\n        # 3. Compute truncation error bound\n        trunc_err_bound = 1.0 / (N + 1)\n\n        # 4. Compute normalized errors\n        norm_err_naive = err_naive / trunc_err_bound\n        norm_err_comp = err_comp / trunc_err_bound\n\n        # 5. Determine if compensation improved accuracy\n        improved = err_comp = err_naive\n\n        # Append the five required metrics for this test case\n        all_results.extend([\n            err_naive,\n            err_comp,\n            norm_err_naive,\n            norm_err_comp,\n            improved\n        ])\n\n    # Final print statement in the exact required format.\n    # The boolean `True`/`False` is converted to string 'True'/'False'.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n# Execute the solver.\nsolve()\n```", "id": "3214610"}, {"introduction": "Compensated summation is not just for simple sums; it is a powerful technique that can stabilize more complex numerical algorithms. In this practice, you will apply this concept to the real-world problem of calculating statistical variance, a cornerstone of data analysis. By integrating a compensation term into the one-pass Welford algorithm, you will see how to build more robust and reliable scientific computing tools [@problem_id:3214603].", "problem": "You are given a finite sequence of real numbers and asked to compute the population variance in floating-point arithmetic using three methods: a two-pass method, a one-pass online update commonly attributed to Welford, and a modified one-pass update that integrates compensated summation in the accumulation of the second central moment. The goal is to evaluate and compare the numerical stability of these methods by quantifying the absolute error relative to a high-precision reference computed from the exact floating-point inputs. The population variance is defined from first principles as follows: for a finite sequence $\\{x_i\\}_{i=1}^n$ with mean $\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i$, the population variance is $\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2$.\n\nBase your reasoning on the following foundational facts:\n- $n$-term summation in floating-point arithmetic may incur rounding error due to the finite precision model of real-number representation. In the widely used Institute of Electrical and Electronics Engineers (IEEE) 754 floating-point arithmetic, basic operations such as addition and multiplication are rounded to a nearby representable value.\n- The definition of the population variance $\\sigma^2$ requires the mean $\\mu$ and the sum of squared deviations $\\sum_{i=1}^n (x_i - \\mu)^2$. Numerical cancellation can occur in the subtraction $x_i - \\mu$ when $x_i$ and $\\mu$ are close in magnitude.\n\nImplement the following in a single program:\n- A two-pass population variance computation that first computes the mean using a straightforward left-to-right summation and then computes the sum of squared deviations in a second pass using a straightforward left-to-right summation. In both passes, use the machine’s standard floating-point arithmetic without any special stabilization techniques.\n- A one-pass online population variance computation using the classical recurrence credited to Welford that updates the running mean and a running accumulator of the second central moment at each new sample, without any special stabilization techniques.\n- A one-pass online population variance computation that uses the same structure as Welford’s update but integrates compensated summation (a correction term) in the accumulation of the second central moment contributions to reduce rounding error in the running sum.\n\nFor all three methods, compute the absolute error relative to a high-precision reference variance obtained by evaluating the same inputs exactly as they are represented in binary floating-point, but using arbitrary-precision decimal arithmetic for the variance formula. To ensure the reference corresponds to the exact floating-point inputs, convert each floating-point input to a high-precision decimal exactly before computing the variance; do not reinterpret the numbers as rational or symbolic values other than their exact floating-point representations.\n\nUse the following test suite of input sequences, chosen to exercise cancellation, scale disparity, symmetry, and boundary behavior:\n- Test case $1$: $[10^{16}, 1.0, 1.0, 1.0, 10^{16} + 1.0]$.\n- Test case $2$: $[1.0, 1.0, 1.0, 1.0, 1.0]$.\n- Test case $3$: $[1.0 + 10^{-10}, 1.0 - 10^{-10}, 1.0 + 2 \\cdot 10^{-10}, 1.0 - 2 \\cdot 10^{-10}, 1.0, 1.0]$.\n- Test case $4$: $[-10^{9}, 10^{9}, -3.0, 3.0, -2.0, 2.0, -1.0, 1.0]$.\n- Test case $5$: $[123456789.0]$.\n\nFor each test case, compute three floating-point results:\n- The absolute error of the two-pass population variance.\n- The absolute error of the one-pass Welford population variance without compensation.\n- The absolute error of the one-pass Welford population variance with compensated summation integrated into the second moment accumulator.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by test case and method, i.e., $[e_{1, \\text{two-pass}}, e_{1, \\text{welford}}, e_{1, \\text{comp}}, e_{2, \\text{two-pass}}, e_{2, \\text{welford}}, e_{2, \\text{comp}}, \\ldots, e_{5, \\text{comp}}]$, where each $e_{k, \\cdot}$ is a floating-point absolute error. No physical units are involved. Angles are not involved. Express all answers as floating-point numbers.", "solution": "The problem requires the implementation and numerical comparison of three distinct algorithms for computing the population variance of a sequence of floating-point numbers. The comparison is to be based on the absolute error of each method relative to a high-precision reference computation. The core of the problem lies in understanding and demonstrating the varying degrees of numerical stability inherent in these algorithms when executed in finite-precision arithmetic.\n\nThe population variance, $\\sigma^2$, for a finite sequence of $n$ numbers $\\{x_i\\}_{i=1}^n$ with mean $\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i$ is defined as the average of the squared deviations from the mean:\n$$\n\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2\n$$\nIn floating-point arithmetic, the direct evaluation of this formula and its variants can lead to significant inaccuracies due to rounding errors and catastrophic cancellation. We will analyze three methods of increasing numerical robustness.\n\n**1. The Two-Pass Algorithm**\n\nThis is the most straightforward transcription of the definition. It proceeds in two distinct passes over the data.\n\n- **Pass 1: Compute the mean.** The mean $\\mu$ is calculated by summing all elements and dividing by the count $n$:\n  $$\n  \\mu = \\frac{1}{n} \\sum_{i=1}^n x_i\n  $$\n- **Pass 2: Compute the sum of squared deviations.** Using the mean $\\mu$ from the first pass, the sum of squared deviations, $S_2$, is computed:\n  $$\n  S_2 = \\sum_{i=1}^n (x_i - \\mu)^2\n  $$\n- **Final variance:** The variance is then $\\sigma^2 = S_2 / n$.\n\nThe primary numerical vulnerability of this method is **catastrophic cancellation**. If the data points $x_i$ are clustered closely together, their values will be very close to the mean $\\mu$. The subtraction $x_i - \\mu$ will then involve two nearly equal numbers, leading to a result with a large relative error and a significant loss of precision. The subsequent squaring and summation can exacerbate this initial error. Additionally, the summation in the first pass can suffer from rounding errors if the dataset contains numbers of vastly different magnitudes, leading to an inaccurate mean $\\mu$ which in turn corrupts the entire second pass.\n\n**2. Welford's Online Algorithm**\n\nThis algorithm computes the variance in a single pass, updating the running mean and the sum of squared deviations as each new data point arrives. This \"online\" property is advantageous for streaming data. The algorithm is based on the following recurrence relations.\n\nLet $M_k$ be the running mean of the first $k$ elements, and let $S_k$ be the running sum of squared deviations for the first $k$ elements, i.e., $S_k = \\sum_{i=1}^k (x_i - M_k)^2$. The initial conditions for the first element $x_1$ are $M_1 = x_1$ and $S_1 = 0$. For subsequent elements $k = 2, 3, \\ldots, n$, the updates are:\n$$\nM_k = M_{k-1} + \\frac{x_k - M_{k-1}}{k}\n$$\n$$\nS_k = S_{k-1} + (x_k - M_{k-1})(x_k - M_k)\n$$\nAfter processing all $n$ points, the population variance is $\\sigma^2 = S_n / n$.\n\nWelford's algorithm is generally more numerically stable than the two-pass method. It mitigates catastrophic cancellation by computing deviations from the *running* mean rather than the final mean. The term $x_k - M_{k-1}$ is less likely to be a subtraction of two nearly-equal large numbers compared to $x_i - \\mu$ in the two-pass method, especially in cases of data drift. However, the summation for $S_k$ can still suffer from accumulated rounding errors, particularly if it involves adding a very small update term to a large running sum $S_{k-1}$.\n\n**3. Welford's Algorithm with Compensated Summation**\n\nTo further improve the accuracy of the sum $S_k$ in Welford's algorithm, we can employ a compensated summation technique, such as the Kahan summation algorithm. This method tracks the round-off error that occurs at each step of the summation and incorporates it as a correction into the next step.\n\nThe update for the running sum $S_k$ is modified as follows, introducing a correction term $c_k$ initialized to $c_0 = 0$. For each step $k$:\n1. Calculate the new term to be added to the sum of squares:\n   $$\n   \\text{term}_k = (x_k - M_{k-1})(x_k - M_k)\n   $$\n2. Incorporate the correction from the previous step:\n   $$\n   y = \\text{term}_k - c_{k-1}\n   $$\n3. Add this corrected term to the running sum $S_{k-1}$. This addition itself introduces a new error:\n   $$\n   t = S_{k-1} + y\n   $$\n4. Calculate the new correction term, which captures the low-order bits lost in the previous addition:\n   $$\n   c_k = (t - S_{k-1}) - y\n   $$\n5. Update the sum:\n   $$\n   S_k = t\n   $$\nThe update for the mean $M_k$ remains the same. This compensated summation for $S_k$ effectively recovers precision that would otherwise be lost in the floating-point additions, leading to a highly accurate accumulation of the sum of squared deviations. This method is exceptionally robust and provides accurate results even for ill-conditioned datasets.\n\n**Reference Computation and Error Metric**\n\nTo objectively assess the accuracy of these three methods, a high-precision reference variance, $\\sigma^2_{\\text{reference}}$, is computed. This is achieved by:\n1. Taking the input floating-point numbers (which are binary, e.g., IEEE 754 `double` precision).\n2. Converting each number to its exact decimal representation using arbitrary-precision arithmetic. Python's `decimal` module is well-suited for this.\n3. Performing all calculations for the population variance formula $\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2$ using this high-precision arithmetic, with a sufficiently large number of digits (e.g., $100$) to render rounding errors negligible.\n\nThe performance of each implemented algorithm is then quantified by the absolute error, $e$, against this reference value:\n$$\ne = |\\sigma^2_{\\text{computed}} - \\sigma^2_{\\text{reference}}|\n$$\nThis provides a direct measure of the accuracy loss for each computational method under the specific conditions of each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef reference_variance(data: list[float]) - float:\n    \"\"\"Computes population variance using high-precision decimal arithmetic.\"\"\"\n    if not data:\n        return 0.0\n\n    getcontext().prec = 100  # Set high precision for reference calculation\n    n = len(data)\n    d_data = [Decimal(x) for x in data]\n    \n    # Handle single-element case\n    if n == 1:\n        return 0.0\n\n    mean = sum(d_data) / Decimal(n)\n    sum_sq_dev = sum((x - mean)**2 for x in d_data)\n    variance = sum_sq_dev / Decimal(n)\n    \n    return float(variance)\n\ndef two_pass_variance(x: np.ndarray) - np.float64:\n    \"\"\"Computes population variance using a standard two-pass algorithm.\"\"\"\n    n = x.size\n    if n = 1:\n        return np.float64(0.0)\n    \n    # Pass 1: Compute mean\n    mean = np.sum(x) / n\n    \n    # Pass 2: Compute sum of squared deviations\n    sum_sq_dev = np.sum((x - mean)**2)\n    \n    return sum_sq_dev / n\n\ndef welford_variance(x: np.ndarray) - np.float64:\n    \"\"\"Computes population variance using Welford's one-pass online algorithm.\"\"\"\n    n = x.size\n    if n = 1:\n        return np.float64(0.0)\n        \n    M = np.float64(0.0)  # Running mean\n    S = np.float64(0.0)  # Running sum of squared deviations\n    \n    for k, val in enumerate(x, 1):\n        M_old = M\n        M += (val - M_old) / k\n        S += (val - M_old) * (val - M)\n        \n    return S / n\n\ndef compensated_welford_variance(x: np.ndarray) - np.float64:\n    \"\"\"\n    Computes population variance using Welford's algorithm with compensated summation\n    for the sum of squared deviations.\n    \"\"\"\n    n = x.size\n    if n = 1:\n        return np.float64(0.0)\n        \n    M = np.float64(0.0)  # Running mean\n    S = np.float64(0.0)  # Running sum of squared deviations\n    c = np.float64(0.0)  # Compensation for S\n    \n    for k, val in enumerate(x, 1):\n        # Update mean\n        M_old = M\n        M += (val - M) / k\n        \n        # Update sum of squares with compensation\n        term = (val - M_old) * (val - M)\n        y = term - c\n        t = S + y\n        c = (t - S) - y\n        S = t\n        \n    return S / n\n\ndef solve():\n    \"\"\"\n    Runs the variance comparison for the specified test cases and prints the\n    absolute errors in the required format.\n    \"\"\"\n    test_cases = [\n        [10**16, 1.0, 1.0, 1.0, 10**16 + 1.0],\n        [1.0, 1.0, 1.0, 1.0, 1.0],\n        [1.0 + 1e-10, 1.0 - 1e-10, 1.0 + 2e-10, 1.0 - 2e-10, 1.0, 1.0],\n        [-10**9, 10**9, -3.0, 3.0, -2.0, 2.0, -1.0, 1.0],\n        [123456789.0],\n    ]\n\n    results = []\n    for case in test_cases:\n        # It's crucial to first convert to np.float64 to simulate the\n        # exact inputs as they would be stored in standard double precision.\n        np_case = np.array(case, dtype=np.float64)\n        \n        # The reference must be computed from the list of python floats,\n        # which will be identical to np.float64 in value.\n        ref_var = reference_variance(case)\n\n        # Compute variance with each method\n        var_2pass = two_pass_variance(np_case)\n        var_welford = welford_variance(np_case)\n        var_comp = compensated_welford_variance(np_case)\n\n        # Calculate and store absolute errors\n        results.append(abs(var_2pass - ref_var))\n        results.append(abs(var_welford - ref_var))\n        results.append(abs(var_comp - ref_var))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3214603"}]}