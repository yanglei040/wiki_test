{"hands_on_practices": [{"introduction": "At its core, a floating-point number is just a pattern of bits interpreted according to a specific set of rules. This first exercise ([@problem_id:1948832]) puts you in the role of the processor, tasking you with manually decoding a 32-bit hexadecimal value into its corresponding decimal number. By working through the IEEE 754 standard step-by-step, you will gain a concrete understanding of how the sign, exponent, and fraction fields come together to represent a number.", "problem": "In a 32-bit microprocessor architecture, a floating-point unit register contains the hexadecimal value $0xC1E80000$. This value is to be interpreted according to the Institute of Electrical and Electronics Engineers (IEEE) 754 standard for single-precision floating-point numbers.\n\nThe 32-bit single-precision format is structured as follows:\n- A 1-bit sign field (S), located at bit 31 (the most significant bit).\n- An 8-bit biased exponent field (E), located at bits 30 through 23.\n- A 23-bit fraction field (F), located at bits 22 through 0.\n\nFor a normalized number (where the exponent field is not all zeros or all ones), the decimal value is given by the formula $N = (-1)^S \\times (1.F)_2 \\times 2^{(E - \\text{bias})}$, where $(1.F)_2$ represents the implicit leading bit followed by the fraction bits, interpreted as a binary number. The exponent bias for single-precision is 127.\n\nDetermine the decimal value of the number represented by the bit pattern $0xC1E80000$.", "solution": "According to IEEE 754 single-precision, the fields are: sign bit $S$ (bit $31$), exponent field $E$ (bits $30$ through $23$), and fraction field $F$ (bits $22$ through $0$). The value is $N = (-1)^{S} \\times (1.F)_{2} \\times 2^{(E - 127)}$ for a normalized number.\n\nThe hexadecimal value $0x\\text{C1E80000}$ in binary is grouped as $1100\\,0001\\,1110\\,1000\\,0000\\,0000\\,0000\\,0000$. The sign bit is $S = 1$ (negative). The exponent bits (bits $30$ through $23$) are $(10000011)_{2}$, so\n$$\nE = 1 \\cdot 2^{7} + 0 \\cdot 2^{6} + \\cdots + 0 \\cdot 2^{2} + 1 \\cdot 2^{1} + 1 \\cdot 2^{0} = 128 + 2 + 1 = 131.\n$$\nSince $E \\neq 0$ and $E \\neq 255$, the number is normalized, and the unbiased exponent is\n$$\ne = E - 127 = 131 - 127 = 4.\n$$\nThe fraction field $F$ is $(11010000000000000000000)_2$. The significand is\n$$\n(1.F)_{2} = 1 + 1 \\cdot 2^{-1} + 1 \\cdot 2^{-2} + 0 \\cdot 2^{-3} + 1 \\cdot 2^{-4} = 1 + \\frac{1}{2} + \\frac{1}{4} + \\frac{1}{16} = 1 + \\frac{8+4+1}{16} = 1 + \\frac{13}{16} = \\frac{29}{16}.\n$$\nTherefore, the represented value is\n$$\nN = (-1)^{1} \\times \\frac{29}{16} \\times 2^{4} = - \\frac{29}{16} \\times 16 = -29.\n$$", "answer": "$$\\boxed{-29}$$", "id": "1948832"}, {"introduction": "While floating-point formats can represent an enormous range of numbers, they cannot represent all numbersâ€”not even all integers. This thought experiment challenges the common misconception that all integers can be stored perfectly. By reasoning from first principles about the format's fixed number of significand bits ([@problem_id:3210700]), you will discover the smallest positive integer that cannot be perfectly represented in a standard single-precision format, revealing the concept of gaps between representable numbers.", "problem": "A simulation system records nonnegative integer event counts in the Institute of Electrical and Electronics Engineers (IEEE) 754 single-precision binary floating-point format, commonly referred to as binary32. A binary32 number uses $1$ sign bit, an $8$-bit exponent field with bias $127$, and a $23$-bit fraction field; normalized values have an implicit leading $1$ in the significand, yielding $24$ bits of precision. Using only the core definition of normalized binary floating-point representation and first-principles reasoning about binary expansions, determine the smallest positive integer $n$ that is not exactly representable in binary32. Then, let $x$ denote the least binary32-representable real number strictly greater than $n$; define the gap $g$ as $g = x - n$. Compute $g$ from first principles by analyzing the spacing of representable values at the magnitude of $n$. Provide your final answers for $n$ and $g$ as exact values. No rounding is required, and no units are involved.", "solution": "The problem requires us to determine the smallest positive integer $n$ that is not exactly representable in the IEEE 754 single-precision (binary32) format, and then to compute the gap $g$ between $n$ and the next larger representable number. This analysis will be performed from first principles based on the structure of binary32 floating-point numbers.\n\nA number in the binary32 format is represented as $v = (-1)^s \\times M \\times 2^E$, where $s$ is the sign bit, $M$ is the significand, and $E$ is the exponent. The problem states that we are dealing with nonnegative integers, so the sign bit $s$ is $0$.\n\nThe binary32 format specifies an $8$-bit exponent field and a $23$-bit fraction field. For normalized numbers, the significand $M$ has an implicit leading bit of $1$. It takes the form $M = (1.f)_{2}$, where $f$ is the $23$-bit fraction. This means the significand is composed of $1$ integer bit and $23$ fractional bits, giving a total precision of $p=24$ bits. The fraction $f$ can be written as a sum:\n$$f = \\sum_{i=1}^{23} d_i 2^{-i}$$\nwhere each $d_i$ is either $0$ or $1$. The significand is therefore $M = 1 + f$. The exponent $E$ is calculated from the $8$-bit exponent field value $e$ using a bias of $127$, so $E = e - 127$. For normalized numbers, $1 \\le e \\le 254$, which corresponds to an exponent range of $-126 \\le E \\le 127$.\n\nAn integer is exactly representable if it can be written in the form $v = (1.f)_2 \\times 2^E$ using the allowed fraction $f$ and exponent $E$. This is possible if and only if the binary representation of the integer can be expressed with no more than $24$ significant bits.\n\nLet's examine integers to find the smallest one that is not representable.\nAny integer $I$ can be written in binary as $(b_k b_{k-1} \\dots b_1 b_0)_2$. To express this in normalized floating-point form, we write it as $(1.b_{k-1} \\dots b_0)_2 \\times 2^k$. The significand is $(1.b_{k-1} \\dots b_0)_2$. The fractional part has $k$ bits. For this to be representable in binary32, the number of fractional bits must be at most $23$. This condition is $k \\le 23$. The total number of significant bits in the integer $I$ is $k+1$. So, if $k+1 \\le 24$, the integer is representable.\n\nThis means all integers that can be written with at most $24$ significant bits are exactly representable. The largest such integer is one whose binary representation consists of $24$ ones:\n$$ 2^{24} - 1 = \\sum_{i=0}^{23} 2^i = (11\\dots1)_{2} \\quad (24 \\text{ ones}) $$\nThis number can be written as $(1.11\\dots1)_2 \\times 2^{23}$ (with $23$ ones after the binary point), which is perfectly representable since the fraction part fits within the $23$ available bits.\n\nNow, consider the next integer, $2^{24}$. In binary, this is a $1$ followed by $24$ zeros: $(100\\dots0)_{2}$. This number has $25$ bits in its standard integer representation. However, in scientific notation, it is $(1.0)_2 \\times 2^{24}$.\n- The significand is $M=1.0$. The fraction part is $f=0$. This is representable.\n- The exponent is $E=24$. The encoded exponent would be $e = E + 127 = 24 + 127 = 151$. Since $1 \\le 151 \\le 254$, this is a valid exponent for a normalized number.\nThus, the integer $2^{24}$ is exactly representable in binary32.\n\nAll integers from $1$ up to $2^{24}$ are therefore exactly representable.\n\nLet's examine the next integer, $n = 2^{24} + 1$. Its value is $16,777,216 + 1 = 16,777,217$. The binary representation of $n$ is:\n$$ n = (1000000000000000000000001)_2 $$\nThis consists of a $1$ at position $24$ and a $1$ at position $0$. To represent this in normalized floating-point form, we must write it as:\n$$ n = (1.000000000000000000000001)_2 \\times 2^{24} $$\nThe significand required is $M = 1 + 2^{-24}$. The fraction part required is $f = 2^{-24}$.\nThe $23$-bit fraction field of the binary32 format can only represent fractions of the form $\\sum_{i=1}^{23} d_i 2^{-i}$. The smallest non-zero term that can be formed is $2^{-23}$. It is impossible to form the term $2^{-24}$. Consequently, the significand $1+2^{-24}$ cannot be created, and the integer $n=2^{24}+1$ is not exactly representable. This makes it the smallest positive integer that cannot be exactly represented.\nSo, $n = 2^{24} + 1 = 16,777,217$.\n\nNext, we must find the least binary32-representable real number $x$ strictly greater than $n$, and compute the gap $g=x-n$.\nNumbers with a magnitude close to $n = 2^{24}+1$ fall into the range where the exponent is $E=24$. For any number $v$ in the interval $[2^{24}, 2^{25})$, the exponent is $E=24$.\nThe representable numbers in this range are of the form:\n$$ v = (1.f_1f_2\\dots f_{23})_2 \\times 2^{24} $$\nThe value of the fraction part, $f = \\sum_{i=1}^{23} d_i 2^{-i}$, can be expressed as $k \\cdot 2^{-23}$ for an integer $k$ where $0 \\le k \\le 2^{23}-1$.\nSubstituting this into the expression for $v$:\n$$ v_k = (1 + k \\cdot 2^{-23}) \\times 2^{24} = 2^{24} + k \\cdot 2^{24-23} = 2^{24} + 2k $$\nThis shows that for integers $k \\ge 0$, the representable numbers at this magnitude are $2^{24}$, $2^{24}+2$, $2^{24}+4$, and so on. The spacing between consecutive representable numbers is $2$. This spacing is known as the Unit in the Last Place (ULP) for this range.\n\nOur non-representable integer is $n = 2^{24}+1$. We can place it between two consecutive representable numbers:\n$$ 2^{24}  2^{24}+1  2^{24}+2 $$\nThe problem asks for $x$, the least binary32-representable real number strictly greater than $n$. From the inequality above, this number must be $x = 2^{24}+2$.\n\nFinally, we compute the gap $g$ as defined by $g = x - n$:\n$$ g = (2^{24}+2) - (2^{24}+1) = 1 $$\n\nThe two values are $n=2^{24}+1 = 16,777,217$ and $g=1$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 16777217  1 \\end{pmatrix}}\n$$", "id": "3210700"}, {"introduction": "Understanding the structure of floating-point numbers is crucial for writing robust numerical software, as algebraically identical formulas can produce vastly different results. This practical coding exercise ([@problem_id:3210660]) demonstrates the classic numerical pitfall of subtractive cancellation. By implementing and comparing two methods for computing $x^2 - y^2$, you will directly observe how an algorithm's design can dramatically affect its accuracy in finite precision.", "problem": "You are asked to write a complete, runnable program that compares two algebraically equivalent formulations for computing the difference of squares for nearly equal inputs, across two floating-point formats: a binary format and a decimal format. The algebraic identity is $x^2 - y^2 = (x-y)(x+y)$. The two formulations to compare are: the naive formulation $x^2 - y^2$ and the factored formulation $(x-y)(x+y)$. Your program must compute both formulations in both formats, measure their numerical error relative to a high-precision reference, and determine which formulation better preserves significance. The binary format should be implemented using the native double-precision floating-point arithmetic of the Institute of Electrical and Electronics Engineers (IEEE) $754$ standard (commonly called binary64), and the decimal format should be implemented using a decimal floating-point arithmetic with precision of $16$ significant digits and round-to-nearest, ties-to-even.\n\nFundamental base for reasoning and design:\n- Floating-point representations approximate real numbers by a finite set of representable values. In IEEE $754$ binary formats, values are encoded as sign, binary exponent, and binary significand. In decimal floating-point, values are encoded as sign, decimal exponent, and decimal significand.\n- Each floating-point format performs rounding to a nearest representable value according to a rounding mode. We will use round-to-nearest, ties-to-even for both formats.\n- Subtractive cancellation occurs when two nearly equal numbers are subtracted in finite precision, causing a large relative error in the result. Algebraic reformulations can mitigate or exacerbate cancellation depending on which intermediate quantities are computed.\n\nMeasurement definition:\n- Let $x$ and $y$ be given as decimal strings; let $t = x^2 - y^2$ be the high-precision reference computed in a high-precision decimal arithmetic that sufficiently reduces rounding effects relative to binary64 and $16$-digit decimal arithmetic.\n- For a computed result $r$ (in either format and formulation), define the scaled relative error\n$$\nE(r; x, y) = \\frac{|r - t|}{\\max\\{ |t|, \\, S \\}}, \\quad \\text{where} \\quad S = \\left(\\max\\{ |x|, |y| \\}\\right)^2.\n$$\nThis definition falls back to an absolute error scaled by a natural magnitude $S$ when $t = 0$, avoiding division by zero while producing a dimensionless indicator. Smaller $E$ indicates better preservation of significance.\n\nTasks your program must perform:\n1. Parse each test case $(x, y)$ from decimal strings.\n2. Compute the high-precision reference $t = x^2 - y^2$ using a decimal arithmetic with sufficiently high precision (at least $100$ decimal digits) and rounding to nearest, ties-to-even.\n3. In IEEE $754$ binary64:\n   - Convert $x$ and $y$ to binary64.\n   - Compute the naive result $r_{\\mathrm{bn}} = x^2 - y^2$.\n   - Compute the factored result $r_{\\mathrm{bf}} = (x-y)(x+y)$.\n   - Compute $E(r_{\\mathrm{bn}}; x, y)$ and $E(r_{\\mathrm{bf}}; x, y)$ using the high-precision $t$ and the scale $S$ defined above.\n   - Determine which is better in binary64 by comparing $E(r_{\\mathrm{bf}}; x, y)$ and $E(r_{\\mathrm{bn}}; x, y)$.\n4. In decimal floating-point with $16$ significant digits and round-to-nearest, ties-to-even:\n   - Quantize $x$ and $y$ to the decimal $16$-digit context and then perform arithmetic in that context.\n   - Compute the naive result $r_{\\mathrm{dn}} = x^2 - y^2$.\n   - Compute the factored result $r_{\\mathrm{df}} = (x-y)(x+y)$.\n   - Compute $E(r_{\\mathrm{dn}}; x, y)$ and $E(r_{\\mathrm{df}}; x, y)$ as above using the same high-precision $t$ and the scale $S$.\n   - Determine which is better in decimal by comparing $E(r_{\\mathrm{df}}; x, y)$ and $E(r_{\\mathrm{dn}}; x, y)$.\n\nTest suite:\nUse the following six $(x,y)$ pairs, provided as decimal strings, which cover typical, boundary, and edge cases including near-cancellation, exact equality, large magnitude operands, small magnitude operands, and opposite signs:\n- Case A (nearby around $1$): $x = \\text{\"1.0000001\"}$, $y = \\text{\"0.9999999\"}$.\n- Case B (very close, asymmetric): $x = \\text{\"1.0000001\"}$, $y = \\text{\"1.0000000\"}$.\n- Case C (very large, near-equal integers): $x = \\text{\"10000000000000001\"}$, $y = \\text{\"9999999999999999\"}$.\n- Case D (very small, near-equal): $x = \\text{\"1.0000001e-16\"}$, $y = \\text{\"9.999999e-17\"}$.\n- Case E (opposite signs, near equal magnitude): $x = \\text{\"10000000000000001\"}$, $y = \\text{\"-9999999999999999\"}$.\n- Case F (exact equality): $x = \\text{\"1.23456789012345\"}$, $y = \\text{\"1.23456789012345\"}$.\n\nFinal output format:\n- For each test case, your program must produce a list with six elements in the following fixed order:\n  - $E(r_{\\mathrm{bn}}; x, y)$ as a floating-point number,\n  - $E(r_{\\mathrm{bf}}; x, y)$ as a floating-point number,\n  - a boolean indicating whether the factored formulation is better in binary64 (that is, whether $E(r_{\\mathrm{bf}}; x, y)  E(r_{\\mathrm{bn}}; x, y)$),\n  - $E(r_{\\mathrm{dn}}; x, y)$ as a floating-point number,\n  - $E(r_{\\mathrm{df}}; x, y)$ as a floating-point number,\n  - a boolean indicating whether the factored formulation is better in decimal $16$-digit arithmetic.\n- Aggregate the per-case lists into a single list and print it as a single line. Concretely, the program should print one line that looks like:\n  $$\n  \\texttt{[[e\\_bn\\_A, e\\_bf\\_A, better\\_bin\\_A, e\\_dn\\_A, e\\_df\\_A, better\\_dec\\_A],\\dots,[e\\_bn\\_F, e\\_bf\\_F, better\\_bin\\_F, e\\_dn\\_F, e\\_df\\_F, better\\_dec\\_F]]}\n  $$\n- All boolean, integer, and floating-point outputs must be plain values; no additional text, units, or annotations should be printed.\n\nNo physical units or angles are involved in this problem. All numbers are dimensionless. Ensure that all arithmetic and comparisons follow the description above. The reference, binary, and decimal computations must be performed within your program; do not require any user input or external data files. The program must be deterministic and self-contained.", "solution": "The problem requires a comparative analysis of two algebraically equivalent formulations for computing the difference of squares, $x^2 - y^2$, across two different floating-point arithmetic systems: the standard IEEE 754 binary64 format and a 16-digit decimal format. The objective is to quantify the numerical error of each formulation and determine which is more robust against the loss of significance.\n\nThe two formulations under examination are:\n1.  The **naive formulation**: $f_n(x, y) = x^2 - y^2$\n2.  The **factored formulation**: $f_f(x, y) = (x-y)(x+y)$\n\nThe fundamental numerical principle at the heart of this problem is **subtractive cancellation**. This phenomenon occurs when two nearly equal numbers are subtracted in finite-precision arithmetic. The leading, significant digits of the numbers cancel each other out, leaving a result dominated by the less significant, and potentially inaccurate, trailing digits. This can lead to a catastrophic loss of relative precision.\n\nIn the context of $x^2 - y^2$, if $x$ and $y$ are close in magnitude ($|x| \\approx |y|$), then $x^2$ and $y^2$ will also be very close. The naive formulation, $x^2 - y^2$, will compute these two nearly equal intermediate values and then subtract them, making it susceptible to subtractive cancellation.\n\nConversely, the factored formulation, $(x-y)(x+y)$, computes the intermediate quantities $x-y$ and $x+y$.\n-   If $x \\approx y$ and they have the same sign, the term $x-y$ is calculated first. While this is a subtraction, the loss of precision happens at a different stage. The value of $x-y$ may be small, but it can often be represented accurately. The second term, $x+y \\approx 2x$, does not involve cancellation. The final product preserves the significance captured in the $x-y$ term.\n-   If $x \\approx -y$, cancellation will occur in the $x+y$ term, while $x-y \\approx 2x$ will be accurate. In this scenario, the factored form is also prone to cancellation, but in a different intermediate step. The naive form, subtracting two large positive numbers ($x^2$ and $y^2$), remains the one vulnerable to cancellation. A careful analysis is required. When $x \\approx -y$, then $x^2 - y^2$ does not involve subtraction of nearly equal numbers if we write $y = -x + \\delta$, where $\\delta$ is small. Then $x^2-y^2 = x^2 - (-x+\\delta)^2 = x^2 - (x^2 - 2x\\delta + \\delta^2) = 2x\\delta - \\delta^2$. The naive method would compute $x^2$ and $y^2$ and subtract, still being nearly equal, thus unstable. The factored form $(x-y)(x+y)$ would compute $(x-(-x+\\delta))(x+(-x+\\delta)) = (2x-\\delta)(\\delta)$, which should be more stable. Therefore, the factored form is generally superior when $|x| \\approx |y|$.\n\nThe solution will be implemented by performing the following steps for each test case $(x, y)$:\n\n1.  **High-Precision Reference**: A trustworthy \"true\" value, $t = x^2 - y^2$, is computed using decimal arithmetic with a precision of $100$ digits. This high-precision context minimizes rounding errors, providing a reliable baseline for comparison. The scale factor $S = (\\max(|x|, |y|))^2$ is also computed in this context.\n\n2.  **Binary64 Arithmetic**: The input strings for $x$ and $y$ are converted to native IEEE 754 double-precision floating-point numbers (`binary64`). Both the naive result, $r_{\\mathrm{bn}}$, and the factored result, $r_{\\mathrm{bf}}$, are computed using standard floating-point operations.\n\n3.  **Decimal16 Arithmetic**: A separate decimal arithmetic context is established with a precision of $16$ digits and round-to-nearest, ties-to-even rounding mode. The inputs $x$ and $y$ are quantized to this precision. The naive ($r_{\\mathrm{dn}}$) and factored ($r_{\\mathrm{df}}$) results are then computed entirely within this decimal context.\n\n4.  **Error Analysis**: For each of the four computed results $r$, the scaled relative error is calculated according to the specified formula:\n    $$\n    E(r; x, y) = \\frac{|r - t|}{\\max\\{ |t|, \\, S \\}}\n    $$\n    This calculation is performed using the high-precision context to ensure the error measurement itself is accurate. The result $r$, which may be a `binary64` float or a `Decimal16` object, is first promoted to a high-precision `Decimal` to be accurately compared with the reference value $t$.\n\n5.  **Conclusion**: For each arithmetic system (binary and decimal), the errors of the two formulations are compared ($E(r_{\\mathrm{bf}})$ vs. $E(r_{\\mathrm{bn}})$ and $E(r_{\\mathrm{df}})$ vs. $E(r_{\\mathrm{dn}})$) to determine which formulation yielded a more accurate result. A boolean value indicates if the factored form was better (i.e., had a strictly smaller error).\n\nThis systematic approach will concretely demonstrate the principles of numerical stability and highlight the differences in behavior between binary and decimal floating-point systems when faced with problematic computations. The test suite is designed to probe various scenarios, including severe cancellation, cases where cancellation is not an issue, and cases where initial input quantization error dominates over algorithmic choice.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, Context, ROUND_HALF_EVEN\n\ndef solve():\n    \"\"\"\n    Compares two formulations for x^2 - y^2 in binary64 and decimal16 floating-point arithmetic.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: nearby around 1\n        (\"1.0000001\", \"0.9999999\"),\n        # Case B: very close, asymmetric\n        (\"1.0000001\", \"1.0000000\"),\n        # Case C: very large, near-equal integers\n        (\"10000000000000001\", \"9999999999999999\"),\n        # Case D: very small, near-equal\n        (\"1.0000001e-16\", \"9.999999e-17\"),\n        # Case E: opposite signs, near equal magnitude\n        (\"10000000000000001\", \"-9999999999999999\"),\n        # Case F: exact equality\n        (\"1.23456789012345\", \"1.23456789012345\"),\n    ]\n\n    # Setup for arithmetic contexts\n    # High-precision for reference calculations\n    ctx_hp = Context(prec=100, rounding=ROUND_HALF_EVEN)\n    # 16-digit decimal for testing\n    ctx_d16 = Context(prec=16, rounding=ROUND_HALF_EVEN)\n\n    results = []\n    \n    for x_str, y_str in test_cases:\n        # 1. High-precision reference computation\n        x_hp = ctx_hp.create_decimal(x_str)\n        y_hp = ctx_hp.create_decimal(y_str)\n\n        # Compute reference t = x^2 - y^2 using the stable factored form.\n        t_ref = ctx_hp.multiply(ctx_hp.subtract(x_hp, y_hp), ctx_hp.add(x_hp, y_hp))\n        \n        # Compute scale S = (max(|x|,|y|))^2\n        s_scale = ctx_hp.power(max(x_hp.copy_abs(), y_hp.copy_abs()), 2)\n        \n        # Denominator for error formula, handles t=0 case\n        error_denominator = max(t_ref.copy_abs(), s_scale)\n        # Avoid division by zero if x=y=0 (not in test suite, but good practice)\n        if error_denominator == 0:\n            error_denominator = Decimal(1)\n\n        # 2. IEEE 754 binary64 computation\n        x_b64 = np.float64(x_str)\n        y_b64 = np.float64(y_str)\n        \n        # Naive formulation\n        r_bn = x_b64**2 - y_b64**2\n        \n        # Factored formulation\n        r_bf = (x_b64 - y_b64) * (x_b64 + y_b64)\n\n        # 3. Decimal 16-digit computation\n        x_d16 = ctx_d16.create_decimal(x_str)\n        y_d16 = ctx_d16.create_decimal(y_str)\n\n        # Naive formulation\n        r_dn = ctx_d16.subtract(ctx_d16.power(x_d16, 2), ctx_d16.power(y_d16, 2))\n        \n        # Factored formulation\n        r_df = ctx_d16.multiply(ctx_d16.subtract(x_d16, y_d16), ctx_d16.add(x_d16, y_d16))\n\n        # 4. Error calculation\n        def get_error(r, t_ref, den, high_prec_ctx):\n            # Promote computed result r (float or Decimal) to a high-precision Decimal\n            # The Decimal constructor accurately represents the exact value of a float.\n            if isinstance(r, (float, np.floating)):\n                r_hp = high_prec_ctx.create_decimal(r)\n            else: # r is already a Decimal\n                r_hp = r\n\n            # Calculate error numerator |r - t| in high precision\n            err_num = high_prec_ctx.subtract(r_hp, t_ref).copy_abs()\n            \n            # Calculate scaled relative error\n            return float(high_prec_ctx.divide(err_num, den))\n\n        e_bn = get_error(r_bn, t_ref, error_denominator, ctx_hp)\n        e_bf = get_error(r_bf, t_ref, error_denominator, ctx_hp)\n        e_dn = get_error(r_dn, t_ref, error_denominator, ctx_hp)\n        e_df = get_error(r_df, t_ref, error_denominator, ctx_hp)\n\n        # 5. Determine which formulation is better\n        better_bin = e_bf  e_bn\n        better_dec = e_df  e_dn\n        \n        case_result = [e_bn, e_bf, better_bin, e_dn, e_df, better_dec]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The default string representation of list of lists is the desired format.\n    print(f\"{results}\")\n\nsolve()\n```", "id": "3210660"}]}