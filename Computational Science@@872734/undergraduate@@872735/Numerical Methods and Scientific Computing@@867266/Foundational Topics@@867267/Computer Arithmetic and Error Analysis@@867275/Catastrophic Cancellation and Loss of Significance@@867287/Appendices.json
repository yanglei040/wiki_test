{"hands_on_practices": [{"introduction": "Let's begin our hands-on exploration with a canonical example that reveals the subtle dangers of floating-point arithmetic. The seemingly simple function $f(x) = \\sqrt{1+x} - 1$ becomes a minefield of numerical error for values of $x$ near zero, as the two terms become nearly identical. This exercise [@problem_id:3212218] will guide you through comparing the direct, naive evaluation with an algebraically equivalent, \"rationalized\" form to quantify the dramatic loss of significance and appreciate the power of algorithmic reformulation.", "problem": "You are to write a complete, runnable program that studies the numerical stability of evaluating the function $f(x)=\\sqrt{1+x}-1$ for small values of $x$, comparing two algebraically equivalent formulas under finite precision arithmetic:\n- Direct subtraction: $f_{\\mathrm{dir}}(x) = \\sqrt{1+x} - 1$.\n- Rationalized form: $f_{\\mathrm{rat}}(x) = \\dfrac{x}{\\sqrt{1+x}+1}$.\n\nYour program must do the following, using only standard double-precision floating-point arithmetic for the approximations and a high-precision reference to quantify error.\n\nFundamental base and background:\n- Use the standard floating-point rounding model: for a real operation producing a real result $t$, its computed floating-point result is $\\operatorname{fl}(t)=t(1+\\delta)$ with $|\\delta|\\leq u$, where $u$ is the unit roundoff (about $u\\approx 10^{-16}$ in binary $64$-bit, commonly known as double precision).\n- Recognize that subtracting nearly equal numbers can cause catastrophic cancellation, where many leading digits cancel, potentially magnifying relative error in the result.\n- The algebraic identity $f(x)=\\dfrac{x}{\\sqrt{1+x}+1}$ holds exactly in real arithmetic for all $x \\geq -1$, and avoids the direct subtraction of nearly equal quantities for small $|x|$.\n\nTasks your program must perform:\n1. For each test value $x$ in the test suite below, compute two floating-point approximations:\n   - $A_{\\mathrm{dir}}=\\operatorname{fl}\\left(\\sqrt{1+x}-1\\right)$,\n   - $A_{\\mathrm{rat}}=\\operatorname{fl}\\left(\\dfrac{x}{\\sqrt{1+x}+1}\\right)$.\n2. Compute a high-precision reference value $f_{\\ast}(x)=\\sqrt{1+x}-1$ using at least $80$ digits of precision in exact real arithmetic emulation. You must not use external files or networks. If $f_{\\ast}(x)=0$, define its relative error as $0$ by convention.\n3. For each approximation $A\\in\\{A_{\\mathrm{dir}},A_{\\mathrm{rat}}\\}$, compute the absolute error $E_{\\mathrm{abs}}=\\left|A-f_{\\ast}(x)\\right|$ and relative error $E_{\\mathrm{rel}}=\\dfrac{\\left|A-f_{\\ast}(x)\\right|}{\\left|f_{\\ast}(x)\\right|}$ (use the stated convention when the denominator is zero).\n4. Aggregate the results for each $x$ as a list $[E_{\\mathrm{abs}}^{\\mathrm{dir}},E_{\\mathrm{rel}}^{\\mathrm{dir}},E_{\\mathrm{abs}}^{\\mathrm{rat}},E_{\\mathrm{rel}}^{\\mathrm{rat}}]$ in that order.\n\nTest suite:\n- Use the following $7$ values of $x$ to probe the behavior across regimes, including positive, zero, and negative small values, as well as near the double-precision threshold for cancellation:\n  - $x\\in\\{10^{-1},\\,10^{-8},\\,10^{-12},\\,10^{-16},\\,0,\\,-10^{-16},\\,-10^{-12}\\}$.\n- All square roots are over nonnegative arguments since $x\\geq -1$ for all cases listed.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of lists in standard programming language literal notation. The $k$-th inner list must correspond to the $k$-th test value $x$ listed above and must be ordered as $[E_{\\mathrm{abs}}^{\\mathrm{dir}},E_{\\mathrm{rel}}^{\\mathrm{dir}},E_{\\mathrm{abs}}^{\\mathrm{rat}},E_{\\mathrm{rel}}^{\\mathrm{rat}}]$.\n- For example, the format should look like $[[e_{11},e_{12},e_{13},e_{14}],[e_{21},e_{22},e_{23},e_{24}],\\dots]$, where each $e_{ij}$ is a floating-point number.\n- No physical units are involved. Do not print any additional text.\n\nScientific realism and derivation expectation:\n- The phenomenon to be revealed is that although the function $f(x)$ is well-conditioned near $x=0$ (its condition number tends to $1$), the direct subtraction $f_{\\mathrm{dir}}(x)$ is algorithmically unstable for small $|x|$ due to catastrophic cancellation, whereas the rationalized form $f_{\\mathrm{rat}}(x)$ is numerically stable. Your code must numerically substantiate this by reporting the errors.", "solution": "The problem requires an analysis of the numerical stability of two algebraically equivalent formulas for computing the function $f(x) = \\sqrt{1+x} - 1$ for values of $x$ close to $0$. The two formulas are the direct evaluation, $f_{\\mathrm{dir}}(x) = \\sqrt{1+x} - 1$, and a rationalized form, $f_{\\mathrm{rat}}(x) = \\frac{x}{\\sqrt{1+x}+1}$. The equivalence is established through algebraic manipulation:\n$$\nf(x) = (\\sqrt{1+x} - 1) \\times \\frac{\\sqrt{1+x}+1}{\\sqrt{1+x}+1} = \\frac{(\\sqrt{1+x})^2 - 1^2}{\\sqrt{1+x}+1} = \\frac{(1+x)-1}{\\sqrt{1+x}+1} = \\frac{x}{\\sqrt{1+x}+1}.\n$$\nThis identity holds for all $x \\ge -1$. While mathematically identical, their behavior under finite-precision floating-point arithmetic differs significantly, particularly for small $|x|$. This analysis serves as a classic illustration of catastrophic cancellation.\n\nThe core of the issue lies in the direct formula, $f_{\\mathrm{dir}}(x)$. As $x$ approaches $0$, the term $\\sqrt{1+x}$ approaches $1$. The Taylor series expansion of $\\sqrt{1+x}$ around $x=0$ is $1 + \\frac{1}{2}x - \\frac{1}{8}x^2 + O(x^3)$. Therefore, for small $|x|$, we have $\\sqrt{1+x} \\approx 1 + \\frac{1}{2}x$. The subtraction $\\sqrt{1+x} - 1$ thus becomes a subtraction of two nearly equal numbers.\n\nIn standard floating-point arithmetic, a real number $y$ is represented as a finite-precision approximation $\\operatorname{fl}(y)$. The process of computation introduces rounding errors. When we compute the difference of two nearly equal numbers, $a-b$ where $a \\approx b$, the leading digits of their mantissas cancel out. If the original numbers $a$ and $b$ had rounding errors, these errors, which were previously in the least significant digits, are now promoted to the most significant digits of the result. This phenomenon is known as catastrophic cancellation and can lead to a drastic loss of relative precision.\n\nLet's model the error in evaluating $f_{\\mathrm{dir}}(x)$. First, $1+x$ is computed as $\\operatorname{fl}(1+x) = (1+x)(1+\\delta_1)$. Then its square root is computed: $\\hat{y} = \\operatorname{fl}(\\sqrt{\\operatorname{fl}(1+x)}) \\approx \\sqrt{1+x}(1+\\delta_2)$. The final step is the subtraction: $A_{\\mathrm{dir}} = \\operatorname{fl}(\\hat{y}-1) \\approx (\\sqrt{1+x}(1+\\delta_2) - 1)(1+\\delta_3)$, where $|\\delta_i| \\le u$, the unit roundoff ($u \\approx 1.11 \\times 10^{-16}$ for $64$-bit double precision). The absolute error in the result is approximately $|A_{\\mathrm{dir}} - f(x)| \\approx |\\sqrt{1+x}\\delta_2| \\approx |\\delta_2|$ since $\\sqrt{1+x} \\approx 1$. The true value is $f(x) \\approx \\frac{1}{2}x$. The relative error is therefore\n$$\nE_{\\mathrm{rel}} = \\frac{|A_{\\mathrm{dir}} - f(x)|}{|f(x)|} \\approx \\frac{|\\delta_2|}{|\\frac{1}{2}x|} = \\frac{2|\\delta_2|}{|x|}.\n$$\nSince $|\\delta_2|$ can be as large as the unit roundoff $u$, the relative error can be magnified by a factor proportional to $1/|x|$. For instance, if $|x| \\approx u$, the relative error can approach $2$, meaning all significant digits are lost.\n\nIn contrast, the rationalized formula $f_{\\mathrm{rat}}(x) = \\frac{x}{\\sqrt{1+x}+1}$ is numerically stable. For small $|x|$, the denominator $\\sqrt{1+x}+1$ is an addition of two positive numbers, which is a benign operation. The denominator is close to $2$, so no cancellation occurs. The computation involves a square root, an addition, and a division. Each operation introduces a small relative error on the order of the unit roundoff $u$. The final computed result, $A_{\\mathrm{rat}}$, will have a relative error that is also a small multiple of $u$, irrespective of how small $|x|$ is.\n\nTo numerically verify this analysis, we will implement a program to perform the computations for a given test suite of $x$ values: $\\{10^{-1}, 10^{-8}, 10^{-12}, 10^{-16}, 0, -10^{-16}, -10^{-12}\\}$.\n$1$. A high-precision reference value, $f_*(x)$, will be computed for each $x$ using at least $80$ digits of precision. This is achieved using Python's `decimal` module, which emulates arbitrary-precision arithmetic. This reference value serves as the \"true\" value against which we measure errors.\n$2$. The two approximations, $A_{\\mathrm{dir}} = \\operatorname{fl}(\\sqrt{1+x}-1)$ and $A_{\\mathrm{rat}} = \\operatorname{fl}(\\frac{x}{\\sqrt{1+x}+1})$, will be computed using standard double-precision floating-point arithmetic (corresponding to `numpy.float64`).\n$3$. For each approximation $A$, we will compute the absolute error, $E_{\\mathrm{abs}} = |A-f_*(x)|$, and the relative error, $E_{\\mathrm{rel}} = E_{\\mathrm{abs}}/|f_*(x)|$. For the case $x=0$, where $f_*(0)=0$, the relative error is taken to be $0$ by convention.\n$4$. The results for each $x$ will be aggregated into a list of four values: $[E_{\\mathrm{abs}}^{\\mathrm{dir}}, E_{\\mathrm{rel}}^{\\mathrm{dir}}, E_{\\mathrm{abs}}^{\\mathrm{rat}}, E_{\\mathrm{rel}}^{\\mathrm{rat}}]$.\n\nThe expected outcome is that for small $|x|$ (e.g., $10^{-8}$ and smaller), the relative error $E_{\\mathrm{rel}}^{\\mathrm{dir}}$ for the direct formula will grow significantly, confirming catastrophic cancellation. Conversely, the relative error $E_{\\mathrm{rel}}^{\\mathrm{rat}}$ for the rationalized formula should remain small, on the order of the unit roundoff $u \\approx 10^{-16}$, across all test cases, demonstrating its numerical stability.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport decimal\n\ndef solve():\n    \"\"\"\n    Analyzes the numerical stability of two formulas for f(x) = sqrt(1+x) - 1.\n    \"\"\"\n    # Set precision for the high-precision reference calculation (more than 80 digits).\n    decimal.getcontext().prec = 100\n\n    # Define the test cases from the problem statement.\n    # Each tuple contains the string representation for high-precision Decimal\n    # and the float representation for standard double-precision numpy calculation.\n    test_cases = [\n        ('1e-1', 1e-1),\n        ('1e-8', 1e-8),\n        ('1e-12', 1e-12),\n        ('1e-16', 1e-16),\n        ('0', 0.0),\n        ('-1e-16', -1e-16),\n        ('-1e-12', -1e-12),\n    ]\n\n    results = []\n    \n    # Pre-calculate Decimal(1) for efficiency.\n    one_dec = decimal.Decimal(1)\n\n    for x_str, x_float in test_cases:\n        # Task 2: Compute a high-precision reference value.\n        x_dec = decimal.Decimal(x_str)\n        f_star = (one_dec + x_dec).sqrt() - one_dec\n\n        # Task 1: Compute two floating-point approximations.\n        x_f64 = np.float64(x_float)\n        one_f64 = np.float64(1.0)\n        \n        # Direct subtraction formula\n        A_dir = np.sqrt(one_f64 + x_f64) - one_f64\n        \n        # Rationalized formula\n        A_rat = x_f64 / (np.sqrt(one_f64 + x_f64) + one_f64)\n\n        # Task 3: Compute absolute and relative errors.\n        # Convert double-precision results to Decimal for accurate error calculation.\n        A_dir_dec = decimal.Decimal(A_dir)\n        A_rat_dec = decimal.Decimal(A_rat)\n\n        # Absolute errors\n        e_abs_dir = abs(A_dir_dec - f_star)\n        e_abs_rat = abs(A_rat_dec - f_star)\n\n        # Relative errors, with special handling for f_star = 0.\n        if f_star == 0:\n            e_rel_dir = decimal.Decimal(0)\n            e_rel_rat = decimal.Decimal(0)\n        else:\n            e_rel_dir = e_abs_dir / abs(f_star)\n            e_rel_rat = e_abs_rat / abs(f_star)\n\n        # Task 4: Aggregate the results for the current x.\n        # Convert Decimal error values to float for the final output.\n        results.append([\n            float(e_abs_dir), float(e_rel_dir),\n            float(e_abs_rat), float(e_rel_rat)\n        ])\n\n    # Final print statement in the exact required format.\n    # Creates a string like '[[e1,e2,e3,e4],[...]]' without extra whitespace.\n    inner_lists = [f\"[{','.join(map(str, r))}]\" for r in results]\n    final_output = f\"[{','.join(inner_lists)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3212218"}, {"introduction": "Building on the previous example, we now turn to a trigonometric function that exhibits similar instability near a critical point. Evaluating $f(x) = \\frac{1-\\cos x}{x}$ for small $x$ presents another classic case of catastrophic cancellation, as $\\cos x$ approaches $1$. This practice [@problem_id:3212289] is designed to expand your problem-solving arsenal by having you implement and compare three powerful, distinct mitigation strategies: using a stable trigonometric identity, employing a local Taylor series approximation, and applying compensated arithmetic to track and correct for rounding errors.", "problem": "Consider the floating-point evaluation of the function $f(x) = \\dfrac{1 - \\cos x}{x}$ when $x$ is small. Catastrophic cancellation occurs in subtracting two nearly equal numbers, and the loss of significance influences the accuracy of subsequent operations. Use the following fundamental bases: the Maclaurin series definition for the cosine function and the standard rounding model of binary floating-point arithmetic.\n\nYou must derive and implement three computational strategies for $f(x)$:\n- The naive formula using direct evaluation of $f(x) = \\dfrac{1 - \\cos x}{x}$.\n- A numerically stable trigonometric identity that avoids subtracting nearly equal quantities.\n- A truncated series approximation of $f(x)$, derived from the Maclaurin series of the cosine function, using the first four nonzero terms of the series for $f(x)$, expressed as an odd polynomial in $x$, and evaluated in a numerically sensible way.\n\nAdditionally, implement a compensated arithmetic strategy for the subtraction $1 - \\cos x$ using an error-free transformation for the sum of two floating-point numbers. This compensated subtraction must produce two floating-point numbers whose sum equals the exact sum of the inputs in real arithmetic. From these, form the compensated $f(x)$ by dividing the compensated difference by $x$.\n\nUse the following definitions and facts as the starting point of your derivation and program design:\n- The Maclaurin series for the cosine function is $\\cos x = \\sum_{k=0}^{\\infty} (-1)^k \\dfrac{x^{2k}}{(2k)!}$ for $x$ in radians.\n- The standard model for rounding in binary floating-point arithmetic is $\\operatorname{fl}(a \\circ b) = (a \\circ b)(1 + \\delta)$ with $|\\delta| \\leq u$, where $u$ is the unit roundoff and $\\circ \\in \\{+, -, \\times, \\div\\}$.\n\nYour program must:\n- Work with angles in $x$ measured in radians.\n- For each test case $x$, compute four approximations: the naive evaluation, the stable identity evaluation, the truncated series evaluation with the first four nonzero terms, and the compensated subtraction evaluation.\n- Use the stable identity evaluation as the reference value to compute the relative error for each of the other three methods. The relative error must be reported as a float computed as $\\displaystyle \\mathrm{rel\\_err} = \\dfrac{|f_{\\text{approx}}(x) - f_{\\text{ref}}(x)|}{|f_{\\text{ref}}(x)|}$.\n\nDesign a small test suite that exercises different regimes:\n- A general small-$x$ case: $x = 10^{-1}$.\n- A very small-$x$ case where cancellation is severe: $x = 10^{-8}$.\n- An extremely small-$x$ case: $x = 10^{-16}$.\n- A sub-ultra-small-$x$ case that will force the naive subtraction to underflow to zero in double precision: $x = 10^{-20}$.\n- A negative small-$x$ case to test odd-function sign behavior: $x = -10^{-8}$.\n- A moderate-$x$ case: $x = 1$.\n- A large-angle case where the series truncation should deteriorate but the identity remains accurate: $x = \\pi$.\n\nFor each test case $x$, your program must produce a list of $7$ floats in the exact order:\n$[f_{\\text{naive}}(x), f_{\\text{identity}}(x), f_{\\text{series}}(x), f_{\\text{compensated}}(x), \\mathrm{rel\\_err}_{\\text{naive}}(x), \\mathrm{rel\\_err}_{\\text{series}}(x), \\mathrm{rel\\_err}_{\\text{compensated}}(x)]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of these per-test-case lists enclosed in square brackets and with no spaces, for example, $[[a_1,a_2,\\dots,a_7],[b_1,b_2,\\dots,b_7],\\dots]$, where each $a_i$, $b_i$, etc., are floats.\n\nAngles must be in $ \\text{radians} $. There are no physical units other than angle; percentages must not be used, and any fraction or decimal must be expressed as a float.", "solution": "The core of this problem is the numerical instability of calculating $f(x) = \\dfrac{1 - \\cos x}{x}$ for values of $x$ close to $0$. As $x \\to 0$, $\\cos x \\to 1$. In floating-point arithmetic, the subtraction of two nearly equal numbers, $1$ and $\\operatorname{fl}(\\cos x)$, leads to catastrophic cancellation, where the most significant digits cancel out, leaving a result dominated by rounding errors. We will implement and compare four methods to compute $f(x)$.\n\n#### 1. Naive Evaluation\nThe naive method directly implements the formula:\n$$ f_{\\text{naive}}(x) = \\frac{1 - \\cos x}{x} $$\nLet $c_x = \\operatorname{fl}(\\cos x)$. For small $x$, $c_x$ is very close to $1$. The computed difference $d = \\operatorname{fl}(1 - c_x)$ will have a large relative error. If $x$ is small enough (e.g., $|x|  \\sqrt{2u}$ where $u$ is the unit roundoff), then $\\operatorname{fl}(\\cos x)$ will be exactly $1$, and the numerator will evaluate to $0$, yielding a completely inaccurate result of $f(x) = 0$.\n\n#### 2. Stable Trigonometric Identity\nTo avoid the subtraction of nearly equal numbers, we can reformulate the numerator $1 - \\cos x$. The half-angle identity for sine is $\\sin^2(\\theta) = \\dfrac{1 - \\cos(2\\theta)}{2}$. Setting $\\theta = x/2$, we obtain $2\\sin^2(x/2) = 1 - \\cos x$. Substituting this into the expression for $f(x)$ yields:\n$$ f_{\\text{identity}}(x) = \\frac{2\\sin^2(x/2)}{x} $$\nThis formulation is numerically stable. For small $x$, the argument $x/2$ to the sine function is also small, and $\\sin(x/2)$ is well-behaved and computed accurately. No subtraction of nearly equal quantities occurs. This method is expected to be the most accurate across all tested ranges of $x$ and will serve as our reference value, $f_{\\text{ref}}(x)$.\n\n#### 3. Truncated Series Approximation\nWe can derive a Maclaurin series for $f(x)$ from the series for $\\cos x$:\n$$ \\cos x = 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} + \\frac{x^8}{8!} - \\dots $$\nSubstituting this into the numerator:\n$$ 1 - \\cos x = 1 - \\left(1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} + \\frac{x^8}{8!} - \\dots\\right) = \\frac{x^2}{2!} - \\frac{x^4}{4!} + \\frac{x^6}{6!} - \\frac{x^8}{8!} + \\dots $$\nDividing by $x$ gives the series for $f(x)$:\n$$ f(x) = \\frac{x}{2!} - \\frac{x^3}{4!} + \\frac{x^5}{6!} - \\frac{x^7}{8!} + \\dots $$\nThe problem requires using the first four nonzero terms. The approximation is:\n$$ f_{\\text{series}}(x) \\approx \\frac{x}{2} - \\frac{x^3}{24} + \\frac{x^5}{720} - \\frac{x^7}{40320} $$\nTo evaluate this polynomial in a numerically stable and efficient manner, we use Horner's method, which minimizes the number of arithmetic operations. The polynomial is factored as:\n$$ f_{\\text{series}}(x) = x \\left( \\frac{1}{2} + x^2 \\left( -\\frac{1}{24} + x^2 \\left( \\frac{1}{720} - \\frac{x^2}{40320} \\right) \\right) \\right) $$\nThis approximation is highly accurate for small $|x|$ but its accuracy deteriorates as $|x|$ increases due to truncation error.\n\n#### 4. Compensated Subtraction\nThis method aims to improve the naive subtraction $1 - \\cos x$ by capturing the error term from the floating-point operation. An error-free transformation, such as the `TwoSum` algorithm, can compute two numbers, $s$ and $e$, representing the exact result of an addition. We apply this to $1 + (-\\cos x)$. The `TwoSum` algorithm for $a+b$ is:\n$s = \\operatorname{fl}(a+b)$\n$v = \\operatorname{fl}(s-a)$\n$e = \\operatorname{fl}((a - (s-v)) + (b-v))$\nLetting $a = 1.0$ and $b = -\\operatorname{fl}(\\cos x)$, we compute $s$ and $e$ such that $s+e = 1.0 - \\operatorname{fl}(\\cos x)$ exactly. The value of $s$ is the result of the naive subtraction, $f_{\\text{naive}}(x) \\cdot x$, and $e$ is the computed error.\nThe compensated value for $f(x)$ is then:\n$$ f_{\\text{compensated}}(x) = \\frac{s + e}{x} $$\nThis method can recover precision lost during the subtraction itself. However, it cannot recover information lost during the initial evaluation of $\\operatorname{fl}(\\cos x)$. If $\\operatorname{fl}(\\cos x)$ rounds to exactly $1.0$, the input to the compensated summation is already flawed, and the method will fail to produce a correct non-zero result, just like the naive approach.\n\n#### Relative Error Calculation\nThe relative error for each approximation ($f_{\\text{approx}}$) is computed with respect to the stable identity evaluation ($f_{\\text{ref}}$) as:\n$$ \\mathrm{rel\\_err} = \\frac{|f_{\\text{approx}}(x) - f_{\\text{ref}}(x)|}{|f_{\\text{ref}}(x)|} $$\nThe choice of $f_{\\text{identity}}$ as the reference is justified by its numerical stability across a wide range of inputs, avoiding both the cancellation issues of the naive method and the truncation error of the series method.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes f(x) = (1 - cos(x)) / x using four different numerical methods\n    and calculates their relative errors with respect to a stable reference method.\n    \"\"\"\n\n    def two_sum(a, b):\n        \"\"\"\n        Computes s, e such that s + e = a + b exactly.\n        This is Knuth's two-sum algorithm.\n        \"\"\"\n        s = a + b\n        v = s - a\n        # (a - (s - v)) is the high-order part of a's error\n        # (b - v) is the low-order part of b's error\n        e = (a - (s - v)) + (b - v)\n        return s, e\n\n    # Test cases in radians\n    test_cases = [\n        1e-1,\n        1e-8,\n        1e-16,\n        1e-20,\n        -1e-8,\n        1.0,\n        np.pi\n    ]\n\n    all_results = []\n    for x in test_cases:\n        # Ensure a copy is used and it's a standard float\n        x_fl = float(x)\n        \n        # --- Method 1: Naive evaluation ---\n        # Suffers from catastrophic cancellation for small x.\n        cos_x = np.cos(x_fl)\n        if x_fl == 0.0:\n            f_naive = 0.0\n        else:\n            f_naive = (1.0 - cos_x) / x_fl\n\n        # --- Method 2: Stable trigonometric identity (Reference) ---\n        # Avoids subtraction of nearly equal numbers.\n        if x_fl == 0.0:\n            f_identity = 0.0\n        else:\n            f_identity = 2.0 * np.sin(x_fl / 2.0)**2 / x_fl\n        \n        f_ref = f_identity\n\n        # --- Method 3: Truncated series approximation ---\n        # Uses first four non-zero terms, evaluated via Horner's method.\n        # f(x) = x/2! - x^3/4! + x^5/6! - x^7/8!\n        #      = x * (1/2 + x^2*(-1/24 + x^2*(1/720 - x^2/40320)))\n        x_sq = x_fl * x_fl\n        c0 = 1.0 / 2.0\n        c1 = -1.0 / 24.0\n        c2 = 1.0 / 720.0\n        c3 = -1.0 / 40320.0\n        # Horner's method evaluation\n        f_series = x_fl * (c0 + x_sq * (c1 + x_sq * (c2 + x_sq * c3)))\n\n        # --- Method 4: Compensated subtraction ---\n        # Uses TwoSum to capture the error in 1 - cos(x).\n        if x_fl == 0.0:\n            f_compensated = 0.0\n        else:\n            # cos_x was computed for the naive method\n            s, e = two_sum(1.0, -cos_x)\n            f_compensated = (s + e) / x_fl\n            \n        # --- Relative Error Calculation ---\n        # Handle case where reference value is zero to avoid division by zero\n        if f_ref == 0.0:\n            rel_err_naive = 0.0 if f_naive == 0.0 else np.inf\n            rel_err_series = 0.0 if f_series == 0.0 else np.inf\n            rel_err_compensated = 0.0 if f_compensated == 0.0 else np.inf\n        else:\n            rel_err_naive = np.abs(f_naive - f_ref) / np.abs(f_ref)\n            rel_err_series = np.abs(f_series - f_ref) / np.abs(f_ref)\n            rel_err_compensated = np.abs(f_compensated - f_ref) / np.abs(f_ref)\n            \n        case_results = [\n            f_naive, \n            f_identity, \n            f_series, \n            f_compensated, \n            rel_err_naive, \n            rel_err_series, \n            rel_err_compensated\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string as specified: [[...],[...],...] with no spaces.\n    inner_parts = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_output_string = f\"[{','.join(inner_parts)}]\"\n    \n    print(final_output_string)\n\nsolve()\n\n```", "id": "3212289"}, {"introduction": "Our final practice moves from abstract functions to a cornerstone of data analysis: the calculation of variance. The common \"one-pass\" textbook formula, $\\sigma^2 = E[X^2] - (E[X])^2$, is notoriously unstable and can lead to meaningless or even negative results for datasets with a small standard deviation relative to their mean. This exercise [@problem_id:3212118] demonstrates this real-world failure and challenges you to implement and compare several numerically robust algorithms, including the standard two-pass method and the clever one-pass Welford's algorithm, underscoring the critical importance of numerical hygiene in scientific computing.", "problem": "You are asked to investigate catastrophic cancellation and loss of significance when computing the population variance using the formula $E[X^2] - (E[X])^2$ and to compare the numerical stability of different algorithms. The foundational base of this investigation is the definition of population mean and variance, and the model of finite precision arithmetic with rounding.\n\nFundamental definitions to be used:\n- The population mean of $n$ real samples $x_1, x_2, \\dots, x_n$ is $\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i$.\n- The population variance is $\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\mu)^2$.\n- An alternative identity for the variance is $\\sigma^2 = E[X^2] - (E[X])^2$ where $E[X] = \\mu$ and $E[X^2] = \\frac{1}{n}\\sum_{i=1}^{n} x_i^2$.\n\nFinite precision model:\n- Real computations are carried out in floating-point arithmetic conforming to the Institute of Electrical and Electronics Engineers (IEEE) $754$ double-precision format. Let the unit roundoff be $u \\approx 2^{-53}$ and suppose each elementary operation introduces a small relative error bounded by a constant times $u$.\n\nYour task:\n1. Implement four algorithms to compute the population variance $\\sigma^2$ for a given list of real numbers:\n   - Algorithm $A_1$ (naive one-pass): compute $E[X]$ and $E[X^2]$ by accumulating $\\sum x_i$ and $\\sum x_i^2$ in a single pass, then return $E[X^2] - (E[X])^2$.\n   - Algorithm $A_2$ (two-pass, un-compensated): compute $\\mu$ in the first pass using $\\mu = \\frac{1}{n}\\sum x_i$, then compute $\\sigma^2 = \\frac{1}{n}\\sum (x_i - \\mu)^2$ in a second pass.\n   - Algorithm $A_3$ (two-pass, compensated with Kahan summation): use Kahan compensated summation to compute the first-pass mean and the second-pass sum of squared deviations.\n   - Algorithm $A_4$ (Welford’s online algorithm): compute $\\sigma^2$ in one pass using the recurrence\n     $$\\text{for } i = 1,2,\\dots,n: \\quad \\delta_i = x_i - \\mu_{i-1}, \\quad \\mu_i = \\mu_{i-1} + \\frac{\\delta_i}{i}, \\quad M2_i = M2_{i-1} + \\delta_i \\cdot (x_i - \\mu_i),$$\n     and return $\\sigma^2 = \\frac{M2_n}{n}$.\n\n2. Use a high-precision baseline to approximate the exact variance for comparison. Compute a reference mean and variance using decimal arithmetic with precision $p = 100$ digits, treating the dataset values as exact decimals or integers as provided.\n\n3. For each algorithm, measure the discrepancy with respect to the high-precision baseline:\n   - If the reference variance is nonzero, compute the relative error $r = \\frac{|\\widehat{\\sigma^2} - \\sigma^2|}{|\\sigma^2|}$ where $\\widehat{\\sigma^2}$ is the algorithm’s result.\n   - If the reference variance is zero, compute the absolute error $a = |\\widehat{\\sigma^2} - 0|$.\n\nTest suite:\nProvide results on the following datasets, designed to exercise a general case, cancellation-prone cases, and boundary cases. In each case, interpret the list as real numbers:\n- Case $1$ (general happy path): $[1,2,3,4,5]$.\n- Case $2$ (large offset, small variance causing catastrophic cancellation in $E[X^2] - (E[X])^2$): $[10^{8} + 0, 10^{8} + 1, 10^{8} + 2, 10^{8} + 3, 10^{8} + 4]$.\n- Case $3$ (constant sequence, true variance zero): $[12345678, 12345678, 12345678, 12345678, 12345678]$.\n- Case $4$ (high dynamic range with sign changes): $[10^{16}, 10^{16} + 1, -10^{16}, -10^{16} + 1]$.\n- Case $5$ (many points, large offset with small increments): $[10^{12} + i \\text{ for } i = 0,1,2,\\dots,999]$.\n- Case $6$ (single element, true variance zero): $[42]$.\n\nImplementation constraints:\n- The program must be a complete, runnable Python program using only the Python standard library and the library NumPy with version $1.23.5$.\n- The program must not read any input and must not access files or networks.\n- Use decimal arithmetic with precision $p = 100$ to compute the reference mean and variance for each case.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets, with no spaces. Each inner list corresponds to a test case in the order above and contains four numbers (floats) representing the error metric for algorithms $A_1$, $A_2$, $A_3$, and $A_4$, respectively.\n- Example format: $[[e_{1,1},e_{1,2},e_{1,3},e_{1,4}],[e_{2,1},e_{2,2},e_{2,3},e_{2,4}],\\dots]$ where each $e_{i,j}$ is either a relative error (as a float) or an absolute error (as a float) depending on whether the reference variance is nonzero or zero as specified above.", "solution": "The core of the problem lies in the numerical properties of different formulas for computing the population variance, $\\sigma^2$. The variance of a population of $n$ samples $\\{x_1, x_2, \\dots, x_n\\}$ with mean $\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i$ is defined as the mean of the squared deviations from the mean:\n$$\n\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\mu)^2\n$$\nA mathematically equivalent formula, derived by expanding the square, is:\n$$\n\\sigma^2 = E[X^2] - (E[X])^2 = \\left(\\frac{1}{n}\\sum_{i=1}^{n} x_i^2\\right) - \\left(\\frac{1}{n}\\sum_{i=1}^{n} x_i\\right)^2\n$$\nWhile these formulas are equivalent in exact arithmetic, their behavior in finite-precision floating-point arithmetic can differ dramatically. We will analyze the four specified algorithms.\n\n**High-Precision Baseline**\n\nTo quantify the error of floating-point algorithms, a \"ground truth\" or reference value is required. Since the input values are given as exact numbers (integers or simple decimals), we can compute a highly accurate reference variance using arbitrary-precision arithmetic. Python's `decimal` module, configured to a high precision (e.g., $p=100$ digits), serves this purpose. By performing all calculations (mean, subtractions, squaring, and summation) with this high precision, we obtain a result that is effectively exact compared to the limited precision of standard double-precision floats.\n\n**Algorithm $A_1$: Naive One-Pass Algorithm**\n\nThis algorithm directly implements the formula $\\sigma^2 = E[X^2] - (E[X])^2$. It computes the sum of samples, $\\sum x_i$, and the sum of squares, $\\sum x_i^2$, in a single pass. This method is susceptible to **catastrophic cancellation**. This occurs when subtracting two nearly equal numbers, which causes a loss of relative precision in the result. If the data consists of values clustered around a large mean $C$ (i.e., $x_i \\approx C$), then $E[X] \\approx C$, and $E[X^2] \\approx C^2$. The variance, which depends on the small deviations of $x_i$ from the mean, is computed by subtracting two large, nearly equal values, $E[X^2]$ and $(E[X])^2$. In floating-point arithmetic, the leading digits of these two numbers will cancel, and the result will be dominated by rounding errors, often yielding a highly inaccurate, possibly even negative, variance.\n\n**Algorithm $A_2$: Two-Pass Algorithm**\n\nThis algorithm adheres to the definitional formula $\\sigma^2 = \\frac{1}{n}\\sum (x_i - \\mu)^2$. It requires two passes over the data:\n1.  First pass: Compute the mean $\\mu = \\frac{1}{n}\\sum x_i$.\n2.  Second pass: Compute the sum of squared deviations $\\sum (x_i - \\mu)^2$ and divide by $n$.\n\nThis method is numerically far more stable than $A_1$. By first computing the mean and then subtracting it from each data point, the subsequent summation is performed on values $(x_i - \\mu)$ that are centered around zero. This avoids the subtraction of two large numbers, thus preventing catastrophic cancellation. The accuracy of this method primarily depends on the accuracy of the computed mean in the first pass.\n\n**Algorithm $A_3$: Two-Pass Algorithm with Kahan Compensated Summation**\n\nThis is a refinement of $A_2$. Standard summation of floating-point numbers can accumulate significant error, especially for large datasets. Kahan summation is an algorithm that mitigates this by tracking a running compensation for the low-order bits lost in each addition. For a sum $S = \\sum x_i$, the Kahan algorithm proceeds as:\n`sum = 0.0`, `c = 0.0`\nFor each `x`:\n  `y = x - c`\n  `t = sum + y`\n  `c = (t - sum) - y`\n  `sum = t`\nThe variable `c` accumulates the round-off error. Algorithm $A_3$ applies this compensated summation to both the first pass (to compute $\\mu$) and the second pass (to compute $\\sum(x_i-\\mu)^2$), yielding a more accurate result than $A_2$, particularly when the number of data points $n$ is large.\n\n**Algorithm $A_4$: Welford's Online Algorithm**\n\nWelford's algorithm is a numerically stable one-pass method. It avoids storing the entire dataset, making it an \"online\" algorithm suitable for streaming data. It iteratively updates the mean $\\mu$ and the sum of squared differences from the mean, $M_2$. For a new data point $x_i$, the updates are:\n$$\n\\delta_i = x_i - \\mu_{i-1} \\\\\n\\mu_i = \\mu_{i-1} + \\frac{\\delta_i}{i} \\\\\nM_{2,i} = M_{2,i-1} + \\delta_i(x_i - \\mu_i)\n$$\nwith initial conditions $\\mu_0 = 0$ and $M_{2,0} = 0$. The final population variance is $\\sigma^2 = M_{2,n}/n$. This algorithm maintains stability by incorporating new data points through updates based on small differences ($\\delta_i$), similar in spirit to the two-pass method, but accomplishing it in a single pass. Its stability is comparable to the two-pass algorithm.\n\n**Expected Results for Test Cases**\n\n- **Case 1 (general):** $[1, 2, 3, 4, 5]$. All algorithms are expected to perform well as the numbers are small and the variance is not small relative to the mean.\n- **Case 2 (large offset, small variance):** $[10^8, \\dots, 10^8+4]$. This is the classic case for catastrophic cancellation. $A_1$ is expected to fail severely, likely yielding a result near zero, while $A_2$, $A_3$, and $A_4$ should be accurate.\n- **Case 3 (zero variance):** $[12345678, \\dots]$. True variance is $0$. $A_1$ may produce a small non-zero or negative result due to round-off error. The other algorithms should produce results very close to $0$.\n- **Case 4 (high dynamic range):** $[10^{16}, 10^{16}+1, -10^{16}, -10^{16}+1]$. This tests the effects of summing large numbers with different signs. The magnitude of the numbers is close to the precision limit of `float64`, which may lead to loss of significance in intermediate calculations. All algorithms may experience some error, but $A_2$, $A_3$, and $A_4$ are expected to be more robust.\n- **Case 5 (many points, large offset):** $[10^{12}, \\dots, 10^{12}+999]$. Similar to Case 2 but with $n=1000$. The larger dataset may amplify summation errors. $A_1$ will fail. $A_3$ may show a slight advantage over $A_2$ due to compensated summation.\n- **Case 6 (single point):** $[42]$. True variance is $0$. All algorithms should correctly compute the variance as $0$.\n\nThe implementation will compute the errors for each algorithm and case relative to the high-precision baseline, demonstrating these numerical properties in practice.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport decimal\n\ndef solve():\n    \"\"\"\n    Main function to run the variance algorithm comparison.\n    \"\"\"\n    decimal.getcontext().prec = 100\n\n    def get_reference_variance(data_list):\n        \"\"\"\n        Computes the population variance using high-precision decimal arithmetic.\n        \"\"\"\n        if not data_list or len(data_list) == 0:\n            return decimal.Decimal(0)\n        \n        n = len(data_list)\n        data_dec = [decimal.Decimal(str(x)) for x in data_list]\n        \n        mean_dec = sum(data_dec) / decimal.Decimal(n)\n        \n        var_dec = sum((x - mean_dec)**2 for x in data_dec) / decimal.Decimal(n)\n        \n        return var_dec\n\n    def alg1_naive(data: np.ndarray) - np.float64:\n        \"\"\"Algorithm A1: Naive one-pass E[X^2] - (E[X])^2.\"\"\"\n        n = data.size\n        if n == 0:\n            return np.float64(0.0)\n        \n        # In a single pass, one would loop. np.sum is equivalent and fast.\n        sum_x = np.sum(data)\n        sum_x_sq = np.sum(data**2)\n        \n        mean_x = sum_x / n\n        mean_x_sq = sum_x_sq / n\n        \n        return mean_x_sq - mean_x**2\n\n    def alg2_twopass(data: np.ndarray) - np.float64:\n        \"\"\"Algorithm A2: Two-pass definitional formula.\"\"\"\n        n = data.size\n        if n == 0:\n            return np.float64(0.0)\n        \n        mean = np.sum(data) / n\n        sum_sq_dev = np.sum((data - mean)**2)\n        \n        return sum_sq_dev / n\n\n    def kahan_sum(arr):\n        \"\"\"Kahan compensated summation.\"\"\"\n        s = np.float64(0.0)\n        c = np.float64(0.0)\n        for x in arr:\n            y = np.float64(x - c)\n            t = s + y\n            c = (t - s) - y\n            s = t\n        return s\n\n    def alg3_kahan(data: np.ndarray) - np.float64:\n        \"\"\"Algorithm A3: Two-pass with Kahan summation.\"\"\"\n        n = data.size\n        if n == 0:\n            return np.float64(0.0)\n            \n        mean = kahan_sum(data) / n\n        \n        # The terms (data - mean)**2 are computed in standard float arithmetic\n        # before being summed with Kahan.\n        sq_devs = (data - mean)**2\n        sum_sq_dev = kahan_sum(sq_devs)\n        \n        return sum_sq_dev / n\n        \n    def alg4_welford(data: np.ndarray) - np.float64:\n        \"\"\"Algorithm A4: Welford's one-pass online algorithm.\"\"\"\n        n = data.size\n        if n == 0:\n            return np.float64(0.0)\n\n        mean = np.float64(0.0)\n        m2 = np.float64(0.0)\n        \n        for i, x in enumerate(data, 1):\n            delta = x - mean\n            mean += delta / i\n            delta2 = x - mean\n            m2 += delta * delta2\n            \n        return m2 / n\n\n    test_cases = [\n        # Case 1: general happy path\n        ([1.0, 2.0, 3.0, 4.0, 5.0]),\n        # Case 2: large offset, small variance\n        ([1e8 + 0.0, 1e8 + 1.0, 1e8 + 2.0, 1e8 + 3.0, 1e8 + 4.0]),\n        # Case 3: constant sequence, true variance zero\n        ([12345678.0] * 5),\n        # Case 4: high dynamic range\n        ([1e16, 1e16 + 1.0, -1e16, -1e16 + 1.0]),\n        # Case 5: many points, large offset\n        ([1e12 + float(i) for i in range(1000)]),\n        # Case 6: single element, true variance zero\n        ([42.0])\n    ]\n\n    results = []\n    algorithms = [alg1_naive, alg2_twopass, alg3_kahan, alg4_welford]\n\n    for data_list in test_cases:\n        case_errors = []\n        data_np = np.array(data_list, dtype=np.float64)\n        ref_var = get_reference_variance(data_list)\n\n        for alg in algorithms:\n            computed_var = alg(data_np)\n            \n            if ref_var == 0:\n                error = abs(computed_var)\n            else:\n                error = abs(computed_var - float(ref_var)) / float(ref_var)\n            case_errors.append(float(error))\n        results.append(case_errors)\n\n    # Final print statement in the exact required format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3212118"}]}