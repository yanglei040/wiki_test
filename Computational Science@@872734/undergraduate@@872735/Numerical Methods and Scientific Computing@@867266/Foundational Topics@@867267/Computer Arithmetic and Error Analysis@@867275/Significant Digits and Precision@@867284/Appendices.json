{"hands_on_practices": [{"introduction": "Numerical computation often reveals surprising ways in which the rules of algebra are bent. A common and critical issue is 'catastrophic cancellation,' which occurs when subtracting two nearly equal floating-point numbers, leading to a drastic loss of significant digits. This first exercise [@problem_id:3273448] provides a classic example of this phenomenon and challenges you to apply a fundamental algebraic technique to transform an unstable expression into a numerically robust equivalent.", "problem": "In standard floating-point arithmetic as described by the widely accepted model in numerical analysis, each basic operation on a real quantity $y$ produces a floating-point result $\\operatorname{fl}(y)$ satisfying $\\operatorname{fl}(y) = y(1+\\delta)$ with $|\\delta| \\leq u$, where $u$ denotes the unit roundoff. Loss of significance occurs when subtracting nearly equal quantities, because the relative error introduced by independent rounding in each term can dominate the small true difference.\n\nConsider the function $f(x) = \\sqrt{x+1} - \\sqrt{x}$, evaluated for large $x  0$. Using only fundamental algebraic identities and the above error model as the foundational base, derive an algebraically equivalent expression for $f(x)$ that avoids loss of significance caused by subtractive cancellation when $x$ is large. Your derivation must make clear why the new expression is less sensitive to rounding. Provide the final expression as a single closed-form analytic function of $x$. No numerical approximation or rounding is required. Your final answer should be a single analytic expression in $x$.", "solution": "The starting point is the floating-point error model $\\operatorname{fl}(y) = y(1+\\delta)$ with $|\\delta| \\leq u$, and the observation that subtracting nearly equal quantities amplifies relative error. For the function $f(x) = \\sqrt{x+1} - \\sqrt{x}$, when $x$ is large, both $\\sqrt{x+1}$ and $\\sqrt{x}$ are close in magnitude: specifically, for large $x$,\n$$\n\\sqrt{x+1} \\approx \\sqrt{x} + \\frac{1}{2\\sqrt{x}},\n$$\nso $f(x)$ is small, of order $\\frac{1}{2\\sqrt{x}}$, while each term $\\sqrt{x+1}$ and $\\sqrt{x}$ is of order $\\sqrt{x}$. Subtracting two large, nearly equal terms produces a small result, which is susceptible to large relative error due to rounding in each term.\n\nTo avoid subtractive cancellation, we seek an algebraically equivalent form that does not require subtracting two almost-equal quantities. A fundamental identity suitable for this transformation is the difference-of-squares identity:\n$$\n(a - b)(a + b) = a^{2} - b^{2}.\n$$\nSet $a = \\sqrt{x+1}$ and $b = \\sqrt{x}$. Then\n$$\n(\\sqrt{x+1} - \\sqrt{x})(\\sqrt{x+1} + \\sqrt{x}) = (x+1) - x = 1.\n$$\nAssuming $x  0$ so that all square roots and sums are positive and nonzero, we can divide both sides by $\\sqrt{x+1} + \\sqrt{x}$ to obtain\n$$\n\\sqrt{x+1} - \\sqrt{x} = \\frac{1}{\\sqrt{x+1} + \\sqrt{x}}.\n$$\nThis expression is algebraically equivalent to the original but avoids the subtraction of nearly equal quantities. In floating-point arithmetic, computing $\\sqrt{x+1}$ and $\\sqrt{x}$ still introduces small relative errors of at most $u$, but forming $\\sqrt{x+1} + \\sqrt{x}$ adds two positive terms to produce a result of magnitude about $2\\sqrt{x}$, which is well conditioned. The subsequent reciprocal does not reintroduce cancellation.\n\nTo justify the improved numerical behavior, consider the error propagation under the floating-point model. Let $\\widehat{a} = \\operatorname{fl}(\\sqrt{x+1}) = \\sqrt{x+1}(1+\\delta_{1})$ and $\\widehat{b} = \\operatorname{fl}(\\sqrt{x}) = \\sqrt{x}(1+\\delta_{2})$ with $|\\delta_{1}|, |\\delta_{2}| \\leq u$. In the subtractive form, the computed difference is\n$$\n\\operatorname{fl}(\\widehat{a} - \\widehat{b}) \\approx (\\sqrt{x+1} - \\sqrt{x}) + \\sqrt{x+1}\\,\\delta_{1} - \\sqrt{x}\\,\\delta_{2},\n$$\nso the absolute error is of order $\\sqrt{x}\\,u$, while the true value is of order $\\frac{1}{2\\sqrt{x}}$, yielding a relative error on the order of $2x u$, which grows with $x$ and can be very large for large $x$. In contrast, in the rewritten form,\n$$\nf(x) = \\frac{1}{\\sqrt{x+1} + \\sqrt{x}},\n$$\nthe computed denominator $\\operatorname{fl}(\\widehat{a} + \\widehat{b})$ has a relative error bounded by a small multiple of $u$, and the denominator itself is of order $2\\sqrt{x}$, so the absolute error in the denominator is of order $\\sqrt{x}\\,u$. The relative error in the final reciprocal is then of order $u$, independent of $x$, which is numerically stable for large $x$.\n\nTherefore, the numerically stable, algebraically equivalent expression is\n$$\n\\frac{1}{\\sqrt{x+1} + \\sqrt{x}}.\n$$", "answer": "$$\\boxed{\\frac{1}{\\sqrt{x+1}+\\sqrt{x}}}$$", "id": "3273448"}, {"introduction": "Building upon the concept of cancellation, this next practice demonstrates that numerical stability is not just about a single expression, but can be a property of an entire algorithm. We will investigate two algebraically equivalent methods for calculating statistical variance: a stable 'two-pass' algorithm and a 'naive' one-pass formula that is highly susceptible to catastrophic cancellation. Through this computational experiment [@problem_id:3273473], you will empirically measure the dramatic loss of precision and see how a thoughtful choice of algorithm is crucial for obtaining reliable scientific results.", "problem": "Consider the computation of the population variance from a finite dataset in floating-point arithmetic. The population variance is defined from first principles as $$\\mathrm{Var}(x) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(x_i - \\mu\\right)^2,\\quad \\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_i,$$ where $n$ is the number of data points. In floating-point arithmetic obeying the IEEE 754 standard (Institute of Electrical and Electronics Engineers 754) for binary64 format (commonly called double precision), each elementary operation is modeled as $$\\mathrm{fl}(a \\circ b) = (a \\circ b)(1+\\delta),\\quad |\\delta| \\le u,$$ where $u$ is the unit roundoff. For round-to-nearest, $u = 2^{-53}$.\n\nYour task is to write a complete program that empirically analyzes loss of significance for variance when the dataset has a large location offset and a small spread. Specifically:\n\n1) Implement two variance computations in binary64 arithmetic:\n   - A two-pass “definition-based” computation that directly evaluates $$\\mathrm{Var}(x) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(x_i - \\mu\\right)^2,$$ where $\\mu$ is computed in a first pass, and the sum of squared deviations in a second pass.\n   - A single-pass computation that accumulates only the sums of $x_i$ and $x_i^2$ in a single traversal and then forms the algebraically equivalent population variance from these two sums using standard real-number algebra. Do not use compensated summation or any multi-pass correction beyond this single traversal and final algebraic combination.\n\n2) Use the following deterministic test suite of datasets. For each test, construct $x_i$ by choosing an integer pattern $k_i \\in \\{-1,0,+1\\}$ with equal counts so that $\\sum_{i=1}^{n} k_i = 0$ and then setting $$x_i = M + d\\,k_i.$$ Concretely, take the first $n/3$ entries with $k_i = -1$, the next $n/3$ with $k_i = 0$, and the last $n/3$ with $k_i = +1$. This yields an exact population variance in real arithmetic equal to $$V_{\\mathrm{true}} = \\mathbb{E}[d^2 k^2] = \\frac{2}{3}\\,d^2.$$\n\n   Use these four test cases (each with $n$ divisible by $3$):\n   - Test A: $n = 6000$, $M = 0$, $d = 10^{-3}$.\n   - Test B: $n = 6000$, $M = 10^{6}$, $d = 1$.\n   - Test C: $n = 6000$, $M = 10^{12}$, $d = 1$.\n   - Test D: $n = 6000$, $M = 10^{16}$, $d = 1$.\n\n3) For each test case, compute:\n   - The naive single-pass variance value $V_{\\mathrm{naive}}$ as described above.\n   - The two-pass variance value $V_{\\mathrm{2pass}}$ as described above.\n   - The relative error of the naive method with respect to the exact real-value variance $V_{\\mathrm{true}}$: $$\\varepsilon_{\\mathrm{rel}} = \\frac{|V_{\\mathrm{naive}} - V_{\\mathrm{true}}|}{V_{\\mathrm{true}}}.$$\n   - The achieved correct significant digits for the naive method, defined as $$D_{\\mathrm{ach}} = \\max\\left(0,\\;\\min\\left(-\\log_{10}(\\varepsilon_{\\mathrm{rel}}),\\;-\\log_{10}(u)\\right)\\right),$$ where the cap by $-\\log_{10}(u)$ reflects the best-possible digits in binary64 arithmetic.\n   - A first-principles prediction of the correct significant digits retained by the naive method based on cancellation analysis. When $|M| \\gg |d|$, the naive formula subtracts two large, nearly equal sums of order $\\Theta(n M^2)$ to obtain a result of order $\\Theta(n d^2)$, so a rough model gives $$\\varepsilon_{\\mathrm{rel}} \\approx C\\,u\\left(\\frac{M}{d}\\right)^2,$$ for some problem-dependent constant $C$ of order $1$. Translate this to a digit prediction by ignoring $C$ and capping by $-\\log_{10}(u)$: $$D_{\\mathrm{pred}} = \\max\\left(0,\\;\\min\\left(-\\log_{10}(u) - 2\\log_{10}\\left(\\frac{|M|}{|d|}\\right),\\;-\\log_{10}(u)\\right)\\right),$$ with the convention that if $M = 0$ then $D_{\\mathrm{pred}} = -\\log_{10}(u)$.\n\n4) Final output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a bracketed 3-tuple in the order $[\\varepsilon_{\\mathrm{rel}}, D_{\\mathrm{ach}}, D_{\\mathrm{pred}}]$. Thus the full output is a single line of the form\n   $$[[e_A, dA_{\\mathrm{ach}}, dA_{\\mathrm{pred}}],[e_B, dB_{\\mathrm{ach}}, dB_{\\mathrm{pred}}],[e_C, dC_{\\mathrm{ach}}, dC_{\\mathrm{pred}}],[e_D, dD_{\\mathrm{ach}}, dD_{\\mathrm{pred}}]].$$\n\nConstraints and notes:\n- Use only binary64 arithmetic; do not employ extended precision, arbitrary precision, compensated summation, or stochastic rounding.\n- All answers are pure numbers with no physical units.\n- Angles are not involved.\n- The output must be exactly one line in the specified bracketed list format with no spaces.", "solution": "The objective is to empirically analyze the numerical stability of two different algorithms for computing the population variance of a dataset, particularly in the presence of a large mean offset relative to the data's spread. The analysis will be conducted using binary64 floating-point arithmetic.\n\nThe population variance of a dataset $\\{x_i\\}_{i=1}^{n}$ is defined as the mean of the squared deviations from the population mean $\\mu$:\n$$\n\\mathrm{Var}(x) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(x_i - \\mu\\right)^2, \\quad \\text{where} \\quad \\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_i\n$$\n\nWe are asked to implement and compare two computational methods:\n\n$1$. **Two-Pass Algorithm:** This method directly implements the definition.\n    - **Pass 1:** Compute the mean $\\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_i$.\n    - **Pass 2:** Compute the sum of squared differences using the pre-computed mean, yielding $\\mathrm{Var}_{\\mathrm{2pass}}(x) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2$.\n    This algorithm is generally numerically stable. By first computing the mean and then the deviations, it centers the data around $0$ before squaring. This prevents the intermediate quantities $(x_i - \\mu)^2$ from becoming excessively large if the original data $x_i$ is far from the origin, thereby preserving precision.\n\n$2$. **Naive Single-Pass Algorithm:** This method uses an algebraically equivalent formula derived as follows:\n$$\n\\mathrm{Var}(x) = \\mathbb{E}[(x-\\mu)^2] = \\mathbb{E}[x^2 - 2x\\mu + \\mu^2] = \\mathbb{E}[x^2] - 2\\mu\\mathbb{E}[x] + \\mu^2 = \\mathbb{E}[x^2] - 2\\mu^2 + \\mu^2 = \\mathbb{E}[x^2] - \\mu^2\n$$\nIn terms of sums, this is:\n$$\n\\mathrm{Var}_{\\mathrm{naive}}(x) = \\left(\\frac{1}{n}\\sum_{i=1}^{n}x_i^2\\right) - \\left(\\frac{1}{n}\\sum_{i=1}^{n}x_i\\right)^2\n$$\nThis allows for the computation of variance in a single pass by accumulating $\\sum x_i$ and $\\sum x_i^2$ simultaneously. However, this method is numerically unstable if the standard deviation is much smaller than the mean. If the data points $x_i$ are all clustered far from the origin, i.e., $x_i \\approx M$ for a large $M$, then $\\mathbb{E}[x^2] \\approx M^2$ and $\\mu^2 \\approx M^2$. The formula involves the subtraction of two large, nearly equal numbers, an operation known as **catastrophic cancellation**, which leads to a significant loss of precision.\n\nThe problem defines a specific test suite to demonstrate this effect. The datasets are constructed as $x_i = M + d\\,k_i$, where $n$ is the total number of points, and we have $n/3$ points for each $k_i \\in \\{-1, 0, +1\\}$.\nFor this dataset structure, the exact mean in real arithmetic is:\n$$\n\\mu = \\mathbb{E}[M+d k] = M + d\\,\\mathbb{E}[k] = M + d\\left(\\frac{1}{3}(-1) + \\frac{1}{3}(0) + \\frac{1}{3}(+1)\\right) = M\n$$\nThe exact population variance $V_{\\mathrm{true}}$ in real arithmetic is:\n$$\nV_{\\mathrm{true}} = \\mathrm{Var}(x) = \\mathbb{E}[(x-\\mu)^2] = \\mathbb{E}[((M+dk) - M)^2] = \\mathbb{E}[(dk)^2] = d^2\\mathbb{E}[k^2]\n$$\nThe expectation of $k^2$ is:\n$$\n\\mathbb{E}[k^2] = \\frac{1}{3}(-1)^2 + \\frac{1}{3}(0)^2 + \\frac{1}{3}(+1)^2 = \\frac{1}{3} + 0 + \\frac{1}{3} = \\frac{2}{3}\n$$\nThus, the exact variance for all test cases is $V_{\\mathrm{true}} = \\frac{2}{3}d^2$.\n\nThe four test cases are designed to vary the ratio $|M/d|$:\n-   Test A: $n = 6000$, $M = 0$, $d = 10^{-3}$. Here $|M/d| = 0$.\n-   Test B: $n = 6000$, $M = 10^{6}$, $d = 1$. Here $|M/d| = 10^6$.\n-   Test C: $n = 6000$, $M = 10^{12}$, $d = 1$. Here $|M/d| = 10^{12}$.\n-   Test D: $n = 6000$, $M = 10^{16}$, $d = 1$. Here $|M/d| = 10^{16}$.\n\nFor each test, we will compute the following quantities for the naive method:\n$1$. The relative error $\\varepsilon_{\\mathrm{rel}} = \\frac{|V_{\\mathrm{naive}} - V_{\\mathrm{true}}|}{V_{\\mathrm{true}}}$.\n$2$. The achieved significant digits $D_{\\mathrm{ach}} = \\max\\left(0,\\;\\min\\left(-\\log_{10}(\\varepsilon_{\\mathrm{rel}}),\\;-\\log_{10}(u)\\right)\\right)$. This quantifies the actual accuracy, capped by the theoretical maximum precision of binary64 arithmetic, where $u = 2^{-53}$ is the unit roundoff, and $-\\log_{10}(u) \\approx 15.95$.\n$3$. The predicted significant digits $D_{\\mathrm{pred}}$. This is based on a first-order error analysis of the catastrophic cancellation, which suggests the relative error grows as $\\varepsilon_{\\mathrm{rel}} \\propto u(M/d)^2$. Ignoring the constant of proportionality, the number of lost digits is approximately $2\\log_{10}(|M/d|)$. The prediction is thus given by $D_{\\mathrm{pred}} = \\max\\left(0,\\;\\min\\left(-\\log_{10}(u) - 2\\log_{10}\\left(\\frac{|M|}{|d|}\\right),\\;-\\log_{10}(u)\\right)\\right)$. For $M=0$, the cancellation does not occur, and the predicted digits are maximal, $-\\log_{10}(u)$.\n\nThe program will iterate through the four test cases, construct the dataset for each, compute $V_{\\mathrm{naive}}$, and then calculate the tuple $(\\varepsilon_{\\mathrm{rel}}, D_{\\mathrm{ach}}, D_{\\mathrm{pred}})$. The final output will be a list of these tuples.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef naive_variance(x: np.ndarray) - float:\n    \"\"\"\n    Computes variance using the unstable single-pass algorithm.\n    Var(x) = E[x^2] - (E[x])^2\n    \"\"\"\n    n = len(x)\n    sum_x = np.sum(x)\n    sum_x_sq = np.sum(x * x)\n    # Using float64 for all calculations to emulate binary64\n    mean_of_squares = sum_x_sq / n\n    square_of_mean = (sum_x / n)**2\n    return mean_of_squares - square_of_mean\n\ndef two_pass_variance(x: np.ndarray) - float:\n    \"\"\"\n    Computes variance using the stable two-pass algorithm.\n    Var(x) = E[(x - E[x])^2]\n    \"\"\"\n    n = len(x)\n    # Pass 1: compute the mean\n    mu = np.sum(x) / n\n    # Pass 2: compute the sum of squared deviations\n    sum_sq_dev = np.sum((x - mu)**2)\n    return sum_sq_dev / n\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis and produce the final output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (n, M, d).\n    test_cases = [\n        (6000, 0.0, 1e-3),\n        (6000, 1e6, 1.0),\n        (6000, 1e12, 1.0),\n        (6000, 1e16, 1.0),\n    ]\n\n    # Unit roundoff for binary64 (double precision)\n    u = 2**-53\n    log10_u = np.log10(u)\n    \n    results = []\n    \n    for n, M, d in test_cases:\n        # 1. Construct the dataset x_i = M + d*k_i\n        n_div_3 = n // 3\n        # Ensure data is float64 (binary64)\n        x = np.concatenate([\n            np.full(n_div_3, M - d, dtype=np.float64),\n            np.full(n_div_3, M, dtype=np.float64),\n            np.full(n_div_3, M + d, dtype=np.float64),\n        ])\n\n        # 2. Compute true variance, V_naive, and V_2pass\n        V_true = (2.0 / 3.0) * d**2\n        V_naive = naive_variance(x)\n        # V_2pass is computed as required by the problem description,\n        # though not used in the final output values. It serves as a\n        # demonstration of a stable algorithm.\n        V_2pass = two_pass_variance(x)\n\n        # 3. Compute relative error of the naive method\n        # Handle the case where V_true is zero (not in this problem)\n        if V_true == 0.0:\n            epsilon_rel = 0.0 if V_naive == 0.0 else np.inf\n        else:\n            epsilon_rel = np.abs(V_naive - V_true) / V_true\n\n        # 4. Compute achieved significant digits\n        # Handle epsilon_rel = 0 to avoid log10(0)\n        if epsilon_rel == 0.0:\n            d_achieved = -log10_u\n        else:\n            d_achieved = np.max([0.0, np.min([-np.log10(epsilon_rel), -log10_u])])\n\n        # 5. Compute predicted significant digits\n        if M == 0.0:\n            d_predicted = -log10_u\n        else:\n            # Note: np.abs is used just in case of negative M or d.\n            log10_ratio = np.log10(np.abs(M) / np.abs(d))\n            pred_val = -log10_u - 2.0 * log10_ratio\n            d_predicted = np.max([0.0, np.min([pred_val, -log10_u])])\n        \n        results.append([epsilon_rel, d_achieved, d_predicted])\n\n    # Final print statement in the exact required format.\n    # Build the string representation manually to avoid spaces.\n    formatted_results = []\n    for res in results:\n        # e.g., res is [1.0, 2.0, 3.0]\n        # creates string \"[1.0,2.0,3.0]\"\n        formatted_results.append(f\"[{res[0]},{res[1]},{res[2]}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3273473"}, {"introduction": "While arithmetic operations are a common source of error, the precision of a result can be limited before any calculation even begins. The ability to represent a number exactly is determined by the finite structure of floating-point formats. This final challenge [@problem_id:3273447] moves beyond operational errors to explore these fundamental representational limits. You will determine the largest integer $n$ for which $n!$ is perfectly representable as a `binary64` number, a task that beautifully merges concepts from number theory with the architecture of computer arithmetic.", "problem": "Consider the Institute of Electrical and Electronics Engineers (IEEE) 754 binary64 format, which represents real numbers using a $1$-bit sign, an $11$-bit exponent, and a $52$-bit fraction field with an implicit leading $1$, yielding a $53$-bit significand. An integer $N$ is exactly representable in this format if and only if its binary representation can be normalized so that the significand encodes it without rounding. Using the fundamental properties of binary normalization and the prime factorization of factorials, determine the largest integer $n$ such that $n!$ can be represented exactly as a binary64 number. Justify your answer from first principles about binary precision and the base-$2$ factor in $n!$. Provide your final answer as a single integer. No rounding is required.", "solution": "A number is represented in the IEEE 754 binary64 format as $N = (-1)^s \\times m \\times 2^e$, where $s$ is the sign bit, $m$ is the $53$-bit significand, and $e$ is the exponent. The significand is a binary number of the form $(1.f_{51}f_{50}...f_0)_2$, where the leading $1$ is implicit and the $52$ bits of the fraction $f$ are stored explicitly. The value of the significand $m$ is always in the range $1 \\le m  2$.\n\nAn integer $N  0$ is exactly representable if and only if it can be written in the form $m \\times 2^k$ for some integer $k$, where $m$ can be represented exactly by the $53$-bit significand. Let the binary representation of the integer $N$ be $N = (b_p b_{p-1} \\dots b_q 0 \\dots 0)_2$, where $b_p=1$ is the most significant bit and $b_q=1$ is the least significant bit. The number of trailing zeros is $q$. We can write $N$ as $O \\times 2^q$, where $O = (b_p b_{p-1} \\dots b_q)_2$ is an odd integer.\n\nTo represent $N$ in floating-point format, we normalize it:\n$$N = (1.b_{p-1}b_{p-2} \\dots b_q)_2 \\times 2^p$$\nThe significand is $(1.b_{p-1}b_{p-2} \\dots b_q)_2$. For this representation to be exact, all bits of the number must be captured by the significand. The binary64 format has a $53$-bit significand. This means the total number of bits from the leading $1$ (at position $p$) to the final $1$ (at position $q$) must be at most $53$. The number of such bits is $p - q + 1$. This is precisely the number of bits in the binary representation of the odd part $O$.\n\nLet the number of bits in $O$ be $L_O$. The condition for exact representation is $L_O \\le 53$. An integer $O$ that requires $L_O$ bits satisfies $2^{L_O-1} \\le O  2^{L_O}$. Thus, the condition $L_O \\le 53$ is equivalent to $O  2^{53}$.\n\nWe are tasked with finding the largest integer $n$ such that $n!$ can be represented exactly. This requires that the odd part of $n!$, which we denote as $O_n$, must satisfy the condition:\n$$O_n  2^{53}$$\nTaking the base-$2$ logarithm of both sides, this is equivalent to:\n$$\\log_2(O_n)  53$$\nThe odd part of $n!$ is given by $O_n = \\frac{n!}{2^{v_2(n!)}}$, where $v_2(n!)$ is the exponent of the highest power of $2$ that divides $n!$. By Legendre's formula, this exponent is calculated as:\n$$v_2(n!) = \\sum_{i=1}^{\\infty} \\left\\lfloor \\frac{n}{2^i} \\right\\rfloor$$\nLet's define the function $L(n) = \\log_2(O_n)$. We have:\n$$L(n) = \\log_2\\left(\\frac{n!}{2^{v_2(n!)}}\\right) = \\log_2(n!) - v_2(n!)$$\nThe function $L(n)$ is monotonically non-decreasing for $n \\ge 1$. To find the largest integer $n$ that satisfies the condition, we must find the point where $L(n)$ crosses the threshold of $53$. We will compute $L(n)$ for values of $n$ until this occurs.\n\nLet's evaluate $L(n)$ for $n=22$:\n$$v_2(22!) = \\left\\lfloor \\frac{22}{2} \\right\\rfloor + \\left\\lfloor \\frac{22}{4} \\right\\rfloor + \\left\\lfloor \\frac{22}{8} \\right\\rfloor + \\left\\lfloor \\frac{22}{16} \\right\\rfloor = 11 + 5 + 2 + 1 = 19$$\nNow we compute $\\log_2(22!)$:\n$$\\log_2(22!) = \\sum_{k=1}^{22} \\log_2(k) = \\frac{1}{\\ln(2)} \\sum_{k=1}^{22} \\ln(k)$$\nUsing computational tools for accuracy:\n$$\\ln(22!) \\approx 48.465903$$\n$$\\log_2(22!) \\approx \\frac{48.465903}{0.693147} \\approx 69.92164$$\nSo, for $n=22$:\n$$L(22) = \\log_2(22!) - v_2(22!) \\approx 69.92164 - 19 = 50.92164$$\nSince $L(22) \\approx 50.92164  53$, the number $22!$ is exactly representable in binary64 format.\n\nNow, let's evaluate $L(n)$ for $n=23$:\nThe number $23$ is odd, so its odd part is $23$ itself. The odd part of $23!$ is related to the odd part of $22!$ by:\n$$O_{23} = O_{22} \\times 23$$\nTaking the base-$2$ logarithm:\n$$L(23) = \\log_2(O_{23}) = \\log_2(O_{22}) + \\log_2(23) = L(22) + \\log_2(23)$$\nWe compute $\\log_2(23)$:\n$$\\log_2(23) = \\frac{\\ln(23)}{\\ln(2)} \\approx \\frac{3.135494}{0.693147} \\approx 4.52356$$\nSo, for $n=23$:\n$$L(23) \\approx 50.92164 + 4.52356 = 55.4452$$\nSince $L(23) \\approx 55.4452  53$, the odd part of $23!$ is greater than $2^{53}$, and thus $23!$ cannot be represented exactly.\n\nBecause $L(n)$ is a non-decreasing function of $n$, and we have found that $L(22)  53$ and $L(23)  53$, the largest integer $n$ for which $n!$ is exactly representable in binary64 is $22$.\n\nWe should also confirm that the exponent is within the allowed range for binary64. The exponent of the normalized number $N = O_n \\times 2^{v_2(n!)}$ is approximately $p = v_2(n!) + \\lfloor \\log_2(O_n) \\rfloor$. For $n=22$:\n$$p_{22} = v_2(22!) + \\lfloor L(22) \\rfloor = 19 + \\lfloor 50.92164 \\rfloor = 19 + 50 = 69$$\nThe IEEE 754 binary64 format uses an $11$-bit exponent with a bias of $1023$. The storable exponent range is from $1-1023=-1022$ to $2046-1023=1023$. The value $69$ is well within this range. Therefore, the limiting factor is indeed the precision of the significand, not the range of the exponent.", "answer": "$$\n\\boxed{22}\n$$", "id": "3273447"}]}