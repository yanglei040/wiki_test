{"hands_on_practices": [{"introduction": "Our first practice confronts a startling consequence of finite precision: adding a positive number to another can result in no change. This exercise [@problem_id:3250049] demonstrates how an iterative process can stall indefinitely if the increment is too small compared to the magnitude of the number being updated, a direct result of the Unit in the Last Place ($ulp$). By simulating this scenario, you will gain a concrete understanding of why floating-point equality checks in loops can be treacherous.", "problem": "You must write a complete, runnable program that demonstrates and detects non-termination in a loop whose condition uses floating-point equality, due to the effect of machine epsilon near equality. Work in the standard binary floating-point model consistent with the Institute of Electrical and Electronics Engineers (IEEE) 754 standard, using rounding to nearest with ties to even. Start from the following foundational base:\n\n- The definition of machine epsilon: the smallest positive real number $\\varepsilon_{\\mathrm{mach}}$ such that, in floating-point arithmetic, $\\mathrm{fl}(1 + \\varepsilon_{\\mathrm{mach}}) \\neq 1$.\n- The unit in the last place (ULP): for a given floating-point number $a$, the distance to the next larger representable floating-point number, denoted $\\mathrm{ulp}(a)$, is $\\mathrm{nextafter}(a,+\\infty) - a$, which is a widely tested fact in floating-point analysis.\n- Rounding to nearest with ties to even: given a real number $z$ lying exactly halfway between two representable floating-point numbers $z_{-}$ and $z_{+}$, the rounded value is the one whose significand has an even least significant bit.\n\nYour program will simulate a family of simple update loops of the form\n- initialize $x \\leftarrow x_{0}$,\n- fix a target $y$ and a constant increment $\\delta  0$,\n- iterate $x \\leftarrow \\mathrm{fl}(x + \\delta)$ until the condition $x = y$ is satisfied.\n\nIn exact real arithmetic, if $x_{0}  y$ and $\\delta  0$, then there exists an integer $k$ such that $x_{0} + k \\delta = y$. However, in floating-point arithmetic, if $\\delta$ is smaller than $\\tfrac{1}{2}\\,\\mathrm{ulp}(x)$ at the current $x$, then $\\mathrm{fl}(x + \\delta) = x$, and the loop makes no progress. In particular, if $x \\neq y$ and $\\mathrm{fl}(x + \\delta) = x$, then the loop body cannot change $x$ and the loop condition $x \\neq y$ remains true forever, so the loop does not terminate.\n\nTasks your program must perform:\n\n1) Compute the machine epsilon $\\varepsilon_{\\mathrm{mach}}$ for the working floating-point type by a first-principles halving procedure. Do not hard-code a constant for $\\varepsilon_{\\mathrm{mach}}$.\n2) Implement a function to compute $\\mathrm{ulp}(a)$ for any finite $a$, defined as $\\left|\\mathrm{nextafter}(a,+\\infty) - a\\right|$.\n3) Implement a simulator for the loop “while $x \\neq y$ do $x \\leftarrow \\mathrm{fl}(x + \\delta)$” with the following behavior:\n   - If $x = y$ at the top of an iteration, declare that the loop terminates.\n   - Otherwise, compute $x_{\\text{new}} \\leftarrow \\mathrm{fl}(x + \\delta)$.\n   - If $x_{\\text{new}} = x$ and $x \\neq y$, declare that the loop cannot terminate because the state is stagnant and the loop condition remains true.\n   - Also guard against pathological cases by stopping after a large fixed number of iterations $N_{\\max}$ and declaring non-termination in that case. You may take $N_{\\max} = 10^{7}$.\n4) For each of the test cases below, construct the specified $x_{0}$, $y$, and $\\delta$ and report a boolean indicating whether the loop terminates (true) or is detected as non-terminating by the stagnation rule or the iteration cap (false).\n\nTest suite specification:\n\nLet $\\mathrm{prev}(a)$ denote the largest representable floating-point number strictly less than $a$, that is, $\\mathrm{prev}(a) = \\mathrm{nextafter}(a,0)$ for $a  0$.\n\nProvide results for the following five cases:\n\n- Case A (non-terminating by stagnation near one): $y = 1$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\tfrac{1}{4}\\,\\mathrm{ulp}(y)$. Expected behavior: since $\\delta  \\tfrac{1}{2}\\,\\mathrm{ulp}(x)$ at $x = x_{0}$, rounding yields $\\mathrm{fl}(x + \\delta) = x$, so no progress is possible when $x \\neq y$.\n- Case B (terminating in one iteration by midpoint rounding): $y = 1$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\tfrac{1}{2}\\,\\mathrm{ulp}(y)$. Expected behavior: $\\mathrm{fl}(x + \\delta)$ rounds to $y$ due to ties to even, so the loop terminates on the next condition check.\n- Case C (immediate termination): $y = 1$, $x_{0} = 1$, and $\\delta = \\tfrac{1}{4}\\,\\mathrm{ulp}(y)$. Expected behavior: the loop does not enter because $x = y$ initially.\n- Case D (non-terminating at large magnitude with too-small increment): $y = 2^{30}$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\tfrac{1}{4}\\,\\mathrm{ulp}(1)$. Expected behavior: since $\\mathrm{ulp}(y) \\gg \\mathrm{ulp}(1)$, one has $\\delta \\ll \\tfrac{1}{2}\\,\\mathrm{ulp}(x)$ at $x \\approx y$, hence $\\mathrm{fl}(x + \\delta) = x$ and the loop cannot make progress.\n- Case E (terminating at large magnitude by adequate increment): $y = 2^{30}$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\tfrac{1}{2}\\,\\mathrm{ulp}(y)$. Expected behavior: as in Case B, one step moves $x$ to $y$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For the five cases A through E in order, output a list of booleans, for example, “[True,False,True,False,True]”. There must be no additional text in the output line.", "solution": "The problem requires an investigation into the properties of floating-point arithmetic, specifically how its discrete nature can lead to the non-termination of loops that rely on equality checks. We will construct a program to demonstrate this phenomenon by implementing a first-principles calculation of machine epsilon, a function to compute the unit in the last place (ULP), and a simulator for a simple iterative loop. The analysis rests on the IEEE 754 standard for binary floating-point arithmetic, using rounding to the nearest representable value, with ties rounded to the value with an even least significant bit in its significand.\n\nThe core of the problem lies in the update step $x \\leftarrow \\mathrm{fl}(x + \\delta)$, where $\\mathrm{fl}(\\cdot)$ denotes the floating-point representation of a real number. Due to the finite precision of floating-point numbers, there is a minimum gap between any number $x$ and the next larger representable number. This gap is known as the unit in the last place, $\\mathrm{ulp}(x)$. If the increment $\\delta$ is too small relative to this gap, the sum $x + \\delta$ may be rounded back to $x$, causing the loop to stagnate.\n\nThe step-by-step solution is as follows:\n\nFirst, we must compute the machine epsilon, $\\varepsilon_{\\mathrm{mach}}$, for the working floating-point type (which we take as 64-bit double precision). $\\varepsilon_{\\mathrm{mach}}$ is defined as the smallest positive number such that $\\mathrm{fl}(1 + \\varepsilon_{\\mathrm{mach}}) \\neq 1$. It is equivalent to $\\mathrm{ulp}(1)$. We can determine $\\varepsilon_{\\mathrm{mach}}$ algorithmically by starting with a value, say $\\varepsilon = 1$, and repeatedly halving it. The loop `while (1.0 + epsilon) > 1.0` continues as long as `epsilon` is large enough to be recognized when added to $1$. The loop terminates when `1.0 + epsilon` is rounded down to $1.0$. If the final value of `epsilon` upon loop termination is $\\varepsilon_{f}$, this means that $\\mathrm{fl}(1 + \\varepsilon_{f}) = 1$, whereas the previous value, $2\\varepsilon_{f}$, satisfied $\\mathrm{fl}(1 + 2\\varepsilon_{f}) > 1$. Therefore, $\\varepsilon_{\\mathrm{mach}} = 2\\varepsilon_{f}$.\n\nSecond, we implement a function to compute $\\mathrm{ulp}(a)$ for a floating-point number $a$. As specified, this is the distance to the next larger representable floating-point number. This is given by the expression $\\mathrm{ulp}(a) = |\\mathrm{nextafter}(a, +\\infty) - a|$. The value of $\\mathrm{ulp}(a)$ is not constant; it scales with the magnitude of $a$. Specifically, for a number $a$ in the range $[2^k, 2^{k+1})$, its ULP is given by $\\mathrm{ulp}(a) = 2^k \\varepsilon_{\\mathrm{mach}}$.\n\nThird, we analyze the condition for loop stagnation. The sum $x + \\delta$ is a real number that must be rounded to a representable floating-point value. The representable numbers nearest to $x$ are $x$ itself and $\\mathrm{nextafter}(x, +\\infty) = x + \\mathrm{ulp}(x)$. The midpoint between these two is $x + \\frac{1}{2}\\mathrm{ulp}(x)$. The \"round to nearest\" rule dictates:\n- If $x + \\delta  x + \\frac{1}{2}\\mathrm{ulp}(x)$, the sum is rounded down to $x$. This is equivalent to $\\delta  \\frac{1}{2}\\mathrm{ulp}(x)$.\n- If $x + \\delta > x + \\frac{1}{2}\\mathrm{ulp}(x)$, the sum is rounded up to $\\mathrm{nextafter}(x, +\\infty)$. This is equivalent to $\\delta > \\frac{1}{2}\\mathrm{ulp}(x)$.\n- If $x + \\delta = x + \\frac{1}{2}\\mathrm{ulp}(x)$, the sum is exactly halfway. The \"ties to even\" rule applies, rounding to the neighbor whose significand has a least significant bit of $0$.\n\nIf $\\delta  \\frac{1}{2}\\mathrm{ulp}(x)$, then $\\mathrm{fl}(x + \\delta) = x$. If at this point $x \\neq y$, the loop variable ceases to change, and the termination condition $x=y$ is never met, leading to an infinite loop. Our simulator must detect this stagnation. It will also include a safeguard limit of $N_{\\max} = 10^7$ iterations.\n\nFourth, we design the loop simulator function. It takes initial values $x_0$, $y$, and $\\delta$. It iterates, updating $x \\leftarrow \\mathrm{fl}(x + \\delta)$. In each iteration, it first checks if $x=y$ (termination), then computes the new value $x_{\\text{new}}$, and finally checks if $x_{\\text{new}} = x$ (stagnation). If the loop completes $N_{\\max}$ iterations without either condition being met, it is also declared as non-terminating.\n\nFinally, we apply this framework to the five specified test cases. We denote $\\mathrm{prev}(a) = \\mathrm{nextafter}(a, 0)$ for $a>0$.\n\nCase A: $y = 1$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\frac{1}{4}\\,\\mathrm{ulp}(y)$.\nHere, $y=1$, so $\\mathrm{ulp}(y) = \\mathrm{ulp}(1) = \\varepsilon_{\\mathrm{mach}}$. The increment is $\\delta = \\frac{1}{4}\\varepsilon_{\\mathrm{mach}}$. The initial value is $x_0 = \\mathrm{prev}(1)$, which is very close to $1$, so $\\mathrm{ulp}(x_0) \\approx \\mathrm{ulp}(1) = \\varepsilon_{\\mathrm{mach}}$. The condition for stagnation is $\\delta  \\frac{1}{2}\\mathrm{ulp}(x_0)$, which becomes $\\frac{1}{4}\\varepsilon_{\\mathrm{mach}}  \\frac{1}{2}\\varepsilon_{\\mathrm{mach}}$. This is true. Thus, $\\mathrm{fl}(x_0 + \\delta) = x_0$. The loop stagnates on the first iteration. The result is non-termination (False).\n\nCase B: $y = 1$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\frac{1}{2}\\,\\mathrm{ulp}(y)$.\nHere, $\\delta = \\frac{1}{2}\\varepsilon_{\\mathrm{mach}}$. The value $x_0 + \\delta$ is exactly halfway between $x_0$ and $y=1$. We must apply the \"ties to even\" rule. The number $1.0$ is represented with a significand whose least significant bit is $0$ (even). The number $x_0 = \\mathrm{prev}(1)$ has a significand whose least significant bit is $1$ (odd). The rounding is to the \"even\" neighbor, which is $y=1$. Thus, $\\mathrm{fl}(x_0 + \\delta) = y$. The loop terminates after one iteration. The result is termination (True).\n\nCase C: $y = 1$, $x_{0} = 1$, and $\\delta = \\frac{1}{4}\\,\\mathrm{ulp}(y)$.\nThe initial condition is $x_0 = y$. The loop condition `while x != y` is false from the beginning. The loop body never executes. The result is immediate termination (True).\n\nCase D: $y = 2^{30}$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\frac{1}{4}\\,\\mathrm{ulp}(1)$.\nThe target $y = 2^{30}$ is a large number. Its ULP is significantly larger than for $1$: $\\mathrm{ulp}(y) = \\mathrm{ulp}(2^{30}) = 2^{30}\\mathrm{ulp}(1) = 2^{30}\\varepsilon_{\\mathrm{mach}}$. The increment is $\\delta = \\frac{1}{4}\\mathrm{ulp}(1) = \\frac{1}{4}\\varepsilon_{\\mathrm{mach}}$. The current value is $x_0 \\approx y$, so $\\mathrm{ulp}(x_0) \\approx \\mathrm{ulp}(y)$. The stagnation condition $\\delta  \\frac{1}{2}\\mathrm{ulp}(x_0)$ becomes $\\frac{1}{4}\\varepsilon_{\\mathrm{mach}}  \\frac{1}{2}(2^{30}\\varepsilon_{\\mathrm{mach}})$, or $\\frac{1}{4}  2^{29}$. This is clearly true. The increment is far too small to have any effect at this magnitude. The loop stagnates. The result is non-termination (False).\n\nCase E: $y = 2^{30}$, $x_{0} = \\mathrm{prev}(y)$, and $\\delta = \\frac{1}{2}\\,\\mathrm{ulp}(y)$.\nThis is analogous to Case B but at a larger magnitude. The increment $\\delta = \\frac{1}{2}\\mathrm{ulp}(y)$ places the sum $x_0 + \\delta$ exactly halfway between $x_0$ and $y$. The number $y=2^{30}$ is a power of two, and its significand has an even least significant bit. The number $x_0 = \\mathrm{prev}(y)$ has an odd least significant bit. Rounding is to the even neighbor, $y$. The loop terminates in one step. The result is termination (True).\n\nThe expected outcomes are [False, True, True, False, True]. The following program formalizes this analysis.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by setting up and running simulations for the five\n    specified test cases and printing the results.\n    \"\"\"\n\n    def compute_machine_epsilon():\n        \"\"\"\n        Computes machine epsilon for float64 using a first-principles halving procedure.\n        Machine epsilon is the smallest positive number eps such that 1.0 + eps != 1.0.\n        \"\"\"\n        # Start with a value that is guaranteed to be significant when added to 1.\n        epsilon = np.float64(1.0)\n        # Repeatedly halve epsilon until 1.0 + epsilon is indistinguishable from 1.0.\n        while np.float64(1.0) + epsilon > np.float64(1.0):\n            epsilon /= np.float64(2.0)\n        # The loop terminates when epsilon is too small. The machine epsilon is the\n        # previous value of epsilon, which is the current value multiplied by 2.\n        return epsilon * np.float64(2.0)\n\n    def compute_ulp(a: np.float64) -> np.float64:\n        \"\"\"\n        Computes the Unit in the Last Place (ULP) for a given float64 number 'a'.\n        Defined as the distance to the next larger representable floating-point number.\n        \"\"\"\n        # Ensure input is a float64 for consistent calculations.\n        a = np.float64(a)\n        # Use np.inf to find the next representable number in the positive direction.\n        # np.nextafter handles negative numbers and special cases correctly.\n        return np.abs(np.nextafter(a, np.inf) - a)\n\n    def simulate_loop(x0: np.float64, y: np.float64, delta: np.float64) -> bool:\n        \"\"\"\n        Simulates the loop 'while x != y: x = x + delta' and detects non-termination.\n\n        Returns:\n            bool: True if the loop terminates, False otherwise.\n        \"\"\"\n        N_MAX = 10**7\n        x = np.float64(x0)\n        y = np.float64(y)\n        delta = np.float64(delta)\n\n        for _ in range(N_MAX):\n            # Condition 1: Loop terminates if x reaches the target y.\n            if x == y:\n                return True\n\n            # Perform one step of the iteration.\n            x_new = x + delta\n\n            # Condition 2: Non-termination due to stagnation.\n            # If the value of x does not change, and x is not at the target,\n            # the loop will never terminate.\n            if x_new == x:\n                return False\n\n            x = x_new\n\n        # Condition 3: Non-termination due to exceeding max iterations.\n        return False\n\n    # Define a helper for prev(a) as specified in the problem\n    def prev(a: np.float64) -> np.float64:\n        return np.nextafter(a, np.float64(0.0))\n\n    # --- Test Case Construction ---\n    \n    # Common helper values\n    y1 = np.float64(1.0)\n    ulp1 = compute_ulp(y1)\n    \n    y2 = np.float64(2**30)\n    ulp2 = compute_ulp(y2)\n\n    test_cases = [\n        # Case A: Non-terminating by stagnation near one\n        (prev(y1), y1, np.float64(0.25) * ulp1),\n        \n        # Case B: Terminating in one iteration by midpoint rounding\n        (prev(y1), y1, np.float64(0.5) * ulp1),\n        \n        # Case C: Immediate termination\n        (y1, y1, np.float64(0.25) * ulp1),\n        \n        # Case D: Non-terminating at large magnitude with too-small increment\n        (prev(y2), y2, np.float64(0.25) * ulp1),\n        \n        # Case E: Terminating at large magnitude by adequate increment\n        (prev(y2), y2, np.float64(0.5) * ulp2),\n    ]\n\n    results = []\n    for x0, y, delta in test_cases:\n        terminates = simulate_loop(x0, y, delta)\n        results.append(terminates)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3250049"}, {"introduction": "Building on the idea of representational limits, this practice explores one of the most infamous numerical pitfalls: catastrophic cancellation. You will investigate the function $f(x) = e^x - 1$ for small values of $x$ and pinpoint the exact threshold where the naive computation `exp(x) - 1` breaks down due to the subtraction of nearly equal numbers [@problem_id:3250061]. This exercise highlights the importance of numerical libraries that provide stable alternatives, like `expm1(x)`, for computations prone to such errors.", "problem": "You are given the task of analyzing the numerical stability of computing the function value of the real-valued map $x \\mapsto e^x - 1$ in floating-point arithmetic. The objective is to detect when computing $e^x - 1$ naively as $\\exp(x) - 1$ in floating-point arithmetic suffers from catastrophic cancellation, and to compare this with a numerically stable alternative that evaluates the same function without cancellation.\n\nBase your reasoning on the following fundamental concepts.\n- Floating-point rounding to nearest, ties-to-even for the standard double-precision format, and the monotonicity of the exponential function $x \\mapsto e^x$ for real $x$.\n- The definition of machine epsilon: the smallest positive floating-point number $\\varepsilon$ such that $1 + \\varepsilon  1$ in the target format.\n- The fact that for small $x$, the real number $e^x$ equals $1 + x + \\frac{x^2}{2} + \\cdots$ and therefore satisfies $e^x - 1 \\approx x$.\n- The phenomenon of catastrophic cancellation: subtracting nearly equal floating-point numbers can erase leading significant digits, causing large relative error.\n\nYour program must implement the following, using double-precision floating point arithmetic (that is, the language’s standard $64$-bit float, as provided by the Numerical Python library).\n1. Compute the machine epsilon $\\varepsilon$ for the floating-point type by an iterative halving method. Denote the result by $\\hat{\\varepsilon}$.\n2. Determine the smallest positive real number $x$ (in the sense of the largest $x$ for which the computed naive difference remains zero) such that the naive floating-point computation of $e^x - 1$ using $\\exp(x) - 1$ returns exactly $0$ in floating-point arithmetic. Formally, find the supremum $x_{\\mathrm{crit}}  0$ such that, in the chosen floating-point type, $\\exp(x) - 1 = 0$ holds. Use monotonicity and a bisection search to approximate $x_{\\mathrm{crit}}$ to within the resolution of the floating-point type.\n3. For the same $x_{\\mathrm{crit}}$, compute the value produced by a cancellation-free implementation of $e^x - 1$ that is designed for small $x$. Denote this value by $y_{\\mathrm{alt}}(x_{\\mathrm{crit}})$. This function should be the language or library-provided implementation that avoids cancellation in the region near zero.\n4. For a well-chosen set of inputs, compare the relative errors of the naive difference and the cancellation-free implementation against a high-precision reference. Use a high-precision real arithmetic facility to compute the reference value $\\operatorname{true}(x)$ for each $x$, and then compute the relative error\n$$\n\\operatorname{relerr}(y;\\operatorname{true}) = \\frac{|y - \\operatorname{true}|}{|\\operatorname{true}|}.\n$$\nThe test suite of inputs to evaluate must be the list\n$$\n\\left[ x_{\\mathrm{crit}}, \\frac{x_{\\mathrm{crit}}}{2}, \\hat{\\varepsilon}, \\frac{\\hat{\\varepsilon}}{2}, \\sqrt{\\hat{\\varepsilon}}, -x_{\\mathrm{crit}}, -\\frac{x_{\\mathrm{crit}}}{2}, -\\hat{\\varepsilon}, -\\frac{\\hat{\\varepsilon}}{2}, -\\sqrt{\\hat{\\varepsilon}} \\right].\n$$\nFor each input $x$ in this list, compute the boolean\n$$\nb(x) = \\left( \\operatorname{relerr}\\big(\\exp(x) - 1; \\operatorname{true}(x)\\big)  \\operatorname{relerr}\\big(\\text{alt}(x); \\operatorname{true}(x)\\big) \\right),\n$$\nwhich answers whether the naive difference has strictly larger relative error than the cancellation-free implementation on that input.\nAssume no physical units are involved. Angles are not relevant. All numerical results must be reported as floating-point numbers or booleans as specified below.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following exact order:\n- First, $\\hat{\\varepsilon}$.\n- Second, $x_{\\mathrm{crit}}$.\n- Third, $y_{\\mathrm{alt}}(x_{\\mathrm{crit}})$.\n- Then the sequence of $10$ booleans $b(x)$ corresponding to the inputs in the test suite listed above, in that exact order.\n\nFor instance, the output must look like\n$[r_1,r_2,r_3,b_1,b_2,\\dots,b_{10}]$\nwith $r_1, r_2, r_3$ being floating-point numbers and $b_1,\\dots,b_{10}$ being booleans. No additional text should be printed.", "solution": "The objective is to analyze the numerical instability of the function $f(x) = e^x - 1$ when computed naively in double-precision floating-point arithmetic for values of $x$ near zero. This instability, known as catastrophic cancellation, is compared against a stable alternative implementation. The analysis involves four main steps: computing the machine epsilon, determining a critical threshold for cancellation, evaluating a stable alternative, and comparing the relative errors of both methods on a specified set of test inputs.\n\n### Part 1: Computation of Machine Epsilon ($\\hat{\\varepsilon}$)\n\nMachine epsilon, denoted by $\\varepsilon$, is the smallest positive floating-point number such that the addition of $1$ and $\\varepsilon$ results in a value strictly greater than $1$. Formally, in floating-point arithmetic, $\\varepsilon = \\min \\{ y > 0 \\mid 1 + y > 1 \\}$. It quantifies the gap between $1$ and the next larger representable floating-point number.\n\nTo compute $\\hat{\\varepsilon}$ for the standard $64$-bit double-precision floating-point type, we employ an iterative halving method as stipulated. We start with a value, for instance $x_0 = 1$, and repeatedly divide it by $2$. For each new value $x_k = x_{k-1}/2$, we test the condition $1 + x_k > 1$. The loop terminates when this condition fails. The last value of $x_{k-1}$ for which the condition held true is our computed machine epsilon, $\\hat{\\varepsilon}$. Using NumPy's `float64` type ensures the operations are performed in double precision.\n\n### Part 2: Determination of the Critical Value ($x_{\\mathrm{crit}}$)\n\nCatastrophic cancellation occurs in the expression $e^x - 1$ when $x$ is close to $0$. In this regime, $e^x$ is close to $1$. The floating-point representation of $e^x$, denoted $\\mathrm{fl}(e^x)$, may lose precision. When we then subtract $1$, the leading bits of $\\mathrm{fl}(e^x)$ and $1$ are identical, and their cancellation leads to a result dominated by rounding errors.\n\nThe problem asks to find the supremum $x_{\\mathrm{crit}} > 0$ such that the naive floating-point computation yields exactly zero:\n$$\n\\mathrm{fl}(\\mathrm{fl}(e^x) - 1) = 0\n$$\nIn standard floating-point arithmetic, this equality holds if and only if $\\mathrm{fl}(e^x)$ is exactly $1.0$. According to the IEEE 754 standard (round-to-nearest, ties-to-even), a real number $z$ is rounded to $1.0$ if it falls within the interval $[1.0 - \\varepsilon/4, 1.0 + \\varepsilon/2)$. Since we consider $x > 0$, we have $e^x > 1$. Thus, $\\mathrm{fl}(e^x) = 1.0$ if $e^x  1.0 + \\varepsilon/2$. The threshold is reached when $e^x = 1.0 + \\varepsilon/2$, which implies $x = \\ln(1.0 + \\varepsilon/2) \\approx \\varepsilon/2$.\n\nA bisection search is used to find this value numerically. We search for the largest positive floating-point number $x$ for which the boolean expression `np.exp(x) - 1.0 == 0.0` is true. The search interval is initialized, for example, to $[0, \\hat{\\varepsilon}]$, and iteratively refined. If for a midpoint `mid`, the condition `np.exp(mid) - 1.0 == 0.0` holds, we know $x_{\\mathrm{crit}}$ is at least `mid`, so we set the lower bound of our search to `mid`. Otherwise, `mid` is too large, and we set the upper bound to `mid`. After a sufficient number of iterations, the lower-bound of the search interval converges to the machine-representable value that is our best approximation of $x_{\\mathrm{crit}}$.\n\n### Part 3: Stable Alternative Implementation ($y_{\\mathrm{alt}}(x_{\\mathrm{crit}})$)\n\nSpecialized library functions exist to compute $e^x - 1$ accurately for small $|x|$. These functions, such as `numpy.expm1`, typically use a Taylor series expansion for $e^x - 1 = x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots$ when $|x|$ is small, and fall back to the direct computation $\\exp(x) - 1$ when $|x|$ is large enough that cancellation is not a concern. We are asked to compute the value of this stable function at our determined critical point, $y_{\\mathrm{alt}}(x_{\\mathrm{crit}}) = \\mathrm{expm1}(x_{\\mathrm{crit}})$.\n\n### Part 4: Relative Error Comparison\n\nTo quantitatively assess the performance of the naive versus the stable method, we compute their relative errors against a high-precision reference value. The relative error of an approximation $y$ with respect to a true value $\\operatorname{true}$ is given by:\n$$\n\\operatorname{relerr}(y; \\operatorname{true}) = \\frac{|y - \\operatorname{true}|}{|\\operatorname{true}|}\n$$\nThe \"true\" value for $e^x - 1$ is computed using Python's `decimal` module, configured with a high precision (e.g., $50$ digits) to serve as a reliable ground truth.\n\nThe comparison is performed for a test suite of $10$ inputs: $[ x_{\\mathrm{crit}}, \\frac{x_{\\mathrm{crit}}}{2}, \\hat{\\varepsilon}, \\frac{\\hat{\\varepsilon}}{2}, \\sqrt{\\hat{\\varepsilon}}, -x_{\\mathrm{crit}}, -\\frac{x_{\\mathrm{crit}}}{2}, -\\hat{\\varepsilon}, -\\frac{\\hat{\\varepsilon}}{2}, -\\sqrt{\\hat{\\varepsilon}} ]$. These inputs are chosen to lie within the region where catastrophic cancellation is expected to be significant. For each input $x$, we compute the boolean indicator:\n$$\nb(x) = \\left( \\operatorname{relerr}\\big(\\mathrm{fl}(\\exp(x) - 1); \\operatorname{true}(x)\\big) > \\operatorname{relerr}\\big(\\mathrm{expm1}(x); \\operatorname{true}(x)\\big) \\right)\n$$\nThis boolean is `True` if the naive computation is strictly less accurate than the stable one. For all specified test inputs, which are small in magnitude, the naive method is expected to suffer from severe cancellation, yielding much larger relative errors than the `expm1` function. Thus, we anticipate all $10$ boolean results to be `True`.\n\nThe final output is an aggregation of these computed values: $\\hat{\\varepsilon}$, $x_{\\mathrm{crit}}$, $y_{\\mathrm{alt}}(x_{\\mathrm{crit}})$, and the $10$ boolean results $b(x_i)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Solves the numerical analysis problem as specified.\n    1. Computes machine epsilon.\n    2. Finds the critical value x_crit where exp(x)-1 underflows to 0.\n    3. Computes the stable alternative expm1(x_crit).\n    4. Compares relative errors for a test suite of inputs.\n    \"\"\"\n    \n    # Task 1: Compute machine epsilon (eps_hat) for float64 by iterative halving.\n    # We are looking for the smallest positive float64 'eps' such that 1.0 + eps  1.0.\n    eps = np.float64(1.0)\n    # The loop finds the largest power-of-two smaller than or equal to the true machine epsilon.\n    # The true epsilon is the *first* value that fails 1+eps  1, so the last one that passes is what we want.\n    # A more precise way is to check 1.0 + eps/2.0.\n    # Let `one = np.float64(1.0)`. Find eps s.t. `one + eps  one` and `one + eps/2 == one`.\n    current_eps = np.float64(1.0)\n    while np.float64(1.0) + current_eps / np.float64(2.0) > np.float64(1.0):\n        current_eps /= np.float64(2.0)\n    eps_hat = current_eps\n\n    # Task 2: Determine the critical value x_crit using a bisection search.\n    # We are looking for the supremum x  0 such that naive exp(x) - 1 == 0.\n    # This happens when exp(x) is rounded to 1.0.\n    # The search space is bounded by 0 and a value known to be too large, like eps_hat.\n    low = np.float64(0.0)\n    high = eps_hat\n    # 100 iterations are sufficient for double precision convergence.\n    for _ in range(100):\n        mid = low + (high - low) / np.float64(2.0)\n        # Check if the naive computation results in zero\n        if np.exp(mid) - np.float64(1.0) == np.float64(0.0):\n            # If so, mid is a candidate; try larger values.\n            low = mid\n        else:\n            # If not, mid is too large.\n            high = mid\n    x_crit = low\n\n    # Task 3: Compute the value using the cancellation-free implementation.\n    y_alt_x_crit = np.expm1(x_crit)\n\n    # Task 4: Compare relative errors for the test suite.\n    \n    # Set precision for the high-precision reference calculation.\n    getcontext().prec = 50\n\n    # Define the test suite of inputs.\n    test_inputs = [\n        x_crit,\n        x_crit / np.float64(2.0),\n        eps_hat,\n        eps_hat / np.float64(2.0),\n        np.sqrt(eps_hat),\n        -x_crit,\n        -x_crit / np.float64(2.0),\n        -eps_hat,\n        -eps_hat / np.float64(2.0),\n        -np.sqrt(eps_hat)\n    ]\n\n    boolean_results = []\n    for x_float in test_inputs:\n        x_dec = Decimal(x_float)\n        \n        # Compute high-precision \"true\" value\n        true_val = x_dec.exp() - Decimal(1)\n\n        # Compute values using naive and stable methods in float64\n        naive_val = np.exp(x_float) - np.float64(1.0)\n        alt_val = np.expm1(x_float)\n        \n        # Convert to Decimal for high-precision error calculation\n        naive_val_dec = Decimal(naive_val)\n        alt_val_dec = Decimal(alt_val)\n\n        # Avoid division by zero if true_val is zero (not possible for these inputs)\n        if true_val == Decimal(0):\n            # This case shouldn't be reached with the given test inputs.\n            # If it were, relative error is ill-defined. We could define error as absolute.\n            # For this problem, we assume it's non-zero.\n            rel_err_naive = Decimal('inf') if naive_val_dec != 0 else Decimal(0)\n            rel_err_alt = Decimal('inf') if alt_val_dec != 0 else Decimal(0)\n        else:\n            rel_err_naive = abs(naive_val_dec - true_val) / abs(true_val)\n            rel_err_alt = abs(alt_val_dec - true_val) / abs(true_val)\n        \n        # Compare relative errors\n        is_naive_worse = rel_err_naive > rel_err_alt\n        boolean_results.append(is_naive_worse)\n\n    # Assemble the final list of results.\n    final_results = [eps_hat, x_crit, y_alt_x_crit] + boolean_results\n    \n    # Format the final list as a string per problem specification.\n    # Booleans must be lowercase 'true' or 'false'.\n    str_results = []\n    for item in final_results:\n        if isinstance(item, bool):\n            str_results.append(str(item).lower())\n        else:\n            str_results.append(str(item))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str_results)}]\")\n\nsolve()\n```", "id": "3250061"}, {"introduction": "Individual rounding errors may seem small, but they can accumulate dramatically in large-scale computations. This final practice [@problem_id:3250053] compares two methods for summing a list of numbers: simple sequential summation and recursive pairwise summation. You will computationally verify that while the error in sequential summation can grow linearly with the number of terms ($O(n)$), a better-designed algorithm can reduce this growth significantly, demonstrating that thoughtful algorithm design is crucial for numerical accuracy.", "problem": "Consider floating-point arithmetic conforming to the Institute of Electrical and Electronics Engineers (IEEE) 754 binary64 (double precision) standard. Let the unit roundoff $u$ be defined as half the distance between $1$ and the next larger representable floating-point number under rounding to nearest, ties to even. For binary64, the base is $\\beta=2$ and the precision is $t=53$, so the unit roundoff is $u=\\tfrac{1}{2}\\beta^{1-t}=2^{-53}$. Assume the standard rounding model: for any two real numbers $a$ and $b$ such that the exact real sum $a+b$ is finite, a single floating-point addition obeys\n$$\n\\operatorname{fl}(a+b) = (a+b)(1+\\delta), \\quad |\\delta| \\le u,\n$$\nwhere $\\operatorname{fl}(\\cdot)$ denotes the computed floating-point result of the addition.\n\nTask A (Derivation): Starting from this rounding model, derive how the forward error in summing $n$ real numbers $x_1,\\dots,x_n$ behaves for two algorithms:\n- Sequential (left-to-right) summation $S_n = (((x_1+x_2)+x_3)+\\cdots)+x_n$, and\n- Pairwise summation that forms a balanced binary tree of additions over the $n$ terms (assume $n=2^k$ so the tree is perfectly balanced).\n\nShow that the sequential summation incurs an error that grows on the order of $O(nu)$, while pairwise summation reduces the growth to $O(\\log n\\,u)$.\n\nTask B (Verification by Computation): Write a complete, runnable program that:\n1. Uses a fixed random seed to generate, for each test case, $n=2^k$ independent and identically distributed real numbers $x_i$ drawn uniformly from the interval $[-1,1]$ as binary64 ($\\text{float64}$) values.\n2. Computes three sums of these $n$ values:\n   - A baseline sum $S^\\star$ using a numerically stable, high-accuracy summation routine that closely approximates the exact real sum, specifically Python's accurate summation function for floating-point numbers. Treat this as the reference value for error measurement.\n   - A sequential (left-to-right) floating-point sum $\\hat{S}_{\\text{seq}}$ by iterating and accumulating in standard binary64 arithmetic.\n   - A pairwise floating-point sum $\\hat{S}_{\\text{pair}}$ by repeatedly summing neighbors in pairs to form a balanced binary tree of additions until a single value remains.\n3. For each algorithm, computes the absolute forward error\n$$\nE_{\\text{seq}} = \\left|\\hat{S}_{\\text{seq}} - S^\\star\\right|, \\quad E_{\\text{pair}} = \\left|\\hat{S}_{\\text{pair}} - S^\\star\\right|.\n$$\n4. Produces the final output as a single line, a comma-separated list enclosed in square brackets, where each entry is itself a bracketed triple $\\left[k, E_{\\text{seq}}, E_{\\text{pair}}\\right]$ for one test case. Use scientific notation for floating-point values.\n\nUse the following test suite of exponents $k$ to ensure coverage of boundary and growth regimes: $k\\in\\{0,1,2,4,8,12,16\\}$, so that $n=2^k$ includes a single-term boundary case and progressively larger sums up to $n=65536$.\n\nYour program must be self-contained and produce exactly one line of output in the format\n$$\n\\big[\\,[k_1,E_{\\text{seq},1},E_{\\text{pair},1}],\\,[k_2,E_{\\text{seq},2},E_{\\text{pair},2}],\\,\\dots\\,\\big],\n$$\nusing scientific notation for the floating-point errors, with no extra spaces inside the brackets of each triple.", "solution": "The problem is assessed to be valid. It presents a well-posed and scientifically grounded question in numerical analysis concerning the error propagation properties of fundamental summation algorithms. The problem is self-contained, providing the necessary mathematical model, concrete algorithmic definitions, a clear computational task, and a precise output format.\n\nHerein, I provide the requested derivation (Task $A$) and the design for the computational verification (Task $B$).\n\n### Task A: Derivation of Forward Error Bounds\n\nWe are given the standard model for a single floating-point addition in binary64 arithmetic:\n$$\n\\operatorname{fl}(a+b) = (a+b)(1+\\delta), \\quad |\\delta| \\le u\n$$\nwhere $u = 2^{-53}$ is the unit roundoff. The error $\\delta$ is a random variable that depends on the arguments $a$ and $b$. For a statistical analysis of rounding errors with random data, it is common to model $\\delta$ as a random variable uniformly distributed in $\\left[-u, u\\right]$, which has mean $E[\\delta] = 0$ and variance $\\operatorname{Var}(\\delta) = E[\\delta^2] = \\frac{u^2}{3}$.\n\nLet $S_n = \\sum_{i=1}^n x_i$ be the exact sum of $n$ real numbers. Let $\\hat{S}_n$ denote the computed floating-point sum. The absolute forward error is $E = |\\hat{S}_n - S_n|$. We analyze its growth for two algorithms.\n\n**1. Sequential (Left-to-Right) Summation**\n\nThe algorithm defines partial sums $\\hat{s}_k$ as:\n$$\n\\hat{s}_1 = x_1, \\quad \\hat{s}_k = \\operatorname{fl}(\\hat{s}_{k-1} + x_k) \\quad \\text{for } k=2, \\dots, n.\n$$\nApplying the rounding model at each step:\n$$\n\\hat{s}_k = (\\hat{s}_{k-1} + x_k)(1+\\delta_k), \\quad |\\delta_k| \\le u.\n$$\nLet $s_k = \\sum_{i=1}^k x_i$ be the exact partial sum and $e_k = \\hat{s}_k - s_k$ be the error at step $k$.\nThe error propagates according to the recurrence:\n$$\ne_k = \\hat{s}_k - s_k = (\\hat{s}_{k-1} + x_k)(1+\\delta_k) - (s_{k-1} + x_k) = (\\hat{s}_{k-1} - s_{k-1})(1+\\delta_k) + s_k \\delta_k = e_{k-1}(1+\\delta_k) + s_k \\delta_k.\n$$\nExpanding this recurrence and ignoring terms of $O(u^2)$ and higher (i.e., products of $\\delta_k$ with $e_{k-1}$), we get:\n$$\ne_n \\approx e_{n-1} + s_n \\delta_n \\approx e_{n-2} + s_{n-1}\\delta_{n-1} + s_n \\delta_n \\approx \\sum_{k=2}^n s_k \\delta_k.\n$$\nThis shows the total error $e_n$ is approximately a sum of the intermediate exact partial sums $s_k$, each scaled by an independent random rounding error $\\delta_k$.\n\nTo find the expected order of magnitude of $e_n$, we compute its variance. Assuming the $\\delta_k$ are independent and uncorrelated with the partial sums $s_k$, and $E[\\delta_k]=0$:\n$$\n\\operatorname{Var}(e_n) = \\operatorname{Var}\\left(\\sum_{k=2}^n s_k \\delta_k\\right) = \\sum_{k=2}^n \\operatorname{Var}(s_k \\delta_k) = \\sum_{k=2}^n E[s_k^2] E[\\delta_k^2] = \\sum_{k=2}^n E[s_k^2] \\frac{u^2}{3}.\n$$\nThe inputs $x_i$ are drawn i.i.d. from $U[-1, 1]$, with mean $0$ and variance $\\sigma_x^2 = \\frac{(1 - (-1))^2}{12} = \\frac{1}{3}$. The exact partial sum $s_k = \\sum_{i=1}^k x_i$ is a sum of i.i.d. random variables, so it acts as a random walk. Its mean is $E[s_k]=0$ and its variance is $E[s_k^2] = \\operatorname{Var}(s_k) = k \\sigma_x^2$.\nPlugging this into the variance of the error:\n$$\n\\operatorname{Var}(e_n) \\approx \\sum_{k=2}^n (k \\sigma_x^2) \\frac{u^2}{3} = \\frac{\\sigma_x^2 u^2}{3} \\sum_{k=2}^n k = \\frac{\\sigma_x^2 u^2}{3} \\left(\\frac{n(n+1)}{2} - 1\\right).\n$$\nFor large $n$, this is dominated by the $n^2$ term:\n$$\n\\operatorname{Var}(e_n) \\approx \\frac{\\sigma_x^2 u^2 n^2}{6}.\n$$\nThe root-mean-square (RMS) forward error is the standard deviation of $e_n$:\n$$\nE_{\\text{seq}}^{\\text{RMS}} = \\sqrt{\\operatorname{Var}(e_n)} \\approx \\frac{\\sigma_x u n}{\\sqrt{6}}.\n$$\nThis shows that for random data with zero mean, the expected absolute forward error grows linearly with $n$, i.e., $E_{\\text{seq}} = O(nu)$.\n\n**2. Pairwise Summation**\n\nFor $n=2^k$, pairwise summation proceeds in $k = \\log_2 n$ stages.\n- **Stage 1**: Compute $n/2$ sums: $\\operatorname{fl}(x_1+x_2), \\operatorname{fl}(x_3+x_4), \\dots$.\n- **Stage 2**: Compute $n/4$ sums using the results from Stage $1$.\n- ...\n- **Stage k**: Compute the final sum.\n\nLet $e_{j,m}$ be the error in the $m$-th sum computed at stage $j$. The total error is the sum of all errors generated at all nodes of the binary tree, propagated to the final result. Using the recurrence $e_{\\text{new}} \\approx e_{\\text{left}} + e_{\\text{right}} + S_{\\text{new}} \\delta$, the final error is approximately the sum over all $n-1$ internal nodes of the tree:\n$$\ne_n \\approx \\sum_{\\text{nodes } p} S_p \\delta_p,\n$$\nwhere $S_p$ is the exact sum corresponding to node $p$ and $\\delta_p$ is the rounding error at that node.\nThe variance of the total error is the sum of the variances contributed by each node's rounding:\n$$\n\\operatorname{Var}(e_n) \\approx \\sum_{\\text{nodes } p} \\operatorname{Var}(S_p \\delta_p) = \\sum_{\\text{nodes } p} E[S_p^2] E[\\delta_p^2] = \\frac{u^2}{3} \\sum_{\\text{nodes } p} E[S_p^2].\n$$\nThe sum can be grouped by stages (levels) of the tree. At stage $j$ (for $j=1, \\dots, k=\\log_2 n$), there are $n/2^j$ sums being computed. Each sum is over $2^j$ original random numbers $x_i$.\nThe variance of such a sum $S_p$ is $E[S_p^2] = \\operatorname{Var}(S_p) = 2^j \\sigma_x^2$.\nThe contribution to the total error variance from Stage $j$ is:\n$$\n\\operatorname{Var}_j(e_n) = (\\text{number of sums}) \\times (\\text{variance per sum}) \\times \\frac{u^2}{3} = \\left(\\frac{n}{2^j}\\right) \\times (2^j \\sigma_x^2) \\times \\frac{u^2}{3} = \\frac{n \\sigma_x^2 u^2}{3}.\n$$\nNotably, the variance contributed by each stage is constant. Since there are $k = \\log_2 n$ stages, the total variance is the sum of contributions from all stages:\n$$\n\\operatorname{Var}(e_n) = \\sum_{j=1}^{\\log_2 n} \\operatorname{Var}_j(e_n) = (\\log_2 n) \\left(\\frac{n \\sigma_x^2 u^2}{3}\\right).\n$$\nThe RMS forward error is therefore:\n$$\nE_{\\text{pair}}^{\\text{RMS}} = \\sqrt{\\operatorname{Var}(e_n)} \\approx \\sqrt{\\frac{n \\log_2 n}{3}} \\sigma_x u.\n$$\nThe error growth is $O(\\sqrt{n \\log n} \\cdot u)$.\n\n**Note on Problem Statement:** The problem asks to show that pairwise summation error grows as $O(\\log n \\cdot u)$. This is a slight imprecision common in introductory contexts. The $O(\\log n \\cdot u)$ bound correctly describes the *backward error* for pairwise summation, meaning the computed sum $\\hat{S}_{\\text{pair}}$ is the exact sum of slightly perturbed inputs: $\\hat{S}_{\\text{pair}} = \\sum_{i=1}^n x_i(1+\\theta_i)$ where $|\\theta_i| \\le u \\log_2 n + O(u^2)$. The absolute forward error, $|\\hat{S}_{\\text{pair}} - S_n| = |\\sum x_i \\theta_i|$, does not generally follow this bound. The statistical analysis above, yielding $O(\\sqrt{n \\log n} \\cdot u)$, more accurately reflects the behavior for random data as will be verified computationally. The superiority of pairwise summation over sequential summation is nonetheless established, as $O(\\sqrt{n \\log n} \\cdot u)$ is a much slower growth than $O(nu)$.\n\n### Task B: Computational Verification Plan\n\nThe program will implement the logic specified in the problem statement.\n1.  A random seed will be fixed at 42 for reproducibility.\n2.  For each $k \\in \\{0, 1, 2, 4, 8, 12, 16\\}$, we calculate $n=2^k$ and generate an array of $n$ `float64` numbers i.i.d. on $[-1, 1]$.\n3.  The reference sum $S^\\star$ will be computed using `math.fsum`, which largely avoids intermediate rounding errors.\n4.  The sequential sum $\\hat{S}_{\\text{seq}}$ will be computed with a standard `for` loop to ensure left-to-right accumulation.\n5.  The pairwise sum $\\hat{S}_{\\text{pair}}$ will be computed using an iterative helper function that repeatedly sums adjacent pairs of a working array until a single value remains. Since $n=2^k$, the array length will always be even at each step, simplifying the implementation.\n6.  Absolute forward errors $E_{\\text{seq}} = |\\hat{S}_{\\text{seq}} - S^\\star|$ and $E_{\\text{pair}} = |\\hat{S}_{\\text{pair}} - S^\\star|$ are calculated.\n7.  The results for each $k$ will be formatted into a string `f\"[{k},{E_seq:.16e},{E_pair:.16e}]\"` with no internal spaces and collected into a list.\n8.  The final output is a single line printing the list of these results, comma-separated and enclosed in brackets.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef sequential_sum(arr: np.ndarray) -> np.float64:\n    \"\"\"\n    Computes the sum of array elements using sequential (left-to-right)\n    floating-point addition.\n    \"\"\"\n    s = np.float64(0.0)\n    for x_i in arr:\n        s += x_i\n    return s\n\ndef pairwise_sum(arr: np.ndarray) -> np.float64:\n    \"\"\"\n    Computes the sum of array elements using pairwise summation, assuming\n    the length of the array is a power of 2.\n    \"\"\"\n    # Handle boundary cases\n    if arr.size == 0:\n        return np.float64(0.0)\n    if arr.size == 1:\n        return arr[0]\n\n    # Use a copy to avoid modifying the original array if it's passed from\n    # a context where it's still needed.\n    current_arr = arr.copy()\n\n    # The problem guarantees n = 2^k, so len(current_arr) will always\n    # be even at each stage of reduction except the last.\n    while current_arr.size > 1:\n        # Sum adjacent pairs. This operation is performed in-place on\n        # a new array.\n        # e.g., for [a, b, c, d], this becomes [a+b, c+d]\n        current_arr = current_arr[0::2] + current_arr[1::2]\n    \n    return current_arr[0]\n\n\ndef solve():\n    \"\"\"\n    Main function to run the summation error comparison experiment.\n    \"\"\"\n    # Use a fixed random seed for reproducibility of the generated numbers.\n    np.random.seed(42)\n\n    # Test suite of exponents k, where n = 2^k.\n    test_cases_k = [0, 1, 2, 4, 8, 12, 16]\n\n    results = []\n    for k in test_cases_k:\n        n = 2**k\n        \n        # 1. Generate n i.i.d. real numbers from U[-1, 1] as binary64.\n        # np.random.uniform defaults to float64, which is binary64.\n        x = np.random.uniform(low=-1.0, high=1.0, size=n)\n\n        # 2. Compute the three sums.\n        # S_star: High-accuracy reference sum using math.fsum.\n        s_star = math.fsum(x)\n        \n        # S_seq: Standard sequential floating-point sum.\n        s_seq = sequential_sum(x)\n        \n        # S_pair: Pairwise floating-point sum.\n        s_pair = pairwise_sum(x)\n\n        # 3. Compute the absolute forward errors.\n        e_seq = abs(s_seq - s_star)\n        e_pair = abs(s_pair - s_star)\n\n        # 4. Format the result for this test case as a string triple.\n        # Using .16e for full double precision representation in scientific notation.\n        result_str = f\"[{k},{e_seq:.16e},{e_pair:.16e}]\"\n        results.append(result_str)\n\n    # Final print statement in the exact required format.\n    # The list of string results is joined by commas and enclosed in brackets.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3250053"}]}