{"hands_on_practices": [{"introduction": "An essential part of implementing any iterative algorithm is knowing when to stop. This exercise [@problem_id:3283230] challenges you to develop and justify a practical stopping criterion for the power method. By analyzing the convergence of the vector iterates, you will derive a condition based on the angle between successive approximations, bridging the gap between theoretical convergence and robust computational practice.", "problem": "You are asked to formulate, justify, and implement a stopping criterion for the Power Method, an iterative algorithm for approximating the dominant eigenvector of a square matrix. Consider a real square matrix $A \\in \\mathbb{R}^{n \\times n}$ and a nonzero initial vector $x_0 \\in \\mathbb{R}^n$. The Power Method generates iterates by repeatedly applying $A$ and renormalizing in the Euclidean norm. Let $x_k$ denote the normalized iterate at step $k$, and let the angle between $x_k$ and $x_{k+1}$ be measured in radians. The cosine of this angle is the inner product of unit vectors.\n\nTask:\n- Starting from the fundamental definition of eigenvalues and eigenvectors, the linearity of matrix-vector multiplication, and the Euclidean norm, derive a practical stopping criterion whose decision variable is the cosine of the angle between successive normalized iterates $x_k$ and $x_{k+1}$. Your criterion must rely only on this cosine and a user-specified tolerance $\\tau  0$, and it must be invariant to scaling of the iterates. Do not assume any shortcut formulas; derive the criterion from first principles of the Power Methodâ€™s behavior in the eigenbasis.\n- Implement a program that applies the Power Method using your stopping criterion. Normalize every iterate using the Euclidean ($2$-norm). At each step $k$, compute the cosine between $x_k$ and $x_{k+1}$ using the inner product of unit vectors, and use only this cosine and a given tolerance $\\tau$ to decide whether to stop.\n- If the method satisfies the stopping criterion at iteration $k$ (meaning after $k$ matrix-vector multiplications), your program should record the integer $k$. If the method does not satisfy the criterion within a prescribed maximum number of iterations $K_{\\max}$, record the integer $-1$ for that test case.\n- Angles are conceptually measured in radians, but the program should use only the cosine values.\n\nUse the following test suite of parameter values:\n- Test case $1$ (general happy path, symmetric with simple dominant eigenvalue): $A = \\begin{bmatrix} 5.0  0.0  0.0 \\\\ 0.0  3.0  0.0 \\\\ 0.0  0.0  1.0 \\end{bmatrix}$, $x_0 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$, $\\tau = 10^{-6}$, $K_{\\max} = 1000$.\n- Test case $2$ (nonsymmetric upper triangular, faster alignment but possible sign effects): $A = \\begin{bmatrix} 2.0  1.0  0.0 \\\\ 0.0  4.0  1.0 \\\\ 0.0  0.0  1.0 \\end{bmatrix}$, $x_0 = \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix}$, $\\tau = 10^{-9}$, $K_{\\max} = 5000$.\n- Test case $3$ (slow convergence due to nearly equal dominant and subdominant magnitudes): $A = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  0.99 \\end{bmatrix}$, $x_0 = \\begin{bmatrix} 1.0 \\\\ 0.1 \\end{bmatrix}$, $\\tau = 10^{-6}$, $K_{\\max} = 5000$.\n- Test case $4$ (edge case with nonunique largest magnitude eigenvalue, potential nonconvergence): $A = \\begin{bmatrix} 2.0  0.0 \\\\ 0.0  -2.0 \\end{bmatrix}$, $x_0 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$, $\\tau = 10^{-6}$, $K_{\\max} = 1000$.\n\nYour program should produce a single line of output containing the iteration counts for all test cases as a comma-separated list enclosed in square brackets. For example, if the counts for the four test cases were $12$, $37$, $-1$, and $9$, the output must be exactly the single line\n$$[12,37,-1,9]$$\nNo other text may be printed. All numeric outputs must be integers. There are no physical units in this problem. Angles are to be understood in radians, but the program operates only on cosine values (dimensionless).", "solution": "The problem requires the derivation and implementation of a stopping criterion for the Power Method based on the angle between successive iterates.\n\n### Derivation of the Stopping Criterion\n\nLet $A$ be a real, diagonalizable $n \\times n$ matrix, $A \\in \\mathbb{R}^{n \\times n}$. Let its eigenvalues be $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$ with corresponding eigenvectors $v_1, v_2, \\ldots, v_n$. The eigenvectors $\\{v_i\\}_{i=1}^n$ form a basis for $\\mathbb{R}^n$. The Power Method is designed to find the eigenvector corresponding to the dominant eigenvalue, which is the eigenvalue with the strictly largest magnitude. We assume such an eigenvalue exists, i.e., $|\\lambda_1|  |\\lambda_2| \\ge |\\lambda_3| \\ge \\ldots \\ge |\\lambda_n|$. An eigenvector $v$ associated with an eigenvalue $\\lambda$ satisfies the fundamental equation $A v = \\lambda v$.\n\nThe Power Method is an iterative algorithm that starts with a non-zero initial vector $x^{(0)} \\in \\mathbb{R}^n$. This initial vector can be expressed as a linear combination of the eigenvectors:\n$$x^{(0)} = c_1 v_1 + c_2 v_2 + \\cdots + c_n v_n$$\nFor the method to succeed, the component in the direction of the dominant eigenvector $v_1$ must be non-zero, i.e., $c_1 \\neq 0$.\n\nThe core of the method is the repeated application of the matrix $A$ to the vector. After $k$ applications, we have:\n$$A^k x^{(0)} = A^k \\left( \\sum_{i=1}^{n} c_i v_i \\right) = \\sum_{i=1}^{n} c_i (A^k v_i) = \\sum_{i=1}^{n} c_i (\\lambda_i^k v_i)$$\nFactoring out the dominant eigenvalue term $\\lambda_1^k$:\n$$A^k x^{(0)} = \\lambda_1^k \\left( c_1 v_1 + c_2 \\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k v_2 + \\cdots + c_n \\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^k v_n \\right)$$\nDue to the dominance condition $|\\lambda_1|  |\\lambda_i|$ for all $i  1$, the ratios $|\\lambda_i / \\lambda_1|$ are all less than $1$. As the number of iterations $k$ approaches infinity, the terms $(\\lambda_i / \\lambda_1)^k$ approach $0$ for $i=2, \\ldots, n$. Consequently, the vector $A^k x^{(0)}$ becomes increasingly aligned with the dominant eigenvector $v_1$:\n$$\\lim_{k \\to \\infty} \\frac{A^k x^{(0)}}{\\lambda_1^k} = c_1 v_1$$\n\nThe Power Method algorithm generates a sequence of normalized iterates. Let $x^{(k)}$ be the normalized iterate after $k$ matrix multiplications. The procedure is as follows:\n$1$. Start with an initial vector $x_0$ and normalize it: $x^{(0)} = x_0 / \\|x_0\\|_2$.\n$2$. For $k = 1, 2, \\ldots$, compute the next iterate:\n   $$z^{(k)} = A x^{(k-1)}$$\n   $$x^{(k)} = \\frac{z^{(k)}}{\\|z^{(k)}\\|_2}$$\nThe vector $x^{(k)}$ is parallel to $A^k x_0$. Based on the analysis above, as $k$ gets large, the sequence of vectors $\\{x^{(k)}\\}$ converges in direction to the dominant eigenvector $v_1$. That is, $x^{(k)}$ becomes nearly parallel to $v_1$.\n\nWe can formulate a stopping criterion by measuring the change between successive iterates. If the method is converging, the direction of $x^{(k-1)}$ and $x^{(k)}$ should become nearly identical. When $x^{(k-1)}$ is already a good approximation of the dominant eigenvector, $x^{(k-1)} \\approx \\pm v_1/\\|v_1\\|_2$, then applying $A$ yields $A x^{(k-1)} \\approx A(\\pm v_1/\\|v_1\\|_2) = \\pm (\\lambda_1/\\|v_1\\|_2) v_1$. Normalizing this vector again results in a vector parallel to $v_1$. Thus, $x^{(k)}$ will also be nearly parallel to $v_1$, and therefore to $x^{(k-1)}$.\n\nThe angle $\\theta_k$ between two successive normalized iterates, $x^{(k-1)}$ and $x^{(k)}$, is a measure of convergence. Since these are unit vectors, the cosine of the angle between them is their inner product:\n$$\\cos(\\theta_k) = (x^{(k-1)})^T x^{(k)}$$\nAs the method converges, the vectors become collinear, meaning $\\theta_k \\to 0$ or $\\theta_k \\to \\pi$. This corresponds to $\\cos(\\theta_k) \\to 1$ or $\\cos(\\theta_k) \\to -1$. The sign of the limit depends on the sign of the dominant eigenvalue $\\lambda_1$. If $\\lambda_1  0$, the iterates will consistently point in the same direction, and $\\cos(\\theta_k) \\to 1$. If $\\lambda_1  0$, the direction will flip at each iteration, and $\\cos(\\theta_k) \\to -1$.\n\nA robust criterion must handle both cases. We can achieve this by considering the absolute value of the cosine, $|\\cos(\\theta_k)| = |(x^{(k-1)})^T x^{(k)}|$. As the method converges, this value approaches $1$. We can stop the iteration when $|\\cos(\\theta_k)|$ is sufficiently close to $1$.\n\nLet's define the deviation from perfect alignment as $\\delta_k = 1 - |\\cos(\\theta_k)| = 1 - |(x^{(k-1)})^T x^{(k)}|$. This quantity is non-negative and approaches $0$ at convergence. The problem specifies a user-defined tolerance $\\tau  0$. A practical and scale-invariant stopping criterion is to terminate the process when this deviation is smaller than the tolerance.\n\nThe stopping criterion is: at iteration $k$ (i.e., after the $k$-th matrix-vector multiplication), compute the normalized iterate $x^{(k)}$ from $x^{(k-1)}$. If the condition\n$$1 - |(x^{(k-1)})^T x^{(k)}|  \\tau$$\nis satisfied, the iteration stops, and the result is $k$. If the criterion is not met within a maximum of $K_{\\max}$ iterations, the method is considered to have not converged, and the result is $-1$. This criterion relies only on the cosine between successive iterates and the tolerance $\\tau$, as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_method(A, x0, tau, K_max):\n    \"\"\"\n    Approximates the dominant eigenvector of a matrix A using the Power Method.\n\n    Args:\n        A (np.ndarray): The square matrix.\n        x0 (np.ndarray): The initial non-zero vector.\n        tau (float): The tolerance for the stopping criterion.\n        K_max (int): The maximum number of iterations.\n\n    Returns:\n        int: The number of iterations until convergence, or -1 if not converged.\n    \"\"\"\n    # Normalize the initial vector x0 using the Euclidean (2-norm).\n    norm_x0 = np.linalg.norm(x0)\n    if norm_x0  1e-12:  # Treat as a zero vector\n        # The problem statement guarantees a non-zero initial vector.\n        # This check is for robustness.\n        return -1\n\n    x_prev = x0 / norm_x0\n\n    for k in range(1, K_max + 1):\n        # Apply the matrix A to the previous iterate\n        y = A @ x_prev\n\n        # Normalize the resulting vector to get the new iterate\n        norm_y = np.linalg.norm(y)\n        if norm_y  1e-12:\n            # If A*x_prev is the zero vector, this means we've converged to\n            # an eigenvector with eigenvalue 0. The next iterate is undefined.\n            # In this specific context, we'll consider this a form of convergence\n            # as the direction won't change further. The cosine would be undefined.\n            # Let's assume the cosine is 1.0, satisfying the criterion for any positive tau.\n            return k\n\n        x_curr = y / norm_y\n\n        # Compute the cosine of the angle between successive normalized iterates.\n        # Since they are unit vectors, this is their dot product.\n        # We take the absolute value to handle cases where the dominant eigenvalue is negative.\n        cosine_val = np.abs(np.dot(x_prev, x_curr))\n\n        # Check the stopping criterion: 1 - |cos(theta)|  tau\n        if (1.0 - cosine_val)  tau:\n            return k\n\n        # Prepare for the next iteration\n        x_prev = x_curr\n\n    # If the loop completes without convergence, return -1.\n    return -1\n\n\ndef solve():\n    \"\"\"\n    Defines the test cases and runs the power method for each, printing the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        (\n            np.array([[5.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 1.0]]),\n            np.array([1.0, 1.0, 1.0]),\n            1e-6,\n            1000\n        ),\n        # Test case 2\n        (\n            np.array([[2.0, 1.0, 0.0], [0.0, 4.0, 1.0], [0.0, 0.0, 1.0]]),\n            np.array([1.0, 2.0, 3.0]),\n            1e-9,\n            5000\n        ),\n        # Test case 3\n        (\n            np.array([[1.0, 0.0], [0.0, 0.99]]),\n            np.array([1.0, 0.1]),\n            1e-6,\n            5000\n        ),\n        # Test case 4\n        (\n            np.array([[2.0, 0.0], [0.0, -2.0]]),\n            np.array([1.0, 1.0]),\n            1e-6,\n            1000\n        )\n    ]\n\n    results = []\n    for A, x0, tau, K_max in test_cases:\n        iteration_count = power_method(A, x0, tau, K_max)\n        results.append(iteration_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\nsolve()\n```", "id": "3283230"}, {"introduction": "The power method's convergence is typically presented for diagonalizable matrices, where it occurs at a geometric rate. This practice [@problem_id:3283226] explores a more nuanced scenario involving a defective matrix, which lacks a full basis of eigenvectors. By analyzing this special case from first principles, you will uncover a different mode of convergence and gain a deeper understanding of how a matrix's fundamental structure governs the algorithm's behavior.", "problem": "Consider the $2 \\times 2$ shear matrix $A$ defined by $A = \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix}$ with real parameter $\\alpha \\neq 0$. The Power Method (PM) for approximating a dominant eigenpair of a matrix generates iterates by repeated multiplication and normalization. Let the initial vector be $x^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, and define the unnormalized iterates by $x^{(k)} = A^{k} x^{(0)}$ and the normalized iterates by $v^{(k)} = \\dfrac{x^{(k)}}{\\|x^{(k)}\\|_{2}}$, where $\\|\\cdot\\|_{2}$ denotes the Euclidean norm. Let $e_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ denote the eigenvector associated with the eigenvalue $1$. Starting from fundamental definitions of matrix powers and the Euclidean norm, derive a closed-form expression for $x^{(k)}$ and $v^{(k)}$, and use this to trace the geometric path of the normalized iterates $v^{(k)}$ on the unit circle. Then, let $\\theta_{k}$ denote the angle in radians between $v^{(k)}$ and $e_{1}$, defined by $\\tan(\\theta_{k}) = \\dfrac{\\text{second component of } v^{(k)}}{\\text{first component of } v^{(k)}}$. Using only foundational properties of matrix multiplication and norms, determine the asymptotic decay of $\\theta_{k}$ and compute the limit $\\lim_{k \\to \\infty} k \\tan(\\theta_{k})$ as a closed-form expression in terms of $\\alpha$. Express your final answer as a single analytical expression. No rounding is required.", "solution": "The problem is well-posed and scientifically sound, resting on fundamental principles of linear algebra and numerical analysis. We can proceed with the solution by following the steps outlined in the problem statement.\n\nFirst, we determine the $k$-th power of the matrix $A = \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix}$. We compute the first few powers to identify a pattern.\n$A^{1} = \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  1\\alpha \\\\ 0  1 \\end{pmatrix}$\n$A^{2} = A \\cdot A = \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + \\alpha \\cdot 0  1 \\cdot \\alpha + \\alpha \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 0  0 \\cdot \\alpha + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1  2\\alpha \\\\ 0  1 \\end{pmatrix}$\n$A^{3} = A^{2} \\cdot A = \\begin{pmatrix} 1  2\\alpha \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  \\alpha + 2\\alpha \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  3\\alpha \\\\ 0  1 \\end{pmatrix}$\nThe pattern is clearly $A^{k} = \\begin{pmatrix} 1  k\\alpha \\\\ 0  1 \\end{pmatrix}$ for any integer $k \\ge 1$. This can be rigorously proven by mathematical induction. The base case for $k=1$ is true. Assuming it holds for some integer $k \\ge 1$, we have $A^{k+1} = A^k A = \\begin{pmatrix} 1  k\\alpha \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  \\alpha + k\\alpha \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  (k+1)\\alpha \\\\ 0  1 \\end{pmatrix}$, which completes the induction. For $k=0$, $A^0=I$ which matches the formula.\n\nNext, we derive the closed-form expression for the unnormalized iterates $x^{(k)} = A^{k} x^{(0)}$. The initial vector is given as $x^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n$$x^{(k)} = A^{k} x^{(0)} = \\begin{pmatrix} 1  k\\alpha \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 0 + k\\alpha \\cdot 1 \\\\ 0 \\cdot 0 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} k\\alpha \\\\ 1 \\end{pmatrix}$$\nThis is the closed-form expression for $x^{(k)}$.\n\nNow, we find the normalized iterates $v^{(k)} = \\dfrac{x^{(k)}}{\\|x^{(k)}\\|_{2}}$. We first compute the Euclidean norm of $x^{(k)}$:\n$$\\|x^{(k)}\\|_{2} = \\left\\| \\begin{pmatrix} k\\alpha \\\\ 1 \\end{pmatrix} \\right\\|_{2} = \\sqrt{(k\\alpha)^2 + 1^2} = \\sqrt{k^2\\alpha^2 + 1}$$\nThus, the normalized iterate $v^{(k)}$ is:\n$$v^{(k)} = \\frac{1}{\\sqrt{k^2\\alpha^2 + 1}} \\begin{pmatrix} k\\alpha \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{k\\alpha}{\\sqrt{k^2\\alpha^2 + 1}} \\\\ \\frac{1}{\\sqrt{k^2\\alpha^2 + 1}} \\end{pmatrix}$$\nThis is the closed-form expression for $v^{(k)}$.\n\nWe trace the geometric path of $v^{(k)}$. By definition, each vector $v^{(k)}$ is a unit vector, so its tip lies on the unit circle in $\\mathbb{R}^2$. The second component of $v^{(k)}$, $\\frac{1}{\\sqrt{k^2\\alpha^2 + 1}}$, is always positive for any $k \\ge 0$. Therefore, all iterates lie on the upper semi-circle.\nFor $k=0$, we have $v^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nAs $k \\to \\infty$, we analyze the components of $v^{(k)}$:\nThe first component: $\\lim_{k \\to \\infty} \\frac{k\\alpha}{\\sqrt{k^2\\alpha^2 + 1}} = \\lim_{k \\to \\infty} \\frac{k\\alpha}{|k\\alpha|\\sqrt{1 + \\frac{1}{k^2\\alpha^2}}} = \\lim_{k \\to \\infty} \\frac{\\alpha}{|\\alpha|\\sqrt{1 + \\frac{1}{k^2\\alpha^2}}} = \\frac{\\alpha}{|\\alpha|} = \\text{sgn}(\\alpha)$.\nThe second component: $\\lim_{k \\to \\infty} \\frac{1}{\\sqrt{k^2\\alpha^2 + 1}} = 0$.\nSo, $\\lim_{k \\to \\infty} v^{(k)} = \\begin{pmatrix} \\text{sgn}(\\alpha) \\\\ 0 \\end{pmatrix}$.\nIf $\\alpha  0$, the iterates $v^{(k)}$ trace a path on the unit circle from $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ towards $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = e_1$.\nIf $\\alpha  0$, the iterates trace a path from $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ towards $\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} = -e_1$.\n\nNext, we use the problem's definition for the angle $\\theta_k$ between $v^{(k)}$ and $e_1$:\n$$\\tan(\\theta_{k}) = \\frac{\\text{second component of } v^{(k)}}{\\text{first component of } v^{(k)}}$$\nSubstituting the components of $v^{(k)}$:\n$$\\tan(\\theta_{k}) = \\frac{\\frac{1}{\\sqrt{k^2\\alpha^2 + 1}}}{\\frac{k\\alpha}{\\sqrt{k^2\\alpha^2 + 1}}} = \\frac{1}{k\\alpha}$$\nNote that this is valid for $k \\ge 1$, since for $k=0$ the first component is $0$.\n\nFrom this expression, we determine the asymptotic decay of $\\theta_k$. As $k \\to \\infty$, $\\tan(\\theta_k) = \\frac{1}{k\\alpha} \\to 0$. This implies that $\\theta_k \\to 0$ if $\\alpha  0$ and $\\theta_k \\to \\pi$ if $\\alpha  0$ (considering the signs of the components of $v^{(k)}$). For large $k$, $\\tan(\\theta_k)$ is small. Using the small-angle approximation $\\tan(x) \\approx x$ for $x \\approx 0$, we find that for $\\alpha0$, $\\theta_k \\approx \\frac{1}{k\\alpha}$. The angle $\\theta_k$ decays to $0$ with an asymptotic rate of $O(k^{-1})$. The problem asks not for $\\theta_k$ itself but its tangent. The asymptotic behavior of $\\tan(\\theta_k)$ is precisely $\\frac{1}{k\\alpha}$.\n\nFinally, we compute the required limit:\n$$\\lim_{k \\to \\infty} k \\tan(\\theta_{k})$$\nUsing our derived expression for $\\tan(\\theta_k)$:\n$$\\lim_{k \\to \\infty} k \\left( \\frac{1}{k\\alpha} \\right) = \\lim_{k \\to \\infty} \\frac{k}{k\\alpha} = \\lim_{k \\to \\infty} \\frac{1}{\\alpha}$$\nSince $\\alpha$ is a non-zero constant, the limit is simply $\\frac{1}{\\alpha}$. This result holds for any real $\\alpha \\neq 0$. This demonstrates that for this defective matrix, the convergence of the direction of the vector iterates is algebraic with rate $O(k^{-1})$, not the geometric rate typically seen in the Power Method for diagonalizable matrices.", "answer": "$$\\boxed{\\frac{1}{\\alpha}}$$", "id": "3283226"}, {"introduction": "While the power method excels at finding the dominant eigenvalue, what if we need to find others? This problem [@problem_id:3283273] introduces you to deflation, a powerful technique for finding sub-dominant eigenvalues by constructing a new matrix where the dominant mode has been removed. More importantly, you will investigate the numerical stability of this process, analyzing how small errors in your first eigenvector estimate can impact the accuracy of the next, a crucial lesson in the practical application of numerical methods.", "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with eigenpairs $\\{(\\lambda_i, v_i)\\}_{i=1}^n$ satisfying $A v_i = \\lambda_i v_i$, where the eigenvectors $\\{v_i\\}$ form an orthonormal basis and the eigenvalues are ordered $|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|$. The power method seeks the dominant eigenpair by iterating a nonzero vector $x_k$ according to $x_{k+1} = A x_k / \\|A x_k\\|_2$ and estimating the eigenvalue via the Rayleigh quotient $x_k^\\top A x_k / (x_k^\\top x_k)$. Deflation removes the contribution of the dominant eigenpair from $A$ to expose the next eigenvalue. If $(\\lambda_1, v_1)$ is known, a deflated matrix can be constructed as $A_1 = A - \\lambda_1 v_1 v_1^\\top$, for which $v_1$ becomes a null vector and the largest eigenvalue of $A_1$ is ideally $\\lambda_2$. In practice, $v_1$ is approximated and the deflation is imperfect.\n\nStarting from the fundamental definitions above, implement the power method to obtain an approximate dominant eigenpair and then perform rank-one deflation to estimate the second largest eigenvalue. Analyze the sensitivity of the deflation to an angular error in the approximation of $v_1$.\n\nAlgorithmic requirements:\n\n- Implement a function that, given a real symmetric matrix $A$, an initial vector $x_0 \\ne 0$, a tolerance $\\varepsilon  0$, and a maximum iteration cap $K_{\\max}$, iterates $x_{k+1} = A x_k / \\|A x_k\\|_2$ until successive Rayleigh quotients differ by less than $\\varepsilon$ or until $K_{\\max}$ iterations are reached. Return the final Rayleigh quotient and the normalized vector.\n- Implement rank-one deflation: given a scalar $\\mu$ and a unit vector $u$, form $\\tilde{A} = A - \\mu\\, u u^\\top$.\n- For sensitivity analysis, construct a perturbed dominant direction $\\hat{v}_1(\\theta) = \\mathrm{normalize}(\\cos \\theta\\, v_1 + \\sin \\theta\\, v_2)$, where the angle $\\theta$ is in radians. Use $\\mu(\\theta) = \\hat{v}_1(\\theta)^\\top A \\hat{v}_1(\\theta)$ in the deflation to simulate the practical use of the Rayleigh quotient associated with the approximate vector.\n\nTest suite:\n\nUse the following $4 \\times 4$ symmetric matrices and angles (in radians), initial vector $x_0$ as the all-ones vector, tolerance $\\varepsilon = 10^{-12}$, and $K_{\\max} = 10000$.\n\n- Matrix $A_{\\mathrm{easy}}$:\n$$\nA_{\\mathrm{easy}} = \\begin{bmatrix}\n4  1  0  0 \\\\\n1  3  1  0 \\\\\n0  1  2  1 \\\\\n0  0  1  1\n\\end{bmatrix}.\n$$\n- Matrix $A_{\\mathrm{close}}$:\n$$\nA_{\\mathrm{close}} = \\begin{bmatrix}\n2  1  0  0 \\\\\n1  2  1  0 \\\\\n0  1  2  1 \\\\\n0  0  1  2\n\\end{bmatrix}.\n$$\n\nAngles for constructing $\\hat{v}_1(\\theta)$: $\\theta \\in \\{0, 0.05, 0.2\\}$ (in radians). For each matrix $A \\in \\{A_{\\mathrm{easy}}, A_{\\mathrm{close}}\\}$ and each angle $\\theta$, do the following:\n\n1. Compute the true second largest eigenvalue $\\lambda_2$ of $A$ by any correct numerical means.\n2. Form $\\hat{v}_1(\\theta) = \\mathrm{normalize}(\\cos \\theta\\, v_1 + \\sin \\theta\\, v_2)$ using the true $v_1$ and $v_2$ of $A$.\n3. Compute $\\mu(\\theta) = \\hat{v}_1(\\theta)^\\top A \\hat{v}_1(\\theta)$.\n4. Deflate: $\\tilde{A}(\\theta) = A - \\mu(\\theta)\\, \\hat{v}_1(\\theta)\\hat{v}_1(\\theta)^\\top$.\n5. Apply the power method to $\\tilde{A}(\\theta)$ starting from $x_0$ to estimate its dominant eigenvalue, denoted $\\tilde{\\lambda}_1(\\theta)$, which serves as the estimate of $\\lambda_2$ of $A$ under imperfect deflation.\n6. Report the absolute error $|\\tilde{\\lambda}_1(\\theta) - \\lambda_2|$ as a floating-point number.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as\n$$\n\\big[ e(A_{\\mathrm{easy}}, 0), e(A_{\\mathrm{easy}}, 0.05), e(A_{\\mathrm{easy}}, 0.2), e(A_{\\mathrm{close}}, 0), e(A_{\\mathrm{close}}, 0.05), e(A_{\\mathrm{close}}, 0.2) \\big],\n$$\nwhere $e(A,\\theta) = |\\tilde{\\lambda}_1(\\theta) - \\lambda_2|$. Express each floating-point value rounded to $10$ decimal places. Angles are specified in radians. No physical units are involved.", "solution": "The user has provided a problem in numerical linear algebra concerning the power method and rank-one deflation. The task is to implement these methods and analyze the sensitivity of the deflation process to errors in the approximation of the dominant eigenvector.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Matrices:** Two real symmetric matrices, $A_{\\mathrm{easy}}$ and $A_{\\mathrm{close}}$, of size $4 \\times 4$.\n$$\nA_{\\mathrm{easy}} = \\begin{bmatrix}\n4  1  0  0 \\\\\n1  3  1  0 \\\\\n0  1  2  1 \\\\\n0  0  1  1\n\\end{bmatrix}, \\quad\nA_{\\mathrm{close}} = \\begin{bmatrix}\n2  1  0  0 \\\\\n1  2  1  0 \\\\\n0  1  2  1 \\\\\n0  0  1  2\n\\end{bmatrix}\n$$\n- **Eigenpairs:** A real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ has eigenpairs $\\{(\\lambda_i, v_i)\\}_{i=1}^n$ where $A v_i = \\lambda_i v_i$. The eigenvectors $\\{v_i\\}$ form an orthonormal basis, and eigenvalues are ordered by magnitude: $|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|$.\n- **Power Method Algorithm:** An iterative method to find the dominant eigenpair. The iteration is defined as $x_{k+1} = A x_k / \\|A x_k\\|_2$. The eigenvalue is estimated via the Rayleigh quotient, $R(x_k) = x_k^\\top A x_k / (x_k^\\top x_k)$.\n- **Power Method Implementation Parameters:**\n    - Initial vector for the deflated matrix: $x_0 = [1, 1, 1, 1]^\\top$.\n    - Tolerance: $\\varepsilon = 10^{-12}$.\n    - Maximum iterations: $K_{\\max} = 10000$.\n- **Rank-One Deflation:** A procedure to form a new matrix $A_1 = A - \\lambda_1 v_1 v_1^\\top$ whose dominant eigenvalue is $\\lambda_2$.\n- **Sensitivity Analysis:**\n    - A perturbed dominant eigenvector is constructed: $\\hat{v}_1(\\theta) = \\mathrm{normalize}(\\cos \\theta\\, v_1 + \\sin \\theta\\, v_2)$, where $v_1$ and $v_2$ are the true dominant eigenvectors of $A$.\n    - The corresponding approximate eigenvalue is $\\mu(\\theta) = \\hat{v}_1(\\theta)^\\top A \\hat{v}_1(\\theta)$.\n    - The deflated matrix uses these approximations: $\\tilde{A}(\\theta) = A - \\mu(\\theta)\\, \\hat{v}_1(\\theta)\\hat{v}_1(\\theta)^\\top$.\n    - The perturbation angles are specified as $\\theta \\in \\{0, 0.05, 0.2\\}$ radians.\n- **Task:** For each matrix and each angle, calculate the absolute error $e(A,\\theta) = |\\tilde{\\lambda}_1(\\theta) - \\lambda_2|$, where $\\tilde{\\lambda}_1(\\theta)$ is the dominant eigenvalue of $\\tilde{A}(\\theta)$ found using the power method.\n- **Output Format:** A list of errors for $(A_{\\mathrm{easy}}, \\theta=0)$, $(A_{\\mathrm{easy}}, \\theta=0.05)$, $(A_{\\mathrm{easy}}, \\theta=0.2)$, $(A_{\\mathrm{close}}, \\theta=0)$, $(A_{\\mathrm{close}}, \\theta=0.05)$, and $(A_{\\mathrm{close}}, \\theta=0.2)$, with each value rounded to $10$ decimal places.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is firmly rooted in numerical linear algebra. The power method and Hotelling's deflation are standard, well-documented algorithms. The sensitivity analysis is a legitimate and important part of studying numerical methods.\n- **Well-Posedness:** The problem specifies all necessary inputs: matrices, parameters, initial vectors, and the procedure for analysis. The power method for a real symmetric matrix with a unique dominant eigenvalue in magnitude ($|\\lambda_1|  |\\lambda_2|$) is guaranteed to converge, a condition that holds for both test matrices. The requested output is a set of definite numerical values.\n- **Objectivity:** The problem is stated using precise mathematical language, free from ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is scientifically sound, well-posed, objective, and complete. All conditions for a valid problem are met. I will proceed with formulating and presenting the solution.\n\n### Solution\n\nThe problem requires a numerical investigation into the stability of rank-one deflation when the dominant eigenpair is known only approximately. We will first describe the underlying principles of the power method and deflation, then outline the computational steps to perform the specified sensitivity analysis.\n\n**1. The Power Method**\n\nThe power method is an iterative algorithm to find the eigenpair $(\\lambda_1, v_1)$ of a matrix $A$ corresponding to the eigenvalue $\\lambda_1$ with the largest magnitude. Starting with a non-zero vector $x_0$ that is not orthogonal to $v_1$, the method proceeds as:\n$$\ny_{k+1} = A x_k, \\qquad x_{k+1} = \\frac{y_{k+1}}{\\|y_{k+1}\\|_2}\n$$\nAs $k \\to \\infty$, the vector $x_k$ converges to the dominant eigenvector $v_1$ (or $-v_1$), and the Rayleigh quotient $R(x_k) = x_k^\\top A x_k$ converges to the dominant eigenvalue $\\lambda_1$, since $x_k$ is a unit vector.\n\nThe convergence is monitored by the change in the Rayleigh quotient between successive iterations. The algorithm terminates when $|\\lambda^{(k+1)} - \\lambda^{(k)}|  \\varepsilon$ or a maximum number of iterations $K_{\\max}$ is reached, where $\\lambda^{(k)} = R(x_k)$.\n\n**2. Rank-One Deflation**\n\nOnce the dominant eigenpair $(\\lambda_1, v_1)$ of a symmetric matrix $A$ is found, we can construct a new matrix $A_1$ whose eigenvalues are $\\{0, \\lambda_2, \\lambda_3, \\dots, \\lambda_n\\}$. This is achieved through rank-one deflation (also known as Hotelling's deflation):\n$$\nA_1 = A - \\lambda_1 v_1 v_1^\\top\n$$\nThe properties of $A_1$ are as follows:\n- For the eigenvector $v_1$:\n$A_1 v_1 = A v_1 - \\lambda_1 v_1 (v_1^\\top v_1) = \\lambda_1 v_1 - \\lambda_1 v_1 (1) = 0$. Thus, $v_1$ is an eigenvector of $A_1$ with eigenvalue $0$.\n- For any other eigenvector $v_i$ (where $i \\ne 1$):\nSince $A$ is symmetric, its eigenvectors are orthogonal, so $v_1^\\top v_i = 0$.\n$A_1 v_i = A v_i - \\lambda_1 v_1 (v_1^\\top v_i) = \\lambda_i v_i - \\lambda_1 v_1 (0) = \\lambda_i v_i$.\nThe eigenpairs $(\\lambda_i, v_i)$ for $i=2, \\dots, n$ are preserved. The dominant eigenvalue of $A_1$ is now $\\lambda_2$, which can be found by applying the power method to $A_1$.\n\n**3. Sensitivity to Perturbations**\n\nIn practice, the true eigenpair $(\\lambda_1, v_1)$ is not known exactly. Instead, an approximation $(\\mu, u)$ is obtained. The deflated matrix is then $\\tilde{A} = A - \\mu u u^\\top$. The problem models this scenario by introducing a controlled perturbation in the dominant eigenvector. The approximate vector $u$ is given by a mixture of the first two true eigenvectors, $v_1$ and $v_2$:\n$$\n\\hat{v}_1(\\theta) = \\cos\\theta\\, v_1 + \\sin\\theta\\, v_2\n$$\nSince $v_1$ and $v_2$ are orthonormal, $\\|\\hat{v}_1(\\theta)\\|_2 = \\sqrt{\\cos^2\\theta \\|v_1\\|_2^2 + \\sin^2\\theta \\|v_2\\|_2^2} = 1$, so this vector is already normalized. The angle $\\theta$ represents the angular error in the approximation. The approximate eigenvalue $\\mu$ is the Rayleigh quotient evaluated at this perturbed vector:\n$$\n\\mu(\\theta) = \\hat{v}_1(\\theta)^\\top A \\hat{v}_1(\\theta) = (\\cos\\theta v_1 + \\sin\\theta v_2)^\\top (\\lambda_1 \\cos\\theta v_1 + \\lambda_2 \\sin\\theta v_2) = \\lambda_1 \\cos^2\\theta + \\lambda_2 \\sin^2\\theta\n$$\nThe deflated matrix is then $\\tilde{A}(\\theta) = A - \\mu(\\theta) \\hat{v}_1(\\theta) \\hat{v}_1(\\theta)^\\top$. When $\\theta=0$, we have perfect deflation, and the dominant eigenvalue of $\\tilde{A}(0)$ is exactly $\\lambda_2$. For $\\theta  0$, the eigenstructure of $\\tilde{A}(\\theta)$ is perturbed, and its dominant eigenvalue $\\tilde{\\lambda}_1(\\theta)$ will only be an approximation of $\\lambda_2$. We expect the error $|\\tilde{\\lambda}_1(\\theta) - \\lambda_2|$ to increase with $\\theta$.\n\n**4. Execution Plan**\n\nThe required computations will proceed as follows for each matrix $A \\in \\{A_{\\mathrm{easy}}, A_{\\mathrm{close}}\\}$ and each angle $\\theta \\in \\{0, 0.05, 0.2\\}$:\n1.  Compute the full eigensystem of $A$ to obtain the true eigenvalues $\\{\\lambda_i\\}$ and eigenvectors $\\{v_i\\}$. We sort them such that $|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots$.\n2.  Extract $\\lambda_2$, $v_1$, and $v_2$.\n3.  Construct the perturbed vector $\\hat{v}_1(\\theta) = \\cos\\theta\\, v_1 + \\sin\\theta\\, v_2$.\n4.  Compute the corresponding Rayleigh quotient $\\mu(\\theta) = \\hat{v}_1(\\theta)^\\top A \\hat{v}_1(\\theta)$.\n5.  Form the imperfectly deflated matrix $\\tilde{A}(\\theta) = A - \\mu(\\theta) \\hat{v}_1(\\theta) \\hat{v}_1(\\theta)^\\top$.\n6.  Apply the power method to $\\tilde{A}(\\theta)$, with initial vector $x_0 = [1, 1, 1, 1]^\\top$, tolerance $\\varepsilon = 10^{-12}$, and $K_{\\max}=10000$, to find its dominant eigenvalue, $\\tilde{\\lambda}_1(\\theta)$.\n7.  Calculate the absolute error $e(A,\\theta) = |\\tilde{\\lambda}_1(\\theta) - \\lambda_2|$.\nAll computed errors are collected and presented in the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_method(A, x0, tol, max_iter):\n    \"\"\"\n    Implements the power method to find the dominant eigenvalue and eigenvector.\n\n    Args:\n        A (np.ndarray): The matrix.\n        x0 (np.ndarray): The initial vector.\n        tol (float): The tolerance for convergence.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing the dominant eigenvalue (float) and\n               the corresponding normalized eigenvector (np.ndarray).\n    \"\"\"\n    if np.linalg.norm(x0) == 0:\n        raise ValueError(\"Initial vector x0 cannot be the zero vector.\")\n    \n    x = x0 / np.linalg.norm(x0)\n    \n    # Calculate initial Rayleigh quotient. Since x is normalized, x.T @ x = 1.\n    lambda_prev = x.T @ A @ x\n    \n    for _ in range(max_iter):\n        Ax = A @ x\n        norm_Ax = np.linalg.norm(Ax)\n        if norm_Ax == 0:\n            # This can happen if the dominant eigenvalue is 0\n            return 0.0, x\n        \n        x = Ax / norm_Ax\n        \n        lambda_curr = x.T @ A @ x\n        \n        if np.abs(lambda_curr - lambda_prev)  tol:\n            return lambda_curr, x\n            \n        lambda_prev = lambda_curr\n        \n    return lambda_curr, x\n\ndef solve():\n    \"\"\"\n    Main function to execute the problem's test suite and print the results.\n    \"\"\"\n    # Define matrices from the problem statement\n    A_easy = np.array([\n        [4, 1, 0, 0],\n        [1, 3, 1, 0],\n        [0, 1, 2, 1],\n        [0, 0, 1, 1]\n    ], dtype=float)\n\n    A_close = np.array([\n        [2, 1, 0, 0],\n        [1, 2, 1, 0],\n        [0, 1, 2, 1],\n        [0, 0, 1, 2]\n    ], dtype=float)\n\n    # Define test parameters\n    test_cases = [\n        (A_easy, \"A_easy\"),\n        (A_close, \"A_close\")\n    ]\n    thetas = [0.0, 0.05, 0.2]  # Angles in radians\n    x0 = np.ones(4)\n    tolerance = 1e-12\n    max_iterations = 10000\n\n    results = []\n\n    for A, name in test_cases:\n        # 1. Compute true eigenpairs of A\n        # np.linalg.eigh returns eigenvalues in ascending order.\n        eigvals_asc, eigvecs_mat = np.linalg.eigh(A)\n        \n        # Reverse to get descending order of eigenvalues by magnitude\n        # For positive definite matrices, this is the same as descending order.\n        true_eigvals = eigvals_asc[::-1]\n        true_eigvecs = eigvecs_mat[:, ::-1]\n\n        # Extract dominant and sub-dominant pairs\n        lambda_1_true = true_eigvals[0]\n        lambda_2_true = true_eigvals[1]\n        v1_true = true_eigvecs[:, 0]\n        v2_true = true_eigvecs[:, 1]\n\n        for theta in thetas:\n            # 2. Form the perturbed vector v1_hat(theta)\n            # Since v1 and v2 are orthonormal, the result is already normalized.\n            v1_hat = np.cos(theta) * v1_true + np.sin(theta) * v2_true\n            \n            # 3. Compute the Rayleigh quotient mu(theta)\n            mu_theta = v1_hat.T @ A @ v1_hat\n            \n            # 4. Deflate the matrix A\n            A_tilde = A - mu_theta * np.outer(v1_hat, v1_hat)\n            \n            # 5. Apply power method to the deflated matrix A_tilde\n            lambda_tilde_1, _ = power_method(A_tilde, x0, tolerance, max_iterations)\n            \n            # 6. Report the absolute error\n            error = np.abs(lambda_tilde_1 - lambda_2_true)\n            results.append(error)\n\n    # Format and print the final output\n    formatted_results = [f\"{r:.10f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3283273"}]}