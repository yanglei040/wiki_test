{"hands_on_practices": [{"introduction": "To truly grasp Newton's method, we must begin with its fundamental mechanics. This first practice exercise guides you through a single, complete iteration of the method for a two-dimensional system. By manually calculating the function vector $\\mathbf{F}(\\mathbf{x})$, the Jacobian matrix $J(\\mathbf{x})$, and the resulting update step, you will build a concrete understanding of the algorithm's core components before moving on to more complex implementations [@problem_id:2190496].", "problem": "An autonomous robot is programmed to navigate to a target location $(x, y)$ on a 2D plane. The target is defined as the intersection of two signal-defined paths. The equations for these paths are given by the following system of non-linear equations:\n$$\n\\begin{cases}\n    x^2 - y + \\cos(y) - 3 = 0 \\\\\n    x + y + \\sin(x) - 2 = 0\n\\end{cases}\n$$\nThe robot's current position is $\\mathbf{x}_0 = (x_0, y_0) = (1.5, 0.5)$. The coordinate system is measured in meters. To find the target, the robot's navigation algorithm will use Newton's method for systems of non-linear equations.\n\nPerform exactly one iteration of Newton's method to find the robot's next estimated position, $\\mathbf{x}_1 = (x_1, y_1)$. Assume that all arguments of trigonometric functions are given in radians.\n\nProvide the components of the updated position vector $\\mathbf{x}_1$. Express your answer for each component in meters, rounded to three significant figures.", "solution": "We frame the system as $\\mathbf{F}(x,y) = \\begin{pmatrix} f_{1}(x,y) \\\\ f_{2}(x,y) \\end{pmatrix}$ with\n$$\nf_{1}(x,y) = x^{2} - y + \\cos(y) - 3,\\quad f_{2}(x,y) = x + y + \\sin(x) - 2.\n$$\nNewton’s method for systems updates $\\mathbf{x}_{1} = \\mathbf{x}_{0} - J(\\mathbf{x}_{0})^{-1}\\mathbf{F}(\\mathbf{x}_{0})$, equivalently solve $J(\\mathbf{x}_{0})\\,\\mathbf{s} = -\\mathbf{F}(\\mathbf{x}_{0})$ for $\\mathbf{s}$ and set $\\mathbf{x}_{1} = \\mathbf{x}_{0} + \\mathbf{s}$.\n\nThe Jacobian is\n$$\nJ(x,y) = \\begin{pmatrix}\n\\frac{\\partial f_{1}}{\\partial x}  \\frac{\\partial f_{1}}{\\partial y} \\\\\n\\frac{\\partial f_{2}}{\\partial x}  \\frac{\\partial f_{2}}{\\partial y}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2x  -1 - \\sin(y) \\\\\n1 + \\cos(x)  1\n\\end{pmatrix}.\n$$\nAt $\\mathbf{x}_{0} = (x_{0},y_{0}) = (1.5, 0.5)$ (radians for trigonometric arguments),\n$$\n\\sin(0.5) \\approx 0.4794255386,\\quad \\cos(0.5) \\approx 0.8775825620,\\quad \\sin(1.5) \\approx 0.9974949866,\\quad \\cos(1.5) \\approx 0.0707372017.\n$$\nEvaluate the function:\n$$\n\\mathbf{F}(\\mathbf{x}_{0}) = \\begin{pmatrix}\n1.5^{2} - 0.5 + \\cos(0.5) - 3 \\\\\n1.5 + 0.5 + \\sin(1.5) - 2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-0.372417438 \\\\\n0.9974949866\n\\end{pmatrix}.\n$$\nEvaluate the Jacobian:\n$$\nJ(\\mathbf{x}_{0}) = \\begin{pmatrix}\n3  -1 - \\sin(0.5) \\\\\n1 + \\cos(1.5)  1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3  -1.4794255386 \\\\\n1.0707372017  1\n\\end{pmatrix}.\n$$\nSet up and solve $J(\\mathbf{x}_{0})\\,\\mathbf{s} = -\\mathbf{F}(\\mathbf{x}_{0})$:\n$$\n\\begin{pmatrix}\n3  -1.4794255386 \\\\\n1.0707372017  1\n\\end{pmatrix}\n\\begin{pmatrix}\ns_{1} \\\\ s_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.372417438 \\\\\n-0.9974949866\n\\end{pmatrix}.\n$$\nFrom the second equation, $s_{2} = -0.9974949866 - 1.0707372017\\,s_{1}$. Substitute into the first:\n$$\n(3 + 1.4794255386 \\cdot 1.0707372017)\\,s_{1} + 1.4794255386 \\cdot 0.9974949866 = 0.372417438,\n$$\nso\n$$\ns_{1} = \\frac{0.372417438 - 1.4794255386 \\cdot 0.9974949866}{3 + 1.4794255386 \\cdot 1.0707372017} \\approx \\frac{-1.103302119}{4.5840759613} \\approx -0.240682.\n$$\nThen\n$$\ns_{2} = -0.9974949866 - 1.0707372017\\,(-0.240682) \\approx -0.739788.\n$$\nUpdate the estimate:\n$$\nx_{1} = x_{0} + s_{1} \\approx 1.5 - 0.240682 = 1.259318,\\quad\ny_{1} = y_{0} + s_{2} \\approx 0.5 - 0.739788 = -0.239788.\n$$\nRounded to three significant figures (meters): $x_{1} \\approx 1.26$, $y_{1} \\approx -0.240$.", "answer": "$$\\boxed{\\begin{pmatrix} 1.26 \\\\ -0.240 \\end{pmatrix}}$$", "id": "2190496"}, {"introduction": "The standard Newton's method, with its full step size, converges remarkably fast when close to a solution but can easily fail when starting from a poor initial guess. This hands-on coding practice demonstrates this exact scenario and introduces the essential technique of a damped Newton's method to fix it. By implementing and comparing both the standard and damped versions, you will see why a backtracking line search is a critical tool for creating a robust and globally convergent solver [@problem_id:3255431].", "problem": "You must write a complete, runnable program that implements a damped Newton's method using a backtracking line search for solving nonlinear systems of equations. The goal is to demonstrate how damping via line search can achieve convergence for a system where the standard full-step Newton's method fails to converge from a poor initial guess. The fundamental base is the following: given a continuously differentiable nonlinear mapping $F:\\mathbb{R}^n\\to\\mathbb{R}^n$, a root is a vector $x^\\star\\in\\mathbb{R}^n$ such that $F(x^\\star)=0$. The first-order Taylor expansion gives $F(x+p)\\approx F(x)+J(x)p$, where $J(x)$ denotes the Jacobian matrix of $F$ at $x$. Use this approximation to determine a search direction at each iteration, and then apply a backtracking line search that ensures a sufficient decrease of the merit function $\\phi(x)=\\frac{1}{2}\\lVert F(x)\\rVert_2^2$. The sufficient decrease criterion must enforce a bound of the form $\\phi(x+\\alpha p)\\le \\phi(x)+\\sigma\\alpha\\,\\nabla\\phi(x)\\cdot p$ for a fixed constant $\\sigma\\in(0,1)$ and step size $\\alpha\\in(0,1]$, where $\\nabla\\phi(x)$ is the gradient of $\\phi$.\n\nYour program must implement two solvers:\n- A standard full-step Newton's method that always takes the step size $\\alpha=1$.\n- A damped Newton's method with backtracking line search that reduces the step size $\\alpha$ multiplicatively until the sufficient decrease condition is met.\n\nYou must solve the following systems with the specified initial points, tolerances, and limits. For each system, use analytically correct mappings $F(x)$ and Jacobians $J(x)$:\n\n1. Rosenbrock-gradient system. Define the Rosenbrock function $f:\\mathbb{R}^2\\to\\mathbb{R}$ by $f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2$ and let $F_R(x)=\\nabla f(x)$, explicitly\n$$\nF_R(x)=\\begin{pmatrix}\n2(x_1-1)-400x_1(x_2-x_1^2)\\\\\n200(x_2-x_1^2)\n\\end{pmatrix}.\n$$\nIts Jacobian $J_R(x)$ is the Hessian of $f$,\n$$\nJ_R(x)=\\begin{pmatrix}\n2 - 400(x_2 - x_1^2) + 800 x_1^2  -400 x_1\\\\\n-400 x_1  200\n\\end{pmatrix}.\n$$\nUse initial point $x^{(0)}=(-1.2,\\,1.0)$, tolerance $\\varepsilon=10^{-10}$ for the stopping criterion $\\lVert F(x^{(k)})\\rVert_2\\le\\varepsilon$, and a maximum of $N_{\\max}=50$ iterations. Use line search parameters $\\sigma=10^{-4}$, reduction factor $\\beta=\\tfrac{1}{2}$, and a minimum step size $\\alpha_{\\min}=10^{-12}$. This case is intended to demonstrate a scenario where the standard full-step Newton method fails to converge from a poor initial guess while the damped method with line search successfully converges.\n\n2. Rosenbrock-gradient system near the solution. Use the same $F_R(x)$ and $J_R(x)$ as above, with initial point $x^{(0)}=(1.2,\\,1.2)$, tolerance $\\varepsilon=10^{-10}$, $N_{\\max}=50$, and line search parameters $\\sigma=10^{-4}$, $\\beta=\\tfrac{1}{2}$, $\\alpha_{\\min}=10^{-12}$. This case is intended to demonstrate a scenario where both methods converge when starting sufficiently close to the solution.\n\n3. Near-singular Jacobian at the start. Define\n$$\nF_S(x)=\\begin{pmatrix}\nx_1 x_2\\\\\nx_1^2 - x_2\n\\end{pmatrix},\\quad\nJ_S(x)=\\begin{pmatrix}\nx_2  x_1\\\\\n2x_1  -1\n\\end{pmatrix}.\n$$\nNote that $J_S(0,0)$ is singular. Use initial point $x^{(0)}=(10^{-4},\\,10^{-4})$, tolerance $\\varepsilon=10^{-8}$, $N_{\\max}=100$, and line search parameters $\\sigma=10^{-4}$, $\\beta=\\tfrac{1}{2}$, $\\alpha_{\\min}=10^{-12}$. This case is intended to test robustness of the implementation near a singular Jacobian.\n\nImplementation requirements:\n- At each iteration, compute the search direction $p$ by solving the linear system $J(x)p=-F(x)$ using a numerically stable method. If the Jacobian is singular or ill-conditioned, compute a least-squares solution to determine $p$ that minimizes $\\lVert J(x)p+F(x)\\rVert_2$.\n- For the damped method, perform a backtracking line search on $\\alpha$ until the sufficient decrease condition is satisfied or $\\alpha$ falls below $\\alpha_{\\min}$.\n- Stop when $\\lVert F(x)\\rVert_2\\le\\varepsilon$ or when $N_{\\max}$ iterations are reached.\n\nTest suite and required outputs:\n- Case A (Rosenbrock-gradient with $x^{(0)}=(-1.2,\\,1.0)$, $\\varepsilon=10^{-10}$, $N_{\\max}=50$): Output a boolean that is $true$ if and only if the damped method converges within $N_{\\max}$ iterations and the full-step method fails to converge within $N_{\\max}$ iterations under the same tolerance.\n- Case B (Rosenbrock-gradient with $x^{(0)}=(1.2,\\,1.2)$, $\\varepsilon=10^{-10}$, $N_{\\max}=50$): Output a boolean that is $true$ if and only if both methods converge within $N_{\\max}$ iterations under the same tolerance.\n- Case C (Near-singular system with $x^{(0)}=(10^{-4},\\,10^{-4})$, $\\varepsilon=10^{-8}$, $N_{\\max}=100$): Output a boolean that is $true$ if and only if the damped method converges within $N_{\\max}$ iterations under the specified tolerance and its final merit value $\\phi(x)$ is less than or equal to that of the full-step method at termination.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC]\") where each result is the boolean defined above for its corresponding case.", "solution": "The user requests the implementation and comparison of two variants of Newton's method for solving systems of nonlinear equations, $F(x) = 0$. The first is the standard full-step Newton's method, and the second is a damped version employing a backtracking line search to ensure convergence from a wider range of initial guesses.\n\n### Problem Formulation\n\nA nonlinear system of equations is given by a function $F:\\mathbb{R}^n \\to \\mathbb{R}^n$. We seek a root $x^\\star$ such that $F(x^\\star) = 0$. Newton's method is an iterative process that, given an iterate $x^{(k)}$, finds the next iterate $x^{(k+1)}$ by approximating $F$ with its first-order Taylor expansion around $x^{(k)}$:\n$$\nF(x^{(k)} + p) \\approx F(x^{(k)}) + J(x^{(k)})p\n$$\nwhere $J(x^{(k)})$ is the Jacobian matrix of $F$ at $x^{(k)}$. To find a root of the approximation, we set the right-hand side to zero and solve for the step $p$, called the Newton step:\n$$\nJ(x^{(k)})p^{(k)} = -F(x^{(k)})\n$$\nThis is a system of linear equations for the search direction $p^{(k)}$.\n\n### Standard vs. Damped Newton's Method\n\n**1. Standard Full-Step Newton's Method:**\nThe next iterate is always computed by taking the full Newton step:\n$$\nx^{(k+1)} = x^{(k)} + p^{(k)}\n$$\nThis is equivalent to setting the step size $\\alpha=1$. This method exhibits local quadratic convergence but may fail to converge (i.e., diverge or oscillate) if the initial guess $x^{(0)}$ is not sufficiently close to the solution $x^\\star$.\n\n**2. Damped Newton's Method with Backtracking Line Search:**\nTo globalize the convergence, a step size $\\alpha \\in (0, 1]$ is introduced:\n$$\nx^{(k+1)} = x^{(k)} + \\alpha p^{(k)}\n$$\nThe step size $\\alpha$ is chosen to ensure a sufficient decrease in a merit function, which measures how far we are from a solution. A common choice for the merit function is $\\phi(x) = \\frac{1}{2}\\lVert F(x)\\rVert_2^2$. The goal is to find an $\\alpha$ that satisfies the Armijo-Goldstein sufficient decrease condition:\n$$\n\\phi(x^{(k)} + \\alpha p^{(k)}) \\le \\phi(x^{(k)}) + \\sigma \\alpha \\nabla\\phi(x^{(k)})^T p^{(k)}\n$$\nfor a small constant $\\sigma \\in (0, 1)$, typically $\\sigma = 10^{-4}$.\n\nThe gradient of the merit function is $\\nabla\\phi(x) = J(x)^T F(x)$. The directional derivative in the direction $p^{(k)}$ is:\n$$\n\\nabla\\phi(x^{(k)})^T p^{(k)} = (J(x^{(k)})^T F(x^{(k)}))^T p^{(k)} = F(x^{(k)})^T J(x^{(k)}) p^{(k)}\n$$\nSubstituting $J(x^{(k)})p^{(k)} = -F(x^{(k)})$, we get:\n$$\n\\nabla\\phi(x^{(k)})^T p^{(k)} = F(x^{(k)})^T (-F(x^{(k)})) = -\\lVert F(x^{(k)}) \\rVert_2^2\n$$\nThis shows that the Newton direction $p^{(k)}$ is a descent direction for the merit function $\\phi(x)$, provided $J(x^{(k)})$ is non-singular. The sufficient decrease condition simplifies to:\n$$\n\\phi(x^{(k)} + \\alpha p^{(k)}) \\le \\phi(x^{(k)}) - \\sigma \\alpha \\lVert F(x^{(k)}) \\rVert_2^2\n$$\nThe backtracking line search algorithm starts with $\\alpha=1$ and successively reduces it by a factor $\\beta \\in (0,1)$ (e.g., $\\beta = \\frac{1}{2}$) until this condition is met or $\\alpha$ becomes smaller than a prescribed minimum $\\alpha_{\\min}$.\n\n### Algorithmic Implementation\n\nA robust solver is developed to encapsulate both methods. It accepts the system's function $F$, its Jacobian $J$, an initial guess $x^{(0)}$, and control parameters.\n\n**Core Steps within the Solver:**\n1.  Initialize the iteration counter $k=0$ and the solution vector $x = x^{(0)}$.\n2.  Begin the main loop, which continues as long as the iteration count $k  N_{\\max}$.\n3.  At each iteration, evaluate $F(x)$ and check for convergence: if $\\lVert F(x) \\rVert_2 \\le \\varepsilon$, the process terminates successfully.\n4.  Evaluate the Jacobian $J(x)$.\n5.  Solve the linear system $J(x) p = -F(x)$ for the search direction $p$. To handle cases where $J(x)$ is singular or ill-conditioned, we use a least-squares solver, which finds the $p$ that minimizes $\\lVert J(x)p + F(x) \\rVert_2$.\n6.  If damping is enabled, perform a backtracking line search:\n    a. Start with $\\alpha = 1$.\n    b. Repeatedly check the sufficient decrease condition.\n    c. If the condition fails, update $\\alpha \\leftarrow \\beta\\alpha$.\n    d. If $\\alpha$ falls below $\\alpha_{\\min}$, the search is considered stalled, and the iteration halts.\n7.  If damping is disabled, simply set $\\alpha=1$.\n8.  Update the solution: $x \\leftarrow x + \\alpha p$.\n9.  Increment the iteration counter $k \\leftarrow k+1$.\n10. If the loop completes without convergence, the method has failed. The final state and a failure status are returned.\n\n### Test Cases Analysis\n\n**Case A  B: Rosenbrock-gradient System**\nThis system is derived from the gradient of the classic Rosenbrock function, a difficult non-convex optimization benchmark. The solution is at $x^\\star = (1, 1)$, where $F_R(x^\\star) = 0$.\n-   **Case A (poor initial guess):** Starting from $x^{(0)}=(-1.2,\\,1.0)$, the initial iterate is far from the solution. The local quadratic approximation of Newton's method is inaccurate, causing the full-step method to take large, non-productive steps, leading to divergence or oscillation. The damped method, by enforcing a decrease in the merit function, takes smaller, more cautious steps, successfully navigating the complex landscape towards the solution. This demonstrates the superior global convergence properties of the damped method.\n-   **Case B (good initial guess):** Starting from $x^{(0)}=(1.2,\\,1.2)$, the iterate is in the basin of attraction of the solution. Here, the local quadratic model is a good approximation. The full step $\\alpha=1$ satisfies the sufficient decrease condition, so the damped method behaves identically to the full-step method. Both are expected to converge rapidly due to Newton's quadratic convergence rate near a solution.\n\n**Case C: Near-singular Jacobian System**\nThe system has a solution at $x^\\star = (0, 0)$, where the Jacobian $J_S(0,0)$ is singular. Starting at $x^{(0)}=(10^{-4},\\,10^{-4})$, the initial Jacobian is nearly singular. This tests the robustness of the linear solver. The use of a least-squares solver for the Newton step is crucial here. The condition for this test case is that the damped method must converge and its final merit function value, $\\phi(x_{final})$, must be less than or equal to that of the full-step method. As the damped method is guaranteed to not worsen the merit function value at each step (unlike the full-step method), it is expected to achieve a final state that is at least as good as, if not better than, the full-step method's final state, thus satisfying the test condition.\n\nThe program implements these functions, solvers, and test cases to produce the required boolean results.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests standard and damped Newton's methods for solving\n    systems of nonlinear equations.\n    \"\"\"\n\n    # --- System Definitions ---\n\n    def F_R(x):\n        \"\"\"Rosenbrock-gradient system function F(x).\"\"\"\n        x1, x2 = x[0], x[1]\n        f1 = 2 * (x1 - 1) - 400 * x1 * (x2 - x1**2)\n        f2 = 200 * (x2 - x1**2)\n        return np.array([f1, f2], dtype=float)\n\n    def J_R(x):\n        \"\"\"Jacobian of the Rosenbrock-gradient system J(x).\"\"\"\n        x1, x2 = x[0], x[1]\n        j11 = 2 - 400 * (x2 - x1**2) + 800 * x1**2\n        j12 = -400 * x1\n        j21 = -400 * x1\n        j22 = 200\n        return np.array([[j11, j12], [j21, j22]], dtype=float)\n\n    def F_S(x):\n        \"\"\"Near-singular system function F(x).\"\"\"\n        x1, x2 = x[0], x[1]\n        f1 = x1 * x2\n        f2 = x1**2 - x2\n        return np.array([f1, f2], dtype=float)\n\n    def J_S(x):\n        \"\"\"Jacobian of the near-singular system J(x).\"\"\"\n        x1, x2 = x[0], x[1]\n        j11 = x2\n        j12 = x1\n        j21 = 2 * x1\n        j22 = -1.0\n        return np.array([[j11, j12], [j21, j22]], dtype=float)\n\n    # --- Newton Solver Implementation ---\n\n    def newton_solver(F, J, x0, tol, max_iter, use_damping, sigma, beta, alpha_min):\n        \"\"\"\n        Solves a nonlinear system F(x)=0 using Newton's method.\n\n        Returns:\n            x_final (np.ndarray): The final solution vector.\n            Fx_final (np.ndarray): The final residual vector F(x_final).\n            converged (bool): True if the method converged to the tolerance.\n        \"\"\"\n        x = np.copy(x0).astype(float)\n        k = 0\n\n        while k  max_iter:\n            Fx = F(x)\n            norm_Fx = np.linalg.norm(Fx)\n\n            if norm_Fx = tol:\n                return x, Fx, True\n\n            Jx = J(x)\n            \n            # Solve the linear system using least-squares for robustness\n            try:\n                p = np.linalg.lstsq(Jx, -Fx, rcond=None)[0]\n            except np.linalg.LinAlgError:\n                # This may happen for extremely ill-conditioned matrices, though lstsq is robust.\n                return x, Fx, False\n\n            alpha = 1.0\n            if use_damping:\n                phi_x = 0.5 * norm_Fx**2\n                grad_phi_p = -norm_Fx**2\n\n                line_search_success = False\n                while alpha  alpha_min:\n                    x_new = x + alpha * p\n                    phi_new = 0.5 * np.linalg.norm(F(x_new))**2\n                    \n                    if phi_new = phi_x + sigma * alpha * grad_phi_p:\n                        line_search_success = True\n                        break\n                    \n                    alpha *= beta\n                \n                if not line_search_success:\n                    # Unable to find a step that satisfies the condition, indicating stalling.\n                    return x, Fx, False\n            \n            x = x + alpha * p\n            k += 1\n\n        # Check for convergence after the final iteration\n        final_Fx = F(x)\n        converged = np.linalg.norm(final_Fx) = tol\n        return x, final_Fx, converged\n\n    # --- Test Cases ---\n    \n    results = []\n    \n    # Common line search parameters\n    ls_params = {'sigma': 1e-4, 'beta': 0.5, 'alpha_min': 1e-12}\n\n    # Case A: Rosenbrock-gradient with poor initial guess\n    x0_A = np.array([-1.2, 1.0])\n    tol_A = 1e-10\n    max_iter_A = 50\n    _, _, converged_damped_A = newton_solver(F_R, J_R, x0_A, tol_A, max_iter_A, True, **ls_params)\n    _, _, converged_full_A = newton_solver(F_R, J_R, x0_A, tol_A, max_iter_A, False, **ls_params)\n    resultA = converged_damped_A and not converged_full_A\n    results.append(resultA)\n\n    # Case B: Rosenbrock-gradient with good initial guess\n    x0_B = np.array([1.2, 1.2])\n    tol_B = 1e-10\n    max_iter_B = 50\n    _, _, converged_damped_B = newton_solver(F_R, J_R, x0_B, tol_B, max_iter_B, True, **ls_params)\n    _, _, converged_full_B = newton_solver(F_R, J_R, x0_B, tol_B, max_iter_B, False, **ls_params)\n    resultB = converged_damped_B and converged_full_B\n    results.append(resultB)\n\n    # Case C: Near-singular Jacobian at the start\n    x0_C = np.array([1e-4, 1e-4])\n    tol_C = 1e-8\n    max_iter_C = 100\n    _, Fx_damped_C, converged_damped_C = newton_solver(F_S, J_S, x0_C, tol_C, max_iter_C, True, **ls_params)\n    _, Fx_full_C, _ = newton_solver(F_S, J_S, x0_C, tol_C, max_iter_C, False, **ls_params)\n    \n    phi_damped_C = 0.5 * np.linalg.norm(Fx_damped_C)**2\n    phi_full_C = 0.5 * np.linalg.norm(Fx_full_C)**2\n    \n    resultC = converged_damped_C and (phi_damped_C = phi_full_C)\n    results.append(resultC)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3255431"}, {"introduction": "With a robust solver in hand, we can now tackle problems from other scientific domains, with unconstrained optimization being a primary application. This exercise challenges you to find the stationary points of a function—points where its gradient is zero—by applying your Newton's method solver to the gradient vector field. By solving $\\nabla f(x,y) = \\mathbf{0}$ for the well-known six-hump camel back function, you will directly connect the abstract root-finding algorithm to the practical task of exploring the critical points of a complex energy landscape [@problem_id:3255464].", "problem": "Consider the twice continuously differentiable bivariate function $$f(x,y) = \\left(4 - 2.1x^2 + \\frac{x^4}{3}\\right)x^2 + xy + \\left(-4 + 4y^2\\right)y^2,$$ known as the six-hump camel back function. A stationary point of this function is any point $$(x^\\star,y^\\star)$$ where the gradient vanishes, that is $$\\nabla f(x^\\star,y^\\star) = \\mathbf{0}.$$ Your task is to design and implement a program that computes stationary points by solving the nonlinear system $$\\mathbf{F}(\\mathbf{z}) = \\mathbf{0}, \\quad \\text{where } \\mathbf{z} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\ \\mathbf{F}(\\mathbf{z}) = \\nabla f(x,y),$$ using Newton's method for systems, starting from specified initial guesses.\n\nUse a derivation that starts from the multivariate Taylor expansion and the definition of the gradient and Jacobian. Implement a globally convergent strategy by coupling the Newton step with a backtracking line search on the merit function $$\\phi(\\mathbf{z}) = \\frac{1}{2}\\left\\|\\mathbf{F}(\\mathbf{z})\\right\\|_2^2.$$ If the Jacobian is singular or ill-conditioned, apply a small diagonal regularization $$\\mu \\mathbf{I}$$ (with $$\\mu > 0$$ adapted as needed) to the Jacobian to obtain a solvable linear system. Terminate the iteration when the infinity norm $$\\|\\mathbf{F}(\\mathbf{z}_k)\\|_\\infty \\le 10^{-10}$$ or when the step infinity norm $$\\|\\mathbf{s}_k\\|_\\infty \\le 10^{-12}$$, or if the iteration count reaches $$100$$. Use radians for any implicit angle measures if any appear (none are required in this problem). All floating point outputs must be rounded to six decimal places.\n\nYour program must process the following test suite of initial guesses:\n- Test case 1: $$\\mathbf{z}_0 = (0.1,-0.7).$$\n- Test case 2: $$\\mathbf{z}_0 = (-0.2,0.6).$$\n- Test case 3 (edge case: already stationary): $$\\mathbf{z}_0 = (0.0,0.0).$$\n- Test case 4: $$\\mathbf{z}_0 = (-1.7,0.8).$$\n- Test case 5: $$\\mathbf{z}_0 = (1.7,-0.8).$$\n\nFor each test case, output the stationary point $$(x^\\star,y^\\star)$$ found and the corresponding function value $$f(x^\\star,y^\\star)$$. The final output must be a single line containing a list of lists in the exact format\n$$[[x_1,y_1,f_1],[x_2,y_2,f_2],[x_3,y_3,f_3],[x_4,y_4,f_4],[x_5,y_5,f_5]],$$\nwhere each $$x_i$$, $$y_i$$, and $$f_i$$ is a float rounded to six decimal places, and there are no spaces in the output string.", "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded problem in the field of numerical analysis, free of contradictions, ambiguities, or pseudo-scientific claims. The task is to find stationary points of a given function using a standard Newton's method for systems, augmented with globalization and regularization techniques. All necessary data, including the objective function, initial conditions, and termination criteria, are provided.\n\n### Principle-Based Solution Design\n\nThe problem requires finding stationary points $(x^\\star, y^\\star)$ of a twice continuously differentiable function $f(x,y)$. A stationary point is defined as a point where the gradient of the function is the zero vector, i.e., $\\nabla f(x^\\star, y^\\star) = \\mathbf{0}$. This condition establishes a system of nonlinear equations that must be solved.\n\n**1. Formulation of the Nonlinear System**\n\nThe function is given by:\n$$f(x,y) = \\left(4 - 2.1x^2 + \\frac{x^4}{3}\\right)x^2 + xy + \\left(-4 + 4y^2\\right)y^2$$\nExpanding this expression gives:\n$$f(x,y) = 4x^2 - 2.1x^4 + \\frac{x^6}{3} + xy - 4y^2 + 4y^4$$\nThe vector of state variables is $\\mathbf{z} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}$. The system of nonlinear equations to solve is $\\mathbf{F}(\\mathbf{z}) = \\mathbf{0}$, where $\\mathbf{F}(\\mathbf{z})$ is the gradient of $f(x,y)$:\n$$\\mathbf{F}(\\mathbf{z}) = \\nabla f(x,y) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$$\nThe partial derivatives are:\n$$\\frac{\\partial f}{\\partial x} = 8x - 8.4x^3 + 2x^5 + y$$\n$$\\frac{\\partial f}{\\partial y} = x - 8y + 16y^3$$\nThus, the system of equations is:\n$$\\mathbf{F}(\\mathbf{z}) = \\begin{bmatrix} F_1(x,y) \\\\ F_2(x,y) \\end{bmatrix} = \\begin{bmatrix} 2x^5 - 8.4x^3 + 8x + y \\\\ 16y^3 - 8y + x \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$$\n\n**2. Newton's Method for Systems**\n\nNewton's method is an iterative procedure for finding roots of a system of equations. Starting with an initial guess $\\mathbf{z}_0$, it generates a sequence of iterates $\\mathbf{z}_k$ that approximate the root. The update rule is derived from the first-order multivariate Taylor expansion of $\\mathbf{F}$ around the current iterate $\\mathbf{z}_k$:\n$$\\mathbf{F}(\\mathbf{z}_{k+1}) \\approx \\mathbf{F}(\\mathbf{z}_k) + J(\\mathbf{z}_k)(\\mathbf{z}_{k+1} - \\mathbf{z}_k)$$\nwhere $J(\\mathbf{z}_k)$ is the Jacobian matrix of $\\mathbf{F}$ evaluated at $\\mathbf{z}_k$.\nWe seek $\\mathbf{z}_{k+1}$ such that $\\mathbf{F}(\\mathbf{z}_{k+1}) = \\mathbf{0}$. Setting the left side to zero and defining the Newton step as $\\mathbf{s}_k = \\mathbf{z}_{k+1} - \\mathbf{z}_k$, we obtain the following linear system for the step $\\mathbf{s}_k$:\n$$J(\\mathbf{z}_k)\\mathbf{s}_k = -\\mathbf{F}(\\mathbf{z}_k)$$\nThe next iterate is then found by $\\mathbf{z}_{k+1} = \\mathbf{z}_k + \\mathbf{s}_k$.\n\n**3. Jacobian Matrix Derivation**\n\nThe Jacobian matrix $J(\\mathbf{z})$ for the system $\\mathbf{F}(\\mathbf{z})$ is given by:\n$$J(\\mathbf{z}) = \\begin{bmatrix} \\frac{\\partial F_1}{\\partial x}  \\frac{\\partial F_1}{\\partial y} \\\\ \\frac{\\partial F_2}{\\partial x}  \\frac{\\partial F_2}{\\partial y} \\end{bmatrix}$$\nSince $\\mathbf{F} = \\nabla f$, the Jacobian $J$ is the Hessian matrix of $f$, $H_f$. For a $C^2$ function like $f$, the Hessian is symmetric by Clairaut's theorem on equality of mixed partials. The components are:\n$$\\frac{\\partial F_1}{\\partial x} = \\frac{\\partial^2 f}{\\partial x^2} = 10x^4 - 25.2x^2 + 8$$\n$$\\frac{\\partial F_1}{\\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x} = 1$$\n$$\\frac{\\partial F_2}{\\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y} = 1$$\n$$\\frac{\\partial F_2}{\\partial y} = \\frac{\\partial^2 f}{\\partial y^2} = 48y^2 - 8$$\nThe Jacobian matrix is therefore:\n$$J(\\mathbf{z}) = \\begin{bmatrix} 10x^4 - 25.2x^2 + 8  1 \\\\ 1  48y^2 - 8 \\end{bmatrix}$$\n\n**4. Globalization via Backtracking Line Search**\n\nThe full Newton step $\\mathbf{s}_k$ may not always lead to convergence, especially if the initial guess is far from the root. A globalization strategy is needed. We employ a backtracking line search to find an appropriate step length $\\alpha_k \\in (0,1]$ that ensures sufficient progress towards the solution. The update rule becomes:\n$$\\mathbf{z}_{k+1} = \\mathbf{z}_k + \\alpha_k \\mathbf{s}_k$$\nProgress is measured using a merit function, which for root-finding is typically the squared L2-norm of the residual:\n$$\\phi(\\mathbf{z}) = \\frac{1}{2}\\|\\mathbf{F}(\\mathbf{z})\\|_2^2 = \\frac{1}{2}\\mathbf{F}(\\mathbf{z})^T\\mathbf{F}(\\mathbf{z})$$\nThe step length $\\alpha_k$ is chosen to satisfy the Armijo condition for sufficient decrease:\n$$\\phi(\\mathbf{z}_k + \\alpha_k \\mathbf{s}_k) \\le \\phi(\\mathbf{z}_k) + c \\alpha_k \\nabla \\phi(\\mathbf{z}_k)^T \\mathbf{s}_k$$\nwhere $c \\in (0, 1)$ is a small constant (e.g., $c=10^{-4}$). The directional derivative $\\nabla \\phi(\\mathbf{z}_k)^T \\mathbf{s}_k$ is computed using the chain rule: $\\nabla \\phi(\\mathbf{z}) = J(\\mathbf{z})^T \\mathbf{F}(\\mathbf{z})$.\n$$\\nabla \\phi(\\mathbf{z}_k)^T \\mathbf{s}_k = \\left(J(\\mathbf{z}_k)^T \\mathbf{F}(\\mathbf{z}_k)\\right)^T \\mathbf{s}_k = \\mathbf{F}(\\mathbf{z}_k)^T J(\\mathbf{z}_k) \\mathbf{s}_k$$\nThe backtracking algorithm starts with $\\alpha=1$ and repeatedly reduces it (e.g., $\\alpha \\leftarrow \\tau \\alpha$ with $\\tau=0.5$) until the Armijo condition is met. For the standard Newton step $\\mathbf{s}_k = -J(\\mathbf{z}_k)^{-1}\\mathbf{F}(\\mathbf{z}_k)$, the directional derivative simplifies to $-\\|\\mathbf{F}(\\mathbf{z}_k)\\|_2^2$, guaranteeing a descent direction for $\\phi$.\n\n**5. Jacobian Regularization**\n\nIf the Jacobian matrix $J(\\mathbf{z}_k)$ becomes singular or numerically ill-conditioned, the linear system for $\\mathbf{s}_k$ cannot be solved reliably. To handle this, a diagonal regularization term $\\mu\\mathbf{I}$ is added, where $\\mu  0$ is a small parameter and $\\mathbf{I}$ is the identity matrix. The modified system is:\n$$(J(\\mathbf{z}_k) + \\mu\\mathbf{I})\\mathbf{s}_k = -\\mathbf{F}(\\mathbf{z}_k)$$\nThis technique, related to Levenberg-Marquardt methods, ensures the matrix is invertible. The parameter $\\mu$ is adapted dynamically: it is set to zero by default. If a singularity is detected during the linear solve, $\\mu$ is initialized to a small positive value (e.g., $10^{-8}$) and increased multiplicatively (e.g., by a factor of $10$) until the system can be solved.\n\n**6. Termination Criteria**\n\nThe iterative process is terminated when one of the following conditions is met:\n1.  The infinity norm of the function residual is sufficiently small: $\\|\\mathbf{F}(\\mathbf{z}_k)\\|_\\infty \\le 10^{-10}$.\n2.  The infinity norm of the full step taken is sufficiently small: $\\|\\alpha_k \\mathbf{s}_k\\|_\\infty \\le 10^{-12}$.\n3.  The number of iterations $k$ reaches a maximum limit of $100$.\n\n**7. Algorithmic Summary**\n\nFor each initial guess $\\mathbf{z}_0$:\n1.  Initialize $k=0$ and $\\mathbf{z}=\\mathbf{z}_0$.\n2.  Loop for $k  100$:\n    a. Compute $\\mathbf{F}_k = \\mathbf{F}(\\mathbf{z})$ and check for convergence: if $\\|\\mathbf{F}_k\\|_\\infty \\le 10^{-10}$, terminate.\n    b. Compute the Jacobian $J_k = J(\\mathbf{z})$.\n    c. Solve $(J_k + \\mu\\mathbf{I})\\mathbf{s}_k = -\\mathbf{F}_k$ for the step $\\mathbf{s}_k$, adaptively choosing $\\mu \\ge 0$ to ensure a solvable system.\n    d. Perform a backtracking line search to find a step length $\\alpha_k$ satisfying the Armijo condition for the merit function $\\phi$.\n    e. Compute the update: $\\Delta\\mathbf{z}_k = \\alpha_k \\mathbf{s}_k$.\n    f. Update the solution: $\\mathbf{z} \\leftarrow \\mathbf{z} + \\Delta\\mathbf{z}_k$.\n    g. Check for convergence: if $\\|\\Delta\\mathbf{z}_k\\|_\\infty \\le 10^{-12}$, terminate.\n3.  Upon termination, the stationary point is $\\mathbf{z}^\\star = \\mathbf{z}$. Compute the function value $f(\\mathbf{z}^\\star)$.\n4.  Store and format the result $(x^\\star, y^\\star, f(x^\\star, y^\\star))$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n\n    def f_camel(z):\n        \"\"\"Computes the six-hump camel back function value.\"\"\"\n        x, y = z[0], z[1]\n        term1 = (4 - 2.1 * x**2 + (x**4) / 3) * x**2\n        term2 = x * y\n        term3 = (-4 + 4 * y**2) * y**2\n        return term1 + term2 + term3\n\n    def F_grad(z):\n        \"\"\"Computes the gradient of the six-hump camel back function.\"\"\"\n        x, y = z[0], z[1]\n        df_dx = 2 * x**5 - 8.4 * x**3 + 8 * x + y\n        df_dy = 16 * y**3 - 8 * y + x\n        return np.array([df_dx, df_dy])\n\n    def J_hess(z):\n        \"\"\"Computes the Jacobian of the gradient (Hessian of the function).\"\"\"\n        x, y = z[0], z[1]\n        d2f_dx2 = 10 * x**4 - 25.2 * x**2 + 8\n        d2f_dxdy = 1.0\n        d2f_dy2 = 48 * y**2 - 8\n        return np.array([[d2f_dx2, d2f_dxdy], [d2f_dxdy, d2f_dy2]])\n\n    def find_stationary_point(z0):\n        \"\"\"\n        Finds a stationary point using Newton's method with line search and regularization.\n        \"\"\"\n        z = np.array(z0, dtype=float)\n        \n        # Parameters\n        max_iter = 100\n        tol_F = 1e-10\n        tol_s = 1e-12\n        c1_armijo = 1e-4\n        tau_backtrack = 0.5\n        max_line_search_iter = 20\n\n        for _ in range(max_iter):\n            Fk = F_grad(z)\n\n            # Termination condition 1: residual norm\n            if np.linalg.norm(Fk, np.inf) = tol_F:\n                break\n\n            Jk = J_hess(z)\n\n            # Solve for Newton step with adaptive regularization\n            mu = 0.0\n            sk = None\n            while True:\n                try:\n                    J_reg = Jk if mu == 0.0 else Jk + mu * np.identity(2)\n                    sk = np.linalg.solve(J_reg, -Fk)\n                    break\n                except np.linalg.LinAlgError:\n                    if mu == 0.0:\n                        mu = 1e-8\n                    else:\n                        mu *= 10\n                    # Failsafe to prevent extreme mu values\n                    if mu  1e16:\n                        sk = np.zeros_like(z) # Force stop if regularization fails\n                        break\n\n            # Backtracking line search\n            alpha = 1.0\n            phi_k = 0.5 * (Fk @ Fk)\n            # The directional derivative of the merit function phi\n            dir_deriv = Fk @ Jk @ sk\n\n            # If dir_deriv is non-negative, the line search will fail naturally\n            # by shrinking alpha, leading to a small step and termination.\n            for _ in range(max_line_search_iter):\n                z_trial = z + alpha * sk\n                F_trial = F_grad(z_trial)\n                phi_trial = 0.5 * (F_trial @ F_trial)\n\n                if phi_trial = phi_k + c1_armijo * alpha * dir_deriv:\n                    break\n                \n                alpha *= tau_backtrack\n            else:\n                # If the line search loop completes without a break,\n                # it means a suitable alpha was not found.\n                # In this case, the step size will be effectively zero.\n                alpha = 0.0\n\n            step = alpha * sk\n            z += step\n\n            # Termination condition 2: step norm\n            if np.linalg.norm(step, np.inf) = tol_s:\n                break\n\n        return z\n\n    # Test cases defined in the problem\n    test_cases = [\n        (0.1, -0.7),\n        (-0.2, 0.6),\n        (0.0, 0.0),\n        (-1.7, 0.8),\n        (1.7, -0.8),\n    ]\n\n    results = []\n    for z0 in test_cases:\n        z_star = find_stationary_point(z0)\n        f_star = f_camel(z_star)\n        \n        # Round to six decimal places for output\n        x_out = round(z_star[0], 6)\n        y_out = round(z_star[1], 6)\n        f_out = round(f_star, 6)\n\n        # Handle negative zero\n        if x_out == 0.0: x_out = 0.0\n        if y_out == 0.0: y_out = 0.0\n        if f_out == 0.0: f_out = 0.0\n        \n        results.append(f\"[{x_out:.6f},{y_out:.6f},{f_out:.6f}]\")\n\n    # Final print statement in the exact required format with no spaces\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3255464"}]}