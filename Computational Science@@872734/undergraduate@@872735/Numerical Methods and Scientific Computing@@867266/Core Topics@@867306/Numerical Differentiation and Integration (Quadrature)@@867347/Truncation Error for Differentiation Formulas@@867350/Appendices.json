{"hands_on_practices": [{"introduction": "The theoretical order of accuracy, often expressed using big-O notation like $\\mathcal{O}(h^p)$, can feel abstract. This practice provides a direct link between theory and measurement by demonstrating a remarkable property of finite difference formulas. You will first prove that for a specific polynomial of degree $p+1$, the truncation error is not just bounded by a term proportional to $h^p$ but is exactly equal to $K h^p$, allowing for the precise measurement of the leading error constant $K$. This exercise solidifies your understanding of how a formula's order is determined and provides a practical method for verifying the accuracy of different numerical stencils [@problem_id:3284683].", "problem": "Let $f:\\mathbb{R}\\to\\mathbb{R}$ be sufficiently smooth. A linear finite-difference approximation to the first derivative at a point $x_0$ on a uniform grid of spacing $h>0$ can be written in the form\n$$\nD_h[f](x_0) \\;=\\; \\frac{1}{h}\\sum_{j=1}^{M} c_j\\, f\\!\\big(x_0 + s_j h\\big),\n$$\nwhere $c_j \\in \\mathbb{R}$ and $s_j \\in \\mathbb{Z}$ are fixed stencil coefficients and offsets, respectively. Such a formula is said to have formal order $p \\in \\mathbb{N}$ if, for any sufficiently smooth $f$, the local truncation error satisfies $D_h[f](x_0) - f'(x_0) = \\mathcal{O}\\!\\big(h^p\\big)$ as $h \\to 0^+$. Use the Fundamental Theorem of Calculus and the Taylor series expansion about $x_0$ as the foundational base to reason about truncation error.\n\nTask A (derivation). Show, from first principles using Taylor expansions and the linearity and order conditions of the stencil, that for the specific polynomial $f(x)=x^{p+1}$, any such first-derivative formula of formal order $p$ produces a pure truncation error of the form\n$$\nD_h[f](x_0) - f'(x_0) \\;=\\; K\\, h^p,\n$$\nfor some constant $K$ that depends only on the stencil $\\{(c_j,s_j)\\}_{j=1}^M$ and not on $h$ or $x_0$. Your derivation must start from the Taylor expansion of $f(x_0+s_j h)$ and the defining order conditions of the stencil; do not assume any pre-derived truncation error expressions.\n\nTask B (measurement). Use the result of Task A to measure $K$ numerically by computing\n$$\n\\widehat{K} \\;=\\; \\frac{D_h[f](x_0) - f'(x_0)}{h^p}\n$$\nfor the test function $f(x)=x^{p+1}$ at a specified $x_0$ and $h>0$.\n\nTest suite. For each of the following stencils and parameters, compute the corresponding $\\widehat{K}$:\n- Case $1$ (forward, formal order $p=1$): $D_h[f](x_0) = \\dfrac{f(x_0+h)-f(x_0)}{h}$, with $f(x)=x^{2}$, $x_0=0$, $h=10^{-2}$.\n- Case $2$ (centered, formal order $p=2$): $D_h[f](x_0) = \\dfrac{f(x_0+h)-f(x_0-h)}{2h}$, with $f(x)=x^{3}$, $x_0=0$, $h=2\\times 10^{-2}$.\n- Case $3$ (forward, formal order $p=3$): $D_h[f](x_0) = \\dfrac{-11\\,f(x_0) + 18\\,f(x_0+h) - 9\\,f(x_0+2h) + 2\\,f(x_0+3h)}{6h}$, with $f(x)=x^{4}$, $x_0=0$, $h=10^{-2}$.\n- Case $4$ (five-point centered, formal order $p=4$): $D_h[f](x_0) = \\dfrac{f(x_0-2h) - 8 f(x_0-h) + 8 f(x_0+h) - f(x_0+2h)}{12h}$, with $f(x)=x^{5}$, $x_0=0$, $h=10^{-2}$.\n\nAnswer specification. For each case, the answer is the single real number $\\widehat{K}$. Your program must compute all four values and aggregate them into a single output line.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as Cases $1$ through $4$.", "solution": "The problem is assessed as valid, being scientifically grounded in numerical analysis, well-posed, and objective. It consists of a theoretical derivation followed by numerical computation, both of which are standard procedures in the study of finite difference methods.\n\n### Task A: Derivation of the Pure Truncation Error\n\nWe are asked to show that for a finite-difference formula of formal order $p$, when applied to the specific polynomial $f(x)=x^{p+1}$, the local truncation error is of the form $K h^p$, where $K$ is a constant independent of the evaluation point $x_0$ and the grid spacing $h$.\n\nA general linear finite-difference approximation to the first derivative $f'(x_0)$ is given by\n$$\nD_h[f](x_0) = \\frac{1}{h}\\sum_{j=1}^{M} c_j f(x_0 + s_j h)\n$$\nwhere $\\{c_j, s_j\\}_{j=1}^M$ constitutes the stencil.\n\nThe derivation proceeds from the Taylor series expansion of $f(x_0 + s_j h)$ around $x_0$:\n$$\nf(x_0 + s_j h) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(x_0)}{k!} (s_j h)^k\n$$\nSubstituting this into the finite-difference formula, we get:\n$$\nD_h[f](x_0) = \\frac{1}{h} \\sum_{j=1}^{M} c_j \\left( \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(x_0)}{k!} (s_j h)^k \\right)\n$$\nBy exchanging the order of summation, we can express $D_h[f](x_0)$ as a power series in $h$:\n$$\nD_h[f](x_0) = \\sum_{k=0}^{\\infty} \\left( \\frac{1}{k!} \\sum_{j=1}^{M} c_j s_j^k \\right) f^{(k)}(x_0) h^{k-1}\n$$\nThe local truncation error is $T_h = D_h[f](x_0) - f'(x_0)$. For the formula to have a formal order of accuracy $p$, the error must be $\\mathcal{O}(h^p)$. This imposes conditions on the coefficients of powers of $h$ in the expansion of $T_h$. Specifically, the coefficients of $h^m$ for $m < p$ must be zero. This leads to the following system of linear equations for the stencil coefficients, known as the method of undetermined coefficients:\n$$\n\\sum_{j=1}^{M} c_j s_j^k = \\delta_{k,1} \\quad \\text{for } k = 0, 1, \\dots, p\n$$\nwhere $\\delta_{k,1}$ is the Kronecker delta. This means:\n\\begin{itemize}\n    \\item For $k=0$: $\\sum_{j=1}^{M} c_j = 0$\n    \\item For $k=1$: $\\sum_{j=1}^{M} c_j s_j = 1$\n    \\item For $k=2, \\dots, p$: $\\sum_{j=1}^{M} c_j s_j^k = 0$\n\\end{itemize}\n\nNow, we consider the specific test function $f(x) = x^{p+1}$. The derivatives of this function are:\n\\begin{itemize}\n    \\item $f^{(k)}(x) = \\frac{(p+1)!}{(p+1-k)!} x^{p+1-k}$ for $k \\le p+1$\n    \\item $f^{(p+1)}(x) = (p+1)!$\n    \\item $f^{(k)}(x) = 0$ for $k > p+1$\n\\end{itemize}\nThe key insight is that for this polynomial, the Taylor series expansion of $f(x_0 + s_j h)$ is finite and exact (by the Binomial Theorem):\n$$\nf(x_0 + s_j h) = (x_0 + s_j h)^{p+1} = \\sum_{k=0}^{p+1} \\binom{p+1}{k} x_0^{p+1-k} (s_j h)^k\n$$\nSubstituting this exact expression into the finite-difference operator gives:\n$$\nD_h[f](x_0) = \\frac{1}{h} \\sum_{j=1}^{M} c_j \\left( \\sum_{k=0}^{p+1} \\binom{p+1}{k} x_0^{p+1-k} (s_j h)^k \\right)\n$$\nAgain, changing the order of summation:\n$$\nD_h[f](x_0) = \\sum_{k=0}^{p+1} \\binom{p+1}{k} x_0^{p+1-k} h^{k-1} \\left( \\sum_{j=1}^{M} c_j s_j^k \\right)\n$$\nWe now apply the order conditions. For $k=0, 1, \\dots, p$, the term $\\sum_{j=1}^M c_j s_j^k$ is equal to $\\delta_{k,1}$.\n\\begin{align*}\nD_h[f](x_0) &= \\left( \\sum_{k=0}^{p} \\binom{p+1}{k} x_0^{p+1-k} h^{k-1} (\\delta_{k,1}) \\right) + \\binom{p+1}{p+1} x_0^{0} h^{p} \\left( \\sum_{j=1}^{M} c_j s_j^{p+1} \\right) \\\\\n&= \\left( \\binom{p+1}{1} x_0^{p} h^{0} \\cdot 1 \\right) + h^{p} \\left( \\sum_{j=1}^{M} c_j s_j^{p+1} \\right) \\\\\n&= (p+1)x_0^p + h^p \\left( \\sum_{j=1}^{M} c_j s_j^{p+1} \\right)\n\\end{align*}\nThe exact derivative of $f(x)=x^{p+1}$ at $x_0$ is $f'(x_0) = (p+1)x_0^p$.\nThe truncation error is therefore:\n$$\nD_h[f](x_0) - f'(x_0) = \\left( (p+1)x_0^p + h^p \\sum_{j=1}^{M} c_j s_j^{p+1} \\right) - (p+1)x_0^p\n$$\n$$\nD_h[f](x_0) - f'(x_0) = \\left( \\sum_{j=1}^{M} c_j s_j^{p+1} \\right) h^p\n$$\nThis is precisely the form $K h^p$, where the constant $K$ is given by\n$$\nK = \\sum_{j=1}^{M} c_j s_j^{p+1}\n$$\nThis constant $K$ depends only on the stencil coefficients $c_j$ and offsets $s_j$ and is independent of $x_0$ and $h$, which completes the derivation.\n\n### Task B: Numerical Measurement of the Constant $K$\n\nWe compute $\\widehat{K} = \\frac{D_h[f](x_0) - f'(x_0)}{h^p}$ for each case.\n\n**Case 1:**\n- Formula: $D_h[f](x_0) = \\frac{f(x_0+h)-f(x_0)}{h}$\n- Parameters: $p=1$, $f(x)=x^2$, $x_0=0$, $h=10^{-2}$.\n- $f'(x) = 2x$, so $f'(0) = 0$.\n- $D_h[f](0) = \\frac{f(h) - f(0)}{h} = \\frac{h^2 - 0}{h} = h$.\n- Error: $D_h[f](0) - f'(0) = h - 0 = h$.\n- $\\widehat{K} = \\frac{h}{h^1} = 1$.\n\n**Case 2:**\n- Formula: $D_h[f](x_0) = \\frac{f(x_0+h)-f(x_0-h)}{2h}$\n- Parameters: $p=2$, $f(x)=x^3$, $x_0=0$, $h=2\\times 10^{-2}$.\n- $f'(x) = 3x^2$, so $f'(0) = 0$.\n- $D_h[f](0) = \\frac{f(h) - f(-h)}{2h} = \\frac{h^3 - (-h^3)}{2h} = \\frac{2h^3}{2h} = h^2$.\n- Error: $D_h[f](0) - f'(0) = h^2 - 0 = h^2$.\n- $\\widehat{K} = \\frac{h^2}{h^2} = 1$.\n\n**Case 3:**\n- Formula: $D_h[f](x_0) = \\frac{-11f(x_0) + 18f(x_0+h) - 9f(x_0+2h) + 2f(x_0+3h)}{6h}$\n- Parameters: $p=3$, $f(x)=x^4$, $x_0=0$, $h=10^{-2}$.\n- $f'(x) = 4x^3$, so $f'(0) = 0$.\n- $D_h[f](0) = \\frac{-11f(0) + 18f(h) - 9f(2h) + 2f(3h)}{6h} = \\frac{18(h^4) - 9((2h)^4) + 2((3h)^4)}{6h}$.\n- $D_h[f](0) = \\frac{18h^4 - 9(16h^4) + 2(81h^4)}{6h} = \\frac{(18 - 144 + 162)h^4}{6h} = \\frac{36h^4}{6h} = 6h^3$.\n- Error: $D_h[f](0) - f'(0) = 6h^3 - 0 = 6h^3$.\n- $\\widehat{K} = \\frac{6h^3}{h^3} = 6$.\n\n**Case 4:**\n- Formula: $D_h[f](x_0) = \\frac{f(x_0-2h) - 8f(x_0-h) + 8f(x_0+h) - f(x_0+2h)}{12h}$\n- Parameters: $p=4$, $f(x)=x^5$, $x_0=0$, $h=10^{-2}$.\n- $f'(x) = 5x^4$, so $f'(0) = 0$.\n- $D_h[f](0) = \\frac{f(-2h) - 8f(-h) + 8f(h) - f(2h)}{12h}$.\n- $D_h[f](0) = \\frac{(-2h)^5 - 8(-h)^5 + 8(h^5) - (2h)^5}{12h} = \\frac{-32h^5 + 8h^5 + 8h^5 - 32h^5}{12h}$.\n- $D_h[f](0) = \\frac{-48h^5}{12h} = -4h^4$.\n- Error: $D_h[f](0) - f'(0) = -4h^4 - 0 = -4h^4$.\n- $\\widehat{K} = \\frac{-4h^4}{h^4} = -4$.\n\nThe computed values for $\\widehat{K}$ are $1$, $1$, $6$, and $-4$ for Cases $1$ through $4$, respectively.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the constant K for the truncation error term for four\n    different finite difference schemes, as specified in the problem.\n    \"\"\"\n\n    # Test cases are defined as a list of dictionaries. Each dictionary\n    # contains all necessary parameters and functions for a single case.\n    test_cases = [\n        # Case 1: Forward difference, p=1\n        {\n            \"p\": 1, \n            \"x0\": 0.0, \n            \"h\": 1e-2,\n            \"f\": lambda x: x**2,\n            \"fp\": lambda x: 2*x,\n            \"stencil_op\": lambda f, x0, h: (f(x0 + h) - f(x0)) / h\n        },\n        # Case 2: Centered difference, p=2\n        {\n            \"p\": 2, \n            \"x0\": 0.0, \n            \"h\": 2e-2,\n            \"f\": lambda x: x**3,\n            \"fp\": lambda x: 3*x**2,\n            \"stencil_op\": lambda f, x0, h: (f(x0 + h) - f(x0 - h)) / (2 * h)\n        },\n        # Case 3: Forward difference, p=3\n        {\n            \"p\": 3, \n            \"x0\": 0.0, \n            \"h\": 1e-2,\n            \"f\": lambda x: x**4,\n            \"fp\": lambda x: 4*x**3,\n            \"stencil_op\": lambda f, x0, h: (-11 * f(x0) + 18 * f(x0 + h) - 9 * f(x0 + 2 * h) + 2 * f(x0 + 3 * h)) / (6 * h)\n        },\n        # Case 4: Five-point centered, p=4\n        {\n            \"p\": 4, \n            \"x0\": 0.0, \n            \"h\": 1e-2,\n            \"f\": lambda x: x**5,\n            \"fp\": lambda x: 5*x**4,\n            \"stencil_op\": lambda f, x0, h: (f(x0 - 2 * h) - 8 * f(x0 - h) + 8 * f(x0 + h) - f(x0 + 2 * h)) / (12 * h)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack parameters for the current case\n        p = case[\"p\"]\n        x0 = case[\"x0\"]\n        h = case[\"h\"]\n        f = case[\"f\"]\n        fp = case[\"fp\"]\n        stencil_op = case[\"stencil_op\"]\n\n        # Compute the finite difference approximation\n        D_h_f = stencil_op(f, x0, h)\n\n        # Compute the exact derivative\n        fp_x0 = fp(x0)\n\n        # Calculate K_hat based on the formula from Task B\n        # K_hat = (D_h[f](x0) - f'(x0)) / h^p\n        K_hat = (D_h_f - fp_x0) / (h**p)\n        \n        results.append(K_hat)\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) function converts each floating-point result to its\n    # string representation before joining them with commas.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3284683"}, {"introduction": "Standard truncation error analysis relies on the assumption that the function is sufficiently smooth, possessing enough continuous derivatives. This exercise challenges that assumption by exploring what happens when a formula is applied to a function with a singularity. By analyzing the central difference formula for $f(x)=|x|^{\\alpha}$, you will discover that the formal $\\mathcal{O}(h^2)$ accuracy degrades, and the true order of convergence depends on the function's behavior near its non-smooth point. This practice is crucial for developing a critical eye for the limitations of numerical methods and learning how to analyze their performance in more realistic, non-ideal scenarios [@problem_id:3284729].", "problem": "Let $f(x)=|x|^{\\alpha}$ where $\\alpha \\in (1,2)$ is non-integer. Consider approximating the first derivative $f'(x)$ at the point $x=h>0$ using the Central Difference (CD) formula with step size $h$, defined by\n$$\nD_{\\text{CD}} f(x;h)=\\frac{f(x+h)-f(x-h)}{2h}.\n$$\nDefine the truncation error as\n$$\n\\mathrm{TE}(x;h)=D_{\\text{CD}} f(x;h)-f'(x).\n$$\nUsing only the definition of the derivative and the explicit form of $f(x)$, derive the leading-order asymptotic behavior of $\\mathrm{TE}(h;h)$ as $h\\to 0^{+}$ and determine the exponent $p(\\alpha)$ such that\n$$\n\\mathrm{TE}(h;h)=C(\\alpha)\\,h^{p(\\alpha)}+o\\!\\left(h^{p(\\alpha)}\\right)\\quad\\text{as}\\quad h\\to 0^{+},\n$$\nfor some nonzero constant $C(\\alpha)$ depending on $\\alpha$. Your final answer must be the single analytic expression for $p(\\alpha)$ in terms of $\\alpha$.", "solution": "The problem is well-posed, scientifically grounded, and objective. All necessary information is provided, and the terms are defined unambiguously. The function $f(x)=|x|^{\\alpha}$ with $\\alpha \\in (1,2)$ is not sufficiently differentiable at $x=0$ for the standard Taylor series-based error analysis of the central difference formula to apply directly, but this does not invalidate the problem. Instead, it requires a more fundamental approach, which is precisely what the problem directs by asking to use basic definitions. The problem is valid.\n\nThe problem asks for the leading-order asymptotic behavior of the truncation error $\\mathrm{TE}(h;h)$ as $h \\to 0^{+}$. The truncation error for the Central Difference (CD) formula is defined as:\n$$\n\\mathrm{TE}(x;h) = D_{\\text{CD}} f(x;h) - f'(x)\n$$\nwhere the CD operator is given by:\n$$\nD_{\\text{CD}} f(x;h) = \\frac{f(x+h) - f(x-h)}{2h}\n$$\nWe are asked to evaluate this at the point $x=h$, with a step size also equal to $h$. Substituting $x=h$ into the expression for the truncation error gives:\n$$\n\\mathrm{TE}(h;h) = D_{\\text{CD}} f(h;h) - f'(h)\n$$\nLet's compute each term on the right-hand side separately.\n\nFirst, we evaluate the CD approximation, $D_{\\text{CD}} f(h;h)$:\n$$\nD_{\\text{CD}} f(h;h) = \\frac{f(h+h) - f(h-h)}{2h} = \\frac{f(2h) - f(0)}{2h}\n$$\nThe function is given as $f(x) = |x|^{\\alpha}$. Since the problem states $h>0$, the argument $2h$ is also positive. Thus, $f(2h) = |2h|^{\\alpha} = (2h)^{\\alpha} = 2^{\\alpha}h^{\\alpha}$.\nThe value at $x=0$ is $f(0) = |0|^{\\alpha} = 0$. This is well-defined since $\\alpha \\in (1,2)$, so $\\alpha > 0$.\nSubstituting these values back into the expression for $D_{\\text{CD}} f(h;h)$:\n$$\nD_{\\text{CD}} f(h;h) = \\frac{2^{\\alpha}h^{\\alpha} - 0}{2h} = \\frac{2^{\\alpha}}{2} \\frac{h^{\\alpha}}{h} = 2^{\\alpha-1}h^{\\alpha-1}\n$$\n\nNext, we evaluate the exact derivative, $f'(h)$. The function is $f(x)=|x|^{\\alpha}$. For any $x>0$, we have $f(x)=x^{\\alpha}$. The derivative with respect to $x$ is:\n$$\nf'(x) = \\frac{d}{dx}(x^{\\alpha}) = \\alpha x^{\\alpha-1}\n$$\nSince $h>0$, we can evaluate the derivative at $x=h$:\n$$\nf'(h) = \\alpha h^{\\alpha-1}\n$$\n\nNow, we can assemble the full expression for the truncation error $\\mathrm{TE}(h;h)$:\n$$\n\\mathrm{TE}(h;h) = D_{\\text{CD}} f(h;h) - f'(h) = 2^{\\alpha-1}h^{\\alpha-1} - \\alpha h^{\\alpha-1}\n$$\nFactoring out the term $h^{\\alpha-1}$, we obtain an exact expression for the error:\n$$\n\\mathrm{TE}(h;h) = (2^{\\alpha-1} - \\alpha) h^{\\alpha-1}\n$$\nThe problem asks for the exponent $p(\\alpha)$ in the asymptotic form $\\mathrm{TE}(h;h) = C(\\alpha)h^{p(\\alpha)} + o(h^{p(\\alpha)})$ as $h \\to 0^{+}$, where $C(\\alpha)$ is a nonzero constant.\nFrom our exact expression, we can identify:\n$$\nC(\\alpha) = 2^{\\alpha-1} - \\alpha\n$$\nand\n$$\np(\\alpha) = \\alpha-1\n$$\nTo confirm that this is the leading-order term, we must verify that $C(\\alpha)$ is nonzero for the specified domain $\\alpha \\in (1,2)$. Let us analyze the function $g(\\alpha) = 2^{\\alpha-1} - \\alpha$.\nWe evaluate $g(\\alpha)$ at the endpoints of the interval $[1,2]$:\n$g(1) = 2^{1-1} - 1 = 2^0 - 1 = 1 - 1 = 0$.\n$g(2) = 2^{2-1} - 2 = 2^1 - 2 = 2 - 2 = 0$.\nTo determine the behavior of $g(\\alpha)$ for $\\alpha \\in (1,2)$, we examine its derivatives. The first derivative is:\n$$\ng'(\\alpha) = \\frac{d}{d\\alpha}(2^{\\alpha-1} - \\alpha) = \\ln(2) \\cdot 2^{\\alpha-1} - 1\n$$\nThe second derivative is:\n$$\ng''(\\alpha) = \\frac{d}{d\\alpha}(\\ln(2) \\cdot 2^{\\alpha-1} - 1) = (\\ln(2))^2 \\cdot 2^{\\alpha-1}\n$$\nSince $\\ln(2) > 0$ and $2^{\\alpha-1} > 0$ for $\\alpha \\in (1,2)$, we have $g''(\\alpha) > 0$. This means that $g(\\alpha)$ is a strictly convex function on the interval $(1,2)$. A strictly convex function that is zero at the endpoints of an interval must be negative for all points within that interval. Therefore, $g(\\alpha)  0$ for all $\\alpha \\in (1,2)$.\nThis confirms that $C(\\alpha) = g(\\alpha)$ is nonzero for the specified range of $\\alpha$.\n\nThus, the expression $\\mathrm{TE}(h;h) = (2^{\\alpha-1} - \\alpha) h^{\\alpha-1}$ correctly represents the leading-order behavior, and the exponent $p(\\alpha)$ is $\\alpha-1$. Note that for $\\alpha \\in (1,2)$, the order of accuracy $p(\\alpha)=\\alpha-1$ is between $0$ and $1$. This is a reduction from the standard $\\mathcal{O}(h^2)$ accuracy of the central difference formula, which occurs because one of the sampling points, $x-h=0$, coincides with a point where the function's higher derivatives are singular.\n\nThe requested quantity is the exponent $p(\\alpha)$.\n$$\np(\\alpha) = \\alpha-1\n$$", "answer": "$$\\boxed{\\alpha-1}$$", "id": "3284729"}, {"introduction": "While decreasing the step size $h$ reduces truncation error, it simultaneously amplifies the round-off error inherent in finite-precision computer arithmetic. This hands-on coding exercise illuminates this fundamental trade-off, guiding you to find the optimal step size $h_{opt}$ that minimizes the total computational error. By implementing differentiation formulas in both single and double precision, you will empirically observe the characteristic \"U-shaped\" error curve and gain a practical understanding of how machine precision dictates the limits of numerical accuracy. This experience demonstrates why simply choosing an extremely small $h$ is often a flawed strategy in scientific computing [@problem_id:3284592].", "problem": "Consider the interplay between truncation error, which arises from replacing limits by finite differences, and rounding error, which arises from finite precision floating-point arithmetic. Use Taylor series expansions as the fundamental base to derive consistent numerical differentiation formulas, and use the standard model of floating-point rounding error as a multiplicative perturbation bounded by machine epsilon. Specifically, model the rounding of a basic arithmetic operation on real numbers as $$\\operatorname{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b)(1 + \\delta), \\quad |\\delta| \\le \\varepsilon,$$ where $\\circ$ denotes $+$, $-$, $\\times$, or $\\div$, and $\\varepsilon$ is the machine epsilon of the working format. Do not introduce ad hoc error terms beyond this model.\n\nYour task is to numerically demonstrate how the optimal step size $h$ that minimizes the total absolute error in approximating $f'(x_0)$ shifts when switching between binary64 (double precision) and binary32 (single precision) arithmetic. Use two differentiation formulas constructed from Taylor expansions:\n- a first-order accurate one-sided difference formula,\n- a second-order accurate symmetric difference formula.\n\nFor each formula, the truncation error is an algebraic function of $h$ determined by the omitted higher-order Taylor terms, while the rounding error grows as subtraction of nearly equal quantities and division by $h$ amplify floating-point perturbations. You must:\n1. Derive both differentiation formulas from Taylor series and justify their leading truncation error term order solely from this base.\n2. Explain, using the rounding model, why the rounding error contribution grows as $h$ decreases.\n3. Design and implement a numerical experiment that, for both formulas and both precisions, scans a logarithmically spaced grid of $h$ values to empirically find the $h$ that minimizes the absolute error $|D_h f(x_0) - f'(x_0)|$, where $D_h f(x_0)$ is the chosen finite-difference approximation computed in the specified precision.\n\nYou must use Institute of Electrical and Electronics Engineers (IEEE) binary64 (double precision) and binary32 (single precision) arithmetic by controlling the data type of all arithmetic operations in each experiment. Evaluate $f$ in the matching precision and compare the numerical derivative to the exact derivative in high precision to form the absolute error. Angles for trigonometric functions must be in radians.\n\nTest Suite:\n- Test Case 1 (happy path, smooth exponential): $f(x) = e^x$, $f'(x) = e^x$, $x_0 = 1$, use the first-order one-sided difference formula.\n- Test Case 2 (happy path, smooth exponential): $f(x) = e^x$, $f'(x) = e^x$, $x_0 = 1$, use the second-order symmetric difference formula.\n- Test Case 3 (trigonometric, nonzero curvature; angles in radians): $f(x) = \\sin(x)$, $f'(x) = \\cos(x)$, $x_0 = \\pi/4$, use the first-order one-sided difference formula.\n- Test Case 4 (trigonometric, nonzero curvature; angles in radians): $f(x) = \\sin(x)$, $f'(x) = \\cos(x)$, $x_0 = \\pi/4$, use the second-order symmetric difference formula.\n\nScanning ranges for $h$ must be logarithmically spaced and chosen to bracket the optimal $h$ for each precision and formula:\n- For binary64 and the first-order formula: $h \\in [10^{-16}, 10^{-1}]$.\n- For binary32 and the first-order formula: $h \\in [10^{-8}, 10^{-1}]$.\n- For binary64 and the second-order formula: $h \\in [10^{-12}, 10^{-1}]$.\n- For binary32 and the second-order formula: $h \\in [10^{-7}, 10^{-1}]$.\nUse at least $300$ grid points in each range.\n\nYour program must, for each test case, compute:\n- the empirically optimal step size in binary64, $h_{\\text{opt,64}}$,\n- the empirically optimal step size in binary32, $h_{\\text{opt,32}}$,\n- the shift factor $h_{\\text{opt,32}} / h_{\\text{opt,64}}$,\n- the minimal absolute error in binary64, $E_{\\min,64}$,\n- the minimal absolute error in binary32, $E_{\\min,32}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing a comma-separated list of four entries (one per test case), where each entry is a list of five floating-point numbers in the order $[h_{\\text{opt,64}}, h_{\\text{opt,32}}, h_{\\text{opt,32}}/h_{\\text{opt,64}}, E_{\\min,64}, E_{\\min,32}]$. The entire output must be enclosed in square brackets. For example: \"[[h64_case1,h32_case1,ratio1,E64_case1,E32_case1],[h64_case2,h32_case2,ratio2,E64_case2,E32_case2],[h64_case3,h32_case3,ratio3,E64_case3,E32_case3],[h64_case4,h32_case4,ratio4,E64_case4,E32_case4]]\".", "solution": "The problem requires a theoretical derivation and numerical demonstration of the optimal step size in finite difference approximations, considering the trade-off between truncation and rounding errors across different floating-point precisions.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n\n*   **Floating-Point Error Model**: $\\operatorname{fl}(a \\circ b) = (a \\circ b)(1 + \\delta)$, where $|\\delta| \\le \\varepsilon$ (machine epsilon).\n*   **Precisions**: IEEE binary64 (double precision) and IEEE binary32 (single precision).\n*   **Differentiation Formulas**:\n    1.  A first-order accurate one-sided difference formula.\n    2.  A second-order accurate symmetric difference formula.\n*   **Numerical Task**: For each formula and precision, find the step size $h$ that minimizes the absolute error $|D_h f(x_0) - f'(x_0)|$ by scanning a logarithmically spaced grid of $h$ values.\n*   **Test Cases**:\n    1.  $f(x) = e^x$, $f'(x) = e^x$, $x_0 = 1$, first-order formula.\n    2.  $f(x) = e^x$, $f'(x) = e^x$, $x_0 = 1$, second-order formula.\n    3.  $f(x) = \\sin(x)$, $f'(x) = \\cos(x)$, $x_0 = \\pi/4$, first-order formula.\n    4.  $f(x) = \\sin(x)$, $f'(x) = \\cos(x)$, $x_0 = \\pi/4$, second-order formula.\n*   **Scanning Ranges for $h$ ($300+$ points)**:\n    *   binary64, 1st-order: $h \\in [10^{-16}, 10^{-1}]$\n    *   binary32, 1st-order: $h \\in [10^{-8}, 10^{-1}]$\n    *   binary64, 2nd-order: $h \\in [10^{-12}, 10^{-1}]$\n    *   binary32, 2nd-order: $h \\in [10^{-7}, 10^{-1}]$\n*   **Required Outputs per Test Case**: $h_{\\text{opt,64}}$, $h_{\\text{opt,32}}$, $h_{\\text{opt,32}} / h_{\\text{opt,64}}$, $E_{\\min,64}$, $E_{\\min,32}$.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientifically Grounded**: The problem is a classic exercise in numerical analysis. It correctly identifies Taylor series as the basis for truncation error and a standard multiplicative model for rounding error. The concepts of catastrophic cancellation, machine epsilon, and the trade-off between error sources are fundamental to scientific computing.\n*   **Well-Posed**: The problem is well-posed. The functions are infinitely differentiable, ensuring the existence of the Taylor series expansions. The objective is clearly defined: to find the minimum of an error function over a specified domain. A unique minimum is expected because the total error function is the sum of a monotonically increasing function of $h$ (truncation error) and a monotonically decreasing function of $h$ (rounding error).\n*   **Objective**: The problem is stated in precise, objective mathematical terms. All parameters, functions, and required calculations are explicitly defined.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. It is a well-defined, scientifically sound problem in numerical methods. I will proceed with the solution.\n\n**Theoretical Analysis**\n\nThe total error in a numerical differentiation formula is the sum of two components: the truncation error, which is an intrinsic error of the approximation formula, and the rounding error, which arises from the finite precision of computer arithmetic.\n\n**1. Derivation of Formulas and Truncation Error**\n\nWe use the Taylor series expansion of a sufficiently smooth function $f(x)$ about a point $x_0$.\n$$f(x_0 + h) = f(x_0) + f'(x_0)h + \\frac{f''(x_0)}{2!}h^2 + \\frac{f'''(x_0)}{3!}h^3 + \\dots$$\n\n**First-Order One-Sided Formula (Forward Difference):**\nTo derive this formula, we rearrange the Taylor expansion for $f(x_0 + h)$ to solve for $f'(x_0)$:\n$$f'(x_0) = \\frac{f(x_0 + h) - f(x_0)}{h} - \\frac{f''(x_0)}{2}h - \\mathcal{O}(h^2)$$\nThe forward difference approximation is defined as:\n$$D_{h,1}f(x_0) = \\frac{f(x_0 + h) - f(x_0)}{h}$$\nThe truncation error, $E_{\\text{trunc},1} = f'(x_0) - D_{h,1}f(x_0)$, is therefore:\n$$E_{\\text{trunc},1} = - \\frac{f''(x_0)}{2}h - \\mathcal{O}(h^2)$$\nThe leading error term is proportional to $h$, so this formula is first-order accurate. The absolute truncation error is approximately $|E_{\\text{trunc},1}| \\approx C_1 h$, where $C_1 = |f''(x_0)|/2$.\n\n**Second-Order Symmetric Formula (Central Difference):**\nFor this formula, we use two Taylor expansions:\n$$f(x_0 + h) = f(x_0) + f'(x_0)h + \\frac{f''(x_0)}{2}h^2 + \\frac{f'''(x_0)}{6}h^3 + \\mathcal{O}(h^4)$$\n$$f(x_0 - h) = f(x_0) - f'(x_0)h + \\frac{f''(x_0)}{2}h^2 - \\frac{f'''(x_0)}{6}h^3 + \\mathcal{O}(h^4)$$\nSubtracting the second equation from the first cancels the even-powered terms in $h$:\n$$f(x_0 + h) - f(x_0 - h) = 2f'(x_0)h + \\frac{f'''(x_0)}{3}h^3 + \\mathcal{O}(h^5)$$\nSolving for $f'(x_0)$ yields:\n$$f'(x_0) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h} - \\frac{f'''(x_0)}{6}h^2 - \\mathcal{O}(h^4)$$\nThe central difference approximation is:\n$$D_{h,2}f(x_0) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h}$$\nThe truncation error, $E_{\\text{trunc},2} = f'(x_0) - D_{h,2}f(x_0)$, is:\n$$E_{\\text{trunc},2} = - \\frac{f'''(x_0)}{6}h^2 - \\mathcal{O}(h^4)$$\nThe leading error term is proportional to $h^2$, making this formula second-order accurate. The absolute truncation error is approximately $|E_{\\text{trunc},2}| \\approx C_2 h^2$, where $C_2 = |f'''(x_0)|/6$.\n\n**2. Rounding Error Analysis**\n\nRounding error stems from representing real numbers in finite precision. When we compute $f(x)$, we obtain a floating-point number $\\operatorname{fl}(f(x)) \\approx f(x)(1+\\delta)$, where $|\\delta| \\le \\varepsilon$. The error in the function evaluation is thus bounded by $\\varepsilon |f(x)|$.\n\nLet's analyze the rounding error for the forward difference formula, $D_{h,1}f(x_0)$. The numerator involves the subtraction of two quantities that become very close as $h \\to 0$. Let $\\hat{f_0} = \\operatorname{fl}(f(x_0)) \\approx f(x_0) + e_0$ and $\\hat{f_1} = \\operatorname{fl}(f(x_0+h)) \\approx f(x_0+h) + e_1$, where the errors $|e_0|$ and $|e_1|$ are bounded by approximately $\\varepsilon|f(x_0)|$ for small $h$.\n\nThe computed difference is $\\operatorname{fl}(\\hat{f_1} - \\hat{f_0})$. The absolute error in this subtraction is bounded by approximately $|e_1| + |e_0| \\approx 2\\varepsilon|f(x_0)|$. This phenomenon, where the relative error of the result is much larger than the relative errors of the inputs, is known as catastrophic cancellation.\n\nThe formula then requires division by $h$. The rounding error in the final result, $E_{\\text{round}}$, is dominated by this effect:\n$$|E_{\\text{round},1}| \\approx \\frac{2\\varepsilon|f(x_0)|}{h}$$\nFor the central difference formula, a similar analysis shows:\n$$|E_{\\text{round},2}| \\approx \\frac{2\\varepsilon|f(x_0)|}{2h} = \\frac{\\varepsilon|f(x_0)|}{h}$$\nIn both cases, the rounding error contribution, $|E_{\\text{round}}|$, is proportional to $1/h$. Thus, as $h$ decreases, the rounding error increases.\n\n**3. Total Error and Optimal Step Size**\n\nThe total absolute error $|E_{\\text{total}}|$ is the sum of the magnitudes of the truncation and rounding errors.\n\nFor the first-order formula:\n$$|E_{\\text{total},1}(h)| \\approx \\frac{|f''(x_0)|}{2}h + \\frac{2\\varepsilon|f(x_0)|}{h}$$\nTo find the optimal $h$ that minimizes this error, we differentiate with respect to $h$ and set the result to zero:\n$$\\frac{d}{dh}|E_{\\text{total},1}(h)| = \\frac{|f''(x_0)|}{2} - \\frac{2\\varepsilon|f(x_0)|}{h^2} = 0$$\n$$h_{\\text{opt},1}^2 = \\frac{4\\varepsilon|f(x_0)|}{|f''(x_0)|} \\implies h_{\\text{opt},1} \\propto \\sqrt{\\varepsilon}$$\n\nFor the second-order formula:\n$$|E_{\\text{total},2}(h)| \\approx \\frac{|f'''(x_0)|}{6}h^2 + \\frac{\\varepsilon|f(x_0)|}{h}$$\nDifferentiating and setting to zero:\n$$\\frac{d}{dh}|E_{\\text{total},2}(h)| = \\frac{|f'''(x_0)|}{3}h - \\frac{\\varepsilon|f(x_0)|}{h^2} = 0$$\n$$h_{\\text{opt},2}^3 = \\frac{3\\varepsilon|f(x_0)|}{|f'''(x_0)|} \\implies h_{\\text{opt},2} \\propto \\sqrt[3]{\\varepsilon}$$\n\nThis analysis predicts that the optimal step size $h_{\\text{opt}}$ depends on the machine epsilon $\\varepsilon$. Specifically, when switching from binary64 ($\\varepsilon_{64}$) to binary32 ($\\varepsilon_{32}$), the optimal step size should shift by a factor of $\\sqrt{\\varepsilon_{32}/\\varepsilon_{64}}$ for the first-order formula and $\\sqrt[3]{\\varepsilon_{32}/\\varepsilon_{64}}$ for the second-order formula. The numerical experiment is designed to verify this theoretical prediction.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical differentiation problem by finding optimal step sizes\n    for different precisions and formulas.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda x: np.exp(x),\n            \"df_exact_func\": lambda x: np.exp(x),\n            \"x0\": 1.0,\n            \"formula\": \"forward\",\n            \"h_range_64\": (-16, -1),\n            \"h_range_32\": (-8, -1),\n        },\n        {\n            \"f\": lambda x: np.exp(x),\n            \"df_exact_func\": lambda x: np.exp(x),\n            \"x0\": 1.0,\n            \"formula\": \"central\",\n            \"h_range_64\": (-12, -1),\n            \"h_range_32\": (-7, -1),\n        },\n        {\n            \"f\": lambda x: np.sin(x),\n            \"df_exact_func\": lambda x: np.cos(x),\n            \"x0\": np.pi / 4,\n            \"formula\": \"forward\",\n            \"h_range_64\": (-16, -1),\n            \"h_range_32\": (-8, -1),\n        },\n        {\n            \"f\": lambda x: np.sin(x),\n            \"df_exact_func\": lambda x: np.cos(x),\n            \"x0\": np.pi / 4,\n            \"formula\": \"central\",\n            \"h_range_64\": (-12, -1),\n            \"h_range_32\": (-7, -1),\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Calculate the exact derivative once in high precision for reference.\n        df_exact_val = case[\"df_exact_func\"](case[\"x0\"])\n\n        # Run experiment for binary64 (double precision)\n        h_opt64, E_min64 = find_optimal_h(\n            f=case[\"f\"],\n            x0=case[\"x0\"],\n            df_exact_val=df_exact_val,\n            formula=case[\"formula\"],\n            dtype=np.float64,\n            h_range_log=case[\"h_range_64\"]\n        )\n\n        # Run experiment for binary32 (single precision)\n        h_opt32, E_min32 = find_optimal_h(\n            f=case[\"f\"],\n            x0=case[\"x0\"],\n            df_exact_val=df_exact_val,\n            formula=case[\"formula\"],\n            dtype=np.float32,\n            h_range_log=case[\"h_range_32\"]\n        )\n        \n        shift_factor = h_opt32 / h_opt64\n        \n        results.append([h_opt64, h_opt32, shift_factor, E_min64, E_min32])\n\n    # Final print statement in the exact required format.\n    # The format string ensures list representation within the main list.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef find_optimal_h(f, x0, df_exact_val, formula, dtype, h_range_log):\n    \"\"\"\n    Finds the optimal step size h by scanning a log-spaced grid.\n\n    Args:\n        f (callable): The function to differentiate.\n        x0 (float): The point at which to differentiate.\n        df_exact_val (float): The exact value of the derivative for error calculation.\n        formula (str): 'forward' or 'central'.\n        dtype (numpy.dtype): The floating point precision (np.float64 or np.float32).\n        h_range_log (tuple): A tuple (log10_start, log10_end) for the h grid.\n\n    Returns:\n        tuple: A tuple containing the optimal h and the minimum error.\n    \"\"\"\n    num_points = 300\n    h_vals = np.logspace(h_range_log[0], h_range_log[1], num_points)\n    x0_prec = dtype(x0)\n    \n    errors = np.zeros(num_points)\n\n    for i, h in enumerate(h_vals):\n        h_prec = dtype(h)\n        numerical_derivative = dtype(0.0)\n\n        if formula == 'forward':\n            # One-sided (forward) difference formula\n            # D_h f(x) = (f(x+h) - f(x)) / h\n            # Operations are performed in the specified precision `dtype`\n            # because inputs (x0_prec, h_prec) have that type.\n            f_x0_plus_h = f(x0_prec + h_prec)\n            f_x0 = f(x0_prec)\n            numerical_derivative = (f_x0_plus_h - f_x0) / h_prec\n        \n        elif formula == 'central':\n            # Symmetric (central) difference formula\n            # D_h f(x) = (f(x+h) - f(x-h)) / (2h)\n            f_x0_plus_h = f(x0_prec + h_prec)\n            f_x0_minus_h = f(x0_prec - h_prec)\n            two_h = dtype(2.0) * h_prec\n            numerical_derivative = (f_x0_plus_h - f_x0_minus_h) / two_h\n\n        # The absolute error is calculated against the high-precision reference value.\n        # numpy will automatically promote the lower precision `numerical_derivative`\n        # to match the higher precision `df_exact_val` for subtraction.\n        errors[i] = np.abs(numerical_derivative - df_exact_val)\n\n    min_error_idx = np.argmin(errors)\n    optimal_h = h_vals[min_error_idx]\n    min_error = errors[min_error_idx]\n    \n    return optimal_h, min_error\n\nsolve()\n```", "id": "3284592"}]}