{"hands_on_practices": [{"introduction": "Before using a numerical solver for scientific discovery, we must first ensure it is correctly implemented. This practice introduces the Method of Manufactured Solutions (MMS), a cornerstone of code verification, where we invent a solution and construct a problem it satisfies exactly. By solving this manufactured problem, you can rigorously test your code's correctness and empirically confirm its theoretical order of accuracy. [@problem_id:3228560]", "problem": "You will implement the method of manufactured solutions for a nonlinear boundary value problem (BVP) on a one-dimensional domain to verify the correctness and empirical order of accuracy of a finite difference solver that uses Newton’s method for nonlinear systems. The governing equation is the second-order ordinary differential equation on the interval $[0,1]$:\n$$\nu''(x) + g(u(x)) = f(x), \\quad x \\in (0,1),\n$$\nwith Dirichlet boundary conditions\n$$\nu(0) = \\beta_0, \\quad u(1) = \\beta_1.\n$$\nAll trigonometric functions must use angles in radians. There are no physical units in this problem. The foundation for the computational method must start from the following well-tested facts: (i) central finite differences provide approximations to derivatives on uniform grids, (ii) Newton’s method solves nonlinear algebraic systems by iteratively linearizing the residual, and (iii) the method of manufactured solutions constructs a consistent right-hand side $f(x)$ by substituting an exact, smooth function into the differential equation and evaluating the left-hand side.\n\nYou will design three manufactured test cases by selecting a smooth exact solution $u_{\\text{exact}}(x)$ and a nonlinear function $g(u)$, then defining the forcing $f(x)$ to make $u_{\\text{exact}}$ satisfy the equation exactly. Your solver must not use any knowledge of $u_{\\text{exact}}(x)$ except for setting the Dirichlet boundary values; it must compute an approximate numerical solution $u_{\\text{num}}(x)$ by solving the discretized nonlinear system on a uniform grid with $N$ nodes. For each test case, compute the maximum norm error\n$$\nE(N) = \\max_{0 \\le i \\le N-1} \\left| u_{\\text{num}}(x_i) - u_{\\text{exact}}(x_i) \\right|,\n$$\nwhere $x_i$ are the grid nodes. Evaluate $E(N)$ on a refinement sequence $N \\in \\{33, 65, 129, 257\\}$ and then report the observed order of accuracy using the last two refinements:\n$$\np = \\frac{\\log\\!\\left(E(N_3)/E(N_4)\\right)}{\\log\\!\\left(h_3/h_4\\right)},\n$$\nwhere $N_3 = 129$, $N_4 = 257$, $h_k = 1/(N_k-1)$, and $\\log$ denotes the natural logarithm.\n\nImplement the following test suite of manufactured solutions:\n\n- Test case $1$ (happy path, nonzero Dirichlet boundaries): \n  - Exact solution $u_{\\text{exact}}(x) = 1 + 0.1 \\sin(2\\pi x)$.\n  - Nonlinearity $g(u) = u^3$.\n  - Define $f(x) = u_{\\text{exact}}''(x) + \\big(u_{\\text{exact}}(x)\\big)^3$.\n  - Boundary values $\\beta_0 = u_{\\text{exact}}(0)$ and $\\beta_1 = u_{\\text{exact}}(1)$.\n\n- Test case $2$ (nonlinearity with exponential term, zero Dirichlet boundaries):\n  - Exact solution $u_{\\text{exact}}(x) = 0.1 \\sin(3\\pi x)$.\n  - Nonlinearity $g(u) = e^{u} - 1$.\n  - Define $f(x) = u_{\\text{exact}}''(x) + e^{u_{\\text{exact}}(x)} - 1$.\n  - Boundary values $\\beta_0 = u_{\\text{exact}}(0)$ and $\\beta_1 = u_{\\text{exact}}(1)$.\n\n- Test case $3$ (stronger smooth nonlinearity, zero Dirichlet boundaries):\n  - Exact solution $u_{\\text{exact}}(x) = 0.15 \\sin(\\pi x) + 0.05 \\sin(4\\pi x)$.\n  - Nonlinearity $g(u) = \\alpha \\sin(u)$ with $\\alpha = 2.5$.\n  - Define $f(x) = u_{\\text{exact}}''(x) + \\alpha \\sin\\!\\big(u_{\\text{exact}}(x)\\big)$.\n  - Boundary values $\\beta_0 = u_{\\text{exact}}(0)$ and $\\beta_1 = u_{\\text{exact}}(1)$.\n\nAlgorithmic requirements:\n\n- Use a uniform grid on $[0,1]$ with $N$ nodes and spacing $h = 1/(N-1)$.\n- Discretize the second derivative using the central difference approximation.\n- Form the nonlinear residual at interior nodes by combining the discrete second derivative, $g(u)$ evaluated at the node, and the forcing $f(x)$.\n- Solve the resulting nonlinear system with Newton’s method. Construct the Jacobian using the derivative $g'(u)$ and the tridiagonal stencil from the second derivative.\n- Initialize the interior unknowns with a simple guess (for example, the linear interpolation between boundary values or zeros) and iterate until the residual infinity norm is below a small tolerance.\n\nOutput specification:\n\n- For each of the three test cases, compute the observed order $p$ using $N \\in \\{33, 65, 129, 257\\}$ as described above.\n- Your program should produce a single line of output containing the three observed orders as a comma-separated list enclosed in square brackets (for example, $[p_1,p_2,p_3]$). The entries must be floating-point numbers.\n\nYour implementation must be a complete, runnable program in a modern programming language, and it must not require any user input. All trigonometric calculations must use angles in radians.", "solution": "The user has provided a well-posed problem in numerical analysis, tasking me with verifying a finite difference solver for a nonlinear boundary value problem (BVP) using the method of manufactured solutions. The problem is scientifically grounded, formally specified, and internally consistent. I will now proceed with a detailed solution.\n\n### 1. Problem Formulation and Discretization\n\nThe governing nonlinear second-order ordinary differential equation (ODE) is defined on the domain $x \\in [0,1]$:\n$$\nu''(x) + g(u(x)) = f(x)\n$$\nsubject to Dirichlet boundary conditions:\n$$\nu(0) = \\beta_0, \\quad u(1) = \\beta_1\n$$\nTo solve this BVP numerically, we discretize the continuous domain $[0,1]$ into a uniform grid with $N$ nodes. The grid points are denoted by $x_i = i h$ for $i=0, 1, \\dots, N-1$, where the grid spacing is $h = 1/(N-1)$. Let $u_i$ be the numerical approximation to the exact solution $u(x_i)$ at each grid node. The values at the boundaries are known: $u_0 = \\beta_0$ and $u_{N-1} = \\beta_1$. The remaining $N-2$ interior values, $u_1, u_2, \\dots, u_{N-2}$, are the unknowns to be determined.\n\nThe second derivative $u''(x)$ at an interior node $x_i$ (where $1 \\le i \\le N-2$) is approximated using a second-order accurate central finite difference formula:\n$$\nu''(x_i) \\approx \\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2}\n$$\nSubstituting this approximation into the ODE yields a system of $N-2$ coupled nonlinear algebraic equations for the interior unknowns:\n$$\n\\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2} + g(u_i) = f(x_i), \\quad \\text{for } i = 1, 2, \\dots, N-2.\n$$\nThese equations, along with the given boundary values, form the discrete problem.\n\n### 2. Newton's Method for the Nonlinear System\n\nThe system of discrete equations can be written in the form of a root-finding problem. Let $\\mathbf{U} = [u_1, u_2, \\dots, u_{N-2}]^T$ be the vector of $M = N-2$ interior unknowns. We define a residual vector function $\\mathbf{R}(\\mathbf{U})$ of size $M$, whose components $R_i$ correspond to the $i$-th interior grid point (using indices $i=1, \\dots, M$ for the vector components, which map to grid indices $i, \\dots, i+M-1$). For consistency with programming array indices, let's use $j = 0, \\dots, M-1$ for the vector index, where $u_j^{\\text{vec}} = u_{j+1}^{\\text{grid}}$. The $j$-th component of the residual is:\n$$\nR_j(\\mathbf{U}) = \\frac{u_{j} - 2u_{j+1} + u_{j+2}}{h^2} + g(u_{j+1}) - f(x_{j+1}), \\quad \\text{for } j = 0, \\dots, M-1.\n$$\nIn this notation, $u_0$ and $u_{N-1}$ are the known boundary values $\\beta_0$ and $\\beta_1$. The problem is to find the vector $\\mathbf{U}$ such that $\\mathbf{R}(\\mathbf{U}) = \\mathbf{0}$.\n\nWe solve this nonlinear system using Newton's method. Starting with an initial guess $\\mathbf{U}^{(0)}$, the method iteratively refines the solution using the update rule:\n$$\n\\mathbf{U}^{(k+1)} = \\mathbf{U}^{(k)} + \\Delta \\mathbf{U}^{(k)}\n$$\nwhere the update vector $\\Delta \\mathbf{U}^{(k)}$ is the solution to the linear system:\n$$\nJ(\\mathbf{U}^{(k)}) \\Delta \\mathbf{U}^{(k)} = -\\mathbf{R}(\\mathbf{U}^{(k)})\n$$\nHere, $J(\\mathbf{U}^{(k)})$ is the Jacobian matrix of the residual function $\\mathbf{R}$ evaluated at the current iterate $\\mathbf{U}^{(k)}$. The iteration continues until the norm of the residual, $\\|\\mathbf{R}(\\mathbf{U}^{(k)})\\|_{\\infty}$, falls below a specified tolerance.\n\n### 3. Jacobian Matrix Derivation\n\nThe Jacobian matrix $J$ is an $M \\times M$ matrix with elements $J_{ij} = \\frac{\\partial R_i}{\\partial U_j}$, where $R_i$ is the $i$-th component of the residual vector and $U_j$ is the $j$-th component of the unknown vector $\\mathbf{U}$ (mapping to grid unknowns $u_{i+1}$ and $u_{j+1}$). The $i$-th residual equation (for grid point $i$, $1 \\le i \\le N-2$) is:\n$$\nR_i(u_1, \\dots, u_{N-2}) = \\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2} + g(u_i) - f_i\n$$\nThe partial derivatives with respect to the unknown $u_j$ are non-zero only if $j$ is $i-1$, $i$, or $i+1$.\n- Diagonal elements ($j=i$):\n  $$\n  \\frac{\\partial R_i}{\\partial u_i} = -\\frac{2}{h^2} + g'(u_i)\n  $$\n- Sub-diagonal elements ($j=i-1$, for $i>1$):\n  $$\n  \\frac{\\partial R_i}{\\partial u_{i-1}} = \\frac{1}{h^2}\n  $$\n- Super-diagonal elements ($j=i+1$, for $i<N-2$):\n  $$\n  \\frac{\\partial R_i}{\\partial u_{i+1}} = \\frac{1}{h^2}\n  $$\nAll other partial derivatives are zero. Thus, the Jacobian is a tridiagonal matrix, which allows the linear system $J \\Delta \\mathbf{U} = -\\mathbf{R}$ to be solved efficiently, typically in $O(M)$ operations.\n\n### 4. Method of Manufactured Solutions and Verification\n\nThe method of manufactured solutions is used to verify the correctness and accuracy of the numerical solver. The process involves a few steps:\n1.  Choose a smooth, non-trivial function $u_{\\text{exact}}(x)$ that will serve as the exact solution.\n2.  Choose a nonlinear function $g(u)$.\n3.  Substitute $u_{\\text{exact}}(x)$ into the left-hand side of the differential equation, $u''(x) + g(u(x))$, to define the forcing function $f(x)$. By construction, $f(x) = u_{\\text{exact}}''(x) + g(u_{\\text{exact}}(x))$.\n4.  The boundary conditions are determined from the manufactured solution: $\\beta_0 = u_{\\text{exact}}(0)$ and $\\beta_1 = u_{\\text{exact}}(1)$.\n\nThe numerical solver is then used to solve the BVP $u'' + g(u) = f$ with these boundary conditions. The solver itself has no knowledge of $u_{\\text{exact}}(x)$, it only uses $g(u)$, $f(x)$, and the boundary values. The resulting numerical solution $u_{\\text{num}}(x_i)$ can then be compared against the known exact solution $u_{\\text{exact}}(x_i)$ to compute the error.\n\n### 5. Error Analysis and Order of Accuracy\n\nThe error of the numerical solution is quantified using the maximum norm (or infinity norm) over all grid points:\n$$\nE(N) = \\max_{0 \\le i \\le N-1} \\left| u_{\\text{num}}(x_i) - u_{\\text{exact}}(x_i) \\right|\n$$\nThe theoretical order of accuracy, $p$, describes how the error decreases as the grid spacing $h$ is reduced: $E \\approx C h^p$ for some constant $C$. To measure this empirically, we compute the error on a sequence of refined grids. Given the errors $E_1$ and $E_2$ on two grids with respective spacings $h_1$ and $h_2$, the observed order of accuracy is calculated as:\n$$\np = \\frac{\\log(E_1 / E_2)}{\\log(h_1 / h_2)}\n$$\nFor this problem, we use the results from the last two refinements, $N_3=129$ and $N_4=257$. The corresponding grid spacings are $h_3 = 1/(N_3-1) = 1/128$ and $h_4 = 1/(N_4-1) = 1/256$. The ratio $h_3/h_4 = 2$, so the formula simplifies to:\n$$\np = \\frac{\\log(E(129) / E(257))}{\\log(2)}\n$$\nSince the central difference scheme for $u''$ is second-order accurate ($O(h^2)$), the expected value for $p$ is approximately $2$.\n\nThe provided Python code implements this entire procedure for the three specified test cases. It defines the manufactured solutions, constructs and solves the nonlinear systems using Newton's method on a sequence of grids, and computes the final observed order of accuracy for each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_bvp_newton(N, u_exact, u_pp_exact, g, g_prime, tol=1e-12, max_iter=50):\n    \"\"\"\n    Solves the nonlinear BVP u'' + g(u) = f using a finite difference\n    method with Newton's method for the resulting nonlinear system.\n\n    Args:\n        N (int): Number of grid points.\n        u_exact (callable): The exact solution function u(x).\n        u_pp_exact (callable): The second derivative of the exact solution u''(x).\n        g (callable): The nonlinear function g(u).\n        g_prime (callable): The derivative of the nonlinear function g'(u).\n        tol (float): Tolerance for Newton's method convergence.\n        max_iter (int): Maximum number of Newton iterations.\n\n    Returns:\n        float: The maximum norm error between the numerical and exact solutions.\n    \"\"\"\n    # 1. Grid and problem setup\n    h = 1.0 / (N - 1)\n    x = np.linspace(0, 1, N)\n    \n    # Boundary conditions from the exact solution\n    beta0 = u_exact(x[0])\n    beta1 = u_exact(x[-1])\n    \n    # Forcing function f(x) is manufactured from the exact solution\n    f_vals = u_pp_exact(x) + g(u_exact(x))\n    \n    # Interior grid points and corresponding forcing function values\n    x_int = x[1:-1]\n    f_int = f_vals[1:-1]\n    M = N - 2\n    \n    if M <= 0:\n        # Trivial case with no interior points\n        u_num = np.array([beta0, beta1]) if N == 2 else np.array([beta0])\n        u_ex_vals = u_exact(x)\n        return np.linalg.norm(u_num - u_ex_vals, np.inf)\n\n    # 2. Initial guess for interior unknowns\n    # Use linear interpolation between boundary values\n    u_int = beta0 + (beta1 - beta0) * x_int\n    \n    # 3. Newton's method iterations\n    h_sq = h * h\n    \n    for k in range(max_iter):\n        # Build residual vector R\n        R = np.zeros(M)\n        \n        # First interior point (i=1)\n        R[0] = (beta0 - 2 * u_int[0] + u_int[1]) / h_sq + g(u_int[0]) - f_int[0]\n        \n        # Central interior points (1 < i < N-2)\n        if M > 2:\n            R[1:-1] = (u_int[:-2] - 2 * u_int[1:-1] + u_int[2:]) / h_sq + g(u_int[1:-1]) - f_int[1:-1]\n\n        # Last interior point (i=N-2)\n        if M > 1:\n            R[-1] = (u_int[-2] - 2 * u_int[-1] + beta1) / h_sq + g(u_int[-1]) - f_int[-1]\n\n        residual_norm = np.linalg.norm(R, np.inf)\n        if residual_norm < tol:\n            break\n            \n        # Build Jacobian matrix J (tridiagonal)\n        diag = -2.0 / h_sq + g_prime(u_int)\n        sup_diag = np.ones(M - 1) / h_sq\n        sub_diag = np.ones(M - 1) / h_sq\n        \n        J = np.diag(diag) + np.diag(sup_diag, k=1) + np.diag(sub_diag, k=-1)\n        \n        # Solve the linear system J * delta_u = -R\n        delta_u = np.linalg.solve(J, -R)\n        \n        # Update solution for interior points\n        u_int += delta_u\n        \n    # 4. Construct full numerical solution and compute error\n    u_num = np.concatenate(([beta0], u_int, [beta1]))\n    u_ex_vals = u_exact(x)\n    error = np.linalg.norm(u_num - u_ex_vals, np.inf)\n    \n    return error\n\ndef solve():\n    \"\"\"\n    Main function to run the verification for all test cases and print the results.\n    \"\"\"\n    pi = np.pi\n    alpha = 2.5\n\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"u_exact\": lambda x: 1 + 0.1 * np.sin(2 * pi * x),\n            \"u_pp_exact\": lambda x: -0.1 * (2*pi)**2 * np.sin(2 * pi * x),\n            \"g\": lambda u: u**3,\n            \"g_prime\": lambda u: 3 * u**2\n        },\n        {\n            \"name\": \"Case 2\",\n            \"u_exact\": lambda x: 0.1 * np.sin(3 * pi * x),\n            \"u_pp_exact\": lambda x: -0.1 * (3*pi)**2 * np.sin(3 * pi * x),\n            \"g\": lambda u: np.exp(u) - 1.0,\n            \"g_prime\": lambda u: np.exp(u)\n        },\n        {\n            \"name\": \"Case 3\",\n            \"u_exact\": lambda x: 0.15 * np.sin(pi * x) + 0.05 * np.sin(4 * pi * x),\n            \"u_pp_exact\": lambda x: -0.15 * pi**2 * np.sin(pi * x) - 0.05 * (4*pi)**2 * np.sin(4 * pi * x),\n            \"g\": lambda u: alpha * np.sin(u),\n            \"g_prime\": lambda u: alpha * np.cos(u)\n        }\n    ]\n    \n    N_values = [33, 65, 129, 257]\n    observed_orders = []\n\n    for case in test_cases:\n        errors = []\n        for N in N_values:\n            error = solve_bvp_newton(N, \n                                     case[\"u_exact\"], \n                                     case[\"u_pp_exact\"], \n                                     case[\"g\"], \n                                     case[\"g_prime\"])\n            errors.append(error)\n        \n        # Compute the observed order of accuracy 'p' using the last two refinements\n        # p = log(E_3/E_4) / log(h_3/h_4)\n        E3 = errors[2]  # Error for N=129\n        E4 = errors[3]  # Error for N=257\n        N3 = N_values[2]\n        N4 = N_values[3]\n        h3 = 1.0 / (N3 - 1)\n        h4 = 1.0 / (N4 - 1)\n        \n        # Note: log(h3/h4) = log((1/128)/(1/256)) = log(2)\n        p = np.log(E3 / E4) / np.log(h3 / h4)\n        observed_orders.append(p)\n        \n    print(f\"[{','.join(map(str, observed_orders))}]\")\n\nsolve()\n\n```", "id": "3228560"}, {"introduction": "With a verified solver in hand, we can explore one of the most fascinating aspects of nonlinear systems: the existence of multiple solutions. This exercise uses the nonlinear pendulum equation, $u'' + \\sin(u) = 0$, to demonstrate how different initial guesses can guide Newton's method to converge to distinct, physically meaningful results. This practice provides a tangible demonstration of basins of attraction and the critical role the initial guess plays in finding non-trivial solutions. [@problem_id:3228545]", "problem": "Consider the two-point boundary value problem (BVP) for a twice continuously differentiable function $u(x)$ on the interval $[0,L]$ given by the nonlinear ordinary differential equation $u''(x)+\\sin(u(x))=0$ with Dirichlet boundary conditions $u(0)=0$ and $u(L)=0$. The task is to construct a finite difference method and a Newton–Raphson method (Newton’s method) to approximate solutions, and then to demonstrate how different initial guesses can lead to distinct solutions when $L=2\\pi$, using angles in radians. Your solution must proceed from fundamental bases as follows.\n\n1. Begin with the definition of the second derivative and use Taylor expansions to obtain a second-order accurate centered finite difference approximation for $u''(x)$ on a uniform grid with $N$ subintervals and grid spacing $h=L/N$. Assemble the discrete nonlinear system for the interior unknowns in vector form, where each interior equation is formed by replacing $u''(x)$ with its centered finite difference approximation and $u(x)$ with the corresponding grid value. Use the boundary conditions $u(0)=0$ and $u(L)=0$ exactly.\n\n2. Derive Newton’s method for the resulting nonlinear algebraic system $\\mathbf{F}(\\mathbf{u})=\\mathbf{0}$ by linearizing $\\mathbf{F}$ through its Jacobian matrix $\\mathbf{J}(\\mathbf{u})$ and solving, at each iteration, the linear system $\\mathbf{J}(\\mathbf{u}^{(k)})\\Delta\\mathbf{u}^{(k)}=-\\mathbf{F}(\\mathbf{u}^{(k)})$ to update $\\mathbf{u}^{(k+1)}=\\mathbf{u}^{(k)}+\\alpha^{(k)}\\Delta\\mathbf{u}^{(k)}$, where $\\alpha^{(k)}$ is a damping factor chosen to ensure a decrease in a suitable norm of the residual. Justify the form of the Jacobian using the chain rule and the derivative of $\\sin(\\cdot)$.\n\n3. Implement the Jacobian as a tridiagonal matrix that captures nearest-neighbor coupling of the centered difference and the local derivative of $\\sin(u)$ evaluated at each iterate. Explain how to efficiently solve the tridiagonal linear systems produced at each Newton iteration.\n\n4. Provide clear stopping criteria based on the residual norm and/or the Newton step norm to ensure convergence to a solution with a well-defined numerical tolerance.\n\n5. Use angles in radians for all trigonometric evaluations. There are no dimensional physical units in this problem beyond the interval length $L$.\n\nYour program must implement the above method generically for any $L>0$ and any integer $N\\ge 2$, and then run the following test suite. For each test case, apply Newton’s method with the specified initial guess function $u_0(x)$ mapped to the grid. After convergence, report the $L^\\infty$-norm (maximum absolute value over the discrete grid including endpoints) of the computed solution, rounded to six decimal places.\n\nTest suite (all angles in radians):\n- Case 1 (happy path, trivial solution on the long interval): $L=2\\pi$, $N=400$, initial guess $u_0(x)\\equiv 0$.\n- Case 2 (happy path, positive nontrivial solution on the long interval): $L=2\\pi$, $N=400$, initial guess $u_0(x)=2.5\\sin\\!\\big(\\pi x/L\\big)$.\n- Case 3 (symmetry check, negative nontrivial solution on the long interval): $L=2\\pi$, $N=400$, initial guess $u_0(x)=-2.5\\sin\\!\\big(\\pi x/L\\big)$.\n- Case 4 (edge case, interval too short for nontrivial solution consistent with the small-amplitude period): $L=\\pi$, $N=400$, initial guess $u_0(x)=2.5\\sin\\!\\big(\\pi x/L\\big)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the above cases, for example, $[r_1,r_2,r_3,r_4]$, where each $r_i$ is the $L^\\infty$-norm of the computed solution for the $i$-th case, rounded to six decimal places (e.g., $[0.000000,2.800000,2.800000,0.000000]$). Do not print any additional text.", "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded problem in numerical analysis, specifically concerning the solution of a nonlinear two-point boundary value problem using the finite difference method and Newton's method. All necessary data, equations, and conditions are provided, and there are no internal contradictions, ambiguities, or factual unsoundness.\n\n### 1. Discretization of the Boundary Value Problem\n\nThe problem is to solve the nonlinear ordinary differential equation (ODE)\n$$\nu''(x) + \\sin(u(x)) = 0\n$$\non the interval $x \\in [0, L]$ with homogeneous Dirichlet boundary conditions $u(0)=0$ and $u(L)=0$.\n\nWe discretize the domain $[0, L]$ using a uniform grid with $N$ subintervals. The grid points are $x_i = i h$ for $i = 0, 1, \\dots, N$, where the grid spacing is $h = L/N$. We seek to find approximations $u_i \\approx u(x_i)$ at these grid points. The boundary conditions give $u_0 = 0$ and $u_N = 0$. The unknown values are the interior grid points $u_1, u_2, \\dots, u_{N-1}$. We denote the vector of these unknowns as $\\mathbf{u} = [u_1, u_2, \\dots, u_{N-1}]^T \\in \\mathbb{R}^{N-1}$.\n\nTo discretize the second derivative term $u''(x)$, we use a second-order accurate centered finite difference approximation. This is derived from Taylor series expansions of $u(x)$ around a point $x_i$:\n$$\nu(x_i+h) = u(x_i) + h u'(x_i) + \\frac{h^2}{2} u''(x_i) + \\frac{h^3}{6} u'''(x_i) + O(h^4)\n$$\n$$\nu(x_i-h) = u(x_i) - h u'(x_i) + \\frac{h^2}{2} u''(x_i) - \\frac{h^3}{6} u'''(x_i) + O(h^4)\n$$\nAdding these two expansions eliminates the odd-derivative terms:\n$$\nu(x_i+h) + u(x_i-h) = 2u(x_i) + h^2 u''(x_i) + O(h^4)\n$$\nSolving for $u''(x_i)$ yields the approximation:\n$$\nu''(x_i) = \\frac{u(x_i+h) - 2u(x_i) + u(x_i-h)}{h^2} + O(h^2)\n$$\nIn terms of our discrete variables $u_i$, this is $u''(x_i) \\approx \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2}$.\n\nBy replacing the continuous derivatives and functions with their discrete counterparts in the ODE at each interior grid point $x_i$ for $i=1, \\dots, N-1$, we obtain a system of $N-1$ nonlinear algebraic equations:\n$$\n\\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2} + \\sin(u_i) = 0, \\quad \\text{for } i = 1, 2, \\dots, N-1\n$$\nWe can write this system in vector form $\\mathbf{F}(\\mathbf{u}) = \\mathbf{0}$, where $\\mathbf{F}: \\mathbb{R}^{N-1} \\to \\mathbb{R}^{N-1}$ and its $i$-th component $F_i(\\mathbf{u})$ is:\n$$\nF_i(\\mathbf{u}) = \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2} + \\sin(u_i)\n$$\nThe boundary conditions $u_0=0$ and $u_N=0$ are incorporated at the endpoints of the system. For $i=1$, the equation is $\\frac{u_2 - 2u_1 + u_0}{h^2} + \\sin(u_1) = 0$, which becomes $\\frac{u_2 - 2u_1}{h^2} + \\sin(u_1) = 0$. For $i=N-1$, the equation is $\\frac{u_N - 2u_{N-1} + u_{N-2}}{h^2} + \\sin(u_{N-1}) = 0$, which becomes $\\frac{-2u_{N-1} + u_{N-2}}{h^2} + \\sin(u_{N-1}) = 0$.\n\n### 2. Newton's Method for the Nonlinear System\n\nNewton's method is an iterative procedure for finding roots of a system of nonlinear equations $\\mathbf{F}(\\mathbf{u}) = \\mathbf{0}$. Starting from an initial guess $\\mathbf{u}^{(0)}$, the method generates a sequence of approximations $\\mathbf{u}^{(k)}$ that ideally converge to a solution. The update rule is derived by linearizing $\\mathbf{F}(\\mathbf{u})$ around the current iterate $\\mathbf{u}^{(k)}$:\n$$\n\\mathbf{F}(\\mathbf{u}) \\approx \\mathbf{F}(\\mathbf{u}^{(k)}) + \\mathbf{J}(\\mathbf{u}^{(k)})(\\mathbf{u} - \\mathbf{u}^{(k)})\n$$\nwhere $\\mathbf{J}(\\mathbf{u}^{(k)})$ is the Jacobian matrix of $\\mathbf{F}$ evaluated at $\\mathbf{u}^{(k)}$. Setting $\\mathbf{F}(\\mathbf{u}) = \\mathbf{0}$ to find the next iterate $\\mathbf{u}^{(k+1)}$, we solve for the update step $\\Delta\\mathbf{u}^{(k)} = \\mathbf{u}^{(k+1)} - \\mathbf{u}^{(k)}$:\n$$\n\\mathbf{J}(\\mathbf{u}^{(k)}) \\Delta\\mathbf{u}^{(k)} = -\\mathbf{F}(\\mathbf{u}^{(k)})\n$$\nThe next iterate is then found as $\\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} + \\Delta\\mathbf{u}^{(k)}$. For improved convergence and robustness, a damping factor (or step size) $\\alpha^{(k)} \\in (0, 1]$ is introduced:\n$$\n\\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} + \\alpha^{(k)} \\Delta\\mathbf{u}^{(k)}\n$$\nThe value of $\\alpha^{(k)}$ is typically chosen using a line search algorithm to ensure that the norm of the residual decreases at each step, e.g., $\\|\\mathbf{F}(\\mathbf{u}^{(k+1)})\\| < \\|\\mathbf{F}(\\mathbf{u}^{(k)})\\|$.\n\nThe Jacobian matrix $\\mathbf{J}$ is an $(N-1) \\times (N-1)$ matrix with entries $J_{ij} = \\frac{\\partial F_i}{\\partial u_j}$. We compute these partial derivatives from the expression for $F_i(\\mathbf{u})$:\n$$\nJ_{ij} = \\frac{\\partial}{\\partial u_j} \\left( \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2} + \\sin(u_i) \\right)\n$$\nThe derivative of the finite difference term is nonzero only when $j$ is $i-1$, $i$, or $i+1$:\n- For $j = i-1$: $J_{i,i-1} = \\frac{\\partial F_i}{\\partial u_{i-1}} = \\frac{1}{h^2}$\n- For $j = i+1$: $J_{i,i+1} = \\frac{\\partial F_i}{\\partial u_{i+1}} = \\frac{1}{h^2}$\nThe derivative of the nonlinear term $\\sin(u_i)$, by the chain rule, contributes only to the diagonal entry: $\\frac{\\partial}{\\partial u_j} \\sin(u_i) = \\cos(u_i) \\frac{\\partial u_i}{\\partial u_j} = \\cos(u_i) \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n- For $j = i$: $J_{ii} = \\frac{\\partial F_i}{\\partial u_i} = -\\frac{2}{h^2} + \\cos(u_i)$\n- For $|i-j| > 1$: $J_{ij} = 0$\n\n### 3. The Tridiagonal Jacobian System\n\nThe resulting Jacobian matrix is a symmetric, tridiagonal matrix:\n$$\n\\mathbf{J}(\\mathbf{u}) = \\begin{pmatrix}\n-\\frac{2}{h^2}+\\cos(u_1) & \\frac{1}{h^2} & 0 & \\dots & 0 \\\\\n\\frac{1}{h^2} & -\\frac{2}{h^2}+\\cos(u_2) & \\frac{1}{h^2} & \\dots & 0 \\\\\n0 & \\ddots & \\ddots & \\ddots & 0 \\\\\n\\vdots & & \\frac{1}{h^2} & -\\frac{2}{h^2}+\\cos(u_{N-2}) & \\frac{1}{h^2} \\\\\n0 & \\dots & 0 & \\frac{1}{h^2} & -\\frac{2}{h^2}+\\cos(u_{N-1})\n\\end{pmatrix}\n$$\nAt each iteration of Newton's method, we must solve the linear system $\\mathbf{J}(\\mathbf{u}^{(k)}) \\Delta\\mathbf{u}^{(k)} = -\\mathbf{F}(\\mathbf{u}^{(k)})$. Since $\\mathbf{J}$ is tridiagonal, this system can be solved very efficiently using the Thomas algorithm (also known as the Tridiagonal Matrix Algorithm, TDMA). The Thomas algorithm is a specialized form of Gaussian elimination with a computational complexity of $O(N)$, which is significantly faster than the $O(N^3)$ complexity required for general dense matrices. This efficiency is crucial for problems with a large number of grid points $N$.\n\n### 4. Stopping Criteria\n\nThe iterative process is terminated when the solution has converged to a desired precision. A common and robust stopping criterion is to check the norm of the residual vector $\\mathbf{F}(\\mathbf{u}^{(k)})$. We stop the iterations when the infinity norm (maximum absolute component) of the residual falls below a small tolerance $\\epsilon$:\n$$\n\\|\\mathbf{F}(\\mathbf{u}^{(k)})\\|_\\infty = \\max_{i} |F_i(\\mathbf{u}^{(k)})| < \\epsilon\n$$\nA typical value for $\\epsilon$ is $10^{-10}$ or smaller. Additionally, a maximum number of iterations is set to prevent infinite loops in case of non-convergence.\n\n### 5. Final Solution Evaluation\n\nAfter the Newton's method converges to a discrete solution vector $\\mathbf{u}_{\\text{sol}} = [u_1, \\dots, u_{N-1}]^T$, the full solution on the grid is constructed by including the boundary points:\n$$\n\\mathbf{u}_{\\text{full}} = [u_0, u_1, \\dots, u_{N-1}, u_N]^T = [0, u_1, \\dots, u_{N-1}, 0]^T\n$$\nThe $L^\\infty$-norm of the computed solution is then the maximum absolute value over this complete set of grid points:\n$$\n\\|\\mathbf{u}_{\\text{full}}\\|_\\infty = \\max_{i=0, \\dots, N} |u_i|\n$$\nThis value is reported for each test case. All trigonometric evaluations must use radians, which is the standard for scientific computing libraries.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef thomas_solver(a, b, c, d):\n    \"\"\"\n    Solves a tridiagonal system of equations Ax = d.\n    a: sub-diagonal (size m-1)\n    b: main diagonal (size m)\n    c: super-diagonal (size m-1)\n    d: right-hand side (size m)\n    \"\"\"\n    m = len(b)\n    # Create copies to avoid modifying the original arrays\n    ac, bc, cc, dc = map(np.copy, (a, b, c, d))\n    \n    # Forward elimination\n    for i in range(1, m):\n        w = ac[i-1] / bc[i-1]\n        bc[i] = bc[i] - w * cc[i-1]\n        dc[i] = dc[i] - w * dc[i-1]\n        \n    # Backward substitution\n    x = np.zeros(m)\n    x[m-1] = dc[m-1] / bc[m-1]\n    for i in range(m-2, -1, -1):\n        x[i] = (dc[i] - cc[i] * x[i+1]) / bc[i]\n        \n    return x\n\ndef newton_bvp_solver(L, N, initial_guess_func, tol=1e-10, max_iter=50):\n    \"\"\"\n    Solves the BVP u'' + sin(u) = 0 with u(0)=u(L)=0 using Newton's method.\n    \n    Args:\n        L (float): Length of the interval [0, L].\n        N (int): Number of subintervals.\n        initial_guess_func (callable): A function u_0(x) for the initial guess.\n        tol (float): Convergence tolerance for the residual norm.\n        max_iter (int): Maximum number of Newton iterations.\n    \n    Returns:\n        numpy.ndarray: The solution vector for the interior grid points.\n    \"\"\"\n    h = L / N\n    m = N - 1  # Number of interior points (unknowns)\n    x_interior = np.linspace(0, L, N + 1)[1:-1]\n    \n    u = initial_guess_func(x_interior)\n    \n    h2_inv = 1.0 / (h * h)\n\n    def compute_residual(u_vec):\n        # Create a padded vector with boundary conditions u(0)=0, u(L)=0\n        u_padded = np.concatenate(([0.0], u_vec, [0.0]))\n        # Compute the finite difference approximation of u''\n        u_xx = (u_padded[2:] - 2 * u_padded[1:-1] + u_padded[:-2]) * h2_inv\n        # Compute the residual F(u) = u'' + sin(u)\n        return u_xx + np.sin(u_vec)\n\n    for k in range(max_iter):\n        # 1. Compute residual F(u)\n        F = compute_residual(u)\n        \n        # 2. Check for convergence\n        res_norm = np.max(np.abs(F))\n        if res_norm < tol:\n            return u\n        \n        # 3. Compute Jacobian diagonals\n        diag_b = -2.0 * h2_inv + np.cos(u)\n        diag_a = np.full(m - 1, h2_inv)\n        diag_c = np.full(m - 1, h2_inv)\n        \n        # 4. Solve J * delta_u = -F for the Newton step delta_u\n        # The Jacobian is symmetric, so sub-diagonal and super-diagonal are identical\n        delta_u = thomas_solver(diag_a, diag_b, diag_c, -F)\n        \n        # 5. Damped update using a simple backtracking line search\n        alpha = 1.0\n        u_new = u + alpha * delta_u\n        res_norm_new = np.max(np.abs(compute_residual(u_new)))\n        \n        # Reduce alpha until the residual norm decreases\n        while res_norm_new >= res_norm and alpha > 1e-4:\n            alpha /= 2.0\n            u_new = u + alpha * delta_u\n            res_norm_new = np.max(np.abs(compute_residual(u_new)))\n\n        # 6. Update solution\n        u = u_new\n        \n        # Check if the update step is too small\n        if np.max(np.abs(alpha * delta_u)) < tol:\n            return u\n\n    raise RuntimeError(f\"Newton's method failed to converge for L={L}, N={N} within {max_iter} iterations.\")\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {'L': 2 * np.pi, 'N': 400, 'u0_func': lambda x: np.zeros_like(x)},\n        {'L': 2 * np.pi, 'N': 400, 'u0_func': lambda x: 2.5 * np.sin(np.pi * x / (2 * np.pi))},\n        {'L': 2 * np.pi, 'N': 400, 'u0_func': lambda x: -2.5 * np.sin(np.pi * x / (2 * np.pi))},\n        {'L': np.pi,     'N': 400, 'u0_func': lambda x: 2.5 * np.sin(np.pi * x / np.pi)}\n    ]\n\n    results = []\n    for case in test_cases:\n        L = case['L']\n        N = case['N']\n        u0_func = case['u0_func']\n        \n        # Compute the numerical solution for the interior points\n        u_solution = newton_bvp_solver(L, N, u0_func)\n        \n        # Construct full solution including boundaries to calculate L-infinity norm\n        u_full = np.concatenate(([0.0], u_solution, [0.0]))\n        \n        # Calculate the L-infinity norm of the full solution\n        L_inf_norm = np.max(np.abs(u_full))\n        results.append(L_inf_norm)\n\n    # Format the final output string exactly as specified\n    # The f-string formatting automatically rounds to the specified number of decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3228545"}, {"introduction": "A powerful numerical tool cannot find a solution that does not exist, and learning to diagnose such situations is a critical skill. This practice challenges you to apply your solver to a boundary value problem whose boundary conditions are physically incompatible with the differential equation. Observing the resulting failure of Newton's method to converge provides a valuable lesson in the importance of analyzing a problem's well-posedness before attempting a numerical solution. [@problem_id:3228408]", "problem": "You are asked to design and implement a robust finite difference solver for one-dimensional, second-order, nonlinear boundary value problems (BVPs) and to use it to demonstrate how physically incompatible boundary data can cause a nonlinear solver to fail to converge. Your program must be self-contained and produce a single line of output summarizing convergence behavior across a small test suite. All quantities in this problem are dimensionless.\n\nFundamental base to use:\n- Definitions and Taylor series: Use the Taylor series of a sufficiently smooth function $y(x)$ about a point $x$ to derive second-order accurate finite difference formulas for the first and second derivatives. For a uniform grid with spacing $h$, this yields second-order accurate approximations for $y'(x)$ and $y''(x)$ in terms of neighboring nodal values.\n- Newton’s method for systems: To solve a nonlinear algebraic system $R(\\mathbf{y}) = \\mathbf{0}$, use Newton’s method in vector form, which linearizes the residual about the current iterate using the Jacobian and iteratively updates the solution.\n\nTask details:\n- Consider the class of nonlinear BVPs on the interval $[0,1]$ of the form\n  $$ y''(x) - f(y(x)) = 0, \\quad x \\in (0,1), $$\n  where $f$ is a smooth nonlinear function. You must discretize the differential operator using a second-order accurate finite difference method on a uniform grid with $N+1$ points (including boundaries), where $h = 1/N$ and $N$ is a positive integer. The discrete unknowns are $\\{y_i\\}_{i=0}^N$ approximating $y(x_i)$ at $x_i = ih$.\n\n- Boundary conditions:\n  - Dirichlet boundary condition at $x=0$ and $x=1$: prescribe $y(0) = \\alpha$ and $y(1) = \\beta$.\n  - Neumann boundary condition at $x=0$ and $x=1$: prescribe $y'(0) = s_0$ and $y'(1) = s_1$.\n  You must derive and implement second-order accurate finite difference formulas for both the interior points and the boundary conditions.\n\n- Nonlinear solve:\n  - Assemble the discrete residual vector $\\mathbf{r}(\\mathbf{y})$ and Jacobian matrix $J(\\mathbf{y})$ associated with the finite difference equations.\n  - Implement Newton’s method with a backtracking line search to attempt to solve $\\mathbf{r}(\\mathbf{y}) = \\mathbf{0}$. The method should:\n    - Start from a reasonable initial guess consistent with the boundary conditions (e.g., linear interpolation for Dirichlet, constant for homogeneous Neumann).\n    - Terminate successfully when the Euclidean (two-)norm of the residual is at most a tolerance $\\varepsilon$.\n    - Declare failure if the maximum number of iterations is reached without meeting the tolerance, if the Jacobian solve fails, or if the line search cannot produce a sufficient decrease in the residual norm.\n\n- Construct and test the following three BVPs:\n  1) Test A (compatible, easy): $$y''(x) + y(x) - y(x)^3 = 0 \\text{ on } [0,1], \\quad y(0)=0,\\ y(1)=0.$$ Here $f(y) = -y + y^3$. This problem admits the trivial solution $y(x) \\equiv 0$ and should be readily solved by Newton’s method from the zero or linear initial guess.\n  2) Test B (physically incompatible, Neumann): $$y''(x) = e^{y(x)} \\text{ on } [0,1], \\quad y'(0)=0,\\ y'(1)=0.$$ Here $f(y)=e^{y}$. Note that $y''(x) = e^{y(x)} > 0$ for all $x$, which implies that $y'(x)$ is strictly increasing on $[0,1]$. The Neumann boundary conditions require $y'(0)=0$ and $y'(1)=0$, which is not possible for a strictly increasing function unless $y'$ is constant, contradicting $y''>0$. Therefore, this continuous BVP has no solution; a well-posed numerical method should fail to converge.\n  3) Test C (compatible, nonlinear Dirichlet): $$y''(x) = e^{y(x)} \\text{ on } [0,1], \\quad y(0) = -2,\\ y(1) = -2.$$ Here $f(y)=e^{y}$. This convex BVP with equal endpoints is compatible and should converge to a nontrivial negative solution.\n\nNumerical parameters to use in all tests:\n- Grid size: $N = 64$ so that $h = 1/N$.\n- Newton tolerance: $\\varepsilon = 10^{-10}$ on the residual two-norm.\n- Maximum Newton iterations: $k_{\\max} = 50$.\n- Backtracking line search parameters should be chosen reasonably (for example, geometric reduction of the step length) to stabilize Newton’s method; justify your design in the solution.\n\nInitial guesses:\n- For Dirichlet problems, use the straight-line interpolant between the boundary values.\n- For homogeneous Neumann problem in Test B, use the constant initial guess consistent with the boundary slopes (for example, $y_i^{(0)} \\equiv 0$).\n\nYour program must:\n- Implement the finite difference residual and Jacobian with second-order accuracy for both interior and boundary conditions based on Taylor expansions.\n- Implement Newton’s method with line search to attempt to solve each test.\n- For each test, return a boolean indicating whether the solver converged, defined as “the final residual two-norm is at most $\\varepsilon$ within at most $k_{\\max}$ iterations using a valid sequence of line-search-accepted Newton steps.”\n\nTest suite and output:\n- You must run the three tests described above with the specified parameters.\n- The final output format must be a single line containing a list of three lowercase boolean values, in order [Test A, Test B, Test C], written exactly as a comma-separated list enclosed in square brackets (for example, “[true,false,true]”). No additional text should be printed.\n\nAngle units do not apply. No physical units are involved in this problem. All answers are pure numbers or booleans as specified. The only outputs are the three booleans in the specified single-line format.", "solution": "The problem requires the design and implementation of a finite difference solver for second-order nonlinear boundary value problems (BVPs), and its application to demonstrate convergence characteristics on three specific test cases. The solver must employ Newton's method with a backtracking line search.\n\nThe general form of the BVP is:\n$$ y''(x) - f(y(x)) = 0, \\quad x \\in (0,1) $$\nsubject to either Dirichlet or Neumann boundary conditions.\n\nThe solution is presented in four parts: finite difference discretization, formulation of the nonlinear system, implementation of Newton's method, and the analysis of each test case.\n\n**Part 1: Finite Difference Discretization**\n\nWe define a uniform grid on the interval $[0,1]$ with $N+1$ points $x_i = ih$ for $i=0, 1, \\dots, N$, where the grid spacing is $h=1/N$. The discrete solution is denoted by the vector $\\mathbf{y} = [y_0, y_1, \\dots, y_N]^T$, where $y_i \\approx y(x_i)$.\n\n**Interior Points Discretization ($i=1, \\dots, N-1$):**\nTo approximate the second derivative $y''(x)$, we use the Taylor series expansions of $y(x+h)$ and $y(x-h)$ around $x$:\n$$ y(x+h) = y(x) + h y'(x) + \\frac{h^2}{2} y''(x) + O(h^3) $$\n$$ y(x-h) = y(x) - h y'(x) + \\frac{h^2}{2} y''(x) + O(h^3) $$\nAdding these two expansions and rearranging for $y''(x)$ yields the second-order accurate central difference formula:\n$$ y''(x) = \\frac{y(x+h) - 2y(x) + y(x-h)}{h^2} + O(h^2) $$\nAt a grid point $x_i$, the differential equation is discretized as:\n$$ \\frac{y_{i+1} - 2y_i + y_{i-1}}{h^2} - f(y_i) = 0 $$\nThis algebraic equation holds for all interior grid points $i=1, \\dots, N-1$.\n\n**Boundary Conditions Discretization:**\n\n**A) Dirichlet Boundary Conditions:** $y(0) = \\alpha, y(1) = \\beta$.\nThese conditions are implemented directly by setting the values at the boundary nodes:\n$$ y_0 = \\alpha $$\n$$ y_N = \\beta $$\nIn this case, $y_0$ and $y_N$ are known constants, not variables. The vector of unknowns consists only of the interior points, $\\mathbf{u} = [y_1, y_2, \\dots, y_{N-1}]^T \\in \\mathbb{R}^{N-1}$. The equations for $i=1$ and $i=N-1$ are modified to incorporate the known boundary values:\nFor $i=1$: $\\frac{y_2 - 2y_1 + \\alpha}{h^2} - f(y_1) = 0$.\nFor $i=N-1$: $\\frac{\\beta - 2y_{N-1} + y_{N-2}}{h^2} - f(y_{N-1}) = 0$.\n\n**B) Neumann Boundary Conditions:** $y'(0) = s_0, y'(1) = s_1$.\nTo maintain second-order accuracy, we use the method of ghost points. At the left boundary $x_0=0$, we introduce a ghost point $x_{-1} = -h$. The central difference approximation for the derivative at $x_0$ is:\n$$ y'(0) \\approx \\frac{y_1 - y_{-1}}{2h} = s_0 \\implies y_{-1} = y_1 - 2hs_0 $$\nThe differential equation is then enforced at the boundary point $x_0$:\n$$ \\frac{y_1 - 2y_0 + y_{-1}}{h^2} - f(y_0) = 0 $$\nSubstituting the expression for the ghost point $y_{-1}$, we obtain the first equation of our system:\n$$ \\frac{y_1 - 2y_0 + (y_1 - 2hs_0)}{h^2} - f(y_0) = 0 \\implies \\frac{2y_1 - 2y_0}{h^2} - \\frac{2s_0}{h} - f(y_0) = 0 $$\nSimilarly, at the right boundary $x_N=1$, we introduce a ghost point $x_{N+1} = 1+h$. The central difference for $y'(1)$ gives:\n$$ y'(1) \\approx \\frac{y_{N+1} - y_{N-1}}{2h} = s_1 \\implies y_{N+1} = y_{N-1} + 2hs_1 $$\nEnforcing the DE at $x_N$ and substituting for $y_{N+1}$ gives the last equation of our system:\n$$ \\frac{(y_{N-1} + 2hs_1) - 2y_N + y_{N-1}}{h^2} - f(y_N) = 0 \\implies \\frac{2y_{N-1} - 2y_N}{h^2} + \\frac{2s_1}{h} - f(y_N) = 0 $$\nFor Neumann problems, all grid points $y_0, \\dots, y_N$ are unknowns, so the system size is $(N+1) \\times (N+1)$.\n\n**Part 2: Formulation of the Nonlinear System**\n\nThe set of discrete finite difference equations forms a nonlinear algebraic system $\\mathbf{r}(\\mathbf{y}) = \\mathbf{0}$. We need to define the residual vector $\\mathbf{r}$ and its Jacobian matrix $J$, where $J_{ij} = \\partial r_i / \\partial y_j$.\n\n**A) Dirichlet Case (Tests A and C):**\nThe vector of unknowns is $\\mathbf{u} = [y_1, \\dots, y_{N-1}]^T$ of size $N-1$. The residual vector $\\mathbf{r}(\\mathbf{u})$ has components $r_k(\\mathbf{u})$ for $k=1, \\dots, N-1$:\n- For $k=1$: $r_1(\\mathbf{u}) = \\frac{y_2 - 2y_1 + \\alpha}{h^2} - f(y_1)$\n- For $k \\in \\{2, \\dots, N-2\\}$: $r_k(\\mathbf{u}) = \\frac{y_{k+1} - 2y_k + y_{k-1}}{h^2} - f(y_k)$\n- For $k=N-1$: $r_{N-1}(\\mathbf{u}) = \\frac{\\beta - 2y_{N-1} + y_{N-2}}{h^2} - f(y_{N-1})$\n\nThe Jacobian $J(\\mathbf{u})$ is an $(N-1) \\times (N-1)$ tridiagonal matrix. The non-zero entries of row $k$ (corresponding to variable $y_k$) are:\n- $\\frac{\\partial r_k}{\\partial y_{k-1}} = \\frac{1}{h^2}$ for $k > 1$\n- $\\frac{\\partial r_k}{\\partial y_k} = -\\frac{2}{h^2} - f'(y_k)$\n- $\\frac{\\partial r_k}{\\partial y_{k+1}} = \\frac{1}{h^2}$ for $k < N-1$\n\n**B) Neumann Case (Test B):**\nThe vector of unknowns is $\\mathbf{y} = [y_0, \\dots, y_N]^T$ of size $N+1$. The residual vector $\\mathbf{r}(\\mathbf{y})$ has components:\n- $r_0(\\mathbf{y}) = \\frac{2y_1 - 2y_0}{h^2} - \\frac{2s_0}{h} - f(y_0)$\n- $r_i(\\mathbf{y}) = \\frac{y_{i+1} - 2y_i + y_{i-1}}{h^2} - f(y_i)$ for $i=1, \\dots, N-1$\n- $r_N(\\mathbf{y}) = \\frac{2y_{N-1} - 2y_N}{h^2} + \\frac{2s_1}{h} - f(y_N)$\n\nThe Jacobian $J(\\mathbf{y})$ is an $(N+1) \\times (N+1)$ matrix. It is also tridiagonal up to a factor of $2$ in the corners:\n- Row $0$: $\\frac{\\partial r_0}{\\partial y_0} = -\\frac{2}{h^2} - f'(y_0)$, $\\frac{\\partial r_0}{\\partial y_1} = \\frac{2}{h^2}$\n- Row $i$ ($1 \\le i \\le N-1$): $\\frac{\\partial r_i}{\\partial y_{i-1}} = \\frac{1}{h^2}$, $\\frac{\\partial r_i}{\\partial y_i} = -\\frac{2}{h^2} - f'(y_i)$, $\\frac{\\partial r_i}{\\partial y_{i+1}} = \\frac{1}{h^2}$\n- Row $N$: $\\frac{\\partial r_N}{\\partial y_{N-1}} = \\frac{2}{h^2}$, $\\frac{\\partial r_N}{\\partial y_N} = -\\frac{2}{h^2} - f'(y_N)$\n\n**Part 3: Newton's Method with Backtracking Line Search**\n\nGiven an initial guess $\\mathbf{y}^{(0)}$, Newton's method iteratively refines the solution. At each iteration $k$, we solve the linear system for the update step $\\Delta \\mathbf{y}^{(k)}$:\n$$ J(\\mathbf{y}^{(k)}) \\Delta \\mathbf{y}^{(k)} = -\\mathbf{r}(\\mathbf{y}^{(k)}) $$\nThe full Newton update would be $\\mathbf{y}^{(k+1)} = \\mathbf{y}^{(k)} + \\Delta \\mathbf{y}^{(k)}$. To improve global convergence, we introduce a backtracking line search. We find a step length $\\lambda \\in (0,1]$ such that the new iterate $\\mathbf{y}^{(k+1)} = \\mathbf{y}^{(k)} + \\lambda \\Delta \\mathbf{y}^{(k)}$ provides a sufficient decrease in the residual norm. We implement a simple version where we seek to satisfy:\n$$ ||\\mathbf{r}(\\mathbf{y}^{(k)} + \\lambda \\Delta \\mathbf{y}^{(k)})||_2 < ||\\mathbf{r}(\\mathbf{y}^{(k)})||_2 $$\nThe line search algorithm is:\n1. Start with $\\lambda = 1$.\n2. Calculate the potential next iterate $\\mathbf{y}_{next} = \\mathbf{y}^{(k)} + \\lambda \\Delta \\mathbf{y}^{(k)}$.\n3. If $||\\mathbf{r}(\\mathbf{y}_{next})||_2 < ||\\mathbf{r}(\\mathbf{y}^{(k)})||_2$, accept the step and set $\\mathbf{y}^{(k+1)} = \\mathbf{y}_{next}$.\n4. If not, reduce $\\lambda$ by a factor (e.g., $\\lambda \\leftarrow \\lambda/2$) and return to step 2.\n5. If $\\lambda$ becomes smaller than a minimum threshold or a maximum number of backtracks is reached, the line search fails, and the Newton solver terminates with failure.\n\nThe overall Newton solver terminates successfully if $||\\mathbf{r}(\\mathbf{y}^{(k)})||_2 \\le \\varepsilon$ within $k_{\\max}$ iterations. It fails if $k_{\\max}$ is reached, the linear solve for $\\Delta\\mathbf{y}$ fails (e.g., singular Jacobian), or the line search fails.\n\n**Part 4: Analysis of Test Cases**\n\nThe numerical parameters are fixed: $N=64$ ($h=1/64$), $\\varepsilon = 10^{-10}$, $k_{\\max}=50$.\n\n**1) Test A (compatible, Dirichlet):**\n- Equation: $y''(x) + y(x) - y(x)^3 = 0 \\implies f(y) = -y + y^3$. The derivative is $f'(y) = -1 + 3y^2$.\n- BCs: $y(0)=0, y(1)=0$.\n- Initial Guess: The linear interpolant is $y_i^{(0)} = 0$ for all $i=0, \\dots, N$.\n- Analysis: With the initial guess $y_i=0$, the discrete residual equations are evaluated. For any interior point $i$, the residual is $r_i = \\frac{0 - 2(0) + 0}{h^2} - f(0) = 0 - (0-0) = 0$. The residual vector is identically zero from the start. The solver will calculate the initial residual norm as $0$, which is less than $\\varepsilon$, and terminate on the first check with success.\n- Expected Result: `true`.\n\n**2) Test B (physically incompatible, Neumann):**\n- Equation: $y''(x) = e^{y(x)} \\implies f(y) = e^y$. The derivative is $f'(y) = e^y$.\n- BCs: $y'(0)=0, y'(1)=0$ (i.e., $s_0=0, s_1=0$).\n- Initial Guess: A constant, $y_i^{(0)} = 0$ for all $i=0, \\dots, N$.\n- Analysis: As established during the problem validation, the continuous BVP has no solution because $y'' = e^y > 0$ implies $y'(x)$ is strictly increasing, which contradicts $y'(0)=y'(1)=0$. The numerical solver is expected to fail. With the initial guess $y_i=0$, the residual at an interior point is $r_i = \\frac{0-2(0)+0}{h^2} - e^0 = -1$. The boundary residuals are $r_0 = \\frac{2(0)-2(0)}{h^2} - e^0 = -1$ and $r_N = \\frac{2(0)-2(0)}{h^2} - e^0 = -1$. The initial residual norm is large. Newton's method will attempt to find a solution that cannot exist. The iterates are likely to grow without bound to satisfy the convexity ($y''>0$), leading to numerical overflow in $e^y$ or failure of the line search to find any descent direction, ultimately causing the solver to exceed the maximum number of iterations.\n- Expected Result: `false`.\n\n**3) Test C (compatible, nonlinear Dirichlet):**\n- Equation: $y''(x) = e^{y(x)} \\implies f(y) = e^y$, with $f'(y) = e^y$.\n- BCs: $y(0)=-2, y(1)=-2$.\n- Initial Guess: The linear interpolant is the constant function $y_i^{(0)} = -2$ for all $i=0, \\dots, N$.\n- Analysis: This BVP is well-posed and has a unique, non-trivial solution. The solution is symmetric about $x=1/2$ and $y(x) \\le -2$ for all $x \\in [0,1]$. With the initial guess $y_i=-2$, the term $f(y_i)=e^{-2}$ is small and positive. The initial residual is non-zero, $r_i = \\frac{-2 - 2(-2) + (-2)}{h^2} - e^{-2} = -e^{-2}$. The initial residual norm is small, and the Jacobian is well-conditioned. Newton's method is expected to converge rapidly from this excellent initial guess to the true solution.\n- Expected Result: `true`.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_bvp(f, df, N, bc_type, bc_vals, tol, max_iter):\n    \"\"\"\n    Solves a 1D nonlinear BVP y'' - f(y) = 0 using a finite difference method\n    and Newton's method with a backtracking line search.\n    \"\"\"\n    h = 1.0 / N\n    \n    # --- System Setup ---\n    if bc_type == 'dirichlet':\n        # Unknowns are y_1, ..., y_{N-1}\n        num_unknowns = N - 1\n        y = np.zeros(N + 1)\n        alpha, beta = bc_vals\n        y[0], y[N] = alpha, beta\n        \n        # Initial guess: linear interpolation\n        y[1:N] = np.linspace(alpha, beta, N + 1)[1:N]\n        \n        # Extract the vector of unknowns for the Newton solver\n        u = y[1:N].copy()\n\n    elif bc_type == 'neumann':\n        # Unknowns are y_0, ..., y_N\n        num_unknowns = N + 1\n        s0, s1 = bc_vals\n        \n        # Initial guess: constant\n        y = np.zeros(N + 1)\n        u = y.copy()\n\n    else:\n        raise ValueError(\"Unknown boundary condition type\")\n\n    # --- Newton's Method Loop ---\n    for k in range(max_iter):\n        \n        if bc_type == 'dirichlet':\n            y[1:N] = u\n        else: # neumann\n            y = u\n\n        # --- Assemble Residual Vector r ---\n        r = np.zeros(num_unknowns)\n        fy = f(y)\n\n        if bc_type == 'dirichlet':\n            # Interior points\n            for i in range(1, N):\n                # Map from u-index (0..N-2) to y-index (1..N-1)\n                # u-index j corresponds to y-index j+1\n                y_im1 = y[i-1]\n                y_i = y[i]\n                y_ip1 = y[i+1]\n                r[i-1] = (y_ip1 - 2 * y_i + y_im1) / h**2 - fy[i]\n        \n        elif bc_type == 'neumann':\n            # Boundary at x=0\n            r[0] = (2*y[1] - 2*y[0]) / h**2 - (2*s0)/h - fy[0]\n            # Interior points\n            for i in range(1, N):\n                r[i] = (y[i+1] - 2*y[i] + y[i-1]) / h**2 - fy[i]\n            # Boundary at x=1\n            r[N] = (2*y[N-1] - 2*y[N]) / h**2 + (2*s1)/h - fy[N]\n\n        # Check for convergence\n        res_norm = np.linalg.norm(r)\n        if res_norm <= tol:\n            return True\n\n        # --- Assemble Jacobian Matrix J ---\n        J = np.zeros((num_unknowns, num_unknowns))\n        dfy = df(y)\n\n        if bc_type == 'dirichlet':\n            for i in range(num_unknowns):\n                # u-index i corresponds to y-index i+1\n                J[i, i] = -2.0 / h**2 - dfy[i+1]\n                if i > 0:\n                    J[i, i-1] = 1.0 / h**2\n                if i < num_unknowns - 1:\n                    J[i, i+1] = 1.0 / h**2\n        \n        elif bc_type == 'neumann':\n            # Row 0\n            J[0, 0] = -2.0 / h**2 - dfy[0]\n            J[0, 1] = 2.0 / h**2\n            # Rows 1 to N-1\n            for i in range(1, N):\n                J[i, i-1] = 1.0 / h**2\n                J[i, i] = -2.0 / h**2 - dfy[i]\n                J[i, i+1] = 1.0 / h**2\n            # Row N\n            J[N, N-1] = 2.0 / h**2\n            J[N, N] = -2.0 / h**2 - dfy[N]\n\n        # --- Solve linear system for Newton step ---\n        try:\n            delta_u = np.linalg.solve(J, -r)\n        except np.linalg.LinAlgError:\n            return False # Jacobian is singular\n\n        # --- Backtracking Line Search ---\n        lambda_step = 1.0\n        max_backtrack = 10\n        step_accepted = False\n        for _ in range(max_backtrack):\n            u_new = u + lambda_step * delta_u\n            \n            if bc_type == 'dirichlet':\n                y_new = y.copy()\n                y_new[1:N] = u_new\n            else: # neumann\n                y_new = u_new\n            \n            fy_new = f(y_new)\n            r_new = np.zeros(num_unknowns)\n            \n            if bc_type == 'dirichlet':\n                for i in range(1, N):\n                    y_im1_new = y_new[i-1]\n                    y_i_new = y_new[i]\n                    y_ip1_new = y_new[i+1]\n                    r_new[i-1] = (y_ip1_new - 2*y_i_new + y_im1_new) / h**2 - fy_new[i]\n            \n            elif bc_type == 'neumann':\n                r_new[0] = (2*y_new[1] - 2*y_new[0]) / h**2 - (2*s0)/h - fy_new[0]\n                for i in range(1, N):\n                    r_new[i] = (y_new[i+1] - 2*y_new[i] + y_new[i-1]) / h**2 - fy_new[i]\n                r_new[N] = (2*y_new[N-1] - 2*y_new[N]) / h**2 + (2*s1)/h - fy_new[N]\n                \n            new_res_norm = np.linalg.norm(r_new)\n\n            if new_res_norm < res_norm:\n                u = u_new\n                step_accepted = True\n                break\n            \n            lambda_step /= 2.0\n\n        if not step_accepted:\n            return False # Line search failed\n\n    # If loop finishes without convergence\n    return False\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Numerical parameters\n    N = 64\n    tol = 1e-10\n    max_iter = 50\n\n    # Test cases\n    test_cases = [\n        {\n            \"name\": \"Test A\",\n            \"f\": lambda y: -y + y**3,\n            \"df\": lambda y: -1 + 3*y**2,\n            \"bc_type\": \"dirichlet\",\n            \"bc_vals\": (0.0, 0.0),\n        },\n        {\n            \"name\": \"Test B\",\n            \"f\": lambda y: np.exp(y),\n            \"df\": lambda y: np.exp(y),\n            \"bc_type\": \"neumann\",\n            \"bc_vals\": (0.0, 0.0),\n        },\n        {\n            \"name\": \"Test C\",\n            \"f\": lambda y: np.exp(y),\n            \"df\": lambda y: np.exp(y),\n            \"bc_type\": \"dirichlet\",\n            \"bc_vals\": (-2.0, -2.0),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        converged = solve_bvp(\n            f=case[\"f\"],\n            df=case[\"df\"],\n            N=N,\n            bc_type=case[\"bc_type\"],\n            bc_vals=case[\"bc_vals\"],\n            tol=tol,\n            max_iter=max_iter,\n        )\n        results.append(str(converged).lower())\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3228408"}]}