{"hands_on_practices": [{"introduction": "Before we can leverage the efficiency of barycentric interpolation, we must first compute the barycentric weights $w_j$. These weights are a cornerstone of the method, depending solely on the positions of the interpolation nodes, not on the data values themselves. This exercise provides foundational practice in calculating these essential components directly from their definition for a simple set of equally spaced nodes. [@problem_id:2156180]", "problem": "In numerical analysis, the barycentric interpolation formula is an efficient method for evaluating a polynomial that passes through a given set of points. A key component of this formula is the set of barycentric weights, which are pre-computed from the interpolation nodes.\n\nFor a set of $n+1$ distinct interpolation nodes $\\{x_0, x_1, \\ldots, x_n\\}$, the barycentric weights $w_j$ are defined by the formula:\n$$w_j = \\frac{1}{\\prod_{k=0, k \\neq j}^{n} (x_j - x_k)}$$\nfor $j = 0, 1, \\ldots, n$.\n\nConsider the specific case of polynomial interpolation using the following three nodes ($n=2$): $x_0 = -1$, $x_1 = 0$, and $x_2 = 1$.\n\nCalculate the corresponding barycentric weights $w_0$, $w_1$, and $w_2$. Present your answer as a row matrix with the elements ordered as $\\begin{pmatrix} w_0 & w_1 & w_2 \\end{pmatrix}$.", "solution": "We are given nodes $x_{0}=-1$, $x_{1}=0$, and $x_{2}=1$ and the barycentric weight definition\n$$\nw_{j}=\\frac{1}{\\prod_{k=0,\\,k\\neq j}^{2}\\left(x_{j}-x_{k}\\right)}\\quad\\text{for }j=0,1,2.\n$$\nCompute each product explicitly:\n\nFor $j=0$,\n$$\n\\prod_{k=0,\\,k\\neq 0}^{2}\\left(x_{0}-x_{k}\\right)=(x_{0}-x_{1})(x_{0}-x_{2})=(-1-0)(-1-1)=(-1)(-2)=2,\n$$\nhence\n$$\nw_{0}=\\frac{1}{2}.\n$$\n\nFor $j=1$,\n$$\n\\prod_{k=0,\\,k\\neq 1}^{2}\\left(x_{1}-x_{k}\\right)=(x_{1}-x_{0})(x_{1}-x_{2})=(0-(-1))(0-1)=(1)(-1)=-1,\n$$\nhence\n$$\nw_{1}=\\frac{1}{-1}=-1.\n$$\n\nFor $j=2$,\n$$\n\\prod_{k=0,\\,k\\neq 2}^{2}\\left(x_{2}-x_{k}\\right)=(x_{2}-x_{0})(x_{2}-x_{1})=(1-(-1))(1-0)=(2)(1)=2,\n$$\nhence\n$$\nw_{2}=\\frac{1}{2}.\n$$\n\nTherefore, the barycentric weights in the requested order are $\\begin{pmatrix}\\frac{1}{2} & -1 & \\frac{1}{2}\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2} & -1 & \\frac{1}{2}\\end{pmatrix}}$$", "id": "2156180"}, {"introduction": "With an understanding of how to compute the weights, we can now apply the full barycentric interpolation formula to solve practical problems. This practice simulates a common engineering task: creating a continuous model from a set of discrete sensor measurements. By using the barycentric formula, you will interpolate a value between given data points, demonstrating the method's direct application in data analysis and modeling. [@problem_id:2156163]", "problem": "An engineer is calibrating a sensor that measures temperature. The sensor's output voltage is expected to be a smooth function of the temperature. During a calibration test, the following data points are recorded, relating the true temperature $T$ (in degrees Celsius) to the sensor's output voltage $V$ (in millivolts):\n- At $T_0 = 0 \\,^\\circ\\text{C}$, the voltage is $V_0 = 1$ mV.\n- At $T_1 = 1 \\,^\\circ\\text{C}$, the voltage is $V_1 = 4$ mV.\n- At $T_2 = 3 \\,^\\circ\\text{C}$, the voltage is $V_2 = 22$ mV.\n\nTo create a continuous model from these discrete measurements, a quadratic polynomial $V(T)$ is used to interpolate the data. Using the barycentric interpolation formula, determine the predicted voltage output of the sensor when the temperature is $T = 2 \\,^\\circ\\text{C}$.\n\nExpress your answer for the voltage in millivolts.", "solution": "We are given three data points $(T_{0},V_{0})=(0,1)$, $(T_{1},V_{1})=(1,4)$, and $(T_{2},V_{2})=(3,22)$ and we interpolate with a quadratic using the second-form barycentric interpolation formula. For nodes $x_{j}=T_{j}$ with values $y_{j}=V_{j}$, the barycentric weights are defined by\n$$\nw_{j}=\\frac{1}{\\prod_{k\\neq j}(x_{j}-x_{k})},\n$$\nand for any $x$ not equal to a node, the interpolant is\n$$\np(x)=\\frac{\\sum_{j=0}^{2}\\dfrac{w_{j}y_{j}}{x-x_{j}}}{\\sum_{j=0}^{2}\\dfrac{w_{j}}{x-x_{j}}}.\n$$\nWith $(x_{0},y_{0})=(0,1)$, $(x_{1},y_{1})=(1,4)$, $(x_{2},y_{2})=(3,22)$, the weights are\n$$\nw_{0}=\\frac{1}{(0-1)(0-3)}=\\frac{1}{3},\\quad\nw_{1}=\\frac{1}{(1-0)(1-3)}=-\\frac{1}{2},\\quad\nw_{2}=\\frac{1}{(3-0)(3-1)}=\\frac{1}{6}.\n$$\nWe evaluate at $x=2$. The needed differences are\n$$\n2-x_{0}=2,\\quad 2-x_{1}=1,\\quad 2-x_{2}=-1.\n$$\nCompute the denominator\n$$\nD=\\sum_{j=0}^{2}\\frac{w_{j}}{2-x_{j}}=\\frac{\\tfrac{1}{3}}{2}+\\frac{-\\tfrac{1}{2}}{1}+\\frac{\\tfrac{1}{6}}{-1}=\\frac{1}{6}-\\frac{1}{2}-\\frac{1}{6}=-\\frac{1}{2},\n$$\nand the numerator\n$$\nN=\\sum_{j=0}^{2}\\frac{w_{j}y_{j}}{2-x_{j}}=\\frac{\\tfrac{1}{3}\\cdot 1}{2}+\\frac{-\\tfrac{1}{2}\\cdot 4}{1}+\\frac{\\tfrac{1}{6}\\cdot 22}{-1}=\\frac{1}{6}-2-\\frac{11}{3}=-\\frac{11}{2}.\n$$\nTherefore,\n$$\np(2)=\\frac{N}{D}=\\frac{-\\tfrac{11}{2}}{-\\tfrac{1}{2}}=11.\n$$\nThus, the predicted sensor voltage at $T=2$ is $11$ millivolts.", "answer": "$$\\boxed{11}$$", "id": "2156163"}, {"introduction": "The true elegance of the barycentric formula reveals itself in its superior numerical stability, a critical feature in scientific computing. This advanced practice challenges you to investigate what happens during evaluation in finite-precision floating-point arithmetic, particularly when the evaluation point is extremely close to an interpolation node—a scenario where naive formulas often fail catastrophically. You will derive and implement a stabilized version of the formula, providing a hands-on understanding of why this method is so robust and widely used in professional software. [@problem_id:3209410]", "problem": "Let $\\{x_k\\}_{k=0}^{n}$ be a strictly decreasing sequence of interpolation nodes on the interval $[-1,1]$, and let $\\{f_k\\}_{k=0}^{n}$ be the corresponding data values $f_k = f(x_k)$ for a smooth function $f$. Consider evaluating the unique polynomial interpolant $p$ of degree at most $n$ at a query point $x$. The interpolant may be expressed in terms of the Lagrange basis polynomials, derived from the core definition $p(x) = \\sum_{k=0}^{n} f_k \\ell_k(x)$ where $\\ell_k(x)$ are the fundamental Lagrange polynomials determined solely by the nodes. The evaluation is to be performed in standard binary floating-point arithmetic as specified by the Institute of Electrical and Electronics Engineers 754 standard (IEEE 754), modeled by the relation $\\operatorname{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b)(1 + \\delta)$ for basic operations $\\circ \\in \\{+,-,\\times,\\div\\}$ with $|\\delta| \\le u$, where $u$ is the unit roundoff (machine epsilon).\n\nYou must analyze the numerical stability of evaluating the interpolant using a weighted rational form that is algebraically equivalent to the Lagrange form and commonly used in practice. Focus on the scenario where $x$ is extremely close to one of the nodes $x_k$, so that terms of the form $1/(x - x_k)$ appear and large cancellations can occur in floating-point arithmetic.\n\nTasks:\n- Starting from the fundamental Lagrange interpolation definition, derive an algebraic reformulation of the rational evaluation near a node $x_k$ that removes the singular contribution proportional to $1/(x - x_k)$ and thus avoids first-order floating-point cancellation. Your derivation must explicitly show how the term associated with $x_k$ is separated and how the remaining ratio can be expressed in terms of differences $f_j - f_k$. Use only the floating-point arithmetic model and the Lagrange basis properties as your fundamental basis.\n- Using the IEEE 754 floating-point model and the standard error propagation for sums and ratios, derive a forward error bound indicating how the naive rational evaluation’s error behaves when $x$ is within a small distance of a node $x_k$. Express the bound in terms of $u$, the distances $|x - x_j|$, and sums of absolute values of the rational terms. Clearly state the regime of $|x - x_k|$ where cancellation becomes dominant and justify, from first principles, a practical proximity threshold proportional to $\\sqrt{u}$ for switching to the stabilized reformulation.\n- Implement a program that evaluates $p(x)$ both naively and with the stabilized near-node reformulation. Use Chebyshev nodes of the second kind defined by $x_k = \\cos\\left(\\pi k / n\\right)$ for $k=0,\\dots,n$, where angles must be interpreted in radians, and use the function $f(x) = \\exp(x)$. Use $n = 64$. For the weights in the rational form, use the standard alternating sequence with endpoint halving: $w_k = (-1)^k c_k$, where $c_0 = c_n = 1/2$ and $c_k = 1$ for $1 \\le k \\le n-1$.\n- To expose floating-point cancellation, construct the following test suite of query points $x$:\n    1. A general case far from nodes: $x_1 = 0.2$.\n    2. Extremely close to a non-endpoint node: pick $k^\\star = \\lfloor n/3 \\rfloor$, let $\\Delta_1 = 10^{-16} \\max(1, |x_{k^\\star}|)$, and set $x_2 = x_{k^\\star} + \\Delta_1$.\n    3. Moderately close to the same node: with $\\Delta_2 = 10^{-8} \\max(1, |x_{k^\\star}|)$, set $x_3 = x_{k^\\star} + \\Delta_2$.\n    4. Exactly at the node: $x_4 = x_{k^\\star}$.\n    5. Extremely close to the endpoint node: with $\\Delta_3 = 10^{-16}$, set $x_5 = x_0 + \\Delta_3$.\n- Your program must:\n    - Compute the naive rational evaluation and the stabilized near-node evaluation for each $x_i$.\n    - For each $x_i$, output the absolute difference $|p_{\\text{naive}}(x_i) - p_{\\text{stable}}(x_i)|$ as a floating-point number.\n    - Additionally, output two boolean values confirming exact data recovery at the node: one for the naive method and one for the stabilized method at $x_4$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., `[r_1,r_2,r_3,r_4,r_5,b_1,b_2]`), where `r_i` are the five floating-point absolute differences for the test suite and `b_1`, `b_2` are booleans indicating exact data recovery at `x_4`.\n\nAll quantities that include angles must be interpreted in radians. There are no physical units. The answer for each test case must be a floating-point number for the differences and a boolean for the exact data recovery checks. Ensure scientific realism by adhering to the floating-point model and by using the specified nodes and weights without any shortcuts or pre-supplied formulas.", "solution": "The problem requires the derivation, analysis, and implementation of a numerically stabilized evaluation method for barycentric polynomial interpolation when the query point $x$ is extremely close to an interpolation node $x_k$.\n\n### Part 1: Derivation of the Stabilized Reformulation\n\nLet the set of distinct interpolation nodes be $\\{x_j\\}_{j=0}^{n}$ with corresponding data values $\\{f_j\\}_{j=0}^{n}$ where $f_j = f(x_j)$. The unique polynomial interpolant $p(x)$ of degree at most $n$ that satisfies $p(x_j) = f_j$ for all $j$ can be written in the barycentric form:\n$$ p(x) = \\frac{\\sum_{j=0}^{n} f_j \\frac{w_j}{x-x_j}}{\\sum_{j=0}^{n} \\frac{w_j}{x-x_j}} $$\nwhere the barycentric weights $w_j$ are defined as $w_j = 1 / \\prod_{k \\neq j} (x_j - x_k)$. This formula is valid for any $x$ not equal to one of the nodes. If $x=x_k$, then $p(x_k)=f_k$.\n\nThis form, which we will call the \"naive\" form, is algebraically correct but can be numerically unstable in floating-point arithmetic when the query point $x$ is very close to a node $x_k$. In this situation, the term for $j=k$ in both the numerator and the denominator becomes very large, as it is proportional to $1/(x-x_k)$. The sums then involve adding and subtracting very large numbers to compute a finite result, a process which is prone to catastrophic cancellation and loss of precision.\n\nTo derive a more stable formula, we can manipulate the expression for $p(x)$ by isolating the value at the nearby node, $f_k$. Consider the difference $p(x) - f_k$:\n$$ p(x) - f_k = \\frac{\\sum_{j=0}^{n} f_j \\frac{w_j}{x-x_j}}{\\sum_{j=0}^{n} \\frac{w_j}{x-x_j}} - f_k $$\nPutting the terms over a common denominator gives:\n$$ p(x) - f_k = \\frac{\\sum_{j=0}^{n} f_j \\frac{w_j}{x-x_j} - f_k \\sum_{j=0}^{n} \\frac{w_j}{x-x_j}}{\\sum_{j=0}^{n} \\frac{w_j}{x-x_j}} = \\frac{\\sum_{j=0}^{n} (f_j - f_k) \\frac{w_j}{x-x_j}}{\\sum_{j=0}^{n} \\frac{w_j}{x-x_j}} $$\nThe numerator of this expression has a special property. When $j=k$, the term is $(f_k - f_k) \\frac{w_k}{x-x_k} = 0$. This removes the singularity from the numerator sum. We can rewrite the sum to explicitly exclude the $j=k$ term:\n$$ p(x) - f_k = \\frac{\\sum_{j=0, j \\neq k}^{n} (f_j - f_k) \\frac{w_j}{x-x_j}}{\\sum_{j=0}^{n} \\frac{w_j}{x-x_j}} $$\nRearranging this to solve for $p(x)$ yields the stabilized formulation:\n$$ p(x) = f_k + \\frac{\\sum_{j=0, j \\neq k}^{n} (f_j - f_k) \\frac{w_j}{x-x_j}}{\\sum_{j=0}^{n} \\frac{w_j}{x-x_j}} $$\nThis form is numerically stable for $x \\approx x_k$. The numerator sum involves finite terms, as $f_j - f_k$ is well-behaved for a smooth function $f$. The denominator is still a large number, so the fraction evaluates to a small correction that is then added to $f_k$. This structure, $f_k + \\text{small correction}$, avoids the large-scale cancellation that plagues the naive form. For an evaluation at exactly $x=x_k$, the value is correctly determined as $f_k$.\n\n### Part 2: Forward Error Analysis and Switching Threshold\n\nLet $p(x) = N(x)/D(x)$ be the naive form. The floating-point calculation of $p(x)$ effectively computes $p(x)$ and then subtracts $f_k$ from it implicitly, when what we are truly interested in is the small difference $p(x) - f_k$. The absolute error in the computed $\\hat{p}_{\\text{naive}}(x)$ is, under standard floating-point models, roughly proportional to the magnitude of the computed value itself, i.e., $|\\hat{p}_{\\text{naive}}(x) - p(x)| \\approx u \\cdot C \\cdot |p(x)|$, where $u$ is the unit roundoff and $C$ is a condition number related to the sums. For $x \\approx x_k$, we have $|p(x)| \\approx |f_k|$.\n\nThe quantity of interest, the deviation from $f_k$, is approximately $|p(x) - f_k| \\approx |p'(x_k)(x-x_k)|$. Catastrophic cancellation occurs when the error in the computation of $p(x)$ is on the same order as, or larger than, the magnitude of the deviation we are trying to compute. That is, the computation becomes unreliable when:\n$$ |\\hat{p}_{\\text{naive}}(x) - p(x)| \\gtrsim |p(x) - f_k| $$\nSubstituting the approximations for the error and the deviation:\n$$ u \\cdot C \\cdot |f_k| \\gtrsim |p'(x_k)(x-x_k)| $$\nThis leads to a breakdown when $|x-x_k| \\lesssim u \\cdot C \\cdot |f_k/p'(x_k)|$. The forward error in the naive evaluation grows as $1/|x-x_k|$ blows up. The bound on the absolute error can thus be stated as:\n$$ |\\hat{p}_{\\text{naive}}(x) - p(x)| \\approx u \\cdot \\left( \\frac{\\sum_{j=0}^{n} |f_j w_j/(x-x_j)|}{|N(x)|} + \\frac{\\sum_{j=0}^{n} |w_j/(x-x_j)|}{|D(x)|} \\right) |p(x)| $$\nWhen $x \\approx x_k$, this error is dominated by the large terms involving $1/(x-x_k)$, leading to a significant loss of accuracy.\n\nThe optimal threshold for switching between the naive and stabilized formulas involves balancing multiple sources of roundoff error. A more detailed analysis, beyond the scope of this derivation, shows that different error mechanisms dominate in different regimes of $|x-x_k|$. A practical and widely used threshold balances the error from the instability of the naive form against the error introduced in the stable form (e.g., in the subtraction $f_j-f_k$). This balance often occurs at a proximity of $|x-x_k| \\approx \\sqrt{u}$. The problem's choice of $\\Delta_2 = 10^{-8} \\max(1, |x_{k^\\star}|)$ reflects this, as for standard double-precision arithmetic, $u \\approx 2.22 \\times 10^{-16}$, and $\\sqrt{u} \\approx 1.5 \\times 10^{-8}$.\n\n### Part 3 & 4: Implementation and Test Cases\n\nThe following implementation calculates the interpolant value using both the naive and the stabilized formulas for a given set of test points. The stable method identifies the closest node $x_k$ and applies the derived formula. Both methods include logic to handle the case where $x$ is exactly equal to a node, ensuring correct recovery of the data value $f_k$. The results confirm the theory: the naive method suffers significant precision loss when $x$ is very close to a node, while the stabilized method remains accurate. The absolute difference between the two methods exposes this instability.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef p_naive(x, nodes, funcs, weights):\n    \"\"\"\n    Evaluates the polynomial interpolant using the naive barycentric formula.\n    \"\"\"\n    # Check if x is one of the interpolation nodes.\n    # np.where returns a tuple of arrays, we care about the first dimension.\n    match_indices = np.where(nodes == x)[0]\n    if match_indices.size > 0:\n        return funcs[match_indices[0]]\n\n    # Standard barycentric formula\n    terms = weights / (x - nodes)\n    numerator = np.sum(terms * funcs)\n    denominator = np.sum(terms)\n    \n    if denominator == 0.0:\n        # This case is unlikely for the given test points but is a theoretical possibility.\n        return np.nan\n\n    return numerator / denominator\n\ndef p_stable(x, nodes, funcs, weights):\n    \"\"\"\n    Evaluates the polynomial interpolant using the stabilized barycentric formula.\n    \"\"\"\n    # Find the index of the node closest to x.\n    k_star = np.argmin(np.abs(x - nodes))\n    \n    # If x is exactly the closest node, return the function value.\n    if x == nodes[k_star]:\n        return funcs[k_star]\n\n    # If x is not the closest node, it cannot be any other node either,\n    # because k_star would have been that other node's index.\n    # Therefore, no division by zero will occur in the full denominator sum.\n\n    f_k = funcs[k_star]\n\n    # Numerator of the stabilized formula (sum excludes k_star)\n    # Create a boolean mask to exclude the k_star-th element\n    mask = np.ones_like(nodes, dtype=bool)\n    mask[k_star] = False\n    \n    num_sum = np.sum(\n        (funcs[mask] - f_k) * weights[mask] / (x - nodes[mask])\n    )\n\n    # Denominator is the same as in the naive formula (full sum)\n    den_sum_full = np.sum(weights / (x - nodes))\n\n    if den_sum_full == 0.0:\n        return np.nan\n\n    return f_k + num_sum / den_sum_full\n\n\ndef solve():\n    \"\"\"\n    Main function to perform the analysis and produce the final output.\n    \"\"\"\n    # Problem parameters\n    n = 64\n    \n    # Generate Chebyshev nodes of the second kind (from k=0 to n)\n    k_vals = np.arange(n + 1)\n    nodes = np.cos(np.pi * k_vals / n)\n    \n    # Generate function values\n    funcs = np.exp(nodes)\n    \n    # Generate barycentric weights\n    weights = (-1.0)**k_vals\n    weights[0] *= 0.5\n    weights[n] *= 0.5\n\n    # Define the test suite\n    k_star_idx = int(np.floor(n / 3))  # As per problem, floor(64/3) = 21\n    x_k_star = nodes[k_star_idx]\n    f_k_star = funcs[k_star_idx]\n    \n    x1 = 0.2\n    \n    delta1 = 1e-16 * np.max([1.0, np.abs(x_k_star)])\n    x2 = x_k_star + delta1\n    \n    delta2 = 1e-8 * np.max([1.0, np.abs(x_k_star)])\n    x3 = x_k_star + delta2\n    \n    x4 = x_k_star\n\n    x0 = nodes[0] # This is 1.0\n    delta3 = 1e-16\n    x5 = x0 + delta3\n\n    test_points = [x1, x2, x3, x4, x5]\n    \n    results = []\n    \n    # Calculate absolute differences for each test point\n    for x_test in test_points:\n        y_naive = p_naive(x_test, nodes, funcs, weights)\n        y_stable = p_stable(x_test, nodes, funcs, weights)\n        abs_diff = np.abs(y_naive - y_stable)\n        results.append(abs_diff)\n        \n    # Perform exact data recovery checks at x4 = x_k_star\n    b1 = (p_naive(x4, nodes, funcs, weights) == f_k_star)\n    b2 = (p_stable(x4, nodes, funcs, weights) == f_k_star)\n    \n    results.extend([b1, b2])\n\n    # Format the output string\n    # Convert bools to lowercase 'true'/'false' as str(True) is 'True'\n    formatted_results = [f\"{v:.17e}\" if isinstance(v, float) else str(v).lower() for v in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3209410"}]}