{"hands_on_practices": [{"introduction": "To build a solid foundation in numerical stability, we begin with a direct application of its core definition. This exercise focuses on the stability function, $R(z)$, which is the amplification factor that determines whether numerical errors grow or decay. By analyzing this function for a given method, we can map out its region of absolute stability, the set of all $z=h\\lambda$ for which the numerical solution remains bounded [@problem_id:2219443]. This practice will sharpen your algebraic skills in solving the fundamental inequality $|R(z)| \\le 1$ for a simple case, an essential first step before tackling more complex systems.", "problem": "In the numerical analysis of Ordinary Differential Equations (ODEs), the stability of a method is crucial for obtaining reliable solutions. This is often studied using the test equation $y'(t) = \\lambda y(t)$, where $\\lambda$ is a complex constant. When a one-step numerical method is applied to this equation with a step size $h$, the numerical solution follows a recurrence relation of the form $y_{n+1} = R(z) y_n$, where $z = h\\lambda$ and $R(z)$ is the stability function of the method.\n\nThe method is defined to be absolutely stable for a given $z$ if $|R(z)| \\le 1$. This condition ensures that the numerical solution does not grow in magnitude for decaying exact solutions.\n\nConsider a numerical method whose stability function is given by the polynomial:\n$$ R(z) = 1 + z + \\frac{z^2}{2} $$\n\nFor a real, negative $\\lambda$, the value of $z$ will be real and negative. The region of absolute stability on the negative real axis is an interval of the form $[a, 0]$. Determine the numerical value of the left endpoint, $a$.", "solution": "The absolute stability condition for a one-step method applied to the test equation $y'(t)=\\lambda y(t)$ with step size $h$ and stability function $R(z)$, where $z=h\\lambda$, is $|R(z)|\\leq 1$. Here $R(z)=1+z+\\frac{z^{2}}{2}$. For real, negative $\\lambda$, the parameter $z$ is real and negative.\n\nSince $z\\in \\mathbb{R}$, $R(z)\\in \\mathbb{R}$, and the stability condition becomes the pair of inequalities\n$$\n-1 \\leq R(z) \\leq 1.\n$$\nFirst, rewrite $R(z)$ by completing the square:\n$$\nR(z)=1+z+\\frac{z^{2}}{2}=\\frac{1}{2}\\left(z^{2}+2z+2\\right)=\\frac{1}{2}\\left((z+1)^{2}+1\\right).\n$$\nTherefore,\n$$\nR(z)\\geq \\frac{1}{2},\n$$\nwith equality at $z=-1$. Hence the lower bound $-1\\leq R(z)$ is automatically satisfied for all real $z$. The stability condition on the negative real axis thus reduces to\n$$\nR(z)\\leq 1.\n$$\nSolve the inequality:\n$$\n1+z+\\frac{z^{2}}{2}\\leq 1 \\quad \\Longleftrightarrow \\quad z+\\frac{z^{2}}{2}\\leq 0 \\quad \\Longleftrightarrow \\quad z\\left(1+\\frac{z}{2}\\right)\\leq 0.\n$$\nThe roots are $z=0$ and $z=-2$, and since the quadratic coefficient is positive, the solution set is the interval\n$$\nz\\in[-2,0].\n$$\nTherefore, the region of absolute stability on the negative real axis is $[a,0]$ with left endpoint $a=-2$.", "answer": "$$\\boxed{-2}$$", "id": "2219443"}, {"introduction": "The abstract concept of a stability region becomes critically important when solving differential equations that model physical phenomena. This problem brings the theory to life by examining a damped harmonic oscillator, a system common in engineering and physics. You will see how the stability of the entire system depends on its most rapidly changing component, a property known as stiffness [@problem_id:2219436]. This exercise guides you through converting a second-order ODE to a first-order system and using its eigenvalues to determine the maximum permissible step size for the Forward Euler method, illustrating a crucial trade-off between computational efficiency and numerical stability.", "problem": "In the study of control systems and mechanical vibrations, it is common to analyze the behavior of damped oscillators. Consider a simplified model of such a system where the dynamics are governed by the following second-order linear homogeneous Ordinary Differential Equation (ODE):\n$$ y''(t) + 101 y'(t) + 100 y(t) = 0 $$\nwhere $y(t)$ represents the displacement of the system at time $t$.\n\nTo approximate the solution numerically, one common technique is to first rewrite this second-order ODE as an equivalent system of two first-order ODEs. Subsequently, a numerical integration scheme is applied. For this problem, we will use the Forward Euler method with a constant step size $h > 0$.\n\nA crucial aspect of numerical integration is stability. If the step size $h$ is too large, the numerical solution can grow without bound, even if the true solution decays to zero. For the Forward Euler method, there is a maximum step size beyond which the integration becomes unstable.\n\nDetermine the maximum possible step size, $h_{max}$, for which the Forward Euler method, when applied to the corresponding first-order system, is absolutely stable.", "solution": "Start by rewriting the second-order ODE as a first-order system. Define the state vector $x(t) = \\begin{pmatrix} y(t) \\\\ v(t) \\end{pmatrix}$ with $v(t) = y'(t)$. Then\n$$\n\\begin{cases}\ny'(t) = v(t), \\\\\nv'(t) = -101\\,v(t) - 100\\,y(t),\n\\end{cases}\n$$\nwhich can be written in matrix form as $x'(t) = A x(t)$ with\n$$\nA = \\begin{pmatrix} 0  1 \\\\ -100  -101 \\end{pmatrix}.\n$$\n\nThe eigenvalues of $A$ are the roots of the characteristic polynomial of the original ODE,\n$$\n\\lambda^{2} + 101 \\lambda + 100 = 0.\n$$\nCompute the discriminant:\n$$\n\\Delta = 101^{2} - 4 \\cdot 100 = 10201 - 400 = 9801,\n$$\nso $\\sqrt{\\Delta} = 99$. Therefore,\n$$\n\\lambda_{1,2} = \\frac{-101 \\pm 99}{2},\n$$\nwhich gives\n$$\n\\lambda_{1} = -1, \\quad \\lambda_{2} = -100.\n$$\n\nApply the Forward Euler method to $x'(t) = A x(t)$:\n$$\nx_{n+1} = x_{n} + h A x_{n} = (I + h A) x_{n}.\n$$\nThe method is absolutely stable if the spectral radius of the amplification matrix is less than or equal to $1$, which for a diagonalizable $A$ is ensured by requiring that for each eigenvalue $\\lambda$ of $A$,\n$$\n|1 + h \\lambda| \\le 1.\n$$\nFor $\\lambda_{1} = -1$, the condition is\n$$\n|1 - h| \\le 1 \\;\\;\\Longleftrightarrow\\;\\; -1 \\le 1 - h \\le 1 \\;\\;\\Longleftrightarrow\\;\\; 0 \\le h \\le 2.\n$$\nFor $\\lambda_{2} = -100$, the condition is\n$$\n|1 - 100 h| \\le 1 \\;\\;\\Longleftrightarrow\\;\\; -1 \\le 1 - 100 h \\le 1 \\;\\;\\Longleftrightarrow\\;\\; 0 \\le h \\le \\frac{2}{100} = \\frac{1}{50}.\n$$\nTo ensure absolute stability for the system, both conditions must hold simultaneously. Since $h>0$, the allowable step sizes are\n$$\n0  h \\le \\min\\left\\{2, \\frac{1}{50}\\right\\} = \\frac{1}{50}.\n$$\nThus, the maximum possible step size for absolute stability is\n$$\nh_{\\max} = \\frac{1}{50}.\n$$", "answer": "$$\\boxed{\\frac{1}{50}}$$", "id": "2219436"}, {"introduction": "Moving from pen-and-paper analysis to computational practice is a key step in mastering numerical methods. This advanced exercise challenges you to build a practical tool for stability analysis from the ground up, starting with a method's fundamental definition given by its Butcher tableau [@problem_id:3197733]. You will implement a procedure to automatically derive the stability function for the classical fourth-order Runge-Kutta method and then use numerical root-finding to determine the maximum stable step size for various systems, including those with oscillatory and damped behavior. This comprehensive problem synthesizes theory and coding, equipping you with the skills to assess the stability of numerical integrations in your own scientific computing projects.", "problem": "You are given the initial value problem $y'(t)=A_{\\text{sys}}\\,y(t)$, where $A_{\\text{sys}}$ is a complex-valued matrix and $y(t)$ is a complex-valued vector function. Consider integrating this system using a fixed-step explicit Runge–Kutta method described by a Butcher tableau. The stability of the time integration is governed by the behavior of the method on the linear test equation $y'(t)=\\lambda\\,y(t)$, where $\\lambda\\in\\mathbb{C}$. For an explicit Runge–Kutta method, its stability region is the set of $z\\in\\mathbb{C}$ such that the corresponding scalar amplification factor applied to $y'(t)=\\lambda\\,y(t)$ satisfies $\\lvert R(z)\\rvert\\leq 1$, where $z=\\Delta t\\,\\lambda$ and $\\Delta t$ is the time step size. A method is called A-stable if its stability region includes the entire left half-plane $\\{z\\in\\mathbb{C}:\\Re(z)\\leq 0\\}$, and called L-stable if it is A-stable and additionally $R(z)\\to 0$ as $z\\to -\\infty$ along the real axis. Explicit Runge–Kutta methods are not A-stable, so the maximal stable $\\Delta t$ is generally finite, even when $\\Re(\\lambda)0$.\n\nStarting from the fundamental base of the linear test equation and the definition of the Runge–Kutta method through its Butcher tableau, derive the stability function $R(z)$ for the given method, justify the stability criterion $\\lvert R(\\Delta t\\,\\lambda)\\rvert\\leq 1$ for each eigenvalue $\\lambda$ of $A_{\\text{sys}}$, and construct a numerical search procedure that, for a fixed Runge–Kutta method and a given system matrix $A_{\\text{sys}}$, finds the maximal $\\Delta t0$ such that all scaled eigenvalues $z_i=\\Delta t\\,\\lambda_i$ lie within the stability region, i.e., $\\lvert R(z_i)\\rvert\\leq 1$ for all eigenvalues $\\lambda_i$ of $A_{\\text{sys}}$. Your program must not assume a pre-tabulated stability polynomial; it must obtain $R(z)$ directly from the Butcher tableau and the definition of the method applied to $y'(t)=\\lambda\\,y(t)$.\n\nUse the classical $4$-stage explicit Runge–Kutta method (often referred to as \"RK$4$\") with the following Butcher tableau:\n$$\nA_{\\text{RK}}=\\begin{bmatrix}\n0  0  0  0\\\\\n\\frac{1}{2}  0  0  0\\\\\n0  \\frac{1}{2}  0  0\\\\\n0  0  1  0\n\\end{bmatrix},\\quad\nb=\\begin{bmatrix}\n\\frac{1}{6}\\\\\n\\frac{1}{3}\\\\\n\\frac{1}{3}\\\\\n\\frac{1}{6}\n\\end{bmatrix},\\quad\nc=\\begin{bmatrix}\n0\\\\\n\\frac{1}{2}\\\\\n\\frac{1}{2}\\\\\n1\n\\end{bmatrix}.\n$$\n\nDesign your program to apply this single RK method to each of the following test matrices $A_{\\text{sys}}^{(k)}$ and for each matrix compute a single float value: the maximal $\\Delta t$ such that the stability condition $\\lvert R(\\Delta t\\,\\lambda_i)\\rvert\\leq 1$ holds simultaneously for all eigenvalues $\\lambda_i$ of $A_{\\text{sys}}^{(k)}$. The search must be performed along the rays $z=\\Delta t\\,\\lambda_i$ in $\\mathbb{C}$ for $\\Delta t\\geq 0$, and should return the largest $\\Delta t$ satisfying the constraint. If the constraint only holds for $\\Delta t=0$, your program must return $0.0$ for that case. All numerical outputs must be rounded to $8$ decimal places.\n\nTest suite:\n- Case $1$ (general negative real eigenvalues):\n$$\nA_{\\text{sys}}^{(1)}=\\begin{bmatrix}\n-2  0\\\\\n0  -5\n\\end{bmatrix}.\n$$\n- Case $2$ (positive real eigenvalues):\n$$\nA_{\\text{sys}}^{(2)}=\\begin{bmatrix}\n1  0\\\\\n0  \\frac{1}{2}\n\\end{bmatrix}.\n$$\n- Case $3$ (complex-conjugate pair with negative real part):\n$$\nA_{\\text{sys}}^{(3)}=\\begin{bmatrix}\n-1  -4\\\\\n1  -1\n\\end{bmatrix}.\n$$\n- Case $4$ (highly oscillatory with small damping):\n$$\nA_{\\text{sys}}^{(4)}=\\begin{bmatrix}\n-0.1  10\\\\\n-10  -0.1\n\\end{bmatrix}.\n$$\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, with each float rounded to $8$ decimal places and no spaces. For example, the output must look like $[d_1,d_2,d_3,d_4]$ where each $d_k$ is a decimal string with exactly $8$ digits after the decimal point.", "solution": "The user-provided problem has been validated and is determined to be a well-posed, scientifically grounded problem in the field of numerical analysis for ordinary differential equations. All necessary information is provided, and the problem is free of contradictions or ambiguities. I will now proceed with a complete solution.\n\n### 1. The Explicit Runge-Kutta Method and the Stability Function\n\nAn $s$-stage explicit Runge–Kutta (ERK) method for solving the initial value problem $y'(t) = f(t, y(t))$ is defined by the equations:\n$$\ny_{n+1} = y_n + \\Delta t \\sum_{i=1}^s b_i k_i \\\\\nk_i = f\\left(t_n + c_i \\Delta t, y_n + \\Delta t \\sum_{j=1}^{i-1} a_{ij} k_j\\right)\n$$\nThe coefficients $a_{ij}$, $b_i$, and $c_i$ are given by a Butcher tableau, which for an explicit method has a strictly lower triangular matrix $A = (a_{ij})$.\n\nTo analyze the stability of the method, we apply it to the Dahlquist test equation, $y'(t) = \\lambda y(t)$, where $\\lambda \\in \\mathbb{C}$. In this case, $f(t, y) = \\lambda y$. The stage values $k_i$ become:\n$$\nk_i = \\lambda \\left(y_n + \\Delta t \\sum_{j=1}^{i-1} a_{ij} k_j\\right)\n$$\nLet us define the scaled time step $z = \\Delta t \\lambda$. We can observe that each stage vector $k_i$ must be proportional to $\\lambda y_n$. Let $k_i = K_i(z) \\lambda y_n$ for some function $K_i(z)$. Substituting this into the stage equation:\n$$\nK_i(z) \\lambda y_n = \\lambda \\left(y_n + \\Delta t \\sum_{j=1}^{i-1} a_{ij} (K_j(z) \\lambda y_n)\\right)\n$$\nDividing by $\\lambda y_n$ (assuming $\\lambda, y_n \\neq 0$), we obtain a recurrence relation for the stage polynomials $K_i(z)$:\n$$\nK_i(z) = 1 + z \\sum_{j=1}^{i-1} a_{ij} K_j(z)\n$$\nwith $K_1(z) = 1$ since the sum is empty for $i=1$. Because $A$ is strictly lower triangular, we can compute each $K_i(z)$ sequentially. $K_i(z)$ is a polynomial in $z$ of degree $i-1$.\n\nThe numerical solution is updated as:\n$$\ny_{n+1} = y_n + \\Delta t \\sum_{i=1}^s b_i (K_i(z) \\lambda y_n) = y_n \\left(1 + z \\sum_{i=1}^s b_i K_i(z)\\right)\n$$\nThe term in the parenthesis is the amplification factor, which maps $y_n$ to $y_{n+1}$. This is the stability function $R(z)$:\n$$\nR(z) = 1 + z \\sum_{i=1}^s b_i K_i(z)\n$$\nFor an $s$-stage ERK method, $R(z)$ is a polynomial in $z$ of degree at most $s$.\n\n### 2. Stability Function for the Classical RK4 Method\n\nThe problem provides the Butcher tableau for the classical $4$-stage Runge-Kutta method ($s=4$):\n$$\nA_{\\text{RK}}=\\begin{bmatrix}\n0  0  0  0\\\\\n\\frac{1}{2}  0  0  0\\\\\n0  \\frac{1}{2}  0  0\\\\\n0  0  1  0\n\\end{bmatrix},\\quad\nb=\\begin{bmatrix}\n\\frac{1}{6}\\\\\n\\frac{1}{3}\\\\\n\\frac{1}{3}\\\\\n\\frac{1}{6}\n\\end{bmatrix}\n$$\nWe derive the stage polynomials $K_i(z)$:\n\\begin{align*}\nK_1(z) = 1 \\\\\nK_2(z) = 1 + z a_{21} K_1(z) = 1 + z \\left(\\frac{1}{2}\\right)(1) = 1 + \\frac{z}{2} \\\\\nK_3(z) = 1 + z (a_{31} K_1(z) + a_{32} K_2(z)) = 1 + z \\left(0 + \\frac{1}{2}\\left(1 + \\frac{z}{2}\\right)\\right) = 1 + \\frac{z}{2} + \\frac{z^2}{4} \\\\\nK_4(z) = 1 + z (a_{41} K_1 + a_{42} K_2 + a_{43} K_3) = 1 + z \\left(0 + 0 + 1\\left(1 + \\frac{z}{2} + \\frac{z^2}{4}\\right)\\right) = 1 + z + \\frac{z^2}{2} + \\frac{z^3}{4}\n\\end{align*}\nNow, we construct the stability function $R(z)$:\n$$\nR(z) = 1 + z \\left( b_1 K_1(z) + b_2 K_2(z) + b_3 K_3(z) + b_4 K_4(z) \\right)\n$$\nSubstituting the values of $b_i$ and polynomials $K_i(z)$:\n$$\nR(z) = 1 + z \\left[ \\frac{1}{6}(1) + \\frac{1}{3}\\left(1+\\frac{z}{2}\\right) + \\frac{1}{3}\\left(1+\\frac{z}{2}+\\frac{z^2}{4}\\right) + \\frac{1}{6}\\left(1+z+\\frac{z^2}{2}+\\frac{z^3}{4}\\right) \\right]\n$$\nCombining terms by powers of $z$ inside the brackets:\n\\begin{itemize}\n    \\item Constant term: $\\frac{1}{6} + \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{6} = 1$\n    \\item Term in $z$: $\\frac{1}{3}\\left(\\frac{1}{2}\\right) + \\frac{1}{3}\\left(\\frac{1}{2}\\right) + \\frac{1}{6}(1) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{2}$\n    \\item Term in $z^2$: $\\frac{1}{3}\\left(\\frac{1}{4}\\right) + \\frac{1}{6}\\left(\\frac{1}{2}\\right) = \\frac{1}{12} + \\frac{1}{12} = \\frac{1}{6}$\n    \\item Term in $z^3$: $\\frac{1}{6}\\left(\\frac{1}{4}\\right) = \\frac{1}{24}$\n\\end{itemize}\nMultiplying the expression in brackets by $z$ and adding $1$, we get:\n$$\nR(z) = 1 + z\\left(1 + \\frac{1}{2}z + \\frac{1}{6}z^2 + \\frac{1}{24}z^3\\right) = 1 + z + \\frac{z^2}{2} + \\frac{z^3}{6} + \\frac{z^4}{24} = \\sum_{k=0}^{4} \\frac{z^k}{k!}\n$$\nThis is the Taylor series expansion of $e^z$ truncated to the 4th order.\n\n### 3. Stability Criterion for Linear Systems\n\nFor the system of ODEs $y'(t) = A_{\\text{sys}} y(t)$, assuming $A_{\\text{sys}}$ is diagonalizable, there exists an invertible matrix $P$ such that $A_{\\text{sys}} = P \\Lambda P^{-1}$, where $\\Lambda$ is the diagonal matrix of eigenvalues $\\lambda_i$ of $A_{\\text{sys}}$.\nBy the change of variables $u(t) = P^{-1} y(t)$, the system decouples into a set of independent scalar equations: $u_i'(t) = \\lambda_i u_i(t)$.\nApplying a Runge-Kutta method to the original system $y' = A_{\\text{sys}} y$ is equivalent to applying the same method to each of these scalar equations. The update rule for the transformed variables is:\n$u_{i, n+1} = R(\\Delta t \\lambda_i) u_{i, n}$.\nFor the numerical solution $y_n$ to remain bounded as $n \\to \\infty$, all components $u_{i,n}$ must remain bounded. This requires that the amplification factor for each component has a magnitude no greater than one:\n$$\n|R(\\Delta t \\lambda_i)| \\leq 1 \\quad \\text{for all eigenvalues } \\lambda_i \\text{ of } A_{\\text{sys}}\n$$\nThis is the condition for numerical stability.\n\n### 4. Algorithm for Maximal Stable Time Step $\\Delta t_{\\max}$\n\nWe seek the largest $\\Delta t \\geq 0$ that satisfies the stability condition for all eigenvalues simultaneously. This is $\\Delta t_{\\max} = \\sup\\{\\Delta t \\geq 0 \\mid |R(\\Delta t \\lambda_i)| \\leq 1 \\text{ for all } i\\}$.\n\nThe algorithm is as follows:\n1.  Compute the set of eigenvalues $\\{\\lambda_i\\}$ of the matrix $A_{\\text{sys}}$.\n2.  For each eigenvalue $\\lambda_i$:\n    a. If $\\Re(\\lambda_i)  0$, the physical system is unstable. The numerical method will also be unstable for any $\\Delta t  0$, because for small $z=\\Delta t \\lambda_i$, $|R(z)| \\approx |1+z| = \\sqrt{(1+\\Delta t \\Re(\\lambda_i))^2 + (\\Delta t \\Im(\\lambda_i))^2}  1$. Thus, $\\Delta t_{\\max} = 0$.\n    b. If $\\lambda_i=0$, $R(0)=1$, so this eigenvalue imposes no restriction on $\\Delta t$.\n    c. If $\\Re(\\lambda_i) \\leq 0$ and $\\lambda_i \\neq 0$, we must find the smallest positive $\\Delta t_i^*$ such that $|R(\\Delta t_i^* \\lambda_i)| = 1$. This value represents the boundary of the stability region along the ray defined by $\\lambda_i$.\n3.  The overall maximal stable time step is the minimum of these individual limits: $\\Delta t_{\\max} = \\min_{i} \\{\\Delta t_i^*\\}$.\n\nTo find $\\Delta t_i^*$, we solve the equation $|R(\\Delta t \\lambda_i)| - 1 = 0$ for the smallest positive root $\\Delta t$. This is a nonlinear equation that we can solve numerically. A robust approach is to first bracket the root and then use a root-finding algorithm like Brent's method.\n-   **Bracketing**: For a given $\\lambda_i$ with $\\Re(\\lambda_i) \\le 0$, the function $h(\\Delta t) = |R(\\Delta t \\lambda_i)| - 1$ is non-positive for small $\\Delta t0$. Since the stability region of any explicit RK method is bounded, we can find an upper bound $b$ where $h(b)  0$ by starting with a guess and increasing it (e.g., by doubling) until the condition is met. This provides an interval $[a, b]$ containing the root.\n-   **Root-finding**: With the root bracketed in $[a, b]$, `scipy.optimize.brentq` can efficiently find the precise value of $\\Delta t_i^*$.\n\nThe implementation will compute $R(z)$ from the Butcher tableau as derived, then execute this numerical search for each test matrix.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It computes the maximum stable time step for the classical RK4 method\n    for several linear systems.\n    \"\"\"\n    # Butcher tableau for the classical 4-stage Runge-Kutta method (RK4)\n    A_rk4 = np.array([\n        [0.0, 0.0, 0.0, 0.0],\n        [0.5, 0.0, 0.0, 0.0],\n        [0.0, 0.5, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0]\n    ], dtype=float)\n    b_rk4 = np.array([1/6, 1/3, 1/3, 1/6], dtype=float)\n\n    # Test suite of system matrices A_sys\n    test_cases = [\n        # Case 1: general negative real eigenvalues\n        np.array([[-2.0, 0.0], [0.0, -5.0]], dtype=float),\n        # Case 2: positive real eigenvalues\n        np.array([[1.0, 0.0], [0.0, 0.5]], dtype=float),\n        # Case 3: complex-conjugate pair with negative real part\n        np.array([[-1.0, -4.0], [1.0, -1.0]], dtype=float),\n        # Case 4: highly oscillatory with small damping\n        np.array([[-0.1, 10.0], [-10.0, -0.1]], dtype=float)\n    ]\n\n    results = []\n    for A_sys in test_cases:\n        max_dt = find_max_dt(A_sys, A_rk4, b_rk4)\n        results.append(f\"{max_dt:.8f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef compute_R_from_tableau(z, A, b):\n    \"\"\"\n    Computes the value of the stability function R(z) for a given z\n    and a Runge-Kutta method specified by its Butcher tableau (A, b).\n    \"\"\"\n    s = len(b)\n    K = np.zeros(s, dtype=np.complex128)\n    for i in range(s):\n        # K_i(z) = 1 + z * sum(a[i,j] * K_j(z) for j  i)\n        stage_sum = np.dot(A[i, :i], K[:i])\n        K[i] = 1.0 + z * stage_sum\n    \n    # R(z) = 1 + z * sum(b_i * K_i(z))\n    return 1.0 + z * np.dot(b, K)\n\ndef find_max_dt(A_sys, butcher_A, butcher_b):\n    \"\"\"\n    Finds the maximum stable time step dt for a system y'(t) = A_sys*y(t)\n    using the stability function derived from the provided Butcher tableau.\n    \"\"\"\n    try:\n        eigenvalues = np.linalg.eigvals(A_sys)\n    except np.linalg.LinAlgError:\n        return 0.0\n\n    # If any eigenvalue has a positive real part, the system is unstable,\n    # and the time integration will be unstable for any dt > 0.\n    if any(lam.real > 1e-9 for lam in eigenvalues):\n        return 0.0\n        \n    min_dt_root = float('inf')\n    \n    for lam in eigenvalues:\n        if abs(lam)  1e-9:  # An eigenvalue of 0 imposes no stability constraint.\n            continue\n\n        def h(dt):\n            \"\"\"Target function for root finding: |R(dt*lam)| - 1.\"\"\"\n            z = dt * lam\n            R_val = compute_R_from_tableau(z, butcher_A, butcher_b)\n            return abs(R_val) - 1.0\n\n        # Search for the smallest positive root of h(dt) = 0.\n        # This determines the stability limit for this eigenvalue.\n        \n        # Step 1: Bracket the root. Find an interval [a, b] such that\n        # h(a) = 0 and h(b) > 0.\n        # For stable/neutral eigenvalues, h(dt) = 0 for small dt > 0.\n        a = 1e-9 # Small positive number to start the search interval.\n        b = 1e-3 # Initial guess for the upper bound.\n        \n        # Exponentially increase b until h(b) > 0.\n        while h(b) = 0:\n            b *= 2.0\n            if b > 1e6: # Safety break to avoid infinite loops\n                b = float('inf')\n                break\n        \n        if b == float('inf'):\n            # This eigenvalue does not seem to impose a stability constraint\n            # within a reasonable range. This shouldn't happen for explicit methods.\n            continue\n            \n        a = b / 2.0\n        if a == 0: a = 1e-9\n\n        # Step 2: Use Brent's method to find the root within the bracketed interval.\n        try:\n            root = brentq(h, a, b)\n            min_dt_root = min(min_dt_root, root)\n        except ValueError:\n            # Should not happen with the bracketing logic above.\n            # If it does, it implies immediate instability for this eigenvalue.\n            min_dt_root = 0.0\n            break\n\n    if min_dt_root == float('inf'):\n        # This case would occur if all eigenvalues were zero.\n        # Any dt would be stable, so there's no finite maximum.\n        # The problem cases avoid this scenario. Returning 0.0 as a safe default.\n        return 0.0\n        \n    return min_dt_root\n\nsolve()\n```", "id": "3197733"}]}