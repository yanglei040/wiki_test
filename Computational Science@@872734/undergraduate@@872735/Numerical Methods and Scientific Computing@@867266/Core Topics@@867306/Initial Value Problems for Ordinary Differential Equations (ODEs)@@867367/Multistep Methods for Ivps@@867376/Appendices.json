{"hands_on_practices": [{"introduction": "Before building and using multistep methods, it's crucial to understand the fundamental theoretical limits that govern their design. These limits, known as the Dahlquist stability barriers, define the trade-offs between a method's order, its number of steps, and its stability properties. This practice guides you through a hands-on investigation of these barriers, allowing you to discover for yourself why certain \"desirable\" methods are impossible to construct [@problem_id:2446838]. By attempting to build a method that violates these rules and observing its failure, you will gain a much deeper appreciation for the constraints that shape the landscape of stable and convergent numerical integrators.", "problem": "Consider linear multistep methods for solving initial value problems of the form $y'(t)=f(t,y(t))$ with step size $h>0$. A $k$-step linear multistep method is defined by coefficients $\\{\\alpha_j\\}_{j=0}^k$ and $\\{\\beta_j\\}_{j=0}^k$ through\n$$\n\\sum_{j=0}^k \\alpha_j \\, y_{n+j} \\;=\\; h \\sum_{j=0}^k \\beta_j \\, f_{n+j},\n$$\nwhere $y_{n+j} \\approx y(t_n + j h)$ and $f_{n+j} = f(t_n + j h, y_{n+j})$. The method is called explicit if $\\beta_k = 0$. The method is called consistent if $\\sum_{j=0}^k \\alpha_j = 0$ and it has order $p \\in \\mathbb{N}$ if, when applied to any function $y(t)$ that is $(p+1)$ times continuously differentiable, its local truncation error vanishes through order $h^{p+1}$. From first principles via Taylor expansions, this order condition is equivalent to requiring that for all integers $m$ with $0 \\le m \\le p$,\n$$\n\\sum_{j=0}^k \\alpha_j \\, j^m \\;=\\; m \\sum_{j=0}^k \\beta_j \\, j^{m-1},\n$$\nwith the convention that the right-hand side is zero when $m=0$.\n\nZero-stability is defined by the root condition on the characteristic polynomial $\\rho(\\xi)=\\sum_{j=0}^k \\alpha_j \\xi^j$: all roots $\\xi_i$ of $\\rho(\\xi)$ satisfy $|\\xi_i| \\le 1$ and any root on the unit circle $|\\xi_i|=1$ is simple. Absolute stability with respect to the linear test equation $y'=\\lambda y$ with $z=h\\lambda \\in \\mathbb{C}$ is characterized as follows: for a given $z$, substitute $y_n=\\xi^n$ into the method to obtain the polynomial equation in $\\xi$\n$$\n\\Pi(\\xi; z) \\;=\\; \\rho(\\xi) \\;-\\; z \\, \\sigma(\\xi) \\;=\\; 0,\n$$\nwhere $\\sigma(\\xi)=\\sum_{j=0}^k \\beta_j \\xi^j$. The method is stable at $z$ if all roots $\\xi_i(z)$ of $\\Pi(\\xi; z)$ satisfy $|\\xi_i(z)| \\le 1$ and any root with $|\\xi_i(z)|=1$ is simple. A-stability means stability at every $z$ with $\\operatorname{Re}(z) \\le 0$.\n\nYour task is to numerically investigate the feasibility of an explicit, linear, $2$-step method of order $p=3$ that is zero-stable, and to contrast with known stable cases. Normalize coefficients by fixing $\\alpha_2=1$ and impose explicitness by $\\beta_2=0$ where applicable. Use only the definitions above, without invoking any external theorems.\n\nTest Suite:\n- Case $1$ (attempted construction): Attempt to construct an explicit $k=2$ method of order $p=3$ by solving the order equations for the unknowns $(\\alpha_0,\\alpha_1,\\beta_0,\\beta_1)$ under $\\alpha_2=1$ and $\\beta_2=0$. If a solution exists, test whether the resulting method is zero-stable by the root condition. Output a boolean indicating zero-stability.\n- Case $2$ (reference, happy path): Consider the explicit $2$-step method with coefficients $(\\alpha_0,\\alpha_1,\\alpha_2)=(0,-1,1)$ and $(\\beta_0,\\beta_1,\\beta_2)=(-\\tfrac{1}{2},\\tfrac{3}{2},0)$. Test whether it is zero-stable. Output a boolean.\n- Case $3$ (A-stability, boundary of the second barrier): Consider the implicit trapezoidal rule with $(\\alpha_0,\\alpha_1)=(-1,1)$ and $(\\beta_0,\\beta_1)=(\\tfrac{1}{2},\\tfrac{1}{2})$. Numerically test A-stability by sampling the set\n$$\n\\mathcal{G}=\\{\\, z = x + i y \\,:\\, x \\in \\{-0.5,-1,-2,-4,-8\\},\\; y \\in \\{-4,-2,0,2,4\\} \\,\\}.\n$$\nDeclare the method A-stable on $\\mathcal{G}$ if it is stable at every $z \\in \\mathcal{G}$. Output a boolean.\n- Case $4$ (non-A-stable explicit method): Using the coefficients from Case $2$, test A-stability on the same grid $\\mathcal{G}$ as in Case $3$. Output a boolean.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list of booleans enclosed in square brackets, in the order of Cases $1$ through $4$, for example, `[false,true,true,false]` but using Python boolean capitalization, i.e., `[False,True,True,False]`. No additional text should be printed.\n\nThere are no physical units involved. All angles, where applicable, are to be interpreted in radians. All outputs are booleans as specified above. The program must be complete and runnable as given, without requiring any user input.", "solution": "The problem statement poses a series of well-defined questions regarding the properties of linear multistep methods for solving ordinary differential equations. The definitions provided for consistency, order, zero-stability, and absolute stability are standard and correct. All required parameters and test conditions are explicitly stated. The problem is scientifically grounded, objective, and self-contained. It is therefore deemed valid. A solution is provided below.\n\nThe core of the problem involves analyzing linear $k$-step methods of the form\n$$\n\\sum_{j=0}^k \\alpha_j \\, y_{n+j} \\;=\\; h \\sum_{j=0}^k \\beta_j \\, f_{n+j}\n$$\nWe will address each of the four cases systematically based on the provided definitions.\n\n**Case 1: Attempted construction of a zero-stable, explicit, 2-step method of order 3**\n\nWe are tasked with finding coefficients $(\\alpha_0, \\alpha_1, \\beta_0, \\beta_1)$ for a method with $k=2$ and order $p=3$. The constraints are that the method is explicit, so $\\beta_k = \\beta_2 = 0$, and the coefficients are normalized by $\\alpha_k = \\alpha_2 = 1$. The unknown vector is $(\\alpha_0, \\alpha_1, \\beta_0, \\beta_1)$. We use the order conditions for $m \\in \\{0, 1, 2, 3\\}$.\n\nThe order conditions are given by $\\sum_{j=0}^k \\alpha_j j^m = m \\sum_{j=0}^k \\beta_j j^{m-1}$. With $k=2$, $\\alpha_2=1$, and $\\beta_2=0$, these become:\n\\begin{itemize}\n    \\item $m=0$: $\\alpha_0 (0)^0 + \\alpha_1 (1)^0 + \\alpha_2 (2)^0 = 0 \\implies \\alpha_0 + \\alpha_1 + 1 = 0$.\n    \\item $m=1$: $\\alpha_0 (0)^1 + \\alpha_1 (1)^1 + \\alpha_2 (2)^1 = 1 (\\beta_0 (0)^0 + \\beta_1 (1)^0 + \\beta_2 (2)^0) \\implies \\alpha_1 + 2 = \\beta_0 + \\beta_1$.\n    \\item $m=2$: $\\alpha_0 (0)^2 + \\alpha_1 (1)^2 + \\alpha_2 (2)^2 = 2 (\\beta_0 (0)^1 + \\beta_1 (1)^1 + \\beta_2 (2)^1) \\implies \\alpha_1 + 4 = 2\\beta_1$.\n    \\item $m=3$: $\\alpha_0 (0)^3 + \\alpha_1 (1)^3 + \\alpha_2 (2)^3 = 3 (\\beta_0 (0)^2 + \\beta_1 (1)^2 + \\beta_2 (2)^2) \\implies \\alpha_1 + 8 = 3\\beta_1$.\n\\end{itemize}\nThis yields a system of four linear equations for four unknowns. From the last two equations:\n$$\n\\begin{cases}\n\\alpha_1 - 2\\beta_1 = -4 \\\\\n\\alpha_1 - 3\\beta_1 = -8\n\\end{cases}\n$$\nSubtracting the first from the second gives $(-\\beta_1) = (-4)$, which implies $\\beta_1=4$. Substituting this into $\\alpha_1 - 2\\beta_1 = -4$ gives $\\alpha_1 - 2(4) = -4$, so $\\alpha_1 = 4$.\nFrom the first equation, $\\alpha_0 + \\alpha_1 + 1 = 0 \\implies \\alpha_0 + 4 + 1 = 0$, so $\\alpha_0 = -5$.\nFinally, from the second equation, $\\alpha_1 + 2 = \\beta_0 + \\beta_1 \\implies 4 + 2 = \\beta_0 + 4$, so $\\beta_0 = 2$.\nThe unique coefficients are $(\\alpha_0, \\alpha_1, \\alpha_2) = (-5, 4, 1)$ and $(\\beta_0, \\beta_1, \\beta_2) = (2, 4, 0)$.\n\nNow, we test for zero-stability. The characteristic polynomial is $\\rho(\\xi) = \\sum_{j=0}^2 \\alpha_j \\xi^j = \\alpha_0 + \\alpha_1 \\xi + \\alpha_2 \\xi^2$.\n$$\n\\rho(\\xi) = \\xi^2 + 4\\xi - 5\n$$\nThe roots are found by solving $\\xi^2 + 4\\xi - 5 = 0$, which factors as $(\\xi+5)(\\xi-1)=0$. The roots are $\\xi_1 = 1$ and $\\xi_2 = -5$.\nThe root condition for zero-stability requires all roots $\\xi_i$ to satisfy $|\\xi_i| \\le 1$. Here, $|\\xi_2| = |-5| = 5 > 1$. The condition is violated.\nTherefore, the method is not zero-stable. This finding is a known consequence of the first Dahlquist stability barrier, which dictates that a zero-stable explicit $k$-step method can have an order of at most $k$. Here, $p=3$ and $k=2$, violating this bound. The result is **False**.\n\n**Case 2: Zero-stability of a reference explicit 2-step method**\n\nThe method is defined by $(\\alpha_0,\\alpha_1,\\alpha_2)=(0,-1,1)$ and $(\\beta_0,\\beta_1,\\beta_2)=(-\\frac{1}{2},\\frac{3}{2},0)$.\nThe characteristic polynomial is $\\rho(\\xi) = \\alpha_0 + \\alpha_1 \\xi + \\alpha_2 \\xi^2$.\n$$\n\\rho(\\xi) = \\xi^2 - \\xi = \\xi(\\xi-1)\n$$\nThe roots are $\\xi_1 = 0$ and $\\xi_2 = 1$.\nWe check the root condition:\n\\begin{itemize}\n    \\item Magnitudes: $|\\xi_1|=0 \\le 1$ and $|\\xi_2|=1 \\le 1$. This part of the condition is satisfied.\n    \\item Simplicity: Any root on the unit circle ($|\\xi|=1$) must be simple. The only such root is $\\xi_2=1$. Since the polynomial $\\xi(\\xi-1)$ has distinct roots, the root at $\\xi=1$ is simple.\n\\end{itemize}\nBoth conditions are met. The method is zero-stable. The result is **True**.\n\n**Case 3: A-stability of the trapezoidal rule on a grid**\n\nThe method is the implicit trapezoidal rule ($k=1$) with $(\\alpha_0,\\alpha_1)=(-1,1)$ and $(\\beta_0,\\beta_1)=(\\frac{1}{2},\\frac{1}{2})$. We must test for absolute stability at each point $z$ in the grid $\\mathcal{G}=\\{ z = x + i y \\,:\\, x \\in \\{-0.5,-1,-2,-4,-8\\},\\; y \\in \\{-4,-2,0,2,4\\} \\}$.\nThe characteristic polynomials are $\\rho(\\xi) = \\xi-1$ and $\\sigma(\\xi) = \\frac{1}{2}(\\xi+1)$.\nThe stability polynomial is $\\Pi(\\xi; z) = \\rho(\\xi) - z \\sigma(\\xi) = 0$.\n$$\n(\\xi-1) - z \\frac{1}{2}(\\xi+1) = 0\n$$\nThis is a linear equation for the single root $\\xi(z)$:\n$$\n\\xi \\left(1 - \\frac{z}{2}\\right) - \\left(1 + \\frac{z}{2}\\right) = 0 \\implies \\xi(z) = \\frac{1 + z/2}{1 - z/2}\n$$\nThe method is stable at $z$ if $|\\xi(z)| \\le 1$. Let $z = x+iy$. We check the squared magnitude:\n$$\n|\\xi(z)|^2 = \\left| \\frac{1+(x+iy)/2}{1-(x+iy)/2} \\right|^2 = \\frac{|(1+x/2) + i(y/2)|^2}{|(1-x/2) - i(y/2)|^2} = \\frac{(1+x/2)^2 + (y/2)^2}{(1-x/2)^2 + (y/2)^2}\n$$\nThe condition $|\\xi(z)|^2 \\le 1$ is equivalent to $(1+x/2)^2 \\le (1-x/2)^2$, which simplifies to $1+x+x^2/4 \\le 1-x+x^2/4$, or $2x \\le 0$, which is $x = \\operatorname{Re}(z) \\le 0$.\nThe method is stable for all $z$ in the closed left half-plane. Every point $z \\in \\mathcal{G}$ has a real part $x \\in \\{-0.5, -1, -2, -4, -8\\}$, all of which are strictly negative. Therefore, $\\operatorname{Re}(z) < 0$ for all $z \\in \\mathcal{G}$, and the stability condition $|\\xi(z)| < 1$ is satisfied for all points in the grid.\nThe method is A-stable on $\\mathcal{G}$. The result is **True**.\n\n**Case 4: A-stability of the explicit method from Case 2 on a grid**\n\nThe method is the explicit 2-step method from Case 2, with $\\rho(\\xi) = \\xi^2 - \\xi$ and $\\sigma(\\xi) = \\frac{3}{2}\\xi - \\frac{1}{2}$. We test on the same grid $\\mathcal{G}$.\nThe stability polynomial is $\\Pi(\\xi; z) = \\rho(\\xi) - z \\sigma(\\xi) = 0$:\n$$\n(\\xi^2-\\xi) - z \\left(\\frac{3}{2}\\xi - \\frac{1}{2}\\right) = 0\n$$\n$$\n\\xi^2 - \\left(1 + \\frac{3z}{2}\\right)\\xi + \\frac{z}{2} = 0\n$$\nThis is a quadratic equation for the roots $\\xi_i(z)$. A-stability requires that for all $z \\in \\mathcal{G}$, all roots satisfy $|\\xi_i(z)| \\le 1$. We only need to find one point $z \\in \\mathcal{G}$ where this condition is violated. Let us test the point $z = -2$, which is in $\\mathcal{G}$ (with $x=-2, y=0$).\nThe equation becomes:\n$$\n\\xi^2 - \\left(1 + \\frac{3(-2)}{2}\\right)\\xi + \\frac{-2}{2} = 0\n$$\n$$\n\\xi^2 - (1 - 3)\\xi - 1 = 0 \\implies \\xi^2 + 2\\xi - 1 = 0\n$$\nThe roots are given by the quadratic formula:\n$$\n\\xi = \\frac{-2 \\pm \\sqrt{2^2 - 4(1)(-1)}}{2(1)} = \\frac{-2 \\pm \\sqrt{8}}{2} = -1 \\pm \\sqrt{2}\n$$\nThe two roots are $\\xi_1 = -1 + \\sqrt{2} \\approx 0.414$ and $\\xi_2 = -1 - \\sqrt{2} \\approx -2.414$.\nThe magnitude of the second root is $|\\xi_2| \\approx 2.414 > 1$. Since a root lies outside the unit disk for $z=-2 \\in \\mathcal{G}$, the method is not stable at this point, and thus it cannot be A-stable on the grid $\\mathcal{G}$. This is an instance of the second Dahlquist stability barrier, which states no explicit linear multistep method can be A-stable. The result is **False**.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by investigating properties of linear multistep methods\n    for four distinct cases.\n    \"\"\"\n\n    # --- Case 1: Construct explicit k=2, p=3 method and test zero-stability ---\n    def case1():\n        # From first principles, we derive the coefficients.\n        # System of equations:\n        # a1 - 3*b1 = -8\n        # a1 - 2*b1 = -4\n        # => b1 = 4, a1 = 4\n        # a0 + a1 = -1 => a0 = -5\n        # b0 = a1 + 2 - b1 = 4 + 2 - 4 = 2\n        \n        alpha_coeffs = [-5, 4, 1]  # (alpha0, alpha1, alpha2)\n        \n        # Characteristic polynomial rho(xi) = a2*xi^2 + a1*xi + a0\n        # The coefficients for np.roots must be in descending power order.\n        rho_poly_coeffs = [alpha_coeffs[2], alpha_coeffs[1], alpha_coeffs[0]]\n        \n        roots = np.roots(rho_poly_coeffs)\n        \n        # Check root condition: all roots must have magnitude <= 1.\n        # A small tolerance is used for floating point comparisons, though\n        # for this specific case the roots are integers.\n        is_zero_stable = np.all(np.abs(roots) <= 1 + 1e-9)\n        \n        return is_zero_stable\n\n    # --- Case 2: Zero-stability of a reference explicit 2-step method ---\n    def case2():\n        alpha_coeffs = [0, -1, 1]  # (alpha0, alpha1, alpha2)\n        \n        # rho(xi) = 1*xi^2 - 1*xi + 0\n        rho_poly_coeffs = [alpha_coeffs[2], alpha_coeffs[1], alpha_coeffs[0]]\n        \n        roots = np.roots(rho_poly_coeffs)\n        \n        # Check magnitude condition: |xi| <= 1\n        mags_ok = np.all(np.abs(roots) <= 1 + 1e-9)\n        \n        # Check simplicity for roots on the unit circle\n        boundary_roots = roots[np.isclose(np.abs(roots), 1)]\n        _, counts = np.unique(np.round(boundary_roots, 5), return_counts=True)\n        simplicity_ok = np.all(counts == 1)\n        \n        return mags_ok and simplicity_ok\n\n    # --- Case 3: A-stability of the implicit trapezoidal rule on a grid ---\n    def case3():\n        # Define the grid G\n        x_vals = [-0.5, -1, -2, -4, -8]\n        y_vals = [-4, -2, 0, 2, 4]\n        grid_G = [complex(x, y) for x in x_vals for y in y_vals]\n        \n        # For the trapezoidal rule, the single root of the stability polynomial is\n        # xi(z) = (1 + z/2) / (1 - z/2).\n        # We need to check if |xi(z)| <= 1 for all z in G.\n        # This is equivalent to Re(z) <= 0.\n        \n        is_stable_on_grid = True\n        for z in grid_G:\n            # All x values in the grid are < 0, so Re(z) < 0.\n            # Thus, |xi(z)| must be < 1. The check is somewhat redundant\n            # but implemented for completeness.\n            xi = (1 + z/2) / (1 - z/2)\n            if np.abs(xi) > 1 + 1e-9:\n                is_stable_on_grid = False\n                break\n        \n        return is_stable_on_grid\n\n    # --- Case 4: A-stability of the explicit method from Case 2 on a grid ---\n    def case4():\n        # Define the grid G\n        x_vals = [-0.5, -1, -2, -4, -8]\n        y_vals = [-4, -2, 0, 2, 4]\n        grid_G = [complex(x, y) for x in x_vals for y in y_vals]\n\n        is_stable_on_grid = True\n        for z in grid_G:\n            # Stability polynomial: xi^2 - (1 + 3z/2)xi + z/2 = 0\n            # Coefficients for np.roots: [a, b, c] for a*x^2 + b*x + c\n            stability_poly_coeffs = [1, -(1 + 3*z/2), z/2]\n            roots = np.roots(stability_poly_coeffs)\n\n            # Check magnitude condition: |xi| <= 1 for all roots\n            if np.any(np.abs(roots) > 1 + 1e-9):\n                is_stable_on_grid = False\n                break\n            \n            # Check simplicity condition for boundary roots\n            boundary_roots = roots[np.isclose(np.abs(roots), 1)]\n            if len(boundary_roots) > 1:\n                # Check for repeated roots on the boundary\n                if np.isclose(boundary_roots[0], boundary_roots[1]):\n                   is_stable_on_grid = False\n                   break\n\n        return is_stable_on_grid\n\n    results = [\n        case1(),\n        case2(),\n        case3(),\n        case4()\n    ]\n    \n    # Format the output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2446838"}, {"introduction": "A core skill in numerical analysis is the ability to derive a method tailored to specific requirements, translating desired properties like order of accuracy and stability into a system of equations for the method's coefficients. In this exercise, you will derive a specific two-step method from scratch by enforcing conditions on its order and zero-stability [@problem_id:3254441]. You will then perform an absolute stability analysis to determine the maximum stable step size for an oscillatory problem, directly connecting abstract stability theory to a practical and concrete constraint.", "problem": "Consider initial value problems (IVPs) of the form $y'(t)=f(t,y(t))$, $y(t_0)=y_0$, and linear multistep methods that approximate the solution on a uniform grid $t_n=t_0+n h$ by formulas of the form\n$$\ny_{n+1} \\;=\\; a_0\\,y_n \\;+\\; a_1\\,y_{n-1} \\;+\\; h\\bigl(b_{-1}\\,y'_{n+1} \\;+\\; b_0\\,y'_n\\bigr),\n$$\nwhere $h$ is the step size and $y'_k=f(t_k,y_k)$.\n\nDerive a specific two-step method within this family that is:\n- consistent and of order $2$ (that is, exact for $y(t)=1$, $y(t)=t$, and $y(t)=t^2$),\n- zero-stable with characteristic polynomial having simple roots at $\\xi=1$ and $\\xi=-1$.\n\nProceed as follows:\n1. Starting from the defining principle of linear multistep methods, enforce exactness on the polynomials $y(t)=1$, $y(t)=t$, and $y(t)=t^2$ to obtain the order conditions up to order $2$ and determine the coefficients $a_0$, $a_1$, $b_{-1}$, and $b_0$ under the additional zero-stability constraint that the characteristic polynomial has roots at $\\xi=1$ and $\\xi=-1$.\n2. Analyze the absolute stability of the resulting method by applying it to the linear test equation $y'(t)=\\lambda y(t)$ with $z=h\\lambda$, and derive the region of absolute stability along the imaginary axis $\\{z \\in \\mathbb{C}: z=\\mathrm{i}\\omega h,\\ \\omega\\in\\mathbb{R}\\}$.\n3. Using your stability condition on the imaginary axis, determine the largest admissible time step $h_{\\max}$ that ensures absolute stability for the oscillatory test equation $y'(t)=\\mathrm{i}\\,\\omega\\,y(t)$ with $\\omega=7.5\\,\\mathrm{s}^{-1}$. Round your answer to four significant figures and express the final step size in seconds. Your final answer must be a single number.", "solution": "The problem requires the derivation and analysis of a specific two-step linear multistep method. The process is divided into three parts: determining the method's coefficients, analyzing its absolute stability, and calculating the maximum stable step size for a given oscillatory problem.\n\nThe general form of the two-step method is given as:\n$$\ny_{n+1} \\;=\\; a_0\\,y_n \\;+\\; a_1\\,y_{n-1} \\;+\\; h\\bigl(b_{-1}\\,y'_{n+1} \\;+\\; b_0\\,y'_n\\bigr)\n$$\nTo analyze this method, we first express it in the standard form for a $k$-step linear multistep method, $\\sum_{j=0}^k \\alpha_j y_{n+j} = h \\sum_{j=0}^k \\beta_j f_{n+j}$, by shifting the index $n \\to n+1$. This gives:\n$$\ny_{n+2} \\;=\\; a_0\\,y_{n+1} \\;+\\; a_1\\,y_{n} \\;+\\; h\\bigl(b_{-1}\\,y'_{n+2} \\;+\\; b_0\\,y'_{n+1}\\bigr)\n$$\nRearranging into standard form:\n$$\ny_{n+2} - a_0\\,y_{n+1} - a_1\\,y_{n} = h\\bigl(b_{-1}\\,y'_{n+2} + b_0\\,y'_{n+1}\\bigr)\n$$\nFrom this, we identify the coefficients for the first and second characteristic polynomials:\n$\\alpha_2=1$, $\\alpha_1=-a_0$, $\\alpha_0=-a_1$.\n$\\beta_2=b_{-1}$, $\\beta_1=b_0$, and all other $\\beta_j=0$.\n\n### 1. Derivation of the Method's Coefficients\n\nThe problem specifies two constraints to determine the four coefficients $a_0, a_1, b_{-1}, b_0$.\n\nFirst, we use the zero-stability constraint. The first characteristic polynomial is $\\rho(\\xi) = \\sum_{j=0}^{2} \\alpha_j \\xi^j = \\alpha_2 \\xi^2 + \\alpha_1 \\xi + \\alpha_0 = \\xi^2 - a_0 \\xi - a_1$. The problem states that the roots of $\\rho(\\xi)$ are $\\xi_1=1$ and $\\xi_2=-1$. A polynomial with these roots must be of the form $C(\\xi-1)(\\xi+1) = C(\\xi^2-1)$. Since the leading coefficient of $\\rho(\\xi)$ is $\\alpha_2=1$, we have $C=1$, so $\\rho(\\xi) = \\xi^2 - 1$.\nComparing this with $\\rho(\\xi) = \\xi^2 - a_0 \\xi - a_1$, we deduce the coefficients:\n$-a_0 = 0 \\implies a_0 = 0$\n$-a_1 = -1 \\implies a_1 = 1$\n\nWith $a_0=0$ and $a_1=1$, the method simplifies to:\n$$\ny_{n+1} = y_{n-1} + h\\bigl(b_{-1}\\,y'_{n+1} + b_0\\,y'_n\\bigr)\n$$\nNext, we use the order conditions. The method must be exact for the polynomials $y(t)=1, y(t)=t,$ and $y(t)=t^2$. We can test this by substituting these functions into the method's equation. For simplicity, let's center the grid points around $t_n$, such that $t_{n+1}=t_n+h$ and $t_{n-1}=t_n-h$.\n\n- **Exactness for $y(t)=1$**:\nIf $y(t)=1$, then $y'(t)=0$. The formula becomes $1 = 1 + h(b_{-1} \\cdot 0 + b_0 \\cdot 0)$, which simplifies to $1=1$. This condition is satisfied for any choice of $b_{-1}, b_0$. This is equivalent to the consistency condition $\\rho(1)=0$, which is already fulfilled by our choice of roots.\n\n- **Exactness for $y(t)=t$**:\nIf $y(t)=t$, then $y'(t)=1$. The exact values at the grid points are $y(t_{n+1})=t_{n+1}$ and $y(t_{n-1})=t_{n-1}$. The formula becomes:\n$t_{n+1} = t_{n-1} + h(b_{-1} \\cdot 1 + b_0 \\cdot 1)$\n$(t_n+h) = (t_n-h) + h(b_{-1} + b_0)$\n$2h = h(b_{-1} + b_0)$\n$b_{-1} + b_0 = 2$\n\n- **Exactness for $y(t)=t^2$**:\nIf $y(t)=t^2$, then $y'(t)=2t$. The exact values are $y(t_{n+1})=t_{n+1}^2$, $y(t_{n-1})=t_{n-1}^2$, $y'(t_{n+1})=2t_{n+1}$, and $y'(t_n)=2t_n$. The formula becomes:\n$t_{n+1}^2 = t_{n-1}^2 + h\\bigl(b_{-1}(2t_{n+1}) + b_0(2t_n)\\bigr)$\n$(t_n+h)^2 = (t_n-h)^2 + 2h\\bigl(b_{-1}(t_n+h) + b_0 t_n\\bigr)$\n$t_n^2+2t_nh+h^2 = t_n^2-2t_nh+h^2 + 2h\\bigl((b_{-1}+b_0)t_n + b_{-1}h\\bigr)$\n$4t_nh = 2h\\bigl((b_{-1}+b_0)t_n + b_{-1}h\\bigr)$\n$2t_n = (b_{-1}+b_0)t_n + b_{-1}h$\nSubstituting the condition $b_{-1}+b_0=2$ from the previous step:\n$2t_n = 2t_n + b_{-1}h$\nThis implies $b_{-1}h = 0$. Since $h \\neq 0$, we must have $b_{-1}=0$.\n\nFrom $b_{-1}+b_0=2$ and $b_{-1}=0$, we find $b_0=2$.\nThe determined coefficients are $a_0=0, a_1=1, b_{-1}=0, b_0=2$. The resulting method is the explicit two-step midpoint rule:\n$$\ny_{n+1} = y_{n-1} + 2h y'_n\n$$\n\n### 2. Absolute Stability Analysis\n\nTo analyze the absolute stability, we apply the method to the Dahlquist test equation $y'(t)=\\lambda y(t)$, where $\\lambda \\in \\mathbb{C}$. Substituting $y'_n = \\lambda y_n$ into the method gives:\n$$\ny_{n+1} = y_{n-1} + 2h\\lambda y_n\n$$\nLet $z=h\\lambda$. The recurrence relation is:\n$$\ny_{n+1} - 2z y_n - y_{n-1} = 0\n$$\nWe seek solutions of the form $y_n = \\xi^n$. Substituting this ansatz into the recurrence yields the stability polynomial:\n$$\n\\xi^{n+1} - 2z\\xi^n - \\xi^{n-1} = 0\n$$\nDividing by $\\xi^{n-1}$ (for $\\xi \\neq 0$), we get:\n$$\n\\xi^2 - 2z\\xi - 1 = 0\n$$\nThe method is absolutely stable for a given $z$ if all roots $\\xi$ of this polynomial satisfy $|\\xi| \\le 1$, and any roots with $|\\xi|=1$ are simple.\n\nWe are asked to find the region of absolute stability along the imaginary axis, where $z=i\\alpha$ for $\\alpha = \\omega h \\in \\mathbb{R}$. The stability polynomial becomes:\n$$\n\\xi^2 - 2i\\alpha\\xi - 1 = 0\n$$\nThe roots are given by the quadratic formula:\n$$\n\\xi = \\frac{2i\\alpha \\pm \\sqrt{(-2i\\alpha)^2 - 4(1)(-1)}}{2} = \\frac{2i\\alpha \\pm \\sqrt{-4\\alpha^2 + 4}}{2} = i\\alpha \\pm \\sqrt{1-\\alpha^2}\n$$\nWe analyze the magnitude of the roots based on the value of $\\alpha$:\n\n- **Case 1: $|\\alpha|<1$**. In this case, $1-\\alpha^2 > 0$, and $\\sqrt{1-\\alpha^2}$ is real. The roots are $\\xi_{1,2} = i\\alpha \\pm \\sqrt{1-\\alpha^2}$. The magnitude squared of these complex roots is:\n$|\\xi|^2 = (\\pm\\sqrt{1-\\alpha^2})^2 + (\\alpha)^2 = (1-\\alpha^2) + \\alpha^2 = 1$.\nSo, $|\\xi|=1$. Since $\\alpha \\neq \\pm 1$, the term $\\sqrt{1-\\alpha^2}$ is non-zero, meaning the two roots are distinct. Thus, for $|\\alpha|<1$, we have two distinct roots on the unit circle. The method is stable.\n\n- **Case 2: $|\\alpha|=1$**. In this case, $1-\\alpha^2=0$. The roots are $\\xi = i\\alpha$. This is a double root on the unit circle ($|\\xi|=|i\\alpha|=|\\alpha|=1$). A method with a multiple root on the boundary of the unit disk is not absolutely stable.\n\n- **Case 3: $|\\alpha|>1$**. In this case, $1-\\alpha^2 < 0$. We can write $\\sqrt{1-\\alpha^2} = i\\sqrt{\\alpha^2-1}$. The roots are:\n$\\xi = i\\alpha \\pm i\\sqrt{\\alpha^2-1} = i(\\alpha \\pm \\sqrt{\\alpha^2-1})$.\nThe roots are purely imaginary. Their magnitudes are $|\\xi| = |\\alpha \\pm \\sqrt{\\alpha^2-1}|$.\nOne root has magnitude $|\\xi_1| = |\\alpha| + \\sqrt{\\alpha^2-1}$. Since $|\\alpha|>1$, $\\sqrt{\\alpha^2-1}>0$, so $|\\xi_1| > |\\alpha| > 1$.\nSince one root has magnitude greater than $1$, the method is unstable.\n\nCombining these findings, the region of absolute stability along the imaginary axis corresponds to the interval where $|\\alpha|<1$. The endpoints are excluded. The region is $\\{z \\in \\mathbb{C}: z=i\\alpha, -1 < \\alpha < 1 \\}$.\n\n### 3. Calculation of the Maximum Step Size $h_{\\max}$\n\nWe are given the oscillatory test equation $y'(t)=i\\omega y(t)$ with $\\omega=7.5\\,\\mathrm{s}^{-1}$. This corresponds to the Dahlquist test equation with $\\lambda = i\\omega$.\nTherefore, $z = h\\lambda = i\\omega h$. In the notation of the stability analysis, $\\alpha = \\omega h$.\nThe condition for absolute stability is $|\\alpha| < 1$, which translates to:\n$$\n|\\omega h| < 1\n$$\nGiven $\\omega=7.5\\,\\mathrm{s}^{-1}$ and that the step size $h$ must be positive, the inequality becomes:\n$$\n7.5 h < 1\n$$\n$$\nh < \\frac{1}{7.5} \\, \\mathrm{s}\n$$\nThe set of admissible time steps is the interval $(0, 1/7.5)$. The largest admissible time step, $h_{\\max}$, is the supremum of this set.\n$$\nh_{\\max} = \\frac{1}{7.5} \\, \\mathrm{s} = \\frac{1}{15/2} \\, \\mathrm{s} = \\frac{2}{15} \\, \\mathrm{s}\n$$\nTo obtain the numerical value, we compute the fraction:\n$$\nh_{\\max} = \\frac{2}{15} \\approx 0.133333... \\, \\mathrm{s}\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\nh_{\\max} \\approx 0.1333 \\, \\mathrm{s}\n$$", "answer": "$$\\boxed{0.1333}$$", "id": "3254441"}, {"introduction": "Not all stable methods are suitable for all types of problems; while Backward Differentiation Formulas (BDFs) are celebrated for their effectiveness on stiff equations, their performance on purely oscillatory systems reveals significant drawbacks. This is because numerical methods can introduce errors not just in amplitude, but also in the phase of the solution. This practice demonstrates the critical concept of matching a numerical method to the characteristics of the problem being solved [@problem_id:3254493]. By applying the BDF2 method to an undamped oscillator, you will analyze and quantify the resulting phase lag and artificial damping, providing a clear illustration of why this method is a poor choice for simulating long-term conservative dynamics.", "problem": "Consider the initial value problem (IVP) given by the second-order ordinary differential equation (ODE) $y''(t) + 100\\,y(t) = 0$ with initial conditions $y(0) = 1$ and $y'(0) = 0$. This IVP describes a non-stiff but highly oscillatory system with angular frequency $\\omega = 10$. To apply a multistep method, first rewrite the IVP as a first-order system by introducing the state vector $Y(t) = \\begin{bmatrix} y(t) \\\\ v(t) \\end{bmatrix}$ where $v(t) = y'(t)$, leading to the system $Y'(t) = A\\,Y(t)$ with the constant matrix $A = \\begin{bmatrix} 0 & 1 \\\\ -100 & 0 \\end{bmatrix}$.\n\nStarting from the fundamental definition of a linear multistep method,\n$$\n\\sum_{j=0}^{k} \\alpha_j\\,y_{n-j} = h \\sum_{j=0}^{k} \\beta_j\\,f(t_{n-j}, y_{n-j}),\n$$\nderive the backward differentiation formula of order $2$ (BDF2) by ensuring second-order accuracy through Taylor series expansions about $t_n$, and the choice $\\beta_0 = 1$, $\\beta_j = 0$ for $j \\geq 1$. Then apply the resulting implicit method to the first-order system $Y'(t) = A\\,Y(t)$.\n\nExplain, using the scalar modal analysis on the complex test equation $y'(t) = i\\omega\\,y(t)$, why backward differentiation formulas (BDF) yield poor phase accuracy for oscillatory problems. Your explanation must be grounded in the mode-wise amplification factor analysis of the derived multistep recurrence, showing how the methodâ€™s step amplification factor $r(\\phi)$ (where $\\phi = \\omega h$) leads to a numerical angle per step $\\theta = \\arg(r)$ that deviates from the exact $\\phi$, and how this deviation accumulates over many steps to produce a significant total phase lag. Additionally, comment on the magnitude $|r|$ and its role in artificial damping of the oscillatory solution.\n\nImplement a program that, for each test case listed below, computes:\n- the accumulated phase lag over $N$ steps,\n$$\n\\Delta \\varphi_N = N\\left(\\theta - \\phi\\right),\n$$\nexpressed in radians, and\n- the amplitude ratio after $N$ steps,\n$$\n\\rho_N = |r|^N,\n$$\nwhich compares the numerical amplitude to the exact unit amplitude.\n\nAngle quantities must be expressed in radians. The final outputs must be floating-point numbers. Your program should not use any external input and should rely only on the provided test suite.\n\nUse the following test suite of step sizes $h$ and step counts $N$ (with $\\omega = 10$ fixed):\n- Test case 1 (happy path, moderate step): $h = 0.05$, $N = 400$.\n- Test case 2 (larger step): $h = 0.10$, $N = 200$.\n- Test case 3 (smaller step): $h = 0.02$, $N = 1000$.\n- Test case 4 (very small step, edge case approaching continuous limit): $h = 0.005$, $N = 4000$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list of the form $[\\Delta \\varphi_N, \\rho_N]$ corresponding to the above test cases in order. For example, the output format must be\n$$\n[ [\\Delta \\varphi_1, \\rho_1], [\\Delta \\varphi_2, \\rho_2], [\\Delta \\varphi_3, \\rho_3], [\\Delta \\varphi_4, \\rho_4] ].\n$$", "solution": "The problem statement is assessed as valid. It is scientifically grounded in the established theory of numerical analysis for ordinary differential equations, specifically concerning linear multistep methods. The problem is well-posed, providing all necessary definitions, equations, and parameters to derive the required formulas, perform the analysis, and implement the specified calculations. The language is objective and precise. Therefore, a full solution is warranted.\n\n### Derivation of the BDF2 Method\n\nThe general form of a $k$-step linear multistep method (LMM) for the initial value problem $y'(t) = f(t, y(t))$ is given by:\n$$\n\\sum_{j=0}^{k} \\alpha_j\\,y_{n-j} = h \\sum_{j=0}^{k} \\beta_j\\,f(t_{n-j}, y_{n-j})\n$$\nwhere $y_{m}$ is the numerical approximation of $y(t_m)$ at time $t_m = m h$.\n\nThe family of Backward Differentiation Formulas (BDF) is characterized by setting $\\beta_0 \\neq 0$ and $\\beta_j = 0$ for $j \\geq 1$. The problem specifies $\\beta_0=1$. This makes the method implicit, as the unknown $y_n$ appears on both sides of the equation via $f_n = f(t_n, y_n)$. The general BDF form is thus:\n$$\n\\sum_{j=0}^{k} \\alpha_j\\,y_{n-j} = h\\,f(t_n, y_n)\n$$\nFor the BDF2 method, the number of steps is $k=2$. The formula involves approximations at three time levels: $y_n$, $y_{n-1}$, and $y_{n-2}$.\n$$\n\\alpha_0 y_n + \\alpha_1 y_{n-1} + \\alpha_2 y_{n-2} = h f_n\n$$\nTo determine the coefficients $\\alpha_0$, $\\alpha_1$, and $\\alpha_2$, we enforce accuracy by minimizing the local truncation error. The local truncation error, $\\tau_n$, is the residual obtained when the exact solution $y(t)$ is substituted into the formula. We expand $y(t_{n-1})$ and $y(t_{n-2})$ in Taylor series around $t_n$:\n\\begin{align*}\ny(t_{n-1}) &= y(t_n - h) = y(t_n) - h y'(t_n) + \\frac{h^2}{2} y''(t_n) - \\frac{h^3}{6} y'''(t_n) + O(h^4) \\\\\ny(t_{n-2}) &= y(t_n - 2h) = y(t_n) - 2h y'(t_n) + \\frac{(2h)^2}{2} y''(t_n) - \\frac{(2h)^3}{6} y'''(t_n) + O(h^4) \\\\\n&= y(t_n) - 2h y'(t_n) + 2h^2 y''(t_n) - \\frac{4h^3}{3} y'''(t_n) + O(h^4)\n\\end{align*}\nTo achieve second-order accuracy, the method must be exact for polynomials of degree up to $2$. This is equivalent to making the first three terms in the Taylor expansion of the local truncation error vanish. Let the operator $\\mathcal{L}_h[y(t)]$ be defined as $\\mathcal{L}_h[y(t)] = \\alpha_0 y(t_n) + \\alpha_1 y(t_{n-1}) + \\alpha_2 y(t_{n-2}) - h y'(t_n)$. Substituting the Taylor series expansions:\n\\begin{align*}\n\\mathcal{L}_h[y(t)] = &\\ \\alpha_0 y(t_n) \\\\\n&+ \\alpha_1 \\left( y(t_n) - h y'(t_n) + \\frac{h^2}{2} y''(t_n) - \\dots \\right) \\\\\n&+ \\alpha_2 \\left( y(t_n) - 2h y'(t_n) + 2h^2 y''(t_n) - \\dots \\right) \\\\\n&- h y'(t_n)\n\\end{align*}\nGrouping terms by derivatives of $y(t_n)$:\n\\begin{align*}\n\\mathcal{L}_h[y(t)] = &\\ (\\alpha_0 + \\alpha_1 + \\alpha_2) y(t_n) \\\\\n&- h (\\alpha_1 + 2\\alpha_2 + 1) y'(t_n) \\\\\n&+ h^2 \\left( \\frac{\\alpha_1}{2} + 2\\alpha_2 \\right) y''(t_n) + O(h^3)\n\\end{align*}\nFor the method to be second-order accurate, the coefficients of the terms up to $O(h^2)$ must be zero. This gives a system of linear equations for the $\\alpha_j$ coefficients:\n1. $y(t_n)$ term: $\\alpha_0 + \\alpha_1 + \\alpha_2 = 0$\n2. $y'(t_n)$ term: $\\alpha_1 + 2\\alpha_2 = -1$\n3. $y''(t_n)$ term: $\\frac{\\alpha_1}{2} + 2\\alpha_2 = 0$\n\nFrom equation (3), we get $\\alpha_1 = -4\\alpha_2$. Substituting this into equation (2) gives $-4\\alpha_2 + 2\\alpha_2 = -1$, which simplifies to $-2\\alpha_2 = -1$, so $\\alpha_2 = 1/2$.\nConsequently, $\\alpha_1 = -4(1/2) = -2$.\nFinally, from equation (1), $\\alpha_0 + (-2) + (1/2) = 0$, which gives $\\alpha_0 = 3/2$.\n\nThe resulting BDF2 formula is:\n$$\n\\frac{3}{2} y_n - 2 y_{n-1} + \\frac{1}{2} y_{n-2} = h f(t_n, y_n)\n$$\n\n### Scalar Modal Analysis and Phase Error\n\nTo analyze the performance of BDF2 on oscillatory problems, we apply it to the scalar complex test equation $y'(t) = \\lambda y(t)$, where $\\lambda = i\\omega$ for a purely oscillatory system with angular frequency $\\omega$. Here, $f(t_n, y_n) = \\lambda y_n$. The BDF2 formula becomes:\n$$\n\\frac{3}{2} y_n - 2 y_{n-1} + \\frac{1}{2} y_{n-2} = h \\lambda y_n\n$$\nThis is a linear homogeneous recurrence relation. We seek a solution of the form $y_n = r^n$, where $r$ is the amplification factor that maps the solution from one step to the next. Substituting this form into the equation and dividing by $r^{n-2}$ yields the characteristic polynomial for $r$:\n$$\n\\frac{3}{2} r^2 - 2 r + \\frac{1}{2} = h \\lambda r^2\n$$\nLet $z = h\\lambda = i\\omega h$. We introduce $\\phi = \\omega h$, which is the exact phase angle increment over one time step $h$. So, $z = i\\phi$. Rearranging the equation gives:\n$$\n\\left(\\frac{3}{2} - z\\right) r^2 - 2 r + \\frac{1}{2} = 0\n$$\nThe roots of this quadratic equation determine the stability and accuracy properties of the method. Using the quadratic formula, the roots are:\n$$\nr(z) = \\frac{2 \\pm \\sqrt{4 - 4\\left(\\frac{3}{2} - z\\right)\\left(\\frac{1}{2}\\right)}}{2\\left(\\frac{3}{2} - z\\right)} = \\frac{2 \\pm \\sqrt{4 - (3 - 2z)}}{3 - 2z} = \\frac{2 \\pm \\sqrt{1 + 2z}}{3 - 2z}\n$$\nThe principal root, which approximates $e^z$ for small $z$, corresponds to the '$+$' sign. Thus, the amplification factor for the BDF2 method is:\n$$\nr(\\phi) = \\frac{2 + \\sqrt{1 + 2i\\phi}}{3 - 2i\\phi}\n$$\n\nThe exact solution to $y' = i\\omega y$ evolves by multiplication with $e^{i\\omega h} = e^{i\\phi}$ at each step. This corresponds to a pure rotation in the complex plane by an angle $\\phi$, with no change in amplitude, since $|e^{i\\phi}| = 1$. The numerical method approximates this operation by multiplication with the complex number $r(\\phi)$.\n\n**Phase Error (Dispersion):** The numerical phase angle per step is $\\theta = \\arg(r(\\phi))$. The exact phase angle is $\\phi$. The phase error per step is $\\theta - \\phi$. For BDF methods and purely imaginary $z$, it can be shown that $\\theta < \\phi$, meaning the numerical solution's phase progression lags behind the true solution. This cumulative effect over many steps results in a significant total phase lag, $\\Delta\\varphi_N$:\n$$\n\\Delta \\varphi_N = N(\\theta - \\phi)\n$$\n\n**Amplitude Error (Artificial Damping):** The amplitude of the numerical solution is multiplied by $|r(\\phi)|$ at each step. For a conservative oscillatory system, the amplitude should remain constant, i.e., the amplification factor should have a magnitude of $1$. However, BDF methods are designed for their strong stability properties for stiff problems (where $\\Re(\\lambda) \\ll 0$), not for preserving oscillations. Their regions of absolute stability only touch the imaginary axis at the origin. For any $z=i\\phi$ with $\\phi \\neq 0$, the amplification factor satisfies $|r(\\phi)| < 1$. This introduces artificial numerical damping, causing the amplitude of the numerical solution to decay exponentially. The ratio of the numerical amplitude to the exact amplitude after $N$ steps, $\\rho_N$, is:\n$$\n\\rho_N = |r(\\phi)|^N\n$$\nThis demonstrates why BDF methods, while excellent for stiff problems, are generally unsuitable for simulating non-stiff, undamped oscillatory systems, as they introduce substantial phase lag and artificial damping.\n\n### Calculation Plan\nFor each test case with parameters $h$ and $N$, and fixed $\\omega=10$:\n1. Calculate the exact phase step: $\\phi = \\omega h$.\n2. Define the complex variable: $z = i\\phi$.\n3. Compute the complex amplification factor: $r = \\frac{2 + \\sqrt{1 + 2z}}{3 - 2z}$.\n4. Extract the numerical phase angle: $\\theta = \\arg(r)$.\n5. Compute the total accumulated phase lag: $\\Delta\\varphi_N = N(\\theta - \\phi)$.\n6. Compute the final amplitude ratio: $\\rho_N = |r|^N$.\nAll angle calculations are performed in radians.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and analyzes the BDF2 method for an oscillatory IVP.\n\n    This function calculates the accumulated phase lag and amplitude ratio\n    for the BDF2 method applied to y''(t) + 100y(t) = 0.\n    \"\"\"\n    \n    # Define the fixed angular frequency from the problem statement.\n    omega = 10.0\n\n    # Define the test cases from the problem statement.\n    # Each tuple is (step_size_h, number_of_steps_N).\n    test_cases = [\n        (0.05, 400),   # Test case 1\n        (0.10, 200),   # Test case 2\n        (0.02, 1000),  # Test case 3\n        (0.005, 4000)  # Test case 4\n    ]\n\n    results = []\n    for h, N in test_cases:\n        # 1. Calculate the exact phase step (phi = omega * h)\n        phi = omega * h\n\n        # 2. Define the complex variable z = i*phi\n        # In Python, 1j is the imaginary unit.\n        z = 1j * phi\n\n        # 3. Compute the complex amplification factor r(z) for BDF2.\n        # The formula for the principal root is r = (2 + sqrt(1 + 2z)) / (3 - 2z).\n        # np.sqrt correctly handles the principal square root of a complex number.\n        r = (2 + np.sqrt(1 + 2 * z)) / (3 - 2 * z)\n\n        # 4. Extract the numerical phase angle theta = arg(r).\n        # np.angle returns the argument of a complex number in radians.\n        theta = np.angle(r)\n        \n        # 5. Compute the total accumulated phase lag after N steps.\n        # This is N * (numerical_angle - exact_angle).\n        accumulated_phase_lag = N * (theta - phi)\n        \n        # 6. Compute the final amplitude ratio after N steps.\n        # This is |r|^N. np.abs() calculates the magnitude of a complex number.\n        amplitude_ratio = np.abs(r)**N\n\n        # Store the pair of results for this test case.\n        results.append([accumulated_phase_lag, amplitude_ratio])\n\n    # Final print statement in the exact required format.\n    # Example format: [[lag1,ratio1],[lag2,ratio2],...]\n    # We use an f-string and a generator expression to build the string.\n    print(f\"[{','.join(f'[{lag},{ratio}]' for lag, ratio in results)}]\")\n\nsolve()\n```", "id": "3254493"}]}