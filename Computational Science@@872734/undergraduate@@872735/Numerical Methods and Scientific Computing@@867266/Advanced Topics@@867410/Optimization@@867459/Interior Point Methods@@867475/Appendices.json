{"hands_on_practices": [{"introduction": "Why are these methods named \"interior point\" methods? The answer lies in the strict requirement to keep all iterates within the interior of the feasible region, never touching the boundary. This first exercise [@problem_id:3242666] is a crucial thought experiment that explores the fundamental algebraic breakdown that occurs if this rule is violated, revealing the mathematical bedrock upon which these powerful algorithms are built.", "problem": "Consider the standard form linear program with variables $x \\in \\mathbb{R}^n$, dual variables $y \\in \\mathbb{R}^m$, and dual slacks $s \\in \\mathbb{R}^n$:\nminimize $c^\\top x$ subject to $A x = b$ and $x \\ge 0$, where $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $c \\in \\mathbb{R}^n$. A primal-dual interior point method enforces the Karush–Kuhn–Tucker (KKT) conditions $A x = b$, $A^\\top y + s = c$, $X S e = \\mu e$, and $x > 0$, $s > 0$, where $X = \\mathrm{diag}(x)$, $S = \\mathrm{diag}(s)$, $e \\in \\mathbb{R}^n$ is the vector of ones, and $\\mu > 0$ is a scalar controlling complementarity. At each iteration, a Newton step $(\\Delta x, \\Delta y, \\Delta s)$ is computed by linearizing the KKT conditions, and the new iterate is updated by $x^+ = x + \\alpha \\Delta x$, $y^+ = y + \\alpha \\Delta y$, and $s^+ = s + \\alpha \\Delta s$, with a step length $0 < \\alpha \\le 1$ chosen to maintain strict positivity of $x$ and $s$.\n\nSuppose the step length rule $\\alpha$ is flawed and allows one component, say $x_j^+$ or $s_j^+$, to become exactly $0$ at the next iterate. Using only the KKT structure and the Newton linearization as a base, identify where the algebra required to form the subsequent Newton system necessarily breaks down at the next iteration. Choose the most precise statement.\n\nA. Forming the next Newton system requires evaluating inverses and square roots of diagonal matrices built from $x$ and $s$, such as $X^{-1}$, $S^{-1}$, and $D = S^{-1/2} X^{1/2}$; with $x_j = 0$ or $s_j = 0$, these operations are undefined, and consequently the normal equations matrix $A S^{-1} X A^\\top$ cannot be assembled.\n\nB. The Newton system remains well-defined because the equations only involve $X$ and $S$ (not their inverses), so zero entries do not affect solvability; only minor adjustments to residuals are needed.\n\nC. The complementarity residual $r_\\mu = X S e - \\sigma \\mu e$ remains computable, so the algebra of the next iteration is intact; failure, if any, occurs only at exact optimality.\n\nD. The central path parameter $\\mu = \\frac{x^\\top s}{n}$ becomes negative when any component is zero, so the algorithm fails because $\\mu < 0$ is not permitted; the Newton system itself remains algebraically well-defined.", "solution": "We start from the Karush–Kuhn–Tucker (KKT) conditions for the linear program: $A x = b$, $A^\\top y + s = c$, $x \\ge 0$, $s \\ge 0$, and complementarity $x_i s_i = 0$ for all $i$ at optimality. In interior point methods, we replace hard complementarity by $X S e = \\mu e$ with $x > 0$, $s > 0$, and $\\mu > 0$, staying strictly in the interior. The Newton system is derived by linearizing the residuals of primal feasibility, dual feasibility, and complementarity. Let the residuals be\n$r_b = A x - b$,\n$r_c = A^\\top y + s - c$,\n$r_\\mu = X S e - \\sigma \\mu e$,\nfor a centering parameter $0 \\le \\sigma \\le 1$.\n\nLinearizing gives the system\n$A \\Delta x = - r_b$,\n$A^\\top \\Delta y + \\Delta s = - r_c$,\n$S \\Delta x + X \\Delta s = - r_\\mu$.\nEliminating $\\Delta s$ from the second equation, we have $\\Delta s = - r_c - A^\\top \\Delta y$. Substituting into the third equation yields\n$S \\Delta x + X(- r_c - A^\\top \\Delta y) = - r_\\mu$,\nwhich rearranges to\n$S \\Delta x - X A^\\top \\Delta y = - r_\\mu + X r_c$.\nTo eliminate $\\Delta x$, solve $S \\Delta x = - r_\\mu + X r_c + X A^\\top \\Delta y$ and multiply by $S^{-1}$, obtaining\n$\\Delta x = S^{-1}(- r_\\mu + X r_c + X A^\\top \\Delta y)$.\nSubstituting this into $A \\Delta x = - r_b$ gives the normal equations for $\\Delta y$:\n$A S^{-1}(- r_\\mu + X r_c + X A^\\top \\Delta y) = - r_b$,\nor equivalently,\n$A S^{-1} X A^\\top \\Delta y = - r_b + A S^{-1}(r_\\mu - X r_c)$.\n\nThis derivation exhibits a key structural requirement: the diagonal matrices $X$ and $S$ must be strictly positive so that $X^{-1}$ and $S^{-1}$ exist. Many implementations also use symmetric scaling with $D = S^{-1/2} X^{1/2}$ to factor the normal equations stably. In addition, if one uses a primal barrier perspective, the gradient of the logarithmic barrier $- \\sum_{i=1}^n \\ln x_i$ is $- X^{-1} e$, which is undefined at $x_j = 0$.\n\nIf the step length $\\alpha$ allows $x_j^+ = 0$ or $s_j^+ = 0$, then at the next iteration $X$ or $S$ has a zero diagonal entry. The algebraic operations $X^{-1}$, $S^{-1}$, $S^{-1/2}$, and $X^{1/2}$ required in forming either the Newton direction (via elimination) or common scalings become undefined. In particular, the normal equations matrix $A S^{-1} X A^\\top$ cannot be formed because $S^{-1}$ does not exist when any $s_j = 0$; similarly, alternative eliminations that involve $X^{-1}$ fail when any $x_j = 0$. Therefore, the breakdown occurs precisely at the point of constructing the linear system for the next Newton step.\n\nOption-by-option analysis:\n\nA. This states that the next Newton system requires inverses and square roots of $X$ and $S$, such as $X^{-1}$, $S^{-1}$, and $D = S^{-1/2} X^{1/2}$, and that with a zero component these operations are undefined, preventing assembly of the normal equations $A S^{-1} X A^\\top$. This matches the derivation: elimination explicitly uses $S^{-1}$ (or, in an alternative formulation, $X^{-1}$), and symmetric scaling uses square roots of positive entries. With $x_j = 0$ or $s_j = 0$, these operations are not defined, so the algebra fails. Verdict: Correct.\n\nB. This claims the Newton system only involves $X$ and $S$ without their inverses, so zeros do not affect solvability. The derivation shows the contrary: elimination to form a reduced system uses $S^{-1}$ (and often $X^{-1}$), and if one attempts to avoid inverses, the block system with $S$ and $X$ becomes singular when diagonal entries are zero. Thus solvability and assembly depend on strict positivity. Verdict: Incorrect.\n\nC. This asserts that because $r_\\mu$ can be computed, the algebra is intact, and failure occurs only at optimality. While $r_\\mu = X S e - \\sigma \\mu e$ can be evaluated even if some entries are zero (it yields finite numbers), computing the Newton direction requires $S^{-1}$ or $X^{-1}$. Hence the algebraic failure arises before optimality, immediately when forming the next Newton system. Verdict: Incorrect.\n\nD. This states that $\\mu = \\frac{x^\\top s}{n}$ becomes negative when any component is zero. If any component is zero, $x^\\top s$ can decrease, possibly to $0$, but it remains a nonnegative real number; division by $n$ yields a nonnegative scalar. The sign issue is false, and regardless, the central concern is the undefined inverses, not the sign of $\\mu$. Verdict: Incorrect.", "answer": "$$\\boxed{A}$$", "id": "3242666"}, {"introduction": "Now that we understand the necessity of staying strictly within the feasible set, let's dissect the mechanics of taking a single step toward the solution. This practice [@problem_id:3242579] connects a high-level optimization task—minimizing the largest eigenvalue of a matrix—to the concrete calculations of a primal-dual interior-point iteration. You will perform one full step of the Nesterov-Todd scaled method, grounding the abstract theory of interior-point methods in a tangible computational process.", "problem": "You will work through a two-part task that connects constrained eigenvalue minimization to semidefinite programming (SDP) and then carries out a single iteration of a primal-dual interior-point method using Nesterov–Todd (NT) scaling.\n\nPart I (Formulation). Consider the constrained eigenvalue minimization problem\n- minimize the largest eigenvalue of a symmetric matrix $Z \\in \\mathbb{S}^{2}$,\n- subject to the linear constraint $\\operatorname{tr}(Z) = 2$.\n\nStarting from the definition of the largest eigenvalue $\\lambda_{\\max}(Z)$ as the minimum real number $t$ such that $t I - Z \\succeq 0$, derive an equivalent linear matrix inequality (LMI) formulation as an SDP with decision variables $(t, Z)$, and state the resulting conic form constraints explicitly.\n\nPart II (One NT-scaled primal-dual iteration and centrality symmetry). Consider the standard-form SDP associated to the conic block $X \\in \\mathbb{S}^{2}$ defined by\n- minimize $C \\bullet X$,\n- subject to $A \\bullet X = b$,\n- and $X \\succ 0$,\nwhere $C = 0 \\cdot I \\in \\mathbb{S}^{2}$, $A = I \\in \\mathbb{S}^{2}$, $b = 2 \\in \\mathbb{R}$, and $\\bullet$ denotes the Frobenius inner product. Its dual is\n- maximize $b y$,\n- subject to $A^{*}(y) + S = C$,\n- and $S \\succ 0$,\nwith $A^{*}(y) = y I$ and $S \\in \\mathbb{S}^{2}$.\n\nStart from the strictly feasible primal-dual point $(X_{0}, y_{0}, S_{0})$ given by $X_{0} = I$, $y_{0} = -2$, $S_{0} = 2 I$. Set the centering parameter to $\\sigma = \\tfrac{1}{2}$. Using the Nesterov–Todd (NT) scaled primal-dual direction, compute a single iteration that uses the full step length if it preserves positive definiteness, and otherwise the largest step that maintains $X \\succ 0$ and $S \\succ 0$. Then:\n- compute the updated duality measure $\\mu_{1} = \\tfrac{1}{n} \\operatorname{tr}(X_{1} S_{1})$ with $n = 2$,\n- and verify the centrality symmetry condition $X_{1} S_{1} = \\mu_{1} I$.\n\nReport the single scalar value of $\\mu_{1}$ as your final answer. No rounding is required; provide the exact value.", "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n**Part I:**\n- Objective: Minimize the largest eigenvalue, $\\lambda_{\\max}(Z)$, of a symmetric matrix $Z \\in \\mathbb{S}^{2}$.\n- Constraint: $\\operatorname{tr}(Z) = 2$.\n- Definition: $\\lambda_{\\max}(Z)$ is the minimum real number $t$ such that $t I - Z \\succeq 0$.\n- Task: Derive an equivalent semidefinite program (SDP) with decision variables $(t, Z)$ and state the conic form constraints.\n\n**Part II:**\n- Primal SDP: Minimize $C \\bullet X$, subject to $A \\bullet X = b$ and $X \\succ 0$.\n- Primal Data: $X \\in \\mathbb{S}^{2}$, $C = 0 \\cdot I \\in \\mathbb{S}^{2}$, $A = I \\in \\mathbb{S}^{2}$, $b = 2 \\in \\mathbb{R}$.\n- Dual SDP: Maximize $b y$, subject to $A^{*}(y) + S = C$ and $S \\succ 0$.\n- Dual Data: $S \\in \\mathbb{S}^{2}$, $A^{*}(y) = y I$.\n- Initial Point: $(X_{0}, y_{0}, S_{0})$ where $X_{0} = I$, $y_{0} = -2$, and $S_{0} = 2 I$.\n- Centering Parameter: $\\sigma = \\frac{1}{2}$.\n- Method: Nesterov–Todd (NT) scaled primal-dual direction.\n- Task: \n  1. Compute one iteration to find $(X_1, S_1, y_1)$ using a full step length if feasible, otherwise the largest feasible step.\n  2. Compute the updated duality measure $\\mu_{1} = \\frac{1}{n} \\operatorname{tr}(X_{1} S_{1})$ with $n = 2$.\n  3. Verify the centrality symmetry condition $X_{1} S_{1} = \\mu_{1} I$.\n  4. Report the value of $\\mu_1$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined and consists of two parts.\n\nPart I is a standard reformulation task in the field of convex optimization, converting an eigenvalue minimization problem into an SDP. The provided definition is correct.\n\nPart II sets up a specific SDP and asks for one iteration of a primal-dual interior-point method. Let's verify the feasibility of the initial point $(X_0, y_0, S_0)$.\n- Primal strict feasibility ($X_0 \\succ 0$): $X_0 = I$, the identity matrix in $\\mathbb{S}^2$, is positive definite. This holds.\n- Primal constraint satisfaction ($A \\bullet X_0 = b$): $A \\bullet X_0 = I \\bullet I = \\operatorname{tr}(I^T I) = \\operatorname{tr}(I) = 2$. The given $b=2$. The constraint is satisfied.\n- Dual strict feasibility ($S_0 \\succ 0$): $S_0 = 2I$ is positive definite. This holds.\n- Dual constraint satisfaction ($A^{*}(y_0) + S_0 = C$): $A^{*}(y_0) + S_0 = y_0 I + S_0 = (-2)I + 2I = 0$. The given $C = 0 \\cdot I = 0$. The constraint is satisfied.\n\nAll data are self-contained and consistent. The problem is scientifically grounded in numerical optimization, is objective, and well-posed. No flaws are detected.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Part I: Formulation\n\nThe problem is to minimize the largest eigenvalue of a symmetric matrix $Z \\in \\mathbb{S}^{2}$, subject to a linear constraint on its trace.\n$$\n\\begin{array}{ll}\n\\text{minimize} & \\lambda_{\\max}(Z) \\\\\n\\text{subject to} & \\operatorname{tr}(Z) = 2, \\\\\n& Z \\in \\mathbb{S}^{2}.\n\\end{array}\n$$\nWe introduce an auxiliary scalar variable $t \\in \\mathbb{R}$. The objective can be reformulated as minimizing $t$ subject to the additional constraint $t \\ge \\lambda_{\\max}(Z)$. The problem becomes:\n$$\n\\begin{array}{ll}\n\\text{minimize} & t \\\\\n\\text{subject to} & \\lambda_{\\max}(Z) \\le t, \\\\\n& \\operatorname{tr}(Z) = 2, \\\\\n& Z \\in \\mathbb{S}^{2}, t \\in \\mathbb{R}.\n\\end{array}\n$$\nThe problem statement provides the key equivalence: $\\lambda_{\\max}(Z) \\le t$ is equivalent to the linear matrix inequality (LMI) $t I - Z \\succeq 0$, where $\\succeq 0$ denotes that the matrix is positive semidefinite and $I$ is the $2 \\times 2$ identity matrix.\n\nSubstituting this equivalence yields the SDP formulation in the decision variables $(t, Z)$:\n$$\n\\begin{array}{ll}\n\\text{minimize} & t \\\\\n\\text{subject to} & \\operatorname{tr}(Z) = 2, \\\\\n& t I - Z \\succeq 0.\n\\end{array}\n$$\nThe resulting constraints are explicitly:\n1.  A linear equality constraint: $\\operatorname{tr}(Z) = 2$.\n2.  A conic constraint (specifically, an LMI): The matrix $t I - Z$ must be positive semidefinite.\n\n### Part II: One NT-scaled primal-dual iteration\n\nWe are given the primal-dual pair of SDPs and an initial strictly feasible point $(X_0, y_0, S_0) = (I, -2, 2I)$. The matrix dimension is $n=2$.\n\nFirst, we compute the initial duality measure $\\mu_0$:\n$$ \\mu_0 = \\frac{1}{n} \\operatorname{tr}(X_0 S_0) = \\frac{1}{2} \\operatorname{tr}(I \\cdot 2I) = \\frac{1}{2} \\operatorname{tr}(2I) = \\frac{1}{2}(2+2) = 2. $$\nWe observe that $X_0 S_0 = I (2I) = 2I$ and $\\mu_0 I = 2I$. Thus, $X_0 S_0 = \\mu_0 I$, which means the initial point lies on the central path.\n\nThe Nesterov-Todd (NT) direction $(\\Delta X, \\Delta y, \\Delta S)$ is found by solving the linearized KKT system for the perturbed barrier problem. Since the initial point is feasible, the feasibility residuals are zero. The system to solve for the direction is:\n1.  Primal feasibility: $A \\bullet \\Delta X = 0 \\implies \\operatorname{tr}(\\Delta X) = 0$.\n2.  Dual feasibility: $A^{*}(\\Delta y) + \\Delta S = 0 \\implies (\\Delta y)I + \\Delta S = 0$.\n3.  Centrality condition: $\\Delta X S_0 + X_0 \\Delta S = \\sigma \\mu_0 I - X_0 S_0$.\n\nFor a starting point on the central path where $X_0$ and $S_0$ are multiples of the identity, they commute, so $X_0 S_0 = S_0 X_0$. In this case, the NT direction system simplifies and is equivalent to other common primal-dual directions.\n\nLet's solve this system.\nFrom (2), we have $\\Delta S = -(\\Delta y)I$.\n\nNow, we compute the right-hand side of equation (3):\n$$ \\sigma \\mu_0 I - X_0 S_0 = \\frac{1}{2}(2)I - 2I = I - 2I = -I. $$\nSubstitute $X_0 = I$, $S_0 = 2I$, and the expression for the right-hand side into (3):\n$$ \\Delta X (2I) + I (\\Delta S) = -I \\implies 2\\Delta X + \\Delta S = -I. $$\nNow substitute $\\Delta S = -(\\Delta y)I$:\n$$ 2\\Delta X - (\\Delta y)I = -I. $$\nTo solve for $\\Delta y$, we take the trace of this equation and use the condition $\\operatorname{tr}(\\Delta X)=0$ from (1):\n$$ \\operatorname{tr}(2\\Delta X - (\\Delta y)I) = \\operatorname{tr}(-I) $$\n$$ 2\\operatorname{tr}(\\Delta X) - \\Delta y \\operatorname{tr}(I) = -2 $$\n$$ 2(0) - \\Delta y(2) = -2 $$\n$$ -2\\Delta y = -2 \\implies \\Delta y = 1. $$\nWith $\\Delta y = 1$, we find $\\Delta S$:\n$$ \\Delta S = -(\\Delta y)I = -I. $$\nAnd we find $\\Delta X$:\n$$ 2\\Delta X - (1)I = -I \\implies 2\\Delta X = 0 \\implies \\Delta X = 0. $$\nThe computed primal-dual direction is $(\\Delta X, \\Delta y, \\Delta S) = (0, 1, -I)$.\n\nNext, we find the step length $\\alpha$. The problem asks to use a full step $\\alpha=1$ if it preserves positive definiteness ($X_1 \\succ 0, S_1 \\succ 0$), and otherwise the largest feasible step. Let's check feasibility for $\\alpha > 0$.\nThe next iterates are given by:\n$X(\\alpha) = X_0 + \\alpha \\Delta X = I + \\alpha(0) = I$.\n$S(\\alpha) = S_0 + \\alpha \\Delta S = 2I + \\alpha(-I) = (2-\\alpha)I$.\n\nFor $X(\\alpha) \\succ 0$: Since $X(\\alpha) = I$, which is positive definite, this condition holds for any $\\alpha$.\nFor $S(\\alpha) \\succ 0$: We need $(2-\\alpha)I \\succ 0$, which requires $2-\\alpha > 0$, or $\\alpha < 2$.\n\nSince the maximum feasible step length is $2$, the full step length $\\alpha = 1$ is feasible. We use $\\alpha=1$.\n\nWe compute the new iterate $(X_1, y_1, S_1)$:\n$y_1 = y_0 + \\alpha \\Delta y = -2 + 1(1) = -1$.\n$X_1 = X_0 + \\alpha \\Delta X = I + 1(0) = I$.\n$S_1 = S_0 + \\alpha \\Delta S = 2I + 1(-I) = I$.\n\nNow, we compute the updated duality measure $\\mu_1$:\n$$ \\mu_1 = \\frac{1}{n} \\operatorname{tr}(X_1 S_1) = \\frac{1}{2} \\operatorname{tr}(I \\cdot I) = \\frac{1}{2} \\operatorname{tr}(I) = \\frac{1}{2}(2) = 1. $$\nFinally, we verify the centrality symmetry condition $X_1 S_1 = \\mu_1 I$:\n$X_1 S_1 = I \\cdot I = I$.\n$\\mu_1 I = 1 \\cdot I = I$.\nThe condition $X_1 S_1 = \\mu_1 I$ is satisfied.\n\nThe final requested value is $\\mu_1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "3242579"}, {"introduction": "A practical solver must not only find an optimum but also gracefully handle problems that are infeasible or unbounded. This final, comprehensive exercise [@problem_id:3242648] challenges you to build a complete solver from scratch using the Homogeneous Self-Dual Embedding (HSDE) model. By implementing this sophisticated framework, you will learn how to create a robust algorithm that unifies the search for a solution with the ability to reliably detect and certify infeasibility.", "problem": "Implement a complete, runnable program that constructs and solves the Homogeneous Self-Dual Embedding (HSDE) of a small family of linear programs using an interior-point method, and classifies each problem as either feasible-optimal, primal infeasible, or dual infeasible (unbounded primal). The program must produce a single line of output, a comma-separated list of integers enclosed in square brackets, where each integer corresponds to the classification of one test case: $0$ for feasible-optimal (solution found), $1$ for primal infeasible, and $2$ for dual infeasible (primal unbounded).\n\nThe linear programming (LP) primal-dual pair in standard form is defined as follows. The primal problem is\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; c^\\top x \\quad \\text{subject to} \\quad A x = b, \\; x \\ge 0,\n$$\nand the dual problem is\n$$\n\\max_{y \\in \\mathbb{R}^m} \\; b^\\top y \\quad \\text{subject to} \\quad A^\\top y + s = c, \\; s \\ge 0,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $c \\in \\mathbb{R}^n$, $x \\in \\mathbb{R}^n$ are primal variables, $y \\in \\mathbb{R}^m$ are dual variables, and $s \\in \\mathbb{R}^n$ are dual slack variables. The Homogeneous Self-Dual Embedding (HSDE) introduces two additional nonnegative scalars $\\tau \\ge 0$ and $\\kappa \\ge 0$ to create a single feasibility problem whose solutions encode all outcomes: feasible-optimality, primal infeasibility, or dual infeasibility.\n\nStarting from core definitions in convex optimization and the Karush–Kuhn–Tucker (KKT) conditions, use the HSDE to form a system that enforces the primal and dual feasibility with a scalar complementarity condition and centrality. Specifically, include variables $(x, s, y, \\tau, \\kappa)$ obeying the homogeneous constraints\n$$\nA x - b \\tau = 0, \\quad A^\\top y + s - c \\tau = 0, \\quad c^\\top x - b^\\top y + \\kappa = 0,\n$$\nwith nonnegativity constraints $x \\ge 0$, $s \\ge 0$, $\\tau \\ge 0$, $\\kappa \\ge 0$, together with a complementarity condition coupling the primal and dual slacks that defines an interior-point central path. Derive a Newton step for a barrier-augmented system that linearizes these equations and the complementarity relations for $(x, s)$ and $(\\tau, \\kappa)$, and use backtracking to maintain strict positivity.\n\nYour implementation must:\n- Initialize $(x, s, y, \\tau, \\kappa)$ at strictly positive values, enforce centrality through a barrier parameter, and iteratively solve the linearized KKT system until a termination criterion is met.\n- Use residuals\n$r_p = A x - b \\tau$, $r_d = A^\\top y + s - c \\tau$, $r_g = c^\\top x - b^\\top y + \\kappa$\nand an averaged complementarity\n$$\n\\mu = \\frac{x^\\top s + \\tau \\kappa}{n+1}\n$$\nto guide progress. Maintain strict positivity of $x$, $s$, $\\tau$, and $\\kappa$ at all times.\n- Implement classification rules based on HSDE outcomes:\n    - Feasible-optimal ($0$): when $\\tau$ is bounded away from $0$, $\\kappa/\\tau$ is small, and the scaled residuals $\\|r_p\\|/\\tau$ and $\\|r_d\\|/\\tau$ are small.\n    - Primal infeasible ($1$): when $\\tau$ is close to $0$, $\\kappa$ is positive, and a Farkas certificate for primal infeasibility is detected via the dual variable $y$, namely $A^\\top y \\ge 0$ and $b^\\top y < 0$.\n    - Dual infeasible ($2$): when $\\tau$ is close to $0$, $\\kappa$ is positive, and a certificate $x \\ge 0$, $A x = 0$, $c^\\top x < 0$ is detected (primal unbounded).\n\nTest Suite:\nProvide three LP instances $(A, b, c)$ that collectively test feasibility, primal infeasibility, and dual infeasibility. Use the following concrete cases:\n1. Case F (feasible-optimal):\n   $$\n   A = \\begin{bmatrix} 1 & 2 \\\\ 1 & -1 \\end{bmatrix},\\quad b = \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix},\\quad c = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}.\n   $$\n   This system admits a strictly feasible solution with $x \\ge 0$.\n2. Case PI (primal infeasible):\n   $$\n   A = \\begin{bmatrix} 1 & 0 \\end{bmatrix},\\quad b = \\begin{bmatrix} -1 \\end{bmatrix},\\quad c = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\n   $$\n   The equality $x_1 = -1$ contradicts $x \\ge 0$, so no primal feasible point exists.\n3. Case DI (dual infeasible, primal unbounded):\n   $$\n   A = \\begin{bmatrix} 1 & -1 \\end{bmatrix},\\quad b = \\begin{bmatrix} 0 \\end{bmatrix},\\quad c = \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}.\n   $$\n   The constraint $A x = 0$ with $x \\ge 0$ admits nontrivial directions with $c^\\top x < 0$, implying primal unboundedness.\n\nYour program should run these three cases in order and produce a single output line containing a list of three integers $[r_F, r_{PI}, r_{DI}]$, where each $r$ is one of $\\{0,1,2\\}$ according to the classification described above.\n\nFinal Output Format:\nYour program should produce a single line containing the results as a comma-separated list enclosed in square brackets (e.g., \"[0,1,2]\"). No other text should be printed.", "solution": "The user has requested the implementation of a Homogeneous Self-Dual Embedding (HSDE) interior-point method for solving a family of linear programs (LPs). The solution must be capable of classifying each LP as feasible-optimal, primal infeasible, or dual infeasible.\n\n### Step 1: Extract Givens\n\n-   **Primal LP**: $\\min_{x \\in \\mathbb{R}^n} \\; c^\\top x \\quad \\text{subject to} \\quad A x = b, \\; x \\ge 0$.\n-   **Dual LP**: $\\max_{y \\in \\mathbb{R}^m} \\; b^\\top y \\quad \\text{subject to} \\quad A^\\top y + s = c, \\; s \\ge 0$.\n-   **HSDE Variables**: $(x, s, y, \\tau, \\kappa)$, where $x \\in \\mathbb{R}^n, s \\in \\mathbb{R}^n, y \\in \\mathbb{R}^m, \\tau \\in \\mathbb{R}, \\kappa \\in \\mathbb{R}$.\n-   **HSDE Homogeneous Constraints**:\n    1.  $A x - b \\tau = 0$\n    2.  $A^\\top y + s - c \\tau = 0$\n    3.  $c^\\top x - b^\\top y + \\kappa = 0$\n-   **Non-negativity Constraints**: $x \\ge 0, s \\ge 0, \\tau \\ge 0, \\kappa \\ge 0$.\n-   **Residuals Definitions**:\n    -   $r_p = A x - b \\tau$\n    -   $r_d = A^\\top y + s - c \\tau$\n    -   $r_g = c^\\top x - b^\\top y + \\kappa$\n-   **Averaged Complementarity**: $\\mu = \\frac{x^\\top s + \\tau \\kappa}{n+1}$.\n-   **Classification Rules**:\n    1.  **Feasible-optimal (0)**: $\\tau$ bounded away from $0$, $\\kappa/\\tau$ small, scaled residuals small.\n    2.  **Primal infeasible (1)**: $\\tau$ close to $0$, $\\kappa > 0$, and certificate $A^\\top y \\ge 0, b^\\top y < 0$.\n    3.  **Dual infeasible (2)**: $\\tau$ close to $0$, $\\kappa > 0$, and certificate $x \\ge 0, A x = 0, c^\\top x < 0$.\n-   **Test Cases**:\n    1.  **Case F (feasible-optimal)**: $A = \\begin{bmatrix} 1 & 2 \\\\ 1 & -1 \\end{bmatrix}, b = \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix}, c = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$.\n    2.  **Case PI (primal infeasible)**: $A = \\begin{bmatrix} 1 & 0 \\end{bmatrix}, b = \\begin{bmatrix} -1 \\end{bmatrix}, c = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n    3.  **Case DI (dual infeasible)**: $A = \\begin{bmatrix} 1 & -1 \\end{bmatrix}, b = \\begin{bmatrix} 0 \\end{bmatrix}, c = \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}$.\n-   **Final Output Format**: A comma-separated list of integers (e.g., `[0,1,2]`).\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is scientifically sound, well-posed, and objective.\n-   **Scientifically Grounded**: The problem describes the Homogeneous Self-Dual Embedding, a standard and rigorously proven technique in the field of convex optimization and numerical methods for solving linear programs. All mathematical formulations are correct and based on fundamental principles of optimization theory, specifically interior-point methods and KKT conditions.\n-   **Well-Posed**: The problem is well-posed. The HSDE framework is specifically designed to have a strictly feasible starting point and to guarantee a solution that allows for unambiguous classification of the original LP's status. The provided test cases are constructed to demonstrate each of the three possible outcomes (feasible-optimal, primal infeasible, dual infeasible).\n-   **Objective**: The language is precise and formal. All terms are standard in the field. The objectives are quantifiable and the classification rules are based on verifiable mathematical conditions.\n-   **Completeness**: The problem is self-contained. It provides the full mathematical specification of the model, the algorithm's core components (Newton step, backtracking), the termination and classification logic, and a complete set of test data.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. We will proceed with providing a solution.\n\n### Algorithmic Derivation and Implementation Plan\n\nThe core of an interior-point method is to iteratively solve a linearized version of the perturbed Karush-Kuhn-Tucker (KKT) conditions, which define a \"central path\" that converges to the solution. For the HSDE, the KKT conditions are augmented with a barrier term parametrized by $\\mu$. The goal is to find a Newton step $(\\Delta x, \\Delta y, \\Delta s, \\Delta \\tau, \\Delta \\kappa)$ that moves the current iterate closer to a point on the central path with a smaller barrier parameter.\n\nThe system of equations to be solved at each iteration is a linearization of the HSDE feasibility and centrality conditions:\n1.  $A(x + \\Delta x) - b(\\tau + \\Delta \\tau) = 0 \\implies A\\Delta x - b\\Delta\\tau = -r_p$\n2.  $A^\\top(y + \\Delta y) + (s + \\Delta s) - c(\\tau + \\Delta \\tau) = 0 \\implies A^\\top\\Delta y + \\Delta s - c\\Delta\\tau = -r_d$\n3.  $c^\\top(x + \\Delta x) - b^\\top(y + \\Delta y) + (\\kappa + \\Delta \\kappa) = 0 \\implies c^\\top\\Delta x - b^\\top\\Delta y + \\Delta \\kappa = -r_g$\n4.  $(X + \\Delta X)(S + \\Delta S)e = \\sigma\\mu e \\implies S\\Delta x + X\\Delta s = \\sigma\\mu e - XSe$\n5.  $(\\tau + \\Delta \\tau)(\\kappa + \\Delta \\kappa) = \\sigma\\mu \\implies \\kappa\\Delta\\tau + \\tau\\Delta\\kappa = \\sigma\\mu - \\tau\\kappa$\n\nHere, $X$ and $S$ are diagonal matrices with elements of $x$ and $s$ respectively, $e$ is a vector of ones, and $\\sigma \\in [0, 1]$ is a centering parameter that balances progress towards feasibility/optimality (`\\sigma=0`, affine-scaling step) and centrality (`\\sigma=1`, centering step).\n\nTo solve this system efficiently, we eliminate $\\Delta s$ and $\\Delta \\kappa$:\nFrom (4): $\\Delta s = X^{-1}(\\sigma\\mu e - XSe - S\\Delta x)$\nFrom (5): $\\Delta\\kappa = \\tau^{-1}(\\sigma\\mu - \\tau\\kappa - \\kappa\\Delta\\tau)$\n\nSubstituting these into the remaining equations (1, 2, 3) yields a reduced linear system for $(\\Delta x, \\Delta y, \\Delta \\tau)$. Let's form this system:\nSubstituting $\\Delta s$ into (2):\n$A^\\top\\Delta y + X^{-1}(\\sigma\\mu e - XSe - S\\Delta x) - c\\Delta\\tau = -r_d$\n$\\implies -X^{-1}S\\Delta x + A^\\top\\Delta y - c\\Delta\\tau = -r_d - X^{-1}(\\sigma\\mu e - XSe) = -r_d - s + \\sigma\\mu X^{-1}e$\n\nSubstituting $\\Delta \\kappa$ into (3):\n$c^\\top\\Delta x - b^\\top\\Delta y + \\tau^{-1}(\\sigma\\mu - \\tau\\kappa - \\kappa\\Delta\\tau) = -r_g$\n$\\implies c^\\top\\Delta x - b^\\top\\Delta y - (\\kappa/\\tau)\\Delta\\tau = -r_g - \\tau^{-1}(\\sigma\\mu - \\tau\\kappa) = -r_g - \\sigma\\mu/\\tau + \\kappa$\n\nThis gives the following $(n+m+1) \\times (n+m+1)$ linear system:\n$$\n\\begin{pmatrix}\n    -X^{-1}S & A^\\top & -c \\\\\n    A & 0 & -b \\\\\n    c^\\top & -b^\\top & -\\kappa/\\tau\n\\end{pmatrix}\n\\begin{pmatrix}\n    \\Delta x \\\\ \\Delta y \\\\ \\Delta \\tau\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n    -r_d + s - \\sigma \\mu X^{-1} e \\\\\n    -r_p \\\\\n    -r_g - \\sigma \\mu/\\tau + \\kappa\n\\end{pmatrix}\n$$\nThis system can be solved using a standard linear solver. Once $(\\Delta x, \\Delta y, \\Delta \\tau)$ are found, $(\\Delta s, \\Delta \\kappa)$ are computed from the substitution formulas.\n\nA backtracking line search is used to find a step size $\\alpha \\in (0, 1]$ that ensures the non-negativity constraints ($x, s, \\tau, \\kappa > 0$) are strictly maintained. The new iterate is then $(x, y, s, \\tau, \\kappa) \\leftarrow (x, y, s, \\tau, \\kappa) + \\alpha (\\Delta x, \\Delta y, \\Delta s, \\Delta \\tau, \\Delta \\kappa)$.\n\nThe algorithm proceeds as follows:\n1.  Initialize with a strictly positive point, e.g., $x=e, s=e, y=0, \\tau=1, \\kappa=1$.\n2.  Iterate until the complementarity gap $\\mu < \\text{TOL}$:\n    a. Calculate residuals $r_p, r_d, r_g$.\n    b. Calculate the gap $\\mu$.\n    c. Form and solve the reduced Newton system for $(\\Delta x, \\Delta y, \\Delta \\tau)$.\n    d. Compute $\\Delta s$ and $\\Delta \\kappa$.\n    e. Find the maximum step size $\\alpha$ that maintains positivity, scaled by a factor $\\eta < 1$.\n    f. Update all variables.\n3.  Upon termination, classify the result based on the final values of $\\tau$ and $\\kappa$:\n    - If $\\tau > \\kappa$ (approximating $\\tau > 0, \\kappa \\approx 0$), the original LP is feasible-optimal (code 0).\n    - If $\\tau \\le \\kappa$ (approximating $\\tau \\approx 0, \\kappa > 0$), the original LP is infeasible.\n        - If $b^\\top y < 0$, it is primal infeasible (code 1).\n        - Otherwise, it is dual infeasible (primal unbounded, code 2).\n\nThis procedure is implemented below for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define and solve the test cases using the HSDE-IPM.\n    \"\"\"\n\n    def _solve_lp_hsde(A, b, c):\n        \"\"\"\n        Solves a linear program standard form using a Homogeneous Self-Dual\n        Embedding Interior-Point Method.\n        \n        Args:\n            A (np.ndarray): The m x n constraint matrix.\n            b (np.ndarray): The m x 1 constraint vector.\n            c (np.ndarray): The n x 1 objective vector.\n            \n        Returns:\n            int: 0 for feasible-optimal, 1 for primal infeasible, \n                 2 for dual infeasible.\n        \"\"\"\n        m, n = A.shape\n\n        # --- Parameters ---\n        MAX_ITER = 100\n        TOL = 1e-9  # Tolerance for complementarity gap\n        SIGMA = 0.1  # Centering parameter\n        ETA = 0.9995 # Backtracking step-size factor\n\n        # --- Initialization ---\n        x = np.ones(n)\n        s = np.ones(n)\n        y = np.zeros(m)\n        tau = 1.0\n        kappa = 1.0\n\n        for _ in range(MAX_ITER):\n            # --- Calculate Residuals and Gap ---\n            r_p = A @ x - b * tau\n            r_d = A.T @ y + s - c * tau\n            r_g = c @ x - b @ y + kappa\n            \n            mu = (x @ s + tau * kappa) / (n + 1)\n\n            if mu < TOL:\n                break\n\n            # --- Form the Newton System M*dz = R ---\n            # Matrix M\n            M_size = n + m + 1\n            M = np.zeros((M_size, M_size))\n            \n            D_inv_sq = -s / x\n            \n            M[:n, :n] = np.diag(D_inv_sq)\n            M[:n, n:n+m] = A.T\n            M[:n, n+m] = -c\n            \n            M[n:n+m, :n] = A\n            # M[n:n+m, n:n+m] is already zero\n            M[n:n+m, n+m] = -b\n            \n            M[n+m, :n] = c\n            M[n+m, n:n+m] = -b\n            M[n+m, n+m] = -kappa / tau\n\n            # RHS vector R\n            R = np.zeros(M_size)\n            R[:n] = -r_d + s - SIGMA * mu / x\n            R[n:n+m] = -r_p\n            R[n+m] = -r_g - (SIGMA * mu / tau) + kappa\n\n            # --- Solve the linear system for the step direction ---\n            try:\n                # Solve for primary step components\n                sol = np.linalg.solve(M, R)\n                dx = sol[:n]\n                dy = sol[n:n+m]\n                dtau = sol[n+m]\n                \n                # Compute remaining step components\n                ds = (SIGMA * mu - x * s - s * dx) / x\n                dkappa = (SIGMA * mu - tau * kappa - kappa * dtau) / tau\n            except np.linalg.LinAlgError:\n                # If matrix is singular, likely numerical issues. Stop iterating.\n                break\n\n            # --- Backtracking Line Search ---\n            alpha = 1.0\n            \n            neg_dx_indices = dx < -1e-12\n            if np.any(neg_dx_indices):\n                alpha = min(alpha, np.min(-x[neg_dx_indices] / dx[neg_dx_indices]))\n            \n            neg_ds_indices = ds < -1e-12\n            if np.any(neg_ds_indices):\n                alpha = min(alpha, np.min(-s[neg_ds_indices] / ds[neg_ds_indices]))\n\n            if dtau < -1e-12:\n                alpha = min(alpha, -tau / dtau)\n            \n            if dkappa < -1e-12:\n                alpha = min(alpha, -kappa / dkappa)\n\n            alpha *= ETA\n            \n            # --- Update variables ---\n            x += alpha * dx\n            y += alpha * dy\n            s += alpha * ds\n            tau += alpha * dtau\n            kappa += alpha * dkappa\n\n        # --- Classification ---\n        if tau > kappa:\n            # Feasible-Optimal case: tau > 0, kappa ~ 0\n            return 0\n        else:\n            # Infeasible case: tau ~ 0, kappa > 0\n            # A certificate for primal infeasibility is y s.t. A'y >= 0 and b'y < 0\n            if b @ y < 0:\n                return 1 # Primal Infeasible\n            else:\n                # Otherwise, it must be dual infeasible (primal unbounded)\n                # A certificate is x s.t. Ax = 0, x >= 0, c'x < 0\n                return 2 # Dual Infeasible\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Feasible-optimal\n        (\n            np.array([[1.0, 2.0], [1.0, -1.0]]),\n            np.array([4.0, 1.0]),\n            np.array([1.0, 2.0])\n        ),\n        # Case 2: Primal infeasible\n        (\n            np.array([[1.0, 0.0]]),\n            np.array([-1.0]),\n            np.array([1.0, 1.0])\n        ),\n        # Case 3: Dual infeasible (primal unbounded)\n        (\n            np.array([[1.0, -1.0]]),\n            np.array([0.0]),\n            np.array([-1.0, -1.0])\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        A, b, c = case\n        result = _solve_lp_hsde(A, b, c)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3242648"}]}