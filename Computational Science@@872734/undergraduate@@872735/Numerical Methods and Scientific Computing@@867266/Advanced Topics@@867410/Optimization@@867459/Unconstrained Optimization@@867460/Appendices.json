{"hands_on_practices": [{"introduction": "Many problems in science and engineering can be framed as finding the \"best fit\" for a set of data or a known function. This practice explores this idea using the powerful principle of least-squares, a cornerstone of data fitting and function approximation. You will derive the optimal coefficients for a polynomial that best approximates the sine function, not by using a pre-packaged library, but by working from the fundamental definition of minimizing an error functional [@problem_id:3285131]. This exercise connects the abstract concept of optimization to a concrete application, reinforcing the core calculus principle that the gradient must be zero at a minimum.", "problem": "Consider the real vector space of square-integrable functions on a finite interval, with inner product defined by $\\langle f, g \\rangle = \\int_{-\\pi}^{\\pi} f(x)\\,g(x)\\,dx$. The least-squares approximation of a target function is defined to be the element in a finite-dimensional subspace that minimizes the squared norm of the residual under this inner product. Let $p(x) = a_{0} + a_{1} x + a_{2} x^{2} + a_{3} x^{3}$ be a polynomial of degree at most $3$. Using only the fundamental definition of least squares minimization (that the minimizer nulls all first-order directional derivatives of the squared error functional) and the inner product specified above, derive the stationarity conditions for the coefficients $\\{a_{0}, a_{1}, a_{2}, a_{3}\\}$ when $p(x)$ approximates $\\sin(x)$ on $[-\\pi,\\pi]$ with $x$ interpreted in radians. Then, solve these conditions exactly to obtain the optimal coefficients. Express your final answer as exact symbolic expressions for $(a_{0}, a_{1}, a_{2}, a_{3})$; do not round or approximate. The final answer should be reported as a single row matrix in the order $(a_{0}, a_{1}, a_{2}, a_{3})$.", "solution": "The problem asks for the least-squares approximation of the function $f(x) = \\sin(x)$ on the interval $[-\\pi, \\pi]$ by a polynomial $p(x) = a_{0} + a_{1} x + a_{2} x^{2} + a_{3} x^{3}$ of degree at most $3$. The vector space is that of square-integrable functions on $[-\\pi, \\pi]$ with the inner product $\\langle f, g \\rangle = \\int_{-\\pi}^{\\pi} f(x)\\,g(x)\\,dx$.\n\nThe objective is to find the coefficients $\\{a_{0}, a_{1}, a_{2}, a_{3}\\}$ that minimize the squared norm of the residual, which is the squared error functional $E$. The residual is $r(x) = \\sin(x) - p(x)$. The squared error is given by:\n$$E(a_{0}, a_{1}, a_{2}, a_{3}) = \\|r(x)\\|^2 = \\langle \\sin(x) - p(x), \\sin(x) - p(x) \\rangle$$\n$$E = \\int_{-\\pi}^{\\pi} \\left( \\sin(x) - (a_{0} + a_{1} x + a_{2} x^{2} + a_{3} x^{3}) \\right)^2 dx$$\n\nThe problem requires using the fundamental definition of minimization, which states that for the optimal set of coefficients, the first-order variation of $E$ must be zero. This is equivalent to setting the partial derivatives of $E$ with respect to each coefficient $a_k$ to zero.\nThe stationarity conditions are $\\frac{\\partial E}{\\partial a_k} = 0$ for $k \\in \\{0, 1, 2, 3\\}$.\n\nUsing the Leibniz integral rule to differentiate under the integral sign:\n$$\\frac{\\partial E}{\\partial a_k} = \\int_{-\\pi}^{\\pi} \\frac{\\partial}{\\partial a_k} \\left( \\sin(x) - \\sum_{j=0}^{3} a_j x^j \\right)^2 dx$$\n$$\\frac{\\partial E}{\\partial a_k} = \\int_{-\\pi}^{\\pi} 2 \\left( \\sin(x) - \\sum_{j=0}^{3} a_j x^j \\right) \\left( - \\frac{\\partial}{\\partial a_k} \\sum_{j=0}^{3} a_j x^j \\right) dx$$\nSince $\\frac{\\partial}{\\partial a_k} \\sum_{j=0}^{3} a_j x^j = x^k$, we have:\n$$\\frac{\\partial E}{\\partial a_k} = -2 \\int_{-\\pi}^{\\pi} (\\sin(x) - p(x)) x^k dx = 0$$\nThis implies that the residual $\\sin(x) - p(x)$ must be orthogonal to each basis function $x^k$ of the polynomial subspace:\n$$\\langle \\sin(x) - p(x), x^k \\rangle = 0 \\quad \\text{for } k=0, 1, 2, 3$$\nBy the linearity of the inner product, this leads to the normal equations:\n$$\\langle p(x), x^k \\rangle = \\langle \\sin(x), x^k \\rangle$$\nSubstituting $p(x) = \\sum_{j=0}^{3} a_j x^j$:\n$$\\left\\langle \\sum_{j=0}^{3} a_j x^j, x^k \\right\\rangle = \\sum_{j=0}^{3} a_j \\langle x^j, x^k \\rangle = \\langle \\sin(x), x^k \\rangle$$\nThis is a system of $4$ linear equations for the $4$ unknown coefficients $a_j$. In matrix form, this is $G \\mathbf{a} = \\mathbf{b}$, where $\\mathbf{a} = [a_0, a_1, a_2, a_3]^T$, and the entries of the Gram matrix $G$ and vector $\\mathbf{b}$ are:\n$$G_{kj} = \\langle x^k, x^j \\rangle = \\int_{-\\pi}^{\\pi} x^{k+j} dx$$\n$$b_k = \\langle \\sin(x), x^k \\rangle = \\int_{-\\pi}^{\\pi} x^k \\sin(x) dx$$\n\nWe now compute the components of $G$ and $\\mathbf{b}$. The interval of integration $[-\\pi, \\pi]$ is symmetric about $x=0$.\nFor the matrix $G$, the integrand is $x^{k+j}$. If the exponent $k+j$ is odd, the integrand is an odd function, and its integral over a symmetric interval is $0$. If $k+j$ is even, the integrand is an even function:\n$$G_{kj} = \\int_{-\\pi}^{\\pi} x^{k+j} dx = \\begin{cases} 0  \\text{if } k+j \\text{ is odd} \\\\ 2 \\int_0^{\\pi} x^{k+j} dx = \\frac{2\\pi^{k+j+1}}{k+j+1}  \\text{if } k+j \\text{ is even} \\end{cases}$$\nThis results in a block-diagonal matrix:\n$G_{00} = 2\\pi$, $G_{01}=0$, $G_{02}=\\frac{2\\pi^3}{3}$, $G_{03}=0$\n$G_{11}=\\frac{2\\pi^3}{3}$, $G_{12}=0$, $G_{13}=\\frac{2\\pi^5}{5}$\n$G_{22}=\\frac{2\\pi^5}{5}$, $G_{23}=0$\n$G_{33}=\\frac{2\\pi^7}{7}$\n$$G = \\begin{pmatrix} 2\\pi  0  \\frac{2\\pi^3}{3}  0 \\\\ 0  \\frac{2\\pi^3}{3}  0  \\frac{2\\pi^5}{5} \\\\ \\frac{2\\pi^3}{3}  0  \\frac{2\\pi^5}{5}  0 \\\\ 0  \\frac{2\\pi^5}{5}  0  \\frac{2\\pi^7}{7} \\end{pmatrix}$$\nFor the vector $\\mathbf{b}$, the integrand is $x^k \\sin(x)$. The function $\\sin(x)$ is odd. If $k$ is even, $x^k$ is even, so the product $x^k \\sin(x)$ is odd, and its integral over $[-\\pi, \\pi]$ is $0$. Therefore, $b_0=0$ and $b_2=0$. If $k$ is odd, $x^k$ is odd, so the product $x^k \\sin(x)$ is even.\n$$b_k = \\int_{-\\pi}^{\\pi} x^k \\sin(x) dx = \\begin{cases} 0  \\text{if } k \\text{ is even} \\\\ 2 \\int_0^{\\pi} x^k \\sin(x) dx  \\text{if } k \\text{ is odd} \\end{cases}$$\nFor $k=1$:\n$b_1 = 2 \\int_0^{\\pi} x \\sin(x) dx = 2 [-x \\cos(x) + \\sin(x)]_0^{\\pi} = 2(-\\pi \\cos(\\pi)) = 2\\pi$.\nFor $k=3$:\n$b_3 = 2 \\int_0^{\\pi} x^3 \\sin(x) dx$. Using integration by parts repeatedly:\n$b_3 = 2 [-x^3 \\cos(x) + 3x^2 \\sin(x) + 6x \\cos(x) - 6 \\sin(x)]_0^{\\pi} = 2(-\\pi^3 \\cos(\\pi) + 6\\pi \\cos(\\pi)) = 2(\\pi^3 - 6\\pi) = 2\\pi^3 - 12\\pi$.\nSo, the vector $\\mathbf{b}$ is $\\mathbf{b} = [0, 2\\pi, 0, 2\\pi^3-12\\pi]^T$.\n\nThe system of equations $G \\mathbf{a} = \\mathbf{b}$ is:\n$$\\begin{pmatrix} 2\\pi  0  \\frac{2\\pi^3}{3}  0 \\\\ 0  \\frac{2\\pi^3}{3}  0  \\frac{2\\pi^5}{5} \\\\ \\frac{2\\pi^3}{3}  0  \\frac{2\\pi^5}{5}  0 \\\\ 0  \\frac{2\\pi^5}{5}  0  \\frac{2\\pi^7}{7} \\end{pmatrix} \\begin{pmatrix} a_0 \\\\ a_1 \\\\ a_2 \\\\ a_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2\\pi \\\\ 0 \\\\ 2\\pi^3 - 12\\pi \\end{pmatrix}$$\nThis decouples into two independent $2 \\times 2$ systems.\n\nSystem 1 (for even coefficients $a_0, a_2$):\n$$\\begin{pmatrix} 2\\pi  \\frac{2\\pi^3}{3} \\\\ \\frac{2\\pi^3}{3}  \\frac{2\\pi^5}{5} \\end{pmatrix} \\begin{pmatrix} a_0 \\\\ a_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nThe determinant of this matrix is $(2\\pi)(\\frac{2\\pi^5}{5}) - (\\frac{2\\pi^3}{3})^2 = \\frac{4\\pi^6}{5} - \\frac{4\\pi^6}{9} = \\frac{16\\pi^6}{45} \\neq 0$. Since the matrix is invertible, the only solution is the trivial one: $a_0 = 0$ and $a_2 = 0$. This is expected, as the target function $\\sin(x)$ is odd, and the optimal approximation on a symmetric interval using a basis of monomials will only use the odd-powered basis functions.\n\nSystem 2 (for odd coefficients $a_1, a_3$):\n$$\\begin{pmatrix} \\frac{2\\pi^3}{3}  \\frac{2\\pi^5}{5} \\\\ \\frac{2\\pi^5}{5}  \\frac{2\\pi^7}{7} \\end{pmatrix} \\begin{pmatrix} a_1 \\\\ a_3 \\end{pmatrix} = \\begin{pmatrix} 2\\pi \\\\ 2\\pi^3 - 12\\pi \\end{pmatrix}$$\nWe can simplify by dividing the first row by $2\\pi$ and the second by $2\\pi$:\n$$\\begin{pmatrix} \\frac{\\pi^2}{3}  \\frac{\\pi^4}{5} \\\\ \\frac{\\pi^4}{5}  \\frac{\\pi^6}{7} \\end{pmatrix} \\begin{pmatrix} a_1 \\\\ a_3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\pi^2 - 6 \\end{pmatrix}$$\nLet's solve for $a_1$ and $a_3$ using Cramer's rule. The determinant of the coefficient matrix is $D = (\\frac{\\pi^2}{3})(\\frac{\\pi^6}{7}) - (\\frac{\\pi^4}{5})(\\frac{\\pi^4}{5}) = \\frac{\\pi^8}{21} - \\frac{\\pi^8}{25} = \\pi^8(\\frac{25-21}{525}) = \\frac{4\\pi^8}{525}$.\n$$a_1 = \\frac{1}{D} \\begin{vmatrix} 1  \\frac{\\pi^4}{5} \\\\ \\pi^2-6  \\frac{\\pi^6}{7} \\end{vmatrix} = \\frac{525}{4\\pi^8} \\left( \\frac{\\pi^6}{7} - \\frac{\\pi^4}{5}(\\pi^2-6) \\right)$$\n$$a_1 = \\frac{525}{4\\pi^8} \\left( \\frac{5\\pi^6 - 7\\pi^4(\\pi^2-6)}{35} \\right) = \\frac{15}{4\\pi^8} (5\\pi^6 - 7\\pi^6 + 42\\pi^4) = \\frac{15}{4\\pi^8} (42\\pi^4 - 2\\pi^6)$$\n$$a_1 = \\frac{15 \\cdot 2\\pi^4(21 - \\pi^2)}{4\\pi^8} = \\frac{15(21 - \\pi^2)}{2\\pi^4}$$\n$$a_3 = \\frac{1}{D} \\begin{vmatrix} \\frac{\\pi^2}{3}  1 \\\\ \\frac{\\pi^4}{5}  \\pi^2-6 \\end{vmatrix} = \\frac{525}{4\\pi^8} \\left( \\frac{\\pi^2}{3}(\\pi^2-6) - \\frac{\\pi^4}{5} \\right)$$\n$$a_3 = \\frac{525}{4\\pi^8} \\left( \\frac{5\\pi^2(\\pi^2-6) - 3\\pi^4}{15} \\right) = \\frac{35}{4\\pi^8} (5\\pi^4 - 30\\pi^2 - 3\\pi^4) = \\frac{35}{4\\pi^8} (2\\pi^4 - 30\\pi^2)$$\n$$a_3 = \\frac{35 \\cdot 2\\pi^2(\\pi^2 - 15)}{4\\pi^8} = \\frac{35(\\pi^2 - 15)}{2\\pi^6}$$\nThe optimal coefficients are therefore:\n$a_0 = 0$\n$a_1 = \\frac{15(21 - \\pi^2)}{2\\pi^4}$\n$a_2 = 0$\n$a_3 = \\frac{35(\\pi^2 - 15)}{2\\pi^6}$\nThe resulting best polynomial approximation is $p(x) = \\frac{15(21 - \\pi^2)}{2\\pi^4} x + \\frac{35(\\pi^2 - 15)}{2\\pi^6} x^3$.", "answer": "$$\\boxed{\\begin{pmatrix} 0  \\frac{15(21 - \\pi^2)}{2\\pi^4}  0  \\frac{35(\\pi^2 - 15)}{2\\pi^6} \\end{pmatrix}}$$", "id": "3285131"}, {"introduction": "While some optimization problems can be solved analytically by setting a gradient to zero and solving the resulting system, most require an iterative approach. This practice introduces the most fundamental iterative algorithm: Steepest Descent, also known as Gradient Descent. You will not just implement the method, but critically analyze its performance by exploring how the choice of a single hyperparameter—the step size $\\alpha$—can mean the difference between rapid convergence, slow progress, or even catastrophic divergence [@problem_id:3285028]. By analytically calculating the convergence rate, this exercise provides deep insight into the theoretical principles that govern the stability and efficiency of numerical algorithms.", "problem": "Consider unconstrained minimization of a twice continuously differentiable objective over $\\mathbb{R}^n$. A fundamental case in numerical methods is the strictly convex quadratic objective, \n$$\nf(x) = \\tfrac{1}{2} x^{\\top} Q x - b^{\\top} x,\n$$\nwhere $Q \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and $b \\in \\mathbb{R}^{n}$. The unique minimizer $x^{\\star}$ satisfies $Q x^{\\star} = b$. The Steepest Descent (SD) method with a fixed step size $\\alpha  0$ updates \n$$\nx_{k+1} = x_k - \\alpha \\nabla f(x_k),\n$$\nwith $\\nabla f(x) = Q x - b$.\n\nThe analytical behavior of SD with a fixed step size $\\alpha$ can exhibit fast convergence, slow convergence, or divergence. The classification in this problem must be grounded in the asymptotic linear rate of the error sequence. Define the error $e_k = x_k - x^{\\star}$ and the asymptotic linear rate \n$$\nr_{\\mathrm{asym}}(\\alpha) = \\limsup_{k \\to \\infty} \\frac{\\|e_{k+1}\\|_2}{\\|e_k\\|_2}.\n$$\nClassify the behavior for each test case using the following rule:\n- If $r_{\\mathrm{asym}}(\\alpha) \\ge 1$ (including the boundary case $r_{\\mathrm{asym}}(\\alpha) = 1$), return the integer $0$ to indicate divergence or non-convergence.\n- If $0.95 \\le r_{\\mathrm{asym}}(\\alpha)  1$, return the integer $1$ to indicate slow convergence.\n- If $0 \\le r_{\\mathrm{asym}}(\\alpha)  0.95$, return the integer $2$ to indicate acceptable convergence speed.\n\nYour program must:\n- Implement the SD iteration with fixed $\\alpha$ for the given quadratic instances, starting at the specified initial point $x_0$, and run for $N$ iterations.\n- Compute $x^{\\star}$ by solving $Q x^{\\star} = b$ and empirically observe the error norms $\\|e_k\\|_2$.\n- Justify the classification of each test case using a mathematically principled criterion consistent with the asymptotic linear rate definition (for example, by analyzing the induced linear iteration on the error and its spectral properties).\n- Produce a single line of output containing the classification results for all test cases as a comma-separated list enclosed in square brackets (e.g., $[0,1,2]$). Each list element must be an integer as defined above.\n\nThere are no physical units and no angles in this problem. All numerical thresholds (such as $0.95$) must be treated as decimals.\n\nTest suite (each test provides $(Q, b, x_0, \\alpha, N)$):\n1. $Q = \\operatorname{diag}(1, 100)$, $b = (1, 1)^{\\top}$, $x_0 = (10, -10)^{\\top}$, $\\alpha = 0.05$, $N = 50$.\n2. $Q = \\operatorname{diag}(1, 100)$, $b = (1, 1)^{\\top}$, $x_0 = (10, -10)^{\\top}$, $\\alpha = 0.02$, $N = 50$.\n3. $Q = \\operatorname{diag}(1, 100)$, $b = (1, 1)^{\\top}$, $x_0 = (10, -10)^{\\top}$, $\\alpha = 0.019$, $N = 200$.\n4. $Q = \\operatorname{diag}(2, 2)$, $b = (1, 1)^{\\top}$, $x_0 = (10, -10)^{\\top}$, $\\alpha = 0.2$, $N = 50$.\n5. $Q = \\operatorname{diag}(0.2, 0.1)$, $b = (1, 1)^{\\top}$, $x_0 = (10, -10)^{\\top}$, $\\alpha = 12$, $N = 50$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3,result4,result5]$).", "solution": "The problem requires the classification of the convergence behavior of the Steepest Descent (SD) method. The method is applied to an unconstrained minimization problem with a strictly convex quadratic objective function $f(x) = \\tfrac{1}{2} x^{\\top} Q x - b^{\\top} x$, where $x \\in \\mathbb{R}^n$, $Q \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD), and $b \\in \\mathbb{R}^{n}$. The classification must be based on the asymptotic linear rate of convergence, $r_{\\mathrm{asym}}(\\alpha)$, for a given fixed step size $\\alpha  0$.\n\nThe SD iteration is defined by the update rule $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$. For the given quadratic objective, the gradient is $\\nabla f(x) = Qx - b$. Substituting this into the iteration formula yields:\n$$\nx_{k+1} = x_k - \\alpha(Qx_k - b) = (I - \\alpha Q)x_k + \\alpha b\n$$\nwhere $I$ is the identity matrix of appropriate dimension.\n\nThe unique minimizer of $f(x)$, denoted $x^{\\star}$, is the solution to the linear system $\\nabla f(x^\\star) = Qx^{\\star} - b = 0$, which implies $Qx^{\\star} = b$. The error at iteration $k$ is defined as the vector $e_k = x_k - x^{\\star}$. We can establish a recursive relationship for the error vector:\n$$\ne_{k+1} = x_{k+1} - x^{\\star} = \\left( (I - \\alpha Q)x_k + \\alpha b \\right) - x^{\\star}\n$$\nSubstituting $b = Qx^{\\star}$, we can write $\\alpha b = \\alpha Qx^{\\star}$. This allows us to express the error recurrence as:\n$$\ne_{k+1} = (I - \\alpha Q)x_k + \\alpha Qx^{\\star} - x^{\\star} = (I - \\alpha Q)x_k - (I - \\alpha Q)x^{\\star}\n$$\nBy factoring out the matrix $(I - \\alpha Q)$, we obtain the linear error dynamics:\n$$\ne_{k+1} = (I - \\alpha Q) e_k\n$$\nThis equation shows that the error at each step is obtained by multiplying the previous error by the fixed iteration matrix $G = I - \\alpha Q$. The long-term behavior of the magnitude of the error, $\\|e_k\\|_2$, is determined by the spectral radius of this iteration matrix, $\\rho(G)$, which is the maximum absolute value among its eigenvalues. The asymptotic linear rate of convergence is precisely this spectral radius:\n$$\nr_{\\mathrm{asym}}(\\alpha) = \\limsup_{k \\to \\infty} \\frac{\\|e_{k+1}\\|_2}{\\|e_k\\|_2} = \\rho(G) = \\rho(I - \\alpha Q)\n$$\nSince the matrix $Q$ is symmetric, it is diagonalizable and its eigenvalues, denoted $\\lambda_i$, are all real. As $Q$ is also positive definite, all its eigenvalues are positive, i.e., $\\lambda_i  0$. The eigenvalues of the matrix $G = I - \\alpha Q$ are given by $\\mu_i = 1 - \\alpha \\lambda_i$. The spectral radius of $G$ is therefore:\n$$\n\\rho(G) = \\max_i |\\mu_i| = \\max_i |1 - \\alpha \\lambda_i|\n$$\nFor a fixed $\\alpha  0$, the function $g(\\lambda) = |1 - \\alpha \\lambda|$ is a V-shaped function whose value depends on the distance of $\\lambda$ from $1/\\alpha$. The maximum of this function over the set of eigenvalues of $Q$ will be attained at either the smallest eigenvalue, $\\lambda_{\\min}$, or the largest eigenvalue, $\\lambda_{\\max}$. Thus, the asymptotic rate can be computed directly using only the extremal eigenvalues of $Q$:\n$$\nr_{\\mathrm{asym}}(\\alpha) = \\max \\left( |1 - \\alpha \\lambda_{\\min}|, |1 - \\alpha \\lambda_{\\max}| \\right)\n$$\nThis formula provides an exact, analytical value for the asymptotic rate, which is superior to empirical estimation from a finite number of iterations ($N$). The convergence classification relies solely on $Q$ and $\\alpha$; the initial point $x_0$ and number of iterations $N$ are irrelevant for determining the asymptotic rate. The condition for convergence is $r_{\\mathrm{asym}}(\\alpha)  1$, which holds if and only if $0  \\alpha  2/\\lambda_{\\max}$.\n\nWe now apply this analytical result to each test case.\n\nCase 1: $Q = \\operatorname{diag}(1, 100)$, $\\alpha = 0.05$.\nThe eigenvalues of the diagonal matrix $Q$ are its diagonal entries. Thus, $\\lambda_{\\min} = 1$ and $\\lambda_{\\max} = 100$.\n$r_{\\mathrm{asym}}(0.05) = \\max(|1 - (0.05)(1)|, |1 - (0.05)(100)|) = \\max(|1 - 0.05|, |1 - 5|) = \\max(0.95, 4.0) = 4.0$.\nSince $r_{\\mathrm{asym}}(\\alpha) = 4.0 \\ge 1$, the method diverges. The classification is $0$.\n\nCase 2: $Q = \\operatorname{diag}(1, 100)$, $\\alpha = 0.02$.\nThe eigenvalues are $\\lambda_{\\min} = 1$ and $\\lambda_{\\max} = 100$. The step size $\\alpha=0.02$ is exactly at the stability boundary, $\\alpha = 2/\\lambda_{\\max} = 2/100 = 0.02$.\n$r_{\\mathrm{asym}}(0.02) = \\max(|1 - (0.02)(1)|, |1 - (0.02)(100)|) = \\max(|1 - 0.02|, |1 - 2|) = \\max(0.98, 1.0) = 1.0$.\nSince $r_{\\mathrm{asym}}(\\alpha) = 1.0 \\ge 1$, the method fails to converge. The classification is $0$.\n\nCase 3: $Q = \\operatorname{diag}(1, 100)$, $\\alpha = 0.019$.\nThe eigenvalues are $\\lambda_{\\min} = 1$ and $\\lambda_{\\max} = 100$.\n$r_{\\mathrm{asym}}(0.019) = \\max(|1 - (0.019)(1)|, |1 - (0.019)(100)|) = \\max(|1 - 0.019|, |1 - 1.9|) = \\max(0.981, 0.9) = 0.981$.\nSince $0.95 \\le 0.981  1$, the convergence is classified as slow. The classification is $1$.\n\nCase 4: $Q = \\operatorname{diag}(2, 2)$, $\\alpha = 0.2$.\nThe eigenvalues are identical: $\\lambda_{\\min} = \\lambda_{\\max} = 2$.\n$r_{\\mathrm{asym}}(0.2) = |1 - (0.2)(2)| = |1 - 0.4| = 0.6$.\nSince $0 \\le 0.6  0.95$, the convergence is classified as acceptable. The classification is $2$.\n\nCase 5: $Q = \\operatorname{diag}(0.2, 0.1)$, $\\alpha = 12$.\nThe eigenvalues are $\\lambda_{\\min} = 0.1$ and $\\lambda_{\\max} = 0.2$. The stability boundary is $2/\\lambda_{\\max} = 2/0.2 = 10$. The step size $\\alpha = 12$ is outside this range.\n$r_{\\mathrm{asym}}(12) = \\max(|1 - (12)(0.1)|, |1 - (12)(0.2)|) = \\max(|1 - 1.2|, |1 - 2.4|) = \\max(0.2, 1.4) = 1.4$.\nSince $r_{\\mathrm{asym}}(\\alpha) = 1.4 \\ge 1$, the method diverges. The classification is $0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the convergence classification for several Steepest Descent scenarios\n    on a quadratic objective function. The classification is based on the analytical\n    asymptotic convergence rate derived from the spectral properties of the iteration matrix.\n    \"\"\"\n    \n    # Test suite format: (Q, b, x0, alpha, N)\n    test_cases = [\n        (np.diag([1.0, 100.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.05, 50),\n        (np.diag([1.0, 100.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.02, 50),\n        (np.diag([1.0, 100.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.019, 200),\n        (np.diag([2.0, 2.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.2, 50),\n        (np.diag([0.2, 0.1]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 12.0, 50),\n    ]\n\n    results = []\n    for Q, b, x0, alpha, N in test_cases:\n        # The convergence classification for a quadratic objective depends only on the \n        # matrix Q and the step size alpha. The parameters b, x0, and N do not affect\n        # the asymptotic rate.\n        \n        # The analytical asymptotic rate, r_asym, is the spectral radius of the \n        # iteration matrix G = I - alpha*Q. For a symmetric matrix Q, this is:\n        # r_asym = max(|1 - alpha*lambda_min|, |1 - alpha*lambda_max|)\n        # where lambda_min and lambda_max are the minimum and maximum eigenvalues of Q.\n        \n        # For the diagonal matrices in the test cases, the eigenvalues are simply the\n        # diagonal elements.\n        eigenvalues = np.diag(Q)\n        lambda_min = np.min(eigenvalues)\n        lambda_max = np.max(eigenvalues)\n        \n        # Calculate the asymptotic convergence rate\n        rate = max(abs(1.0 - alpha * lambda_min), abs(1.0 - alpha * lambda_max))\n        \n        # Classify the behavior based on the problem's rules for the rate.\n        classification = 0 # Default: divergence or non-convergence\n        if rate  1.0:\n            if rate  0.95:\n                classification = 2 # Acceptable convergence\n            else: # 0.95 = rate  1.0\n                classification = 1 # Slow convergence\n        \n        results.append(classification)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3285028"}, {"introduction": "Steepest Descent often takes many small steps to reach a minimum. To achieve faster convergence, we can incorporate second-order information about the function's curvature using Newton's method. This practice guides you through implementing Newton's method for a specially designed function that has both local minima and a saddle point, revealing a crucial and often misunderstood aspect of the algorithm: it seeks stationary points (where the gradient is zero), which are not always minima [@problem_id:3285127]. By observing how the algorithm's trajectory changes with different starting points, you will gain invaluable intuition about the local and global behavior of second-order optimization methods.", "problem": "Consider unconstrained optimization in two dimensions. A point $x^\\star \\in \\mathbb{R}^2$ is a stationary point of a twice continuously differentiable objective $f:\\mathbb{R}^2 \\to \\mathbb{R}$ if the gradient $\\nabla f(x^\\star)$ equals $0$. A stationary point is classified by the eigenvalues of the Hessian matrix $\\nabla^2 f(x^\\star)$: positive definiteness yields a local minimum, negative definiteness yields a local maximum, and mixed signs (indefiniteness) yield a saddle point. Newton's method for unconstrained optimization arises by forming the second-order Taylor model of $f$ at a current iterate $x_k$, then choosing the step $p_k$ that makes the gradient of that quadratic model vanish at $x_k+p_k$, and updating $x_{k+1}=x_k+p_k$. In practice, one may include a backtracking line search on the norm of the gradient to encourage stable progress when the Hessian is indefinite.\n\nDesign the following two-dimensional objective function:\n$$\nf(x,y) = x^2 - y^2 + (x^2 + y^2)^2,\n$$\nwhich is twice continuously differentiable on $\\mathbb{R}^2$. Starting from the fundamental definition of a stationary point and the second-order Taylor model for $f$, derive the Newton iteration for this problem and implement it as a program that:\n- Computes the gradient $\\nabla f(x,y)$ and the Hessian $\\nabla^2 f(x,y)$ symbolically and evaluates them numerically at any $(x,y)$.\n- Applies Newton's method by solving the linear system for the step $p_k$ that nulls the gradient of the quadratic model at $x_k+p_k$ and updates the iterate with a backtracking line search on the gradient norm to avoid unstable steps when the Hessian is indefinite.\n- Terminates when the gradient norm is below a small tolerance or when the step norm is extremely small, or when a specified maximum number of iterations is reached.\n\nUsing the eigenvalues of $\\nabla^2 f$ at the converged point, classify the stationary point into one of the following categories:\n- $0$ for a saddle point (eigenvalues have mixed signs).\n- $1$ for a local minimum (both eigenvalues strictly positive).\n- $2$ for a local maximum (both eigenvalues strictly negative).\n- $3$ for failure to converge to a stationary point within the iteration limit.\n\nYour program must run the Newton method on the following test suite of starting points (each is an ordered pair $(x_0,y_0)$):\n- $(0.05,\\,0.05)$\n- $(0.05,\\,0.9)$\n- $(0.05,\\,-0.9)$\n- $(0.8,\\,0.1)$\n- $(2.0,\\,2.0)$\n- $(0.0,\\,0.0)$\n\nFor each test case, return a list containing the classification integer, the number of iterations used, and the final point coordinates $(x^\\star,y^\\star)$ rounded to six decimal places. The final output format must be a single line containing a comma-separated list of these per-case lists, enclosed in square brackets, for example:\n$$\n[\\,[c_1,n_1,x^\\star_1,y^\\star_1],\\,[c_2,n_2,x^\\star_2,y^\\star_2],\\,\\ldots\\,]\n$$\nThere are no physical units or angles in this problem. Use a gradient norm tolerance of $10^{-8}$, a step norm tolerance of $10^{-12}$, and a maximum of $100$ iterations. If the Hessian is singular or ill-conditioned at any iteration, stabilize the linear solve with a small diagonal regularization added to the Hessian, and use backtracking to reduce the gradient norm monotonically.", "solution": "The objective $f(x,y) = x^2 - y^2 + (x^2 + y^2)^2$ is a polynomial, hence twice continuously differentiable. We begin with the foundational definitions for unconstrained optimization. A stationary point $x^\\star$ satisfies $\\nabla f(x^\\star)=0$. The second-order Taylor expansion of $f$ around $x_k$ is\n$$\nm_k(p) = f(x_k) + \\nabla f(x_k)^\\top p + \\frac{1}{2} p^\\top \\nabla^2 f(x_k) p,\n$$\nwhere $p \\in \\mathbb{R}^2$ is the step from $x_k$. Newton's method is obtained by choosing $p_k$ so that the gradient of the model vanishes at $p$, namely\n$$\n\\nabla m_k(p) = \\nabla f(x_k) + \\nabla^2 f(x_k) p = 0,\n$$\nwhich leads to the linear system\n$$\n\\nabla^2 f(x_k) \\, p_k = -\\nabla f(x_k).\n$$\nThe iterate is updated by $x_{k+1}=x_k+p_k$. When $\\nabla^2 f(x_k)$ is indefinite or ill-conditioned, a pure Newton step can be unstable. A widely used stabilization is to add a small diagonal regularization $\\lambda I$ and a backtracking line search; here we monitor the norm of the gradient to ensure monotone decrease:\n$$\n\\|\\nabla f(x_k + \\alpha p_k)\\|  \\|\\nabla f(x_k)\\|, \\quad \\text{with backtracking on } \\alpha \\in (0,1].\n$$\n\nWe now derive the gradient and Hessian of $f$. Define $r^2 = x^2 + y^2$. Then\n$$\nf(x,y) = x^2 - y^2 + r^4.\n$$\nThe gradient components are\n$$\n\\frac{\\partial f}{\\partial x} = 2x + 4x r^2 = 2x + 4x(x^2 + y^2), \\quad\n\\frac{\\partial f}{\\partial y} = -2y + 4y r^2 = -2y + 4y(x^2 + y^2).\n$$\nHence\n$$\n\\nabla f(x,y) = \\begin{bmatrix} 2x + 4x(x^2 + y^2) \\\\ -2y + 4y(x^2 + y^2) \\end{bmatrix}.\n$$\nFor the Hessian, differentiate the gradient:\n$$\n\\frac{\\partial^2 f}{\\partial x^2} = 2 + 12x^2 + 4y^2, \\quad\n\\frac{\\partial^2 f}{\\partial y^2} = -2 + 4x^2 + 12y^2, \\quad\n\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x} = 8xy.\n$$\nTherefore\n$$\n\\nabla^2 f(x,y) = \\begin{bmatrix}\n2 + 12x^2 + 4y^2  8xy \\\\\n8xy  -2 + 4x^2 + 12y^2\n\\end{bmatrix}.\n$$\n\nStationary points are obtained by solving $\\nabla f(x,y)=0$. The $x$-component equation is\n$$\n2x + 4x(x^2+y^2) = 2x\\left(1 + 2(x^2+y^2)\\right) = 0,\n$$\nwhich implies either $x=0$ or $1 + 2(x^2+y^2)=0$, but the latter has no real solution, so $x=0$. The $y$-component equation is\n$$\n-2y + 4y(x^2+y^2) = 2y\\left(-1 + 2(x^2+y^2)\\right) = 0,\n$$\nwhich implies either $y=0$ or $-1 + 2(x^2+y^2) = 0$. With $x=0$, the second alternative gives $2y^2 = 1$, hence $y = \\pm \\frac{1}{\\sqrt{2}}$. Thus the stationary points are $(0,0)$, $\\left(0, \\frac{1}{\\sqrt{2}}\\right)$, and $\\left(0, -\\frac{1}{\\sqrt{2}}\\right)$.\n\nTo classify the stationary points, examine the Hessian. At $(0,0)$,\n$$\n\\nabla^2 f(0,0) = \\begin{bmatrix} 2  0 \\\\ 0  -2 \\end{bmatrix},\n$$\nwhich is indefinite (one positive and one negative eigenvalue). Therefore $(0,0)$ is a saddle point. At $\\left(0, \\pm \\frac{1}{\\sqrt{2}}\\right)$, using $x=0$ and $y^2 = \\frac{1}{2}$,\n$$\n\\nabla^2 f\\left(0, \\pm \\tfrac{1}{\\sqrt{2}}\\right) = \\begin{bmatrix} 2 + 4\\cdot \\tfrac{1}{2}  0 \\\\ 0  -2 + 12\\cdot \\tfrac{1}{2} \\end{bmatrix} = \\begin{bmatrix} 4  0 \\\\ 0  4 \\end{bmatrix},\n$$\nwhich is positive definite. Thus $\\left(0, \\pm \\frac{1}{\\sqrt{2}}\\right)$ are strict local minima.\n\nNewton's method behavior near $(0,0)$ can be analyzed via the local linearization. For small $(x,y)$, the gradient is approximately $\\nabla f(x,y) \\approx \\begin{bmatrix} 2x \\\\ -2y \\end{bmatrix}$ and the Hessian is approximately $\\nabla^2 f(x,y) \\approx \\begin{bmatrix} 2  0 \\\\ 0  -2 \\end{bmatrix}$. The Newton step $p$ solves\n$$\n\\begin{bmatrix} 2  0 \\\\ 0  -2 \\end{bmatrix} p = - \\begin{bmatrix} 2x \\\\ -2y \\end{bmatrix},\n$$\nwhich gives $p \\approx \\begin{bmatrix} -x \\\\ y \\end{bmatrix}$, hence $x_{k+1} \\approx \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ in one iteration. This shows attraction to the saddle when both coordinates are sufficiently small. Conversely, when $y$ is moderate so that $-1 + 2(x^2 + y^2)$ becomes positive, the curvature in the $y$-direction becomes strongly positive, and the Newton step combined with a backtracking line search tends to move toward the minima at $y=\\pm \\frac{1}{\\sqrt{2}}$, hence repulsion from the saddle along the unstable direction depending on the starting point. This dichotomy illustrates how Newton's method for unconstrained optimization solves $\\nabla f(x)=0$ without regard to the stationary point's type: it can be attracted to a saddle if the initial guess lies in its basin of attraction, or repelled toward minima otherwise.\n\nAlgorithmic design for the implementation:\n- Compute $\\nabla f(x,y)$ and $\\nabla^2 f(x,y)$ exactly as derived.\n- At each iteration, attempt to solve $\\nabla^2 f(x_k)\\, p_k = -\\nabla f(x_k)$. If the Hessian is singular or nearly singular, stabilize with a small diagonal $\\lambda I$ before solving.\n- Use backtracking on the step length $\\alpha$ to decrease $\\|\\nabla f(x_k + \\alpha p_k)\\|$ monotonically.\n- Terminate when $\\|\\nabla f(x_k)\\| \\le 10^{-8}$ or when $\\| \\alpha p_k \\| \\le 10^{-12}$, or after $100$ iterations.\n- Classify the converged point by the signs of the Hessian eigenvalues.\n\nExpected outcomes for the specified test suite based on the analysis:\n- Starting at $(0.05,0.05)$: attraction to the saddle $(0,0)$, classification $0$.\n- Starting at $(0.05,0.9)$: repulsion from the saddle along $y$, convergence to $\\left(0,\\tfrac{1}{\\sqrt{2}}\\right)$, classification $1$.\n- Starting at $(0.05,-0.9)$: symmetric to the previous case, convergence to $\\left(0,-\\tfrac{1}{\\sqrt{2}}\\right)$, classification $1$.\n- Starting at $(0.8,0.1)$: attraction to the saddle via reduction of both coordinates, classification $0$.\n- Starting at $(2.0,2.0)$: repulsion from the saddle along $y$, convergence to a local minimum near $\\left(0,\\tfrac{1}{\\sqrt{2}}\\right)$, classification $1$.\n- Starting at $(0.0,0.0)$: immediate recognition of a stationary saddle, classification $0$.\n\nThe program will report, for each case, the classification integer, the iteration count, and the final $(x^\\star,y^\\star)$ with six decimal places, in the single-line format prescribed.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f(xy: np.ndarray) - float:\n    x, y = xy\n    r2 = x*x + y*y\n    return x*x - y*y + r2*r2\n\ndef grad(xy: np.ndarray) - np.ndarray:\n    x, y = xy\n    r2 = x*x + y*y\n    return np.array([2.0*x + 4.0*x*r2, -2.0*y + 4.0*y*r2], dtype=float)\n\ndef hess(xy: np.ndarray) - np.ndarray:\n    x, y = xy\n    h11 = 2.0 + 12.0*x*x + 4.0*y*y\n    h22 = -2.0 + 4.0*x*x + 12.0*y*y\n    h12 = 8.0*x*y\n    return np.array([[h11, h12], [h12, h22]], dtype=float)\n\ndef newton_optimize(x0: tuple,\n                    tol_grad: float = 1e-8,\n                    tol_step: float = 1e-12,\n                    max_iter: int = 100) - tuple[np.ndarray, int, bool]:\n    x = np.array([float(x0[0]), float(x0[1])], dtype=float)\n    iters = 0\n    for k in range(max_iter):\n        g = grad(x)\n        normg = float(np.linalg.norm(g))\n        if normg = tol_grad:\n            return x, k, True\n        H = hess(x)\n\n        # Stabilize Hessian if singular or ill-conditioned\n        # Try small diagonal regularization if needed.\n        lam = 0.0\n        p = None\n        for attempt in range(6):\n            try:\n                if lam  0.0:\n                    Hp = H + lam * np.eye(2)\n                else:\n                    Hp = H\n                p = np.linalg.solve(Hp, -g)\n                break\n            except np.linalg.LinAlgError:\n                lam = 1e-8 if lam == 0.0 else lam * 10.0\n                continue\n        if p is None:\n            return x, k, False\n\n        # Backtracking line search to reduce gradient norm\n        alpha = 1.0\n        accepted = False\n        for _ in range(20):\n            x_new = x + alpha * p\n            if np.linalg.norm(grad(x_new))  normg:\n                x = x_new\n                accepted = True\n                break\n            alpha *= 0.5\n        if not accepted:\n            # Even if not strictly decreasing, take a very small step to avoid stagnation\n            x = x + alpha * p\n\n        step_norm = float(np.linalg.norm(alpha * p))\n        iters = k + 1\n        if step_norm = tol_step:\n            # If step is tiny, check convergence by gradient\n            if float(np.linalg.norm(grad(x))) = tol_grad:\n                return x, iters, True\n            else:\n                return x, iters, False\n\n    # Max iterations reached\n    return x, max_iter, float(np.linalg.norm(grad(x))) = tol_grad\n\ndef classify_stationary_point(xy: np.ndarray) - int:\n    # 0: saddle (mixed signs), 1: minimum (all positive), 2: maximum (all negative), 3: failure (handled outside)\n    H = hess(xy)\n    eigs = np.linalg.eigvals(H)\n    tol = 1e-7\n    if np.all(eigs  tol):\n        return 1\n    elif np.all(eigs  -tol):\n        return 2\n    else:\n        return 0\n\ndef format_result(code: int, iters: int, xy: np.ndarray) - str:\n    # Produce no-space list representation: [code,iters,x,y] with six decimal places\n    return f\"[{code},{iters},{xy[0]:.6f},{xy[1]:.6f}]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.05, 0.05),\n        (0.05, 0.9),\n        (0.05, -0.9),\n        (0.8, 0.1),\n        (2.0, 2.0),\n        (0.0, 0.0),\n    ]\n\n    results_strs = []\n    for case in test_cases:\n        xy_star, iters, success = newton_optimize(case, tol_grad=1e-8, tol_step=1e-12, max_iter=100)\n        if success:\n            code = classify_stationary_point(xy_star)\n        else:\n            code = 3\n        results_strs.append(format_result(code, iters, xy_star))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_strs)}]\")\n\nsolve()\n```", "id": "3285127"}]}