{"hands_on_practices": [{"introduction": "To truly grasp the Broyden method, we begin with its most fundamental building block: a single iteration. This first exercise walks you through the core computational step of using an approximate Jacobian, $B_k$, to solve for the next iterate, $x_{k+1}$. By working through this calculation by hand, you will solidify your understanding of how the method generates a search direction and takes a step towards the solution of a nonlinear system. [@problem_id:2158069]", "problem": "Consider the system of nonlinear equations given by $F(x) = 0$, where $x = (x_1, x_2)^T$ and the function $F: \\mathbb{R}^2 \\to \\mathbb{R}^2$ is defined as:\n$$\nF(x) = \\begin{pmatrix} x_1^2 - x_2 + 1 \\\\ x_1 + x_2^2 - 3 \\end{pmatrix}\n$$\nYou are to perform a single step of the secant-updating quasi-Newton method known as Broyden's method to find an improved approximation of the root. Starting with the initial guess $x_0 = (1, 1)^T$ and using the initial approximation of the Jacobian matrix\n$$\nB_0 = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix},\n$$\ncalculate the next iterate, $x_1$. Express your answer as a vector of two components.", "solution": "We are given the system $F(x) = 0$ with $F: \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$ defined by\n$$\nF(x) = \\begin{pmatrix} x_{1}^{2} - x_{2} + 1 \\\\ x_{1} + x_{2}^{2} - 3 \\end{pmatrix},\n$$\nan initial guess $x_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, and an initial Jacobian approximation\n$$\nB_{0} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix}.\n$$\nIn Broyden’s method, the next iterate $x_{1}$ is computed by first solving the linear system\n$$\nB_{0} s_{0} = -F(x_{0}),\n$$\nand then updating\n$$\nx_{1} = x_{0} + s_{0}.\n$$\n\nFirst evaluate $F(x_{0})$:\n$$\nF(x_{0}) = \\begin{pmatrix} 1^{2} - 1 + 1 \\\\ 1 + 1^{2} - 3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\nSolve for $s_{0}$ using $B_{0} s_{0} = -F(x_{0})$. Since $B_{0} = 2 I$, we have\n$$\ns_{0} = B_{0}^{-1}\\big(-F(x_{0})\\big) = \\frac{1}{2} I \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}.\n$$\nThen,\n$$\nx_{1} = x_{0} + s_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix}.\n$$\nThus the next iterate is the vector with components $\\frac{1}{2}$ and $\\frac{3}{2}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix}}$$", "id": "2158069"}, {"introduction": "Broyden's method holds a surprising property that distinguishes it from Newton's method: the sequence of iterates $x_k$ can converge to a root $x^*$ even if the sequence of approximate Jacobians $B_k$ does not converge to the true Jacobian $J(x^*)$. This hands-on coding exercise challenges you to demonstrate this counter-intuitive yet fundamental behavior. By implementing the method and observing this phenomenon, you will gain a deeper appreciation for the theoretical underpinnings that make quasi-Newton methods both powerful and efficient. [@problem_id:3211914]", "problem": "You are asked to design and implement a program that demonstrates a core phenomenon of Broyden's method for solving systems of nonlinear equations: for a nonlinear two-dimensional system, the sequence of approximate Jacobians, denoted by $B_k$, can converge to a matrix that differs from the true Jacobian $J(x^*)$ at the solution $x^*$, while the iterates $x_k$ still converge to the root. The fundamental base you must employ is the definition of solving $F(x) = 0$ for a vector-valued function $F:\\mathbb{R}^n \\to \\mathbb{R}^n$ via iterative methods, the Newton iteration using the Jacobian, and the secant condition that underpins quasi-Newton schemes such as Broyden's method. You must not assume any special formula beyond these core definitions within your reasoning when crafting the algorithm. Your program must implement the standard Broyden update that enforces the secant condition and seeks a minimal change to the approximate Jacobian in the Frobenius norm sense at each iteration.\n\nImplement a solver for a two-dimensional system (that is, $n = 2$) using Broyden's method with the following requirements:\n- Use the iteration $x_{k+1} = x_k + s_k$, where $s_k$ solves $B_k s_k = -F(x_k)$, with $B_k$ being the current approximation to the Jacobian. If $B_k$ is singular or nearly singular, use a least-squares step computed by a pseudoinverse or an equivalent stable fallback that remains within the rules of standard numerical linear algebra. Use a simple backtracking strategy on the step length to ensure that the sequence $\\|F(x_k)\\|_2$ is not increasing.\n- Update $B_k$ at each step using the classic (so-called “good”) Broyden formula that enforces the most recent secant equation and produces the smallest change in $B_k$ in the Frobenius norm among all rank-one updates that satisfy the secant condition. Do not hard-code any special-case algebraic simplifications tied to a particular $F$.\n- Terminate when either $\\|F(x_k)\\|_2 \\le \\varepsilon_f$ or $\\|s_k\\|_2 \\le \\varepsilon_s$, with prescribed tolerances $\\varepsilon_f$ and $\\varepsilon_s$, or when a maximum number of iterations $N_{\\max}$ is reached. All norms are Euclidean norms for vectors and Frobenius norms for matrices.\n\nYou will test your implementation on the following small test suite, each with fully specified data and an exact root to compare against. There are no physical units involved in this task. All angles, if any were to appear, would be in radians, but no trigonometric functions of dimensional angles are required here.\n\nDefine the nonlinear system $F_{\\mathrm{nl}}:\\mathbb{R}^2 \\to \\mathbb{R}^2$ by\n$$\nF_{\\mathrm{nl}}(x) =\n\\begin{bmatrix}\nx_1^2 + x_2 - 1 \\\\\nx_1 + x_2^2 - 1\n\\end{bmatrix},\n$$\nwith analytic Jacobian\n$$\nJ_{\\mathrm{nl}}(x) =\n\\begin{bmatrix}\n2 x_1 & 1 \\\\\n1 & 2 x_2\n\\end{bmatrix}.\n$$\nThis system has an exact root at $x^*_{\\mathrm{nl}} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n\nDefine the linear system $F_{\\mathrm{lin}}:\\mathbb{R}^2 \\to \\mathbb{R}^2$ by\n$$\nF_{\\mathrm{lin}}(x) = A x - b, \\quad A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix},\n$$\nwith constant Jacobian $J_{\\mathrm{lin}}(x) \\equiv A$ and exact root $x^*_{\\mathrm{lin}} = A^{-1} b$.\n\nYour program must run these three test cases in order, using Broyden's method as specified:\n- Test $1$ (happy path, nonlinear): Use $F_{\\mathrm{nl}}$, initial guess $x_0 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}$, initial approximate Jacobian $B_0 = I_2$ (the $2 \\times 2$ identity), residual tolerance $\\varepsilon_f = 10^{-10}$, step tolerance $\\varepsilon_s = 10^{-12}$, and maximum iterations $N_{\\max} = 200$. After termination, compute two outputs:\n  - $d_1 = \\|x_{\\mathrm{final}} - x^*_{\\mathrm{nl}}\\|_2$,\n  - $e_1 = \\|B_{\\mathrm{final}} - J_{\\mathrm{nl}}(x^*_{\\mathrm{nl}})\\|_F$.\n- Test $2$ (boundary, start at the root): Use $F_{\\mathrm{nl}}$, initial guess $x_0 = x^*_{\\mathrm{nl}} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, initial approximate Jacobian $B_0 = 2 I_2$, tolerances $\\varepsilon_f = 10^{-10}$, $\\varepsilon_s = 10^{-12}$, and maximum iterations $N_{\\max} = 200$. Since the starting point is the exact root, the method should terminate immediately without updates. Compute:\n  - $d_2 = \\|x_{\\mathrm{final}} - x^*_{\\mathrm{nl}}\\|_2$,\n  - $e_2 = \\|B_{\\mathrm{final}} - J_{\\mathrm{nl}}(x^*_{\\mathrm{nl}})\\|_F$.\n- Test $3$ (contrast with linear system): Use $F_{\\mathrm{lin}}$, initial guess $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, initial approximate Jacobian $B_0 = I_2$, tolerances $\\varepsilon_f = 10^{-12}$, $\\varepsilon_s = 10^{-12}$, and maximum iterations $N_{\\max} = 50$. Compute:\n  - $d_3 = \\|x_{\\mathrm{final}} - x^*_{\\mathrm{lin}}\\|_2$,\n  - $e_3 = \\|B_{\\mathrm{final}} - A\\|_F$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the six floating-point values in this exact order:\n$$\n[d_1, e_1, d_2, e_2, d_3, e_3].\n$$\nDo not include any spaces in the output. The values should be represented as decimal floating-point numbers (scientific notation is acceptable). No additional text should be printed.", "solution": "The problem requires the implementation of Broyden's method to solve systems of nonlinear equations, $F(x) = 0$, and to demonstrate a key property of the method: the sequence of approximate Jacobians, $B_k$, does not necessarily converge to the true Jacobian at the root, $J(x^*)$, even when the iterates $x_k$ converge to the root $x^*$.\n\n**$1$. Broyden's Method: Foundations**\n\nBroyden's method is a quasi-Newton method. Newton's method for solving $F(x)=0$ follows the iteration $x_{k+1} = x_k - J(x_k)^{-1} F(x_k)$, where $J(x_k)$ is the Jacobian matrix of $F$ at $x_k$. This requires computing and inverting the Jacobian at each step, which can be computationally expensive.\n\nQuasi-Newton methods replace the true Jacobian $J(x_k)$ with an approximation $B_k$. The iteration becomes:\n$1$. Solve the linear system $B_k s_k = -F(x_k)$ for the step $s_k$.\n$2$. Update the solution: $x_{k+1} = x_k + s_k$.\n\nThe core idea is to update $B_k$ to $B_{k+1}$ in a way that is computationally cheap and incorporates new information about the function $F$. The update is based on the secant condition, which is a multidimensional generalization of the secant method for root-finding in one dimension. The secant condition requires the new approximate Jacobian $B_{k+1}$ to correctly relate the change in $x$ to the change in $F$ for the most recent step:\n$$B_{k+1} s_k = y_k$$\nwhere $s_k = x_{k+1} - x_k$ and $y_k = F(x_{k+1}) - F(x_k)$.\n\n**$2$. The \"Good\" Broyden Update**\n\nThere are infinitely many matrices $B_{k+1}$ that satisfy the secant equation. Broyden's \"good\" method chooses the matrix $B_{k+1}$ that is closest to the previous approximation $B_k$ in the Frobenius norm, subject to the secant condition. This is a constrained optimization problem:\n$$\\min_{B} \\|B - B_k\\|_F \\quad \\text{subject to} \\quad B s_k = y_k$$\nThe solution to this problem is a rank-$1$ update to $B_k$:\n$$B_{k+1} = B_k + \\frac{(y_k - B_k s_k) s_k^T}{s_k^T s_k}$$\nThis formula forms the heart of the Jacobian approximation update in our implementation. The term $y_k - B_k s_k$ represents the discrepancy between the actual change in $F$ and the change predicted by the current approximation $B_k$. This discrepancy is used to \"correct\" $B_k$ in the direction of the step $s_k$.\n\n**$3$. Algorithmic Implementation**\n\nThe solver is structured as an iterative loop with the following key components at each iteration $k$:\n\n*   **Step Calculation**: The step $s_k$ is found by solving the linear system $B_k s_k = -F(x_k)$. For robustness against singular or ill-conditioned matrices $B_k$, which can occur during the iteration, a least-squares solver is employed. This is equivalent to using the Moore-Penrose pseudoinverse, finding the solution $s_k = -B_k^\\dagger F(x_k)$ that minimizes $\\|B_k s_k + F(x_k)\\|_2$. In NumPy, `numpy.linalg.lstsq` provides this functionality.\n\n*   **Line Search**: A full step $s_k$ might not lead to a decrease in the residual norm $\\|F(x)\\|_2$. To ensure robust convergence, a simple backtracking line search is used. We start with a step length $\\alpha=1$ and successively reduce it (e.g., by halving) until the condition $\\|F(x_k + \\alpha s_k)\\|_2 < \\|F(x_k)\\|_2$ is satisfied. This ensures that each step makes progress towards the solution. The final step taken is then $\\alpha s_k$.\n\n*   **Jacobian Update**: After taking the step $s_k$ which leads to $x_{k+1} = x_k + s_k$, the new Jacobian approximation $B_{k+1}$ is computed using the Broyden formula mentioned above. To avoid numerical instability, the update is skipped if the step norm $\\|s_k\\|_2$ is close to $0$, as this would involve division by a very small number.\n\n*   **Termination Conditions**: The iteration stops when one of the following criteria is met:\n    $1$. The norm of the residual is below a tolerance: $\\|F(x_k)\\|_2 \\le \\varepsilon_f$.\n    $2$. The norm of the step is below a tolerance: $\\|s_k\\|_2 \\le \\varepsilon_s$. This indicates that the iterates are no longer changing significantly.\n    $3$. A maximum number of iterations, $N_{\\max}$, is reached.\n\n**$4$. Analysis of Test Cases**\n\nThe algorithm is applied to three test cases to observe its behavior.\n\n*   **Test $1$ (Nonlinear System)**: Starting from $x_0 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}$ and $B_0=I_2$, the method is applied to the nonlinear system $F_{\\mathrm{nl}}$. The iterates $x_k$ are expected to converge to the root $x^*_{\\mathrm{nl}} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, so the distance $d_1 = \\|x_{\\mathrm{final}} - x^*_{\\mathrm{nl}}\\|_2$ should be near $0$. However, as the iterates converge, the steps $s_k$ may become aligned in a specific direction (subspace concentration). The Broyden update only enforces the secant condition in these directions. Consequently, the components of $B_k$ corresponding to orthogonal directions are not refined. Thus, the final matrix $B_{\\mathrm{final}}$ may not equal the true Jacobian $J_{\\mathrm{nl}}(x^*_{\\mathrm{nl}})$, and the error $e_1 = \\|B_{\\mathrm{final}} - J_{\\mathrm{nl}}(x^*_{\\mathrm{nl}})\\|_F$ is expected to be non-$0$. The true Jacobian at the root is $J_{\\mathrm{nl}}(x^*_{\\mathrm{nl}}) = \\begin{bmatrix} 2 & 1 \\\\ 1 & 0 \\end{bmatrix}$.\n\n*   **Test $2$ (Boundary Case)**: The method starts at the exact root, $x_0 = x^*_{\\mathrm{nl}}$. The initial residual $\\|F(x_0)\\|_2$ is $0$. The algorithm correctly identifies this and terminates on the first check, without performing any steps or updates. Therefore, $x_{\\mathrm{final}} = x_0$ and $B_{\\mathrm{final}} = B_0$. The distance $d_2$ will be $0$. The error $e_2 = \\|B_0 - J_{\\mathrm{nl}}(x^*_{\\mathrm{nl}})\\|_F$ will be non-$0$, calculated as $\\|2I_2 - J_{\\mathrm{nl}}(x^*_{\\mathrm{nl}})\\|_F$.\n\n*   **Test $3$ (Linear System)**: The method is applied to a linear system $F(x) = Ax-b$. For a linear system, the change in function value is $y_k = F(x_{k+1}) - F(x_k) = A(x_{k+1}-x_k) = As_k$. The Broyden update becomes $B_{k+1} = B_k + \\frac{(A-B_k)s_k s_k^T}{s_k^T s_k}$. This update has the property that $(B_{k+1}-A)s_k=0$. In $n$ dimensions, if the method generates $n$ linearly independent steps, $B_{n}$ will equal $A$. Broyden's method is known to terminate for linear systems in at most $2n$ iterations. For this $2$-dimensional case, convergence should be very fast, and the final approximation $B_{\\mathrm{final}}$ is expected to be very close to the true (and constant) Jacobian $A$. Both errors $d_3$ and $e_3$ should be close to $0$.\n\nThis structured set of tests illuminates the fundamental behavior of Broyden's method in different scenarios, particularly highlighting the distinction between convergence of the iterates $x_k$ and convergence of the approximate Jacobians $B_k$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef broyden_solver(F, x0, B0, tol_f, tol_s, max_iter):\n    \"\"\"\n    Solves a system of nonlinear equations F(x) = 0 using Broyden's method.\n\n    Args:\n        F (callable): The vector-valued function to find a root of.\n        x0 (np.ndarray): The initial guess for the solution x.\n        B0 (np.ndarray): The initial guess for the Jacobian matrix.\n        tol_f (float): Tolerance for the norm of the residual F(x).\n        tol_s (float): Tolerance for the norm of the step s.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        tuple: A tuple (x, B) containing the final solution iterate and the\n               final approximate Jacobian.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    B = np.array(B0, dtype=float)\n    \n    Fx = F(x)\n    \n    for _ in range(max_iter):\n        # 1. Check termination on residual norm\n        if np.linalg.norm(Fx) <= tol_f:\n            break\n            \n        # 2. Solve for the step s_trial = -B_k^{-1} F(x_k)\n        # We use lstsq for robustness against singularity.\n        try:\n            s_trial = np.linalg.lstsq(B, -Fx, rcond=None)[0]\n        except np.linalg.LinAlgError:\n            # This is a fallback, but lstsq should generally not fail.\n            # If it does, the matrix is extremely ill-conditioned. Stop iteration.\n            break\n\n        # 3. Check termination on step size (before backtracking)\n        if np.linalg.norm(s_trial) <= tol_s:\n            break\n\n        # 4. Backtracking line search\n        alpha = 1.0\n        norm_Fx = np.linalg.norm(Fx)\n        x_next = x + alpha * s_trial\n        Fx_next = F(x_next)\n        \n        # Backtrack until the residual norm is not increasing. Limit to 10 steps.\n        for _ in range(10):\n            if np.linalg.norm(Fx_next) < norm_Fx:\n                break\n            alpha /= 2.0\n            x_next = x + alpha * s_trial\n            Fx_next = F(x_next)\n        else: # if loop finished without break, step is not productive\n            # Could indicate stalling, so we halt.\n            break\n\n        s = x_next - x\n        y = Fx_next - Fx\n        \n        # 5. Update Jacobian B using Broyden's \"good\" formula\n        # B_{k+1} = B_k + (y_k - B_k s_k)s_k^T / (s_k^T s_k)\n        s_norm_sq = np.dot(s, s)\n        if s_norm_sq > 1e-14: # Avoid division by zero for very small steps\n            update_vec = y - B @ s\n            B += np.outer(update_vec, s) / s_norm_sq\n            \n        # 6. Update x and Fx for the next iteration\n        x = x_next\n        Fx = Fx_next\n        \n    return x, B\n\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    # --- Define Systems and Exact Solutions ---\n    def F_nl(x):\n        return np.array([\n            x[0]**2 + x[1] - 1.0, \n            x[0] + x[1]**2 - 1.0\n        ])\n\n    def J_nl(x):\n        return np.array([\n            [2.0 * x[0], 1.0], \n            [1.0, 2.0 * x[1]]\n        ])\n    \n    A = np.array([[3.0, 1.0], [1.0, 2.0]])\n    b = np.array([1.0, 0.0])\n    def F_lin(x):\n        return A @ x - b\n\n    x_star_nl = np.array([1.0, 0.0])\n    x_star_lin = np.linalg.solve(A, b)\n\n    results = []\n\n    # --- Test Case 1: Nonlinear system, standard start ---\n    x0_1 = [0.5, 0.5]\n    B0_1 = np.identity(2)\n    tol_f_1, tol_s_1, N_max_1 = 1e-10, 1e-12, 200\n    \n    x_final_1, B_final_1 = broyden_solver(F_nl, x0_1, B0_1, tol_f_1, tol_s_1, N_max_1)\n    \n    d1 = np.linalg.norm(x_final_1 - x_star_nl)\n    J_star_nl = J_nl(x_star_nl)\n    e1 = np.linalg.norm(B_final_1 - J_star_nl, ord='fro')\n    results.extend([d1, e1])\n\n    # --- Test Case 2: Nonlinear system, start at root ---\n    x0_2 = x_star_nl\n    B0_2 = 2.0 * np.identity(2)\n    tol_f_2, tol_s_2, N_max_2 = 1e-10, 1e-12, 200\n\n    x_final_2, B_final_2 = broyden_solver(F_nl, x0_2, B0_2, tol_f_2, tol_s_2, N_max_2)\n    \n    d2 = np.linalg.norm(x_final_2 - x_star_nl)\n    e2 = np.linalg.norm(B_final_2 - J_star_nl, ord='fro')\n    results.extend([d2, e2])\n\n    # --- Test Case 3: Linear system ---\n    x0_3 = [0.0, 0.0]\n    B0_3 = np.identity(2)\n    tol_f_3, tol_s_3, N_max_3 = 1e-12, 1e-12, 50\n\n    x_final_3, B_final_3 = broyden_solver(F_lin, x0_3, B0_3, tol_f_3, tol_s_3, N_max_3)\n    \n    d3 = np.linalg.norm(x_final_3 - x_star_lin)\n    e3 = np.linalg.norm(B_final_3 - A, ord='fro')\n    results.extend([d3, e3])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3211914"}, {"introduction": "In many real-world scientific and engineering applications, nonlinear systems are not only large but also sparse, meaning their Jacobian matrices are mostly filled with zeros. This final practice problem guides you through adapting Broyden's method for such scenarios, a crucial skill in high-performance scientific computing. You will implement a sparse variant that preserves the known sparsity pattern of the Jacobian, preventing the computationally expensive 'fill-in' that a standard Broyden update would cause, thereby making the algorithm practical for large-scale problems. [@problem_id:3211840]", "problem": "You are asked to implement a program that solves nonlinear systems of equations using a projected sparse variant of Broyden's method. The method must preserve a known sparsity pattern of the true Jacobian matrix by projecting the standard dense rank-one update back to that pattern at each iteration. Angles appearing in trigonometric functions must be interpreted in radians.\n\nStart from the following foundational base. Newton's method for solving a nonlinear system $F(x) = 0$ with $F : \\mathbb{R}^n \\to \\mathbb{R}^n$ uses the Jacobian $J(x)$ to compute iterates $x_{k+1} = x_k + s_k$, where the step $s_k$ solves $J(x_k) s_k = -F(x_k)$. When $J(x)$ is unknown or too expensive to evaluate, quasi-Newton methods maintain an approximation $B_k$ and impose the secant condition $B_{k+1} s_k = y_k$ with $y_k = F(x_{k+1}) - F(x_k)$. Among such methods, Broyden's \"good\" update arises by selecting $B_{k+1}$ to be the closest matrix (in the Frobenius norm) to $B_k$ subject to the secant condition. In this problem, you must further restrict $B_k$ to a linear subspace of sparse matrices defined by a fixed sparsity pattern, and obtain $B_{k+1}$ by orthogonally projecting the dense rank-one update onto this subspace with respect to the Frobenius inner product. This projection is the elementwise masking that zeros out entries outside the sparsity pattern. The update should be skipped if the step $s_k$ is numerically negligible.\n\nYour program must implement the following algorithmic components strictly from these principles:\n- Maintain an approximate Jacobian $B_k$ constrained to a given sparsity pattern, initialized via a sparse forward-difference approximation restricted to the pattern. Use a step size $h = 10^{-8}$ for finite differences.\n- At iteration $k$, compute a step $s_k$ by solving $B_k s_k \\approx -F(x_k)$ in the least-squares sense if $B_k$ is not invertible. Update $x_{k+1} = x_k + s_k$.\n- Form the dense rank-one Broyden update implied by the secant framework, then project it to the sparsity pattern by zeroing entries outside the pattern. If $\\|s_k\\|_2^2$ is below a numerical threshold, do not change $B_k$.\n- Terminate when $\\|F(x_k)\\|_2 \\le \\tau$ with tolerance $\\tau = 10^{-10}$, or when the iteration count reaches $K_{\\max} = 50$.\n\nTest suite. You must evaluate your implementation on the following four cases. In all cases, $n = 3$ and all angles are in radians.\n\nCase A (banded/tridiagonal-like sparsity with coupling):\n- Function $F_1 : \\mathbb{R}^3 \\to \\mathbb{R}^3$:\n  $$F_1(x) = \\begin{bmatrix}\n  \\sin(x_1) + x_2^2 \\\\\n  x_1 + \\tanh(x_2) + x_3 \\\\\n  x_2^2 + e^{x_3} - 1\n  \\end{bmatrix}.$$\n- Known sparsity pattern (allowed indices using $1$-based indexing): $\\{(1,1),(1,2),(2,1),(2,2),(2,3),(3,2),(3,3)\\}$.\n- Initial guess: $x_0 = [0.3,-0.2,0.1]$.\n\nCase B (diagonal system with diagonal sparsity):\n- Function $F_2 : \\mathbb{R}^3 \\to \\mathbb{R}^3$:\n  $$F_2(x) = \\begin{bmatrix}\n  x_1^3 + x_1 \\\\\n  2 x_2 + x_2^3 \\\\\n  3 x_3 + x_3^3\n  \\end{bmatrix}.$$\n- Known sparsity pattern: diagonal only, that is $\\{(1,1),(2,2),(3,3)\\}$.\n- Initial guess: $x_0 = [0.8,-0.5,0.3]$.\n\nCase C (zero residual at initialization; update-skipping edge case):\n- Function $F_1$ as in Case A.\n- Same sparsity pattern as Case A.\n- Initial guess: $x_0 = [0,0,0]$.\n\nCase D (mismatch between true coupling and diagonal sparsity; small cross-coupling):\n- Function $F_3 : \\mathbb{R}^3 \\to \\mathbb{R}^3$:\n  $$F_3(x) = \\begin{bmatrix}\n  x_1 + 0.2 \\sin(x_3) \\\\\n  x_2 + x_1^2 - 0.1 \\\\\n  x_3 + 0.3 \\cos(x_1) - 0.3\n  \\end{bmatrix}.$$\n- Known sparsity pattern: diagonal only, that is $\\{(1,1),(2,2),(3,3)\\}$.\n- Initial guess: $x_0 = [0.5,0.0,0.5]$.\n\nNumerical details and conventions:\n- Use forward differences with step $h = 10^{-8}$ to initialize $B_0$ restricted to the pattern.\n- Use the Euclidean norm $\\|\\cdot\\|_2$ for residuals and steps.\n- Use a numerical threshold on $\\|s_k\\|_2^2$ equal to $10^{-16}$ to decide whether to skip the update.\n\nOutput specification:\n- For each case, return the final residual norm $\\|F(x_{\\text{final}})\\|_2$ as a floating-point number, formatted in scientific notation with exactly $6$ digits after the decimal point.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order Case A, Case B, Case C, Case D; for example, a valid output format is\n  $$[r_A,r_B,r_C,r_D].$$", "solution": "The problem requires the implementation of a projected sparse variant of Broyden's method to solve systems of nonlinear equations of the form $F(x) = 0$, where $F: \\mathbb{R}^n \\to \\mathbb{R}^n$. The solution process involves validating the problem statement, deriving the algorithm from first principles, and then implementing it to solve a given set of test cases.\n\n### Problem Validation\n\nFollowing the prescribed protocol, the problem statement is first subjected to rigorous validation.\n\n**Step 1: Extract Givens**\n- **Method**: Projected sparse Broyden's method for solving $F(x)=0$.\n- **System Iteration**: $x_{k+1} = x_k + s_k$, where the step $s_k$ is computed from a linear system involving an approximate Jacobian $B_k$.\n- **Linear System for Step**: $B_k s_k \\approx -F(x_k)$, to be solved in the least-squares sense.\n- **Jacobian Approximation Update**: The update from $B_k$ to $B_{k+1}$ is based on the secant condition $B_{k+1}s_k = y_k$, where $s_k = x_{k+1} - x_k$ and $y_k = F(x_{k+1}) - F(x_k)$. The update formula is a projection of the standard \"good\" Broyden update onto a subspace of sparse matrices.\n- **Sparsity**: The matrix $B_k$ is constrained to a fixed sparsity pattern. The projection is defined as elementwise masking.\n- **Initial Jacobian $B_0$**: Computed via sparse forward-difference approximation with step size $h = 10^{-8}$, restricted to the given sparsity pattern.\n- **Update Skip Condition**: The update to $B_k$ is skipped if $\\|s_k\\|_2^2 < 10^{-16}$.\n- **Termination Criteria**: The iteration stops when $\\|F(x_k)\\|_2 \\le \\tau = 10^{-10}$ or the number of iterations reaches $K_{\\max} = 50$.\n- **Test Cases**: Four specific cases (A, B, C, D) are provided, each with a function $F(x)$, a sparsity pattern, and an initial guess $x_0$. For all cases, $n=3$ and angles are in radians.\n- **Output Format**: A single line containing a comma-separated list of the final residual norms for each case, `[r_A,r_B,r_C,r_D]`, with each norm formatted in scientific notation with $6$ decimal places.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically and mathematically sound. It describes a well-established numerical technique (a sparse variant of a quasi-Newton method). The components of the algorithm—secant condition, rank-one updates, projection onto a sparse subspace, finite-difference initialization, and least-squares solve for the step—are all standard and well-defined concepts in numerical analysis and scientific computing. The problem is self-contained, providing all necessary functions, parameters, and initial conditions. The test cases are well-posed and include a standard convergence case, a diagonally-dominant case, a zero-residual edge case, and a case with a sparsity mismatch, which together form a reasonable test suite. There are no contradictions, ambiguities, or violations of scientific principles.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Principle-Based Solution Derivation\n\nThe core task is to find a root $x^*$ such that $F(x^*) = 0$. Quasi-Newton methods construct a sequence of approximations $x_k \\to x^*$ by modeling the nonlinear function $F$ at each step with a linear approximation based on an approximate Jacobian, $B_k$.\n\n**1. The Quasi-Newton Framework**\nAt an iterate $x_k$, the function $F$ is approximated by its first-order Taylor expansion around $x_k$: $F(x) \\approx F(x_k) + J_F(x_k)(x - x_k)$. In a quasi-Newton method, the true Jacobian $J_F(x_k)$ is replaced by an approximation $B_k$. To find the next iterate $x_{k+1}$ which should be an approximate root of this linear model, we set $F(x_{k+1}) = 0$:\n$$\n0 \\approx F(x_k) + B_k (x_{k+1} - x_k)\n$$\nDefining the step $s_k = x_{k+1} - x_k$, this leads to the linear system for the step:\n$$\nB_k s_k = -F(x_k)\n$$\nAs specified, this system is to be solved in the least-squares sense, which provides a robust way to find $s_k$ even if $B_k$ is singular or ill-conditioned. The next iterate is then $x_{k+1} = x_k + s_k$.\n\n**2. The Sparse Broyden Update**\nAfter computing the step $s_k$ and the new iterate $x_{k+1}$, the Jacobian approximation $B_k$ must be updated to $B_{k+1}$. The update incorporates information gained from the new function evaluation $F(x_{k+1})$. The guiding principle is the secant condition, which requires the new approximation $B_{k+1}$ to match the gradient of $F$ along the direction of the step $s_k$:\n$$\nB_{k+1} s_k = y_k\n$$\nwhere $y_k = F(x_{k+1}) - F(x_k)$ is the change in function values.\n\nFor $n>1$, this single vector equation provides $n$ scalar constraints, which is insufficient to uniquely determine the $n^2$ elements of $B_{k+1}$. Broyden's \"good\" method adds the constraint that $B_{k+1}$ should be as close as possible to $B_k$ in the Frobenius norm, i.e., minimizing $\\|B_{k+1} - B_k\\|_F$. This leads to the classic rank-one update:\n$$\nB_{k+1}^{\\text{dense}} = B_k + \\frac{(y_k - B_k s_k)s_k^T}{s_k^T s_k}\n$$\nThe problem requires this method to be adapted for a known sparsity pattern. Let the sparsity pattern be represented by a binary mask matrix $S$, where $S_{ij}=1$ for allowed non-zero entries and $S_{ij}=0$ otherwise. The space of matrices conforming to this pattern is a linear subspace of $\\mathbb{R}^{n \\times n}$. The problem specifies that the update should be performed by first calculating the dense rank-one update matrix, $\\Delta B_k = \\frac{(y_k - B_k s_k)s_k^T}{s_k^T s_k}$, and then orthogonally projecting it onto this sparse subspace. The orthogonal projection of a matrix $A$ onto this subspace with respect to the Frobenius inner product is simply the element-wise product $A \\odot S$.\n\nThus, the projected sparse update rule is:\n$$\nB_{k+1} = B_k + \\left( \\frac{(y_k - B_k s_k)s_k^T}{s_k^T s_k} \\right) \\odot S\n$$\nThis ensures that if $B_k$ has the desired sparsity pattern (i.e., $B_k = B_k \\odot S$), then $B_{k+1}$ will as well, since the update term is forced to be sparse. A small step $s_k$ can lead to numerical instability in the update formula's denominator and provides little new information. Therefore, the update is skipped if $\\|s_k\\|_2^2$ is below a threshold of $10^{-16}$.\n\n**3. Algorithm Initialization**\nThe process starts with an initial guess $x_0$ and requires an initial Jacobian approximation $B_0$ that conforms to the sparsity pattern $S$. $B_0$ is constructed using a sparse forward-difference scheme. For each entry $(i, j)$ where $S_{ij}=1$, the partial derivative $\\frac{\\partial F_i}{\\partial x_j}$ is approximated at $x_0$ using a small perturbation in the direction of the $j$-th coordinate axis:\n$$\n(B_0)_{ij} = \\frac{F_i(x_0 + h e_j) - F_i(x_0)}{h}\n$$\nwhere $e_j$ is the $j$-th standard basis vector and $h = 10^{-8}$ is the finite difference step size. All other entries of $B_0$ are initialized to $0$.\n\n**Summary of the Algorithm**\n1.  **Initialize**: Given $F$, $x_0$, $S$, $\\tau=10^{-10}$, $K_{\\max}=50$, $h=10^{-8}$, and step norm tolerance $\\epsilon_s^2 = 10^{-16}$.\n2.  **Construct $B_0$**: Use sparse forward differences based on the mask $S$.\n3.  **Iterate** for $k = 0, 1, \\dots, K_{\\max}-1$:\n    a. Compute $f_k = F(x_k)$ and check for convergence: if $\\|f_k\\|_2 \\le \\tau$, terminate.\n    b. Solve for the step $s_k$ by finding the least-squares solution to $B_k s_k = -f_k$.\n    c. Update the iterate: $x_{k+1} = x_k + s_k$.\n    d. Check step size: If $\\|s_k\\|_2^2 < \\epsilon_s^2$, set $B_{k+1} = B_k$ and go to step 3h.\n    e. Compute $f_{k+1} = F(x_{k+1})$ and $y_k = f_{k+1} - f_k$.\n    f. Compute the projected sparse update: $B_{k+1} = B_k + \\left( \\frac{(y_k - B_k s_k)s_k^T}{s_k^T s_k} \\right) \\odot S$.\n    h. Set $x_k \\leftarrow x_{k+1}$, $B_k \\leftarrow B_{k+1}$.\n4.  **Terminate**: Return the final residual norm, $\\|F(x_{k+1})\\|_2$ or $\\|F(x_{K_{\\max}})|_2$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define and run the test cases for the projected\n    sparse Broyden's method.\n    \"\"\"\n\n    def projected_sparse_broyden(\n        F, x0, sparsity_pattern, n,\n        k_max=50, tau=1e-10, h=1e-8, s_thresh_sq=1e-16\n    ):\n        \"\"\"\n        Solves a nonlinear system F(x)=0 using a projected sparse variant\n        of Broyden's method.\n\n        Args:\n            F (callable): The vector function F(x).\n            x0 (np.ndarray): The initial guess for the solution.\n            sparsity_pattern (set): A set of (i, j) tuples (1-based)\n                                     for the non-zero entries of the Jacobian.\n            n (int): The dimension of the system.\n            k_max (int): Maximum number of iterations.\n            tau (float): Tolerance for the residual norm for convergence.\n            h (float): Step size for finite difference approximation.\n            s_thresh_sq (float): Squared L2-norm threshold for skipping B update.\n\n        Returns:\n            float: The final L2-norm of the residual F(x_final).\n        \"\"\"\n        # Create a 0-based sparsity mask matrix\n        S = np.zeros((n, n))\n        for i, j in sparsity_pattern:\n            S[i - 1, j - 1] = 1.0\n\n        # --- Initialization ---\n        # Initialize B0 using sparse forward differences\n        x_k = np.array(x0, dtype=float)\n        B_k = np.zeros((n, n), dtype=float)\n        f0 = F(x_k)\n        \n        for i in range(1, n + 1):\n            for j in range(1, n + 1):\n                if (i, j) in sparsity_pattern:\n                    e_j = np.zeros(n)\n                    e_j[j - 1] = 1.0\n                    x_h = x_k + h * e_j\n                    f_h = F(x_h)\n                    B_k[i - 1, j - 1] = (f_h[i - 1] - f0[i - 1]) / h\n        \n        f_k = f0\n        \n        # --- Iteration Loop ---\n        for k in range(k_max):\n            residual_norm = np.linalg.norm(f_k)\n            if residual_norm <= tau:\n                return residual_norm\n\n            # Solve for the step s_k using least squares\n            # B_k s_k = -f_k\n            s_k = np.linalg.lstsq(B_k, -f_k, rcond=None)[0]\n\n            x_k_plus_1 = x_k + s_k\n            s_k_norm_sq = np.dot(s_k, s_k)\n\n            # Skip Jacobian update if step is too small\n            if s_k_norm_sq < s_thresh_sq:\n                B_k_plus_1 = B_k\n            else:\n                # Compute update based on secant condition\n                f_k_plus_1 = F(x_k_plus_1)\n                y_k = f_k_plus_1 - f_k\n                \n                # Dense rank-one update term\n                update_numerator = np.outer(y_k - B_k @ s_k, s_k)\n                if s_k_norm_sq > 0: # Defensive check\n                    update_term = update_numerator / s_k_norm_sq\n                else: # Should be caught by s_thresh_sq\n                    update_term = np.zeros_like(B_k)\n\n                # Project onto sparse subspace and update\n                B_k_plus_1 = B_k + update_term * S\n\n            # Prepare for next iteration\n            x_k = x_k_plus_1\n            B_k = B_k_plus_1\n            f_k = F(x_k)\n\n        return np.linalg.norm(F(x_k))\n\n    # --- Test Cases Definition ---\n\n    # Case A: F1, tridiagonal-like sparsity\n    def F1(x):\n        x1, x2, x3 = x\n        return np.array([\n            np.sin(x1) + x2**2,\n            x1 + np.tanh(x2) + x3,\n            x2**2 + np.exp(x3) - 1.0\n        ])\n    sparsity_A = {(1,1),(1,2),(2,1),(2,2),(2,3),(3,2),(3,3)}\n    x0_A = [0.3, -0.2, 0.1]\n\n    # Case B: F2, diagonal sparsity\n    def F2(x):\n        x1, x2, x3 = x\n        return np.array([\n            x1**3 + x1,\n            2.0 * x2 + x2**3,\n            3.0 * x3 + x3**3\n        ])\n    sparsity_B = {(1,1), (2,2), (3,3)}\n    x0_B = [0.8, -0.5, 0.3]\n\n    # Case C: F1, zero residual at initialization\n    # Reuses F1 and sparsity_A\n    x0_C = [0.0, 0.0, 0.0]\n\n    # Case D: F3, diagonal sparsity with true cross-coupling\n    def F3(x):\n        x1, x2, x3 = x\n        return np.array([\n            x1 + 0.2 * np.sin(x3),\n            x2 + x1**2 - 0.1,\n            x3 + 0.3 * np.cos(x1) - 0.3\n        ])\n    sparsity_D = {(1,1), (2,2), (3,3)}\n    x0_D = [0.5, 0.0, 0.5]\n\n    test_cases = [\n        {'F': F1, 'x0': x0_A, 'sparsity': sparsity_A, 'n': 3},\n        {'F': F2, 'x0': x0_B, 'sparsity': sparsity_B, 'n': 3},\n        {'F': F1, 'x0': x0_C, 'sparsity': sparsity_A, 'n': 3},\n        {'F': F3, 'x0': x0_D, 'sparsity': sparsity_D, 'n': 3},\n    ]\n\n    results = []\n    for case in test_cases:\n        final_residual = projected_sparse_broyden(\n            F=case['F'],\n            x0=case['x0'],\n            sparsity_pattern=case['sparsity'],\n            n=case['n']\n        )\n        results.append(\"{:.6e}\".format(final_residual))\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3211840"}]}