{"hands_on_practices": [{"introduction": "This first exercise goes back to the fundamentals of the least squares method. By deriving the normal equations from scratch for a simple quadratic model, you will solidify your understanding of how the principle of minimizing squared error leads to a solvable system of linear equations. This problem [@problem_id:3262996] also introduces the correct way to handle replicate measurements, a common scenario in experimental data collection.", "problem": "A laboratory is calibrating a sensor whose output is modeled by a quadratic polynomial. Four independent measurements are taken at input values $x \\in \\{0,1,1,2\\}$, producing outputs $y \\in \\{10,8,6,3\\}$ in the same order. The repeated input $x=1$ arises from an intentional replicate to reduce uncertainty; both replicate observations must be treated as distinct contributions to the fitting criterion. The model is a quadratic polynomial $p(x) = ax^2 + bx + c$. The best approximation in the sense of the method of least squares is defined as the choice of coefficients $a$, $b$, and $c$ that minimizes the sum of squared residuals $\\sum (p(x_i) - y_i)^2$ over the given data points.\n\nStarting only from the fundamental definition of least squares minimization and standard linear algebra facts, derive the necessary conditions for optimality by setting the gradient of the objective with respect to the coefficients equal to zero, carefully accounting for the fact that there are two measurements at $x=1$ with different outputs. Then solve the resulting system exactly to determine the leading coefficient $a$ of the best-fit quadratic polynomial.\n\nExpress your final answer exactly as a single number; do not round.", "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n-   Input values: $x \\in \\{0,1,1,2\\}$.\n-   Output values: $y \\in \\{10,8,6,3\\}$, corresponding to the input values in order.\n-   Data points $(x_i, y_i)$: $(0, 10)$, $(1, 8)$, $(1, 6)$, $(2, 3)$. The total number of measurements is $n=4$.\n-   Model: A quadratic polynomial $p(x) = ax^2 + bx + c$.\n-   Objective: Minimize the sum of squared residuals, $S = \\sum_{i=1}^{4} (p(x_i) - y_i)^2$, to find the best-fit coefficients $a$, $b$, and $c$.\n-   Task: Derive the necessary conditions for the minimum from first principles by setting the gradient of $S$ to zero, and then solve for the coefficient $a$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem describes a standard polynomial least squares fitting procedure, which is a fundamental and widely used technique in numerical methods, statistics, and data analysis. It is based on well-established mathematical principles.\n-   **Well-Posed:** The problem provides four data points to determine three unknown coefficients ($a, b, c$). The system is overdetermined, which is the standard scenario for a least squares fit. A unique solution exists if and only if the columns of the design matrix are linearly independent. The design matrix $A$ for this problem is given by\n    $$A = \\begin{pmatrix} x_1^2  x_1  1 \\\\ x_2^2  x_2  1 \\\\ x_3^2  x_3  1 \\\\ x_4^2  x_4  1 \\end{pmatrix} = \\begin{pmatrix} 0^2  0  1 \\\\ 1^2  1  1 \\\\ 1^2  1  1 \\\\ 2^2  2  1 \\end{pmatrix} = \\begin{pmatrix} 0  0  1 \\\\ 1  1  1 \\\\ 1  1  1 \\\\ 4  2  1 \\end{pmatrix}$$\n    The columns are linearly independent, as shown by setting a linear combination to zero: $\\alpha_1 [0,1,1,4]^T + \\alpha_2 [0,1,1,2]^T + \\alpha_3 [1,1,1,1]^T = [0,0,0,0]^T$. The first row implies $\\alpha_3=0$. This reduces the system to $\\alpha_1+\\alpha_2=0$ and $4\\alpha_1+2\\alpha_2=0$, which yields $\\alpha_1=\\alpha_2=0$. Thus, the columns are linearly independent, the matrix $A^T A$ is invertible, and a unique solution exists. The problem is well-posed.\n-   **Objective:** The problem statement is clear, precise, and uses standard terminology. The instruction to treat the replicate measurements as distinct contributions is unambiguous.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically sound, well-posed, and objective. I will proceed with the solution.\n\n### Derivation and Solution\n\nThe model for the sensor's output is the quadratic polynomial $p(x) = ax^2 + bx + c$. The data points are $(x_1, y_1) = (0, 10)$, $(x_2, y_2) = (1, 8)$, $(x_3, y_3) = (1, 6)$, and $(x_4, y_4) = (2, 3)$.\n\nThe objective is to find the coefficients $a$, $b$, and $c$ that minimize the sum of squared residuals, $S(a, b, c)$. The function $S$ is defined as:\n$$S(a, b, c) = \\sum_{i=1}^{4} (p(x_i) - y_i)^2$$\nSubstituting the given data and the polynomial model:\n$$S(a, b, c) = (a(0)^2 + b(0) + c - 10)^2 + (a(1)^2 + b(1) + c - 8)^2 + (a(1)^2 + b(1) + c - 6)^2 + (a(2)^2 + b(2) + c - 3)^2$$\n$$S(a, b, c) = (c - 10)^2 + (a + b + c - 8)^2 + (a + b + c - 6)^2 + (4a + 2b + c - 3)^2$$\n\nTo minimize $S(a,b,c)$, we must find the point where its gradient with respect to the coefficients is the zero vector. This gives the necessary conditions for optimality:\n$$\\frac{\\partial S}{\\partial a} = 0, \\quad \\frac{\\partial S}{\\partial b} = 0, \\quad \\frac{\\partial S}{\\partial c} = 0$$\n\nWe compute each partial derivative using the chain rule:\n\n1.  Partial derivative with respect to $a$:\n    $$\\frac{\\partial S}{\\partial a} = 0 + 2(a + b + c - 8)(1) + 2(a + b + c - 6)(1) + 2(4a + 2b + c - 3)(4) = 0$$\n    Dividing by $2$:\n    $$(a + b + c - 8) + (a + b + c - 6) + 4(4a + 2b + c - 3) = 0$$\n    $$(1+1+16)a + (1+1+8)b + (1+1+4)c = 8 + 6 + 12$$\n    $$18a + 10b + 6c = 26$$\n    $$9a + 5b + 3c = 13 \\quad (1)$$\n\n2.  Partial derivative with respect to $b$:\n    $$\\frac{\\partial S}{\\partial b} = 0 + 2(a + b + c - 8)(1) + 2(a + b + c - 6)(1) + 2(4a + 2b + c - 3)(2) = 0$$\n    Dividing by $2$:\n    $$(a + b + c - 8) + (a + b + c - 6) + 2(4a + 2b + c - 3) = 0$$\n    $$(1+1+8)a + (1+1+4)b + (1+1+2)c = 8 + 6 + 6$$\n    $$10a + 6b + 4c = 20$$\n    $$5a + 3b + 2c = 10 \\quad (2)$$\n\n3.  Partial derivative with respect to $c$:\n    $$\\frac{\\partial S}{\\partial c} = 2(c - 10)(1) + 2(a + b + c - 8)(1) + 2(a + b + c - 6)(1) + 2(4a + 2b + c - 3)(1) = 0$$\n    Dividing by $2$:\n    $$(c - 10) + (a + b + c - 8) + (a + b + c - 6) + (4a + 2b + c - 3) = 0$$\n    $$(1+1+4)a + (1+1+2)b + (1+1+1+1)c = 10 + 8 + 6 + 3$$\n    $$6a + 4b + 4c = 27 \\quad (3)$$\n\nWe now have a system of three linear equations, known as the normal equations, for the three unknown coefficients $a$, $b$, and $c$:\n1.  $9a + 5b + 3c = 13$\n2.  $5a + 3b + 2c = 10$\n3.  $6a + 4b + 4c = 27$\n\nThe problem requires solving for the leading coefficient $a$. We can use elimination to solve the system. Let's eliminate $c$.\nMultiply equation (2) by $3$ and equation (1) by $2$:\n$$2 \\times (1): \\quad 18a + 10b + 6c = 26$$\n$$3 \\times (2): \\quad 15a + 9b + 6c = 30$$\nSubtracting the second new equation from the first:\n$$(18a - 15a) + (10b - 9b) + (6c - 6c) = 26 - 30$$\n$$3a + b = -4 \\quad (4)$$\n\nNext, use equations (2) and (3) to eliminate $c$. Multiply equation (2) by $2$:\n$$2 \\times (2): \\quad 10a + 6b + 4c = 20$$\n$$(3): \\quad 6a + 4b + 4c = 27$$\nSubtracting the first new equation from the second:\n$$(6a - 10a) + (4b - 6b) + (4c - 4c) = 27 - 20$$\n$$-4a - 2b = 7 \\quad (5)$$\n\nNow we have a system of two equations for $a$ and $b$:\n4.  $3a + b = -4$\n5.  $-4a - 2b = 7$\n\nFrom equation (4), we can express $b$ in terms of $a$:\n$$b = -4 - 3a$$\nSubstitute this expression for $b$ into equation (5):\n$$-4a - 2(-4 - 3a) = 7$$\n$$-4a + 8 + 6a = 7$$\n$$2a = 7 - 8$$\n$$2a = -1$$\n$$a = -\\frac{1}{2}$$\n\nThe leading coefficient of the best-fit quadratic polynomial is exactly $-\\frac{1}{2}$.", "answer": "$$\\boxed{-\\frac{1}{2}}$$", "id": "3262996"}, {"introduction": "In real-world applications, not all data points are created equal; some measurements are more precise than others. This practice introduces weighted least squares, a crucial extension that incorporates measurement uncertainty into the fitting process by giving more weight to more reliable data. By formulating and solving this problem [@problem_id:3262884], you will learn how to modify the standard least squares objective to find a more statistically sound approximation.", "problem": "A laboratory collects $3$ scalar measurements of an underlying linear response at inputs $x_1=-1$, $x_2=0$, and $x_3=1$. The reported measurements are $y_1=0$, $y_2=1$, and $y_3=2$. The measurement errors are independent, zero-mean, and have known standard deviations $\\sigma_1=1$, $\\sigma_2=\\tfrac{1}{2}$, and $\\sigma_3=1$. You seek a polynomial least squares approximation with a linear polynomial $p(x)=c_0+c_1x$ that accounts for the varying reliability of the measurements by assigning higher weight to more reliable data. Use the diagonal matrix $W=\\mathrm{diag}\\!\\big(\\tfrac{1}{\\sigma_1},\\tfrac{1}{\\sigma_2},\\tfrac{1}{\\sigma_3}\\big)$ to define the objective as the Euclidean two-norm (L2) squared of the weighted residual, that is, minimize $\\|W(Ac-y)\\|_2^2$ where $A$ is the design matrix, $c=\\begin{pmatrix}c_0\\\\c_1\\end{pmatrix}$, and $y=\\begin{pmatrix}y_1\\\\y_2\\\\y_3\\end{pmatrix}$.\n\nTasks:\n1) Write down the explicit matrices and vectors $A$, $W$, and $y$ associated with this problem.\n2) Starting from the fundamental principle of minimizing a differentiable scalar objective by setting its gradient with respect to $c$ to zero, derive the linear system that characterizes the minimizer.\n3) Solve for $c_0$ and $c_1$, and then evaluate the fitted polynomial at $x=2$.\n\nProvide the single final value of $p(2)$ as an exact number. Do not round.", "solution": "The problem is well-posed and scientifically grounded, representing a standard application of the weighted least squares method in numerical analysis. All data and conditions required for a unique solution are provided and are self-consistent.\n\n### Step 1: Problem Validation\n\n**Givens Extracted Verbatim:**\n*   Number of measurements: $3$.\n*   Inputs: $x_1=-1$, $x_2=0$, $x_3=1$.\n*   Measurements: $y_1=0$, $y_2=1$, $y_3=2$.\n*   Standard deviations: $\\sigma_1=1$, $\\sigma_2=\\tfrac{1}{2}$, $\\sigma_3=1$.\n*   Model: linear polynomial $p(x)=c_0+c_1x$.\n*   Weight Matrix: $W=\\mathrm{diag}\\!\\big(\\tfrac{1}{\\sigma_1},\\tfrac{1}{\\sigma_2},\\tfrac{1}{\\sigma_3}\\big)$.\n*   Objective function: minimize $\\|W(Ac-y)\\|_2^2$.\n*   Parameter vector: $c=\\begin{pmatrix}c_0\\\\c_1\\end{pmatrix}$.\n*   Measurement vector: $y=\\begin{pmatrix}y_1\\\\y_2\\\\y_3\\end{pmatrix}$.\n\n**Validation Analysis:**\nThe problem is a well-defined mathematical exercise in weighted linear regression. It is scientifically sound, as the method of weighted least squares is a fundamental statistical technique for fitting models to data with non-uniform variance. The problem is self-contained, providing all necessary numerical values and functional forms. The inputs $x_i$ lead to a design matrix $A$ with linearly independent columns, ensuring that the resulting normal equations have a unique solution, thus making the problem well-posed. The terminology is precise and objective. The problem is valid.\n\n### Step 2: Solution Derivation\n\nThe problem requires finding the coefficients $c_0$ and $c_1$ of the linear polynomial $p(x) = c_0 + c_1x$ that minimizes the squared Euclidean norm of the weighted residual vector.\n\n#### Task 1: Explicit Matrices and Vectors\n\nFirst, we construct the matrices and vectors $A$, $W$, and $y$ from the given data.\n\nThe system of equations is $p(x_i) \\approx y_i$ for $i=1, 2, 3$:\n$$\n\\begin{cases}\nc_0 + c_1(-1) \\approx 0 \\\\\nc_0 + c_1(0) \\approx 1 \\\\\nc_0 + c_1(1) \\approx 2\n\\end{cases}\n$$\nThis can be written in matrix form as $Ac \\approx y$. The design matrix $A$ is formed by evaluating the basis functions ($1$ and $x$) at each input $x_i$:\n$$\nA = \\begin{pmatrix} 1  x_1 \\\\ 1  x_2 \\\\ 1  x_3 \\end{pmatrix} = \\begin{pmatrix} 1  -1 \\\\ 1  0 \\\\ 1  1 \\end{pmatrix}\n$$\nThe vector of measurements $y$ is:\n$$\ny = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\end{pmatrix}\n$$\nThe weight matrix $W$ is given by $W=\\mathrm{diag}\\!\\big(\\tfrac{1}{\\sigma_1},\\tfrac{1}{\\sigma_2},\\tfrac{1}{\\sigma_3}\\big)$. With $\\sigma_1=1$, $\\sigma_2=\\tfrac{1}{2}$, and $\\sigma_3=1$, we have:\n$$\nW = \\mathrm{diag}\\!\\big(\\tfrac{1}{1},\\tfrac{1}{1/2},\\tfrac{1}{1}\\big) = \\mathrm{diag}(1, 2, 1) = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n\n#### Task 2: Derivation of the Linear System\n\nThe objective is to minimize the scalar function $J(c) = \\|W(Ac-y)\\|_2^2$.\nUsing the definition of the Euclidean norm ($ \\|v\\|_2^2 = v^T v $), we can write the objective function as:\n$$\nJ(c) = \\big(W(Ac-y)\\big)^T \\big(W(Ac-y)\\big)\n$$\nUsing the transpose property $(XY)^T = Y^T X^T$:\n$$\nJ(c) = (Ac-y)^T W^T W (Ac-y)\n$$\nTo find the minimum, we compute the gradient of $J(c)$ with respect to $c$ and set it to zero. Let's expand the expression for $J(c)$:\n$$\nJ(c) = (c^T A^T - y^T) W^T W (Ac - y)\n$$\n$$\nJ(c) = c^T A^T W^T W A c - c^T A^T W^T W y - y^T W^T W A c + y^T W^T W y\n$$\nThe terms $c^T A^T W^T W y$ and $y^T W^T W A c$ are scalars, and they are transposes of each other. Therefore, they are equal.\n$$\nJ(c) = c^T (A^T W^T W A) c - 2 c^T (A^T W^T W y) + y^T W^T W y\n$$\nThe gradient with respect to the vector $c$ is:\n$$\n\\nabla_c J(c) = \\nabla_c \\Big( c^T (A^T W^T W A) c - 2 c^T (A^T W^T W y) + y^T W^T W y \\Big)\n$$\nUsing the matrix calculus identities $\\nabla_c(c^T M c) = 2Mc$ for a symmetric matrix $M$ and $\\nabla_c(c^T b) = b$:\n$$\n\\nabla_c J(c) = 2(A^T W^T W A) c - 2(A^T W^T W y)\n$$\nSetting the gradient to zero, $\\nabla_c J(c) = 0$:\n$$\n2(A^T W^T W A) c - 2(A^T W^T W y) = 0\n$$\n$$\n(A^T W^T W A) c = A^T W^T W y\n$$\nThis is the linear system, known as the normal equations for the weighted least squares problem, that characterizes the minimizer $c$.\n\n#### Task 3: Solving for $c_0$, $c_1$ and Evaluating $p(2)$\n\nWe now solve the system $(A^T W^T W A) c = A^T W^T W y$.\nSince $W$ is a real diagonal matrix, $W^T = W$. The system becomes $(A^T W^2 A) c = A^T W^2 y$.\nFirst, we compute the matrix $W^2$:\n$$\nW^2 = W^T W = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  4  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\nThis matrix contains the inverse variances $1/\\sigma_i^2$ on its diagonal, as expected for standard weighted least squares.\n\nNext, we calculate the matrix for the left-hand side, $A^T W^2 A$:\n$$\nA^T W^2 A = \\begin{pmatrix} 1  1  1 \\\\ -1  0  1 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  4  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 1  -1 \\\\ 1  0 \\\\ 1  1 \\end{pmatrix}\n$$\n$$\nA^T W^2 A = \\begin{pmatrix} 1  1  1 \\\\ -1  0  1 \\end{pmatrix} \\begin{pmatrix} 1  -1 \\\\ 4  0 \\\\ 1  1 \\end{pmatrix}\n$$\n$$\nA^T W^2 A = \\begin{pmatrix} (1)(1)+(1)(4)+(1)(1)  (1)(-1)+(1)(0)+(1)(1) \\\\ (-1)(1)+(0)(4)+(1)(1)  (-1)(-1)+(0)(0)+(1)(1) \\end{pmatrix} = \\begin{pmatrix} 6  0 \\\\ 0  2 \\end{pmatrix}\n$$\nNow, we calculate the vector for the right-hand side, $A^T W^2 y$:\n$$\nA^T W^2 y = \\begin{pmatrix} 1  1  1 \\\\ -1  0  1 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  4  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\end{pmatrix}\n$$\n$$\nA^T W^2 y = \\begin{pmatrix} 1  1  1 \\\\ -1  0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 4 \\\\ 2 \\end{pmatrix}\n$$\n$$\nA^T W^2 y = \\begin{pmatrix} (1)(0)+(1)(4)+(1)(2) \\\\ (-1)(0)+(0)(4)+(1)(2) \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 2 \\end{pmatrix}\n$$\nThe linear system to solve for $c = \\begin{pmatrix} c_0 \\\\ c_1 \\end{pmatrix}$ is:\n$$\n\\begin{pmatrix} 6  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} c_0 \\\\ c_1 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 2 \\end{pmatrix}\n$$\nThis diagonal system yields the solution directly:\n$$\n6c_0 = 6 \\implies c_0 = 1\n$$\n$$\n2c_1 = 2 \\implies c_1 = 1\n$$\nThe fitted polynomial is $p(x) = c_0 + c_1 x = 1 + x$.\n\nFinally, we evaluate this polynomial at $x=2$:\n$$\np(2) = 1 + 2 = 3\n$$\nThe data points $(-1, 0)$, $(0, 1)$, and $(1, 2)$ are collinear and lie perfectly on the line $y=x+1$. Thus, the least squares fit, regardless of weighting, yields this exact line, resulting in zero residual error. Our derived coefficients $c_0=1$ and $c_1=1$ are consistent with this observation.", "answer": "$$\\boxed{3}$$", "id": "3262884"}, {"introduction": "While the monomial basis ($1, x, x^2, \\dots$) is simple, it leads to notoriously ill-conditioned Vandermonde matrices, especially for higher-degree polynomials. This computational exercise [@problem_id:3262882] demonstrates a powerful and numerically stable alternative: using an orthogonal basis of Chebyshev polynomials. You will implement a fit using their three-term recurrence relation, a technique that avoids numerical errors and is fundamental to robust scientific computing.", "problem": "Consider a family of approximations where a real-valued function sampled at discrete points is approximated by a polynomial expressed in the Chebyshev basis of the first kind. Let $\\{(x_i, y_i)\\}_{i=1}^n$ be sample pairs with $x_i \\in [a,b]$, and define the affine scaling $s(x) = \\frac{2(x-a)}{b-a} - 1$ to map $[a,b]$ to $[-1,1]$. For a chosen nonnegative integer degree $m$, we form the design matrix $A \\in \\mathbb{R}^{n \\times (m+1)}$ whose $i$-th row contains $[T_0(s_i), T_1(s_i), \\dots, T_m(s_i)]$, where $s_i = s(x_i)$ and the Chebyshev polynomials of the first kind $\\{T_k(x)\\}$ are defined by $T_0(x) = 1$, $T_1(x) = x$, and the recurrence\n$$\nT_{k+1}(x) = 2x T_k(x) - T_{k-1}(x), \\quad k \\ge 1.\n$$\nThe least squares coefficients $c \\in \\mathbb{R}^{m+1}$ minimize the sum of squared residuals\n$$\n\\sum_{i=1}^n \\left( y_i - \\sum_{k=0}^m c_k T_k(s_i) \\right)^2,\n$$\nand the fitted values are $\\hat{y}_i = \\sum_{k=0}^m c_k T_k(s_i)$. The root-mean-square residual is defined as\n$$\n\\mathrm{RMS} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}.\n$$\nYour task is to implement a program that, for each of the test cases below, constructs $A$ incrementally using the Chebyshev recurrence (without explicitly forming powers as in a Vandermonde matrix), solves the least squares problem for $c$, and computes the $\\mathrm{RMS}$ residual. Angles in trigonometric functions must be interpreted in radians.\n\nFundamental base to use:\n- The definition of least squares approximation in Euclidean norm and the corresponding normal equations or equivalent stable solvers.\n- The recurrence definition of Chebyshev polynomials of the first kind on $[-1,1]$.\n- Linear affine scaling from $[a,b]$ to $[-1,1]$.\n\nTest suite:\n- Case $1$ (happy path): $a = 0$, $b = 2$, $n = 64$, $m = 7$, $y_i = \\cos(3 x_i)$ for $x_i$ uniformly spaced in $[a,b]$ including endpoints. Angles are in radians.\n- Case $2$ (exact polynomial fit with endpoints): $a = -5$, $b = 5$, $n = 21$, $m = 3$, $y_i = 1 + 2 x_i + 3 x_i^2 + 4 x_i^3$ for $x_i$ uniformly spaced in $[a,b]$ including endpoints.\n- Case $3$ (degree zero edge case): $a = -1$, $b = 1$, $n = 11$, $m = 0$, $y_i = x_i^2$ for $x_i$ uniformly spaced in $[a,b]$ including endpoints.\n- Case $4$ (wide interval, higher degree): $a = 0$, $b = 10$, $n = 50$, $m = 10$, $y_i = \\exp(x_i)$ for $x_i$ uniformly spaced in $[a,b]$ including endpoints.\n\nOutput specification:\n- Your program should produce a single line of output containing the $\\mathrm{RMS}$ residuals for the four cases as a comma-separated list enclosed in square brackets in the order of the cases above, for example, $[\\mathrm{RMS}_1,\\mathrm{RMS}_2,\\mathrm{RMS}_3,\\mathrm{RMS}_4]$. The results must be real numbers (floats). No other text should be printed.", "solution": "The problem requires the implementation of a polynomial least squares approximation using the Chebyshev basis of the first kind. For a given set of data points $\\{(x_i, y_i)\\}_{i=1}^n$ on an interval $[a,b]$, we seek a polynomial of degree $m$, $p(x) = \\sum_{k=0}^m c_k T_k(s(x))$, that minimizes the sum of squared residuals. Here, $T_k$ are the Chebyshev polynomials and $s(x)$ is an affine map from $[a,b]$ to $[-1,1]$.\n\nThe core of the problem is to find the coefficient vector $c = [c_0, c_1, \\dots, c_m]^T \\in \\mathbb{R}^{m+1}$ that minimizes the Euclidean norm of the residual vector $r = y - \\hat{y}$, where $y = [y_1, \\dots, y_n]^T$ and $\\hat{y}$ are the fitted values. This can be formulated as a linear least squares problem:\n$$\n\\min_{c \\in \\mathbb{R}^{m+1}} \\|y - Ac\\|_2^2\n$$\nwhere $A$ is the $n \\times (m+1)$ design matrix. The solution to this problem is found by solving the system for $c$. While the normal equations, $A^T A c = A^T y$, provide a theoretical solution, more numerically stable methods such as QR decomposition or Singular Value Decomposition (SVD) are preferred in practice. We will use a standard library function that implements such a stable solver.\n\nThe overall algorithm proceeds in four main steps for each test case:\n1.  **Data Generation and Scaling**: The independent variable samples $x_i$ are generated uniformly over the specified interval $[a,b]$. The corresponding dependent variable samples $y_i$ are computed from the given function. Each $x_i$ is then scaled to $s_i \\in [-1,1]$ using the provided affine map:\n    $$\n    s_i = \\frac{2(x_i-a)}{b-a} - 1\n    $$\n    This scaling is crucial as the Chebyshev polynomials $\\{T_k(x)\\}$ are orthogonal over $[-1,1]$ with respect to the weight function $1/\\sqrt{1-x^2}$, which leads to better-conditioned matrices compared to using a monomial basis on arbitrary intervals.\n\n2.  **Design Matrix Construction**: The $n \\times (m+1)$ design matrix $A$ is constructed such that its entry $A_{ik}$ (using $0$-based indexing for column $k$) is $T_k(s_i)$. The columns of $A$ are the evaluations of the Chebyshev polynomials at the scaled points $s_i$. We construct these columns incrementally using the three-term recurrence relation for Chebyshev polynomials of the first kind:\n    $$\n    T_0(x) = 1 \\\\\n    T_1(x) = x \\\\\n    T_{k+1}(x) = 2x T_k(x) - T_{k-1}(x), \\quad \\text{for } k \\ge 1\n    $$\n    Let $A_{:,k}$ denote the $k$-th column of $A$. The construction proceeds as follows:\n    -   The first column ($k=0$) is $A_{:,0} = [T_0(s_1), \\dots, T_0(s_n)]^T$, which is a vector of ones.\n    -   The second column ($k=1$) is $A_{:,1} = [T_1(s_1), \\dots, T_1(s_n)]^T$, which is the vector of scaled points $s = [s_1, \\dots, s_n]^T$.\n    -   Subsequent columns for $k=2, \\dots, m$ are generated via the recurrence:\n        $$\n        A_{:,k} = 2 \\cdot s \\odot A_{:,k-1} - A_{:,k-2}\n        $$\n        where $\\odot$ denotes element-wise multiplication. This approach avoids the potential numerical instability associated with forming a Vandermonde-like matrix from powers of $s_i$. Note that for $m=0$, only the first column is needed.\n\n3.  **Solving for Coefficients**: With the design matrix $A$ and the data vector $y$ constructed, we solve the linear least squares problem $Ac \\approx y$. A robust numerical solver is used to find the coefficient vector $c \\in \\mathbb{R}^{m+1}$.\n\n4.  **Root-Mean-Square (RMS) Residual Calculation**: Once the optimal coefficients $c$ are determined, the vector of fitted values $\\hat{y}$ is computed by the matrix-vector product $\\hat{y} = Ac$. The residual vector is then $r = y - \\hat{y}$. Finally, the root-mean-square residual, a measure of the average error of the fit, is calculated according to its definition:\n    $$\n    \\mathrm{RMS} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2} = \\sqrt{\\frac{1}{n} r^T r} = \\frac{\\|r\\|_2}{\\sqrt{n}}\n    $$\nThis procedure is applied to each of the four test cases specified in the problem statement to obtain the required $\\mathrm{RMS}$ values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_rms_chebyshev(a, b, n, m, func):\n    \"\"\"\n    Performs least squares approximation using the Chebyshev basis and computes the RMS residual.\n\n    Args:\n        a (float): The start of the interval.\n        b (float): The end of the interval.\n        n (int): The number of sample points.\n        m (int): The degree of the approximating polynomial.\n        func (callable): The function to generate y values from x values.\n\n    Returns:\n        float: The root-mean-square (RMS) residual.\n    \"\"\"\n    # Step 1: Data Generation and Scaling\n    if n == 1:\n        # Handle the edge case of a single point to avoid division by zero in linspace\n        # for interval length  0. A single point is an endpoint.\n        x = np.array([a])\n    else:\n        # Generate n uniformly spaced points in [a,b] including endpoints\n        x = np.linspace(a, b, n)\n    \n    y = func(x)\n\n    # Affine scaling from [a,b] to [-1,1]\n    # Handle the case where a == b to avoid division by zero.\n    if a == b:\n        # If the interval is a single point, scaling maps to 0 if we interpret it\n        # as the center of a zero-width interval [-1,1].\n        s = np.zeros_like(x)\n    else:\n        s = (2.0 * (x - a) / (b - a)) - 1.0\n\n    # Step 2: Design Matrix Construction\n    # The design matrix A has n rows and m+1 columns.\n    A = np.zeros((n, m + 1))\n\n    # T_0(s) = 1\n    A[:, 0] = 1.0\n\n    # If m  0, compute T_1(s) = s\n    if m  0:\n        A[:, 1] = s\n\n    # Use the recurrence T_{k+1}(s) = 2*s*T_k(s) - T_{k-1}(s) for k = 1\n    # This corresponds to A[:, k+1] = 2 * s * A[:, k] - A[:, k-1] for k = 1\n    for k in range(1, m):\n        A[:, k + 1] = 2.0 * s * A[:, k] - A[:, k - 1]\n\n    # Step 3: Solving for Coefficients\n    # Solve the least squares problem Ac = y for the coefficients c.\n    # np.linalg.lstsq uses a stable SVD-based algorithm.\n    c, _, _, _ = np.linalg.lstsq(A, y, rcond=None)\n\n    # Step 4: Root-Mean-Square (RMS) Residual Calculation\n    # Calculate the fitted values y_hat = A @ c\n    y_hat = A @ c\n    \n    # Calculate the residuals\n    residuals = y - y_hat\n    \n    # Calculate the RMS residual\n    rms = np.sqrt(np.mean(residuals**2))\n    \n    return rms\n\ndef solve():\n    \"\"\"\n    Defines and runs the test cases, printing the results in the specified format.\n    \"\"\"\n    # Test suite from the problem statement\n    test_cases = [\n        # Case 1: a=0, b=2, n=64, m=7, y=cos(3x)\n        {'a': 0.0, 'b': 2.0, 'n': 64, 'm': 7, 'func': lambda x: np.cos(3.0 * x)},\n        \n        # Case 2: a=-5, b=5, n=21, m=3, y=1+2x+3x^2+4x^3\n        {'a': -5.0, 'b': 5.0, 'n': 21, 'm': 3, 'func': lambda x: 1.0 + 2.0*x + 3.0*x**2 + 4.0*x**3},\n        \n        # Case 3: a=-1, b=1, n=11, m=0, y=x^2\n        {'a': -1.0, 'b': 1.0, 'n': 11, 'm': 0, 'func': lambda x: x**2},\n        \n        # Case 4: a=0, b=10, n=50, m=10, y=exp(x)\n        {'a': 0.0, 'b': 10.0, 'n': 50, 'm': 10, 'func': lambda x: np.exp(x)},\n    ]\n\n    results = []\n    for case in test_cases:\n        rms_val = calculate_rms_chebyshev(case['a'], case['b'], case['n'], case['m'], case['func'])\n        results.append(rms_val)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3262882"}]}