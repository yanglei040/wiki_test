{"hands_on_practices": [{"introduction": "To truly grasp what makes a linear system ill-conditioned, there is no better way than to build one from its fundamental components. This exercise challenges you to construct a matrix by directly manipulating its singular values using the Singular Value Decomposition (SVD) framework. By choosing singular values that are orders of magnitude apart, you will create a system that is highly sensitive to perturbation and see firsthand how the condition number, a key measure of this sensitivity, is determined entirely by the ratio of the largest to the smallest singular value [@problem_id:3280607].", "problem": "Let $A \\in \\mathbb{R}^{5 \\times 5}$ be constructed using the Singular Value Decomposition (SVD), that is, $A = U \\Sigma V^{\\mathsf{T}}$, where $U \\in \\mathbb{R}^{5 \\times 5}$ and $V \\in \\mathbb{R}^{5 \\times 5}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{5 \\times 5}$ is diagonal with nonnegative diagonal entries $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge \\sigma_{5} \\ge 0$. \n\nYour task is to design $A$ so that its singular values consist of two well-separated clusters: three singular values equal to $1$ and two singular values equal to $10^{-16}$. Then analyze the conditioning of the corresponding linear system.\n\nTasks:\n1. Using only the definition of the SVD and orthogonal matrices, explicitly construct one such matrix $A$ by specifying valid choices of $U$, $\\Sigma$, and $V$ that achieve the requested singular value clustering. Explain why your construction indeed has the desired singular values.\n2. Starting from the definition of the vector $2$-norm and the induced matrix $2$-norm, and using only properties of orthogonal transformations, derive an expression for the $2$-norm condition number $\\kappa_{2}(A)$ of a nonsingular matrix in terms of the singular values of $A$.\n3. Evaluate $\\kappa_{2}(A)$ exactly for your constructed matrix. State your final result as a single number. No rounding is required.", "solution": "### Part 1: Construction of the Matrix $A$\n\nThe problem requires the construction of a matrix $A \\in \\mathbb{R}^{5 \\times 5}$ of the form $A = U \\Sigma V^{\\mathsf{T}}$, where $U$ and $V$ are orthogonal matrices and $\\Sigma$ is a diagonal matrix of singular values. The specified singular values are three instances of $1$ and two instances of $10^{-16}$. By convention, the singular values are ordered non-increasingly along the diagonal of $\\Sigma$.\n\nLet the singular values be $\\sigma_1, \\sigma_2, \\sigma_3, \\sigma_4, \\sigma_5$. According to the problem statement, we must have:\n$$\n\\sigma_1 = \\sigma_2 = \\sigma_3 = 1\n$$\n$$\n\\sigma_4 = \\sigma_5 = 10^{-16}\n$$\nThese values are all non-negative, satisfying the definition of singular values. The matrix $\\Sigma \\in \\mathbb{R}^{5 \\times 5}$ is therefore defined as:\n$$\n\\Sigma = \\operatorname{diag}(1, 1, 1, 10^{-16}, 10^{-16}) =\n\\begin{pmatrix}\n1  0  0  0  0 \\\\\n0  1  0  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  0  10^{-16}  0 \\\\\n0  0  0  0  10^{-16}\n\\end{pmatrix}\n$$\nThe problem requires specifying valid choices for the orthogonal matrices $U$ and $V$. The simplest choice for an orthogonal matrix is the identity matrix, $I_5 \\in \\mathbb{R}^{5 \\times 5}$. A matrix $Q$ is orthogonal if $Q^{\\mathsf{T}}Q = I$. For the identity matrix, $I_5^{\\mathsf{T}}I_5 = I_5 I_5 = I_5$, so it is indeed orthogonal.\n\nWe choose $U = I_5$ and $V = I_5$.\n\nWith these choices, the matrix $A$ is constructed as:\n$$\nA = U \\Sigma V^{\\mathsf{T}} = I_5 \\Sigma I_5^{\\mathsf{T}} = \\Sigma\n$$\nThus, one such matrix $A$ that satisfies the given conditions is:\n$$\nA =\n\\begin{pmatrix}\n1  0  0  0  0 \\\\\n0  1  0  0  0 \\\\\n0  0  1  0  0 \\\\\n0  0  0  10^{-16}  0 \\\\\n0  0  0  0  10^{-16}\n\\end{pmatrix}\n$$\nBy definition, the singular values of a matrix are the diagonal entries of the $\\Sigma$ matrix in its Singular Value Decomposition. Our construction $A = I_5 \\Sigma I_5^{\\mathsf{T}}$ is a valid SVD, and the diagonal entries of $\\Sigma$ are precisely the values required by the problem. Therefore, this construction is valid and achieves the desired singular value clustering.\n\n### Part 2: Derivation of the Condition Number Expression\n\nThe $2$-norm condition number of a nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$ is defined as $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. The induced matrix $2$-norm is defined as $\\|A\\|_2 = \\max_{\\|x\\|_2=1} \\|Ax\\|_2$, where $\\|x\\|_2 = \\sqrt{x^{\\mathsf{T}}x}$ is the Euclidean vector norm.\n\nLet the Singular Value Decomposition of $A$ be $A = U \\Sigma V^{\\mathsf{T}}$, where $U, V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma = \\text{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_n)$ with $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n \\ge 0$.\n\nFirst, we derive an expression for $\\|A\\|_2$. Consider the squared norm $\\|Ax\\|_2^2$ for a vector $x$ such that $\\|x\\|_2 = 1$:\n$$\n\\|Ax\\|_2^2 = \\|U \\Sigma V^{\\mathsf{T}} x\\|_2^2\n$$\nSince $U$ is an orthogonal matrix, it preserves the $2$-norm, meaning $\\|Uy\\|_2 = \\|y\\|_2$ for any vector $y$. Applying this property, we get:\n$$\n\\|Ax\\|_2^2 = \\|\\Sigma V^{\\mathsf{T}} x\\|_2^2\n$$\nLet us define a new vector $y = V^{\\mathsf{T}} x$. Since $V$ is orthogonal, $V^{\\mathsf{T}}$ is also orthogonal. Thus, $V^{\\mathsf{T}}$ also preserves the $2$-norm: $\\|y\\|_2 = \\|V^{\\mathsf{T}} x\\|_2 = \\|x\\|_2 = 1$. As $x$ varies over all vectors of unit length, the corresponding vector $y$ also varies over all vectors of unit length. Therefore, the problem of finding $\\|A\\|_2$ is equivalent to finding the maximum of $\\|\\Sigma y\\|_2$ for $\\|y\\|_2=1$.\n\nLet's expand $\\|\\Sigma y\\|_2^2$:\n$$\n\\|\\Sigma y\\|_2^2 = (\\Sigma y)^{\\mathsf{T}}(\\Sigma y) = y^{\\mathsf{T}}\\Sigma^{\\mathsf{T}}\\Sigma y = y^{\\mathsf{T}}\\Sigma^2 y\n$$\nSince $\\Sigma$ is diagonal, $\\Sigma^2 = \\text{diag}(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_n^2)$. The expression becomes a weighted sum:\n$$\n\\|\\Sigma y\\|_2^2 = \\sum_{i=1}^{n} \\sigma_i^2 y_i^2\n$$\nWe want to maximize this quantity subject to the constraint $\\|y\\|_2^2 = \\sum_{i=1}^{n} y_i^2 = 1$. Since the singular values are ordered $\\sigma_1^2 \\ge \\sigma_2^2 \\ge \\dots \\ge \\sigma_n^2$, we can establish an upper bound:\n$$\n\\sum_{i=1}^{n} \\sigma_i^2 y_i^2 \\le \\sigma_1^2 \\sum_{i=1}^{n} y_i^2 = \\sigma_1^2 \\cdot 1 = \\sigma_1^2\n$$\nThis maximum value is achieved when $y$ is the first standard basis vector $e_1 = (1, 0, \\dots, 0)^{\\mathsf{T}}$.\nTherefore, $\\max_{\\|y\\|_2=1} \\|\\Sigma y\\|_2^2 = \\sigma_1^2$, which implies that $\\|A\\|_2 = \\sqrt{\\sigma_1^2} = \\sigma_1$. This is the largest singular value of $A$, denoted $\\sigma_{\\max}(A)$. So, $\\|A\\|_2 = \\sigma_{\\max}(A)$.\n\nNext, we derive an expression for $\\|A^{-1}\\|_2$. Since $A$ is nonsingular, all its singular values must be strictly positive: $\\sigma_n  0$. The inverse of $A$ is:\n$$\nA^{-1} = (U \\Sigma V^{\\mathsf{T}})^{-1} = (V^{\\mathsf{T}})^{-1} \\Sigma^{-1} U^{-1} = V \\Sigma^{-1} U^{\\mathsf{T}}\n$$\nThis expression is a valid SVD for $A^{-1}$, where the orthogonal matrices are $V$ and $U$, and the diagonal matrix is $\\Sigma^{-1} = \\text{diag}(1/\\sigma_1, 1/\\sigma_2, \\dots, 1/\\sigma_n)$. The singular values of $A^{-1}$ are the diagonal entries of $\\Sigma^{-1}$. To find the largest singular value of $A^{-1}$, we must identify the largest element among $\\{1/\\sigma_i\\}$. Since $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n  0$, it follows that $1/\\sigma_n \\ge 1/\\sigma_{n-1} \\ge \\dots \\ge 1/\\sigma_1  0$.\nThe largest singular value of $A^{-1}$ is $\\sigma_{\\max}(A^{-1}) = 1/\\sigma_n$, which is the reciprocal of the smallest singular value of $A$, $\\sigma_{\\min}(A)$.\n\nUsing the result we derived for the $2$-norm of a matrix, we have:\n$$\n\\|A^{-1}\\|_2 = \\sigma_{\\max}(A^{-1}) = \\frac{1}{\\sigma_{\\min}(A)}\n$$\nFinally, we combine these results to find the condition number $\\kappa_2(A)$:\n$$\n\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2 = \\sigma_{\\max}(A) \\cdot \\frac{1}{\\sigma_{\\min}(A)} = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}\n$$\nThis is the desired expression for the $2$-norm condition number in terms of the singular values of $A$.\n\n### Part 3: Evaluation of $\\kappa_2(A)$\n\nFor the matrix $A$ constructed in Part 1, the singular values are given as:\n$$\n\\sigma_1 = 1, \\quad \\sigma_2 = 1, \\quad \\sigma_3 = 1, \\quad \\sigma_4 = 10^{-16}, \\quad \\sigma_5 = 10^{-16}\n$$\nThe largest singular value is $\\sigma_{\\max}(A) = \\sigma_1 = 1$.\nThe smallest singular value is $\\sigma_{\\min}(A) = \\sigma_5 = 10^{-16}$.\nThe matrix $A$ is nonsingular because its smallest singular value is greater than $0$.\n\nUsing the formula derived in Part 2, the $2$-norm condition number of our constructed matrix $A$ is:\n$$\n\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)} = \\frac{1}{10^{-16}} = 10^{16}\n$$\nThe condition number is extremely large, indicating that the linear system $Ax=b$ with this matrix $A$ is severely ill-conditioned. Small perturbations in $b$ can lead to very large changes in the solution $x$, a characteristic behavior explained by the vast ratio between its largest and smallest singular values.", "answer": "$$\\boxed{10^{16}}$$", "id": "3280607"}, {"introduction": "Ill-conditioning often arises in data analysis when features are redundant or highly correlated. This practice provides an intuitive example of this phenomenon by creating a rank-deficient matrix simply by duplicating a column, simulating perfect multicollinearity in a regression problem [@problem_id:3280708]. By deriving the SVD for this matrix, you will discover how it precisely identifies the data redundancy within its null space and how the Moore-Penrose pseudoinverse provides the unique minimum-norm solution, elegantly distributing influence across the identical features.", "problem": "Consider a linear regression design matrix with two features represented by the columns of the matrix $A_{0} \\in \\mathbb{R}^{3 \\times 2}$,\n$$\nA_{0} = \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  0\n\\end{pmatrix}.\n$$\nNow form a new matrix $A \\in \\mathbb{R}^{3 \\times 3}$ by duplicating the first column of $A_{0}$, so that\n$$\nA = \\begin{pmatrix}\n1  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix}.\n$$\nLet the observed response vector be\n$$\n\\mathbf{b} = \\begin{pmatrix}\n2 \\\\\n3 \\\\\n0\n\\end{pmatrix}.\n$$\nStarting from the core definitions of the Singular Value Decomposition (SVD) and the Mooreâ€“Penrose pseudoinverse, analyze how duplicating a feature (column) affects the singular values and the conditioning of the matrix. Derive an explicit Singular Value Decomposition of $A$, identify the zero singular value and the right singular vector associated with the duplicate-feature nullspace, and use this to obtain the minimum-norm least squares solution $\\mathbf{x}^{+}$ to $A \\mathbf{x} = \\mathbf{b}$. Explain, from first principles, how the pseudoinverse redistributes weight across duplicate columns.\n\nReport, as your final answer, the value of the first component $x_{1}^{+}$ of the minimum-norm least squares solution. No rounding is required, and no units are involved.", "solution": "The minimum-norm least squares solution $\\mathbf{x}^{+}$ to the system $A\\mathbf{x} = \\mathbf{b}$ is given by $\\mathbf{x}^{+} = A^{+}\\mathbf{b}$, where $A^{+}$ is the Moore-Penrose pseudoinverse of $A$. We compute $A^{+}$ from the Singular Value Decomposition (SVD) of $A$, which is $A = U \\Sigma V^T$.\n\n**1. Compute the SVD of $A$**\n\nThe SVD requires finding the matrices $U$, $\\Sigma$, and $V$. We start by computing $A^T A$.\n$$\nA^T = \\begin{pmatrix}\n1  0  0 \\\\\n1  0  0 \\\\\n0  1  0\n\\end{pmatrix}\n$$\n$$\nA^T A = \\begin{pmatrix}\n1  0  0 \\\\\n1  0  0 \\\\\n0  1  0\n\\end{pmatrix}\n\\begin{pmatrix}\n1  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix}\n= \\begin{pmatrix}\n1  1  0 \\\\\n1  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\nThe singular values $\\sigma_i$ of $A$ are the square roots of the eigenvalues $\\lambda_i$ of $A^T A$. The columns of $V$ are the corresponding normalized eigenvectors. We find the eigenvalues by solving the characteristic equation $\\det(A^T A - \\lambda I) = 0$.\n$$\n\\det \\begin{pmatrix}\n1-\\lambda  1  0 \\\\\n1  1-\\lambda  0 \\\\\n0  0  1-\\lambda\n\\end{pmatrix} = (1-\\lambda) \\left[ (1-\\lambda)^2 - 1 \\right] = 0\n$$\n$$\n(1-\\lambda) (\\lambda^2 - 2\\lambda + 1 - 1) = 0\n$$\n$$\n(1-\\lambda) (\\lambda^2 - 2\\lambda) = 0\n$$\n$$\n\\lambda (1-\\lambda) (\\lambda - 2) = 0\n$$\nThe eigenvalues are $\\lambda_1 = 2$, $\\lambda_2 = 1$, and $\\lambda_3 = 0$.\nThe singular values are their square roots, ordered from largest to smallest:\n$\\sigma_1 = \\sqrt{2}$, $\\sigma_2 = \\sqrt{1} = 1$, $\\sigma_3 = \\sqrt{0} = 0$.\nThe matrix $\\Sigma$ is a $3 \\times 3$ diagonal matrix with these singular values:\n$$\n\\Sigma = \\begin{pmatrix}\n\\sqrt{2}  0  0 \\\\\n0  1  0 \\\\\n0  0  0\n\\end{pmatrix}\n$$\nNow, we find the eigenvectors of $A^T A$ to form the columns of $V$.\n\n- For $\\lambda_1 = 2$: $(A^T A - 2I)\\mathbf{v}_1 = \\mathbf{0}$\n$$\n\\begin{pmatrix}\n-1  1  0 \\\\\n1  -1  0 \\\\\n0  0  -1\n\\end{pmatrix}\n\\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\mathbf{0} \\implies\n\\begin{cases} -x+y=0 \\\\ z=0 \\end{cases}\n$$\nAn eigenvector is $\\begin{pmatrix} 1  1  0 \\end{pmatrix}^T$. Normalizing it gives $\\mathbf{v}_1 = \\begin{pmatrix} 1/\\sqrt{2}  1/\\sqrt{2}  0 \\end{pmatrix}^T$.\n\n- For $\\lambda_2 = 1$: $(A^T A - 1I)\\mathbf{v}_2 = \\mathbf{0}$\n$$\n\\begin{pmatrix}\n0  1  0 \\\\\n1  0  0 \\\\\n0  0  0\n\\end{pmatrix}\n\\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\mathbf{0} \\implies\n\\begin{cases} y=0 \\\\ x=0 \\end{cases}\n$$\nAn eigenvector is $\\begin{pmatrix} 0  0  1 \\end{pmatrix}^T$. It is already normalized, so $\\mathbf{v}_2 = \\begin{pmatrix} 0  0  1 \\end{pmatrix}^T$.\n\n- For $\\lambda_3 = 0$: $(A^T A - 0I)\\mathbf{v}_3 = \\mathbf{0}$\n$$\n\\begin{pmatrix}\n1  1  0 \\\\\n1  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\mathbf{0} \\implies\n\\begin{cases} x+y=0 \\\\ z=0 \\end{cases}\n$$\nAn eigenvector is $\\begin{pmatrix} 1  -1  0 \\end{pmatrix}^T$. Normalizing gives $\\mathbf{v}_3 = \\begin{pmatrix} 1/\\sqrt{2}  -1/\\sqrt{2}  0 \\end{pmatrix}^T$. This vector spans the null space of $A$ and represents the linear dependency between the first two columns.\n\nCombining these normalized eigenvectors, we get $V$:\n$$\nV = \\begin{pmatrix}\n\\mathbf{v}_1  \\mathbf{v}_2  \\mathbf{v}_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n1/\\sqrt{2}  0  1/\\sqrt{2} \\\\\n1/\\sqrt{2}  0  -1/\\sqrt{2} \\\\\n0  1  0\n\\end{pmatrix}\n$$\nThe columns of $U$ are given by $\\mathbf{u}_i = \\frac{1}{\\sigma_i} A \\mathbf{v}_i$ for $\\sigma_i  0$.\n$$\n\\mathbf{u}_1 = \\frac{1}{\\sqrt{2}} A \\mathbf{v}_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix}\n1  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix}\n\\begin{pmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\\\ 0 \\end{pmatrix}\n= \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2/\\sqrt{2} \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\mathbf{u}_2 = \\frac{1}{1} A \\mathbf{v}_2 = \\begin{pmatrix}\n1  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n= \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nSince $U$ must be an orthonormal matrix, $\\mathbf{u}_3$ must be a unit vector orthogonal to $\\mathbf{u}_1$ and $\\mathbf{u}_2$. The obvious choice is $\\mathbf{u}_3 = \\begin{pmatrix} 0  0  1 \\end{pmatrix}^T$. This vector spans the null space of $A^T$.\n$$\nU = \\begin{pmatrix}\n\\mathbf{u}_1  \\mathbf{u}_2  \\mathbf{u}_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} = I\n$$\n\n**2. Compute the Pseudoinverse $A^{+}$**\n\nThe pseudoinverse $A^{+}$ is given by $A^{+} = V \\Sigma^{+} U^T$. The matrix $\\Sigma^{+}$ is obtained by taking the reciprocal of the non-zero singular values in $\\Sigma$ and transposing the result.\n$$\n\\Sigma^{+} = \\begin{pmatrix}\n1/\\sqrt{2}  0  0 \\\\\n0  1  0 \\\\\n0  0  0\n\\end{pmatrix}\n$$\nSince $U = I$, we have $U^T = I$.\n$$\nA^{+} = V \\Sigma^{+} I = V \\Sigma^{+} = \n\\begin{pmatrix}\n1/\\sqrt{2}  0  1/\\sqrt{2} \\\\\n1/\\sqrt{2}  0  -1/\\sqrt{2} \\\\\n0  1  0\n\\end{pmatrix}\n\\begin{pmatrix}\n1/\\sqrt{2}  0  0 \\\\\n0  1  0 \\\\\n0  0  0\n\\end{pmatrix}\n$$\n$$\nA^{+} = \\begin{pmatrix}\n(1/\\sqrt{2})(1/\\sqrt{2})  0  0 \\\\\n(1/\\sqrt{2})(1/\\sqrt{2})  0  0 \\\\\n0  1  0\n\\end{pmatrix}\n= \\begin{pmatrix}\n1/2  0  0 \\\\\n1/2  0  0 \\\\\n0  1  0\n\\end{pmatrix}\n$$\n\n**3. Compute the Minimum-Norm Least Squares Solution $\\mathbf{x}^{+}$**\n\nThe solution is $\\mathbf{x}^{+} = A^{+} \\mathbf{b}$.\n$$\n\\mathbf{x}^{+} = \\begin{pmatrix}\n1/2  0  0 \\\\\n1/2  0  0 \\\\\n0  1  0\n\\end{pmatrix}\n\\begin{pmatrix}\n2 \\\\\n3 \\\\\n0\n\\end{pmatrix}\n= \\begin{pmatrix}\n(1/2)(2) + (0)(3) + (0)(0) \\\\\n(1/2)(2) + (0)(3) + (0)(0) \\\\\n(0)(2) + (1)(3) + (0)(0)\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 \\\\\n1 \\\\\n3\n\\end{pmatrix}\n$$\nSo the minimum-norm least squares solution is $\\mathbf{x}^{+} = \\begin{pmatrix} 1  1  3 \\end{pmatrix}^T$. The first component is $x_1^{+} = 1$.\n\n**4. Explanation of Weight Redistribution**\n\nThe original problem using $A_0$ would have coefficients for two features. The least squares solution for $A_0 \\mathbf{x}' = \\mathbf{b}$ would assign a coefficient of $2$ to the first feature and $3$ to the second feature.\n\nIn the modified problem $A\\mathbf{x}=\\mathbf{b}$, the first feature is now represented by two identical columns (the first and second columns of $A$), with associated coefficients $x_1$ and $x_2$. The second feature is represented by the third column, with coefficient $x_3$. The total contribution of the first feature to the model is $(x_1 + x_2)$ times the feature vector.\n\nThe normal equations for the least squares problem are $A^T A \\mathbf{x} = A^T \\mathbf{b}$.\n$$\n\\begin{pmatrix}\n1  1  0 \\\\\n1  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}\n=\n\\begin{pmatrix}\n1  0  0 \\\\\n1  0  0 \\\\\n0  1  0\n\\end{pmatrix}\n\\begin{pmatrix} 2 \\\\ 3 \\\\ 0 \\end{pmatrix}\n=\n\\begin{pmatrix} 2 \\\\ 2 \\\\ 3 \\end{pmatrix}\n$$\nThis gives the system:\n$$\n\\begin{cases}\nx_1 + x_2 = 2 \\\\\nx_1 + x_2 = 2 \\\\\nx_3 = 3\n\\end{cases}\n$$\nThis defines an affine subspace of least squares solutions. Any vector of the form $\\mathbf{x}_{LS} = \\begin{pmatrix} c  2-c  3 \\end{pmatrix}^T$ for any scalar $c$ minimizes the residual norm $\\|A\\mathbf{x} - \\mathbf{b}\\|$.\n\nThe Moore-Penrose pseudoinverse selects the unique solution from this subspace that has the minimum Euclidean norm $\\|\\mathbf{x}_{LS}\\|$. We need to minimize $\\|\\mathbf{x}_{LS}\\|^2 = c^2 + (2-c)^2 + 3^2$.\n$$\nf(c) = c^2 + 4 - 4c + c^2 + 9 = 2c^2 - 4c + 13\n$$\nTo find the minimum, we set the derivative to zero: $f'(c) = 4c - 4 = 0$, which yields $c=1$.\nSubstituting $c=1$ back gives the minimum-norm solution:\n$$\n\\mathbf{x}^{+} = \\begin{pmatrix} 1  2-1  3 \\end{pmatrix}^T = \\begin{pmatrix} 1  1  3 \\end{pmatrix}^T\n$$\nThis confirms our result from the SVD. The total coefficient weight for the first feature, which is $2$, has been distributed equally between the two duplicate columns ($x_1^{+} = 1$, $x_2^{+} = 1$) because this configuration minimizes the norm $x_1^2+x_2^2$ subject to the constraint $x_1+x_2=2$. This is a general property of the pseudoinverse solution for problems with duplicated features.\n\nThe final requested value is the first component of this solution, $x_1^{+}$.", "answer": "$$\\boxed{1}$$", "id": "3280708"}, {"introduction": "Theoretical exercises with clean data are essential, but real-world problems are almost always contaminated with noise. This computational practice bridges the gap between theory and application by simulating the effect of measurement noise on an ill-conditioned system solved with Truncated SVD (TSVD) [@problem_id:3280674]. By systematically increasing the noise level in the right-hand side vector and observing the resulting solution error, you will gain a practical appreciation for the central challenge of ill-posed problems: the amplification of noise, and understand why regularization is a critical but delicate balancing act.", "problem": "You are given a square, ill-conditioned linear system represented by a real matrix $A \\in \\mathbb{R}^{n \\times n}$ and a right-hand-side vector $b \\in \\mathbb{R}^{n}$. The goal is to investigate, for a fixed truncation level $k$, how the quality of the Truncated Singular Value Decomposition (TSVD) solution to $A x = b$ deteriorates as the noise level in $b$ increases. Your program must be a complete and runnable implementation that constructs a specific ill-conditioned matrix, generates controlled noisy right-hand-sides, computes TSVD solutions, and reports a quantitative measure of solution quality across a predefined test suite.\n\nUse the following fundamental base and setup:\n- Singular Value Decomposition (SVD) is defined for any real matrix $A$ as $A = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthogonal and $\\Sigma$ is diagonal with nonnegative entries.\n- For a fixed truncation level $k$, the TSVD solution is defined as the least-squares solution obtained by restricting to the dominant $k$ singular components of $A$.\n- The matrix $A$ is constructed to be ill-conditioned by prescribing rapidly decaying singular values.\n\nFollow this deterministic construction:\n1. Choose the problem size $n$ as $n = 30$ and fix the truncation level as $k = 10$.\n2. Construct orthogonal matrices $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{n \\times n}$ as the $Q$ factors from the QR decomposition of two independent matrices with independently and identically distributed standard normal entries, each of size $n \\times n$. Use a fixed pseudorandom seed to ensure determinism for both matrices, with seed $0$ for $U$ and seed $1$ for $V$.\n3. Define singular values $\\sigma_i$ for $i = 1, 2, \\dots, n$ as $\\sigma_i = 10^{-\\frac{i-1}{2}}$, and form the diagonal matrix $\\Sigma = \\operatorname{diag}(\\sigma_1, \\dots, \\sigma_n)$.\n4. Set $A = U \\Sigma V^{\\top}$.\n5. Define the true solution vector $x_{\\text{true}} \\in \\mathbb{R}^{n}$ by $x_{\\text{true},i} = \\frac{1}{i}$ for $i = 1, 2, \\dots, n$, and compute the clean right-hand-side $b_{\\text{clean}} = A x_{\\text{true}}$.\n6. Fix a deterministic noise direction $z \\in \\mathbb{R}^{n}$ generated as a standard normal vector with seed $7$, normalized to unit Euclidean norm, i.e., $z \\leftarrow z / \\|z\\|_2$.\n7. For each noise fraction $\\eta$, define the noisy right-hand-side $b(\\eta) = b_{\\text{clean}} + \\eta \\|b_{\\text{clean}}\\|_2 \\, z$.\n\nImplement the TSVD solution at rank $k$ for each $b(\\eta)$ and quantify solution quality using the relative solution error:\n$$\ne(\\eta) = \\frac{\\|x_k(\\eta) - x_{\\text{true}}\\|_2}{\\|x_{\\text{true}}\\|_2},\n$$\nwhere $x_k(\\eta)$ denotes the TSVD solution obtained from $b(\\eta)$ at truncation level $k$.\n\nTest Suite:\n- Use the following set of noise fractions $\\eta$ to probe different regimes, including a boundary case and increasing noise magnitudes:\n$$\n\\{\\eta_1, \\eta_2, \\eta_3, \\eta_4, \\eta_5, \\eta_6\\} = \\{0.0, 10^{-8}, 10^{-4}, 10^{-2}, 10^{-1}, 1.0\\}.\n$$\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered according to the test suite noise fractions:\n$$\n[e(\\eta_1), e(\\eta_2), e(\\eta_3), e(\\eta_4), e(\\eta_5), e(\\eta_6)].\n$$\n- The outputs are dimensionless floats. No physical units are involved.", "solution": "The core of the problem lies in solving the linear system $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is an ill-conditioned matrix, $x \\in \\mathbb{R}^{n}$ is the vector of unknowns, and $b \\in \\mathbb{R}^{n}$ is the vector of observations.\n\nThe Singular Value Decomposition (SVD) of the matrix $A$ provides the necessary framework for analysis and solution. The SVD is given by $A = U \\Sigma V^{\\top}$, where $U, V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices ($U^{\\top}U = I$, $V^{\\top}V = I$) and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix with non-negative, non-increasing entries $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n \\ge 0$ on its diagonal. These entries are the singular values of $A$. The columns of $U$ are the left singular vectors, and the columns of $V$ are the right singular vectors.\n\nSubstituting the SVD into the linear system yields $U \\Sigma V^{\\top} x = b$. A formal solution can be derived by isolating $x$:\n$$\nx = V \\Sigma^{-1} U^{\\top} b\n$$\nThis can be expressed as a summation:\n$$\nx = \\sum_{i=1}^{n} \\frac{u_i^{\\top} b}{\\sigma_i} v_i\n$$\nwhere $u_i$ and $v_i$ are the $i$-th columns of $U$ and $V$, respectively.\n\nA matrix is ill-conditioned if its condition number, $\\kappa(A) = \\sigma_1 / \\sigma_n$, is very large. This occurs when the singular values decay rapidly, causing $\\sigma_n$ to be close to zero. From the summation form of the solution, it is evident that any component of the right-hand-side $b$ that projects onto a left singular vector $u_i$ is amplified by the factor $1/\\sigma_i$. If $\\sigma_i$ is small, this amplification is enormous. In practical applications, the vector $b$ is often contaminated with noise, i.e., $b = b_{\\text{clean}} + \\delta b$. The noise term $\\delta b$ is invariably amplified by the small singular values, overwhelming the true solution.\n\nThe problem specifies the singular values as $\\sigma_i = 10^{-\\frac{i-1}{2}}$ for $i=1, \\dots, n$. With $n=30$, the condition number is $\\kappa(A) = \\sigma_1 / \\sigma_{30} = 10^0 / 10^{-29/2} = 10^{14.5}$, which indicates severe ill-conditioning.\n\nTruncated SVD (TSVD) is a regularization method to combat this noise amplification. Instead of using all $n$ singular components, the sum is truncated at a level $k  n$. The TSVD solution, denoted $x_k$, is defined as:\n$$\nx_k = \\sum_{i=1}^{k} \\frac{u_i^{\\top} b}{\\sigma_i} v_i\n$$\nThis is equivalent to using a truncated pseudo-inverse of $A$, $A_k^{\\dagger} = V_k \\Sigma_k^{-1} U_k^{\\top}$, where $U_k$ and $V_k$ contain the first $k$ columns of $U$ and $V$, and $\\Sigma_k$ is the leading $k \\times k$ submatrix of $\\Sigma$. The solution is then $x_k = A_k^{\\dagger} b$. By discarding terms for $i  k$, we filter out the unstable components associated with small singular values $\\sigma_{k+1}, \\dots, \\sigma_n$. The choice of $k$ is critical: it must be small enough to ensure stability but large enough to capture the essential features of the solution.\n\nThe problem constructs a specific, reproducible scenario to observe this behavior.\n1.  Set problem dimensions $n=30$ and truncation level $k=10$.\n2.  Construct deterministic orthogonal matrices $U$ and $V$ from the QR decomposition of two specific random matrices, ensuring reproducibility via fixed seeds ($0$ and $1$).\n3.  Construct the matrix $A = U \\Sigma V^{\\top}$ using the prescribed singular values $\\sigma_i = 10^{-\\frac{i-1}{2}}$.\n4.  Define a true solution $x_{\\text{true}}$ with components $x_{\\text{true},i} = 1/i$.\n5.  Compute the corresponding \"clean\" right-hand-side $b_{\\text{clean}} = A x_{\\text{true}}$.\n6.  Generate a deterministic noise vector $z$ (using seed $7$) and normalize it.\n7.  Create a series of noisy right-hand sides $b(\\eta) = b_{\\text{clean}} + \\eta \\|b_{\\text{clean}}\\|_2 \\, z$ for a set of noise fractions $\\eta \\in \\{0.0, 10^{-8}, 10^{-4}, 10^{-2}, 10^{-1}, 1.0\\}$.\n\nFor each $\\eta$, we compute the TSVD solution $x_k(\\eta)$ using the fixed truncation level $k=10$:\n$$\nx_k(\\eta) = \\sum_{i=1}^{k} \\frac{u_i^{\\top} b(\\eta)}{\\sigma_i} v_i\n$$\nThe quality of this solution is measured by the relative solution error:\n$$\ne(\\eta) = \\frac{\\|x_k(\\eta) - x_{\\text{true}}\\|_2}{\\|x_{\\text{true}}\\|_2}\n$$\nThe error $e(\\eta)$ arises from two sources:\n-   **Truncation (or Regularization) Error**: The error present even with no noise ($\\eta=0$). It is caused by discarding the components of $x_{\\text{true}}$ corresponding to singular values $\\sigma_{k+1}, \\dots, \\sigma_n$. This error is $e(0)=\\|x_k(0) - x_{\\text{true}}\\|_2 / \\|x_{\\text{true}}\\|_2$.\n-   **Noise Propagation Error**: The error resulting from the amplification of the noise term $\\eta \\|b_{\\text{clean}}\\|_2 \\, z$. This part of the error grows with $\\eta$.\n\nThe implementation will systematically follow these steps. For each value of $\\eta$ in the test suite, we will construct $b(\\eta)$, calculate $x_k(\\eta)$ using the TSVD formula, and compute the relative error $e(\\eta)$. The final output will be the list of these errors, demonstrating how the solution quality degrades as the noise level increases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes TSVD solutions and their errors for an ill-conditioned system\n    with varying noise levels in the right-hand-side vector.\n    \"\"\"\n    # 1. Choose problem size n and truncation level k.\n    n = 30\n    k = 10\n\n    # 2. Construct orthogonal matrices U and V.\n    # Use fixed seeds for determinism.\n    rng_u = np.random.default_rng(seed=0)\n    matrix_for_u = rng_u.standard_normal(size=(n, n))\n    U, _ = np.linalg.qr(matrix_for_u)\n\n    rng_v = np.random.default_rng(seed=1)\n    matrix_for_v = rng_v.standard_normal(size=(n, n))\n    V, _ = np.linalg.qr(matrix_for_v)\n\n    # 3. Define singular values sigma_i and form diagonal matrix Sigma.\n    # sigma_i = 10^(-(i-1)/2) for i = 1, ..., n\n    indices = np.arange(n)\n    sigmas = 10.0**(-0.5 * indices)\n    Sigma = np.diag(sigmas)\n\n    # 4. Set A = U * Sigma * V^T.\n    A = U @ Sigma @ V.T\n\n    # 5. Define the true solution vector x_true and compute b_clean.\n    # x_true_i = 1/i for i = 1, ..., n\n    x_true = 1.0 / np.arange(1, n + 1)\n    b_clean = A @ x_true\n\n    # 6. Fix a deterministic noise direction z.\n    rng_z = np.random.default_rng(seed=7)\n    z = rng_z.standard_normal(size=n)\n    z /= np.linalg.norm(z)\n\n    # Test Suite: noise fractions.\n    noise_fractions = [0.0, 1e-8, 1e-4, 1e-2, 1e-1, 1.0]\n    \n    results = []\n    \n    # Pre-calculate components for TSVD solution for efficiency.\n    U_k = U[:, :k]\n    V_k = V[:, :k]\n    sigmas_k_inv = 1.0 / sigmas[:k]\n    norm_x_true = np.linalg.norm(x_true)\n    norm_b_clean = np.linalg.norm(b_clean)\n\n    for eta in noise_fractions:\n        # 7. Define the noisy right-hand-side b(eta).\n        b_eta = b_clean + eta * norm_b_clean * z\n\n        # Implement the TSVD solution at rank k.\n        # x_k(eta) = V_k @ (diag(1/sigma_i) @ U_k.T @ b_eta)\n        # The inner part is a component-wise multiplication.\n        x_k_eta = V_k @ (sigmas_k_inv * (U_k.T @ b_eta))\n        \n        # Quantify solution quality using relative solution error.\n        error = np.linalg.norm(x_k_eta - x_true) / norm_x_true\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3280674"}]}