{"hands_on_practices": [{"introduction": "The best way to understand an algorithm is often to build it from the ground up. This first practice guides you through the implementation of forward-mode automatic differentiation by creating a `Dual` number class [@problem_id:3207038]. This exercise demystifies the process by showing how derivative information can be carried alongside a value through a series of calculations, turning the abstract rules of calculus into concrete object-oriented code.", "problem": "You are asked to implement forward mode automatic differentiation (forward mode AD) using operator overloading in a high-level language. The core idea is to track both the value and the derivative of expressions with respect to a single scalar input by propagating derivatives through elementary operations. You will create a class that represents a pair consisting of a real value and its derivative with respect to an independent variable. You must then use this class to evaluate a specific polynomial and its derivative at a set of inputs. The implementation must not rely on any prebuilt automatic differentiation tools or symbolic manipulation libraries.\n\nImplement a class whose instances represent numbers augmented with derivative information. The class must support standard arithmetic so that when the class is used in a computation, both the value and the derivative are computed automatically. The implementation must support addition, subtraction, multiplication, division, and integer exponentiation. Interactions with built-in numeric types must behave naturally on either side of the operator.\n\nUse the class to evaluate the polynomial function\n$$\nf(x) = 3x^{5} - 2x^{3} + 7x - 11\n$$\nand its derivative at several inputs. Your program should compute, for each input $x$, both the function value $f(x)$ and the derivative $f'(x)$ using forward mode automatic differentiation, and also compute the same two quantities using ordinary floating-point evaluation and standard calculus for verification. For each input $x$, compute the absolute errors\n$$\ne_{\\text{val}} = \\lvert f_{\\text{AD}}(x) - f(x) \\rvert, \\quad e_{\\text{der}} = \\lvert f'_{\\text{AD}}(x) - f'(x) \\rvert,\n$$\nwhere $f_{\\text{AD}}(x)$ and $f'_{\\text{AD}}(x)$ denote, respectively, the value and derivative returned by your automatic differentiation evaluation.\n\nAngle units are not involved. No physical units are involved.\n\nTest suite:\n- Evaluate at the following ordered list of inputs $[0.0, 1.0, -1.0, 2.5, 10.0]$. These cover a boundary input $0.0$, simple integer inputs $1.0$ and $-1.0$, a non-integer input $2.5$, and a larger magnitude input $10.0$.\n\nProgram requirements:\n- Define the polynomial $f(x)$ once using arithmetic operators; a single definition must work identically whether $x$ is a built-in real number or an instance of your class.\n- For each test input $x$ in the specified order, create an instance representing the independent variable with derivative $1.0$, evaluate the polynomial to obtain $f_{\\text{AD}}(x)$ and $f'_{\\text{AD}}(x)$, evaluate the same polynomial using ordinary floating-point arithmetic to obtain $f(x)$, evaluate the analytical derivative $f'(x)$ using standard calculus, and compute $e_{\\text{val}}$ and $e_{\\text{der}}$ as defined above.\n- Collect the results in the same order as the inputs.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element must be a two-element list $[e_{\\text{val}}, e_{\\text{der}}]$ for the corresponding input. For example, the printed line should look like\n\"[ [e_val_for_x0, e_der_for_x0], [e_val_for_x1, e_der_for_x1], ... ]\"\nwith no spaces requirement enforced other than those produced by your print, but it must be a single line that encodes a list of lists in the natural language’s list syntax.\n\nYour program must be self-contained and must not read any input from the user or any external file. The results for the provided test suite must be the only content of the printed output line.", "solution": "We derive forward mode automatic differentiation (automatic differentiation (AD)) for a single scalar input using well-established differential calculus rules. The foundational base comprises standard differentiation rules: linearity, the product rule, the quotient rule, the chain rule, and the power rule for integer exponents.\n\nWe represent a computation on an independent variable $x$ by associating to each intermediate quantity a pair $\\left(v, d\\right)$, where $v$ is the real value and $d = \\frac{dv}{dx}$ is the derivative with respect to $x$. We now derive how to propagate these pairs through arithmetic operations directly from calculus.\n\nAddition and subtraction:\nSuppose $u = (u, u')$ and $v = (v, v')$ represent two differentiable quantities with respect to $x$. Then\n$$\n(u + v, \\, \\frac{d}{dx}(u+v)) = (u + v, \\, u' + v'),\n$$\nand similarly for subtraction,\n$$\n(u - v, \\, \\frac{d}{dx}(u-v)) = (u - v, \\, u' - v').\n$$\nThese follow from linearity of differentiation.\n\nMultiplication:\nUsing the product rule,\n$$\n\\frac{d}{dx}(uv) = u'v + uv'.\n$$\nHence, for pairs $(u,u')$ and $(v,v')$, the product becomes\n$$\n(uv, \\, u'v + uv').\n$$\n\nDivision:\nUsing the quotient rule,\n$$\n\\frac{d}{dx}\\left(\\frac{u}{v}\\right) = \\frac{u'v - uv'}{v^{2}},\n$$\nso the propagated pair is\n$$\n\\left(\\frac{u}{v}, \\, \\frac{u'v - uv'}{v^{2}}\\right),\n$$\nprovided $v \\neq 0$.\n\nInteger power:\nFor an integer $n$, by the power rule and chain rule,\n$$\n\\frac{d}{dx}\\left(u^{n}\\right) = n u^{n-1} u'.\n$$\nThus the propagated pair is\n$$\n\\left(u^{n}, \\, n u^{n-1} u'\\right).\n$$\nFor a real constant exponent $p$, the same expression $\\left(u^{p}, \\, p u^{p-1} u'\\right)$ holds when $u > 0$, but for this task it suffices to correctly support integer exponents because the target function is a polynomial.\n\nForward mode AD implementation:\nWe encode each quantity as a class instance storing two real numbers $(v,d)$. The class overloads the arithmetic operators so that when it participates in an expression, it applies the above propagation formulas. Interactions with built-in real numbers are handled by treating a real $c$ as the pair $(c, 0)$. The independent variable $x$ is represented as $(x,1)$.\n\nTarget function:\nWe consider the polynomial\n$$\nf(x) = 3x^{5} - 2x^{3} + 7x - 11.\n$$\nBecause $f$ is a polynomial, when it is written once in terms of overloaded arithmetic, it computes both the value and the derivative automatically. For verification, we use its analytical derivative obtained from standard calculus:\n$$\nf'(x) = 15x^{4} - 6x^{2} + 7.\n$$\n\nAlgorithmic steps:\n1. Define a class whose instances store $(v,d)$ and implement the overloaded operators $+$, $-$, $\\times$, $\\div$, and exponentiation by an integer as derived above.\n2. Define $f(x)$ once using arithmetic operators. This single definition can be applied to either a built-in real or to an instance of the class.\n3. For each test input $x \\in \\{0.0, 1.0, -1.0, 2.5, 10.0\\}$, create the seeded variable $(x,1)$, evaluate $f$ to obtain $(f_{\\text{AD}}(x), f'_{\\text{AD}}(x))$, evaluate $f(x)$ and $f'(x)$ as ordinary floating-point computations, and compute\n$$\ne_{\\text{val}} = \\lvert f_{\\text{AD}}(x) - f(x) \\rvert, \\quad e_{\\text{der}} = \\lvert f'_{\\text{AD}}(x) - f'(x) \\rvert.\n$$\n4. Collect the pair $[e_{\\text{val}}, e_{\\text{der}}]$ for each $x$ in the specified order and print them as a single list on one line.\n\nCorrectness:\nThe propagation rules are directly derived from the linearity of differentiation, the product rule, the quotient rule, and the power rule. Because a polynomial is composed via repeated applications of these elementary operations, the forward mode AD evaluation yields the exact analytical derivative of $f$ at any point $x$ within floating-point rounding. Therefore, the absolute errors $e_{\\text{val}}$ and $e_{\\text{der}}$ are expected to be extremely small, typically near zero up to floating-point roundoff, across all test inputs.\n\nOutput:\nThe program prints a single line representing a list of two-element lists $[e_{\\text{val}}, e_{\\text{der}}]$, one per test case, in the same order as the inputs. No other output is produced.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass Dual:\n    \"\"\"\n    Dual number for forward-mode automatic differentiation with respect to a single scalar variable.\n    Each instance represents a pair (value, derivative).\n    \"\"\"\n    __slots__ = (\"val\", \"der\")\n\n    def __init__(self, val, der=0.0):\n        self.val = float(val)\n        self.der = float(der)\n\n    @staticmethod\n    def _coerce(other):\n        if isinstance(other, Dual):\n            return other\n        else:\n            return Dual(other, 0.0)\n\n    # Addition\n    def __add__(self, other):\n        o = Dual._coerce(other)\n        return Dual(self.val + o.val, self.der + o.der)\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    # Subtraction\n    def __sub__(self, other):\n        o = Dual._coerce(other)\n        return Dual(self.val - o.val, self.der - o.der)\n\n    def __rsub__(self, other):\n        o = Dual._coerce(other)\n        return Dual(o.val - self.val, o.der - self.der)\n\n    # Multiplication\n    def __mul__(self, other):\n        o = Dual._coerce(other)\n        # (u, u') * (v, v') = (uv, u'v + uv')\n        return Dual(self.val * o.val, self.der * o.val + self.val * o.der)\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    # True division\n    def __truediv__(self, other):\n        o = Dual._coerce(other)\n        # (u, u') / (v, v') = (u/v, (u'v - uv')/v^2)\n        denom = o.val * o.val\n        return Dual(self.val / o.val, (self.der * o.val - self.val * o.der) / denom)\n\n    def __rtruediv__(self, other):\n        o = Dual._coerce(other)\n        # o / self\n        denom = self.val * self.val\n        return Dual(o.val / self.val, (o.der * self.val - o.val * self.der) / denom)\n\n    # Unary negation\n    def __neg__(self):\n        return Dual(-self.val, -self.der)\n\n    # Power: support real (int/float) exponents, commonly used for integer exponents in polynomials\n    def __pow__(self, power):\n        if isinstance(power, (int, float)):\n            if power == 0:\n                # x**0 = 1, derivative 0\n                return Dual(1.0, 0.0)\n            # For real power p: d(x**p) = p * x**(p - 1) * dx\n            primal = self.val ** power\n            deriv = power * (self.val ** (power - 1.0)) * self.der\n            return Dual(primal, deriv)\n        else:\n            raise TypeError(\"Power must be a real number for Dual.__pow__\")\n\ndef poly(x):\n    # f(x) = 3x^5 - 2x^3 + 7x - 11\n    return 3 * (x ** 5) - 2 * (x ** 3) + 7 * x - 11\n\ndef poly_float(x):\n    return 3.0 * (x ** 5) - 2.0 * (x ** 3) + 7.0 * x - 11.0\n\ndef poly_derivative_float(x):\n    # f'(x) = 15x^4 - 6x^2 + 7\n    return 15.0 * (x ** 4) - 6.0 * (x ** 2) + 7.0\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [0.0, 1.0, -1.0, 2.5, 10.0]\n\n    results = []\n    for x in test_cases:\n        # Seed the independent variable: derivative w.r.t. x is 1\n        dx = Dual(x, 1.0)\n        y = poly(dx)              # Dual result: (value, derivative)\n        f_val = poly_float(x)     # Float value\n        f_der = poly_derivative_float(x)  # Analytical derivative\n\n        val_err = abs(y.val - f_val)\n        der_err = abs(y.der - f_der)\n\n        results.append([val_err, der_err])\n\n    # Final print statement in the exact required format: a single line list of [val_err, der_err] pairs.\n    # Format with reasonable precision for readability.\n    formatted = \"[\" + \",\".join(f\"[{val:.12g},{der:.12g}]\" for val, der in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3207038"}, {"introduction": "While forward mode is intuitive, reverse-mode automatic differentiation is the powerhouse behind modern deep learning. This exercise shifts our focus from code to concept, asking you to manually trace the computational graph and the backward flow of derivatives for a given function [@problem_id:3100431]. By performing this 'by-hand' backpropagation, you will gain a deep, fundamental understanding of the Vector-Jacobian Product (VJP) and the core logic that enables efficient gradient computation in complex models.", "problem": "Consider reverse-mode automatic differentiation (AD) in the context of deep learning, where gradients of scalar loss functions with respect to parameters are computed efficiently by traversing a computational graph backward using the chain rule. The function of interest is the scalar map $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ given by $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ with $y\\neq 0$. Using only elementary operations compatible with a computational graph (multiplication, sine, exponential, and division), construct a minimal set of intermediate variables that evaluates $f(x,y)$ and a tape that records the parent-child relationships for these operations. Then, using the principle of the chain rule for composite functions and the concept of Vector-Jacobian Product (VJP), derive the exact sequence of backward passes (VJP pulls) needed to obtain the gradient $\\nabla f(x,y)$ by hand. Your derivation should clearly identify the order of traversing the tape in reverse and the local contributions to the adjoints of the inputs at each step. Provide the final analytical expression for $\\nabla f(x,y)$ as a row vector. Do not round; the final answer must be an exact symbolic expression.", "solution": "The problem statement is found to be valid. It is scientifically grounded, well-posed, objective, and contains sufficient information to derive a unique and meaningful solution. The task concerns the application of reverse-mode automatic differentiation (AD), a cornerstone algorithm in computational calculus and deep learning, to a differentiable function. The procedure is formalizable and aligns with established principles.\n\nWe are tasked with computing the gradient $\\nabla f(x,y)$ of the function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ given by $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ for $y \\neq 0$, using the principles of reverse-mode AD. This involves a forward pass to construct a computational graph and evaluate the function, followed by a backward pass to propagate gradients.\n\nFirst, we decompose the function into a sequence of elementary operations. This sequence defines the computational graph, or \"tape\". Let the inputs be $v_1 = x$ and $v_2 = y$.\n\n**Forward Pass: Constructing the Computational Graph**\n\nThe evaluation of $f(x,y)$ can be represented by the following minimal set of intermediate variables:\n1.  $v_3 = v_1 \\cdot v_2 = x \\cdot y$\n2.  $v_4 = \\sin(v_3) = \\sin(xy)$\n3.  $v_5 = \\exp(v_1) = \\exp(x)$\n4.  $v_6 = \\frac{v_5}{v_2} = \\frac{\\exp(x)}{y}$\n5.  $v_7 = v_4 + v_6 = \\sin(xy) + \\frac{\\exp(x)}{y} = f(x,y)$\n\nThis sequence constitutes the forward pass. The tape records these operations and their dependencies: $(v_3, \\text{mul}, v_1, v_2)$, $(v_4, \\sin, v_3)$, $(v_5, \\exp, v_1)$, $(v_6, \\text{div}, v_5, v_2)$, $(v_7, \\text{add}, v_4, v_6)$.\n\n**Backward Pass: Gradient Computation using the Chain Rule**\n\nThe backward pass computes the partial derivatives of the final output $v_7$ with respect to each intermediate variable $v_i$, which are denoted as adjoints, $\\bar{v}_i = \\frac{\\partial f}{\\partial v_i} = \\frac{\\partial v_7}{\\partial v_i}$. The process begins by initializing the adjoint of the output node to $1$, i.e., $\\bar{v}_7 = \\frac{\\partial v_7}{\\partial v_7} = 1$. All other adjoints are initialized to $0$. We then traverse the graph in reverse topological order.\n\nThe core principle is the chain rule. For an operation $v_k = g(v_i, v_j, \\dots)$, the adjoints of its parents are updated by accumulating the child's adjoint multiplied by the local partial derivative:\n$$ \\bar{v}_i = \\bar{v}_i + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_i} $$\n$$ \\bar{v}_j = \\bar{v}_j + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_j} $$\n... and so on. This operation is effectively a Vector-Jacobian Product (VJP) pull.\n\nLet's compute the adjoints in reverse order of the forward pass:\n\n1.  **Start:** Initialize adjoints: $\\bar{v}_1=0, \\bar{v}_2=0, \\bar{v}_3=0, \\bar{v}_4=0, \\bar{v}_5=0, \\bar{v}_6=0$.\n    Set the seed for the backward pass: $\\bar{v}_7 = 1$.\n\n2.  **Node $v_7 = v_4 + v_6$:**\n    The parents are $v_4$ and $v_6$.\n    Local partial derivatives: $\\frac{\\partial v_7}{\\partial v_4} = 1$, $\\frac{\\partial v_7}{\\partial v_6} = 1$.\n    Update parent adjoints:\n    $\\bar{v}_4 \\leftarrow \\bar{v}_4 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_4} = 0 + 1 \\cdot 1 = 1$.\n    $\\bar{v}_6 \\leftarrow \\bar{v}_6 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_6} = 0 + 1 \\cdot 1 = 1$.\n    Current state: $\\bar{v}_4=1, \\bar{v}_6=1$.\n\n3.  **Node $v_6 = \\frac{v_5}{v_2}$:**\n    The parents are $v_5$ and $v_2$.\n    Local partial derivatives: $\\frac{\\partial v_6}{\\partial v_5} = \\frac{1}{v_2}$, $\\frac{\\partial v_6}{\\partial v_2} = -\\frac{v_5}{v_2^2}$.\n    Update parent adjoints:\n    $\\bar{v}_5 \\leftarrow \\bar{v}_5 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_5} = 0 + 1 \\cdot \\frac{1}{v_2} = \\frac{1}{y}$.\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_2} = 0 + 1 \\cdot \\left(-\\frac{v_5}{v_2^2}\\right) = -\\frac{\\exp(x)}{y^2}$.\n    Current state: $\\bar{v}_5 = \\frac{1}{y}$, $\\bar{v}_2=-\\frac{\\exp(x)}{y^2}$.\n\n4.  **Node $v_5 = \\exp(v_1)$:**\n    The parent is $v_1$.\n    Local partial derivative: $\\frac{\\partial v_5}{\\partial v_1} = \\exp(v_1)$.\n    Update parent adjoint:\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_5 \\cdot \\frac{\\partial v_5}{\\partial v_1} = 0 + \\frac{1}{y} \\cdot \\exp(v_1) = \\frac{\\exp(x)}{y}$.\n    Current state: $\\bar{v}_1 = \\frac{\\exp(x)}{y}$.\n\n5.  **Node $v_4 = \\sin(v_3)$:**\n    The parent is $v_3$.\n    Local partial derivative: $\\frac{\\partial v_4}{\\partial v_3} = \\cos(v_3)$.\n    Update parent adjoint:\n    $\\bar{v}_3 \\leftarrow \\bar{v}_3 + \\bar{v}_4 \\cdot \\frac{\\partial v_4}{\\partial v_3} = 0 + 1 \\cdot \\cos(v_3) = \\cos(xy)$.\n    Current state: $\\bar{v}_3 = \\cos(xy)$.\n\n6.  **Node $v_3 = v_1 \\cdot v_2$:**\n    The parents are $v_1$ and $v_2$. Note that $v_1$ and $v_2$ have already received gradients from other paths; we accumulate the new contributions.\n    Local partial derivatives: $\\frac{\\partial v_3}{\\partial v_1} = v_2$, $\\frac{\\partial v_3}{\\partial v_2} = v_1$.\n    Update parent adjoints:\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_1} = \\frac{\\exp(x)}{y} + \\cos(xy) \\cdot v_2 = \\frac{\\exp(x)}{y} + y \\cos(xy)$.\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_2} = -\\frac{\\exp(x)}{y^2} + \\cos(xy) \\cdot v_1 = -\\frac{\\exp(x)}{y^2} + x \\cos(xy)$.\n\nThe process terminates as we have computed the adjoints for all input nodes.\nThe final gradients are the final values of the adjoints of the input variables:\n$$ \\frac{\\partial f}{\\partial x} = \\bar{v}_1 = y \\cos(xy) + \\frac{\\exp(x)}{y} $$\n$$ \\frac{\\partial f}{\\partial y} = \\bar{v}_2 = x \\cos(xy) - \\frac{\\exp(x)}{y^2} $$\n\nThe gradient vector $\\nabla f(x,y)$ is the row vector of these partial derivatives:\n$$ \\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} y \\cos(xy) + \\frac{\\exp(x)}{y} & x \\cos(xy) - \\frac{\\exp(x)}{y^2} \\end{pmatrix} $$\nThis derivation rigorously follows the mechanical steps of reverse-mode automatic differentiation.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\ny \\cos(xy) + \\frac{\\exp(x)}{y} & x \\cos(xy) - \\frac{\\exp(x)}{y^2}\n\\end{pmatrix}\n}\n$$", "id": "3100431"}, {"introduction": "Automatic differentiation is not just for computing gradients; its power extends to approximating second-order derivative information efficiently. This advanced practice challenges you to implement a Hessian-vector product routine, a key application of AD that avoids the prohibitive cost of forming the full Hessian matrix [@problem_id:3100512]. You will then use this tool to drive a Newton-Conjugate-Gradient optimizer, demonstrating how AD enables sophisticated, curvature-aware optimization methods that can dramatically outperform standard gradient descent.", "problem": "You are asked to design and implement a complete, runnable program that trains a small multilayer perceptron (MLP) by Newton-Conjugate-Gradient (Newton-CG) using Automatic Differentiation (AD) to compute Hessian-vector products. The work must be framed from first principles and be testable end-to-end.\n\nThe target model is a one-hidden-layer multilayer perceptron mapping from input dimension $d=2$ to a scalar output. Let the hidden width be $h=5$. Denote the parameters by $\\theta = \\{\\mathbf{W}_1 \\in \\mathbb{R}^{h \\times d}, \\mathbf{b}_1 \\in \\mathbb{R}^{h}, \\mathbf{W}_2 \\in \\mathbb{R}^{1 \\times h}, b_2 \\in \\mathbb{R}\\}$. For an input $\\mathbf{x} \\in \\mathbb{R}^d$, define the network as\n$$\n\\mathbf{z}_1 = \\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1,\\quad\n\\mathbf{a}_1 = \\tanh(\\mathbf{z}_1),\\quad\nz_2 = \\mathbf{W}_2 \\mathbf{a}_1 + b_2,\\quad\n\\hat{y} = z_2.\n$$\nGiven a dataset $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$, define the empirical mean-squared error loss\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{2N}\\sum_{i=1}^N \\left(\\hat{y}_i - y_i\\right)^2.\n$$\nYou must implement the following, strictly from the fundamental base of the chain rule for derivatives and the definitions of gradient and Hessian.\n\n1) Derive and implement reverse-mode automatic differentiation (backpropagation) to compute the gradient $\\nabla \\mathcal{L}(\\theta)$ for the network above. Your derivation must start from the chain rule and the definition of the gradient of a scalar-valued function as the vector of partial derivatives.\n\n2) Derive and implement a forward-over-reverse Automatic Differentiation procedure (Pearlmutter’s trick) to compute, for any given direction vector $\\mathbf{v}$ in parameter space, the Hessian-vector product $\\mathbf{H}(\\theta)\\mathbf{v}$, where $\\mathbf{H}(\\theta) = \\nabla^2 \\mathcal{L}(\\theta)$. Your derivation must begin with the definition $\\mathbf{H}(\\theta)\\mathbf{v} = \\left.\\frac{d}{d\\epsilon}\\right|_{\\epsilon=0}\\nabla \\mathcal{L}(\\theta + \\epsilon \\mathbf{v})$ and must use only first-order Automatic Differentiation passes. No finite differences are permitted.\n\n3) Implement a Newton-Conjugate-Gradient optimizer that, at parameter iterate $\\theta_k$, approximately solves the linear system\n$$\n\\left(\\mathbf{H}(\\theta_k) + \\mu \\mathbf{I}\\right)\\mathbf{p}_k = -\\nabla \\mathcal{L}(\\theta_k)\n$$\nby the Conjugate Gradient method using only matrix-vector products with $\\mathbf{H}(\\theta_k)$ (i.e., your Hessian-vector product routine) and a damping parameter $\\mu \\ge 0$. Use a backtracking Armijo line search to select a step length $\\alpha_k$ satisfying\n$$\n\\mathcal{L}(\\theta_k + \\alpha_k \\mathbf{p}_k) \\le \\mathcal{L}(\\theta_k) + c_1 \\alpha_k \\nabla \\mathcal{L}(\\theta_k)^\\top \\mathbf{p}_k,\n$$\nwhere $c_1 \\in (0,1)$ is a fixed constant. Your Conjugate Gradient implementation must explicitly detect negative curvature by testing whether $\\mathbf{d}^\\top \\left(\\mathbf{H}(\\theta_k) + \\mu \\mathbf{I}\\right)\\mathbf{d} \\le 0$ for the current search direction $\\mathbf{d}$; upon detection, terminate CG early for that iterate and record that negative curvature was encountered.\n\n4) Implement a baseline gradient descent with backtracking Armijo line search for comparison.\n\n5) Analyze when curvature helps versus hurts by running both methods on a fixed dataset and multiple initializations that induce qualitatively different curvature regimes.\n\nFundamental base you must use in your derivations:\n- The chain rule for derivatives for composite functions.\n- The definition of the gradient $\\nabla \\mathcal{L}(\\theta)$ as the vector of first partial derivatives of $\\mathcal{L}$ with respect to $\\theta$.\n- The definition of the Hessian $\\mathbf{H}(\\theta)$ as the Jacobian of the gradient field, equivalently the matrix of second partial derivatives.\n- The Conjugate Gradient method as an algorithm to minimize a strictly convex quadratic function $q(\\mathbf{p}) = \\frac{1}{2}\\mathbf{p}^\\top \\mathbf{A}\\mathbf{p} + \\mathbf{b}^\\top \\mathbf{p}$ with $\\mathbf{A}$ symmetric positive definite by Krylov subspace iterations using only products $\\mathbf{A}\\mathbf{v}$.\n- The Armijo condition for sufficient decrease in line search.\n\nAngle units do not apply. No physical units are involved.\n\nTest suite and required output:\n- Use the fixed dataset of size $N=8$ in $d=2$ dimensions (an XOR-like pattern), with inputs and targets\n$$\n\\mathbf{X} = \\begin{bmatrix}\n-1 & -1\\\\\n-1 & 1\\\\\n1 & -1\\\\\n1 & 1\\\\\n-1 & -1\\\\\n-1 & 1\\\\\n1 & -1\\\\\n1 & 1\n\\end{bmatrix},\\quad\n\\mathbf{y} = \\begin{bmatrix}\n-1\\\\\n1\\\\\n1\\\\\n-1\\\\\n-1\\\\\n1\\\\\n1\\\\\n-1\n\\end{bmatrix}.\n$$\n- Hidden width is $h=5$.\n- Use $T=10$ outer iterations for both Newton-CG and gradient descent.\n- Conjugate Gradient parameters: maximum inner iterations $50$, tolerance $10^{-10}$.\n- Armijo parameter $c_1 = 10^{-4}$; initial step $1$; backtracking shrink factor $1/2$; at most $20$ backtracking steps per line search.\n- Run the following three cases, each fully specified by its initialization and damping $\\mu$:\n    - Case $1$ (well-scaled near-convex regime): random seed $0$; initialize every parameter independently from $\\mathcal{N}(0, (0.01)^2)$; damping $\\mu = 10^{-4}$.\n    - Case $2$ (strong nonconvexity; likely negative curvature): random seed $1$; initialize every parameter independently from $\\mathcal{N}(0, (3)^2)$; damping $\\mu = 0$.\n    - Case $3$ (saturation-induced flatness): set $\\mathbf{W}_1 = \\mathbf{0}$, $\\mathbf{b}_1 = 5\\cdot \\mathbf{1}$, $\\mathbf{W}_2 = \\mathbf{0}$, $b_2=0$; damping $\\mu = 10^{-2}$.\n\nFor each case, run both optimizers from the same initialization and report four quantities:\n- The final training loss after Newton-CG, as a float.\n- The final training loss after gradient descent, as a float.\n- A boolean indicating whether curvature helped, defined as whether the Newton-CG final loss is strictly less than the gradient descent final loss.\n- A boolean indicating whether negative curvature was encountered at least once during any Conjugate Gradient solve within Newton-CG.\n\nYour program should produce a single line of output containing the results for the three cases as a comma-separated Python-style list of lists, each inner list ordered as\n$$\n[\\text{ncg\\_final\\_loss},\\ \\text{gd\\_final\\_loss},\\ \\text{ncg\\_helped},\\ \\text{neg\\_curvature\\_detected}].\n$$\nFor example:\n$$\n[[0.123456, 0.234567, True, False], […], […]]\n$$", "solution": "The solution requires implementing a multilayer perceptron and two optimization algorithms: gradient descent (GD) and Newton-Conjugate-Gradient (Newton-CG). Both optimizers will use an Armijo backtracking line search. The core of the Newton-CG method is the efficient computation of the Hessian-vector product (HVP), which will be derived and implemented using forward-over-reverse mode automatic differentiation.\n\n### Parameter Management\nThe model's parameters, $\\theta = \\{\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, b_2\\}$, form a dictionary of matrices and vectors. For optimization algorithms, it is convenient to work with a single, flattened vector of parameters. We will implement helper functions to convert between the dictionary representation and a flat vector representation. The total number of parameters is $D = (5 \\times 2) + 5 + (1 \\times 5) + 1 = 21$.\n\n### Part 1: Gradient Computation via Reverse-Mode AD (Backpropagation)\n\nWe derive the gradient $\\nabla \\mathcal{L}(\\theta)$ for the loss $\\mathcal{L}(\\theta) = \\frac{1}{2N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2$. The gradient of the total loss is the average of the gradients for each sample's loss contribution, $\\nabla \\mathcal{L} = \\frac{1}{N} \\sum_i \\nabla \\mathcal{L}_i$, where $\\mathcal{L}_i = \\frac{1}{2}(\\hat{y}_i-y_i)^2$. The gradient of $\\mathcal{L}_i$ is $(\\hat{y}_i-y_i)\\nabla\\hat{y}_i$. Therefore, $\\nabla\\mathcal{L} = \\frac{1}{N} \\sum_i (\\hat{y}_i-y_i)\\nabla\\hat{y}_i$.\n\nLet $\\bar{v} = \\frac{\\partial \\mathcal{L}}{\\partial v}$ denote the adjoint of a variable $v$. The computation proceeds in two stages: a forward pass to compute intermediate values, and a backward pass to propagate gradients. For a batch of $N$ samples, these operations are vectorized. Let $\\mathbf{X}$ be the input matrix of shape $(N, d)$ and $\\mathbf{Y}$ be the target matrix of shape $(N, 1)$.\n\n**Forward Pass (Batch computation):**\n1.  $\\mathbf{Z}_1 = \\mathbf{X} \\mathbf{W}_1^\\top + \\mathbf{b}_1^\\top$ (shape $N \\times h$)\n2.  $\\mathbf{A}_1 = \\tanh(\\mathbf{Z}_1)$ (shape $N \\times h$)\n3.  $\\mathbf{Z}_2 = \\mathbf{A}_1 \\mathbf{W}_2^\\top + b_2$ (shape $N \\times 1$)\n4.  $\\hat{\\mathbf{Y}} = \\mathbf{Z}_2$ (shape $N \\times 1$)\n\n**Backward Pass (Batch computation):**\n1.  Initialize with the derivative of the loss with respect to the output: $\\bar{\\hat{\\mathbf{Y}}} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{Y}}} = \\frac{1}{N}(\\hat{\\mathbf{Y}} - \\mathbf{Y})$.\n2.  Backpropagate through the output layer: $\\bar{\\mathbf{Z}}_2 = \\bar{\\hat{\\mathbf{Y}}}$.\n3.  Gradients for the second layer:\n    $\\nabla_{\\mathbf{W}_2} \\mathcal{L} = \\bar{\\mathbf{Z}}_2^\\top \\mathbf{A}_1$ (shape $1 \\times h$)\n    $\\nabla_{b_2} \\mathcal{L} = \\sum_{i=1}^N (\\bar{\\mathbf{Z}}_2)_i$ (scalar sum)\n    Adjoint for hidden activations: $\\bar{\\mathbf{A}}_1 = \\bar{\\mathbf{Z}}_2 \\mathbf{W}_2$ (shape $N \\times h$)\n4.  Backpropagate through the hidden activation: $\\bar{\\mathbf{Z}}_1 = \\bar{\\mathbf{A}}_1 \\odot (1 - \\mathbf{A}_1^2)$, where $\\odot$ is the element-wise product.\n5.  Gradients for the first layer:\n    $\\nabla_{\\mathbf{W}_1} \\mathcal{L} = \\bar{\\mathbf{Z}}_1^\\top \\mathbf{X}$ (shape $h \\times d$)\n    $\\nabla_{\\mathbf{b}_1} \\mathcal{L} = \\sum_{i=1}^N (\\bar{\\mathbf{Z}}_1)_{i,:}$ (sum over batch dimension)\n\n### Part 2: Hessian-Vector Product via Forward-over-Reverse AD\n\nWe wish to compute $\\mathbf{H}(\\theta)\\mathbf{v}$, where $\\mathbf{H} = \\nabla^2 \\mathcal{L}(\\theta)$, for an arbitrary vector $\\mathbf{v}$ in the parameter space. This is achieved by applying the directional derivative operator $R_\\mathbf{v}\\{\\cdot\\} = \\left.\\frac{d}{d\\epsilon}\\right|_{\\epsilon=0} [\\cdot](\\theta + \\epsilon \\mathbf{v})$ to the gradient computation graph. Let $\\dot{f} = R_\\mathbf{v}\\{f\\}$. This requires an augmented forward pass to compute tangents (the $\\dot{f}$ quantities) and an augmented backward pass to compute the HVP components. The vector $\\mathbf{v}$ is un-flattened into components $\\{\\mathbf{v}_{W_1}, \\mathbf{v}_{b_1}, \\mathbf{v}_{W_2}, \\mathbf{v}_{b_2}\\}$.\n\n**Augmented Forward Pass (Primals and Tangents):**\n1.  $\\mathbf{Z}_1, \\mathbf{A}_1, \\mathbf{Z}_2, \\hat{\\mathbf{Y}}$ are computed as in the standard forward pass.\n2.  $\\dot{\\mathbf{Z}}_1 = \\mathbf{X} \\mathbf{v}_{W_1}^\\top + \\mathbf{v}_{b_1}^\\top$\n3.  $\\dot{\\mathbf{A}}_1 = (1 - \\mathbf{A}_1^2) \\odot \\dot{\\mathbf{Z}}_1$\n4.  $\\dot{\\mathbf{Z}}_2 = (\\mathbf{A}_1 \\mathbf{v}_{W_2}^\\top + \\dot{\\mathbf{A}}_1 \\mathbf{W}_2^\\top) + \\mathbf{v}_{b_2}$\n5.  $\\dot{\\hat{\\mathbf{Y}}} = \\dot{\\mathbf{Z}}_2$\n\n**Augmented Backward Pass (Adjoints and HVP):**\nThis pass computes $R_\\mathbf{v}\\{\\nabla \\mathcal{L}\\}$. For clarity, we define adjoints with respect to the summed loss $\\mathcal{L}_{sum} = \\frac{1}{2}\\sum_i (\\hat{y}_i-y_i)^2$, and apply the $\\frac{1}{N}$ scaling at the end.\n1.  Primal adjoints: $\\bar{\\hat{\\mathbf{Y}}}_{sum} = \\hat{\\mathbf{Y}} - \\mathbf{Y}$. And subsequently $\\bar{\\mathbf{Z}}_{2,sum}, \\bar{\\mathbf{A}}_{1,sum}, \\bar{\\mathbf{Z}}_{1,sum}$.\n2.  Tangent of the initial adjoint: $\\dot{\\bar{\\hat{\\mathbf{Y}}}}_{sum} = \\dot{\\hat{\\mathbf{Y}}}$.\n3.  Backpropagate tangent adjoints:\n    $\\dot{\\bar{\\mathbf{Z}}}_{2,sum} = \\dot{\\bar{\\hat{\\mathbf{Y}}}}_{sum}$\n    $\\dot{\\bar{\\mathbf{A}}}_{1,sum} = \\dot{\\bar{\\mathbf{Z}}}_{2,sum} \\mathbf{W}_2 + \\bar{\\mathbf{Z}}_{2,sum} \\mathbf{v}_{W_2}$\n    $\\dot{\\bar{\\mathbf{Z}}}_{1,sum} = \\dot{\\bar{\\mathbf{A}}}_{1,sum} \\odot (1 - \\mathbf{A}_1^2) + \\bar{\\mathbf{A}}_{1,sum} \\odot (-2\\mathbf{A}_1 \\odot \\dot{\\mathbf{A}}_1)$\n4.  Assemble HVP components by applying the product rule to the gradient expressions and averaging over the batch:\n    $(\\mathbf{H}\\mathbf{v})_{W_2} = \\frac{1}{N}(\\dot{\\bar{\\mathbf{Z}}}_{2,sum}^\\top \\mathbf{A}_1 + \\bar{\\mathbf{Z}}_{2,sum}^\\top \\dot{\\mathbf{A}}_1)$\n    $(\\mathbf{H}\\mathbf{v})_{b_2} = \\frac{1}{N} \\sum_i (\\dot{\\bar{\\mathbf{Z}}}_{2,sum})_i$\n    $(\\mathbf{H}\\mathbf{v})_{W_1} = \\frac{1}{N}(\\dot{\\bar{\\mathbf{Z}}}_{1,sum}^\\top \\mathbf{X})$\n    $(\\mathbf{H}\\mathbf{v})_{b_1} = \\frac{1}{N} \\sum_i (\\dot{\\bar{\\mathbf{Z}}}_{1,sum})_{i,:}$\n\n### Part 3: Newton-Conjugate-Gradient Optimizer\n\nThe Newton-CG method proceeds in outer iterations. At each iteration $k$:\n1.  Compute the gradient $\\mathbf{g}_k = \\nabla \\mathcal{L}(\\theta_k)$.\n2.  Use the Conjugate Gradient (CG) method to find an approximate solution $\\mathbf{p}_k$ to the Newton system $(\\mathbf{H}_k + \\mu\\mathbf{I})\\mathbf{p}_k = -\\mathbf{g}_k$, where $\\mathbf{H}_k = \\mathbf{H}(\\theta_k)$. The CG algorithm is ideal as it only requires matrix-vector products with $(\\mathbf{H}_k + \\mu\\mathbf{I})$, which we compute as $(\\mathbf{H}_k\\mathbf{d}) + \\mu\\mathbf{d}$ using our HVP function.\n3.  During CG, we monitor for non-positive curvature. For a CG search direction $\\mathbf{d}_j$, we compute $\\mathbf{d}_j^\\top(\\mathbf{H}_k + \\mu\\mathbf{I})\\mathbf{d}_j$. If this quantity is non-positive, it signals that the Hessian is not sufficiently positive definite. CG is terminated early, and a flag is raised. If this occurs on the first CG iteration ($j=0$), the search direction $\\mathbf{p}_k$ defaults to the negative gradient $-\\mathbf{g}_k$. Otherwise, the direction computed so far by CG is used.\n4.  A step length $\\alpha_k$ is found using a backtracking line search to satisfy the Armijo condition: $\\mathcal{L}(\\theta_k + \\alpha_k \\mathbf{p}_k) \\le \\mathcal{L}(\\theta_k) + c_1 \\alpha_k \\mathbf{g}_k^\\top \\mathbf{p}_k$.\n5.  The parameters are updated: $\\theta_{k+1} = \\theta_k + \\alpha_k \\mathbf{p}_k$.\n\n### Part 4: Gradient Descent Baseline\n\nThe GD implementation serves as a baseline. At each iteration $k$:\n1.  Compute $\\mathbf{g}_k = \\nabla \\mathcal{L}(\\theta_k)$.\n2.  The search direction is $\\mathbf{p}_k = -\\mathbf{g}_k$.\n3.  Use the same backtracking Armijo line search to find the step length $\\alpha_k$.\n4.  Update the parameters: $\\theta_{k+1} = \\theta_k + \\alpha_k \\mathbf{p}_k$.\n\n### Part 5: Analysis of Test Cases\n\nThe three test cases are designed to probe the behavior of the optimizers in different regimes:\n- **Case 1 (Near-Convex)**: Small initial weights keep the $\\tanh$ units in their linear region, making the loss surface nearly quadratic. Newton-CG is expected to converge much faster than GD by leveraging curvature information.\n- **Case 2 (Strongly Non-Convex)**: Large initial weights lead to sharp nonlinearities and potential saturation. Without damping ($\\mu=0$), the true Hessian is used, which is likely to have negative eigenvalues (negative curvature). Newton-CG's ability to detect and handle this is tested.\n- **Case 3 (Saturated/Flat)**: The specific initialization places the network in a region where the hidden units are saturated ($\\tanh(5) \\approx 1$). This causes gradients with respect to first-layer weights to be near zero, creating a flat loss landscape. GD will be very slow. Newton-CG, with damping $\\mu > 0$, effectively uses a rescaled gradient step ($\\mathbf{p} \\approx -(\\mu \\mathbf{I})^{-1}\\mathbf{g} = -\\mathbf{g}/\\mu$), which may allow it to escape the plateau more effectively.", "answer": "```python\nimport numpy as np\n\ndef flatten_params(params_dict):\n    \"\"\"Flattens a dictionary of parameters into a single vector.\"\"\"\n    return np.concatenate([p.flatten() for p in params_dict.values()])\n\ndef unflatten_params(flat_params, shapes):\n    \"\"\"Unflattens a vector into a dictionary of parameters.\"\"\"\n    params_dict = {}\n    current_pos = 0\n    for name, shape in shapes.items():\n        size = int(np.prod(shape))\n        params_dict[name] = flat_params[current_pos : current_pos + size].reshape(shape)\n        current_pos += size\n    return params_dict\n\nclass MLP:\n    \"\"\"A one-hidden-layer MLP with full support for AD-based derivatives.\"\"\"\n\n    def __init__(self, d=2, h=5):\n        self.d = d\n        self.h = h\n        self.param_shapes = {\n            'W1': (h, d), 'b1': (h,), 'W2': (1, h), 'b2': (1,)\n        }\n        self.num_params = sum(int(np.prod(s)) for s in self.param_shapes.values())\n\n    def init_params(self, case_config):\n        \"\"\"Initializes parameters based on case configuration.\"\"\"\n        seed = case_config.get('seed')\n        if seed is not None:\n            np.random.seed(seed)\n        \n        init_type = case_config['init_type']\n        \n        if init_type == 'normal':\n            scale = case_config['scale']\n            params = {name: np.random.randn(*shape) * scale for name, shape in self.param_shapes.items()}\n        elif init_type == 'specific':\n            params = {\n                'W1': np.zeros(self.param_shapes['W1']),\n                'b1': np.full(self.param_shapes['b1'], 5.0),\n                'W2': np.zeros(self.param_shapes['W2']),\n                'b2': np.zeros(self.param_shapes['b2'])\n            }\n        else:\n            raise ValueError(\"Unknown initialization type\")\n        \n        return params\n\n    def forward(self, params, X):\n        \"\"\"Performs a forward pass, returning all intermediate values.\"\"\"\n        W1, b1, W2, b2 = params['W1'], params['b1'], params['W2'], params['b2']\n        z1 = X @ W1.T + b1.reshape(1, -1)\n        a1 = np.tanh(z1)\n        z2 = a1 @ W2.T + b2\n        y_hat = z2\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'W2': W2}\n        return y_hat, cache\n\n    def loss_grad(self, params, X, y):\n        \"\"\"Computes the MSE loss and its gradient via backpropagation.\"\"\"\n        N = X.shape[0]\n        y_hat, cache = self.forward(params, X)\n        a1, W2 = cache['a1'], cache['W2']\n        \n        loss = np.sum((y_hat - y)**2) / (2 * N)\n\n        # Backward Pass (derivatives of L)\n        dy_hat_bar = (y_hat - y) / N\n        dz2_bar = dy_hat_bar\n        \n        grad_W2 = dz2_bar.T @ a1\n        grad_b2 = np.sum(dz2_bar, axis=0)\n        \n        da1_bar = dz2_bar @ W2\n        dz1_bar = da1_bar * (1 - a1**2)\n        \n        grad_W1 = dz1_bar.T @ X\n        grad_b1 = np.sum(dz1_bar, axis=0)\n        \n        grads = {'W1': grad_W1, 'b1': grad_b1, 'W2': grad_W2, 'b2': grad_b2}\n        return loss, grads\n\n    def hvp(self, params, v_flat, X, y):\n        \"\"\"Computes the Hessian-vector product using forward-over-reverse AD.\"\"\"\n        N = X.shape[0]\n        v = unflatten_params(v_flat, self.param_shapes)\n        W1, b1, W2, b2 = params['W1'], params['b1'], params['W2'], params['b2']\n        v_W1, v_b1, v_W2, v_b2 = v['W1'], v['b1'], v['W2'], v['b2']\n\n        # Augmented Forward Pass\n        z1 = X @ W1.T + b1.reshape(1, -1)\n        a1 = np.tanh(z1)\n        R_z1 = X @ v_W1.T + v_b1.reshape(1, -1)\n        R_a1 = (1 - a1**2) * R_z1\n        R_z2 = a1 @ v_W2.T + R_a1 @ W2.T + v_b2\n        \n        # Augmented Backward Pass (using L_sum = 0.5 * sum((y_hat-y)^2))\n        y_hat = a1 @ W2.T + b2\n        dz2_bar_sum = y_hat - y\n        da1_bar_sum = dz2_bar_sum @ W2\n\n        R_dz2_bar_sum = R_z2\n        \n        hvp_W2_sum = R_dz2_bar_sum.T @ a1 + dz2_bar_sum.T @ R_a1\n        hvp_b2_sum = np.sum(R_dz2_bar_sum, axis=0)\n        \n        R_da1_bar_sum = R_dz2_bar_sum @ W2 + dz2_bar_sum @ v_W2\n        R_dz1_bar_sum = R_da1_bar_sum * (1 - a1**2) - 2 * da1_bar_sum * a1 * R_a1\n\n        hvp_W1_sum = R_dz1_bar_sum.T @ X\n        hvp_b1_sum = np.sum(R_dz1_bar_sum, axis=0)\n\n        hvp_dict = {'W1': hvp_W1_sum/N, 'b1': hvp_b1_sum/N, 'W2': hvp_W2_sum/N, 'b2': hvp_b2_sum/N}\n        return flatten_params(hvp_dict)\n\ndef line_search(model, params, p_flat, grad_flat, X, y, c1, alpha_init, shrink, max_steps):\n    \"\"\"Backtracking Armijo line search.\"\"\"\n    loss_0, _ = model.loss_grad(params, X, y)\n    g_dot_p = grad_flat.T @ p_flat\n    \n    alpha = alpha_init\n    for _ in range(max_steps):\n        params_new_flat = flatten_params(params) + alpha * p_flat\n        params_new = unflatten_params(params_new_flat, model.param_shapes)\n        loss_new, _ = model.loss_grad(params_new, X, y)\n        if loss_new <= loss_0 + c1 * alpha * g_dot_p:\n            return alpha\n        alpha *= shrink\n    return alpha\n\ndef conjugate_gradient(hvp_fn, b, tol, max_iter, mu):\n    \"\"\"Solves (H+muI)p=b using Conjugate Gradient.\"\"\"\n    p = np.zeros_like(b)\n    r = b.copy()\n    d = r.copy()\n    rs_old = r.T @ r\n    neg_curvature_detected = False\n\n    for j in range(max_iter):\n        if np.sqrt(rs_old) < tol:\n            break\n        \n        Ad = hvp_fn(d) + mu * d\n        d_Ad = d.T @ Ad\n        \n        if d_Ad <= 0:\n            neg_curvature_detected = True\n            if j == 0: p = b\n            break\n        \n        alpha = rs_old / d_Ad\n        p += alpha * d\n        r -= alpha * Ad\n        rs_new = r.T @ r\n        \n        d = r + (rs_new / rs_old) * d\n        rs_old = rs_new\n    \n    return p, neg_curvature_detected\n\ndef train(model, params_init, X, y, optimizer_type, T, cg_max_iter, cg_tol, armijo_c1, mu):\n    \"\"\"Main training loop.\"\"\"\n    params = {k: v.copy() for k, v in params_init.items()}\n    neg_curvature_found_ever = False\n    \n    for _ in range(T):\n        loss, grads = model.loss_grad(params, X, y)\n        grad_flat = flatten_params(grads)\n\n        if optimizer_type == 'gd':\n            p_flat = -grad_flat\n        elif optimizer_type == 'ncg':\n            hvp_fn = lambda v: model.hvp(params, v, X, y)\n            p_flat, neg_curv = conjugate_gradient(hvp_fn, -grad_flat, cg_tol, cg_max_iter, mu)\n            if neg_curv:\n                neg_curvature_found_ever = True\n        else:\n            raise ValueError(\"Unknown optimizer\")\n        \n        alpha = line_search(model, params, p_flat, grad_flat, X, y, armijo_c1, 1.0, 0.5, 20)\n        \n        params_flat = flatten_params(params)\n        params_flat += alpha * p_flat\n        params = unflatten_params(params_flat, model.param_shapes)\n        \n    final_loss, _ = model.loss_grad(params, X, y)\n    return final_loss, neg_curvature_found_ever\n\ndef solve():\n    # Dataset\n    X = np.array([[-1,-1],[-1,1],[1,-1],[1,1],[-1,-1],[-1,1],[1,-1],[1,1]], dtype=float)\n    y = np.array([-1,1,1,-1,-1,1,1,-1], dtype=float).reshape(-1, 1)\n\n    # Global Parameters\n    T = 10\n    cg_max_iter = 50\n    cg_tol = 1e-10\n    armijo_c1 = 1e-4\n\n    model = MLP(d=2, h=5)\n\n    test_cases = [\n        {'id': 1, 'seed': 0, 'init_type': 'normal', 'scale': 0.01, 'mu': 1e-4},\n        {'id': 2, 'seed': 1, 'init_type': 'normal', 'scale': 3.0, 'mu': 0.0},\n        {'id': 3, 'init_type': 'specific', 'mu': 1e-2},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        params_init = model.init_params(case)\n\n        ncg_loss, neg_curv_detected = train(\n            model, params_init, X, y, 'ncg', T, cg_max_iter, cg_tol, armijo_c1, case['mu']\n        )\n        gd_loss, _ = train(\n            model, params_init, X, y, 'gd', T, None, None, armijo_c1, None\n        )\n        \n        ncg_helped = ncg_loss < gd_loss\n        \n        case_results = [float(ncg_loss), float(gd_loss), bool(ncg_helped), bool(neg_curv_detected)]\n        all_results.append(case_results)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3100512"}]}