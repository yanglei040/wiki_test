{"hands_on_practices": [{"introduction": "This first exercise is a deep dive into the theoretical underpinnings of Fourier spectral methods. Before writing any code, it is crucial to understand how the properties of a function and the structure of the grid interact to produce a derivative. By analyzing a simple, symmetric function, this problem reveals the remarkable precision of spectral differentiation and shows how it can be analyzed from first principles without ever explicitly constructing the full matrix [@problem_id:3277346].", "problem": "Let $N$ be an even integer with $N \\geq 4$, and consider the $N$ equispaced Fourier collocation nodes $x_{j} = \\frac{2\\pi j}{N}$ for $j = 0, 1, \\dots, N-1$ on the interval $[0, 2\\pi)$, where all angles are measured in radians. Let $f(x) = \\sin(\\cos(x))$. Define the trigonometric interpolant $p_{N}(x)$ of $f(x)$ as the unique trigonometric polynomial of degree at most $\\frac{N}{2}$ that interpolates $f$ at the nodes $\\{x_{j}\\}_{j=0}^{N-1}$. The Fourier spectral differentiation matrix $D \\in \\mathbb{R}^{N \\times N}$ is defined so that the vector of nodal derivatives of $p_{N}$ satisfies $(D\\boldsymbol{f})_{j} = p_{N}'(x_{j})$, where $\\boldsymbol{f} = \\big(f(x_{0}), \\dots, f(x_{N-1})\\big)^{\\mathsf{T}}$.\n\nUsing only foundational definitions of trigonometric interpolation and the induced spectral differentiation matrix, compute the exact value of $(D\\boldsymbol{f})_{0}$ and verify that it equals the analytic chain-rule derivative $f'(0)$. Provide your final answer as a single exact number (no rounding).", "solution": "The problem asks for the computation of $(D\\boldsymbol{f})_{0}$, which is the derivative of the trigonometric interpolant $p_{N}(x)$ of the function $f(x) = \\sin(\\cos(x))$ evaluated at the node $x_{0}=0$. We are to verify that this value equals the analytic derivative $f'(0)$.\n\nThe trigonometric interpolant $p_{N}(x)$ of a function $f(x)$ at the $N$ equispaced nodes $x_{j} = \\frac{2\\pi j}{N}$ for $j = 0, 1, \\dots, N-1$ is the unique trigonometric polynomial of degree at most $\\frac{N}{2}$ that satisfies $p_{N}(x_{j}) = f(x_{j})$ for all $j$. For an even integer $N$, this interpolant is given by the complex Fourier series representation:\n$$p_{N}(x) = \\sum_{k=-N/2}^{N/2} ' \\hat{f}_{k} e^{ikx}$$\nwhere the prime on the summation symbol indicates that the terms for $k = -N/2$ and $k=N/2$ are multiplied by a factor of $\\frac{1}{2}$. The coefficients $\\hat{f}_{k}$ are the discrete Fourier coefficients of the sampled function values $\\boldsymbol{f} = \\big(f(x_{0}), \\dots, f(x_{N-1})\\big)^{\\mathsf{T}}$, defined as:\n$$\\hat{f}_{k} = \\frac{1}{N} \\sum_{j=0}^{N-1} f(x_{j}) e^{-ikx_{j}}$$\n\nThe quantity to be computed is $(D\\boldsymbol{f})_{0}$, which is by definition $p_{N}'(x_{0})$. First, we find the derivative of the interpolating polynomial, $p_{N}'(x)$, by differentiating term-by-term:\n$$p_{N}'(x) = \\frac{d}{dx} \\left( \\sum_{k=-N/2}^{N/2} ' \\hat{f}_{k} e^{ikx} \\right) = \\sum_{k=-N/2}^{N/2} ' (ik) \\hat{f}_{k} e^{ikx}$$\nEvaluating this at the node $x_{0} = 0$:\n$$(D\\boldsymbol{f})_{0} = p_{N}'(0) = \\sum_{k=-N/2}^{N/2} ' (ik) \\hat{f}_{k} e^{ik(0)} = i \\sum_{k=-N/2}^{N/2} ' k \\hat{f}_{k}$$\n\nTo evaluate this sum, we must first analyze the properties of the coefficients $\\hat{f}_{k}$. These properties are dictated by the symmetries of the function $f(x) = \\sin(\\cos(x))$ and the grid $\\{x_{j}\\}$.\n\nThe function $f(x)$ is an even function, because $f(-x) = \\sin(\\cos(-x)) = \\sin(\\cos(x)) = f(x)$.\nThe grid points are symmetric about $x=0$ on the interval $[0, 2\\pi)$. For any node $x_{j}$ with $j \\in \\{1, \\dots, N-1\\}$, its symmetric counterpart is $x_{N-j} = \\frac{2\\pi(N-j)}{N} = 2\\pi - \\frac{2\\pi j}{N} = 2\\pi - x_{j}$.\nSince $f(x)$ is $2\\pi$-periodic, $f(x_{N-j}) = f(2\\pi - x_{j}) = f(-x_{j})$.\nBecause $f(x)$ is an even function, $f(-x_{j}) = f(x_{j})$.\nThus, the sampled function values exhibit symmetry: $f(x_{N-j}) = f(x_{j})$ for $j=1, \\dots, N-1$.\n\nThis symmetry property of the sampled data implies a symmetry in the discrete Fourier coefficients. Specifically, we will show that $\\hat{f}_{k}$ is a real-valued sequence and, as a consequence, $\\hat{f}_{-k} = \\hat{f}_{k}$.\n\nFor a real-valued function $f(x)$, we have the general property $\\hat{f}_{-k} = \\overline{\\hat{f}_{k}}$:\n$$\\overline{\\hat{f}_{k}} = \\overline{\\frac{1}{N} \\sum_{j=0}^{N-1} f(x_{j}) e^{-ikx_{j}}} = \\frac{1}{N} \\sum_{j=0}^{N-1} \\overline{f(x_{j})} \\overline{e^{-ikx_{j}}} = \\frac{1}{N} \\sum_{j=0}^{N-1} f(x_{j}) e^{ikx_{j}} = \\hat{f}_{-k}$$\nNow we demonstrate that $\\hat{f}_k$ is real for our specific even function $f(x)$. The imaginary part of $\\hat{f}_k$ is given by:\n$$\\text{Im}(\\hat{f}_{k}) = -\\frac{1}{N} \\sum_{j=0}^{N-1} f(x_{j}) \\sin(kx_{j})$$\nWe can split the sum:\n$$\\sum_{j=0}^{N-1} f(x_{j}) \\sin(kx_{j}) = f(x_{0})\\sin(kx_{0}) + \\sum_{j=1}^{N-1} f(x_{j})\\sin(kx_{j})$$\nThe first term is zero since $x_{0}=0$. The sum can be paired up. Since $N$ is even, there is a term for $j=N/2$.\n$$\\sum_{j=1}^{N-1} (\\dots) = \\sum_{j=1}^{N/2-1} \\left[ f(x_{j})\\sin(kx_{j}) + f(x_{N-j})\\sin(kx_{N-j}) \\right] + f(x_{N/2})\\sin(kx_{N/2})$$\nUsing $f(x_{N-j})=f(x_{j})$ and $\\sin(kx_{N-j}) = \\sin(k(2\\pi-x_{j})) = \\sin(-kx_{j}) = -\\sin(kx_{j})$, each pair in the summation is:\n$$f(x_{j})\\sin(kx_{j}) + f(x_{j})(-\\sin(kx_{j})) = 0$$\nThe term for the midpoint $j=N/2$ is $f(x_{N/2})\\sin(kx_{N/2}) = f(\\pi)\\sin(k\\pi) = 0$ since $k$ is an integer.\nThus, $\\text{Im}(\\hat{f}_{k}) = 0$, which means $\\hat{f}_{k}$ is real.\nSince $\\hat{f}_{k}$ is real and $\\hat{f}_{-k} = \\overline{\\hat{f}_{k}}$, it follows that $\\hat{f}_{-k}=\\hat{f}_{k}$.\n\nNow we can evaluate the sum for $p_{N}'(0)$.\n$$p_{N}'(0) = i \\sum_{k=-N/2}^{N/2} ' k \\hat{f}_{k}$$\nLet's expand the sum, using the prime notation:\n$$ \\sum_{k=-N/2}^{N/2} ' k \\hat{f}_{k} = \\frac{1}{2}(-N/2)\\hat{f}_{-N/2} + \\sum_{k=1-N/2}^{-1} k\\hat{f}_{k} + (0)\\hat{f}_{0} + \\sum_{k=1}^{N/2-1} k\\hat{f}_{k} + \\frac{1}{2}(N/2)\\hat{f}_{N/2}$$\nWe can rewrite the first summation by letting $m=-k$:\n$$\\sum_{k=1-N/2}^{-1} k\\hat{f}_{k} = \\sum_{m=1}^{N/2-1} (-m)\\hat{f}_{-m}$$\nUsing the symmetry property $\\hat{f}_{-m} = \\hat{f}_{m}$, this becomes $-\\sum_{m=1}^{N/2-1} m\\hat{f}_{m}$.\nThe sum is therefore:\n$$-\\frac{N}{4}\\hat{f}_{-N/2} - \\sum_{m=1}^{N/2-1} m\\hat{f}_{m} + \\sum_{k=1}^{N/2-1} k\\hat{f}_{k} + \\frac{N}{4}\\hat{f}_{N/2}$$\nThe two summations cancel each other out. The remaining terms are:\n$$-\\frac{N}{4}\\hat{f}_{-N/2} + \\frac{N}{4}\\hat{f}_{N/2}$$\nSince the symmetry $\\hat{f}_{-k} = \\hat{f}_{k}$ also holds for the endpoint $k=N/2$, this expression is:\n$$-\\frac{N}{4}\\hat{f}_{N/2} + \\frac{N}{4}\\hat{f}_{N/2} = 0$$\nThus, the entire sum is $0$, and we have $(D\\boldsymbol{f})_{0} = p_{N}'(0) = i \\cdot 0 = 0$.\n\nTo verify this result, we compute the analytic derivative of $f(x)=\\sin(\\cos(x))$ using the chain rule:\n$$f'(x) = \\frac{d}{dx}\\sin(\\cos(x)) = \\cos(\\cos(x)) \\cdot \\frac{d}{dx}(\\cos(x)) = \\cos(\\cos(x)) \\cdot (-\\sin(x))$$\nEvaluating the derivative at $x=0$:\n$$f'(0) = -\\sin(0) \\cos(\\cos(0)) = -0 \\cdot \\cos(1) = 0$$\nThe value of the derivative of the interpolant at $x_{0}=0$ is $0$, which is identical to the analytic derivative $f'(0)$. This confirms the correctness of the spectral differentiation for this particular case, a known property for the derivative of an even function evaluated at the point of symmetry.\n\nThe requested exact value is $0$.", "answer": "$$\\boxed{0}$$", "id": "3277346"}, {"introduction": "Having explored differentiation, we now turn to its inverse operation: integration. A naive approach of inverting the differentiation matrix $D_N$ fails because it is singular, as it maps constant functions to zero. This hands-on coding practice introduces a sophisticated and powerful solution by using the Moore-Penrose pseudo-inverse to define a robust spectral integration matrix, effectively transforming definite integration into a problem of applied linear algebra [@problem_id:3277283].", "problem": "You are to implement a complete, runnable program that constructs a spectral integration matrix for Chebyshev–Lobatto nodes by using the Moore–Penrose pseudo-inverse of the spectral differentiation matrix, and then uses it to approximate definite integrals. The context is polynomial interpolation and spectral differentiation, framed in purely mathematical terms.\n\nStarting point and definitions:\n- Consider the interval $[-1,1]$ and the Chebyshev–Lobatto nodes $x_j$ for $j=0,1,\\dots,N$ given by $x_j = \\cos\\left(\\frac{\\pi j}{N}\\right)$.\n- Let $p_N(x)$ denote the unique interpolating polynomial of degree at most $N$ that satisfies $p_N(x_j) = f_j$, where $f_j = f(x_j)$ are the nodal values of an unknown function $f$.\n- Define the spectral differentiation matrix $D_N \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ by the rule that for a nodal vector $v \\in \\mathbb{R}^{N+1}$ representing samples of a sufficiently smooth function $g$ at the nodes $x_j$, the product $D_N v$ approximates the nodal samples of $g'(x)$ at the same nodes, i.e., $\\left(D_N v\\right)_i \\approx \\frac{d}{dx}p_N(x)\\big|_{x=x_i}$, where $p_N$ interpolates $g$ at the nodes. This definition must be derived from the fundamental notion of polynomial interpolation and the derivative of the interpolant; no shortcut formulas are to be assumed in advance.\n- Let $I_N$ be the Moore–Penrose pseudo-inverse of $D_N$, denoted $D_N^{+}$. This is the unique matrix satisfying the Moore–Penrose conditions $D_N D_N^{+} D_N = D_N$, $D_N^{+} D_N D_N^{+} = D_N^{+}$, $(D_N D_N^{+})^\\top = D_N D_N^{+}$, and $(D_N^{+} D_N)^\\top = D_N^{+} D_N$.\n\nIntegration via pseudo-inverse:\n- For a nodal vector $f \\in \\mathbb{R}^{N+1}$ representing samples of a function $f(x)$, define $g = I_N f$. Then $g$ represents a discrete anti-derivative of $f$ up to an additive constant, in the sense that $D_N g$ approximates $f$ in the least-squares sense inherent to the pseudo-inverse. To obtain a definite integral from the left endpoint, enforce a physically meaningful constant by setting the integral at $x=-1$ to be zero: $\\int_{-1}^{x_j} f(x)\\,dx \\approx g_j - g_{N}$, since $x_N = -1$.\n- The whole-interval definite integral is then approximated by $\\int_{-1}^{1} f(x)\\,dx \\approx g_0 - g_N$, since $x_0 = 1$ and $x_N = -1$.\n\nAngle unit convention:\n- All trigonometric functions must use angles in radians.\n\nAccuracy metrics to compute:\n- For each test case, compute:\n  1. The maximum absolute error across nodes for the cumulative definite integrals from $-1$ to $x_j$, i.e., $\\max_{0 \\le j \\le N} \\left|\\left(g_j - g_N\\right) - \\left(F(x_j) - F(-1)\\right)\\right|$, where $F$ is an analytic anti-derivative of $f$ satisfying $F'(x) = f(x)$.\n  2. The absolute error for the whole-interval integral, i.e., $\\left| \\left(g_0 - g_N\\right) - \\int_{-1}^{1} f(x)\\,dx \\right|$.\n\nTest suite:\n- Implement the following test cases, each specified by a function $f(x)$ and an integer $N$:\n  1. Analytic function with exponential growth: $f(x) = e^{x}$, with $N = 8$.\n  2. Same analytic function to assess convergence: $f(x) = e^{x}$, with $N = 16$.\n  3. Same analytic function for further convergence: $f(x) = e^{x}$, with $N = 32$.\n  4. Oscillatory analytic function: $f(x) = \\sin(7x)$, with $N = 16$.\n  5. Polynomial function: $f(x) = x^{5} - 2x^{3} + x$, with $N = 6$.\n  6. Constant function (nullspace edge case): $f(x) = 3$, with $N = 10$.\n  7. Non-smooth function: $f(x) = |x|$, with $N = 64$.\n\nFor each $f(x)$ above, take its anti-derivative $F(x)$ as follows, to permit exact error evaluation:\n- Case $f(x) = e^{x}$: $F(x) = e^{x}$.\n- Case $f(x) = \\sin(7x)$: $F(x) = -\\frac{\\cos(7x)}{7}$.\n- Case $f(x) = x^{5} - 2x^{3} + x$: $F(x) = \\frac{x^{6}}{6} - \\frac{x^{4}}{2} + \\frac{x^{2}}{2}$.\n- Case $f(x) = 3$: $F(x) = 3x$.\n- Case $f(x) = |x|$: $F(x) = \\frac{1}{2}x|x|$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the flat order\n  $[E_{1,\\text{nodes}}, E_{1,\\text{interval}}, E_{2,\\text{nodes}}, E_{2,\\text{interval}}, \\dots]$,\n  where $E_{k,\\text{nodes}}$ is the maximum nodal cumulative integral error and $E_{k,\\text{interval}}$ is the whole-interval integral error for the $k$-th test case. Each entry must be a floating-point number. No other text should be printed.", "solution": "The task is to construct a spectral integration matrix for Chebyshev–Lobatto nodes by computing the Moore–Penrose pseudo-inverse of the corresponding spectral differentiation matrix. This integration matrix is then used to approximate definite integrals for a suite of test functions.\n\n### Principle of the Method\nThe core principle of spectral methods is the approximation of a function $f(x)$ on an interval, here $[-1, 1]$, by a single high-degree polynomial, $p_N(x)$. This polynomial is uniquely defined by the condition that it interpolates the function at a set of $N+1$ distinct nodes, $x_j$. For high accuracy and stability, Chebyshev nodes are a preferred choice. The Chebyshev–Lobatto nodes are given by $x_j = \\cos\\left(\\frac{j\\pi}{N}\\right)$ for $j=0, 1, \\dots, N$.\n\nThe derivative of the interpolating polynomial, $p_N'(x)$, serves as an approximation to the derivative of the original function, $f'(x)$. Since differentiation is a linear operation, the mapping from the nodal values of the function, $f_j = f(x_j)$, to the nodal values of its approximate derivative, $p_N'(x_i)$, can be represented by a matrix-vector product. This matrix is the spectral differentiation matrix, $D_N$.\n\nIntegration is the inverse operation of differentiation. Consequently, an approximation to the integral of $f(x)$ can be obtained by applying the inverse of the matrix $D_N$ to the vector of nodal values $f_j$. However, the differentiation operator has a one-dimensional null space consisting of constant functions; any constant vector $\\mathbf{c} = (c, c, \\dots, c)^\\top$ is mapped to zero. Therefore, $D_N$ is singular and its standard inverse does not exist.\n\nThe Moore–Penrose pseudo-inverse, denoted $D_N^+$, provides a well-defined substitute for the inverse in such cases. For a vector of nodal values $f$, the vector $g = D_N^+ f$ represents the nodal values of an anti-derivative of $f$. Specifically, $D_N g$ is the least-squares best approximation of $f$ within the range of $D_N$. The resulting anti-derivative $g$ is unique under the condition that it is orthogonal to the null space of $D_N$, which for Chebyshev differentiation means it has a zero mean value.\n\n### Construction of the Spectral Differentiation Matrix $D_N$\nThe interpolating polynomial $p_N(x)$ through the points $(x_j, f_j)$ is given in the Lagrange form as $p_N(x) = \\sum_{j=0}^{N} f_j L_j(x)$, where $L_j(x)$ are the Lagrange basis polynomials. The derivative is $p_N'(x) = \\sum_{j=0}^{N} f_j L_j'(x)$. Evaluating this at the nodes $x_i$ gives $p_N'(x_i) = \\sum_{j=0}^{N} f_j L_j'(x_i)$. The entries of the differentiation matrix $D_N$ are therefore given by $(D_N)_{ij} = L_j'(x_i)$.\n\nWhile these entries can be derived by differentiating the explicit formula for $L_j(x)$, a more practical and stable set of formulas can be derived by leveraging the properties of Chebyshev polynomials. This standard derivation, found in texts on spectral methods, yields the following expressions for the entries of $D_N$ for the Chebyshev–Lobatto nodes $x_j = \\cos(j\\pi/N)$:\n- The off-diagonal entries are:\n$$ (D_N)_{ij} = \\frac{c_i}{c_j} \\frac{(-1)^{i+j}}{x_i - x_j}, \\quad i \\neq j $$\n- The diagonal entries are:\n$$ (D_N)_{ii} = -\\frac{x_i}{2(1-x_i^2)}, \\quad i=1, 2, \\dots, N-1 $$\n- The corner (diagonal) entries are special cases:\n$$ (D_N)_{00} = \\frac{2N^2+1}{6} $$\n$$ (D_N)_{NN} = -\\frac{2N^2+1}{6} $$\n- The coefficients $c_j$ are defined as:\n$$ c_j = \\begin{cases} 2 & j=0 \\text{ or } j=N \\\\ 1 & 1 \\le j \\le N-1 \\end{cases} $$\nThese formulas will be used to construct the matrix $D_N$.\n\n### Integration Procedure\nOnce $D_N$ is constructed, its Moore–Penrose pseudo-inverse, $I_N = D_N^+$, is computed using a standard numerical linear algebra algorithm, typically one based on Singular Value Decomposition (SVD).\n\nGiven a vector $f$ of function values at the nodes, we compute the vector $g = I_N f$, which contains the nodal values of a particular anti-derivative. To compute a definite integral, for example $\\int_{-1}^{x_j} f(t)dt$, we must account for the constant of integration. We can enforce the condition that the integral is zero at its lower limit, $x=-1$. In our nodal indexing, $x_N = \\cos(\\pi N/N) = \\cos(\\pi) = -1$. An approximation to the cumulative integral $F(x_j)-F(-1)$ is thus given by $g_j - g_N$.\n\nThe definite integral over the entire interval, $\\int_{-1}^{1} f(t)dt$, is then approximated by evaluating the cumulative integral at the upper endpoint, $x=1$. Since $x_0 = \\cos(0) = 1$, this corresponds to $g_0 - g_N$.\n\n### Algorithm Summary\nThe implementation will follow these steps for each test case $(f(x), F(x), N)$:\n1.  Construct the $(N+1) \\times (N+1)$ Chebyshev differentiation matrix $D_N$ using the formulas specified above.\n2.  Compute its Moore–Penrose pseudo-inverse $I_N = D_N^+$ using `numpy.linalg.pinv`.\n3.  Generate the vector of Chebyshev–Lobatto nodes $x_j$.\n4.  Evaluate the function $f(x)$ at these nodes to create the vector $f$.\n5.  Compute the nodal values of the anti-derivative: $g = I_N f$.\n6.  Calculate the approximate cumulative integral from $-1$ to each $x_j$ as $g_j - g_N$.\n7.  Calculate the approximate whole-interval integral as $g_0 - g_N$.\n8.  Compute the true cumulative integral values using the provided anti-derivative $F(x)$ as $F(x_j) - F(-1)$.\n9.  Compute the true whole-interval integral as $F(1) - F(-1)$.\n10. Calculate the two required error metrics: the maximum absolute error across all nodes for the cumulative integral, and the absolute error for the whole-interval integral.\n\nThe test case $f(x)=3$ is designed to highlight a key property of this method. Since constant functions are in the null space of $D_N$, and $D_N^+$ gives a solution orthogonal to the null space, the method effectively computes an antiderivative whose nodal values have a zero mean. This procedure will yield an incorrect result for the integral of a function with a non-zero mean component unless a correction is applied. This problem does not include such a correction, so a large error is expected, correctly demonstrating a key feature of the pseudo-inverse approach.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef chebyshev_lobatto_differentiation_matrix(N):\n    \"\"\"\n    Constructs the (N+1)x(N+1) Chebyshev spectral differentiation matrix\n    for the Chebyshev-Lobatto nodes.\n    \"\"\"\n    if N == 0:\n        return np.array([[0.0]])\n    \n    n_plus_1 = N + 1\n    # Create nodes x_j = cos(j*pi/N)\n    j = np.arange(n_plus_1)\n    x = np.cos(np.pi * j / N)\n    \n    # Initialize differentiation matrix\n    D = np.zeros((n_plus_1, n_plus_1))\n    \n    # c_j coefficients\n    c = np.ones(n_plus_1)\n    c[0] = 2.0\n    c[N] = 2.0\n    \n    # Off-diagonal elements (vectorized for columns)\n    for i in range(n_plus_1):\n        # Create a view of x without x_i\n        x_diff = x[i] - x\n        # Avoid division by zero, it will be overwritten by the diagonal value\n        x_diff[i] = 1.0  \n        \n        term = (c[i] / c) * ((-1)**(i + j)) / x_diff\n        D[i, :] = term\n        \n    # Diagonal elements\n    # For i = 1, ..., N-1\n    for i in range(1, N):\n        D[i, i] = -x[i] / (2.0 * (1.0 - x[i]**2))\n    \n    # Corner elements\n    D[0, 0] = (2.0 * N**2 + 1.0) / 6.0\n    D[N, N] = -(2.0 * N**2 + 1.0) / 6.0\n    \n    return D\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute integration errors.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case: (function f, anti-derivative F, integer N)\n    test_cases = [\n        (lambda x: np.exp(x), lambda x: np.exp(x), 8),\n        (lambda x: np.exp(x), lambda x: np.exp(x), 16),\n        (lambda x: np.exp(x), lambda x: np.exp(x), 32),\n        (lambda x: np.sin(7.0 * x), lambda x: -np.cos(7.0 * x) / 7.0, 16),\n        (lambda x: x**5 - 2.0*x**3 + x, lambda x: x**6/6.0 - x**4/2.0 + x**2/2.0, 6),\n        (lambda x: 3.0 + 0.0*x, lambda x: 3.0 * x, 10), # 0.0*x for vectorization\n        (lambda x: np.abs(x), lambda x: 0.5 * x * np.abs(x), 64),\n    ]\n\n    results = []\n    \n    for f, F, N in test_cases:\n        # 1. Construct Chebyshev nodes\n        j_indices = np.arange(N + 1)\n        nodes = np.cos(np.pi * j_indices / N)\n        \n        # 2. Construct the spectral differentiation matrix D_N\n        D_N = chebyshev_lobatto_differentiation_matrix(N)\n        \n        # 3. Compute the Moore-Penrose pseudo-inverse I_N\n        I_N = np.linalg.pinv(D_N)\n        \n        # 4. Get nodal values of the function f\n        f_vec = f(nodes)\n        \n        # 5. Compute the discrete anti-derivative g = I_N * f\n        g_vec = I_N @ f_vec\n        \n        # 6. Compute approximate integrals\n        # Cumulative integral from -1 (node N) to x_j\n        approx_cumulative_integral = g_vec - g_vec[N]\n        # Whole-interval integral from -1 (node N) to 1 (node 0)\n        approx_total_integral = g_vec[0] - g_vec[N]\n        \n        # 7. Compute exact integrals\n        # Exact cumulative integral from -1 to x_j\n        exact_cumulative_integral = F(nodes) - F(-1.0)\n        # Exact whole-interval integral from -1 to 1\n        exact_total_integral = F(1.0) - F(-1.0)\n\n        # 8. Calculate errors\n        # Max absolute error for cumulative integrals\n        max_nodal_error = np.max(np.abs(approx_cumulative_integral - exact_cumulative_integral))\n        # Absolute error for the whole-interval integral\n        total_interval_error = np.abs(approx_total_integral - exact_total_integral)\n\n        results.append(max_nodal_error)\n        results.append(total_interval_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "3277283"}, {"introduction": "We now advance to a challenge that is central to applying spectral methods in science and engineering: solving nonlinear equations. When nonlinear terms like $u^2$ are computed, they generate high-frequency content that can fold back and corrupt the solution, a phenomenon known as aliasing. This practice asks you to implement and compare a naive approach with a standard de-aliasing technique, the \"2/3-rule,\" demonstrating a critical tool for ensuring stability and accuracy in complex simulations [@problem_id:3277414].", "problem": "Consider the periodic domain $[0,2\\pi)$ with $N$ equispaced collocation points $x_j = \\frac{2\\pi j}{N}$ for $j=0,1,\\dots,N-1$. You will construct a Fourier spectral differentiation matrix to approximate spatial derivatives and investigate aliasing and de-aliasing when computing the nonlinear flux derivative $(u^2)_x$ that arises in the inviscid Burgers equation $u_t + (u^2)_x = 0$. Angles must be interpreted in radians.\n\nStarting from fundamental bases only, use the facts that exponentials $e^{i k x}$ are eigenfunctions of the derivative operator with $ \\frac{d}{dx} e^{i k x} = i k e^{i k x}$ for integer $k$, and that the discrete Fourier transform represents a function on the grid through its discrete Fourier coefficients. From these principles:\n\n- Derive the matrix representation $D \\in \\mathbb{C}^{N \\times N}$ of the first derivative operator on the grid $\\{x_j\\}$ in the Fourier collocation method. The derivation must begin from how the discrete Fourier transform and its inverse map nodal values to discrete Fourier coefficients and back, and from the eigenfunction property of $e^{i k x}$, without assuming any pre-stated differentiation matrix formula. The resulting matrix $D$ should satisfy, for any grid function $f \\in \\mathbb{C}^N$, that the vector $Df$ approximates $\\frac{d f}{dx}$ at the grid points.\n- Explain why evaluating a nonlinear product in physical space (e.g., computing $u^2$ pointwise) and then transforming to Fourier space introduces aliasing errors. Use the discrete convolution interpretation to argue how unresolved high-frequency modes wrap into lower modes on a grid with $N$ points.\n- Implement two pseudospectral approximations of $(u^2)_x$:\n  1. A naive pseudospectral method: compute $f = u^2$ pointwise, then approximate $(u^2)_x$ by applying the spectral differentiation matrix $D$ to $f$, i.e., $(u^2)_x \\approx D f$.\n  2. A de-aliased method using the $2/3$-rule: after forming $f = u^2$ in physical space, transform $f$ to Fourier space to obtain $\\hat{f}_k$, zero out all modes with $|k| > \\lfloor N/3 \\rfloor$, multiply by $i k$ in Fourier space, and transform back to physical space to obtain a de-aliased approximation of $(u^2)_x$.\n\nFor each test case below, compute the pointwise exact derivative $(u^2)_x = 2 u u_x$ from the given $u(x)$ and $u_x(x)$, and then compute the following two errors:\n- The naive pseudospectral maximum-norm error $E_{\\text{naive}} = \\max_{0 \\le j \\le N-1} \\left| (D f)_j - 2 u(x_j) u_x(x_j) \\right|$.\n- The $2/3$-rule de-aliased maximum-norm error $E_{2/3} = \\max_{0 \\le j \\le N-1} \\left| \\left[\\mathcal{F}^{-1}\\left( i k \\cdot \\chi_{|k| \\le \\lfloor N/3 \\rfloor} \\cdot \\hat{f}_k \\right)\\right]_j - 2 u(x_j) u_x(x_j) \\right|$, where $\\chi$ is the indicator function and $\\mathcal{F}^{-1}$ is the inverse discrete Fourier transform.\n\nTest suite (all angles in radians):\n- Case A (well-resolved smooth input): $N = 32$, $u(x) = \\sin(x) + 0.5 \\sin(2x)$, so $u_x(x) = \\cos(x) + \\cos(2x)$ and $(u^2)_x = 2 u u_x$.\n- Case B (undersampling to expose aliasing): $N = 15$, $u(x) = \\sin(5x) + 0.4 \\cos(7x)$, so $u_x(x) = 5 \\cos(5x) - 2.8 \\sin(7x)$ and $(u^2)_x = 2 u u_x$.\n- Case C (borderline $2/3$-rule content): $N = 12$, $u(x) = \\sin(4x)$, so $u_x(x) = 4 \\cos(4x)$ and $(u^2)_x = 2 u u_x$.\n\nYour program must:\n- Construct the Fourier spectral differentiation matrix $D$ from first principles as implied by your derivation (you may use fast Fourier transform routines to assemble $D$ or to apply it).\n- For each case, compute both $E_{\\text{naive}}$ and $E_{2/3}$ as defined above.\n- Produce a single line of output containing the six results in the order $[E_{\\text{naive}}^{\\text{A}}, E_{2/3}^{\\text{A}}, E_{\\text{naive}}^{\\text{B}}, E_{2/3}^{\\text{B}}, E_{\\text{naive}}^{\\text{C}}, E_{2/3}^{\\text{C}}]$ as a comma-separated list enclosed in square brackets. Values must be decimal floating-point numbers.\n\nNo user input is required. All computations are nondimensional and unitless, and all functions of $x$ use radians.", "solution": "The solution is presented in three parts: first, a derivation of the Fourier spectral differentiation matrix from fundamental principles; second, an explanation of aliasing in nonlinear terms; and third, an analysis of the two pseudospectral methods to be implemented.\n\n### 1. Derivation of the Fourier Spectral Differentiation Matrix\n\nA function $f(x)$ defined on a periodic domain $[0, 2\\pi)$ can be represented on a grid of $N$ equispaced points $x_j = \\frac{2\\pi j}{N}$ for $j=0, 1, \\dots, N-1$ by the vector of its nodal values, $\\mathbf{f} = [f(x_0), f(x_1), \\dots, f(x_{N-1})]^T$. The core principle of Fourier spectral methods is to use a truncated Fourier series as a global interpolant for the function.\n\nThe Discrete Fourier Transform (DFT) maps the vector of physical space values $\\mathbf{f}$ to a vector of complex Fourier coefficients $\\hat{\\mathbf{f}}$. The conventional definition of the forward DFT is:\n$$\n\\hat{f}_k = \\sum_{j=0}^{N-1} f_j e^{-i k x_j} = \\sum_{j=0}^{N-1} f_j e^{-i 2\\pi k j / N}\n$$\nwhere $f_j = f(x_j)$ and the integer $k$ represents the wavenumber index. The inverse DFT (IDFT) transforms the coefficients back to the physical space values:\n$$\nf_j = \\frac{1}{N} \\sum_{k=0}^{N-1} \\hat{f}_k e^{i k x_j} = \\frac{1}{N} \\sum_{k=0}^{N-1} \\hat{f}_k e^{i 2\\pi k j / N}\n$$\nThe problem states that the complex exponentials $e^{ikx}$ are eigenfunctions of the differentiation operator, i.e., $\\frac{d}{dx} e^{ikx} = ik e^{ikx}$. This property allows differentiation to be performed in Fourier space as a simple multiplication. The spectral differentiation of $f(x)$ is achieved via a three-step process:\n1.  Transform the grid function $\\mathbf{f}$ into its Fourier coefficients $\\hat{\\mathbf{f}}$ using the DFT.\n2.  Multiply each Fourier coefficient $\\hat{f}_k$ by its corresponding differentiated eigenvalue, $ik'$, where $k'$ is the true angular wavenumber.\n3.  Transform the resulting coefficients back to physical space using the IDFT.\n\nLet us denote the vector of differentiated Fourier coefficients as $\\hat{\\mathbf{g}}$. Then, $\\hat{g}_k = ik' \\hat{f}_k$. The resulting vector of derivatives in physical space, $\\mathbf{g}$, is $\\mathbf{g} = \\mathcal{F}^{-1}(\\hat{\\mathbf{g}})$.\n\nThe mapping from the DFT index $k \\in \\{0, 1, \\dots, N-1\\}$ to the true angular wavenumber $k'$ must be established. For a grid of $N$ points, the representable wavenumbers are band-limited. The standard set of wavenumbers is given by $k' \\in \\{0, \\pm 1, \\dots, \\pm (N/2-1), -N/2\\}$ for $N$ even, and $k' \\in \\{0, \\pm 1, \\dots, \\pm (N-1)/2\\}$ for $N$ odd. This corresponds to the ordering provided by `numpy.fft.fftfreq`.\n\nA special consideration arises for $N$ even. The Nyquist frequency, corresponding to $k'=N/2$ (or $-N/2$), represents the highest frequency mode, $e^{i (N/2) x_j} = e^{i\\pi j} = (-1)^j$. Differentiating this mode as $i(N/2)e^{i(N/2)x_j}$ would produce a complex result from a real function, breaking the real-to-real property of the differentiation operator. To preserve this property, the derivative of the Nyquist mode is conventionally set to zero.\n\nWe can express this entire process as a single matrix-vector product, $\\mathbf{g} = D\\mathbf{f}$, where $D$ is the $N \\times N$ spectral differentiation matrix. Let $\\mathcal{F}$ and $\\mathcal{F}^{-1}$ be the matrices representing the DFT and IDFT operations, and let $\\Lambda$ be the diagonal matrix of eigenvalues $ik'$. The differentiation matrix is then $D = \\mathcal{F}^{-1} \\Lambda \\mathcal{F}$.\n\nThe element $D_{jl}$ of the matrix $D$ can be derived explicitly:\n$$\n(D\\mathbf{f})_j = \\frac{1}{N} \\sum_{k} (ik') \\left( \\sum_{l=0}^{N-1} f_l e^{-ik'x_l} \\right) e^{ik'x_j} = \\sum_{l=0}^{N-1} \\left( \\frac{1}{N} \\sum_{k} ik' e^{ik'(x_j - x_l)} \\right) f_l\n$$\nThe term in parentheses is the matrix element $D_{jl}$, where the sum is over the appropriate set of wavenumbers $k'$. This shows that $D$ is a circulant matrix since its entries depend only on the difference $j-l$. While this formula defines $D$, in practice, the operator is applied using the efficient three-step FFT-based procedure rather than forming the dense matrix $D$.\n\n### 2. Aliasing Errors from Nonlinear Products\n\nWhen a nonlinear term like $u^2$ is computed, new frequencies are generated. Consider a function $u(x)$ that is band-limited, meaning its Fourier series representation $u(x) = \\sum_{k=-K}^{K} \\hat{u}_k e^{ikx}$ contains a finite number of modes. The product $u^2(x)$ is then:\n$$\nu^2(x) = \\left(\\sum_{p=-K}^{K} \\hat{u}_p e^{ipx}\\right) \\left(\\sum_{q=-K}^{K} \\hat{u}_q e^{iqx}\\right) = \\sum_{p=-K}^{K} \\sum_{q=-K}^{K} \\hat{u}_p \\hat{u}_q e^{i(p+q)x}\n$$\nThe resulting function $u^2(x)$ has modes with wavenumbers up to $p+q = \\pm 2K$.\n\nOn a discrete grid of $N$ points, only wavenumbers up to the Nyquist frequency (roughly $N/2$) can be uniquely represented. Any frequency $|k'| > N/2$ is \"aliased\" and becomes indistinguishable from a lower frequency on the grid. This is because for any integer $m$:\n$$\ne^{i(k' + mN)x_j} = e^{i(k' + mN) \\frac{2\\pi j}{N}} = e^{ik' \\frac{2\\pi j}{N}} e^{i m N \\frac{2\\pi j}{N}} = e^{ik'x_j} e^{i 2\\pi mj} = e^{ik'x_j}\n$$\nTherefore, the mode $k'+mN$ appears identical to the mode $k'$ on the grid.\n\nWhen we compute $f_j = u(x_j)^2$ pointwise in physical space, we are implicitly creating these high frequencies (up to $2K$). If $2K > N/2$, these high frequencies are aliased to lower frequencies, corrupting the Fourier coefficients that should represent the true low-frequency content of $u^2(x)$. This corruption is the aliasing error. The naive pseudospectral method, which computes $u^2$ pointwise and then differentiates, is susceptible to this error.\n\n### 3. Pseudospectral Approximations and De-aliasing\n\nThe problem specifies two methods to approximate $(u^2)_x$.\n\n**1. Naive Pseudospectral Method:**\nThis method directly implements the physical space multiplication.\n- Compute the vector $\\mathbf{f}$ where $f_j = u(x_j)^2$. This step introduces aliasing errors if $u(x)$ is not sufficiently band-limited relative to the grid size $N$.\n- Apply the spectral differentiation operator $D$ to $\\mathbf{f}$. Computationally, this is done as $(u^2)_x \\approx \\mathcal{F}^{-1}(ik' \\cdot \\mathcal{F}(\\mathbf{f}))$.\nThe resulting approximation contains both the derivative of the true signal and the derivative of the aliasing errors.\n\n**2. The $2/3$-Rule De-aliased Method:**\nThis method attempts to mitigate aliasing. The name arises from the condition to prevent aliasing entirely: if the initial signal $u(x)$ is band-limited to $|k| < N/3$, then the product $u^2(x)$ will have modes up to $|k| < 2N/3$. Since $2N/3 \\le N$ for $N \\ge 0$, and more importantly the highest-frequency alias interactions are avoided, the convolution in Fourier space can be computed without wraparound error.\n\nThe procedure specified in the problem is a low-pass filtering approach based on this principle:\n- Compute $\\mathbf{f}$ with $f_j = u(x_j)^2$, just as in the naive method. The resulting grid function is aliased.\n- Transform to Fourier space to get the aliased coefficients $\\hat{\\mathbf{f}}$.\n- Assume that the true, unaliased signal is predominantly contained in the lower third of the wavenumbers. Zero out all Fourier coefficients $\\hat{f}_k$ for which $|k'| > \\lfloor N/3 \\rfloor$. This defines a new set of filtered coefficients, $\\hat{\\mathbf{f}}_{\\text{filt}}$.\n- Differentiate in Fourier space by computing $ik' \\hat{\\mathbf{f}}_{\\text{filt}}$.\n- Transform back to physical space via IDFT.\n\nThis filtering removes high-frequency content, which is assumed to be dominated by aliasing error. However, it cannot correct for aliasing errors that have already contaminated the lower-frequency modes that are preserved by the filter. It is most effective when the true signal is spectrally well-separated from the aliased components.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the spectral differentiation problem for three test cases, computing\n    the maximum-norm error for a naive pseudospectral method and a de-aliased\n    method using the 2/3-rule.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"id\": \"A\",\n            \"N\": 32,\n            \"u_func\": lambda x: np.sin(x) + 0.5 * np.sin(2 * x),\n            \"ux_func\": lambda x: np.cos(x) + np.cos(2 * x),\n        },\n        {\n            \"id\": \"B\",\n            \"N\": 15,\n            \"u_func\": lambda x: np.sin(5 * x) + 0.4 * np.cos(7 * x),\n            \"ux_func\": lambda x: 5 * np.cos(5 * x) - 2.8 * np.sin(7 * x),\n        },\n        {\n            \"id\": \"C\",\n            \"N\": 12,\n            \"u_func\": lambda x: np.sin(4 * x),\n            \"ux_func\": lambda x: 4 * np.cos(4 * x),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        u_func = case[\"u_func\"]\n        ux_func = case[\"ux_func\"]\n\n        # 1. Set up the grid and evaluate the exact functions\n        x_grid = 2 * np.pi * np.arange(N) / N\n        u_vec = u_func(x_grid)\n        ux_vec = ux_func(x_grid)\n\n        # The exact derivative of the nonlinear flux: (u^2)_x = 2*u*u_x\n        d_u2_exact = 2 * u_vec * ux_vec\n\n        # The nonlinear flux evaluated on the grid\n        f_vec = u_vec**2\n        \n        # 2. Naive Pseudospectral Method\n        \n        # Wavenumbers for differentiation. For real-valued functions, the\n        # derivative of the Nyquist mode (for N even) is set to 0.\n        k_wave = np.fft.fftfreq(N, d=1.0/N)\n        if N % 2 == 0:\n            k_wave[N // 2] = 0.0\n\n        # Differentiate in Fourier space and transform back\n        f_hat = np.fft.fft(f_vec)\n        d_u2_naive = np.real(np.fft.ifft(1j * k_wave * f_hat))\n\n        # Compute max-norm error\n        E_naive = np.max(np.abs(d_u2_naive - d_u2_exact))\n        results.append(E_naive)\n\n        # 3. 2/3-Rule De-aliased Method\n        \n        # Wavenumbers for filtering and differentiation\n        k_wave_dealias = np.fft.fftfreq(N, d=1.0/N)\n        \n        # Apply the 2/3-rule filter mask in Fourier space\n        K_cut = np.floor(N / 3)\n        mask = np.abs(k_wave_dealias) = K_cut\n        \n        f_hat_filtered = f_hat * mask\n\n        # Differentiate the filtered coefficients and transform back.\n        # No special Nyquist handling is needed for k_wave_dealias here because\n        # for N>3, floor(N/3)  N/2, so the filter mask already sets the\n        # Nyquist mode coefficient to zero.\n        d_u2_dealias = np.real(np.fft.ifft(1j * k_wave_dealias * f_hat_filtered))\n\n        # Compute max-norm error\n        E_2_3 = np.max(np.abs(d_u2_dealias - d_u2_exact))\n        results.append(E_2_3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n\n```", "id": "3277414"}]}