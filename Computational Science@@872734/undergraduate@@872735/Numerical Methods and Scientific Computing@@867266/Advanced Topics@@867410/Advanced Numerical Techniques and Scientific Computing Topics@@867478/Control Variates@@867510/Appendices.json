{"hands_on_practices": [{"introduction": "We begin with a classic textbook problem to build a solid foundation. By estimating a simple polynomial integral, you will perform a complete, from-scratch derivation of the optimal control variate coefficient and calculate the precise variance reduction factor, reinforcing the core mechanics of the method [@problem_id:3218918].", "problem": "Consider estimating the definite integral $I = \\int_{0}^{1} x^{2} \\, dx$ using the Monte Carlo (MC) method with $n$ independent and identically distributed samples $U_{1}, \\dots, U_{n}$ drawn from the $\\text{Uniform}[0,1]$ distribution. Define the plain MC estimator $\\hat{I}_{\\text{MC}} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$ where $X_{i} = U_{i}^{2}$. Now apply the control variate (CV) technique using the control variate $Y = U$ with known mean $\\mathbb{E}[Y] = \\frac{1}{2}$, and define the adjusted sample $Z_{c} = X - c\\,(Y - \\mathbb{E}[Y])$ and the corresponding estimator $\\hat{I}_{\\text{CV}}(c) = \\frac{1}{n} \\sum_{i=1}^{n} Z_{c,i}$. \n\nStarting from the definitions of expectation, variance, and covariance, and using fundamental calculus and algebra, derive the value $c^{\\star}$ that minimizes $\\operatorname{Var}(Z_{c})$, then compute the variance reduction factor\n$R = \\frac{\\operatorname{Var}\\!\\big(\\hat{I}_{\\text{CV}}(c^{\\star})\\big)}{\\operatorname{Var}\\!\\big(\\hat{I}_{\\text{MC}}\\big)}.$\nProvide the exact value of $R$ as a single rational number. No rounding is required; express the final answer as a rational number.", "solution": "The user wants to find the optimal control variate coefficient and the resulting variance reduction for a Monte Carlo integration problem.\n\n### Step 1: Extract Givens\n- The integral to estimate is $I = \\int_{0}^{1} x^{2} \\, dx$.\n- The sampling distribution is $U_i \\sim \\text{Uniform}[0,1]$ for $i = 1, \\dots, n$.\n- The plain Monte Carlo estimator is $\\hat{I}_{\\text{MC}} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$, with $X_i = U_i^2$.\n- The control variate is $Y = U$, with known mean $\\mathbb{E}[Y] = \\frac{1}{2}$.\n- The control variate random variable is $Z_c = X - c(Y - \\mathbb{E}[Y])$.\n- The control variate estimator is $\\hat{I}_{\\text{CV}}(c) = \\frac{1}{n} \\sum_{i=1}^{n} Z_{c,i}$.\n- The objective is to find the value $c^{\\star}$ that minimizes $\\operatorname{Var}(Z_c)$ and then compute the variance reduction factor $R = \\frac{\\operatorname{Var}\\!\\big(\\hat{I}_{\\text{CV}}(c^{\\star})\\big)}{\\operatorname{Var}\\!\\big(\\hat{I}_{\\text{MC}}\\big)}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined, standard exercise in the field of numerical methods, specifically Monte Carlo techniques.\n- **Scientifically Grounded:** The problem is based on established principles of probability theory, statistics, and calculus. All concepts (Monte Carlo integration, control variates, variance, covariance) are standard.\n- **Well-Posed:** The problem provides all necessary information to derive a unique solution for $c^{\\star}$ and $R$. The objective is clear and unambiguous.\n- **Objective:** The problem is stated in precise mathematical language, free from subjective or qualitative claims.\n\nThe problem does not violate any of the invalidity criteria. It is a complete, consistent, and solvable problem within the scope of scientific computing.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation\n\nThe goal is to find the optimal coefficient $c^{\\star}$ that minimizes the variance of the control variate estimator, and then to compute the variance reduction factor.\n\nThe random variable for the control variate estimator is $Z_c = X - c(Y - \\mathbb{E}[Y])$. Its variance is a function of $c$. Since $\\mathbb{E}[Y]$ is a constant, it does not affect the variance.\n$$\n\\operatorname{Var}(Z_c) = \\operatorname{Var}(X - cY)\n$$\nUsing the properties of variance, we can expand this expression:\n$$\n\\operatorname{Var}(Z_c) = \\operatorname{Var}(X) + \\operatorname{Var}(cY) - 2\\operatorname{Cov}(X, cY)\n$$\n$$\n\\operatorname{Var}(Z_c) = \\operatorname{Var}(X) + c^2\\operatorname{Var}(Y) - 2c\\operatorname{Cov}(X, Y)\n$$\nThis expression is a quadratic function of $c$. To find the value $c^{\\star}$ that minimizes this variance, we take the derivative with respect to $c$ and set it to $0$:\n$$\n\\frac{d}{dc}\\operatorname{Var}(Z_c) = 2c\\operatorname{Var}(Y) - 2\\operatorname{Cov}(X, Y)\n$$\nSetting the derivative to zero yields:\n$$\n2c^{\\star}\\operatorname{Var}(Y) - 2\\operatorname{Cov}(X, Y) = 0\n$$\n$$\nc^{\\star} = \\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(Y)}\n$$\nTo compute $c^{\\star}$, we must calculate $\\operatorname{Var}(Y)$ and $\\operatorname{Cov}(X, Y)$. This requires computing moments of the random variables $X = U^2$ and $Y = U$, where $U \\sim \\text{Uniform}[0,1]$.\n\nThe $k$-th moment of a random variable $U \\sim \\text{Uniform}[0,1]$ is given by:\n$$\n\\mathbb{E}[U^k] = \\int_{0}^{1} u^k \\cdot 1 \\, du = \\left[ \\frac{u^{k+1}}{k+1} \\right]_{0}^{1} = \\frac{1}{k+1}\n$$\nFirst, let's compute the moments and variance of $Y=U$.\n$$\n\\mathbb{E}[Y] = \\mathbb{E}[U] = \\frac{1}{1+1} = \\frac{1}{2}\n$$\n$$\n\\mathbb{E}[Y^2] = \\mathbb{E}[U^2] = \\frac{1}{2+1} = \\frac{1}{3}\n$$\nThe variance of $Y$ is:\n$$\n\\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = \\frac{1}{3} - \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{3} - \\frac{1}{4} = \\frac{4-3}{12} = \\frac{1}{12}\n$$\nNext, we compute the moments and variance of $X=U^2$.\n$$\n\\mathbb{E}[X] = \\mathbb{E}[U^2] = \\frac{1}{2+1} = \\frac{1}{3}\n$$\nThis is the true value of the integral $I = \\int_0^1 x^2 dx$.\n$$\n\\mathbb{E}[X^2] = \\mathbb{E}[(U^2)^2] = \\mathbb{E}[U^4] = \\frac{1}{4+1} = \\frac{1}{5}\n$$\nThe variance of $X$ is:\n$$\n\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\frac{1}{5} - \\left(\\frac{1}{3}\\right)^2 = \\frac{1}{5} - \\frac{1}{9} = \\frac{9-5}{45} = \\frac{4}{45}\n$$\nNow, we compute the covariance between $X$ and $Y$.\n$$\n\\operatorname{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\n$$\nThe term $\\mathbb{E}[XY]$ is:\n$$\n\\mathbb{E}[XY] = \\mathbb{E}[U^2 \\cdot U] = \\mathbb{E}[U^3] = \\frac{1}{3+1} = \\frac{1}{4}\n$$\nSo, the covariance is:\n$$\n\\operatorname{Cov}(X, Y) = \\frac{1}{4} - \\left(\\frac{1}{3}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} - \\frac{1}{6} = \\frac{3-2}{12} = \\frac{1}{12}\n$$\nWe can now calculate the optimal coefficient $c^{\\star}$:\n$$\nc^{\\star} = \\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(Y)} = \\frac{1/12}{1/12} = 1\n$$\nThe next step is to compute the variance reduction factor $R$.\nThe estimators $\\hat{I}_{\\text{MC}}$ and $\\hat{I}_{\\text{CV}}(c)$ are averages of $n$ i.i.d. samples. Therefore, their variances are related to the variances of the individual samples as follows:\n$$\n\\operatorname{Var}(\\hat{I}_{\\text{MC}}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\operatorname{Var}(X_i) = \\frac{n \\operatorname{Var}(X)}{n^2} = \\frac{\\operatorname{Var}(X)}{n}\n$$\n$$\n\\operatorname{Var}(\\hat{I}_{\\text{CV}}(c^{\\star})) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n Z_{c^{\\star},i}\\right) = \\frac{\\operatorname{Var}(Z_{c^{\\star}})}{n}\n$$\nThe variance reduction factor $R$ is the ratio of these two variances:\n$$\nR = \\frac{\\operatorname{Var}(\\hat{I}_{\\text{CV}}(c^{\\star}))}{\\operatorname{Var}(\\hat{I}_{\\text{MC}})} = \\frac{\\operatorname{Var}(Z_{c^{\\star}})/n}{\\operatorname{Var}(X)/n} = \\frac{\\operatorname{Var}(Z_{c^{\\star}})}{\\operatorname{Var}(X)}\n$$\nWe need to find the minimized variance $\\operatorname{Var}(Z_{c^{\\star}})$. We substitute $c^{\\star}$ back into the expression for $\\operatorname{Var}(Z_c)$:\n$$\n\\operatorname{Var}(Z_{c^{\\star}}) = \\operatorname{Var}(X) + (c^{\\star})^2\\operatorname{Var}(Y) - 2c^{\\star}\\operatorname{Cov}(X, Y)\n$$\nSubstituting $c^{\\star} = \\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(Y)}$:\n$$\n\\operatorname{Var}(Z_{c^{\\star}}) = \\operatorname{Var}(X) + \\left(\\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(Y)}\\right)^2 \\operatorname{Var}(Y) - 2\\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(Y)}\\operatorname{Cov}(X, Y)\n$$\n$$\n\\operatorname{Var}(Z_{c^{\\star}}) = \\operatorname{Var}(X) + \\frac{\\operatorname{Cov}(X, Y)^2}{\\operatorname{Var}(Y)} - 2\\frac{\\operatorname{Cov}(X, Y)^2}{\\operatorname{Var}(Y)} = \\operatorname{Var}(X) - \\frac{\\operatorname{Cov}(X, Y)^2}{\\operatorname{Var}(Y)}\n$$\nNow, we substitute the numerical values we calculated:\n$$\n\\operatorname{Var}(Z_{c^{\\star}}) = \\frac{4}{45} - \\frac{(1/12)^2}{1/12} = \\frac{4}{45} - \\frac{1}{12}\n$$\nTo subtract the fractions, we find a common denominator, which is $180$:\n$$\n\\operatorname{Var}(Z_{c^{\\star}}) = \\frac{4 \\cdot 4}{180} - \\frac{1 \\cdot 15}{180} = \\frac{16 - 15}{180} = \\frac{1}{180}\n$$\nFinally, we compute the variance reduction factor $R$:\n$$\nR = \\frac{\\operatorname{Var}(Z_{c^{\\star}})}{\\operatorname{Var}(X)} = \\frac{1/180}{4/45} = \\frac{1}{180} \\cdot \\frac{45}{4} = \\frac{45}{720}\n$$\nSimplifying the fraction:\n$$\nR = \\frac{1 \\cdot 45}{16 \\cdot 45} = \\frac{1}{16}\n$$\nThe variance has been reduced by a factor of $16$.", "answer": "$$\n\\boxed{\\frac{1}{16}}\n$$", "id": "3218918"}, {"introduction": "The effectiveness of a control variate is often linked to its linear correlation with the target function, but this is not the whole story. This insightful exercise guides you to construct a scenario where the linear correlation is zero, yet a well-chosen non-linear control variate yields a dramatic variance reduction [@problem_id:3218904]. This practice sharpens your intuition for identifying deeper structural relationships between functions to devise powerful control variates.", "problem": "Consider a Monte Carlo estimation problem for the mean $\\mu = \\mathbb{E}[X]$ where you have access to samples of two random variables $X$ and $Y$. You will construct a joint distribution of $(X,Y)$ for which the Pearson correlation coefficient $\\rho(X,Y)$ is zero, yet a non-linear control variate $Z = g(Y)$ leads to a very large variance reduction. Use only foundational definitions of expectation, variance, covariance, and properties of the normal distribution.\n\nLet $Y \\sim \\mathcal{N}(0,1)$ and $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$, independent of $Y$, with $\\sigma^{2} = 0.01$. Define\n$X = (Y^{2} - 1) + \\varepsilon$\nand consider the non-linear control variate\n$Z = g(Y) = Y^{2} - 1,$\nwhose mean $\\mathbb{E}[Z]$ is known from the distribution of $Y$.\n\nTasks:\n- Using the core definitions of covariance and properties of moments of the standard normal distribution, verify that the covariance $\\operatorname{Cov}(X,Y)$ equals $0$, and hence the correlation $\\rho(X,Y)$ equals $0$.\n- Starting from the definition of variance and covariance and without assuming any particular formula, derive the value of the coefficient $\\beta$ that minimizes the variance of the adjusted estimator for $\\mu$ given by $X - \\beta(Z - \\mathbb{E}[Z])$.\n- Compute the per-sample variance of the adjusted estimator at the optimal choice of $\\beta$, and the variance reduction factor\n$$\nR = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}\\big(X - \\beta^{\\star}(Z - \\mathbb{E}[Z])\\big)}.\n$$\n\nProvide your final answer as the ordered pair $(\\beta^{\\star}, R)$ in exact form. No rounding is required. The final answer must be a calculation as specified.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It constitutes a formal exercise in probability theory and numerical methods. All necessary information is provided, and the tasks are well-defined. We may proceed with the solution.\n\nThe problem asks for three tasks related to a Monte Carlo estimation scenario. We are given the random variables $Y \\sim \\mathcal{N}(0,1)$ and $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ with $\\sigma^{2} = 0.01$. The random variables $Y$ and $\\varepsilon$ are independent. The variable of interest is $X = (Y^{2} - 1) + \\varepsilon$, and the control variate is $Z = g(Y) = Y^{2} - 1$.\n\nFirst, we establish the necessary moments of the standard normal variable $Y$.\nThe probability density function of $Y$ is symmetric about $0$. Therefore, all odd moments of $Y$ are zero.\n$\\mathbb{E}[Y] = 0$\n$\\mathbb{E}[Y^3] = 0$\nThe even moments are well-known. The second moment is the variance, since the mean is zero:\n$\\mathbb{E}[Y^2] = \\operatorname{Var}(Y) = 1$\nThe fourth moment of a standard normal distribution is:\n$\\mathbb{E}[Y^4] = 3$\n\nWith these, we can determine the properties of $X$ and $Z$.\nThe mean of $Z$ is:\n$\\mathbb{E}[Z] = \\mathbb{E}[Y^2 - 1] = \\mathbb{E}[Y^2] - 1 = 1 - 1 = 0$.\nThe mean of $X$, which is the quantity $\\mu$ to be estimated, is:\n$\\mu = \\mathbb{E}[X] = \\mathbb{E}[(Y^2 - 1) + \\varepsilon] = \\mathbb{E}[Y^2 - 1] + \\mathbb{E}[\\varepsilon] = \\mathbb{E}[Z] + 0 = 0$.\n\n**Task 1: Verify $\\operatorname{Cov}(X,Y) = 0$**\n\nThe covariance is defined as $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$.\nWe have $\\mathbb{E}[Y] = 0$, so the second term vanishes:\n$\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X] \\cdot 0 = \\mathbb{E}[XY]$.\nWe compute $\\mathbb{E}[XY]$ by substituting the definition of $X$:\n$\\mathbb{E}[XY] = \\mathbb{E}[((Y^2 - 1) + \\varepsilon)Y] = \\mathbb{E}[Y^3 - Y + \\varepsilon Y]$.\nBy linearity of expectation:\n$\\mathbb{E}[XY] = \\mathbb{E}[Y^3] - \\mathbb{E}[Y] + \\mathbb{E}[\\varepsilon Y]$.\nAs established, $\\mathbb{E}[Y^3] = 0$ and $\\mathbb{E}[Y] = 0$. For the last term, since $\\varepsilon$ and $Y$ are independent, $\\mathbb{E}[\\varepsilon Y] = \\mathbb{E}[\\varepsilon]\\mathbb{E}[Y]$. We know $\\mathbb{E}[\\varepsilon]=0$ and $\\mathbb{E}[Y]=0$, so $\\mathbb{E}[\\varepsilon Y] = 0 \\cdot 0 = 0$.\nTherefore, $\\mathbb{E}[XY] = 0 - 0 + 0 = 0$.\nThis confirms that $\\operatorname{Cov}(X,Y) = 0$.\nThe Pearson correlation coefficient is $\\rho(X,Y) = \\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}$. Since the numerator is $0$ and the variances are non-zero (as will be shown), $\\rho(X,Y) = 0$.\n\n**Task 2: Derive the optimal coefficient $\\beta^{\\star}$**\n\nWe are tasked to find the value of $\\beta$, denoted $\\beta^{\\star}$, that minimizes the variance of the adjusted estimator $X_{\\beta} = X - \\beta(Z - \\mathbb{E}[Z])$. Let $V(\\beta) = \\operatorname{Var}(X_{\\beta})$.\nSince $\\mathbb{E}[Z] = 0$, the estimator is $X_{\\beta} = X - \\beta Z$.\nThe variance to be minimized is:\n$V(\\beta) = \\operatorname{Var}(X - \\beta Z)$.\nUsing the properties of variance, where $\\beta$ is a constant:\n$V(\\beta) = \\operatorname{Var}(X) + \\operatorname{Var}(-\\beta Z) + 2\\operatorname{Cov}(X, -\\beta Z)$.\n$V(\\beta) = \\operatorname{Var}(X) + \\beta^{2}\\operatorname{Var}(Z) - 2\\beta\\operatorname{Cov}(X, Z)$.\nThis is a quadratic function of $\\beta$. To find the minimum, we compute the derivative with respect to $\\beta$ and set it to zero:\n$\\frac{d V}{d\\beta} = \\frac{d}{d\\beta} \\left( \\operatorname{Var}(X) + \\beta^{2}\\operatorname{Var(Z)} - 2\\beta\\operatorname{Cov}(X, Z) \\right) = 2\\beta\\operatorname{Var}(Z) - 2\\operatorname{Cov}(X, Z)$.\nSetting the derivative to zero:\n$2\\beta\\operatorname{Var}(Z) - 2\\operatorname{Cov}(X, Z) = 0$.\nSolving for the optimal coefficient $\\beta^{\\star}$:\n$\\beta^{\\star} = \\frac{\\operatorname{Cov}(X, Z)}{\\operatorname{Var}(Z)}$.\nThe second derivative $\\frac{d^2 V}{d\\beta^2} = 2\\operatorname{Var}(Z)$ is positive since variance is non-negative (and non-zero in this case), confirming that this is a minimum.\n\n**Task 3: Compute $\\beta^{\\star}$ and the variance reduction factor $R$**\n\nTo compute $\\beta^{\\star}$, we need to calculate $\\operatorname{Var}(Z)$ and $\\operatorname{Cov}(X,Z)$.\nFirst, we find the variance of the control variate $Z$:\n$\\operatorname{Var}(Z) = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2$. Since $\\mathbb{E}[Z]=0$, $\\operatorname{Var}(Z) = \\mathbb{E}[Z^2]$.\n$\\operatorname{Var}(Z) = \\mathbb{E}[(Y^2-1)^2] = \\mathbb{E}[Y^4 - 2Y^2 + 1]$.\nUsing linearity of expectation and the moments of $Y$:\n$\\operatorname{Var}(Z) = \\mathbb{E}[Y^4] - 2\\mathbb{E}[Y^2] + 1 = 3 - 2(1) + 1 = 2$.\n\nNext, we calculate the covariance between $X$ and $Z$:\n$\\operatorname{Cov}(X,Z) = \\mathbb{E}[XZ] - \\mathbb{E}[X]\\mathbb{E}[Z]$.\nSince $\\mathbb{E}[X] = 0$ and $\\mathbb{E}[Z] = 0$, we have $\\operatorname{Cov}(X,Z) = \\mathbb{E}[XZ]$.\nSubstitute $X = Z + \\varepsilon$:\n$\\operatorname{Cov}(X,Z) = \\mathbb{E}[(Z+\\varepsilon)Z] = \\mathbb{E}[Z^2 + \\varepsilon Z] = \\mathbb{E}[Z^2] + \\mathbb{E}[\\varepsilon Z]$.\nWe know $\\mathbb{E}[Z^2] = \\operatorname{Var}(Z) = 2$.\nFor the term $\\mathbb{E}[\\varepsilon Z]$, we use the independence of $\\varepsilon$ and $Y$. Since $Z = Y^2-1$ is a function of $Y$, $Z$ and $\\varepsilon$ are also independent.\nTherefore, $\\mathbb{E}[\\varepsilon Z] = \\mathbb{E}[\\varepsilon]\\mathbb{E}[Z] = 0 \\cdot 0 = 0$.\nSo, $\\operatorname{Cov}(X,Z) = 2 + 0 = 2$.\n\nNow we can compute the optimal coefficient $\\beta^{\\star}$:\n$\\beta^{\\star} = \\frac{\\operatorname{Cov}(X, Z)}{\\operatorname{Var}(Z)} = \\frac{2}{2} = 1$.\n\nFinally, we compute the variance reduction factor $R = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}\\big(X - \\beta^{\\star}(Z - \\mathbb{E}[Z])\\big)}$.\nFirst, the numerator $\\operatorname{Var}(X)$:\n$X = Z + \\varepsilon$. Since $Z$ and $\\varepsilon$ are independent, the variance of their sum is the sum of their variances:\n$\\operatorname{Var}(X) = \\operatorname{Var}(Z) + \\operatorname{Var}(\\varepsilon) = 2 + \\sigma^2 = 2 + 0.01 = 2.01$.\n\nNext, the denominator, which is the variance of the adjusted estimator with the optimal $\\beta^{\\star}=1$ and $\\mathbb{E}[Z]=0$:\n$\\operatorname{Var}\\big(X - \\beta^{\\star}(Z - \\mathbb{E}[Z])\\big) = \\operatorname{Var}(X - 1 \\cdot (Z - 0)) = \\operatorname{Var}(X-Z)$.\nSubstitute $X = Z + \\varepsilon$:\n$\\operatorname{Var}(X-Z) = \\operatorname{Var}((Z+\\varepsilon) - Z) = \\operatorname{Var}(\\varepsilon)$.\nWe are given $\\operatorname{Var}(\\varepsilon) = \\sigma^2 = 0.01$.\n\nNow, we can compute the variance reduction factor $R$:\n$R = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}(X-Z)} = \\frac{2.01}{0.01} = 201$.\n\nThe final answer is the ordered pair $(\\beta^{\\star}, R)$.\n$\\beta^{\\star} = 1$\n$R = 201$\nThe final pair is $(1, 201)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  201 \\end{pmatrix}}\n$$", "id": "3218904"}, {"introduction": "In most practical scenarios, the optimal coefficient $\\beta^{\\star}$ is unknown and must be estimated from the available data. This hands-on coding challenge bridges the gap between theory and application by having you implement a control variate estimator with an empirically determined coefficient [@problem_id:3218755]. You will evaluate its performance on a non-trivial integral and compare it against both the basic Monte Carlo method and a hybrid approach involving numerical quadrature.", "problem": "You are to implement and analyze Monte Carlo (MC) integration with control variates for the integral $$I=\\int_{0}^{1} e^{-x^2}\\,dx,$$ using two different control variates and a hybrid comparison with Gauss–Legendre (GL) quadrature on the residual. The target is to demonstrate principled variance reduction by choosing the control variate coefficient empirically from sampled data, and to compare the outcome with deterministic quadrature applied to the residual function.\n\nUse the following fundamental base:\n- The expectation of a function under a probability density integrates that function over its domain: if $X \\sim \\mathrm{Uniform}(0,1)$, then $$\\mathbb{E}[f(X)] = \\int_{0}^{1} f(x)\\,dx.$$\n- For any integrable function $g$ with known mean $\\mu_g = \\int_{0}^{1} g(x)\\,dx$, the control variate adjusted random variable $$Z_\\beta = f(X) - \\beta \\big(g(X) - \\mu_g\\big)$$ satisfies $$\\mathbb{E}[Z_\\beta] = \\mathbb{E}[f(X)],$$ for any scalar $\\beta \\in \\mathbb{R}$, because $\\mathbb{E}[g(X)] = \\mu_g$.\n- Variance is minimized by an appropriate choice of $\\beta$ obtained by minimizing the sample variance of the adjusted values $Z_\\beta$ over $\\beta \\in \\mathbb{R}$. You must do this strictly from sampled data with no closed-form formula provided in the problem statement. If the empirical variance of the control variate is numerically zero, set $\\beta=0$ to avoid instability.\n\nYour task is to:\n1. Implement MC estimators for the integral $I$ using $X_i \\sim \\mathrm{Uniform}(0,1)$, independent and identically distributed, for $i=1,\\dots,N$, where $N$ is the number of samples. The function is $f(x)=e^{-x^2}$.\n2. Use the control variates $g_1(x)=1-x$ and $g_2(x)=x(1-x)$, whose exact means over $[0,1]$ are $\\mu_{g_1}=\\int_{0}^{1} (1-x)\\,dx = \\tfrac{1}{2}$ and $\\mu_{g_2}=\\int_{0}^{1} x(1-x)\\,dx = \\tfrac{1}{6}$.\n3. Empirically select $\\beta$ from random samples by minimizing the sample variance of the adjusted values $Z_\\beta$ over $\\beta \\in \\mathbb{R}$. You must do this strictly from sampled data with no closed-form formula provided in the problem statement. If the empirical variance of the control variate is numerically zero, set $\\beta=0$ to avoid instability.\n4. Construct the following three estimators and compute their absolute errors with respect to the exact value of $I$:\n   - Plain MC: $$\\hat{I}_{\\mathrm{MC}} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i).$$\n   - MC with control variates (empirical $\\beta$): $$\\hat{I}_{\\mathrm{CV}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[f(X_i) - \\beta \\big(g(X_i) - \\mu_g\\big)\\right].$$\n   - GL quadrature on the residual using the empirically selected $\\beta$: define the residual function $$r_\\beta(x) = f(x) - \\beta \\big(g(x) - \\mu_g\\big),$$ then approximate $$\\hat{I}_{\\mathrm{GL,res}} \\approx \\int_{0}^{1} r_\\beta(x)\\,dx$$ using $m$-point Gauss–Legendre quadrature on $[0,1]$. Use nodes and weights from standard $m$-point Gauss–Legendre quadrature on $[-1,1]$ mapped to $[0,1]$ via $t=\\tfrac{1}{2}(u+1)$ with weights scaled by $\\tfrac{1}{2}$. Note that if $g$ is a polynomial of low degree, and $m$ is sufficiently large, GL quadrature integrates $g$ exactly, and thus applying GL to $r_\\beta$ can coincide with applying GL directly to $f$; nevertheless, perform the residual quadrature as specified.\n\nThe exact value of $I$ can be expressed using the Gaussian error function as $$I = \\frac{\\sqrt{\\pi}}{2}\\,\\mathrm{erf}(1).$$ Use this to compute absolute errors.\n\nTest Suite:\nProvide a program that evaluates the three absolute errors for each of the following parameter sets:\n- Case $1$ (happy path, quadratic control variate): $N=100000$, seed $=2024$, control variate $g_2$, $m=16$.\n- Case $2$ (happy path, linear control variate): $N=100000$, seed $=99$, control variate $g_1$, $m=16$.\n- Case $3$ (small $N$ boundary, coarse quadrature not exact for quadratic): $N=10$, seed $=7$, control variate $g_2$, $m=1$.\n- Case $4$ (extreme $N=1$ edge, linear control variate, coarse quadrature): $N=1$, seed $=42$, control variate $g_1$, $m=1$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the absolute errors in the order of cases $1$ through $4$, and within each case, in the order plain MC, MC with control variates, GL residual. That is, the output format is $$[\\mathrm{err}_{1,\\mathrm{MC}},\\mathrm{err}_{1,\\mathrm{CV}},\\mathrm{err}_{1,\\mathrm{GLres}},\\mathrm{err}_{2,\\mathrm{MC}},\\mathrm{err}_{2,\\mathrm{CV}},\\mathrm{err}_{2,\\mathrm{GLres}},\\mathrm{err}_{3,\\mathrm{MC}},\\mathrm{err}_{3,\\mathrm{CV}},\\mathrm{err}_{3,\\mathrm{GLres}},\\mathrm{err}_{4,\\mathrm{MC}},\\mathrm{err}_{4,\\mathrm{CV}},\\mathrm{err}_{4,\\mathrm{GLres}}].$$", "solution": "The problem requires the implementation and analysis of three numerical methods for approximating the definite integral $I=\\int_{0}^{1} e^{-x^2}\\,dx$. The methods are: standard Monte Carlo (MC) integration, MC integration with a control variate (CV), and Gauss-Legendre (GL) quadrature applied to a residual function derived from the control variate method. The analysis will compare the absolute errors of these three estimators against the known exact value of the integral for a set of specified test cases.\n\nThe function to be integrated is $f(x) = e^{-x^2}$ over the domain $[0, 1]$. The basis for the stochastic methods is the generation of $N$ independent and identically distributed random samples $X_i \\sim \\mathrm{Uniform}(0,1)$. The integral $I$ is then equivalent to the expectation $\\mathbb{E}[f(X)]$.\n\nThe exact value of the integral is required for error computation and is given by $I = \\frac{\\sqrt{\\pi}}{2}\\,\\mathrm{erf}(1)$, where $\\mathrm{erf}$ is the Gaussian error function.\n\nThe problem specifies two potential control variates:\n1.  A linear function $g_1(x) = 1-x$, with known mean over $[0,1]$:\n    $$\\mu_{g_1} = \\int_{0}^{1} (1-x)\\,dx = \\left[x - \\frac{x^2}{2}\\right]_{0}^{1} = 1 - \\frac{1}{2} = \\frac{1}{2}.$$\n2.  A quadratic function $g_2(x) = x(1-x)$, with known mean over $[0,1]$:\n    $$\\mu_{g_2} = \\int_{0}^{1} (x-x^2)\\,dx = \\left[\\frac{x^2}{2} - \\frac{x^3}{3}\\right]_{0}^{1} = \\frac{1}{2} - \\frac{1}{3} = \\frac{1}{6}.$$\n\nThe process for determining each of the three estimators is as follows:\n\n1.  **Plain Monte Carlo (MC) Estimator**\n    The standard MC estimator, $\\hat{I}_{\\mathrm{MC}}$, is the sample mean of the function $f(x)$ evaluated at the random points $X_i$:\n    $$\\hat{I}_{\\mathrm{MC}} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i).$$\n    By the Law of Large Numbers, $\\hat{I}_{\\mathrm{MC}} \\to I$ as $N \\to \\infty$.\n\n2.  **Control Variate (CV) Estimator**\n    This method aims to reduce the variance of the MC estimator. It introduces an adjusted random variable $Z_\\beta = f(X) - \\beta(g(X) - \\mu_g)$, where $g(x)$ is a control variate with a known mean $\\mu_g$. The expectation of $Z_\\beta$ is $\\mathbb{E}[f(X)] = I$, for any scalar coefficient $\\beta \\in \\mathbb{R}$. The estimator is the mean of the samples of this new variable:\n    $$\\hat{I}_{\\mathrm{CV}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[f(X_i) - \\beta \\big(g(X_i) - \\mu_g\\big)\\right].$$\n    The optimal coefficient, $\\beta^*$, minimizes the variance $\\mathrm{Var}(Z_\\beta)$. This variance is a quadratic function of $\\beta$:\n    $$\\mathrm{Var}(Z_\\beta) = \\mathrm{Var}(f(X) - \\beta g(X)) = \\mathrm{Var}(f(X)) - 2\\beta \\mathrm{Cov}(f(X), g(X)) + \\beta^2 \\mathrm{Var}(g(X)).$$\n    Minimizing with respect to $\\beta$ yields the optimal value:\n    $$\\beta^* = \\frac{\\mathrm{Cov}(f(X), g(X))}{\\mathrm{Var}(g(X))}.$$\n    The problem mandates that $\\beta$ be estimated empirically from the generated samples. Let $f_i = f(X_i)$ and $g_i = g(X_i)$. The sample-based estimate $\\hat{\\beta}$ is calculated as:\n    $$\\hat{\\beta} = \\frac{\\widehat{\\mathrm{Cov}}(f, g)}{\\widehat{\\mathrm{Var}}(g)} = \\frac{\\sum_{i=1}^{N} (f_i - \\bar{f})(g_i - \\bar{g})}{\\sum_{i=1}^{N} (g_i - \\bar{g})^2},$$\n    where $\\bar{f}$ and $\\bar{g}$ are the respective sample means. If the denominator, which is proportional to the sample variance of $g$, is numerically zero (which occurs if $N \\le 1$ or if all $g_i$ are coincidentally equal), $\\hat{\\beta}$ is set to $0$ as per the problem instructions to prevent numerical instability. The CV estimator can then be computed efficiently as:\n    $$\\hat{I}_{\\mathrm{CV}} = \\bar{f} - \\hat{\\beta}(\\bar{g} - \\mu_g).$$\n\n3.  **Gauss-Legendre Quadrature on the Residual (GL,res) Estimator**\n    This method combines the control variate concept with deterministic quadrature. First, the empirical coefficient $\\hat{\\beta}$ is determined from the $N$ MC samples as described above. Then, a residual function $r_{\\hat{\\beta}}(x)$ is defined:\n    $$r_{\\hat{\\beta}}(x) = f(x) - \\hat{\\beta}(g(x) - \\mu_g).$$\n    The integral of this residual function is then approximated using $m$-point Gauss-Legendre quadrature. The standard GL quadrature rule is defined for the interval $[-1, 1]$:\n    $$\\int_{-1}^{1} h(u)\\,du \\approx \\sum_{j=1}^{m} w_j h(u_j),$$\n    where $u_j$ are the nodes (roots of the $m$-th degree Legendre polynomial) and $w_j$ are the corresponding weights. To apply this to the interval $[0, 1]$, a linear change of variables $x = \\frac{1}{2}(u+1)$ is used, with $dx = \\frac{1}{2}du$. The quadrature rule becomes:\n    $$\\int_{0}^{1} r_{\\hat{\\beta}}(x)\\,dx \\approx \\sum_{j=1}^{m} \\frac{w_j}{2} r_{\\hat{\\beta}}\\left(\\frac{u_j+1}{2}\\right).$$\n    This sum provides the estimator $\\hat{I}_{\\mathrm{GL,res}}$. The principle here is that if $g(x)$ is a good approximation to $f(x)$, the residual $r_{\\hat{\\beta}}(x)$ will be a \"smoother\" or smaller-magnitude function than $f(x)$ itself, potentially leading to a more accurate result from a fixed-order quadrature rule.\n\nFor each of the four test cases, these three estimators are computed. Finally, their absolute errors are calculated with respect to the exact value $I = \\frac{\\sqrt{\\pi}}{2}\\,\\mathrm{erf}(1)$.", "answer": "[0.0003923485770453738,9.754807490076292e-06,1.4057850064295318e-15,0.0006734346850697968,0.0003290070621255474,2.378942207903827e-14,0.019566270457639535,0.002824905872199042,0.03859746305682226,0.02497677864409395,0.02497677864409395,0.03859746305682226]", "id": "3218755"}]}