{"hands_on_practices": [{"introduction": "Iterative solvers are the workhorses of large-scale scientific computing, but their performance is not one-size-fits-all. The efficiency of an algorithm like the Conjugate Gradient method is deeply connected to the mathematical properties of the linear system it is solving. This practice provides a concrete demonstration of this link by having you systematically construct matrices with a specific condition number, $\\kappa_2(A)$, and measure the resulting impact on the solver's iteration count [@problem_id:3209858]. You will observe firsthand how an increasing condition number, which signifies a more ill-conditioned problem, directly degrades the convergence rate of this fundamental algorithm.", "problem": "You will study how the iteration count of an iterative solver degrades as the condition number of the system matrix increases. Work entirely in exact, dimensionless, purely mathematical terms.\n\nStart from the following fundamental base:\n- For a real, symmetric positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$, the spectral condition number in the spectral norm is $\\kappa_2(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$, where $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ denote the largest and smallest eigenvalues of $A$, respectively.\n- The Conjugate Gradient method (abbreviated as CG) for solving $A x = b$ with $A$ symmetric positive definite iteratively minimizes the error in the $A$-norm over a Krylov subspace and terminates when the residual $r_k = b - A x_k$ satisfies a specified stopping criterion.\n\nTask. Implement a complete, runnable program that:\n1. Constructs, for each specified target condition number $\\kappa$, a family member $A(\\kappa)$ of real, symmetric positive definite matrices of size $n \\times n$ with $\\kappa_2(A(\\kappa)) = \\kappa$, by prescribing the eigenvalues and mixing with a fixed orthogonal matrix:\n   - Use $n = 64$.\n   - Let the eigenvalues be $\\lambda_i$ linearly spaced between $1$ and $\\kappa$ for $i = 1, \\dots, n$.\n   - Form $A(\\kappa) = Q^\\top \\operatorname{diag}(\\lambda_1,\\dots,\\lambda_n) Q$, where $Q$ is a fixed orthogonal matrix, generated reproducibly as follows: construct a dense matrix $M \\in \\mathbb{R}^{n \\times n}$ with independent, identically distributed standard normal entries using a fixed seed $0$; compute a reduced $Q R$ factorization $M = \\widehat{Q} R$; define $D = \\operatorname{diag}(\\operatorname{sign}(R_{11}), \\dots, \\operatorname{sign}(R_{nn}))$ with the convention $\\operatorname{sign}(0)=1$, and set $Q = \\widehat{Q} D$. This guarantees $R$ with positive diagonal and fixes the column signs of $Q$ deterministically.\n2. Solves $A(\\kappa) x = b$ using the Conjugate Gradient method with:\n   - Right-hand side $b = \\mathbf{1} \\in \\mathbb{R}^n$ (all ones).\n   - Initial guess $x_0 = 0$.\n   - Stopping criterion $\\lVert r_k \\rVert_2 \\le \\text{tol} \\cdot \\lVert b \\rVert_2$ with $\\text{tol} = 10^{-8}$, where $\\lVert \\cdot \\rVert_2$ is the Euclidean norm.\n   - Maximum number of iterations equal to $n$.\n3. Reports, for each test case, the number of iterations $k$ taken by the Conjugate Gradient method to satisfy the stopping criterion, or the maximum $n$ if the stopping criterion is not met within $n$ iterations.\n\nTest suite. Run the program for the following parameter values:\n- Case $1$: $(n, \\kappa, \\text{tol}) = (64, 1, 10^{-8})$.\n- Case $2$: $(n, \\kappa, \\text{tol}) = (64, 10, 10^{-8})$.\n- Case $3$: $(n, \\kappa, \\text{tol}) = (64, 100, 10^{-8})$.\n- Case $4$: $(n, \\kappa, \\text{tol}) = (64, 1000, 10^{-8})$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the same order as the test suite, for example, $[k_1,k_2,k_3,k_4]$, where each $k_i$ is an integer iteration count for Case $i$.\n\nYour implementation must not read any input and must not use any external files or network access. All computations must use double-precision arithmetic. No physical units are involved, and no angles are used. The reported quantities are integers.", "solution": "The problem is valid. It presents a well-posed and scientifically sound numerical experiment to investigate the effect of a matrix's condition number on the convergence of the Conjugate Gradient method. All parameters and procedures are specified clearly and unambiguously.\n\nThe objective is to quantify the relationship between the spectral condition number, $\\kappa_2(A)$, of a real, symmetric positive definite (SPD) matrix $A \\in \\mathbb{R}^{n \\times n}$ and the number of iterations required by the Conjugate Gradient (CG) method to solve the linear system $A x = b$. The experiment is designed to be fully reproducible and is conducted for a series of matrices with progressively increasing condition numbers.\n\nFirst, we must construct a family of matrices $A(\\kappa)$ of size $n \\times n$ with a precisely controlled condition number $\\kappa$. The problem specifies $n=64$. A standard method to achieve this is to define the matrix through its eigendecomposition, $A(\\kappa) = Q \\Lambda(\\kappa) Q^\\top$, or equivalently, $A(\\kappa) = Q^\\top \\Lambda(\\kappa) Q$ since $Q$ is orthogonal. The matrix $\\Lambda(\\kappa) = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_n)$ is a diagonal matrix containing the eigenvalues of $A(\\kappa)$. The condition number is defined as $\\kappa_2(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$. To achieve a target condition number $\\kappa$, we can set $\\lambda_{\\min} = 1$ and $\\lambda_{\\max} = \\kappa$. The problem specifies that the $n$ eigenvalues, $\\lambda_i$, are to be linearly spaced between $1$ and $\\kappa$. Thus, the set of eigenvalues is $\\{\\lambda_i\\}_{i=1}^n$ where $\\lambda_i = 1 + (i-1)\\frac{\\kappa-1}{n-1}$. Since all $\\kappa \\ge 1$, all eigenvalues $\\lambda_i$ are greater than or equal to $1$, ensuring that the resulting matrix $A(\\kappa)$ is positive definite. The construction $A(\\kappa) = Q^\\top \\Lambda(\\kappa) Q$ guarantees that $A(\\kappa)$ is symmetric.\n\nThe orthogonal matrix $Q$ is fixed for all test cases to ensure that a consistent rotational transformation is applied to the eigensystem. This isolates the effect of the eigenvalues' distribution. The matrix $Q$ is generated deterministically to ensure reproducibility. This is accomplished by first creating a dense matrix $M \\in \\mathbb{R}^{n \\times n}$ with entries drawn from a standard normal distribution, using a fixed random seed of $0$. A QR factorization of $M$ is then computed, $M = \\widehat{Q}R$. The matrix $\\widehat{Q}$ is orthogonal, but the QR decomposition is not unique; the signs of the columns of $\\widehat{Q}$ and corresponding rows of $R$ can be flipped. To obtain a unique, deterministic $Q$, we enforce a convention that the diagonal elements of $R$ must be non-negative. This is achieved by creating a diagonal matrix $D = \\operatorname{diag}(\\operatorname{sign}(R_{11}), \\dots, \\operatorname{sign}(R_{nn}))$, with the specified convention $\\operatorname{sign}(0)=1$. The final, fixed orthogonal matrix is then $Q = \\widehat{Q}D$.\n\nWith the matrix $A(\\kappa)$ constructed, we solve the linear system $A(\\kappa)x = b$ using the Conjugate Gradient method. The parameters for the solver are: a right-hand side vector $b = \\mathbf{1} \\in \\mathbb{R}^n$ (a vector of all ones), an initial guess $x_0 = 0 \\in \\mathbb{R}^n$, and a maximum of $n=64$ iterations. The CG algorithm iteratively generates a sequence of solution approximations. The process starts with the initial residual $r_0 = b - Ax_0 = b$ and the initial search direction $p_0 = r_0$. For each iteration $k = 0, 1, 2, \\dots$, the algorithm computes:\n$$ \\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k} $$\n$$ x_{k+1} = x_k + \\alpha_k p_k $$\n$$ r_{k+1} = r_k - \\alpha_k A p_k $$\nThe iteration is terminated when the Euclidean norm of the residual, $\\lVert r_{k+1} \\rVert_2$, satisfies the stopping criterion $\\lVert r_{k+1} \\rVert_2 \\le \\text{tol} \\cdot \\lVert b \\rVert_2$, where the tolerance is $\\text{tol} = 10^{-8}$. If convergence is achieved at step $k+1$, the number of iterations is recorded as $k+1$. To continue the iteration, a new search direction is computed:\n$$ \\beta_k = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k} $$\n$$ p_{k+1} = r_{k+1} + \\beta_k p_k $$\n\nThis entire procedure is executed for each of the specified test cases, corresponding to $\\kappa \\in \\{1, 10, 100, 1000\\}$. The number of iterations for each case is collected and reported. For $\\kappa=1$, $A(1)=I$, and the CG method is expected to converge in a single iteration. For increasing $\\kappa$, the problem becomes more ill-conditioned, and the number of iterations is expected to increase, potentially reaching the maximum limit of $n=64$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conjugate_gradient(A, b, tol, max_iter):\n    \"\"\"\n    Solves the system Ax=b using the Conjugate Gradient method.\n\n    Args:\n        A (np.ndarray): The symmetric positive definite matrix of the system.\n        b (np.ndarray): The right-hand side vector.\n        tol (float): The relative tolerance for the stopping criterion.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        int: The number of iterations performed.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n, dtype=np.float64)\n\n    r = b - A @ x  # Since x is zero, r = b\n    p = r.copy()\n    rs_old = np.dot(r, r)\n\n    norm_b = np.linalg.norm(b)\n    # Handle the trivial case where b is the zero vector.\n    if norm_b == 0.0:\n        return 0\n\n    stop_threshold = tol * norm_b\n\n    # The initial residual r_0 is b. If it already meets the criterion,\n    # 0 iterations are needed. This is unlikely for the given problem.\n    if np.linalg.norm(r) <= stop_threshold:\n        return 0\n\n    for k in range(1, max_iter + 1):\n        Ap = A @ p\n        \n        # Numerator for alpha is r_k^T * r_k\n        # Denominator is p_k^T * A * p_k\n        alpha = rs_old / np.dot(p, Ap)\n        \n        # Update solution and residual\n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        # Check for convergence using the L2-norm of the residual\n        if np.linalg.norm(r) <= stop_threshold:\n            return k\n            \n        rs_new = np.dot(r, r)\n        \n        # Update search direction\n        beta = rs_new / rs_old\n        p = r + beta * p\n        \n        rs_old = rs_new\n        \n    # If the loop completes, the method did not converge within max_iter.\n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the numerical experiment and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, kappa, tol)\n        (64, 1.0, 1e-8),\n        (64, 10.0, 1e-8),\n        (64, 100.0, 1e-8),\n        (64, 1000.0, 1e-8),\n    ]\n\n    results = []\n    \n    # Common parameters\n    n = test_cases[0][0]\n    seed = 0\n\n    # 1. Construct the fixed orthogonal matrix Q, as per the problem description.\n    # Use the modern, preferred way of seeding for reproducibility.\n    rng = np.random.default_rng(seed)\n    M = rng.normal(size=(n, n))\n    \n    # Compute the QR factorization. For a square matrix, 'reduced' is default.\n    Q_hat, R = np.linalg.qr(M)\n    \n    # Create the sign-correction diagonal matrix D to ensure Q is unique.\n    # The problem specifies sign(0) = 1. numpy.sign(0) is 0, so we correct for it.\n    signs = np.sign(np.diag(R))\n    signs[signs == 0] = 1.0\n    # The final fixed orthogonal matrix Q.\n    Q = Q_hat @ np.diag(signs)\n\n    for n_case, kappa, tol in test_cases:\n        # 2. Construct the SPD matrix A(kappa) for the current case.\n        # Eigenvalues are linearly spaced between 1 and kappa.\n        eigenvalues = np.linspace(1.0, kappa, n, dtype=np.float64)\n        Lambda = np.diag(eigenvalues)\n        \n        # Form A = Q^T * Lambda * Q\n        A = Q.T @ Lambda @ Q\n\n        # 3. Define the linear system and solve it.\n        # Right-hand side is a vector of all ones.\n        b = np.ones(n, dtype=np.float64)\n        \n        # The maximum number of iterations is n.\n        max_iterations = n_case\n        \n        # Run the Conjugate Gradient solver.\n        iterations_count = conjugate_gradient(A, b, tol, max_iterations)\n        results.append(iterations_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3209858"}, {"introduction": "Ordinary differential equations (ODEs) model countless phenomena in science and engineering, but not all ODEs are equally easy to solve. Systems with widely separated timescales are known as \"stiff\" and pose a significant stability challenge for many numerical methods. This exercise puts you at the heart of this classic problem by benchmarking an explicit solver (RK4) against an implicit one (Backward Euler) [@problem_id:3209907]. By finding the maximum stable and accurate step size for each, you will discover why the unconditional stability of implicit methods makes them indispensable for tackling stiff systems, even if they require more computation per step.", "problem": "Consider the initial value problem for a two-dimensional, decoupled, linear stiff system of ordinary differential equations (ODEs):\n$$\n\\frac{d}{dt}\n\\begin{bmatrix}\ny_1(t) \\\\\ny_2(t)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\lambda_{\\text{slow}} & 0 \\\\\n0 & \\lambda_{\\text{fast}}\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1(t) \\\\\ny_2(t)\n\\end{bmatrix}, \\quad\n\\begin{bmatrix}\ny_1(0) \\\\\ny_2(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 \\\\\n1\n\\end{bmatrix},\n$$\nwhere $\\lambda_{\\text{slow}} < 0$, $\\lambda_{\\text{fast}} < 0$, and the stiffness ratio is defined by $S = \\left|\\lambda_{\\text{fast}}\\right| / \\left|\\lambda_{\\text{slow}}\\right|$. The exact solution is $y_i(t) = \\exp\\left(\\lambda_i t\\right)$ for $i \\in \\{\\text{slow}, \\text{fast}\\}$. You will benchmark the explicit classical fourth-order Runge–Kutta method (RK4) against the implicit backward Euler method on this family of problems.\n\nFundamental base definitions to use:\n- A one-step method applied to the scalar linear test equation $y'(t) = \\lambda y(t)$ yields an update of the form $y_{n+1} = R(z)\\,y_n$, where $z = \\lambda \\Delta t$ and $R(z)$ is the method-dependent stability function.\n- A method is said to be linearly stable for a given step size $\\Delta t$ if $\\left|R(\\lambda \\Delta t)\\right| \\leq 1$ for all eigenvalues $\\lambda$ of the system matrix. For the two-dimensional diagonal system above, this reduces to requiring stability for both $\\lambda_{\\text{slow}}$ and $\\lambda_{\\text{fast}}$.\n- The numerical solution after $N$ full steps of size $\\Delta t$ and one final partial step of size $\\Delta t_{\\text{last}} \\in [0,\\Delta t)$ satisfies $t_N + \\Delta t_{\\text{last}} = T$ and can be represented componentwise as $y_i^{\\text{num}}(T) = R\\!\\left(\\lambda_i \\Delta t\\right)^N \\, R\\!\\left(\\lambda_i \\Delta t_{\\text{last}}\\right) \\, y_i(0)$, while the exact solution is $y_i^{\\text{exact}}(T) = \\exp\\!\\left(\\lambda_i T\\right)\\,y_i(0)$.\n\nYour task:\n1. Implement both the explicit Runge–Kutta method of order $4$ (RK4) and the implicit backward Euler method for the diagonal linear system described above.\n2. For each solver and each test case, determine the largest step size $\\Delta t_{\\max}$ that simultaneously satisfies:\n   - Linear stability for all components, i.e., $\\left|R\\!\\left(\\lambda_{\\text{slow}} \\Delta t\\right)\\right| \\leq 1$ and $\\left|R\\!\\left(\\lambda_{\\text{fast}} \\Delta t\\right)\\right| \\leq 1$.\n   - An accuracy constraint at final time $T$ given by the infinity norm of the error,\n     $$\n     \\left\\| y^{\\text{num}}(T) - y^{\\text{exact}}(T) \\right\\|_{\\infty}\n     =\n     \\max\\left(\n       \\left| y_1^{\\text{num}}(T) - y_1^{\\text{exact}}(T) \\right|,\n       \\left| y_2^{\\text{num}}(T) - y_2^{\\text{exact}}(T) \\right|\n     \\right)\n     \\leq \\varepsilon.\n     $$\n   The final time is denoted by $T$, and the tolerance by $\\varepsilon$. Use $N = \\left\\lfloor T / \\Delta t \\right\\rfloor$ full steps and a final partial step of size $\\Delta t_{\\text{last}} = T - N\\,\\Delta t$ (possibly zero). You must determine $\\Delta t_{\\max}$ by a monotone search (for example, a bisection) over $\\Delta t \\in (0, \\Delta t_{\\text{upper}}]$ where $\\Delta t_{\\text{upper}}$ is a safe upper bound. For RK4 on stiff problems, restrict $\\Delta t_{\\text{upper}}$ by stability; for backward Euler, restrict by $\\Delta t_{\\text{upper}} \\leq T$.\n3. Use the following test suite. Each test case specifies $(\\lambda_{\\text{slow}}, S, T, \\varepsilon)$; set $\\lambda_{\\text{fast}} = -S \\left|\\lambda_{\\text{slow}}\\right|$.\n   - Test A (happy path): $(\\lambda_{\\text{slow}}, S, T, \\varepsilon) = (-1, 100, 1, 10^{-6})$.\n   - Test B (strong stiffness, looser accuracy): $(\\lambda_{\\text{slow}}, S, T, \\varepsilon) = (-1, 1000, 1, 10^{-4})$.\n   - Test C (long horizon, moderate stiffness, tight accuracy): $(\\lambda_{\\text{slow}}, S, T, \\varepsilon) = (-0.1, 50, 10, 10^{-8})$.\n4. For each test case, compute two floats: the largest stable and accurate step size for RK4, $\\Delta t_{\\max}^{\\text{RK4}}$, and for backward Euler, $\\Delta t_{\\max}^{\\text{BE}}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the six results as a comma-separated list enclosed in square brackets, in the order\n$$\n\\left[\n\\Delta t_{\\max}^{\\text{RK4}}(\\text{A}),\n\\Delta t_{\\max}^{\\text{BE}}(\\text{A}),\n\\Delta t_{\\max}^{\\text{RK4}}(\\text{B}),\n\\Delta t_{\\max}^{\\text{BE}}(\\text{B}),\n\\Delta t_{\\max}^{\\text{RK4}}(\\text{C}),\n\\Delta t_{\\max}^{\\text{BE}}(\\text{C})\n\\right].\n$$\nAll values must be floats. No units are involved in this purely mathematical setup.", "solution": "The problem statement has been critically reviewed and is determined to be **valid**. It is a well-posed, scientifically grounded exercise in numerical analysis, designed to benchmark the performance of explicit and implicit numerical methods on a stiff system of ordinary differential equations (ODEs). All parameters, definitions, and constraints are provided unambiguously, allowing for a unique and meaningful solution.\n\nThe task is to find the maximum step size, $\\Delta t_{\\max}$, for two numerical methods—the fourth-order Runge-Kutta method (RK4) and the backward Euler method (BE)—that satisfies both a stability and an accuracy constraint for a given linear a two-dimensional system. The system is defined by\n$$\n\\frac{d\\mathbf{y}}{dt} = \\mathbf{A}\\mathbf{y}, \\quad \\mathbf{A} = \\begin{bmatrix} \\lambda_{\\text{slow}} & 0 \\\\ 0 & \\lambda_{\\text{fast}} \\end{bmatrix}, \\quad \\mathbf{y}(0) = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\n$$\nThe solution involves a monotone search (specifically, a bisection search) for $\\Delta t_{\\max}$ over a defined interval for each test case.\n\nFirst, we must define the stability function, $R(z)$, for each method, where $z = \\lambda \\Delta t$. For a scalar test equation $y' = \\lambda y$, the numerical scheme gives $y_{n+1} = R(\\lambda \\Delta t) y_n$.\n\nFor the implicit backward Euler method, the update rule is $y_{n+1} = y_n + \\Delta t (\\lambda y_{n+1})$. Solving for $y_{n+1}$ yields $y_{n+1} = (1 - \\lambda \\Delta t)^{-1} y_n$. Thus, the stability function is:\n$$\nR_{\\text{BE}}(z) = \\frac{1}{1 - z}\n$$\n\nFor the explicit classical fourth-order Runge-Kutta method, the stability function is the truncated Taylor series of the exponential function up to the fourth order:\n$$\nR_{\\text{RK4}}(z) = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\frac{z^4}{4!}\n$$\n\nNext, we analyze the constraints on the step size $\\Delta t$.\n\nThe first constraint is linear stability, which requires $|R(\\lambda \\Delta t)| \\leq 1$ for all eigenvalues of the system. Since $\\lambda_{\\text{slow}}$ and $\\lambda_{\\text{fast}}$ are both real and negative, $z = \\lambda \\Delta t$ is real and negative.\nFor the backward Euler method, with $z < 0$, we have $1 - z > 1$, so $|R_{\\text{BE}}(z)| = \\frac{1}{1-z}$ is always between $0$ and $1$. Therefore, the backward Euler method is A-stable and imposes no stability restriction on $\\Delta t$ for this problem. The search for $\\Delta t_{\\max}$ for BE is therefore bounded above by the integration time $T$, as specified.\n\nFor the RK4 method, the region of absolute stability on the real axis is approximately $[-2.785, 0]$. The condition $|R_{\\text{RK4}}(z)| \\leq 1$ for $z < 0$ requires $z \\geq z_{\\text{stab}}$, where $z_{\\text{stab}} \\approx -2.785287335359$. This must hold for both $z_{\\text{slow}} = \\lambda_{\\text{slow}} \\Delta t$ and $z_{\\text{fast}} = \\lambda_{\\text{fast}} \\Delta t$. Since $|\\lambda_{\\text{fast}}| \\geq |\\lambda_{\\text{slow}}|$, the more restrictive condition is $\\lambda_{\\text{fast}} \\Delta t \\geq z_{\\text{stab}}$, which implies $\\Delta t \\leq \\frac{z_{\\text{stab}}}{\\lambda_{\\text{fast}}} = \\frac{|z_{\\text{stab}}|}{|\\lambda_{\\text{fast}}|}$. This value serves as the upper bound $\\Delta t_{\\text{upper}}$ for the bisection search for RK4.\n\nThe second constraint is accuracy. The infinity norm of the error at the final time $T$ must not exceed a tolerance $\\varepsilon$:\n$$\n\\left\\| \\mathbf{y}^{\\text{num}}(T) - \\mathbf{y}^{\\text{exact}}(T) \\right\\|_{\\infty} \\leq \\varepsilon\n$$\nThe problem provides the explicit formulae for the exact and numerical solutions. The component-wise exact solution is $y_i^{\\text{exact}}(T) = \\exp(\\lambda_i T) y_i(0)$. The numerical solution, accounting for a final partial step, is $y_i^{\\text{num}}(T) = R(\\lambda_i \\Delta t)^N R(\\lambda_i \\Delta t_{\\text{last}}) y_i(0)$, where $N = \\lfloor T / \\Delta t \\rfloor$ and $\\Delta t_{\\text{last}} = T - N \\Delta t$. Since $y_i(0)=1$, the condition on each component $i$ is:\n$$\n\\left| R(\\lambda_i \\Delta t)^N \\, R(\\lambda_i \\Delta t_{\\text{last}}) - \\exp(\\lambda_i T) \\right| \\leq \\varepsilon\n$$\n\nThe overall task is to find the largest $\\Delta t > 0$ that satisfies both constraints. For a given method and problem parameters, we define a function `is_valid(Δt)` that returns true if and only if both stability and accuracy constraints are met. For RK4, the stability constraint is handled by limiting the search space to $\\Delta t \\in (0, \\Delta t_{\\text{upper}}]$. For BE, stability is always satisfied. The accuracy constraint is checked by computing the error for a given $\\Delta t$ and comparing it to $\\varepsilon$.\n\nThe function `is_valid(Δt)` is expected to be monotonic: if a step size $\\Delta t$ is valid, any smaller step size $\\Delta t' < \\Delta t$ will also be valid, as numerical error generally decreases with step size for stable integrations. This monotonicity allows us to use a bisection search to efficiently find $\\Delta t_{\\max}$.\n\nThe bisection algorithm proceeds as follows:\n1. Initialize a search interval $[\\Delta t_{\\text{low}}, \\Delta t_{\\text{high}}]$ with $\\Delta t_{\\text{low}} = 0$ and $\\Delta t_{\\text{high}} = \\Delta t_{\\text{upper}}$.\n2. Iterate a fixed number of times (e.g., $100$, sufficient for double-precision accuracy):\n   a. Calculate the midpoint $\\Delta t_{\\text{mid}} = (\\Delta t_{\\text{low}} + \\Delta t_{\\text{high}}) / 2$.\n   b. If `is_valid(Δt_mid)` is true, it means a step size of $\\Delta t_{\\text{mid}}$ is acceptable, and we may be able to do better. We set $\\Delta t_{\\text{low}} = \\Delta t_{\\text{mid}}$.\n   c. If `is_valid(Δt_mid)` is false, the step size is too large. We set $\\Delta t_{\\text{high}} = \\Delta t_{\\text{mid}}$.\n3. After the iterations, $\\Delta t_{\\text{low}}$ is a very close approximation of the desired $\\Delta t_{\\max}$.\n\nThis procedure is applied to each of the three test cases for both the RK4 and BE methods to compute the six required values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the maximum stable and accurate step size for RK4 and Backward Euler\n    methods on a stiff ODE system for a suite of test cases.\n    \"\"\"\n\n    # The stability boundary for the RK4 method on the negative real axis.\n    # Root of 1 + z + z^2/2 + z^3/6 + z^4/24 = -1.\n    RK4_STABILITY_BOUNDARY = 2.785287335359041\n\n    def R_rk4(z: float) -> float:\n        \"\"\"Stability function for the classical 4th order Runge-Kutta method.\"\"\"\n        return 1.0 + z * (1.0 + z * (1/2.0 + z * (1/6.0 + z / 24.0)))\n\n    def R_be(z: float) -> float:\n        \"\"\"Stability function for the implicit Backward Euler method.\"\"\"\n        return 1.0 / (1.0 - z)\n\n    def find_max_dt(params: tuple, method_type: str) -> float:\n        \"\"\"\n        Finds the largest step size dt for a given method and problem parameters\n        that satisfies both stability and accuracy constraints, using bisection search.\n        \"\"\"\n        lambda_slow, S, T, epsilon = params\n        lambda_fast = -S * abs(lambda_slow)\n\n        if method_type == 'RK4':\n            stability_func = R_rk4\n            # For RK4, the step size is limited by the stability region.\n            # |lambda_fast * dt| <= RK4_STABILITY_BOUNDARY\n            dt_upper = RK4_STABILITY_BOUNDARY / abs(lambda_fast)\n        elif method_type == 'BE':\n            stability_func = R_be\n            # Backward Euler is A-stable, no stability restriction for these lambda < 0.\n            # The problem specifies using T as the upper bound for the search.\n            dt_upper = T\n        else:\n            raise ValueError(f\"Unknown method type: {method_type}\")\n\n        def is_valid(dt: float) -> bool:\n            \"\"\"\n            Checks if a given step size dt satisfies the accuracy constraint.\n            Stability is enforced by the search range of the bisection method.\n            \"\"\"\n            if dt <= 1e-15:  # Effectively zero dt, considered valid but we search for dt > 0.\n                return True\n\n            N = np.floor(T / dt)\n            dt_last = T - N * dt\n\n            # Component 1 (slow)\n            z_slow = lambda_slow * dt\n            z_slow_last = lambda_slow * dt_last\n            y1_num = (stability_func(z_slow)**N) * stability_func(z_slow_last)\n            y1_exact = np.exp(lambda_slow * T)\n            err1 = abs(y1_num - y1_exact)\n\n            # Component 2 (fast)\n            z_fast = lambda_fast * dt\n            z_fast_last = lambda_fast * dt_last\n            y2_num = (stability_func(z_fast)**N) * stability_func(z_fast_last)\n            y2_exact = np.exp(lambda_fast * T)\n            err2 = abs(y2_num - y2_exact)\n            \n            return max(err1, err2) <= epsilon\n\n        # Bisection search to find the boundary of the valid dt region.\n        low = 0.0\n        high = dt_upper\n\n        # Bisection loop for a fixed number of iterations for high precision.\n        for _ in range(100):\n            mid = (low + high) / 2.0\n            if mid == low or mid == high: # Reached floating point precision limit\n                break\n            \n            if is_valid(mid):\n                low = mid  # mid is valid, so it's a candidate. Try for a larger dt.\n            else:\n                high = mid # mid is invalid, dt must be smaller.\n        \n        return low\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lambda_slow, S, T, epsilon)\n        (-1.0, 100.0, 1.0, 1e-6),    # Case A\n        (-1.0, 1000.0, 1.0, 1e-4),   # Case B\n        (-0.1, 50.0, 10.0, 1e-8),   # Case C\n    ]\n\n    results = []\n    for params in test_cases:\n        dt_max_rk4 = find_max_dt(params, 'RK4')\n        results.append(dt_max_rk4)\n        \n        dt_max_be = find_max_dt(params, 'BE')\n        results.append(dt_max_be)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3209907"}, {"introduction": "Asymptotic complexity provides a powerful lens for comparing algorithms, suggesting that methods with better scaling, like Strassen's $\\mathcal{O}(N^{2.807})$ matrix multiplication, will always win in the long run. However, theory often collides with practical reality, where implementation overheads and constant factors can dominate for small to medium-sized problems. This hands-on benchmark challenges you to move beyond pure theory and empirically find the performance \"crossover point\" [@problem_id:3209812]. By measuring wall-clock time, you will identify the problem size at which the higher overhead of Strassen's algorithm gives way to its superior asymptotic efficiency, reinforcing a crucial lesson in computational science: the fastest algorithm on paper is not always the fastest in practice.", "problem": "You are to implement a reproducible benchmarking program to empirically identify a finite-size crossover, within a prescribed finite test grid, between the classical cubic-time matrix multiplication and the Strassen algorithm when run on dense square matrices of real numbers. The goal is to quantify where the classical algorithm is no slower than Strassen because of implementation overheads that dominate at small sizes. The benchmarking must be done in seconds, using the median of multiple runs for robustness, and without any user input.\n\nStart from these fundamental bases:\n- The classical dense matrix multiplication of two square matrices of size $N \\times N$ has time complexity $\\mathcal{O}(N^3)$ under the standard arithmetic cost model where addition and multiplication of real numbers have constant cost.\n- The Strassen algorithm reduces the number of sub-multiplications by using $7$ block multiplications at each recursion, along with block additions and subtractions whose combined cost per level is bounded by a constant times $N^2$ under the same model.\n- Empirical benchmarking measures actual runtime $t(N)$ in seconds, which can be modeled as a positive function that depends on implementation constants and overheads not captured by asymptotic notation.\n\nDefinitions and requirements:\n- Implement two algorithms:\n  1. A classical multiplication that computes $C = A \\times B$ for $A, B \\in \\mathbb{R}^{N \\times N}$, with time complexity $\\mathcal{O}(N^3)$.\n  2. A Strassen multiplication that uses the standard $7$-multiplication recursion on square blocks, with a base-case threshold parameter $b$ such that if $N \\le b$, the multiplication is delegated to the classical algorithm. For general $N$, pad inputs with zeros to the next power of two so that block partitioning is always valid; return the output cropped back to size $N \\times N$.\n- For each candidate size $N$, generate inputs $A, B$ with independent entries drawn from a standard normal distribution $\\mathcal{N}(0,1)$ using a fixed random seed to ensure reproducibility. Perform one untimed warm-up call per algorithm and per size to mitigate cold-start artifacts.\n- For each $N$ and each algorithm, measure the runtime in seconds using a monotonic high-resolution clock and record the median over $r$ repeated timed runs. Denote the medians by $t_{\\mathrm{classical}}(N)$ and $t_{\\mathrm{Strassen}}(N)$.\n- Define the finite-grid crossover decision for a given parameter set as follows: among the provided candidate sizes, return the largest $N$ such that $t_{\\mathrm{classical}}(N) \\le t_{\\mathrm{Strassen}}(N)$. If no such $N$ exists in the candidate list, return $-1$.\n- Verify numerical correctness once per $N$ by checking that the maximum absolute entrywise difference between the two algorithms’ outputs is at most $\\tau = 10^{-8}$; do not include this check in any timed run. If a violation occurs, treat this as an implementation error and do not alter timing behavior; but your program should still complete and produce outputs for the test suite.\n\nUnits and measurement:\n- All times must be measured and reported internally in seconds.\n- Angles are not involved.\n- Any ratio, if used for internal decisions, must be treated as a pure number (dimensionless) and not printed.\n\nTest suite:\nRun the benchmark for the following five parameter sets; each set is a tuple $(\\text{sizes}, b, r, s)$:\n- Case $1$: sizes $=[\\,16,32,64\\,]$, base-case threshold $b=16$, repeats $r=3$, seed $s=0$.\n- Case $2$: sizes $=[\\,24,32,48\\,]$, base-case threshold $b=8$, repeats $r=3$, seed $s=1$.\n- Case $3$: sizes $=[\\,8,16,32\\,]$, base-case threshold $b=64$, repeats $r=5$, seed $s=2$.\n- Case $4$: sizes $=[\\,32,64\\,]$, base-case threshold $b=32$, repeats $r=2$, seed $s=3$.\n- Case $5$: sizes $=[\\,16\\,]$, base-case threshold $b=8$, repeats $r=5$, seed $s=4$.\n\nFinal output format:\n- Your program should produce a single line of output containing the crossover decisions for the five cases as a comma-separated list enclosed in square brackets, for example, $[n_1,n_2,n_3,n_4,n_5]$, where each $n_i$ is an integer equal to the largest $N$ in the corresponding candidate list for which $t_{\\mathrm{classical}}(N) \\le t_{\\mathrm{Strassen}}(N)$, or $-1$ if no such $N$ exists for that case.\n- Print nothing else.", "solution": "The problem requires the implementation and empirical benchmarking of classical and Strassen matrix multiplication algorithms to identify the crossover point in performance. The crossover point is defined as the largest matrix size $N$ within a given finite set for which the classical algorithm, benefiting from lower overheads, is no slower than the asymptotically superior Strassen algorithm.\n\nFirst, the theoretical foundations of the two algorithms are considered. The classical algorithm for multiplying two matrices $A, B \\in \\mathbb{R}^{N \\times N}$ to compute $C = A \\times B$ is defined by the element-wise formula $C_{ij} = \\sum_{k=1}^{N} A_{ik} B_{kj}$. This requires $N^3$ scalar multiplications and $N^2(N-1)$ scalar additions, resulting in a time complexity of $\\mathcal{O}(N^3)$. In contrast, Strassen's algorithm is a recursive, divide-and-conquer method. It partitions the $N \\times N$ matrices into four $N/2 \\times N/2$ sub-blocks and computes the product matrix using only $7$ recursive multiplications and a fixed number of matrix additions and subtractions. The computational cost is described by the recurrence relation $T(N) = 7 T(N/2) + \\mathcal{O}(N^2)$, which solves to a time complexity of $\\mathcal{O}(N^{\\log_2 7}) \\approx \\mathcal{O}(N^{2.807})$. Despite its better asymptotic scaling, Strassen's algorithm incurs significant overhead from recursive function calls and the additional matrix-add-subtract operations, making it less efficient for small $N$.\n\nThe implementation consists of two main functions, one for each algorithm, and a benchmarking framework to execute the comparison.\n\nThe classical matrix multiplication is implemented as a function `classical_matmul(A, B)`. To provide a realistic and high-performance baseline, this function leverages `numpy.dot`. When using a high-level language like `Python` with the `numpy` library, this is the canonical implementation of classical matrix multiplication, typically linking to optimized Basic Linear Algebra Subprograms (BLAS) libraries.\n\nThe Strassen matrix multiplication is implemented in a recursive function `strassen_matmul(A, B, b)`. This function is designed according to the problem's specifications:\n1.  **Base Case**: The recursion terminates when the matrix size $N$ satisfies $N \\le b$, where $b$ is the base-case threshold. In this scenario, the computation is delegated to the `classical_matmul` function. This creates a hybrid algorithm that avoids the overhead of recursion for small matrices where it is not beneficial.\n2.  **Padding and Cropping**: The recursive partitioning of Strassen's algorithm requires matrices with dimensions that are powers of two. The implementation handles a general input matrix of size $N \\times N$ by first checking if $N$ is a power of two. If it is not, the input matrices $A$ and $B$ are padded with zeros to create new square matrices of size $m \\times m$, where $m$ is the smallest power of two greater than or equal to $N$. The recursive algorithm is then called on these padded matrices. The resulting $m \\times m$ product matrix is then cropped to the original size $N \\times N$ before being returned. This padding-and-cropping wrapper ensures the core recursive logic can operate exclusively on dimensions that are powers of two.\n3.  **Recursive Step**: If $N > b$ and $N$ is a power of two, the matrices $A$ and $B$ are each partitioned into four sub-blocks of size $N/2 \\times N/2$. The algorithm then proceeds to compute the $7$ intermediate products $M_1, \\dots, M_7$ via recursive calls to `strassen_matmul`. For example, $M_1$ is calculated as `strassen_matmul(A11 + A22, B11 + B22, b)`. Finally, the sub-blocks of the result matrix $C$ are computed by combining these $7$ products according to Strassen's formulas (e.g., $C_{11} = M_1 + M_4 - M_5 + M_7$).\n\nThe benchmarking framework is encapsulated within a main `solve` function that processes a suite of test cases. For each test case, defined by a tuple of parameters $(\\text{sizes}, b, r, s)$:\n1.  **Reproducibility**: The random number generator is initialized with the seed $s$ using `numpy.random.seed(s)`. The input matrices $A$ and $B$ of size $N \\times N$ are then generated with entries drawn from a standard normal distribution, $\\mathcal{N}(0,1)$.\n2.  **Timing Measurement**: Prior to timed runs, one untimed \"warm-up\" call is made for each algorithm to mitigate cold-start effects. For an accurate performance measurement, each algorithm is executed $r$ times. The wall-clock time for each execution is measured in seconds using the high-resolution monotonic clock `time.perf_counter()`. The median of these $r$ time measurements is taken as the representative runtime, denoted $t_{\\mathrm{classical}}(N)$ and $t_{\\mathrm{Strassen}}(N)$ respectively. The median is used for its robustness to system-related timing noise.\n3.  **Numerical Validation**: As a sanity check, the correctness of the Strassen implementation is verified once per size $N$ by computing the maximum absolute entrywise difference between its output and the classical algorithm's output, ensuring it does not exceed a tolerance of $\\tau = 10^{-8}$. This check is not included in the timed portion of the benchmark.\n4.  **Crossover Decision**: After obtaining the median runtimes for all specified sizes $N$ in a test case, the program identifies the set of sizes for which $t_{\\mathrm{classical}}(N) \\le t_{\\mathrm{Strassen}}(N)$. The final result for the test case is the largest $N$ in this set. If the set is empty (i.e., Strassen's algorithm is faster for all test sizes), the result is $-1$.\n\nThe program systematically executes this procedure for all five test cases and collates the resulting crossover values. The final output is a single line containing these five integer results, formatted as a comma-separated list enclosed in square brackets, e.g., $[n_1,n_2,n_3,n_4,n_5]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport time\n\ndef classical_matmul(A, B):\n    \"\"\"\n    Computes the matrix product C = A @ B using NumPy's optimized dot product.\n    This serves as the baseline classical O(N^3) algorithm.\n    \"\"\"\n    return np.dot(A, B)\n\ndef strassen_matmul(A, B, b):\n    \"\"\"\n    Computes the matrix product C = A @ B using Strassen's algorithm.\n    - b: The base-case threshold. If N <= b, switches to classical_matmul.\n    - Handles non-power-of-two matrices by padding and cropping.\n    \"\"\"\n    N = A.shape[0]\n\n    # Base case: If the matrix size is below or at the threshold,\n    # delegate to the classical algorithm.\n    if N <= b:\n        return classical_matmul(A, B)\n\n    # If N is not a power of two, pad it to the next power of two.\n    is_power_of_two = (N > 0) and (N & (N - 1) == 0)\n    if not is_power_of_two:\n        # Find the next power of two\n        m = 1 << (N - 1).bit_length()\n        \n        # Create padded matrices\n        A_pad = np.zeros((m, m))\n        A_pad[:N, :N] = A\n        B_pad = np.zeros((m, m))\n        B_pad[:N, :N] = B\n        \n        # Perform Strassen multiplication on the padded matrices\n        C_pad = strassen_matmul(A_pad, B_pad, b)\n        \n        # Crop the result back to the original size\n        return C_pad[:N, :N]\n\n    # --- Recursive Step ---\n    # At this point, N is guaranteed to be a power of two and N > b.\n    mid = N // 2\n    \n    # Partition matrices into four sub-blocks of size mid x mid\n    A11, A12 = A[:mid, :mid], A[:mid, mid:]\n    A21, A22 = A[mid:, :mid], A[mid:, mid:]\n    B11, B12 = B[:mid, :mid], B[:mid, mid:]\n    B21, B22 = B[mid:, :mid], B[mid:, mid:]\n\n    # 7 recursive calls as per Strassen's algorithm\n    M1 = strassen_matmul(A11 + A22, B11 + B22, b)\n    M2 = strassen_matmul(A21 + A22, B11, b)\n    M3 = strassen_matmul(A11, B12 - B22, b)\n    M4 = strassen_matmul(A22, B21 - B11, b)\n    M5 = strassen_matmul(A11 + A12, B22, b)\n    M6 = strassen_matmul(A21 - A11, B11 + B12, b)\n    M7 = strassen_matmul(A12 - A22, B21 + B22, b)\n\n    # Combine the 7 products to form the sub-blocks of the result matrix C\n    C11 = M1 + M4 - M5 + M7\n    C12 = M3 + M5\n    C21 = M2 + M4\n    C22 = M1 - M2 + M3 + M6\n\n    # Assemble the final result matrix C from its sub-blocks\n    C = np.empty((N, N), dtype=A.dtype)\n    C[:mid, :mid] = C11\n    C[:mid, mid:] = C12\n    C[mid:, :mid] = C21\n    C[mid:, mid:] = C22\n\n    return C\n\ndef solve():\n    \"\"\"\n    Main function to run the benchmarking suite and find crossover points.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (sizes, base_case_threshold, repeats, seed)\n        ([16, 32, 64], 16, 3, 0),\n        ([24, 32, 48], 8, 3, 1),\n        ([8, 16, 32], 64, 5, 2),\n        ([32, 64], 32, 2, 3),\n        ([16], 8, 5, 4),\n    ]\n\n    results = []\n    tau = 1e-8\n\n    for sizes, b, r, s in test_cases:\n        np.random.seed(s)\n        \n        crossover_candidates = []\n        \n        for N in sizes:\n            # Generate reproducible random matrices\n            A = np.random.randn(N, N)\n            B = np.random.randn(N, N)\n            \n            # Untimed warm-up calls to mitigate cold-start artifacts\n            _ = classical_matmul(A, B)\n            _ = strassen_matmul(A, B, b)\n            \n            # Benchmark classical algorithm\n            classical_times = []\n            for _ in range(r):\n                start = time.perf_counter()\n                C_classical = classical_matmul(A, B)\n                end = time.perf_counter()\n                classical_times.append(end - start)\n            t_classical = np.median(classical_times)\n            \n            # Benchmark Strassen algorithm\n            strassen_times = []\n            for _ in range(r):\n                start = time.perf_counter()\n                C_strassen = strassen_matmul(A, B, b)\n                end = time.perf_counter()\n                strassen_times.append(end - start)\n            t_strassen = np.median(strassen_times)\n            \n            # Verify numerical correctness (not part of timing)\n            max_abs_diff = np.max(np.abs(C_classical - C_strassen))\n            if max_abs_diff > tau:\n                # Per problem spec, this is an implementation error but should not\n                # alter program completion or output format.\n                pass \n\n            # Check if classical is no slower than Strassen\n            if t_classical <= t_strassen:\n                crossover_candidates.append(N)\n        \n        # Determine the crossover decision for the current case\n        if not crossover_candidates:\n            case_result = -1\n        else:\n            case_result = max(crossover_candidates)\n            \n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3209812"}]}