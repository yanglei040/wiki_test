{"hands_on_practices": [{"introduction": "To effectively use importance sampling, we must first be able to measure its performance. This practice provides a foundational exercise in calculating the variance of an importance sampling estimator, which is the primary metric for its efficiency. By working through a classic scenario involving Gaussian distributions, you will gain hands-on experience with the core formula for variance and see how the choice of proposal distribution directly impacts the estimator's precision. [@problem_id:767801]", "problem": "Importance Sampling is a variance reduction technique used in Monte Carlo methods to estimate properties of a particular distribution, while sampling from a different distribution. Consider the problem of estimating the expectation $I = E_p[h(x)]$, where $p(x)$ is the target probability density function (PDF) and $h(x)$ is a function of interest. Instead of drawing samples from $p(x)$, we draw $N$ samples $\\{x_i\\}_{i=1}^N$ from a proposal distribution $q(x)$. The expectation is then estimated using the importance sampling estimator:\n$$\n\\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^N w(x_i) h(x_i)\n$$\nwhere $w(x) = \\frac{p(x)}{q(x)}$ are the importance weights. The estimator is unbiased, i.e., $E_q[\\hat{I}_N] = I$. The variance of the estimator for a single sample ($N=1$) is given by $\\text{Var}_q(w(X)h(X))$, where $X \\sim q(x)$.\n\n**Problem:**\nLet the target distribution $p(x)$ be the standard normal distribution, $\\mathcal{N}(x|0, 1)$, whose PDF is $p(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}$. We wish to estimate the fourth moment of this distribution, corresponding to the function $h(x) = x^4$.\n\nThe proposal distribution $q(x)$ is chosen to be a zero-mean normal distribution with a different variance $\\sigma_q^2$, i.e., $q(x) = \\mathcal{N}(x|0, \\sigma_q^2)$, with PDF $q(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_q} e^{-x^2/(2\\sigma_q^2)}$.\n\nDerive a closed-form expression for the variance of the single-sample importance sampling estimator, $\\text{Var}_q(w(X)h(X))$, as a function of $\\sigma_q$. You may assume that the variance is finite, which holds for $\\sigma_q^2 > 1/2$.", "solution": "1. The variance is  \n$$\n\\mathrm{Var}_q\\bigl(w(X)h(X)\\bigr)\n=E_q\\bigl[(w(X)h(X))^2\\bigr]-\\bigl(E_q[w(X)h(X)]\\bigr)^2.\n$$\nSince $h(x)=x^4$, $w(x)=\\frac{p(x)}{q(x)}$, and $I=E_p[x^4]=3$, we need \n$$\nE_q\\bigl[(w(X)x^4)^2\\bigr]\n=\\int q(x)\\Bigl(\\frac{p(x)}{q(x)}\\Bigr)^2 x^8\\,dx\n=\\int \\frac{p(x)^2}{q(x)}\\,x^8\\,dx.\n$$\n\n2. Substitute the Gaussian densities:\n$$\np(x)=\\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2},\\quad\nq(x)=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_q}e^{-x^2/(2\\sigma_q^2)}.\n$$\nThen\n$$\n\\frac{p(x)^2}{q(x)}\n=\\frac{\\frac1{2\\pi}e^{-x^2}}{\\frac1{\\sqrt{2\\pi}\\,\\sigma_q}e^{-x^2/(2\\sigma_q^2)}}\n=\\frac{\\sigma_q}{\\sqrt{2\\pi}}\\,e^{-x^2\\bigl(1-\\tfrac1{2\\sigma_q^2}\\bigr)}.\n$$\n\n3. Let \n$$\na=1-\\frac1{2\\sigma_q^2}=\\frac{2\\sigma_q^2-1}{2\\sigma_q^2},\\quad\n\\int_{-\\infty}^{\\infty}x^8e^{-a x^2}dx\n=\\frac{7!!}{2^4}\\,a^{-4-\\frac12}\\sqrt\\pi\n=\\frac{105\\sqrt\\pi}{16\\,a^{9/2}}.\n$$\nThus\n$$\nE_q\\bigl[(w(X)x^4)^2\\bigr]\n=\\frac{\\sigma_q}{\\sqrt{2\\pi}}\\cdot\\frac{105\\sqrt\\pi}{16\\,a^{9/2}}\n=\\frac{105\\,\\sigma_q}{16\\sqrt2\\,a^{9/2}}\n=\\frac{105\\,\\sigma_q^{10}}{(2\\sigma_q^2-1)^{9/2}}.\n$$\n\n4. Hence\n$$\n\\mathrm{Var}_q(w(X)x^4)\n=\\frac{105\\,\\sigma_q^{10}}{(2\\sigma_q^2-1)^{9/2}}-3^2\n=\\frac{105\\,\\sigma_q^{10}}{(2\\sigma_q^2-1)^{9/2}}-9.\n$$", "answer": "$$\\boxed{\\frac{105\\,\\sigma_q^{10}}{(2\\sigma_q^2-1)^{9/2}}-9}$$", "id": "767801"}, {"introduction": "Importance sampling is a powerful tool, but it is not foolproof, and a poor choice of proposal distribution can lead to disastrous results. This exercise serves as a critical cautionary tale, demonstrating a scenario where the variance of the estimator is infinite, making it practically useless despite being formally unbiased. By analyzing the estimation of an integral with singularities using a naive uniform proposal, you will learn to identify the conditions that lead to such failures and appreciate the importance of matching the proposal's behavior to the target integrand. [@problem_id:3241984]", "problem": "Consider the task of estimating the integral $$I=\\int_{-1}^{1}\\frac{1}{\\sqrt{1-x^{2}}}\\,dx$$ using Importance Sampling (IS), where the proposal density is uniform on the interval $$[-1,1]$$, that is $$q(x)=\\frac{1}{2}$$ for $$x\\in[-1,1]$$ and $$q(x)=0$$ otherwise. The IS weight associated with a sample $$X\\sim q$$ is defined by $$W=\\frac{f(X)}{q(X)}$$ where $$f(x)=\\frac{1}{\\sqrt{1-x^{2}}}$$. \n\nStarting from the foundational definitions of importance sampling and properties of integrable singularities, perform the following analyses to assess the reliability of the estimator:\n- Derive the IS weight $$W$$ explicitly as a function of $$X$$ and analyze its behavior near the endpoints $$x=\\pm 1$$.\n- Using the definition of the second moment under the proposal distribution, determine whether $$\\int_{-1}^{1}\\frac{f(x)^{2}}{q(x)}\\,dx$$ is finite or infinite, and justify your conclusion by a principled argument that does not rely on heuristic shortcuts.\n- To quantify the tail heaviness of $$W$$ under $$X\\sim q$$, determine the exact leading-order asymptotic decay of the tail probability $$\\mathbb{P}(W>t)$$ as $$t\\to\\infty$$. In particular, derive the exact constant $$c$$ in the asymptotic $$\\mathbb{P}(W>t)\\sim c\\,t^{-2}$$ as $$t\\to\\infty$$.\n\nProvide a complete derivation from first principles. Your final reported answer must be the single constant $$c$$ (no intermediate quantities), written in exact form. No rounding is required and no units are involved.", "solution": "The problem statement is a valid exercise in the analysis of Monte Carlo methods, specifically Importance Sampling (IS). It is scientifically grounded in the principles of numerical integration and probability theory, is well-posed with all necessary information provided, and is formulated objectively. We may therefore proceed with the solution.\n\nThe problem asks for a three-part analysis of an Importance Sampling scheme for the integral $I=\\int_{-1}^{1}\\frac{1}{\\sqrt{1-x^{2}}}\\,dx$. The target integrand is $f(x)=\\frac{1}{\\sqrt{1-x^{2}}}$, and the proposal probability density function (PDF) is a uniform distribution on the interval $[-1,1]$, given by $q(x)=\\frac{1}{2}$ for $x\\in[-1,1]$ and $q(x)=0$ otherwise.\n\nFirst, we derive the IS weight $W(x)$ and analyze its behavior. The weight function is defined as the ratio of the target integrand to the proposal density.\n$$W(x) = \\frac{f(x)}{q(x)} = \\frac{\\frac{1}{\\sqrt{1-x^{2}}}}{\\frac{1}{2}} = \\frac{2}{\\sqrt{1-x^{2}}}$$\nThis expression is valid for $x \\in (-1, 1)$. We analyze the behavior of $W(x)$ as $x$ approaches the endpoints of the interval, $x=\\pm 1$.\nAs $x \\to 1^{-}$ or $x \\to -1^{+}$, the term $1-x^2$ approaches $0$ from the positive side. Consequently, $\\sqrt{1-x^2} \\to 0^{+}$, and the weight $W(x)$ diverges to infinity:\n$$\\lim_{x\\to \\pm 1} W(x) = \\lim_{x\\to \\pm 1} \\frac{2}{\\sqrt{1-x^{2}}} = \\infty$$\nThis indicates that samples drawn near the boundaries of the integration domain will have extremely large weights, which is a sign of a potentially high-variance or infinite-variance estimator.\n\nSecond, we determine if the second moment of the weight under the proposal distribution is finite. The variance of the IS estimator is finite if and only if the second moment of the weights, $\\mathbb{E}_q[W^2]$, is finite. This moment is given by the integral:\n$$\\mathbb{E}_q[W^2] = \\int_{-1}^{1} W(x)^2 q(x) \\, dx = \\int_{-1}^{1} \\left(\\frac{f(x)}{q(x)}\\right)^2 q(x) \\, dx = \\int_{-1}^{1} \\frac{f(x)^2}{q(x)} \\, dx$$\nSubstituting the expressions for $f(x)$ and $q(x)$:\n$$\\frac{f(x)^2}{q(x)} = \\frac{\\left(\\frac{1}{\\sqrt{1-x^{2}}}\\right)^2}{\\frac{1}{2}} = \\frac{\\frac{1}{1-x^2}}{\\frac{1}{2}} = \\frac{2}{1-x^2}$$\nThus, we must evaluate the integral:\n$$\\mathbb{E}_q[W^2] = \\int_{-1}^{1} \\frac{2}{1-x^2} \\, dx$$\nThis is an improper integral due to the non-integrable singularities at $x=-1$ and $x=1$. To demonstrate its divergence, we can use the p-integral test. Near $x=1$, we can write $x=1-\\epsilon$ for a small $\\epsilon > 0$. The denominator becomes $1-x^2 = (1-x)(1+x) = \\epsilon(2-\\epsilon) \\approx 2\\epsilon$. The integrand behaves like $\\frac{2}{2\\epsilon} = \\frac{1}{\\epsilon}$. The integral near $x=1$ behaves like $\\int \\frac{1}{\\epsilon} \\, d\\epsilon$, which is $\\ln(\\epsilon)$, a logarithmically divergent form.\nMore formally, we can compute the indefinite integral:\n$$\\int \\frac{2}{1-x^2} \\, dx = 2 \\cdot \\frac{1}{2} \\ln\\left|\\frac{1+x}{1-x}\\right| + C = \\ln\\left|\\frac{1+x}{1-x}\\right| + C$$\nEvaluating the definite integral from $-1$ to $1$:\n$$\\int_{-1}^{1} \\frac{2}{1-x^2} \\, dx = \\lim_{a \\to 1^{-}} \\lim_{b \\to -1^{+}} \\left[ \\ln\\left(\\frac{1+x}{1-x}\\right) \\right]_b^a$$\nAs $a \\to 1^{-}$, $\\ln\\left(\\frac{1+a}{1-a}\\right) \\to \\ln\\left(\\frac{2}{0^{+}}\\right) \\to \\infty$.\nAs $b \\to -1^{+}$, $\\ln\\left(\\frac{1+b}{1-b}\\right) \\to \\ln\\left(\\frac{0^{+}}{2}\\right) \\to -\\infty$.\nThe integral diverges. Therefore, the second moment $\\mathbb{E}_q[W^2]$ is infinite, which implies that the variance of the IS estimator is infinite.\n\nThird, we find the constant $c$ in the asymptotic decay of the tail probability $\\mathbb{P}(W>t) \\sim c\\,t^{-2}$. The probability is with respect to a random variable $X$ drawn from the proposal distribution $q$, i.e., $X \\sim \\text{Uniform}[-1, 1]$.\nWe need to find the probability of the event $W(X) > t$:\n$$\\mathbb{P}(W > t) = \\mathbb{P}\\left(\\frac{2}{\\sqrt{1-X^2}} > t\\right)$$\nFor $t>0$, we can rearrange the inequality:\n$$\\frac{2}{t} > \\sqrt{1-X^2}$$\nSince both sides are positive, we can square them:\n$$\\frac{4}{t^2} > 1-X^2$$\n$$X^2 > 1 - \\frac{4}{t^2}$$\nFor large $t$, specifically $t>2$, the right-hand side is positive. This inequality is equivalent to $|X| > \\sqrt{1 - \\frac{4}{t^2}}$.\nLet $a_t = \\sqrt{1 - \\frac{4}{t^2}}$. The event is $|X| > a_t$. Since $X \\in [-1, 1]$, this corresponds to $X \\in (-1, -a_t) \\cup (a_t, 1)$.\nThe probability is calculated by integrating the PDF $q(x)=\\frac{1}{2}$ over this set:\n$$\\mathbb{P}(W > t) = \\int_{-1}^{-a_t} \\frac{1}{2} \\, dx + \\int_{a_t}^{1} \\frac{1}{2} \\, dx$$\n$$\\mathbb{P}(W > t) = \\frac{1}{2} \\left( -a_t - (-1) \\right) + \\frac{1}{2} \\left( 1 - a_t \\right) = \\frac{1}{2}(1-a_t) + \\frac{1}{2}(1-a_t) = 1 - a_t$$\nSubstituting the expression for $a_t$:\n$$\\mathbb{P}(W > t) = 1 - \\sqrt{1 - \\frac{4}{t^2}}$$\nTo find the leading-order asymptotic behavior as $t \\to \\infty$, we use the Taylor expansion of $\\sqrt{1-u}$ for small $u$. Let $u = \\frac{4}{t^2}$. As $t \\to \\infty$, $u \\to 0$. The expansion is:\n$$\\sqrt{1-u} = 1 - \\frac{1}{2}u + O(u^2)$$\nSubstituting this into our expression for the probability:\n$$\\mathbb{P}(W > t) = 1 - \\left(1 - \\frac{1}{2}\\left(\\frac{4}{t^2}\\right) + O\\left(\\left(\\frac{4}{t^2}\\right)^2\\right)\\right)$$\n$$\\mathbb{P}(W > t) = 1 - 1 + \\frac{2}{t^2} - O\\left(\\frac{1}{t^4}\\right) = \\frac{2}{t^2} + O\\left(\\frac{1}{t^4}\\right)$$\nThe asymptotic relationship is defined as $\\lim_{t\\to\\infty} \\frac{\\mathbb{P}(W > t)}{c\\,t^{-2}} = 1$. Using our result:\n$$\\lim_{t\\to\\infty} \\frac{\\frac{2}{t^2} + O(\\frac{1}{t^4})}{c\\,t^{-2}} = \\lim_{t\\to\\infty} \\frac{2 + O(\\frac{1}{t^2})}{c} = \\frac{2}{c}$$\nFor this limit to equal $1$, we must have $c=2$.", "answer": "$$\\boxed{2}$$", "id": "3241984"}, {"introduction": "Beyond simply avoiding infinite variance, we can often design a proposal distribution that is, in a specific sense, optimal. This advanced practice introduces the powerful technique of exponential tilting, guided by the principles of large deviations theory, to construct an asymptotically optimal importance sampler for estimating rare-event probabilities. By deriving the ideal tilting parameter, you will move from analyzing pre-defined samplers to proactively engineering efficient solutions for challenging estimation problems. [@problem_id:3241853]", "problem": "Let $X_{1}, X_{2}, \\dots, X_{n}$ be independent and identically distributed random variables with $X_{i} \\sim \\text{Exp}(1)$, and define the partial sum $S_{n} = \\sum_{i=1}^{n} X_{i}$. Consider the rare-event probability $\\mathbb{P}(S_{n} > c)$ with a fixed threshold $c > n$. To estimate this probability efficiently, use importance sampling with an exponentially tilted change of measure: for a parameter $\\theta$ with $\\theta < 1$, draw each $X_{i}$ under a new density $f_{\\theta}(x)$ that is proportional to $\\exp(\\theta x)$ times the original density of $\\text{Exp}(1)$, and weight samples by the appropriate likelihood ratio to form an unbiased estimator of $\\mathbb{P}(S_{n} > c)$. \n\nStarting from the following bases:\n- the moment-generating function $M_{X}(\\theta) = \\mathbb{E}[\\exp(\\theta X)]$ and cumulant-generating function $\\Lambda(\\theta) = \\ln M_{X}(\\theta)$ for $X \\sim \\text{Exp}(1)$,\n- the Cramér large deviations principle for the sample mean $\\bar{X}_{n} = S_{n}/n$ with its good rate function $I(x) = \\sup_{\\theta < 1} \\{\\theta x - \\Lambda(\\theta)\\}$,\n- the Esscher (exponential) change of measure for independent and identically distributed sums,\n\nderive, from first principles and without invoking any pre-stated shortcuts, the choice of exponential tilting parameter $\\theta^{\\star}$ that asymptotically minimizes the variance of the importance sampling estimator for $\\mathbb{P}(S_{n} > c)$ as $n \\to \\infty$ according to large deviations theory. Express your final answer as a closed-form analytic expression in terms of $n$ and $c$. No numerical approximation or rounding is required.", "solution": "We aim to identify an exponential tilting parameter that yields an asymptotically optimal importance sampling (IS) scheme for estimating the rare-event probability $\\mathbb{P}(S_{n} > c)$ when $c > n$. The derivation proceeds from the definitions of the moment-generating function (MGF), cumulant-generating function (CGF), and the Cramér large deviations principle.\n\nFirst, recall that for $X \\sim \\text{Exp}(1)$, the probability density function is $f(x) = \\exp(-x)$ for $x \\ge 0$. The moment-generating function $M_{X}(\\theta)$ and cumulant-generating function $\\Lambda(\\theta)$ are\n$$\nM_{X}(\\theta) = \\mathbb{E}[\\exp(\\theta X)] = \\int_{0}^{\\infty} \\exp(\\theta x) \\exp(-x)\\, dx = \\frac{1}{1 - \\theta}, \\quad \\theta < 1,\n$$\n$$\n\\Lambda(\\theta) = \\ln M_{X}(\\theta) = -\\ln(1 - \\theta), \\quad \\theta < 1.\n$$\n\nUnder exponential tilting with parameter $\\theta$, the new density for $X$ is the Esscher transform\n$$\nf_{\\theta}(x) = \\exp\\big(\\theta x - \\Lambda(\\theta)\\big) f(x) = \\exp\\big(\\theta x + \\ln(1-\\theta)\\big) \\exp(-x) = (1-\\theta)\\,\\exp\\big(-(1-\\theta) x\\big),\n$$\nwhich is the density of an exponential distribution with rate $1 - \\theta$ (so the tilted $X$ has mean $1/(1-\\theta)$). Under this change of measure, sampling $X_{1},\\dots,X_{n}$ independently from $f_{\\theta}$ produces $S_{n}$ whose distribution is the sum of $n$ independent exponentials with rate $1-\\theta$, i.e., a gamma distribution with shape $n$ and rate $1-\\theta$.\n\nThe importance sampling estimator for $\\mathbb{P}(S_{n} > c)$ using this tilt is\n$$\n\\hat{p} = \\exp\\big(-n \\Lambda(\\theta)\\big) \\exp\\big(\\theta S_{n}\\big)\\,\\mathbf{1}\\{S_{n} > c\\},\n$$\nwhere the likelihood ratio for a path $\\{X_{i}\\}$ equals $\\prod_{i=1}^{n} \\exp(\\theta X_{i} - \\Lambda(\\theta)) = \\exp(\\theta S_{n} - n \\Lambda(\\theta))$.\n\nTo asymptotically minimize the variance of $\\hat{p}$ (equivalently, its second moment) as $n \\to \\infty$, we use large deviations theory. Let $\\bar{X}_{n} = S_{n}/n$. By Cramér’s theorem for independent and identically distributed random variables with common CGF $\\Lambda(\\theta)$, the sample mean satisfies a Large Deviations Principle (LDP) with good convex rate function given by the Legendre transform\n$$\nI(x) = \\sup_{\\theta < 1}\\{\\theta x - \\Lambda(\\theta)\\}.\n$$\nFor the exponential distribution considered here, this rate function is explicitly computable, but we only need its variational characterization.\n\nThe guiding principle for variance reduction via exponential tilting is that the optimal tilt aligns the typical value under the new measure with the rare-event threshold. Formally, the Esscher tilt that asymptotically minimizes the second moment of $\\hat{p}$ corresponds to choosing the tilt parameter that maximizes the variational expression in the rate function at the point $x = c/n$. This is the optimizer of the Legendre transform at $x = c/n$, which is characterized by the first-order condition\n$$\n\\Lambda'(\\theta^{\\star}) = x,\n$$\nevaluated at $x = c/n$. This condition arises because the function $\\theta \\mapsto \\theta x - \\Lambda(\\theta)$ is strictly concave in $\\theta$ for fixed $x$ when $\\Lambda$ is strictly convex, and the maximizer satisfies the derivative condition $x - \\Lambda'(\\theta) = 0$.\n\nWe compute $\\Lambda'(\\theta)$:\n$$\n\\Lambda(\\theta) = -\\ln(1-\\theta) \\quad \\Rightarrow \\quad \\Lambda'(\\theta) = \\frac{1}{1 - \\theta}.\n$$\nSetting $\\Lambda'(\\theta^{\\star}) = c/n$ yields\n$$\n\\frac{1}{1 - \\theta^{\\star}} = \\frac{c}{n}\n\\quad \\Rightarrow \\quad \n1 - \\theta^{\\star} = \\frac{n}{c}\n\\quad \\Rightarrow \\quad \n\\theta^{\\star} = 1 - \\frac{n}{c}.\n$$\nSince $c > n$, we have $\\theta^{\\star} \\in (0, 1)$, which is admissible for the Esscher tilt. Interpreting this choice, under the optimal tilt the mean of each tilted $X_{i}$ is\n$$\n\\mathbb{E}_{\\theta^{\\star}}[X_{i}] = \\frac{1}{1 - \\theta^{\\star}} = \\frac{c}{n},\n$$\nso the typical value of the sample mean $\\bar{X}_{n}$ under the tilted measure equals the rare-event threshold $c/n$. This alignment ensures, by the large deviations principle and the convex duality structure of the Legendre transform, that the IS scheme asymptotically minimizes the variance of the estimator among exponential tilts.\n\nTherefore, the optimal exponential tilting parameter derived from large deviations considerations is\n$$\n\\theta^{\\star} = 1 - \\frac{n}{c}.\n$$", "answer": "$$\\boxed{1 - \\frac{n}{c}}$$", "id": "3241853"}]}