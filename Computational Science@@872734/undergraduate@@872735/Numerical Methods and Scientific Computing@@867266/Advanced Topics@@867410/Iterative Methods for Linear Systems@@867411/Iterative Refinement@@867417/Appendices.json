{"hands_on_practices": [{"introduction": "Understanding iterative refinement begins with a single, concrete step. This first practice uses a simple $2 \\times 2$ linear system to demonstrate the core mechanics of the algorithm, showing how calculating a residual and solving for a correction can significantly improve an inaccurate initial guess. This exercise specifically highlights how iterative refinement addresses the challenges posed by ill-conditioned systems, where a small residual can misleadingly mask a large error in the solution [@problem_id:2182580].", "problem": "Consider the linear system of equations $Ax = b$, where the matrix $A$ and the vector $b$ are defined as:\n$$A = \\begin{pmatrix} 1  1 \\\\ 1  1.001 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 3 \\\\ 3.002 \\end{pmatrix}$$\nAn approximate solution to this system, denoted by $x_0$, is given as $x_0 = \\begin{pmatrix} 2.998 \\\\ 0 \\end{pmatrix}$.\nUsing the provided matrix $A$ and the initial approximation $x_0$, perform a single step of iterative refinement to compute a more accurate solution, $x_1$. In this single step, you are required to solve the associated correction equation exactly.\n\nProvide the components of the refined solution vector $x_1$. Express your final answer as a row matrix containing the two components of $x_1$. Round each component to four significant figures if necessary.", "solution": "We perform one step of iterative refinement. Given $x_{0} = \\begin{pmatrix} 2.998 \\\\ 0 \\end{pmatrix}$, compute the residual\n$$r = b - A x_{0}.$$\nFirst compute\n$$A x_{0} = \\begin{pmatrix} 1  1 \\\\ 1  1.001 \\end{pmatrix} \\begin{pmatrix} 2.998 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2.998 \\\\ 2.998 \\end{pmatrix}.$$\nHence\n$$r = \\begin{pmatrix} 3 \\\\ 3.002 \\end{pmatrix} - \\begin{pmatrix} 2.998 \\\\ 2.998 \\end{pmatrix} = \\begin{pmatrix} 0.002 \\\\ 0.004 \\end{pmatrix}.$$\nThe correction $d$ is obtained by solving the exact correction equation\n$$A d = r, \\quad \\text{that is} \\quad \\begin{pmatrix} 1  1 \\\\ 1  1.001 \\end{pmatrix} \\begin{pmatrix} d_{1} \\\\ d_{2} \\end{pmatrix} = \\begin{pmatrix} 0.002 \\\\ 0.004 \\end{pmatrix}.$$\nThis gives the system\n$$d_{1} + d_{2} = 0.002, \\quad d_{1} + 1.001 d_{2} = 0.004.$$\nSubtract the first equation from the second:\n$$(d_{1} + 1.001 d_{2}) - (d_{1} + d_{2}) = 0.004 - 0.002 \\;\\Rightarrow\\; 0.001 d_{2} = 0.002 \\;\\Rightarrow\\; d_{2} = 2.$$\nThen\n$$d_{1} = 0.002 - d_{2} = 0.002 - 2 = -1.998.$$\nUpdate the approximation:\n$$x_{1} = x_{0} + d = \\begin{pmatrix} 2.998 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} -1.998 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.$$\nTherefore, the refined solution is exactly $\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, which already meets any rounding requirement.", "answer": "$$\\boxed{\\begin{pmatrix} 1  2 \\end{pmatrix}}$$", "id": "2182580"}, {"introduction": "The effectiveness of iterative refinement in the real world hinges on how we handle the limitations of floating-point arithmetic. This exercise delves into a crucial detail: the precision of the residual calculation. You will investigate a hypothetical scenario where a low-precision calculation incorrectly suggests the solution is perfect (a zero residual), while a higher-precision calculation reveals the true, non-zero error, demonstrating why computing residuals with extra precision is essential for the algorithm's success [@problem_id:3245538].", "problem": "Consider a normalized base-$10$ floating-point system $\\mathcal{F}_{10}(t)$ with $t$ significant digits, rounding to nearest with ties to even. Suppose that the working precision has $t_{w} = 2$ and the guard precision has $t_{g} = 5$. In iterative refinement for solving the linear system $A x = b$, the residual used to form the correction is computed as $r^{(0)}_{t} = \\operatorname{fl}_{t}\\!\\left(b - \\operatorname{fl}_{t}(A x^{(0)})\\right)$, where $\\operatorname{fl}_{t}$ denotes rounding every intermediate operation to $t$ significant digits in $\\mathcal{F}_{10}(t)$. For a diagonal matrix, the matrix-vector product reduces to componentwise scalar products.\n\nDesign the data to provoke a misleading zero residual in working precision while revealing a nonzero residual in guard precision by choosing\n$$\nA = \\begin{pmatrix}\n1.0005  0 \\\\\n0  0.9996\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n1.0 \\\\\n1.0\n\\end{pmatrix}, \\quad\nx^{(0)} = \\begin{pmatrix}\n1.0 \\\\\n1.0\n\\end{pmatrix},\n$$\nwith $A$, $b$, and $x^{(0)}$ stored in guard precision $\\mathcal{F}_{10}(t_{g})$.\n\nUsing only the rules above and the fundamental definition of the residual in floating-point arithmetic, do the following:\n- Verify that the working-precision residual $r^{(0)}_{t_{w}}$ is exactly the zero vector in $\\mathcal{F}_{10}(t_{w})$.\n- Compute the guard-precision residual $r^{(0)}_{t_{g}}$ in $\\mathcal{F}_{10}(t_{g})$.\n- Compute the Euclidean norm (the $2$-norm) $\\|r^{(0)}_{t_{g}}\\|_{2}$.\n\nRound your final numeric answer for $\\|r^{(0)}_{t_{g}}\\|_{2}$ to four significant figures. No units are required.", "solution": "The problem requires the analysis of a residual calculation in two different floating-point precisions to illustrate a potential pitfall in iterative refinement. The floating-point system is base-$10$, $\\mathcal{F}_{10}(t)$, with rounding to nearest (ties to even).\n\nThe provided data is:\nWorking precision digits, $t_w = 2$.\nGuard precision digits, $t_g = 5$.\nMatrix $A = \\begin{pmatrix} 1.0005  0 \\\\ 0  0.9996 \\end{pmatrix}$.\nVector $b = \\begin{pmatrix} 1.0 \\\\ 1.0 \\end{pmatrix}$.\nInitial guess $x^{(0)} = \\begin{pmatrix} 1.0 \\\\ 1.0 \\end{pmatrix}$.\nThe data $A$, $b$, and $x^{(0)}$ are stored in guard precision, $\\mathcal{F}_{10}(5)$.\nThe residual is defined as $r^{(0)}_{t} = \\operatorname{fl}_{t}\\!\\left(b - \\operatorname{fl}_{t}(A x^{(0)})\\right)$, where $\\operatorname{fl}_{t}$ indicates that each intermediate operation is rounded to $t$ significant digits.\n\nThe problem is divided into three tasks: verifying the working-precision residual is zero, computing the guard-precision residual, and finding the Euclidean norm of the guard-precision residual.\n\n**Part 1: Verification of the working-precision residual $r^{(0)}_{t_{w}}$**\n\nWe compute the residual using the working precision $t_w = 2$. The formula is $r^{(0)}_{t_{w}} = \\operatorname{fl}_{2}\\!\\left(b - \\operatorname{fl}_{2}(A x^{(0)})\\right)$. The calculation is performed component-wise.\n\nFirst, we compute the matrix-vector product $y = \\operatorname{fl}_{2}(A x^{(0)})$. The term $\\operatorname{fl}_{2}(A x^{(0)})$ implies that the arithmetic operations to compute the product are performed in $\\mathcal{F}_{10}(2)$. The input matrices $A$ and $x^{(0)}$ are used from their $t_g=5$ storage, with results of operations being rounded to $t_w=2$ digits.\n\nFor the first component, $y_1$:\n$$\ny_1 = \\operatorname{fl}_{2}\\left( A_{11} x_1^{(0)} + A_{12} x_2^{(0)} \\right) = \\operatorname{fl}_{2}\\left( 1.0005 \\times 1.0 + 0 \\times 1.0 \\right) = \\operatorname{fl}_{2}(1.0005)\n$$\nTo round $1.0005$ to $2$ significant digits, we write it in normalized form $1.0005 \\times 10^0$. We must truncate after the second significant digit. The third digit is $0$, which is less than $5$, so we round down.\n$$\ny_1 = 1.0\n$$\n\nFor the second component, $y_2$:\n$$\ny_2 = \\operatorname{fl}_{2}\\left( A_{21} x_1^{(0)} + A_{22} x_2^{(0)} \\right) = \\operatorname{fl}_{2}\\left( 0 \\times 1.0 + 0.9996 \\times 1.0 \\right) = \\operatorname{fl}_{2}(0.9996)\n$$\nTo round $0.9996$ to $2$ significant digits, we write it as $9.996 \\times 10^{-1}$. We must truncate after the second significant digit. The third digit is $9$, which is greater than or equal to $5$, so we round up. The number $9.9$ rounds up to $10$.\n$$\ny_2 = 10 \\times 10^{-1} = 1.0\n$$\nSo, the computed matrix-vector product in working precision is $\\operatorname{fl}_{2}(A x^{(0)}) = \\begin{pmatrix} 1.0 \\\\ 1.0 \\end{pmatrix}$.\n\nNext, we compute the residual vector $r^{(0)}_{t_w} = \\operatorname{fl}_{2}(b - y)$. The vector $b$ has components $b_1 = 1.0$ and $b_2 = 1.0$.\nFor the first component of the residual:\n$$\nr^{(0)}_{t_w, 1} = \\operatorname{fl}_{2}(b_1 - y_1) = \\operatorname{fl}_{2}(1.0 - 1.0) = \\operatorname{fl}_{2}(0.0) = 0.0\n$$\nFor the second component of the residual:\n$$\nr^{(0)}_{t_w, 2} = \\operatorname{fl}_{2}(b_2 - y_2) = \\operatorname{fl}_{2}(1.0 - 1.0) = \\operatorname{fl}_{2}(0.0) = 0.0\n$$\nThus, the working-precision residual is the zero vector, as was to be verified:\n$$\nr^{(0)}_{t_{w}} = \\begin{pmatrix} 0.0 \\\\ 0.0 \\end{pmatrix}\n$$\n\n**Part 2: Computation of the guard-precision residual $r^{(0)}_{t_{g}}$**\n\nWe now compute the residual using the guard precision $t_g = 5$. The formula is $r^{(0)}_{t_{g}} = \\operatorname{fl}_{5}\\!\\left(b - \\operatorname{fl}_{5}(A x^{(0)})\\right)$. All data ($A, b, x^{(0)}$) are already stored in $\\mathcal{F}_{10}(5)$, so their values are used directly.\n- $A=\\begin{pmatrix} 1.0005  0 \\\\ 0  0.99960 \\end{pmatrix}$\n- $b=\\begin{pmatrix} 1.0000 \\\\ 1.0000 \\end{pmatrix}$\n- $x^{(0)}=\\begin{pmatrix} 1.0000 \\\\ 1.0000 \\end{pmatrix}$\n\nFirst, we compute the matrix-vector product $y = \\operatorname{fl}_{5}(A x^{(0)})$.\nFor the first component, $y_1$:\n$$\ny_1 = \\operatorname{fl}_{5}\\left( 1.0005 \\times 1.0000 \\right) = \\operatorname{fl}_{5}(1.0005)\n$$\nThe number $1.0005$ has $5$ significant digits, so no rounding is needed. $y_1 = 1.0005$.\n\nFor the second component, $y_2$:\n$$\ny_2 = \\operatorname{fl}_{5}\\left( 0.99960 \\times 1.0000 \\right) = \\operatorname{fl}_{5}(0.99960)\n$$\nThe number $0.99960$ ($9.9960 \\times 10^{-1}$) has $5$ significant digits, so no rounding is needed. $y_2 = 0.99960$.\nSo, $\\operatorname{fl}_{5}(A x^{(0)}) = \\begin{pmatrix} 1.0005 \\\\ 0.99960 \\end{pmatrix}$.\n\nNext, we compute the residual vector $r^{(0)}_{t_g} = \\operatorname{fl}_{5}(b - y)$.\nFor the first component:\n$$\nr^{(0)}_{t_g, 1} = \\operatorname{fl}_{5}(b_1 - y_1) = \\operatorname{fl}_{5}(1.0000 - 1.0005) = \\operatorname{fl}_{5}(-0.0005)\n$$\nThe number $-0.0005$ can be written as $-5.0000 \\times 10^{-4}$ in $\\mathcal{F}_{10}(5)$. No rounding is needed. $r^{(0)}_{t_g, 1} = -0.0005$.\n\nFor the second component:\n$$\nr^{(0)}_{t_g, 2} = \\operatorname{fl}_{5}(b_2 - y_2) = \\operatorname{fl}_{5}(1.0000 - 0.99960) = \\operatorname{fl}_{5}(0.00040)\n$$\nThe number $0.00040$ can be written as $4.0000 \\times 10^{-4}$ in $\\mathcal{F}_{10}(5)$. No rounding is needed. $r^{(0)}_{t_g, 2} = 0.00040$.\n\nThus, the guard-precision residual is:\n$$\nr^{(0)}_{t_{g}} = \\begin{pmatrix} -0.0005 \\\\ 0.00040 \\end{pmatrix}\n$$\n\n**Part 3: Computation of the Euclidean norm $\\|r^{(0)}_{t_{g}}\\|_{2}$**\n\nThe final step is to compute the Euclidean norm of the guard-precision residual vector $r^{(0)}_{t_{g}}$. The calculation is performed in exact arithmetic, and the final result is rounded to four significant figures.\n$$\n\\|r^{(0)}_{t_{g}}\\|_{2} = \\sqrt{(r^{(0)}_{t_g, 1})^2 + (r^{(0)}_{t_g, 2})^2} = \\sqrt{(-0.0005)^2 + (0.00040)^2}\n$$\nWe evaluate the terms inside the square root:\n$$\n(-0.0005)^2 = (-5 \\times 10^{-4})^2 = 25 \\times 10^{-8}\n$$\n$$\n(0.00040)^2 = (4 \\times 10^{-4})^2 = 16 \\times 10^{-8}\n$$\nSumming these values:\n$$\n25 \\times 10^{-8} + 16 \\times 10^{-8} = 41 \\times 10^{-8} = 4.1 \\times 10^{-7}\n$$\nNow, we take the square root:\n$$\n\\|r^{(0)}_{t_{g}}\\|_{2} = \\sqrt{41 \\times 10^{-8}} = \\sqrt{41} \\times 10^{-4}\n$$\nNumerically evaluating $\\sqrt{41}$:\n$$\n\\sqrt{41} \\approx 6.403124237\n$$\nSo, the norm is approximately $6.403124237 \\times 10^{-4}$. Rounding this value to four significant figures, we get:\n$$\n\\|r^{(0)}_{t_{g}}\\|_{2} \\approx 6.403 \\times 10^{-4}\n$$", "answer": "$$\\boxed{6.403 \\times 10^{-4}}$$", "id": "3245538"}, {"introduction": "Now, let's move from manual calculations to a computational setting and analyze the performance of iterative refinement on a famously difficult problem. This coding exercise challenges you to apply the full iterative refinement process to the ill-conditioned Hilbert matrix. By tracking the gain in the number of correct digits at each step, you will gain a quantitative understanding of how the method's effectiveness changes with the severity of the system's ill-conditioning [@problem_id:3245403].", "problem": "You are to implement and analyze iterative refinement for solving a linear system with a Hilbert matrix, a classically ill-conditioned matrix. The goal is to quantify, per iteration, how many base-$10$ digits of accuracy are gained in the solution.\n\nFundamental base and definitions to use:\n- A linear system has the form $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$, $x \\in \\mathbb{R}^{n}$, and $b \\in \\mathbb{R}^{n}$. The Hilbert matrix $H \\in \\mathbb{R}^{n \\times n}$ is defined by $H_{ij} = \\frac{1}{i + j - 1}$ for $i, j \\in \\{1, \\dots, n\\}$.\n- The residual at iteration $k$ is $r^{(k)} = b - A x^{(k)}$.\n- The iterative refinement update at iteration $k$ generates a correction $d^{(k)}$ by solving $A d^{(k)} = r^{(k)}$, and sets $x^{(k+1)} = x^{(k)} + d^{(k)}$.\n- The number of correct base-$10$ digits at iteration $k$ is defined by\n$$\nD^{(k)} = -\\log_{10}\\!\\left(\\frac{\\lVert x^{(k)} - x^\\star \\rVert_\\infty}{\\lVert x^\\star \\rVert_\\infty}\\right),\n$$\nwhere $x^\\star$ is the exact solution. For numerical stability in double precision arithmetic, report $D^{(k)}$ saturated at $16$ digits, meaning use\n$$\n\\tilde{D}^{(k)} = -\\log_{10}\\!\\left(\\max\\!\\left(\\frac{\\lVert x^{(k)} - x^\\star \\rVert_\\infty}{\\lVert x^\\star \\rVert_\\infty},\\,10^{-16}\\right)\\right).\n$$\n- The per-iteration gain in correct digits is $G^{(k)} = \\tilde{D}^{(k)} - \\tilde{D}^{(k-1)}$ for $k \\ge 1$. The initial accuracy $\\tilde{D}^{(0)}$ corresponds to the solution obtained by a single direct solve of $A x = b$ before any refinement iterations.\n\nScientific realism and setup:\n- The Hilbert matrix is known to be ill-conditioned, with condition number growing rapidly as $n$ increases. Iterative refinement can improve the solution by correcting accumulated errors via residual solves.\n- Use double precision floating-point arithmetic ($64$-bit) for all computations.\n\nProgram requirements:\n- Construct $A$ as the Hilbert matrix of size $n$.\n- Set the true solution to $x^\\star = \\mathbf{1}$ (the all-ones vector of length $n$). Compute $b = A x^\\star$.\n- Compute an initial solution $x^{(0)}$ by directly solving $A x = b$.\n- Perform $m$ iterative refinement steps as described above.\n- After each iteration $k \\in \\{1, \\dots, m\\}$, compute and record $G^{(k)}$.\n- For each test case, output the list $[G^{(1)}, G^{(2)}, \\dots, G^{(m)}]$.\n\nTest suite:\n- Case $1$: $n = 2$, $m = 5$ (boundary case, relatively well-conditioned).\n- Case $2$: $n = 5$, $m = 5$ (moderately ill-conditioned).\n- Case $3$: $n = 8$, $m = 5$ (challenging ill-conditioning).\n- Case $4$: $n = 12$, $m = 5$ (severe ill-conditioning edge case).\n\nFinal output format:\n- Your program should produce a single line of output containing the per-iteration gains for all test cases as a comma-separated list of lists, with no spaces, enclosed in square brackets. For example, the output should look like $[[g_{1,1},\\dots,g_{1,m}],[g_{2,1},\\dots,g_{2,m}],\\dots]$, where $g_{i,k}$ is the gain for iteration $k$ in test case $i$.\n- All numbers should be printed as standard decimal floats, and there are no physical units involved in this problem.", "solution": "The user-provided problem statement has been independently validated and is determined to be a well-posed, scientifically grounded, and objective problem in the domain of numerical linear algebra. The problem is free of contradictions, ambiguities, and factual errors. Therefore, a complete solution is provided below.\n\nThe problem requires the implementation and analysis of the iterative refinement algorithm for solving a linear system $A x = b$, where $A$ is the notoriously ill-conditioned Hilbert matrix. The objective is to quantify the gain in solution accuracy, measured in base-$10$ digits, at each refinement step.\n\nThe Hilbert matrix $H$ of size $n \\times n$ is defined by its entries $H_{ij} = \\frac{1}{i + j - 1}$ for row and column indices $i, j$ starting from $1$. Its condition number grows extremely rapidly with $n$, making it a classic test case for numerical stability. For a system $A x = b$, a direct solution using methods like LU decomposition can accumulate significant floating-point error when $A$ is ill-conditioned, leading to an inaccurate computed solution.\n\nIterative refinement is a procedure designed to improve the accuracy of a computed solution. Let $x^{(0)}$ be the initial solution obtained by a direct solver. Due to finite precision arithmetic, $x^{(0)}$ differs from the true solution $x^\\star$ by an error $e^{(0)} = x^\\star - x^{(0)}$. The foundation of the method lies in estimating and correcting this error.\n\nThe residual vector for an approximate solution $x^{(k)}$ is defined as $r^{(k)} = b - A x^{(k)}$. By substituting $b = A x^\\star$, the residual can be related to the true error $e^{(k)} = x^\\star - x^{(k)}$:\n$$\nr^{(k)} = A x^\\star - A x^{(k)} = A (x^\\star - x^{(k)}) = A e^{(k)}\n$$\nThis equation shows that the true error $e^{(k)}$ is the solution to the linear system $A e^{(k)} = r^{(k)}$. Although we cannot compute $e^{(k)}$ exactly (as this would be equivalent to solving the original problem perfectly), we can compute an approximation to it, which we denote $d^{(k)}$, by solving the residual system:\n$$\nA d^{(k)} = r^{(k)}\n$$\nThe vector $d^{(k)}$ serves as a computed correction to the current solution. The next, hopefully more accurate, solution $x^{(k+1)}$ is obtained by applying this correction:\n$$\nx^{(k+1)} = x^{(k)} + d^{(k)}\n$$\nThis process is repeated iteratively. A critical aspect of iterative refinement is that the residual $r^{(k)}$ should ideally be computed with higher precision than the rest of the calculations. This problem, however, specifies the use of standard double precision ($64$-bit floats) for all operations, which allows us to observe the limits of the method when higher precision is not available.\n\nTo quantify the performance of the algorithm, we measure the accuracy of the solution at each step. The number of correct base-$10$ digits in the solution $x^{(k)}$ is defined relative to the true solution $x^\\star$:\n$$\nD^{(k)} = -\\log_{10}\\!\\left(\\frac{\\lVert x^{(k)} - x^\\star \\rVert_\\infty}{\\lVert x^\\star \\rVert_\\infty}\\right)\n$$\nwhere $\\lVert \\cdot \\rVert_\\infty$ is the infinity norm (maximum absolute value of the vector's components). Since double-precision floating-point arithmetic has a finite precision of approximately $16$ decimal digits, it is practical to use a saturated measure of accuracy that does not exceed this limit and avoids taking the logarithm of zero:\n$$\n\\tilde{D}^{(k)} = -\\log_{10}\\!\\left(\\max\\!\\left(\\frac{\\lVert x^{(k)} - x^\\star \\rVert_\\infty}{\\lVert x^\\star \\rVert_\\infty},\\,10^{-16}\\right)\\right)\n$$\nThe gain in accuracy at iteration $k$ is the difference in correct digits from the previous step:\n$$\nG^{(k)} = \\tilde{D}^{(k)} - \\tilde{D}^{(k-1)} \\quad \\text{for } k \\ge 1\n$$\nHere, $\\tilde{D}^{(0)}$ is the accuracy of the initial solution $x^{(0)}$ from the direct solve.\n\nThe algorithmic procedure for each test case $(n, m)$ is as follows:\n1.  **System Setup**:\n    *   Construct the $n \\times n$ Hilbert matrix $A$, where the entry at zero-based row $i$ and column $j$ is $A_{ij} = \\frac{1}{(i+1) + (j+1) - 1} = \\frac{1}{i+j+1}$.\n    *   Define the true solution as the all-ones vector, $x^\\star = \\mathbf{1} \\in \\mathbb{R}^n$.\n    *   Calculate the right-hand side vector $b = A x^\\star$. This ensures a known ground truth for error calculation. Since $x^\\star_j = 1$ for all $j$, each component $b_i$ is the $i$-th row sum of $A$: $b_i = \\sum_{j=1}^{n} \\frac{1}{i+j-1}$.\n\n2.  **Initial Solution**:\n    *   Compute the initial approximate solution $x^{(0)}$ by solving the system $A x = b$ using a standard direct numerical solver.\n    *   Calculate the initial accuracy $\\tilde{D}^{(0)}$.\n\n3.  **Iterative Refinement**:\n    *   Initialize the current solution $x \\leftarrow x^{(0)}$ and previous accuracy $D_{prev} \\leftarrow \\tilde{D}^{(0)}$.\n    *   For $k$ from $1$ to $m$:\n        a. Compute the residual: $r = b - A x$.\n        b. Solve for the correction: $A d = r$.\n        c. Update the solution: $x \\leftarrow x + d$.\n        d. Calculate the new accuracy: $D_{current} = \\tilde{D}^{(k)}$.\n        e. Calculate and record the gain: $G^{(k)} = D_{current} - D_{prev}$.\n        f. Update the previous accuracy: $D_{prev} \\leftarrow D_{current}$.\n\n4.  **Output**: Report the list of gains $[G^{(1)}, G^{(2)}, \\dots, G^{(m)}]$ for each test case.\n\nFor matrices with low condition numbers (e.g., $n=2$), the initial solution is already highly accurate, and refinement yields little to no gain. As $n$ increases ($n=5, 8$), the condition number grows, the initial solution degrades, and iterative refinement is expected to provide significant accuracy gains in the first few iterations. For $n=12$, the condition number of the Hilbert matrix exceeds $10^{16}$, which is the approximate precision limit of double-precision numbers. At this point, the computed residual is dominated by noise, and the refinement process is expected to stagnate or fail, yielding minimal or even negative gains.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes iterative refinement for linear systems involving\n    the Hilbert matrix for a suite of test cases.\n    \"\"\"\n    # Test cases are defined as tuples (n, m), where n is the matrix size\n    # and m is the number of refinement iterations.\n    test_cases = [\n        (2, 5),   # Case 1: Well-conditioned\n        (5, 5),   # Case 2: Moderately ill-conditioned\n        (8, 5),   # Case 3: Challenging ill-conditioning\n        (12, 5),  # Case 4: Severe ill-conditioning\n    ]\n\n    all_results = []\n\n    for n, m in test_cases:\n        # Step 1: System Setup\n        # Construct the n x n Hilbert matrix A.\n        # For 0-based indices i, j, the formula is H_ij = 1 / (i + j + 1).\n        A = np.fromfunction(lambda i, j: 1.0 / (i + j + 1), (n, n), dtype=float)\n\n        # Define the true solution as the all-ones vector.\n        x_star = np.ones(n, dtype=float)\n\n        # Calculate the right-hand side vector b = A * x_star.\n        b = A @ x_star\n\n        # Define a helper function to calculate the number of correct digits.\n        def get_saturated_digits(x_approx, x_true):\n            \"\"\"\n            Calculates the saturated number of correct base-10 digits.\n            \"\"\"\n            # The infinity norm of x_star is always 1.0.\n            norm_x_true_inf = 1.0\n            \n            # Calculate relative error using the infinity norm.\n            relative_error = np.linalg.norm(x_approx - x_true, np.inf) / norm_x_true_inf\n            \n            # Apply saturation at 10^-16 to handle finite precision and avoid log(0).\n            effective_error = max(relative_error, 1e-16)\n            \n            return -np.log10(effective_error)\n\n        # Step 2: Initial Solution\n        # Compute the initial solution x^(0) using a direct solver.\n        x_k = np.linalg.solve(A, b)\n\n        # Calculate the initial number of correct digits, D_tilde^(0).\n        D_prev = get_saturated_digits(x_k, x_star)\n\n        # Step 3: Iterative Refinement\n        gains_for_case = []\n        for _ in range(m):\n            # a. Compute the residual in double precision.\n            r_k = b - A @ x_k\n\n            # b. Solve for the correction vector d.\n            d_k = np.linalg.solve(A, r_k)\n\n            # c. Update the solution.\n            x_k = x_k + d_k\n\n            # d. Calculate the new accuracy.\n            D_current = get_saturated_digits(x_k, x_star)\n\n            # e. Calculate and record the gain.\n            gain = D_current - D_prev\n            gains_for_case.append(gain)\n\n            # f. Update the previous accuracy for the next iteration.\n            D_prev = D_current\n        \n        all_results.append(gains_for_case)\n\n    # Final print statement in the exact required format.\n    # Format each sublist of gains into a comma-separated string \"[g1,g2,...]\".\n    formatted_sublists = [f\"[{','.join(map(str, sublist))}]\" for sublist in all_results]\n    # Join all formatted sublists into the final output string.\n    print(f\"[{','.join(formatted_sublists)}]\")\n\nsolve()\n```", "id": "3245403"}]}