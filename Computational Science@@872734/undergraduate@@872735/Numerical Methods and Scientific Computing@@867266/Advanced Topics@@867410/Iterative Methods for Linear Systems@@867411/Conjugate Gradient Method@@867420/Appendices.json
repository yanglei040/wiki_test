{"hands_on_practices": [{"introduction": "This problem provides a gentle entry point into the Conjugate Gradient (CG) method. By performing a single iteration on a small, well-behaved system, you will ground your understanding in the concrete mechanics of the algorithm. This exercise demystifies the sequence of calculations—from computing the initial residual to updating the solution—that form the core of every CG implementation [@problem_id:1393666].", "problem": "Consider the linear system of equations $Ax=b$, where the matrix $A$ and the vector $b$ are given by:\n$$\nA = \\begin{pmatrix} 2 & -1 \\\\ -1 & 3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}\n$$\nThe matrix $A$ is symmetric and positive-definite.\n\nStarting with an initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, apply one iteration of the Conjugate Gradient method to find the first updated solution, $x_1$.\n\nExpress your answer as a row matrix containing the components of $x_1$ as exact fractions.", "solution": "We apply the Conjugate Gradient method for a symmetric positive-definite matrix $A$ starting from $x_{0}$. The standard first-iteration formulas are:\n$$\nr_{0} = b - A x_{0}, \\quad p_{0} = r_{0}, \\quad \\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}}, \\quad x_{1} = x_{0} + \\alpha_{0} p_{0}.\n$$\nGiven $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and $b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}$, compute\n$$\nr_{0} = b - A x_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}, \\quad p_{0} = r_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}.\n$$\nNext, compute $A p_{0}$:\n$$\nA p_{0} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 4 + (-1)(-3) \\\\ (-1) \\cdot 4 + 3 \\cdot (-3) \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ -13 \\end{pmatrix}.\n$$\nCompute the scalar products:\n$$\nr_{0}^{T} r_{0} = 4^{2} + (-3)^{2} = 16 + 9 = 25, \\quad p_{0}^{T} A p_{0} = r_{0}^{T} (A p_{0}) = 4 \\cdot 11 + (-3) \\cdot (-13) = 44 + 39 = 83.\n$$\nThus,\n$$\n\\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}} = \\frac{25}{83}.\n$$\nUpdate the solution:\n$$\nx_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{25}{83} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} \\frac{100}{83} \\\\ -\\frac{75}{83} \\end{pmatrix}.\n$$\nExpressed as a row matrix, the components of $x_{1}$ are $\\begin{pmatrix} \\frac{100}{83} & -\\frac{75}{83} \\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{100}{83} & -\\frac{75}{83} \\end{pmatrix}}$$", "id": "1393666"}, {"introduction": "While the Conjugate Gradient method guarantees a decrease in the error's $A$-norm, a common misconception is that the magnitude of the residual vector must also decrease at every step. This practice presents a carefully chosen counterexample to challenge that assumption [@problem_id:1393663]. By working through this calculation, you will gain a deeper appreciation for the method's specific convergence properties and learn to distinguish between different measures of progress toward the solution.", "problem": "The Conjugate Gradient (CG) method is an iterative algorithm for solving systems of linear equations $Ax=b$, where the matrix $A$ is symmetric and positive-definite. The algorithm generates a sequence of solutions $x_k$ that converge to the true solution.\n\nThe standard algorithm is defined as follows, starting with an initial guess $x_0$:\n1.  Initialize the residual $r_0 = b - Ax_0$ and the search direction $p_0 = r_0$.\n2.  For $k = 0, 1, 2, \\dots$ until convergence:\n    a. Compute the step size: $\\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k}$\n    b. Update the solution: $x_{k+1} = x_k + \\alpha_k p_k$\n    c. Update the residual: $r_{k+1} = r_k - \\alpha_k A p_k$\n    d. Compute the improvement factor: $\\beta_k = \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}$\n    e. Update the search direction: $p_{k+1} = r_{k+1} + \\beta_k p_k$\n\nConsider the linear system $Ax=b$ where the matrix $A$ and vector $b$ are given by:\n$$ A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 10 & 0 \\\\ 0 & 0 & 100 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0.1 \\\\ 0.01 \\end{pmatrix} $$\nUsing an initial guess of $x_0 = (0, 0, 0)^T$, perform one full iteration of the Conjugate Gradient algorithm (i.e., the calculations corresponding to $k=0$ in the loop) to determine the first updated residual vector, $r_1$.\n\nCalculate the Euclidean norm (also known as the $L^2$-norm) of this residual vector, $\\|r_1\\|_2$. Report your answer as a numerical value rounded to four significant figures.", "solution": "We apply the Conjugate Gradient (CG) algorithm for one iteration starting from $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nCompute the initial residual and search direction:\n$$\nr_{0} = b - A x_{0} = b = \\begin{pmatrix} 1 \\\\ 0.1 \\\\ 0.01 \\end{pmatrix}, \\quad p_{0} = r_{0}.\n$$\nThe step size is\n$$\n\\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}} = \\frac{1^{2} + 0.1^{2} + 0.01^{2}}{1 \\cdot 1^{2} + 10 \\cdot (0.1)^{2} + 100 \\cdot (0.01)^{2}} = \\frac{1.0101}{1.11} = 0.91.\n$$\nCompute $A p_{0}$:\n$$\nA p_{0} = A r_{0} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 10 & 0 \\\\ 0 & 0 & 100 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0.1 \\\\ 0.01 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\nUpdate the residual:\n$$\nr_{1} = r_{0} - \\alpha_{0} A p_{0} = \\begin{pmatrix} 1 \\\\ 0.1 \\\\ 0.01 \\end{pmatrix} - 0.91 \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.09 \\\\ -0.81 \\\\ -0.9 \\end{pmatrix}.\n$$\nCompute its Euclidean norm:\n$$\n\\|r_{1}\\|_{2} = \\sqrt{(0.09)^{2} + (-0.81)^{2} + (-0.9)^{2}} = \\sqrt{0.0081 + 0.6561 + 0.81} = \\sqrt{1.4742} \\approx 1.214.\n$$\nRounded to four significant figures, the norm is $1.214$.", "answer": "$$\\boxed{1.214}$$", "id": "1393663"}, {"introduction": "The true power of iterative methods like Conjugate Gradient is realized when solving large-scale systems where explicitly storing the matrix $A$ is impractical or impossible. This advanced practice guides you through implementing a \"matrix-free\" solver, which interacts with the matrix only through its action on a vector ($v \\mapsto Av$). This exercise is fundamental to applying CG to real-world problems in fields like computational physics and engineering, where systems can involve millions of variables [@problem_id:3216688].", "problem": "You are asked to design and implement a matrix-free Conjugate Gradient (CG) solver for symmetric positive definite (SPD) linear systems that uses only a function implementing the matrix-vector product $v \\mapsto Av$, never explicitly forming the matrix $A$. Your solver must start from first principles: treat the solution of $Ax=b$ with SPD $A$ as minimizing a strictly convex quadratic objective whose gradient is the residual, and derive an algorithm that iteratively finds directions that are mutually conjugate with respect to $A$ and performs line searches along these directions. Your implementation must strictly use only products of the form $Ap$ provided by a callback. The initial guess must be the zero vector. The stopping rule must use a relative residual criterion based on the initial residual norm: stop at the first iteration $k$ where $\\lVert r_k \\rVert_2 \\le \\text{tol} \\cdot \\lVert r_0 \\rVert_2$, where $r_k = b - Ax_k$ and $r_0$ is the initial residual. If $\\lVert r_0 \\rVert_2 = 0$, the solver must return immediately with zero iterations. The solver must also respect a user-specified maximum number of iterations.\n\nYour program must apply your solver to the following four test cases, each described purely in terms of a matrix-vector product. In each test, also form a right-hand side $b = A x_{\\star}$ from a known exact solution $x_{\\star}$ so that you can measure the error of the computed solution. In all trigonometric expressions, use angles measured in radians.\n\nTest case $1$ (one-dimensional Poisson operator): Let $n = 10$. Define $A$ implicitly by the action on any $x \\in \\mathbb{R}^n$, yielding $y = Ax \\in \\mathbb{R}^n$ given componentwise by $y_i = 2 x_i - x_{i-1} - x_{i+1}$ with the convention that $x_0 = 0$ and $x_{n+1} = 0$. This corresponds to the standard tridiagonal operator with main diagonal $2$ and off-diagonals $-1$ (Dirichlet boundary conditions). Define the exact solution by $x_{\\star,i} = \\sin\\!\\left(\\pi i/(n+1)\\right)$ for $i = 1,2,\\dots,n$, and set $b = A x_{\\star}$. Use tolerance $\\text{tol} = 10^{-12}$ and maximum iterations $n$. Report the number of iterations taken and the maximum absolute error $\\max_i |x_i - x_{\\star,i}|$.\n\nTest case $2$ (zero right-hand side): Reuse the same operator $A$ and size $n = 10$ from test case $1$. Let $b$ be the zero vector in $\\mathbb{R}^n$ and let $x_{\\star}$ be the zero vector. Use tolerance $\\text{tol} = 10^{-12}$ and maximum iterations $n$. Report the number of iterations and the maximum absolute error.\n\nTest case $3$ (identity operator): Let $n = 50$. Define $A$ by $Ax = x$ for all $x \\in \\mathbb{R}^n$. Let the exact solution be $x_{\\star,i} = \\sin(i)$ for $i = 1,2,\\dots,n$ and set $b = A x_{\\star} = x_{\\star}$. Use tolerance $\\text{tol} = 10^{-12}$ and maximum iterations $n$. Report the number of iterations and the maximum absolute error.\n\nTest case $4$ (two-dimensional Poisson operator): Let $n_x = 20$ and $n_y = 20$ and define $N = n_x n_y$. Represent vectors $x \\in \\mathbb{R}^N$ as arrays $X \\in \\mathbb{R}^{n_y \\times n_x}$ in row-major order. Define $A$ implicitly by the five-point Laplacian with homogeneous Dirichlet boundary conditions: for interior indices $(i,j)$ with $1 \\le i \\le n_y$ and $1 \\le j \\le n_x$, define $(AX)_{i,j} = 4 X_{i,j} - X_{i-1,j} - X_{i+1,j} - X_{i,j-1} - X_{i,j+1}$ with the convention that any neighbor outside the grid contributes zero. Here, $(AX)$ is then flattened back to a vector in $\\mathbb{R}^N$. Take $x_{\\star}$ to be the all-ones vector in $\\mathbb{R}^N$ and set $b = A x_{\\star}$. Use tolerance $\\text{tol} = 10^{-8}$ and maximum iterations $N$. Report the number of iterations and the maximum absolute error.\n\nYour program must implement a single solver that handles all the above cases by receiving a callable that computes $v \\mapsto Av$, a right-hand side $b$, a tolerance, and a maximum number of iterations, and returns the approximate solution and the iteration count. For each test, compute the reported pair in the specified order: first the iteration count, then the maximum absolute error.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list corresponding to one test case in the order $1$ through $4$. For example: [[i1,e1],[i2,e2],[i3,e3],[i4,e4]], where $i\\ell$ is the iteration count and $e\\ell$ is the maximum absolute error for test case $\\ell$.", "solution": "The problem is valid. It requests the design and implementation of a matrix-free Conjugate Gradient (CG) solver, derived from first principles, and its application to four well-defined, scientifically sound test cases from numerical linear algebra. The problem statement is self-contained, consistent, and provides all necessary parameters and specifications.\n\nThe Conjugate Gradient method is an iterative algorithm for solving systems of linear equations $Ax=b$, where the matrix $A$ is symmetric and positive-definite (SPD). The problem specifies that the solver should be \"matrix-free,\" meaning it must not explicitly construct the matrix $A$. Instead, it should rely solely on a function that computes the matrix-vector product $v \\mapsto Av$ for any given vector $v$.\n\nThe solution to the system $Ax=b$ is equivalent to the unique minimizer of the strictly convex quadratic objective function:\n$$f(x) = \\frac{1}{2}x^\\top A x - b^\\top x$$\nThe gradient of this function, $\\nabla f(x) = Ax - b$, is the negative of the residual vector, $r = b - Ax$. At the solution $x_{\\star}$, the gradient is zero, which means $Ax_{\\star} - b = 0$.\n\nThe CG method generates a sequence of approximations $x_0, x_1, \\dots$ that converge to $x_{\\star}$. Starting with an initial guess $x_0$, each subsequent iterate is found by moving along a chosen search direction $p_k$:\n$$x_{k+1} = x_k + \\alpha_k p_k$$\nThe step size $\\alpha_k$ is chosen to minimize the objective function along the direction $p_k$. This is a one-dimensional minimization problem, solved by setting the derivative of $f(x_k + \\alpha p_k)$ with respect to $\\alpha$ to zero:\n$$\\frac{d}{d\\alpha} f(x_k + \\alpha p_k) = \\nabla f(x_{k+1})^\\top p_k = (A(x_k + \\alpha_k p_k) - b)^\\top p_k = (Ax_k - b + \\alpha_k Ap_k)^\\top p_k = (-r_k + \\alpha_k A p_k)^\\top p_k = 0$$\nSolving for $\\alpha_k$ yields the optimal step size:\n$$\\alpha_k = \\frac{r_k^\\top p_k}{p_k^\\top A p_k}$$\n\nThe core innovation of the CG method lies in the choice of search directions $p_k$. They are chosen to be mutually $A$-conjugate, meaning they satisfy the condition:\n$$p_i^\\top A p_j = 0 \\quad \\text{for} \\quad i \\neq j$$\nThis property ensures that the minimization performed in step $k$ along direction $p_k$ does not interfere with the minimization achieved in previous steps.\n\nThe search directions are constructed iteratively. The first search direction $p_0$ is taken as the initial residual $r_0 = b - Ax_0$, which is the direction of steepest descent from $x_0$. Each subsequent direction $p_{k+1}$ is constructed from the new residual $r_{k+1}$ by making it $A$-conjugate to the previous direction $p_k$. This is achieved by setting:\n$$p_{k+1} = r_{k+1} + \\beta_k p_k$$\nThe coefficient $\\beta_k$ is chosen to enforce $A$-conjugacy, $p_{k+1}^\\top A p_k = 0$. Substituting the expression for $p_{k+1}$ gives:\n$$(r_{k+1} + \\beta_k p_k)^\\top A p_k = r_{k+1}^\\top A p_k + \\beta_k p_k^\\top A p_k = 0 \\implies \\beta_k = -\\frac{r_{k+1}^\\top A p_k}{p_k^\\top A p_k}$$\nThrough algebraic manipulation involving the properties of the residuals and search directions, this expression for $\\beta_k$ and the one for $\\alpha_k$ can be simplified, leading to a highly efficient algorithm. A key property is that the residuals are mutually orthogonal, i.e., $r_i^\\top r_j = 0$ for $i \\ne j$. This leads to the relations $p_k^\\top r_k = r_k^\\top r_k$ and a computationally cheaper formula for $\\beta_k$.\n\nThe complete algorithm proceeds as follows, starting with the initial guess $x_0 = 0$ as specified:\n\n1.  Initialize:\n    $k = 0$\n    $x_0 = 0$\n    $r_0 = b - Ax_0 = b$\n    $p_0 = r_0$\n    $\\rho_0 = r_0^\\top r_0$\n\n2.  Check for trivial solution: If $\\lVert r_0 \\rVert_2 = \\sqrt{\\rho_0} = 0$, the solution is $x_0 = 0$. Terminate with $0$ iterations. Otherwise, compute the stopping tolerance threshold $\\tau = \\text{tol} \\cdot \\lVert r_0 \\rVert_2$.\n\n3.  Iterate for $k = 0, 1, 2, \\dots, \\text{max\\_iter}-1$:\n    a. Compute the matrix-vector product: $v_k = A p_k$.\n    b. Compute the step size: $\\alpha_k = \\frac{\\rho_k}{p_k^\\top v_k}$.\n    c. Update the solution: $x_{k+1} = x_k + \\alpha_k p_k$.\n    d. Update the residual: $r_{k+1} = r_k - \\alpha_k v_k$.\n    e. Compute the new residual norm squared: $\\rho_{k+1} = r_{k+1}^\\top r_{k+1}$.\n    f. Check for convergence: If $\\sqrt{\\rho_{k+1}} \\le \\tau$, terminate and return $x_{k+1}$ and iteration count $k+1$.\n    g. Update the search direction:\n       i. $\\beta_k = \\frac{\\rho_{k+1}}{\\rho_k}$\n       ii. $p_{k+1} = r_{k+1} + \\beta_k p_k$\n    h. Prepare for next iteration: $\\rho_k \\leftarrow \\rho_{k+1}$.\n\n4.  If the loop completes without convergence, return the current solution $x_{\\text{max\\_iter}}$ and the count $\\text{max\\_iter}$.\n\nThis algorithm requires only one matrix-vector product per iteration, making it ideal for matrix-free applications.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conjugate_gradient(matvec, b, tol, max_iter):\n    \"\"\"\n    Implements a matrix-free Conjugate Gradient (CG) solver.\n\n    Solves the symmetric positive-definite linear system Ax = b using the CG\n    method. The matrix A is implicitly provided via a function `matvec` that\n    computes the matrix-vector product Av.\n\n    Args:\n        matvec (callable): A function that takes a vector v and returns Av.\n        b (np.ndarray): The right-hand side vector of the linear system.\n        tol (float): The relative tolerance for the residual norm stopping criterion.\n        max_iter (int): The maximum number of iterations allowed.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The approximate solution vector x.\n            - int: The number of iterations performed.\n    \"\"\"\n    x = np.zeros_like(b, dtype=float)\n    r = b.copy()  # Since x_0 is zero, r_0 = b - A*0 = b\n    p = r.copy()\n\n    rs_old_sq = np.dot(r, r)\n    r0_norm = np.sqrt(rs_old_sq)\n\n    if r0_norm == 0.0:\n        return x, 0\n\n    stop_norm = tol * r0_norm\n\n    for i in range(max_iter):\n        Ap = matvec(p)\n        alpha = rs_old_sq / np.dot(p, Ap)\n\n        x += alpha * p\n        r -= alpha * Ap\n\n        rs_new_sq = np.dot(r, r)\n\n        if np.sqrt(rs_new_sq) <= stop_norm:\n            return x, i + 1\n\n        p = r + (rs_new_sq / rs_old_sq) * p\n        rs_old_sq = rs_new_sq\n\n    return x, max_iter\n\ndef solve():\n    \"\"\"\n    Sets up and runs the four test cases for the CG solver.\n    \"\"\"\n    # Test case 1: 1D Poisson operator\n    n1 = 10\n    def matvec1(x):\n        y = np.zeros_like(x)\n        y[0] = 2.0 * x[0] - x[1]\n        y[-1] = 2.0 * x[-1] - x[-2]\n        y[1:-1] = 2.0 * x[1:-1] - x[:-2] - x[2:]\n        return y\n    idx1 = np.arange(1, n1 + 1)\n    x_star1 = np.sin(np.pi * idx1 / (n1 + 1))\n    b1 = matvec1(x_star1)\n    tol1 = 1e-12\n    max_iter1 = n1\n\n    # Test case 2: Zero right-hand side\n    n2 = 10\n    def matvec2(x): # Same operator as case 1\n        y = np.zeros_like(x)\n        y[0] = 2.0 * x[0] - x[1]\n        y[-1] = 2.0 * x[-1] - x[-2]\n        y[1:-1] = 2.0 * x[1:-1] - x[:-2] - x[2:]\n        return y\n    x_star2 = np.zeros(n2)\n    b2 = np.zeros(n2)\n    tol2 = 1e-12\n    max_iter2 = n2\n\n    # Test case 3: Identity operator\n    n3 = 50\n    def matvec3(x):\n        return x\n    idx3 = np.arange(1, n3 + 1)\n    x_star3 = np.sin(idx3)\n    b3 = x_star3.copy()  # Since A is identity, b = x_star\n    tol3 = 1e-12\n    max_iter3 = n3\n\n    # Test case 4: 2D Poisson operator\n    nx4, ny4 = 20, 20\n    N4 = nx4 * ny4\n    def matvec4(x_flat):\n        X = x_flat.reshape((ny4, nx4))\n        Y = 4.0 * X\n        Y[1:, :] -= X[:-1, :]    # Subtract contribution from neighbor above\n        Y[:-1, :] -= X[1:, :]   # Subtract contribution from neighbor below\n        Y[:, 1:] -= X[:, :-1]   # Subtract contribution from neighbor left\n        Y[:, :-1] -= X[:, 1:]   # Subtract contribution from neighbor right\n        return Y.flatten()\n    x_star4 = np.ones(N4)\n    b4 = matvec4(x_star4)\n    tol4 = 1e-8\n    max_iter4 = N4\n\n    test_cases = [\n        (matvec1, b1, tol1, max_iter1, x_star1),\n        (matvec2, b2, tol2, max_iter2, x_star2),\n        (matvec3, b3, tol3, max_iter3, x_star3),\n        (matvec4, b4, tol4, max_iter4, x_star4),\n    ]\n\n    results = []\n    for matvec, b, tol, max_iter, x_star in test_cases:\n        x_sol, iters = conjugate_gradient(matvec, b, tol, max_iter)\n        error = np.max(np.abs(x_sol - x_star))\n        results.append([iters, error])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3216688"}]}