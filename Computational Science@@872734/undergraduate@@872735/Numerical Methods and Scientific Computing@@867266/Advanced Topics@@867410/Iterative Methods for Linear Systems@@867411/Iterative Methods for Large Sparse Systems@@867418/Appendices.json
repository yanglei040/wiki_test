{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we first address a fundamental question: why can't we always use a simple iterative method? This exercise provides a clear and concrete answer by contrasting a direct solver with the Jacobi iterative method. By working through a small system where the iterative approach fails to converge, you will gain an intuitive appreciation for the importance of convergence conditions, such as diagonal dominance, and understand the scenarios where robust direct methods remain indispensable [@problem_id:2160102].", "problem": "In numerical analysis, linear systems of the form $A\\mathbf{x} = \\mathbf{b}$ can be solved using either direct methods (like Gaussian elimination) or iterative methods (like the Jacobi method). While iterative methods can be very efficient for large, sparse systems, their convergence is not guaranteed for all matrices. A sufficient, but not necessary, condition for the Jacobi method to converge is that the matrix $A$ must be strictly diagonally dominant.\n\nConsider the following $2 \\times 2$ linear system, which is not strictly diagonally dominant:\n$$\n\\begin{pmatrix} 1  2 \\\\ 3  1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 7 \\end{pmatrix}\n$$\nAn attempt to solve this system using the Jacobi method with an initial guess $\\mathbf{x}^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ reveals that the iterates diverge rapidly. To obtain the correct solution, one must use a direct method.\n\nDetermine the exact solution vector $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ for this system. Express your answer as a row matrix $\\begin{pmatrix} x_1  x_2 \\end{pmatrix}$.", "solution": "We solve the linear system $A\\mathbf{x}=\\mathbf{b}$ with $A=\\begin{pmatrix}1  2 \\\\ 3  1\\end{pmatrix}$ and $\\mathbf{b}=\\begin{pmatrix}5 \\\\ 7\\end{pmatrix}$ by a direct method (Gaussian elimination).\n\nWrite the system as equations:\n$$\n\\begin{cases}\nx_{1}+2x_{2}=5, \\\\\n3x_{1}+x_{2}=7.\n\\end{cases}\n$$\nEliminate $x_{1}$ by multiplying the first equation by $3$ and subtracting the second equation:\n$$\n(3x_{1}+6x_{2})-(3x_{1}+x_{2})=15-7 \\;\\;\\Rightarrow\\;\\; 5x_{2}=8 \\;\\;\\Rightarrow\\;\\; x_{2}=\\frac{8}{5}.\n$$\nSubstitute $x_{2}=\\frac{8}{5}$ into the first equation:\n$$\nx_{1}+2\\left(\\frac{8}{5}\\right)=5 \\;\\;\\Rightarrow\\;\\; x_{1}+\\frac{16}{5}=5 \\;\\;\\Rightarrow\\;\\; x_{1}=5-\\frac{16}{5}=\\frac{9}{5}.\n$$\nTherefore, the exact solution vector is $\\begin{pmatrix}\\frac{9}{5} \\\\ \\frac{8}{5}\\end{pmatrix}$. For completeness, this agrees with the matrix inverse method: $\\det(A)=1\\cdot 1-2\\cdot 3=-5$, $\\operatorname{adj}(A)=\\begin{pmatrix}1  -2 \\\\ -3  1\\end{pmatrix}$, so\n$$\n\\mathbf{x}=A^{-1}\\mathbf{b}=\\frac{1}{-5}\\begin{pmatrix}1  -2 \\\\ -3  1\\end{pmatrix}\\begin{pmatrix}5 \\\\ 7\\end{pmatrix}=\\frac{1}{-5}\\begin{pmatrix}-9 \\\\ -8\\end{pmatrix}=\\begin{pmatrix}\\frac{9}{5} \\\\ \\frac{8}{5}\\end{pmatrix}.\n$$\nExpressed as a row matrix, this is $\\begin{pmatrix}\\frac{9}{5}  \\frac{8}{5}\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{9}{5}  \\frac{8}{5}\\end{pmatrix}}$$", "id": "2160102"}, {"introduction": "The true power of iterative methods shines when dealing with large, sparse systems that arise frequently in science and engineering. Storing these huge matrices densely is impossible, so we rely on efficient formats like Compressed Sparse Row (CSR). This practice guides you through the essential task of building a solver from the ground up, starting with the implementation of a matrix-vector product for the CSR format and embedding it within the celebrated Conjugate Gradient (CG) algorithm to solve a realistic problem [@problem_id:3244695].", "problem": "You are tasked with implementing an iterative solver for sparse linear systems that operates directly on a matrix stored in Compressed Sparse Row (CSR) format. The focus is on correctly implementing the matrix-vector product for the CSR structure, and then using it within an iterative method to solve a symmetric positive definite linear system. The goal is to solve systems of the form $A x = b$ where $A$ is a large sparse matrix, and to report the quality of the computed solution via residual norms.\n\nFundamental base for the derivation: Start from the definition of a linear system $A x = b$ on a finite-dimensional real vector space, assume $A$ is symmetric positive definite, and use Euclidean inner products and norms. Use the well-tested facts that minimizing a strictly convex quadratic functional yields the unique solution, and that Krylov subspace methods can be constructed to generate search directions that are conjugate with respect to $A$.\n\nYou must implement:\n- A representation of a sparse matrix in Compressed Sparse Row (CSR) format, defined by three arrays: `data`, `indices`, and `indptr`, where `indptr` demarcates rows, `indices` holds column indices for each nonzero, and `data` holds corresponding nonzero values.\n- A function to compute the matrix-vector product $y = A x$ using only the CSR representation, without converting $A$ to any dense structure or using any external sparse linear algebra library function.\n- An iterative solver that uses the matrix-vector product to solve $A x = b$ for $x$, with termination based on the relative residual norm criterion $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$, where $r_k = b - A x_k$, and $\\varepsilon$ is a prescribed tolerance. Use the Conjugate Gradient (CG) method, which is appropriate for symmetric positive definite matrices.\n\nConstraints:\n- You must not form $A$ as a dense array or use any dense matrix multiplication to compute $A x$. All multiplications must be performed via the CSR matrix-vector product that you implement.\n- Your implementation should be numerically stable for the provided test cases and should handle sparse patterns correctly.\n- Use an initial guess $x_0 = 0$ unless specified otherwise.\n- Use the Euclidean two-norm $\\|\\cdot\\|_2$ for residuals and the relative residual criterion $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$ for stopping.\n\nTest suite:\nImplement the following three test cases inside your program. For each case, compute the final relative residual norm $\\|r_k\\|_2 / \\|b\\|_2$ upon termination.\n\n- Test Case $1$ (Happy path, structured symmetric positive definite system): Let $A$ be the $5$-point finite difference discretization of the two-dimensional Laplacian on an interior grid of size $3 \\times 3$ with Dirichlet boundary conditions. For each interior point, set the diagonal entry to $4$ and the entries corresponding to its valid up, down, left, and right neighbors to $-1$. This yields an $n \\times n$ matrix with $n = 9$. Let $b$ be the $n$-vector with all entries equal to $1$. Use tolerance $\\varepsilon = 10^{-10}$ and maximum iterations $k_{\\max} = 1000$.\n\n- Test Case $2$ (Boundary condition, purely diagonal symmetric positive definite system): Let $A$ be diagonal with entries $[2, 3, 5, 7]$, so $n = 4$. Let $b = [1, 2, 3, 4]$. Use tolerance $\\varepsilon = 10^{-12}$ and maximum iterations $k_{\\max} = 100$.\n\n- Test Case $3$ (Ill-conditioned symmetric positive definite system): Let $A$ be diagonal with entries $[10^{-6}, 10^{-3}, 1, 10^{3}, 10^{6}]$, so $n = 5$. Let $b = [1, 1, 1, 1, 1]$. Use tolerance $\\varepsilon = 10^{-12}$ and maximum iterations $k_{\\max} = 200$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example, $[result_1,result_2,result_3]$, where each $result_i$ is the final relative residual norm for Test Case $i$. The outputs must be real numbers (floating-point values). No additional text should be printed.", "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in numerical linear algebra, requiring the implementation of the Conjugate Gradient algorithm for solving a sparse symmetric positive definite linear system, with the matrix stored in Compressed Sparse Row (CSR) format. All parameters and test cases are clearly defined and constitute a standard exercise in scientific computing.\n\nThe task is to solve the linear system of equations $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a sparse, symmetric, and positive definite (SPD) matrix, $x \\in \\mathbb{R}^n$ is the unknown vector, and $b \\in \\mathbb{R}^n$ is a given vector.\n\nThe solution to this system is equivalent to finding the unique minimizer of the quadratic functional $f(x) = \\frac{1}{2} x^T A x - x^T b$. The gradient of this functional is $\\nabla f(x) = A x - b$, which is zero at the solution. Since $A$ is SPD, the Hessian of $f(x)$, which is $A$, is positive definite, confirming that $f(x)$ is strictly convex and has a unique minimum.\n\nIterative methods, particularly Krylov subspace methods, are well-suited for large sparse systems. The Conjugate Gradient (CG) method is the canonical choice for SPD systems. It constructs a sequence of search directions $\\{p_k\\}$ that are conjugate with respect to $A$ (i.e., $p_i^T A p_j = 0$ for $i \\ne j$), which guarantees convergence to the exact solution in at most $n$ iterations in exact arithmetic.\n\nThe core of any iterative method for sparse systems is an efficient matrix-vector product. We first define the Compressed Sparse Row (CSR) format and the corresponding matrix-vector product.\n\nA sparse matrix $A$ is represented in CSR format by three arrays:\n1.  `data`: A real-valued array containing all the non-zero elements of $A$ in row-major order.\n2.  `indices`: An integer array storing the column index for each corresponding value in the `data` array.\n3.  `indptr` (index pointer): An integer array of size $n+1$. The non-zero elements of row $i$ are stored in `data` from index `indptr[i]` up to (but not including) index `indptr[i+1]`. The number of non-zero elements in row $i$ is thus `indptr[i+1] - indptr[i]`.\n\nThe matrix-vector product $y = A x$ is computed as follows. For each row $i$ from $0$ to $n-1$, the component $y_i$ is given by the dot product of the $i$-th row of $A$ with $x$. In CSR format, this is:\n$$ y_i = \\sum_{k=\\text{indptr}[i]}^{\\text{indptr}[i+1]-1} \\text{data}[k] \\cdot x_{\\text{indices}[k]} $$\nThis computation avoids storing or operating on the zero elements of $A$.\n\nThe Conjugate Gradient algorithm proceeds as follows:\n1.  Initialize:\n    -   Set an initial guess $x_0$. Per the problem, $x_0 = 0$.\n    -   Compute the initial residual: $r_0 = b - A x_0$. Since $x_0 = 0$, $r_0 = b$.\n    -   Set the initial search direction: $p_0 = r_0$.\n    -   Calculate the squared norm of the initial residual: $\\rho_0 = r_0^T r_0$.\n    -   Calculate the norm of $b$: $\\|b\\|_2$.\n2.  Iterate for $k = 0, 1, 2, \\dots$ until convergence or maximum iterations:\n    a.  Compute the matrix-vector product: $v_k = A p_k$.\n    b.  Calculate the step size: $\\alpha_k = \\frac{\\rho_k}{p_k^T v_k}$.\n    c.  Update the solution vector: $x_{k+1} = x_k + \\alpha_k p_k$.\n    d.  Update the residual: $r_{k+1} = r_k - \\alpha_k v_k$.\n    e.  Check for convergence: The relative residual norm is checked against a tolerance $\\varepsilon$. If $\\|r_{k+1}\\|_2 / \\|b\\|_2 \\le \\varepsilon$, the algorithm terminates.\n    f.  Calculate the squared norm of the new residual: $\\rho_{k+1} = r_{k+1}^T r_{k+1}$.\n    g.  Compute the improvement factor for the search direction: $\\beta_k = \\frac{\\rho_{k+1}}{\\rho_k}$.\n    h.  Update the search direction: $p_{k+1} = r_{k+1} + \\beta_k p_k$.\n    i.  Set $\\rho_k \\leftarrow \\rho_{k+1}$ for the next iteration.\n\nThis procedure will be applied to the three specified test cases.\n\nTest Case $1$: The matrix $A$ is the $9 \\times 9$ matrix from a $5$-point stencil discretization of the 2D Laplacian on a $3 \\times 3$ grid. A grid point $(i, j)$ with $i, j \\in \\{0, 1, 2\\}$ is mapped to a single index $k = 3i + j$. For each row $k$, the diagonal entry $A_{k,k}$ is $4$, and entries corresponding to valid North, South, East, and West neighbors are $-1$. The vector $b$ is a vector of all ones. This matrix is known to be SPD.\n\nTest Case $2$: The matrix $A$ is a diagonal $4 \\times 4$ matrix with all diagonal entries positive, $A = \\text{diag}(2, 3, 5, 7)$. This is trivially SPD. The vector $b$ is $[1, 2, 3, 4]^T$.\n\nTest Case $3$: The matrix $A$ is a diagonal $5 \\times 5$ matrix, $A = \\text{diag}(10^{-6}, 10^{-3}, 1, 10^3, 10^6)$. It is SPD, but its condition number $\\kappa_2(A) = \\frac{\\max(\\lambda_i)}{\\min(\\lambda_i)} = \\frac{10^6}{10^{-6}} = 10^{12}$ is very large. This tests the solver's performance on an ill-conditioned problem. The vector $b$ is a vector of all ones.\n\nThe implementation will construct these matrices in CSR format and apply the CG solver, reporting the final relative residual norm.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to set up and run the test cases for the Conjugate Gradient solver.\n    \"\"\"\n\n    def generate_laplacian_csr(grid_size):\n        \"\"\"\n        Generates the CSR representation of the 5-point Laplacian on a 2D grid.\n        \"\"\"\n        n = grid_size * grid_size\n        data = []\n        indices = []\n        indptr = [0]\n        \n        for k in range(n):\n            i, j = k // grid_size, k % grid_size\n            \n            # Temporary storage for one row's non-zero entries\n            row_entries = {}\n            \n            # Diagonal element\n            row_entries[k] = 4.0\n            \n            # Neighbor connections\n            # North\n            if i  0:\n                neighbor_idx = (i - 1) * grid_size + j\n                row_entries[neighbor_idx] = -1.0\n            # South\n            if i  grid_size - 1:\n                neighbor_idx = (i + 1) * grid_size + j\n                row_entries[neighbor_idx] = -1.0\n            # West\n            if j  0:\n                neighbor_idx = i * grid_size + (j - 1)\n                row_entries[neighbor_idx] = -1.0\n            # East\n            if j  grid_size - 1:\n                neighbor_idx = i * grid_size + (j + 1)\n                row_entries[neighbor_idx] = -1.0\n\n            # Sort by column index and append to CSR lists\n            sorted_cols = sorted(row_entries.keys())\n            for col in sorted_cols:\n                data.append(row_entries[col])\n                indices.append(col)\n            indptr.append(len(data))\n            \n        return np.array(data, dtype=float), np.array(indices, dtype=int), np.array(indptr, dtype=int), n\n\n    def csr_matvec(n, data, indices, indptr, x):\n        \"\"\"\n        Computes the matrix-vector product y = A*x for a matrix in CSR format.\n        \"\"\"\n        y = np.zeros(n, dtype=float)\n        for i in range(n):\n            row_sum = 0.0\n            start = indptr[i]\n            end = indptr[i+1]\n            for k in range(start, end):\n                j = indices[k]\n                val = data[k]\n                row_sum += val * x[j]\n            y[i] = row_sum\n        return y\n\n    def conjugate_gradient(n, data, indices, indptr, b, tol, max_iter):\n        \"\"\"\n        Solves A*x = b using the Conjugate Gradient method for a CSR matrix.\n        \"\"\"\n        x = np.zeros(n, dtype=float)\n        # For x_0 = 0, the initial residual r_0 is b.\n        r = b.copy()\n        p = r.copy()\n        rs_old = np.dot(r, r)\n        norm_b = np.linalg.norm(b)\n\n        if norm_b == 0.0:\n            return 0.0\n\n        rel_res = np.sqrt(rs_old) / norm_b\n        if rel_res  tol:\n            return rel_res\n\n        for k in range(max_iter):\n            Ap = csr_matvec(n, data, indices, indptr, p)\n            \n            p_dot_Ap = np.dot(p, Ap)\n            # If p_dot_Ap is zero or negative, the matrix might not be SPD\n            # or we might have found the exact solution.\n            if p_dot_Ap = 0:\n                break\n                \n            alpha = rs_old / p_dot_Ap\n            x += alpha * p\n            r -= alpha * Ap\n            \n            rs_new = np.dot(r, r)\n            rel_res = np.sqrt(rs_new) / norm_b\n\n            if rel_res  tol:\n                break\n            \n            p = r + (rs_new / rs_old) * p\n            rs_old = rs_new\n        \n        # After loop, compute final residual for verification\n        final_residual_norm = np.linalg.norm(b - csr_matvec(n, data, indices, indptr, x))\n        final_rel_res = final_residual_norm / norm_b if norm_b  0 else 0.0\n\n        return final_rel_res\n\n    # === Define the test cases from the problem statement. ===\n\n    # Test Case 1: 2D Laplacian on 3x3 grid\n    data1, indices1, indptr1, n1 = generate_laplacian_csr(grid_size=3)\n    b1 = np.ones(n1, dtype=float)\n    tol1 = 1e-10\n    max_iter1 = 1000\n\n    # Test Case 2: Diagonal system\n    n2 = 4\n    data2 = np.array([2.0, 3.0, 5.0, 7.0])\n    indices2 = np.array([0, 1, 2, 3], dtype=int)\n    indptr2 = np.array([0, 1, 2, 3, 4], dtype=int)\n    b2 = np.array([1.0, 2.0, 3.0, 4.0])\n    tol2 = 1e-12\n    max_iter2 = 100\n\n    # Test Case 3: Ill-conditioned diagonal system\n    n3 = 5\n    data3 = np.array([1e-6, 1e-3, 1.0, 1e3, 1e6])\n    indices3 = np.array([0, 1, 2, 3, 4], dtype=int)\n    indptr3 = np.array([0, 1, 2, 3, 4, 5], dtype=int)\n    b3 = np.ones(n3, dtype=float)\n    tol3 = 1e-12\n    max_iter3 = 200\n    \n    test_cases = [\n        (n1, data1, indices1, indptr1, b1, tol1, max_iter1),\n        (n2, data2, indices2, indptr2, b2, tol2, max_iter2),\n        (n3, data3, indices3, indptr3, b3, tol3, max_iter3),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, data, indices, indptr, b, tol, max_iter = case\n        final_rel_res = conjugate_gradient(n, data, indices, indptr, b, tol, max_iter)\n        results.append(final_rel_res)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3244695"}, {"introduction": "A masterful understanding of any tool includes knowing its limitations. The Conjugate Gradient method is designed for symmetric positive definite (SPD) systems, a property that guarantees its convergence. In this exercise, you will venture beyond this safe harbor and apply the CG algorithm to symmetric but indefinite matrices, where the underlying mathematical assumptions are violated. By implementing a check to detect the exact point of failure, you will directly observe how the theoretical requirement of positive definiteness translates into a tangible computational breakdown, deepening your understanding of the algorithm's mechanics [@problem_id:3244707].", "problem": "Consider the task of solving linear systems of the form $A x = b$ where $A \\in \\mathbb{R}^{n \\times n}$ is symmetric and $b \\in \\mathbb{R}^{n}$. The Conjugate Gradient (CG) method is traditionally applied to symmetric positive definite matrices. Its well-posedness is grounded in the minimization of the strictly convex quadratic functional $\\varphi(x) = \\tfrac{1}{2} x^{T} A x - b^{T} x$ when $A$ is symmetric positive definite. In that case, the search directions are chosen to be mutually $A$-conjugate and the step length along each search direction is selected to enforce orthogonality of the next residual to the current search direction. For symmetric but indefinite matrices, the quadratic functional is not strictly convex, and certain curvature quantities can become non-positive, rendering the usual CG step length undefined.\n\nYour task is to implement the unpreconditioned Conjugate Gradient (CG) iteration for symmetric matrices starting from first principles and to detect the precise iteration at which the method becomes ill-defined due to a non-positive curvature quantity. The implementation must:\n\n- Start from the zero initial guess $x_{0} = 0$ for each test case.\n- Use the standard CG update logic derived from minimizing the quadratic functional along $A$-conjugate directions for symmetric positive definite matrices.\n- At iteration $k$, before computing the step length, evaluate the curvature quantity $p_{k}^{T} A p_{k}$. If $p_{k}^{T} A p_{k} \\le 0$, the algorithm must immediately stop and report a failure at iteration $k$.\n- Use the Euclidean norm $\\|r_{k}\\|_{2}$ of the residual $r_{k} = b - A x_{k}$ as the stopping criterion for successful convergence.\n- Use the tolerance $\\varepsilon = 10^{-10}$ for the residual norm convergence test.\n- Use a maximum of $m = n$ iterations for a system of dimension $n$.\n\nReturn value specification per test case:\n\n- If the method converges successfully (that is, $\\|r_{k}\\|_{2} \\le \\varepsilon$ for some $k$ without encountering $p_{k}^{T} A p_{k} \\le 0$), return the positive integer equal to the number of completed iterations $k$ at the first satisfaction of the tolerance.\n- If the method encounters $p_{k}^{T} A p_{k} \\le 0$ at iteration $k$, return the negative integer $-(k+1)$, which uniquely encodes the zero-based iteration index of failure (this avoids ambiguity with $-0$).\n\nTest suite:\n\n- Test case $1$ (symmetric positive definite, expected successful convergence): \n  - $A_{1} = \\begin{bmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{bmatrix}$,\n  - $b_{1} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$,\n  - $x_{0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\varepsilon = 10^{-10}$, $m = 3$.\n- Test case $2$ (symmetric indefinite, division by zero curvature at the start):\n  - $A_{2} = \\begin{bmatrix} 1  0 \\\\ 0  -1 \\end{bmatrix}$,\n  - $b_{2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$,\n  - $x_{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\varepsilon = 10^{-10}$, $m = 2$.\n- Test case $3$ (symmetric indefinite, later negative curvature):\n  - $A_{3} = \\begin{bmatrix} 2  0 \\\\ 0  -1 \\end{bmatrix}$,\n  - $b_{3} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$,\n  - $x_{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\varepsilon = 10^{-10}$, $m = 2$.\n\nAngle units and physical units do not apply in this problem.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain exactly three integers corresponding to the three test cases, in order. For example, a valid output could be $[a,b,c]$ where $a$, $b$, and $c$ are integers as defined above for each test case.", "solution": "The user-provided problem is a well-posed and scientifically grounded exercise in numerical linear algebra, requiring the implementation of the Conjugate Gradient (CG) method with a specific modification to handle symmetric indefinite matrices. The problem is valid and can be solved as stated.\n\nThe problem asks for an implementation of the unpreconditioned Conjugate Gradient (CG) method to solve the linear system $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric matrix. The standard CG method is derived for symmetric positive definite (SPD) matrices, where it is guaranteed to converge. The method's foundation lies in the iterative minimization of the quadratic functional $\\varphi(x) = \\frac{1}{2} x^T A x - b^T x$. The gradient of this functional is $\\nabla\\varphi(x) = A x - b = -r(x)$, where $r(x)$ is the residual. For an SPD matrix $A$, $\\varphi(x)$ is a strictly convex paraboloid with a unique global minimum at the solution $x$ where $\\nabla\\varphi(x) = 0$.\n\nThe CG algorithm generates a sequence of iterates $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is the search direction and $\\alpha_k$ is the step length. The search directions are constructed to be $A$-conjugate (or $A$-orthogonal), meaning $p_i^T A p_j = 0$ for $i \\neq j$. The step length $\\alpha_k$ is chosen to minimize $\\varphi(x_{k+1})$ along the direction $p_k$, leading to the formula:\n$$\n\\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k}\n$$\nThe quantity $p_k^T A p_k$ in the denominator is the curvature of $\\varphi(x)$ in the direction $p_k$. For an SPD matrix $A$, this term is equivalent to the squared $A$-norm of the search direction, $\\|p_k\\|_A^2$, which is strictly greater than $0$ for any non-zero $p_k$. This positivity guarantees that $\\alpha_k$ is well-defined and positive, ensuring a descent step that reduces the value of $\\varphi(x)$.\n\nHowever, if $A$ is symmetric but indefinite, it may have non-positive eigenvalues. Consequently, the quadratic functional $\\varphi(x)$ is no longer convex and may have saddle points. The curvature term $p_k^T A p_k$ can become zero or negative during the iteration.\n\\begin{itemize}\n    \\item If $p_k^T A p_k = 0$, the step length $\\alpha_k$ is undefined, causing a division by zero.\n    \\item If $p_k^T A p_k  0$, the step length $\\alpha_k$ becomes negative. The update step moves in a direction of increasing $\\varphi(x)$, violating the minimization principle of CG.\n\\end{itemize}\nIn either case, the standard CG algorithm breaks down. The problem requires us to detect this failure precisely at the iteration $k$ where $p_k^T A p_k \\le 0$.\n\nThe algorithm to be implemented is as follows, starting with $x_0 = 0$:\n\n1.  Initialize:\n    $k = 0$\n    $x_0 = 0$\n    $r_0 = b - A x_0 = b$\n    $p_0 = r_0$\n    $\\rho_0 = r_0^T r_0$\n\n2.  Iterate for $k = 0, 1, 2, \\dots, m-1$ where $m=n$ is the maximum number of iterations:\n    a. Compute the matrix-vector product $v_k = A p_k$.\n    b. **Failure Detection:** Compute the curvature $\\gamma_k = p_k^T v_k$. If $\\gamma_k \\le 0$, the method has failed. Stop and return the value $-(k+1)$.\n    c. Compute the step length: $\\alpha_k = \\rho_k / \\gamma_k$.\n    d. Update the solution: $x_{k+1} = x_k + \\alpha_k p_k$.\n    e. Update the residual: $r_{k+1} = r_k - \\alpha_k v_k$.\n    f. **Convergence Check:** Compute the Euclidean norm of the new residual, $\\|r_{k+1}\\|_2$. If $\\|r_{k+1}\\|_2 \\le \\varepsilon = 10^{-10}$, the method has converged. Stop and return the number of completed iterations, $k+1$.\n    g. Prepare for the next iteration:\n        i.  Compute the squared norm of the new residual: $\\rho_{k+1} = r_{k+1}^T r_{k+1}$.\n        ii. Compute the coefficient for the search direction update: $\\beta_k = \\rho_{k+1} / \\rho_k$.\n        iii. Update the search direction: $p_{k+1} = r_{k+1} + \\beta_k p_k$.\n        iv. Update the squared residual norm for the next step: $\\rho_k \\leftarrow \\rho_{k+1}$.\n\nThis procedure will be applied to each of the three test cases provided. For the first case (SPD matrix), we expect successful convergence. For the second and third cases (indefinite matrices), we expect a failure due to non-positive curvature at a specific iteration. The return values are encoded as positive integers for success and negative integers for failure, as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing the Conjugate Gradient method\n    with a breakdown check for symmetric indefinite matrices.\n    \"\"\"\n\n    def conjugate_gradient_solver(A, b, tolerance, max_iterations):\n        \"\"\"\n        Implements the unpreconditioned Conjugate Gradient (CG) method.\n\n        Args:\n            A (np.ndarray): A symmetric n x n matrix.\n            b (np.ndarray): A vector of size n.\n            tolerance (float): The convergence tolerance for the residual norm.\n            max_iterations (int): The maximum number of iterations.\n\n        Returns:\n            int: A positive integer k for successful convergence in k iterations,\n                 or a negative integer -(k+1) for failure at zero-based iteration k.\n        \"\"\"\n        # Start with the zero initial guess x_0 = 0.\n        x = np.zeros_like(b, dtype=float)\n        \n        # Initial residual r_0 = b - A*x_0 = b.\n        r = b.copy()\n        \n        # Initial search direction p_0 = r_0.\n        p = r.copy()\n        \n        # Squared norm of the initial residual.\n        rs_old = np.dot(r, r)\n\n        # Check for immediate convergence (if b is close to zero vector).\n        if np.sqrt(rs_old) = tolerance:\n            # Per problem, return a positive integer. 0 is not positive.\n            # This case will not be triggered by the test suite.\n            # Returning 0 would conventionally mean 0 iterations.\n            return 0\n\n        # Main iteration loop.\n        for k in range(max_iterations):\n            # Compute the matrix-vector product A*p_k.\n            Ap = np.dot(A, p)\n            \n            # Evaluate the curvature quantity p_k^T * A * p_k.\n            p_T_Ap = np.dot(p, Ap)\n            \n            # If curvature is non-positive, CG breakdown occurs.\n            if p_T_Ap = 0:\n                return -(k + 1)\n            \n            # Calculate step length alpha_k.\n            alpha = rs_old / p_T_Ap\n            \n            # Update solution: x_{k+1} = x_k + alpha_k * p_k.\n            x += alpha * p\n            \n            # Update residual: r_{k+1} = r_k - alpha_k * A*p_k.\n            r -= alpha * Ap\n            \n            # Calculate the squared norm of the new residual.\n            rs_new = np.dot(r, r)\n            \n            # Check for convergence using the Euclidean norm of the new residual.\n            if np.sqrt(rs_new) = tolerance:\n                return k + 1\n            \n            # Update search direction: p_{k+1} = r_{k+1} + beta_k * p_k, where beta_k = rs_new / rs_old.\n            p = r + (rs_new / rs_old) * p\n            \n            # Update the squared residual norm for the next iteration.\n            rs_old = rs_new\n            \n        # If the method reaches max_iterations without converging, this is also a failure.\n        # This path should not be taken for the specified test cases.\n        # A conventional return for this state might be e.g., -(max_iterations + 1).\n        # We assume the problem constraints ensure one of the explicit conditions is met.\n        return -(max_iterations + 1) # Placeholder for non-convergence.\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"A\": np.array([[4, 1, 0], [1, 3, 1], [0, 1, 2]], dtype=float),\n            \"b\": np.array([1, 2, 3], dtype=float),\n        },\n        # Test case 2\n        {\n            \"A\": np.array([[1, 0], [0, -1]], dtype=float),\n            \"b\": np.array([1, 1], dtype=float),\n        },\n        # Test case 3\n        {\n            \"A\": np.array([[2, 0], [0, -1]], dtype=float),\n            \"b\": np.array([1, 1], dtype=float),\n        },\n    ]\n\n    tolerance = 1e-10\n    results = []\n\n    for case in test_cases:\n        A = case[\"A\"]\n        b = case[\"b\"]\n        n = A.shape[0]\n        max_iter = n\n        result = conjugate_gradient_solver(A, b, tolerance, max_iter)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3244707"}]}