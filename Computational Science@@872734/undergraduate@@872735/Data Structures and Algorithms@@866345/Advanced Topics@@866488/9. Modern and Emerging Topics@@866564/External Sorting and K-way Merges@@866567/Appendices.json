{"hands_on_practices": [{"introduction": "The efficiency of external sorting is fundamentally tied to minimizing disk I/O. This first practice provides a concrete scenario to analyze a powerful optimization: removing duplicate records on-the-fly during the merge passes. By calculating the exact number of saved disk block transfers, you will gain a quantitative understanding of how data reduction during the merging process directly translates into significant performance gains [@problem_id:3232889].", "problem": "You are given a large file of records stored on disk, consisting of exactly $D$ disk blocks, with each block fully utilized by fixed-length records. There is a main memory of $M$ block buffers (each buffer holds one disk block). An external mergesort is performed using the standard model where, in the initial run generation pass, contiguous chunks of size $M$ blocks are read into memory, sorted in memory, and written back as initial runs; then the algorithm performs a $k$-way merge pass repeatedly until a single fully sorted run remains, where $k = M - 1$ (one output buffer and one input buffer per run). Assume that all disk Input/Output (I/O) is sequential and block-aligned and that in-memory processing cost is negligible compared to disk I/O. Also assume that replacement selection is not used, so each initial run has exactly $M$ blocks.\n\nYou are asked to modify the merging phase so that duplicate records are removed on the fly. Specifically, during each $k$-way merge, the merger emits a record only if its key differs from the last key that was written to the output; ties across input streams are resolved by emitting a single representative and discarding all subsequent equal keys before they reach disk. Assume that equal keys are byte-identical and compare equal under the sort order. This modification does not change the number of input blocks read in any pass, but it can reduce the number of blocks written in the pass where duplicates are first eliminated, and it reduces both reads and writes in subsequent passes due to the smaller data volume.\n\nConsider the following concrete instance under these assumptions:\n- The number of memory buffers is $M = 101$, so $k = 100$.\n- The dataset occupies $D = 1010000$ disk blocks.\n- Initial runs are of length $M$ blocks, so the number of initial runs is $R_{0} = D / M$.\n- Because $R_{0}$ is an exact power of $k$, the baseline (no deduplication) sort completes in exactly two $k$-way merge passes after the initial run generation pass.\n- The dataset consists of records with exactly $r = 5$ duplicates per key, i.e., every distinct key appears exactly $5$ times in the dataset. Assume the deduplication during the first $k$-way merge pass removes all redundant copies so that only one representative per key is written. This reduces the data volume by a factor of $r$ in all subsequent materializations. Assume this reduction translates exactly into a factor of $r$ fewer blocks written or read (no padding or partial-block effects).\n\nCompute the exact number of disk block I/Os saved by performing this on-the-fly deduplication during merging, compared to the baseline external mergesort that performs the same sequence of passes but writes all records at every pass (i.e., no deduplication until after sorting). Count both reads and writes across all passes, including the initial run generation pass and both merge passes, and assume the final sorted output is written to disk.\n\nProvide your final answer as a single integer giving the total number of block I/Os saved. No rounding is necessary. Do not include any unit in your final answer.", "solution": "To compute the I/O savings, we compare the total block transfers in two scenarios: a baseline external sort and an external sort with on-the-fly deduplication. The savings will be the difference between these two costs.\n\nFirst, let's establish the parameters from the problem description:\n- Total initial disk blocks: $D = 1,010,000$\n- Memory buffers: $M = 101$ blocks\n- Merge fan-in: $k = M - 1 = 100$\n- Duplication factor: $r = 5$\n\nThe number of initial runs is $R_0 = D/M = 1,010,000 / 101 = 10,000$. Since $R_0 = 10,000 = 100^2 = k^2$, the sort requires exactly two merge passes after the initial run generation pass. The total process involves three passes over the data.\n\n**1. Baseline I/O Cost (No Deduplication)**\n\nEach pass in the baseline scenario reads and writes the entire dataset of size $D$.\n- **Pass 0 (Run Generation):** Reads $D$ blocks and writes $D$ blocks. Cost: $2D$.\n- **Pass 1 (First Merge):** Reads $D$ blocks from initial runs and writes $D$ blocks for intermediate runs. Cost: $2D$.\n- **Pass 2 (Final Merge):** Reads $D$ blocks from intermediate runs and writes the final sorted file of $D$ blocks. Cost: $2D$.\n\nThe total I/O cost for the baseline scenario is:\n$$I/O_{\\text{baseline}} = 2D + 2D + 2D = 6D$$\n\n**2. Deduplication I/O Cost**\n\nIn this scenario, duplicates are removed during the first merge pass.\n- **Pass 0 (Run Generation):** This pass is identical to the baseline. No merging or deduplication occurs. Cost: $2D$.\n- **Pass 1 (First Merge):** Reads $D$ blocks from the initial runs. During the merge, duplicates are removed, reducing the data volume by a factor of $r=5$. The output size is $D/r$. Cost: $D_{\\text{read}} + (D/r)_{\\text{write}} = D + D/r$.\n- **Pass 2 (Final Merge):** The input data for this pass has already been deduplicated and has a size of $D/r$. This pass reads $D/r$ blocks and writes the final sorted, unique file of size $D/r$. Cost: $D/r + D/r = 2(D/r)$.\n\nThe total I/O cost for the deduplication scenario is:\n$$I/O_{\\text{dedup}} = 2D + (D + D/r) + 2(D/r) = 3D + \\frac{3D}{r}$$\n\n**3. I/O Savings**\n\nThe total number of I/O block transfers saved is the difference between the two scenarios:\n$$I/O_{\\text{saved}} = I/O_{\\text{baseline}} - I/O_{\\text{dedup}}$$\n$$I/O_{\\text{saved}} = 6D - \\left(3D + \\frac{3D}{r}\\right) = 3D - \\frac{3D}{r} = 3D \\left(1 - \\frac{1}{r}\\right)$$\n\nSubstituting the given values:\n$$I/O_{\\text{saved}} = 3 \\cdot (1,010,000) \\left(1 - \\frac{1}{5}\\right) = 3 \\cdot (1,010,000) \\cdot \\frac{4}{5}$$\n$$I/O_{\\text{saved}} = \\frac{12,120,000}{5} = 2,424,000$$\n\nThe on-the-fly deduplication saves a total of 2,424,000 disk block I/Os.", "answer": "$$\\boxed{2424000}$$", "id": "3232889"}, {"introduction": "The best algorithm is often dictated by the characteristics of the underlying hardware. This exercise explores a classic scenario: sorting a massive dataset on a magnetic tape system, where sequential access is fast but rewinding is prohibitively expensive. You will compare different high-level merging strategies, such as polyphase and balanced merging, to determine which is best suited for this constrained environment, reinforcing the principle that algorithm design must account for the physical realities of the storage medium [@problem_id:3232903].", "problem": "A dataset of size $N = 120{,}000{,}000$ fixed-size records must be externally sorted on a sequential-access, helical-scan magnetic tape system. The system has $T = 4$ tape drives. Repositioning a tape to its beginning (a rewind) is extremely expensive relative to forward streaming. Main memory can hold $M = 5{,}000{,}000$ records at once. Assume the following fundamental facts:\n- A single forward streaming Input/Output (I/O) operation per record costs $C_{\\mathrm{seq}}$ time units.\n- A single rewind of any tape costs $C_{\\mathrm{rw}}$ time units, with $C_{\\mathrm{rw}}$ equal to the time to sequentially process $20{,}000{,}000$ records, i.e., $C_{\\mathrm{rw}} = 20{,}000{,}000 \\cdot C_{\\mathrm{seq}}$.\n- Replacement selection on a random input stream produces initial runs whose expected length is approximately $2M$ records.\n\nYou must choose a high-level external sorting strategy tailored to the medium’s access constraints. A valid external merge pass reads each record exactly once and writes each record exactly once, but the scheduling and the number of runs and passes vary across strategies. The goal is to minimize total time dominated by the cost of rewinds and sequential scans. Based on first principles of run formation, multiway merging, and tape role scheduling, which option yields the minimal total time on this system, and why?\n\nA. Use replacement selection to form initial runs, then perform a polyphase $k$-way merge across the $T = 4$ tapes with $k = T - 1 = 3$ inputs and $1$ output per pass, using a run distribution that keeps exactly one tape empty to receive output and rotates roles across passes. This yields approximately $R \\approx \\lceil N / (2M) \\rceil$ initial runs, about $3$ merge passes, and about $1$ rewind per pass (only the previous pass’s output tape is rewound), for a total of about $3$ rewinds.\n\nB. Use replacement selection to form initial runs, then perform repeated balanced two-way merging using $2$ tapes only, alternating one as input and one as output until a single run remains. This yields about $\\lceil \\log_{2} R \\rceil$ merge passes and requires rewinding both tapes after each pass, for a total of about $2 \\cdot \\lceil \\log_{2} R \\rceil$ rewinds.\n\nC. Use replacement selection to form initial runs, then perform balanced $3$-way merging using $3$ input tapes and $3$ output tapes, alternating the sets between passes. This yields about $\\lceil \\log_{3} R \\rceil$ merge passes but requires rewinding all $6$ tapes after each pass, for a total of about $6 \\cdot \\lceil \\log_{3} R \\rceil$ rewinds.\n\nD. Skip replacement selection. Instead, build a selection tree over the $M$ records in memory and repeatedly scan the single input tape forward without rewinds to pick the globally next smallest record, writing directly to an output tape, achieving one pass and zero rewinds.\n\nSelect the single best option.", "solution": "The problem asks for the optimal external sorting strategy for a tape-based system where tape rewinds are extremely expensive. The goal is to minimize the total time, which is the sum of sequential I/O time and rewind time. Given the high cost of rewinds ($C_{\\text{rw}} = 20,000,000 \\cdot C_{\\text{seq}}$), the primary objective is to minimize the number of rewinds.\n\nFirst, we calculate the number of initial sorted runs ($R$) created using replacement selection:\n- Memory capacity: $M = 5,000,000$ records.\n- Expected run length: $L \\approx 2M = 10,000,000$ records.\n- Total records: $N = 120,000,000$ records.\n- Number of initial runs: $R = \\lceil N/L \\rceil = \\lceil 120,000,000 / 10,000,000 \\rceil = 12$.\n\nNow, let's analyze each proposed strategy:\n\n**A. Polyphase Merge:**\nThis strategy uses $T=4$ tapes for a $k=T-1=3$-way merge. Polyphase merging is specifically designed to minimize rewinds on tape systems.\n- **Passes:** The number of merge passes is approximately $\\lceil \\log_{k} R \\rceil = \\lceil \\log_{3} 12 \\rceil = 3$. Including the initial run generation pass, this is about 4 total passes over the data.\n- **Rewinds:** The key advantage is that it avoids rewinding all tapes simultaneously. In each phase, only the tape that just received output needs to be rewound to become an input. The option's estimate of about 3 total rewinds for the merge phase is a reasonable high-level approximation of this benefit.\n- **Cost:** Low number of passes and a very low number of rewinds. This is a very strong candidate for a tape system.\n- **Verdict: Correct.** The strategy is well-suited for the hardware constraints.\n\n**B. Balanced Two-way Merge:**\nThis strategy describes a balanced merge, which on $T=4$ tapes typically performs a $2$-way merge.\n- **Passes:** The number of merge passes is $\\lceil \\log_{2} R \\rceil = \\lceil \\log_{2} 12 \\rceil = 4$. Total passes are $1+4=5$.\n- **Rewinds:** Balanced merging requires rewinding tapes after each pass to switch their roles from output to input. The option estimates $2 \\cdot 4 = 8$ rewinds for the merge phase.\n- **Cost:** This strategy has more passes and significantly more rewinds than the polyphase merge. With rewinds being extremely expensive, this is a much less efficient choice.\n- **Verdict: Incorrect.**\n\n**C. Balanced 3-way Merge (requiring 6 tapes):**\nThis strategy requires $3$ input tapes and $3$ output tapes, for a total of $6$ tape drives.\n- **Feasibility:** The system has only $T=4$ tape drives.\n- **Verdict: Incorrect.** The strategy is infeasible with the available hardware.\n\n**D. Single Pass Selection Tree:**\nThis strategy is based on a misunderstanding of external sorting. To find the globally next smallest record at each step would require scanning the entire remaining dataset.\n- **Feasibility:** This is not a valid sorting algorithm. It would have a prohibitive I/O cost on the order of $O(N^2)$. The claim of \"one pass\" is false.\n- **Verdict: Incorrect.**\n\n**Conclusion:**\nComparing the options, the polyphase merge (Option A) is the only strategy that is both feasible and efficiently designed for the given hardware constraints. It minimizes the number of costly rewind operations, making it the clear optimal choice.", "answer": "$$\\boxed{A}$$", "id": "3232903"}, {"introduction": "Moving beyond simple I/O counts, this final practice challenges you to model the real-world throughput of a modern external merge implementation. You will derive and implement a performance model for a $k$-way merge that uses asynchronous I/O and double buffering to overlap computation and data transfer. By analyzing the pipeline's bottleneck across different device types (SSD vs. HDD) and parameter sets, you will build a practical tool that predicts sustained throughput, bridging the gap between abstract algorithmic theory and concrete system performance [@problem_id:3232934].", "problem": "You are asked to model and implement the sustained throughput of an external k-way merge driven by multi-stream asynchronous Input/Output (I/O) with double buffering. The objective is to derive the steady-state throughput under two device classes, Solid-State Drive (SSD) and Hard Disk Drive (HDD), using a principled performance model. You must then write a complete, runnable program that computes the predicted throughput for a fixed set of test cases.\n\nAssumptions and context:\n- External sorting merges runs stored on secondary storage where the data far exceeds Random Access Memory (RAM) capacity. A k-way merge reads from $K$ sorted input streams and produces a single sorted output stream.\n- The algorithm uses two buffers per input stream and two buffers for the output stream (double buffering). While a buffer is being processed by the Central Processing Unit (CPU), the other buffer is filled or flushed by asynchronous I/O.\n- The merge uses a min-heap of size $K$ to choose the next output element, with per-element CPU work that grows logarithmically in $K$.\n- Each round processes exactly one per-stream input chunk of size $C$ bytes, for a total of $K \\cdot C$ bytes of output per round.\n- Element size is $s$ bytes and each comparison costs $t_{\\mathrm{comp}}$ seconds. Assume comparisons dominate the CPU time for merging.\n- I/O timing model for a single blocking request of size $X$ bytes on a single device with access latency $L$ seconds and streaming bandwidth $B$ bytes per second is defined by the fundamental device performance relation: the time for the request equals latency plus transfer time. The specific device-class assumptions per round are:\n  - SSD model: each input read request of size $C$ costs one access latency plus transfer time; there are $K$ such reads. The output write request of size $K \\cdot C$ costs one access latency plus transfer time.\n  - HDD model: each input read request of size $C$ incurs average seek and rotation latency plus transfer time; there are $K$ such reads. The output write request of size $K \\cdot C$ similarly incurs one latency plus transfer time. Treat the $K$ input reads as serialized at the device due to mechanical movement. Treat the input device and the output device as independent, so reading and writing can overlap.\n- Double buffering with asynchronous I/O yields a pipeline where reading the next round’s input, writing the current round’s output, and CPU merging of the current round overlap in steady state. The round time equals the time of the bottleneck stage in this pipeline, and the sustained throughput equals bytes per round divided by the round time.\n\nYour tasks:\n- Derive from first principles the steady-state round time using the above assumptions. Start only from the definitions above and the fundamental device performance relation that a single blocking I/O of size $X$ takes latency plus transfer time.\n- Implement a program that computes the sustained throughput in mebibytes per second (MiB/s), where one mebibyte equals $2^{20}$ bytes, rounded to three decimal places. Use double buffering overlap as described for steady-state throughput.\n- For heap-based merging cost, use binary logarithm. You may assume the number of comparisons per output element is proportional to $\\log_{2}(K)$.\n\nInput and output specification:\n- There is no runtime input. Your program must embed and evaluate the following test suite of parameter sets. For each test case, compute the sustained throughput as a single floating-point number in MiB/s rounded to three decimals. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[x,y,z]\").\n\nTest suite:\nEach test case is a tuple of the form (device-type, $K$, $C$, $s$, $t_{\\mathrm{comp}}$, $B_{\\mathrm{in}}$, $B_{\\mathrm{out}}$, $L_{\\mathrm{in}}$, $L_{\\mathrm{out}}$), where:\n- device-type is either \"SSD\" or \"HDD\"; it determines the latency interpretation but uses the same computation structure,\n- $K$ is the number of input streams,\n- $C$ is the per-stream chunk size in bytes,\n- $s$ is the element size in bytes,\n- $t_{\\mathrm{comp}}$ is the time per comparison in seconds,\n- $B_{\\mathrm{in}}$ and $B_{\\mathrm{out}}$ are input and output bandwidths in bytes per second,\n- $L_{\\mathrm{in}}$ and $L_{\\mathrm{out}}$ are input and output access latencies in seconds.\n\nUse the following seven cases:\n- Case A (SSD, general throughput): (\"SSD\", $8$, $4\\,000\\,000$, $8$, $1 \\times 10^{-9}$, $1.5 \\times 10^{9}$, $1.5 \\times 10^{9}$, $5.0 \\times 10^{-5}$, $5.0 \\times 10^{-5}$).\n- Case B (HDD, seek-dominated): (\"HDD\", $8$, $4\\,000\\,000$, $8$, $1 \\times 10^{-9}$, $1.5 \\times 10^{8}$, $1.5 \\times 10^{8}$, $1.0 \\times 10^{-2}$, $1.0 \\times 10^{-2}$).\n- Case C (SSD, compute-bound): (\"SSD\", $8$, $4\\,000\\,000$, $8$, $5.0 \\times 10^{-8}$, $1.5 \\times 10^{9}$, $1.5 \\times 10^{9}$, $5.0 \\times 10^{-5}$, $5.0 \\times 10^{-5}$).\n- Case D (SSD, latency-dominated small chunks): (\"SSD\", $8$, $4\\,096$, $8$, $1 \\times 10^{-9}$, $1.5 \\times 10^{9}$, $1.5 \\times 10^{9}$, $5.0 \\times 10^{-5}$, $5.0 \\times 10^{-5}$).\n- Case E (SSD, large $K$): (\"SSD\", $64$, $4\\,000\\,000$, $8$, $1 \\times 10^{-9}$, $1.5 \\times 10^{9}$, $1.5 \\times 10^{9}$, $5.0 \\times 10^{-5}$, $5.0 \\times 10^{-5}$).\n- Case F (HDD, $K = 1$ boundary): (\"HDD\", $1$, $4\\,000\\,000$, $8$, $1 \\times 10^{-9}$, $1.5 \\times 10^{8}$, $1.5 \\times 10^{8}$, $1.0 \\times 10^{-2}$, $1.0 \\times 10^{-2}$).\n- Case G (SSD, write-bandwidth bottleneck): (\"SSD\", $8$, $4\\,000\\,000$, $8$, $1 \\times 10^{-9}$, $1.5 \\times 10^{9}$, $3.0 \\times 10^{8}$, $5.0 \\times 10^{-5}$, $5.0 \\times 10^{-5}$).\n\nOutput unit and format:\n- Express each throughput in mebibytes per second (MiB/s), rounded to three decimal places.\n- Your program must print exactly one line in the format \"[v1,v2,v3,v4,v5,v6,v7]\" where each $v_{i}$ is the rounded throughput for the corresponding case in the same order as above.", "solution": "The objective is to derive the steady-state throughput of a $K$-way merge algorithm using double buffering and asynchronous I/O. The system's performance is modeled as a three-stage pipeline. In steady state, the time per round is dictated by the slowest stage (the bottleneck), and the throughput is the amount of data processed per round divided by this round time.\n\nA single round of the merge process involves reading one chunk of data of size $C$ from each of the $K$ input streams and merging them to produce one output chunk of size $K \\cdot C$. The total amount of data processed per round is therefore $N_{\\text{bytes\\_round}} = K \\cdot C$.\n\nThe pipeline consists of three stages that operate in parallel on different rounds of data:\n$1$. **CPU Merge**: The CPU merges the input chunks from round $i$ to generate the output for round $i$.\n$2$. **Input Read**: The I/O system reads the $K$ input chunks for the subsequent round, $i+1$.\n$3$. **Output Write**: The I/O system writes the merged output chunk from the previous round, $i-1$. In steady state, we consider this the output of the current round $i$.\n\nThe time to complete one round in steady state, $T_{\\text{round}}$, is the maximum of the time taken by each of these three stages.\n$$T_{\\text{round}} = \\max(T_{\\text{CPU}}, T_{\\text{Read}}, T_{\\text{Write}})$$\n\nWe now derive the expression for the time taken by each stage.\n\n**1. CPU Time ($T_{\\text{CPU}}$)**\nThe CPU time is dominated by the element comparisons required for the heap-based merge. A min-heap of size $K$ is used to find the next smallest element among all input streams.\n- The number of elements processed in one round is $N_{\\text{elem}} = \\frac{N_{\\text{bytes\\_round}}}{s} = \\frac{K \\cdot C}{s}$, where $s$ is the size of each element in bytes.\n- The problem states that the number of comparisons per output element is proportional to $\\log_{2}(K)$. We assume a standard binary heap implementation where replacing the minimum element requires a number of comparisons on the order of $\\log_{2}(K)$. We take the proportionality constant to be $1$, so the number of comparisons per element is $\\log_{2}(K)$.\n- The time to perform one comparison is given as $t_{\\text{comp}}$.\n- Therefore, the total CPU time for merging all elements in one round is:\n$$T_{\\text{CPU}} = N_{\\text{elem}} \\cdot (\\text{comparisons per element}) \\cdot t_{\\text{comp}} = \\left(\\frac{K \\cdot C}{s}\\right) \\cdot \\log_{2}(K) \\cdot t_{\\text{comp}}$$\nFor the special case where $K=1$, no merging is required, and $\\log_{2}(1) = 0$, which correctly yields $T_{\\text{CPU}} = 0$.\n\n**2. Input Read Time ($T_{\\text{Read}}$)**\nThis is the time required to read the $K$ input chunks for the next round. The problem specifies that this involves $K$ distinct read requests, each of size $C$. The fundamental I/O performance relation for a single request of size $X$ is given as $T_{\\text{I/O}}(X) = L + \\frac{X}{B}$, where $L$ is latency and $B$ is bandwidth.\n- For both SSD and HDD models, the problem implies that the $K$ input reads are effectively serialized from the perspective of total time calculation. For HDD, this is explicit due to mechanical head movement. For SSD, the instruction \"each input read request of size $C$ costs one access latency plus transfer time\" leads to an additive model.\n- Time for a single input read of size $C$: $L_{\\text{in}} + \\frac{C}{B_{\\text{in}}}$.\n- Total time to read all $K$ chunks for one round:\n$$T_{\\text{Read}} = K \\cdot \\left(L_{\\text{in}} + \\frac{C}{B_{\\text{in}}}\\right)$$\n\n**3. Output Write Time ($T_{\\text{Write}}$)**\nThis is the time to write the single, consolidated output chunk of size $K \\cdot C$.\n- The total size of the write is $X = K \\cdot C$.\n- The write is performed as a single I/O request.\n- Using the fundamental I/O relation, the total write time is:\n$$T_{\\text{Write}} = L_{\\text{out}} + \\frac{K \\cdot C}{B_{\\text{out}}}$$\n\n**4. Sustained Throughput ($\\Theta$)**\nThe sustained throughput is the total data processed per round divided by the round time.\n- Throughput in bytes per second:\n$$\\Theta_{\\text{B/s}} = \\frac{N_{\\text{bytes\\_round}}}{T_{\\text{round}}} = \\frac{K \\cdot C}{\\max\\left(\\frac{K \\cdot C \\cdot t_{\\text{comp}} \\cdot \\log_{2}(K)}{s}, K \\cdot \\left(L_{\\text{in}} + \\frac{C}{B_{\\text{in}}}\\right), L_{\\text{out}} + \\frac{K \\cdot C}{B_{\\text{out}}}\\right)}$$\n- The problem requires the result in mebibytes per second (MiB/s), where $1 \\text{ MiB} = 2^{20}$ bytes.\n$$\\Theta_{\\text{MiB/s}} = \\frac{\\Theta_{\\text{B/s}}}{2^{20}}$$\n\nThis model applies to both SSD and HDD device types, with the performance differences captured by the specific values of the parameters $L_{\\text{in}}$, $L_{\\text{out}}$, $B_{\\text{in}}$, and $B_{\\text{out}}$. The derived formulas will now be implemented to compute the throughputs for the given test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the sustained throughput for a k-way merge based on a\n    pipelined performance model.\n    \"\"\"\n\n    # Test suite of parameter sets. Each tuple is of the form:\n    # (device-type, K, C, s, t_comp, B_in, B_out, L_in, L_out)\n    test_cases = [\n        # Case A (SSD, general throughput)\n        (\"SSD\", 8, 4_000_000, 8, 1e-9, 1.5e9, 1.5e9, 5.0e-5, 5.0e-5),\n        # Case B (HDD, seek-dominated)\n        (\"HDD\", 8, 4_000_000, 8, 1e-9, 1.5e8, 1.5e8, 1.0e-2, 1.0e-2),\n        # Case C (SSD, compute-bound)\n        (\"SSD\", 8, 4_000_000, 8, 5.0e-8, 1.5e9, 1.5e9, 5.0e-5, 5.0e-5),\n        # Case D (SSD, latency-dominated small chunks)\n        (\"SSD\", 8, 4096, 8, 1e-9, 1.5e9, 1.5e9, 5.0e-5, 5.0e-5),\n        # Case E (SSD, large K)\n        (\"SSD\", 64, 4_000_000, 8, 1e-9, 1.5e9, 1.5e9, 5.0e-5, 5.0e-5),\n        # Case F (HDD, K = 1 boundary)\n        (\"HDD\", 1, 4_000_000, 8, 1e-9, 1.5e8, 1.5e8, 1.0e-2, 1.0e-2),\n        # Case G (SSD, write-bandwidth bottleneck)\n        (\"SSD\", 8, 4_000_000, 8, 1e-9, 1.5e9, 3.0e8, 5.0e-5, 5.0e-5),\n    ]\n\n    results = []\n    \n    # Conversion factor from bytes to mebibytes\n    BYTES_PER_MIB = 2**20\n\n    for case in test_cases:\n        _device_type, K, C, s, t_comp, B_in, B_out, L_in, L_out = case\n\n        # Total bytes processed per round\n        bytes_per_round = K * C\n\n        # 1. Calculate T_CPU: Time for CPU merge stage\n        if K > 1:\n            log2_K = np.log2(K)\n            T_cpu = (K * C * t_comp * log2_K) / s\n        else:\n            # For K=1, no comparisons are needed.\n            T_cpu = 0.0\n\n        # 2. Calculate T_Read: Time for input read stage\n        # This models K serialized read requests of size C.\n        T_read = K * (L_in + C / B_in)\n\n        # 3. Calculate T_Write: Time for output write stage\n        # This models a single write request of size K*C.\n        T_write = L_out + (K * C) / B_out\n\n        # Determine the round time (bottleneck of the pipeline)\n        T_round = max(T_cpu, T_read, T_write)\n\n        # Calculate throughput in bytes per second\n        if T_round > 0:\n            throughput_bps = bytes_per_round / T_round\n        else:\n            throughput_bps = float('inf')\n\n        # Convert throughput to MiB/s and round to 3 decimal places\n        throughput_mibs = throughput_bps / BYTES_PER_MIB\n        rounded_throughput = round(throughput_mibs, 3)\n        \n        results.append(rounded_throughput)\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3232934"}]}