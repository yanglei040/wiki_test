{"hands_on_practices": [{"introduction": "Many algorithms operate iteratively, where the total cost is a sum of costs incurred at each step. This practice challenges you to compare two such algorithms by analyzing their total costs, which are expressed as summations. By using integral calculus to approximate these sums, you can precisely determine their asymptotic growth and understand which algorithm is more efficient in the long run [@problem_id:3222276].", "problem": "Consider two algorithms, Algorithm $A$ and Algorithm $B$, that process an input of size $n$ using iterative refinements. At iteration $i$, Algorithm $A$ incurs an incremental cost equal to the natural logarithm $\\ln i$, while Algorithm $B$ incurs an incremental cost equal to the square root $\\sqrt{i}$. The total costs satisfy the recurrences\n$$T_{A}(1) = 0,\\quad T_{A}(n) = T_{A}(n-1) + \\ln n,$$\n$$T_{B}(1) = 0,\\quad T_{B}(n) = T_{B}(n-1) + \\sqrt{n},$$\nfor all integers $n \\geq 2$. Starting from core definitions for summations and properties of the natural logarithm and using first-principles approximations justified by monotonicity and integration, determine the exact value of the limit\n$$L \\;=\\; \\lim_{n \\to \\infty} \\frac{T_{B}(n) - T_{A}(n)}{n^{3/2}}.$$\nYour final answer must be a single real number. No rounding is required and no physical units are involved.", "solution": "The problem as stated is well-posed, mathematically sound, and self-contained. It is free of scientific inaccuracies, contradictions, and ambiguities. Therefore, a rigorous solution can be derived.\n\nThe total costs $T_{A}(n)$ and $T_{B}(n)$ are defined by the recurrence relations:\n$$T_{A}(1) = 0, \\quad T_{A}(n) = T_{A}(n-1) + \\ln n \\quad \\text{for } n \\geq 2$$\n$$T_{B}(1) = 0, \\quad T_{B}(n) = T_{B}(n-1) + \\sqrt{n} \\quad \\text{for } n \\geq 2$$\nBy unrolling these recurrences, we can express $T_{A}(n)$ and $T_{B}(n)$ as summations:\n$$T_{A}(n) = T_{A}(1) + \\sum_{k=2}^{n} \\ln k = 0 + \\sum_{k=2}^{n} \\ln k = \\sum_{k=2}^{n} \\ln k$$\n$$T_{B}(n) = T_{B}(1) + \\sum_{k=2}^{n} \\sqrt{k} = 0 + \\sum_{k=2}^{n} \\sqrt{k} = \\sum_{k=2}^{n} \\sqrt{k}$$\nThe problem requires computing the limit:\n$$L = \\lim_{n \\to \\infty} \\frac{T_{B}(n) - T_{A}(n)}{n^{3/2}}$$\nWe can split the limit if the individual limits exist:\n$$L = \\lim_{n \\to \\infty} \\left( \\frac{T_{B}(n)}{n^{3/2}} - \\frac{T_{A}(n)}{n^{3/2}} \\right) = \\left( \\lim_{n \\to \\infty} \\frac{T_{B}(n)}{n^{3/2}} \\right) - \\left( \\lim_{n \\to \\infty} \\frac{T_{A}(n)}{n^{3/2}} \\right)$$\nWe will evaluate each limit separately using integral bounds, as suggested by the problem's requirement for a first-principles approach based on monotonicity and integration.\n\nFirst, let us analyze the term involving $T_{A}(n)$.\nThe function $f(x) = \\ln x$ is continuous and monotonically increasing for $x > 0$. We can therefore bound the sum $T_{A}(n) = \\sum_{k=2}^{n} \\ln k$ using definite integrals.\nFor an increasing function $f(x)$, the following inequalities hold:\n$$\\int_{a-1}^{b} f(x) \\, dx \\leq \\sum_{k=a}^{b} f(k) \\leq \\int_{a}^{b+1} f(x) \\, dx$$\nFor $T_{A}(n)$, we have $a=2$ and $b=n$.\nThe lower bound is:\n$$T_{A}(n) \\geq \\int_{2-1}^{n} \\ln x \\, dx = \\int_{1}^{n} \\ln x \\, dx$$\nThe integral of $\\ln x$ is found using integration by parts: $\\int \\ln x \\, dx = x \\ln x - x + C$.\n$$T_{A}(n) \\geq [x \\ln x - x]_{1}^{n} = (n \\ln n - n) - (1 \\ln 1 - 1) = n \\ln n - n + 1$$\nThe upper bound is:\n$$T_{A}(n) \\leq \\int_{2}^{n+1} \\ln x \\, dx = [x \\ln x - x]_{2}^{n+1} = ((n+1)\\ln(n+1) - (n+1)) - (2\\ln 2 - 2)$$\nSo we have the bounds for $T_{A}(n)$:\n$$n \\ln n - n + 1 \\leq T_{A}(n) \\leq (n+1)\\ln(n+1) - n + 1 - 2\\ln 2$$\nNow, we can find the limit of $\\frac{T_{A}(n)}{n^{3/2}}$ by applying the Squeeze Theorem.\n$$\\frac{n \\ln n - n + 1}{n^{3/2}} \\leq \\frac{T_{A}(n)}{n^{3/2}} \\leq \\frac{(n+1)\\ln(n+1) - n + 1 - 2\\ln 2}{n^{3/2}}$$\nLet's evaluate the limit of the lower bound:\n$$\\lim_{n \\to \\infty} \\frac{n \\ln n - n + 1}{n^{3/2}} = \\lim_{n \\to \\infty} \\left( \\frac{\\ln n}{n^{1/2}} - \\frac{1}{n^{1/2}} + \\frac{1}{n^{3/2}} \\right)$$\nThe term $\\lim_{n \\to \\infty} \\frac{\\ln n}{n^{1/2}}$ can be evaluated using L'Hôpital's Rule:\n$$\\lim_{n \\to \\infty} \\frac{\\ln n}{n^{1/2}} = \\lim_{n \\to \\infty} \\frac{1/n}{\\frac{1}{2}n^{-1/2}} = \\lim_{n \\to \\infty} \\frac{2}{n^{1/2}} = 0$$\nThus, the limit of the lower bound is $0 - 0 + 0 = 0$.\nThe limit of the upper bound is asymptotically equivalent to $\\lim_{n \\to \\infty} \\frac{n \\ln n}{n^{3/2}} = \\lim_{n \\to \\infty} \\frac{\\ln n}{n^{1/2}} = 0$.\nBy the Squeeze Theorem, we conclude:\n$$\\lim_{n \\to \\infty} \\frac{T_{A}(n)}{n^{3/2}} = 0$$\n\nNext, we analyze the term involving $T_{B}(n)$.\nThe function $g(x) = \\sqrt{x}$ is continuous and monotonically increasing for $x \\geq 0$. We bound the sum $T_{B}(n) = \\sum_{k=2}^{n} \\sqrt{k}$ using the same integral inequality.\nThe lower bound for $T_B(n)$ is:\n$$T_{B}(n) \\geq \\int_{2-1}^{n} \\sqrt{x} \\, dx = \\int_{1}^{n} x^{1/2} \\, dx$$\nThe integral is:\n$$T_{B}(n) \\geq \\left[ \\frac{2}{3}x^{3/2} \\right]_{1}^{n} = \\frac{2}{3}n^{3/2} - \\frac{2}{3}$$\nThe upper bound is:\n$$T_{B}(n) \\leq \\int_{2}^{n+1} \\sqrt{x} \\, dx = \\left[ \\frac{2}{3}x^{3/2} \\right]_{2}^{n+1} = \\frac{2}{3}(n+1)^{3/2} - \\frac{2}{3}(2)^{3/2} = \\frac{2}{3}(n+1)^{3/2} - \\frac{4\\sqrt{2}}{3}$$\nSo we have the bounds for $T_{B}(n)$:\n$$\\frac{2}{3}n^{3/2} - \\frac{2}{3} \\leq T_{B}(n) \\leq \\frac{2}{3}(n+1)^{3/2} - \\frac{4\\sqrt{2}}{3}$$\nApplying the Squeeze Theorem to $\\frac{T_{B}(n)}{n^{3/2}}$:\n$$\\frac{\\frac{2}{3}n^{3/2} - \\frac{2}{3}}{n^{3/2}} \\leq \\frac{T_{B}(n)}{n^{3/2}} \\leq \\frac{\\frac{2}{3}(n+1)^{3/2} - \\frac{4\\sqrt{2}}{3}}{n^{3/2}}$$\n$$\\frac{2}{3} - \\frac{2}{3n^{3/2}} \\leq \\frac{T_{B}(n)}{n^{3/2}} \\leq \\frac{2}{3}\\left(\\frac{n+1}{n}\\right)^{3/2} - \\frac{4\\sqrt{2}}{3n^{3/2}}$$\nTaking the limit as $n \\to \\infty$:\nThe limit of the lower bound is $\\lim_{n \\to \\infty} \\left(\\frac{2}{3} - \\frac{2}{3n^{3/2}}\\right) = \\frac{2}{3}$.\nThe limit of the upper bound is $\\lim_{n \\to \\infty} \\left(\\frac{2}{3}\\left(1+\\frac{1}{n}\\right)^{3/2} - \\frac{4\\sqrt{2}}{3n^{3/2}}\\right) = \\frac{2}{3}(1)^{3/2} - 0 = \\frac{2}{3}$.\nBy the Squeeze Theorem, we conclude:\n$$\\lim_{n \\to \\infty} \\frac{T_{B}(n)}{n^{3/2}} = \\frac{2}{3}$$\n\nFinally, we substitute these results back into the expression for $L$:\n$$L = \\left( \\lim_{n \\to \\infty} \\frac{T_{B}(n)}{n^{3/2}} \\right) - \\left( \\lim_{n \\to \\infty} \\frac{T_{A}(n)}{n^{3/2}} \\right) = \\frac{2}{3} - 0 = \\frac{2}{3}$$\nThe exact value of the limit is $\\frac{2}{3}$.", "answer": "$$ \\boxed{\\frac{2}{3}} $$", "id": "3222276"}, {"introduction": "Divide-and-conquer is a cornerstone of algorithm design, but comparing two such algorithms requires a careful analysis of their recurrence relations. This exercise presents two similar-looking recurrences that lead to surprisingly different growth rates. Solving this problem will sharpen your skills in unrolling recurrences and highlight how the work done at each recursive level dictates the algorithm's overall efficiency [@problem_id:3222269].", "problem": "Two divide-and-conquer procedures on arrays of size $n$ have running times defined for $n=2^{k}$, where $k \\in \\mathbb{Z}_{\\ge 0}$, by the recurrences\n$$T_{1}(n)=2\\,T_{1}\\!\\left(\\frac{n}{2}\\right)+\\frac{n}{\\ln n}\\quad\\text{and}\\quad T_{2}(n)=2\\,T_{2}\\!\\left(\\frac{n}{2}\\right)+n,$$\nfor all $n \\ge 2$, with base conditions $T_{1}(1)=1$ and $T_{2}(1)=1$. Assume $T_{1}(n)$ and $T_{2}(n)$ are defined only on powers of $2$. Using only fundamental definitions from asymptotic analysis and well-tested facts about divide-and-conquer recurrences, determine the exact value of the limit\n$$L=\\lim_{n \\to \\infty}\\frac{T_{1}(n)}{T_{2}(n)}.$$\nYour final answer must be a single real number. No rounding is required.", "solution": "The problem as stated is a well-posed question in the analysis of algorithms concerning the asymptotic comparison of two functions defined by recurrence relations. All provided information—the recurrences, base conditions, and the domain of definition ($n=2^k$)—is self-contained, consistent, and scientifically grounded in standard computer science and mathematical principles. The problem does not violate any of the validation criteria. Therefore, it is deemed valid, and a solution can be derived.\n\nThe task is to evaluate the limit $L=\\lim_{n \\to \\infty}\\frac{T_{1}(n)}{T_{2}(n)}$, where $T_1(n)$ and $T_2(n)$ are defined for $n=2^k$ ($k \\in \\mathbb{Z}_{\\ge 0}$) by the recurrences:\n$$T_{1}(n)=2\\,T_{1}\\!\\left(\\frac{n}{2}\\right)+\\frac{n}{\\ln n}, \\quad T_{1}(1)=1$$\n$$T_{2}(n)=2\\,T_{2}\\!\\left(\\frac{n}{2}\\right)+n, \\quad T_{2}(1)=1$$\n\nFirst, we find an exact closed-form expression for $T_{2}(n)$.\nLet $n = 2^k$ for some integer $k \\ge 0$. Then $k = \\log_2 n$. The recurrence for $T_2$ is defined for $n \\ge 2$, which corresponds to $k \\ge 1$.\nWe unroll the recurrence:\n\\begin{align*} T_2(n) = 2 T_2\\left(\\frac{n}{2}\\right) + n \\\\ = 2 \\left( 2 T_2\\left(\\frac{n}{4}\\right) + \\frac{n}{2} \\right) + n = 4 T_2\\left(\\frac{n}{4}\\right) + n + n \\\\ = 4 \\left( 2 T_2\\left(\\frac{n}{8}\\right) + \\frac{n}{4} \\right) + 2n = 8 T_2\\left(\\frac{n}{8}\\right) + 3n \\\\  \\quad \\vdots \\\\ = 2^i T_2\\left(\\frac{n}{2^i}\\right) + i \\cdot n \\end{align*}\nWe unroll this until we reach the base case $T_2(1)$, which occurs when $\\frac{n}{2^k}=1$, or $i=k$.\n$$T_2(n) = 2^k T_2\\left(\\frac{n}{2^k}\\right) + k \\cdot n = 2^k T_2(1) + k n$$\nSince $n=2^k$ and $T_2(1)=1$, we have:\n$$T_2(n) = n \\cdot 1 + k n = n(1+k)$$\nSubstituting $k = \\log_2 n$, we obtain the closed form for $T_2(n)$:\n$$T_2(n) = n(1 + \\log_2 n)$$\n\nNext, we find a similar expression for $T_1(n)$. Let $n = 2^k$, so $k = \\log_2 n$. The recurrence is $T_1(n) = 2 T_1(n/2) + \\frac{n}{\\ln n}$ for $n \\ge 2$ ($k \\ge 1$).\nWe unroll this recurrence:\n\\begin{align*} T_1(n) = 2 T_1\\left(\\frac{n}{2}\\right) + \\frac{n}{\\ln n} \\\\ = 2 \\left( 2 T_1\\left(\\frac{n}{4}\\right) + \\frac{n/2}{\\ln(n/2)} \\right) + \\frac{n}{\\ln n} \\\\ = 4 T_1\\left(\\frac{n}{4}\\right) + \\frac{n}{\\ln(n/2)} + \\frac{n}{\\ln n} \\\\  \\quad \\vdots \\\\ = 2^k T_1\\left(\\frac{n}{2^k}\\right) + \\sum_{i=0}^{k-1} 2^i \\frac{n/2^i}{\\ln(n/2^i)} \\end{align*}\nUsing the base case $T_1(1)=1$ and $n=2^k$:\n$$T_1(n) = 2^k T_1(1) + \\sum_{i=0}^{k-1} \\frac{n}{\\ln(2^{k-i})}$$\n$$T_1(n) = n + n \\sum_{i=0}^{k-1} \\frac{1}{(k-i)\\ln 2}$$\nLet $j=k-i$. As $i$ goes from $0$ to $k-1$, $j$ goes from $k$ to $1$.\n$$T_1(n) = n + \\frac{n}{\\ln 2} \\sum_{j=1}^{k} \\frac{1}{j}$$\nThe sum $\\sum_{j=1}^{k} \\frac{1}{j}$ is the $k$-th Harmonic number, denoted $H_k$. So, with $k = \\log_2 n$:\n$$T_1(n) = n + \\frac{n}{\\ln 2} H_k = n \\left( 1 + \\frac{H_{\\log_2 n}}{\\ln 2} \\right)$$\n\nNow we can evaluate the limit $L$:\n$$L = \\lim_{n \\to \\infty} \\frac{T_1(n)}{T_2(n)} = \\lim_{n \\to \\infty} \\frac{n \\left( 1 + \\frac{H_{\\log_2 n}}{\\ln 2} \\right)}{n (1 + \\log_2 n)}$$\nWe can cancel the factor of $n$:\n$$L = \\lim_{n \\to \\infty} \\frac{1 + \\frac{H_{\\log_2 n}}{\\ln 2}}{1 + \\log_2 n}$$\nLet $k = \\log_2 n$. As $n \\to \\infty$, we have $k \\to \\infty$. The limit becomes:\n$$L = \\lim_{k \\to \\infty} \\frac{1 + \\frac{H_k}{\\ln 2}}{1 + k}$$\nWe can split the fraction:\n$$L = \\lim_{k \\to \\infty} \\left( \\frac{1}{1+k} + \\frac{H_k}{(1+k)\\ln 2} \\right)$$\nThe first term clearly goes to $0$:\n$$\\lim_{k \\to \\infty} \\frac{1}{1+k} = 0$$\nFor the second term, we analyze the limit $\\lim_{k \\to \\infty} \\frac{H_k}{1+k}$. To do this rigorously, we can use the Squeeze Theorem. The Harmonic number $H_k = \\sum_{i=1}^k \\frac{1}{i}$ can be bounded by integrals of the function $f(x) = 1/x$:\n$$\\int_1^{k+1} \\frac{1}{x} \\, dx \\le \\sum_{i=1}^k \\frac{1}{i} \\le 1 + \\int_1^k \\frac{1}{x} \\, dx$$\nEvaluating the integrals gives:\n$$\\ln(k+1) \\le H_k \\le 1 + \\ln(k)$$\nNow divide the entire inequality by $1+k$:\n$$\\frac{\\ln(k+1)}{1+k} \\le \\frac{H_k}{1+k} \\le \\frac{1+\\ln(k)}{1+k}$$\nWe take the limits of the lower and upper bounds as $k \\to \\infty$. Using L'Hôpital's rule for the lower bound:\n$$\\lim_{k \\to \\infty} \\frac{\\ln(k+1)}{1+k} = \\lim_{k \\to \\infty} \\frac{1/(k+1)}{1} = 0$$\nAnd for the upper bound:\n$$\\lim_{k \\to \\infty} \\frac{1+\\ln(k)}{1+k} = \\lim_{k \\to \\infty} \\frac{1/(k)}{1} = 0$$\nBy the Squeeze Theorem, we conclude:\n$$\\lim_{k \\to \\infty} \\frac{H_k}{1+k} = 0$$\nSubstituting this back into the expression for $L$:\n$$L = 0 + \\lim_{k \\to \\infty} \\frac{1}{\\ln 2} \\left( \\frac{H_k}{1+k} \\right) = 0 + \\frac{1}{\\ln 2} \\cdot 0 = 0$$\nThus, the exact value of the limit is $0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "3222269"}, {"introduction": "Comparing functions that involve factorials and unusual exponentiation can be daunting, and simple limit comparisons may not be straightforward. This advanced practice requires you to rank a set of complex functions that push beyond typical polynomial or exponential forms. You will learn to employ powerful techniques, such as logarithmic comparison and Stirling's approximation, to tame this complexity and establish a clear hierarchy of growth rates [@problem_id:3222399].", "problem": "An algorithm design team is comparing four asymptotic running-time candidates to understand their relative growth rates as the input size $n$ tends to infinity. The four functions are defined by\n$f_1(n) = n!$, $f_2(n) = (\\log n)!$, $f_3(n) = n^n$, and $f_4(n) = n^{\\log n}$, where $\\log n$ denotes the natural logarithm. Using the core definitions of asymptotic comparison from the theory of algorithms (for example, the notions embodied in little-oh), and without relying on any shortcut ranking, determine the strict total order of these four functions from the slowest-growing to the fastest-growing as $n \\to \\infty$. Encode your final ranking as a row vector of indices $(i_1, i_2, i_3, i_4)$, where each $i_k \\in \\{1,2,3,4\\}$ identifies the function $f_{i_k}$ and the sequence is in increasing order of growth rate. Express your final answer as a single row matrix using the $\\operatorname{pmatrix}$ environment. No rounding is required.", "solution": "The problem requires determining the strict total order of four functions based on their asymptotic growth rates as the input size $n$ tends to infinity. The functions are given as $f_1(n) = n!$, $f_2(n) = (\\log n)!$, $f_3(n) = n^n$, and $f_4(n) = n^{\\log n}$. The notation $\\log n$ refers to the natural logarithm.\n\nTo establish the strict ordering, we must arrange the functions $f_{i_k}$ such that $f_{i_1}(n) = o(f_{i_2}(n))$, $f_{i_2}(n) = o(f_{i_3}(n))$, and $f_{i_3}(n) = o(f_{i_4}(n))$. The little-oh notation, $f(n) = o(g(n))$, signifies that $g(n)$ grows strictly faster than $f(n)$, which is formally defined by the limit $\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0$.\n\nWe will perform a series of pairwise comparisons to establish this order. A powerful method for comparing functions is to analyze the asymptotic behavior of their logarithms. Since the logarithm is a monotonically increasing function, the growth order of the functions is preserved in their logarithms. Specifically, if $\\lim_{n \\to \\infty} (\\log g(n) - \\log f(n)) = \\infty$, it follows that $\\lim_{n \\to \\infty} \\log\\left(\\frac{g(n)}{f(n)}\\right) = \\infty$, which implies $\\lim_{n \\to \\infty} \\frac{g(n)}{f(n)} = \\infty$. This is equivalent to $\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0$, thus proving $f(n) = o(g(n))$.\n\nThe logarithms of the four functions are:\n$\\log(f_1(n)) = \\log(n!)$\n$\\log(f_2(n)) = \\log((\\log n)!)$\n$\\log(f_3(n)) = \\log(n^n) = n \\log n$\n$\\log(f_4(n)) = \\log(n^{\\log n}) = (\\log n) \\log n = (\\log n)^2$\n\nFor analyzing the factorial terms, we use Stirling's approximation for the logarithm of a factorial, which states that for large $k$, $\\log(k!) = k \\log k - k + O(\\log k)$. This provides the asymptotic behavior of $\\log(f_1(n))$ and $\\log(f_2(n))$.\n\n**Comparison 1: $f_2(n)$ versus $f_4(n)$**\n\nWe compare $f_2(n) = (\\log n)!$ and $f_4(n) = n^{\\log n}$. Let us analyze their logarithms.\n$\\log(f_4(n)) = (\\log n)^2$.\nFor $\\log(f_2(n))$, we apply Stirling's approximation with $k = \\log n$. As $n \\to \\infty$, $k \\to \\infty$.\n$\\log(f_2(n)) = \\log((\\log n)!) = (\\log n)\\log(\\log n) - \\log n + O(\\log(\\log n))$.\nNow, we examine the difference of the logarithms as $n \\to \\infty$:\n$$ \\log(f_4(n)) - \\log(f_2(n)) = (\\log n)^2 - \\left((\\log n)\\log(\\log n) - \\log n + O(\\log(\\log n))\\right) $$\n$$ = (\\log n)^2 - (\\log n)\\log(\\log n) + \\log n - O(\\log(\\log n)) $$\nWe can factor out the dominant term of this expression, which is $(\\log n)^2$, as we are interested in the limit. Alternatively, we can see that $(\\log n)^2$ grows faster than $(\\log n)\\log(\\log n)$. Let's factor out $\\log n$:\n$$ \\log n \\left(\\log n - \\log(\\log n) + 1 - O\\left(\\frac{\\log(\\log n)}{\\log n}\\right)\\right) $$\nAs $n \\to \\infty$, the term $\\log n - \\log(\\log n)$ tends to infinity. The trailing terms are either constant or tend to $0$. Thus, the entire expression tends to infinity.\n$$ \\lim_{n \\to \\infty} (\\log(f_4(n)) - \\log(f_2(n))) = \\infty $$\nThis implies that $f_2(n) = o(f_4(n))$.\n\n**Comparison 2: $f_4(n)$ versus $f_1(n)$**\n\nWe compare $f_4(n) = n^{\\log n}$ and $f_1(n) = n!$. Again, we examine their logarithms.\n$\\log(f_4(n)) = (\\log n)^2$.\nFor $\\log(f_1(n))$, we use Stirling's approximation with $k = n$:\n$\\log(f_1(n)) = \\log(n!) = n \\log n - n + O(\\log n)$.\nWe examine the difference of the logarithms as $n \\to \\infty$:\n$$ \\log(f_1(n)) - \\log(f_4(n)) = (n \\log n - n + O(\\log n)) - (\\log n)^2 $$\n$$ = n \\log n - n - (\\log n)^2 + O(\\log n) $$\nAs $n \\to \\infty$, the term $n \\log n$ grows faster than all other terms in the expression ($n$, $(\\log n)^2$, and $O(\\log n)$). Therefore, the limit of the difference is determined by the dominant term $n \\log n$.\n$$ \\lim_{n \\to \\infty} (\\log(f_1(n)) - \\log(f_4(n))) = \\infty $$\nThis implies that $f_4(n) = o(f_1(n))$.\n\n**Comparison 3: $f_1(n)$ versus $f_3(n)$**\n\nWe compare $f_1(n) = n!$ and $f_3(n) = n^n$. For this pair, it is straightforward to compute the limit of their ratio directly.\n$$ \\lim_{n \\to \\infty} \\frac{f_1(n)}{f_3(n)} = \\lim_{n \\to \\infty} \\frac{n!}{n^n} = \\lim_{n \\to \\infty} \\frac{1 \\cdot 2 \\cdot 3 \\cdots n}{n \\cdot n \\cdot n \\cdots n} $$\nWe can write the ratio as a product:\n$$ \\frac{n!}{n^n} = \\left(\\frac{1}{n}\\right) \\left(\\frac{2}{n}\\right) \\left(\\frac{3}{n}\\right) \\cdots \\left(\\frac{n}{n}\\right) $$\nFor any $n \\ge 2$, we can establish an upper bound for this product. Each term $\\frac{k}{n} \\le 1$ for $k=1, \\dots, n$. The first term is $\\frac{1}{n}$.\n$$ 0  \\frac{n!}{n^n} = \\frac{1}{n} \\cdot \\left(\\frac{2}{n} \\cdots \\frac{n}{n}\\right) \\le \\frac{1}{n} \\cdot (1 \\cdots 1) = \\frac{1}{n} $$\nSince we have $0  \\frac{n!}{n^n} \\le \\frac{1}{n}$ and $\\lim_{n \\to \\infty} \\frac{1}{n} = 0$, the Squeeze Theorem implies that:\n$$ \\lim_{n \\to \\infty} \\frac{n!}{n^n} = 0 $$\nThis proves that $f_1(n) = o(f_3(n))$.\n\n**Conclusion**\n\nFrom our pairwise comparisons, we have established the following strict ordering:\n$1.$ $f_2(n) = o(f_4(n))$\n$2.$ $f_4(n) = o(f_1(n))$\n$3.$ $f_1(n) = o(f_3(n))$\n\nBy the transitivity of the little-oh relationship, we can combine these results into a single strict total order:\n$$ f_2(n) \\prec f_4(n) \\prec f_1(n) \\prec f_3(n) $$\nwhere $\\prec$ denotes \"grows strictly slower than\".\n\nThe problem requires the answer to be encoded as a row vector of indices $(i_1, i_2, i_3, i_4)$ corresponding to this ordering. The slowest-growing function is $f_2(n)$, followed by $f_4(n)$, then $f_1(n)$, and the fastest-growing is $f_3(n)$. The sequence of indices is therefore $(2, 4, 1, 3)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2  4  1  3\n\\end{pmatrix}\n}\n$$", "id": "3222399"}]}