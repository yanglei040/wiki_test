{"hands_on_practices": [{"introduction": "The power of Abstract Data Types (ADTs) lies in separating the \"what\" from the \"how.\" This exercise challenges you to apply this principle by creating a pure ADT specification for a `RingBuffer` with a fixed capacity and overwrite-on-full semantics. By defining its state and operations using only abstract mathematical concepts like sequences, you will practice the crucial skill of formal reasoning and see clearly how its axiomatic definition differs from that of an unbounded `Queue` [@problem_id:3202558].", "problem": "An Abstract Data Type is defined by a set of states $S$, a set of operations $Ops$ with precise domains and codomains, and axioms that constrain how these operations transform states and produce results. For a standard first-in-first-out queue (Queue), the state is a sequence over an element set $E$ (with $E$ fixed but otherwise arbitrary), and operations such as $enqueue$, $dequeue$, and $front$ are defined over that sequence with axioms that enforce the first-in-first-out discipline. A pure Abstract Data Type specification must refer only to mathematical objects such as sets, sequences, and total or partial functions, and must not commit to implementation details such as arrays, pointers, indices, or modulo arithmetic.\n\nConsider defining a RingBuffer (Circular Queue) as a pure Abstract Data Type with overwrite-on-full semantics. The capacity is fixed at creation and does not change thereafter. The intent is that when the buffer is full and an $enqueue$ occurs, the oldest element is discarded to make room for the new element; all other behaviors (such as $dequeue$ and $front$) must preserve the first-in-first-out order of the remaining elements. In contrast, a standard Queue Abstract Data Type is conceptually unbounded and never overwrites elements; when a Queue is implemented with a finite container, that finiteness is an implementation constraint and not part of the Abstract Data Type’s axioms.\n\nWhich option most correctly gives a pure Abstract Data Type specification for a RingBuffer with overwrite-on-full semantics, and explicitly contrasts its axioms with those of a standard Queue Abstract Data Type, without referring to representation details?\n\nA. States are sequences $s \\in E^{\\le k}$, where $k \\in \\mathbb{N}$ is fixed at creation and $|s| \\le k$. Operations are partial functions:\n- $empty(s)$ returns $\\mathsf{true}$ iff $|s| = 0$; $full(s)$ returns $\\mathsf{true}$ iff $|s| = k$; $size(s) = |s|$; $capacity(s) = k$.\n- $enqueue(s, x)$ returns a new state $s'$ defined by:\n  if $|s| < k$ then $s' = s \\cdot x$; otherwise $s' = drop\\_1(s) \\cdot x$, where $drop\\_1(s)$ removes the first (oldest) element of $s$ and $\\cdot$ denotes sequence concatenation.\n- $dequeue(s)$ is defined only if $|s| > 0$ and returns a pair $(y, s')$ with $y$ equal to the first element of $s$ and $s'$ equal to $drop\\_1(s)$; $front(s)$ is defined only if $|s| > 0$ and returns the first element.\nAxioms enforce that the sequence order is preserved under $enqueue$ and $dequeue$, that $capacity(s)$ is an invariant equal to $k$, and that $enqueue$ never increases $|s|$ beyond $k$. In contrast, a standard Queue has states $s \\in E^{*}$ (no bound on $|s|$), $full(s)$ is always $\\mathsf{false}$, and $enqueue(s, x)$ always yields $s' = s \\cdot x$ (never overwriting).\n\nB. States are given by array indices $head, tail \\in \\mathbb{Z}$ and a fixed array $A[0..k-1]$. The operation $enqueue$ writes $x$ to $A[tail]$, then sets $tail := (tail + 1) \\bmod k$, and if $tail = head$ then $head := (head + 1) \\bmod k$. The operation $dequeue$ reads $A[head]$ and sets $head := (head + 1) \\bmod k$. This differs from a Queue by using modulo arithmetic on indices but is otherwise the same.\n\nC. States are sequences $s \\in E^{*}$ with a mutable capacity function $capacity(s)$ that can grow when needed. The operation $enqueue(s, x)$ always returns $s' = s \\cdot x$, and if $|s| = capacity(s)$ then $capacity(s') := capacity(s) + 1$. This differs from a Queue by tracking a capacity that auto-resizes.\n\nD. States are sequences $s \\in E^{\\le k}$, but $enqueue(s, x)$ when $|s| = k$ returns an error without modifying $s$, and $dequeue(s)$ on $|s| = 0$ returns a distinguished value $\\bot \\in E$. This differs from a Queue by having a capacity and an error on full, but otherwise the axioms are identical.\n\nE. States are sets $S \\subseteq E$ of at most $k$ elements. The operation $enqueue(S, x)$ when $|S| = k$ removes an arbitrary element from $S$ and inserts $x$. The operation $dequeue(S)$ returns any element of $S$. This differs from a Queue by bounding the cardinality but keeps the abstract nature by avoiding sequences entirely.", "solution": "The problem requires the formulation of a pure Abstract Data Type (ADT) for a RingBuffer (or Circular Queue) with a fixed capacity and overwrite-on-full semantics. A pure ADT must be defined using abstract mathematical concepts, such as sets and sequences, and must be devoid of implementation details like arrays, indices, or pointers. The specification must also be contrasted with that of a standard, unbounded Queue ADT.\n\nFirst, let us establish the core principles of a pure ADT specification for a sequence-based data structure.\n- **State Space**: The state of the ADT is represented by a mathematical object. For an ordered collection, a sequence is the natural choice. A standard, unbounded queue has a state space of $s \\in E^*$, where $E$ is the set of elements and $E^*$ is the set of all finite sequences over $E$. For a fixed-capacity RingBuffer of capacity $k$, the state must be a sequence $s$ whose length $|s|$ cannot exceed $k$. Thus, the state space is $E^{\\le k} = \\{s \\in E^* | |s| \\le k \\}$.\n- **Operations**: Operations are defined as mathematical functions over the state space. They map a state (and possibly other arguments) to a new state and/or a result value.\n- **Axioms**: These are formal rules that describe the behavior of the operations. For example, they define how $enqueue$ and $dequeue$ interact to enforce the First-In-First-Out (FIFO) discipline.\n\nNow, let's define the required RingBuffer ADT with overwrite-on-full semantics for a fixed capacity $k \\in \\mathbb{N}$.\n- Let the state be a sequence $s \\in E^{\\le k}$. The head of the queue (the oldest element) is the first element of the sequence, and the tail (the most recent) is the last element.\n- The operation $enqueue(s, x)$ must handle two cases:\n    1. If the buffer is not full, i.e., $|s| < k$, the new element $x$ is appended to the sequence. The new state is $s' = s \\cdot x$, where $\\cdot$ denotes sequence concatenation.\n    2. If the buffer is full, i.e., $|s| = k$, the oldest element must be removed to make space. The oldest element is the first one in the sequence. Let $drop\\_1(s)$ be the operation that returns the sequence $s$ with its first element removed. The new element $x$ is then appended. The new state is $s' = drop\\_1(s) \\cdot x$.\n- The operation $dequeue(s)$ is a partial function, defined only if the buffer is not empty ($|s| > 0$). It must return the oldest element (the first element of the sequence) and the new state, which is the sequence with the first element removed. So, $dequeue(s)$ returns a pair $(y, s')$ where $y$ is the first element of $s$ and $s' = drop\\_1(s)$.\n- The operation $front(s)$ is also a partial function, defined for $|s| > 0$. It returns the oldest element (the first element of the sequence) without modifying the state.\n- Auxiliary operations like $size(s)=|s|$, $capacity(s)=k$, $empty(s) \\iff |s|=0$, and $full(s) \\iff |s|=k$ complete the specification.\n\nThis specification is pure because it relies solely on the mathematical theory of sequences. It contrasts with a standard Queue ADT, where the state space is $E^*$ (unbounded), $full(s)$ is never true, and the axiom for $enqueue$ is always $enqueue(s, x) \\rightarrow s \\cdot x$, without an overwrite case.\n\nNow, we evaluate each option against this formal derivation.\n\n**Option A**\n- **States**: Defined as sequences $s \\in E^{\\le k}$, with a fixed capacity $k$. This correctly models a fixed-capacity structure.\n- **Operations**:\n    - The definitions for $empty(s)$, $full(s)$, $size(s)$, and $capacity(s)$ based on sequence length $|s|$ are correct.\n    - The definition of $enqueue(s, x)$ correctly distinguishes between the non-full case ($|s| < k$), where it returns $s \\cdot x$, and the full case, where it returns $drop\\_1(s) \\cdot x$. This precisely captures the overwrite-on-full semantic using abstract sequence operations.\n    - The definitions for $dequeue(s)$ and $front(s)$ correctly identify the first element of the sequence as the one to be returned/removed, preserving the FIFO discipline.\n- **Purity**: The specification uses only sequences, sequence length, concatenation, and element access/removal by position (first). It avoids implementation details like arrays, indices, and modulo arithmetic. It is a pure ADT specification.\n- **Contrast with Queue**: The contrast is accurate. A standard Queue has states $s \\in E^*$, is never full, and its $enqueue$ operation is simpler, never overwriting.\n- **Verdict**: **Correct**. This option provides a flawless pure ADT specification for the RingBuffer as described.\n\n**Option B**\n- **States**: Defined using an array $A[0..k-1]$ and integer indices $head$ and $tail$.\n- **Operations**: Defined using array access and modulo arithmetic on indices, e.g., $tail := (tail + 1) \\bmod k$.\n- **Purity**: This is a direct violation of the requirement for a pure ADT specification. It is a description of a common **implementation** of a RingBuffer, not its abstract type. The problem explicitly forbids mentioning \"arrays, pointers, indices, or modulo arithmetic\".\n- **Verdict**: **Incorrect**.\n\n**Option C**\n- **States**: Defined as sequences $s \\in E^*$, which implies an unbounded conceptual size. It introduces a \"mutable capacity function\".\n- **Operations**: The $enqueue$ operation always appends, and if capacity is met, the capacity itself grows.\n- **Behavior**: This describes a dynamically resizing queue, not a RingBuffer with a **fixed capacity** and **overwrite** semantics. The specified behavior is fundamentally different from what is requested.\n- **Verdict**: **Incorrect**.\n\n**Option D**\n- **States**: Defined as sequences $s \\in E^{\\le k}$, which is a correct starting point.\n- **Operations**: The $enqueue(s, x)$ operation, when $|s|=k$, \"returns an error without modifying $s$\".\n- **Behavior**: This describes a **bounded queue**, which signals an error or blocks when full. It does not implement the specified \"overwrite-on-full\" semantics where the oldest element is discarded to make room for the new one.\n- **Verdict**: **Incorrect**.\n\n**Option E**\n- **States**: Defined as sets $S \\subseteq E$. A set is an unordered collection of distinct elements.\n- **Behavior**: Using a set as the state fundamentally breaks the FIFO discipline. An ordering is essential for a queue. The operations confirm this flaw: $enqueue$ on full removes an \"arbitrary\" element, and $dequeue$ returns \"any\" element. This is not a queue.\n- ** verdict**: **Incorrect**.\n\nBased on the analysis, Option A is the only one that provides a correct and pure ADT specification for a RingBuffer with overwrite-on-full semantics, consistent with all requirements of the problem statement.", "answer": "$$\\boxed{A}$$", "id": "3202558"}, {"introduction": "Many powerful data structures are built by composing simpler ones. This practice guides you through the design of an `OnlineMedian` ADT, a classic problem that requires finding the median of a dynamically changing dataset efficiently. You will learn how to combine two heap ADTs—one max-heap and one min-heap—and maintain critical size and ordering invariants to support all operations, including deletions, in logarithmic time [@problem_id:3202610].", "problem": "Consider designing an Abstract Data Type (ADT) for an online median service that stores a multiset of comparable keys and supports streaming updates. The ADT, called OnlineMedian, must support the following operations with well-defined semantics: insert a key $\\;x\\;$ as $\\;\\mathrm{insert}(x)$, delete one occurrence of key $\\;x\\;$ if present as $\\;\\mathrm{delete}(x)$, query the current median as $\\;\\mathrm{median}()\\;$, report the current size as $\\;\\mathrm{size}()\\;$, and test emptiness as $\\;\\mathrm{isEmpty}()\\;$. The median must be defined deterministically as the lower median: for a multiset of size $\\;n\\;$, if $\\;n\\;$ is odd, $\\;\\mathrm{median}()\\;$ returns the unique middle element in sorted order; if $\\;n\\;$ is even, it returns the element at rank $\\;\\lfloor n/2 \\rfloor\\;$ in sorted order, which is the maximum of the lower half. The ADT must maintain the invariant that $\\;\\mathrm{median}()\\;$ returns one of the stored keys (not an average). The implementation constraint is that OnlineMedian must be realized using exactly two heaps: a maximum binary heap (priority queue) $\\;H_{\\mathrm{low}}\\;$ storing the lower half and a minimum binary heap $\\;H_{\\mathrm{high}}\\;$ storing the upper half. The goal is to preserve correctness under arbitrary streams of $\\;\\mathrm{insert}\\;$ and $\\;\\mathrm{delete}\\;$ operations and to keep every operation in $\\;\\mathcal{O}(\\log n)\\;$ time, with $\\;n\\;$ the current size, including any necessary rebalancing after updates. Additionally, the design must be stable under deletions and rebalancing, in the sense that the chosen median changes only when logically forced by the set update and the heaps’ partition boundary, and not due to implementation artifacts; specifically, repeated inserts and deletes of keys equal to the median must not cause non-deterministic oscillation of $\\;\\mathrm{median}()\\;$.\n\nStarting from fundamental definitions and well-tested facts:\n- A binary heap is a complete binary tree with the heap-order property; a maximum binary heap always has the maximum key at the root, and a minimum binary heap always has the minimum key at the root.\n- Standard heap operations (insert, extract-root, increase/decrease-key when applicable) run in $\\;\\mathcal{O}(\\log m)\\;$ time for heap size $\\;m\\;$, due to the height of a complete binary tree being $\\;\\mathcal{O}(\\log m)\\;$.\n- A priority queue implemented by a binary heap supports root access in $\\;\\mathcal{O}(1)\\;$ time and updates in $\\;\\mathcal{O}(\\log m)\\;$ time.\n\nWhich option below gives a correct ADT specification together with invariants, deletion strategy, and rebalancing rules that are sufficient to ensure correctness of $\\;\\mathrm{median}()\\;$ and stability under deletions and rebalancing, while keeping each operation in $\\;\\mathcal{O}(\\log n)\\;$ time?\n\nA. Maintain two heaps with the invariants: $\\;|H_{\\mathrm{low}}| \\in \\{|H_{\\mathrm{high}}|, |H_{\\mathrm{high}}|+1\\}\\;$ and every key in $\\;H_{\\mathrm{low}}\\;$ is $\\;\\le\\;$ every key in $\\;H_{\\mathrm{high}}\\;$. Define $\\;\\mathrm{median}()\\;$ to be the root of $\\;H_{\\mathrm{low}}\\;$ (the lower median) for all $\\;n \\ge 1\\;$. Implement $\\;\\mathrm{insert}(x)\\;$ by inserting $\\;x\\;$ into $\\;H_{\\mathrm{low}}\\;$ if $\\;x \\le \\mathrm{root}(H_{\\mathrm{low}})\\;$ (or if $\\;H_{\\mathrm{low}}\\;$ is empty), else into $\\;H_{\\mathrm{high}}\\;$. After insertion, if $\\;|H_{\\mathrm{low}}| = |H_{\\mathrm{high}}|+2\\;$, move $\\;\\mathrm{root}(H_{\\mathrm{low}})\\;$ to $\\;H_{\\mathrm{high}}\\;$; if $\\;|H_{\\mathrm{high}}| = |H_{\\mathrm{low}}|+1\\;$, move $\\;\\mathrm{root}(H_{\\mathrm{high}})\\;$ to $\\;H_{\\mathrm{low}}\\;$. Implement $\\;\\mathrm{delete}(x)\\;$ via lazy deletion with per-value counters in a hash map: decrement the counter for $\\;x\\;$ on delete, and whenever accessing $\\;\\mathrm{root}(H_{\\mathrm{low}})\\;$ or $\\;\\mathrm{root}(H_{\\mathrm{high}})\\;$, repeatedly pop stale roots whose counters indicate no live occurrences. Rebalance after deletions using the same size rules. Argue that $\\;\\mathrm{median}()\\;$ is correct under the maintained partition and that each $\\;\\mathrm{insert}\\;$ and $\\;\\mathrm{delete}\\;$ performs a constant number of heap operations, each $\\;\\mathcal{O}(\\log n)\\;$, plus $\\;\\mathcal{O}(1)\\;$ hash map updates, and that pruning of stale roots is amortized $\\;\\mathcal{O}(1)\\;$ per update.\n\nB. Maintain two heaps with the invariant that every key in $\\;H_{\\mathrm{low}}\\;$ is $\\;<\\;$ every key in $\\;H_{\\mathrm{high}}\\;$, but allow $\\;|H_{\\mathrm{low}}|-|H_{\\mathrm{high}}|\\;$ to grow unbounded. Define $\\;\\mathrm{median}()\\;$ as the root of $\\;H_{\\mathrm{low}}\\;$ regardless of $\\;n\\;$, and never rebalance sizes because heap accesses are $\\;\\mathcal{O}(1)\\;$ and heap inserts are $\\;\\mathcal{O}(\\log n)\\;$ anyway. Handle $\\;\\mathrm{delete}(x)\\;$ by deleting from whichever heap contains $\\;x\\;$ if it is at the root; otherwise do nothing until $\\;x\\;$ reaches the root.\n\nC. Maintain the same partition as in Option A, but whenever $\\;|H_{\\mathrm{low}}|-|H_{\\mathrm{high}}| = 1\\;$, rebuild both heaps by extracting all elements and repartitioning them to restore exact equality of sizes. Define $\\;\\mathrm{median}()\\;$ as the average of the two roots when $\\;n\\;$ is even and as $\\;\\mathrm{root}(H_{\\mathrm{low}})\\;$ when $\\;n\\;$ is odd. Implement $\\;\\mathrm{delete}(x)\\;$ by scanning both heaps linearly to find $\\;x\\;$ and then bubbling up or down to fix the heap.\n\nD. Maintain two heaps with the size invariant $\\;||H_{\\mathrm{low}}|-|H_{\\mathrm{high}}|| \\le 2\\;$ and the ordering invariant that every key in $\\;H_{\\mathrm{low}}\\;$ is $\\;\\le\\;$ every key in $\\;H_{\\mathrm{high}}\\;$. Define $\\;\\mathrm{median}()\\;$ to be the root of the larger heap; when sizes differ by $\\;2\\;$, return the root of the larger heap and rebalance only if the difference reaches $\\;3\\;$. Implement $\\;\\mathrm{delete}(x)\\;$ by removing $\\;x\\;$ from the heap that currently contains it using a linear search through the array backing the heap, and then reheapify.\n\nSelect the single best option that meets all requirements.", "solution": "### Principle-Based Derivation and Option Analysis\n\nThe goal is to partition the `n` elements into a lower part `L` and an upper part `H` such that the median can be found efficiently. The problem defines the median as the element at rank `\\lceil n/2 \\rceil`, which is the maximum element in the set of the `\\lceil n/2 \\rceil` smallest elements.\n\nThis naturally leads to the following design:\n1.  `H_low` (a max-heap) will store the `\\lceil n/2 \\rceil` smallest elements.\n2.  `H_high` (a min-heap) will store the remaining `\\lfloor n/2 \\rfloor` largest elements.\n\nFrom this design, two core invariants emerge:\n*   **Partition Invariant:** Every key in `H_low` must be less than or equal to every key in `H_high`. This is maintained if `max(H_low) \\le min(H_high)`, which translates to `root(H_low) \\le root(H_high)`.\n*   **Size Invariant:** The sizes of the heaps must be `|H_low| = \\lceil n/2 \\rceil` and `|H_high| = \\lfloor n/2 \\rfloor`. This is equivalent to requiring `|H_low| = |H_high|` (when `n` is even) or `|H_low| = |H_high| + 1` (when `n` is odd).\n\nWith these invariants, `median()` is simply the maximum of the lower half, which is `root(H_low)`. Accessing the root is $\\mathcal{O}(1)$.\n\n**Operations:**\n*   **`insert(x)`**: To maintain the partition invariant, if `x` is less than or equal to the current median (`root(H_low)`), it must belong to the lower set. So, we insert `x` into `H_low`. Otherwise, we insert `x` into `H_high`. After insertion, the size invariant may be violated. A rebalancing step is needed: if one heap has grown too large, its root is extracted and inserted into the other heap. This involves a constant number of heap operations, making `insert(x)` an $\\mathcal{O}(\\log n)$ operation.\n*   **`delete(x)`**: Finding an arbitrary element `x` in a heap takes $\\mathcal{O}(n)$ time, which violates the performance constraint. To achieve $\\mathcal{O}(\\log n)$, a more sophisticated method is needed. One cannot simply augment the heap with pointers from a hash map because elements move during heap operations. The standard and most robust solution is **lazy deletion**. We maintain a hash map of counts for the elements. A `delete(x)` operation just decrements a counter in $\\mathcal{O}(1)$ time. The heaps will contain \"stale\" elements whose counts are zero. These stale elements must be physically removed when they reach the root of either heap and are needed for a median query or a rebalancing step. This cleanup process (e.g., `while count[root(H)] == 0: H.extract_root()`) adds a cost. While a single operation might trigger many such removals, this cost is amortized to $\\mathcal{O}(\\log n)$ per update operation, as each element is inserted once, marked for deletion once, and physically removed from the heap at most once.\n\nNow, we evaluate the given options based on this design.\n\n**A. Maintain two heaps with the invariants: `|H_{\\mathrm{low}}| \\in \\{|H_{\\mathrm{high}}|, |H_{\\mathrm{high}}|+1\\}` and every key in `H_{\\mathrm{low}}` is `\\le` every key in `H_{\\mathrm{high}}`. Define `median()` to be the root of `H_{\\mathrm{low}}`. ... Implement `delete(x)` via lazy deletion with per-value counters in a hash map...**\n\n*   **Analysis:** This option correctly identifies the size invariant `|H_{\\mathrm{low}}| = \\lceil n/2 \\rceil` and the partition invariant `max(H_{low}) \\le min(H_{high})`. The definition of `median()` as `root(H_{\\mathrm{low}})` is a direct consequence of these invariants. The insertion logic (`x \\le root(H_{\\mathrm{low}})` directs to `H_{\\mathrm{low}}`) and the rebalancing rules are perfectly correct. Critically, it proposes lazy deletion with a hash map to handle `delete(x)`, which is the correct approach to achieve the required $\\mathcal{O}(\\log n)$ amortized time complexity. The pruning of stale roots is correctly identified as a necessary step when accessing the roots. The time complexity analysis is sound; each operation requires a constant number of heap operations and hash map lookups, leading to an overall amortized $\\mathcal{O}(\\log n)$ time. The stability is also ensured because an element equal to the median is deterministically placed in `H_{\\mathrm{low}}`.\n*   **Verdict:** **Correct**.\n\n**B. Maintain two heaps with the invariant that every key in `H_{\\mathrm{low}}` is `<` every key in `H_{\\mathrm{high}}`, but allow `|H_{\\mathrm{low}}|-|H_{\\mathrm{high}}|$ to grow unbounded. Define `median()` as the root of `H_{\\mathrm{low}}` regardless of `n`, and never rebalance sizes...**\n\n*   **Analysis:** This option has multiple fatal flaws. First, if `|H_{\\mathrm{low}}|-|H_{\\mathrm{high}}|` is unbounded, the heaps no longer represent the lower and upper halves of the data. `root(H_{\\mathrm{low}})` would not be the median. Forgoing rebalancing invalidates the entire premise of the two-heap median-finding algorithm. Second, the deletion strategy—\"do nothing until `x` reaches the root\"—is not a valid algorithm, as an element is not guaranteed to ever reach the root. Third, the strict inequality `key_low < key_high` is problematic for multisets, as it's unclear where to place duplicates of the boundary value `max(H_{low})`.\n*   **Verdict:** **Incorrect**.\n\n**C. Maintain the same partition as in Option A, but whenever `|H_{\\mathrm{low}}|-|H_{\\mathrm{high}}| = 1`, rebuild both heaps by extracting all elements and repartitioning them... Define `median()` as the average of the two roots when `n` is even... Implement `delete(x)` by scanning both heaps linearly...**\n\n*   **Analysis:** This option is incorrect for several reasons. The rebalancing strategy of rebuilding the heaps is extremely inefficient, taking $\\mathcal{O}(n)$ or $\\mathcal{O}(n \\log n)$ time, which violates the $\\mathcal{O}(\\log n)$ per-operation constraint. Defining the median as an average for even `n` directly contradicts the problem requirement that the median must be one of the stored keys. Finally, implementing `delete(x)` with a linear scan is an $\\mathcal{O}(n)$ operation, again violating the time complexity constraint.\n*   **Verdict:** **Incorrect**.\n\n**D. Maintain two heaps with the size invariant `||H_{\\mathrm{low}}|-|H_{\\mathrm{high}}|| \\le 2` ... Define `median()` to be the root of the larger heap... Implement `delete(x)` by removing `x` from the heap that currently contains it using a linear search...**\n\n*   **Analysis:** This option is also flawed. The size invariant `||H_{\\mathrm{low}}|-|H_{\\mathrm{high}}|| \\le 2` is too loose. If `n=10`, the correct split is `(5,5)`. This invariant would allow `(6,4)`. In that case, `H_{\\mathrm{low}}` has size `6`, and its root might be the element of rank `6`, not the required median at rank `5`. The median definition as \"the root of the larger heap\" is not a consistent way to find the `\\lceil n/2 \\rceil`-th element. Like option C, it also proposes an $\\mathcal{O}(n)$ linear search for deletion, violating the performance requirements.\n*   **Verdict:** **Incorrect**.\n\nBased on the detailed analysis, Option A is the only one that provides a correct, comprehensive, and efficient specification for the `OnlineMedian` ADT, satisfying all constraints laid out in the problem statement.", "answer": "$$\\boxed{A}$$", "id": "3202610"}, {"introduction": "While asymptotic analysis provides invaluable high-level guidance, real-world performance is often dictated by the underlying hardware. This exercise moves beyond simple Big-O notation to explore a more nuanced microarchitectural cost model that includes penalties for cache misses and branch mispredictions. By comparing a balanced binary search tree with a simple array scan, you will discover and quantify the practical scenarios where a theoretically slower algorithm can win, a vital lesson in performance engineering [@problem_id:3202574].", "problem": "You are given two competing Abstract Data Types (ADTs) for static membership queries on a fixed set of $n$ keys, under a microarchitectural cost model that explicitly accounts for cache misses and branch mispredictions. The goal is to construct a workload in which a theoretically superior ADT (asymptotically fewer logical steps) performs worse on real hardware, and to quantify the regime in $n$ where this inversion happens according to the model.\n\nDefinitions and assumptions:\n- A balanced binary search tree (BST) implements unsuccessful membership query by descending from the root to a leaf, visiting exactly $h$ nodes, where $h = \\lceil \\log_2(n + 1) \\rceil$ by the standard property of balanced trees. Each visited node incurs three independent costs:\n  - One key comparison costing $c_{\\mathrm{cmp}}$ cycles.\n  - With independent probability $p_{\\mathrm{miss}}^{\\mathrm{bst}}$, a cache miss, each miss costing $c_{\\mathrm{miss}}$ cycles.\n  - With independent probability $p_{\\mathrm{bm}}^{\\mathrm{bst}}$, a branch misprediction, each misprediction costing $c_{\\mathrm{bm}}$ cycles.\n- A sorted array with branchless linear scan implements unsuccessful membership query by scanning all $n$ elements (an adversarial workload of all misses to magnify the gap). It performs $n$ key comparisons, each costing $c_{\\mathrm{cmp}}$ cycles. Memory is contiguous with cache line size $L$ bytes and element (key) size $s$ bytes, so each cache line holds $E = \\left\\lfloor \\frac{L}{s} \\right\\rfloor$ elements. Under a standard streaming model, one cache miss is paid per cache line, so the scan pays $\\left\\lceil \\frac{n}{E} \\right\\rceil$ cache misses. The scan is branchless, so it pays zero branch misprediction cost.\n\nFundamental base to use:\n- The definition of the balanced tree height $h = \\lceil \\log_2(n + 1) \\rceil$.\n- The linearity of expectation for summing independent expected costs across visited nodes or scanned elements.\n- The cache-line model with $E = \\left\\lfloor \\frac{L}{s} \\right\\rfloor$ elements per line and $\\left\\lceil \\frac{n}{E} \\right\\rceil$ line fills for a streaming scan.\n- The independence of miss and misprediction events per step with fixed probabilities for the BST.\n\nTasks:\n1. Starting only from the definitions above, derive the expected cycle count per unsuccessful query for each ADT as a function of $n$, $L$, $s$, $c_{\\mathrm{cmp}}$, $c_{\\mathrm{miss}}$, $c_{\\mathrm{bm}}$, $p_{\\mathrm{miss}}^{\\mathrm{bst}}$, and $p_{\\mathrm{bm}}^{\\mathrm{bst}}$. Your derivation must invoke the balanced tree height, the cache line occupancy, and linearity of expectation.\n2. Define the function $T_{\\mathrm{bst}}(n)$ for the BST and $T_{\\mathrm{arr}}(n)$ for the array scan from your derivation. Then, for a given upper bound $N_{\\max}$, define\n   $$ n_{\\max}^{\\star} = \\max \\{ n \\in \\{1,2,\\dots,N_{\\max}\\} : T_{\\mathrm{arr}}(n) < T_{\\mathrm{bst}}(n) \\}, $$\n   with the convention that if the set is empty, then $n_{\\max}^{\\star} = 0$.\n3. Implement a complete, runnable program that computes $n_{\\max}^{\\star}$ for each of the following parameter sets. Each parameter set is an $8$-tuple $\\left(L, s, c_{\\mathrm{cmp}}, c_{\\mathrm{miss}}, c_{\\mathrm{bm}}, p_{\\mathrm{miss}}^{\\mathrm{bst}}, p_{\\mathrm{bm}}^{\\mathrm{bst}}, N_{\\max}\\right)$:\n   - Test A: $\\left(64, 8, 1, 200, 15, 0.9, 0.5, 1000\\right)$\n   - Test B: $\\left(64, 8, 1, 30, 5, 0.9, 0.5, 1000\\right)$\n   - Test C: $\\left(64, 8, 1, 300, 20, 1.0, 0.5, 5000\\right)$\n   - Test D: $\\left(64, 4, 1, 200, 15, 0.9, 0.5, 5000\\right)$\n   - Test E: $\\left(64, 8, 1, 200, 15, 0.2, 0.1, 1000\\right)$\n\nYour program must:\n- For each test, compute $E = \\left\\lfloor \\frac{L}{s} \\right\\rfloor$ and enforce $E \\ge 1$.\n- Evaluate $T_{\\mathrm{bst}}(n)$ and $T_{\\mathrm{arr}}(n)$ for all integers $n$ from $1$ to $N_{\\max}$, using your derived expressions, and return the largest $n$ where the array scan is faster.\n- Aggregate the five results in a single Python-style list literal on one line, in the exact order of Tests A through E; for example, a valid format is $\\left[\\dots\\right]$ with comma separation like $\\left[3,7,0,42,11\\right]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the five integers $\\left[n_A, n_B, n_C, n_D, n_E\\right]$, where each $n_{\\cdot}$ is the corresponding $n_{\\max}^{\\star}$ for that test case, printed as a comma-separated list enclosed in square brackets, with no additional characters or lines.\n\nNo physical units or angles are involved. Probabilities must be treated as real numbers in the closed interval $\\left[0,1\\right]$. All computed quantities and constants are pure numbers measured in CPU cycles for costs, consistent with the cycle-based model specified above.", "solution": "### Step 1: Derivation of the Cost Models\n\nThe first task is to derive the expected cycle count per unsuccessful query for the Balanced Binary Search Tree (BST) and the sorted array.\n\n**Balanced Binary Search Tree (BST) Cost Model: $T_{\\mathrm{bst}}(n)$**\n\nAn unsuccessful membership query in a balanced BST involves traversing a path from the root to a leaf. The problem defines the number of nodes visited along this path as $h = \\lceil \\log_2(n + 1) \\rceil$, where $n$ is the number of keys in the set.\n\nFor each of the $h$ nodes visited, three distinct costs are incurred. The problem states that the probabilistic events (cache miss and branch misprediction) are independent for each node visit.\n\n1.  **Comparison Cost**: A key comparison is performed at each node. This cost is deterministic, contributing $c_{\\mathrm{cmp}}$ cycles per node.\n2.  **Cache Miss Cost**: A cache miss occurs with probability $p_{\\mathrm{miss}}^{\\mathrm{bst}}$, and each miss adds $c_{\\mathrm{miss}}$ cycles to the total cost. By the definition of expected value, the expected cost from cache misses per node visit is $p_{\\mathrm{miss}}^{\\mathrm{bst}} \\times c_{\\mathrm{miss}}$.\n3.  **Branch Misprediction Cost**: A branch misprediction occurs with probability $p_{\\mathrm{bm}}^{\\mathrm{bst}}$, and each misprediction adds $c_{\\mathrm{bm}}$ cycles. The expected cost from branch mispredictions per node visit is $p_{\\mathrm{bm}}^{\\mathrm{bst}} \\times c_{\\mathrm{bm}}$.\n\nThe total expected cost to process a single node, denoted $C_{\\mathrm{node}}$, is the sum of these individual costs:\n$$ C_{\\mathrm{node}} = c_{\\mathrm{cmp}} + p_{\\mathrm{miss}}^{\\mathrm{bst}} c_{\\mathrm{miss}} + p_{\\mathrm{bm}}^{\\mathrm{bst}} c_{\\mathrm{bm}} $$\n\nBy the principle of linearity of expectation, the total expected cost for the entire query, which involves visiting $h$ independent nodes, is the product of the number of nodes visited and the expected cost per node.\n\nTherefore, the total expected cycle count for an unsuccessful query in the BST is:\n$$ T_{\\mathrm{bst}}(n) = h \\times C_{\\mathrm{node}} = \\lceil \\log_2(n + 1) \\rceil \\left( c_{\\mathrm{cmp}} + p_{\\mathrm{miss}}^{\\mathrm{bst}} c_{\\mathrm{miss}} + p_{\\mathrm{bm}}^{\\mathrm{bst}} c_{\\mathrm{bm}} \\right) $$\n\n**Sorted Array with Linear Scan Cost Model: $T_{\\mathrm{arr}}(n)$**\n\nFor an unsuccessful query, the sorted array is scanned from the beginning, examining all $n$ elements in an adversarial scenario.\n\n1.  **Comparison Cost**: The scan performs $n$ key comparisons, one for each element. With each comparison costing $c_{\\mathrm{cmp}}$ cycles, the total comparison cost is $n \\times c_{\\mathrm{cmp}}$.\n\n2.  **Cache Miss Cost**: The array elements are stored contiguously in memory. The cache line size is $L$ bytes, and each element has a size of $s$ bytes. The number of elements that fit into a single cache line is $E = \\left\\lfloor \\frac{L}{s} \\right\\rfloor$.\n   For a linear (streaming) scan of $n$ elements, a cache miss is incurred only for the first element accessed within each new cache line. The total number of cache lines touched by the scan is $\\left\\lceil \\frac{n}{E} \\right\\rceil$. Since each cache miss costs $c_{\\mathrm{miss}}$ cycles, the total cost from cache misses is $\\left\\lceil \\frac{n}{E} \\right\\rceil \\times c_{\\mathrm{miss}}$.\n\n3.  **Branch Misprediction Cost**: The problem specifies that the linear scan is implemented in a \"branchless\" fashion, a common optimization to avoid the cost of conditional jumps. Therefore, the cost from branch mispredictions is $0$.\n\nThe total cost for the array scan is the sum of the comparison and cache miss costs.\n\nTherefore, the total cycle count for an unsuccessful query using the array scan is:\n$$ T_{\\mathrm{arr}}(n) = n \\cdot c_{\\mathrm{cmp}} + \\left\\lceil \\frac{n}{E} \\right\\rceil c_{\\mathrm{miss}} \\quad \\text{where} \\quad E = \\left\\lfloor \\frac{L}{s} \\right\\rfloor $$\n\n### Step 2: Formal Problem Definition and Solution Strategy\n\nBased on the derivations above, the two cost functions are:\n$$ T_{\\mathrm{bst}}(n) = \\lceil \\log_2(n + 1) \\rceil (c_{\\mathrm{cmp}} + p_{\\mathrm{miss}}^{\\mathrm{bst}} c_{\\mathrm{miss}} + p_{\\mathrm{bm}}^{\\mathrm{bst}} c_{\\mathrm{bm}}) $$\n$$ T_{\\mathrm{arr}}(n) = n \\cdot c_{\\mathrm{cmp}} + \\lceil n / \\lfloor L/s \\rfloor \\rceil c_{\\mathrm{miss}} $$\n\nThe objective is to find $n_{\\max}^{\\star}$, defined as the largest integer $n$ in the range $\\{1, 2, \\dots, N_{\\max}\\}$ for which the array scan is faster than the BST search. Formally:\n$$ n_{\\max}^{\\star} = \\max \\{ n \\in \\{1, 2, \\dots, N_{\\max}\\} \\mid T_{\\mathrm{arr}}(n) < T_{\\mathrm{bst}}(n) \\} $$\nIf this set is empty (i.e., the condition is never met for any $n$ in the range), $n_{\\max}^{\\star}$ is defined as $0$.\n\nTo compute $n_{\\max}^{\\star}$ for each given parameter set, we will implement an iterative search. For each test case, we initialize a variable for the result, say `n_star_max`, to $0$. We then iterate with $n$ from $1$ to $N_{\\max}$. In each iteration, we calculate the values of $T_{\\mathrm{arr}}(n)$ and $T_{\\mathrm{bst}}(n)$ using the derived formulas and the specific parameters for that test. If the condition $T_{\\mathrm{arr}}(n) < T_{\\mathrm{bst}}(n)$ is satisfied, we update `n_star_max` to the current value of $n$. After the loop completes, `n_star_max` will hold the largest value of $n$ for which the condition was true, which is the desired $n_{\\max}^{\\star}$. This procedure is implemented in the provided Python code.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for n_max_star for five different parameter sets,\n    where n_max_star is the largest number of keys 'n' for which\n    a linear array scan outperforms a balanced binary search tree search.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (L, s, c_cmp, c_miss, c_bm, p_miss_bst, p_bm_bst, N_max)\n    test_cases = [\n        (64, 8, 1, 200, 15, 0.9, 0.5, 1000),  # Test A\n        (64, 8, 1, 30, 5, 0.9, 0.5, 1000),   # Test B\n        (64, 8, 1, 300, 20, 1.0, 0.5, 5000),  # Test C\n        (64, 4, 1, 200, 15, 0.9, 0.5, 5000),  # Test D\n        (64, 8, 1, 200, 15, 0.2, 0.1, 1000),  # Test E\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        L, s, c_cmp, c_miss, c_bm, p_miss_bst, p_bm_bst, N_max = case\n\n        # Calculate elements per cache line (E).\n        # The problem requires enforcing E >= 1, but with the given positive L and s,\n        # np.floor automatically handles it, and it will be >= 1 for test data.\n        E = np.floor(L / s)\n        if E < 1:\n            # Per problem spec, this case should be handled, though not expected with test data.\n            # If E < 1, array scan is infinitely costly. In this hypothetical, BST always wins.\n            results.append(0)\n            continue\n            \n        # Calculate the constant per-node cost for the BST.\n        cost_per_bst_node = c_cmp + p_miss_bst * c_miss + p_bm_bst * c_bm\n\n        n_star_max = 0\n        \n        # Iterate n from 1 to N_max to find the crossover point.\n        for n in range(1, N_max + 1):\n            # Calculate cost for balanced BST\n            h = np.ceil(np.log2(n + 1))\n            T_bst = h * cost_per_bst_node\n            \n            # Calculate cost for sorted array with linear scan\n            num_cache_misses = np.ceil(n / E)\n            T_arr = n * c_cmp + num_cache_misses * c_miss\n            \n            # Check if the array scan is faster\n            if T_arr < T_bst:\n                # Update the maximum n for which this condition holds\n                n_star_max = n\n        \n        results.append(n_star_max)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3202574"}]}