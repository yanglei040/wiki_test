{"hands_on_practices": [{"introduction": "Reference counting is an appealing approach to memory management due to its immediacy—memory is reclaimed as soon as an object becomes unreachable. However, its classic weakness is its inability to collect \"islands\" of cyclic garbage where objects refer to each other but are cut off from the rest of the program. This practice challenges you to implement a cycle detection algorithm based on trial deletion, a technique that isolates and identifies these unreachable cyclic structures by treating it as a graph reachability problem [@problem_id:3236414]. By implementing this, you will gain a deep, algorithmic understanding of how to solve the primary correctness issue in reference-counting systems.", "problem": "You are given the conceptual framework of reference counting in automatic memory management and the challenge of collecting cyclic garbage when using reference counts alone. Start from the following foundational base: a heap object graph is modeled as a directed graph $G = (V, E)$ with $|V| = n$, where each vertex $v \\in V$ is an object, and each directed edge $(u,v) \\in E$ is a reference from object $u$ to object $v$. The reference count $rc(v)$ of an object $v$ equals the total number of references to $v$ from all objects plus the number of references from outside the heap (for example, from roots such as stacks or registers), which are modeled as an external-reference vector $ext: V \\to \\mathbb{N}$. A set of candidate objects $S \\subseteq V$ is provided by the mutator as “suspect” objects that may participate in cycles.\n\nUsing only these base notions, derive the semantics of trial deletion restricted to $S$: it must conservatively detect precisely those objects in $S$ that are not reachable from outside $S$. Formally, let $ext\\_in(v)$ denote the number of references into $v$ from outside $S$, given by $ext\\_in(v) = ext(v) + |\\{(u,v) \\in E \\mid u \\notin S\\}|$. Define the base set $B = \\{ v \\in S \\mid ext\\_in(v)  0 \\}$ and the survivor set $R \\subseteq S$ as all vertices in $S$ reachable from $B$ by following only edges within $S$. The recyclable garbage set is $G_S = S \\setminus R$. Your program must implement these semantics algorithmically and must behave as a cycle detector for a reference counting system that uses trial deletion on the limited candidate set $S$.\n\nInput format and execution model: There is no runtime input. Instead, your program must hard-code a small suite of test cases. Each test case is a quadruple $(n, E, ext, S)$, where $n$ is the number of objects labeled as integers from $0$ to $n-1$, $E$ is a list of ordered pairs $(u,v)$, $ext$ is a list of length $n$ giving $ext(i)$ for $i \\in \\{0,\\dots,n-1\\}$, and $S$ is a list of candidate integers. For each test case, compute the sorted list of object identifiers in $G_S$.\n\nTest suite:\n- Test case $1$: $n = 5$, $E = [(0,1),(1,2),(2,0),(2,3)]$, $ext = [0,0,0,1,0]$, $S = [0,1,2]$.\n- Test case $2$: $n = 3$, $E = [(0,1)]$, $ext = [0,0,0]$, $S = []$.\n- Test case $3$: $n = 3$, $E = [(0,1),(1,2)]$, $ext = [1,0,0]$, $S = [0,1,2]$.\n- Test case $4$: $n = 3$, $E = [(0,1),(1,2),(2,0)]$, $ext = [0,0,0]$, $S = [0,1]$.\n- Test case $5$: $n = 1$, $E = [(0,0)]$, $ext = [0]$, $S = [0]$.\n- Test case $6$: $n = 6$, $E = [(0,1),(1,2),(2,0),(3,4),(4,5),(5,3)]$, $ext = [0,1,0,0,0,0]$, $S = [0,1,2,3,4,5]$.\n\nRequirements:\n- Derive and implement a correct and efficient algorithm consistent with trial deletion semantics limited to $S$ as defined using the foundational notions above. Your program must not rely on any external input.\n- For each test case, output the sorted list of object identifiers that would be reclaimed by trial deletion on $S$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no whitespace. Each element is the per-test-case sorted list. For example, the final output should look like $[[\\dots],[\\dots],\\dots]$.\n\nAnswer specification:\n- For each test case, the answer is a list of integers (possibly empty).\n- The single-line output must aggregate the lists for all test cases in the order they are presented above and contain no whitespace characters anywhere in the line.", "solution": "The user-provided problem statement has been analyzed and validated against the specified criteria.\n\n### Step 1: Extract Givens\n\n-   **Heap Model**: A directed graph $G = (V, E)$ with $|V| = n$ vertices (objects) and a set of directed edges $E$ (references).\n-   **Object Identifiers**: Integers from $0$ to $n-1$.\n-   **External References**: A vector $ext: V \\to \\mathbb{N}$, where $ext(v)$ is the number of references to object $v$ from outside the heap.\n-   **Candidate Set**: A subset of objects $S \\subseteq V$, suspected to be part of garbage cycles.\n-   **External-in References to S**: For an object $v \\in S$, the number of references from outside $S$ is defined as $ext\\_in(v) = ext(v) + |\\{(u,v) \\in E \\mid u \\notin S\\}|$.\n-   **Base Set**: $B = \\{ v \\in S \\mid ext\\_in(v)  0 \\}$. These are objects in $S$ that are directly reachable from outside $S$.\n-   **Survivor Set**: $R \\subseteq S$, defined as the set of all vertices in $S$ that are reachable from the base set $B$ by paths consisting entirely of edges within $S$.\n-   **Recyclable Garbage Set**: $G_S = S \\setminus R$.\n-   **Task**: For a given suite of test cases, compute the sorted list of object identifiers in $G_S$.\n-   **Test Suite**:\n    1.  Test case 1: $n = 5$, $E = [(0,1),(1,2),(2,0),(2,3)]$, $ext = [0,0,0,1,0]$, $S = [0,1,2]$.\n    2.  Test case 2: $n = 3$, $E = [(0,1)]$, $ext = [0,0,0]$, $S = []$.\n    3.  Test case 3: $n = 3$, $E = [(0,1),(1,2)]$, $ext = [1,0,0]$, $S = [0,1,2]$.\n    4.  Test case 4: $n = 3$, $E = [(0,1),(1,2),(2,0)]$, $ext = [0,0,0]$, $S = [0,1]$.\n    5.  Test case 5: $n = 1$, $E = [(0,0)]$, $ext = [0]$, $S = [0]$.\n    6.  Test case 6: $n = 6$, $E = [(0,1),(1,2),(2,0),(3,4),(4,5),(5,3)]$, $ext = [0,1,0,0,0,0]$, $S = [0,1,2,3,4,5]$.\n-   **Output Format**: A single-line string `[[...],[...],...]` representing the list of results for all test cases, with no whitespace.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is evaluated against the validation criteria:\n\n-   **Scientifically Grounded**: The problem is well-grounded in the field of computer science, specifically in the domain of data structures and algorithms concerning automatic memory management (garbage collection). The graph-based model of object heaps, the concept of reference counting, and the problem of cyclic garbage are fundamental topics in this field. The \"trial deletion\" semantics are rigorously defined and represent a valid approach to cycle detection. The problem is directly relevant to the topic of *garbage collection concepts*.\n-   **Well-Posed**: The problem is mathematically well-posed. All terms ($G, E, S, ext, ext\\_in, B, R, G_S$) are defined unambiguously. The objective—to compute $G_S$—is clear. The process involves standard graph algorithms (reachability), for which unique and stable solutions exist. All necessary data are provided for each test case.\n-   **Objective**: The problem statement uses precise, formal, and objective language, free from any subjectivity or ambiguity.\n\nThe problem does not exhibit any of the listed flaws (Scientific Unsoundness, Non-Formalizable, Incomplete Setup, Unrealistic, Ill-Posed, Pseudo-Profound, Outside Verifiability).\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be derived and implemented.\n\n### Derivation of the Algorithm\n\nThe task is to compute the recyclable garbage set $G_S = S \\setminus R$ for each test case. The algorithm directly implements the formal definitions provided. The process is decomposed into three main computational steps.\n\n1.  **Computation of the Base Set $B$**: The base set $B$ consists of all objects in the candidate set $S$ that are reachable from outside $S$. An object is reachable from outside $S$ if it has a non-zero external reference count ($ext(v) > 0$) or if there is at least one reference to it from an object not in $S$. This corresponds to the condition $ext\\_in(v)  0$.\n\n    To compute $B$, we first must calculate $ext\\_in(v)$ for every $v \\in S$. The algorithm proceeds as follows:\n    - For each object $v \\in S$, initialize a counter for $ext\\_in(v)$ with the value from the external reference vector, $ext(v)$.\n    - Iterate through all edges $(u, w) \\in E$. If the source $u$ is not in $S$ ($u \\notin S$) and the destination $w$ is in $S$ ($w \\in S$), increment the counter for $ext\\_in(w)$.\n    - After evaluating all edges, the base set $B$ is formed by collecting all objects $v \\in S$ for which the final computed $ext\\_in(v)$ is greater than $0$.\n\n2.  **Computation of the Survivor Set $R$**: The survivor set $R$ contains all objects in $S$ that are reachable from the base set $B$ using paths that lie entirely within $S$. This is a classical graph reachability problem on the subgraph induced by $S$. The nodes of this subgraph are the objects in $S$, and its edges are all edges $(u, v) \\in E$ where both $u$ and $v$ are in $S$.\n\n    An efficient algorithm to find all reachable nodes from a starting set of nodes ($B$) is the Breadth-First Search (BFS). The algorithm proceeds as follows:\n    - Initialize the survivor set $R$ as a copy of the base set $B$.\n    - Initialize a queue for the traversal with all elements of $B$.\n    - While the queue is not empty:\n        - Dequeue an object $u$.\n        - For each object $v$ that $u$ references (i.e., for each edge $(u, v) \\in E$):\n            - If $v$ is in the candidate set $S$ and has not yet been added to the survivor set $R$, add $v$ to $R$ and enqueue it.\n    - Upon termination of the BFS, the set $R$ contains precisely all objects in $S$ reachable from $B$ through paths within $S$.\n\n3.  **Computation of the Recyclable Garbage Set $G_S$**: The problem defines the recyclable garbage set as $G_S = S \\setminus R$. This is the set of objects that are in the candidate set $S$ but are not in the survivor set $R$. These are the objects in $S$ that are not reachable from any external source, either directly or indirectly through a path of other survivor objects.\n\n    This final step is a simple set difference operation. The resulting set of object identifiers is then sorted in ascending order as required by the problem specification.\n\nThis three-step procedure correctly and efficiently implements the specified semantics of trial deletion on the candidate set $S$. The implementation will apply this logic to each of the provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the cyclic garbage collection problem based on trial deletion\n    semantics for a predefined suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: A simple cycle [0,1,2] that is garbage.\n        {\n            \"n\": 5,\n            \"E\": [(0, 1), (1, 2), (2, 0), (2, 3)],\n            \"ext\": [0, 0, 0, 1, 0],\n            \"S\": [0, 1, 2]\n        },\n        # Test case 2: Empty candidate set.\n        {\n            \"n\": 3,\n            \"E\": [(0, 1)],\n            \"ext\": [0, 0, 0],\n            \"S\": []\n        },\n        # Test case 3: A chain kept alive by an external reference.\n        {\n            \"n\": 3,\n            \"E\": [(0, 1), (1, 2)],\n            \"ext\": [1, 0, 0],\n            \"S\": [0, 1, 2]\n        },\n        # Test case 4: A cycle where S is a subset of the cycle, and an\n        #              external reference (from node 2) saves the S-subset.\n        {\n            \"n\": 3,\n            \"E\": [(0, 1), (1, 2), (2, 0)],\n            \"ext\": [0, 0, 0],\n            \"S\": [0, 1]\n        },\n        # Test case 5: A single-node cycle with no external refs.\n        {\n            \"n\": 1,\n            \"E\": [(0, 0)],\n            \"ext\": [0],\n            \"S\": [0]\n        },\n        # Test case 6: Two disjoint cycles. One is saved by an ext ref, one is not.\n        {\n            \"n\": 6,\n            \"E\": [(0, 1), (1, 2), (2, 0), (3, 4), (4, 5), (5, 3)],\n            \"ext\": [0, 1, 0, 0, 0, 0],\n            \"S\": [0, 1, 2, 3, 4, 5]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        n, E, ext, S = case[\"n\"], case[\"E\"], case[\"ext\"], case[\"S\"]\n        \n        # For efficient lookups, convert S to a set.\n        S_set = set(S)\n        if not S_set:\n            results.append([])\n            continue\n\n        # Step 1: Compute the Base Set B\n        # B = { v in S | ext_in(v)  0 }\n        # where ext_in(v) = ext(v) + |{(u,v) in E | u not in S}|\n        \n        ext_in = {v: ext[v] for v in S_set}\n        \n        # To find references from outside S, we can build a predecessor graph\n        # or iterate through all edges. Iterating through edges is straightforward.\n        for u, v in E:\n            if v in S_set and u not in S_set:\n                ext_in[v] += 1\n        \n        B = {v for v, count in ext_in.items() if count  0}\n        \n        # Step 2: Compute the Survivor Set R\n        # R is the set of all vertices in S reachable from B using only edges within S.\n        # This is a graph traversal (BFS) starting from all nodes in B.\n        \n        adj = {i: [] for i in range(n)}\n        for u, v in E:\n            adj[u].append(v)\n            \n        R = set(B)\n        queue = list(B)\n        head = 0\n        \n        while head  len(queue):\n            u = queue[head]\n            head += 1\n            \n            for v in adj[u]:\n                # Traversal is restricted to paths within S\n                if v in S_set and v not in R:\n                    R.add(v)\n                    queue.append(v)\n\n        # Step 3: Compute the Recyclable Garbage Set G_S = S \\ R\n        G_S = S_set - R\n        \n        # Sort the results as required by the problem statement.\n        sorted_garbage = sorted(list(G_S))\n        results.append(sorted_garbage)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists, with no whitespace.\n    print(str(results).replace(' ', ''))\n\nsolve()\n```", "id": "3236414"}, {"introduction": "Modern garbage collectors heavily rely on the \"weak generational hypothesis,\" the empirical observation that most objects have very short lifetimes. This allows for significant efficiency gains by segregating objects by age and collecting the \"young\" generation more frequently. This simulation exercise invites you to explore what happens when this assumption breaks down [@problem_id:3236439]. You will model a two-generation collector and test it against a workload specifically designed to cause premature promotions, revealing the performance costs of frequent major collections and making the theoretical basis for generational GC tangible.", "problem": "You must write a complete, runnable program that simulates a simplified two-generation Garbage Collector (GC) and evaluates a concrete allocation pattern that intentionally violates the weak generational hypothesis. The weak generational hypothesis asserts that most objects die young; the violating pattern instead produces many medium-lived objects that survive enough minor collections to be promoted, and then die soon after in the old generation, thereby forcing inefficient promotions and costly major collections.\n\nStart from the following foundational base:\n- Garbage Collector (GC) terminology and core definitions:\n  - A generational GC partitions heap objects into disjoint generations. The young generation is collected frequently (minor collections), the old generation less frequently (major collections).\n  - Promotion occurs when an object survives a prescribed number of minor collections.\n  - A collection scans a set of objects to identify which are dead (unreachable) and reclaims them.\n- The weak generational hypothesis is an empirical observation that most objects die young.\n- Mark-and-sweep collection scans and removes dead objects without moving survivors; in this problem, objects are abstract and reachability is modeled deterministically by a lifetime counter, not by graph traversal.\n\nModel to implement:\n- Heap model:\n  - Two generations: young and old.\n  - Capacities: young capacity $C_y$ and old capacity $C_o$ measured in object counts.\n- Object model:\n  - Each object has:\n    - Remaining lifetime $L \\in \\mathbb{Z}_{\\ge 0}$ in time steps. An object is considered dead when $L \\le 0$.\n    - Minor-survival age $a \\in \\mathbb{Z}_{\\ge 0}$, the number of minor collections it has survived while resident in the young generation.\n    - A generation tag, either young or old.\n- Allocation pattern:\n  - At each discrete time step $t \\in \\{1,2,\\dots,T\\}$, exactly $A$ new objects are allocated into the young generation.\n  - The $i$-th object allocated in a test case receives a deterministic lifetime assigned by cycling through the inclusive integer range $[L_{\\min}, L_{\\max}]$: the $k$-th allocation in the whole run is assigned\n    $$L = L_{\\min} + \\left((k - 1) \\bmod (L_{\\max} - L_{\\min} + 1)\\right).$$\n  - This cycling removes randomness and makes the simulation deterministic.\n- Time and GC events per step (strict order):\n  1. Allocate $A$ new young objects, as above.\n  2. While the number of young objects exceeds $C_y$, run a minor collection:\n     - Scan all current young objects (costed, see below).\n     - Remove dead young objects (those with $L \\le 0$).\n     - For each remaining young object, increment its minor-survival age $a \\leftarrow a + 1$.\n     - If $a \\ge p$ (promotion threshold), promote the object to the old generation and reset its age bookkeeping (you may discard $a$ after promotion).\n     - Count every promotion.\n  3. Decrement $L \\leftarrow L - 1$ for every object in both generations (this models the passage of one time step).\n  4. While the number of old objects exceeds $C_o$, run a major collection:\n     - Scan all objects in both generations (costed, see below).\n     - Remove dead old objects (those with $L \\le 0$).\n     - Young objects are not promoted by major collections in this model.\n- Cost model:\n  - Assign scalar costs to approximate work:\n    - Minor collection cost is $\\alpha$ times the number of young objects scanned at the start of the collection.\n    - Major collection cost is $\\beta$ times the total number of objects scanned across both generations at the start of the collection.\n    - Promotion cost is $\\gamma$ per promoted object.\n  - Total GC cost is the sum of all scan costs and promotion costs over the entire run.\n- Inefficiency index:\n  - Define the inefficiency index as\n    $$I = \\frac{\\text{total GC cost}}{\\text{total allocations}} = \\frac{\\text{total GC cost}}{T \\cdot A}.$$\n\nConcrete pattern that violates the weak generational hypothesis:\n- Choose lifetimes so that most objects live just long enough to survive enough minor collections to be promoted (by meeting or exceeding the promotion threshold $p$), but then die shortly after in the old generation. This creates frequent promotions and costly major collections because many promoted objects do not remain long-lived.\n- The frequency of minor collections is governed by load; if the young generation fills after approximately $s = \\left\\lceil \\frac{C_y}{A} \\right\\rceil$ steps, then setting lifetimes near $p \\cdot s$ ensures many objects will be promoted and soon die. In the test suite below, the provided $(L_{\\min}, L_{\\max})$ values are chosen to concretely instantiate this pattern.\n\nConstants to use across all tests:\n- Minor scan weight $\\alpha = 1.0$.\n- Major scan weight $\\beta = 5.0$.\n- Promotion cost $\\gamma = 0.2$.\n\nYour task:\n- Implement the simulator exactly as specified.\n- For each test case, compute the inefficiency index $I$ defined above.\n- Your program must execute with no input and must print a single line containing a list of decimal numbers in the specified order.\n\nTest suite:\n- Each test case is given as a tuple $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max})$.\n- Use the following four test cases that exercise different behaviors:\n  1. Happy-path violation of the weak generational hypothesis:\n     - $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (100, 500, 2, 30, 200, 9, 10)$.\n  2. Conforming workload where most objects die young:\n     - $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (100, 500, 2, 30, 200, 1, 3)$.\n  3. Boundary case with extremely frequent minor collections and immediate promotions:\n     - $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (30, 60, 1, 40, 120, 2, 2)$.\n  4. Large old-generation capacity delaying major collections even under violating pattern:\n     - $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (50, 10000, 2, 25, 300, 5, 6)$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the inefficiency indices for the four test cases as a comma-separated list enclosed in square brackets, in the same order as listed above. For example, the format must look like\n  $$[I_1, I_2, I_3, I_4].$$\n- Each $I_k$ must be printed as a decimal number. You may format them to a fixed number of fractional digits if you wish, but they must be valid decimal numerals.", "solution": "The problem statement is valid. It presents a well-defined, self-contained, and scientifically grounded simulation task within the domain of computer science, specifically concerning memory management algorithms. The parameters, rules, and objectives are specified with sufficient precision to permit a unique, deterministic solution.\n\n### Step 1: Extraction of Givens\n\n- **Generational GC Model**: A garbage collector (GC) with two generations: young and old.\n- **Heap Capacities**: Young generation capacity $C_y$, old generation capacity $C_o$.\n- **Object State**: Each object possesses a remaining lifetime $L \\in \\mathbb{Z}_{\\ge 0}$, a minor-survival age $a \\in \\mathbb{Z}_{\\ge 0}$, and a generation tag (young or old). An object is dead if $L \\le 0$.\n- **Promotion Threshold**: An object is promoted from the young to the old generation if its minor-survival age $a$ meets or exceeds the threshold $p$, i.e., $a \\ge p$.\n- **Simulation Time**: The simulation runs for $T$ discrete time steps, from $t=1$ to $t=T$.\n- **Allocation Pattern**: At each time step $t$, $A$ new objects are allocated into the young generation.\n- **Lifetime Assignment**: The $k$-th object allocated during the simulation is assigned a lifetime $L = L_{\\min} + \\left((k - 1) \\bmod (L_{\\max} - L_{\\min} + 1)\\right)$.\n- **Order of Operations per Time Step**:\n    1.  Allocate $A$ new objects into the young generation.\n    2.  Execute minor collections in a loop as long as the number of young objects exceeds $C_y$.\n    3.  Decrement the lifetime $L$ of all objects in both generations by $1$.\n    4.  Execute major collections in a loop as long as the number of old objects exceeds $C_o$.\n- **Minor Collection Process**:\n    - The cost incurred is $\\alpha$ times the number of objects in the young generation at the start of the collection.\n    - All dead objects ($L \\le 0$) in the young generation are removed.\n    - The minor-survival age $a$ of each surviving young object is incremented.\n    - Surviving young objects with $a \\ge p$ are promoted to the old generation. The cost of each promotion is $\\gamma$.\n- **Major Collection Process**:\n    - The cost incurred is $\\beta$ times the total number of objects in both generations at the start of the collection.\n    - All dead objects ($L \\le 0$) in the old generation are removed.\n- **Cost Model Constants**:\n    - Minor scan weight: $\\alpha = 1.0$.\n    - Major scan weight: $\\beta = 5.0$.\n    - Promotion cost: $\\gamma = 0.2$.\n- **Inefficiency Index**: The metric for evaluation is $I = \\frac{\\text{total GC cost}}{\\text{total allocations}} = \\frac{\\text{total GC cost}}{T \\cdot A}$.\n- **Test Suite**:\n    1.  $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (100, 500, 2, 30, 200, 9, 10)$.\n    2.  $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (100, 500, 2, 30, 200, 1, 3)$.\n    3.  $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (30, 60, 1, 40, 120, 2, 2)$.\n    4.  $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (50, 10000, 2, 25, 300, 5, 6)$.\n\n### Step 2: Validation of Givens\n\nThe provided problem statement is valid.\n- **Scientifically Grounded**: The model is a simplified but standard representation of a two-generation garbage collector, a fundamental concept in systems programming and computer science. The weak generational hypothesis is a well-established empirical principle that motivates this design. The problem is a deterministic simulation based on these ideas.\n- **Well-Posed**: All parameters and operational rules are explicitly defined, ensuring that the state of the simulation at each step is unique and computable. The problem is structured to yield a single, meaningful numerical result ($I$) for each test case.\n- **Objective**: The problem uses precise, technical language and avoids subjectivity. All quantities are formally defined.\n\nThe problem formulation exhibits none of the specified flaws (e.g., scientific unsoundness, incompleteness, ambiguity). It is a formal, solvable problem.\n\n### Step 3: Solution Procedure\n\nA deterministic simulation will be implemented according to the specified model. The state of the system at any time is defined by the set of objects in the young and old generations, their respective lifetimes and ages, and the cumulative GC cost.\n\n**State Representation**\nThe heap will be represented by two collections of objects, one for the young generation and one for the old. Each object will be an instance of a class encapsulating its state: its remaining lifetime $L$ and its minor-survival age $a$.\n\n- `young_generation`: A list of objects in the young generation.\n- `old_generation`: A list of objects in the old generation.\n- `total_gc_cost`: A floating-point number tracking the cumulative cost.\n- `total_allocations_count`: An integer $k$ tracking the total number of objects allocated so far, starting from $1$.\n\n**Simulation Algorithm**\nThe simulation proceeds through discrete time steps $t$ from $1$ to $T$.\n\nFor each test case with parameters $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max})$:\n1.  Initialize `young_generation` and `old_generation` to be empty. Initialize `total_gc_cost = 0.0` and `total_allocations_count = 0$.\n2.  Begin the main loop: for $t$ from $1$ to $T$:\n    a.  **Allocation**:\n        - Generate $A$ new objects. For each $j \\in \\{1, \\dots, A\\}$:\n            - Increment `total_allocations_count`. Let the current value be $k$.\n            - Calculate the object lifetime: $L = L_{\\min} + \\left((k - 1) \\bmod (L_{\\max} - L_{\\min} + 1)\\right)$.\n            - Create a new object with this lifetime $L$ and initial age $a=0$.\n            - Add the object to `young_generation`.\n\n    b.  **Minor Collection Phase**:\n        - Loop `while` the number of objects in `young_generation` is greater than $C_y$:\n            i.   Add $\\alpha \\times |\\text{young\\_generation}|$ to `total_gc_cost`.\n            ii.  Create three temporary lists: `survivors`, `promoted`, `next_young_generation`.\n            iii. For each object in `young_generation`:\n                 - If its lifetime $L > 0$, it is a survivor. Increment its age: $a \\leftarrow a + 1$.\n                 - If its new age $a \\ge p$, add it to the `promoted` list.\n                 - Otherwise (if $L > 0$ and $a  p$), add it to the `next_young_generation` list.\n            iv.  Add the `promoted` objects to `old_generation`.\n            v.   Add $\\gamma \\times |\\text{promoted}|$ to `total_gc_cost`.\n            vi.  Replace `young_generation` with `next_young_generation`.\n\n    c.  **Time Advancement**:\n        - For each object in `young_generation`, decrement its lifetime: $L \\leftarrow L - 1$.\n        - For each object in `old_generation`, decrement its lifetime: $L \\leftarrow L - 1$.\n\n    d.  **Major Collection Phase**:\n        - Loop `while` the number of objects in `old_generation` is greater than $C_o$:\n            i.   Add $\\beta \\times (|\\text{young\\_generation}| + |\\text{old\\_generation}|)$ to `total_gc_cost`.\n            ii.  Filter `old_generation` in place, keeping only objects with $L > 0$.\n\n3.  **Final Calculation**:\n    - After the loop over $T$ steps is complete, compute the inefficiency index:\n      $$I = \\frac{\\text{total\\_gc\\_cost}}{T \\cdot A}$$\n    - Store this value.\n\nThis procedure is executed for each of the four test cases provided, and the resulting indices are collected.", "answer": "```python\nimport numpy as np\n\n# Constants from the problem statement\nALPHA = 1.0  # Minor scan weight\nBETA = 5.0   # Major scan weight\nGAMMA = 0.2  # Promotion cost\n\nclass Object:\n    \"\"\"Represents an object in the heap with its state.\"\"\"\n    def __init__(self, lifetime):\n        self.lifetime = lifetime\n        self.age = 0  # Minor-survival age\n\ndef run_simulation(Cy, Co, p, A, T, Lmin, Lmax):\n    \"\"\"\n    Runs a single simulation for a given set of GC parameters.\n    \n    Args:\n        Cy (int): Young generation capacity.\n        Co (int): Old generation capacity.\n        p (int): Promotion threshold (age).\n        A (int): Allocations per time step.\n        T (int): Total time steps for the simulation.\n        Lmin (int): Minimum lifetime for allocated objects.\n        Lmax (int): Maximum lifetime for allocated objects.\n    \n    Returns:\n        float: The calculated inefficiency index.\n    \"\"\"\n    young_gen = []\n    old_gen = []\n    total_gc_cost = 0.0\n    total_allocations_count = 0\n    \n    lifetime_range_size = Lmax - Lmin + 1\n\n    for _ in range(1, T + 1):\n        # 1. Allocate A new objects\n        for _ in range(A):\n            total_allocations_count += 1\n            lifetime_offset = (total_allocations_count - 1) % lifetime_range_size\n            lifetime = Lmin + lifetime_offset\n            new_obj = Object(lifetime)\n            young_gen.append(new_obj)\n\n        # 2. Minor collection phase\n        while len(young_gen)  Cy:\n            # Add cost for scanning young generation\n            total_gc_cost += ALPHA * len(young_gen)\n            \n            survivors = []\n            promoted_count = 0\n            \n            # Identify survivors and objects to be promoted\n            for obj in young_gen:\n                if obj.lifetime  0:\n                    obj.age += 1\n                    if obj.age = p:\n                        # This object gets promoted. Resetting its age is not\n                        # strictly necessary as it's no longer used.\n                        old_gen.append(obj)\n                        promoted_count += 1\n                    else:\n                        survivors.append(obj)\n            \n            # Add promotion cost\n            total_gc_cost += GAMMA * promoted_count\n            \n            # Update the young generation\n            young_gen = survivors\n\n        # 3. Decrement lifetime for all objects\n        for obj in young_gen:\n            obj.lifetime -= 1\n        for obj in old_gen:\n            obj.lifetime -= 1\n\n        # 4. Major collection phase\n        while len(old_gen)  Co:\n            # Add cost for scanning both generations\n            total_gc_cost += BETA * (len(young_gen) + len(old_gen))\n            \n            # Reclaim dead objects in the old generation\n            old_gen = [obj for obj in old_gen if obj.lifetime  0]\n\n    # Calculate the inefficiency index\n    total_allocations = T * A\n    if total_allocations == 0:\n        return 0.0\n    \n    inefficiency_index = total_gc_cost / total_allocations\n    return inefficiency_index\n\ndef solve():\n    \"\"\"\n    Executes the simulation for all test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        # (Cy, Co, p, A, T, Lmin, Lmax)\n        (100, 500, 2, 30, 200, 9, 10),    # 1. Weak generational hypothesis violation\n        (100, 500, 2, 30, 200, 1, 3),     # 2. Conforming workload (objects die young)\n        (30, 60, 1, 40, 120, 2, 2),       # 3. Frequent GCs and immediate promotions\n        (50, 10000, 2, 25, 300, 5, 6),    # 4. Large old gen delaying major collections\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(*params)\n        results.append(result)\n\n    # Format output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3236439"}, {"introduction": "While garbage collection automates memory safety, it can introduce its own performance challenges, particularly with pause times. Object finalizers—code that runs before an object is reclaimed—can be a major source of unpredictable latency, especially if one finalizer can trigger another in a \"daisy chain.\" This exercise challenges you to move beyond deterministic algorithms and into the realm of probabilistic performance modeling [@problem_id:3236467]. You will derive and apply a model based on a truncated geometric distribution to quantify the expected, percentile, and worst-case GC pause times caused by finalizer chains, providing a crucial lesson in analyzing and bounding system latency.", "problem": "Consider a runtime that uses Garbage Collection (GC) with a stop-the-world mark-and-sweep collector. The collector performs a reachability analysis over the object graph, then sweeps unreachable objects. Some objects have finalizers that must run before their memory can be reclaimed. Finalizers are processed on a queue in First In, First Out (FIFO) order. Suppose that each finalizer task is capable of triggering exactly one additional finalizer task with an independent probability $q$, forming a \"daisy chain\" of finalizer tasks. Each finalizer task has a deterministic cost composed of an execution time $t_f$ and a queuing overhead $t_q$. Let $c = t_f + t_q$ denote the per-finalizer cost. Let the base pause time of the GC cycle without any finalizers be $T_0$. Let the maximum number of finalizers available to be chained during a GC cycle be a hard cap $F$, which is a function of the heap state and satisfies $F \\in \\mathbb{Z}_{\\ge 0}$. All time quantities must be expressed in milliseconds.\n\nStarting from the core definitions of mark-and-sweep and independent Bernoulli trials, model the number of finalizer tasks $L$ processed in a GC cycle as follows:\n- If $F = 0$, then $L = 0$.\n- Otherwise, there is at least one initial finalizer task. Each processed finalizer independently triggers exactly one further task with probability $q$. This produces a geometric \"continue-or-stop\" chain with truncation at $F$.\n\nFrom first principles, derive expressions in terms of $q$ and $F$ for:\n1. The expected number of finalizer tasks $\\mathbb{E}[L]$ under the truncated geometric chain model.\n2. The $p$-quantile chain length $L_p$ where $p \\in (0, 1)$, defined as the smallest $k \\in \\mathbb{Z}_{\\ge 0}$ such that $\\Pr(L \\le k) \\ge p$, with truncation at $F$.\n3. The worst-case chain length $L_{\\text{wc}}$ under the cap $F$.\n\nUse these to quantify GC latency:\n- The expected GC pause $T_{\\text{mean}} = T_0 + c \\cdot \\mathbb{E}[L]$.\n- The $p$-quantile GC pause $T_p = T_0 + c \\cdot L_p$.\n- The worst-case GC pause $T_{\\text{wc}} = T_0 + c \\cdot L_{\\text{wc}}$.\n\nImplement a complete, runnable program that computes, for each test case, the triple $\\left(T_{\\text{mean}}, T_p, T_{\\text{wc}}\\right)$ with $p = 0.99$ (Ninety-ninth percentile (P99)). The program must:\n- Use only the provided test suite parameters.\n- Produce its single-line output as a comma-separated list enclosed in square brackets, containing the sequence of all computed values across all test cases, in the order $\\left(T_{\\text{mean}}, T_{0.99}, T_{\\text{wc}}\\right)$ per test case, concatenated across cases.\n- Round each float to $6$ decimal places.\n\nYou must rely only on definitions and laws of probability for independent trials and do not introduce shortcut formulas that bypass the derivation.\n\nTest Suite:\n- Case $1$: $T_0 = 50.0$ ms, $t_f = 0.1$ ms, $t_q = 0.05$ ms, $F = 0$, $q = 0.5$.\n- Case $2$: $T_0 = 50.0$ ms, $t_f = 0.2$ ms, $t_q = 0.1$ ms, $F = 10$, $q = 0.0$.\n- Case $3$: $T_0 = 50.0$ ms, $t_f = 0.2$ ms, $t_q = 0.05$ ms, $F = 100$, $q = 0.5$.\n- Case $4$: $T_0 = 50.0$ ms, $t_f = 0.05$ ms, $t_q = 0.05$ ms, $F = 1000$, $q = 0.9$.\n- Case $5$: $T_0 = 10.0$ ms, $t_f = 0.005$ ms, $t_q = 0.005$ ms, $F = 100000$, $q = 0.99$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[ \\text{result}_1, \\text{result}_2, \\text{result}_3 \\right]$). The list must contain, in order, for each case, the values $T_{\\text{mean}}$, $T_{0.99}$, and $T_{\\text{wc}}$, each rounded to $6$ decimal places and expressed in milliseconds.", "solution": "The problem statement is critically evaluated for validity before a solution is attempted.\n\n### Step 1: Extract Givens\n- **System Model**: Stop-the-world mark-and-sweep garbage collector (GC).\n- **Finalizers**: Some objects have finalizers processed on a First In, First Out (FIFO) queue.\n- **Daisy Chain Probability**: A processed finalizer triggers one additional finalizer task with independent probability $q$.\n- **Cost**: Per-finalizer cost is $c = t_f + t_q$, where $t_f$ is execution time and $t_q$ is queuing overhead.\n- **Base Pause Time**: $T_0$ is the GC pause time without finalizers.\n- **Finalizer Cap**: $F \\in \\mathbb{Z}_{\\ge 0}$ is the maximum number of finalizers in a GC cycle.\n- **Time Units**: All time quantities are in milliseconds (ms).\n- **Finalizer Count Model ($L$)**:\n    - If $F = 0$, then $L = 0$.\n    - If $F  0$, there is at least one initial finalizer. The process forms a geometric \"continue-or-stop\" chain, truncated at $F$.\n- **Required Derivations**:\n    1. Expected number of finalizer tasks: $\\mathbb{E}[L]$.\n    2. The $p$-quantile chain length $L_p$, defined as the smallest $k \\in \\mathbb{Z}_{\\ge 0}$ such that $\\Pr(L \\le k) \\ge p$.\n    3. The worst-case chain length $L_{\\text{wc}}$.\n- **GC Latency Formulas**:\n    - Expected pause: $T_{\\text{mean}} = T_0 + c \\cdot \\mathbb{E}[L]$.\n    - $p$-quantile pause: $T_p = T_0 + c \\cdot L_p$.\n    - Worst-case pause: $T_{\\text{wc}} = T_0 + c \\cdot L_{\\text{wc}}$.\n- **Implementation Parameters**:\n    - Quantile: $p = 0.99$.\n    - Output: Compute the triple $(T_{\\text{mean}}, T_{0.99}, T_{\\text{wc}})$ for each test case, rounded to $6$ decimal places.\n- **Test Suite**:\n    - Case $1$: $T_0 = 50.0$, $t_f = 0.1$, $t_q = 0.05$, $F = 0$, $q = 0.5$.\n    - Case $2$: $T_0 = 50.0$, $t_f = 0.2$, $t_q = 0.1$, $F = 10$, $q = 0.0$.\n    - Case $3$: $T_0 = 50.0$, $t_f = 0.2$, $t_q = 0.05$, $F = 100$, $q = 0.5$.\n    - Case $4$: $T_0 = 50.0$, $t_f = 0.05$, $t_q = 0.05$, $F = 1000$, $q = 0.9$.\n    - Case $5$: $T_0 = 10.0$, $t_f = 0.005$, $t_q = 0.005$, $F = 100000$, $q = 0.99$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation against the specified criteria.\n- **Scientifically Grounded**: The problem is well-grounded in computer science (garbage collection) and probability theory (Bernoulli trials, geometric distributions). The model of finalizer chain reactions is a standard and useful simplification of a real-world phenomenon.\n- **Well-Posed**: The problem is well-posed. All variables ($T_0, t_f, t_q, F, q, p$) are clearly defined, and the objectives (derivation and calculation) are specific. The model for the random variable $L$ is unambiguously specified, allowing for a unique solution.\n- **Objective**: The problem is stated in precise, objective, and technical language, free from subjectivity or ambiguity.\n- **Completeness and Consistency**: The setup is complete and self-consistent. All parameters required for the derivations and computations are provided. The model for $L$ covers all cases of $F$ and is internally consistent.\n- **Realism**: The parameters provided in the test suite are within realistic bounds for modern computing systems. The model itself, while an abstraction, is plausible.\n- **Structure**: The problem is structured logically, first requesting theoretical derivations from first principles and then asking for the application of these derivations to concrete test cases.\n\n### Step 3: Verdict and Action\nThe problem is scientifically sound, well-posed, and complete. It is therefore deemed **valid**. A full solution will be provided.\n\n### Solution Derivations\nThe solution requires modeling the number of finalizer tasks, $L$, and deriving its expected value, quantile, and worst-case value.\n\n**1. Probability Model for Finalizer Chain Length $L$**\nThe number of finalizer tasks, $L$, is a random variable whose distribution depends on the parameters $F$ and $q$.\n\n- If $F=0$, no finalizers can be processed. Thus, $L=0$ with probability $1$.\n- If $F0$, the process starts with one guaranteed finalizer task. Each subsequent task is triggered with probability $q$. This is a sequence of independent Bernoulli trials. The chain stops either upon a \"failure\" (a task not triggering a new one, probability $1-q$) or when the total number of tasks reaches the cap $F$.\n\nLet's define the probability mass function (PMF) of $L$ for $F0$. The random variable $L$ takes values in $\\{1, 2, \\dots, F\\}$.\n- For the chain to have length $k$, where $1 \\le k  F$, we must have $k-1$ successful triggers followed by one failure. The probability of this sequence is $\\Pr(L=k) = q^{k-1}(1-q)$.\n- For the chain to have length $F$, we must have at least $F-1$ successful triggers. The process is capped at $F$, so any sequence of $F-1$ or more successes results in $L=F$. The probability of $F-1$ consecutive successes is $q^{F-1}$.\nThe PMF for $L$ given $F0$ is therefore:\n$$\n\\Pr(L=k) = \\begin{cases}\n(1-q)q^{k-1}  \\text{for } 1 \\le k  F \\\\\nq^{F-1}  \\text{for } k=F \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nThis is a truncated geometric distribution on $\\{1, 2, \\dots, F\\}$. One can verify that $\\sum_{k=1}^{F} \\Pr(L=k) = \\sum_{k=1}^{F-1} (1-q)q^{k-1} + q^{F-1} = (1-q)\\frac{1-q^{F-1}}{1-q} + q^{F-1} = 1-q^{F-1} + q^{F-1} = 1$ for $q \\neq 1$. If $q=1$, $\\Pr(L=F)=1$ and the sum is also $1$.\n\n**2. Derivation of the Expected Chain Length $\\mathbb{E}[L]$**\nThe expected value is derived from its definition.\n- If $F=0$, then $\\mathbb{E}[L]=0$.\n- If $F0$, a more elegant method than direct summation uses the property $\\mathbb{E}[X] = \\sum_{k=0}^{\\infty} \\Pr(Xk)$ for a non-negative integer-valued random variable $X$. For $L \\in \\{1, ..., F\\}$, this becomes $\\mathbb{E}[L] = \\sum_{k=0}^{F-1} \\Pr(Lk)$.\nThe event $\\{Lk\\}$ for $k \\in \\{1, \\dots, F-1\\}$ means that at least $k$ successful triggers occurred. The probability is $\\Pr(Lk) = q^k$. By definition, $\\Pr(L0)=1$.\n$$\n\\mathbb{E}[L] = \\sum_{k=0}^{F-1} \\Pr(Lk) = \\Pr(L0) + \\sum_{k=1}^{F-1} \\Pr(Lk) = 1 + \\sum_{k=1}^{F-1} q^k = \\sum_{k=0}^{F-1} q^k\n$$\nThis is a standard finite geometric series.\n- If $q  1$, the sum is $\\sum_{k=0}^{F-1} q^k = \\frac{1-q^F}{1-q}$.\n- If $q = 1$, the sum is $\\sum_{k=0}^{F-1} 1^k = F$.\nThis can be summarized as:\n$$\n\\mathbb{E}[L] = \\begin{cases}\n0  \\text{if } F=0 \\\\\n\\frac{1-q^F}{1-q}  \\text{if } F0, q1 \\\\\nF  \\text{if } F0, q=1\n\\end{cases}\n$$\n\n**3. Derivation of the $p$-Quantile Chain Length $L_p$**\n$L_p$ is the smallest integer $k \\ge 0$ such that $\\Pr(L \\le k) \\ge p$.\nThis requires the Cumulative Distribution Function (CDF), $C(k) = \\Pr(L \\le k)$.\n- If $F=0$, $L=0$. The CDF is $C(k)=1$ for all $k \\ge 0$. The smallest $k \\ge 0$ satisfying $C(k) \\ge p$ is $k=0$. Thus, $L_p=0$.\n- If $F0$:\n    - For $1 \\le k  F$, the CDF is $C(k) = \\sum_{i=1}^{k} \\Pr(L=i) = \\sum_{i=1}^{k} (1-q)q^{i-1} = (1-q)\\frac{1-q^k}{1-q} = 1-q^k$, for $q1$.\n    - At the cap, $C(F) = \\Pr(L \\le F) = 1$.\nThe CDF for $F0$ and $q1$ is:\n$$\nC(k) = \\Pr(L \\le k) = \\begin{cases}\n0  \\text{for } k1 \\\\\n1-q^k  \\text{for } 1 \\le k  F \\\\\n1  \\text{for } k \\ge F\n\\end{cases}\n$$\nWe seek the smallest integer $k \\ge 1$ such that $C(k) \\ge p$.\nIf $q=1$, $L=F$ with probability $1$. $C(k)=0$ for $kF$ and $C(F)=1$. Thus, for any $p \\in (0,1)$, $L_p=F$.\nIf $q=0$, $L=1$ with probability $1$. $C(k)=0$ for $k1$ and $C(1)=1$. Thus, $L_p=1$.\nIf $0  q  1$, we solve for the smallest integer $k \\ge 1$ where $1-q^k \\ge p$.\n$$\n1-p \\ge q^k \\implies \\log(1-p) \\ge k \\log(q)\n$$\nSince $0q1$, $\\log(q)$ is negative. Dividing by $\\log(q)$ reverses the inequality:\n$$\nk \\ge \\frac{\\log(1-p)}{\\log(q)}\n$$\nThe smallest integer $k$ satisfying this is $k = \\lceil \\frac{\\log(1-p)}{\\log(q)} \\rceil$. This value must be capped by $F$, as the quantile cannot exceed the maximum possible value of $L$.\nSummary for $L_p$:\n$$\nL_p = \\begin{cases}\n0  \\text{if } F=0 \\\\\nF  \\text{if } F0, q=1 \\\\\n1  \\text{if } F0, q=0 \\\\\n\\min\\left(F, \\left\\lceil \\frac{\\log(1-p)}{\\log(q)} \\right\\rceil\\right)  \\text{if } F0, 0q1\n\\end{cases}\n$$\nFor the specified problem, $p=0.99$, so we use $\\log(0.01)$ in the calculation.\n\n**4. Derivation of the Worst-Case Chain Length $L_{\\text{wc}}$**\nThe worst-case chain length is the maximum possible value of $L$. By definition, the number of finalizers is hard-capped at $F$. Therefore, the range of $L$ is $\\{0\\}$ if $F=0$ and $\\{1, 2, \\dots, F\\}$ if $F0$. The maximum value in either case is $F$.\n$$\nL_{\\text{wc}} = F\n$$\n\n**5. GC Pause Time Expressions**\nWith these derivations, the GC pause time metrics are calculated as specified:\n- $T_{\\text{mean}} = T_0 + c \\cdot \\mathbb{E}[L]$\n- $T_p = T_0 + c \\cdot L_p$\n- $T_{\\text{wc}} = T_0 + c \\cdot L_{\\text{wc}}$\n\nThese formulas will be implemented to solve for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the garbage collection latency problem for a given test suite.\n\n    The solution involves:\n    1. Deriving expressions for the expected, quantile, and worst-case\n       number of finalizer tasks (L) based on a truncated geometric model.\n    2. Applying these expressions to calculate the mean, P99, and worst-case\n       GC pause times for each test case.\n    \"\"\"\n\n    # Test suite parameters: T0, tf, tq, F, q\n    test_cases = [\n        (50.0, 0.1, 0.05, 0, 0.5),\n        (50.0, 0.2, 0.1, 10, 0.0),\n        (50.0, 0.2, 0.05, 100, 0.5),\n        (50.0, 0.05, 0.05, 1000, 0.9),\n        (10.0, 0.005, 0.005, 100000, 0.99),\n    ]\n\n    p = 0.99  # 99th percentile\n    results = []\n\n    for T0, tf, tq, F, q in test_cases:\n        c = tf + tq\n\n        # Handle the trivial case where no finalizers are possible\n        if F == 0:\n            E_L = 0\n            L_p = 0\n            L_wc = 0\n        else:\n            # 1. Calculate worst-case chain length L_wc\n            # The chain length is hard-capped at F.\n            L_wc = F\n\n            # 2. Calculate expected chain length E[L]\n            # This is the sum of a finite geometric series sum_{k=0}^{F-1} q^k.\n            if q == 1.0:\n                E_L = F\n            else:\n                # Use numpy.power for potential precision with large F\n                E_L = (1 - np.power(q, F)) / (1 - q)\n\n            # 3. Calculate p-quantile chain length L_p\n            # This is the smallest integer k = 0 with Pr(L = k) = p.\n            if q == 1.0:\n                # If q=1, the chain always reaches the cap F.\n                L_p = F\n            elif q == 0.0:\n                # If q=0, only the first finalizer runs.\n                L_p = 1\n            else:\n                # For 0  q  1, solve k = log(1-p)/log(q) for integer k.\n                # The result is ceil(log(1-p)/log(q)).\n                # We cap this value at F, the maximum possible length.\n                log_val = np.log(1 - p) / np.log(q)\n                L_p = int(np.ceil(log_val))\n                L_p = min(F, L_p)\n\n        # Calculate GC pause time metrics\n        T_mean = T0 + c * E_L\n        T_p99 = T0 + c * L_p\n        T_wc = T0 + c * L_wc\n\n        # Append results for the current test case\n        results.extend([T_mean, T_p99, T_wc])\n\n    # Final print statement in the exact required format.\n    # Each value rounded to 6 decimal places.\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```", "id": "3236467"}]}