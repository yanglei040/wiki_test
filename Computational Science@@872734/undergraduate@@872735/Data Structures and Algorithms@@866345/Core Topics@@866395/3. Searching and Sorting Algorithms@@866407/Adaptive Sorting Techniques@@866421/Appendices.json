{"hands_on_practices": [{"introduction": "Many real-world datasets are \"nearly sorted.\" This exercise explores a practical definition of this property: every element is at most $k$ positions away from its final sorted location. Your task is to design an algorithm that leverages this structural information to sort the array much faster than a general-purpose $O(n \\log n)$ algorithm, aiming for a complexity of $O(n \\log k)$ [@problem_id:3203352]. This practice is fundamental to understanding how specific guarantees about input data can lead to significant performance gains.", "problem": "You are given arrays over a totally ordered domain with the guarantee that, for each element, its displacement from its final position in the sorted order is at most $k$ indices. Formally, let the input be an array $A$ of length $n$ over a set with a total order relation $\\leq$. For each element $A[i]$, let $pos(A[i])$ denote the index of $A[i]$ in the array obtained by sorting $A$ in nondecreasing order (with ties resolved by any deterministic rule). The guarantee is that for all $i \\in \\{0,1,\\dots,n-1\\}$, the displacement satisfies $\\lvert pos(A[i]) - i \\rvert \\leq k$, where $k$ is a nonnegative integer. Your task is to design and implement an adaptive sorting technique that exploits this displacement bound to achieve worst-case time complexity $O(n \\log k)$ and space complexity $O(k)$, while returning the array sorted in nondecreasing order.\n\nBase definitions you may use:\n- A total order is a binary relation $\\leq$ on a set $S$ such that for all $x,y \\in S$, exactly one of $x  y$, $x = y$, or $x  y$ holds, and the relation is transitive and antisymmetric.\n- A priority queue (PQ) is an abstract data type supporting insertion of an element and extraction of the minimum element under $\\leq$, both in logarithmic time with respect to the number of elements currently stored.\n- A binary heap is a data structure implementing a priority queue with insertion and extract-min operations running in $O(\\log m)$ time where $m$ is the current number of elements in the heap.\n\nYour program must implement the described adaptive sorting algorithm, apply it to the following test suite, and produce the results as specified.\n\nTest suite (each test case is the pair $(A,k)$):\n1. $A = [12,3,5,9,24,15,18,21,30,27]$, $k = 3$.\n2. $A = [1,2,2,3]$, $k = 0$.\n3. $A = [2,1,4,3,6,5]$, $k = 1$.\n4. $A = []$, $k = 2$.\n5. $A = [42]$, $k = 10$.\n6. $A = [-2,-5,-2,3,0,3,7]$, $k = 2$.\n7. $A = [23,42,4,8,15,16]$, $k = 5$.\n8. $A = [30,40,50,10,20]$, $k = 100$.\n9. $A = [3,1,2,6,4,5,9,7,8,10]$, $k = 2$.\n\nOutput specification:\n- For each test case, the program must output the sorted array (a list of integers) corresponding to $A$ sorted in nondecreasing order.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above. For example, the output format must be $[result_1,result_2,\\dots,result_9]$, where each $result_i$ is the sorted list for test case $i$.\n- All numeric values in the arrays are plain integers; no physical units or angles apply.\n\nConstraints:\n- $n$ is any nonnegative integer.\n- $k$ is any nonnegative integer (when $k \\geq n-1$, the guarantee is vacuous but the algorithm must still run and return the correct sorted order).", "solution": "The problem requires the design of an adaptive sorting algorithm for arrays that are \"nearly sorted.\" The specific property of near-sortedness is that any element at index $i$ in the input array $A$ will be at an index $j$ in the final sorted array such that its displacement $\\lvert i - j \\rvert$ is at most $k$, a given nonnegative integer. The algorithm must achieve a worst-case time complexity of $O(n \\log k)$ and an auxiliary space complexity of $O(k)$, where $n$ is the number of elements in the array.\n\n### Fundamental Principle\nThe core of the adaptive approach rests on a critical deduction from the given property: $\\lvert pos(A[i]) - i \\rvert \\leq k$, where $pos(A[i])$ is the final sorted index of element $A[i]$. This inequality can be rearranged to analyze the origin of elements in the sorted sequence. Let $S$ be the array $A$ sorted in nondecreasing order. For an element $S[j]$ at index $j$ in the sorted array, its original index $i$ in $A$ must satisfy $\\lvert j - i \\rvert \\leq k$. This implies $j-k \\leq i \\leq j+k$.\n\nThis is a powerful constraint. It tells us that the element that belongs at position $j$ in the sorted output, $S[j]$, must be located in the input array $A$ within the index range $[\\max(0, j-k), \\min(n-1, j+k)]$. More importantly for a constructive algorithm, it guarantees that the smallest overall element, $S[0]$, must originate from an index $i \\in [0, k]$ in $A$. The second smallest element, $S[1]$, must originate from an index $i \\in [0, 1+k]$ in $A$. In general, the $j$-th element of the sorted array, $S[j]$, must be present in the prefix $A[0 \\dots j+k]$ of the input array.\n\nThis locality suggests that we do not need to consider the entire array to find the next smallest element. Instead, we can maintain a \"window\" of candidate elements and repeatedly extract the minimum from this window. A min-priority queue, implemented as a min-heap, is the ideal data structure for this task, as it allows for efficient extraction of the minimum and insertion of new elements.\n\n### Algorithmic Design\nThe algorithm proceeds as follows, using a min-heap to manage a sliding window of candidates:\n1.  Initialize an empty list, `result`, which will store the sorted elements.\n2.  Initialize a min-priority queue, `pq`.\n3.  Populate the `pq` with the first $\\min(n, k+1)$ elements from the input array $A$. This forms the initial window of candidates. In the case where $n \\le k$, all elements of $A$ are added. This step takes $O(k)$ time using a linear-time heapify algorithm.\n4.  Iterate $n$ times to build the sorted `result` array. In each iteration:\n    a. Extract the minimum element from the `pq`. By the principle established above, this element is guaranteed to be the next smallest element for the sorted sequence. Append it to `result`.\n    b. To maintain the sliding window, if there is a next uninspected element in the input array $A$, insert it into the `pq`. The element to be considered after processing the initial $\\min(n, k+1)$ elements and extracting one is at index $\\min(n, k+1)$, and so on.\n\nThe size of the priority queue, `pq`, is crucial for the algorithm's performance. It is initialized with at most $k+1$ elements. In each step of the main loop, one element is removed and one is added (until the input array is exhausted). Therefore, the number of elements in `pq` never exceeds $k+1$.\n\n### Correctness Argument\nThe algorithm's correctness hinges on the loop invariant that before each extraction, the `pq` contains the smallest element among all yet-to-be-sorted elements.\nLet's formalize this. Suppose we are about to determine the $j$-th element of the sorted array, $S[j]$. At this point, the algorithm has already placed $S[0], \\dots, S[j-1]$ into the `result` array. The elements from the input array $A$ that have been inserted into the `pq` are from indices $0$ up to at least $j+k-1$ (or fewer if $n$ is smaller).\nThe element $S[j]$ must come from an original index $i \\le j+k$. Because all elements from $A[0 \\dots j+k-1]$ have been considered (either placed in `result` or are still in `pq`), and we are about to add $A[j+k]$ if it exists, the true $S[j]$ is guaranteed to be in the `pq`. Since `pq` is a min-priority queue, the `extract-min` operation will correctly return $S[j]$. This holds for all $j$ from $0$ to $n-1$.\n\n### Complexity Analysis\n-   **Time Complexity**: The initialization of the `pq` with $\\min(n, k+1)$ elements takes $O(\\min(n, k)) = O(k)$ time (since if $n \\le k$, it's $O(n)$, and if we assume $k  n$ for an adaptive sort, it's $O(k)$). The main loop runs $n$ times. Inside the loop, we perform one `extract-min` and at most one `insert` operation. The size of the `pq` is bounded by $k+1$. Thus, each heap operation takes $O(\\log k)$ time. The total time complexity is $O(k) + n \\times O(\\log k) = O(n \\log k)$. This meets the specified requirement. If $k=0$, the complexity is $O(n)$, and if $k \\geq n-1$, it becomes $O(n \\log n)$, behaving like a standard heapsort.\n-   **Space Complexity**: The auxiliary space is dominated by the priority queue. As its size is bounded by $k+1$, the space complexity is $O(k)$. The output array `result` requires $O(n)$ space, which is typically classified as output space rather than auxiliary space. The algorithm fulfills the $O(k)$ space complexity requirement for auxiliary storage.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport heapq\n\ndef adaptive_sort(A, k):\n    \"\"\"\n    Sorts a k-nearly-sorted array in O(n log k) time and O(k) auxiliary space.\n\n    An array is k-nearly-sorted if for every element, its final sorted position\n    is at most k indices away from its current position.\n\n    Args:\n        A (list): The input array of numbers.\n        k (int): The maximum displacement of any element.\n\n    Returns:\n        list: The sorted array.\n    \"\"\"\n    n = len(A)\n    if n == 0:\n        return []\n\n    # The size of the initial heap and the sliding window.\n    # If k >= n-1, this will be n, correctly turning the algorithm into a heapsort.\n    window_size = min(n, k + 1)\n    \n    # Python's heapq implements a min-heap.\n    # Initialize the heap with the first `window_size` elements.\n    pq = A[:window_size]\n    heapq.heapify(pq)  # This operation takes O(window_size) time.\n\n    result = []\n    # Index of the next element from A to be added to the heap.\n    next_elem_idx = window_size\n\n    # The loop runs n times, once for each element to be placed in the result.\n    for _ in range(n):\n        # The smallest element in the heap is the next element in the sorted sequence.\n        # We can extract it.\n        min_val = heapq.heappop(pq)\n        result.append(min_val)\n\n        # If there are more elements in the input array, add the next one to the heap\n        # to maintain the sliding window of candidates.\n        if next_elem_idx  n:\n            heapq.heappush(pq, A[next_elem_idx])\n            next_elem_idx += 1\n            \n    return result\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ([12, 3, 5, 9, 24, 15, 18, 21, 30, 27], 3),\n        ([1, 2, 2, 3], 0),\n        ([2, 1, 4, 3, 6, 5], 1),\n        ([], 2),\n        ([42], 10),\n        ([-2, -5, -2, 3, 0, 3, 7], 2),\n        ([23, 42, 4, 8, 15, 16], 5),\n        ([30, 40, 50, 10, 20], 100),\n        ([3, 1, 2, 6, 4, 5, 9, 7, 8, 10], 2)\n    ]\n\n    results = []\n    for A, k in test_cases:\n        sorted_A = adaptive_sort(list(A), k) # Use a copy to not modify original\n        results.append(sorted_A)\n\n    # Format the final list of results into the specified string format.\n    # e.g., [[1, 2], [3, 4]] -> \"[[1, 2], [3, 4]]\"\n    # Using str() on a list automatically includes spaces, e.g., '[1, 2, 3]'.\n    # To match a compact format, we might need custom string formatting.\n    # However, the prompt's example code `','.join(map(str, results))` suggests `str()` is fine.\n    # Let's verify: `str([1,2])` is `[1, 2]`. This is list-like. Let's make it compact.\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3203352"}, {"introduction": "What does it truly mean for a sequence to be \"nearly sorted\"? This question is more nuanced than it appears, as different algorithms are \"adaptive\" to different measures of order. This exercise [@problem_id:3203325] challenges you to construct an array that has very few sorted \"runs\" but a very large number of \"inversions.\" By analyzing how algorithms sensitive to each of these measures would perform, you will gain a deeper appreciation for why there is no single best adaptive sorting algorithm.", "problem": "Consider a set of $n$ distinct keys with the intended sorted order $1,2,\\dots,n$, where $n$ is an even integer and $n \\geq 4$. Two adaptive comparison-based sorting algorithms, $A_1$ and $A_2$, exploit different measures of presortedness. Define the number of inversions $I(\\pi)$ of a permutation $\\pi$ as the number of index pairs $(i,j)$ with $ij$ and $\\pi(i)\\pi(j)$, and define the number of ascending runs $R(\\pi)$ as the number of maximal contiguous non-decreasing subsequences in $\\pi$. Use the following well-tested facts as the fundamental base for modeling cost: an algorithm that merges existing runs executes a number of comparisons proportional to $n \\log R(\\pi)$, and an insertion-based algorithm executes a number of comparisons proportional to $n + I(\\pi)$. For this problem, model the comparison counts exactly as $C_1(n,R) = n \\ln R$ and $C_2(n,I) = n + I$, where $\\ln$ denotes the natural logarithm.\n\nConstruct a permutation $\\pi$ that is nearly sorted by the run measure, in the sense that $R(\\pi)$ is small, but far from sorted by the inversion measure, in the sense that $I(\\pi)$ is large. Specifically, construct $\\pi$ by concatenating two strictly increasing contiguous blocks of length $n/2$ each such that every key in the first block is larger than every key in the second block. Then, using the definitions of $I(\\pi)$ and $R(\\pi)$ and the given cost models $C_1$ and $C_2$, derive the exact closed-form expression (in terms of $n$ only) for the speedup factor $\\rho(n)$ defined as the ratio of $A_2$’s comparisons to $A_1$’s comparisons on this input:\n$$\n\\rho(n) = \\frac{C_2\\!\\left(n, I(\\pi)\\right)}{C_1\\!\\left(n, R(\\pi)\\right)}.\n$$\nProvide your final answer as a single simplified analytic expression. No rounding is required, and no physical units apply.", "solution": "The problem requires constructing a specific permutation $\\pi$ of the keys $\\{1, 2, \\dots, n\\}$, where $n$ is an even integer and $n \\ge 4$. The permutation is formed by concatenating two strictly increasing contiguous blocks of length $n/2$ each, with the additional constraint that every key in the first block is larger than every key in the second block.\n\nTo satisfy these constraints, the set of keys $\\{1, 2, \\dots, n\\}$ must be partitioned into two sets: the smaller keys $\\{1, 2, \\dots, n/2\\}$ and the larger keys $\\{n/2+1, \\dots, n\\}$. For all keys in the first block to be larger than all keys in the second, the first block must be comprised of the larger keys and the second block of the smaller keys. Since each block is also strictly increasing, they must be sorted internally.\nThe first block, of length $n/2$, is therefore the sequence $(n/2+1, n/2+2, \\dots, n)$.\nThe second block, of length $n/2$, is the sequence $(1, 2, \\dots, n/2)$.\n\nConcatenating these two blocks gives the permutation $\\pi$:\n$$\n\\pi = \\left(\\frac{n}{2}+1, \\frac{n}{2}+2, \\dots, n, 1, 2, \\dots, \\frac{n}{2}\\right)\n$$\n\nNext, we must determine the number of ascending runs, $R(\\pi)$, and the number of inversions, $I(\\pi)$, for this specific permutation.\n\n1.  **Calculation of the number of ascending runs, $R(\\pi)$**:\n    An ascending run is a maximal contiguous non-decreasing subsequence.\n    The first part of the sequence, $(\\frac{n}{2}+1, \\frac{n}{2}+2, \\dots, n)$, is strictly increasing and thus forms a single ascending run.\n    The sequence breaks at the junction between the element $n$ and the element $1$, since $n > 1$.\n    The second part of the sequence, $(1, 2, \\dots, \\frac{n}{2})$, is also strictly increasing and forms a second ascending run.\n    Therefore, the permutation $\\pi$ consists of exactly two ascending runs.\n    $$\n    R(\\pi) = 2\n    $$\n    This permutation is nearly sorted with respect to the run measure, as $R(\\pi)$ is a small constant.\n\n2.  **Calculation of the number of inversions, $I(\\pi)$**:\n    An inversion is a pair of indices $(i, j)$ such that $i  j$ and $\\pi(i) > \\pi(j)$. Let us analyze the possible pairs of indices.\n    - If both indices $i$ and $j$ are within the first block (i.e., $1 \\le i  j \\le n/2$), then $\\pi(i)  \\pi(j)$ because the first block is strictly increasing. This contributes $0$ inversions.\n    - If both indices $i$ and $j$ are within the second block (i.e., $n/2+1 \\le i  j \\le n$), then $\\pi(i)  \\pi(j)$ because the second block is strictly increasing. This contributes $0$ inversions.\n    - If index $i$ is in the first block ($1 \\le i \\le n/2$) and index $j$ is in the second block ($n/2+1 \\le j \\le n$), then $\\pi(i)$ is a key from the set $\\{\\frac{n}{2}+1, \\dots, n\\}$ and $\\pi(j)$ is a key from the set $\\{1, \\dots, \\frac{n}{2}\\}$. By construction, every key in the first block is larger than every key in the second block. Therefore, for every such pair $(i, j)$, it is true that $\\pi(i) > \\pi(j)$, creating an inversion.\n\n    To find the total number of such inversions, we multiply the number of possible choices for $i$ by the number of possible choices for $j$. The number of elements in the first block is $n/2$, and the number of elements in the second block is also $n/2$.\n    The total number of inversions is:\n    $$\n    I(\\pi) = \\left(\\frac{n}{2}\\right) \\times \\left(\\frac{n}{2}\\right) = \\frac{n^2}{4}\n    $$\n    This permutation is far from sorted with respect to the inversion measure, as $I(\\pi)$ grows quadratically with $n$.\n\nNow, we use the given cost models to find the number of comparisons for each algorithm.\nThe cost for algorithm $A_1$, which is based on merging runs, is:\n$$\nC_1(n, R(\\pi)) = n \\ln(R(\\pi)) = n \\ln(2)\n$$\nThe cost for algorithm $A_2$, which is an insertion-based algorithm, is:\n$$\nC_2(n, I(\\pi)) = n + I(\\pi) = n + \\frac{n^2}{4}\n$$\n\nFinally, we compute the speedup factor $\\rho(n)$, defined as the ratio of $C_2$ to $C_1$:\n$$\n\\rho(n) = \\frac{C_2(n, I(\\pi))}{C_1(n, R(\\pi))} = \\frac{n + \\frac{n^2}{4}}{n \\ln(2)}\n$$\nWe can simplify this expression. Factoring out $n$ from the numerator (permissible since $n \\ge 4$):\n$$\n\\rho(n) = \\frac{n \\left(1 + \\frac{n}{4}\\right)}{n \\ln(2)} = \\frac{1 + \\frac{n}{4}}{\\ln(2)}\n$$\nTo present the expression in a more standard form without a fraction in the numerator, we can write:\n$$\n\\rho(n) = \\frac{\\frac{4+n}{4}}{\\ln(2)} = \\frac{n+4}{4\\ln(2)}\n$$\nThis is the final, simplified, closed-form expression for the speedup factor $\\rho(n)$.", "answer": "$$\n\\boxed{\\frac{n+4}{4\\ln(2)}}\n$$", "id": "3203325"}, {"introduction": "Natural Mergesort is a classic adaptive algorithm that excels on inputs with long, pre-existing sorted subsequences, or \"runs.\" To sharpen your analytical skills, this problem asks you to calculate the exact number of comparisons it performs on a specific, non-trivial input: a bitonic array [@problem_id:3203381]. By carefully tracing the algorithm's two main phases—run identification and merging—you will see precisely how it adapts to the input's structure and achieves its efficiency.", "problem": "Consider an array $A$ of length $n$, where $n \\geq 3$ is odd, whose contents form a bitonic pattern given explicitly by $A = \\langle 1, 3, 5, \\dots, n, n-1, \\dots, 2 \\rangle$. An adaptive sorting algorithm is one whose running time depends on a formal measure of presortedness of the input rather than solely on $n$. Natural mergesort is an adaptive sorting technique that operates according to the following rules:\n- It scans the array from left to right to partition it into maximal monotonic runs, where a run is a contiguous subsequence that is either strictly nondecreasing or strictly nonincreasing.\n- Any strictly nonincreasing run is reversed in place to become strictly nondecreasing before merging.\n- It then repeatedly performs a stable two-way merge of adjacent runs until a single sorted run covering the entire array remains.\nAssume the standard comparison model in which the cost of the algorithm is measured by counting only key-to-key comparisons. In particular, a scan that tests monotonicity of adjacent elements uses exactly one comparison per adjacent pair, a reversal uses no comparisons, and the stable two-way merge of two strictly increasing runs of lengths $a$ and $b$ uses exactly $a + b - 1$ comparisons when all keys are distinct.\n\nUnder these assumptions, derive an exact closed-form expression, as a function of $n$, for the total number of key comparisons that natural mergesort performs on the array $A$ described above, including both the initial run detection phase and the merging phase. Express your final answer as a single symbolic expression. No rounding is required.", "solution": "To find the total number of comparisons, we must sum the comparisons from the run detection phase and the merging phase. Let the total number of comparisons be $C_T$.\n\n**Phase 1: Run Detection**\nThe algorithm scans the array $A$ of length $n$ from left to right to identify maximal monotonic runs. This involves comparing each element $A[i]$ with $A[i+1]$ for $i = 0, \\dots, n-2$. This requires a total of $n-1$ comparisons. Let's call this cost $C_{\\text{scan}} = n-1$.\n\nThe input array is $A = \\langle 1, 3, 5, \\dots, n, n-1, \\dots, 2 \\rangle$.\n- The first part, $\\langle 1, 3, \\dots, n \\rangle$, is strictly increasing. The comparison $n > n-1$ breaks the nondecreasing pattern. This first part constitutes a single run, $R_1$. Its length is $L_1 = \\frac{n-1}{2} + 1 = \\frac{n+1}{2}$.\n- The second part, $\\langle n-1, n-3, \\dots, 2 \\rangle$, is strictly decreasing. This constitutes the second run, $R_2$. Its length is $L_2 = n - L_1 = n - \\frac{n+1}{2} = \\frac{n-1}{2}$.\n\nThe scan thus partitions the array into exactly two runs.\n\n**Phase 2: Merging**\nAccording to the rules, the nonincreasing run $R_2$ must first be reversed in place. This operation costs 0 comparisons. After reversal, we have two strictly increasing runs to merge:\n- $R_1 = \\langle 1, 3, \\dots, n \\rangle$ of length $a = L_1 = \\frac{n+1}{2}$.\n- $R_2' = \\langle 2, 4, \\dots, n-1 \\rangle$ of length $b = L_2 = \\frac{n-1}{2}$.\n\nSince there are only two runs, a single merge operation is required. The problem states that merging two strictly increasing runs of lengths $a$ and $b$ costs exactly $a+b-1$ comparisons. The cost of this merge, $C_{\\text{merge}}$, is therefore:\n$$ C_{\\text{merge}} = a + b - 1 = \\left(\\frac{n+1}{2}\\right) + \\left(\\frac{n-1}{2}\\right) - 1 $$\n$$ C_{\\text{merge}} = \\frac{(n+1) + (n-1)}{2} - 1 = \\frac{2n}{2} - 1 = n - 1 $$\n\n**Total Comparisons**\nThe total number of comparisons is the sum of the costs from both phases:\n$$ C_T = C_{\\text{scan}} + C_{\\text{merge}} = (n-1) + (n-1) = 2n-2 $$\nThis is the final closed-form expression for the total number of key comparisons.", "answer": "$$\n\\boxed{2n - 2}\n$$", "id": "3203381"}]}