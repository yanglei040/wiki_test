{"hands_on_practices": [{"introduction": "Negative sampling is more than just a computational shortcut; it's a principled method for approximating a complex probability distribution. This exercise delves into the theory by having you derive the ideal score function that a model learns under negative sampling [@problem_id:3156698]. By working through this problem, you will not only understand the target objective but also quantify the 'mismatch cost' that arises when the data distribution shifts between training and testing—a crucial challenge in real-world applications.", "problem": "Consider a discrete vocabulary with a finite set of outcomes. Let there be a training distribution over outcomes denoted by $p_{\\text{train}}$ and a test distribution denoted by $p_{\\text{test}}$. In a negative sampling setup, for each real (positive) sample, $k$ noise (negative) samples are drawn independently from a noise distribution $q$. The negative sampling classifier uses a scalar score function $s(w)$ for an outcome $w$, and its surrogate loss for a pair $(w, \\text{label})$ is based on the logistic function. Specifically, the loss for a real draw is defined as the negative logarithm of the logistic function of $s(w)$, and the loss for a noise draw is defined as the negative logarithm of the logistic function of $-s(w)$.\n\nNoise distributions are tied to data frequencies via an exponent $\\alpha \\in [0,1]$. For any distribution $r$ over the vocabulary, define the noise distribution $q^{(\\alpha)}(r)$ by raising each probability to the power $\\alpha$ and normalizing, i.e., for each outcome $w$, $q^{(\\alpha)}(r)(w) \\propto r(w)^{\\alpha}$ with normalization to ensure $\\sum_{w} q^{(\\alpha)}(r)(w) = 1$.\n\nAssume a model is trained to minimize the expected surrogate loss under the training mixture formed by $p_{\\text{train}}$ for real samples and $q_{\\text{train}} = q^{(\\alpha)}(p_{\\text{train}})$ for noise samples. At test time, the true real distribution is $p_{\\text{test}}$, and two evaluation regimes are considered:\n- Regime A: Negatives continue to be drawn from $q_{\\text{train}}$.\n- Regime B: Negatives are drawn from $q_{\\text{test}} = q^{(\\alpha)}(p_{\\text{test}})$.\n\nStarting from fundamental definitions of the logistic function $\\sigma(x)$ and the Bernoulli cross-entropy, and the expected negative sampling surrogate risk over the mixture of real and noise draws, derive from first principles:\n1. The expected surrogate risk functional for a general score function $s(w)$ when real outcomes follow a distribution $p$ and noise outcomes follow a distribution $q$ with $k$ negatives per real.\n2. The Bayes-optimal score function $s^{\\star}(w)$ that minimizes the expected surrogate risk for given $(p,q,k)$.\n3. A quantitative mismatch cost defined as the difference between the expected surrogate risk achieved by the trained score function $s_{\\text{train}}$ and the minimal expected surrogate risk under the test mixture. Compute this mismatch cost for both Regime A (where the evaluation noise is $q_{\\text{train}}$) and Regime B (where the evaluation noise is $q_{\\text{test}}$).\n\nImplement a program that, for each test case specified below, computes two floating-point numbers:\n- The Regime A mismatch cost, defined as the expected surrogate risk under $(p_{\\text{test}}, q_{\\text{train}}, k)$ when using $s_{\\text{train}}$, minus the minimal expected surrogate risk under $(p_{\\text{test}}, q_{\\text{train}}, k)$.\n- The Regime B mismatch cost, defined analogously but with $q_{\\text{test}}$ in place of $q_{\\text{train}}$.\n\nUse the following test suite, where each probability vector is over a vocabulary of five outcomes and sums to $1$, and where $q^{(\\alpha)}(\\cdot)$ is computed as described. All probabilities and parameters are strictly positive and scientifically plausible.\n\n- Test Case $1$ (general shift, moderate $k$):\n  - $p_{\\text{train}} = (0.40, 0.25, 0.20, 0.10, 0.05)$\n  - $p_{\\text{test}} = (0.10, 0.15, 0.25, 0.30, 0.20)$\n  - $\\alpha = 0.75$\n  - $k = 5$\n\n- Test Case $2$ (no shift, identical train and test):\n  - $p_{\\text{train}} = (0.20, 0.20, 0.20, 0.20, 0.20)$\n  - $p_{\\text{test}} = (0.20, 0.20, 0.20, 0.20, 0.20)$\n  - $\\alpha = 0.50$\n  - $k = 5$\n\n- Test Case $3$ (uniform noise, large $k$):\n  - $p_{\\text{train}} = (0.55, 0.15, 0.10, 0.10, 0.10)$\n  - $p_{\\text{test}} = (0.10, 0.25, 0.25, 0.20, 0.20)$\n  - $\\alpha = 0.00$\n  - $k = 10$\n\n- Test Case $4$ (strong shift, small $k$):\n  - $p_{\\text{train}} = (0.70, 0.10, 0.10, 0.05, 0.05)$\n  - $p_{\\text{test}} = (0.05, 0.65, 0.10, 0.10, 0.10)$\n  - $\\alpha = 1.00$\n  - $k = 1$\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output the Regime A mismatch cost followed by the Regime B mismatch cost. The final output must therefore contain eight numbers corresponding to the four test cases, in order. Express each number as a floating-point value rounded to six decimal places, with no units, and print exactly the format $[r_1,r_2,\\dots,r_8]$.", "solution": "The problem asks for the derivation of the expected surrogate risk in a negative sampling setup, the corresponding Bayes-optimal score function, and the computation of a mismatch cost under two different evaluation regimes.\n\nThe analysis proceeds in three steps as requested. First, we derive the expected surrogate risk functional. Second, we find the score function that minimizes this risk. Third, we use these results to define and subsequently compute the mismatch costs for the specified test cases.\n\nLet $\\sigma(x) = (1 + e^{-x})^{-1}$ be the logistic function. The loss for a positive sample $w$ is given as $L_{+} = -\\log(\\sigma(s(w)))$, and the loss for a negative (noise) sample $w$ is $L_{-} = -\\log(\\sigma(-s(w)))$. We can rewrite these using the identity $\\log(\\sigma(x)) = -\\log(1+e^{-x})$:\n$L_{+} = \\log(1+e^{-s(w)})$\n$L_{-} = \\log(1+e^{s(w)})$\n\n### 1. Expected Surrogate Risk Functional\n\nThe negative sampling procedure involves, for each real outcome $w_{\\text{real}}$ drawn from a data distribution $p$, drawing $k$ noise outcomes $w_{\\text{noise},1}, \\dots, w_{\\text{noise},k}$ independently from a noise distribution $q$. The total loss for this \"event\" (one real sample and its $k$ associated negative samples) is the sum of the individual losses:\n$$\nL_{\\text{event}}(w_{\\text{real}}, w_{\\text{noise},1}, \\dots, w_{\\text{noise},k}) = -\\log(\\sigma(s(w_{\\text{real}}))) - \\sum_{i=1}^{k} \\log(\\sigma(-s(w_{\\text{noise},i})))\n$$\nThe expected surrogate risk, denoted $R(s; p, q, k)$, is the expectation of this total loss over all possible choices of real and noise samples, drawn according to their respective distributions. By the linearity of expectation, we can separate the contributions from the real and noise samples:\n$$\nR(s; p, q, k) = \\mathbb{E}_{w_{\\text{real}} \\sim p, \\{w_{\\text{noise},i}\\}_{i=1}^k \\sim q} \\left[ -\\log(\\sigma(s(w_{\\text{real}}))) - \\sum_{i=1}^{k} \\log(\\sigma(-s(w_{\\text{noise},i}))) \\right]\n$$\n$$\nR(s; p, q, k) = \\mathbb{E}_{w_{\\text{real}} \\sim p} [-\\log(\\sigma(s(w_{\\text{real}})))] + \\sum_{i=1}^{k} \\mathbb{E}_{w_{\\text{noise},i} \\sim q} [-\\log(\\sigma(-s(w_{\\text{noise},i})))]\n$$\nSince the noise samples are identically distributed, their expected losses are the same. Thus, we can simplify the sum:\n$$\nR(s; p, q, k) = \\mathbb{E}_{w \\sim p} [-\\log(\\sigma(s(w)))] + k \\cdot \\mathbb{E}_{w' \\sim q} [-\\log(\\sigma(-s(w')))]\n$$\nFor a discrete vocabulary $V$, this expectation is expressed as a sum over all outcomes $w \\in V$:\n$$\nR(s; p, q, k) = \\sum_{w \\in V} p(w) [-\\log(\\sigma(s(w)))] + k \\sum_{w \\in V} q(w) [-\\log(\\sigma(-s(w)))]\n$$\nUsing the numerically stable form with logarithms, this is:\n$$\nR(s; p, q, k) = \\sum_{w \\in V} \\left[ p(w)\\log(1+e^{-s(w)}) + k \\, q(w)\\log(1+e^{s(w)}) \\right]\n$$\nThis is the desired expected surrogate risk functional.\n\n### 2. Bayes-Optimal Score Function\n\nTo find the Bayes-optimal score function $s^{\\star}(w)$ that minimizes $R(s; p, q, k)$, we can optimize the risk with respect to $s(w)$ for each outcome $w$ independently, as the total risk is a sum of terms, each depending on only one $s(w)$. Let us consider the component of the risk associated with a single outcome $w_j \\in V$:\n$$\nR_j(s(w_j)) = p(w_j)\\log(1+e^{-s(w_j)}) + k \\, q(w_j)\\log(1+e^{s(w_j)})\n$$\nWe find the minimum by setting the derivative with respect to $s(w_j)$ to zero. We use the derivatives $\\frac{d}{dx}\\log(1+e^{-x}) = \\frac{-e^{-x}}{1+e^{-x}} = -\\sigma(-x)$ and $\\frac{d}{dx}\\log(1+e^{x}) = \\frac{e^{x}}{1+e^{x}} = \\sigma(x)$.\n$$\n\\frac{\\partial R_j}{\\partial s(w_j)} = p(w_j) (-\\sigma(-s(w_j))) + k \\, q(w_j) (\\sigma(s(w_j))) = 0\n$$\nAt the optimum $s^{\\star}(w_j)$, this gives:\n$$\np(w_j) \\sigma(-s^{\\star}(w_j)) = k \\, q(w_j) \\sigma(s^{\\star}(w_j))\n$$\nUsing $\\sigma(-x) = 1 - \\sigma(x)$:\n$$\np(w_j) (1 - \\sigma(s^{\\star}(w_j))) = k \\, q(w_j) \\sigma(s^{\\star}(w_j))\n$$\nRearranging to solve for $\\sigma(s^{\\star}(w_j))$:\n$$\np(w_j) = p(w_j)\\sigma(s^{\\star}(w_j)) + k \\, q(w_j)\\sigma(s^{\\star}(w_j)) = \\sigma(s^{\\star}(w_j)) (p(w_j) + k \\, q(w_j))\n$$\n$$\n\\sigma(s^{\\star}(w_j)) = \\frac{p(w_j)}{p(w_j) + k \\, q(w_j)}\n$$\nTo find $s^{\\star}(w_j)$ itself, we invert the logistic function using the logit function, $\\text{logit}(y) = \\log(y/(1-y))$:\n$$\ns^{\\star}(w_j) = \\text{logit}\\left(\\frac{p(w_j)}{p(w_j) + k \\, q(w_j)}\\right) = \\log\\left( \\frac{\\frac{p(w_j)}{p(w_j) + k \\, q(w_j)}}{1 - \\frac{p(w_j)}{p(w_j) + k \\, q(w_j)}} \\right)\n$$\n$$\ns^{\\star}(w_j) = \\log\\left( \\frac{\\frac{p(w_j)}{p(w_j) + k \\, q(w_j)}}{\\frac{k \\, q(w_j)}{p(w_j) + k \\, q(w_j)}} \\right) = \\log\\left(\\frac{p(w_j)}{k \\, q(w_j)}\\right)\n$$\nThus, the Bayes-optimal score function for a given set of distributions $(p,q)$ and parameter $k$ is $s^{\\star}(w) = \\log(p(w)) - \\log(k) - \\log(q(w))$.\n\n### 3. Mismatch Cost Computation\n\nThe mismatch cost is the excess risk incurred by using a non-optimal score function. A model trained on $(p_{\\text{train}}, q_{\\text{train}}, k)$ learns the score function:\n$$\ns_{\\text{train}}(w) = s^{\\star}(w; p_{\\text{train}}, q_{\\text{train}}, k) = \\log\\left(\\frac{p_{\\text{train}}(w)}{k \\, q_{\\text{train}}(w)}\\right)\n$$\nwhere $q_{\\text{train}}(w) \\propto p_{\\text{train}}(w)^{\\alpha}$.\n\n**Regime A:** The evaluation setting is $(p_{\\text{test}}, q_{\\text{train}}, k)$.\nThe risk incurred by the trained model is $R(s_{\\text{train}}; p_{\\text{test}}, q_{\\text{train}}, k)$.\nThe minimal possible risk in this regime is achieved by the optimal score function for this regime:\n$$\ns^{\\star}_{\\text{test},A}(w) = s^{\\star}(w; p_{\\text{test}}, q_{\\text{train}}, k) = \\log\\left(\\frac{p_{\\text{test}}(w)}{k \\, q_{\\text{train}}(w)}\\right)\n$$\nThe corresponding minimal risk is $R(s^{\\star}_{\\text{test},A}; p_{\\text{test}}, q_{\\text{train}}, k)$.\nThe mismatch cost for Regime A is the difference:\n$$\n\\text{Cost}_A = R(s_{\\text{train}}; p_{\\text{test}}, q_{\\text{train}}, k) - R(s^{\\star}_{\\text{test},A}; p_{\\text{test}}, q_{\\text{train}}, k)\n$$\n\n**Regime B:** The evaluation setting is $(p_{\\text{test}}, q_{\\text{test}}, k)$, where $q_{\\text{test}}(w) \\propto p_{\\text{test}}(w)^{\\alpha}$.\nThe risk incurred by the trained model is $R(s_{\\text{train}}; p_{\\text{test}}, q_{\\text{test}}, k)$.\nThe minimal possible risk in this regime is achieved by the optimal score function for this regime:\n$$\ns^{\\star}_{\\text{test},B}(w) = s^{\\star}(w; p_{\\text{test}}, q_{\\text{test}}, k) = \\log\\left(\\frac{p_{\\text{test}}(w)}{k \\, q_{\\text{test}}(w)}\\right)\n$$\nThe corresponding minimal risk is $R(s^{\\star}_{\\text{test},B}; p_{\\text{test}}, q_{\\text{test}}, k)$.\nThe mismatch cost for Regime B is the difference:\n$$\n\\text{Cost}_B = R(s_{\\text{train}}; p_{\\text{test}}, q_{\\text{test}}, k) - R(s^{\\star}_{\\text{test},B}; p_{\\text{test}}, q_{\\text{test}}, k)\n$$\n\nThe algorithm for the computation is as follows:\n1.  For each test case, given $p_{\\text{train}}$, $p_{\\text{test}}$, $\\alpha$, and $k$.\n2.  Construct the noise distributions $q_{\\text{train}}(w) = \\frac{p_{\\text{train}}(w)^\\alpha}{\\sum_{w'} p_{\\text{train}}(w')^\\alpha}$ and $q_{\\text{test}}(w) = \\frac{p_{\\text{test}}(w)^\\alpha}{\\sum_{w'} p_{\\text{test}}(w')^\\alpha}$.\n3.  Calculate the score functions $s_{\\text{train}}$, $s^{\\star}_{\\text{test},A}$, and $s^{\\star}_{\\text{test},B}$ using the derived formula.\n4.  Calculate the four necessary risk values using the risk functional $R(s; p, q, k)$.\n5.  Compute the differences $\\text{Cost}_A$ and $\\text{Cost}_B$.\n\nThis procedure is implemented for each test case provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the negative sampling mismatch cost problem for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1 (general shift, moderate k)\n        {\n            \"p_train\": np.array([0.40, 0.25, 0.20, 0.10, 0.05]),\n            \"p_test\": np.array([0.10, 0.15, 0.25, 0.30, 0.20]),\n            \"alpha\": 0.75,\n            \"k\": 5\n        },\n        # Test Case 2 (no shift, identical train and test)\n        {\n            \"p_train\": np.array([0.20, 0.20, 0.20, 0.20, 0.20]),\n            \"p_test\": np.array([0.20, 0.20, 0.20, 0.20, 0.20]),\n            \"alpha\": 0.50,\n            \"k\": 5\n        },\n        # Test Case 3 (uniform noise, large k)\n        {\n            \"p_train\": np.array([0.55, 0.15, 0.10, 0.10, 0.10]),\n            \"p_test\": np.array([0.10, 0.25, 0.25, 0.20, 0.20]),\n            \"alpha\": 0.00,\n            \"k\": 10\n        },\n        # Test Case 4 (strong shift, small k)\n        {\n            \"p_train\": np.array([0.70, 0.10, 0.10, 0.05, 0.05]),\n            \"p_test\": np.array([0.05, 0.65, 0.10, 0.10, 0.10]),\n            \"alpha\": 1.00,\n            \"k\": 1\n        },\n    ]\n\n    results = []\n    \n    def compute_q(p, alpha):\n        \"\"\"Computes the noise distribution q from a data distribution p and exponent alpha.\"\"\"\n        p_pow_alpha = np.power(p, alpha)\n        normalization_constant = np.sum(p_pow_alpha)\n        return p_pow_alpha / normalization_constant\n\n    def risk_functional(s, p, q, k):\n        \"\"\"Computes the expected surrogate risk R(s; p, q, k).\"\"\"\n        # Uses the numerically stable form: R = sum(p*log(1+exp(-s)) + k*q*log(1+exp(s)))\n        # np.logaddexp(0, x) computes log(exp(0) + exp(x)) = log(1 + exp(x))\n        term1 = p * np.logaddexp(0, -s)\n        term2 = k * q * np.logaddexp(0, s)\n        return np.sum(term1 + term2)\n\n    def optimal_score(p, q, k):\n        \"\"\"Computes the Bayes-optimal score function s_star.\"\"\"\n        # s_star(w) = log(p(w) / (k * q(w)))\n        # Probabilities are strictly positive, so no log(0) issues.\n        return np.log(p) - np.log(k) - np.log(q)\n\n    for case in test_cases:\n        p_train = case[\"p_train\"]\n        p_test = case[\"p_test\"]\n        alpha = case[\"alpha\"]\n        k = case[\"k\"]\n\n        # Compute noise distributions\n        q_train = compute_q(p_train, alpha)\n        q_test = compute_q(p_test, alpha)\n        \n        # Compute the score function learned during training\n        s_train = optimal_score(p_train, q_train, k)\n\n        # --- Regime A ---\n        # Evaluation with test data distribution but training noise distribution\n        \n        # Optimal score for Regime A's setting (p_test, q_train)\n        s_star_test_A = optimal_score(p_test, q_train, k)\n        \n        # Risk of the trained model in Regime A\n        risk_A_trained = risk_functional(s_train, p_test, q_train, k)\n        \n        # Minimal possible risk in Regime A\n        risk_A_optimal = risk_functional(s_star_test_A, p_test, q_train, k)\n        \n        cost_A = risk_A_trained - risk_A_optimal\n        results.append(cost_A)\n        \n        # --- Regime B ---\n        # Evaluation with test data distribution and test noise distribution\n        \n        # Optimal score for Regime B's setting (p_test, q_test)\n        s_star_test_B = optimal_score(p_test, q_test, k)\n        \n        # Risk of the trained model in Regime B\n        risk_B_trained = risk_functional(s_train, p_test, q_test, k)\n        \n        # Minimal possible risk in Regime B\n        risk_B_optimal = risk_functional(s_star_test_B, p_test, q_test, k)\n        \n        cost_B = risk_B_trained - risk_B_optimal\n        results.append(cost_B)\n\n    # Format output to 6 decimal places as requested\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3156698"}, {"introduction": "A key challenge in negative sampling is the risk of 'false negatives'—samples that are semantically positive but are accidentally chosen as negatives, which can confuse the model. This practice introduces a powerful technique to build resilience against such noisy data [@problem_id:3156749]. You will implement a robust, weighted loss function that learns to down-weight the influence of suspicious negatives, thereby safeguarding the integrity of the learning signal and improving model performance.", "problem": "You are given a single positive pair score and a set of negative pair scores for a contrastive learning setup with negative sampling. Assume the standard softmax cross-entropy base: for a vector of logits $\\{z_j\\}$, the softmax assigns probability $\\exp(z_j)/\\sum_\\ell \\exp(z_\\ell)$, and the cross-entropy loss for the true class is the negative logarithm of this probability.\n\nStarting from this base, consider a scenario with one positive logit $z_{+}$ and $K$ negative logits $\\{z_i^{-}\\}_{i=1}^{K}$, each constructed from similarity scores $\\{s\\}$ and a temperature $\\tau$ via $z = s / \\tau$. In standard contrastive learning with negative sampling, a negative is treated as a negative with certainty. However, in the presence of noisy negatives, a negative item $i$ is actually a positive with probability $\\epsilon_i$ (unknown), and you have per-negative estimates $\\hat{\\epsilon}_i \\in [0,1]$. A robust approach seeks to downweight the contribution of each negative $i$ proportionally to its estimated cleanliness, using weights $w_i \\propto (1 - \\hat{\\epsilon}_i)$, with a normalization chosen so that $\\sum_{i=1}^{K} w_i = K$ when $\\sum_{i=1}^{K} (1 - \\hat{\\epsilon}_i)  0$, and $w_i = 0$ for all $i$ otherwise.\n\nYour tasks are:\n- Derive the unweighted contrastive loss for one positive and $K$ negatives from the softmax cross-entropy base.\n- Derive the weighted counterpart by replacing each negative’s unnormalized contribution with $w_i \\exp(z_i^{-})$, where $w_i$ is defined as above. Justify this choice using importance-weighting logic under the noisy negative model.\n- Derive the gradient of the loss with respect to any negative logit $z_i^{-}$ in both the unweighted and weighted cases.\n- Implement a numerically stable program that:\n  1. Computes the unweighted loss $L_{\\mathrm{std}}$ and the weighted loss $L_{\\mathrm{rob}}$ for given $s_{+}$, $\\{s_i^{-}\\}$, $\\tau$, and $\\{\\hat{\\epsilon}_i\\}$.\n  2. Computes the gradient magnitude with respect to a specified negative logit in both settings, and returns their ratio $r = \\|\\partial L_{\\mathrm{rob}}/\\partial z_i^{-}\\| / \\|\\partial L_{\\mathrm{std}}/\\partial z_i^{-}\\|$ for that index.\n\nNumerical stability requirement: implement all softmax-like sums using a log-sum-exp trick or an equivalent approach that is stable for large-magnitude logits.\n\nAngle and physical units do not apply. All outputs must be real numbers or integers as specified below.\n\nTest suite to implement and evaluate:\n- Test $1$ (clean negatives, invariance): $s_{+} = 2.5$, $\\{s_i^{-}\\} = [-0.2, 0.1, -0.5, 0.3]$, $\\tau = 0.5$, $\\hat{\\epsilon} = [0, 0, 0, 0]$. Output the absolute difference $\\lvert L_{\\mathrm{std}} - L_{\\mathrm{rob}} \\rvert$ as a float.\n- Test $2$ (single obvious false negative, robustness): $s_{+} = 0.8$, $\\{s_i^{-}\\} = [0.75, -0.1, -0.2]$, $\\tau = 0.2$, $\\hat{\\epsilon} = [1, 0, 0]$. Output the integer $1$ if $L_{\\mathrm{rob}}  L_{\\mathrm{std}}$, else $0$.\n- Test $3$ (mixed noise weighting): $s_{+} = 1.0$, $\\{s_i^{-}\\} = [0.9, 0.1, -0.3, 0.2]$, $\\tau = 0.7$, $\\hat{\\epsilon} = [0.8, 0.2, 0, 0.5]$. Output $L_{\\mathrm{std}}$ and $L_{\\mathrm{rob}}$ as two floats, in this order.\n- Test $4$ (all negatives predicted corrupted, boundary): $s_{+} = 1.2$, $\\{s_i^{-}\\} = [1.1, 0.9]$, $\\tau = 0.3$, $\\hat{\\epsilon} = [1, 1]$. Output $L_{\\mathrm{rob}}$ as a float.\n- Test $5$ (gradient suppression for a fully corrupted negative): reuse Test $2$ and compute the gradient ratio $r$ for the negative at index $0$. Output $r$ as a float.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[x_1,x_2,x_3,x_4,x_5,x_6]$).\n- For the five tests above, your program must output a single list containing, in order: the float from Test $1$, the integer from Test $2$, then the two floats from Test $3$, then the float from Test $4$, and finally the float from Test $5$, i.e., $[T1,T2,T3\\_L\\_{\\mathrm{std}},T3\\_L\\_{\\mathrm{rob}},T4,T5]$.", "solution": "The problem requires the derivation and implementation of standard and robust contrastive loss functions, along with their gradients, grounded in the principles of softmax cross-entropy and importance sampling. We shall proceed with a rigorous, step-by-step derivation.\n\n### Part 1: Derivation of the Unweighted Contrastive Loss ($L_{\\mathrm{std}}$)\n\nThe foundation of the loss is the softmax cross-entropy framework. For a set of logits $\\{z_j\\}$, the softmax function computes the probability of the $j$-th class as:\n$$ P_j = \\frac{\\exp(z_j)}{\\sum_{\\ell} \\exp(z_\\ell)} $$\nThe cross-entropy loss for the true class, say class $c$, is the negative logarithm of its predicted probability:\n$$ L = -\\log(P_c) $$\nIn our contrastive learning setup, we have one positive pair with score $s_+$ and $K$ negative pairs with scores $\\{s_i^{-}\\}_{i=1}^{K}$. The logits are given by $z = s / \\tau$, where $\\tau$ is a temperature parameter. This gives one positive logit $z_+ = s_+ / \\tau$ and $K$ negative logits $z_i^{-} = s_i^{-} / \\tau$.\n\nThe set of all logits for the classification task is $\\{z_+\\} \\cup \\{z_i^{-}\\}_{i=1}^{K}$. The \"true class\" is the positive pair. Therefore, the probability assigned to the positive pair is:\n$$ P(\\text{positive}) = \\frac{\\exp(z_+)}{\\exp(z_+) + \\sum_{i=1}^{K} \\exp(z_i^{-})} $$\nThe standard contrastive loss, $L_{\\mathrm{std}}$, is the negative log-likelihood of correctly identifying the positive pair:\n$$ L_{\\mathrm{std}} = -\\log\\left(\\frac{\\exp(z_+)}{\\exp(z_+) + \\sum_{i=1}^{K} \\exp(z_i^{-})}\\right) $$\nUsing the property of logarithms $\\log(a/b) = \\log(a) - \\log(b)$, we can rewrite the loss as:\n$$ L_{\\mathrm{std}} = -\\left( \\log(\\exp(z_+)) - \\log\\left(\\exp(z_+) + \\sum_{i=1}^{K} \\exp(z_i^{-})\\right) \\right) $$\n$$ L_{\\mathrm{std}} = -z_+ + \\log\\left(\\exp(z_+) + \\sum_{i=1}^{K} \\exp(z_i^{-})\\right) $$\nThis is a common form of the InfoNCE loss.\n\n### Part 2: Derivation and Justification of the Weighted Contrastive Loss ($L_{\\mathrm{rob}}$)\n\nThe standard loss assumes all sampled negatives are truly negative. In a realistic setting, some negatives may be \"false negatives\" (i.e., semantically positive pairs that were accidentally sampled as negatives). Let $\\epsilon_i$ be the probability that the $i$-th negative sample is actually a positive. The standard loss is an estimator for an expectation over a \"clean\" distribution of negatives, but we are sampling from a noisy one.\n\nImportance sampling provides a principled way to correct for this distributional mismatch. We want to downweight the contribution of samples that are likely to be false negatives. The problem provides per-negative estimates $\\hat{\\epsilon}_i$ for $\\epsilon_i$. A sample $i$ is estimated to be a true negative with probability $1 - \\hat{\\epsilon}_i$. We can thus re-weight the contribution of each negative logit in the partition function (the denominator of the softmax) by a factor proportional to its estimated \"cleanliness\", $(1 - \\hat{\\epsilon}_i)$.\n\nThe problem defines the weights $w_i$ as:\n$$ w_i = \\begin{cases} \\frac{K (1 - \\hat{\\epsilon}_i)}{\\sum_{j=1}^{K} (1 - \\hat{\\epsilon}_j)}  \\text{if } \\sum_{j=1}^{K} (1 - \\hat{\\epsilon}_j)  0 \\\\ 0  \\text{otherwise} \\end{cases} $$\nThis normalization ensures that $\\sum_{i=1}^{K} w_i = K$ when at least one negative is not fully corrupted, preserving the scale of the partition function relative to the unweighted case when all negatives are clean (i.e., all $\\hat{\\epsilon}_i = 0$, which implies all $w_i = 1$).\n\nThe robust loss, $L_{\\mathrm{rob}}$, is formed by replacing each term $\\exp(z_i^{-})$ in the partition function with its weighted counterpart, $w_i \\exp(z_i^{-})$.\n$$ P_{\\mathrm{rob}}(\\text{positive}) = \\frac{\\exp(z_+)}{\\exp(z_+) + \\sum_{i=1}^{K} w_i \\exp(z_i^{-})} $$\nThe corresponding loss is:\n$$ L_{\\mathrm{rob}} = -\\log\\left(P_{\\mathrm{rob}}(\\text{positive})\\right) = -z_+ + \\log\\left(\\exp(z_+) + \\sum_{i=1}^{K} w_i \\exp(z_i^{-})\\right) $$\n\n### Part 3: Gradient Derivation\n\nWe now derive the gradient of each loss with respect to an arbitrary negative logit $z_j^{-}$.\n\n**Gradient of $L_{\\mathrm{std}}$:**\nLet $S_{\\mathrm{std}} = \\exp(z_+) + \\sum_{i=1}^{K} \\exp(z_i^{-})$. The loss is $L_{\\mathrm{std}} = -z_+ + \\log(S_{\\mathrm{std}})$.\n$$ \\frac{\\partial L_{\\mathrm{std}}}{\\partial z_j^{-}} = \\frac{\\partial}{\\partial z_j^{-}} \\left( -z_+ + \\log(S_{\\mathrm{std}}) \\right) = \\frac{1}{S_{\\mathrm{std}}} \\frac{\\partial S_{\\mathrm{std}}}{\\partial z_j^{-}} $$\nThe derivative of the sum $S_{\\mathrm{std}}$ with respect to $z_j^{-}$ is $\\exp(z_j^{-})$, as the terms are independent.\n$$ \\frac{\\partial L_{\\mathrm{std}}}{\\partial z_j^{-}} = \\frac{\\exp(z_j^{-})}{\\exp(z_+) + \\sum_{i=1}^{K} \\exp(z_i^{-})} $$\nThis is the softmax probability computed for the $j$-th negative sample over the set of all $K+1$ samples.\n\n**Gradient of $L_{\\mathrm{rob}}$:**\nLet $S_{\\mathrm{rob}} = \\exp(z_+) + \\sum_{i=1}^{K} w_i \\exp(z_i^{-})$. The loss is $L_{\\mathrm{rob}} = -z_+ + \\log(S_{\\mathrm{rob}})$. The weights $w_i$ are functions of $\\{\\hat{\\epsilon}_k\\}$ and are constant with respect to the logits $\\{z_k^{-}\\}$.\n$$ \\frac{\\partial L_{\\mathrm{rob}}}{\\partial z_j^{-}} = \\frac{\\partial}{\\partial z_j^{-}} \\left( -z_+ + \\log(S_{\\mathrm{rob}}) \\right) = \\frac{1}{S_{\\mathrm{rob}}} \\frac{\\partial S_{\\mathrm{rob}}}{\\partial z_j^{-}} $$\nThe derivative of the sum $S_{\\mathrm{rob}}$ with respect to $z_j^{-}$ is $w_j \\exp(z_j^{-})$.\n$$ \\frac{\\partial L_{\\mathrm{rob}}}{\\partial z_j^{-}} = \\frac{w_j \\exp(z_j^{-})}{\\exp(z_+) + \\sum_{i=1}^{K} w_i \\exp(z_i^{-})} $$\nThis gradient is a weighted version of the softmax probability. If a sample $j$ is deemed fully corrupted ($\\hat{\\epsilon}_j=1$), its weight $w_j$ becomes $0$, and consequently, the gradient with respect to its logit $z_j^{-}$ also becomes $0$. This effectively stops the model from learning to push this (likely false) negative pair further apart.\n\n### Part 4: Numerical Stability\n\nThe direct computation of expressions involving $\\exp(z)$ can lead to numerical overflow if $z$ is large, and underflow if $z$ is very negative. A standard technique to mitigate this is the log-sum-exp trick. The identity is:\n$$ \\log\\left(\\sum_j \\exp(x_j)\\right) = c + \\log\\left(\\sum_j \\exp(x_j - c)\\right), \\quad \\text{where } c = \\max_j(x_j) $$\nThis transformation prevents overflow because the maximum argument to $\\exp$ becomes $0$.\n\nFor $L_{\\mathrm{std}}$, we apply this directly to the logits $\\{z_+, z_1^{-}, \\dots, z_K^{-}\\}$.\nFor $L_{\\mathrm{rob}}$, the sum is $\\exp(z_+) + \\sum_i w_i \\exp(z_i^{-})$. This can be rewritten for numerical stability by factoring the log into the exponent, for $w_i0$: $\\exp(\\log(w_i) + z_i^{-})$. The expression for the loss is then:\n$$ L_{\\mathrm{rob}} = -z_+ + \\log\\left(\\exp(z_+) + \\sum_{i: w_i0} \\exp(\\log(w_i) + z_i^{-})\\right) $$\nWe can now apply the log-sum-exp trick to the set of modified logits $\\{z_+\\} \\cup \\{\\log(w_i) + z_i^{-} \\mid w_i  0\\}$. If all $w_i=0$, the sum is $0$ and $L_{\\mathrm{rob}} = -z_+ + \\log(\\exp(z_+)) = 0$.\nThe gradients, being softmax-like expressions, can be stabilized similarly:\n$$ \\frac{\\exp(x_j)}{\\sum_k \\exp(x_k)} = \\frac{\\exp(x_j - c)}{\\sum_k \\exp(x_k - c)}, \\quad c = \\max_k(x_k) $$\nThis is equivalent to computing $\\exp(x_j - \\text{LogSumExp}(\\{x_k\\}))$. This will be used in the implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the five test cases specified in the problem statement.\n    \"\"\"\n\n    def compute_loss_and_grad(s_plus, s_neg, tau, eps_hat, grad_idx=None):\n        \"\"\"\n        Computes standard and robust losses and gradients.\n        \"\"\"\n        z_plus = s_plus / tau\n        z_neg = np.array(s_neg) / tau\n        K = len(z_neg)\n\n        # Log-sum-exp utility for numerical stability\n        def log_sum_exp(x):\n            if x.size == 0:\n                return -np.inf  # log(0)\n            c = np.max(x)\n            return c + np.log(np.sum(np.exp(x - c)))\n\n        # --- Part 1: Unweighted Loss and Gradient ---\n        all_logits_std = np.concatenate(([z_plus], z_neg))\n        lse_std = log_sum_exp(all_logits_std)\n        l_std = -z_plus + lse_std\n\n        grad_std = None\n        if grad_idx is not None:\n            # Gradient is the softmax probability of the negative sample\n            grad_std = np.exp(z_neg[grad_idx] - lse_std)\n\n        # --- Part 2: Weighted Loss and Gradient ---\n        # Calculate weights\n        one_minus_eps = 1.0 - np.array(eps_hat)\n        sum_one_minus_eps = np.sum(one_minus_eps)\n        \n        weights = np.zeros(K)\n        # As per problem: weights are non-zero only if sum > 0\n        if sum_one_minus_eps > 0:\n            weights = K * one_minus_eps / sum_one_minus_eps\n\n        # Robust Loss\n        # Handle the case where all negatives are marked as corrupted\n        if sum_one_minus_eps = 0:\n            l_rob = 0.0\n            lse_rob = z_plus # Analytically, log(exp(z_plus))\n        else:\n            # Use log-sum-exp with modified logits for stability\n            valid_mask = weights > 0\n            log_weights = np.log(weights[valid_mask])\n            modified_neg_logits = z_neg[valid_mask] + log_weights\n            \n            all_logits_rob = np.concatenate(([z_plus], modified_neg_logits))\n            lse_rob = log_sum_exp(all_logits_rob)\n            l_rob = -z_plus + lse_rob\n\n        # Robust Gradient\n        grad_rob = None\n        if grad_idx is not None:\n            w_j = weights[grad_idx]\n            if w_j = 0:\n                grad_rob = 0.0\n            else:\n                # Gradient is the weighted softmax probability\n                grad_rob = np.exp(np.log(w_j) + z_neg[grad_idx] - lse_rob)\n        \n        return l_std, l_rob, grad_std, grad_rob\n\n    results = []\n    \n    # Test 1: clean negatives, invariance\n    s_plus, s_neg, tau, eps_hat = 2.5, [-0.2, 0.1, -0.5, 0.3], 0.5, [0, 0, 0, 0]\n    l_std_1, l_rob_1, _, _ = compute_loss_and_grad(s_plus, s_neg, tau, eps_hat)\n    results.append(abs(l_std_1 - l_rob_1))\n    \n    # Test 2: single obvious false negative, robustness\n    s_plus, s_neg, tau, eps_hat = 0.8, [0.75, -0.1, -0.2], 0.2, [1, 0, 0]\n    l_std_2, l_rob_2, _, _ = compute_loss_and_grad(s_plus, s_neg, tau, eps_hat)\n    results.append(1 if l_rob_2  l_std_2 else 0)\n\n    # Test 3: mixed noise weighting\n    s_plus, s_neg, tau, eps_hat = 1.0, [0.9, 0.1, -0.3, 0.2], 0.7, [0.8, 0.2, 0, 0.5]\n    l_std_3, l_rob_3, _, _ = compute_loss_and_grad(s_plus, s_neg, tau, eps_hat)\n    results.append(l_std_3)\n    results.append(l_rob_3)\n\n    # Test 4: all negatives predicted corrupted, boundary\n    s_plus, s_neg, tau, eps_hat = 1.2, [1.1, 0.9], 0.3, [1, 1]\n    _, l_rob_4, _, _ = compute_loss_and_grad(s_plus, s_neg, tau, eps_hat)\n    results.append(l_rob_4)\n\n    # Test 5: gradient suppression for a fully corrupted negative\n    s_plus, s_neg, tau, eps_hat, grad_idx = 0.8, [0.75, -0.1, -0.2], 0.2, [1, 0, 0], 0\n    _, _, grad_std_5, grad_rob_5 = compute_loss_and_grad(s_plus, s_neg, tau, eps_hat, grad_idx)\n    # The gradients are non-negative, so magnitude is the value itself.\n    # grad_std_5 will be non-zero for these inputs.\n    ratio = grad_rob_5 / grad_std_5\n    results.append(ratio)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3156749"}, {"introduction": "Beyond the loss function, the choice of the negative sampling distribution itself is critical for efficient training. This advanced practice reframes the problem through the lens of statistical estimation theory, treating the negative sampling process as a form of importance sampling [@problem_id:3156767]. Your task will be to find the optimal mix of frequency-based and hardness-aware sampling to minimize the variance of the gradient estimator, leading to faster convergence and more stable training.", "problem": "You are given a finite set of candidate negative items indexed by $i \\in \\{1,\\dots,n\\}$ with a base data distribution $p(i)$ over items. Consider the task of estimating an expectation of the form $\\mu = \\mathbb{E}_{i \\sim p}[h(i)]$ where $h(i) \\ge 0$ quantifies the magnitude of a per-item contribution to a gradient term in a sampled objective. A standard unbiased importance sampling estimator using $k$ independent negative samples drawn from a proposal distribution $q$ is\n$$\n\\widehat{\\mu} \\;=\\; \\frac{1}{k}\\sum_{j=1}^{k} \\frac{p(i_j)}{q(i_j)}\\,h(i_j), \\quad i_j \\sim q \\text{ independently}.\n$$\nThis estimator has variance\n$$\n\\mathrm{Var}[\\widehat{\\mu}] \\;=\\; \\frac{1}{k}\\left(\\sum_{i=1}^{n} \\frac{p(i)^{2} h(i)^{2}}{q(i)} \\;-\\; \\mu^{2}\\right),\n$$\nwhich is minimized, for a fixed $k$, by appropriately choosing $q$. In negative sampling practice, one often uses a mixture sampler that combines a frequency-based distribution with a hardness-based distribution. Define the mixture sampler\n$$\nq_{\\lambda}(i) \\;=\\; \\lambda\\, q_{\\mathrm{freq}}(i) \\;+\\; (1-\\lambda)\\, q_{\\mathrm{hard}}(i),\n$$\nwhere $q_{\\mathrm{freq}}(i) = p(i)$ and $q_{\\mathrm{hard}}(i) \\propto h(i)$, that is,\n$$\nq_{\\mathrm{hard}}(i) \\;=\\; \\frac{h(i)}{\\sum_{j=1}^{n} h(j)}.\n$$\nFor a fixed sampling budget $k \\in \\mathbb{N}$, the generalization-error proxy in this setting is taken to be the variance $\\mathrm{Var}[\\widehat{\\mu}]$, because lower estimator variance for the negative term yields lower noise in training signals and improved generalization. Your task is to choose the mixture weight $\\lambda \\in [0,1]$ that minimizes $\\mathrm{Var}[\\widehat{\\mu}]$ under the mixture family $q_{\\lambda}$.\n\nStarting from the fundamental definition of importance sampling and the variance formula above, derive an algorithm that, given a discrete distribution $p$ and nonnegative weights $h$, finds the optimal $\\lambda^{\\star} \\in [0,1]$ minimizing $\\mathrm{Var}[\\widehat{\\mu}]$. Assume $h(i)  0$ for all $i$ in the provided test cases so that $q_{\\mathrm{hard}}(i)$ is well-defined and strictly positive.\n\nImplementation requirements:\n- Your program must implement a numerically stable solver for $\\lambda^{\\star}$ using only the principle that the variance is minimized when the function\n$$\nJ(\\lambda) \\;=\\; \\sum_{i=1}^{n} \\frac{p(i)^{2} h(i)^{2}}{\\lambda\\,p(i) + (1-\\lambda)\\,q_{\\mathrm{hard}}(i)}\n$$\nis minimized over $\\lambda \\in [0,1]$.\n- In case multiple $\\lambda$ achieve the same minimum within a numerical tolerance of $\\varepsilon = 10^{-12}$, return the smallest such $\\lambda$.\n- For each test case, output only the optimal $\\lambda^{\\star}$ rounded to four decimal places.\n\nTest suite:\n- Case A (interior optimum expected): $n=5$, $p = [\\,0.40,\\,0.25,\\,0.20,\\,0.10,\\,0.05\\,]$, $h = [\\,1.0,\\,2.0,\\,2.5,\\,3.0,\\,5.0\\,]$, $k = 10$.\n- Case B (boundary at $\\lambda = 1$ expected): $n=4$, $p = [\\,0.50,\\,0.30,\\,0.15,\\,0.05\\,]$, $h = [\\,1.0,\\,1.0,\\,1.0,\\,1.0\\,]$, $k = 7$.\n- Case C (boundary at $\\lambda = 0$ likely): $n=5$, $p = [\\,0.40,\\,0.25,\\,0.20,\\,0.10,\\,0.05\\,]$, $h = [\\,0.1,\\,0.1,\\,0.5,\\,4.0,\\,10.0\\,]$, $k = 3$.\n- Case D (degenerate equality $q_{\\mathrm{hard}} = p$): $n=4$, $p = [\\,0.40,\\,0.30,\\,0.20,\\,0.10\\,]$, $h = [\\,4.0,\\,3.0,\\,2.0,\\,1.0\\,]$, $k = 5$.\n\nFinal output format:\n- Your program should produce a single line of output containing the four optimal values $\\lambda^{\\star}$ for Cases A, B, C, and D, as a comma-separated list enclosed in square brackets, for example, $[\\lambda_{A},\\lambda_{B},\\lambda_{C},\\lambda_{D}]$, where each entry is rounded to four decimal places.", "solution": "The objective is to find the mixture weight $\\lambda \\in [0,1]$ that minimizes the variance of the importance sampling estimator, $\\mathrm{Var}[\\widehat{\\mu}]$. The variance is given by:\n$$\n\\mathrm{Var}[\\widehat{\\mu}] = \\frac{1}{k}\\left(\\sum_{i=1}^{n} \\frac{p(i)^{2} h(i)^{2}}{q(i)} - \\mu^{2}\\right)\n$$\nTo minimize $\\mathrm{Var}[\\widehat{\\mu}]$ with respect to the choice of sampling distribution $q$, we only need to consider the term that depends on $q$. Since $k$, $p(i)$, $h(i)$, and $\\mu$ are fixed for this optimization problem, the task is equivalent to minimizing the summation term. The proposal distribution is a mixture model $q_{\\lambda}(i) = \\lambda\\, q_{\\mathrm{freq}}(i) + (1-\\lambda)\\, q_{\\mathrm{hard}}(i)$, where $q_{\\mathrm{freq}}(i) = p(i)$ and $q_{\\mathrm{hard}}(i) = h(i) / \\sum_{j=1}^{n} h(j)$. Substituting $q_{\\lambda}(i)$ for $q(i)$, we obtain an objective function $J(\\lambda)$ to be minimized over the interval $\\lambda \\in [0,1]$:\n$$\nJ(\\lambda) = \\sum_{i=1}^{n} \\frac{p(i)^{2} h(i)^{2}}{\\lambda\\,p(i) + (1-\\lambda)\\,q_{\\mathrm{hard}}(i)}\n$$\nThis function $J(\\lambda)$ is a sum of individual functions, one for each item $i \\in \\{1, \\dots, n\\}$. Let's denote each term by $f_i(\\lambda)$:\n$$\nf_i(\\lambda) = \\frac{c_i}{\\ell_i(\\lambda)}\n$$\nwhere $c_i = p(i)^2 h(i)^2$ is a non-negative constant, and $\\ell_i(\\lambda) = \\lambda p(i) + (1-\\lambda) q_{\\mathrm{hard}}(i)$ is the denominator, which is a linear function of $\\lambda$. The problem assumes that $h(i)  0$ for all $i$, which implies $q_{\\mathrm{hard}}(i)  0$. As $p(i)$ is a probability distribution over the $n$ items, we can assume $p(i)0$ for all $i$ in the support. Therefore, for $\\lambda \\in [0,1]$, $\\ell_i(\\lambda)$ is a convex combination of two strictly positive numbers, $p(i)$ and $q_{\\mathrm{hard}}(i)$, and is thus strictly positive.\n\nTo determine the nature of the optimization problem, we analyze the convexity of $J(\\lambda)$. The second derivative of each term $f_i(\\lambda)$ with respect to $\\lambda$ is:\n$$\n\\frac{d^2 f_i}{d\\lambda^2} = \\frac{d^2}{d\\lambda^2} \\left( c_i (\\ell_i(\\lambda))^{-1} \\right) = \\frac{d}{d\\lambda} \\left( -c_i (\\ell_i(\\lambda))^{-2} \\frac{d\\ell_i}{d\\lambda} \\right)\n$$\nThe derivative of the denominator is $\\frac{d\\ell_i}{d\\lambda} = p(i) - q_{\\mathrm{hard}}(i)$. So,\n$$\n\\frac{d^2 f_i}{d\\lambda^2} = -c_i \\left( -2 (\\ell_i(\\lambda))^{-3} \\left(\\frac{d\\ell_i}{d\\lambda}\\right)^2 \\right) = \\frac{2c_i(p(i) - q_{\\mathrm{hard}}(i))^2}{(\\lambda p(i) + (1-\\lambda) q_{\\mathrm{hard}}(i))^3}\n$$\nSince $c_i \\ge 0$ and the denominator is positive, the second derivative $\\frac{d^2 f_i}{d\\lambda^2}$ is non-negative for all $\\lambda \\in [0,1]$. This proves that each function $f_i(\\lambda)$ is convex. The sum of convex functions is also convex, so $J(\\lambda)$ is a convex function on the interval $[0,1]$.\n\nThe minimum of a convex function on a closed interval occurs either at one of the boundaries ($\\lambda=0$ or $\\lambda=1$) or at an interior point $\\lambda^{\\star} \\in (0,1)$ where the first derivative is zero. The first derivative of $J(\\lambda)$ is:\n$$\nJ'(\\lambda) = \\frac{dJ}{d\\lambda} = \\sum_{i=1}^{n} \\frac{-c_i(p(i) - q_{\\mathrm{hard}}(i))}{(\\ell_i(\\lambda))^2} = - \\sum_{i=1}^{n} \\frac{p(i)^2 h(i)^2 (p(i) - q_{\\mathrm{hard}}(i))}{(\\lambda p(i) + (1-\\lambda) q_{\\mathrm{hard}}(i))^2}\n$$\nSince $J(\\lambda)$ is convex, its derivative $J'(\\lambda)$ is a monotonically non-decreasing function. This property allows for an efficient search for the optimal $\\lambda^{\\star}$:\n1.  If $J'(0) \\ge 0$, the function is non-decreasing on $[0,1]$, so the minimum is at $\\lambda^{\\star}=0$.\n2.  If $J'(1) \\le 0$, the function is non-increasing on $[0,1]$, so the minimum is at $\\lambda^{\\star}=1$.\n3.  If $J'(0)  0$ and $J'(1)  0$, the continuity and monotonicity of $J'(\\lambda)$ guarantee a unique root $\\lambda^{\\star} \\in (0,1)$ where $J'(\\lambda^{\\star})=0$. This root corresponds to the unique minimum of $J(\\lambda)$.\n\nThis leads to the following algorithm:\nFirst, compute the derivatives at the boundaries, $J'(0)$ and $J'(1)$.\n$$\nJ'(0) = - \\sum_{i=1}^{n} \\frac{p(i)^2 h(i)^2 (p(i) - q_{\\mathrm{hard}}(i))}{q_{\\mathrm{hard}}(i)^2}\n$$\n$$\nJ'(1) = - \\sum_{i=1}^{n} \\frac{p(i)^2 h(i)^2 (p(i) - q_{\\mathrm{hard}}(i))}{p(i)^2} = - \\sum_{i=1}^{n} h(i)^2 (p(i) - q_{\\mathrm{hard}}(i))\n$$\nBased on their signs, we either identify $\\lambda^{\\star}$ as $0$ or $1$, or we proceed to find the root of $J'(\\lambda)=0$ in the interval $(0,1)$.\nFor the interior case, the equation $J'(\\lambda)=0$ is a non-linear equation that can be solved numerically. Given that $J'(\\lambda)$ is monotonic and we have bracketed a root between $\\lambda=0$ and $\\lambda=1$, the bisection method is a simple and robust choice for finding $\\lambda^{\\star}$. The bisection algorithm iteratively halves the search interval $[low, high]$ by evaluating $J'(mid)$ at the midpoint and updating the interval based on the sign, guaranteeing convergence to the unique root. In the special case where $p(i) = q_{\\mathrm{hard}}(i)$ for all $i$, $J'(\\lambda)=0$ for all $\\lambda \\in [0,1]$, making all values of $\\lambda$ minimizers. The problem requires returning the smallest one, which is $\\lambda^{\\star}=0$. This case is correctly handled by the condition $J'(0) \\ge 0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the optimization problem for the given test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'p': [0.40, 0.25, 0.20, 0.10, 0.05], 'h': [1.0, 2.0, 2.5, 3.0, 5.0]},\n        # Case B\n        {'p': [0.50, 0.30, 0.15, 0.05], 'h': [1.0, 1.0, 1.0, 1.0]},\n        # Case C\n        {'p': [0.40, 0.25, 0.20, 0.10, 0.05], 'h': [0.1, 0.1, 0.5, 4.0, 10.0]},\n        # Case D\n        {'p': [0.40, 0.30, 0.20, 0.10], 'h': [4.0, 3.0, 2.0, 1.0]},\n    ]\n\n    def find_optimal_lambda(p, h):\n        \"\"\"\n        Calculates the optimal lambda that minimizes the variance estimator.\n\n        Args:\n            p (list or np.ndarray): The base data distribution.\n            h (list or np.ndarray): The per-item contribution weights.\n\n        Returns:\n            float: The optimal lambda value.\n        \"\"\"\n        p_vec = np.array(p, dtype=np.float64)\n        h_vec = np.array(h, dtype=np.float64)\n\n        sum_h = np.sum(h_vec)\n        q_hard_vec = h_vec / sum_h if sum_h > 0 else np.full_like(p_vec, 1.0/len(p_vec))\n\n        def J_prime(lam, p, h, q_hard):\n            \"\"\"\n            Computes the derivative of the objective function J with respect to lambda.\n            \"\"\"\n            numerator = p**2 * h**2 * (p - q_hard)\n            denominator = (lam * p + (1.0 - lam) * q_hard)**2\n            \n            # To handle potential division by zero in theory, though not \n            # expected with p(i)>0, h(i)>0.\n            # Using np.divide with a where clause is safer than adding an epsilon.\n            terms = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)\n            \n            return -np.sum(terms)\n\n        # A small tolerance for floating-point comparisons near zero.\n        # This is based on the logic that if the derivative is extremely\n        # close to zero at a boundary, we can consider the optimum to be there.\n        tol = 1e-12\n\n        # 1. Check boundary at lambda = 0\n        J_prime_at_0 = J_prime(0.0, p_vec, h_vec, q_hard_vec)\n        if J_prime_at_0 >= -tol:\n            return 0.0\n\n        # 2. Check boundary at lambda = 1\n        J_prime_at_1 = J_prime(1.0, p_vec, h_vec, q_hard_vec)\n        if J_prime_at_1 = tol:\n            return 1.0\n\n        # 3. Find interior minimum using bisection\n        low = 0.0\n        high = 1.0\n        # 100 iterations are sufficient for double-precision floating-point accuracy.\n        for _ in range(100):\n            mid = low + 0.5 * (high - low)\n            # If mid is indistinguishable from low or high, stop.\n            if mid == low or mid == high:\n                break\n            val = J_prime(mid, p_vec, h_vec, q_hard_vec)\n            if val  0:\n                low = mid\n            else:\n                high = mid\n        \n        return (low + high) / 2.0\n\n    results = []\n    for case in test_cases:\n        p_dist = case['p']\n        h_weights = case['h']\n        lambda_star = find_optimal_lambda(p_dist, h_weights)\n        results.append(lambda_star)\n\n    # Format results to four decimal places for the final output string.\n    formatted_results = [f\"{res:.4f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3156767"}]}