{"hands_on_practices": [{"introduction": "A key goal of representation learning is to untangle complex data so that a simple model, like a linear classifier, can solve the task. This exercise provides a hands-on look at this principle by tackling a problem that is not linearly separable in its original space. By learning a new representation that includes an interaction feature, we can transform the problem into one that a linear model can easily solve [@problem_id:3108536].", "problem": "You are given a synthetic binary hierarchical labeling scheme on two-dimensional inputs and two levels of representations. The goal is to evaluate whether a given representation yields linearly separable classes for coarse labels at shallower layers and for fine-grained labels only at deeper layers. Use the following fundamental base: the definition of linear separability, the notion of a feature representation as a deterministic mapping, and the feasibility interpretation of linear separability via a system of linear inequalities.\n\nDefinitions:\n- A dataset of inputs $\\{x_i\\}_{i=1}^N$ with $x_i \\in \\mathbb{R}^2$ is linearly separable with binary labels $y_i \\in \\{-1,+1\\}$ under a feature representation $z = \\phi(x)$ if there exists a weight vector $w \\in \\mathbb{R}^d$ and a bias $b \\in \\mathbb{R}$ such that $y_i (w^\\top z_i + b)  0$ for all $i \\in \\{1,\\dots,N\\}$, where $z_i = \\phi(x_i)$.\n- Coarse labels are defined as $y_i^{\\text{coarse}} = \\operatorname{sign}(x_{i,1})$, where $x_{i,1}$ is the first coordinate of $x_i$ and $\\operatorname{sign}(u) = +1$ if $u \\ge 0$ and $-1$ otherwise.\n- Fine-grained labels are defined as $y_i^{\\text{fine}} = \\operatorname{sign}(x_{i,1} \\cdot x_{i,2})$, which produces the classical exclusive-or structure when viewed in the raw input space.\n- The shallow representation is $\\phi_1(x) = x$ (identity mapping).\n- The deep representation is $\\phi_2(x) = [x_1, x_2, s \\cdot x_1 x_2]$, where $s \\in \\{0,1\\}$ controls whether the interaction feature $x_1 x_2$ is present ($s=1$) or absent ($s=0$).\n\nEvaluation procedure from first principles:\n- For a given representation $\\phi$ and binary labels $\\{y_i\\}$, check for linear separability by solving the system of linear inequalities $y_i (w^\\top z_i + b) \\ge 1$ for all $i$, with variables $(w,b)$ bounded in a box to ensure bounded feasibility search. If the system is feasible in that box, the dataset is considered linearly separable under $\\phi$. This encodes the strict inequality via a positive margin and uses a bounded search region to avoid unboundedness in the feasibility problem.\n\nData generation:\n- For each test case, generate $N$ points independently with $x_i$ drawn uniformly from $[-1,1]^2$. Add independent Gaussian noise $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2 I)$ to obtain $\\tilde{x}_i = x_i + \\epsilon_i$, where $\\sigma \\ge 0$ is the standard deviation and $I$ is the identity. Labels are computed from $\\tilde{x}_i$. Optionally, a random fraction $p \\in [0,1]$ of labels may be flipped to model label noise. All randomness must be reproducible.\n\nLinear separability check as feasibility in a bounded domain:\n- Form the constraint $y_i (w^\\top z_i + b) \\ge 1$ for all $i$. Equivalently, $-y_i (w^\\top z_i + b) \\le -1$. Use a box constraint $-L \\le w_j \\le L$ and $-L \\le b \\le L$ for all coordinates $j$, where $L  0$ is a large bound (e.g., $L = 10^6$) ensuring the feasible region captures the existence of a separating hyperplane if one exists with reasonable scale.\n\nYour program must:\n- Implement the data generation, hierarchical labeling, feature mapping for $\\phi_1$ and $\\phi_2$, and the linear separability check via feasibility of linear inequalities with box bounds.\n- Evaluate, for each test case, four booleans:\n  1. $B_{\\text{coarse},1}$: linear separability of coarse labels under $\\phi_1$,\n  2. $B_{\\text{fine},1}$: linear separability of fine-grained labels under $\\phi_1$,\n  3. $B_{\\text{coarse},2}$: linear separability of coarse labels under $\\phi_2$,\n  4. $B_{\\text{fine},2}$: linear separability of fine-grained labels under $\\phi_2$.\n- Aggregate results from all test cases into a single line of output containing a Python-style list of lists, one inner list per test case in the order above.\n\nTest suite (use exactly these parameter sets, with fixed random seed for reproducibility):\n- Test case $1$ (happy path): $N = 200$, $\\sigma = 0.05$, $s = 1$, $p = 0$, $L = 10^6$.\n- Test case $2$ (insufficient deep features): $N = 200$, $\\sigma = 0.05$, $s = 0$, $p = 0$, $L = 10^6$.\n- Test case $3$ (label noise edge case): $N = 200$, $\\sigma = 0.05$, $s = 1$, $p = 0.3$, $L = 10^6$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated Python list of lists, with booleans in each inner list in the exact order $\\left[B_{\\text{coarse},1}, B_{\\text{fine},1}, B_{\\text{coarse},2}, B_{\\text{fine},2}\\right]$ for each test case. For example, if there are three test cases, print something like $\\left[[\\text{True},\\text{False},\\text{True},\\text{True}],[\\dots],[\\dots]\\right]$.", "solution": "The problem statement has been evaluated and is determined to be **valid**. It is scientifically grounded in the fundamental principles of machine learning, specifically representation learning and linear separability. The problem is well-posed, with all necessary data, definitions, and procedures clearly specified. The language is objective and formal, presenting a solvable, non-trivial task that is relevant to the specified topic.\n\nThe solution proceeds as follows, based on the principles outlined in the problem.\n\n### 1. Data Generation and Labeling\nFor each test case, a dataset of $N$ points is generated. This process involves three steps:\nFirst, $N$ feature vectors $\\{x_i\\}_{i=1}^N$ are sampled independently from a uniform distribution over the two-dimensional space $[-1,1]^2$, i.e., $x_i \\sim U([-1,1]^2)$.\nSecond, to simulate real-world data imperfections, independent and identically distributed Gaussian noise is added to each point. The noisy data points are $\\tilde{x}_i = x_i + \\epsilon_i$, where the noise vector $\\epsilon_i$ is drawn from a zero-mean bivariate normal distribution with a diagonal covariance matrix, $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2 I)$. Here, $I$ is the $2 \\times 2$ identity matrix and $\\sigma$ is the noise standard deviation.\nThird, two sets of binary labels, coarse and fine-grained, are computed based on the noisy data $\\tilde{x}_i = [\\tilde{x}_{i,1}, \\tilde{x}_{i,2}]^\\top$:\n- **Coarse labels**: $y_i^{\\text{coarse}} = \\operatorname{sign}(\\tilde{x}_{i,1})$. The decision boundary is the vertical line $\\tilde{x}_{i,1} = 0$. Datasets with these labels are inherently linearly separable in the input space. The sign function is defined as $\\operatorname{sign}(u) = +1$ for $u \\ge 0$ and $\\operatorname{sign}(u) = -1$ for $u  0$.\n- **Fine-grained labels**: $y_i^{\\text{fine}} = \\operatorname{sign}(\\tilde{x}_{i,1} \\cdot \\tilde{x}_{i,2})$. This corresponds to the classical XOR problem, where points in the first and third quadrants are labeled $+1$, and points in the second and fourth quadrants are labeled $-1$. This labeling scheme is not linearly separable in the input space $\\mathbb{R}^2$.\n\nFinally, to model label noise, a fraction $p$ of the data points are randomly selected, and both their coarse and fine labels are flipped (i.e., $y_i \\to -y_i$). All random processes (data generation, noise, and label flipping) are seeded to ensure reproducibility.\n\n### 2. Feature Representations\nThe problem evaluates two different feature representations, $\\phi_1$ and $\\phi_2$, which map the input data $\\tilde{x} \\in \\mathbb{R}^2$ to a new feature space.\n- **Shallow representation**: $\\phi_1(\\tilde{x}) = \\tilde{x}$. This is the identity map, so the feature space is the same as the input space, $\\mathbb{R}^2$.\n- **Deep representation**: $\\phi_2(\\tilde{x}) = [\\tilde{x}_1, \\tilde{x}_2, s \\cdot \\tilde{x}_1 \\tilde{x}_2]^\\top$. This map augments the input features with a new feature, the product of the original two, controlled by the parameter $s \\in \\{0, 1\\}$. When $s=1$, this new feature is designed to make the XOR problem linearly separable. The feature space is $\\mathbb{R}^3$.\n\nFor each point $\\tilde{x}_i$, we thus obtain two feature vectors, $z_{i,1} = \\phi_1(\\tilde{x}_i)$ and $z_{i,2} = \\phi_2(\\tilde{x}_i)$.\n\n### 3. Verification of Linear Separability\nThe core of the task is to determine if a given set of labeled feature vectors $\\{(z_i, y_i)\\}_{i=1}^N$ is linearly separable. According to the provided definition, this is true if there exists a weight vector $w \\in \\mathbb{R}^d$ (where $d$ is the dimension of the feature space) and a bias term $b \\in \\mathbb{R}$ such that the strict inequality $y_i (w^\\top z_i + b)  0$ holds for all $i=1, \\dots, N$.\n\nTo make this condition computationally verifiable, it is strengthened to include a margin, resulting in the system of $N$ linear inequalities:\n$$y_i (w^\\top z_i + b) \\ge 1, \\quad \\forall i \\in \\{1, \\dots, N\\}$$\nThe existence of a solution $(w, b)$ to this system is equivalent to the existence of a separating hyperplane. This is a feasibility problem for a system of linear inequalities. To solve it, we use linear programming. The problem can be written in the standard form $A_{\\text{ub}} x \\le b_{\\text{ub}}$ required by many solvers. Our unknown is the vector $x = [w_1, \\dots, w_d, b]^\\top \\in \\mathbb{R}^{d+1}$. Each inequality $y_i (w^\\top z_i + b) \\ge 1$ is rewritten as:\n$$-y_i (w^\\top z_i + b) \\le -1$$\n$$-y_i z_i^\\top w - y_i b \\le -1$$\nThis defines the $i$-th row of the constraint matrix $A_{\\text{ub}}$ and vector $b_{\\text{ub}}$. The $i$-th row of $A_{\\text{ub}}$ is $[-y_i z_{i,1}, \\dots, -y_i z_{i,d}, -y_i]$, and the $i$-th element of $b_{\\text{ub}}$ is $-1$.\n\nThe problem also specifies box constraints on the variables, $-L \\le w_j \\le L$ and $-L \\le b \\le L$, to ensure the search for a feasible solution occurs within a bounded region.\n\nTo check for feasibility, we can solve a linear program with a zero objective function, $c = 0$. If the solver finds a feasible solution, the dataset is linearly separable under the given representation. This check is performed for four scenarios for each test case:\n1.  $B_{\\text{coarse},1}$: Coarse labels, shallow representation $\\phi_1$.\n2.  $B_{\\text{fine},1}$: Fine labels, shallow representation $\\phi_1$.\n3.  $B_{\\text{coarse},2}$: Coarse labels, deep representation $\\phi_2$.\n4.  $B_{\\text{fine},2}$: Fine labels, deep representation $\\phi_2$.\n\nThe implementation utilizes `numpy` for numerical computations and `scipy.optimize.linprog` to solve the linear feasibility problem. The success status returned by this function directly indicates whether the corresponding dataset and representation are linearly separable.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Set a fixed random seed for reproducibility of the entire script.\n    np.random.seed(0)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, sigma, s, p, L)\n        (200, 0.05, 1, 0.0, 1e6), # Test case 1\n        (200, 0.05, 0, 0.0, 1e6), # Test case 2\n        (200, 0.05, 1, 0.3, 1e6), # Test case 3\n    ]\n\n    all_results = []\n    for params in test_cases:\n        case_results = evaluate_case(*params)\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists is converted to a compact\n    # form without spaces to match the specified output style.\n    print(str(all_results).replace(\" \", \"\"))\n\ndef evaluate_case(N, sigma, s, p, L):\n    \"\"\"\n    Evaluates a single test case for the four specified conditions.\n\n    Args:\n        N (int): Number of data points.\n        sigma (float): Standard deviation of Gaussian noise.\n        s (int): Switch for the interaction feature in phi_2 (0 or 1).\n        p (float): Fraction of labels to flip.\n        L (float): Bound for the linear programming variables.\n\n    Returns:\n        list: A list of four booleans [B_coarse,1, B_fine,1, B_coarse,2, B_fine,2].\n    \"\"\"\n    # 1. Generate data\n    x_clean = np.random.uniform(-1, 1, size=(N, 2))\n    noise = np.random.normal(0, sigma, size=(N, 2))\n    xtilde = x_clean + noise\n\n    # 2. Compute hierarchical labels based on noisy data\n    # Coarse labels: sign(x_1)\n    y_coarse = np.ones(N)\n    y_coarse[xtilde[:, 0]  0] = -1\n\n    # Fine-grained labels: sign(x_1 * x_2)\n    y_fine = np.ones(N)\n    y_fine[(xtilde[:, 0] * xtilde[:, 1])  0] = -1\n\n    # 3. Apply label noise by flipping a fraction p of labels\n    if p  0:\n        num_flips = int(p * N)\n        if num_flips  0:\n            flip_indices = np.random.choice(N, size=num_flips, replace=False)\n            y_coarse[flip_indices] *= -1\n            y_fine[flip_indices] *= -1\n\n    # 4. Generate feature representations\n    # Shallow representation phi_1(x) = x\n    z1 = xtilde\n\n    # Deep representation phi_2(x) = [x1, x2, s*x1*x2]\n    z2 = np.c_[xtilde, s * xtilde[:, 0] * xtilde[:, 1]]\n\n    # 5. Evaluate linear separability for the four scenarios\n    B_coarse1 = check_linear_separability(z1, y_coarse, L)\n    B_fine1 = check_linear_separability(z1, y_fine, L)\n    B_coarse2 = check_linear_separability(z2, y_coarse, L)\n    B_fine2 = check_linear_separability(z2, y_fine, L)\n\n    return [B_coarse1, B_fine1, B_coarse2, B_fine2]\n\ndef check_linear_separability(z, y, L):\n    \"\"\"\n    Checks if a dataset is linearly separable by solving a feasibility problem.\n\n    Args:\n        z (np.ndarray): Feature vectors, shape (N, d).\n        y (np.ndarray): Labels, shape (N,).\n        L (float): Bound for weights and bias.\n\n    Returns:\n        bool: True if the dataset is linearly separable, False otherwise.\n    \"\"\"\n    N, d = z.shape\n\n    # We are solving a feasibility problem, so the objective function is zero.\n    c = np.zeros(d + 1)\n\n    # The constraints are y_i * (w^T z_i + b) = 1, which are rewritten as\n    # -y_i * z_i^T w - y_i * b = -1 for the solver.\n    # A_ub has shape (N, d+1)\n    A_ub = -np.c_[z * y[:, np.newaxis], y]\n    \n    # b_ub has shape (N,)\n    b_ub = -np.ones(N)\n\n    # Box constraints for all variables (d weights and 1 bias)\n    bounds = [(-L, L)] * (d + 1)\n\n    # Use scipy.optimize.linprog to find if a feasible solution exists.\n    # method='highs' is robust and is the default in recent scipy versions.\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n\n    # res.success is True if the optimizer found an optimal (and thus feasible) solution.\n    return res.success\n\n# Execute the main function when the script is run.\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3108536"}, {"introduction": "Beyond just learning from a single objective, we can actively shape what our representations encode by introducing additional, or \"auxiliary,\" learning tasks. This practice explores how adding a secondary goal—predicting an object's rotation—forces a representation to capture angle information that it would otherwise discard as irrelevant to the main task. You will use linear probing, a standard technique for evaluating a representation's quality, to measure the impact of this auxiliary loss [@problem_id:3108515].", "problem": "Construct a complete, runnable program that tests, on a controlled synthetic task, whether adding an auxiliary rotation prediction head encourages encoding of angle information in a latent representation $z$, as measured by linear regression from $z$ to angle after training. Your experiment must be designed and justified from core definitions in representation learning and optimization, without relying on any precomputed models or external data. Angles must be measured in radians.\n\nStart from the following universally accepted bases and definitions:\n- Empirical Risk Minimization (ERM): given data $\\{(x_i,y_i)\\}_{i=1}^N$ and a parametric model $f_\\theta$, minimize the empirical loss $\\frac{1}{N}\\sum_{i=1}^N \\ell(f_\\theta(x_i),y_i)$ over parameters $\\theta$.\n- Binary Cross-Entropy (BCE) for binary classification with target $y\\in\\{0,1\\}$ and predicted probability $p\\in(0,1)$: $\\ell_{\\mathrm{BCE}}(p,y) = -\\left[y\\log p + (1-y)\\log(1-p)\\right]$.\n- Mean Squared Error (MSE) for regression with target $t\\in\\mathbb{R}^k$ and prediction $\\hat{t}\\in\\mathbb{R}^k$: $\\ell_{\\mathrm{MSE}}(\\hat{t},t)=\\frac{1}{2}\\|\\hat{t}-t\\|_2^2$.\n- Linear probing: after training a representation $z$, fit a separate linear regressor to map $z$ to a target quantity and evaluate generalization error on a held-out test set.\n\nData generation. For each sample $i$, draw a scalar content variable $\\alpha_i \\sim \\mathcal{N}(0,1)$ and an angle $\\theta_i \\sim \\mathrm{Uniform}([-\\pi,\\pi])$. Construct a $3$-dimensional input\n$$\nx_i = \\begin{bmatrix} \\alpha_i \\\\ \\cos\\theta_i \\\\ \\sin\\theta_i \\end{bmatrix} + \\varepsilon_i,\n$$\nwhere noise $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2 I_3)$ with $\\sigma = 0.05$. Define a binary label $y_i = \\mathbb{I}[\\alpha_i \\ge 0]$ and a rotation target vector $t_i = \\begin{bmatrix} \\cos\\theta_i \\\\ \\sin\\theta_i \\end{bmatrix}$. Angles are in radians. Split into a training set of size $N_{\\mathrm{train}} = 800$ and a test set of size $N_{\\mathrm{test}} = 200$.\n\nModel. Use a linear encoder $f_\\phi(x) = W_e x + b_e$ with latent $z \\in \\mathbb{R}^{d_z}$. Use a binary classifier head $g_w(z) = \\sigma(w_c^\\top z + b_c)$ with logistic sigmoid $\\sigma(\\cdot)$ to predict $y$. Use an auxiliary rotation head $h_u(z) = U z + c$ to predict $t = [\\cos\\theta,\\sin\\theta]^\\top$.\n\nTraining objectives. Train by minimizing the average of the main classification loss and a weighted auxiliary loss:\n$$\n\\mathcal{L}(\\phi,w,u) = \\frac{1}{N_{\\mathrm{train}}}\\sum_{i=1}^{N_{\\mathrm{train}}} \\ell_{\\mathrm{BCE}}(g_w(f_\\phi(x_i)),y_i) + \\lambda \\cdot \\frac{1}{N_{\\mathrm{train}}}\\sum_{i=1}^{N_{\\mathrm{train}}} \\ell_{\\mathrm{MSE}}(h_u(f_\\phi(x_i)),t_i),\n$$\nwhere $\\lambda \\ge 0$ controls the auxiliary head strength. Optimize parameters $(\\phi,w,u)$ by batch gradient descent with learning rate $\\eta$ and small $\\ell_2$ weight decay $\\gamma$.\n\nEvaluation protocol. After training, freeze the encoder and fit a linear probe from $z$ to $t$ on the training set using ridge regression with regularization $\\rho$, then compute the test MSE:\n$$\n\\mathrm{MSE}_{\\mathrm{probe}} = \\frac{1}{2N_{\\mathrm{test}}}\\sum_{i=1}^{N_{\\mathrm{test}}}\\left\\| \\hat{t}_i - t_i \\right\\|_2^2,\n$$\nwhere $\\hat{t}_i$ is the probe prediction. Also compute test classification accuracy, defined as the fraction of correct predictions using threshold $0.5$ on the classifier output.\n\nExperimental comparison. For each test case, compare two training conditions on the same dataset and initialization seed:\n- Baseline: $\\lambda = 0$ (no auxiliary rotation loss).\n- Auxiliary: $\\lambda = 1$ (with auxiliary rotation loss).\n\nDefine a boolean outcome per test case as true if and only if both of the following hold:\n- The auxiliary condition improves angle decodability by at least a margin $\\tau$, i.e., $\\mathrm{MSE}_{\\mathrm{probe}}^{(\\lambda=0)} - \\mathrm{MSE}_{\\mathrm{probe}}^{(\\lambda=1)} \\ge \\tau$.\n- The auxiliary condition maintains acceptable main-task performance, i.e., test accuracy under $\\lambda=1$ is at least $a_{\\min}$.\n\nHyperparameters. Use the following fixed values: $N_{\\mathrm{train}} = 800$, $N_{\\mathrm{test}} = 200$, noise standard deviation $\\sigma = 0.05$, learning rate $\\eta = 0.05$, weight decay $\\gamma = 10^{-4}$, training epochs $T = 500$, ridge regression regularization $\\rho = 10^{-3}$, improvement margin $\\tau = 0.2$, accuracy threshold $a_{\\min} = 0.9$.\n\nTest suite. Run exactly three test cases, each specified by a pair $(d_z,s)$ of latent dimension and random seed:\n- Case $1$: $(d_z,s) = (3,0)$, expected to have sufficient capacity to encode both content and angle.\n- Case $2$: $(d_z,s) = (2,1)$, a capacity boundary case where encoding both content and angle requires compression.\n- Case $3$: $(d_z,s) = (1,2)$, an edge case with insufficient capacity for full angle encoding.\n\nRequired final output. Your program should produce a single line of output containing the three boolean results, in order for the three cases above, as a comma-separated Python-style list with no spaces, e.g., \"[true1,true2,true3]\" where each entry is either True or False. There must be exactly one line printed. No other output is permitted.", "solution": "We formalize and solve the posed question using core definitions of representation learning and optimization. The goal is to determine whether an auxiliary rotation prediction head encourages the latent representation $z$ to encode angle information in a way that is linearly decodable after training.\n\nFoundational setup. We adopt Empirical Risk Minimization (ERM) to train a parametric model. The dataset is generated such that the main label $y$ depends only on a scalar content variable $\\alpha$, while the input also contains a rotation-parameterized component $[\\cos\\theta,\\sin\\theta]^\\top$. Specifically, for each sample $i$, we draw $\\alpha_i \\sim \\mathcal{N}(0,1)$ and $\\theta_i \\sim \\mathrm{Uniform}([-\\pi,\\pi])$, build\n$$\nx_i = \\begin{bmatrix} \\alpha_i \\\\ \\cos\\theta_i \\\\ \\sin\\theta_i \\end{bmatrix} + \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2 I_3),\n$$\nset $y_i = \\mathbb{I}[\\alpha_i \\ge 0]$, and define the rotation target $t_i = \\begin{bmatrix}\\cos\\theta_i \\\\ \\sin\\theta_i\\end{bmatrix}$. Because $y_i$ depends only on $\\alpha_i$, the main classification task is invariant to the angle $\\theta_i$, while the auxiliary regression task depends only on $\\theta_i$.\n\nModel and objectives. The encoder is linear: $z = f_\\phi(x) = W_e x + b_e \\in \\mathbb{R}^{d_z}$. The classifier head is $g_w(z) = \\sigma(w_c^\\top z + b_c)$, trained with Binary Cross-Entropy (BCE),\n$$\n\\ell_{\\mathrm{BCE}}(p,y) = -\\left[y\\log p + (1-y)\\log(1-p)\\right],\n$$\nwhich is the negative log-likelihood under a Bernoulli model. The auxiliary rotation head is $h_u(z) = U z + c \\in \\mathbb{R}^2$, trained with Mean Squared Error (MSE),\n$$\n\\ell_{\\mathrm{MSE}}(\\hat{t},t)=\\frac{1}{2}\\|\\hat{t}-t\\|_2^2.\n$$\nThe total loss averaged over the training set is\n$$\n\\mathcal{L}(\\phi,w,u) = \\frac{1}{N_{\\mathrm{train}}}\\sum_{i=1}^{N_{\\mathrm{train}}} \\ell_{\\mathrm{BCE}}(g_w(f_\\phi(x_i)),y_i) + \\lambda \\cdot \\frac{1}{N_{\\mathrm{train}}}\\sum_{i=1}^{N_{\\mathrm{train}}} \\ell_{\\mathrm{MSE}}(h_u(f_\\phi(x_i)),t_i),\n$$\nwith weight decay implemented as $\\ell_2$ regularization of parameters during gradient updates.\n\nWhy the auxiliary head should help. Consider the gradients with respect to the latent $z_i$. For the classifier,\n$$\n\\frac{\\partial}{\\partial z_i}\\ell_{\\mathrm{BCE}}(g_w(z_i),y_i) = (p_i - y_i) \\, w_c,\n$$\nwhere $p_i = g_w(z_i)$. Since $y_i$ depends only on $\\alpha_i$, and the angle components $[\\cos\\theta_i,\\sin\\theta_i]$ carry no information about $y_i$, ERM tends to drive $w_c$ to focus on encoder directions aligned with $\\alpha$ and to suppress encoder directions aligned with the angle (to minimize variance). Consequently, if $\\lambda = 0$, the gradient signal that would preserve angle in $z$ is minimal, because directions corresponding to angle have vanishing correlation with the classification target, and hence receive little normative pressure to be retained by $W_e$. In contrast, with $\\lambda  0$, the auxiliary MSE introduces a gradient term\n$$\n\\frac{\\partial}{\\partial z_i}\\left(\\lambda \\ell_{\\mathrm{MSE}}(h_u(z_i),t_i)\\right) = \\lambda \\, U^\\top \\left(h_u(z_i) - t_i\\right),\n$$\nwhich directly encourages $z_i$ to retain linear information sufficient to reconstruct $t_i = [\\cos\\theta_i,\\sin\\theta_i]^\\top$. If $d_z \\ge 2$, the encoder has enough degrees of freedom to linearly pass through the two-dimensional rotation subspace; if $d_z \\ge 3$, it can also preserve the scalar content $\\alpha$ simultaneously, allowing both tasks to be solved. If $d_z  2$, linear retention of both $\\cos\\theta$ and $\\sin\\theta$ is impossible, so improvement in linear decodability is limited.\n\nAlgorithmic design from principles. We implement full-batch gradient descent with learning rate $\\eta$ and weight decay $\\gamma$. Let $Z = X W_e^\\top + \\mathbf{1} b_e^\\top$ be the batch latent matrix. The classifier logits are $a = Z w_c + b_c \\mathbf{1}$, with predictions $p = \\sigma(a)$, and the auxiliary predictions are $\\hat{T} = Z U^\\top + \\mathbf{1} c^\\top$. The gradients are obtained by the chain rule:\n- For the classifier, define $e_c = p - y$, then\n$$\n\\nabla_{w_c} = \\frac{1}{N}\\, Z^\\top e_c + \\gamma w_c,\\quad \\nabla_{b_c} = \\frac{1}{N}\\, \\mathbf{1}^\\top e_c,\n$$\nand contribution to latent is $G_c = e_c w_c^\\top$, where division by $N$ is applied in the aggregation for encoder gradients.\n- For the auxiliary head, define $R = \\hat{T} - T$, then\n$$\n\\nabla_{U} = \\lambda\\left(\\frac{1}{N}\\, R^\\top Z + \\gamma U\\right),\\quad \\nabla_{c} = \\lambda\\left(\\frac{1}{N}\\, \\mathbf{1}^\\top R\\right),\n$$\nand contribution to latent is $G_a = \\lambda\\, R U$.\n- For the encoder,\n$$\n\\nabla_{W_e} = \\frac{1}{N}\\,(G_c + G_a)^\\top X + \\gamma W_e,\\quad \\nabla_{b_e} = \\frac{1}{N}\\,\\mathbf{1}^\\top (G_c + G_a).\n$$\nWe update parameters by subtracting $\\eta$ times the respective gradients at each step.\n\nEvaluation by linear probing. After training, we compute $Z_{\\mathrm{train}}$ and $Z_{\\mathrm{test}}$. We fit a ridge regression probe with bias from $Z_{\\mathrm{train}}$ to $T_{\\mathrm{train}}$ by augmenting $Z$ with a column of ones, solving the normal equations:\n$$\n\\Theta^\\star = \\arg\\min_{\\Theta}\\ \\|Z_{\\mathrm{train}}^{\\mathrm{aug}} \\Theta - T_{\\mathrm{train}}\\|_F^2 + \\rho \\|\\Theta\\|_F^2,\n$$\nwhose closed form is\n$$\n\\Theta^\\star = \\left((Z_{\\mathrm{train}}^{\\mathrm{aug}})^\\top Z_{\\mathrm{train}}^{\\mathrm{aug}} + \\rho I\\right)^{-1} (Z_{\\mathrm{train}}^{\\mathrm{aug}})^\\top T_{\\mathrm{train}}.\n$$\nWe then compute $\\hat{T}_{\\mathrm{test}} = Z_{\\mathrm{test}}^{\\mathrm{aug}} \\Theta^\\star$ and evaluate\n$$\n\\mathrm{MSE}_{\\mathrm{probe}} = \\frac{1}{2N_{\\mathrm{test}}}\\sum_{i=1}^{N_{\\mathrm{test}}}\\|\\hat{t}_i - t_i\\|_2^2.\n$$\nThis metric directly tests linear decodability of angle from $z$ via the surrogate target $[\\cos\\theta,\\sin\\theta]$.\n\nTestable hypotheses and edge cases. We consider three cases:\n- Case $1$ with $d_z = 3$ should allow the encoder to simultaneously pass through one dimension for $\\alpha$ and two for $[\\cos\\theta,\\sin\\theta]$, so the auxiliary head with $\\lambda=1$ should reduce probe MSE by at least $\\tau$ while maintaining accuracy above $a_{\\min}$.\n- Case $2$ with $d_z = 2$ must trade off content versus angle; we expect improved angle decodability but potential degradation in classification accuracy below $a_{\\min}$, thereby failing the boolean criterion.\n- Case $3$ with $d_z = 1$ lacks capacity to linearly represent both $\\cos\\theta$ and $\\sin\\theta$, so we expect little to no improvement in linear decodability.\n\nHyperparameters. We set $N_{\\mathrm{train}} = 800$, $N_{\\mathrm{test}} = 200$, $\\sigma = 0.05$, $\\eta = 0.05$, $\\gamma = 10^{-4}$, $T = 500$, $\\rho = 10^{-3}$, $\\tau = 0.2$, and $a_{\\min} = 0.9$. Angles are in radians.\n\nFinal program behavior. For each test case $(d_z,s)$ in $\\{(3,0),(2,1),(1,2)\\}$, we train two models with $\\lambda=0$ and $\\lambda=1$ respectively on the same dataset and seed, compute the probe MSEs and test accuracies, and output a boolean indicating whether the auxiliary condition both improves decodability by at least $\\tau$ and maintains accuracy at least $a_{\\min}$. The program prints a single line with the three booleans as a comma-separated list in order, formatted as in Python, with no extra text.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef generate_data(n_train, n_test, sigma, seed):\n    rng = np.random.default_rng(seed)\n    # Angles in radians\n    theta_train = rng.uniform(-np.pi, np.pi, size=(n_train, 1))\n    theta_test = rng.uniform(-np.pi, np.pi, size=(n_test, 1))\n    # Content variable\n    alpha_train = rng.normal(0.0, 1.0, size=(n_train, 1))\n    alpha_test = rng.normal(0.0, 1.0, size=(n_test, 1))\n    # Targets for auxiliary head (noise-free cos/sin)\n    T_train = np.hstack([np.cos(theta_train), np.sin(theta_train)])\n    T_test = np.hstack([np.cos(theta_test), np.sin(theta_test)])\n    # Inputs with noise\n    X_train_clean = np.hstack([alpha_train, T_train])\n    X_test_clean = np.hstack([alpha_test, T_test])\n    X_train = X_train_clean + rng.normal(0.0, sigma, size=X_train_clean.shape)\n    X_test = X_test_clean + rng.normal(0.0, sigma, size=X_test_clean.shape)\n    # Labels depend only on alpha\n    y_train = (alpha_train[:, 0] = 0.0).astype(float)\n    y_test = (alpha_test[:, 0] = 0.0).astype(float)\n    return X_train, y_train, T_train, X_test, y_test, T_test\n\ndef train_model(X, y, T, dz, lam, lr, weight_decay, epochs, seed):\n    rng = np.random.default_rng(seed)\n    n, dx = X.shape\n    # Initialize parameters\n    We = rng.normal(0.0, 0.1, size=(dz, dx))\n    be = np.zeros(dz)\n    wc = rng.normal(0.0, 0.1, size=(dz,))\n    bc = 0.0\n    U = rng.normal(0.0, 0.1, size=(2, dz))\n    c = np.zeros(2)\n\n    for _ in range(epochs):\n        # Forward\n        Z = X @ We.T + be  # (n, dz)\n        logits = Z @ wc + bc  # (n,)\n        p = sigmoid(logits)  # (n,)\n        Th = Z @ U.T + c  # (n,2)\n\n        # Errors\n        ec = (p - y)  # (n,)\n        R = (Th - T)  # (n,2)\n\n        # Gradients for classifier\n        grad_wc = (Z.T @ ec) / n + weight_decay * wc  # (dz,)\n        grad_bc = np.mean(ec)\n\n        # Contribution to Z from classifier\n        Gc = ec[:, None] * wc[None, :]  # (n,dz)\n\n        # Gradients for aux head\n        grad_U = lam * ((R.T @ Z) / n + weight_decay * U)  # (2,dz)\n        grad_c = lam * np.mean(R, axis=0)  # (2,)\n\n        # Contribution to Z from aux\n        Ga = lam * (R @ U)  # (n,dz)\n\n        # Total grad to encoder latent\n        G = Gc + Ga  # (n,dz)\n\n        # Encoder gradients\n        grad_We = (G.T @ X) / n + weight_decay * We  # (dz,dx)\n        grad_be = np.mean(G, axis=0)  # (dz,)\n\n        # Updates\n        We -= lr * grad_We\n        be -= lr * grad_be\n        wc -= lr * grad_wc\n        bc -= lr * grad_bc\n        U -= lr * grad_U\n        c -= lr * grad_c\n\n    params = {\"We\": We, \"be\": be, \"wc\": wc, \"bc\": bc, \"U\": U, \"c\": c}\n    return params\n\ndef evaluate(params, X_train, y_train, T_train, X_test, y_test, T_test, ridge_reg):\n    We = params[\"We\"]; be = params[\"be\"]; wc = params[\"wc\"]; bc = params[\"bc\"]\n    # Latents\n    Z_train = X_train @ We.T + be\n    Z_test = X_test @ We.T + be\n    # Classification accuracy\n    p_test = sigmoid(Z_test @ wc + bc)\n    y_pred = (p_test = 0.5).astype(float)\n    acc = float(np.mean(y_pred == y_test))\n    # Linear probe ridge regression Z - T\n    Ztr_aug = np.hstack([Z_train, np.ones((Z_train.shape[0], 1))])\n    Zte_aug = np.hstack([Z_test, np.ones((Z_test.shape[0], 1))])\n    # Closed-form ridge\n    A = Ztr_aug.T @ Ztr_aug\n    A += ridge_reg * np.eye(A.shape[0])\n    B = Ztr_aug.T @ T_train\n    Theta = np.linalg.solve(A, B)  # (dz+1, 2)\n    That = Zte_aug @ Theta\n    mse = float(np.mean(0.5 * np.sum((That - T_test) ** 2, axis=1)))\n    return acc, mse\n\ndef run_case(dz, seed, lam_aux, hyper):\n    # Generate data\n    Xtr, ytr, Ttr, Xte, yte, Tte = generate_data(\n        hyper[\"n_train\"], hyper[\"n_test\"], hyper[\"sigma\"], seed\n    )\n    # Baseline (lam=0)\n    params_base = train_model(\n        Xtr, ytr, Ttr, dz, 0.0, hyper[\"lr\"], hyper[\"wd\"], hyper[\"epochs\"], seed\n    )\n    acc_base, mse_base = evaluate(\n        params_base, Xtr, ytr, Ttr, Xte, yte, Tte, hyper[\"ridge\"]\n    )\n    # Auxiliary (lam=lam_aux)\n    params_aux = train_model(\n        Xtr, ytr, Ttr, dz, lam_aux, hyper[\"lr\"], hyper[\"wd\"], hyper[\"epochs\"], seed\n    )\n    acc_aux, mse_aux = evaluate(\n        params_aux, Xtr, ytr, Ttr, Xte, yte, Tte, hyper[\"ridge\"]\n    )\n    improved = (mse_base - mse_aux) = hyper[\"tau\"]\n    good_acc = acc_aux = hyper[\"acc_min\"]\n    return improved and good_acc, (acc_base, mse_base, acc_aux, mse_aux)\n\ndef solve():\n    # Hyperparameters as specified\n    hyper = {\n        \"n_train\": 800,\n        \"n_test\": 200,\n        \"sigma\": 0.05,\n        \"lr\": 0.05,\n        \"wd\": 1e-4,\n        \"epochs\": 500,\n        \"ridge\": 1e-3,\n        \"tau\": 0.2,\n        \"acc_min\": 0.9,\n    }\n    lam_aux = 1.0\n    # Test suite: (dz, seed)\n    test_cases = [\n        (3, 0),  # sufficient capacity\n        (2, 1),  # boundary capacity\n        (1, 2),  # insufficient capacity\n    ]\n    results = []\n    for dz, seed in test_cases:\n        outcome, _ = run_case(dz, seed, lam_aux, hyper)\n        results.append(outcome)\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3108515"}, {"introduction": "Many representation learning methods are built on the assumption that similar inputs should have similar representations, a concept known as the manifold hypothesis. This exercise makes this idea concrete through graph-based regularization, where we construct a neighborhood graph of data points and add a penalty that encourages connected points to have nearby representations. You will derive the elegant closed-form solution for this objective and observe how this smoothing process can improve the quality of downstream clustering [@problem_id:3108447].", "problem": "You are given a set of points $x_1,\\dots,x_N \\in \\mathbb{R}^d$ organized into $K$ ground-truth clusters. Consider learning a smoothed representation $z_1,\\dots,z_N \\in \\mathbb{R}^d$ by penalizing differences between neighboring representations on an undirected neighbor graph. Starting only from core definitions of a graph and the Euclidean norm, formulate and implement a principled method to obtain $Z \\in \\mathbb{R}^{N \\times d}$ that minimizes a smoothness-augmented objective. Then, evaluate its impact on clustering quality using cluster purity.\n\nFundamental base:\n- Let $X \\in \\mathbb{R}^{N \\times d}$ be the data matrix whose $i$-th row is $x_i^\\top$.\n- Construct an undirected, unweighted $k$-nearest neighbors (k-NN) graph with adjacency matrix $A \\in \\mathbb{R}^{N \\times N}$, where $A_{ij} = 1$ if $j$ is among the $k$ nearest neighbors of $i$ or $i$ is among the $k$ nearest neighbors of $j$, and $A_{ij} = 0$ otherwise. No self-loops are allowed, so $A_{ii} = 0$ for all $i$. Distances are measured by the Euclidean norm.\n- Define the degree matrix $D \\in \\mathbb{R}^{N \\times N}$ by $D_{ii} = \\sum_{j=1}^N A_{ij}$, and the combinatorial graph Laplacian $L \\in \\mathbb{R}^{N \\times N}$ as $L = D - A$.\n- Define the representation smoothing penalty by summing Euclidean squared differences over edges: $\\sum_{(i,j)\\in E} \\lVert z_i - z_j \\rVert^2$, where $E$ is the undirected edge set corresponding to $A$.\n\nTask:\n1. Starting from the definitions above and standard calculus of gradients, derive the first-order optimality condition for the minimizer $Z^\\star \\in \\mathbb{R}^{N \\times d}$ of the objective\n   $$J(Z) = \\sum_{i=1}^N \\lVert z_i - x_i \\rVert^2 + \\lambda \\sum_{(i,j)\\in E} \\lVert z_i - z_j \\rVert^2,$$\n   where $\\lambda \\ge 0$ is a smoothing coefficient and $Z$ stacks the $z_i$ row-wise. Use this to obtain a computable expression for $Z^\\star$ in terms of $X$, $\\lambda$, and $L$.\n2. Implement a program that:\n   - Generates a synthetic dataset $X$ of $N=120$ points in $d=2$ dimensions from $K=3$ isotropic Gaussian clusters with equal size, using centers $(-4,-4)$, $(0,5)$, and $(5,-2)$, and standard deviation $\\sigma = 0.8$. Use a fixed random seed to ensure reproducibility.\n   - Builds the undirected $k$-nearest neighbors graph for specified $k$ using the Euclidean distance, with adjacency matrix $A$ and Laplacian $L$ as defined above.\n   - Computes the smoothed representation $Z^\\star$ by solving the first-order optimality condition you derived.\n   - Runs $K$-means clustering (with $K=3$) on both the original $X$ and the smoothed $Z^\\star$ using Lloyd’s algorithm with a fixed random initialization seed and a fixed number of iterations. If any cluster becomes empty during an update, reinitialize its centroid to a random data point.\n   - Computes cluster purity for both $X$ and $Z^\\star$. Cluster purity for a partition $\\mathcal{C} = \\{C_1,\\dots,C_K\\}$ of the indices $\\{1,\\dots,N\\}$ relative to ground-truth labels $\\ell_1,\\dots,\\ell_N$ is\n     $$\\text{purity}(\\mathcal{C}, \\ell) = \\frac{1}{N} \\sum_{k=1}^K \\max_{c \\in \\{1,\\dots,K\\}} \\left|\\{ i \\in C_k : \\ell_i = c \\} \\right|,$$\n     expressed as a decimal in $[0,1]$ (not as a percentage).\n   - Reports the purity change $\\Delta = \\text{purity}(Z^\\star) - \\text{purity}(X)$ as a real number for each test case.\n\nTest suite and coverage:\n- Use the same synthetic dataset across all test cases to isolate the effect of graph parameters and smoothing.\n- Evaluate the following four parameter sets $(k, \\lambda)$ to cover a general case, a boundary case, and edge conditions:\n  1. $(k = 5, \\lambda = 0.0)$: boundary case with no smoothing.\n  2. $(k = 5, \\lambda = 0.1)$: general \"happy path\" case with mild smoothing.\n  3. $(k = 10, \\lambda = 2.0)$: strong smoothing with denser neighborhood.\n  4. $(k = 0, \\lambda = 5.0)$: edge case with no edges in the graph (no smoothing effect regardless of $\\lambda$).\n\nFinal output format:\n- Your program should produce a single line of output containing the purity changes for the four test cases as a comma-separated list enclosed in square brackets, with each value rounded to three decimal places, for example:\n  $$[0.000,0.012,0.034,0.000].$$", "solution": "The problem asks for the formulation and implementation of a graph-based representation smoothing method. The solution requires deriving the optimal representation $Z^\\star$ that minimizes a given objective function and then evaluating the effect of this smoothing on clustering performance. The validation of the problem statement precedes the solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Data**: A set of points $x_1, \\dots, x_N \\in \\mathbb{R}^d$ organized into $K$ ground-truth clusters. The data matrix is $X \\in \\mathbb{R}^{N \\times d}$.\n-   **Representation**: A smoothed representation $z_1, \\dots, z_N \\in \\mathbb{R}^d$, with matrix $Z \\in \\mathbb{R}^{N \\times d}$.\n-   **Graph**: An undirected, unweighted $k$-nearest neighbors (k-NN) graph with adjacency matrix $A \\in \\mathbb{R}^{N \\times N}$, where $A_{ij} = 1$ if node $j$ is among the $k$ nearest neighbors of $i$ or vice versa, and $A_{ij} = 0$ otherwise. No self-loops ($A_{ii} = 0$).\n-   **Graph Matrices**: Degree matrix $D_{ii} = \\sum_{j} A_{ij}$, and combinatorial graph Laplacian $L = D - A$.\n-   **Objective Function**: $J(Z) = \\sum_{i=1}^N \\lVert z_i - x_i \\rVert^2 + \\lambda \\sum_{(i,j)\\in E} \\lVert z_i - z_j \\rVert^2$, with smoothing coefficient $\\lambda \\ge 0$.\n-   **Task 1**: Derive the first-order optimality condition for the minimizer $Z^\\star$ and find a computable expression for it.\n-   **Task 2**: Implement a program with the following specifications:\n    -   **Dataset**: $N=120$, $d=2$, $K=3$ isotropic Gaussian clusters of equal size. Centers at $(-4,-4)$, $(0,5)$, $(5,-2)$. Standard deviation $\\sigma=0.8$. Fixed random seed for generation.\n    -   **Graph Construction**: Build the k-NN graph as defined for given $k$.\n    -   **Representation Computation**: Compute $Z^\\star$ using the derived expression.\n    -   **Clustering**: Run $K$-means ($K=3$) on both $X$ and $Z^\\star$ with a fixed random initialization seed and a fixed number of iterations. Empty clusters are to be reinitialized.\n    -   **Evaluation**: Compute cluster purity for both clustering results. The purity formula is given by $\\text{purity}(\\mathcal{C}, \\ell) = \\frac{1}{N} \\sum_{k=1}^K \\max_{c \\in \\{1,\\dots,K\\}} \\left|\\{ i \\in C_k : \\ell_i = c \\} \\right|$.\n    -   **Report**: The purity change $\\Delta = \\text{purity}(Z^\\star) - \\text{purity}(X)$.\n-   **Test Suite**: Evaluate for four parameter sets $(k, \\lambda)$: $(5, 0.0)$, $(5, 0.1)$, $(10, 2.0)$, $(0, 5.0)$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding**: The problem is well-grounded in the fields of graph signal processing and manifold regularization, a key concept in representation learning. The objective function is a form of Tikhonov regularization, where a smoothness prior is defined on a graph. The use of the graph Laplacian is fundamental to this area. The evaluation methodology (K-means, cluster purity) is standard practice in unsupervised machine learning.\n-   **Well-Posedness**: The objective function $J(Z)$ is a sum of squared Euclidean norms. The first term, $\\sum_{i=1}^N \\lVert z_i - x_i \\rVert^2$, is strictly convex in $Z$. The second term, involving the graph Laplacian, is also convex (as $L$ is positive semi-definite). The sum of a strictly convex function and a convex function is strictly convex. Therefore, $J(Z)$ has a unique global minimum. The problem is well-posed.\n-   **Objectivity**: The problem is stated using precise mathematical definitions and algorithmic steps. There is no ambiguity, subjectivity, or opinion-based language. All parameters are clearly specified, and the evaluation metric is defined formally.\n\n**Step 3: Verdict and Action**\nThe problem is scientifically sound, mathematically well-posed, and all its components are formally defined. It is therefore deemed **valid**. The solution process will proceed.\n\n### Derivation of the Optimal Representation $Z^\\star$\n\nThe objective is to find the representation matrix $Z^\\star$ that minimizes the function:\n$$J(Z) = \\sum_{i=1}^N \\lVert z_i - x_i \\rVert^2 + \\lambda \\sum_{(i,j)\\in E} \\lVert z_i - z_j \\rVert^2$$\nWe begin by expressing $J(Z)$ in matrix notation. The first term is the squared Frobenius norm of the difference between the matrices $Z$ and $X$:\n$$\\sum_{i=1}^N \\lVert z_i - x_i \\rVert^2 = \\lVert Z - X \\rVert_F^2 = \\text{Tr}((Z-X)^\\top(Z-X))$$\nwhere $\\text{Tr}(\\cdot)$ denotes the trace of a matrix.\n\nThe second term is the smoothing penalty. It can be expressed using the graph Laplacian $L$. The sum over undirected edges $(i,j) \\in E$ can be written as $\\frac{1}{2}$ times a sum over all ordered pairs $(i,j)$ weighted by the adjacency matrix $A$:\n$$\\sum_{(i,j)\\in E} \\lVert z_i - z_j \\rVert^2 = \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N A_{ij} \\lVert z_i - z_j \\rVert^2$$\nExpanding the squared norm and summing over each dimension $k=1,\\dots,d$ independently yields the well-known quadratic form involving the Laplacian:\n$$\\frac{1}{2} \\sum_{i,j} A_{ij} \\lVert z_i - z_j \\rVert^2 = \\text{Tr}(Z^\\top L Z)$$\nThis identity can be seen by noting that $\\text{Tr}(Z^\\top L Z) = \\sum_{k=1}^d Z_{:,k}^\\top L Z_{:,k}$, where $Z_{:,k}$ is the $k$-th column of $Z$, and $Z_{:,k}^\\top L Z_{:,k}$ is the standard Laplacian quadratic form for the $k$-th dimension of the data.\n\nCombining these terms, the objective function in matrix form is:\n$$J(Z) = \\text{Tr}((Z-X)^\\top(Z-X)) + \\lambda \\text{Tr}(Z^\\top L Z)$$\nTo find the minimum, we compute the matrix derivative of $J(Z)$ with respect to $Z$ and set it to the zero matrix. Using standard matrix calculus identities ($\\frac{\\partial}{\\partial A} \\text{Tr}(A^\\top B) = B$ and $\\frac{\\partial}{\\partial A} \\text{Tr}(A^\\top C A) = (C+C^\\top)A$), we get:\n$$\\frac{\\partial J(Z)}{\\partial Z} = \\frac{\\partial}{\\partial Z} \\text{Tr}(Z^\\top Z - Z^\\top X - X^\\top Z) + \\lambda \\frac{\\partial}{\\partial Z} \\text{Tr}(Z^\\top L Z)$$\n$$\\frac{\\partial J(Z)}{\\partial Z} = (2Z - 2X) + \\lambda (L+L^\\top)Z$$\nSince the graph is undirected, its adjacency matrix $A$ is symmetric, and thus the Laplacian $L = D-A$ is also symmetric ($L=L^\\top$). The expression simplifies to:\n$$\\frac{\\partial J(Z)}{\\partial Z} = 2(Z - X) + 2\\lambda L Z$$\nSetting the gradient to zero gives the first-order optimality condition for the minimizer $Z^\\star$:\n$$2(Z^\\star - X) + 2\\lambda L Z^\\star = 0$$\n$$Z^\\star - X + \\lambda L Z^\\star = 0$$\n$$(I + \\lambda L) Z^\\star = X$$\nwhere $I$ is the $N \\times N$ identity matrix. This is the desired first-order condition.\n\nTo obtain a computable expression for $Z^\\star$, we solve this system of linear equations for $Z^\\star$. The matrix $(I + \\lambda L)$ is invertible because the identity matrix $I$ is positive definite and the matrix $\\lambda L$ is positive semi-definite (since $\\lambda \\ge 0$ and $L$ is positive semi-definite). The sum of a positive definite and a positive semi-definite matrix is positive definite, hence invertible.\nTherefore, the unique solution $Z^\\star$ is:\n$$Z^\\star = (I + \\lambda L)^{-1} X$$\nThis expression provides a method to compute the optimal smoothed representation by solving a linear system.\n\n### Implementation Details\n\nThe implementation will follow the derived formula and the steps outlined in the problem.\n1.  A synthetic dataset $X$ is generated.\n2.  For each test case $(k, \\lambda)$:\n    a. The symmetric k-NN adjacency matrix $A$ is constructed, and from it, the Laplacian $L$.\n    b. The optimal representation $Z^\\star$ is calculated by solving the linear system $(I + \\lambda L)Z^\\star = X$. This is more numerically stable than computing the matrix inverse.\n    c. A deterministic K-means algorithm (fixed seed, fixed iterations) is run on both the original data $X$ and the smoothed data $Z^\\star$ to obtain two sets of cluster assignments.\n    d. The cluster purity is calculated for both assignments with respect to the ground-truth labels.\n    e. The change in purity, $\\Delta = \\text{purity}(Z^\\star) - \\text{purity}(X)$, is recorded.\n3.  The final output is a list of the computed $\\Delta$ values for the four test cases, formatted as specified.\nThe edge cases $\\lambda=0$ and $k=0$ are handled correctly. If $\\lambda=0$, the equation becomes $I Z^\\star = X$, so $Z^\\star=X$. If $k=0$, the graph has no edges, $A=0$, $D=0$, and $L=0$. The equation again reduces to $I Z^\\star = X$, so $Z^\\star=X$. In both scenarios, the purity change $\\Delta$ must be $0.0$.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef run_kmeans(data, K, n_iter, rng):\n    \"\"\"\n    Runs K-means clustering with fixed iterations and random re-initialization \n    for empty clusters.\n    \"\"\"\n    N, d = data.shape\n    \n    # Deterministic initialization of centroids from data points\n    initial_centroid_indices = rng.choice(N, K, replace=False)\n    centroids = data[initial_centroid_indices].copy()\n\n    for _ in range(n_iter):\n        # Assignment step\n        dist_to_centroids = cdist(data, centroids, 'euclidean')\n        labels = np.argmin(dist_to_centroids, axis=1)\n        \n        # Update step\n        new_centroids = np.zeros((K, d))\n        for i in range(K):\n            points_in_cluster = data[labels == i]\n            if len(points_in_cluster) == 0:\n                # Re-initialize empty cluster to a random data point\n                reinit_idx = rng.choice(N)\n                new_centroids[i] = data[reinit_idx]\n            else:\n                new_centroids[i] = np.mean(points_in_cluster, axis=0)\n        centroids = new_centroids\n        \n    # Final assignment\n    dist_to_centroids = cdist(data, centroids, 'euclidean')\n    final_labels = np.argmin(dist_to_centroids, axis=1)\n    \n    return final_labels\n\ndef calculate_purity(y_pred, y_true, N, K):\n    \"\"\"\n    Calculates cluster purity.\n    \"\"\"\n    contingency_matrix = np.zeros((K, K), dtype=int)\n    np.add.at(contingency_matrix, (y_pred, y_true), 1)\n    \n    purity = np.sum(np.max(contingency_matrix, axis=1)) / N\n    return purity\n\ndef solve():\n    # --- Problem constants and parameters ---\n    N = 120\n    d = 2\n    K = 3\n    sigma = 0.8\n    cluster_centers = [np.array([-4, -4]), np.array([0, 5]), np.array([5, -2])]\n    points_per_cluster = N // K\n    \n    DATA_SEED = 42\n    KMEANS_SEED = 123\n    KMEANS_ITER = 20 # A reasonable fixed number of iterations\n\n    test_cases = [\n        (5, 0.0),   # Case 1: boundary case, no smoothing\n        (5, 0.1),   # Case 2: general case\n        (10, 2.0),  # Case 3: strong smoothing, denser graph\n        (0, 5.0),   # Case 4: edge case, no edges\n    ]\n\n    # --- 1. Generate synthetic dataset ---\n    data_rng = np.random.default_rng(DATA_SEED)\n    X_parts = []\n    y_true_parts = []\n    cov = np.eye(d) * (sigma**2)\n    for i in range(K):\n        X_parts.append(data_rng.multivariate_normal(cluster_centers[i], cov, size=points_per_cluster))\n        y_true_parts.append(np.full(points_per_cluster, i))\n        \n    X = np.vstack(X_parts)\n    y_true = np.hstack(y_true_parts)\n\n    results = []\n\n    for k, lam in test_cases:\n        # --- 2. Build graph and compute smoothed representation Z* ---\n        # Handle trivial cases where Z_star = X\n        if k == 0 or lam == 0.0:\n            Z_star = X\n        else:\n            # Construct k-NN graph\n            dist_matrix = cdist(X, X, 'euclidean')\n            \n            # Find k-NN for each point (excluding self)\n            # A more robust way to exclude self is to sort and take indices [1:k+1]\n            A = np.zeros((N, N))\n            sorted_indices = np.argsort(dist_matrix, axis=1)\n            # Get the indices of the k nearest neighbors for each point\n            neighbor_indices = sorted_indices[:, 1:k+1]\n            \n            # Populate A based on one-way neighborhood\n            np.put_along_axis(A, neighbor_indices, 1, axis=1)\n\n            # Symmetrize to make the graph undirected\n            A = np.maximum(A, A.T)\n            \n            # Compute Graph Laplacian L = D - A\n            D = np.diag(np.sum(A, axis=1))\n            L = D - A\n            \n            # Solve for Z* using the derived formula: (I + lambda*L)Z* = X\n            M = np.eye(N) + lam * L\n            Z_star = np.linalg.solve(M, X)\n            \n        # --- 3. Run K-means on X and Z* ---\n        # Use the same seed for fair comparison\n        kmeans_rng_x = np.random.default_rng(KMEANS_SEED)\n        labels_X = run_kmeans(X, K, KMEANS_ITER, kmeans_rng_x)\n        \n        kmeans_rng_z = np.random.default_rng(KMEANS_SEED)\n        labels_Z = run_kmeans(Z_star, K, KMEANS_ITER, kmeans_rng_z)\n\n        # --- 4. Compute purity change ---\n        purity_X = calculate_purity(labels_X, y_true, N, K)\n        purity_Z = calculate_purity(labels_Z, y_true, N, K)\n        \n        delta = purity_Z - purity_X\n        results.append(delta)\n\n    # --- Final output ---\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```", "id": "3108447"}]}