{"hands_on_practices": [{"introduction": "Deep networks are powerful function approximators, but we can sometimes achieve similar expressivity in shallow networks by using clever input features. This practice explores positional encoding, a technique to map a simple scalar input into a high-dimensional feature vector that makes it easier for a simple model to learn high-frequency functions. In this hands-on coding exercise [@problem_id:3098829], you will implement and quantify the dramatic boost in expressivity that this architectural choice provides.", "problem": "You are asked to formalize and quantify how a fixed nonlinear feature mapping known as positional encoding can enhance the effective expressivity of a shallow feedforward architecture, relative to a baseline model operating directly on raw input. Consider the following components and definitions on the domain $[0,1]$.\n\n1. Define the positional encoding $\\gamma_K(x)$ of order $K$ as the concatenation of sine and cosine features at exponentially increasing angular frequencies:\n$$\n\\gamma_K(x) = \\Big[ \\sin(2^0 \\pi x), \\cos(2^0 \\pi x), \\sin(2^1 \\pi x), \\cos(2^1 \\pi x), \\ldots, \\sin(2^K \\pi x), \\cos(2^K \\pi x) \\Big].\n$$\n\n2. Define a shallow model class built on the positional encoding, namely the linear readout on $\\gamma_K(x)$,\n$$\n\\mathcal{F}_{\\text{PE},K} = \\left\\{ f(x) = \\sum_{k=0}^{K} \\big(a_k \\sin(2^k \\pi x) + b_k \\cos(2^k \\pi x)\\big) + c \\,:\\, a_k, b_k, c \\in \\mathbb{R} \\right\\}.\n$$\nThis is a single-layer linear model on fixed nonlinear features, which is a standard shallow architecture with a prescribed feature map in deep learning practice.\n\n3. Define a baseline shallow model operating on raw input without positional encoding,\n$$\n\\mathcal{F}_{\\text{plain}} = \\left\\{ f(x) = w_1 x + w_0 \\,:\\, w_1, w_0 \\in \\mathbb{R} \\right\\}.\n$$\n\n4. Define the target functions to be approximated on $[0,1]$:\n   - A single high-frequency target: $g_1(x) = \\sin(2^m \\pi x)$ with $m = 3$.\n   - A mixed-frequency target: $g_2(x) = 0.5 \\sin(2 \\pi x) + 0.25 \\cos(4 \\pi x) + 0.2 \\sin(8 \\pi x)$.\n   - A smooth polynomial target: $g_3(x) = x^2$.\n\n5. Let the empirical least-squares fit of a model class to a target $g(x)$ over a uniform grid $x_i = \\frac{i}{N-1}$ for $i \\in \\{0,1,\\ldots,N-1\\}$ be defined by minimizing\n$$\n\\frac{1}{N}\\sum_{i=0}^{N-1} \\big( f(x_i) - g(x_i) \\big)^2.\n$$\nIn this question, use a grid size of $N = 4096$ points, and obtain the minimizer by linear least squares over the model’s features.\n\n6. Define the empirical mean squared error (MSE) of the fit $f$ to target $g$ as\n$$\nE(f, g) = \\frac{1}{N}\\sum_{i=0}^{N-1} \\big( f(x_i) - g(x_i) \\big)^2.\n$$\n\n7. Define the expressivity boost ratio for a given target $g$ and positional encoding order $K$ as\n$$\nR(g, K) = \\frac{E\\big(f_{\\text{plain}}^\\star, g\\big)}{E\\big(f_{\\text{PE},K}^\\star, g\\big)},\n$$\nwhere $f_{\\text{plain}}^\\star \\in \\mathcal{F}_{\\text{plain}}$ and $f_{\\text{PE},K}^\\star \\in \\mathcal{F}_{\\text{PE},K}$ denote the least-squares minimizers in their respective classes. To avoid numerical division by zero in perfectly representable cases, use a floor $10^{-12}$ in the denominator, i.e., divide by $\\max\\big(E(f_{\\text{PE},K}^\\star, g), 10^{-12}\\big)$.\n\n8. Define a simple capacity proxy based on the maximum number of sign changes (zero-crossings) on $[0,1]$:\n   - For the baseline model $\\mathcal{F}_{\\text{plain}}$, the maximum number of sign changes is $S_{\\text{plain}} = 1$.\n   - For the positional encoding model $\\mathcal{F}_{\\text{PE},K}$ (a trigonometric polynomial of maximum angular frequency $2^K \\pi$), use the upper bound $S_{\\text{PE}}(K) = 2 \\cdot 2^K$.\n\nYour tasks:\n\nA) Implement the least-squares fits for $\\mathcal{F}_{\\text{plain}}$ and $\\mathcal{F}_{\\text{PE},K}$ to each target $g_1, g_2, g_3$ for $K \\in \\{0,1,3\\}$, and compute $R(g,K)$.\n\nB) Compute the sign-change capacity proxy $S_{\\text{plain}}$ and $S_{\\text{PE}}(K)$ for $K \\in \\{0,1,3\\}$.\n\nTest suite and required outputs:\n\n- Use $N = 4096$, $m = 3$, $K \\in \\{0,1,3\\}$, and the three targets $g_1, g_2, g_3$ as specified above.\n- Produce the single-line output containing the following $13$ values in this exact order:\n$$\n\\big[ S_{\\text{plain}}, S_{\\text{PE}}(0), S_{\\text{PE}}(1), S_{\\text{PE}}(3), R(g_1,0), R(g_1,1), R(g_1,3), R(g_2,0), R(g_2,1), R(g_2,3), R(g_3,0), R(g_3,1), R(g_3,3) \\big].\n$$\nAll outputs are real numbers or integers without units. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\big[ \\text{result}_1, \\text{result}_2, \\ldots \\big]$).", "solution": "This problem requires us to quantify the increase in expressivity afforded by positional encoding features compared to a simple linear model. The core of the problem lies in function approximation using linear least squares. We will solve for the optimal parameters of two different model classes, $\\mathcal{F}_{\\text{plain}}$ and $\\mathcal{F}_{\\text{PE},K}$, to best fit three distinct target functions, and then compare their resulting approximation errors.\n\nThe fundamental principle is that both model classes, $\\mathcal{F}_{\\text{plain}}$ and $\\mathcal{F}_{\\text{PE},K}$, are linear in their parameters. A function $f(x; \\mathbf{w})$ is linear in its parameters $\\mathbf{w} = (w_1, \\ldots, w_p)^T$ if it can be written as a linear combination of basis functions $\\phi_j(x)$:\n$$\nf(x; \\mathbf{w}) = \\sum_{j=1}^{p} w_j \\phi_j(x) = \\mathbf{w}^T \\mathbf{\\phi}(x)\n$$\nThe goal of linear least squares is to find the parameter vector $\\mathbf{w}^\\star$ that minimizes the Mean Squared Error (MSE) over a set of $N$ data points $(x_i, g_i)$:\n$$\nE(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( \\sum_{j=1}^{p} w_j \\phi_j(x_i) - g(x_i) \\right)^2\n$$\nIn matrix form, let $\\mathbf{g}$ be the vector of target values $g(x_i)$ and $\\mathbf{\\Phi}$ be the $N \\times p$ design matrix where $\\mathbf{\\Phi}_{ij} = \\phi_j(x_i)$. The MSE is $E(\\mathbf{w}) = \\frac{1}{N} \\| \\mathbf{\\Phi}\\mathbf{w} - \\mathbf{g} \\|_2^2$. The optimal $\\mathbf{w}^\\star$ that minimizes this error can be found by solving the linear system known as the normal equations, $\\mathbf{\\Phi}^T\\mathbf{\\Phi}\\mathbf{w} = \\mathbf{\\Phi}^T\\mathbf{g}$, or more robustly through methods like QR decomposition or Singular Value Decomposition (SVD), which are standard in numerical libraries.\n\nThe capacity proxies, $S_{\\text{plain}}$ and $S_{\\text{PE}}(K)$, are calculated directly from the provided formulae. $S_{\\text{plain}} = 1$ is fixed. For $K \\in \\{0, 1, 3\\}$, we calculate $S_{\\text{PE}}(K) = 2 \\cdot 2^K$.\n\nThe main computational task is to find the minimized errors $E(f^\\star, g)$ for each model and target.\n\n1.  **Define Grid and Targets**: We begin by creating the uniform grid of $N=4096$ points $x_i = \\frac{i}{N-1}$ on the interval $[0,1]$. Then, we evaluate the target functions $g_1(x) = \\sin(8\\pi x)$, $g_2(x) = 0.5 \\sin(2 \\pi x) + 0.25 \\cos(4 \\pi x) + 0.2 \\sin(8 \\pi x)$, and $g_3(x) = x^2$ on this grid to obtain the target vectors $\\mathbf{g}_1, \\mathbf{g}_2, \\mathbf{g}_3$.\n\n2.  **Fit Baseline Model $\\mathcal{F}_{\\text{plain}}$**: The model is $f(x) = w_1 x + w_0$. The basis functions are $\\phi_1(x) = x$ and $\\phi_2(x) = 1$. The design matrix $\\mathbf{\\Phi}_{\\text{plain}}$ is an $N \\times 2$ matrix with the first column being the $x_i$ values and the second column being all ones. We solve the least-squares problem for each target $\\mathbf{g}_j$ to find the minimized MSE, $E(f_{\\text{plain}}^\\star, g_j)$.\n\n3.  **Fit Positional Encoding Model $\\mathcal{F}_{\\text{PE},K}$**: For each order $K \\in \\{0, 1, 3\\}$, the model is a trigonometric polynomial. The basis functions are $\\{\\sin(2^k\\pi x), \\cos(2^k\\pi x)\\}_{k=0}^K$ plus a constant basis function $\\phi(x)=1$. The total number of parameters (and basis functions) is $p = 2(K+1) + 1$. For each $K$, we construct the corresponding $N \\times p$ design matrix $\\mathbf{\\Phi}_{\\text{PE},K}$ by evaluating these basis functions on the grid. For each target $\\mathbf{g}_j$, we solve the least-squares problem to find the minimized MSE, $E(f_{\\text{PE},K}^\\star, g_j)$.\n\n4.  **Compute Expressivity Boost Ratios**: With the minimal MSEs computed for both model classes, we calculate the ratio $R(g, K)$ for each target $g_j$ and order $K$:\n    $$\n    R(g_j, K) = \\frac{E(f_{\\text{plain}}^\\star, g_j)}{\\max\\big(E(f_{\\text{PE},K}^\\star, g_j), 10^{-12}\\big)}\n    $$\n    This ratio measures how many times smaller the error of the positional encoding model is compared to the baseline linear model. A large ratio indicates a significant expressivity advantage. The denominator is floored to prevent division by zero, which is likely to occur when the model class $\\mathcal{F}_{\\text{PE},K}$ can perfectly represent the target function (e.g., for $g_1, g_2$ when $K=3$).\n\nThe algorithm proceeds by first calculating the capacity proxies. Then, for each of the three target functions, it calculates the baseline error $E(f_{\\text{plain}}^\\star, g)$. Subsequently, for each $K \\in \\{0, 1, 3\\}$, it calculates the positional encoding model error $E(f_{\\text{PE},K}^\\star, g)$ and the corresponding ratio $R(g, K)$. Finally, all computed values are assembled into a single list in the specified order.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes capacity proxies and expressivity boost ratios for shallow models\n    with and without positional encoding.\n    \"\"\"\n    \n    # --- Problem Parameters ---\n    N = 4096\n    m = 3\n    K_values = [0, 1, 3]\n\n    # --- Step 1: Define Grid and Target Functions ---\n    x_grid = np.linspace(0, 1, N, dtype=np.float64)\n    \n    # g1(x) = sin(2^3 * pi * x) = sin(8 * pi * x)\n    g1 = np.sin(2**m * np.pi * x_grid)\n    \n    # g2(x) = 0.5*sin(2*pi*x) + 0.25*cos(4*pi*x) + 0.2*sin(8*pi*x)\n    g2 = (0.5 * np.sin(2 * np.pi * x_grid) + \n          0.25 * np.cos(4 * np.pi * x_grid) + \n          0.2 * np.sin(8 * np.pi * x_grid))\n          \n    # g3(x) = x^2\n    g3 = x_grid**2\n    \n    targets = {\n        'g1': g1,\n        'g2': g2,\n        'g3': g3\n    }\n    target_names = ['g1', 'g2', 'g3']\n\n    # --- Task B: Compute Capacity Proxies ---\n    S_plain = 1\n    S_PE = {K: 2 * (2**K) for K in K_values}\n\n    def get_mse(X, y):\n        \"\"\"\n        Solves the linear least-squares problem and returns the mean squared error.\n        \n        A robust method for calculating MSE is used by finding the optimal weights `w`\n        and then explicitly computing the error `mean((X@w - y)**2)`. This handles\n        cases of perfect fits where the 'residuals' output of np.linalg.lstsq\n        can be an empty array.\n        \"\"\"\n        w, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        y_pred = X @ w\n        mse = np.mean((y - y_pred)**2)\n        return mse\n\n    # --- Fit Baseline Model (F_plain) ---\n    X_plain = np.vstack([x_grid, np.ones(N)]).T\n    mse_plain_results = {}\n    for name in target_names:\n        mse_plain_results[name] = get_mse(X_plain, targets[name])\n        \n    # --- Fit Positional Encoding Model (F_PE,K) ---\n    mse_pe_results = {name: {} for name in target_names}\n    for K in K_values:\n        # Construct the design matrix X_PE for the given K\n        features = []\n        for k in range(K + 1):\n            freq = 2**k\n            features.append(np.sin(freq * np.pi * x_grid))\n            features.append(np.cos(freq * np.pi * x_grid))\n        features.append(np.ones(N)) # Bias term\n        X_pe = np.vstack(features).T\n        \n        # Calculate MSE for each target\n        for name in target_names:\n            mse_pe_results[name][K] = get_mse(X_pe, targets[name])\n            \n    # --- Task A: Compute Expressivity Boost Ratios (R) ---\n    R_results = {name: {} for name in target_names}\n    for name in target_names:\n        for K in K_values:\n            numerator = mse_plain_results[name]\n            denominator = max(mse_pe_results[name][K], 1e-12)\n            R_results[name][K] = numerator / denominator\n\n    # --- Assemble Final Output in the specified order ---\n    final_output = []\n    # Capacity Proxies\n    final_output.append(S_plain)\n    final_output.append(S_PE[0])\n    final_output.append(S_PE[1])\n    final_output.append(S_PE[3])\n    \n    # Ratios for g1, g2, g3\n    for name in target_names:\n        for K in K_values:\n            final_output.append(R_results[name][K])\n            \n    # Print the formatted output\n    print(f\"[{','.join(f'{v:.6f}' for v in final_output)}]\")\n\nsolve()\n```", "id": "3098829"}, {"introduction": "While adding features can increase expressivity, effective architectural design often involves managing complexity and parameter counts for efficiency. This exercise focuses on low-rank factorization, a method to represent large weight matrices with significantly fewer parameters. By working through this problem [@problem_id:3098849], you will calculate the parameter savings and reason about how this constraint impacts the network's representational capacity, touching upon the interplay between depth, nonlinearity, and matrix rank.", "problem": "Consider a fully connected deep feedforward network with input dimension $n_0 = 600$, one hidden layer of width $n_1 = 400$, and output dimension $n_2 = 200$. Each layer uses a bias vector. For the first layer, the weight matrix is denoted by $W_1 \\in \\mathbb{R}^{n_1 \\times n_0}$ and the bias by $b_1 \\in \\mathbb{R}^{n_1}$; for the second layer, the weight matrix is $W_2 \\in \\mathbb{R}^{n_2 \\times n_1}$ and the bias is $b_2 \\in \\mathbb{R}^{n_2}$. The activation function between layers is the Rectified Linear Unit (ReLU), defined elementwise by $\\mathrm{ReLU}(z) = \\max(0, z)$.\n\nThe network is trained under two design choices:\n- Design $A$: unfactorized weights $W_1$ and $W_2$.\n- Design $B$: each weight matrix is constrained to be low-rank via a factorization $W_i = U_i V_i^\\top$ with $U_1 \\in \\mathbb{R}^{n_1 \\times r_1}$, $V_1 \\in \\mathbb{R}^{n_0 \\times r_1}$, $U_2 \\in \\mathbb{R}^{n_2 \\times r_2}$, and $V_2 \\in \\mathbb{R}^{n_1 \\times r_2}$, where $r_1 = 50$ and $r_2 = 40$.\n\nUse only fundamental definitions of affine maps, matrix rank, and the behavior of linear compositions and pointwise nonlinearities to reason about parameter counts and representational capacity. Select all statements that are correct:\n\nA. The total number of trainable parameters in Design $A$ is $320600$, whereas in Design $B$ it is $74600$.\n\nB. Under the given dimensions and ranks, Design $B$ reduces the number of parameters by more than $70\\%$ compared to Design $A$.\n\nC. If the activation function is removed (so the network becomes purely linear), the overall linear map $x \\mapsto W_2 W_1 x + W_2 b_1 + b_2$ has matrix part $W_2 W_1$ of rank at most $\\min(r_1, r_2) = 40$.\n\nD. With the Rectified Linear Unit (ReLU) between layers, increasing depth can compensate for rank limitations by partitioning the input space into multiple regions, so that the network implements different low-rank linear maps in different regions; although the Jacobian at any single input has rank at most $\\min(r_1, r_2)$, the overall piecewise linear function can approximate mappings that a single rank-$r$ linear layer cannot.\n\nE. With the Rectified Linear Unit (ReLU) between layers, there exist inputs at which the Jacobian of the network has rank strictly greater than $\\min(r_1, r_2)$.", "solution": "We begin from first principles. A fully connected layer computes an affine map $x \\mapsto W x + b$, where $W$ is a matrix of free parameters and $b$ is a vector of free parameters. The number of parameters in such a layer equals the number of free entries in $W$ plus the number of entries in $b$. If a matrix $W \\in \\mathbb{R}^{m \\times n}$ is unconstrained, it has $m n$ free entries. If $W$ is constrained to factorize as $U V^\\top$ with $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$, then the number of free entries is $m r + n r$, and the matrix rank is at most $r$. For compositions of linear maps, the rank of a product satisfies $\\mathrm{rank}(A B) \\le \\min(\\mathrm{rank}(A), \\mathrm{rank}(B))$. For a network with a pointwise nonlinearity such as the Rectified Linear Unit (ReLU), the function becomes piecewise linear; at any input $x$, the Jacobian of the network equals the product of weight matrices and diagonal activation masks, and its rank is bounded by the minimal rank bottleneck in that product. However, different activation masks across different regions yield different local linear maps, enabling richer function classes than a single global linear map.\n\nCompute parameter counts.\n\n- Design $A$ (unfactorized):\n\nFirst layer parameters: $W_1$ has $n_1 n_0 = 400 \\cdot 600 = 240000$ entries; $b_1$ has $n_1 = 400$ entries. Second layer parameters: $W_2$ has $n_2 n_1 = 200 \\cdot 400 = 80000$ entries; $b_2$ has $n_2 = 200$ entries. Total parameters in Design $A$:\n$$240000 + 400 + 80000 + 200 = 320600.$$\n\n- Design $B$ (factorized):\n\nFirst layer parameters: $U_1$ has $n_1 r_1 = 400 \\cdot 50 = 20000$ entries; $V_1$ has $n_0 r_1 = 600 \\cdot 50 = 30000$ entries; $b_1$ has $n_1 = 400$ entries. Sum: $$20000 + 30000 + 400 = 50400.$$\nSecond layer parameters: $U_2$ has $n_2 r_2 = 200 \\cdot 40 = 8000$ entries; $V_2$ has $n_1 r_2 = 400 \\cdot 40 = 16000$ entries; $b_2$ has $n_2 = 200$ entries. Sum: $$8000 + 16000 + 200 = 24200.$$\nTotal parameters in Design $B$:\n$$50400 + 24200 = 74600.$$\n\nCompute relative reduction:\n$$\\text{reduction fraction} = 1 - \\frac{74600}{320600} \\approx 1 - 0.2326 \\approx 0.7674,$$\nwhich is a reduction of approximately $76.74\\%$, i.e., greater than $70\\%$.\n\nRank properties.\n\nIf the activation is removed, the overall map is linear: $x \\mapsto W_2 W_1 x + W_2 b_1 + b_2$. The matrix part is $W_2 W_1$. Under factorization constraints, $\\mathrm{rank}(W_1) \\le r_1$ and $\\mathrm{rank}(W_2) \\le r_2$. Hence\n$$\\mathrm{rank}(W_2 W_1) \\le \\min(\\mathrm{rank}(W_2), \\mathrm{rank}(W_1)) \\le \\min(r_2, r_1) = 40.$$\n\nWith the Rectified Linear Unit (ReLU), the network function is piecewise linear. At any particular input $x$, the activation pattern induces a diagonal matrix $D$ with diagonal entries in $\\{0,1\\}$, so the local Jacobian equals $J(x) = W_2 D W_1$. Therefore, for any fixed $x$,\n$$\\mathrm{rank}(J(x)) \\le \\min(\\mathrm{rank}(W_2), \\mathrm{rank}(D), \\mathrm{rank}(W_1)) \\le \\min(r_2, r_1) = 40.$$\nHowever, as $x$ varies, $D$ changes, yielding different local linear maps $W_2 D W_1$ across regions. This partitioning allows the network to implement a large number of distinct low-rank linear pieces glued together, thereby approximating functions that a single global low-rank linear layer cannot represent. This is a central mechanism by which depth with nonlinearities compensates for narrow or low-rank layers: it increases the number of linear regions and enables compositional structure, not the pointwise rank of the Jacobian at a given input.\n\nOption-by-option analysis:\n\nA. The computation above shows Design $A$ has $320600$ parameters and Design $B$ has $74600$ parameters. Verdict: Correct.\n\nB. The reduction is approximately $76.74\\%$, which is indeed more than $70\\%$. Verdict: Correct.\n\nC. Without nonlinearities, the rank of $W_2 W_1$ is at most $\\min(r_1, r_2) = 40$ by the rank submultiplicative property. Verdict: Correct.\n\nD. With the Rectified Linear Unit (ReLU), the network becomes piecewise linear with different activation masks $D$ across regions. Each region’s Jacobian rank is bounded by $\\min(r_1, r_2)$, yet the network can approximate mappings beyond what a single rank-$r$ linear layer can represent by stitching together many low-rank pieces. This accurately captures how depth compensates for rank limitations. Verdict: Correct.\n\nE. The Jacobian at any input $x$ equals $W_2 D W_1$, whose rank is bounded above by $\\min(r_1, r_2)$. Thus, it cannot exceed $\\min(r_1, r_2)$. Verdict: Incorrect.", "answer": "$$\\boxed{ABCD}$$", "id": "3098849"}, {"introduction": "The architecture of a network profoundly influences not just what functions it can represent, but also the geometry of the loss landscape that optimization algorithms must navigate. This advanced practice explores the concept of permutation symmetry in fully-connected layers, which creates vast regions of equivalent solutions. You will analyze how different architectural modifications [@problem_id:3098855] can explicitly break this symmetry, individualizing neurons and potentially creating a more structured landscape for optimization.", "problem": "Consider a fully connected deep feedforward network with $L$ layers, input $x \\in \\mathbb{R}^{d}$, hidden representations $h^{(l)} \\in \\mathbb{R}^{k_l}$, element-wise nonlinearity $\\phi$, and parameters $\\theta = \\{(W^{(l)}, b^{(l)})\\}_{l=1}^{L}$, where for $l \\in \\{1,\\dots,L-1\\}$,\n$$\nh^{(l)} = \\phi\\!\\left(W^{(l)} h^{(l-1)} + b^{(l)}\\right), \\quad h^{(0)} = x,\n$$\nand the output layer is $h^{(L)} = f_{\\theta}(x)$ (for simplicity, assume a pointwise output nonlinearity, if any, is also element-wise). The network is trained by minimizing the empirical risk\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\big(f_{\\theta}(x_i), y_i\\big),\n$$\nfor a dataset $\\{(x_i, y_i)\\}_{i=1}^{N}$ and a pointwise loss $\\ell(\\cdot,\\cdot)$.\n\nA permutation matrix $P \\in \\mathbb{R}^{k \\times k}$ satisfies $P P^{\\top} = I$ and permutes the standard basis of $\\mathbb{R}^{k}$. Using only the facts that $\\phi$ acts element-wise and that multiplying by a permutation matrix reorders vector components, reason about the following: in a hidden layer of width $k_l$, neuron permutations can create multiple parameter settings that compute the same function $f_{\\theta}$ on every input, producing multiple equivalent minima of $\\mathcal{L}(\\theta)$.\n\nYour task: Which of the following architectural choices or constraints can reduce the multiplicity of such permutation-induced equivalent minima by explicitly breaking the neuron permutation symmetry within layers, thereby reducing symmetry-induced flatness in the landscape of equivalent solutions? Select all that apply.\n\nA. Apply Dropout during training with the same drop probability $p \\in (0,1)$ for every hidden unit and no additional constraints at test time.\n\nB. Replace each dense hidden layer with a masked dense layer that uses a fixed binary mask $M^{(l)}$ to enforce an autoregressive ordering: the $i$-th row of $M^{(l)}$ has nonzero entries only in a prescribed index set $S_i \\subset \\{1,\\dots,k_{l-1}\\}$ with strictly nested supports $S_1 \\subset S_2 \\subset \\dots \\subset S_{k_l}$, and the next layer uses a fixed column-wise mask coupled to this ordering.\n\nC. Add Euclidean norm (Frobenius norm) weight decay to each weight matrix, i.e., add $\\lambda \\sum_{l=1}^{L} \\|W^{(l)}\\|_{F}^{2}$ with $\\lambda > 0$ to the training objective.\n\nD. Add an index-dependent unit-norm target regularizer for each hidden layer’s incoming weight vectors: choose strictly increasing targets $t_1 < t_2 < \\dots < t_{k_l}$ and add $\\lambda \\sum_{l=1}^{L-1} \\sum_{i=1}^{k_l} \\big(\\|w^{(l)}_{i:}\\|_{2} - t_i\\big)^{2}$, where $w^{(l)}_{i:}$ is the $i$-th row of $W^{(l)}$.\n\nE. Insert Batch Normalization (BN) after every hidden layer with learnable per-unit scale $\\gamma_i$ and bias $\\beta_i$ (Batch Normalization (BN) normalizes each unit’s pre-activation using batch statistics and then applies an affine transform governed by $\\gamma_i$ and $\\beta_i$).", "solution": "The problem investigates the permutation symmetry in a fully connected deep feedforward network and asks which mechanisms can break this symmetry. A symmetry exists if there are multiple distinct parameter settings $\\theta$ that result in the identical function $f_{\\theta}(x)$ for all inputs $x$ and also yield the same value for the total training objective, $\\mathcal{J}(\\theta)$. The training objective is the sum of the empirical risk and any regularization terms, $\\mathcal{J}(\\theta) = \\mathcal{L}(\\theta) + R(\\theta)$.\n\nFirst, let us formalize the permutation symmetry. Consider a hidden layer $l$ with $k_l$ neurons. The neurons in this layer are indexed from $1$ to $k_l$. A permutation of these neurons can be represented by a $k_l \\times k_l$ permutation matrix $P$. If we permute the neurons in layer $l$, we are re-arranging their order. For the network to compute the same overall function, this permutation must be counteracted by a corresponding change in the next layer, $l+1$.\n\nThe relevant transformations are:\n- The pre-activations at layer $l$ are $z^{(l)} = W^{(l)} h^{(l-1)} + b^{(l)}$.\n- The activations at layer $l$ are $h^{(l)} = \\phi(z^{(l)})$. Since $\\phi$ is element-wise, it commutes with permutation operators: $\\phi(P z^{(l)}) = P \\phi(z^{(l)})$.\n\nA permutation of neurons in layer $l$ corresponds to a new parameter set $\\tilde{\\theta}$ where:\n1. The weights and biases of layer $l$ are permuted: $\\tilde{W}^{(l)} = P W^{(l)}$ and $\\tilde{b}^{(l)} = P b^{(l)}$. This permutes the rows of the weight matrix and the elements of the bias vector. This results in permuted activations: $\\tilde{h}^{(l)} = \\phi(\\tilde{W}^{(l)}h^{(l-1)} + \\tilde{b}^{(l)}) = \\phi(P(W^{(l)}h^{(l-1)} + b^{(l)})) = P h^{(l)}$.\n2. To ensure the input to layer $l+1$ remains unchanged, the weights of layer $l+1$ must be adjusted. The original pre-activation at layer $l+1$ is $z^{(l+1)} = W^{(l+1)}h^{(l)} + b^{(l+1)}$. The new pre-activation is $\\tilde{z}^{(l+1)} = \\tilde{W}^{(l+1)}\\tilde{h}^{(l)} + \\tilde{b}^{(l+1)}$. We require $\\tilde{z}^{(l+1)} = z^{(l+1)}$. Substituting $\\tilde{h}^{(l)} = Ph^{(l)}$ and setting $\\tilde{b}^{(l+1)} = b^{(l+1)}$, we need $\\tilde{W}^{(l+1)}Ph^{(l)} = W^{(l+1)}h^{(l)}$. This holds for all $h^{(l)}$ if $\\tilde{W}^{(l+1)}P = W^{(l+1)}$, which implies $\\tilde{W}^{(l+1)} = W^{(l+1)}P^{-1} = W^{(l+1)}P^{\\top}$. This transformation permutes the columns of $W^{(l+1)}$.\n3. All other parameters in the network remain unchanged.\n\nThis new parameter set $\\tilde{\\theta}$ produces the exact same function, so $f_{\\tilde{\\theta}}(x) = f_{\\theta}(x)$ for all $x$, and thus the empirical risk is unchanged: $\\mathcal{L}(\\tilde{\\theta}) = \\mathcal{L}(\\theta)$. The symmetry is broken if the total objective $\\mathcal{J}(\\theta)$ is not invariant, which means the regularization term $R(\\theta)$ must change, i.e., $R(\\tilde{\\theta}) \\neq R(\\theta)$ for a non-identity permutation $P$. This occurs if the regularizer or architectural constraint treats neurons with different indices differently.\n\nWe now evaluate each option against this principle.\n\n**A. Apply Dropout during training with the same drop probability $p \\in (0,1)$ for every hidden unit and no additional constraints at test time.**\nDropout is a training-time regularization technique. At test time, when the function $f_{\\theta}$ is evaluated, dropout is inactive. The permutation symmetry argument applies to the final, deterministic function specified by the parameters $\\theta$. The existence of equivalent parameter sets $\\theta$ and $\\tilde{\\theta}$ is a property of the network architecture and loss landscape, independent of the stochastic training process. Furthermore, even during training, dropout is applied with the same probability $p$ to every hidden unit. It does not distinguish between neuron $i$ and neuron $j$. This means the neurons remain stochastically interchangeable. Dropout does not introduce any index-dependent constraint or penalty. Therefore, the permutation symmetry is not broken.\n**Verdict: Incorrect**\n\n**B. Replace each dense hidden layer with a masked dense layer that uses a fixed binary mask $M^{(l)}$ to enforce an autoregressive ordering...**\nThis option introduces a fixed binary mask $M^{(l)}$ such that the computation for layer $l$ is $h^{(l)} = \\phi((M^{(l)} \\odot W^{(l)}) h^{(l-1)} + b^{(l)})$. The symbol $\\odot$ denotes the element-wise product. The mask enforces an autoregressive structure with strictly nested supports, $S_1 \\subset S_2 \\subset \\dots \\subset S_{k_l}$, where $S_i$ is the set of indices of allowed incoming connections for neuron $i$. This means the mask matrix $M^{(l)}$ has rows that are not permutations of each other; for any pair of indices $i \\neq j$, the $i$-th row of $M^{(l)}$ is different from the $j$-th row.\nLet's apply the symmetry transformation for a permutation $P$. The new parameters would be $\\tilde{W}^{(l)} = P W^{(l)}$ and $\\tilde{b}^{(l)} = P b^{(l)}$. The effective weight matrix under the *fixed* mask $M^{(l)}$ would be $M^{(l)} \\odot \\tilde{W}^{(l)} = M^{(l)} \\odot (P W^{(l)})$. This is not, in general, equal to a permutation of the original effective weight matrix, $P (M^{(l)} \\odot W^{(l)})$, because the mask $M^{(l)}$ is not permuted along with the weights. Since the mask $M^{(l)}$ explicitly treats each neuron's input connectivity differently based on its index $i$, the neurons are no longer interchangeable. Any permutation of the rows of $W^{(l)}$ would change the effective connectivity graph and thus change the function computed by the network. Therefore, this architectural constraint explicitly breaks the permutation symmetry.\n**Verdict: Correct**\n\n**C. Add Euclidean norm (Frobenius norm) weight decay to each weight matrix...**\nThe regularization term is $R(\\theta) = \\lambda \\sum_{l=1}^{L} \\|W^{(l)}\\|_{F}^{2}$. The Frobenius norm is defined as $\\|A\\|_{F} = \\sqrt{\\sum_{i,j} A_{ij}^2}$.\nLet's analyze the effect of the symmetry transformation on this regularizer for a permutation in layer $l$. The affected weight matrices are $W^{(l)}$ and $W^{(l+1)}$.\nThe new weights are $\\tilde{W}^{(l)} = P W^{(l)}$ and $\\tilde{W}^{(l+1)} = W^{(l+1)} P^{\\top}$.\nThe Frobenius norm is invariant under left or right multiplication by an orthogonal (or unitary) matrix. A permutation matrix $P$ is an orthogonal matrix ($PP^{\\top}=I$).\nThus, $\\|\\tilde{W}^{(l)}\\|_{F}^{2} = \\|P W^{(l)}\\|_{F}^{2} = \\|W^{(l)}\\|_{F}^{2}$. This is because multiplying by $P$ on the left only shuffles the rows of $W^{(l)}$, which does not change the sum of the squares of its elements.\nSimilarly, $\\|\\tilde{W}^{(l+1)}\\|_{F}^{2} = \\|W^{(l+1)} P^{\\top}\\|_{F}^{2} = \\|W^{(l+1)}\\|_{F}^{2}$.\nSince all other weight matrices are unchanged, the total regularization term is invariant: $R(\\tilde{\\theta}) = R(\\theta)$. As $\\mathcal{L}(\\tilde{\\theta}) = \\mathcal{L}(\\theta)$, the total objective is also invariant: $\\mathcal{J}(\\tilde{\\theta}) = \\mathcal{J}(\\theta)$. The permutation symmetry is not broken.\n**Verdict: Incorrect**\n\n**D. Add an index-dependent unit-norm target regularizer for each hidden layer’s incoming weight vectors...**\nThe regularization term is $R(\\theta) = \\lambda \\sum_{l=1}^{L-1} \\sum_{i=1}^{k_l} \\big(\\|w^{(l)}_{i:}\\|_{2} - t_i\\big)^{2}$, where $w^{(l)}_{i:}$ is the $i$-th row of $W^{(l)}$ and the targets $t_i$ are strictly increasing, $t_1 < t_2 < \\dots < t_{k_l}$.\nThis regularizer explicitly depends on the index $i$ of each neuron through the unique target $t_i$. Let's consider a permutation $P$ that swaps two neurons, say $1$ and $2$. The transformed weight matrix is $\\tilde{W}^{(l)} = P W^{(l)}$, so its first row is the original second row ($\\tilde{w}^{(l)}_{1:} = w^{(l)}_{2:}$) and its second row is the original first row ($\\tilde{w}^{(l)}_{2:} = w^{(l)}_{1:}$).\nThe contribution to the regularizer from these two neurons in layer $l$ changes from\n$R_{1,2}(\\theta) = \\big(\\|w^{(l)}_{1:}\\|_{2} - t_1\\big)^{2} + \\big(\\|w^{(l)}_{2:}\\|_{2} - t_2\\big)^{2}$\nto\n$R_{1,2}(\\tilde{\\theta}) = \\big(\\|\\tilde{w}^{(l)}_{1:}\\|_{2} - t_1\\big)^{2} + \\big(\\|\\tilde{w}^{(l)}_{2:}\\|_{2} - t_2\\big)^{2} = \\big(\\|w^{(l)}_{2:}\\|_{2} - t_1\\big)^{2} + \\big(\\|w^{(l)}_{1:}\\|_{2} - t_2\\big)^{2}$.\nSince $t_1 \\neq t_2$, in general $R_{1,2}(\\tilde{\\theta}) \\neq R_{1,2}(\\theta)$ unless $\\|w^{(l)}_{1:}\\|_{2} = \\|w^{(l)}_{2:}\\|_{2}$. However, the regularizer actively encourages $\\|w^{(l)}_{i:}\\|_{2}$ to be close to $t_i$. At a minimum of the total objective function, we expect $\\|w^{(l)}_{i:}\\|_{2} \\approx t_i$, and thus $\\|w^{(l)}_{1:}\\|_{2} \\approx t_1 \\neq t_2 \\approx \\|w^{(l)}_{2:}\\|_{2}$. Therefore, the permuted parameter set $\\tilde{\\theta}$ will have a different (and likely higher) objective value, $\\mathcal{J}(\\tilde{\\theta}) \\neq \\mathcal{J}(\\theta)$. The parameter settings are no longer equivalent minima. The symmetry is broken.\n**Verdict: Correct**\n\n**E. Insert Batch Normalization (BN) after every hidden layer with learnable per-unit scale $\\gamma_i$ and bias $\\beta_i$.**\nBatch Normalization introduces a normalization step for each neuron's pre-activation followed by a learned affine transformation. The parameters for BN in layer $l$ are the vectors $\\gamma^{(l)}$ and $\\beta^{(l)}$ of size $k_l$. These parameters are associated with the neurons. The pre-activation for neuron $i$ is transformed as $a_i^{(l)} = \\gamma_i^{(l)} \\frac{z_i^{(l)} - \\mu_{B,i}}{\\sqrt{\\sigma^2_{B,i} + \\epsilon}} + \\beta_i^{(l)}$.\nWhen we permute the neurons in layer $l$ using a matrix $P$, the entire \"unit\" associated with each neuron is permuted. This means not only are the incoming weights ($W^{(l)}$) and biases ($b^{(l)}$) permuted, but the BN parameters are permuted as well: $\\tilde{\\gamma}^{(l)} = P \\gamma^{(l)}$ and $\\tilde{\\beta}^{(l)} = P \\beta^{(l)}$. The permutation of weights and biases leads to permuted pre-activations $\\tilde{z}^{(l)} = P z^{(l)}$. This in turn means the batch statistics are permuted. The subsequent application of the permuted BN parameters $\\tilde{\\gamma}^{(l)}$ and $\\tilde{\\beta}^{(l)}$ results in a permuted post-BN pre-activation vector $\\tilde{a}^{(l)} = P a^{(l)}$. Since the nonlinearity $\\phi$ is element-wise, the final layer activation is also permuted: $\\tilde{h}^{(l)} = P h^{(l)}$. This is the same situation as in the network without BN. The effect is cancelled out by permuting the columns of the next layer's weight matrix, $\\tilde{W}^{(l+1)} = W^{(l+1)} P^{\\top}$. The entire set of parameters $(\\theta, \\gamma, \\beta)$ has a corresponding permuted set $(\\tilde{\\theta}, \\tilde{\\gamma}, \\tilde{\\beta})$ that computes the identical function. Therefore, BN does not break the permutation symmetry.\n**Verdict: Incorrect**", "answer": "$$\\boxed{BD}$$", "id": "3098855"}]}