{"hands_on_practices": [{"introduction": "To truly grasp how automatic differentiation works under the hood, there is no substitute for performing the process by hand. This exercise guides you through the fundamental mechanics of reverse-mode AD, the engine behind modern deep learning frameworks. By constructing a computational graph and manually propagating gradients for a simple function, you will demystify the \"magic\" of backpropagation and build a concrete mental model of how gradients are calculated efficiently [@problem_id:3100431].", "problem": "Consider reverse-mode automatic differentiation (AD) in the context of deep learning, where gradients of scalar loss functions with respect to parameters are computed efficiently by traversing a computational graph backward using the chain rule. The function of interest is the scalar map $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ given by $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ with $y\\neq 0$. Using only elementary operations compatible with a computational graph (multiplication, sine, exponential, and division), construct a minimal set of intermediate variables that evaluates $f(x,y)$ and a tape that records the parent-child relationships for these operations. Then, using the principle of the chain rule for composite functions and the concept of Vector-Jacobian Product (VJP), derive the exact sequence of backward passes (VJP pulls) needed to obtain the gradient $\\nabla f(x,y)$ by hand. Your derivation should clearly identify the order of traversing the tape in reverse and the local contributions to the adjoints of the inputs at each step. Provide the final analytical expression for $\\nabla f(x,y)$ as a row vector. Do not round; the final answer must be an exact symbolic expression.", "solution": "The problem statement is found to be valid. It is scientifically grounded, well-posed, objective, and contains sufficient information to derive a unique and meaningful solution. The task concerns the application of reverse-mode automatic differentiation (AD), a cornerstone algorithm in computational calculus and deep learning, to a differentiable function. The procedure is formalizable and aligns with established principles.\n\nWe are tasked with computing the gradient $\\nabla f(x,y)$ of the function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ given by $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ for $y \\neq 0$, using the principles of reverse-mode AD. This involves a forward pass to construct a computational graph and evaluate the function, followed by a backward pass to propagate gradients.\n\nFirst, we decompose the function into a sequence of elementary operations. This sequence defines the computational graph, or \"tape\". Let the inputs be $v_1 = x$ and $v_2 = y$.\n\n**Forward Pass: Constructing the Computational Graph**\n\nThe evaluation of $f(x,y)$ can be represented by the following minimal set of intermediate variables:\n1.  $v_3 = v_1 \\cdot v_2 = x \\cdot y$\n2.  $v_4 = \\sin(v_3) = \\sin(xy)$\n3.  $v_5 = \\exp(v_1) = \\exp(x)$\n4.  $v_6 = \\frac{v_5}{v_2} = \\frac{\\exp(x)}{y}$\n5.  $v_7 = v_4 + v_6 = \\sin(xy) + \\frac{\\exp(x)}{y} = f(x,y)$\n\nThis sequence constitutes the forward pass. The tape records these operations and their dependencies: $(v_3, \\text{mul}, v_1, v_2)$, $(v_4, \\sin, v_3)$, $(v_5, \\exp, v_1)$, $(v_6, \\text{div}, v_5, v_2)$, $(v_7, \\text{add}, v_4, v_6)$.\n\n**Backward Pass: Gradient Computation using the Chain Rule**\n\nThe backward pass computes the partial derivatives of the final output $v_7$ with respect to each intermediate variable $v_i$, which are denoted as adjoints, $\\bar{v}_i = \\frac{\\partial f}{\\partial v_i} = \\frac{\\partial v_7}{\\partial v_i}$. The process begins by initializing the adjoint of the output node to $1$, i.e., $\\bar{v}_7 = \\frac{\\partial v_7}{\\partial v_7} = 1$. All other adjoints are initialized to $0$. We then traverse the graph in reverse topological order.\n\nThe core principle is the chain rule. For an operation $v_k = g(v_i, v_j, \\dots)$, the adjoints of its parents are updated by accumulating the child's adjoint multiplied by the local partial derivative:\n$$ \\bar{v}_i = \\bar{v}_i + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_i} $$\n$$ \\bar{v}_j = \\bar{v}_j + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_j} $$\n... and so on. This operation is effectively a Vector-Jacobian Product (VJP) pull.\n\nLet's compute the adjoints in reverse order of the forward pass:\n\n1.  **Start:** Initialize adjoints: $\\bar{v}_1=0, \\bar{v}_2=0, \\bar{v}_3=0, \\bar{v}_4=0, \\bar{v}_5=0, \\bar{v}_6=0$.\n    Set the seed for the backward pass: $\\bar{v}_7 = 1$.\n\n2.  **Node $v_7 = v_4 + v_6$:**\n    The parents are $v_4$ and $v_6$.\n    Local partial derivatives: $\\frac{\\partial v_7}{\\partial v_4} = 1$, $\\frac{\\partial v_7}{\\partial v_6} = 1$.\n    Update parent adjoints:\n    $\\bar{v}_4 \\leftarrow \\bar{v}_4 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_4} = 0 + 1 \\cdot 1 = 1$.\n    $\\bar{v}_6 \\leftarrow \\bar{v}_6 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_6} = 0 + 1 \\cdot 1 = 1$.\n    Current state: $\\bar{v}_4=1, \\bar{v}_6=1$.\n\n3.  **Node $v_6 = \\frac{v_5}{v_2}$:**\n    The parents are $v_5$ and $v_2$.\n    Local partial derivatives: $\\frac{\\partial v_6}{\\partial v_5} = \\frac{1}{v_2}$, $\\frac{\\partial v_6}{\\partial v_2} = -\\frac{v_5}{v_2^2}$.\n    Update parent adjoints:\n    $\\bar{v}_5 \\leftarrow \\bar{v}_5 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_5} = 0 + 1 \\cdot \\frac{1}{v_2} = \\frac{1}{y}$.\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_2} = 0 + 1 \\cdot \\left(-\\frac{v_5}{v_2^2}\\right) = -\\frac{\\exp(x)}{y^2}$.\n    Current state: $\\bar{v}_5 = \\frac{1}{y}$, $\\bar{v}_2=-\\frac{\\exp(x)}{y^2}$.\n\n4.  **Node $v_5 = \\exp(v_1)$:**\n    The parent is $v_1$.\n    Local partial derivative: $\\frac{\\partial v_5}{\\partial v_1} = \\exp(v_1)$.\n    Update parent adjoint:\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_5 \\cdot \\frac{\\partial v_5}{\\partial v_1} = 0 + \\frac{1}{y} \\cdot \\exp(v_1) = \\frac{\\exp(x)}{y}$.\n    Current state: $\\bar{v}_1 = \\frac{\\exp(x)}{y}$.\n\n5.  **Node $v_4 = \\sin(v_3)$:**\n    The parent is $v_3$.\n    Local partial derivative: $\\frac{\\partial v_4}{\\partial v_3} = \\cos(v_3)$.\n    Update parent adjoint:\n    $\\bar{v}_3 \\leftarrow \\bar{v}_3 + \\bar{v}_4 \\cdot \\frac{\\partial v_4}{\\partial v_3} = 0 + 1 \\cdot \\cos(v_3) = \\cos(xy)$.\n    Current state: $\\bar{v}_3 = \\cos(xy)$.\n\n6.  **Node $v_3 = v_1 \\cdot v_2$:**\n    The parents are $v_1$ and $v_2$. Note that $v_1$ and $v_2$ have already received gradients from other paths; we accumulate the new contributions.\n    Local partial derivatives: $\\frac{\\partial v_3}{\\partial v_1} = v_2$, $\\frac{\\partial v_3}{\\partial v_2} = v_1$.\n    Update parent adjoints:\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_1} = \\frac{\\exp(x)}{y} + \\cos(xy) \\cdot v_2 = \\frac{\\exp(x)}{y} + y \\cos(xy)$.\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_2} = -\\frac{\\exp(x)}{y^2} + \\cos(xy) \\cdot v_1 = -\\frac{\\exp(x)}{y^2} + x \\cos(xy)$.\n\nThe process terminates as we have computed the adjoints for all input nodes.\nThe final gradients are the final values of the adjoints of the input variables:\n$$ \\frac{\\partial f}{\\partial x} = \\bar{v}_1 = y \\cos(xy) + \\frac{\\exp(x)}{y} $$\n$$ \\frac{\\partial f}{\\partial y} = \\bar{v}_2 = x \\cos(xy) - \\frac{\\exp(x)}{y^2} $$\n\nThe gradient vector $\\nabla f(x,y)$ is the row vector of these partial derivatives:\n$$ \\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} y \\cos(xy) + \\frac{\\exp(x)}{y} & x \\cos(xy) - \\frac{\\exp(x)}{y^2} \\end{pmatrix} $$\nThis derivation rigorously follows the mechanical steps of reverse-mode automatic differentiation.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\ny \\cos(xy) + \\frac{\\exp(x)}{y} & x \\cos(xy) - \\frac{\\exp(x)}{y^2}\n\\end{pmatrix}\n}\n$$", "id": "3100431"}, {"introduction": "While the conceptual mechanics of AD are straightforward, implementing them in real-world software reveals practical challenges, chief among them being numerical stability. A naive application of differentiation rules can lead to overflow or loss of precision when dealing with floating-point numbers. This practice problem delves into this critical issue by having you implement a custom, numerically stable Vector-Jacobian Product (VJP) for the softplus function, a common activation function, demonstrating why carefully engineered primitives are essential for robust and reliable training of deep learning models [@problem_id:3100433].", "problem": "You are tasked with constructing a numerically stable reverse-mode Automatic Differentiation (AD) primitive for the function $y=\\mathrm{softplus}(x)$ with $y=\\log(1+e^{x})$ that supplies a custom Vector-Jacobian Product (VJP). The Vector-Jacobian Product (VJP) maps a cotangent (an upstream sensitivity) $v$ to $v$ multiplied by the gradient of $y$ with respect to $x$. Your implementation must be robust in the regime where $|x|\\gg 0$, specifically near $x=100$ and $x=-100$, and must be tested on a broader set of values.\n\nStarting from the mathematical definition of derivative and the chain rule, implement two versions of the computation:\n- A baseline \"Naive AD\" version that evaluates $y=\\log(1+e^{x})$ and its gradient using direct application of rules to this exact expression without any algebraic rearrangements for numerical stability.\n- A custom VJP version that returns $y$ and a callable that, given $v$, returns the VJP $v\\cdot \\frac{dy}{dx}$, with both the primal $y$ and the gradient $\\frac{dy}{dx}$ computed in a numerically stable manner for large positive and large negative $x$.\n\nDesign a test suite with the following input scalar values $x$: $100$, $-100$, $0$, $10000$, $-10000$. For each test case, compute:\n1. Whether the Naive AD primal and the custom VJP primal agree within an absolute tolerance of $10^{-12}$ and are both finite.\n2. Whether the Naive AD gradient and the custom VJP gradient agree within an absolute tolerance of $10^{-12}$ and are both finite.\n\nThe final output of your program must be a single line containing a comma-separated list enclosed in square brackets, consisting of booleans in the order\n$[\\text{agree\\_y}(x_1),\\text{agree\\_grad}(x_1),\\text{agree\\_y}(x_2),\\text{agree\\_grad}(x_2),\\dots]$,\nwhere $\\text{agree\\_y}(x)$ is true if and only if both primal values are finite and their absolute difference is less than or equal to $10^{-12}$, and $\\text{agree\\_grad}(x)$ is defined analogously for gradients. No physical units are involved and all angles, if any, are measured in radians, but this problem does not use angles.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\dots]$).", "solution": "The problem requires the implementation and comparison of two reverse-mode Automatic Differentiation (AD) primitives for the softplus function, $y(x) = \\log(1+e^x)$. One implementation is a \"naive\" direct translation of the formula, and the other is a \"custom VJP\" designed for numerical stability.\n\nFirst, we define the function and its derivative. The softplus function is given by:\n$$y(x) = \\log(1+e^x)$$\nUsing the chain rule, its derivative with respect to $x$ is:\n$$\\frac{dy}{dx} = \\frac{1}{1+e^x} \\cdot \\frac{d}{dx}(1+e^x) = \\frac{e^x}{1+e^x}$$\nThis derivative is the logistic sigmoid function, often denoted as $\\sigma(x)$.\n\nA reverse-mode AD primitive for a function $y=f(x)$ generally consists of two parts: a forward pass that computes the primal output value $y$, and a function that provides the Vector-Jacobian Product (VJP). The VJP maps a vector from the cotangent space of the output, $\\bar{y}$, to a vector in the cotangent space of the input, $\\bar{x}$, via the relation $\\bar{x} = J_f^T \\bar{y}$, where $J_f$ is the Jacobian matrix of $f$. For a scalar function $y=f(x)$, the Jacobian is simply the scalar derivative $\\frac{dy}{dx}$. The cotangent \"vector\" $\\bar{y}$ is a scalar, denoted as $v$ in the problem statement. Therefore, the VJP is the scalar product $v \\cdot \\frac{dy}{dx}$. A standard method to recover the gradient itself is to evaluate the VJP with $v=1$.\n\nWe will now analyze the numerical stability of the naive implementation and derive a stable alternative.\n\n**Naive AD Implementation**\nA direct implementation uses the formulas as written:\n- Primal: $y = \\log(1+e^x)$\n- Gradient: $\\frac{dy}{dx} = \\frac{e^x}{1+e^x}$\n\nLet us analyze the behavior of these expressions in floating-point arithmetic for extreme values of $x$.\n1.  **Large Positive $x$ (e.g., $x \\to \\infty$):** The term $e^x$ grows exponentially. For sufficiently large $x$ (e.g., $x > 709.78$ in standard $64$-bit floats), $e^x$ overflows, yielding an infinite value (`inf`).\n    - Naive Primal: $y = \\log(1 + \\text{inf}) = \\log(\\text{inf}) = \\text{inf}$. This is numerically incorrect. Asymptotically, for large $x$, $y(x) \\approx \\log(e^x) = x$.\n    - Naive Gradient: $\\frac{dy}{dx} = \\frac{\\text{inf}}{1+\\text{inf}} = \\frac{\\text{inf}}{\\text{inf}}$, which evaluates to `NaN` (Not a Number). This is also incorrect. Asymptotically, the gradient should approach $1$.\n\n2.  **Large Negative $x$ (e.g., $x \\to -\\infty$):** The term $e^x$ underflows to $0$.\n    - Naive Primal: $y = \\log(1+0) = 0$. This is numerically stable and correct.\n    - Naive Gradient: $\\frac{dy}{dx} = \\frac{0}{1+0} = 0$. This is also a stable and correct evaluation.\n\nThe naive implementation is clearly flawed due to overflow for large positive inputs.\n\n**Custom VJP: A Numerically Stable Implementation**\nTo create a stable implementation, we must algebraically rearrange the expressions to avoid computing $e^x$ for large positive $x$.\n\nFor the primal value $y(x)$, when $x > 0$:\n$$y(x) = \\log(1+e^x) = \\log\\left(e^x \\cdot (e^{-x} + 1)\\right) = \\log(e^x) + \\log(1+e^{-x}) = x + \\log(1+e^{-x})$$\nIn this form, for large positive $x$, we compute $e^{-x}$, which safely underflows to $0$. The expression $x + \\log(1+e^{-x})$ correctly evaluates to approximately $x$, avoiding overflow. For $x \\le 0$, the original expression $y(x) = \\log(1+e^x)$ is stable.\n\nFor the gradient $\\frac{dy}{dx}$, when $x > 0$:\n$$\\frac{dy}{dx} = \\frac{e^x}{1+e^x} = \\frac{e^x \\cdot e^{-x}}{(1+e^x) \\cdot e^{-x}} = \\frac{1}{e^{-x} + 1}$$\nThis form also avoids overflow for large positive $x$ by using $e^{-x}$, and correctly evaluates to approximately $1$. For $x \\le 0$, the original expression $\\frac{dy}{dx} = \\frac{e^x}{1+e^x}$ is stable.\n\nCombining these observations, we can define a piecewise function for stable computation:\n- **Stable Primal $y(x)$**:\n$$\ny_{stable}(x) = \n\\begin{cases} \nx + \\log(1+e^{-x}) & \\text{if } x > 0 \\\\\n\\log(1+e^x) & \\text{if } x \\le 0 \n\\end{cases}\n$$\n\n- **Stable Gradient $\\frac{dy}{dx}(x)$**:\n$$\ng_{stable}(x) = \n\\begin{cases} \n\\frac{1}{1+e^{-x}} & \\text{if } x \\ge 0 \\\\\n\\frac{e^x}{1+e^x} & \\text{if } x < 0 \n\\end{cases}\n$$\nThe split point for the gradient is chosen as $x=0$, where both expressions evaluate to $\\frac{1}{2}$, ensuring continuity. Using $x \\ge 0$ for the first case ensures the point $x=0$ is handled correctly.\n\nThe custom VJP primitive will implement these stable computations. It will compute $y_{stable}$ and $g_{stable}$, then return $y_{stable}$ and a callable function (a closure) that takes $v$ and returns the product $v \\cdot g_{stable}$.\n\nThe test suite will then compare the outputs of the naive and stable primitives for a set of input values, checking for finiteness and agreement within a specified tolerance, thereby demonstrating the failure of the naive approach and the correctness of the stable one.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares naive vs. numerically stable reverse-mode AD\n    primitives for the softplus function.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [100.0, -100.0, 0.0, 10000.0, -10000.0]\n    tolerance = 1e-12\n\n    def naive_ad_primitive(x: float):\n        \"\"\"\n        Computes the softplus primal and gradient using a direct,\n        numerically unstable implementation.\n        \"\"\"\n        # Catch numpy warnings about overflow/invalid values during naive computation\n        with np.errstate(over='ignore', invalid='ignore'):\n            # Primal: y = log(1 + exp(x))\n            primal_y = np.log(1.0 + np.exp(x))\n            \n            # Gradient: dy/dx = exp(x) / (1 + exp(x))\n            grad = np.exp(x) / (1.0 + np.exp(x))\n        \n        return primal_y, grad\n\n    def custom_vjp_primitive(x: float):\n        \"\"\"\n        Computes the softplus primal and a VJP callable using a\n        numerically stable implementation.\n        \"\"\"\n        # Stable primal computation\n        if x > 0:\n            primal_y = x + np.log(1.0 + np.exp(-x))\n        else:\n            primal_y = np.log(1.0 + np.exp(x))\n        \n        # Stable gradient computation\n        if x >= 0:\n            grad = 1.0 / (1.0 + np.exp(-x))\n        else:\n            grad = np.exp(x) / (1.0 + np.exp(x))\n\n        # The VJP is a function that takes an upstream gradient v\n        # and multiplies it by the local gradient.\n        def vjp_callable(v: float):\n            return v * grad\n            \n        return primal_y, vjp_callable\n\n    results = []\n    for x_val in test_cases:\n        # Get results from the naive implementation\n        y_naive, grad_naive = naive_ad_primitive(x_val)\n        \n        # Get results from the custom stable implementation\n        # The gradient is recovered by calling the VJP with v=1.0\n        y_custom, vjp_fn = custom_vjp_primitive(x_val)\n        grad_custom = vjp_fn(1.0)\n        \n        # 1. Compare primal values (y)\n        y_are_finite = np.isfinite(y_naive) and np.isfinite(y_custom)\n        if y_are_finite:\n            y_agree = np.abs(y_naive - y_custom) = tolerance\n        else:\n            y_agree = False\n        results.append(y_agree)\n        \n        # 2. Compare gradient values (dy/dx)\n        grad_are_finite = np.isfinite(grad_naive) and np.isfinite(grad_custom)\n        if grad_are_finite:\n            grad_agree = np.abs(grad_naive - grad_custom) = tolerance\n        else:\n            grad_agree = False\n        results.append(grad_agree)\n\n    # Final print statement in the exact required format.\n    # str(bool) gives 'True'/'False', which is the required format.\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```", "id": "3100433"}, {"introduction": "Neural networks frequently employ functions like the Rectified Linear Unit ($\\mathrm{ReLU}$) which are not differentiable at certain points. This can create confusion about whether gradient-based optimization is applicable. This final exercise challenges you to analyze a function built upon $\\mathrm{ReLU}$ to explore how AD frameworks handle such non-differentiabilities. You will discover that even when a component function has an undefined derivative, the chain rule applied by an AD system can still yield a correct and meaningful gradient for the overall function, revealing important subtleties about the training dynamics of neural networks [@problem_id:3100437].", "problem": "Consider the scalar function $f:\\mathbb{R}\\to\\mathbb{R}$ given by $f(x)=\\mathrm{ReLU}(x)^3$, where $\\mathrm{ReLU}(x)=\\max(0,x)$. Using the limit definition of the derivative and the chain rule as foundational tools, analyze the behavior of $f$ at $x=0$ and the consequences for gradient-based training. In particular, reason about subgradients at nondifferentiable points and how Automatic Differentiation (AD) frameworks might implement $\\mathrm{ReLU}'(0)$ with different choices. Assume squared-error loss $L(w)=\\frac{1}{2}(f(w)-t)^2$ for a scalar parameter $w$ and a finite target $t\\in\\mathbb{R}$, and consider an initialization $w=0$. Select all statements that are correct.\n\nA. The subdifferential of $f$ at $x=0$ is the interval $[0,1]$, so different Automatic Differentiation (AD) frameworks may legally return any value in $[0,1]$ for the gradient at $x=0$; this could noticeably change training updates at initialization.\n\nB. The function $f$ is differentiable at $x=0$ with $f'(0)=0$, so any Automatic Differentiation (AD) framework using the chain rule computes a gradient of $0$ at $x=0$, independent of its choice of $\\mathrm{ReLU}'(0)$.\n\nC. Even if an AD framework chooses $\\mathrm{ReLU}'(0)=1$, the gradient of $f$ at $x=0$ obtained via the chain rule is $0$.\n\nD. For the squared loss $L(w)=\\frac{1}{2}(f(w)-t)^2$, initializing $w=0$ yields $\\frac{dL}{dw}=0$ at $w=0$ for any finite target $t$, so gradient-based training will not move away from $w=0$ on the first update.\n\nE. Because $f$ is non-differentiable at $x=0$, AD frameworks must return not-a-number ($\\text{NaN}$) there; this typically destabilizes training.", "solution": "The problem statement is scientifically grounded, well-posed, and objective. All provided definitions and conditions are mathematically sound and sufficient for a rigorous analysis. The problem is valid.\n\nWe will analyze the function $f(x)=\\mathrm{ReLU}(x)^3$ and its role in a gradient-based optimization problem.\n\nFirst, let's analyze the differentiability of $f(x)$ at $x=0$ using the limit definition of the derivative. The function $f(x)$ can be written in a piecewise form:\n$$\nf(x) = \\left(\\max(0,x)\\right)^3 =\n\\begin{cases}\nx^3  \\text{if } x \\ge 0 \\\\\n0  \\text{if } x  0\n\\end{cases}\n$$\nThe derivative at $x=0$, if it exists, is given by the limit:\n$$ f'(0) = \\lim_{h \\to 0} \\frac{f(0+h) - f(0)}{h} $$\nWe have $f(0) = (\\max(0,0))^3 = 0^3 = 0$. The limit becomes:\n$$ f'(0) = \\lim_{h \\to 0} \\frac{f(h)}{h} $$\nTo evaluate this limit, we must check the left-hand and right-hand limits.\nThe right-hand limit:\n$$ \\lim_{h \\to 0^+} \\frac{f(h)}{h} = \\lim_{h \\to 0^+} \\frac{h^3}{h} = \\lim_{h \\to 0^+} h^2 = 0 $$\nThe left-hand limit:\n$$ \\lim_{h \\to 0^-} \\frac{f(h)}{h} = \\lim_{h \\to 0^-} \\frac{0}{h} = \\lim_{h \\to 0^-} 0 = 0 $$\nSince the left-hand and right-hand limits exist and are equal, the derivative exists and $f'(0)=0$. Thus, the function $f(x)$ is differentiable at $x=0$.\n\nNext, we analyze how an Automatic Differentiation (AD) framework would compute this derivative using the chain rule. Let $u(x) = \\mathrm{ReLU}(x)$ and $g(u) = u^3$. Then $f(x) = g(u(x))$. An AD system applies the chain rule mechanically:\n$$ \\frac{df}{dx} = \\frac{dg}{du} \\cdot \\frac{du}{dx} $$\nThe derivatives of the elementary functions are $\\frac{dg}{du} = 3u^2$ and $\\frac{du}{dx} = \\mathrm{ReLU}'(x)$. Substituting $u(x)$ back:\n$$ \\frac{df}{dx} = 3(\\mathrm{ReLU}(x))^2 \\cdot \\mathrm{ReLU}'(x) $$\nAt $x=0$, this expression becomes:\n$$ \\left.\\frac{df}{dx}\\right|_{x=0} = 3(\\mathrm{ReLU}(0))^2 \\cdot \\mathrm{ReLU}'(0) = 3(0)^2 \\cdot \\mathrm{ReLU}'(0) = 0 \\cdot \\mathrm{ReLU}'(0) $$\nThe function $\\mathrm{ReLU}(x)$ is not differentiable at $x=0$, so $\\mathrm{ReLU}'(0)$ is technically undefined. AD frameworks handle this by assigning a specific value from the subdifferential of $\\mathrm{ReLU}$ at $0$, which is $[0,1]$. Common choices for $\\mathrm{ReLU}'(0)$ are $0$ or $1$. However, regardless of which finite value is chosen for $\\mathrm{ReLU}'(0)$, the product $0 \\cdot \\mathrm{ReLU}'(0)$ is always $0$. Therefore, any AD framework implementing the chain rule will compute the derivative of $f(x)$ at $x=0$ to be $0$.\n\nNow, let's consider the loss function $L(w) = \\frac{1}{2}(f(w)-t)^2$ with initialization $w=0$. The gradient of the loss with respect to $w$ is found using the chain rule:\n$$ \\frac{dL}{dw} = (f(w)-t) \\cdot f'(w) $$\nAt the initialization $w=0$, the gradient is:\n$$ \\left.\\frac{dL}{dw}\\right|_{w=0} = (f(0)-t) \\cdot f'(0) $$\nUsing our previously calculated values $f(0)=0$ and $f'(0)=0$, we get:\n$$ \\left.\\frac{dL}{dw}\\right|_{w=0} = (0-t) \\cdot 0 = -t \\cdot 0 = 0 $$\nThis result holds for any finite target value $t$. In a standard gradient descent update step, $w_{k+1} = w_k - \\eta \\frac{dL}{dw}$, if the initial parameter is $w_0=0$ and the gradient is $0$, the update will be $w_1 = 0 - \\eta \\cdot 0 = 0$. The parameter will not change.\n\nWith these analyses, we evaluate each option:\n\nA. The subdifferential of $f$ at $x=0$ is the interval $[0,1]$, so different Automatic Differentiation (AD) frameworks may legally return any value in $[0,1]$ for the gradient at $x=0$; this could noticeably change training updates at initialization.\nThis is **Incorrect**. The subdifferential of a function at a point where it is differentiable is the singleton set containing its derivative. Since we proved $f'(0)=0$, the subdifferential of $f$ at $x=0$ is $\\partial f(0) = \\{0\\}$. The interval $[0,1]$ is the subdifferential of $\\mathrm{ReLU}(x)$ at $x=0$, not of $f(x) = \\mathrm{ReLU}(x)^3$. Consequently, the AD framework's computed gradient for $f$ at $x=0$ is unambiguously $0$.\n\nB. The function $f$ is differentiable at $x=0$ with $f'(0)=0$, so any Automatic Differentiation (AD) framework using the chain rule computes a gradient of $0$ at $x=0$, independent of its choice of $\\mathrm{ReLU}'(0)$.\nThis is **Correct**. Our analysis showed both parts of this statement to be true. The function is differentiable at $x=0$ with derivative $0$. The mechanical application of the chain rule results in the expression $3(\\mathrm{ReLU}(0))^2 \\cdot \\mathrm{ReLU}'(0) = 0$, regardless of the finite value chosen for $\\mathrm{ReLU}'(0)$.\n\nC. Even if an AD framework chooses $\\mathrm{ReLU}'(0)=1$, the gradient of $f$ at $x=0$ obtained via the chain rule is $0$.\nThis is **Correct**. This is a specific case of the general principle established in B. If $\\mathrm{ReLU}'(0)=1$, the calculation is $\\left.\\frac{df}{dx}\\right|_{x=0} = 3(\\mathrm{ReLU}(0))^2 \\cdot 1 = 3(0)^2 \\cdot 1 = 0$.\n\nD. For the squared loss $L(w)=\\frac{1}{2}(f(w)-t)^2$, initializing $w=0$ yields $\\frac{dL}{dw}=0$ at $w=0$ for any finite target $t$, so gradient-based training will not move away from $w=0$ on the first update.\nThis is **Correct**. As derived, $\\left.\\frac{dL}{dw}\\right|_{w=0} = (f(0)-t)f'(0) = (-t)(0) = 0$. A gradient of zero means a standard gradient descent step will result in no change to the parameter $w$. The training is \"stuck\" at initialization.\n\nE. Because $f$ is non-differentiable at $x=0$, AD frameworks must return not-a-number ($\\text{NaN}$) there; this typically destabilizes training.\nThis is **Incorrect**. The premise, \"$f$ is non-differentiable at $x=0$\", is false. We proved $f$ is differentiable at $x=0$. Furthermore, AD frameworks are designed to handle non-differentiable points for common functions like $\\mathrm{ReLU}$ by returning a valid subgradient (e.g., $0$ or $1$), not $\\text{NaN}$.", "answer": "$$\\boxed{BCD}$$", "id": "3100437"}]}