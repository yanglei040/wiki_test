{"hands_on_practices": [{"introduction": "The forward pass in a multilayer perceptron appears as a straightforward sequence of matrix multiplications and additions. However, practical implementations rely on features like broadcasting, where numerical libraries automatically adjust tensor shapes to make them compatible. This exercise [@problem_id:3185351] explores a common programming error—a mismatched bias vector shape—to reveal the precise mechanics of broadcasting. By tracing how tensor shapes evolve during the computation, you will develop a critical skill for debugging real-world neural network models.", "problem": "A two-layer Multilayer Perceptron (MLP) forward pass is defined as follows: given an input column vector $x \\in \\mathbb{R}^{d_{\\mathrm{in}} \\times 1}$, first-layer parameters $W^{(1)} \\in \\mathbb{R}^{d \\times d_{\\mathrm{in}}}$ and $b^{(1)} \\in \\mathbb{R}^{d \\times 1}$, and second-layer parameters $W^{(2)} \\in \\mathbb{R}^{o \\times d}$ and $b^{(2)} \\in \\mathbb{R}^{o \\times 1}$, the forward propagation is\n$$\nz^{(1)} = W^{(1)} x + b^{(1)}, \\quad a^{(1)} = \\sigma\\!\\left(z^{(1)}\\right), \\quad z^{(2)} = W^{(2)} a^{(1)} + b^{(2)},\n$$\nwhere $\\sigma(\\cdot)$ is any element-wise activation function. Suppose a practitioner accidentally stores the first-layer bias as a row vector of shape $\\left(1,d\\right)$ rather than a column vector of shape $\\left(d,1\\right)$, while using a numerical library that implements standard broadcasting for element-wise operations: comparing dimensions from the trailing end, two dimensions are compatible if they are equal or one of them is $1$, and a dimension equal to $1$ is replicated to match the other. For example, adding arrays of shapes $\\left(d,1\\right)$ and $\\left(1,d\\right)$ produces a $\\left(d,d\\right)$ array whose entries are pairwise sums of broadcasted elements.\n\nAssume matrix-matrix and matrix-vector multiplications follow standard linear algebra rules and are not broadcasted across inner dimensions. Under these conditions, what is the most accurate description of the effect of using $b^{(1)}$ with shape $\\left(1,d\\right)$ in the forward pass?\n\n- A. The computation of $z^{(1)}$ raises a shape error because $\\left(d,1\\right)$ and $\\left(1,d\\right)$ cannot be added; no broadcasting is possible in this case.\n\n- B. The computation proceeds by broadcasting, producing $z^{(1)} \\in \\mathbb{R}^{d \\times d}$ with entries $z^{(1)}_{ij} = \\left(W^{(1)}x\\right)_{i1} + b^{(1)}_{1j}$; hence $a^{(1)} \\in \\mathbb{R}^{d \\times d}$ and $z^{(2)} \\in \\mathbb{R}^{o \\times d}$, as if a pseudo-batch of size $d$ were processed with distinct per-column bias offsets.\n\n- C. The library implicitly transposes $b^{(1)}$ to shape $\\left(d,1\\right)$, so $z^{(1)} \\in \\mathbb{R}^{d \\times 1}$ is computed correctly and there is no unintended effect.\n\n- D. The library broadcasts $b^{(1)}$ by collapsing it into a scalar via summation, so $z^{(1)} \\in \\mathbb{R}^{d \\times 1}$ equals $W^{(1)}x$ plus a single scalar offset added to all hidden units.\n\n- E. The mismatch only affects $z^{(1)}$’s numerical values but not its shape; $z^{(1)} \\in \\mathbb{R}^{d \\times 1}$ still results, with each entry offset by the average of the $d$ bias components.\n\nChoose the single best answer and justify your choice by reasoning from the definitions of matrix multiplication and broadcasting.", "solution": "The user wants me to analyze the consequences of a shape mismatch in the bias vector of a neural network's first layer, based on a specific set of rules for broadcasting and matrix multiplication.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Input vector: $x \\in \\mathbb{R}^{d_{\\mathrm{in}} \\times 1}$\n- First-layer weight matrix: $W^{(1)} \\in \\mathbb{R}^{d \\times d_{\\mathrm{in}}}$\n- Erroneous first-layer bias vector: $b^{(1)}$ has shape $(1, d)$ instead of the correct $(d, 1)$.\n- Second-layer weight matrix: $W^{(2)} \\in \\mathbb{R}^{o \\times d}$\n- Second-layer bias vector: $b^{(2)} \\in \\mathbb{R}^{o \\times 1}$\n- Forward propagation equations:\n  1. $z^{(1)} = W^{(1)} x + b^{(1)}$\n  2. $a^{(1)} = \\sigma(z^{(1)})$, where $\\sigma(\\cdot)$ is an element-wise activation function.\n  3. $z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}$\n- Broadcasting rule for element-wise operations (like addition):\n  - Dimensions are compared from the trailing end.\n  - Two dimensions are compatible if they are equal, or if one of them is $1$.\n  - A dimension of size $1$ is replicated (stretched) to match the other dimension.\n  - Example provided: adding an array of shape $(d, 1)$ to an array of shape $(1, d)$ produces an array of shape $(d, d)$.\n- Matrix multiplication rule: Follows standard linear algebra rules; no broadcasting is applied.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly based on the standard architecture of a Multilayer Perceptron and the forward propagation algorithm, which are core concepts in deep learning. The described broadcasting mechanism is not arbitrary; it precisely matches the behavior of major numerical computing libraries like NumPy (used by TensorFlow) and PyTorch, making the problem highly relevant to practical machine learning engineering.\n- **Well-Posed:** The problem is well-posed. The shapes of all matrices and vectors are given, the operations are defined, and the rules for handling shape mismatches (broadcasting) are explicitly stated. These conditions are sufficient to uniquely determine the outcome of the computation.\n- **Objective:** The problem is stated in precise, objective mathematical language, free from ambiguity or subjective interpretation.\n\nThe problem statement is valid. It describes a common and realistic error scenario in numerical programming for machine learning and asks for a logical deduction of its consequences based on a clear set of computational rules.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with the derivation.\n\n### Derivation and Option Analysis\n\nLet us trace the forward propagation step-by-step, applying the given rules and shapes.\n\n1.  **Computation of $W^{(1)}x$**:\n    - The weight matrix $W^{(1)}$ has shape $(d, d_{\\mathrm{in}})$.\n    - The input vector $x$ has shape $(d_{\\mathrm{in}}, 1)$.\n    - The matrix-vector product $W^{(1)}x$ is a standard linear algebra operation. The inner dimensions ($d_{\\mathrm{in}}$ and $d_{\\mathrm{in}}$) match.\n    - The resulting matrix, let's call it $p$, will have the outer dimensions, which are $(d, 1)$.\n    - So, $p = W^{(1)}x \\in \\mathbb{R}^{d \\times 1}$.\n\n2.  **Computation of $z^{(1)} = W^{(1)}x + b^{(1)}$**:\n    - This is the addition of $p \\in \\mathbb{R}^{d \\times 1}$ and the erroneous bias vector $b^{(1)} \\in \\mathbb{R}^{1 \\times d}$.\n    - According to the problem's broadcasting rule, this is an element-wise operation where shapes are made compatible.\n    - Let's align the shapes and compare dimensions from the trailing end:\n      - Shape of $p$: $(d, 1)$\n      - Shape of $b^{(1)}$: $(1, d)$\n    - The trailing dimensions are $1$ and $d$. They are compatible, and the resulting dimension is $d$.\n    - The next dimensions are $d$ and $1$. They are also compatible, and the resulting dimension is $d$.\n    - Therefore, the shape of the output $z^{(1)}$ is $(d, d)$.\n    - The broadcasting mechanism works as follows:\n        - The matrix $p$ (shape $(d, 1)$) is replicated $d$ times along its second dimension to become a $(d, d)$ matrix. Each column of this broadcasted matrix is a copy of $p$.\n        - The matrix $b^{(1)}$ (shape $(1, d)$) is replicated $d$ times along its first dimension to become a $(d, d)$ matrix. Each row of this broadcasted matrix is a copy of $b^{(1)}$.\n    - The addition is performed element-wise on these two broadcasted $(d,d)$ matrices. The element at position $(i, j)$ of the result $z^{(1)}$ is given by the sum of the corresponding elements from the broadcasted matrices.\n    - Let $p_i$ be the $i$-th element of the column vector $p = W^{(1)}x$, and let $b_j$ be the $j$-th element of the row vector $b^{(1)}$.\n    - The element $z^{(1)}_{ij}$ is thus $p_i + b_j$. More formally, using the notation from the problem, $(W^{(1)}x)$ is a column vector and its $i$-th element is denoted $(W^{(1)}x)_{i1}$, while the $j$-th element of the row vector $b^{(1)}$ is $b^{(1)}_{1j}$.\n    - Therefore, $z^{(1)}_{ij} = (W^{(1)}x)_{i1} + b^{(1)}_{1j}$. The resulting matrix is $z^{(1)} \\in \\mathbb{R}^{d \\times d}$.\n\n3.  **Computation of $a^{(1)} = \\sigma(z^{(1)})$**:\n    - The activation function $\\sigma(\\cdot)$ is applied element-wise.\n    - This operation does not change the shape of its input matrix.\n    - Since $z^{(1)} \\in \\mathbb{R}^{d \\times d}$, the output $a^{(1)}$ will also be in $\\mathbb{R}^{d \\times d}$.\n\n4.  **Computation of $z^{(2)} = W^{(2)}a^{(1)} + b^{(2)}$**:\n    - First, we compute the matrix product $W^{(2)}a^{(1)}$.\n    - $W^{(2)}$ has shape $(o, d)$.\n    - $a^{(1)}$ has shape $(d, d)$.\n    - The multiplication is valid as the inner dimensions ($d$ and $d$) match.\n    - The result of this multiplication is a matrix with shape $(o, d)$.\n    - Next, we add the second-layer bias $b^{(2)} \\in \\mathbb{R}^{o \\times 1}$ to the $(o, d)$ matrix from the previous step.\n    - Again, broadcasting is applied:\n      - Shape 1: $(o, d)$\n      - Shape 2: $(o, 1)$\n    - The trailing dimensions are $d$ and $1$. They are compatible, resulting in dimension $d$.\n    - The next dimensions are $o$ and $o$. They are equal and compatible, resulting in dimension $o$.\n    - The final shape for $z^{(2)}$ is thus $(o, d)$.\n    - The computation proceeds without raising an error, but produces an output $z^{(2)}$ of shape $(o, d)$ instead of the correct $(o, 1)$.\n\n### Option-by-Option Analysis\n\n- **A. The computation of $z^{(1)}$ raises a shape error because $\\left(d,1\\right)$ and $\\left(1,d\\right)$ cannot be added; no broadcasting is possible in this case.**\n  - **Verdict: Incorrect.** The problem explicitly states that a numerical library implementing broadcasting is used. The example given in the prompt, \"adding arrays of shapes $\\left(d,1\\right)$ and $\\left(1,d\\right)$ produces a $\\left(d,d\\right)$ array,\" directly contradicts this option. The computation does not fail; it produces a result of an unintended shape.\n\n- **B. The computation proceeds by broadcasting, producing $z^{(1)} \\in \\mathbb{R}^{d \\times d}$ with entries $z^{(1)}_{ij} = \\left(W^{(1)}x\\right)_{i1} + b^{(1)}_{1j}$; hence $a^{(1)} \\in \\mathbb{R}^{d \\times d}$ and $z^{(2)} \\in \\mathbb{R}^{o \\times d}$, as if a pseudo-batch of size $d$ were processed with distinct per-column bias offsets.**\n  - **Verdict: Correct.** This option accurately describes every step of the aformentioned derivation.\n    - It correctly states that the computation proceeds via broadcasting.\n    - It correctly identifies the resulting shape of $z^{(1)}$ as $(d, d)$, or $\\mathbb{R}^{d \\times d}$.\n    - It correctly provides the formula for the entries of $z^{(1)}$, which results from broadcasting a column vector and a row vector.\n    - It correctly deduces that the shape of $a^{(1)}$ will also be $(d,d)$.\n    - It correctly deduces the final shape of $z^{(2)}$ to be $(o, d)$ after the multiplication with $W^{(2)}$ and the broadcasted addition of $b^{(2)}$.\n    - The final analogy is a reasonable interpretation of the result: a single input $x$ is effectively transformed into $d$ different activation vectors (the columns of $a^{(1)}$), which are then processed by the second layer, mimicking a batch process where each \"item\" in the pseudo-batch received a different bias modification.\n\n- **C. The library implicitly transposes $b^{(1)}$ to shape $\\left(d,1\\right)$, so $z^{(1)} \\in \\mathbb{R}^{d \\times 1}$ is computed correctly and there is no unintended effect.**\n  - **Verdict: Incorrect.** This describes a hypothetical \"smart\" behavior that is not consistent with the explicitly stated broadcasting rules. The rules are based on replicating dimensions of size $1$, not on transposing arrays to force dimension agreement. Following the given rules leads to a $(d, d)$ matrix, not a corrected $(d, 1)$ vector.\n\n- **D. The library broadcasts $b^{(1)}$ by collapsing it into a scalar via summation, so $z^{(1)} \\in \\mathbb{R}^{d \\times 1}$ equals $W^{(1)}x$ plus a single scalar offset added to all hidden units.**\n  - **Verdict: Incorrect.** This proposes an entirely different mechanism for broadcasting involving aggregation (summation) and collapsing dimensions. The broadcasting rule described in the problem is purely based on replication to expand dimensions. No summation or collapsing occurs. Furthermore, this would result in a shape of $(d, 1)$, which our analysis shows to be false.\n\n- **E. The mismatch only affects $z^{(1)}$’s numerical values but not its shape; $z^{(1)} \\in \\mathbb{R}^{d \\times 1}$ still results, with each entry offset by the average of the $d$ bias components.**\n  - **Verdict: Incorrect.** This option makes two false claims. First, it states that the shape of $z^{(1)}$ is unchanged, which is the primary error that occurs—the shape changes from the intended $(d, 1)$ to $(d, d)$. Second, it suggests an averaging mechanism, which is not part of the standard broadcasting rule defined in the problem.", "answer": "$$\\boxed{B}$$", "id": "3185351"}, {"introduction": "Moving beyond the arithmetic of forward propagation, we can ask what a layer of neurons computes from a geometric perspective. A neuron with a ReLU activation function defines a hyperplane that divides the input space, activating only for inputs on one side. This practice [@problem_id:3185431] delves into this geometric interpretation of a neural layer. By constructing specific inputs and weights to control which neurons fire, you will gain a powerful intuition for how networks learn to approximate complex functions by partitioning the input space into many simple regions.", "problem": "Consider a single-layer feed-forward network with Rectified Linear Unit (ReLU) activation, where the forward pass is defined by $a = \\mathrm{ReLU}(W x + b)$ applied elementwise. The Rectified Linear Unit (ReLU) function is defined by $\\mathrm{ReLU}(z) = \\max(0, z)$. Let the input dimension be $n = 2$ and the number of units be $m = 4$. Throughout, denote the rows of $W$ by $w_1, w_2, w_3, w_4 \\in \\mathbb{R}^2$.\n\nYou will analyze how to force exactly $k = 2$ nonzero activations with carefully chosen $x$, $W$, and $b$, and then reason from first principles about how the induced partition of $\\mathbb{R}^2$ into regions depends on $W$ and $b$.\n\nSelect all options that are correct.\n\nA. Let $W$ have rows $w_1 = (1, 0)$, $w_2 = (-1, 0)$, $w_3 = (0, 1)$, $w_4 = (0, -1)$ and let $b = \\mathbf{0} \\in \\mathbb{R}^4$. For $x = (1, 1)$, the forward pass yields exactly $k = 2$ nonzero activations. In this configuration, the induced region boundaries are the hyperplanes $x_1 = 0$ and $x_2 = 0$.\n\nB. With the same $W$ and $b = \\mathbf{0}$ as in option A, choosing $x = (0, 0)$ yields exactly $k = 2$ nonzero activations.\n\nC. With the same $W$ as in option A, choose $b = (0.5, 0.5, -0.5, -0.5)$ and $x = (0, 0)$. Then the forward pass yields exactly $k = 2$ nonzero activations.\n\nD. For $b = \\mathbf{0}$, scaling the entire matrix $W$ by any positive scalar $\\alpha  0$ does not change the partition of $\\mathbb{R}^2$ into regions of constant activation patterns (i.e., the support of $a$ across $x \\in \\mathbb{R}^2$ is unchanged).\n\nE. For $b = \\mathbf{0}$, replacing any single row $w_i$ by $-w_i$ leaves the geometric hyperplane boundary $\\{x \\in \\mathbb{R}^2 : w_i^{\\top} x = 0\\}$ unchanged but flips which side of that boundary is active for unit $i$, thereby changing the activation pattern assigned to each region.\n\nF. For any choice of $W \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$, as $x$ varies over $\\mathbb{R}^n$ the number of distinct activation patterns realized is exactly $2^m$.", "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n- The neural network is a single-layer feed-forward network.\n- The activation function is the Rectified Linear Unit (ReLU), applied elementwise: $\\mathrm{ReLU}(z) = \\max(0, z)$.\n- The forward pass is defined by the equation $a = \\mathrm{ReLU}(W x + b)$.\n- The input vector dimension is $n = 2$, so $x \\in \\mathbb{R}^2$.\n- The number of units (output dimension) is $m = 4$, so $a, b \\in \\mathbb{R}^4$ and $W \\in \\mathbb{R}^{4 \\times 2}$.\n- The rows of the weight matrix $W$ are denoted by $w_1, w_2, w_3, w_4 \\in \\mathbb{R}^2$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard single-neuron-layer network, a fundamental building block in deep learning. The components—input vector, weight matrix, bias vector, ReLU activation, and the forward propagation equation—are all standard and well-defined in the field. The questions posed in the options are specific, testable mathematical claims about the behavior of this system under different parameterizations.\n\n- **Scientifically Grounded:** The problem is based on the established mathematical framework of artificial neural networks. All definitions and concepts are standard.\n- **Well-Posed:** The problem provides all necessary information to evaluate the claims made in the options. Each option presents a clear, falsifiable hypothesis.\n- **Objective:** The problem is stated in precise mathematical language, free from ambiguity or subjective content.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. A full solution and evaluation of options will be performed.\n\nThe core of the analysis involves the pre-activation vector $z = Wx + b$, whose components are $z_i = w_i^\\top x + b_i$ for $i \\in \\{1, 2, 3, 4\\}$. The $i$-th activation is $a_i = \\mathrm{ReLU}(z_i)$. An activation $a_i$ is non-zero if and only if its corresponding pre-activation $z_i$ is positive, i.e., $a_i  0 \\iff w_i^\\top x + b_i  0$. The boundary between the active and inactive regions for the $i$-th unit is the hyperplane (a line in $\\mathbb{R}^2$) defined by the equation $w_i^\\top x + b_i = 0$.\n\n### Option-by-Option Analysis\n\n**A. Let $W$ have rows $w_1 = (1, 0)$, $w_2 = (-1, 0)$, $w_3 = (0, 1)$, $w_4 = (0, -1)$ and let $b = \\mathbf{0} \\in \\mathbb{R}^4$. For $x = (1, 1)$, the forward pass yields exactly $k = 2$ nonzero activations. In this configuration, the induced region boundaries are the hyperplanes $x_1 = 0$ and $x_2 = 0$.**\n\nFirst, we compute the pre-activation vector $z = Wx + b$ with the given values.\n$W = \\begin{pmatrix} 1  0 \\\\ -1  0 \\\\ 0  1 \\\\ 0  -1 \\end{pmatrix}$, $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, $b = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\n$z = Wx = \\begin{pmatrix} 1  0 \\\\ -1  0 \\\\ 0  1 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 1 \\\\ -1 \\cdot 1 + 0 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 1 \\\\ 0 \\cdot 1 + (-1) \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{pmatrix}$.\n\nNext, we apply the ReLU function to get the activation vector $a$:\n$a = \\mathrm{ReLU}(z) = \\mathrm{ReLU}\\left(\\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{pmatrix}\\right) = \\begin{pmatrix} \\max(0, 1) \\\\ \\max(0, -1) \\\\ \\max(0, 1) \\\\ \\max(0, -1) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nThe number of nonzero activations is $k=2$ ($a_1$ and $a_3$). This part of the statement is correct.\n\nNow, we analyze the region boundaries. The boundaries are defined by $w_i^\\top x + b_i = 0$. Since $b = \\mathbf{0}$, this simplifies to $w_i^\\top x = 0$.\nLet $x = (x_1, x_2)^\\top$.\n- For $i=1$: $w_1^\\top x = (1, 0) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x_1 = 0$.\n- For $i=2$: $w_2^\\top x = (-1, 0) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = -x_1 = 0 \\implies x_1 = 0$.\n- For $i=3$: $w_3^\\top x = (0, 1) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x_2 = 0$.\n- For $i=4$: $w_4^\\top x = (0, -1) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = -x_2 = 0 \\implies x_2 = 0$.\nThe set of distinct boundary hyperplanes is $\\{x_1 = 0\\}$ and $\\{x_2 = 0\\}$. These are the coordinate axes. This part of the statement is also correct.\n\nVerdict: **Correct**.\n\n**B. With the same $W$ and $b = \\mathbf{0}$ as in option A, choosing $x = (0, 0)$ yields exactly $k = 2$ nonzero activations.**\n\nUsing $W$ from option A, $b = \\mathbf{0}$, and $x = (0, 0)^\\top$.\n$z = Wx + b = W\\mathbf{0} + \\mathbf{0} = \\mathbf{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n$a = \\mathrm{ReLU}(z) = \\mathrm{ReLU}(\\mathbf{0}) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe number of nonzero activations is $k=0$. The statement claims $k=2$.\n\nVerdict: **Incorrect**.\n\n**C. With the same $W$ as in option A, choose $b = (0.5, 0.5, -0.5, -0.5)$ and $x = (0, 0)$. Then the forward pass yields exactly $k = 2$ nonzero activations.**\n\nUsing $W$ from option A, $x = (0, 0)^\\top$, and $b = (0.5, 0.5, -0.5, -0.5)^\\top$.\n$z = Wx + b = W\\mathbf{0} + b = b = \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ -0.5 \\\\ -0.5 \\end{pmatrix}$.\n$a = \\mathrm{ReLU}(z) = \\mathrm{ReLU}\\left(\\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ -0.5 \\\\ -0.5 \\end{pmatrix}\\right) = \\begin{pmatrix} \\max(0, 0.5) \\\\ \\max(0, 0.5) \\\\ \\max(0, -0.5) \\\\ \\max(0, -0.5) \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe number of nonzero activations is $k=2$ ($a_1$ and $a_2$).\n\nVerdict: **Correct**.\n\n**D. For $b = \\mathbf{0}$, scaling the entire matrix $W$ by any positive scalar $\\alpha  0$ does not change the partition of $\\mathbb{R}^2$ into regions of constant activation patterns (i.e., the support of $a$ across $x \\in \\mathbb{R}^2$ is unchanged).**\n\nThe activation pattern is determined by the signs of the pre-activations $z_i$. When $b = \\mathbf{0}$, we have $z_i = w_i^\\top x$. The $i$-th unit is active if $w_i^\\top x  0$.\nLet the scaled weight matrix be $W' = \\alpha W$, where $\\alpha  0$. The new weight vectors are $w'_i = \\alpha w_i$.\nThe new pre-activations are $z'_i = (w'_i)^\\top x = (\\alpha w_i)^\\top x = \\alpha(w_i^\\top x)$.\nSince $\\alpha  0$, the sign of $z'_i$ is the same as the sign of $z_i = w_i^\\top x$.\n- $z'_i  0 \\iff \\alpha(w_i^\\top x)  0 \\iff w_i^\\top x  0$.\n- $z'_i \\le 0 \\iff \\alpha(w_i^\\top x) \\le 0 \\iff w_i^\\top x \\le 0$.\nFor any given input $x$, the set of active units (the activation pattern) remains identical. The boundary hyperplanes are defined by $w_i^\\top x = 0$. For the scaled system, they are defined by $(\\alpha w_i)^\\top x = 0$, which is equivalent to $w_i^\\top x = 0$. The geometric locations of the boundaries are unchanged. Thus, the partition of the input space into regions of constant activation patterns is unchanged.\n\nVerdict: **Correct**.\n\n**E. For $b = \\mathbf{0}$, replacing any single row $w_i$ by $-w_i$ leaves the geometric hyperplane boundary $\\{x \\in \\mathbb{R}^2 : w_i^{\\top} x = 0\\}$ unchanged but flips which side of that boundary is active for unit $i$, thereby changing the activation pattern assigned to each region.**\n\nLet $w'_i = -w_i$.\nThe original boundary for unit $i$ is the set of points $x$ such that $w_i^\\top x = 0$.\nThe new boundary is the set of points $x$ such that $(w'_i)^\\top x = 0$. This is $(-w_i)^\\top x = - (w_i^\\top x) = 0$, which is equivalent to $w_i^\\top x = 0$. The geometric boundary is indeed unchanged.\n\nThe original active region for unit $i$ is where $w_i^\\top x  0$.\nThe new active region for unit $i$ is where $(w'_i)^\\top x  0$, which means $(-w_i)^\\top x  0$, or $-(w_i^\\top x)  0$. This simplifies to $w_i^\\top x  0$.\nThe region of activation for unit $i$ has been flipped from one side of the hyperplane to the other. Consequently, for any point $x$ not on the boundary itself, the activation status of the $i$-th unit will be flipped compared to the original configuration. This changes the activation pattern (the vector of $0$s and $1$s indicating which units are active) for every region in the partition.\n\nVerdict: **Correct**.\n\n**F. For any choice of $W \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$, as $x$ varies over $\\mathbb{R}^n$ the number of distinct activation patterns realized is exactly $2^m$.**\n\nAn activation pattern is a binary vector of length $m$, so there are $2^m$ possible patterns in total. The question is whether all of them can be realized for *any* choice of $W$ and $b$.\nThe hyperplanes $w_i^\\top x + b_i = 0$ for $i=1, \\dots, m$ partition the input space $\\mathbb{R}^n$ into a number of regions. Within each open, connected region, the signs of all $w_i^\\top x + b_i$ are constant, so the activation pattern is constant. The number of distinct activation patterns is therefore bounded by the number of regions created by these hyperplanes.\nThe maximum number of regions that $m$ hyperplanes can partition $\\mathbb{R}^n$ into is given by Zaslavsky's formula, which for hyperplanes in general position is $\\sum_{j=0}^{n} \\binom{m}{j}$.\nIn our case, $n=2$ and $m=4$. The maximum number of regions is:\n$$ \\sum_{j=0}^{2} \\binom{4}{j} = \\binom{4}{0} + \\binom{4}{1} + \\binom{4}{2} = 1 + 4 + \\frac{4 \\cdot 3}{2} = 1 + 4 + 6 = 11 $$\nThe total number of possible activation patterns is $2^m = 2^4 = 16$.\nSince the number of regions ($11$) is less than the number of possible patterns ($16$), it is impossible to realize all $2^m$ patterns. The statement is universal (\"For any choice...\"), so a single counterexample is sufficient to disprove it. The geometric argument above shows that no choice of $W, b$ can achieve this for $n=2, m=4$ (even without considering degenerate cases like parallel or coincident hyperplanes which would reduce the number of regions further). For example, if we choose all $w_i$ to be identical, say $w_i = (1,0)$ for all $i$, and $b=\\mathbf{0}$, we get only one distinct hyperplane $x_1=0$. This creates two regions ($x_10$ and $x_10$), realizing only two patterns: all units on or all units off.\n\nVerdict: **Incorrect**.", "answer": "$$\\boxed{ACDE}$$", "id": "3185431"}, {"introduction": "The principles of forward propagation extend far beyond simple dense layers, forming the backbone of modern architectures like the Transformer. At the heart of the Transformer is the self-attention mechanism, a powerful module built from the same fundamental operations. This exercise [@problem_id:3185352] provides a hands-on walkthrough of the Scaled Dot-Product Attention formula, the core of this mechanism. By meticulously calculating the output for a small, synthetic example, you will demystify how attention works and appreciate how simple matrix operations and normalization can create sophisticated, context-aware representations.", "problem": "Consider the forward propagation through a single-head self-attention layer, which is based on dot products between query and key vectors, scaling by the square root of the key dimensionality, applying the softmax function row-wise to obtain attention weights, and then a matrix multiplication with the value vectors to produce the output. Use the following small synthetic example with three tokens and key dimensionality $d_k=2$. The query matrix $Q$, key matrix $K$, and value matrix $V$ are all equal and given by\n$$\nQ=K=V=\\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n1  0\n\\end{pmatrix}.\n$$\nYou must proceed only from fundamental definitions: the dot product of vectors, the square-root scaling by $\\sqrt{d_k}$, row-wise application of the softmax function defined by $\\mathrm{softmax}(x)_i=\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$, and standard matrix multiplication. Without invoking any shortcut formulas, compute the forward pass of Scaled Dot-Product Attention (SDPA) for this example. By analyzing the structure of $Q$, $K$, and $V$, identify which rows of the output are identical due to symmetry. Then, compute the common value of the first component of those identical output rows. Express your final numerical result rounded to four significant figures and report it as a dimensionless quantity.", "solution": "The problem asks for the computation of the forward pass of a single-head Scaled Dot-Product Attention (SDPA) layer for a given set of input matrices $Q$, $K$, and $V$. The final output should be a specific numerical value derived from the resulting attention output matrix. The process is governed by the formula:\n$$Z = \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\nwhere $Z$ is the output matrix. We are instructed to proceed from fundamental definitions, breaking down the computation into sequential steps.\n\nThe given inputs are:\nThe query, key, and value matrices are identical:\n$$Q=K=V=\\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  0 \\end{pmatrix}$$\nThe key dimensionality is given as $d_k=2$.\n\nStep 1: Compute the matrix of dot product scores, $QK^T$.\nFirst, we find the transpose of the key matrix $K$:\n$$K^T = \\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\end{pmatrix}$$\nNext, we perform the matrix multiplication $QK^T$:\n$$QK^T = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\end{pmatrix}$$\nThis product yields a $3 \\times 3$ matrix of attention scores. The elements are calculated as follows:\n$$(QK^T)_{11} = (1)(1) + (0)(0) = 1$$\n$$(QK^T)_{12} = (1)(0) + (0)(1) = 0$$\n$$(QK^T)_{13} = (1)(1) + (0)(0) = 1$$\n$$(QK^T)_{21} = (0)(1) + (1)(0) = 0$$\n$$(QK^T)_{22} = (0)(0) + (1)(1) = 1$$\n$$(QK^T)_{23} = (0)(1) + (1)(0) = 0$$\n$$(QK^T)_{31} = (1)(1) + (0)(0) = 1$$\n$$(QK^T)_{32} = (1)(0) + (0)(1) = 0$$\n$$(QK^T)_{33} = (1)(1) + (0)(0) = 1$$\nThe resulting scores matrix is:\n$$S_{raw} = QK^T = \\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 1  0  1 \\end{pmatrix}$$\n\nStep 2: Scale the scores matrix by $1/\\sqrt{d_k}$.\nWith $d_k=2$, the scaling factor is $1/\\sqrt{2}$. The scaled scores matrix, which we denote as $S$, is:\n$$S = \\frac{QK^T}{\\sqrt{d_k}} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 1  0  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{2}} \\\\ 0  \\frac{1}{\\sqrt{2}}  0 \\\\ \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{2}} \\end{pmatrix}$$\n\nStep 3: Compute the attention weight matrix $A_w$ by applying the softmax function row-wise to $S$.\nThe softmax function for a vector $x$ is defined as $\\mathrm{softmax}(x)_i=\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$.\n\nFor the first row of $S$, $s_1 = (\\frac{1}{\\sqrt{2}}, 0, \\frac{1}{\\sqrt{2}})$:\nThe denominator for the softmax is $\\sum_{j=1}^3 \\exp(s_{1j}) = \\exp(\\frac{1}{\\sqrt{2}}) + \\exp(0) + \\exp(\\frac{1}{\\sqrt{2}}) = 2\\exp(\\frac{1}{\\sqrt{2}}) + 1$.\nThe components of the first row of $A_w$ are:\n$$A_{w,11} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}, \\quad A_{w,12} = \\frac{\\exp(0)}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1} = \\frac{1}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}, \\quad A_{w,13} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}$$\n\nFor the second row of $S$, $s_2 = (0, \\frac{1}{\\sqrt{2}}, 0)$:\nThe denominator for the softmax is $\\sum_{j=1}^3 \\exp(s_{2j}) = \\exp(0) + \\exp(\\frac{1}{\\sqrt{2}}) + \\exp(0) = 2 + \\exp(\\frac{1}{\\sqrt{2}})$.\nThe components of the second row of $A_w$ are:\n$$A_{w,21} = \\frac{1}{2 + \\exp(\\frac{1}{\\sqrt{2}})}, \\quad A_{w,22} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2 + \\exp(\\frac{1}{\\sqrt{2}})}, \\quad A_{w,23} = \\frac{1}{2 + \\exp(\\frac{1}{\\sqrt{2}})}$$\n\nThe third row of $S$, $s_3 = (\\frac{1}{\\sqrt{2}}, 0, \\frac{1}{\\sqrt{2}})$, is identical to the first row $s_1$. Thus, the third row of the attention matrix $A_w$ is identical to its first row: $A_{w,3j} = A_{w,1j}$ for $j \\in \\{1, 2, 3\\}$.\n\nStep 4: Compute the final output matrix $Z = A_w V$.\n$$Z = A_w V = \\begin{pmatrix} A_{w,11}  A_{w,12}  A_{w,13} \\\\ A_{w,21}  A_{w,22}  A_{w,23} \\\\ A_{w,31}  A_{w,32}  A_{w,33} \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  0 \\end{pmatrix}$$\nThe rows of the output matrix $Z$ are linear combinations of the rows of $V$. Let $v_1, v_2, v_3$ be the rows of $V$, so $v_1 = (1, 0)$, $v_2 = (0, 1)$, and $v_3 = (1, 0)$. Note that $v_1=v_3$.\n\nThe problem asks to identify which rows of the output are identical due to symmetry. The input query vectors are $q_1=(1,0)$, $q_2=(0,1)$, and $q_3=(1,0)$. Because $q_1=q_3$ and the key matrix $K$ is shared, the attention scores for the first and third queries will be identical. As established in Step 2, the first and third rows of the scaled scores matrix $S$ are the same. This leads to the first and third rows of the attention weight matrix $A_w$ being identical, as shown in Step 3.\nThe first and third rows of the output $Z$ are:\n$$Z_1 = A_{w,11}v_1 + A_{w,12}v_2 + A_{w,13}v_3$$\n$$Z_3 = A_{w,31}v_1 + A_{w,32}v_2 + A_{w,33}v_3$$\nSince $A_{w,1j} = A_{w,3j}$ for all $j$, we have $Z_1 = Z_3$. The first and third rows of the output are identical.\n\nThe problem then asks for the common value of the first component of these identical output rows. We will compute the first component of the first output row, $Z_{11}$.\n$$Z_1 = (Z_{11}, Z_{12}) = A_{w,11}(1, 0) + A_{w,12}(0, 1) + A_{w,13}(1, 0) = (A_{w,11} + A_{w,13}, A_{w,12})$$\nThe first component is $Z_{11} = A_{w,11} + A_{w,13}$. From Step 3, we know $A_{w,11} = A_{w,13}$. Therefore:\n$$Z_{11} = 2A_{w,11} = 2 \\left( \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1} \\right) = \\frac{2\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}$$\n\nFinally, we compute the numerical value and round it to four significant figures.\nThe value of the exponent is $\\frac{1}{\\sqrt{2}} \\approx 0.70710678$.\nThe exponential term is $\\exp(\\frac{1}{\\sqrt{2}}) \\approx 2.02813039$.\nSubstituting this into the expression for $Z_{11}$:\n$$Z_{11} = \\frac{2 \\times 2.02813039}{2 \\times 2.02813039 + 1} = \\frac{4.05626078}{5.05626078} \\approx 0.80218349$$\nRounding this result to four significant figures gives $0.8022$.", "answer": "$$\\boxed{0.8022}$$", "id": "3185352"}]}