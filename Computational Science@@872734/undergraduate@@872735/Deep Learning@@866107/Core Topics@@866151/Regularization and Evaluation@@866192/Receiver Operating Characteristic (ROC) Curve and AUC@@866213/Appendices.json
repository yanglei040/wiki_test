{"hands_on_practices": [{"introduction": "Before we can analyze or compare models using the Area Under the Curve (AUC), we must first know how to compute it from a model's performance on a dataset. This practice [@problem_id:3284361] guides you through implementing the AUC calculation from a discrete set of $(\\mathrm{FPR}, \\mathrm{TPR})$ points, applying the trapezoidal rule to approximate the integral $\\int_{0}^{1} \\mathrm{TPR}(\\mathrm{FPR}) \\, d\\mathrm{FPR}$. By building a robust algorithm that includes critical preprocessing steps, you will gain a foundational, practical understanding of how this essential metric is realized in code.", "problem": "You are given discrete receiver operating characteristic (ROC) outputs as a finite set of points $\\{(\\mathrm{FPR}_i,\\mathrm{TPR}_i)\\}_{i=1}^m$, where $\\mathrm{FPR}$ denotes the false positive rate and $\\mathrm{TPR}$ denotes the true positive rate. The Area Under the ROC Curve (AUC) is the area under the graph of the function $\\mathrm{TPR}(\\mathrm{FPR})$ over the interval $[0,1]$, which can be formalized as the Riemann integral $\\int_{0}^{1} \\mathrm{TPR}(x)\\,dx$ when $\\mathrm{TPR}$ is regarded as a function of $x=\\mathrm{FPR}$. In practice, only discrete samples are available.\n\nYour task is to write a complete program that approximates this area using the trapezoidal rule derived from first principles of Riemann sums and piecewise-linear approximation, applied to a preprocessed version of the given points. The preprocessing must be as follows:\n- Include endpoints: ensure that $(0,0)$ and $(1,1)$ are included. If either endpoint is absent, it must be added.\n- Resolve duplicate abscissae: if multiple points share the same false positive rate $x$ value, retain only the point with the maximum true positive rate at that $x$ (this constructs the upper envelope at repeated abscissae).\n- Sort by abscissa: sort the remaining points by $\\mathrm{FPR}$ in nondecreasing order so that the abscissae form a valid partition of $[0,1]$.\n\nAssume that all provided coordinates satisfy $0 \\le \\mathrm{FPR} \\le 1$ and $0 \\le \\mathrm{TPR} \\le 1$. Use the trapezoidal rule on the sorted sequence to approximate $\\int_{0}^{1} \\mathrm{TPR}(x)\\,dx$ by summing the areas of trapezoids under consecutive line segments. The final AUC for each test case must be rounded to six decimal places.\n\nTest suite (each test case is a list of points $(\\mathrm{FPR},\\mathrm{TPR})$):\n- Case $1$ (general monotone case): $[(0,0),(0.2,0.5),(0.6,0.8),(1,1)]$.\n- Case $2$ (unsorted inputs): $[(1,1),(0.3,0.6),(0,0),(0.7,0.9)]$.\n- Case $3$ (duplicate abscissae, take maximum $\\mathrm{TPR}$): $[(0,0),(0.5,0.4),(0.5,0.6),(1,1)]$.\n- Case $4$ (vertical rise at zero, perfect classifier envelope): $[(0,0),(0,1),(1,1)]$.\n- Case $5$ (degenerate classifier): $[(0,0),(1,0)]$.\n- Case $6$ (missing endpoints, require augmentation): $[(0.2,0.3),(0.4,0.6),(0.9,0.95)]$.\n\nYour program must compute the AUC for each case using only the procedure described above, then print a single line containing a list of the six AUC values in the same order as the cases above, rounded to six decimal places. The output format must be exactly a single line containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,\\dots,r_6]$ where each $r_i$ is a floating-point number rounded to six decimal places. No units are involved in this problem; the output is unitless.", "solution": "The problem requires the computation of the Area Under the Receiver Operating Characteristic Curve (AUC) from a discrete set of points $\\{(\\mathrm{FPR}_i, \\mathrm{TPR}_i)\\}$. The AUC is defined as the definite integral of the True Positive Rate ($\\mathrm{TPR}$) with respect to the False Positive Rate ($\\mathrm{FPR}$) over the interval $[0, 1]$.\n$$\n\\mathrm{AUC} = \\int_{0}^{1} \\mathrm{TPR}(x) \\,dx\n$$\nwhere $x$ represents the $\\mathrm{FPR}$. Since the function $\\mathrm{TPR}(x)$ is only known at a finite number of points, the integral must be approximated numerically. The problem specifies the use of the trapezoidal rule, which is derived from a piecewise-linear approximation of the function.\n\nThe solution proceeds in two main stages: data preprocessing followed by numerical integration.\n\n**1. Data Preprocessing**\n\nBefore applying the trapezoidal rule, the raw data points must be processed to form a well-defined, monotonic function on a partition of the interval $[0, 1]$. This involves three sequential steps as stipulated.\n\n**Step 1.1: Endpoint Augmentation**\nThe integral is defined over the domain $[0, 1]$. To ensure the entire domain is covered, the standard endpoints of an ROC curve, $(0, 0)$ and $(1, 1)$, must be part of the dataset. The point $(0, 0)$ corresponds to a classifier threshold that never classifies an instance as positive, resulting in zero true positives and zero false positives. The point $(1, 1)$ corresponds to a threshold that always classifies an instance as positive, resulting in all true positives and all false positives. Therefore, we augment the initial set of points by including $(0, 0)$ and $(1, 1)$.\n\n**Step 1.2: Resolution of Duplicate Abscissae**\nA function must have a single value for each input. If multiple points share the same $\\mathrm{FPR}$ value, they represent different classifier performances at the same false positive cost. The ROC curve itself is defined as the upper envelope of all possible $(\\mathrm{FPR}, \\mathrm{TPR})$ pairs. Consequently, for any given $\\mathrm{FPR}$ value $x_i$, we must select the point with the maximum corresponding $\\mathrm{TPR}$ value. This is achieved by grouping points by their $\\mathrm{FPR}$ coordinate and retaining only the maximum $\\mathrm{TPR}$ for each group. For instance, if the points $(0.5, 0.4)$ and $(0.5, 0.6)$ are present, we retain $(0.5, 0.6)$ and discard $(0.5, 0.4)$.\n\n**Step 1.3: Sorting by Abscissa**\nThe trapezoidal rule sums the areas of trapezoids formed by consecutive points. This requires the points to be ordered by their x-coordinate ($\\mathrm{FPR}$). After the previous steps, the resulting unique points $\\{ (x_j, y_j) \\}_{j=0}^N$ are sorted in nondecreasing order of their $x_j$ values. This yields an ordered sequence of points $(x_0, y_0), (x_1, y_1), \\dots, (x_N, y_N)$ such that $0 = x_0 < x_1 < \\dots < x_N = 1$.\n\n**2. Numerical Integration using the Trapezoidal Rule**\n\nWith the preprocessed and sorted points $(x_i, y_i)$ for $i=0, \\dots, N$, we approximate the function $\\mathrm{TPR}(x)$ by connecting consecutive points with straight line segments. The area under this piecewise-linear curve is the sum of the areas of the trapezoids formed by each segment and the x-axis.\n\nFor any two consecutive points, $P_{i-1} = (x_{i-1}, y_{i-1})$ and $P_i = (x_i, y_i)$, they form a trapezoid with vertices at $(x_{i-1}, 0)$, $(x_i, 0)$, $(x_i, y_i)$, and $(x_{i-1}, y_{i-1})$. The area of this $i$-th trapezoid, $A_i$, is given by the formula:\n$$\nA_i = \\frac{1}{2} (y_{i-1} + y_i) (x_i - x_{i-1})\n$$\nThis formula represents the average height of the two parallel sides ($y_{i-1}$ and $y_i$) multiplied by the width of the base ($x_i - x_{i-1}$).\n\nThe total AUC is the sum of the areas of all such trapezoids from $i=1$ to $N$:\n$$\n\\mathrm{AUC} \\approx \\sum_{i=1}^{N} A_i = \\sum_{i=1}^{N} \\frac{1}{2} (y_{i-1} + y_i) (x_i - x_{i-1})\n$$\nThis summation provides the numerical approximation of $\\int_{0}^{1} \\mathrm{TPR}(x) \\,dx$. The final result for each test case is rounded to six decimal places as required.", "answer": "```python\nimport numpy as np\n\ndef calculate_auc(points_list):\n    \"\"\"\n    Calculates the Area Under the ROC Curve (AUC) from a list of (FPR, TPR) points.\n\n    The process involves three preprocessing steps followed by the trapezoidal rule application:\n    1. Augment points with (0,0) and (1,1).\n    2. Resolve duplicate FPRs by taking the maximum TPR.\n    3. Sort the points by FPR.\n    4. Apply the trapezoidal rule to the processed points.\n\n    Args:\n        points_list (list of tuples): A list where each tuple is an (FPR, TPR) point.\n\n    Returns:\n        float: The calculated AUC.\n    \"\"\"\n    # Step 1: Augment points with the definitional endpoints (0,0) and (1,1).\n    # This creates a comprehensive list to start with.\n    all_points = list(points_list)\n    all_points.append((0, 0))\n    all_points.append((1, 1))\n\n    # Step 2: Resolve duplicate abscissae (FPRs).\n    # We use a dictionary to store the maximum TPR for each unique FPR.\n    # The ROC curve is the upper envelope of performance, justifying taking the max TPR.\n    roc_map = {}\n    for fpr, tpr in all_points:\n        # If the FPR is already in the map, update it only if the new TPR is higher.\n        # Otherwise, add the new (FPR, TPR) pair.\n        roc_map[fpr] = max(roc_map.get(fpr, -1.0), tpr)\n    \n    # Convert the map back to a list of points.\n    processed_points = list(roc_map.items())\n\n    # Step 3: Sort the points by FPR in nondecreasing order.\n    # This prepares the points for the trapezoidal rule, ensuring a valid partition.\n    processed_points.sort(key=lambda p: p[0])\n\n    # Convert the list of points to a NumPy array for efficient computation.\n    roc_array = np.array(processed_points)\n    x_coords = roc_array[:, 0]  # FPR values\n    y_coords = roc_array[:, 1]  # TPR values\n\n    # Step 4: Apply the trapezoidal rule.\n    # np.trapz(y, x) computes the integral of y(x) using the trapezoidal rule.\n    # This is equivalent to sum(0.5 * (y_i + y_{i-1}) * (x_i - x_{i-1})).\n    auc = np.trapz(y_coords, x_coords)\n    \n    return auc\n\n\ndef solve():\n    \"\"\"\n    Main function to execute the AUC calculation for all specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: General monotone case\n        [(0,0),(0.2,0.5),(0.6,0.8),(1,1)],\n        # Case 2: Unsorted inputs\n        [(1,1),(0.3,0.6),(0,0),(0.7,0.9)],\n        # Case 3: Duplicate abscissae, take maximum TPR\n        [(0,0),(0.5,0.4),(0.5,0.6),(1,1)],\n        # Case 4: Vertical rise at zero, perfect classifier envelope\n        [(0,0),(0,1),(1,1)],\n        # Case 5: Degenerate classifier\n        [(0,0),(1,0)],\n        # Case 6: Missing endpoints, require augmentation\n        [(0.2,0.3),(0.4,0.6),(0.9,0.95)]\n    ]\n\n    results = []\n    for case in test_cases:\n        auc_value = calculate_auc(case)\n        # Format the result to exactly six decimal places as a string.\n        results.append(f\"{auc_value:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3284361"}, {"introduction": "Why is AUC so widely used? A key reason is its focus on ranking quality, making it invariant to any monotonic transformation of model scores. This exercise [@problem_id:3167199] demonstrates this fundamental property using temperature scaling, a transformation $z \\mapsto z/\\alpha$. You will observe firsthand that while AUC remains constant, threshold-dependent metrics like precision and recall do not, clarifying the distinct roles of different evaluation metrics.", "problem": "Consider binary classification in deep learning where a model outputs real-valued logits. Temperature scaling transforms logits by $z \\mapsto z/\\alpha$ for any positive scalar $\\alpha$, and predicted probabilities are obtained by applying the logistic sigmoid $\\sigma(s) = 1/(1+e^{-s})$ to a score $s$. At a fixed probability decision threshold $t$, the predicted class is positive if and only if $\\sigma(z/\\alpha) \\ge t$. Precision and recall are defined in terms of the confusion matrix counts for the chosen threshold, whereas the Receiver Operating Characteristic (ROC) curve is defined by the set of true positive rate and false positive rate pairs obtained by varying a threshold across the entire score range. The Area Under the ROC Curve (AUC) is the area under this ROC curve and can be equivalently interpreted as the probability that a randomly chosen positive example is ranked above a randomly chosen negative example by the scoring function.\n\nStarting from these core definitions, write a complete, runnable program that, for each test case below, performs the following:\n\n1. Applies temperature scaling with $\\alpha \\in \\{0.5, 1.0, 2.0\\}$ to the provided logits, computes scores $s_i = z_i/\\alpha$, and then:\n   - Computes the ROC Area Under the Curve (AUC) using the pairwise ranking-based definition, namely the probability that a randomly chosen positive score exceeds a randomly chosen negative score, with ties contributing half credit.\n   - Computes precision and recall at a fixed probability decision threshold $t = 0.7$ using the transformed probabilities $p_i = \\sigma(s_i)$, with the following conventions to handle zero denominators: if there are no predicted positives, define precision as $0$; if there are no actual positives, define recall as $0$.\n\n2. Demonstrates metric-specific sensitivity by:\n   - Checking the invariance of AUC under temperature scaling, i.e., whether the AUC values for $\\alpha = 0.5$ and $\\alpha = 2.0$ are equal to the baseline AUC for $\\alpha = 1.0$ within an absolute tolerance of $10^{-12}$.\n   - Checking whether either precision or recall, computed at the fixed threshold $t = 0.7$, changes across the three $\\alpha$ values.\n\n3. Produces, for each test case, the following outputs:\n   - The baseline AUC for $\\alpha = 1.0$, rounded to $6$ decimal places.\n   - A boolean indicating whether AUC invariance holds within the specified tolerance across $\\alpha \\in \\{0.5, 1.0, 2.0\\}$.\n   - A boolean indicating whether precision or recall at threshold $t = 0.7$ changes across $\\alpha \\in \\{0.5, 1.0, 2.0\\}$.\n\nUse the following test suite, which covers a general mixed case, a case with ties, and a class-imbalanced case:\n\n- Test Case $1$ (general mixed case):\n  - Labels $\\mathbf{y} = [1, 0, 1, 0, 1, 0, 0, 1, 0, 1]$.\n  - Logits $\\mathbf{z} = [2.4, -0.5, 0.3, -1.0, 1.2, 0.4, -2.2, 0.9, -0.8, 3.1]$.\n\n- Test Case $2$ (ties present):\n  - Labels $\\mathbf{y} = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]$.\n  - Logits $\\mathbf{z} = [0.5, 0.5, 0.5, -0.5, -0.5, -0.5, 1.2, 1.2, -1.2, -1.2]$.\n\n- Test Case $3$ (class imbalance):\n  - Labels $\\mathbf{y} = [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]$.\n  - Logits $\\mathbf{z} = [-2.1, -1.8, -0.2, 0.1, -0.5, 0.6, -1.0, 1.1, -0.7, 0.3, -0.9, -0.4, 2.0, -3.0, 0.8]$.\n\nYour program should produce a single line of output containing the aggregated results for the three test cases, ordered as follows:\n$[\\text{auc\\_case1}, \\text{invariant\\_case1}, \\text{sensitive\\_case1}, \\text{auc\\_case2}, \\text{invariant\\_case2}, \\text{sensitive\\_case2}, \\text{auc\\_case3}, \\text{invariant\\_case3}, \\text{sensitive\\_case3}]$.\n\nAll $9$ outputs must be in the specified types: floats for AUC values (rounded to $6$ decimal places) and booleans for the invariance and sensitivity checks. There are no physical units involved in this problem. The final line of output must be printed exactly as a comma-separated list enclosed in square brackets.", "solution": "The problem has been validated and found to be scientifically sound, well-posed, and complete. All definitions, data, and constraints are provided and are consistent with established principles in machine learning and statistics.\n\nThe problem explores the effect of temperature scaling on different binary classification metrics. It contrasts rank-based metrics, such as the Area Under the Receiver Operating Characteristic Curve (AUC), with threshold-based metrics like precision and recall.\n\n### Core Concepts and Mathematical Formulation\n\nA deep learning model for binary classification outputs logits, $\\mathbf{z}$, which are real-valued scores. These are transformed into probabilities using a scoring function and a probability mapping.\n\n1.  **Temperature Scaling**: The logits $z_i$ are scaled by a positive temperature parameter $\\alpha$:\n    $$s_i = \\frac{z_i}{\\alpha}$$\n    This scaling sharpens ($\\alpha < 1$) or softens ($\\alpha > 1$) the distribution of scores. The baseline is $\\alpha = 1.0$, where $s_i = z_i$.\n\n2.  **Probability Calculation**: The scaled scores $s_i$ are converted to probabilities $p_i$ using the logistic sigmoid function $\\sigma$:\n    $$p_i = \\sigma(s_i) = \\frac{1}{1 + e^{-s_i}}$$\n\n3.  **AUC (Area Under the ROC Curve)**: The AUC is a rank-based metric. It evaluates the model's ability to rank a randomly chosen positive example higher than a randomly chosen negative example. Given the set of scores for positive examples, $S_{pos}$, and for negative examples, $S_{neg}$, the AUC is calculated as:\n    $$ \\text{AUC} = \\frac{1}{|S_{pos}| |S_{neg}|} \\sum_{s_p \\in S_{pos}} \\sum_{s_n \\in S_{neg}} K(s_p, s_n) $$\n    where $K(s_p, s_n)$ is a comparison kernel:\n    $$ K(s_p, s_n) = \\begin{cases} 1 & \\text{if } s_p > s_n \\\\ 0.5 & \\text{if } s_p = s_n \\\\ 0 & \\text{if } s_p < s_n \\end{cases} $$\n    Since temperature scaling with $\\alpha > 0$ is a monotonic transformation ($z_i > z_j \\iff z_i/\\alpha > z_j/\\alpha$), the relative ordering of scores is preserved. Consequently, the value of $K(s_p, s_n)$ remains unchanged for any positive $\\alpha$. This implies that AUC is theoretically invariant to temperature scaling. The computational check will verify this property within a small floating-point tolerance.\n\n4.  **Precision and Recall**: These are threshold-based metrics. A decision threshold $t$ is applied to the predicted probabilities $p_i$ to obtain binary predictions $\\hat{y}_i$:\n    $$ \\hat{y}_i = \\begin{cases} 1 & \\text{if } p_i \\ge t \\\\ 0 & \\text{if } p_i < t \\end{cases} $$\n    Precision and recall are then computed from the confusion matrix elements: True Positives ($TP$), False Positives ($FP$), and False Negatives ($FN$).\n    $$ \\text{Precision} = \\frac{TP}{TP + FP} \\quad (\\text{defined as } 0 \\text{ if } TP+FP=0) $$\n    $$ \\text{Recall} = \\frac{TP}{TP + FN} \\quad (\\text{defined as } 0 \\text{ if } TP+FN=0) $$\n    The classification rule $p_i \\ge t$ is equivalent to operating on the logits with a derived threshold. Substituting the definitions:\n    $$ \\sigma(z_i/\\alpha) \\ge t \\implies \\frac{1}{1 + e^{-z_i/\\alpha}} \\ge t \\implies \\frac{z_i}{\\alpha} \\ge \\sigma^{-1}(t) \\implies z_i \\ge \\alpha \\cdot \\sigma^{-1}(t) $$\n    where $\\sigma^{-1}(t) = \\ln(t / (1-t))$ is the logit-space threshold. The decision boundary for the logits, $z_i \\ge \\alpha \\cdot \\ln(t/(1-t))$, directly depends on $\\alpha$. For a fixed probability threshold $t=0.7$, the logit threshold changes with $\\alpha$:\n    -   For $\\alpha = 0.5$: $z_i \\ge 0.5 \\cdot \\ln(0.7/0.3) \\approx 0.4236$\n    -   For $\\alpha = 1.0$: $z_i \\ge 1.0 \\cdot \\ln(0.7/0.3) \\approx 0.8473$\n    -   For $\\alpha = 2.0$: $z_i \\ge 2.0 \\cdot \\ln(0.7/0.3) \\approx 1.6946$\n    Since the decision boundary changes, the set of predicted positive instances will change, leading to different values of $TP$, $FP$, and $FN$. Therefore, precision and recall are sensitive to temperature scaling.\n\n### Algorithm for Solution\n\nFor each test case provided:\n1.  Initialize an empty list to store the final results for that case.\n2.  Define the labels $\\mathbf{y}$ and logits $\\mathbf{z}$ from the test case data.\n3.  Define the set of temperature scaling factors $\\mathcal{A} = \\{0.5, 1.0, 2.0\\}$ and the probability threshold $t = 0.7$.\n4.  Create storage for metrics computed for each $\\alpha \\in \\mathcal{A}$.\n5.  Iterate through each $\\alpha \\in \\mathcal{A}$:\n    a.  Compute the scaled scores $\\mathbf{s} = \\mathbf{z} / \\alpha$.\n    b.  **Calculate AUC**: Separate scores into positive ($S_{pos}$) and negative ($S_{neg}$) sets based on labels $\\mathbf{y}$. Systematically perform all pairwise comparisons as per the AUC formula above, handling ties with a score of $0.5$.\n    c.  **Calculate Precision and Recall**:\n        i.  Compute probabilities $\\mathbf{p} = \\sigma(\\mathbf{s})$.\n        ii. Make predictions $\\hat{\\mathbf{y}}$ by thresholding probabilities at $t$.\n        iii. Count $TP, FP, FN$ by comparing $\\hat{\\mathbf{y}}$ with $\\mathbf{y}$.\n        iv. Compute precision and recall, applying the provided rules for zero denominators.\n    d.  Store the computed AUC, precision, and recall for the current $\\alpha$.\n6.  After iterating through all $\\alpha$ values:\n    a.  **AUC Invariance Check**: Compare the AUC values for $\\alpha=0.5$ and $\\alpha=2.0$ against the baseline AUC for $\\alpha=1.0$. The invariance holds if the absolute differences are all within the tolerance of $10^{-12}$.\n    b.  **Precision/Recall Sensitivity Check**: Compare the `(precision, recall)` pair for $\\alpha=0.5$ and $\\alpha=2.0$ against the baseline pair for $\\alpha=1.0$. The metrics are sensitive if at least one of these pairs is different from the baseline.\n7.  Format the final results for the test case: the baseline AUC (for $\\alpha=1.0$) rounded to $6$ decimal places, the boolean result of the AUC invariance check, and the boolean result of the sensitivity check.\n8.  Aggregate the formatted results from all test cases into a single list and print in the specified format.\n\nThis procedure systematically addresses all requirements of the problem, leading to the final verifiable outputs.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the final formatted result.\n    \"\"\"\n    test_cases = [\n        {\n            \"labels\": np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 1]),\n            \"logits\": np.array([2.4, -0.5, 0.3, -1.0, 1.2, 0.4, -2.2, 0.9, -0.8, 3.1]),\n        },\n        {\n            \"labels\": np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0]),\n            \"logits\": np.array([0.5, 0.5, 0.5, -0.5, -0.5, -0.5, 1.2, 1.2, -1.2, -1.2]),\n        },\n        {\n            \"labels\": np.array([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]),\n            \"logits\": np.array([-2.1, -1.8, -0.2, 0.1, -0.5, 0.6, -1.0, 1.1, -0.7, 0.3, -0.9, -0.4, 2.0, -3.0, 0.8]),\n        },\n    ]\n\n    all_results = []\n    alphas = [0.5, 1.0, 2.0]\n    prob_threshold = 0.7\n    tolerance = 1e-12\n\n    for case in test_cases:\n        labels, logits = case[\"labels\"], case[\"logits\"]\n        \n        metrics_by_alpha = {}\n\n        for alpha in alphas:\n            scores = logits / alpha\n            \n            # --- AUC Calculation ---\n            pos_scores = scores[labels == 1]\n            neg_scores = scores[labels == 0]\n            \n            num_pos = len(pos_scores)\n            num_neg = len(neg_scores)\n            \n            if num_pos == 0 or num_neg == 0:\n                auc = 0.5 # Or undefined, per problem context this won't happen\n            else:\n                pairwise_wins = 0\n                for s_p in pos_scores:\n                    for s_n in neg_scores:\n                        if s_p > s_n:\n                            pairwise_wins += 1\n                        elif s_p == s_n:\n                            pairwise_wins += 0.5\n                auc = pairwise_wins / (num_pos * num_neg)\n\n            # --- Precision and Recall Calculation ---\n            probs = 1.0 / (1.0 + np.exp(-scores))\n            predictions = (probs >= prob_threshold).astype(int)\n            \n            true_positives = np.sum((predictions == 1) & (labels == 1))\n            false_positives = np.sum((predictions == 1) & (labels == 0))\n            false_negatives = np.sum((predictions == 0) & (labels == 1))\n            \n            num_predicted_pos = true_positives + false_positives\n            num_actual_pos = true_positives + false_negatives\n            \n            precision = true_positives / num_predicted_pos if num_predicted_pos > 0 else 0.0\n            recall = true_positives / num_actual_pos if num_actual_pos > 0 else 0.0\n            \n            metrics_by_alpha[alpha] = {\"auc\": auc, \"precision\": precision, \"recall\": recall}\n\n        # --- Perform Checks ---\n        baseline_metrics = metrics_by_alpha[1.0]\n        \n        # AUC Invariance Check\n        auc_0_5 = metrics_by_alpha[0.5][\"auc\"]\n        auc_2_0 = metrics_by_alpha[2.0][\"auc\"]\n        is_auc_invariant = (abs(auc_0_5 - baseline_metrics[\"auc\"]) <= tolerance and\n                            abs(auc_2_0 - baseline_metrics[\"auc\"]) <= tolerance)\n        \n        # Precision/Recall Sensitivity Check\n        pr_0_5 = (metrics_by_alpha[0.5][\"precision\"], metrics_by_alpha[0.5][\"recall\"])\n        pr_1_0 = (baseline_metrics[\"precision\"], baseline_metrics[\"recall\"])\n        pr_2_0 = (metrics_by_alpha[2.0][\"precision\"], metrics_by_alpha[2.0][\"recall\"])\n        \n        is_pr_sensitive = (pr_0_5 != pr_1_0) or (pr_2_0 != pr_1_0)\n\n        # --- Store Results ---\n        all_results.append(f\"{baseline_metrics['auc']:.6f}\")\n        all_results.append(str(is_auc_invariant))\n        all_results.append(str(is_pr_sensitive))\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3167199"}, {"introduction": "A single, overall AUC score doesn't always tell the whole story, especially when the ROC curves of competing models cross. In many real-world scenarios, such as medical diagnostics or fraud detection, performance in a low-false-positive-rate regime is what truly matters. This exercise [@problem_id:3167178] introduces the partial AUC (pAUC), defined as $\\int_{0}^{\\alpha} \\mathrm{TPR}(\\mathrm{FPR}) \\, d\\mathrm{FPR}$, to focus evaluation on these critical operating ranges. You will implement pAUC to perform a more nuanced model comparison, a skill essential for deploying models where controlling false alarms is paramount.", "problem": "You are given two binary classifiers (for example, two deep neural network models) that output real-valued scores for each example. From first principles, the Receiver Operating Characteristic (ROC) curve is the set of points of the form $\\left(\\mathrm{FPR}(t), \\mathrm{TPR}(t)\\right)$ obtained by varying a decision threshold $t$ on the scores, where the False Positive Rate (FPR) and True Positive Rate (TPR) at threshold $t$ are defined by\n$$\n\\mathrm{FPR}(t) \\equiv \\frac{\\mathrm{FP}(t)}{\\mathrm{FP}(t)+\\mathrm{TN}(t)}, \\quad\n\\mathrm{TPR}(t) \\equiv \\frac{\\mathrm{TP}(t)}{\\mathrm{TP}(t)+\\mathrm{FN}(t)}.\n$$\nHere, $\\mathrm{TP}(t)$, $\\mathrm{FP}(t)$, $\\mathrm{TN}(t)$, and $\\mathrm{FN}(t)$ are the counts of true positives, false positives, true negatives, and false negatives induced by thresholding the scores at $t$ on a dataset with binary labels. The Area Under the Curve (AUC) is the integral of $\\mathrm{TPR}$ with respect to $\\mathrm{FPR}$ along the ROC curve over the full range $\\mathrm{FPR}\\in[0,1]$. For applications requiring very low false alarm rates, a relevant metric is the partial AUC over a restricted false positive rate regime, defined as\n$$\n\\mathrm{pAUC}(\\alpha) \\equiv \\int_{0}^{\\alpha} \\mathrm{TPR}(\\mathrm{FPR}) \\, d\\,\\mathrm{FPR},\n$$\nwhere $\\alpha \\in [0,1]$ specifies the maximum allowable false positive rate.\n\nYour task is to implement, from these definitions, an algorithm that:\n- Computes the ROC points by sweeping thresholds across the sorted scores.\n- Computes the partial AUC $\\mathrm{pAUC}(\\alpha)$ by integrating $\\mathrm{TPR}$ with respect to $\\mathrm{FPR}$ over the interval $[0,\\alpha]$ using a piecewise-linear interpolation between successive ROC points and the trapezoidal rule.\n- Computes $\\mathrm{TPR}$ at an exact $\\mathrm{FPR}=\\alpha$ by linear interpolation along the ROC curve; when the ROC has vertical segments (i.e., $\\mathrm{FPR}$ does not change while $\\mathrm{TPR}$ increases), define $\\mathrm{TPR}$ at a given $\\mathrm{FPR}$ as the maximum $\\mathrm{TPR}$ achieved at that $\\mathrm{FPR}$.\n- Picks the better model for low-false-alarm applications by the following deterministic rule: prefer the model with larger $\\mathrm{pAUC}(\\alpha)$; if the $\\mathrm{pAUC}(\\alpha)$ values are equal within a numerical tolerance, prefer the model with larger $\\mathrm{TPR}$ at $\\mathrm{FPR}=\\alpha$; if still equal, prefer the model with larger full AUC; if still equal, choose Model A.\n\nConstruct and evaluate the following test suite of parameter values. Each test case specifies the binary label vector and the score vectors from each model aligned by index, followed by the value of $\\alpha$. All values are unitless real numbers.\n\n- Test case $1$ (crossing ROC curves; low $\\alpha$):\n  - Labels $y^{(1)}$: $[\\,1,\\,0,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0\\,]$.\n  - Model A scores $s_A^{(1)}$: $[\\,0.98,\\,0.62,\\,0.61,\\,0.95,\\,0.59,\\,0.60,\\,0.57,\\,0.58,\\,0.55,\\,0.45,\\,0.50,\\,0.49,\\,0.42,\\,0.47,\\,0.43,\\,0.40,\\,0.41,\\,0.37,\\,0.38,\\,0.36\\,]$.\n  - Model B scores $s_B^{(1)}$: $[\\,0.80,\\,0.60,\\,0.82,\\,0.78,\\,0.58,\\,0.76,\\,0.56,\\,0.74,\\,0.54,\\,0.72,\\,0.52,\\,0.79,\\,0.70,\\,0.50,\\,0.48,\\,0.68,\\,0.46,\\,0.44,\\,0.66,\\,0.42\\,]$.\n  - $\\alpha^{(1)} = 0.10$.\n\n- Test case $2$ (same data; full AUC comparison):\n  - Labels $y^{(2)}$: identical to $y^{(1)}$.\n  - Model A scores $s_A^{(2)}$: identical to $s_A^{(1)}$.\n  - Model B scores $s_B^{(2)}$: identical to $s_B^{(1)}$.\n  - $\\alpha^{(2)} = 1.00$.\n\n- Test case $3$ (same data; zero false positive rate boundary):\n  - Labels $y^{(3)}$: identical to $y^{(1)}$.\n  - Model A scores $s_A^{(3)}$: identical to $s_A^{(1)}$.\n  - Model B scores $s_B^{(3)}$: identical to $s_B^{(1)}$.\n  - $\\alpha^{(3)} = 0.00$.\n\n- Test case $4$ (identical models; tie-breaking to Model A):\n  - Labels $y^{(4)}$: $[\\,1,\\,0,\\,0,\\,0,\\,0,\\,1,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,1,\\,0,\\,0\\,]$.\n  - Model A scores $s_A^{(4)}$: $[\\,0.90,\\,0.80,\\,0.65,\\,0.55,\\,0.53,\\,0.70,\\,0.51,\\,0.49,\\,0.47,\\,0.45,\\,0.43,\\,0.41,\\,0.60,\\,0.39,\\,0.37\\,]$.\n  - Model B scores $s_B^{(4)}$: identical to $s_A^{(4)}$.\n  - $\\alpha^{(4)} = 0.20$.\n\nYour program must implement the above algorithm without using any external datasets or user input and produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case $i \\in \\{1,2,3,4\\}$, output an integer where $0$ means Model A is preferred and $1$ means Model B is preferred under the specified decision rule, so the final output format is exactly of the form $[\\,r_1, r_2, r_3, r_4\\,]$ with each $r_i \\in \\{0,1\\}$ expressed as integers.", "solution": "The problem requires the implementation of an algorithm to compare two binary classifiers, Model A and Model B, based on their performance in a low-false-alarm regime. The comparison is based on the partial Area Under the Receiver Operating Characteristic (ROC) curve, denoted as $\\mathrm{pAUC}(\\alpha)$, where $\\alpha$ is the maximum acceptable False Positive Rate (FPR). The solution involves several steps: generating the ROC curve, calculating the full and partial AUC, determining the True Positive Rate (TPR) at a specific FPR, and applying a deterministic decision rule.\n\n### Step 1: ROC Curve Generation\n\nThe Receiver Operating Characteristic (ROC) curve is a plot of the True Positive Rate ($\\mathrm{TPR}$) against the False Positive Rate ($\\mathrm{FPR}$) at various decision thresholds. The $\\mathrm{TPR}$ is the fraction of positive instances correctly classified, and the $\\mathrm{FPR}$ is the fraction of negative instances incorrectly classified.\n$$\n\\mathrm{TPR}(t) = \\frac{\\mathrm{TP}(t)}{P}, \\quad \\mathrm{FPR}(t) = \\frac{\\mathrm{FP}(t)}{N}\n$$\nwhere $P$ is the total number of positive instances and $N$ is the total number of negative instances. $\\mathrm{TP}(t)$ and $\\mathrm{FP}(t)$ are the counts of true positives and false positives when a threshold $t$ is applied to the classifier's scores.\n\nAn efficient algorithm to generate the ROC curve points is as follows:\n1.  Combine the true labels $y \\in \\{0, 1\\}$ and the classifier scores $s$ into pairs.\n2.  Sort these pairs in descending order based on the scores. A stable sort (such as `mergesort`) is used to handle ties in scores consistently.\n3.  Iterate through the sorted list, calculating the cumulative sum of true positives ($\\mathrm{TP}$) and false positives ($\\mathrm{FP}$). The points of the ROC curve correspond to the unique thresholds, which are the unique score values.\n4.  To correctly handle tied scores, we identify the indices where the score value changes. These indices mark the end of a group of instances with the same score, which are processed as a single threshold step. The cumulative $\\mathrm{TP}$ and $\\mathrm{FP}$ counts at these indices give the coordinates of the ROC curve's \"corners\".\n5.  The counts are normalized by $P$ and $N$ to get $\\mathrm{TPR}$ and $\\mathrm{FPR}$ values. The point $(0, 0)$ is prepended to the lists, representing a threshold higher than any score (classifying all instances as negative).\n\nThe result is a set of coordinates $(f_i, t_i)$ representing the ROC curve. The array of $f_i$ values is monotonically non-decreasing.\n\n### Step 2: Defining the Functional ROC Curve\n\nThe problem states that for interpolation and integration, the ROC curve should be treated as a function $\\mathrm{TPR}(\\mathrm{FPR})$. In cases where the curve has vertical segments (multiple $\\mathrm{TPR}$ values for a single $\\mathrm{FPR}$), the $\\mathrm{TPR}$ at that $\\mathrm{FPR}$ is defined as the *maximum* $\\mathrm{TPR}$ achieved. This creates the upper envelope of the ROC curve.\n\n1.  From the generated sequence of ROC points $(f_i, t_i)$, we identify the set of unique $\\mathrm{FPR}$ values, let's call them $f'_j$.\n2.  For each unique $f'_j$, we find the maximum corresponding $\\mathrm{TPR}$ value: $t'_j = \\max \\{ t_i \\mid f_i = f'_j \\}$.\n3.  The resulting points $(f'_j, t'_j)$ form a well-defined function $\\mathrm{TPR} = g(\\mathrm{FPR})$, where the $f'_j$ values are strictly increasing. This well-behaved representation is suitable for both interpolation and numerical integration.\n\n### Step 3: Calculation of Metrics\n\nUsing the functional ROC curve representation $(f'_j, t'_j)$, we compute the required metrics:\n\n1.  **Full Area Under the Curve (AUC):** The full AUC is the integral of the functional ROC curve from $\\mathrm{FPR}=0$ to $\\mathrm{FPR}=1$. This is computed using the trapezoidal rule on the points $(f'_j, t'_j)$.\n    $$\n    \\mathrm{AUC} = \\int_{0}^{1} \\mathrm{TPR}(\\mathrm{FPR}) \\, d\\mathrm{FPR} \\approx \\sum_{j=1}^{m} \\frac{t'_{j-1} + t'_j}{2} (f'_j - f'_{j-1})\n    $$\n\n2.  **$\\mathrm{TPR}$ at $\\mathrm{FPR}=\\alpha$:** This value is found by performing linear interpolation on the functional ROC curve points $(f'_j, t'_j)$ at the point $\\mathrm{FPR}=\\alpha$.\n\n3.  **Partial AUC ($\\mathrm{pAUC}(\\alpha)$):** This is the integral of the functional ROC curve from $\\mathrm{FPR}=0$ to $\\mathrm{FPR}=\\alpha$.\n    $$\n    \\mathrm{pAUC}(\\alpha) = \\int_{0}^{\\alpha} \\mathrm{TPR}(\\mathrm{FPR}) \\, d\\mathrm{FPR}\n    $$\n    To compute this, we select the subset of points $(f'_j, t'_j)$ where $f'_j \\le \\alpha$. If $\\alpha$ itself is not one of the $f'_j$ values, we add a new point $(\\alpha, \\mathrm{TPR}(\\alpha))$ to this subset, where $\\mathrm{TPR}(\\alpha)$ is the value obtained from linear interpolation. The trapezoidal rule is then applied to this subset of points.\n\n### Step 4: Deterministic Decision Rule\n\nThe final step is to select the better model based on the following hierarchical rule, using a small numerical tolerance (e.g., $10^{-9}$) for equality comparisons:\n1.  Prefer the model with the larger $\\mathrm{pAUC}(\\alpha)$.\n2.  If the $\\mathrm{pAUC}(\\alpha)$ values are equal, prefer the model with the larger $\\mathrm{TPR}$ at $\\mathrm{FPR} = \\alpha$.\n3.  If these are also equal, prefer the model with the larger full AUC.\n4.  If all three metrics are equal, the tie is broken by choosing Model A by default.\n\nThis entire procedure is implemented for each test case to determine the preferred model, outputting $0$ for Model A and $1$ for Model B.", "answer": "```python\nimport numpy as np\n\ndef compute_metrics(y_true, y_scores, alpha):\n    \"\"\"\n    Computes Receiver Operating Characteristic (ROC) curve-based metrics from first principles.\n\n    This function calculates the partial Area Under the Curve (pAUC), the True Positive Rate (TPR)\n    at a specific False Positive Rate (FPR), and the full AUC.\n\n    The ROC curve is first generated by sorting scores and calculating cumulative TP/FP counts.\n    To satisfy the problem's requirements for interpolation and integration, a functional\n    representation of the curve (its upper envelope) is created, ensuring a unique TPR for each FPR.\n    Metrics are then calculated on this well-defined function.\n\n    Args:\n        y_true (list or np.ndarray): True binary labels (0 or 1).\n        y_scores (list or np.ndarray): Target scores from a classifier.\n        alpha (float): The maximum False Positive Rate for pAUC calculation.\n\n    Returns:\n        tuple: A tuple containing (pAUC, TPR_at_alpha, full_AUC). Returns (0.0, 0.0, 0.0)\n               if the data contains only one class.\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=np.int32)\n    y_scores = np.asarray(y_scores, dtype=np.float64)\n\n    # Sort scores and corresponding truth values in descending order.\n    # 'mergesort' is a stable sort, which is important for reproducibility.\n    desc_score_indices = np.argsort(y_scores, kind=\"mergesort\")[::-1]\n    y_scores = y_scores[desc_score_indices]\n    y_true = y_true[desc_score_indices]\n\n    # Calculate cumulative true positives (TPs) and false positives (FPs).\n    tps_cum = np.cumsum(y_true)\n    fps_cum = np.cumsum(1 - y_true)\n    \n    # Get total positives (P) and negatives (N).\n    P = tps_cum[-1] if len(tps_cum) > 0 else 0\n    N = fps_cum[-1] if len(fps_cum) > 0 else 0\n\n    if P == 0 or N == 0:\n        # ROC is undefined for single-class data.\n        return 0.0, 0.0, 0.0\n\n    # Find the indices corresponding to unique thresholds. A new threshold step\n    # is taken for each unique score value.\n    distinct_value_indices = np.where(np.diff(y_scores))[0]\n    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n\n    # Extract the (TP, FP) counts at each threshold step.\n    tps = tps_cum[threshold_idxs]\n    fps = fps_cum[threshold_idxs]\n\n    # Convert counts to rates and prepend the (0,0) origin point.\n    fpr_raw = fps / N\n    tpr_raw = tps / P\n    fpr = np.r_[0.0, fpr_raw]\n    tpr = np.r_[0.0, tpr_raw]\n\n    # The problem defines TPR at a given FPR as the maximum TPR achieved.\n    # This forms the upper envelope of the ROC curve, making it a well-defined function.\n    # This also ensures the `fpr_u` array is strictly increasing for interpolation.\n    fpr_u = np.unique(fpr)\n    tpr_u = np.array([tpr[fpr == f].max() for f in fpr_u])\n\n    # 1. Calculate full AUC using the trapezoidal rule on the functional curve.\n    full_auc = np.trapz(tpr_u, fpr_u)\n    \n    # 2. Calculate TPR at FPR=alpha via linear interpolation on the functional curve.\n    tpr_at_alpha = np.interp(alpha, fpr_u, tpr_u)\n\n    # 3. Calculate partial AUC up to alpha.\n    pauc = 0.0\n    if alpha > 0.0:\n        # Select the part of the functional curve up to the FPR limit `alpha`.\n        fpr_pauc = fpr_u[fpr_u <= alpha]\n        tpr_pauc = tpr_u[fpr_u <= alpha]\n        \n        # If alpha is not one of the existing FPR points, add it via interpolation\n        # to correctly calculate the area of the final trapezoid.\n        if not np.isclose(fpr_pauc[-1], alpha):\n            fpr_pauc = np.append(fpr_pauc, alpha)\n            tpr_pauc = np.append(tpr_pauc, tpr_at_alpha)\n\n        pauc = np.trapz(tpr_pauc, fpr_pauc)\n        \n    return pauc, tpr_at_alpha, full_auc\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and determine the preferred model for each case.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"y\": [1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n            \"s_A\": [0.98, 0.62, 0.61, 0.95, 0.59, 0.60, 0.57, 0.58, 0.55, 0.45, 0.50, 0.49, 0.42, 0.47, 0.43, 0.40, 0.41, 0.37, 0.38, 0.36],\n            \"s_B\": [0.80, 0.60, 0.82, 0.78, 0.58, 0.76, 0.56, 0.74, 0.54, 0.72, 0.52, 0.79, 0.70, 0.50, 0.48, 0.68, 0.46, 0.44, 0.66, 0.42],\n            \"alpha\": 0.10\n        },\n        # Test case 2\n        {\n            \"y\": [1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n            \"s_A\": [0.98, 0.62, 0.61, 0.95, 0.59, 0.60, 0.57, 0.58, 0.55, 0.45, 0.50, 0.49, 0.42, 0.47, 0.43, 0.40, 0.41, 0.37, 0.38, 0.36],\n            \"s_B\": [0.80, 0.60, 0.82, 0.78, 0.58, 0.76, 0.56, 0.74, 0.54, 0.72, 0.52, 0.79, 0.70, 0.50, 0.48, 0.68, 0.46, 0.44, 0.66, 0.42],\n            \"alpha\": 1.00\n        },\n        # Test case 3\n        {\n            \"y\": [1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n            \"s_A\": [0.98, 0.62, 0.61, 0.95, 0.59, 0.60, 0.57, 0.58, 0.55, 0.45, 0.50, 0.49, 0.42, 0.47, 0.43, 0.40, 0.41, 0.37, 0.38, 0.36],\n            \"s_B\": [0.80, 0.60, 0.82, 0.78, 0.58, 0.76, 0.56, 0.74, 0.54, 0.72, 0.52, 0.79, 0.70, 0.50, 0.48, 0.68, 0.46, 0.44, 0.66, 0.42],\n            \"alpha\": 0.00\n        },\n        # Test case 4\n        {\n            \"y\": [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n            \"s_A\": [0.90, 0.80, 0.65, 0.55, 0.53, 0.70, 0.51, 0.49, 0.47, 0.45, 0.43, 0.41, 0.60, 0.39, 0.37],\n            \"s_B\": [0.90, 0.80, 0.65, 0.55, 0.53, 0.70, 0.51, 0.49, 0.47, 0.45, 0.43, 0.41, 0.60, 0.39, 0.37],\n            \"alpha\": 0.20\n        },\n    ]\n\n    results = []\n    TOLERANCE = 1e-9\n\n    for case in test_cases:\n        y, s_A, s_B, alpha = case[\"y\"], case[\"s_A\"], case[\"s_B\"], case[\"alpha\"]\n        \n        pauc_A, tpr_A, auc_A = compute_metrics(y, s_A, alpha)\n        pauc_B, tpr_B, auc_B = compute_metrics(y, s_B, alpha)\n\n        # Apply the deterministic decision rule with tie-breaking logic.\n        # Prefer the model with the larger pAUC(alpha).\n        if pauc_A - pauc_B > TOLERANCE:\n            results.append(0)  # Model A is better\n        elif pauc_B - pauc_A > TOLERANCE:\n            results.append(1)  # Model B is better\n        # If pAUCs are equal, tie-break with TPR at alpha.\n        elif tpr_A - tpr_B > TOLERANCE:\n            results.append(0)\n        elif tpr_B - tpr_A > TOLERANCE:\n            results.append(1)\n        # If TPRs are equal, tie-break with full AUC.\n        elif auc_A - auc_B > TOLERANCE:\n            results.append(0)\n        elif auc_B - auc_A > TOLERANCE:\n            results.append(1)\n        # If all are equal, choose Model A by default.\n        else:\n            results.append(0)\n            \n    # Format and print the final output as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3167178"}]}