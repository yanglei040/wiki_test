{"hands_on_practices": [{"introduction": "The standard \"inverted dropout\" technique includes a crucial scaling step during training. This exercise explores the fundamental reason for this design choice by modeling a hypothetical scenario where this scaling is omitted [@problem_id:3118056]. By analyzing a simple linear network, you will quantify the train-test mismatch that arises, revealing a systematic bias in the model's output and solidifying why scaling is essential for ensuring that the expected output remains consistent between the stochastic training phase and the deterministic inference phase.", "problem": "A single-hidden-unit linear network is trained on a noise-free regression task using dropout as regularization but with an incorrect implementation of inverted dropout (no train-time scaling). The input feature $x$ is a random variable with $\\mathbb{E}[x]=0$ and $\\operatorname{Var}(x)=\\sigma_x^2$, and the true target is $y=\\beta x$. The network output is $\\hat{y}=v\\,(m\\,u\\,x)$, where $u$ and $v$ are weights and $m\\sim\\text{Bernoulli}(p)$ is an independent dropout mask applied to the hidden activation, with retention probability $p$. Normally, inverted dropout scales activations at training to keep their expectation unchanged between training and testing, but here no scaling is used during training. Assume the optimizer has achieved $\\mathbb{E}_m[\\hat{y}\\,|\\,x]=y$ for every fixed $x$ under this incorrect training procedure. At test time, dropout is removed and no scaling is applied (i.e., $m=1$ deterministically).\n\nGiven $p=0.5$, $\\beta=2$, and $\\sigma_x^2=4$, which option correctly gives the test-time Mean Squared Error (MSE) and explains the source of the observed train–test mismatch?\n\nA. Test-time MSE is $16$, caused by an upward bias: the test output is amplified by a factor $1/p$ relative to training-time expectations, so $\\mathbb{E}[\\hat{y}_{\\text{test}}\\,|\\,x]=(\\beta/p)\\,x>\\beta x$.\n\nB. Test-time MSE is $4$, caused purely by variance due to the mask; there is no bias because the expectation matches at test.\n\nC. Test-time MSE is $0$, because matching the training expectation to the target guarantees perfect predictions at test.\n\nD. Test-time MSE is $4$, caused by a downward bias: the test output is attenuated relative to the training-time expectation.", "solution": "The user wants me to validate the problem statement, and if it is valid, derive the test-time Mean Squared Error (MSE) for a single-unit linear network trained with a non-standard dropout implementation.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Network model: A single-hidden-unit linear network.\n-   Task: Noise-free regression.\n-   Input: A random variable $x$ with $\\mathbb{E}[x]=0$ and $\\operatorname{Var}(x)=\\sigma_x^2$.\n-   True target: $y=\\beta x$.\n-   Network output: $\\hat{y}=v\\,(m\\,u\\,x)$, where $u$ and $v$ are scalar weights.\n-   Dropout mask: $m\\sim\\text{Bernoulli}(p)$, an independent random variable with retention probability $p$.\n-   Training procedure: Incorrect inverted dropout; no scaling is used during training.\n-   Training result: The optimizer has achieved $\\mathbb{E}_m[\\hat{y}\\,|\\,x]=y$ for every fixed $x$.\n-   Test procedure: Dropout is removed ($m=1$ deterministically), and no scaling is applied.\n-   Constants: $p=0.5$, $\\beta=2$, $\\sigma_x^2=4$.\n-   Question: Determine the test-time MSE and the source of the train-test mismatch.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Groundedness**: The problem is firmly grounded in the principles of neural networks and regularization techniques, specifically dropout. It describes a plausible, albeit incorrect, implementation scenario to test conceptual understanding. All concepts (MSE, expectation, variance, Bernoulli distribution, bias) are standard in statistics and machine learning.\n-   **Well-Posed**: The problem is well-posed. The training objective is clearly stated, allowing for the determination of the learned parameters' effective value. The test-time conditions are also explicit. A unique, stable, and meaningful solution (the test MSE) can be derived from the given information.\n-   **Objectivity**: The problem is stated in precise, objective mathematical language.\n-   **Completeness and Consistency**: The problem is self-contained. It provides all necessary parameters ($p$, $\\beta$, $\\sigma_x^2$) and conditions to solve for the required quantities. There are no internal contradictions.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. I will proceed with the derivation and solution.\n\n### Derivation\n\n**1. Analyze the Training Phase**\n\nThe network output during training is given by $\\hat{y}_{\\text{train}} = v(m u x)$. The weights $u$ and $v$ are adjusted by an optimizer to satisfy the condition $\\mathbb{E}_m[\\hat{y}_{\\text{train}} \\,|\\, x] = y$ for any given $x$. Let's compute this expectation. The expectation is taken over the random dropout mask $m$.\n\n$$\n\\mathbb{E}_m[\\hat{y}_{\\text{train}} \\,|\\, x] = \\mathbb{E}_m[v m u x \\,|\\, x]\n$$\n\nSince $v$, $u$, and $x$ are treated as constants with respect to the expectation over $m$, we have:\n\n$$\n\\mathbb{E}_m[\\hat{y}_{\\text{train}} \\,|\\, x] = v u x \\, \\mathbb{E}[m]\n$$\n\nThe dropout mask $m$ follows a Bernoulli distribution, $m \\sim \\text{Bernoulli}(p)$. The expectation of a Bernoulli random variable is its success probability, so $\\mathbb{E}[m] = p$.\n\nSubstituting this back, we get:\n\n$$\n\\mathbb{E}_m[\\hat{y}_{\\text{train}} \\,|\\, x] = p v u x\n$$\n\nThe training condition states that this expected output must equal the true target $y = \\beta x$.\n\n$$\np v u x = \\beta x\n$$\n\nFor this equality to hold for all values of $x$, the coefficients of $x$ on both sides must be equal. Let $W = vu$ be the effective weight of the linear network.\n\n$$\np W = \\beta \\implies W = \\frac{\\beta}{p}\n$$\n\nThis is the effective weight that the network learns under this specific (and incorrect) training procedure.\n\n**2. Analyze the Test Phase**\n\nAt test time, dropout is disabled, which means the mask is deterministically set to $m=1$. No scaling is applied. The test-time output, $\\hat{y}_{\\text{test}}$, is:\n\n$$\n\\hat{y}_{\\text{test}} = v (1 \\cdot u x) = v u x = W x\n$$\n\nSubstituting the learned effective weight $W = \\beta/p$:\n\n$$\n\\hat{y}_{\\text{test}} = \\left(\\frac{\\beta}{p}\\right) x\n$$\n\n**3. Calculate the Test-Time Mean Squared Error (MSE)**\n\nThe MSE is the expected squared difference between the test-time prediction and the true target, where the expectation is over the distribution of the input data $x$.\n\n$$\n\\text{MSE} = \\mathbb{E}_x[(\\hat{y}_{\\text{test}} - y)^2]\n$$\n\nSubstitute the expressions for $\\hat{y}_{\\text{test}}$ and $y$:\n\n$$\n\\text{MSE} = \\mathbb{E}_x\\left[ \\left( \\left(\\frac{\\beta}{p}\\right)x - \\beta x \\right)^2 \\right]\n$$\n\nFactor out $\\beta$ and $x$:\n\n$$\n\\text{MSE} = \\mathbb{E}_x\\left[ \\left( \\beta \\left(\\frac{1}{p} - 1\\right) x \\right)^2 \\right] = \\mathbb{E}_x\\left[ \\beta^2 \\left(\\frac{1}{p} - 1\\right)^2 x^2 \\right]\n$$\n\nSince $\\beta$ and $p$ are constants, we can pull them out of the expectation:\n\n$$\n\\text{MSE} = \\beta^2 \\left(\\frac{1}{p} - 1\\right)^2 \\mathbb{E}_x[x^2]\n$$\n\nWe are given that $\\operatorname{Var}(x) = \\sigma_x^2$ and $\\mathbb{E}[x]=0$. The variance is defined as $\\operatorname{Var}(x) = \\mathbb{E}[x^2] - (\\mathbb{E}[x])^2$.\nTherefore, $\\sigma_x^2 = \\mathbb{E}[x^2] - 0^2$, which implies $\\mathbb{E}[x^2] = \\sigma_x^2$.\n\nSubstitute this into the MSE formula:\n\n$$\n\\text{MSE} = \\beta^2 \\left(\\frac{1}{p} - 1\\right)^2 \\sigma_x^2\n$$\n\n**4. Substitute Numerical Values**\n\nWe are given $p=0.5$, $\\beta=2$, and $\\sigma_x^2=4$.\n\n$$\n\\text{MSE} = (2)^2 \\left(\\frac{1}{0.5} - 1\\right)^2 (4)\n$$\n\n$$\n\\text{MSE} = 4 \\left(2 - 1\\right)^2 (4)\n$$\n\n$$\n\\text{MSE} = 4 (1)^2 (4)\n$$\n\n$$\n\\text{MSE} = 16\n$$\n\n**5. Analyze the Source of Error (Train-Test Mismatch)**\n\nDuring training, the network's expected output was matched to the target: $\\mathbb{E}_m[\\hat{y}_{\\text{train}}\\,|\\,x] = y = \\beta x$.\nAt test time, the deterministic output is $\\hat{y}_{\\text{test}} = (\\beta/p)x$.\nSince $p=0.5$, we have $1/p = 2$.\nSo, $\\hat{y}_{\\text{test}} = (\\beta/0.5)x = 2\\beta x$.\nThe true target remains $y = \\beta x$.\n\nThe test-time output is consistently a factor of $1/p = 2$ larger than the true target. This systematic over-prediction is a form of conditional bias, where $\\mathbb{E}[\\hat{y}_{\\text{test}} - y \\,|\\, x] = (\\beta/p)x - \\beta x \\neq 0$. This occurred because the standard inverted dropout practice is to scale the activations by $1/p$ during *training* to ensure the expected activation magnitude remains the same as without dropout. Since this was not done, the network weights $W=vu$ were \"inflated\" by a factor of $1/p$ to compensate. At test time, when the dropout mask is removed, these inflated weights lead to an amplified output. This is an \"upward bias\" or amplification of the output signal.\n\n### Option-by-Option Analysis\n\n**A. Test-time MSE is $16$, caused by an upward bias: the test output is amplified by a factor $1/p$ relative to training-time expectations, so $\\mathbb{E}[\\hat{y}_{\\text{test}}\\,|\\,x]=(\\beta/p)\\,x>\\beta x$.**\n-   The calculated test-time MSE is indeed $16$.\n-   The cause is correctly identified as an upward bias (amplification).\n-   The amplification factor is correctly identified as $1/p$. The test output $\\hat{y}_{\\text{test}} = (\\beta/p)x$ is compared to the training target $y = \\beta x$, which was the training-time expectation.\n-   The relation shown is $\\hat{y}_{\\text{test}} = (\\beta/p)x > \\beta x = y$ (for $x>0$), correctly describing the amplification. The use of $\\mathbb{E}[\\hat{y}_{\\text{test}}\\,|\\,x]$ is equivalent to $\\hat{y}_{\\text{test}}$ since the test output is deterministic for a given $x$.\n-   **Verdict: Correct.**\n\n**B. Test-time MSE is $4$, caused purely by variance due to the mask; there is no bias because the expectation matches at test.**\n-   The MSE is $16$, not $4$.\n-   The error source is a systematic bias, not variance. The dropout mask is not used at test time, so it cannot be a source of variance at test time.\n-   The conditional expectation does not match at test; $\\hat{y}_{\\text{test}} = (\\beta/p)x \\neq \\beta x = y$.\n-   **Verdict: Incorrect.**\n\n**C. Test-time MSE is $0$, because matching the training expectation to the target guarantees perfect predictions at test.**\n-   The MSE is $16$, not $0$.\n-   The reasoning is flawed. Matching the expected output during training does not guarantee correct predictions at test time if the model's behavior changes between training and testing (i.e., due to the removal of the stochastic mask without proper scaling compensation).\n-   **Verdict: Incorrect.**\n\n**D. Test-time MSE is $4$, caused by a downward bias: the test output is attenuated relative to the training-time expectation.**\n-   The MSE is $16$, not $4$.\n-   The bias is upward (amplification), not downward (attenuation), because $p=0.5 < 1$, which makes $1/p = 2 > 1$.\n-   **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3118056"}, {"introduction": "In modern neural networks, dropout is rarely used in isolation; it often coexists with other layers like Batch Normalization (BN). This practice investigates the critical and often misunderstood interaction between these two powerful techniques [@problem_id:3118023]. You will analyze how the relative order of dropout and BN layers affects the statistical properties of the activations, helping you diagnose and prevent subtle bugs caused by distribution shifts between training and inference modes.", "problem": "You are given two otherwise identical deep neural network blocks that differ only in the placement of a dropout layer relative to a Batch Normalization (BN) layer. Batch Normalization (BN) denotes the standard transformation that, during training, normalizes each channel using batch mean and batch variance and, during inference, normalizes using running (exponentially averaged) mean and variance accumulated during training, followed by a learned affine transformation. Dropout uses the common “inverted” implementation during training: each unit is independently kept with probability $p$ and scaled by $1/p$, and during inference dropout is disabled. Consider a generic pre-activation $x$ entering the BN layer in each block, and assume that the dropout mask is independent of $x$. You will measure, for each block and for each relevant tensor location, the empirical layerwise statistics $\\operatorname{E}[x]$ and $\\operatorname{Var}[x]$ on held-out data under training mode and under inference mode, to diagnose distribution shift. The two blocks are:\n- Block A (“dropout before BN”): previous activations $\\to$ dropout with keep probability $p$ and inverted scaling $\\to$ BN $\\to$ remaining layers.\n- Block B (“dropout after BN”): previous activations $\\to$ BN $\\to$ dropout with keep probability $p$ and inverted scaling $\\to$ remaining layers.\nAssume the upstream inputs to these blocks have finite first and second moments and that training uses sufficiently large batches so that BN’s running statistics converge to the corresponding training-time batch statistics. Under these assumptions, which statement best predicts the pattern you will observe when comparing $\\operatorname{E}[x]$ and $\\operatorname{Var}[x]$ measured at the BN input and BN output across training versus inference for Block A versus Block B?\n\nA. In Block A, the BN running variance is inflated by the masking noise seen during training, so at inference the BN input has a smaller variance than the running variance and the BN output variance contracts to approximately $p$ (less than $1$) while the mean remains approximately $0$. In Block B, BN statistics closely match between training and inference at both BN input and BN output, so their means and variances align; any train–inference variance discrepancy appears only after the dropout layer, where training variance is larger by roughly $1/p$.\n\nB. In Block A, BN eliminates any train–inference mismatch, so both the BN input and BN output have exactly matching means and variances between training and inference. In Block B, placing dropout after BN induces a mean shift at the BN output proportional to $(1-p)$ at inference.\n\nC. In Block A, masking before BN biases the BN running mean toward $0$, so at inference the BN output has a nonzero mean mismatch, while its variance matches. In Block B, masking after BN biases the BN running variance upward, so at inference the BN output variance inflates by a factor of $1/p$.\n\nD. Placement does not matter with inverted dropout: for any $p$, the expectation and variance at every layer match between training and inference, so all measured $\\operatorname{E}[x]$ and $\\operatorname{Var}[x]$ coincide at BN inputs and outputs in both blocks.", "solution": "The user wants me to analyze the statistical properties of two neural network architectural blocks involving Batch Normalization (BN) and Dropout, specifically focusing on the distribution shift between training and inference modes.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Block A (“dropout before BN”):** previous activations $\\to$ dropout $\\to$ BN $\\to$ remaining layers.\n-   **Block B (“dropout after BN”):** previous activations $\\to$ BN $\\to$ dropout $\\to$ remaining layers.\n-   **Batch Normalization (BN):**\n    -   Training: Normalizes using batch mean/variance. Followed by a learned affine transformation $z = \\gamma \\hat{x} + \\beta$.\n    -   Inference: Normalizes using running mean/variance accumulated during training. Followed by the same affine transformation.\n-   **Dropout:**\n    -   \"Inverted\" implementation.\n    -   Training: Each unit is kept with probability $p$, scaled by $1/p$. Zeroed out otherwise.\n    -   Inference: Dropout is an identity function (disabled).\n-   **Input:** A generic pre-activation, let's call it $y$, entering the block. The dropout mask is independent of $y$.\n-   **Task:** Compare empirical layerwise statistics $\\operatorname{E}[x]$ and $\\operatorname{Var}[x]$ at BN input and BN output, across training vs. inference modes for both blocks.\n-   **Assumptions:**\n    1.  Upstream inputs $y$ have finite first and second moments. Let $\\operatorname{E}[y] = \\mu_y$ and $\\operatorname{Var}[y] = \\sigma_y^2$.\n    2.  Training uses sufficiently large batches such that BN’s running statistics converge to the corresponding training-time batch statistics. This implies that the running mean $\\mu_{\\text{run}}$ and running variance $\\sigma^2_{\\text{run}}$ stored by BN are equal to the population mean and variance of the tensor entering the BN layer during training.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is well-grounded in the principles of deep learning and statistics. The descriptions of Batch Normalization and inverted Dropout are standard. The interaction between these layers is a known and important practical and theoretical consideration in designing neural network architectures.\n-   **Well-Posed:** The problem is clearly defined. The two blocks, the operations, and the analysis objective are specified unambiguously. The assumption about large batches and converged statistics makes the problem analytically tractable and leads to a unique conclusion.\n-   **Objective:** The problem requires a rigorous, quantitative comparison of statistical moments, which is an objective task.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is a well-posed problem in the domain of deep learning theory. I will proceed with a formal derivation.\n\n### Derivation\n\nLet $y$ be the input to the block, with $\\operatorname{E}[y] = \\mu_y$ and $\\operatorname{Var}[y] = \\sigma_y^2$.\nLet $m$ be a Bernoulli random variable for dropout, $P(m=1) = p$, independent of $y$.\nThe BN layer has learnable parameters $\\gamma$ and $\\beta$. For simplicity and without loss of generality, we will often analyze the normalized output before the affine transformation (where mean is $0$, variance is $1$) and then consider the effect of $\\gamma$ and $\\beta$. The target mean and variance after BN during training are thus $\\beta$ and $\\gamma^2$.\n\n#### Analysis of Block A (Dropout $\\to$ BN)\n\nLet the block be $y \\xrightarrow{\\text{Dropout}} x \\xrightarrow{\\text{BN}} z$. The tensor $x$ is the input to the BN layer.\n\n**1. Training Mode:**\n-   The input to the BN layer is $x_{\\text{train}} = y \\cdot \\frac{m}{p}$.\n-   We compute its statistics, which (by assumption) become the running statistics for BN.\n-   **Mean:** $\\operatorname{E}[x_{\\text{train}}] = \\operatorname{E}[y \\cdot \\frac{m}{p}] = \\operatorname{E}[y]\\operatorname{E}[\\frac{m}{p}] = \\mu_y \\cdot \\frac{\\operatorname{E}[m]}{p} = \\mu_y \\cdot \\frac{p}{p} = \\mu_y$.\n    The running mean is $\\mu_{\\text{run}} = \\mu_y$.\n-   **Variance:** To compute $\\operatorname{Var}[x_{\\text{train}}]$, we first find $\\operatorname{E}[x_{\\text{train}}^2]$.\n    $\\operatorname{E}[x_{\\text{train}}^2] = \\operatorname{E}[(y \\frac{m}{p})^2] = \\operatorname{E}[y^2 \\frac{m^2}{p^2}] = \\operatorname{E}[y^2]\\operatorname{E}[\\frac{m^2}{p^2}]$.\n    Since $m \\in \\{0, 1\\}$, $m^2=m$, so $\\operatorname{E}[m^2]=\\operatorname{E}[m]=p$.\n    $\\operatorname{E}[x_{\\text{train}}^2] = \\operatorname{E}[y^2] \\frac{p}{p^2} = \\frac{1}{p}\\operatorname{E}[y^2] = \\frac{1}{p}(\\sigma_y^2 + \\mu_y^2)$.\n    $\\operatorname{Var}[x_{\\text{train}}] = \\operatorname{E}[x_{\\text{train}}^2] - (\\operatorname{E}[x_{\\text{train}}])^2 = \\frac{1}{p}(\\sigma_y^2 + \\mu_y^2) - \\mu_y^2 = \\frac{\\sigma_y^2}{p} + \\mu_y^2(\\frac{1}{p} - 1) = \\frac{\\sigma_y^2}{p} + \\mu_y^2(\\frac{1-p}{p})$.\n    The running variance is $\\sigma_{\\text{run}}^2 = \\frac{\\sigma_y^2}{p} + \\mu_y^2(\\frac{1-p}{p})$. This variance is \"inflated\" by the dropout noise, as $1/p > 1$.\n-   **BN Output:** The BN output during training $z_{\\text{train}}$ will have $\\operatorname{E}[z_{\\text{train}}] \\approx \\beta$ and $\\operatorname{Var}[z_{\\text{train}}] \\approx \\gamma^2$.\n\n**2. Inference Mode:**\n-   Dropout is disabled. The input to the BN layer is $x_{\\text{inf}} = y$.\n-   **Statistics at BN input:** $\\operatorname{E}[x_{\\text{inf}}] = \\mu_y$ and $\\operatorname{Var}[x_{\\text{inf}}] = \\sigma_y^2$.\n-   **Train vs. Inference at BN input:**\n    -   Mean matches: $\\operatorname{E}[x_{\\text{inf}}] = \\operatorname{E}[x_{\\text{train}}] = \\mu_y$.\n    -   Variance mismatch: $\\operatorname{Var}[x_{\\text{inf}}] = \\sigma_y^2 < \\sigma_{\\text{run}}^2 = \\operatorname{Var}[x_{\\text{train}}]$. The running variance stored by BN is an overestimate of the true inference-time input variance.\n-   **BN Output:** The BN layer normalizes $x_{\\text{inf}}$ using the stored running statistics:\n    $z_{\\text{inf}} = \\gamma \\frac{x_{\\text{inf}} - \\mu_{\\text{run}}}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}} + \\beta = \\gamma \\frac{y - \\mu_y}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}} + \\beta$.\n-   **Statistics of BN Output:**\n    -   Mean: $\\operatorname{E}[z_{\\text{inf}}] = \\frac{\\gamma}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}}\\operatorname{E}[y - \\mu_y] + \\beta = \\beta$. The mean of the output is consistent with training.\n    -   Variance: $\\operatorname{Var}[z_{\\text{inf}}] = \\operatorname{Var}[\\gamma \\frac{y - \\mu_y}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}}] = \\frac{\\gamma^2}{\\sigma_{\\text{run}}^2 + \\epsilon}\\operatorname{Var}[y] = \\frac{\\gamma^2 \\sigma_y^2}{\\sigma_{\\text{run}}^2 + \\epsilon}$.\n    -   Since $\\sigma_{\\text{run}}^2 > \\sigma_y^2$, we have $\\operatorname{Var}[z_{\\text{inf}}] < \\gamma^2 = \\operatorname{Var}[z_{\\text{train}}]$. The output variance at inference is smaller than at training.\n    -   If we assume for simplicity that the input is centered ($\\mu_y=0$), then $\\sigma_{\\text{run}}^2 = \\sigma_y^2/p$. The output variance becomes $\\operatorname{Var}[z_{\\text{inf}}] \\approx \\frac{\\gamma^2 \\sigma_y^2}{\\sigma_y^2/p} = \\gamma^2 p$. The variance is contracted by a factor of $p$.\n\n**Summary for Block A:** A train-inference distribution shift occurs. The BN layer uses an inflated variance estimate from training, causing it to incorrectly normalize the inference data, leading to a contracted variance at its output.\n\n#### Analysis of Block B (BN $\\to$ Dropout)\n\nLet the block be $y \\xrightarrow{\\text{BN}} x \\xrightarrow{\\text{Dropout}} z$. The tensor $x$ is now the output of the BN layer.\n\n**1. Training Mode:**\n-   The input to the BN layer is $y$.\n-   It computes and stores statistics from $y$.\n-   Running mean: $\\mu_{\\text{run}} = \\operatorname{E}[y] = \\mu_y$.\n-   Running variance: $\\sigma_{\\text{run}}^2 = \\operatorname{Var}[y] = \\sigma_y^2$.\n-   **BN Output / Dropout Input:** $x_{\\text{train}}$ will have $\\operatorname{E}[x_{\\text{train}}] \\approx \\beta$ and $\\operatorname{Var}[x_{\\text{train}}] \\approx \\gamma^2$.\n-   **Dropout Output:** The final output is $z_{\\text{train}} = x_{\\text{train}} \\cdot \\frac{m}{p}$. Its variance will be inflated by dropout noise, $\\operatorname{Var}[z_{\\text{train}}] \\approx \\frac{\\gamma^2}{p} + \\beta^2(\\frac{1-p}{p})$.\n\n**2. Inference Mode:**\n-   Dropout is disabled.\n-   **Input to BN:** The input is still $y$. Its statistics are unchanged from training. Thus, at the BN input, there is no train-inference distribution shift.\n-   **BN Output / Dropout Input:** The BN layer normalizes $y$ using $\\mu_{\\text{run}} = \\mu_y$ and $\\sigma_{\\text{run}}^2 = \\sigma_y^2$.\n    $x_{\\text{inf}} = \\gamma \\frac{y - \\mu_{\\text{run}}}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}} + \\beta = \\gamma \\frac{y - \\mu_y}{\\sqrt{\\sigma_y^2 + \\epsilon}} + \\beta$.\n-   **Statistics of BN Output:**\n    -   Mean: $\\operatorname{E}[x_{\\text{inf}}] \\approx \\beta$.\n    -   Variance: $\\operatorname{Var}[x_{\\text{inf}}] \\approx \\gamma^2$.\n-   **Train vs. Inference at BN output:** The statistics of $x$ (BN output) match between training and inference: $\\operatorname{E}[x_{\\text{train}}] \\approx \\operatorname{E}[x_{\\text{inf}}]$ and $\\operatorname{Var}[x_{\\text{train}}] \\approx \\operatorname{Var}[x_{\\text{inf}}]$.\n-   **Dropout Output:** Dropout is an identity map, so $z_{\\text{inf}} = x_{\\text{inf}}$. The final output has $\\operatorname{E}[z_{\\text{inf}}] \\approx \\beta$ and $\\operatorname{Var}[z_{\\text{inf}}] \\approx \\gamma^2$.\n-   The train-inference discrepancy appears only *after* the dropout layer, where $\\operatorname{Var}[z_{\\text{train}}] > \\operatorname{Var}[z_{\\text{inf}}]$.\n\n**Summary for Block B:** There is no distribution shift at the input or output of the BN layer. The BN layer sees the same data distribution in train and test, and its stored statistics are appropriate. The train-inference discrepancy is isolated to the output of the Dropout layer itself.\n\n### Option-by-Option Analysis\n\n**A. In Block A, the BN running variance is inflated by the masking noise seen during training, so at inference the BN input has a smaller variance than the running variance and the BN output variance contracts to approximately $p$ (less than $1$) while the mean remains approximately $0$. In Block B, BN statistics closely match between training and inference at both BN input and BN output, so their means and variances align; any train–inference variance discrepancy appears only after the dropout layer, where training variance is larger by roughly $1/p$.**\n-   The description of Block A is fully consistent with our analysis. The running variance is inflated by noise. This causes the BN output variance to contract at inference time by a factor of approximately $p$. The mean remains stable.\n-   The description of Block B is also fully consistent. The BN layer's input and output statistics are stable between training and inference. The variance mismatch is isolated to the output of the dropout layer, where the training variance is inflated by a factor of roughly $1/p$.\n-   **Verdict: Correct.**\n\n**B. In Block A, BN eliminates any train–inference mismatch, so both the BN input and BN output have exactly matching means and variances between training and inference. In Block B, placing dropout after BN induces a mean shift at the BN output proportional to $(1-p)$ at inference.**\n-   The Block A statement is incorrect. BN does not eliminate the mismatch; it's the source of the mismatch at its output due to using incorrect running statistics. There is a variance mismatch at the BN output.\n-   The Block B statement is incorrect. Inverted dropout preserves the mean. There is no mean shift.\n-   **Verdict: Incorrect.**\n\n**C. In Block A, masking before BN biases the BN running mean toward $0$, so at inference the BN output has a nonzero mean mismatch, while its variance matches. In Block B, masking after BN biases the BN running variance upward, so at inference the BN output variance inflates by a factor of $1/p$.**\n-   The Block A statement is incorrect. Inverted dropout ensures the running mean is an unbiased estimate ($\\operatorname{E}[x_{\\text{train}}] = \\mu_y$). It also incorrectly claims the variance matches, when in fact it contracts.\n-   The Block B statement is incorrect. Masking happens *after* BN, so it cannot bias the BN's running variance. The BN output variance at inference matches the training one, it does not inflate.\n-   **Verdict: Incorrect.**\n\n**D. Placement does not matter with inverted dropout: for any $p$, the expectation and variance at every layer match between training and inference, so all measured $\\operatorname{E}[x]$ and $\\operatorname{Var}[x]$ coincide at BN inputs and outputs in both blocks.**\n-   This statement is patently false. Our analysis shows that placement matters significantly. Block A suffers from a variance mismatch at the BN output, while Block B does not. Inverted dropout preserves the first moment (mean) but not the second moment (variance), and this is the root cause of the observed phenomena.\n-   **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3118023"}, {"introduction": "Once you understand how dropout works, a natural architectural question is *where* in the network to apply it for the greatest effect. This practice explores how the location of a dropout layer can dramatically alter its impact on the network's output [@problem_id:3118042]. By deriving and implementing formulas for output variance, you will see how dropout noise injected at different depths is amplified by the network's weights, developing a deeper intuition for how its regularizing effect is shaped by the signal path of the entire architecture.", "problem": "Consider a feedforward linear network with two hidden layers. Let the input vector be $x \\in \\mathbb{R}^{d}$, and define the first hidden layer pre-activation as $h^{(1)} = W_{1} x$ with $W_{1} \\in \\mathbb{R}^{h_{1} \\times d}$. Define the second hidden layer pre-activation as $h^{(2)} = W_{2} h^{(1)}$ with $W_{2} \\in \\mathbb{R}^{h_{2} \\times h_{1}}$. The scalar output is $\\hat{y} = v^{\\top} h^{(2)}$ with $v \\in \\mathbb{R}^{h_{2}}$. Assume inputs are independent and identically distributed as a zero-mean multivariate Gaussian with identity covariance, that is $x \\sim \\mathcal{N}(0, I_{d})$, and focus on Mean Squared Error (MSE) prediction $\\mathbb{E}[(\\hat{y} - y)^{2}]$ for a fixed teacher function, but only analyze the effect of dropout noise on the student network's output variance.\n\nDuring training, inverted dropout is applied with retention probability $q \\in (0,1]$ (and dropout probability $p = 1 - q$), which scales the kept units by $1/q$ so that the pre-activation expectation is preserved. Consider two locations for applying dropout:\n- Early-layer dropout: apply the dropout mask elementwise to $h^{(1)}$, that is $\\tilde{h}^{(1)} = (m^{(1)}/q) \\odot h^{(1)}$, where $m^{(1)} \\in \\{0,1\\}^{h_{1}}$ has independent $\\text{Bernoulli}(q)$ entries. The rest of the network is unchanged, yielding $\\hat{y}_{\\mathrm{early}} = v^{\\top} W_{2} \\tilde{h}^{(1)}$.\n- Final-layer dropout: apply the dropout mask elementwise to $h^{(2)}$, that is $\\tilde{h}^{(2)} = (m^{(2)}/q) \\odot h^{(2)}$, where $m^{(2)} \\in \\{0,1\\}^{h_{2}}$ has independent $\\text{Bernoulli}(q)$ entries. The output is $\\hat{y}_{\\mathrm{final}} = v^{\\top} \\tilde{h}^{(2)}$.\n\nStarting only from the core definitions above and well-tested facts about Gaussian random vectors, do the following:\n1. Derive an expression for the expected output variance due to dropout noise at the early layer, averaged over both the dropout masks and the input distribution. Denote this quantity by $\\mathrm{Var}_{\\mathrm{early}} = \\mathbb{E}_{x}[\\mathrm{Var}_{m^{(1)}}(\\hat{y}_{\\mathrm{early}} \\mid x)]$.\n2. Derive an expression for the expected output variance due to dropout noise at the final layer, averaged over both the dropout masks and the input distribution. Denote this quantity by $\\mathrm{Var}_{\\mathrm{final}} = \\mathbb{E}_{x}[\\mathrm{Var}_{m^{(2)}}(\\hat{y}_{\\mathrm{final}} \\mid x)]$.\n3. Use these expressions to define a sensitivity metric to dropout location, $S = \\mathrm{Var}_{\\mathrm{early}} / \\mathrm{Var}_{\\mathrm{final}}$, which quantifies how much dropout at the early layer perturbs the output relative to dropout at the final layer. If $\\mathrm{Var}_{\\mathrm{final}} = 0$, define $S$ to be $0$ when $\\mathrm{Var}_{\\mathrm{early}} = 0$, and define $S$ to be $+\\infty$ when $\\mathrm{Var}_{\\mathrm{early}} > 0$.\n\nInterpret the sensitivity via signal amplification: explain how the network weights influence $\\mathrm{Var}_{\\mathrm{early}}$ and $\\mathrm{Var}_{\\mathrm{final}}$ and thus $S$, making clear why certain locations of dropout lead to larger output perturbations.\n\nImplement a program that, for a provided test suite of parameter values, computes and outputs the triple $[\\mathrm{Var}_{\\mathrm{early}}, \\mathrm{Var}_{\\mathrm{final}}, S]$ for each case. All probabilities should be provided as decimals (for example, $p = 0.5$), and there are no physical units in this problem. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element of the list corresponds to one test case and is itself a list of three floats $[\\mathrm{Var}_{\\mathrm{early}}, \\mathrm{Var}_{\\mathrm{final}}, S]$.\n\nUse the following test suite:\n- Case $1$ (general case): $d = 3$, $h_{1} = 2$, $h_{2} = 2$, $p = 0.2$, $W_{1} = \\begin{bmatrix} 0.8 & -0.4 & 0.3 \\\\ 0.1 & 0.5 & -0.6 \\end{bmatrix}$, $W_{2} = \\begin{bmatrix} 1.2 & -0.7 \\\\ 0.5 & 0.9 \\end{bmatrix}$, $v = \\begin{bmatrix} 0.9 \\\\ -0.3 \\end{bmatrix}$.\n- Case $2$ (boundary, no dropout): same $W_{1}$, $W_{2}$, and $v$ as Case $1$, with $p = 0.0$.\n- Case $3$ (amplification in later weights): $d = 3$, $h_{1} = 2$, $h_{2} = 2$, $p = 0.5$, $W_{1} = \\begin{bmatrix} 0.4 & 0.4 & 0.4 \\\\ 0.3 & -0.2 & 0.1 \\end{bmatrix}$, $W_{2} = \\begin{bmatrix} 3.0 & -2.5 \\\\ 2.0 & 1.5 \\end{bmatrix}$, $v = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$.\n- Case $4$ (near-maximal dropout and structural sparsity): $d = 3$, $h_{1} = 2$, $h_{2} = 2$, $p = 0.9$, $W_{1} = \\begin{bmatrix} 0.9 & -0.1 & 0.2 \\\\ 0.0 & 0.3 & -0.4 \\end{bmatrix}$, $W_{2} = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.5 & 0.0 \\end{bmatrix}$, $v = \\begin{bmatrix} 1.0 \\\\ 0.7 \\end{bmatrix}$.\n\nYour program must compute $\\mathrm{Var}_{\\mathrm{early}}$, $\\mathrm{Var}_{\\mathrm{final}}$, and $S$ using the derived formulas, and output a single line containing a list of the $4$ result triples in the exact format: $[[\\mathrm{Var}_{\\mathrm{early}}^{(1)},\\mathrm{Var}_{\\mathrm{final}}^{(1)},S^{(1)}],[\\mathrm{Var}_{\\mathrm{early}}^{(2)},\\mathrm{Var}_{\\mathrm{final}}^{(2)},S^{(2)}],[\\mathrm{Var}_{\\mathrm{early}}^{(3)},\\mathrm{Var}_{\\mathrm{final}}^{(3)},S^{(3)}],[\\mathrm{Var}_{\\mathrm{early}}^{(4)},\\mathrm{Var}_{\\mathrm{final}}^{(4)},S^{(4)}]]$.", "solution": "We begin with the network definitions. The layers are linear: $h^{(1)} = W_{1} x$, $h^{(2)} = W_{2} h^{(1)}$, and the output is $\\hat{y} = v^{\\top} h^{(2)}$. The inverted dropout masks are applied elementwise, with retention probability $q = 1 - p$ and scaling by $1/q$ on the kept units.\n\nFor early-layer dropout, the mask $m^{(1)} \\in \\{0,1\\}^{h_{1}}$ is applied to $h^{(1)}$ to produce $\\tilde{h}^{(1)} = (m^{(1)}/q) \\odot h^{(1)}$. The output becomes\n$$\n\\hat{y}_{\\mathrm{early}} = v^{\\top} W_{2} \\tilde{h}^{(1)} = v^{\\top} W_{2}\\left(\\frac{m^{(1)}}{q} \\odot h^{(1)}\\right).\n$$\nDefine $a = W_{2}^{\\top} v \\in \\mathbb{R}^{h_{1}}$, so that\n$$\n\\hat{y}_{\\mathrm{early}} = a^{\\top}\\left(\\frac{m^{(1)}}{q} \\odot h^{(1)}\\right) = \\sum_{i=1}^{h_{1}} a_{i}\\left(\\frac{m_{i}^{(1)}}{q}\\right) h^{(1)}_{i}.\n$$\nConditioned on $x$, the random variables $\\{m_{i}^{(1)}\\}$ are independent and $\\mathbb{E}[m_{i}^{(1)}] = q$, $\\mathrm{Var}(m_{i}^{(1)}/q) = \\frac{q(1-q)}{q^{2}} = \\frac{1-q}{q} = \\frac{p}{q}$. Therefore, conditioned on $x$,\n$$\n\\mathrm{Var}\\left(\\hat{y}_{\\mathrm{early}} \\mid x\\right) = \\sum_{i=1}^{h_{1}} a_{i}^{2} \\left(h^{(1)}_{i}\\right)^{2} \\cdot \\frac{p}{q}.\n$$\nWe take expectation over the Gaussian input distribution $x \\sim \\mathcal{N}(0, I_{d})$. For the linear mapping $h^{(1)} = W_{1} x$, the $i$-th component satisfies $h^{(1)}_{i} = w_{1,i}^{\\top} x$, where $w_{1,i}^{\\top}$ is the $i$-th row of $W_{1}$. A standard fact for zero-mean Gaussian vectors is\n$$\n\\mathbb{E}\\left[\\left(w^{\\top} x\\right)^{2}\\right] = \\|w\\|_{2}^{2}.\n$$\nTherefore,\n$$\n\\mathbb{E}_{x}\\left[\\left(h^{(1)}_{i}\\right)^{2}\\right] = \\|w_{1,i}\\|_{2}^{2},\n$$\nand hence\n$$\n\\mathrm{Var}_{\\mathrm{early}} = \\mathbb{E}_{x}\\left[\\mathrm{Var}\\left(\\hat{y}_{\\mathrm{early}} \\mid x\\right)\\right] = \\frac{p}{q} \\sum_{i=1}^{h_{1}} a_{i}^{2} \\|w_{1,i}\\|_{2}^{2}.\n$$\nThis can be written compactly as a squared Frobenius norm,\n$$\n\\mathrm{Var}_{\\mathrm{early}} = \\frac{p}{q} \\left\\| \\mathrm{diag}(a) W_{1} \\right\\|_{F}^{2},\n$$\nsince $\\left\\| \\mathrm{diag}(a) W_{1} \\right\\|_{F}^{2} = \\sum_{i=1}^{h_{1}} a_{i}^{2} \\|w_{1,i}\\|_{2}^{2}$.\n\nFor final-layer dropout, the mask $m^{(2)} \\in \\{0,1\\}^{h_{2}}$ is applied to $h^{(2)}$ to produce $\\tilde{h}^{(2)} = (m^{(2)}/q) \\odot h^{(2)}$, yielding\n$$\n\\hat{y}_{\\mathrm{final}} = v^{\\top} \\tilde{h}^{(2)} = \\sum_{j=1}^{h_{2}} v_{j}\\left(\\frac{m_{j}^{(2)}}{q}\\right) h^{(2)}_{j}.\n$$\nConditioned on $x$, the variance accumulates independently across the $h_{2}$ components:\n$$\n\\mathrm{Var}\\left(\\hat{y}_{\\mathrm{final}} \\mid x\\right) = \\sum_{j=1}^{h_{2}} v_{j}^{2} \\left(h^{(2)}_{j}\\right)^{2} \\cdot \\frac{p}{q}.\n$$\nWe now need $\\mathbb{E}_{x}\\left[\\left(h^{(2)}_{j}\\right)^{2}\\right]$. Note $h^{(2)} = W_{2} h^{(1)}$ and $h^{(1)} = W_{1} x$. Write the $j$-th component as $h^{(2)}_{j} = b_{j}^{\\top} h^{(1)}$, where $b_{j}^{\\top}$ is the $j$-th row of $W_{2}$. The random vector $h^{(1)} = W_{1} x$ is zero-mean Gaussian with covariance $W_{1} W_{1}^{\\top}$. For a zero-mean Gaussian vector $\\xi$ with covariance $\\Sigma$, the variance of $b^{\\top} \\xi$ is $b^{\\top} \\Sigma b$. Therefore,\n$$\n\\mathbb{E}_{x}\\left[\\left(h^{(2)}_{j}\\right)^{2}\\right] = b_{j}^{\\top} W_{1} W_{1}^{\\top} b_{j} = \\left\\| W_{1}^{\\top} b_{j} \\right\\|_{2}^{2}.\n$$\nThus,\n$$\n\\mathrm{Var}_{\\mathrm{final}} = \\frac{p}{q} \\sum_{j=1}^{h_{2}} v_{j}^{2} \\left\\| W_{1}^{\\top} b_{j} \\right\\|_{2}^{2}.\n$$\nThis too admits a compact Frobenius form. Let $D_{v} = \\mathrm{diag}(v)$. Observe that\n$$\n\\sum_{j=1}^{h_{2}} v_{j}^{2} \\left\\| W_{1}^{\\top} b_{j} \\right\\|_{2}^{2} = \\left\\| W_{1}^{\\top} W_{2}^{\\top} D_{v} \\right\\|_{F}^{2} = \\left\\| D_{v} W_{2} W_{1} \\right\\|_{F}^{2}.\n$$\nTherefore,\n$$\n\\mathrm{Var}_{\\mathrm{final}} = \\frac{p}{q} \\left\\| \\mathrm{diag}(v) \\, W_{2} \\, W_{1} \\right\\|_{F}^{2}.\n$$\n\nThe sensitivity metric is defined as\n$$\nS = \\frac{\\mathrm{Var}_{\\mathrm{early}}}{\\mathrm{Var}_{\\mathrm{final}}}.\n$$\nBy convention, if $\\mathrm{Var}_{\\mathrm{final}} = 0$ and $\\mathrm{Var}_{\\mathrm{early}} = 0$, then $S = 0$; if $\\mathrm{Var}_{\\mathrm{final}} = 0$ and $\\mathrm{Var}_{\\mathrm{early}} > 0$, then $S = +\\infty$.\n\nInterpretation via signal amplification: Both $\\mathrm{Var}_{\\mathrm{early}}$ and $\\mathrm{Var}_{\\mathrm{final}}$ scale with the factor $\\frac{p}{q}$, reflecting that higher dropout probability $p$ increases the output variance proportionally to the mask variance, while smaller $q$ amplifies due to inverted scaling. The amplification through weights differs by location:\n- Early-layer dropout noise is multiplied by $a = W_{2}^{\\top} v$ before contributing to the output. This yields $\\mathrm{Var}_{\\mathrm{early}} \\propto \\sum_{i} a_{i}^{2} \\|w_{1,i}\\|_{2}^{2}$, showing that noise on $h^{(1)}$ is amplified by the downstream \"fan-out\" weights into the second layer and the readout. Large entries in $W_{2}^{\\top} v$ and large row norms of $W_{1}$ increase this variance.\n- Final-layer dropout noise is applied directly to $h^{(2)}$ and weighted by $v$, yielding $\\mathrm{Var}_{\\mathrm{final}} \\propto \\left\\| \\mathrm{diag}(v) W_{2} W_{1} \\right\\|_{F}^{2}$. This aggregates amplification across the entire forward path from $x$ to each component of $h^{(2)}$ weighted by $v$. Large row norms of $W_{2}$ and the composition $W_{2} W_{1}$, together with large entries in $v$, increase this variance.\n\nConsequently, $S$ is driven by the relative alignment and magnitudes of $W_{2}^{\\top} v$ with the row norms of $W_{1}$ versus the combined path norms in $\\mathrm{diag}(v) W_{2} W_{1}$. If $W_{2}$ has large columns aligned with $v$ (making $a$ large), early-layer dropout noise is strongly amplified, increasing $S$. If the composition $W_{2} W_{1}$ weighted by $v$ has large Frobenius norm, final-layer dropout causes larger variance, decreasing $S$.\n\nAlgorithmic computation:\n1. Compute $q = 1 - p$ and the factor $r = \\frac{p}{q}$.\n2. Compute $a = W_{2}^{\\top} v$.\n3. Compute the squared row norms of $W_{1}$, that is $\\|w_{1,i}\\|_{2}^{2}$ for each $i$.\n4. Compute $\\mathrm{Var}_{\\mathrm{early}} = r \\sum_{i} a_{i}^{2} \\|w_{1,i}\\|_{2}^{2}$.\n5. Compute $M = \\mathrm{diag}(v) W_{2} W_{1}$ and $\\mathrm{Var}_{\\mathrm{final}} = r \\| M \\|_{F}^{2}$.\n6. Compute $S$ using the convention described above.\n7. Output $[\\mathrm{Var}_{\\mathrm{early}}, \\mathrm{Var}_{\\mathrm{final}}, S]$ for each test case as specified.\n\nThe test suite covers a general case with moderate weights, a boundary case with $p = 0$ yielding zero variances, a case with large later-layer weights showing amplification effects, and a near-maximal dropout case with structural sparsity in $W_{2}$ highlighting how certain paths shut off amplification differently by location.", "answer": "```python\nimport numpy as np\n\ndef var_early(W1: np.ndarray, W2: np.ndarray, v: np.ndarray, p: float) -> float:\n    \"\"\"\n    Compute Var_early = (p/q) * sum_i (a_i^2 * ||w1_i||^2), where a = W2^T v.\n    \"\"\"\n    if p == 0.0:\n        return 0.0\n    q = 1.0 - p\n    if q <= 0:\n        # Inverted dropout scaling undefined at q=0; but p in test suite avoids q=0.\n        return float('inf')\n    a = W2.T @ v  # shape (h1,)\n    row_norm_sq = np.sum(W1**2, axis=1)  # shape (h1,)\n    return (p / q) * float(np.sum((a**2) * row_norm_sq))\n\ndef var_final(W1: np.ndarray, W2: np.ndarray, v: np.ndarray, p: float) -> float:\n    \"\"\"\n    Compute Var_final = (p/q) * ||diag(v) @ W2 @ W1||_F^2.\n    \"\"\"\n    if p == 0.0:\n        return 0.0\n    q = 1.0 - p\n    if q <= 0:\n        return float('inf')\n    Dv = np.diag(v)  # shape (h2, h2)\n    M = Dv @ W2 @ W1  # shape (h2, d)\n    frob_sq = float(np.sum(M**2))\n    return (p / q) * frob_sq\n\ndef sensitivity(W1: np.ndarray, W2: np.ndarray, v: np.ndarray, p: float):\n    ve = var_early(W1, W2, v, p)\n    vf = var_final(W1, W2, v, p)\n    # Sensitivity S = ve / vf with conventions.\n    if not np.isfinite(vf):\n        # If vf is inf due to q=0 edge (not present in tests), handle gracefully.\n        S = 0.0 if ve == vf else 0.0\n    elif vf == 0.0:\n        if ve == 0.0:\n            S = 0.0\n        else:\n            S = float('inf')\n    else:\n        S = ve / vf\n    return [ve, vf, S]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"W1\": np.array([[0.8, -0.4, 0.3],\n                            [0.1,  0.5, -0.6]], dtype=float),\n            \"W2\": np.array([[1.2, -0.7],\n                            [0.5,  0.9]], dtype=float),\n            \"v\": np.array([0.9, -0.3], dtype=float),\n            \"p\": 0.2\n        },\n        # Case 2 (boundary p=0.0)\n        {\n            \"W1\": np.array([[0.8, -0.4, 0.3],\n                            [0.1,  0.5, -0.6]], dtype=float),\n            \"W2\": np.array([[1.2, -0.7],\n                            [0.5,  0.9]], dtype=float),\n            \"v\": np.array([0.9, -0.3], dtype=float),\n            \"p\": 0.0\n        },\n        # Case 3 (amplification in later weights)\n        {\n            \"W1\": np.array([[0.4, 0.4, 0.4],\n                            [0.3, -0.2, 0.1]], dtype=float),\n            \"W2\": np.array([[3.0, -2.5],\n                            [2.0,  1.5]], dtype=float),\n            \"v\": np.array([1.0, 1.0], dtype=float),\n            \"p\": 0.5\n        },\n        # Case 4 (near-maximal dropout and structural sparsity)\n        {\n            \"W1\": np.array([[0.9, -0.1, 0.2],\n                            [0.0,  0.3, -0.4]], dtype=float),\n            \"W2\": np.array([[1.0, 0.0],\n                            [0.5, 0.0]], dtype=float),\n            \"v\": np.array([1.0, 0.7], dtype=float),\n            \"p\": 0.9\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        W1 = case[\"W1\"]\n        W2 = case[\"W2\"]\n        v = case[\"v\"]\n        p = case[\"p\"]\n        result = sensitivity(W1, W2, v, p)\n        results.append(result)\n\n    # Final print statement in the exact required format: a single line list of 4 triples.\n    print(f\"[{','.join([str(r) for r in results])}]\")\n\nsolve()\n```", "id": "3118042"}]}