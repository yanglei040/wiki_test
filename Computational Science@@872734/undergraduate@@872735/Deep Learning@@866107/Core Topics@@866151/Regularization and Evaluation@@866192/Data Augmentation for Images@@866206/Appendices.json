{"hands_on_practices": [{"introduction": "CutOut is a simple yet effective regularization technique that forces a model to learn from incomplete data by erasing random patches from an image. But is erasing a patch of blank sky as useful as erasing the subject's face? This practice challenges you to move beyond random augmentation and explore a 'saliency-aware' strategy, where the most important regions of an image are targeted for occlusion. By implementing and comparing these two approaches, you will quantify how strategic data transformation can provide a stronger regularization signal, building an intuition for designing more intelligent augmentation pipelines [@problem_id:3111355].", "problem": "You are asked to write a complete, runnable program that quantifies how the placement of CutOut masks relative to salient regions affects the increase in training loss, which we use as a proxy for regularization strength, for a simple linear model on images. The problem is framed in purely mathematical terms, suitable for implementation in any modern programming language.\n\nConsider a single-image binary regression model with a linear predictor. Let an image be represented as a two-dimensional array of real numbers $x \\in \\mathbb{R}^{H \\times W}$, and let the model weights be $w \\in \\mathbb{R}^{H \\times W}$. Define the prediction as $y = \\langle w, x \\rangle = \\sum_{i=1}^{H}\\sum_{j=1}^{W} w_{ij}\\,x_{ij}$ and the mean squared error (MSE) loss as $L(x; w, t) = (y - t)^2$, where $t \\in \\mathbb{R}$ is the target. A CutOut mask of size $h \\times w$ zeroes out a contiguous rectangular region of the input. Let $m \\in \\{0,1\\}^{H \\times W}$ be a binary mask with a single $h \\times w$ rectangle of ones set to zero and all other entries equal to one, and denote the masked input as $x' = m \\odot x$, where $\\odot$ is the Hadamard (elementwise) product. The masked loss is $L(x'; w, t) = \\big(\\langle w, x' \\rangle - t\\big)^2$.\n\nDefine the saliency map as the absolute value of the gradient of the loss with respect to the input evaluated at the unmasked input, that is $s = \\left| \\nabla_x L(x; w, t) \\right|$. For the MSE loss with a linear model, this simplifies to $s_{ij} = 2\\,\\big| \\langle w, x \\rangle - t \\big|\\,|w_{ij}|$. Therefore, a saliency-aware CutOut placement of size $h \\times w$ is chosen to maximize the sum of $s_{ij}$ over the masked region, which is equivalent to maximizing the sum of $|w_{ij}|$ over the rectangle, because the factor $2\\,\\big| \\langle w, x \\rangle - t \\big|$ is constant across $(i,j)$.\n\nWe measure regularization strength as the increase in loss due to masking. Specifically, define the unmasked loss $L_0 = L(x; w, t)$, the expected masked loss under uniformly random placement of the CutOut $h \\times w$ window (over all valid top-left positions) as $\\mathbb{E}_{\\text{rand}}[L]$, and the masked loss under the saliency-aware placement as $L_{\\text{sal}}$. Define the random-masking increase as $\\Delta_{\\text{rand}} = \\mathbb{E}_{\\text{rand}}[L] - L_0$, and the saliency-aware increase as $\\Delta_{\\text{sal}} = L_{\\text{sal}} - L_0$. Also define the gain and the ratio as $G = \\Delta_{\\text{sal}} - \\Delta_{\\text{rand}}$ and $R = \\Delta_{\\text{sal}} / \\Delta_{\\text{rand}}$ (if the denominator is zero, treat $R$ as positive infinity).\n\nYou must compute these quantities exactly (without Monte Carlo approximation) by enumerating all valid top-left placements for the random case and by selecting the saliency-aware placement defined above. To ensure universal reproducibility, inputs $x$ and $w$ must be generated by a fixed linear congruential generator (LCG) with modulus $m = 2^{32}$, multiplier $a = 1664525$, increment $c = 1013904223$, and seed $s \\in \\{0,1,\\dots,2^{32}-1\\}$. The generator evolves as $u_{k+1} = (a\\,u_k + c) \\bmod m$, and each pseudo-random real value in $[0,1)$ is $r_k = u_k / m$. To construct an $H \\times W$ array from a seed $s$, generate $H \\cdot W$ values $r_k$ in row-major order and map each to $v_k = 2\\,r_k - 1 \\in [-1,1)$.\n\nFor each test case, you are given $H$, $W$, $h$, $w$, seeds $s_x$ for $x$, $s_w$ for $w$, and target $t$. Construct $x$ and $w$ as described above. Compute $L_0$, $\\Delta_{\\text{rand}}$, $\\Delta_{\\text{sal}}$, $G$, and $R$ exactly. The CutOut placements are the set of all top-left indices $(i,j)$ with $i \\in \\{0,1,\\dots,H-h\\}$ and $j \\in \\{0,1,\\dots,W-w\\}$.\n\nImportant mathematical details to use:\n- Empirical Risk Minimization (ERM) and definition of MSE: $L(x; w, t) = (\\langle w, x \\rangle - t)^2$.\n- Saliency by gradient magnitude: $s = \\left| \\nabla_x L \\right|$.\n- Linear model prediction: $\\langle w, x \\rangle = \\sum_{i,j} w_{ij} x_{ij}$.\n- Masked prediction under a rectangle that zeroes a region $R$: $\\langle w, x' \\rangle = \\langle w, x \\rangle - \\sum_{(i,j)\\in R} w_{ij} x_{ij}$.\n- Saliency-aware selection: choose the rectangle maximizing $\\sum_{(i,j)\\in R} |w_{ij}|$.\n\nAngles are not involved. There are no physical units. You must express the final results as real numbers rounded to exactly $6$ decimal places.\n\nTest suite:\nCompute the outputs for the following five test cases:\n1. $H = 8$, $W = 8$, $h = 3$, $w = 3$, $s_x = 12345$, $s_w = 54321$, $t = 1.0$.\n2. $H = 8$, $W = 8$, $h = 1$, $w = 1$, $s_x = 1$, $s_w = 2$, $t = 1.0$.\n3. $H = 8$, $W = 8$, $h = 7$, $w = 7$, $s_x = 987654321$, $s_w = 123456789$, $t = -1.0$.\n4. $H = 6$, $W = 10$, $h = 2$, $w = 5$, $s_x = 555$, $s_w = 777$, $t = 1.0$.\n5. $H = 5$, $W = 5$, $h = 5$, $w = 5$, $s_x = 42$, $s_w = 43$, $t = -1.0$.\n\nRequired final output format:\nYour program should produce a single line of output containing a list of length $5$, where each element is a list corresponding to one test case. For each test case, output a list of four floats: $[\\Delta_{\\text{rand}}, \\Delta_{\\text{sal}}, G, R]$, each rounded to exactly $6$ decimal places. The final output must be a single line of the form\n$[[d_{1r}, d_{1s}, g_1, r_1],[d_{2r}, d_{2s}, g_2, r_2],[d_{3r}, d_{3s}, g_3, r_3],[d_{4r}, d_{4s}, g_4, r_4],[d_{5r}, d_{5s}, g_5, r_5]]$\nwith no spaces added beyond those necessary to separate list elements. If $\\Delta_{\\text{rand}} = 0$, define $R = +\\infty$ for that case before rounding and printing.\n\nYour program must implement the LCG and data generation exactly as specified, compute the exact averages by enumerating all valid placements, and follow the rounding rule. No user input is required. The output must match the specified format exactly.", "solution": "The problem requires us to quantify the effect of different placement strategies for the CutOut data augmentation technique on a simple linear regression model. We will measure this effect by the increase in the mean squared error (MSE) loss, which serves as a proxy for the regularization strength imparted by the augmentation.\n\n### 1. Model and Loss Function\nLet the input image be a matrix $x \\in \\mathbb{R}^{H \\times W}$ and the model weights be a corresponding matrix $w \\in \\mathbb{R}^{H \\times W}$. The model is linear, with the prediction $y$ given by the Frobenius inner product:\n$$\ny = \\langle w, x \\rangle = \\sum_{i=1}^{H}\\sum_{j=1}^{W} w_{ij}\\,x_{ij}\n$$\nGiven a scalar target value $t \\in \\mathbb{R}$, the loss is the Mean Squared Error (MSE):\n$$\nL(x; w, t) = (\\langle w, x \\rangle - t)^2\n$$\n\n### 2. CutOut Augmentation\nCutOut is a data augmentation method that masks a contiguous rectangular region of the input image by setting its pixel values to zero. A CutOut mask of size $h \\times w$ can be represented by a binary matrix $m \\in \\{0,1\\}^{H \\times W}$, where the entries corresponding to the cutout rectangle are $0$ and all other entries are $1$.\nThe augmented input, $x'$, is obtained by the Hadamard (elementwise) product:\n$$\nx' = m \\odot x\n$$\nLet a specific cutout rectangle be denoted by $R$. The prediction on the masked input, $y'$, is:\n$$\ny' = \\langle w, x' \\rangle = \\sum_{(i,j) \\notin R} w_{ij}x_{ij} = \\sum_{i,j} w_{ij}x_{ij} - \\sum_{(i,j) \\in R} w_{ij}x_{ij}\n$$\nLet's define the original prediction as $y_0 = \\langle w, x \\rangle$ and the change in prediction due to the mask as $\\delta_R = \\sum_{(i,j) \\in R} w_{ij}x_{ij}$. Then, the masked prediction is $y' = y_0 - \\delta_R$. The corresponding loss is:\n$$\nL(x'; w, t) = (y' - t)^2 = (y_0 - \\delta_R - t)^2\n$$\n\n### 3. Placement Strategies and Evaluation Metrics\nWe are asked to compare two placement strategies for the CutOut mask: uniformly random and saliency-aware.\n\n**Unmasked Baseline:** The baseline loss without any augmentation is $L_0 = (y_0 - t)^2$.\n\n**Uniformly Random Placement:**\nThis strategy involves choosing the top-left corner of the $h \\times w$ cutout window uniformly at random from all possible valid positions. The set of valid top-left positions is $\\{(i,j) \\mid 0 \\le i \\le H-h, 0 \\le j \\le W-w\\}$. Let the total number of such positions be $N_p = (H-h+1)(W-w+1)$.\nThe expected loss under random placement, $\\mathbb{E}_{\\text{rand}}[L]$, is the average loss over all possible placements:\n$$\n\\mathbb{E}_{\\text{rand}}[L] = \\frac{1}{N_p} \\sum_{\\text{all valid } R} (y_0 - \\delta_R - t)^2\n$$\nThe increase in loss due to random masking is $\\Delta_{\\text{rand}} = \\mathbb{E}_{\\text{rand}}[L] - L_0$.\n\n**Saliency-Aware Placement:**\nThis strategy places the cutout mask over the region deemed most \"important\" or \"salient\". The saliency map is defined as the magnitude of the gradient of the loss with respect to the input: $s = \\left| \\nabla_x L \\right|$.\n$$\n\\frac{\\partial L}{\\partial x_{ij}} = \\frac{\\partial}{\\partial x_{ij}} (\\langle w, x \\rangle - t)^2 = 2(\\langle w, x \\rangle - t) \\frac{\\partial}{\\partial x_{ij}} (\\sum_{k,l} w_{kl}x_{kl}) = 2(y_0 - t)w_{ij}\n$$\nThus, the saliency map is $s_{ij} = |\\frac{\\partial L}{\\partial x_{ij}}| = 2|y_0 - t||w_{ij}|$.\nTo maximize the total saliency within the cutout region, we must find the rectangle $R^*$ that maximizes $\\sum_{(i,j) \\in R^*} s_{ij}$. Since the term $2|y_0 - t|$ is a constant positive scalar for a given unmasked input, this is equivalent to finding the region $R^*$ that maximizes the sum of absolute weight magnitudes:\n$$\nR^* = \\arg\\max_{R} \\sum_{(i,j) \\in R} |w_{ij}|\n$$\nThe loss under this saliency-aware placement is $L_{\\text{sal}} = (y_0 - \\delta_{R^*} - t)^2$.\nThe increase in loss is $\\Delta_{\\text{sal}} = L_{\\text{sal}} - L_0$.\n\n**Comparative Metrics:**\nFinally, we compute two metrics to compare the strategies:\n1.  The gain: $G = \\Delta_{\\text{sal}} - \\Delta_{\\text{rand}}$\n2.  The ratio: $R = \\Delta_{\\text{sal}} / \\Delta_{\\text{rand}}$ (defined as $+\\infty$ if $\\Delta_{\\text{rand}}=0$)\n\n### 4. Computational Algorithm\nA direct implementation of the summations over all sliding windows would be inefficient. We can significantly optimize the computation using summed-area tables, also known as integral images.\n\n**Data Generation:** The input arrays $x$ and $w$ are generated using a specific Linear Congruential Generator (LCG): $u_{k+1} = (a u_k + c) \\pmod m$, with $m = 2^{32}$, $a = 1664525$, and $c = 1013904223$. The initial state $u_0$ is the given seed $s$. Each pseudo-random integer $u_k$ is converted to a real number $r_k = u_k/m \\in [0,1)$, which is then mapped to $v_k = 2r_k - 1 \\in [-1,1)$. These values populate the arrays in row-major order.\n\n**Efficient Summation using Integral Images:**\nAn integral image $I_A$ of an array $A$ is an array where $I_A(i,j)$ stores the sum of all elements in $A$ in the rectangle from the origin $(0,0)$ to $(i,j)$. This allows the sum over any arbitrary rectangle to be computed in $O(1)$ time using four lookups. We will pre-compute two integral images:\n1.  $I_{w \\odot x}$ for the array of elementwise products $w_{ij}x_{ij}$. This is used to rapidly calculate $\\delta_R$ for any region $R$.\n2.  $I_{|w|}$ for the array of absolute weights $|w_{ij}|$. This is used to find the saliency-aware region $R^*$ by quickly calculating $\\sum_{(i,j) \\in R} |w_{ij}|$ for all candidate regions.\n\n**Step-by-Step Calculation:**\n1.  For each test case, generate the matrices $x$ and $w$ of size $H \\times W$ using the specified LCG and seeds.\n2.  Calculate the unmasked prediction $y_0 = \\sum_{i,j} w_{ij}x_{ij}$ and the unmasked loss $L_0 = (y_0 - t)^2$.\n3.  Compute the integral images $I_{w \\odot x}$ and $I_{|w|}$.\n4.  Initialize a variable for total loss, `total_loss_sum = 0`, and variables to track the best saliency placement, `max_sal_sum = -1` and `best_placement = None`.\n5.  Iterate through all $N_p$ valid top-left positions $(i,j)$ for the $h \\times w$ window. For each position:\n    a. Use $I_{|w|}$ to find the sum of absolute weights in the current window. If this sum is greater than `max_sal_sum`, update `max_sal_sum` and store the current position in `best_placement`.\n    b. Use $I_{w \\odot x}$ to find the sum $\\delta_{ij}$ for the current window.\n    c. Calculate the masked loss $L_{ij} = (y_0 - \\delta_{ij} - t)^2$.\n    d. Add $L_{ij}$ to `total_loss_sum`.\n6.  After the loop, calculate the expected random loss $\\mathbb{E}_{\\text{rand}}[L] = \\text{total_loss_sum} / N_p$. Then compute $\\Delta_{\\text{rand}} = \\mathbb{E}_{\\text{rand}}[L] - L_0$.\n7.  Using the stored `best_placement`, use $I_{w \\odot x}$ to find the corresponding $\\delta_{R^*}$.\n8.  Calculate the saliency-aware loss $L_{\\text{sal}} = (y_0 - \\delta_{R^*} - t)^2$. Then compute $\\Delta_{\\text{sal}} = L_{\\text{sal}} - L_0$.\n9.  Calculate the final metrics $G = \\Delta_{\\text{sal}} - \\Delta_{\\text{rand}}$ and $R = \\Delta_{\\text{sal}} / \\Delta_{\\text{rand}}$. Handle the case where $\\Delta_{\\text{rand}}$ is zero (or numerically close to it).\n10. Round the four resulting metrics ($\\Delta_{\\text{rand}}$, $\\Delta_{\\text{sal}}$, $G$, $R$) to $6$ decimal places and store them.\nThis procedure ensures an exact calculation as required, without resorting to Monte Carlo approximation, while remaining computationally efficient.", "answer": "```python\nimport numpy as np\n# No other libraries are imported, as scipy is not strictly needed.\n# numpy provides sufficient functionality for this problem.\n\n# LCG parameters from the problem description.\nLCG_M = 2**32\nLCG_A = 1664525\nLCG_C = 1013904223\n\ndef generate_array(seed, H, W):\n    \"\"\"\n    Generates an HxW numpy array using the specified LCG.\n    Values are mapped to the range [-1, 1).\n    \"\"\"\n    n_values = H * W\n    u = seed\n    values = []\n    for _ in range(n_values):\n        u = (LCG_A * u + LCG_C)  (LCG_M - 1)\n        # Convert to float in [0, 1) then to [-1, 1)\n        v = 2 * (u / LCG_M) - 1\n        values.append(v)\n    \n    return np.array(values, dtype=np.float64).reshape((H, W))\n\ndef integral_image(arr):\n    \"\"\"\n    Computes the summed-area table (integral image) of a 2D array.\n    \"\"\"\n    S = np.zeros((arr.shape[0] + 1, arr.shape[1] + 1), dtype=np.float64)\n    S[1:, 1:] = np.cumsum(np.cumsum(arr, axis=0), axis=1)\n    return S\n\ndef sum_rect(integral_img, r, c, h, w):\n    \"\"\"\n    Calculates the sum over a rectangle using the integral image.\n    r, c are 0-indexed top-left corner coordinates.\n    \"\"\"\n    r1, c1 = r, c\n    r2, c2 = r + h, c + w\n    # The integral_img is 1-padded, so coordinates map directly.\n    return integral_img[r2, c2] - integral_img[r1, c2] - integral_img[r2, c1] + integral_img[r1, c1]\n\ndef compute_metrics(H, W, h, w_mask, s_x, s_w, t):\n    \"\"\"\n    Computes all required metrics for a single test case.\n    \"\"\"\n    # 1. Generate data\n    x = generate_array(s_x, H, W)\n    weights = generate_array(s_w, H, W)\n\n    # 2. Calculate unmasked baseline\n    y0 = np.sum(weights * x)\n    L0 = (y0 - t)**2\n\n    # 3. Pre-compute for efficient summation\n    integral_wx = integral_image(weights * x)\n    integral_abs_w = integral_image(np.abs(weights))\n    \n    num_placements = (H - h + 1) * (W - w_mask + 1)\n    \n    total_masked_loss = 0.0\n    \n    max_sal_sum = -1.0\n    best_placement_ij = (0, 0)\n    \n    # 4. Iterate over all possible placements\n    for i in range(H - h + 1):\n        for j in range(W - w_mask + 1):\n            # Saliency-aware placement search\n            current_sal_sum = sum_rect(integral_abs_w, i, j, h, w_mask)\n            if current_sal_sum > max_sal_sum:\n                max_sal_sum = current_sal_sum\n                best_placement_ij = (i, j)\n\n            # Random placement calculation\n            delta_ij = sum_rect(integral_wx, i, j, h, w_mask)\n            loss_ij = (y0 - delta_ij - t)**2\n            total_masked_loss += loss_ij\n            \n    # 5. Calculate random masking metrics\n    E_rand_L = total_masked_loss / num_placements\n    delta_rand = E_rand_L - L0\n\n    # 6. Calculate saliency-aware masking metrics\n    i_star, j_star = best_placement_ij\n    delta_sal = sum_rect(integral_wx, i_star, j_star, h, w_mask)\n    L_sal = (y0 - delta_sal - t)**2\n    delta_sal_val = L_sal - L0\n    \n    # 7. Calculate final comparative metrics\n    G = delta_sal_val - delta_rand\n    \n    if abs(delta_rand)  1e-12: # Treat as zero\n        R = np.inf\n    else:\n        R = delta_sal_val / delta_rand\n\n    return [delta_rand, delta_sal_val, G, R]\n    \ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # (H, W, h, w_mask, s_x, s_w, t)\n        (8, 8, 3, 3, 12345, 54321, 1.0),\n        (8, 8, 1, 1, 1, 2, 1.0),\n        (8, 8, 7, 7, 987654321, 123456789, -1.0),\n        (6, 10, 2, 5, 555, 777, 1.0),\n        (5, 5, 5, 5, 42, 43, -1.0),\n    ]\n\n    all_results = []\n    \n    def format_float(val):\n        \"\"\"Formats floats to .6f, handles infinity.\"\"\"\n        if val == np.inf:\n            return 'inf'\n        return f'{val:.6f}'\n\n    for case in test_cases:\n        H, W, h, w_mask, s_x, s_w, t = case\n        result = compute_metrics(H, W, h, w_mask, s_x, s_w, t)\n        all_results.append(result)\n\n    # Format the output string as per problem specification.\n    # e.g., [[d1,d2,g1,r1],[d3,d4,g2,r2]]\n    result_strings = []\n    for res in all_results:\n        formatted_res = [format_float(v) for v in res]\n        result_strings.append(f\"[{','.join(formatted_res)}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3111355"}, {"introduction": "Unlike traditional augmentations that transform a single image, Mixup creates new virtual samples by linearly interpolating between two images and their labels. While this method is a powerful regularizer, its assumption of linearity can be a double-edged sword. This exercise will guide you through a simulation to investigate a key side-effect: how Mixup can encourage overly linear decision boundaries, which may be harmful for classification problems where classes are separated by complex, curved surfaces. By exploring the role of the Mixup hyperparameter $\\alpha$, you will gain a deeper geometric understanding of how this technique shapes the learning process [@problem_id:3111279].", "problem": "You are given a synthetic, purely mathematical model of two fine-grained image classes whose Bayes-optimal decision boundary is highly curved. The goal is to investigate when Mixup augmentation with parameter $\\alpha$ produces harmful label interpolation that encourages overly linear decision boundaries. You must write a complete, runnable program that computes, by Monte Carlo simulation, the range of $\\alpha$ values where this harmful effect occurs according to a principled criterion derived from first principles and well-tested facts.\n\nThe setting is as follows. Consider two classes in $\\mathbb{R}^2$ defined by points on two concentric circles. Class $\\mathcal{C}_0$ is the inner circle of radius $r_0$, and class $\\mathcal{C}_1$ is the outer circle of radius $r_1$. The Bayes-optimal classifier for these two classes is the indicator of the radial threshold $R$, that is, a point $x \\in \\mathbb{R}^2$ is classified as $\\mathcal{C}_0$ if $\\|x\\|  R$ and as $\\mathcal{C}_1$ if $\\|x\\| \\ge R$. This decision boundary is a circle and is therefore highly curved. Mixup generates augmented samples by convex combination: given points $x_i \\in \\mathcal{C}_0$, $x_j \\in \\mathcal{C}_1$, a mixing coefficient $\\lambda \\sim \\operatorname{Beta}(\\alpha,\\alpha)$, and one-hot labels $y_i = [1,0]$, $y_j = [0,1]$, the augmented point is $x_{\\text{mix}} = \\lambda x_i + (1 - \\lambda) x_j$ and the augmented soft label is $y_{\\text{mix}} = \\lambda [1,0] + (1 - \\lambda) [0,1]$. For fine-grained classes with curved boundaries, Mixup can apply ambiguous labels near $0.5$ to spatial locations that are far from the Bayes decision boundary, pressuring the learned decision surface toward linearity along chords instead of respecting curvature.\n\nYou must design a quantitative criterion for when Mixup is \"harmfully linearizing.\" Use the following definitions, parameters, and units:\n\n- The inner and outer radii are $r_0 = 1.0$ and $r_1 = 2.0$, respectively.\n- The Bayes decision threshold radius is $R = 1.5$.\n- Angles are sampled uniformly in radians over $[0, 2\\pi)$.\n- A Mixup sample is considered \"deep\" relative to the Bayes boundary if $|\\|x_{\\text{mix}}\\| - R| \\ge m$, with margin $m = 0.2$. A \"deep\" sample is confidently in one class region spatially, not near the boundary.\n- A Mixup label is \"ambiguous\" if $|\\lambda - 0.5| \\le \\delta$, with $\\delta = 0.1$, meaning the soft label is close to a half-half interpolation.\n- Define the harmfulness score $H(\\alpha)$ as the fraction of augmented samples that are both \"deep\" and have \"ambiguous\" labels. Intuitively, a large $H(\\alpha)$ indicates that Mixup is frequently assigning near-$0.5$ labels to points that are clearly in one class region, which encourages overly linear decision boundaries across curved regions.\n- Declare Mixup to be \"overly linear\" at a given $\\alpha$ if $H(\\alpha) \\ge \\tau$, with threshold $\\tau = 0.15$.\n\nYour program must do the following:\n\n1. Fix the random seed for reproducibility. Generate $N = 10000$ random pairs $(x_i, x_j)$ with $x_i$ on the inner circle of radius $r_0$ and $x_j$ on the outer circle of radius $r_1$, each constructed from independent angles sampled uniformly in radians from $[0, 2\\pi)$ as $x = r[\\cos(\\theta), \\sin(\\theta)]$.\n\n2. For a grid of $\\alpha$ values $\\mathcal{A}$ consisting of $M_{\\alpha} = 41$ linearly spaced points from $\\alpha_{\\text{min}} = 0.1$ to $\\alpha_{\\text{max}} = 64.0$, estimate $H(\\alpha)$ by:\n   - Sampling $\\lambda_k \\sim \\operatorname{Beta}(\\alpha, \\alpha)$ independently for each of the $N$ pairs.\n   - Forming $x_{\\text{mix},k} = \\lambda_k x_{i,k} + (1 - \\lambda_k) x_{j,k}$.\n   - Computing the fraction of indices $k$ for which both $|\\|x_{\\text{mix},k}\\| - R| \\ge m$ and $|\\lambda_k - 0.5| \\le \\delta$ hold.\n\n3. Identify the contiguous interval $[\\alpha_{\\mathrm{low}}, \\alpha_{\\mathrm{high}}]$ within the grid $\\mathcal{A}$ for which $H(\\alpha) \\ge \\tau$. If no grid points satisfy $H(\\alpha) \\ge \\tau$, then define $\\alpha_{\\mathrm{low}} = 0.0$ and $\\alpha_{\\mathrm{high}} = 0.0$.\n\n4. Evaluate the condition $H(\\alpha) \\ge \\tau$ for the following test suite of $\\alpha$ values: $\\alpha \\in \\{0.2, 0.5, 1.0, 2.0, 8.0, 64.0\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The first two elements must be the interval endpoints $\\alpha_{\\mathrm{low}}$ and $\\alpha_{\\mathrm{high}}$ rounded to $3$ decimals, followed by the boolean results for each of the test suite $\\alpha$ values in the order given. For example, the output format must be exactly of the form $[\\alpha_{\\mathrm{low}},\\alpha_{\\mathrm{high}},b_1,b_2,b_3,b_4,b_5,b_6]$, where each $b_i$ is either $\\text{True}$ or $\\text{False}$.\n\nAll angles must be in radians. No physical units are involved. The threshold $\\tau$ must be treated as a decimal (not a percentage).\n\nThe problem is designed to be testable and includes the following cases for coverage:\n- A general case: $\\alpha = 2.0$.\n- Boundary-like cases: $\\alpha = 1.0$ and $\\alpha = 0.5$.\n- Edge cases: very small $\\alpha = 0.2$ and very large $\\alpha = 64.0$.\n- A high-but-not-extreme case: $\\alpha = 8.0$.\n\nYour program must be self-contained, require no user input, and strictly follow the specified output format.", "solution": "The problem requires a Monte Carlo simulation to determine the range of the Mixup parameter $\\alpha$ for which the augmentation has a \"harmfully linearizing\" effect on a synthetic dataset with a curved decision boundary. The solution involves formalizing the problem's criteria, implementing the simulation, and analyzing the results.\n\n### 1. The Geometric Model and Bayes-Optimal Classifier\n\nThe problem defines a two-class classification task in $\\mathbb{R}^2$. Class $\\mathcal{C}_0$ consists of points on a circle of radius $r_0 = 1.0$, and class $\\mathcal{C}_1$ consists of points on a concentric circle of radius $r_1 = 2.0$. The Bayes-optimal decision boundary, which minimizes the classification error, separates these two classes. For this concentric circle model, the optimal boundary is a circle with a radius $R$ somewhere between $r_0$ and $r_1$. The problem specifies this Bayes decision threshold radius as $R = 1.5$. A point $x \\in \\mathbb{R}^2$ is classified as $\\mathcal{C}_0$ if its Euclidean norm $\\|x\\|  R$ and as $\\mathcal{C}_1$ if $\\|x\\| \\ge R$. This boundary is \"highly curved,\" and an ideal classifier should learn this circular separation.\n\nPoints for the simulation are generated by sampling an angle $\\theta$ uniformly from $[0, 2\\pi)$ and using the polar-to-Cartesian transformation:\n-   For class $\\mathcal{C}_0$: $x_i = [r_0 \\cos(\\theta_i), r_0 \\sin(\\theta_i)]$\n-   For class $\\mathcal{C}_1$: $x_j = [r_1 \\cos(\\theta_j), r_1 \\sin(\\theta_j)]$\n\n### 2. Mixup Augmentation\n\nMixup generates new training samples by taking convex combinations of pairs of existing samples. Given a point $x_i$ from class $\\mathcal{C}_0$ and a point $x_j$ from class $\\mathcal{C}_1$, with corresponding one-hot labels $y_i = [1, 0]$ and $y_j = [0, 1]$, a new sample $(x_{\\text{mix}}, y_{\\text{mix}})$ is created as follows:\n$$\n\\lambda \\sim \\operatorname{Beta}(\\alpha, \\alpha)\n$$\n$$\nx_{\\text{mix}} = \\lambda x_i + (1 - \\lambda) x_j\n$$\n$$\ny_{\\text{mix}} = \\lambda y_i + (1 - \\lambda) y_j = [\\lambda, 1-\\lambda]\n$$\nThe parameter $\\alpha$ controls the distribution of the mixing coefficient $\\lambda$. For $\\alpha \\to \\infty$, $\\lambda$ concentrates sharply around $0.5$. For $\\alpha \\to 0$, $\\lambda$ concentrates at the endpoints $0$ and $1$. For $\\alpha=1$, $\\lambda$ is uniformly distributed on $[0,1]$.\n\nThe geometric interpretation is that $x_{\\text{mix}}$ lies on the line segment (the chord) connecting $x_i$ and $x_j$. The issue arises because this linear interpolation in space is paired with a linear interpolation of labels. For a curved boundary, a point spatially far from the boundary might be assigned a highly ambiguous label (e.g., $[0.5, 0.5]$), encouraging the model to learn a linear decision boundary along the chord rather than respecting the true curvature.\n\n### 3. Quantitative Criterion for \"Harmful Linearization\"\n\nThe problem provides a precise, quantitative method to identify this harmful effect. It defines two conditions:\n\n1.  **\"Ambiguous\" Label**: The interpolated label $y_{\\text{mix}} = [\\lambda, 1-\\lambda]$ is considered ambiguous if its components are close to $0.5$. This is formalized as $|\\lambda - 0.5| \\le \\delta$, where $\\delta = 0.1$. This corresponds to $\\lambda \\in [0.4, 0.6]$.\n\n2.  **\"Deep\" Sample**: The generated point $x_{\\text{mix}}$ is \"deep\" with respect to the Bayes boundary if it is spatially far from it. This is formalized as $|\\|x_{\\text{mix}}\\| - R| \\ge m$, where the margin is $m = 0.2$. This means the point is confidently inside one of the class regions, not in the ambiguous zone near the boundary circle of radius $R=1.5$.\n\nThe **harmfulness score $H(\\alpha)$** is defined as the joint probability of these two events occurring: the fraction of Mixup samples that are simultaneously \"deep\" spatially and have \"ambiguous\" labels. A high $H(\\alpha)$ indicates that Mixup is frequently generating confusing signals for the classifier.\n\nMixup is deemed **\"overly linear\"** if this score exceeds a threshold: $H(\\alpha) \\ge \\tau$, with $\\tau = 0.15$.\n\n### 4. Monte Carlo Simulation Design\n\nWe estimate $H(\\alpha)$ using a Monte Carlo simulation. The procedure is as follows:\n\n1.  **Fix Randomness**: Set a global random seed for reproducibility.\n2.  **Generate Base Geometry**: Generate a fixed set of $N = 10000$ pairs of points $(x_{i,k}, x_{j,k})$ for $k=1, \\dots, N$. The angles $\\theta_{i,k}$ and $\\theta_{j,k}$ are sampled independently and uniformly from $[0, 2\\pi)$. This set of geometric configurations remains constant throughout the simulation to reduce variance.\n3.  **Iterate over $\\alpha$**: For each value of $\\alpha$ in the specified grid and test suite:\n    a.  **Sample Mixing Coefficients**: Draw $N$ independent samples $\\lambda_k \\sim \\operatorname{Beta}(\\alpha, \\alpha)$.\n    b.  **Create Mixup Samples**: Compute $x_{\\text{mix},k} = \\lambda_k x_{i,k} + (1 - \\lambda_k) x_{j,k}$ for each $k$.\n    c.  **Evaluate Conditions**: For each generated sample $k$, check if the \"ambiguous\" and \"deep\" conditions are met:\n        -   $C_{\\text{ambiguous}, k}: |\\lambda_k - 0.5| \\le \\delta$\n        -   $C_{\\text{deep}, k}: |\\|x_{\\text{mix},k}\\| - R| \\ge m$\n    d.  **Estimate $H(\\alpha)$**: The score is the empirical fraction of samples where both conditions are true:\n        $$\n        H(\\alpha) \\approx \\frac{1}{N} \\sum_{k=1}^{N} \\mathbb{I}(C_{\\text{ambiguous}, k} \\land C_{\\text{deep}, k})\n        $$\n        where $\\mathbb{I}(\\cdot)$ is the indicator function.\n\n### 5. Algorithmic Implementation and Analysis\n\nThe simulation is implemented in Python using the `numpy` library for efficient, vectorized operations.\n\n-   A helper function `calculate_H` encapsulates the logic for computing $H(\\alpha)$ for a given $\\alpha$, using the pre-generated geometric pairs.\n-   This function is first called for each $\\alpha$ on a linearly spaced grid from $\\alpha_{\\text{min}}=0.1$ to $\\alpha_{\\text{max}}=64.0$.\n-   The resulting array of $H(\\alpha)$ values is compared against the threshold $\\tau=0.15$. The minimum and maximum $\\alpha$ from the grid that satisfy $H(\\alpha) \\ge \\tau$ define the interval $[\\alpha_{\\mathrm{low}}, \\alpha_{\\mathrm{high}}]$. If no values satisfy the condition, the interval defaults to $[0.0, 0.0]$.\n-   The `calculate_H` function is then called for each $\\alpha$ in the specific test suite $\\{0.2, 0.5, 1.0, 2.0, 8.0, 64.0\\}$ to determine the boolean outcome $H(\\alpha) \\ge \\tau$ for each case.\n-   Finally, the results are formatted into the required string output, with floating-point numbers rounded to three decimal places.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the range of Mixup alpha values that cause \"harmful linearization\"\n    and evaluates this condition for a specific test suite.\n    \"\"\"\n    # 1. Define constants and set random seed for reproducibility\n    SEED = 42\n    np.random.seed(SEED)\n\n    r0 = 1.0\n    r1 = 2.0\n    R = 1.5\n    m = 0.2\n    delta = 0.1\n    tau = 0.15\n\n    N = 10000\n    M_alpha = 41\n    alpha_min = 0.1\n    alpha_max = 64.0\n\n    # 2. Generate N random pairs of points (xi, xj)\n    # This geometric data is fixed for all subsequent alpha evaluations\n    theta_i = np.random.uniform(0.0, 2.0 * np.pi, N)\n    theta_j = np.random.uniform(0.0, 2.0 * np.pi, N)\n    \n    xi = r0 * np.column_stack([np.cos(theta_i), np.sin(theta_i)])\n    xj = r1 * np.column_stack([np.cos(theta_j), np.sin(theta_j)])\n\n    def calculate_H(alpha, n_samples, xi_data, xj_data):\n        \"\"\"\n        Calculates the harmfulness score H(alpha) for a given alpha.\n        \"\"\"\n        if alpha = 0:\n            return 0.0\n            \n        # Sample N mixing coefficients from the Beta(alpha, alpha) distribution\n        lambdas = np.random.beta(alpha, alpha, n_samples)\n        \n        # Form the mixed points\n        # Reshape lambdas for broadcasting: (n_samples,) -> (n_samples, 1)\n        x_mix = lambdas[:, np.newaxis] * xi_data + (1 - lambdas)[:, np.newaxis] * xj_data\n        \n        # Calculate the Euclidean norms of the mixed points\n        x_mix_norm = np.linalg.norm(x_mix, axis=1)\n        \n        # Check the \"deep\" and \"ambiguous\" conditions\n        is_deep = np.abs(x_mix_norm - R) >= m\n        is_ambiguous = np.abs(lambdas - 0.5) = delta\n        \n        # The harmfulness score is the fraction of samples meeting both criteria\n        H = np.mean(is_deep  is_ambiguous)\n        return H\n\n    # 3. Compute H(alpha) over the specified grid of alpha values\n    alpha_grid = np.linspace(alpha_min, alpha_max, M_alpha)\n    H_values = np.array([calculate_H(alpha, N, xi, xj) for alpha in alpha_grid])\n\n    # 4. Identify the interval [alpha_low, alpha_high]\n    is_overly_linear = H_values >= tau\n    harmful_alphas = alpha_grid[is_overly_linear]\n    \n    if harmful_alphas.size > 0:\n        alpha_low = np.min(harmful_alphas)\n        alpha_high = np.max(harmful_alphas)\n    else:\n        alpha_low = 0.0\n        alpha_high = 0.0\n\n    # 5. Evaluate the condition for the test suite of alpha values\n    test_suite = [0.2, 0.5, 1.0, 2.0, 8.0, 64.0]\n    test_results = []\n    for alpha_test in test_suite:\n        H_test = calculate_H(alpha_test, N, xi, xj)\n        test_results.append(H_test >= tau)\n\n    # 6. Format the final output string\n    # Format: [alpha_low, alpha_high, b1, b2, b3, b4, b5, b6]\n    output_list = [f\"{alpha_low:.3f}\", f\"{alpha_high:.3f}\"] + [str(b) for b in test_results]\n    print(f\"[{','.join(output_list)}]\")\n\nsolve()\n```", "id": "3111279"}, {"introduction": "CutMix can be seen as the best of both worlds, combining the regional dropout of CutOut with the label-mixing strategy of Mixup. It encourages models to use information from across the entire image rather than over-relying on small, discriminative features. However, what happens when the most important feature *is* the global arrangement of the image content? This hands-on problem challenges you to design a synthetic dataset where class identity is defined by global context, and then use it to test whether the patch-swapping mechanism of CutMix disrupts the model's ability to learn this crucial information. This practice will reinforce the critical lesson that there is no universally best augmentation; the optimal choice always depends on the nature of your data and your task [@problem_id:3151909].", "problem": "You must write a complete, runnable program that constructs a synthetic image dataset in which class identity is determined by global spatial arrangement, trains a multinomial logistic regression model under different augmentation configurations, and evaluates whether the augmentations disrupt the model’s ability to learn global context. Base your approach on empirical risk minimization with cross-entropy under the softmax model and on formal definitions of the augmentations. The dataset and learning problem must be purely mathematical and logically defined without any dependence on external files. Your program must implement all steps from scratch using linear algebra operations.\n\nDataset design: Create grayscale images of size $H \\times W$ with intensity values in $[0,1]$. There are exactly two classes, encoded with one-hot vectors in $\\{[1,0],[0,1]\\}$. Each image is partitioned into four equal quadrants. Let $c_{\\mathrm{lo}} \\in (0,1)$ and $c_{\\mathrm{hi}} \\in (0,1)$ with $c_{\\mathrm{lo}}  c_{\\mathrm{hi}}$. Define the class-conditional generation rule as follows:\n- For class $0$: the top-left and bottom-right quadrants are filled with $c_{\\mathrm{hi}}$, and the top-right and bottom-left quadrants are filled with $c_{\\mathrm{lo}}$.\n- For class $1$: the top-left and bottom-right quadrants are filled with $c_{\\mathrm{lo}}$, and the top-right and bottom-left quadrants are filled with $c_{\\mathrm{hi}}$.\nAfter this deterministic construction, add independent Gaussian noise $\\mathcal{N}(0,\\sigma^{2})$ to each pixel and clip to $[0,1]$. This construction ensures the classes are distinguished only by the global arrangement of quadrants, not by local textures.\n\nModel and learning objective: Use multinomial logistic regression (softmax regression) with two classes. For an input vector $x \\in \\mathbb{R}^{D}$ with $D = H \\cdot W$, model the class probabilities as\n$$\np_{\\theta}(y=k \\mid x) = \\frac{\\exp(w_{k}^{\\top} x + b_{k})}{\\sum_{j=0}^{1} \\exp(w_{j}^{\\top} x + b_{j})}, \\quad k \\in \\{0,1\\},\n$$\nwith parameters $\\theta = \\{W,b\\}$, where $W \\in \\mathbb{R}^{D \\times 2}$ and $b \\in \\mathbb{R}^{2}$. Train by minimizing the empirical risk under cross-entropy with possibly soft targets:\n$$\n\\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_{i=1}^{N} \\sum_{k=0}^{1} y_{i,k} \\log p_{\\theta}(y=k \\mid x_{i}),\n$$\nwhere $y_{i} \\in [0,1]^{2}$ is one-hot for unaugmented examples and can be a convex combination for CutMix as specified below. Optimize $\\mathcal{L}$ with mini-batch gradient descent.\n\nAugmentations to implement:\n- Cutout: Given an image $x \\in [0,1]^{H \\times W}$ and a square mask of side length $s = \\lfloor \\sqrt{f} \\cdot H \\rfloor$ for fraction $f \\in (0,1)$, choose a uniformly random top-left location that keeps the square inside bounds and fill the masked region with a constant value $m \\in [0,1]$ (use the dataset mean intensity), leaving the label unchanged.\n- CutMix: Given two images $x_{a}, x_{b}$ with labels $y_{a}, y_{b}$, sample $\\lambda \\sim \\operatorname{Beta}(\\alpha,\\alpha)$ for $\\alpha0$. Compute a rectangular region whose area fraction is $(1-\\lambda)$ by setting its side lengths proportional to $\\sqrt{1-\\lambda}$ and placing it at a uniformly random location. Replace that region in $x_{a}$ with the corresponding patch from $x_{b}$ to obtain $\\tilde{x}$, and use the mixed label $\\tilde{y} = \\lambda y_{a} + (1-\\lambda) y_{b}$. As a boundary case, allow full replacement with $\\lambda = 0$, which corresponds to using the entire $x_{b}$ and label $y_{b}$.\n\nTraining and evaluation protocol: Flatten images to vectors in $\\mathbb{R}^{D}$, train the model for a fixed number of epochs with a fixed learning rate and batch size, and then report the test accuracy defined as the fraction of correctly predicted class indices on a held-out test set. Use a fixed random seed to ensure deterministic behavior.\n\nTest suite: Your program must run the following four configurations and return the test accuracy for each, in the listed order.\n- Case $1$ (happy path): no augmentation.\n- Case $2$ (coverage variant): Cutout with fraction $f = 0.50$ and fill value equal to the mean intensity $m$ of the dataset.\n- Case $3$ (augmentation under study): CutMix with $\\alpha = 1.0$ in the Beta distribution $\\operatorname{Beta}(\\alpha,\\alpha)$.\n- Case $4$ (boundary condition): CutMix with full replacement, i.e., deterministically set $\\lambda = 0$ so the pasted rectangle is the entire image.\n\nFixed hyperparameters and data specifications to use in all cases:\n- Image height $H = 16$ and width $W = 16$.\n- Low and high intensities $c_{\\mathrm{lo}} = 0.20$ and $c_{\\mathrm{hi}} = 0.80$.\n- Noise standard deviation $\\sigma = 0.05$.\n- Training set size $N_{\\mathrm{train}} = 400$ and test set size $N_{\\mathrm{test}} = 200$.\n- Batch size $B = 64$, number of epochs $E = 60$, learning rate $\\eta = 0.1$.\n- Random seed $s_{0} = 42$.\n- Use one-hot encoding for the two labels and soft convex combinations only when CutMix applies.\n\nRequired final output format: Your program should produce a single line of output containing the results as a comma-separated list of floating-point accuracies in $[0,1]$, ordered as $[a_{1},a_{2},a_{3},a_{4}]$ corresponding to Cases $1$ through $4$, enclosed in square brackets and with no extra whitespace or text (for example, $[0.9750,0.9600,0.9100,0.5200]$). No units are involved in this problem, and any fractional quantities must be expressed as decimals in the program output.", "solution": "The objective is to investigate the impact of spatial data augmentations—specifically Cutout and CutMix—on the performance of a multinomial logistic regression model. The learning task is designed such that class identity is determined exclusively by the global spatial arrangement of features, not by local content. We will implement the entire experimental pipeline from first principles, including dataset generation, model training via gradient descent, and the augmentation algorithms, to evaluate four distinct training configurations.\n\nFirst, we formally define the synthetic dataset. Images are of size $H \\times W$, where $H=16$ and $W=16$. There are two classes, $k \\in \\{0, 1\\}$. The image canvas is partitioned into four equal $8 \\times 8$ quadrants. For an image of class $k=0$, the top-left and bottom-right quadrants are filled with a high intensity value $c_{\\mathrm{hi}}=0.80$, while the top-right and bottom-left quadrants are filled with a low intensity value $c_{\\mathrm{lo}}=0.20$. For class $k=1$, this assignment is inverted. Following this deterministic construction, we add independent and identically distributed Gaussian noise, drawn from $\\mathcal{N}(0, \\sigma^2)$ with $\\sigma=0.05$, to each pixel. The final pixel intensities are clipped to the range $[0, 1]$. This procedure generates a dataset where the core distinguishing feature is a global \"checkerboard\" pattern of intensity. Training and test sets of sizes $N_{\\mathrm{train}}=400$ and $N_{\\mathrm{test}}=200$ are generated, with balanced classes. The random seed for all stochastic processes is fixed at $s_0=42$ to ensure reproducibility.\n\nThe model employed is multinomial logistic regression, also known as softmax regression. An input image is first flattened into a vector $x \\in \\mathbb{R}^{D}$, where the dimensionality $D = H \\cdot W = 256$. The model parameters are a weight matrix $W \\in \\mathbb{R}^{D \\times 2}$ and a bias vector $b \\in \\mathbb{R}^{2}$. For a given input $x$, the model computes scores $z_k = w_k^\\top x + b_k$ for each class $k$. These scores are transformed into probabilities using the softmax function:\n$$\np_{\\theta}(y=k \\mid x) = \\frac{\\exp(z_k)}{\\sum_{j=0}^{1} \\exp(z_j)}\n$$\nwhere $\\theta = \\{W, b\\}$.\n\nThe model parameters are learned by minimizing the empirical risk, specifically the average cross-entropy loss over the training dataset. For a training set of $N$ samples $\\{(x_i, y_i)\\}_{i=1}^N$, where $y_i$ is the label vector, the loss function is:\n$$\n\\mathcal{L}(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=0}^{1} y_{i,k} \\log p_{\\theta}(y=k \\mid x_{i})\n$$\nThe label vector $y_i$ is a one-hot encoding for standard classification (e.g., $[1, 0]$ for class $0$). For samples created by CutMix, $y_i$ becomes a \"soft\" label representing a convex combination of the original one-hot labels.\n\nOptimization is performed using mini-batch gradient descent. For a mini-batch of size $B$, the gradients of the loss with respect to the parameters are:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W} = \\frac{1}{B} X_{\\text{batch}}^\\top (P - Y_{\\text{batch}})\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{B} \\sum_{i=1}^{B} (p_i - y_i)\n$$\nwhere $X_{\\text{batch}} \\in \\mathbb{R}^{B \\times D}$ is the matrix of input vectors, $Y_{\\text{batch}} \\in \\mathbb{R}^{B \\times 2}$ is the matrix of label vectors, and $P \\in \\mathbb{R}^{B \\times 2}$ is the matrix of predicted probabilities. The parameters are updated iteratively for $E=60$ epochs using a learning rate of $\\eta=0.1$:\n$$\nW \\leftarrow W - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W}\n$$\n$$\nb \\leftarrow b - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b}\n$$\n\nWe implement and evaluate two data augmentation techniques:\n\n1.  **Cutout**: For each image in a batch, a square region is selected and its pixels are replaced with a constant value. The side length of the square is $s = \\lfloor \\sqrt{f} \\cdot H \\rfloor$, where the fraction $f=0.50$, yielding $s = \\lfloor \\sqrt{0.50} \\cdot 16 \\rfloor = 11$. The top-left corner of the square is chosen uniformly at random such that the square remains within the image boundaries. The fill value is the mean intensity of the training dataset. The class label of the image remains unchanged. This augmentation occludes a significant portion of the image, potentially disrupting the global pattern.\n\n2.  **CutMix**: This technique combines pairs of training examples. For a pair of images $(x_a, x_b)$ with labels $(y_a, y_b)$, a mixing a coefficient $\\lambda$ is sampled from a Beta distribution, $\\lambda \\sim \\operatorname{Beta}(\\alpha, \\alpha)$ with $\\alpha=1.0$ (which is equivalent to a uniform distribution $\\mathcal{U}[0,1]$). A rectangular patch is cut from $x_b$ and pasted onto $x_a$. The area of this patch is $(1-\\lambda)$ times the total image area, with side lengths proportional to $\\sqrt{1-\\lambda}$. The location of the patch is chosen uniformly at random. The resulting synthetic image $\\tilde{x}$ is assigned a soft label $\\tilde{y} = \\lambda y_a + (1-\\lambda) y_b$. This forces the model to learn from fragmented patterns and associate them with proportionally mixed labels.\n\nWe will execute four test cases to assess the impact of these augmentations:\n- **Case 1**: No augmentation, serving as a baseline.\n- **Case 2**: Training with Cutout ($f=0.50$).\n- **Case 3**: Training with CutMix ($\\alpha=1.0$).\n- **Case 4**: A boundary condition of CutMix where $\\lambda$ is deterministically set to $0$. This results in replacing a training sample $(x_a, y_a)$ with another randomly chosen sample from the same batch, $(x_b, y_b)$.\n\nFor each case, we train a model from scratch on the same training data and report its final accuracy on the same held-out test set. This controlled comparison will illuminate how these augmentations interact with a learning problem that depends critically on global context.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the four test cases and print the results.\n    \"\"\"\n    \n    # --- Fixed Hyperparameters and Data Specifications ---\n    H, W = 16, 16\n    c_lo, c_hi = 0.20, 0.80\n    sigma = 0.05\n    N_train, N_test = 400, 200\n    B = 64\n    E = 60\n    eta = 0.1\n    s0 = 42\n    \n    # --- Case-specific parameters ---\n    case_params = [\n        {'aug': 'none'},\n        {'aug': 'cutout', 'f': 0.50},\n        {'aug': 'cutmix', 'alpha': 1.0},\n        {'aug': 'cutmix_lambda_0'},\n    ]\n\n    results = []\n    for params in case_params:\n        # Each case must be fully deterministic and reproducible\n        accuracy = train_and_evaluate(\n            H=H, W=W, c_lo=c_lo, c_hi=c_hi, sigma=sigma,\n            N_train=N_train, N_test=N_test, B=B, E=E, eta=eta,\n            seed=s0, aug_params=params\n        )\n        results.append(f\"{accuracy:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef train_and_evaluate(H, W, c_lo, c_hi, sigma, N_train, N_test, B, E, eta, seed, aug_params):\n    \"\"\"\n    Generates data, trains a model under a specific augmentation, and evaluates it.\n    \"\"\"\n    \n    # --- Seeding for reproducibility ---\n    rng = np.random.default_rng(seed)\n\n    # --- Helper Functions ---\n    def softmax(z):\n        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def cross_entropy_loss(y_true, y_pred):\n        # Clip y_pred to avoid log(0)\n        y_pred = np.clip(y_pred, 1e-12, 1. - 1e-12)\n        return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n        \n    # --- Dataset Generation ---\n    def generate_dataset(N, H, W, c_lo, c_hi, sigma, rng_gen):\n        n_per_class = N // 2\n        X = np.zeros((N, H, W), dtype=np.float32)\n        Y = np.zeros((N, 2), dtype=np.float32)\n        \n        h_half, w_half = H // 2, W // 2\n        \n        # Class 0\n        for i in range(n_per_class):\n            img = np.full((H, W), c_lo, dtype=np.float32)\n            img[:h_half, :w_half] = c_hi\n            img[h_half:, w_half:] = c_hi\n            X[i] = img\n            Y[i] = [1, 0]\n            \n        # Class 1\n        for i in range(n_per_class, N):\n            img = np.full((H, W), c_hi, dtype=np.float32)\n            img[:h_half, :w_half] = c_lo\n            img[h_half:, w_half:] = c_lo\n            X[i] = img\n            Y[i] = [0, 1]\n\n        # Add noise and clip\n        X += rng_gen.normal(0, sigma, X.shape)\n        X = np.clip(X, 0.0, 1.0)\n        \n        # Shuffle dataset\n        indices = np.arange(N)\n        rng_gen.shuffle(indices)\n        X, Y = X[indices], Y[indices]\n        \n        return X, Y\n\n    X_train, Y_train = generate_dataset(N_train, H, W, c_lo, c_hi, sigma, rng)\n    X_test, Y_test = generate_dataset(N_test, H, W, c_lo, c_hi, sigma, rng)\n\n    # --- Model Initialization ---\n    D = H * W\n    K = 2 \n    # Use the same RNG for reproducible weight initialization\n    w_rng = np.random.default_rng(seed)\n    W_mat = w_rng.normal(0, 0.01, (D, K))\n    b_vec = np.zeros((1, K))\n\n    # --- Augmentation setup ---\n    aug = aug_params['aug']\n    \n    # Calculate dataset mean for Cutout\n    mean_intensity = 0.0\n    if aug == 'cutout':\n        mean_intensity = np.mean(X_train)\n\n    # --- Training Loop ---\n    for epoch in range(E):\n        indices = np.arange(N_train)\n        rng.shuffle(indices)\n        X_train_shuffled, Y_train_shuffled = X_train[indices], Y_train[indices]\n\n        for i in range(0, N_train, B):\n            X_batch_orig = X_train_shuffled[i:i+B]\n            Y_batch_orig = Y_train_shuffled[i:i+B]\n            \n            actual_B = X_batch_orig.shape[0]\n            if actual_B == 0: continue\n\n            # Apply augmentations\n            X_batch_aug, Y_batch_aug = X_batch_orig.copy(), Y_batch_orig.copy()\n\n            if aug == 'cutout':\n                f = aug_params['f']\n                s = int(np.floor(np.sqrt(f) * H))\n                for j in range(actual_B):\n                    y1 = rng.integers(0, H - s + 1)\n                    x1 = rng.integers(0, W - s + 1)\n                    X_batch_aug[j, y1:y1+s, x1:x1+s] = mean_intensity\n            \n            elif aug == 'cutmix':\n                alpha = aug_params['alpha']\n                for j in range(actual_B):\n                    lam = rng.beta(alpha, alpha)\n                    rand_index = rng.integers(actual_B)\n                    \n                    xa, ya = X_batch_aug[j], Y_batch_aug[j]\n                    xb, yb = X_batch_aug[rand_index], Y_batch_aug[rand_index]\n                    \n                    ratio = np.sqrt(1. - lam)\n                    patch_h = int(H * ratio)\n                    patch_w = int(W * ratio)\n\n                    if patch_h > 0 and patch_w > 0:\n                        cy = rng.integers(H - patch_h + 1)\n                        cx = rng.integers(W - patch_w + 1)\n                        xa[cy:cy+patch_h, cx:cx+patch_w] = xb[cy:cy+patch_h, cx:cx+patch_w]\n\n                    X_batch_aug[j] = xa\n                    Y_batch_aug[j] = lam * ya + (1. - lam) * yb\n\n            elif aug == 'cutmix_lambda_0':\n                # Deterministic lambda = 0\n                for j in range(actual_B):\n                    rand_index = rng.integers(actual_B)\n                    # Complete replacement of image and label\n                    X_batch_aug[j] = X_batch_orig[rand_index]\n                    Y_batch_aug[j] = Y_batch_orig[rand_index]\n\n            # Flatten images\n            X_batch_flat = X_batch_aug.reshape(actual_B, D)\n\n            # Forward pass\n            scores = X_batch_flat @ W_mat + b_vec\n            probs = softmax(scores)\n\n            # Backward pass (gradient calculation)\n            grad_scores = (probs - Y_batch_aug) / actual_B\n            grad_W = X_batch_flat.T @ grad_scores\n            grad_b = np.sum(grad_scores, axis=0, keepdims=True)\n\n            # Update parameters\n            W_mat -= eta * grad_W\n            b_vec -= eta * grad_b\n\n    # --- Evaluation ---\n    X_test_flat = X_test.reshape(N_test, D)\n    test_scores = X_test_flat @ W_mat + b_vec\n    test_probs = softmax(test_scores)\n    \n    predictions = np.argmax(test_probs, axis=1)\n    ground_truth = np.argmax(Y_test, axis=1)\n    \n    accuracy = np.mean(predictions == ground_truth)\n    return accuracy\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3151909"}]}