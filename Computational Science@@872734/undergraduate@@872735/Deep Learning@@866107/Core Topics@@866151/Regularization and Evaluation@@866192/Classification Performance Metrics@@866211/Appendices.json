{"hands_on_practices": [{"introduction": "The reliability of classification metrics is not absolute; it can be sensitive to the underlying data distribution. A common challenge in deploying machine learning models is \"prevalence shift,\" where the balance of positive and negative classes in the real world differs from the validation data. This exercise [@problem_id:3105734] will guide you through a first-principles derivation to understand how metrics like Precision and Recall behave under such shifts. By modeling classifier scores and working through the mathematics, you will gain a deeper intuition for why metrics like Recall are robust to prevalence changes while Precision is not, a critical concept for building reliable systems.", "problem": "A binary classifier used in a deep learning pipeline outputs a nonnegative score $s$ where larger values indicate a higher likelihood of belonging to the positive class. On a validation set, the classifier’s decision threshold $\\tau$ was selected to maximize the F1-score (F1), defined by the harmonic mean of precision and recall. After deployment, the class prevalence changes. Starting only from fundamental definitions of the confusion matrix and the standard classification metrics, derive how precision and recall depend on the class prevalence when the score threshold is fixed, and then re-tune the threshold to maximize the F1-score under the new prevalence.\n\nAssume the following scientifically plausible model for the score $s$:\n- The conditional score distribution for the positive class is exponential with rate $\\lambda_{1}$, namely $s \\mid y=1 \\sim \\text{Exp}(\\lambda_{1})$.\n- The conditional score distribution for the negative class is exponential with rate $\\lambda_{0}$, namely $s \\mid y=0 \\sim \\text{Exp}(\\lambda_{0})$.\n\nA decision rule predicts $y=1$ if and only if $s \\geq \\tau$ and predicts $y=0$ otherwise. Let the validation prevalence be $\\pi_{\\text{val}}$, and the deployment prevalence be $\\pi_{\\text{dep}}$. Use only the core definitions of true positive rate, false positive rate, precision, recall, and F1-score to derive the required expressions.\n\nConcretely, work with parameters $\\lambda_{1}=1$, $\\lambda_{0}=2$, $\\pi_{\\text{val}}=0.4$, and $\\pi_{\\text{dep}}=0.2$:\n- First, using only the definitions, derive expressions for true positive rate and false positive rate as functions of $\\tau$.\n- Then, derive expressions for precision and recall as functions of $\\tau$ and the prevalence $\\pi$.\n- Explain qualitatively which of precision or recall changes when the prevalence shifts from $\\pi_{\\text{val}}$ to $\\pi_{\\text{dep}}$, and why.\n- Determine the validation threshold $\\tau_{\\text{val}}$ that maximizes the F1-score under $\\pi_{\\text{val}}$.\n- Finally, compute the deployment threshold $\\tau_{\\text{dep}}$ that maximizes the F1-score under $\\pi_{\\text{dep}}$.\n\nRound your final numerical answer for the deployment threshold $\\tau_{\\text{dep}}$ to four significant figures. Provide only the numerical value of $\\tau_{\\text{dep}}$ as your final answer.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. It is a standard problem in statistical decision theory applied to classifier evaluation, using common definitions and a plausible, formalizable model for classifier scores. All necessary parameters are provided. Therefore, the problem is valid, and a solution will be derived.\n\nThe solution proceeds in five parts as requested.\n\nFirst, we derive the expressions for the True Positive Rate (TPR) and False Positive Rate (FPR) as functions of the decision threshold $\\tau$. By definition, the TPR, also known as Recall or Sensitivity, is the probability that a positive instance is correctly classified as positive. The FPR is the probability that a negative instance is incorrectly classified as positive. The decision rule states that a sample is classified as positive ($\\hat{y}=1$) if its score $s$ is greater than or equal to the threshold $\\tau$.\n\nGiven a true class $y$, the TPR and FPR are:\n$$TPR(\\tau) = P(\\hat{y}=1 | y=1) = P(s \\geq \\tau | y=1)$$\n$$FPR(\\tau) = P(\\hat{y}=1 | y=0) = P(s \\geq \\tau | y=0)$$\nThe conditional score distributions are given as exponential. For a random variable $X \\sim \\text{Exp}(\\lambda)$, its probability density function is $f(x) = \\lambda e^{-\\lambda x}$ for $x \\geq 0$, and its survival function is $P(X \\geq x) = e^{-\\lambda x}$.\nFor the positive class, $s \\mid y=1 \\sim \\text{Exp}(\\lambda_1)$. The TPR is therefore:\n$$TPR(\\tau) = \\exp(-\\lambda_1 \\tau)$$\nFor the negative class, $s \\mid y=0 \\sim \\text{Exp}(\\lambda_0)$. The FPR is therefore:\n$$FPR(\\tau) = \\exp(-\\lambda_0 \\tau)$$\n\nSecond, we derive the expressions for Precision (PREC) and Recall (REC) as functions of $\\tau$ and the class prevalence $\\pi = P(y=1)$.\nRecall is, by definition, the same as the True Positive Rate:\n$$REC(\\tau) = TPR(\\tau) = \\exp(-\\lambda_1 \\tau)$$\nPrecision, or Positive Predictive Value (PPV), is the probability that a sample is truly positive, given that it was classified as positive. Using Bayes' theorem:\n$$PREC(\\tau, \\pi) = P(y=1 | \\hat{y}=1) = \\frac{P(\\hat{y}=1 | y=1)P(y=1)}{P(\\hat{y}=1)}$$\nThe numerator is $TPR(\\tau) \\cdot \\pi$. The denominator can be expanded using the law of total probability:\n$$P(\\hat{y}=1) = P(\\hat{y}=1 | y=1)P(y=1) + P(\\hat{y}=1 | y=0)P(y=0)$$\n$$P(\\hat{y}=1) = TPR(\\tau) \\cdot \\pi + FPR(\\tau) \\cdot (1-\\pi)$$\nSubstituting these into the expression for precision gives:\n$$PREC(\\tau, \\pi) = \\frac{TPR(\\tau) \\cdot \\pi}{TPR(\\tau) \\cdot \\pi + FPR(\\tau) \\cdot (1-\\pi)}$$\nSubstituting the exponential forms for TPR and FPR, we get the final expressions:\n$$REC(\\tau) = \\exp(-\\lambda_1 \\tau)$$\n$$PREC(\\tau, \\pi) = \\frac{\\pi \\exp(-\\lambda_1 \\tau)}{\\pi \\exp(-\\lambda_1 \\tau) + (1-\\pi) \\exp(-\\lambda_0 \\tau)}$$\n\nThird, we explain the effect of changing prevalence.\nThe expression for Recall, $REC(\\tau) = \\exp(-\\lambda_1 \\tau)$, does not depend on the prevalence $\\pi$. Recall is a measure of a classifier's ability to identify all positive instances, conditioned on the ground truth being positive. This property is intrinsic to the classifier's performance on the positive class and is not affected by the proportion of positive instances in the population.\nThe expression for Precision, conversely, clearly shows a dependence on $\\pi$. Precision measures the proportion of correct predictions among all positive predictions. The set of positive predictions is composed of True Positives (from the positive class) and False Positives (from the negative class). A change in prevalence $\\pi$ alters the balance between the number of available positive and negative samples. If $\\pi$ decreases (as from $\\pi_{\\text{val}}=0.4$ to $\\pi_{\\text{dep}}=0.2$), the negative class becomes more dominant. For a fixed FPR, this leads to a larger absolute number of False Positives, which dilutes the set of predicted positives and consequently lowers the precision.\n\nFourth, we determine the optimal threshold $\\tau$ that maximizes the F1-score. The F1-score is the harmonic mean of Precision and Recall:\n$$F1 = 2 \\frac{PREC \\cdot REC}{PREC + REC}$$\nMaximizing the F1-score is equivalent to minimizing its reciprocal, $\\frac{1}{F1}$:\n$$\\frac{1}{F1} = \\frac{1}{2}\\left(\\frac{1}{PREC} + \\frac{1}{REC}\\right)$$\nLet's define a function $f(\\tau) = \\frac{2}{F1(\\tau, \\pi)}$ which we want to minimize.\n$$\\frac{1}{REC(\\tau)} = \\frac{1}{\\exp(-\\lambda_1 \\tau)} = \\exp(\\lambda_1 \\tau)$$\n$$\\frac{1}{PREC(\\tau, \\pi)} = \\frac{\\pi \\exp(-\\lambda_1 \\tau) + (1-\\pi) \\exp(-\\lambda_0 \\tau)}{\\pi \\exp(-\\lambda_1 \\tau)} = 1 + \\frac{1-\\pi}{\\pi} \\frac{\\exp(-\\lambda_0 \\tau)}{\\exp(-\\lambda_1 \\tau)} = 1 + \\frac{1-\\pi}{\\pi} \\exp((\\lambda_1 - \\lambda_0)\\tau)$$\nSo, the function to minimize is:\n$$f(\\tau) = \\exp(\\lambda_1 \\tau) + 1 + \\frac{1-\\pi}{\\pi} \\exp((\\lambda_1 - \\lambda_0)\\tau)$$\nTo find the minimum, we compute the derivative with respect to $\\tau$ and set it to zero:\n$$\\frac{df}{d\\tau} = \\lambda_1 \\exp(\\lambda_1 \\tau) + \\frac{1-\\pi}{\\pi}(\\lambda_1 - \\lambda_0) \\exp((\\lambda_1 - \\lambda_0)\\tau) = 0$$\n$$\\lambda_1 \\exp(\\lambda_1 \\tau) = -\\frac{1-\\pi}{\\pi}(\\lambda_1 - \\lambda_0) \\exp((\\lambda_1 - \\lambda_0)\\tau)$$\n$$\\lambda_1 \\exp(\\lambda_1 \\tau) = \\frac{1-\\pi}{\\pi}(\\lambda_0 - \\lambda_1) \\exp(\\lambda_1 \\tau)\\exp(-\\lambda_0 \\tau)$$\nSince $\\tau \\geq 0$, we know $\\exp(\\lambda_1 \\tau) > 0$ and can divide by it:\n$$\\lambda_1 = \\frac{1-\\pi}{\\pi}(\\lambda_0 - \\lambda_1) \\exp(-\\lambda_0 \\tau)$$\nNow, we solve for $\\tau$:\n$$\\exp(-\\lambda_0 \\tau) = \\frac{\\pi \\lambda_1}{(1-\\pi)(\\lambda_0 - \\lambda_1)}$$\n$$-\\lambda_0 \\tau = \\ln\\left(\\frac{\\pi \\lambda_1}{(1-\\pi)(\\lambda_0 - \\lambda_1)}\\right)$$\n$$\\tau = -\\frac{1}{\\lambda_0} \\ln\\left(\\frac{\\pi \\lambda_1}{(1-\\pi)(\\lambda_0 - \\lambda_1)}\\right) = \\frac{1}{\\lambda_0} \\ln\\left(\\frac{(1-\\pi)(\\lambda_0 - \\lambda_1)}{\\pi \\lambda_1}\\right)$$\nThis is the general expression for the F1-maximizing threshold $\\tau$. The validation threshold $\\tau_{\\text{val}}$ is obtained by substituting $\\pi = \\pi_{\\text{val}} = 0.4$, $\\lambda_1 = 1$, and $\\lambda_0 = 2$:\n$$\\tau_{\\text{val}} = \\frac{1}{2} \\ln\\left(\\frac{(1-0.4)(2-1)}{0.4 \\cdot 1}\\right) = \\frac{1}{2} \\ln\\left(\\frac{0.6}{0.4}\\right) = \\frac{1}{2} \\ln(1.5)$$\n\nFinally, we compute the deployment threshold $\\tau_{\\text{dep}}$ by substituting the deployment prevalence $\\pi = \\pi_{\\text{dep}} = 0.2$ into the general expression for $\\tau$:\n$$\\tau_{\\text{dep}} = \\frac{1}{2} \\ln\\left(\\frac{(1-0.2)(2-1)}{0.2 \\cdot 1}\\right)$$\n$$\\tau_{\\text{dep}} = \\frac{1}{2} \\ln\\left(\\frac{0.8}{0.2}\\right) = \\frac{1}{2} \\ln(4)$$\nSince $\\ln(4) = \\ln(2^2) = 2\\ln(2)$, the expression simplifies to:\n$$\\tau_{\\text{dep}} = \\frac{1}{2} (2\\ln(2)) = \\ln(2)$$\nThe numerical value is $\\tau_{\\text{dep}} \\approx 0.693147...$. Rounding to four significant figures gives $0.6931$.", "answer": "$$\\boxed{0.6931}$$", "id": "3105734"}, {"introduction": "Choosing the right evaluation metric is not just an academic exercise; it has direct consequences on the model you ultimately deploy. In practice, we often use a metric to decide when to stop training a model, a process known as early stopping. This practice problem [@problem_id:3105763] simulates a model's training trajectory on an imbalanced dataset, allowing you to compare the outcomes of using accuracy versus the $F_1$-score as the stopping criterion. It provides a hands-on demonstration of how an accuracy-focused strategy can prematurely terminate training, yielding a model that performs poorly on the underrepresented class.", "problem": "You are given a binary classification validation setting with imbalanced classes and two early stopping strategies. The goal is to construct a deterministic validation trajectory across epochs using class-conditional correctness probabilities, derive performance metrics from first principles, and decide for each test case whether stopping on validation $\\text{Accuracy}$ yields a worse minority-class $\\text{Recall}$ than stopping on validation $\\text{F1}$.\n\nFundamental base definitions to use:\n- A binary classifier induces counts of True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN) on a dataset; these are the building blocks from which performance metrics are defined.\n- Performance metrics to derive and implement from these counts are: $\\text{Accuracy}$, $\\text{Precision}$, $\\text{Recall}$, and the $\\text{F1}$-score (commonly denoted as $\\text{F1}$).\n\nValidation trajectory generation:\n- Let the total number of validation samples be $N$.\n- Let the minority-class (positive) fraction be $r$, so the majority-class (negative) fraction is $1 - r$.\n- The trajectory spans $T$ epochs indexed by $t \\in \\{1,2,\\ldots,T\\}$.\n- For each epoch $t$, define two class-conditional correctness probabilities:\n  - The negative-class correctness probability $p_n(t)$.\n  - The positive-class correctness probability $p_p(t)$.\n- The trajectory is generated by linear trends with clipping:\n  - $p_n(t)$ follows a decreasing linear trend: starting at $p_{n,0}$ and changing by a nonnegative rate $\\delta_n$ per epoch step, i.e., $p_n(t) = \\mathrm{clip}\\big(p_{n,0} - \\delta_n \\cdot (t - 1), 0, 1\\big)$.\n  - $p_p(t)$ follows an increasing linear trend: starting at $p_{p,0}$ and changing by a nonnegative rate $\\delta_p$ per epoch step, i.e., $p_p(t) = \\mathrm{clip}\\big(p_{p,0} + \\delta_p \\cdot (t - 1), 0, 1\\big)$.\n- Here $\\mathrm{clip}(x,0,1)$ denotes restricting $x$ to the interval $[0,1]$.\n\nComputation requirements:\n- For each epoch $t$, treat $\\{p_n(t), p_p(t)\\}$ as the probabilities of correct classification for negative and positive classes, respectively, and derive the expected confusion counts $\\{TP(t), FP(t), TN(t), FN(t)\\}$ from first principles, starting with $N_+ = r N$ and $N_- = (1 - r) N$ as the expected positive and negative sample counts.\n- Using only these definitions, derive and implement $\\text{Accuracy}(t)$, $\\text{Precision}(t)$, $\\text{Recall}(t)$, and $\\text{F1}(t)$ as decimals in $[0,1]$ for each epoch $t$.\n- Define two early stopping strategies:\n  - Strategy $\\mathcal{A}$: stop at the epoch $t$ that maximizes validation $\\text{Accuracy}(t)$.\n  - Strategy $\\mathcal{F}$: stop at the epoch $t$ that maximizes validation $\\text{F1}(t)$.\n- In case of ties for the maximum value across epochs, select the earliest epoch (smallest $t$).\n- For each strategy, report the minority-class $\\text{Recall}$ at the chosen epoch, and compare them.\n\nOutput specification:\n- For each test case, output a boolean indicating whether the minority-class $\\text{Recall}$ under Strategy $\\mathcal{A}$ is strictly lower than under Strategy $\\mathcal{F}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\ldots]$). Each $result_i$ must be a boolean.\n\nAll metrics must be expressed as decimals in $[0,1]$; do not use percentages.\n\nTest suite:\nUse the following four parameter sets to instantiate $(N, r, T, p_{n,0}, \\delta_n, p_{p,0}, \\delta_p)$.\n\n- Case $1$ (happy path, strong imbalance and diverging trends):\n  - $N = 10000$, $r = 0.05$, $T = 12$, $p_{n,0} = 0.995$, $\\delta_n = 0.01$, $p_{p,0} = 0.20$, $\\delta_p = 0.06$.\n- Case $2$ (balanced classes):\n  - $N = 10000$, $r = 0.50$, $T = 12$, $p_{n,0} = 0.995$, $\\delta_n = 0.01$, $p_{p,0} = 0.20$, $\\delta_p = 0.06$.\n- Case $3$ (mild imbalance, gentler trends):\n  - $N = 10000$, $r = 0.20$, $T = 12$, $p_{n,0} = 0.99$, $\\delta_n = 0.005$, $p_{p,0} = 0.30$, $\\delta_p = 0.03$.\n- Case $4$ (edge case, constant performance):\n  - $N = 10000$, $r = 0.10$, $T = 12$, $p_{n,0} = 0.99$, $\\delta_n = 0.0$, $p_{p,0} = 0.10$, $\\delta_p = 0.0$.\n\nYour task:\n- Implement the above trajectory generator and metric computations.\n- For each case, determine the early stopping epoch for $\\mathcal{A}$ and $\\mathcal{F}$, extract the corresponding minority-class $\\text{Recall}$ values, and output whether $\\text{Recall}_{\\mathcal{A}} < \\text{Recall}_{\\mathcal{F}}$ as a boolean.\n- Produce the final output in the exact single-line format described above.", "solution": "The problem requires an analysis of two early stopping strategies for a binary classifier, based on validation Accuracy and validation F1-score, respectively. We are asked to determine whether stopping on Accuracy leads to a worse minority-class Recall compared to stopping on F1-score. The analysis is performed on a deterministic validation trajectory generated from class-conditional correctness probabilities.\n\nFirst, we establish the theoretical foundations by deriving the necessary metrics from first principles as specified.\n\nLet $N$ be the total number of samples in the validation set.\nLet $r$ be the fraction of the minority (positive) class.\nThe number of positive samples is $N_+ = rN$.\nThe number of negative samples is $N_- = (1 - r)N$.\n\nThe validation trajectory spans $T$ epochs, indexed by $t \\in \\{1, 2, ..., T\\}$. For each epoch $t$, we are given the probabilities of correct classification for each class:\n- $p_p(t)$: probability of correctly classifying a positive sample.\n- $p_n(t)$: probability of correctly classifying a negative sample.\n\nThese probabilities evolve over epochs according to the following clipped linear functions:\n$$p_p(t) = \\mathrm{clip}(p_{p,0} + \\delta_p \\cdot (t - 1), 0, 1)$$\n$$p_n(t) = \\mathrm{clip}(p_{n,0} - \\delta_n \\cdot (t - 1), 0, 1)$$\nwhere $p_{p,0}$ and $p_{n,0}$ are initial probabilities, and $\\delta_p \\ge 0$ and $\\delta_n \\ge 0$ are the rates of change. The $\\mathrm{clip}(x, a, b)$ function constrains $x$ to the interval $[a, b]$.\n\nFrom these probabilities, we can derive the expected counts for the confusion matrix components at each epoch $t$:\n- True Positives ($TP(t)$): A positive sample is correctly classified.\n  $$TP(t) = N_+ \\cdot p_p(t) = rN \\cdot p_p(t)$$\n- False Negatives ($FN(t)$): A positive sample is incorrectly classified. The probability is $1 - p_p(t)$.\n  $$FN(t) = N_+ \\cdot (1 - p_p(t)) = rN \\cdot (1 - p_p(t))$$\n- True Negatives ($TN(t)$): A negative sample is correctly classified.\n  $$TN(t) = N_- \\cdot p_n(t) = (1 - r)N \\cdot p_n(t)$$\n- False Positives ($FP(t)$): A negative sample is incorrectly classified. The probability is $1 - p_n(t)$.\n  $$FP(t) = N_- \\cdot (1 - p_n(t)) = (1 - r)N \\cdot (1 - p_n(t))$$\n\nThe sum of all components is $TP(t) + FN(t) + TN(t) + FP(t) = N_+ + N_- = N$, as expected.\n\nNext, we derive the performance metrics for each epoch $t$ using these expected counts.\n1.  **Accuracy**: The fraction of correctly classified samples.\n    $$\\text{Accuracy}(t) = \\frac{TP(t) + TN(t)}{N} = \\frac{rN \\cdot p_p(t) + (1-r)N \\cdot p_n(t)}{N} = r \\cdot p_p(t) + (1-r) \\cdot p_n(t)$$\n    This shows that Accuracy is a weighted average of the class-wise correctness probabilities, with weights corresponding to class prevalence.\n\n2.  **Recall (Sensitivity or True Positive Rate)**: The fraction of actual positive samples that are correctly identified. This is the minority-class recall.\n    $$\\text{Recall}(t) = \\frac{TP(t)}{TP(t) + FN(t)} = \\frac{TP(t)}{N_+} = \\frac{rN \\cdot p_p(t)}{rN} = p_p(t)$$\n    This is a crucial result: the minority-class Recall at epoch $t$ is simply the correctness probability for the positive class, $p_p(t)$.\n\n3.  **Precision (Positive Predictive Value)**: The fraction of positive predictions that are correct.\n    $$\\text{Precision}(t) = \\frac{TP(t)}{TP(t) + FP(t)} = \\frac{rN \\cdot p_p(t)}{rN \\cdot p_p(t) + (1-r)N \\cdot (1-p_n(t))}$$\n    If the denominator $TP(t) + FP(t)$ is $0$, Precision is conventionally defined as $0$.\n\n4.  **F1-Score**: The harmonic mean of Precision and Recall.\n    $$\\text{F1}(t) = 2 \\cdot \\frac{\\text{Precision}(t) \\cdot \\text{Recall}(t)}{\\text{Precision}(t) + \\text{Recall}(t)}$$\n    If Precision and Recall are both $0$, the F1-score is also $0$.\n\nThe problem defines two early stopping strategies:\n- Strategy $\\mathcal{A}$: Stop at epoch $t_\\mathcal{A} = \\arg\\max_{t} \\text{Accuracy}(t)$.\n- Strategy $\\mathcal{F}$: Stop at epoch $t_\\mathcal{F} = \\arg\\max_{t} \\text{F1}(t)$.\nIn case of ties for the maximum value, the earliest epoch (smallest $t$) is chosen.\n\nWe must determine if the minority-class Recall under strategy $\\mathcal{A}$ is strictly lower than under strategy $\\mathcal{F}$. This is the condition:\n$$\\text{Recall}(t_\\mathcal{A}) < \\text{Recall}(t_\\mathcal{F})$$\nUsing our derived formula for Recall, this is equivalent to:\n$$p_p(t_\\mathcal{A}) < p_p(t_\\mathcal{F})$$\nThe function $p_p(t)$ is defined as a monotonically non-decreasing function of $t$ (since $\\delta_p \\ge 0$). Therefore, $p_p(t_1) < p_p(t_2)$ if and only if $t_1 < t_2$, assuming $p_p(t)$ has not been clipped at its maximum value of $1$. If it has been clipped, it is possible for $t_1 < t_2$ but $p_p(t_1) = p_p(t_2) = 1$. However, the strict inequality $p_p(t_\\mathcal{A}) < p_p(t_\\mathcal{F})$ can only hold if $t_\\mathcal{A} < t_\\mathcal{F}$. If $t_\\mathcal{A} \\ge t_\\mathcal{F}$, then $p_p(t_\\mathcal{A}) \\ge p_p(t_\\mathcal{F})$.\nThus, the problem reduces to determining if $t_\\mathcal{A} < t_\\mathcal{F}$.\n\nThe algorithm to solve the problem for each test case is as follows:\n1.  For the given parameters $(N, r, T, p_{n,0}, \\delta_n, p_{p,0}, \\delta_p)$, initialize arrays to store the values of $\\text{Accuracy}(t)$ and $\\text{F1}(t)$ for $t=1, \\dots, T$.\n2.  Loop for $t$ from $1$ to $T$:\n    a. Calculate $p_n(t)$ and $p_p(t)$ using the provided clipped linear formulas.\n    b. Calculate the confusion matrix components $TP(t)$, $FP(t)$, $TN(t)$, $FN(t)$.\n    c. Calculate $\\text{Accuracy}(t)$, $\\text{Recall}(t)$, and $\\text{Precision}(t)$, handling potential divisions by zero.\n    d. Calculate $\\text{F1}(t)$.\n    e. Store $\\text{Accuracy}(t)$ and $\\text{F1}(t)$ in their respective arrays.\n3.  Find the epoch $t_\\mathcal{A}$ that maximizes the $\\text{Accuracy}(t)$ array. The index of the first maximum element corresponds to the earliest epoch rule.\n4.  Find the epoch $t_\\mathcal{F}$ that maximizes the $\\text{F1}(t)$ array, again using the index of the first maximum.\n5.  Evaluate the boolean condition $t_\\mathcal{A} < t_\\mathcal{F}$.\n6.  Repeat for all test cases and collect the boolean results.\nThis procedure will be implemented to generate the final answer.", "answer": "[True,False,True,False]", "id": "3105763"}, {"introduction": "When dealing with imbalanced datasets, a common strategy is to artificially balance the training data, for instance by oversampling the minority class. While this can improve a model's ability to detect positive cases (increasing Recall), it can also lead to overfitting, causing more false alarms (decreasing Precision). This exercise [@problem_id:3105759] models this exact trade-off, asking you to find the optimal oversampling rate that maximizes the $F_1$-score. This task mirrors the real-world challenge of tuning a modeling strategy to find the best balance between competing performance objectives.", "problem": "You are given a principled model of how oversampling positive training instances in a binary classifier affects validation performance. In this model, a binary classifier is trained using an oversampling rate $r \\in \\{1,2,\\dots,R_{\\max}\\}$, which replicates each positive training example $r$ times. On a fixed validation set of size $N$ with a positive class fraction $\\pi$, the classifier exhibits a baseline True Positive Rate (TPR) $t_0$ and baseline False Positive Rate (FPR) $f_0$ when $r = 1$. Due to increased sensitivity, oversampling is assumed to increase the TPR, but it also increases the FPR due to overfitting to positive-like features. To capture these effects, suppose the validation TPR and FPR as functions of $r$ are\n$$\nt(r) = t_0 + (1 - t_0)\\left(1 - e^{-a(r - 1)}\\right), \\quad f(r) = f_0 + b(1 - f_0)\\left(1 - e^{-c(r - 1)}\\right),\n$$\nwhere $a > 0$ controls the sensitivity growth, $b \\in (0,1]$ bounds the maximal false positive blow-up, and $c > 0$ controls how quickly false positives increase with oversampling.\n\nLet the validation confusion matrix counts be denoted by True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN). Using the foundational definitions relating these counts to rates and class proportions, derive the Accuracy, Precision, Recall, and the F1-score (F1) on validation for each $r$, where Accuracy is the fraction of correctly classified validation instances, Precision is the fraction of predicted positives that are truly positive, Recall is the fraction of actual positives that are correctly detected, and the F1-score is the harmonic mean of Precision and Recall. Then, for each test case, determine the oversampling rate $r^\\star$ that maximizes the F1-score on validation over the discrete set $\\{1,2,\\dots,R_{\\max}\\}$. In case of ties, choose the smallest $r$ that achieves the maximal F1-score.\n\nAll rates and fractions must be treated as decimals in $[0,1]$. No physical units or angles are involved. Your program must implement these definitions and the model above to compute the requested quantities.\n\nTest suite:\n- Case $1$: $N = 1000$, $\\pi = 0.1$, $t_0 = 0.5$, $f_0 = 0.05$, $a = 0.9$, $b = 0.6$, $c = 0.7$, $R_{\\max} = 10$.\n- Case $2$: $N = 800$, $\\pi = 0.5$, $t_0 = 0.6$, $f_0 = 0.1$, $a = 0.7$, $b = 0.9$, $c = 1.0$, $R_{\\max} = 10$.\n- Case $3$: $N = 1200$, $\\pi = 0.02$, $t_0 = 0.3$, $f_0 = 0.01$, $a = 1.2$, $b = 0.4$, $c = 0.8$, $R_{\\max} = 12$.\n\nFinal output format:\nYour program should produce a single line of output containing the optimal oversampling rates, one per test case, as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3]$). Each $r_i$ must be an integer in the specified range $\\{1,2,\\dots,R_{\\max}\\}$ for its case.", "solution": "The problem is valid as it is scientifically grounded in the principles of classification performance evaluation, is mathematically well-posed with a clear objective, and provides a complete and consistent set of information.\n\nThe objective is to find the optimal oversampling rate $r^\\star$ from the discrete set $\\{1, 2, \\dots, R_{\\max}\\}$ that maximizes the F1-score on a validation set. To achieve this, we must first derive the F1-score as a function of the oversampling rate $r$. This requires expressing the fundamental confusion matrix counts—True Positives ($TP$), False Positives ($FP$), False Negatives ($FN$), and True Negatives ($TN$)—in terms of the given parameters and functions.\n\nLet $N$ be the total size of the validation set and $\\pi$ be the fraction of positive instances. The number of actual positive instances is $P = N \\pi$, and the number of actual negative instances is $N_{\\text{neg}} = N(1-\\pi)$.\n\nThe True Positive Rate, $t(r)$, is defined as the fraction of actual positives that are correctly classified. This is also known as Recall. Thus, $\\text{Recall}(r) = t(r)$. The number of True Positives is a function of $r$:\n$$\nTP(r) = P \\cdot t(r) = N \\pi t(r)\n$$\nThe False Positive Rate, $f(r)$, is defined as the fraction of actual negatives that are incorrectly classified. The number of False Positives is:\n$$\nFP(r) = N_{\\text{neg}} \\cdot f(r) = N(1-\\pi)f(r)\n$$\nThe remaining confusion matrix counts can be derived from these. The number of False Negatives is the count of actual positives not identified as positive:\n$$\nFN(r) = P - TP(r) = N \\pi - N \\pi t(r) = N \\pi (1 - t(r))\n$$\nThe number of True Negatives is the count of actual negatives correctly identified as negative:\n$$\nTN(r) = N_{\\text{neg}} - FP(r) = N(1-\\pi) - N(1-\\pi)f(r) = N(1-\\pi)(1 - f(r))\n$$\n\nWith these counts, we can define the required performance metrics.\nThe Accuracy is the fraction of total correct predictions:\n$$\n\\text{Accuracy}(r) = \\frac{TP(r) + TN(r)}{N} = \\frac{N \\pi t(r) + N(1-\\pi)(1 - f(r))}{N} = \\pi t(r) + (1-\\pi)(1 - f(r))\n$$\nThe Precision is the fraction of predicted positives that are actually positive:\n$$\n\\text{Precision}(r) = \\frac{TP(r)}{TP(r) + FP(r)} = \\frac{N \\pi t(r)}{N \\pi t(r) + N(1-\\pi)f(r)} = \\frac{\\pi t(r)}{\\pi t(r) + (1-\\pi)f(r)}\n$$\nNote that for Precision to be well-defined, the denominator $TP(r) + FP(r)$ must be non-zero. If it is zero (no positive predictions are made), we can define Precision as $0$.\n\nThe F1-score is the harmonic mean of Precision and Recall. A common and algebraically convenient form is:\n$$\nF1(r) = \\frac{2 \\cdot TP(r)}{2 \\cdot TP(r) + FP(r) + FN(r)}\n$$\nSubstituting the expressions for $TP(r)$, $FP(r)$, and $FN(r)$:\n$$\nF1(r) = \\frac{2 N \\pi t(r)}{2 N \\pi t(r) + N(1-\\pi)f(r) + N \\pi (1 - t(r))}\n$$\nThe total validation set size $N$ cancels from the numerator and denominator, simplifying the expression to:\n$$\nF1(r) = \\frac{2 \\pi t(r)}{2 \\pi t(r) + (1-\\pi)f(r) + \\pi (1 - t(r))} = \\frac{2 \\pi t(r)}{\\pi t(r) + (1-\\pi)f(r) + \\pi}\n$$\n$$\nF1(r) = \\frac{2 \\pi t(r)}{\\pi(1+t(r)) + (1-\\pi)f(r)}\n$$\nThe functions for the True Positive Rate, $t(r)$, and False Positive Rate, $f(r)$, are given as:\n$$\nt(r) = t_0 + (1 - t_0)\\left(1 - e^{-a(r - 1)}\\right)\n$$\n$$\nf(r) = f_0 + b(1 - f_0)\\left(1 - e^{-c(r - 1)}\\right)\n$$\n\nThe overall algorithm to find the optimal oversampling rate $r^\\star$ is as follows:\nFor each test case, we iterate through each integer value of $r$ from $1$ to $R_{\\max}$. In each iteration, we:\n$1$. Calculate the values of $t(r)$ and $f(r)$ using the provided formulas and parameters for the specific case.\n$2$. Substitute these values into the derived expression for $F1(r)$ to compute the F1-score.\n$3$. Keep track of the maximum F1-score found so far and the corresponding value of $r$. If the current F1-score is greater than the maximum found previously, we update the maximum and set the current $r$ as the new optimal rate, $r^\\star$.\n$4$. Per the problem's tie-breaking rule, if a calculated F1-score is equal to the current maximum, we do not update $r^\\star$. This ensures that the smallest $r$ achieving the maximum F1-score is selected.\n\nThis procedure is implemented for each of the test cases provided.", "answer": "[2,1,3]", "id": "3105759"}]}