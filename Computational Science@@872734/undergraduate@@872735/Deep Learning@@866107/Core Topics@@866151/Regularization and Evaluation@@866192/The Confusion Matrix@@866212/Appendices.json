{"hands_on_practices": [{"introduction": "A high accuracy score can be deceptively reassuring, especially on imbalanced datasets. This exercise challenges the naive reliance on accuracy by presenting a scenario where two classifiers have identical accuracy but vastly different performance profiles [@problem_id:3094202]. By dissecting the underlying confusion matrices, you will explore the critical trade-off between precision ($P$) and recall ($R$) and understand why the $F_1$-score often provides a more robust and insightful measure of a model's true effectiveness.", "problem": "A binary classifier is evaluated on a dataset summarized by a confusion matrix with entries $(TP, FP, TN, FN)$, where $TP$ denotes the number of true positives, $FP$ denotes the number of false positives, $TN$ denotes the number of true negatives, and $FN$ denotes the number of false negatives. Accuracy ($(Acc)$) is defined by $ \\text{Acc} = \\dfrac{TP + TN}{TP + FP + TN + FN} $. Precision ($(P)$) and recall ($(R)$) are defined by $ P = \\dfrac{TP}{TP + FP} $ and $ R = \\dfrac{TP}{TP + FN} $, respectively. The $\\text{F}_{1}$-score ($(F_1)$) is the standard single-number summary of $P$ and $R$.\n\nSelect the single option that presents two confusion matrices $(TP, FP, TN, FN)$ for two systems, say System $X$ and System $Y$, that have the same $ \\text{Acc} $ but drastically different $ F_1 $ (interpret “drastically different” as a difference of at least $0.30$ in $F_1$), and that also provides the correct structural explanation of why $F_1$ differs, stated in terms of $P$ and $R$ and grounded in how $FP$ and $FN$ differ. Only one option satisfies both the numerical and explanatory requirements.\n\nA. System $X$: $(TP, FP, TN, FN) = (20, 5, 895, 80)$; System $Y$: $(TP, FP, TN, FN) = (80, 65, 835, 20)$. Explanation: Both systems have the same $TP + TN$, so $ \\text{Acc} $ matches. $F_1$ differs because $FP$ and $FN$ are traded off: System $X$ has small $FP$ (high $P$) but large $FN$ (low $R$); System $Y$ has larger $FP$ (lower $P$) but smaller $FN$ (higher $R$), yielding a substantially different harmonic balance of $P$ and $R$.\n\nB. System $X$: $(TP, FP, TN, FN) = (50, 50, 850, 50)$; System $Y$: $(TP, FP, TN, FN) = (45, 45, 855, 55)$. Explanation: Because both have the same $ \\text{Acc} $, their $F_1$ scores are necessarily equal; $F_1$ is determined solely by $ \\text{Acc} $.\n\nC. System $X$: $(TP, FP, TN, FN) = (10, 10, 980, 0)$; System $Y$: $(TP, FP, TN, FN) = (10, 0, 970, 20)$. Explanation: Their $ \\text{Acc} $ values are equal and $F_1$ differs because the number of true negatives $TN$ drives $F_1$ directly.\n\nD. System $X$: $(TP, FP, TN, FN) = (90, 10, 890, 10)$; System $Y$: $(TP, FP, TN, FN) = (80, 0, 900, 20)$. Explanation: Any $F_1$ difference here is caused primarily by changing $TN$, since $TN$ appears in the denominators of $P$ and $R$ and directly alters both.", "solution": "The user has requested a critical validation of the problem statement, followed by a full derivation and evaluation of the options provided.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem provides the following definitions and conditions for a binary classifier evaluated on a dataset:\n- Confusion matrix entries: $(TP, FP, TN, FN)$, representing true positives, false positives, true negatives, and false negatives.\n- Accuracy definition: $ \\text{Acc} = \\dfrac{TP + TN}{TP + FP + TN + FN} $.\n- Precision definition: $ P = \\dfrac{TP}{TP + FP} $.\n- Recall definition: $ R = \\dfrac{TP}{TP + FN} $.\n- The $\\text{F}_{1}$-score, $( F_1 )$, is defined as \"the standard single-number summary of $P$ and $R$\".\n- The task is to select the single option that presents two confusion matrices for System $X$ and System $Y$ that satisfy two numerical criteria and one explanatory criterion:\n    1.  The accuracies are the same: $\\text{Acc}_X = \\text{Acc}_Y$.\n    2.  The $F_1$ scores are \"drastically different\", interpreted as a difference of at least $0.30$: $|F_{1,X} - F_{1,Y}| \\geq 0.30$.\n    3.  The option provides the correct structural explanation for the difference in $F_1$ in terms of $P$, $R$, $FP$, and $FN$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem is based on fundamental and standard metrics used in statistical learning and model evaluation. The definitions for accuracy, precision, and recall are correct. The reference to the $F_1$-score as the \"standard single-number summary\" correctly points to the harmonic mean of precision and recall, a well-established concept.\n- **Well-Posed:** The problem is a multiple-choice question requiring the application of given formulas and evaluation of logical explanations. The term \"drastically different\" is given a precise quantitative meaning ($|F_{1,X} - F_{1,Y}| \\geq 0.30$), removing ambiguity. The problem is structured to have a single correct answer satisfying both numerical and explanatory criteria.\n- **Objective:** The problem statement is written in precise, objective language, using mathematical definitions. There is no subjectivity.\n- **Completeness and Consistency:** All necessary definitions and data (within the options) are provided to solve the problem. The reference to the standard $F_1$ score is unambiguous in this context. The problem is internally consistent.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. I will proceed with the solution.\n\n### Solution Derivation\n\nThe $F_1$-score is the harmonic mean of precision ($P$) and recall ($R$). Its standard formula is:\n$$ F_1 = 2 \\cdot \\frac{P \\cdot R}{P + R} $$\nSubstituting the definitions of $P$ and $R$ gives a formula directly in terms of the confusion matrix components:\n$$ F_1 = 2 \\cdot \\frac{\\frac{TP}{TP + FP} \\cdot \\frac{TP}{TP + FN}}{\\frac{TP}{TP + FP} + \\frac{TP}{TP + FN}} = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN} $$\nThis latter form is computationally convenient. The total number of samples is $N = TP + FP + TN + FN$. Accuracy can be written as $\\text{Acc} = \\frac{TP + TN}{N}$.\n\nWe will now evaluate each option by calculating the required metrics and assessing the validity of the provided explanation.\n\n**Analysis of Option A**\n\nSystem $X$: $(TP, FP, TN, FN) = (20, 5, 895, 80)$\n- Total samples: $N_X = 20 + 5 + 895 + 80 = 1000$.\n- Accuracy: $\\text{Acc}_X = \\frac{20 + 895}{1000} = \\frac{915}{1000} = 0.915$.\n- Precision: $P_X = \\frac{20}{20 + 5} = \\frac{20}{25} = 0.8$.\n- Recall: $R_X = \\frac{20}{20 + 80} = \\frac{20}{100} = 0.2$.\n- $F_1$-score: $F_{1,X} = \\frac{2 \\cdot TP_X}{2 \\cdot TP_X + FP_X + FN_X} = \\frac{2 \\cdot 20}{2 \\cdot 20 + 5 + 80} = \\frac{40}{40 + 85} = \\frac{40}{125} = 0.32$.\n\nSystem $Y$: $(TP, FP, TN, FN) = (80, 65, 835, 20)$\n- Total samples: $N_Y = 80 + 65 + 835 + 20 = 1000$.\n- Accuracy: $\\text{Acc}_Y = \\frac{80 + 835}{1000} = \\frac{915}{1000} = 0.915$.\n- Precision: $P_Y = \\frac{80}{80 + 65} = \\frac{80}{145} = \\frac{16}{29} \\approx 0.5517$.\n- Recall: $R_Y = \\frac{80}{80 + 20} = \\frac{80}{100} = 0.8$.\n- $F_1$-score: $F_{1,Y} = \\frac{2 \\cdot TP_Y}{2 \\cdot TP_Y + FP_Y + FN_Y} = \\frac{2 \\cdot 80}{2 \\cdot 80 + 65 + 20} = \\frac{160}{160 + 85} = \\frac{160}{245} = \\frac{32}{49} \\approx 0.6531$.\n\n**Evaluation of Option A:**\n1.  **Equal Accuracy:** $\\text{Acc}_X = 0.915$ and $\\text{Acc}_Y = 0.915$. This condition is satisfied.\n2.  **F1 Difference:** $|F_{1,X} - F_{1,Y}| = |0.32 - 0.6531| = 0.3331$. Since $0.3331 \\geq 0.30$, this condition is satisfied.\n3.  **Explanation:** The explanation states that $\\text{Acc}$ matches because $TP + TN$ is the same ($915$) for both, while the total $N$ is also the same ($1000$). This is correct. It further explains that $F_1$ differs because of a trade-off between $FP$ and $FN$. System $X$ has low $FP$ ($5$) and high $FN$ ($80$), resulting in high $P$ ($0.8$) and low $R$ ($0.2$). System $Y$ has high $FP$ ($65$) and low $FN$ ($20$), resulting in lower $P$ ($\\approx 0.55$) and high $R$ ($0.8$). This trade-off between $P$ and $R$ causes the difference in the $F_1$-score, which as a harmonic mean is sensitive to such imbalances. The explanation is structurally and factually correct.\n\nVerdict for Option A: **Correct**.\n\n**Analysis of Option B**\n\nSystem $X$: $(TP, FP, TN, FN) = (50, 50, 850, 50)$\nSystem $Y$: $(TP, FP, TN, FN) = (45, 45, 855, 55)$\n- $\\text{Acc}_X = \\frac{50 + 850}{1000} = 0.9$.\n- $\\text{Acc}_Y = \\frac{45 + 855}{1000} = 0.9$.\nThe accuracies are equal.\n- $F_{1,X} = \\frac{2 \\cdot 50}{2 \\cdot 50 + 50 + 50} = \\frac{100}{200} = 0.5$.\n- $F_{1,Y} = \\frac{2 \\cdot 45}{2 \\cdot 45 + 45 + 55} = \\frac{90}{90 + 100} = \\frac{90}{190} = \\frac{9}{19} \\approx 0.4737$.\n- $|F_{1,X} - F_{1,Y}| = |0.5 - 0.4737| \\approx 0.0263$. This is not $\\geq 0.30$.\n- **Explanation:** The explanation claims \"Because both have the same $ \\text{Acc} $, their $F_1$ scores are necessarily equal; $F_1$ is determined solely by $ \\text{Acc} $.\" This statement is fundamentally false. Accuracy and $F_1$-score are different metrics that are not deterministically linked. Our analysis of Option A has already provided a counterexample.\n\nVerdict for Option B: **Incorrect**.\n\n**Analysis of Option C**\n\nSystem $X$: $(TP, FP, TN, FN) = (10, 10, 980, 0)$\nSystem $Y$: $(TP, FP, TN, FN) = (10, 0, 970, 20)$\n- $\\text{Acc}_X = \\frac{10 + 980}{1000} = 0.99$.\n- $\\text{Acc}_Y = \\frac{10 + 970}{1000} = 0.98$.\n- The accuracies are not equal ($\\text{Acc}_X \\neq \\text{Acc}_Y$). The first condition of the problem is not met.\n- **Explanation:** The explanation contains two false statements. First, it incorrectly claims the accuracies are equal. Second, it claims \"$TN$ drives $F_1$ directly.\" The formula for $F_1$ is $F_1 = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}$, which does not include the $TN$ term. Therefore, $TN$ has no direct effect on the $F_1$-score.\n\nVerdict for Option C: **Incorrect**.\n\n**Analysis of Option D**\n\nSystem $X$: $(TP, FP, TN, FN) = (90, 10, 890, 10)$\nSystem $Y$: $(TP, FP, TN, FN) = (80, 0, 900, 20)$\n- $\\text{Acc}_X = \\frac{90 + 890}{1000} = 0.98$.\n- $\\text{Acc}_Y = \\frac{80 + 900}{1000} = 0.98$.\nThe accuracies are equal.\n- $F_{1,X} = \\frac{2 \\cdot 90}{2 \\cdot 90 + 10 + 10} = \\frac{180}{200} = 0.9$.\n- $F_{1,Y} = \\frac{2 \\cdot 80}{2 \\cdot 80 + 0 + 20} = \\frac{160}{180} = \\frac{8}{9} \\approx 0.8889$.\n- $|F_{1,X} - F_{1,Y}| = |0.9 - 0.8889| \\approx 0.0111$. This is not $\\geq 0.30$.\n- **Explanation:** The explanation claims the $F_1$ difference is \"caused primarily by changing $TN$, since $TN$ appears in the denominators of $P$ and $R$\". This is factually incorrect. The formulas for $P$ and $R$ are $P = \\frac{TP}{TP + FP}$ and $R = \\frac{TP}{TP + FN}$. The $TN$ term does not appear in either formula, nor in the formula for $F_1$.\n\nVerdict for Option D: **Incorrect**.\n\nBased on the detailed analysis, only Option A satisfies all the stated numerical and explanatory requirements.", "answer": "$$\\boxed{A}$$", "id": "3094202"}, {"introduction": "Evaluating performance in multi-class settings adds another layer of complexity, particularly when class distribution is skewed. This practice demonstrates how different averaging strategies can paint dramatically different pictures of a classifier's performance [@problem_id:3181107]. You will analyze a system where high overall accuracy and micro-averaged $F_1$ hide a complete failure to identify minority classes, a critical flaw only revealed by the macro-averaged $F_1$ score.", "problem": "A single-label, multi-class classifier is evaluated on a dataset with three classes $C_1$, $C_2$, and $C_3$. The total number of instances is $N = 1000$, with the true class counts given by $|C_1| = 950$, $|C_2| = 40$, and $|C_3| = 10$, making the dataset highly imbalanced. A $3 \\times 3$ confusion matrix records, for each true class (rows), how many instances were predicted into each class (columns). Assume rows correspond to true classes in the order $(C_1, C_2, C_3)$, and columns correspond to predicted classes in the order $(C_1, C_2, C_3)$.\n\nUsing standard definitions of per-class precision, per-class recall, and their harmonic mean (the $F_1$ score), define the macro-averaged $F_1$ as the unweighted average of per-class $F_1$ scores. Define the micro-averaged $F_1$ by first aggregating true positives, false positives, and false negatives across classes, then forming the $F_1$ score from these aggregated quantities.\n\nSelect all confusion matrices below that simultaneously satisfy all of the following:\n- Overall accuracy at least $0.90$.\n- Micro-averaged $F_1$ at least $0.90$.\n- Macro-averaged $F_1$ less than $0.50$.\n\nOption A:\n$$\n\\begin{bmatrix}\n950 & 0 & 0\\\\\n40 & 0 & 0\\\\\n10 & 0 & 0\n\\end{bmatrix}\n$$\n\nOption B:\n$$\n\\begin{bmatrix}\n930 & 15 & 5\\\\\n10 & 25 & 5\\\\\n3 & 2 & 5\n\\end{bmatrix}\n$$\n\nOption C:\n$$\n\\begin{bmatrix}\n940 & 5 & 5\\\\\n30 & 10 & 0\\\\\n8 & 2 & 0\n\\end{bmatrix}\n$$\n\nOption D:\n$$\n\\begin{bmatrix}\n700 & 200 & 50\\\\\n25 & 5 & 10\\\\\n5 & 3 & 2\n\\end{bmatrix}\n$$\n\nChoose all that apply: A, B, C, D.", "solution": "The problem asks to identify which of the given confusion matrices satisfy three conditions simultaneously: an overall accuracy of at least $0.90$, a micro-averaged $F_1$ score of at least $0.90$, and a macro-averaged $F_1$ score of less than $0.50$. The dataset has $N=1000$ instances distributed among three classes $C_1$, $C_2$, and $C_3$ with true counts $|C_1|=950$, $|C_2|=40$, and $|C_3|=10$.\n\nFirst, let's define the performance metrics based on the confusion matrix $M$, where $M_{ij}$ is the number of instances of true class $C_i$ predicted as class $C_j$.\n\n**Per-Class Metrics for Class $C_i$**:\n- True Positives ($TP_i$): $TP_i = M_{ii}$\n- False Positives ($FP_i$): $FP_i = \\left(\\sum_{k=1}^3 M_{ki}\\right) - M_{ii}$ (sum of column $i$ minus the diagonal element)\n- False Negatives ($FN_i$): $FN_i = \\left(\\sum_{j=1}^3 M_{ij}\\right) - M_{ii}$ (sum of row $i$ minus the diagonal element)\n- Precision ($P_i$): $P_i = \\frac{TP_i}{TP_i + FP_i}$. By convention, if $TP_i + FP_i = 0$, $P_i=0$.\n- Recall ($R_i$): $R_i = \\frac{TP_i}{TP_i + FN_i} = \\frac{TP_i}{|C_i|}$\n- $F_1$ Score ($F_{1,i}$): $F_{1,i} = 2 \\cdot \\frac{P_i \\cdot R_i}{P_i + R_i}$. If $P_i + R_i = 0$, then $F_{1,i}=0$.\n\n**Overall and Averaged Metrics**:\n- **Overall Accuracy ($Acc$)**: This is the fraction of correctly classified instances.\n$$Acc = \\frac{\\sum_{i=1}^3 TP_i}{N} = \\frac{\\sum_{i=1}^3 M_{ii}}{N}$$\n- **Micro-averaged $F_1$ Score ($F_{1,micro}$)**: This is calculated from aggregated counts.\n  - Aggregated True Positives: $TP_{micro} = \\sum_{i=1}^3 TP_i$\n  - Aggregated False Positives: $FP_{micro} = \\sum_{i=1}^3 FP_i$\n  - Aggregated False Negatives: $FN_{micro} = \\sum_{i=1}^3 FN_i$\n  In multi-class, single-label classification, $FP_{micro} = FN_{micro}$ because each misclassification is a false positive for one class and a false negative for another.\n  - Micro-Precision: $P_{micro} = \\frac{TP_{micro}}{TP_{micro} + FP_{micro}} = \\frac{\\sum_i M_{ii}}{\\sum_i \\sum_k M_{ki}} = \\frac{\\sum_i M_{ii}}{N} = Acc$\n  - Micro-Recall: $R_{micro} = \\frac{TP_{micro}}{TP_{micro} + FN_{micro}} = \\frac{\\sum_i M_{ii}}{\\sum_i \\sum_j M_{ij}} = \\frac{\\sum_i M_{ii}}{N} = Acc$\n  - Since $P_{micro} = R_{micro}$, the micro-averaged $F_1$ score is equal to both: $F_{1,micro} = P_{micro} = R_{micro} = Acc$.\n- **Macro-averaged $F_1$ Score ($F_{1,macro}$)**: This is the unweighted mean of per-class $F_1$ scores.\n$$F_{1,macro} = \\frac{1}{3} \\sum_{i=1}^3 F_{1,i}$$\n\n**Conditions to check**:\n1. $Acc \\ge 0.90$\n2. $F_{1,micro} \\ge 0.90$ (This is identical to condition 1)\n3. $F_{1,macro} < 0.50$\n\nWe will now evaluate each option.\n\n### Option A\nThe confusion matrix is:\n$$ M_A = \\begin{bmatrix} 950 & 0 & 0\\\\ 40 & 0 & 0\\\\ 10 & 0 & 0 \\end{bmatrix} $$\n1.  **Accuracy and Micro-F1**: The sum of diagonal elements is $950+0+0 = 950$.\n    $Acc = F_{1,micro} = \\frac{950}{1000} = 0.95$.\n    Since $0.95 \\ge 0.90$, the first two conditions are satisfied.\n\n2.  **Macro-F1**:\n    -   **Class $C_1$**: $TP_1=950$, $FN_1=0$. Column 1 sum is $950+40+10=1000$, so $FP_1=1000-950=50$.\n        $P_1 = \\frac{950}{950+50} = \\frac{950}{1000} = 0.95$.\n        $R_1 = \\frac{950}{950+0} = 1$.\n        $F_{1,1} = 2 \\cdot \\frac{0.95 \\cdot 1}{0.95+1} = \\frac{1.9}{1.95} = \\frac{38}{39}$.\n    -   **Class $C_2$**: $TP_2=0$, $FN_2=40$. Column 2 sum is $0$, so $FP_2=0$.\n        $P_2 = \\frac{0}{0+0} = 0$ (by convention).\n        $R_2 = \\frac{0}{0+40} = 0$.\n        $F_{1,2} = 0$.\n    -   **Class $C_3$**: $TP_3=0$, $FN_3=10$. Column 3 sum is $0$, so $FP_3=0$.\n        $P_3 = \\frac{0}{0+0} = 0$ (by convention).\n        $R_3 = \\frac{0}{0+10} = 0$.\n        $F_{1,3} = 0$.\n    -   **Macro-F1 Calculation**:\n        $F_{1,macro} = \\frac{1}{3} \\left( \\frac{38}{39} + 0 + 0 \\right) = \\frac{38}{117} \\approx 0.3248$.\n        Since $0.3248 < 0.50$, the third condition is satisfied.\n\n**Verdict for Option A**: Correct.\n\n### Option B\nThe confusion matrix is:\n$$ M_B = \\begin{bmatrix} 930 & 15 & 5\\\\ 10 & 25 & 5\\\\ 3 & 2 & 5 \\end{bmatrix} $$\n1.  **Accuracy and Micro-F1**: The sum of diagonal elements is $930+25+5 = 960$.\n    $Acc = F_{1,micro} = \\frac{960}{1000} = 0.96$.\n    Since $0.96 \\ge 0.90$, the first two conditions are satisfied.\n\n2.  **Macro-F1**:\n    -   **Class $C_1$**: $TP_1=930$, $FN_1=15+5=20$, $FP_1=10+3=13$.\n        $P_1 = \\frac{930}{930+13} = \\frac{930}{943}$. $R_1 = \\frac{930}{930+20} = \\frac{930}{950}$.\n        $F_{1,1} = 2 \\cdot \\frac{P_1 R_1}{P_1+R_1} = \\frac{2 \\cdot \\frac{930}{943} \\cdot \\frac{930}{950}}{\\frac{930}{943} + \\frac{930}{950}} = \\frac{2}{\\frac{943}{930} + \\frac{950}{930}} = \\frac{1860}{1893} \\approx 0.9826$.\n    -   **Class $C_2$**: $TP_2=25$, $FN_2=10+5=15$, $FP_2=15+2=17$.\n        $P_2 = \\frac{25}{25+17} = \\frac{25}{42}$. $R_2 = \\frac{25}{25+15} = \\frac{25}{40}$.\n        $F_{1,2} = \\frac{2}{\\frac{42}{25} + \\frac{40}{25}} = \\frac{50}{82} = \\frac{25}{41} \\approx 0.6098$.\n    -   **Class $C_3$**: $TP_3=5$, $FN_3=3+2=5$, $FP_3=5+5=10$.\n        $P_3 = \\frac{5}{5+10} = \\frac{5}{15} = \\frac{1}{3}$. $R_3 = \\frac{5}{5+5} = \\frac{5}{10} = \\frac{1}{2}$.\n        $F_{1,3} = 2 \\cdot \\frac{\\frac{1}{3} \\cdot \\frac{1}{2}}{\\frac{1}{3}+\\frac{1}{2}} = 2 \\cdot \\frac{\\frac{1}{6}}{\\frac{5}{6}} = \\frac{2}{5} = 0.4$.\n    -   **Macro-F1 Calculation**:\n        $F_{1,macro} = \\frac{1}{3} \\left( \\frac{1860}{1893} + \\frac{25}{41} + 0.4 \\right) \\approx \\frac{1}{3} (0.9826 + 0.6098 + 0.4) \\approx \\frac{1.9924}{3} \\approx 0.6641$.\n        Since $0.6641 \\not< 0.50$, the third condition is not satisfied.\n\n**Verdict for Option B**: Incorrect.\n\n### Option C\nThe confusion matrix is:\n$$ M_C = \\begin{bmatrix} 940 & 5 & 5\\\\ 30 & 10 & 0\\\\ 8 & 2 & 0 \\end{bmatrix} $$\n1.  **Accuracy and Micro-F1**: The sum of diagonal elements is $940+10+0=950$.\n    $Acc = F_{1,micro} = \\frac{950}{1000} = 0.95$.\n    Since $0.95 \\ge 0.90$, the first two conditions are satisfied.\n\n2.  **Macro-F1**:\n    -   **Class $C_1$**: $TP_1=940$, $FN_1=5+5=10$, $FP_1=30+8=38$.\n        $P_1 = \\frac{940}{940+38} = \\frac{940}{978}$. $R_1 = \\frac{940}{940+10} = \\frac{940}{950}$.\n        $F_{1,1} = \\frac{2}{\\frac{978}{940} + \\frac{950}{940}} = \\frac{1880}{1928} = \\frac{235}{241} \\approx 0.9751$.\n    -   **Class $C_2$**: $TP_2=10$, $FN_2=30+0=30$, $FP_2=5+2=7$.\n        $P_2 = \\frac{10}{10+7} = \\frac{10}{17}$. $R_2 = \\frac{10}{10+30} = \\frac{10}{40} = \\frac{1}{4}$.\n        $F_{1,2} = \\frac{2}{\\frac{17}{10} + \\frac{40}{10}} = \\frac{20}{57} \\approx 0.3509$.\n    -   **Class $C_3$**: $TP_3=0$, $FN_3=8+2=10$, $FP_3=5+0=5$.\n        $P_3 = \\frac{0}{0+5} = 0$. $R_3 = \\frac{0}{0+10} = 0$.\n        $F_{1,3} = 0$.\n    -   **Macro-F1 Calculation**:\n        $F_{1,macro} = \\frac{1}{3} \\left( \\frac{235}{241} + \\frac{20}{57} + 0 \\right) \\approx \\frac{1}{3} (0.9751 + 0.3509) \\approx \\frac{1.3260}{3} \\approx 0.4420$.\n        Since $0.4420 < 0.50$, the third condition is satisfied.\n\n**Verdict for Option C**: Correct.\n\n### Option D\nThe confusion matrix is:\n$$ M_D = \\begin{bmatrix} 700 & 200 & 50\\\\ 25 & 5 & 10\\\\ 5 & 3 & 2 \\end{bmatrix} $$\n1.  **Accuracy and Micro-F1**: The sum of diagonal elements is $700+5+2=707$.\n    $Acc = F_{1,micro} = \\frac{707}{1000} = 0.707$.\n    Since $0.707 \\not\\ge 0.90$, the first two conditions are not satisfied. We do not need to proceed further.\n\n**Verdict for Option D**: Incorrect.\n\nIn summary, only options A and C satisfy all three specified criteria.", "answer": "$$\\boxed{AC}$$", "id": "3181107"}, {"introduction": "An evaluation metric's true power is unlocked when we can directly train a model to optimize for it. Standard metrics like the $F_1$-score are non-differentiable, making them incompatible with gradient-based optimization. This advanced coding practice guides you through the process of designing a 'soft' or differentiable surrogate for the macro-$F_1$ score, a key technique for training deep learning models that are better aligned with complex, real-world objectives [@problem_id:3182549].", "problem": "You are given a finite dataset of inputs with associated ground-truth class labels and real-valued model scores per class. Consider a $K$-class single-label classification problem with $N$ samples. For each input $i \\in \\{1,\\dots,N\\}$ and class $c \\in \\{0,\\dots,K-1\\}$, the model produces a score $f_\\theta(x_i)_c \\in [0,1]$. A hard classifier would predict class membership for class $c$ by comparing $f_\\theta(x_i)_c$ to a threshold $\\tau \\in [0,1]$ and using the indicator function $I\\{f_\\theta(x_i)_c \\ge \\tau\\}$. For each class $c$, define the standard confusion-matrix-derived counts under one-vs-rest evaluation: true positives $\\mathrm{TP}_c$, false positives $\\mathrm{FP}_c$, and false negatives $\\mathrm{FN}_c$, all computed by summing over samples the appropriate indicator-weighted contributions of the ground-truth labels. The macro-$F_1$ score is the arithmetic mean of the class-wise $F_1$ scores, where the class-wise $F_1$ score is defined from $\\mathrm{TP}_c$, $\\mathrm{FP}_c$, and $\\mathrm{FN}_c$ in the usual way.\n\nYour task is to replace the non-differentiable indicator $I\\{\\cdot\\}$ with a smooth, differentiable function so that the resulting macro-$F_1$ objective becomes differentiable with respect to both the model outputs and the threshold $\\tau$. Use the logistic (sigmoid) function as a smooth, monotone approximation to the indicator with a tunable steepness parameter $\\alpha > 0$, specifically $s(z) = \\frac{1}{1 + e^{-\\alpha z}}$, where $z$ is the margin $f_\\theta(x_i)_c - \\tau$. Using this smooth surrogate in place of the indicator, construct differentiable analogues of $\\mathrm{TP}_c$, $\\mathrm{FP}_c$, and $\\mathrm{FN}_c$, then define a differentiable surrogate of the macro-$F_1$ score and the corresponding loss as one minus the surrogate macro-$F_1$. Include a small positive constant $\\varepsilon$ in any denominator where needed for numerical stability.\n\nImplement a program that:\n- Accepts no input and uses the predefined test suite below.\n- For each test case, computes the differentiable surrogate loss as described above.\n- Produces a single line of output containing the losses for all test cases as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal places, for example, $\"[0.123456,0.000001]\"$.\n\nBase definitions you must start from:\n- For a given class $c$, the one-vs-rest hard counts are defined by summing over samples $i$:\n  - $\\mathrm{TP}_c$ counts samples with true label $c$ and predicted as class $c$.\n  - $\\mathrm{FP}_c$ counts samples with true label not equal to $c$ but predicted as class $c$.\n  - $\\mathrm{FN}_c$ counts samples with true label $c$ but not predicted as class $c$.\n- The class-wise $F_1$ score for class $c$ is defined in terms of $\\mathrm{TP}_c$, $\\mathrm{FP}_c$, and $\\mathrm{FN}_c$.\n- The macro-$F_1$ score is the arithmetic mean of the class-wise $F_1$ scores across all classes.\n\nYour differentiable design must be derived by replacing the hard indicator with the logistic function $s(z)$ applied to the margin $z = f_\\theta(x_i)_c - \\tau$ and then aggregating soft contributions per class.\n\nTest suite (each case specifies the labels vector, the score matrix, and the hyperparameters $(\\alpha,\\tau,\\varepsilon)$). For each matrix below, the $i$-th row corresponds to sample $i$ and the $c$-th column corresponds to class $c$:\n- Case $1$ (binary, reasonably calibrated scores): $K=2$, $N=6$,\n  labels $y^{(1)} = [\\,1,\\,0,\\,1,\\,0,\\,1,\\,0\\,]$,\n  scores\n  $$S^{(1)} = \\begin{bmatrix}\n  0.8 & 0.2\\\\\n  0.3 & 0.7\\\\\n  0.6 & 0.4\\\\\n  0.4 & 0.6\\\\\n  0.55 & 0.45\\\\\n  0.2 & 0.8\n  \\end{bmatrix},\\quad (\\alpha,\\tau,\\varepsilon) = (20.0,\\,0.5,\\,10^{-8}).$$\n- Case $2$ (multi-class, near-perfect predictions): $K=3$, $N=5$,\n  labels $y^{(2)} = [\\,0,\\,1,\\,2,\\,1,\\,0\\,]$,\n  scores\n  $$S^{(2)} = \\begin{bmatrix}\n  0.99 & 0.005 & 0.005\\\\\n  0.01 & 0.98 & 0.01\\\\\n  0.02 & 0.01 & 0.97\\\\\n  0.005 & 0.99 & 0.005\\\\\n  0.97 & 0.02 & 0.01\n  \\end{bmatrix},\\quad (\\alpha,\\tau,\\varepsilon) = (30.0,\\,0.5,\\,10^{-8}).$$\n- Case $3$ (binary, high threshold suppresses positives): $K=2$, $N=4$,\n  labels $y^{(3)} = [\\,1,\\,0,\\,1,\\,0\\,]$,\n  scores\n  $$S^{(3)} = \\begin{bmatrix}\n  0.2 & 0.8\\\\\n  0.3 & 0.7\\\\\n  0.4 & 0.6\\\\\n  0.1 & 0.9\n  \\end{bmatrix},\\quad (\\alpha,\\tau,\\varepsilon) = (20.0,\\,0.9,\\,10^{-8}).$$\n- Case $4$ (multi-class, one class absent in labels but present in predictions): $K=3$, $N=6$,\n  labels $y^{(4)} = [\\,0,\\,1,\\,1,\\,0,\\,1,\\,0\\,]$,\n  scores\n  $$S^{(4)} = \\begin{bmatrix}\n  0.6 & 0.2 & 0.2\\\\\n  0.2 & 0.5 & 0.3\\\\\n  0.1 & 0.6 & 0.3\\\\\n  0.55 & 0.15 & 0.3\\\\\n  0.3 & 0.5 & 0.2\\\\\n  0.4 & 0.2 & 0.4\n  \\end{bmatrix},\\quad (\\alpha,\\tau,\\varepsilon) = (25.0,\\,0.5,\\,10^{-8}).$$\n- Case $5$ (multi-class, very soft surrogate due to small steepness): $K=3$, $N=5$,\n  labels $y^{(5)} = [\\,0,\\,1,\\,2,\\,1,\\,0\\,]$,\n  scores\n  $$S^{(5)} = \\begin{bmatrix}\n  0.6 & 0.3 & 0.1\\\\\n  0.2 & 0.5 & 0.3\\\\\n  0.3 & 0.2 & 0.5\\\\\n  0.4 & 0.45 & 0.15\\\\\n  0.55 & 0.25 & 0.2\n  \\end{bmatrix},\\quad (\\alpha,\\tau,\\varepsilon) = (1.0,\\,0.5,\\,10^{-8}).$$\n\nYour program should produce a single line of output containing the five losses as a comma-separated list enclosed in square brackets, rounded to $6$ decimal places, with no spaces, in the order of the cases above, for example, $\"[a_1,a_2,a_3,a_4,a_5]\"$ where each $a_j$ is a decimal representation rounded to $6$ places.", "solution": "The problem requires the formulation and implementation of a differentiable surrogate for the macro-$F_1$ loss function, commonly used in multi-class classification tasks. The standard macro-$F_1$ score is non-differentiable due to its reliance on the indicator function, $I\\{\\cdot\\}$, which makes it unsuitable for direct use as a loss function in gradient-based optimization algorithms like stochastic gradient descent. The task is to replace this discontinuous function with a smooth approximation, specifically the logistic (sigmoid) function, and derive the corresponding differentiable loss.\n\nThe framework is a $K$-class classification problem with $N$ samples. The model outputs a matrix of scores $S$, where $S_{ic} = f_\\theta(x_i)_c \\in [0,1]$ is the score for the $i$-th sample belonging to the $c$-th class. A prediction for class $c$ is made by thresholding this score at a value $\\tau \\in [0,1]$.\n\nFirst, we define the smooth surrogate for the indicator function. The hard prediction for sample $i$ and class $c$ is based on the indicator function applied to the margin, $I\\{S_{ic} - \\tau \\ge 0\\}$. We replace this with the logistic (sigmoid) function, $s(z)$, which is a smooth, differentiable, and monotonic approximation of the Heaviside step function. The argument to the sigmoid, $z$, is the margin $z_{ic} = S_{ic} - \\tau$. The steepness of the sigmoid, which controls how closely it approximates the step function, is governed by a parameter $\\alpha > 0$. The soft, differentiable prediction, $\\tilde{y}_{ic}$, is thus given by:\n$$ \\tilde{y}_{ic} = s(\\alpha(S_{ic} - \\tau)) = \\frac{1}{1 + e^{-\\alpha(S_{ic} - \\tau)}} $$\nThis value $\\tilde{y}_{ic}$ can be interpreted as a soft probability that sample $i$ is predicted as class $c$.\n\nNext, we construct the differentiable analogues of the confusion matrix counts for each class $c$ in a one-vs-rest manner. Let $Y$ be the one-hot encoded matrix of ground-truth labels, where $y_{ic}=1$ if the true label of sample $i$ is $c$, and $y_{ic}=0$ otherwise. The standard \"hard\" counts are sums of products involving indicator functions. By replacing the indicator-based hard predictions with our soft predictions $\\tilde{y}_{ic}$, we obtain the differentiable or \"soft\" counts:\n\n-   **Soft True Positives ($\\widetilde{\\mathrm{TP}}_c$)**: The sum of soft predictions for samples that truly belong to class $c$.\n    $$ \\widetilde{\\mathrm{TP}}_c = \\sum_{i=1}^N y_{ic} \\cdot \\tilde{y}_{ic} $$\n-   **Soft False Positives ($\\widetilde{\\mathrm{FP}}_c$)**: The sum of soft predictions for samples that do not belong to class $c$.\n    $$ \\widetilde{\\mathrm{FP}}_c = \\sum_{i=1}^N (1 - y_{ic}) \\cdot \\tilde{y}_{ic} $$\n-   **Soft False Negatives ($\\widetilde{\\mathrm{FN}}_c$)**: The sum of \"missed\" probabilities for samples that truly belong to class $c$.\n    $$ \\widetilde{\\mathrm{FN}}_c = \\sum_{i=1}^N y_{ic} \\cdot (1 - \\tilde{y}_{ic}) $$\n\nWith these differentiable counts, we can define a surrogate for the class-wise $F_1$ score. The standard $F_1$ score for class $c$ is $F_{1,c} = \\frac{2 \\cdot \\mathrm{TP}_c}{2 \\cdot \\mathrm{TP}_c + \\mathrm{FP}_c + \\mathrm{FN}_c}$. To create a numerically stable and differentiable version, we substitute the soft counts and add a small positive constant $\\varepsilon$ to the numerator and denominator to prevent division by zero, especially when a class might have no true positives.\n$$ \\widetilde{F}_{1,c} = \\frac{2 \\cdot \\widetilde{\\mathrm{TP}}_c + \\varepsilon}{2 \\cdot \\widetilde{\\mathrm{TP}}_c + \\widetilde{\\mathrm{FP}}_c + \\widetilde{\\mathrm{FN}}_c + \\varepsilon} $$\nThe term $2 \\cdot \\widetilde{\\mathrm{TP}}_c + \\widetilde{\\mathrm{FP}}_c + \\widetilde{\\mathrm{FN}}_c$ simplifies to $\\sum_{i=1}^N \\tilde{y}_{ic} + \\sum_{i=1}^N y_{ic}$, which is the sum of total soft predictions and total true instances for class $c$.\n\nThe macro-$F_1$ score is the arithmetic mean of the class-wise $F_1$ scores. The differentiable surrogate follows this definition:\n$$ \\widetilde{\\text{macro-}F_1} = \\frac{1}{K} \\sum_{c=0}^{K-1} \\widetilde{F}_{1,c} $$\n\nFinally, the objective to be minimized, the loss $\\mathcal{L}$, is defined as one minus the surrogate macro-$F_1$ score. This transforms the maximization of the score into a minimization problem suitable for gradient-based learning.\n$$ \\mathcal{L} = 1 - \\widetilde{\\text{macro-}F_1} $$\nThis entire computational pipeline is differentiable with respect to the model scores $S_{ic}$ (and thus the model parameters $\\theta$) and the threshold $\\tau$, allowing it to be used as a training objective.\n\nThe implementation computes these quantities for the given test cases. For each case, it takes the labels, scores, and hyperparameters $(\\alpha, \\tau, \\varepsilon)$. It first constructs the one-hot ground-truth matrix $Y$, then the soft prediction matrix $\\tilde{Y}$. It computes the vectors of soft counts $\\widetilde{\\mathrm{TP}}$, $\\widetilde{\\mathrm{FP}}$, and $\\widetilde{\\mathrm{FN}}$ using vectorized array operations. From these, the class-wise surrogate $F_1$ scores are calculated, followed by the macro-average and the final loss.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating the differentiable surrogate F1 loss\n    for a predefined suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"labels\": [1, 0, 1, 0, 1, 0],\n            \"scores\": np.array([\n                [0.8, 0.2],\n                [0.3, 0.7],\n                [0.6, 0.4],\n                [0.4, 0.6],\n                [0.55, 0.45],\n                [0.2, 0.8]\n            ]),\n            \"params\": (20.0, 0.5, 1e-8)\n        },\n        {\n            \"labels\": [0, 1, 2, 1, 0],\n            \"scores\": np.array([\n                [0.99, 0.005, 0.005],\n                [0.01, 0.98, 0.01],\n                [0.02, 0.01, 0.97],\n                [0.005, 0.99, 0.005],\n                [0.97, 0.02, 0.01]\n            ]),\n            \"params\": (30.0, 0.5, 1e-8)\n        },\n        {\n            \"labels\": [1, 0, 1, 0],\n            \"scores\": np.array([\n                [0.2, 0.8],\n                [0.3, 0.7],\n                [0.4, 0.6],\n                [0.1, 0.9]\n            ]),\n            \"params\": (20.0, 0.9, 1e-8)\n        },\n        {\n            \"labels\": [0, 1, 1, 0, 1, 0],\n            \"scores\": np.array([\n                [0.6, 0.2, 0.2],\n                [0.2, 0.5, 0.3],\n                [0.1, 0.6, 0.3],\n                [0.55, 0.15, 0.3],\n                [0.3, 0.5, 0.2],\n                [0.4, 0.2, 0.4]\n            ]),\n            \"params\": (25.0, 0.5, 1e-8)\n        },\n        {\n            \"labels\": [0, 1, 2, 1, 0],\n            \"scores\": np.array([\n                [0.6, 0.3, 0.1],\n                [0.2, 0.5, 0.3],\n                [0.3, 0.2, 0.5],\n                [0.4, 0.45, 0.15],\n                [0.55, 0.25, 0.2]\n            ]),\n            \"params\": (1.0, 0.5, 1e-8)\n        }\n    ]\n\n    def calculate_loss(labels, scores, alpha, tau, epsilon):\n        \"\"\"\n        Computes the differentiable F1-surrogate loss for a single case.\n        \n        Args:\n            labels (list): A list of ground-truth integer class labels.\n            scores (np.ndarray): An N x K matrix of model scores.\n            alpha (float): The steepness parameter for the sigmoid function.\n            tau (float): The classification threshold.\n            epsilon (float): A small constant for numerical stability.\n\n        Returns:\n            float: The computed loss value.\n        \"\"\"\n        labels = np.array(labels)\n        N, K = scores.shape\n        \n        # Create one-hot encoded ground truth matrix Y\n        Y_one_hot = np.zeros((N, K))\n        Y_one_hot[np.arange(N), labels] = 1.0\n        \n        # Calculate soft predictions matrix Y_tilde using the sigmoid surrogate\n        margins = scores - tau\n        Y_tilde = 1.0 / (1.0 + np.exp(-alpha * margins))\n        \n        # Calculate surrogate counts (TP, FP, FN) for each class\n        # Results are vectors of length K.\n        tp_surrogate = np.sum(Y_one_hot * Y_tilde, axis=0)\n        fp_surrogate = np.sum((1.0 - Y_one_hot) * Y_tilde, axis=0)\n        fn_surrogate = np.sum(Y_one_hot * (1.0 - Y_tilde), axis=0)\n        \n        # Calculate class-wise surrogate F1 scores\n        numerator = 2.0 * tp_surrogate + epsilon\n        denominator = 2.0 * tp_surrogate + fp_surrogate + fn_surrogate + epsilon\n        \n        f1_classwise = numerator / denominator\n        \n        # Calculate macro-F1 surrogate\n        macro_f1 = np.mean(f1_classwise)\n        \n        # Calculate final loss\n        loss = 1.0 - macro_f1\n        \n        return loss\n\n    results = []\n    for case in test_cases:\n        result = calculate_loss(case[\"labels\"], case[\"scores\"], *case[\"params\"])\n        # Format the result to 6 decimal places as a string\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3182549"}]}