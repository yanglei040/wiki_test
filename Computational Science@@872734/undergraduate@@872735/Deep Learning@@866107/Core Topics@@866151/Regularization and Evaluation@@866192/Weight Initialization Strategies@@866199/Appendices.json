{"hands_on_practices": [{"introduction": "Training very deep neural networks was historically challenging due to the vanishing or exploding of signals as they propagate through the layers. This exercise allows you to empirically investigate the solution: modern weight initialization strategies designed to preserve signal variance. By implementing and testing Xavier and He initializations, you will directly observe how they are matched with specific activation functions ($\\tanh$ and $\\mathrm{ReLU}$, respectively) to ensure stable signal flow, a critical foundation for successful deep learning [@problem_id:3199598].", "problem": "Given a fully connected feedforward network with $L$ layers, define the pre-activation at layer $l$ as $z^{(l)} = W^{(l)} a^{(l-1)}$ and the post-activation as $a^{(l)} = \\phi\\!\\left(z^{(l)}\\right)$, with $a^{(0)} = x$. Assume zero biases, independently and identically distributed weights, and an input $x \\in \\mathbb{R}^{n}$ whose components are independent, have zero mean, and finite variance. Consider two widely used random weight initialization strategies: Xavier (Glorot) normal initialization and He (Kaiming) normal initialization, and two activation functions: $\\tanh$ and Rectified Linear Unit ($\\mathrm{ReLU}$). The objective is to empirically verify, by Monte Carlo simulation, when an initialization strategy approximately preserves the variance of pre-activations across layers, that is, when $\\operatorname{Var}\\!\\left(z^{(l)}\\right) \\approx \\operatorname{Var}(x)$ for all layers $l \\in \\{1,\\dots,L\\}$ under a given activation function.\n\nFundamental base to use:\n- Independence of weights and activations across coordinates at initialization, and linearity of variance for independent sums.\n- The definitions of $\\tanh$ and $\\mathrm{ReLU}$ as pointwise nonlinearities.\n- The sample variance estimator defined for an array $Y \\in \\mathbb{R}^{s \\times d}$ along the $s$ samples as $\\widehat{\\operatorname{Var}}(Y) = \\frac{1}{d}\\sum_{j=1}^{d} \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}^{2} - \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}\\right)^{2} \\right)$.\n\nYour program must:\n1. Construct networks with specified $L$ and layer widths, where each layer has shape $(n_{\\text{in}}, n_{\\text{out}})$ and here $n_{\\text{in}} = n_{\\text{out}}$ for simplicity, using either Xavier normal or He normal initialization for each layer’s weights $W^{(l)}$. Use zero-mean normal initializations with variances prescribed by each strategy; do not add any biases.\n2. Draw the input $x$ as $s$ independent samples of dimension $n$, each component distributed as $\\mathcal{N}(0,1)$, i.e., zero mean and variance $1$.\n3. For each layer $l$, compute the empirical pre-activation variance $\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right)$ over the $s$ samples by averaging per-coordinate sample variances, and compute the empirical input variance $\\widehat{\\operatorname{Var}}(x)$ similarly. Define the relative deviation at layer $l$ as $$\\delta^{(l)} = \\frac{\\left|\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right) - \\widehat{\\operatorname{Var}}(x)\\right|}{\\widehat{\\operatorname{Var}}(x)}.$$ A test case is deemed to preserve variance if $\\max_{1 \\le l \\le L} \\delta^{(l)} \\le \\varepsilon$, with tolerance $\\varepsilon = 0.25$.\n4. Use a pseudo-random generator with a fixed seed $12345$ to ensure reproducibility.\n\nTest suite:\n- Case $1$: $L=5$, width $n=32$, samples $s=8000$, activation $\\tanh$, initialization Xavier normal.\n- Case $2$: $L=5$, width $n=32$, samples $s=8000$, activation $\\tanh$, initialization He normal.\n- Case $3$: $L=5$, width $n=32$, samples $s=8000$, activation $\\mathrm{ReLU}$, initialization He normal.\n- Case $4$: $L=5$, width $n=32$, samples $s=8000$, activation $\\mathrm{ReLU}$, initialization Xavier normal.\n- Case $5$: $L=1$, width $n=32$, samples $s=20000$, activation $\\tanh$, initialization Xavier normal.\n- Case $6$: $L=1$, width $n=32$, samples $s=20000$, activation $\\mathrm{ReLU}$, initialization He normal.\n- Case $7$: $L=15$, width $n=16$, samples $s=8000$, activation $\\tanh$, initialization Xavier normal.\n- Case $8$: $L=15$, width $n=16$, samples $s=8000$, activation $\\mathrm{ReLU}$, initialization He normal.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$), where each $\\text{result}_i$ is a boolean indicating whether variance was preserved for the $i$-th test case, in the exact order of the test suite above.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the established principles of deep learning, specifically concerning weight initialization and its impact on signal propagation. The problem is well-posed, providing all necessary parameters, definitions, and a clear, objective criterion for success. It is free of contradictions, ambiguities, and factual errors. We may therefore proceed with a solution.\n\nThe objective is to empirically verify the conditions under which the variance of pre-activations, $z^{(l)}$, is preserved across the layers of a deep neural network. The core of this analysis lies in the recursive relationship between the variance of pre-activations in successive layers.\n\nLet us consider a single neuron's pre-activation at layer $l$:\n$$ z_i^{(l)} = \\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)} $$\nHere, $W_{ij}^{(l)}$ is the weight connecting neuron $j$ of layer $l-1$ to neuron $i$ of layer $l$, and $a_j^{(l-1)}$ is the activation of neuron $j$ from the previous layer. The problem specifies that biases are zero, weights $W_{ij}^{(l)}$ are drawn from a distribution with zero mean, and the input components $x_j$ (which constitute $a_j^{(0)}$) also have zero mean. We assume that at initialization, the activations $a_j^{(l-1)}$ for all $j$ are independent of the weights $W_{ij}^{(l)}$ and are identically distributed. Furthermore, if the activations $a^{(l-1)}$ are the output of a symmetric activation function applied to zero-mean inputs $z^{(l-1)}$, they will also have zero mean. For the $\\mathrm{ReLU}$ activation, this is not the case, but the resulting pre-activations $z^{(l)}$ will still have zero mean because the weights themselves are zero-mean: $\\mathbb{E}[z_i^{(l)}] = \\sum_j \\mathbb{E}[W_{ij}^{(l)}] \\mathbb{E}[a_j^{(l-1)}] = \\sum_j 0 \\cdot \\mathbb{E}[a_j^{(l-1)}] = 0$.\n\nUnder these conditions, the variance of $z_i^{(l)}$ is given by:\n$$ \\operatorname{Var}(z_i^{(l)}) = \\operatorname{Var}\\left(\\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)}\\right) $$\nDue to the independence of the terms in the sum (as weights and previous layer activations are independent), the variance of the sum is the sum of the variances:\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)} a_j^{(l-1)}) $$\nFor two independent random variables $U$ and $V$ with at least one having zero mean (e.g., $\\mathbb{E}[U]=0$), $\\operatorname{Var}(UV) = \\mathbb{E}[U^2V^2] - (\\mathbb{E}[UV])^2 = \\mathbb{E}[U^2]\\mathbb{E}[V^2] - (\\mathbb{E}[U]\\mathbb{E}[V])^2 = \\operatorname{Var}(U)\\operatorname{Var}(V)$. Since $\\mathbb{E}[W_{ij}^{(l)}] = 0$, we have:\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)}) \\operatorname{Var}(a_j^{(l-1)}) $$\nAssuming all weights in layer $l$ are drawn i.i.d. with variance $\\operatorname{Var}(W^{(l)})$ and all activations from layer $l-1$ are i.i.d. with variance $\\operatorname{Var}(a^{(l-1)})$, this simplifies to:\n$$ \\operatorname{Var}(z^{(l)}) = n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(a^{(l-1)}) $$\nwhere $n_{l-1}$ is the number of neurons in layer $l-1$. To maintain stable signal propagation, we require the variance to be preserved, i.e., $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$. This requires a careful choice of the weight initialization variance, $\\operatorname{Var}(W^{(l)})$, to counteract the effect of the activation function $\\phi$ on the variance, which is captured by the term $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\phi(z^{(l-1)}))$.\n\n**Activation Function Analysis**\n\n1.  **$\\tanh$ Activation:** The hyperbolic tangent function, $\\tanh(z)$, is symmetric around the origin ($\\tanh(0)=0$) and behaves like the identity function for small inputs ($\\tanh(z) \\approx z$ for $z \\approx 0$). If we assume the pre-activations $z^{(l-1)}$ are concentrated around zero, which is a desirable state during initial training phases, then $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\tanh(z^{(l-1)})) \\approx \\operatorname{Var}(z^{(l-1)})$. Substituting this into our propagation equation yields:\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(z^{(l-1)}) $$\n    To achieve $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$, we must set $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$.\n    **Xavier (Glorot) normal initialization** is designed precisely for this situation. It sets the variance of the weights $W^{(l)}$ drawn from $\\mathcal{N}(0, \\sigma^2)$ as:\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1} + n_l} $$\n    In our specific problem, $n_{l-1} = n_l = n$, so this becomes $\\operatorname{Var}(W^{(l)}) = \\frac{2}{2n} = \\frac{1}{n} = \\frac{1}{n_{l-1}}$. This choice perfectly satisfies the condition $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$. Thus, Xavier initialization is expected to preserve variance for the $\\tanh$ activation function.\n\n2.  **$\\mathrm{ReLU}$ Activation:** The Rectified Linear Unit, $\\mathrm{ReLU}(z) = \\max(0, z)$, is not symmetric. For a zero-mean, symmetric input distribution for $z^{(l-1)}$ (like a Gaussian), exactly half of the inputs will be set to zero. This affects the variance. Let $z \\sim \\mathcal{N}(0, \\sigma_z^2)$. The variance of the output $a = \\mathrm{ReLU}(z)$ is $\\operatorname{Var}(a) = \\mathbb{E}[a^2] - (\\mathbb{E}[a])^2$.\n    The expectation of the squared activation is $\\mathbb{E}[a^2] = \\mathbb{E}[\\max(0, z)^2] = \\int_0^\\infty z^2 p(z) dz$. Due to the symmetry of the normal distribution $p(z)$, this integral is half of the total integral for $z^2$: $\\mathbb{E}[a^2] = \\frac{1}{2} \\int_{-\\infty}^\\infty z^2 p(z) dz = \\frac{1}{2}\\mathbb{E}[z^2] = \\frac{1}{2}\\operatorname{Var}(z)$.\n    So, for $\\mathrm{ReLU}$, we have $\\operatorname{Var}(a^{(l-1)}) \\approx \\frac{1}{2}\\operatorname{Var}(z^{(l-1)})$. The variance propagation equation becomes:\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\left(\\frac{1}{2}\\operatorname{Var}(z^{(l-1)})\\right) $$\n    To preserve variance, we must have $n_{l-1} \\operatorname{Var}(W^{(l)}) \\frac{1}{2} = 1$, which implies $\\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}}$.\n    **He (Kaiming) normal initialization** is designed for this case. It sets the variance as:\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}} $$\n    This choice satisfies the condition exactly. Therefore, He initialization is expected to preserve variance for the $\\mathrm{ReLU}$ activation function. Mismatched pairs (e.g., $\\tanh$ with He, $\\mathrm{ReLU}$ with Xavier) are predicted to lead to exploding or vanishing variances, respectively.\n\n**Simulation Procedure**\n\nThe program will implement a Monte Carlo simulation for each of the $8$ test cases. For each case:\n1.  A pseudo-random number generator is seeded with the value $12345$ to ensure reproducibility.\n2.  An input data matrix $x \\in \\mathbb{R}^{s \\times n}$ is generated, where each element is drawn from $\\mathcal{N}(0, 1)$. The empirical input variance, $\\widehat{\\operatorname{Var}}(x)$, is calculated using the provided formula.\n3.  The network is processed layer by layer, from $l=1$ to $L$. In each layer, the weight matrix $W^{(l)}$ is initialized from a zero-mean normal distribution with the variance dictated by the specified strategy (Xavier or He).\n4.  The pre-activations $z^{(l)} = a^{(l-1)} W^{(l)}$ are computed.\n5.  The empirical variance $\\widehat{\\operatorname{Var}}(z^{(l)})$ is computed. The relative deviation $\\delta^{(l)} = |\\widehat{\\operatorname{Var}}(z^{(l)}) - \\widehat{\\operatorname{Var}}(x)| / \\widehat{\\operatorname{Var}}(x)$ is calculated.\n6.  The maximum relative deviation across all layers, $\\max_{1 \\le l \\le L} \\delta^{(l)}$, is tracked.\n7.  The post-activations $a^{(l)} = \\phi(z^{(l)})$ are computed to serve as input for the next layer.\n8.  After iterating through all layers, the test case is deemed to preserve variance if $\\max_{l} \\delta^{(l)} \\le \\varepsilon$, where $\\varepsilon = 0.25$. This check will yield a boolean result for each case, which is then reported.", "answer": "```python\nimport numpy as np\n\ndef tanh(x):\n    \"\"\"Hyperbolic tangent activation function.\"\"\"\n    return np.tanh(x)\n\ndef relu(x):\n    \"\"\"Rectified Linear Unit (ReLU) activation function.\"\"\"\n    return np.maximum(0, x)\n\ndef empirical_variance(Y):\n    \"\"\"\n    Computes the empirical variance as defined in the problem statement.\n    For an array Y of shape (s, d), this is the average of the biased sample \n    variances of each of the d columns.\n    \n    Formula: Var(Y) = (1/d) * sum_j [ (1/s) * sum_i(Y_ij^2) - ((1/s) * sum_i(Y_ij))^2 ]\n    This is equivalent to the mean of np.var(Y, axis=0).\n    \"\"\"\n    if Y.ndim == 1:\n        # If input is a 1D array, treat it as (s, 1)\n        return np.var(Y)\n    return np.mean(np.var(Y, axis=0))\n\ndef run_simulation(L, n, s, activation_func, init_strategy, seed, epsilon):\n    \"\"\"\n    Runs a single simulation for a given network configuration.\n\n    Args:\n        L (int): Number of layers.\n        n (int): Width of each layer.\n        s (int): Number of samples.\n        activation_func (callable): The activation function to use.\n        init_strategy (str): The weight initialization strategy ('xavier' or 'he').\n        seed (int): The seed for the random number generator.\n        epsilon (float): The tolerance for variance preservation.\n\n    Returns:\n        bool: True if variance is preserved, False otherwise.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate input data x of shape (s, n) from N(0, 1)\n    x = rng.normal(loc=0.0, scale=1.0, size=(s, n))\n    var_x = empirical_variance(x)\n    \n    a_prev = x\n    max_deviation = 0.0\n    \n    # Propagate through the network for L layers\n    for l in range(1, L + 1):\n        # Layer dimensions are (n_in, n_out)\n        n_in = n\n        n_out = n\n        \n        # Determine standard deviation for weight initialization\n        if init_strategy == 'xavier':\n            # Variance = 2 / (n_in + n_out)\n            std_dev = np.sqrt(2.0 / (n_in + n_out))\n        elif init_strategy == 'he':\n            # Variance = 2 / n_in\n            std_dev = np.sqrt(2.0 / n_in)\n        else:\n            raise ValueError(f\"Unknown initialization strategy: {init_strategy}\")\n            \n        # Initialize weights W of shape (n_in, n_out)\n        W = rng.normal(loc=0.0, scale=std_dev, size=(n_in, n_out))\n        \n        # Compute pre-activations z = a_prev @ W of shape (s, n_out)\n        z = a_prev @ W\n        \n        # Compute empirical variance and relative deviation\n        var_z = empirical_variance(z)\n        if var_x > 1e-9: # Avoid division by zero\n            deviation = np.abs(var_z - var_x) / var_x\n        else:\n            deviation = np.abs(var_z)\n\n        if deviation > max_deviation:\n            max_deviation = deviation\n            \n        # Compute post-activations for the next layer\n        a_prev = activation_func(z)\n        \n    # Check if the maximum deviation is within the tolerance\n    return max_deviation = epsilon\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define test cases as (L, n, s, activation_name, init_strategy_name)\n    test_cases = [\n        (5, 32, 8000, 'tanh', 'xavier'),\n        (5, 32, 8000, 'tanh', 'he'),\n        (5, 32, 8000, 'relu', 'he'),\n        (5, 32, 8000, 'relu', 'xavier'),\n        (1, 32, 20000, 'tanh', 'xavier'),\n        (1, 32, 20000, 'relu', 'he'),\n        (15, 16, 8000, 'tanh', 'xavier'),\n        (15, 16, 8000, 'relu', 'he'),\n    ]\n    \n    # Map string names to functions and parameters\n    activation_map = {'tanh': tanh, 'relu': relu}\n    epsilon = 0.25\n    seed = 12345\n    \n    results = []\n    \n    # Run simulation for each test case\n    for case in test_cases:\n        L, n, s, act_name, init_name = case\n        activation_func = activation_map[act_name]\n        \n        # Each test case is an independent experiment, so we use the same seed,\n        # creating a new RNG instance for each to ensure reproducibility.\n        result = run_simulation(L, n, s, activation_func, init_name, seed, epsilon)\n        results.append(result)\n        \n    # Format and print the final output as a single-line list of booleans\n    # e.g., [True,False,True,False,True,True,True,True]\n    # np.bool_ maps to Python's True/False, and str() correctly converts them.\n    # The problem asks for booleans, and Python's `str(True)` is 'True'.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solution\nsolve()\n```", "id": "3199598"}, {"introduction": "Effective weight initialization does more than just prevent signals from disappearing; it establishes a fertile ground for the learning process to begin. This practice explores the immediate impact of initialization choices on the structure of the features learned in the very first training update. You will design a computational experiment to connect the initial weight variance in a $\\mathrm{ReLU}$ network to the resulting activation sparsity and the \"effective dimensionality\" of the learned hidden representations, providing a deeper understanding of how initialization shapes early learning dynamics [@problem_id:3199561].", "problem": "You are to design and implement a computational protocol to measure neuron activation sparsity at initialization for Rectified Linear Unit (ReLU) layers under differing weight variances, and to relate this sparsity to the effective dimensionality of features after the first epoch of training. The protocol must be instantiated as a complete runnable program that creates a synthetic dataset, initializes a one-hidden-layer network, measures the initialization sparsity for different variance hyperparameters, performs one full-batch gradient descent update on a mean squared error objective, and then quantifies the effective dimensionality of the hidden representations obtained after that single update. The program must output a single line containing the results aggregated across all test cases in the exact format specified below.\n\nFundamental base for derivation and protocol design:\n- The Rectified Linear Unit (ReLU) nonlinearity is defined by the function $ \\phi(z) = \\max(0,z) $ applied elementwise.\n- If $ z $ is a zero-mean, symmetric real-valued random variable (for example, a Gaussian $ \\mathcal{N}(0,\\sigma_z^2) $), then $ \\mathbb{P}(z  0) = \\mathbb{P}(z  0) = \\tfrac{1}{2} $.\n- For independent inputs $ x \\in \\mathbb{R}^d $ with i.i.d. entries and weights $ w \\in \\mathbb{R}^d $ whose entries are independent, zero-mean, and symmetric, the pre-activation $ z = w^\\top x $ is approximately symmetric and zero-mean by the Central Limit Theorem, with variance controlled by the weight variance.\n- The effective dimensionality of a matrix of features $ H \\in \\mathbb{R}^{N \\times m} $ can be quantified by the stable rank, defined as $ \\mathrm{sr}(H) = \\lVert H \\rVert_F^2 / \\lVert H \\rVert_2^2 $, where $ \\lVert \\cdot \\rVert_F $ is the Frobenius norm and $ \\lVert \\cdot \\rVert_2 $ is the spectral norm (largest singular value). This measure is invariant to global rescalings of $ H $ and is widely used to capture how spread out the singular values are, thereby indicating an effective dimensionality.\n\nProtocol to implement:\n1. Generate a synthetic dataset $ X \\in \\mathbb{R}^{N \\times d} $ with i.i.d. standard normal entries and a regression target $ y \\in \\mathbb{R}^N $ defined by $ y = X u $, where $ u \\in \\mathbb{R}^d $ has i.i.d. standard normal entries. Use fixed random seeds to ensure reproducibility.\n2. Consider a one-hidden-layer network with $ m $ hidden units, parameters $ W \\in \\mathbb{R}^{m \\times d} $ and $ v \\in \\mathbb{R}^m $, and output $ \\hat{y} = \\phi(X W^\\top) v $. Initialize the bias to $ 0 $.\n3. For each test case weight variance parameter $ \\sigma^2 $:\n   - Initialize $ W $ with i.i.d. entries $ \\sim \\mathcal{N}\\!\\left(0, \\frac{\\sigma^2}{d}\\right) $ and initialize $ v $ with i.i.d. entries $ \\sim \\mathcal{N}\\!\\left(0, \\frac{1}{m}\\right) $. This scaling keeps the pre-activation variance approximately controlled by $ \\sigma^2 $.\n   - Compute the hidden activations at initialization $ H = \\phi(X W^\\top) $.\n   - Measure initialization sparsity as the fraction of zeros in $ H $, namely $ s = \\frac{1}{N m} \\sum_{i=1}^N \\sum_{j=1}^m \\mathbf{1}\\{ H_{ij} = 0 \\} $.\n   - Perform one full-batch gradient descent step on the mean squared error (MSE) objective $ L = \\frac{1}{N} \\lVert \\hat{y} - y \\rVert_2^2 $ with learning rate $ \\alpha  0 $ to obtain updated parameters $ W' $ and $ v' $.\n   - Compute the updated hidden activations $ H' = \\phi(X (W')^\\top) $. Center $ H' $ by subtracting the column means to obtain $ \\tilde{H}' $ and compute the stable rank $ r = \\lVert \\tilde{H}' \\rVert_F^2 / \\lVert \\tilde{H}' \\rVert_2^2 $ as the effective dimensionality after the first epoch.\n   - Record the pair $ [s, r] $.\n4. Aggregate the results for all test cases and print them on a single line in the exact specified format.\n\nConcrete parameter values and test suite:\n- Use $ N = 512 $ samples, $ d = 128 $ input dimension, $ m = 256 $ hidden units, and learning rate $ \\alpha = 0.01 $.\n- Use the following test suite of weight variances $ \\sigma^2 $: $ [0.01, 0.5, 2.0, 5.0, 10.0] $. This set covers small-variance, moderate-variance (including values comparable to Rectified Linear Unit (ReLU) He initialization which uses $ \\sigma^2 \\approx 2 $ for pre-activations), and large-variance regimes.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-float list $ [s, r] $ with $ s $ the initialization sparsity and $ r $ the stable rank after one epoch. For example, the output must be of the form $ [[s_1,r_1],[s_2,r_2],\\dots] $ with no additional text.\n\nThere are no physical units or angles in this problem. All reported quantities must be real numbers. The program must be self-contained, use only the specified libraries, and must not require any input.", "solution": "The problem requires the design and implementation of a computational experiment to investigate the relationship between the variance of initial weights in a single-hidden-layer Rectified Linear Unit (ReLU) network, the resulting activation sparsity at initialization, and the effective dimensionality of the hidden layer representations after a single step of gradient descent.\n\nThe solution proceeds by first establishing the theoretical underpinnings of each component of the protocol, followed by the computational implementation.\n\n**Theoretical Framework**\n\n1.  **Dataset and Network Model**: We construct a synthetic regression problem. The input data matrix $X \\in \\mathbb{R}^{N \\times d}$ consists of $N$ samples of dimension $d$, with entries drawn independently from a standard normal distribution, $X_{ij} \\sim \\mathcal{N}(0, 1)$. The target vector $y \\in \\mathbb{R}^N$ is generated as a linear combination of the inputs, $y = X u$, where the true weight vector $u \\in \\mathbb{R}^d$ also has entries drawn from $\\mathcal{N}(0, 1)$. This setup provides a simple, well-controlled environment.\n\n    The network model is a single-hidden-layer perceptron with $m$ hidden units and no bias terms. The output is given by $\\hat{y} = \\phi(X W^\\top) v$, where $W \\in \\mathbb{R}^{m \\times d}$ is the input-to-hidden weight matrix, $v \\in \\mathbb{R}^m$ is the hidden-to-output weight vector, and $\\phi(z) = \\max(0, z)$ is the ReLU activation function applied element-wise.\n\n2.  **Weight Initialization and Activation Sparsity**: The weights $W$ are initialized with independent and identically distributed (i.i.d.) entries from a zero-mean normal distribution, $W_{jk} \\sim \\mathcal{N}(0, \\sigma^2/d)$. The scaling factor $1/d$ is crucial. Consider the pre-activation $z_{ij}$ for sample $i$ and neuron $j$: $z_{ij} = \\sum_{k=1}^d W_{jk} X_{ik}$. Since both $W_{jk}$ and $X_{ik}$ are independent, zero-mean random variables, the expectation of their product is $\\mathbb{E}[W_{jk}X_{ik}] = \\mathbb{E}[W_{jk}]\\mathbb{E}[X_{ik}] = 0$, and thus $\\mathbb{E}[z_{ij}] = 0$.\n\n    The variance is given by:\n    $$ \\mathrm{Var}(z_{ij}) = \\sum_{k=1}^d \\mathrm{Var}(W_{jk} X_{ik}) = \\sum_{k=1}^d \\mathbb{E}[(W_{jk} X_{ik})^2] - (\\mathbb{E}[W_{jk}X_{ik}])^2 $$\n    $$ \\mathrm{Var}(z_{ij}) = \\sum_{k=1}^d \\mathbb{E}[W_{jk}^2] \\mathbb{E}[X_{ik}^2] = \\sum_{k=1}^d \\mathrm{Var}(W_{jk}) \\mathrm{Var}(X_{ik}) = \\sum_{k=1}^d \\left(\\frac{\\sigma^2}{d}\\right)(1) = d \\cdot \\frac{\\sigma^2}{d} = \\sigma^2 $$\n    By the Central Limit Theorem, for large $d$, the pre-activation $z_{ij}$ is approximately normally distributed, $z_{ij} \\approx \\mathcal{N}(0, \\sigma^2)$. A ReLU neuron becomes \"inactive\" or \"dead\" if its pre-activation is non-positive. Since $\\mathcal{N}(0, \\sigma^2)$ is a symmetric distribution centered at zero, the probability of this is $\\mathbb{P}(z_{ij} \\le 0) = 0.5$. Consequently, the expected fraction of zero entries in the post-activation matrix $H = \\phi(X W^\\top)$, known as the initialization sparsity $s$, is theoretically $0.5$, irrespective of the variance parameter $\\sigma^2$. The experiment will measure this value empirically.\n\n3.  **Gradient Descent Update**: A single full-batch gradient descent step is performed on the mean squared error (MSE) loss function, $L = \\frac{1}{N} \\lVert \\hat{y} - y \\rVert_2^2$. The gradients of the loss with respect to the parameters $W$ and $v$ are required. Let $e = \\hat{y} - y$ be the error vector, and $Z = XW^\\top$ be the matrix of pre-activations. The gradients are derived using the chain rule:\n\n    -   For the output layer weights $v$:\n        $$ \\frac{\\partial L}{\\partial v} = \\frac{2}{N} H^\\top e $$\n    -   For the input layer weights $W$:\n        $$ \\frac{\\partial L}{\\partial W} = \\left( \\left( \\frac{2}{N} e v^\\top \\right) \\odot \\phi'(Z) \\right)^\\top X $$\n        where $\\odot$ denotes the Hadamard (element-wise) product and $\\phi'(Z)$ is the element-wise derivative of the ReLU function, which is a matrix of indicators $\\mathbf{1}_{Z_{ij}0}$.\n\n    The parameters are updated as $W' = W - \\alpha \\frac{\\partial L}{\\partial W}$ and $v' = v - \\alpha \\frac{\\partial L}{\\partial v}$, using the specified learning rate $\\alpha$.\n\n4.  **Effective Dimensionality**: After the update, new hidden activations $H' = \\phi(X(W')^\\top)$ are computed. To assess the dimensionality of the feature space spanned by these activation vectors, we use the stable rank. First, the activation matrix is centered by subtracting the mean of each column (feature), yielding $\\tilde{H}'$. The stable rank $r$ is then computed as:\n    $$ r = \\mathrm{sr}(\\tilde{H}') = \\frac{\\lVert \\tilde{H}' \\rVert_F^2}{\\lVert \\tilde{H}' \\rVert_2^2} = \\frac{\\sum_{i=1}^{\\min(N, m)} \\lambda_i}{\\lambda_{\\max}} $$\n    where $\\lVert \\cdot \\rVert_F$ is the Frobenius norm, $\\lVert \\cdot \\rVert_2$ is the spectral norm (the largest singular value), and $\\{\\lambda_i\\}$ are the eigenvalues of the covariance matrix $\\tilde{H}'^\\top \\tilde{H}'$ (which are the squares of the singular values of $\\tilde{H}'$). The stable rank provides a measure of how evenly the energy of the matrix is distributed among its singular values. A value close to the matrix's true rank indicates a well-conditioned feature space with evenly spread singular values, while a value close to $1$ suggests that the matrix's energy is concentrated in a single dominant direction.\n\nThe overall protocol systematically varies the initial weight variance $\\sigma^2$ to observe its effect, through the mechanism of gradient updates, on the structure of the learned feature representations as quantified by the stable rank.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the computational protocol to measure neuron activation sparsity\n    at initialization and effective dimensionality after one training epoch.\n    \"\"\"\n    # 4. Concrete parameter values and test suite\n    N = 512  # number of samples\n    d = 128  # input dimension\n    m = 256  # number of hidden units\n    alpha = 0.01  # learning rate\n    sigma_squared_values = [0.01, 0.5, 2.0, 5.0, 10.0]\n\n    # Use a fixed random seed for reproducibility of the entire experiment.\n    rng = np.random.default_rng(seed=42)\n\n    # 1. Generate a single synthetic dataset for all test cases.\n    X = rng.standard_normal(size=(N, d))\n    u = rng.standard_normal(size=(d,))\n    y = X @ u\n\n    # Utility function for the ReLU activation.\n    def relu(z):\n        return np.maximum(0, z)\n\n    # Utility function for the derivative of the ReLU activation.\n    def relu_prime(z):\n        return (z > 0).astype(z.dtype)\n\n    results = []\n    # 3. Loop through each test case for the weight variance parameter.\n    for sigma_sq in sigma_squared_values:\n        # Initialize network parameters W and v for the current test case.\n        # The initialization must be inside the loop to be fresh for each sigma_sq.\n        w_std = np.sqrt(sigma_sq / d)\n        v_std = np.sqrt(1 / m)\n        W = rng.normal(loc=0.0, scale=w_std, size=(m, d))\n        v = rng.normal(loc=0.0, scale=v_std, size=(m,))\n\n        # Compute pre-activations and activations at initialization.\n        Z = X @ W.T\n        H = relu(Z)\n\n        # Measure initialization sparsity (s).\n        s = np.mean(H == 0)\n\n        # Perform one full-batch gradient descent step.\n        # Compute the prediction and the error.\n        y_hat = H @ v\n        e = y_hat - y\n\n        # Compute gradients.\n        grad_v = (2 / N) * (H.T @ e)\n        \n        grad_Z_elementwise = (2 / N) * np.outer(e, v)\n        grad_Z = grad_Z_elementwise * relu_prime(Z)\n        grad_W = grad_Z.T @ X\n\n        # Update parameters to get W' and v'.\n        W_prime = W - alpha * grad_W\n        # v_prime is not used for the final calculation but is part of the update.\n        # v_prime = v - alpha * grad_v\n\n        # Compute updated hidden activations H' with the new weights W'.\n        H_prime = relu(X @ W_prime.T)\n\n        # Center H' by subtracting column means to get tilde(H').\n        H_prime_centered = H_prime - H_prime.mean(axis=0)\n\n        # Compute the Frobenius norm squared of the centered activations.\n        fro_norm_sq = np.sum(H_prime_centered**2)\n\n        # To compute the spectral norm squared, we find the largest eigenvalue\n        # of the covariance matrix H_tilde.T @ H_tilde. This is more stable\n        # and efficient than a full SVD for this purpose.\n        # The matrix is symmetric and positive semi-definite.\n        covariance_matrix = H_prime_centered.T @ H_prime_centered\n        eigenvalues = np.linalg.eigvalsh(covariance_matrix)\n        spec_norm_sq = np.max(eigenvalues)\n\n        # Compute the stable rank (r), handling the case of a zero matrix.\n        if spec_norm_sq > 1e-12:\n            r = fro_norm_sq / spec_norm_sq\n        else:\n            # If the largest eigenvalue is zero, all are zero. This implies\n            # the matrix is zero, so its effective dimension is zero.\n            r = 0.0\n\n        # Record the pair [s, r].\n        results.append([s, r])\n    \n    # 4. Aggregate and print results in the specified format.\n    # The map(str, ...) converts each inner list [s, r] into its string\n    # representation, e.g., \"[0.5, 128.3]\".\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3199561"}, {"introduction": "The principles of good initialization are not unique to deep learning but are rooted in the fundamentals of numerical optimization. This exercise bridges this gap by examining how to handle a common data-related challenge: input features with heterogeneous scales. You will analyze how a per-feature scaled initialization acts as a form of pre-conditioning for a linear model, and use spectral analysis to precisely quantify its dramatic effect on convergence speed compared to a standard initialization strategy [@problem_id:3199538].", "problem": "You will design and analyze a first-layer weight initialization strategy in the presence of input features with heterogeneous scales within a linear model trained by batch gradient descent on a quadratically smooth objective. Start from the fundamental base consisting of the definition of Mean Squared Error (MSE), linear predictions, and the batch gradient descent update. Specifically, consider a linear model with parameters $\\mathbf{w} \\in \\mathbb{R}^d$, data matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$, and targets $\\mathbf{y} \\in \\mathbb{R}^N$, trained to minimize the MSE. Use a purely mathematical and algorithmic specification without any physical units.\n\nYour task is to construct a reproducible synthetic dataset with feature-wise scale heterogeneity, define two different first-layer initialization strategies, and compare their convergence speed under the same learning rate. The comparison must use the exact step count obtained from the spectral characterization of batch gradient descent on a quadratic objective, not by simulating updates step-by-step.\n\nDefinitions and constraints to use as the fundamental base:\n- Mean Squared Error (MSE): minimize $L(\\mathbf{w}) = \\frac{1}{2N}\\lVert \\mathbf{X}\\mathbf{w} - \\mathbf{y} \\rVert_2^2$.\n- Batch gradient descent with fixed step size $\\eta$: iterate $\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\nabla L(\\mathbf{w}_t)$, where $\\nabla L(\\mathbf{w}) = \\frac{1}{N}\\mathbf{X}^\\top(\\mathbf{X}\\mathbf{w}-\\mathbf{y})$.\n- For reproducibility, you must use a pseudorandom generator with the exact seeds specified below to generate data and initial weights.\n\nDataset construction for each test case:\n- Let $N$ be the number of samples and $d$ the number of features.\n- Let $\\mathbf{s} \\in \\mathbb{R}^d$ denote positive feature scales. Draw a standardized matrix $\\mathbf{Z} \\in \\mathbb{R}^{N \\times d}$ with independent standard normal entries and set $\\mathbf{X} = \\mathbf{Z} \\operatorname{diag}(\\mathbf{s})$, i.e., column $j$ has empirical scale approximately $s_j$.\n- Use the zero target $\\mathbf{y} = \\mathbf{0}$ (equivalently, the true parameter $\\mathbf{w}_\\star = \\mathbf{0}$), to isolate the effect of feature scales and initialization on convergence speed.\n\nInitialization strategies for the first layer (i.e., $\\mathbf{w}_0$):\n- Standard uniform scaling (denote as “uniform”): draw $\\tilde{\\mathbf{u}} \\in \\mathbb{R}^d$ with entries independently sampled from a symmetric uniform distribution with width chosen as a standard fan-based scheme for a single-output linear layer. Then set $\\mathbf{w}_0^{\\text{uni}} = \\tilde{\\mathbf{u}}$. Use the same $\\tilde{\\mathbf{u}}$ across both strategies to isolate the effect of the scaling transform.\n- Per-feature scaled initialization (denote as “scaled”): starting from the exact same $\\tilde{\\mathbf{u}}$, rescale each component by the empirical standard deviation of the corresponding input feature, i.e., divide component $j$ by the empirical standard deviation of column $j$ of $\\mathbf{X}$, yielding $\\mathbf{w}_0^{\\text{sc}}$.\n\nConvergence speed metric to compute exactly:\n- Let $\\mathbf{H} = \\frac{1}{N}\\mathbf{X}^\\top \\mathbf{X}$ be the Hessian of $L(\\mathbf{w})$. Choose a learning rate $\\eta = \\frac{1}{L}$, where $L$ is the largest eigenvalue of $\\mathbf{H}$.\n- Define the initial error $\\mathbf{e}_0 = \\mathbf{w}_0 - \\mathbf{w}_\\star$. Since $\\mathbf{w}_\\star = \\mathbf{0}$, $\\mathbf{e}_0 = \\mathbf{w}_0$.\n- Using only linear algebra (no iterative simulation of parameter updates), compute the exact minimal nonnegative integer $t$ such that $L(\\mathbf{w}_t) \\le \\varepsilon$, where $\\varepsilon$ is a prescribed tolerance. You may use the spectral decomposition of $\\mathbf{H}$ to express $L(\\mathbf{w}_t)$ as a function of $t$ and then determine the smallest $t$ satisfying the inequality.\n\nLearning rate and tolerance:\n- For each test case, compute the learning rate as $\\eta = 1/L$, where $L$ is the largest eigenvalue of $\\mathbf{H}$ from that test case.\n- Use the loss tolerance $\\varepsilon$ provided in each test case.\n\nTest suite:\nFor each test case below, construct $\\mathbf{X}$ and $\\mathbf{y}$ as specified above, draw the raw initialization vector $\\tilde{\\mathbf{u}}$ from the standard uniform scaling scheme (fan-in $d$, fan-out $1$) using the given seed, and then derive both initializations from the same $\\tilde{\\mathbf{u}}$. Compute the exact number of gradient descent steps to reach $L(\\mathbf{w}_t) \\le \\varepsilon$ for each initialization, and output the difference in step counts, defined as $\\Delta = t_{\\text{uni}} - t_{\\text{sc}}$.\n\n- Case $1$: $N = 128$, $d = 5$, $\\mathbf{s} = [1,1,1,1,1]$, seed $= 1$, $\\varepsilon = 10^{-4}$.\n- Case $2$: $N = 128$, $d = 5$, $\\mathbf{s} = [1,10,10,1,5]$, seed $= 2$, $\\varepsilon = 10^{-4}$.\n- Case $3$: $N = 256$, $d = 10$, $\\mathbf{s} = [1,3,9,27,9,3,1,27,9,3]$, seed $= 3$, $\\varepsilon = 10^{-4}$.\n- Case $4$: $N = 64$, $d = 8$, $\\mathbf{s} = [0.5,2,4,8,16,1,0.5,16]$, seed $= 4$, $\\varepsilon = 10^{-4}$.\n\nRequired final output:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The $k$-th entry must equal $\\Delta$ for Case $k$, in the order above. For example, an output with four integers must look like $[r_1,r_2,r_3,r_4]$.\n- Each result must be an integer. If a case cannot reach the tolerance for numerical reasons, return a large upper bound integer that you chose consistently for both initializations (but with the specified learning rate and method, the cases here should be well-posed and convergent).", "solution": "The problem requires the design and analysis of two weight initialization strategies for a linear model trained with batch gradient descent on a Mean Squared Error (MSE) objective. The analysis must be exact, leveraging the spectral properties of the Hessian, rather than iterative simulation of gradient updates. The core of the problem is to compare the convergence speed of a standard uniform initialization against a proposed per-feature scaled initialization, particularly in the presence of input features with heterogeneous scales.\n\nFirst, we formalize the components of the problem.\n\nThe model is a linear predictor $\\hat{\\mathbf{y}} = \\mathbf{Xw}$, where $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$ is the data matrix, $\\mathbf{w} \\in \\mathbb{R}^d$ is the weight vector, $N$ is the number of samples, and $d$ is the number of features. The objective is to minimize the MSE loss. Given that the target vector is $\\mathbf{y} = \\mathbf{0}$, the loss function is:\n$$L(\\mathbf{w}) = \\frac{1}{2N} \\lVert \\mathbf{Xw} - \\mathbf{0} \\rVert_2^2 = \\frac{1}{2N}(\\mathbf{Xw})^\\top(\\mathbf{Xw}) = \\frac{1}{2}\\mathbf{w}^\\top \\left(\\frac{1}{N}\\mathbf{X}^\\top\\mathbf{X}\\right) \\mathbf{w}$$\nThis is a quadratic form. We identify the Hessian matrix of the loss function, which is constant with respect to $\\mathbf{w}$:\n$$\\mathbf{H} = \\nabla^2 L(\\mathbf{w}) = \\frac{1}{N}\\mathbf{X}^\\top\\mathbf{X}$$\nThus, the loss can be written compactly as $L(\\mathbf{w}) = \\frac{1}{2}\\mathbf{w}^\\top\\mathbf{Hw}$. The gradient of the loss is $\\nabla L(\\mathbf{w}) = \\mathbf{Hw}$. The optimal weight vector that minimizes the loss is evidently $\\mathbf{w}_\\star = \\mathbf{0}$, as $\\mathbf{H}$ is positive semi-definite, and for the cases specified ($N > d$), it is almost surely positive definite.\n\nThe optimization algorithm is batch gradient descent with a fixed learning rate $\\eta$:\n$$\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\nabla L(\\mathbf{w}_t) = \\mathbf{w}_t - \\eta \\mathbf{Hw}_t = (\\mathbf{I} - \\eta \\mathbf{H})\\mathbf{w}_t$$\nThis is a linear recurrence relation. The weight vector at step $t$ can be expressed in terms of the initial weight vector $\\mathbf{w}_0$ as:\n$$\\mathbf{w}_t = (\\mathbf{I} - \\eta \\mathbf{H})^t \\mathbf{w}_0$$\n\nTo analyze the convergence of the loss $L(\\mathbf{w}_t)$, we employ a spectral decomposition of the Hessian $\\mathbf{H}$. Since $\\mathbf{H}$ is a real symmetric matrix, it is orthogonally diagonalizable: $\\mathbf{H} = \\mathbf{Q\\Lambda Q}^\\top$, where $\\mathbf{Q}$ is an orthogonal matrix ($\\mathbf{Q}\\mathbf{Q}^\\top = \\mathbf{I}$) whose columns are the eigenvectors of $\\mathbf{H}$, and $\\mathbf{\\Lambda}$ is a diagonal matrix containing the corresponding real eigenvalues, $\\lambda_1, \\lambda_2, \\ldots, \\lambda_d$. We sort them in non-decreasing order: $0  \\lambda_1 \\le \\dots \\le \\lambda_d$. The learning rate is set to $\\eta = 1/L$, where $L = \\lambda_{\\max}(\\mathbf{H}) = \\lambda_d$.\n\nThe loss at step $t$ is $L(\\mathbf{w}_t) = \\frac{1}{2}\\mathbf{w}_t^\\top \\mathbf{Hw}_t$. Substituting the expressions for $\\mathbf{w}_t$ and $\\mathbf{H}$:\n$$L(\\mathbf{w}_t) = \\frac{1}{2} \\left((\\mathbf{I} - \\eta \\mathbf{H})^t \\mathbf{w}_0\\right)^\\top \\mathbf{H} \\left((\\mathbf{I} - \\eta \\mathbf{H})^t \\mathbf{w}_0\\right)$$\nSince $\\mathbf{H}$ and $(\\mathbf{I} - \\eta\\mathbf{H})$ commute, we can write:\n$$L(\\mathbf{w}_t) = \\frac{1}{2} \\mathbf{w}_0^\\top (\\mathbf{I} - \\eta \\mathbf{H})^{2t} \\mathbf{H} \\mathbf{w}_0$$\nUsing the spectral decomposition, $\\mathbf{I} - \\eta \\mathbf{H} = \\mathbf{Q}(\\mathbf{I} - \\eta\\mathbf{\\Lambda})\\mathbf{Q}^\\top$. This gives:\n$$L(\\mathbf{w}_t) = \\frac{1}{2} \\mathbf{w}_0^\\top \\mathbf{Q} (\\mathbf{I} - \\eta\\mathbf{\\Lambda})^{2t} \\mathbf{\\Lambda} \\mathbf{Q}^\\top \\mathbf{w}_0$$\nLet $\\tilde{\\mathbf{w}}_0 = \\mathbf{Q}^\\top\\mathbf{w}_0$ be the projection of the initial weights onto the eigenvector basis. The loss becomes a sum over the eigenmodes:\n$$L(\\mathbf{w}_t) = \\frac{1}{2} \\tilde{\\mathbf{w}}_0^\\top (\\mathbf{I} - \\eta\\mathbf{\\Lambda})^{2t} \\mathbf{\\Lambda} \\tilde{\\mathbf{w}}_0 = \\frac{1}{2} \\sum_{i=1}^{d} \\tilde{w}_{0,i}^2 \\lambda_i (1-\\eta\\lambda_i)^{2t}$$\nwhere $\\tilde{w}_{0,i}$ is the $i$-th component of $\\tilde{\\mathbf{w}}_0$.\n\nThe task is to find the minimum non-negative integer $t$ for which $L(\\mathbf{w}_t) \\le \\varepsilon$. The right-hand side is a monotonically decreasing function of $t$, as $0 \\le (1-\\eta\\lambda_i) \\le 1$ for all eigenvalues $\\lambda_i$. A closed-form solution for $t$ is intractable. However, we can compute $L(\\mathbf{w}_t)$ for successive integer values of $t$ starting from $t=0$ and stop when the condition is met. This provides the exact integer $t$ without simulating the weight updates.\n\nThe two initialization strategies are:\n1.  **Standard Uniform Scaling (\"uniform\"):** According to standard practices (e.g., Glorot/Xavier uniform initialization), for a linear layer with fan-in $d_{in}$ and fan-out $d_{out}$, weights are drawn from $U[-a, a]$ where $a = \\sqrt{6 / (d_{in} + d_{out})}$. Here, $d_{in}=d$ and $d_{out}=1$. So, the raw initialization vector $\\tilde{\\mathbf{u}}$ has components drawn from $U\\left[-\\sqrt{\\frac{6}{d+1}}, \\sqrt{\\frac{6}{d+1}}\\right]$. For this strategy, $\\mathbf{w}_0^{\\text{uni}} = \\tilde{\\mathbf{u}}$.\n2.  **Per-feature Scaled Initialization (\"scaled\"):** This strategy adapts the initialization to the scale of the input features. Let $\\hat{\\sigma}_j$ be the empirical standard deviation of the $j$-th column of $\\mathbf{X}$. The initial weight vector is constructed as $(\\mathbf{w}_0^{\\text{sc}})_j = \\tilde{u}_j / \\hat{\\sigma}_j$, using the same $\\tilde{\\mathbf{u}}$ as in the uniform case. This pre-conditions the initial weights, aiming to balance the influence of features with disparate scales. A feature with a large standard deviation $\\hat{\\sigma}_j$ receives a smaller initial weight, and vice versa.\n\nThe algorithm to solve each test case is as follows:\n1.  Generate the data matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$ using the specified parameters ($N, d, \\mathbf{s}$) and random seed.\n2.  Compute the Hessian $\\mathbf{H} = \\frac{1}{N}\\mathbf{X}^\\top\\mathbf{X}$.\n3.  Perform an eigendecomposition of $\\mathbf{H}$ to find its eigenvalues $\\{\\lambda_i\\}_{i=1}^d$ and the eigenvector matrix $\\mathbf{Q}$.\n4.  Determine the learning rate $\\eta = 1/\\lambda_d$, where $\\lambda_d$ is the largest eigenvalue.\n5.  Generate the raw initialization vector $\\tilde{\\mathbf{u}} \\in \\mathbb{R}^d$ using the specified seed.\n6.  Construct the two initial weight vectors: $\\mathbf{w}_0^{\\text{uni}}$ and $\\mathbf{w}_0^{\\text{sc}}$.\n7.  For each initialization $\\mathbf{w}_0$:\n    a. Project $\\mathbf{w}_0$ onto the eigenbasis: $\\tilde{\\mathbf{w}}_0 = \\mathbf{Q}^\\top\\mathbf{w}_0$.\n    b. Define the coefficients $c_i = \\frac{1}{2}\\tilde{w}_{0,i}^2 \\lambda_i$ and the bases $b_i = (1-\\eta\\lambda_i)^2$.\n    c. Calculate the loss function $L(t) = \\sum_{i=1}^d c_i (b_i)^t$.\n    d. Iterate $t=0, 1, 2, \\ldots$ and find the smallest $t$ such that $L(t) \\le \\varepsilon$. This gives $t_{\\text{uni}}$ and $t_{\\text{sc}}$.\n8.  Compute the difference $\\Delta = t_{\\text{uni}} - t_{\\text{sc}}$.\n\nThis procedure will be implemented for each test case to find the required difference in convergence steps.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing the convergence speed of two different\n    weight initialization strategies for a linear model.\n    \"\"\"\n\n    test_cases = [\n        {'N': 128, 'd': 5, 's': [1, 1, 1, 1, 1], 'seed': 1, 'epsilon': 1e-4},\n        {'N': 128, 'd': 5, 's': [1, 10, 10, 1, 5], 'seed': 2, 'epsilon': 1e-4},\n        {'N': 256, 'd': 10, 's': [1, 3, 9, 27, 9, 3, 1, 27, 9, 3], 'seed': 3, 'epsilon': 1e-4},\n        {'N': 64, 'd': 8, 's': [0.5, 2, 4, 8, 16, 1, 0.5, 16], 'seed': 4, 'epsilon': 1e-4},\n    ]\n\n    results = []\n    for case in test_cases:\n        delta = calculate_step_difference(\n            N=case['N'],\n            d=case['d'],\n            s=np.array(case['s']),\n            seed=case['seed'],\n            epsilon=case['epsilon']\n        )\n        results.append(delta)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef get_steps_to_convergence(w0, Q, eigenvalues, L, epsilon):\n    \"\"\"\n    Calculates the exact number of steps t to reach the loss tolerance.\n\n    Args:\n        w0 (np.ndarray): The initial weight vector.\n        Q (np.ndarray): The matrix of eigenvectors of the Hessian.\n        eigenvalues (np.ndarray): The eigenvalues of the Hessian.\n        L (float): The largest eigenvalue of the Hessian (Lipschitz constant).\n        epsilon (float): The loss tolerance.\n\n    Returns:\n        int: The minimum number of steps t.\n    \"\"\"\n    # Project initial weights onto the eigenvector basis\n    w0_tilde = Q.T @ w0\n\n    # Pre-compute coefficients and bases for the loss expression\n    # L(t) = 1/2 * sum_{i} (w0_tilde_i^2 * lambda_i * (1 - lambda_i/L)^(2t))\n    # L(t) = sum_{i} coeffs_i * (bases_i)^t\n    coeffs = 0.5 * (w0_tilde**2) * eigenvalues\n    bases = (1 - eigenvalues / L)**2\n\n    # Handle the case t=0\n    loss_at_0 = np.sum(coeffs)\n    if loss_at_0 = epsilon:\n        return 0\n\n    t = 0\n    loss = loss_at_0\n    \n    # Iteratively evaluate the loss L(t) for t = 1, 2, ...\n    # This is an exact calculation of the loss at step t, not a simulation.\n    while loss > epsilon:\n        t += 1\n        loss = np.sum(coeffs * (bases**t))\n        # Add a safeguard for potential infinite loops, though not expected\n        # for these well-posed problems.\n        if t > 1_000_000:\n             # Returning a large number as per problem notes if tolerance not reached\n            return 1_000_000\n    \n    return t\n\ndef calculate_step_difference(N, d, s, seed, epsilon):\n    \"\"\"\n    Performs the full analysis for a single test case.\n\n    Args:\n        N (int): Number of samples.\n        d (int): Number of features.\n        s (np.ndarray): Feature scales.\n        seed (int): Seed for the pseudorandom number generator.\n        epsilon (float): Loss tolerance.\n\n    Returns:\n        int: The difference in steps, t_uni - t_sc.\n    \"\"\"\n    # 1. Reproducible dataset construction\n    rng = np.random.default_rng(seed)\n    Z = rng.standard_normal(size=(N, d))\n    X = Z @ np.diag(s)\n\n    # 2. Hessian and its spectral decomposition\n    H = (1/N) * X.T @ X\n    eigenvalues, Q = np.linalg.eigh(H)\n    \n    # 3. Learning rate\n    L = eigenvalues[-1]  # np.linalg.eigh returns eigenvalues in ascending order\n    # Ensure L is not zero to avoid division by zero, though highly unlikely for N>d\n    if L = 0:\n        # This case should not be reached with the given problem setup\n        # as H is positive definite almost surely.\n        raise ValueError(\"Largest eigenvalue must be positive.\")\n    eta = 1.0 / L\n\n    # 4. Initialization strategies\n    # Raw initialization vector based on Glorot/Xavier uniform scheme\n    limit = np.sqrt(6 / (d + 1))\n    u_tilde = rng.uniform(-limit, limit, size=d)\n\n    # Strategy 1: Standard uniform scaling\n    w0_uni = u_tilde\n\n    # Strategy 2: Per-feature scaled initialization\n    empirical_std = np.std(X, axis=0)\n    # Avoid division by zero for features with no variance\n    empirical_std[empirical_std == 0] = 1.0 \n    w0_sc = u_tilde / empirical_std\n\n    # 5. Compute steps to convergence for both initializations\n    t_uni = get_steps_to_convergence(w0_uni, Q, eigenvalues, L, epsilon)\n    t_sc = get_steps_to_convergence(w0_sc, Q, eigenvalues, L, epsilon)\n\n    # 6. Return the difference\n    return t_uni - t_sc\n\nsolve()\n```", "id": "3199538"}]}