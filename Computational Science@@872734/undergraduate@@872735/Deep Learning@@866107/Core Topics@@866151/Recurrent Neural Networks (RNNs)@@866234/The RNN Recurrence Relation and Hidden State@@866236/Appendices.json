{"hands_on_practices": [{"introduction": "To train a Recurrent Neural Network, we must understand how to adjust its parameters to minimize error. This fundamental exercise delves into the heart of the learning process by deriving the gradient of the loss with respect to the recurrent weight matrix, $W_h$ [@problem_id:3192146]. By unrolling the recurrence relation, you will derive a closed-form expression that explicitly shows how errors from all future timesteps contribute to the gradient at a single point in the past, providing a first-principles understanding of Backpropagation Through Time (BPTT) and the challenges of learning long-term dependencies.", "problem": "Consider a linear Recurrent Neural Network (RNN) with hidden state dimension $d_h$, input dimension $d_x$, and output dimension $d_y$, driven by a single sequence of length $T$. The recurrence and readout are given by the core definitions\n$h_t = W_h h_{t-1} + W_x x_t$ with $h_0 = 0$, and $y_t = W_y h_t$,\nfor $t \\in \\{1,2,\\dots,T\\}$, where $W_h \\in \\mathbb{R}^{d_h \\times d_h}$, $W_x \\in \\mathbb{R}^{d_h \\times d_x}$, and $W_y \\in \\mathbb{R}^{d_y \\times d_h}$. The input vectors are $\\{x_t \\in \\mathbb{R}^{d_x}\\}_{t=1}^T$ and the targets are $\\{s_t \\in \\mathbb{R}^{d_y}\\}_{t=1}^T$. Consider the quadratic loss\n$L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2$.\nLet the output residuals be $e_t = y_t - s_t$ for each $t$. Using only the recurrence definition, the readout definition, the chain rule from multivariable calculus, and standard linear algebraic facts, derive a closed-form analytic expression for the gradient $\\nabla_{W_h} L$ that makes explicit its decomposition as a sum of time-shifted correlation terms of the form $\\sum e_{\\cdot}\\, h_{\\cdot}^{\\top}$. Then, briefly discuss the computational complexity (in Bigâ€“$O$ notation) of evaluating this gradient when implemented (i) directly from your closed-form expression by summing the time-shifted terms, and (ii) via Backpropagation Through Time (BPTT) using the first-order recursion for the adjoint signals. Your final answer must be a single closed-form analytic matrix expression for $\\nabla_{W_h} L$ written in terms of $W_h$, $W_y$, $\\{e_t\\}$, and $\\{h_t\\}$ only. Do not include any numerical evaluation.", "solution": "The problem is assessed to be valid. It is a well-posed, scientifically grounded, and objective problem statement from the field of deep learning theory. It requires the derivation of a gradient for a standard linear Recurrent Neural Network (RNN), which is a formal and solvable task based on established principles of multivariable calculus and linear algebra. All necessary definitions and conditions are provided.\n\nWe begin by stating the core definitions provided in the problem:\nRecurrence relation: $h_t = W_h h_{t-1} + W_x x_t$ for $t \\in \\{1, 2, \\dots, T\\}$, with initial state $h_0 = 0$.\nReadout equation: $y_t = W_y h_t$.\nLoss function: $L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2 = \\frac{1}{2} \\sum_{t=1}^T \\|e_t\\|_2^2$, where $e_t = y_t - s_t$.\nThe parameters are matrices $W_h \\in \\mathbb{R}^{d_h \\times d_h}$, $W_x \\in \\mathbb{R}^{d_h \\times d_x}$, and $W_y \\in \\mathbb{R}^{d_y \\times d_h}$. The inputs are vectors $x_t \\in \\mathbb{R}^{d_x}$ and targets $s_t \\in \\mathbb{R}^{d_y}$.\n\nOur goal is to derive a closed-form expression for the gradient of the loss function $L$ with respect to the recurrent weight matrix $W_h$, denoted as $\\nabla_{W_h} L$. We will use the chain rule for matrix calculus.\n\nThe loss $L$ is a function of $W_h$ through its influence on the hidden states $\\{h_t\\}_{t=1}^T$. The gradient can be expressed as a sum of contributions from each timestep where $W_h$ is used in the forward pass:\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\nabla_{W_h}^{(\\text{local at } t)} L\n$$\nwhere the \"local\" gradient refers to the gradient with respect to the instance of $W_h$ used in the computation $h_t = W_h h_{t-1} + W_x x_t$. Using the chain rule, this contribution is given by the outer product of the gradient of the loss with respect to the state $h_t$, and the gradient of the state-update term with respect to $W_h$. Let $\\delta_t = \\frac{\\partial L}{\\partial h_t} \\in \\mathbb{R}^{d_h}$ be the gradient of the total loss with respect to the hidden state $h_t$. The gradient of the pre-activation $a_t = W_h h_{t-1} + W_x x_t$ with respect to $W_h$ is $h_{t-1}^T$. Thus, the contribution to the gradient from timestep $t$ is $\\delta_t h_{t-1}^T$. Summing over all timesteps gives the total gradient:\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\delta_t h_{t-1}^T\n$$\nThis is the standard formula for the recurrent weight gradient in BPTT. However, the problem requires a closed-form expression in terms of the given quantities $W_h, W_y, \\{e_t\\}, \\{h_t\\}$. To achieve this, we must derive a closed form for $\\delta_t$.\n\nThe vector $\\delta_t$ represents the total influence of $h_t$ on the loss $L$. The state $h_t$ influences the loss through the outputs $y_k$ for all subsequent timesteps $k \\ge t$.\n$$\n\\delta_t = \\frac{\\partial L}{\\partial h_t} = \\sum_{k=t}^{T} \\frac{\\partial L}{\\partial y_k} \\frac{\\partial y_k}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_t}\n$$\nHere, we apply the chain rule, summing over all paths from $h_t$ to the final loss. The partial derivatives are (using numerator-layout matrix calculus conventions where the gradient of a scalar w.r.t. a column vector is a column vector):\n$1.$ $\\frac{\\partial L}{\\partial y_k} = \\frac{\\partial}{\\partial y_k} \\left( \\frac{1}{2} \\sum_{j=1}^T \\|y_j - s_j\\|_2^2 \\right) = y_k - s_k = e_k$. This is a column vector of size $d_y$.\n$2.$ $\\frac{\\partial y_k}{\\partial h_k} = W_y$. This is the Jacobian matrix of size $d_y \\times d_h$. The term in the chain rule is its transpose, $(\\frac{\\partial y_k}{\\partial h_k})^T = W_y^T$.\n$3.$ $\\frac{\\partial h_k}{\\partial h_t}$. We find this by unrolling the recurrence relation:\n$h_k = W_h h_{k-1} + W_x x_k = W_h (W_h h_{k-2} + W_x x_{k-1}) + W_x x_k = \\dots$\nThe dependence of $h_k$ on $h_t$ for $k > t$ is given by:\n$h_k = W_h^{k-t} h_t + \\sum_{j=t+1}^{k} W_h^{k-j} W_x x_j$.\nThe Jacobian matrix is therefore $\\frac{\\partial h_k}{\\partial h_t} = W_h^{k-t}$ for $k \\ge t$. The term in the chain rule composition is its transpose, $(W_h^{k-t})^T = (W_h^T)^{k-t}$.\n\nSubstituting these derivatives back into the expression for $\\delta_t$:\n$$\n\\delta_t = \\sum_{k=t}^{T} \\left( \\frac{\\partial h_k}{\\partial h_t} \\right)^T \\left( \\frac{\\partial y_k}{\\partial h_k} \\right)^T \\frac{\\partial L}{\\partial y_k} = \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k\n$$\nThis provides the closed-form expression for the gradient signal $\\delta_t$ propagated back from all future timesteps $k \\ge t$.\n\nFinally, we substitute this expression for $\\delta_t$ into our equation for the total gradient $\\nabla_{W_h} L$:\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T\n$$\nThis is the required closed-form analytic expression. It decomposes the gradient into a double summation over time. Each term $(W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$ can be interpreted as a \"correlation\" between the error $e_k$ at a future time $k$ and the hidden state $h_{t-1}$ at a past time $t-1$, mediated by the matrix $(W_h^T)^{k-t} W_y^T$ which accounts for the propagation of influence through the network's dynamics and readout layer.\n\nNext, we discuss the computational complexity.\n(i) Direct evaluation from the closed-form expression:\nThe formula is $\\sum_{t=1}^{T} \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$.\nAssume the forward pass has been run, so all $e_t \\in \\mathbb{R}^{d_y}$ and $h_t \\in \\mathbb{R}^{d_h}$ are known.\nThe double loop involves $O(T^2)$ terms. For each term $(t, k)$, the computation involves:\n- A matrix power $(W_h^T)^{k-t}$, which, if not pre-computed, takes $O((k-t) d_h^3)$. A pre-computation of all necessary powers of $W_h^T$ up to $T-1$ takes $O(T d_h^3)$.\n- A matrix-vector product $W_y^T e_k$, costing $O(d_h d_y)$.\n- A matrix-vector product $(W_h^T)^{k-t} (W_y^T e_k)$, costing $O(d_h^2)$.\n- An outer product with $h_{t-1}^T$, costing $O(d_h^2)$.\nA naive implementation would be very expensive. A more optimized direct evaluation would proceed as:\n`total_grad = 0`\n`for t = 1 to T:`\n  `inner_sum_vec = 0`\n  `for k = t to T:`\n    `vec = ... compute (W_h^T)^{k-t} W_y^T e_k ...`\n    `inner_sum_vec += vec`\n  `total_grad += outer(inner_sum_vec, h_{t-1})`\nThe most expensive part is the double loop structure. There are approximately $T^2/2$ pairs of $(t,k)$. For each pair, the dominant cost is computing the matrix-vector product with the matrix power, which is $O(d_h^2)$ assuming powers are precomputed. The precomputation costs $O(T d_h^3)$. Therefore, the total complexity is dominated by the nested loops and the precomputation, leading to a complexity of $O(T d_h^3 + T^2 d_h^2)$. The quadratic dependence on the sequence length $T$ makes this method computationally prohibitive for long sequences.\n\n(ii) Evaluation via Backpropagation Through Time (BPTT):\nBPTT avoids the explicit calculation of the double summation by using dynamic programming. It computes the gradients via a single backward pass through time. The key is the recursive relationship for $\\delta_t$:\n$\\delta_T = W_y^T e_T$\n$\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$ for $t = T-1, \\dots, 1$.\nThe BPTT algorithm for computing $\\nabla_{W_h} L$ is:\n$1.$ Perform a forward pass from $t=1$ to $T$ to compute and store all $h_t$ and $e_t$. Cost: $O(T(d_h^2 + d_h d_x + d_y d_h))$.\n$2.$ Initialize $\\nabla_{W_h} L = 0$ and $\\delta_{T+1} = 0$.\n$3.$ Perform a backward pass from $t=T$ down to $1$:\n   a. Compute $\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$. This involves two matrix-vector products and a vector addition, costing $O(d_h d_y + d_h^2)$.\n   b. Add the contribution to the gradient: $\\nabla_{W_h} L \\leftarrow \\nabla_{W_h} L + \\delta_t h_{t-1}^T$. This is an outer product and matrix addition, costing $O(d_h^2)$.\nThe backward pass consists of a single loop of length $T$, with each step costing $O(d_h^2 + d_h d_y)$. The total complexity of the backward pass is $O(T(d_h^2 + d_h d_y))$.\nThe total complexity of BPTT is therefore $O(T(d_h^2 + d_h d_x + d_y d_h))$, which is linear in the sequence length $T$. This is significantly more efficient than the direct evaluation of the closed-form expression.", "answer": "$$\n\\boxed{\\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T}\n$$", "id": "3192146"}, {"introduction": "While linear models are analytically tractable, the true power of neural networks comes from nonlinear activation functions. This practice problem moves beyond linear dynamics to explore the behavior of an RNN with a Rectified Linear Unit (ReLU) activation, a cornerstone of modern deep learning [@problem_id:3192158]. You will analyze how the elementwise nonlinearity can guide the hidden state trajectory into a lower-dimensional subspace, a phenomenon related to the \"dying ReLU\" problem, and see how the system's long-term behavior becomes dependent on its inputs and initial conditions in a non-trivial way.", "problem": "Consider a discrete-time Recurrent Neural Network (RNN) with a two-dimensional hidden state and Rectified Linear Unit (ReLU) nonlinearity. Let the recurrence be defined by the elementwise application of the ReLU (Rectified Linear Unit) function to the preactivation, so that the hidden state update is given by\n$$\n\\mathbf{h}_{t}=\\phi\\!\\left(\\mathbf{W}\\mathbf{h}_{t-1}+\\mathbf{U}u+\\mathbf{b}\\right),\n$$\nwhere $\\phi$ acts componentwise as $\\phi(z)=\\max(0,z)$, $\\mathbf{h}_{t}\\in\\mathbb{R}^{2}$ is the hidden state at time $t$, $\\mathbf{W}\\in\\mathbb{R}^{2\\times 2}$, $\\mathbf{U}\\in\\mathbb{R}^{2\\times 1}$, $\\mathbf{b}\\in\\mathbb{R}^{2}$, and the input is a constant scalar $u\\in\\mathbb{R}$ applied at every time step. The parameters and initial hidden state are\n$$\n\\mathbf{W}=\\begin{pmatrix}0.6  -0.4\\\\ -0.5  0.2\\end{pmatrix},\\quad\n\\mathbf{U}=\\begin{pmatrix}0.3\\\\ 0.1\\end{pmatrix},\\quad\n\\mathbf{b}=\\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix},\\quad\n\\mathbf{h}_{0}=\\begin{pmatrix}0.1\\\\ 0\\end{pmatrix}.\n$$\nDefine the one-dimensional cone\n$$\n\\mathcal{C}=\\left\\{(h_{1},h_{2})\\in\\mathbb{R}^{2}\\,:\\,h_{1}0,\\ h_{2}=0\\right\\}.\n$$\nStarting from the fundamental definitions of the RNN recurrence and the ReLU nonlinearity, analyze how activation sparsity (zeros created by the ReLU) can persist in time and derive conditions under which the hidden state becomes trapped in the low-dimensional cone $\\mathcal{C}$ for all $t\\geq 1$. Then, compute the largest real $u$ such that, with the constant input $u$ applied at every step, the hidden state satisfies $\\mathbf{h}_{t}\\in\\mathcal{C}$ for all $t\\geq 1$. The final answer must be a single real number. No rounding is required.", "solution": "The user wants me to find the largest real number $u$ for which the hidden state $\\mathbf{h}_t$ of a given Recurrent Neural Network (RNN) is constrained to a specific cone $\\mathcal{C}$ for all time steps $t \\geq 1$.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n- **Recurrence Relation**: $\\mathbf{h}_{t}=\\phi\\!\\left(\\mathbf{W}\\mathbf{h}_{t-1}+\\mathbf{U}u+\\mathbf{b}\\right)$, where $\\phi(z)=\\max(0,z)$ (ReLU) is applied elementwise.\n- **State Vector**: $\\mathbf{h}_{t} = \\begin{pmatrix} h_{t,1} \\\\ h_{t,2} \\end{pmatrix} \\in\\mathbb{R}^{2}$.\n- **Weight Matrix**: $\\mathbf{W}=\\begin{pmatrix}0.6  -0.4\\\\ -0.5  0.2\\end{pmatrix}$.\n- **Input Matrix**: $\\mathbf{U}=\\begin{pmatrix}0.3\\\\ 0.1\\end{pmatrix}$.\n- **Bias Vector**: $\\mathbf{b}=\\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix}$.\n- **Input**: $u \\in \\mathbb{R}$, constant for all $t$.\n- **Initial State**: $\\mathbf{h}_{0}=\\begin{pmatrix}0.1\\\\ 0\\end{pmatrix}$.\n- **Target Cone**: $\\mathcal{C}=\\left\\{(h_{1},h_{2})\\in\\mathbb{R}^{2}\\,:\\,h_{1}0,\\ h_{2}=0\\right\\}$.\n- **Condition**: Find the largest $u$ such that $\\mathbf{h}_{t} \\in \\mathcal{C}$ for all $t \\geq 1$.\n\n**1.2. Validate Using Extracted Givens**\nThe problem is a well-defined exercise in analyzing the dynamics of a discrete-time switched linear system, which is the model for an RNN with a ReLU activation function.\n- **Scientifically Grounded**: The problem is based on standard definitions from deep learning and dynamical systems theory.\n- **Well-Posed**: It asks for the maximum value of a parameter subject to a set of derivable constraints. The problem statement is self-contained and provides all necessary information to determine a unique solution.\n- **Objective**: All terms are defined mathematically, and the objective is to compute a single numerical value.\n\nThe problem is valid as it does not violate any of the specified criteria. I will proceed with the solution.\n\n### Step 2: Deriving Conditions for State Confinement\n\nThe condition that $\\mathbf{h}_{t} \\in \\mathcal{C}$ for all $t \\geq 1$ means that for every $t \\geq 1$, the hidden state vector $\\mathbf{h}_t = \\begin{pmatrix} h_{t,1} \\\\ h_{t,2} \\end{pmatrix}$ must satisfy:\n1.  $h_{t,1}  0$\n2.  $h_{t,2} = 0$\n\nLet the pre-activation vector at time $t$ be $\\mathbf{z}_t = \\mathbf{W}\\mathbf{h}_{t-1}+\\mathbf{U}u+\\mathbf{b}$. Then $\\mathbf{h}_t = \\phi(\\mathbf{z}_t)$.\nThe elementwise ReLU function implies:\n$h_{t,1} = \\max(0, z_{t,1})$\n$h_{t,2} = \\max(0, z_{t,2})$\n\nFor $\\mathbf{h}_t$ to be in $\\mathcal{C}$, we must have:\n1.  $\\max(0, z_{t,1})  0 \\implies z_{t,1}  0$. In this case, $h_{t,1} = z_{t,1}$.\n2.  $\\max(0, z_{t,2}) = 0 \\implies z_{t,2} \\leq 0$.\n\nSo, for all $t \\geq 1$, we must satisfy $z_{t,1}  0$ and $z_{t,2} \\leq 0$.\n\n### Step 3: Analysis for Time Step $t=1$\n\nWe begin with the initial state $\\mathbf{h}_0 = \\begin{pmatrix} 0.1 \\\\ 0 \\end{pmatrix}$. We compute the pre-activation $\\mathbf{z}_1$:\n$$\n\\mathbf{z}_1 = \\mathbf{W}\\mathbf{h}_0 + \\mathbf{U}u + \\mathbf{b} = \\begin{pmatrix}0.6  -0.4\\\\ -0.5  0.2\\end{pmatrix} \\begin{pmatrix}0.1\\\\ 0\\end{pmatrix} + \\begin{pmatrix}0.3\\\\ 0.1\\end{pmatrix}u + \\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix}\n$$\n$$\n\\mathbf{z}_1 = \\begin{pmatrix}0.6 \\times 0.1\\\\ -0.5 \\times 0.1\\end{pmatrix} + \\begin{pmatrix}0.3u\\\\ 0.1u\\end{pmatrix} + \\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix} = \\begin{pmatrix}0.06 + 0.3u - 0.2\\\\ -0.05 + 0.1u - 0.05\\end{pmatrix} = \\begin{pmatrix}0.3u - 0.14\\\\ 0.1u - 0.1\\end{pmatrix}\n$$\nFor $\\mathbf{h}_1 \\in \\mathcal{C}$, we need $z_{1,1}  0$ and $z_{1,2} \\leq 0$:\n1.  $z_{1,1} = 0.3u - 0.14  0 \\implies 0.3u  0.14 \\implies u  \\frac{0.14}{0.3} = \\frac{14}{30} = \\frac{7}{15}$.\n2.  $z_{1,2} = 0.1u - 0.1 \\leq 0 \\implies 0.1u \\leq 0.1 \\implies u \\leq 1$.\n\nThus, a necessary condition on $u$ is $\\frac{7}{15}  u \\leq 1$.\nIf this condition holds, the state at $t=1$ is $\\mathbf{h}_1 = \\begin{pmatrix} z_{1,1} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0.3u - 0.14 \\\\ 0 \\end{pmatrix}$.\n\n### Step 4: Inductive Analysis for $t \\geq 2$\n\nWe proceed by induction. Assume that for some $t-1 \\geq 1$, the state is in the cone, i.e., $\\mathbf{h}_{t-1} = \\begin{pmatrix} h_{t-1,1} \\\\ 0 \\end{pmatrix}$ with $h_{t-1,1}  0$. We need to find the conditions on $u$ such that $\\mathbf{h}_t$ also lies in $\\mathcal{C}$.\n\nLet's compute $\\mathbf{z}_t$:\n$$\n\\mathbf{z}_t = \\mathbf{W}\\mathbf{h}_{t-1} + \\mathbf{U}u + \\mathbf{b} = \\begin{pmatrix}0.6  -0.4\\\\ -0.5  0.2\\end{pmatrix} \\begin{pmatrix}h_{t-1,1}\\\\ 0\\end{pmatrix} + \\begin{pmatrix}0.3u\\\\ 0.1u\\end{pmatrix} + \\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix}\n$$\n$$\n\\mathbf{z}_t = \\begin{pmatrix} 0.6 h_{t-1,1} + 0.3u - 0.2 \\\\ -0.5 h_{t-1,1} + 0.1u - 0.05 \\end{pmatrix}\n$$\nFor $\\mathbf{h}_t \\in \\mathcal{C}$, we need $z_{t,1}  0$ and $z_{t,2} \\leq 0$:\n1.  $z_{t,1}  0 \\implies h_{t,1} = 0.6 h_{t-1,1} + 0.3u - 0.2  0$.\n2.  $z_{t,2} \\leq 0 \\implies -0.5 h_{t-1,1} + 0.1u - 0.05 \\leq 0$.\n\nThe second inequality provides a lower bound on $h_{t-1,1}$:\n$$\n0.5 h_{t-1,1} \\geq 0.1u - 0.05 \\implies h_{t-1,1} \\geq \\frac{0.1u - 0.05}{0.5} \\implies h_{t-1,1} \\geq 0.2u - 0.1\n$$\nThis inequality must hold for all $t-1 \\geq 1$. Thus, the sequence of the first hidden state component, let's call it $x_t = h_{t,1}$, must satisfy $x_t \\geq 0.2u - 0.1$ for all $t \\geq 1$.\n\nThe dynamics of $x_t$ are given by the linear recurrence from the first condition:\n$x_t = 0.6 x_{t-1} + (0.3u - 0.2)$ for $t \\geq 2$, with $x_1 = 0.3u - 0.14$.\n\nFirst, consider the case where the lower bound is non-positive: $0.2u - 0.1 \\leq 0 \\implies u \\leq 0.5$. The full range on $u$ so far is $\\frac{7}{15}  u \\leq 1$. If $u \\in (\\frac{7}{15}, 0.5]$, then $0.3u - 0.2$ is negative. The fixed point of the recurrence for $x_t$ is $x_{fp} = \\frac{0.3u - 0.2}{1 - 0.6} = \\frac{0.3u - 0.2}{0.4} = 0.75u - 0.5$, which is also negative for $u \\leq 0.5$. The sequence $x_t$ starts at a positive value $x_1  0$ and converges to a negative fixed point $x_{fp}  0$. Therefore, $x_t$ must eventually become negative, violating the condition $x_t0$. Thus, there are no solutions for $u \\leq 0.5$.\n\nWe must have $u  0.5$. In this case, $0.2u - 0.1  0$. The condition $x_t \\geq 0.2u-0.1$ is stronger than and implies $x_t  0$. So we only need to ensure $x_t \\geq 0.2u - 0.1$ for all $t \\geq 1$. This is equivalent to requiring that the minimum of the sequence $\\{x_t\\}_{t\\geq 1}$ satisfies this bound: $\\min_{t \\geq 1} x_t \\geq 0.2u - 0.1$.\n\nThe sequence $x_t$ is monotonic, as it's a first-order linear recurrence. Its behavior depends on the relation between the start-point $x_1$ and the fixed-point $x_{fp}$.\n- $x_1 = 0.3u - 0.14$\n- $x_{fp} = 0.75u - 0.5$\nThe difference is $x_1 - x_{fp} = (0.3u - 0.14) - (0.75u - 0.5) = -0.45u + 0.36 = 0.09(4-5u)$.\nThe sign of the difference depends on whether $u$ is smaller or larger than $\\frac{4}{5} = 0.8$.\n\n**Case 1: $u  0.8$**\nIn this case, $x_1 - x_{fp}  0$, so $x_1  x_{fp}$. The sequence $\\{x_t\\}$ is monotonically decreasing and converges to $x_{fp}$ from above.\nThe minimum value is the infimum, $\\min_{t \\geq 1} x_t = \\inf_{t \\geq 1} x_t = x_{fp}$.\nThe condition becomes $x_{fp} \\geq 0.2u - 0.1$.\n$0.75u - 0.5 \\geq 0.2u - 0.1 \\implies 0.55u \\geq 0.4 \\implies u \\geq \\frac{0.4}{0.55} = \\frac{40}{55} = \\frac{8}{11}$.\nSo for this case we need $u \\in [\\frac{8}{11}, 0.8)$. Note that $\\frac{8}{11} \\approx 0.727$ which is consistent with $u  0.5$ and $u  7/15 \\approx 0.467$.\n\n**Case 2: $u  0.8$**\nIn this case, $x_1 - x_{fp}  0$, so $x_1  x_{fp}$. The sequence $\\{x_t\\}$ is monotonically increasing and converges to $x_{fp}$ from below.\nThe minimum value is the first term, $\\min_{t \\geq 1} x_t = x_1$.\nThe condition becomes $x_1 \\geq 0.2u - 0.1$.\n$0.3u - 0.14 \\geq 0.2u - 0.1 \\implies 0.1u \\geq 0.04 \\implies u \\geq 0.4$.\nFor this case, we have $u  0.8$. The condition $u \\geq 0.4$ is automatically satisfied. We must also satisfy the initial constraint $u \\leq 1$. So this case gives the range $u \\in (0.8, 1]$.\n\n**Case 3: $u = 0.8$**\nIn this case, $x_1 = x_{fp}$. The sequence is constant: $x_t = x_1$ for all $t\\geq 1$.\n$x_1 = 0.3(0.8) - 0.14 = 0.24 - 0.14 = 0.1$.\nThe condition is $x_t \\geq 0.2u - 0.1$.\n$0.1 \\geq 0.2(0.8) - 0.1 = 0.16 - 0.1 = 0.06$.\nThis is true. The initial constraint $u \\in (\\frac{7}{15}, 1]$ is also satisfied for $u=0.8$. So $u=0.8$ is a valid solution.\n\n### Step 5: Final Solution\n\nCombining the intervals from all cases:\n- From Case 1: $u \\in [\\frac{8}{11}, 0.8)$\n- From Case 3: $u = 0.8$\n- From Case 2: $u \\in (0.8, 1]$\n\nThe union of these sets gives the total range of valid values for $u$:\n$$\nu \\in \\left[\\frac{8}{11}, 0.8\\right) \\cup \\{0.8\\} \\cup (0.8, 1] = \\left[\\frac{8}{11}, 1\\right]\n$$\nThe problem asks for the largest real number $u$ that satisfies the condition. The largest value in the interval $[\\frac{8}{11}, 1]$ is $1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "3192158"}, {"introduction": "The mathematical models we study on paper often assume infinite precision, but real-world neural networks are deployed on hardware with finite numerical representations. This exercise investigates the practical consequences of these limitations by analyzing how small, unavoidable quantization errors accumulate in a recurrent system over time [@problem_id:3192083]. By deriving a worst-case bound on the divergence between the ideal and fixed-point trajectories, you will gain insight into the numerical stability of RNNs, using a technique that is conceptually similar to the analysis of the exploding gradient problem.", "problem": "Consider a scalar Recurrent Neural Network (RNN) with hyperbolic tangent activation function defined by the recurrence relation $h_{t}=\\tanh(w h_{t-1}+u x_{t}+b)$, where $h_{t}\\in\\mathbb{R}$ is the hidden state at time $t$, $w,u,b\\in\\mathbb{R}$ are fixed parameters, and $\\{x_{t}\\}$ is a given input sequence. Two trajectories are computed: one using real arithmetic (ideal floating-point) and one using fixed-point arithmetic with quantization unit $s0$, rounding to the nearest representable value at each elementary operation. Assume the following scientifically realistic conditions:\n- The initial hidden state is $h_{0}=0$ and is exactly representable.\n- The inputs satisfy $|x_{t}|\\leq 1$ for all $t$ and are exactly representable in the fixed-point system.\n- The parameters $w$, $u$, and $b$ are exactly representable in fixed-point.\n- Each multiplication and each addition performed to compute the preactivation $a_{t}=w h_{t-1}+u x_{t}+b$ is rounded to the nearest fixed-point value, incurring a rounding error in each operation with magnitude at most $s/2$. There are two multiplications and two additions in forming $a_{t}$, so the total preactivation error per time step is bounded in magnitude by $2 s$.\n- After evaluating the activation function $\\tanh(\\cdot)$ in real arithmetic, the resulting hidden state is rounded once to the nearest fixed-point value, incurring an additional rounding error of magnitude at most $s/2$.\n- The hyperbolic tangent function satisfies the Lipschitz continuity property $|\\tanh(a)-\\tanh(b)|\\leq L_{f}|a-b|$ for all $a,b\\in\\mathbb{R}$ with Lipschitz constant $L_{f}=1$.\n\nLet $h_{t}^{\\mathrm{float}}$ denote the hidden state computed with real arithmetic and $h_{t}^{\\mathrm{fix}}$ denote the hidden state computed with the described fixed-point arithmetic. Using only the above assumptions and fundamental definitions, derive a worst-case upper bound on $|h_{T}^{\\mathrm{fix}}-h_{T}^{\\mathrm{float}}|$ as an explicit closed-form function of $T$, $w$, $s$, and $L_{f}$. Then, for the concrete values $w=0.7$, $u=0.2$, $b=0$, $s=1.0\\times 10^{-4}$, $L_{f}=1$, and $T=50$, evaluate this bound numerically. Round your final numeric bound to four significant figures. Report a single real-valued number as your final answer.", "solution": "The problem is valid as it is scientifically grounded in the analysis of numerical errors in recurrent neural networks, is well-posed with a clear objective, and provides a self-contained, consistent set of assumptions.\n\nLet $h_{t}^{\\mathrm{float}}$ denote the hidden state computed with ideal real arithmetic and $h_{t}^{\\mathrm{fix}}$ denote the hidden state computed with fixed-point arithmetic. The problem asks for a worst-case upper bound on the error magnitude $|h_{T}^{\\mathrm{fix}} - h_{T}^{\\mathrm{float}}|$.\n\nThe ideal trajectory is given by the recurrence:\n$$h_{t}^{\\mathrm{float}} = \\tanh(w h_{t-1}^{\\mathrm{float}} + u x_{t} + b)$$\n\nThe fixed-point trajectory incorporates rounding errors. Let the preactivation argument in the fixed-point case be $a_{t}^{\\mathrm{comp}}$. Based on the problem description, this is the ideal preactivation based on the previous fixed-point state, plus a computational error $\\delta_{a,t}$. The total preactivation error, resulting from two multiplications and two additions, is bounded in magnitude by $2s$.\n$$a_{t}^{\\mathrm{comp}} = w h_{t-1}^{\\mathrm{fix}} + u x_{t} + b + \\delta_{a,t}, \\quad \\text{where } |\\delta_{a,t}| \\le 2s$$\nAfter the application of the $\\tanh$ function, the result is rounded to the nearest fixed-point value, introducing another error, $\\delta_{h,t}$, with magnitude at most $s/2$.\n$$h_{t}^{\\mathrm{fix}} = \\tanh(a_{t}^{\\mathrm{comp}}) + \\delta_{h,t} = \\tanh(w h_{t-1}^{\\mathrm{fix}} + u x_{t} + b + \\delta_{a,t}) + \\delta_{h,t}, \\quad \\text{where } |\\delta_{h,t}| \\le \\frac{s}{2}$$\n\nLet $\\Delta_{t} = h_{t}^{\\mathrm{fix}} - h_{t}^{\\mathrm{float}}$ be the error in the hidden state at time step $t$. We want to find an upper bound on $|\\Delta_{T}|$.\nFor any time step $t \\ge 1$:\n$$\\Delta_{t} = \\left[ \\tanh(w h_{t-1}^{\\mathrm{fix}} + u x_{t} + b + \\delta_{a,t}) + \\delta_{h,t} \\right] - \\left[ \\tanh(w h_{t-1}^{\\mathrm{float}} + u x_{t} + b) \\right]$$\nWe take the absolute value and apply the triangle inequality:\n$$|\\Delta_{t}| \\le \\left| \\tanh(w h_{t-1}^{\\mathrm{fix}} + u x_{t} + b + \\delta_{a,t}) - \\tanh(w h_{t-1}^{\\mathrm{float}} + u x_{t} + b) \\right| + |\\delta_{h,t}|$$\nThe hyperbolic tangent function $\\tanh(\\cdot)$ is Lipschitz continuous with constant $L_{f}=1$. This means for any two inputs $z_1, z_2$, $|\\tanh(z_1)-\\tanh(z_2)| \\le L_f |z_1-z_2|$. Applying this property:\n$$|\\Delta_{t}| \\le L_{f} \\left| (w h_{t-1}^{\\mathrm{fix}} + u x_{t} + b + \\delta_{a,t}) - (w h_{t-1}^{\\mathrm{float}} + u x_{t} + b) \\right| + |\\delta_{h,t}|$$\n$$|\\Delta_{t}| \\le L_{f} \\left| w h_{t-1}^{\\mathrm{fix}} - w h_{t-1}^{\\mathrm{float}} + \\delta_{a,t} \\right| + |\\delta_{h,t}|$$\n$$|\\Delta_{t}| \\le L_{f} \\left| w (h_{t-1}^{\\mathrm{fix}} - h_{t-1}^{\\mathrm{float}}) + \\delta_{a,t} \\right| + |\\delta_{h,t}|$$\nSubstituting $\\Delta_{t-1} = h_{t-1}^{\\mathrm{fix}} - h_{t-1}^{\\mathrm{float}}$:\n$$|\\Delta_{t}| \\le L_{f} \\left| w \\Delta_{t-1} + \\delta_{a,t} \\right| + |\\delta_{h,t}|$$\nUsing the triangle inequality on the first term:\n$$|\\Delta_{t}| \\le L_{f} (|w| |\\Delta_{t-1}| + |\\delta_{a,t}|) + |\\delta_{h,t}|$$\nTo find a worst-case bound, we substitute the maximum magnitudes for the error terms $|\\delta_{a,t}| \\le 2s$ and $|\\delta_{h,t}| \\le s/2$:\n$$|\\Delta_{t}| \\le L_{f} |w| |\\Delta_{t-1}| + L_{f}(2s) + \\frac{s}{2}$$\nThis is a linear recurrence relation for the error bound. Let $B_t$ be the upper bound for $|\\Delta_t|$, i.e., $|\\Delta_t| \\le B_t$. We have:\n$$B_t = (L_f |w|) B_{t-1} + \\left(2sL_f + \\frac{s}{2}\\right)$$\nThe initial condition is $h_0^{\\mathrm{fix}} = h_0^{\\mathrm{float}} = 0$ since $h_0=0$ is assumed to be exactly representable. Thus, $\\Delta_0 = 0$ and $B_0=0$.\nLet $\\gamma = L_f|w|$ and $\\epsilon = 2sL_f + \\frac{s}{2}$. The recurrence is $B_t = \\gamma B_{t-1} + \\epsilon$. We can unroll this recurrence:\n$B_0 = 0$\n$B_1 = \\gamma B_0 + \\epsilon = \\epsilon$\n$B_2 = \\gamma B_1 + \\epsilon = \\gamma \\epsilon + \\epsilon = \\epsilon(1+\\gamma)$\n$B_3 = \\gamma B_2 + \\epsilon = \\gamma(\\epsilon(1+\\gamma)) + \\epsilon = \\epsilon(1+\\gamma+\\gamma^2)$\nBy induction, the solution for $B_T$ is:\n$$B_T = \\epsilon \\sum_{k=0}^{T-1} \\gamma^k$$\nThis is a finite geometric series. The sum is given by $\\frac{\\gamma^T - 1}{\\gamma - 1}$ if $\\gamma \\neq 1$.\nSubstituting back the expressions for $\\gamma$ and $\\epsilon$:\n$$B_T = \\left(2sL_f + \\frac{s}{2}\\right) \\frac{(L_f|w|)^T - 1}{L_f|w| - 1}$$\nThis is the closed-form upper bound on $|h_{T}^{\\mathrm{fix}} - h_{T}^{\\mathrm{float}}|$ for the case $L_f|w| \\neq 1$. Since $|w|1$ and $L_f=1$, we can also write the fraction as $\\frac{1-(L_f|w|)^T}{1-L_f|w|}$. If $L_f|w|=1$, the sum would simply be $T$, leading to $B_T = \\epsilon T$.\n\nNow, we evaluate this bound for the given values: $w=0.7$, $s=1.0\\times 10^{-4}$, $L_{f}=1$, and $T=50$.\nFirst, calculate $\\gamma$ and $\\epsilon$:\n$$\\gamma = L_f|w| = 1 \\times |0.7| = 0.7$$\n$$\\epsilon = 2sL_f + \\frac{s}{2} = 2(1.0\\times 10^{-4})(1) + \\frac{1.0\\times 10^{-4}}{2} = 2.0\\times 10^{-4} + 0.5\\times 10^{-4} = 2.5\\times 10^{-4}$$\nSince $\\gamma=0.7 \\neq 1$, we use the geometric series formula for the bound $B_{50}$:\n$$B_{50} = \\epsilon \\frac{1 - \\gamma^T}{1 - \\gamma} = (2.5 \\times 10^{-4}) \\frac{1 - (0.7)^{50}}{1 - 0.7}$$\n$$B_{50} = (2.5 \\times 10^{-4}) \\frac{1 - (0.7)^{50}}{0.3}$$\nThe factor $\\frac{2.5}{0.3} = \\frac{25}{3}$.\n$$B_{50} = \\frac{25}{3} \\times 10^{-4} (1 - (0.7)^{50})$$\nWe now compute the numerical value:\n$(0.7)^{50} \\approx 1.7979 \\times 10^{-8}$\n$1 - (0.7)^{50} \\approx 1 - 1.7979 \\times 10^{-8} = 0.999999982021$\n$\\frac{25}{3} \\approx 8.333333...$\n$$B_{50} \\approx (8.333333...) \\times 10^{-4} \\times (0.999999982021)$$\n$$B_{50} \\approx 8.3333331835 \\times 10^{-4}$$\nThe problem requires rounding the final numeric bound to four significant figures. The first four significant figures are $8, 3, 3, 3$. The fifth digit is $3$, which is less than $5$, so we round down.\n$$B_{50} \\approx 8.333 \\times 10^{-4}$$\nOr, in decimal form, $0.0008333$.", "answer": "$$\\boxed{8.333 \\times 10^{-4}}$$", "id": "3192083"}]}