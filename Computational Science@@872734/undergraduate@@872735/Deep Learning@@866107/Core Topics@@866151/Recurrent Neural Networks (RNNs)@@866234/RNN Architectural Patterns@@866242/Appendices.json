{"hands_on_practices": [{"introduction": "A critical architectural decision in designing RNNs is how to apply supervision. This practice explores the fundamental trade-off between a many-to-many pattern, with a learning signal at every step, and a many-to-one pattern, with a single signal at the sequence's end. By implementing both approaches to learn a simple cumulative sum, you will gain tangible insights into how the density of supervision impacts gradient flow and convergence speed, a core concept in training recurrent models [@problem_id:3171353].", "problem": "You are asked to design and analyze a minimal Recurrent Neural Network (RNN) learning scenario to compare two supervision patterns: many-to-many with per-step losses versus many-to-one with final-only loss. The task must be framed purely mathematically and implemented as a complete, runnable program. The program must produce a single line of output as specified at the end.\n\nConsider a scalar RNN with hidden state update and output defined by\n$$\nh_t = \\alpha h_{t-1} + \\beta x_t,\\quad h_0 = 0,\\quad o_t = w h_t,\n$$\nwhere $x_t \\in \\mathbb{R}$ is the input at time $t$, $h_t \\in \\mathbb{R}$ is the hidden state, and $o_t \\in \\mathbb{R}$ is the output. Let the supervised targets be the cumulative sums\n$$\ny_t = \\sum_{i=1}^t x_i,\n$$\nand the final outcome $y = y_T = \\sum_{i=1}^T x_i$. The goal of learning is to recover the cumulative sum mapping. You must compare the following two learning regimes:\n- Many-to-many with per-step loss: define the mean squared error over all time steps,\n$$\n\\mathcal{L}_{\\text{per}} = \\frac{1}{NT}\\sum_{n=1}^N \\sum_{t=1}^T \\big(o_t^{(n)} - y_t^{(n)}\\big)^2,\n$$\n- Many-to-one with final-only loss:\n$$\n\\mathcal{L}_{\\text{final}} = \\frac{1}{N}\\sum_{n=1}^N \\big(o_T^{(n)} - y_T^{(n)}\\big)^2,\n$$\nwhere $N$ is the number of sequences, $T$ is the sequence length, and the superscript $(n)$ indexes sequences in the dataset.\n\nUse full-batch gradient descent with Backpropagation Through Time (BPTT) to optimize the parameters $\\alpha$, $\\beta$, and $w$. Start each training run with the same initial parameter values $\\alpha = 0.5$, $\\beta = 0.5$, $w = 0.5$.\n\nDataset generation: for a given $(N,T)$, draw $x_t^{(n)}$ independently from a standard normal distribution and hold the dataset fixed for both regimes during comparison. Compute $y_t^{(n)}$ exactly as the cumulative sum.\n\nConvergence measurement: for each training regime and parameter set, record the earliest epoch $e$ for which the corresponding training loss $\\mathcal{L}$ falls strictly below a given threshold $\\varepsilon$. If the threshold is not met by the maximum number of epochs, return that maximum number of epochs as the reported epoch count.\n\nTest suite: run the program on the following parameter sets. Each case must use its own random seed to ensure reproducibility, and the same dataset must be used for both regimes within a case.\n\n- Case $1$ (happy path): $N=32$, $T=20$, learning rate $\\eta = 0.05$, maximum epochs $E_{\\max} = 1000$, threshold $\\varepsilon = 0.005$, random seed $s=42$.\n- Case $2$ (boundary condition): $N=32$, $T=1$, learning rate $\\eta = 0.10$, maximum epochs $E_{\\max} = 300$, threshold $\\varepsilon = 0.0001$, random seed $s=43$.\n- Case $3$ (longer sequence edge case): $N=32$, $T=50$, learning rate $\\eta = 0.03$, maximum epochs $E_{\\max} = 1500$, threshold $\\varepsilon = 0.01$, random seed $s=44$.\n\nFinal output format: your program should produce a single line of output containing a comma-separated list of per-case results, where each case result is the pair $[e_{\\text{per}},e_{\\text{final}}]$ with no spaces inside brackets. For example, the final printed line must look like\n$$\n[[e_{\\text{per}}^{(1)},e_{\\text{final}}^{(1)}],[e_{\\text{per}}^{(2)},e_{\\text{final}}^{(2)}],[e_{\\text{per}}^{(3)},e_{\\text{final}}^{(3)}]]\n$$\nwith the actual integers computed by your program. The program must not print any other text.", "solution": "The problem requires a comparative analysis of two supervision patterns for a simple Recurrent Neural Network (RNN). The core of the solution lies in deriving and implementing the learning algorithm, full-batch gradient descent with Backpropagation Through Time (BPTT), for each of the two specified loss functions.\n\nThe RNN model is defined by the following equations for each sequence $n$ at time step $t$:\n$$\nh_t^{(n)} = \\alpha h_{t-1}^{(n)} + \\beta x_t^{(n)}\n$$\n$$\no_t^{(n)} = w h_t^{(n)}\n$$\nThe hidden state is initialized to zero, $h_0^{(n)} = 0$. The parameters to be learned are $\\alpha$, $\\beta$, and $w$. The task is to make the network output $o_t^{(n)}$ approximate the cumulative sum of the input sequence, $y_t^{(n)} = \\sum_{i=1}^t x_i^{(n)}$.\n\nThe goal is to find the optimal parameters $\\theta = (\\alpha, \\beta, w)$ by minimizing a loss function $\\mathcal{L}(\\theta)$. We use gradient descent, where parameters are updated iteratively according to the rule:\n$$\n\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}\n$$\nwhere $\\eta$ is the learning rate. The gradient $\\nabla_{\\theta} \\mathcal{L}$ is computed using BPTT. We analyze two distinct loss functions. For clarity in the derivations below, we consider the gradients for a single data sequence and then specify how to average them over the batch of $N$ sequences.\n\n**Regime 1: Many-to-Many with Per-Step Loss ($\\mathcal{L}_{\\text{per}}$)**\n\nThe loss function is the mean squared error averaged over all time steps and all sequences:\n$$\n\\mathcal{L}_{\\text{per}} = \\frac{1}{NT}\\sum_{n=1}^N \\sum_{t=1}^T \\big(o_t^{(n)} - y_t^{(n)}\\big)^2\n$$\nTo compute the gradients, we use the chain rule.\nThe gradient with respect to $w$ is straightforward as $w$ only locally influences the output $o_t$ at each step:\n$$\n\\frac{\\partial \\mathcal{L}_{\\text{per}}}{\\partial w} = \\frac{1}{NT}\\sum_{n=1}^N \\sum_{t=1}^T \\frac{\\partial}{\\partial w} (o_t^{(n)} - y_t^{(n)})^2 = \\frac{1}{NT}\\sum_{n=1}^N \\sum_{t=1}^T 2(o_t^{(n)} - y_t^{(n)}) h_t^{(n)}\n$$\nThe parameters $\\alpha$ and $\\beta$ influence the hidden state $h_t$ at all times, which in turn affects all subsequent outputs. BPTT provides a systematic way to compute these gradients. We define an error signal $\\delta_t = \\frac{\\partial \\mathcal{L}_{\\text{total}}}{\\partial h_t}$, where $\\mathcal{L}_{\\text{total}} = \\sum_{n,t} (o_t-y_t)^2$. This signal propagates backward in time. For a single sequence, the total loss is $L^{(n)} = \\sum_{t=1}^T (o_t^{(n)}-y_t^{(n)})^2$. The gradient of $L^{(n)}$ with respect to $h_t^{(n)}$ is:\n$$\n\\frac{\\partial L^{(n)}}{\\partial h_t^{(n)}} = \\frac{\\partial L^{(n)}}{\\partial o_t^{(n)}}\\frac{\\partial o_t^{(n)}}{\\partial h_t^{(n)}} + \\frac{\\partial L^{(n)}}{\\partial h_{t+1}^{(n)}}\\frac{\\partial h_{t+1}^{(n)}}{\\partial h_t^{(n)}}\n$$\nLet $\\delta_t^{(n)} = \\frac{\\partial L^{(n)}}{\\partial h_t^{(n)}}$. The recurrence relation for this error signal is:\n$$\n\\delta_t^{(n)} = 2(o_t^{(n)} - y_t^{(n)})w + \\delta_{t+1}^{(n)}\\alpha\n$$\nThis is computed backward from $t=T$ to $t=1$, with the base case $\\delta_{T+1}^{(n)} = 0$, so $\\delta_T^{(n)} = 2(o_T^{(n)} - y_T^{(n)})w$. The first term represents the error injected at the current time step, and the second represents the error propagated from the future.\nOnce all $\\delta_t^{(n)}$ are computed, the gradients for $\\alpha$ and $\\beta$ are found by summing their local contributions at each time step:\n$$\n\\frac{\\partial L^{(n)}}{\\partial \\alpha} = \\sum_{t=1}^T \\frac{\\partial L^{(n)}}{\\partial h_t^{(n)}}\\frac{\\partial h_t^{(n)}}{\\partial \\alpha} = \\sum_{t=1}^T \\delta_t^{(n)} h_{t-1}^{(n)}\n$$\n$$\n\\frac{\\partial L^{(n)}}{\\partial \\beta} = \\sum_{t=1}^T \\frac{\\partial L^{(n)}}{\\partial h_t^{(n)}}\\frac{\\partial h_t^{(n)}}{\\partial \\beta} = \\sum_{t=1}^T \\delta_t^{(n)} x_t^{(n)}\n$$\nThe final gradients for the full batch are obtained by averaging these sums over all $N$ samples and normalizing by $NT$:\n$\\nabla_{\\theta} \\mathcal{L}_{\\text{per}} = \\frac{1}{NT} \\sum_{n=1}^N \\nabla_{\\theta} L^{(n)}$.\n\n**Regime 2: Many-to-One with Final-Only Loss ($\\mathcal{L}_{\\text{final}}$)**\n\nThe loss function is the mean squared error at the final time step $T$:\n$$\n\\mathcal{L}_{\\text{final}} = \\frac{1}{N}\\sum_{n=1}^N \\big(o_T^{(n)} - y_T^{(n)}\\big)^2\n$$\nThe gradient computation follows a similar BPTT procedure, but the error signal originates only at $t=T$. For a single sequence, $L^{(n)} = (o_T^{(n)}-y_T^{(n)})^2$.\n\nThe gradient with respect to $w$ depends only on the final state and output:\n$$\n\\frac{\\partial L^{(n)}}{\\partial w} = 2(o_T^{(n)} - y_T^{(n)}) h_T^{(n)}\n$$\nThe error signal $\\delta_t^{(n)} = \\frac{\\partial L^{(n)}}{\\partial h_t^{(n)}}$ has a different structure. Since the loss depends only on $h_T$, for any $t  T$, the error must propagate from $h_{t+1}$:\n$$\n\\delta_t^{(n)} = \\frac{\\partial L^{(n)}}{\\partial h_{t+1}^{(n)}}\\frac{\\partial h_{t+1}^{(n)}}{\\partial h_t^{(n)}} = \\delta_{t+1}^{(n)}\\alpha\n$$\nThere is no local error injection for $t  T$. The base case at $t=T$ is:\n$$\n\\delta_T^{(n)} = \\frac{\\partial L^{(n)}}{\\partial o_T^{(n)}}\\frac{\\partial o_T^{(n)}}{\\partial h_T^{(n)}} = 2(o_T^{(n)} - y_T^{(n)})w\n$$\nThe gradients for $\\alpha$ and $\\beta$ are then computed as before:\n$$\n\\frac{\\partial L^{(n)}}{\\partial \\alpha} = \\sum_{t=1}^T \\delta_t^{(n)} h_{t-1}^{(n)}\n$$\n$$\n\\frac{\\partial L^{(n)}}{\\partial \\beta} = \\sum_{t=1}^T \\delta_t^{(n)} x_t^{(n)}\n$$\nThe key difference is that for $t  T$, the signal $\\delta_t^{(n)}$ is an attenuated version of $\\delta_T^{(n)}$ (i.e., $\\delta_t^{(n)} = \\delta_T^{(n)}\\alpha^{T-t}$). If $|\\alpha|1$, this can lead to vanishing gradients for early time steps, making it difficult to learn long-term dependencies.\nThe final gradients for the batch are averaged: $\\nabla_{\\theta} \\mathcal{L}_{\\text{final}} = \\frac{1}{N} \\sum_{n=1}^N \\nabla_{\\theta} L^{(n)}$.\n\nThe implementation proceeds by first generating the fixed dataset $(X, Y)$ for a given test case. Then, for each of the two regimes, a training loop is executed. In each epoch, the model performs a forward pass to compute outputs and the current loss. If the loss is below the threshold $\\varepsilon$, the epoch number is recorded. Otherwise, the gradients are computed via BPTT as derived above, and the parameters $(\\alpha, \\beta, w)$ are updated. If the loss does not fall below the threshold within $E_{\\max}$ epochs, $E_{\\max}$ is recorded. This process is repeated for all test cases. The special case where $T=1$ serves as a sanity check, as both loss functions become identical, and thus their training dynamics must be identical.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef train(X, Y, N, T, eta, E_max, eps, regime, initial_params):\n    \"\"\"\n    Trains the RNN for a given regime and returns the convergence epoch.\n    \"\"\"\n    alpha, beta, w = initial_params\n    \n    for epoch in range(1, E_max + 1):\n        # Forward pass\n        h = np.zeros((N, T))\n        o = np.zeros((N, T))\n        h_prev = np.zeros(N)\n        for t in range(T):\n            h_t = alpha * h_prev + beta * X[:, t]\n            h[:, t] = h_t\n            o[:, t] = w * h_t\n            h_prev = h_t\n\n        # Calculate loss based on the regime\n        if regime == 'per_step':\n            loss = np.mean((o - Y)**2)\n        elif regime == 'final_only':\n            loss = np.mean((o[:, -1] - Y[:, -1])**2)\n        else:\n            # This path should not be reached with valid inputs\n            raise ValueError(\"Invalid regime specified.\")\n\n        # Check for convergence\n        if loss  eps:\n            return epoch\n\n        # Backward pass (BPTT) and gradient calculation\n        h_padded = np.concatenate((np.zeros((N, 1)), h[:, :-1]), axis=1)\n\n        if regime == 'per_step':\n            do = 2 * (o - Y) / (N * T)\n            grad_w = np.sum(do * h)\n            \n            delta_h = np.zeros((N, T))\n            delta_h_next = np.zeros(N)\n            for t in range(T - 1, -1, -1):\n                current_delta_h = do[:, t] * w + delta_h_next * alpha\n                delta_h[:, t] = current_delta_h\n                delta_h_next = current_delta_h\n            \n            grad_alpha = np.sum(delta_h * h_padded)\n            grad_beta = np.sum(delta_h * X)\n            \n        elif regime == 'final_only':\n            errors_final = o[:, -1] - Y[:, -1]\n            do_T = 2 * errors_final / N\n            \n            grad_w = np.sum(do_T * h[:, -1])\n            \n            delta_h = np.zeros((N, T))\n            if T  0:\n                delta_h[:, -1] = do_T * w\n                for t in range(T - 2, -1, -1):\n                    delta_h[:, t] = delta_h[:, t + 1] * alpha\n            \n            grad_alpha = np.sum(delta_h * h_padded)\n            grad_beta = np.sum(delta_h * X)\n\n        # Update parameters using full-batch gradient descent\n        alpha -= eta * grad_alpha\n        beta -= eta * grad_beta\n        w -= eta * grad_w\n        \n    # Return max epochs if convergence threshold is not met\n    return E_max\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (N, T, learning_rate, max_epochs, threshold, random_seed)\n        (32, 20, 0.05, 1000, 0.005, 42),\n        (32, 1, 0.10, 300, 0.0001, 43),\n        (32, 50, 0.03, 1500, 0.01, 44),\n    ]\n\n    all_results = []\n    initial_params = (0.5, 0.5, 0.5)\n\n    for case in test_cases:\n        N, T, eta, E_max, eps, seed = case\n        \n        # Generate a fixed dataset for the case\n        np.random.seed(seed)\n        X = np.random.randn(N, T)\n        Y = np.cumsum(X, axis=1)\n\n        # Run training for the per-step loss regime\n        e_per = train(X, Y, N, T, eta, E_max, eps, 'per_step', initial_params)\n        \n        # Run training for the final-only loss regime\n        e_final = train(X, Y, N, T, eta, E_max, eps, 'final_only', initial_params)\n        \n        all_results.append(f\"[{e_per},{e_final}]\")\n\n    # Print the final output in the exact specified format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3171353"}, {"introduction": "Many real-world tasks involve processing a continuous input stream to predict events that occur at irregular, sparse intervals. This exercise addresses this \"many-to-sparse-many\" pattern by having you implement an RNN that is supervised only at specific event times. The core challenge, and the key learning objective, is to understand how Backpropagation Through Time correctly propagates gradients through the \"silent\" intervals, ensuring the model learns to maintain relevant information over time even without constant supervision [@problem_id:3171336].", "problem": "You are asked to design and implement an event-driven many-to-sparse-many Recurrent Neural Network (RNN) that receives a continuous input stream and produces outputs only at irregularly spaced event times. Your task is to derive the algorithm from first principles using Backpropagation Through Time (BPTT), implement it, and evaluate it on a small test suite.\n\nFundamental base and model definition. Start from the standard definition of a Recurrent Neural Network (RNN). Let the input at discrete time be $\\mathbf{x}_t \\in \\mathbb{R}^D$, the hidden state be $\\mathbf{h}_t \\in \\mathbb{R}^H$, and the output be $\\hat{\\mathbf{y}}_t \\in \\mathbb{R}^O$. The forward recurrence is defined by\n$$\n\\mathbf{h}_t = \\tanh(\\mathbf{W}_{xh}\\mathbf{x}_t + \\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{b}_h), \\quad \\hat{\\mathbf{y}}_t = \\mathbf{W}_{hy}\\mathbf{h}_t + \\mathbf{b}_y,\n$$\nwith initial state $\\mathbf{h}_{-1} = \\mathbf{0}$. Outputs are only evaluated at a specified event index set $\\mathcal{E} \\subset \\{0,1,\\ldots,T-1\\}$. Define an event mask $m_t \\in \\{0,1\\}$ where $m_t = 1$ if $t \\in \\mathcal{E}$ and $m_t = 0$ otherwise. The loss is defined only on event times as the sum of squared errors\n$$\n\\mathcal{L} = \\frac{1}{2} \\sum_{t=0}^{T-1} m_t \\left\\lVert \\hat{\\mathbf{y}}_t - \\mathbf{y}_t \\right\\rVert_2^2,\n$$\nwhere $\\mathbf{y}_t$ is the target only defined when $m_t = 1$. Your implementation must compute gradients by Backpropagation Through Time, and crucially, propagate gradients through the \"silent\" intervals where $m_t = 0$, since those hidden states influence future event outputs.\n\nArchitectural and dimensional specification. Use the following fixed dimensions for all test cases: $D = 2$, $H = 3$, and $O = 1$. Initialize the parameters identically for every test case as\n$$\n\\mathbf{W}_{xh} = \\begin{bmatrix} 0.2  -0.1 \\\\ 0.0  0.1 \\\\ -0.1  0.2 \\end{bmatrix}, \\quad\n\\mathbf{W}_{hh} = \\begin{bmatrix} 0.1  0.0  -0.1 \\\\ 0.05  0.1  0.0 \\\\ 0.0  -0.05  0.1 \\end{bmatrix}, \\quad\n\\mathbf{b}_h = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\n\\mathbf{W}_{hy} = \\begin{bmatrix} 0.3  -0.2  0.1 \\end{bmatrix}, \\quad \\mathbf{b}_y = \\begin{bmatrix} 0.0 \\end{bmatrix}.\n$$\nUse a single gradient descent update step with learning rate $\\alpha = 0.1$.\n\nTest suite. For each test case below, you must compute the total loss before the gradient step and after one gradient step. Always use the zero vector for the initial hidden state. In every case, the inputs are a sequence $\\{\\mathbf{x}_t\\}_{t=0}^{T-1}$, the event index set is $\\mathcal{E}$, and the targets are defined for $t \\in \\mathcal{E}$.\n\n- Case $\\mathrm{A}$: $T=5$, inputs\n$\\mathbf{x}_0 = \\begin{bmatrix} 0.5 \\\\ -0.2 \\end{bmatrix}$,\n$\\mathbf{x}_1 = \\begin{bmatrix} -0.3 \\\\ 0.1 \\end{bmatrix}$,\n$\\mathbf{x}_2 = \\begin{bmatrix} 0.0 \\\\ 0.2 \\end{bmatrix}$,\n$\\mathbf{x}_3 = \\begin{bmatrix} -0.1 \\\\ -0.1 \\end{bmatrix}$,\n$\\mathbf{x}_4 = \\begin{bmatrix} 0.2 \\\\ 0.0 \\end{bmatrix}$;\nevent set $\\mathcal{E} = \\{1,4\\}$; targets $y_1 = 0.1$, $y_4 = -0.05$.\n\n- Case $\\mathrm{B}$: $T=6$, inputs\n$\\mathbf{x}_0 = \\begin{bmatrix} 0.1 \\\\ 0.0 \\end{bmatrix}$,\n$\\mathbf{x}_1 = \\begin{bmatrix} 0.0 \\\\ 0.1 \\end{bmatrix}$,\n$\\mathbf{x}_2 = \\begin{bmatrix} -0.2 \\\\ 0.2 \\end{bmatrix}$,\n$\\mathbf{x}_3 = \\begin{bmatrix} 0.3 \\\\ -0.3 \\end{bmatrix}$,\n$\\mathbf{x}_4 = \\begin{bmatrix} 0.0 \\\\ 0.2 \\end{bmatrix}$,\n$\\mathbf{x}_5 = \\begin{bmatrix} -0.1 \\\\ 0.0 \\end{bmatrix}$;\nevent set $\\mathcal{E} = \\{5\\}$; target $y_5 = 0.2$.\n\n- Case $\\mathrm{C}$: $T=4$, inputs\n$\\mathbf{x}_0 = \\begin{bmatrix} 0.2 \\\\ 0.2 \\end{bmatrix}$,\n$\\mathbf{x}_1 = \\begin{bmatrix} -0.2 \\\\ 0.1 \\end{bmatrix}$,\n$\\mathbf{x}_2 = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$,\n$\\mathbf{x}_3 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$;\nevent set $\\mathcal{E} = \\emptyset$; no targets.\n\n- Case $\\mathrm{D}$: $T=7$, inputs\n$\\mathbf{x}_0 = \\begin{bmatrix} 0.0 \\\\ 0.1 \\end{bmatrix}$,\n$\\mathbf{x}_1 = \\begin{bmatrix} 0.1 \\\\ 0.0 \\end{bmatrix}$,\n$\\mathbf{x}_2 = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix}$,\n$\\mathbf{x}_3 = \\begin{bmatrix} -0.2 \\\\ 0.2 \\end{bmatrix}$,\n$\\mathbf{x}_4 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix}$,\n$\\mathbfx}_5 = \\begin{bmatrix} -0.1 \\\\ 0.1 \\end{bmatrix}$,\n$\\mathbf{x}_6 = \\begin{bmatrix} 0.0 \\\\ -0.1 \\end{bmatrix}$;\nevent set $\\mathcal{E} = \\{2,3\\}$; targets $y_2 = -0.1$, $y_3 = 0.05$.\n\nProcessing and required outputs. For each case, perform a complete forward pass to compute $\\mathcal{L}_{\\text{before}}$, compute exact gradients by Backpropagation Through Time that respect the event mask $m_t$, take one parameter update with learning rate $\\alpha$, and compute the new loss $\\mathcal{L}_{\\text{after}}$. Your program should produce a single line of output containing the results as a flat, comma-separated list of floats rounded to $6$ decimal places and enclosed in square brackets, ordered as\n$$\n[\\mathcal{L}_{\\text{before}}^{(A)}, \\mathcal{L}_{\\text{after}}^{(A)}, \\mathcal{L}_{\\text{before}}^{(B)}, \\mathcal{L}_{\\text{after}}^{(B)}, \\mathcal{L}_{\\text{before}}^{(C)}, \\mathcal{L}_{\\text{after}}^{(C)}, \\mathcal{L}_{\\text{before}}^{(D)}, \\mathcal{L}_{\\text{after}}^{(D)}].\n$$\nNo other text should be printed. There are no physical units involved. Angles do not appear. All numerical answers must be floats.", "solution": "The user-provided problem requires the design and implementation of an event-driven many-to-sparse-many Recurrent Neural Network (RNN). The core of the task is to derive and implement the Backpropagation Through Time (BPTT) algorithm for this specific architecture, where the loss is computed only at discrete, irregular event times. The solution will proceed by first detailing the forward propagation of states and outputs, followed by a rigorous derivation of the BPTT algorithm for computing the gradients of the loss function with respect to all model parameters. Finally, the gradient descent update rule will be stated, completing the algorithm's specification.\n\nLet the model parameters be denoted by the set $\\theta = \\{\\mathbf{W}_{xh}, \\mathbf{W}_{hh}, \\mathbf{b}_h, \\mathbf{W}_{hy}, \\mathbf{b}_y\\}$. The dimensions are given as input $D=2$, hidden $H=3$, and output $O=1$.\n\n**1. Forward Propagation**\n\nThe forward pass computes the network's state and output sequence. Given an input sequence $\\{\\mathbf{x}_t\\}_{t=0}^{T-1}$ and an initial hidden state $\\mathbf{h}_{-1} = \\mathbf{0}$, the network's evolution is described by the following recurrence relations for $t = 0, 1, \\ldots, T-1$:\n\nThe pre-activation of the hidden layer at time $t$ is a linear combination of the current input $\\mathbf{x}_t$ and the previous hidden state $\\mathbf{h}_{t-1}$:\n$$\n\\mathbf{a}_t = \\mathbf{W}_{xh}\\mathbf{x}_t + \\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{b}_h\n$$\nThe hidden state $\\mathbf{h}_t$ is obtained by applying the hyperbolic tangent activation function element-wise:\n$$\n\\mathbf{h}_t = \\tanh(\\mathbf{a}_t)\n$$\nThe network output $\\hat{\\mathbf{y}}_t$ is a linear transformation of the current hidden state:\n$$\n\\hat{\\mathbf{y}}_t = \\mathbf{W}_{hy}\\mathbf{h}_t + \\mathbf{b}_y\n$$\nThe total loss $\\mathcal{L}$ is the sum of squared errors, calculated only at event times specified by the set $\\mathcal{E}$. Using the event mask $m_t$ (where $m_t=1$ if $t \\in \\mathcal{E}$ and $m_t=0$ otherwise), the loss is:\n$$\n\\mathcal{L} = \\frac{1}{2} \\sum_{t=0}^{T-1} m_t \\left\\lVert \\hat{\\mathbf{y}}_t - \\mathbf{y}_t \\right\\rVert_2^2\n$$\nIn this problem, since $O=1$, the output $\\hat{y}_t$ and target $y_t$ are scalars, so the squared norm becomes a simple square: $\\mathcal{L} = \\frac{1}{2} \\sum_{t=0}^{T-1} m_t (\\hat{y}_t - y_t)^2$.\n\n**2. Backward Propagation Through Time (BPTT)**\n\nTo update the parameters using gradient descent, we must compute the gradient of the total loss $\\mathcal{L}$ with respect to each parameter in $\\theta$. This is achieved by applying the chain rule recursively, moving backward in time from $t=T-1$ to $t=0$.\n\n**Gradients of Output Layer Parameters ($\\mathbf{W}_{hy}, \\mathbf{b}_y$)**\n\nThe loss $\\mathcal{L}$ depends on $\\mathbf{W}_{hy}$ and $\\mathbf{b}_y$ only through the outputs $\\hat{\\mathbf{y}}_t$. The gradient of the loss with respect to the output at time $t$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}_t} = m_t (\\hat{\\mathbf{y}}_t - \\mathbf{y}_t)\n$$\nUsing the chain rule, we find the gradients for $\\mathbf{W}_{hy}$ and $\\mathbf{b}_y$ by summing contributions from all time steps:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{hy}} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}_t} \\frac{\\partial \\hat{\\mathbf{y}}_t}{\\partial \\mathbf{W}_{hy}} = \\sum_{t=0}^{T-1} m_t (\\hat{\\mathbf{y}}_t - \\mathbf{y}_t) \\mathbf{h}_t^T\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_y} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}_t} \\frac{\\partial \\hat{\\mathbf{y}}_t}{\\partial \\mathbf{b}_y} = \\sum_{t=0}^{T-1} m_t (\\hat{\\mathbf{y}}_t - \\mathbf{y}_t)\n$$\n\n**Gradients of Hidden Layer Parameters ($\\mathbf{W}_{xh}, \\mathbf{W}_{hh}, \\mathbf{b}_h$)**\n\nThe gradients for the hidden layer parameters are more complex because the hidden state $\\mathbf{h}_t$ influences the loss at time $t$ and all subsequent times. BPTT provides a systematic way to compute these gradients. The key is to compute the gradient of the loss with respect to the hidden pre-activation, $\\delta_t \\equiv \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_t}$.\n\nThe gradient $\\delta_t$ can be found via the chain rule: $\\delta_t = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{a}_t}$.\nThe derivative of the activation function is $\\frac{d \\tanh(z)}{dz} = 1 - \\tanh^2(z)$. Element-wise, this is $\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{a}_t} = \\text{diag}(1-\\mathbf{h}_t^2)$, where the squares are element-wise.\nThe gradient $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t}$ has two components: the influence of $\\mathbf{h}_t$ on the current output $\\hat{\\mathbf{y}}_t$, and its influence on the next hidden state's pre-activation $\\mathbf{a}_{t+1}$.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}_t} \\frac{\\partial \\hat{\\mathbf{y}}_t}{\\partial \\mathbf{h}_t}}_{\\text{from current output}} + \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_{t+1}} \\frac{\\partial \\mathbf{a}_{t+1}}{\\partial \\mathbf{h}_t}}_{\\text{from next state}} = \\mathbf{W}_{hy}^T m_t(\\hat{\\mathbf{y}}_t - \\mathbf{y}_t) + \\mathbf{W}_{hh}^T \\delta_{t+1}\n$$\nCombining these gives the backward recurrence for $\\delta_t$:\n$$\n\\delta_t = \\left( \\mathbf{W}_{hy}^T m_t(\\hat{\\mathbf{y}}_t - \\mathbf{y}_t) + \\mathbf{W}_{hh}^T \\delta_{t+1} \\right) \\circ (1 - \\mathbf{h}_t^2)\n$$\nwhere $\\circ$ denotes the element-wise (Hadamard) product. The recurrence is initialized with $\\delta_T = \\mathbf{0}$, as there is no state after $t=T-1$.\n\nWith $\\delta_t$ computed for all $t$, we can find the gradients for the hidden layer parameters by summing over time:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{xh}} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_t} \\frac{\\partial \\mathbf{a}_t}{\\partial \\mathbf{W}_{xh}} = \\sum_{t=0}^{T-1} \\delta_t \\mathbf{x}_t^T\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{hh}} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_t} \\frac{\\partial \\mathbf{a}_t}{\\partial \\mathbf{W}_{hh}} = \\sum_{t=0}^{T-1} \\delta_t \\mathbf{h}_{t-1}^T\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_h} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_t} \\frac{\\partial \\mathbf{a}_t}{\\partial \\mathbf{b}_h} = \\sum_{t=0}^{T-1} \\delta_t\n$$\nCrucially, even when $m_t=0$ (a \"silent\" time step), the term $\\mathbf{W}_{hh}^T \\delta_{t+1}$ propagates the gradient from future events, ensuring that the hidden states at non-event times are trained to carry information that will be useful for future predictions.\n\n**3. Parameter Update**\n\nAfter computing all gradients, a single step of gradient descent is performed with learning rate $\\alpha = 0.1$. For any parameter $\\mathbf{P} \\in \\theta$:\n$$\n\\mathbf{P}_{\\text{new}} = \\mathbf{P}_{\\text{old}} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{P}}\n$$\n\n**4. Algorithm Summary**\n\nFor each test case, the following procedure is executed:\n1.  **Initial Forward Pass:** Perform a forward pass using the initial parameters to compute all hidden states $\\{\\mathbf{h}_t\\}$ and outputs $\\{\\hat{\\mathbf{y}}_t\\}$, and calculate the initial total loss, $\\mathcal{L}_{\\text{before}}$. Store intermediate values ($\\mathbf{x}_t, \\mathbf{h}_t, \\mathbf{h}_{t-1}$) for the backward pass.\n2.  **Backward Pass:** Execute the BPTT algorithm as derived above, iterating from $t=T-1$ down to $0$, to compute the gradients of the loss with respect to all parameters.\n3.  **Parameter Update:** Compute the new parameter values using the gradient descent update rule.\n4.  **Final Forward Pass:** Perform a new forward pass using the updated parameters to compute the new total loss, $\\mathcal{L}_{\\text{after}}$.\nThe results $[\\mathcal{L}_{\\text{before}}, \\mathcal{L}_{\\text{after}}]$ for each case are then collected.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the event-driven RNN problem by implementing BPTT,\n    running it on four test cases, and printing the formatted results.\n    \"\"\"\n\n    def run_case(params, T, xs, event_set, targets_dict, D, H, O, alpha):\n        \"\"\"\n        Runs a single test case: forward pass, backward pass, parameter update,\n        and a final forward pass to compute before/after losses.\n        \"\"\"\n        W_xh, W_hh, b_h, W_hy, b_y = params\n\n        # 1. First forward pass to compute L_before\n        h_s = {-1: np.zeros((H, 1))}\n        a_s = {}\n        y_hats = {}\n        loss_before = 0.0\n\n        for t in range(T):\n            x_t = xs[t].reshape(D, 1)\n            \n            # Recurrence relation\n            a_s[t] = W_xh @ x_t + W_hh @ h_s[t-1] + b_h\n            h_s[t] = np.tanh(a_s[t])\n            \n            # Output computation\n            y_hat_t = W_hy @ h_s[t] + b_y\n            y_hats[t] = y_hat_t\n            \n            # Loss calculation at event times\n            if t in event_set:\n                y_t = np.array([[targets_dict[t]]])\n                loss_before += 0.5 * np.sum((y_hat_t - y_t)**2)\n\n        # 2. Backward pass (BPTT)\n        # Initialize gradients to zero\n        dW_xh, dW_hh, db_h = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(b_h)\n        dW_hy, db_y = np.zeros_like(W_hy), np.zeros_like(b_y)\n        \n        # Initialize gradient from the future (dL/da_{t+1}) to zero\n        delta_a_next = np.zeros((H, 1))\n\n        # Iterate backwards through time from T-1 to 0\n        for t in reversed(range(T)):\n            # Gradient of loss w.r.t. output y_hat_t\n            dy_hat = np.zeros((O, 1))\n            if t in event_set:\n                y_t = np.array([[targets_dict[t]]])\n                dy_hat = y_hats[t] - y_t\n            \n            # Accumulate gradients for output layer parameters\n            dW_hy += dy_hat @ h_s[t].T\n            db_y += dy_hat\n            \n            # Backpropagate gradient to hidden state h_t\n            # This is dL/dh_t, combining gradient from output and from next state\n            delta_h = W_hy.T @ dy_hat + W_hh.T @ delta_a_next\n            \n            # Backpropagate through tanh non-linearity to get dL/da_t\n            delta_a = delta_h * (1 - h_s[t]**2)\n\n            # Accumulate gradients for hidden layer parameters\n            dW_hh += delta_a @ h_s[t-1].T\n            dW_xh += delta_a @ xs[t].reshape(D, 1).T\n            db_h += delta_a\n            \n            # Pass the gradient w.r.t. pre-activation to the previous time step\n            delta_a_next = delta_a\n\n        # 3. Update parameters using gradient descent\n        W_xh_new = W_xh - alpha * dW_xh\n        W_hh_new = W_hh - alpha * dW_hh\n        b_h_new = b_h - alpha * db_h\n        W_hy_new = W_hy - alpha * dW_hy\n        b_y_new = b_y - alpha * db_y\n\n        # 4. Second forward pass with new parameters to compute L_after\n        h_s_new = {-1: np.zeros((H, 1))}\n        loss_after = 0.0\n        \n        for t in range(T):\n            x_t = xs[t].reshape(D, 1)\n            a_t_new = W_xh_new @ x_t + W_hh_new @ h_s_new[t-1] + b_h_new\n            h_s_new[t] = np.tanh(a_t_new)\n            y_hat_t_new = W_hy_new @ h_s_new[t] + b_y_new\n            \n            if t in event_set:\n                y_t = np.array([[targets_dict[t]]])\n                loss_after += 0.5 * np.sum((y_hat_t_new - y_t)**2)\n                \n        return loss_before, loss_after\n\n    # Define fixed model parameters and dimensions\n    D, H, O = 2, 3, 1\n    alpha = 0.1\n    W_xh_init = np.array([[0.2, -0.1], [0.0, 0.1], [-0.1, 0.2]])\n    W_hh_init = np.array([[0.1, 0.0, -0.1], [0.05, 0.1, 0.0], [0.0, -0.05, 0.1]])\n    b_h_init = np.array([[0.0], [0.0], [0.0]])\n    W_hy_init = np.array([[0.3, -0.2, 0.1]])\n    b_y_init = np.array([[0.0]])\n    initial_params = (W_xh_init, W_hh_init, b_h_init, W_hy_init, b_y_init)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            \"T\": 5,\n            \"xs\": [np.array([0.5, -0.2]), np.array([-0.3, 0.1]), np.array([0.0, 0.2]), np.array([-0.1, -0.1]), np.array([0.2, 0.0])],\n            \"event_set\": {1, 4},\n            \"targets_dict\": {1: 0.1, 4: -0.05}\n        },\n        # Case B\n        {\n            \"T\": 6,\n            \"xs\": [np.array([0.1, 0.0]), np.array([0.0, 0.1]), np.array([-0.2, 0.2]), np.array([0.3, -0.3]), np.array([0.0, 0.2]), np.array([-0.1, 0.0])],\n            \"event_set\": {5},\n            \"targets_dict\": {5: 0.2}\n        },\n        # Case C\n        {\n            \"T\": 4,\n            \"xs\": [np.array([0.2, 0.2]), np.array([-0.2, 0.1]), np.array([0.1, -0.1]), np.array([0.0, 0.0])],\n            \"event_set\": set(),\n            \"targets_dict\": {}\n        },\n        # Case D\n        {\n            \"T\": 7,\n            \"xs\": [np.array([0.0, 0.1]), np.array([0.1, 0.0]), np.array([0.2, -0.1]), np.array([-0.2, 0.2]), np.array([0.1, -0.2]), np.array([-0.1, 0.1]), np.array([0.0, -0.1])],\n            \"event_set\": {2, 3},\n            \"targets_dict\": {2: -0.1, 3: 0.05}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Each case must start with the same initial parameters\n        params_copy = tuple(p.copy() for p in initial_params)\n        \n        loss_before, loss_after = run_case(\n            params_copy,\n            case[\"T\"],\n            case[\"xs\"],\n            case[\"event_set\"],\n            case[\"targets_dict\"],\n            D, H, O, alpha\n        )\n        results.append(loss_before)\n        results.append(loss_after)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3171336"}, {"introduction": "Standard RNN classifiers produce ordered outputs, but what if the target is an unordered collection of items? This practice delves into the \"sequence-to-set\" problem, a common scenario in fields like multi-label classification. You will mathematically derive a loss function that handles permutation-invariant targets by marginalizing over all possible assignments between the model's outputs and the elements of the target set, a powerful and principled technique for training models to predict sets [@problem_id:3171341].", "problem": "You are designing a sequence-to-set predictor using a Recurrent Neural Network (RNN). A length-$T$ input sequence $x_{1:T}$ is encoded by the RNN into hidden states $h_{1:T}$, which are then aggregated by a permutation-invariant set function of the Deep Sets form applied over time, specifically the sum aggregator $u=\\sum_{t=1}^{T} h_t$. A head then produces $m$ independent categorical distributions over $K$ classes from $u$ via $m$ separate affine maps with a softmax, that is, for slot $s \\in \\{1,\\dots,m\\}$ and class index $k \\in \\{1,\\dots,K\\}$,\n$$\np_{s}(y=k \\mid x_{1:T}) \\;=\\; \\frac{\\exp\\!\\left(z_{s,k}\\right)}{\\sum_{j=1}^{K}\\exp\\!\\left(z_{s,j}\\right)},\n$$\nwhere $z_{s} \\in \\mathbb{R}^{K}$ are the logits of slot $s$.\n\nThe target output is an unordered set $S=\\{y_{1},\\dots,y_{m}\\}$ of $m$ distinct class labels. There is no canonical ordering of the elements of $S$, but the model emits $m$ slot-wise categorical distributions. You will treat the unknown correspondence between the $m$ emitted slots and the $m$ elements of $S$ as a latent permutation.\n\nStarting from the laws of probability and the definition of the negative log-likelihood, do the following:\n\n1. Introduce a latent permutation variable $\\pi$ ranging over the symmetric group of $m$ elements and derive an expression for the likelihood of observing the unordered set $S$ given $x_{1:T}$ by marginalizing over $\\pi$. Then write the negative log-likelihood loss for this unordered target.\n\n2. Now specialize to the concrete case $m=2$ and $K=3$. The class set is indexed as $\\{c_{1},c_{2},c_{3}\\}$. For a particular input sequence $x_{1:T}$, the two slot logits are given by\n$$\nz_{1} \\;=\\; \\begin{pmatrix} 0.7  -0.2  1.1 \\end{pmatrix}, \\quad\nz_{2} \\;=\\; \\begin{pmatrix} 1.0  0.0  -0.5 \\end{pmatrix},\n$$\nwhere the $k$-th entry of $z_{s}$ corresponds to class $c_{k}$. The ground-truth unordered set is $S=\\{c_{1},c_{3}\\}$. Using the expression you derived, compute the resulting negative log-likelihood loss. Use the natural logarithm $\\ln$ and round your final numerical answer to four significant figures. The answer is dimensionless; do not include units.", "solution": "### Part 1: Derivation of the Negative Log-Likelihood Loss\n\nThe problem requires a model to predict an unordered set of $m$ distinct class labels, $S = \\{y_1, \\dots, y_m\\}$, where each $y_i \\in \\{1, \\dots, K\\}$. The model produces $m$ independent categorical distributions from $m$ slots. For each slot $s \\in \\{1, \\dots, m\\}$, the probability of emitting class $k \\in \\{1, \\dots, K\\}$ is given by the softmax function applied to a vector of logits $z_s \\in \\mathbb{R}^K$:\n$$\np_s(k) \\equiv p(y=k \\mid x_{1:T}, \\text{slot } s) = \\frac{\\exp(z_{s,k})}{\\sum_{j=1}^{K} \\exp(z_{s,j})}\n$$\nSince the target $S$ is an unordered set, there is no pre-defined correspondence between the model's $m$ output slots and the $m$ target labels in $S$. We must account for all possible one-to-one assignments. An assignment can be represented by a permutation $\\pi \\in S_m$, where $S_m$ is the symmetric group on $m$ elements. Let the elements of the target set $S$ be arbitrarily indexed as a tuple $(y'_1, \\dots, y'_m)$. A permutation $\\pi$ defines an assignment where slot $s$ is matched with target label $y'_{\\pi(s)}$ for $s=1, \\dots, m$.\n\nThe likelihood of a single such assignment, specified by $\\pi$, is the joint probability of all slots producing their assigned target labels. Due to the independence of the slots, this is the product of the individual slot probabilities:\n$$\nP(\\text{assignment } \\pi \\mid x_{1:T}) = \\prod_{s=1}^{m} p_s(y'_{\\pi(s)})\n$$\nTo find the total likelihood of observing the unordered set $S$, we must sum the likelihoods of all possible assignments. This is equivalent to marginalizing over the latent permutation variable $\\pi$. Since there are $m!$ permutations in $S_m$, the total likelihood $L(S \\mid x_{1:T})$ is:\n$$\nL(S \\mid x_{1:T}) = P(S \\mid x_{1:T}) = \\sum_{\\pi \\in S_m} \\prod_{s=1}^{m} p_s(y'_{\\pi(s)})\n$$\nThe negative log-likelihood (NLL) loss, denoted $\\mathcal{L}$, is the negative natural logarithm of this total likelihood:\n$$\n\\mathcal{L} = -\\ln(L(S \\mid x_{1:T})) = -\\ln\\left( \\sum_{\\pi \\in S_m} \\prod_{s=1}^{m} p_s(y'_{\\pi(s)}) \\right)\n$$\nThis expression represents the loss for an unordered target set.\n\n### Part 2: Computation for the Concrete Case\n\nWe are given the following specific values:\n- Number of slots/target labels: $m=2$\n- Number of classes: $K=3$, indexed as $\\{c_1, c_2, c_3\\}$\n- Slot logits:\n  $$\n  z_1 = \\begin{pmatrix} 0.7  -0.2  1.1 \\end{pmatrix}\n  $$\n  $$\n  z_2 = \\begin{pmatrix} 1.0  0.0  -0.5 \\end{pmatrix}\n  $$\n- Ground-truth unordered set: $S=\\{c_1, c_3\\}$\n\nFor $m=2$, the symmetric group $S_2$ contains two permutations: the identity $\\pi_1=(1, 2)$ and the swap $\\pi_2=(2, 1)$. Let's denote the target labels as $y'_1=c_1$ and $y'_2=c_3$.\n\nThe total likelihood is the sum of the likelihoods of two possible assignments:\n1. Slot 1 predicts $c_1$ and Slot 2 predicts $c_3$. The likelihood is $p_1(c_1) p_2(c_3)$.\n2. Slot 1 predicts $c_3$ and Slot 2 predicts $c_1$. The likelihood is $p_1(c_3) p_2(c_1)$.\n\nThe total likelihood is $L(S \\mid x_{1:T}) = p_1(c_1)p_2(c_3) + p_1(c_3)p_2(c_1)$.\nThe NLL loss is therefore:\n$$\n\\mathcal{L} = -\\ln\\left( p_1(c_1)p_2(c_3) + p_1(c_3)p_2(c_1) \\right)\n$$\nTo compute this numerically, we can first compute the probabilities $p_s(k)$. However, for numerical stability, it is preferable to work in the log domain. Let's substitute the definition of $p_s(k)$ into the likelihood expression:\n$$\nL = \\frac{\\exp(z_{1,c_1})}{\\sum_{j=1}^3 \\exp(z_{1,j})} \\frac{\\exp(z_{2,c_3})}{\\sum_{j=1}^3 \\exp(z_{2,j})} + \\frac{\\exp(z_{1,c_3})}{\\sum_{j=1}^3 \\exp(z_{1,j})} \\frac{\\exp(z_{2,c_1})}{\\sum_{j=1}^3 \\exp(z_{2,j})}\n$$\n$$\nL = \\frac{\\exp(z_{1,c_1} + z_{2,c_3}) + \\exp(z_{1,c_3} + z_{2,c_1})}{\\left(\\sum_{j=1}^3 \\exp(z_{1,j})\\right) \\left(\\sum_{j=1}^3 \\exp(z_{2,j})\\right)}\n$$\nThe NLL loss is $\\mathcal{L} = -\\ln(L)$. Using the properties of logarithms, we can write:\n$$\n\\mathcal{L} = \\ln\\left(\\sum_{j=1}^3 \\exp(z_{1,j})\\right) + \\ln\\left(\\sum_{j=1}^3 \\exp(z_{2,j})\\right) - \\ln\\left(\\exp(z_{1,c_1} + z_{2,c_3}) + \\exp(z_{1,c_3} + z_{2,c_1})\\right)\n$$\nThis form involving log-sum-exp operations is numerically stable. Let's compute each term. The logits corresponding to classes $(c_1, c_2, c_3)$ are:\n$z_1 = (0.7, -0.2, 1.1)$\n$z_2 = (1.0, 0.0, -0.5)$\n\nThe first term is the log-sum-exp of $z_1$:\n$$\n\\ln(\\exp(0.7) + \\exp(-0.2) + \\exp(1.1)) \\approx \\ln(2.01375 + 0.81873 + 3.00417) = \\ln(5.83665) \\approx 1.76413\n$$\nThe second term is the log-sum-exp of $z_2$:\n$$\n\\ln(\\exp(1.0) + \\exp(0.0) + \\exp(-0.5)) \\approx \\ln(2.71828 + 1 + 0.60653) = \\ln(4.32481) \\approx 1.46431\n$$\nFor the third term, we first compute the sums of logits for each permutation:\n- Assignment 1 (slot 1 $\\to c_1$, slot 2 $\\to c_3$): Sum of logits = $z_{1,c_1} + z_{2,c_3} = 0.7 + (-0.5) = 0.2$\n- Assignment 2 (slot 1 $\\to c_3$, slot 2 $\\to c_1$): Sum of logits = $z_{1,c_3} + z_{2,c_1} = 1.1 + 1.0 = 2.1$\n\nThe third term is the log-sum-exp of these two values:\n$$\n\\ln(\\exp(0.2) + \\exp(2.1)) \\approx \\ln(1.22140 + 8.16617) = \\ln(9.38757) \\approx 2.23934\n$$\nFinally, we combine the terms to get the loss $\\mathcal{L}$:\n$$\n\\mathcal{L} \\approx 1.76413 + 1.46431 - 2.23934 = 0.98910\n$$\nUsing higher precision for the intermediate calculations gives:\n$\\mathcal{L} = 1.7641293... + 1.4643101... - 2.2393351... = 0.9891042...$\n\nRounding the final answer to four significant figures gives $0.9891$.", "answer": "$$\\boxed{0.9891}$$", "id": "3171341"}]}