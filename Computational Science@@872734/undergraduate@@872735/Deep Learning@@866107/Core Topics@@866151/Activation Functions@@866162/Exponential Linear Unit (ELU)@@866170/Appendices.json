{"hands_on_practices": [{"introduction": "Understanding how to compute gradients is the cornerstone of training neural networks. This exercise will guide you through applying the multivariate chain rule to a small network that uses the Exponential Linear Unit (ELU). By propagating derivatives back from a loss function through two distinct computational branches, you will practice handling the non-differentiable point at the originâ€”a common feature in modern activation functions that requires careful mathematical treatment. [@problem_id:3190277]", "problem": "Consider a scalar input $x \\in \\mathbb{R}$ that feeds into an Exponential Linear Unit (ELU) activation $g(x)$ with parameter $\\alpha \\in (0, \\infty)$, defined by\n$$\ng(x) = \n\\begin{cases}\nx  \\text{if } x  0, \\\\\n\\alpha\\left(\\exp(x) - 1\\right)  \\text{if } x \\le 0.\n\\end{cases}\n$$\nThis activation fans out to a two-branch network:\n- Branch 1 computes $y_{1} = \\sigma\\!\\left(w_{1}\\,g(x) + b_{1}\\right)$, where $\\sigma(z)$ is the logistic sigmoid, $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$, and $w_{1}, b_{1} \\in \\mathbb{R}$ are fixed parameters.\n- Branch 2 computes $y_{2} = \\left(g(x)\\right)^{2}$.\n\nThe scalar loss is the half-sum of squared errors\n$$\nL = \\frac{1}{2}\\left(y_{1} - t_{1}\\right)^{2} + \\frac{1}{2}\\left(y_{2} - t_{2}\\right)^{2},\n$$\nwith fixed targets $t_{1}, t_{2} \\in \\mathbb{R}$.\n\nUsing only foundational definitions of derivatives and the multivariate chain rule, derive the backpropagation expressions for $\\frac{\\partial L}{\\partial x}$ for $x  0$ and $x  0$ by propagating derivatives through both branches. Then, evaluate $\\frac{\\partial L}{\\partial x}$ at the kink $x = 0$ using subgradients. For the nondifferentiable point $x = 0$, adopt the convention that backpropagation selects the right-hand derivative of the ELU (i.e., take $\\frac{d g}{d x}\\big|_{x=0}$ equal to the derivative from $x  0$). Provide your final expression for $\\frac{\\partial L}{\\partial x}$ at $x = 0$ in terms of $w_{1}$, $b_{1}$, $t_{1}$, and standard functions. No numerical rounding is required.", "solution": "The problem is valid. It is a well-posed problem in calculus and its application to neural networks, a standard topic in deep learning. The problem is scientifically grounded, self-contained, and objective. The provided convention for handling the non-differentiable point of the ELU activation function makes the problem unambiguous.\n\nThe objective is to find the derivative of the scalar loss $L$ with respect to the scalar input $x$, denoted as $\\frac{\\partial L}{\\partial x}$. The loss $L$ depends on $x$ through a series of intermediate computations involving the activation function $g(x)$. The overall dependency graph is:\n$x \\rightarrow g(x)$\n$g(x) \\rightarrow y_{1} \\rightarrow L$\n$g(x) \\rightarrow y_{2} \\rightarrow L$\n\nThe total derivative $\\frac{\\partial L}{\\partial x}$ can be found using the multivariate chain rule. Since the input $x$ influences the loss $L$ through a single intermediate variable $g(x)$, which then branches out, the chain rule can be structured as:\n$$\n\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial g(x)} \\frac{d g(x)}{d x}\n$$\nThe first term, $\\frac{\\partial L}{\\partial g(x)}$, accounts for the two branches originating from $g(x)$. It is the sum of the partial derivatives along each path:\n$$\n\\frac{\\partial L}{\\partial g(x)} = \\frac{\\partial L}{\\partial y_{1}} \\frac{\\partial y_{1}}{\\partial g(x)} + \\frac{\\partial L}{\\partial y_{2}} \\frac{\\partial y_{2}}{\\partial g(x)}\n$$\nWe will compute each component of this expression.\n\nFirst, we compute the derivatives of the loss $L$ with respect to its direct inputs $y_{1}$ and $y_{2}$.\nThe loss is $L = \\frac{1}{2}\\left(y_{1} - t_{1}\\right)^{2} + \\frac{1}{2}\\left(y_{2} - t_{2}\\right)^{2}$.\nThe partial derivatives are:\n$$\n\\frac{\\partial L}{\\partial y_{1}} = y_{1} - t_{1}\n$$\n$$\n\\frac{\\partial L}{\\partial y_{2}} = y_{2} - t_{2}\n$$\n\nNext, we compute the derivatives of $y_{1}$ and $y_{2}$ with respect to $g(x)$.\nFor Branch 1, $y_{1} = \\sigma\\!\\left(w_{1}\\,g(x) + b_{1}\\right)$. Let $z_{1} = w_{1}\\,g(x) + b_{1}$. The derivative of the logistic sigmoid function $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ is $\\frac{d\\sigma}{dz} = \\sigma(z)(1 - \\sigma(z))$.\nUsing the chain rule for $y_{1}$:\n$$\n\\frac{\\partial y_{1}}{\\partial g(x)} = \\frac{d\\sigma(z_{1})}{dz_{1}} \\frac{\\partial z_{1}}{\\partial g(x)} = \\sigma(z_{1})(1 - \\sigma(z_{1})) \\cdot w_{1} = y_{1}(1 - y_{1})w_{1}\n$$\nFor Branch 2, $y_{2} = \\left(g(x)\\right)^{2}$. The derivative is:\n$$\n\\frac{\\partial y_{2}}{\\partial g(x)} = 2 g(x)\n$$\n\nNow we combine these parts to find $\\frac{\\partial L}{\\partial g(x)}$:\n$$\n\\frac{\\partial L}{\\partial g(x)} = (y_{1} - t_{1}) \\cdot [y_{1}(1 - y_{1})w_{1}] + (y_{2} - t_{2}) \\cdot [2 g(x)]\n$$\n$$\n\\frac{\\partial L}{\\partial g(x)} = w_{1}(y_{1} - t_{1})y_{1}(1 - y_{1}) + 2(y_{2} - t_{2})g(x)\n$$\n\nThe final step is to determine the derivative of the ELU activation, $\\frac{d g(x)}{d x}$. This depends on the value of $x$.\nThe ELU function is defined as:\n$$\ng(x) = \n\\begin{cases}\nx  \\text{if } x  0, \\\\\n\\alpha\\left(\\exp(x) - 1\\right)  \\text{if } x \\le 0.\n\\end{cases}\n$$\n- For $x  0$: $g(x) = x$, so $\\frac{d g}{d x} = 1$.\n- For $x  0$: $g(x) = \\alpha(\\exp(x) - 1)$, so $\\frac{d g}{d x} = \\alpha \\exp(x)$.\n- For $x = 0$: The function is non-differentiable at this point (a \"kink\"), as the left-hand derivative is $\\alpha \\exp(0) = \\alpha$ and the right-hand derivative is $1$. The problem specifies a convention: \"adopt the convention that backpropagation selects the right-hand derivative\". Therefore, we must use:\n$$\n\\frac{d g}{d x}\\bigg|_{x=0} = 1\n$$\n\nWe are asked to find the final expression for $\\frac{\\partial L}{\\partial x}$ at $x = 0$. We evaluate all intermediate quantities at $x=0$:\nFirst, at $x=0$, the value of the activation function is:\n$$\ng(0) = \\alpha(\\exp(0) - 1) = \\alpha(1-1) = 0\n$$\nNext, we evaluate $y_{1}$ and $y_{2}$ at $x=0$:\n$$\ny_{1}\\big|_{x=0} = \\sigma(w_{1} g(0) + b_{1}) = \\sigma(w_{1}(0) + b_{1}) = \\sigma(b_{1})\n$$\n$$\ny_{2}\\big|_{x=0} = (g(0))^{2} = 0^{2} = 0\n$$\nNow we can evaluate the gradient term $\\frac{\\partial L}{\\partial g(x)}$ at $x=0$:\n$$\n\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0} = w_{1}(y_{1}\\big|_{x=0} - t_{1})y_{1}\\big|_{x=0}(1 - y_{1}\\big|_{x=0}) + 2(y_{2}\\big|_{x=0} - t_{2})g(0)\n$$\nSubstituting the values we just found:\n$$\n\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0} = w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1})) + 2(0 - t_{2})(0)\n$$\nThe second term vanishes:\n$$\n\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0} = w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1}))\n$$\nFinally, we compute $\\frac{\\partial L}{\\partial x}$ at $x=0$ using the main chain rule formula and the specified convention for $\\frac{d g}{d x}$:\n$$\n\\frac{\\partial L}{\\partial x}\\bigg|_{x=0} = \\left(\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0}\\right) \\left(\\frac{d g}{d x}\\bigg|_{x=0}\\right)\n$$\n$$\n\\frac{\\partial L}{\\partial x}\\bigg|_{x=0} = \\left( w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1})) \\right) \\cdot 1\n$$\n$$\n\\frac{\\partial L}{\\partial x}\\bigg|_{x=0} = w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1}))\n$$\nThis expression is in terms of the required parameters $w_{1}$, $b_{1}$, $t_{1}$, and the standard sigmoid function $\\sigma$, as requested. Note that $t_{2}$ and $\\alpha$ do not appear in the final expression for the derivative at $x=0$.", "answer": "$$\n\\boxed{w_{1}\\left(\\sigma(b_{1}) - t_{1}\\right)\\sigma(b_{1})\\left(1 - \\sigma(b_{1})\\right)}\n$$", "id": "3190277"}, {"introduction": "Beyond using fixed activation functions, we can treat their parameters, such as the $\\alpha$ in ELU, as learnable variables to be optimized during training. This hands-on problem challenges you to derive the gradient $\\frac{\\partial L}{\\partial \\alpha}$ and then implement a full verification cycle in code, including a numerical gradient check. This practice is essential for developing custom neural network components and also introduces the important practical concept of parameter identifiability, where different model parameters can lead to identical outputs. [@problem_id:3123807]", "problem": "You are asked to implement and analyze a single hidden layer using the Exponential Linear Unit (ELU), where the ELU parameter is a learnable scalar per layer. The analysis must be grounded in the fundamental definitions of the ELU function and the chain rule for derivatives.\n\nConsider a single hidden layer with pre-activations $z \\in \\mathbb{R}^n$, elapsed through an Exponential Linear Unit (ELU) activation $f(x;\\alpha)$ with a layer-specific learnable parameter $\\alpha \\in \\mathbb{R}$. The ELU function is defined piecewise by $f(x;\\alpha) = x$ for $x \\ge 0$ and $f(x;\\alpha) = \\alpha (\\exp(x) - 1)$ for $x  0$. The layer output is then passed to a one-dimensional linear output $y_{\\text{hat}} = w^\\top f(z;\\alpha) + b$ with weights $w \\in \\mathbb{R}^n$ and bias $b \\in \\mathbb{R}$. The loss is the Mean Squared Error (MSE) $L = \\frac{1}{2}(y_{\\text{hat}} - y)^2$ with target $y \\in \\mathbb{R}$.\n\nTasks:\n- Derive from first principles using the chain rule for derivatives a closed-form expression for the gradient $\\partial L / \\partial \\alpha$ that depends on $z$, $w$, $y_{\\text{hat}}$, $y$, and $\\alpha$. The derivation must start from the definitions of $f(x;\\alpha)$, $y_{\\text{hat}}$, and $L$, and apply the chain rule without using any shortcut formulas.\n- Implement a complete program that:\n  1. Computes the forward pass $y_{\\text{hat}}$, the loss $L$, and the analytical gradient $\\partial L / \\partial \\alpha$ for given inputs.\n  2. Validates the analytical gradient against a numerical gradient computed via central finite differences on $\\alpha$.\n  3. Examines identifiability of $\\alpha$ under layer scaling: discuss and test when scaling $\\alpha$ by a factor $s$ and simultaneously scaling the subsequent linear layer weights $w$ by $1/s$ yields the same output, and when it does not. Identifiability refers to whether different parameter settings produce indistinguishable model outputs for given inputs.\n  4. Covers boundary behavior at $x=0$ and the case of all-positive pre-activations.\n\nUse the following specific test suite:\n- Mixed-sign case for gradient check:\n  - $z_{\\text{mixed}} = [-1.2, 0.8, -0.3]$, $\\alpha = 1.3$, $w = [0.9, -1.1, 0.5]$, $b = 0.2$, $y = 0.7$, finite-difference step $\\varepsilon = 10^{-6}$.\n  - Report the relative error between the analytical and numerical gradients as a float computed as $\\frac{|\\text{analytical} - \\text{numerical}|}{\\max(10^{-12}, |\\text{analytical}| + |\\text{numerical}|)}$.\n- Boundary case at zero:\n  - $z_{0} = [0.0]$, $\\alpha = 1.0$, $w = [1.0]$, $b = 0.0$, $y = 0.0$.\n  - Report the analytical gradient $\\partial L / \\partial \\alpha$ as a float.\n- Identifiability with all-negative pre-activations:\n  - $z_{\\text{neg}} = [-1.0, -0.5]$, $\\alpha = 0.7$, $w = [1.2, -0.8]$, $b = 0.3$, scale factor $s = 2.5$.\n  - Report a boolean indicating whether $w^\\top f(z_{\\text{neg}};\\alpha) + b$ equals $(w/s)^\\top f(z_{\\text{neg}}; s\\alpha) + b$ within a tolerance of $10^{-12}$.\n- Identifiability fails for mixed signs:\n  - $z_{\\text{mix2}} = [-1.0, 0.5]$, use the same $\\alpha$, $w$, $b$, and $s$ as above.\n  - Report a boolean indicating whether $w^\\top f(z_{\\text{mix2}};\\alpha) + b$ equals $(w/s)^\\top f(z_{\\text{mix2}}; s\\alpha) + b$ within a tolerance of $10^{-12}$.\n- All-positive pre-activations:\n  - $z_{\\text{pos}} = [0.4, 0.2]$, $\\alpha = 1.7$, $w = [0.6, -0.3]$, $b = -0.1$, $y = 0.5$.\n  - Report the analytical gradient $\\partial L / \\partial \\alpha$ as a float.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4,result_5]$), in the exact order specified by the test suite above: mixed-case relative error (float), boundary-case gradient at zero (float), all-negative identifiability boolean, mixed-sign identifiability boolean, all-positive-case gradient (float). No other text should be printed.", "solution": "The problem is valid. It is scientifically grounded in the principles of neural network theory, mathematically well-posed, and objectively stated. All necessary data and definitions for a unique solution are provided.\n\n### 1. Derivation of the Gradient $\\partial L / \\partial \\alpha$\n\nThe primary task is to derive a closed-form expression for the gradient of the loss function $L$ with respect to the Exponential Linear Unit (ELU) parameter $\\alpha$. This derivation starts from the provided definitions and applies the chain rule of calculus.\n\nThe relevant quantities are:\n- The ELU activation function, defined for a scalar input $x$ and parameter $\\alpha$:\n$$\nf(x; \\alpha) = \\begin{cases}\nx  \\text{if } x \\ge 0 \\\\\n\\alpha (\\exp(x) - 1)  \\text{if } x  0\n\\end{cases}\n$$\n- The model's output prediction $y_{\\text{hat}}$, for a pre-activation vector $z \\in \\mathbb{R}^n$, weight vector $w \\in \\mathbb{R}^n$, and scalar bias $b$:\n$$\ny_{\\text{hat}} = w^\\top f(z; \\alpha) + b = \\sum_{i=1}^n w_i f(z_i; \\alpha) + b\n$$\nwhere $f(z; \\alpha)$ is applied element-wise to the vector $z$.\n- The Mean Squared Error (MSE) loss function, for a target value $y$:\n$$\nL = \\frac{1}{2}(y_{\\text{hat}} - y)^2\n$$\n\nWe seek to compute $\\frac{\\partial L}{\\partial \\alpha}$ by applying the chain rule:\n$$\n\\frac{\\partial L}{\\partial \\alpha} = \\frac{\\partial L}{\\partial y_{\\text{hat}}} \\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha}\n$$\n\nFirst, we compute the derivative of the loss $L$ with respect to the prediction $y_{\\text{hat}}$:\n$$\n\\frac{\\partial L}{\\partial y_{\\text{hat}}} = \\frac{\\partial}{\\partial y_{\\text{hat}}} \\left( \\frac{1}{2}(y_{\\text{hat}} - y)^2 \\right) = y_{\\text{hat}} - y\n$$\n\nNext, we compute the derivative of the prediction $y_{\\text{hat}}$ with respect to the parameter $\\alpha$.\n$$\n\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left( \\sum_{i=1}^n w_i f(z_i; \\alpha) + b \\right)\n$$\nSince differentiation is a linear operator, and $b$ is not a function of $\\alpha$:\n$$\n\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha} = \\sum_{i=1}^n w_i \\frac{\\partial f(z_i; \\alpha)}{\\partial \\alpha}\n$$\nNow, we must find the derivative of the ELU function $f(x; \\alpha)$ with respect to $\\alpha$. We consider the two cases from its definition:\n- If $x \\ge 0$, then $f(x; \\alpha) = x$. The derivative with respect to $\\alpha$ is:\n$$\n\\frac{\\partial f(x; \\alpha)}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha}(x) = 0\n$$\n- If $x  0$, then $f(x; \\alpha) = \\alpha (\\exp(x) - 1)$. The derivative with respect to $\\alpha$ is:\n$$\n\\frac{\\partial f(x; \\alpha)}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} (\\alpha (\\exp(x) - 1)) = \\exp(x) - 1\n$$\nThis can be expressed compactly using an indicator function, $I(c)$, which is $1$ if condition $c$ is true and $0$ otherwise:\n$$\n\\frac{\\partial f(x; \\alpha)}{\\partial \\alpha} = (\\exp(x) - 1) \\cdot I(x  0)\n$$\nNote that this derivative is well-defined and continuous at $x=0$, as $\\lim_{x\\to0^-}(\\exp(x)-1) = 0$, which matches the derivative for $x \\ge 0$.\n\nSubstituting this back into the expression for $\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha}$:\n$$\n\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha} = \\sum_{i=1}^n w_i (\\exp(z_i) - 1) \\cdot I(z_i  0)\n$$\nThis sum includes only the terms for which the pre-activation $z_i$ is negative.\n\nFinally, we combine the parts to obtain the full expression for $\\frac{\\partial L}{\\partial \\alpha}$:\n$$\n\\frac{\\partial L}{\\partial \\alpha} = (y_{\\text{hat}} - y) \\left( \\sum_{i=1}^n w_i (\\exp(z_i) - 1) \\cdot I(z_i  0) \\right)\n$$\nThis is the closed-form expression for the gradient, which depends only on the given variables $z, w, y_{\\text{hat}}, y$, and $\\alpha$.\n\n### 2. Analysis of Special Cases and Identifiability\n\n- **All-positive or zero pre-activations ($z_i \\ge 0$ for all $i$)**: In this case, $I(z_i  0) = 0$ for all $i$. The sum in the gradient expression becomes zero. Consequently, $\\frac{\\partial L}{\\partial \\alpha} = 0$. This is logical, as for non-negative inputs, the ELU function behaves as the identity function, $f(z_i; \\alpha) = z_i$, which is independent of $\\alpha$. Therefore, the model's output $y_{\\text{hat}}$ and the loss $L$ do not depend on $\\alpha$, and its gradient must be zero. This is tested in the boundary case ($z=[0.0]$) and the all-positive case.\n\n- **Identifiability under scaling**: We analyze whether scaling $\\alpha$ by a factor $s$ and $w$ by $1/s$ leaves the model output unchanged. Let the new parameters be $\\alpha' = s\\alpha$ and $w' = w/s$. The new output is $y'_{\\text{hat}} = (w')^\\top f(z; \\alpha') + b$.\n\n- **Case A: All-negative pre-activations ($z_i  0$ for all $i$)**:\nFor any component $i$, $f(z_i; \\alpha) = \\alpha(\\exp(z_i)-1)$. The original output is:\n$$y_{\\text{hat}} = \\sum_i w_i \\alpha (\\exp(z_i)-1) + b$$\nThe new output is:\n$$y'_{\\text{hat}} = \\sum_i w'_i f(z_i; \\alpha') + b = \\sum_i \\frac{w_i}{s} \\left( (s\\alpha)(\\exp(z_i)-1) \\right) + b = \\sum_i w_i \\alpha (\\exp(z_i)-1) + b$$\nHere, $y'_{\\text{hat}} = y_{\\text{hat}}$. The model is not identifiable, as different parameter sets $(\\alpha, w)$ and $(s\\alpha, w/s)$ produce the same output for any input $z$ with all-negative components.\n\n- **Case B: Mixed-sign pre-activations**:\nSuppose some $z_j \\ge 0$ and some $z_k  0$. The original output is:\n$$y_{\\text{hat}} = \\sum_{i \\mid z_i \\ge 0} w_i z_i + \\sum_{i \\mid z_i  0} w_i \\alpha(\\exp(z_i)-1) + b$$\nThe new output is:\n$$y'_{\\text{hat}} = \\sum_{i \\mid z_i \\ge 0} w'_i z_i + \\sum_{i \\mid z_i  0} w'_i \\alpha'(\\exp(z_i)-1) + b$$\n$$y'_{\\text{hat}} = \\sum_{i \\mid z_i \\ge 0} \\frac{w_i}{s} z_i + \\sum_{i \\mid z_i  0} \\frac{w_i}{s} (s\\alpha)(\\exp(z_i)-1) + b$$\n$$y'_{\\text{hat}} = \\sum_{i \\mid z_i \\ge 0} \\frac{w_i}{s} z_i + \\sum_{i \\mid z_i  0} w_i \\alpha(\\exp(z_i)-1) + b$$\nComparing $y_{\\text{hat}}$ and $y'_{\\text{hat}}$, the terms corresponding to negative $z_i$ are identical, but the terms for non-negative $z_i$ are scaled by $1/s$. Unless $s=1$ or all $w_i z_i=0$ for the non-negative part, $y'_{\\text{hat}} \\neq y_{\\text{hat}}$. In this case, the parameters are identifiable.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes results for all test cases as specified in the problem statement.\n    \"\"\"\n    \n    def elu(z, alpha):\n        \"\"\"Computes the element-wise ELU activation.\"\"\"\n        return np.where(z = 0, z, alpha * (np.exp(z) - 1))\n\n    def compute_forward_loss(z, alpha, w, b, y):\n        \"\"\"Computes the forward pass and loss.\"\"\"\n        activations = elu(z, alpha)\n        y_hat = np.dot(w, activations) + b\n        loss = 0.5 * (y_hat - y)**2\n        return y_hat, loss\n\n    def analytical_gradient(z, alpha, w, y_hat, y):\n        \"\"\"Computes the analytical gradient dL/d(alpha).\"\"\"\n        # Derivative of ELU with respect to alpha\n        d_elu_d_alpha = np.where(z = 0, 0.0, np.exp(z) - 1)\n        \n        # Derivative of y_hat with respect to alpha\n        d_yhat_d_alpha = np.dot(w, d_elu_d_alpha)\n        \n        # Derivative of Loss with respect to y_hat\n        d_L_d_yhat = y_hat - y\n        \n        # Final gradient using the chain rule\n        d_L_d_alpha = d_L_d_yhat * d_yhat_d_alpha\n        return d_L_d_alpha\n\n    def numerical_gradient(z, w, b, y, alpha, epsilon):\n        \"\"\"Computes the numerical gradient using central finite differences.\"\"\"\n        _, loss_plus = compute_forward_loss(z, alpha + epsilon, w, b, y)\n        _, loss_minus = compute_forward_loss(z, alpha - epsilon, w, b, y)\n        return (loss_plus - loss_minus) / (2 * epsilon)\n\n    results = []\n\n    # Test Case 1: Mixed-sign case for gradient check\n    z_mixed = np.array([-1.2, 0.8, -0.3])\n    alpha_mixed = 1.3\n    w_mixed = np.array([0.9, -1.1, 0.5])\n    b_mixed = 0.2\n    y_mixed = 0.7\n    epsilon = 1e-6\n    \n    y_hat_mixed, _ = compute_forward_loss(z_mixed, alpha_mixed, w_mixed, b_mixed, y_mixed)\n    grad_analyt = analytical_gradient(z_mixed, alpha_mixed, w_mixed, y_hat_mixed, y_mixed)\n    grad_numer = numerical_gradient(z_mixed, w_mixed, b_mixed, y_mixed, alpha_mixed, epsilon)\n    \n    rel_error_num = np.abs(grad_analyt - grad_numer)\n    rel_error_den = np.maximum(1e-12, np.abs(grad_analyt) + np.abs(grad_numer))\n    relative_error = rel_error_num / rel_error_den\n    results.append(relative_error)\n    \n    # Test Case 2: Boundary case at zero\n    z_0 = np.array([0.0])\n    alpha_0 = 1.0\n    w_0 = np.array([1.0])\n    b_0 = 0.0\n    y_0 = 0.0\n    \n    y_hat_0, _ = compute_forward_loss(z_0, alpha_0, w_0, b_0, y_0)\n    grad_at_zero = analytical_gradient(z_0, alpha_0, w_0, y_hat_0, y_0)\n    results.append(grad_at_zero)\n    \n    # Test Case 3: Identifiability with all-negative pre-activations\n    z_neg = np.array([-1.0, -0.5])\n    alpha_neg = 0.7\n    w_neg = np.array([1.2, -0.8])\n    b_neg = 0.3\n    s_neg = 2.5\n    \n    y_hat_orig_neg, _ = compute_forward_loss(z_neg, alpha_neg, w_neg, b_neg, 0.0)\n    \n    alpha_scaled_neg = s_neg * alpha_neg\n    w_scaled_neg = w_neg / s_neg\n    y_hat_scaled_neg, _ = compute_forward_loss(z_neg, alpha_scaled_neg, w_scaled_neg, b_neg, 0.0)\n    \n    is_identifiable_neg = np.isclose(y_hat_orig_neg, y_hat_scaled_neg, atol=1e-12)\n    results.append(bool(is_identifiable_neg))\n    \n    # Test Case 4: Identifiability fails for mixed signs\n    z_mix2 = np.array([-1.0, 0.5])\n    # The problem specifies to use the same alpha, w, b, s as in the previous case.\n    alpha_mix2 = alpha_neg #0.7\n    w_mix2 = w_neg #[1.2, -0.8]\n    b_mix2 = b_neg #0.3\n    s_mix2 = s_neg # 2.5\n    \n    y_hat_orig_mix2, _ = compute_forward_loss(z_mix2, alpha_mix2, w_mix2, b_mix2, 0.0)\n    \n    alpha_scaled_mix2 = s_mix2 * alpha_mix2\n    w_scaled_mix2 = w_mix2 / s_mix2\n    y_hat_scaled_mix2, _ = compute_forward_loss(z_mix2, alpha_scaled_mix2, w_scaled_mix2, b_mix2, 0.0)\n    \n    is_identifiable_mix2 = np.isclose(y_hat_orig_mix2, y_hat_scaled_mix2, atol=1e-12)\n    # The question is whether they are equal, so non-identifiability means the outputs are different.\n    # The problem asks for a boolean indicating whether the outputs are equal.\n    results.append(bool(is_identifiable_mix2))\n\n    # Test Case 5: All-positive pre-activations\n    z_pos = np.array([0.4, 0.2])\n    alpha_pos = 1.7\n    w_pos = np.array([0.6, -0.3])\n    b_pos = -0.1\n    y_pos = 0.5\n\n    y_hat_pos, _ = compute_forward_loss(z_pos, alpha_pos, w_pos, b_pos, y_pos)\n    grad_all_pos = analytical_gradient(z_pos, alpha_pos, w_pos, y_hat_pos, y_pos)\n    results.append(grad_all_pos)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3123807"}, {"introduction": "The choice of activation function can have subtle but significant effects on training dynamics and representational capacity. In this problem, we move beyond analyzing ELU in isolation and compare it to another popular function, Softplus. By deriving tight bounds on the difference between the two functions for negative inputs and considering their second derivatives, you will develop a more nuanced understanding of properties like curvature and smoothness, which are key to appreciating the design trade-offs among different activations. [@problem_id:3123738]", "problem": "Consider a feedforward layer that applies either the Exponential Linear Unit (ELU) activation or the Softplus activation to a pre-activation scalar input $x \\in \\mathbb{R}$. For a fixed parameter $\\alpha0$, the negative branch of ELU is defined by $f_{\\mathrm{ELU}}(x)=\\alpha(\\exp(x)-1)$ for $x0$, while the scaled Softplus is $f_{\\mathrm{SP}}(x)=\\alpha \\ln(1+\\exp(x))$ for all $x \\in \\mathbb{R}$. Starting from fundamental definitions of derivatives, the properties of the natural logarithm, and basic calculus, determine tight constants $C_{\\mathrm{lower}}$ and $C_{\\mathrm{upper}}$ such that for all $x0$,\n$$\nC_{\\mathrm{lower}} \\leq \\left|\\,\\alpha(\\exp(x)-1)-\\alpha \\ln(1+\\exp(x))\\,\\right| \\leq C_{\\mathrm{upper}}.\n$$\nYou must express $C_{\\mathrm{lower}}$ and $C_{\\mathrm{upper}}$ in closed form as functions of $\\alpha$ only (that is, independent of $x$). Report your final answer as the pair $\\big(C_{\\mathrm{lower}},\\,C_{\\mathrm{upper}}\\big)$. No numerical approximation is required.\n\nAfter finding these constants, briefly discuss, using first principles and without invoking any specialized theorems beyond introductory calculus, what the bound implies about the comparative second-derivative smoothness of the two activations near $x=0$ and for large negative $x$. Your discussion will not be graded for the final numeric answer but will be evaluated for reasoning quality.", "solution": "The problem as stated is a well-posed mathematical question grounded in the established definitions of the ELU and Softplus activation functions. It is self-contained, objective, and free from any scientific, logical, or factual flaws. Essential data and conditions are provided, and the task is to perform a standard calculus-based analysis to find the bounds of a function. Therefore, the problem is deemed valid.\n\nWe are tasked with finding tight constants $C_{\\mathrm{lower}}$ and $C_{\\mathrm{upper}}$ such that for all $x0$, the following inequality holds:\n$$\nC_{\\mathrm{lower}} \\leq \\left|\\,\\alpha(\\exp(x)-1)-\\alpha \\ln(1+\\exp(x))\\,\\right| \\leq C_{\\mathrm{upper}}\n$$\nLet us define the function $F(x)$ as the expression inside the absolute value, scaled by $\\frac{1}{\\alpha}$:\n$$\nF(x) = (\\exp(x)-1) - \\ln(1+\\exp(x))\n$$\nWe need to find the bounds on $|\\alpha F(x)|$ for $x \\in (-\\infty, 0)$. First, we determine the sign of $F(x)$. Let $u = \\exp(x)$. Since $x  0$, we have $u \\in (0, 1)$. The function becomes $G(u) = u-1-\\ln(1+u)$ for $u \\in (0,1)$.\n\nTo analyze the sign of $G(u)$, we examine its derivative with respect to $u$:\n$$\nG'(u) = \\frac{d}{du}(u-1-\\ln(1+u)) = 1 - \\frac{1}{1+u} = \\frac{1+u-1}{1+u} = \\frac{u}{1+u}\n$$\nFor $u \\in (0, 1)$, both $u$ and $1+u$ are positive, so $G'(u)  0$. This indicates that $G(u)$ is a strictly increasing function on the interval $(0, 1)$. We can evaluate the function at the boundaries of its domain:\n$$\n\\lim_{u \\to 0^+} G(u) = 0 - 1 - \\ln(1+0) = -1\n$$\n$$\n\\lim_{u \\to 1^-} G(u) = 1 - 1 - \\ln(1+1) = -\\ln(2)\n$$\nSince $G(u)$ is strictly increasing on $(0, 1)$, its values are bounded by these limits, i.e., $G(u) \\in (-1, -\\ln(2))$. This means $G(u)$ is always negative. Consequently, $F(x) = G(\\exp(x))$ is always negative for $x \\in (-\\infty, 0)$.\n\nGiven that $\\alpha  0$ and $F(x)  0$, the expression inside the absolute value is always negative. We can therefore rewrite the inequality by removing the absolute value and reversing the sign of the inner expression:\n$$\n|\\alpha F(x)| = -\\alpha F(x) = \\alpha [ \\ln(1+\\exp(x)) - (\\exp(x)-1) ]\n$$\nLet us define a new function $H(x) = \\alpha [ \\ln(1+\\exp(x)) - \\exp(x) + 1 ]$ for $x \\in (-\\infty, 0)$. We seek the infimum and supremum of $H(x)$, which will correspond to $C_{\\mathrm{lower}}$ and $C_{\\mathrm{upper}}$, respectively.\n\nTo find the extrema of $H(x)$, we compute its first derivative:\n$$\nH'(x) = \\frac{d}{dx} \\left( \\alpha [ \\ln(1+\\exp(x)) - \\exp(x) + 1 ] \\right)\n$$\n$$\nH'(x) = \\alpha \\left[ \\frac{1}{1+\\exp(x)} \\cdot \\exp(x) - \\exp(x) \\right]\n$$\n$$\nH'(x) = \\alpha \\exp(x) \\left[ \\frac{1}{1+\\exp(x)} - 1 \\right] = \\alpha \\exp(x) \\left[ \\frac{1 - (1+\\exp(x))}{1+\\exp(x)} \\right]\n$$\n$$\nH'(x) = -\\frac{\\alpha \\exp(2x)}{1+\\exp(x)}\n$$\nFor all $x \\in \\mathbb{R}$, $\\exp(x)  0$. Given $\\alpha  0$, every term in the expression for $H'(x)$ is positive, except for the leading negative sign. Thus, $H'(x)  0$ for all $x \\in (-\\infty, 0)$. This shows that $H(x)$ is a strictly decreasing function on its domain.\n\nSince $H(x)$ is strictly decreasing on the open interval $(-\\infty, 0)$, its supremum (the tightest upper bound $C_{\\mathrm{upper}}$) is the limit as $x$ approaches the left boundary of the interval, and its infimum (the tightest lower bound $C_{\\mathrm{lower}}$) is the limit as $x$ approaches the right boundary.\n\nLet's calculate the limit for the upper bound:\n$$\nC_{\\mathrm{upper}} = \\lim_{x \\to -\\infty} H(x) = \\lim_{x \\to -\\infty} \\alpha [ \\ln(1+\\exp(x)) - \\exp(x) + 1 ]\n$$\nAs $x \\to -\\infty$, $\\exp(x) \\to 0$. Substituting this into the expression:\n$$\nC_{\\mathrm{upper}} = \\alpha [ \\ln(1+0) - 0 + 1 ] = \\alpha [ \\ln(1) + 1 ] = \\alpha(0+1) = \\alpha\n$$\nNow, let's calculate the limit for the lower bound:\n$$\nC_{\\mathrm{lower}} = \\lim_{x \\to 0^-} H(x) = \\lim_{x \\to 0^-} \\alpha [ \\ln(1+\\exp(x)) - \\exp(x) + 1 ]\n$$\nAs $x \\to 0^-$, $\\exp(x) \\to \\exp(0) = 1$. Substituting this into the expression:\n$$\nC_{\\mathrm{lower}} = \\alpha [ \\ln(1+1) - 1 + 1 ] = \\alpha [ \\ln(2) ] = \\alpha \\ln(2)\n$$\nThus, for all $x0$, we have $\\alpha\\ln(2)  H(x)  \\alpha$. The tightest constants are the infimum and supremum of the function $H(x)$ on the interval $(-\\infty, 0)$, which are $C_{\\mathrm{lower}} = \\alpha \\ln(2)$ and $C_{\\mathrm{upper}} = \\alpha$.\n\nRegarding the comparative second-derivative smoothness:\nThe analysis above directly relates to the derivatives of the two functions, $f_{\\mathrm{ELU}}(x)=\\alpha(\\exp(x)-1)$ and $f_{\\mathrm{SP}}(x)=\\alpha \\ln(1+\\exp(x))$. The difference we analyzed, $H(x)$, can be written as $H(x) = f_{\\mathrm{SP}}(x) - f_{\\mathrm{ELU}}(x) + \\alpha$. The first and second derivatives of $H(x)$ are therefore the differences of the derivatives of $f_{\\mathrm{SP}}$ and $f_{\\mathrm{ELU}}$:\n$H'(x) = f'_{\\mathrm{SP}}(x) - f'_{\\mathrm{ELU}}(x)$\n$H''(x) = f''_{\\mathrm{SP}}(x) - f''_{\\mathrm{ELU}}(x)$\n\nLet's compute the second derivatives of each function for $x0$:\n$f''_{\\mathrm{ELU}}(x) = \\frac{d^2}{dx^2}[\\alpha(\\exp(x)-1)] = \\alpha \\exp(x)$\n$f'_{\\mathrm{SP}}(x) = \\alpha \\frac{\\exp(x)}{1+\\exp(x)}$\n$f''_{\\mathrm{SP}}(x) = \\alpha \\frac{\\exp(x)(1+\\exp(x)) - \\exp(x)\\exp(x)}{(1+\\exp(x))^2} = \\alpha \\frac{\\exp(x)}{(1+\\exp(x))^2}$\n\nNear $x=0$ (as $x \\to 0^-$):\n$\\lim_{x \\to 0^-} f''_{\\mathrm{ELU}}(x) = \\alpha \\exp(0) = \\alpha$\n$\\lim_{x \\to 0^-} f''_{\\mathrm{SP}}(x) = \\alpha \\frac{\\exp(0)}{(1+\\exp(0))^2} = \\frac{\\alpha}{4}$\nSince $\\alpha  \\frac{\\alpha}{4}$, the curvature of ELU is four times that of Softplus as $x$ approaches $0$ from the negative side. This implies ELU is less \"smooth\" (changes curvature more sharply) at the origin. Furthermore, for $x0$, $f_{\\mathrm{ELU}}(x)=x$ (by standard ELU definition, not given but assumed for context), so $f''_{\\mathrm{ELU}}(x)=0$ for $x0$. This means $f''_{\\mathrm{ELU}}$ is discontinuous at $x=0$, jumping from $\\alpha$ to $0$, whereas $f''_{\\mathrm{SP}}$ is continuous.\n\nFor large negative $x$ (as $x \\to -\\infty$):\n$\\lim_{x \\to -\\infty} f''_{\\mathrm{ELU}}(x) = \\lim_{x \\to -\\infty} \\alpha \\exp(x) = 0$\n$\\lim_{x \\to -\\infty} f''_{\\mathrm{SP}}(x) = \\lim_{x \\to -\\infty} \\alpha \\frac{\\exp(x)}{(1+\\exp(x))^2} = 0$\nBoth second derivatives approach $0$ as $x \\to -\\infty$, meaning both functions become extremely flat (asymptotic). The analysis of the difference bound is informative here. The extremum $C_{\\mathrm{upper}}=\\alpha$ is reached as $x \\to -\\infty$. This represents the difference in the horizontal asymptotes of the two functions: $f_{\\mathrm{ELU}} \\to -\\alpha$ and $f_{\\mathrm{SP}} \\to 0$. While the function values differ by $\\alpha$, their derivatives and second derivatives become indistinguishable. The rate at which the difference in second derivatives, $H''(x)$, approaches $0$ is proportional to $\\exp(2x)$, which is faster than the rate at which the individual second derivatives approach $0$ (proportional to $\\exp(x)$). This confirms that for large negative inputs, the smoothness profiles of the two functions converge rapidly.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\alpha\\ln(2)  \\alpha\n\\end{pmatrix}\n}\n$$", "id": "3123738"}]}