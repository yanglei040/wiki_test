{"hands_on_practices": [{"introduction": "Understanding an activation function's behavior near the origin is critical for analyzing gradient flow and model initialization. Unlike the sharp corner of the ReLU function, GELU offers a smooth transition. This exercise delves into the heart of GELU's smoothness by asking you to derive its Taylor series expansion, which acts as a polynomial approximation for small inputs [@problem_id:3128551]. By performing this analysis, you will gain a precise, quantitative understanding of how GELU deviates from a simple linear function, revealing the subtle curvature that helps stabilize training.", "problem": "A single-hidden-layer neural network uses the Gaussian Error Linear Unit (GELU) activation near the origin, where inputs are small. To study local gradient flow, approximate the activation by its Taylor series around $x=0$ up to and including degree $x^{5}$.\n\nUse the following foundational base:\n\n- The standard normal probability density function (PDF) is $\\phi(x)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{x^{2}}{2}\\right)$.\n- The standard normal cumulative distribution function (CDF) is $\\Phi(x)=\\int_{-\\infty}^{x}\\phi(t)\\,\\mathrm{d}t$.\n- The Gaussian Error Linear Unit (GELU) activation is defined by $\\mathrm{GELU}(x)=x\\,\\Phi(x)$.\n- The Maclaurin series for a function $f$ is $f(x)=\\sum_{k=0}^{\\infty}\\frac{f^{(k)}(0)}{k!}x^{k}$ when all derivatives exist.\n- You may start from the well-tested local approximation of the standard normal CDF near the origin,\n$$\n\\Phi(x)\\approx \\frac{1}{2}+\\frac{1}{\\sqrt{2\\pi}}x-\\frac{1}{6\\sqrt{2\\pi}}x^{3}+\\cdots,\n$$\nand use Taylor series machinery and the relationship between $\\Phi$ and $\\phi$ to determine any additional coefficients needed to capture terms up to $x^{5}$ in $\\mathrm{GELU}(x)$.\n\nDerive the Taylor expansion of $\\mathrm{GELU}(x)$ about $x=0$ up to and including terms of degree $x^{5}$. Present your final answer as a single closed-form analytic expression in $x$ with exact coefficients. No rounding is required.", "solution": "The user wants to find the Taylor series expansion of the Gaussian Error Linear Unit (GELU) function around the origin, $x=0$, up to and including the term of degree $x^5$.\n\nFirst, we validate the problem statement.\n\n### Step 1: Extract Givens\n-   The standard normal probability density function (PDF) is $\\phi(x)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{x^{2}}{2}\\right)$.\n-   The standard normal cumulative distribution function (CDF) is $\\Phi(x)=\\int_{-\\infty}^{x}\\phi(t)\\,\\mathrm{d}t$.\n-   The Gaussian Error Linear Unit (GELU) activation is defined by $\\mathrm{GELU}(x)=x\\,\\Phi(x)$.\n-   The Maclaurin series for a function $f$ is $f(x)=\\sum_{k=0}^{\\infty}\\frac{f^{(k)}(0)}{k!}x^{k}$.\n-   An approximation for the CDF is provided: $\\Phi(x)\\approx \\frac{1}{2}+\\frac{1}{\\sqrt{2\\pi}}x-\\frac{1}{6\\sqrt{2\\pi}}x^{3}+\\cdots$.\n-   The task is to derive the Taylor expansion of $\\mathrm{GELU}(x)$ about $x=0$ up to and including terms of degree $x^{5}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. The definitions of $\\phi(x)$, $\\Phi(x)$, and $\\mathrm{GELU}(x)$ are standard in statistics and deep learning. The request to find a Taylor series expansion is a well-posed mathematical problem. The function $\\mathrm{GELU}(x)$ is infinitely differentiable, ensuring the existence of its Maclaurin series. The problem is self-contained and objective. The provided approximation for $\\Phi(x)$ is correct but incomplete for the task, and the problem statement itself acknowledges this by instructing the solver to determine additional coefficients, which is a part of the problem's challenge rather than a flaw.\n\n### Step 3: Verdict and Action\nThe problem is valid. We will proceed with the solution.\n\nThe Taylor expansion of $\\mathrm{GELU}(x)$ about $x=0$ is given by the Maclaurin series:\n$$ \\mathrm{GELU}(x) = \\sum_{k=0}^{\\infty} \\frac{\\mathrm{GELU}^{(k)}(0)}{k!} x^k $$\nWe need to find the terms up to $x^5$, which requires computing the first five derivatives of $\\mathrm{GELU}(x)$ at $x=0$.\n\nAn alternative, and more direct, method is to first find the Taylor series for $\\Phi(x)$ and then multiply it by $x$. Let the Taylor series for $\\Phi(x)$ be\n$$ \\Phi(x) = c_0 + c_1 x + c_2 x^2 + c_3 x^3 + c_4 x^4 + O(x^5) $$\nThen the series for $\\mathrm{GELU}(x)$ will be\n$$ \\mathrm{GELU}(x) = x \\Phi(x) = c_0 x + c_1 x^2 + c_2 x^3 + c_3 x^4 + c_4 x^5 + O(x^6) $$\nTo find the expansion of $\\mathrm{GELU}(x)$ up to degree $5$, we need the expansion of $\\Phi(x)$ up to degree $4$. The coefficients $c_k$ are given by $c_k = \\frac{\\Phi^{(k)}(0)}{k!}$.\n\nWe use the fundamental theorem of calculus: $\\Phi'(x) = \\frac{d}{dx}\\int_{-\\infty}^{x}\\phi(t)\\,\\mathrm{d}t = \\phi(x)$.\nThis implies that for $k \\ge 1$, $\\Phi^{(k)}(x) = \\phi^{(k-1)}(x)$.\n\nThe coefficients for $\\Phi(x)$'s series are:\n$c_0 = \\Phi(0) = \\int_{-\\infty}^{0} \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{t^2}{2}) dt = \\frac{1}{2}$, due to the symmetry of the standard normal distribution.\n$c_1 = \\frac{\\Phi'(0)}{1!} = \\phi(0)$.\n$c_2 = \\frac{\\Phi''(0)}{2!} = \\frac{\\phi'(0)}{2}$.\n$c_3 = \\frac{\\Phi'''(0)}{3!} = \\frac{\\phi''(0)}{6}$.\n$c_4 = \\frac{\\Phi^{(4)}(0)}{4!} = \\frac{\\phi'''(0)}{24}$.\n\nNow we compute the derivatives of $\\phi(x)$ at $x=0$:\n$\\phi(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{x^2}{2})$.\n1.  $\\phi(0) = \\frac{1}{\\sqrt{2\\pi}}\\exp(0) = \\frac{1}{\\sqrt{2\\pi}}$.\n2.  $\\phi'(x) = \\frac{d}{dx}\\left[\\phi(x)\\right] = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{x^2}{2})(-x) = -x\\phi(x)$.\n    At $x=0$, $\\phi'(0) = -0 \\cdot \\phi(0) = 0$.\n3.  $\\phi''(x) = \\frac{d}{dx}\\left[-x\\phi(x)\\right] = -1 \\cdot \\phi(x) - x \\cdot \\phi'(x) = -\\phi(x) - x(-x\\phi(x)) = (x^2-1)\\phi(x)$.\n    At $x=0$, $\\phi''(0) = (0^2-1)\\phi(0) = -\\phi(0) = -\\frac{1}{\\sqrt{2\\pi}}$.\n4.  $\\phi'''(x) = \\frac{d}{dx}\\left[(x^2-1)\\phi(x)\\right] = 2x\\phi(x) + (x^2-1)\\phi'(x) = 2x\\phi(x) - x(x^2-1)\\phi(x) = (2x - x^3 + x)\\phi(x) = (3x-x^3)\\phi(x)$.\n    At $x=0$, $\\phi'''(0) = (0-0)\\phi(0) = 0$.\n\nNow we can calculate the coefficients $c_k$ for $\\Phi(x)$:\n$c_0 = \\frac{1}{2}$.\n$c_1 = \\phi(0) = \\frac{1}{\\sqrt{2\\pi}}$.\n$c_2 = \\frac{\\phi'(0)}{2} = \\frac{0}{2} = 0$.\n$c_3 = \\frac{\\phi''(0)}{6} = \\frac{-1/\\sqrt{2\\pi}}{6} = -\\frac{1}{6\\sqrt{2\\pi}}$.\n$c_4 = \\frac{\\phi'''(0)}{24} = \\frac{0}{24} = 0$.\n\nThe Taylor expansion for $\\Phi(x)$ up to degree $4$ is:\n$$ \\Phi(x) \\approx \\frac{1}{2} + \\frac{1}{\\sqrt{2\\pi}}x + 0 \\cdot x^2 - \\frac{1}{6\\sqrt{2\\pi}}x^3 + 0 \\cdot x^4 $$\n$$ \\Phi(x) \\approx \\frac{1}{2} + \\frac{1}{\\sqrt{2\\pi}}x - \\frac{1}{6\\sqrt{2\\pi}}x^3 $$\nThis demonstrates that the given approximation correctly identifies the non-zero terms but omits the zero-coefficient terms.\n\nTo find the Taylor expansion for $\\mathrm{GELU}(x)$ up to degree $5$, we multiply the expansion of $\\Phi(x)$ up to degree $4$ by $x$:\n$$ \\mathrm{GELU}(x) = x \\Phi(x) \\approx x \\left( \\frac{1}{2} + \\frac{1}{\\sqrt{2\\pi}}x - \\frac{1}{6\\sqrt{2\\pi}}x^3 \\right) $$\n$$ \\mathrm{GELU}(x) \\approx \\frac{1}{2}x + \\frac{1}{\\sqrt{2\\pi}}x^2 - \\frac{1}{6\\sqrt{2\\pi}}x^4 $$\nThe Taylor polynomial of degree $5$ is the polynomial that includes all terms up to and including the power $x^{5}$. The coefficient of the $x^3$ term is $c_2 = 0$, and the coefficient of the $x^5$ term is $c_4 = 0$. Thus, the requested expansion is:\n$$ \\frac{1}{2}x + \\frac{1}{\\sqrt{2\\pi}}x^2 + 0 \\cdot x^3 - \\frac{1}{6\\sqrt{2\\pi}}x^4 + 0 \\cdot x^5 $$\nWhen writing the final expression, we omit terms with zero coefficients.\n\nThe final Taylor expansion of $\\mathrm{GELU}(x)$ about $x=0$ up to and including the $x^5$ term is:\n$$ \\frac{1}{2}x + \\frac{1}{\\sqrt{2\\pi}}x^2 - \\frac{1}{6\\sqrt{2\\pi}}x^4 $$", "answer": "$$\n\\boxed{\\frac{1}{2}x + \\frac{1}{\\sqrt{2\\pi}}x^2 - \\frac{1}{6\\sqrt{2\\pi}}x^4}\n$$", "id": "3128551"}, {"introduction": "In deep learning, theoretical elegance often competes with computational efficiency. This practice puts you in the role of a model architect weighing these trade-offs by comparing GELU to a popular, faster alternative: the HardSwish function [@problem_id:3128657]. You will analyze their derivatives and \"smoothness penalties\" to quantify the differences, discovering why a perfectly smooth function might be approximated by a simpler, piecewise one. This exercise highlights the practical engineering decisions that balance model performance with computational cost.", "problem": "Consider two activation functions used in deep neural networks. The first is the Gaussian Error Linear Unit (GELU), defined operationally as follows: for an input $x \\in \\mathbb{R}$, draw $Z \\sim \\mathcal{N}(0,1)$, and output the expected gated input $\\mathbb{E}_{Z}\\big[x \\cdot \\mathbf{1}\\{Z \\le x\\}\\big]$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The second is the hard-saturating swish, $\\operatorname{HardSwish}(x)=x \\cdot \\max\\big(0,\\min\\big(1,\\frac{x+3}{6}\\big)\\big)$, which equals $x \\cdot \\frac{x+3}{6}$ for $x \\in [-3,3]$, equals $0$ for $x \\le -3$, and equals $x$ for $x \\ge 3$. In optimization, two local properties are often contrasted: the slope at $x=0$ and a smoothness penalty that quantifies curvature and kinks.\n\nAdopt the following penalty functionals on the interval $[-3,3]$:\n- The curvature-only penalty $P_2(f) \\equiv \\int_{-3}^{3} \\big(f''(x)\\big)^2 \\, dx$.\n- The curvature-plus-kink penalty $P_{2+\\kappa}(f) \\equiv P_2(f) + \\sum_{x_i \\in \\{-3,3\\}} \\big(\\Delta f'(x_i)\\big)^2$, where $\\Delta f'(x_i) \\equiv \\lim_{h \\downarrow 0} \\big(f'(x_i+h) - f'(x_i-h)\\big)$ is the jump in the first derivative at $x_i$.\n\nUsing only the base definitions of derivatives, the standard normal Probability Density Function (PDF) and Cumulative Distribution Function (CDF), and the piecewise structure of $\\operatorname{HardSwish}$ stated above, determine which of the following statements are correct:\n\nA. The slopes at $x=0$ match: $f'_{\\mathrm{GELU}}(0) = f'_{\\mathrm{HS}}(0)$.\n\nB. Under the curvature-only penalty $P_2$, $\\operatorname{HardSwish}$ has a smaller penalty on $[-3,3]$ than $\\mathrm{GELU}$.\n\nC. Under the curvature-plus-kink penalty $P_{2+\\kappa}$, $\\operatorname{HardSwish}$ has a larger penalty than $\\mathrm{GELU}$.\n\nD. Matching slopes at $x=0$ implies that the second derivatives at $x=0$ are equal.", "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   **Gaussian Error Linear Unit (GELU)**: For an input $x \\in \\mathbb{R}$, draw $Z \\sim \\mathcal{N}(0,1)$. The output is $\\mathbb{E}_{Z}\\big[x \\cdot \\mathbf{1}\\{Z \\le x\\}\\big]$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n-   **HardSwish**: $\\operatorname{HardSwish}(x)=x \\cdot \\max\\big(0,\\min\\big(1,\\frac{x+3}{6}\\big)\\big)$.\n-   **Piecewise structure of HardSwish**:\n    -   Equals $x \\cdot \\frac{x+3}{6}$ for $x \\in [-3,3]$.\n    -   Equals $0$ for $x \\le -3$.\n    -   Equals $x$ for $x \\ge 3$.\n-   **Curvature-only penalty**: $P_2(f) \\equiv \\int_{-3}^{3} \\big(f''(x)\\big)^2 \\, dx$.\n-   **Curvature-plus-kink penalty**: $P_{2+\\kappa}(f) \\equiv P_2(f) + \\sum_{x_i \\in \\{-3,3\\}} \\big(\\Delta f'(x_i)\\big)^2$.\n-   **Jump in first derivative**: $\\Delta f'(x_i) \\equiv \\lim_{h \\downarrow 0} \\big(f'(x_i+h) - f'(x_i-h)\\big)$.\n-   **Assumptions**: Use only base definitions of derivatives, standard normal PDF ($\\phi$) and CDF ($\\Phi$), and the given piecewise structure of HardSwish.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Grounding**: The problem involves two authentic activation functions, GELU and HardSwish, used in deep learning. Their definitions are standard. The penalty functionals are well-defined mathematical objects used to quantify function smoothness, a concept rooted in functional analysis. The problem is scientifically and mathematically sound.\n2.  **Well-Posedness**: The functions and penalties are explicitly defined. The questions are quantitative comparisons that admit unique answers derivable from the givens. The problem is well-posed.\n3.  **Objectivity**: The language is precise and mathematical, with no subjective or ambiguous terms.\n4.  **Consistency**: The piecewise definition of HardSwish is consistent with its closed-form expression. For $x \\le -3$, $\\frac{x+3}{6} \\le 0$, so $\\max(0, \\min(1, \\frac{x+3}{6})) = 0$, giving $x \\cdot 0 = 0$. For $x \\ge 3$, $\\frac{x+3}{6} \\ge 1$, so $\\max(0, \\min(1, \\frac{x+3}{6})) = 1$, giving $x \\cdot 1 = x$. For $x \\in [-3,3]$, $0 \\le \\frac{x+3}{6} \\le 1$, so $\\max(0, \\min(1, \\frac{x+3}{6})) = \\frac{x+3}{6}$, giving $x \\cdot \\frac{x+3}{6}$. The definitions are consistent.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. A full solution will be derived.\n\n### Solution Derivation\n\nLet $f_{\\mathrm{GELU}}(x)$ and $f_{\\mathrm{HS}}(x)$ denote the GELU and HardSwish functions, respectively.\n\n**Function Analysis: GELU**\nThe definition of GELU is $f_{\\mathrm{GELU}}(x) = \\mathbb{E}_{Z}\\big[x \\cdot \\mathbf{1}\\{Z \\le x\\}\\big]$. Since $x$ is a constant with respect to the expectation over $Z$, we have:\n$$f_{\\mathrm{GELU}}(x) = x \\cdot \\mathbb{E}_{Z}\\big[\\mathbf{1}\\{Z \\le x\\}\\big] = x \\cdot P(Z \\le x)$$\nLet $\\Phi(x)$ be the CDF of the standard normal distribution $\\mathcal{N}(0,1)$ and $\\phi(x)$ be its PDF.\n$$f_{\\mathrm{GELU}}(x) = x\\Phi(x)$$\nThe first derivative, using the product rule, is:\n$$f'_{\\mathrm{GELU}}(x) = \\frac{d}{dx}(x\\Phi(x)) = 1 \\cdot \\Phi(x) + x \\cdot \\Phi'(x) = \\Phi(x) + x\\phi(x)$$\nThe second derivative is:\n$$f''_{\\mathrm{GELU}}(x) = \\frac{d}{dx}(\\Phi(x) + x\\phi(x)) = \\phi(x) + (1 \\cdot \\phi(x) + x\\phi'(x)) = 2\\phi(x) + x\\phi'(x)$$\nThe PDF is $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}$, so its derivative is $\\phi'(x) = -x \\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2} = -x\\phi(x)$.\nSubstituting this into the second derivative expression:\n$$f''_{\\mathrm{GELU}}(x) = 2\\phi(x) - x^2\\phi(x) = (2-x^2)\\phi(x)$$\n\n**Function Analysis: HardSwish**\nOn the interval $x \\in [-3,3]$, the function is given as:\n$$f_{\\mathrm{HS}}(x) = x \\cdot \\frac{x+3}{6} = \\frac{1}{6}(x^2 + 3x)$$\nThe first derivative on $x \\in (-3,3)$ is:\n$$f'_{\\mathrm{HS}}(x) = \\frac{1}{6}(2x+3) = \\frac{1}{3}x + \\frac{1}{2}$$\nThe second derivative on $x \\in (-3,3)$ is:\n$$f''_{\\mathrm{HS}}(x) = \\frac{1}{3}$$\nFor $x  -3$, $f_{\\mathrm{HS}}(x) = 0$, so $f'_{\\mathrm{HS}}(x) = 0$.\nFor $x  3$, $f_{\\mathrm{HS}}(x) = x$, so $f'_{\\mathrm{HS}}(x) = 1$.\n\n### Option-by-Option Analysis\n\n**A. The slopes at $x=0$ match: $f'_{\\mathrm{GELU}}(0) = f'_{\\mathrm{HS}}(0)$.**\n\nFor GELU at $x=0$:\n$$f'_{\\mathrm{GELU}}(0) = \\Phi(0) + 0 \\cdot \\phi(0) = \\Phi(0)$$\nThe median of the standard normal distribution is $0$, so $\\Phi(0) = 1/2$.\n$$f'_{\\mathrm{GELU}}(0) = \\frac{1}{2}$$\nFor HardSwish at $x=0$ (which is in $[-3,3]$):\n$$f'_{\\mathrm{HS}}(0) = \\frac{1}{3}(0) + \\frac{1}{2} = \\frac{1}{2}$$\nThe slopes at $x=0$ are indeed equal.\n- **Verdict for A**: **Correct**\n\n**B. Under the curvature-only penalty $P_2$, $\\operatorname{HardSwish}$ has a smaller penalty on $[-3,3]$ than $\\mathrm{GELU}$.**\n\nFirst, calculate the penalty for HardSwish:\n$$P_2(f_{\\mathrm{HS}}) = \\int_{-3}^{3} \\big(f''_{\\mathrm{HS}}(x)\\big)^2 dx = \\int_{-3}^{3} \\left(\\frac{1}{3}\\right)^2 dx = \\int_{-3}^{3} \\frac{1}{9} dx = \\frac{1}{9}[x]_{-3}^{3} = \\frac{1}{9}(3 - (-3)) = \\frac{6}{9} = \\frac{2}{3}$$\nNext, set up the penalty for GELU:\n$$P_2(f_{\\mathrm{GELU}}) = \\int_{-3}^{3} \\big((2-x^2)\\phi(x)\\big)^2 dx = \\int_{-3}^{3} (2-x^2)^2 \\left(\\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}\\right)^2 dx$$\n$$P_2(f_{\\mathrm{GELU}}) = \\frac{1}{2\\pi} \\int_{-3}^{3} (4 - 4x^2 + x^4) e^{-x^2} dx$$\nThe interval $[-3,3]$ covers the vast majority of the mass of the function $e^{-x^2}$. We can approximate the integral by extending the limits to $\\pm\\infty$.\nUsing the standard Gaussian integrals $\\int_{-\\infty}^{\\infty} e^{-ax^2} dx = \\sqrt{\\frac{\\pi}{a}}$, $\\int_{-\\infty}^{\\infty} x^2 e^{-ax^2} dx = \\frac{1}{2a}\\sqrt{\\frac{\\pi}{a}}$, and $\\int_{-\\infty}^{\\infty} x^4 e^{-ax^2} dx = \\frac{3}{4a^2}\\sqrt{\\frac{\\pi}{a}}$, with $a=1$:\n$$\\int_{-\\infty}^{\\infty} (4 - 4x^2 + x^4) e^{-x^2} dx = 4\\sqrt{\\pi} - 4\\left(\\frac{1}{2}\\sqrt{\\pi}\\right) + \\frac{3}{4}\\sqrt{\\pi} = \\left(4 - 2 + \\frac{3}{4}\\right)\\sqrt{\\pi} = \\frac{11}{4}\\sqrt{\\pi}$$\nThus, the penalty is approximately:\n$$P_2(f_{\\mathrm{GELU}}) \\approx \\frac{1}{2\\pi} \\left(\\frac{11\\sqrt{\\pi}}{4}\\right) = \\frac{11}{8\\sqrt{\\pi}} \\approx \\frac{11}{8 \\times 1.772} \\approx \\frac{11}{14.176} \\approx 0.776$$\nComparing the two penalties:\n$$P_2(f_{\\mathrm{HS}}) = \\frac{2}{3} \\approx 0.667$$\n$$P_2(f_{\\mathrm{GELU}}) \\approx 0.776$$\nSince $0.667  0.776$, HardSwish has a smaller curvature-only penalty on this interval.\n- **Verdict for B**: **Correct**\n\n**C. Under the curvature-plus-kink penalty $P_{2+\\kappa}$, $\\operatorname{HardSwish}$ has a larger penalty than $\\mathrm{GELU}$.**\n\nThe penalty is $P_{2+\\kappa}(f) = P_2(f) + (\\Delta f'(-3))^2 + (\\Delta f'(3))^2$.\n\nFor GELU, the function $f_{\\mathrm{GELU}}(x)$ and all its derivatives are continuous for all $x \\in \\mathbb{R}$. Therefore, the derivative jumps are zero: $\\Delta f'_{\\mathrm{GELU}}(-3) = 0$ and $\\Delta f'_{\\mathrm{GELU}}(3) = 0$.\n$$P_{2+\\kappa}(f_{\\mathrm{GELU}}) = P_2(f_{\\mathrm{GELU}}) \\approx 0.776$$\n\nFor HardSwish, we must calculate the derivative jumps at $x_i = \\{-3, 3\\}$.\nAt $x=-3$:\n$$ \\lim_{h \\downarrow 0} f'_{\\mathrm{HS}}(-3-h) = 0 $$\n$$ \\lim_{h \\downarrow 0} f'_{\\mathrm{HS}}(-3+h) = \\frac{1}{3}(-3) + \\frac{1}{2} = -1 + \\frac{1}{2} = -\\frac{1}{2} $$\n$$ \\Delta f'_{\\mathrm{HS}}(-3) = \\lim_{h \\downarrow 0} (f'_{\\mathrm{HS}}(-3+h) - f'_{\\mathrm{HS}}(-3-h)) = -\\frac{1}{2} - 0 = -\\frac{1}{2} $$\nAt $x=3$:\n$$ \\lim_{h \\downarrow 0} f'_{\\mathrm{HS}}(3-h) = \\frac{1}{3}(3) + \\frac{1}{2} = 1 + \\frac{1}{2} = \\frac{3}{2} $$\n$$ \\lim_{h \\downarrow 0} f'_{\\mathrm{HS}}(3+h) = 1 $$\n$$ \\Delta f'_{\\mathrm{HS}}(3) = \\lim_{h \\downarrow 0} (f'_{\\mathrm{HS}}(3+h) - f'_{\\mathrm{HS}}(3-h)) = 1 - \\frac{3}{2} = -\\frac{1}{2} $$\nThe total kink penalty for HardSwish is:\n$$ \\sum_{x_i \\in \\{-3,3\\}} \\big(\\Delta f'(x_i)\\big)^2 = \\left(-\\frac{1}{2}\\right)^2 + \\left(-\\frac{1}{2}\\right)^2 = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\nThe total penalty for HardSwish is:\n$$ P_{2+\\kappa}(f_{\\mathrm{HS}}) = P_2(f_{\\mathrm{HS}}) + \\frac{1}{2} = \\frac{2}{3} + \\frac{1}{2} = \\frac{4}{6} + \\frac{3}{6} = \\frac{7}{6} $$\nComparing the total penalties:\n$$P_{2+\\kappa}(f_{\\mathrm{HS}}) = \\frac{7}{6} \\approx 1.167$$\n$$P_{2+\\kappa}(f_{\\mathrm{GELU}}) \\approx 0.776$$\nSince $1.167  0.776$, HardSwish has a larger curvature-plus-kink penalty.\n- **Verdict for C**: **Correct**\n\n**D. Matching slopes at $x=0$ implies that the second derivatives at $x=0$ are equal.**\n\nThis is a general statement about functions. Let two functions be $g(x)$ and $h(x)$. The statement claims that if $g'(0) = h'(0)$, then it must follow that $g''(0) = h''(0)$. This is logically fallacious. The value of a function's derivative at a point does not determine the value of its second derivative at that same point.\n\nAs a counterexample, consider $g(x) = 2x$ and $h(x)=2x+x^2$.\n$g'(x)=2$, so $g'(0)=2$. $h'(x)=2+2x$, so $h'(0)=2$. The slopes match at $x=0$.\nHowever, $g''(x)=0$ and $h''(x)=2$. Thus $g''(0)=0 \\neq h''(0)=2$.\n\nFor the specific functions in this problem, we have already shown $f'_{\\mathrm{GELU}}(0) = f'_{\\mathrm{HS}}(0) = 1/2$. Let's compare their second derivatives at $x=0$.\n$$ f''_{\\mathrm{HS}}(0) = \\frac{1}{3} $$\n$$ f''_{\\mathrm{GELU}}(0) = (2-0^2)\\phi(0) = 2\\phi(0) = 2 \\cdot \\frac{1}{\\sqrt{2\\pi}} = \\frac{\\sqrt{2}}{\\sqrt{\\pi}} \\approx 0.798 $$\nClearly, $1/3 \\neq \\sqrt{2}/\\sqrt{\\pi}$. The statement is false for this specific case as well.\n- **Verdict for D**: **Incorrect**", "answer": "$$\\boxed{ABC}$$", "id": "3128657"}, {"introduction": "In massive models like Transformers, the computational overhead of functions like GELU, which relies on the error function, can be significant. This has led to the development of highly accurate and fast approximations, a common practice in modern deep learning engineering. This hands-on coding exercise challenges you to create one such approximation by fitting a $\\tanh$-based function to GELU using numerical optimization [@problem_id:3128590]. You will not only implement a least-squares fitting procedure but also interpret the resulting parameters, connecting numerical results back to the function's underlying mathematical properties.", "problem": "Let $x \\in \\mathbb{R}$ denote a pre-activation value in a feedforward layer of a deep neural network. The Gaussian Error Linear Unit (GELU) activation is defined by the well-tested formula $ \\mathrm{GELU}(x) = \\dfrac{1}{2} x \\left( 1 + \\mathrm{erf}\\!\\left(\\dfrac{x}{\\sqrt{2}}\\right) \\right) $, where $ \\mathrm{erf}(\\cdot) $ is the error function. Consider the parametric approximation family $ f_{\\alpha,\\beta}(x) = \\dfrac{1}{2} x \\left( 1 + \\tanh\\!\\left( \\alpha \\left( x + \\beta x^3 \\right) \\right) \\right) $, with parameters $ \\alpha \\in \\mathbb{R} $ and $ \\beta \\in \\mathbb{R} $. The goal is to fit $ \\alpha $ and $ \\beta $ via least squares to minimize the squared $ L^2 $ discrepancy between $ f_{\\alpha,\\beta} $ and $ \\mathrm{GELU} $ under a specified weight function $ w(x) $ on a domain $ D \\subset \\mathbb{R} $.\n\nStarting from the core definitions above and standard facts about numerical integration and least-squares fitting, formulate the minimization of the squared $ L^2 $ error\n$$\n\\left\\| f_{\\alpha,\\beta} - \\mathrm{GELU} \\right\\|_{L^2(w;D)}^2\n=\n\\int_{D} \\left( f_{\\alpha,\\beta}(x) - \\mathrm{GELU}(x) \\right)^2 w(x) \\, dx,\n$$\nand solve it numerically by discretizing the integral on an evenly spaced grid. Use a uniform grid with $ N $ points over $ D = [a,b] $ so that the step size is $ \\Delta = \\dfrac{b-a}{N-1} $, and approximate the integral by the Riemann sum\n$$\nJ(\\alpha,\\beta) \\approx \\sum_{i=1}^{N} \\left( f_{\\alpha,\\beta}(x_i) - \\mathrm{GELU}(x_i) \\right)^2 w(x_i) \\, \\Delta,\n$$\nwhere $ x_i $ are the grid points. Implement a numerical optimizer that searches for $ (\\alpha,\\beta) $ to minimize $ J(\\alpha,\\beta) $ subject to simple bounds where specified. For constrained cases, hold a parameter fixed and optimize only over the remaining free parameter.\n\nInterpret the resulting optimal parameters $ \\alpha $ and $ \\beta $ by relating them to local behavior near $ x = 0 $ using small-$ x $ expansions, and explain how $ \\alpha $ controls the local slope scaling and how $ \\beta $ provides a cubic correction to better align with the Gaussian shape.\n\nYour program must evaluate the following test suite and return numerical results:\n\n- Test case $ 1 $ (happy path): $ D = [-10,10] $, $ N = 10001 $, $ w(x) = \\dfrac{1}{\\sqrt{2\\pi}} e^{-x^2/2} $, with bound constraints $ \\alpha \\in [0,2] $, $ \\beta \\in [0,0.2] $. Optimize both $ \\alpha $ and $ \\beta $.\n- Test case $ 2 $ (boundary condition): $ D = [-10,10] $, $ N = 10001 $, $ w(x) = \\dfrac{1}{\\sqrt{2\\pi}} e^{-x^2/2} $, with bound constraint $ \\alpha \\in [0,2] $ and fixed $ \\beta = 0 $. Optimize only $ \\alpha $.\n- Test case $ 3 $ (edge domain): $ D = [-1,1] $, $ N = 5001 $, $ w(x) = 1 $ (uniform), with bound constraints $ \\alpha \\in [0,2] $, $ \\beta \\in [0,0.2] $. Optimize both $ \\alpha $ and $ \\beta $.\n\nFor each test case, compute and return the tuple of three floats $ [\\alpha^\\star,\\beta^\\star,J^\\star] $, where $ \\alpha^\\star $ and $ \\beta^\\star $ are the fitted parameters that minimize $ J(\\alpha,\\beta) $ according to the specified constraints, and $ J^\\star $ is the minimized discretized integral value of the squared $ L^2 $ error. If a parameter is fixed, report its fixed value as $ \\beta^\\star $ (for the second case). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each inner result formatted as a list of three floats rounded to six decimal places, for example:\n$$\n\\text{[}[\\alpha_1,\\beta_1,J_1],[\\alpha_2,\\beta_2,J_2],[\\alpha_3,\\beta_3,J_3]\\text{]}.\n$$\nNo physical units, angle units, or percentages are involved in this problem; all quantities are unitless real numbers. The program must be self-contained and must not require any user input.", "solution": "The user has provided a well-defined numerical optimization problem. The problem is scientifically grounded in numerical analysis and deep learning, is well-posed, objective, and complete. All necessary functions, parameters, and constraints are explicitly provided. Therefore, the problem is deemed valid and a full solution will be provided.\n\nThe core task is to find the optimal parameters $(\\alpha, \\beta)$ for the function family $f_{\\alpha,\\beta}(x)$ that best approximate the Gaussian Error Linear Unit ($\\mathrm{GELU}$) function. The measure of \"best\" is the minimization of the weighted squared $L^2$ error over a domain $D$.\n\nThe $\\mathrm{GELU}$ function is defined as:\n$$\n\\mathrm{GELU}(x) = \\frac{1}{2} x \\left( 1 + \\mathrm{erf}\\left(\\frac{x}{\\sqrt{2}}\\right) \\right)\n$$\nwhere $\\mathrm{erf}(\\cdot)$ is the Gauss error function.\n\nThe parametric approximation is given by:\n$$\nf_{\\alpha,\\beta}(x) = \\frac{1}{2} x \\left( 1 + \\tanh\\left( \\alpha \\left( x + \\beta x^3 \\right) \\right) \\right)\n$$\n\nThe objective is to minimize the squared $L^2$ error, defined by the integral:\n$$\nE(\\alpha, \\beta) = \\int_{D} \\left( f_{\\alpha,\\beta}(x) - \\mathrm{GELU}(x) \\right)^2 w(x) \\, dx\n$$\nwhere $w(x)$ is a given weight function.\n\nTo solve this numerically, we first discretize the integral. The domain $D = [a,b]$ is partitioned into a uniform grid of $N$ points, $x_i = a + i \\cdot \\Delta$ for $i = 0, 1, \\dots, N-1$, with a step size of $\\Delta = \\frac{b-a}{N-1}$. The integral is then approximated by a Riemann sum, resulting in the discrete objective function $J(\\alpha, \\beta)$:\n$$\nJ(\\alpha, \\beta) = \\sum_{i=0}^{N-1} \\left( f_{\\alpha,\\beta}(x_i) - \\mathrm{GELU}(x_i) \\right)^2 w(x_i) \\Delta\n$$\nNote that the problem states the summation is from $i=1$ to $N$, which is a common convention, but for an implementation with $0$-based indexing over $N$ points, the sum from $i=0$ to $N-1$ is equivalent.\n\nThis function $J(\\alpha, \\beta)$ is a scalar function of the two parameters $\\alpha$ and $\\beta$. The problem reduces to finding $(\\alpha^\\star, \\beta^\\star) = \\arg\\min_{\\alpha, \\beta} J(\\alpha, \\beta)$ subject to the given bound constraints. This is a standard non-linear, constrained optimization problem that can be solved using numerical algorithms. We will employ the L-BFGS-B algorithm, a quasi-Newton method that is efficient and can handle simple box-constraints, as implemented in the SciPy library.\n\nA deeper understanding of the roles of $\\alpha$ and $\\beta$ can be gained by analyzing the behavior of the functions near $x=0$ using Taylor series expansions.\n\nThe Taylor series for $\\mathrm{GELU}(x)$ around $x=0$ is:\n$$\n\\mathrm{GELU}(x) = \\frac{1}{2}x + \\frac{1}{\\sqrt{2\\pi}}x^2 - \\frac{1}{6\\sqrt{2\\pi}}x^4 + O(x^6)\n$$\n\nThe Taylor series for the approximation $f_{\\alpha,\\beta}(x)$ around $x=0$ can be derived by expanding the $\\tanh$ function. Using $\\tanh(u) = u - u^3/3 + O(u^5)$ with $u = \\alpha(x + \\beta x^3)$:\n$$\n\\begin{align*}\nf_{\\alpha,\\beta}(x) = \\frac{1}{2}x \\left( 1 + \\tanh(\\alpha x + \\alpha \\beta x^3) \\right) \\\\\n= \\frac{1}{2}x \\left( 1 + \\left[ (\\alpha x + \\alpha \\beta x^3) - \\frac{1}{3}(\\alpha x + \\alpha \\beta x^3)^3 + \\dots \\right] \\right) \\\\\n= \\frac{1}{2}x \\left( 1 + \\alpha x + \\alpha \\beta x^3 - \\frac{\\alpha^3 x^3}{3} + O(x^5) \\right) \\\\\n= \\frac{1}{2}x + \\frac{\\alpha}{2}x^2 + \\left( \\frac{\\alpha \\beta}{2} - \\frac{\\alpha^3}{6} \\right)x^4 + O(x^6)\n\\end{align*}\n$$\n\nBy matching the coefficients of the two series, we can determine the theoretical optimal values for the parameters:\n1.  The linear term $\\frac{1}{2}x$ matches identically, which is a desirable property.\n2.  Matching the $x^2$ term: $\\frac{\\alpha}{2} = \\frac{1}{\\sqrt{2\\pi}} \\implies \\alpha = \\sqrt{\\frac{2}{\\pi}}$. This shows that $\\alpha$ primarily controls the quadratic behavior of the approximation near the origin. Its theoretical value is $\\alpha \\approx 0.79788$.\n3.  Matching the $x^4$ term: $\\frac{\\alpha \\beta}{2} - \\frac{\\alpha^3}{6} = -\\frac{1}{6\\sqrt{2\\pi}}$. Substituting the value for $\\alpha$, we can solve for $\\beta$:\n    $$\n    \\beta = \\frac{2}{\\alpha} \\left( \\frac{\\alpha^3}{6} - \\frac{1}{6\\sqrt{2\\pi}} \\right) = \\frac{\\alpha^2}{3} - \\frac{1}{3\\alpha\\sqrt{2\\pi}} = \\frac{1}{3}\\left(\\frac{2}{\\pi}\\right) - \\frac{1}{3\\sqrt{2/\\pi}\\sqrt{2\\pi}} = \\frac{2}{3\\pi} - \\frac{1}{6}\n    $$\n    This reveals that $\\beta$ provides a cubic correction to the argument of $\\tanh$, which translates into a quartic correction ($x^4$) for the final function, allowing a finer fit to the shape of GELU. Its theoretical value is $\\beta \\approx 0.04551$.\n\nThe numerical optimization is expected to yield parameters close to these theoretical values, especially in Test Case $1$, where the Gaussian weight function $w(x)$ emphasizes the region around $x=0$ where the Taylor series approximation is most accurate.\n\nFor each test case, the procedure is as follows:\n1.  Define the grid of points $x_i$ and the integration step $\\Delta$.\n2.  Pre-compute the values of $\\mathrm{GELU}(x_i)$ and the weights $w(x_i)$ on the grid.\n3.  Define an objective function that takes a parameter vector (e.g., $(\\alpha, \\beta)$) and returns the value of $J(\\alpha, \\beta)$.\n4.  Use `scipy.optimize.minimize` with the 'L-BFGS-B' method, providing the objective function, an initial guess for the parameters, and the specified bounds.\n5.  For Test Case $2$, where $\\beta$ is fixed, the optimization is performed over a single variable $\\alpha$.\n6.  The resulting optimal parameters $(\\alpha^\\star, \\beta^\\star)$ and the minimized objective function value $J^\\star$ are collected and formatted.", "answer": "```python\nimport numpy as np\nfrom scipy.special import erf\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves for the optimal parameters (alpha, beta) for an approximation\n    of the GELU function by minimizing the discretized weighted L^2 error.\n    \"\"\"\n\n    def gelu(x):\n        \"\"\"Gaussian Error Linear Unit (GELU) activation function.\"\"\"\n        return 0.5 * x * (1.0 + erf(x / np.sqrt(2.0)))\n\n    def f_approx(x, alpha, beta):\n        \"\"\"Parametric approximation of GELU.\"\"\"\n        arg = alpha * (x + beta * x**3)\n        return 0.5 * x * (1.0 + np.tanh(arg))\n\n    test_cases = [\n        {\n            \"D\": (-10.0, 10.0), \"N\": 10001,\n            \"w_func\": lambda x: (1.0 / np.sqrt(2.0 * np.pi)) * np.exp(-x**2 / 2.0),\n            \"bounds\": [(0.0, 2.0), (0.0, 0.2)],\n            \"fixed_params\": {},\n            \"initial_guess\": [0.8, 0.05]\n        },\n        {\n            \"D\": (-10.0, 10.0), \"N\": 10001,\n            \"w_func\": lambda x: (1.0 / np.sqrt(2.0 * np.pi)) * np.exp(-x**2 / 2.0),\n            \"bounds\": [(0.0, 2.0)],\n            \"fixed_params\": {\"beta\": 0.0},\n            \"initial_guess\": [0.8]\n        },\n        {\n            \"D\": (-1.0, 1.0), \"N\": 5001,\n            \"w_func\": lambda x: 1.0,\n            \"bounds\": [(0.0, 2.0), (0.0, 0.2)],\n            \"fixed_params\": {},\n            \"initial_guess\": [0.8, 0.05]\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        a, b = case[\"D\"]\n        N = case[\"N\"]\n        w_func = case[\"w_func\"]\n        bounds = case[\"bounds\"]\n        fixed_params = case[\"fixed_params\"]\n        initial_guess = case[\"initial_guess\"]\n\n        x_grid = np.linspace(a, b, N)\n        delta = (b - a) / (N - 1)\n\n        gelu_values = gelu(x_grid)\n        weights = w_func(x_grid)\n\n        def objective_function(params):\n            if \"beta\" in fixed_params:\n                alpha = params[0]\n                beta = fixed_params[\"beta\"]\n            else:\n                alpha, beta = params\n\n            f_values = f_approx(x_grid, alpha, beta)\n            squared_errors = (f_values - gelu_values)**2\n            weighted_errors = squared_errors * weights\n            integral = np.sum(weighted_errors) * delta\n            return integral\n\n        opt_result = minimize(\n            objective_function,\n            x0=initial_guess,\n            method='L-BFGS-B',\n            bounds=bounds\n        )\n\n        J_star = opt_result.fun\n        if \"beta\" in fixed_params:\n            alpha_star = opt_result.x[0]\n            beta_star = fixed_params[\"beta\"]\n        else:\n            alpha_star, beta_star = opt_result.x\n\n        results.append([alpha_star, beta_star, J_star])\n\n    # Format the output as specified\n    formatted_results = [f\"[{r[0]:.6f},{r[1]:.6f},{r[2]:.6f}]\" for r in results]\n    print(f\"[[{formatted_results[0]}],[{formatted_results[1]}],[{formatted_results[2]}]]\")\n\nsolve()\n```", "id": "3128590"}]}