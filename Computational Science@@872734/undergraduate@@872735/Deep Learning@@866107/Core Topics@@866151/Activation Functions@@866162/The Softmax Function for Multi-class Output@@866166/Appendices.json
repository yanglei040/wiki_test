{"hands_on_practices": [{"introduction": "The softmax function is a cornerstone of multi-class classification, but its direct implementation using exponentials is a minefield of numerical instability. Large positive logit values can cause overflow, while a large range of logit values can lead to a loss of precision. This first exercise tackles this fundamental challenge head-on by guiding you through the derivation and implementation of the \"log-sum-exp\" trick, a vital technique for ensuring your softmax computations are both stable and accurate across different numerical regimes [@problem_id:3193214]. Mastering this is a prerequisite for building robust deep learning models.", "problem": "You are given a vector of real-valued logits $\\mathbf{z} = (z_1, z_2, \\dots, z_n) \\in \\mathbb{R}^n$. In multi-class classification with the softmax function, the log-normalizer is the quantity $A(\\mathbf{z}) = \\log\\left(\\sum_{j=1}^n e^{z_j}\\right)$, which must be computed reliably in finite-precision arithmetic. Your task is to derive, implement, and analyze a numerically stable method to compute $A(\\mathbf{z})$.\n\nStarting from the fundamental definitions and properties of the exponential function and the natural logarithm, and using only algebraic manipulations that are valid for all real numbers, derive an expression for $A(\\mathbf{z})$ that avoids overflow and severe loss of significance by factoring out the largest component of $\\mathbf{z}$. Let $m = \\max_j z_j$. Show that this transformation preserves the exact mathematical value of $A(\\mathbf{z})$ and explain why it is numerically beneficial.\n\nAssume a standard finite-precision rounding model for Institute of Electrical and Electronics Engineers (IEEE 754) binary64 arithmetic with unit roundoff $u$ (half the machine epsilon $\\epsilon_{\\text{mach}}$ for rounding to nearest). Model each basic operation $x \\circ y$ with $\\circ \\in \\{+, \\times\\}$ as $\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\delta)$ with $|\\delta| \\leq u$, and treat elementary functions $f \\in \\{\\exp, \\log\\}$ as introducing at most a relative error of $u$ on their outputs. For the summation of $n$ positive terms using a straightforward left-to-right accumulation, use the standard bound that the computed sum $\\widehat{S}$ satisfies a relative error bounded by $\\gamma_{n-1} = \\dfrac{(n-1)u}{1 - (n-1)u}$.\n\nUsing these models, derive a conservative upper bound on the absolute error of the numerically stable computation of $A(\\mathbf{z})$ expressed as a function of $m$, the length $n$, and the machine precision parameters $u$ and $\\epsilon_{\\text{mach}}$, and any quantities that arise naturally from the stabilized expression (for example, the stabilized sum $S = \\sum_{j=1}^n e^{z_j - m}$ and its logarithm). Your bound must be explicit and computable from $\\mathbf{z}$, $u$, and $n$.\n\nImplement a program that:\n- Computes the naive value $A_{\\text{naive}}(\\mathbf{z})$ using direct exponentiation and summation when it does not overflow, for verification purposes.\n- Computes the stabilized value $A_{\\text{stable}}(\\mathbf{z})$ using your derived expression.\n- Computes a high-precision reference $A_{\\text{ref}}(\\mathbf{z})$ using arbitrary precision arithmetic with a normalization by $m$ to avoid overflow in the reference computation.\n- Computes your theoretical absolute error bound $B(\\mathbf{z})$ for $A_{\\text{stable}}(\\mathbf{z})$ in terms of $m$, $u$, $n$, and any stabilized intermediate quantities you defined.\n- Produces, for each test case, a boolean that is true if $|A_{\\text{stable}}(\\mathbf{z}) - A_{\\text{ref}}(\\mathbf{z})| \\leq B(\\mathbf{z})$ and false otherwise.\n\nUse the following test suite of input vectors $\\mathbf{z}$ to exercise different numerical regimes:\n1. $\\mathbf{z}_1 = (-2.0, 0.0, 1.5, -0.5)$, a general case with modest magnitudes.\n2. $\\mathbf{z}_2 = (1000.0, 0.0, -1000.0)$, a case that would overflow in naive exponentiation in binary64 arithmetic.\n3. $\\mathbf{z}_3 = (709.0, 700.0, 0.0)$, values near the overflow threshold for $e^{x}$ in binary64 arithmetic.\n4. $\\mathbf{z}_4 = (-745.0, -746.0, -747.0)$, values near the underflow/subnormal regime for $e^{x}$ in binary64 arithmetic.\n5. $\\mathbf{z}_5 = (0.0, 0.0, 0.0, 0.0, 0.0)$, equal entries to test symmetry and accumulation.\n6. $\\mathbf{z}_6$ consisting of $101$ entries linearly spaced from $-10$ to $10$, i.e., $z_j = -10 + \\dfrac{20(j-1)}{100}$ for $j = 1, 2, \\dots, 101$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_6]$, where each $\\text{result}_k$ is a boolean for the corresponding test case. No physical units, angles, or percentages appear in this problem; all outputs are unitless.", "solution": "The problem statement is critically reviewed and found to be valid. It is scientifically grounded in the principles of numerical analysis and floating-point arithmetic, well-posed with a clear objective, and provides sufficient information for a unique, verifiable solution. The minor ambiguity regarding the use of \"arbitrary precision arithmetic\" is resolved by interpreting it as the highest precision available within the specified environment, `numpy.longdouble`, which is a standard and practical approach for establishing a reference value.\n\nThe task is to derive and analyze a numerically stable method for computing the log-normalizer, or log-sum-exp function, defined for a vector $\\mathbf{z} = (z_1, z_2, \\dots, z_n) \\in \\mathbb{R}^n$ as:\n$$A(\\mathbf{z}) = \\log\\left(\\sum_{j=1}^n e^{z_j}\\right)$$\n\n**1. Derivation of the Numerically Stable Expression**\n\nThe primary numerical challenge in computing $A(\\mathbf{z})$ arises from the exponential function, $e^{z_j}$. If any $z_j$ is a large positive number (e.g., for binary64 arithmetic, $z_j > 709.78$), $e^{z_j}$ will overflow to infinity. If the $z_j$ values have a large dynamic range, the sum may be dominated by a single term, leading to a loss of significance for smaller terms.\n\nTo mitigate these issues, we introduce a normalization constant. Let $m = \\max_{1 \\le j \\le n} z_j$. This is the largest component of the vector $\\mathbf{z}$. We can factor the term $e^m$ from the sum inside the logarithm. This manipulation is algebraically exact:\n$$\n\\sum_{j=1}^n e^{z_j} = \\sum_{j=1}^n e^{m + (z_j - m)} = \\sum_{j=1}^n e^m e^{z_j - m} = e^m \\left(\\sum_{j=1}^n e^{z_j - m}\\right)\n$$\nSubstituting this back into the expression for $A(\\mathbf{z})$:\n$$\nA(\\mathbf{z}) = \\log\\left( e^m \\sum_{j=1}^n e^{z_j - m} \\right)\n$$\nUsing the fundamental property of logarithms, $\\log(xy) = \\log(x) + \\log(y)$, we can separate the terms:\n$$\nA(\\mathbf{z}) = \\log(e^m) + \\log\\left(\\sum_{j=1}^n e^{z_j - m}\\right)\n$$\nFinally, since the natural logarithm and the exponential function are inverses, $\\log(e^m) = m$. This yields the stabilized expression:\n$$\nA(\\mathbf{z}) = m + \\log\\left(\\sum_{j=1}^n e^{z_j - m}\\right)\n$$\nThis transformation is mathematically exact for any $\\mathbf{z} \\in \\mathbb{R}^n$.\n\nThe numerical benefits of this form are substantial. Let $y_j = z_j - m$. By the definition of $m$, we have $y_j \\le 0$ for all $j$. The maximum value of any $y_j$ is $0$, which occurs for the index $k$ where $z_k = m$.\n- **Overflow Prevention**: The arguments to the exponential function, $y_j$, are now in the range $(-\\infty, 0]$. The largest value, $e^0 = 1$, cannot cause an overflow.\n- **Accuracy Improvement**: The sum $S = \\sum_{j=1}^n e^{y_j}$ contains the term $e^0 = 1$, so $S \\ge 1$. This prevents the sum from underflowing to zero, which would cause the subsequent logarithm to fail. While individual terms $e^{y_j}$ for very negative $y_j$ may underflow to $0$, this is a graceful degradation, as such terms are mathematically negligible to the sum's value. This formulation avoids summing numbers of vastly different magnitudes, which is a primary source of precision loss in floating-point arithmetic.\n\n**2. Derivation of the Absolute Error Bound**\n\nWe now derive a conservative upper bound for the absolute error $|\\hat{A} - A(\\mathbf{z})|$, where $\\hat{A}$ is the value computed using the stable formula in finite-precision arithmetic. We use the floating-point model specified in the problem, with unit roundoff $u$.\nLet $A(\\mathbf{z}) = m + \\log(S)$, where $S = \\sum_{j=1}^n e^{z_j-m}$.\nThe computed value $\\hat{A}$ results from the sequence of floating-point operations:\n1. $\\hat{y}_j = \\operatorname{fl}(z_j - m)$\n2. $\\hat{t}_j = \\operatorname{fl}(e^{\\hat{y}_j})$\n3. $\\hat{S} = \\operatorname{fl}(\\sum_j \\hat{t}_j)$\n4. $\\hat{L} = \\operatorname{fl}(\\log(\\hat{S}))$\n5. $\\hat{A} = \\operatorname{fl}(m + \\hat{L})$\n\nThe total absolute error is $|\\hat{A} - A(\\mathbf{z})|$. We analyze the error contribution from each step.\nThe final operation is an addition, so $\\hat{A} = (m+\\hat{L})(1+\\theta_A)$ with $|\\theta_A| \\le u$.\n$$|\\hat{A} - A(\\mathbf{z})| = |(m+\\hat{L})(1+\\theta_A) - (m+\\log(S))| = |\\hat{L}-\\log(S) + (m+\\hat{L})\\theta_A|$$\n$$ \\le |\\hat{L}-\\log(S)| + |m+\\hat{L}|u $$\nThe next step is to bound the error in $\\hat{L} = \\operatorname{fl}(\\log(\\hat{S})) = \\log(\\hat{S})(1+\\theta_L)$ with $|\\theta_L|\\le u$.\n$$ |\\hat{L} - \\log(S)| = |\\log(\\hat{S})(1+\\theta_L) - \\log(S)| = |\\log(\\hat{S}/S) + \\log(\\hat{S})\\theta_L| $$\n$$ \\le |\\log(\\hat{S}/S)| + |\\log(\\hat{S})|u $$\nTo bound $|\\log(\\hat{S}/S)|$, we use the mean value theorem, which implies $|\\log(\\hat{S}/S)| \\approx |\\hat{S}/S - 1|$ for $\\hat{S} \\approx S$. This term represents the relative error in $S$.\nThe error in $S$ comes from two sources: the error in computing the terms $\\hat{t}_j$ and the error in their summation.\n- The error in each term $\\hat{t}_j = \\operatorname{fl}(e^{\\operatorname{fl}(z_j-m)})$ relative to the true term $s_j = e^{z_j-m}$ can be bounded. A first-order analysis shows that $|\\hat{t}_j - s_j| \\lesssim s_j(1+|z_j-m|)u$.\n- The sum of these individual term errors, $\\sum |\\hat{t}_j - s_j|$, is thus bounded by $u \\sum s_j(1+|z_j-m|)$.\n- The summation of the computed terms $\\hat{t}_j$ introduces a relative error bounded by $\\gamma_{n-1} = \\frac{(n-1)u}{1-(n-1)u}$.\n\nCombining these sources, the relative error on the sum $S$ is bounded by:\n$$ \\left|\\frac{\\hat{S}-S}{S}\\right| \\lesssim \\gamma_{n-1} + u \\frac{\\sum_{j=1}^n s_j(1+|z_j-m|)}{S} $$\nReplacing the exact (and unknown) quantities $s_j$ and $S$ with their computed estimates $\\hat{t}_j$ and $\\hat{S}$, and defining the computable sum $\\hat{Q}_{\\text{comp}} = \\sum_{j=1}^n \\hat{t}_j(1+|z_j-m|)$, we get a computable bound for the relative error:\n$$ \\left|\\frac{\\hat{S}-S}{S}\\right| \\lesssim \\gamma_{n-1} + u \\frac{\\hat{Q}_{\\text{comp}}}{\\hat{S}} $$\nCombining all parts, the total absolute error is bounded by:\n$$ |\\hat{A} - A(\\mathbf{z})| \\lesssim \\left( \\gamma_{n-1} + u \\frac{\\hat{Q}_{\\text{comp}}}{\\hat{S}} \\right) + |\\hat{L}|u + |m+\\hat{L}|u $$\nWe define our conservative, computable upper bound $B(\\mathbf{z})$ as:\n$$ B(\\mathbf{z}) = \\gamma_{n-1} + u \\left( \\frac{\\hat{Q}_{\\text{comp}}}{\\hat{S}} + |\\hat{L}| + |m+\\hat{L}| \\right) $$\nwhere all quantities on the right-hand side are computed in the working precision. This bound accounts for the errors from summation ($\\gamma_{n-1}$), propagation from term computation ($u\\hat{Q}_{\\text{comp}}/\\hat{S}$), the logarithm ($u|\\hat{L}|$), and the final addition ($u|m+\\hat{L}|$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives, implements, and analyzes a numerically stable method for \n    computing the log-sum-exp function, and verifies a theoretical \n    error bound against a high-precision reference computation.\n    \"\"\"\n\n    def compute_and_verify(z_in: tuple or list):\n        \"\"\"\n        Computes the stable log-sum-exp, its error bound, a reference value,\n        and verifies that the actual error is within the theoretical bound.\n\n        Args:\n            z_in: A tuple or list of real numbers representing the input vector.\n\n        Returns:\n            A boolean, True if |A_stable - A_ref| <= Bound, False otherwise.\n        \"\"\"\n        # --- Environment Setup (IEEE 754 binary64) ---\n        u = np.finfo(np.float64).eps / 2.0\n        z = np.array(z_in, dtype=np.float64)\n        n = z.size\n\n        if n > 1 and (n - 1) * u >= 1:\n            raise ValueError(\"n is too large for the summation error model to be valid.\")\n        \n        gamma_n_minus_1 = ((n - 1) * u) / (1 - (n - 1) * u) if n > 1 else 0.0\n\n        # --- Stable Computation (float64) ---\n        # A_stable = m + log(sum(exp(z - m)))\n        m = np.max(z)\n        y = z - m  # These are the shifted exponents\n        t = np.exp(y)  # These are the terms in the sum\n        S_hat = np.sum(t)\n        L_hat = np.log(S_hat)\n        A_stable = m + L_hat\n\n        # --- Theoretical Absolute Error Bound Computation (float64) ---\n        # B(z) = gamma_{n-1} + u * (Q_hat/S_hat + |L_hat| + |m+L_hat|)\n        # where Q_hat = sum(t_j * (1 + |y_j|))\n        Q_hat_comp = np.sum(t * (1.0 + m - z)) # |y_j| = m - z_j since m is max\n        \n        bound_sum_err = gamma_n_minus_1\n        bound_term_prop_err = u * (Q_hat_comp / S_hat)\n        bound_log_err = u * np.abs(L_hat)\n        bound_add_err = u * np.abs(m + L_hat)\n        \n        bound = bound_sum_err + bound_term_prop_err + bound_log_err + bound_add_err\n\n        # --- High-Precision Reference Computation (longdouble) ---\n        # The reference computation also uses the stable method to avoid overflow,\n        # but with higher precision arithmetic (numpy.longdouble).\n        z_ref = np.array(z_in, dtype=np.longdouble)\n        m_ref = np.max(z_ref)\n        y_ref = z_ref - m_ref\n        S_ref = np.sum(np.exp(y_ref))\n        A_ref = m_ref + np.log(S_ref)\n\n        # --- Comparison ---\n        actual_error = np.abs(A_stable - A_ref)\n        \n        return actual_error <= bound\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # 1. General case\n        (-2.0, 0.0, 1.5, -0.5),\n        # 2. Overflow case\n        (1000.0, 0.0, -1000.0),\n        # 3. Near overflow threshold\n        (709.0, 700.0, 0.0),\n        # 4. Underflow/subnormal regime\n        (-745.0, -746.0, -747.0),\n        # 5. Equal entries\n        (0.0, 0.0, 0.0, 0.0, 0.0),\n        # 6. Larger summation\n        np.linspace(-10.0, 10.0, 101).tolist(),\n    ]\n\n    results = []\n    for case in test_cases:\n        is_bound_valid = compute_and_verify(case)\n        results.append(str(is_bound_valid).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3193214"}, {"introduction": "While the standard cross-entropy loss works well in many scenarios, it can be inefficient when there is a large imbalance between easy and hard-to-classify examples. The focal loss addresses this by dynamically adjusting the weight of each example, forcing the model to focus more on the \"hard\" cases it gets wrong. This practice will have you derive the gradient for this powerful loss function from scratch and verify its correctness with numerical gradient checking, providing you with essential skills for developing and debugging custom training objectives [@problem_id:3193212].", "problem": "You will study a multi-class classifier that maps a real-valued logit vector to class probabilities using the softmax function and trains with a focal loss objective. Begin from fundamental definitions only, and do not assume any pre-derived gradients. Let the logits be $\\mathbf{z} \\in \\mathbb{R}^K$ and the predicted probabilities be given by the softmax\n$$\np_i(\\mathbf{z}) \\equiv \\frac{\\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)} \\quad \\text{for } i \\in \\{1,\\dots,K\\}.\n$$\nFor a true class index $y \\in \\{1,\\dots,K\\}$ and focusing parameter $\\gamma \\ge 0$, the focal loss is\n$$\nL(\\mathbf{z};y,\\gamma) \\equiv -\\left(1 - p_y(\\mathbf{z})\\right)^{\\gamma} \\,\\log\\!\\left(p_y(\\mathbf{z})\\right),\n$$\nwhere $\\log$ denotes the natural logarithm.\n\nYour tasks are:\n- Derive the gradient $\\frac{\\partial L}{\\partial z_i}$ for each component $i \\in \\{1,\\dots,K\\}$ starting only from the given definitions and standard multivariable calculus rules. Do not assume any pre-known Jacobian of the softmax; derive it from the definition.\n- Implement a numerically stable softmax, the focal loss, and your derived analytic gradient with respect to $\\mathbf{z}$.\n- Evaluate the effect of the focusing parameter on hard examples where $p_y(\\mathbf{z})$ is small, compared to easy examples where $p_y(\\mathbf{z})$ is close to one.\n\nProgram requirements:\n- Implement a function that computes a numerically stable softmax for any input $\\mathbf{z}$ by first subtracting $\\max_i z_i$ before exponentiation.\n- Implement a function that computes $L(\\mathbf{z};y,\\gamma)$ directly from $\\mathbf{z}$ using the stable softmax.\n- Implement a function that computes the analytic gradient $\\nabla_{\\mathbf{z}} L$ using your derived expression.\n- Implement a central-difference numerical gradient checker with step size $h = 10^{-6}$ to approximate $\\nabla_{\\mathbf{z}} L$:\n$$\n\\left[\\nabla_{\\mathbf{z}} L(\\mathbf{z})\\right]_i \\approx \\frac{L(\\mathbf{z} + h\\,\\mathbf{e}_i) - L(\\mathbf{z} - h\\,\\mathbf{e}_i)}{2h}.\n$$\n\nTest suite and outputs:\n- General-case gradient check: let $K=4$, $\\mathbf{z} = [0.0,\\,1.0,\\,-0.5,\\,2.0]$, $y = 4$, and $\\gamma = 2.0$. Compute the maximum absolute difference between your analytic gradient and the central-difference gradient with $h = 10^{-6}$, and output this difference as a floating-point number.\n- Boundary-case focusing parameter: let $K=3$, $\\mathbf{z} = [2.0,\\,-1.0,\\,0.5]$, $y = 1$, and $\\gamma = 0.0$. In this case, the focal loss reduces to Cross-Entropy (CE). Compute the maximum absolute difference between your analytic gradient and the CE gradient $\\mathbf{p} - \\mathbf{e}_y$, and output this difference as a floating-point number. Here $\\mathbf{e}_y$ is the one-hot vector with a one at index $y$ and zeros elsewhere.\n- Hard-versus-easy emphasis: consider two $K=4$ cases with the same true class $y = 1$ and $\\gamma = 2.0$.\n  - Easy example: $\\mathbf{z}_{\\mathrm{easy}} = [10.0,\\,0.0,\\,0.0,\\,0.0]$.\n  - Hard example: $\\mathbf{z}_{\\mathrm{hard}} = [-10.0,\\,0.0,\\,0.0,\\,0.0]$.\n  For each, compute the ratio\n  $$\n  S \\equiv \\frac{\\left\\|\\nabla_{\\mathbf{z}} L_{\\mathrm{focal}}\\right\\|_2}{\\left\\|\\nabla_{\\mathbf{z}} L_{\\mathrm{CE}}\\right\\|_2},\n  $$\n  where $L_{\\mathrm{CE}}(\\mathbf{z};y) \\equiv -\\log p_y(\\mathbf{z})$ and $\\nabla_{\\mathbf{z}} L_{\\mathrm{CE}} = \\mathbf{p} - \\mathbf{e}_y$. Output a Boolean that is true if the hard example has a strictly larger $S$ than the easy example. Additionally, output the pair of focusing weights $\\left[(1-p_y(\\mathbf{z}_{\\mathrm{easy}}))^{\\gamma},\\,(1-p_y(\\mathbf{z}_{\\mathrm{hard}}))^{\\gamma}\\right]$ as a list of two floating-point numbers.\n\nFinal output format:\n- Your program must produce a single line of output containing the results in the following order as a comma-separated list enclosed in square brackets:\n  - the general-case maximum absolute gradient difference (floating-point),\n  - the boundary-case maximum absolute difference to Cross-Entropy (floating-point),\n  - the hard-versus-easy emphasis comparison (Boolean),\n  - the list of focusing weights for the easy and hard examples in that order (list of two floating-point numbers).\n- Concretely, the single line must look like\n$$\n[\\;r_{\\mathrm{gen}},\\;r_{\\mathrm{ce0}},\\;b_{\\mathrm{hard>easy}},\\;[w_{\\mathrm{easy}},w_{\\mathrm{hard}}]\\;].\n$$\nNo user input is required, and there are no physical units involved. Use the natural logarithm and report raw decimal values without percentage signs.", "solution": "The user has provided a well-defined problem in the domain of deep learning, asking for the derivation and implementation of the gradient of the focal loss function. The problem is scientifically sound, self-contained, and algorithmically specified. I will proceed with the solution.\n\n### Part 1: Gradient Derivation\n\nThe primary task is to derive the gradient of the focal loss function $L(\\mathbf{z}; y, \\gamma)$ with respect to the logit vector $\\mathbf{z} \\in \\mathbb{R}^K$. The focal loss is defined as:\n$$\nL(\\mathbf{z};y,\\gamma) \\equiv -\\left(1 - p_y(\\mathbf{z})\\right)^{\\gamma} \\,\\log\\!\\left(p_y(\\mathbf{z})\\right)\n$$\nwhere $p_y(\\mathbf{z})$ is the softmax probability for the true class $y$:\n$$\np_y(\\mathbf{z}) = \\frac{\\exp(z_y)}{\\sum_{j=1}^{K} \\exp(z_j)}\n$$\nWe seek to compute the partial derivative $\\frac{\\partial L}{\\partial z_i}$ for each component $i \\in \\{1, \\dots, K\\}$. We will use the chain rule. The loss $L$ is a function of $p_y$, which in turn is a function of all logits $z_j$.\n$$\n\\frac{\\partial L}{\\partial z_i} = \\frac{\\mathrm{d}L}{\\mathrm{d}p_y} \\frac{\\partial p_y}{\\partial z_i}\n$$\n\n**Step 1.1: Derivative of Loss with respect to Probability, $\\frac{\\mathrm{d}L}{\\mathrm{d}p_y}$**\n\nWe apply the product rule to $L = -(1-p_y)^\\gamma \\log(p_y)$. Let $u(p_y) = -(1-p_y)^\\gamma$ and $v(p_y) = \\log(p_y)$.\n$$\n\\frac{\\mathrm{d}L}{\\mathrm{d}p_y} = \\frac{\\mathrm{d}u}{\\mathrm{d}p_y} v(p_y) + u(p_y) \\frac{\\mathrm{d}v}{\\mathrm{d}p_y}\n$$\nThe derivatives of the components are:\n$$\n\\frac{\\mathrm{d}u}{\\mathrm{d}p_y} = \\frac{\\mathrm{d}}{\\mathrm{d}p_y} \\left[-(1-p_y)^\\gamma\\right] = - \\left[\\gamma(1-p_y)^{\\gamma-1}(-1)\\right] = \\gamma(1-p_y)^{\\gamma-1}\n$$\n$$\n\\frac{\\mathrm{d}v}{\\mathrm{d}p_y} = \\frac{\\mathrm{d}}{\\mathrm{d}p_y} \\left[\\log(p_y)\\right] = \\frac{1}{p_y}\n$$\nSubstituting these back, we get:\n$$\n\\frac{\\mathrm{d}L}{\\mathrm{d}p_y} = \\gamma(1-p_y)^{\\gamma-1} \\log(p_y) - (1-p_y)^\\gamma \\frac{1}{p_y}\n$$\n$$\n\\frac{\\mathrm{d}L}{\\mathrm{d}p_y} = \\gamma(1-p_y)^{\\gamma-1} \\log(p_y) - \\frac{(1-p_y)^\\gamma}{p_y}\n$$\n\n**Step 1.2: Derivative of Softmax Probability with respect to Logits, $\\frac{\\partial p_k}{\\partial z_i}$**\n\nAs required, we derive the Jacobian of the softmax function. Let $p_k(\\mathbf{z}) = \\frac{\\exp(z_k)}{\\sum_{j=1}^{K} \\exp(z_j)}$. Let $S = \\sum_{j=1}^{K} \\exp(z_j)$.\n\nCase 1: $i = k$. Using the quotient rule:\n$$\n\\frac{\\partial p_k}{\\partial z_k} = \\frac{\\left(\\frac{\\partial}{\\partial z_k}\\exp(z_k)\\right)S - \\exp(z_k)\\left(\\frac{\\partial S}{\\partial z_k}\\right)}{S^2} = \\frac{\\exp(z_k)S - \\exp(z_k)\\exp(z_k)}{S^2}\n$$\n$$\n= \\frac{\\exp(z_k)}{S} \\left(\\frac{S - \\exp(z_k)}{S}\\right) = p_k(1-p_k)\n$$\n\nCase 2: $i \\neq k$. Using the quotient rule:\n$$\n\\frac{\\partial p_k}{\\partial z_i} = \\frac{\\left(\\frac{\\partial}{\\partial z_i}\\exp(z_k)\\right)S - \\exp(z_k)\\left(\\frac{\\partial S}{\\partial z_i}\\right)}{S^2} = \\frac{0 \\cdot S - \\exp(z_k)\\exp(z_i)}{S^2}\n$$\n$$\n= -\\frac{\\exp(z_k)}{S}\\frac{\\exp(z_i)}{S} = -p_k p_i\n$$\n\nThese two cases can be unified using the Kronecker delta, $\\delta_{ik}$:\n$$\n\\frac{\\partial p_k}{\\partial z_i} = p_k(\\delta_{ik} - p_i)\n$$\nFor our loss function, we are interested in the probability of the true class $y$, so we set $k=y$:\n$$\n\\frac{\\partial p_y}{\\partial z_i} = p_y(\\delta_{iy} - p_i)\n$$\n\n**Step 1.3: Combining the Terms**\n\nWe now substitute the results from Step 1.1 and 1.2 into the chain rule expression:\n$$\n\\frac{\\partial L}{\\partial z_i} = \\left( \\gamma(1-p_y)^{\\gamma-1} \\log(p_y) - \\frac{(1-p_y)^\\gamma}{p_y} \\right) \\cdot \\left( p_y(\\delta_{iy} - p_i) \\right)\n$$\nDistributing the $p_y$ factor into the first parenthesis simplifies the expression:\n$$\n\\frac{\\partial L}{\\partial z_i} = \\left( \\gamma p_y (1-p_y)^{\\gamma-1} \\log(p_y) - (1-p_y)^\\gamma \\right) (\\delta_{iy} - p_i)\n$$\nThis is the final expression for the $i$-th component of the gradient $\\nabla_{\\mathbf{z}}L$.\n\n**Step 1.4: Verification for $\\gamma=0$**\n\nFor $\\gamma=0$, the focal loss reduces to the standard cross-entropy loss, $L = -\\log(p_y)$. Let's verify our gradient formula for this case. The term in the first parenthesis becomes:\n$$\n\\left( 0 \\cdot p_y (1-p_y)^{-1} \\log(p_y) - (1-p_y)^0 \\right) = -1\n$$\nThus, the gradient becomes:\n$$\n\\frac{\\partial L}{\\partial z_i}\\bigg|_{\\gamma=0} = (-1)(\\delta_{iy} - p_i) = p_i - \\delta_{iy}\n$$\nIn vector form, $\\nabla_{\\mathbf{z}}L = \\mathbf{p} - \\mathbf{e}_y$, where $\\mathbf{e}_y$ is the one-hot vector for the true class $y$. This matches the known gradient for the cross-entropy loss and confirms our derivation.\n\n### Part 2: Implementation and Evaluation\n\nThe following sections will be implemented in the final Python code.\n\n**Numerically Stable Softmax**: To avoid numerical overflow with large logits, we use the identity $\\text{softmax}(\\mathbf{z}) = \\text{softmax}(\\mathbf{z} - c)$ for any constant $c$. By choosing $c = \\max_j z_j$, we ensure that the arguments to the exponential function are non-positive, preventing overflow.\n\n**Focal Loss and Gradient Functions**: We will implement functions for the stable softmax, the focal loss, its analytic gradient based on our derived formula, and a numerical gradient checker using central differences.\n\n**Test Suite Execution**: The code will execute the three test cases specified in the problem:\n1.  A general gradient check comparing the analytic and numerical gradients.\n2.  A boundary case check for $\\gamma=0$, comparing the focal loss gradient to the standard cross-entropy gradient.\n3.  An analysis of the focal loss's emphasis on hard vs. easy examples by comparing the ratio of the focal gradient norm to the cross-entropy gradient norm. The boolean result of $S_{\\text{hard}} > S_{\\text{easy}}$ and the focusing weights will be reported.\n\nThe implementation will adhere to the specified Python environment and output format. All class indices are converted from the problem's 1-based convention to Python's 0-based convention.", "answer": "```python\nimport numpy as np\n\ndef stable_softmax(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes numerically stable softmax probabilities.\n    \"\"\"\n    z_max = np.max(z)\n    exp_z = np.exp(z - z_max)\n    return exp_z / np.sum(exp_z)\n\ndef focal_loss(z: np.ndarray, y_1based: int, gamma: float) -> float:\n    \"\"\"\n    Computes the focal loss.\n    y_1based is the 1-based true class index.\n    \"\"\"\n    y_idx = y_1based - 1  # Convert to 0-based index\n    p = stable_softmax(z)\n    p_y = p[y_idx]\n\n    # Clip p_y to avoid log(0) issues if it is exactly 0.\n    eps = 1e-12\n    p_y = np.clip(p_y, eps, 1.0 - eps)\n\n    loss = -(1 - p_y)**gamma * np.log(p_y)\n    return loss\n\ndef analytic_gradient(z: np.ndarray, y_1based: int, gamma: float) -> np.ndarray:\n    \"\"\"\n    Computes the analytic gradient of the focal loss w.r.t. z.\n    y_1based is the 1-based true class index.\n    \"\"\"\n    y_idx = y_1based - 1\n    K = len(z)\n    p = stable_softmax(z)\n    p_y = p[y_idx]\n    \n    e_y = np.zeros(K)\n    e_y[y_idx] = 1.0\n\n    if gamma == 0.0:\n        return p - e_y\n\n    eps = 1e-12\n    p_y = np.clip(p_y, eps, 1.0 - eps)\n    \n    one_minus_py = 1.0 - p_y\n    log_py = np.log(p_y)\n\n    term = gamma * p_y * (one_minus_py**(gamma - 1.0)) * log_py - (one_minus_py**gamma)\n    \n    grad = term * (e_y - p)\n    # The problem definition gives dL/dz = (p-e_y) * (-term)\n    # This is equivalent to (e_y-p)*term. My derivation is correct.\n    # dL/dz_i = (d_{iy}-p_i) * term\n    \n    return grad\n\ndef numerical_gradient(z: np.ndarray, y_1based: int, gamma: float, h: float) -> np.ndarray:\n    \"\"\"\n    Computes the numerical gradient of the focal loss using central differences.\n    \"\"\"\n    K = len(z)\n    grad_num = np.zeros(K)\n    \n    for i in range(K):\n        e_i = np.zeros(K)\n        e_i[i] = 1.0\n        \n        z_plus_h = z + h * e_i\n        z_minus_h = z - h * e_i\n        \n        loss_plus = focal_loss(z_plus_h, y_1based, gamma)\n        loss_minus = focal_loss(z_minus_h, y_1based, gamma)\n        \n        grad_num[i] = (loss_plus - loss_minus) / (2 * h)\n        \n    return grad_num\n\ndef solve():\n    \"\"\"\n    Executes all test cases and prints the final result.\n    \"\"\"\n    results = []\n\n    # Test 1: General-case gradient check\n    z1 = np.array([0.0, 1.0, -0.5, 2.0])\n    y1 = 4\n    gamma1 = 2.0\n    h1 = 1e-6\n    grad_ana1 = analytic_gradient(z1, y1, gamma1)\n    grad_num1 = numerical_gradient(z1, y1, gamma1, h=h1)\n    r_gen = np.max(np.abs(grad_ana1 - grad_num1))\n    results.append(r_gen)\n\n    # Test 2: Boundary-case focusing parameter (gamma=0, Cross-Entropy)\n    z2 = np.array([2.0, -1.0, 0.5])\n    y2 = 1\n    gamma2 = 0.0\n    grad_focal_gamma0 = analytic_gradient(z2, y2, gamma2)\n    p2 = stable_softmax(z2)\n    e_y2 = np.zeros_like(z2)\n    e_y2[y2 - 1] = 1.0\n    grad_ce2 = p2 - e_y2\n    r_ce0 = np.max(np.abs(grad_focal_gamma0 - grad_ce2))\n    results.append(r_ce0)\n\n    # Test 3: Hard-versus-easy emphasis\n    y3 = 1\n    gamma3 = 2.0\n    y3_idx = y3 - 1\n    z_easy = np.array([10.0, 0.0, 0.0, 0.0])\n    z_hard = np.array([-10.0, 0.0, 0.0, 0.0])\n    \n    # Easy example\n    p_easy = stable_softmax(z_easy)\n    py_easy = p_easy[y3_idx]\n    grad_focal_easy = analytic_gradient(z_easy, y3, gamma3)\n    e_y3 = np.zeros_like(z_easy)\n    e_y3[y3_idx] = 1.0\n    grad_ce_easy = p_easy - e_y3\n    S_easy = np.linalg.norm(grad_focal_easy, 2) / np.linalg.norm(grad_ce_easy, 2)\n    w_easy = (1 - py_easy)**gamma3\n\n    # Hard example\n    p_hard = stable_softmax(z_hard)\n    py_hard = p_hard[y3_idx]\n    grad_focal_hard = analytic_gradient(z_hard, y3, gamma3)\n    grad_ce_hard = p_hard - e_y3\n    S_hard = np.linalg.norm(grad_focal_hard, 2) / np.linalg.norm(grad_ce_hard, 2)\n    w_hard = (1 - py_hard)**gamma3\n\n    b_hard_gt_easy = S_hard > S_easy\n    weights = [w_easy, w_hard]\n    \n    results.append(b_hard_gt_easy)\n    results.append(weights)\n    \n    # Format the final output string exactly as requested.\n    # The default string representation of list and bool in Python matches the required format.\n    print(f\"[{results[0]},{results[1]},{str(results[2]).lower()},{results[3]}]\")\n\nsolve()\n```", "id": "3193212"}, {"introduction": "A model's accuracy is not the only measure of its usefulness; the reliability of its confidence scores is also critical, a property known as calibration. A well-trained but poorly calibrated model might be \"99% confident\" but correct only 70% of the time. This final practice explores a popular post-hoc calibration method that learns a simple affine transformation on the output logits to improve calibration without costly retraining [@problem_id:3193227]. You will formalize this as an optimization problem on a validation set and implement a solution, a crucial skill for deploying trustworthy AI systems.", "problem": "Consider a multi-class classification setting with $K$ classes. At inference time, you apply a post-hoc calibration to pre-trained logits by an affine transform $z' = a z + b$, where $z \\in \\mathbb{R}^K$ are the original logits for a sample, $a \\in \\mathbb{R}$ is a scalar shared across classes, and $b \\in \\mathbb{R}^K$ is a class-wise offset vector. The predicted categorical distribution is obtained by the softmax of the transformed logits. You are asked to derive the optimal parameters $(a,b)$ by minimizing the Negative Log-Likelihood (NLL) over a given validation set.\n\nUse the following fundamental bases only:\n- The definition of the softmax function: for $s \\in \\mathbb{R}^K$, $\\operatorname{softmax}(s)_c = \\dfrac{\\exp(s_c)}{\\sum_{k=1}^K \\exp(s_k)}$ for $c \\in \\{1,\\dots,K\\}$.\n- The definition of the Negative Log-Likelihood (NLL) for independent samples under a categorical model: if a sample $i$ has true class $y_i \\in \\{0,1,\\dots,K-1\\}$ and predicted probabilities $p_i \\in \\Delta^{K-1}$ (the probability simplex), then its contribution to the NLL is $-\\log(p_{i,y_i})$, and the dataset NLL is the sum over samples.\n- The independence of samples and the chain rule of differentiation.\n\nYour task is to:\n- Formalize the NLL in terms of $(a,b)$, the original logits $z$, and the softmax function.\n- Derive the necessary optimality conditions by computing gradients with respect to $a$ and $b$.\n- Implement a numerically stable procedure to find $(a,b)$ that minimizes the NLL for each test case below. You must account for the numerical stability of the softmax by using a stable computation for the log-sum-exp and softmax, particularly when logits have large magnitudes.\n- For the purpose of this problem, treat $b$ as a $K$-dimensional vector (one offset per class). Note that adding the same scalar to all components of $b$ does not change the softmax outputs; your method should nonetheless return one minimizer as produced by a consistent optimization routine.\n\nThere are no physical units involved in this problem. Angles are not applicable. Percentages, if any arise conceptually, should be represented as decimals.\n\nTest suite:\n- Test Case $1$ (happy path): $K=3$, $N=6$. Original logits and labels:\n  - $z_1 = (2.0, 0.5, -1.0)$, $y_1 = 0$.\n  - $z_2 = (0.0, 1.0, 0.0)$, $y_2 = 1$.\n  - $z_3 = (1.5, -0.5, 0.0)$, $y_3 = 0$.\n  - $z_4 = (0.2, 0.0, 2.0)$, $y_4 = 2$.\n  - $z_5 = (-1.0, 0.0, 1.0)$, $y_5 = 2$.\n  - $z_6 = (3.0, 1.0, 0.0)$, $y_6 = 0$.\n- Test Case $2$ (edge case with zero logits): $K=4$, $N=5$. Original logits and labels:\n  - $z_1 = (0.0, 0.0, 0.0, 0.0)$, $y_1 = 0$.\n  - $z_2 = (0.0, 0.0, 0.0, 0.0), $y_2 = 1$.\n  - $z_3 = (0.0, 0.0, 0.0, 0.0), $y_3 = 2$.\n  - $z_4 = (0.0, 0.0, 0.0, 0.0), $y_4 = 3$.\n  - $z_5 = (0.0, 0.0, 0.0, 0.0), $y_5 = 1$.\n- Test Case $3$ (large-magnitude logits for numerical stability): $K=3$, $N=4$. Original logits and labels:\n  - $z_1 = (1000.0, 0.0, -1000.0)$, $y_1 = 0$.\n  - $z_2 = (1200.0, 1100.0, 1000.0)$, $y_2 = 0$.\n  - $z_3 = (-800.0, -900.0, -850.0)$, $y_3 = 2$.\n  - $z_4 = (0.0, 0.0, 0.0)$, $y_4 = 1$.\n\nFinal output specification:\n- Your program should produce a single line containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a bracketed comma-separated list of the optimized parameters, with the scalar $a$ first, followed by the components of $b$ in increasing class index order.\n- All numerical values must be rounded to six decimal places.\n- For example, the output format is $[[a_1,b_{1,0},\\dots],[a_2,b_{2,0},\\dots],[a_3,b_{3,0},\\dots]]$ with each $a_j$ and $b_{j,c}$ presented as decimals to six places.", "solution": "The user wants to find the optimal parameters $(a, b)$ for an affine calibration transform $z' = az + b$ applied to logits $z \\in \\mathbb{R}^K$. The parameters $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}^K$ are determined by minimizing the Negative Log-Likelihood (NLL) on a validation dataset. This requires deriving the NLL and its gradients with respect to $a$ and $b$, and then using a numerical optimization algorithm.\n\n### Step 1: Formalize the Negative Log-Likelihood (NLL)\n\nLet the validation set be $\\{(z_i, y_i)\\}_{i=1}^N$, where $z_i \\in \\mathbb{R}^K$ are the input logits for sample $i$, and $y_i \\in \\{0, 1, \\dots, K-1\\}$ is the true class index.\n\nThe transformed logits for sample $i$ are given by the affine transformation:\n$$ z'_i = a z_i + b $$\nIn component form, for each class $c \\in \\{0, \\dots, K-1\\}$, this is $z'_{i,c} = a z_{i,c} + b_c$.\n\nThe predicted probability for class $c$ is obtained by applying the softmax function to the transformed logits:\n$$ p_{i,c} = \\operatorname{softmax}(z'_i)_c = \\frac{\\exp(z'_{i,c})}{\\sum_{k=0}^{K-1} \\exp(z'_{i,k})} = \\frac{\\exp(a z_{i,c} + b_c)}{\\sum_{k=0}^{K-1} \\exp(a z_{i,k} + b_k)} $$\n\nThe Negative Log-Likelihood for a single sample $i$ with true class $y_i$ is defined as $\\mathcal{L}_i(a,b) = -\\log(p_{i,y_i})$. Substituting the expression for the probability, we get:\n$$ \\mathcal{L}_i(a, b) = -\\log\\left(\\frac{\\exp(a z_{i,y_i} + b_{y_i})}{\\sum_{k=0}^{K-1} \\exp(a z_{i,k} + b_k)}\\right) $$\nUsing the property $\\log(X/Y) = \\log(X) - \\log(Y)$, this simplifies to:\n$$ \\mathcal{L}_i(a, b) = -\\left(a z_{i,y_i} + b_{y_i}\\right) + \\log\\left(\\sum_{k=0}^{K-1} \\exp(a z_{i,k} + b_k)\\right) $$\nThe term $\\log(\\sum_k \\exp(\\cdot))$ is the log-sum-exp function, often denoted as $\\operatorname{LSE}(\\cdot)$.\n\nThe total NLL for the entire dataset is the sum of the NLLs for each independent sample:\n$$ \\mathcal{L}(a, b) = \\sum_{i=1}^N \\mathcal{L}_i(a, b) = \\sum_{i=1}^N \\left[ \\log\\left(\\sum_{k=0}^{K-1} \\exp(a z_{i,k} + b_k)\\right) - (a z_{i,y_i} + b_{y_i}) \\right] $$\n\n### Step 2: Derive the Optimality Conditions (Gradients)\n\nTo find the optimal parameters $(a,b)$ that minimize $\\mathcal{L}(a,b)$, we must find the points where the gradients are zero. We compute the partial derivatives of $\\mathcal{L}$ with respect to $a$ and each component $b_j$ of the vector $b$.\n\n**Gradient with respect to $b_j$:**\nWe differentiate $\\mathcal{L}$ with respect to $b_j$ for $j \\in \\{0, \\dots, K-1\\}$. The derivative of a sum is the sum of derivatives, so we can focus on a single sample's contribution, $\\mathcal{L}_i$:\n$$ \\frac{\\partial \\mathcal{L}_i}{\\partial b_j} = \\frac{\\partial}{\\partial b_j}\\left( \\log\\left(\\sum_{k=0}^{K-1} \\exp(z'_{i,k})\\right) \\right) - \\frac{\\partial}{\\partial b_j}(a z_{i,y_i} + b_{y_i}) $$\nUsing the chain rule, the first term is:\n$$ \\frac{1}{\\sum_{k=0}^{K-1} \\exp(z'_{i,k})} \\cdot \\exp(z'_{i,j}) = p_{i,j} $$\nThe second term is $1$ if $j=y_i$ and $0$ otherwise. This can be written using the Kronecker delta, $\\delta_{j,y_i}$.\n$$ \\frac{\\partial \\mathcal{L}_i}{\\partial b_j} = p_{i,j} - \\delta_{j,y_i} $$\nThe total gradient with respect to $b_j$ is the sum over all samples:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b_j} = \\sum_{i=1}^N (p_{i,j} - \\delta_{j,y_i}) $$\n\n**Gradient with respect to $a$:**\nSimilarly, we differentiate $\\mathcal{L}_i$ with respect to $a$:\n$$ \\frac{\\partial \\mathcal{L}_i}{\\partial a} = \\frac{\\partial}{\\partial a}\\left( \\log\\left(\\sum_{k=0}^{K-1} \\exp(z'_{i,k})\\right) \\right) - \\frac{\\partial}{\\partial a}(a z_{i,y_i} + b_{y_i}) $$\nThe first term using the chain rule gives:\n$$ \\frac{1}{\\sum_{k=0}^{K-1} \\exp(z'_{i,k})} \\cdot \\sum_{k=0}^{K-1} \\left( \\exp(z'_{i,k}) \\cdot \\frac{\\partial z'_{i,k}}{\\partial a} \\right) = \\sum_{k=0}^{K-1} p_{i,k} \\cdot z_{i,k} $$\nThe second term is simply $z_{i,y_i}$. So, for one sample:\n$$ \\frac{\\partial \\mathcal{L}_i}{\\partial a} = \\sum_{k=0}^{K-1} p_{i,k} z_{i,k} - z_{i,y_i} $$\nThis can be written more compactly as $\\sum_{k=0}^{K-1} (p_{i,k} - \\delta_{k,y_i}) z_{i,k}$.\nThe total gradient with respect to $a$ is the sum over all samples:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial a} = \\sum_{i=1}^N \\left( \\sum_{k=0}^{K-1} p_{i,k} z_{i,k} - z_{i,y_i} \\right) = \\sum_{i=1}^N \\sum_{k=0}^{K-1} (p_{i,k} - \\delta_{k,y_i}) z_{i,k} $$\n\n### Step 3: Numerically Stable Implementation\n\nThe NLL is a convex function of $(a,b)$, so a global minimum exists. We can find it using a gradient-based optimization algorithm like L-BFGS-B, which is efficient and robust. The algorithm requires the objective function value and its gradient at each iteration.\n\nA critical aspect of the implementation is numerical stability, especially when dealing with the `exp` function on logits of large magnitude.\nTo compute $\\operatorname{LSE}(s) = \\log(\\sum_k \\exp(s_k))$ and $\\operatorname{softmax}(s)$, we use the identity:\n$$ \\operatorname{LSE}(s) = c + \\log\\left(\\sum_k \\exp(s_k - c)\\right), \\quad \\text{where } c = \\max_k(s_k) $$\nThis prevents numerical overflow by ensuring the largest argument to $\\exp$ is $0$. The same shift is used for the softmax calculation.\n\nThe optimization parameters are the scalar $a$ and the vector $b$, forming a $(K+1)$-dimensional vector. We initialize with a reasonable guess, such as $a=1$ and $b=\\mathbf{0}$, which corresponds to the uncalibrated model. The optimization algorithm then iteratively updates $(a,b)$ using the computed gradients until convergence.\n\nNote that the softmax function is invariant to a common shift in its input, i.e., $\\operatorname{softmax}(s) = \\operatorname{softmax}(s+C)$ for any scalar $C$. This implies that if $(a, b)$ is a minimizer, so is $(a, b+C\\mathbf{1})$ where $\\mathbf{1}$ is a vector of ones. The L-BFGS-B algorithm will converge to a single, consistent minimizer on this solution manifold.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves for the optimal affine calibration parameters (a, b) for multiple test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"logits\": [\n                (2.0, 0.5, -1.0), (0.0, 1.0, 0.0), (1.5, -0.5, 0.0),\n                (0.2, 0.0, 2.0), (-1.0, 0.0, 1.0), (3.0, 1.0, 0.0)\n            ],\n            \"labels\": [0, 1, 0, 2, 2, 0],\n        },\n        {\n            \"logits\": [\n                (0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0),\n                (0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0)\n            ],\n            \"labels\": [0, 1, 2, 3, 1],\n        },\n        {\n            \"logits\": [\n                (1000.0, 0.0, -1000.0), (1200.0, 1100.0, 1000.0),\n                (-800.0, -900.0, -850.0), (0.0, 0.0, 0.0)\n            ],\n            \"labels\": [0, 0, 2, 1],\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        logits = np.array(case[\"logits\"], dtype=np.float64)\n        labels = np.array(case[\"labels\"], dtype=np.int64)\n        \n        N, K = logits.shape\n\n        def objective_and_grad(params, Z, Y):\n            \"\"\"\n            Computes the Negative Log-Likelihood (NLL) and its gradient \n            with respect to the calibration parameters (a, b).\n            This function is designed to be used with scipy.optimize.minimize.\n\n            Args:\n                params (np.ndarray): A 1D array of length K+1, where params[0] is 'a'\n                                     and params[1:] is the vector 'b'.\n                Z (np.ndarray): The (N, K) matrix of original logits.\n                Y (np.ndarray): The (N,) array of true class labels.\n\n            Returns:\n                tuple: A tuple containing the NLL (float) and its gradient (np.ndarray).\n            \"\"\"\n            a = params[0]\n            b = params[1:]\n            \n            # shape (N, K): Calculate transformed logits using broadcasting\n            Z_prime = a * Z + b \n\n            # Numerically stable computation of log-sum-exp and softmax\n            max_logits = np.max(Z_prime, axis=1, keepdims=True)\n            exp_Z_prime = np.exp(Z_prime - max_logits)\n            sum_exp_Z_prime = np.sum(exp_Z_prime, axis=1, keepdims=True)\n            \n            # Probabilities P has shape (N, K)\n            P = exp_Z_prime / sum_exp_Z_prime\n\n            # Calculate the NLL objective function\n            # log(sum(exp(z'))) = max_logit + log(sum(exp(z' - max_logit)))\n            log_sum_exp = max_logits.flatten() + np.log(sum_exp_Z_prime.flatten())\n            # NLL contribution from each sample is log_sum_exp - z'_{true_class}\n            true_class_logits = Z_prime[np.arange(N), Y]\n            nll = np.sum(log_sum_exp - true_class_logits)\n\n            # Calculate gradients\n            # The core of the gradient is the difference between predicted probabilities\n            # and the one-hot encoded true labels.\n            diff = P.copy()\n            diff[np.arange(N), Y] -= 1.0\n\n            # Gradient w.r.t. 'a' is sum over all samples and classes of diff * Z\n            grad_a = np.sum(diff * Z)\n\n            # Gradient w.r.t. 'b' is sum of diffs over samples for each class\n            grad_b = np.sum(diff, axis=0)\n            \n            grad = np.concatenate(([grad_a], grad_b))\n            \n            return nll, grad\n\n        # Initial guess: a=1 (no change in scale), b=0 (no bias)\n        x0 = np.zeros(K + 1, dtype=np.float64)\n        x0[0] = 1.0\n\n        # Run the L-BFGS-B optimizer. jac=True signifies that our function\n        # returns both the objective value and the gradient vector.\n        res = minimize(objective_and_grad, \n                       x0, \n                       args=(logits, labels), \n                       method='L-BFGS-B', \n                       jac=True)\n        \n        optimal_params = res.x\n        formatted_params = [f\"{p:.6f}\" for p in optimal_params]\n        all_results.append(f\"[{','.join(formatted_params)}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3193227"}]}