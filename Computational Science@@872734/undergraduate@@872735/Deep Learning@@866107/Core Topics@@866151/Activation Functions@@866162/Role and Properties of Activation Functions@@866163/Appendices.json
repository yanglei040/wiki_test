{"hands_on_practices": [{"introduction": "The stability of training in deep neural networks critically depends on how the statistical properties of signals, like their mean and variance, propagate from one layer to the next. This exercise challenges you to derive the mathematical map that governs this signal propagation under idealized conditions. By analyzing this map for common activation functions, you will discover the concept of a fixed point and understand why some activations, like SELU, are specifically designed to prevent the vanishing or exploding gradient problems [@problem_id:3171987].", "problem": "A fully connected neural layer is defined by the preactivation $z = W x + b$ and postactivation $a = f(z)$, where $x \\in \\mathbb{R}^{n}$ is the input vector to the layer, $W \\in \\mathbb{R}^{m \\times n}$ is the weight matrix, and $b \\in \\mathbb{R}^{m}$ is the bias vector. Assume the following statistical model for forward signal propagation:\n\n1. The entries of $W$ are independent and identically distributed (i.i.d.) Gaussian random variables with $W_{ji} \\sim \\mathcal{N}\\!\\left(0, \\frac{g^{2}}{n}\\right)$.\n2. The components $\\{x_{i}\\}_{i=1}^{n}$ are i.i.d. and independent of $W$, each with mean $\\mu$ and variance $\\sigma^{2}$.\n3. The bias is set to $b = 0$.\n\nConsider three activation functions: the hyperbolic tangent $f(z) = \\tanh(z)$, the Rectified Linear Unit (ReLU) $f(z) = \\max\\{0, z\\}$, and the Scaled Exponential Linear Unit (SELU),\n$$\nf(z) =\n\\begin{cases}\n\\lambda z,  z  0, \\\\\n\\lambda \\alpha \\left(\\exp(z) - 1\\right),  z \\leq 0,\n\\end{cases}\n$$\nwith $\\lambda = 1.050700987355480$ and $\\alpha = 1.673263242354377$.\n\nStarting from the core definitions of expectation and variance, the independence assumptions, and the Gaussian approximation implied by the Central Limit Theorem for large $n$, derive the mean/variance propagation map $(\\mu, \\sigma^{2}) \\mapsto (\\mu', \\sigma'^{2})$ across the layer for a general activation $f$ by expressing $\\mu' = \\mathbb{E}[f(Z)]$ and $\\sigma'^{2} = \\mathbb{V}\\mathrm{ar}[f(Z)]$ in terms of the Gaussian law of $Z$. Specialize your derivation for each activation $f = \\tanh$, $f = \\mathrm{ReLU}$, and $f = \\mathrm{SELU}$.\n\nThen, restrict to the zero-mean case $\\mu = 0$, gain $g = 1$, and $b = 0$, so that the preactivation is approximately Gaussian with zero mean and variance equal to the input variance. Under this restriction, determine the fixed-point variances $\\sigma_{\\star}^{2}$ for each activation by solving $\\sigma'^{2} = \\sigma^{2}$ and comparing the solutions in the three cases. Report your final answer as the row matrix of the three fixed-point variances, in the order $(\\tanh, \\mathrm{ReLU}, \\mathrm{SELU})$. If any numerical evaluation is required, use the provided values of $\\lambda$ and $\\alpha$ exactly and the standard normal cumulative distribution function $\\Phi(\\cdot)$ and probability density function $\\varphi(\\cdot)$ to express results analytically without rounding. The final answer must be a single row matrix containing the three fixed-point variances, with no units.", "solution": "The problem asks for the derivation of the mean/variance propagation map for a fully connected neural layer under specific statistical assumptions and for the determination of the fixed-point variances for three activation functions: hyperbolic tangent ($\\tanh$), Rectified Linear Unit (ReLU), and Scaled Exponential Linear Unit (SELU).\n\nFirst, we characterize the distribution of the preactivation $z_j$ for a neuron $j$ in the layer. The preactivation is given by $z_j = \\sum_{i=1}^{n} W_{ji} x_i$, as the bias $b$ is set to $0$. The input components $x_i$ are i.i.d. with mean $\\mathbb{E}[x_i] = \\mu$ and variance $\\mathbb{V}\\mathrm{ar}[x_i] = \\sigma^2$. The weights $W_{ji}$ are i.i.d. Gaussian random variables, $W_{ji} \\sim \\mathcal{N}(0, g^2/n)$, and are independent of the inputs $x_i$.\n\nThe mean of the preactivation $z_j$ is:\n$$\n\\mathbb{E}[z_j] = \\mathbb{E}\\left[\\sum_{i=1}^{n} W_{ji} x_i\\right] = \\sum_{i=1}^{n} \\mathbb{E}[W_{ji} x_i]\n$$\nDue to the independence of $W_{ji}$ and $x_i$, we have $\\mathbb{E}[W_{ji} x_i] = \\mathbb{E}[W_{ji}] \\mathbb{E}[x_i]$.\n$$\n\\mathbb{E}[z_j] = \\sum_{i=1}^{n} (0) \\cdot \\mu = 0\n$$\nThe mean of the preactivation is always $0$.\n\nThe variance of the preactivation $z_j$ is:\n$$\n\\mathbb{V}\\mathrm{ar}[z_j] = \\mathbb{V}\\mathrm{ar}\\left[\\sum_{i=1}^{n} W_{ji} x_i\\right]\n$$\nSince the terms $W_{ji} x_i$ are independent for different indices $i$, the variance of the sum is the sum of the variances:\n$$\n\\mathbb{V}\\mathrm{ar}[z_j] = \\sum_{i=1}^{n} \\mathbb{V}\\mathrm{ar}[W_{ji} x_i]\n$$\nFor two independent random variables $A$ and $B$, $\\mathbb{V}\\mathrm{ar}[AB] = \\mathbb{E}[A^2]\\mathbb{E}[B^2] - (\\mathbb{E}[A]\\mathbb{E}[B])^2$.\nWe have $\\mathbb{E}[W_{ji}] = 0$ and $\\mathbb{V}\\mathrm{ar}[W_{ji}] = g^2/n$, so $\\mathbb{E}[W_{ji}^2] = \\mathbb{V}\\mathrm{ar}[W_{ji}] + (\\mathbb{E}[W_{ji}])^2 = g^2/n$.\nFor the input, $\\mathbb{E}[x_i] = \\mu$ and $\\mathbb{V}\\mathrm{ar}[x_i] = \\sigma^2$, so $\\mathbb{E}[x_i^2] = \\mathbb{V}\\mathrm{ar}[x_i] + (\\mathbb{E}[x_i])^2 = \\sigma^2 + \\mu^2$.\nThus, the variance of a single term is:\n$$\n\\mathbb{V}\\mathrm{ar}[W_{ji} x_i] = \\left(\\frac{g^2}{n}\\right)(\\sigma^2 + \\mu^2) - (0 \\cdot \\mu)^2 = \\frac{g^2}{n}(\\sigma^2 + \\mu^2)\n$$\nSumming over the $n$ terms, we get the total variance of the preactivation:\n$$\n\\sigma_z^2 = \\mathbb{V}\\mathrm{ar}[z_j] = \\sum_{i=1}^{n} \\frac{g^2}{n}(\\sigma^2 + \\mu^2) = n \\cdot \\frac{g^2}{n}(\\sigma^2 + \\mu^2) = g^2(\\sigma^2 + \\mu^2)\n$$\nFor large $n$, by the Central Limit Theorem, the preactivation $z_j$ is approximately Gaussian. We denote this random variable by $Z$:\n$$\nZ \\sim \\mathcal{N}(0, g^2(\\sigma^2 + \\mu^2))\n$$\nThe output of the activation function is $a = f(Z)$. The mean and variance of the output activations are $\\mu' = \\mathbb{E}[a]$ and $\\sigma'^2 = \\mathbb{V}\\mathrm{ar}[a]$. We are asked to find the fixed-point variances $\\sigma_\\star^2$ by solving $\\sigma'^2 = \\sigma^2$ under the specific conditions $\\mu=0$ and $g=1$.\n\nUnder these restrictions, the preactivation distribution simplifies to:\n$$\nZ \\sim \\mathcal{N}(0, \\sigma^2)\n$$\nThe variance propagation map is $\\sigma^2 \\mapsto \\sigma'^2 = \\mathbb{V}\\mathrm{ar}[f(Z)]$. We now solve $\\sigma'^2 = \\sigma^2$ for each activation function.\n\n**1. Hyperbolic Tangent: $f(z) = \\tanh(z)$**\n\nSince $f(z) = \\tanh(z)$ is an odd function and the distribution of $Z \\sim \\mathcal{N}(0, \\sigma^2)$ is symmetric about $0$, the mean of the output is $\\mu' = \\mathbb{E}[\\tanh(Z)] = 0$.\nThe variance of the output is $\\sigma'^2 = \\mathbb{E}[\\tanh^2(Z)] - (\\mu')^2 = \\mathbb{E}[\\tanh^2(Z)]$.\nThe fixed-point equation is $\\sigma^2 = \\mathbb{E}[\\tanh^2(Z)]$.\nA trivial solution is $\\sigma^2 = 0$. If $\\sigma^2 = 0$, then $Z \\equiv 0$, so $\\tanh(Z) \\equiv 0$, and $\\sigma'^2 = 0$.\nFor any $\\sigma^2  0$, we have $|\\tanh(z)|  1$ for any finite $z$, which implies $\\tanh^2(z)  1$. Thus, $\\sigma'^2 = \\mathbb{E}[\\tanh^2(Z)]  \\mathbb{E}[1] = 1$. This means any non-zero fixed point must satisfy $\\sigma^2  1$.\nTo analyze the behavior near the origin, we use the Taylor expansion of $\\tanh(z)$ around $z=0$: $\\tanh(z) = z - \\frac{z^3}{3} + O(z^5)$.\nThen $\\tanh^2(z) = \\left(z - \\frac{z^3}{3} + \\dots\\right)^2 = z^2 - \\frac{2z^4}{3} + O(z^6)$.\nTaking the expectation with respect to $Z \\sim \\mathcal{N}(0, \\sigma^2)$:\n$$\n\\sigma'^2 = \\mathbb{E}[\\tanh^2(Z)] \\approx \\mathbb{E}[Z^2] - \\frac{2}{3}\\mathbb{E}[Z^4]\n$$\nFor a zero-mean Gaussian, $\\mathbb{E}[Z^2] = \\sigma^2$ and $\\mathbb{E}[Z^4] = 3\\sigma^4$.\n$$\n\\sigma'^2 \\approx \\sigma^2 - \\frac{2}{3}(3\\sigma^4) = \\sigma^2 - 2\\sigma^4\n$$\nThe fixed-point equation $\\sigma^2 = \\sigma^2 - 2\\sigma^4$ yields $2\\sigma^4 = 0$, whose only real solution is $\\sigma=0$. For any small $\\sigma^2  0$, we have $\\sigma'^2  \\sigma^2$, indicating that the mapping is contractive towards the origin. Thus, the only fixed point is $\\sigma_{\\star, \\tanh}^2 = 0$.\n\n**2. Rectified Linear Unit: $f(z) = \\max\\{0, z\\}$**\n\nFor $Z \\sim \\mathcal{N}(0, \\sigma^2)$ with PDF $p(z) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{z^2}{2\\sigma^2})$, we first compute the output mean $\\mu'$.\n$$\n\\mu' = \\mathbb{E}[\\max\\{0, Z\\}] = \\int_{0}^{\\infty} z \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{z^2}{2\\sigma^2}\\right) dz = \\left[-\\frac{\\sigma^2}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{z^2}{2\\sigma^2}\\right)\\right]_{0}^{\\infty} = \\frac{\\sigma}{\\sqrt{2\\pi}}\n$$\nNext, we compute the second moment of the output.\n$$\n\\mathbb{E}[f(Z)^2] = \\mathbb{E}[\\max\\{0, Z\\}^2] = \\int_{0}^{\\infty} z^2 p(z) dz = \\frac{1}{2}\\int_{-\\infty}^{\\infty} z^2 p(z) dz = \\frac{1}{2}\\mathbb{E}[Z^2] = \\frac{\\sigma^2}{2}\n$$\nThe output variance is:\n$$\n\\sigma'^2 = \\mathbb{E}[f(Z)^2] - (\\mu')^2 = \\frac{\\sigma^2}{2} - \\left(\\frac{\\sigma}{\\sqrt{2\\pi}}\\right)^2 = \\frac{\\sigma^2}{2} - \\frac{\\sigma^2}{2\\pi} = \\sigma^2\\left(\\frac{1}{2} - \\frac{1}{2\\pi}\\right)\n$$\nThe fixed-point equation is $\\sigma^2 = \\sigma'^2$:\n$$\n\\sigma^2 = \\sigma^2\\left(\\frac{1}{2} - \\frac{1}{2\\pi}\\right)\n$$\nThis equation can be rewritten as $\\sigma^2\\left(1 - \\left(\\frac{1}{2} - \\frac{1}{2\\pi}\\right)\\right) = 0$, or $\\sigma^2\\left(\\frac{1}{2} + \\frac{1}{2\\pi}\\right)=0$. Since the term in the parentheses is non-zero, the only solution is $\\sigma^2=0$.\nThus, the fixed-point variance is $\\sigma_{\\star, \\mathrm{ReLU}}^2 = 0$.\n\n**3. Scaled Exponential Linear Unit (SELU)**\n\nThe SELU activation function is given by:\n$$\nf(z) =\n\\begin{cases}\n\\lambda z,  z  0, \\\\\n\\lambda \\alpha \\left(\\exp(z) - 1\\right),  z \\leq 0,\n\\end{cases}\n$$\nwith specific constants $\\lambda = 1.050700987355480$ and $\\alpha = 1.673263242354377$. These constants are not arbitrary; they are precisely chosen to ensure that the activation function yields self-normalizing properties. Specifically, if the input to a layer consists of activations with mean $0$ and variance $1$, the output activations will also have a mean of approximately $0$ and a variance of approximately $1$.\nIn our framework, this means that if the input to the activation function $Z$ is a standard normal variable, $Z \\sim \\mathcal{N}(0, 1)$, then the output $a=f(Z)$ will have $\\mathbb{E}[a] = 0$ and $\\mathbb{V}\\mathrm{ar}[a]=1$.\n\nLet's test this in our fixed-point problem. We are looking for $\\sigma^2$ such that if $Z \\sim \\mathcal{N}(0, \\sigma^2)$, then $\\sigma'^2 = \\mathbb{V}\\mathrm{ar}[f(Z)] = \\sigma^2$.\nConsider the case where the input variance is $\\sigma^2=1$. The preactivation is then $Z \\sim \\mathcal{N}(0, 1)$.\nBy the design property of SELU, the output activations $a = f(Z)$ will have mean $\\mu' = \\mathbb{E}[f(Z)] = 0$ and variance $\\sigma'^2 = \\mathbb{V}\\mathrm{ar}[f(Z)] = 1$.\nSince the input variance $\\sigma^2=1$ results in an output variance $\\sigma'^2=1$, the condition $\\sigma'^2=\\sigma^2$ is met.\nTherefore, $\\sigma^2=1$ is a fixed point of the variance propagation map for SELU. This non-trivial fixed point is crucial for maintaining a stable signal flow in deep networks.\nThus, the fixed-point variance is $\\sigma_{\\star, \\mathrm{SELU}}^2 = 1$.\n\nIn summary, the fixed-point variances for the three activation functions under the given conditions are:\n- For tanh: $\\sigma_{\\star}^2 = 0$\n- For ReLU: $\\sigma_{\\star}^2 = 0$\n- For SELU: $\\sigma_{\\star}^2 = 1$\n\nThe final answer is the row matrix of these three values in the specified order.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  0  1\n\\end{pmatrix}\n}\n$$", "id": "3171987"}, {"introduction": "An activation function's mathematical form directly defines the \"shape\" of the building blocks a neural network uses to approximate a target function. This hands-on coding exercise provides a striking demonstration of this principle by constructing a scenario where a network with Rectified Linear Unit (ReLU) activations is forced into a purely linear regime, rendering it incapable of learning a simple sine wave. By comparing its performance to a network using the smooth, non-linear hyperbolic tangent (tanh) activation, you will gain a concrete understanding of how the choice of activation governs a model's expressive power [@problem_id:3171992].", "problem": "You will study how the choice of activation function shapes the hypothesis class and thereby affects approximation error for a target function with high curvature. Work in one dimension. Consider the Rectified Linear Unit (ReLU) activation $g_{\\mathrm{ReLU}}(z)=\\max(0,z)$ and the hyperbolic tangent activation $g_{\\tanh}(z)=\\tanh(z)$. Let the target be $f^\\ast(x)=\\sin(\\omega x)$ with angles in radians. You will construct a dataset and two families of single-hidden-layer models of equal width $m$, then compare their empirical mean squared errors (MSE) when fit by least squares.\n\nBase definitions to use:\n- A single-hidden-layer model with fixed hidden parameters $\\{a_i,b_i\\}_{i=1}^m$ and activation $g$ has the form $h(x)=c+\\sum_{i=1}^m w_i\\,g(a_i x+b_i)$, where $c$ and $\\{w_i\\}$ are the trainable output layer parameters. For a dataset $\\{(x_j,y_j)\\}_{j=1}^N$, the empirical mean squared error is $\\frac{1}{N}\\sum_{j=1}^N\\left(h(x_j)-y_j\\right)^2$.\n- Fitting by least squares over the output layer means choosing $(c,\\{w_i\\})$ to minimize the empirical mean squared error, equivalently solving a linear least-squares problem over the feature matrix formed by $g(a_i x_j+b_i)$ augmented with a bias feature.\n\nConstruct the dataset and models as follows.\n1) Dataset: Let $N=1001$ and $x_j$ be $N$ equally spaced points in $[-1,1]$. Let $y_j=f^\\ast(x_j)=\\sin(\\omega x_j)$, with $\\omega$ specified per test case and interpreted in radians.\n2) ReLU model family $H_{\\mathrm{ReLU}}$: Fix width $m$. Choose slopes $a_i$ as $m$ equally spaced values in $[-\\alpha,\\alpha]$ with $\\alpha=0.5$. Choose a shared bias $b_i^{\\mathrm{ReLU}}=1.0$ for all hidden units. On $x\\in[-1,1]$, these choices guarantee $a_i x + b_i^{\\mathrm{ReLU}}\\ge 0.50$ for all $i$, so $g_{\\mathrm{ReLU}}(a_i x+b_i^{\\mathrm{ReLU}})=a_i x+b_i^{\\mathrm{ReLU}}$. Consequently, any $h\\in H_{\\mathrm{ReLU}}$ reduces to an affine function of $x$.\n3) Tanh model family $H_{\\tanh}$: Fix width $m$. Choose slopes $a_i$ as $m$ equally spaced values in $[-A,A]$ with $A=\\max(1.0,0.7\\,\\omega)$ and set $b_i^{\\tanh}=0$ for all hidden units. This places the hidden pre-activations near the high-slope region of $g_{\\tanh}$ so that $H_{\\tanh}$ spans a richer, smooth, nonlinear family.\n\nFor each model family, construct a feature matrix $\\Phi\\in\\mathbb{R}^{N\\times(m+1)}$ whose first column is the bias feature $\\phi_0(x)\\equiv 1$, and whose remaining $m$ columns are $\\phi_i(x)=g(a_i x+b_i)$ evaluated at the $N$ inputs. Compute the least-squares solution $\\hat{\\theta}=\\arg\\min_{\\theta}\\|\\Phi\\theta - y\\|_2^2$ and report the empirical mean squared error $E=\\frac{1}{N}\\|\\Phi\\hat{\\theta}-y\\|_2^2$.\n\nAngle unit requirement: All trigonometric evaluations use radians. The output must not use a percentage sign; any rates must be in decimal form.\n\nTest suite and required outputs:\n- Use the following parameter sets $(\\omega,m)$: $(0.0,5)$, $(3.0,5)$, $(9.0,5)$, $(9.0,2)$, $(15.0,10)$.\n- For each parameter set, compute three floats: $E_{\\mathrm{ReLU}}$, $E_{\\tanh}$, and the difference $E_{\\mathrm{ReLU}}-E_{\\tanh}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the triple for one test case, itself formatted as a comma-separated list enclosed in square brackets. For example: $[[e_{1,\\mathrm{ReLU}},e_{1,\\tanh},d_1],[e_{2,\\mathrm{ReLU}},e_{2,\\tanh},d_2],\\ldots]$. Round all floats to $6$ decimal places in the final output string.\n\nDeliverable: Provide a complete, runnable program that constructs the dataset, builds both feature matrices per test, solves the least-squares problems, computes the requested errors, and prints exactly one line in the required format. No user input or external files are allowed.", "solution": "The user-provided problem is a valid, well-posed, and scientifically grounded exercise in computational machine learning. It asks for a comparison of the approximation capabilities of two single-hidden-layer neural network models, one using the Rectified Linear Unit (ReLU) activation function and the other using the hyperbolic tangent (tanh) activation function. The analysis hinges on how the choice of fixed hidden-layer parameters affects the richness of the function class (hypothesis space) that each model can represent.\n\nThe solution proceeds by first formalizing the problem as a linear least-squares task. Subsequently, we construct the feature matrices corresponding to each model family for the given test parameters. The least-squares solution is then computed to find the best-fit model in each family, and their respective mean squared errors are calculated and compared.\n\n**1. Problem Formulation as Linear Least-Squares**\n\nA single-hidden-layer model is defined as $h(x) = c + \\sum_{i=1}^m w_i\\,g(a_i x+b_i)$. The hidden layer parameters, slopes $\\{a_i\\}_{i=1}^m$ and biases $\\{b_i\\}_{i=1}^m$, are fixed according to the problem specification for each model family. The trainable parameters are the output-layer bias $c$ and weights $\\{w_i\\}_{i=1}^m$.\n\nFor a dataset of $N$ points $\\{(x_j, y_j)\\}_{j=1}^N$, we can express the model's predictions as a matrix-vector product. Let $\\theta = [c, w_1, \\ldots, w_m]^T$ be the column vector of trainable parameters. We define a feature matrix $\\Phi \\in \\mathbb{R}^{N \\times (m+1)}$ where each row corresponds to an input point $x_j$ and each column corresponds to a basis function. The first column is a constant bias term, $\\phi_0(x_j) = 1$ for all $j$. The subsequent $m$ columns are the outputs of the hidden neurons, $\\phi_i(x_j) = g(a_i x_j + b_i)$ for $i=1, \\ldots, m$.\n\nThe vector of model predictions for all $x_j$ is given by $\\hat{y} = \\Phi\\theta$. The objective is to find the parameter vector $\\theta$ that minimizes the empirical mean squared error (MSE), which is proportional to the squared Euclidean norm of the residual vector $r = y - \\hat{y}$:\n$$\nE = \\frac{1}{N} \\sum_{j=1}^N (h(x_j) - y_j)^2 = \\frac{1}{N} \\| \\Phi\\theta - y \\|_2^2\n$$\nThis is a standard linear least-squares problem. The optimal parameter vector $\\hat{\\theta}$ that minimizes this error is given by the solution to the normal equations:\n$$\n(\\Phi^T \\Phi) \\hat{\\theta} = \\Phi^T y\n$$\nNumerically, it is more stable to solve for $\\hat{\\theta}$ using methods like QR decomposition or Singular Value Decomposition (SVD), which are implemented in standard numerical libraries. Once $\\hat{\\theta}$ is found, the minimum MSE is calculated as $E = \\frac{1}{N} \\| \\Phi\\hat{\\theta} - y \\|_2^2$.\n\n**2. Analysis of the ReLU Model Family ($H_{\\mathrm{ReLU}}$)**\n\nThe ReLU model family is defined with the activation function $g_{\\mathrm{ReLU}}(z) = \\max(0, z)$. The hidden parameters are chosen as follows: width $m$, slopes $a_i$ as $m$ equally spaced values in $[-\\alpha, \\alpha]$ with $\\alpha=0.5$, and a shared bias $b_i^{\\mathrm{ReLU}} = 1.0$ for all units.\n\nA critical insight is the behavior of the pre-activation term $z_i(x) = a_i x + b_i^{\\mathrm{ReLU}}$ for inputs $x \\in [-1, 1]$. Since $|a_i| \\le 0.5$ and $|x| \\le 1$, the term $a_i x$ is bounded: $|a_i x| \\le 0.5$. The pre-activation is therefore bounded as:\n$$\nz_i(x) = a_i x + 1.0 \\ge -0.5 + 1.0 = 0.5\n$$\nSince $z_i(x) \\ge 0.5$ for all $i$ and all $x$ in the domain, the ReLU activation function always operates in its linear region, i.e., $g_{\\mathrm{ReLU}}(z_i(x)) = z_i(x) = a_i x + 1.0$.\n\nConsequently, the model function $h(x) \\in H_{\\mathrm{ReLU}}$ simplifies to:\n$$\nh(x) = c + \\sum_{i=1}^m w_i (a_i x + 1.0) = \\left(c + \\sum_{i=1}^m w_i\\right) + \\left(\\sum_{i=1}^m w_i a_i\\right) x\n$$\nThis is an affine function of the form $C_0 + C_1 x$. This means that despite being formulated as a neural network, this specific construction of the ReLU model can only represent linear functions. Its ability to approximate a non-linear target function like $f^\\ast(x) = \\sin(\\omega x)$ is fundamentally limited to the best possible linear approximation. For $m \\ge 2$, the hypothesis space does not expand with increasing width $m$, as the basis functions $\\{1, a_1 x+1, \\ldots, a_m x+1\\}$ all lie within the span of $\\{1, x\\}$.\n\n**3. Analysis of the Tanh Model Family ($H_{\\tanh}$)**\n\nThe Tanh model family uses the activation $g_{\\tanh}(z) = \\tanh(z)$. The hidden parameters are: width $m$, slopes $a_i$ as $m$ equally spaced values in $[-A, A]$ with $A = \\max(1.0, 0.7\\,\\omega)$, and biases $b_i^{\\tanh} = 0$.\n\nThe model function is $h(x) = c + \\sum_{i=1}^m w_i \\tanh(a_i x)$. Unlike the ReLU case, the $\\tanh$ function is non-linear across its entire domain (except at $z=0$). The basis functions $\\phi_i(x) = \\tanh(a_i x)$ are smooth, sigmoidal functions. The range of slopes $[-A, A]$ is adapted to the frequency $\\omega$ of the target function, ensuring that the basis functions have varying degrees of steepness appropriate for capturing the oscillations of $\\sin(\\omega x)$. A linear combination of these non-linear basis functions can form a rich class of smooth, non-linear approximations, which is far more powerful than the affine class of $H_{\\mathrm{ReLU}}$.\n\n**4. Computational Algorithm**\n\nFor each parameter set $(\\omega, m)$ provided in the test suite, we execute the following steps:\n1.  **Generate Dataset**: Create a vector of $N=1001$ input points $x$ uniformly spaced in $[-1, 1]$ and the corresponding target vector $y = \\sin(\\omega x)$.\n2.  **Compute $E_{\\mathrm{ReLU}}$**:\n    -   Determine the ReLU slopes $a_i$ and bias $b_i^{\\mathrm{ReLU}}$.\n    -   Construct the $N \\times (m+1)$ feature matrix $\\Phi_{\\mathrm{ReLU}}$ with columns $[1, g_{\\mathrm{ReLU}}(a_1 x + b_1^{\\mathrm{ReLU}}), \\ldots, g_{\\mathrm{ReLU}}(a_m x + b_m^{\\mathrm{ReLU}})]$.\n    -   Solve the least-squares problem for $\\hat{\\theta}_{\\mathrm{ReLU}} = \\arg\\min_{\\theta} \\|\\Phi_{\\mathrm{ReLU}}\\theta - y\\|_2^2$.\n    -   Calculate the error $E_{\\mathrm{ReLU}} = \\frac{1}{N} \\| \\Phi_{\\mathrm{ReLU}}\\hat{\\theta}_{\\mathrm{ReLU}} - y \\|_2^2$.\n3.  **Compute $E_{\\tanh}$**:\n    -   Determine the Tanh slopes $a_i$ (based on $\\omega$) and bias $b_i^{\\tanh}$.\n    -   Construct the $N \\times (m+1)$ feature matrix $\\Phi_{\\tanh}$ with columns $[1, g_{\\tanh}(a_1 x + b_1^{\\tanh}), \\ldots, g_{\\tanh}(a_m x + b_m^{\\tanh})]$.\n    -   Solve for $\\hat{\\theta}_{\\tanh} = \\arg\\min_{\\theta} \\|\\Phi_{\\tanh}\\theta - y\\|_2^2$.\n    -   Calculate the error $E_{\\tanh} = \\frac{1}{N} \\| \\Phi_{\\tanh}\\hat{\\theta}_{\\tanh} - y \\|_2^2$.\n4.  **Store Results**: The triplet $(E_{\\mathrm{ReLU}}, E_{\\tanh}, E_{\\mathrm{ReLU}} - E_{\\tanh})$ is computed and stored.\n\nThis procedure is repeated for all test cases, and the final results are formatted into a single string as specified. The expected outcome is that for any non-trivial target ($\\omega > 0$), $E_{\\mathrm{ReLU}}$ will be significantly larger than $E_{\\tanh}$, demonstrating the critical role of non-linearity in the activation function for approximating complex functions.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the approximation errors of two single-hidden-layer\n    neural network models (ReLU and Tanh) on a sinusoidal target function.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 5),\n        (3.0, 5),\n        (9.0, 5),\n        (9.0, 2),\n        (15.0, 10),\n    ]\n\n    all_results = []\n    \n    # Constants\n    N = 1001\n\n    for omega, m in test_cases:\n        # 1. Construct the dataset\n        x = np.linspace(-1.0, 1.0, N)\n        y = np.sin(omega * x)\n\n        # 2. ReLU Model Family Calculation\n        alpha = 0.5\n        a_relu = np.linspace(-alpha, alpha, m)\n        b_relu = 1.0\n\n        # Construct ReLU feature matrix Phi_relu\n        Phi_relu = np.ones((N, m + 1))\n        # Vectorized computation of hidden unit outputs\n        # x is (N,), a_relu is (m,). We need an (N, m) matrix.\n        # x[:, np.newaxis] is (N, 1), a_relu[np.newaxis, :] is (1, m)\n        # Broadcasting (N, 1) * (1, m) - (N, m)\n        pre_acts_relu = x[:, np.newaxis] * a_relu[np.newaxis, :] + b_relu\n        # Per problem statement logic, pre_acts_relu is always  0,\n        # so np.maximum(0, a) is equivalent to a. We use the formal definition.\n        Phi_relu[:, 1:] = np.maximum(0, pre_acts_relu)\n\n        # Solve least squares and compute MSE\n        try:\n            theta_relu, _, _, _ = np.linalg.lstsq(Phi_relu, y, rcond=None)\n            y_pred_relu = Phi_relu @ theta_relu\n            E_relu = np.mean((y - y_pred_relu)**2)\n        except np.linalg.LinAlgError:\n            # Should not happen with np.linalg.lstsq, but as a safeguard\n            E_relu = np.nan\n\n        # 3. Tanh Model Family Calculation\n        A = max(1.0, 0.7 * omega)\n        a_tanh = np.linspace(-A, A, m)\n        b_tanh = 0.0\n\n        # Construct Tanh feature matrix Phi_tanh\n        Phi_tanh = np.ones((N, m + 1))\n        pre_acts_tanh = x[:, np.newaxis] * a_tanh[np.newaxis, :] + b_tanh\n        Phi_tanh[:, 1:] = np.tanh(pre_acts_tanh)\n\n        # Solve least squares and compute MSE\n        try:\n            theta_tanh, _, _, _ = np.linalg.lstsq(Phi_tanh, y, rcond=None)\n            y_pred_tanh = Phi_tanh @ theta_tanh\n            E_tanh = np.mean((y - y_pred_tanh)**2)\n        except np.linalg.LinAlgError:\n            E_tanh = np.nan\n            \n        # 4. Difference\n        diff = E_relu - E_tanh\n        \n        all_results.append((E_relu, E_tanh, diff))\n\n    # 5. Format the final output string\n    output_parts = []\n    for e_relu, e_tanh, d in all_results:\n        part = f\"[{e_relu:.6f},{e_tanh:.6f},{d:.6f}]\"\n        output_parts.append(part)\n    \n    final_output_string = f\"[{','.join(output_parts)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "3171992"}, {"introduction": "While most neural networks use activation functions with fixed shapes, it is possible to design activations with trainable parameters that can adapt during learning. This advanced exercise invites you to explore this frontier by working with a novel activation function whose slope and saturation level are learnable. You will derive a unique, curvature-based regularizer designed to prevent the function from collapsing into a simple linear identity, thereby preserving the essential non-linearity that gives neural networks their power [@problem_id:3171903].", "problem": "In supervised deep learning using Empirical Risk Minimization (ERM), the training objective typically augments a data-fit loss with a regularization term to control model capacity and bias toward desired inductive properties. Activation functions govern nonlinearity, gradient flow, and representational saturation; to study the role and properties of activation functions with trainable shape, consider a scalar activation $f_{\\theta}$ parameterized by slope and saturation. Let the input to the activation be supported on the bounded interval $[-L,L]$ for a fixed, known $L0$. You are to propose an activation with trainable slope and saturation, then derive a regularizer, grounded in calculus-based curvature, that prevents the activation from collapsing to the identity $f(x)=x$ (which would eliminate nonlinearity).\n\nPropose the activation\n$$\nf_{\\theta}(x) = s\\,\\tanh\\!\\left(\\frac{a x}{s}\\right),\n$$\nwith trainable parameters $\\theta=(a,s)$ satisfying $a0$ and $s0$. This choice has a tunable slope at the origin and saturates smoothly to $\\pm s$ for large $|x|$. To discourage collapse toward a linear map (and specifically toward the identity), define a curvature-based measure\n$$\n\\mathcal{C}(\\theta) \\equiv \\int_{-L}^{L} \\left(f_{\\theta}''(x)\\right)^{2}\\,dx,\n$$\nand let the regularizer be strictly decreasing in $\\mathcal{C}(\\theta)$ so that minimizing the ERM objective incentivizes nonlinearity. Using only core definitions from calculus and properties of $\\tanh$, derive the closed-form analytical expression for\n$$\nR(\\theta) \\equiv \\frac{1}{\\mathcal{C}(\\theta)}\n$$\nas a function of $a$, $s$, and $L$.\n\nYour final answer must be a single closed-form expression for $R(\\theta)$ in terms of $a$, $s$, and $L$. No numerical approximation is required.", "solution": "The problem requires the derivation of a closed-form analytical expression for the regularizer $R(\\theta)$, defined as the reciprocal of a curvature-based measure $\\mathcal{C}(\\theta)$. The problem is well-posed and scientifically grounded in calculus and the theory of neural network activators. All terms are clearly defined, and the premises are consistent. We may proceed with the solution.\n\nThe activation function is given by $f_{\\theta}(x) = s\\,\\tanh\\left(\\frac{a x}{s}\\right)$, where $\\theta=(a,s)$ are trainable parameters with $a0$ and $s0$. The input $x$ is defined on the interval $[-L, L]$ for a known constant $L0$.\nThe curvature measure is $\\mathcal{C}(\\theta) \\equiv \\int_{-L}^{L} \\left(f_{\\theta}''(x)\\right)^{2}\\,dx$.\nThe regularizer is $R(\\theta) \\equiv \\frac{1}{\\mathcal{C}(\\theta)}$.\n\nOur first step is to compute the second derivative of the activation function, $f_{\\theta}''(x)$. We begin with the first derivative, $f_{\\theta}'(x)$, using the chain rule.\nLet the argument of the hyperbolic tangent be $u(x) = \\frac{ax}{s}$. The derivative of $\\tanh(u)$ with respect to $u$ is $\\text{sech}^{2}(u)$.\nThe first derivative $f_{\\theta}'(x)$ is therefore:\n$$\nf_{\\theta}'(x) = \\frac{d}{dx} \\left[s\\,\\tanh\\left(\\frac{a x}{s}\\right)\\right] = s \\cdot \\text{sech}^{2}\\left(\\frac{a x}{s}\\right) \\cdot \\frac{d}{dx}\\left(\\frac{a x}{s}\\right) = s \\cdot \\text{sech}^{2}\\left(\\frac{a x}{s}\\right) \\cdot \\frac{a}{s} = a\\,\\text{sech}^{2}\\left(\\frac{a x}{s}\\right)\n$$\nNext, we compute the second derivative, $f_{\\theta}''(x)$, by differentiating $f_{\\theta}'(x)$. We apply the chain rule again, using the identity $\\frac{d}{du}\\text{sech}(u) = -\\text{sech}(u)\\tanh(u)$.\n$$\nf_{\\theta}''(x) = \\frac{d}{dx} \\left[a\\,\\text{sech}^{2}\\left(\\frac{a x}{s}\\right)\\right] = a \\cdot 2\\,\\text{sech}\\left(\\frac{a x}{s}\\right) \\cdot \\frac{d}{dx}\\left(\\text{sech}\\left(\\frac{a x}{s}\\right)\\right)\n$$\nThe derivative of the inner sech term is:\n$$\n\\frac{d}{dx}\\left(\\text{sech}\\left(\\frac{a x}{s}\\right)\\right) = -\\text{sech}\\left(\\frac{a x}{s}\\right)\\tanh\\left(\\frac{a x}{s}\\right) \\cdot \\frac{a}{s}\n$$\nSubstituting this back into the expression for $f_{\\theta}''(x)$:\n$$\nf_{\\theta}''(x) = 2a\\,\\text{sech}\\left(\\frac{a x}{s}\\right) \\cdot \\left[-\\text{sech}\\left(\\frac{a x}{s}\\right)\\tanh\\left(\\frac{a x}{s}\\right) \\cdot \\frac{a}{s}\\right] = -\\frac{2a^{2}}{s}\\,\\text{sech}^{2}\\left(\\frac{a x}{s}\\right)\\tanh\\left(\\frac{a x}{s}\\right)\n$$\nNow, we square this expression to form the integrand required for $\\mathcal{C}(\\theta)$:\n$$\n\\left(f_{\\theta}''(x)\\right)^{2} = \\left(-\\frac{2a^{2}}{s}\\,\\text{sech}^{2}\\left(\\frac{a x}{s}\\right)\\tanh\\left(\\frac{a x}{s}\\right)\\right)^{2} = \\frac{4a^{4}}{s^{2}}\\,\\text{sech}^{4}\\left(\\frac{a x}{s}\\right)\\tanh^{2}\\left(\\frac{a x}{s}\\right)\n$$\nThe curvature measure $\\mathcal{C}(\\theta)$ is the integral of this non-negative function over the interval $[-L, L]$:\n$$\n\\mathcal{C}(\\theta) = \\int_{-L}^{L} \\frac{4a^{4}}{s^{2}}\\,\\text{sech}^{4}\\left(\\frac{a x}{s}\\right)\\tanh^{2}\\left(\\frac{a x}{s}\\right) \\,dx\n$$\nThe integrand is an even function of $x$, since $\\text{sech}(z)$ is an even function and $\\tanh(z)$ is an odd function, which makes the product $\\text{sech}^{4}(z)\\tanh^{2}(z)$ an even function. Therefore, the integral over a symmetric interval $[-L, L]$ is twice the integral over $[0, L]$:\n$$\n\\mathcal{C}(\\theta) = 2 \\int_{0}^{L} \\frac{4a^{4}}{s^{2}}\\,\\text{sech}^{4}\\left(\\frac{a x}{s}\\right)\\tanh^{2}\\left(\\frac{a x}{s}\\right) \\,dx = \\frac{8a^{4}}{s^{2}} \\int_{0}^{L} \\text{sech}^{4}\\left(\\frac{a x}{s}\\right)\\tanh^{2}\\left(\\frac{a x}{s}\\right) \\,dx\n$$\nTo solve this integral, we perform a substitution. Let $u = \\frac{ax}{s}$. This implies $du = \\frac{a}{s}\\,dx$, so $dx = \\frac{s}{a}\\,du$. The limits of integration for $u$ are from $0$ to $\\frac{aL}{s}$.\n$$\n\\mathcal{C}(\\theta) = \\frac{8a^{4}}{s^{2}} \\int_{0}^{aL/s} \\text{sech}^{4}(u)\\tanh^{2}(u) \\cdot \\left(\\frac{s}{a}\\right) \\,du = \\frac{8a^{3}}{s} \\int_{0}^{aL/s} \\text{sech}^{4}(u)\\tanh^{2}(u) \\,du\n$$\nTo evaluate the remaining integral, we use a second substitution. Let $v = \\tanh(u)$. Then $dv = \\text{sech}^{2}(u)\\,du$. We rewrite the integrand using the identity $\\text{sech}^{2}(u) = 1 - \\tanh^{2}(u) = 1 - v^{2}$:\n$$\n\\int \\text{sech}^{4}(u)\\tanh^{2}(u) \\,du = \\int \\text{sech}^{2}(u)\\tanh^{2}(u) \\cdot \\left(\\text{sech}^{2}(u) \\,du\\right) = \\int (1-v^{2})v^{2} \\,dv = \\int (v^{2} - v^{4}) \\,dv\n$$\nThe antiderivative is $\\frac{v^{3}}{3} - \\frac{v^{5}}{5}$. Substituting back $v = \\tanh(u)$, we obtain the antiderivative in terms of $u$: $\\frac{1}{3}\\tanh^{3}(u) - \\frac{1}{5}\\tanh^{5}(u)$.\n\nNow, we evaluate the definite integral from $0$ to $\\frac{aL}{s}$:\n$$\n\\int_{0}^{aL/s} \\text{sech}^{4}(u)\\tanh^{2}(u) \\,du = \\left[\\frac{1}{3}\\tanh^{3}(u) - \\frac{1}{5}\\tanh^{5}(u)\\right]_{0}^{aL/s}\n$$\nAt the lower limit $u=0$, $\\tanh(0)=0$, so the expression evaluates to $0$. At the upper limit $u=\\frac{aL}{s}$, the value is $\\frac{1}{3}\\tanh^{3}\\left(\\frac{aL}{s}\\right) - \\frac{1}{5}\\tanh^{5}\\left(\\frac{aL}{s}\\right)$.\nThus, the definite integral is $\\frac{1}{3}\\tanh^{3}\\left(\\frac{aL}{s}\\right) - \\frac{1}{5}\\tanh^{5}\\left(\\frac{aL}{s}\\right)$.\n\nWe substitute this result back into the expression for $\\mathcal{C}(\\theta)$:\n$$\n\\mathcal{C}(\\theta) = \\frac{8a^{3}}{s} \\left[ \\frac{1}{3}\\tanh^{3}\\left(\\frac{aL}{s}\\right) - \\frac{1}{5}\\tanh^{5}\\left(\\frac{aL}{s}\\right) \\right]\n$$\nCombining the terms in the bracket with a common denominator of $15$ yields:\n$$\n\\mathcal{C}(\\theta) = \\frac{8a^{3}}{s} \\left[ \\frac{5\\tanh^{3}\\left(\\frac{aL}{s}\\right) - 3\\tanh^{5}\\left(\\frac{aL}{s}\\right)}{15} \\right] = \\frac{8a^{3}}{15s} \\left[ 5\\tanh^{3}\\left(\\frac{aL}{s}\\right) - 3\\tanh^{5}\\left(\\frac{aL}{s}\\right) \\right]\n$$\nWe can factor out the term $\\tanh^{3}\\left(\\frac{aL}{s}\\right)$:\n$$\n\\mathcal{C}(\\theta) = \\frac{8a^{3}}{15s} \\tanh^{3}\\left(\\frac{aL}{s}\\right) \\left[ 5 - 3\\tanh^{2}\\left(\\frac{aL}{s}\\right) \\right]\n$$\nFinally, the regularizer $R(\\theta) \\equiv \\frac{1}{\\mathcal{C}(\\theta)}$ is the reciprocal of this expression:\n$$\nR(\\theta) = \\frac{1}{\\frac{8a^{3}}{15s} \\tanh^{3}\\left(\\frac{aL}{s}\\right) \\left[ 5 - 3\\tanh^{2}\\left(\\frac{aL}{s}\\right) \\right]} = \\frac{15s}{8a^{3} \\tanh^{3}\\left(\\frac{aL}{s}\\right) \\left( 5 - 3\\tanh^{2}\\left(\\frac{aL}{s}\\right) \\right)}\n$$\nThis represents the final closed-form analytical expression for $R(\\theta)$.", "answer": "$$\n\\boxed{\\frac{15s}{8a^{3} \\tanh^{3}\\left(\\frac{aL}{s}\\right)\\left(5 - 3\\tanh^{2}\\left(\\frac{aL}{s}\\right)\\right)}}\n$$", "id": "3171903"}]}