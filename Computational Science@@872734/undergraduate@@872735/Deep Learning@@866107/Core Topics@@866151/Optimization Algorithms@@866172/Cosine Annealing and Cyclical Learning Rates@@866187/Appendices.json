{"hands_on_practices": [{"introduction": "This first exercise gets to the heart of why cyclical learning rates are so effective for training deep neural networks. By exploring a simple, one-dimensional non-convex function, you will see firsthand how a learning rate schedule that periodically \"restarts\" to a high value can help an optimizer escape from a shallow local minimum and find a much better solution. This hands-on comparison with a traditional, monotonically decreasing learning rate will build your intuition for the exploratory power of Stochastic Gradient Descent with Restarts (SGDR). [@problem_id:3110220]", "problem": "You are given a synthetic one-dimensional nonconvex loss defined by the quartic polynomial $f(\\theta)=\\theta^4-2a\\theta^2+b\\theta$ for real parameters $a$ and $b$. Consider deterministic gradient descent with parameter updates governed by the rule $\\theta_{t+1}=\\theta_t-\\eta_t\\nabla f(\\theta_t)$, where the scalar learning rate at step $t$ is $\\eta_t > 0$ and the gradient is $\\nabla f(\\theta)=\\frac{d}{d\\theta}f(\\theta)$. The aim is to examine, from first principles, how the choice of learning rate schedule affects the ability of gradient descent to make progress on a nonconvex landscape that can exhibit both shallow and deep minima.\n\nYour tasks:\n\n1. Starting from the definition of the loss and elementary calculus, derive the analytical gradient $\\nabla f(\\theta)$ needed for deterministic gradient descent.\n\n2. Starting from the definition of a learning rate sequence $\\{\\eta_t\\}_{t=0}^{T-1}$ over a period of length $T$, and the constraints that it be smooth, have zero slope at the boundaries (to avoid abrupt changes), attain a maximum $\\eta_{\\max}$ at the start of each period, and attain a minimum $\\eta_{\\min}$ at the end of each period, derive a learning rate schedule over one period that satisfies these constraints and then define a warm-restart mechanism that repeats this schedule every $T$ steps. Angles for any trigonometric functions must be expressed in radians.\n\n3. Define a contrasting monotone learning rate schedule $\\eta_t$ that strictly decreases over time according to a well-tested rule. Use this to perform deterministic gradient descent without restarts.\n\n4. Implement both schedules and run deterministic gradient descent for a fixed number of epochs. For each test case in the suite below, compute the final losses obtained under the warm-restart schedule and under the monotone schedule. For each case, return a boolean that is true if and only if the final loss under warm restarts is strictly smaller than the final loss under monotone decay by at least a margin $\\varepsilon$ (use $\\varepsilon=10^{-2}$), and false otherwise. This boolean is intended to operationalize the statement that warm restarts can enable escape from shallow minima compared to monotone decay, in the sense of achieving a strictly lower final objective value after the same number of steps.\n\nUse the following test suite. Each test case is a tuple specifying $(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)$, where $a$ and $b$ parameterize the loss $f(\\theta)$, $\\theta_0$ is the initial parameter value, $E$ is the number of gradient descent steps (epochs), $(\\eta_{\\max},\\eta_{\\min},T)$ specify the warm-restart schedule’s maximum, minimum, and period, and $(\\eta_0,\\gamma)$ specify the monotone schedule’s initial learning rate and multiplicative decay factor:\n\n- Case 1 (general nonconvex, warm restarts have room to help): $(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(1.0,0.01,1.30,120,0.20,0.0005,30,0.005,0.90)$.\n- Case 2 (boundary-like: one long cycle approximates monotone): $(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(1.0,0.01,1.30,120,0.050,0.049,120,0.050,0.99)$.\n- Case 3 (nearly convex regime with a strong linear term): $(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(0.25,1.20,0.00,100,0.10,0.001,25,0.10,0.97)$.\n- Case 4 (frequent restarts): $(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(1.0,0.05,1.30,100,0.20,0.0005,10,0.010,0.95)$.\n\nAngle unit requirement: any use of trigonometric functions must use radians.\n\nOutput specification: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each result is the boolean for the corresponding test case in the order listed above.", "solution": "We begin with the synthetic nonconvex loss $f(\\theta)=\\theta^4-2a\\theta^2+b\\theta$. From the definition of the derivative and elementary calculus, the analytical gradient is obtained by differentiating term-by-term:\n$$\n\\nabla f(\\theta)=\\frac{d}{d\\theta}\\left(\\theta^4-2a\\theta^2+b\\theta\\right)=4\\theta^3-4a\\theta+b.\n$$\nDeterministic gradient descent updates the parameter using the rule:\n$$\n\\theta_{t+1}=\\theta_t-\\eta_t\\nabla f(\\theta_t)=\\theta_t-\\eta_t\\left(4\\theta_t^3-4a\\theta_t+b\\right),\n$$\nwhere $\\eta_t$ is the learning rate at iteration $t$. This rule follows directly from the steepest descent principle: move in the direction of the negative gradient scaled by a positive step size.\n\nWe next derive a warm-restart learning rate schedule over a period of length $T$ that satisfies smoothness and boundary constraints. We require a function $g:[0,T]\\to\\mathbb{R}$ such that:\n1. $g(0)=\\eta_{\\max}$,\n2. $g(T)=\\eta_{\\min}$,\n3. $g'(0)=0$ and $g'(T)=0$ for boundary smoothness,\n4. $g$ is sufficiently smooth within $(0,T)$,\n5. The schedule restarts every $T$ steps, so that the effective $\\eta_t=g(t\\bmod T)$.\n\nA minimal smooth function achieving these constraints can be constructed from a shifted and scaled cosine over a half period. Consider the cosine function, which is smooth, even, and has zero slope at its extrema. On the interval $[0,T]$, the function $\\cos\\left(\\pi t/T\\right)$ has $\\cos(0)=1$, $\\cos(\\pi)=-1$, and zero slope at both ends. Scaling and shifting to match the boundary values yields:\n$$\ng(t)=\\eta_{\\min}+\\frac{1}{2}(\\eta_{\\max}-\\eta_{\\min})\\left(1+\\cos\\left(\\pi\\frac{t}{T}\\right)\\right).\n$$\nThis satisfies all stated conditions, with angles in radians as required. To incorporate warm restarts (Stochastic Gradient Descent with Restarts (SGDR) in this deterministic setting), we repeat $g$ every $T$ steps by setting:\n$$\n\\eta_t=\\eta_{\\min}+\\frac{1}{2}(\\eta_{\\max}-\\eta_{\\min})\\left(1+\\cos\\left(\\pi\\frac{t\\bmod T}{T}\\right)\\right).\n$$\nEach restart resets the learning rate to $\\eta_{\\max}$ and then smoothly anneals to $\\eta_{\\min}$ over one period, encouraging periodic exploration and escape from shallow regions by intermittently increasing the step size.\n\nFor contrast, we define a monotone learning rate schedule that decreases over time according to a well-tested exponential rule:\n$$\n\\eta_t=\\eta_0\\gamma^t,\n$$\nwith $\\eta_0 > 0$ and $0  \\gamma  1$, providing a strictly decreasing sequence. This monotone decay is widely used in practice and reduces step sizes over time, which can help with convergence but may impede escaping shallow minima due to diminishing step sizes.\n\nAlgorithmic design:\n- For each test case, compute the final loss under two regimes: cosine annealing with warm restarts and monotone exponential decay.\n- Initialize $\\theta_0$ and iterate $E$ steps of deterministic gradient descent using the respective schedules. At each step $t$, compute the gradient $\\nabla f(\\theta_t)=4\\theta_t^3-4a\\theta_t+b$ and update $\\theta_{t+1}=\\theta_t-\\eta_t\\nabla f(\\theta_t)$.\n- After $E$ steps, compute $f(\\theta_E)$ for both schedules. Return a boolean indicating whether the final loss under warm restarts is strictly lower than the final loss under monotone decay by at least $\\varepsilon=10^{-2}$:\n$$\n\\text{result}=\\left(f_{\\text{warm}}+ \\varepsilon  f_{\\text{mono}}\\right).\n$$\n\nPrinciple-based interpretation:\n- In a nonconvex landscape like the quartic $f$, shallow minima and plateaus can cause small gradients. Under monotone decay, step sizes shrink continuously, which can trap the iterates near suboptimal shallow regions because the product $\\eta_t\\lVert\\nabla f(\\theta_t)\\rVert$ becomes too small to make meaningful progress.\n- The cosine annealing with warm restarts periodically increases the learning rate to $\\eta_{\\max}$, producing larger steps that can move the iterate out of shallow valleys or plateaus. Because the schedule is smooth and has zero slope at period boundaries, it avoids sudden jumps that may destabilize training while still injecting sufficient exploratory capacity at restarts.\n\nTest suite rationale:\n- Case 1 provides a nonconvex setting with parameters that make shallow regions discernible; warm restarts are expected to outperform monotone decay, yielding true.\n- Case 2 uses a single long period with near-constant learning rate, approximating monotone behavior; the advantage of restarts diminishes, often yielding false.\n- Case 3 has a strong linear term, making the loss closer to convex-like behavior (effectively one dominant basin), where both schedules tend to perform similarly, yielding false.\n- Case 4 uses frequent restarts to induce substantial periodic exploration, which can improve final loss relative to monotone decay, potentially yielding true.\n\nThe final output is a single line containing a list of four booleans in the order of the test cases, formatted as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f(theta, a, b):\n    # Quartic nonconvex loss f(theta) = theta^4 - 2 a theta^2 + b theta\n    return theta**4 - 2.0*a*theta**2 + b*theta\n\ndef grad_f(theta, a, b):\n    # Analytical gradient: df/dtheta = 4 theta^3 - 4 a theta + b\n    return 4.0*theta**3 - 4.0*a*theta + b\n\ndef cosine_annealing_lr(t, eta_max, eta_min, T):\n    # Cosine schedule over one period T with warm restarts.\n    # Angle in radians; uses a half-cosine anneal from eta_max to eta_min.\n    # t_mod cycles every T steps.\n    t_mod = t % T\n    return eta_min + 0.5*(eta_max - eta_min)*(1.0 + np.cos(np.pi * (t_mod / T)))\n\ndef monotone_exp_lr(t, eta0, gamma):\n    # Monotone exponential decay: eta_t = eta0 * gamma^t\n    return eta0 * (gamma ** t)\n\ndef run_descent(a, b, theta0, E, lr_schedule_fn, lr_params):\n    theta = theta0\n    for t in range(E):\n        eta_t = lr_schedule_fn(t, *lr_params)\n        g = grad_f(theta, a, b)\n        theta = theta - eta_t * g\n    return f(theta, a, b), theta\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (a, b, theta0, E, eta_max, eta_min, T, eta0, gamma)\n    test_cases = [\n        (1.0, 0.01, 1.30, 120, 0.20, 0.0005, 30, 0.005, 0.90),   # Case 1\n        (1.0, 0.01, 1.30, 120, 0.050, 0.049, 120, 0.050, 0.99),  # Case 2\n        (0.25, 1.20, 0.00, 100, 0.10, 0.001, 25, 0.10, 0.97),    # Case 3\n        (1.0, 0.05, 1.30, 100, 0.20, 0.0005, 10, 0.010, 0.95),   # Case 4\n    ]\n\n    epsilon = 1e-2  # Margin for declaring warm restarts strictly better\n\n    results = []\n    for case in test_cases:\n        a, b, theta0, E, eta_max, eta_min, T, eta0, gamma = case\n\n        # Warm restarts via cosine annealing with restarts\n        warm_loss, warm_theta = run_descent(\n            a, b, theta0, E,\n            lr_schedule_fn=cosine_annealing_lr,\n            lr_params=(eta_max, eta_min, T)\n        )\n\n        # Monotone exponential decay\n        mono_loss, mono_theta = run_descent(\n            a, b, theta0, E,\n            lr_schedule_fn=monotone_exp_lr,\n            lr_params=(eta0, gamma)\n        )\n\n        # Boolean result: True if warm_loss + epsilon  mono_loss\n        result = (warm_loss + epsilon)  mono_loss\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3110220"}, {"introduction": "Real-world training is noisy, and cyclical learning rate schedules must be robust in this stochastic environment. This practice moves from a deterministic setting to a more realistic simulation of Stochastic Gradient Descent (SGD), where you will investigate the interplay between cyclical schedules, gradient noise, and gradient clipping. By measuring convergence speed under different conditions, you will gain practical insights into how the shape of the learning rate cycle (e.g., smooth cosine vs. sharp triangular) and clipping thresholds can impact training efficiency. [@problem_id:3110142]", "problem": "You are to write a complete, runnable program that simulates stochastic gradient descent with cyclical learning rates and gradient noise clipping on a one-dimensional convex quadratic loss. Your goal is to quantify how clipping of gradient noise interacts with high learning rate phases and the convergence speed under different cyclical learning rate schedules. All angles must be treated in radians.\n\nFundamental base and setting:\n- Consider the convex quadratic objective $f(x) = \\tfrac{1}{2} x^2$. Its gradient is $\\nabla f(x) = x$.\n- At discrete iteration $t \\in \\{1,2,\\dots\\}$, define a stochastic gradient of the form $g_t = \\nabla f(x_{t-1}) + \\xi_t$, where $\\xi_t$ is additive zero-mean Gaussian noise with variance $\\sigma^2$, i.e., $\\xi_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n- Introduce gradient noise clipping as follows: the noise term is clipped component-wise in one dimension by $c \\ge 0$, so that the effective noise is $\\tilde{\\xi}_t = \\mathrm{clip}(\\xi_t, -c, c)$. Equivalently, $\\tilde{\\xi}_t = \\max(-c, \\min(c, \\xi_t))$. The stochastic gradient becomes $g_t = x_{t-1} + \\tilde{\\xi}_t$. No clipping of the deterministic gradient is performed, only the noise term is clipped. The convention for no clipping is $c = +\\infty$.\n- The update rule is $x_t = x_{t-1} - \\eta_t g_t$, where $\\eta_t$ is a cyclical learning rate that depends on $t$.\n\nCyclical learning rate schedules:\n- You must implement two cyclical schedules with restarts of period $P \\in \\mathbb{N}$:\n  1) A triangular restart schedule that, within each cycle of $P$ iterations, decreases linearly from a maximum learning rate $\\eta_{\\max}$ at the cycle start to a minimum learning rate $\\eta_{\\min}$ at the cycle end, then instantaneously restarts to $\\eta_{\\max}$ at the next cycle start. This is a piecewise linear schedule; do not use any trigonometric functions for it.\n  2) A smooth restart schedule that, within each cycle of $P$ iterations, decreases smoothly from $\\eta_{\\max}$ at the cycle start to $\\eta_{\\min}$ at the cycle end, with zero slope at both endpoints. Derive a suitable smooth periodic mapping on the unit interval that satisfies these endpoint and symmetry properties, and express it using a cosine function with angles in radians. Do not use any degrees; all trigonometric arguments must be in radians.\n- Both schedules are periodic with restarts, and within each cycle of length $P$ they traverse from $\\eta_{\\max}$ to $\\eta_{\\min}$ exactly once before restarting.\n\nMeasurement of convergence speed:\n- For a given parameter set, define a convergence threshold $\\tau > 0$ on the loss. Let the first hitting time be the smallest iteration index $t \\in \\{1,2,\\dots\\}$ such that $f(x_t) \\le \\tau$. If this never occurs within a specified iteration budget $T_{\\max}$, define the first hitting time to be $T_{\\max}$.\n- Because $g_t$ is stochastic, estimate a robust representative convergence speed by running $R$ independent replicates and taking the median of their first hitting times. Each replicate must be driven by an independent random seed that is deterministically derived from a base seed and the replicate index so results are reproducible. Round the median to the nearest integer to report an integer number of iterations.\n\nSimulation protocol:\n- Initialize $x_0 = x_{\\text{init}}$ deterministically.\n- For each replicate $r \\in \\{0,1,\\dots,R-1\\}$ and each iteration $t \\in \\{1,2,\\dots,T_{\\max}\\}$:\n  1) Compute the learning rate $\\eta_t$ from the chosen cyclical schedule using period $P$, $\\eta_{\\min}$, and $\\eta_{\\max}$.\n  2) Draw noise $\\xi_t \\sim \\mathcal{N}(0,\\sigma^2)$ using a pseudorandom generator seeded deterministically from a specified base seed, the case index, and $r$. Clip it to $\\tilde{\\xi}_t = \\mathrm{clip}(\\xi_t, -c, c)$, with the convention that $c = +\\infty$ disables clipping.\n  3) Form the stochastic gradient $g_t = x_{t-1} + \\tilde{\\xi}_t$ and update $x_t = x_{t-1} - \\eta_t g_t$.\n  4) After the update, check if $f(x_t) \\le \\tau$; if so, record the current $t$ for this replicate and stop the replicate. If no such $t$ occurs by $T_{\\max}$, record $T_{\\max}$.\n\nAngle unit requirement:\n- All uses of cosine must interpret the argument in radians.\n\nTest suite:\nUse the following four parameter cases, each requiring two measurements: one with the smooth cosine-based restart schedule and one with the triangular restart schedule. In all cases, use $R = 200$ replicates, base seed $s_{\\text{base}} = 12345$, and $x_{\\text{init}} = 5.0$.\n- Case A (baseline with moderate clipping): $\\eta_{\\min} = 0.05$, $\\eta_{\\max} = 0.8$, $P = 40$, $\\sigma = 0.5$, $c = 0.2$, $\\tau = 0.01$, $T_{\\max} = 1000$.\n- Case B (clipping eliminates noise): $\\eta_{\\min} = 0.05$, $\\eta_{\\max} = 0.8$, $P = 40$, $\\sigma = 0.5$, $c = 0$, $\\tau = 0.01$, $T_{\\max} = 1000$.\n- Case C (no clipping): $\\eta_{\\min} = 0.05$, $\\eta_{\\max} = 0.8$, $P = 40$, $\\sigma = 0.5$, $c = +\\infty$, $\\tau = 0.01$, $T_{\\max} = 1000$.\n- Case D (high maximum learning rate): $\\eta_{\\min} = 0.1$, $\\eta_{\\max} = 1.8$, $P = 40$, $\\sigma = 0.5$, $c = 0.2$, $\\tau = 0.01$, $T_{\\max} = 1000$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the eight integer results in a single flat list, in the order\n$[$A-cosine, A-triangular, B-cosine, B-triangular, C-cosine, C-triangular, D-cosine, D-triangular$]$,\nwith entries separated by commas and no extra spaces, enclosed in square brackets. For example, a syntactically correct output would look like $[12,10,8,8,14,13,20,18]$ (these numbers are illustrative only).", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It describes a standard simulation task in the field of machine learning optimization. We proceed to the solution.\n\nThe objective is to analyze the convergence behavior of stochastic gradient descent (SGD) on a simple one-dimensional convex quadratic function, $f(x) = \\tfrac{1}{2} x^2$. The gradient of this function is $\\nabla f(x) = x$. The SGD algorithm iteratively updates a position estimate $x_t$ starting from an initial point $x_0$. The update rule at iteration $t \\in \\{1, 2, \\dots\\}$ is given by:\n$$ x_t = x_{t-1} - \\eta_t g_t $$\nwhere $\\eta_t$ is the learning rate at iteration $t$, and $g_t$ is the stochastic gradient. The stochastic gradient is modeled as the true gradient at the previous position $x_{t-1}$ plus an additive noise term:\n$$ g_t = \\nabla f(x_{t-1}) + \\tilde{\\xi}_t = x_{t-1} + \\tilde{\\xi}_t $$\nThe noise term $\\tilde{\\xi}_t$ is derived from a zero-mean Gaussian random variable $\\xi_t \\sim \\mathcal{N}(0, \\sigma^2)$ which is then clipped. The clipping operation is defined by a non-negative threshold $c \\ge 0$:\n$$ \\tilde{\\xi}_t = \\mathrm{clip}(\\xi_t, -c, c) = \\max(-c, \\min(c, \\xi_t)) $$\nA value of $c = +\\infty$ corresponds to no clipping, while $c = 0$ removes the noise entirely.\n\nThe learning rate $\\eta_t$ follows a cyclical schedule with restarts after a period of $P$ iterations. We are tasked to implement two such schedules, both transitioning from a maximum value $\\eta_{\\max}$ to a minimum value $\\eta_{\\min}$ within each cycle. Let $i_t = (t-1) \\pmod P$ be the zero-indexed step within a cycle for the global iteration $t \\ge 1$. The fractional completion of a cycle is $\\alpha_t = \\frac{i_t}{P-1}$ for $P > 1$.\n\n1.  **Triangular Restart Schedule**: This schedule implements a linear decrease in the learning rate from $\\eta_{\\max}$ to $\\eta_{\\min}$ over the course of one cycle. The formula for this linear interpolation is:\n    $$ \\eta_t = (1 - \\alpha_t)\\eta_{\\max} + \\alpha_t\\eta_{\\min} = \\eta_{\\max} - \\left( \\frac{(t-1) \\pmod P}{P-1} \\right) (\\eta_{\\max} - \\eta_{\\min}) $$\n    At the beginning of a cycle ($t$ such that $(t-1) \\pmod P = 0$), $\\alpha_t=0$ and $\\eta_t = \\eta_{\\max}$. At the end of a cycle ($t$ such that $(t-1) \\pmod P = P-1$), $\\alpha_t=1$ and $\\eta_t = \\eta_{\\min}$.\n\n2.  **Smooth Restart (Cosine Annealing) Schedule**: This schedule ensures a smooth transition with zero slope at the cycle's start and end points. A suitable mapping from the fractional progress $\\alpha_t \\in [0, 1]$ to a normalized range $[1, 0]$ with these properties is the half-period cosine function $g(\\alpha) = \\frac{1}{2}(1 + \\cos(\\pi \\alpha))$. The final learning rate is obtained by scaling and shifting this function to the range $[\\eta_{\\min}, \\eta_{\\max}]$:\n    $$ \\eta_t = \\eta_{\\min} + (\\eta_{\\max} - \\eta_{\\min}) g(\\alpha_t) = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max} - \\eta_{\\min}) \\left(1 + \\cos\\left(\\pi \\frac{(t-1) \\pmod P}{P-1}\\right)\\right) $$\n    The argument to the cosine function is in radians, as required.\n\nThe convergence speed is measured by the first hitting time, defined as the smallest iteration count $t$ for which the loss $f(x_t) = \\frac{1}{2} x_t^2$ falls below a given threshold $\\tau > 0$. If convergence is not achieved within a budget of $T_{\\max}$ iterations, the hitting time is recorded as $T_{\\max}$. To obtain a robust estimate, this process is repeated for $R$ independent replicates, and the median of the resulting first hitting times is taken as the final measure. The median is then rounded to the nearest integer.\n\nFor reproducibility and a fair comparison between the two schedules, the simulation protocol employs a deterministic seeding strategy. For each of the four test cases (A, B, C, D) and for each replicate $r \\in \\{0, 1, \\dots, R-1\\}$, a unique seed is generated based on a base seed $s_{\\text{base}}$, the case index, and the replicate index $r$. The same seed is used to initialize the random number generator for both the cosine and triangular schedule simulations within that replicate, ensuring they are exposed to the identical sequence of stochastic noise draws $\\xi_t$. This paired experimental design allows for a direct comparison of the schedules' performance under the same stochastic conditions.\n\nThe program will implement this simulation across the four specified parameter cases, calculating the median first hitting time for both learning rate schedules in each case, and will format the eight resulting integer values into a single list as required.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Simulates SGD with cyclical learning rates to find median convergence time.\n    \"\"\"\n    # Define test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'eta_min': 0.05, 'eta_max': 0.8, 'P': 40, 'sigma': 0.5, 'c': 0.2, 'tau': 0.01, 'T_max': 1000},\n        # Case B\n        {'eta_min': 0.05, 'eta_max': 0.8, 'P': 40, 'sigma': 0.5, 'c': 0.0, 'tau': 0.01, 'T_max': 1000},\n        # Case C\n        {'eta_min': 0.05, 'eta_max': 0.8, 'P': 40, 'sigma': 0.5, 'c': np.inf, 'tau': 0.01, 'T_max': 1000},\n        # Case D\n        {'eta_min': 0.1, 'eta_max': 1.8, 'P': 40, 'sigma': 0.5, 'c': 0.2, 'tau': 0.01, 'T_max': 1000},\n    ]\n\n    # Global simulation parameters\n    R = 200\n    base_seed = 12345\n    x_init = 5.0\n    \n    final_results = []\n\n    for case_idx, params in enumerate(test_cases):\n        eta_min = params['eta_min']\n        eta_max = params['eta_max']\n        P = params['P']\n        sigma = params['sigma']\n        c = params['c']\n        tau = params['tau']\n        T_max = params['T_max']\n\n        hitting_times_cosine = []\n        hitting_times_triangular = []\n        \n        for r in range(R):\n            # Generate a unique, deterministic seed for each replicate of each case\n            seed = base_seed + case_idx * R + r\n            \n            # --- Paired Simulation for Cosine Schedule ---\n            rng = np.random.default_rng(seed)\n            x = x_init\n            h_time_cosine = T_max\n            for t in range(1, T_max + 1):\n                # Calculate learning rate using cosine annealing schedule\n                i_cycle = (t - 1) % P\n                alpha_t = i_cycle / (P - 1) if P > 1 else 1.0\n                eta = eta_min + 0.5 * (eta_max - eta_min) * (1 + math.cos(math.pi * alpha_t))\n                \n                # Generate and clip noise\n                noise = rng.normal(0, sigma)\n                clipped_noise = np.clip(noise, -c, c)\n                \n                # Perform SGD update\n                stochastic_grad = x + clipped_noise\n                x = x - eta * stochastic_grad\n                \n                # Check for convergence\n                loss = 0.5 * x**2\n                if loss = tau:\n                    h_time_cosine = t\n                    break\n            hitting_times_cosine.append(h_time_cosine)\n\n            # --- Paired Simulation for Triangular Schedule ---\n            # Re-seed the generator to get the same noise sequence for this replicate\n            rng = np.random.default_rng(seed)\n            x = x_init\n            h_time_triangular = T_max\n            for t in range(1, T_max + 1):\n                # Calculate learning rate using triangular schedule\n                i_cycle = (t - 1) % P\n                alpha_t = i_cycle / (P - 1) if P > 1 else 1.0\n                eta = eta_max - alpha_t * (eta_max - eta_min)\n                \n                # Generate and clip noise\n                noise = rng.normal(0, sigma)\n                clipped_noise = np.clip(noise, -c, c)\n                \n                # Perform SGD update\n                stochastic_grad = x + clipped_noise\n                x = x - eta * stochastic_grad\n                \n                # Check for convergence\n                loss = 0.5 * x**2\n                if loss = tau:\n                    h_time_triangular = t\n                    break\n            hitting_times_triangular.append(h_time_triangular)\n            \n        # Calculate and round median hitting times\n        median_cosine = np.median(hitting_times_cosine)\n        median_triangular = np.median(hitting_times_triangular)\n\n        final_results.append(int(np.round(median_cosine)))\n        final_results.append(int(np.round(median_triangular)))\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3110142"}, {"introduction": "While fixed cyclical schedules are powerful, the most advanced methods adapt the learning rate to the geometry of the loss landscape as it changes during training. This final exercise challenges you to build an intelligent controller that dynamically adjusts the maximum learning rate of each cycle based on local curvature information. You will implement the power iteration method to estimate the Hessian's largest eigenvalue and use this to enforce a stability constraint, ensuring that even the highest learning rates in your schedule remain theoretically sound and effective. [@problem_id:3110195]", "problem": "You are asked to implement a complete, runnable program that constructs a controller for a cyclical learning rate schedule using Cosine Annealing (CA) within the context of Gradient Descent (GD) in deep learning. The controller must adjust the maximum learning rate for each cycle based on a curvature estimate obtained via power iteration on a symmetric positive semidefinite matrix, and must enforce the constraint that the maximum learning rate is no larger than the reciprocal stability threshold derived from the largest curvature estimate. The implementation must follow standard mathematical and algorithmic principles without relying on shortcut formulas supplied in the problem statement.\n\nThe fundamental base you must use is the following widely accepted facts:\n- For a local quadratic approximation of a smooth loss function near a point, the loss can be written as $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} \\mathbf{H} \\mathbf{x}$ where $\\mathbf{H}$ is a symmetric positive semidefinite matrix representing curvature.\n- The Gradient Descent (GD) iteration is $\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\eta \\nabla f(\\mathbf{x}_{k})$ and, in the quadratic case, it reduces to a linear system driven by the matrix $\\mathbf{I} - \\eta \\mathbf{H}$.\n- Stability of GD on a quadratic objective is controlled by the spectral radius of the update matrix, which is bounded by the largest eigenvalue of $\\mathbf{H}$.\n- The largest eigenvalue of a symmetric matrix can be approximated from first principles via power iteration that leverages the Rayleigh quotient, without needing explicit eigendecomposition.\n\nYou must implement the following components in your program:\n- A power iteration routine that, given a symmetric positive semidefinite matrix $\\mathbf{H}$ and an iteration budget $K$, returns an estimate $\\hat{\\lambda}_{\\max}$ of the largest eigenvalue of $\\mathbf{H}$.\n- A controller that, at the start of each cycle, sets the maximum learning rate $\\eta_{\\max}$ to $\\min(\\eta_{\\text{target}}, \\tfrac{2}{\\hat{\\lambda}_{\\max}})$, where $\\eta_{\\text{target}}$ is a user-specified target amplitude and $\\tfrac{2}{\\hat{\\lambda}_{\\max}}$ is the stability threshold derived from the quadratic GD dynamics. When $\\hat{\\lambda}_{\\max} \\le 0$, interpret $\\tfrac{2}{\\hat{\\lambda}_{\\max}}$ as $+\\infty$.\n- A cyclical schedule per cycle based on Cosine Annealing (CA), with angles measured in radians. In each cycle of length $T$ steps, the learning rate must start at $\\eta_{\\max}$ and end at $\\eta_{\\min}$, where $\\eta_{\\min}$ is a user-specified lower bound. For the boundary case $T = 1$, the schedule must contain a single value equal to $\\eta_{\\max}$.\n- A verification routine that checks, for every cycle, that all learning rate values $\\eta_t$ for that cycle satisfy $\\eta_{\\min} \\le \\eta_t \\le \\eta_{\\max}$ and $\\eta_t \\le \\tfrac{2}{\\hat{\\lambda}_{\\max}}$, and that the schedule starts at $\\eta_{\\max}$ and ends at $\\eta_{\\min}$ when $T > 1$, or is constant at $\\eta_{\\max}$ when $T = 1$.\n\nAngle units must be in radians. No physical quantities with units are involved.\n\nTest Suite:\nImplement and evaluate your controller on the following test cases. Each test case specifies the curvature matrix $\\mathbf{H}$, the cycle configuration, and controller parameters. For reproducibility, use independent random seeds for power iteration per cycle as given. All matrices are symmetric, and all lengths and counts are integers.\n\n- Test Case $1$ (happy path, moderate curvature):\n  - $\\mathbf{H} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  0.5 \\end{bmatrix}$,\n  - $\\eta_{\\min} = 0.0$,\n  - $\\eta_{\\text{target}} = 0.9$,\n  - number of cycles $C = 2$,\n  - cycle length $T = 10$,\n  - power iteration steps $K = 50$,\n  - base seed $s = 101$.\n\n- Test Case $2$ (high curvature requiring capping):\n  - $\\mathbf{H} = \\begin{bmatrix} 5.0  0.0 \\\\ 0.0  3.0 \\end{bmatrix}$,\n  - $\\eta_{\\min} = 0.05$,\n  - $\\eta_{\\text{target}} = 0.9$,\n  - number of cycles $C = 3$,\n  - cycle length $T = 8$,\n  - power iteration steps $K = 30$,\n  - base seed $s = 202$.\n\n- Test Case $3$ (near-zero curvature, large stability threshold):\n  - $\\mathbf{H} = 0.001 \\cdot \\mathbf{I}_{3}$,\n  - $\\eta_{\\min} = 0.0$,\n  - $\\eta_{\\text{target}} = 1.5$,\n  - number of cycles $C = 2$,\n  - cycle length $T = 4$,\n  - power iteration steps $K = 20$,\n  - base seed $s = 303$.\n\n- Test Case $4$ (ill-conditioned curvature):\n  - $\\mathbf{H} = \\begin{bmatrix} 10.0  0.0 \\\\ 0.0  0.01 \\end{bmatrix}$,\n  - $\\eta_{\\min} = 0.0$,\n  - $\\eta_{\\text{target}} = 0.25$,\n  - number of cycles $C = 3$,\n  - cycle length $T = 7$,\n  - power iteration steps $K = 25$,\n  - base seed $s = 404$.\n\n- Test Case $5$ (boundary case $T = 1$):\n  - $\\mathbf{H} = \\begin{bmatrix} 2.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n  - $\\eta_{\\min} = 0.1$,\n  - $\\eta_{\\text{target}} = 1.0$,\n  - number of cycles $C = 1$,\n  - cycle length $T = 1$,\n  - power iteration steps $K = 40$,\n  - base seed $s = 505$.\n\nYour program must produce a single line of output containing the verification results for the five test cases as a comma-separated list enclosed in square brackets, with each item being a boolean indicating whether all constraints were satisfied for that test case across all cycles, for example, $\\left[\\text{True},\\text{False},\\dots\\right]$. Angles must be in radians throughout. No user input is allowed; the program must be fully self-contained and runnable as is.", "solution": "We start from the local quadratic approximation of a smooth deep learning loss near a point, which is a well-tested fact. Let the loss be $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} \\mathbf{H} \\mathbf{x}$, where $\\mathbf{H}$ is a symmetric positive semidefinite matrix capturing curvature. The Gradient Descent (GD) iteration with learning rate $\\eta$ is\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\eta \\nabla f(\\mathbf{x}_{k}) = \\mathbf{x}_{k} - \\eta \\mathbf{H} \\mathbf{x}_{k} = \\left(\\mathbf{I} - \\eta \\mathbf{H}\\right)\\mathbf{x}_{k}.\n$$\nConvergence and stability of this linear system depends on the spectral radius $\\rho\\left(\\mathbf{I} - \\eta \\mathbf{H}\\right)$. Because $\\mathbf{H}$ is symmetric, it is diagonalizable with real, nonnegative eigenvalues $\\lambda_{1}, \\dots, \\lambda_{n}$. The eigenvalues of $\\mathbf{I} - \\eta \\mathbf{H}$ are $1 - \\eta \\lambda_{i}$. Stability requires\n$$\n\\max_{i} \\left|1 - \\eta \\lambda_{i}\\right|  1,\n$$\nwhich implies for every $i$,\n$$\n-1  1 - \\eta \\lambda_{i}  1 \\quad \\Rightarrow \\quad 0  \\eta \\lambda_{i}  2.\n$$\nTo satisfy this simultaneously for all eigenvalues, it suffices to enforce\n$$\n0  \\eta  \\frac{2}{\\lambda_{\\max}},\n$$\nwhere $\\lambda_{\\max} = \\max_{i} \\lambda_{i}$. This inequality gives a principled upper bound on the stable learning rate in the quadratic regime.\n\nWe do not directly compute $\\lambda_{\\max}$ via explicit eigendecomposition; instead, we estimate it using the power iteration method, a first-principles algorithm. Let $\\mathbf{v}_{0}$ be a nonzero initial vector. The power iteration updates are\n$$\n\\mathbf{w}_{k} = \\mathbf{H} \\mathbf{v}_{k}, \\quad \\mathbf{v}_{k+1} = \\frac{\\mathbf{w}_{k}}{\\lVert \\mathbf{w}_{k} \\rVert}.\n$$\nUnder standard conditions for symmetric matrices with a unique largest eigenvalue in magnitude, $\\mathbf{v}_{k}$ converges to the eigenvector associated with $\\lambda_{\\max}$, and the Rayleigh quotient\n$$\n\\hat{\\lambda}_{\\max}^{(k)} = \\mathbf{v}_{k}^{\\top} \\mathbf{H} \\mathbf{v}_{k}\n$$\nconverges to $\\lambda_{\\max}$. If $\\mathbf{H}$ is the zero matrix, then $\\mathbf{H}\\mathbf{v}_{k} = \\mathbf{0}$ and the Rayleigh quotient is $0$, which we interpret as yielding an infinite stability threshold $\\frac{2}{\\hat{\\lambda}_{\\max}} = +\\infty$.\n\nWe now design the cyclical learning rate controller. In each cycle, before constructing the learning rate schedule, we:\n- Run $K$ steps of power iteration on $\\mathbf{H}$ to obtain $\\hat{\\lambda}_{\\max}$.\n- Compute the stability cap $c = \\frac{2}{\\hat{\\lambda}_{\\max}}$ if $\\hat{\\lambda}_{\\max} > 0$, and $c = +\\infty$ otherwise.\n- Set the maximum learning rate in the cycle to\n$$\n\\eta_{\\max} = \\min\\left(\\eta_{\\text{target}}, c\\right).\n$$\n- Use Cosine Annealing (CA) with angles in radians to construct a schedule of length $T$ that starts at $\\eta_{\\max}$ and ends at $\\eta_{\\min}$. The CA schedule is the standard practice in Cyclical Learning Rate (CLR) designs and is derived by mapping discrete step indices $t \\in \\{0, 1, \\dots, T-1\\}$ to points on a cosine curve in $[0, \\pi]$. This mapping ensures a smooth transition from $\\eta_{\\max}$ to $\\eta_{\\min}$. For the boundary case $T = 1$, the schedule must be the single value $\\eta_{\\max}$, which is the limiting behavior as the cosine path degenerates to a point.\n\nWe verify, for each cycle, the following properties:\n- Bounds: $\\eta_{\\min} \\le \\eta_{t} \\le \\eta_{\\max}$ for all steps $t$.\n- Stability: $\\eta_{t} \\le \\frac{2}{\\hat{\\lambda}_{\\max}}$ for all steps $t$ in that cycle.\n- Endpoints: If $T > 1$, the first value equals $\\eta_{\\max}$ and the last value equals $\\eta_{\\min}$. If $T = 1$, the single value equals $\\eta_{\\max}$.\n\nFinally, we aggregate the verification results for the entire test suite into a single list of booleans. Each boolean indicates whether all cycles of the corresponding test case satisfy the constraints. Angles are consistently in radians.\n\nAlgorithmic steps implemented in code:\n- Construct the given test matrices $\\mathbf{H}$ and parameters $(\\eta_{\\min}, \\eta_{\\text{target}}, C, T, K, s)$ for each test case.\n- For each cycle $c \\in \\{0, \\dots, C-1\\}$, run power iteration with a reproducible random seed derived from $s + c$, compute $\\eta_{\\max}$ via the controller, and build the CA schedule.\n- Check constraints and accumulate a pass/fail boolean per test case.\n- Print the list of five booleans as a single line in the specified format.\n\nThis design aligns with the fundamental stability condition of GD in the quadratic regime, leverages a principled eigenvalue estimation, and implements a well-established CLR mechanism using CA with radians while correctly handling edge cases such as $T = 1$ and near-zero curvature.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_iteration(H: np.ndarray, num_iters: int, seed: int) - float:\n    \"\"\"\n    Estimate the largest eigenvalue of a symmetric matrix H using power iteration.\n    Uses the Rayleigh quotient on the normalized iterate.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    n = H.shape[0]\n    v = rng.normal(size=(n,))\n    # Handle potential zero vector initialization, enforce nonzero\n    if np.linalg.norm(v) == 0.0:\n        v = np.ones(n)\n    v = v / np.linalg.norm(v)\n\n    lambda_hat = 0.0\n    for _ in range(num_iters):\n        w = H @ v\n        norm_w = np.linalg.norm(w)\n        if norm_w == 0.0:\n            # H might be (near) zero; lambda_hat remains 0\n            lambda_hat = 0.0\n            break\n        v = w / norm_w\n        # Rayleigh quotient on the new direction\n        lambda_hat = float(v.T @ (H @ v))\n    return lambda_hat\n\ndef cosine_cycle_schedule(eta_min: float, eta_max: float, T: int) - np.ndarray:\n    \"\"\"\n    Construct a cosine annealing schedule over T steps in radians,\n    starting at eta_max and ending at eta_min. For T == 1, return [eta_max].\n    \"\"\"\n    if T = 0:\n        raise ValueError(\"Cycle length T must be a positive integer.\")\n    if T == 1:\n        return np.array([eta_max], dtype=float)\n    t = np.arange(T, dtype=float)\n    # Cosine in radians: angle = pi * t / (T - 1)\n    lrs = eta_min + 0.5 * (eta_max - eta_min) * (1.0 + np.cos(np.pi * t / (T - 1)))\n    return lrs\n\ndef verify_cycle(lrs: np.ndarray, eta_min: float, eta_max: float, lambda_hat: float, T: int) -> bool:\n    \"\"\"\n    Verify that the schedule respects [eta_min, eta_max], is capped by 2/lambda_hat,\n    and has correct endpoints (start at eta_max, end at eta_min for T>1; constant for T==1).\n    \"\"\"\n    tol = 1e-12\n    # Range checks\n    cond_range = (lrs.min() >= eta_min - tol) and (lrs.max() = eta_max + tol)\n    # Stability cap\n    cap = np.inf if lambda_hat = 0.0 else (2.0 / lambda_hat) + tol\n    cond_cap = np.all(lrs = cap)\n    # Endpoint checks\n    if T > 1:\n        cond_start = abs(lrs[0] - eta_max) = tol\n        cond_end = abs(lrs[-1] - eta_min) = tol\n        cond_endpoints = cond_start and cond_end\n    else:\n        cond_endpoints = abs(lrs[0] - eta_max) = tol\n    return cond_range and cond_cap and cond_endpoints\n\ndef controller_and_verify(H: np.ndarray, eta_min: float, eta_target: float,\n                          cycles: int, T: int, K: int, base_seed: int) -> bool:\n    \"\"\"\n    For each cycle, estimate curvature via power iteration, set eta_max = min(eta_target, 2/lambda_hat),\n    build the cosine annealing schedule, and verify constraints. Return True iff all cycles pass.\n    \"\"\"\n    all_ok = True\n    for c in range(cycles):\n        # Per-cycle seed to vary the initial direction deterministically\n        seed = base_seed + c\n        lambda_hat = power_iteration(H, K, seed)\n        eta_cap = np.inf if lambda_hat = 0.0 else 2.0 / lambda_hat\n        eta_max = min(eta_target, eta_cap)\n        lrs = cosine_cycle_schedule(eta_min, eta_max, T)\n        ok = verify_cycle(lrs, eta_min, eta_max, lambda_hat, T)\n        all_ok = all_ok and ok\n    return all_ok\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"H\": np.array([[1.0, 0.0],\n                           [0.0, 0.5]], dtype=float),\n            \"eta_min\": 0.0,\n            \"eta_target\": 0.9,\n            \"cycles\": 2,\n            \"T\": 10,\n            \"K\": 50,\n            \"seed\": 101,\n        },\n        # Test Case 2\n        {\n            \"H\": np.array([[5.0, 0.0],\n                           [0.0, 3.0]], dtype=float),\n            \"eta_min\": 0.05,\n            \"eta_target\": 0.9,\n            \"cycles\": 3,\n            \"T\": 8,\n            \"K\": 30,\n            \"seed\": 202,\n        },\n        # Test Case 3\n        {\n            \"H\": 0.001 * np.eye(3, dtype=float),\n            \"eta_min\": 0.0,\n            \"eta_target\": 1.5,\n            \"cycles\": 2,\n            \"T\": 4,\n            \"K\": 20,\n            \"seed\": 303,\n        },\n        # Test Case 4\n        {\n            \"H\": np.array([[10.0, 0.0],\n                           [0.0, 0.01]], dtype=float),\n            \"eta_min\": 0.0,\n            \"eta_target\": 0.25,\n            \"cycles\": 3,\n            \"T\": 7,\n            \"K\": 25,\n            \"seed\": 404,\n        },\n        # Test Case 5\n        {\n            \"H\": np.array([[2.0, 0.0],\n                           [0.0, 1.0]], dtype=float),\n            \"eta_min\": 0.1,\n            \"eta_target\": 1.0,\n            \"cycles\": 1,\n            \"T\": 1,\n            \"K\": 40,\n            \"seed\": 505,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        ok = controller_and_verify(\n            H=case[\"H\"],\n            eta_min=case[\"eta_min\"],\n            eta_target=case[\"eta_target\"],\n            cycles=case[\"cycles\"],\n            T=case[\"T\"],\n            K=case[\"K\"],\n            base_seed=case[\"seed\"]\n        )\n        results.append(ok)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3110195"}]}