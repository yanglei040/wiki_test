{"hands_on_practices": [{"introduction": "The \"adaptive\" in adaptive learning rates hinges on tracking the scale of gradients over time. In algorithms like Adam, this is achieved with a second-moment estimator, $v_t$, which is an exponential moving average of squared gradients controlled by a hyperparameter $\\beta_2$. This exercise isolates this mechanism to explore a fundamental trade-off: a high $\\beta_2$ value provides a stable estimate by averaging over a long history, but it may adapt too slowly to abrupt changes in the loss landscape's curvature. By implementing a simplified simulation [@problem_id:3096921], you will directly measure the responsiveness of the $v_t$ estimator and build a strong intuition for how $\\beta_2$ governs the optimizer's ability to \"see\" and react to new information.", "problem": "You are asked to design and analyze a simplified training scenario to study how the second raw moment estimator in Adaptive Moment Estimation (Adam) reacts to abrupt changes in loss curvature. Consider a one-dimensional parameter with a loss locally approximated by a quadratic function whose curvature changes abruptly at specified iteration indices. In this setting, the squared gradient’s true second moment is modeled as a piecewise-constant sequence that undergoes sudden regime shifts (phase transitions).\n\nFundamental base and setup:\n- Let the per-iteration stochastic gradient be denoted by $g_t$. The second raw moment estimator used by Adaptive Moment Estimation (Adam) is an Exponential Moving Average (EMA) defined by\n$$\nv_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, g_t^2,\n$$\nwhere $v_t$ is the estimator at iteration $t$ and $\\beta_2 \\in [0,1)$ is the decay parameter for the second moment.\n- To isolate responsiveness to curvature changes without confounding optimization dynamics, replace the random input $g_t^2$ by its true regime-dependent second moment $m_t = \\mathbb{E}[g_t^2]$, which is assumed piecewise constant over time and changes abruptly at specified iteration indices. Under this substitution, the estimator evolves deterministically via\n$$\nv_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, m_t,\n$$\nwith $v_0 = 0$.\n- After each abrupt change in $m_t$, define the responsiveness metric as the smallest nonnegative integer lag $\\ell$ (measured in iterations) such that $|v_{t_c+\\ell} - m_{\\text{new}}| \\le \\delta \\, m_{\\text{new}}$, where $t_c$ is the iteration index of the change, $m_{\\text{new}}$ is the new constant value of $m_t$ after the change, and $\\delta \\in (0,1)$ is a specified tolerance. If the tolerance is not met within the provided horizon $T$, record the lag as $T - t_c$. The final score for a test case is the average of these lags over all changes in that case, rounded to the nearest integer.\n\nYour task:\n- Implement the deterministic update $v_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, m_t$ over a fixed horizon $T$ for several test scenarios, each defined by $(\\beta_2, T, \\{t_c\\}, \\{m\\}, \\delta)$, where $\\{t_c\\}$ is an increasing list of change indices and $\\{m\\}$ is a list of regime values of $m_t$ whose length is $|\\{t_c\\}| + 1$.\n- For each scenario, compute the mean settling time across all changes and round to the nearest integer.\n\nTest suite:\nUse the following six scenarios that explore a range of behaviors including a general case, large upward and downward phase transitions, multiple transitions, and extreme slow adaptation. In each item below, all mathematical entities are given and must be used exactly.\n\n1. Scenario $1$: $\\beta_2 = 0.0$, $T = 200$, changes at $\\{50, 120\\}$, regimes $\\{1.0, 10.0, 0.1\\}$, $\\delta = 0.05$.\n2. Scenario $2$: $\\beta_2 = 0.9$, $T = 200$, changes at $\\{50, 120\\}$, regimes $\\{1.0, 10.0, 0.1\\}$, $\\delta = 0.05$.\n3. Scenario $3$: $\\beta_2 = 0.99$, $T = 200$, changes at $\\{50, 120\\}$, regimes $\\{1.0, 10.0, 0.1\\}$, $\\delta = 0.05$.\n4. Scenario $4$: $\\beta_2 = 0.999$, $T = 200$, changes at $\\{50, 120\\}$, regimes $\\{1.0, 10.0, 0.1\\}$, $\\delta = 0.05$.\n5. Scenario $5$: $\\beta_2 = 0.99$, $T = 200$, changes at $\\{20, 40, 80, 120\\}$, regimes $\\{1.0, 4.0, 0.5, 2.0, 1.5\\}$, $\\delta = 0.05$.\n6. Scenario $6$: $\\beta_2 = 0.9999$, $T = 1000$, changes at $\\{50, 120\\}$, regimes $\\{1.0, 10.0, 0.1\\}$, $\\delta = 0.05$.\n\nFinal output format:\nYour program should produce a single line containing the results as a comma-separated list enclosed in square brackets, where each entry corresponds to the rounded mean settling time for one scenario, in the order $1$ through $6$ (e.g., $[r_1,r_2,r_3,r_4,r_5,r_6]$). All entries must be integers. No physical units or angles are involved in this problem; all quantities are dimensionless.", "solution": "The user has provided a problem that requires analyzing the response of a simplified second moment estimator from the Adaptive Moment Estimation (Adam) algorithm to sudden changes in the loss landscape's curvature. This analysis is to be performed under a deterministic model where the stochastic squared gradient is replaced by its piecewise-constant expectation.\n\nThe core of the problem is the deterministic, first-order linear recurrence relation governing the evolution of the second moment estimate, $v_t$:\n$$\nv_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, m_t\n$$\nwith the initial condition $v_0 = 0$. Here, $v_t$ is the estimator at iteration $t$, $\\beta_2 \\in [0, 1)$ is the exponential decay rate, and $m_t$ is the true second raw moment of the gradient, which is given as a piecewise-constant function of $t$.\n\nTo understand the dynamics, consider a period where $m_t$ is constant, say $m_t = m_{\\text{const}}$. The fixed point of the recurrence is $v = m_{\\text{const}}$. Let's define the error or deviation from this fixed point as $\\epsilon_t = v_t - m_{\\text{const}}$. Substituting this into the update rule gives:\n$$\n\\epsilon_t + m_{\\text{const}} = \\beta_2 (\\epsilon_{t-1} + m_{\\text{const}}) + (1 - \\beta_2) m_{\\text{const}}\n$$\n$$\n\\epsilon_t + m_{\\text{const}} = \\beta_2 \\epsilon_{t-1} + \\beta_2 m_{\\text{const}} + m_{\\text{const}} - \\beta_2 m_{\\text{const}}\n$$\n$$\n\\epsilon_t = \\beta_2 \\epsilon_{t-1}\n$$\nThis shows that the deviation from the target moment decays exponentially with a factor of $\\beta_2$ at each iteration. Consequently, after $\\ell$ iterations into a new constant regime, the initial deviation $\\epsilon_0$ will have been reduced by a factor of $\\beta_2^\\ell$. A value of $\\beta_2$ close to $1$ implies a slow decay and thus slow adaptation to a new moment, as the estimator retains a long memory of past values. Conversely, a $\\beta_2$ of $0$ implies instant adaptation, as $v_t = m_t$.\n\nThe problem asks for a responsiveness metric: the lag $\\ell$, which is the number of iterations required for $v_t$ to settle within a tolerance band $\\delta$ of a new target moment $m_{\\text{new}}$ following a change at iteration $t_c$. Specifically, we must find the smallest non-negative integer $\\ell$ such that $|v_{t_c+\\ell} - m_{\\text{new}}| \\le \\delta m_{\\text{new}}$.\n\nWhile an analytical expression for $\\ell$ can be derived for an isolated regime change, the problem specifies scenarios with multiple, successive changes. The evolution of $v_t$ after a change at $t_c$ is affected not only by the new moment $m_{\\text{new}}$ but also by any subsequent changes to $m_t$ that occur before $v_t$ has fully converged. This complexity makes a direct simulation of the process the most robust and faithful method for solving the problem.\n\nThe algorithm proceeds as follows:\n1.  **Construct the Moment Sequence**: For each scenario, we first construct the complete sequence of true moments, $m_t$, for all iterations $t$ from $1$ to the horizon $T$, based on the given change points $\\{t_c\\}$ and regime values $\\{m\\}$.\n2.  **Simulate the Estimator's Evolution**: We then simulate the evolution of the estimator $v_t$ over the entire horizon. Starting with $v_0 = 0$, we iteratively compute each $v_t$ for $t=1, \\dots, T$ using the update rule $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) m_t$ and the pre-computed moment sequence.\n3.  **Calculate Settling Lags**: For each specified change point $t_c$ in a scenario, we analyze the resulting $v_t$ sequence to find the settling lag.\n    - The new target moment is $m_{\\text{new}}$, which is the value of the $m_t$ sequence for the regime starting at $t_c$.\n    - The convergence criterion is $|v_{t_c+\\ell} - m_{\\text{new}}| \\le \\delta m_{\\text{new}}$.\n    - We search for the smallest non-negative integer $\\ell$ (from $\\ell=0$ up to $T-t_c$) for which this condition is met.\n    - If the condition is not satisfied for any $\\ell$ within the search window (i.e., up to a time of $t=T$), the lag is recorded as the maximum possible value, $T-t_c$, as per the problem specification.\n4.  **Compute the Final Score**: The final score for each scenario is the arithmetic mean of all calculated lags, rounded to the nearest integer.\n\nThis computational procedure accurately models the specified deterministic system and allows for the precise calculation of the responsiveness metric under complex, multi-stage regime shifts.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_mean_lag(beta2, T, changes, regimes, delta):\n    \"\"\"\n    Calculates the rounded mean settling time for a single scenario.\n    \"\"\"\n    # Step 1: Construct the piecewise-constant moment sequence m_t\n    # m_seq is 1-indexed for problem clarity, so size is T+1\n    m_seq = np.zeros(T + 1)\n    \n    # Define the start and end of each regime\n    regime_starts = [1] + changes\n    regime_ends = changes + [T + 1]\n    \n    for i, m_val in enumerate(regimes):\n        start_idx = regime_starts[i]\n        end_idx = regime_ends[i]\n        if start_idx  end_idx: # Ensure the interval is valid\n            m_seq[start_idx:end_idx] = m_val\n\n    # Step 2: Simulate the evolution of the estimator v_t\n    # v_seq is 0-indexed for v_0, so size is T+1\n    v_seq = np.zeros(T + 1)  # v_seq[0] is v_0 = 0\n    \n    for t in range(1, T + 1):\n        v_seq[t] = beta2 * v_seq[t-1] + (1 - beta2) * m_seq[t]\n        \n    # Step 3: Calculate settling lags for each change\n    all_lags = []\n    for i, tc in enumerate(changes):\n        m_new = regimes[i + 1]\n        threshold = delta * m_new\n        lag_found = False\n        \n        # Search for the smallest non-negative lag l\n        max_l = T - tc\n        for l in range(max_l + 1):\n            t = tc + l\n            if abs(v_seq[t] - m_new) = threshold:\n                all_lags.append(l)\n                lag_found = True\n                break\n        \n        # If tolerance is not met, use the capped value\n        if not lag_found:\n            all_lags.append(max_l)\n            \n    # Step 4: Compute the final score (rounded mean lag)\n    if not all_lags:\n        return 0\n        \n    mean_lag = np.mean(all_lags)\n    return int(round(mean_lag))\n\ndef solve():\n    \"\"\"\n    Main function to run all test scenarios and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (beta2, T, changes, regimes, delta)\n        (0.0, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.9, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.99, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.999, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.99, 200, [20, 40, 80, 120], [1.0, 4.0, 0.5, 2.0, 1.5], 0.05),\n        (0.9999, 1000, [50, 120], [1.0, 10.0, 0.1], 0.05)\n    ]\n\n    results = []\n    for case in test_cases:\n        beta2, T, changes, regimes, delta = case\n        result = calculate_mean_lag(beta2, T, changes, regimes, delta)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3096921"}, {"introduction": "Adaptive optimizers like Adam combine a momentum term $m_t$ with an adaptive scaling term $v_t$. While we have examined $v_t$ in isolation, their interaction can produce complex and sometimes counter-intuitive dynamics. This practice challenges you to analyze such a scenario by deriving and simulating the optimizer's steady-state behavior under a deterministic, oscillating gradient signal [@problem_id:3097011]. By solving for the net parameter drift over a full cycle, you will uncover how momentum and adaptive scaling can conspire to create persistent movement, providing a crucial lesson in the emergent properties of these powerful algorithms.", "problem": "You are to analyze a one-dimensional optimizer that combines momentum and adaptive per-step scaling. Consider a scalar parameter sequence $ \\theta_t $ updated using a momentum accumulator $ m_t $ and a second-moment accumulator $ v_t $ as follows. The momentum is an exponential moving average (Exponential Moving Average (EMA)) of gradients defined by $ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $. The second-moment accumulator is an EMA of squared gradients defined by $ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 $. The parameter is updated by a scaled step $ \\theta_{t+1} = \\theta_t - \\alpha \\, m_t / \\sqrt{v_t + \\varepsilon} $, where $ \\alpha  0 $ is the base learning rate and $ \\varepsilon  0 $ is a small constant.\n\nAssume a deterministic periodic gradient signal with period $ 2 $:\n- On even steps $ t = 0, 2, 4, \\dots $, the gradient is $ g_t = +a $.\n- On odd steps $ t = 1, 3, 5, \\dots $, the gradient is $ g_t = -b $.\nHere $ a  0 $ and $ b  0 $ are constants. This scenario models alternation between two recurrent batches with unequal gradient magnitudes and opposite signs.\n\nStarting from the definitions of $ m_t $, $ v_t $, and the update rule above, do the following:\n1) Derive the steady-state $ 2 $-cycle values $ (m_{\\mathrm{even}}, v_{\\mathrm{even}}) $ and $ (m_{\\mathrm{odd}}, v_{\\mathrm{odd}}) $ that the internal states $ (m_t, v_t) $ approach when the periodic gradient has been applied for a long time. Express $ m_{\\mathrm{even}} $ and $ m_{\\mathrm{odd}} $ in terms of $ a $, $ b $, and $ \\beta_1 $ only, and express $ v_{\\mathrm{even}} $ and $ v_{\\mathrm{odd}} $ in terms of $ a $, $ b $, and $ \\beta_2 $ only. You must work directly from the EMA definitions without introducing any unproven simplifications.\n\n2) Using the steady-state values, derive the net two-step drift of the parameter over one full period,\n$$\n\\Delta_\\theta \\equiv \\theta_{t+2} - \\theta_t = - \\alpha \\left( \\frac{m_{\\mathrm{even}}}{\\sqrt{v_{\\mathrm{even}} + \\varepsilon}} + \\frac{m_{\\mathrm{odd}}}{\\sqrt{v_{\\mathrm{odd}} + \\varepsilon}} \\right),\n$$\nand simplify it as far as possible as a function of $ \\alpha, \\beta_1, \\beta_2, a, b, \\varepsilon $.\n\n3) Implement a program that:\n- Computes $ \\Delta_\\theta $ using the exact closed-form steady-state values from part $ 1 $ for each parameter set in the test suite below.\n- Produces a single line of output containing the results as a comma-separated list of decimal numbers rounded to $ 12 $ decimal places and enclosed in square brackets, for example, $ [0.123000000000,-0.045600000000] $.\n\nUse the following test suite of parameter sets $ (\\alpha, \\beta_1, \\beta_2, a, b, \\varepsilon) $:\n- Case $ 1 $ (happy path): $ (0.001, 0.9, 0.999, 1.0, 0.5, 1\\mathrm{e}{-8}) $.\n- Case $ 2 $ (no momentum): $ (0.001, 0.0, 0.9, 1.0, 0.5, 1\\mathrm{e}{-8}) $.\n- Case $ 3 $ (instantaneous second moment): $ (0.001, 0.9, 0.0, 1.0, 0.5, 1\\mathrm{e}{-8}) $.\n- Case $ 4 $ (symmetric magnitudes): $ (0.001, 0.9, 0.999, 1.0, 1.0, 1\\mathrm{e}{-8}) $.\n- Case $ 5 $ (extreme smoothing and imbalance): $ (0.001, 0.99, 0.9999, 1.5, 0.2, 1\\mathrm{e}{-8}) $.\n\nAll quantities are dimensionless. Your program must compute the five values of $ \\Delta_\\theta $ for the five cases in order and print them on a single line in the exact format described above. No user input is required or allowed. The final output must be one line only, with no additional text.", "solution": "The problem requires the analysis of a one-dimensional optimizer with momentum and adaptive scaling, akin to the Adam optimizer, under a deterministic, periodic gradient signal. We must first validate the problem statement. The problem is scientifically grounded, well-posed, and objective. It provides a complete and consistent set of definitions and parameters based on established principles of numerical optimization and exponential moving averages. The analysis of an optimizer's steady-state behavior under a simplified periodic input is a standard and valuable technique for understanding its dynamics. The problem is therefore deemed valid. We proceed to the solution.\n\nThe core of the problem is to determine the steady-state behavior of the momentum accumulator $m_t$ and the second-moment accumulator $v_t$ under a 2-cycle periodic gradient.\n\nThe update rules are given as:\n$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$\n$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$\n$\\theta_{t+1} = \\theta_t - \\alpha \\frac{m_t}{\\sqrt{v_t + \\varepsilon}}$\n\nThe gradient $g_t$ follows a 2-cycle period:\n$g_t = +a$ for even $t$ ($t=0, 2, \\dots$)\n$g_t = -b$ for odd $t$ ($t=1, 3, \\dots$)\nwhere $a  0$ and $b  0$.\n\n**1. Derivation of Steady-State 2-Cycle Values**\n\nIn steady state, the system enters a 2-cycle where the values of the accumulators at the end of even steps are constant, and the values at the end of odd steps are also constant. Let's denote these steady-state values as $(m_{\\mathrm{even}}, v_{\\mathrm{even}})$ and $(m_{\\mathrm{odd}}, v_{\\mathrm{odd}})$.\n\n**Momentum Accumulator ($m_t$)**\n\nConsider an even timestep $t$. The momentum is $m_t$, which in steady state approaches $m_{\\mathrm{even}}$. The previous momentum was $m_{t-1}$, which approaches $m_{\\mathrm{odd}}$. The gradient is $g_t = a$. The recurrence relation becomes:\n$$m_{\\mathrm{even}} = \\beta_1 m_{\\mathrm{odd}} + (1 - \\beta_1) a \\quad (1)$$\n\nNow consider the subsequent odd timestep $t+1$. The momentum is $m_{t+1}$, which approaches $m_{\\mathrm{odd}}$. The previous momentum was $m_t$, which approaches $m_{\\mathrm{even}}$. The gradient is $g_{t+1} = -b$. The recurrence relation becomes:\n$$m_{\\mathrm{odd}} = \\beta_1 m_{\\mathrm{even}} + (1 - \\beta_1) (-b) \\quad (2)$$\n\nWe have a system of two linear equations for $m_{\\mathrm{even}}$ and $m_{\\mathrm{odd}}$. To solve it, we can substitute equation (2) into equation (1):\n$$m_{\\mathrm{even}} = \\beta_1 (\\beta_1 m_{\\mathrm{even}} - (1 - \\beta_1) b) + (1 - \\beta_1) a$$\n$$m_{\\mathrm{even}} = \\beta_1^2 m_{\\mathrm{even}} - \\beta_1 (1 - \\beta_1) b + (1 - \\beta_1) a$$\n$$m_{\\mathrm{even}} (1 - \\beta_1^2) = (1 - \\beta_1) a - \\beta_1 (1 - \\beta_1) b$$\nAssuming $\\beta_1 \\neq 1$, we can divide by $(1 - \\beta_1)$:\n$$m_{\\mathrm{even}} (1 + \\beta_1) = a - \\beta_1 b$$\n$$m_{\\mathrm{even}} = \\frac{a - \\beta_1 b}{1 + \\beta_1}$$\n\nNow, substitute this result back into equation (2) to find $m_{\\mathrm{odd}}$:\n$$m_{\\mathrm{odd}} = \\beta_1 \\left(\\frac{a - \\beta_1 b}{1 + \\beta_1}\\right) - (1 - \\beta_1) b$$\n$$m_{\\mathrm{odd}} = \\frac{\\beta_1 a - \\beta_1^2 b}{1 + \\beta_1} - \\frac{(1 - \\beta_1)(1 + \\beta_1)b}{1 + \\beta_1}$$\n$$m_{\\mathrm{odd}} = \\frac{\\beta_1 a - \\beta_1^2 b - (1 - \\beta_1^2)b}{1 + \\beta_1}$$\n$$m_{\\mathrm{odd}} = \\frac{\\beta_1 a - \\beta_1^2 b - b + \\beta_1^2 b}{1 + \\beta_1}$$\n$$m_{\\mathrm{odd}} = \\frac{\\beta_1 a - b}{1 + \\beta_1}$$\n\nThus, the steady-state momentum values are:\n$$m_{\\mathrm{even}} = \\frac{a - \\beta_1 b}{1 + \\beta_1} \\quad \\text{and} \\quad m_{\\mathrm{odd}} = \\frac{\\beta_1 a - b}{1 + \\beta_1}$$\n\n**Second-Moment Accumulator ($v_t$)**\n\nThe derivation for $v_t$ is perfectly analogous. The recurrence is $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$. The squared gradients are:\n$g_t^2 = a^2$ for even $t$.\n$g_t^2 = (-b)^2 = b^2$ for odd $t$.\n\nThe system of equations for the steady-state values $v_{\\mathrm{even}}$ and $v_{\\mathrm{odd}}$ is:\n$$v_{\\mathrm{even}} = \\beta_2 v_{\\mathrm{odd}} + (1 - \\beta_2) a^2 \\quad (3)$$\n$$v_{\\mathrm{odd}} = \\beta_2 v_{\\mathrm{even}} + (1 - \\beta_2) b^2 \\quad (4)$$\n\nA direct re-derivation is safer to avoid sign errors. Substitute (4) into (3):\n$$v_{\\mathrm{even}} = \\beta_2 (\\beta_2 v_{\\mathrm{even}} + (1 - \\beta_2) b^2) + (1 - \\beta_2) a^2$$\n$$v_{\\mathrm{even}} (1 - \\beta_2^2) = \\beta_2 (1 - \\beta_2) b^2 + (1 - \\beta_2) a^2$$\n$$v_{\\mathrm{even}} (1 + \\beta_2) = \\beta_2 b^2 + a^2$$\n$$v_{\\mathrm{even}} = \\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2}$$\n\nSubstitute this into (4):\n$$v_{\\mathrm{odd}} = \\beta_2 \\left(\\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2}\\right) + (1 - \\beta_2) b^2$$\n$$v_{\\mathrm{odd}} = \\frac{\\beta_2 a^2 + \\beta_2^2 b^2 + (1 - \\beta_2^2)b^2}{1 + \\beta_2}$$\n$$v_{\\mathrm{odd}} = \\frac{\\beta_2 a^2 + \\beta_2^2 b^2 + b^2 - \\beta_2^2 b^2}{1 + \\beta_2}$$\n$$v_{\\mathrm{odd}} = \\frac{\\beta_2 a^2 + b^2}{1 + \\beta_2}$$\n\nThe correct steady-state second-moment values are:\n$$v_{\\mathrm{even}} = \\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2} \\quad \\text{and} \\quad v_{\\mathrm{odd}} = \\frac{\\beta_2 a^2 + b^2}{1 + \\beta_2}$$\n\n**2. Derivation of Net Two-Step Drift ($\\Delta_\\theta$)**\n\nThe net drift $\\Delta_\\theta$ over one full period (two steps) is defined as $\\theta_{t+2} - \\theta_t$. Let's assume step $t$ is an even step. The parameter is updated as follows:\nStep $1$ (from $t$ to $t+1$):\n$$\\theta_{t+1} = \\theta_t - \\alpha \\frac{m_t}{\\sqrt{v_t + \\varepsilon}}$$\nIn steady state, this becomes:\n$$\\theta_{t+1} - \\theta_t = - \\alpha \\frac{m_{\\mathrm{even}}}{\\sqrt{v_{\\mathrm{even}} + \\varepsilon}}$$\n\nStep $2$ (from $t+1$ to $t+2$):\n$$\\theta_{t+2} = \\theta_{t+1} - \\alpha \\frac{m_{t+1}}{\\sqrt{v_{t+1} + \\varepsilon}}$$\nIn steady state, this becomes:\n$$\\theta_{t+2} - \\theta_{t+1} = - \\alpha \\frac{m_{\\mathrm{odd}}}{\\sqrt{v_{\\mathrm{odd}} + \\varepsilon}}$$\n\nThe total drift is the sum of these two changes:\n$$\\Delta_\\theta = (\\theta_{t+2} - \\theta_{t+1}) + (\\theta_{t+1} - \\theta_t) = - \\alpha \\left( \\frac{m_{\\mathrm{even}}}{\\sqrt{v_{\\mathrm{even}} + \\varepsilon}} + \\frac{m_{\\mathrm{odd}}}{\\sqrt{v_{\\mathrm{odd}} + \\varepsilon}} \\right)$$\n\nSubstituting the derived steady-state expressions yields the final formula for the net drift:\n$$ \\Delta_\\theta = - \\alpha \\left( \\frac{\\frac{a - \\beta_1 b}{1 + \\beta_1}}{\\sqrt{\\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2} + \\varepsilon}} + \\frac{\\frac{\\beta_1 a - b}{1 + \\beta_1}}{\\sqrt{\\frac{\\beta_2 a^2 + b^2}{1 + \\beta_2} + \\varepsilon}} \\right) $$\nThis expression can be factored slightly:\n$$ \\Delta_\\theta = - \\frac{\\alpha}{1 + \\beta_1} \\left( \\frac{a - \\beta_1 b}{\\sqrt{\\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2} + \\varepsilon}} + \\frac{\\beta_1 a - b}{\\sqrt{\\frac{\\beta_2 a^2 + b^2}{1 + \\beta_2} + \\varepsilon}} \\right) $$\nThis expression is the closed-form solution for the two-step drift and will be implemented to compute the numerical results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the net two-step drift of a parameter for a one-dimensional\n    optimizer with periodic gradients, based on a derived closed-form solution\n    for the steady-state behavior of its internal accumulators.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (alpha, beta1, beta2, a, b, epsilon)\n    test_cases = [\n        (0.001, 0.9, 0.999, 1.0, 0.5, 1e-8),      # Case 1 (happy path)\n        (0.001, 0.0, 0.9, 1.0, 0.5, 1e-8),       # Case 2 (no momentum)\n        (0.001, 0.9, 0.0, 1.0, 0.5, 1e-8),       # Case 3 (instantaneous second moment)\n        (0.001, 0.9, 0.999, 1.0, 1.0, 1e-8),      # Case 4 (symmetric magnitudes)\n        (0.001, 0.99, 0.9999, 1.5, 0.2, 1e-8),   # Case 5 (extreme smoothing and imbalance)\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, beta1, beta2, a, b, epsilon = case\n\n        # Calculate steady-state momentum values\n        # m_even = (a - beta1 * b) / (1 + beta1)\n        # m_odd = (beta1 * a - b) / (1 + beta1)\n        # Handle beta1 = -1 case to avoid division by zero, though not in test cases\n        if beta1 == -1.0:\n            # The recurrence does not converge to a 2-cycle in this case.\n            # However, problem constraints imply beta1 is not -1.\n            # For a numerically robust implementation, one might add handling.\n            # We proceed assuming beta1 != -1 as per standard optimizer design.\n            pass\n\n        m_even = (a - beta1 * b) / (1.0 + beta1)\n        m_odd = (beta1 * a - b) / (1.0 + beta1)\n\n        # Calculate steady-state second-moment values\n        # v_even = (a**2 + beta2 * b**2) / (1 + beta2)\n        # v_odd = (beta2 * a**2 + b**2) / (1 + beta2)\n        # Handle beta2 = -1 case.\n        if beta2 == -1.0:\n            pass # Similar logic as for beta1\n        \n        v_even = (a**2 + beta2 * b**2) / (1.0 + beta2)\n        v_odd = (beta2 * a**2 + b**2) / (1.0 + beta2)\n\n        # Calculate the two terms of the parameter update\n        term_even = m_even / np.sqrt(v_even + epsilon)\n        term_odd = m_odd / np.sqrt(v_odd + epsilon)\n\n        # Calculate the net two-step drift delta_theta\n        delta_theta = -alpha * (term_even + term_odd)\n        \n        results.append(delta_theta)\n\n    # Final print statement in the exact required format.\n    # The format specifier .12f ensures rounding to 12 decimal places.\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3097011"}, {"introduction": "While adaptive methods are powerful, they are not immune to the instabilities caused by large, anomalous gradients, which can lead to exploding parameter updates. A common practical solution is gradient clipping, but a naive, fixed threshold can interfere with the benefits of adaptive scaling. This exercise guides you through the design of a more intelligent \"scale-aware\" clipping strategy, where the clipping bound for each parameter is itself proportional to its estimated gradient scale $\\sqrt{v_{i,t}}$ [@problem_id:3097005]. You will first derive an elegant bound on the step size this method guarantees and then implement a simulation to confirm its ability to prevent exploding updates while preserving the optimizer's performance.", "problem": "You are asked to reason from first principles about scale-aware clipping in adaptive learning rate algorithms and to implement and test your conclusions in a complete, runnable program. Begin from the standard Root Mean Square Propagation (RMSProp) update, an adaptive method that rescales per-parameter gradients by an exponential moving average of squared gradients. Specifically, let $i$ index parameters and $t$ index steps. Let $g_{i,t}$ denote the stochastic gradient of a differentiable objective at step $t$, and define the second-moment accumulator $v_{i,t}$ by the exponential recursion $v_{i,t} \\leftarrow \\beta v_{i,t-1} + (1-\\beta) g_{i,t}^{2}$ with $v_{i,0} = 0$ and $\\beta \\in [0,1)$. The baseline RMSProp step for parameter $i$ at time $t$ is $\\Delta \\theta_{i,t} \\leftarrow -\\alpha \\, g_{i,t} / \\left(\\sqrt{v_{i,t-1}} + \\varepsilon\\right)$ with learning rate $\\alpha  0$ and stabilizer $\\varepsilon \\ge 0$, followed by $\\theta_{i,t} \\leftarrow \\theta_{i,t-1} + \\Delta \\theta_{i,t}$. Note that the preconditioning uses $v_{i,t-1}$ to avoid circular dependence within step $t$.\n\nTask A (derivation). Propose and justify a per-parameter clipping rule that is proportional to the square root of the second-moment accumulator, parameterized by a constant $\\lambda  0$ and the same stabilizer $\\varepsilon \\ge 0$. Your rule should be stated as an explicit inequality on a clipped gradient $\\tilde{g}_{i,t}$ in terms of $v_{i,t-1}$, $\\lambda$, and $\\varepsilon$. From the RMSProp definitions above and your clipping rule, derive a step-size bound of the form $\\lvert \\Delta \\theta_{i,t} \\rvert \\le \\text{(expression independent of $g_{i,t}$)}$ that does not depend on the unbounded magnitude of $g_{i,t}$, and explain why this is expected to reduce exploding updates while preserving the adaptive rescaling benefits of RMSProp.\n\nTask B (implementation and test). Implement two optimizers over a synthetic convex quadratic objective $f(\\theta) = \\tfrac{1}{2} \\sum_{i=1}^{d} c_i \\, \\theta_i^2$, where $d$ is the dimensionality and the curvature vector $c \\in \\mathbb{R}^{d}_{0}$ is provided per test. The exact gradient is $g_{i,t}^{\\text{base}} = c_i \\, \\theta_{i,t-1}$. To simulate bursty or heavy-tailed stochasticity, the observed gradient used for updates at step $t$ is $g_{i,t} = g_{i,t}^{\\text{base}} + s_{i,t}$, where the spike schedule $s_{i,t}$ is a sparse vector with nonzero entries only at specified $(t,i)$ pairs. Implement:\n- A baseline RMSProp variant (no clipping): use $g_{i,t}$ in the step and update $v_{i,t}$ by the standard recursion.\n- A scale-aware clipped variant: apply your proposed per-parameter clipping to $g_{i,t}$ to obtain $\\tilde{g}_{i,t}$, then use $\\tilde{g}_{i,t}$ in the step while still updating $v_{i,t}$ by the standard recursion with the unmodified $g_{i,t}$.\n\nFor each run, track the maximum absolute per-parameter step magnitude $M = \\max_{t,i} \\lvert \\Delta \\theta_{i,t} \\rvert$ and the final objective value $f(\\theta_T)$ at the end of $T$ steps.\n\nDefine two evaluation predicates per test case $j$:\n- Exploding-update reduction predicate $E_j$: true if the baseline exhibits an oversized step relative to a supplied threshold $\\tau$ while the clipped variant’s maximum step is bounded by your derived bound. Formally, $E_j$ is true if $\\left(M^{\\text{base}}  \\tau\\right)$ and $\\left(M^{\\text{clip}} \\le \\alpha \\lambda + \\text{tol}\\right)$, where $\\text{tol}  0$ is a small numerical tolerance.\n- Adaptive-benefit preservation predicate $P_j$: true if the clipped final loss is not worse than the baseline by more than a supplied relative tolerance $\\delta  0$, i.e., $f^{\\text{clip}}(\\theta_T) \\le (1+\\delta) \\, f^{\\text{base}}(\\theta_T)$.\n\nFor each test case $j$, output $r_j = 1$ if both $E_j$ and $P_j$ are true, and $r_j = 0$ otherwise.\n\nTest suite. Your program must run exactly the following $3$ test cases, each specified by a tuple containing $(d, c, \\theta_0, T, \\alpha, \\beta, \\varepsilon, \\lambda, \\text{spikes}, \\tau, \\delta, \\text{tol})$:\n- Case $1$ (happy path with a large spike on a high-curvature coordinate):\n  - $d = 3$\n  - $c = [1.0, 0.1, 10.0]$\n  - $\\theta_0 = [1.0, 1.0, 1.0]$\n  - $T = 60$\n  - $\\alpha = 0.05$\n  - $\\beta = 0.9$\n  - $\\varepsilon = 10^{-3}$\n  - $\\lambda = 2.0$\n  - spikes: at step $t=10$, add $+200.0$ to coordinate $i=2$ (zero-based index), and at step $t=20$, add $-150.0$ to coordinate $i=2$\n  - $\\tau = 50.0$\n  - $\\delta = 0.5$\n  - $\\text{tol} = 10^{-12}$\n- Case $2$ (no spikes, boundary condition where clipping should be inactive):\n  - $d = 3$\n  - $c = [1.0, 0.3, 3.0]$\n  - $\\theta_0 = [1.5, -0.5, 0.75]$\n  - $T = 100$\n  - $\\alpha = 0.05$\n  - $\\beta = 0.9$\n  - $\\varepsilon = 10^{-3}$\n  - $\\lambda = 5.0$\n  - spikes: none\n  - $\\tau = 1.0$\n  - $\\delta = 0.05$\n  - $\\text{tol} = 10^{-12}$\n- Case $3$ (multiple spikes on a very low-curvature coordinate and one moderate spike elsewhere):\n  - $d = 4$\n  - $c = [1.0, 0.2, 5.0, 0.05]$\n  - $\\theta_0 = [1.0, 1.0, 1.0, 1.0]$\n  - $T = 120$\n  - $\\alpha = 0.04$\n  - $\\beta = 0.9$\n  - $\\varepsilon = 10^{-3}$\n  - $\\lambda = 1.5$\n  - spikes: at step $t=5$ add $+300.0$ to coordinate $i=3$; at step $t=6$ add $-300.0$ to coordinate $i=3$; at step $t=30$ add $+150.0$ to coordinate $i=0$\n  - $\\tau = 30.0$\n  - $\\delta = 0.5$\n  - $\\text{tol} = 10^{-12}$\n\nAngle units are not applicable. No physical units are used. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, in the order of the test cases. For example, if $r_1 = 1$, $r_2 = 0$, and $r_3 = 1$, the required output is \"[1,0,1]\".", "solution": "### Task A: Derivation of Scale-Aware Clipping and Step-Size Bound\n\nThe objective is to propose and justify a per-parameter clipping rule for the RMSProp optimizer that is proportional to the estimated scale of the gradients. The RMSProp algorithm maintains a per-parameter second-moment accumulator, $v_{i,t}$, which tracks an exponential moving average of the squared gradients:\n$$v_{i,t} \\leftarrow \\beta v_{i,t-1} + (1-\\beta) g_{i,t}^{2}$$\nHere, $g_{i,t}$ is the gradient of the objective with respect to parameter $\\theta_i$ at step $t$, and $\\beta \\in [0,1)$ is a decay factor. The magnitude of recent gradients for parameter $i$ is estimated by $\\sqrt{v_{i,t-1}}$. The standard RMSProp update normalizes the gradient by this scale estimate (plus a small stabilizer $\\varepsilon \\ge 0$):\n$$\\Delta \\theta_{i,t} \\leftarrow -\\alpha \\frac{g_{i,t}}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\n\n**Proposed Clipping Rule**\n\nTo make the clipping \"scale-aware,\" we define a clipping threshold that is itself proportional to the gradient scale estimate. The problem specifies this should be parameterized by a constant $\\lambda  0$. A natural choice for the clipping threshold for parameter $i$ at step $t$ is $C_{i,t} = \\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)$. This directly ties the clipping bound to the same adaptive scaling factor used in the RMSProp update itself.\n\nThe clipping rule transforms the raw gradient $g_{i,t}$ into a clipped gradient $\\tilde{g}_{i,t}$. We enforce that the magnitude of the clipped gradient does not exceed this threshold. This is stated as the following inequality on the clipped gradient $\\tilde{g}_{i,t}$:\n$$|\\tilde{g}_{i,t}| \\le \\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)$$\nThis rule can be implemented by clipping the value of $g_{i,t}$ to the range $[-C_{i,t}, C_{i,t}]$.\n\n**Derivation of the Step-Size Bound**\n\nWith the scale-aware clipping rule established, we now consider the modified parameter update which uses the clipped gradient $\\tilde{g}_{i,t}$:\n$$\\Delta \\theta_{i,t} \\leftarrow -\\alpha \\frac{\\tilde{g}_{i,t}}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\nTo derive a bound on the magnitude of this update step, $|\\Delta \\theta_{i,t}|$, we take the absolute value of the expression:\n$$|\\Delta \\theta_{i,t}| = \\left| -\\alpha \\frac{\\tilde{g}_{i,t}}{\\sqrt{v_{i,t-1}} + \\varepsilon} \\right| = \\alpha \\frac{|\\tilde{g}_{i,t}|}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\nNow, we substitute the inequality from our proposed clipping rule, $|\\tilde{g}_{i,t}| \\le \\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)$:\n$$|\\Delta \\theta_{i,t}| \\le \\alpha \\frac{\\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\nThe term $(\\sqrt{v_{i,t-1}} + \\varepsilon)$ cancels out, yielding the final step-size bound:\n$$|\\Delta \\theta_{i,t}| \\le \\alpha \\lambda$$\nThis bound is an elegant result: the magnitude of the parameter update is guaranteed to be no larger than the product of the learning rate $\\alpha$ and the clipping parameter $\\lambda$. Critically, this bound is completely independent of the magnitude of the raw, unclipped gradient $g_{i,t}$.\n\n**Justification and Benefits**\n\nThis scale-aware clipping mechanism provides two principal advantages:\n\n$1$. **Reduction of Exploding Updates**: The derived bound $|\\Delta \\theta_{i,t}| \\le \\alpha \\lambda$ demonstrates that large, anomalous gradients (spikes) cannot cause arbitrarily large parameter updates. By clipping the gradient before it is used in the update step, the algorithm prevents such events from destabilizing the training process and pushing the parameters into poor regions of the loss landscape.\n\n$2$. **Preservation of Adaptive Rescaling**: The clipping is not performed with a single, global threshold. Instead, the threshold $C_{i,t}$ is adaptive and parameter-specific, as it depends on $v_{i,t-1}$. For parameters that naturally have large gradients, $v_{i,t-1}$ will be large, resulting in a more lenient clipping threshold. Conversely, for parameters with small gradients, the threshold will be tighter. This preserves the fundamental benefit of RMSProp, which is to adapt the effective learning rate on a per-parameter basis according to its historical gradient scale. Furthermore, the problem specifies that the second-moment accumulator $v_{i,t}$ is updated using the *unmodified* gradient $g_{i,t}$. This is a crucial design choice. It ensures that $v_{i,t}$ remains an accurate, unbiased estimator of the true second moment of the gradients, including any spikes. If $v_{i,t}$ were updated with the clipped gradient $\\tilde{g}_{i,t}$, it would systematically underestimate the gradient variance, thereby compromising the integrity of the adaptive scaling itself over time.\n\n### Task B: Implementation and Test\n\nWe now proceed to the implementation and testing phase. Two optimizers, a baseline RMSProp and our scale-aware clipped variant, will be simulated on a synthetic convex quadratic objective. Their performance will be evaluated according to the specified predicates for exploding-update reduction and adaptive-benefit preservation across three test cases. The final program will encapsulate this entire simulation and evaluation process.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to run the complete simulation and print the final result.\n    It defines test cases and calls helper functions to perform the optimization\n    and evaluation logic.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Spike on the highest-curvature coordinate.\n        (3, [1.0, 0.1, 10.0], [1.0, 1.0, 1.0], 60, 0.05, 0.9, 1e-3, 2.0, {(10, 2): 200.0, (20, 2): -150.0}, 50.0, 0.5, 1e-12),\n        # Case 2: No spikes, testing behavior from initial state.\n        (3, [1.0, 0.3, 3.0], [1.5, -0.5, 0.75], 100, 0.05, 0.9, 1e-3, 5.0, {}, 1.0, 0.05, 1e-12),\n        # Case 3: Multiple spikes on different coordinates.\n        (4, [1.0, 0.2, 5.0, 0.05], [1.0, 1.0, 1.0, 1.0], 120, 0.04, 0.9, 1e-3, 1.5, {(5, 3): 300.0, (6, 3): -300.0, (30, 0): 150.0}, 30.0, 0.5, 1e-12),\n    ]\n\n    def _run_optimizer(d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, mode):\n        \"\"\"\n        Runs a single optimization trajectory for either baseline or clipped RMSProp.\n        \n        Returns:\n            - max_abs_step (float): The maximum absolute step magnitude over the trajectory.\n            - final_loss (float): The final value of the objective function.\n        \"\"\"\n        # Use np.float64 for higher numerical precision and stability.\n        theta = np.array(theta0, dtype=np.float64)\n        c_arr = np.array(c, dtype=np.float64)\n        v = np.zeros(d, dtype=np.float64)\n        max_abs_step = 0.0\n\n        for t_step in range(1, T + 1):\n            # Calculate the base gradient from the convex quadratic objective\n            grad_base = c_arr * theta\n            \n            # Construct the sparse spike vector for the current step\n            spikes = np.zeros(d, dtype=np.float64)\n            for i in range(d):\n                if (t_step, i) in spikes_dict:\n                    spikes[i] = spikes_dict.get((t_step, i), 0.0)\n            \n            # The observed gradient is the sum of the base gradient and the spike\n            grad_observed = grad_base + spikes\n\n            # Denominator for the RMSProp update\n            denominator = np.sqrt(v) + epsilon\n\n            if mode == 'baseline':\n                # Standard RMSProp update uses the observed gradient\n                step = -alpha * grad_observed / denominator\n            elif mode == 'clipped':\n                # Scale-aware clipping: threshold is proportional to the scale estimate\n                clip_threshold = lam * denominator\n                grad_clipped = np.clip(grad_observed, -clip_threshold, clip_threshold)\n                # The step uses the clipped gradient\n                step = -alpha * grad_clipped / denominator\n            else:\n                raise ValueError(\"Invalid optimizer mode specified.\")\n\n            # Track the maximum absolute per-parameter step magnitude\n            current_max_abs_step = np.max(np.abs(step))\n            if current_max_abs_step > max_abs_step:\n                max_abs_step = current_max_abs_step\n            \n            # Update parameters\n            theta += step\n            \n            # Update the second-moment accumulator using the unclipped gradient\n            v = beta * v + (1 - beta) * np.square(grad_observed)\n\n        # Calculate the final objective value\n        final_loss = 0.5 * np.sum(c_arr * np.square(theta))\n        \n        return max_abs_step, final_loss\n\n    def _run_simulation(case_params):\n        \"\"\"\n        Manages a single test case, running both optimizers and evaluating predicates.\n        \n        Returns:\n            - 1 if both predicates (E_j and P_j) are true, 0 otherwise.\n        \"\"\"\n        d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, tau, delta, tol = case_params\n\n        # Run both baseline and clipped optimizers\n        M_base, f_final_base = _run_optimizer(d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, 'baseline')\n        M_clip, f_final_clip = _run_optimizer(d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, 'clipped')\n\n        # Evaluate Predicate E_j: Exploding-update reduction\n        # Checks if the baseline step exploded while the clipped step remained bounded as derived.\n        e_j = (M_base > tau) and (M_clip = alpha * lam + tol)\n        \n        # Evaluate Predicate P_j: Adaptive-benefit preservation\n        # Checks if the clipped version's final loss is not substantially worse than the baseline.\n        p_j = f_final_clip = (1 + delta) * f_final_base\n\n        return 1 if e_j and p_j else 0\n\n    results = []\n    for case in test_cases:\n        result = _run_simulation(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3097005"}]}