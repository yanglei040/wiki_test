{"hands_on_practices": [{"introduction": "Before we can effectively tune a learning rate, we must first understand the fundamental condition that governs its stability. This practice explores the relationship between the learning rate $\\eta$ and the curvature of the loss function, which we model locally as a quadratic form. By deriving and numerically verifying the stability boundary for gradient descent, you will gain a foundational intuition for how the geometry of the loss landscape dictates the maximum permissible step size [@problem_id:3187300].", "problem": "Consider minimizing a twice-differentiable quadratic objective in deep learning, where the parameter vector is denoted by $\\theta \\in \\mathbb{R}^d$, and the loss is $f(\\theta)=\\tfrac{1}{2}\\theta^\\top A\\theta$ with a symmetric positive definite matrix $A \\in \\mathbb{R}^{d \\times d}$. Gradient Descent (GD) updates are defined by the iterative map $\\theta_{t+1}=\\theta_t-\\eta\\nabla f(\\theta_t)$ for learning rate $\\eta0$. The Spectral Radius (SR) of a matrix is defined as $\\rho(M)=\\max_i |\\lambda_i(M)|$, where $\\lambda_i(M)$ are the eigenvalues of $M$. Starting from these foundational definitions, derive the necessary and sufficient condition on the learning rate $\\eta$ for the GD iteration to be asymptotically stable (i.e., $\\lim_{t\\to\\infty}\\theta_t=\\mathbf{0}$ for any initial $\\theta_0$) and express the maximum asymptotically stable $\\eta$ in terms of the Spectral Radius of $A$. Then, verify this stability condition numerically by simulating GD along eigen-directions of $A$.\n\nYour program must:\n- For each test case matrix $A$, compute the Spectral Radius $\\rho(A)$ and the predicted maximum asymptotically stable learning rate, denoted $\\eta_{\\text{max}}$.\n- Using eigenvectors of $A$, simulate the GD update $\\theta_{t+1}=\\theta_t-\\eta A\\theta_t$ for $T$ steps with initial conditions chosen as individual eigen-directions, specifically $\\theta_0=v_i$ for each eigenvector $v_i$ of $A$.\n- For each of the three candidate learning rates per test case: a value strictly below the predicted threshold ($\\eta_{\\text{below}}=0.9\\,\\eta_{\\text{max}}$), the value at the threshold ($\\eta_{\\text{at}}=\\eta_{\\text{max}}$), and a value strictly above the threshold ($\\eta_{\\text{above}}=1.1\\,\\eta_{\\text{max}}$), report whether the GD iterates converge to the origin along all eigen-directions, that is, whether $\\|\\theta_T\\|\\le \\varepsilon$ holds for every eigen-direction when simulated for $T$ steps.\n- Angle unit specification: all angles that appear in the test definitions are in radians.\n\nUse the following test suite:\n1. Matrix $A_1=\\operatorname{diag}(1.0,4.0)$, so $d=2$.\n2. Matrix $A_2=Q_2 \\operatorname{diag}(0.5,1.5,3.0) Q_2^\\top$ with rotation $Q_2=\\begin{bmatrix}\\cos(\\pi/4)-\\sin(\\pi/4)0\\\\ \\sin(\\pi/4)\\cos(\\pi/4)0\\\\ 001\\end{bmatrix}$, so $d=3$.\n3. Matrix $A_3=Q_3 \\operatorname{diag}(0.1,2.0,10.0,20.0) Q_3^\\top$ where $Q_3$ is the product of two block rotations in $\\mathbb{R}^4$: a rotation by $\\phi=\\pi/6$ in the $(0,1)$-plane and a rotation by $\\psi=\\pi/3$ in the $(2,3)$-plane (both angles in radians), so $d=4$.\n\nSimulation parameters:\n- Number of GD steps $T=200$.\n- Convergence tolerance $\\varepsilon=10^{-6}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case $i\\in\\{1,2,3\\}$, append the following four values in order:\n- The predicted $\\eta_{\\text{max}}^{(i)}$ as a floating point number,\n- A boolean for stability at $\\eta_{\\text{below}}^{(i)}$,\n- A boolean for stability at $\\eta_{\\text{at}}^{(i)}$,\n- A boolean for stability at $\\eta_{\\text{above}}^{(i)}$.\n\nThus, the final output is a flat list of $12$ entries of the form $[\\eta_{\\text{max}}^{(1)}, s_{\\text{below}}^{(1)}, s_{\\text{at}}^{(1)}, s_{\\text{above}}^{(1)}, \\eta_{\\text{max}}^{(2)}, s_{\\text{below}}^{(2)}, s_{\\text{at}}^{(2)}, s_{\\text{above}}^{(2)}, \\eta_{\\text{max}}^{(3)}, s_{\\text{below}}^{(3)}, s_{\\text{at}}^{(3)}, s_{\\text{above}}^{(3)}]$, where each $s$ is a boolean indicating whether all eigen-direction simulations for that $\\eta$ satisfy $\\|\\theta_T\\|\\le \\varepsilon$.", "solution": "The user wants to find the stability condition for the learning rate $\\eta$ in a Gradient Descent (GD) optimization of a quadratic objective function and then numerically verify this condition.\n\n### Problem Validation\nThe problem statement is analyzed according to the specified criteria.\n\n**Step 1: Extract Givens**\n- Objective function: $f(\\theta)=\\tfrac{1}{2}\\theta^\\top A\\theta$, where $\\theta \\in \\mathbb{R}^d$.\n- Matrix $A$: a symmetric positive definite matrix, $A \\in \\mathbb{R}^{d \\times d}$.\n- Gradient Descent (GD) update rule: $\\theta_{t+1}=\\theta_t-\\eta\\nabla f(\\theta_t)$ for a learning rate $\\eta0$.\n- Spectral Radius: $\\rho(M)=\\max_i |\\lambda_i(M)|$, where $\\lambda_i(M)$ are the eigenvalues of matrix $M$.\n- Asymptotic stability condition: $\\lim_{t\\to\\infty}\\theta_t=\\mathbf{0}$ for any initial vector $\\theta_0$.\n- Test Matrices:\n  1. $A_1=\\operatorname{diag}(1.0,4.0)$, $d=2$.\n  2. $A_2=Q_2 \\operatorname{diag}(0.5,1.5,3.0) Q_2^\\top$ with $Q_2$ being a rotation by $\\pi/4$ in the $(0,1)$-plane of $\\mathbb{R}^3$.\n  3. $A_3=Q_3 \\operatorname{diag}(0.1,2.0,10.0,20.0) Q_3^\\top$ with $Q_3$ being a product of rotations by $\\phi=\\pi/6$ in the $(0,1)$-plane and $\\psi=\\pi/3$ in the $(2,3)$-plane of $\\mathbb{R}^4$.\n- Simulation Parameters:\n  - Number of steps: $T=200$.\n  - Convergence tolerance: $\\varepsilon=10^{-6}$.\n- Numerical Verification:\n  - Initial conditions: $\\theta_0=v_i$ for each eigenvector $v_i$ of $A$.\n  - Learning rates to test: $\\eta_{\\text{below}}=0.9\\,\\eta_{\\text{max}}$, $\\eta_{\\text{at}}=\\eta_{\\text{max}}$, $\\eta_{\\text{above}}=1.1\\,\\eta_{\\text{max}}$.\n  - Stability check: $\\|\\theta_T\\|\\le \\varepsilon$ must hold for simulations starting from every eigenvector.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a standard exercise in optimization theory and numerical linear algebra.\n- **Scientifically Grounded**: The problem is based on fundamental principles of calculus (gradients of quadratic forms) and linear algebra (eigenvalue analysis of linear dynamical systems). It is a simplification of the behavior of optimizers near a local minimum, which is a core concept in deep learning.\n- **Well-Posed**: The problem is clearly defined. It requests a derivation for a specific quantity ($\\eta_{\\text{max}}$) and a numerical verification with all parameters and matrices specified. This structure leads to a unique and meaningful solution.\n- **Objective**: The problem is stated in precise, mathematical terms, free from any subjectivity or ambiguity.\n\nThe problem does not exhibit any of the invalidity flaws. It is complete, consistent, scientifically sound, and well-posed.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of the Stability Condition\n\nThe objective function is a quadratic form $f(\\theta)=\\tfrac{1}{2}\\theta^\\top A\\theta$. The gradient of this function with respect to $\\theta$ is $\\nabla f(\\theta) = A\\theta$, since $A$ is symmetric.\n\nThe Gradient Descent (GD) update rule is given as:\n$$\n\\theta_{t+1}=\\theta_t-\\eta\\nabla f(\\theta_t)\n$$\nSubstituting the gradient, we obtain a linear iterative map:\n$$\n\\theta_{t+1} = \\theta_t - \\eta A\\theta_t = (I - \\eta A)\\theta_t\n$$\nwhere $I$ is the identity matrix. Let the matrix for the iteration be $M = I - \\eta A$. The update is then $\\theta_{t+1} = M\\theta_t$. After $t$ steps, the parameter vector is given by $\\theta_t = M^t \\theta_0$.\n\nFor the iterates to converge to the zero vector, $\\lim_{t\\to\\infty}\\theta_t=\\mathbf{0}$, for any arbitrary initial vector $\\theta_0$, it is necessary and sufficient that the spectral radius of the iteration matrix $M$ be strictly less than $1$. The spectral radius is defined as the maximum absolute value of its eigenvalues.\n$$\n\\rho(M)  1\n$$\nLet $\\lambda_i(A)$ be the eigenvalues of matrix $A$ with corresponding eigenvectors $v_i$. Since $A$ is a real symmetric matrix, its eigenvalues are real. The eigenvalues of the iteration matrix $M = I - \\eta A$ can be found by applying $M$ to an eigenvector $v_i$ of $A$:\n$$\nMv_i = (I - \\eta A)v_i = Iv_i - \\eta Av_i = v_i - \\eta\\lambda_i(A)v_i = (1 - \\eta\\lambda_i(A))v_i\n$$\nThus, the eigenvalues of $M$, denoted $\\lambda_i(M)$, are $\\lambda_i(M) = 1 - \\eta\\lambda_i(A)$.\n\nThe stability condition $\\rho(M)  1$ translates to:\n$$\n\\max_i |\\lambda_i(M)| = \\max_i |1 - \\eta\\lambda_i(A)|  1\n$$\nThis must hold for all eigenvalues $\\lambda_i(A)$ of $A$. This single inequality is equivalent to the pair of inequalities:\n$$\n-1  1 - \\eta\\lambda_i(A)  1 \\quad \\text{for all } i\n$$\nWe analyze each part of the inequality:\n1.  $1 - \\eta\\lambda_i(A)  1 \\implies -\\eta\\lambda_i(A)  0$. Since the learning rate $\\eta  0$ by definition, this simplifies to $\\lambda_i(A)  0$. This condition is guaranteed to be true for all $i$ because the problem states that matrix $A$ is positive definite.\n\n2.  $-1  1 - \\eta\\lambda_i(A) \\implies \\eta\\lambda_i(A)  2 \\implies \\eta  \\frac{2}{\\lambda_i(A)}$. This inequality must hold for all eigenvalues $\\lambda_i(A)$. To ensure this, $\\eta$ must be smaller than the minimum of all values of $\\frac{2}{\\lambda_i(A)}$. This is equivalent to choosing the largest eigenvalue, $\\lambda_{\\max}(A)$, as it provides the tightest constraint on $\\eta$:\n    $$\n    \\eta  \\frac{2}{\\lambda_{\\max}(A)}\n    $$\n\nThe problem defines the spectral radius of a matrix $M$ as $\\rho(M)=\\max_i |\\lambda_i(M)|$. For the symmetric positive definite matrix $A$, all eigenvalues $\\lambda_i(A)$ are positive, so its spectral radius is simply its largest eigenvalue: $\\rho(A) = \\lambda_{\\max}(A)$.\n\nCombining these findings, the necessary and sufficient condition for the GD iteration to be asymptotically stable is:\n$$\n0  \\eta  \\frac{2}{\\rho(A)}\n$$\nThe maximum asymptotically stable learning rate, $\\eta_{\\text{max}}$, is the supremum of this interval:\n$$\n\\eta_{\\text{max}} = \\frac{2}{\\rho(A)}\n$$\n\n### Numerical Verification\nThe program will implement this derivation. For each given matrix $A$, it will first compute its eigenvalues to find $\\rho(A) = \\lambda_{\\max}(A)$ and the corresponding $\\eta_{\\text{max}}$. Then, it will simulate the GD process for $T=200$ steps starting from each of the eigenvectors of $A$. This is done for three learning rates: $\\eta_{\\text{below}}=0.9\\,\\eta_{\\text{max}}$ (expected to converge), $\\eta_{\\text{at}}=\\eta_{\\text{max}}$ (expected to be unstable for the mode corresponding to $\\lambda_{\\max}(A)$), and $\\eta_{\\text{above}}=1.1\\,\\eta_{\\text{max}}$ (expected to diverge). The stability is numerically checked by verifying if the norm of the final iterate, $\\|\\theta_T\\|$, is less than or equal to a small tolerance $\\varepsilon=10^{-6}$.\n- For $\\eta_{\\text{below}}$, we expect $|1 - \\eta\\lambda_i(A)|  1$ for all $i$, leading to convergence.\n- For $\\eta_{\\text{at}}$, the mode corresponding to $\\lambda_{\\max}(A)$ will have an eigenvalue of $1 - \\frac{2}{\\lambda_{\\max}(A)}\\lambda_{\\max}(A) = -1$. The iterate magnitude for this mode will not decay, failing the convergence test.\n- For $\\eta_{\\text{above}}$, the mode corresponding to $\\lambda_{\\max}(A)$ will have an eigenvalue with magnitude greater than $1$, causing divergence.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and numerically verifies the stability condition for Gradient Descent\n    on a quadratic objective.\n    \"\"\"\n    # Simulation parameters from the problem statement\n    T = 200\n    eps = 1e-6\n    \n    # === Define Test Cases ===\n\n    # Test Case 1: A1 = diag(1.0, 4.0)\n    A1 = np.diag([1.0, 4.0])\n\n    # Test Case 2: A2 = Q2 * diag(0.5, 1.5, 3.0) * Q2^T\n    angle2 = np.pi / 4\n    c2, s2 = np.cos(angle2), np.sin(angle2)\n    Q2 = np.array([\n        [c2, -s2, 0],\n        [s2, c2, 0],\n        [0, 0, 1]\n    ])\n    D2 = np.diag([0.5, 1.5, 3.0])\n    A2 = Q2 @ D2 @ Q2.T\n\n    # Test Case 3: A3 = Q3 * diag(0.1, 2.0, 10.0, 20.0) * Q3^T\n    phi = np.pi / 6\n    psi = np.pi / 3\n    c_phi, s_phi = np.cos(phi), np.sin(phi)\n    c_psi, s_psi = np.cos(psi), np.sin(psi)\n    \n    # Rotation in (0,1)-plane\n    R01 = np.array([\n        [c_phi, -s_phi, 0, 0],\n        [s_phi, c_phi, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1]\n    ])\n    \n    # Rotation in (2,3)-plane\n    R23 = np.array([\n        [1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, c_psi, -s_psi],\n        [0, 0, s_psi, c_psi]\n    ])\n    \n    Q3 = R01 @ R23\n    D3 = np.diag([0.1, 2.0, 10.0, 20.0])\n    A3 = Q3 @ D3 @ Q3.T\n    \n    test_cases = [A1, A2, A3]\n    all_results = []\n\n    for A in test_cases:\n        # Since A is symmetric, eigh is preferred for numerical stability\n        # and guarantees orthogonal eigenvectors and real eigenvalues.\n        eigenvalues, eigenvectors = np.linalg.eigh(A)\n        \n        # The spectral radius of a symmetric positive definite matrix is its largest eigenvalue.\n        rho_A = np.max(eigenvalues)\n        \n        # The predicted maximum stable learning rate from the derivation.\n        eta_max = 2.0 / rho_A\n        all_results.append(eta_max)\n        \n        etas_to_test = {\n            'below': 0.9 * eta_max,\n            'at': eta_max,\n            'above': 1.1 * eta_max\n        }\n        \n        # Test stability for each candidate learning rate\n        for key in ['below', 'at', 'above']:\n            eta = etas_to_test[key]\n            is_stable_for_this_eta = True\n            \n            # Dimension of the parameter space\n            d = A.shape[0]\n            \n            # Iterate through each eigen-direction as an initial condition\n            for i in range(d):\n                theta_0 = eigenvectors[:, i]\n                theta = theta_0.copy()\n                \n                # Perform T steps of Gradient Descent\n                for _ in range(T):\n                    theta = theta - eta * (A @ theta)\n                \n                # Check if the final state has converged to the origin\n                final_norm = np.linalg.norm(theta)\n                if final_norm > eps:\n                    is_stable_for_this_eta = False\n                    # If one direction is unstable, no need to check others for this eta\n                    break\n            \n            all_results.append(is_stable_for_this_eta)\n            \n    # Final print statement in the exact required format.\n    # The default str() for booleans ('True'/'False') is used.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3187300"}, {"introduction": "Standard gradient descent can be slow in navigating long, shallow valleys in the loss landscape, so momentum is a powerful technique that accelerates convergence by accumulating a velocity in consistent gradient directions. This practice guides you to derive and test the concept of an 'effective learning rate' in momentum-based optimizers, revealing that the actual step size scales by a factor of $\\frac{1}{1-\\beta}$ in steady state. Mastering this insight is crucial for effectively tuning optimizers like SGD with momentum [@problem_id:3187263].", "problem": "Consider Gradient Descent with Momentum (GDM), also known as the heavy-ball method. Let a parameter vector be denoted by $w_t \\in \\mathbb{R}^d$ at iteration $t$, the gradient by $g_t \\in \\mathbb{R}^d$, the momentum coefficient by $\\beta \\in [0,1)$, and the learning rate by $\\eta  0$. The update equations are the following fundamental definitions: velocity $v_t \\in \\mathbb{R}^d$ obeys $v_{t+1} = \\beta v_t + g_t$, and parameters update as $w_{t+1} = w_t - \\eta v_{t+1}$. Assume $v_0 = 0$ and $w_0$ is given.\n\nTask 1 (derivation): Starting solely from these update equations, and without assuming any shortcut formulas, derive the steady-state per-iteration parameter update under a constant gradient regime where $g_t = g$ is a fixed nonzero vector, and $\\beta \\in [0,1)$. Express your result in terms of $g$, $\\eta$, and $\\beta$, and provide the mathematical conditions under which the derivation is valid.\n\nTask 2 (computational test): In one dimension ($d = 1$), implement a program that simulates GDM under a constant scalar gradient $g \\in \\mathbb{R}$. For a given $(\\beta,\\eta)$ pair, run $T$ iterations starting from $v_0 = 0$ and $w_0 = 0$, record the per-iteration update magnitudes $\\Delta_t = |w_{t+1} - w_t|$, and compute the average over the last $K$ iterations. Define the notion of equalized late-stage training dynamics across different $\\beta$ values as follows: for a set of $\\beta$ values, if the maximum minus minimum of the computed averages (over the last $K$ iterations) is less than or equal to a tolerance $\\varepsilon$, then the dynamics are considered equalized.\n\nTest Suite: Your program must evaluate the following four test cases and output a boolean for each case indicating whether late-stage training dynamics are equalized according to the above criterion. All scalar numbers in the test suite below are dimensionless.\n\n- Test 1 (general case): Constant gradient $g = 1.2$, momentum coefficients $\\beta \\in \\{0.0, 0.5, 0.9\\}$, target steady-state scaling parameter $c = 0.05$, choose learning rates to satisfy $\\eta = c \\cdot (1 - \\beta)$ for each $\\beta$, number of iterations $T = 200$, last-window length $K = 50$, tolerance $\\varepsilon = 10^{-6}$.\n\n- Test 2 (boundary near $\\beta \\to 1$): Constant gradient $g = 0.7$, momentum coefficients $\\beta \\in \\{0.0, 0.99\\}$, choose $\\eta = c \\cdot (1 - \\beta)$ with $c = 0.1$, number of iterations $T = 3000$, last-window length $K = 500$, tolerance $\\varepsilon = 10^{-6}$.\n\n- Test 3 (mismatched learning rates): Constant gradient $g = 1.0$, momentum coefficients $\\beta \\in \\{0.8, 0.9\\}$, use the same learning rate $\\eta = 0.01$ for both $\\beta$ values (do not adjust by $(1 - \\beta)$), number of iterations $T = 200$, last-window length $K = 50$, tolerance $\\varepsilon = 10^{-6}$.\n\n- Test 4 (zero gradient edge case): Constant gradient $g = 0.0$, momentum coefficients $\\beta \\in \\{0.0, 0.95, 0.99\\}$, choose $\\eta = c \\cdot (1 - \\beta)$ with $c = 0.2$, number of iterations $T = 100$, last-window length $K = 50$, tolerance $\\varepsilon = 10^{-9}$.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4]$, where each $r_i$ is a boolean corresponding to Test $i$ in the order listed above.", "solution": "The problem statement has been validated and found to be scientifically sound, well-posed, and complete. It describes a standard optimization algorithm, Gradient Descent with Momentum (GDM), and poses two tasks: a mathematical derivation of its steady-state behavior under a constant gradient, and a computational test to verify this behavior under specific hyperparameter settings. All parameters and conditions are clearly defined.\n\n### Task 1: Derivation of Steady-State Update\n\nThe GDM update equations are given as:\n$$v_{t+1} = \\beta v_t + g_t$$\n$$w_{t+1} = w_t - \\eta v_{t+1}$$\nwith initial conditions $v_0 = 0$ and a given $w_0$. The parameters are the momentum coefficient $\\beta \\in [0,1)$ and the learning rate $\\eta  0$. We are asked to analyze the system under a constant gradient regime, where $g_t = g$ for all iterations $t$, with $g$ being a fixed non-zero vector.\n\nThe goal is to find the steady-state per-iteration parameter update, which we denote as $\\Delta w_{ss} = \\lim_{t \\to \\infty} (w_{t+1} - w_t)$. The parameter update at any iteration $t$ is $\\Delta w_t = w_{t+1} - w_t = -\\eta v_{t+1}$. Therefore, deriving the steady-state update requires finding the steady-state velocity, $v_{ss} = \\lim_{t \\to \\infty} v_t$.\n\nLet's analyze the recurrence relation for the velocity vector $v_t$:\n$$v_{t+1} = \\beta v_t + g$$\nWe can unroll this recurrence starting from $v_0 = 0$:\nAt $t=0$: $v_1 = \\beta v_0 + g = \\beta(0) + g = g$\nAt $t=1$: $v_2 = \\beta v_1 + g = \\beta g + g = (1 + \\beta)g$\nAt $t=2$: $v_3 = \\beta v_2 + g = \\beta(1+\\beta)g + g = (1 + \\beta + \\beta^2)g$\n\nBy induction, we can see the general form for $v_t$ at any iteration $t  0$:\n$$v_t = \\left(\\sum_{i=0}^{t-1} \\beta^i\\right) g$$\nThe summation is a finite geometric series. For $\\beta \\neq 1$, the sum is given by:\n$$\\sum_{i=0}^{t-1} \\beta^i = \\frac{1 - \\beta^t}{1 - \\beta}$$\nThus, the velocity at iteration $t$ is:\n$$v_t = \\frac{1 - \\beta^t}{1 - \\beta} g$$\nTo find the steady-state velocity $v_{ss}$, we take the limit as $t \\to \\infty$:\n$$v_{ss} = \\lim_{t \\to \\infty} v_t = \\lim_{t \\to \\infty} \\left(\\frac{1 - \\beta^t}{1 - \\beta}\\right) g$$\nThis limit converges if and only if the term $\\beta^t$ converges. The problem states that $\\beta \\in [0, 1)$, which is a stricter condition than $|\\beta|  1$ but sufficient. Under this condition, $\\lim_{t \\to \\infty} \\beta^t = 0$.\n\nTherefore, the steady-state velocity is:\n$$v_{ss} = \\frac{1 - 0}{1 - \\beta} g = \\frac{g}{1 - \\beta}$$\nThe velocity exponentially approaches this terminal value. The term $1/(1-\\beta)$ acts as a scalar that amplifies the gradient $g$.\n\nNow, we can find the steady-state per-iteration parameter update, $\\Delta w_{ss}$. In the steady state, $v_{t+1} \\approx v_{ss}$, so:\n$$\\Delta w_{ss} = w_{t+1} - w_t = -\\eta v_{ss}$$\nSubstituting the expression for $v_{ss}$, we get:\n$$\\Delta w_{ss} = -\\eta \\left(\\frac{g}{1 - \\beta}\\right)$$\nThis is the derived steady-state per-iteration parameter update. The derivation is valid under the condition that the geometric series for the velocity converges, which is guaranteed by the given constraint $\\beta \\in [0, 1)$.\n\n### Task 2: Computational Test and Analysis\n\nThe computational task requires simulating GDM in one dimension ($d=1$) and evaluating a criterion for \"equalized late-stage training dynamics.\" This criterion checks if the average update magnitudes, $\\bar{\\Delta} = \\text{avg}_{t \\in \\{T-K, \\dots, T-1\\}} |w_{t+1} - w_t|$, are similar across different values of $\\beta$.\n\nFrom our derivation, the magnitude of the steady-state update is:\n$$|\\Delta w_{ss}| = \\left|-\\eta \\frac{g}{1 - \\beta}\\right| = \\frac{\\eta |g|}{1 - \\beta}$$\nThis formula is key to understanding the test cases.\n\n**Analysis of Test Cases 1, 2, and 4:**\nIn these tests, the learning rate $\\eta$ is coupled with the momentum coefficient $\\beta$ via the rule $\\eta = c \\cdot (1 - \\beta)$ for some constant $c$. Let's substitute this into our steady-state update magnitude formula:\n$$|\\Delta w_{ss}| = \\frac{(c \\cdot (1 - \\beta)) |g|}{1 - \\beta} = c|g|$$\nThis result is profound: when $\\eta$ is scaled by $(1 - \\beta)$, the steady-state update magnitude becomes independent of $\\beta$. It depends only on the constant scalar $c$ and the magnitude of the gradient $g$.\n\n- **Test 1 ($g=1.2, c=0.05$)  Test 2 ($g=0.7, c=0.1$):** For these tests, since $g \\neq 0$, we expect the system for each $\\beta$ to converge to a non-zero steady-state update magnitude of $c|g|$. For Test 1, $|\\Delta w_{ss}| = 0.05 \\times 1.2 = 0.06$. For Test 2, $|\\Delta w_{ss}| = 0.1 \\times 0.7 = 0.07$. Since this theoretical value is the same for all $\\beta$ values within a given test, we expect the difference between the maximum and minimum of the simulated average update magnitudes to be very small (within the tolerance $\\varepsilon$). The number of iterations $T$ is chosen to be much larger than the characteristic convergence time scale (which is on the order of $1/(1-\\beta)$), ensuring the system is in steady state during the final $K$ iterations. Therefore, we predict the dynamics will be equalized, and the result should be `True` for both tests.\n\n- **Test 4 ($g=0.0, c=0.2$):** Here, the gradient is zero. The velocity update is $v_{t+1} = \\beta v_t$. With $v_0 = 0$, it follows that $v_t = 0$ for all $t \\geq 1$. Consequently, the parameter update $w_{t+1} - w_t = -\\eta v_{t+1}$ is always zero. The average update magnitude over the last $K$ iterations will be $0$ for all values of $\\beta$. The difference between the maximum ($0$) and minimum ($0$) average is $0$, which is less than or equal to the tolerance $\\varepsilon = 10^{-9}$. We predict the result to be `True`.\n\n**Analysis of Test Case 3:**\nIn this test, the learning rate $\\eta = 0.01$ is held constant for two different $\\beta$ values, $\\beta_1 = 0.8$ and $\\beta_2 = 0.9$. The rule $\\eta = c(1-\\beta)$ is not used. We must use the general formula for the steady-state update magnitude: $|\\Delta w_{ss}| = \\frac{\\eta |g|}{1 - \\beta}$.\n\n- For $\\beta_1 = 0.8$: $|\\Delta w_{ss}| = \\frac{0.01 \\times |1.0|}{1 - 0.8} = \\frac{0.01}{0.2} = 0.05$.\n- For $\\beta_2 = 0.9$: $|\\Delta w_{ss}| = \\frac{0.01 \\times |1.0|}{1 - 0.9} = \\frac{0.01}{0.1} = 0.10$.\n\nThe theoretical steady-state update magnitudes are significantly different ($0.05$ vs $0.10$). The difference is $0.05$, which is much larger than the tolerance $\\varepsilon = 10^{-6}$. Therefore, we predict the dynamics will not be equalized, and the result for this test should be `False`.\n\n**Summary of Predictions:**\n- Test 1: `True`\n- Test 2: `True`\n- Test 3: `False`\n- Test 4: `True`\n\nThe following program implements the simulation to verify these predictions.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_gdm_simulation(g: float, beta: float, eta: float, T: int, K: int) -> float:\n    \"\"\"\n    Simulates Gradient Descent with Momentum for a constant scalar gradient.\n\n    Args:\n        g: The constant scalar gradient.\n        beta: The momentum coefficient.\n        eta: The learning rate.\n        T: The total number of iterations.\n        K: The number of last iterations to average over.\n\n    Returns:\n        The average update magnitude over the last K iterations.\n    \"\"\"\n    v = 0.0\n    update_magnitudes = []\n\n    for _ in range(T):\n        # Velocity update: v_{t+1} = beta * v_t + g_t\n        # In our case, v_t is the current v, and g_t is the constant g.\n        v = beta * v + g\n\n        # Parameter update magnitude: |w_{t+1} - w_t| = |-eta * v_{t+1}|\n        # The newly computed v is effectively v_{t+1}.\n        update_mag = eta * abs(v)\n        update_magnitudes.append(update_mag)\n        # Note: We do not need to track the parameter w itself, as only the\n        # magnitude of its change is required for the analysis.\n\n    # Calculate the average over the last K iterations.\n    # The slice update_magnitudes[T-K:] extracts the last K elements.\n    avg_update = np.mean(update_magnitudes[T - K:])\n    return avg_update\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for GDM dynamics equalization.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test 1 (general case)\n        {\n            'g': 1.2, \n            'betas': [0.0, 0.5, 0.9], \n            'eta_rule': 'c(1-beta)', \n            'c': 0.05,\n            'T': 200, \n            'K': 50, \n            'eps': 1e-6\n        },\n        # Test 2 (boundary near beta -> 1)\n        {\n            'g': 0.7, \n            'betas': [0.0, 0.99], \n            'eta_rule': 'c(1-beta)',\n            'c': 0.1,\n            'T': 3000, \n            'K': 500, \n            'eps': 1e-6\n        },\n        # Test 3 (mismatched learning rates)\n        {\n            'g': 1.0, \n            'betas': [0.8, 0.9], \n            'eta_rule': 'fixed',\n            'eta': 0.01,\n            'T': 200, \n            'K': 50, \n            'eps': 1e-6\n        },\n        # Test 4 (zero gradient edge case)\n        {\n            'g': 0.0, \n            'betas': [0.0, 0.95, 0.99], \n            'eta_rule': 'c(1-beta)',\n            'c': 0.2,\n            'T': 100, \n            'K': 50, \n            'eps': 1e-9\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        averages = []\n        for beta in case['betas']:\n            # Determine the learning rate based on the rule for the test case\n            if case['eta_rule'] == 'c(1-beta)':\n                eta = case['c'] * (1.0 - beta)\n            else:  # 'fixed' rule\n                eta = case['eta']\n            \n            # Run the simulation and get the average update magnitude\n            avg_mag = run_gdm_simulation(\n                g=case['g'], \n                beta=beta, \n                eta=eta, \n                T=case['T'], \n                K=case['K']\n            )\n            averages.append(avg_mag)\n        \n        # Check if the dynamics are equalized\n        if len(averages) > 0:\n            diff = max(averages) - min(averages)\n            is_equalized = diff = case['eps']\n        else:\n            is_equalized = True # Vacuously true for empty set of betas\n            \n        results.append(is_equalized)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3187263"}, {"introduction": "In traditional implementations, $L_2$ regularization is often intertwined with the gradient update, causing its effective strength to depend on the learning rate. Modern optimizers often use 'decoupled weight decay' to resolve this, applying the weight penalty as a separate shrinking step. This hands-on exercise will have you derive and verify that the effective regularization strength in such a scheme scales with the ratio $\\frac{\\lambda}{\\eta}$, a critical insight for robust hyperparameter tuning in models like Transformers [@problem_id:3187375].", "problem": "You are asked to formalize, derive, and verify the scaling of the effective quadratic parameter penalty when using decoupled weight decay in gradient-based optimization. Consider a one-dimensional quadratic loss and a decoupled weight decay update that applies a multiplicative shrink to the parameter followed by a gradient step. Begin from the fundamental base of empirical risk minimization and first principles for gradient descent dynamics, then derive the fixed point of the iterative update. Use the derived expression to determine how the effective quadratic penalty depends on the learning rate and the decoupled decay parameter, and design a numerical program to verify the prediction across a test suite.\n\nAll mathematical entities must be written in LaTeX. Let the loss be a strictly convex quadratic given by\n$$\n\\mathcal{L}(w) = \\frac{1}{2} a \\left(w - w^\\star\\right)^2,\n$$\nwith curvature $a  0$ and minimizer $w^\\star$. Define the decoupled weight decay update as the composition of a multiplicative shrink by a factor $(1 - \\lambda)$ applied to the parameter $w$, followed by a gradient descent step with learning rate $\\eta  0$ on the original loss $\\mathcal{L}(w)$ (no penalty term is added to $\\mathcal{L}$). The resulting discrete-time update is\n$$\nw_{t+1} = (1 - \\lambda) w_t - \\eta \\, \\nabla \\mathcal{L}(w_t).\n$$\n\nTasks:\n1) Starting from the definition of gradient descent on a differentiable function and the stated decoupled weight decay scheme, derive a closed-form expression for the fixed point $w_\\infty$ of the iteration when it converges, in terms of $a$, $w^\\star$, $\\eta$, and $\\lambda$. Express your answer explicitly and justify the convergence condition in terms of the magnitude of the linear coefficient.\n\n2) Using your fixed point expression, show that the decoupled shrink plus gradient step is equivalent, at its fixed point, to minimizing a quadratic objective of the form\n$$\n\\mathcal{L}(w) + \\frac{\\alpha}{2}\\,w^2,\n$$\nfor some effective penalty strength $\\alpha$ that depends on $\\lambda$ and $\\eta$. Solve for $\\alpha$ and simplify your expression as much as possible. Explain why keeping the ratio $\\lambda / \\eta$ fixed while varying $\\eta$ leaves the fixed point unchanged for a given $a$ and $w^\\star$, thereby isolating the pure effect of the learning rate on the transient dynamics rather than on the regularization bias.\n\n3) Implement a program that numerically simulates the stated update for a finite number of steps and verifies the derived predictions on the following test suite. In all cases use $a = 3$, $w^\\star = 2$, $w_0 = 10$ and run $T = 500$ iterations:\n- Case A (happy path, moderate settings): $\\eta = 0.1$, $\\lambda = 0.02$.\n- Case B (same $\\lambda / \\eta$ as Case A but different $\\eta$): $\\eta = 0.05$, $\\lambda = 0.01$.\n- Case C (different $\\lambda / \\eta$ from Case A, same $\\eta$ as Case A): $\\eta = 0.1$, $\\lambda = 0.01$.\n- Case D (boundary case of no decay): $\\eta = 0.1$, $\\lambda = 0$.\n\nFor each case, define the iteration\n$$\nw_{t+1} = (1 - \\lambda)\\,w_t - \\eta\\,a\\,(w_t - w^\\star),\n$$\nand compute:\n- $r_1$: The absolute difference between the final parameters of Case A and Case B after $T$ steps, $|w_T^{\\mathrm{A}} - w_T^{\\mathrm{B}}|$. This tests invariance when $\\lambda / \\eta$ is fixed but $\\eta$ changes.\n- $r_2$: The maximum absolute discrepancy across Cases A, B, and C between the simulated final parameter and the predicted fixed point value $w_\\infty = \\frac{a}{a + \\lambda / \\eta} w^\\star$. Formally,\n$$\nr_2 = \\max\\Big(\\big|w_T^{\\mathrm{A}} - \\tfrac{a}{a + \\lambda_{\\mathrm{A}} / \\eta_{\\mathrm{A}}} w^\\star\\big|,\\ \\big|w_T^{\\mathrm{B}} - \\tfrac{a}{a + \\lambda_{\\mathrm{B}} / \\eta_{\\mathrm{B}}} w^\\star\\big|,\\ \\big|w_T^{\\mathrm{C}} - \\tfrac{a}{a + \\lambda_{\\mathrm{C}} / \\eta_{\\mathrm{C}}} w^\\star\\big|\\Big).\n$$\n- $r_3$: The absolute difference between the final parameters of Case A and Case C after $T$ steps, $|w_T^{\\mathrm{A}} - w_T^{\\mathrm{C}}|$. This tests that changing $\\lambda / \\eta$ changes the fixed point.\n- $r_4$: The absolute error in Case D relative to the unregularized minimizer, $|w_T^{\\mathrm{D}} - w^\\star|$.\n\nAll numerical answers must be unitless real numbers. The final output format must be a single line containing the results as a comma-separated list enclosed in square brackets, in the order $[r_1, r_2, r_3, r_4]$.\n\nYour program must be a complete, runnable program that produces exactly this single-line output with no additional text.", "solution": "The problem is valid as it is scientifically grounded in the principles of gradient-based optimization, is well-posed with a clear objective and sufficient data, and is expressed in precise, objective language. We will proceed with the derivation and implementation.\n\nThe problem asks for an analysis of decoupled weight decay for a simple one-dimensional quadratic loss function. The solution is structured into three parts as requested: derivation of the fixed point and convergence condition, derivation of the effective quadratic penalty, and numerical verification.\n\n### Task 1: Fixed Point Derivation and Convergence\n\nWe are given the loss function $\\mathcal{L}(w) = \\frac{1}{2} a (w - w^\\star)^2$, where $a  0$. The gradient of this loss with respect to the parameter $w$ is:\n$$\n\\nabla \\mathcal{L}(w) = \\frac{d\\mathcal{L}}{dw} = a (w - w^\\star)\n$$\nThe update rule for the parameter $w$ at each discrete time step $t$ is defined as a composition of a multiplicative shrink and a gradient descent step:\n$$\nw_{t+1} = (1 - \\lambda) w_t - \\eta \\, \\nabla \\mathcal{L}(w_t)\n$$\nwhere $\\lambda$ is the decoupled weight decay parameter and $\\eta  0$ is the learning rate.\n\nSubstituting the expression for the gradient into the update rule gives:\n$$\nw_{t+1} = (1 - \\lambda) w_t - \\eta a (w_t - w^\\star)\n$$\nTo find the fixed point of this iteration, denoted by $w_\\infty$, we seek a value of $w$ such that $w_{t+1} = w_t = w_\\infty$. At the fixed point, the update rule becomes an algebraic equation:\n$$\nw_\\infty = (1 - \\lambda) w_\\infty - \\eta a (w_\\infty - w^\\star)\n$$\nWe can now solve for $w_\\infty$:\n$$\nw_\\infty = w_\\infty - \\lambda w_\\infty - \\eta a w_\\infty + \\eta a w^\\star\n$$\nSubtracting $w_\\infty$ from both sides yields:\n$$\n0 = - \\lambda w_\\infty - \\eta a w_\\infty + \\eta a w^\\star\n$$\nGrouping the terms containing $w_\\infty$:\n$$\n(\\lambda + \\eta a) w_\\infty = \\eta a w^\\star\n$$\nAssuming $\\lambda + \\eta a \\neq 0$, which is true since $a  0$, $\\eta  0$, and we consider $\\lambda \\ge 0$, we can isolate $w_\\infty$:\n$$\nw_\\infty = \\frac{\\eta a}{\\lambda + \\eta a} w^\\star\n$$\nTo facilitate the analysis in the next part, we can divide the numerator and denominator by $\\eta$:\n$$\nw_\\infty = \\frac{a}{(\\lambda / \\eta) + a} w^\\star\n$$\nThis is the closed-form expression for the fixed point of the iteration.\n\nFor the iteration to converge to this fixed point, we analyze the update rule as a linear recurrence relation. Let's rearrange the terms in the update rule to isolate $w_t$:\n$$\nw_{t+1} = (1 - \\lambda - \\eta a) w_t + \\eta a w^\\star\n$$\nThis is a first-order linear recurrence relation of the form $w_{t+1} = C w_t + D$, where the coefficient $C = 1 - \\lambda - \\eta a$ and the constant term $D = \\eta a w^\\star$. Such an iteration converges to a unique fixed point if and only if the magnitude of the linear coefficient $C$ is strictly less than $1$, i.e., $|C|  1$.\nTherefore, the condition for convergence is:\n$$\n|1 - \\lambda - \\eta a|  1\n$$\nThis inequality can be expanded into two separate inequalities:\n$1$) $1 - \\lambda - \\eta a  1 \\implies -\\lambda - \\eta a  0 \\implies \\lambda + \\eta a  0$. Since $a  0$, $\\eta  0$, and $\\lambda \\ge 0$, this condition is always fulfilled.\n$2$) $1 - \\lambda - \\eta a  -1 \\implies 2  \\lambda + \\eta a$.\n\nCombining these, the necessary and sufficient condition for convergence is $0  \\lambda + \\eta a  2$. All numerical cases provided in the problem statement satisfy this condition.\n\n### Task 2: Effective Quadratic Penalty\n\nWe now show that the fixed point $w_\\infty$ is equivalent to the minimizer of an objective function with an added quadratic penalty term. Consider the regularized objective:\n$$\n\\mathcal{L}_{eff}(w) = \\mathcal{L}(w) + \\frac{\\alpha}{2} w^2 = \\frac{1}{2} a (w - w^\\star)^2 + \\frac{\\alpha}{2} w^2\n$$\nTo find the minimizer of $\\mathcal{L}_{eff}(w)$, we set its first derivative with respect to $w$ to zero:\n$$\n\\frac{d\\mathcal{L}_{eff}}{dw} = \\frac{d}{dw} \\left( \\frac{1}{2} a (w - w^\\star)^2 + \\frac{\\alpha}{2} w^2 \\right) = 0\n$$\n$$\na (w - w^\\star) + \\alpha w = 0\n$$\nSolving for $w$:\n$$\na w - a w^\\star + \\alpha w = 0\n$$\n$$\n(a + \\alpha) w = a w^\\star\n$$\nThe minimizer of the regularized objective is:\n$$\nw = \\frac{a}{a + \\alpha} w^\\star\n$$\nFor this minimizer to be identical to the fixed point $w_\\infty$ of the decoupled weight decay update, their expressions must be equal:\n$$\n\\frac{a}{a + \\alpha} w^\\star = \\frac{a}{a + (\\lambda / \\eta)} w^\\star\n$$\nAssuming $w^\\star \\neq 0$ and $a \\neq 0$, we can equate the denominators of the coefficients:\n$$\na + \\alpha = a + \\frac{\\lambda}{\\eta}\n$$\nThis gives the effective quadratic penalty strength $\\alpha$ as:\n$$\n\\alpha = \\frac{\\lambda}{\\eta}\n$$\nThis result demonstrates that decoupled weight decay with parameter $\\lambda$ and learning rate $\\eta$ is equivalent, at its fixed point, to standard $L_2$ regularization (or weight decay) with a penalty strength of $\\lambda / \\eta$.\n\nThe expression for the fixed point, $w_\\infty = \\frac{a}{a + \\lambda / \\eta} w^\\star$, depends on $\\lambda$ and $\\eta$ only through their ratio $\\lambda / \\eta$. Consequently, if we vary $\\eta$ and $\\lambda$ such that their ratio $\\lambda / \\eta$ is held constant, the fixed point $w_\\infty$ remains unchanged for a given curvature $a$ and unregularized minimizer $w^\\star$. The learning rate $\\eta$ still influences the transient dynamics of the optimization processâ€”specifically, the rate of convergence, which is governed by $|1 - \\lambda - \\eta a|$. By keeping $\\lambda / \\eta$ constant, one can adjust the learning rate $\\eta$ to tune the convergence speed without altering the final regularized solution (the regularization bias). This decoupling is a key advantage of this optimization scheme.\n\n### Task 3: Numerical Verification\n\nThe third task requires a numerical program to simulate the update rule for several test cases and verify the theoretical predictions derived above. The program will implement the iteration $w_{t+1} = (1 - \\lambda)\\,w_t - \\eta\\,a\\,(w_t - w^\\star)$ for the specified parameters and compute the required metrics $r_1$, $r_2$, $r_3$, and $r_4$. The implementation will be provided in the final answer block.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs numerical simulation of decoupled weight decay to verify theoretical predictions.\n    \"\"\"\n    # Define common parameters and test cases from the problem statement.\n    a = 3.0\n    w_star = 2.0\n    w0 = 10.0\n    T = 500\n\n    test_cases_params = {\n        'A': {'eta': 0.1, 'lambda': 0.02},\n        'B': {'eta': 0.05, 'lambda': 0.01},\n        'C': {'eta': 0.1, 'lambda': 0.01},\n        'D': {'eta': 0.1, 'lambda': 0.0}\n    }\n\n    # --- Simulation Phase ---\n    # Store the final parameter value for each case after T iterations.\n    final_ws = {}\n    for case_id, params in test_cases_params.items():\n        eta = params['eta']\n        lam = params['lambda']\n        \n        w = w0\n        for _ in range(T):\n            # The decoupled weight decay update rule for a quadratic loss.\n            w = (1.0 - lam) * w - eta * a * (w - w_star)\n        \n        final_ws[case_id] = w\n\n    # --- Verification Phase ---\n    # Calculate the required metrics r1, r2, r3, r4.\n\n    # r1: Absolute difference between Case A and Case B final parameters.\n    # Theoretical expectation: r1 should be very small as they share the same fixed point.\n    r1 = np.abs(final_ws['A'] - final_ws['B'])\n\n    # r2: Maximum absolute discrepancy between simulated and theoretical fixed points for A, B, C.\n    # The theoretical fixed point is w_inf = (a / (a + lambda/eta)) * w_star\n    w_inf_A = a / (a + test_cases_params['A']['lambda'] / test_cases_params['A']['eta']) * w_star\n    w_inf_B = a / (a + test_cases_params['B']['lambda'] / test_cases_params['B']['eta']) * w_star\n    w_inf_C = a / (a + test_cases_params['C']['lambda'] / test_cases_params['C']['eta']) * w_star\n    \n    err_A = np.abs(final_ws['A'] - w_inf_A)\n    err_B = np.abs(final_ws['B'] - w_inf_B)\n    err_C = np.abs(final_ws['C'] - w_inf_C)\n    \n    r2 = max(err_A, err_B, err_C)\n\n    # r3: Absolute difference between Case A and Case C final parameters.\n    # Theoretical expectation: r3 should be non-trivial as their fixed points differ.\n    r3 = np.abs(final_ws['A'] - final_ws['C'])\n    \n    # r4: Absolute error in Case D relative to the unregularized minimizer w_star.\n    # Case D is standard gradient descent, which should converge to w_star.\n    r4 = np.abs(final_ws['D'] - w_star)\n    \n    results = [r1, r2, r3, r4]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3187375"}]}