{"hands_on_practices": [{"introduction": "To truly understand the vanishing gradient problem, it's essential to go back to its origins in deep networks with sigmoidal activation functions. This practice asks you to quantify the two primary culprits: neuronal saturation and the multiplicative effect of the chain rule. By implementing metrics for the fraction of saturated units and the overall gradient decay factor along a path, you will gain a concrete, computational intuition for how and why gradients diminish to the point of stalling learning [@problem_id:3194533]. This exercise makes the abstract theory tangible, allowing you to directly measure the phenomenon.", "problem": "You are given a deep network composed of a chain of logistic sigmoid activations. Let the logistic sigmoid activation be defined by the function $\\sigma(z)$, where $z \\in \\mathbb{R}$ denotes the pre-activation input to a unit. Consider a one-dimensional path through a deep autoencoder consisting of $L$ layers, where the forward map along this path is a composition of affine scalings and sigmoids. Your tasks are:\n\n1) Starting only from the definition of the logistic sigmoid and the chain rule of calculus, argue why for large-magnitude pre-activations $\\lvert z \\rvert \\gg 0$, the derivative $\\sigma^{\\prime}(z)$ becomes close to $0$, and explain how this leads to a vanishing gradient along a deep chain of sigmoids.\n\n2) Quantify training stall using two metrics:\n- The fraction of units in saturation, defined with respect to a threshold $t > 0$ as the proportion of pre-activations $z$ that satisfy $\\lvert z \\rvert \\ge t$.\n- The gradient norm decay factor along a one-dimensional path, defined as the product over layers of the absolute scaling $\\lvert a_{\\ell} \\rvert$ and the activation derivative $\\sigma^{\\prime}(z_{\\ell})$, where $a_{\\ell} \\in \\mathbb{R}$ is the scalar weight on the path at layer $\\ell$ and $z_{\\ell} \\in \\mathbb{R}$ is the pre-activation at that layer.\n\nFormally, for a given path of length $L$, define\n$$\nR \\;=\\; \\prod_{\\ell=1}^{L} \\left(\\lvert a_{\\ell} \\rvert \\cdot \\sigma^{\\prime}(z_{\\ell})\\right),\n$$\nand define the saturated fraction for a set of pre-activations $\\{z^{(i)}\\}_{i=1}^{N}$ by\n$$\n\\text{frac\\_sat} \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} \\mathbf{1}\\left(\\lvert z^{(i)} \\rvert \\ge t\\right),\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function. Declare a binary stall indicator equal to $1$ if both $\\text{frac\\_sat} \\ge s_{\\min}$ and $R  \\varepsilon$, and equal to $0$ otherwise, where $s_{\\min} \\in (0,1]$ and $\\varepsilon  0$ are given thresholds.\n\n3) Implement a program that, for each test case, computes:\n- The saturated fraction $\\text{frac\\_sat}$.\n- The gradient decay factor $R$.\n- The stall indicator (as an integer) given thresholds $s_{\\min}$ and $\\varepsilon$.\n\nAll floating-point results must be rounded to six decimal places. The stall indicator must be an integer in $\\{0,1\\}$.\n\nTest Suite:\nEach test case $k$ provides a threshold $t_k$, a stall fraction threshold $s_{\\min,k}$, a gradient threshold $\\varepsilon_k$, a list of path weights $\\{a_{\\ell}\\}_{\\ell=1}^{L_k}$, a path pre-activation list $\\{z_{\\ell}\\}_{\\ell=1}^{L_k}$ for the same layers, and a snapshot of network-wide pre-activations $\\{z^{(i)}\\}_{i=1}^{N_k}$ for computing the saturation fraction. Use the following four cases:\n\n- Case $1$ (happy path, mostly non-saturated):\n  - $t = 4.0$, $s_{\\min} = 0.6$, $\\varepsilon = 10^{-4}$,\n  - path weights $[\\,1.0,\\,1.0,\\,1.0\\,]$,\n  - path pre-activations $[\\, -0.5,\\, 0.0,\\, 0.3 \\,]$,\n  - network snapshot $[\\, -0.5,\\, 0.1,\\, 0.3,\\, -1.2,\\, 2.0,\\, -3.0,\\, 0.0,\\, 0.8,\\, 5.0 \\,]$.\n\n- Case $2$ (strongly saturated, clear stall):\n  - $t = 4.0$, $s_{\\min} = 0.6$, $\\varepsilon = 10^{-4}$,\n  - path weights $[\\,1.0,\\, 0.9,\\, 1.1,\\, 1.0,\\, 0.95\\,]$,\n  - path pre-activations $[\\, 8.0,\\, -9.0,\\, 7.5,\\, -6.0,\\, 10.0 \\,]$,\n  - network snapshot $[\\, 8.0,\\, -9.0,\\, 7.5,\\, -6.0,\\, 10.0,\\, 0.2,\\, -0.1,\\, 5.0,\\, -4.5,\\, 4.1 \\,]$.\n\n- Case $3$ (mixed saturation, gradient small but saturation fraction low):\n  - $t = 4.0$, $s_{\\min} = 0.5$, $\\varepsilon = 10^{-4}$,\n  - path weights $[\\,0.8,\\, 0.7,\\, 0.9,\\, 0.85\\,]$,\n  - path pre-activations $[\\, 0.0,\\, 5.5,\\, -0.2,\\, 4.2 \\,]$,\n  - network snapshot $[\\, -0.1,\\, 5.5,\\, 0.0,\\, -0.2,\\, 4.2,\\, 0.3,\\, -0.7,\\, 0.6 \\,]$.\n\n- Case $4$ (boundary at threshold, high saturation but limited decay due to larger weights):\n  - $t = 4.0$, $s_{\\min} = 0.9$, $\\varepsilon = 10^{-4}$,\n  - path weights $[\\,3.0,\\, 3.0\\,]$,\n  - path pre-activations $[\\, 4.0,\\, -4.0 \\,]$,\n  - network snapshot $[\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0 \\,]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list of case results, enclosed in square brackets. Each case result itself must be a list of the form $[\\,\\text{frac\\_sat},\\, R,\\, \\text{stall}\\,]$ with the two floating-point values rounded to six decimals and the stall indicator as an integer. For example: \n\"[ [0.123456,0.000789,1],[0.000000,0.015625,0],... ]\" (without extra spaces is also acceptable). No other text should be printed.", "solution": "The problem is valid as it is scientifically grounded in the principles of deep learning, mathematically well-posed, and provides a complete and consistent set of data for the required computations. The request to start from the definition of the logistic sigmoid function, while not explicitly providing its formula, reasonably assumes knowledge of this fundamental function, $\\sigma(z) = (1 + e^{-z})^{-1}$, which is standard in the field.\n\nThe analysis proceeds in two parts. First, a conceptual argument for the vanishing gradient phenomenon is derived from first principles. Second, the specified metrics are computed for each test case to quantify training stall.\n\n**1. The Vanishing Gradient Phenomenon with Sigmoid Activations**\n\nThe logistic sigmoid activation function is defined as:\n$$\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n$$\nwhere $z$ is the pre-activation input to a neuron. Using the chain rule, its derivative with respect to $z$ is:\n$$\n\\sigma^{\\prime}(z) = \\frac{d}{dz} \\left( (1 + e^{-z})^{-1} \\right) = -1 \\cdot (1 + e^{-z})^{-2} \\cdot (-e^{-z}) = \\frac{e^{-z}}{(1 + e^{-z})^2}\n$$\nThis expression for the derivative can be rewritten in terms of the sigmoid function itself:\n$$\n\\sigma^{\\prime}(z) = \\frac{1}{1 + e^{-z}} \\cdot \\frac{e^{-z}}{1 + e^{-z}} = \\frac{1}{1 + e^{-z}} \\cdot \\frac{(1 + e^{-z}) - 1}{1 + e^{-z}} = \\sigma(z) \\left( 1 - \\sigma(z) \\right)\n$$\nTo understand the behavior for large-magnitude pre-activations, we examine the limits of $\\sigma(z)$ and $\\sigma^{\\prime}(z)$:\n- As $z \\to \\infty$, $e^{-z} \\to 0$. Consequently, $\\sigma(z) \\to \\frac{1}{1+0} = 1$. The derivative becomes $\\sigma^{\\prime}(z) \\to \\sigma(z)(1 - \\sigma(z)) \\to 1(1-1) = 0$.\n- As $z \\to -\\infty$, $e^{-z} \\to \\infty$. Consequently, $\\sigma(z) \\to \\lim_{u \\to \\infty} \\frac{1}{1+u} = 0$. The derivative becomes $\\sigma^{\\prime}(z) \\to \\sigma(z)(1 - \\sigma(z)) \\to 0(1-0) = 0$.\n\nIn both cases, for $\\lvert z \\rvert \\gg 0$, the sigmoid function saturates (its output approaches either $0$ or $1$) and its derivative $\\sigma^{\\prime}(z)$ approaches $0$. The maximum value of the derivative occurs at $z=0$, where $\\sigma^{\\prime}(0) = \\sigma(0)(1-\\sigma(0)) = 0.5 \\cdot 0.5 = 0.25$. For any non-zero $z$, $\\sigma^{\\prime}(z)  0.25$.\n\nNow, consider a deep network. The gradient of a loss function $\\mathcal{L}$ with respect to a parameter (e.g., a weight or bias) in an early layer is computed using the chain rule, which involves multiplying gradients back through the layers. Let us analyze the gradient flow along the given one-dimensional path. The pre-activation at layer $\\ell+1$ is a function of the activation at layer $\\ell$, $h_{\\ell} = \\sigma(z_{\\ell})$, via a scaling $a_{\\ell+1}$. Thus, the partial derivative of $z_{\\ell+1}$ with respect to $z_{\\ell}$ is:\n$$\n\\frac{\\partial z_{\\ell+1}}{\\partial z_{\\ell}} = \\frac{\\partial z_{\\ell+1}}{\\partial h_{\\ell}} \\frac{\\partial h_{\\ell}}{\\partial z_{\\ell}} = a_{\\ell+1} \\cdot \\sigma^{\\prime}(z_{\\ell})\n$$\nThe gradient of the loss with respect to a pre-activation $z_k$ in some layer $k$ is propagated from the final layer $L$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial z_k} = \\frac{\\partial \\mathcal{L}}{\\partial z_L} \\frac{\\partial z_L}{\\partial z_{L-1}} \\frac{\\partial z_{L-1}}{\\partial z_{L-2}} \\cdots \\frac{\\partial z_{k+1}}{\\partial z_k} = \\frac{\\partial \\mathcal{L}}{\\partial z_L} \\prod_{\\ell=k}^{L-1} \\frac{\\partial z_{\\ell+1}}{\\partial z_{\\ell}} = \\frac{\\partial \\mathcal{L}}{\\partial z_L} \\prod_{\\ell=k}^{L-1} \\left(a_{\\ell+1} \\sigma^{\\prime}(z_{\\ell})\\right)\n$$\nThe term $\\prod_{\\ell=1}^{L} (\\lvert a_{\\ell} \\rvert \\cdot \\sigma^{\\prime}(z_{\\ell}))$ defined as $R$ in the problem is a measure of the magnitude of this gradient signal propagation across the entire path. Since each term $\\sigma^{\\prime}(z_{\\ell})$ is at most $0.25$, and is close to $0$ for saturated units, this product can become extremely small for deep networks (large $L$). This exponential decrease in the gradient's magnitude as it is backpropagated to earlier layers is the vanishing gradient problem. It severely slows down or stalls the training of early layers, as their parameter updates become negligible.\n\n**2. Quantitative Analysis of Test Cases**\n\nThe required computations are performed for each of the four test cases. The function required for calculation is $\\sigma^{\\prime}(z) = \\sigma(z)(1-\\sigma(z))$, where $\\sigma(z) = (1+e^{-z})^{-1}$.\n\n**Case 1:**\n- Parameters: $t=4.0$, $s_{\\min}=0.6$, $\\varepsilon=10^{-4}$.\n- Network snapshot $\\{z^{(i)}\\}_{i=1}^{9}$: $[\\, -0.5,\\, 0.1,\\, 0.3,\\, -1.2,\\, 2.0,\\, -3.0,\\, 0.0,\\, 0.8,\\, 5.0 \\,]$.\n- The number of units with $\\lvert z^{(i)} \\rvert \\ge 4.0$ is $1$ (for $z=5.0$).\n- $\\text{frac\\_sat} = 1/9 \\approx 0.111111$.\n- Path details: weights $\\{a_{\\ell}\\}=[\\,1.0,\\,1.0,\\,1.0\\,]$, pre-activations $\\{z_{\\ell}\\}=[\\, -0.5,\\, 0.0,\\, 0.3 \\,]$.\n- Derivatives: $\\sigma^{\\prime}(-0.5) \\approx 0.235004$, $\\sigma^{\\prime}(0.0) = 0.25$, $\\sigma^{\\prime}(0.3) \\approx 0.244458$.\n- $R = (\\lvert 1.0 \\rvert \\cdot \\sigma^{\\prime}(-0.5)) \\cdot (\\lvert 1.0 \\rvert \\cdot \\sigma^{\\prime}(0.0)) \\cdot (\\lvert 1.0 \\rvert \\cdot \\sigma^{\\prime}(0.3)) \\approx 0.235004 \\cdot 0.25 \\cdot 0.244458 \\approx 0.014365$.\n- Stall condition: $\\text{frac\\_sat} \\ge s_{\\min}$ ($0.111111 \\ge 0.6$) is false. The stall indicator is $0$.\n- Result: $[\\, 0.111111, 0.014365, 0 \\,]$.\n\n**Case 2:**\n- Parameters: $t=4.0$, $s_{\\min}=0.6$, $\\varepsilon=10^{-4}$.\n- Network snapshot $\\{z^{(i)}\\}_{i=1}^{10}$: $[\\, 8.0,\\, -9.0,\\, 7.5,\\, -6.0,\\, 10.0,\\, 0.2,\\, -0.1,\\, 5.0,\\, -4.5,\\, 4.1 \\,]$.\n- The number of units with $\\lvert z^{(i)} \\rvert \\ge 4.0$ is $8$.\n- $\\text{frac\\_sat} = 8/10 = 0.8$.\n- Path details: weights $\\{a_{\\ell}\\}=[\\,1.0,\\, 0.9,\\, 1.1,\\, 1.0,\\, 0.95\\,]$, pre-activations $\\{z_{\\ell}\\}=[\\, 8.0,\\, -9.0,\\, 7.5,\\, -6.0,\\, 10.0 \\,]$.\n- Derivatives are very small for these large-magnitude pre-activations: $\\sigma^{\\prime}(8.0) \\approx 3.35 \\times 10^{-4}$, $\\sigma^{\\prime}(-9.0) \\approx 1.23 \\times 10^{-4}$, $\\sigma^{\\prime}(7.5) \\approx 5.53 \\times 10^{-4}$, $\\sigma^{\\prime}(-6.0) \\approx 2.47 \\times 10^{-3}$, $\\sigma^{\\prime}(10.0) \\approx 4.54 \\times 10^{-5}$.\n- $R = (\\lvert 1.0 \\rvert \\sigma^{\\prime}(8.0)) \\cdot (\\lvert 0.9 \\rvert \\sigma^{\\prime}(-9.0)) \\cdot (\\lvert 1.1 \\rvert \\sigma^{\\prime}(7.5)) \\cdot (\\lvert 1.0 \\rvert \\sigma^{\\prime}(-6.0)) \\cdot (\\lvert 0.95 \\rvert \\sigma^{\\prime}(10.0)) \\approx 2.408 \\times 10^{-18}$, which rounds to $0.000000$.\n- Stall condition: $\\text{frac\\_sat} \\ge s_{\\min}$ ($0.8 \\ge 0.6$) is true. $R  \\varepsilon$ ($0.0  10^{-4}$) is true. Both are true, so the stall indicator is $1$.\n- Result: $[\\, 0.800000, 0.000000, 1 \\,]$.\n\n**Case 3:**\n- Parameters: $t=4.0$, $s_{\\min}=0.5$, $\\varepsilon=10^{-4}$.\n- Network snapshot $\\{z^{(i)}\\}_{i=1}^{8}$: $[\\, -0.1,\\, 5.5,\\, 0.0,\\, -0.2,\\, 4.2,\\, 0.3,\\, -0.7,\\, 0.6 \\,]$.\n- The number of units with $\\lvert z^{(i)} \\rvert \\ge 4.0$ is $2$ (for $z=5.5, 4.2$).\n- $\\text{frac\\_sat} = 2/8 = 0.25$.\n- Path details: weights $\\{a_{\\ell}\\}=[\\,0.8,\\, 0.7,\\, 0.9,\\, 0.85\\,]$, pre-activations $\\{z_{\\ell}\\}=[\\, 0.0,\\, 5.5,\\, -0.2,\\, 4.2 \\,]$.\n- Derivatives: $\\sigma^{\\prime}(0.0) = 0.25$, $\\sigma^{\\prime}(5.5) \\approx 0.004073$, $\\sigma^{\\prime}(-0.2) \\approx 0.247516$, $\\sigma^{\\prime}(4.2) \\approx 0.014696$.\n- $R = (\\lvert 0.8 \\rvert \\sigma^{\\prime}(0.0)) \\cdot (\\lvert 0.7 \\rvert \\sigma^{\\prime}(5.5)) \\cdot (\\lvert 0.9 \\rvert \\sigma^{\\prime}(-0.2)) \\cdot (\\lvert 0.85 \\rvert \\sigma^{\\prime}(4.2)) \\approx 0.000002$.\n- Stall condition: $\\text{frac\\_sat} \\ge s_{\\min}$ ($0.25 \\ge 0.5$) is false. The stall indicator is $0$.\n- Result: $[\\, 0.250000, 0.000002, 0 \\,]$.\n\n**Case 4:**\n- Parameters: $t=4.0$, $s_{\\min}=0.9$, $\\varepsilon=10^{-4}$.\n- Network snapshot $\\{z^{(i)}\\}_{i=1}^{10}$: $[\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0 \\,]$.\n- All $10$ units have $\\lvert z^{(i)} \\rvert = 4.0$, so all satisfy $\\lvert z^{(i)} \\rvert \\ge 4.0$.\n- $\\text{frac\\_sat} = 10/10 = 1.0$.\n- Path details: weights $\\{a_{\\ell}\\}=[\\,3.0,\\, 3.0\\,]$, pre-activations $\\{z_{\\ell}\\}=[\\, 4.0,\\, -4.0 \\,]$.\n- Derivatives: $\\sigma^{\\prime}(4.0) = \\sigma^{\\prime}(-4.0) \\approx 0.017663$.\n- $R = (\\lvert 3.0 \\rvert \\cdot \\sigma^{\\prime}(4.0)) \\cdot (\\lvert 3.0 \\rvert \\cdot \\sigma^{\\prime}(-4.0)) \\approx (3.0 \\cdot 0.017663)^2 \\approx 0.052988^2 \\approx 0.002808$.\n- Stall condition: $\\text{frac\\_sat} \\ge s_{\\min}$ ($1.0 \\ge 0.9$) is true. $R  \\varepsilon$ ($0.002808  0.0001$) is false. The stall indicator is $0$.\n- Result: $[\\, 1.000000, 0.002808, 0 \\,]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the vanishing gradient problem for the given test cases.\n    It calculates the saturated fraction, gradient decay factor, and a stall indicator.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"t\": 4.0, \"s_min\": 0.6, \"epsilon\": 1.0e-4,\n            \"path_weights\": [1.0, 1.0, 1.0],\n            \"path_pre_activations\": [-0.5, 0.0, 0.3],\n            \"network_snapshot\": [-0.5, 0.1, 0.3, -1.2, 2.0, -3.0, 0.0, 0.8, 5.0]\n        },\n        {\n            \"t\": 4.0, \"s_min\": 0.6, \"epsilon\": 1.0e-4,\n            \"path_weights\": [1.0, 0.9, 1.1, 1.0, 0.95],\n            \"path_pre_activations\": [8.0, -9.0, 7.5, -6.0, 10.0],\n            \"network_snapshot\": [8.0, -9.0, 7.5, -6.0, 10.0, 0.2, -0.1, 5.0, -4.5, 4.1]\n        },\n        {\n            \"t\": 4.0, \"s_min\": 0.5, \"epsilon\": 1.0e-4,\n            \"path_weights\": [0.8, 0.7, 0.9, 0.85],\n            \"path_pre_activations\": [0.0, 5.5, -0.2, 4.2],\n            \"network_snapshot\": [-0.1, 5.5, 0.0, -0.2, 4.2, 0.3, -0.7, 0.6]\n        },\n        {\n            \"t\": 4.0, \"s_min\": 0.9, \"epsilon\": 1.0e-4,\n            \"path_weights\": [3.0, 3.0],\n            \"path_pre_activations\": [4.0, -4.0],\n            \"network_snapshot\": [4.0, -4.0, 4.0, -4.0, 4.0, -4.0, 4.0, -4.0, 4.0, -4.0]\n        }\n    ]\n\n    def sigmoid_derivative(z):\n        \"\"\"\n        Calculates the derivative of the logistic sigmoid function.\n        sigma'(z) = sigma(z) * (1 - sigma(z))\n        \"\"\"\n        # To avoid overflow in exp(-z) for large negative z, handle cases.\n        # However, numpy's exp is robust enough for the given inputs.\n        s = 1.0 / (1.0 + np.exp(-z))\n        return s * (1.0 - s)\n\n    results = []\n    for case in test_cases:\n        t = case[\"t\"]\n        s_min = case[\"s_min\"]\n        epsilon = case[\"epsilon\"]\n        path_weights = case[\"path_weights\"]\n        path_pre_activations = case[\"path_pre_activations\"]\n        network_snapshot = case[\"network_snapshot\"]\n\n        # 1. Calculate the saturated fraction (frac_sat)\n        N = len(network_snapshot)\n        saturated_count = sum(1 for z in network_snapshot if abs(z) >= t)\n        frac_sat = saturated_count / N if N > 0 else 0.0\n\n        # 2. Calculate the gradient decay factor (R)\n        R = 1.0\n        for a_ell, z_ell in zip(path_weights, path_pre_activations):\n            R *= abs(a_ell) * sigmoid_derivative(z_ell)\n        \n        # 3. Determine the stall indicator\n        is_stalled = (frac_sat >= s_min) and (R  epsilon)\n        stall_indicator = 1 if is_stalled else 0\n        \n        # Collect results for this case\n        results.append([frac_sat, R, stall_indicator])\n\n    # Format the final output string\n    case_strings = []\n    for res in results:\n        # Round floats to six decimal places for output\n        frac_sat_str = f\"{res[0]:.6f}\"\n        R_str = f\"{res[1]:.6f}\"\n        stall_str = str(res[2])\n        case_strings.append(f\"[{frac_sat_str},{R_str},{stall_str}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3194533"}, {"introduction": "The vanishing gradient problem is not limited to network depth; it is famously problematic in Recurrent Neural Networks (RNNs), where it hinders the ability to learn long-term dependencies. In this context, the gradient signal must propagate backward \"through time,\" and its magnitude can decay exponentially with the length of the temporal sequence. This hands-on task guides you to formalize this decay in a simple, unrolled recurrence, first through analytical derivation and then through empirical verification [@problem_id:3194489]. By observing how the gradient signal from a target at time $t$ weakens as it travels back to an input at time $t-T$, you will understand the fundamental challenge that led to the development of more complex architectures like LSTMs and GRUs.", "problem": "You are asked to formalize and empirically verify the vanishing gradient phenomenon in a recurrent computation that encodes a long-term dependency. Consider a scalar, time-unrolled recurrent computation defined by the recurrence $h_t = w \\, h_{t-1} + x_t$ with initial condition $h_0 = 0$ and output $y_t = h_t$, where $w \\in \\mathbb{R}$ is a trainable scalar parameter and $\\{x_t\\}$ is a scalar input sequence. You will construct a synthetic dependency where the supervised signal at time $t$ depends on an input $x_{t-T}$ that occurred $T$ steps earlier. The training loss at time $t$ is $L_t = \\tfrac{1}{2} \\, (y_t - \\mathrm{target}_t)^2$. In this setup, the gradient of $L_t$ with respect to the parameter $w$ is obtained via backpropagation through time, which applies the chain rule repeatedly across $T$ steps.\n\nFundamental base you may use:\n- The chain rule from calculus: for a composition of differentiable functions $f \\circ g$, one has $\\frac{d}{dx} f(g(x)) = f'(g(x)) \\, g'(x)$.\n- The definition of backpropagation through time for a scalar recurrence: if $\\delta_t = \\frac{\\partial L_t}{\\partial h_t}$, then for the linear recurrence given above, $\\delta_{t-1} = \\delta_t \\, w$ and the per-time-step parameter gradient contribution is $\\frac{\\partial L_t}{\\partial w}\\bigg|_{\\text{at step }k} = \\delta_k \\, h_{k-1}$.\n\nYour program must do the following, purely in mathematical terms:\n1. Construct a sequence $\\{x_t\\}$ of length $T+1$ where $x_1 = 1$ and $x_t = 0$ for all $t \\neq 1$. This is a unit impulse at time $t=1$. For each fixed lag $T$, define the supervised time index $t^\\star = T+1$ and the target $\\mathrm{target}_{t^\\star} = x_{t^\\star - T} = x_1 = 1$; thus the loss is $L_{t^\\star} = \\tfrac{1}{2}(y_{t^\\star} - 1)^2$.\n2. For each chosen $T$, compute the forward pass to obtain $h_{t^\\star}$, then set $\\delta_{t^\\star} = \\frac{\\partial L_{t^\\star}}{\\partial h_{t^\\star}} = h_{t^\\star} - 1$, and then propagate the error $T$ steps backward using $\\delta_{k-1} = w \\, \\delta_k$ to obtain $\\delta_{t^\\star - T}$. Define the ratio $r(T) = \\left|\\delta_{t^\\star - T}\\right| \\big/ \\left|\\delta_{t^\\star}\\right|$. This ratio isolates the pure multiplicative effect of transporting the gradient signal across $T$ steps.\n3. For fixed $w$ with $|w|  1$, model $r(T)$ by an exponential $r(T) \\approx C \\, \\lambda^T$ and estimate $\\lambda$ by performing a least-squares fit of $\\log r(T)$ versus $T$ to the affine model $\\log r(T) \\approx a \\, T + b$. The estimate is $\\hat{\\lambda} = e^a$. This empirically captures the $O(\\lambda^T)$ decay rate of the backpropagated signal.\n4. Repeat the estimation across a small test suite of parameter values to verify coverage of different regimes, including a case with sign oscillation due to negative $w$ and a slow-decay case where $w$ is close to $1$ in magnitude.\n\nAnalytical requirement to show in your solution:\n- Derive from first principles (chain rule and linear recurrence) that the backpropagated error satisfies $\\delta_{t^\\star - k} = w^k \\, \\delta_{t^\\star}$ for $k \\in \\{1,\\dots,T\\}$, hence $r(T) = |w|^T$. Then argue that the per-step contribution to the gradient with respect to $w$ at time $t^\\star - T$ is $\\delta_{t^\\star - T} \\, h_{t^\\star - T - 1}$, and because $|h_{t^\\star - T - 1}|$ is bounded for $|w|1$ under the specified impulse input, the magnitude of this contribution is $O(|w|^T)$. Conclude that the gradient signal decays geometrically with depth, with base $\\lambda = |w|  1$.\n\nTest suite:\n- Use four cases with $w \\in \\{0.2, 0.5, -0.8, 0.95\\}$.\n- For each case, compute $r(T)$ for every integer $T$ in the range $T \\in \\{5, 6, \\dots, 80\\}$ and fit $\\hat{\\lambda}$ as specified above.\n\nFinal output format:\n- Your program should produce a single line of output containing a comma-separated list of the four estimated values $\\hat{\\lambda}$, in the same order as the test suite $w$ values, enclosed in square brackets. For determinism, round each estimated value to six decimal places. For example, an output with placeholders would look like $[0.123456,0.234567,0.345678,0.456789]$. There are no physical units or angles involved in this task.", "solution": "The problem asks for an analytical derivation and empirical verification of the vanishing gradient phenomenon in a simple, time-unrolled recurrent computation.\n\n### 1. Problem Formalization and Analytical Derivation\n\nWe are given a scalar recurrent computation defined by the recurrence relation:\n$$\nh_t = w \\, h_{t-1} + x_t\n$$\nwith an initial condition $h_0 = 0$. The parameter $w \\in \\mathbb{R}$ is learnable, and $\\{x_t\\}$ is an input sequence. The output is $y_t = h_t$.\n\nThe synthetic task uses an impulse input sequence of length $T+1$ where $x_1 = 1$ and $x_t = 0$ for all $t \\neq 1$. The supervision occurs at time step $t^\\star = T+1$, with the target being $\\mathrm{target}_{t^\\star} = x_{t^\\star - T} = x_1 = 1$. The loss function at this time step is:\n$$\nL_{t^\\star} = \\frac{1}{2} (y_{t^\\star} - \\mathrm{target}_{t^\\star})^2 = \\frac{1}{2} (h_{t^\\star} - 1)^2\n$$\n\n**Forward Pass Analysis**\nWe can unroll the recurrence for the given input sequence to find an expression for $h_t$:\n- For $t=1$: $h_1 = w h_0 + x_1 = w \\cdot 0 + 1 = 1$.\n- For $t=2$: $h_2 = w h_1 + x_2 = w \\cdot 1 + 0 = w$.\n- For $t=3$: $h_3 = w h_2 + x_3 = w \\cdot w + 0 = w^2$.\nBy induction, for any time step $k \\ge 1$, the hidden state is $h_k = w^{k-1}$.\nAt the time of supervision, $t^\\star = T+1$, the hidden state is $h_{t^\\star} = h_{T+1} = w^{(T+1)-1} = w^T$.\n\n**Backward Pass Analysis (Backpropagation Through Time)**\nThe gradient of the loss with respect to the parameter $w$ is found by applying the chain rule through time. The process starts by calculating the gradient of the loss with respect to the output state, $h_{t^\\star}$. Let $\\delta_k = \\frac{\\partial L_{t^\\star}}{\\partial h_k}$ represent the error signal at time step $k$.\n\nThe initial error signal at time $t^\\star$ is:\n$$\n\\delta_{t^\\star} = \\frac{\\partial L_{t^\\star}}{\\partial h_{t^\\star}} = \\frac{\\partial}{\\partial h_{t^\\star}} \\left( \\frac{1}{2} (h_{t^\\star} - 1)^2 \\right) = h_{t^\\star} - 1 = w^T - 1\n$$\nThe problem provides the rule for propagating this error signal backward in time: $\\delta_{k-1} = \\frac{\\partial L_{t^\\star}}{\\partial h_{k-1}} = \\frac{\\partial L_{t^\\star}}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_{k-1}} = \\delta_k \\cdot w$. This is a geometric progression. We can derive a closed-form expression for the error signal $\\delta_{t^\\star-k}$ after being propagated back $k$ steps:\n- For $k=1$: $\\delta_{t^\\star-1} = w \\, \\delta_{t^\\star}$.\n- For $k=2$: $\\delta_{t^\\star-2} = w \\, \\delta_{t^\\star-1} = w (w \\, \\delta_{t^\\star}) = w^2 \\delta_{t^\\star}$.\nBy induction, for any integer $k \\in \\{1, \\dots, T\\}$, the error signal is:\n$$\n\\delta_{t^\\star-k} = w^k \\delta_{t^\\star}\n$$\nThis demonstrates that the error signal's magnitude is scaled by a factor of $w$ at each step of backpropagation.\n\nThe ratio $r(T)$ is defined to isolate this multiplicative effect over the full dependency length $T$:\n$$\nr(T) = \\frac{|\\delta_{t^\\star-T}|}{|\\delta_{t^\\star}|}\n$$\nUsing our derived formula for the backpropagated error with $k=T$, we get $\\delta_{t^\\star-T} = w^T \\delta_{t^\\star}$. Substituting this into the definition of $r(T)$:\n$$\nr(T) = \\frac{|w^T \\delta_{t^\\star}|}{|\\delta_{t^\\star}|} = |w^T| = |w|^T\n$$\nThis analytical result shows that the ratio of the backpropagated error signal to the initial error signal decays exponentially with the time lag $T$, with the base of the exponent being $|w|$. For $|w|1$, this ratio tends to zero as $T$ increases, which is the essence of the vanishing gradient problem.\n\n**Gradient Contribution Analysis**\nThe total gradient of the loss with respect to the weight, $\\frac{\\partial L_{t^\\star}}{\\partial w}$, is the sum of contributions from each time step in the computational graph: $\\frac{\\partial L_{t^\\star}}{\\partial w} = \\sum_{k=1}^{t^\\star} \\frac{\\partial L_{t^\\star}}{\\partial w}\\big|_{\\text{at step }k}$. The per-step contribution is given as $\\delta_k h_{k-1}$. The problem asks to analyze this contribution at time $t^\\star - T = (T+1) - T = 1$.\nThe contribution at step $k=1$ is $\\delta_1 h_0$.\nFrom the backpropagation analysis, we have $\\delta_1 = \\delta_{t^\\star - T} = w^T \\delta_{t^\\star}$. The initial condition is given as $h_0 = 0$.\nTherefore, the contribution at this step is $\\delta_1 h_0 = (w^T \\delta_{t^\\star}) \\cdot 0 = 0$.\nThe magnitude of this contribution is $0$, which trivially satisfies the condition of being $O(|w|^T)$. More generally, any gradient contribution term $\\delta_k h_{k-1}$ contains the factor $\\delta_k = w^{t^\\star-k} \\delta_{t^\\star}$. For contributions arising from long time lags (i.e., small $k$), the term $w^{t^\\star-k}$ will be a high power of $w$, leading to an exponentially small value when $|w|1$. This is the \"vanishing gradient signal\" that makes it difficult for the model to learn long-term dependencies.\n\n### 2. Empirical Verification\n\nThe analytical result $r(T) = |w|^T$ is an exponential function of $T$. To empirically verify this and estimate the decay base $\\lambda = |w|$, we can linearize the relationship by taking the logarithm:\n$$\n\\log r(T) = \\log(|w|^T) = T \\log|w|\n$$\nThis equation is in the form of a line, $y = aT + b$, where the dependent variable is $y = \\log r(T)$, the independent variable is $T$, the slope is $a = \\log|w|$, and the intercept is $b=0$.\n\nWe will perform the following procedure for each given value of $w$:\n1. Generate data pairs $(T, \\log r(T))$ for $T \\in \\{5, 6, \\dots, 80\\}$ using the derived formula $r(T) = |w|^T$.\n2. Perform a linear least-squares regression on these data points to fit the model $\\log r(T) \\approx aT + b$. This will yield an estimate for the slope, $\\hat{a}$.\n3. From the relationship $a = \\log \\lambda$, we can estimate the decay base as $\\hat{\\lambda} = e^{\\hat{a}}$.\nBased on our analysis, we expect the empirical estimate $\\hat{\\lambda}$ to be very close to the theoretical value $|w|$. The provided implementation will carry out this estimation for the specified test suite of $w$ values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the vanishing gradient problem for a simple recurrent computation.\n\n    For different values of a weight parameter 'w', this function simulates\n    the decay of a gradient signal over a time lag 'T'. It then performs a\n    log-linear regression to empirically estimate the decay rate lambda, which is\n    theoretically equal to |w|.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # w values for the recurrence h_t = w * h_{t-1} + x_t\n        0.2,\n        0.5,\n        -0.8,\n        0.95,\n    ]\n\n    # Define the range of time lags T for the analysis.\n    T_values = np.arange(5, 81)\n    \n    # List to store the estimated decay rates.\n    estimated_lambdas = []\n\n    for w in test_cases:\n        # 1. Analytically determine the ratio r(T).\n        # From the derivation, r(T) = |w|^T.\n        # We handle the case w=0 separately to avoid log(0).\n        if w == 0:\n            # For w=0, the gradient signal is always zero for T>1, so decay is immediate.\n            # The base lambda is technically 0.\n            estimated_lambdas.append(0.0)\n            continue\n            \n        r_T = np.abs(w) ** T_values\n        \n        # 2. Linearize the model by taking the logarithm.\n        # log(r(T)) = log(|w|^T) = T * log|w|.\n        # This is the form y = a*x, where y=log(r(T)), x=T, a=log|w|.\n        log_r_T = np.log(r_T)\n        \n        # 3. Perform a linear least-squares fit to estimate the slope 'a'.\n        # We fit the model y = a*x + b to the data (T, log r(T)).\n        # The matrix 'A' sets up the system of linear equations for the fit.\n        A = np.vstack([T_values, np.ones_like(T_values)]).T\n        \n        # `np.linalg.lstsq` solves the equation Ax = y for x, where x = [a, b].\n        # It returns the solution that minimizes the Euclidean 2-norm ||y - Ax||^2.\n        # The slope 'a' is the first element of the solution vector.\n        slope, _ = np.linalg.lstsq(A, log_r_T, rcond=None)[0]\n        \n        # 4. Estimate lambda from the slope.\n        # Since slope 'a' is an estimate of log(|w|) = log(lambda),\n        # lambda can be estimated by exponentiating the slope.\n        lambda_hat = np.exp(slope)\n        \n        # 5. Store the result, rounded to the specified precision.\n        estimated_lambdas.append(round(lambda_hat, 6))\n\n    # Final print statement in the exact required format.\n    # e.g., [0.200000,0.500000,0.800000,0.950000]\n    print(f\"[{','.join(map(str, estimated_lambdas))}]\")\n\nsolve()\n```", "id": "3194489"}, {"introduction": "While understanding the problem is crucial, modern deep learning thrives on effective solutions. One of the most important breakthroughs in mitigating the vanishing and exploding gradient problem was the development of principled weight initialization schemes. This practice explores how to maintain stable signal propagation in deep networks with Rectified Linear Unit (ReLU) activations, which are the standard in many modern architectures [@problem_id:3194508]. You will derive and empirically verify why He initialization, which sets the variance of weights to $\\frac{2}{n}$, is critical for preserving the variance of both forward-propagating activations and backward-propagating gradients, thereby enabling the training of much deeper networks.", "problem": "Consider a fully connected feedforward neural network of depth $L$ with Rectified Linear Unit (ReLU) nonlinearity. Let the layer index be $l \\in \\{1,\\dots,L\\}$, the width (number of units) of each layer be $n$, and the input batch size be $B$. Denote the post-activation at layer $l$ as $x_l \\in \\mathbb{R}^{B \\times n}$ and the preactivation at layer $l$ as $z_l \\in \\mathbb{R}^{B \\times n}$. The forward propagation obeys the rules $z_l = x_{l-1} W_l$ and $x_l = \\phi(z_l)$, where $\\phi$ is the ReLU function defined by $\\phi(u) = \\max(0,u)$ elementwise, and $W_l \\in \\mathbb{R}^{n \\times n}$ is the weight matrix for layer $l$. Assume $x_0$ has entries that are independent and identically distributed (i.i.d.) Gaussian with zero mean and unit variance. At initialization, assume the entries of each $W_l$ are i.i.d., zero mean, and independent of $x_{l-1}$, and consider two initialization schemes: He initialization with $\\mathrm{Var}(W_{l,ij}) = \\frac{2}{n}$ and Xavier (Glorot) initialization with $\\mathrm{Var}(W_{l,ij}) = \\frac{1}{n}$. Biases are zero.\n\nThe objective is to analyze the vanishing gradient problem from first principles. You must start from the following base:\n- The chain rule of calculus for backpropagation in feedforward neural networks.\n- The definition of variance $\\mathrm{Var}[X] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$ and the independence rule for variances of sums of independent zero-mean random variables, namely, if $U_i$ are independent and have zero mean, then $\\mathrm{Var}\\left[\\sum_i U_i\\right] = \\sum_i \\mathrm{Var}[U_i]$.\n- The properties of ReLU acting on a zero-mean symmetric input: its derivative is an indicator $\\phi'(u) = \\mathbb{1}\\{u0\\}$, which acts as a gate with probability $\\frac{1}{2}$ under symmetry.\n\nUsing these bases, reason about how moments propagate forward and backward at initialization. In particular, define the forward “energy” at layer $l$ as the second raw moment $m_l^{\\text{fwd}} = \\mathbb{E}\\big[x_l^2\\big]$ (the expectation is over both batch and units), and define the backward “energy” at layer $l$ as the second raw moment of the gradient with respect to preactivations $m_l^{\\text{bwd}} = \\mathbb{E}\\big[(\\partial L/\\partial z_l)^2\\big]$ for the squared norm loss $L = \\tfrac{1}{2}\\lVert x_L\\rVert_2^2$ computed per sample without averaging over the batch. Under the He initialization and the independence assumptions, show why $m_l^{\\text{fwd}}$ and $m_l^{\\text{bwd}}$ are preserved near $1$ across layers at initialization. Contrast this with Xavier initialization under ReLU, and explain why it tends to reduce $m_l^{\\text{fwd}}$ as depth increases, creating a vanishing gradient scenario.\n\nYour program must empirically verify these statements by Monte Carlo simulation. For each test case, construct a random network and batch as specified, perform a single forward pass to compute all $z_l$ and $x_l$, and a single backward pass using the chain rule to compute $\\partial L/\\partial z_l$ for all $l$. For each layer, compute:\n- The forward second raw moment $m_l^{\\text{fwd}}$ as the empirical mean of squared entries of $x_l$.\n- The backward second raw moment $m_l^{\\text{bwd}}$ as the empirical mean of squared entries of $\\partial L/\\partial z_l$.\nAggregate the deviations from $1$ by computing the maximum absolute deviation across layers for forward and backward, that is, $\\Delta^{\\text{fwd}} = \\max_l \\lvert m_l^{\\text{fwd}} - 1\\rvert$ and $\\Delta^{\\text{bwd}} = \\max_l \\lvert m_l^{\\text{bwd}} - 1\\rvert$. A test case “passes” if both $\\Delta^{\\text{fwd}}$ and $\\Delta^{\\text{bwd}}$ are strictly less than a tolerance $\\varepsilon = 0.2$.\n\nTest suite:\n- Case $1$: He initialization, $L=1$, $n=64$, $B=2000$.\n- Case $2$: He initialization, $L=20$, $n=64$, $B=2000$.\n- Case $3$: He initialization, $L=40$, $n=64$, $B=2000$.\n- Case $4$: Xavier initialization, $L=20$, $n=64$, $B=2000$.\n\nAnswer format:\n- Your program should produce a single line of output containing the pass/fail results for the four test cases as a comma-separated list enclosed in square brackets, for example, $\\texttt{[True,True,True,False]}$.\n\nNo physical units or angle units are involved. All expectations and variances are dimensionless. Implement the simulation and computations precisely as described, without using any external files or inputs, and use fixed random seeds to ensure reproducibility.", "solution": "The problem is valid as it is scientifically grounded in the principles of deep learning, is well-posed with all necessary information provided, and is formulated objectively. We proceed to the theoretical analysis and subsequent empirical verification.\n\nThe core of the analysis lies in deriving recurrence relations for the propagation of the second raw moments of activations (forward pass) and gradients (backward pass) through the layers of the network at initialization. We define the forward \"energy\" as $m_l^{\\text{fwd}} = \\mathbb{E}\\big[x_{l,ik}^2\\big]$ and the backward \"energy\" as $m_l^{\\text{bwd}} = \\mathbb{E}\\big[(\\partial L/\\partial z_{l,ik})^2\\big]$, where the expectation is taken over the batch dimension $i$, the unit dimension $k$, and the random initialization.\n\n**Forward Propagation Analysis**\n\nThe forward pass is defined by $z_l = x_{l-1} W_l$ and $x_l = \\phi(z_l)$, where $\\phi$ is the ReLU function. Let's consider a single pre-activation element $z_{l,ik} = \\sum_{j=1}^{n} x_{l-1, ij} W_{l,jk}$. At initialization, the weights $W_{l,jk}$ are independent of the inputs $x_{l-1, ij}$, and both have zero mean. Consequently, $\\mathbb{E}[z_{l,ik}] = \\sum_{j=1}^{n} \\mathbb{E}[x_{l-1, ij}] \\mathbb{E}[W_{l,jk}] = 0$, under the assumption that $\\mathbb{E}[x_{l-1, ij}] = 0$. This holds for the input layer $x_0$, and we assume it approximately holds for subsequent layers due to the symmetric nature of the updates.\n\nThe variance of $z_{l,ik}$ is $\\mathrm{Var}(z_{l,ik}) = \\mathbb{E}[z_{l,ik}^2]$ since its mean is zero. Using the independence of the terms in the sum:\n$$ \\mathrm{Var}(z_{l,ik}) = \\mathrm{Var}\\left(\\sum_{j=1}^{n} x_{l-1, ij} W_{l,jk}\\right) = \\sum_{j=1}^{n} \\mathrm{Var}(x_{l-1, ij} W_{l,jk}) $$\nBy independence of $x_{l-1}$ and $W_l$, and their zero-mean property, $\\mathrm{Var}(AB) = \\mathbb{E}[A^2]\\mathbb{E}[B^2] = \\mathrm{Var}(A)\\mathrm{Var}(B)$. Thus:\n$$ \\mathrm{Var}(z_{l,ik}) = \\sum_{j=1}^{n} \\mathrm{Var}(x_{l-1, ij}) \\mathrm{Var}(W_{l,jk}) = n \\cdot \\mathrm{Var}(x_{l-1}) \\cdot \\mathrm{Var}(W_l) $$\nHere, $\\mathrm{Var}(x_{l-1})$ is the variance of any element in $x_{l-1}$, and $\\mathrm{Var}(W_l)$ is the variance of any element in $W_l$.\nThe activation is $x_l = \\phi(z_l)$. We need the second moment $m_l^{\\text{fwd}} = \\mathbb{E}[x_l^2] = \\mathbb{E}[\\phi(z_l)^2]$. For a zero-mean symmetric input $z_l$ (approximated as Gaussian by the Central Limit Theorem), the ReLU function $\\phi(u)=\\max(0,u)$ effectively nullifies half of the distribution. The second moment of the output is half the second moment of the input: $\\mathbb{E}[\\phi(z_l)^2] = \\frac{1}{2}\\mathbb{E}[z_l^2]$.\nThus, $m_l^{\\text{fwd}} = \\frac{1}{2} \\mathbb{E}[z_l^2] = \\frac{1}{2} \\mathrm{Var}(z_l)$.\n\nCombining these results, we get a recurrence for the forward energy:\n$$ m_l^{\\text{fwd}} = \\frac{1}{2} n \\cdot \\mathrm{Var}(x_{l-1}) \\cdot \\mathrm{Var}(W_l) $$\nWe make the standard approximation that the mean of activations remains close to zero, so $\\mathrm{Var}(x_{l-1}) \\approx \\mathbb{E}[x_{l-1}^2] = m_{l-1}^{\\text{fwd}}$. This gives:\n$$ m_l^{\\text{fwd}} \\approx \\frac{1}{2} n \\cdot m_{l-1}^{\\text{fwd}} \\cdot \\mathrm{Var}(W_l) $$\nThe base case is for the input layer $x_0$, which has i.i.d. entries with mean $0$ and variance $1$. Therefore, $m_0^{\\text{fwd}} = \\mathbb{E}[x_0^2] = \\mathrm{Var}(x_0) = 1$.\n\n*   **He Initialization**: With $\\mathrm{Var}(W_l) = \\frac{2}{n}$, the recurrence becomes $m_l^{\\text{fwd}} \\approx \\frac{1}{2} n \\cdot m_{l-1}^{\\text{fwd}} \\cdot \\frac{2}{n} = m_{l-1}^{\\text{fwd}}$. Starting from $m_0^{\\text{fwd}}=1$, the forward energy is preserved across layers, i.e., $m_l^{\\text{fwd}} \\approx 1$ for all $l$.\n*   **Xavier Initialization**: With $\\mathrm{Var}(W_l) = \\frac{1}{n}$, the recurrence becomes $m_l^{\\text{fwd}} \\approx \\frac{1}{2} n \\cdot m_{l-1}^{\\text{fwd}} \\cdot \\frac{1}{n} = \\frac{1}{2} m_{l-1}^{\\text{fwd}}$. This leads to an exponential decay: $m_l^{\\text{fwd}} \\approx (\\frac{1}{2})^l m_0^{\\text{fwd}} = (\\frac{1}{2})^l$. The activations' energy vanishes as the network depth $L$ increases.\n\n**Backward Propagation Analysis**\n\nThe gradient with respect to the pre-activations is given by the chain rule: $\\frac{\\partial L}{\\partial z_l} = \\frac{\\partial L}{\\partial z_{l+1}} \\frac{\\partial z_{l+1}}{\\partial x_l} \\frac{\\partial x_l}{\\partial z_l}$. Let $\\delta_l = \\frac{\\partial L}{\\partial z_l}$. A single component is:\n$$ \\delta_{l,ik} = \\left( (\\delta_{l+1} W_{l+1}^T)_{ik} \\right) \\cdot \\phi'(z_{l,ik}) = \\left( \\sum_{j=1}^n \\delta_{l+1, ij} W_{l+1, kj} \\right) \\cdot \\phi'(z_{l,ik}) $$\nWe want to find $m_l^{\\text{bwd}} = \\mathbb{E}[\\delta_{l,ik}^2]$. The term in parentheses is independent of $\\phi'(z_{l,ik})$ at initialization. Thus, $\\mathbb{E}[\\delta_{l,ik}^2] = \\mathbb{E}[(\\sum_j \\dots)^2] \\cdot \\mathbb{E}[(\\phi'(z_{l,ik}))^2]$.\nThe derivative of ReLU is $\\phi'(u) = \\mathbb{1}\\{u0\\}$, so $(\\phi'(u))^2 = \\phi'(u)$. For a symmetric, zero-mean $z_{l,ik}$, $\\mathbb{P}(z_{l,ik}0) = \\frac{1}{2}$, so $\\mathbb{E}[(\\phi'(z_{l,ik}))^2] = \\frac{1}{2}$.\nThe variance of the sum is $\\mathrm{Var}(\\sum_j \\delta_{l+1, ij} W_{l+1, kj}) = n \\cdot \\mathrm{Var}(\\delta_{l+1}) \\cdot \\mathrm{Var}(W_{l+1})$. Assuming $\\mathbb{E}[\\delta_{l+1}] \\approx 0$, this is $n \\cdot m_{l+1}^{\\text{bwd}} \\cdot \\mathrm{Var}(W_{l+1})$.\n\nCombining these, we get the backward recurrence relation:\n$$ m_l^{\\text{bwd}} \\approx \\frac{1}{2} n \\cdot m_{l+1}^{\\text{bwd}} \\cdot \\mathrm{Var}(W_{l+1}) $$\nFor the base case at layer $L$, the loss is $L = \\frac{1}{2} \\|x_L\\|_2^2$ per sample. The gradient is $\\delta_L = \\frac{\\partial L}{\\partial z_L} = \\frac{\\partial L}{\\partial x_L}\\frac{\\partial x_L}{\\partial z_L}$. Here, $\\frac{\\partial L}{\\partial x_L} = x_L$ and $\\frac{\\partial x_L}{\\partial z_L}$ is a diagonal matrix of $\\phi'(z_L)$. So $\\delta_L = x_L \\odot \\phi'(z_L)$. Since $x_L = \\phi(z_L)$, we have $\\delta_L = \\phi(z_L) \\odot \\phi'(z_L) = \\phi(z_L) = x_L$.\nTherefore, the base case for the backward energy is $m_L^{\\text{bwd}} = \\mathbb{E}[\\delta_L^2] = \\mathbb{E}[x_L^2] = m_L^{\\text{fwd}}$.\n\n*   **He Initialization**: With $\\mathrm{Var}(W_{l+1}) = \\frac{2}{n}$, the recurrence is $m_l^{\\text{bwd}} \\approx m_{l+1}^{\\text{bwd}}$. From the forward pass, $m_L^{\\text{fwd}} \\approx 1$, so $m_L^{\\text{bwd}} \\approx 1$. Propagating backward, we find $m_l^{\\text{bwd}} \\approx 1$ for all $l$. The gradient energy is preserved.\n*   **Xavier Initialization**: With $\\mathrm{Var}(W_{l+1}) = \\frac{1}{n}$, the recurrence is $m_l^{\\text{bwd}} \\approx \\frac{1}{2} m_{l+1}^{\\text{bwd}}$. The gradient energy decays exponentially as it propagates from layer $L$ to layer $1$. This is the vanishing gradient problem. The gradients in early layers become too small to facilitate effective learning.\n\nThe simulation will empirically verify these two contrasting behaviors.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs Monte Carlo simulations to verify the signal propagation properties of\n    He and Xavier initializations in deep ReLU networks.\n    \"\"\"\n    # Set a fixed random seed for reproducibility.\n    np.random.seed(42)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'init': 'he', 'L': 1, 'n': 64, 'B': 2000, 'name': 'Case 1'},\n        {'init': 'he', 'L': 20, 'n': 64, 'B': 2000, 'name': 'Case 2'},\n        {'init': 'he', 'L': 40, 'n': 64, 'B': 2000, 'name': 'Case 3'},\n        {'init': 'xavier', 'L': 20, 'n': 64, 'B': 2000, 'name': 'Case 4'},\n    ]\n\n    results = []\n    for case in test_cases:\n        passes_test = run_simulation(\n            init_scheme=case['init'],\n            L=case['L'],\n            n=case['n'],\n            B=case['B']\n        )\n        results.append(passes_test)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(init_scheme: str, L: int, n: int, B: int) -> bool:\n    \"\"\"\n    Performs a single forward and backward pass for a given network configuration\n    and checks if the deviation criteria are met.\n\n    Args:\n        init_scheme: Either 'he' or 'xavier'.\n        L: Depth of the network.\n        n: Width of each layer.\n        B: Batch size.\n\n    Returns:\n        A boolean indicating if the test case passes.\n    \"\"\"\n    # 1. Initialization\n    # Input data: i.i.d. Gaussian with zero mean and unit variance.\n    x0 = np.random.randn(B, n)\n\n    # Determine weight variance based on initialization scheme.\n    if init_scheme == 'he':\n        var_w = 2.0 / n\n    elif init_scheme == 'xavier':\n        var_w = 1.0 / n\n    else:\n        raise ValueError(\"Unknown initialization scheme.\")\n    \n    std_w = np.sqrt(var_w)\n\n    # Initialize weights for L layers.\n    weights = [np.random.randn(n, n) * std_w for _ in range(L)]\n\n    # 2. Forward Pass\n    z_values = {}\n    x_values = {0: x0}\n    \n    x_current = x0\n    for l in range(1, L + 1):\n        # Linear transformation\n        z_l = x_current @ weights[l-1]\n        # ReLU activation\n        x_l = np.maximum(0, z_l)\n        \n        z_values[l] = z_l\n        x_values[l] = x_l\n        x_current = x_l\n    \n    # 3. Compute Forward Moments\n    m_fwd = []\n    for l in range(1, L + 1):\n        # Calculate empirical second raw moment over batch and units.\n        moment = np.mean(x_values[l]**2)\n        m_fwd.append(moment)\n\n    # 4. Backward Pass\n    dz_values = {}\n    \n    # Base case: Gradient of the loss w.r.t final pre-activations.\n    # For L = 0.5 * ||x_L||^2, dL/dz_L = x_L.\n    # This simplification comes from dL/dx_L = x_L and dL/dz_L = dL/dx_L * phi'(z_L),\n    # which simplifies to x_L * phi'(z_L) = phi(z_L) = x_L.\n    dz_L = x_values[L]\n    dz_values[L] = dz_L\n    \n    # Propagate gradients backward from L-1 to 1.\n    dz_current = dz_L\n    for l in range(L - 1, 0, -1):\n        # Gradient w.r.t previous activation layer\n        dx_l = dz_current @ weights[l].T\n        \n        # Derivative of ReLU\n        phi_prime_l = (z_values[l] > 0).astype(float)\n        \n        # Gradient w.r.t pre-activation layer\n        dz_l = dx_l * phi_prime_l\n        \n        dz_values[l] = dz_l\n        dz_current = dz_l\n\n    # 5. Compute Backward Moments\n    m_bwd = []\n    for l in range(1, L + 1):\n        # Calculate empirical second raw moment over batch and units.\n        moment = np.mean(dz_values[l]**2)\n        m_bwd.append(moment)\n\n    # 6. Check Pass/Fail Condition\n    epsilon = 0.2\n    \n    # Maximum absolute deviation from 1 for forward moments\n    delta_fwd = np.max(np.abs(np.array(m_fwd) - 1))\n    \n    # Maximum absolute deviation from 1 for backward moments\n    delta_bwd = np.max(np.abs(np.array(m_bwd) - 1))\n\n    return delta_fwd  epsilon and delta_bwd  epsilon\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3194508"}]}