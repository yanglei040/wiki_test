{"hands_on_practices": [{"introduction": "To truly grasp the mechanics of an optimizer, there is no substitute for performing a calculation by hand. This exercise will guide you through a single update step of the AdamW algorithm for a simple scalar parameter. By manually computing the moment estimates, applying the crucial bias correction, and combining the adaptive step with the decoupled weight decay, you will solidify your understanding of how each component contributes to the final parameter update [@problem_id:3096505]. This fundamental practice builds an intuition that is invaluable when diagnosing and tuning optimizers in more complex scenarios.", "problem": "Consider a single scalar parameter $w$ optimized to minimize a differentiable loss $L(w)$ using gradient-based learning. In Stochastic Gradient Descent (SGD), the gradient step is proportional to the instantaneous gradient $g_t = \\frac{dL}{dw}\\big|_{t}$. The Adaptive Moment Estimation with decoupled Weight decay (AdamW) optimizer maintains exponential moving averages of past gradients and squared gradients. Let the first moment and second moment be defined by the exponential moving average recurrences $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$ and $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$, respectively, with $m_0 = 0$ and $v_0 = 0$. To correct the initialization bias inherent in these moving averages, define the bias-corrected estimates $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^{t}}$ and $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^{t}}$. In AdamW, the decoupled weight decay principle stipulates that a shrinkage term proportional to the current parameter value is applied independently of the gradient normalization, producing an additive update component that scales linearly with $w_t$ and the weight decay coefficient.\n\nYou are given the following numerical values: initial parameter $w_0 = 2.0$, first-step gradient $g_1 = 0.3$, learning rate $\\alpha = 1.0 \\times 10^{-3}$, exponential decay rates $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$, numerical stabilizer $\\epsilon = 1.0 \\times 10^{-8}$, and weight decay coefficient $\\lambda = 0.01$. Using only the definitions above of exponential moving averages, their bias corrections, and the decoupled weight decay principle, derive the signed first-step change $\\Delta w_1$ to be applied to $w_0$ by the AdamW optimizer at $t=1$. Your derivation must make explicit how the quantities $\\hat{m}_1 = \\frac{m_1}{1 - \\beta_1}$ and $\\hat{v}_1 = \\frac{v_1}{1 - \\beta_2}$ determine the magnitude of the normalized gradient step.\n\nCompute the single real number $\\Delta w_1$ and report it as your final answer. Round your answer to four significant figures.", "solution": "The problem is well-posed and scientifically grounded, providing a complete set of definitions and parameters to compute the first-step change, $\\Delta w_1$, for a scalar parameter $w$ being optimized by the AdamW algorithm.\n\nThe AdamW optimizer update rule with decoupled weight decay is defined by the change $\\Delta w_t$ applied to the parameter $w_{t-1}$ at step $t$:\n$$ \\Delta w_t = w_t - w_{t-1} = -\\alpha \\left( \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda w_{t-1} \\right) $$\nwhere $\\alpha$ is the learning rate, $\\lambda$ is the weight decay coefficient, $\\epsilon$ is a small constant for numerical stability, and $\\hat{m}_t$ and $\\hat{v}_t$ are the bias-corrected first and second moment estimates, respectively. The parameter $w_{t-1}$ is the value of the parameter at the beginning of the update step $t$. We are asked to compute this change for the first step, $t=1$, which is denoted as $\\Delta w_1$.\n\nThe given values are:\nInitial parameter: $w_0 = 2.0$\nGradient at step $1$: $g_1 = 0.3$\nLearning rate: $\\alpha = 1.0 \\times 10^{-3}$\nWeight decay coefficient: $\\lambda = 0.01$\nExponential decay rate for the first moment: $\\beta_1 = 0.9$\nExponential decay rate for the second moment: $\\beta_2 = 0.999$\nInitial first moment: $m_0 = 0$\nInitial second moment: $v_0 = 0$\nNumerical stabilizer: $\\epsilon = 1.0 \\times 10^{-8}$\n\nThe calculation proceeds in steps:\n\nFirst, we compute the first and second moments, $m_1$ and $v_1$, at step $t=1$.\nThe first moment $m_t$ is the exponential moving average of the gradients, defined by the recurrence $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$. For $t=1$:\n$$ m_1 = \\beta_1 m_0 + (1 - \\beta_1) g_1 $$\nSubstituting the given values:\n$$ m_1 = (0.9)(0) + (1 - 0.9)(0.3) = (0.1)(0.3) = 0.03 $$\n\nThe second moment $v_t$ is the exponential moving average of the squared gradients, defined by $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$. For $t=1$:\n$$ v_1 = \\beta_2 v_0 + (1 - \\beta_2) g_1^2 $$\nSubstituting the given values:\n$$ v_1 = (0.999)(0) + (1 - 0.999)(0.3)^2 = (0.001)(0.09) = 0.00009 $$\n\nSecond, we compute the bias-corrected moment estimates, $\\hat{m}_1$ and $\\hat{v}_1$. These corrections account for the fact that the moving averages are initialized at zero.\nThe bias-corrected first moment is $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$. For $t=1$:\n$$ \\hat{m}_1 = \\frac{m_1}{1 - \\beta_1^1} = \\frac{m_1}{1 - \\beta_1} $$\n$$ \\hat{m}_1 = \\frac{0.03}{1 - 0.9} = \\frac{0.03}{0.1} = 0.3 $$\nAs a check, we note that for the first step, $\\hat{m}_1 = \\frac{(1 - \\beta_1) g_1}{1 - \\beta_1} = g_1 = 0.3$.\n\nThe bias-corrected second moment is $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$. For $t=1$:\n$$ \\hat{v}_1 = \\frac{v_1}{1 - \\beta_2^1} = \\frac{v_1}{1 - \\beta_2} $$\n$$ \\hat{v}_1 = \\frac{0.00009}{1 - 0.999} = \\frac{0.00009}{0.001} = 0.09 $$\nSimilarly, for the first step, $\\hat{v}_1 = \\frac{(1 - \\beta_2) g_1^2}{1 - \\beta_2} = g_1^2 = (0.3)^2 = 0.09$.\n\nThird, we compute the total change $\\Delta w_1$ using the AdamW update rule for $t=1$:\n$$ \\Delta w_1 = -\\alpha \\left( \\frac{\\hat{m}_1}{\\sqrt{\\hat{v}_1} + \\epsilon} + \\lambda w_0 \\right) $$\nSubstituting the computed and given values:\n$$ \\Delta w_1 = -(1.0 \\times 10^{-3}) \\left( \\frac{0.3}{\\sqrt{0.09} + 1.0 \\times 10^{-8}} + (0.01)(2.0) \\right) $$\n\nNow, we evaluate the expression term by term:\nThe square root of the bias-corrected second moment is:\n$$ \\sqrt{\\hat{v}_1} = \\sqrt{0.09} = 0.3 $$\nThe adaptive gradient term is:\n$$ \\frac{\\hat{m}_1}{\\sqrt{\\hat{v}_1} + \\epsilon} = \\frac{0.3}{0.3 + 1.0 \\times 10^{-8}} = \\frac{0.3}{0.30000001} \\approx 0.9999999667 $$\nThe decoupled weight decay term is:\n$$ \\lambda w_0 = (0.01)(2.0) = 0.02 $$\nThe sum inside the parentheses is:\n$$ 0.9999999667 + 0.02 = 1.0199999667 $$\nFinally, we multiply by the negative learning rate:\n$$ \\Delta w_1 = -(1.0 \\times 10^{-3}) (1.0199999667) = -0.0010199999667 $$\n\nThe problem requires rounding the final answer to four significant figures. The first significant figure is the first non-zero digit, which is $1$. The first four significant figures are $1$, $0$, $1$, $9$. The fifth significant figure is $9$, which is greater than or equal to $5$, so we round up the fourth significant figure.\n$$ -0.00101999... \\approx -0.001020 $$\nThe trailing zero is significant.", "answer": "$$\\boxed{-0.001020}$$", "id": "3096505"}, {"introduction": "Now that we have seen the mechanics of a single AdamW step, we can explore one of its primary motivations: stability. Deep learning loss landscapes can be notoriously difficult to navigate, with extremely steep \"cliffs\" in some directions and flat \"valleys\" in others. This exercise uses a carefully designed quadratic function to simulate such a landscape, allowing you to observe how a standard Adam optimizer can produce excessively large updates along high-curvature directions [@problem_id:3096484]. By comparing this to the behavior of AdamW, you will see firsthand how the simple, additive weight decay term provides a powerful stabilizing force, preventing runaway updates and leading to a more controlled optimization trajectory.", "problem": "You are tasked with designing and analyzing a simulation that isolates how Decoupled Adaptive Moment Estimation with Weight Decay (AdamW) alters update magnitudes along directions of high curvature in a quadratic objective. The exercise must begin from fundamental definitions and proceed by explicit construction. Consider the differentiable function defined by the quadratic form\n$$\nf(\\mathbf{x}) \\;=\\; \\tfrac{1}{2}\\,\\mathbf{x}^\\top \\mathbf{H}\\,\\mathbf{x} \\;=\\; \\tfrac{1}{2}\\sum_{i=1}^{d} h_i\\,x_i^2,\n$$\nwhere $\\mathbf{H}=\\mathrm{diag}(h_1,\\dots,h_d)$ is a diagonal, positive definite matrix and $h_i>0$. The gradient is\n$$\n\\nabla f(\\mathbf{x}) \\;=\\; \\mathbf{H}\\,\\mathbf{x}.\n$$\nAdaptive Moment Estimation (Adam) maintains exponential moving averages of gradients and squared gradients, with bias correction and a numerical stabilizer. Decoupled weight decay in AdamW applies a shrinkage directly to parameters at each step rather than coupling it to the gradient of $f(\\mathbf{x})$. Your program must:\n- Implement two optimizers acting on $f(\\mathbf{x})$: \n  - Adaptive Moment Estimation (Adam), defined by exponential moving averages of $\\mathbf{g}_t=\\nabla f(\\mathbf{x}_t)$ and $\\mathbf{g}_t\\odot\\mathbf{g}_t$ with bias correction, and parameter update using a per-coordinate step size given by the ratio of the first and the square-rooted second moment plus a stabilizing constant.\n  - Adam with Decoupled Weight Decay (AdamW), which uses the same adaptive step as Adam and additionally applies a decoupled parameter shrinkage proportional to the current parameter vector.\n- Evolve the discrete-time dynamics for a fixed number of steps $T$, starting from an initial condition $\\mathbf{x}_0$.\n\nFrom fundamental base entities:\n- Exponential moving average uses the recursion of the form “new equals a convex combination of old and current observation,” which is a well-tested fact in stochastic signal processing.\n- Gradients on the quadratic are linear in $\\mathbf{x}$ via the Hessian $\\mathbf{H}$.\n- Bias correction divides by the factor $1-\\beta^t$ to account for the initial bias introduced by starting the exponential moving averages at zero.\n\nDefine the following measurable quantity to capture “update scale” in a single coordinate $i$ at iteration $t$:\n$$\n\\Delta_t^{(i)} \\;=\\; x_{t+1}^{(i)} - x_t^{(i)}.\n$$\nFor each run, let $i^\\star$ be the index of the largest curvature, that is $i^\\star \\in \\arg\\max_i h_i$. For each optimizer, compute\n$$\nM \\;=\\; \\max_{1 \\le t \\le T} \\left|\\Delta_t^{(i^\\star)}\\right|.\n$$\nYour objective is to demonstrate numerically that, for pathological curvature spectra (that is, $h_i$ spanning several orders of magnitude), the decoupled weight decay in AdamW can counteract the tendency of update magnitudes to grow in highly curved directions, particularly when the stabilizer dominates the denominator and the adaptive step partially scales with the gradient magnitude. Use the following test suite of parameter values:\n\n- Test case $\\mathbf{A}$ (happy path with large curvature contrast and moderate stabilizer):\n  - Dimension $d = 3$.\n  - Curvatures $\\mathbf{h} = [\\,1,\\;10^3,\\;10^6\\,]$.\n  - Initial point $\\mathbf{x}_0 = [\\,1,\\;1,\\;1\\,]$.\n  - Learning rate $\\alpha = 10^{-3}$.\n  - First moment coefficient $\\beta_1 = 0.9$.\n  - Second moment coefficient $\\beta_2 = 0.999$.\n  - Stabilizer $\\epsilon = 10^{-2}$.\n  - Weight decay $\\lambda = 10^{-1}$.\n  - Steps $T = 200$.\n\n- Test case $\\mathbf{B}$ (boundary case: no decay, should match Adam):\n  - Same as $\\mathbf{A}$ except weight decay $\\lambda = 0$.\n\n- Test case $\\mathbf{C}$ (different spectrum emphasizing moderately large top curvature and stronger stabilizer effect):\n  - Dimension $d = 3$.\n  - Curvatures $\\mathbf{h} = [\\,10^{-3},\\;1,\\;10^3\\,]$.\n  - Initial point $\\mathbf{x}_0 = [\\,1,\\;1,\\;1\\,]$.\n  - Learning rate $\\alpha = 10^{-3}$.\n  - First moment coefficient $\\beta_1 = 0.9$.\n  - Second moment coefficient $\\beta_2 = 0.999$.\n  - Stabilizer $\\epsilon = 10^{-1}$.\n  - Weight decay $\\lambda = 5\\times 10^{-2}$.\n  - Steps $T = 200$.\n\nFor each test case, perform two runs starting from the same initial $\\mathbf{x}_0$: one with Adam and one with AdamW. For each run, compute $M_{\\mathrm{Adam}}$ and $M_{\\mathrm{AdamW}}$ as the maximum magnitude of $\\Delta_t^{(i^\\star)}$ across iterations. The required result per test case is the float ratio\n$$\nR \\;=\\; \\frac{M_{\\mathrm{AdamW}}}{M_{\\mathrm{Adam}}}.\n$$\nYour program should produce a single line of output containing the three ratios for $\\mathbf{A}$, $\\mathbf{B}$, and $\\mathbf{C}$, as a comma-separated list enclosed in square brackets, for example\n$$\n[\\,R_{\\mathbf{A}},R_{\\mathbf{B}},R_{\\mathbf{C}}\\,].\n$$\nNo external input or files are permitted. All computations are unitless; there are no physical units or angle units involved. The numerical output must be exactly this single line.", "solution": "The problem requires a comparative analysis of the Adaptive Moment Estimation (Adam) optimizer and its variant with decoupled weight decay (AdamW). The analysis is to be performed via numerical simulation on a simple quadratic objective function, chosen to possess a challenging curvature spectrum. The goal is to numerically demonstrate how decoupled weight decay in AdamW mitigates the issue of large update steps in directions of high curvature, a scenario that can occur in standard Adam under certain conditions.\n\nFirst, we formalize the optimizers and the simulation environment.\n\nThe objective function is a $d$-dimensional quadratic form:\n$$\nf(\\mathbf{x}) = \\frac{1}{2}\\,\\mathbf{x}^\\top \\mathbf{H}\\,\\mathbf{x} = \\frac{1}{2}\\sum_{i=1}^{d} h_i\\,x_i^2\n$$\nwhere $\\mathbf{H} = \\mathrm{diag}(h_1, \\dots, h_d)$ is a diagonal matrix with positive entries $h_i > 0$, representing the curvatures along each coordinate axis. The gradient of this function is linear:\n$$\n\\mathbf{g}(\\mathbf{x}) = \\nabla f(\\mathbf{x}) = \\mathbf{H}\\,\\mathbf{x}\n$$\nThe optimizers aim to find the minimum of $f(\\mathbf{x})$, which is at $\\mathbf{x}=\\mathbf{0}$. We will simulate the discrete-time evolution of the parameter vector $\\mathbf{x}$ for $T$ steps, starting from an initial point $\\mathbf{x}_0$. Let $\\mathbf{x}_t$ be the parameter vector at the beginning of step $t$ (with $\\mathbf{x}_0$ being the initial state). The update from $\\mathbf{x}_{t-1}$ to $\\mathbf{x}_t$ is computed during step $t$.\n\nThe core of both Adam and AdamW is the use of exponential moving averages of the gradient and its square. Let $\\mathbf{g}_t = \\mathbf{g}(\\mathbf{x}_{t-1})$ be the gradient computed at step $t$. The first and second moment vectors, $\\mathbf{m}_t$ and $\\mathbf{v}_t$, are updated as follows:\n$$\n\\mathbf{m}_t = \\beta_1 \\mathbf{m}_{t-1} + (1-\\beta_1) \\mathbf{g}_t \\\\\n\\mathbf{v}_t = \\beta_2 \\mathbf{v}_{t-1} + (1-\\beta_2) (\\mathbf{g}_t \\odot \\mathbf{g}_t)\n$$\nwhere $\\odot$ denotes the element-wise product. The initial moments are $\\mathbf{m}_0 = \\mathbf{0}$ and $\\mathbf{v}_0 = \\mathbf{0}$. To counteract the initialization bias, bias-corrected estimates are used:\n$$\n\\hat{\\mathbf{m}}_t = \\frac{\\mathbf{m}_t}{1-\\beta_1^t} \\\\\n\\hat{\\mathbf{v}}_t = \\frac{\\mathbf{v}_t}{1-\\beta_2^t}\n$$\nThe parameters $\\beta_1$ and $\\beta_2$ are the exponential decay rates for the moving averages.\n\nThe update rules for the two optimizers are as follows:\n\n**1. Adam Optimizer:**\nThe problem specifies Adam without any weight decay. Its update rule for step $t$ is:\n$$\n\\mathbf{x}_t = \\mathbf{x}_{t-1} - \\alpha \\frac{\\hat{\\mathbf{m}}_t}{\\sqrt{\\hat{\\mathbf{v}}_t} + \\epsilon}\n$$\nHere, $\\alpha$ is the learning rate and $\\epsilon$ is a small constant for numerical stability.\n\n**2. AdamW Optimizer (Adam with Decoupled Weight Decay):**\nAdamW modifies the update by decoupling the weight decay from the gradient-based update. The weight decay is applied directly as a shrinkage of the parameters:\n$$\n\\mathbf{x}_t = \\mathbf{x}_{t-1} - \\alpha \\frac{\\hat{\\mathbf{m}}_t}{\\sqrt{\\hat{\\mathbf{v}}_t} + \\epsilon} - \\alpha \\lambda \\mathbf{x}_{t-1}\n$$\nwhere $\\lambda$ is the weight decay coefficient. The term $- \\alpha \\lambda \\mathbf{x}_{t-1}$ is the decoupled weight decay, which shrinks the parameters towards zero at a rate proportional to their magnitude, independent of the gradient information. For $\\lambda=0$, the AdamW update rule becomes identical to the Adam update rule.\n\nThe simulation will proceed for $T$ steps for each test case, for both optimizers. The measurable quantity of interest is the per-coordinate update magnitude, $\\Delta_t^{(i)} = x_{t}^{(i)} - x_{t-1}^{(i)}$. We are interested in the behavior along the direction of highest curvature, indexed by $i^\\star = \\arg\\max_i h_i$. For each optimization run, we compute the maximum update magnitude over all steps:\n$$\nM = \\max_{1 \\le t \\le T} \\left| \\Delta_t^{(i^\\star)} \\right|\n$$\nThe final result for each test case is the ratio $R = M_{\\mathrm{AdamW}} / M_{\\mathrm{Adam}}$.\n\nThe hypothesis being tested is that for ill-conditioned problems (large range of $h_i$), standard Adam can produce large, unstable updates. This can happen if the adaptive denominator $\\sqrt{\\hat{\\mathbf{v}}_t} + \\epsilon$ becomes dominated by the stabilizer $\\epsilon$ while the numerator $\\hat{\\mathbf{m}}_t$ remains large. The decoupled decay term in AdamW provides a consistent pull towards the origin, which can stabilize the trajectory and reduce the magnitude of these updates. For test case B where $\\lambda=0$, we expect $M_{\\mathrm{AdamW}} = M_{\\mathrm{Adam}}$, yielding a ratio $R=1$, which serves as a sanity check for the implementation. For the other cases with $\\lambda > 0$, we anticipate $R < 1$, indicating that AdamW produces smaller maximum updates along the high-curvature axis.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Test case A (happy path with large curvature contrast and moderate stabilizer)\n        {\n            'd': 3,\n            'h': np.array([1.0, 1e3, 1e6]),\n            'x0': np.array([1.0, 1.0, 1.0]),\n            'alpha': 1e-3,\n            'beta1': 0.9,\n            'beta2': 0.999,\n            'epsilon': 1e-2,\n            'lmbda': 1e-1,\n            'T': 200,\n        },\n        # Test case B (boundary case: no decay, should match Adam)\n        {\n            'd': 3,\n            'h': np.array([1.0, 1e3, 1e6]),\n            'x0': np.array([1.0, 1.0, 1.0]),\n            'alpha': 1e-3,\n            'beta1': 0.9,\n            'beta2': 0.999,\n            'epsilon': 1e-2,\n            'lmbda': 0.0,\n            'T': 200,\n        },\n        # Test case C (different spectrum emphasizing moderately large top curvature and stronger stabilizer effect)\n        {\n            'd': 3,\n            'h': np.array([1e-3, 1.0, 1e3]),\n            'x0': np.array([1.0, 1.0, 1.0]),\n            'alpha': 1e-3,\n            'beta1': 0.9,\n            'beta2': 0.999,\n            'epsilon': 1e-1,\n            'lmbda': 5e-2,\n            'T': 200,\n        },\n    ]\n\n    ratios = []\n\n    for case in test_cases:\n        # Adam run (lambda = 0)\n        adam_params = case.copy()\n        adam_params['lmbda'] = 0.0\n        M_adam = run_optimizer(adam_params)\n        \n        # AdamW run (lambda from test case)\n        M_adamw = run_optimizer(case)\n        \n        # Guard against division by zero, though not expected here\n        if M_adam == 0.0:\n            ratio = 0.0 if M_adamw == 0.0 else float('inf')\n        else:\n            ratio = M_adamw / M_adam\n        \n        ratios.append(ratio)\n\n    print(f\"[{','.join(f'{r:.7f}' for r in ratios)}]\")\n\n\ndef run_optimizer(params):\n    \"\"\"\n    Runs a single optimization trajectory for T steps.\n    \n    Args:\n        params (dict): A dictionary containing all hyperparameters for the run,\n                       including the weight decay 'lmbda'. If lmbda is 0, this\n                       simulates the Adam optimizer.\n    \n    Returns:\n        float: The maximum update magnitude along the direction of highest curvature.\n    \"\"\"\n    h = params['h']\n    x0 = params['x0']\n    alpha = params['alpha']\n    beta1 = params['beta1']\n    beta2 = params['beta2']\n    epsilon = params['epsilon']\n    lmbda = params['lmbda']\n    T = params['T']\n    \n    d = len(h)\n    x = np.copy(x0).astype(np.float64)\n    m = np.zeros(d, dtype=np.float64)\n    v = np.zeros(d, dtype=np.float64)\n    \n    H = np.diag(h)\n    istar = np.argmax(h)\n    \n    max_delta_magnitude = 0.0\n\n    # The loop for steps t = 1, 2, ..., T\n    for t in range(1, T + 1):\n        x_prev = np.copy(x)\n        \n        # Calculate gradient at x_{t-1}\n        grad = H @ x_prev\n        \n        # Update first and second moment estimates\n        m = beta1 * m + (1 - beta1) * grad\n        v = beta2 * v + (1 - beta2) * (grad**2)\n        \n        # Compute bias-corrected moment estimates\n        m_hat = m / (1 - beta1**t)\n        v_hat = v / (1 - beta2**t)\n        \n        # Compute the Adam-like update step\n        adaptive_step = alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n        \n        # Apply decoupled weight decay term\n        weight_decay_step = alpha * lmbda * x_prev\n        \n        # Update parameters to get x_t\n        x = x_prev - adaptive_step - weight_decay_step\n        \n        # Calculate update delta for this step\n        delta_t = x - x_prev\n        \n        # Track the maximum magnitude of the update for the highest curvature coordinate\n        current_delta_mag = np.abs(delta_t[istar])\n        if current_delta_mag > max_delta_magnitude:\n            max_delta_magnitude = current_delta_mag\n            \n    return max_delta_magnitude\n\nsolve()\n```", "id": "3096484"}, {"introduction": "Moving from core principles to practical application, we address a crucial question: should all model parameters be treated equally by weight decay? This practice problem investigates the common and highly effective heuristic of excluding bias parameters from regularization. Through a hands-on coding experiment with a toy convolutional network, you will test the hypothesis that applying decay to weights but not biases leads to better performance, especially when the data itself is not centered at zero [@problem_id:3096544]. This exercise provides not only empirical evidence for this best practice but also reinforces the powerful interpretation of $L_2$ regularization as a zero-mean Gaussian prior, a belief that is often appropriate for weights but not for biases.", "problem": "You are asked to implement and analyze a minimal, fully deterministic experiment that isolates the effect of decoupled weight decay on different parameter types in a convolutional neural network. The goal is to demonstrate, in a simple and reproducible setting, that excluding bias parameters from decay while including convolution kernels improves test accuracy when the data distribution demands a nonzero bias, and to explain this observation through a Bayesian interpretation involving an $L_2$ prior.\n\nYou must write a complete, runnable program that:\n- Implements a toy one-dimensional convolutional classifier with a single filter of length $k$, applied in valid mode with stride $1$, followed by averaging and a sigmoid output for binary classification. For an input vector $x \\in \\mathbb{R}^{L}$, let the number of valid positions be $M = L - k + 1$. Define the feature vector $f(x) \\in \\mathbb{R}^{k}$ by\n$$\nf_i(x) = \\frac{1}{M} \\sum_{j=0}^{M-1} x_{j+i}, \\quad \\text{for } i \\in \\{0,1,\\dots,k-1\\}.\n$$\nWith convolution kernel weights $w \\in \\mathbb{R}^{k}$ and a scalar bias $b \\in \\mathbb{R}$, the logit is $z(x) = w^\\top f(x) + b$ and the prediction is $\\sigma(z(x))$, where $\\sigma(u) = \\frac{1}{1 + e^{-u}}$.\n- Trains the model using Adaptive Moment Estimation with decoupled Weight decay (AdamW), once with decay applied to both $w$ and $b$ and once with decay applied to $w$ only. Use binary cross-entropy as the objective. You must implement a decoupled weight decay consistent with AdamW; do not couple the decay into the data gradient.\n- Evaluates test accuracy for both training regimes on the same test set and reports the difference.\n\nThe dataset is synthetic and must be generated as follows. For a specified pair of class-dependent baselines $(\\mu_0,\\mu_1)$, sequence length $L$, noise standard deviation $\\sigma$, and sample size $N$, generate inputs $x \\in \\mathbb{R}^{L}$ by sampling a class label $y \\in \\{0,1\\}$, setting a baseline $m_y \\in \\{\\mu_0,\\mu_1\\}$, and drawing\n$$\nx = m_y \\cdot \\mathbf{1}_L + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_L),\n$$\nwhere $\\mathbf{1}_L$ is the all-ones vector in $\\mathbb{R}^{L}$. Labels $y$ are drawn uniformly at random unless otherwise specified. The training loss is the empirical mean of binary cross-entropy over the training set.\n\nUse a single hyperparameter configuration for training, applied identically across all cases:\n- Learning rate $\\alpha$ fixed to a positive scalar you deem reasonable and constant across all test cases.\n- First and second moment coefficients $(\\beta_1,\\beta_2)$ fixed across all test cases.\n- Number of epochs $T$ fixed across all test cases.\n- Numerical stability parameter $\\varepsilon$ fixed across all test cases.\n\nYou must construct four test cases that collectively form the test suite below. For each case, you must generate independent training and test sets using the specified seeds. Keep the training and evaluation procedures identical except for whether the bias $b$ is decayed or not.\n\nTest suite specification:\n- Case $1$ (happy path, asymmetric means, moderate decay): $(L,k,N_{\\text{train}},N_{\\text{test}}) = (16,3,256,512)$, $(\\mu_0,\\mu_1) = (0.2, 1.2)$, $\\sigma = 0.3$, weight decay $\\lambda = 0.05$, training random seed $42$, testing random seed $1042$.\n- Case $2$ (boundary, zero decay): same as Case $1$ except $\\lambda = 0.0$, training random seed $43$, testing random seed $1043$.\n- Case $3$ (edge, strong decay): same as Case $1$ except $\\lambda = 0.5$, training random seed $44$, testing random seed $1044$.\n- Case $4$ (control, symmetric means around zero): $(L,k,N_{\\text{train}},N_{\\text{test}}) = (16,3,256,512)$, $(\\mu_0,\\mu_1) = (-0.6, 0.6)$, $\\sigma = 0.3$, weight decay $\\lambda = 0.1$, training random seed $45$, testing random seed $1045$.\n\nWhat to compute and output:\n- For each case $c \\in \\{1,2,3,4\\}$, train two models: one with decay on both $w$ and $b$ and one with decay on $w$ only. Evaluate the test accuracy in both regimes on the corresponding test set, and compute the difference\n$$\n\\Delta_c = \\text{Acc}_{\\text{no-bias-decay}} - \\text{Acc}_{\\text{with-bias-decay}}.\n$$\n- Your program must produce a single line containing the list $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$, with each value rounded to exactly three digits after the decimal point.\n\nScientific grounding requirement:\n- Your reasoning in the solution must begin from the definition of binary cross-entropy, the logistic function, and the Maximum A Posteriori (MAP) view that an $L_2$ penalty corresponds to a zero-mean Gaussian prior. Do not state or rely on any update formula for AdamW in the problem statement; instead, implement it directly in the code using decoupled weight decay.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets; for example: $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$.", "solution": "The problem asks for an implementation and analysis of decoupled weight decay, specifically its differential application to weights and biases in a simple neural network. We are to demonstrate that excluding the bias parameter from weight decay is advantageous when the data's underlying distribution necessitates a non-zero optimal bias.\n\nThe analysis begins from a Bayesian probabilistic perspective on model regularization. In a supervised learning context, we aim to find the parameters $\\theta$ of a model that best explain the data $D$. This is often framed as Maximum A Posteriori (MAP) estimation, where we seek to maximize the posterior probability of the parameters given the data, $P(\\theta|D)$. Using Bayes' theorem, this is equivalent to minimizing the negative log-posterior:\n$$\n\\theta_{\\text{MAP}} = \\arg\\min_{\\theta} [-\\log P(D|\\theta) - \\log P(\\theta)]\n$$\nThe term $-\\log P(D|\\theta)$ is the negative log-likelihood of the data, which corresponds to the loss function. For the binary classification task specified, the model output is a probability $p = \\sigma(z)$, where $\\sigma(\\cdot)$ is the logistic sigmoid function and $z$ is the logit. The likelihood for a single data point $(x, y)$ with label $y \\in \\{0, 1\\}$ is given by the Bernoulli distribution, $P(y|x, \\theta) = p^y(1-p)^{1-y}$. The negative log-likelihood for one sample is therefore:\n$$\n-\\log P(y|x, \\theta) = -[y \\log(p) + (1-y)\\log(1-p)]\n$$\nThis is precisely the binary cross-entropy loss function. The total negative log-likelihood is the sum of this loss over all training samples.\n\nThe second term, $-\\log P(\\theta)$, is derived from a prior distribution over the parameters. This prior encodes our beliefs about the parameters before observing any data. A common choice is to assume that parameters are likely to be small and centered around zero. This belief can be formalized by placing an independent, zero-mean Gaussian prior on each parameter $\\theta_i$:\n$$\nP(\\theta_i) \\propto \\exp\\left(-\\frac{\\lambda}{2}\\theta_i^2\\right)\n$$\nThe corresponding negative log-prior for the entire parameter vector $\\theta$ becomes:\n$$\n-\\log P(\\theta) = \\frac{\\lambda}{2} \\sum_i \\theta_i^2 + \\text{const} = \\frac{\\lambda}{2} \\|\\theta\\|_2^2 + \\text{const}\n$$\nThis term is the $L_2$ regularization penalty. Minimizing the sum of the loss and this penalty is thus equivalent to MAP estimation with a zero-mean Gaussian prior.\n\nIn traditional gradient descent, this $L_2$ penalty is added to the loss function, and its gradient, $\\lambda\\theta$, is added to the data loss gradient. For adaptive optimizers like Adam, this \"coupling\" of the regularization gradient with the data gradient is problematic, as the adaptive scaling (from the momentum terms) rescales the regularization effect in a complex, parameter-dependent way. Decoupled weight decay, as popularized by AdamW, restores the original intent of $L_2$ regularization by applying the weight decay update `theta <- theta - eta * lambda * theta` separately from the adaptive gradient-based update.\n\nThe core of the problem lies in critically assessing the validity of the zero-mean Gaussian prior for different parameter types. For weights ($w$), which mediate the interaction between features, assuming they are small and centered at zero is often a reasonable default. However, for bias parameters ($b$), this assumption can be flawed. A bias term's function is to shift the output of a neuron, effectively centering the activation function over the distribution of its inputs. The optimal bias, therefore, depends directly on the mean of the features it receives.\n\nIn this problem, the input data is generated as $x = m_y \\cdot \\mathbf{1}_L + \\epsilon$, where $m_y$ is the class-conditional mean. The feature vector $f(x)$ is constructed such that the expected value of each of its components is the class mean: $E[f_i(x)|y] = m_y$. The logit is $z(x) = w^\\top f(x) + b$. Its expectation is:\n$$\nE[z(x)|y] = w^\\top E[f(x)|y] + b = w^\\top (m_y \\mathbf{1}_k) + b = m_y \\left(\\sum_i w_i\\right) + b\n$$\nConsider Case 1, with asymmetric means $(\\mu_0, \\mu_1) = (0.2, 1.2)$. Both class means are positive. To correctly classify the inputs, the model must learn parameters such that, on average, $E[z(x)|y=0] < 0$ and $E[z(x)|y=1] > 0$. This requires $0.2(\\sum w_i) + b < 0$ and $1.2(\\sum w_i) + b > 0$. It is evident that the optimal bias $b$ must be non-zero to shift the decision boundary appropriately to accommodate the positive offset in the data. Applying weight decay to $b$ imposes a penalty that pulls it towards zero, directly conflicting with the optimization objective. This should result in lower test accuracy.\n\nConversely, in Case 4 with symmetric means $(\\mu_0, \\mu_1) = (-0.6, 0.6)$, the data is centered around zero. The optimal solution likely has a bias $b$ close to zero. In this scenario, applying weight decay to the bias is not detrimental and may provide a minor regularizing effect. Thus, the performance difference between decaying and not decaying the bias should be negligible.\n\nCase 2, with zero weight decay $(\\lambda=0)$, serves as a control. Both training regimes are identical, so the accuracy difference $\\Delta_2$ must be zero. Case 3 uses a strong decay factor, which should magnify the negative effect of inappropriately decaying the bias in the asymmetric data setting.\n\nTherefore, we hypothesize that $\\Delta_c = \\text{Acc}_{\\text{no-bias-decay}} - \\text{Acc}_{\\text{with-bias-decay}}$ will be significantly positive for Cases 1 and 3, approximately zero for Case 4, and exactly zero for Case 2. The experiment is designed to confirm this through a direct, reproducible simulation.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the complete experiment to analyze decoupled weight decay.\n    \"\"\"\n    \n    # --- Hyperparameters ---\n    # Fixed for all test cases as per the problem description.\n    ALPHA = 0.01  # Learning rate\n    BETA_1 = 0.9\n    BETA_2 = 0.999\n    EPSILON = 1e-8\n    EPOCHS = 100  # Number of training epochs\n\n    def generate_data(n_samples, l_seq, mu0, mu1, sigma_noise, seed):\n        \"\"\"Generates synthetic data according to the problem specification.\"\"\"\n        rng = np.random.RandomState(seed)\n        labels = rng.randint(0, 2, size=(n_samples, 1))\n        means = np.where(labels == 0, mu0, mu1)\n        # Ensure means array is broadcastable to data shape\n        means = means.reshape(-1, 1)\n        noise = rng.randn(n_samples, l_seq) * sigma_noise\n        data = means * np.ones((1, l_seq)) + noise\n        return data, labels\n\n    def get_features(X, k):\n        \"\"\"Computes the feature vector f(x) for each sample in X.\"\"\"\n        N, L = X.shape\n        M = L - k + 1\n        features = np.zeros((N, k))\n        for i in range(k):\n            features[:, i] = np.mean(X[:, i:i + M], axis=1)\n        return features\n\n    def train(X_train, y_train, k, weight_decay, decay_bias, seed):\n        \"\"\"Trains the model using AdamW, as specified in the original paper.\"\"\"\n        rng = np.random.RandomState(seed)\n        # Initialize parameters\n        w = rng.randn(k, 1) * 0.01\n        b = 0.0\n\n        # Adam moment estimates\n        m_w, v_w = np.zeros_like(w), np.zeros_like(w)\n        m_b, v_b = 0.0, 0.0\n\n        num_samples = X_train.shape[0]\n        F_train = get_features(X_train, k)\n\n        for t in range(1, EPOCHS + 1):\n            # Forward pass (at w_{t-1}, b_{t-1})\n            z = F_train @ w + b\n            p = 1.0 / (1.0 + np.exp(-z))  # Sigmoid activation\n\n            # Backward pass (gradient of the loss)\n            grad_z = p - y_train\n            grad_w = (F_train.T @ grad_z) / num_samples\n            grad_b = np.mean(grad_z)\n\n            # --- Adam Moment Updates ---\n            m_w = BETA_1 * m_w + (1 - BETA_1) * grad_w\n            m_b = BETA_1 * m_b + (1 - BETA_1) * grad_b\n            \n            v_w = BETA_2 * v_w + (1 - BETA_2) * (grad_w ** 2)\n            v_b = BETA_2 * v_b + (1 - BETA_2) * (grad_b ** 2)\n\n            # Bias-corrected estimates\n            m_w_hat = m_w / (1 - BETA_1 ** t)\n            m_b_hat = m_b / (1 - BETA_1 ** t)\n            v_w_hat = v_w / (1 - BETA_2 ** t)\n            v_b_hat = v_b / (1 - BETA_2 ** t)\n            \n            # --- Parameter Update ---\n            # Update from Adam step\n            w -= ALPHA * m_w_hat / (np.sqrt(v_w_hat) + EPSILON)\n            b -= ALPHA * m_b_hat / (np.sqrt(v_b_hat) + EPSILON)\n            \n            # Update from decoupled weight decay\n            w -= ALPHA * weight_decay * w\n            if decay_bias:\n                b -= ALPHA * weight_decay * b\n        \n        return w, b\n\n    def evaluate(X_test, y_test, w, b, k):\n        \"\"\"Evaluates model accuracy on the test set.\"\"\"\n        F_test = get_features(X_test, k)\n        z = F_test @ w + b\n        predictions = (z > 0).astype(int)\n        accuracy = np.mean(predictions == y_test)\n        return accuracy\n\n    test_cases = [\n        # Case 1 (happy path, asymmetric means, moderate decay)\n        {'L': 16, 'k': 3, 'N_train': 256, 'N_test': 512, 'mu0': 0.2, 'mu1': 1.2, 'sigma': 0.3, 'lambda': 0.05, 'train_seed': 42, 'test_seed': 1042},\n        # Case 2 (boundary, zero decay)\n        {'L': 16, 'k': 3, 'N_train': 256, 'N_test': 512, 'mu0': 0.2, 'mu1': 1.2, 'sigma': 0.3, 'lambda': 0.0, 'train_seed': 43, 'test_seed': 1043},\n        # Case 3 (edge, strong decay)\n        {'L': 16, 'k': 3, 'N_train': 256, 'N_test': 512, 'mu0': 0.2, 'mu1': 1.2, 'sigma': 0.3, 'lambda': 0.5, 'train_seed': 44, 'test_seed': 1044},\n        # Case 4 (control, symmetric means around zero)\n        {'L': 16, 'k': 3, 'N_train': 256, 'N_test': 512, 'mu0': -0.6, 'mu1': 0.6, 'sigma': 0.3, 'lambda': 0.1, 'train_seed': 45, 'test_seed': 1045},\n    ]\n\n    delta_results = []\n    for case in test_cases:\n        # Generate data\n        X_train, y_train = generate_data(case['N_train'], case['L'], case['mu0'], case['mu1'], case['sigma'], case['train_seed'])\n        X_test, y_test = generate_data(case['N_test'], case['L'], case['mu0'], case['mu1'], case['sigma'], case['test_seed'])\n\n        # --- Regime 1: Decay applied to weights and bias ---\n        w_decay, b_decay = train(X_train, y_train, case['k'], case['lambda'], decay_bias=True, seed=case['train_seed'])\n        acc_with_bias_decay = evaluate(X_test, y_test, w_decay, b_decay, case['k'])\n\n        # --- Regime 2: Decay applied to weights only ---\n        w_nodecay, b_nodecay = train(X_train, y_train, case['k'], case['lambda'], decay_bias=False, seed=case['train_seed'])\n        acc_no_bias_decay = evaluate(X_test, y_test, w_nodecay, b_nodecay, case['k'])\n\n        # Compute and store the difference\n        delta = acc_no_bias_decay - acc_with_bias_decay\n        delta_results.append(delta)\n\n    # Format the results as specified\n    formatted_results = [f\"{r:.3f}\" for r in delta_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3096544"}]}