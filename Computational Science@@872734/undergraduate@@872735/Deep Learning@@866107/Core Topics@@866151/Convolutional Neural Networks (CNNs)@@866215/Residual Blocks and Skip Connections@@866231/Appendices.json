{"hands_on_practices": [{"introduction": "A powerful way to understand residual networks is to view them as a discretization of a continuous-time dynamical system. For a network to remain trainable as it becomes very deep, the transformations it learns must be stable, preventing the signal from either vanishing or exploding. This exercise provides a rigorous, first-principles derivation of the stability conditions for a simplified linear residual block, linking the architecture of deep neural networks to the foundational concepts of numerical analysis and stability theory [@problem_id:3169711].", "problem": "Consider a residual block used in deep neural networks with skip connections, where the layer update is given by $x_{l+1} = x_{l} + h F(x_{l})$ for a step size $h>0$ and layer index $l \\in \\mathbb{N}$. Suppose the residual function is linear, $F(x) = A x$, with $A \\in \\mathbb{C}^{n \\times n}$ a normal matrix (that is, $A^{*}A = A A^{*}$), so that $A$ is unitarily diagonalizable. Denote by $\\|\\cdot\\|_{2}$ the Euclidean norm on $\\mathbb{C}^{n}$ and the associated induced operator norm on matrices. To prevent exploding norms under repeated application of the residual block, we require the single-step map $B(h) \\equiv I + h A$ to be non-expansive in the Euclidean norm, i.e., $\\|B(h)\\|_{2} \\leq 1$.\n\nStarting only from the spectral theorem for normal matrices, the definition of the induced operator norm, and the property that the eigenvalues of a polynomial in a matrix are the polynomial evaluated at the eigenvalues, derive a condition on $h$ and the eigenvalues $\\{\\lambda_{i}\\}_{i=1}^{n}$ of $A$ that guarantees $\\|I + h A\\|_{2} \\leq 1$. Express this as a stability region in the complex plane for $h \\lambda$, written as an explicit inequality involving $\\operatorname{Re}(\\lambda)$ and $|\\lambda|$.\n\nThen, consider a concrete normal matrix $A$ whose spectrum is $\\{-2,\\,-0.5 + i,\\,-0.5 - i,\\,-0.2\\}$. Determine the supremum step size $h^{\\star} > 0$ such that $\\|I + h A\\|_{2} \\leq 1$ holds for all $0 \\leq h \\leq h^{\\star}$. Express your final answer as a single real number. No rounding is required.", "solution": "The problem asks for two parts: first, to derive a stability condition for a residual block with a linear residual function, and second, to apply this condition to a specific case to find a maximum allowable step size.\n\nPart 1: Derivation of the Stability Condition\n\nWe are given the layer update rule $x_{l+1} = (I + hA)x_l$ where $A$ is a normal matrix. The condition to prevent exploding norms is that the map $B(h) = I + hA$ is non-expansive, i.e., $\\|I + hA\\|_2 \\leq 1$.\n\nWe start by determining the 2-norm of the matrix $B(h) = I + hA$.\nThe problem states that $A \\in \\mathbb{C}^{n \\times n}$ is a normal matrix. It follows that $B(h)$ is also a normal matrix for any real step size $h$.\n\nAccording to the spectral theorem, a normal matrix is unitarily diagonalizable. A key property that follows is that the induced 2-norm of a normal matrix is equal to its spectral radius (the maximum absolute value of its eigenvalues).\nLet $\\{\\lambda_i\\}_{i=1}^n$ be the eigenvalues of $A$. The problem states that the eigenvalues of a polynomial in a matrix are the polynomial evaluated at the eigenvalues. The matrix $B(h) = I + hA$ can be seen as the evaluation of the polynomial $p(z) = 1 + hz$ at $A$. Therefore, the eigenvalues of $B(h)$ are $\\{1 + h\\lambda_i\\}_{i=1}^n$.\n\nThe 2-norm of $B(h)$ is then:\n$$ \\|I + hA\\|_2 = \\rho(I + hA) = \\max_{i \\in \\{1, \\dots, n\\}} |1 + h\\lambda_i| $$\nThe non-expansive condition $\\|I + hA\\|_2 \\leq 1$ thus requires that for every eigenvalue $\\lambda_i$ of $A$, the following inequality holds:\n$$ |1 + h\\lambda_i| \\leq 1 $$\nThis is the fundamental condition. Let's analyze it further. Letting $z_i = h\\lambda_i$, the condition is that for each $i$, the complex number $z_i$ must lie within the stability region defined by $|1+z| \\leq 1$. Geometrically, this is a closed disk in the complex plane centered at $(-1, 0)$ with a radius of $1$.\n\nTo derive the explicit inequality involving $\\operatorname{Re}(\\lambda)$ and $|\\lambda|$, we square both sides of $|1 + h\\lambda| \\leq 1$:\n$$ |1 + h\\lambda|^2 \\leq 1^2 $$\nLet $\\lambda = \\operatorname{Re}(\\lambda) + i\\operatorname{Im}(\\lambda)$. The term $h\\lambda = h\\operatorname{Re}(\\lambda) + ih\\operatorname{Im}(\\lambda)$.\nThe squared magnitude is:\n$$ (1 + h\\operatorname{Re}(\\lambda))^2 + (h\\operatorname{Im}(\\lambda))^2 \\leq 1 $$\nExpanding the left side gives:\n$$ 1 + 2h\\operatorname{Re}(\\lambda) + h^2(\\operatorname{Re}(\\lambda))^2 + h^2(\\operatorname{Im}(\\lambda))^2 \\leq 1 $$\nCombining the terms with $h^2$:\n$$ 1 + 2h\\operatorname{Re}(\\lambda) + h^2((\\operatorname{Re}(\\lambda))^2 + (\\operatorname{Im}(\\lambda))^2) \\leq 1 $$\nRecognizing that $(\\operatorname{Re}(\\lambda))^2 + (\\operatorname{Im}(\\lambda))^2 = |\\lambda|^2$, we have:\n$$ 1 + 2h\\operatorname{Re}(\\lambda) + h^2|\\lambda|^2 \\leq 1 $$\nSubtracting $1$ from both sides yields:\n$$ 2h\\operatorname{Re}(\\lambda) + h^2|\\lambda|^2 \\leq 0 $$\nSince the step size $h$ is given to be positive ($h>0$), we can divide the inequality by $h$ without changing its direction:\n$$ 2\\operatorname{Re}(\\lambda) + h|\\lambda|^2 \\leq 0 $$\nThis is the condition that must be satisfied by $h$ and every eigenvalue $\\lambda$ of the matrix $A$.\n\nPart 2: Calculation of the Supremum Step Size $h^{\\star}$\n\nWe are given a matrix $A$ with the spectrum (set of eigenvalues) $\\{-2, -0.5 + i, -0.5 - i, -0.2\\}$. We must find the supremum $h^{\\star} > 0$ such that the condition $2\\operatorname{Re}(\\lambda) + h|\\lambda|^2 \\leq 0$ is satisfied for all $h \\in [0, h^{\\star}]$ and for all eigenvalues $\\lambda$ in the spectrum.\n\nFor a non-zero eigenvalue $\\lambda$, we can rearrange the inequality to find an upper bound on $h$:\n$$ h|\\lambda|^2 \\leq -2\\operatorname{Re}(\\lambda) $$\nFor a positive solution $h > 0$ to exist, we must have $-2\\operatorname{Re}(\\lambda) > 0$, which implies $\\operatorname{Re}(\\lambda) < 0$. We check this for all given eigenvalues:\n\\begin{itemize}\n    \\item $\\operatorname{Re}(-2) = -2 < 0$\n    \\item $\\operatorname{Re}(-0.5+i) = -0.5 < 0$\n    \\item $\\operatorname{Re}(-0.5-i) = -0.5 < 0$\n    \\item $\\operatorname{Re}(-0.2) = -0.2 < 0$\n\\end{itemize}\nSince all eigenvalues have negative real parts, a positive step size $h$ is possible.\nIf $|\\lambda|^2 \\neq 0$, the inequality for $h$ is:\n$$ h \\leq \\frac{-2\\operatorname{Re}(\\lambda)}{|\\lambda|^2} $$\nThis inequality must hold for every eigenvalue. Therefore, $h$ must be less than or equal to the minimum of these upper bounds over all eigenvalues. The supremum $h^{\\star}$ is this minimum value.\n$$ h^{\\star} = \\min_{\\lambda \\in \\text{spectrum}} \\left( \\frac{-2\\operatorname{Re}(\\lambda)}{|\\lambda|^2} \\right) $$\nLet's compute this value for each eigenvalue:\n\n1.  For $\\lambda_1 = -2$:\n    $\\operatorname{Re}(\\lambda_1) = -2$ and $|\\lambda_1|^2 = (-2)^2 = 4$.\n    The bound on $h$ is $h \\leq \\frac{-2(-2)}{4} = \\frac{4}{4} = 1$.\n\n2.  For $\\lambda_2 = -0.5 + i$:\n    $\\operatorname{Re}(\\lambda_2) = -0.5$ and $|\\lambda_2|^2 = (-0.5)^2 + 1^2 = 0.25 + 1 = 1.25$.\n    The bound on $h$ is $h \\leq \\frac{-2(-0.5)}{1.25} = \\frac{1}{1.25} = \\frac{1}{5/4} = \\frac{4}{5} = 0.8$.\n\n3.  For $\\lambda_3 = -0.5 - i$:\n    $\\operatorname{Re}(\\lambda_3) = -0.5$ and $|\\lambda_3|^2 = (-0.5)^2 + (-1)^2 = 0.25 + 1 = 1.25$.\n    The bound on $h$ is $h \\leq \\frac{-2(-0.5)}{1.25} = \\frac{1}{1.25} = 0.8$.\n\n4.  For $\\lambda_4 = -0.2$:\n    $\\operatorname{Re}(\\lambda_4) = -0.2$ and $|\\lambda_4|^2 = (-0.2)^2 = 0.04$.\n    The bound on $h$ is $h \\leq \\frac{-2(-0.2)}{0.04} = \\frac{0.4}{0.04} = 10$.\n\nThe set of upper bounds for $h$ is $\\{1, 0.8, 0.8, 10\\}$. To satisfy the condition for all eigenvalues simultaneously, $h$ must be less than or equal to the minimum of these bounds.\n$$ h^{\\star} = \\min\\{1, 0.8, 10\\} = 0.8 $$\nThus, the supremum step size for which the system remains non-expansive for all $h \\in [0, h^{\\star}]$ is $0.8$.", "answer": "$$\\boxed{0.8}$$", "id": "3169711"}, {"introduction": "While identity skip connections provide a crucial stability backbone, they are not a universal solution for all tasks. Their effectiveness hinges on the assumption that the desired mapping is a small perturbation of the identity. This practice challenges that assumption by exploring a scenario where the network must learn to significantly suppress its input, a task for which a simple identity skip is ill-suited. By analyzing and quantifying the performance gap between a standard residual block and one with a learnable projection, you will gain a deeper appreciation for the functional limitations of the identity mapping and the value of more flexible shortcut paths [@problem_id:3169751].", "problem": "Consider the canonical residual block used in deep learning, where an input vector $x \\in \\mathbb{R}^n$ is transformed to an output $y \\in \\mathbb{R}^n$ using a skip connection and a learnable residual branch. Two variants are considered:\n\n- Identity skip: $y = x + F_\\theta(x)$.\n- Projection skip: $y = P_\\phi x + F_\\theta(x)$, where $P_\\phi$ is a learnable linear projection.\n\nAssume $F_\\theta$ is instantiated using a two-layer network with the hyperbolic tangent nonlinearity, that is $F_\\theta(x) = W_2 \\tanh(W_1 x)$ with $W_1 \\in \\mathbb{R}^{k \\times n}$ and $W_2 \\in \\mathbb{R}^{n \\times k}$, where $k \\in \\mathbb{N}$. Due to the bounded range of the hyperbolic tangent, and standard weight regularization practices, suppose that the residual output is component-wise bounded: for all $x$ and all output indices $i$, $F_\\theta(x)_i \\in [-c, c]$ for a given constant $c \\ge 0$. This models a common regime in which the residual branch cannot arbitrarily cancel large input magnitudes.\n\nWe will study a target mapping that strongly suppresses inputs, defined for a given scalar $s \\in \\mathbb{R}$ as $T(x) = s x$, with $|s| \\le 1$. For a finite dataset consisting of input vectors $\\{x^{(j)}\\}_{j=1}^N$, define the Mean Squared Error (MSE) as\n$$\n\\operatorname{MSE} = \\frac{1}{n N} \\sum_{j=1}^N \\left\\| \\hat{y}(x^{(j)}) - T\\left(x^{(j)}\\right) \\right\\|_2^2.\n$$\n\nYour task is to, from first principles and without using any machine learning libraries, compute the minimal achievable MSE for each scenario under the stated residual branch bound:\n- Identity skip: $y = x + F_\\theta(x)$ with the constraint that each component of $F_\\theta(x)$ lies in $[-c, c]$.\n- Projection skip: $y = \\alpha x + F_\\theta(x)$ where the projection is restricted to scalar scaling $P_\\phi = \\alpha I_n$ and $I_n$ is the identity matrix in $\\mathbb{R}^{n \\times n}$, with the same residual constraint on $F_\\theta(x)$.\n\nYou should reason about the optimal choice of the residual output under the bound, as well as the optimal scalar $\\alpha$ when allowed, and compute the resulting minimal MSEs.\n\nTest Suite:\nUse the following parameter sets, each consisting of a single input vector $x \\in \\mathbb{R}^n$, a scalar target factor $s$, and a residual bound $c$:\n- Case $1$: $x = [5, 5, 5, 5]$, $s = 0$, $c = 1$.\n- Case $2$: $x = [1, 1, 1, 1]$, $s = 0$, $c = 1$.\n- Case $3$: $x = [2, -3, 4, -1]$, $s = 0.2$, $c = 1$.\n- Case $4$: $x = [3, -3, 3, -3]$, $s = 0$, $c = 3$.\n- Case $5$: $x = [10, -1, 3, -7]$, $s = 0.9$, $c = 0.05$.\n\nFor each case, compute two quantities:\n- The minimal MSE achievable with identity skip.\n- The minimal MSE achievable with projection skip.\n\nFinally, report, for each case, the difference between these two minimal MSEs (identity minus projection). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order of the cases $1$ through $5$ (e.g., $[d_1,d_2,d_3,d_4,d_5]$), where each $d_j$ is a floating-point number giving the difference for case $j$.\n\nNo physical units or angle units are involved. All results must be numerical floats. The program must be self-contained and not rely on external files or user input.", "solution": "The problem asks us to find the minimal achievable Mean Squared Error (MSE) for two types of residual blocks, given a constraint on the residual branch output. Since the test cases consist of a single input vector ($N=1$), the MSE formula simplifies to $\\operatorname{MSE} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - T(x)_i)^2$. We can minimize the total MSE by minimizing each squared error term $(\\hat{y}_i - s x_i)^2$ independently.\n\n**Case 1: Identity Skip**\n\nFor the identity skip block, the output is $y = x + F_\\theta(x)$. The $i$-th component is $\\hat{y}_i = x_i + z_i$, where $z_i = (F_\\theta(x))_i$ is the output of the residual branch, constrained to $z_i \\in [-c, c]$.\n\nThe squared error for the $i$-th component is:\n$$ E_i = (x_i + z_i - s x_i)^2 = ((1-s)x_i + z_i)^2 $$\nTo minimize this squared term with respect to $z_i$, we must choose the value of $z_i$ in the interval $[-c, c]$ that is closest to $-(1-s)x_i = (s-1)x_i$. This is a clipping operation. The optimal residual component $z_i^*$ is:\n$$ z_i^* = \\operatorname{clip}((s-1)x_i, -c, c) $$\nwhere $\\operatorname{clip}(v, v_{\\min}, v_{\\max}) = \\max(v_{\\min}, \\min(v, v_{\\max}))$.\n\nThe minimum squared error for this component is $E_i^* = ((1-s)x_i + z_i^*)^2$. The total minimal MSE for the identity skip case is the average of these component-wise errors:\n$$ \\operatorname{MSE}_{\\text{id}} = \\frac{1}{n} \\sum_{i=1}^n ((1-s)x_i + \\operatorname{clip}((s-1)x_i, -c, c))^2 $$\n\n**Case 2: Projection Skip**\n\nFor the projection skip block, the output is $y = \\alpha x + F_\\theta(x)$. Here, both the scalar $\\alpha$ and the residual function $F_\\theta$ are learnable. The $i$-th component is $\\hat{y}_i = \\alpha x_i + z_i$, with the same constraint $z_i \\in [-c, c]$.\n\nThe squared error for the $i$-th component is:\n$$ E_i = (\\alpha x_i + z_i - s x_i)^2 = ((\\alpha-s)x_i + z_i)^2 $$\nThe total MSE is $\\operatorname{MSE}_{\\text{proj}} = \\frac{1}{n} \\sum_{i=1}^n ((\\alpha-s)x_i + z_i)^2$. We need to find the minimum of this expression over all valid choices of $\\alpha \\in \\mathbb{R}$ and all vectors $z$ where each component $z_i \\in [-c, c]$.\n\nSince MSE is a sum of squares, its minimum value is zero. We can check if it is possible to achieve an MSE of zero. This would require $(\\alpha-s)x_i + z_i = 0$ for all $i=1, \\dots, n$.\nWe can choose the learnable scalar projection to be $\\alpha = s$. This simplifies the error term to just $z_i$. The condition becomes $z_i = 0$ for all $i$.\nThe problem states $c \\ge 0$, which means the interval $[-c, c]$ always contains $0$. Therefore, setting the residual branch output $F_\\theta(x)$ to be the zero vector is a valid choice.\n\nBy setting $\\alpha = s$ and $F_\\theta(x) = 0$, we achieve a total error of zero. Since MSE cannot be negative, this is the global minimum.\n$$ \\operatorname{MSE}_{\\text{proj}} = 0 $$\n\n**Conclusion**\n\nThe minimal MSE for the identity skip block is calculated for each case using the formula derived above. The minimal MSE for the projection skip block is always $0$. The problem asks for the difference, which is simply $\\operatorname{MSE}_{\\text{id}} - \\operatorname{MSE}_{\\text{proj}} = \\operatorname{MSE}_{\\text{id}}$. The provided Python code in the answer calculates this value for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the difference in minimal MSE for identity-skip and projection-skip\n    residual blocks for a series of test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: x = [5, 5, 5, 5], s = 0, c = 1.\n        {'x': np.array([5, 5, 5, 5], dtype=float), 's': 0.0, 'c': 1.0},\n        # Case 2: x = [1, 1, 1, 1], s = 0, c = 1.\n        {'x': np.array([1, 1, 1, 1], dtype=float), 's': 0.0, 'c': 1.0},\n        # Case 3: x = [2, -3, 4, -1], s = 0.2, c = 1.\n        {'x': np.array([2, -3, 4, -1], dtype=float), 's': 0.2, 'c': 1.0},\n        # Case 4: x = [3, -3, 3, -3], s = 0, c = 3.\n        {'x': np.array([3, -3, 3, -3], dtype=float), 's': 0.0, 'c': 3.0},\n        # Case 5: x = [10, -1, 3, -7], s = 0.9, c = 0.05.\n        {'x': np.array([10, -1, 3, -7], dtype=float), 's': 0.9, 'c': 0.05}\n    ]\n\n    results = []\n    for case in test_cases:\n        x, s, c = case['x'], case['s'], case['c']\n        \n        # --- Minimal MSE for Identity Skip ---\n        # The goal is to make y = x + F(x) as close as possible to T(x) = sx.\n        # This means x + F(x) approx sx, so F(x) should approximate (s-1)x.\n        # The ideal residual is v = (s-1)x.\n        target_residual = (s - 1.0) * x\n        \n        # The residual branch output is bounded, so the best we can do is clip the\n        # ideal residual to the allowed range [-c, c].\n        # The residual branch output is z, where z_i must be in [-c, c].\n        # We want to minimize ((1-s)x_i + z_i)^2 = (-v_i + z_i)^2 for each component i.\n        # This is minimized when z_i is the value in [-c, c] closest to v_i.\n        optimal_residual_z = np.clip(target_residual, -c, c)\n        \n        # The minimal squared error for each component is ( (1-s)x_i + z_i^* )^2.\n        # (1-s)x = -target_residual.\n        squared_errors_id = (-target_residual + optimal_residual_z)**2\n        \n        # The minimal MSE is the mean of these component-wise squared errors.\n        min_mse_id = np.mean(squared_errors_id)\n\n        # --- Minimal MSE for Projection Skip ---\n        # The model is y = alpha*x + F(x). We can choose both alpha and F(x).\n        # The error is || alpha*x + F(x) - s*x ||^2 = || (alpha-s)x + F(x) ||^2.\n        # To minimize this, we can choose the learnable scalar alpha = s.\n        # The error term then becomes || F(x) ||^2.\n        # The residual F(x) is a vector z where each component z_i is in [-c, c].\n        # To minimize ||z||^2 subject to z_i in [-c, c], we choose z_i = 0 for all i.\n        # This is always possible since c >= 0.\n        # Thus, the minimum achievable MSE for the projection skip case is 0.\n        min_mse_proj = 0.0\n\n        # The problem asks for the difference between the two minimal MSEs.\n        difference = min_mse_id - min_mse_proj\n        results.append(difference)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3169751"}, {"introduction": "Beyond the design of a single block, the overall network architecture plays a critical role in performance. This practice elevates our perspective from local block mechanics to global architectural trade-offs, specifically comparing the use of many short-range \"local\" skips against a single long-range \"global\" skip. You will formalize the intuitive benefits of ResNets by deriving expressions for gradient path diversity and computational cost, allowing for a quantitative comparison of these distinct architectural philosophies. This analysis will equip you with a framework for reasoning about the costs and benefits of different skip connection strategies in your own designs [@problem_id:3169728].", "problem": "You are asked to formalize and compare two placements of skip connections in feedforward networks: local residual skips at each block and a single global skip from input to output. Your goal is to derive, from first principles, how these placements affect the number of distinct gradient paths and an operations-based proxy for training time, and then implement a program that computes these quantities for a set of test architectures.\n\nStart from the following fundamental base:\n- Backpropagation uses the chain rule of calculus, and when a nodeâ€™s output is the sum of branch outputs, the Jacobian of the sum equals the sum of the branch Jacobians. Let the input be $x$, the layer functions be $F_i$, and the output be $y$.\n- For a fully connected linear layer mapping $\\mathbb{R}^{d_i} \\to \\mathbb{R}^{d_{i+1}}$ with weights of shape $d_{i+1} \\times d_i$, the forward floating-point operation count (floating-point operations (FLOPs)) for the matrix-vector product scales as $2\\,d_i\\,d_{i+1}$ (one multiply and one add per entry in the output). Under standard reverse-mode automatic differentiation for linear layers, the backward pass includes computing the gradient with respect to inputs and weights, each with cost scaling as $2\\,d_i\\,d_{i+1}$, for a per-layer total of $6\\,d_i\\,d_{i+1}$ FLOPs per iteration (one forward and one backward). Bias costs are negligible here and are ignored. A scalar addition of two vectors in $\\mathbb{R}^{k}$ costs $k$ FLOPs.\n\nArchitectures to analyze:\n1. Plain chain (no skips): $L$ linear layers with dimensions $[d_0,d_1,\\dots,d_L]$; output is $y = F_{L-1}\\circ \\dots \\circ F_0(x)$.\n2. Global skip only: same chain plus a single skip added at the output $y = F_{L-1}\\circ \\dots \\circ F_0(x) + x$, requiring $d_0 = d_L$ for the sum to be well-defined.\n3. Local residual skips only: each block outputs $x_{i+1} = x_i + F_i(x_i)$, requiring $d_i = d_{i+1}$ for each addition to be well-defined.\n\nDefinitions to compute:\n- Gradient path diversity $D$: the number of algebraically distinct additive paths by which gradients from $y$ can reach $x$, counting paths made distinct by the presence or absence of traversing each residual branch.\n- Operations-based training-time proxy $C$: the total FLOPs for one training iteration (one forward plus one backward pass), computed by summing per-layer matrix multiplication FLOPs and the FLOPs of any addition operations introduced by skip connections in the forward pass. Backward FLOPs for the addition nodes are taken as negligible in this model, while backward FLOPs for linear layers are included as above.\n\nYou must implement a program that, for each test case, returns a pair $[D,C]$ as integers. No physical units are involved, and angles are not used. The architecture is specified by a string indicating the skip type and a list of dimensions indicating the layer widths.\n\nTest suite to implement (in this order):\n- Case $1$: local residual skips, $L = 3$, dimensions $[64,64,64,64]$.\n- Case $2$: global skip only, $L = 3$, dimensions $[64,64,64,64]$.\n- Case $3$: plain chain, $L = 3$, dimensions $[64,64,64,64]$.\n- Case $4$: local residual skips, $L = 1$, dimensions $[32,32]$.\n- Case $5$: global skip only, $L = 10$, dimensions $[16,16,16,16,16,16,16,16,16,16,16]$.\n- Case $6$: local residual skips, $L = 10$, dimensions $[16,16,16,16,16,16,16,16,16,16,16]$.\n\nOutput specification:\n- Your program should produce a single line of output containing the six results as a comma-separated list enclosed in square brackets, where each result is itself a two-element list $[D,C]$. For example, the format is $[[D_1,C_1],[D_2,C_2],\\dots,[D_6,C_6]]$ with no spaces anywhere in the line.", "solution": "The problem requires the formalization and analysis of three distinct feedforward network architectures: a plain sequential chain, a chain with a single global skip connection, and a network composed of local residual blocks. For each architecture, we must derive expressions for two quantities: the gradient path diversity, denoted by $D$, and an operations-based proxy for training time, denoted by $C$. These derivations must stem from the fundamental principles provided.\n\nThe number of layers is denoted by $L$, and the dimensions of the network are given by a list $[d_0, d_1, \\dots, d_L]$, where $d_0$ is the input dimension and $d_i$ for $i > 0$ is the output dimension of the $(i-1)$-th layer. The function implemented by the $i$-th layer (for $i$ from $0$ to $L-1$) is $F_i$, which maps from $\\mathbb{R}^{d_i}$ to $\\mathbb{R}^{d_{i+1}}$.\n\nThe operations cost $C$ is the total number of floating-point operations (FLOPs) for one full training iteration (one forward pass and one backward pass). The provided cost model is as follows:\n- A linear layer $F_i: \\mathbb{R}^{d_i} \\to \\mathbb{R}^{d_{i+1}}$ has a total iteration cost of $6 d_i d_{i+1}$ FLOPs.\n- The addition of two vectors in $\\mathbb{R}^k$ costs $k$ FLOPs for the forward pass, with negligible cost for the backward pass.\n\nThe gradient path diversity $D$ is the number of distinct additive paths for the gradient to flow from the output $y$ back to the input $x$. This is derived from the chain rule and the principle that the Jacobian of a sum of functions is the sum of their Jacobians.\n\nWe will now derive the expressions for $D$ and $C$ for each architecture.\n\n**1. Plain Chain Architecture**\n\nIn this configuration, the layers are composed sequentially. The output of layer $i$, denoted $x_{i+1}$, is the result of applying function $F_i$ to the previous output $x_i$.\nThe architecture is defined by:\n$$ y = x_L = F_{L-1}(x_{L-1}) = F_{L-1} \\circ F_{L-2} \\circ \\dots \\circ F_0(x_0) $$\nwhere $x_0$ is the network input.\n\nGradient Path Diversity ($D_{plain}$):\nThe Jacobian of the output $y$ with respect to the input $x_0$ is found by applying the chain rule:\n$$ \\frac{\\partial y}{\\partial x_0} = \\frac{\\partial F_{L-1}}{\\partial x_{L-1}} \\frac{\\partial F_{L-2}}{\\partial x_{L-2}} \\dots \\frac{\\partial F_0}{\\partial x_0} $$\nThis expression is a single product of Jacobians. There are no additive terms in the functional dependence of $y$ on $x_0$. Consequently, there is only one pathway for gradients to propagate back.\n$$ D_{plain} = 1 $$\n\nOperations-based Training-time Proxy ($C_{plain}$):\nThe total cost is the sum of the costs for each of the $L$ linear layers. The cost for layer $F_i$ is $6 d_i d_{i+1}$.\n$$ C_{plain} = \\sum_{i=0}^{L-1} 6 d_i d_{i+1} $$\n\n**2. Global Skip Only Architecture**\n\nThis architecture adds a single skip connection from the input $x_0$ to the final output $y$.\nThe output is defined as:\n$$ y = (F_{L-1} \\circ F_{L-2} \\circ \\dots \\circ F_0(x_0)) + x_0 $$\nFor the vector addition to be well-defined, the dimension of the input must match the dimension of the final layer's output, i.e., $d_0 = d_L$.\n\nGradient Path Diversity ($D_{global}$):\nThe output $y$ is a sum of two functions of $x_0$: the deep path through the layers, let's call it $H(x_0)$, and the identity path $I(x_0) = x_0$. Based on the provided principle, the Jacobian of the sum is the sum of the Jacobians:\n$$ \\frac{\\partial y}{\\partial x_0} = \\frac{\\partial H(x_0)}{\\partial x_0} + \\frac{\\partial I(x_0)}{\\partial x_0} = \\left( \\prod_{i=0}^{L-1} \\frac{\\partial F_{L-1-i}}{\\partial x_{L-1-i}} \\right) + I_{d_0} $$\nwhere $I_{d_0}$ is the identity matrix of size $d_0 \\times d_0$. The two additive terms correspond to two distinct algebraic paths for the gradient.\n$$ D_{global} = 2 $$\n\nOperations-based Training-time Proxy ($C_{global}$):\nThe cost includes the cost of the $L$ linear layers, identical to the plain chain, plus the cost of the final vector addition. The addition is between two vectors in $\\mathbb{R}^{d_L}$. The cost of this addition is $d_L$ FLOPs.\n$$ C_{global} = \\left( \\sum_{i=0}^{L-1} 6 d_i d_{i+1} \\right) + d_L $$\n\n**3. Local Residual Skips Only Architecture**\n\nThis architecture consists of a sequence of residual blocks. In each block $i$, the input $x_i$ is passed through a layer $F_i$ and the result is added back to the input $x_i$.\nThe recurrent definition is:\n$$ x_{i+1} = x_i + F_i(x_i) \\quad \\text{for } i = 0, \\dots, L-1 $$\nThe final output is $y = x_L$. This structure requires that the input and output dimensions of each block are identical, so $d_i = d_{i+1}$ for all $i = 0, \\dots, L-1$. This implies all dimensions $d_0, \\dots, d_L$ must be equal.\n\nGradient Path Diversity ($D_{local}$):\nLet's represent each block's transformation as an operator. Since the layers $F_i$ are linear, we can write $x_{i+1} = (I + F_i)(x_i)$, where $I$ is the identity operator. Unrolling the recurrence from $y=x_L$ back to $x_0$:\n$$ y = (I + F_{L-1}) (x_{L-1}) = (I + F_{L-1}) (I + F_{L-2}) \\dots (I + F_0) (x_0) $$\nExpanding this product of $L$ binomials results in a sum of $2^L$ distinct terms. Each term is a unique composition of functions, formed by choosing either the identity $I$ or the layer function $F_i$ from each block $(I+F_i)$. For example, with $L=2$, we get $(I+F_1)(I+F_0) = F_1 \\circ F_0 + F_1 + F_0 + I$. Each of these $2^L$ terms constitutes a distinct additive path from input to output.\n$$ D_{local} = 2^L $$\n\nOperations-based Training-time Proxy ($C_{local}$):\nThe total cost is the sum of costs for each of the $L$ residual blocks. For each block $i$, the computation involves one linear layer $F_i$ and one vector addition. The cost for the layer is $6 d_i d_{i+1}$. The addition $x_i + F_i(x_i)$ involves vectors in $\\mathbb{R}^{d_{i+1}}$, costing $d_{i+1}$ FLOPs. The total cost is the sum of these block costs over all $L$ blocks.\n$$ C_{local} = \\sum_{i=0}^{L-1} (6 d_i d_{i+1} + d_{i+1}) $$\n\nThese derived formulae are implemented to compute the values for the specified test cases.", "answer": "```python\nimport numpy as np\n\ndef calculate_costs(skip_type: str, dims: list[int]) -> list[int]:\n    \"\"\"\n    Calculates gradient path diversity (D) and operations proxy (C) for a given architecture.\n\n    Args:\n        skip_type: A string indicating the architecture ('plain', 'global', 'local').\n        dims: A list of integers representing layer dimensions [d_0, d_1, ..., d_L].\n\n    Returns:\n        A list [D, C] containing the two computed integer values.\n    \"\"\"\n    L = len(dims) - 1\n    if L == 0: # A network with 0 layers is just an identity\n        if skip_type == 'plain':\n            return [1, 0]\n        elif skip_type == 'global': # x = x+x, not well defined in problem, assume plain\n             return [1, 0]\n        elif skip_type == 'local': # x_1 = x_0 + F_0(x_0) -> L must be at least 1\n            return [0, 0] # Or error, but problem cases have L >= 1.\n\n    # Calculate base cost from linear layers, common to all architectures\n    cost_layers = 0\n    for i in range(L):\n        d_i = dims[i]\n        d_i_plus_1 = dims[i+1]\n        cost_layers += 6 * d_i * d_i_plus_1\n\n    D = 0\n    C = 0\n\n    if skip_type == 'plain':\n        # D: Single path through all layers.\n        D = 1\n        # C: Sum of costs of linear layers.\n        C = cost_layers\n    elif skip_type == 'global':\n        # D: Two paths - one through layers, one through identity skip.\n        D = 2\n        # C: Cost of layers plus one vector addition at the output.\n        d_L = dims[L]\n        C = cost_layers + d_L\n    elif skip_type == 'local':\n        # D: 2 choices at each of L blocks (identity or layer) -> 2^L paths.\n        D = 2**L\n        # C: Cost of layers plus L vector additions, one for each block.\n        cost_additions = 0\n        for i in range(L):\n            d_i_plus_1 = dims[i+1]\n            cost_additions += d_i_plus_1\n        C = cost_layers + cost_additions\n    \n    return [int(D), int(C)]\n\ndef solve():\n    \"\"\"\n    Executes the analysis for the predefined test suite and prints the results.\n    \"\"\"\n    test_cases = [\n        ('local', [64, 64, 64, 64]),\n        ('global', [64, 64, 64, 64]),\n        ('plain', [64, 64, 64, 64]),\n        ('local', [32, 32]),\n        ('global', [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]),\n        ('local', [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]),\n    ]\n\n    results = []\n    for skip_type, dims in test_cases:\n        result = calculate_costs(skip_type, dims)\n        results.append(result)\n\n    # Format the output string to be exactly as specified: [[D1,C1],[D2,C2],...]\n    # str() on a list adds spaces, so we remove them.\n    results_str = [str(r).replace(' ', '') for r in results]\n    output_str = f\"[{','.join(results_str)}]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "3169728"}]}