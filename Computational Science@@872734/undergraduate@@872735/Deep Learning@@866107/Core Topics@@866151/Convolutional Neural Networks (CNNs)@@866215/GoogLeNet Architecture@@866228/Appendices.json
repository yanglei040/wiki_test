{"hands_on_practices": [{"introduction": "The foundational idea behind the GoogLeNet architecture is the Inception module, which processes an input signal at multiple spatial scales in parallel. This practice provides a first-principles look into why this multi-scale approach is effective, particularly for tasks like denoising. By analyzing how convolutional kernels of different sizes affect a signal corrupted by noise, you will gain a concrete understanding of the trade-offs involved and the motivation for the Inception module's design [@problem_id:3130750].", "problem": "You will implement and analyze a simplified Inception-like multi-branch module to study how spatial kernel size affects denoising by inherent averaging under a white noise model, using only first-principles reasoning. Consider a single-channel image and an Inception-like module with parallel branches that each apply a single convolution with a square kernel of size $k \\times k$ and stride $1$. Each branch uses a uniform averaging kernel, normalized to unit sum, which models the spatial aggregation behavior of larger receptive fields in an Inception branch. Use the following foundational base only: the linearity of convolution, the definition of convolution as a weighted sum, the definition of Signal-to-Noise Ratio (SNR) as signal power divided by noise power, and the properties of independent and identically distributed zero-mean Gaussian noise.\n\nTask. Given a clean image $S$ and additive noise $N$ where $N$ is independent and identically distributed zero-mean Gaussian with variance $\\sigma^2$, define the input SNR over a specified spatial support $\\Omega$ as\n$$\n\\mathrm{SNR_{in}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega}\\left[S(i,j)^2\\right]}{\\mathbb{E}\\left[N(i,j)^2\\right]}.\n$$\nFor a branch with kernel $h_k$ of size $k \\times k$, define the output signal as $S_{\\mathrm{out}} = h_k * S$ and the output noise as $N_{\\mathrm{out}} = h_k * N$. The output SNR is\n$$\n\\mathrm{SNR_{out}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega_k}\\left[S_{\\mathrm{out}}(i,j)^2\\right]}{\\mathbb{E}\\left[N_{\\mathrm{out}}(i,j)^2\\right]}.\n$$\nHere $*$ denotes convolution. To avoid edge effects and ensure the same filter is applied at each output location, you must use valid convolution (no padding) so that the output support $\\Omega_k$ is the set of all positions where the $k \\times k$ kernel is fully contained in the input. For each branch, the SNR improvement factor is defined as the ratio\n$$\nG_k = \\frac{\\mathrm{SNR_{out}}}{\\mathrm{SNR_{in}}}.\n$$\nYou must compute $G_k$ for each kernel size in each test case by deriving the noise power after filtering from first principles under the independent and identically distributed Gaussian assumption. Do not use empirical averaging over many noise realizations; instead, use the properties of linear combinations of independent variables to obtain the noise power after filtering. All convolutions must be implemented with stride $1$ and valid mode.\n\nLow-light dataset model. Construct the clean image $S$ deterministically according to the test case description. Model low light by choosing small positive amplitudes $A$ and relatively larger noise standard deviations $\\sigma$. The noise distribution is used only to determine the noise power symbolically as required by the derivation; you do not sample noise.\n\nFilters. For a given kernel size $k$, the branch filter is a uniform averaging kernel with coefficients $h(u,v)=1/k^2$ for all $u,v$ in the $k \\times k$ window.\n\nRequired outputs. For each test case, report a list of SNR improvement factors $G_k$ for the specified kernel sizes $k$, in the same order as listed in the test case. Round each reported factor to $6$ decimal places.\n\nTest suite. Implement the following test cases exactly:\n- Test case $1$: image size $H=W=32$, clean signal type: constant, amplitude $A=0.1$, noise standard deviation $\\sigma=0.2$, kernel sizes $k \\in \\{1,3,5\\}$.\n- Test case $2$: image size $H=W=32$, clean signal type: checkerboard with values in $\\{0,A\\}$ alternating per pixel, amplitude $A=0.1$, noise standard deviation $\\sigma=0.2$, kernel sizes $k \\in \\{1,3,5\\}$.\n- Test case $3$: image size $H=W=32$, clean signal type: constant, amplitude $A=0.05$, noise standard deviation $\\sigma=0.3$, kernel sizes $k \\in \\{1,5,7\\}$.\n- Test case $4$: image size $H=W=7$, clean signal type: constant, amplitude $A=0.1$, noise standard deviation $\\sigma=0.2$, kernel sizes $k \\in \\{1,7\\}$.\n\nFinal output format. Your program should produce a single line of output containing the results for all test cases as a single list of lists. Each inner list must contain the SNR improvement factors $G_k$ for that test case, in the order of kernel sizes given for that test case. All numbers must be rounded to $6$ decimal places. The line must be formatted as a JSON-like list of lists without spaces, for example using symbolic placeholders: $[[g_{1,1},g_{1,2},\\dots],[g_{2,1},\\dots],\\dots]$.", "solution": "The user problem is deemed valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The task is to derive and compute the Signal-to-Noise Ratio (SNR) improvement factor $G_k$ for a simplified Inception-like module branch with a uniform averaging kernel of size $k \\times k$.\n\nThe derivation proceeds from first principles as stipulated. The SNR improvement factor $G_k$ is defined as the ratio of the output SNR to the input SNR:\n$$\nG_k = \\frac{\\mathrm{SNR_{out}}}{\\mathrm{SNR_{in}}}\n$$\nThe input and output SNRs are defined as the ratio of signal power to noise power.\n$$\n\\mathrm{SNR_{in}} = \\frac{P_{\\mathrm{S, in}}}{P_{\\mathrm{N, in}}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega}\\left[S(i,j)^2\\right]}{\\mathbb{E}\\left[N(i,j)^2\\right]}\n$$\n$$\n\\mathrm{SNR_{out}} = \\frac{P_{\\mathrm{S, out}}}{P_{\\mathrm{N, out}}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega_k}\\left[S_{\\mathrm{out}}(i,j)^2\\right]}{\\mathbb{E}\\left[N_{\\mathrm{out}}(i,j)^2\\right]}\n$$\nHere, $S$ and $N$ are the input signal and noise, while $S_{\\mathrm{out}}$ and $N_{\\mathrm{out}}$ are the signal and noise after convolution with the kernel $h_k$. The notation $\\mathbb{E}_{(i,j)\\in\\Omega}[\\cdot]$ denotes spatial averaging over a support $\\Omega$, while $\\mathbb{E}[\\cdot]$ denotes the statistical expectation over the noise distribution.\n\nSubstituting these definitions into the expression for $G_k$ gives:\n$$\nG_k = \\frac{P_{\\mathrm{S, out}} / P_{\\mathrm{N, out}}}{P_{\\mathrm{S, in}} / P_{\\mathrm{N, in}}} = \\left(\\frac{P_{\\mathrm{S, out}}}{P_{\\mathrm{S, in}}}\\right) \\left(\\frac{P_{\\mathrm{N, in}}}{P_{\\mathrm{N, out}}}\\right)\n$$\nWe analyze the noise power ratio and the signal power ratio separately.\n\nFirst, we analyze the noise power ratio. The input noise $N(i,j)$ is independent and identically distributed (i.i.d.) with zero mean and variance $\\sigma^2$. The input noise power at any pixel is therefore:\n$$\nP_{\\mathrm{N, in}} = \\mathbb{E}[N(i,j)^2] = \\mathrm{Var}(N(i,j)) + (\\mathbb{E}[N(i,j)])^2 = \\sigma^2 + 0^2 = \\sigma^2\n$$\nThe output noise $N_{\\mathrm{out}}$ is the result of convolving the input noise $N$ with the uniform averaging kernel $h_k$, where $h_k(u,v) = 1/k^2$. At an output location $(i,j)$, the output noise is a linear combination of input noise samples:\n$$\nN_{\\mathrm{out}}(i,j) = (h_k * N)(i,j) = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\frac{1}{k^2} N(i+u, j+v)\n$$\nThe expectation of the output noise is zero due to the linearity of expectation and $\\mathbb{E}[N]=0$:\n$$\n\\mathbb{E}[N_{\\mathrm{out}}(i,j)] = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\frac{1}{k^2} \\mathbb{E}[N(i+u, j+v)] = 0\n$$\nThe output noise power is its variance. Since the input noise samples $N(i,j)$ are independent, the variance of their weighted sum is the sum of the variances of each term:\n$$\nP_{\\mathrm{N, out}} = \\mathbb{E}[N_{\\mathrm{out}}(i,j)^2] = \\mathrm{Var}(N_{\\mathrm{out}}(i,j)) = \\mathrm{Var}\\left(\\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\frac{1}{k^2} N(i+u, j+v)\\right)\n$$\n$$\nP_{\\mathrm{N, out}} = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\mathrm{Var}\\left(\\frac{1}{k^2} N(i+u, j+v)\\right) = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\left(\\frac{1}{k^2}\\right)^2 \\mathrm{Var}(N(i+u, j+v))\n$$\nSince each of the $k^2$ noise samples has variance $\\sigma^2$:\n$$\nP_{\\mathrm{N, out}} = k^2 \\cdot \\frac{1}{k^4} \\sigma^2 = \\frac{\\sigma^2}{k^2}\n$$\nThe noise power ratio is therefore:\n$$\n\\frac{P_{\\mathrm{N, in}}}{P_{\\mathrm{N, out}}} = \\frac{\\sigma^2}{\\sigma^2 / k^2} = k^2\n$$\nThis demonstrates the fundamental principle of noise reduction through averaging: averaging $k^2$ i.i.d. noise samples reduces the noise power by a factor of $k^2$.\n\nNow, we consider the signal power ratio. This is computed through spatial averaging over the respective domains. The input signal power, over an image of size $H \\times W$, is:\n$$\nP_{\\mathrm{S, in}} = \\mathbb{E}_{(i,j)\\in \\Omega}\\left[S(i,j)^2\\right] = \\frac{1}{HW} \\sum_{i=0}^{H-1} \\sum_{j=0}^{W-1} S(i,j)^2\n$$\nThe output signal is $S_{\\mathrm{out}} = h_k * S$. Using a 'valid' convolution, the output image has dimensions $(H-k+1) \\times (W-k+1)$. The output signal power is:\n$$\nP_{\\mathrm{S, out}} = \\mathbb{E}_{(i,j)\\in \\Omega_k}\\left[S_{\\mathrm{out}}(i,j)^2\\right] = \\frac{1}{(H-k+1)(W-k+1)} \\sum_{i=0}^{H-k} \\sum_{j=0}^{W-k} S_{\\mathrm{out}}(i,j)^2\n$$\nCombining the noise and signal power ratios, the final expression for the SNR improvement factor $G_k$ is:\n$$\nG_k = k^2 \\frac{P_{\\mathrm{S, out}}}{P_{\\mathrm{S, in}}}\n$$\nThis formula is implemented to solve for $G_k$ for each test case.\nFor a constant signal $S(i,j)=A$, the averaging kernel produces a constant output $S_{\\mathrm{out}}(i,j) = A$. Thus, $P_{\\mathrm{S, out}} = A^2$ and $P_{\\mathrm{S, in}} = A^2$. The ratio of signal powers is $1$, so $G_k=k^2$.\nFor a high-frequency signal like a checkerboard, the averaging acts as a low-pass filter, attenuating the signal. This results in $P_{\\mathrm{S, out}} < P_{\\mathrm{S, in}}$, and consequently $G_k < k^2$. The implementation computes the convolution and subsequent spatial averages numerically to determine the signal power ratio for each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_g_factors(H, W, signal_type, A, k_list):\n    \"\"\"\n    Computes the SNR improvement factor G_k for a list of kernel sizes.\n\n    Args:\n        H (int): Image height.\n        W (int): Image width.\n        signal_type (str): Type of clean signal ('constant' or 'checkerboard').\n        A (float): Amplitude of the signal.\n        k_list (list): List of kernel sizes (k).\n\n    Returns:\n        list: A list of SNR improvement factors G_k, rounded to 6 decimal places.\n    \"\"\"\n    # 1. Construct the clean signal S\n    if signal_type == 'constant':\n        S = np.full((H, W), A, dtype=np.float64)\n    elif signal_type == 'checkerboard':\n        S = np.zeros((H, W), dtype=np.float64)\n        # Use broadcasting for efficient checkerboard generation\n        i_coords = np.arange(H).reshape(-1, 1)\n        j_coords = np.arange(W).reshape(1, -1)\n        mask = (i_coords + j_coords) % 2 == 0\n        S[mask] = A\n    else:\n        raise ValueError(\"Unknown signal type\")\n\n    # 2. Calculate input signal power P_s_in\n    # This is the mean of the squared signal values\n    P_s_in = np.mean(S**2)\n\n    g_factors = []\n    \n    # Check for zero-signal case to prevent division by zero, though not in test cases\n    if P_s_in == 0:\n        # If signal is zero, SNR is zero both in and out. The ratio is ill-defined.\n        # Based on the formula, P_s_out will also be 0, leading to 0/0.\n        # We can return a placeholder or handle as an error. For this problem, A > 0.\n        # Arbitrarily returning k^2 as if signal power ratio is 1.\n        return [float(k**2) for k in k_list]\n\n    for k in k_list:\n        # 3. Calculate output signal S_out by 'valid' convolution\n        H_out = H - k + 1\n        W_out = W - k + 1\n        S_out = np.zeros((H_out, W_out), dtype=np.float64)\n        \n        # Explicitly implement convolution with a uniform averaging kernel\n        for i in range(H_out):\n            for j in range(W_out):\n                patch = S[i:i+k, j:j+k]\n                S_out[i, j] = np.mean(patch)\n\n        # 4. Calculate output signal power P_s_out\n        P_s_out = np.mean(S_out**2) if S_out.size > 0 else 0.0\n\n        # 5. Calculate G_k using the derived formula\n        g_k = (k**2) * P_s_out / P_s_in\n        g_factors.append(g_k)\n\n    return [round(g, 6) for g in g_factors]\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {'H': 32, 'W': 32, 'signal_type': 'constant', 'A': 0.1, 'k_list': [1, 3, 5]},\n        {'H': 32, 'W': 32, 'signal_type': 'checkerboard', 'A': 0.1, 'k_list': [1, 3, 5]},\n        {'H': 32, 'W': 32, 'signal_type': 'constant', 'A': 0.05, 'k_list': [1, 5, 7]},\n        {'H': 7, 'W': 7, 'signal_type': 'constant', 'A': 0.1, 'k_list': [1, 7]}\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = calculate_g_factors(case['H'], case['W'], case['signal_type'], case['A'], case['k_list'])\n        all_results.append(results)\n\n    # Format the final output as a single-line JSON-like list of lists\n    result_str = '[' + ','.join(\n        '[' + ','.join([f'{val:.6f}' for val in res]) + ']' for res in all_results\n    ) + ']'\n    print(result_str)\n\nsolve()\n\n```", "id": "3130750"}, {"introduction": "While Inception modules are powerful, stacking many of them can become computationally expensive, especially when using large convolutional kernels. This exercise explores a key optimization introduced in later versions of the architecture: factorization. You will derive the computational savings achieved by replacing a large, square $K \\times K$ convolution with a sequence of a $1 \\times K$ and a $K \\times 1$ convolution, a technique that significantly reduces parameters and floating-point operations (FLOPs) while preserving a large receptive field [@problem_id:3130734].", "problem": "You are studying the computational efficiency techniques in the GoogLeNet (Inception) architecture, specifically the factorization of a square convolution into two separable rectangular convolutions. Using the standard definition of convolutional cost in deep learning, derive from first principles the exact Floating Point Operations (FLOPs) for a single two-dimensional convolution and for a factorized sequence of two convolutions. Then, quantify the computational savings when replacing a single $K \\times K$ convolution with a $1 \\times K$ convolution followed by a $K \\times 1$ convolution. Finally, compute top-$1$ accuracy on an abstract ImageNet-$100$ subset with provided labels and predicted classes to assess whether factorization maintains accuracy.\n\nFoundational base and definitions to be used:\n- A two-dimensional convolution with input of $C_{\\text{in}}$ channels and output of $C_{\\text{out}}$ channels, kernel size $K \\times K$, and output spatial dimensions $H_{\\text{out}} \\times W_{\\text{out}}$ performs, by convention, $2$ FLOPs per multiply-accumulate (one multiplication and one addition). Under this convention, the FLOPs of a single $K \\times K$ convolution are\n$$\nF_{\\text{orig}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2.\n$$\n- For a factorized pair of convolutions consisting of a $1 \\times K$ convolution mapping $C_{\\text{in}} \\to C_{\\text{mid}}$ followed by a $K \\times 1$ convolution mapping $C_{\\text{mid}} \\to C_{\\text{out}}$, the FLOPs are\n$$\nF_{\\text{fact}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C_{\\text{in}} \\cdot C_{\\text{mid}} \\cdot K \\;+\\; C_{\\text{mid}} \\cdot C_{\\text{out}} \\cdot K\\right).\n$$\n- The FLOP savings fraction is defined as\n$$\nS \\;=\\; \\frac{F_{\\text{orig}} - F_{\\text{fact}}}{F_{\\text{orig}}}.\n$$\n- Top-$1$ accuracy is defined as the fraction of examples where the predicted class index equals the ground-truth label, i.e.,\n$$\n\\text{acc} \\;=\\; \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}\\{ \\hat{y}_i = y_i \\},\n$$\nwith $N$ the number of samples, $y_i \\in \\{0,\\dots,C-1\\}$ the true labels, and $\\hat{y}_i$ the predicted class indices.\n\nTasks:\n1) Prove algebraically that if $C_{\\text{mid}} = C_{\\text{out}} = C_{\\text{in}}$ and $K = 7$, then the factorized pair uses exactly a fraction $2/7$ of the FLOPs of the original $7 \\times 7$ convolution, hence the FLOPs are reduced to $2/7$ of the original. Report the exact savings fraction $S$.\n2) Compute the exact FLOP savings fraction $S$ for the following three test cases, assuming stride $1$ and output spatial dimensions as given. Count FLOPs according to the above convention and express the savings as a decimal rounded to six places.\n   - Test A (happy path): $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 64$, $C_{\\text{mid}} = 64$, $K = 7$.\n   - Test B (boundary): $H_{\\text{out}} = 28$, $W_{\\text{out}} = 28$, $C_{\\text{in}} = 32$, $C_{\\text{out}} = 32$, $C_{\\text{mid}} = 32$, $K = 1$.\n   - Test C (bottleneck factorization): $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 128$, $C_{\\text{mid}} = 64$, $K = 7$.\n3) Using an abstract ImageNet-$100$ subset with $N = 12$ samples and $C = 100$ classes, with labels and predictions provided below, compute the top-$1$ accuracies for a baseline model (single $K \\times K$ convolution) and a factorized model ($1 \\times K$ followed by $K \\times 1$), and report both accuracies and their difference (factorized minus baseline). The labels and predicted class indices are:\n   - Ground-truth labels $y$: $[5,12,23,23,11,65,99,0,3,42,77,13]$.\n   - Baseline predicted classes $\\hat{y}^{\\text{base}}$: $[5,11,23,3,11,65,13,0,2,42,8,13]$.\n   - Factorized predicted classes $\\hat{y}^{\\text{fact}}$: $[5,12,23,23,9,65,99,0,3,1,77,88]$.\n\nAngle or physical units do not apply. All requested numeric outputs must be decimal numbers rounded to six decimal places.\n\nProgram requirements:\n- Implement a program that computes:\n  - The FLOP savings fraction $S$ for Test A, Test B, and Test C.\n  - The top-$1$ accuracies for the baseline and factorized predictions on the provided ImageNet-$100$ subset and their difference.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n  $[S_{\\text{A}}, S_{\\text{B}}, S_{\\text{C}}, \\text{acc}_{\\text{base}}, \\text{acc}_{\\text{fact}}, \\Delta \\text{acc}]$,\n  where $\\Delta \\text{acc} = \\text{acc}_{\\text{fact}} - \\text{acc}_{\\text{base}}$.\n- All six numbers must be rounded to six decimal places.", "solution": "The problem requires an analysis of the computational savings and performance impact of factorizing a square convolution into two separable rectangular convolutions, a technique prominently used in the GoogLeNet (Inception) architecture. I will first validate the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- FLOPs for original convolution: $F_{\\text{orig}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2$\n- FLOPs for factorized convolution: $F_{\\text{fact}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C_{\\text{in}} \\cdot C_{\\text{mid}} \\cdot K \\;+\\; C_{\\text{mid}} \\cdot C_{\\text{out}} \\cdot K\\right)$\n- FLOP savings fraction: $S \\;=\\; \\frac{F_{\\text{orig}} - F_{\\text{fact}}}{F_{\\text{orig}}}$\n- Top-1 accuracy: $\\text{acc} \\;=\\; \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}\\{ \\hat{y}_i = y_i \\}$\n- Task 1 conditions: $C_{\\text{mid}} = C_{\\text{out}} = C_{\\text{in}}$, $K = 7$\n- Test A parameters: $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 64$, $C_{\\text{mid}} = 64$, $K = 7$\n- Test B parameters: $H_{\\text{out}} = 28$, $W_{\\text{out}} = 28$, $C_{\\text{in}} = 32$, $C_{\\text{out}} = 32$, $C_{\\text{mid}} = 32$, $K = 1$\n- Test C parameters: $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 128$, $C_{\\text{mid}} = 64$, $K = 7$\n- Task 3 data: $N = 12$, $C = 100$\n    - $y$: $[5,12,23,23,11,65,99,0,3,42,77,13]$\n    - $\\hat{y}^{\\text{base}}$: $[5,11,23,3,11,65,13,0,2,42,8,13]$\n    - $\\hat{y}^{\\text{fact}}$: $[5,12,23,23,9,65,99,0,3,1,77,88]$\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the principles of deep learning and computational complexity analysis of neural network operations. All definitions and formulas are standard in the field. The problem is well-posed, providing all necessary data and constraints to arrive at unique numerical solutions for each task. The language is objective and formal. The parameters given are realistic for convolutional neural network architectures. The problem does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe solution proceeds by addressing each of the three tasks in order.\n\n**Task 1: Algebraic Proof and Savings Fraction**\n\nWe are asked to prove that for the specific case where $C_{\\text{in}} = C_{\\text{mid}} = C_{\\text{out}} = C$ and $K=7$, the factorized convolution uses a fraction $2/7$ of the FLOPs of the original, and to find the savings fraction $S$.\n\nLet $C_{\\text{in}} = C_{\\text{mid}} = C_{\\text{out}} = C$. The FLOPs expressions become:\n$$\nF_{\\text{orig}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C \\cdot C \\cdot K^2 \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C^2 \\cdot K^2\n$$\n$$\nF_{\\text{fact}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C \\cdot C \\cdot K \\;+\\; C \\cdot C \\cdot K\\right) \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(2 \\cdot C^2 \\cdot K\\right)\n$$\nTo find the ratio of factorized FLOPs to original FLOPs, we compute $\\frac{F_{\\text{fact}}}{F_{\\text{orig}}}$:\n$$\n\\frac{F_{\\text{fact}}}{F_{\\text{orig}}} \\;=\\; \\frac{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot 2 \\cdot C^2 \\cdot K}{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C^2 \\cdot K^2} \\;=\\; \\frac{2K}{K^2} \\;=\\; \\frac{2}{K}\n$$\nThe common factor $2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C^2$ cancels out, demonstrating that this ratio is independent of the spatial dimensions and channel counts, provided they are equal.\nFor a kernel size of $K=7$, this ratio is:\n$$\n\\frac{F_{\\text{fact}}}{F_{\\text{orig}}} \\;=\\; \\frac{2}{7}\n$$\nThis proves that the factorized pair uses exactly $2/7$ of the FLOPs of the original $7 \\times 7$ convolution under these conditions.\n\nThe savings fraction $S$ is defined as $S = \\frac{F_{\\text{orig}} - F_{\\text{fact}}}{F_{\\text{orig}}} = 1 - \\frac{F_{\\text{fact}}}{F_{\\text{orig}}}$.\nSubstituting the derived ratio:\n$$\nS \\;=\\; 1 - \\frac{2}{K}\n$$\nFor $K=7$, the savings fraction is:\n$$\nS \\;=\\; 1 - \\frac{2}{7} \\;=\\; \\frac{5}{7}\n$$\nThe exact savings fraction is $5/7$.\n\n**Task 2: FLOP Savings Fraction for Test Cases**\n\nTo compute the savings fraction for the given test cases, we first derive a general simplified expression for $S$.\n$$\nS \\;=\\; 1 - \\frac{F_{\\text{fact}}}{F_{\\text{orig}}} \\;=\\; 1 - \\frac{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C_{\\text{in}} \\cdot C_{\\text{mid}} \\cdot K \\;+\\; C_{\\text{mid}} \\cdot C_{\\text{out}} \\cdot K\\right)}{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2}\n$$\nThe term $2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}}$ cancels. We can also factor out $C_{\\text{mid}} \\cdot K$ from the numerator:\n$$\nS \\;=\\; 1 - \\frac{C_{\\text{mid}} \\cdot K \\cdot (C_{\\text{in}} + C_{\\text{out}})}{C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2} \\;=\\; 1 - \\frac{C_{\\text{mid}} (C_{\\text{in}} + C_{\\text{out}})}{K \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}}\n$$\nUsing this simplified formula, we compute $S$ for each test case.\n\n- **Test A:** $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 64$, $C_{\\text{mid}} = 64$, $K = 7$.\nThis case matches the conditions of Task $1$.\n$$\nS_{\\text{A}} \\;=\\; 1 - \\frac{64 \\cdot (64 + 64)}{7 \\cdot 64 \\cdot 64} \\;=\\; 1 - \\frac{64 \\cdot 128}{7 \\cdot 4096} \\;=\\; 1 - \\frac{128}{7 \\cdot 64} \\;=\\; 1 - \\frac{2}{7} \\;=\\; \\frac{5}{7} \\approx 0.714286\n$$\n\n- **Test B:** $H_{\\text{out}} = 28$, $W_{\\text{out}} = 28$, $C_{\\text{in}} = 32$, $C_{\\text{out}} = 32$, $C_{\\text{mid}} = 32$, $K = 1$.\nThis boundary case involves a $1 \\times 1$ kernel, where factorization is not typically beneficial.\n$$\nS_{\\text{B}} \\;=\\; 1 - \\frac{32 \\cdot (32 + 32)}{1 \\cdot 32 \\cdot 32} \\;=\\; 1 - \\frac{32 \\cdot 64}{1024} \\;=\\; 1 - \\frac{2048}{1024} \\;=\\; 1 - 2 \\;=\\; -1\n$$\nA savings of $-1$ indicates that the FLOPs have doubled, which is expected when replacing one $1 \\times 1$ convolution with two sequential $1 \\times 1$ convolutions.\n\n- **Test C:** $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 128$, $C_{\\text{mid}} = 64$, $K = 7$.\nThis represents a bottleneck design, where $C_{\\text{mid}}$ is smaller than $C_{\\text{out}}$.\n$$\nS_{\\text{C}} \\;=\\; 1 - \\frac{64 \\cdot (64 + 128)}{7 \\cdot 64 \\cdot 128} \\;=\\; 1 - \\frac{64 \\cdot 192}{7 \\cdot 8192} \\;=\\; 1 - \\frac{192}{7 \\cdot 128} \\;=\\; 1 - \\frac{192}{896}\n$$\nSimplifying the fraction $\\frac{192}{896}$ by dividing numerator and denominator by their greatest common divisor, $64$: $\\frac{192 \\div 64}{896 \\div 64} = \\frac{3}{14}$.\n$$\nS_{\\text{C}} \\;=\\; 1 - \\frac{3}{14} \\;=\\; \\frac{11}{14} \\approx 0.785714\n$$\n\n**Task 3: Top-1 Accuracy Calculation**\n\nWe compute the top-$1$ accuracy for both the baseline and factorized models on the provided abstract data set with $N=12$ samples. The accuracy is the count of correct predictions divided by the total number of samples.\n\n- **Ground-truth labels $y$**: $[5,12,23,23,11,65,99,0,3,42,77,13]$\n\n- **Baseline predicted classes $\\hat{y}^{\\text{base}}$**: $[5,11,23,3,11,65,13,0,2,42,8,13]$\nWe compare $y$ and $\\hat{y}^{\\text{base}}$ element-wise to find matches:\n- Index $0$: $5 = 5$ (Match)\n- Index $1$: $12 \\neq 11$\n- Index $2$: $23 = 23$ (Match)\n- Index $3$: $23 \\neq 3$\n- Index $4$: $11 = 11$ (Match)\n- Index $5$: $65 = 65$ (Match)\n- Index $6$: $99 \\neq 13$\n- Index $7$: $0 = 0$ (Match)\n- Index $8$: $3 \\neq 2$\n- Index $9$: $42 = 42$ (Match)\n- Index $10$: $77 \\neq 8$\n- Index $11$: $13 = 13$ (Match)\nThere are $7$ correct predictions.\n$$\n\\text{acc}_{\\text{base}} \\;=\\; \\frac{7}{12} \\approx 0.583333\n$$\n\n- **Factorized predicted classes $\\hat{y}^{\\text{fact}}$**: $[5,12,23,23,9,65,99,0,3,1,77,88]$\nWe compare $y$ and $\\hat{y}^{\\text{fact}}$ element-wise to find matches:\n- Index $0$: $5 = 5$ (Match)\n- Index $1$: $12 = 12$ (Match)\n- Index $2$: $23 = 23$ (Match)\n- Index $3$: $23 = 23$ (Match)\n- Index $4$: $11 \\neq 9$\n- Index $5$: $65 = 65$ (Match)\n- Index $6$: $99 = 99$ (Match)\n- Index $7$: $0 = 0$ (Match)\n- Index $8$: $3 = 3$ (Match)\n- Index $9$: $42 \\neq 1$\n- Index $10$: $77 = 77$ (Match)\n- Index $11$: $13 \\neq 88$\nThere are $9$ correct predictions.\n$$\n\\text{acc}_{\\text{fact}} \\;=\\; \\frac{9}{12} \\;=\\; \\frac{3}{4} \\;=\\; 0.75\n$$\n\n- **Difference in accuracy $\\Delta \\text{acc}$**:\n$$\n\\Delta \\text{acc} \\;=\\; \\text{acc}_{\\text{fact}} - \\text{acc}_{\\text{base}} \\;=\\; \\frac{9}{12} - \\frac{7}{12} \\;=\\; \\frac{2}{12} \\;=\\; \\frac{1}{6} \\approx 0.166667\n$$\nThe results will be compiled into the required output format in the program.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the three tasks defined in the problem:\n    1. Computes FLOP savings fractions S for three test cases.\n    2. Computes top-1 accuracies for baseline and factorized models.\n    3. Computes the difference in accuracies.\n    \"\"\"\n\n    # --- Task 2: Compute FLOP Savings Fraction S ---\n\n    def calculate_savings_fraction(C_in, C_out, C_mid, K):\n        \"\"\"\n        Computes the FLOP savings fraction S using the derived formula:\n        S = 1 - (C_mid * (C_in + C_out)) / (K * C_in * C_out)\n        \n        A kernel size K=0 would lead to division by zero, but is physically\n        meaningless and not present in the test cases.\n        \"\"\"\n        if K == 0 or C_in == 0 or C_out == 0:\n            # Handle edge cases to prevent division by zero, though not strictly\n            # needed for the given problem data. An invalid configuration would\n            # result in undefined savings.\n            return np.nan\n\n        # Simplified formula derived in the solution text\n        numerator = C_mid * (C_in + C_out)\n        denominator = K * C_in * C_out\n        \n        # When K=1, the original convolution cost formula for F_orig can be zero if K=0.\n        # However, the problem states KxK conv, so K>=1. For K=1, F_orig has K^2=1.\n        # F_fact has K=1. The formula is arithmetically sound.\n        \n        ratio = numerator / denominator\n        S = 1 - ratio\n        return S\n\n    # Test Case A\n    params_A = {'C_in': 64, 'C_out': 64, 'C_mid': 64, 'K': 7}\n    S_A = calculate_savings_fraction(**params_A)\n\n    # Test Case B\n    params_B = {'C_in': 32, 'C_out': 32, 'C_mid': 32, 'K': 1}\n    S_B = calculate_savings_fraction(**params_B)\n\n    # Test Case C\n    params_C = {'C_in': 64, 'C_out': 128, 'C_mid': 64, 'K': 7}\n    S_C = calculate_savings_fraction(**params_C)\n\n    # --- Task 3: Compute Top-1 Accuracies ---\n\n    # Provided data for the abstract ImageNet-100 subset\n    y_true = np.array([5, 12, 23, 23, 11, 65, 99, 0, 3, 42, 77, 13])\n    y_pred_base = np.array([5, 11, 23, 3, 11, 65, 13, 0, 2, 42, 8, 13])\n    y_pred_fact = np.array([5, 12, 23, 23, 9, 65, 99, 0, 3, 1, 77, 88])\n\n    # Top-1 accuracy is the mean of correct predictions\n    acc_base = np.mean(y_true == y_pred_base)\n    acc_fact = np.mean(y_true == y_pred_fact)\n\n    # Difference in accuracies\n    delta_acc = acc_fact - acc_base\n\n    # --- Final Output Formatting ---\n\n    results = [S_A, S_B, S_C, acc_base, acc_fact, delta_acc]\n    \n    # Format each result to six decimal places and join into the required string format\n    formatted_results = [f\"{val:.6f}\" for val in results]\n    output_string = f\"[{','.join(formatted_results)}]\"\n\n    print(output_string)\n\n# This block ensures the solve function is called when the script is executed.\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3130734"}, {"introduction": "To train even deeper networks, the Inception module was later integrated with residual connections, leading to the powerful Inception-ResNet family of models. This practice delves into a critical implementation detail: how the residual \"skip\" connection is combined with the main Inception block. By comparing pre-activation and post-activation residual wrappers, you will investigate their profound impact on gradient flow, which is essential for successfully training very deep neural networks [@problem_id:3130783].", "problem": "You will implement and analyze a simplified version of the GoogLeNet (Inception) architecture with a residual wrapper, comparing residual pre-activation to post-activation in terms of gradient norms and test error. The analysis must be grounded in first principles of feed-forward networks and the chain rule of calculus. The final deliverable is a complete, runnable program that computes the specified quantities for a small test suite and outputs them in the required format.\n\nThe fundamental base is the chain rule for multivariate functions: if a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ is composed of differentiable maps, then the Jacobian $J_f$ satisfies the product rule along the composition. For a loss function $L(y, t)$ with $y \\in \\mathbb{R}^n$ and target $t \\in \\mathbb{R}^n$, the gradient with respect to input $x \\in \\mathbb{R}^n$ is $ \\nabla_x L = J_y(x)^\\top \\nabla_y L$. For Mean Squared Error (MSE) defined as $ \\mathrm{MSE}(y, t) = \\frac{1}{n} \\| y - t \\|_2^2 $, the gradient satisfies $ \\nabla_y L = y - t $ when $ L(y, t) = \\frac{1}{2} \\| y - t \\|_2^2 $ and the MSE is computed as $ \\frac{1}{n} \\| y - t \\|_2^2 $.\n\nDefine a simplified Inception block operator $F: \\mathbb{R}^d \\to \\mathbb{R}^d$ with $d = 8$ as follows. Let $\\phi$ denote the Rectified Linear Unit activation, $\\phi(z) = \\max\\{0, z\\}$ applied elementwise, with derivative $\\phi'(z) = \\begin{cases} 1 & \\text{if } z > 0, \\\\ 0 & \\text{if } z \\le 0. \\end{cases}$ at each coordinate. The Inception block has four branches parameterized by matrices:\n- Branch $1$: $W_1 \\in \\mathbb{R}^{4 \\times 8}$, output $b_1(x) = \\phi(W_1 x)$, Jacobian $J_{b_1}(x) = D_1 W_1$, where $D_1 = \\mathrm{diag}(\\phi'(W_1 x))$.\n- Branch $2$: $W_{2a} \\in \\mathbb{R}^{6 \\times 8}$ followed by $W_{2b} \\in \\mathbb{R}^{4 \\times 6}$, output $a_2(x) = \\phi(W_{2a} x)$, then $b_2(x) = \\phi(W_{2b} a_2(x))$, Jacobian $J_{b_2}(x) = D_{2b} W_{2b} D_{2a} W_{2a}$ with $D_{2a} = \\mathrm{diag}(\\phi'(W_{2a} x))$ and $D_{2b} = \\mathrm{diag}(\\phi'(W_{2b} a_2(x)))$.\n- Branch $3$: $W_{3a} \\in \\mathbb{R}^{10 \\times 8}$ followed by $W_{3b} \\in \\mathbb{R}^{4 \\times 10}$, output $a_3(x) = \\phi(W_{3a} x)$, then $b_3(x) = \\phi(W_{3b} a_3(x))$, Jacobian $J_{b_3}(x) = D_{3b} W_{3b} D_{3a} W_{3a}$ with $D_{3a} = \\mathrm{diag}(\\phi'(W_{3a} x))$ and $D_{3b} = \\mathrm{diag}(\\phi'(W_{3b} a_3(x)))$.\n- Branch $4$: Average pooling $P \\in \\mathbb{R}^{5 \\times 8}$ followed by $W_4 \\in \\mathbb{R}^{4 \\times 5}$, output $p(x) = P x$, $b_4(x) = \\phi(W_4 p(x))$, Jacobian $J_{b_4}(x) = D_4 W_4 P$ with $D_4 = \\mathrm{diag}(\\phi'(W_4 P x))$.\n\nConcatenate the branch outputs $v(x) = [b_1(x); b_2(x); b_3(x); b_4(x)] \\in \\mathbb{R}^{16}$, and map back to $\\mathbb{R}^8$ using $W_o \\in \\mathbb{R}^{8 \\times 16}$: $F(x) = W_o v(x)$. The Jacobian of $F$ is $J_F(x) = W_o \\cdot J_v(x)$, where $J_v(x)$ is the vertical stack of the branch Jacobians $J_{b_i}(x)$.\n\nThe fixed average pooling matrix $P \\in \\mathbb{R}^{5 \\times 8}$ is defined deterministically by\n- Row $1$: averages coordinates $1$ and $2$ with weights $0.5$ on each,\n- Row $2$: averages coordinates $3$ and $4$,\n- Row $3$: averages coordinates $5$ and $6$,\n- Row $4$: averages coordinates $7$ and $8$,\n- Row $5$: averages all $8$ coordinates equally with weight $0.125$ on each entry.\n\nConsider two residual wrappers around the Inception block:\n- Pre-activation residual: $y_{\\mathrm{pre}}(x) = x + F(\\phi(x))$. Its Jacobian is $J_{\\mathrm{pre}}(x) = I_d + J_F(\\phi(x)) \\cdot D_x$, where $I_d$ is the $d \\times d$ identity and $D_x = \\mathrm{diag}(\\phi'(x))$.\n- Post-activation residual: $y_{\\mathrm{post}}(x) = \\phi(x + F(x))$. Its Jacobian is $J_{\\mathrm{post}}(x) = D_s \\cdot (I_d + J_F(x))$, where $s = x + F(x)$ and $D_s = \\mathrm{diag}(\\phi'(s))$.\n\nGiven a target vector $t \\in \\mathbb{R}^8$, define the loss $L(y, t) = \\frac{1}{2} \\| y - t \\|_2^2$ and the Mean Squared Error $\\mathrm{MSE}(y, t) = \\frac{1}{8} \\| y - t \\|_2^2$. The gradient with respect to $x$ is $\\nabla_x L = J_y(x)^\\top (y - t)$, where $y$ is either $y_{\\mathrm{pre}}(x)$ or $y_{\\mathrm{post}}(x)$ accordingly. The gradient norm is $\\| \\nabla_x L \\|_2$.\n\nWeight initialization: For a given random seed $s$ and weight scale $\\alpha > 0$, all matrices $W_1, W_{2a}, W_{2b}, W_{3a}, W_{3b}, W_4, W_o$ are sampled i.i.d. from a normal distribution $\\mathcal{N}(0, \\alpha^2)$ using the specified seed $s$.\n\nYour task is to implement a program that:\n- Constructs the matrices with the specified shapes and initialization.\n- Implements the forward computation of $F(x)$ and its Jacobian $J_F(x)$ via explicit Jacobian calculus.\n- Computes $y_{\\mathrm{pre}}(x)$, $y_{\\mathrm{post}}(x)$, their Jacobians $J_{\\mathrm{pre}}(x)$ and $J_{\\mathrm{post}}(x)$, the gradients $\\nabla_x L$ for both cases, their Euclidean norms, and the Mean Squared Error values for both cases.\n\nUse the following test suite, each case specified by $(s, \\alpha, x, t)$:\n- Case $1$: $s = 42$, $\\alpha = 0.5$, $x = [-1, 1, -1, 1, -1, 1, -1, 1]^\\top$, $t = [0, 0, 0, 0, 0, 0, 0, 0]^\\top$.\n- Case $2$: $s = 7$, $\\alpha = 0.01$, $x = [-0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1]^\\top$, $t = [0, 0, 0, 0, 0, 0, 0, 0]^\\top$.\n- Case $3$: $s = 0$, $\\alpha = 2.0$, $x = [0, 0, 0, 0, 0, 0, 0, 0]^\\top$, $t = [0.3, -0.2, 0.1, -0.4, 0.5, -0.6, 0.7, -0.8]^\\top$.\n- Case $4$: $s = 99$, $\\alpha = 1.2$, $x = [1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5]^\\top$, $t = [1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5]^\\top$.\n\nAngle units are not applicable. There are no physical units. All outputs must be decimals.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of four-tuples enclosed in square brackets. Each four-tuple corresponds to one test case and contains the values $(\\| \\nabla_x L_{\\mathrm{pre}} \\|_2, \\| \\nabla_x L_{\\mathrm{post}} \\|_2, \\mathrm{MSE}_{\\mathrm{pre}}, \\mathrm{MSE}_{\\mathrm{post}})$ in that order, with each value formatted as a decimal. For example: \"[(a,b,c,d),(e,f,g,h),(...)]\" where a,b,c,d,e,f,g,h are floating-point numbers.", "solution": "We start from the chain rule for multivariate functions and linear algebra representations of feed-forward neural networks. Let $x \\in \\mathbb{R}^d$ be the input and $t \\in \\mathbb{R}^d$ be the target. For a loss $L(y, t) = \\frac{1}{2} \\| y - t \\|_2^2$, we have $\\nabla_y L = y - t$. The gradient with respect to $x$ is $\\nabla_x L = J_y(x)^\\top (y - t)$, so the computation reduces to determining the Jacobians $J_y(x)$ for the pre-activation and post-activation residual wrappers.\n\nWe define the Rectified Linear Unit (ReLU) nonlinearity as $\\phi(z) = \\max\\{0, z\\}$ elementwise, with derivative $\\phi'(z) = 1$ when $z > 0$ and $\\phi'(z) = 0$ when $z \\le 0$. At $z = 0$, we choose the subgradient $\\phi'(0) = 0$, which is a common convention in practice and simplifies analysis.\n\nThe simplified Inception block $F$ uses parallel linear-nonlinear branches and concatenates their outputs, followed by a linear projection back to $\\mathbb{R}^d$. Specifically:\n- Branch $1$: $b_1(x) = \\phi(W_1 x)$. The Jacobian is $J_{b_1}(x) = D_1 W_1$, where $D_1 = \\mathrm{diag}(\\phi'(W_1 x))$.\n- Branch $2$: $a_2(x) = \\phi(W_{2a} x)$ and $b_2(x) = \\phi(W_{2b} a_2(x))$. The Jacobian is $J_{b_2}(x) = D_{2b} W_{2b} D_{2a} W_{2a}$, with $D_{2a} = \\mathrm{diag}(\\phi'(W_{2a} x))$ and $D_{2b} = \\mathrm{diag}(\\phi'(W_{2b} a_2(x)))$.\n- Branch $3$: $a_3(x) = \\phi(W_{3a} x)$ and $b_3(x) = \\phi(W_{3b} a_3(x))$. The Jacobian is $J_{b_3}(x) = D_{3b} W_{3b} D_{3a} W_{3a}$, with $D_{3a} = \\mathrm{diag}(\\phi'(W_{3a} x))$ and $D_{3b} = \\mathrm{diag}(\\phi'(W_{3b} a_3(x)))$.\n- Branch $4$: $p(x) = P x$ and $b_4(x) = \\phi(W_4 p(x))$. The Jacobian is $J_{b_4}(x) = D_4 W_4 P$, with $D_4 = \\mathrm{diag}(\\phi'(W_4 P x))$.\n\nConcatenate $v(x) = [b_1(x); b_2(x); b_3(x); b_4(x)] \\in \\mathbb{R}^{16}$, then $F(x) = W_o v(x)$. The Jacobian is obtained by the chain rule: $J_F(x) = W_o \\cdot J_v(x)$, where $J_v(x)$ is the vertical stack of $J_{b_i}(x)$, i.e., $J_v(x) = \\begin{bmatrix} J_{b_1}(x) \\\\ J_{b_2}(x) \\\\ J_{b_3}(x) \\\\ J_{b_4}(x) \\end{bmatrix}$.\n\nWe analyze the two residual wrappers:\n- Pre-activation residual: $y_{\\mathrm{pre}}(x) = x + F(\\phi(x))$. Let $x_\\phi = \\phi(x)$ and $D_x = \\mathrm{diag}(\\phi'(x))$. The Jacobian of $F(\\phi(x))$ with respect to $x$ is $J_F(x_\\phi) \\cdot D_x$ by the chain rule, since $x \\mapsto \\phi(x)$ has Jacobian $D_x$, and then $F$ at $x_\\phi$ has Jacobian $J_F(x_\\phi)$. Therefore,\n  $$ J_{\\mathrm{pre}}(x) = I_d + J_F(\\phi(x)) \\cdot D_x. $$\n  The gradient is\n  $$ \\nabla_x L_{\\mathrm{pre}} = J_{\\mathrm{pre}}(x)^\\top \\cdot (y_{\\mathrm{pre}}(x) - t), \\quad \\| \\nabla_x L_{\\mathrm{pre}} \\|_2 = \\left\\| J_{\\mathrm{pre}}(x)^\\top (y_{\\mathrm{pre}}(x) - t) \\right\\|_2. $$\n- Post-activation residual: $y_{\\mathrm{post}}(x) = \\phi(s)$ where $s = x + F(x)$. The Jacobian of $s$ is $J_s(x) = I_d + J_F(x)$. The Jacobian of $\\phi(s)$ is $D_s = \\mathrm{diag}(\\phi'(s))$. By the chain rule,\n  $$ J_{\\mathrm{post}}(x) = D_s \\cdot (I_d + J_F(x)). $$\n  The gradient is\n  $$ \\nabla_x L_{\\mathrm{post}} = J_{\\mathrm{post}}(x)^\\top \\cdot (y_{\\mathrm{post}}(x) - t), \\quad \\| \\nabla_x L_{\\mathrm{post}} \\|_2 = \\left\\| J_{\\mathrm{post}}(x)^\\top (y_{\\mathrm{post}}(x) - t) \\right\\|_2. $$\n\nFor the Mean Squared Error, we compute\n$$ \\mathrm{MSE}_{\\mathrm{pre}} = \\frac{1}{8} \\| y_{\\mathrm{pre}}(x) - t \\|_2^2, \\qquad \\mathrm{MSE}_{\\mathrm{post}} = \\frac{1}{8} \\| y_{\\mathrm{post}}(x) - t \\|_2^2. $$\n\nAlgorithmic steps:\n1. Construct $P \\in \\mathbb{R}^{5 \\times 8}$ deterministically as specified (four rows averaging adjacent coordinate pairs and a fifth row averaging all eight coordinates with weight $0.125$).\n2. For each test case $(s, \\alpha, x, t)$, initialize the random number generator with seed $s$ and sample each entry of $W_1, W_{2a}, W_{2b}, W_{3a}, W_{3b}, W_4, W_o$ i.i.d. from $\\mathcal{N}(0, \\alpha^2)$.\n3. Implement functions to compute $\\phi$ and $\\phi'$ elementwise, with $\\phi'(0) = 0$.\n4. Implement the forward pass of $F(x)$ and compute its Jacobian $J_F(x)$ by assembling the branch Jacobians $J_{b_i}(x)$ and stacking them to form $J_v(x)$, then multiply by $W_o$.\n5. Compute $y_{\\mathrm{pre}}(x)$, $J_{\\mathrm{pre}}(x)$, and the gradient $\\nabla_x L_{\\mathrm{pre}} = J_{\\mathrm{pre}}(x)^\\top (y_{\\mathrm{pre}}(x) - t)$; then compute its Euclidean norm.\n6. Compute $y_{\\mathrm{post}}(x)$, $J_{\\mathrm{post}}(x)$, and the gradient $\\nabla_x L_{\\mathrm{post}} = J_{\\mathrm{post}}(x)^\\top (y_{\\mathrm{post}}(x) - t)$; then compute its Euclidean norm.\n7. Compute $\\mathrm{MSE}_{\\mathrm{pre}}$ and $\\mathrm{MSE}_{\\mathrm{post}}$.\n8. Aggregate results for each test case as a four-tuple $(\\| \\nabla_x L_{\\mathrm{pre}} \\|_2, \\| \\nabla_x L_{\\mathrm{post}} \\|_2, \\mathrm{MSE}_{\\mathrm{pre}}, \\mathrm{MSE}_{\\mathrm{post}})$, and print them as a single comma-separated list enclosed in square brackets, as specified.\n\nInterpretation:\n- When $x$ is negative and $\\alpha$ is small (Case $2$), $s = x + F(x)$ tends to be negative componentwise, so $D_s$ becomes the zero matrix, suppressing the gradient in the post-activation residual. However, $J_{\\mathrm{pre}}(x)$ includes the identity $I_d$, allowing direct gradient flow through the skip path even when $D_x = 0$, preserving $\\nabla_x L_{\\mathrm{pre}} = (y_{\\mathrm{pre}} - t)$.\n- When $x = 0$ (Case $3$), $D_x = 0$, and $F(0)$ tends to be $0$ due to the ReLU nonlinearities in the branches, so $y_{\\mathrm{post}} = 0$ with $D_s = 0$, completely blocking the gradient in the post-activation path. In contrast, the pre-activation path still retains the identity skip connection, producing a nonzero gradient equal to $-t$ scaled by the identity.\n- For moderate or large $\\alpha$ and positive $x$ (Cases $1$ and $4$), the behavior depends on the specific activations; the pre-activation wrapper systematically preserves a stable gradient path via $I_d$, while the post-activation wrapper may gate it depending on the sign of $s$.\n\nThis approach demonstrates from first principles why residual pre-activation can yield stronger gradient flow than post-activation, especially under activation saturation, and how this impacts both gradient norms and the MSE proxy on the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef relu(z: np.ndarray) -> np.ndarray:\n    return np.maximum(z, 0.0)\n\ndef drelu(z: np.ndarray) -> np.ndarray:\n    # Derivative of ReLU, choose 0 at z == 0\n    return (z > 0).astype(float)\n\ndef build_pooling_matrix(d: int = 8) -> np.ndarray:\n    # P in R^{5 x 8}\n    P = np.zeros((5, d))\n    # Row 1: average coords 1 and 2\n    P[0, 0] = 0.5\n    P[0, 1] = 0.5\n    # Row 2: average coords 3 and 4\n    P[1, 2] = 0.5\n    P[1, 3] = 0.5\n    # Row 3: average coords 5 and 6\n    P[2, 4] = 0.5\n    P[2, 5] = 0.5\n    # Row 4: average coords 7 and 8\n    P[3, 6] = 0.5\n    P[3, 7] = 0.5\n    # Row 5: average all 8 coordinates\n    P[4, :] = 0.125\n    return P\n\ndef init_params(seed: int, scale: float):\n    rng = np.random.default_rng(seed)\n    # Shapes as specified\n    W1 = rng.normal(loc=0.0, scale=scale, size=(4, 8))\n    W2a = rng.normal(loc=0.0, scale=scale, size=(6, 8))\n    W2b = rng.normal(loc=0.0, scale=scale, size=(4, 6))\n    W3a = rng.normal(loc=0.0, scale=scale, size=(10, 8))\n    W3b = rng.normal(loc=0.0, scale=scale, size=(4, 10))\n    P = build_pooling_matrix(8)\n    W4 = rng.normal(loc=0.0, scale=scale, size=(4, 5))\n    Wo = rng.normal(loc=0.0, scale=scale, size=(8, 16))\n    params = {\n        \"W1\": W1, \"W2a\": W2a, \"W2b\": W2b,\n        \"W3a\": W3a, \"W3b\": W3b,\n        \"P\": P, \"W4\": W4,\n        \"Wo\": Wo\n    }\n    return params\n\ndef inception_F_and_J(x: np.ndarray, params: dict):\n    W1 = params[\"W1\"]; W2a = params[\"W2a\"]; W2b = params[\"W2b\"]\n    W3a = params[\"W3a\"]; W3b = params[\"W3b\"]\n    P = params[\"P\"]; W4 = params[\"W4\"]\n    Wo = params[\"Wo\"]\n\n    # Branch 1\n    z1 = W1 @ x\n    b1 = relu(z1)\n    D1 = np.diag(drelu(z1))\n    Jb1 = D1 @ W1  # 4x8\n\n    # Branch 2\n    z2a = W2a @ x\n    a2 = relu(z2a)\n    z2b = W2b @ a2\n    b2 = relu(z2b)\n    D2a = np.diag(drelu(z2a))\n    D2b = np.diag(drelu(z2b))\n    Jb2 = D2b @ W2b @ D2a @ W2a  # 4x8\n\n    # Branch 3\n    z3a = W3a @ x\n    a3 = relu(z3a)\n    z3b = W3b @ a3\n    b3 = relu(z3b)\n    D3a = np.diag(drelu(z3a))\n    D3b = np.diag(drelu(z3b))\n    Jb3 = D3b @ W3b @ D3a @ W3a  # 4x8\n\n    # Branch 4 (pooling -> linear -> ReLU)\n    px = P @ x\n    z4 = W4 @ px\n    b4 = relu(z4)\n    D4 = np.diag(drelu(z4))\n    Jb4 = D4 @ W4 @ P  # 4x8\n\n    # Concatenate outputs\n    v = np.concatenate([b1, b2, b3, b4], axis=0)  # 16\n    # Stack Jacobians vertically: shape (16, 8)\n    Jv = np.vstack([Jb1, Jb2, Jb3, Jb4])  # 16x8\n\n    # Final linear projection\n    F_x = Wo @ v  # 8\n    JF = Wo @ Jv  # 8x8\n\n    return F_x, JF\n\ndef eval_case(seed: int, scale: float, x: np.ndarray, t: np.ndarray):\n    d = x.shape[0]\n    I = np.eye(d)\n    params = init_params(seed, scale)\n\n    # Pre-activation residual: y_pre = x + F(relu(x))\n    x_phi = relu(x)\n    F_xt, JF_xt = inception_F_and_J(x_phi, params)\n    Dx = np.diag(drelu(x))\n    J_pre = I + JF_xt @ Dx\n    y_pre = x + F_xt\n    grad_pre = J_pre.T @ (y_pre - t)\n    grad_norm_pre = float(np.linalg.norm(grad_pre))\n    mse_pre = float(np.sum((y_pre - t) ** 2) / d)\n\n    # Post-activation residual: y_post = relu(x + F(x))\n    F_x, JF_x = inception_F_and_J(x, params)\n    s = x + F_x\n    Ds = np.diag(drelu(s))\n    J_post = Ds @ (I + JF_x)\n    y_post = relu(s)\n    grad_post = J_post.T @ (y_post - t)\n    grad_norm_post = float(np.linalg.norm(grad_post))\n    mse_post = float(np.sum((y_post - t) ** 2) / d)\n\n    return grad_norm_pre, grad_norm_post, mse_pre, mse_post\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (42, 0.5, np.array([-1, 1, -1, 1, -1, 1, -1, 1], dtype=float),\n         np.zeros(8, dtype=float)),\n        # Case 2\n        (7, 0.01, np.array([-0.1]*8, dtype=float),\n         np.zeros(8, dtype=float)),\n        # Case 3\n        (0, 2.0, np.zeros(8, dtype=float),\n         np.array([0.3, -0.2, 0.1, -0.4, 0.5, -0.6, 0.7, -0.8], dtype=float)),\n        # Case 4\n        (99, 1.2, np.array([1.5]*8, dtype=float),\n         np.array([1.5]*8, dtype=float)),\n    ]\n\n    results = []\n    for seed, scale, x, t in test_cases:\n        gpre, gpost, mpre, mpost = eval_case(seed, scale, x, t)\n        # Format with 6 decimal places for readability\n        results.append((round(gpre, 6), round(gpost, 6), round(mpre, 6), round(mpost, 6)))\n\n    # Final print statement in the exact required format.\n    # Print list of tuples: [(gpre,gpost,mpre,mpost),(...)]\n    formatted = \",\".join(f\"({a},{b},{c},{d})\" for (a, b, c, d) in results)\n    print(f\"[{formatted}]\")\n\nsolve()\n```", "id": "3130783"}]}