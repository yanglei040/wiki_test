{"hands_on_practices": [{"introduction": "One of the most compelling reasons for the adoption of Global Average Pooling (GAP) in modern convolutional neural networks is its dramatic reduction in model parameters compared to traditional fully-connected layers. This practice guides you through a foundational calculation to quantify this efficiency gain, a key architectural innovation that made deeper and more efficient networks possible. By deriving the parameter counts for both approaches, you will gain a concrete understanding of how GAP helps prevent overfitting and reduces computational and memory demands [@problem_id:3129826].", "problem": "A convolutional neural network produces a final feature tensor of shape $C \\times H \\times W$ before classification. You consider two alternatives for the final classification stage to produce $K$ outputs: \n(i) a Fully Connected (FC) layer directly from the flattened $C \\cdot H \\cdot W$ inputs to $K$ outputs, and \n(ii) Global Average Pooling (GAP) over the spatial dimensions to obtain a $C$-dimensional vector, followed by a linear $C \\to K$ layer.\n\nStarting only from the definition that a fully connected linear layer with input dimension $n$ and output dimension $m$ contains $n \\cdot m$ weights and $m$ biases, and that Global Average Pooling replaces each channel’s $H \\times W$ map by its mean value, derive expressions for the total parameter counts in the two alternatives. Then, for the specific configuration $C=256$, $H=14$, $W=14$, and $K=1000$, compute the multiplicative reduction factor in parameter memory bandwidth when switching from the FC to the GAP-plus-linear alternative, assuming $4$ bytes per parameter and that all parameters must be read once per forward pass.\n\nReport as your final answer the dimensionless reduction factor (FC bytes divided by GAP-plus-linear bytes), rounded to four significant figures. Briefly justify the direction of the memory bandwidth savings in your derivation.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Input feature tensor shape: $C \\times H \\times W$\n- Number of final outputs (classes): $K$\n- Alternative (i): A Fully Connected (FC) layer mapping flattened $C \\cdot H \\cdot W$ inputs to $K$ outputs.\n- Alternative (ii): Global Average Pooling (GAP) over spatial dimensions, followed by a linear layer from $C$ inputs to $K$ outputs.\n- Definition of FC layer parameters: For an input of dimension $n$ and output of dimension $m$, it has $n \\cdot m$ weights and $m$ biases.\n- Definition of GAP: Replaces each channel’s $H \\times W$ feature map with its mean value.\n- Specific configuration: $C=256$, $H=14$, $W=14$, $K=1000$.\n- Memory assumption: $4$ bytes per parameter. All parameters are read once per forward pass.\n- Task: Derive expressions for the total parameter counts, and compute the multiplicative reduction factor in parameter memory bandwidth when switching from alternative (i) to (ii).\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is valid.\n- **Scientifically Grounded:** The problem describes a standard and fundamental architectural trade-off in convolutional neural networks (CNNs), contrasting a traditional fully connected head with a more modern Global Average Pooling head. The definitions provided for the number of parameters in a linear layer and the function of GAP are correct and central to the field of deep learning.\n- **Well-Posed:** The problem is self-contained, providing all necessary variables ($C, H, W, K$), definitions, and numerical values to derive a unique analytical expression and compute a final numerical answer. The objective is clearly stated.\n- **Objective:** The language is formal and precise, with no subjective or ambiguous terminology.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be provided.\n\n**Derivation of Parameter Counts**\n\nLet $P$ denote the total number of parameters in a given layer or set of layers. According to the provided definition, a linear layer with an input of dimension $n$ and an output of dimension $m$ has a total parameter count of $n \\cdot m + m$.\n\n**Alternative (i): Fully Connected (FC) Layer**\nIn this alternative, the input feature tensor of shape $C \\times H \\times W$ is first flattened into a vector. The dimension of this input vector, $n$, is:\n$$n = C \\cdot H \\cdot W$$\nThis vector is then fed into a single fully connected layer that produces $K$ outputs. The output dimension, $m$, is:\n$$m = K$$\nThe total number of parameters for the FC layer, $P_{FC}$, is the sum of weights and biases:\n$$P_{FC} = n \\cdot m + m = (C \\cdot H \\cdot W) \\cdot K + K$$\nFactoring out $K$, we get:\n$$P_{FC} = K(C \\cdot H \\cdot W + 1)$$\n\n**Alternative (ii): Global Average Pooling (GAP) plus Linear Layer**\nThis alternative involves two stages.\n1.  **Global Average Pooling (GAP):** The GAP operation takes the input tensor of shape $C \\times H \\times W$ and computes the spatial average for each of the $C$ feature maps. This produces a vector of length $C$. The GAP operation itself is a fixed function (averaging) and has no trainable parameters.\n    $$P_{pool} = 0$$\n2.  **Linear Layer:** The resulting $C$-dimensional vector is the input to a final linear layer. For this layer, the input dimension $n$ is:\n    $$n = C$$\n    The output dimension $m$ remains the same:\n    $$m = K$$\n    The total number of parameters for this linear layer, $P_{lin}$, is:\n    $$P_{lin} = n \\cdot m + m = C \\cdot K + K$$\nThe total number of parameters for the entire GAP-plus-linear alternative, $P_{GAP}$, is the sum of parameters from both stages:\n$$P_{GAP} = P_{pool} + P_{lin} = 0 + (C \\cdot K + K) = K(C + 1)$$\n\n**Derivation of the Reduction Factor**\nThe problem defines parameter memory bandwidth as being proportional to the total number of parameters, with a constant factor of $4$ bytes per parameter. Let $B_{FC}$ be the memory bandwidth for the FC alternative and $B_{GAP}$ be the bandwidth for the GAP alternative.\n$$B_{FC} = 4 \\cdot P_{FC} = 4K(C \\cdot H \\cdot W + 1)$$\n$$B_{GAP} = 4 \\cdot P_{GAP} = 4K(C + 1)$$\nThe multiplicative reduction factor, $R$, is the ratio of the two bandwidths:\n$$R = \\frac{B_{FC}}{B_{GAP}} = \\frac{4K(C \\cdot H \\cdot W + 1)}{4K(C + 1)}$$\nThe constant factors $4$ and the variable $K$ cancel out, yielding the general expression for the reduction factor:\n$$R = \\frac{C \\cdot H \\cdot W + 1}{C + 1}$$\n\nThis expression provides the justification for the memory savings. The dominant term in the parameter count for the FC layer is $C \\cdot H \\cdot W \\cdot K$, which scales with the product of the spatial dimensions $H \\cdot W$. In contrast, the dominant term for the GAP-based alternative is $C \\cdot K$. By performing pooling *before* the linear transformation, the dependence on the spatial dimensions $H \\cdot W$ is eliminated from the parameter count, leading to a substantial reduction whenever $H > 1$ or $W > 1$.\n\n**Numerical Calculation**\nWe now substitute the given numerical values into the derived expression for $R$:\n$C=256$, $H=14$, $W=14$.\n$$R = \\frac{256 \\cdot 14 \\cdot 14 + 1}{256 + 1}$$\nFirst, compute the product in the numerator:\n$$256 \\cdot 14 \\cdot 14 = 256 \\cdot 196 = 50176$$\nNow substitute this back into the expression for $R$:\n$$R = \\frac{50176 + 1}{257} = \\frac{50177}{257}$$\nPerforming the division:\n$$R \\approx 195.241245...$$\nRounding the result to four significant figures, as required by the problem statement, gives:\n$$R \\approx 195.2$$", "answer": "$$\n\\boxed{195.2}\n$$", "id": "3129826"}, {"introduction": "While Global Average Pooling is highly efficient, its effectiveness depends on the nature of the features it is designed to aggregate. This exercise explores the concept of \"representation bias\" by contrasting GAP with Global Max Pooling (GMP) through a carefully designed hypothetical scenario. You will analyze a toy problem where features are sparse and localized, a situation that challenges GAP's underlying assumption of spatially distributed evidence, thereby illustrating the conditions under which an alternative like GMP might be superior [@problem_id:3129750].", "problem": "Consider a single-channel convolutional feature map produced by a Convolutional Neural Network (CNN) with spatial size $H \\times W$. Let $S \\equiv H \\cdot W$. For an image, denote the activation at spatial location $i \\in \\{1,\\dots,S\\}$ by the random variable $X_i$. Suppose class-conditional distributions follow this toy model: under the negative class ($Y=0$), all $X_i$ are independent and identically distributed as a Gaussian with mean $\\mu_0$ and variance $\\sigma^2$, and under the positive class ($Y=1$), exactly one unknown location $j$ contains a discriminative part with $X_j \\sim \\mathcal{N}(\\mu_1,\\sigma^2)$ while all other locations are background with $X_i \\sim \\mathcal{N}(\\mu_0,\\sigma^2)$ for $i \\neq j$. We assume independence across locations and equal class priors. Consider two pooling operators followed by a single-threshold classifier: Global Average Pooling (GAP), which outputs $Z_{\\text{avg}} \\equiv \\frac{1}{S}\\sum_{i=1}^S X_i$, and Global Max Pooling (GMP), which outputs $Z_{\\max} \\equiv \\max_{1 \\le i \\le S} X_i$. The classifier predicts $\\hat{Y}=1$ if $Z \\ge t$ and $\\hat{Y}=0$ otherwise. Define Balanced Accuracy (BA) as $\\text{BA} \\equiv \\frac{1}{2}\\left(\\Pr(\\hat{Y}=0 \\mid Y=0) + \\Pr(\\hat{Y}=1 \\mid Y=1)\\right)$.\n\nThis toy captures a representation bias contrast: GAP encourages global, spatially distributed descriptors by averaging evidence across locations, whereas GMP emphasizes part-based reasoning by attending to the strongest local evidence. Use only basic definitions of Gaussian random variables, independence, and the definition of pooling to reason about performance.\n\nFix parameters $H=W=10$ so $S=100$, $\\mu_0=0$, $\\mu_1=5$, $\\sigma=1$. For GMP, use threshold $t=4$. For GAP, assume the threshold $t$ is chosen to be Bayes optimal under the equal-variance Gaussian assumption for the two class-conditional pooled outputs. Using scientifically reasonable approximations to any Gaussian tail probabilities that arise, answer the following. Select all statements that are correct.\n\nA. Under GAP, the class-conditional distributions of $Z_{\\text{avg}}$ are Gaussian with equal variance; the Bayes-optimal threshold yields approximately $\\text{BA} \\approx 0.60$. Under GMP with $t=4$, one obtains approximately $\\text{BA} \\approx 0.92$. Therefore, on this part-based toy problem, max pooling is superior to average pooling and better aligned with the data-generating mechanism.\n\nB. Because GAP is translation-invariant, as $S$ increases with fixed $\\mu_1>\\mu_0$ and fixed $\\sigma$, its accuracy strictly increases and approaches $1$ as $S \\to \\infty$.\n\nC. With the given parameters, GMP at threshold $t=4$ has false positive rate about $3\\%$ but true positive rate about $16\\%$, giving $\\text{BA} \\approx 0.57$, so GMP is inferior to GAP on this toy problem.\n\nD. With suitable thresholds, GAP and GMP necessarily achieve the same Bayes error on this toy distribution because both are permutation-invariant functions of $\\{X_i\\}_{i=1}^S$.", "solution": "The problem statement is critically validated before proceeding to a solution.\n\n### Step 1: Extract Givens\n- Feature map spatial size: $H \\times W$.\n- Total number of spatial locations: $S \\equiv H \\cdot W$.\n- Activation at location $i$: Random variable $X_i$, for $i \\in \\{1,\\dots,S\\}$.\n- Class label: $Y \\in \\{0, 1\\}$.\n- Class $Y=0$ (negative class): $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$ are independent and identically distributed (i.i.d.) for all $i \\in \\{1, \\dots, S\\}$.\n- Class $Y=1$ (positive class): For exactly one unknown location $j$, $X_j \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$. For all other locations $i \\neq j$, $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$. All $X_i$ are independent.\n- Class priors: $\\Pr(Y=0) = \\Pr(Y=1) = 1/2$.\n- Pooling operators:\n  - Global Average Pooling (GAP): $Z_{\\text{avg}} \\equiv \\frac{1}{S}\\sum_{i=1}^S X_i$.\n  - Global Max Pooling (GMP): $Z_{\\text{max}} \\equiv \\max_{1 \\le i \\le S} X_i$.\n- Classifier: Predicts $\\hat{Y}=1$ if the pooled output $Z \\ge t$ and $\\hat{Y}=0$ otherwise.\n- Performance metric: Balanced Accuracy (BA) $\\equiv \\frac{1}{2}\\left(\\Pr(\\hat{Y}=0 \\mid Y=0) + \\Pr(\\hat{Y}=1 \\mid Y=1)\\right)$.\n- Parameters: $H=10$, $W=10$, so $S=100$. $\\mu_0=0$, $\\mu_1=5$, $\\sigma=1$.\n- Thresholds: For GMP, $t=4$. For GAP, $t$ is Bayes optimal for the class-conditional distributions of $Z_{\\text{avg}}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is a well-defined statistical toy model. It uses standard, non-controversial concepts from probability theory (Gaussian distributions, independence) to model a plausible scenario in representation learning (local vs. distributed features). It is scientifically sound.\n2.  **Well-Posed:** All necessary parameters ($\\mu_0, \\mu_1, \\sigma, S$) and distributions are defined. The objectives (calculate BA for GAP and GMP) are clear. A unique solution can be derived.\n3.  **Objective:** The problem is stated in precise, quantitative, and objective language.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. A solution will be derived.\n\n### Derivation of Solution\n\nFirst, we analyze the Global Average Pooling (GAP) operator.\nThe class-conditional distributions for $Z_{\\text{avg}}$ are derived as follows:\n\nIf $Y=0$, all $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$ are i.i.d. Since the sum of independent Gaussian random variables is also Gaussian, $Z_{\\text{avg}} = \\frac{1}{S}\\sum_i X_i$ is Gaussian.\nThe mean is $E[Z_{\\text{avg}} \\mid Y=0] = \\frac{1}{S} \\sum_i E[X_i] = \\frac{1}{S} S \\mu_0 = \\mu_0$.\nThe variance is $\\text{Var}(Z_{\\text{avg}} \\mid Y=0) = \\frac{1}{S^2} \\sum_i \\text{Var}(X_i) = \\frac{1}{S^2} S \\sigma^2 = \\frac{\\sigma^2}{S}$.\nSo, $Z_{\\text{avg}} \\mid (Y=0) \\sim \\mathcal{N}(\\mu_0, \\sigma^2/S)$.\n\nIf $Y=1$, one $X_j \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ and $S-1$ variables are $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$. $Z_{\\text{avg}}$ is again a sum of independent Gaussians and thus is Gaussian.\nThe mean is $E[Z_{\\text{avg}} \\mid Y=1] = \\frac{1}{S} \\left( E[X_j] + \\sum_{i \\neq j} E[X_i] \\right) = \\frac{1}{S}(\\mu_1 + (S-1)\\mu_0)$.\nThe variance is $\\text{Var}(Z_{\\text{avg}} \\mid Y=1) = \\frac{1}{S^2} \\left( \\text{Var}(X_j) + \\sum_{i \\neq j} \\text{Var}(X_i) \\right) = \\frac{1}{S^2}(\\sigma^2 + (S-1)\\sigma^2) = \\frac{\\sigma^2}{S}$.\nSo, $Z_{\\text{avg}} \\mid (Y=1) \\sim \\mathcal{N}(\\frac{\\mu_1 + (S-1)\\mu_0}{S}, \\sigma^2/S)$.\n\nThe class-conditional distributions for $Z_{\\text{avg}}$ are both Gaussian with equal variance $\\sigma^2/S$.\nGiven equal class priors and equal variances, the Bayes-optimal threshold is the midpoint of the two means:\n$t_{\\text{GAP}} = \\frac{1}{2} \\left( \\mu_0 + \\frac{\\mu_1 + (S-1)\\mu_0}{S} \\right)$.\nWith parameters $S=100$, $\\mu_0=0$, $\\mu_1=5$, $\\sigma=1$:\n- $Z_{\\text{avg}} \\mid (Y=0) \\sim \\mathcal{N}(0, 1/100) = \\mathcal{N}(0, (0.1)^2)$.\n- $Z_{\\text{avg}} \\mid (Y=1) \\sim \\mathcal{N}(\\frac{5 + 99 \\cdot 0}{100}, 1/100) = \\mathcal{N}(0.05, (0.1)^2)$.\n- $t_{\\text{GAP}} = \\frac{1}{2}(0 + 0.05) = 0.025$.\n\nThe True Positive Rate (TPR) is $\\Pr(\\hat{Y}=1 \\mid Y=1) = \\Pr(Z_{\\text{avg}} \\ge t_{\\text{GAP}} \\mid Y=1)$. Let $\\Phi$ be the standard normal CDF.\n$\\text{TPR} = \\Pr\\left(\\frac{Z_{\\text{avg}} - 0.05}{0.1} \\ge \\frac{0.025-0.05}{0.1}\\right) = \\Pr(\\mathcal{N}(0,1) \\ge -0.25) = 1 - \\Phi(-0.25) = \\Phi(0.25)$.\nThe True Negative Rate (TNR) is $\\Pr(\\hat{Y}=0 \\mid Y=0) = \\Pr(Z_{\\text{avg}} < t_{\\text{GAP}} \\mid Y=0)$.\n$\\text{TNR} = \\Pr\\left(\\frac{Z_{\\text{avg}} - 0}{0.1} < \\frac{0.025-0}{0.1}\\right) = \\Pr(\\mathcal{N}(0,1) < 0.25) = \\Phi(0.25)$.\nUsing $\\Phi(0.25) \\approx 0.5987$:\n$\\text{BA}_{\\text{GAP}} = \\frac{1}{2}(\\text{TNR} + \\text{TPR}) = \\frac{1}{2}(\\Phi(0.25) + \\Phi(0.25)) = \\Phi(0.25) \\approx 0.5987 \\approx 0.60$.\n\nNext, we analyze the Global Max Pooling (GMP) operator with threshold $t=4$.\nThe CDF of $Z_{\\max}$ is needed. Let $\\Phi_{\\mu, \\sigma}(x)$ be the CDF of $\\mathcal{N}(\\mu, \\sigma^2)$.\n\nIf $Y=0$, all $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$ are i.i.d.\n$\\Pr(Z_{\\max} < t \\mid Y=0) = \\Pr(\\text{all } X_i < t) = \\prod_{i=1}^S \\Pr(X_i < t) = [\\Phi_{\\mu_0, \\sigma}(t)]^S$.\nIf $Y=1$, one $X_j \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ and others $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$.\n$\\Pr(Z_{\\max} < t \\mid Y=1) = \\Pr(X_j < t) \\prod_{i \\neq j} \\Pr(X_i < t) = \\Phi_{\\mu_1, \\sigma}(t) [\\Phi_{\\mu_0, \\sigma}(t)]^{S-1}$.\n\nWith parameters $S=100, \\mu_0=0, \\mu_1=5, \\sigma=1, t=4$:\n$\\Phi_{\\mu_0, \\sigma}(4) = \\Phi(\\frac{4-0}{1}) = \\Phi(4)$.\n$\\Phi_{\\mu_1, \\sigma}(4) = \\Phi(\\frac{4-5}{1}) = \\Phi(-1)$.\nWe use the approximations $\\Phi(4) \\approx 1 - 3.167 \\times 10^{-5}$ and $\\Phi(-1) = 1 - \\Phi(1) \\approx 1 - 0.8413 = 0.1587$.\n\n$\\text{TNR} = \\Pr(\\hat{Y}=0 \\mid Y=0) = \\Pr(Z_{\\max} < 4 \\mid Y=0) = [\\Phi(4)]^{100}$.\nUsing the approximation $(1-\\epsilon)^n \\approx 1 - n\\epsilon$ for small $\\epsilon$:\n$\\text{TNR} \\approx (1 - 3.167 \\times 10^{-5})^{100} \\approx 1 - 100 \\cdot (3.167 \\times 10^{-5}) = 1 - 0.003167 = 0.996833$.\n\n$\\text{TPR} = \\Pr(\\hat{Y}=1 \\mid Y=1) = \\Pr(Z_{\\max} \\ge 4 \\mid Y=1) = 1 - \\Pr(Z_{\\max} < 4 \\mid Y=1)$.\n$\\Pr(Z_{\\max} < 4 \\mid Y=1) = \\Phi(-1) [\\Phi(4)]^{99} \\approx 0.1587 \\cdot (1 - 99 \\cdot (3.167 \\times 10^{-5}))$.\n$\\approx 0.1587 \\cdot (1 - 0.003135) \\approx 0.1587 \\cdot 0.996865 \\approx 0.15819$.\n$\\text{TPR} \\approx 1 - 0.15819 = 0.84181$.\n\n$\\text{BA}_{\\text{GMP}} = \\frac{1}{2}(\\text{TNR} + \\text{TPR}) \\approx \\frac{1}{2}(0.996833 + 0.84181) = \\frac{1.838643}{2} \\approx 0.9193 \\approx 0.92$.\n\n### Evaluation of Options\n\n**A. Under GAP, the class-conditional distributions of $Z_{\\text{avg}}$ are Gaussian with equal variance; the Bayes-optimal threshold yields approximately $\\text{BA} \\approx 0.60$. Under GMP with $t=4$, one obtains approximately $\\text{BA} \\approx 0.92$. Therefore, on this part-based toy problem, max pooling is superior to average pooling and better aligned with the data-generating mechanism.**\n- The claim that $Z_{\\text{avg}}$ distributions are Gaussian with equal variance is correct, as derived above.\n- The calculation $\\text{BA}_{\\text{GAP}} \\approx 0.60$ is correct.\n- The calculation $\\text{BA}_{\\text{GMP}} \\approx 0.92$ is correct.\n- The conclusion that GMP is superior to GAP for this problem follows directly from the fact that $0.92 \\gg 0.60$. This superiority aligns with the intuition that GMP is suited for detecting sparse, localized signals, which describes the data-generating process for $Y=1$.\n- Verdict: **Correct**.\n\n**B. Because GAP is translation-invariant, as $S$ increases with fixed $\\mu_1>\\mu_0$ and fixed $\\sigma$, its accuracy strictly increases and approaches $1$ as $S \\to \\infty$.**\n- The reasoning is a non-sequitur. The scaling of accuracy derives from the statistical properties of $Z_{\\text{avg}}$, not merely from translation invariance.\n- Let's analyze the behavior of $\\text{BA}_{\\text{GAP}}$ as $S \\to \\infty$. The separability of the two conditional distributions of $Z_{\\text{avg}}$ depends on the distance between their means, normalized by their standard deviation. This is often called $d'$.\n$d' = \\frac{|E[Z_{\\text{avg}}|Y=1] - E[Z_{\\text{avg}}|Y=0]|}{\\sqrt{\\text{Var}(Z_{\\text{avg}})}} = \\frac{|\\frac{\\mu_1+(S-1)\\mu_0}{S} - \\mu_0|}{\\sigma/\\sqrt{S}} = \\frac{|\\frac{\\mu_1-\\mu_0}{S}|}{\\sigma/\\sqrt{S}} = \\frac{\\mu_1-\\mu_0}{\\sigma\\sqrt{S}}$.\nAs $S \\to \\infty$, $d' \\to 0$. The two distributions become indistinguishable.\nThe BA for the optimal threshold is $\\text{BA}_{\\text{GAP}} = \\Phi(\\frac{d'}{2}) = \\Phi\\left(\\frac{\\mu_1-\\mu_0}{2\\sigma\\sqrt{S}}\\right)$.\nAs $S \\to \\infty$, the argument of $\\Phi$ goes to $0$, and thus $\\text{BA}_{\\text{GAP}} \\to \\Phi(0) = 0.5$.\nThis is chance performance. The statement claims accuracy approaches $1$.\n- Verdict: **Incorrect**.\n\n**C. With the given parameters, GMP at threshold $t=4$ has false positive rate about $3\\%$ but true positive rate about $16\\%$, giving $\\text{BA} \\approx 0.57$, so GMP is inferior to GAP on this toy problem.**\n- False Positive Rate (FPR) is $1 - \\text{TNR} = 1 - \\Pr(Z_{\\max} < 4 \\mid Y=0) = 1 - [\\Phi(4)]^{100}$.\n$\\text{FPR} \\approx 1 - (1 - 100(1-\\Phi(4))) = 100(1-\\Phi(4)) \\approx 100 \\cdot (3.167 \\times 10^{-5}) = 0.003167$, which is $0.32\\%$. This is not \"about $3\\%$\". The claim is off by an order of magnitude.\n- True Positive Rate (TPR) was calculated as $\\approx 0.84$ ($84\\%$). The claim is \"$16\\%$\". $16\\%$ is approximately $\\Pr(X_j<4)$, which is not the TPR. The claim is incorrect.\n- The BA calculation in the statement is based on these flawed rates. The conclusion that GMP is inferior to GAP is based on this flawed BA and contradicts our findings.\n- Verdict: **Incorrect**.\n\n**D. With suitable thresholds, GAP and GMP necessarily achieve the same Bayes error on this toy distribution because both are permutation-invariant functions of $\\{X_i\\}_{i=1}^S$.**\n- Both GAP and GMP are indeed permutation-invariant functions of the activations $\\{X_i\\}$.\n- However, being a permutation-invariant function is not sufficient to be a Baves-optimal decision function, nor does it guarantee that any two such functions yield the same performance.\n- The Bayes-optimal classifier for this problem would use the likelihood ratio, which is a function of $\\sum_i \\exp(c X_i)$ for some constant $c$, not a simple sum or maximum of the $X_i$.\n- GAP and GMP are different summary statistics that discard information in different ways. There is no a priori reason they should have the same performance.\n- Our explicit calculations in the analysis of option A show very different performances: $\\text{BA}_{\\text{GAP}} \\approx 0.60$ and $\\text{BA}_{\\text{GMP}} \\approx 0.92$. This empirically demonstrates that they do not achieve the same error.\n- Verdict: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "3129750"}, {"introduction": "Moving from theoretical concepts to a practical application, this exercise demonstrates how the basic GAP operator can be adapted to solve more complex, real-world problems. You will implement and evaluate a prior-weighted version of GAP in the context of medical image analysis, where a weak segmentation network provides a spatial prior for locating small lesions. This hands-on coding problem highlights how incorporating domain-specific knowledge into the pooling mechanism can significantly improve model sensitivity and performance on challenging tasks [@problem_id:3129763].", "problem": "Consider a binary lesion-present classification head in a convolutional neural network for medical imaging, where the final spatial activation map is a single-channel array representing location-wise logits. Let the activation map be denoted by $A \\in \\mathbb{R}^{H \\times W}$ and let a prior weight map from a weak segmentation network be denoted by $M \\in \\mathbb{R}^{H \\times W}$, with entries in $[0,1]$. The classification head aggregates $A$ to a single scalar logit by pooling over spatial positions. You will compare two pooling strategies: the uniform pooling based on the empirical expectation and a prior-weighted pooling using $M$.\n\nFundamental base:\n- The empirical expectation over a finite uniform sample space is the arithmetic mean. For a finite set of points, the empirical expectation of a quantity is computed by averaging over those points.\n- A weighted expectation uses nonnegative weights normalized to form a discrete probability mass function; in this case, a prior weight map acts as the weighting function.\n- The log-odds to probability map is given by the logistic function that transforms a scalar logit to a probability in $[0,1]$.\n\nYou must:\n1. Implement two pooling estimators to map $A$ to a pooled logit:\n   - Uniform empirical pooling (Global Average Pooling (GAP)): interpret pooling as an empirical expectation under the uniform distribution over spatial positions.\n   - Prior-weighted pooling: interpret pooling as an empirical expectation under a discrete distribution whose unnormalized weights are given by $M$; if the sum of weights is zero, fall back to uniform empirical pooling.\n2. Map each pooled logit $z$ to a predicted probability $p \\in [0,1]$ using the standard logistic function derived from log-odds.\n3. Evaluate calibration using Expected Calibration Error (ECE). Partition the interval $[0,1]$ into $B$ equally sized bins. For each bin, compute the empirical accuracy (fraction of samples with $y=1$ in the bin) and the average predicted probability in the bin. The ECE is obtained by aggregating the absolute difference between bin accuracy and bin confidence, weighted by the bin’s fraction of total samples. Empty bins contribute zero to the ECE.\n4. Evaluate sensitivity as True Positive Rate (TPR): the fraction of truly positive cases whose predicted probability exceeds a fixed threshold $\\tau$. Use $\\tau = 0.5$.\n5. Compute these metrics separately for GAP and prior-weighted pooling over the provided test suite.\n\nTest suite:\nUse $H=W=4$ in all cases. For each case, you are given $(A, M, y)$, where $y \\in \\{0,1\\}$ is the ground-truth label indicating lesion presence.\n\n- Case $1$ (happy path: small $2 \\times 2$ lesion region emphasized by the prior):\n  $$\n  A^{(1)} = \\begin{pmatrix}\n  -2.0 & -1.5 & -1.2 & -2.2 \\\\\n  -1.9 & 2.5 & 2.0 & -1.7 \\\\\n  -2.1 & 2.2 & 1.8 & -1.8 \\\\\n  -2.3 & -1.6 & -1.4 & -2.0\n  \\end{pmatrix}, \\quad\n  M^{(1)} = \\begin{pmatrix}\n  0.05 & 0.05 & 0.05 & 0.05 \\\\\n  0.05 & 0.9 & 0.9 & 0.05 \\\\\n  0.05 & 0.9 & 0.9 & 0.05 \\\\\n  0.05 & 0.05 & 0.05 & 0.05\n  \\end{pmatrix}, \\quad\n  y^{(1)} = 1.\n  $$\n\n- Case $2$ (edge case: single-pixel lesion with strong prior, background strongly negative):\n  $$\n  A^{(2)} = \\begin{pmatrix}\n  -2.0 & -1.8 & -2.2 & -2.1 \\\\\n  -1.9 & -1.7 & -1.6 & -1.8 \\\\\n  -2.3 & -2.2 & 1.5 & -2.0 \\\\\n  -2.4 & -2.1 & -2.2 & -2.3\n  \\end{pmatrix}, \\quad\n  M^{(2)} = \\begin{pmatrix}\n  0.01 & 0.01 & 0.01 & 0.01 \\\\\n  0.01 & 0.01 & 0.01 & 0.01 \\\\\n  0.01 & 0.01 & 1.0 & 0.01 \\\\\n  0.01 & 0.01 & 0.01 & 0.01\n  \\end{pmatrix}, \\quad\n  y^{(2)} = 1.\n  $$\n\n- Case $3$ (negative sample: mildly negative logits, non-informative prior):\n  $$\n  A^{(3)} = \\begin{pmatrix}\n  -0.6 & -0.5 & -0.4 & -0.5 \\\\\n  -0.7 & -0.6 & -0.5 & -0.6 \\\\\n  -0.5 & -0.4 & -0.5 & -0.6 \\\\\n  -0.6 & -0.5 & -0.6 & -0.5\n  \\end{pmatrix}, \\quad\n  M^{(3)} = \\begin{pmatrix}\n  0.2 & 0.5 & 0.1 & 0.3 \\\\\n  0.4 & 0.2 & 0.6 & 0.1 \\\\\n  0.3 & 0.4 & 0.2 & 0.5 \\\\\n  0.1 & 0.3 & 0.4 & 0.2\n  \\end{pmatrix}, \\quad\n  y^{(3)} = 0.\n  $$\n\n- Case $4$ (boundary: prior degenerated to zero; must fall back to uniform pooling):\n  $$\n  A^{(4)} = \\begin{pmatrix}\n  -1.0 & -1.1 & -0.9 & -1.2 \\\\\n  -1.0 & -1.3 & -1.1 & -1.0 \\\\\n  -1.2 & -1.0 & -1.1 & -1.0 \\\\\n  -1.1 & -1.0 & -1.2 & -1.1\n  \\end{pmatrix}, \\quad\n  M^{(4)} = \\begin{pmatrix}\n  0.0 & 0.0 & 0.0 & 0.0 \\\\\n  0.0 & 0.0 & 0.0 & 0.0 \\\\\n  0.0 & 0.0 & 0.0 & 0.0 \\\\\n  0.0 & 0.0 & 0.0 & 0.0\n  \\end{pmatrix}, \\quad\n  y^{(4)} = 0.\n  $$\n\nEvaluation parameters:\n- Use $B = 5$ bins for Expected Calibration Error.\n- Use threshold $\\tau = 0.5$ for True Positive Rate.\n\nRequired output:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order\n$$\n[\\mathrm{ECE}_{\\mathrm{GAP}}, \\mathrm{ECE}_{\\mathrm{weighted}}, \\mathrm{TPR}_{\\mathrm{GAP}}, \\mathrm{TPR}_{\\mathrm{weighted}}].\n$$\nAll four entries must be real numbers (floats). No other text should be printed.", "solution": "The problem is valid as it is scientifically grounded in the principles of deep learning and statistical evaluation, is mathematically well-posed, and provides a complete and consistent set of definitions and data for a solvable task.\n\nThe task requires the comparison of two spatial pooling strategies for a binary classification head in a convolutional neural network. These strategies aggregate a spatial activation map $A \\in \\mathbb{R}^{H \\times W}$ into a single scalar logit $z$, which is then converted to a probability. The two strategies are Global Average Pooling (GAP) and a prior-weighted pooling method.\n\nFirst, we formalize the pooling estimators. The activation map $A$ contains logits for each spatial position $(i,j)$.\nThe uniform empirical pooling, or GAP, treats each spatial position as an equally likely sample from a uniform distribution. The resulting logit, $z_{\\mathrm{GAP}}$, is the arithmetic mean of all activations:\n$$\nz_{\\mathrm{GAP}} = \\frac{1}{H \\times W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} A_{ij}\n$$\nThis method is agnostic to any prior spatial information.\n\nThe prior-weighted pooling incorporates a prior weight map $M \\in \\mathbb{R}^{H \\times W}$, where $M_{ij} \\in [0,1]$. These weights define an unnormalized discrete probability distribution over the spatial positions. The pooled logit, $z_{\\mathrm{weighted}}$, is the weighted average of the activations, where the weights are given by $M$:\n$$\nz_{\\mathrm{weighted}} = \\frac{\\sum_{i=1}^{H} \\sum_{j=1}^{W} M_{ij} A_{ij}}{\\sum_{i=1}^{H} \\sum_{j=1}^{W} M_{ij}}\n$$\nThis corresponds to computing the empirical expectation of $A$ under the distribution defined by the normalized weights from $M$. As specified, if the sum of weights $\\sum_{i,j} M_{ij}$ is $0$, this method defaults to uniform pooling, i.e., $z_{\\mathrm{weighted}} = z_{\\mathrm{GAP}}$, which prevents division by zero and provides a sensible fallback.\n\nOnce a pooled logit $z$ is obtained, it is mapped to a probability $p \\in [0,1]$ using the standard logistic function, $\\sigma(z)$, defined as:\n$$\np = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n$$\nThis function is the inverse of the log-odds (logit) transformation.\n\nWith the predicted probabilities for the entire test suite, we evaluate the two pooling methods using two metrics: sensitivity and Expected Calibration Error (ECE).\n\nSensitivity, also known as the True Positive Rate (TPR), measures the ability of the model to correctly identify positive cases. Given a decision threshold $\\tau$, it is the ratio of true positives (TP) to the total number of actual positives (P):\n$$\n\\mathrm{TPR} = \\frac{\\mathrm{TP}}{\\mathrm{P}} = \\frac{|\\{k | p^{(k)} > \\tau \\text{ and } y^{(k)} = 1\\}|}{|\\{k | y^{(k)} = 1\\}|}\n$$\nFor this problem, the threshold is $\\tau=0.5$.\n\nExpected Calibration Error (ECE) measures how well the predicted probabilities reflect the true likelihood of the outcome. The probability space $[0,1]$ is divided into $B$ equally-sized bins. For each bin $b_m$, we compute its accuracy $\\mathrm{acc}(b_m)$ (the fraction of positive samples in that bin) and its average confidence $\\mathrm{conf}(b_m)$ (the average predicted probability in that bin). The ECE is the weighted average of the absolute difference between accuracy and confidence across all bins:\n$$\n\\mathrm{ECE} = \\sum_{m=1}^{B} \\frac{|S_m|}{N} |\\mathrm{acc}(b_m) - \\mathrm{conf}(b_m)|\n$$\nwhere $S_m$ is the set of samples whose predicted probability falls into bin $b_m$, $N$ is the total number of samples, and $B=5$ is the number of bins. Empty bins do not contribute to the sum.\n\nApplying these steps to the provided test suite of $N=4$ samples with $H=W=4$:\nFor each case $(A^{(k)}, M^{(k)}, y^{(k)})$, we compute $z_{\\mathrm{GAP}}^{(k)}$ and $z_{\\mathrm{weighted}}^{(k)}$, followed by their corresponding probabilities $p_{\\mathrm{GAP}}^{(k)}$ and $p_{\\mathrm{weighted}}^{(k)}$.\n\nFor the GAP method, the computed probabilities for cases $1$ through $4$ are approximately $\\{0.360, 0.129, 0.369, 0.253\\}$. The ground truths are $\\{1, 1, 0, 0\\}$. The total number of positive samples is $2$. Since all predicted probabilities are below the threshold $\\tau=0.5$, the number of true positives is $0$, yielding $\\mathrm{TPR}_{\\mathrm{GAP}} = 0/2 = 0.0$.\nFor the ECE calculation with $B=5$ bins, one sample falls in the bin $(0.0, 0.2]$ and three samples fall in $(0.2, 0.4]$, leading to $\\mathrm{ECE}_{\\mathrm{GAP}} \\approx 0.22229$.\n\nFor the prior-weighted method, the computed probabilities are approximately $\\{0.827, 0.736, 0.369, 0.253\\}$. Note that for case $4$, $M^{(4)}$ is a zero matrix, so the calculation falls back to GAP. The two positive samples (cases $1$ and $2$) have probabilities $0.827$ and $0.736$, both exceeding $\\tau=0.5$. Thus, there are $2$ true positives, yielding $\\mathrm{TPR}_{\\mathrm{weighted}} = 2/2 = 1.0$.\nFor the ECE calculation, two samples fall in $(0.2, 0.4]$, one in $(0.6, 0.8]$, and one in $(0.8, 1.0]$, resulting in $\\mathrm{ECE}_{\\mathrm{weighted}} \\approx 0.26482$.\n\nThe final results will be computed precisely in the following program. The weighted-pooling strategy significantly improves sensitivity by focusing on diagnostically relevant regions indicated by the prior map, while the calibration as measured by ECE on this small dataset is slightly degraded compared to the uniform pooling approach.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Implements and evaluates two pooling strategies (GAP and prior-weighted)\n    for a binary classification task on a small test suite.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([\n                [-2.0, -1.5, -1.2, -2.2],\n                [-1.9,  2.5,  2.0, -1.7],\n                [-2.1,  2.2,  1.8, -1.8],\n                [-2.3, -1.6, -1.4, -2.0]\n            ]),\n            np.array([\n                [0.05, 0.05, 0.05, 0.05],\n                [0.05,  0.9,  0.9, 0.05],\n                [0.05,  0.9,  0.9, 0.05],\n                [0.05, 0.05, 0.05, 0.05]\n            ]),\n            1\n        ),\n        (\n            np.array([\n                [-2.0, -1.8, -2.2, -2.1],\n                [-1.9, -1.7, -1.6, -1.8],\n                [-2.3, -2.2,  1.5, -2.0],\n                [-2.4, -2.1, -2.2, -2.3]\n            ]),\n            np.array([\n                [0.01, 0.01, 0.01, 0.01],\n                [0.01, 0.01, 0.01, 0.01],\n                [0.01, 0.01,  1.0, 0.01],\n                [0.01, 0.01, 0.01, 0.01]\n            ]),\n            1\n        ),\n        (\n            np.array([\n                [-0.6, -0.5, -0.4, -0.5],\n                [-0.7, -0.6, -0.5, -0.6],\n                [-0.5, -0.4, -0.5, -0.6],\n                [-0.6, -0.5, -0.6, -0.5]\n            ]),\n            np.array([\n                [0.2, 0.5, 0.1, 0.3],\n                [0.4, 0.2, 0.6, 0.1],\n                [0.3, 0.4, 0.2, 0.5],\n                [0.1, 0.3, 0.4, 0.2]\n            ]),\n            0\n        ),\n        (\n            np.array([\n                [-1.0, -1.1, -0.9, -1.2],\n                [-1.0, -1.3, -1.1, -1.0],\n                [-1.2, -1.0, -1.1, -1.0],\n                [-1.1, -1.0, -1.2, -1.1]\n            ]),\n            np.zeros((4, 4)),\n            0\n        )\n    ]\n\n    # Evaluation parameters\n    B = 5\n    TAU = 0.5\n\n    # Store ground truths and predictions for both models\n    y_true = np.array([case[2] for case in test_cases])\n    p_gap = []\n    p_weighted = []\n\n    for A, M, y in test_cases:\n        # 1. Implement pooling estimators\n        # Uniform empirical pooling (GAP)\n        z_gap = np.mean(A)\n        \n        # Prior-weighted pooling\n        sum_weights = np.sum(M)\n        if sum_weights == 0:\n            z_weighted = z_gap\n        else:\n            z_weighted = np.sum(M * A) / sum_weights\n        \n        # 2. Map logits to probabilities using the logistic function\n        p_gap.append(expit(z_gap))\n        p_weighted.append(expit(z_weighted))\n\n    p_gap = np.array(p_gap)\n    p_weighted = np.array(p_weighted)\n    \n    # 3. Evaluate calibration using Expected Calibration Error (ECE)\n    def calculate_ece(predictions, labels, n_bins):\n        total_samples = len(predictions)\n        bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n        \n        ece = 0.0\n        for i in range(n_bins):\n            # Define bin boundaries\n            bin_lower = bin_edges[i]\n            bin_upper = bin_edges[i+1]\n            \n            # Find samples in the current bin\n            in_bin = (predictions > bin_lower) & (predictions <= bin_upper)\n            if i == 0: # Include 0 in the first bin\n                in_bin |= (predictions == bin_lower)\n\n            n_in_bin = np.sum(in_bin)\n            \n            if n_in_bin > 0:\n                bin_accuracy = np.mean(labels[in_bin])\n                bin_confidence = np.mean(predictions[in_bin])\n                ece += (n_in_bin / total_samples) * np.abs(bin_accuracy - bin_confidence)\n                \n        return ece\n\n    ece_gap = calculate_ece(p_gap, y_true, B)\n    ece_weighted = calculate_ece(p_weighted, y_true, B)\n\n    # 4. Evaluate sensitivity as True Positive Rate (TPR)\n    def calculate_tpr(predictions, labels, threshold):\n        n_positives = np.sum(labels == 1)\n        if n_positives == 0:\n            return 0.0 # Or undefined, but 0.0 is a common convention\n            \n        true_positives = np.sum((predictions > threshold) & (labels == 1))\n        tpr = true_positives / n_positives\n        return tpr\n\n    tpr_gap = calculate_tpr(p_gap, y_true, TAU)\n    tpr_weighted = calculate_tpr(p_weighted, y_true, TAU)\n    \n    # 5. Collate results and print in the specified format\n    results = [ece_gap, ece_weighted, tpr_gap, tpr_weighted]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3129763"}]}