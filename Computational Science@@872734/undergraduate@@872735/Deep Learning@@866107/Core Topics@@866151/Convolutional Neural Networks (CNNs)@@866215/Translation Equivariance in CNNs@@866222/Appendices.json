{"hands_on_practices": [{"introduction": "Perfect translation equivariance is the ideal property we associate with convolutions, but this ideal depends critically on how we handle the boundaries of a finite signal. This exercise provides a foundational look into this concept by comparing two different padding schemes in one dimension: circular padding and the more common zero padding. By implementing and contrasting these methods on a periodic signal, you will gain a concrete, mathematical understanding of the exact conditions under which perfect equivariance is achieved and how seemingly innocuous choices like zero padding introduce boundary-related errors. [@problem_id:3196029]", "problem": "You are given the task of evaluating translation equivariance for one-dimensional discrete cross-correlation as used in a Convolutional Neural Network (CNN). Work strictly with periodic signals and formalize two padding schemes: circular padding and zero padding. Your job is to implement both schemes and to quantify equivariance errors when the input is circularly shifted. The assessment should be purely mathematical and algorithmic, without any appeal to external files or user input.\n\nBase definitions to use:\n- Let the input signal be $x \\in \\mathbb{R}^N$ with indices $n \\in \\{0,1,\\dots,N-1\\}$ and the kernel be $w \\in \\mathbb{R}^M$ with $M$ odd. Let $c = \\frac{M-1}{2}$ denote the kernel center index.\n- Define the circular shift operator $T_s$ for an integer shift $s$ by\n$$\n\\left(T_s x\\right)[n] = x\\left[(n - s) \\bmod N\\right].\n$$\n- Define the $1$-dimensional circularly padded cross-correlation (output length $N$) by\n$$\n\\left(\\mathrm{Corr}_{\\mathrm{circ}}(x,w)\\right)[n] = \\sum_{m=0}^{M-1} w[m]\\; x\\left[(n + m - c) \\bmod N\\right].\n$$\n- Define the $1$-dimensional zero-padded cross-correlation (output length $N$) by\n$$\n\\left(\\mathrm{Corr}_{0}(x,w)\\right)[n] = \\sum_{m=0}^{M-1} w[m]\\; x\\left[n + m - c\\right]\\mathbf{1}\\left(0 \\le n + m - c \\le N-1\\right),\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function, so values outside $\\{0,1,\\dots,N-1\\}$ are treated as $0$.\n\nTask:\n- For each given test case, compute two nonnegative real numbers:\n    - The maximum absolute deviation from translation equivariance under circular padding,\n    $$\n    \\varepsilon_{\\mathrm{circ}} = \\max_{0 \\le n \\le N-1} \\left| \\left(\\mathrm{Corr}_{\\mathrm{circ}}(T_s x, w)\\right)[n] - \\left(T_s\\,\\mathrm{Corr}_{\\mathrm{circ}}(x,w)\\right)[n] \\right|.\n    $$\n    - The maximum absolute deviation from translation equivariance under zero padding,\n    $$\n    \\varepsilon_{0} = \\max_{0 \\le n \\le N-1} \\left| \\left(\\mathrm{Corr}_{0}(T_s x, w)\\right)[n] - \\left(T_s\\,\\mathrm{Corr}_{0}(x,w)\\right)[n] \\right|.\n    $$\n- Implement the circular shift operator $T_s$ using modular indexing as defined above.\n- Implement $\\mathrm{Corr}_{\\mathrm{circ}}$ using wrap-around indexing and $\\mathrm{Corr}_{0}$ using the indicator as above.\n- Use floating-point arithmetic. There are no physical units involved. Angles are mentioned only as a motivating example of periodic signals; all computations are purely numerical.\n\nTest suite:\nProvide results for the following six parameter sets $(x,w,s)$, each with the stated $N$ and odd $M$:\n- Case $1$: $N = 8$, $x = [0,1,2,3,4,5,6,7]$, $w = [1,0,-1]$, $s = 2$.\n- Case $2$: $N = 8$, $x[n] = \\sin\\left(2\\pi n / 8\\right)$ for $n \\in \\{0,1,2,3,4,5,6,7\\}$, $w = [1,2,3,2,1]/9$, $s = 3$.\n- Case $3$: $N = 5$, $x = [1,0,0,0,0]$, $w = [-1,0,2]$, $s = 4$.\n- Case $4$: $N = 7$, $x = [0.2,-0.1,0.5,-0.3,0.7,-0.2,0.0]$, $w = [0.25,0.5,0.25]$, $s = 0$.\n- Case $5$: $N = 9$, $x[n] = \\cos\\left(2\\pi n / 9\\right)$ for $n \\in \\{0,1,2,3,4,5,6,7,8\\}$, $w = [1,-1,0,1,-1]$, $s = 4$.\n- Case $6$: $N = 8$, $x = [1,-1,1,-1,1,-1,1,-1]$, $w = [2,-1,0,1,-2,1,0]$, $s = 1$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- The list must contain, in order, the $12$ floating-point values\n$$\n[\\varepsilon_{\\mathrm{circ}}^{(1)}, \\varepsilon_{0}^{(1)}, \\varepsilon_{\\mathrm{circ}}^{(2)}, \\varepsilon_{0}^{(2)}, \\dots, \\varepsilon_{\\mathrm{circ}}^{(6)}, \\varepsilon_{0}^{(6)}],\n$$\nwhere the superscript indicates the case number. Represent each value as a decimal number. For example, a syntactically correct line would look like $[0.0,0.5,0.0,1.25,\\dots]$.", "solution": "The problem requires an evaluation of the translation equivariance property for one-dimensional discrete cross-correlation under two distinct padding schemes: circular padding and zero padding. Translation equivariance is a fundamental concept in signal processing and convolutional neural networks ($CNNs$). An operator $F$ is said to be equivariant to a transformation $T$ if applying the transformation before or after the operator yields the same result, i.e., $F(T(x)) = T(F(x))$ for any input $x$. In this context, the operator $F$ is the cross-correlation ($\\mathrm{Corr}_{\\mathrm{circ}}$ or $\\mathrm{Corr}_{0}$) and the transformation $T$ is the circular shift operator $T_s$. We will analyze each padding scheme's adherence to this property and then present an algorithm to compute the specified deviation metrics, $\\varepsilon_{\\mathrm{circ}}$ and $\\varepsilon_{0}$.\n\nFirst, let us formalize the quantities to be compared. For a given padding scheme, we compute the output of the correlation on the shifted input, which we denote as $y'_{\\mathrm{pad}} = \\mathrm{Corr}_{\\mathrm{pad}}(T_s x, w)$. We also compute the output of the correlation on the original input, $y_{\\mathrm{pad}} = \\mathrm{Corr}_{\\mathrm{pad}}(x, w)$, and then shift this output, $T_s y_{\\mathrm{pad}}$. The equivariance error is the maximum absolute difference between these two resulting signals, element-wise.\n\n**Analysis of Circular Padding and Equivariance**\n\nThe circular cross-correlation operator, as defined, is perfectly equivariant with respect to the circular shift operator $T_s$. We can prove this mathematically. Let $y_{\\mathrm{circ}} = \\mathrm{Corr}_{\\mathrm{circ}}(x, w)$. We seek to show that $\\mathrm{Corr}_{\\mathrm{circ}}(T_s x, w) = T_s y_{\\mathrm{circ}}$.\n\nLet's expand the left-hand side (LHS) at an arbitrary index $n \\in \\{0, 1, \\dots, N-1\\}$:\n$$\n\\text{LHS}[n] = \\left(\\mathrm{Corr}_{\\mathrm{circ}}(T_s x, w)\\right)[n] = \\sum_{m=0}^{M-1} w[m]\\; (T_s x)\\left[(n + m - c) \\bmod N\\right]\n$$\nBy the definition of the shift operator, $(T_s x)[k] = x[(k - s) \\bmod N]$. Substituting $k = (n + m - c) \\bmod N$:\n$$\n\\text{LHS}[n] = \\sum_{m=0}^{M-1} w[m]\\; x\\left[\\left( ((n + m - c) \\bmod N) - s \\right) \\bmod N\\right]\n$$\nUsing the property of modular arithmetic that $((a \\bmod N) - b) \\bmod N = (a-b) \\bmod N$, we can simplify the index:\n$$\n\\text{LHS}[n] = \\sum_{m=0}^{M-1} w[m]\\; x\\left[(n - s + m - c) \\bmod N\\right]\n$$\nNow let's examine the right-hand side (RHS), $T_s y_{\\mathrm{circ}}$.\n$$\n\\text{RHS}[n] = (T_s y_{\\mathrm{circ}})[n] = y_{\\mathrm{circ}}[(n-s) \\bmod N]\n$$\nBy definition of $y_{\\mathrm{circ}} = \\mathrm{Corr}_{\\mathrm{circ}}(x, w)$, we have:\n$$\n\\text{RHS}[n] = \\left(\\mathrm{Corr}_{\\mathrm{circ}}(x, w)\\right)[(n-s) \\bmod N] = \\sum_{m=0}^{M-1} w[m]\\; x\\left[(((n-s)\\bmod N) + m - c) \\bmod N\\right]\n$$\nAgain, using the property of modular arithmetic, this simplifies to:\n$$\n\\text{RHS}[n] = \\sum_{m=0}^{M-1} w[m]\\; x\\left[(n - s + m - c) \\bmod N\\right]\n$$\nSince $\\text{LHS}[n] = \\text{RHS}[n]$ for all $n \\in \\{0, 1, \\dots, N-1\\}$, the equality $\\mathrm{Corr}_{\\mathrm{circ}}(T_s x, w) = T_s \\mathrm{Corr}_{\\mathrm{circ}}(x, w)$ holds. Therefore, the theoretical value for the equivariance error is $\\varepsilon_{\\mathrm{circ}} = 0$. Any non-zero result from a numerical computation would be attributable to floating-point precision errors. An exception is the case of $s=0$, where the shift is an identity operation, making equivariance trivial for any operator.\n\n**Analysis of Zero Padding and Non-Equivariance**\n\nIn contrast, cross-correlation with zero padding is generally not equivariant to circular shifts. The core of the issue lies in the incompatibility between the circular nature of the shift $T_s$ and the acyclic, bounded nature of the zero-padded correlation $\\mathrm{Corr}_{0}$. The operator $T_s$ wraps values from one end of the signal to the other. The operator $\\mathrm{Corr}_{0}$, however, treats the signal boundaries as absolute, padding with zeros beyond the domain $\\{0, 1, \\dots, N-1\\}$.\n\nLet's analyze the two sides of the equivariance equation for the zero-padded case.\nThe left-hand side is:\n$$\n\\text{LHS}[n] = (\\mathrm{Corr}_{0}(T_s x, w))[n] = \\sum_{m=0}^{M-1} w[m]\\; (T_s x)[n + m - c]\\; \\mathbf{1}(0 \\le n + m - c \\le N-1)\n$$\nSubstituting $(T_s x)[k] = x[(k-s) \\bmod N]$:\n$$\n\\text{LHS}[n] = \\sum_{m=0}^{M-1} w[m]\\; x[((n + m - c) - s) \\bmod N]\\; \\mathbf{1}(0 \\le n + m - c \\le N-1)\n$$\nThe right-hand side is:\n$$\n\\text{RHS}[n] = (T_s \\mathrm{Corr}_{0}(x,w))[n] = (\\mathrm{Corr}_{0}(x,w))[(n-s) \\bmod N]\n$$\nLet $n' = (n-s) \\bmod N$. Then:\n$$\n\\text{RHS}[n] = \\sum_{m=0}^{M-1} w[m]\\; x[n' + m - c]\\; \\mathbf{1}(0 \\le n' + m - c \\le N-1)\n$$\nThe expressions for $\\text{LHS}[n]$ and $\\mathrm{RHS}[n]$ are not equivalent. The $\\mathrm{LHS}$ applies the indicator function to the index $k = n + m - c$ and then uses the wrapped value $x[(k-s) \\bmod N]$. The $\\mathrm{RHS}$ uses the shifted output index $n'$, and applies the indicator function to an index based on $n'$, using the unwrapped value $x[n' + m - c]$. This discrepancy is most pronounced at the boundaries. For instance, when the kernel support near an edge in the $\\mathrm{RHS}$ computation would read a value outside $[0, N-1]$ (and thus get a zero), the corresponding kernel in the $\\mathrm{LHS}$ might read a non-zero value that has been wrapped around by $T_s$. This leads to a non-zero equivariance error, $\\varepsilon_{0} > 0$, except for trivial cases (like $s=0$).\n\n**Algorithmic Procedure**\n\nThe verification for each test case $(x, w, s)$ will proceed as follows:\n$1$. Determine the signal length $N = \\mathrm{len}(x)$ and kernel length $M = \\mathrm{len}(w)$. Calculate the kernel center index $c = (M-1)/2$.\n$2$. Implement the circular shift operator $T_s$ according to the definition $(T_s x)[n] = x[(n - s) \\bmod N]$.\n$3$. Implement the circular cross-correlation $\\mathrm{Corr}_{\\mathrm{circ}}(x, w)$ by summing over the kernel and using modular arithmetic for input signal indexing: $(n+m-c)\\bmod N$.\n$4$. Implement the zero-padded cross-correlation $\\mathrm{Corr}_0(x, w)$ by summing over the kernel and using an indicator function (or conditional logic) to use $x[k]$ for $k = n+m-c$ if $0 \\le k \\le N-1$ and $0$ otherwise.\n$5$. For the circular case:\n    a. Compute the shifted input signal: $x' = T_s x$.\n    b. Compute the correlation of the shifted input: $y'_{\\mathrm{circ}} = \\mathrm{Corr}_{\\mathrm{circ}}(x', w)$.\n    c. Compute the correlation of the original input: $y_{\\mathrm{circ}} = \\mathrm{Corr}_{\\mathrm{circ}}(x, w)$.\n    d. Shift the resulting output: $y_{\\mathrm{circ\\_shifted}} = T_s y_{\\mathrm{circ}}$.\n    e. Calculate the error: $\\varepsilon_{\\mathrm{circ}} = \\max |y'_{\\mathrm{circ}} - y_{\\mathrm{circ\\_shifted}}|$.\n$6$. For the zero-padded case:\n    a. Compute the shifted input signal: $x' = T_s x$.\n    b. Compute the correlation of the shifted input: $y'_{0} = \\mathrm{Corr}_{0}(x', w)$.\n    c. Compute the correlation of the original input: $y_{0} = \\mathrm{Corr}_{0}(x, w)$.\n    d. Shift the resulting output: $y_{0\\_\\mathrm{shifted}} = T_s y_{0}$.\n    e. Calculate the error: $\\varepsilon_{0} = \\max |y'_{0} - y_{0\\_\\mathrm{shifted}}|$.\n$7$. The final result for the test case is the pair $(\\varepsilon_{\\mathrm{circ}}, \\varepsilon_{0})$. This procedure is repeated for all specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n#\n# Helper functions for the specified operations\n#\n\ndef circular_shift(x: np.ndarray, s: int) -> np.ndarray:\n    \"\"\"Implements the circular shift operator T_s.\n    (T_s x)[n] = x[(n - s) mod N]\n    This corresponds to a right shift by s.\n    \"\"\"\n    return np.roll(x, s)\n\ndef corr_circ(x: np.ndarray, w: np.ndarray) -> np.ndarray:\n    \"\"\"Implements 1D circularly padded cross-correlation.\n    (Corr_circ(x,w))[n] = sum_{m=0}^{M-1} w[m] x[(n + m - c) mod N]\n    \"\"\"\n    N = len(x)\n    M = len(w)\n    c = (M - 1) // 2\n    y = np.zeros(N, dtype=float)\n    for n in range(N):\n        val = 0.0\n        for m in range(M):\n            idx = (n + m - c) % N\n            val += w[m] * x[idx]\n        y[n] = val\n    return y\n\ndef corr_zero(x: np.ndarray, w: np.ndarray) -> np.ndarray:\n    \"\"\"Implements 1D zero-padded cross-correlation.\n    (Corr_0(x,w))[n] = sum_{m=0}^{M-1} w[m] x[n + m - c] * 1(0 <= n+m-c <= N-1)\n    \"\"\"\n    N = len(x)\n    M = len(w)\n    c = (M - 1) // 2\n    y = np.zeros(N, dtype=float)\n    for n in range(N):\n        val = 0.0\n        for m in range(M):\n            idx = n + m - c\n            if 0 <= idx < N:\n                val += w[m] * x[idx]\n        y[n] = val\n    return y\n\ndef calculate_equivariance_error(x, w, s):\n    \"\"\"\n    Calculates the equivariance errors ε_circ and ε_0 for a given\n    input signal x, kernel w, and shift s.\n    \"\"\"\n    # Circular Padding Case\n    x_shifted = circular_shift(x, s)\n    y_corr_of_shifted = corr_circ(x_shifted, w)\n    \n    y = corr_circ(x, w)\n    y_shifted_of_corr = circular_shift(y, s)\n    \n    eps_circ = np.max(np.abs(y_corr_of_shifted - y_shifted_of_corr))\n\n    # Zero Padding Case\n    # x_shifted is the same\n    y_corr_of_shifted_zero = corr_zero(x_shifted, w)\n\n    y_zero = corr_zero(x, w)\n    y_shifted_of_corr_zero = circular_shift(y_zero, s)\n    \n    eps_zero = np.max(np.abs(y_corr_of_shifted_zero - y_shifted_of_corr_zero))\n    \n    return eps_circ, eps_zero\n\ndef solve():\n    \"\"\"\n    Defines the test cases, computes the equivariance errors for each,\n    and prints the results in the specified format.\n    \"\"\"\n    \n    # Define test cases\n    case1 = (np.array([0,1,2,3,4,5,6,7], dtype=float), np.array([1,0,-1], dtype=float), 2)\n    \n    N2 = 8\n    n2 = np.arange(N2)\n    x2 = np.sin(2 * np.pi * n2 / N2)\n    w2 = np.array([1,2,3,2,1], dtype=float) / 9.0\n    case2 = (x2, w2, 3)\n\n    case3 = (np.array([1,0,0,0,0], dtype=float), np.array([-1,0,2], dtype=float), 4)\n\n    case4 = (np.array([0.2,-0.1,0.5,-0.3,0.7,-0.2,0.0], dtype=float), np.array([0.25,0.5,0.25], dtype=float), 0)\n\n    N5 = 9\n    n5 = np.arange(N5)\n    x5 = np.cos(2 * np.pi * n5 / N5)\n    w5 = np.array([1,-1,0,1,-1], dtype=float)\n    case5 = (x5, w5, 4)\n\n    case6 = (np.array([1,-1,1,-1,1,-1,1,-1], dtype=float), np.array([2,-1,0,1,-2,1,0], dtype=float), 1)\n\n    test_cases = [case1, case2, case3, case4, case5, case6]\n    \n    results = []\n    for x, w, s in test_cases:\n        eps_circ, eps_zero = calculate_equivariance_error(x, w, s)\n        results.append(eps_circ)\n        results.append(eps_zero)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3196029"}, {"introduction": "While convolution itself can be perfectly equivariant, modern CNNs contain other layers that break this property, most notably the pooling layer. Max-pooling is a non-linear downsampling operation that is only equivariant to input shifts that are an integer multiple of the pooling stride, creating discrepancies for all other \"sub-stride\" shifts. This exercise challenges you to quantify this break in equivariance by computing a \"commutator magnitude,\" which measures the difference between processing a shifted input versus shifting a processed output. [@problem_id:3196094] By testing this on a synthetic checkerboard pattern, you will see precisely how and by how much max-pooling compromises the network's equivariance.", "problem": "You are given the task of quantifying translation equivariance breaks induced by max pooling in a Convolutional Neural Network (CNN) pipeline using synthetic checkerboard inputs. The assessment must be done by computing a commutator magnitude that compares applying a spatial shift before the pipeline to applying a corresponding downsampled shift after the pipeline. The pipeline consists of circular convolution, followed by a Rectified Linear Unit (ReLU), followed by non-overlapping max pooling. You will systematically vary the input phase of the checkerboard with respect to the pooling grid and aggregate the commutator magnitudes.\n\nDefinitions and setup:\n- Let the input be a square image of size $N \\times N$ with periodic boundary conditions (circular wrap).\n- Define the circular translation operator $T_{(\\Delta u,\\Delta v)}$ on an image $x$ by\n$$\n(T_{(\\Delta u,\\Delta v)} x)[u,v] \\;=\\; x[(u-\\Delta u) \\bmod N,\\; (v-\\Delta v) \\bmod N].\n$$\n- Define the circular convolution operator $C_K$ with kernel $K$ by\n$$\n(C_K x)[u,v] \\;=\\; \\sum_{m}\\sum_{n} K[m,n]\\; x[(u-m) \\bmod N,\\; (v-n) \\bmod N].\n$$\n- Define the pointwise Rectified Linear Unit (ReLU) $\\phi$ by\n$$\n\\phi(z) \\;=\\; \\max(0,z).\n$$\n- Define non-overlapping max pooling with window size $s \\times s$ and stride $s$ on an $N \\times N$ input (assume $N$ is divisible by $s$) as the operator $P_s$ producing an output of size $(N/s) \\times (N/s)$:\n$$\n(P_s x)[i,j] \\;=\\; \\max_{0 \\le a < s,\\; 0 \\le b < s} \\; x[i s + a,\\; j s + b],\n$$\nwith indices computed in the usual array sense and relying on the fact that $N$ is divisible by $s$ (no boundary wrap is needed inside each pooling window).\n- Define the downsampled shift operator $U_{(\\delta_u,\\delta_v)}$ acting on pooled outputs of size $(N/s)\\times(N/s)$ by\n$$\n(U_{(\\delta_u,\\delta_v)} y)[i,j] \\;=\\; y[(i-\\delta_u) \\bmod (N/s), \\; (j-\\delta_v) \\bmod (N/s)].\n$$\n- Define the composite CNN operator $F$ by\n$$\nF \\;=\\; P_s \\circ \\phi \\circ C_K.\n$$\n- For any input $x$ and integer input-space shift $(\\Delta u,\\Delta v)$, define the corresponding output-space shift\n$$\n(\\delta_u,\\delta_v) \\;=\\; \\left(\\left\\lfloor \\frac{\\Delta u}{s} \\right\\rfloor, \\; \\left\\lfloor \\frac{\\Delta v}{s} \\right\\rfloor \\right).\n$$\n- Define the commutator magnitude (normalized) by\n$$\nM(x; \\Delta u,\\Delta v) \\;=\\; \\frac{\\left\\| F\\left(T_{(\\Delta u,\\Delta v)} x\\right) \\;-\\; U_{(\\delta_u,\\delta_v)}\\left(F(x)\\right) \\right\\|_2}{\\left\\| F(x) \\right\\|_2 + \\varepsilon},\n$$\nwhere $\\|\\cdot\\|_2$ is the Euclidean norm over the pooled output and $\\varepsilon$ is a small positive constant to avoid division by zero.\n\nInput generation and phase sweep:\n- Use a binary checkerboard pattern with period $2$:\n$$\nB[u,v] \\;=\\; \\mathbf{1}\\left\\{ \\left((u+v) \\bmod 2\\right) = 0 \\right\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n- Define phase-shifted inputs relative to the pooling grid by\n$$\nB_{p,q}[u,v] \\;=\\; B[(u+p) \\bmod N,\\; (v+q) \\bmod N],\n$$\nfor all integer phases $p \\in \\{0,1,\\dots,s-1\\}$ and $q \\in \\{0,1,\\dots,s-1\\}$.\n\nKernels:\n- Use a fixed $3 \\times 3$ Sobel kernel in the horizontal direction,\n$$\nK \\;=\\; \\begin{bmatrix}\n-1 & 0 & 1 \\\\\n-2 & 0 & 2 \\\\\n-1 & 0 & 1\n\\end{bmatrix}.\n$$\n\nTasks:\n1. Implement $T_{(\\Delta u,\\Delta v)}$, $C_K$ with circular boundary conditions, $\\phi$, $P_s$, and $U_{(\\delta_u,\\delta_v)}$ as defined above.\n2. For each test case below, generate all $s \\times s$ phase-shifted inputs $B_{p,q}$, compute $M(B_{p,q}; \\Delta u,\\Delta v)$ for each $(p,q)$, and aggregate by computing:\n   - The mean over all phases,\n   - The maximum over all phases.\n3. For numerical stability, use $\\varepsilon = 10^{-12}$ in the definition of $M$.\n4. Round each aggregated float to $6$ decimal places.\n\nTest suite:\n- Use the Sobel kernel $K$ above for all cases. For each case, $N$ is the input size, $s$ is the pooling stride/window, and $(\\Delta u,\\Delta v)$ is the input-space shift.\n  1. Case A: $N=24$, $s=2$, $(\\Delta u,\\Delta v) = (1,0)$.\n  2. Case B: $N=24$, $s=2$, $(\\Delta u,\\Delta v) = (2,0)$.\n  3. Case C: $N=24$, $s=3$, $(\\Delta u,\\Delta v) = (1,1)$.\n  4. Case D: $N=24$, $s=3$, $(\\Delta u,\\Delta v) = (3,0)$.\n  5. Case E: $N=24$, $s=3$, $(\\Delta u,\\Delta v) = (0,0)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a two-element list $[\\text{mean}, \\text{max}]$ in the order of the cases above. For example, the output must look like\n$$\n\\big[ [m_A, M_A], [m_B, M_B], [m_C, M_C], [m_D, M_D], [m_E, M_E] \\big],\n$$\nwith each float rounded to $6$ decimal places.", "solution": "### Principle and Methodology\n\nThe core of this problem is to quantify the degree to which a simple CNN pipeline fails to be translation equivariant. An operator $F$ is said to be translation equivariant if shifting the input results in a correspondingly shifted output. Formally, for a translation operator $T_\\Delta$, there must exist a corresponding output-space translation operator $T'_\\delta$ such that $F(T_\\Delta x) = T'_\\delta(F(x))$ for any input $x$. If this equality does not hold, the operator is not strictly equivariant.\n\nThe degree of non-equivariance can be measured by the magnitude of the commutator between the pipeline operator $F$ and the translation operator $T$. The commutator $[F, T]$ is defined as $F \\circ T - T' \\circ F$. If the operators commuted perfectly (i.e., the system were perfectly equivariant), the result of applying this commutator to an input $x$ would be zero. A non-zero result signifies a break in equivariance. The problem defines a normalized magnitude of this commutator:\n$$\nM(x; \\Delta u,\\Delta v) = \\frac{\\left\\| F\\left(T_{(\\Delta u,\\Delta v)} x\\right) - U_{(\\delta_u,\\delta_v)}\\left(F(x)\\right) \\right\\|_2}{\\left\\| F(x) \\right\\|_2 + \\varepsilon}\n$$\nHere, $F(T_{(\\Delta u,\\Delta v)} x)$ represents the \"shift-then-process\" path, while $U_{(\\delta_u,\\delta_v)}(F(x))$ represents the \"process-then-shift\" path. The discrepancy between these two paths is the break in equivariance.\n\nThe CNN pipeline $F = P_s \\circ \\phi \\circ C_K$ consists of three stages:\n1.  **Circular Convolution ($C_K$)**: This operation is perfectly translation equivariant. Convolving a shifted signal is identical to shifting the convolved signal.\n2.  **ReLU ($\\phi$)**: This pointwise activation function is also perfectly translation equivariant. Applying it before or after a shift yields the same result.\n3.  **Max Pooling ($P_s$)**: This is the source of the equivariance break. Max pooling is only equivariant to shifts that are an integer multiple of its stride $s$. For any other shift value (a \"sub-stride\" shift), the output changes in a non-linear way that cannot be described by a simple translation of the un-shifted output. The composition of these operators results in a pipeline $F$ that is not, in general, translation equivariant.\n\nThe input signal is a synthetic checkerboard pattern. By systematically varying its phase $(p, q)$ with respect to the fixed pooling grid, we can assess the equivariance break over all possible input alignments. This is crucial because the magnitude of the break is highly dependent on how the high-frequency components of the signal align with the pooling window boundaries. Averaging the commutator magnitude over all phases provides a robust, aggregate measure of the pipeline's lack of equivariance for a given input shift $(\\Delta u, \\Delta v)$.\n\n### Algorithmic Implementation\n\nThe solution is implemented by following these steps for each test case:\n\n1.  **Define Operators**: Functions are created for each mathematical operator specified:\n    *   `circular_translate` ($T$ and $U$): Implemented using `numpy.roll`, which performs the required circular shift on a given axis.\n    *   `circular_convolve` ($C_K$): Implemented using `scipy.signal.convolve2d` with `mode='same'` and `boundary='wrap'` to match the circular convolution definition.\n    *   `relu` ($\\phi$): Implemented using `numpy.maximum`, which performs the element-wise operation $\\max(0,z)$.\n    *   `max_pool` ($P_s$): Implemented by reshaping the $N \\times N$ input array into $(N/s, N/s, s, s)$ blocks and then taking the maximum value within each $s \\times s$ block.\n\n2.  **Define CNN Pipeline ($F$)**: A function `cnn_pipeline` composes the above operators in the specified order: $F(x) = P_s(\\phi(C_K(x)))$.\n\n3.  **Phase Sweep and Commutator Calculation**:\n    *   A base checkerboard pattern $B$ of size $N \\times N$ is generated.\n    *   A nested loop iterates through all possible input phases $p, q \\in \\{0, 1, \\dots, s-1\\}$.\n    *   In each iteration, a phase-shifted input $x = B_{p,q}$ is created by applying `circular_translate` to the base checkerboard.\n    *   The two paths are computed:\n        *   Path 1: `F_Tx = cnn_pipeline(circular_translate(x, du, dv), ...)`\n        *   Path 2: `Fx = cnn_pipeline(x, ...)`, `delta_u = du // s`, `delta_v = dv // s`, `U_Fx = downsampled_shift(Fx, delta_u, delta_v)`\n    *   The L2-norms of the difference `F_Tx - U_Fx` and the baseline `Fx` are computed using `numpy.linalg.norm`.\n    *   The commutator magnitude $M$ is calculated as per the formula, using the provided $\\varepsilon = 10^{-12}$. The calculated magnitudes for all phases are stored.\n\n4.  **Aggregation and Output**:\n    *   After iterating through all phases for a given test case, the mean and maximum of the collected commutator magnitudes are computed using `numpy.mean` and `numpy.max`.\n    *   These two aggregated values are formatted to $6$ decimal places and stored.\n    *   Finally, the results for all test cases are formatted into the specified string format `[[mean_A, max_A], [mean_B, max_B], ...]` and printed to standard output.\n\nThis design directly translates the mathematical formulation of the problem into a verifiable numerical experiment. Sanity checks, such as when the input shift $(\\Delta u, \\Delta v)$ is an integer multiple of the pooling stride $s$ or zero, should yield a commutator magnitude of $0.0$, confirming the theoretical understanding of equivariance in this context.", "answer": "```python\nimport numpy as np\nfrom scipy.signal import convolve2d\n\ndef solve():\n    \"\"\"\n    Computes the mean and maximum commutator magnitudes to quantify translation\n    equivariance breaks in a simple CNN pipeline for a suite of test cases.\n    \"\"\"\n\n    def circular_translate(x: np.ndarray, du: int, dv: int) -> np.ndarray:\n        \"\"\"\n        Applies a circular translation T_{(\\Delta u, \\Delta v)} to an image x.\n        (T_(\\Delta u,\\Delta v) x)[u,v] = x[(u-\\Delta u) mod N, (v-\\Delta v) mod N]\n        \"\"\"\n        return np.roll(x, shift=(du, dv), axis=(0, 1))\n\n    def circular_convolve(x: np.ndarray, K: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Applies circular convolution C_K with kernel K to an image x.\n        \"\"\"\n        return convolve2d(x, K, mode='same', boundary='wrap')\n\n    def relu(x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Applies the pointwise Rectified Linear Unit (ReLU) function.\n        \"\"\"\n        return np.maximum(0, x)\n\n    def max_pool(x: np.ndarray, s: int) -> np.ndarray:\n        \"\"\"\n        Applies non-overlapping max pooling P_s with window size s x s.\n        \"\"\"\n        N = x.shape[0]\n        N_out = N // s\n        reshaped = x.reshape(N_out, s, N_out, s)\n        # Swap axes to group pooling windows together: (N_out, N_out, s, s)\n        pooled = reshaped.swapaxes(1, 2)\n        return pooled.max(axis=(2, 3))\n\n    def downsampled_shift(y: np.ndarray, du: int, dv: int) -> np.ndarray:\n        \"\"\"\n        Applies a circular translation U_{(\\delta_u, \\delta_v)} on a pooled output.\n        \"\"\"\n        return np.roll(y, shift=(du, dv), axis=(0, 1))\n\n    def cnn_pipeline(x: np.ndarray, K: np.ndarray, s: int) -> np.ndarray:\n        \"\"\"\n        Computes the full pipeline F = P_s o phi o C_K.\n        \"\"\"\n        convolved = circular_convolve(x, K)\n        activated = relu(convolved)\n        pooled = max_pool(activated, s)\n        return pooled\n\n    def generate_checkerboard(N: int) -> np.ndarray:\n        \"\"\"\n        Generates a binary checkerboard pattern of size N x N.\n        \"\"\"\n        u, v = np.meshgrid(np.arange(N), np.arange(N), indexing='ij')\n        board = ((u + v) % 2) == 0\n        return board.astype(float)\n\n    test_cases = [\n        # (N, s, (du, dv))\n        (24, 2, (1, 0)),  # Case A\n        (24, 2, (2, 0)),  # Case B\n        (24, 3, (1, 1)),  # Case C\n        (24, 3, (3, 0)),  # Case D\n        (24, 3, (0, 0)),  # Case E\n    ]\n    \n    K = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=float)\n    epsilon = 1e-12\n    \n    final_results = []\n    \n    for case_params in test_cases:\n        N, s, (du, dv) = case_params\n        \n        commutator_magnitudes = []\n        base_board = generate_checkerboard(N)\n        \n        for p in range(s):\n            for q in range(s):\n                # 1. Generate phase-shifted input x = B_{p,q}\n                x = circular_translate(base_board, p, q)\n                \n                # 2. Compute \"shift-then-process\" path: F(T(x))\n                shifted_x = circular_translate(x, du, dv)\n                F_Tx = cnn_pipeline(shifted_x, K, s)\n                \n                # 3. Compute \"process-then-shift\" path: U(F(x))\n                Fx = cnn_pipeline(x, K, s)\n                delta_u = du // s\n                delta_v = dv // s\n                U_Fx = downsampled_shift(Fx, delta_u, delta_v)\n                \n                # 4. Calculate the commutator magnitude M(x; du, dv)\n                norm_diff = np.linalg.norm(F_Tx - U_Fx)\n                norm_Fx = np.linalg.norm(Fx)\n                \n                M = norm_diff / (norm_Fx + epsilon)\n                commutator_magnitudes.append(M)\n        \n        # 5. Aggregate results (mean and max) for the test case\n        mean_M = np.mean(commutator_magnitudes)\n        max_M = np.max(commutator_magnitudes)\n        \n        final_results.append([mean_M, max_M])\n        \n    # 6. Format and print the final output\n    formatted_pairs = []\n    for mean_val, max_val in final_results:\n        formatted_pairs.append(f\"[{mean_val:.6f}, {max_val:.6f}]\")\n    \n    print(f\"[{', '.join(formatted_pairs)}]\")\n\nsolve()\n```", "id": "3196094"}, {"introduction": "We have seen how architectural choices like padding and pooling can theoretically break equivariance, but do these subtle effects have meaningful consequences? This final practice demonstrates that they do, by showing how different boundary conditions can lead to different classification outcomes for the exact same feature, depending only on its position. You will construct a simple \"adversarial\" input where merely shifting a feature to the edge of an image is enough to flip a model's prediction when the padding scheme is changed. [@problem_id:3126196] This exercise powerfully connects the abstract principle of equivariance to the practical goal of building robust and predictable models.", "problem": "You are given a directive to construct adversarial inputs that exploit boundary conditions in a Convolutional Neural Network (CNN). Begin from the foundational definitions of two-dimensional discrete convolution, translation, and receptive fields, then reason about how boundary conditions affect translation equivariance and effective receptive fields near image edges.\n\nFundamental base:\n- Define the two-dimensional discrete convolution of a finite image signal $x \\in \\mathbb{R}^{H \\times W}$ with a finite kernel $k \\in \\mathbb{R}^{m \\times n}$ on an infinite lattice as\n$$\n(y \\ast k)[i,j] \\triangleq \\sum_{u=0}^{m-1}\\sum_{v=0}^{n-1} k[u,v]\\; y[i+u-\\lfloor m/2 \\rfloor, j+v-\\lfloor n/2 \\rfloor],\n$$\nwhere $y$ is an extension of $x$ by a boundary condition. The extension $y$ is defined by a padding scheme, for example, zero padding or reflection padding. The unpadded case computes only where indices are valid. The receptive field of an output position is the subset of the input that influences that output via the convolution sum. The translation operator $T_{\\Delta i, \\Delta j}$ acts on $x$ by $(T_{\\Delta i, \\Delta j}x)[i,j] = x[i-\\Delta i,j-\\Delta j]$. In the ideal infinite-lattice case without boundary truncation, convolution is translation equivariant, meaning $(T_{\\Delta i, \\Delta j}(x) \\ast k) = T_{\\Delta i, \\Delta j}(x \\ast k)$.\n\nTask:\n- Implement a single-layer Convolutional Neural Network (CNN) with one $3 \\times 3$ convolutional kernel $K$ whose entries are all $1$, identity nonlinearity, and global average pooling that produces a scalar score $s$. Explicitly, for an output feature map $z$, define the pooled score\n$$\ns \\triangleq \\frac{1}{|z|}\\sum_{i,j} z[i,j],\n$$\nwhere $|z|$ is the number of elements in the feature map. Use a binary classifier that outputs the label $1$ if $s \\ge \\tau$ and $0$ otherwise, with the threshold fixed to $\\tau = 1.13$.\n\n- Construct input images $x \\in \\mathbb{R}^{8 \\times 8}$ with background intensity $0$ and a single bright square feature of intensity $1$ and size $3 \\times 3$ placed at specified coordinates $(i_0,j_0)$, meaning\n$$\nx[i,j] = \\begin{cases}\n1 & \\text{if } i_0 \\le i \\le i_0+2 \\text{ and } j_0 \\le j \\le j_0+2,\\\\\n0 & \\text{otherwise.}\n\\end{cases}\n$$\n\n- Evaluate the network under three boundary conditions:\n    1. Same convolution with zero padding (\"same_zero\"), where the image is padded by a $1$-pixel border of zeros before convolution and the output size is $8 \\times 8$.\n    2. Same convolution with reflection padding (\"same_reflect\"), where the image is padded by a $1$-pixel reflection before convolution and the output size is $8 \\times 8$.\n    3. Valid convolution (\"valid\"), where no padding is applied and only positions where the $3 \\times 3$ kernel fully fits inside the image are convolved, resulting in an output size of $6 \\times 6$.\n\n- For each test case, compute two scores $s_A$ and $s_B$ using two specified boundary conditions $A$ and $B$, classify with the threshold $\\tau$, and return:\n    - A boolean indicating whether the predicted class under $A$ differs from the predicted class under $B$.\n    - The margin difference $s_B - s_A$ as a float.\n\nTest suite:\n- Use the following parameter values, each test case encoded as $(H, k, i_0, j_0, A, B)$:\n    1. $(8, 3, 2, 2, 'same_zero', 'same_reflect')$ — feature near the center (happy path).\n    2. $(8, 3, 2, 5, 'same_zero', 'same_reflect')$ — feature touching the right edge.\n    3. $(8, 3, 0, 0, 'valid', 'same_reflect')$ — feature at the top-left corner, comparing valid versus reflective padding.\n    4. $(8, 3, 5, 2, 'valid', 'same_zero')$ — feature touching the bottom edge, comparing valid versus zero padding.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, each inner list in the form $[\\text{flip}, \\text{margin}]$. For example, the output must look like\n$$\n[[bool_1,float_1],[bool_2,float_2],[bool_3,float_3],[bool_4,float_4]].\n$$\nNo physical units or angles are involved. All numeric outputs must be plain booleans and floats. The program must be self-contained and require no input.", "solution": "This problem requires constructing a simple single-layer CNN to demonstrate how different boundary conditions can affect classification outcomes. The network comprises three stages: convolution with a specified padding scheme, global average pooling, and threshold-based classification.\n\nFirst, let's establish the core computational components.\n\n**1. Input Image Generation**\nFor each test case, an input image $x \\in \\mathbb{R}^{8 \\times 8}$ is generated. This image consists of a constant background of intensity $0$ and a $3 \\times 3$ square feature of intensity $1$. The feature's top-left corner is located at coordinates $(i_0, j_0)$.\nThe image $x$ is defined as:\n$$\nx[i,j] = \\begin{cases}\n1 & \\text{if } i_0 \\le i < i_0+3 \\text{ and } j_0 \\le j < j_0+3 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nfor $i, j \\in \\{0, 1, \\dots, 7\\}$.\n\n**2. Convolutional Layer with Padding**\nThe convolutional layer uses a $3 \\times 3$ kernel $K$ where all entries are $1$. The operation performed is a two-dimensional discrete cross-correlation, which is equivalent to convolution here because the kernel is symmetric. The problem's convolution definition is:\n$$\nz[i,j] = (y \\ast k)[i,j] = \\sum_{u=0}^{2}\\sum_{v=0}^{2} K[u,v]\\; y[i+u-1, j+v-1]\n$$\nwhere $y$ is the padded input image. This operation can be implemented using standard library functions that compute cross-correlation.\n\nThe problem specifies three padding schemes (`A`, `B`):\n\n- **`same_zero`**: The $8 \\times 8$ input image $x$ is padded with a $1$-pixel border of zeros to produce a $10 \\times 10$ image $y$. A 'valid' correlation is then performed on $y$, resulting in an output feature map $z$ of size $8 \\times 8$.\n- **`same_reflect`**: The input image $x$ is padded with a $1$-pixel border using reflection of the edge values. This also produces a $10 \\times 10$ image $y$. A 'valid' correlation on $y$ yields an $8 \\times 8$ feature map $z$.\n- **`valid`**: No padding is applied. The correlation is performed directly on the $8 \\times 8$ input image $x$. The resulting feature map $z$ has a size of $6 \\times 6$.\n\n**3. Global Average Pooling**\nThe scalar score $s$ is computed by taking the arithmetic mean of all elements in the output feature map $z$:\n$$\ns = \\frac{1}{|z|}\\sum_{i,j} z[i,j]\n$$\nHere, $|z|$ denotes the total number of elements in $z$. Critically, $|z|$ is $8 \\times 8 = 64$ for `same_zero` and `same_reflect` padding, but $6 \\times 6 = 36$ for `valid` padding.\n\n**4. Classification and Comparison**\nFor each test case, we compute two scores, $s_A$ and $s_B$, using the two specified padding schemes $A$ and $B$. These scores are then used to determine the predicted classes:\n$$\n\\text{class}_A = \\begin{cases} 1 & \\text{if } s_A \\ge \\tau \\\\ 0 & \\text{if } s_A < \\tau \\end{cases} \\quad \\text{and} \\quad \\text{class}_B = \\begin{cases} 1 & \\text{if } s_B \\ge \\tau \\\\ 0 & \\text{if } s_B < \\tau \\end{cases}\n$$\nwith the given threshold $\\tau = 1.13$.\n\nThe final outputs for each test case are:\n- A boolean value indicating if the classification outcome is different: $\\text{flip} = (\\text{class}_A \\neq \\text{class}_B)$.\n- The floating-point difference in scores: $\\text{margin} = s_B - s_A$.\n\nLet's analyze test case 3: $(i_0=0, j_0=0, A=\\text{valid}, B=\\text{same\\_reflect})$.\n\n- **Input `x`**: A $3 \\times 3$ block of ones at the top-left corner of an $8 \\times 8$ zero-matrix.\n- **Path A (`valid`)**: The output $z_A$ is $6 \\times 6$. A manual or computational calculation shows the sum of its elements is 36. The score is $s_A = \\frac{1}{|z_A|} \\sum z_A = \\frac{36}{36} = 1.0$. Since $1.0  1.13$, $\\text{class}_A = 0$.\n- **Path B (`same_reflect`)**: The input `x` is padded with reflection. The $3 \\times 3$ feature at the corner interacts with the padding. A computational check shows the sum of the resulting $8 \\times 8$ feature map $z_B$ is 81. The score is $s_B = \\frac{1}{|z_B|} \\sum z_B = \\frac{81}{64} \\approx 1.266$. Since $1.266 \\ge 1.13$, $\\text{class}_B = 1$.\n- **Result for Case 3**:\n    - $\\text{flip} = (\\text{class}_A \\neq \\text{class}_B) = (0 \\neq 1) = \\text{True}$.\n    - $\\text{margin} = s_B - s_A \\approx 1.266 - 1.0 = 0.266$.\nThe pair to be returned is $[\\text{True}, 0.265625]$. This same logic is implemented for all test cases in the provided code.", "answer": "```python\nimport numpy as np\nfrom scipy.signal import correlate2d\n\ndef cnn_forward_pass(x, padding_mode, kernel):\n    \"\"\"\n    Performs a single forward pass of the simple CNN for a given padding mode.\n\n    Args:\n        x (np.ndarray): The input image.\n        padding_mode (str): The padding mode ('same_zero', 'same_reflect', 'valid').\n        kernel (np.ndarray): The convolutional kernel.\n\n    Returns:\n        float: The scalar score 's'.\n    \"\"\"\n    if padding_mode == 'same_zero':\n        # Pad with a 1-pixel border of zeros. Output size will be 8x8.\n        padded_x = np.pad(x, pad_width=1, mode='constant', constant_values=0)\n        # Correlate to get 8x8 output.\n        # (10x10 input corr 3x3) - (10-3+1)x(10-3+1) = 8x8 output.\n        z = correlate2d(padded_x, kernel, mode='valid')\n    elif padding_mode == 'same_reflect':\n        # Pad with a 1-pixel reflection. Output size will be 8x8.\n        padded_x = np.pad(x, pad_width=1, mode='reflect')\n        # Correlate to get 8x8 output.\n        z = correlate2d(padded_x, kernel, mode='valid')\n    elif padding_mode == 'valid':\n        # No padding. Output size will be 6x6.\n        # (8x8 input corr 3x3) - (8-3+1)x(8-3+1) = 6x6 output.\n        z = correlate2d(x, kernel, mode='valid')\n    else:\n        raise ValueError(f\"Unknown padding mode: {padding_mode}\")\n\n    # Global Average Pooling\n    score = np.mean(z)\n    return score\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and generate the final output.\n    \"\"\"\n    # Problem constants\n    H, W = 8, 8\n    kernel_size = 3\n    feature_size = 3\n    tau = 1.13\n    kernel = np.ones((kernel_size, kernel_size))\n\n    # Test suite: (i0, j0, mode_A, mode_B)\n    test_cases = [\n        (2, 2, 'same_zero', 'same_reflect'),\n        (2, 5, 'same_zero', 'same_reflect'),\n        (0, 0, 'valid', 'same_reflect'),\n        (5, 2, 'valid', 'same_zero')\n    ]\n\n    results = []\n    for i0, j0, mode_A, mode_B in test_cases:\n        # 1. Construct input image x\n        x = np.zeros((H, W), dtype=np.float64)\n        x[i0 : i0 + feature_size, j0 : j0 + feature_size] = 1.0\n\n        # 2. Evaluate for condition A\n        s_A = cnn_forward_pass(x, mode_A, kernel)\n        class_A = 1 if s_A = tau else 0\n\n        # 3. Evaluate for condition B\n        s_B = cnn_forward_pass(x, mode_B, kernel)\n        class_B = 1 if s_B = tau else 0\n\n        # 4. Compare results\n        classification_flipped = (class_A != class_B)\n        margin_difference = s_B - s_A\n\n        results.append([classification_flipped, margin_difference])\n\n    # Format the final output string exactly as specified.\n    # [ [bool_1,float_1], [bool_2,float_2], ... ]\n    # We build the string manually to avoid spaces introduced by str(list).\n    inner_strings = []\n    for flip, margin in results:\n        # Python's str(bool) is 'True'/'False'. The problem states \"plain booleans\".\n        # This is the most direct interpretation.\n        inner_strings.append(f\"[{str(flip).lower()},{margin}]\")\n    \n    final_output = f\"[{','.join(inner_strings)}]\"\n    # A quick fix to match the example output format (True/False)\n    final_output = final_output.replace(\"true\", \"True\").replace(\"false\", \"False\")\n    print(final_output)\n\nsolve()\n```", "id": "3126196"}]}