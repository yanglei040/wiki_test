{"hands_on_practices": [{"introduction": "This practice provides a concrete, hands-on introduction to the core motivation behind parameter tying: enforcing desired symmetries in a model's behavior. By implementing and comparing a standard neural network with a parameter-tied version, you will empirically verify how tying parameters across symmetric parts of an architecture can guarantee invariance to specific input transformations, such as swapping coordinates [@problem_id:3161977]. This exercise will build your intuition for how architectural choices can embed prior knowledge directly into a model.", "problem": "Consider a binary classifier implemented as a feedforward neural network with one hidden layer over a two-dimensional input vector $x = (x_1, x_2)$. The hidden layer uses the Rectified Linear Unit (ReLU) activation, where the Rectified Linear Unit (ReLU) is defined as $\\mathrm{ReLU}(z) = \\max(0, z)$. The output layer is a scalar affine combination of hidden activations, and the final class label is determined by thresholding at $0$: the predicted label is $1$ if the scalar output is greater than or equal to $0$ and $0$ otherwise.\n\nYou will implement two variants of this classifier:\n\n- An untied-parameter model with hidden units\n  $$h_1 = \\mathrm{ReLU}(a_1 x_1 + b_1), \\quad h_2 = \\mathrm{ReLU}(a_2 x_2 + b_2),$$\n  and output\n  $$y = v_1 h_1 + v_2 h_2 + c.$$\n\n- A tied-parameter model with hidden units\n  $$h_1 = \\mathrm{ReLU}(a x_1 + b), \\quad h_2 = \\mathrm{ReLU}(a x_2 + b),$$\n  and output\n  $$y = v (h_1 + h_2) + c.$$\n\nParameter tying here means setting parameters equal across structurally symmetric components, for example $a_1 = a_2$, $b_1 = b_2$, and $v_1 = v_2$.\n\nWe define invariance under a transformation $T$ of the input space as follows: for a model $f$ mapping inputs to labels, $f$ is invariant under $T$ on a set $X$ if and only if $f(x) = f(T(x))$ for all $x \\in X$. The two transformations to be tested are:\n- The coordinate-swap transformation $S$ defined by $S(x_1, x_2) = (x_2, x_1)$.\n- The sign-flip transformation $F$ defined by $F(x_1, x_2) = (-x_1, -x_2)$.\n\nStarting from the fundamental base that a feedforward network computes compositions of affine maps and pointwise non-linearities, and the definition of invariance above, construct a program that:\n- Implements both the untied and tied models.\n- Generates synthetic test inputs $x$ by sampling each coordinate independently from the uniform distribution on the interval $[-1, 1]$.\n- For a given transformation $T \\in \\{S, F\\}$, empirically verifies invariance by checking whether all predicted labels satisfy $f(x) = f(T(x))$ over the sampled set.\n\nYour program must use the following test suite, where $N$ is the number of samples, and “seed” specifies the pseudorandom number generator seed:\n\n- Test case $1$ (untied, expected non-invariance under $S$): parameters $a_1 = 1.0$, $b_1 = 0.1$, $a_2 = 2.0$, $b_2 = -0.3$, $v_1 = 1.0$, $v_2 = -0.5$, $c = 0.05$, transformation $T = S$, $N = 200$, seed $= 42$.\n- Test case $2$ (tied, expected invariance under $S$): parameters $a = 1.5$, $b = -0.2$, $v = 0.7$, $c = -0.1$, transformation $T = S$, $N = 200$, seed $= 42$.\n- Test case $3$ (untied but equal, expected invariance under $S$): parameters $a_1 = 0.8$, $b_1 = 0.0$, $a_2 = 0.8$, $b_2 = 0.0$, $v_1 = 1.0$, $v_2 = 1.0$, $c = 0.0$, transformation $T = S$, $N = 200$, seed $= 7$.\n- Test case $4$ (tied, expected non-invariance under $F$): parameters $a = 1.0$, $b = 0.0$, $v = 0.5$, $c = 0.05$, transformation $T = F$, $N = 200$, seed $= 42$.\n- Test case $5$ (tied with zero output weight, expected invariance under $S$ by trivial constancy): parameters $a = 1.0$, $b = 0.5$, $v = 0.0$, $c = 0.3$, transformation $T = S$, $N = 200$, seed $= 123$.\n\nFor each test case, compute a boolean indicating whether the model is invariant under the specified transformation on the sampled set. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\dots]$), where each $result_i$ is the boolean value for test case $i$ in order.", "solution": "The problem requires an analysis and empirical verification of the relationship between parameter tying in a simple neural network and its invariance to specific input transformations. We will first establish the mathematical basis for the expected behavior and then use this framework to verify the outcomes of the prescribed test cases.\n\nA binary classifier is defined by a function $f(x)$ that maps an input vector $x$ to a label in $\\{0, 1\\}$. The classifier is said to be invariant under a transformation $T$ if $f(x) = f(T(x))$ for all inputs $x$ in a given domain. The classification rule is given by the sign of a scalar output function $y$, such that the label is $1$ if $y \\ge 0$ and $0$ otherwise. Therefore, invariance $f(x) = f(T(x))$ holds if and only if the sign of $y(x)$ is the same as the sign of $y(T(x))$, i.e., $(\\,y(x) \\ge 0 \\land y(T(x)) \\ge 0\\,) \\lor (\\,y(x)  0 \\land y(T(x))  0\\,)$. A sufficient, but not necessary, condition for this is $y(x) = y(T(x))$.\n\nThe input vector is $x = (x_1, x_2)$. The activation function is the Rectified Linear Unit, $\\mathrm{ReLU}(z) = \\max(0, z)$.\n\nThe two models are:\n1.  **Untied Model**: The hidden units are $h_1 = \\mathrm{ReLU}(a_1 x_1 + b_1)$ and $h_2 = \\mathrm{ReLU}(a_2 x_2 + b_2)$. The scalar output is $y_{untied}(x_1, x_2) = v_1 h_1 + v_2 h_2 + c$. The structure is asymmetric, processing each input coordinate with distinct parameters.\n\n2.  **Tied Model**: The hidden units are $h_1 = \\mathrm{ReLU}(a x_1 + b)$ and $h_2 = \\mathrm{ReLU}(a x_2 + b)$. The scalar output is $y_{tied}(x_1, x_2) = v (h_1 + h_2) + c$. The parameters $(a, b)$ for the hidden layer and the weight $v$ for the output layer are shared (or \"tied\") across the two input branches.\n\nThe two transformations are:\n1.  **Coordinate-Swap ($S$)**: $S(x_1, x_2) = (x_2, x_1)$.\n2.  **Sign-Flip ($F$)**: $F(x_1, x_2) = (-x_1, -x_2)$.\n\n**Analysis of Invariance under Coordinate-Swap ($S$)**\n\nLet's analyze the output of each model for a swapped input $S(x) = (x_2, x_1)$.\n\n*   **Tied Model**:\n    The output for the original input is:\n    $$y_{tied}(x_1, x_2) = v (\\mathrm{ReLU}(a x_1 + b) + \\mathrm{ReLU}(a x_2 + b)) + c$$\n    The output for the swapped input is:\n    $$y_{tied}(x_2, x_1) = v (\\mathrm{ReLU}(a x_2 + b) + \\mathrm{ReLU}(a x_1 + b)) + c$$\n    Due to the commutative property of addition, $\\mathrm{ReLU}(a x_1 + b) + \\mathrm{ReLU}(a x_2 + b) = \\mathrm{ReLU}(a x_2 + b) + \\mathrm{ReLU}(a x_1 + b)$. Thus, $y_{tied}(x_1, x_2) = y_{tied}(x_2, x_1)$ for all $x$. This implies that the predicted labels will also be identical, $f_{tied}(x) = f_{tied}(S(x))$. The tied model architecture inherently enforces invariance to the coordinate-swap transformation.\n\n*   **Untied Model**:\n    The output for the original input is:\n    $$y_{untied}(x_1, x_2) = v_1 \\mathrm{ReLU}(a_1 x_1 + b_1) + v_2 \\mathrm{ReLU}(a_2 x_2 + b_2) + c$$\n    The output for the swapped input is:\n    $$y_{untied}(x_2, x_1) = v_1 \\mathrm{ReLU}(a_1 x_2 + b_1) + v_2 \\mathrm{ReLU}(a_2 x_1 + b_2) + c$$\n    In general, $y_{untied}(x_1, x_2) \\neq y_{untied}(x_2, x_1)$ unless the parameters possess a specific symmetry. Invariance is achieved if the function is symmetric with respect to swapping $x_1$ and $x_2$. This happens if the terms can be permuted, which requires $a_1=a_2$, $b_1=b_2$, and $v_1=v_2$. These are precisely the constraints imposed by parameter tying.\n\n**Analysis of Invariance under Sign-Flip ($F$)**\n\nLet's analyze the output of the tied model for a sign-flipped input $F(x) = (-x_1, -x_2)$.\nThe output for the original input is:\n$$y_{tied}(x_1, x_2) = v (\\mathrm{ReLU}(a x_1 + b) + \\mathrm{ReLU}(a x_2 + b)) + c$$\nThe output for the flipped input is:\n$$y_{tied}(-x_1, -x_2) = v (\\mathrm{ReLU}(a(-x_1) + b) + \\mathrm{ReLU}(a(-x_2) + b)) + c = v (\\mathrm{ReLU}(-a x_1 + b) + \\mathrm{ReLU}(-a x_2 + b)) + c$$\nFor invariance, we need $y_{tied}(x_1, x_2)$ and $y_{tied}(-x_1, -x_2)$ to have the same sign. In general this is not true. The $\\mathrm{ReLU}$ function is not an even function, i.e., $\\mathrm{ReLU}(z) \\neq \\mathrm{ReLU}(-z)$ except for $z=0$. Therefore, the tied architecture does not inherently enforce invariance to the sign-flip transformation.\n\n**Empirical Verification via Test Cases**\n\nWe will now apply this reasoning to the specific test cases. For each case, we generate $N=200$ samples of $x=(x_1, x_2)$ with coordinates drawn from $U[-1, 1]$ and check if $f(x) = f(T(x))$ for all samples.\n\n*   **Test Case 1**: Untied model with asymmetric parameters ($a_1=1.0, b_1=0.1, a_2=2.0, b_2=-0.3, v_1=1.0, v_2=-0.5, c=0.05$) under transformation $S$. As predicted by our analysis, the lack of parameter symmetry will break the invariance. The result is expected to be `False`.\n\n*   **Test Case 2**: Tied model ($a=1.5, b=-0.2, v=0.7, c=-0.1$) under transformation $S$. As shown analytically, the tied model is invariant to coordinate swapping. The result is expected to be `True`.\n\n*   **Test Case 3**: Untied model with parameters that are manually set to be symmetric ($a_1=0.8, b_1=0.0, a_2=0.8, b_2=0.0, v_1=1.0, v_2=1.0, c=0.0$). This model is functionally equivalent to a tied model: $y_{untied} = \\mathrm{ReLU}(0.8 x_1) + \\mathrm{ReLU}(0.8 x_2)$. Therefore, it will be invariant to transformation $S$. This case highlights that invariance is a property of the computed function, which is determined by the parameter values, not merely the model's structural classification as \"untied\". The result is expected to be `True`.\n\n*   **Test Case 4**: Tied model ($a=1.0, b=0.0, v=0.5, c=0.05$) under transformation $F$. The problem expects non-invariance. Let's analyze the output function:\n    $$y = 0.5 \\cdot (\\mathrm{ReLU}(1.0 \\cdot x_1 + 0.0) + \\mathrm{ReLU}(1.0 \\cdot x_2 + 0.0)) + 0.05 = 0.5 \\cdot (\\mathrm{ReLU}(x_1) + \\mathrm{ReLU}(x_2)) + 0.05$$\n    The input coordinates $x_1, x_2$ are sampled from $[-1, 1]$. The $\\mathrm{ReLU}$ function is non-negative.\n    The minimum value of $\\mathrm{ReLU}(x_1) + \\mathrm{ReLU}(x_2)$ is $0$ (when $x_1 \\le 0$ and $x_2 \\le 0$).\n    In this case, the minimum output is $y_{min} = 0.5 \\cdot 0 + 0.05 = 0.05$.\n    Since the output $y$ is always greater than or equal to $0.05$, it is always greater than the threshold of $0$. This means the predicted label is always $1$, i.e., $f(x) \\equiv 1$ for all inputs in the domain. A constant function is trivially invariant under any transformation, including $F$. Thus, contrary to the problem's informal expectation, the model will be empirically verified as invariant. The result will be `True`.\n\n*   **Test Case 5**: Tied model with a zero output weight ($a=1.0, b=0.5, v=0.0, c=0.3$) under transformation $S$. The output function is:\n    $$y = 0.0 \\cdot (\\mathrm{ReLU}(1.0 \\cdot x_1 + 0.5) + \\mathrm{ReLU}(1.0 \\cdot x_2 + 0.5)) + 0.3 = 0.3$$\n    The output is a constant $0.3$. Since $0.3 \\ge 0$, the predicted label is always $1$. As in the previous case, the classifier is a constant function and is therefore trivially invariant to any transformation. The result is `True`.\n\nThe program will implement these models and checks to produce a list of booleans corresponding to these five cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests untied and tied neural network models for invariance\n    under specified transformations.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"model\": \"untied\",\n            \"params\": {\"a1\": 1.0, \"b1\": 0.1, \"a2\": 2.0, \"b2\": -0.3, \"v1\": 1.0, \"v2\": -0.5, \"c\": 0.05},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 42\n        },\n        {\n            \"model\": \"tied\",\n            \"params\": {\"a\": 1.5, \"b\": -0.2, \"v\": 0.7, \"c\": -0.1},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 42\n        },\n        {\n            \"model\": \"untied\",\n            \"params\": {\"a1\": 0.8, \"b1\": 0.0, \"a2\": 0.8, \"b2\": 0.0, \"v1\": 1.0, \"v2\": 1.0, \"c\": 0.0},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 7\n        },\n        {\n            \"model\": \"tied\",\n            \"params\": {\"a\": 1.0, \"b\": 0.0, \"v\": 0.5, \"c\": 0.05},\n            \"transformation\": \"F\", \"N\": 200, \"seed\": 42\n        },\n        {\n            \"model\": \"tied\",\n            \"params\": {\"a\": 1.0, \"b\": 0.5, \"v\": 0.0, \"c\": 0.3},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 123\n        }\n    ]\n\n    def relu(z):\n        \"\"\"Rectified Linear Unit activation function.\"\"\"\n        return np.maximum(0, z)\n\n    def untied_model(X, params):\n        \"\"\"Computes labels for the untied-parameter model.\"\"\"\n        x1, x2 = X[:, 0], X[:, 1]\n        p = params\n        h1 = relu(p[\"a1\"] * x1 + p[\"b1\"])\n        h2 = relu(p[\"a2\"] * x2 + p[\"b2\"])\n        y = p[\"v1\"] * h1 + p[\"v2\"] * h2 + p[\"c\"]\n        return (y = 0).astype(int)\n\n    def tied_model(X, params):\n        \"\"\"Computes labels for the tied-parameter model.\"\"\"\n        x1, x2 = X[:, 0], X[:, 1]\n        p = params\n        h1 = relu(p[\"a\"] * x1 + p[\"b\"])\n        h2 = relu(p[\"a\"] * x2 + p[\"b\"])\n        y = p[\"v\"] * (h1 + h2) + p[\"c\"]\n        return (y = 0).astype(int)\n\n    model_funcs = {\n        \"untied\": untied_model,\n        \"tied\": tied_model\n    }\n\n    transform_funcs = {\n        \"S\": lambda X: X[:, ::-1],  # Coordinate-swap\n        \"F\": lambda X: -X           # Sign-flip\n    }\n\n    results = []\n    for case in test_cases:\n        # 1. Set seed and generate data\n        np.random.seed(case[\"seed\"])\n        X_original = np.random.uniform(-1, 1, size=(case[\"N\"], 2))\n\n        # 2. Apply transformation\n        transform_func = transform_funcs[case[\"transformation\"]]\n        X_transformed = transform_func(X_original)\n\n        # 3. Select model and compute labels\n        model_func = model_funcs[case[\"model\"]]\n        labels_original = model_func(X_original, case[\"params\"])\n        labels_transformed = model_func(X_transformed, case[\"params\"])\n\n        # 4. Check for invariance\n        is_invariant = np.array_equal(labels_original, labels_transformed)\n        results.append(is_invariant)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3161977"}, {"introduction": "Moving beyond a simple demonstration, this exercise delves into a more formal and powerful method for implementing parameter tying using linear algebra. You will construct \"tying matrices\" that explicitly define symmetry constraints (odd and even) on a one-dimensional filter, a technique fundamental to signal processing and computer vision [@problem_id:3161902]. By training these symmetric filters to detect specific patterns like edges and blobs, you will discover the direct link between a filter's enforced symmetry and its functional specialization.", "problem": "You will implement parameter tying for one-dimensional discrete filters to enforce odd and even symmetries, train the tied filters by linear least squares on small synthetic datasets, and evaluate their behavior on edge and blob patterns. The task is framed in purely mathematical terms.\n\nFundamental base and definitions to use:\n- A discrete linear filter of length $L$ is represented as a vector $K \\in \\mathbb{R}^{L}$. The response of $K$ to a discrete window $s \\in \\mathbb{R}^{L}$ is the inner product $r = \\sum_{j=0}^{L-1} K[j]\\, s[j]$.\n- For an odd-length kernel, define a centered index set $\\{i\\}_{i=-c}^{c}$ where $c = \\frac{L-1}{2}$ and the mapping between index $i$ and array position $j$ is $j = i + c$.\n- Odd symmetry constraint: $K[i] = -K[-i]$ for all $i$, which implies $K[0] = 0$.\n- Even symmetry constraint: $K[i] = K[-i]$ for all $i$, where $K[0]$ is an unconstrained parameter.\n- Parameter tying expresses $K$ as $K = T \\theta$, where $T \\in \\mathbb{R}^{L \\times d}$ is a fixed matrix encoding the symmetry and $\\theta \\in \\mathbb{R}^{d}$ is the vector of free parameters.\n- Discrete windows to be used in this problem are defined on the centered index set $i \\in \\{-c,\\ldots,0,\\ldots,c\\}$ as follows:\n  1. Rising step of amplitude $a$: $s_{\\text{rise}}(i;a) = 0$ if $i  0$ and $s_{\\text{rise}}(i;a) = a$ if $i \\ge 0$.\n  2. Falling step of amplitude $a$: $s_{\\text{fall}}(i;a) = a$ if $i  0$ and $s_{\\text{fall}}(i;a) = 0$ if $i \\ge 0$.\n  3. Gaussian blob of amplitude $a$ and width $\\sigma$: $s_{\\text{blob}}(i;a,\\sigma) = a \\exp\\!\\big(-\\frac{i^2}{2 \\sigma^2}\\big)$.\n  4. Constant window: $s_{\\text{const}}(i) = 1$ for all $i$.\n\nProgram requirements:\n1. Use kernel length $L = 9$ (so $c = 4$). Construct two tying matrices:\n   - $T_{\\text{odd}} \\in \\mathbb{R}^{L \\times d_{\\text{odd}}}$ that enforces $K[i] = -K[-i]$ by pairing indices $i$ and $-i$ for $i \\in \\{1,2,3,4\\}$, and forcing $K[0] = 0$. Each free parameter controls a pair with weights $+1$ at $i$ and $-1$ at $-i$. This yields $d_{\\text{odd}} = 4$.\n   - $T_{\\text{even}} \\in \\mathbb{R}^{L \\times d_{\\text{even}}}$ that enforces $K[i] = K[-i]$ by pairing indices $i$ and $-i$ for $i \\in \\{1,2,3,4\\}$, plus one free parameter for the center. Each pair has weights $+1$ at both $i$ and $-i$, and the center column has $+1$ at $i=0$. This yields $d_{\\text{even}} = 5$.\n2. Train an odd-symmetric filter by solving a linear least squares problem with tying. Construct a design matrix $X_{\\text{odd}} \\in \\mathbb{R}^{N_{\\text{odd}} \\times L}$ whose rows are the training windows, and a target vector $y_{\\text{odd}} \\in \\mathbb{R}^{N_{\\text{odd}}}$. Minimize $\\|X_{\\text{odd}} T_{\\text{odd}} \\theta_{\\text{odd}} - y_{\\text{odd}}\\|_2^2$ to obtain $\\theta_{\\text{odd}}$ and $K_{\\text{odd}} = T_{\\text{odd}} \\theta_{\\text{odd}}$. Use the following training set:\n   - Rising step with amplitude $a \\in \\{1.0, 0.5\\}$ and label $+a$.\n   - Falling step with amplitude $a \\in \\{1.0, 0.5\\}$ and label $-a$.\n   Thus $N_{\\text{odd}} = 4$.\n3. Train an even-symmetric filter similarly by minimizing $\\|X_{\\text{even}} T_{\\text{even}} \\theta_{\\text{even}} - y_{\\text{even}}\\|_2^2$ to obtain $K_{\\text{even}}$. Use the following training set of Gaussian blobs:\n   - Widths $\\sigma \\in \\{0.8, 1.2, 1.8\\}$ and amplitudes $a \\in \\{1.0, -0.8\\}$ (all combinations), with label equal to $a$. Thus $N_{\\text{even}} = 6$.\n4. After training, normalize each kernel to unit Euclidean norm: replace $K$ by $K / \\|K\\|_2$ for both $K_{\\text{odd}}$ and $K_{\\text{even}}$.\n5. Build the following test windows (all of length $L = 9$):\n   - Rising step with amplitude $1.0$.\n   - Gaussian blob with amplitude $1.0$ and $\\sigma = 1.2$.\n   - Constant window.\n   - Falling step with amplitude $1.0$.\n6. Compute the following quantities:\n   - The number of free parameters $d_{\\text{odd}}$ and $d_{\\text{even}}$.\n   - Boolean $b_1$: whether the absolute response magnitude of the odd-symmetric filter on the rising step is strictly greater than that of the even-symmetric filter on the same rising step.\n   - Boolean $b_2$: whether the absolute response magnitude of the even-symmetric filter on the Gaussian blob is strictly greater than that of the odd-symmetric filter on the same blob.\n   - Float $f$: the response of the odd-symmetric filter on the constant window, rounded to within an absolute tolerance of $\\varepsilon = 10^{-12}$ (if the magnitude is less than $\\varepsilon$, output exactly $0.0$).\n   - Boolean $b_3$: whether the odd-symmetric filter responses on the unit-amplitude rising and falling steps are negatives of each other to within the same absolute tolerance $\\varepsilon = 10^{-12}$.\n7. Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order\n   $[d_{\\text{odd}}, d_{\\text{even}}, b_1, b_2, f, b_3]$,\n   where $d_{\\text{odd}}$ and $d_{\\text{even}}$ are integers, $b_1$ and $b_2$ and $b_3$ are booleans, and $f$ is a float.\n\nAll mathematical entities and numbers in this problem are defined with their standard meanings in discrete linear algebra and elementary functions. No physical units are involved, and no angles are used. The answer values are fully determined by the definitions above, so no user input is required.", "solution": "We begin from the core definitions of discrete linear systems, least squares, and parameter tying. A length-$L$ filter $K \\in \\mathbb{R}^{L}$ produces response $r = \\sum_{j=0}^{L-1} K[j] s[j] = s^{\\top} K$ on a window $s \\in \\mathbb{R}^{L}$. For an odd length $L = 9$, we center the index at $i \\in \\{-4,-3,-2,-1,0,1,2,3,4\\}$, with array position $j = i + 4$.\n\nParameter tying expresses the symmetry constraints as linear maps. For odd symmetry, $K[i] = -K[-i]$ for all $i$, hence $K[0] = 0$. The free parameters correspond to $i \\in \\{1,2,3,4\\}$, and the full kernel is a linear combination of columns that place $+1$ at $i$ and $-1$ at $-i$. This defines a matrix $T_{\\text{odd}} \\in \\mathbb{R}^{9 \\times 4}$ such that $K_{\\text{odd}} = T_{\\text{odd}} \\theta_{\\text{odd}}$. For even symmetry, $K[i] = K[-i]$ for all $i$ and $K[0]$ is free, so the free parameters are the center and the $4$ index pairs, for a total of $d_{\\text{even}} = 5$. Each pair column has $+1$ at $i$ and $+1$ at $-i$, and the center column has $+1$ at $i=0$, forming $T_{\\text{even}} \\in \\mathbb{R}^{9 \\times 5}$.\n\nTraining under tying uses least squares. For a set of training windows assembled into a design matrix $X \\in \\mathbb{R}^{N \\times L}$ and target vector $y \\in \\mathbb{R}^{N}$, the tied model predicts $X K = X T \\theta$. The optimal parameters minimize the quadratic objective\n$$\n\\min_{\\theta \\in \\mathbb{R}^{d}} \\| X T \\theta - y \\|_2^2.\n$$\nThis is a standard linear least squares problem with solution given by the normal equations\n$$\n(T^{\\top} X^{\\top} X T)\\, \\theta^{\\star} = T^{\\top} X^{\\top} y,\n$$\nwhenever $T^{\\top} X^{\\top} X T$ is invertible; otherwise a minimum-norm solution is given by the Moore–Penrose pseudoinverse. Computationally, this can be solved stably using a least squares routine applied to $X T$.\n\nWe now specify the training sets deterministically:\n\n1. Odd-symmetric filter training set ($N_{\\text{odd}} = 4$). For amplitudes $a \\in \\{1.0, 0.5\\}$:\n   - Include rising step $s_{\\text{rise}}(i;a)$ with label $+a$.\n   - Include falling step $s_{\\text{fall}}(i;a)$ with label $-a$.\n   The objective prefers responses $r = s^{\\top} K_{\\text{odd}}$ to match the signed amplitude for edges. Due to odd symmetry and the step structure, any constant $c$ satisfying $\\sum_{i0} K_{\\text{odd}}[i] = 1$ makes both rising and falling constraints consistent, and the minimum-norm odd-symmetric solution distributes weight evenly across positive indices, with negatives on the negative indices, and zero at the center.\n\n2. Even-symmetric filter training set ($N_{\\text{even}} = 6$). For widths $\\sigma \\in \\{0.8, 1.2, 1.8\\}$ and amplitudes $a \\in \\{1.0, -0.8\\}$:\n   - Include Gaussian blobs $s_{\\text{blob}}(i; a, \\sigma)$ with label $a$.\n   By symmetry, the even-symmetric kernel will align with the blob structure and learn a shape similar to a symmetric smoothing profile.\n\nAfter obtaining $K_{\\text{odd}}$ and $K_{\\text{even}}$, normalize them to unit Euclidean norm:\n$$\nK \\leftarrow \\frac{K}{\\|K\\|_2}.\n$$\n\nWe evaluate on the following test windows:\n- Rising step with amplitude $1.0$.\n- Gaussian blob with amplitude $1.0$ and $\\sigma = 1.2$.\n- Constant window of ones.\n- Falling step with amplitude $1.0$.\n\nKey analytical properties:\n- For any odd-symmetric kernel $K_{\\text{odd}}$ with $K_{\\text{odd}}[0] = 0$, the response on the constant window is exactly zero. This follows because for each $i0$, the pair contribution $K_{\\text{odd}}[i] \\cdot 1 + K_{\\text{odd}}[-i] \\cdot 1 = K_{\\text{odd}}[i] + K_{\\text{odd}}[-i] = 0$, and the center contributes $0$.\n- For odd-symmetric $K_{\\text{odd}}$, the responses on unit-amplitude rising and falling steps are negatives of each other. Indeed, the rising step picks out $\\sum_{i \\ge 0} K_{\\text{odd}}[i] = \\sum_{i0} K_{\\text{odd}}[i]$, while the falling step picks out $\\sum_{i  0} K_{\\text{odd}}[i] = -\\sum_{i0} K_{\\text{odd}}[i]$.\n- For an even-symmetric kernel trained on blobs, the response on a symmetric blob is typically large in magnitude, whereas an odd-symmetric kernel produces exactly zero on any symmetric blob centered at the origin, since for each $i0$, $K_{\\text{odd}}[i] s[i] + K_{\\text{odd}}[-i] s[-i] = s[i]\\,(K_{\\text{odd}}[i] + K_{\\text{odd}}[-i]) = 0$ and $K_{\\text{odd}}[0] s[0] = 0$.\n\nThese facts, combined with unit-norm normalization, support the inequalities to be tested:\n- The odd-symmetric filter responds more strongly to a step edge than the even-symmetric filter trained on blobs, so the absolute response comparison on the rising step should be true.\n- The even-symmetric filter responds more strongly to a Gaussian blob than the odd-symmetric filter, which is exactly zero on a centered symmetric blob, so the absolute response comparison on the blob should be true.\n- The odd-symmetric response on the constant window is zero, which we output as a float rounded within an absolute tolerance $\\varepsilon = 10^{-12}$.\n- The odd-symmetric responses on rising and falling steps are negatives, which we test within the same tolerance.\n\nFinally, we report the number of free parameters $d_{\\text{odd}} = 4$ and $d_{\\text{even}} = 5$, the two booleans comparing response magnitudes, the float response on the constant window, and the boolean for the sign inversion property, in the order\n$$\n[d_{\\text{odd}}, d_{\\text{even}}, b_1, b_2, f, b_3].\n$$\nThis produces a single line of output aggregating all test results as required.", "answer": "```python\nimport numpy as np\n\ndef build_tying_matrices(L: int):\n    \"\"\"\n    Build tying matrices for odd and even symmetry for a 1D kernel of length L (odd).\n    Returns:\n        T_odd: shape (L, d_odd)\n        T_even: shape (L, d_even)\n    \"\"\"\n    assert L % 2 == 1, \"L must be odd\"\n    c = L // 2\n    # Odd symmetry: K[i] = -K[-i], K[0] = 0. Free params for i=1..c.\n    odd_cols = []\n    for i in range(1, c + 1):\n        col = np.zeros(L, dtype=float)\n        col[c + i] = 1.0   # +1 at +i\n        col[c - i] = -1.0  # -1 at -i\n        odd_cols.append(col)\n    T_odd = np.column_stack(odd_cols) if odd_cols else np.zeros((L, 0))\n\n    # Even symmetry: K[i] = K[-i], center is free. Free params: center + i=1..c.\n    even_cols = []\n    # Center parameter\n    col0 = np.zeros(L, dtype=float)\n    col0[c] = 1.0\n    even_cols.append(col0)\n    for i in range(1, c + 1):\n        col = np.zeros(L, dtype=float)\n        col[c + i] = 1.0  # +1 at +i\n        col[c - i] = 1.0  # +1 at -i\n        even_cols.append(col)\n    T_even = np.column_stack(even_cols)\n    return T_odd, T_even\n\ndef make_indices(L: int):\n    c = L // 2\n    return np.arange(-c, c + 1, dtype=float)\n\ndef rising_step_window(L: int, a: float):\n    i = make_indices(L)\n    w = np.where(i  0, 0.0, a)\n    return w\n\ndef falling_step_window(L: int, a: float):\n    i = make_indices(L)\n    w = np.where(i  0, a, 0.0)\n    return w\n\ndef gaussian_blob_window(L: int, a: float, sigma: float):\n    i = make_indices(L)\n    w = a * np.exp(-(i ** 2) / (2.0 * (sigma ** 2)))\n    return w\n\ndef constant_window(L: int):\n    return np.ones(L, dtype=float)\n\ndef fit_tied_least_squares(T: np.ndarray, windows: list, targets: list):\n    \"\"\"\n    Solve min_theta || X T theta - y ||^2\n    Returns K = T theta_hat\n    \"\"\"\n    X = np.vstack(windows)  # shape (N, L)\n    y = np.asarray(targets, dtype=float)  # shape (N,)\n    XR = X @ T  # Reduced design matrix, shape (N, d)\n    # Solve least squares for theta\n    theta_hat, *_ = np.linalg.lstsq(XR, y, rcond=None)\n    K = T @ theta_hat\n    return K\n\ndef normalize_kernel(K: np.ndarray):\n    norm = np.linalg.norm(K)\n    if norm == 0.0:\n        return K.copy()\n    return K / norm\n\ndef solve():\n    L = 9\n    eps = 1e-12\n\n    # Build tying matrices\n    T_odd, T_even = build_tying_matrices(L)\n    d_odd = T_odd.shape[1]\n    d_even = T_even.shape[1]\n\n    # Training data for odd-symmetric filter (edges)\n    windows_odd = []\n    targets_odd = []\n    for a in [1.0, 0.5]:\n        windows_odd.append(rising_step_window(L, a))\n        targets_odd.append(+a)\n        windows_odd.append(falling_step_window(L, a))\n        targets_odd.append(-a)\n\n    # Training data for even-symmetric filter (blobs)\n    windows_even = []\n    targets_even = []\n    for sigma in [0.8, 1.2, 1.8]:\n        for a in [1.0, -0.8]:\n            windows_even.append(gaussian_blob_window(L, a, sigma))\n            targets_even.append(a)\n\n    # Fit tied least squares\n    K_odd = fit_tied_least_squares(T_odd, windows_odd, targets_odd)\n    K_even = fit_tied_least_squares(T_even, windows_even, targets_even)\n\n    # Normalize kernels\n    K_odd = normalize_kernel(K_odd)\n    K_even = normalize_kernel(K_even)\n\n    # Test windows\n    w_edge_rise = rising_step_window(L, 1.0)\n    w_edge_fall = falling_step_window(L, 1.0)\n    w_blob = gaussian_blob_window(L, 1.0, 1.2)\n    w_const = constant_window(L)\n\n    # Responses\n    r_odd_edge = float(w_edge_rise @ K_odd)\n    r_even_edge = float(w_edge_rise @ K_even)\n    r_even_blob = float(w_blob @ K_even)\n    r_odd_blob = float(w_blob @ K_odd)\n    r_odd_const = float(w_const @ K_odd)\n    r_odd_fall = float(w_edge_fall @ K_odd)\n\n    # Tests\n    b1 = abs(r_odd_edge)  abs(r_even_edge)\n    b2 = abs(r_even_blob)  abs(r_odd_blob)\n    f = 0.0 if abs(r_odd_const)  eps else float(r_odd_const)\n    b3 = abs(r_odd_edge + r_odd_fall) = eps\n\n    results = [d_odd, d_even, b1, b2, f, b3]\n\n    # Final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3161902"}, {"introduction": "In many practical scenarios, enforcing exact parameter equality (hard tying) can be too restrictive. This practice explores \"soft\" parameter tying, a flexible alternative that uses regularization to encourage, rather than force, parameters to be similar [@problem_id:3161931]. You will implement and compare different regularization penalties, such as the $L_1$ and $L_2$ norms, on the difference between parameters, and in doing so, you will analyze the crucial trade-off between model compression and predictive accuracy.", "problem": "You are to implement and analyze soft parameter tying via regularization on the difference of parameter vectors in a simple binary classifier. The problem explores how different norms affect both compression (how many parameters become effectively shared) and predictive accuracy. The implementation must be a complete, runnable program.\n\nThe foundational base for this problem is Empirical Risk Minimization (ERM), Binary Cross-Entropy (BCE), gradient-based optimization, and convex norm regularization. Let $\\sigma(u) = \\frac{1}{1 + e^{-u}}$ denote the logistic sigmoid function. BCE for a single example $(x, y)$ with predicted probability $p(x)$ is $-\\left(y \\log p(x) + (1-y)\\log(1-p(x))\\right)$, and ERM approximates expected risk by the sample average.\n\nModel specification:\n- Inputs are vectors $x \\in \\mathbb{R}^d$ and labels are $y \\in \\{0,1\\}$.\n- The model maintains two weight vectors $w^{(a)}, w^{(b)} \\in \\mathbb{R}^d$.\n- The prediction is $p(x) = \\sigma\\!\\left(\\frac{1}{2}\\left(w^{(a)\\top}x + w^{(b)\\top}x\\right)\\right)$.\n- The training objective is the average Binary Cross-Entropy (BCE) over the training set plus a soft-tying penalty applied to the parameter difference $d = w^{(a)} - w^{(b)}$.\n\nSoft-tying penalties to compare:\n- $L_1$ penalty: $\\lambda_1 \\|d\\|_1$.\n- $L_2$ penalty (squared): $\\lambda_2 \\|d\\|_2^2$.\n- Elastic net penalty: $\\lambda_1 \\|d\\|_1 + \\lambda_2 \\|d\\|_2^2$.\n\nData generation (deterministic):\n- Use pseudo-random seed $0$ for all randomness.\n- Dimension $d = 10$.\n- Training set size $n_{\\text{train}} = 512$ and validation set size $n_{\\text{val}} = 256$.\n- Draw $w^\\star \\sim \\mathcal{N}(0, I_d)$ once and fix it for both sets.\n- Draw inputs $x \\sim \\mathcal{N}(0, I_d)$ independently for all samples.\n- Draw noise $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ with $\\sigma^2 = 0.25$ independently for all samples.\n- Define labels as $y = \\mathbf{1}\\{w^{\\star\\top} x + \\epsilon  0\\}$.\n\nTraining procedure:\n- Initialize $w^{(a)} = 0$ and $w^{(b)} = 0$ in $\\mathbb{R}^d$.\n- Optimize the objective using full-batch gradient descent for $T = 400$ iterations with step size $\\eta = 0.05$.\n- For the $L_1$ penalty, use a subgradient $\\text{sign}(d)$.\n- For the $L_2$ penalty (squared), use gradient $2\\lambda_2 d$.\n- For elastic net, sum the respective gradients.\n\nEvaluation metrics:\n- Compression ratio: the fraction of indices $i \\in \\{1,\\dots,d\\}$ such that $|w^{(a)}_i - w^{(b)}_i| \\le \\tau$, with tolerance $\\tau = 10^{-2}$.\n- Accuracy: on the validation set, the fraction of correctly classified examples using the decision rule $\\hat{y} = \\mathbf{1}\\{p(x) \\ge 0.5\\}$.\n\nTest suite:\nImplement and run the following $5$ test cases, each defined by $(\\lambda_1, \\lambda_2)$ and the penalty type:\n1. Baseline, no tying: $\\lambda_1 = 0$, $\\lambda_2 = 0$.\n2. $L_2$ moderate: $\\lambda_1 = 0$, $\\lambda_2 = 0.1$.\n3. $L_1$ moderate: $\\lambda_1 = 0.05$, $\\lambda_2 = 0$.\n4. $L_2$ strong: $\\lambda_1 = 0$, $\\lambda_2 = 1.0$.\n5. Elastic net balanced: $\\lambda_1 = 0.05$, $\\lambda_2 = 0.2$.\n\nYour program should implement the above setup and produce, for each test case in the given order, a pair $[\\text{compression\\_ratio}, \\text{accuracy}]$ with both values as floating-point numbers. The final output format must be a single line containing a list of these $5$ pairs, in order, for example:\n$[[c_1,a_1],[c_2,a_2],[c_3,a_3],[c_4,a_4],[c_5,a_5]]$,\nwith each $c_i$ and $a_i$ being floating-point numbers rounded to $4$ decimal places.", "solution": "The problem statement is first validated against the criteria of scientific soundness, well-posedness, and objectivity.\n\n### Step 1: Extract Givens\n- **Model**: Binary classifier with two weight vectors $w^{(a)}, w^{(b)} \\in \\mathbb{R}^d$.\n- **Prediction function**: $p(x) = \\sigma\\!\\left(\\frac{1}{2}\\left(w^{(a)\\top}x + w^{(b)\\top}x\\right)\\right)$, where $\\sigma(u) = (1 + e^{-u})^{-1}$ is the logistic sigmoid function.\n- **Loss function**: Average Binary Cross-Entropy (BCE) over the training set, where for a single sample $(x, y)$, the loss is $-\\left(y \\log p(x) + (1-y)\\log(1-p(x))\\right)$.\n- **Regularization**: A penalty term is applied to the difference vector $d = w^{(a)} - w^{(b)}$.\n    - $L_1$ penalty: $\\lambda_1 \\|d\\|_1$.\n    - $L_2$ squared penalty: $\\lambda_2 \\|d\\|_2^2$.\n    - Elastic net penalty: $\\lambda_1 \\|d\\|_1 + \\lambda_2 \\|d\\|_2^2$.\n- **Data Generation**:\n    - Pseudo-random seed: $0$.\n    - Dimension $d = 10$.\n    - Training set size $n_{\\text{train}} = 512$.\n    - Validation set size $n_{\\text{val}} = 256$.\n    - True weights $w^\\star \\sim \\mathcal{N}(0, I_d)$.\n    - Inputs $x \\sim \\mathcal{N}(0, I_d)$.\n    - Noise $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ with noise variance $\\sigma^2 = 0.25$.\n    - Labels $y = \\mathbf{1}\\{w^{\\star\\top} x + \\epsilon  0\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n- **Optimization**:\n    - Algorithm: Full-batch gradient descent.\n    - Initialization: $w^{(a)} = 0$, $w^{(b)} = 0$.\n    - Iterations: $T = 400$.\n    - Step size: $\\eta = 0.05$.\n    - Gradients for penalties: subgradient $\\text{sign}(d)$ for $L_1$, and gradient $2\\lambda_2 d$ for squared $L_2$.\n- **Evaluation**:\n    - **Compression ratio**: Fraction of indices $i$ where $|w^{(a)}_i - w^{(b)}_i| \\le \\tau$, with tolerance $\\tau = 10^{-2}$.\n    - **Accuracy**: Fraction of correct predictions on the validation set, with decision rule $\\hat{y} = \\mathbf{1}\\{p(x) \\ge 0.5\\}$.\n- **Test Suite**: $5$ cases with $(\\lambda_1, \\lambda_2)$ values: (1) $(0, 0)$, (2) $(0, 0.1)$, (3) $(0.05, 0)$, (4) $(0, 1.0)$, (5) $(0.05, 0.2)$.\n- **Output Format**: A list of $5$ pairs $[\\text{compression\\_ratio}, \\text{accuracy}]$, rounded to $4$ decimal places.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, being based on standard principles of machine learning, including empirical risk minimization, logistic regression, gradient-based optimization, and regularization techniques ($L_1$, $L_2$, elastic net). The concept of parameter tying is a well-established practice in neural network design.\n\nThe problem is well-posed. The objective function is the sum of the average BCE loss (a convex function of the model parameters) and a convex regularization penalty. The resulting objective is convex, and its minimization via gradient descent is a standard and stable numerical problem. All parameters, data generation procedures, and evaluation metrics are specified precisely and unambiguously, ensuring a unique and meaningful solution can be computed.\n\nThe problem is objective, complete, and consistent. It is described in formal mathematical language, free from subjective statements. All necessary information for implementation is provided, and there are no internal contradictions.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be developed.\n\n### Solution Derivation\n\nThe objective is to train a model by minimizing a composite objective function $J(w^{(a)}, w^{(b)})$ using gradient descent. This objective combines a data-fitting term (average BCE loss) and a regularization term that promotes parameter tying.\n\n**Objective Function**\n\nLet the training data be $\\{(x_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$. The model's prediction for a sample $x_i$ is $p_i = p(x_i) = \\sigma(z_i)$, where the logit is $z_i = \\frac{1}{2}(w^{(a)\\top}x_i + w^{(b)\\top}x_i)$. The total objective function is:\n$$\nJ(w^{(a)}, w^{(b)}) = J_{\\text{BCE}}(w^{(a)}, w^{(b)}) + R(w^{(a)} - w^{(b)})\n$$\nThe first term is the average BCE loss:\n$$\nJ_{\\text{BCE}}(w^{(a)}, w^{(b)}) = -\\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]\n$$\nThe second term is the general form of the soft-tying penalty on the difference vector $d = w^{(a)} - w^{(b)}$:\n$$\nR(d) = \\lambda_1 \\|d\\|_1 + \\lambda_2 \\|d\\|_2^2\n$$\n\n**Gradient Calculation**\nTo implement gradient descent, we need the gradients of $J$ with respect to $w^{(a)}$ and $w^{(b)}$. We compute these using the chain rule.\n\nThe gradient of the BCE loss for a single sample $L_i$ with respect to the logit $z_i$ is well-known:\n$$\n\\frac{\\partial L_i}{\\partial z_i} = p_i - y_i\n$$\nThe gradient of the logit $z_i$ with respect to the weight vectors is:\n$$\n\\nabla_{w^{(a)}} z_i = \\frac{1}{2}x_i \\quad \\text{and} \\quad \\nabla_{w^{(b)}} z_i = \\frac{1}{2}x_i\n$$\nApplying the chain rule for the BCE term over the entire training set:\n$$\n\\nabla_{w^{(a)}} J_{\\text{BCE}} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} \\frac{\\partial L_i}{\\partial z_i} \\nabla_{w^{(a)}} z_i = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} (p_i - y_i) \\frac{1}{2}x_i = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y)\n$$\nBy symmetry, the gradient with respect to $w^{(b)}$ is identical:\n$$\n\\nabla_{w^{(b)}} J_{\\text{BCE}} = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y)\n$$\nwhere $X$ is the $n_{\\text{train}} \\times d$ data matrix, $P$ is the vector of predictions, and $Y$ is the vector of true labels.\n\nNext, we find the gradient of the regularization term $R(d) = R(w^{(a)} - w^{(b)})$.\n$$\n\\nabla_{w^{(a)}} R(d) = \\nabla_{d} R(d) \\cdot \\frac{\\partial d}{\\partial w^{(a)}} = \\nabla_{d} R(d)\n$$\n$$\n\\nabla_{w^{(b)}} R(d) = \\nabla_{d} R(d) \\cdot \\frac{\\partial d}{\\partial w^{(b)}} = -\\nabla_{d} R(d)\n$$\nThe (sub)gradient $\\nabla_{d} R(d)$ is given by:\n$$\n\\nabla_{d} R(d) = \\lambda_1 \\text{sign}(d) + 2\\lambda_2 d\n$$\nHere, $\\text{sign}(d)$ is the subgradient of the $L_1$ norm. For algorithmic implementation, $\\text{sign}(0)$ can be taken as $0$.\n\nCombining these terms, the full gradients are:\n$$\n\\nabla_{w^{(a)}} J = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y) + (\\lambda_1 \\text{sign}(d) + 2\\lambda_2 d)\n$$\n$$\n\\nabla_{w^{(b)}} J = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y) - (\\lambda_1 \\text{sign}(d) + 2\\lambda_2 d)\n$$\n\n**Optimization Algorithm**\nThe weights are updated using full-batch gradient descent for $T=400$ iterations with learning rate $\\eta=0.05$. Starting from $w^{(a)}_0 = 0$ and $w^{(b)}_0 = 0$, the update rules at step $t$ are:\n$$\nw^{(a)}_{t+1} = w^{(a)}_t - \\eta \\nabla_{w^{(a)}} J(w^{(a)}_t, w^{(b)}_t)\n$$\n$$\nw^{(b)}_{t+1} = w^{(b)}_t - \\eta \\nabla_{w^{(b)}} J(w^{(a)}_t, w^{(b)}_t)\n$$\nThis process is repeated for each of the $5$ test cases defined by the $(\\lambda_1, \\lambda_2)$ pairs.\n\n**Evaluation**\nAfter training, two metrics are computed:\n1.  **Compression Ratio**: This measures the degree of parameter sharing. It is the fraction of dimensions for which the absolute difference between corresponding weights is below a tolerance $\\tau = 10^{-2}$.\n    $$\n    \\text{Compression Ratio} = \\frac{1}{d} \\sum_{i=1}^d \\mathbf{1}\\{|w^{(a)}_i - w^{(b)}_i| \\le \\tau\\}\n    $$\n2.  **Accuracy**: This measures predictive performance on the unseen validation set. The predicted label $\\hat{y}$ is $1$ if the predicted probability $p(x) \\ge 0.5$, and $0$ otherwise. This is equivalent to checking if the logit $z = \\frac{1}{2}(w^{(a)\\top}x + w^{(b)\\top}x) \\ge 0$.\n    $$\n    \\text{Accuracy} = \\frac{1}{n_{\\text{val}}} \\sum_{i=1}^{n_{\\text{val}}} \\mathbf{1}\\{\\hat{y}_i = y_i^{\\text{val}}\\}\n    $$\nThe implementation will follow these derivations to produce the required output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes soft parameter tying in a binary classifier.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    SEED = 0\n    D = 10  # Dimension of input vectors\n    N_TRAIN = 512\n    N_VAL = 256\n    NOISE_VAR = 0.25\n    \n    T = 400  # Number of iterations\n    ETA = 0.05  # Step size (learning rate)\n    TAU = 1e-2  # Tolerance for compression ratio\n\n    # --- Test Cases ---\n    test_cases = [\n        # (lambda_1, lambda_2), penalty_type\n        (0.0, 0.0),      # 1. Baseline, no tying\n        (0.0, 0.1),      # 2. L2 moderate\n        (0.05, 0.0),     # 3. L1 moderate\n        (0.0, 1.0),      # 4. L2 strong\n        (0.05, 0.2),     # 5. Elastic net balanced\n    ]\n\n    # --- Helper Functions ---\n    def sigmoid(u):\n        \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-u))\n\n    def generate_data(n_samples, d, w_star, noise_std, rng):\n        \"\"\"Generates synthetic data for binary classification.\"\"\"\n        X = rng.standard_normal(size=(n_samples, d))\n        epsilon = rng.normal(0, noise_std, size=n_samples)\n        logits = X @ w_star + epsilon\n        y = (logits  0).astype(int)\n        return X, y\n\n    # --- Data Generation ---\n    rng = np.random.default_rng(SEED)\n    w_star = rng.standard_normal(size=D)\n    noise_std = np.sqrt(NOISE_VAR)\n    \n    X_train, y_train = generate_data(N_TRAIN, D, w_star, noise_std, rng)\n    X_val, y_val = generate_data(N_VAL, D, w_star, noise_std, rng)\n\n    results = []\n\n    # --- Main Loop: Train and Evaluate for each test case ---\n    for lambda_1, lambda_2 in test_cases:\n        # Initialize weights\n        w_a = np.zeros(D)\n        w_b = np.zeros(D)\n\n        # Full-batch gradient descent\n        for _ in range(T):\n            # Forward pass\n            logits = 0.5 * (X_train @ w_a + X_train @ w_b)\n            predictions = sigmoid(logits)\n            \n            # --- Gradient Calculation ---\n            # Gradient of BCE loss term\n            error = predictions - y_train\n            grad_bce = (1 / (2 * N_TRAIN)) * X_train.T @ error\n\n            # Gradient of regularization term\n            d = w_a - w_b\n            grad_reg = lambda_1 * np.sign(d) + 2 * lambda_2 * d\n            \n            # Full gradients for w_a and w_b\n            grad_w_a = grad_bce + grad_reg\n            grad_w_b = grad_bce - grad_reg\n\n            # --- Weight Update ---\n            w_a -= ETA * grad_w_a\n            w_b -= ETA * grad_w_b\n\n        # --- Evaluation ---\n        # 1. Compression Ratio\n        final_d = w_a - w_b\n        compression_ratio = np.mean(np.abs(final_d) = TAU)\n\n        # 2. Accuracy on validation set\n        val_logits = 0.5 * (X_val @ w_a + X_val @ w_b)\n        y_hat_val = (val_logits = 0).astype(int)\n        accuracy = np.mean(y_hat_val == y_val)\n        \n        results.append((compression_ratio, accuracy))\n\n    # --- Format and Print Output ---\n    output_str = \"[\" + \",\".join([f\"[{c:.4f},{a:.4f}]\" for c, a in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3161931"}]}