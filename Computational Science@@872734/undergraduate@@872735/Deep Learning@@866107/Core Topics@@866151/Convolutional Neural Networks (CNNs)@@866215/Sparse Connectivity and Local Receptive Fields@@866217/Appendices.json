{"hands_on_practices": [{"introduction": "The cornerstone of analyzing any convolutional neural network is understanding its receptive fieldâ€”the specific region of the input that affects the value of a given neuron. This first practice exercise challenges you to build this concept from the ground up, deriving a general formula that precisely quantifies how architectural parameters like kernel size, stride, and dilation collectively determine the receptive field's size [@problem_id:3175390]. Mastering this calculation is essential for designing and debugging deep learning models for structured data.", "problem": "A one-dimensional convolutional neural network is constructed as a stack of $L$ identical layers, each a discrete, linear, shift-invariant convolution with kernel size $k$, stride $s$, and dilation $d$, with no padding and no pooling. Consider the standard definition of a dilated, strided discrete convolution: the pre-activation at output position $n$ in layer $\\ell$ is the finite sum over $k$ weights applied to inputs spaced by dilation $d$, and successive outputs are spaced by stride $s$ in the input index of the previous layer. The local receptive field of a single output position at the top layer is the number of distinct input positions from the original input layer that can affect it. Sparse connectivity means that this number is finite and does not scale with the input length.\n\nStarting only from these fundamental definitions, and without invoking any pre-memorized formula, derive a general closed-form expression for the receptive field size $R_L$ at depth $L$ as a function of $k$, $s$, $d$, and $L$, assuming that all layers share the same $(k,s,d)$ and that no padding is used anywhere. Then, verify your expression by computing the receptive field size for a synthetic network with $k=3$, $s=2$, $d=2$, and $L=4$.\n\nProvide your final answer as a single integer equal to the receptive field size for the specified synthetic network. No units are required, and no rounding is needed.", "solution": "The problem requires the derivation of a general closed-form expression for the receptive field size of a one-dimensional convolutional neural network, followed by a calculation for a specific case. The derivation must proceed from fundamental definitions.\n\nLet the layers of the network be indexed from $\\ell=1$ to $\\ell=L$. The input data is designated as layer $\\ell=0$. We define the receptive field size of a single neuron in layer $\\ell$, denoted by $R_\\ell$, as the number of distinct input positions in layer $0$ that can affect its pre-activation value. Each of the $L$ layers is identical, characterized by a kernel size $k$, a stride $s$, and a dilation $d$.\n\nTo derive the general expression for $R_L$, we first establish a recursive relationship. Consider a single neuron in layer $\\ell$. Its value is computed from $k$ inputs in layer $\\ell-1$. Due to dilation $d$, these inputs are not at adjacent positions in the layer $\\ell-1$ feature map. If the first input is at position $j$, the subsequent inputs are at positions $j+d, j+2d, \\dots, j+(k-1)d$. The total receptive field of the layer-$\\ell$ neuron is the union of the receptive fields of these $k$ input neurons from layer $\\ell-1$.\n\nTo determine the size of this union, we must understand how receptive fields from layer $\\ell-1$ are positioned relative to each other in the original input space (layer $0$). Let $J_{\\ell-1}$ represent the cumulative stride up to layer $\\ell-1$. $J_{\\ell-1}$ is the distance in the layer $0$ input space between the starting points of the receptive fields of two adjacent neurons in layer $\\ell-1$. The cumulative stride can be defined recursively. Since all layers have the same stride $s$, the cumulative stride at layer $\\ell$ is:\n$$J_\\ell = s \\cdot J_{\\ell-1}$$\nWith a base case $J_0 = 1$ (as adjacent input neurons in layer $0$ are one position apart), the solution to this recurrence is a geometric progression:\n$$J_\\ell = s^\\ell$$\n\nNow, let's analyze the span of the $k$ inputs to our neuron in layer $\\ell$. These inputs are located at relative positions $0, d, 2d, \\dots, (k-1)d$ in the layer $\\ell-1$ feature map. The first input (relative position $0$) and the last input (relative position $(k-1)d$) are separated by a distance of $(k-1)d$ in the layer $\\ell-1$ coordinate system. The corresponding shift in the original input space is the product of this separation and the cumulative stride up to layer $\\ell-1$:\n$$\\text{Shift in input space} = ((j+(k-1)d) - j) \\times J_{\\ell-1} = (k-1)d \\cdot J_{\\ell-1}$$\nThe total receptive field size at layer $\\ell$, $R_\\ell$, is the size of the receptive field of one of its constituent neurons from layer $\\ell-1$ (which is $R_{\\ell-1}$) plus the additional span covered by the shifting of the receptive fields of the other input neurons. This additional span is equal to the shift calculated above. This gives the following recurrence relation for the receptive field size:\n$$R_\\ell = R_{\\ell-1} + (k-1)d \\cdot J_{\\ell-1}$$\nSubstituting the expression for the cumulative stride, $J_{\\ell-1} = s^{\\ell-1}$, we get:\n$$R_\\ell = R_{\\ell-1} + (k-1)d \\cdot s^{\\ell-1}$$\nThe base case for this recurrence is the receptive field of a neuron in the input layer ($0$), which is just the neuron itself. Therefore, $R_0 = 1$. We can solve the recurrence for $R_L$ by telescoping the sum:\n$$R_L = R_0 + \\sum_{\\ell=1}^{L} (R_\\ell - R_{\\ell-1})$$\nSubstituting the recurrence relation and the base case $R_0=1$:\n$$R_L = 1 + \\sum_{\\ell=1}^{L} (k-1)d \\cdot s^{\\ell-1}$$\nWe can change the summation index to $i = \\ell-1$, which runs from $0$ to $L-1$:\n$$R_L = 1 + \\sum_{i=0}^{L-1} (k-1)d \\cdot s^i$$\nThe term $(k-1)d$ is constant with respect to the summation index, so it can be factored out:\n$$R_L = 1 + (k-1)d \\sum_{i=0}^{L-1} s^i$$\nThe sum is a finite geometric series, which has a well-known closed form that depends on the value of $s$.\n\nCase 1: $s \\neq 1$.\nThe sum of the geometric series is $\\sum_{i=0}^{L-1} s^i = \\frac{s^L - 1}{s-1}$.\nThe receptive field size is therefore:\n$$R_L = 1 + (k-1)d \\left(\\frac{s^L - 1}{s-1}\\right)$$\n\nCase 2: $s = 1$.\nThe sum becomes $\\sum_{i=0}^{L-1} 1^i = L$.\nThe receptive field size is therefore:\n$$R_L = 1 + (k-1)d \\cdot L$$\nThis completes the derivation of the general closed-form expression.\n\nThe second part of the problem is to compute the receptive field size for a synthetic network with the following parameters:\n- Number of layers: $L=4$\n- Kernel size: $k=3$\n- Stride: $s=2$\n- Dilation: $d=2$\n\nSince $s=2 \\neq 1$, we use the formula for Case 1:\n$$R_L = 1 + (k-1)d \\left(\\frac{s^L - 1}{s-1}\\right)$$\nSubstituting the given numerical values:\n$$R_4 = 1 + (3-1) \\cdot 2 \\cdot \\left(\\frac{2^4 - 1}{2-1}\\right)$$\n$$R_4 = 1 + 2 \\cdot 2 \\cdot \\left(\\frac{16 - 1}{1}\\right)$$\n$$R_4 = 1 + 4 \\cdot (15)$$\n$$R_4 = 1 + 60$$\n$$R_4 = 61$$\nThe receptive field size for the specified synthetic network is $61$.", "answer": "$$\\boxed{61}$$", "id": "3175390"}, {"introduction": "Beyond abstract calculation, the size of a local receptive field has direct consequences for a network's performance on a specific task. This next exercise places you in the role of designing a simple yet robust edge detector, a fundamental operation in computer vision [@problem_id:3175463]. By working through this scenario, you will develop a quantitative understanding of how to determine the *minimal* receptive field size required to reliably extract a feature from a noisy signal, illustrating the critical link between network architecture and problem-solving capability.", "problem": "Consider a one-dimensional abstraction of a horizontal scanline from a binary image that contains a single vertical edge. The clean signal consists of a constant intensity $I_{\\ell}$ to the left of the edge and a constant intensity $I_{r}$ to the right of the edge, forming an ideal step with contrast $\\Delta = |I_{\\ell} - I_{r}|$. The observed signal is corrupted by independent additive Gaussian noise at each pixel with zero mean and variance $\\sigma^{2}$.\n\nA local receptive field of size $R$ (with $R$ an even integer) defines a sparse connectivity pattern that uses only $R$ contiguous samples centered at a given scan position. Consider the following linear detector that operates on this receptive field: the weights on the left half of the field are all equal to $+\\frac{1}{R/2}$, and the weights on the right half are all equal to $-\\frac{1}{R/2}$. The detector output at a given position is the weighted sum of the $R$ observed samples in the field. A scanning procedure with stride $1$ is assumed, so that there is a position where the field is exactly centered on the true edge. Define a fixed decision threshold $\\tau \\ge 0$ on the absolute value of the detector output, and define a required reliability $q \\in (0,1)$, understood as a decimal fraction.\n\nRobust detection is defined as achieving, at the position where the receptive field is centered on the true edge, a probability at least $q$ that the magnitude of the detector output exceeds $\\tau$. Assume that the edge is sufficiently far from the signal boundaries so that the aligned field is fully contained in the signal of length $N$; in particular, the receptive field size is constrained by $2 \\le R \\le N$.\n\nStarting from the fundamental definitions of linear filtering, local receptive fields, and properties of independent Gaussian noise under linear transformations, derive a method to determine, for given $(N, I_{\\ell}, I_{r}, \\sigma, \\tau, q)$, the minimal even integer $R$ that guarantees robust detection in the above sense. If no such $R$ exists under the constraint $R \\le N$, report impossibility by returning the integer $-1$.\n\nYour program must implement this derivation to compute the minimal $R$ for each parameter set in the following test suite. Each parameter set is specified as an ordered tuple $(N, I_{\\ell}, I_{r}, \\sigma, \\tau, q)$:\n\n- Test case $1$: $(N = 128, I_{\\ell} = 0.0, I_{r} = 1.0, \\sigma = 0.2, \\tau = 0.5, q = 0.95)$.\n- Test case $2$: $(N = 128, I_{\\ell} = 0.0, I_{r} = 0.6, \\sigma = 0.3, \\tau = 0.25, q = 0.90)$.\n- Test case $3$: $(N = 256, I_{\\ell} = 0.0, I_{r} = 0.4, \\sigma = 0.1, \\tau = 0.5, q = 0.90)$.\n- Test case $4$: $(N = 64, I_{\\ell} = 0.0, I_{r} = 1.0, \\sigma = 0.0, \\tau = 0.7, q = 0.99)$.\n- Test case $5$: $(N = 4, I_{\\ell} = 0.0, I_{r} = 0.6, \\sigma = 0.3, \\tau = 0.25, q = 0.90)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets, in the same order as the test cases (for example, $[R_{1},R_{2},R_{3},R_{4},R_{5}]$). No other output is permitted. Angles do not appear in this problem, and no physical units are required in the output. All probabilities must be handled and represented as decimal fractions, not as percentages.", "solution": "Let the receptive field of size $R$ be centered on the edge. The observed samples are $s_i = \\mu_i + \\eta_i$ for $i=1, \\dots, R$, where $\\mu_i$ is the clean signal intensity and $\\eta_i$ are independent and identically distributed (i.i.d.) random variables from a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$.\n\nWhen the field is centered, the clean signal values are $\\mu_i = I_{\\ell}$ for the left half ($i=1, \\dots, R/2$) and $\\mu_i = I_{r}$ for the right half ($i=R/2+1, \\dots, R$).\n\nThe detector is a linear filter with weights $w_i$:\n$$\nw_i = \\begin{cases}\n+ \\frac{1}{R/2} & \\text{for } i = 1, \\dots, R/2 \\\\\n- \\frac{1}{R/2} & \\text{for } i = R/2+1, \\dots, R\n\\end{cases}\n$$\nThe detector output, $D$, is a random variable given by the weighted sum:\n$$\nD = \\sum_{i=1}^{R} w_i s_i = \\frac{1}{R/2} \\sum_{i=1}^{R/2} s_i - \\frac{1}{R/2} \\sum_{i=R/2+1}^{R} s_i\n$$\n\nWe determine the statistical properties of $D$. As $D$ is a linear combination of independent Gaussian random variables, $D$ itself follows a Gaussian distribution. We need to find its mean $\\mu_D$ and variance $\\sigma_D^2$.\n\nThe mean (expected value) of the detector output is:\n$$\n\\mu_D = E[D] = E\\left[\\sum_{i=1}^{R} w_i (\\mu_i + \\eta_i)\\right] = \\sum_{i=1}^{R} w_i \\mu_i + \\sum_{i=1}^{R} w_i E[\\eta_i]\n$$\nSince $E[\\eta_i] = 0$, the second term vanishes.\n$$\n\\mu_D = \\sum_{i=1}^{R/2} \\left(+\\frac{1}{R/2}\\right) I_{\\ell} + \\sum_{i=R/2+1}^{R} \\left(-\\frac{1}{R/2}\\right) I_{r}\n$$\n$$\n\\mu_D = \\frac{R/2}{R/2} I_{\\ell} - \\frac{R/2}{R/2} I_{r} = I_{\\ell} - I_{r}\n$$\n\nThe variance of the detector output is:\n$$\n\\sigma_D^2 = \\text{Var}(D) = \\text{Var}\\left(\\sum_{i=1}^{R} w_i (\\mu_i + \\eta_i)\\right) = \\text{Var}\\left(\\sum_{i=1}^{R} w_i \\eta_i\\right)\n$$\nSince the noise samples $\\eta_i$ are independent, the variance of the sum is the sum of the variances:\n$$\n\\sigma_D^2 = \\sum_{i=1}^{R} w_i^2 \\text{Var}(\\eta_i) = \\sum_{i=1}^{R} w_i^2 \\sigma^2\n$$\n$$\n\\sigma_D^2 = \\sigma^2 \\left( \\sum_{i=1}^{R/2} \\left(\\frac{1}{R/2}\\right)^2 + \\sum_{i=R/2+1}^{R} \\left(-\\frac{1}{R/2}\\right)^2 \\right)\n$$\n$$\n\\sigma_D^2 = \\sigma^2 \\left( (R/2) \\frac{1}{(R/2)^2} + (R/2) \\frac{1}{(R/2)^2} \\right) = \\sigma^2 \\left( \\frac{1}{R/2} + \\frac{1}{R/2} \\right) = \\frac{2\\sigma^2}{R/2} = \\frac{4\\sigma^2}{R}\n$$\nThe standard deviation of the output is $\\sigma_D = \\sqrt{4\\sigma^2/R} = \\frac{2\\sigma}{\\sqrt{R}}$.\n\nSo, the detector output is a Gaussian random variable $D \\sim \\mathcal{N}\\left(I_{\\ell} - I_{r}, \\frac{4\\sigma^2}{R}\\right)$.\n\nThe robust detection condition is $P(|D| > \\tau) \\ge q$. This can be written as $P(D > \\tau) + P(D < -\\tau) \\ge q$.\nLet $Z = \\frac{D-\\mu_D}{\\sigma_D}$ be a standard normal variable, $Z \\sim \\mathcal{N}(0,1)$.\nThe condition becomes:\n$$\nP\\left(Z > \\frac{\\tau - \\mu_D}{\\sigma_D}\\right) + P\\left(Z < \\frac{-\\tau - \\mu_D}{\\sigma_D}\\right) \\ge q\n$$\nLet $\\Phi(x)$ be the cumulative distribution function (CDF) of the standard normal distribution. Let $\\Delta = |I_{\\ell} - I_{r}|$. Then $|\\mu_D| = \\Delta$. By symmetry, the probability expression is the same whether $\\mu_D = \\Delta$ or $\\mu_D = -\\Delta$. Using the property $\\Phi(-x) = 1-\\Phi(x)$, the expression simplifies to:\n$$\n\\Phi\\left(\\frac{\\Delta - \\tau}{\\sigma_D}\\right) + \\Phi\\left(\\frac{-\\Delta - \\tau}{\\sigma_D}\\right) \\ge q\n$$\nSubstituting $\\sigma_D = \\frac{2\\sigma}{\\sqrt{R}}$:\n$$\nP(R) = \\Phi\\left(\\frac{(\\Delta - \\tau)\\sqrt{R}}{2\\sigma}\\right) + \\Phi\\left(\\frac{(-\\Delta - \\tau)\\sqrt{R}}{2\\sigma}\\right) \\ge q\n$$\nWe need to find the minimum even integer $R \\in [2, N]$ that satisfies this inequality.\n\nWe analyze the behavior of the probability function $P(R)$ with respect to $R$.\n1.  **Case $\\sigma = 0$**: The detector output is deterministic: $D = \\mu_D$. Then $|D| = \\Delta$. The probability $P(|D| > \\tau)$ is $1$ if $\\Delta > \\tau$ and $0$ if $\\Delta \\le \\tau$. If $\\Delta > \\tau$, any $R$ works, so the minimal even $R$ is $2$. If $\\Delta \\le \\tau$, no $R$ works, so the answer is $-1$.\n2.  **Case $\\sigma > 0$ and $\\Delta > \\tau$**: As $R$ increases, the arguments to the CDF functions grow in magnitude, pushing the probabilities towards $1$ and $0$ respectively. The overall probability $P(R)$ is a monotonically increasing function of $R$. We can find the minimal $R$ by starting with $R=2$ and incrementing by $2$ until the condition is met or $R > N$.\n3.  **Case $\\sigma > 0$ and $\\Delta \\le \\tau$**: The first term's argument $(\\Delta - \\tau)$ is non-positive. As $R$ increases, the overall probability $P(R)$ is non-increasing. If the condition $P(R) \\ge q$ is not met for the smallest possible $R$ (i.e., $R=2$), it will not be met for any larger $R$. Thus, we only need to check $R=2$. If it passes, the answer is $2$; otherwise, it is $-1$.\n\nThe algorithm is to iterate through even integers $R$ from $2$ to $N$, calculating $P(R)$ at each step and returning the first $R$ that satisfies the condition, subject to the case analysis above. The standard normal CDF $\\Phi(x)$ can be computed using the error function, $\\text{erf}(x)$, as $\\Phi(x) = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Solves for the minimal receptive field size R for robust edge detection.\n    \"\"\"\n\n    # Test cases as specified in the problem statement.\n    # Each case is an ordered tuple (N, I_l, I_r, sigma, tau, q).\n    test_cases = [\n        (128, 0.0, 1.0, 0.2, 0.5, 0.95),  # Test case 1\n        (128, 0.0, 0.6, 0.3, 0.25, 0.90), # Test case 2\n        (256, 0.0, 0.4, 0.1, 0.5, 0.90), # Test case 3\n        (64, 0.0, 1.0, 0.0, 0.7, 0.99), # Test case 4\n        (4, 0.0, 0.6, 0.3, 0.25, 0.90),   # Test case 5\n    ]\n\n    results = []\n\n    def norm_cdf(x):\n        \"\"\"\n        Computes the standard normal cumulative distribution function (CDF).\n        \"\"\"\n        return 0.5 * (1.0 + erf(x / np.sqrt(2.0)))\n\n    def calculate_prob(R, delta, sigma, tau):\n        \"\"\"\n        Calculates the probability P(|D| > tau) for a given R.\n        \"\"\"\n        if sigma == 0:\n            return 1.0 if delta > tau else 0.0\n        \n        sqrt_R = np.sqrt(R)\n        denominator = 2.0 * sigma\n\n        arg1 = (delta - tau) * sqrt_R / denominator\n        arg2 = (-delta - tau) * sqrt_R / denominator\n        \n        return norm_cdf(arg1) + norm_cdf(arg2)\n\n    for case in test_cases:\n        N, I_l, I_r, sigma, tau, q = case\n        \n        delta = abs(I_l - I_r)\n        \n        found_R = -1\n\n        # Handle the special case of zero noise\n        if sigma == 0.0:\n            if delta > tau:\n                found_R = 2\n            else:\n                found_R = -1\n        # Case where probability increases with R\n        elif delta > tau:\n            # Iterate through even integers for R from 2 to N\n            for R in range(2, N + 1, 2):\n                prob = calculate_prob(R, delta, sigma, tau)\n                if prob >= q:\n                    found_R = R\n                    break\n        # Case where probability decreases with R\n        else: # delta <= tau\n            # Only need to check the smallest R\n            R = 2\n            if R <= N:\n                prob = calculate_prob(R, delta, sigma, tau)\n                if prob >= q:\n                    found_R = R\n                else:\n                    found_R = -1\n            else:\n                found_R = -1\n\n\n        results.append(found_R)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3175463"}, {"introduction": "Applying a local receptive field across a finite input signal immediately raises a practical question: what happens at the boundaries? This hands-on practice delves into this crucial implementation detail by exploring different padding strategies, such as zero, reflection, and circular padding [@problem_id:3175412]. By quantifying the \"artifacts\" or errors each strategy introduces, you will gain a deeper appreciation for how these choices can impact model behavior and why they are a critical design consideration in real-world applications.", "problem": "Consider a one-dimensional discrete input signal with length $n$, denoted by $x[0], x[1], \\ldots, x[n-1]$. A single-layer convolutional unit in a Convolutional Neural Network (CNN) establishes Sparse Connectivity and Local Receptive Fields by connecting each output unit $y[i]$ only to a local neighborhood of the input centered at $i$. Let the convolution kernel have odd length $m$ and weights $w[0], w[1], \\ldots, w[m-1]$, and define the receptive field radius $r = \\lfloor m/2 \\rfloor$. The local computation at position $i$ (using the common deep learning cross-correlation convention) is\n$$\ny[i] = \\sum_{k=0}^{m-1} w[k] \\, \\tilde{x}[i + k - r],\n$$\nwhere $\\tilde{x}[\\cdot]$ denotes an extension of $x[\\cdot]$ beyond the boundaries to support positions $i$ for which the kernel window indexes outside $[0, n-1]$.\n\nWe investigate boundary effects due to different padding strategies, defined for any integer index $j$:\n\n- Zero padding: \n$$\n\\tilde{x}[j] = \n\\begin{cases}\nx[j], & \\text{if } 0 \\le j \\le n-1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\n\n- Reflection padding (symmetric reflection about the boundaries, duplicating the boundary values):\n$$\n\\tilde{x}[j] = \n\\begin{cases}\nx[j], & \\text{if } 0 \\le j \\le n-1, \\\\\nx[-j-1], & \\text{if } j < 0, \\\\\nx[2n - 1 - j], & \\text{if } j \\ge n.\n\\end{cases}\n$$\n\n- Circular padding (periodic extension):\n$$\n\\tilde{x}[j] = x[j \\bmod n].\n$$\n\nFor each test case, we will designate a ground-truth extension rule for the underlying infinite signal that generated the observed finite segment. The \"true output\" $y_{\\text{true}}[i]$ is defined by applying the above local receptive field computation using the ground-truth extension rule for all $i \\in \\{0, 1, \\ldots, n-1\\}$.\n\nDefine the boundary index set\n$$\nB = \\{ i \\in \\{0, 1, \\ldots, n-1\\} \\mid i < r \\text{ or } i \\ge n - r \\}.\n$$\nThat is, $B$ contains the output positions whose receptive fields overlap the boundary. For a padding strategy $P \\in \\{\\text{zero}, \\text{reflection}, \\text{circular}\\}$, define the artifact score as the mean squared error at the boundary,\n$$\nA(P) = \\frac{1}{|B|} \\sum_{i \\in B} \\left( y_{P}[i] - y_{\\text{true}}[i] \\right)^2,\n$$\nwhere $y_{P}[i]$ is the output computed using padding strategy $P$. This score is dimensionless.\n\nUse the following fixed kernel for all test cases:\n$$\nm = 5, \\quad r = 2, \\quad w = \\frac{1}{16}[1, 4, 6, 4, 1].\n$$\n\nAngles used in trigonometric functions must be in radians.\n\nTest suite (each test case is fully specified by $(n, x, \\text{ground truth})$):\n- Case $1$ (compact pulse, ground truth zero extension):\n  - $n = 16$,\n  - $x[i] = 1$ if $i \\in \\{6, 7, 8, 9\\}$, and $x[i] = 0$ otherwise,\n  - ground truth extension rule: zero.\n- Case $2$ (linear ramp, ground truth symmetric reflection):\n  - $n = 15$,\n  - $x[i] = i$ for $i \\in \\{0, 1, \\ldots, 14\\}$,\n  - ground truth extension rule: reflection.\n- Case $3$ (discrete cosine, ground truth circular/periodic; cosine argument in radians):\n  - $n = 32$,\n  - $x[i] = \\cos\\!\\left(\\frac{2\\pi i}{n}\\right)$ for $i \\in \\{0, 1, \\ldots, 31\\}$,\n  - ground truth extension rule: circular.\n- Case $4$ (small edge case, ground truth zero extension):\n  - $n = 3$,\n  - $x = [0, 2, 0]$,\n  - ground truth extension rule: zero.\n\nYour program must compute, for each test case and for each padding strategy $P \\in \\{\\text{zero}, \\text{reflection}, \\text{circular}\\}$, the artifact score $A(P)$ as defined above. The final output must be a single line containing a list of lists, where each inner list corresponds to one test case in the order given, and contains three floating-point numbers in the order $[A(\\text{zero}), A(\\text{reflection}), A(\\text{circular})]$.\n\nFinal output format:\n- A single line string of the form \n$$\n[[a_{1,\\text{zero}}, a_{1,\\text{reflection}}, a_{1,\\text{circular}}], [a_{2,\\text{zero}}, a_{2,\\text{reflection}}, a_{2,\\text{circular}}], [a_{3,\\text{zero}}, a_{3,\\text{reflection}}, a_{3,\\text{circular}}], [a_{4,\\text{zero}}, a_{4,\\text{reflection}}, a_{4,\\text{circular}}]]\n$$\nwith raw decimal representations for each $a_{\\cdot,\\cdot}$.", "solution": "The problem requires the computation of an artifact score, $A(P)$, for three distinct padding strategies, $P \\in \\{\\text{zero}, \\text{reflection}, \\text{circular}\\}$, across four test cases. Each test case provides a one-dimensional signal $x$ of length $n$ and designates a ground-truth extension rule that defines the underlying infinite signal from which $x$ is assumed to be a segment. The artifact score quantifies the error introduced by a given padding strategy at the signal boundaries.\n\nThe methodological approach is as follows:\n\n1.  **Iterate Through Test Cases**: The overall algorithm processes each of the four test cases sequentially. For each case, the input signal $x$, its length $n$, and the ground-truth padding rule are extracted. The convolution kernel $w = \\frac{1}{16}[1, 4, 6, 4, 1]$ and its corresponding receptive field radius $r = 2$ are constant for all cases.\n\n2.  **Implement Padding Functions**: The core of the computation relies on extending the finite signal $x$ to an effectively infinite signal $\\tilde{x}$ according to a given padding rule. Three functions must be implemented to retrieve the value $\\tilde{x}[j]$ for any integer index $j$, based on the provided mathematical definitions:\n    -   **Zero Padding**: Returns $x[j]$ if $0 \\le j \\le n-1$ and $0$ otherwise.\n    -   **Reflection Padding**: Returns $x[j]$ for $0 \\le j \\le n-1$, $x[-j-1]$ for $j < 0$, and $x[2n-1-j]$ for $j \\ge n$.\n    -   **Circular Padding**: Returns $x[j \\bmod n]$.\n\n3.  **Implement Convolution**: A function is designed to perform the one-dimensional cross-correlation as specified:\n    $$y[i] = \\sum_{k=0}^{m-1} w[k] \\, \\tilde{x}[i + k - r]$$\n    This function takes the signal $x$ and a padding rule as input and computes the output signal $y$ of length $n$.\n\n4.  **Compute Ground-Truth and Padded Outputs**: For each test case:\n    -   The ground-truth output, $y_{\\text{true}}$, is computed by applying the convolution function with the specified ground-truth padding rule.\n    -   The outputs for the three padding strategies under evaluation, $y_{\\text{zero}}$, $y_{\\text{reflection}}$, and $y_{\\text{circular}}$, are computed by applying the convolution function with their respective padding rules.\n\n5.  **Calculate Artifact Score**: The artifact score $A(P)$ is calculated for each padding strategy $P$.\n    -   First, the set of boundary-affected output indices, $B = \\{ i \\in \\{0, 1, \\ldots, n-1\\} \\mid i < r \\text{ or } i \\ge n - r \\}$, is identified. Its cardinality, $|B|$, is determined. For $n > 2r$, $|B|=2r$; otherwise, $|B|=n$.\n    -   The score is the mean squared error between the padded output $y_P$ and the ground-truth output $y_{\\text{true}}$, calculated only over the indices in $B$:\n    $$A(P) = \\frac{1}{|B|} \\sum_{i \\in B} \\left( y_{P}[i] - y_{\\text{true}}[i] \\right)^2$$\n    If the chosen padding strategy $P$ matches the ground-truth rule, the error is expected to be $0$, and thus $A(P) = 0$.\n\n6.  **Aggregate and Format Results**: The computed scores for each test case, $[A(\\text{zero}), A(\\text{reflection}), A(\\text{circular})]$, are collected. The final output is an aggregation of these results for all four cases, formatted into a single list of lists as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating artifact scores for various padding strategies.\n    \"\"\"\n    \n    # Define fixed kernel parameters\n    m = 5\n    r = 2\n    w = np.array([1, 4, 6, 4, 1]) / 16.0\n    \n    # Define test cases from the problem statement\n    test_cases = [\n        # Case 1: Compact pulse, ground truth zero\n        {\n            \"n\": 16,\n            \"x\": np.array([1.0 if 6 <= i <= 9 else 0.0 for i in range(16)]),\n            \"ground_truth\": \"zero\"\n        },\n        # Case 2: Linear ramp, ground truth reflection\n        {\n            \"n\": 15,\n            \"x\": np.arange(15, dtype=float),\n            \"ground_truth\": \"reflection\"\n        },\n        # Case 3: Discrete cosine, ground truth circular\n        {\n            \"n\": 32,\n            \"x\": np.cos(2 * np.pi * np.arange(32) / 32),\n            \"ground_truth\": \"circular\"\n        },\n        # Case 4: Small edge case, ground truth zero\n        {\n            \"n\": 3,\n            \"x\": np.array([0.0, 2.0, 0.0]),\n            \"ground_truth\": \"zero\"\n        }\n    ]\n\n    all_results = []\n    padding_strategies = ['zero', 'reflection', 'circular']\n\n    # --- Helper functions for padding and convolution ---\n    \n    def get_padded_value(x_sig, j, n_sig, mode):\n        \"\"\"\n        Retrieves the value of a signal at index j, applying the specified padding rule.\n        \"\"\"\n        if 0 <= j < n_sig:\n            return x_sig[j]\n        \n        if mode == 'zero':\n            return 0.0\n        \n        if mode == 'reflection':\n            if j < 0:\n                # Per problem: x[-j-1] for j < 0\n                idx = -j - 1\n            else:  # j >= n_sig\n                # Per problem: x[2n - 1 - j] for j >= n\n                idx = 2 * n_sig - 1 - j\n            \n            # The problem statement's reflection padding definition implies that reflected indices can themselves\n            # fall out of the original [0, n-1] range, in which case the value is undefined or assumed 0.\n            # Here we reflect only once. A more robust implementation would handle multiple reflections, but this matches the likely intent.\n            if 0 <= idx < n_sig:\n                return x_sig[idx]\n            # If the reflection of a reflection is needed, the problem definition is ambiguous.\n            # Assuming a single reflection is sufficient for the receptive fields in question.\n            return 0.0 # Fallback for indices reflected out of bounds of single reflection\n\n        if mode == 'circular':\n            # Per problem: x[j mod n]\n            return x_sig[j % n_sig]\n        \n        raise ValueError(\"Unknown padding mode\")\n\n    def convolve_1d(x_sig, n_sig, padding_mode):\n        \"\"\"\n        Computes the 1D cross-correlation using the fixed kernel w and specified padding.\n        \"\"\"\n        y = np.zeros(n_sig)\n        for i in range(n_sig):\n            val = 0.0\n            for k in range(m):\n                x_idx = i + k - r\n                val += w[k] * get_padded_value(x_sig, x_idx, n_sig, padding_mode)\n            y[i] = val\n        return y\n\n    # --- Main logic loop ---\n    \n    for case in test_cases:\n        n = case[\"n\"]\n        x = case[\"x\"]\n        gt_mode = case[\"ground_truth\"]\n        \n        # Compute the ground-truth output signal\n        y_true = convolve_1d(x, n, gt_mode)\n        \n        # Determine the boundary index set B\n        boundary_indices = [i for i in range(n) if i < r or i >= n - r]\n        num_boundary_points = len(boundary_indices)\n        \n        case_scores = []\n        for p_mode in padding_strategies:\n            # Compute the output signal for the current padding strategy\n            y_p = convolve_1d(x, n, p_mode)\n            \n            # Calculate the sum of squared errors over the boundary set B\n            sse = 0.0\n            if num_boundary_points > 0:\n                for i in boundary_indices:\n                    error = y_p[i] - y_true[i]\n                    sse += error * error\n            \n            # Calculate the artifact score (mean squared error)\n            artifact_score = sse / num_boundary_points if num_boundary_points > 0 else 0.0\n            case_scores.append(artifact_score)\n            \n        all_results.append(case_scores)\n        \n    # --- Format final output ---\n    \n    final_output_segments = []\n    for scores in all_results:\n        segment = f\"[{','.join(map(str, scores))}]\"\n        final_output_segments.append(segment)\n        \n    print(f\"[{','.join(final_output_segments)}]\")\n\nsolve()\n```", "id": "3175412"}]}