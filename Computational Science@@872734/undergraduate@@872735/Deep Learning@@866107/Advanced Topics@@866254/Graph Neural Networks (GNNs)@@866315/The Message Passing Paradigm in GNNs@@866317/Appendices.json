{"hands_on_practices": [{"introduction": "When aggregating messages from neighbors, a naive summation can cause the feature scales to explode, especially in graphs where nodes have widely varying degrees. A principled normalization scheme is essential for stable and effective message passing. This practice [@problem_id:3189832] will guide you to derive the canonical symmetric normalization factor from fundamental constraints, helping you understand its theoretical underpinnings and its practical effect on balancing information flow between high- and low-degree nodes.", "problem": "You are given an undirected simple graph with node degrees and scalar node features. In the message passing framework of Graph Neural Networks (GNNs), consider a linear, degree-aware normalizer that rescales each incoming message along an edge $u \\leftarrow v$ by a factor that depends only on the degrees $d_u$ and $d_v$. Let the unnormalized scalar message be $m_{uv}$ and the normalized message be $\\tilde{m}_{uv}$. The aggregated signal at node $u$ is $s_u = \\sum_{v \\in \\mathcal{N}(u)} \\tilde{m}_{uv}$, where $\\mathcal{N}(u)$ is the set of neighbors of $u$.\n\nStarting from the following fundamental and widely accepted constraints for undirected graphs:\n\n- The normalization factor is a scalar function $\\gamma$ of the degrees only, so $\\tilde{m}_{uv} = \\gamma(d_u, d_v)\\, m_{uv}$.\n- Symmetry requirement for undirected graphs: $\\gamma(d_u, d_v) = \\gamma(d_v, d_u)$.\n- Multiplicative separability: there exists a positive function $\\alpha$ such that $\\gamma(d_u, d_v) = \\alpha(d_u)\\,\\alpha(d_v)$.\n- Constant-signal preservation on $d$-regular graphs: if $x_u = c$ for all nodes and $m_{uv} = x_v$, then for any $d \\geq 1$ the aggregated signal satisfies $s_u = c$ for all $u$.\n\nDerive the unique positive function $\\gamma(d_u, d_v)$ that satisfies these constraints. Then, use that result to compute the requested evaluations described below. Throughout, assume degrees are positive integers and all graphs are undirected.\n\nFor all computations in this problem, use scalar features with $x_v = 1$ for every node $v$, and define the unnormalized message on an edge to be $m_{uv} = w_{uv}\\, x_v$, where $w_{uv}$ is a given nonnegative scalar weight. In homogeneous graphs, take $w_{uv} = 1$ for all edges; in heterogeneous graphs, $w_{uv}$ may vary by edge type. The attenuation factor along an edge is defined as the ratio\n$$\nA_{u \\leftarrow v} \\equiv \\frac{\\tilde{m}_{uv}}{m_{uv}},\n$$\nand the node-level total attenuation at node $u$ is defined as\n$$\nR_u \\equiv \\frac{\\sum_{v \\in \\mathcal{N}(u)} \\tilde{m}_{uv}}{\\sum_{v \\in \\mathcal{N}(u)} m_{uv}} \\quad \\text{when the denominator is nonzero.}\n$$\n\nTest suite and required outputs:\n\nCompute the following three quantities using the normalization you have derived. Each required output is a single real number.\n\n1) Star graph, edge-level effect on a low-degree node receiving from a hub:\n- Graph: a star with $n = 6$ nodes, with one center node of degree $d_{\\text{center}} = 5$ and five leaves of degree $1$.\n- Homogeneous edges: $w_{uv} = 1$ for all edges.\n- Quantity: the attenuation factor $A_{\\text{leaf} \\leftarrow \\text{center}}$ on any edge where $u$ is a leaf and $v$ is the center.\n\n2) Star graph, node-level total effect on the hub:\n- Same star graph as in case $1$ with homogeneous edges $w_{uv} = 1$.\n- Quantity: the node-level total attenuation $R_{\\text{center}}$ at the center node.\n\n3) Heterogeneous small graph, node-level total effect at a node receiving from neighbors of mixed degrees and types:\n- Consider a node $u$ with degree $d_u = 3$ that is connected to three neighbors $v_1, v_2, v_3$ of degrees $d_{v_1} = 5$, $d_{v_2} = 2$, $d_{v_3} = 2$.\n- Heterogeneous edge weights: $w_{u v_1} = 2.0$, $w_{u v_2} = 0.5$, $w_{u v_3} = 1.5$.\n- Quantity: the node-level total attenuation $R_u$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3]$), where $r_1$, $r_2$, and $r_3$ correspond to the cases $1$, $2$, and $3$ respectively, each rounded to exactly $6$ decimal places.", "solution": "The problem requires the derivation of a unique, positive, symmetric, and separable normalization function $\\gamma(d_u, d_v)$ for message passing in Graph Neural Networks, based on four fundamental constraints. Subsequently, this function must be used to compute three specified quantities.\n\nPart 1: Derivation of the Normalization Function $\\gamma(d_u, d_v)$\n\nThe derivation proceeds by systematically applying the given constraints.\n\n1.  **Normalization Definition**: The normalized message $\\tilde{m}_{uv}$ is related to the unnormalized message $m_{uv}$ by a factor $\\gamma$ that depends on the degrees of the source and target nodes, $d_v$ and $d_u$.\n    $$\n    \\tilde{m}_{uv} = \\gamma(d_u, d_v) m_{uv}\n    $$\n\n2.  **Symmetry**: For undirected graphs, the normalization function must be symmetric with respect to its arguments.\n    $$\n    \\gamma(d_u, d_v) = \\gamma(d_v, d_u)\n    $$\n\n3.  **Multiplicative Separability**: The function $\\gamma$ can be expressed as the product of a positive function $\\alpha$ evaluated at each degree.\n    $$\n    \\gamma(d_u, d_v) = \\alpha(d_u) \\alpha(d_v) \\quad \\text{where } \\alpha(d) > 0 \\text{ for } d \\ge 1\n    $$\n    The symmetry constraint is automatically satisfied by this form, as $\\alpha(d_u) \\alpha(d_v) = \\alpha(d_v) \\alpha(d_u)$.\n\n4.  **Constant-Signal Preservation**: On any $d$-regular graph (where all nodes have degree $d_u=d$ for some $d \\ge 1$), if the input features are constant ($x_u = c$ for all $u$) and the unnormalized message is taken as the source node's feature ($m_{uv} = x_v$), then the aggregated signal at any node $u$ must be equal to that constant $c$.\n    $$\n    s_u = \\sum_{v \\in \\mathcal{N}(u)} \\tilde{m}_{uv} = c\n    $$\n    Let's expand this condition. For a $d$-regular graph, any node $u$ has $|\\mathcal{N}(u)| = d_u = d$ neighbors. For any edge $(u, v)$, both nodes have degree $d$, so $d_u = d_v = d$. The unnormalized message is $m_{uv} = x_v = c$.\n    Substituting these into the aggregation formula:\n    $$\n    s_u = \\sum_{v \\in \\mathcal{N}(u)} \\gamma(d_u, d_v) m_{uv} = \\sum_{v \\in \\mathcal{N}(u)} \\gamma(d, d) c\n    $$\n    Since the sum is over the $d$ neighbors of $u$, and the term $\\gamma(d, d) c$ is constant for all neighbors:\n    $$\n    s_u = d \\cdot \\gamma(d, d) \\cdot c\n    $$\n    The constraint requires $s_u = c$. Therefore, for any $c \\ne 0$:\n    $$\n    d \\cdot \\gamma(d, d) \\cdot c = c \\implies d \\cdot \\gamma(d, d) = 1\n    $$\n    This must hold for any integer degree $d \\ge 1$.\n\nNow, we combine this result with the multiplicative separability constraint. From constraint $3$, we have $\\gamma(d, d) = \\alpha(d) \\alpha(d) = (\\alpha(d))^2$. Substituting this into our derived equation:\n$$\nd \\cdot (\\alpha(d))^2 = 1\n$$\nSolving for $\\alpha(d)$:\n$$\n(\\alpha(d))^2 = \\frac{1}{d}\n$$\nSince $\\alpha$ is a positive function, we take the positive square root:\n$$\n\\alpha(d) = \\sqrt{\\frac{1}{d}} = \\frac{1}{\\sqrt{d}} = d^{-1/2}\n$$\nHaving found the unique positive function $\\alpha(d)$, we can now construct the unique normalization function $\\gamma(d_u, d_v)$:\n$$\n\\gamma(d_u, d_v) = \\alpha(d_u) \\alpha(d_v) = (d_u^{-1/2}) (d_v^{-1/2}) = (d_u d_v)^{-1/2} = \\frac{1}{\\sqrt{d_u d_v}}\n$$\nThis is the well-known symmetric normalization used in Graph Convolutional Networks (GCNs).\n\nPart 2: Calculation of Requested Quantities\n\nWe use the derived normalization function $\\gamma(d_u, d_v) = 1 / \\sqrt{d_u d_v}$ to compute the three quantities. The problem specifies that node features are $x_v = 1$ for all nodes, and unnormalized messages are $m_{uv} = w_{uv} x_v = w_{uv}$.\n\nThe definitions for the quantities are:\n-   Attenuation factor: $A_{u \\leftarrow v} = \\frac{\\tilde{m}_{uv}}{m_{uv}} = \\frac{\\gamma(d_u, d_v) m_{uv}}{m_{uv}} = \\gamma(d_u, d_v)$.\n-   Node-level total attenuation: $R_u = \\frac{\\sum_{v \\in \\mathcal{N}(u)} \\tilde{m}_{uv}}{\\sum_{v \\in \\mathcal{N}(u)} m_{uv}} = \\frac{\\sum_{v \\in \\mathcal{N}(u)} \\gamma(d_u, d_v) w_{uv}}{\\sum_{v \\in \\mathcal{N}(u)} w_{uv}}$.\n\n**Case 1: Star graph, edge-level effect**\n-   Graph: A star graph with one center node and $5$ leaf nodes.\n-   Degrees: $d_{\\text{center}} = 5$, $d_{\\text{leaf}} = 1$.\n-   Quantity: Attenuation factor $A_{\\text{leaf} \\leftarrow \\text{center}}$.\n-   Here, the receiving node $u$ is a leaf ($d_u = 1$) and the sending node $v$ is the center ($d_v = 5$).\n-   The calculation is:\n    $$\n    A_{\\text{leaf} \\leftarrow \\text{center}} = \\gamma(d_{\\text{leaf}}, d_{\\text{center}}) = \\gamma(1, 5) = \\frac{1}{\\sqrt{1 \\cdot 5}} = \\frac{1}{\\sqrt{5}} \\approx 0.4472136\n    $$\n\n**Case 2: Star graph, node-level effect**\n-   Graph: Same star graph with $d_{\\text{center}} = 5$ and $5$ neighbors of degree $d_{\\text{leaf}} = 1$.\n-   Edges: Homogeneous, so $w_{uv} = 1$ for all edges.\n-   Quantity: Node-level total attenuation $R_{\\text{center}}$ at the center node.\n-   The receiving node is the center, so $u = \\text{center}$ and $d_u = 5$. The neighbors $v_i$ are the $5$ leaves, so $d_{v_i} = 1$.\n-   The formula for $R_u$ becomes:\n    $$\n    R_{\\text{center}} = \\frac{\\sum_{i=1}^5 \\gamma(d_{\\text{center}}, d_{\\text{leaf}}) \\cdot w_{uv_i}}{\\sum_{i=1}^5 w_{uv_i}} = \\frac{\\sum_{i=1}^5 \\gamma(5, 1) \\cdot 1}{\\sum_{i=1}^5 1} = \\frac{5 \\cdot \\gamma(5, 1)}{5} = \\gamma(5, 1)\n    $$\n-   The calculation is:\n    $$\n    R_{\\text{center}} = \\frac{1}{\\sqrt{5 \\cdot 1}} = \\frac{1}{\\sqrt{5}} \\approx 0.4472136\n    $$\n\n**Case 3: Heterogeneous small graph**\n-   Graph: A node $u$ with degree $d_u = 3$ is connected to three neighbors $v_1, v_2, v_3$.\n-   Neighbor degrees: $d_{v_1} = 5$, $d_{v_2} = 2$, $d_{v_3} = 2$.\n-   Edge weights: $w_{uv_1} = 2.0$, $w_{uv_2} = 0.5$, $w_{uv_3} = 1.5$.\n-   Quantity: Node-level total attenuation $R_u$.\n-   We compute the numerator and denominator of the $R_u$ formula separately.\n-   Denominator (sum of unnormalized messages):\n    $$\n    \\sum_{v \\in \\mathcal{N}(u)} m_{uv} = w_{uv_1} + w_{uv_2} + w_{uv_3} = 2.0 + 0.5 + 1.5 = 4.0\n    $$\n-   Numerator (sum of normalized messages):\n    $$\n    \\sum_{v \\in \\mathcal{N}(u)} \\tilde{m}_{uv} = \\gamma(d_u, d_{v_1}) w_{uv_1} + \\gamma(d_u, d_{v_2}) w_{uv_2} + \\gamma(d_u, d_{v_3}) w_{uv_3}\n    $$\n    $$\n    = \\gamma(3, 5) \\cdot 2.0 + \\gamma(3, 2) \\cdot 0.5 + \\gamma(3, 2) \\cdot 1.5\n    $$\n    $$\n    = \\frac{1}{\\sqrt{3 \\cdot 5}} \\cdot 2.0 + \\frac{1}{\\sqrt{3 \\cdot 2}} \\cdot 0.5 + \\frac{1}{\\sqrt{3 \\cdot 2}} \\cdot 1.5\n    $$\n    $$\n    = \\frac{2.0}{\\sqrt{15}} + \\frac{0.5}{\\sqrt{6}} + \\frac{1.5}{\\sqrt{6}} = \\frac{2.0}{\\sqrt{15}} + \\frac{2.0}{\\sqrt{6}}\n    $$\n-   Combining them to find $R_u$:\n    $$\n    R_u = \\frac{\\frac{2.0}{\\sqrt{15}} + \\frac{2.0}{\\sqrt{6}}}{4.0} = \\frac{1}{2.0} \\left( \\frac{1}{\\sqrt{15}} + \\frac{1}{\\sqrt{6}} \\right) \\approx 0.5 \\cdot (0.2581989 + 0.4082483) \\approx 0.3332236\n    $$\n\nThe final numerical results, rounded to $6$ decimal places, are:\n-   Case 1: $0.447214$\n-   Case 2: $0.447214$\n-   Case 3: $0.333224$", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies a GNN normalization factor to solve three test cases.\n\n    The normalization factor is derived from first principles to be\n    gamma(du, dv) = 1 / sqrt(du * dv). This function is then used\n    to compute attenuation factors as requested.\n    \"\"\"\n\n    def gamma(du, dv):\n        \"\"\"Computes the symmetric normalization factor.\"\"\"\n        if du = 0 or dv = 0:\n            raise ValueError(\"Degrees must be positive integers.\")\n        return 1.0 / np.sqrt(du * dv)\n\n    # Test Case 1: Star graph, edge-level effect\n    # Attenuation factor A_{leaf - center}\n    # u = leaf, v = center\n    d_leaf_1 = 1\n    d_center_1 = 5\n    r1 = gamma(d_leaf_1, d_center_1)\n\n    # Test Case 2: Star graph, node-level total effect\n    # Total attenuation R_center at the hub\n    # u = center, neighbors v_i are leaves\n    # Since edge weights are homogeneous (w_uv=1), the total attenuation\n    # reduces to the per-edge attenuation factor.\n    # R_center = (sum_i gamma(d_u, d_vi) * 1) / (sum_i 1)\n    #          = (N * gamma(d_u, d_vi)) / N = gamma(d_u, d_vi)\n    d_center_2 = 5\n    d_leaf_2 = 1\n    r2 = gamma(d_center_2, d_leaf_2)\n\n    # Test Case 3: Heterogeneous small graph, node-level total effect\n    # Total attenuation R_u for a node u with diverse neighbors.\n    # R_u = (sum_v tilde_m_uv) / (sum_v m_uv)\n    # m_uv = w_uv * x_v, with x_v=1\n    d_u_3 = 3\n    neighbors = [\n        {'d_v': 5, 'w_uv': 2.0},\n        {'d_v': 2, 'w_uv': 0.5},\n        {'d_v': 2, 'w_uv': 1.5},\n    ]\n\n    sum_m_uv = 0.0\n    sum_tilde_m_uv = 0.0\n\n    for neighbor in neighbors:\n        d_v = neighbor['d_v']\n        w_uv = neighbor['w_uv']\n        \n        # Unnormalized message m_uv = w_uv * x_v = w_uv (since x_v=1)\n        m_uv = w_uv\n        sum_m_uv += m_uv\n\n        # Normalized message tilde_m_uv = gamma(d_u, d_v) * m_uv\n        tilde_m_uv = gamma(d_u_3, d_v) * m_uv\n        sum_tilde_m_uv += tilde_m_uv\n\n    if sum_m_uv == 0:\n        r3 = 0.0 # As per problem statement, denominator is non-zero\n    else:\n        r3 = sum_tilde_m_uv / sum_m_uv\n\n    results = [r1, r2, r3]\n    \n    # Format the output rounded to exactly 6 decimal places.\n    # The f-string format `{r:.6f}` handles the rounding.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3189832"}, {"introduction": "Stacking many message passing layers allows a GNN to access information from distant nodes, but it can also lead to the loss of crucial local structural details through a phenomenon known as over-smoothing. How can a network see both the forest and the trees? This exercise [@problem_id:3189831] explores the \"Jumping Knowledge\" (JK) framework, which uses skip connections to create a final representation that aggregates information from all layers, demonstrating its superior ability to capture multi-scale structural properties.", "problem": "You are asked to design and implement a principled comparison between two node representation schemes in the message passing paradigm for Graph Neural Networks (GNNs): a deep single-output representation versus a skip-connected Jumping Knowledge (JK) representation. The setting is purely mathematical and concerns functions on graphs defined via permutation-invariant neighborhood aggregation. Use only linear readouts trained by ordinary least squares to map node representations to specified targets, and evaluate prediction error.\n\nStart from the fundamental base that a message passing GNN updates node states layer by layer using a permutation-invariant neighborhood aggregation. Concretely, at layer $t$, the state of node $v$ is updated from its previous state and aggregated messages from its neighbors. Depth $t$ expands the receptive field to the $t$-hop neighborhood. Skip connections preserve intermediate states, and Jumping Knowledge concatenates all intermediate hidden states. In this problem, you will instantiate an idealized cumulative-sum aggregator over metric balls defined by shortest path distance and compare representational power of the two schemes under a linear readout.\n\nDefinitions to use:\n- Let $G=(V,E)$ be an undirected graph with $|V| = N$ nodes labeled $0,1,\\dots,N-1$.\n- Let the initial scalar node feature be $x_v \\in \\mathbb{R}$ for each node $v$.\n- Let $d(u,v)$ denote the unweighted shortest path distance between nodes $u$ and $v$.\n- For a fixed nonnegative integer $T$, define the idealized cumulative-sum message passing representation by\n$$\nh_v^{(t)} \\triangleq \\sum_{u \\in V \\,:\\, d(u,v) \\le t} x_u \\quad \\text{for} \\quad t \\in \\{0,1,\\dots,T\\}.\n$$\nThis $h_v^{(t)}$ is the sum of features within the closed ball of radius $t$ around $v$ under the shortest-path metric.\n- Define the deep single-output representation as $h_v^{(T)}$.\n- Define the Jumping Knowledge representation as the concatenation $h_v^{\\mathrm{JK}} = \\mathrm{concat}\\!\\left(h_v^{(0)},h_v^{(1)},\\dots,h_v^{(T)}\\right)$.\n\nTargets to predict:\n- For a fixed nonnegative integer $d$, define the ring-sum target\n$$\ny_v^{(d)} \\triangleq \\sum_{u \\in V \\,:\\, d(u,v) = d} x_u.\n$$\n\nLearning and evaluation protocol:\n- For a given graph, features, $T$, and $d$, use ordinary least squares to fit a linear readout with bias to predict $y_v^{(d)}$ from node representations, using all nodes as the training set.\n- Case A (deep single-output): fit $y_v^{(d)} \\approx a \\cdot h_v^{(T)} + b$ with scalar $a$ and bias $b$.\n- Case B (Jumping Knowledge): fit $y_v^{(d)} \\approx \\sum_{t=0}^{T} w_t \\, h_v^{(t)} + b$ with weights $w_t$ and bias $b$.\n- For each case, report the mean squared error\n$$\n\\mathrm{MSE} \\triangleq \\frac{1}{N} \\sum_{v \\in V} \\left(\\widehat{y}_v - y_v^{(d)}\\right)^2.\n$$\n\nTest suite:\nUse the following four test cases, each with deterministic features $x_v = v+1$.\n\n- Test $1$ (happy path): path graph on $N=7$ nodes with edges $\\{(i,i+1)\\,:\\, i \\in \\{0,1,2,3,4,5\\}\\}$, depth $T=2$, target ring distance $d=1$.\n- Test $2$ (longer radius at the depth limit): same path graph on $N=7$, depth $T=2$, target ring distance $d=2$.\n- Test $3$ (structural edge case): star graph on $N=7$ with center node $0$ connected to all other nodes, depth $T=1$, target ring distance $d=1$.\n- Test $4$ (cycle with long-range ring): cycle graph on $N=8$ with edges $\\{(i,(i+1)\\bmod 8)\\,:\\, i \\in \\{0,1,2,3,4,5,6,7\\}\\}$, depth $T=3$, target ring distance $d=3$.\n\nProgram requirements:\n- Implement exact shortest path distances $d(u,v)$ and the cumulative-sum representations $h_v^{(t)}$ for $t \\in \\{0,1,\\dots,T\\}$.\n- Fit the linear readouts by ordinary least squares with a bias term.\n- For each test case, compute the difference\n$$\n\\Delta \\mathrm{MSE} \\triangleq \\mathrm{MSE}_{\\text{single}} - \\mathrm{MSE}_{\\text{JK}}.\n$$\n- Your program should produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, in the order of Tests $1$ through $4$. The elements must be real numbers. For example, the required format is $[r_1,r_2,r_3,r_4]$ where each $r_i$ is the computed $\\Delta \\mathrm{MSE}$ for Test $i$.\n\nAngle units and physical units are not applicable. All reported quantities are pure real numbers. The final printed list must be a single line with no extra text.", "solution": "The problem asks for a quantitative comparison between two schemes for generating node representations in a Graph Neural Network (GNN): a deep single-output representation and a Jumping Knowledge (JK) skip-connected representation. The comparison is based on their ability to predict a specific structural property of the graph—the sum of features on a \"ring\" at a fixed distance—using a linear readout model. The core of the solution lies in first principles of linear algebra and graph theory, as applied to an idealized GNN model.\n\nFirst, we establish the necessary graph-theoretic and GNN-related quantities. For a given undirected graph $G=(V, E)$ with $N=|V|$ nodes, and initial node features $x_v$, we must compute the all-pairs shortest path distance matrix, $D$, where $D_{uv} = d(u,v)$. This can be achieved by running a Breadth-First Search (BFS) starting from each node $v \\in V$.\n\nWith the distance matrix $D$, we can compute the two key quantities for each node $v$: the representation vectors $h_v^{(t)}$ and the target value $y_v^{(d)}$.\n\nThe representation at layer $t$ is the cumulative sum of features within the closed ball of radius $t$ centered at $v$:\n$$\nh_v^{(t)} = \\sum_{u \\in V \\,:\\, d(u,v) \\le t} x_u\n$$\nThis is computed for each node $v$ and for each layer depth $t \\in \\{0, 1, \\dots, T\\}$. This gives us $T+1$ vectors of representations, $\\{H^{(t)}\\}_{t=0}^T$, where $(H^{(t)})_v = h_v^{(t)}$.\n\nThe target value for node $v$ is the sum of features on the ring (or shell) of radius $d$ around $v$:\n$$\ny_v^{(d)} = \\sum_{u \\in V \\,:\\, d(u,v) = d} x_u\n$$\nThis gives a target vector $Y^{(d)}$, where $(Y^{(d)})_v = y_v^{(d)}$.\n\nA crucial observation establishes the relationship between these quantities. The cumulative sum $h_v^{(t)}$ can be expressed recursively:\n$$\nh_v^{(t)} = \\left(\\sum_{u \\in V \\,:\\, d(u,v) \\le t-1} x_u\\right) + \\left(\\sum_{u \\in V \\,:\\, d(u,v) = t} x_u\\right) = h_v^{(t-1)} + y_v^{(t)}\n$$\nfor $t \\ge 1$. For $t=0$, the ball of radius $0$ contains only the node itself, so $h_v^{(0)} = x_v$. The ring of radius $0$ also contains only the node, so $y_v^{(0)} = x_v$. Thus, $h_v^{(0)} = y_v^{(0)}$.\nFrom this, it follows that for any $d \\ge 1$, the ring-sum can be expressed as the difference of two consecutive cumulative-sum representations:\n$$\ny_v^{(d)} = h_v^{(d)} - h_v^{(d-1)}\n$$\n\nNow, we analyze the two representation schemes and their predictive power under a linear readout trained with ordinary least squares (OLS). The OLS procedure finds the linear model that minimizes the mean squared error (MSE) between predictions and true target values over all nodes.\n\nCase A (Deep Single-Output): The representation for node $v$ is simply the final layer's output, $h_v^{(T)}$. We fit a linear model $y_v^{(d)} \\approx \\widehat{y}_v = a \\cdot h_v^{(T)} + b$. The coefficients $a$ and $b$ are determined by OLS to minimize $\\frac{1}{N}\\sum_{v \\in V}(a \\cdot h_v^{(T)} + b - y_v^{(d)})^2$. In general, $y_v^{(d)}$ is not a linear function of $h_v^{(T)}$, so this model will have a non-zero error, $\\mathrm{MSE}_{\\text{single}} > 0$. The coefficients are found using the standard formulas for simple linear regression: $a = \\mathrm{Cov}(H^{(T)}, Y^{(d)}) / \\mathrm{Var}(H^{(T)})$ and $b = \\overline{Y^{(d)}} - a \\overline{H^{(T)}}$.\n\nCase B (Jumping Knowledge): The representation is the concatenation of all intermediate layer outputs, $(h_v^{(0)}, h_v^{(1)}, \\dots, h_v^{(T)})$. We fit a multiple linear regression model $y_v^{(d)} \\approx \\widehat{y}_v = \\sum_{t=0}^{T} w_t h_v^{(t)} + b$. The weights $\\{w_t\\}_{t=0}^T$ and bias $b$ are found via OLS.\nThe key insight is that if the target distance $d$ is within the maximum depth of the GNN, i.e., $d \\le T$, the target vector $Y^{(d)}$ can be expressed *exactly* as a linear combination of the JK feature vectors.\n- If $d=0$, then $Y^{(0)} = H^{(0)}$. The OLS solution will be $w_0=1$, $w_t=0$ for $t>0$, and $b=0$.\n- If $1 \\le d \\le T$, then $Y^{(d)} = H^{(d)} - H^{(d-1)}$. The OLS solution will be $w_d=1$, $w_{d-1}=-1$, $w_t=0$ for $t \\notin \\{d, d-1\\}$, and $b=0$.\nIn both scenarios, the linear model can perfectly reconstruct the target. Consequently, the predictions are exact, $\\widehat{y}_v = y_v^{(d)}$ for all $v$, and the resulting error is zero: $\\mathrm{MSE}_{\\text{JK}} = 0$. This holds for all test cases in the problem, as for each, the target distance $d$ is less than or equal to the network depth $T$.\n\nThe final quantity to calculate is $\\Delta \\mathrm{MSE} = \\mathrm{MSE}_{\\text{single}} - \\mathrm{MSE}_{\\text{JK}}$. Since $\\mathrm{MSE}_{\\text{JK}} = 0$ for all test cases, this simplifies to $\\Delta \\mathrm{MSE} = \\mathrm{MSE}_{\\text{single}}$. Our task is therefore reduced to calculating the OLS regression error for the single-output model for each test case.\n\nThe procedure for each test case is:\n1. Construct the graph's adjacency list from the problem description.\n2. Compute the all-pairs shortest path matrix $D$ using BFS from each node.\n3. Generate the initial feature vector $x_v=v+1$.\n4. Using $D$ and $x$, compute the target vector $Y^{(d)}$ and the representation vector $H^{(T)}$.\n5. With $p=H^{(T)}$ as the predictor and $q=Y^{(d)}$ as the target, solve the simple linear regression problem to find the optimal coefficients $a$ and $b$.\n6. Calculate the predictions $\\widehat{Y}^{(d)} = a \\cdot H^{(T)} + b$.\n7. Compute the mean squared error $\\mathrm{MSE}_{\\text{single}} = \\frac{1}{N} \\sum_{v \\in V} (\\widehat{y}_v^{(d)} - y_v^{(d)})^2$. This value is our $\\Delta \\mathrm{MSE}$.\n\nThis process is repeated for all four test cases, and the resulting $\\Delta \\mathrm{MSE}$ values are collected. The calculation relies on standard numerical linear algebra, for which `numpy.linalg.lstsq` provides a robust and numerically stable implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Solves the GNN representation comparison problem for the four specified test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test 1: path graph, T=2, d=1\n        {'N': 7, 'edges': [(i, i + 1) for i in range(6)], 'T': 2, 'd': 1},\n        # Test 2: path graph, T=2, d=2\n        {'N': 7, 'edges': [(i, i + 1) for i in range(6)], 'T': 2, 'd': 2},\n        # Test 3: star graph, T=1, d=1\n        {'N': 7, 'edges': [(0, i) for i in range(1, 7)], 'T': 1, 'd': 1},\n        # Test 4: cycle graph, T=3, d=3\n        {'N': 8, 'edges': [(i, (i + 1) % 8) for i in range(8)], 'T': 3, 'd': 3},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = case['N']\n        edges = case['edges']\n        T = case['T']\n        d = case['d']\n\n        # 1. Build adjacency list for the graph\n        adj = [[] for _ in range(N)]\n        for u, v in edges:\n            adj[u].append(v)\n            adj[v].append(u)\n\n        # 2. Compute all-pairs shortest paths using BFS from each node\n        dist_matrix = np.full((N, N), -1, dtype=int)\n        for start_node in range(N):\n            dist_matrix[start_node, start_node] = 0\n            q = deque([(start_node, 0)])\n            visited = {start_node}\n            while q:\n                curr_node, curr_dist = q.popleft()\n                for neighbor in adj[curr_node]:\n                    if neighbor not in visited:\n                        visited.add(neighbor)\n                        dist_matrix[start_node, neighbor] = curr_dist + 1\n                        q.append((neighbor, curr_dist + 1))\n\n        # 3. Define initial node features: x_v = v + 1\n        features = np.arange(1, N + 1, dtype=float)\n\n        # 4. Compute target vector y_v^(d)\n        target_y = np.zeros(N, dtype=float)\n        for v in range(N):\n            # Find nodes u such that d(u, v) == d\n            ring_nodes = np.where(dist_matrix[v, :] == d)[0]\n            if ring_nodes.size > 0:\n                target_y[v] = np.sum(features[ring_nodes])\n\n        # 5. Compute representations h_v^(t)\n        H = np.zeros((N, T + 1), dtype=float)\n        for t in range(T + 1):\n            for v in range(N):\n                # Find nodes u such that d(u, v) = t\n                ball_nodes = np.where(dist_matrix[v, :] = t)[0]\n                if ball_nodes.size > 0:\n                     H[v, t] = np.sum(features[ball_nodes])\n        \n        # 6. Case A (Single-Output): Fit y ~ a * h^(T) + b\n        p = H[:, T]  # Predictor vector h^(T)\n        X_single = np.vstack([p, np.ones(N)]).T\n        \n        # Use np.linalg.lstsq for robust OLS regression\n        _, single_residuals, _, _ = np.linalg.lstsq(X_single, target_y, rcond=None)\n        \n        # The sum of squared residuals is returned by lstsq\n        if single_residuals.size > 0:\n            mse_single = single_residuals[0] / N\n        else: # This case occurs if the system is underdetermined\n            coeffs_single = np.linalg.lstsq(X_single, target_y, rcond=None)[0]\n            preds_single = X_single @ coeffs_single\n            mse_single = np.mean((preds_single - target_y)**2)\n\n        # 7. Case B (Jumping Knowledge): Fit y ~ sum(w_t * h^(t)) + b\n        # As reasoned in the solution, the target y^(d) for d=T is perfectly\n        # expressible as a linear combination of h^(t) vectors.\n        # So, the residual error must be (close to) zero.\n        mse_jk = 0.0\n        \n        # Optional: Verify mse_jk is near zero numerically\n        # X_jk = np.hstack([H, np.ones((N, 1))])\n        # _, jk_residuals, _, _ = np.linalg.lstsq(X_jk, target_y, rcond=None)\n        # if jk_residuals.size > 0:\n        #    mse_jk_calc = jk_residuals[0] / N\n        # else:\n        #     coeffs_jk = np.linalg.lstsq(X_jk, target_y, rcond=None)[0]\n        #     pred_jk = X_jk @ coeffs_jk\n        #     mse_jk_calc = np.mean((pred_jk-target_y)**2)\n        # mse_jk = mse_jk_calc\n        \n        # 8. Compute the difference in MSE\n        delta_mse = mse_single - mse_jk\n        results.append(delta_mse)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3189831"}, {"introduction": "The message passing paradigm, for all its power, has fundamental limits on its ability to distinguish different graph structures, a property formally linked to the Weisfeiler-Lehman test of isomorphism. This means that some seemingly simple tasks, like counting triangles, are provably difficult for standard MPNNs. This thought experiment [@problem_id:3189816] confronts this limitation head-on, using a classic pair of indistinguishable graphs to reveal why standard neighborhood aggregation fails and how higher-order, motif-aware GNNs can provide a solution.", "problem": "You are asked to propose and analyze a synthetic graph classification task that probes the expressive limits of the message passing paradigm in Graph Neural Networks (GNNs). The goal is to reason from core definitions to show when standard Message Passing Neural Networks (MPNNs) provably fail to detect triangle motifs and how a modified, motif-aware neighborhood definition can overcome this limitation.\n\nConsider the following general setting. A simple, undirected graph is denoted by $G=(V,E)$ with node set $V$ and edge set $E$. A Message Passing Neural Network (MPNN) updates node states by iterating, for $t=0,1,\\dots,T-1$,\n$$\nh_v^{(t+1)} \\;=\\; \\phi^{(t)}\\Big(h_v^{(t)}, \\;\\square_{u \\in \\mathcal{N}(v)} \\psi^{(t)}\\big(h_v^{(t)}, h_u^{(t)}, e_{vu}\\big)\\Big),\n$$\nwhere $h_v^{(0)}=x_v$ are initial node features, $\\mathcal{N}(v)$ is the neighborhood of $v$, $\\psi^{(t)}$ and $\\phi^{(t)}$ are learnable functions, $e_{vu}$ encodes edge features if present, and $\\square$ is a permutation-invariant multiset aggregator such as sum, mean, or max. The graph-level output is produced by a permutation-invariant readout on $\\{h_v^{(T)}: v \\in V\\}$. It is a widely accepted fact that isomorphism-invariant MPNNs with such local, permutation-invariant aggregation are at most as powerful as the $1$-dimensional Weisfeiler–Lehman (WL) test at distinguishing non-isomorphic graphs.\n\nDefine the triangle-motif neighborhood of a node $v$ as\n$$\n\\mathcal{N}_{\\triangle}(v) \\;=\\; \\left\\{\\, u \\in V \\;:\\; \\exists w \\in V \\;\\text{with}\\; \\{v,u\\}\\in E,\\; \\{u,w\\}\\in E,\\; \\{w,v\\}\\in E \\,\\right\\},\n$$\nthat is, $\\mathcal{N}_{\\triangle}(v)$ collects exactly those neighbors $u$ that, together with some $w$, close a $3$-cycle through $v$. One can define a motif-aware MPNN by replacing $\\mathcal{N}(v)$ with $\\mathcal{N}_{\\triangle}(v)$ in the aggregation.\n\nNow, consider constructing a synthetic binary graph classification task “triangle-existence”: given $G$, predict label $y(G)=1$ if $G$ contains at least one triangle and $y(G)=0$ otherwise. All nodes start with identical features, $x_v = \\mathbf{c} \\in \\mathbb{R}^{d}$ for all $v \\in V$ with fixed $d \\in \\mathbb{N}$, and the MPNN is isomorphism-invariant with any standard permutation-invariant aggregator. You may assume graphs have no self-loops and no parallel edges.\n\nWhich of the following option(s) correctly specify a concrete pair of graphs and provide a correct reasoning for why a standard MPNN fails on this task in the worst case, and why switching to motif-based neighborhoods $\\mathcal{N}_{\\triangle}(v)$ can succeed?\n\nA. Use $G_{1}$ equal to the triangular prism on $6$ nodes (two disjoint triangles with corresponding vertices connected, which contains triangles) and $G_{2}$ equal to $K_{3,3}$ (a complete bipartite graph on partitions of size $3$, which is triangle-free). Both are $3$-regular on $6$ nodes. With identical initial node features, an isomorphism-invariant MPNN with standard $1$-hop neighborhoods cannot distinguish $G_{1}$ and $G_{2}$ because its expressive power matches the $1$-dimensional Weisfeiler–Lehman test, which assigns uniform colors on these regular graphs and never refines them. Hence the model outputs identical graph-level embeddings for $G_{1}$ and $G_{2}$, despite $y(G_{1})=1$ and $y(G_{2})=0$. If we instead aggregate over $\\mathcal{N}_{\\triangle}(v)$, then every node in $G_{1}$ has $\\lvert \\mathcal{N}_{\\triangle}(v) \\rvert = 2$ while every node in $G_{2}$ has $\\lvert \\mathcal{N}_{\\triangle}(v) \\rvert = 0$, so a single motif-aware layer can separate the graphs via nontrivial messages in $G_{1}$ versus null messages in $G_{2}$.\n\nB. Use any pair of graphs with different triangle counts. A standard MPNN with enough layers always captures triangles because stacking $3$ layers encodes $3$-hop walks, which directly count $3$-cycles in the aggregated messages; therefore motif-based neighborhoods are unnecessary.\n\nC. The failure of standard MPNNs on triangle counting arises primarily from vanishing gradients when training with deep stacks of layers; introducing $\\mathcal{N}_{\\triangle}(v)$ fixes the optimization landscape by rescaling messages, not by changing expressivity, so any pair of graphs with distinct triangle counts will be separated after training.\n\nD. Replace $\\mathcal{N}(v)$ with the closed $2$-hop neighborhood $\\mathcal{N}_{2}(v)=\\{u \\in V : \\mathrm{dist}(u,v) \\le 2\\}$ in a standard MPNN. This is sufficient to detect $3$-cycles because all triangle vertices lie within $2$ hops, so the model distinguishes $K_{3,3}$ and the triangular prism without using motif-based neighborhoods.\n\nE. Augment each node’s initial feature with its degree, $x_v = [\\mathbf{c}; \\deg(v)]$. Since triangle-rich graphs are not $d$-regular for any $d$, a standard MPNN can exactly count triangles from degree information alone; $\\mathcal{N}_{\\triangle}(v)$ is unnecessary for perfect separation.", "solution": "The problem explores the expressive limitations of standard Message Passing Neural Networks (MPNNs). The core principle is that the expressive power of an isomorphism-invariant MPNN with identical initial node features is upper-bounded by the 1-dimensional Weisfeiler–Lehman (1-WL) test. The 1-WL test iteratively refines node representations (or \"colors\") based on the multiset of their neighbors' representations.\n\nA standard MPNN will fail to distinguish between two graphs if the 1-WL test cannot. This occurs, for example, with two regular graphs that have the same number of nodes and the same degree. If all nodes start with an identical feature vector $h^{(0)}$, then after one round of message passing, every node in both graphs will have aggregated information from an identical multiset of neighbor features. Consequently, all nodes will be updated to a new identical feature vector $h^{(1)}$. This process repeats, and at no layer can the MPNN compute different node embeddings for the two graphs. A final permutation-invariant readout will therefore produce the same graph-level representation, making classification impossible.\n\nTo solve the \"triangle-existence\" task, a successful option must present a pair of graphs where one has triangles and the other does not, but both are indistinguishable to a standard MPNN. The option must also show how a motif-aware neighborhood definition breaks this symmetry.\n\n**Analysis of Options:**\n\n*   **Option A:** This option proposes using the triangular prism ($G_1$) and the complete bipartite graph $K_{3,3}$ ($G_2$).\n    *   **Graphs:** Both are 6-node, 3-regular graphs. $G_1$ contains triangles, while $K_{3,3}$ is bipartite and thus triangle-free. This pair fits the criteria for MPNN failure.\n    *   **Standard MPNN:** As explained above, because both graphs are 3-regular and start with uniform node features, a standard MPNN cannot distinguish them. The reasoning provided is correct.\n    *   **Motif-Aware MPNN:** The proposed solution is to switch to the triangle-motif neighborhood $\\mathcal{N}_{\\triangle}(v)$. In $G_1$, every vertex is part of one triangle, so its two neighbors in that triangle form its motif neighborhood, meaning $|\\mathcal{N}_{\\triangle}(v)|=2$ for all $v \\in V(G_1)$. In $G_2$, there are no triangles, so $\\mathcal{N}_{\\triangle}(v)$ is empty and $|\\mathcal{N}_{\\triangle}(v)|=0$ for all $v \\in V(G_2)$. A GNN aggregating over this neighborhood would receive messages from two neighbors in $G_1$ and zero neighbors in $G_2$. This difference in aggregated messages allows the model to compute distinct embeddings and successfully classify the graphs. This reasoning is sound.\n\n*   **Option B:** This is incorrect. Stacking layers allows aggregation from a larger radius but does not give the MPNN the ability to check for edges *between* neighbors, which is required for triangle counting. The permutation-invariant aggregator loses this structural information.\n\n*   **Option C:** This is incorrect. The failure is a fundamental limitation of **expressivity**, not an **optimization** issue like vanishing gradients. The MPNN architecture is mathematically incapable of representing the function needed to distinguish the graphs in Option A.\n\n*   **Option D:** This is incorrect. Aggregating over a 2-hop neighborhood does not solve the problem. As with the 1-hop case, the permutation-invariant aggregator still receives a multiset of features. For the prism and $K_{3,3}$, every node has the same number of nodes at distance 1 and distance 2, so the aggregated feature multisets would still be identical if starting from uniform features.\n\n*   **Option E:** This is incorrect. The premise that triangle-rich graphs are not regular is false (e.g., complete graphs). Furthermore, degree information is insufficient; for example, a 6-cycle and two disjoint 3-cycles have identical degree sequences (all nodes have degree 2) but different triangle counts. For the pair in Option A, both graphs are 3-regular, so adding degree as a feature provides no distinguishing information.\n\nBased on this analysis, Option A is the only one that correctly identifies a valid counterexample and provides sound reasoning for both the failure of standard MPNNs and the success of the proposed motif-aware modification.", "answer": "$$\\boxed{A}$$", "id": "3189816"}]}