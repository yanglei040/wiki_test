{"hands_on_practices": [{"introduction": "The heart of any Graph Neural Network is its mechanism for passing messages between nodes. This exercise provides a hands-on comparison of several fundamental propagation models, from simple label diffusion to more sophisticated methods. By implementing and analyzing these techniques, you will gain a foundational understanding of how information spreads across a graph and encounter the common pitfall of \"over-smoothing,\" where node features can become indistinguishable after too many propagation steps [@problem_id:3106157]. This practice is essential for grasping both the power and the limitations of basic GNN architectures.", "problem": "You are given an undirected simple graph with weak node features and sparse supervision. Your goal is to implement three propagation-based methods and compare their test accuracies under a controlled set of parameters and to examine the boundary behavior as the number of propagation steps grows. The methods are: label propagation using powers of the normalized adjacency, Simplified Graph Convolution (SGC), and Approximate Personalized Propagation of Neural Predictions (APPNP). You must derive each method from core definitions and implement it as a complete program that produces the requested output format without user input.\n\nStart from the following fundamental base:\n- A graph is represented by an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ with $A_{ij} = 1$ if there is an edge between node $i$ and node $j$, and $A_{ij} = 0$ otherwise. Let $I$ denote the identity matrix. Define $\\tilde{A} = A + I$ to include self-loops. Define the degree matrix $\\tilde{D}$ by $\\tilde{D}_{ii} = \\sum_{j} \\tilde{A}_{ij}$. The symmetric normalized adjacency is $\\hat{A} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$.\n- For any nonnegative integer $K$, the $K$-step propagation by $\\hat{A}$ is $\\hat{A}^K$. For a connected graph with $\\hat{A}$ symmetric and stochastic normalization implied by $\\tilde{A}$ with self-loops, the spectral radius satisfies $\\rho(\\hat{A}) = 1$, and for $K \\to \\infty$ one has convergence of $\\hat{A}^K$ to a rank-$1$ projector onto the principal eigenvector direction under standard conditions (e.g., aperiodicity ensured by self-loops).\n\nGraph, labels, features, and split:\n- Number of nodes $n = 8$ with node indices $\\{0,1,2,3,4,5,6,7\\}$.\n- Edges: there are two dense clusters of size $4$ joined by a single bridge. Specifically:\n  - For all distinct $i,j \\in \\{0,1,2,3\\}$, include the edge $\\{i,j\\}$.\n  - For all distinct $i,j \\in \\{4,5,6,7\\}$, include the edge $\\{i,j\\}$.\n  - Include the bridge edge $\\{3,4\\}$.\n- Ground-truth classes: nodes $\\{0,1,2,3\\}$ belong to class $0$, nodes $\\{4,5,6,7\\}$ belong to class $1$. Let $C = 2$ be the number of classes. Let $Y \\in \\mathbb{R}^{n \\times C}$ be the one-hot encoding of ground-truth labels, that is, $Y_{ic} = 1$ if node $i$ has class $c$, otherwise $Y_{ic} = 0$.\n- Labeled training set indices $L = \\{0,7\\}$ and test set indices $T = \\{1,2,3,4,5,6\\}$. Define the semi-supervised label seed matrix $Y_{0} \\in \\mathbb{R}^{n \\times C}$ by $Y_{0i:} = Y_{i:}$ if $i \\in L$ and $Y_{0i:} = 0$ if $i \\notin L$.\n- Weak node features: feature dimension $d = 3$. Let $X \\in \\mathbb{R}^{n \\times d}$ with entries $X_{ij} = 1 + \\epsilon_{ij} + s_{i} \\cdot \\delta \\cdot \\mathbb{1}\\{j=0\\}$ where $\\epsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^{2})$ are independent noise terms with $\\sigma = 0.02$, and $s_{i} = +1$ if node $i$ is in class $0$ and $s_{i} = -1$ if node $i$ is in class $1$, and $\\delta = 0.05$. This yields a weakly informative first feature and otherwise nearly constant features.\n\nMethods to implement:\n- Label propagation baseline: for a given integer $K \\ge 0$, compute $Y_{\\text{LP}} = \\hat{A}^{K} Y_{0}$, and classify each node $i$ by $\\arg\\max_{c} (Y_{\\text{LP}})_{ic}$. Evaluate accuracy only on the test set indices $T$ by comparing to ground-truth $Y$.\n- Simplified Graph Convolution (SGC): for a given integer $K \\ge 0$, compute $\\tilde{X} = \\hat{A}^{K} X$, fit a linear classifier by multi-output ridge regression on the labeled nodes $L$ to obtain weights $W \\in \\mathbb{R}^{d \\times C}$ that minimize $\\|\\tilde{X}_{L} W - Y_{L}\\|_{F}^{2} + \\lambda \\|W\\|_{F}^{2}$ with $\\lambda = 10^{-2}$, then predict $Z = \\tilde{X} W$ and classify with $\\arg\\max_{c} Z_{ic}$. Evaluate accuracy on $T$.\n- Approximate Personalized Propagation of Neural Predictions (APPNP): initialize $H^{(0)} = Y_{0}$ and iterate for $t = 0,1,\\dots,T_{\\max}-1$ the update $H^{(t+1)} = (1-\\alpha) \\hat{A} H^{(t)} + \\alpha H^{(0)}$ with teleportation parameter $\\alpha = 0.15$ and $T_{\\max} = 100$, then classify with $\\arg\\max_{c} H^{(T_{\\max})}_{ic}$ and evaluate accuracy on $T$.\n\nBoundary analysis to study:\n- For $K \\to \\infty$ in the label propagation and in SGC’s smoothing $\\hat{A}^{K}$, argue what happens to class scores in terms of the spectrum of $\\hat{A}$, and how this affects accuracy when there are few labels and weak features.\n\nTest suite parameters:\n- Use the shared graph, labels, features, and splits as specified above.\n- Evaluate the three methods on the following propagation exponents $K \\in \\{0,1,5,200\\}$ for the label propagation and SGC methods, while APPNP uses fixed $\\alpha = 0.15$ and $T_{\\max} = 100$ for all cases.\n\nRequired outputs:\n- For each $K$ in the test suite, compute the tuple of test accuracies $(a_{\\text{LP}}, a_{\\text{SGC}}, a_{\\text{APPNP}})$, where each $a$ is the fraction of correctly classified nodes in the test set, expressed as a decimal rounded to three decimals.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the order $K \\in \\{0,1,5,200\\}$, and is itself a three-element list of floats $[a_{\\text{LP}},a_{\\text{SGC}},a_{\\text{APPNP}}]$ rounded to three decimals. For example, the format must be like $[[x_{1},y_{1},z_{1}],[x_{2},y_{2},z_{2}],\\dots]$.\n\nAll random quantities must be generated with a fixed seed to ensure deterministic results. No physical units are involved in this problem. Angles are not used. The final output must match the specified single-line format exactly.", "solution": "The user provided a valid problem statement.\n\nThe problem requires the implementation and comparison of three semi-supervised learning methods on graphs: Label Propagation (LP), Simplified Graph Convolution (SGC), and Approximate Personalized Propagation of Neural Predictions (APPNP). We will first establish the common mathematical framework, then derive each method, analyze their behavior in the limit of infinite propagation steps, and finally implement them to obtain the specified test accuracies.\n\n### 1. Graph and Data Representation\n\nThe graph has $n=8$ nodes and consists of two 4-node cliques, $\\{0,1,2,3\\}$ and $\\{4,5,6,7\\}$, connected by a single edge $\\{3,4\\}$. The adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ represents this structure. To incorporate node self-connections, which is standard practice in Graph Neural Networks (GNNs) to ensure aperiodicity and include a node's own features in its update, we define $\\tilde{A} = A + I$, where $I$ is the $n \\times n$ identity matrix.\n\nThe propagation dynamics are governed by the symmetrically normalized adjacency matrix. We define the degree matrix $\\tilde{D}$ of $\\tilde{A}$ as a diagonal matrix with entries $\\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}$. The normalized adjacency matrix is then:\n$$ \\hat{A} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} $$\nThis matrix is symmetric and its eigenvalues $\\lambda$ are bounded, $|\\lambda| \\le 1$. The operation $H' = \\hat{A} H$ can be interpreted as aggregating information from neighboring nodes, where $H$ could be a matrix of node features or labels.\n\nThe data consists of:\n- **Node Features** $X \\in \\mathbb{R}^{n \\times d}$: A matrix where row $i$ is the feature vector of node $i$. The features are designed to be weakly informative.\n- **Ground Truth Labels** $Y \\in \\{0,1\\}^{n \\times C}$: A one-hot encoded matrix of the true class assignments, with $C=2$ classes.\n- **Seed Labels** $Y_0 \\in \\mathbb{R}^{n \\times C}$: A sparse matrix containing the one-hot labels for the training nodes in set $L=\\{0,7\\}$ and zeros for all other nodes. This matrix seeds the propagation process.\n\n### 2. Propagation-Based Methods\n\n**Method 1: Label Propagation (LP)**\nLP is a simple semi-supervised algorithm that diffuses label information from labeled nodes aross the graph. The core idea is that connected nodes are likely to share the same label. The propagation for $K$ steps is achieved by multiplying the seed label matrix $Y_0$ by the $K$-th power of the normalized adjacency matrix $\\hat{A}$.\n\nThe predicted label scores after $K$ steps are given by:\n$$ Y_{\\text{LP}} = \\hat{A}^K Y_0 $$\nThe final class prediction for node $i$ is the one that maximizes its score: $\\arg\\max_c (Y_{\\text{LP}})_{ic}$. For $K=0$, $\\hat{A}^0 = I$, so only the initially labeled nodes have non-zero scores, and all unlabeled nodes are predicted as class $0$ (due to `argmax` tie-breaking).\n\n**Method 2: Simplified Graph Convolution (SGC)**\nSGC simplifies GNNs by decoupling the feature propagation from the non-linear transformation. It consists of two stages:\n1.  **Feature Propagation**: The node features $X$ are smoothed over the graph by applying the propagation matrix $\\hat{A}^K$:\n    $$ \\tilde{X} = \\hat{A}^K X $$\n    This step effectively creates new features for each node by aggregating feature information from its $K$-hop neighborhood.\n2.  **Linear Classification**: A linear classifier is trained on the propagated features of the labeled nodes. We seek a weight matrix $W \\in \\mathbb{R}^{d \\times C}$ that minimizes a ridge regression objective:\n    $$ \\underset{W}{\\text{minimize}} \\quad \\|\\tilde{X}_L W - Y_L\\|_F^2 + \\lambda \\|W\\|_F^2 $$\n    where $\\tilde{X}_L$ and $Y_L$ are the rows of $\\tilde{X}$ and $Y$ corresponding to the labeled set $L$, and $\\| \\cdot \\|_F$ is the Frobenius norm. The regularization parameter is given as $\\lambda=10^{-2}$. This optimization problem has a closed-form solution:\n    $$ W = (\\tilde{X}_L^T \\tilde{X}_L + \\lambda I_d)^{-1} \\tilde{X}_L^T Y_L $$\n    where $I_d$ is the $d \\times d$ identity matrix.\nPredictions for all nodes are then computed as $Z = \\tilde{X} W$, and the class for node $i$ is $\\arg\\max_c Z_{ic}$.\n\n**Method 3: Approximate Personalized Propagation of Neural Predictions (APPNP)**\nAPPNP is inspired by Personalized PageRank and is designed to combat the over-smoothing problem inherent in deep GNNs. It maintains a balance between propagating neighborhood information and preserving the initial \"root\" information. While the original APPNP uses the output of a neural network as the root information, here we use the seed labels $Y_0$.\n\nStarting with $H^{(0)} = Y_0$, APPNP performs the following iterative update for $t=0, 1, \\dots, T_{\\max}-1$:\n$$ H^{(t+1)} = (1-\\alpha) \\hat{A} H^{(t)} + \\alpha H^{(0)} $$\nHere, $\\alpha \\in [0,1]$ is the teleport probability, given as $\\alpha=0.15$. At each step, the process involves one step of neighborhood aggregation (the $\\hat{A}H^{(t)}$ term) and a \"teleport\" back to the initial predictions (the $H^{(0)}$ term). This ensures that the influence from the seed nodes is never completely lost. The final predictions are taken from a converged or near-converged state, here $H^{(T_{\\max})}$, with $T_{\\max}=100$. Classification is performed via $\\arg\\max_c (H^{(T_{\\max})})_{ic}$.\n\n### 3. Boundary Analysis for $K \\to \\infty$\n\nThe behavior of LP and SGC for a large number of propagation steps $K$ is dictated by the spectral properties of $\\hat{A}$. Since the graph is connected and aperiodic (due to self-loops), the Perron-Frobenius theorem implies that the largest eigenvalue of $\\hat{A}$ is $\\lambda_1 = 1$ with multiplicity one. All other eigenvalues $|\\lambda_i| < 1$. As $K \\to \\infty$, the powers of $\\hat{A}$ converge to a rank-1 projector onto the principal eigenvector $u_1$:\n$$ \\lim_{K \\to \\infty} \\hat{A}^K = u_1 u_1^T $$\nwhere $u_1$ is the normalized eigenvector corresponding to $\\lambda_1=1$. For the symmetric normalization, this eigenvector is given by $(u_1)_i \\propto \\sqrt{\\tilde{D}_{ii}}$.\n\n**LP Over-smoothing**:\nFor large $K$, the LP predictions become:\n$$ Y_{\\text{LP}} \\approx (u_1 u_1^T) Y_0 = u_1 (u_1^T Y_0) $$\nThe term $v^T = u_1^T Y_0$ is a $1 \\times C$ row vector of constants. The score matrix $Y_{\\text{LP}}$ becomes $u_1 v^T$, meaning the score vector for node $i$ is $(u_1)_i v^T$. Since $(u_1)_i > 0$, the classification decision $\\arg\\max_c ((u_1)_i v_c)$ simplifies to $\\arg\\max_c v_c$, which is independent of the node $i$. Consequently, all nodes receive the same prediction. This phenomenon is known as \"over-smoothing\". The test accuracy will collapse to the proportion of test nodes belonging to this single predicted class. In this problem, the degrees of nodes $0$ and $7$ are equal, so the weights they contribute to the final aggregated score are equal, leading to a tie that `argmax` breaks in favor of class $0$. This yields an accuracy of $3/6 = 0.5$.\n\n**SGC Over-smoothing**:\nA similar fate befalls SGC. For large $K$, the smoothed features become:\n$$ \\tilde{X} \\approx (u_1 u_1^T) X = u_1 (u_1^T X) $$\nThe term $f^T = u_1^T X$ is a $1 \\times d$ row vector. The feature vector for any node $i$ becomes $(\\tilde{X})_{i:} = (u_1)_i f^T$. All node features become collinear, pointing in the direction of $f^T$. A linear classifier trained on such features will make the same prediction for every node, as the separating hyperplane will divide the feature space, and all feature vectors lie on the same line. This again results in an accuracy of $0.5$.\n\n**APPNP's Resistance to Over-smoothing**:\nAPPNP avoids this issue due to the teleportation mechanism. The converged solution for APPNP is $H^{(\\infty)} = \\alpha(I - (1-\\alpha)\\hat{A})^{-1} H^{(0)}$. The presence of the identity matrix $I$ and the teleport probability $\\alpha$ prevents the propagation from completely washing out the initial information in $H^{(0)}$. It ensures that the final predictions remain \"personalized\" to the initial seed nodes, thus maintaining high accuracy even on structures susceptible to over-smoothing.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates Label Propagation, SGC, and APPNP on a\n    specified graph and dataset.\n    \"\"\"\n\n    def build_graph():\n        \"\"\"Constructs the adjacency matrix for the specified graph.\"\"\"\n        n = 8\n        A = np.zeros((n, n), dtype=np.float64)\n        # Clique on {0, 1, 2, 3}\n        for i in range(4):\n            for j in range(i + 1, 4):\n                A[i, j] = A[j, i] = 1.0\n        # Clique on {4, 5, 6, 7}\n        for i in range(4, 8):\n            for j in range(i + 1, 8):\n                A[i, j] = A[j, i] = 1.0\n        # Bridge {3, 4}\n        A[3, 4] = A[4, 3] = 1.0\n        return A\n\n    def normalize_adjacency(A):\n        \"\"\"Computes the symmetrically normalized adjacency matrix.\"\"\"\n        A_tilde = A + np.eye(A.shape[0])\n        D_tilde_diag = A_tilde.sum(axis=1)\n        # Replace zeros in D_tilde_diag to avoid division by zero, though not expected here.\n        D_tilde_diag[D_tilde_diag == 0] = 1e-12\n        D_tilde_inv_sqrt = np.diag(np.power(D_tilde_diag, -0.5))\n        A_hat = D_tilde_inv_sqrt @ A_tilde @ D_tilde_inv_sqrt\n        return A_hat\n\n    def generate_data(n, seed):\n        \"\"\"Generates labels, features, and data splits.\"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Ground truth labels\n        y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n        C = 2\n        Y = np.eye(C, dtype=np.float64)[y_true]\n        \n        # Labeled and test sets\n        L = [0, 7]\n        T = [1, 2, 3, 4, 5, 6]\n        \n        # Seed labels (for LP and APPNP)\n        Y_0 = np.zeros_like(Y)\n        Y_0[L] = Y[L]\n        \n        # Node features\n        d = 3\n        sigma = 0.02\n        delta = 0.05\n        s = np.array([1, 1, 1, 1, -1, -1, -1, -1])\n        \n        noise = rng.normal(0, sigma, size=(n, d))\n        signal = np.zeros((n, d))\n        signal[:, 0] = s * delta\n        \n        X = 1.0 + noise + signal\n        \n        return X, Y, y_true, Y_0, L, T\n\n    def solve_lp(A_hat, Y_0, K, T, y_true):\n        \"\"\"Calculates test accuracy for Label Propagation.\"\"\"\n        if K == 0:\n            A_hat_K = np.eye(A_hat.shape[0])\n        else:\n            A_hat_K = np.linalg.matrix_power(A_hat, K)\n        \n        Y_lp = A_hat_K @ Y_0\n        preds = np.argmax(Y_lp, axis=1)\n        \n        accuracy = np.mean(preds[T] == y_true[T])\n        return accuracy\n\n    def solve_sgc(A_hat, X, Y, K, T, y_true, L, reg_lambda):\n        \"\"\"Calculates test accuracy for Simplified Graph Convolution.\"\"\"\n        if K == 0:\n            A_hat_K = np.eye(A_hat.shape[0])\n        else:\n            A_hat_K = np.linalg.matrix_power(A_hat, K)\n            \n        X_tilde = A_hat_K @ X\n        \n        # Ridge regression\n        X_L = X_tilde[L]\n        Y_L = Y[L]\n        \n        d = X.shape[1]\n        term1 = X_L.T @ X_L + reg_lambda * np.eye(d)\n        term2 = X_L.T @ Y_L\n        W = np.linalg.solve(term1, term2)\n        \n        Z = X_tilde @ W\n        preds = np.argmax(Z, axis=1)\n        \n        accuracy = np.mean(preds[T] == y_true[T])\n        return accuracy\n\n    def solve_appnp(A_hat, Y_0, T, y_true, alpha, T_max):\n        \"\"\"Calculates test accuracy for APPNP.\"\"\"\n        H = Y_0.copy()\n        for _ in range(T_max):\n            H = (1 - alpha) * (A_hat @ H) + alpha * Y_0\n        \n        preds = np.argmax(H, axis=1)\n        \n        accuracy = np.mean(preds[T] == y_true[T])\n        return accuracy\n\n    # Main execution logic\n    n_nodes = 8\n    random_seed = 42 # For reproducibility\n    \n    A = build_graph()\n    A_hat = normalize_adjacency(A)\n    X, Y, y_true, Y_0, L, T = generate_data(n=n_nodes, seed=random_seed)\n    \n    test_suite_K = [0, 1, 5, 200]\n    sgc_lambda = 1e-2\n    appnp_alpha = 0.15\n    appnp_T_max = 100\n    \n    all_results = []\n    \n    # APPNP result is constant for all K\n    acc_appnp = solve_appnp(A_hat, Y_0, T, y_true, appnp_alpha, appnp_T_max)\n    \n    for K in test_suite_K:\n        acc_lp = solve_lp(A_hat, Y_0, K, T, y_true)\n        acc_sgc = solve_sgc(A_hat, X, Y, K, T, y_true, L, sgc_lambda)\n        \n        all_results.append([acc_lp, acc_sgc, acc_appnp])\n\n    # Format output string without spaces\n    result_strings = []\n    for r in all_results:\n        inner_str = '[' + ','.join([f'{val:.3f}' for val in r]) + ']'\n        result_strings.append(inner_str)\n    \n    final_output = '[' + ','.join(result_strings) + ']'\n    print(final_output)\n\nsolve()\n\n```", "id": "3106157"}, {"introduction": "While basic message passing treats all neighbors equally, more advanced GNNs can learn to weigh information adaptively. This practice explores the power of attention mechanisms, particularly in challenging scenarios like \"heterophily,\" where connected nodes tend to have different attributes or labels. By implementing a GAT-style aggregator on a specially designed graph, you will see firsthand how attention allows a GNN to selectively focus on the most relevant neighbors, dramatically improving its predictive power [@problem_id:3131968].", "problem": "You are given a small suite of synthetic graphs designed to exhibit strong heterophily, where neighboring nodes predominantly have opposite attribute values. The task is to perform graph-level classification using two different message-passing schemes within a Graph Neural Network (GNN): a mean aggregator and an attention-based aggregator. The graphs are stars with a single hub (center) node connected to $n$ leaves. Each node has a feature vector in $\\mathbb{R}^2$ with components $(x,z)$, where $x \\in \\{-1,+1\\}$ encodes a semantic type and $z \\in \\{0,1\\}$ encodes a latent marker. Strong heterophily is enforced by setting each leaf node’s $x$ to be the opposite sign of the hub’s $x$. The graph label $y \\in \\{0,1\\}$ equals the hub node’s marker $z$.\n\nYou must implement two message-passing layers ($L=2$) for both schemes:\n- Mean aggregation (excluding self-loops): For node $u$, the update at layer $\\ell$ is\n$$\nh_u^{(\\ell+1)} = \\frac{1}{|\\mathcal{N}(u)|} \\sum_{v \\in \\mathcal{N}(u)} W h_v^{(\\ell)},\n$$\nwhere $W \\in \\mathbb{R}^{2 \\times 2}$ is the identity matrix, $h_u^{(0)} = [x_u, z_u]^\\top$, and $\\mathcal{N}(u)$ denotes the set of neighbors of $u$.\n\n- Attention-based aggregation with self-loops following the Graph Attention Network (GAT): For node $u$, define the unnormalized attention score\n$$\ne_{uv} = a^\\top [W h_u^{(\\ell)} \\,\\|\\, W h_v^{(\\ell)}],\n$$\nwhere $[\\cdot \\,\\|\\, \\cdot]$ denotes concatenation, $W \\in \\mathbb{R}^{2 \\times 2}$ is the identity matrix, and $a \\in \\mathbb{R}^{4}$ is a fixed attention vector. Use the softmax to normalize over the neighborhood that includes a self-loop,\n$$\n\\alpha_{uv} = \\frac{\\exp(e_{uv})}{\\sum_{t \\in \\mathcal{N}'(u)} \\exp(e_{ut})}, \\quad \\mathcal{N}'(u) = \\mathcal{N}(u) \\cup \\{u\\},\n$$\nand update\n$$\nh_u^{(\\ell+1)} = \\sum_{v \\in \\mathcal{N}'(u)} \\alpha_{uv} \\, W h_v^{(\\ell)}.\n$$\nUse the fixed attention vector\n$$\na = [2, 1, 2, 1]^\\top.\n$$\n\nAfter $L=2$ layers, compute a graph-level decision as follows. Let $N$ be the number of nodes in the graph. Define the fraction\n$$\nf = \\frac{1}{N} \\sum_{u} \\mathbf{1}\\{h_{u,z}^{(L)} \\geq 0.5\\},\n$$\nwhere $h_{u,z}^{(L)}$ is the $z$-component of $h_u^{(L)}$ and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. Predict\n$$\n\\hat{y} = \\begin{cases}\n1 & \\text{if } f \\geq 0.5, \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n$$\n\nYou must implement both schemes on the following test suite of star graphs, each specified by $(n, x_{\\text{center}}, z_{\\text{center}})$, where $n$ is the number of leaves, $x_{\\text{center}} \\in \\{-1,+1\\}$ is the hub’s $x$ value, and $z_{\\text{center}} \\in \\{0,1\\}$ is the hub’s marker. In every case, each leaf has $x_{\\text{leaf}} = - x_{\\text{center}}$ and $z_{\\text{leaf}} = 0$.\n\nTest suite:\n- Case $1$: $(n = 3, x_{\\text{center}} = +1, z_{\\text{center}} = 1)$.\n- Case $2$: $(n = 3, x_{\\text{center}} = +1, z_{\\text{center}} = 0)$.\n- Case $3$: $(n = 5, x_{\\text{center}} = -1, z_{\\text{center}} = 1)$.\n- Case $4$: $(n = 5, x_{\\text{center}} = -1, z_{\\text{center}} = 0)$.\n- Case $5$: $(n = 1, x_{\\text{center}} = +1, z_{\\text{center}} = 1)$.\n\nFor each case, the ground-truth label is $y = z_{\\text{center}}$.\n\nYour program must:\n- Construct the adjacency matrix and initial node features for each case.\n- Apply two layers ($L=2$) of mean aggregation and attention-based aggregation as specified.\n- Produce the predicted labels $\\hat{y}_{\\text{mean}}$ and $\\hat{y}_{\\text{attn}}$ for each case.\n- Compute the overall accuracies across all cases as decimals (not percentages), namely\n$$\n\\text{acc}_{\\text{mean}} = \\frac{\\text{number of correct } \\hat{y}_{\\text{mean}}}{5}, \\qquad \\text{acc}_{\\text{attn}} = \\frac{\\text{number of correct } \\hat{y}_{\\text{attn}}}{5}.\n$$\n- For each case, also compute a boolean indicator $b$ that equals $1$ if attention is correct and mean is incorrect, and equals $0$ otherwise.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$$\n[\\hat{y}_{\\text{mean},1}, \\hat{y}_{\\text{attn},1}, \\hat{y}_{\\text{mean},2}, \\hat{y}_{\\text{attn},2}, \\hat{y}_{\\text{mean},3}, \\hat{y}_{\\text{attn},3}, \\hat{y}_{\\text{mean},4}, \\hat{y}_{\\text{attn},4}, \\hat{y}_{\\text{mean},5}, \\hat{y}_{\\text{attn},5}, \\text{acc}_{\\text{mean}}, \\text{acc}_{\\text{attn}}, B],\n$$\nwhere $B$ is the total number of cases for which attention outperforms mean,\n$$\nB = \\sum_{i=1}^{5} b_i.\n$$\nNo physical units are involved. Angles are not involved. All quantities are dimensionless. Ensure scientific realism by following the exact aggregation definitions above. The program must be self-contained and not require external input.\n\nExplain, in your solution, from first principles, why attention helps under label disparity in heterophilous graphs by weighting messages according to feature compatibility rather than indiscriminate averaging.", "solution": "The user-provided problem is rigorously defined and scientifically sound, situated within the established domain of graph neural networks (GNNs). It presents a didactic scenario to contrast the behavior of mean and attention-based aggregation mechanisms on heterophilous graphs. All parameters, update rules, and evaluation metrics are specified without ambiguity. The problem is therefore deemed valid and a full solution is provided below.\n\nThe core task is to classify star-structured graphs characterized by strong heterophily. In these graphs, a central hub node and its connected leaf nodes have opposing semantic features ($x_{\\text{center}} = -x_{\\text{leaf}}$). The graph's label $y \\in \\{0, 1\\}$ is determined solely by a latent marker $z_{\\text{center}}$ on the hub node. This setup is designed to challenge standard GNN aggregators, which typically assume homophily (i.e., that connected nodes are similar). We will analyze and implement two aggregation schemes: mean aggregation and attention-based aggregation.\n\nLet the graph have $N=n+1$ nodes, with node $u=0$ as the center and nodes $u=1, \\dots, n$ as the leaves. The initial features (at layer $\\ell=0$) are $h_0^{(0)} = [x_c, z_c]^\\top$ for the center and $h_i^{(0)} = [-x_c, 0]^\\top$ for any leaf $i$.\n\n### Analysis of Mean Aggregation\n\nThe mean aggregation update rule is given by:\n$$\nh_u^{(\\ell+1)} = \\frac{1}{|\\mathcal{N}(u)|} \\sum_{v \\in \\mathcal{N}(u)} W h_v^{(\\ell)}\n$$\nWith the weight matrix $W$ being the identity matrix $I$, this simplifies to averaging the feature vectors of a node's neighbors.\n\n**Layer 1 ($L=1$) Update:**\n- **Hub Node ($u=0$):** The hub is connected to all $n$ leaves. Its updated feature vector is the average of the initial leaf features.\n$$\nh_0^{(1)} = \\frac{1}{n} \\sum_{i=1}^n h_i^{(0)} = \\frac{1}{n} \\sum_{i=1}^n \\begin{pmatrix} -x_c \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -x_c \\\\ 0 \\end{pmatrix}\n$$\nCritically, the hub's original marker $z_c$, which contains the ground-truth label information, is immediately erased and replaced with $0$. The hub's features become identical to those of a leaf.\n\n- **Leaf Nodes ($u=i \\in \\{1,\\dots,n\\}$):** Each leaf is connected only to the hub.\n$$\nh_i^{(1)} = \\frac{1}{1} h_0^{(0)} = \\begin{pmatrix} x_c \\\\ z_c \\end{pmatrix}\n$$\nThe leaves inherit the hub's original features, including the marker $z_c$.\n\n**Layer 2 ($L=2$) Update:**\n- **Hub Node ($u=0$):** The hub now averages the features of the leaves from the previous layer.\n$$\nh_0^{(2)} = \\frac{1}{n} \\sum_{i=1}^n h_i^{(1)} = \\frac{1}{n} \\sum_{i=1}^n \\begin{pmatrix} x_c \\\\ z_c \\end{pmatrix} = \\begin{pmatrix} x_c \\\\ z_c \\end{pmatrix}\n$$\nThe hub's features revert to their original state, restoring the marker $z_c$.\n\n- **Leaf Nodes ($u=i \\in \\{1,\\dots,n\\}$):** Each leaf receives the features of the hub from the previous layer.\n$$\nh_i^{(2)} = h_0^{(1)} = \\begin{pmatrix} -x_c \\\\ 0 \\end{pmatrix}\n$$\nThe leaves revert to their original features.\n\n**Prediction with Mean Aggregation:**\nAfter $L=2$ layers, the final $z$-components of the feature vectors are $h_{0,z}^{(2)} = z_c$ and $h_{i,z}^{(2)} = 0$ for all leaves $i$. The prediction $\\hat{y}$ is based on the fraction of nodes $f$ for which $h_{u,z}^{(2)} \\geq 0.5$.\n\n- If the true label is $y=z_c=1$: The condition $h_{u,z}^{(2)} \\geq 0.5$ is met only by the hub node ($u=0$). The number of such nodes is $1$. The fraction is $f = 1/N = 1/(n+1)$. The prediction is $\\hat{y}_{\\text{mean}} = 1$ if and only if $1/(n+1) \\geq 0.5$, which simplifies to $n \\leq 1$. For any star graph with more than one leaf ($n>1$), the prediction will be $\\hat{y}_{\\text{mean}}=0$, which is incorrect.\n\n- If the true label is $y=z_c=0$: No node satisfies $h_{u,z}^{(2)} \\geq 0.5$. The count is $0$, so $f=0$. The prediction is $\\hat{y}_{\\text{mean}}=0$, which is always correct.\n\nIn summary, mean aggregation fails on these heterophilous graphs whenever the label is $1$ and the graph is larger than a simple pair of nodes.\n\n### Analysis of Attention-Based Aggregation\n\nThe GAT-style attention mechanism updates features as:\n$$\nh_u^{(\\ell+1)} = \\sum_{v \\in \\mathcal{N}'(u)} \\alpha_{uv} \\, W h_v^{(\\ell)}\n$$\nwhere $\\mathcal{N}'(u) = \\mathcal{N}(u) \\cup \\{u\\}$ includes a self-loop. The attention coefficients $\\alpha_{uv}$ are computed via a softmax over scores $e_{uv} = a^\\top [W h_u^{(\\ell)} \\,\\|\\, W h_v^{(\\ell)}]$. With $W=I$ and $a = [2, 1, 2, 1]^\\top$, the score is $e_{uv} = 2x_u^{(\\ell)} + z_u^{(\\ell)} + 2x_v^{(\\ell)} + z_v^{(\\ell)}$.\n\nThe key difference from mean aggregation is two-fold:\n1.  **Self-Loops:** A node `u` can attend to itself, allowing it to preserve its own features via the weight $\\alpha_{uu}$.\n2.  **Feature-Dependent Weights:** The weights $\\alpha_{uv}$ are not uniform; they depend on the features of both the source node $v$ and the target node $u$. This allows the model to learn or, in this case, be designed to prioritize messages from more \"compatible\" or important neighbors.\n\nLet's analyze the behavior when $y=z_c=1$.\n**Layer 1 ($L=1$) Update:**\n- **Hub Node ($u=0$):** The hub attends to itself and the $n$ leaves.\n    - **Self-attention score:** $e_{00} = 2x_c^{(0)} + z_c^{(0)} + 2x_c^{(0)} + z_c^{(0)} = 4x_c + 2(1) = 4x_c+2$.\n    - **Neighbor attention score (to leaf $i$):** $e_{0i} = 2x_c^{(0)} + z_c^{(0)} + 2x_i^{(0)} + z_i^{(0)} = 2x_c + 1 + 2(-x_c) + 0 = 1$.\nThe score for the self-loop ($e_{00}$) is significantly different from the score for neighbors ($e_{0i}$). For $x_c=+1$, $e_{00}=6$; for $x_c=-1$, $e_{00}=-2$. In either case, the exponentiated self-attention score $\\exp(e_{00})$ will be substantially different from the neighbor score $\\exp(1)$, causing the self-attention weight $\\alpha_{00}$ to be large. This mechanism allows the hub to largely retain its own features, preventing the critical marker $z_c=1$ from being averaged out by the leaves' $z=0$ features.\n\n- **Leaf Nodes ($u=i$):** Each leaf attends to itself and the hub.\n    - **Self-attention score:** $e_{ii} = 4x_i^{(0)} + 2z_i^{(0)} = 4(-x_c) + 0 = -4x_c$.\n    - **Neighbor attention score (to hub $0$):** $e_{i0} = 2x_i^{(0)} + z_i^{(0)} + 2x_c^{(0)} + z_c^{(0)} = 2(-x_c) + 0 + 2x_c + 1 = 1$.\nThe score for attending to the hub is $1$, while the self-attention score is $\\pm 4$. The weight $\\alpha_{i0}$ will be high, meaning the leaf incorporates a large portion of the hub's features. Specifically, the $z$-component of the leaf's new feature vector $h_{i,z}^{(1)}$ will be a weighted average where the hub's feature $h_{0,z}^{(0)}=1$ receives a high weight.\n\n**Outcome of Attention Mechanism:**\nWhen $z_c=1$, the hub preserves its marker $z_c=1$ due to strong self-attention. Simultaneously, the leaves strongly attend to the hub, and their $z$-components are updated from $0$ to a value close to $1$. After the first layer, all nodes in the graph will have a $z$-component $h_{u,z}^{(1)}$ significantly greater than $0.5$. In the second layer, the nodes are now highly similar (homophilous) in their features, and aggregation will maintain this property. The final features $h_{u,z}^{(2)}$ will remain high for all nodes.\n\n**Prediction with Attention Aggregation:**\n- If $y=z_c=1$: All $N=n+1$ nodes will have $h_{u,z}^{(2)} \\geq 0.5$. The fraction is $f = (n+1)/(n+1) = 1$. The prediction is $\\hat{y}_{\\text{attn}} = 1$, which is correct regardless of $n$.\n- If $y=z_c=0$: The attention scores change. For the hub, $e_{0i}=0$. For the leaf, $e_{i0}=0$. The $z$-component of all messages is $0$, so the final $z$-components of all nodes remain $0$. The fraction is $f=0$, and the prediction $\\hat{y}_{\\text{attn}}=0$ is correct.\n\nTherefore, the attention-based aggregator correctly classifies all cases in the test suite by selectively weighting information based on feature compatibility, using self-loops to preserve essential local information and directed attention to propagate it where needed. The specific attention vector $a$ is well-chosen to reward attending to the informative hub when its marker is active ($z_c=1$).\n\nThe numerical calculations for each test case are performed by the program in the `<final_answer>` section, which implements these described update rules for the two layers.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GNN classification problem for a suite of synthetic heterophilous graphs.\n    \"\"\"\n    \n    # Test suite: (n, x_center, z_center)\n    test_cases = [\n        (3, 1, 1),   # Case 1\n        (3, 1, 0),   # Case 2\n        (5, -1, 1),  # Case 3\n        (5, -1, 0),  # Case 4\n        (1, 1, 1),   # Case 5\n    ]\n    \n    results_list = []\n    y_preds_mean = []\n    y_preds_attn = []\n    y_trues = []\n\n    for n, x_c, z_c in test_cases:\n        # --- 1. Graph and Feature Initialization ---\n        num_nodes = n + 1\n        y_true = z_c\n        y_trues.append(y_true)\n\n        # Initial features h^(0)\n        h0 = np.zeros((num_nodes, 2), dtype=np.float64)\n        h0[0] = [x_c, z_c]  # Hub node\n        if n > 0:\n            h0[1:] = [-x_c, 0] # Leaf nodes\n\n        # Adjacency matrix\n        adj = np.zeros((num_nodes, num_nodes), dtype=int)\n        if n > 0:\n            adj[0, 1:] = 1\n            adj[1:, 0] = 1\n\n        # --- 2. Mean Aggregation ---\n        h_mean = h0.copy()\n        \n        # Apply L=2 layers\n        for _ in range(2):\n            h_next = np.zeros_like(h_mean)\n            for u in range(num_nodes):\n                neighbors = np.where(adj[u] == 1)[0]\n                if len(neighbors) == 0:\n                    # Handle disconnected node (n=0 case which is not in test suite)\n                    # A node with no neighbors keeps its features in mean aggregation.\n                    h_next[u] = h_mean[u]\n                    continue\n\n                neighbor_features = h_mean[neighbors]\n                aggregated_message = np.sum(neighbor_features, axis=0) / len(neighbors)\n                h_next[u] = aggregated_message\n            h_mean = h_next\n\n        # Prediction for mean aggregation\n        z_components_mean = h_mean[:, 1]\n        count_mean = np.sum(z_components_mean >= 0.5)\n        f_mean = count_mean / num_nodes if num_nodes > 0 else 0\n        y_pred_mean = 1 if f_mean >= 0.5 else 0\n        \n        y_preds_mean.append(y_pred_mean)\n        results_list.extend([y_pred_mean])\n\n        # --- 3. Attention-based Aggregation ---\n        h_attn = h0.copy()\n        a = np.array([2.0, 1.0, 2.0, 1.0])\n        \n        # Apply L=2 layers\n        for _ in range(2):\n            h_next = np.zeros_like(h_attn)\n            for u in range(num_nodes):\n                # Neighborhood with self-loop\n                neighbors_prime_list = list(np.where(adj[u] == 1)[0])\n                neighbors_prime_list.append(u)\n                neighbors_prime = np.array(neighbors_prime_list, dtype=int)\n\n                # Compute unnormalized attention scores e_uv\n                e = np.zeros(len(neighbors_prime))\n                h_u = h_attn[u]\n                for j, v in enumerate(neighbors_prime):\n                    h_v = h_attn[v]\n                    concat_features = np.concatenate((h_u, h_v))\n                    e[j] = np.dot(a, concat_features)\n                \n                # Normalize using softmax\n                # Subtract max for numerical stability, although not strictly necessary here.\n                e_stable = e - np.max(e)\n                exp_e = np.exp(e_stable)\n                alpha = exp_e / np.sum(exp_e)\n                \n                # Apply attention weights\n                neighbor_features = h_attn[neighbors_prime]\n                h_next[u] = np.sum(neighbor_features * alpha[:, np.newaxis], axis=0)\n\n            h_attn = h_next\n            \n        # Prediction for attention aggregation\n        z_components_attn = h_attn[:, 1]\n        count_attn = np.sum(z_components_attn >= 0.5)\n        f_attn = count_attn / num_nodes if num_nodes > 0 else 0\n        y_pred_attn = 1 if f_attn >= 0.5 else 0\n        \n        y_preds_attn.append(y_pred_attn)\n        results_list.extend([y_pred_attn])\n\n    # --- 4. Final Metrics Calculation ---\n    y_preds_mean = np.array(y_preds_mean)\n    y_preds_attn = np.array(y_preds_attn)\n    y_trues = np.array(y_trues)\n\n    num_cases = len(test_cases)\n    \n    # Accuracies\n    acc_mean = np.sum(y_preds_mean == y_trues) / num_cases\n    acc_attn = np.sum(y_preds_attn == y_trues) / num_cases\n    \n    # B: count of cases where attention is correct AND mean is incorrect\n    attn_correct = (y_preds_attn == y_trues)\n    mean_incorrect = (y_preds_mean != y_trues)\n    B = np.sum(attn_correct & mean_incorrect)\n\n    results_list.extend([acc_mean, acc_attn, B])\n    \n    # Format the final output string\n    output_str = \",\".join([\n        str(int(x)) if i < 10 or i == 12 else f\"{x:.1f}\" for i, x in enumerate(results_list)\n    ])\n    \n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3131968"}, {"introduction": "Beyond classifying nodes, GNNs are highly effective at understanding the relational structure of a graph, making them ideal for tasks like link prediction. This exercise demonstrates how to apply GNN principles to infer missing connections by scoring potential edges based on local structural information, such as the number of shared neighbors (triadic closure). You will implement a parameter-free GNN to generate node representations and use them to predict links, evaluating your model with the industry-standard Average Precision metric to understand the precision-recall tradeoff [@problem_id:3131905].", "problem": "You are given the task of designing a purely algorithmic Graph Neural Network (GNN)-based procedure that infers missing edges in undirected graphs using a local structural prior akin to triadic closure. The procedure must be derived from foundational definitions in graph theory and message passing, and must quantify the precision-recall tradeoff using a principled aggregate metric. The final result must be computed for a fixed test suite and output in a precisely specified format.\n\nThe fundamental base for the derivation is as follows.\n- A graph is represented by an undirected adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ with $A_{ij} = A_{ji}$ and $A_{ii} = 0$. The set of nodes is indexed by $\\{0,1,\\dots,n-1\\}$ and the set of edges is $E = \\{(i,j) \\mid i \\lt j, A_{ij} = 1\\}$.\n- Message passing in a Graph Neural Network (GNN) aggregates neighboring features. With self-loops added, the augmented adjacency $\\tilde{A} = A + I$, and degree matrix $\\tilde{D} = \\operatorname{diag}(\\sum_j \\tilde{A}_{ij})$, a symmetric normalization yields $\\hat{A} = \\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$.\n- Beginning from identity node features $X = I \\in \\mathbb{R}^{n \\times n}$, a two-step neighborhood aggregation without learned parameters yields the representation $H = \\hat{A}^2 X = \\hat{A}^2$. This captures second-order neighborhood information used by triadic closure, because larger values in $H$ reflect stronger connectivity through shared neighbors.\n\nYour program must implement the following pipeline.\n1. For each test graph, two matrices are provided: the observed adjacency $A^{\\text{obs}}$ and the true adjacency $A^{\\text{true}}$. The observed adjacency $A^{\\text{obs}}$ is the input graph from which the GNN must infer missing edges. The true adjacency $A^{\\text{true}}$ determines which missing edges are truly present in the underlying graph and which are truly absent.\n2. Compute the symmetric normalized adjacency $\\hat{A}^{\\text{obs}}$ from $A^{\\text{obs}}$ using $\\tilde{A}^{\\text{obs}} = A^{\\text{obs}} + I$ and $\\hat{A}^{\\text{obs}} = (\\tilde{D}^{\\text{obs}})^{-1/2}\\tilde{A}^{\\text{obs}}(\\tilde{D}^{\\text{obs}})^{-1/2}$ with $\\tilde{D}^{\\text{obs}} = \\operatorname{diag}(\\sum_j \\tilde{A}^{\\text{obs}}_{ij})$.\n3. Compute the two-layer message-passing representations $H = (\\hat{A}^{\\text{obs}})^2$.\n4. For every unordered node pair $(i,j)$ with $i \\lt j$ that is a non-edge in $A^{\\text{obs}}$ (i.e., $A^{\\text{obs}}_{ij} = 0$), compute a scalar link score $s(i,j)$ using the inner product of node representations followed by a logistic squashing: \n   $$ s(i,j) = \\sigma\\big(H_{i,:}^\\top H_{j,:}\\big), \\quad \\sigma(z) = \\frac{1}{1 + e^{-z}}. $$\n5. Define binary labels $y(i,j)$ for each candidate pair as follows:\n   - If $(i,j)$ is an edge in $A^{\\text{true}}$ (i.e., $A^{\\text{true}}_{ij} = 1$), then $y(i,j) = 1$ and the pair is a truly missing edge to be recovered.\n   - If $(i,j)$ is not an edge in $A^{\\text{true}}$ (i.e., $A^{\\text{true}}_{ij} = 0$), then $y(i,j) = 0$ and the pair is a truly absent edge that should not be proposed.\n6. Measure the precision-recall tradeoff by computing the Average Precision (AP), defined as the area under the precision-recall curve induced by ranking candidate pairs by $s(i,j)$ in descending order. Let there be $m$ positive labels overall. If the ranked list of candidates has labels $\\{y_1, y_2, \\dots, y_N\\}$ and precision at rank $k$ is $P(k) = \\frac{1}{k}\\sum_{i=1}^k y_i$, then the Average Precision is\n   $$ \\mathrm{AP} = \\begin{cases}\n   \\frac{1}{m}\\sum_{k=1}^N P(k)\\cdot \\mathbb{1}\\{y_k = 1\\}, & \\text{if } m > 0, \\\\\n   0, & \\text{if } m = 0,\n   \\end{cases} $$\n   which is a standard, threshold-free summary of the precision-recall tradeoff. Precision is defined as $ \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}} $ and recall as $ \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} $, where $\\mathrm{TP}$, $\\mathrm{FP}$, and $\\mathrm{FN}$ are the counts of true positives, false positives, and false negatives at a given threshold.\n\nThe test suite consists of three undirected graphs, each specified by $(A^{\\text{obs}}, A^{\\text{true}})$ with node indices starting at $0$. In all cases, graphs are simple (no parallel edges), unweighted, and symmetric.\n- Test Case $1$ (triadic closure-friendly sparse graph):\n  - Number of nodes $n = 6$.\n  - Observed edges $E^{\\text{obs}} = \\{(0,1),(1,2),(2,3),(3,4),(4,5)\\}$.\n  - True edges $E^{\\text{true}} = E^{\\text{obs}} \\cup \\{(0,2),(2,4)\\}$.\n  - Candidate pairs are all $(i,j)$ with $i \\lt j$ and $(i,j) \\notin E^{\\text{obs}}$. Among them, the truly missing edges are $(0,2)$ and $(2,4)$, while all other candidate pairs are truly absent.\n- Test Case $2$ (boundary case with no initial triangles):\n  - Number of nodes $n = 5$.\n  - Observed edges $E^{\\text{obs}} = \\{(0,1),(1,2),(2,3),(3,4)\\}$.\n  - True edges $E^{\\text{true}} = E^{\\text{obs}} \\cup \\{(1,3)\\}$.\n  - Candidate pairs are all $(i,j)$ with $i \\lt j$ and $(i,j) \\notin E^{\\text{obs}}$. Among them, the truly missing edge is $(1,3)$, while all other candidate pairs are truly absent.\n- Test Case $3$ (near-complete graph with a mix of truly missing and truly absent edges):\n  - Number of nodes $n = 6$.\n  - True edges $E^{\\text{true}}$ are all pairs $(i,j)$ with $i \\lt j$, except $(0,2)$, so the underlying true graph is nearly complete with exactly one truly absent edge $(0,2)$.\n  - Observed edges $E^{\\text{obs}} = E^{\\text{true}} \\setminus \\{(0,5),(1,3)\\}$, so the observed graph is missing the two truly present edges $(0,5)$ and $(1,3)$ in addition to the truly absent edge $(0,2)$.\n  - Candidate pairs are the three pairs absent in $E^{\\text{obs}}$: $(0,2)$, $(0,5)$, and $(1,3)$. Among these, $(0,5)$ and $(1,3)$ are truly missing, while $(0,2)$ is truly absent.\n\nYour program must:\n- Implement the above GNN-based scoring and the Average Precision computation for each test case.\n- Produce a single line of output containing the three Average Precision values for the test cases in order, as a comma-separated list enclosed in square brackets, for example $[\\mathrm{AP}_1,\\mathrm{AP}_2,\\mathrm{AP}_3]$. No units are involved in this problem. The numbers should be standard decimal floats without additional text.\n\nYour solution must be self-contained and must not read any input or access external files or networks. All computations must be performed using the specified algorithmic steps above.", "solution": "The problem presents a valid, well-posed task within the domain of graph deep learning. It requires the implementation of a specific, parameter-free Graph Neural Network (GNN) algorithm for link prediction and its evaluation using the Average Precision (AP) metric on a defined set of test cases. All definitions, constraints, and data are provided, are scientifically sound, and are free from ambiguity or contradiction. We may therefore proceed with a formal solution.\n\nThe core objective is to infer missing edges in an observed graph, represented by an adjacency matrix $A^{\\text{obs}}$, by leveraging its local structure. The ground truth for this inference task is provided by a true adjacency matrix, $A^{\\text{true}}$. The method is based on the principle of triadic closure, which posits that two nodes sharing a common neighbor are more likely to be connected. GNNs provide a natural framework for formalizing this intuition through message passing.\n\nThe prescribed algorithm operates as follows:\n\nFirst, for an input graph with $n$ nodes and an observed adjacency matrix $A^{\\text{obs}} \\in \\{0,1\\}^{n \\times n}$, we compute a symmetrically normalized version of the adjacency matrix. This is a standard practice in Graph Convolutional Networks (GCNs) to ensure that node representations do not explode or vanish during propagation and to properly average features from neighboring nodes. The process involves three steps:\n1.  Addition of self-loops to the graph, resulting in an augmented adjacency matrix $\\tilde{A}^{\\text{obs}} = A^{\\text{obs}} + I$, where $I$ is the $n \\times n$ identity matrix. This ensures that a node includes its own features in the aggregation process.\n2.  Calculation of the augmented degree matrix $\\tilde{D}^{\\text{obs}} = \\operatorname{diag}(\\sum_j \\tilde{A}^{\\text{obs}}_{ij})$. Each diagonal entry $(\\tilde{D}^{\\text{obs}})_{ii}$ corresponds to the degree of node $i$ in the graph with self-loops.\n3.  Symmetric normalization to obtain $\\hat{A}^{\\text{obs}} = (\\tilde{D}^{\\text{obs}})^{-1/2}\\tilde{A}^{\\text{obs}}(\\tilde{D}^{\\text{obs}})^{-1/2}$. The matrix $(\\tilde{D}^{\\text{obs}})^{-1/2}$ is a diagonal matrix where each entry is the inverse square root of the corresponding diagonal entry in $\\tilde{D}^{\\text{obs}}$.\n\nSecond, we perform a two-layer message passing operation without any trainable parameters. Starting with identity features $X=I$, one layer of propagation yields $\\hat{A}^{\\text{obs}}X = \\hat{A}^{\\text{obs}}$. A second layer yields $H = \\hat{A}^{\\text{obs}}(\\hat{A}^{\\text{obs}}X) = (\\hat{A}^{\\text{obs}})^2$. The resulting matrix $H \\in \\mathbb{R}^{n \\times n}$ contains node representations where each row $H_{i,:}$ is the feature vector for node $i$. An entry $H_{ij}$ aggregates information about paths of length up to $2$ between nodes $i$ and $j$, effectively encoding their $2$-hop neighborhood structure.\n\nThird, we identify all candidate pairs for link prediction. These are the unordered node pairs $(i,j)$ with $i < j$ that are not connected in the observed graph, i.e., $A^{\\text{obs}}_{ij} = 0$.\n\nFourth, for each candidate pair $(i,j)$, we compute a link score $s(i,j)$. The score is derived from the similarity of the nodes' representations. Specifically, we calculate the inner product of their feature vectors, $H_{i,:}^\\top H_{j,:}$, which measures the similarity of their $2$-hop neighborhoods. This dot product is then passed through a logistic sigmoid function $\\sigma(z) = (1 + e^{-z})^{-1}$ to squash the score into the range $(0,1)$:\n$$ s(i,j) = \\sigma\\big(H_{i,:}^\\top H_{j,:}\\big) $$\nA higher score indicates a stronger structural similarity and thus a higher predicted likelihood of a link.\n\nFifth, to evaluate the performance of our scoring function, we assign a ground-truth label $y(i,j)$ to each candidate pair. The label is $y(i,j) = 1$ if the edge $(i,j)$ exists in the true graph ($A^{\\text{true}}_{ij} = 1$) and $y(i,j) = 0$ otherwise ($A^{\\text{true}}_{ij} = 0$).\n\nFinally, we quantify the model's ability to rank the truly missing edges (positive labels) above the truly absent edges (negative labels) using the Average Precision (AP) metric. The procedure is as follows:\n1.  Rank all candidate pairs in descending order based on their scores $s(i,j)$. Let the corresponding sequence of true labels be $\\{y_1, y_2, \\dots, y_N\\}$, where $N$ is the total number of candidate pairs.\n2.  Count the total number of positive instances, $m = \\sum_{k=1}^N y_k$.\n3.  If $m > 0$, the AP is calculated as the average of precision values at each rank $k$ where a true positive is found:\n    $$ \\mathrm{AP} = \\frac{1}{m}\\sum_{k=1}^N P(k)\\cdot \\mathbb{1}\\{y_k = 1\\} $$\n    where $P(k) = \\frac{1}{k}\\sum_{i=1}^k y_i$ is the precision at rank $k$, and $\\mathbb{1}\\{\\cdot\\}$ is the indicator function. This metric rewards models that place true positives at the top of the ranked list.\n4.  If $m = 0$ (no missing edges to find), the AP is defined to be $0$.\n\nThis entire pipeline is applied independently to each of the three test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef create_adj_matrix(n, edges):\n    \"\"\"Creates an n x n symmetric adjacency matrix from a list of edges.\"\"\"\n    A = np.zeros((n, n), dtype=np.float64)\n    for i, j in edges:\n        A[i, j] = 1\n        A[j, i] = 1\n    return A\n\ndef compute_ap(A_obs, A_true):\n    \"\"\"\n    Computes the Average Precision for link prediction on a single graph.\n    \n    Args:\n        A_obs (np.ndarray): The observed adjacency matrix.\n        A_true (np.ndarray): The ground truth adjacency matrix.\n        \n    Returns:\n        float: The Average Precision score.\n    \"\"\"\n    n = A_obs.shape[0]\n\n    # Step 2: Compute the symmetric normalized adjacency matrix A_hat_obs\n    A_obs_tilde = A_obs + np.identity(n)\n    D_obs_tilde_vec = np.sum(A_obs_tilde, axis=1)\n    # Handle case of isolated nodes (degree 0) to avoid division by zero\n    with np.errstate(divide='ignore'):\n        D_inv_sqrt_vec = 1.0 / np.sqrt(D_obs_tilde_vec)\n    D_inv_sqrt_vec[np.isinf(D_inv_sqrt_vec)] = 0\n    D_inv_sqrt_mat = np.diag(D_inv_sqrt_vec)\n    \n    A_hat_obs = D_inv_sqrt_mat @ A_obs_tilde @ D_inv_sqrt_mat\n\n    # Step 3: Compute the two-layer message-passing representations H\n    H = A_hat_obs @ A_hat_obs\n\n    # Step 4, 5: Generate candidates, compute scores, and get labels\n    candidates = []\n    # Iterate over upper triangle to consider each unordered pair once\n    for i in range(n):\n        for j in range(i + 1, n):\n            if A_obs[i, j] == 0:\n                # Compute score s(i,j)\n                score_raw = H[i, :] @ H[j, :]\n                score = 1.0 / (1.0 + np.exp(-score_raw))\n                \n                # Get label y(i,j)\n                label = A_true[i, j]\n                \n                candidates.append({'score': score, 'label': label})\n\n    # Step 6: Compute Average Precision\n    # Sort candidates by score in descending order\n    candidates.sort(key=lambda x: x['score'], reverse=True)\n    \n    m = sum(c['label'] for c in candidates)\n    if m == 0:\n        return 0.0\n\n    ap_sum = 0.0\n    tp_count = 0\n    for k, candidate in enumerate(candidates, 1):\n        if candidate['label'] == 1:\n            tp_count += 1\n            precision_at_k = tp_count / k\n            ap_sum += precision_at_k\n            \n    return ap_sum / m\n\ndef solve():\n    \"\"\"\n    Defines the test suite, runs the GNN-based link prediction pipeline,\n    and prints the results in the specified format.\n    \"\"\"\n    \n    # Test Case 1\n    n1 = 6\n    E_obs1 = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)]\n    E_true1 = E_obs1 + [(0, 2), (2, 4)]\n    A_obs1 = create_adj_matrix(n1, E_obs1)\n    A_true1 = create_adj_matrix(n1, E_true1)\n\n    # Test Case 2\n    n2 = 5\n    E_obs2 = [(0, 1), (1, 2), (2, 3), (3, 4)]\n    E_true2 = E_obs2 + [(1, 3)]\n    A_obs2 = create_adj_matrix(n2, E_obs2)\n    A_true2 = create_adj_matrix(n2, E_true2)\n\n    # Test Case 3\n    n3 = 6\n    # True graph is complete except for edge (0, 2)\n    all_edges_n3 = [(i, j) for i in range(n3) for j in range(i + 1, n3)]\n    E_true3_list = [edge for edge in all_edges_n3 if edge != (0, 2)]\n    A_true3 = create_adj_matrix(n3, E_true3_list)\n    # Observed graph is true graph minus edges (0, 5) and (1, 3)\n    missing_in_obs = {(0, 5), (1, 3)}\n    E_obs3_list = [edge for edge in E_true3_list if edge not in missing_in_obs]\n    A_obs3 = create_adj_matrix(n3, E_obs3_list)\n\n    test_cases = [\n        (A_obs1, A_true1),\n        (A_obs2, A_true2),\n        (A_obs3, A_true3),\n    ]\n\n    results = []\n    for A_obs, A_true in test_cases:\n        ap_score = compute_ap(A_obs, A_true)\n        results.append(ap_score)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3131905"}]}