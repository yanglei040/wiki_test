{"hands_on_practices": [{"introduction": "Training an Energy-Based Model involves tuning its parameters to assign low energy to data points from the true distribution. This is typically done by maximizing the log-likelihood, whose gradient has a characteristic structure. This first practice [@problem_id:3122263] provides a foundational exercise where you will derive this gradient from first principles for a simple, analytically tractable model, giving you a clear view of the crucial 'positive' (data-driven) and 'negative' (model-driven) phases of learning.", "problem": "Consider an Energy-Based Model (EBM) over two-dimensional real-valued inputs $x \\in \\mathbb{R}^{2}$ with density $p_{\\theta}(x)$ defined by the core EBM principle $p_{\\theta}(x) = Z(\\theta)^{-1} \\exp(-E_{\\theta}(x))$, where the partition function $Z(\\theta)$ is given by $Z(\\theta) = \\int_{\\mathbb{R}^{2}} \\exp(-E_{\\theta}(x)) \\, dx$ and the energy is specified as $E_{\\theta}(x) = \\frac{1}{2} \\|x\\|^{2} - \\theta^{\\top} x$, with parameter $\\theta \\in \\mathbb{R}^{2}$. You are given a toy dataset of $N = 3$ two-dimensional samples,\n$$\nx^{(1)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad\nx^{(2)} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad\nx^{(3)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nStarting from first principles, namely the definition of the log-likelihood and the partition function, derive the exact gradient of the average log-likelihood $\\ell(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\ln p_{\\theta}(x^{(i)})$ with respect to $\\theta$, and explicitly separate the contribution coming from the data expectation (often called the positive phase) and the contribution coming from the model expectation (often called the negative phase). Then, evaluate the resulting exact gradient at the parameter\n$$\n\\theta = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix}.\n$$\nExpress the final gradient as a single exact row vector using rational numbers where appropriate, and do not round.", "solution": "The user wants to find the gradient of the average log-likelihood of an Energy-Based Model (EBM) and evaluate it at a specific point. The solution will proceed in three main stages: first, deriving the general form of the gradient; second, simplifying the gradient by identifying the specific probability distribution defined by the given energy function; and third, evaluating this gradient at the specified parameter value using the provided data.\n\nThe average log-likelihood $\\ell(\\theta)$ for a dataset of $N$ samples $\\{x^{(i)}\\}_{i=1}^{N}$ is given by:\n$$\n\\ell(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\ln p_{\\theta}(x^{(i)})\n$$\nThe probability density function $p_{\\theta}(x)$ is defined as $p_{\\theta}(x) = \\frac{1}{Z(\\theta)} \\exp(-E_{\\theta}(x))$, where $E_{\\theta}(x)$ is the energy function and $Z(\\theta)$ is the partition function. Substituting this into the log-likelihood expression gives:\n$$\n\\ell(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\ln\\left(\\frac{1}{Z(\\theta)}\\right) + \\ln(\\exp(-E_{\\theta}(x^{(i)}))) \\right)\n$$\n$$\n\\ell(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\left( - \\ln Z(\\theta) - E_{\\theta}(x^{(i)}) \\right)\n$$\nSince $\\ln Z(\\theta)$ does not depend on the summation index $i$, we can simplify this to:\n$$\n\\ell(\\theta) = - \\left( \\frac{1}{N} \\sum_{i=1}^{N} E_{\\theta}(x^{(i)}) \\right) - \\ln Z(\\theta)\n$$\nTo find the gradient with respect to the parameter vector $\\theta$, we differentiate $\\ell(\\theta)$:\n$$\n\\nabla_{\\theta} \\ell(\\theta) = - \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_{\\theta} E_{\\theta}(x^{(i)}) - \\nabla_{\\theta} \\ln Z(\\theta)\n$$\nThis expression separates the gradient into two components. Let's analyze each one.\n\nThe first component involves the gradient of the energy function, $E_{\\theta}(x) = \\frac{1}{2} \\|x\\|^{2} - \\theta^{\\top} x$. Differentiating with respect to $\\theta$:\n$$\n\\nabla_{\\theta} E_{\\theta}(x) = \\nabla_{\\theta} \\left( \\frac{1}{2} x^{\\top}x - \\theta^{\\top} x \\right) = -x\n$$\nSubstituting this into the first component of the gradient of the log-likelihood:\n$$\n- \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_{\\theta} E_{\\theta}(x^{(i)}) = - \\frac{1}{N} \\sum_{i=1}^{N} (-x^{(i)}) = \\frac{1}{N} \\sum_{i=1}^{N} x^{(i)}\n$$\nThis term is the empirical mean of the data, often denoted as $\\mathbb{E}_{\\text{data}}[x]$ or $\\bar{x}$. This is the contribution from the data expectation, known as the **positive phase**.\n\nThe second component is the gradient of the log-partition function, $-\\nabla_{\\theta} \\ln Z(\\theta)$. We use the log-derivative trick: $\\nabla_{\\theta} \\ln Z(\\theta) = \\frac{1}{Z(\\theta)} \\nabla_{\\theta} Z(\\theta)$. The partition function is $Z(\\theta) = \\int_{\\mathbb{R}^{2}} \\exp(-E_{\\theta}(x)) \\, dx$. Using the Leibniz integral rule to differentiate under the integral sign:\n$$\n\\nabla_{\\theta} Z(\\theta) = \\int_{\\mathbb{R}^{2}} \\nabla_{\\theta} \\exp(-E_{\\theta}(x)) \\, dx = \\int_{\\mathbb{R}^{2}} \\exp(-E_{\\theta}(x)) (-\\nabla_{\\theta} E_{\\theta}(x)) \\, dx\n$$\nUsing our earlier result that $\\nabla_{\\theta} E_{\\theta}(x) = -x$, we get:\n$$\n\\nabla_{\\theta} Z(\\theta) = \\int_{\\mathbb{R}^{2}} x \\exp(-E_{\\theta}(x)) \\, dx\n$$\nTherefore, the second component of the log-likelihood gradient is:\n$$\n-\\nabla_{\\theta} \\ln Z(\\theta) = - \\frac{1}{Z(\\theta)} \\int_{\\mathbb{R}^{2}} x \\exp(-E_{\\theta}(x)) \\, dx = - \\int_{\\mathbb{R}^{2}} x \\left( \\frac{\\exp(-E_{\\theta}(x))}{Z(\\theta)} \\right) \\, dx\n$$\nRecognizing the term in the parentheses as the model's probability density $p_{\\theta}(x)$, this integral is the definition of the expected value of $x$ under the model distribution:\n$$\n-\\nabla_{\\theta} \\ln Z(\\theta) = - \\mathbb{E}_{x \\sim p_{\\theta}(x)}[x]\n$$\nThis term is the contribution from the model expectation, known as the **negative phase**.\n\nCombining the positive and negative phases, the full gradient of the average log-likelihood is:\n$$\n\\nabla_{\\theta} \\ell(\\theta) = \\underbrace{\\frac{1}{N} \\sum_{i=1}^{N} x^{(i)}}_{\\text{Positive Phase}} - \\underbrace{\\mathbb{E}_{x \\sim p_{\\theta}(x)}[x]}_{\\text{Negative Phase}}\n$$\nTo evaluate this, we must find the model expectation $\\mathbb{E}_{x \\sim p_{\\theta}(x)}[x]$. This requires identifying the distribution $p_{\\theta}(x)$. We analyze the energy function $E_{\\theta}(x) = \\frac{1}{2}\\|x\\|^2 - \\theta^{\\top}x$ by completing the square with respect to $x$:\n$$\nE_{\\theta}(x) = \\frac{1}{2} (x^{\\top}x) - \\theta^{\\top}x = \\frac{1}{2} (x^{\\top}x - 2\\theta^{\\top}x)\n$$\n$$\n= \\frac{1}{2} (x^{\\top}x - 2\\theta^{\\top}x + \\theta^{\\top}\\theta - \\theta^{\\top}\\theta) = \\frac{1}{2} ( (x-\\theta)^{\\top}(x-\\theta) - \\theta^{\\top}\\theta )\n$$\n$$\n= \\frac{1}{2}\\|x-\\theta\\|^2 - \\frac{1}{2}\\|\\theta\\|^2\n$$\nThe probability density is thus:\n$$\np_{\\theta}(x) = \\frac{1}{Z(\\theta)} \\exp\\left( -\\left(\\frac{1}{2}\\|x-\\theta\\|^2 - \\frac{1}{2}\\|\\theta\\|^2\\right) \\right) = \\frac{\\exp(\\frac{1}{2}\\|\\theta\\|^2)}{Z(\\theta)} \\exp\\left(-\\frac{1}{2} (x-\\theta)^{\\top} I^{-1} (x-\\theta)\\right)\n$$\nwhere $I$ is the $2 \\times 2$ identity matrix. This is the functional form of a bivariate Gaussian distribution with mean $\\mu = \\theta$ and covariance matrix $\\Sigma = I$. The normalization constant for a general multivariate Gaussian $\\mathcal{N}(\\mu, \\Sigma)$ in $\\mathbb{R}^d$ is $(2\\pi)^{d/2} |\\det(\\Sigma)|^{1/2}$. For our case, $d=2$, $\\mu=\\theta$, $\\Sigma=I$, so the normalization constant is $(2\\pi)^{2/2} |\\det(I)|^{1/2} = 2\\pi$.\nThus, $p_{\\theta}(x) = \\mathcal{N}(x | \\theta, I)$.\nThe expectation of a random variable $x$ following a Gaussian distribution $\\mathcal{N}(\\mu, \\Sigma)$ is its mean $\\mu$. Therefore:\n$$\n\\mathbb{E}_{x \\sim p_{\\theta}(x)}[x] = \\theta\n$$\nThe gradient expression simplifies significantly:\n$$\n\\nabla_{\\theta} \\ell(\\theta) = \\bar{x} - \\theta\n$$\nNow we can evaluate this gradient using the given data and parameter value. First, we compute the sample mean $\\bar{x}$:\n$$\nx^{(1)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad x^{(2)} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad x^{(3)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\n\\bar{x} = \\frac{1}{3} \\left( x^{(1)} + x^{(2)} + x^{(3)} \\right) = \\frac{1}{3} \\left( \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix} 0+2+1 \\\\ 1+0+1 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{2}{3} \\end{pmatrix}\n$$\nThe gradient is evaluated at $\\theta = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix}$:\n$$\n\\nabla_{\\theta} \\ell(\\theta) = \\begin{pmatrix} 1 \\\\ \\frac{2}{3} \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{1}{2} \\\\ \\frac{2}{3} - \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{4}{6} - \\frac{9}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{5}{6} \\end{pmatrix}\n$$\nThe problem asks for the final gradient as a single exact row vector. This is the transpose of the column vector we calculated.\n$$\n\\left( \\nabla_{\\theta} \\ell(\\theta) \\right)^{\\top} = \\begin{pmatrix} \\frac{1}{2}  -\\frac{5}{6} \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2}  -\\frac{5}{6} \\end{pmatrix}}\n$$", "id": "3122263"}, {"introduction": "One of the most elegant and powerful features of EBMs is their compositionality—new models can be constructed by combining the energy functions of existing ones. In this practice [@problem_id:3122280], you will explore this concept by building class-conditional models and then creating a new, hybrid model by taking a weighted average of their energies. Your task is to implement this interpolation and verify how this simple algebraic operation in 'energy space' leads to meaningful semantic mixing in the data space.", "problem": "Consider an Energy-Based Model (EBM), that is, a model that assigns an energy function to data and turns it into a probability density via exponentiation and normalization. For a class-conditional setup for handwritten digits, suppose we have an energy function $E_{\\theta}(x \\mid y)$ over feature vectors $x \\in \\mathbb{R}^d$ conditioned on a class label $y$, where $\\theta$ denotes parameters. The probability density of $x$ given class $y$ is defined by\n$$\np_{\\theta}(x \\mid y) = \\frac{\\exp\\left(-E_{\\theta}(x \\mid y)\\right)}{Z_{\\theta}(y)},\n$$\nwhere $Z_{\\theta}(y)$ is the partition function ensuring normalization.\n\nStarting from core definitions of energy-based models, propose a class-conditional energy of the quadratic form suitable for intermediate-dimensional features of handwritten digits (for example, roundness and slant), namely\n$$\nE_{\\theta}(x \\mid y) = \\frac{1}{2} \\left(x - \\mu_y\\right)^{\\top} \\Sigma_y^{-1} \\left(x - \\mu_y\\right),\n$$\nwhere $\\mu_y \\in \\mathbb{R}^d$ is a class-dependent mean, and $\\Sigma_y \\in \\mathbb{R}^{d \\times d}$ is a symmetric positive definite covariance matrix. This choice ensures a finite partition function and a well-defined probability density.\n\nDefine an interpolation of two classes $y_0$ and $y_1$ by the additive energy\n$$\nE_{\\alpha}(x) = \\alpha \\, E_{\\theta}(x \\mid y_0) + (1 - \\alpha) \\, E_{\\theta}(x \\mid y_1),\n$$\nwith interpolation weight $\\alpha \\in [0,1]$. Show that $E_{\\alpha}(x)$ is again quadratic and thus induces a Gaussian density over $x$, and derive its mean and covariance in closed form from first principles.\n\nImplement a program that, given specific class parameters and interpolation weights, computes the interpolated Gaussian’s mean and covariance, and then tests whether “adding energies yields meaningful class interpolation” according to the following quantitative criteria:\n- Boundary consistency: At $\\alpha = 0$, the interpolated distribution coincides with the class $y_1$ distribution; at $\\alpha = 1$, it coincides with class $y_0$.\n- Linear mean interpolation under equal covariances: If $\\Sigma_{y_0} = \\Sigma_{y_1}$, then the interpolated mean equals the linear interpolation $\\alpha \\mu_{y_0} + (1-\\alpha)\\mu_{y_1}$.\n- Precision-weighted bias under unequal covariances: If $\\Sigma_{y_1}$ has smaller variance than $\\Sigma_{y_0}$ along the line connecting $\\mu_{y_0}$ and $\\mu_{y_1}$, then the interpolated mean is biased closer to $\\mu_{y_1}$ than the naive linear interpolation would suggest.\n- Stationarity: The gradient of $E_{\\alpha}(x)$ evaluated at the interpolated mean is the zero vector.\n- Variance bounds: Each coordinate of the interpolated covariance lies between the corresponding coordinates of $\\Sigma_{y_0}$ and $\\Sigma_{y_1}$ when both are diagonal.\n\nUse the following test suite with $d = 2$ and parameters specified below. In all cases, treat $y_0$ and $y_1$ as two digit classes:\n\n- Test $1$ (Boundary at $\\alpha = 0$): $\\mu_{y_0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\Sigma_{y_0} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$; $\\mu_{y_1} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$, $\\Sigma_{y_1} = \\begin{bmatrix} 0.5  0 \\\\ 0  0.5 \\end{bmatrix}$; $\\alpha = 0$. Check both $\\mu_{\\alpha} = \\mu_{y_1}$ and $\\Sigma_{\\alpha} = \\Sigma_{y_1}$.\n- Test $2$ (Equal covariances and $\\alpha = 0.3$): $\\mu_{y_0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\Sigma_{y_0} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$; $\\mu_{y_1} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$, $\\Sigma_{y_1} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$; $\\alpha = 0.3$. Check $\\mu_{\\alpha} = \\alpha \\mu_{y_0} + (1 - \\alpha)\\mu_{y_1}$.\n- Test $3$ (Bias with unequal covariances and $\\alpha = 0.5$): $\\mu_{y_0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\Sigma_{y_0} = \\begin{bmatrix} 1.0  0 \\\\ 0  0.5 \\end{bmatrix}$; $\\mu_{y_1} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$, $\\Sigma_{y_1} = \\begin{bmatrix} 0.25  0 \\\\ 0  0.5 \\end{bmatrix}$; $\\alpha = 0.5$. Let $t$ be the scalar such that the projection of $\\mu_{\\alpha}$ onto the line from $\\mu_{y_0}$ to $\\mu_{y_1}$ satisfies $\\mu_{\\alpha} = \\mu_{y_0} + t(\\mu_{y_1} - \\mu_{y_0})$. Check $t  \\alpha$ and $0 \\le t \\le 1$.\n- Test $4$ (Stationarity with off-diagonal covariances and $\\alpha = 0.4$): $\\mu_{y_0} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}$, $\\Sigma_{y_0} = \\begin{bmatrix} 1.0  0.3 \\\\ 0.3  1.0 \\end{bmatrix}$; $\\mu_{y_1} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$, $\\Sigma_{y_1} = \\begin{bmatrix} 1.5  -0.2 \\\\ -0.2  1.0 \\end{bmatrix}$; $\\alpha = 0.4$. Check that $\\nabla E_{\\alpha}(\\mu_{\\alpha}) = \\mathbf{0}$.\n- Test $5$ (Variance bounds with diagonal covariances and $\\alpha = 0.75$): $\\mu_{y_0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\Sigma_{y_0} = \\begin{bmatrix} 1.0  0 \\\\ 0  4.0 \\end{bmatrix}$; $\\mu_{y_1} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\Sigma_{y_1} = \\begin{bmatrix} 9.0  0 \\\\ 0  0.25 \\end{bmatrix}$; $\\alpha = 0.75$. Check that each diagonal entry of $\\Sigma_{\\alpha}$ lies between the corresponding entries of $\\Sigma_{y_0}$ and $\\Sigma_{y_1}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$), where each entry is a boolean indicating whether the corresponding test passed.", "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of energy-based models and multivariate statistics, is well-posed with all necessary information provided, and is framed using objective, formal language. The tasks are mathematically tractable and verifiable.\n\nThe problem requires us to analyze the properties of an interpolated energy function formed by the linear combination of two quadratic energy functions, which correspond to two class-conditional Gaussian distributions. The primary tasks are to derive the parameters of the resulting distribution and then to verify several key properties of this interpolation scheme through a series of numerical tests.\n\n### Principle-Based Derivation of Interpolated Gaussian Parameters\n\nAn energy-based model defines a probability distribution over a variable $x$ via an energy function $E(x)$ as $p(x) \\propto \\exp(-E(x))$. The given class-conditional energy function is:\n$$\nE_{\\theta}(x \\mid y) = \\frac{1}{2} (x - \\mu_y)^{\\top} \\Sigma_y^{-1} (x - \\mu_y)\n$$\nThis is, up to an additive constant, the negative log-likelihood of a multivariate Gaussian distribution $N(x; \\mu_y, \\Sigma_y)$. The matrix $\\Sigma_y^{-1}$ is known as the precision matrix.\n\nThe interpolated energy for two classes, $y_0$ and $y_1$, is defined as:\n$$\nE_{\\alpha}(x) = \\alpha E_{\\theta}(x \\mid y_0) + (1 - \\alpha) E_{\\theta}(x \\mid y_1)\n$$\nwhere $\\alpha \\in [0,1]$ is the interpolation weight.\n\nTo show that $E_{\\alpha}(x)$ is also quadratic and to find the parameters of its corresponding Gaussian distribution, we first substitute the explicit form of the energy functions:\n$$\nE_{\\alpha}(x) = \\frac{\\alpha}{2} (x - \\mu_{y_0})^{\\top} \\Sigma_{y_0}^{-1} (x - \\mu_{y_0}) + \\frac{1-\\alpha}{2} (x - \\mu_{y_1})^{\\top} \\Sigma_{y_1}^{-1} (x - \\mu_{y_1})\n$$\nWe expand the quadratic terms. A general quadratic form $(x - \\mu)^{\\top}P(x - \\mu)$ expands to $x^{\\top}Px - 2\\mu^{\\top}Px + \\mu^{\\top}P\\mu$, assuming $P$ is symmetric (which precision matrices are). Applying this, we get:\n$$\n2E_{\\alpha}(x) = \\alpha (x^{\\top}\\Sigma_{y_0}^{-1}x - 2\\mu_{y_0}^{\\top}\\Sigma_{y_0}^{-1}x + C_0) + (1-\\alpha) (x^{\\top}\\Sigma_{y_1}^{-1}x - 2\\mu_{y_1}^{\\top}\\Sigma_{y_1}^{-1}x + C_1)\n$$\nwhere $C_0$ and $C_1$ are constants with respect to $x$. We can group the terms based on their dependence on $x$:\n$$\n2E_{\\alpha}(x) = x^{\\top} \\left( \\alpha\\Sigma_{y_0}^{-1} + (1-\\alpha)\\Sigma_{y_1}^{-1} \\right) x - 2 \\left( \\alpha\\mu_{y_0}^{\\top}\\Sigma_{y_0}^{-1} + (1-\\alpha)\\mu_{y_1}^{\\top}\\Sigma_{y_1}^{-1} \\right) x + C_{total}\n$$\nThis expression is clearly a quadratic function of $x$. We can match this form to the general energy function of a Gaussian distribution $N(x; \\mu_{\\alpha}, \\Sigma_{\\alpha})$, which is $E(x) = \\frac{1}{2}(x - \\mu_{\\alpha})^{\\top}\\Sigma_{\\alpha}^{-1}(x - \\mu_{\\alpha}) + \\text{const}$. Expanding this gives:\n$$\n2E(x) = x^{\\top}\\Sigma_{\\alpha}^{-1}x - 2\\mu_{\\alpha}^{\\top}\\Sigma_{\\alpha}^{-1}x + \\text{const}\n$$\nBy comparing the coefficients of the quadratic ($x^{\\top}(\\cdot)x$) and linear ($(\\cdot)x$) terms between the two expressions for the energy, we can identify the parameters $\\mu_{\\alpha}$ and $\\Sigma_{\\alpha}$ of the interpolated distribution.\n\n1.  **Interpolated Covariance**: Comparing the quadratic terms, we find the inverse covariance (precision matrix) of the interpolated distribution, $\\Sigma_{\\alpha}^{-1}$:\n    $$\n    \\Sigma_{\\alpha}^{-1} = \\alpha \\Sigma_{y_0}^{-1} + (1 - \\alpha) \\Sigma_{y_1}^{-1}\n    $$\n    This shows that \"adding energies\" corresponds to a linear interpolation of the precision matrices. The new covariance matrix is the inverse of this result:\n    $$\n    \\Sigma_{\\alpha} = \\left( \\alpha \\Sigma_{y_0}^{-1} + (1 - \\alpha) \\Sigma_{y_1}^{-1} \\right)^{-1}\n    $$\n    Since $\\Sigma_{y_0}$ and $\\Sigma_{y_1}$ are positive definite and $\\alpha \\in [0,1]$, $\\Sigma_{y_0}^{-1}$ and $\\Sigma_{y_1}^{-1}$ are positive definite. Their weighted sum $\\Sigma_{\\alpha}^{-1}$ is also positive definite, ensuring that $\\Sigma_{\\alpha}$ is well-defined and positive definite.\n\n2.  **Interpolated Mean**: Comparing the linear terms, we have:\n    $$\n    \\mu_{\\alpha}^{\\top}\\Sigma_{\\alpha}^{-1} = \\alpha\\mu_{y_0}^{\\top}\\Sigma_{y_0}^{-1} + (1-\\alpha)\\mu_{y_1}^{\\top}\\Sigma_{y_1}^{-1}\n    $$\n    To solve for $\\mu_{\\alpha}$, we can transpose both sides and then pre-multiply by $\\Sigma_{\\alpha}$:\n    $$\n    (\\mu_{\\alpha}^{\\top}\\Sigma_{\\alpha}^{-1})^{\\top} = (\\alpha\\mu_{y_0}^{\\top}\\Sigma_{y_0}^{-1} + (1-\\alpha)\\mu_{y_1}^{\\top}\\Sigma_{y_1}^{-1})^{\\top}\n    $$\n    $$\n    (\\Sigma_{\\alpha}^{-1})^{\\top} \\mu_{\\alpha} = (\\alpha\\Sigma_{y_0}^{-1})^{\\top}\\mu_{y_0} + ((1-\\alpha)\\Sigma_{y_1}^{-1})^{\\top}\\mu_{y_1}\n    $$\n    Since precision matrices are symmetric, this simplifies to:\n    $$\n    \\Sigma_{\\alpha}^{-1} \\mu_{\\alpha} = \\alpha\\Sigma_{y_0}^{-1}\\mu_{y_0} + (1-\\alpha)\\Sigma_{y_1}^{-1}\\mu_{y_1}\n    $$\n    Finally, pre-multiplying by $\\Sigma_{\\alpha}$:\n    $$\n    \\mu_{\\alpha} = \\Sigma_{\\alpha} \\left( \\alpha\\Sigma_{y_0}^{-1}\\mu_{y_0} + (1-\\alpha)\\Sigma_{y_1}^{-1}\\mu_{y_1} \\right)\n    $$\n    The mean of the interpolated distribution is a precision-weighted average of the component means.\n\n### Algorithmic Design and Test Verification\n\nThe implementation will compute $\\mu_{\\alpha}$ and $\\Sigma_{\\alpha}$ using the derived closed-form expressions. The five tests verify important behaviors of this interpolation scheme.\n\n-   **Test 1 (Boundary consistency)**: Setting $\\alpha=0$ in the formulas yields $\\Sigma_{\\alpha}^{-1} = \\Sigma_{y_1}^{-1} \\implies \\Sigma_{\\alpha} = \\Sigma_{y_1}$ and $\\mu_{\\alpha} = \\Sigma_{y_1}(\\Sigma_{y_1}^{-1}\\mu_{y_1}) = \\mu_{y_1}$. The case for $\\alpha=1$ is symmetric. This test confirms the implementation correctly recovers the original distributions at the boundaries of the interpolation.\n-   **Test 2 (Linear mean interpolation under equal covariances)**: If $\\Sigma_{y_0} = \\Sigma_{y_1} = \\Sigma$, then $\\Sigma_{\\alpha}^{-1} = (\\alpha + 1-\\alpha)\\Sigma^{-1} = \\Sigma^{-1}$, so $\\Sigma_{\\alpha} = \\Sigma$. The mean becomes $\\mu_{\\alpha} = \\Sigma(\\alpha\\Sigma^{-1}\\mu_{y_0} + (1-\\alpha)\\Sigma^{-1}\\mu_{y_1}) = \\alpha\\mu_{y_0} + (1-\\alpha)\\mu_{y_1}$. The test confirms this special case, where precision-weighting reduces to simple linear interpolation of the means.\n-   **Test 3 (Precision-weighted bias)**: When covariances are unequal, the mean is pulled towards the mean of the distribution with higher precision (lower variance). The component with lower variance has a \"stronger\" influence, as its energy function rises more steeply away from its minimum. The test quantifies this bias.\n-   **Test 4 (Stationarity)**: The mean of a distribution is also its mode (point of maximum probability), which corresponds to the an extremum (minimum) of the energy function. A necessary condition for an extremum is that the gradient is zero. The test verifies that $\\nabla E_{\\alpha}(\\mu_{\\alpha}) = \\mathbf{0}$, confirming that our derived $\\mu_{\\alpha}$ is indeed the minimum of the interpolated energy function. The gradient is $\\nabla E_{\\alpha}(x) = \\alpha\\Sigma_{y_0}^{-1}(x-\\mu_{y_0}) + (1-\\alpha)\\Sigma_{y_1}^{-1}(x-\\mu_{y_1})$. Substituting $x=\\mu_\\alpha$ and using the derived formulas shows this is analytically zero.\n-   **Test 5 (Variance bounds)**: The interpolated precision $\\Sigma_{\\alpha,ii}^{-1}$ is a linear interpolation of the component precisions. Since the function $f(v) = 1/v$ is convex for $v  0$, the interpolated variance $(\\Sigma_{\\alpha,ii}^{-1})^{-1}$ lies between the component variances. This is a consequence of the properties of weighted harmonic means. The test verifies this bounding property for diagonal covariance matrices.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by performing five tests on the interpolation of two \n    Gaussian distributions defined via an energy-based model formulation.\n    \"\"\"\n    \n    test_cases = [\n        # Test 1: Boundary at alpha = 0\n        {\n            \"id\": 1,\n            \"mu0\": np.array([0., 0.]),\n            \"S0\": np.array([[1., 0.], [0., 1.]]),\n            \"mu1\": np.array([2., 1.]),\n            \"S1\": np.array([[0.5, 0.], [0., 0.5]]),\n            \"alpha\": 0.0\n        },\n        # Test 2: Equal covariances and alpha = 0.3\n        {\n            \"id\": 2,\n            \"mu0\": np.array([0., 0.]),\n            \"S0\": np.array([[1., 0.], [0., 1.]]),\n            \"mu1\": np.array([2., 0.]),\n            \"S1\": np.array([[1., 0.], [0., 1.]]),\n            \"alpha\": 0.3\n        },\n        # Test 3: Bias with unequal covariances and alpha = 0.5\n        {\n            \"id\": 3,\n            \"mu0\": np.array([0., 0.]),\n            \"S0\": np.array([[1.0, 0.], [0., 0.5]]),\n            \"mu1\": np.array([2., 0.]),\n            \"S1\": np.array([[0.25, 0.], [0., 0.5]]),\n            \"alpha\": 0.5\n        },\n        # Test 4: Stationarity with off-diagonal covariances and alpha = 0.4\n        {\n            \"id\": 4,\n            \"mu0\": np.array([-1., 1.]),\n            \"S0\": np.array([[1.0, 0.3], [0.3, 1.0]]),\n            \"mu1\": np.array([1., -1.]),\n            \"S1\": np.array([[1.5, -0.2], [-0.2, 1.0]]),\n            \"alpha\": 0.4\n        },\n        # Test 5: Variance bounds with diagonal covariances and alpha = 0.75\n        {\n            \"id\": 5,\n            \"mu0\": np.array([0., 0.]),\n            \"S0\": np.array([[1.0, 0.], [0., 4.0]]),\n            \"mu1\": np.array([0., 0.]),\n            \"S1\": np.array([[9.0, 0.], [0., 0.25]]),\n            \"alpha\": 0.75\n        }\n    ]\n\n    def compute_interpolated_params(mu0, S0, mu1, S1, alpha):\n        \"\"\"\n        Computes the mean and covariance of the interpolated Gaussian distribution.\n        \"\"\"\n        S0_inv = np.linalg.inv(S0)\n        S1_inv = np.linalg.inv(S1)\n        \n        Sa_inv = alpha * S0_inv + (1 - alpha) * S1_inv\n        Sa = np.linalg.inv(Sa_inv)\n        \n        # Note: Sa is pre-multiplied\n        mu_a_term = alpha * (S0_inv @ mu0) + (1 - alpha) * (S1_inv @ mu1)\n        mu_a = Sa @ mu_a_term\n        \n        return mu_a, Sa\n\n    results = []\n    for case in test_cases:\n        mu0, S0, mu1, S1, alpha = case[\"mu0\"], case[\"S0\"], case[\"mu1\"], case[\"S1\"], case[\"alpha\"]\n        mu_a, Sa = compute_interpolated_params(mu0, S0, mu1, S1, alpha)\n        \n        test_id = case[\"id\"]\n        result = False\n        \n        if test_id == 1:\n            check_mu = np.allclose(mu_a, mu1)\n            check_S = np.allclose(Sa, S1)\n            result = check_mu and check_S\n        \n        elif test_id == 2:\n            mu_linear_interp = alpha * mu0 + (1 - alpha) * mu1\n            result = np.allclose(mu_a, mu_linear_interp)\n\n        elif test_id == 3:\n            # We calculated that mu_a lies on the line connecting mu0 and mu1 for this case.\n            # So the projection is mu_a itself.\n            # We solve mu_a = mu0 + t * (mu1-mu0) for t.\n            #  mu_a - mu0 = t * (mu1-mu0)\n            # Take dot product with (mu1-mu0) to solve for t\n            v = mu1 - mu0\n            w = mu_a - mu0\n            t = (w @ v) / (v @ v)\n            result = (t > alpha) and (0 = t = 1)\n\n        elif test_id == 4:\n            # Check stationarity: grad E_alpha(mu_a) == 0\n            S0_inv = np.linalg.inv(S0)\n            S1_inv = np.linalg.inv(S1)\n            grad = alpha * S0_inv @ (mu_a - mu0) + (1 - alpha) * S1_inv @ (mu_a - mu1)\n            result = np.allclose(grad, np.zeros_like(grad))\n\n        elif test_id == 5:\n            # Check variance bounds for diagonal matrices\n            Sa_diag = np.diag(Sa)\n            S0_diag = np.diag(S0)\n            S1_diag = np.diag(S1)\n            \n            lower_bounds = np.minimum(S0_diag, S1_diag)\n            upper_bounds = np.maximum(S0_diag, S1_diag)\n            \n            check_lower = np.all(Sa_diag >= lower_bounds)\n            check_upper = np.all(Sa_diag = upper_bounds)\n            result = check_lower and check_upper\n            \n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3122280"}, {"introduction": "After training an EBM, a critical next step is to evaluate its performance. A well-trained model should not only assign low energy to data it has seen but also correctly reflect the relative likelihood of different outcomes. This hands-on exercise [@problem_id:3122243] introduces the concept of model calibration and guides you through using rank-based statistical metrics to quantitatively assess whether your EBM's energy values are consistent with the empirical data distribution.", "problem": "You are given a finite set of discrete outcomes and an Energy-Based Model (EBM). The EBM assigns a scalar energy to each outcome via an energy function $E_\\theta(x)$ parameterized by $\\theta$. For a discrete sample space $\\mathcal{X}$, the model defines a probability distribution that is proportional to $\\exp(-E_\\theta(x))$ over $\\mathcal{X}$. Your task is to design and implement a small, self-contained experiment to assess the calibration of $E_\\theta(x)$ on small datasets by verifying, using rank-based metrics, whether $E_\\theta(x)$ is consistent with the empirical data distribution.\n\nStarting from the foundational definition of an Energy-Based Model (EBM) over a finite set $\\mathcal{X}$, use only rank-based metrics to test whether lower energy corresponds to higher empirical probability. To do this, compute the partition function $Z_\\theta$ from the provided energies and compare the vector $E_\\theta(x)+\\log Z_\\theta$ to the vector $-\\log \\hat{p}_{\\text{data}}(x)$ computed from data using additive (Laplace) smoothing. All logarithms must be natural logarithms. Use the following precise experimental design and computation rules.\n\nFundamental base and definitions:\n- An Energy-Based Model (EBM) over a finite set $\\mathcal{X}$ assigns to each $x \\in \\mathcal{X}$ an energy $E_\\theta(x) \\in \\mathbb{R}$, and defines a probability distribution $p_\\theta(x)$ that is proportional to $\\exp(-E_\\theta(x))$ over $\\mathcal{X}$.\n- The partition function $Z_\\theta$ is the normalization constant given by the finite sum over $\\mathcal{X}$.\n- The empirical data distribution $\\hat{p}_{\\text{data}}(x)$ is estimated from observed counts with additive (Laplace) smoothing of strength $\\alpha$, defined as $\\hat{p}_{\\text{data}}(x) = \\dfrac{n_x + \\alpha}{N + \\alpha K}$, where $n_x$ is the count of outcome $x$, $N = \\sum_{x \\in \\mathcal{X}} n_x$ is the total number of observations, and $K = |\\mathcal{X}|$ is the number of distinct outcomes.\n\nComputation protocol:\n- Compute $Z_\\theta$ using a numerically stable log-sum-exp method on the vector $-E_\\theta(x)$ to obtain $\\log Z_\\theta$.\n- Compute the vector $v_{\\text{model}}(x) = E_\\theta(x) + \\log Z_\\theta$.\n- Compute the smoothed empirical distribution $\\hat{p}_{\\text{data}}(x)$ with additive smoothing $\\alpha = 1$ and then compute $v_{\\text{data}}(x) = -\\log \\hat{p}_{\\text{data}}(x)$.\n- Evaluate rank-based calibration metrics that compare the rankings induced by $v_{\\text{model}}$ and $v_{\\text{data}}$:\n  1. Spearman rank correlation coefficient $\\rho_S$ between $v_{\\text{model}}$ and $v_{\\text{data}}$.\n  2. Kendall rank correlation coefficient $\\tau$ between $v_{\\text{model}}$ and $v_{\\text{data}}$.\n  3. Top-$1$ agreement as a boolean indicating whether the lowest element of $v_{\\text{model}}$ matches the lowest element of $v_{\\text{data}}$ under the specified tie-breaking rule.\n  4. Top-$2$ overlap, defined as the fraction (expressed as a decimal) of overlap between the sets of the two lowest elements of $v_{\\text{model}}$ and $v_{\\text{data}}$ under the specified tie-breaking rule.\n  5. Pairwise concordance rate, defined as the fraction (expressed as a decimal) of unordered index pairs whose ordering (or equality) is consistent between $v_{\\text{model}}$ and $v_{\\text{data}}$ using the sign of pairwise differences.\n\nTie-breaking rule:\n- When sorting to identify top-$k$ elements, sort indices by increasing value and break ties by increasing index (that is, sort by the pair $(\\text{value}, \\text{index})$ lexicographically). This ensures exactly $k$ indices are selected.\n\nNaN handling:\n- If either $\\rho_S$ or $\\tau$ is not a number due to degenerate inputs (for example, constant vectors), set the corresponding metric value to $0.0$.\n\nRounding:\n- Round all floating-point metric outputs to $6$ decimal places. The top-$1$ agreement must be a boolean.\n\nTest suite:\nImplement your program to evaluate the following four test cases. In all cases, the outcome space is $\\mathcal{X} = \\{0,1,2,3,4\\}$, so $K = 5$.\n- Test case $1$ (near-perfect calibration by construction):\n  - Counts: $\\mathbf{c}^{(1)} = [\\,50,\\,30,\\,10,\\,7,\\,3\\,]$.\n  - Energy construction: Let $N^{(1)} = \\sum_i c^{(1)}_i = 100$, define $p^{(1)}_i = c^{(1)}_i / N^{(1)}$, and set $E^{(1)}_i = -\\log p^{(1)}_i + 0.3$ for $i \\in \\{0,1,2,3,4\\}$.\n- Test case $2$ (moderate mismatch with controlled noise):\n  - Counts: $\\mathbf{c}^{(2)} = [\\,40,\\,25,\\,20,\\,10,\\,5\\,]$.\n  - Energy construction: Let $N^{(2)} = \\sum_i c^{(2)}_i = 100$, define $p^{(2)}_i = c^{(2)}_i / N^{(2)}$, and set $E^{(2)}_i = -\\log p^{(2)}_i + 0.2 + \\epsilon_i$, with $\\boldsymbol{\\epsilon} = [\\,0.05,\\,-0.02,\\,0.1,\\,-0.05,\\,-0.08\\,]$.\n- Test case $3$ (uniform data and constant energies; degenerate ranking):\n  - Counts: $\\mathbf{c}^{(3)} = [\\,20,\\,20,\\,20,\\,20,\\,20\\,]$.\n  - Energies: $E^{(3)} = [\\,2,\\,2,\\,2,\\,2,\\,2\\,]$.\n- Test case $4$ (extreme skew with zero counts and miscalibrated energies):\n  - Counts: $\\mathbf{c}^{(4)} = [\\,0,\\,1,\\,0,\\,0,\\,99\\,]$.\n  - Energies: $E^{(4)} = [\\,1.0,\\,2.0,\\,3.0,\\,4.0,\\,5.0\\,]$.\n\nOutput specification:\n- For each test case, compute the metrics in the order $[\\,\\rho_S,\\,\\tau,\\,\\text{top1},\\,\\text{top2overlap},\\,\\text{pairwiseConcordance}\\,]$.\n- Your program should produce a single line of output containing the results as a Python-like list of lists, one inner list per test case, with values in the order above, rounded or boolean as specified. For example, the output must look like $[[\\dots],[\\dots],[\\dots],[\\dots]]$ on a single line.\n\nNo physical units or angles are involved in this problem. All computations must be purely mathematical with the definitions and rules stated above. The program must be fully self-contained and must not require any user input or external files.", "solution": "We begin from the foundational definition of an Energy-Based Model (EBM) over a finite set $\\mathcal{X}$. An EBM assigns an energy $E_\\theta(x)$ to each $x \\in \\mathcal{X}$ and defines a probability distribution $p_\\theta(x)$ that is proportional to $\\exp(-E_\\theta(x))$. Concretely, for finite $\\mathcal{X}$ we have\n$$\np_\\theta(x) = \\frac{\\exp(-E_\\theta(x))}{Z_\\theta},\n$$\nwhere the partition function $Z_\\theta$ is defined as\n$$\nZ_\\theta = \\sum_{x' \\in \\mathcal{X}} \\exp\\big(-E_\\theta(x')\\big).\n$$\nBy taking the negative logarithm of the model distribution, we obtain\n$$\n-\\log p_\\theta(x) = -\\log \\left( \\frac{\\exp(-E_\\theta(x))}{Z_\\theta} \\right)\n= - \\big( -E_\\theta(x) - \\log Z_\\theta \\big)\n= E_\\theta(x) + \\log Z_\\theta.\n$$\nThis demonstrates that the negative log-probability under the model decomposes into the energy and the log-partition function. Calibration in this context means that $E_\\theta(x) + \\log Z_\\theta$ is consistent with $-\\log p_{\\text{data}}(x)$, where $p_{\\text{data}}(x)$ is the true data distribution. On small datasets, we approximate $p_{\\text{data}}(x)$ using a smoothed empirical distribution.\n\nTo estimate the data distribution robustly, we adopt additive (Laplace) smoothing with strength $\\alpha = 1$. For counts $\\{n_x\\}_{x \\in \\mathcal{X}}$, total count $N = \\sum_{x \\in \\mathcal{X}} n_x$, and $K = |\\mathcal{X}|$, we set\n$$\n\\hat{p}_{\\text{data}}(x) = \\frac{n_x + \\alpha}{N + \\alpha K}.\n$$\nThis avoids zero probabilities and makes $-\\log \\hat{p}_{\\text{data}}(x)$ finite even when some $n_x$ are zero.\n\nThe objective is to test whether the ranking induced by $E_\\theta(x) + \\log Z_\\theta$ matches the ranking induced by $-\\log \\hat{p}_{\\text{data}}(x)$. Rank-based metrics are invariant to additive constants and monotonic rescalings, making them suitable for assessing calibration without requiring scale-perfect equality. We use the following rank-based metrics:\n- Spearman rank correlation coefficient $\\rho_S$, which is the Pearson correlation of the rank variables and measures monotonic relationship strength. If either input is constant, $\\rho_S$ is undefined; we set it to $0.0$ in such cases.\n- Kendall rank correlation coefficient $\\tau$, which measures the difference between the number of concordant and discordant pairs normalized by the total number of pairs. If undefined due to degeneracy, we set it to $0.0$.\n- Top-$1$ agreement, a boolean that is $\\mathrm{True}$ if the index of the minimal value in $v_{\\text{model}}(x) = E_\\theta(x) + \\log Z_\\theta$ matches the index of the minimal value in $v_{\\text{data}}(x) = -\\log \\hat{p}_{\\text{data}}(x)$, using a deterministic tie-breaking rule.\n- Top-$2$ overlap, the fraction (decimal) of indices shared between the sets of the two minimal elements in $v_{\\text{model}}$ and $v_{\\text{data}}$, again using deterministic tie-breaking. This is computed as $\\frac{|S_{\\text{model}} \\cap S_{\\text{data}}|}{2}$ where both sets have size $2$.\n- Pairwise concordance rate, the fraction (decimal) of unordered pairs whose relative order is the same in both $v_{\\text{model}}$ and $v_{\\text{data}}$. For indices $i \\neq j$, define $\\operatorname{sgn}(a) \\in \\{-1,0,1\\}$ as the sign of $a$. A pair $(i,j)$ is concordant if $\\operatorname{sgn}\\big(v_{\\text{model}}(i) - v_{\\text{model}}(j)\\big) = \\operatorname{sgn}\\big(v_{\\text{data}}(i) - v_{\\text{data}}(j)\\big)$.\n\nAlgorithmic details:\n1. For each test case, construct the energy vector $\\{E_i\\}_{i=0}^{K-1}$ and the count vector $\\{c_i\\}_{i=0}^{K-1}$ for $K = 5$.\n2. Compute $\\log Z_\\theta$ using a numerically stable log-sum-exp on $-E$:\n   - Let $a_i = -E_i$ and $m = \\max_i a_i$. Then\n     $$\n     \\log Z_\\theta = m + \\log\\left( \\sum_{i=0}^{K-1} \\exp(a_i - m) \\right).\n     $$\n   This avoids overflow and underflow by recentring.\n3. Form $v_{\\text{model}}$ as $v_{\\text{model},i} = E_i + \\log Z_\\theta$.\n4. Compute smoothed empirical probabilities with $\\alpha = 1$:\n   $$\n   \\hat{p}_i = \\frac{c_i + 1}{N + 5}, \\quad \\text{where } N = \\sum_{i=0}^{4} c_i.\n   $$\n   Then set $v_{\\text{data},i} = -\\log \\hat{p}_i$.\n5. Compute $\\rho_S$ and $\\tau$ between $v_{\\text{model}}$ and $v_{\\text{data}}$. If either is not a number, set it to $0.0$.\n6. Identify top-$k$ indices using the lexicographic tie-breaking rule on $(\\text{value}, \\text{index})$. For top-$1$, compare the single minimal index in both vectors for equality to produce a boolean. For top-$2$, compute the intersection size of the two sets of indices and divide by $2$ to produce a decimal.\n7. Compute pairwise concordance by iterating over all unordered pairs $\\{(i,j) \\mid 0 \\le i  j \\le 4\\}$, comparing the signs of differences as defined, and dividing the number of concordant pairs by the total number of pairs $\\binom{5}{2} = 10$.\n8. Round all floating-point metrics to $6$ decimal places; output the top-$1$ agreement as a boolean.\n\nExplanation of the test cases:\n- Test case $1$ is constructed to be nearly perfectly calibrated: $E_i = -\\log p_i + 0.3$ with $p_i = c_i / 100$. Since the partition function satisfies $\\log Z_\\theta = -0.3$ in this construction, we obtain $E_i + \\log Z_\\theta = -\\log p_i$ exactly, so ranks match. Rank metrics should indicate strong agreement.\n- Test case $2$ adds controlled noise $\\epsilon_i$ to the calibrated energies and a constant shift, creating a modest deviation. The ranking should largely agree, but not perfectly.\n- Test case $3$ has uniform counts and constant energies. Both $v_{\\text{model}}$ and $v_{\\text{data}}$ are constant, leading to undefined correlation metrics that we set to $0.0$ by rule. Top-$k$ metrics are determined by the tie-breaking rule.\n- Test case $4$ is highly skewed with zeros in counts and a strongly miscalibrated energy ordering. Smoothing with $\\alpha = 1$ avoids infinities. Rank metrics should reflect poor agreement.\n\nThe final program implements these steps for the four test cases, computes the metrics in the order $[\\,\\rho_S,\\,\\tau,\\,\\text{top1},\\,\\text{top2overlap},\\,\\text{pairwiseConcordance}\\,]$ for each case, rounds floating-point values to $6$ decimals, and prints a single line containing a Python-like list of these lists.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import spearmanr, kendalltau\n\ndef stable_logsumexp(a: np.ndarray) - float:\n    \"\"\"\n    Compute log(sum(exp(a))) in a numerically stable way.\n    \"\"\"\n    m = np.max(a)\n    return float(m + np.log(np.sum(np.exp(a - m))))\n\ndef lex_top_k_indices(values: np.ndarray, k: int) - np.ndarray:\n    \"\"\"\n    Return indices of the k smallest values using lexicographic tie-breaking:\n    first by value ascending, then by index ascending.\n    \"\"\"\n    idx = np.arange(values.shape[0])\n    # np.lexsort sorts by last key first; we want to sort by values then index.\n    order = np.lexsort((idx, values))\n    return order[:k]\n\ndef pairwise_concordance(v1: np.ndarray, v2: np.ndarray) - float:\n    \"\"\"\n    Compute the fraction of unordered pairs whose ordering (or equality) is consistent\n    between v1 and v2 using sign of pairwise differences.\n    \"\"\"\n    n = v1.shape[0]\n    total_pairs = n * (n - 1) // 2\n    concordant = 0\n    for i in range(n):\n        for j in range(i + 1, n):\n            d1 = v1[i] - v1[j]\n            d2 = v2[i] - v2[j]\n            s1 = 0\n            if d1  0:\n                s1 = 1\n            elif d1  0:\n                s1 = -1\n            s2 = 0\n            if d2  0:\n                s2 = 1\n            elif d2  0:\n                s2 = -1\n            if s1 == s2:\n                concordant += 1\n    return concordant / total_pairs if total_pairs  0 else 0.0\n\ndef metrics_for_case(energies: np.ndarray, counts: np.ndarray, alpha: float = 1.0, top_k: int = 2):\n    \"\"\"\n    Compute v_model = E + logZ and v_data = -log p_hat with Laplace smoothing alpha,\n    then compute rank-based metrics:\n      - Spearman rho_S\n      - Kendall tau\n      - Top-1 agreement (boolean)\n      - Top-k overlap fraction (decimal)\n      - Pairwise concordance fraction (decimal)\n    Returns a list: [rho_S, tau, top1_bool, topk_overlap, pairwise_conc].\n    \"\"\"\n    # Compute log Z via stable log-sum-exp on -energies\n    logZ = stable_logsumexp(-energies)\n\n    # Model negative log values\n    v_model = energies + logZ\n\n    # Smoothed empirical probabilities\n    N = np.sum(counts)\n    K = counts.shape[0]\n    p_hat = (counts.astype(float) + alpha) / (N + alpha * K)\n\n    # Data negative log values\n    v_data = -np.log(p_hat)\n\n    # Spearman correlation\n    spearman_corr = spearmanr(v_model, v_data).correlation\n    if np.isnan(spearman_corr):\n        spearman_corr = 0.0\n\n    # Kendall tau\n    kendall_corr = kendalltau(v_model, v_data).correlation\n    if np.isnan(kendall_corr):\n        kendall_corr = 0.0\n\n    # Top-1 agreement using deterministic tie-breaking\n    top1_model_idx = lex_top_k_indices(v_model, 1)[0]\n    top1_data_idx = lex_top_k_indices(v_data, 1)[0]\n    top1_agree = (int(top1_model_idx) == int(top1_data_idx))\n\n    # Top-k overlap\n    k = top_k\n    topk_model = set(lex_top_k_indices(v_model, k).tolist())\n    topk_data = set(lex_top_k_indices(v_data, k).tolist())\n    overlap_fraction = len(topk_model.intersection(topk_data)) / k\n\n    # Pairwise concordance\n    conc_fraction = pairwise_concordance(v_model, v_data)\n\n    # Round float metrics to 6 decimals\n    rho_S = round(float(spearman_corr), 6)\n    tau = round(float(kendall_corr), 6)\n    topk_overlap = round(float(overlap_fraction), 6)\n    conc_fraction = round(float(conc_fraction), 6)\n\n    return [rho_S, tau, top1_agree, topk_overlap, conc_fraction]\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case 1: near-perfect calibration by construction\n    counts1 = np.array([50, 30, 10, 7, 3], dtype=float)  # N=100\n    p1 = counts1 / np.sum(counts1)\n    c_shift1 = 0.3\n    energies1 = -np.log(p1) + c_shift1\n\n    # Case 2: moderate mismatch with controlled noise\n    counts2 = np.array([40, 25, 20, 10, 5], dtype=float)  # N=100\n    p2 = counts2 / np.sum(counts2)\n    c_shift2 = 0.2\n    noise2 = np.array([0.05, -0.02, 0.1, -0.05, -0.08], dtype=float)\n    energies2 = -np.log(p2) + c_shift2 + noise2\n\n    # Case 3: uniform data, constant energies\n    counts3 = np.array([20, 20, 20, 20, 20], dtype=float)  # N=100\n    energies3 = np.array([2, 2, 2, 2, 2], dtype=float)\n\n    # Case 4: extreme skew with zeros, miscalibrated energies\n    counts4 = np.array([0, 1, 0, 0, 99], dtype=float)  # N=100\n    energies4 = np.array([1.0, 2.0, 3.0, 4.0, 5.0], dtype=float)\n\n    test_cases = [\n        (energies1, counts1),\n        (energies2, counts2),\n        (energies3, counts3),\n        (energies4, counts4),\n    ]\n\n    results = []\n    for E, c in test_cases:\n        res = metrics_for_case(E, c, alpha=1.0, top_k=2)\n        results.append(res)\n\n    # Final print statement in the exact required format: single line list of lists.\n    print(str(results))\n\nsolve()\n```", "id": "3122243"}]}