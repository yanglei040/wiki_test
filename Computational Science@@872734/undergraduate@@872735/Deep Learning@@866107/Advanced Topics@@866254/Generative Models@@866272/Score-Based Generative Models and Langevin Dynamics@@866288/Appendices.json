{"hands_on_practices": [{"introduction": "Before diving into complex neural networks to approximate score functions, it is essential to build intuition with a foundational case. This exercise involves deriving the analytical score function for the multivariate Gaussian distribution, one of the most fundamental distributions in statistics. By working through this derivation, you will see how the abstract definition $s(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\ln p(\\mathbf{x})$ translates into a simple and intuitive linear function, providing a crucial sanity check for understanding the behavior of the drift term in Langevin dynamics.", "problem": "Consider a $d$-dimensional random variable $x \\in \\mathbb{R}^{d}$ distributed according to a multivariate Gaussian with mean $\\mu \\in \\mathbb{R}^{d}$ and strictly positive-definite covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$. The probability density function is\n$$\np(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\!\\left(-\\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu)\\right).\n$$\nIn score-based generative models, the score function is defined as $s(x) = \\nabla_{x} \\ln p(x)$. Starting from the provided density and the definition of the score function, derive an explicit analytic expression for $s(x)$ in terms of $\\mu$ and $\\Sigma$. Then briefly explain, using the overdamped Langevin dynamics stochastic differential equation (SDE) $dX_{t} = s(X_{t})\\, dt + \\sqrt{2}\\, dW_{t}$ (where $W_{t}$ is a standard $d$-dimensional Wiener process), why the deterministic drift is an affine function of $x$ and identify the point where this drift vanishes.\n\nProvide your final result by giving the closed-form expression for $s(x)$ only. No numerical rounding is required, and no physical units are involved.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It provides all necessary definitions and constraints to derive a unique and meaningful solution within the standard mathematical framework of probability theory and stochastic processes.\n\nThe starting point is the probability density function (PDF) of a $d$-dimensional multivariate Gaussian distribution for a random variable $x \\in \\mathbb{R}^{d}$ with mean $\\mu \\in \\mathbb{R}^{d}$ and a strictly positive-definite covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$:\n$$\np(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\!\\left(-\\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu)\\right)\n$$\nThe score function, denoted $s(x)$, is defined as the gradient of the logarithm of the probability density function with respect to $x$:\n$$\ns(x) = \\nabla_{x} \\ln p(x)\n$$\nTo derive the expression for $s(x)$, we first compute the natural logarithm of $p(x)$:\n$$\n\\ln p(x) = \\ln \\left( \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\!\\left(-\\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu)\\right) \\right)\n$$\nUsing the properties of the logarithm, specifically $\\ln(a \\cdot b) = \\ln(a) + \\ln(b)$ and $\\ln(\\exp(c)) = c$, we can separate the terms:\n$$\n\\ln p(x) = \\ln \\left( \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\right) + \\ln \\left( \\exp\\!\\left(-\\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu)\\right) \\right)\n$$\n$$\n\\ln p(x) = -\\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(|\\Sigma|) - \\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu)\n$$\nThe first two terms, $-\\frac{d}{2} \\ln(2\\pi)$ and $-\\frac{1}{2} \\ln(|\\Sigma|)$, are constant with respect to the variable $x$. Therefore, their gradient is zero. We can now compute the score function $s(x)$ by taking the gradient of $\\ln p(x)$:\n$$\ns(x) = \\nabla_{x} \\left( -\\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(|\\Sigma|) - \\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu) \\right)\n$$\n$$\ns(x) = 0 - 0 - \\frac{1}{2} \\nabla_{x} \\left( (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu) \\right)\n$$\nTo evaluate the gradient of the quadratic form, we use a standard result from matrix calculus. For a vector variable $z$ and a symmetric matrix $A$, the gradient is given by $\\nabla_{z} (z^{\\top} A z) = 2 A z$. In our case, the variable is $x$, the vector corresponding to $z$ is $(x - \\mu)$, and the matrix corresponding to $A$ is $\\Sigma^{-1}$. The covariance matrix $\\Sigma$ is symmetric by definition, and the inverse of a symmetric matrix is also symmetric, so $\\Sigma^{-1}$ is symmetric. Applying the chain rule, we have:\n$$\n\\nabla_{x} \\left( (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu) \\right) = 2 \\Sigma^{-1} (x - \\mu)\n$$\nSubstituting this result back into the expression for $s(x)$:\n$$\ns(x) = -\\frac{1}{2} \\left( 2 \\Sigma^{-1} (x - \\mu) \\right) = - \\Sigma^{-1} (x - \\mu)\n$$\nThis is the explicit analytic expression for the score function of a multivariate Gaussian distribution.\n\nThe problem further asks to explain why the deterministic drift in the overdamped Langevin dynamics SDE is an affine function of $x$ and to identify where it vanishes. The SDE is given by:\n$$\ndX_{t} = s(X_{t})\\, dt + \\sqrt{2}\\, dW_{t}\n$$\nThe deterministic drift term is $s(X_{t})$. Based on our derived expression for the score, we can write this as:\n$$\ns(X_t) = -\\Sigma^{-1}(X_t - \\mu) = -\\Sigma^{-1}X_t + \\Sigma^{-1}\\mu\n$$\nThis expression is in the form $A x + b$, where $A = -\\Sigma^{-1}$ is a $d \\times d$ matrix and $b = \\Sigma^{-1}\\mu$ is a $d \\times 1$ vector. A function of this form is, by definition, an affine transformation. Therefore, the deterministic drift is an affine function of the state $X_t$.\n\nTo find the point where the drift vanishes, we set $s(x) = 0$:\n$$\n- \\Sigma^{-1} (x - \\mu) = 0\n$$\nSince $\\Sigma$ is a strictly positive-definite matrix, it is invertible, and its inverse $\\Sigma^{-1}$ is also invertible. We can multiply both sides of the equation by $-\\Sigma$ from the left:\n$$\n(-\\Sigma)(-\\Sigma^{-1}) (x - \\mu) = (-\\Sigma) 0\n$$\n$$\nI (x - \\mu) = 0\n$$\nwhere $I$ is the $d \\times d$ identity matrix. This simplifies to:\n$$\nx - \\mu = 0 \\implies x = \\mu\n$$\nThus, the drift vanishes at $x = \\mu$, which is the mean of the Gaussian distribution. This is expected, as the score function points in the direction of the steepest ascent of the log-probability, and the gradient must be zero at the mode of the distribution, which for a Gaussian is its mean.", "answer": "$$\\boxed{-\\Sigma^{-1}(x - \\mu)}$$", "id": "3172987"}, {"introduction": "The Langevin dynamics SDE describes a continuous-time process, but implementing it on a computer requires discretization into finite time steps. The choice of numerical integration scheme is not merely a technical detail; it critically affects the stability and accuracy of the sampling process. In this practice, you will implement and compare the simple Euler-Maruyama sampler against a more sophisticated predictor-corrector method for a one-dimensional system [@problem_id:3172952]. This hands-on comparison will illuminate the practical bias-variance trade-offs involved in simulating SDEs and how different samplers converge to slightly different stationary distributions.", "problem": "You must write a complete, runnable program that compares a simple Euler–Maruyama sampler to a predictor–corrector sampler for a one-dimensional score-based process. The comparison must be based on the bias–variance tradeoff for estimating the second moment of the stationary distribution generated by each sampler. Begin from the following fundamental bases and definitions and do not assume any shortcut formulas.\n\nConsider a one-dimensional stochastic differential equation (SDE) driven by a model score function in score-based generative modeling. Let the score function be $s_{\\theta}(x) = -\\left(1+\\delta\\right)x$, where $\\delta$ is a scalar parameter encoding model mismatch relative to the true score of a standard normal target. The SDE is\n$$\n\\mathrm{d}X_t = s_{\\theta}(X_t) \\,\\mathrm{d}t + \\sqrt{2}\\,\\mathrm{d}W_t,\n$$\nwhere $W_t$ is a standard Wiener process.\n\nDiscretize the SDE with time step $h$ using the Euler–Maruyama method (Euler–Maruyama (EM)), which produces updates of the form\n$$\nX_{k+1} = X_k + h\\, s_{\\theta}(X_k) + \\sqrt{2h}\\, Z_k,\n$$\nwhere $Z_k \\sim \\mathcal{N}(0,1)$ are independent standard normal random variables.\n\nDefine a predictor–corrector (PC) scheme that first applies the Euler–Maruyama predictor as above to obtain an intermediate $X_{k+1}^{(\\text{pred})}$, and then applies a deterministic corrector step using the same score function with step $c$:\n$$\nX_{k+1} = X_{k+1}^{(\\text{pred})} + c\\, s_{\\theta}\\!\\left(X_{k+1}^{(\\text{pred})}\\right).\n$$\nThis PC scheme combines a stochastic predictor with a deterministic drift corrector based on $s_{\\theta}$.\n\nThe target second moment of the true distribution is that of a standard normal variable, namely $E[X^2] = 1$. For each sampler (EM and PC), assume the number of steps $T$ is sufficiently large for the chain to reach stationarity. Focusing on the second moment $E[X^2]$ under the stationary distribution induced by each scheme, compute:\n- The absolute bias, defined as $\\left| E[X^2] - 1 \\right|$.\n- The variance of the Monte Carlo estimator of $E[X^2]$ formed by the average of $M$ independent samples from the stationary distribution. Under the stationary distribution, for $X \\sim \\mathcal{N}(0, V)$, use the fact that $E[X^2] = V$ and $E[X^4] = 3V^2$ to determine the estimator variance of the sample mean of $X^2$ across $M$ independent samples.\n\nYour program must compute these quantities for both EM and PC for each test case specified below. Use the independence idealization at stationarity to express the estimator variance as a closed-form function of the stationary variance $V$ and the sample size $M$.\n\nTest suite (each test case is a tuple $(\\delta, h, T, c)$):\n1. $(0,\\, 0.05,\\, 400,\\, 0)$: baseline with no corrector (predictor–corrector reduces to Euler–Maruyama).\n2. $(0,\\, 0.05,\\, 400,\\, 0.005)$: corrector with small deterministic drift, matched score.\n3. $(0.1,\\, 0.05,\\, 400,\\, 0.005)$: positive model mismatch, same steps.\n4. $(-0.2,\\, 0.1,\\, 400,\\, 0.1)$: negative model mismatch, larger steps.\n\nFor the estimator variance calculation, use $M = 10000$ independent samples per method per test case.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case, append four floating-point numbers in this order: $\\left[\\text{bias}_{\\text{EM}}, \\text{var}_{\\text{EM}}, \\text{bias}_{\\text{PC}}, \\text{var}_{\\text{PC}}\\right]$.\n- Concatenate the results for all test cases into one list, preserving the order of the test suite as listed above. For example, a two-case output would look like $\\left[\\text{b}_{1,\\text{EM}}, \\text{v}_{1,\\text{EM}}, \\text{b}_{1,\\text{PC}}, \\text{v}_{1,\\text{PC}}, \\text{b}_{2,\\text{EM}}, \\text{v}_{2,\\text{EM}}, \\text{b}_{2,\\text{PC}}, \\text{v}_{2,\\text{PC}}\\right]$.\n\nYour program must be self-contained, require no user input, and adhere to the execution environment constraints.", "solution": "The problem requires a comparison of the Euler–Maruyama (EM) and a predictor–corrector (PC) sampler for a one-dimensional Ornstein–Uhlenbeck-type stochastic differential equation (SDE) arising in score-based generative modeling. The comparison is based on the bias and variance for estimating the second moment of the stationary distribution generated by each sampler. We will first derive analytical expressions for the stationary variance of each numerical scheme.\n\nThe SDE is given by:\n$$\n\\mathrm{d}X_t = s_{\\theta}(X_t) \\,\\mathrm{d}t + \\sqrt{2}\\,\\mathrm{d}W_t\n$$\nwith the score function $s_{\\theta}(x) = -(1+\\delta)x$. Substituting the score function, we get a linear SDE:\n$$\n\\mathrm{d}X_t = -(1+\\delta)X_t \\,\\mathrm{d}t + \\sqrt{2}\\,\\mathrm{d}W_t\n$$\nThe stationary distribution of this continuous-time process is a zero-mean normal distribution, $\\mathcal{N}(0, V_{true})$, where the variance $V_{true}$ satisfies the fluctuation-dissipation relation $-(1+\\delta)V_{true} + 1 = 0$, which gives $V_{true} = 1/(1+\\delta)$. The problem, however, specifies the target second moment as $E[X^2] = 1$, corresponding to a standard normal distribution (i.e., the case where $\\delta=0$ in the underlying SDE). The bias will be measured against this target value of $1$.\n\nBoth discretization schemes are linear, of the general form $X_{k+1} = A X_k + B Z_k$, where $Z_k \\sim \\mathcal{N}(0,1)$. This is an autoregressive process of order 1 (AR(1)). If $|A|1$, the process has a unique stationary distribution. Since the innovation $Z_k$ is Gaussian, the stationary distribution is also Gaussian with mean zero, $X \\sim \\mathcal{N}(0, V)$. The variance $V$ can be found by equating the variance of both sides at stationarity:\n$$\n\\mathrm{Var}(X_{k+1}) = \\mathrm{Var}(A X_k + B Z_k)\n$$\nSince $X_k$ and $Z_k$ are independent,\n$$\nV = A^2 \\mathrm{Var}(X_k) + B^2 \\mathrm{Var}(Z_k) = A^2 V + B^2\n$$\nSolving for $V$, we get the stationary variance:\n$$\nV = \\frac{B^2}{1 - A^2}\n$$\nThis formula is the cornerstone of our analysis.\n\nFor each sampler, we first find its stationary variance $V$. The quantities of interest are then derived from $V$:\n1.  The absolute bias of the second moment estimator: $b = |E[X^2] - 1| = |V-1|$.\n2.  The variance of the Monte Carlo estimator for $E[X^2]$. The estimator is the sample mean $\\hat{E}[X^2] = \\frac{1}{M}\\sum_{i=1}^M X_i^2$, where $X_i \\sim \\mathcal{N}(0, V)$ are $M$ independent samples from the stationary distribution. The variance of this estimator is:\n    $$\n    \\mathrm{Var}(\\hat{E}[X^2]) = \\mathrm{Var}\\left(\\frac{1}{M}\\sum_{i=1}^M X_i^2\\right) = \\frac{1}{M}\\mathrm{Var}(X^2)\n    $$\n    For $X \\sim \\mathcal{N}(0, V)$, we are given $E[X^4] = 3V^2$. Thus:\n    $$\n    \\mathrm{Var}(X^2) = E[(X^2)^2] - (E[X^2])^2 = E[X^4] - V^2 = 3V^2 - V^2 = 2V^2\n    $$\n    The estimator variance is therefore $v = \\frac{2V^2}{M}$.\n\nNow, we apply this framework to each sampler.\n\n**1. Euler–Maruyama (EM) Sampler**\n\nThe update rule is:\n$$\nX_{k+1} = X_k + h s_{\\theta}(X_k) + \\sqrt{2h} Z_k\n$$\nSubstituting $s_{\\theta}(X_k) = -(1+\\delta)X_k$:\n$$\nX_{k+1} = X_k - h(1+\\delta)X_k + \\sqrt{2h} Z_k = \\left(1 - h(1+\\delta)\\right)X_k + \\sqrt{2h} Z_k\n$$\nThis is an AR(1) process with coefficients:\n$$\nA_{\\text{EM}} = 1 - h(1+\\delta) \\quad \\text{and} \\quad B_{\\text{EM}} = \\sqrt{2h}\n$$\nThe stationary variance $V_{\\text{EM}}$ is:\n$$\nV_{\\text{EM}} = \\frac{B_{\\text{EM}}^2}{1-A_{\\text{EM}}^2} = \\frac{(\\sqrt{2h})^2}{1 - (1 - h(1+\\delta))^2} = \\frac{2h}{1 - (1 - 2h(1+\\delta) + h^2(1+\\delta)^2)} = \\frac{2h}{2h(1+\\delta) - h^2(1+\\delta)^2}\n$$\n$$\nV_{\\text{EM}} = \\frac{2}{(1+\\delta)(2 - h(1+\\delta))}\n$$\nThe bias and estimator variance for the EM sampler are:\n$$\n\\text{bias}_{\\text{EM}} = |V_{\\text{EM}} - 1| \\quad \\text{and} \\quad \\text{var}_{\\text{EM}} = \\frac{2V_{\\text{EM}}^2}{M}\n$$\n\n**2. Predictor-Corrector (PC) Sampler**\n\nThe PC scheme consists of a predictor step followed by a corrector step.\nPredictor:\n$$\nX_{k+1}^{(\\text{pred})} = X_k + h s_{\\theta}(X_k) + \\sqrt{2h} Z_k = (1 - h(1+\\delta))X_k + \\sqrt{2h} Z_k\n$$\nCorrector:\n$$\nX_{k+1} = X_{k+1}^{(\\text{pred})} + c s_{\\theta}(X_{k+1}^{(\\text{pred})}) = X_{k+1}^{(\\text{pred})} - c(1+\\delta)X_{k+1}^{(\\text{pred})} = (1 - c(1+\\delta))X_{k+1}^{(\\text{pred})}\n$$\nCombining these two steps, we express $X_{k+1}$ directly in terms of $X_k$:\n$$\nX_{k+1} = (1 - c(1+\\delta)) \\left[ (1 - h(1+\\delta))X_k + \\sqrt{2h}Z_k \\right]\n$$\n$$\nX_{k+1} = (1 - c(1+\\delta))(1 - h(1+\\delta)) X_k + (1 - c(1+\\delta))\\sqrt{2h} Z_k\n$$\nThis is again an AR(1) process with coefficients:\n$$\nA_{\\text{PC}} = (1 - c(1+\\delta))(1 - h(1+\\delta)) \\quad \\text{and} \\quad B_{\\text{PC}} = (1 - c(1+\\delta))\\sqrt{2h}\n$$\nThe stationary variance $V_{\\text{PC}}$ is:\n$$\nV_{\\text{PC}} = \\frac{B_{\\text{PC}}^2}{1-A_{\\text{PC}}^2} = \\frac{\\left((1 - c(1+\\delta))\\sqrt{2h}\\right)^2}{1 - \\left((1 - c(1+\\delta))(1 - h(1+\\delta))\\right)^2}\n$$\n$$\nV_{\\text{PC}} = \\frac{(1 - c(1+\\delta))^2 (2h)}{1 - (1 - c(1+\\delta))^2 (1 - h(1+\\delta))^2}\n$$\nThe bias and estimator variance for the PC sampler are:\n$$\n\\text{bias}_{\\text{PC}} = |V_{\\text{PC}} - 1| \\quad \\text{and} \\quad \\text{var}_{\\text{PC}} = \\frac{2V_{\\text{PC}}^2}{M}\n$$\nNote that if $c=0$, $A_{\\text{PC}}=A_{\\text{EM}}$ and $B_{\\text{PC}}=B_{\\text{EM}}$, so $V_{\\text{PC}} = V_{\\text{EM}}$. This is consistent, as a zero-step corrector makes the PC scheme identical to the EM scheme. The program will implement these derived formulas to compute the required quantities for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias-variance tradeoff for EM and PC samplers for a 1D SDE.\n\n    The solution is based on the analytical derivation of the stationary variance\n    for the discretized processes, which are AR(1) models.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple is (delta, h, T, c). The parameter T is not used in the\n    # analytical stationary-state calculation.\n    test_cases = [\n        (0.0, 0.05, 400, 0.0),\n        (0.0, 0.05, 400, 0.005),\n        (0.1, 0.05, 400, 0.005),\n        (-0.2, 0.1, 400, 0.1),\n    ]\n\n    # Sample size for Monte Carlo estimator variance calculation.\n    M = 10000.0\n\n    results = []\n    for delta, h, T, c in test_cases:\n        # Common term in the score function s_theta(x) = -(1 + delta) * x\n        one_plus_delta = 1.0 + delta\n\n        # --- Euler-Maruyama (EM) Analysis ---\n        \n        # The EM update is X_{k+1} = (1 - h*(1+delta)) * X_k + sqrt(2*h) * Z_k\n        # This is an AR(1) process X_{k+1} = A*X_k + B*Z_k\n        # The stationary variance is V_EM = B^2 / (1 - A^2)\n        \n        # A_em = 1.0 - h * one_plus_delta\n        # B_em_sq = 2.0 * h\n        # v_em = B_em_sq / (1.0 - A_em_sq**2)\n        \n        # Using an expanded form for better numerical stability with small h\n        v_em_numerator = 2.0\n        v_em_denominator = one_plus_delta * (2.0 - h * one_plus_delta)\n\n        # Ensure stability by checking denominator  0, which corresponds to\n        # 0  h*(1+delta)  2\n        if v_em_denominator = 0:\n            # This case won't be hit with the given test values.\n            v_em = float('inf')\n        else:\n            v_em = v_em_numerator / v_em_denominator\n        \n        # Absolute bias: |E[X^2] - 1| = |V_em - 1|\n        bias_em = abs(v_em - 1.0)\n        \n        # Estimator variance: Var([sum X_i^2]/M) = Var(X^2) / M = 2*V_em^2/M\n        var_em_estimator = (2.0 * v_em**2) / M\n\n        # --- Predictor-Corrector (PC) Analysis ---\n\n        # The PC update is X_{k+1} = (1-c*(1+d)) * X_{k+1_pred}\n        # which results in X_{k+1} = A_pc * X_k + B_pc * Z_k\n        \n        # Coefficients for the AR(1) form of the PC scheme\n        alpha_h = 1.0 - h * one_plus_delta\n        alpha_c = 1.0 - c * one_plus_delta\n        \n        A_pc = alpha_c * alpha_h\n        B_pc_sq = (alpha_c**2) * (2.0 * h)\n        \n        v_pc_numerator = B_pc_sq\n        v_pc_denominator = 1.0 - A_pc**2\n        \n        # Ensure stability by checking |A_pc|  1\n        if v_pc_denominator = 0:\n            # This case won't be hit with the given test values.\n            v_pc = float('inf')\n        else:\n            v_pc = v_pc_numerator / v_pc_denominator\n\n        # Absolute bias: |E[X^2] - 1| = |V_pc - 1|\n        bias_pc = abs(v_pc - 1.0)\n        \n        # Estimator variance: 2*V_pc^2/M\n        var_pc_estimator = (2.0 * v_pc**2) / M\n\n        results.extend([bias_em, var_em_estimator, bias_pc, var_pc_estimator])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3172952"}, {"introduction": "A true score function $s(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$ is a gradient field, which in vector calculus is known as a conservative field. A key property is that its line integral between two points is independent of the path taken. However, a neural network $s_\\theta(\\mathbf{x})$ only approximates the true score and may not be perfectly conservative, often containing a residual \"rotational\" component. This exercise provides a powerful method to diagnose this issue by numerically calculating the line integral of a score field along two different paths between the same endpoints [@problem_id:3172994]. By measuring the path dependence, you can quantify the non-conservativity of the score model, gaining deep insight into a fundamental source of error in score-based generation.", "problem": "Consider a two-dimensional density $p(\\mathbf{x})$ on $\\mathbb{R}^2$ and a score-based generative model that produces a parametric score field $s_\\theta(\\mathbf{x})$. The score field is intended to approximate the true score $s(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$. In overdamped Langevin dynamics, which is a Stochastic Differential Equation (SDE) used for sampling, the continuous-time update is $\\mathrm{d}\\mathbf{x}_t = \\tfrac{1}{2}s_\\theta(\\mathbf{x}_t)\\,\\mathrm{d}t + \\mathrm{d}\\mathbf{W}_t$, where $\\mathbf{W}_t$ is a standard Wiener process. If $s_\\theta(\\mathbf{x})$ equals the true gradient field, then by the fundamental theorem for line integrals, the difference $\\log p(\\mathbf{x}) - \\log p(\\mathbf{x}_0)$ can be computed as a path integral of $s_\\theta$ between $\\mathbf{x}_0$ and $\\mathbf{x}$. Deviations from a conservative (gradient) field can be detected by the path dependence of such line integrals.\n\nYour task is to implement a program that:\n- Defines a scientifically plausible target distribution $p(\\mathbf{x})$ as a mixture of two Gaussians in $d = 2$ dimensions with equal weights. Use\n$$\np(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathcal{N}\\!\\left(\\mathbf{x}\\mid\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_1\\right) + \\tfrac{1}{2}\\,\\mathcal{N}\\!\\left(\\mathbf{x}\\mid\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}_2\\right),\n$$\nwhere $\\boldsymbol{\\mu}_1 = [0,0]^\\top$, $\\boldsymbol{\\mu}_2 = [2,-1]^\\top$, $\\boldsymbol{\\Sigma}_1 = \\begin{bmatrix}1.0  0.3 \\\\ 0.3  0.5\\end{bmatrix}$, and $\\boldsymbol{\\Sigma}_2 = \\begin{bmatrix}0.9  -0.2 \\\\ -0.2  0.8\\end{bmatrix}$.\n- Computes the true score $s(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$ from first principles using $s(\\mathbf{x}) = \\left(\\nabla_{\\mathbf{x}} p(\\mathbf{x})\\right)/p(\\mathbf{x})$ and the identity $\\nabla_{\\mathbf{x}} \\mathcal{N}(\\mathbf{x}\\mid\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) = -\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\,\\mathcal{N}(\\mathbf{x}\\mid\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$.\n- Constructs a parametric score field $s_\\theta(\\mathbf{x})$ that includes a controlled non-conservative component:\n$$\ns_\\theta(\\mathbf{x}) = s(\\mathbf{x}) + \\varepsilon\\,\\mathbf{R}(\\mathbf{x}),\n$$\nwhere $\\varepsilon \\ge 0$ is a scalar parameter and $\\mathbf{R}(\\mathbf{x})$ is a rotational field in $d=2$ defined by\n$$\n\\mathbf{R}(\\mathbf{x}) = \\frac{\\mathbf{Q}\\,(\\mathbf{x} - \\mathbf{c})}{1 + \\lVert \\mathbf{x} - \\mathbf{c} \\rVert^2}, \\quad \\mathbf{Q} = \\begin{bmatrix}0  -1 \\\\ 1  0\\end{bmatrix}, \\quad \\mathbf{c} = [0.5,-0.5]^\\top.\n$$\nNote that $\\mathbf{Q}$ rotates vectors by $90^\\circ$; $\\mathbf{R}(\\mathbf{x})$ has nonzero curl and thus is not the gradient of any scalar potential.\n- Estimates $\\log p(\\mathbf{x}) - \\log p(\\mathbf{x}_0)$ by numerically integrating $s_\\theta(\\mathbf{x})$ along two paths that share endpoints:\n    1. A straight path\n    $$\n    \\boldsymbol{\\gamma}_{\\text{straight}}(t) = \\mathbf{x}_0 + t\\,(\\mathbf{x} - \\mathbf{x}_0), \\quad t \\in [0,1],\n    $$\n    with $\\boldsymbol{\\gamma}'_{\\text{straight}}(t) = \\mathbf{x} - \\mathbf{x}_0$.\n    2. A curved path with a sinusoidal detour orthogonal to the displacement:\n    $$\n    \\boldsymbol{\\gamma}_{\\text{curved}}(t) = \\mathbf{x}_0 + t\\,\\mathbf{d} + a\\,\\sin(\\pi t)\\,\\hat{\\mathbf{n}}, \\quad t \\in [0,1],\n    $$\n    where $\\mathbf{d} = \\mathbf{x} - \\mathbf{x}_0$, $\\hat{\\mathbf{n}} = \\frac{\\mathbf{Q}\\,\\mathbf{d}}{\\lVert \\mathbf{d} \\rVert}$ is the unit normal obtained by rotating $\\mathbf{d}$ by $90^\\circ$, and $a \\ge 0$ controls detour amplitude. The path derivative is\n    $$\n    \\boldsymbol{\\gamma}'_{\\text{curved}}(t) = \\mathbf{d} + a\\,\\pi\\,\\cos(\\pi t)\\,\\hat{\\mathbf{n}}.\n    $$\n- Uses a uniform discretization of $t \\in [0,1]$ with $N$ steps and the composite trapezoidal rule to approximate the line integrals\n$$\nI_{\\text{path}} \\approx \\int_{0}^{1} s_\\theta(\\boldsymbol{\\gamma}(t)) \\cdot \\boldsymbol{\\gamma}'(t)\\,\\mathrm{d}t.\n$$\nIf $\\lVert \\mathbf{d} \\rVert = 0$, both integrals must be reported as $0$ to satisfy the fundamental theorem under zero displacement.\n\nThen, for each test case in the suite below, compute two quantities:\n- $D = \\left\\lvert I_{\\text{straight}} - I_{\\text{curved}} \\right\\rvert$, which quantifies path dependence and thus non-conservativity of $s_\\theta$.\n- $E = \\left\\lvert I_{\\text{straight}} - \\left(\\log p(\\mathbf{x}) - \\log p(\\mathbf{x}_0)\\right) \\right\\rvert$, which measures the absolute error of the straight-path estimate relative to the true log-density difference.\n\nUse $N = 4096$ discretization steps. For numerical stability in computing $\\log p(\\mathbf{x})$, use a mathematically sound log-sum-exp consolidation of the mixture components. Round each reported float to $6$ decimal places.\n\nTest suite:\n- Case $1$: $\\mathbf{x}_0 = [0,0]^\\top$, $\\mathbf{x} = [2,1]^\\top$, $\\varepsilon = 0.0$, $a = 0.5$.\n- Case $2$: $\\mathbf{x}_0 = [0,0]^\\top$, $\\mathbf{x} = [2,1]^\\top$, $\\varepsilon = 0.05$, $a = 0.5$.\n- Case $3$: $\\mathbf{x}_0 = [0,0]^\\top$, $\\mathbf{x} = [2,1]^\\top$, $\\varepsilon = 0.2$, $a = 0.5$.\n- Case $4$ (boundary): $\\mathbf{x}_0 = [1,-1]^\\top$, $\\mathbf{x} = [1,-1]^\\top$, $\\varepsilon = 0.25$, $a = 1.0$.\n- Case $5$ (high curvature): $\\mathbf{x}_0 = [0,-2]^\\top$, $\\mathbf{x} = [3,1]^\\top$, $\\varepsilon = 0.1$, $a = 2.0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a list of lists, one inner list per test case in order. Each inner list must be of the form $[D,E]$ with both entries rounded to $6$ decimal places. For example: $[[d_1,e_1],[d_2,e_2],\\dots]$.", "solution": "The problem requires an analysis of a parametric score field $s_\\theta(\\mathbf{x})$ used in score-based generative models. A key property of the true score field, $s(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$, is that it is a conservative vector field, meaning its line integral between two points is path-independent. An approximate score field $s_\\theta(\\mathbf{x})$ may fail to be conservative, introducing errors in sampling algorithms like Langevin dynamics. This exercise quantifies the effects of a non-conservative perturbation on line integrals of the score.\n\nThe solution proceeds in several steps:\n1.  Define the target probability density $p(\\mathbf{x})$ and derive expressions for its logarithm, $\\log p(\\mathbf{x})$, and its true score, $s(\\mathbf{x})$.\n2.  Define the non-conservative parametric score field $s_\\theta(\\mathbf{x})$.\n3.  Define the two integration paths, $\\boldsymbol{\\gamma}_{\\text{straight}}(t)$ and $\\boldsymbol{\\gamma}_{\\text{curved}}(t)$, and their derivatives.\n4.  Formulate the numerical line integral using the composite trapezoidal rule.\n5.  Define the metrics $D$ (path dependence) and $E$ (estimation error).\n\n**1. Target Density and True Score**\n\nThe target density is a Gaussian Mixture Model (GMM) in $d=2$ dimensions:\n$$\np(\\mathbf{x}) = \\frac{1}{2}\\,\\mathcal{N}(\\mathbf{x}\\mid\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_1) + \\frac{1}{2}\\,\\mathcal{N}(\\mathbf{x}\\mid\\boldsymbol{\\mu}_2, \\boldsymbol{\\Sigma}_2)\n$$\nwith parameters $\\boldsymbol{\\mu}_1 = [0,0]^\\top$, $\\boldsymbol{\\mu}_2 = [2,-1]^\\top$, $\\boldsymbol{\\Sigma}_1 = \\begin{bmatrix}1.0  0.3 \\\\ 0.3  0.5\\end{bmatrix}$, and $\\boldsymbol{\\Sigma}_2 = \\begin{bmatrix}0.9  -0.2 \\\\ -0.2  0.8\\end{bmatrix}$.\n\nTo compute the logarithm $\\log p(\\mathbf{x})$ stably, we use the log-sum-exp (LSE) identity. Letting $p_i(\\mathbf{x}) = \\mathcal{N}(\\mathbf{x}\\mid\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i)$, we have:\n$$\n\\log p(\\mathbf{x}) = \\log\\left(\\frac{1}{2}\\left(e^{\\log p_1(\\mathbf{x})} + e^{\\log p_2(\\mathbf{x})}\\right)\\right) = -\\log(2) + \\text{LSE}(\\log p_1(\\mathbf{x}), \\log p_2(\\mathbf{x}))\n$$\nThe log-density of a multivariate normal distribution is given by:\n$$\n\\log \\mathcal{N}(\\mathbf{x}\\mid\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) = -\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\det(\\boldsymbol{\\Sigma})| - \\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\n$$\n\nThe true score is the gradient of the log-density, $s(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$. Using the chain rule and the identity $\\nabla_{\\mathbf{x}} p(\\mathbf{x}) = p(\\mathbf{x}) \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$:\n$$\ns(\\mathbf{x}) = \\frac{\\nabla_{\\mathbf{x}} p(\\mathbf{x})}{p(\\mathbf{x})} = \\frac{\\frac{1}{2}\\nabla_{\\mathbf{x}} p_1(\\mathbf{x}) + \\frac{1}{2}\\nabla_{\\mathbf{x}} p_2(\\mathbf{x})}{\\frac{1}{2} p_1(\\mathbf{x}) + \\frac{1}{2} p_2(\\mathbf{x})}\n$$\nUsing the provided identity $\\nabla_{\\mathbf{x}} p_i(\\mathbf{x}) = p_i(\\mathbf{x}) \\nabla_{\\mathbf{x}} \\log p_i(\\mathbf{x})$ where $\\nabla_{\\mathbf{x}} \\log p_i(\\mathbf{x}) = -\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_i) = s_i(\\mathbf{x})$, the score can be expressed as a weighted average of the individual component scores:\n$$\ns(\\mathbf{x}) = \\frac{p_1(\\mathbf{x})s_1(\\mathbf{x}) + p_2(\\mathbf{x})s_2(\\mathbf{x})}{p_1(\\mathbf{x}) + p_2(\\mathbf{x})}\n$$\nThis form can be computed stably using the LSE trick by factoring out the maximum of $\\log p_1(\\mathbf{x})$ and $\\log p_2(\\mathbf{x})$.\n\n**2. Parametric Score Field**\n\nThe parametric score field $s_\\theta(\\mathbf{x})$ is constructed by adding a non-conservative (rotational) component to the true score:\n$$\ns_\\theta(\\mathbf{x}) = s(\\mathbf{x}) + \\varepsilon\\,\\mathbf{R}(\\mathbf{x})\n$$\nwhere $\\varepsilon \\ge 0$ is a scalar controlling the magnitude of the non-conservative part. The rotational field $\\mathbf{R}(\\mathbf{x})$ is defined as:\n$$\n\\mathbf{R}(\\mathbf{x}) = \\frac{\\mathbf{Q}\\,(\\mathbf{x} - \\mathbf{c})}{1 + \\lVert \\mathbf{x} - \\mathbf{c} \\rVert^2}, \\quad \\text{with } \\mathbf{Q} = \\begin{bmatrix}0  -1 \\\\ 1  0\\end{bmatrix}, \\quad \\mathbf{c} = [0.5,-0.5]^\\top.\n$$\nThe matrix $\\mathbf{Q}$ rotates a $2$D vector by $+90^\\circ$. The curl of $\\mathbf{R}(\\mathbf{x})$ is non-zero, confirming it is not a gradient field.\n\n**3. Integration Paths**\n\nWe compute the line integral of $s_\\theta(\\mathbf{x})$ from a starting point $\\mathbf{x}_0$ to an endpoint $\\mathbf{x}$ along two distinct paths, both parameterized by $t \\in [0,1]$.\n1.  **Straight path**: A direct line segment.\n    $$\n    \\boldsymbol{\\gamma}_{\\text{straight}}(t) = \\mathbf{x}_0 + t\\,(\\mathbf{x} - \\mathbf{x}_0) \\quad \\implies \\quad \\boldsymbol{\\gamma}'_{\\text{straight}}(t) = \\mathbf{x} - \\mathbf{x}_0\n    $$\n2.  **Curved path**: A sinusoidal detour. Let $\\mathbf{d} = \\mathbf{x} - \\mathbf{x}_0$ be the displacement vector and $\\hat{\\mathbf{n}} = \\mathbf{Q}\\,\\mathbf{d} / \\lVert \\mathbf{d} \\rVert$ be the orthogonal unit vector.\n    $$\n    \\boldsymbol{\\gamma}_{\\text{curved}}(t) = \\mathbf{x}_0 + t\\,\\mathbf{d} + a\\,\\sin(\\pi t)\\,\\hat{\\mathbf{n}} \\quad \\implies \\quad \\boldsymbol{\\gamma}'_{\\text{curved}}(t) = \\mathbf{d} + a\\,\\pi\\,\\cos(\\pi t)\\,\\hat{\\mathbf{n}}\n    $$\nThe parameter $a \\ge 0$ controls the amplitude of the detour.\n\n**4. Numerical Line Integral**\n\nThe line integral along a path $\\boldsymbol{\\gamma}$ is given by $I_{\\text{path}} = \\int_{\\boldsymbol{\\gamma}} s_\\theta(\\mathbf{x}) \\cdot \\mathrm{d}\\mathbf{x}$. Using the parameterization $\\boldsymbol{\\gamma}(t)$, this becomes:\n$$\nI_{\\text{path}} = \\int_{0}^{1} s_\\theta(\\boldsymbol{\\gamma}(t)) \\cdot \\boldsymbol{\\gamma}'(t)\\,\\mathrm{d}t\n$$\nThis integral is approximated numerically using the composite trapezoidal rule with $N=4096$ steps. Let $t_k = k/N$ for $k=0, 1, \\dots, N$, and let $g(t) = s_\\theta(\\boldsymbol{\\gamma}(t)) \\cdot \\boldsymbol{\\gamma}'(t)$. The integral is approximated as:\n$$\nI_{\\text{path}} \\approx \\frac{1}{N} \\left( \\frac{g(t_0) + g(t_N)}{2} + \\sum_{k=1}^{N-1} g(t_k) \\right)\n$$\nA special case occurs if $\\mathbf{x}_0 = \\mathbf{x}$, which implies $\\mathbf{d}=\\mathbf{0}$. In this case, both paths are stationary, the path derivatives are zero, and both integrals $I_{\\text{straight}}$ and $I_{\\text{curved}}$ are defined to be $0$.\n\n**5. Error Metrics**\n\nTwo metrics are computed for each test case:\n-   **Path Dependence ($D$)**: The absolute difference between the integrals along the two paths. If $s_\\theta$ were conservative, this would be $0$. A non-zero value indicates a deviation.\n    $$\n    D = \\left\\lvert I_{\\text{straight}} - I_{\\text{curved}} \\right\\rvert\n    $$\n-   **Estimation Error ($E$)**: The absolute error between the straight-path integral and the true log-density difference. For a perfectly conservative field ($\\varepsilon=0$), this reflects the numerical integration error. For $\\varepsilon  0$, it also includes the error from the non-conservative component.\n    $$\n    E = \\left\\lvert I_{\\text{straight}} - \\left(\\log p(\\mathbf{x}) - \\log p(\\mathbf{x}_0)\\right) \\right\\rvert\n    $$\n\nThe program implements these steps in a vectorized manner using `numpy` for efficiency, processing each test case to compute and report the values of $D$ and $E$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes path-dependence and estimation error for a non-conservative score field.\n    \"\"\"\n    # Define problem constants\n    D_DIM = 2\n    MU1 = np.array([0.0, 0.0])\n    MU2 = np.array([2.0, -1.0])\n    SIGMA1 = np.array([[1.0, 0.3], [0.3, 0.5]])\n    SIGMA2 = np.array([[0.9, -0.2], [-0.2, 0.8]])\n    Q = np.array([[0.0, -1.0], [1.0, 0.0]])\n    C = np.array([0.5, -0.5])\n    N_STEPS = 4096\n\n    # Pre-compute matrix inverses and log-determinants\n    SIGMA1_INV = np.linalg.inv(SIGMA1)\n    SIGMA2_INV = np.linalg.inv(SIGMA2)\n    _, LOG_DET_SIGMA1 = np.linalg.slogdet(SIGMA1)\n    _, LOG_DET_SIGMA2 = np.linalg.slogdet(SIGMA2)\n    \n    LOG_PDF_CONST = -0.5 * D_DIM * np.log(2 * np.pi)\n\n    def log_pdf_gaussian_batch(x, mu, inv_sigma, log_det_sigma):\n        \"\"\"Computes log-PDF for a batch of points for a single Gaussian.\"\"\"\n        # x shape: (D_DIM, M)\n        # mu shape: (D_DIM,)\n        diff = x - mu[:, np.newaxis]  # shape (D_DIM, M)\n        mahalanobis = np.einsum('im,ij,jm-m', diff, inv_sigma, diff)\n        return LOG_PDF_CONST - 0.5 * log_det_sigma - 0.5 * mahalanobis\n\n    def log_pdf_gmm_batch(x):\n        \"\"\"Computes log-PDF for a batch of points for the GMM.\"\"\"\n        log_p1 = log_pdf_gaussian_batch(x, MU1, SIGMA1_INV, LOG_DET_SIGMA1)\n        log_p2 = log_pdf_gaussian_batch(x, MU2, SIGMA2_INV, LOG_DET_SIGMA2)\n        \n        max_log = np.maximum(log_p1, log_p2)\n        lse = max_log + np.log(np.exp(log_p1 - max_log) + np.exp(log_p2 - max_log))\n        \n        return -np.log(2) + lse\n\n    def true_score_batch(x):\n        \"\"\"Computes the true score field for a batch of points.\"\"\"\n        diff1 = x - MU1[:, np.newaxis]\n        diff2 = x - MU2[:, np.newaxis]\n        s1 = -SIGMA1_INV @ diff1\n        s2 = -SIGMA2_INV @ diff2\n        \n        log_p1 = log_pdf_gaussian_batch(x, MU1, SIGMA1_INV, LOG_DET_SIGMA1)\n        log_p2 = log_pdf_gaussian_batch(x, MU2, SIGMA2_INV, LOG_DET_SIGMA2)\n        \n        max_log = np.maximum(log_p1, log_p2)\n        exp1 = np.exp(log_p1 - max_log)\n        exp2 = np.exp(log_p2 - max_log)\n        \n        # Broadcasting for weighted average\n        score = (exp1[np.newaxis, :] * s1 + exp2[np.newaxis, :] * s2) / (exp1 + exp2)[np.newaxis, :]\n        return score\n\n    def rotational_field_batch(x):\n        \"\"\"Computes the rotational field component for a batch of points.\"\"\"\n        diff = x - C[:, np.newaxis]\n        norm_sq = np.sum(diff * diff, axis=0)\n        return (Q @ diff) / (1.0 + norm_sq)\n\n    def parametric_score_batch(x, epsilon):\n        \"\"\"Computes the perturbed parametric score field.\"\"\"\n        return true_score_batch(x) + epsilon * rotational_field_batch(x)\n\n    def compute_metrics(case):\n        \"\"\"Computes D and E for a single test case.\"\"\"\n        x0, x, epsilon, a = case\n        x0, x = np.array(x0), np.array(x)\n\n        d_vec = x - x0\n        d_norm = np.linalg.norm(d_vec)\n\n        if np.isclose(d_norm, 0):\n            return 0.0, 0.0\n\n        t = np.linspace(0, 1, N_STEPS + 1)\n\n        # Path 1: Straight\n        gamma_straight = x0[:, np.newaxis] + d_vec[:, np.newaxis] * t\n        gamma_prime_straight = np.tile(d_vec[:, np.newaxis], (1, len(t)))\n        \n        s_theta_straight = parametric_score_batch(gamma_straight, epsilon)\n        integrand_straight = np.einsum('ij,ij-j', s_theta_straight, gamma_prime_straight)\n        I_straight = np.trapz(integrand_straight, t)\n\n        # Path 2: Curved\n        n_hat = (Q @ d_vec) / d_norm\n        gamma_curved = (x0[:, np.newaxis] + d_vec[:, np.newaxis] * t +\n                        a * np.sin(np.pi * t) * n_hat[:, np.newaxis])\n        gamma_prime_curved = (d_vec[:, np.newaxis] +\n                              a * np.pi * np.cos(np.pi * t) * n_hat[:, np.newaxis])\n\n        s_theta_curved = parametric_score_batch(gamma_curved, epsilon)\n        integrand_curved = np.einsum('ij,ij-j', s_theta_curved, gamma_prime_curved)\n        I_curved = np.trapz(integrand_curved, t)\n\n        # True log-density difference\n        log_p_x = log_pdf_gmm_batch(x[:, np.newaxis])[0]\n        log_p_x0 = log_pdf_gmm_batch(x0[:, np.newaxis])[0]\n        true_diff = log_p_x - log_p_x0\n        \n        # Metrics D and E\n        D = np.abs(I_straight - I_curved)\n        E = np.abs(I_straight - true_diff)\n\n        return D, E\n\n    test_cases = [\n        ([0.0, 0.0], [2.0, 1.0], 0.0, 0.5),      # Case 1\n        ([0.0, 0.0], [2.0, 1.0], 0.05, 0.5),     # Case 2\n        ([0.0, 0.0], [2.0, 1.0], 0.2, 0.5),      # Case 3\n        ([1.0, -1.0], [1.0, -1.0], 0.25, 1.0),   # Case 4\n        ([0.0, -2.0], [3.0, 1.0], 0.1, 2.0),     # Case 5\n    ]\n\n    results = []\n    for case in test_cases:\n        D_val, E_val = compute_metrics(case)\n        results.append(f\"[{D_val:.6f},{E_val:.6f}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3172994"}]}