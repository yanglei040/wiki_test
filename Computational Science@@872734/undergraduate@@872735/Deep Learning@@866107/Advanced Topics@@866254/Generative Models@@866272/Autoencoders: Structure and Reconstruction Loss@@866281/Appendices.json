{"hands_on_practices": [{"introduction": "Autoencoders are not limited to static data; they can be powerfully applied to sequences like time series or text by incorporating recurrent neural networks (RNNs). This exercise explores a fundamental aspect of training recurrent decoders by contrasting two distinct operational regimes. You will implement and compare \"teacher forcing,\" where the model is guided by ground truth data at each step, with \"free running,\" where the model generates the sequence autonomously based on its own previous outputs, providing insight into the trade-offs between stable training and generative performance [@problem_id:3099756].", "problem": "Consider an autoencoder for sequences whose decoder is a Recurrent Neural Network (RNN). The decoder evolves a hidden state and produces a reconstruction at each time step. The encoder compresses the input sequence into an initial hidden state. You will implement and compare two reconstruction loss regimes for sequence decoding: teacher forcing and free running.\n\nFundamental base:\n- Autoencoder: An encoder $E$ maps an input sequence $x_{1:T}$ to a latent representation $h_0$, and a decoder $D$ maps the latent representation to a reconstructed sequence $\\hat{x}_{1:T}$.\n- Recurrent Neural Network (RNN): At each time step $t$, a hidden state $h_t$ is updated by a nonlinear function of the previous hidden state and an input signal, and the output $\\hat{x}_t$ is a function of the current hidden state.\n- Mean squared error (MSE): The reconstruction loss is the average squared difference between the reconstructed sequence and the ground truth sequence.\n\nSetup:\n- Use a scalar encoder that maps the input sequence $x_{1:T}$ to the initial hidden state $h_0$ by\n$$\nh_0 = \\tanh\\!\\big(w_e \\cdot \\bar{x} + b_e\\big),\n$$\nwhere $\\bar{x} = \\frac{1}{T}\\sum_{t=1}^T x_t$ is the sample mean, $w_e$ is a scalar encoder weight, $b_e$ is a scalar encoder bias, and $\\tanh(\\cdot)$ is the hyperbolic tangent function.\n- The decoder is a scalar RNN with update and output equations\n$$\nh_t = \\tanh\\!\\big(w_h \\cdot h_{t-1} + w_x \\cdot u_t + b_h\\big), \\quad \\hat{x}_t = w_y \\cdot h_t + b_y,\n$$\nwhere $w_h$ is the recurrent weight, $w_x$ is the input weight, $b_h$ is the hidden bias, $w_y$ is the output weight, $b_y$ is the output bias, and $u_t$ is the decoder input signal at time $t$ defined below.\n- Teacher forcing regime: $u_t = x_t$ for all $t$. The teacher forcing reconstruction loss is\n$$\nL_{\\text{TF}} = \\frac{1}{T}\\sum_{t=1}^T \\big(\\hat{x}_t - x_t\\big)^2.\n$$\n- Free running regime: $u_1 = u_{\\text{start}}$ (a given scalar start token) and $u_t = \\hat{x}_{t-1}$ for $t \\ge 2$. The free running reconstruction loss is\n$$\nL_{\\text{FR}} = \\frac{1}{T}\\sum_{t=1}^T \\big(\\hat{x}_t - x_t\\big)^2.\n$$\n\nTask:\n- For each test case, compute both $L_{\\text{TF}}$ and $L_{\\text{FR}}$ using the definitions above.\n\nAll quantities are dimensionless real numbers. Angles, if any, are not used. All outputs must be floats.\n\nTest suite:\n1. Case A (happy path): $x_{1:4} = [0.5,-0.2,0.1,-0.1]$, $w_e=0.6$, $b_e=0.0$, $w_h=0.7$, $w_x=0.8$, $b_h=0.0$, $w_y=1.0$, $b_y=0.0$, $u_{\\text{start}}=0.0$.\n2. Case B (boundary: zero sequence): $x_{1:4} = [0.0,0.0,0.0,0.0]$, $w_e=0.6$, $b_e=0.0$, $w_h=0.7$, $w_x=0.8$, $b_h=0.0$, $w_y=1.0$, $b_y=0.0$, $u_{\\text{start}}=0.0$.\n3. Case C (edge: stronger feedback): $x_{1:4} = [1.0,1.0,1.0,1.0]$, $w_e=0.5$, $b_e=0.1$, $w_h=1.2$, $w_x=1.1$, $b_h=0.0$, $w_y=1.0$, $b_y=0.0$, $u_{\\text{start}}=0.0$.\n4. Case D (boundary: single step): $x_{1:1} = [0.3]$, $w_e=0.4$, $b_e=0.0$, $w_h=0.9$, $w_x=1.0$, $b_h=0.0$, $w_y=0.9$, $b_y=0.0$, $u_{\\text{start}}=0.0$.\n\nRequired final output format:\n- Your program should produce a single line of output containing a comma-separated list of two-element lists for the four test cases, each inner list ordered as $[L_{\\text{TF}},L_{\\text{FR}}]$, with each float formatted to $8$ decimal places and with no spaces. For example,\n$[[0.00000000,0.00000000],[0.00000000,0.00000000],[0.00000000,0.00000000],[0.00000000,0.00000000]]$.", "solution": "The problem statement is valid. It is scientifically grounded in the principles of recurrent neural networks and autoencoders, providing a complete and consistent set of definitions and parameters to solve a well-posed computational task. The objective is to calculate the reconstruction loss of a sequence autoencoder under two distinct decoding regimes: teacher forcing and free running.\n\nThe methodology is as follows. First, the encoder processes the entire input sequence $x_{1:T}$ to produce a single initial hidden state $h_0$ for the decoder. The length of the sequence is denoted by $T$. The encoder computes the sample mean of the input sequence, $\\bar{x} = \\frac{1}{T}\\sum_{t=1}^T x_t$, and then generates the initial hidden state $h_0$ via the equation:\n$$\nh_0 = \\tanh\\!\\big(w_e \\cdot \\bar{x} + b_e\\big)\n$$\nwhere $w_e$ and $b_e$ are the encoder's weight and bias parameters, and $\\tanh$ is the hyperbolic tangent activation function.\n\nNext, the decoder, a Recurrent Neural Network (RNN), uses this initial hidden state $h_0$ to generate a reconstructed sequence $\\hat{x}_{1:T}$ one step at a time. For each time step $t$ from $1$ to $T$, the decoder updates its hidden state $h_t$ and computes an output $\\hat{x}_t$. The governing equations are:\n$$\nh_t = \\tanh\\!\\big(w_h \\cdot h_{t-1} + w_x \\cdot u_t + b_h\\big)\n$$\n$$\n\\hat{x}_t = w_y \\cdot h_t + b_y\n$$\nHere, $h_{t-1}$ is the hidden state from the previous time step, and $u_t$ is an external input to the decoder at the current time step. The parameters $w_h, w_x, b_h, w_y, b_y$ are the weights and biases of the decoder RNN.\n\nThe two regimes, teacher forcing and free running, differ solely in how the decoder input $u_t$ is defined.\n\nIn the **teacher forcing** regime, the decoder is guided by the ground truth sequence at every step. The input at time $t$ is the actual data point from the original sequence, $x_t$.\n$$\nu_t = x_t \\quad \\text{for } t \\in \\{1, 2, \\dots, T\\}\n$$\nThis approach prevents the accumulation of errors during decoding, as any deviation in the prediction at step $t-1$ does not affect the input for step $t$. The hidden state $h_t$ is thus a function of the previous state $h_{t-1}$ and the veridical input $x_t$.\n\nIn the **free running** (or generative) regime, the decoder operates autonomously after initialization. The input for the first time step, $u_1$, is a specified start token, $u_{\\text{start}}$. For all subsequent time steps, the input is the decoder's own output from the previous step.\n$$\nu_t = \\begin{cases} u_{\\text{start}} & \\text{if } t=1 \\\\ \\hat{x}_{t-1} & \\text{if } t \\ge 2 \\end{cases}\n$$\nIn this mode, errors can propagate and compound over time, as a poor prediction $\\hat{x}_{t-1}$ will serve as a flawed input for calculating $\\hat{x}_t$. This makes the free running reconstruction task generally more challenging for the model.\n\nFor both regimes, the reconstruction loss is quantified by the Mean Squared Error (MSE) between the generated sequence $\\hat{x}_{1:T}$ and the original sequence $x_{1:T}$. The loss $L$ is given by:\n$$\nL = \\frac{1}{T}\\sum_{t=1}^T \\big(\\hat{x}_t - x_t\\big)^2\n$$\nThe teacher forcing loss is denoted $L_{\\text{TF}}$, and the free running loss is $L_{\\text{FR}}$. We will now compute these two values for each provided test case by systematically applying the equations specified for the encoder and the two decoder regimes. For each case, we first compute $h_0$, then run the recurrent calculations for $t=1, \\dots, T$ to obtain the sequence $\\hat{x}_{1:T}$ and finally compute the corresponding MSE loss.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the autoencoder reconstruction loss problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A (happy path)\n        {'x_seq': [0.5, -0.2, 0.1, -0.1], 'w_e': 0.6, 'b_e': 0.0, 'w_h': 0.7, 'w_x': 0.8, 'b_h': 0.0, 'w_y': 1.0, 'b_y': 0.0, 'u_start': 0.0},\n        # Case B (boundary: zero sequence)\n        {'x_seq': [0.0, 0.0, 0.0, 0.0], 'w_e': 0.6, 'b_e': 0.0, 'w_h': 0.7, 'w_x': 0.8, 'b_h': 0.0, 'w_y': 1.0, 'b_y': 0.0, 'u_start': 0.0},\n        # Case C (edge: stronger feedback)\n        {'x_seq': [1.0, 1.0, 1.0, 1.0], 'w_e': 0.5, 'b_e': 0.1, 'w_h': 1.2, 'w_x': 1.1, 'b_h': 0.0, 'w_y': 1.0, 'b_y': 0.0, 'u_start': 0.0},\n        # Case D (boundary: single step)\n        {'x_seq': [0.3], 'w_e': 0.4, 'b_e': 0.0, 'w_h': 0.9, 'w_x': 1.0, 'b_h': 0.0, 'w_y': 0.9, 'b_y': 0.0, 'u_start': 0.0}\n    ]\n\n    def _calculate_loss(params, regime):\n        \"\"\"Helper function to calculate loss for a given regime.\"\"\"\n        x = np.array(params['x_seq'])\n        T = len(x)\n        w_e, b_e = params['w_e'], params['b_e']\n        w_h, w_x, b_h = params['w_h'], params['w_x'], params['b_h']\n        w_y, b_y = params['w_y'], params['b_y']\n        u_start = params['u_start']\n\n        # Encoder step\n        x_bar = np.mean(x)\n        h_prev = np.tanh(w_e * x_bar + b_e)  # This is h_0\n\n        total_sq_err = 0.0\n        x_hat_prev = 0.0 # Used for free running\n\n        for t_idx in range(T):\n            # Define decoder input u_t based on regime\n            if regime == 'teacher_forcing':\n                u_t = x[t_idx]\n            elif regime == 'free_running':\n                if t_idx == 0:\n                    u_t = u_start\n                else:\n                    u_t = x_hat_prev\n            else:\n                raise ValueError(\"Invalid regime specified.\")\n\n            # Decoder step\n            h_current = np.tanh(w_h * h_prev + w_x * u_t + b_h)\n            x_hat_current = w_y * h_current + b_y\n\n            # Accumulate error\n            total_sq_err += (x_hat_current - x[t_idx])**2\n\n            # Update states for next iteration\n            h_prev = h_current\n            if regime == 'free_running':\n                x_hat_prev = x_hat_current\n\n        return total_sq_err / T\n\n    all_results = []\n    for case in test_cases:\n        l_tf = _calculate_loss(case, 'teacher_forcing')\n        l_fr = _calculate_loss(case, 'free_running')\n        \n        case_result_str = f\"[{l_tf:.8f},{l_fr:.8f}]\"\n        all_results.append(case_result_str)\n    \n    # Format the final output string as specified\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3099756"}, {"introduction": "For data with a grid-like structure, such as signals or images, convolutional autoencoders are the architecture of choice due to their ability to learn hierarchical features. This practice delves into the core mechanics of a simple one-dimensional convolutional autoencoder to investigate a crucial design parameter: the receptive field size. By analyzing how different kernel sizes affect the reconstruction of signals with varying frequencies, you will gain a practical understanding of the trade-off between capturing fine details and smoothing noise, a central theme in signal processing and computer vision [@problem_id:3099855].", "problem": "You are asked to implement and analyze a simplified one-dimensional convolutional autoencoder to study how receptive field size affects fine-detail reconstruction quality under two different reconstruction losses: mean squared error and mean absolute error. Your program must be a complete, runnable script that computes the specified quantities and prints the final results in the required format.\n\nStart from the following fundamental base:\n\n- A one-dimensional autoencoder maps an input signal $x \\in \\mathbb{R}^N$ to a latent representation via an encoder and reconstructs it via a decoder. In this problem, both encoder and decoder are linear, shift-invariant operators implemented as discrete cross-correlations with finite impulse response kernels.\n- The encoder computes a cross-correlation $y = x \\star w$, where, for an odd-length kernel $w \\in \\mathbb{R}^K$ with $K \\ge 1$, the discrete cross-correlation with zero-padding is defined as\n$$\ny[n] = \\sum_{j=0}^{K-1} w[j]\\; x[n + j - c], \\quad c = \\left\\lfloor \\frac{K}{2} \\right\\rfloor,\n$$\nwith $x[m] = 0$ for indices $m$ outside $[0, N-1]$. This produces an output $y$ of the same length $N$ as $x$.\n- The decoder computes another cross-correlation with the reversed kernel, $\\tilde{w}[j] = w[K-1-j]$, yielding the reconstruction $\\hat{x} = y \\star \\tilde{w}$. This corresponds to a tied-weights linear convolutional autoencoder with stride $1$ and zero-padding, so that the overall mapping is a linear, time-invariant smoothing operator.\n- Reconstruction losses are defined per-sample and averaged along the signal:\n  - Mean squared error (MSE): $L_2(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left( \\hat{x}[n] - x[n] \\right)^2$.\n  - Mean absolute error ($L_1$): $L_1(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left| \\hat{x}[n] - x[n] \\right|$.\n- The receptive field is the kernel length $K$, and the kernel entries are taken to be a normalized box filter: $w[j] = \\frac{1}{K}$ for $j \\in \\{0,1,\\dots,K-1\\}$.\n\nYour task is to quantify how $K$ affects fine-detail reconstruction under both $L_2$ and $L_1$ losses using the specified architecture (no training is performed; the weights are fixed by definition). To isolate the effect of receptive field on fine-detail content, you will evaluate three synthetic inputs of length $N$:\n\n- $x^{(1)}$: a unit impulse at the center, $x^{(1)}[n] = 1$ if $n = \\frac{N}{2}$ and $x^{(1)}[n] = 0$ otherwise.\n- $x^{(2)}$: a high-frequency alternating cosine, $x^{(2)}[n] = \\cos(\\pi n)$.\n- $x^{(3)}$: a low-frequency cosine, $x^{(3)}[n] = \\cos\\!\\left( \\frac{2\\pi f_0 n}{N} \\right)$.\n\nUse $N = 128$ and $f_0 = 4$.\n\nFor each kernel size $K$ in the test suite below, compute the following:\n\n1. For each signal $x^{(i)}$ with $i \\in \\{1,2,3\\}$, form $\\hat{x}^{(i)}$ using the encoder-decoder defined above.\n2. Compute $L_2\\left(\\hat{x}^{(i)}, x^{(i)}\\right)$ and $L_1\\left(\\hat{x}^{(i)}, x^{(i)}\\right)$.\n3. Average the losses over the three signals to obtain\n$$\n\\overline{L}_2(K) = \\frac{1}{3} \\sum_{i=1}^3 L_2\\left(\\hat{x}^{(i)}, x^{(i)}\\right), \\quad\n\\overline{L}_1(K) = \\frac{1}{3} \\sum_{i=1}^3 L_1\\left(\\hat{x}^{(i)}, x^{(i)}\\right).\n$$\n4. Quantify fine-detail retention using the impulse signal: compute the ratio\n$$\nr(K) = \\frac{\\hat{x}^{(1)}[N/2]}{x^{(1)}[N/2]}.\n$$\nSince $x^{(1)}[N/2] = 1$, this simplifies to $r(K) = \\hat{x}^{(1)}[N/2]$.\n\nTest suite:\n\n- Use kernel sizes $K \\in \\{1, 3, 7, 15\\}$.\n\nAnswer specification:\n\n- For each $K$ in ascending order, produce a result list $[K, \\overline{L}_2(K), \\overline{L}_1(K), r(K)]$.\n- Your program should produce a single line of output containing the results as a comma-separated list of these lists, enclosed in square brackets. The values $\\overline{L}_2(K)$, $\\overline{L}_1(K)$, and $r(K)$ must be rounded to six decimal places. For example, a valid output format is\n$[[1,0.000000,0.000000,1.000000],[3, \\dots],[7, \\dots],[15, \\dots]]$\n(with no spaces in the printed line).", "solution": "The problem requires an analysis of a simplified one-dimensional linear convolutional autoencoder. The core of the task is to implement the specified signal processing chain and quantify how the receptive field size, determined by the kernel length $K$, affects the reconstruction of signals with different frequency characteristics. The autoencoder's weights are fixed as a normalized box filter, meaning no training is involved. The analysis is purely based on the signal processing properties of the defined architecture.\n\nThe input is a one-dimensional signal $x \\in \\mathbb{R}^N$. The encoder maps the input $x$ to a latent representation $y$ via a discrete cross-correlation with a kernel $w \\in \\mathbb{R}^K$: $y = x \\star w$. The decoder reconstructs the signal, $\\hat{x}$, by applying another cross-correlation to $y$ with the reversed kernel $\\tilde{w}$: $\\hat{x} = y \\star \\tilde{w}$. The kernel size $K$ must be an odd integer. The cross-correlation is defined with zero-padding to maintain the signal length $N$:\n$$y[n] = \\sum_{j=0}^{K-1} w[j]\\; x[n + j - c], \\quad c = \\left\\lfloor \\frac{K}{2} \\right\\rfloor$$\nThe kernel weights are given by a normalized box filter, $w[j] = 1/K$ for all $j \\in \\{0, \\dots, K-1\\}$. A key property of this kernel is its symmetry, meaning $w[j] = w[K-1-j]$. Consequently, the reversed kernel $\\tilde{w}$ is identical to the original kernel $w$. The overall transformation from input $x$ to reconstruction $\\hat{x}$ is therefore a cascade of two identical cross-correlation operations:\n$$\\hat{x} = (x \\star w) \\star w$$\nThis operation is equivalent to a single convolution of the input signal $x$ with an effective kernel that is the self-convolution of $w$. This transformation constitutes a linear, time-invariant (LTI) system.\n\nThe kernel $w$ acts as a moving average filter, which is a fundamental form of low-pass filter. Applying this filter twice in succession results in a more potent low-pass filtering effect. The effective impulse response of the complete autoencoder system is a triangular filter of length $2K-1$. The parameter $K$, representing the receptive field size, controls the width of this filter. A larger $K$ leads to a wider effective filter, which causes more aggressive smoothing of the signal. This increased smoothing attenuates high-frequency components and fine details more severely. The problem uses three specific signals to probe this behavior:\n1.  $x^{(1)}$: A unit impulse. The reconstruction $\\hat{x}^{(1)}$ is the impulse response of the system, directly revealing the shape of the effective filter.\n2.  $x^{(2)}$: A high-frequency cosine, $\\cos(\\pi n)$. This signal will be strongly attenuated by the low-pass filter, especially for large $K$.\n3.  $x^{(3)}$: A low-frequency cosine. This signal is expected to be better preserved than $x^{(2)}$, serving as a reference.\n\nThe computational procedure is as follows. The analysis is conducted for kernel sizes $K \\in \\{1, 3, 7, 15\\}$. For each value of $K$, the following steps are executed:\n1.  The three input signals, $x^{(1)}$, $x^{(2)}$, and $x^{(3)}$, are generated for a length of $N = 128$ and frequency parameter $f_0 = 4$.\n    -   $x^{(1)}[n] = \\delta[n - N/2]$, with $N/2 = 64$.\n    -   $x^{(2)}[n] = \\cos(\\pi n)$.\n    -   $x^{(3)}[n] = \\cos(2\\pi f_0 n / N)$.\n2.  The kernel $w$ of size $K$ is created with entries $w[j] = 1/K$.\n3.  For each signal $x^{(i)}$, the reconstruction $\\hat{x}^{(i)}$ is computed by applying the specified cross-correlation operation twice.\n4.  The reconstruction quality is measured using two loss functions, the Mean Squared Error ($L_2$) and the Mean Absolute Error ($L_1$), defined as:\n    $$L_2(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} (\\hat{x}[n] - x[n])^2$$\n    $$L_1(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} |\\hat{x}[n] - x[n]|$$\n5.  These individual losses are averaged over the three signals to produce the final metrics $\\overline{L}_2(K)$ and $\\overline{L}_1(K)$.\n6.  The retention of fine detail is quantified by the ratio $r(K) = \\hat{x}^{(1)}[N/2] / x^{(1)}[N/2]$. Since $x^{(1)}[N/2] = 1$, this simplifies to $r(K) = \\hat{x}^{(1)}[N/2]$. An analytical derivation based on the system's impulse response confirms that this value is exactly $1/K$, which serves as a valuable check for the implementation.\nThe final output aggregates these computed values for each $K$ from the test suite.", "answer": "```python\nimport numpy as np\n\ndef cross_correlate(x, w):\n    \"\"\"\n    Computes the 1D cross-correlation with zero-padding as defined in the problem.\n    y[n] = sum_{j=0}^{K-1} w[j] * x[n + j - c], where c = floor(K/2).\n    \"\"\"\n    N = len(x)\n    K = len(w)\n    # For odd K, floor(K/2) is equivalent to (K-1)//2.\n    c = (K - 1) // 2\n    y = np.zeros(N, dtype=np.float64)\n    \n    for n in range(N):\n        sum_val = 0.0\n        for j in range(K):\n            m = n + j - c\n            if 0 <= m < N:\n                sum_val += w[j] * x[m]\n        y[n] = sum_val\n    return y\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem as specified.\n    \"\"\"\n    # Define problem parameters\n    N = 128\n    f0 = 4\n    kernel_sizes = [1, 3, 7, 15]\n\n    # Generate input signals\n    n_indices = np.arange(N)\n    \n    # x^(1): Unit impulse at the center\n    x1 = np.zeros(N, dtype=np.float64)\n    center_idx = N // 2\n    x1[center_idx] = 1.0\n\n    # x^(2): High-frequency alternating cosine\n    x2 = np.cos(np.pi * n_indices)\n\n    # x^(3): Low-frequency cosine\n    x3 = np.cos(2 * np.pi * f0 * n_indices / N)\n    \n    signals = [x1, x2, x3]\n    \n    all_results = []\n\n    for K in kernel_sizes:\n        # Create the normalized box filter kernel\n        w = np.full(K, 1.0 / K, dtype=np.float64)\n        \n        # The reversed kernel w_tilde is the same as w because w is symmetric\n        w_tilde = w\n        \n        total_l2_loss = 0.0\n        total_l1_loss = 0.0\n        r_K = 0.0\n        \n        # Process each signal\n        for i, x in enumerate(signals):\n            # Form reconstruction x_hat by applying the operation twice\n            # Encoder pass\n            y = cross_correlate(x, w)\n            # Decoder pass\n            x_hat = cross_correlate(y, w_tilde)\n            \n            # Compute losses\n            l2_loss = np.mean((x_hat - x)**2)\n            l1_loss = np.mean(np.abs(x_hat - x))\n            \n            # Accumulate losses for averaging\n            total_l2_loss += l2_loss\n            total_l1_loss += l1_loss\n            \n            # For signal x1, compute the retention ratio r(K)\n            if i == 0:  # Signal x1 is at index 0\n                # r(K) = x_hat[N/2] / x[N/2]. Since x[N/2]=1, it's just x_hat[N/2]\n                r_K = x_hat[center_idx]\n\n        # Average the losses over the three signals\n        avg_l2 = total_l2_loss / 3.0\n        avg_l1 = total_l1_loss / 3.0\n        \n        # Store raw float values for this K\n        result_tuple = [K, avg_l2, avg_l1, r_K]\n        all_results.append(result_tuple)\n        \n    # Format the final output string as required\n    # The f-string formatting with ':.6f' handles rounding to 6 decimal places\n    formatted_results = [f\"[{k},{l2:.6f},{l1:.6f},{r:.6f}]\" for k, l2, l1, r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the solver\nsolve()\n```", "id": "3099855"}, {"introduction": "A key to building effective autoencoders is designing a reconstruction loss that accurately reflects the desired outcome. While standard metrics like Mean Squared Error ($L_2$ loss) are mathematically convenient, they often fail to capture perceptual similarity, especially for complex data like images. This advanced practice guides you through designing and implementing a custom, perceptually-aware loss function for an image autoencoder, which penalizes color shifts more than luminance changes by operating in an opponent-color space, a technique that leads to more visually plausible reconstructions [@problem_id:3099813].", "problem": "Consider an autoencoder in deep learning that maps an input image tensor to a reconstruction through an encoder and decoder composition. Let the input image be denoted by $X \\in \\mathbb{R}^{H \\times W \\times 3}$ and the reconstructed image be denoted by $\\hat{X} \\in \\mathbb{R}^{H \\times W \\times 3}$, where the last dimension corresponds to the red, green, and blue channels. The standard empirical risk minimization objective uses a reconstruction loss between $X$ and $\\hat{X}$ that aggregates pixelwise errors. The goal in this problem is to design a reconstruction loss that penalizes perceptual color shifts more than luminance errors, by measuring errors in an opponent-color space and then applying channel-wise weighting before averaging.\n\nUse a linear opponent-color transform defined per pixel by a matrix $M \\in \\mathbb{R}^{3 \\times 3}$ that maps $\\begin{bmatrix}R & G & B\\end{bmatrix}^\\top$ to $\\begin{bmatrix}L & RG & BY\\end{bmatrix}^\\top$ through\n$$\nM \\;=\\;\n\\begin{bmatrix}\n0.2126 & 0.7152 & 0.0722 \\\\\n1 & -1 & 0 \\\\\n0.5 & 0.5 & -1\n\\end{bmatrix},\n$$\nwhere $L$ denotes a luminance channel, $RG$ denotes a red-green opponent channel, and $BY$ denotes a blue-yellow opponent channel. You must construct the reconstruction loss by: (i) computing the per-pixel opponent-color difference induced by $M$ between $\\hat{X}$ and $X$, (ii) applying channel-wise nonnegative weights $\\alpha_L$, $\\alpha_{RG}$, and $\\alpha_{BY}$ to these differences, and (iii) aggregating the result by averaging over all pixels using the squared Euclidean norm. The entire construction must be expressed and implemented only from the foundational definitions of linear transforms, Euclidean norms, and mean aggregation; do not introduce any additional heuristic terms.\n\nYour program must implement this loss and evaluate it on the following test suite. All image values are intended to be dimensionless intensities in the closed interval $[0,1]$:\n\n- Test case $1$ (general case with mixed luminance and color errors): Let $H = 2$ and $W = 2$. Define\n$$\nX_1 =\n\\begin{bmatrix}\n\\begin{bmatrix} 0.50 & 0.50 & 0.50 \\end{bmatrix} &\n\\begin{bmatrix} 0.20 & 0.60 & 0.20 \\end{bmatrix} \\\\\n\\begin{bmatrix} 0.90 & 0.10 & 0.10 \\end{bmatrix} &\n\\begin{bmatrix} 0.10 & 0.90 & 0.90 \\end{bmatrix}\n\\end{bmatrix},\n\\quad\n\\hat{X}_1 =\n\\begin{bmatrix}\n\\begin{bmatrix} 0.51 & 0.50 & 0.49 \\end{bmatrix} &\n\\begin{bmatrix} 0.18 & 0.62 & 0.20 \\end{bmatrix} \\\\\n\\begin{bmatrix} 0.90 & 0.12 & 0.08 \\end{bmatrix} &\n\\begin{bmatrix} 0.13 & 0.89 & 0.88 \\end{bmatrix}\n\\end{bmatrix}.\n$$\nUse weights $\\alpha_L = 1.0$, $\\alpha_{RG} = 3.0$, and $\\alpha_{BY} = 3.0$.\n\n- Test case $2$ (boundary case of perfect reconstruction): Let $H = 2$ and $W = 2$. Define\n$$\nX_2 =\n\\begin{bmatrix}\n\\begin{bmatrix} 0.15 & 0.25 & 0.35 \\end{bmatrix} &\n\\begin{bmatrix} 0.45 & 0.55 & 0.65 \\end{bmatrix} \\\\\n\\begin{bmatrix} 0.40 & 0.30 & 0.20 \\end{bmatrix} &\n\\begin{bmatrix} 0.60 & 0.70 & 0.80 \\end{bmatrix}\n\\end{bmatrix},\n\\quad\n\\hat{X}_2 = X_2.\n$$\nUse weights $\\alpha_L = 1.0$, $\\alpha_{RG} = 3.0$, and $\\alpha_{BY} = 3.0$.\n\n- Test case $3$ (edge case of pure color shift with no luminance change per pixel): Let $H = 2$ and $W = 2$. Define\n$$\nX_3 =\n\\begin{bmatrix}\n\\begin{bmatrix} 0.50 & 0.50 & 0.50 \\end{bmatrix} &\n\\begin{bmatrix} 0.50 & 0.50 & 0.50 \\end{bmatrix} \\\\\n\\begin{bmatrix} 0.50 & 0.50 & 0.50 \\end{bmatrix} &\n\\begin{bmatrix} 0.50 & 0.50 & 0.50 \\end{bmatrix}\n\\end{bmatrix},\n$$\nand construct $\\hat{X}_3$ by setting, for each pixel, a channelwise increment $\\Delta R = 0.05$, $\\Delta G = -\\left(\\frac{0.2126}{0.7152}\\right)\\Delta R$, and $\\Delta B = 0.0$, then defining $\\hat{X}_3 = X_3 + \\begin{bmatrix} \\Delta R & \\Delta G & \\Delta B \\end{bmatrix}$ per pixel. Use weights $\\alpha_L = 1.0$, $\\alpha_{RG} = 3.0$, and $\\alpha_{BY} = 3.0$.\n\nYour program should produce a single line of output containing the three loss values for test cases $1$, $2$, and $3$, respectively, as a comma-separated list enclosed in square brackets. Each number must be a floating-point value rounded to six decimal places, for example, $\\left[0.123456,0.000000,0.987654\\right]$.", "solution": "The problem statement is valid as it is scientifically grounded in color theory and deep learning practices, well-posed with all necessary information provided, and objective in its formulation. We will proceed to derive the specified reconstruction loss and apply it to the given test cases.\n\nThe objective is to construct a reconstruction loss function $\\mathcal{L}$ that operates on an input image tensor $X \\in \\mathbb{R}^{H \\times W \\times 3}$ and its reconstruction $\\hat{X} \\in \\mathbb{R}^{H \\times W \\times 3}$. This loss function should be more sensitive to perceptual color shifts than to changes in luminance. This is achieved by transforming pixel-wise errors into an opponent-color space, applying channel-specific weights, and then aggregating the results. The derivation follows a principled, step-by-step construction based on foundational mathematical operations.\n\nLet $X_{ij} \\in \\mathbb{R}^3$ and $\\hat{X}_{ij} \\in \\mathbb{R}^3$ represent the Red-Green-Blue (RGB) color vectors for the pixel at spatial location $(i,j)$, where $i \\in \\{1, \\dots, H\\}$ and $j \\in \\{1, \\dots, W\\}$.\n\n**1. Per-Pixel Error in RGB Space**\nThe initial step is to compute the difference between the reconstructed and original pixel values in the standard RGB color space. This per-pixel difference vector is denoted by $\\Delta_{ij}$:\n$$\n\\Delta_{ij} = \\hat{X}_{ij} - X_{ij} \\in \\mathbb{R}^3\n$$\n\n**2. Transformation to Opponent-Color Space**\nThe core of the perceptual weighting strategy is to analyze this error in an opponent-color space. The problem provides a linear transformation matrix $M \\in \\mathbb{R}^{3 \\times 3}$ that maps an RGB vector to a vector containing luminance ($L$), red-green ($RG$), and blue-yellow ($BY$) components.\n$$\nM =\n\\begin{bmatrix}\n0.2126 & 0.7152 & 0.0722 \\\\\n1 & -1 & 0 \\\\\n0.5 & 0.5 & -1\n\\end{bmatrix}\n$$\nApplying this linear transformation to the RGB error vector $\\Delta_{ij}$ yields the error vector in the opponent-color space, $o_{ij} \\in \\mathbb{R}^3$:\n$$\no_{ij} = M \\Delta_{ij} = \\begin{bmatrix} o_{L,ij} \\\\ o_{RG,ij} \\\\ o_{BY,ij} \\end{bmatrix}\n$$\nHere, $o_{L,ij}$, $o_{RG,ij}$, and $o_{BY,ij}$ represent the error components along the luminance, red-green, and blue-yellow axes, respectively.\n\n**3. Weighted Per-Pixel Error**\nTo penalize color errors more heavily than luminance errors, non-negative weights $\\alpha_L$, $\\alpha_{RG}$, and $\\alpha_{BY}$ are applied to the squared components of the opponent-color error vector $o_{ij}$. The per-pixel loss, $L_{ij}$, is the weighted sum of these squared errors, which is equivalent to a weighted squared Euclidean norm of $o_{ij}$.\n$$\nL_{ij} = \\alpha_L (o_{L,ij})^2 + \\alpha_{RG} (o_{RG,ij})^2 + \\alpha_{BY} (o_{BY,ij})^2\n$$\n\n**4. Aggregation into Final Loss**\nThe total reconstruction loss for the entire image, $\\mathcal{L}(X, \\hat{X})$, is the mean of the per-pixel losses $L_{ij}$ over all $N = H \\times W$ pixels.\n$$\n\\mathcal{L}(X, \\hat{X}) = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} L_{ij} = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\left( \\alpha_L (o_{L,ij})^2 + \\alpha_{RG} (o_{RG,ij})^2 + \\alpha_{BY} (o_{BY,ij})^2 \\right)\n$$\nThis completes the formal definition of the reconstruction loss. We now evaluate this loss for the three specified test cases. In all cases, the weights are $\\alpha_L = 1.0$, $\\alpha_{RG} = 3.0$, and $\\alpha_{BY} = 3.0$.\n\n**Test Case 1: General Case**\nWe are given $X_1$ and $\\hat{X}_1$. The per-pixel RGB difference tensor is:\n$$\n\\Delta_1 = \\hat{X}_1 - X_1 =\n\\begin{bmatrix}\n\\begin{bmatrix} 0.01 & 0.00 & -0.01 \\end{bmatrix} &\n\\begin{bmatrix} -0.02 & 0.02 & 0.00 \\end{bmatrix} \\\\\n\\begin{bmatrix} 0.00 & 0.02 & -0.02 \\end{bmatrix} &\n\\begin{bmatrix} 0.03 & -0.01 & -0.01 \\end{bmatrix}\n\\end{bmatrix}\n$$\nWe apply the transformation $o_{ij} = M \\Delta_{ij}$ and compute the weighted squared error for each of the $4$ pixels. The sum of these errors is calculated and then divided by $4$. The computation yields a total loss $\\mathcal{L}_1 \\approx 0.003986$.\n\n**Test Case 2: Perfect Reconstruction**\nHere, $\\hat{X}_2 = X_2$. This implies that the difference tensor $\\Delta_2 = \\hat{X}_2 - X_2$ is a zero tensor. Consequently, the opponent-color error vectors $o_{ij}$ are all zero vectors, and the per-pixel losses $L_{ij}$ are all $0$. The total loss is therefore:\n$$\n\\mathcal{L}_2 = 0.0\n$$\n\n**Test Case 3: Pure Color Shift**\nThe image $X_3$ is a uniform gray. $\\hat{X}_3$ is constructed by adding a constant vector $[\\Delta R, \\Delta G, \\Delta B]^\\top$ to each pixel of $X_3$, where $\\Delta R = 0.05$, $\\Delta G = -\\left(\\frac{0.2126}{0.7152}\\right)\\Delta R$, and $\\Delta B = 0.0$. The per-pixel RGB difference is constant across the image:\n$$\n\\Delta_{ij} = \\left[ 0.05, -0.05 \\left(\\frac{0.2126}{0.7152}\\right), 0 \\right]^\\top \\approx \\left[ 0.05, -0.014863, 0 \\right]^\\top\n$$\nThe transformation into the opponent space gives $o_{ij} = M \\Delta_{ij}$. By construction, the luminance component of this error is zero:\n$$\no_{L,ij} = 0.2126(\\Delta R) + 0.7152(\\Delta G) = 0.2126(\\Delta R) + 0.7152\\left(-\\frac{0.2126}{0.7152}\\Delta R\\right) = 0\n$$\nThe color components are non-zero. Since the per-pixel error is the same for all pixels, the total loss $\\mathcal{L}_3$ is equal to the error for a single pixel:\n$$\n\\mathcal{L}_3 = L_{ij} = \\alpha_L (0)^2 + \\alpha_{RG} (o_{RG,ij})^2 + \\alpha_{BY} (o_{BY,ij})^2\n$$\nThe calculation gives $o_{RG,ij} \\approx 0.064863$ and $o_{BY,ij} \\approx 0.017568$. The resulting loss is $\\mathcal{L}_3 \\approx 0.013548$.\nThe final results are $[0.003986, 0.000000, 0.013548]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_loss(X, X_hat, M, alpha):\n    \"\"\"\n    Calculates the perceptually-weighted reconstruction loss.\n\n    Args:\n        X (np.ndarray): The original image tensor of shape (H, W, 3).\n        X_hat (np.ndarray): The reconstructed image tensor of shape (H, W, 3).\n        M (np.ndarray): The opponent-color transformation matrix of shape (3, 3).\n        alpha (np.ndarray): The channel-wise weights of shape (3,).\n\n    Returns:\n        float: The final aggregated loss value.\n    \"\"\"\n    # Step 1: Compute the difference tensor in RGB space.\n    # The shape of delta is (H, W, 3).\n    delta = X_hat - X\n\n    # Step 2: Transform the RGB differences to the opponent-color space.\n    # We perform a matrix multiplication for each pixel's 3-element vector.\n    # In numpy, (H, W, 3) @ (3, 3) correctly performs this operation,\n    # resulting in a tensor of shape (H, W, 3).\n    # M must be transposed to align dimensions for the matmul.\n    opp_diff = delta @ M.T\n\n    # Step 3: Apply channel-wise weights and compute the squared error.\n    # Square the opponent differences element-wise.\n    opp_diff_sq = opp_diff**2\n    # Apply weights. The alpha array of shape (3,) is broadcast\n    # across the (H, W) dimensions.\n    weighted_opp_diff_sq = alpha * opp_diff_sq\n    # Sum the weighted squared errors across the color channels (axis=2)\n    # to get the per-pixel loss.\n    pixel_errors = np.sum(weighted_opp_diff_sq, axis=2)\n\n    # Step 4: Aggregate by averaging over all pixels.\n    # np.mean computes the average of all elements in the pixel_errors tensor.\n    loss = np.mean(pixel_errors)\n\n    return loss\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and compute their losses.\n    \"\"\"\n    # Define the opponent-color transform matrix and weights\n    M = np.array([\n        [0.2126, 0.7152, 0.0722],\n        [1.0, -1.0, 0.0],\n        [0.5, 0.5, -1.0]\n    ])\n    alpha = np.array([1.0, 3.0, 3.0])\n\n    # Test Case 1\n    X1 = np.array([\n        [[0.50, 0.50, 0.50], [0.20, 0.60, 0.20]],\n        [[0.90, 0.10, 0.10], [0.10, 0.90, 0.90]]\n    ])\n    X1_hat = np.array([\n        [[0.51, 0.50, 0.49], [0.18, 0.62, 0.20]],\n        [[0.90, 0.12, 0.08], [0.13, 0.89, 0.88]]\n    ])\n    loss1 = calculate_loss(X1, X1_hat, M, alpha)\n\n    # Test Case 2\n    X2 = np.array([\n        [[0.15, 0.25, 0.35], [0.45, 0.55, 0.65]],\n        [[0.40, 0.30, 0.20], [0.60, 0.70, 0.80]]\n    ])\n    X2_hat = X2.copy()  # Perfect reconstruction\n    loss2 = calculate_loss(X2, X2_hat, M, alpha)\n\n    # Test Case 3\n    X3 = np.full((2, 2, 3), 0.50)\n    delta_R = 0.05\n    delta_G = -(0.2126 / 0.7152) * delta_R\n    delta_B = 0.0\n    pixel_delta = np.array([delta_R, delta_G, delta_B])\n    X3_hat = X3 + pixel_delta\n    loss3 = calculate_loss(X3, X3_hat, M, alpha)\n\n    results = [loss1, loss2, loss3]\n    \n    # Format the output string as required\n    results_str = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3099813"}]}