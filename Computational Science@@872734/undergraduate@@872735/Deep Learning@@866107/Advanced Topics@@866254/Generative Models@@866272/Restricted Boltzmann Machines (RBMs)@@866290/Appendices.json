{"hands_on_practices": [{"introduction": "Training a Restricted Boltzmann Machine effectively hinges on understanding the Contrastive Divergence (CD) algorithm. Since CD provides only an approximation of the true gradient, a natural question arises: how does the quality of this approximation affect learning? This exercise [@problem_id:3170448] allows you to empirically investigate this by comparing CD with a single Gibbs step (CD-1) to CD with more steps (CD-10). You will discover how a more accurate gradient estimate can help the RBM better capture complex, multi-modal data distributions and avoid the common pitfall of mode collapse.", "problem": "You are tasked with designing and implementing a complete, runnable program that empirically compares Contrastive Divergence with $k=1$ steps (CD-1) and with $k=10$ steps (CD-10) for training a Restricted Boltzmann Machine (RBM) on a synthetic dataset whose underlying modes are known. The comparison criterion is mode coverage measured via cluster assignment to the known modes.\n\nBegin from the following fundamental base:\n\n- A Restricted Boltzmann Machine (RBM) defines a joint distribution over binary visible variables $v \\in \\{0,1\\}^D$ and binary hidden variables $h \\in \\{0,1\\}^H$ by an energy function $E(v,h)$ and a partition function $Z$. The joint probability is $p(v,h) = \\frac{1}{Z} \\exp(-E(v,h))$.\n- The energy function for a binary-binary RBM is parameterized by a weight matrix, visible biases, and hidden biases. Visible units are conditionally independent given hidden units, and hidden units are conditionally independent given visible units.\n- Contrastive Divergence with $k$ Gibbs steps (CD-$k$) approximates the gradient of the log-likelihood by starting a short Gibbs chain at the data and running $k$ alternating conditional sampling steps.\n\nProblem requirements:\n\n1. Construct a synthetic dataset with a known set of binary base modes. Let $D=8$ and $M=4$ base modes $m_1, m_2, m_3, m_4 \\in \\{0,1\\}^8$ be defined explicitly as:\n   - $m_1 = [1,1,0,0,1,0,1,0]$\n   - $m_2 = [0,1,1,0,0,1,0,1]$\n   - $m_3 = [1,0,1,1,0,0,1,0]$\n   - $m_4 = [0,0,0,1,1,1,0,0]$\n   For each mode $m_i$, generate $n_i$ samples by independently flipping each bit with probability $p_{\\text{flip}} \\in [0,1]$ and aggregating all modes' samples. This yields a dataset with known mode counts $(n_1,n_2,n_3,n_4)$.\n\n2. Train two RBMs with binary visible and binary hidden units on the same dataset and identical initial parameters, differing only in the Contrastive Divergence steps $k$:\n   - RBM with CD-1,\n   - RBM with CD-10.\n   Use a fixed number of hidden units $H=6$, a learning rate $\\alpha$, mini-batch training, and a fixed number of epochs. Ensure identical random initialization for both RBMs to isolate the effect of $k$.\n\n3. After training, draw samples from each trained RBM by running a Gibbs chain starting from a random binary visible vector. For each generated sample, assign it to the nearest base mode by minimum Hamming distance. Using these assignments, define the mode coverage metric as follows:\n   - Let $\\mathcal{M} = \\{m_1, m_2, m_3, m_4\\}$ be the set of base modes.\n   - For a set of generated samples $\\{v^{(s)}\\}_{s=1}^S$, define the assigned mode index for each sample by $i^\\star(s) = \\arg\\min_{i \\in \\{1,\\dots,M\\}} d_H(v^{(s)}, m_i)$, where $d_H$ is the Hamming distance.\n   - Define coverage as the fraction of modes with at least one assigned sample: $\\text{coverage} = \\frac{|\\{i \\in \\{1,\\dots,M\\} : \\exists s \\text{ with } i^\\star(s) = i\\}|}{M}$.\n   Express coverage as a decimal in $[0,1]$.\n\n4. Implement the full training and evaluation pipeline starting from the RBM energy-based definition and the conditional independence structure. Derive and implement the CD-$k$ training updates based on the difference between data-driven and model-driven correlations and biases.\n\n5. Use the following test suite, which specifies $(n_1,n_2,n_3,n_4)$ and $p_{\\text{flip}}$:\n   - Case A (balanced, low noise): $(250,250,250,250)$, $p_{\\text{flip}} = 0.05$.\n   - Case B (balanced, higher noise): $(250,250,250,250)$, $p_{\\text{flip}} = 0.25$.\n   - Case C (imbalanced, low noise): $(400,100,75,25)$, $p_{\\text{flip}} = 0.05$.\n\n6. For each case, train both RBMs with the same configuration: $H=6$, learning rate $\\alpha = 0.1$, mini-batch size $50$, epochs $100$, and sample $S=200$ points from each trained model with $100$ Gibbs steps per sample for assignment.\n\n7. Final output specification: Your program should produce a single line of output containing the results for the three cases as a comma-separated list of pairs, where each pair lists $[\\text{coverage}_{\\text{CD-1}}, \\text{coverage}_{\\text{CD-10}}]$. Round each coverage value to three decimal places and format the output exactly as:\n   - $[[c_{A,1},c_{A,10}],[c_{B,1},c_{B,10}],[c_{C,1},c_{C,10}]]$\n   where $c_{\\cdot,\\cdot} \\in [0,1]$ are decimal numbers.\n\nNo physical units are involved in this problem, and angles are not present. All answers are decimals in $[0,1]$ rounded to three decimal places. Ensure scientific realism by using the described RBM architecture, the specified dataset generation process, and the defined evaluation metric.", "solution": "The present task requires a rigorous empirical comparison of Contrastive Divergence training for Restricted Boltzmann Machines (RBMs) with $k=1$ and $k=10$ Gibbs sampling steps. The solution is structured around fundamental principles of RBMs, from the energy-based model definition to the gradient approximation via CD-$k$.\n\n**1. Restricted Boltzmann Machine: Theoretical Framework**\n\nAn RBM is an energy-based model for a joint probability distribution over binary visible units $v \\in \\{0, 1\\}^D$ and binary hidden units $h \\in \\{0, 1\\}^H$. The model is parameterized by $\\theta = \\{W, b, c\\}$, where $W \\in \\mathbb{R}^{H \\times D}$ is the weight matrix connecting hidden and visible units, $b \\in \\mathbb{R}^D$ is the visible unit bias vector, and $c \\in \\mathbb{R}^H$ is the hidden unit bias vector.\n\nThe energy function $E(v, h; \\theta)$ defines the system's state:\n$$\nE(v, h) = - \\sum_{i=1}^D b_i v_i - \\sum_{j=1}^H c_j h_j - \\sum_{i=1}^D \\sum_{j=1}^H h_j W_{ji} v_i\n$$\nIn matrix notation, this is $E(v, h) = -b^T v - c^T h - h^T W v$.\n\nThe joint probability distribution is given by the Boltzmann distribution:\n$$\np(v, h) = \\frac{1}{Z} e^{-E(v, h)}\n$$\nwhere $Z = \\sum_{v', h'} e^{-E(v', h')}$ is the partition function, an intractable normalization constant. The marginal probability of a visible vector is $p(v) = \\frac{1}{Z} \\sum_h e^{-E(v, h)}$.\n\nA key property of RBMs is the conditional independence of units within a layer given the state of the other layer. This allows for efficient block Gibbs sampling. The conditional probabilities are:\n$$\np(h_j=1 | v) = \\sigma\\left(c_j + \\sum_{i=1}^D W_{ji} v_i\\right) = \\sigma(c_j + (Wv)_j)\n$$\n$$\np(v_i=1 | h) = \\sigma\\left(b_i + \\sum_{j=1}^H h_j W_{ji}\\right) = \\sigma(b_i + (W^T h)_i)\n$$\nwhere $\\sigma(x) = (1 + e^{-x})^{-1}$ is the logistic sigmoid function.\n\n**2. Training via Contrastive Divergence (CD-$k$)**\n\nTraining an RBM involves adjusting $\\theta$ to maximize the log-likelihood of the training data $\\mathcal{D} = \\{v^{(n)}\\}$. The gradient of the log-likelihood for a single data point $v$ is:\n$$\n\\frac{\\partial \\log p(v)}{\\partial \\theta} = \\mathbb{E}_{h \\sim p(h|v)}\\left[-\\frac{\\partial E(v,h)}{\\partial \\theta}\\right] - \\mathbb{E}_{v',h' \\sim p(v',h')}\\left[-\\frac{\\partial E(v',h')}{\\partial \\theta}\\right]\n$$\nThe first term, the \"positive phase,\" is the expectation over hidden states given the data. The second term, the \"negative phase,\" is the expectation over the full model distribution. The intractability of $Z$ makes the negative phase computationally infeasible.\n\nContrastive Divergence with $k$ steps (CD-$k$) approximates this gradient. It replaces the model expectation with an expectation over samples obtained from a short Gibbs chain of $k$ steps, initialized with a data vector. For a given data vector $v^{(0)}$:\n1.  **Positive Phase Statistics**: Calculate the expectation of correlations based on the data. For the weights, this is $v^{(0)} p(h|v^{(0)})^T$.\n2.  **Negative Phase Generation**: Run a $k$-step Gibbs chain, starting with $h_s^{(0)} \\sim p(h|v^{(0)})$:\n    $$\n    v^{(1)} \\sim p(v|h_s^{(0)}), h_s^{(1)} \\sim p(h|v^{(1)}), \\dots, v^{(k)} \\sim p(v|h_s^{(k-1)})\n    $$\n3.  **Negative Phase Statistics**: Calculate correlations based on the final \"reconstruction\" $v^{(k)}$, giving $v^{(k)} p(h|v^{(k)})^T$.\n\nThe parameter updates for a mini-batch are derived from the average difference between positive and negative phase statistics, scaled by a learning rate $\\alpha$:\n$$\n\\Delta W \\propto \\alpha \\left( \\langle v^{(0)} p(h|v^{(0)})^T \\rangle_{\\text{batch}} - \\langle v^{(k)} p(h|v^{(k)})^T \\rangle_{\\text{batch}} \\right)\n$$\n$$\n\\Delta b \\propto \\alpha \\left( \\langle v^{(0)} \\rangle_{\\text{batch}} - \\langle v^{(k)} \\rangle_{\\text{batch}} \\right)\n$$\n$$\n\\Delta c \\propto \\alpha \\left( \\langle p(h|v^{(0)}) \\rangle_{\\text{batch}} - \\langle p(h|v^{(k)}) \\rangle_{\\text{batch}} \\right)\n$$\nThis experiment compares $k=1$ (a coarse but fast approximation) with $k=10$ (a more accurate but slower approximation). It is expected that a larger $k$ allows the Gibbs chain to move further from the initial data point and closer to the model's stationary distribution, yielding a better gradient estimate. This often leads to the model learning a better representation of the data distribution, including capturing multiple modes more effectively.\n\n**3. Experimental Design and Implementation**\n\nThe implementation follows the problem specification precisely.\n-   **Dataset Generation**: A function generates synthetic data by taking $M=4$ base modes and creating $n_i$ noisy versions of each mode $m_i$ by flipping bits with probability $p_{\\text{flip}}$.\n-   **RBM Class**: An RBM class encapsulates the parameters ($W, b, c$) and methods for sampling ($p(h|v)$, $p(v|h)$) and training (the CD-$k$ update). To ensure a fair comparison, both the CD-1 and CD-10 RBMs are initialized with identical parameters by using the same random seed.\n-   **Training Pipeline**: A training function iterates for a fixed number of epochs, processes the data in mini-batches, and applies the CD-$k$ update rule for the specified $k$. The shuffling of data is also seeded to be identical for both training runs within a given test case.\n-   **Evaluation**: After training, samples are generated from each RBM by running a long Gibbs chain ($100$ steps) starting from random initial vectors. Each generated sample is assigned to the nearest base mode using Hamming distance. The mode coverage is a practical metric for this problem, measuring the model's ability to reproduce the various underlying patterns in the data. An RBM that learns all modes will have its generated samples fall near all four base modes, resulting in a high coverage score.\n\nThe experiment is conducted for three distinct cases: a balanced dataset with low noise, a balanced dataset with high noise, and an imbalanced dataset. This tests the algorithms' robustness to noise and data imbalance. The expectation is that CD-10 will exhibit superior mode coverage, especially in the more challenging imbalanced case, as CD-1's biased gradient might cause the model to focus only on the most frequent modes.", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\n# Main class for the Restricted Boltzmann Machine\nclass RBM:\n    \"\"\"\n    A class for a binary-binary Restricted Boltzmann Machine (RBM).\n    \"\"\"\n\n    def __init__(self, n_visible, n_hidden, seed=None):\n        \"\"\"\n        Initializes the RBM with random weights and zero biases.\n        \n        Args:\n            n_visible (int): Number of visible units (D).\n            n_hidden (int): Number of hidden units (H).\n            seed (int, optional): Seed for the random number generator for reproducibility.\n        \"\"\"\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        self.rng = np.random.default_rng(seed)\n\n        # Initialize parameters\n        # Weights are initialized from a normal distribution with small variance\n        self.W = self.rng.normal(0, 0.01, (self.n_hidden, self.n_visible))\n        # Biases are initialized to zero\n        self.b = np.zeros(self.n_visible)\n        self.c = np.zeros(self.n_hidden)\n\n    def _propup(self, v):\n        \"\"\"Calculates hidden layer activation probabilities given visible layer state.\"\"\"\n        return sigmoid(self.c + v @ self.W.T)\n\n    def _propdown(self, h):\n        \"\"\"Calculates visible layer activation probabilities given hidden layer state.\"\"\"\n        return sigmoid(self.b + h @ self.W)\n\n    def sample_h_given_v(self, v):\n        \"\"\"Samples hidden layer states given visible layer states.\"\"\"\n        h_probs = self._propup(v)\n        return self.rng.binomial(1, h_probs)\n\n    def sample_v_given_h(self, h):\n        \"\"\"Samples visible layer states given hidden layer states.\"\"\"\n        v_probs = self._propdown(h)\n        return self.rng.binomial(1, v_probs)\n\n    def update_params(self, v_batch, k, learning_rate):\n        \"\"\"\n        Performs a single parameter update using k-step Contrastive Divergence (CD-k).\n        \n        Args:\n            v_batch (np.ndarray): A mini-batch of data samples.\n            k (int): The number of Gibbs sampling steps.\n            learning_rate (float): The learning rate for the update.\n        \"\"\"\n        batch_size = v_batch.shape[0]\n        \n        # --- Positive Phase ---\n        v0 = v_batch\n        ph0_probs = self._propup(v0)\n        \n        # --- Negative Phase ---\n        vk = v0\n        for _ in range(k):\n            hk_samples = self.sample_h_given_v(vk)\n            vk = self.sample_v_given_h(hk_samples)\n        \n        phk_probs = self._propup(vk)\n\n        # --- Calculate Gradients ---\n        # Note: We use probabilities for the hidden units for a lower variance gradient\n        grad_W = (ph0_probs.T @ v0 - phk_probs.T @ vk) / batch_size\n        grad_b = np.mean(v0 - vk, axis=0)\n        grad_c = np.mean(ph0_probs - phk_probs, axis=0)\n\n        # --- Update Parameters ---\n        self.W += learning_rate * grad_W\n        self.b += learning_rate * grad_b\n        self.c += learning_rate * grad_c\n\n    def sample(self, n_samples, n_gibbs_steps, seed=None):\n        \"\"\"\n        Generates samples from the RBM by running a Gibbs chain.\n        \"\"\"\n        eval_rng = np.random.default_rng(seed)\n        v = eval_rng.integers(0, 2, size=(n_samples, self.n_visible))\n        for _ in range(n_gibbs_steps):\n            h = self.sample_h_given_v(v)\n            v = self.sample_v_given_h(h)\n        return v\n\ndef generate_dataset(modes, mode_counts, p_flip, seed=None):\n    \"\"\"\n    Generates a synthetic dataset from base modes by flipping bits.\n    \"\"\"\n    data_rng = np.random.default_rng(seed)\n    dataset = []\n    for mode, count in zip(modes, mode_counts):\n        for _ in range(count):\n            noise = data_rng.choice([0, 1], size=mode.shape, p=[1 - p_flip, p_flip])\n            sample = np.bitwise_xor(mode, noise)\n            dataset.append(sample)\n    \n    dataset = np.array(dataset)\n    data_rng.shuffle(dataset)\n    return dataset\n\ndef train_rbm(rbm, data, epochs, batch_size, k, learning_rate, seed=None):\n    \"\"\"\n    Trains an RBM on the provided data.\n    \"\"\"\n    train_rng = np.random.default_rng(seed)\n    n_samples = data.shape[0]\n    \n    for epoch in range(epochs):\n        indices = np.arange(n_samples)\n        train_rng.shuffle(indices)\n        \n        for i in range(0, n_samples, batch_size):\n            batch_indices = indices[i:i + batch_size]\n            v_batch = data[batch_indices]\n            rbm.update_params(v_batch, k, learning_rate)\n\ndef calculate_coverage(samples, modes):\n    \"\"\"\n    Calculates mode coverage based on Hamming distance.\n    \"\"\"\n    # Calculate Hamming distance from each sample to each mode\n    # Broadcasting: (n_samples, 1, D) vs (1, n_modes, D)\n    distances = np.sum(samples[:, np.newaxis, :] != modes[np.newaxis, :, :], axis=2)\n    \n    # Assign each sample to the nearest mode\n    assignments = np.argmin(distances, axis=1)\n    \n    # Find the unique modes that were covered\n    covered_mode_indices = np.unique(assignments)\n    \n    # Calculate coverage fraction\n    coverage = len(covered_mode_indices) / len(modes)\n    return coverage\n\ndef solve():\n    \"\"\"\n    Main function to run the RBM comparison experiment.\n    \"\"\"\n    # --- Problem Constants and Parameters ---\n    D = 8  # Number of visible units\n    H = 6  # Number of hidden units\n    M = 4  # Number of modes\n    \n    base_modes = np.array([\n        [1, 1, 0, 0, 1, 0, 1, 0],\n        [0, 1, 1, 0, 0, 1, 0, 1],\n        [1, 0, 1, 1, 0, 0, 1, 0],\n        [0, 0, 0, 1, 1, 1, 0, 0]\n    ])\n\n    # Test cases: (mode_counts, p_flip)\n    test_cases = [\n        ((250, 250, 250, 250), 0.05),  # Case A\n        ((250, 250, 250, 250), 0.25),  # Case B\n        ((400, 100, 75, 25), 0.05),    # Case C\n    ]\n\n    # Training and evaluation parameters\n    learning_rate = 0.1\n    epochs = 100\n    batch_size = 50\n    n_eval_samples = 200\n    n_gibbs_steps_eval = 100\n\n    results = []\n    \n    # Master RNG for reproducibility of the entire experiment\n    master_rng = np.random.default_rng(42)\n\n    for mode_counts, p_flip in test_cases:\n        # Generate seeds for this case to ensure fair comparison\n        data_seed = master_rng.integers(2**32 - 1)\n        init_seed = master_rng.integers(2**32 - 1)\n        train_seed = master_rng.integers(2**32 - 1)\n        eval_seed = master_rng.integers(2**32 - 1)\n\n        # 1. Generate dataset\n        dataset = generate_dataset(base_modes, mode_counts, p_flip, seed=data_seed)\n\n        # 2. Train and evaluate for k=1 and k=10\n        case_results = []\n        for k_val in [1, 10]:\n            # Initialize RBM with identical starting weights\n            rbm = RBM(n_visible=D, n_hidden=H, seed=init_seed)\n            \n            # Train RBM with identical batch ordering\n            train_rbm(rbm, dataset, epochs=epochs, batch_size=batch_size, \n                      k=k_val, learning_rate=learning_rate, seed=train_seed)\n            \n            # Generate samples for evaluation\n            samples = rbm.sample(n_samples=n_eval_samples, \n                                 n_gibbs_steps=n_gibbs_steps_eval, \n                                 seed=eval_seed)\n            \n            # Calculate mode coverage\n            coverage = calculate_coverage(samples, base_modes)\n            case_results.append(round(coverage, 3))\n        \n        results.append(case_results)\n    \n    # Format and print the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3170448"}, {"introduction": "While evaluating a final model is essential, monitoring the training process itself provides deeper insights and helps debug learning dynamics. This advanced exercise [@problem_id:3170392] introduces a powerful diagnostic tool based on the concept of free energy. You will implement a consistency check to verify that each training update correctly lowers the free energy of the data samples more than it does for model-generated \"negative\" samples, confirming that the model is learning to prefer real data over its own fantasies.", "problem": "You are tasked with implementing a consistency check for training a Restricted Boltzmann Machine (RBM) with binary visible and hidden units using Contrastive Divergence (CD). The check should rigorously verify whether, over training epochs, CD updates decrease the average free energy of the data more than that of model-generated samples, and quantify deviations when this expectation fails.\n\nFundamental base and core definitions to use:\n- A Restricted Boltzmann Machine is an undirected probabilistic graphical model with visible units and hidden units. For binary visible vector $v \\in \\{0,1\\}^{n_v}$ and binary hidden vector $h \\in \\{0,1\\}^{n_h}$, the energy function is\n$$\nE(v,h) = - v^\\top W h - b^\\top v - c^\\top h,\n$$\nwhere $W \\in \\mathbb{R}^{n_v \\times n_h}$ is the weight matrix, $b \\in \\mathbb{R}^{n_v}$ is the visible bias vector, and $c \\in \\mathbb{R}^{n_h}$ is the hidden bias vector.\n- The free energy of a visible configuration $v$ is\n$$\nF(v) = - b^\\top v - \\sum_{j=1}^{n_h} \\log\\big(1 + \\exp(c_j + W_{\\cdot j}^\\top v)\\big).\n$$\n- The conditional distributions implied by the RBM are given by\n$$\np(h_j = 1 \\mid v) = \\sigma\\!\\left(c_j + W_{\\cdot j}^\\top v\\right), \\quad p(v_i = 1 \\mid h) = \\sigma\\!\\left(b_i + W_{i \\cdot}^\\top h\\right),\n$$\nwhere $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the logistic sigmoid function.\n- Contrastive Divergence with one step (CD-$1$) approximates the gradient of the log-likelihood by the difference between the data-dependent expectations and the model-dependent expectations formed by one step of alternating conditional sampling or mean-field propagation.\n\nYou must implement the following consistency check protocol:\n1. Construct a synthetic binary dataset of size $200$ with visible dimension $n_v = 6$ comprising two clusters centered at $[1,1,1,0,0,0]$ and $[0,0,0,1,1,1]$, each with bitwise flip noise probability $0.1$ per bit. The dataset generation must be deterministic and identical across test cases.\n2. Initialize RBM parameters $(W,b,c)$ with small random values for $W$ and zeros for $b$ and $c$. Use deterministic initialization governed by a seed per test case.\n3. Train the RBM for $T$ epochs using CD-$1$ with mean-field expectations (no stochastic sampling in the update rule), mini-batch updates, and the specified learning rate. At the end of each epoch $t \\in \\{1,\\dots,T\\}$, compute:\n   - The average free energy over the data,\n   $$\n   \\bar{F}_{\\text{data}}(t) = \\frac{1}{N}\\sum_{i=1}^{N} F(v^{(i)}),\n   $$\n   where $N$ is the dataset size.\n   - The average free energy over one-step model-generated negatives from the data,\n   $$\n   \\bar{F}_{\\text{neg}}(t) = \\frac{1}{N}\\sum_{i=1}^{N} F(\\tilde{v}^{(i)}),\n   $$\n   where for each data point $v^{(i)}$ you form $h^{(i)} = \\sigma(c + W^\\top v^{(i)})$, then $\\tilde{v}^{(i)} = \\mathbb{I}\\{\\sigma(b + W h^{(i)}) \\ge 0.5\\}$, where $\\mathbb{I}\\{\\cdot\\}$ is the indicator mapping to $\\{0,1\\}$ elementwise.\n4. Define epoch-to-epoch changes,\n   $$\n   \\Delta \\bar{F}_{\\text{data}}(t) = \\bar{F}_{\\text{data}}(t) - \\bar{F}_{\\text{data}}(t-1),\n   \\quad\n   \\Delta \\bar{F}_{\\text{neg}}(t) = \\bar{F}_{\\text{neg}}(t) - \\bar{F}_{\\text{neg}}(t-1),\n   $$\n   for $t = 1, \\dots, T$ with an initial baseline at $t=0$ computed from the untrained parameters.\n5. For each test case, evaluate the consistency criterion:\n   - Let $\\tau$ be the threshold defined as $\\tau = 0.6$.\n   - Compute the proportion of epochs where both $\\Delta \\bar{F}_{\\text{data}}(t)  0$ and $\\Delta \\bar{F}_{\\text{data}}(t)  \\Delta \\bar{F}_{\\text{neg}}(t)$ hold.\n   - Compute the average margin,\n   $$\n   M = \\frac{1}{T} \\sum_{t=1}^{T} \\left(\\Delta \\bar{F}_{\\text{data}}(t) - \\Delta \\bar{F}_{\\text{neg}}(t)\\right).\n   $$\n   - Return a boolean verdict that is $\\text{True}$ if and only if the proportion is at least $\\tau$ and $M  0$, and $\\text{False}$ otherwise.\n\nYour program must implement the above and evaluate the following test suite of parameter settings, each specified as $(\\text{seed}, \\text{learning rate}, \\text{epochs}, \\text{hidden units}, \\text{batch size})$:\n- Case $1$: $(0, 0.1, 15, 4, 20)$ — expected \"happy path\" where the consistency should generally hold.\n- Case $2$: $(1, 0.8, 15, 4, 20)$ — high learning rate to test potential instability and deviations.\n- Case $3$: $(2, 0.0, 10, 4, 20)$ — zero learning rate as a boundary case (no parameter updates).\n- Case $4$: $(3, 0.05, 3, 4, 20)$ — few epochs as another boundary case.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each result is the boolean verdict defined above for the corresponding test case.\n- No physical units, angles, or percentages are involved; all quantities are dimensionless real numbers or booleans.", "solution": "The objective is to compute the independence deviation metric $\\mathrm{ID}(C)$ and the orthogonality penalty $\\mathrm{OP}(W)$ for three given test cases. This requires a multi-step derivation and calculation.\n\nFirst, we must derive the expression for the conditional expectation of the hidden units given the visible units, $\\mathbb{E}[h \\mid v]$. The conditional probability distribution $p(h \\mid v)$ is given by the ratio of the joint probability $p(v,h)$ to the marginal probability $p(v)$:\n$$\np(h \\mid v) = \\frac{p(v,h)}{p(v)} = \\frac{\\frac{1}{Z} \\exp(-E(v,h))}{\\sum_{h' \\in \\{0,1\\}^n} \\frac{1}{Z} \\exp(-E(v,h'))} = \\frac{\\exp(-E(v,h))}{\\sum_{h' \\in \\{0,1\\}^n} \\exp(-E(v,h'))}.\n$$\nSubstituting the energy function $E(v,h) = -a^{\\top} v - b^{\\top} h - v^{\\top} W h$:\n$$\np(h \\mid v) = \\frac{\\exp(a^{\\top} v + b^{\\top} h + v^{\\top} W h)}{\\sum_{h' \\in \\{0,1\\}^n} \\exp(a^{\\top} v + b^{\\top} h' + v^{\\top} W h')}.\n$$\nThe term $\\exp(a^{\\top} v)$ is constant with respect to the summation variable $h'$ in the denominator, so it can be factored out and canceled with the term in the numerator. This demonstrates that the conditional distribution $p(h \\mid v)$ is independent of the visible bias vector $a$, as noted in the problem statement. The expression simplifies to:\n$$\np(h \\mid v) = \\frac{\\exp(b^{\\top} h + v^{\\top} W h)}{\\sum_{h' \\in \\{0,1\\}^n} \\exp(b^{\\top} h' + v^{\\top} W h')}.\n$$\nThe exponent can be rewritten as a sum over the hidden units: $b^{\\top} h + v^{\\top} W h = \\sum_{j=1}^{n} (b_j h_j + (v^{\\top} W)_{j} h_j) = \\sum_{j=1}^{n} (b_j + v^{\\top} W_{:,j}) h_j$. Due to the lack of connections between hidden units in an RBM, the joint conditional probability factorizes into a product of individual conditional probabilities:\n$$\np(h \\mid v) = \\prod_{j=1}^{n} p(h_j \\mid v).\n$$\nThis conditional independence is a key property of RBMs. For a single binary hidden unit $h_j \\in \\{0,1\\}$, its conditional probability is:\n$$\np(h_j=1 \\mid v) = \\frac{\\exp(b_j + v^{\\top} W_{:,j})}{1 + \\exp(b_j + v^{\\top} W_{:,j})} = \\frac{1}{1 + \\exp(-(b_j + v^{\\top} W_{:,j}))} = \\sigma(b_j + v^{\\top} W_{:,j}),\n$$\nwhere $\\sigma(x) = 1/(1+e^{-x})$ is the logistic sigmoid function.\nThe expectation of a binary variable is the probability of it being $1$. Thus, the expectation of the $j$-th hidden unit is:\n$$\n\\mathbb{E}[h_j \\mid v] = 1 \\cdot p(h_j=1 \\mid v) + 0 \\cdot p(h_j=0 \\mid v) = p(h_j=1 \\mid v).\n$$\nIn vector form, the conditional expectation of the hidden vector $h$ given a visible vector $v$ is:\n$$\n\\mathbb{E}[h \\mid v] = \\sigma(b + W^{\\top}v),\n$$\nwhere the sigmoid function $\\sigma$ is applied element-wise to the vector argument $b + W^{\\top}v$.\n\nWith this result, we can outline the computational procedure:\n\n1.  **Compute Expected Activations**: For each visible vector $v^{(i)}$ in the dataset $\\{v^{(i)}\\}_{i=1}^{N}$, calculate the vector of expected hidden activations $h^{(i)*} = \\mathbb{E}[h \\mid v^{(i)}]$. These $N$ vectors, each of size $n \\times 1$, are then used to form the rows of a matrix $H \\in \\mathbb{R}^{N \\times n}$, where the $i$-th row is $(h^{(i)*})^{\\top}$.\n\n2.  **Compute Sample Covariance Matrix $C$**: First, compute the sample mean of the expected hidden activations, which is an $n$-dimensional column vector $\\mu = \\frac{1}{N} \\sum_{i=1}^{N} h^{(i)*}$. Then, compute the $n \\times n$ sample covariance matrix $C$ using the provided formula with a denominator of $N-1$ for an unbiased estimate:\n$$\nC = \\frac{1}{N-1} \\sum_{i=1}^{N} (h^{(i)*} - \\mu)(h^{(i)*} - \\mu)^{\\top}.\n$$\n\n3.  **Compute Independence Deviation $\\mathrm{ID}(C)$**: Using the computed matrix $C$, we calculate the independence deviation metric. This involves partitioning the sum of squared elements of $C$ into diagonal and off-diagonal components:\n$$\n\\mathrm{ID}(C) = \\frac{\\sum_{i \\neq j} C_{ij}^{2}}{\\sum_{k=1}^{n} C_{kk}^{2} + \\epsilon}.\n$$\nThe numerator is a measure of the total off-diagonal covariance, while the denominator is a measure of the total variance. A small value signifies that the expected hidden features are approximately uncorrelated across the dataset. The constant $\\epsilon = 10^{-12}$ ensures numerical stability if the variances are all zero.\n\n4.  **Compute Orthogonality Penalty $\\mathrm{OP}(W)$**: This metric quantifies the non-orthogonality of the weight vectors associated with the hidden units. First, we compute the Gram matrix $G = W^{\\top} W$. Then, similar to $\\mathrm{ID}(C)$, we calculate $\\mathrm{OP}(W)$:\n$$\n\\mathrm{OP}(W) = \\frac{\\sum_{i \\neq j} G_{ij}^{2}}{\\sum_{k=1}^{n} G_{kk}^{2} + \\epsilon}.\n$$\nThe term $G_{ij} = (W_{:,i})^{\\top}W_{:,j}$ is the dot product of the weight vectors for hidden units $i$ and $j$. A small value of $\\mathrm{OP}(W)$ indicates that these weight vectors are nearly orthogonal.\n\nThese steps are implemented for each of the three test cases provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the independence deviation and orthogonality penalty for given RBM parameters.\n    \"\"\"\n\n    def sigmoid(x):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-x))\n\n    def compute_metrics(W, b, V, epsilon):\n        \"\"\"\n        Computes ID(C) and OP(W) for a single RBM configuration.\n\n        Args:\n            W (np.ndarray): Weight matrix of size (m, n).\n            b (np.ndarray): Hidden bias vector of size (n, 1).\n            V (np.ndarray): Dataset of visible vectors of size (N, m).\n            epsilon (float): Small constant for numerical stability.\n\n        Returns:\n            tuple: A tuple containing (id_c, op_w).\n        \"\"\"\n        N, m = V.shape\n        n = W.shape[1]\n\n        # 1. Compute H, the matrix of expected hidden activations\n        H = np.zeros((N, n))\n        for i in range(N):\n            v_i = V[i, :].reshape(-1, 1)  # Shape (m, 1)\n            # Argument to sigmoid: b (n,1) + W.T (n,m) @ v_i (m,1) - (n,1)\n            h_exp = sigmoid(b + W.T @ v_i)\n            H[i, :] = h_exp.T  # Store as a row vector (1, n)\n\n        # 2. Compute the sample covariance matrix C\n        if N = 1:\n            C = np.zeros((n, n))\n        else:\n            # np.cov with rowvar=False computes covariance of columns.\n            # ddof=1 uses N-1 in the denominator for an unbiased estimate.\n            C = np.cov(H, rowvar=False, ddof=1)\n            # Manually:\n            # mu = np.mean(H, axis=0)\n            # H_centered = H - mu.reshape(1, -1)\n            # C = (H_centered.T @ H_centered) / (N - 1)\n\n        # 3. Compute ID(C)\n        diag_C_sq_sum = np.sum(np.diag(C)**2)\n        total_C_sq_sum = np.sum(C**2)\n        off_diag_C_sq_sum = total_C_sq_sum - diag_C_sq_sum\n        id_c = off_diag_C_sq_sum / (diag_C_sq_sum + epsilon)\n\n        # 4. Compute G and OP(W)\n        G = W.T @ W\n        diag_G_sq_sum = np.sum(np.diag(G)**2)\n        total_G_sq_sum = np.sum(G**2)\n        off_diag_G_sq_sum = total_G_sq_sum - diag_G_sq_sum\n        op_w = off_diag_G_sq_sum / (diag_G_sq_sum + epsilon)\n\n        return id_c, op_w\n\n    epsilon = 1e-12\n    m, n = 4, 3\n    V_data = np.array([\n        [0., 0., 0., 0.], [1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.],\n        [1., 1., 0., 0.], [1., 0., 1., 0.], [0., 1., 1., 0.], [1., 1., 1., 0.]\n    ])\n    b_common = np.zeros((n, 1))\n\n    test_cases = [\n        # Test case 1 (general \"happy path\"): Orthogonal weights\n        (\n            np.array([[5., 0., 0.], [0., 5., 0.], [0., 0., 5.], [0., 0., 0.]]),\n            b_common,\n            V_data\n        ),\n        # Test case 2 (correlated hidden features): Correlated weights\n        (\n            np.array([[5., 5., 0.], [5., 5., 0.], [0., 0., 5.], [0., 0., 0.]]),\n            b_common,\n            V_data\n        ),\n        # Test case 3 (boundary case): Zero weights\n        (\n            np.zeros((m, n)),\n            b_common,\n            V_data\n        ),\n    ]\n\n    results = []\n    for W, b, V in test_cases:\n        id_c, op_w = compute_metrics(W, b, V, epsilon)\n        results.append(id_c)\n        results.append(op_w)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3170455"}, {"introduction": "While evaluating a final model is essential, monitoring the training process itself provides deeper insights and helps debug learning dynamics. This advanced exercise [@problem_id:3170392] introduces a powerful diagnostic tool based on the concept of free energy. You will implement a consistency check to verify that each training update correctly lowers the free energy of the data samples more than it does for model-generated \"negative\" samples, confirming that the model is learning to prefer real data over its own fantasies.", "problem": "You are tasked with implementing a consistency check for training a Restricted Boltzmann Machine (RBM) with binary visible and hidden units using Contrastive Divergence (CD). The check should rigorously verify whether, over training epochs, CD updates decrease the average free energy of the data more than that of model-generated samples, and quantify deviations when this expectation fails.\n\nFundamental base and core definitions to use:\n- A Restricted Boltzmann Machine is an undirected probabilistic graphical model with visible units and hidden units. For binary visible vector $v \\in \\{0,1\\}^{n_v}$ and binary hidden vector $h \\in \\{0,1\\}^{n_h}$, the energy function is\n$$\nE(v,h) = - v^\\top W h - b^\\top v - c^\\top h,\n$$\nwhere $W \\in \\mathbb{R}^{n_v \\times n_h}$ is the weight matrix, $b \\in \\mathbb{R}^{n_v}$ is the visible bias vector, and $c \\in \\mathbb{R}^{n_h}$ is the hidden bias vector.\n- The free energy of a visible configuration $v$ is\n$$\nF(v) = - b^\\top v - \\sum_{j=1}^{n_h} \\log\\big(1 + \\exp(c_j + W_{\\cdot j}^\\top v)\\big).\n$$\n- The conditional distributions implied by the RBM are given by\n$$\np(h_j = 1 \\mid v) = \\sigma\\!\\left(c_j + W_{\\cdot j}^\\top v\\right), \\quad p(v_i = 1 \\mid h) = \\sigma\\!\\left(b_i + W_{i \\cdot}^\\top h\\right),\n$$\nwhere $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the logistic sigmoid function.\n- Contrastive Divergence with one step (CD-$1$) approximates the gradient of the log-likelihood by the difference between the data-dependent expectations and the model-dependent expectations formed by one step of alternating conditional sampling or mean-field propagation.\n\nYou must implement the following consistency check protocol:\n1. Construct a synthetic binary dataset of size $200$ with visible dimension $n_v = 6$ comprising two clusters centered at $[1,1,1,0,0,0]$ and $[0,0,0,1,1,1]$, each with bitwise flip noise probability $0.1$ per bit. The dataset generation must be deterministic and identical across test cases.\n2. Initialize RBM parameters $(W,b,c)$ with small random values for $W$ and zeros for $b$ and $c$. Use deterministic initialization governed by a seed per test case.\n3. Train the RBM for $T$ epochs using CD-$1$ with mean-field expectations (no stochastic sampling in the update rule), mini-batch updates, and the specified learning rate. At the end of each epoch $t \\in \\{1,\\dots,T\\}$, compute:\n   - The average free energy over the data,\n   $$\n   \\bar{F}_{\\text{data}}(t) = \\frac{1}{N}\\sum_{i=1}^{N} F(v^{(i)}),\n   $$\n   where $N$ is the dataset size.\n   - The average free energy over one-step model-generated negatives from the data,\n   $$\n   \\bar{F}_{\\text{neg}}(t) = \\frac{1}{N}\\sum_{i=1}^{N} F(\\tilde{v}^{(i)}),\n   $$\n   where for each data point $v^{(i)}$ you form $h^{(i)} = \\sigma(c + W^\\top v^{(i)})$, then $\\tilde{v}^{(i)} = \\mathbb{I}\\{\\sigma(b + W h^{(i)}) \\ge 0.5\\}$, where $\\mathbb{I}\\{\\cdot\\}$ is the indicator mapping to $\\{0,1\\}$ elementwise.\n4. Define epoch-to-epoch changes,\n   $$\n   \\Delta \\bar{F}_{\\text{data}}(t) = \\bar{F}_{\\text{data}}(t) - \\bar{F}_{\\text{data}}(t-1),\n   \\quad\n   \\Delta \\bar{F}_{\\text{neg}}(t) = \\bar{F}_{\\text{neg}}(t) - \\bar{F}_{\\text{neg}}(t-1),\n   $$\n   for $t = 1, \\dots, T$ with an initial baseline at $t=0$ computed from the untrained parameters.\n5. For each test case, evaluate the consistency criterion:\n   - Let $\\tau$ be the threshold defined as $\\tau = 0.6$.\n   - Compute the proportion of epochs where both $\\Delta \\bar{F}_{\\text{data}}(t)  0$ and $\\Delta \\bar{F}_{\\text{data}}(t)  \\Delta \\bar{F}_{\\text{neg}}(t)$ hold.\n   - Compute the average margin,\n   $$\n   M = \\frac{1}{T} \\sum_{t=1}^{T} \\left(\\Delta \\bar{F}_{\\text{data}}(t) - \\Delta \\bar{F}_{\\text{neg}}(t)\\right).\n   $$\n   - Return a boolean verdict that is $\\text{True}$ if and only if the proportion is at least $\\tau$ and $M  0$, and $\\text{False}$ otherwise.\n\nYour program must implement the above and evaluate the following test suite of parameter settings, each specified as $(\\text{seed}, \\text{learning rate}, \\text{epochs}, \\text{hidden units}, \\text{batch size})$:\n- Case $1$: $(0, 0.1, 15, 4, 20)$ — expected \"happy path\" where the consistency should generally hold.\n- Case $2$: $(1, 0.8, 15, 4, 20)$ — high learning rate to test potential instability and deviations.\n- Case $3$: $(2, 0.0, 10, 4, 20)$ — zero learning rate as a boundary case (no parameter updates).\n- Case $4$: $(3, 0.05, 3, 4, 20)$ — few epochs as another boundary case.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each result is the boolean verdict defined above for the corresponding test case.\n- No physical units, angles, or percentages are involved; all quantities are dimensionless real numbers or booleans.", "solution": "The problem requires implementing a consistency check for the training of a Restricted Boltzmann Machine (RBM), a diagnostic procedure grounded in the model's energy-based formulation.\n\nA Restricted Boltzmann Machine (RBM) is a generative stochastic neural network that can learn a probability distribution over its set of inputs. It consists of a layer of binary visible units, $v \\in \\{0,1\\}^{n_v}$, and a layer of binary hidden units, $h \\in \\{0,1\\}^{n_h}$. The connections are restricted to be between visible and hidden layers only. The joint configuration $(v,h)$ is governed by an energy function:\n$$\nE(v,h; W, b, c) = - v^\\top W h - b^\\top v - c^\\top h\n$$\nwhere $W \\in \\mathbb{R}^{n_v \\times n_h}$ is the weight matrix connecting the layers, and $b \\in \\mathbb{R}^{n_v}$ and $c \\in \\mathbb{R}^{n_h}$ are the bias vectors for the visible and hidden units, respectively. The probability of a configuration is given by the Boltzmann distribution, $P(v,h) = \\frac{1}{Z} e^{-E(v,h)}$, where $Z$ is the partition function.\n\nA key quantity for an RBM is the free energy of a visible configuration $v$, which is defined by marginalizing the joint probability over all possible hidden configurations. It is given by:\n$$\nF(v) = -\\log \\sum_h e^{-E(v,h)} = -b^\\top v - \\sum_{j=1}^{n_h} \\log\\left(1 + \\exp\\left(c_j + W_{\\cdot j}^\\top v\\right)\\right)\n$$\nwhere $W_{\\cdot j}$ is the $j$-th column of the weight matrix $W$. Training an RBM on a dataset aims to adjust the parameters $(W, b, c)$ to minimize the average free energy of the data points, thereby assigning them high probability.\n\nTraining is performed by maximizing the log-likelihood of the data, which involves computing gradients that are intractable due to the partition function $Z$. Contrastive Divergence (CD), particularly CD-$k$, provides an efficient approximation. For this problem, we use CD-$1$ with mean-field updates. This means we do not perform stochastic sampling for the gradient calculation but instead use the probabilities (expected values) of the units being active.\n\nThe CD-$1$ mean-field update procedure for a mini-batch of data $V_0$ is as follows:\n1.  **Positive Phase**: Compute the hidden unit activation probabilities given the data:\n$$\nH_0 = \\sigma(V_0 W + c^\\top)\n$$\nwhere $\\sigma(x) = (1+e^{-x})^{-1}$ is the element-wise logistic sigmoid function.\n2.  **Negative Phase**: Reconstruct the visible units from the hidden activations, and then re-compute the hidden activations:\n$$\nV_1 = \\sigma(H_0 W^\\top + b^\\top)\n$$\n$$\nH_1 = \\sigma(V_1 W + c^\\top)\n$$\n3.  **Parameter Updates**: The parameters are updated using the statistics from the positive and negative phases. Given a learning rate $\\eta$ and batch size $N_{batch}$:\n$$\n\\Delta W = \\eta \\frac{1}{N_{batch}} \\left( V_0^\\top H_0 - V_1^\\top H_1 \\right)\n$$\n$$\n\\Delta b = \\eta \\frac{1}{N_{batch}} \\sum_{i=1}^{N_{batch}} \\left( V_0^{(i)} - V_1^{(i)} \\right)\n$$\n$$\n\\Delta c = \\eta \\frac{1}{N_{batch}} \\sum_{i=1}^{N_{batch}} \\left( H_0^{(i)} - H_1^{(i)} \\right)\n$$\nThese updates are applied iteratively over epochs.\n\nThe core of this problem is to implement a consistency check based on the dynamics of the free energy. A well-behaving training process should lower the free energy of the real data more significantly than that of \"negative\" samples generated by the model. The protocol is as follows:\nAt the end of each training epoch $t$ (from $t=0$ for the initial state to $t=T$ after the final epoch):\n- Calculate the average free energy of the training data, $\\bar{F}_{\\text{data}}(t)$.\n- Generate one-step reconstructions $\\tilde{v}$ for each data point $v$. This involves computing hidden probabilities $h_{prob} = \\sigma(W^\\top v + c)$, then visible probabilities $\\tilde{v}_{prob} = \\sigma(W h_{prob} + b)$, and finally deterministically setting $\\tilde{v} = \\mathbb{I}\\{\\tilde{v}_{prob} \\ge 0.5\\}$.\n- Calculate the average free energy of these reconstructed samples, $\\bar{F}_{\\text{neg}}(t)$.\n- The epoch-to-epoch changes are $\\Delta \\bar{F}_{\\text{data}}(t) = \\bar{F}_{\\text{data}}(t) - \\bar{F}_{\\text{data}}(t-1)$ and $\\Delta \\bar{F}_{\\text{neg}}(t) = \\bar{F}_{\\text{neg}}(t) - \\bar{F}_{\\text{neg}}(t-1)$ for $t=1, \\dots, T$.\n\nThe final consistency verdict is a boolean value based on two conditions over the $T$ epochs:\n1.  The proportion of epochs where training improves the data's fit while ensuring the model prefers data to reconstructions ($\\Delta \\bar{F}_{\\text{data}}(t)  0$ and $\\Delta \\bar{F}_{\\text{data}}(t)  \\Delta \\bar{F}_{\\text{neg}}(t)$) must be at least a threshold $\\tau=0.6$.\n2.  The average margin $M = \\frac{1}{T} \\sum_{t=1}^{T} (\\Delta \\bar{F}_{\\text{data}}(t) - \\Delta \\bar{F}_{\\text{neg}}(t))$ must be negative, indicating that, on average, the data's free energy decreased more than the negatives' free energy.\n\nThe algorithmic implementation will consist of a class representing the RBM, a function to generate the synthetic dataset deterministically, and a main loop to execute the protocol for each provided test case. Numerical stability for the free energy calculation, specifically the $\\log(1+\\exp(x))$ term, is handled by using the `numpy.logaddexp(0, x)` function. The initialization of weights and biases, as well as the training process, will strictly follow the deterministic requirements of the problem.", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\ndef generate_dataset():\n    \"\"\"\n    Generates a deterministic synthetic binary dataset with two clusters.\n    \"\"\"\n    dataset_size = 200\n    n_v = 6\n    noise_prob = 0.1\n    \n    # Use a fixed seed for deterministic dataset generation across all test cases.\n    rng = np.random.RandomState(1234)\n    \n    center1 = np.array([1, 1, 1, 0, 0, 0])\n    center2 = np.array([0, 0, 0, 1, 1, 1])\n    \n    # Create 100 samples for each cluster\n    cluster1_samples = np.tile(center1, (dataset_size // 2, 1))\n    cluster2_samples = np.tile(center2, (dataset_size // 2, 1))\n    \n    dataset = np.vstack([cluster1_samples, cluster2_samples])\n    \n    # Add bitwise flip noise\n    noise_mask = rng.rand(dataset_size, n_v)  noise_prob\n    dataset[noise_mask] = 1 - dataset[noise_mask]\n    \n    return dataset.astype(np.float64)\n\nclass RBM:\n    \"\"\"\n    A Restricted Boltzmann Machine with binary units.\n    \"\"\"\n    def __init__(self, n_v, n_h, seed):\n        \"\"\"\n        Initializes the RBM parameters.\n        \"\"\"\n        self.n_v = n_v\n        self.n_h = n_h\n        \n        # Use a case-specific seed for deterministic parameter initialization.\n        rng = np.random.RandomState(seed)\n        \n        self.W = rng.normal(0, 0.01, (n_v, n_h)).astype(np.float64)\n        self.b = np.zeros(n_v, dtype=np.float64)\n        self.c = np.zeros(n_h, dtype=np.float64)\n\n    def free_energy(self, v):\n        \"\"\"\n        Computes the free energy for a batch of visible vectors.\n        v shape: (batch_size, n_v)\n        \"\"\"\n        v_bias_term = v @ self.b\n        hidden_activation = v @ self.W + self.c\n        # Use np.logaddexp for numerical stability, equivalent to log(1 + exp(x))\n        log_term = np.sum(np.logaddexp(0, hidden_activation), axis=1)\n        return -v_bias_term - log_term\n\n    def train_step(self, v_batch, learning_rate):\n        \"\"\"\n        Performs one CD-1 training step on a mini-batch.\n        \"\"\"\n        if learning_rate == 0.0:\n            return\n            \n        n_batch = v_batch.shape[0]\n\n        # Positive phase\n        v0 = v_batch\n        h0_prob = sigmoid(v0 @ self.W + self.c)\n\n        # Negative phase (mean-field)\n        v1_prob = sigmoid(h0_prob @ self.W.T + self.b)\n        h1_prob = sigmoid(v1_prob @ self.W + self.c)\n\n        # Gradient updates\n        self.W += learning_rate * (v0.T @ h0_prob - v1_prob.T @ h1_prob) / n_batch\n        self.b += learning_rate * np.mean(v0 - v1_prob, axis=0)\n        self.c += learning_rate * np.mean(h0_prob - h1_prob, axis=0)\n\n    def get_negative_samples(self, v_data):\n        \"\"\"\n        Generates one-step negative samples for free energy calculation.\n        \"\"\"\n        h_prob = sigmoid(v_data @ self.W + self.c)\n        v_tilde_prob = sigmoid(h_prob @ self.W.T + self.b)\n        v_tilde = (v_tilde_prob >= 0.5).astype(np.float64)\n        return v_tilde\n\ndef solve():\n    \"\"\"\n    Main function to run the RBM consistency check for all test cases.\n    \"\"\"\n    test_cases = [\n        # (seed, learning_rate, epochs, hidden_units, batch_size)\n        (0, 0.1, 15, 4, 20),\n        (1, 0.8, 15, 4, 20),\n        (2, 0.0, 10, 4, 20),\n        (3, 0.05, 3, 4, 20),\n    ]\n\n    dataset = generate_dataset()\n    n_v = dataset.shape[1]\n    \n    results = []\n\n    for seed, lr, epochs, n_h, batch_size in test_cases:\n        rbm = RBM(n_v=n_v, n_h=n_h, seed=seed)\n        \n        f_data_history = []\n        f_neg_history = []\n        \n        # Calculate initial state at t=0\n        f_data_history.append(np.mean(rbm.free_energy(dataset)))\n        neg_samples = rbm.get_negative_samples(dataset)\n        f_neg_history.append(np.mean(rbm.free_energy(neg_samples)))\n        \n        # Training loop\n        for epoch in range(epochs):\n            # Shuffle dataset for mini-batch training\n            indices = np.arange(dataset.shape[0])\n            np.random.RandomState(seed + epoch).shuffle(indices)\n            shuffled_dataset = dataset[indices]\n            \n            for i in range(0, dataset.shape[0], batch_size):\n                v_batch = shuffled_dataset[i:i + batch_size]\n                rbm.train_step(v_batch, lr)\n            \n            # Record free energies at the end of the epoch\n            f_data_history.append(np.mean(rbm.free_energy(dataset)))\n            neg_samples = rbm.get_negative_samples(dataset)\n            f_neg_history.append(np.mean(rbm.free_energy(neg_samples)))\n            \n        # Analyze results\n        f_data_history = np.array(f_data_history)\n        f_neg_history = np.array(f_neg_history)\n        \n        delta_f_data = np.diff(f_data_history)\n        delta_f_neg = np.diff(f_neg_history)\n        \n        good_epochs = 0\n        if epochs > 0:\n            for t in range(epochs):\n                if delta_f_data[t]  0 and delta_f_data[t]  delta_f_neg[t]:\n                    good_epochs += 1\n            \n            proportion = good_epochs / epochs\n            margin = np.mean(delta_f_data - delta_f_neg)\n            \n            threshold = 0.6\n            verdict = (proportion >= threshold) and (margin  0)\n        else: # Handle case with 0 epochs\n            verdict = False\n\n        results.append(verdict)\n\n    # Format the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3170392"}]}