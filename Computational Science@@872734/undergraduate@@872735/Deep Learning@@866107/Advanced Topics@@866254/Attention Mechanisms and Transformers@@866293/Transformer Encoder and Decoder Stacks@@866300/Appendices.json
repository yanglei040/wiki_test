{"hands_on_practices": [{"introduction": "At its core, the attention mechanism that powers Transformers can be understood as a sophisticated, differentiable key-value retrieval system. This powerful analogy helps demystify how the model learns to select and weigh information from an input sequence. This exercise [@problem_id:3195550] allows you to explore this fundamental concept by building a toy cipher, where attention is used to decrypt messages by matching queries (ciphertext) to a memory bank of keys to retrieve the corresponding values (plaintext).", "problem": "You will design and analyze a toy cipher task that demonstrates how a transformer encoder can perform decryption via attention-based key matching and how this mechanism functions as content-addressable memory. The core of the task is a retrieval process in which a set of key vectors and corresponding value vectors store plaintext symbols, and a set of query vectors represent ciphertext that must be decrypted by matching queries to keys using a similarity-based weighting, followed by a normalized weighting scheme and a weighted aggregation of values. All mathematical entities must be treated rigorously as vectors and matrices over the real numbers. Your program must implement the retrieval mechanism and quantify decryption success using a well-defined accuracy metric for a set of test cases.\n\nFundamental basis to use:\n- Vector spaces over the real numbers, with vectors in $\\mathbb{R}^{d}$ and matrices in $\\mathbb{R}^{n \\times d}$.\n- The Euclidean dot product, for $x, y \\in \\mathbb{R}^{d}$, defined by $x \\cdot y = \\sum_{i=1}^{d} x_i y_i$.\n- The principle that larger dot products indicate greater similarity under a fixed norm constraint.\n- A normalization procedure that transforms a set of real-valued scores into a probability distribution over keys by exponentiation and normalization by the sum of exponentials.\n- A scaling principle that counteracts dimensionality growth in the dot product when $d$ increases, implemented by dividing the dot product scores by a dimension-dependent factor before exponentiation.\n\nCipher-memory setup:\n- Keys $K \\in \\mathbb{R}^{N \\times d_k}$ store addresses, values $V \\in \\mathbb{R}^{N \\times d_v}$ store plaintext symbols in one-hot form, and queries $Q \\in \\mathbb{R}^{N \\times d_k}$ represent ciphertext that must be matched to keys to recover plaintext. Use $N = 5$, $d_k = 4$, and $d_v = 5$.\n\n- Define the keys $K$ by rows $k_0, k_1, k_2, k_3, k_4 \\in \\mathbb{R}^{4}$ as follows, with each written as a unit-norm vector:\n  $k_0 = (1, 0, 0, 0)$,\n  $k_1 = (0, 1, 0, 0)$,\n  $k_2 = (0, 0, 1, 0)$,\n  $k_3 = (0, 0, 0, 1)$,\n  $k_4 = \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0, 0\\right)$.\n  Collect these into $K$ as $K = \\begin{bmatrix} k_0^\\top \\\\ k_1^\\top \\\\ k_2^\\top \\\\ k_3^\\top \\\\ k_4^\\top \\end{bmatrix}$.\n\n- Define the values $V \\in \\mathbb{R}^{5 \\times 5}$ as the $5 \\times 5$ identity matrix, so that the plaintext label of $k_i$ is the index $i \\in \\{0, 1, 2, 3, 4\\}$ and the associated value vector is the one-hot vector $e_i \\in \\mathbb{R}^{5}$ with a $1$ at position $i$ and $0$ elsewhere.\n\n- The predicted plaintext symbol for query $q \\in \\mathbb{R}^{4}$ is obtained by:\n  1. Computing similarity scores against all keys using the dot product.\n  2. Dividing scores by a dimension-dependent scale factor to maintain stability across $d_k$.\n  3. Applying exponentiation and normalization to obtain a probability distribution over keys.\n  4. Taking a weighted sum of the value vectors using these probabilities to produce an output vector in $\\mathbb{R}^{5}$.\n  5. Choosing the symbol whose index is the position of the largest component of the output vector (top-$1$ prediction).\n\nNormalization details:\n- All queries must be normalized to unit $\\ell_2$ norm after any additive noise is introduced.\n\nDecryption accuracy:\n- For a sequence of $N$ queries, the top-$1$ decryption accuracy is the fraction of positions for which the predicted symbol index equals the ground-truth label associated with the queryâ€™s intended key.\n\nTest suite:\nImplement three test cases that exercise different facets of attention-based decryption.\n\n- Test Case A (happy path, exact matches):\n  Use the permutation $\\pi_A = [2, 0, 4, 1, 3]$ and define $Q_A$ by permuting the keys: $Q_A = \\begin{bmatrix} k_{\\pi_A(0)}^\\top \\\\ k_{\\pi_A(1)}^\\top \\\\ k_{\\pi_A(2)}^\\top \\\\ k_{\\pi_A(3)}^\\top \\\\ k_{\\pi_A(4)}^\\top \\end{bmatrix}$. The ground-truth plaintext sequence is the label list $[ \\pi_A(0), \\pi_A(1), \\pi_A(2), \\pi_A(3), \\pi_A(4) ]$.\n\n- Test Case B (noisy queries, stability under perturbations):\n  Start from $Q_A$ and add independent Gaussian noise with standard deviation $\\sigma = 0.30$ to each component, then renormalize each query to unit norm to obtain $Q_B$. Use the same ground-truth labels as in Test Case A.\n\n- Test Case C (key collision, ambiguity in content addressing):\n  Create a colliding key set $K_C$ by taking $K$ and replacing $k_1$ with $k_3$, i.e., set $k_1 := k_3$. Use the permutation $\\pi_C = [1, 3, 0, 4, 2]$ and define $Q_C$ by permuting the rows of $K_C$ accordingly. Use the ground-truth plaintext labels $[ \\pi_C(0), \\pi_C(1), \\pi_C(2), \\pi_C(3), \\pi_C(4) ]$. The values $V$ remain the $5 \\times 5$ identity, meaning the two identical keys still correspond to different plaintext symbols.\n\nRequired output:\n- Your program must compute the top-$1$ decryption accuracy for each of the three test cases in the order A, B, C.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[a_A,a_B,a_C]$, where each $a_\\cdot$ is a floating-point number in decimal form.", "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded in the principles of linear algebra and the concepts of attention mechanisms from deep learning. The problem is well-posed, self-contained, and formalizable into a computational task. There are no contradictions, ambiguities, or factual inaccuracies.\n\nThe task is to design and analyze a toy cipher based on the scaled dot-product attention mechanism, which serves as a model for content-addressable memory. This involves retrieving plaintext symbols (values) by matching ciphertext (queries) against a set of stored addresses (keys). The success of this decryption process is quantified by a top-$1$ accuracy metric.\n\nThe core of the decryption mechanism is the attention function, which computes an output for a set of queries $Q \\in \\mathbb{R}^{N \\times d_k}$ given a set of keys $K \\in \\mathbb{R}^{N \\times d_k}$ and values $V \\in \\mathbb{R}^{N \\times d_v}$. The dimensions are given as $N=5$, $d_k=4$, and $d_v=5$. The process for a matrix of queries $Q$ is defined as:\n$1$. Compute similarity scores between each query and all keys. This is performed via matrix multiplication: $S = QK^\\top$. The resulting score matrix $S \\in \\mathbb{R}^{N \\times N}$ contains the dot product of each query vector with each key vector.\n$2$. Scale the scores to counteract the growth of dot product variance with dimension. Each score is divided by $\\sqrt{d_k}$. The scaled score matrix is $S' = \\frac{QK^\\top}{\\sqrt{d_k}}$.\n$3$. Normalize the scaled scores for each query to form a probability distribution over the keys. This is achieved by applying the softmax function row-wise to $S'$: $A = \\text{softmax}(S')$. The element $A_{ij}$ of the resulting attention matrix $A \\in \\mathbb{R}^{N \\times N}$ represents the weight of key $j$ for query $i$.\n$4$. Compute the output vectors as a weighted sum of the value vectors, where the weights are the attention probabilities. This is computed as $O = AV$. The resulting output matrix $O \\in \\mathbb{R}^{N \\times d_v}$ contains the retrieved (decrypted) representations.\n$5$. For each output vector $o_i$ (a row in $O$), the predicted plaintext symbol index $\\hat{y}_i$ is the index of its largest component: $\\hat{y}_i = \\arg\\max_j (o_i)_j$.\n\nThe decryption accuracy is the fraction of queries for which the predicted symbol index matches the ground-truth symbol index: $\\text{Accuracy} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\mathbb{I}(\\hat{y}_i = y_i)$, where $\\mathbb{I}$ is the indicator function.\n\nThe key and value matrices are defined as follows. The key vectors are:\n$k_0 = (1, 0, 0, 0)$\n$k_1 = (0, 1, 0, 0)$\n$k_2 = (0, 0, 1, 0)$\n$k_3 = (0, 0, 0, 1)$\n$k_4 = \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0, 0\\right)$\nThese are collected as rows into the key matrix $K \\in \\mathbb{R}^{5 \\times 4}$. The value matrix $V \\in \\mathbb{R}^{5 \\times 5}$ is the identity matrix, $V=I_5$. This means the value vector $v_i$ associated with key $k_i$ is the one-hot vector $e_i$ corresponding to the label $i$.\n\nWe now analyze the three specified test cases.\n\n**Test Case A: Exact Matches**\nIn this case, the query vectors are perfect copies of the key vectors, albeit in a permuted order. The permutation is $\\pi_A = [2, 0, 4, 1, 3]$, so the query matrix is $Q_A$, where the $i$-th row is $k_{\\pi_A(i)}$. The ground-truth labels are $Y_A = \\pi_A$.\nFor any query $q_i = k_{\\pi_A(i)}$, its dot product with the matching key $k_{\\pi_A(i)}$ will be $1$ (since all keys are unit-norm), while its dot product with any other orthogonal key (e.g., $k_0 \\cdot k_1 = 0$) will be $0$. The dot product with the non-orthogonal key $k_4$ will be less than $1$. For example, for $q_0 = k_2$, the scores will be $[k_2 \\cdot k_0, k_2 \\cdot k_1, k_2 \\cdot k_2, k_2 \\cdot k_3, k_2 \\cdot k_4] = [0, 0, 1, 0, 0]$. The softmax function will concentrate all probability mass on the index of the matching key. Therefore, the attention mechanism will perfectly retrieve the correct value vector $v_{\\pi_A(i)}$, and the predicted label will be $\\pi_A(i)$. This should hold for all queries, resulting in a decryption accuracy of $1.0$.\n\n**Test Case B: Noisy Queries**\nHere, we start with $Q_A$ and add Gaussian noise with standard deviation $\\sigma = 0.30$ to each component. The resulting noisy query vectors are then renormalized to unit $\\ell_2$ norm. This simulates a more realistic scenario where ciphertext may be corrupted.\nThe noise perturbs the query vectors. A noisy query $q_i'$ derived from $k_{\\pi_A(i)}$ will still be most similar to its original key $k_{\\pi_A(i)}$, but its dot product scores with other keys will no longer be exactly zero. As long as the noise is not excessively large, the score $q_i' \\cdot k_{\\pi_A(i)}$ will remain the largest score. The softmax function will thus still assign the highest attention weight to the correct key. However, if the noise is large enough to make a query more similar to an incorrect key, a decryption error will occur. With $\\sigma=0.30$, we expect high but not perfect accuracy.\n\n**Test Case C: Key Collision**\nThis case demonstrates a failure mode of content-addressable memory: ambiguity from non-unique keys. The key matrix $K_C$ is created by replacing key $k_1$ in the original matrix $K$ with key $k_3$. So, $K_C$ has two identical rows at indices $1$ and $3$, both equal to the vector $k_3 = (0, 0, 0, 1)$. The value matrix $V$ remains the identity matrix, meaning the key at index $1$ is associated with plaintext label $1$ (value $v_1=e_1$) and the key at index $3$ is associated with label $3$ (value $v_3=e_3$).\nThe queries $Q_C$ are generated by permuting the rows of this new key matrix $K_C$ with $\\pi_C = [1, 3, 0, 4, 2]$. The ground-truth labels are $Y_C = \\pi_C$.\nLet's analyze the query $q_0 = k_{C, \\pi_C(0)} = k_{C,1}$. Since row $1$ of $K_C$ is $k_3$, this query is $q_0 = k_3$. The ground-truth label is $y_0 = \\pi_C(0) = 1$. When we compute the attention scores for this query against $K_C$, the dot product $q_0 \\cdot k_{C,j}$ will be maximal and equal for both $j=1$ and $j=3$, as both $k_{C,1}$ and $k_{C,3}$ are identical to $q_0$. The softmax function will assign equal, high attention weights to keys $1$ and $3$. The resulting output vector will be a sum of value vectors, dominated by $w_1 v_1 + w_3 v_3$, where $w_1=w_3$. This means the output vector will have equal large components at indices $1$ and $3$. The $\\arg\\max$ function, by convention (e.g., in NumPy), breaks ties by returning the first index of the maximum value. Thus, the prediction for $q_0$ will be $1$, which matches the ground truth $y_0=1$.\nNow consider the query $q_1 = k_{C, \\pi_C(1)} = k_{C,3}$. This query is also the vector $k_3$, identical to $q_0$. The prediction process is identical, so the predicted label will again be $1$. However, the ground-truth label is $y_1 = \\pi_C(1) = 3$. This is a decryption error.\nFor the other queries ($q_2, q_3, q_4$), they match unique keys in $K_C$ and are expected to be decrypted correctly. In total, we expect $1$ error out of $5$ queries, leading to an accuracy of $4/5 = 0.8$.\nThis case highlights that when keys are not unique, the attention mechanism cannot distinguish between them, and the retrieval becomes ambiguous, with the outcome potentially depending on arbitrary tie-breaking rules.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the toy cipher problem by implementing and testing a simplified\n    attention mechanism.\n    \"\"\"\n    N = 5\n    d_k = 4\n    d_v = 5\n\n    def softmax(x, axis=-1):\n        \"\"\"Numerically stable softmax function.\"\"\"\n        e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n        return e_x / np.sum(e_x, axis=axis, keepdims=True)\n\n    def attention_decrypt(Q, K, V):\n        \"\"\"\n        Performs decryption using the scaled dot-product attention mechanism.\n        Returns the predicted symbol indices.\n        \"\"\"\n        # 1. Compute similarity scores and scale them\n        scores = (Q @ K.T) / np.sqrt(d_k)\n        \n        # 2. Compute attention weights\n        attention_weights = softmax(scores, axis=1)\n        \n        # 3. Compute output vectors (weighted sum of values)\n        output = attention_weights @ V\n        \n        # 4. Predict symbols by taking the argmax\n        predictions = np.argmax(output, axis=1)\n        \n        return predictions\n\n    def calculate_accuracy(predictions, ground_truth):\n        \"\"\"Calculates the top-1 decryption accuracy.\"\"\"\n        return np.mean(predictions == ground_truth)\n\n    # --- Common Setup ---\n    # Define the base key matrix K\n    K = np.array([\n        [1.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0],\n        [1/np.sqrt(2), 1/np.sqrt(2), 0.0, 0.0]\n    ])\n\n    # Define the value matrix V as the identity matrix\n    V = np.identity(N)\n\n    results = []\n\n    # --- Test Case A: Happy path, exact matches ---\n    pi_A = np.array([2, 0, 4, 1, 3])\n    Q_A = K[pi_A]\n    Y_A = pi_A\n    \n    predictions_A = attention_decrypt(Q_A, K, V)\n    accuracy_A = calculate_accuracy(predictions_A, Y_A)\n    results.append(accuracy_A)\n\n    # --- Test Case B: Noisy queries, stability under perturbations ---\n    # Use a fixed seed for reproducibility\n    np.random.seed(42)\n    sigma = 0.30\n    \n    noise = np.random.normal(scale=sigma, size=Q_A.shape)\n    Q_B_noisy = Q_A + noise\n    \n    # Renormalize each query to unit norm\n    norms = np.linalg.norm(Q_B_noisy, axis=1, keepdims=True)\n    Q_B = Q_B_noisy / norms\n    \n    # Ground truth remains the same\n    Y_B = Y_A\n    \n    predictions_B = attention_decrypt(Q_B, K, V)\n    accuracy_B = calculate_accuracy(predictions_B, Y_B)\n    results.append(accuracy_B)\n\n    # --- Test Case C: Key collision, ambiguity in content addressing ---\n    # Create the colliding key set K_C\n    K_C = K.copy()\n    K_C[1] = K[3]  # k_1 is replaced by k_3\n    \n    pi_C = np.array([1, 3, 0, 4, 2])\n    \n    # Queries Q_C are permuted rows of K_C\n    Q_C = K_C[pi_C]\n    \n    # Ground truth labels\n    Y_C = pi_C\n    \n    predictions_C = attention_decrypt(Q_C, K_C, V)\n    accuracy_C = calculate_accuracy(predictions_C, Y_C)\n    results.append(accuracy_C)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3195550"}, {"introduction": "With a grasp of attention as a retrieval mechanism, we can now examine how it is deployed within encoder and decoder stacks. The crucial distinction is the scope of information they can access: encoders look at the entire sequence (bidirectional attention), while decoders can only look at past and current positions (causal attention). This practice [@problem_id:3195539] crystallizes this difference through a task that only an encoder can solve, providing a concrete demonstration of how architectural constraints define a model's capabilities and intended use cases.", "problem": "You will construct, analyze, and algorithmically decide a synthetic sequence classification task that fundamentally requires bidirectional context, and you will compare what can be computed by a stack of encoder layers versus a stack of decoder layers with a causal mask. You will then implement a program that, given a small test suite of parameter values, determines which architecture can solve the task under those parameters. All mathematical entities must be written in LaTeX.\n\nTask definition. Consider binary sequences of odd length $n \\in \\mathbb{N}$ with a special query token at the middle position. Write the sequence as\n$$\nx = \\big(a_{1}, a_{2}, \\dots, a_{m-1}, \\mathrm{[Q]}, b_{1}, b_{2}, \\dots, b_{m-1}\\big),\n$$\nwhere $n = 2m-1$, $m = \\frac{n+1}{2}$, and each $a_{k}, b_{k} \\in \\{0,1\\}$ for $k \\in \\{1,\\dots,m-1\\}$. Define the classification function $f:\\{0,1\\}^{n-1}\\to \\{0,1\\}$ by\n$$\nf(x) = 1 \\;\\;\\text{if and only if}\\;\\; \\forall d \\in \\{1,\\dots,m-1\\},\\; a_{d} = b_{d},\n$$\nand $f(x)=0$ otherwise. In words, the token at distance $d$ to the left of $\\mathrm{[Q]}$ must equal the token at distance $d$ to the right of $\\mathrm{[Q]}$; this is a center-palindrome check around $\\mathrm{[Q]}$.\n\nArchitectural model. Consider two architectures built from $L \\in \\mathbb{N}$ identical layers of Multi-Head Attention (MHA) with a per-layer, per-head windowed self-attention constraint of width $w \\in \\mathbb{N}$, followed by position-wise feed-forward networks. The attention graph abstraction for one layer is:\n- Encoder stack: from each position $i$, there are directed edges to all positions $j$ with $|i-j| \\le w$ (bidirectional within the window).\n- Decoder stack with a causal mask: from each position $i$, there are directed edges to all positions $j$ with $0 \\le i-j \\le w$ (only to the left within the window, including self).\n\nInformation-propagation model. Model each layer as enabling one-step message passing along edges. Over $L$ layers, information can propagate along any path of at most $L$ edges in the directed graph defined by the layer connectivity. The decision for this task must be produced at the middle query position $\\mathrm{[Q]}$ after exactly $L$ layers.\n\nYour goals.\n1. From first principles of directed graph reachability induced by the attention windows, prove that for the decoder with a causal mask, the token at $\\mathrm{[Q]}$ cannot access any information from the right half $\\{b_{1},\\dots,b_{m-1}\\}$ for any $n1$, regardless of $w$ and $L$. Conclude that no decoder of this form can compute $f$ exactly for all inputs when $n1$.\n2. From the same principles, derive a necessary and sufficient condition on $n$, $w$, and $L$ under which an encoder stack can compute $f$ exactly at $\\mathrm{[Q]}$. Your derivation must begin from the directed graph reachability view and yield a closed-form condition involving $n$, $w$, and $L$, justified by a constructive scheme that aggregates evidence to $\\mathrm{[Q]}$.\n3. Design a decision rule in terms of $n$, $w$, and $L$ that returns two booleans: one indicating whether an encoder stack can solve the task at $\\mathrm{[Q]}$ exactly for all inputs of length $n$, and one indicating whether a decoder with a causal mask can do so. Your rule must correctly handle the degenerate case $n=1$.\n\nProgram specification. Implement a complete, runnable program that evaluates your decision rule on the following test suite of $(n,w,L)$ triplets:\n- $(n,w,L) = (21,3,4)$,\n- $(n,w,L) = (17,4,2)$,\n- $(n,w,L) = (31,3,4)$,\n- $(n,w,L) = (1,1,1)$,\n- $(n,w,L) = (19,2,5)$.\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output an integer code $c \\in \\{0,1,2,3\\}$ defined by\n$$\nc \\;=\\; 2\\cdot \\mathbf{1}\\{\\text{encoder can solve}\\}\\;+\\; \\mathbf{1}\\{\\text{decoder can solve}\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function that returns $1$ if the statement is true and $0$ otherwise. For example, $c=2$ means the encoder can solve and the decoder cannot, $c=3$ means both can, $c=0$ means neither can, and $c=1$ means the decoder can but the encoder cannot. The final output format must be exactly like $[c_{1},c_{2},c_{3},c_{4},c_{5}]$ for the five cases listed above.", "solution": "The problem statement is assessed to be valid. It is a well-posed, scientifically grounded, and objective problem that explores the computational limits of idealized transformer architectures. The problem is based on a standard and useful abstraction of information flow in neural networks as graph reachability, a concept central to the theoretical analysis of deep learning models. All definitions and constraints are clear and self-consistent.\n\nWe now proceed to the solution, which is divided into three parts as requested.\n\nFirst, we formalize the problem's information-flow model. The sequence has length $n$, with positions indexed from $1$ to $n$. The central query token $\\mathrm{[Q]}$ is at position $m = (n+1)/2$. The task is to verify if $a_d = b_d$ for all $d \\in \\{1, \\dots, m-1\\}$, where $a_d$ is at position $m-d$ and $b_d$ is at position $m+d$. The decision is made at position $m$. Following the standard interpretation of transformer mechanics, a layer's computation at a position $i$ gathers information *from* a set of positions $\\{j\\}$. Therefore, the problem's description of \"directed edges from each position $i$ to all positions $j$\" is interpreted as the set of positions $\\{j\\}$ that position $i$ attends to, implying information flows from $j$ to $i$. The directed edges of the information-flow graph are thus $(j, i)$.\n\n### 1. Analysis of the Decoder Stack with Causal Mask\n\nThe decoder architecture is defined by a causal attention mask. Under our formal model, an edge $(j, i)$ exists in the information-flow graph if and only if position $i$ attends to position $j$. The rule is given by $0 \\le i-j \\le w$. This implies that $i-w \\le j \\le i$.\n\nA crucial property of this rule is that for any edge $(j, i)$, the source index $j$ must be less than or equal to the destination index $i$ (i.e., $j \\le i$). Now, consider a path of information propagation, which is a sequence of connected nodes $(p_0, p_1, \\dots, p_k)$ in the graph, where $p_0$ is the original source of information and $p_k$ is the final destination. For this to be a valid path, an edge $(p_s, p_{s+1})$ must exist for each $s \\in \\{0, \\dots, k-1\\}$. Applying the edge rule, this requires $p_s \\le p_{s+1}$ for all steps $s$. Consequently, any path in the decoder's information-flow graph must have non-decreasing position indices: $p_0 \\le p_1 \\le \\dots \\le p_k$.\n\nThe classification task requires checking if $a_d = b_d$, where $b_d$ is the token at position $m+d$. For any $d \\in \\{1, \\dots, m-1\\}$, the position $m+d$ is strictly greater than $m$. For the decision to be made at position $m$, information about the token $b_d$ must propagate from its source position $j_{src} = m+d$ to the destination position $i_{dst} = m$.\n\nThis would require the existence of a path from $p_0 = m+d$ to $p_k = m$. However, as proven above, any such path must satisfy $p_0 \\le p_k$, which would mean $m+d \\le m$. This inequality is false for any $d \\ge 1$. Therefore, no path exists from any position in the right half of the sequence (positions greater than $m$) to the central query position $m$.\n\nSince the value of $f(x)$ depends on tokens $\\{b_1, \\dots, b_{m-1}\\}$, and information from these tokens cannot reach position $m$, a decoder stack with a causal mask cannot compute $f(x)$ for any input sequence where $n  1$.\n\nThe degenerate case is $n=1$. Here, $m = (1+1)/2 = 1$, and the range of $d$ is $\\{1, \\dots, m-1\\} = \\emptyset$. The condition $\\forall d \\in \\emptyset, a_d=b_d$ is vacuously true. Thus, for $n=1$, $f(x)=1$ for all inputs. A decoder can trivially learn to output this constant value.\n\nConclusion: A decoder with a causal mask can solve the task if and only if $n=1$.\n\n### 2. Analysis of the Encoder Stack\n\nThe encoder architecture has bidirectional attention. An edge $(j,i)$ exists if $|i-j| \\le w$. This condition is symmetric, meaning an edge $(i,j)$ also exists. Information can flow in both directions between any two positions within the window $w$.\n\nTo compute $f(x)$ at position $m$, the model must have access to information from all other positions in the sequence. The model consists of $L$ layers. In one layer, information at position $j$ can propagate to any position $i$ in the interval $[j-w, j+w]$. After $L$ layers, information from an initial position $j$ has propagated to cover the interval $[j-Lw, j+Lw]$. This is the effective receptive field.\n\nFor the decision at position $m$ to be fully informed, information from every position in the sequence $\\{1, \\dots, n\\}$ must be able to reach $m$ within $L$ layers. This means that for any position $j \\in \\{1, \\dots, n\\}$, $m$ must be within the receptive field of $j$ after $L$ layers. That is, $m \\in [j-Lw, j+Lw]$, which is equivalent to $|m-j| \\le Lw$.\n\nWe must ensure this condition holds for all $j$. The most stringent requirement comes from the positions most distant from the center $m$. These are the endpoints of the sequence, $j=1$ and $j=n$.\nThe distance from the center $m = (n+1)/2$ to the endpoints is:\n- To position $1$: $|m-1| = |\\frac{n+1}{2} - 1| = |\\frac{n-1}{2}| = \\frac{n-1}{2}$.\n- To position $n$: $|m-n| = |\\frac{n+1}{2} - n| = |\\frac{1-n}{2}| = \\frac{n-1}{2}$.\n\nSince these are the maximum distances, the condition simplifies to ensuring that information can travel this far. The total distance information can propagate in $L$ layers is $Lw$. Therefore, a necessary and sufficient condition for all information to reach the center is:\n$$\nLw \\ge \\frac{n-1}{2}\n$$\nIf this condition is met, information from all pairs $(a_d, b_d)$ can be aggregated at position $m$, allowing a sufficiently powerful model (which we assume) to compute the function $f(x)$. If the condition is not met, there is at least one token (at an endpoint) whose information cannot reach the center, making the computation impossible for certain inputs. Note that since $n$ is odd, $n-1$ is even, and $(n-1)/2$ is always an integer.\n\nFor the special case $n=1$, the condition becomes $Lw \\ge (1-1)/2$, which is $Lw \\ge 0$. As $L \\in \\mathbb{N}$ and $w \\in \\mathbb{N}$, we have $L \\ge 1$ and $w \\ge 1$, so this is always true. This aligns with the fact that the task is trivial for $n=1$.\n\nConclusion: An encoder stack can solve the task if and only if $Lw \\ge (n-1)/2$.\n\n### 3. Final Decision Rule\n\nBased on the analysis above, we can formulate a decision rule for a given triplet $(n, w, L)$.\n\n- **Encoder Solvability**: An encoder stack can solve the task if and only if the cumulative information propagation distance $Lw$ is sufficient to cover the distance from the sequence endpoints to the center.\n  $$ \\mathbf{1}\\{\\text{encoder can solve}\\} = \\mathbf{1}\\left\\{ Lw \\ge \\frac{n-1}{2} \\right\\} $$\n\n- **Decoder Solvability**: A decoder stack with a causal mask can solve the task if and only if the sequence is of length $n=1$, where the classification task is trivial.\n  $$ \\mathbf{1}\\{\\text{decoder can solve}\\} = \\mathbf{1}\\{ n = 1 \\} $$\n\nThe integer code $c$ for each test case is computed as specified:\n$$ c = 2\\cdot \\mathbf{1}\\{\\text{encoder can solve}\\} + \\mathbf{1}\\{\\text{decoder can solve}\\} $$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the transformer architecture problem by applying the derived decision rules\n    to a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (21, 3, 4),  # (n, w, L) triplet 1\n        (17, 4, 2),  # (n, w, L) triplet 2\n        (31, 3, 4),  # (n, w, L) triplet 3\n        (1, 1, 1),   # (n, w, L) triplet 4\n        (19, 2, 5),  # (n, w, L) triplet 5\n    ]\n\n    results = []\n    for n, w, l in test_cases:\n        # A. Decision rule for the decoder stack with a causal mask.\n        # The task is a center-palindrome check.\n        # A decoder with a causal mask prevents information flow from the future (right side)\n        # to the past (left side or center).\n        # When n  1, the decision at the center position m = (n+1)/2 cannot access\n        # information from any position j  m.\n        # Thus, the comparison required by the task is impossible.\n        # The only exception is the trivial case n=1, where the palindrome condition is\n        # vacuously true, and the model only needs to output a constant 1.\n        decoder_can_solve = (n == 1)\n\n        # B. Decision rule for the encoder stack.\n        # An encoder has bidirectional attention. Information can flow from any position j to\n        # any position i, provided enough layers L and a large enough window w.\n        # The key condition is that information from the sequence endpoints (positions 1 and n)\n        # must be able to reach the center position m = (n+1)/2.\n        # The distance from either endpoint to the center is (n-1)/2.\n        # In L layers, with an attention window of width w, information can propagate\n        # a maximum distance of L * w.\n        # The task is solvable if and only if this reach is sufficient to cover the distance.\n        # This condition also correctly handles the n=1 case (l * w = 0, which is always true).\n        encoder_can_solve = (l * w = (n - 1) / 2)\n\n        # C. Calculate the integer code c as per the problem specification.\n        # c = 2 * I{encoder can solve} + 1 * I{decoder can solve}\n        # where I{.} is the indicator function (1 if true, 0 if false).\n        c = 2 * int(encoder_can_solve) + 1 * int(decoder_can_solve)\n        results.append(c)\n\n    # Final print statement in the exact required format: [c1,c2,c3,c4,c5]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3195539"}, {"introduction": "A single attention mechanism can only focus on one type of relationship at a time, which is why Transformers employ Multi-Head Attention (MHA) to learn from different representational subspaces simultaneously. However, a common failure mode called 'head collapse' occurs when multiple heads learn redundant patterns, diminishing the model's effectiveness. This final practice [@problem_id:3195528] delves into this practical issue, challenging you to identify its causes and implement a regularization technique to promote head diversity, a key aspect of training robust Transformer models.", "problem": "Consider a Transformer encoder or decoder with Multi-Head Attention (MHA). Each attention head computes Scaled Dot-Product Attention (SDPA) on an input sequence. Let the input be a matrix $X \\in \\mathbb{R}^{L \\times d_{\\text{model}}}$ with $L$ tokens and model dimension $d_{\\text{model}}$. For head $h \\in \\{1,\\dots,H\\}$, queries and keys are computed as $Q^{(h)} = X W_Q^{(h)}$ and $K^{(h)} = X W_K^{(h)}$, where $W_Q^{(h)} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{k}}}$ and $W_K^{(h)} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{k}}}$, and $d_{\\text{k}}$ is the head dimension. The scaled score matrix is $S^{(h)} = \\frac{1}{\\sqrt{d_{\\text{k}}}} Q^{(h)} (K^{(h)})^{\\top} \\in \\mathbb{R}^{L \\times L}$. The attention map for head $h$ is $A^{(h)} = \\operatorname{Softmax}_{\\text{row}}(S^{(h)})$, where $\\operatorname{Softmax}_{\\text{row}}$ applies the softmax function row-wise: for row $i \\in \\{1,\\dots,L\\}$, $A_{i,:}^{(h)} = \\operatorname{softmax}(S_{i,:}^{(h)})$, and $\\operatorname{softmax}(z)_j = \\frac{\\exp(z_j)}{\\sum_{m=1}^{L} \\exp(z_m)}$. Throughout, the softmax is invariant to adding a constant to all entries of a row, and produces a uniform distribution if all entries in the row are equal.\n\nDefine an attention-head diversity regularizer that penalizes similarity among attention maps. For each head $h$, flatten its attention map $A^{(h)}$ to a vector $a^{(h)} \\in \\mathbb{R}^{L \\cdot L}$ (concatenate rows). For any two distinct heads $h \\neq k$, define their cosine similarity as\n$$\n\\operatorname{cos}(a^{(h)}, a^{(k)}) = \\frac{\\langle a^{(h)}, a^{(k)} \\rangle}{\\|a^{(h)}\\|_2 \\, \\|a^{(k)}\\|_2}.\n$$\nLet the regularizer be the average pairwise cosine similarity scaled by a nonnegative weight $\\lambda$, namely\n$$\n\\mathcal{R}(A^{(1)},\\dots,A^{(H)}) = \\lambda \\cdot \\frac{1}{\\binom{H}{2}} \\sum_{1 \\le h  k \\le H} \\operatorname{cos}\\!\\left(a^{(h)}, a^{(k)}\\right).\n$$\n\nTask A (input pattern construction): From first principles, construct input patterns $X$ that lead to head collapse, meaning all heads produce near-identical attention maps $A^{(h)}$. Argue using fundamental properties of $\\operatorname{Softmax}$ and SDPA why such inputs cause $A^{(h)}$ to be identical across $h$. Provide at least two distinct patterns grounded in the base definitions.\n\nTask B (regularizer gradient): Derive from the definition of cosine similarity the gradient of $\\mathcal{R}$ with respect to the flattened attention vector $a^{(h)}$ for each head $h$. Your derivation must start from the basic definitions of inner product, vector norm, and the chain rule, and must not use any shortcut formulas. Express the final gradient $\\nabla_{a^{(h)}} \\mathcal{R}$ as a function of $\\{a^{(j)}\\}_{j=1}^{H}$.\n\nImplementation Task: Write a complete, runnable program that\n- Implements SDPA to compute $\\{A^{(h)}\\}_{h=1}^{H}$ for given $X$, $\\{W_Q^{(h)}\\}$, and $\\{W_K^{(h)}\\}$.\n- Flattens each $A^{(h)}$ to $a^{(h)}$.\n- Computes $\\mathcal{R}$ and the analytical gradient $\\{\\nabla_{a^{(h)}} \\mathcal{R}\\}_{h=1}^{H}$ derived in Task B.\n- Detects head collapse by computing the average pairwise cosine similarity and checking if it exceeds a threshold near $1$.\n- Produces results for a test suite of three cases that cover a typical case, a diverse case, and an edge case.\n\nYou must use the following test suite with explicit numerical parameters:\n- Case $1$ (collapse via identical tokens): $L = 3$, $d_{\\text{model}} = 4$, $d_{\\text{k}} = 2$, $H = 3$, $\\lambda = 0.5$. Let $X$ have identical rows $x = [2, -1, 0.5, 3]$, so $X = \\begin{bmatrix}2  -1  0.5  3 \\\\ 2  -1  0.5  3 \\\\ 2  -1  0.5  3\\end{bmatrix}$. Let\n$\nW_Q^{(1)} = \\begin{bmatrix}1  0 \\\\ 0  1 \\\\ 1  -1 \\\\ 0  2\\end{bmatrix},\\;\nW_K^{(1)} = \\begin{bmatrix}1  1 \\\\ 0  1 \\\\ 1  0 \\\\ 0  1\\end{bmatrix},\n$\n$\nW_Q^{(2)} = \\begin{bmatrix}0  1 \\\\ 1  0 \\\\ -1  1 \\\\ 2  0\\end{bmatrix},\\;\nW_K^{(2)} = \\begin{bmatrix}1  0 \\\\ 1  -1 \\\\ 0  2 \\\\ 1  0\\end{bmatrix},\n$\n$\nW_Q^{(3)} = \\begin{bmatrix}1  -1 \\\\ 0  2 \\\\ 1  0 \\\\ -1  1\\end{bmatrix},\\;\nW_K^{(3)} = \\begin{bmatrix}2  0 \\\\ 0  1 \\\\ 1  -1 \\\\ 0  1\\end{bmatrix}.\n$\n- Case $2$ (diverse heads and tokens): $L = 3$, $d_{\\text{model}} = 4$, $d_{\\text{k}} = 2$, $H = 3$, $\\lambda = 0.5$. Let\n$\nX = \\begin{bmatrix}\n1  0  1  0 \\\\\n0  1  0  1 \\\\\n1  1  0  0\n\\end{bmatrix}.\n$\nLet\n$\nW_Q^{(1)} = \\begin{bmatrix}1  0 \\\\ 0  1 \\\\ 0  1 \\\\ 1  0\\end{bmatrix},\\;\nW_K^{(1)} = \\begin{bmatrix}1  0 \\\\ 1  0 \\\\ 0  1 \\\\ 0  1\\end{bmatrix},\n$\n$\nW_Q^{(2)} = \\begin{bmatrix}2  0 \\\\ 0  -1 \\\\ 1  1 \\\\ 0  1\\end{bmatrix},\\;\nW_K^{(2)} = \\begin{bmatrix}-1  0 \\\\ 0  2 \\\\ 1  0 \\\\ 0  1\\end{bmatrix},\n$\n$\nW_Q^{(3)} = \\begin{bmatrix}0  1 \\\\ 1  0 \\\\ -1  1 \\\\ 1  -1\\end{bmatrix},\\;\nW_K^{(3)} = \\begin{bmatrix}1  1 \\\\ 0  -1 \\\\ 2  0 \\\\ 1  0\\end{bmatrix}.\n$\n- Case $3$ (edge case: zero input): $L = 3$, $d_{\\text{model}} = 4$, $d_{\\text{k}} = 2$, $H = 3$, $\\lambda = 0.5$. Let $X$ be the $L \\times d_{\\text{model}}$ zero matrix, with the same $\\{W_Q^{(h)}\\}$ and $\\{W_K^{(h)}\\}$ as Case $2$.\n\nFor each case, compute:\n- The average pairwise cosine similarity of $\\{a^{(h)}\\}_{h=1}^{H}$ as a real number.\n- A collapse indicator $\\in \\{\\text{True}, \\text{False}\\}$ which is $\\text{True}$ if the average pairwise cosine similarity is at least $0.99$ and $\\text{False}$ otherwise.\n- The list of Euclidean norms of the gradients $\\{\\|\\nabla_{a^{(h)}} \\mathcal{R}\\|_2\\}_{h=1}^{H}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated Python-style list, where each element is a list of the form $[\\text{average\\_similarity}, \\text{collapse\\_boolean}, \\text{gradient\\_norms\\_list}]$.\n- All real numbers must be printed as decimals rounded to $6$ places.\n- Example shape only (do not use these placeholder names): $[[\\text{r1}, \\text{b1}, \\text{list1}], [\\text{r2}, \\text{b2}, \\text{list2}], [\\text{r3}, \\text{b3}, \\text{list3}]]$.\n\nYour derivations and arguments in Tasks A and B must be based solely on the base definitions above, the properties of $\\operatorname{Softmax}$, the inner product, the Euclidean norm, and the chain rule, without introducing any shortcut formulas or external assumptions.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of deep learning, specifically Transformer architectures, and is mathematically well-posed, self-contained, and formalizable. All provided parameters and definitions are consistent and sufficient for deriving a solution.\n\n### Task A: Input Patterns for Head Collapse\n\nHead collapse in Multi-Head Attention occurs when the attention maps $A^{(h)} \\in \\mathbb{R}^{L \\times L}$ produced by different heads $h \\in \\{1,\\dots,H\\}$ become identical or highly similar. The attention map for head $h$ is defined as $A^{(h)} = \\operatorname{Softmax}_{\\text{row}}(S^{(h)})$, where the score matrix is $S^{(h)} = \\frac{1}{\\sqrt{d_k}} Q^{(h)} (K^{(h)})^{\\top}$, with $Q^{(h)}=XW_Q^{(h)}$ and $K^{(h)}=XW_K^{(h)}$. Head collapse is guaranteed if, for every row $i$, the score vector $S_{i,:}^{(h)}$ is structured such that the output of the softmax function is independent of the head $h$. Based on the fundamental properties of the softmax function and matrix operations, we can construct at least two distinct input patterns $X$ that induce this behavior.\n\n1.  **Pattern of Identical Input Tokens**: Consider an input matrix $X \\in \\mathbb{R}^{L \\times d_{\\text{model}}}$ where all row vectors are identical. Let $X_{i,:} = x^{\\top}$ for all $i \\in \\{1, \\dots, L\\}$, where $x \\in \\mathbb{R}^{d_{\\text{model}}}$ is a constant vector.\n    -   For any head $h$, the query matrix is $Q^{(h)} = X W_Q^{(h)}$. Each row of $Q^{(h)}$ is the product of $x^{\\top}$ with $W_Q^{(h)}$, resulting in a matrix where all rows are identical to the vector $q^{(h)\\top} = x^{\\top} W_Q^{(h)}$.\n    -   Similarly, the key matrix $K^{(h)} = X W_K^{(h)}$ has all rows identical to the vector $k^{(h)\\top} = x^{\\top} W_K^{(h)}$.\n    -   The score matrix element $S_{ij}^{(h)}$ is the scaled dot product of the $i$-th row of $Q^{(h)}$ and the $j$-th row of $K^{(h)}$. Since all rows are identical within their respective matrices, $S_{ij}^{(h)} = \\frac{1}{\\sqrt{d_k}} q^{(h)\\top} k^{(h)}$.\n    -   This value is a scalar, let's call it $c^{(h)}$, that depends on the head $h$ but is constant for all token positions $(i, j)$. Thus, the score matrix $S^{(h)}$ is a constant matrix where every entry is $c^{(h)}$.\n    -   The attention map $A^{(h)}$ is computed by applying the softmax function to each row of $S^{(h)}$. For any row $i$, the input to the softmax is a vector of $L$ identical values, $[c^{(h)}, c^{(h)}, \\dots, c^{(h)}]$.\n    -   According to the definition $\\operatorname{softmax}(z)_j = \\frac{\\exp(z_j)}{\\sum_{m=1}^{L} \\exp(z_m)}$, the output for each element is $\\frac{\\exp(c^{(h)})}{L \\cdot \\exp(c^{(h)})} = \\frac{1}{L}$.\n    -   Therefore, the attention map for any head $h$ is a matrix with all entries equal to $1/L$. This resulting matrix is independent of the head-specific weights $W_Q^{(h)}$ and $W_K^{(h)}$, causing all heads to produce identical uniform attention maps and thus leading to complete head collapse.\n\n2.  **Pattern of Zero Input Tokens**: Let the input matrix be the zero matrix, $X = \\mathbf{0}_{L \\times d_{\\text{model}}}$.\n    -   This is a specific instance of the first pattern, where the identical token vector is the zero vector, $x = \\mathbf{0}$.\n    -   The query and key matrices for any head $h$ are consequently zero matrices: $Q^{(h)} = \\mathbf{0} \\cdot W_Q^{(h)} = \\mathbf{0}_{L \\times d_k}$ and $K^{(h)} = \\mathbf{0} \\cdot W_K^{(h)} = \\mathbf{0}_{L \\times d_k}$.\n    -   The score matrix for every head is also a zero matrix: $S^{(h)} = \\frac{1}{\\sqrt{d_k}} \\mathbf{0} \\cdot \\mathbf{0}^{\\top} = \\mathbf{0}_{L \\times L}$.\n    -   This corresponds to the previous analysis with the constant $c^{(h)}=0$. Applying the row-wise softmax to a vector of zeros yields the uniform distribution, as $\\frac{\\exp(0)}{\\sum_{m=1}^{L} \\exp(0)} = \\frac{1}{L}$.\n    -   Again, for every head $h$, the attention map $A^{(h)}$ is a matrix where all entries are $1/L$. This causes identical attention across all heads, resulting in collapse. This pattern is a distinct, singular point in the input space, whereas the first pattern represents a more general subspace of inputs that cause collapse.\n\n### Task B: Derivation of the Regularizer Gradient\n\nThe objective is to derive the gradient $\\nabla_{a^{(p)}} \\mathcal{R}$ of the regularizer $\\mathcal{R}$ with respect to a flattened attention vector $a^{(p)}$ for a specific head $p$. The regularizer is:\n$$\n\\mathcal{R}(\\{a^{(j)}\\}_{j=1}^H) = \\lambda \\cdot \\frac{1}{\\binom{H}{2}} \\sum_{1 \\le h  k \\le H} \\operatorname{cos}\\!\\left(a^{(h)}, a^{(k)}\\right)\n$$\nLet $C = \\lambda / \\binom{H}{2}$. The gradient with respect to $a^{(p)}$ is obtained by differentiating the sum term by term. A term $\\operatorname{cos}(a^{(h)}, a^{(k)})$ contributes to the gradient only if $h=p$ or $k=p$. We can rewrite the sum to isolate terms involving $a^{(p)}$:\n$$\n\\nabla_{a^{(p)}} \\mathcal{R} = C \\sum_{k \\neq p} \\nabla_{a^{(p)}} \\operatorname{cos}\\!\\left(a^{(p)}, a^{(k)}\\right)\n$$\nThe core of the derivation is to find the gradient of the cosine similarity function $\\operatorname{cos}(u, v) = \\frac{\\langle u, v \\rangle}{\\|u\\|_2 \\|v\\|_2}$ with respect to one of its vector arguments, say $u$. From first principles, $\\langle u, v \\rangle = u^\\top v$ and $\\|u\\|_2 = (u^\\top u)^{1/2}$. Let $f(u) = \\operatorname{cos}(u, v)$.\n$$\nf(u) = \\frac{u^\\top v}{(u^\\top u)^{1/2} \\|v\\|_2}\n$$\nWe use the quotient rule for vector differentiation. Let $N(u) = u^\\top v$ and $D(u) = (u^\\top u)^{1/2} \\|v\\|_2$. The gradient is $\\nabla_u f = \\frac{(\\nabla_u N)D - N(\\nabla_u D)}{D^2}$.\n1.  The gradient of the numerator is $\\nabla_u N(u) = \\nabla_u(u^\\top v) = v$.\n2.  The gradient of the denominator requires the chain rule. The gradient of $\\|u\\|_2 = (u^\\top u)^{1/2}$ with respect to $u$ is $\\nabla_u \\|u\\|_2 = \\frac{u}{\\|u\\|_2}$. Thus, $\\nabla_u D(u) = \\nabla_u (\\|u\\|_2 \\|v\\|_2) = \\frac{u}{\\|u\\|_2} \\|v\\|_2$.\n\nSubstituting these into the quotient rule expression:\n$$\n\\nabla_u f = \\frac{v \\cdot (\\|u\\|_2 \\|v\\|_2) - (u^\\top v) \\cdot \\left(\\frac{u}{\\|u\\|_2} \\|v\\|_2\\right)}{\\left(\\|u\\|_2 \\|v\\|_2\\right)^2}\n$$\nFactor out $\\|v\\|_2$ from the numerator and simplify:\n$$\n\\nabla_u f = \\frac{\\|v\\|_2 \\left(v \\|u\\|_2 - (u^\\top v) \\frac{u}{\\|u\\|_2}\\right)}{\\|u\\|_2^2 \\|v\\|_2^2} = \\frac{v \\|u\\|_2 - \\frac{u^\\top v}{\\|u\\|_2} u}{\\|u\\|_2^2 \\|v\\|_2}\n$$\nMultiply the numerator and denominator by $\\|u\\|_2$ to clear the fraction in the numerator:\n$$\n\\nabla_u f = \\frac{v \\|u\\|_2^2 - (u^\\top v) u}{\\|u\\|_2^3 \\|v\\|_2}\n$$\nThis expression can be rewritten by separating the terms:\n$$\n\\nabla_u \\operatorname{cos}(u,v) = \\frac{v \\|u\\|_2^2}{\\|u\\|_2^3 \\|v\\|_2} - \\frac{u(u^\\top v)}{\\|u\\|_2^3 \\|v\\|_2} = \\frac{v}{\\|u\\|_2 \\|v\\|_2} - \\frac{u (u^\\top v)}{\\|u\\|_2^2 (\\|u\\|_2 \\|v\\|_2)}\n$$\nBy definition, $\\operatorname{cos}(u, v) = \\frac{u^\\top v}{\\|u\\|_2 \\|v\\|_2}$, so substituting this back gives:\n$$\n\\nabla_u \\operatorname{cos}(u,v) = \\frac{v}{\\|u\\|_2 \\|v\\|_2} - \\frac{u \\operatorname{cos}(u, v)}{\\|u\\|_2^2}\n$$\nApplying this result to our regularizer gradient, let $u=a^{(p)}$ and $v=a^{(k)}$:\n$$\n\\nabla_{a^{(p)}} \\mathcal{R} = C \\sum_{k \\neq p} \\left( \\frac{a^{(k)}}{\\|a^{(p)}\\|_2 \\|a^{(k)}\\|_2} - \\frac{a^{(p)} \\operatorname{cos}(a^{(p)}, a^{(k)})}{\\|a^{(p)}\\|_2^2} \\right)\n$$\nWe can factor out terms not dependent on the summation index $k$:\n$$\n\\nabla_{a^{(p)}} \\mathcal{R} = C \\left( \\frac{1}{\\|a^{(p)}\\|_2} \\sum_{k \\neq p} \\frac{a^{(k)}}{\\|a^{(k)}\\|_2} - \\frac{a^{(p)}}{\\|a^{(p)}\\|_2^2} \\sum_{k \\neq p} \\operatorname{cos}(a^{(p)}, a^{(k)}) \\right)\n$$\nThis is the final analytical expression for the gradient, derived from fundamental definitions as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by running three test cases through the attention head\n    diversity analysis pipeline.\n    \"\"\"\n\n    def softmax_row(z):\n        \"\"\"Numerically stable softmax for a 1D array.\"\"\"\n        e_z = np.exp(z - np.max(z))\n        return e_z / e_z.sum()\n\n    def scaled_dot_product_attention(X, W_Q, W_K):\n        \"\"\"Computes the attention map for a single head.\"\"\"\n        d_k = W_Q.shape[1]\n        Q = X @ W_Q\n        K = X @ W_K\n        S = (Q @ K.T) / np.sqrt(d_k)\n        A = np.apply_along_axis(softmax_row, 1, S)\n        return A\n\n    def analyze_case(params):\n        \"\"\"\n        Performs the full analysis for a single test case.\n        Computes attention maps, regularizer value, collapse indicator,\n        and gradient norms.\n        \"\"\"\n        L, d_model, d_k, H, lambda_val, X, W_Q_list, W_K_list = params\n\n        # Compute attention maps and flatten them\n        attention_maps = [scaled_dot_product_attention(X, W_Q_list[h], W_K_list[h]) for h in range(H)]\n        flattened_attentions = [A.flatten() for A in attention_maps]\n\n        # Compute average pairwise cosine similarity\n        pairwise_similarities = []\n        if H  1:\n            num_pairs = H * (H - 1) / 2\n            for h in range(H):\n                for k in range(h + 1, H):\n                    a_h = flattened_attentions[h]\n                    a_k = flattened_attentions[k]\n                    norm_h = np.linalg.norm(a_h)\n                    norm_k = np.linalg.norm(a_k)\n                    if norm_h  0 and norm_k  0:\n                        sim = (a_h @ a_k) / (norm_h * norm_k)\n                        pairwise_similarities.append(sim)\n            \n            avg_similarity = sum(pairwise_similarities) / num_pairs if num_pairs  0 else 0.0\n        else:\n            avg_similarity = 0.0\n\n        # Check for collapse\n        collapse_indicator = avg_similarity = 0.99\n\n        # Compute gradients\n        grad_norms = []\n        if H  1:\n            C = lambda_val / (H * (H - 1) / 2)\n            norms = [np.linalg.norm(a) for a in flattened_attentions]\n\n            for p in range(H):\n                a_p = flattened_attentions[p]\n                norm_p = norms[p]\n\n                if norm_p == 0:\n                    grad_p = np.zeros_like(a_p)\n                else:\n                    sum_term1 = np.zeros_like(a_p)\n                    sum_term2 = 0.0\n                    for k in range(H):\n                        if k == p:\n                            continue\n                        a_k = flattened_attentions[k]\n                        norm_k = norms[k]\n                        \n                        if norm_k  0:\n                            sum_term1 += a_k / norm_k\n                            sum_term2 += (a_p @ a_k) / (norm_p * norm_k)\n                    \n                    term1 = (1 / norm_p) * sum_term1\n                    term2 = (a_p / (norm_p**2)) * sum_term2\n                    grad_p = C * (term1 - term2)\n                \n                grad_norms.append(np.linalg.norm(grad_p))\n        \n        return [avg_similarity, collapse_indicator, grad_norms]\n\n    # --- Test Case Definitions ---\n    # Case 1: Collapse via identical tokens\n    L1, d_model1, d_k1, H1, lambda1 = 3, 4, 2, 3, 0.5\n    X1 = np.array([[2, -1, 0.5, 3]] * 3, dtype=float)\n    W_Q1_list = [\n        np.array([[1, 0], [0, 1], [1, -1], [0, 2]], dtype=float),\n        np.array([[0, 1], [1, 0], [-1, 1], [2, 0]], dtype=float),\n        np.array([[1, -1], [0, 2], [1, 0], [-1, 1]], dtype=float)\n    ]\n    W_K1_list = [\n        np.array([[1, 1], [0, 1], [1, 0], [0, 1]], dtype=float),\n        np.array([[1, 0], [1, -1], [0, 2], [1, 0]], dtype=float),\n        np.array([[2, 0], [0, 1], [1, -1], [0, 1]], dtype=float)\n    ]\n    case1 = (L1, d_model1, d_k1, H1, lambda1, X1, W_Q1_list, W_K1_list)\n\n    # Case 2: Diverse heads and tokens\n    L2, d_model2, d_k2, H2, lambda2 = 3, 4, 2, 3, 0.5\n    X2 = np.array([\n        [1, 0, 1, 0],\n        [0, 1, 0, 1],\n        [1, 1, 0, 0]\n    ], dtype=float)\n    W_Q2_list = [\n        np.array([[1, 0], [0, 1], [0, 1], [1, 0]], dtype=float),\n        np.array([[2, 0], [0, -1], [1, 1], [0, 1]], dtype=float),\n        np.array([[0, 1], [1, 0], [-1, 1], [1, -1]], dtype=float)\n    ]\n    W_K2_list = [\n        np.array([[1, 0], [1, 0], [0, 1], [0, 1]], dtype=float),\n        np.array([[-1, 0], [0, 2], [1, 0], [0, 1]], dtype=float),\n        np.array([[1, 1], [0, -1], [2, 0], [1, 0]], dtype=float)\n    ]\n    case2 = (L2, d_model2, d_k2, H2, lambda2, X2, W_Q2_list, W_K2_list)\n\n    # Case 3: Edge case: zero input\n    L3, d_model3, d_k3, H3, lambda3 = 3, 4, 2, 3, 0.5\n    X3 = np.zeros((3, 4), dtype=float)\n    case3 = (L3, d_model3, d_k3, H3, lambda3, X3, W_Q2_list, W_K2_list)\n    \n    test_cases = [case1, case2, case3]\n    \n    # --- Run Analysis and Format Output ---\n    results = []\n    for case in test_cases:\n        avg_sim, collapse, grad_norms_list = analyze_case(case)\n        \n        # Format for printing\n        r_str = f\"{avg_sim:.6f}\"\n        b_str = str(collapse)\n        grad_norms_str_list = [f'{g:.6f}' for g in grad_norms_list]\n        list_str = f\"[{','.join(grad_norms_str_list)}]\"\n        \n        # Manually assemble the final string to avoid extra quotes\n        # around the inner list string.\n        results.append(f\"[{r_str},{b_str},{list_str}]\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3195528"}]}