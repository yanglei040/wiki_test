{"hands_on_practices": [{"introduction": "To truly master the attention mechanism, we must look beyond the high-level APIs of deep learning frameworks and understand how information flows during training. This practice delves into the core mechanics of backpropagation by having you implement the Vector-Jacobian Product (VJP) for a causal self-attention layer from first principles. By manually deriving and coding the gradient calculations, you will gain a profound understanding of how causal masks enforce the arrow of time, ensuring that the model cannot \"cheat\" by looking into the future [@problem_id:3100434].", "problem": "Implement a reverse-mode automatic differentiation (AD) Vector–Jacobian Product (VJP) for a single-head scaled dot-product attention mechanism with causal masking. The goal is to demonstrate, from first principles, that backpropagation through the attention with a causal mask does not propagate gradients from a token at time index $t$ into parameters associated with future tokens at indices $j$ with $j  t$. Use the following formal setting and requirements.\n\nFundamental base and definitions:\n- Let $L$ denote the sequence length and let $d$ denote the dimensionality of queries and keys. Let $d_v$ denote the dimensionality of values.\n- Let $\\mathbf{Q} \\in \\mathbb{R}^{L \\times d}$, $\\mathbf{K} \\in \\mathbb{R}^{L \\times d}$, and $\\mathbf{V} \\in \\mathbb{R}^{L \\times d_v}$ denote the query, key, and value matrices, respectively.\n- Define the scaled dot-product score matrix by\n$$\n\\mathbf{S} = \\frac{1}{\\sqrt{d}} \\, \\mathbf{Q} \\, \\mathbf{K}^\\top \\in \\mathbb{R}^{L \\times L}.\n$$\n- Define the causal mask $\\mathbf{M} \\in \\{0,1\\}^{L \\times L}$ by $\\mathbf{M}_{i,j} = 1$ if and only if $j \\le i$, and $\\mathbf{M}_{i,j} = 0$ otherwise.\n- Construct the masked scores $\\widetilde{\\mathbf{S}}$ by setting $\\widetilde{\\mathbf{S}}_{i,j} = \\mathbf{S}_{i,j}$ for $j \\le i$ and $\\widetilde{\\mathbf{S}}_{i,j} = -\\infty$ for $j  i$.\n- Let $\\operatorname{softmax}$ denote the softmax function applied row-wise. Define the attention weights by\n$$\n\\mathbf{A} = \\operatorname{softmax}(\\widetilde{\\mathbf{S}}) \\in \\mathbb{R}^{L \\times L},\n$$\nand the attention output by\n$$\n\\mathbf{Y} = \\mathbf{A} \\, \\mathbf{V} \\in \\mathbb{R}^{L \\times d_v}.\n$$\n\nTask and constraints:\n- Implement a complete, runnable program that:\n  - Computes the forward attention $\\mathbf{Y}$ with the causal mask as defined.\n  - Implements the reverse-mode AD Vector–Jacobian Product (VJP) that maps an upstream cotangent $\\mathbf{G_Y} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}} \\in \\mathbb{R}^{L \\times d_v}$ to downstream cotangents $(\\mathbf{G_Q}, \\mathbf{G_K}, \\mathbf{G_V}) = \\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Q}}, \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}}, \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{V}} \\right)$, using only the chain rule, the definition of the softmax function, and linear algebra identities. Do not use any external automatic differentiation frameworks; rely only on the fundamental rules of differentiation and linear algebra to derive and implement the VJP formulas.\n  - Uses numerically stable softmax computations by subtracting the row-wise maximum before exponentiation.\n- Testing objective:\n  - For each test, set the upstream cotangent $\\mathbf{G_Y}$ to be all zeros except a single nonzero entry at a chosen time index $t$ and value channel $c$, namely $\\mathbf{G_Y}_{t, c} = 1$ and all other entries equal to $0$.\n  - Verify that the VJP computed gradients $\\mathbf{G_K}$ and $\\mathbf{G_V}$ are zero for all future token rows $j$ with $j  t$ under causal masking. Use a numerical tolerance of $\\varepsilon = 10^{-12}$ for the zero check.\n- Angle units are not applicable to this task.\n- There are no physical units in this problem.\n\nTest suite and parameter values:\n- Use the following deterministic test cases, each specified by $(L, d, d_v, t, c, s, \\text{causal})$, where $s$ is a random seed for constructing inputs, and $\\text{causal}$ is a boolean indicating whether to use the causal mask.\n  - Test $1$: $(L, d, d_v, t, c, s, \\text{causal}) = (4, 3, 2, 1, 0, 0, \\text{True})$.\n  - Test $2$: $(L, d, d_v, t, c, s, \\text{causal}) = (1, 3, 2, 0, 1, 1, \\text{True})$.\n  - Test $3$: $(L, d, d_v, t, c, s, \\text{causal}) = (5, 4, 3, 4, 2, 2, \\text{True})$.\n  - Test $4$ (control without masking): $(L, d, d_v, t, c, s, \\text{causal}) = (4, 3, 2, 1, 1, 3, \\text{False})$.\n- For each test case, construct $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ by sampling independent standard normal entries using the given seed $s$ for reproducibility, and then scaling by a factor of $0.5$ for numerical moderation. Formally, set the pseudorandom number generator seed to $s$, sample $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ with independent entries from $\\mathcal{N}(0,1)$, and then replace each sampled matrix by $0.5$ times that matrix.\n- For each test, define the upstream cotangent $\\mathbf{G_Y}$ as the zero matrix in $\\mathbb{R}^{L \\times d_v}$ except for $\\mathbf{G_Y}_{t,c} = 1$.\n- For each test, the program must evaluate whether, under causal masking, gradients to future tokens are zero by computing the boolean predicate\n$$\n\\max_{j  t} \\left( \\max\\left( \\left| \\mathbf{G_K}[j,:] \\right| \\right), \\, \\max\\left( \\left| \\mathbf{G_V}[j,:] \\right| \\right) \\right) \\le \\varepsilon,\n$$\nwhere $\\varepsilon = 10^{-12}$. If the set $\\{ j : j  t \\}$ is empty (for example, when $t = L - 1$), treat the predicate as true.\n- The final output format must be a single line containing the boolean results for the $4$ tests as a comma-separated list enclosed in square brackets, for example $[\\text{True},\\text{False},\\text{True},\\text{True}]$.\n\nYour program must produce exactly one line of output in the specified format and must not read any input.", "solution": "The problem requires the implementation of a reverse-mode automatic differentiation (AD) Vector-Jacobian Product (VJP) for a single-head scaled dot-product attention mechanism, with a specific focus on demonstrating the effect of causal masking on gradient flow. We must validate that gradients from an output token at time $t$ do not propagate to input parameters associated with future tokens $j  t$.\n\nFirst, we will derive the VJP equations from first principles by applying the chain rule to the sequence of operations in the forward pass. Let $\\mathcal{L}$ be a scalar loss function. The VJP for a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ maps the cotangent (gradient) with respect to its output, $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\in \\mathbb{R}^m$, to the cotangent with respect to its input, $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}} \\in \\mathbb{R}^n$. We are given the upstream cotangent $\\mathbf{G_Y} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}}$ and must compute $\\mathbf{G_Q} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Q}}$, $\\mathbf{G_K} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}}$, and $\\mathbf{G_V} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{V}}$.\n\nThe forward pass is defined by the following sequence of operations:\n1.  Scaled dot-product scores: $\\mathbf{S} = \\frac{1}{\\sqrt{d}} \\mathbf{Q} \\mathbf{K}^\\top$\n2.  Masking: $\\widetilde{\\mathbf{S}} = \\operatorname{mask}(\\mathbf{S}, \\mathbf{M})$ (where masked elements are set to $-\\infty$)\n3.  Row-wise softmax: $\\mathbf{A} = \\operatorname{softmax}(\\widetilde{\\mathbf{S}})$\n4.  Output computation: $\\mathbf{Y} = \\mathbf{A} \\mathbf{V}$\n\nWe will derive the VJPs by reversing this sequence.\n\n**Step 1: VJP for Output Computation $\\mathbf{Y} = \\mathbf{A} \\mathbf{V}$**\n\nThis operation is a standard matrix multiplication. We are given $\\mathbf{G_Y} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}}$. Using the VJP rules for matrix multiplication ($C = AB \\implies \\mathbf{G_A} = \\mathbf{G_C} B^\\top, \\mathbf{G_B} = A^\\top \\mathbf{G_C}$), we find the cotangents for $\\mathbf{A}$ and $\\mathbf{V}$:\n$$\n\\mathbf{G_A} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{A}} = \\mathbf{G_Y} \\mathbf{V}^\\top\n$$\n$$\n\\mathbf{G_V} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{V}} = \\mathbf{A}^\\top \\mathbf{G_Y}\n$$\n\n**Step 2: VJP for Row-wise Softmax $\\mathbf{A} = \\operatorname{softmax}(\\widetilde{\\mathbf{S}})$**\n\nThe softmax function is applied to each row of $\\widetilde{\\mathbf{S}}$. For a single row vector $\\mathbf{a} = \\operatorname{softmax}(\\mathbf{s})$, the VJP mapping $\\mathbf{g_a}$ to $\\mathbf{g_s}$ is derived from the Jacobian of the softmax function, $J_{jk} = \\frac{\\partial a_j}{\\partial s_k} = a_j(\\delta_{jk} - a_k)$. The VJP is:\n$$\n(\\mathbf{g_s})_k = \\sum_j (\\mathbf{g_a})_j J_{jk} = \\sum_j (\\mathbf{g_a})_j a_j(\\delta_{jk} - a_k) = (\\mathbf{g_a})_k a_k - a_k \\sum_j (\\mathbf{g_a})_j a_j\n$$\nIn vector notation, this is $\\mathbf{g_s} = \\mathbf{a} \\odot (\\mathbf{g_a} - (\\mathbf{g_a} \\cdot \\mathbf{a}))$, where $\\odot$ is the element-wise product and $\\cdot$ is the dot product. Applying this to each row $i$ of the matrices $\\mathbf{A}$ and $\\mathbf{G_A}$:\n$$\n(\\mathbf{G}_{\\widetilde{\\mathbf{S}}})_{i,:} = \\mathbf{A}_{i,:} \\odot \\left( \\mathbf{G_A}_{i,:} - \\left( \\sum_{k=1}^L \\mathbf{A}_{i,k} \\mathbf{G_A}_{i,k} \\right) \\right)\n$$\nThis computes $\\mathbf{G}_{\\widetilde{\\mathbf{S}}} = \\frac{\\partial \\mathcal{L}}{\\partial \\widetilde{\\mathbf{S}}}$ from $\\mathbf{G_A}$.\n\n**Step 3: VJP for Masking $\\widetilde{\\mathbf{S}} = \\operatorname{mask}(\\mathbf{S}, \\mathbf{M})$**\n\nIn the forward pass, if $\\mathbf{M}_{i,j} = 0$ (i.e., $j  i$), then $\\widetilde{\\mathbf{S}}_{i,j}$ is set to the constant $-\\infty$. If $\\mathbf{M}_{i,j} = 1$ (i.e., $j \\le i$), then $\\widetilde{\\mathbf{S}}_{i,j} = \\mathbf{S}_{i,j}$. The gradient is passed backward accordingly:\n$$\n(\\mathbf{G_S})_{i,j} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{S}_{i,j}} = \\frac{\\partial \\mathcal{L}}{\\partial \\widetilde{\\mathbf{S}}_{i,j}} \\frac{\\partial \\widetilde{\\mathbf{S}}_{i,j}}{\\partial \\mathbf{S}_{i,j}}\n$$\nThe derivative $\\frac{\\partial \\widetilde{\\mathbf{S}}_{i,j}}{\\partial \\mathbf{S}_{i,j}}$ is $1$ if $j \\le i$ and $0$ if $j  i$. Therefore, the gradient only flows through the unmasked elements:\n$$\n(\\mathbf{G_S})_{i,j} = \\begin{cases} (\\mathbf{G}_{\\widetilde{\\mathbf{S}}})_{i,j}  \\text{if } j \\le i \\\\ 0  \\text{if } j  i \\end{cases}\n$$\nIf causal masking is disabled, $\\mathbf{G_S} = \\mathbf{G}_{\\widetilde{\\mathbf{S}}}$.\n\n**Step 4: VJP for Scaled Dot-Product $\\mathbf{S} = \\frac{1}{\\sqrt{d}} \\mathbf{Q} \\mathbf{K}^\\top$**\n\nThis step is again a matrix multiplication, scaled by a constant $\\frac{1}{\\sqrt{d}}$. Applying the VJP rules:\n$$\n\\mathbf{G_Q} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Q}} = \\frac{1}{\\sqrt{d}} \\mathbf{G_S} (\\mathbf{K}^\\top)^\\top = \\frac{1}{\\sqrt{d}} \\mathbf{G_S} \\mathbf{K}\n$$\nFor $\\mathbf{K}$, we first find the gradient with respect to $\\mathbf{K}^\\top$ and then transpose the result.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}^\\top} = \\frac{1}{\\sqrt{d}} \\mathbf{Q}^\\top \\mathbf{G_S}\n$$\n$$\n\\mathbf{G_K} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}} = \\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}^\\top} \\right)^\\top = \\frac{1}{\\sqrt{d}} (\\mathbf{Q}^\\top \\mathbf{G_S})^\\top = \\frac{1}{\\sqrt{d}} \\mathbf{G_S}^\\top \\mathbf{Q}\n$$\n\n**Analysis of Causal Gradient Flow**\nThe testing objective is to verify that for an upstream cotangent $\\mathbf{G_Y}$ that is non-zero only at row $t$, the downstream cotangents $\\mathbf{G_K}$ and $\\mathbf{G_V}$ are zero for all rows $j  t$.\n\n1.  **Gradient to $\\mathbf{V}$**: $\\mathbf{G_V} = \\mathbf{A}^\\top \\mathbf{G_Y}$. The $j$-th row of $\\mathbf{G_V}$ is $(\\mathbf{G_V})_{j,:} = \\sum_{i=0}^{L-1} (\\mathbf{A}^\\top)_{j,i} (\\mathbf{G_Y})_{i,:} = \\sum_{i=0}^{L-1} \\mathbf{A}_{i,j} (\\mathbf{G_Y})_{i,:}$. Since $\\mathbf{G_Y}$ is non-zero only at row $t$, this sum reduces to $(\\mathbf{G_V})_{j,:} = \\mathbf{A}_{t,j} (\\mathbf{G_Y})_{t,:}$. With causal masking, the attention weights matrix $\\mathbf{A}$ is lower triangular, meaning $\\mathbf{A}_{i,j} = 0$ for $j  i$. Thus, for our case where $i=t$, we have $\\mathbf{A}_{t,j} = 0$ for all $j  t$. Consequently, $(\\mathbf{G_V})_{j,:}$ is a zero vector for all $j  t$.\n\n2.  **Gradient to $\\mathbf{K}$**: Tracing the gradient propagation to $\\mathbf{K}$ reveals a similar logic.\n    - $\\mathbf{G_A} = \\mathbf{G_Y} \\mathbf{V}^\\top$. Since $\\mathbf{G_Y}$ has only row $t$ non-zero, $\\mathbf{G_A}$ will also have only row $t$ non-zero.\n    - $\\mathbf{G}_{\\widetilde{\\mathbf{S}}}$ is computed from $\\mathbf{G_A}$, so it also will have only row $t$ non-zero.\n    - $\\mathbf{G_S}$ is computed from $\\mathbf{G}_{\\widetilde{\\mathbf{S}}}$ by applying the causal mask. This means $(\\mathbf{G_S})_{i,j} = 0$ for all $i \\ne t$, and also $(\\mathbf{G_S})_{t,j} = 0$ for all $j  t$. In summary, $\\mathbf{G_S}$ is a matrix with non-zero entries existing only at indices $(t,j)$ where $j \\le t$.\n    - Finally, $\\mathbf{G_K} = \\frac{1}{\\sqrt{d}} \\mathbf{G_S}^\\top \\mathbf{Q}$. The $j$-th row of $\\mathbf{G_K}$ is given by $(\\mathbf{G_K})_{j,:} = \\frac{1}{\\sqrt{d}} \\sum_{i=0}^{L-1} (\\mathbf{G_S}^\\top)_{j,i} \\mathbf{Q}_{i,:} = \\frac{1}{\\sqrt{d}} \\sum_{i=0}^{L-1} (\\mathbf{G_S})_{i,j} \\mathbf{Q}_{i,:}$.\n    - Since $(\\mathbf{G_S})_{i,j}$ is non-zero only for $i=t$, this becomes $(\\mathbf{G_K})_{j,:} = \\frac{1}{\\sqrt{d}} (\\mathbf{G_S})_{t,j} \\mathbf{Q}_{t,:}$.\n    - For any future token $j  t$, we know from the structure of $\\mathbf{G_S}$ that $(\\mathbf{G_S})_{t,j} = 0$. Therefore, $(\\mathbf{G_K})_{j,:}$ must be a zero vector for all $j  t$.\n\nThis confirms from first principles that causal masking correctly prevents information from a given time step from flowing backward to influence the parameters of future time steps. The implementation will codify these derived VJP equations and verify this property numerically.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests the Vector-Jacobian Product (VJP) for scaled\n    dot-product attention with and without causal masking, verifying the\n    gradient flow properties of causal attention.\n    \"\"\"\n    \n    # Test suite format: (L, d, d_v, t, c, s, causal)\n    # L: sequence length, d: query/key dim, d_v: value dim,\n    # t: time index for gradient, c: channel index for gradient,\n    # s: random seed, causal: boolean for causal mask.\n    test_cases = [\n        (4, 3, 2, 1, 0, 0, True),\n        (1, 3, 2, 0, 1, 1, True),\n        (5, 4, 3, 4, 2, 2, True),\n        (4, 3, 2, 1, 1, 3, False),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        L, d, d_v, t, c, s, causal = case\n\n        # 1. Initialize inputs\n        rng = np.random.default_rng(s)\n        Q = rng.standard_normal((L, d), dtype=np.float64) * 0.5\n        K = rng.standard_normal((L, d), dtype=np.float64) * 0.5\n        V = rng.standard_normal((L, d_v), dtype=np.float64) * 0.5\n\n        # 2. Forward Pass\n        \n        # S = (Q @ K.T) / sqrt(d)\n        scores = (Q @ K.T) / np.sqrt(d)\n        \n        # Apply causal mask if specified\n        masked_scores = np.copy(scores)\n        if causal:\n            mask = np.triu(np.ones((L, L), dtype=bool), k=1)\n            masked_scores[mask] = -np.inf\n\n        # Numerically stable softmax\n        stable_scores = masked_scores - np.max(masked_scores, axis=1, keepdims=True)\n        A = np.exp(stable_scores)\n        A /= np.sum(A, axis=1, keepdims=True)\n        \n        # Y = A @ V\n        _Y = A @ V\n\n        # 3. Backward Pass (VJP)\n\n        # Initialize upstream gradient G_Y\n        G_Y = np.zeros((L, d_v), dtype=np.float64)\n        G_Y[t, c] = 1.0\n\n        # Backprop through Y = A @ V\n        # G_A = G_Y @ V.T\n        # G_V = A.T @ G_Y\n        G_A = G_Y @ V.T\n        G_V = A.T @ G_Y\n\n        # Backprop through A = softmax(S_tilde)\n        # G_S_tilde = A * (G_A - sum(G_A * A, axis=1))\n        row_wise_dot = np.sum(G_A * A, axis=1, keepdims=True)\n        G_S_tilde = A * (G_A - row_wise_dot)\n\n        # Backprop through masking\n        G_S = np.copy(G_S_tilde)\n        if causal:\n            # Gradient is zero for masked-out elements\n            mask = np.triu(np.ones((L, L), dtype=bool), k=1)\n            G_S[mask] = 0.0\n\n        # Backprop through S = (Q @ K.T) / sqrt(d)\n        # G_Q = (G_S @ K) / sqrt(d)\n        # G_K = (G_S.T @ Q) / sqrt(d)\n        sqrt_d = np.sqrt(d)\n        G_Q = (G_S @ K) / sqrt_d\n        G_K = (G_S.T @ Q) / sqrt_d\n\n        # 4. Verification\n        \n        # Check if gradients to future tokens are zero.\n        # This is vacuously true if t is the last token index.\n        if t >= L - 1:\n            predicate_holds = True\n        else:\n            # Extract gradients corresponding to future tokens (j > t)\n            G_K_future = G_K[t + 1 :, :]\n            G_V_future = G_V[t + 1 :, :]\n\n            # Compute max absolute value of these future gradients\n            max_grad_k = np.max(np.abs(G_K_future))\n            max_grad_v = np.max(np.abs(G_V_future))\n            \n            # Check if the maximum is below the tolerance\n            tolerance = 1e-12\n            predicate_holds = max(max_grad_k, max_grad_v) = tolerance\n            \n        results.append(predicate_holds)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3100434"}, {"introduction": "The behavior of the attention mechanism, particularly its \"sharpness\" or \"focus,\" is critically controlled by the temperature parameter $\\tau$ within the softmax function. This exercise explores the deep connection between soft attention and hard selection by examining the limit as $\\tau \\to 0$, where softmax converges to an $\\operatorname{argmax}$ operation. Through a hands-on simulation, you will see how this theoretical property manifests as \"brittleness\" in models and how tuning the temperature can create more robust and stable behavior by smoothing the attention distribution [@problem_id:3100390].", "problem": "Consider an attention mechanism used in Transformer models based on Scaled Dot-Product Attention (SDPA). Let there be a query vector $q \\in \\mathbb{R}^d$, a set of key vectors $\\{k_i\\}_{i=1}^n \\subset \\mathbb{R}^d$, and a corresponding set of value vectors $\\{v_i\\}_{i=1}^n \\subset \\mathbb{R}^m$. Define the score of key $k_i$ as $s_i = q^\\top k_i$. For a temperature parameter $\\tau  0$, define the softmax weights $w_i(\\tau)$ by normalizing exponentials of the scores, and define the attention output $a(\\tau)$ as the weighted sum of values using these weights. Define the argmax attention output $a^\\star$ by selecting the value corresponding to the index $i^\\star$ that maximizes the score. Starting only from the fundamental definitions of the exponential function, limits, and normalization by sums, derive the limiting relationship between $a(\\tau)$ and $a^\\star$ as $\\tau \\to 0$, and analyze the brittleness of argmax selection under small perturbations of $q$.\n\nYour task is to write a complete, runnable program that performs the following computations on a specified test suite and produces a single line of output in the exact format described below. No physical units, angle units, or percentages are involved; all outputs must be numeric floats, integers, booleans, or lists thereof.\n\nDefinitions to use:\n- Softmax weights: $w_i(\\tau) = \\dfrac{\\exp\\!\\left(s_i/\\tau\\right)}{\\sum_{j=1}^n \\exp\\!\\left(s_j/\\tau\\right)}$, where $s_i = q^\\top k_i$.\n- Softmax attention output: $a(\\tau) = \\sum_{i=1}^n w_i(\\tau) v_i$.\n- Argmax index: $i^\\star = \\operatorname{argmax}_{1 \\leq i \\leq n} s_i$ (assume an arbitrary but fixed tie-breaking rule).\n- Argmax attention output: $a^\\star = v_{i^\\star}$.\n\nTest suite specification:\n\n- Test Case $1$ (unique maximum, approximation quality across temperatures):\n  - Dimension $d = 2$, value dimension $m = 2$.\n  - Query $q = (1.0, 0.2)$.\n  - Keys $k_1 = (1.0, 0.0)$, $k_2 = (0.5, 0.7)$, $k_3 = (-0.5, 0.3)$, $k_4 = (0.1, 0.2)$.\n  - Values $v_1 = (1.0, 0.0)$, $v_2 = (0.0, 1.0)$, $v_3 = (1.0, 1.0)$, $v_4 = (-1.0, 1.0)$.\n  - Temperatures $\\tau \\in \\{1.0, 0.5, 0.1, 0.01, 10^{-6}\\}$.\n  - For each $\\tau$, compute the Euclidean norm $\\|a(\\tau) - a^\\star\\|_2$ and return these as a list of floats ordered by $\\tau$.\n\n- Test Case $2$ (near-tie brittleness under small query perturbations and smoothing by temperature):\n  - Dimension $d = 2$, value dimension $m = 2$.\n  - Base query $q_0 = (1.0, 0.0)$, perturbed queries $q_+ = (1.0, \\epsilon)$ and $q_- = (1.0, -\\epsilon)$ with $\\epsilon = 10^{-3}$.\n  - Keys $k_1 = (1.0, 0.0)$, $k_2 = (1.0, \\eta)$ with $\\eta = 1000.0$.\n  - Values $v_1 = (1.0, 0.0)$, $v_2 = (0.0, 1.0)$.\n  - Temperatures for comparison $\\tau \\in \\{0.5, 0.1, 0.01\\}$.\n  - Compute:\n    - The integer indicator of argmax flip under perturbation, defined as $I = 1$ if $\\operatorname{argmax}_i q_+^\\top k_i \\neq \\operatorname{argmax}_i q_-^\\top k_i$, and $I = 0$ otherwise.\n    - For each specified $\\tau$, compute the Euclidean distance $\\|a_{+}(\\tau) - a_{-}(\\tau)\\|_2$ between the two softmax attention outputs for $q_+$ and $q_-$.\n  - Return a list containing $I$ followed by the three distances ordered by the listed $\\tau$ values.\n\n- Test Case $3$ (exact tie, symmetry of softmax weights):\n  - Dimension $d = 2$, value dimension $m = 2$.\n  - Query $q = (1.0, 0.0)$.\n  - Keys $k_1 = (1.0, 0.0)$, $k_2 = (1.0, 0.0)$, $k_3 = (0.0, 1.0)$ (so $k_1$ and $k_2$ tie exactly).\n  - Values $v_1 = (1.0, 0.0)$, $v_2 = (0.0, 1.0)$, $v_3 = (1.0, 1.0)$.\n  - Temperature $\\tau = 0.1$.\n  - Compute the softmax weights $w_1(\\tau)$ and $w_2(\\tau)$ and return the float $|w_1(\\tau) - w_2(\\tau)|$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result appearing in order as described:\n  - The first element is the list of floats from Test Case $1$.\n  - The second element is the list from Test Case $2$ containing one integer followed by three floats.\n  - The third element is the single float from Test Case $3$.\n- For example, the final line should look like: $[[x_1,x_2,x_3,x_4,x_5],[i,d_1,d_2,d_3],y]$, where each $x_j$, $i$, $d_j$, and $y$ are the computed numeric results for the specified test suite.", "solution": "The problem is well-posed, scientifically grounded, and internally consistent. It provides all necessary definitions and data for a unique and meaningful solution. We proceed with the derivation and computational implementation.\n\nThe analysis is structured in two parts. First, we derive the limiting behavior of the softmax attention output, $a(\\tau)$, as the temperature parameter $\\tau$ approaches zero. Second, we analyze the stability of the attention mechanism under small perturbations, contrasting the argmax selection with the temperate softmax.\n\n**1. Limiting Behavior of Softmax Attention as $\\tau \\to 0^+$**\n\nThe softmax attention output is defined as a weighted sum of value vectors:\n$$ a(\\tau) = \\sum_{i=1}^n w_i(\\tau) v_i $$\nwhere the weights $w_i(\\tau)$ are given by the temperature-scaled softmax function applied to the scores $s_i = q^\\top k_i$:\n$$ w_i(\\tau) = \\frac{\\exp(s_i/\\tau)}{\\sum_{j=1}^n \\exp(s_j/\\tau)} $$\nWe wish to compute the limit $\\lim_{\\tau \\to 0^+} a(\\tau)$. The analysis hinges on the behavior of the weights $w_i(\\tau)$ in this limit.\n\nLet $s_{\\max} = \\max_{1 \\leq j \\leq n} s_j$ be the maximum score. To avoid numerical overflow and facilitate the limit calculation, we can factor out the term $\\exp(s_{\\max}/\\tau)$ from the numerator and denominator:\n$$ w_i(\\tau) = \\frac{\\exp(s_{\\max}/\\tau) \\exp((s_i - s_{\\max})/\\tau)}{\\exp(s_{\\max}/\\tau) \\sum_{j=1}^n \\exp((s_j - s_{\\max})/\\tau)} = \\frac{\\exp((s_i - s_{\\max})/\\tau)}{\\sum_{j=1}^n \\exp((s_j - s_{\\max})/\\tau)} $$\nNow, we examine the limit of the exponential term $\\exp((s_j - s_{\\max})/\\tau)$ as $\\tau \\to 0^+$. The behavior depends on whether $s_j$ is equal to $s_{\\max}$.\n\nCase 1: $s_j = s_{\\max}$. In this case, the exponent is $(s_j - s_{\\max})/\\tau = 0/\\tau = 0$. Thus, $\\exp((s_j - s_{\\max})/\\tau) = \\exp(0) = 1$.\n\nCase 2: $s_j  s_{\\max}$. Here, the difference $s_j - s_{\\max}$ is a strictly negative constant. As $\\tau \\to 0^+$, the exponent $(s_j - s_{\\max})/\\tau \\to -\\infty$. Consequently, $\\lim_{\\tau \\to 0^+} \\exp((s_j - s_{\\max})/\\tau) = 0$.\n\nLet $I_{\\max} = \\{i \\mid s_i = s_{\\max}\\}$ be the set of indices corresponding to the maximum score, and let $M = |I_{\\max}|$ be the number of such indices. We can now evaluate the limit of the denominator of $w_i(\\tau)$:\n$$ \\lim_{\\tau \\to 0^+} \\sum_{j=1}^n \\exp((s_j - s_{\\max})/\\tau) = \\sum_{j \\in I_{\\max}} \\lim_{\\tau \\to 0^+} \\exp((s_j - s_{\\max})/\\tau) + \\sum_{j \\notin I_{\\max}} \\lim_{\\tau \\to 0^+} \\exp((s_j - s_{\\max})/\\tau) $$\n$$ = \\sum_{j \\in I_{\\max}} 1 + \\sum_{j \\notin I_{\\max}} 0 = M $$\nThe limit of the numerator $\\exp((s_i - s_{\\max})/\\tau)$ is $1$ if $i \\in I_{\\max}$ and $0$ if $i \\notin I_{\\max}$.\n\nCombining these results, the limit of the weight $w_i(\\tau)$ is:\n$$ \\lim_{\\tau \\to 0^+} w_i(\\tau) = \\begin{cases} 1/M  \\text{if } i \\in I_{\\max} \\\\ 0  \\text{if } i \\notin I_{\\max} \\end{cases} $$\nThis demonstrates that as $\\tau \\to 0^+$, the softmax function concentrates all of its probability mass on the indices that achieve the maximum score, distributing it uniformly among them. It effectively becomes a \"soft argmax\".\n\nNow, we can find the limit of the attention output $a(\\tau)$:\n$$ \\lim_{\\tau \\to 0^+} a(\\tau) = \\lim_{\\tau \\to 0^+} \\sum_{i=1}^n w_i(\\tau) v_i = \\sum_{i=1}^n \\left(\\lim_{\\tau \\to 0^+} w_i(\\tau)\\right) v_i $$\nSubstituting the limiting weights:\n$$ \\lim_{\\tau \\to 0^+} a(\\tau) = \\sum_{i \\in I_{\\max}} \\frac{1}{M} v_i + \\sum_{i \\notin I_{\\max}} 0 \\cdot v_i = \\frac{1}{M} \\sum_{i \\in I_{\\max}} v_i $$\nThe limiting attention output is the arithmetic mean of the value vectors whose corresponding keys achieved the maximum score.\n\nThe problem defines the argmax attention output as $a^\\star = v_{i^\\star}$, where $i^\\star = \\operatorname{argmax}_{i} s_i$ with a fixed tie-breaking rule. If the maximum score is unique, then $I_{\\max} = \\{i^\\star\\}$ and $M=1$. In this common scenario, the limit simplifies to:\n$$ \\lim_{\\tau \\to 0^+} a(\\tau) = \\frac{1}{1} v_{i^\\star} = v_{i^\\star} = a^\\star $$\nThus, when the maximum score is unique, the softmax attention output converges to the argmax attention output.\n\n**2. Brittleness of Argmax Selection and the Regularizing Role of Temperature**\n\nThe argmax selection is inherently discontinuous. The choice of $i^\\star$ depends on the ordering of the scores $s_i(q) = q^\\top k_i$. A small perturbation to the query, $q' = q + \\delta q$, induces a change in the scores: $s_i(q') = s_i(q) + (\\delta q)^\\top k_i$. If two scores, say $s_j(q)$ and $s_k(q)$, are very close, even an infinitesimal perturbation $\\delta q$ can alter their relative order, causing $i^\\star$ to jump from one index to another. This results in a discrete change in the output $a^\\star$, from $v_j$ to $v_k$, which can be substantial. This instability is a form of \"brittleness\".\n\nIn contrast, for any strictly positive temperature $\\tau  0$, the softmax attention output $a(\\tau)$ is a continuous and differentiable function of the query $q$. The weights $w_i(\\tau)$ are smooth functions of the scores $s_i$, which are linear in $q$. Consequently, a small change $\\delta q$ in the query leads to a correspondingly small change in the output $a(\\tau)$. The change is governed by the gradient: $a(q+\\delta q, \\tau) \\approx a(q, \\tau) + (\\nabla_q a(q, \\tau)) \\cdot \\delta q$.\n\nA higher temperature $\\tau$ has a smoothing or regularizing effect. It \"flattens\" the softmax distribution, meaning the weights $w_i(\\tau)$ are less sensitive to small differences in scores. This makes the output $a(\\tau)$ more robust to minor perturbations in $q$. Conversely, as $\\tau \\to 0$, the softmax function becomes steeper, and the gradient of $a(\\tau)$ with respect to $q$ can become very large, particularly in directions that affect the difference between near-maximum scores. In this regime, the smooth softmax output $a(\\tau)$ closely approximates the brittle, discontinuous behavior of the argmax output $a^\\star$, exhibiting high sensitivity to small perturbations that can flip the ordering of top scores, as demonstrated in Test Case $2$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the three test cases specified in the problem statement regarding\n    Scaled Dot-Product Attention.\n    \"\"\"\n\n    def softmax_attention_with_weights(q, K, V, tau):\n        \"\"\"\n        Computes the softmax attention output and corresponding weights.\n\n        Args:\n            q (np.ndarray): Query vector of shape (d,).\n            K (np.ndarray): Key matrix of shape (n, d).\n            V (np.ndarray): Value matrix of shape (n, m).\n            tau (float): Temperature parameter.\n\n        Returns:\n            tuple: A tuple containing:\n                - np.ndarray: Attention output vector of shape (m,).\n                - np.ndarray: Softmax weights vector of shape (n,).\n        \"\"\"\n        scores = K @ q\n        # Scale scores by temperature as per the definition.\n        scaled_scores = scores / tau\n        # Use the max-subtraction trick for numerical stability of exp.\n        stable_scores = scaled_scores - np.max(scaled_scores)\n        exps = np.exp(stable_scores)\n        weights = exps / np.sum(exps)\n        # Attention output is the weighted sum of value vectors.\n        # V.T is (m, n), weights is (n,). Result is (m,).\n        attention_output = V.T @ weights\n        return attention_output, weights\n\n    # --- Test Case 1: Unique maximum, approximation quality ---\n    q1 = np.array([1.0, 0.2])\n    K1 = np.array([[1.0, 0.0], [0.5, 0.7], [-0.5, 0.3], [0.1, 0.2]])\n    V1 = np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0], [-1.0, 1.0]])\n    taus1 = [1.0, 0.5, 0.1, 0.01, 1e-6]\n    \n    scores1 = K1 @ q1\n    # np.argmax respects the \"fixed tie-breaking rule\" by taking the first occurrence.\n    i_star1 = np.argmax(scores1)\n    a_star1 = V1[i_star1]\n    \n    results1 = []\n    for tau in taus1:\n        a_tau, _ = softmax_attention_with_weights(q1, K1, V1, tau)\n        error = np.linalg.norm(a_tau - a_star1)\n        results1.append(error)\n\n    # --- Test Case 2: Near-tie brittleness ---\n    epsilon = 1e-3\n    q_plus = np.array([1.0, epsilon])\n    q_minus = np.array([1.0, -epsilon])\n    eta = 1000.0\n    K2 = np.array([[1.0, 0.0], [1.0, eta]])\n    V2 = np.array([[1.0, 0.0], [0.0, 1.0]])\n    taus2 = [0.5, 0.1, 0.01]\n    \n    scores_plus = K2 @ q_plus\n    scores_minus = K2 @ q_minus\n    argmax_plus = np.argmax(scores_plus)\n    argmax_minus = np.argmax(scores_minus)\n    I = 1 if argmax_plus != argmax_minus else 0\n    \n    results2 = [I]\n    for tau in taus2:\n        a_plus_tau, _ = softmax_attention_with_weights(q_plus, K2, V2, tau)\n        a_minus_tau, _ = softmax_attention_with_weights(q_minus, K2, V2, tau)\n        dist = np.linalg.norm(a_plus_tau - a_minus_tau)\n        results2.append(dist)\n        \n    # --- Test Case 3: Exact tie, symmetry of weights ---\n    q3 = np.array([1.0, 0.0])\n    K3 = np.array([[1.0, 0.0], [1.0, 0.0], [0.0, 1.0]])\n    # V3 is not strictly needed as we only compute weights.\n    # Pass a dummy V matrix of the correct shape.\n    V3 = np.zeros((3, 2))\n    tau3 = 0.1\n    \n    _, weights3 = softmax_attention_with_weights(q3, K3, V3, tau3)\n    w1_tau = weights3[0]\n    w2_tau = weights3[1]\n    result3 = abs(w1_tau - w2_tau)\n    \n    # --- Format and print the final output ---\n    # The output format is a single line string `[[...],[...],...]`.\n    str_res1 = f\"[{','.join(map(str, results1))}]\"\n    str_res2 = f\"[{','.join(map(str, results2))}]\"\n    str_res3 = str(result3)\n    \n    print(f\"[{str_res1},{str_res2},{str_res3}]\")\n\nsolve()\n```", "id": "3100390"}, {"introduction": "The power of attention extends far beyond simply weighting a sequence of values; it can be conceptualized as a general-purpose, differentiable routing network. This advanced practice reframes attention as the core of a Mixture-of-Experts (MoE) system, where the attention weights dynamically select which \"expert\" sub-networks to consult for a given input. You will explore how to encourage efficient, sparse routing by adding a budget regularizer based on the Shannon entropy of the attention distribution, a technique at the heart of modern, scalable large language models [@problem_id:3180926].", "problem": "You are given a differentiable routing view of the attention mechanism. Consider a set of $K$ experts. For each input sample, a routing network produces a vector of real-valued logits $z \\in \\mathbb{R}^K$, which is then normalized into a categorical distribution over experts to obtain attention $A \\in \\Delta^{K-1}$ (the $(K-1)$-simplex). Treat this attention as the routing distribution over experts. Outputs are discrete classes. For each expert, a predictive distribution over classes is provided implicitly by a parameterized reliability scheme described below. The model prediction is the mixture of expert predictions weighted by attention. A budget regularizer on Shannon entropy (SE) $H(A)$ encourages sparse routing subject to a budget weight. Your task is to implement this setup from first principles and evaluate it on modular addition classification tasks, computing an average unregularized data loss, an average attention entropy, and a budget-regularized objective.\n\nFundamental base and assumptions:\n- Normalize $z$ into a categorical distribution $A$ by exponentiating and renormalizing so that all entries are strictly positive and sum to $1$. This operation must be computed in a numerically stable manner.\n- Use the Shannon entropy (SE), defined as the expected negative natural logarithm of the probabilities, applied to $A$ to quantify routing uncertainty. All logarithms must be natural logarithms, and information is measured in nats.\n- Combine expert predictions by the law of total probability: the model’s predictive distribution over classes is the mixture of expert distributions weighted by $A$.\n- Use the average negative log-likelihood (NLL) of the correct class under the mixture distribution as the data loss. Then add a nonnegative budget penalty proportional to the average SE of $A$ across samples.\n\nExpert predictive distributions:\n- Each expert $e \\in \\{0,\\dots,K-1\\}$ has a reliability parameter $r_e \\in (0,1)$. For a given input with a unique correct class index $y^\\star$, expert $e$ assigns probability $r_e$ to $y^\\star$ and distributes the remaining probability mass uniformly across the remaining classes. That is, if there are $M$ classes, expert $e$ places probability $r_e$ on the correct class and probability $(1-r_e)/(M-1)$ on each incorrect class.\n\nTask and definitions to implement:\n- For each sample $i$, compute $A_i$ from $z_i$ as a valid categorical distribution. Compute the model’s mixture predictive distribution over the $M$ classes for that sample. Compute the per-sample NLL as the negative natural logarithm of the mixture probability assigned to the correct class. Compute the per-sample SE $H(A_i)$ as the expected negative natural logarithm of the attention probabilities. Average these quantities over all samples in the test case to get the average data loss and the average SE. Finally, compute the total objective as the sum of the average data loss and a nonnegative budget weight $\\beta$ times the average SE. All logarithms must be natural logarithms.\n- Modular arithmetic task: Each sample specifies integers $a$, $b$, and modulus $m$. The correct class is $y^\\star = (a + b) \\bmod m$, which is an index in $\\{0,1,\\dots,m-1\\}$.\n\nGlobal constants for all test cases:\n- Number of experts $K = 3$.\n- Expert reliabilities $r = [\\, r_0, r_1, r_2 \\,] = [\\, $0.5$, $0.7$, $0.9$ \\,]$.\n- Budget weight $\\beta = $0.2$.\n\nTest suite:\n- Test case $1$ (happy path):\n  - Modulus $M = $5$.\n  - Samples: $[(a,b)] = [\\, ($1$,$3$), ($2$,$4$), ($0$,$0$), ($4$,$4$) \\,]$.\n  - Attention logits per sample $z$ (each in $\\mathbb{R}^3$): $[\\, [\\, $0.0$, $0.5$, $1.0$ \\,], [\\, $1.2$, $0.0$, $-0.3$ \\,], [\\, $0.1$, $0.1$, $0.1$ \\,], [\\, $2.0$, $1.0$, $-1.0$ \\,] \\,]$.\n- Test case $2$ (maximum-entropy routing due to uniform logits):\n  - Modulus $M = $7$.\n  - Samples: $[(a,b)] = [\\, ($3$,$5$), ($6$,$6$), ($2$,$1$) \\,]$.\n  - Attention logits per sample $z$: $[\\, [\\, $0.0$, $0.0$, $0.0$ \\,], [\\, $0.0$, $0.0$, $0.0$ \\,], [\\, $0.0$, $0.0$, $0.0$ \\,] \\,]$.\n- Test case $3$ (near one-hot routing due to dominant logits):\n  - Modulus $M = $4$.\n  - Samples: $[(a,b)] = [\\, ($3$,$1$), ($2$,$2$), ($1$,$0$) \\,]$.\n  - Attention logits per sample $z$: $[\\, [\\, $5.0$, $0.0$, $0.0$ \\,], [\\, $0.0$, $5.0$, $0.0$ \\,], [\\, $0.0$, $0.0$, $5.0$ \\,] \\,]$.\n\nOutput requirements:\n- For each test case, compute three floats: the average unregularized data loss, the average SE of attention, and the total objective with the budget penalty. Round each float to $6$ decimal places using standard rounding to the nearest representable value at that precision.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the three rounded floats in the order specified. For example: $[ [\\dots], [\\dots], [\\dots] ]$.", "solution": "The problem requires the implementation and evaluation of a specialized attention mechanism, framed as a differentiable routing system for a Mixture-of-Experts (MoE) model. The core task is to compute on several test cases a set of three metrics: the average data loss, the average attention entropy, and a final budget-regularized objective. The solution will be derived from first principles as defined in the problem statement.\n\nFirst, we define the fundamental components and mathematical relationships. For each input sample $i$, we are given a vector of real-valued logits $z_i \\in \\mathbb{R}^K$, where $K$ is the number of experts.\n\nThe first step is to transform these logits into a valid probability distribution over the experts. This routing distribution, denoted as the attention vector $A_i \\in \\Delta^{K-1}$, is obtained via the softmax function. To ensure numerical stability, especially when dealing with large logit values, the log-sum-exp trick is employed. For each expert $e \\in \\{0, \\dots, K-1\\}$, the attention weight $A_{i,e}$ is calculated as:\n$$A_{i,e} = \\frac{\\exp(z_{i,e} - z_{i,\\max})}{\\sum_{j=0}^{K-1} \\exp(z_{i,j} - z_{i,\\max})}$$\nwhere $z_{i,e}$ is the logit for expert $e$ for sample $i$, and $z_{i,\\max} = \\max_{j} z_{i,j}$. The resulting attention vector $A_i$ has elements that are strictly positive and sum to $1$.\n\nThe problem introduces a budget regularizer to encourage sparse routing, meaning the model should ideally select a small subset of experts for any given input. This sparsity is measured by the Shannon entropy (SE) of the attention distribution, calculated in nats (using the natural logarithm). For each sample $i$, the entropy $H(A_i)$ is:\n$$H(A_i) = -\\sum_{e=0}^{K-1} A_{i,e} \\ln(A_{i,e})$$\nA lower entropy corresponds to a more concentrated, or sparse, attention distribution.\n\nThe task is a classification problem where inputs are pairs of integers $(a,b)$ and a modulus $M$. The correct class label, $y^\\star$, is the result of modular addition: $y^\\star = (a + b) \\bmod M$. The number of classes is equal to the modulus $M$.\n\nEach of the $K$ experts has an associated reliability parameter $r_e \\in (0,1)$. The predictive model for an expert is defined such that for a given input with correct class $y^\\star$, expert $e$ assigns a probability of $r_e$ to this correct class. The remaining probability mass, $1-r_e$, is distributed uniformly among the other $M-1$ incorrect classes.\nSpecifically, the conditional probability of a class $y$ given expert $e$ is:\n$$P_e(y | y^\\star) = \\begin{cases} r_e  \\text{if } y = y^\\star \\\\ \\frac{1-r_e}{M-1}  \\text{if } y \\neq y^\\star \\end{cases}$$\n\nThe final model prediction is a mixture of these expert predictions, weighted by the attention distribution $A_i$. According to the law of total probability, the mixture model's probability for any class $y$ is $P(y) = \\sum_{e=0}^{K-1} A_{i,e} P_e(y|y^\\star)$. Our specific goal is to compute the data loss, which only requires the probability assigned to the correct class, $y_i^\\star$. This simplifies the calculation significantly:\n$$P(y_i^\\star) = \\sum_{e=0}^{K-1} A_{i,e} P_e(y_i^\\star | y_i^\\star) = \\sum_{e=0}^{K-1} A_{i,e} r_e = A_i \\cdot r$$\nwhere $r = [r_0, \\dots, r_{K-1}]$ is the vector of expert reliabilities.\n\nThe data loss for a single sample $i$ is the negative log-likelihood (NLL) of the correct class:\n$$\\mathcal{L}_i = -\\ln(P(y_i^\\star)) = -\\ln(A_i \\cdot r)$$\n\nFor each test case, which consists of $N$ samples, we must compute three summary statistics.\n1.  The average unregularized data loss, $\\mathcal{L}_{\\text{data}}$:\n    $$\\mathcal{L}_{\\text{data}} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}_i$$\n2.  The average attention entropy, $\\mathcal{H}_{\\text{attn}}$:\n    $$\\mathcal{H}_{\\text{attn}} = \\frac{1}{N} \\sum_{i=1}^{N} H(A_i)$$\n3.  The total budget-regularized objective, $\\mathcal{J}$:\n    $$\\mathcal{J} = \\mathcal{L}_{\\text{data}} + \\beta \\mathcal{H}_{\\text{attn}}$$\n    where $\\beta$ is the given non-negative budget weight.\n\nThe implementation will process each test case by iterating through its samples. For each sample, it will:\n1.  Calculate the correct class $y^\\star$.\n2.  Compute the attention distribution $A_i$ from the logits $z_i$.\n3.  Calculate the per-sample NLL, $\\mathcal{L}_i$, and the per-sample SE, $H(A_i)$.\nThese per-sample values are then averaged across the entire test case to find $\\mathcal{L}_{\\text{data}}$ and $\\mathcal{H}_{\\text{attn}}$, which are then used to compute the final objective $\\mathcal{J}$. All computations use the provided global constants: number of experts $K=3$, reliabilities $r=[0.5, 0.7, 0.9]$, and budget weight $\\beta=0.2$. The final results for each test case are rounded to $6$ decimal places.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of evaluating a differentiable routing attention mechanism\n    on modular addition classification tasks.\n    \"\"\"\n    # Global constants for all test cases\n    K = 3\n    r = np.array([0.5, 0.7, 0.9])\n    beta = 0.2\n\n    test_cases = [\n        {\n            \"M\": 5,\n            \"samples\": [(1, 3), (2, 4), (0, 0), (4, 4)],\n            \"logits\": [\n                [0.0, 0.5, 1.0],\n                [1.2, 0.0, -0.3],\n                [0.1, 0.1, 0.1],\n                [2.0, 1.0, -1.0],\n            ],\n        },\n        {\n            \"M\": 7,\n            \"samples\": [(3, 5), (6, 6), (2, 1)],\n            \"logits\": [\n                [0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0],\n            ],\n        },\n        {\n            \"M\": 4,\n            \"samples\": [(3, 1), (2, 2), (1, 0)],\n            \"logits\": [\n                [5.0, 0.0, 0.0],\n                [0.0, 5.0, 0.0],\n                [0.0, 0.0, 5.0],\n            ],\n        },\n    ]\n\n    all_test_results = []\n    \n    for case in test_cases:\n        M = case[\"M\"]\n        samples = case[\"samples\"]\n        logits_list = case[\"logits\"]\n        num_samples = len(samples)\n        \n        total_nll = 0.0\n        total_se = 0.0\n        \n        for i in range(num_samples):\n            a, b = samples[i]\n            z = np.array(logits_list[i])\n            \n            # Step 1: Calculate correct class y_star\n            y_star = (a + b) % M\n            \n            # Step 2: Compute attention A from logits z (numerically stable softmax)\n            z_max = np.max(z)\n            exp_z = np.exp(z - z_max)\n            A = exp_z / np.sum(exp_z)\n            \n            # Step 3: Compute mixture probability for the correct class\n            # P(y_star) = A . r\n            p_y_star = np.dot(A, r)\n            \n            # Step 4: Compute per-sample NLL (data loss)\n            nll = -np.log(p_y_star)\n            total_nll += nll\n            \n            # Step 5: Compute per-sample Shannon Entropy of attention\n            # H(A) = -sum(A * log(A))\n            # Since softmax output is always > 0, we don't need to handle log(0)\n            se = -np.sum(A * np.log(A))\n            total_se += se\n            \n        # Step 6: Average the quantities over all samples\n        avg_data_loss = total_nll / num_samples\n        avg_se = total_se / num_samples\n        \n        # Step 7: Compute the total objective\n        total_objective = avg_data_loss + beta * avg_se\n        \n        # Round results to 6 decimal places\n        results_rounded = [\n            round(avg_data_loss, 6),\n            round(avg_se, 6),\n            round(total_objective, 6),\n        ]\n        \n        all_test_results.append(results_rounded)\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list of lists in Python matches the\n    # visual format \"[[...], [...], [...]]\" including spaces.\n    print(str(all_test_results).replace(\"'\", \"\"))\n\nsolve()\n```", "id": "3180926"}]}