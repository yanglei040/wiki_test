{"hands_on_practices": [{"introduction": "A cornerstone of autoregressive models like the GPT family is their ability to process information sequentially, ensuring that the prediction for a given position only depends on previous positions. This is achieved through masked attention, a mechanism that explicitly forbids tokens from attending to \"future\" positions. This practice solidifies the mathematical equivalence between additive masking in the logit space and multiplicative masking in the probability space, a key implementation detail. More importantly, it provides a hands-on demonstration of \"information leakage,\" a subtle but critical bug that occurs when masks are implemented incorrectly, allowing models to cheat by looking ahead. [@problem_id:3193602]", "problem": "You are to reason from first principles about masked attention in the Transformer architecture. Use only the following foundational bases: the definition of the SoftMax function, basic matrix multiplication, the property of the exponential and logarithm functions, and the scaled dot-product attention definition. Your task is to derive and then empirically verify that applying a binary mask before SoftMax is equivalent to an elementwise multiplication by the mask in the exponential domain, and to demonstrate how incorrect masks cause information leakage from future tokens to past outputs.\n\nDefinitions to be used as the starting point:\n- The SoftMax function over a vector $x \\in \\mathbb{R}^n$ is\n$$\n\\operatorname{SoftMax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}} \\quad \\text{for } i \\in \\{1,\\dots,n\\}.\n$$\n- The Scaled Dot-Product Attention (SDPA) logits for queries $Q \\in \\mathbb{R}^{L \\times d_k}$ and keys $K \\in \\mathbb{R}^{L \\times d_k}$ are\n$$\nZ = \\frac{QK^\\top}{\\sqrt{d_k}} \\in \\mathbb{R}^{L \\times L}.\n$$\n- A binary mask $M \\in \\{0,1\\}^{L \\times L}$ indicates disallowed positions by $0$ and allowed positions by $1$. The Hadamard (elementwise) product is denoted by $Z \\odot M$.\n\nYour program must:\n- Derive (in your own reasoning, not inside the program) from the above definitions that adding an additive mask $A$ with entries $A_{ij} = \\log M_{ij}$ to the logits $Z$ before SoftMax is equivalent to performing an elementwise multiplication by $M$ in the exponential domain before normalization, i.e., that the masked SoftMax equals\n$$\n\\operatorname{SoftMax}(Z + \\log M) = \\frac{e^{Z} \\odot M}{\\sum_{j} e^{Z_{:,j}} \\odot M_{:,j}} \\quad \\text{row-wise}.\n$$\n- Explain why using a finite large negative constant $-C$ in place of $-\\infty$ for masked entries causes small but nonzero probability mass on masked positions, which can lead to leakage if the mask is incorrect.\n\nThen implement a single program that computes the following four test cases with the exact numerical parameters given below and outputs their results as specified.\n\nGiven constants and matrices:\n- Use sequence length $L = 4$, key dimension $d_k = 3$, and value dimension $d_v = 2$.\n- Use\n$$\nQ = \\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\\\\\n1 & 1 & 1\n\\end{bmatrix}, \\quad\nK = \\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\\\\\n1 & 1 & 1\n\\end{bmatrix}, \\quad\nV = \\begin{bmatrix}\n1 & 2\\\\\n3 & 4\\\\\n5 & 6\\\\\n7 & 8\n\\end{bmatrix}.\n$$\n- Define the correct causal binary mask $M^{\\mathrm{causal}} \\in \\{0,1\\}^{4 \\times 4}$ by $M^{\\mathrm{causal}}_{ij} = 1$ if $j \\le i$ and $M^{\\mathrm{causal}}_{ij} = 0$ otherwise.\n- Define the incorrect off-by-one binary mask $M^{\\mathrm{off}} \\in \\{0,1\\}^{4 \\times 4}$ by $M^{\\mathrm{off}}_{ij} = 1$ if $j \\le i+1$ and $M^{\\mathrm{off}}_{ij} = 0$ otherwise.\n- For demonstrative modification, define $V^{\\mathrm{mod}}$ to be equal to $V$ except its last row is changed to $[700, 800]$.\n\nAdditionally, for the SoftMax masking equivalence tests, use the following smaller logits and mask:\n$$\nZ^{(a)} = \\begin{bmatrix}\n0.2 & -0.1 & 0.4\\\\\n1.0 & -1.0 & 0.0\n\\end{bmatrix}, \\quad\nM^{(a)} = \\begin{bmatrix}\n1 & 0 & 1\\\\\n0 & 1 & 1\n\\end{bmatrix}.\n$$\n\nTest suite to implement and compute:\n- Case $1$ (Equivalence of masking as multiplication before SoftMax): Compute row-wise SoftMax on $Z^{(a)}$ with mask $M^{(a)}$ in two ways:\n  - Ideal multiplicative form: numerator $e^{Z^{(a)}} \\odot M^{(a)}$ and denominator the row-wise sum of that numerator.\n  - Additive approximation with a large negative constant $-C$ in logits for masked positions, with $C = 10^9$.\n  Report the maximum absolute difference between the two resulting probability matrices as a floating-point number.\n- Case $2$ (No leakage with correct causal mask): Compute attention outputs $Y$ using $Q$, $K$, $V$, and $M^{\\mathrm{causal}}$. Then recompute using $Q$, $K$, $V^{\\mathrm{mod}}$, and $M^{\\mathrm{causal}}$. Check whether the first $3$ rows (positions $0$, $1$, and $2$) of the outputs are unchanged within an absolute tolerance of $10^{-12}$. Report a boolean that is true if unchanged and false otherwise.\n- Case $3$ (Leakage with incorrect mask): Compute attention outputs using $Q$, $K$, $V$ with $M^{\\mathrm{off}}$ and then with $V^{\\mathrm{mod}}$ with $M^{\\mathrm{off}}$. Check whether any of the first $3$ rows changed by more than an absolute tolerance of $10^{-6}$. Report a boolean that is true if leakage is detected and false otherwise.\n- Case $4$ (Finite-mask boundary effect): Repeat Case $1$ but use $C = 50$ instead of $C = 10^9$ in the additive approximation. Report the maximum absolute difference as a floating-point number.\n\nAngle units are not involved. No physical units are involved. All reported numeric answers must be the specified booleans or floating-point numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\text{case1\\_float}, \\text{case2\\_bool}, \\text{case3\\_bool}, \\text{case4\\_float}]$. For example, a syntactically valid format is $[0.0,True,False,1e-12]$.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the mathematical principles of the Transformer architecture, well-posed with a complete and consistent set of definitions and parameters, and objective in its formulation. The tasks require a rigorous derivation from first principles and an empirical verification, both of which are central to scientific and engineering disciplines. All conditions for a valid problem are met.\n\nThe core of the problem lies in understanding how masking is implemented in the SoftMax function within the scaled dot-product attention mechanism and the consequences of imperfect implementations.\n\n**Part 1: Derivation of Masking Equivalence**\n\nWe are tasked to derive that adding an additive mask $A$ where $A_{ij} = \\log M_{ij}$ to the logits $Z$ before applying the SoftMax function is equivalent to an elementwise multiplication by the binary mask $M$ in the exponential domain.\n\nLet $Z \\in \\mathbb{R}^{L \\times L}$ be the matrix of attention logits, and let $M \\in \\{0, 1\\}^{L \\times L}$ be a binary mask where $M_{ij}=1$ indicates an allowed attention connection and $M_{ij}=0$ indicates a disallowed one. We define an additive mask matrix $A \\in (\\mathbb{R} \\cup \\{-\\infty\\})^{L \\times L}$ with entries $A_{ij} = \\log M_{ij}$.\n\nThe SoftMax function is applied row-wise to the logits matrix. For a given row $i$, the masked SoftMax probability for the connection to column $j$ is computed on the additively masked logits $Z'_{ij} = Z_{ij} + A_{ij}$.\n\nAccording to the provided definition of the SoftMax function, for row $i$:\n$$\n\\operatorname{SoftMax}(Z'_i)_j = \\frac{e^{Z'_{ij}}}{\\sum_{k=1}^L e^{Z'_{ik}}}\n$$\nSubstitute $Z'_{ij} = Z_{ij} + A_{ij} = Z_{ij} + \\log M_{ij}$:\n$$\n\\operatorname{SoftMax}(Z_i + \\log M_i)_j = \\frac{e^{Z_{ij} + \\log M_{ij}}}{\\sum_{k=1}^L e^{Z_{ik} + \\log M_{ik}}}\n$$\nUsing the property of the exponential function, $e^{a+b} = e^a e^b$, we can rewrite the expression as:\n$$\n\\frac{e^{Z_{ij}} e^{\\log M_{ij}}}{\\sum_{k=1}^L e^{Z_{ik}} e^{\\log M_{ik}}}\n$$\nNow, we analyze the term $e^{\\log M_{ij}}$. The binary mask $M$ has entries that are either $1$ or $0$.\n- If $M_{ij} = 1$, then $\\log M_{ij} = \\log 1 = 0$. Consequently, $e^{\\log M_{ij}} = e^0 = 1$. So, $e^{\\log M_{ij}} = M_{ij}$.\n- If $M_{ij} = 0$, then $\\log M_{ij} = \\log 0$. In the limiting sense required for this context, $\\log 0 \\to -\\infty$. Consequently, $e^{\\log M_{ij}} \\to e^{-\\infty} = 0$. So, $e^{\\log M_{ij}} = M_{ij}$.\n\nIn both cases, $e^{\\log M_{ij}} = M_{ij}$. Substituting this identity back into the main expression:\n$$\n\\frac{e^{Z_{ij}} M_{ij}}{\\sum_{k=1}^L e^{Z_{ik}} M_{ik}}\n$$\nThis expression can be written using Hadamard (elementwise) product notation. Let $E$ be the matrix with entries $E_{ij}=e^{Z_{ij}}$. Then the numerator is $(E \\odot M)_{ij}$. The denominator is the sum of the $i$-th row of the matrix $E \\odot M$. This proves the equivalence:\n$$\n\\operatorname{SoftMax}(Z + \\log M) = \\frac{e^{Z} \\odot M}{\\sum_{j} (e^{Z} \\odot M)_{:,j}} \\quad \\text{(row-wise)}\n$$\nThis derivation formally shows that adding the logarithm of the mask to the logits is mathematically equivalent to multiplying by the mask after exponentiation but before the normalization step of SoftMax.\n\n**Part 2: Information Leakage with Incorrect or Finite Masks**\n\nIn a practical floating-point implementation, we cannot represent $-\\infty$. Instead, we approximate $\\log 0$ with a large-magnitude negative number, $-C$, where $C$ is a large positive constant (e.g., $10^9$).\n\nThe additive mask is applied as follows: for a disallowed connection ($M_{ij}=0$), we add $-C$ to the logit $Z_{ij}$. The corresponding term in the SoftMax calculation becomes:\n$$\ne^{Z_{ij} - C} = e^{Z_{ij}} e^{-C}\n$$\nFor a large $C$, the value $e^{-C}$ is extremely small but crucially non-zero. This means that disallowed positions in the attention matrix will be assigned a tiny, non-zero probability mass, whereas the ideal mathematical formulation would assign them exactly zero probability.\n\nInformation leakage occurs when this non-zero probability is combined with an **incorrect mask**. A correct causal mask for a decoder, $M^{\\mathrm{causal}}$, ensures that for any query position $i$, the attention weights $P_{ij}$ are zero for all key positions $j > i$. This prevents a token from \"seeing\" future tokens.\n\nThe given incorrect mask, $M^{\\mathrm{off}}$, is defined such that $M^{\\mathrm{off}}_{ij} = 1$ if $j \\le i+1$. This allows a query at position $i$ to attend to key/value at position $i+1$, which is one step into the future. For example, for $i=2$, it can attend to $j=0, 1, 2, 3$. Because it can attend to position $j=3$, if the value vector $V_3$ is changed, the output at position $Y_2$ will also change.\n\nThe attention output for query $i$ is a weighted sum:\n$$\nY_i = \\sum_{j=1}^L P_{ij} V_j\n$$\nIf $M^{\\mathrm{off}}$ is used, the attention probability $P_{2,3}$ will be non-zero. Let's compare the output $Y_2$ using the original values $V$ with the output $Y'_2$ using the modified values $V^{\\mathrm{mod}}$. The change in the output at position $2$ is:\n$$\nY'_2 - Y_2 = \\sum_{j=1}^L P_{2,j} V^{\\mathrm{mod}}_j - \\sum_{j=1}^L P_{2,j} V_j = \\sum_{j=1}^L P_{2,j} (V^{\\mathrm{mod}}_j - V_j)\n$$\nSince $V^{\\mathrm{mod}}$ only differs from $V$ in the last row (index $3$), this simplifies to:\n$$\nY'_2 - Y_2 = P_{2,3} (V^{\\mathrm{mod}}_3 - V_3)\n$$\nAs $M^{\\mathrm{off}}$ allows attention from $i=2$ to $j=3$, $P_{2,3}$ is non-zero. The problem defines a very large change in $V_3$, so $(V^{\\mathrm{mod}}_3 - V_3)$ is large. The product, which represents the change in the output for a \"past\" token, becomes significant. This is a direct demonstration of information leaking from the future (the value at step $3$) to the past (the output at step $2$).\n\nWith a correct causal mask, $M^{\\mathrm{causal}}$, the weight $P_{2,3}$ would be exactly zero, and thus $Y_2$ would be completely unaffected by any changes to $V_3$.\n\nThe magnitude of the constant $C$ in the finite approximation determines how close the masked probabilities are to zero. A smaller $C$ (like $50$) results in $e^{-50}$ being a larger number than $e^{-10^9}$, leading to a greater deviation from the ideal zero-probability case and more pronounced \"leakage\" or numerical error. This is what Case $4$ demonstrates.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs all calculations for the four test cases regarding\n    masked attention in Transformers.\n    \"\"\"\n\n    # --- Given Constants and Matrices ---\n    L = 4\n    dk = 3\n    dv = 2\n\n    Q = np.array([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [1, 1, 1]\n    ], dtype=np.float64)\n\n    K = np.array([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [1, 1, 1]\n    ], dtype=np.float64)\n\n    V = np.array([\n        [1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8]\n    ], dtype=np.float64)\n\n    V_mod = np.array([\n        [1, 2],\n        [3, 4],\n        [5, 6],\n        [700, 800]\n    ], dtype=np.float64)\n\n    # M_causal: j <= i\n    M_causal = np.tril(np.ones((L, L), dtype=np.float64))\n\n    # M_off: j <= i + 1\n    i_indices = np.arange(L).reshape(-1, 1)\n    j_indices = np.arange(L).reshape(1, -1)\n    M_off = (j_indices <= i_indices + 1).astype(np.float64)\n\n    # Matrices for smaller test cases\n    Z_a = np.array([\n        [0.2, -0.1, 0.4],\n        [1.0, -1.0, 0.0]\n    ], dtype=np.float64)\n\n    M_a = np.array([\n        [1, 0, 1],\n        [0, 1, 1]\n    ], dtype=np.float64)\n\n    # --- Helper Function for Attention Calculation ---\n    def compute_attention(q_mat, k_mat, v_mat, mask, C=1e9):\n        \"\"\"\n        Computes Scaled Dot-Product Attention with an additive mask.\n        Uses a numerically stable softmax.\n        \"\"\"\n        dk_val = q_mat.shape[1]\n        logits = (q_mat @ k_mat.T) / np.sqrt(dk_val)\n        \n        # Apply the additive mask\n        additive_mask = (mask - 1) * C\n        masked_logits = logits + additive_mask\n        \n        # Numerically stable softmax\n        shifted_logits = masked_logits - np.max(masked_logits, axis=1, keepdims=True)\n        attention_weights = np.exp(shifted_logits)\n        attention_weights /= np.sum(attention_weights, axis=1, keepdims=True)\n        \n        output = attention_weights @ v_mat\n        return output\n\n    # --- Test Cases ---\n    results = []\n\n    # Case 1: Equivalence of masking with C = 1e9\n    # Ideal multiplicative form\n    exp_Z_a = np.exp(Z_a)\n    numerator_ideal = exp_Z_a * M_a\n    denominator_ideal = np.sum(numerator_ideal, axis=1, keepdims=True)\n    # Handle cases where a whole row is masked, to avoid division by zero\n    denominator_ideal[denominator_ideal == 0] = 1.0 \n    P_ideal = numerator_ideal / denominator_ideal\n    \n    # Additive approximation with C = 1e9\n    C1 = 1e9\n    additive_mask_1 = (M_a - 1) * C1\n    Z_masked_1 = Z_a + additive_mask_1\n    # Stable softmax for additive form\n    shifted_Z_masked_1 = Z_masked_1 - np.max(Z_masked_1, axis=1, keepdims=True)\n    exp_Z_masked_1 = np.exp(shifted_Z_masked_1)\n    P_approx_1 = exp_Z_masked_1 / np.sum(exp_Z_masked_1, axis=1, keepdims=True)\n    \n    case1_diff = np.max(np.abs(P_ideal - P_approx_1))\n    results.append(case1_diff)\n\n    # Case 2: No leakage with correct causal mask\n    Y1 = compute_attention(Q, K, V, M_causal)\n    Y2 = compute_attention(Q, K, V_mod, M_causal)\n    case2_unchanged = bool(np.allclose(Y1[:3, :], Y2[:3, :], atol=1e-12, rtol=0))\n    results.append(case2_unchanged)\n\n    # Case 3: Leakage with incorrect mask\n    Y3 = compute_attention(Q, K, V, M_off)\n    Y4 = compute_attention(Q, K, V_mod, M_off)\n    diff = np.abs(Y3[:3, :] - Y4[:3, :])\n    case3_leakage_detected = bool(np.any(diff > 1e-6))\n    results.append(case3_leakage_detected)\n\n    # Case 4: Finite-mask boundary effect with C = 50\n    C4 = 50.0\n    additive_mask_4 = (M_a - 1) * C4\n    Z_masked_4 = Z_a + additive_mask_4\n    # Stable softmax\n    shifted_Z_masked_4 = Z_masked_4 - np.max(Z_masked_4, axis=1, keepdims=True)\n    exp_Z_masked_4 = np.exp(shifted_Z_masked_4)\n    P_approx_4 = exp_Z_masked_4 / np.sum(exp_Z_masked_4, axis=1, keepdims=True)\n    \n    case4_diff = np.max(np.abs(P_ideal - P_approx_4))\n    results.append(case4_diff)\n    \n    # Final print statement\n    # Explicitly format booleans to be lowercase 'true'/'false' if needed, but str() is fine.\n    print(f\"[{results[0]},{str(results[1])},{str(results[2])},{results[3]}]\")\n\nsolve()\n```", "id": "3193602"}, {"introduction": "The standard softmax function distributes attention weights across all allowed tokens, but the \"sharpness\" of this distribution can be critical for performance. Temperature scaling offers a simple but powerful knob to control this behavior. This exercise challenges you to explore the trade-off between soft aggregation (high temperature, $\\tau \\gt 1$), which is useful for blending information from multiple sources, and hard selection (low temperature, $\\tau \\lt 1$), which is useful for pointing to a single, most relevant source. By implementing tasks that favor each of these regimes, you will develop a practical intuition for how the temperature parameter $\\tau$ shapes the attention mechanism's behavior. [@problem_id:3193530]", "problem": "You are asked to study temperature scaling in the attention mechanism of the Transformer architecture using first principles. Consider a single attention step where a query produces a vector of logits $Z \\in \\mathbb{R}^n$, and attention weights are computed by the temperature-scaled softmax function $\\mathrm{softmax}(Z / \\tau)$ with temperature $\\tau \\in \\mathbb{R}_{>0}$. The output is the attention-weighted aggregation $y(\\tau) = \\sum_{i=1}^n a_i(\\tau) \\, v_i$, where $a_i(\\tau)$ are the coordinates of $\\mathrm{softmax}(Z / \\tau)$ and $V \\in \\mathbb{R}^n$ is a vector of values.\n\nTasks:\n1) Theory: Starting from the definition of the softmax function and basic calculus, derive the Jacobian $\\partial a_i / \\partial z_j$ with respect to $Z$ for general temperature $\\tau$, and analyze its behavior as $\\tau \\to 0$ in the following cases:\n- A unique maximizer: There exists a unique index $m$ such that $z_m > z_k$ for all $k \\neq m$.\n- A non-unique maximizer (a tie): There is a set $\\mathcal{M}$ of $r \\ge 2$ indices where $z_i = \\max_k z_k$ for all $i \\in \\mathcal{M}$.\nConclude rigorously, using limits, whether and how the gradient becomes localized as $\\tau \\to 0$.\n\n2) Computation: Implement a numerically stable computation to investigate a simple task family that separates the need for soft aggregation from hard selection. For each case, define the predicted scalar output $y(\\tau) = \\sum_{i=1}^n a_i(\\tau) v_i$ and a target $y^\\star$. Then define the squared error loss $L(\\tau) = \\big(y(\\tau) - y^\\star\\big)^2$. You must sweep temperature $\\tau$ over a fixed grid and report:\n- The minimizing temperature $\\tau_{\\min}$ in the sweep that achieves the smallest $L(\\tau)$. If there are ties, choose the smallest $\\tau$ among the minimizers.\n- A phase-transition proxy temperature $\\tau_{\\mathrm{pt}}$, defined as the smallest $\\tau$ in the sweep for which $\\max_i a_i(\\tau) \\ge \\alpha$, where $\\alpha$ is a fixed threshold in $(0,1)$. If no temperature in the sweep satisfies the inequality, report $-1.0$.\n\nUse the following fixed sweep and threshold:\n- Temperature sweep: $\\{\\tau\\} = \\{\\,0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0\\,\\}$.\n- Threshold: $\\alpha = 0.9$.\n\nDefine the following four test cases, each with a tuple $(Z, V, \\text{task})$:\n- Case A (soft aggregation preferred): $Z$ has coordinates $3.0, 2.9, 0.0, -1.0$ and $V$ has coordinates $1.0, -1.0, 0.5, 2.0$. The target requires averaging the values indexed by a provided set $S$: here $S = \\{0, 1\\}$, so $y^\\star = \\frac{1}{|S|} \\sum_{i \\in S} v_i$. This case should be treated as a soft aggregation task.\n- Case B (hard selection preferred): $Z$ has coordinates $3.0, 2.0, 1.0, 0.0$ and $V$ has coordinates $2.0, -1.0, 0.0, 0.0$. The target requires selecting the value at the index of the maximal logit: $y^\\star = v_{\\arg\\max_i z_i}$. In the event of a tie, define $\\arg\\max$ to return the smallest index among maximizers. This case should be treated as a hard selection task.\n- Case C (soft aggregation with a tie in logits): $Z$ has coordinates $1.0, 1.0, 0.0, 0.0$ and $V$ has coordinates $1.0, -1.0, 0.2, -0.2$. Use the same soft aggregation target with $S = \\{0, 1\\}$, that is, $y^\\star = \\frac{1}{2} (v_0 + v_1)$.\n- Case D (hard selection with all logits equal): $Z$ has coordinates $0.0, 0.0, 0.0, 0.0$ and $V$ has coordinates $1.0, 0.0, 0.0, 0.0$. Use the hard selection target $y^\\star = v_{\\arg\\max_i z_i}$ with the same tie-breaking rule as above.\n\nNumerical stability requirement: Implement the computation of $\\mathrm{softmax}(Z / \\tau)$ using a numerically stable method such as subtracting the maximum of $Z / \\tau$ before exponentiation.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case in the order A, B, C, D, append two floating-point numbers: first $\\tau_{\\min}$, then $\\tau_{\\mathrm{pt}}$. The overall output thus contains eight numbers in total. For example, your program should print a line of the form [$r_1$,$r_2$,$r_3$,$r_4$,$r_5$,$r_6$,$r_7$,$r_8$], where each $r_k$ is a floating-point number.", "solution": "The problem asks for a theoretical analysis of the Jacobian of the temperature-scaled softmax function and a computational investigation of its role in balancing soft aggregation and hard selection tasks.\n\n### Part 1: Theoretical Analysis\n\nWe are given the temperature-scaled attention weights $a_i(\\tau)$ for a vector of logits $Z \\in \\mathbbR}^n$:\n$$a_i(\\tau) = \\frac{\\exp(z_i / \\tau)}{\\sum_{k=1}^n \\exp(z_k / \\tau)}$$\nOur goal is to derive the Jacobian elements $\\frac{\\partial a_i}{\\partial z_j}$ and analyze their behavior as the temperature $\\tau \\to 0^+$.\n\n**Derivation of the Jacobian $\\frac{\\partial a_i}{\\partial z_j}$**\n\nWe use the quotient rule for differentiation. Let $N_i = \\exp(z_i / \\tau)$ and $D = \\sum_{k=1}^n \\exp(z_k / \\tau)$. Then $a_i = N_i / D$.\n\nCase 1: $i = j$.\nThe derivative of $a_i$ with respect to $z_i$ is:\n$$\\frac{\\partial a_i}{\\partial z_i} = \\frac{\\frac{\\partial N_i}{\\partial z_i} D - N_i \\frac{\\partial D}{\\partial z_i}}{D^2}$$\nThe partial derivatives of $N_i$ and $D$ are:\n$$\\frac{\\partial N_i}{\\partial z_i} = \\frac{1}{\\tau}\\exp(z_i / \\tau) = \\frac{1}{\\tau} N_i$$\n$$\\frac{\\partial D}{\\partial z_i} = \\frac{1}{\\tau}\\exp(z_i / \\tau) = \\frac{1}{\\tau} N_i$$\nSubstituting these into the quotient rule expression:\n$$\\frac{\\partial a_i}{\\partial z_i} = \\frac{(\\frac{1}{\\tau} N_i) D - N_i (\\frac{1}{\\tau} N_i)}{D^2} = \\frac{1}{\\tau} \\frac{N_i}{D} \\frac{D - N_i}{D} = \\frac{1}{\\tau} a_i(1 - a_i)$$\n\nCase 2: $i \\neq j$.\nThe derivative of $a_i$ with respect to $z_j$ is:\n$$\\frac{\\partial a_i}{\\partial z_j} = \\frac{\\frac{\\partial N_i}{\\partial z_j} D - N_i \\frac{\\partial D}{\\partial z_j}}{D^2}$$\nThe partial derivatives are:\n$$\\frac{\\partial N_i}{\\partial z_j} = 0$$\n$$\\frac{\\partial D}{\\partial z_j} = \\frac{1}{\\tau}\\exp(z_j / \\tau) = \\frac{1}{\\tau} N_j$$\nSubstituting these:\n$$\\frac{\\partial a_i}{\\partial z_j} = \\frac{0 \\cdot D - N_i (\\frac{1}{\\tau} N_j)}{D^2} = -\\frac{1}{\\tau} \\frac{N_i}{D} \\frac{N_j}{D} = -\\frac{1}{\\tau} a_i a_j$$\n\nCombining both cases using the Kronecker delta, $\\delta_{ij}$:\n$$\\frac{\\partial a_i}{\\partial z_j} = \\frac{1}{\\tau} (a_i \\delta_{ij} - a_i a_j)$$\n\n**Limit Analysis as $\\tau \\to 0^+$**\n\nFirst, we analyze the limit of the attention weights $a_i(\\tau)$. Let $z_{\\max} = \\max_k z_k$. We can write $a_i(\\tau)$ as:\n$$a_i(\\tau) = \\frac{\\exp((z_i - z_{\\max}) / \\tau)}{\\sum_{k=1}^n \\exp((z_k - z_{\\max}) / \\tau)}$$\n\n**Case A: Unique Maximizer**\nThere is a unique index $m$ such that $z_m = z_{\\max}$ and $z_k < z_{\\max}$ for all $k \\neq m$.\nAs $\\tau \\to 0^+$, the term $(z_k - z_{\\max})/\\tau$ goes to $-\\infty$ for $k \\neq m$, and it is $0$ for $k=m$. Consequently, $\\exp((z_k - z_{\\max})/\\tau) \\to 0$ for $k \\neq m$ and is $1$ for $k=m$.\nThe limit of the denominator is $\\sum_{k=1}^n \\exp((z_k - z_{\\max})/\\tau) \\to 1$.\nTherefore, the attention weights converge to a one-hot vector:\n$$\\lim_{\\tau \\to 0^+} a_m(\\tau) = 1 \\quad \\text{and} \\quad \\lim_{\\tau \\to 0^+} a_k(\\tau) = 0 \\text{ for } k \\neq m$$\nNow, we analyze the Jacobian elements. For any $i,j$:\nThe expression $\\frac{1}{\\tau}(a_i \\delta_{ij} - a_i a_j)$ involves terms that behave like $\\frac{1}{\\tau} \\exp(-C/\\tau)$ for some $C>0$.\nLet $u = 1/\\tau$. As $\\tau \\to 0^+$, $u \\to \\infty$. The limit becomes $\\lim_{u \\to \\infty} u e^{-Cu} = 0$.\nA more rigorous analysis for each subcase shows that every element of the Jacobian tends to zero:\n- If $i \\neq m$ and $j \\neq m$: $\\frac{\\partial a_i}{\\partial z_j} = \\frac{1}{\\tau} (\\delta_{ij} a_i - a_i a_j)$. Both $a_i$ and $a_j$ are of order $\\exp((z_{i,j}-z_m)/\\tau)$, so the term vanishes faster than $1/\\tau$ grows. The limit is $0$.\n- If one index is $m$, e.g., $\\frac{\\partial a_m}{\\partial z_k}$ for $k \\neq m$: $-\\frac{1}{\\tau} a_m a_k \\approx -\\frac{1}{\\tau} (1) \\exp((z_k-z_m)/\\tau) \\to 0$.\n- For $\\frac{\\partial a_m}{\\partial z_m}$: $\\frac{1}{\\tau} a_m (1-a_m) = \\frac{1}{\\tau} a_m \\sum_{k \\neq m} a_k$. This is approximately $\\frac{1}{\\tau} \\sum_{k \\neq m} \\exp((z_k-z_m)/\\tau)$, which is a sum of terms that each go to $0$. The limit is $0$.\n\nConclusion for unique maximizer: In the limit $\\tau \\to 0^+$, the Jacobian matrix $\\frac{\\partial a}{\\partial z}$ converges to the zero matrix. The gradient signal vanishes entirely, a phenomenon known as gradient saturation. Thus, the gradient does not \"localize\" to a subset of influential inputs; it disappears.\n\n**Case B: Non-Unique Maximizer**\nThere is a set $\\mathcal{M}$ of $r \\ge 2$ indices such that $z_k = z_{\\max}$ for all $k \\in \\mathcal{M}$.\nAs $\\tau \\to 0^+$, for $k \\in \\mathcal{M}$, $(z_k - z_{\\max})/\\tau = 0$, so $\\exp \\to 1$. For $k \\notin \\mathcal{M}$, $(z_k - z_{\\max})/\\tau \\to -\\infty$, so $\\exp \\to 0$.\nThe denominator tends to $\\sum_{k \\in \\mathcal{M}} 1 + \\sum_{k \\notin \\mathcal{M}} 0 = r$.\nTherefore, the attention weights converge to a uniform distribution over the maximizers:\n$$\\lim_{\\tau \\to 0^+} a_k(\\tau) = \\frac{1}{r} \\text{ for } k \\in \\mathcal{M} \\quad \\text{and} \\quad \\lim_{\\tau \\to 0^+} a_k(\\tau) = 0 \\text{ for } k \\notin \\mathcal{M}$$\nNow, we analyze the Jacobian elements.\n- If $i \\notin \\mathcal{M}$ or $j \\notin \\mathcal{M}$: At least one of $a_i$ or $a_j$ will vanish exponentially fast, causing the corresponding Jacobian element to approach $0$, similar to the unique maximizer case.\n- If both $i, j \\in \\mathcal{M}$: At the point where $z_i=z_j=z_{\\max}$, the attention values are $a_i=a_j=\\frac{1}{r+\\dots}$. For small $\\tau$, $a_i \\approx 1/r$.\n  - For $i=j \\in \\mathcal{M}$:\n    $$\\frac{\\partial a_i}{\\partial z_i} = \\frac{1}{\\tau} a_i(1 - a_i) \\approx \\frac{1}{\\tau} \\frac{1}{r} \\left(1 - \\frac{1}{r}\\right) = \\frac{r-1}{r^2 \\tau}$$\n    As $\\tau \\to 0^+$, this term diverges to $+\\infty$.\n  - For $i, j \\in \\mathcal{M}$ and $i \\neq j$:\n    $$\\frac{\\partial a_i}{\\partial z_j} = -\\frac{1}{\\tau} a_i a_j \\approx -\\frac{1}{\\tau} \\frac{1}{r^2}$$\n    As $\\tau \\to 0^+$, this term diverges to $-\\infty$.\n\nConclusion for non-unique maximizer: The limit function $a_i(Z)$ is non-differentiable at points $Z$ where ties for the maximum exist. The derivatives diverge as $\\tau \\to 0^+$. Specifically, the sub-matrix of the Jacobian corresponding to the indices in $\\mathcal{M}$ has elements that diverge to $\\pm\\infty$, while all other elements of the Jacobian go to $0$. In this sense, the gradient becomes strongly localized to the set of maximizers.\n\n### Part 2: Computational Investigation\n\nWe implement a numerically stable softmax function and evaluate the loss for four test cases across a sweep of temperatures.\n\nThe numerically stable softmax with temperature $\\tau$ is computed by first shifting the scaled logits $Z/\\tau$ by their maximum value to prevent overflow during exponentiation. Let $X = Z/\\tau$.\n$$a_i = \\frac{\\exp(x_i - \\max_k x_k)}{\\sum_j \\exp(x_j - \\max_k x_k)}$$\nWe define the test cases and targets:\n- Case A (Soft Agg. Pref.): $Z=[3.0, 2.9, 0.0, -1.0]$, $V=[1.0, -1.0, 0.5, 2.0]$. Target $y^\\star = \\frac{1}{2}(v_0+v_1) = 0.0$. Low loss is expected for higher $\\tau$ that averages $v_0$ and $v_1$.\n- Case B (Hard Sel. Pref.): $Z=[3.0, 2.0, 1.0, 0.0]$, $V=[2.0, -1.0, 0.0, 0.0]$. Target $y^\\star = v_{\\arg\\max Z} = v_0 = 2.0$. Low loss is expected for lower $\\tau$ that selects $v_0$.\n- Case C (Soft Agg. Tie): $Z=[1.0, 1.0, 0.0, 0.0]$, $V=[1.0, -1.0, 0.2, -0.2]$. Target $y^\\star = \\frac{1}{2}(v_0+v_1) = 0.0$. Because $z_0=z_1$ and $z_2=z_3$, by symmetry $a_0=a_1$ and $a_2=a_3$. The output is $y(\\tau) = a_0(v_0+v_1) + a_2(v_2+v_3) = a_0(0) + a_2(0) = 0$. The loss is $0$ for all $\\tau$. Thus, $\\tau_{\\min}$ will be the smallest in the sweep, $0.02$. The maximum attention is $a_0 = 1/(2+2e^{-1/\\tau})$, which never exceeds $0.5$, so it cannot reach the threshold $\\alpha=0.9$. $\\tau_{\\mathrm{pt}}$ will be $-1.0$.\n- Case D (Hard Sel. All Tie): $Z=[0.0, 0.0, 0.0, 0.0]$, $V=[1.0, 0.0, 0.0, 0.0]$. Target $y^\\star = v_{\\arg\\max Z} = v_0 = 1.0$. With all logits equal, $a_i=1/4$ for all $\\tau$. The output is constant: $y(\\tau) = \\frac{1}{4}\\sum v_i = 0.25$. The loss is constant. $\\tau_{\\min}$ will be the smallest in the sweep, $0.02$. The maximum attention is always $0.25$, so $\\tau_{\\mathrm{pt}}$ will be $-1.0$.\n\nThe implementation will sweep through the given temperature grid, calculate the loss $L(\\tau) = (y(\\tau) - y^\\star)^2$ and $\\max_i a_i(\\tau)$ for each $\\tau$, and determine $\\tau_{\\min}$ and $\\tau_{\\mathrm{pt}}$ according to the specified rules.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the temperature scaling problem by analyzing four test cases.\n    For each case, it finds the temperature that minimizes a squared error loss (tau_min)\n    and a phase-transition proxy temperature (tau_pt).\n    \"\"\"\n\n    # Define the fixed sweep grid and threshold from the problem statement.\n    temp_sweep = np.array([0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0])\n    alpha_threshold = 0.9\n\n    # Define the four test cases as tuples: (Z, V, y_star).\n    # y_star is pre-calculated based on a soft aggregation or hard selection task.\n    test_cases = [\n        # Case A: Soft aggregation preferred\n        (\n            np.array([3.0, 2.9, 0.0, -1.0]),\n            np.array([1.0, -1.0, 0.5, 2.0]),\n            0.5 * (1.0 - 1.0)  # y_star = (v_0 + v_1) / 2\n        ),\n        # Case B: Hard selection preferred\n        (\n            np.array([3.0, 2.0, 1.0, 0.0]),\n            np.array([2.0, -1.0, 0.0, 0.0]),\n            2.0  # y_star = v_argmax(Z) = v_0\n        ),\n        # Case C: Soft aggregation with a tie in logits\n        (\n            np.array([1.0, 1.0, 0.0, 0.0]),\n            np.array([1.0, -1.0, 0.2, -0.2]),\n            0.5 * (1.0 - 1.0)  # y_star = (v_0 + v_1) / 2\n        ),\n        # Case D: Hard selection with all logits equal\n        (\n            np.array([0.0, 0.0, 0.0, 0.0]),\n            np.array([1.0, 0.0, 0.0, 0.0]),\n            1.0  # y_star = v_argmin_idx(argmax(Z)) = v_0\n        )\n    ]\n\n    # List to store the final results: [tau_min_A, tau_pt_A, tau_min_B, tau_pt_B, ...]\n    final_results = []\n\n    def stable_softmax(z, tau):\n        \"\"\"Computes numerically stable softmax with temperature.\"\"\"\n        if tau <= 0:\n            raise ValueError(\"Temperature tau must be positive.\")\n        z_scaled = z / tau\n        # Subtract max for numerical stability (log-sum-exp trick)\n        z_stable = z_scaled - np.max(z_scaled)\n        exps = np.exp(z_stable)\n        return exps / np.sum(exps)\n\n    for Z, V, y_star in test_cases:\n        losses = []\n        max_attentions = []\n\n        # Sweep through the temperature grid\n        for tau in temp_sweep:\n            # Calculate attention weights\n            a = stable_softmax(Z, tau)\n            \n            # Calculate the predicted scalar output\n            y_tau = np.sum(a * V)\n            \n            # Calculate and store the squared error loss\n            loss = (y_tau - y_star)**2\n            losses.append(loss)\n            \n            # Store the maximum attention weight\n            max_attentions.append(np.max(a))\n\n        # Determine tau_min: the smallest temperature that results in the minimum loss.\n        # np.argmin returns the index of the first occurrence of the minimum value.\n        min_loss_idx = np.argmin(losses)\n        tau_min = temp_sweep[min_loss_idx]\n\n        # Determine tau_pt: the smallest temperature where max attention weight >= threshold.\n        tau_pt = -1.0\n        for i, max_a in enumerate(max_attentions):\n            if max_a >= alpha_threshold:\n                tau_pt = temp_sweep[i]\n                break # Found the smallest tau, so we can stop.\n\n        # Append results for the current case\n        final_results.append(tau_min)\n        final_results.append(tau_pt)\n\n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3193530"}, {"introduction": "Scaled dot-product attention relies on the inner product as its core similarity measure between queries and keys. While effective, this measure is sensitive not just to the alignment (angle) of the vectors but also to their magnitude (norm). This practice guides you through a \"stress test\" of the attention mechanism, demonstrating how a key with an artificially large norm can produce a dominant dot product, effectively \"hijacking\" the attention weights. By creating such an adversarial key and then implementing mitigation strategies like norm clipping and cosine similarity, you will gain a deeper appreciation for the design choices that lead to more robust and stable models. [@problem_id:3193536]", "problem": "Consider the Scaled Dot-Product Attention (SDPA) mechanism used in the transformer architecture. Given a set of Query (Q), Key (K), and Value (V) vectors, attention weights are formed by applying a softmax function to a set of attention logits. The attention logits arise from inner products between the query and each key, scaled by the square root of the key dimensionality. The softmax function converts arbitrary real-valued scores to a probability distribution over keys. Assume the following well-tested definitions: the inner product between vectors is the sum of element-wise products, the Euclidean norm is the square root of the sum of squared elements, and the softmax function maps a vector of real numbers to a vector of non-negative real numbers summing to one by exponentiating each element and normalizing by the sum of exponentials.\n\nAdversarial “attention hijacking” can occur when an attacker adds a new key token with unusually large norm, thereby producing a large inner product with the query and dominating the softmax distribution. This problem asks you to demonstrate, through computation, how such an attack can dominate attention and to implement two mitigation strategies: norm clipping of keys and cosine similarity normalization of logits.\n\nYou must implement the following from first principles:\n- Compute scaled dot-product attention logits for a single query $Q$ and a set of keys $\\{K_i\\}$ in dimension $d_k$, scale each logit by $1/\\sqrt{d_k}$, and apply the softmax function to obtain attention weights.\n- Demonstrate adversarial hijacking by appending a single attack key $K_{\\text{attack}}$ whose direction is specified and whose norm is set to a large value.\n- Implement two mitigations:\n    1. Norm clipping: replace each key $K_i$ by $\\tilde{K}_i = \\min\\left(1, \\frac{c}{\\|K_i\\|}\\right) K_i$ for a clip cap $c$, applied to all keys including $K_{\\text{attack}}$.\n    2. Cosine similarity normalization: replace each logit by $\\frac{Q^\\top K_i}{\\|Q\\| \\|K_i\\| + \\varepsilon}$ for a small $\\varepsilon > 0$ before applying the softmax function.\n\nFor each test case below, compute four quantities:\n- The attack token’s attention weight under baseline (attack token norm set to $1$).\n- The attack token’s attention weight under adversarial conditions (attack token norm set to a large value).\n- The attack token’s attention weight with norm clipping applied to adversarial keys (clip cap $c$).\n- The attack token’s attention weight with cosine similarity normalization applied to adversarial keys (with a small $\\varepsilon$ to avoid division by zero).\n\nYour program must use the following test suite. In each case, the attack token is appended as the last key. Use the provided vectors exactly as given.\n\nTest Case $1$ (happy path: aligned query and attack direction):\n- $d_k = 4$\n- $Q = [0.9, 0.1, 0.0, 0.0]$\n- Normal keys: $K_1 = [1, 0, 0, 0]$, $K_2 = [0, 1, 0, 0]$, $K_3 = [0, 0, 1, 0]$\n- Attack direction: $K_{\\text{dir}} = [1, 0, 0, 0]$\n- Baseline attack norm: $1$\n- Adversarial attack norm: $100$\n- Clip cap: $2$\n\nTest Case $2$ (boundary case: zero query vector):\n- $d_k = 4$\n- $Q = [0.0, 0.0, 0.0, 0.0]$\n- Normal keys: $K_1 = [1, 0, 0, 0]$, $K_2 = [0, 1, 0, 0]$, $K_3 = [0, 0, 1, 0]$\n- Attack direction: $K_{\\text{dir}} = [0, 0, 0, 1]$\n- Baseline attack norm: $1$\n- Adversarial attack norm: $100$\n- Clip cap: $2$\n\nTest Case $3$ (orthogonal query and attack direction):\n- $d_k = 4$\n- $Q = [0.0, 1.0, 0.0, 0.0]$\n- Normal keys: $K_1 = [0, 1, 0, 0]$, $K_2 = [0, 0, 1, 0]$, $K_3 = [0, 0, 0, 1]$\n- Attack direction: $K_{\\text{dir}} = [1, 0, 0, 0]$\n- Baseline attack norm: $1$\n- Adversarial attack norm: $100$\n- Clip cap: $2$\n\nTest Case $4$ (extreme adversarial norm):\n- $d_k = 8$\n- $Q = [1.0, -0.5, 0.3, -0.2, 0.0, 0.0, 0.0, 0.0]$\n- Normal keys: $K_1 = [0, 1, 0, 0, 0, 0, 0, 0]$, $K_2 = [0, 0, 1, 0, 0, 0, 0, 0]$, $K_3 = [0, 0, 0, 1, 0, 0, 0, 0]$\n- Attack direction: $K_{\\text{dir}} = [1, 0, 0, 0, 0, 0, 0, 0]$\n- Baseline attack norm: $1$\n- Adversarial attack norm: $10^6$\n- Clip cap: $10$\n\nYour program must compute, for each test case, a list of four floating-point numbers in the order described above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the four-number list for the corresponding test case. For example: $[[w_{1,\\text{base}}, w_{1,\\text{adv}}, w_{1,\\text{clip}}, w_{1,\\text{cos}}],[w_{2,\\text{base}}, w_{2,\\text{adv}}, w_{2,\\text{clip}}, w_{2,\\text{cos}}],\\ldots]$.", "solution": "The problem requires an implementation and demonstration of an adversarial attack on the Scaled Dot-Product Attention (SDPA) mechanism, along with two mitigation strategies. I will first formalize the operations, then outline the computational steps for each part of the problem.\n\n### 1. Fundamental Concepts\n\n**Scaled Dot-Product Attention:**\nThe attention mechanism computes a distribution of weights, $\\alpha_i$, over a set of key-value pairs, based on a query vector $Q$. For a single query $Q \\in \\mathbb{R}^{d_k}$ and a set of keys $\\{K_1, K_2, \\ldots, K_N\\}$, where each $K_i \\in \\mathbb{R}^{d_k}$, the attention logits $e_i$ are computed as:\n$$\ne_i = \\frac{Q^\\top K_i}{\\sqrt{d_k}}\n$$\nHere, $d_k$ is the dimensionality of the key vectors. The attention weights $\\alpha_i$ are then obtained by applying the softmax function to the vector of logits:\n$$\n\\alpha_i = \\text{softmax}(e)_i = \\frac{\\exp(e_i)}{\\sum_{j=1}^{N} \\exp(e_j)}\n$$\nThe output of the attention layer is a weighted sum of value vectors, $\\sum_i \\alpha_i V_i$, which is not part of this problem.\n\n**Adversarial Hijacking:**\nThe inner product $Q^\\top K_i$ is directly proportional to the norm of the key vector, $\\|K_i\\|$. An attacker can exploit this by introducing an adversarial key, $K_{\\text{attack}}$, with an exceptionally large norm. If $K_{\\text{attack}}$ is not orthogonal to $Q$, the resulting dot product $Q^\\top K_{\\text{attack}}$ will be large, leading to a large logit $e_{\\text{attack}}$. Consequently, after exponentiation, $\\exp(e_{\\text{attack}})$ will dominate the sum in the softmax denominator, causing the attention weight $\\alpha_{\\text{attack}}$ to approach $1$. This effectively forces the model to attend almost exclusively to the adversarial token, \"hijacking\" the attention mechanism.\n\n### 2. Mitigation Strategies\n\nTwo mitigation strategies are to be implemented.\n\n**Mitigation 1: Norm Clipping**\nThis method enforces a maximum norm, $c$, on all key vectors. Each key $K_i$ is replaced by a clipped version $\\tilde{K}_i$:\n$$\n\\tilde{K}_i = \\min\\left(1, \\frac{c}{\\|K_i\\|}\\right) K_i\n$$\nIf a key's norm $\\|K_i\\|$ is already less than or equal to the clip cap $c$, it remains unchanged. If $\\|K_i\\| > c$, it is scaled down such that its new norm becomes exactly $c$. This prevents any single key from having an arbitrarily large norm and thus an excessively large influence on the logits. For this calculation, a potential division by zero for a zero-norm key is noted, but not present in the test data.\n\n**Mitigation 2: Cosine Similarity Normalization**\nThis strategy replaces the scaled dot-product logit with the cosine similarity between the query and key vectors. The logit $e_i$ is redefined as:\n$$\ne'_i = \\frac{Q^\\top K_i}{\\|Q\\| \\|K_i\\| + \\varepsilon}\n$$\nCosine similarity is inherently normalized by the magnitudes of the vectors, making it insensitive to their norms. The value of the resulting logit is bounded in $[-1, 1]$. The small constant $\\varepsilon > 0$ is added for numerical stability to prevent division by zero if either $Q$ or $K_i$ is a zero vector. For this implementation, a standard value of $\\varepsilon = 10^{-8}$ will be used. Note that this formulation replaces the entire scaled dot-product, including the $1/\\sqrt{d_k}$ scaling factor.\n\n### 3. Computational Procedure\n\nFor each test case, we must compute the attention weight of the attack token under four distinct scenarios. The attack key, $K_{\\text{attack}}$, is constructed by scaling a given direction vector $K_{\\text{dir}}$ to a specified norm. If $\\|K_{\\text{dir}}\\| \\neq 0$, then $K_{\\text{attack}} = (\\text{norm} / \\|K_{\\text{dir}}\\| ) \\cdot K_{\\text{dir}}$. The attack key is always appended as the last key in the sequence.\n\nLet the set of normal keys be $\\{K_1, \\ldots, K_{N-1}\\}$ and the attack key be $K_N = K_{\\text{attack}}$.\n\n**A. Baseline Attention:**\n1. Construct $K_{\\text attack}}$ using the specified `baseline_attack_norm`.\n2. Form the complete set of keys $\\{K_1, \\ldots, K_{N-1}, K_{\\text{attack}}\\}$.\n3. Compute the scaled dot-product logits $e_i = \\frac{Q^\\top K_i}{\\sqrt{d_k}}$ for all $i=1, \\ldots, N$.\n4. Apply the softmax function to obtain weights $\\{\\alpha_1, \\ldots, \\alpha_N\\}$.\n5. The result is $\\alpha_N$.\n\n**B. Adversarial Attention:**\n1. Construct $K_{\\text{attack}}$ using the `adversarial_attack_norm`.\n2. Form the complete set of keys using this new adversarial key.\n3. Repeat steps 3-5 from the baseline procedure.\n4. The result is the new $\\alpha_N$.\n\n**C. Norm Clipping Mitigation:**\n1. Use the set of keys from the adversarial scenario, including the high-norm $K_{\\text{attack}}$.\n2. Apply the norm clipping formula to every key $K_i$ to produce a set of clipped keys $\\{\\tilde{K}_1, \\ldots, \\tilde{K}_N\\}$, using the specified clip cap $c$.\n3. Compute the scaled dot-product logits using these clipped keys: $\\tilde{e}_i = \\frac{Q^\\top \\tilde{K}_i}{\\sqrt{d_k}}$.\n4. Apply the softmax function to obtain weights $\\{\\tilde{\\alpha}_1, \\ldots, \\tilde{\\alpha}_N\\}$.\n5. The result is $\\tilde{\\alpha}_N$.\n\n**D. Cosine Similarity Mitigation:**\n1. Use the set of keys from the adversarial scenario.\n2. Compute the logits using the cosine similarity formula for each key: $e'_i = \\frac{Q^\\top K_i}{\\|Q\\| \\|K_i\\| + \\varepsilon}$.\n3. Apply the softmax function to these new logits $\\{e'_1, \\ldots, e'_N\\}$ to get weights $\\{\\alpha'_1, \\ldots, \\alpha'_N\\}$.\n4. The result is $\\alpha'_N$.\n\nThese four values, computed in order, constitute the solution for a single test case. The final output aggregates the results from all test cases as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes attentional hijacking and mitigation effects for SDPA.\n    \"\"\"\n    # A small constant for numerical stability in cosine similarity.\n    EPSILON = 1e-8\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"d_k\": 4,\n            \"Q\": [0.9, 0.1, 0.0, 0.0],\n            \"normal_keys\": [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]],\n            \"attack_direction\": [1, 0, 0, 0],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 100,\n            \"clip_cap\": 2,\n        },\n        {\n            \"d_k\": 4,\n            \"Q\": [0.0, 0.0, 0.0, 0.0],\n            \"normal_keys\": [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]],\n            \"attack_direction\": [0, 0, 0, 1],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 100,\n            \"clip_cap\": 2,\n        },\n        {\n            \"d_k\": 4,\n            \"Q\": [0.0, 1.0, 0.0, 0.0],\n            \"normal_keys\": [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]],\n            \"attack_direction\": [1, 0, 0, 0],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 100,\n            \"clip_cap\": 2,\n        },\n        {\n            \"d_k\": 8,\n            \"Q\": [1.0, -0.5, 0.3, -0.2, 0.0, 0.0, 0.0, 0.0],\n            \"normal_keys\": [\n                [0, 1, 0, 0, 0, 0, 0, 0],\n                [0, 0, 1, 0, 0, 0, 0, 0],\n                [0, 0, 0, 1, 0, 0, 0, 0],\n            ],\n            \"attack_direction\": [1, 0, 0, 0, 0, 0, 0, 0],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 1e6,\n            \"clip_cap\": 10,\n        },\n    ]\n\n    def softmax(x):\n        \"\"\"Computes softmax of vector x for numerical stability.\"\"\"\n        if x.size == 0:\n            return np.array([])\n        e_x = np.exp(x - np.max(x))\n        return e_x / np.sum(e_x)\n\n    def create_attack_key(direction, norm):\n        \"\"\"Creates a key vector from a direction and a norm.\"\"\"\n        direction_vec = np.array(direction, dtype=float)\n        dir_norm = np.linalg.norm(direction_vec)\n        if dir_norm == 0:\n            return np.zeros_like(direction_vec)\n        return norm * (direction_vec / dir_norm)\n\n    def calculate_sdpa_weights(Q, keys, d_k):\n        \"\"\"Computes weights using scaled dot-product attention.\"\"\"\n        logits = np.array([np.dot(Q, k) for k in keys]) / np.sqrt(d_k)\n        return softmax(logits)\n\n    def calculate_cosine_sim_weights(Q, keys):\n        \"\"\"Computes weights using cosine similarity logits.\"\"\"\n        Q_norm = np.linalg.norm(Q)\n        logits = []\n        for k in keys:\n            k_norm = np.linalg.norm(k)\n            dot_product = np.dot(Q, k)\n            logit = dot_product / (Q_norm * k_norm + EPSILON)\n            logits.append(logit)\n        return softmax(np.array(logits))\n\n    results = []\n    for case in test_cases:\n        Q_vec = np.array(case[\"Q\"], dtype=float)\n        normal_keys_vecs = [np.array(k, dtype=float) for k in case[\"normal_keys\"]]\n        d_k = case[\"d_k\"]\n        clip_cap = case[\"clip_cap\"]\n\n        # 1. Baseline calculation\n        K_attack_base = create_attack_key(case[\"attack_direction\"], case[\"baseline_attack_norm\"])\n        all_keys_base = normal_keys_vecs + [K_attack_base]\n        weights_base = calculate_sdpa_weights(Q_vec, all_keys_base, d_k)\n        w_base = weights_base[-1]\n\n        # 2. Adversarial calculation\n        K_attack_adv = create_attack_key(case[\"attack_direction\"], case[\"adversarial_attack_norm\"])\n        all_keys_adv = normal_keys_vecs + [K_attack_adv]\n        weights_adv = calculate_sdpa_weights(Q_vec, all_keys_adv, d_k)\n        w_adv = weights_adv[-1]\n\n        # 3. Norm clipping mitigation\n        all_keys_clipped = []\n        for k in all_keys_adv:\n            k_norm = np.linalg.norm(k)\n            # Use min(1, c/||K||) * K formulation\n            scale_factor = min(1.0, clip_cap / k_norm if k_norm > 0 else 1.0)\n            all_keys_clipped.append(k * scale_factor)\n        weights_clip = calculate_sdpa_weights(Q_vec, all_keys_clipped, d_k)\n        w_clip = weights_clip[-1]\n\n        # 4. Cosine similarity mitigation\n        weights_cos = calculate_cosine_sim_weights(Q_vec, all_keys_adv)\n        w_cos = weights_cos[-1]\n\n        results.append([w_base, w_adv, w_clip, w_cos])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3193536"}]}