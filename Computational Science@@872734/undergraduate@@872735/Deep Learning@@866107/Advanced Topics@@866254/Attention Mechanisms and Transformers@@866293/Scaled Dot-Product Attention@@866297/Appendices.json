{"hands_on_practices": [{"introduction": "To build a solid intuition for scaled dot-product attention, we will start with the simplest possible case: where queries, keys, and values are all single numbers (i.e., the key dimension $d_k=1$). This exercise [@problem_id:3172468] strips away the matrix algebra to reveal the core mechanism, showing how attention weights are derived from simple scalar products and how the scaling factor behaves in this fundamental scenario. By working through this calculation, you'll see how the mechanism functions as a sophisticated, weighted-averaging scheme based on similarity.", "problem": "A system uses the mechanism commonly called attention to combine a set of scalar values based on their similarity to a scalar query. In the edge case where the key dimension equals one, the similarity between a query and a key reduces to their scalar product. Let the query be $q$, the keys be $k_1, k_2, k_3$, and the values be $v_1, v_2, v_3$, all real numbers. Assume that the conventional scaling by the square-root of the key dimension is applied to the similarity scores before transforming them by a normalized exponential mapping. Starting from the definition of the dot product for scalars and the definition of the normalized exponential mapping, derive the weights assigned to each value and use them to compute the single scalar output of the attention operation for the following data:\n$q = 2$, $k_1 = 0.1$, $k_2 = 0.2$, $k_3 = 0.5$, $v_1 = 1$, $v_2 = -1$, $v_3 = 3$, with key dimension $d_k = 1$.\nRound your final numeric result to four significant figures and express it as a pure number without units.", "solution": "The goal is to construct the attention output from first principles in the case where the key dimension equals one. The fundamental bases we use are the definition of a dot product and the definition of the normalized exponential mapping, typically referred to as the $\\mathrm{softmax}$ function.\n1. Compatibility scores from scalar dot products and conventional scaling. With key dimension $d_k = 1$, the square-root scaling factor is $\\sqrt{d_k} = \\sqrt{1} = 1$. For a scalar query $q$ and scalar keys $k_i$, the similarity (compatibility) scores are\n$$\ns_i = \\frac{q \\cdot k_i}{\\sqrt{d_k}} = q\\,k_i.\n$$\n2. Normalized exponential mapping ($\\mathrm{softmax}$). Given a set of scores $(s_1, s_2, s_3)$, the attention weights are obtained by exponentiating the scores and normalizing so they sum to one:\n$$\n\\alpha_i = \\frac{\\exp(s_i)}{\\sum_{j=1}^{3} \\exp(s_j)}.\n$$\n3. Attention output as a convex combination of values. The attention output $y$ is the weighted sum of the values:\n$$\ny = \\sum_{i=1}^{3} \\alpha_i v_i.\n$$\nWe now apply these steps to the given numerical data with $q = 2$, $k_1 = 0.1$, $k_2 = 0.2$, $k_3 = 0.5$, $v_1 = 1$, $v_2 = -1$, $v_3 = 3$, and $d_k = 1$.\n- Compute the scores using $s_i = q k_i$:\n$$\ns_1 = 2 \\cdot 0.1 = 0.2,\\quad s_2 = 2 \\cdot 0.2 = 0.4,\\quad s_3 = 2 \\cdot 0.5 = 1.0.\n$$\n- Exponentiate the scores:\n$$\n\\exp(s_1) = \\exp(0.2),\\quad \\exp(s_2) = \\exp(0.4),\\quad \\exp(s_3) = \\exp(1.0).\n$$\nKeep these symbolic until the final numeric step. The normalization constant is\n$$\nZ = \\exp(0.2) + \\exp(0.4) + \\exp(1.0).\n$$\n- The weights are\n$$\n\\alpha_1 = \\frac{\\exp(0.2)}{Z},\\quad \\alpha_2 = \\frac{\\exp(0.4)}{Z},\\quad \\alpha_3 = \\frac{\\exp(1.0)}{Z}.\n$$\n- The output is\n$$\ny = \\alpha_1 \\cdot 1 + \\alpha_2 \\cdot (-1) + \\alpha_3 \\cdot 3 = \\frac{\\exp(0.2) - \\exp(0.4) + 3\\exp(1.0)}{Z}.\n$$\nSubstitute numerical evaluations for the exponentials to produce the final number:\n$$\n\\exp(0.2) \\approx 1.221402758160170,\\quad \\exp(0.4) \\approx 1.491824697641270,\\quad \\exp(1.0) \\approx 2.718281828459045.\n$$\nThen\n$$\nZ \\approx 1.221402758160170 + 1.491824697641270 + 2.718281828459045 \\approx 5.431509284260485,\n$$\nand\n$$\n\\text{numerator} \\approx 1.221402758160170 - 1.491824697641270 + 3 \\cdot 2.718281828459045 \\approx 7.884423545896035.\n$$\nThus\n$$\ny \\approx \\frac{7.884423545896035}{5.431509284260485} \\approx 1.45160822\\ldots\n$$\nRounded to four significant figures, this yields $1.452$.\nInterpretation in relation to scalar correlation models: When $d_k = 1$, the compatibility $s_i = q k_i$ is simply the product of two scalars. For a positive query $q$, larger (more positive) keys $k_i$ produce larger $s_i$, hence larger weights after the normalized exponential mapping. This behavior mirrors a simple scalar correlation model where alignment is captured by the product of two scalar signals, and the $\\mathrm{softmax}$ provides a normalized, positive weighting that emphasizes higher correlations while maintaining a convex combination of the values.", "answer": "$$\\boxed{1.452}$$", "id": "3172468"}, {"introduction": "While the full attention mechanism is powerful, computing it for every token in a long sequence can be computationally expensive. This practice [@problem_id:3172461] introduces a practical optimization strategy: certainty-based pruning, where we skip the full computation for tokens where the attention is highly focused. You will implement this heuristic and quantify the classic trade-off between computational speedup and model accuracy, a central challenge in deploying large-scale models.", "problem": "Consider a single-head Scaled Dot-Product Attention (SDPA) mechanism operating on a set of queries and keys in a real vector space of dimension $d$, with corresponding value vectors. Let there be $n_q$ query vectors $\\{q_i\\}_{i=1}^{n_q}$, $n_k$ key vectors $\\{k_j\\}_{j=1}^{n_k}$, and $n_k$ value vectors $\\{v_j\\}_{j=1}^{n_k}$, all in $\\mathbb{R}^d$. The keys are assumed to be unit-norm. Using the fundamental definitions of the Euclidean inner product and the softmax function, SDPA maps each query $q_i$ to an attention distribution over keys and then to an output vector constructed from the values. Define the certainty-based pruning rule: if the largest attention probability for a query $q_i$ exceeds a threshold $\\tau$ (that is, if $\\max_j \\alpha^{(i)}_j > \\tau$), then approximate the output of SDPA for that query by the single value vector associated with the maximizing key, and skip the downstream per-token computation. Otherwise, compute the full attention output and keep the downstream computation.\n\nYou must produce a program that:\n- Computes the baseline SDPA output for each $q_i$ without pruning.\n- Applies the certainty-based pruning rule per token for a given $\\tau$, replacing the output with the maximizing value vector when the rule triggers, and otherwise using the baseline SDPA output.\n- Quantifies the accuracy loss as the average squared Euclidean distance between the pruned outputs and the baseline outputs across all queries, expressed as a decimal number (no percentage sign).\n- Quantifies the speedup as the ratio of baseline total operation count to pruned total operation count, under the following cost model:\n    - Computing all scaled inner products $s_j$ for one query has cost $n_k \\cdot d$.\n    - Applying the softmax for one query has cost $2 \\cdot n_k$.\n    - Computing the full weighted sum of values for one query has cost $n_k \\cdot d$.\n    - Copying a single value vector when pruning has cost $d$.\n    - A downstream per-token cost $C_{\\mathrm{tail}}$ is incurred when no pruning occurs and is fully skipped when pruning occurs.\n  Under this model, for each query, the baseline cost is $(n_k \\cdot d) + (2 \\cdot n_k) + (n_k \\cdot d) + C_{\\mathrm{tail}}$, while the pruned-path cost is $(n_k \\cdot d) + (2 \\cdot n_k) + d$.\n\nUse the following fixed data for all test cases:\n- Dimensionality $d = 3$.\n- Number of queries $n_q = 3$ and number of keys $n_k = 3$.\n- Keys (already unit-norm): $k_1 = [1, 0, 0]$, $k_2 = [0, 1, 0]$, $k_3 = [0, 0, 1]$.\n- Values: $v_1 = [1, 0, 0]$, $v_2 = [0, 1, 0]$, $v_3 = [0, 0, 1]$.\n- Queries: $q_1 = [2.0, -2.0, -2.0]$, $q_2 = [0.0, 1.8, -1.8]$, $q_3 = [-1.8, 0.0, 1.8]$.\n\nFor softmax stability, subtract $\\max_j s_j$ from the logits before exponentiation; this stabilizes the computation without changing the probabilities.\n\nTest suite:\n- Case 1: threshold $\\tau = 0.7$, downstream cost $C_{\\mathrm{tail}} = 500$.\n- Case 2: threshold $\\tau = 0.95$, downstream cost $C_{\\mathrm{tail}} = 500$.\n- Case 3: threshold $\\tau = 0.0$, downstream cost $C_{\\mathrm{tail}} = 500$.\n- Case 4: threshold $\\tau = 1.0$, downstream cost $C_{\\mathrm{tail}} = 500$.\n- Case 5: threshold $\\tau = 0.6$, downstream cost $C_{\\mathrm{tail}} = 50$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a two-element list $[S, L]$ with $S$ the speedup (as a decimal float) and $L$ the accuracy loss (as a decimal float). For example: $[[S_1, L_1],[S_2, L_2],[S_3, L_3],[S_4, L_4],[S_5, L_5]]$.", "solution": "The problem requires an analysis of a certainty-based pruning heuristic for the Scaled Dot-Product Attention (SDPA) mechanism. We will first establish the baseline computation for SDPA, then apply the specified pruning rule, and finally quantify the trade-off between computational speedup and accuracy loss for several test cases.\n\nThe problem provides the following data:\n- The dimension of the vector space is $d=3$.\n- The number of query vectors is $n_q=3$.\n- The number of key vectors is $n_k=3$.\n- The query matrix $Q \\in \\mathbb{R}^{n_q \\times d}$ is given by:\n$$\nQ = \\begin{pmatrix} 2.0  -2.0  -2.0 \\\\ 0.0  1.8  -1.8 \\\\ -1.8  0.0  1.8 \\end{pmatrix}\n$$\n- The key matrix $K \\in \\mathbb{R}^{n_k \\times d}$ is given by:\n$$\nK = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n- The value matrix $V \\in \\mathbb{R}^{n_k \\times d}$ is given by:\n$$\nV = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\nIt is noted that the key vectors (rows of $K$) are unit-norm.\n\nThe core operation of SDPA is defined as:\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V\n$$\n\nLet's dissect this computation for a single query vector $q_i$ (the $i$-th row of $Q$).\n\n1.  **Scaled Dot-Product Scores**: For each query $q_i$, we compute its dot product with every key vector $k_j$ (the $j$-th row of $K$) and scale by the inverse square root of the dimension $d$. These are the attention scores or logits, $s^{(i)}_j$.\n    $$\n    s^{(i)}_j = \\frac{q_i \\cdot k_j}{\\sqrt{d}}\n    $$\n    In matrix form, we compute the matrix of scores $M \\in \\mathbb{R}^{n_q \\times n_k}$ as $M = \\frac{QK^T}{\\sqrt{d}}$. With $d=3$, we have:\n    $$\n    M = \\frac{1}{\\sqrt{3}} QK^T = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 2.0  -2.0  -2.0 \\\\ 0.0  1.8  -1.8 \\\\ -1.8  0.0  1.8 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}^T \\approx \\begin{pmatrix} 1.1547  -1.1547  -1.1547 \\\\ 0.0  1.0392  -1.0392 \\\\ -1.0392  0.0  1.0392 \\end{pmatrix}\n    $$\n\n2.  **Attention Probabilities**: The scores are converted into probabilities $\\alpha^{(i)}_j$ using the softmax function, applied row-wise to the matrix $M$. For numerical stability, we subtract the maximum score of each row before exponentiation. For the $i$-th query:\n    $$\n    \\alpha^{(i)}_j = \\frac{\\exp(s^{(i)}_j - \\max_l s^{(i)}_l)}{\\sum_{l=1}^{n_k} \\exp(s^{(i)}_l - \\max_l s^{(i)}_l)}\n    $$\n    Let $A \\in \\mathbb{R}^{n_q \\times n_k}$ be the matrix of attention probabilities. Its rows $A_{i,:}$ are the probability distributions $\\{\\alpha^{(i)}_j\\}_{j=1}^{n_k}$.\n    Applying this to $M$, we obtain the attention matrix $A$:\n    $$\n    A \\approx \\begin{pmatrix} 0.8343  0.0828  0.0828 \\\\ 0.2392  0.6762  0.0846 \\\\ 0.0846  0.2392  0.6762 \\end{pmatrix}\n    $$\n\n3.  **Baseline Output Vectors**: The output vector $o_i$ for query $q_i$ is the weighted sum of the value vectors, using the attention probabilities as weights.\n    $$\n    o_i = \\sum_{j=1}^{n_k} \\alpha^{(i)}_j v_j\n    $$\n    In matrix form, the baseline output matrix $O \\in \\mathbb{R}^{n_q \\times d}$ is $O = AV$. Since in this problem $V$ is the identity matrix, the baseline output matrix is simply the attention matrix, $O = A$.\n\n4.  **Certainty-Based Pruning**: The pruning rule is applied to each query $q_i$.\n    - Let $j^* = \\arg\\max_j \\alpha^{(i)}_j$ be the index of the key with the highest attention probability.\n    - If $\\alpha^{(i)}_{j^*} > \\tau$, the rule triggers. The pruned output $o'_i$ is approximated as the single value vector corresponding to the most-attended key: $o'_i = v_{j^*}$.\n    - Otherwise, the full attention output is used: $o'_i = o_i$.\n\n5.  **Metric Calculation**:\n    - **Accuracy Loss ($L$)**: The accuracy loss is the average squared Euclidean distance between the baseline and pruned outputs:\n      $$\n      L = \\frac{1}{n_q} \\sum_{i=1}^{n_q} \\|o_i - o'_i\\|^2\n      $$\n    - **Speedup ($S$)**: The speedup is the ratio of total baseline cost to total pruned path cost. The costs per query are defined as:\n      - Baseline cost: $C_{\\text{base}} = 2n_k d + 2n_k + C_{\\text{tail}} = 2(3)(3) + 2(3) + C_{\\text{tail}} = 24 + C_{\\text{tail}}$.\n      - Pruned-path cost: $C_{\\text{pruned}} = n_k d + 2n_k + d = (3)(3) + 2(3) + 3 = 18$.\n      - Let $P$ be the set of indices of pruned queries. The total pruned cost is $\\sum_{i \\in P} C_{\\text{pruned}} + \\sum_{i \\notin P} C_{\\text{base}}$.\n      - The speedup is $S = \\frac{n_q \\cdot C_{\\text{base}}}{\\sum_{i \\in P} C_{\\text{pruned}} + \\sum_{i \\notin P} C_{\\text{base}}}$.\n\nWe now apply this procedure to each test case.\n\n**General Calculations:**\nThe maximum attention probabilities for each query are:\n- For $q_1$: $\\max_j \\alpha^{(1)}_j \\approx 0.8343$.\n- For $q_2$: $\\max_j \\alpha^{(2)}_j \\approx 0.6762$.\n- For $q_3$: $\\max_j \\alpha^{(3)}_j \\approx 0.6762$.\n\n**Case 1: $\\tau = 0.7$, $C_{\\text{tail}} = 500$**\n- $C_{\\text{base}} = 24 + 500 = 524$. $C_{\\text{pruned}} = 18$.\n- For $q_1$: $0.8343 > 0.7$, so we prune. $j^*=1$. $o'_1 = v_1 = [1,0,0]$. Cost is $18$.\n- For $q_2$: $0.6762 \\le 0.7$, no pruning. $o'_2 = o_2$. Cost is $524$.\n- For $q_3$: $0.6762 \\le 0.7$, no pruning. $o'_3 = o_3$. Cost is $524$.\n- Total pruned cost = $18 + 524 + 524 = 1066$.\n- Total baseline cost = $3 \\times 524 = 1572$.\n- $S_1 = 1572 / 1066 \\approx 1.47467$.\n- Loss: $L_1 = \\frac{1}{3} (\\|o_1 - v_1\\|^2 + \\|o_2 - o_2\\|^2 + \\|o_3 - o_3\\|^2) = \\frac{1}{3} \\|o_1 - v_1\\|^2 \\approx \\frac{1}{3} (0.04118) \\approx 0.01373$.\n\n**Case 2: $\\tau = 0.95$, $C_{\\text{tail}} = 500$**\n- $C_{\\text{base}} = 524$.\n- For all queries, $\\max_j \\alpha^{(i)}_j \\le 0.95$. No queries are pruned.\n- Total pruned cost = $3 \\times 524 = 1572$.\n- Total baseline cost = $1572$.\n- $S_2 = 1572 / 1572 = 1.0$.\n- Loss: No pruning occurs, so $o'_i = o_i$ for all $i$. $L_2 = 0.0$.\n\n**Case 3: $\\tau = 0.0$, $C_{\\text{tail}} = 500$**\n- $C_{\\text{base}} = 524$, $C_{\\text{pruned}} = 18$.\n- For all queries, $\\max_j \\alpha^{(i)}_j > 0.0$. All queries are pruned.\n- $j^*$ for $q_1$ is $1$; for $q_2$ is $2$; for $q_3$ is $3$. $o'_1=v_1, o'_2=v_2, o'_3=v_3$.\n- Total pruned cost = $3 \\times 18 = 54$.\n- Total baseline cost = $1572$.\n- $S_3 = 1572 / 54 \\approx 29.11111$.\n- Loss: $L_3 = \\frac{1}{3} (\\|o_1 - v_1\\|^2 + \\|o_2 - v_2\\|^2 + \\|o_3 - v_3\\|^2) \\approx \\frac{1}{3}(0.04118 + 0.16923 + 0.16923) \\approx 0.12655$.\n\n**Case 4: $\\tau = 1.0$, $C_{\\text{tail}} = 500$**\n- $C_{\\text{base}} = 524$.\n- Since $\\sum_j \\alpha^{(i)}_j = 1$, $\\max_j \\alpha^{(i)}_j \\le 1$. The rule $\\max > 1.0$ can never trigger. No queries are pruned.\n- Result is identical to Case 2: $S_4 = 1.0$, $L_4 = 0.0$.\n\n**Case 5: $\\tau = 0.6$, $C_{\\text{tail}} = 50$**\n- $C_{\\text{base}} = 24 + 50 = 74$. $C_{\\text{pruned}} = 18$.\n- For all queries, $\\max_j \\alpha^{(i)}_j > 0.6$. All queries are pruned.\n- Total pruned cost = $3 \\times 18 = 54$.\n- Total baseline cost = $3 \\times 74 = 222$.\n- $S_5 = 222 / 54 \\approx 4.11111$.\n- Loss: The loss calculation is independent of $C_{\\text{tail}}$. All queries are pruned, similar to Case 3. $L_5 = L_3 \\approx 0.12655$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes speedup and accuracy loss for a certainty-based pruning heuristic\n    in Scaled Dot-Product Attention.\n    \"\"\"\n    \n    # Define fixed data from the problem statement.\n    d = 3.0\n    nq = 3\n    nk = 3\n    \n    Q = np.array([\n        [2.0, -2.0, -2.0],\n        [0.0, 1.8, -1.8],\n        [-1.8, 0.0, 1.8]\n    ])\n    \n    K = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0]\n    ])\n    \n    V = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0]\n    ])\n\n    # Step 1: Compute baseline SDPA output.\n    # Scaled dot-product scores\n    sqrt_d = np.sqrt(d)\n    scaled_scores = (Q @ K.T) / sqrt_d\n    \n    # Numerically stable softmax to get attention probabilities\n    max_scores = scaled_scores.max(axis=1, keepdims=True)\n    exp_scores = np.exp(scaled_scores - max_scores)\n    attention_probs = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n    \n    # Baseline output vectors\n    baseline_outputs = attention_probs @ V\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.7, 500),  # Case 1\n        (0.95, 500), # Case 2\n        (0.0, 500),  # Case 3\n        (1.0, 500),  # Case 4\n        (0.6, 50),   # Case 5\n    ]\n    \n    results = []\n    \n    for tau, C_tail in test_cases:\n        # Define costs based on the model\n        cost_base = 2 * nk * d + 2 * nk + C_tail\n        cost_pruned = nk * d + 2 * nk + d\n        \n        total_pruned_cost = 0.0\n        total_squared_error = 0.0\n        \n        pruned_outputs = np.zeros_like(baseline_outputs)\n        \n        for i in range(nq):\n            probs_i = attention_probs[i]\n            max_prob = np.max(probs_i)\n            \n            is_pruned = max_prob > tau\n            \n            baseline_output_i = baseline_outputs[i]\n            \n            if is_pruned:\n                # Pruning rule triggers\n                max_idx = np.argmax(probs_i)\n                pruned_output_i = V[max_idx]\n                total_pruned_cost += cost_pruned\n            else:\n                # Full computation\n                pruned_output_i = baseline_output_i\n                total_pruned_cost += cost_base\n                \n            # Calculate squared Euclidean distance for this query\n            squared_error_i = np.sum((baseline_output_i - pruned_output_i)**2)\n            total_squared_error += squared_error_i\n            \n        # Calculate final metrics for the test case\n        # Accuracy Loss\n        accuracy_loss = total_squared_error / nq\n        \n        # Speedup\n        total_baseline_cost = nq * cost_base\n        speedup = total_baseline_cost / total_pruned_cost if total_pruned_cost > 0 else float('inf')\n        \n        results.append([speedup, accuracy_loss])\n\n    # Final print statement in the exact required format.\n    # The format string uses default float formatting which is sufficient.\n    formatted_results = [f\"[{s},{l}]\" for s, l in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3172461"}, {"introduction": "The stability and performance of a Transformer often depend on subtle details beyond the core attention formula. This exercise [@problem_id:3172395] challenges you to think like a model architect, analyzing how an imbalance in the initialization of query and key projection weights can lead to training issues like softmax saturation and skewed gradients. By reasoning through the statistical consequences, you will gain a deeper appreciation for why techniques like Layer Normalization are not just add-ons but essential components for robust models.", "problem": "Consider a single-head scaled dot-product attention mechanism in deep learning. Let an input sequence be represented by a matrix $X \\in \\mathbb{R}^{n \\times d_{\\text{model}}}$, and let queries and keys be computed by linear maps $W_Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d}$ and $W_K \\in \\mathbb{R}^{d_{\\text{model}} \\times d}$ as $Q = X W_Q$ and $K = X W_K$. For a fixed query vector $q \\in \\mathbb{R}^d$ (one row of $Q$) and keys $\\{k_j\\}_{j=1}^m$ (rows of $K$), the attention logits are $z_j = \\frac{q^\\top k_j}{\\sqrt{d}}$, and the attention weights are $a_j = \\operatorname{softmax}(z)_j$. Suppose training uses a standard cross-entropy loss against a one-hot target over keys for this query. Assume the following initialization regime and statistical model at the start of training: the entries of $X$ are independent and identically distributed with zero mean and variance $\\sigma_x^2$, $W_Q$ and $W_K$ are independent of $X$, and their row norms are approximately uniform across rows. Let the Frobenius norms satisfy $\\lVert W_Q \\rVert_F = \\alpha \\lVert W_K \\rVert_F$ with $\\alpha  1$. Under these assumptions, reason from first principles about how the attention logits and gradients behave.\n\nWhich of the following statements are most accurate? Select all that apply.\n\nA. Under the stated statistical model, the variance of each logit $z_j$ increases by a factor on the order of $\\alpha^2$ compared to the balanced case $\\alpha = 1$, leading to more peaked attention distributions (softmax saturation) at initialization.\n\nB. The gradient with respect to $W_Q$ will generally be larger in magnitude than the gradient with respect to $W_K$ at initialization, because the larger $q$ amplifies upstream gradients uniformly.\n\nC. At initialization, the gradient with respect to $W_K$ can be larger in magnitude than that with respect to $W_Q$, because each per-key gradient scales proportionally to $q$, while the per-query gradient scales with a weighted sum of the comparatively smaller $k_j$ vectors.\n\nD. Replacing the scale factor $\\frac{1}{\\sqrt{d}}$ by $\\frac{1}{d}$ in the logits completely corrects the scale imbalance induced by $\\alpha  1$ without introducing other adverse effects.\n\nE. Applying Layer Normalization (LN) on the query and key vectors, or $\\ell_2$-normalizing $q$ and $k_j$ to unit norms before the dot product, or introducing a temperature parameter $\\tau  1$ to downscale the logits, are principled normalization strategies to mitigate the imbalance and soften softmax saturation.", "solution": "### Derivation from First Principles\n\n**1. Statistical Properties of Query and Key Vectors**\n\nLet the query vector $q$ be derived from the $i$-th input vector $x_i$ (a row of $X$), and a key vector $k_j$ from the $j$-th input vector $x_j$.\n$q = W_Q^\\top x_i$ and $k_j = W_K^\\top x_j$.\nThe entries of $x_i$ and $x_j$ are i.i.d. with $\\mathbb{E}[(x_i)_l] = 0$ and $\\mathbb{E}[(x_i)_l^2] = \\sigma_x^2$. This implies $\\mathbb{E}[x_i] = 0$ and the covariance matrix is $\\mathbb{E}[x_i x_i^\\top] = \\sigma_x^2 I_{d_{\\text{model}}}$.\nThe expectation of the query and key vectors is zero:\n$\\mathbb{E}[q] = W_Q^\\top \\mathbb{E}[x_i] = 0$.\n$\\mathbb{E}[k_j] = W_K^\\top \\mathbb{E}[x_j] = 0$.\n\nLet's compute the expected squared Euclidean norm of these vectors.\n$\\mathbb{E}[\\lVert q \\rVert^2] = \\mathbb{E}[q^\\top q] = \\mathbb{E}[\\operatorname{Tr}(q q^\\top)] = \\operatorname{Tr}(\\mathbb{E}[q q^\\top])$.\n$\\mathbb{E}[q q^\\top] = \\mathbb{E}[W_Q^\\top x_i x_i^\\top W_Q] = W_Q^\\top \\mathbb{E}[x_i x_i^\\top] W_Q = W_Q^\\top (\\sigma_x^2 I) W_Q = \\sigma_x^2 W_Q^\\top W_Q$.\nSo, $\\mathbb{E}[\\lVert q \\rVert^2] = \\operatorname{Tr}(\\sigma_x^2 W_Q^\\top W_Q) = \\sigma_x^2 \\operatorname{Tr}(W_Q^\\top W_Q) = \\sigma_x^2 \\lVert W_Q \\rVert_F^2$.\nSimilarly, $\\mathbb{E}[\\lVert k_j \\rVert^2] = \\sigma_x^2 \\lVert W_K \\rVert_F^2$.\n\nGiven the condition $\\lVert W_Q \\rVert_F = \\alpha \\lVert W_K \\rVert_F$, we have:\n$$ \\frac{\\mathbb{E}[\\lVert q \\rVert^2]}{\\mathbb{E}[\\lVert k_j \\rVert^2]} = \\frac{\\sigma_x^2 \\lVert W_Q \\rVert_F^2}{\\sigma_x^2 \\lVert W_K \\rVert_F^2} = \\alpha^2 $$\nThus, at initialization, the query vectors are expected to have a squared norm that is $\\alpha^2$ times larger than that of the key vectors.\n\n**2. Statistical Properties of Attention Logits**\n\nThe attention logit is $z_j = \\frac{q^\\top k_j}{\\sqrt{d}}$. Assuming the query and key are derived from different input tokens ($i \\neq j$), $x_i$ and $x_j$ are independent.\nThe expectation of the logit is $\\mathbb{E}[z_j] = \\frac{1}{\\sqrt{d}} \\mathbb{E}[q^\\top k_j] = \\frac{1}{\\sqrt{d}} \\mathbb{E}[q]^\\top \\mathbb{E}[k_j] = 0$.\nThe variance is $\\operatorname{Var}(z_j) = \\mathbb{E}[z_j^2]$.\n$\\mathbb{E}[z_j^2] = \\frac{1}{d} \\mathbb{E}[(q^\\top k_j)^2] = \\frac{1}{d} \\mathbb{E}[\\operatorname{Tr}(q q^\\top k_j k_j^\\top)]$.\nDue to independence of $q$ and $k_j$, this becomes $\\frac{1}{d} \\operatorname{Tr}(\\mathbb{E}[q q^\\top] \\mathbb{E}[k_j k_j^\\top])$.\n$\\operatorname{Var}(z_j) = \\frac{1}{d} \\operatorname{Tr}((\\sigma_x^2 W_Q^\\top W_Q) (\\sigma_x^2 W_K^\\top W_K)) = \\frac{\\sigma_x^4}{d} \\operatorname{Tr}(W_Q^\\top W_Q W_K^\\top W_K)$.\n\nTo see how this variance depends on $\\alpha$, we can use a scaling argument. Assume $W_Q$ is statistically equivalent to $\\alpha W_K$ (e.g., its elements are drawn from a distribution with std. dev. scaled by $\\alpha$).\nThen $q = W_Q^\\top x_i \\approx \\alpha (W_K^\\top x_i)$.\n$z_j = \\frac{q^\\top k_j}{\\sqrt{d}} \\approx \\alpha \\frac{(W_K^\\top x_i)^\\top (W_K^\\top x_j)}{\\sqrt{d}}$.\nThis suggests the logit $z_j$ is scaled by $\\alpha$ compared to the balanced case where $W_Q$ has the same norm as $W_K$.\nTherefore, $\\operatorname{Var}(z_j)$ is scaled by $\\alpha^2$.\n\n**3. Gradient Analysis**\n\nThe loss is $L = -\\log(a_p)$, where $p$ is the index of the one-hot target. The gradient of the loss with respect to the logits is $\\frac{\\partial L}{\\partial z_j} = a_j - y_j$, where $y_j = \\delta_{jp}$ is the one-hot target.\n\nUsing the chain rule, we find the gradients with respect to the weight matrices $W_Q$ and $W_K$.\nThe gradient with respect to a single query $q$ is $\\frac{\\partial L}{\\partial q} = \\sum_j \\frac{\\partial L}{\\partial z_j}\\frac{\\partial z_j}{\\partial q} = \\sum_j (a_j-y_j) \\frac{k_j}{\\sqrt{d}}$.\nThe gradient with respect to a single key $k_j$ is $\\frac{\\partial L}{\\partial k_j} = \\frac{\\partial L}{\\partial z_j}\\frac{\\partial z_j}{\\partial k_j} = (a_j-y_j) \\frac{q}{\\sqrt{d}}$.\n\nPropagating to the weight matrices using $\\frac{\\partial L}{\\partial W} = x (\\frac{\\partial L}{\\partial v})^\\top$ for $v=W^\\top x$:\n$\\nabla_{W_Q} L = \\frac{\\partial L}{\\partial W_Q} = x_i \\left(\\frac{\\partial L}{\\partial q}\\right)^\\top = \\frac{1}{\\sqrt{d}} x_i \\left( \\sum_j (a_j-y_j) k_j \\right)^\\top$.\n$\\nabla_{W_K} L = \\frac{\\partial L}{\\partial W_K} = \\sum_j x_j \\left(\\frac{\\partial L}{\\partial k_j}\\right)^\\top = \\frac{1}{\\sqrt{d}} \\sum_j (a_j-y_j) x_j q^\\top$.\n\nTo compare the magnitudes, we examine their Frobenius norms.\n$\\lVert \\nabla_{W_Q} L \\rVert_F = \\frac{1}{\\sqrt{d}} \\lVert x_i \\rVert_2 \\left\\lVert \\sum_j (a_j-y_j) k_j \\right\\rVert_2$.\n$\\lVert \\nabla_{W_K} L \\rVert_F = \\frac{1}{\\sqrt{d}} \\left\\lVert \\sum_j (a_j-y_j) x_j \\right\\rVert_2 \\lVert q \\rVert_2$.\n\nAt initialization, $a_j \\approx 1/m$. The terms $(a_j-y_j)$ are of order $O(1)$.\nThe sum $\\sum_j (a_j-y_j) k_j$ is a weighted sum of key vectors. Its norm will be proportional to the typical norm of a key vector, $\\mathbb{E}[\\lVert k \\rVert_2]$.\nThe sum $\\sum_j (a_j-y_j) x_j$ is a weighted sum of input vectors, with norm proportional to $\\mathbb{E}[\\lVert x \\rVert_2]$. Also, $\\lVert x_i \\rVert$ is of the same order.\nSo, very roughly:\n$\\lVert \\nabla_{W_Q} L \\rVert_F \\propto \\mathbb{E}[\\lVert k \\rVert_2]$.\n$\\lVert \\nabla_{W_K} L \\rVert_F \\propto \\mathbb{E}[\\lVert q \\rVert_2]$.\n\nSince we found that $\\mathbb{E}[\\lVert q \\rVert^2] = \\alpha^2 \\mathbb{E}[\\lVert k \\rVert^2]$, it follows that $\\mathbb{E}[\\lVert q \\rVert] \\approx \\alpha \\mathbb{E}[\\lVert k \\rVert]$.\nTherefore, we expect the magnitude of the gradient with respect to $W_K$ to be approximately $\\alpha$ times larger than the magnitude of the gradient with respect to $W_Q$.\n\n### Evaluation of Options\n\n**A. Under the stated statistical model, the variance of each logit $z_j$ increases by a factor on the order of $\\alpha^2$ compared to the balanced case $\\alpha = 1$, leading to more peaked attention distributions (softmax saturation) at initialization.**\n\nThis statement is consistent with our derivation. As shown in section (2), the variance of the logits $\\operatorname{Var}(z_j)$ is proportional to $\\alpha^2$. An increase in the variance of the inputs to a softmax function causes its output distribution to become less uniform and more \"peaked\" or \"saturated,\" where one output probability approaches $1$ and the others approach $0$.\n\n**Verdict: Correct**\n\n**B. The gradient with respect to $W_Q$ will generally be larger in magnitude than the gradient with respect to $W_K$ at initialization, because the larger $q$ amplifies upstream gradients uniformly.**\n\nThis statement is contradicted by our analysis in section (3). The magnitude of the gradient $\\nabla_{W_K} L$ is proportional to $\\lVert q \\rVert$, while the magnitude of $\\nabla_{W_Q} L$ is proportional to a weighted average of $\\lVert k_j \\rVert$. Since $\\lVert q \\rVert$ is larger than $\\lVert k_j \\rVert$ by a factor of $\\alpha > 1$, the gradient with respect to $W_K$ will be larger, not smaller.\n\n**Verdict: Incorrect**\n\n**C. At initialization, the gradient with respect to $W_K$ can be larger in magnitude than that with respect to $W_Q$, because each per-key gradient scales proportionally to $q$, while the per-query gradient scales with a weighted sum of the comparatively smaller $k_j$ vectors.**\n\nThis statement aligns perfectly with our gradient analysis. As derived, $\\|\\nabla_{W_K} L\\|_F \\propto \\|q\\|$ and $\\|\\nabla_{W_Q} L\\|_F \\propto$ weighted average norm of $k_j$. Since $\\alpha>1$, we have $\\mathbb{E}[\\lVert q \\rVert] > \\mathbb{E}[\\lVert k_j \\rVert]$. The reasoning provided in the option is a correct qualitative summary of the gradient expressions.\n\n**Verdict: Correct**\n\n**D. Replacing the scale factor $\\frac{1}{\\sqrt{d}}$ by $\\frac{1}{d}$ in the logits completely corrects the scale imbalance induced by $\\alpha > 1$ without introducing other adverse effects.**\n\nReplacing the scaling factor changes the logits to $z_j' = q^\\top k_j / d$. This reduces the variance of all logits by a factor of $d$. While this would reduce logit magnitude and thus combat saturation, it does not address the fundamental imbalance in the norms of $q$ and $k_j$. The ratio of the gradient norms, $\\|\\nabla_{W_K} L\\| / \\|\\nabla_{W_Q} L\\| \\approx \\alpha$, would remain unchanged. Furthermore, this aggressive downscaling of logits by an extra factor of $\\sqrt{d}$ can lead to an adverse effect known as over-smoothing or softmax \"undersaturation,\" where the attention distribution becomes nearly uniform ($a_j \\approx 1/m$), hindering the model's ability to focus on relevant keys. The claim that it \"completely corrects\" the imbalance \"without introducing other adverse effects\" is false.\n\n**Verdict: Incorrect**\n\n**E. Applying Layer Normalization (LN) on the query and key vectors, or $\\ell_2$-normalizing $q$ and $k_j$ to unit norms before the dot product, or introducing a temperature parameter $\\tau > 1$ to downscale the logits, are principled normalization strategies to mitigate the imbalance and soften softmax saturation.**\n\nLet's evaluate each strategy's effect on the twofold problem of (1) softmax saturation and (2) gradient/norm imbalance.\n1.  **Layer Normalization (LN) or $\\ell_2$-normalization**: Both methods rescale $q$ and $k_j$ to have fixed norms before the dot product. This directly equalizes their effective magnitudes in the attention score calculation, regardless of $\\alpha$. This a) stabilizes the logit variance, preventing saturation, and b) balances the inputs to the dot product, which in turn helps to balance the gradient magnitudes flowing back to $W_Q$ and $W_K$. These are indeed principled strategies that mitigate both issues.\n2.  **Temperature $\\tau > 1$**: This strategy modifies the logits to $z_j / \\tau$. This directly reduces the logit variance by $1/\\tau^2$, softening the softmax and mitigating saturation. However, it does not fix the source imbalance between $q$ and $k_j$ norms. Both gradients $\\nabla_{W_Q} L$ and $\\nabla_{W_K} L$ are scaled by $1/\\tau$, leaving their relative magnitude ratio of $\\approx \\alpha$ unchanged. Nevertheless, preventing saturation-induced vanishing gradients is a critical part of mitigating the pathological training dynamics. In the context of deep learning practice, controlling temperature is a standard and principled method to improve training stability. Therefore, it is reasonable to classify it as a strategy that \"mitigates\" the overall problem, which includes both imbalance and saturation.\n\nGiven that all three are standard, principled techniques used to improve the stability of attention mechanisms, the statement is accurate.\n\n**Verdict: Correct**", "answer": "$$\\boxed{ACE}$$", "id": "3172395"}]}