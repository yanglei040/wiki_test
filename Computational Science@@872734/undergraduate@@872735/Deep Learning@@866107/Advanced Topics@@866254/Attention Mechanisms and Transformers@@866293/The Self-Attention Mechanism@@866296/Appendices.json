{"hands_on_practices": [{"introduction": "At the core of the self-attention mechanism lies the softmax function, which transforms raw scores into a valid probability distribution. However, the direct computation of $\\exp(x)$ can be numerically unstable in finite-precision arithmetic, leading to overflow or underflow. This first practice [@problem_id:3192585] guides you through the derivation and implementation of the 'log-sum-exp' trick, a fundamental technique for ensuring the stability of softmax, and asks you to quantify its impact across different floating-point precisions.", "problem": "Consider a self-attention scoring setup where a set of energies $e_{ij}$ is computed for queries indexed by $i$ against keys indexed by $j$, and attention weights are obtained by applying the softmax function row-wise to the energies. The softmax function for a row vector $\\mathbf{e}_i$ is defined from first principles as $softmax(\\mathbf{e}_i)_j = \\exp(e_{ij}) \\big/ \\sum_{k} \\exp(e_{ik})$, where $\\exp$ denotes the exponential function and $\\sum$ denotes finite summation. The computation of $\\log \\sum_{j} \\exp(e_{ij})$ is central to numerical analysis of softmax since $\\log$ is the inverse of $\\exp$ and converts products into sums. The Institute of Electrical and Electronics Engineers (IEEE) $754$ standard floating-point formats $fp16$ (binary $16$) and $fp32$ (binary $32$) differ in precision and dynamic range, which affects numerical stability and rounding error when evaluating $\\exp$, $\\log$, and $softmax$.\n\nStarting only from the definitions of the exponential function $\\exp$, the natural logarithm $\\log$, and the softmax function $softmax$, derive a numerically stable transformation for computing $\\log \\sum_{j} \\exp(e_{ij})$ for each row $i$ and explain why the stable transformation reduces overflow and underflow without changing the mathematical value. Then, using that stable transformation, design an algorithm that computes row-wise $softmax(\\mathbf{e}_i)$ in a specified floating-point precision while mitigating catastrophic numerical issues.\n\nImplement the algorithm to quantify numerical error across floating-point precisions by comparing $fp16$ and $fp32$ stabilized softmax outputs to a high-precision $fp64$ (binary $64$) stabilized reference. For each test case below, produce the following four quantities:\n- The maximum absolute difference between $fp16$ stabilized softmax and the $fp64$ stabilized reference across all entries of the test matrix (a real number).\n- The maximum absolute difference between $fp32$ stabilized softmax and the $fp64$ stabilized reference across all entries of the test matrix (a real number).\n- The maximum absolute difference, across rows, between the naive direct computation $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$ and the stabilized computation of the same quantity in $fp64$ (a real number; if the naive value is not finite for a row, treat its contribution to this maximum as $nan$).\n- A boolean indicating whether any row’s naive $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$ was not finite in $fp64$ (i.e., overflow to $+\\infty$ or propagation of $nan$).\n\nUse the following test suite of energy matrices, each understood as row-wise inputs $\\mathbf{e}_i$ to $softmax$. All entries are in unitless real values. You must treat each matrix row independently in computations. The matrices are:\n- Case $1$ (moderate values): $$E^{(1)} = \\begin{bmatrix} -1 & 0 & 1 & 2 \\\\ 0.5 & -0.5 & 3 & -3 \\end{bmatrix}.$$\n- Case $2$ (extreme range): $$E^{(2)} = \\begin{bmatrix} 1000 & -1000 & 0 & 1 \\\\ 88 & 87 & 86 & 85 \\end{bmatrix}.$$\n- Case $3$ (uniform inputs): $$E^{(3)} = \\begin{bmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}.$$\n- Case $4$ (very negative values): $$E^{(4)} = \\begin{bmatrix} -1000 & -1001 & -999 & -1200 \\\\ -50 & -60 & -70 & -80 \\end{bmatrix}.$$\n\nAlgorithmic requirements:\n- For each matrix $E^{(k)}$, compute a stabilized $fp64$ reference softmax row-wise from the definition of $softmax$ and your derived stable transformation.\n- For each matrix $E^{(k)}$, compute stabilized $softmax$ row-wise in $fp16$ and $fp32$ using the same algorithm but performing the arithmetic in the specified precision.\n- For each matrix $E^{(k)}$, compute the naive $fp64$ values $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$ row-wise directly from the definitions, and compare to the stabilized $fp64$ values of the same quantity.\n\nAnswer format specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the $k$-th element is a list of four items corresponding to Case $k$ in the order described above. For example, the final output should look like `[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3],[a_4,b_4,c_4,d_4]]`, with no spaces inserted beyond what is necessary to delimit numbers and booleans.\n\nNo external input is required. All computations are unitless real numbers. Angles are not involved. Express all difference quantities as real numbers. The final line must be the only output.", "solution": "The goal is to compute row-wise attention weights by applying the softmax function to energies $e_{ij}$ while maintaining numerical stability and then to measure precision-dependent numerical error. We begin with the core definitions and properties:\n\n$1.$ Definition of softmax for a row vector $\\mathbf{e}_i$:\n$$softmax(\\mathbf{e}_i)_j = \\frac{\\exp(e_{ij})}{\\sum_{k} \\exp(e_{ik})}.$$\nSoftmax maps real-valued scores $e_{ij}$ to nonnegative weights that sum to $1$, a requirement for attention distributions in self-attention.\n\n$2.$ Fundamental properties of the exponential and logarithm:\nThe exponential function $\\exp$ is strictly increasing and maps $\\mathbb{R}$ to $(0,\\infty)$. The natural logarithm $\\log$ is its inverse on $(0,\\infty)$ and satisfies $\\log(ab) = \\log(a) + \\log(b)$ for $a>0$ and $b>0$. These properties allow controlled manipulation of sums of exponentials.\n\n$3.$ Numerical issue:\nDirectly computing $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$ can overflow if any $e_{ij}$ is very large (making $\\exp(e_{ij})$ exceed representable range) or underflow if $e_{ij}$ is very negative (making $\\exp(e_{ij})$ round to $0$). Such overflow or underflow compromises the correctness of the softmax denominator and thereby of $softmax(\\mathbf{e}_i)$.\n\nTo derive a numerically stable transformation for $\\log \\sum_{j} \\exp(e_{ij})$, we use factorization based on the logarithm-exponential relationship. Let $m_i$ be the maximum element of the row $\\mathbf{e}_i$, i.e., $m_i = \\max_j e_{ij}$. Then:\n\n$$\n\\sum_{j} \\exp(e_{ij}) = \\sum_{j} \\exp\\left((e_{ij} - m_i) + m_i\\right) = \\sum_{j} \\left[\\exp(e_{ij} - m_i)\\cdot \\exp(m_i)\\right] = \\exp(m_i)\\cdot \\sum_{j} \\exp(e_{ij} - m_i).\n$$\n\nApplying $\\log$ and the property $\\log(ab) = \\log(a) + \\log(b)$ yields\n\n$$\n\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right) = \\log\\left( \\exp(m_i)\\cdot \\sum_{j} \\exp(e_{ij} - m_i) \\right) = m_i + \\log \\left( \\sum_{j} \\exp(e_{ij} - m_i) \\right).\n$$\n\nThis transformation is numerically stable because:\n- The term $e_{ij} - m_i \\le 0$ for all $j$, so the largest shifted value is $0$ and its exponential is exactly $1$, avoiding overflow even if $m_i$ is large.\n- Very negative values of $e_{ij}$ produce $e_{ij} - m_i \\ll 0$ whose exponentials are very small; underflowing these contributions to $0$ has only a small effect on the sum because they are already negligible relative to the largest term.\n- The overall mathematical value is preserved by algebraic identity; the transformation does not alter $\\log \\sum_{j} \\exp(e_{ij})$.\n\nFor stabilized softmax, we use the same shift. For a row $\\mathbf{e}_i$, define $m_i = \\max_j e_{ij}$ and compute\n\n$$\nsoftmax(\\mathbf{e}_i)_j = \\frac{\\exp(e_{ij} - m_i)}{\\sum_{k} \\exp(e_{ik} - m_i)}.\n$$\n\nThis leaves the softmax values unchanged mathematically because the common multiplicative factor $\\exp(m_i)$ cancels in numerator and denominator, but it dramatically improves numerical stability by preventing overflow in the numerator and denominator.\n\nFloating-point considerations:\n- Under the Institute of Electrical and Electronics Engineers (IEEE) $754$ standard, $fp16$ (binary $16$) has a smaller dynamic range and fewer fraction bits than $fp32$ (binary $32$), which in turn is less precise than $fp64$ (binary $64$). Smaller precision increases rounding error and increases the likelihood of underflow in very small exponentials and overflow in very large exponentials if unshifted.\n- By computing softmax with the stabilized algorithm and performing arithmetic in $fp16$, $fp32$, and $fp64$, we can quantify the impact of precision on the final attention weights by measuring the maximum absolute difference from a high-precision $fp64$ reference.\n\nAlgorithmic design:\n- For each test matrix $E^{(k)}$, compute a stabilized $fp64$ reference softmax row-wise using the shift-by-maximum technique described above.\n- For the same inputs, compute stabilized softmax in $fp16$ and $fp32$ by performing the same operations with arrays in the specified dtype. Convert the outputs to $fp64$ for comparison.\n- Separately, for each row, compute the naive $fp64$ value $\\log\\left( \\sum_{j} \\exp(e_{ij}) \\right)$ directly. This can produce non-finite values (such as $+\\infty$) when there is overflow in $\\exp(e_{ij})$, particularly for very large positive $e_{ij}$. Compare this naive value to the stabilized $fp64$ value $m_i + \\log\\left( \\sum_{j} \\exp(e_{ij} - m_i) \\right)$ and report the maximum absolute difference across rows. If the naive computation is not finite for a row, that row’s contribution to the maximum is $nan$, and the boolean overflow indicator is set to true if any such non-finite values occur.\n\nTest suite coverage rationale:\n- Case $1$ uses moderate values, representing a typical regime where naive computations are safe and rounding errors are small.\n- Case $2$ includes a row with an extremely large positive value $1000$ mixed with very negative values $-1000$ and a moderate range $0$ to $1$, which causes overflow in the naive $\\exp$ and tests the necessity of stabilization; the second row uses values $88$ to $85$, large yet finite in $fp64$.\n- Case $3$ uses uniform zeros, producing exactly uniform softmax weights and testing precision effects on symmetry.\n- Case $4$ uses very negative values to study underflow behavior in lower precision formats and sensitivity of softmax to tiny exponentials.\n\nOutput specification:\n- For each matrix $E^{(k)}$, output a list $[a_k, b_k, c_k, d_k]$ where $a_k$ is the maximum absolute difference for $fp16$ stabilized softmax against the $fp64$ reference, $b_k$ is the corresponding quantity for $fp32$, $c_k$ is the maximum absolute difference between naive and stabilized $\\log \\sum_{j} \\exp(e_{ij})$ in $fp64$ aggregated over rows (with non-finite naive values contributing $nan$), and $d_k$ is a boolean indicating whether any row’s naive computation was not finite. Aggregate the four case results into a single comma-separated list enclosed in square brackets.\n\nThis principled design links the mathematical identities of $\\exp$ and $\\log$ to the algorithmic choices used in self-attention softmax computations, demonstrating both the theoretical stability and the empirical precision-dependent error characteristics.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef logsumexp_naive(row: np.ndarray) -> float:\n    \"\"\"Compute naive log(sum(exp(row))) in float64.\"\"\"\n    row64 = row.astype(np.float64)\n    s = np.sum(np.exp(row64))\n    return float(np.log(s))\n\ndef logsumexp_stable(row: np.ndarray) -> float:\n    \"\"\"Compute stable logsumexp in float64: m + log(sum(exp(row - m))).\"\"\"\n    row64 = row.astype(np.float64)\n    m = np.max(row64)\n    shifted = row64 - m\n    s = np.sum(np.exp(shifted))\n    return float(m + np.log(s))\n\ndef softmax_stable(matrix: np.ndarray, dtype: np.dtype) -> np.ndarray:\n    \"\"\"\n    Compute row-wise stabilized softmax in the specified dtype.\n    Returns results as float64 for comparison.\n    \"\"\"\n    x = matrix.astype(dtype)\n    # Row-wise max\n    m = np.max(x, axis=1, keepdims=True)\n    # Shift and exponentiate in dtype; cast back to dtype explicitly\n    shifted = (x - m).astype(dtype)\n    exps = np.exp(shifted).astype(dtype)\n    sums = np.sum(exps, axis=1, keepdims=True).astype(dtype)\n    # Avoid division by zero: if sum is zero (underflow), result remains zero\n    soft = exps / sums\n    return soft.astype(np.float64)\n\ndef max_abs_diff(a: np.ndarray, b: np.ndarray) -> float:\n    \"\"\"Maximum absolute difference between two arrays.\"\"\"\n    return float(np.max(np.abs(a - b)))\n\ndef any_nonfinite(arr: np.ndarray) -> bool:\n    \"\"\"Check if any element is non-finite.\"\"\"\n    return bool(np.any(~np.isfinite(arr)))\n\ndef format_item(item):\n    \"\"\"Format item without spaces, recursively for lists.\"\"\"\n    if isinstance(item, list):\n        return \"[\" + \",\".join(format_item(x) for x in item) + \"]\"\n    elif isinstance(item, float):\n        # Ensure Python's default float string is used (includes 'nan' or 'inf' if present)\n        return str(item)\n    elif isinstance(item, (int, np.integer)):\n        return str(int(item))\n    elif isinstance(item, (bool, np.bool_)):\n        return \"True\" if bool(item) else \"False\"\n    else:\n        return str(item)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([[-1.0, 0.0, 1.0, 2.0],\n                  [0.5, -0.5, 3.0, -3.0]], dtype=np.float64),\n        np.array([[1000.0, -1000.0, 0.0, 1.0],\n                  [88.0, 87.0, 86.0, 85.0]], dtype=np.float64),\n        np.array([[0.0, 0.0, 0.0, 0.0],\n                  [0.0, 0.0, 0.0, 0.0]], dtype=np.float64),\n        np.array([[-1000.0, -1001.0, -999.0, -1200.0],\n                  [-50.0, -60.0, -70.0, -80.0]], dtype=np.float64),\n    ]\n\n    results = []\n    for E in test_cases:\n        # Reference stabilized softmax in float64\n        soft_ref = softmax_stable(E, np.float64)\n\n        # Stabilized softmax in fp16 and fp32\n        soft16 = softmax_stable(E, np.float16)\n        soft32 = softmax_stable(E, np.float32)\n\n        # Error metrics (max absolute difference w.r.t. fp64 reference)\n        err16 = max_abs_diff(soft16, soft_ref)\n        err32 = max_abs_diff(soft32, soft_ref)\n\n        # Naive vs stabilized logsumexp in float64, row-wise\n        naive_vals = []\n        stable_vals = []\n        nonfinite_flag = False\n        diffs = []\n        for i in range(E.shape[0]):\n            row = E[i, :]\n            naive = logsumexp_naive(row)\n            stable = logsumexp_stable(row)\n            naive_vals.append(naive)\n            stable_vals.append(stable)\n            if not np.isfinite(naive):\n                nonfinite_flag = True\n                diffs.append(np.nan)\n            else:\n                diffs.append(abs(naive - stable))\n\n        # Max absolute difference (nan propagated if present)\n        # If any nan present, np.nanmax ignores nan and returns max of finite values.\n        # If all are nan, result is nan.\n        diffs_arr = np.array(diffs, dtype=np.float64)\n        if np.all(np.isnan(diffs_arr)):\n            max_logsumexp_diff = float(np.nan)\n        else:\n            max_logsumexp_diff = float(np.nanmax(diffs_arr))\n\n        results.append([err16, err32, max_logsumexp_diff, nonfinite_flag])\n\n    # Final print statement in the exact required format: nested list without extra spaces.\n    print(format_item(results))\n\nsolve()\n```", "id": "3192585"}, {"introduction": "Beyond just applying attention as a generic component, it is crucial to understand its expressive power and how its internal structure can be engineered to solve specific tasks. This exercise [@problem_id:3192596] challenges you to construct a single attention head from first principles that solves the parity problem, a task requiring alternating focus. By carefully designing the query, key, and value projections in conjunction with sinusoidal positional encodings, you will gain a deep, constructive understanding of how attention patterns are formed.", "problem": "You are asked to design and implement a complete, runnable program that constructs a toy dataset in which a single scaled dot-product self-attention head alternates its focus across positions using sinusoidal positional encodings, and then quantitatively analyzes the resulting attention patterns with respect to a parity task.\n\nThe fundamental bases you must use are:\n\n- The definition of a single-head scaled dot-product self-attention mechanism. Given queries $Q \\in \\mathbb{R}^{T \\times d_k}$, keys $K \\in \\mathbb{R}^{T \\times d_k}$, and values $V \\in \\mathbb{R}^{T \\times d_v}$ for a sequence of length $T$, the attention output at query index $t$ is\n$$\n\\mathrm{Attn}(t) = \\sum_{s=0}^{T-1} \\alpha_{t,s} V_s,\n\\quad\n\\alpha_{t,s} = \\frac{\\exp\\left(z_{t,s}\\right)}{\\sum_{u=0}^{T-1} \\exp\\left(z_{t,u}\\right)},\n\\quad\nz_{t,s} = \\frac{\\langle Q_t, K_s \\rangle}{\\sqrt{d_k}},\n$$\nwith $\\langle \\cdot , \\cdot \\rangle$ denoting the standard Euclidean dot product.\n\n- Sinusoidal positional encodings parameterized by an angle $\\theta_p$ in radians. You must use $\\theta_p = \\pi p$ for position index $p \\in \\{0,1,\\dots,T-1\\}$. The positional encoding is the two-dimensional vector $[\\sin(\\theta_p), \\cos(\\theta_p)]$.\n\n- The trigonometric identity for all real angles $a$ and $b$:\n$$\n\\cos(a-b) = \\cos(a)\\cos(b) + \\sin(a)\\sin(b).\n$$\n\nYour construction must satisfy the following specifications:\n\n- Inputs are binary sequences $x \\in \\{0,1\\}^T$. For each position $p$, define an embedding vector $e_p \\in \\mathbb{R}^3$ by concatenating the token value and its positional encoding:\n$$\ne_p = \\big[x_p,\\, \\sin(\\theta_p),\\, \\cos(\\theta_p)\\big].\n$$\nAll sines and cosines are to be computed in radians.\n\n- You must construct linear projections $W_Q \\in \\mathbb{R}^{3 \\times d_k}$, $W_K \\in \\mathbb{R}^{3 \\times d_k}$, and $W_V \\in \\mathbb{R}^{3 \\times 1}$ for a single attention head with key-query dimension $d_k = 2$ and value dimension $d_v = 1$, such that:\n  - The query $Q_t \\in \\mathbb{R}^{2}$ and key $K_s \\in \\mathbb{R}^{2}$ depend only on the positional components of $e_t$ and $e_s$.\n  - The value $V_s \\in \\mathbb{R}$ depends only on the token component $x_s$ of $e_s$.\n  - Consequently, the unnormalized attention score $z_{t,s}$ depends only on the positional relationship between $t$ and $s$.\n\n- Introduce a non-negative scalar temperature $\\beta \\in \\mathbb{R}_{\\ge 0}$ that multiplies the score $z_{t,s}$ before the softmax, that is, use scores $z_{t,s}^{(\\beta)} = \\beta \\cdot \\langle Q_t, K_s \\rangle$ in place of $z_{t,s}$, and do not change $d_k$ elsewhere. This separates the effect of the magnitude of the dot product from the sharpness of the softmax.\n\n- Focus your attention analysis on the query position $t = 0$. With $\\theta_p = \\pi p$, positions of even index and odd index have opposite phase in $\\cos(\\theta_p)$, which leads to an alternating attention pattern across positions when $z_{t,s}^{(\\beta)}$ depends on $\\theta_t - \\theta_s$. Quantify this alternation by the fidelity metric\n$$\nF = \\sum_{\\substack{s \\in \\{0,\\dots,T-1\\}\\\\ s \\text{ even}}} \\alpha_{0,s} \\;-\\; \\sum_{\\substack{s \\in \\{0,\\dots,T-1\\}\\\\ s \\text{ odd}}} \\alpha_{0,s}.\n$$\n\n- Define the target label for each sequence $x$ as the parity\n$$\ny(x) = \\left(\\sum_{p=0}^{T-1} x_p\\right) \\bmod 2.\n$$\n\n- Using only the single head described above and the attention weights for the query $t=0$, recover exact parity as follows. Because your design must make $\\alpha_{0,s}$ constant within the two parity groups $\\{s \\,:\\, s \\text{ even}\\}$ and $\\{s \\,:\\, s \\text{ odd}\\}$, you can compute within-group weighted means of the token values and then re-scale by group sizes to obtain exact group sums:\n  - Let $E = \\{s \\in \\{0,\\dots,T-1\\} \\,:\\, s \\text{ even}\\}$ and $O = \\{s \\in \\{0,\\dots,T-1\\} \\,:\\, s \\text{ odd}\\}$. Define\n  $$\n  \\mu_E(x) = \\frac{\\sum_{s \\in E} \\alpha_{0,s} \\, x_s}{\\sum_{s \\in E} \\alpha_{0,s}}, \\quad\n  \\mu_O(x) = \\frac{\\sum_{s \\in O} \\alpha_{0,s} \\, x_s}{\\sum_{s \\in O} \\alpha_{0,s}},\n  $$\n  with the convention that if a denominator is zero because the corresponding set is empty, then the corresponding mean is defined as $0$ and the group size is treated as $0$. Let $k_E = |E|$ and $k_O = |O|$. Form the predicted total sum\n  $$\n  \\widehat{S}(x) = k_E \\cdot \\mu_E(x) + k_O \\cdot \\mu_O(x),\n  $$\n  and the predicted parity\n  $$\n  \\widehat{y}(x) = \\big\\lfloor \\widehat{S}(x) + \\tfrac{1}{2} \\big\\rfloor \\bmod 2.\n  $$\n  This procedure must be implemented numerically; it must not make use of ground-truth labels during inference beyond computing accuracy.\n\n- For each test case below, evaluate the alternation fidelity $F$ and the classification accuracy\n$$\n\\mathrm{Acc} = \\frac{1}{2^T} \\sum_{x \\in \\{0,1\\}^T} \\mathbf{1}\\{\\widehat{y}(x) = y(x)\\},\n$$\nover the exhaustive dataset of all binary sequences of length $T$.\n\nAngle unit requirement: all trigonometric functions must use radians.\n\nTest suite and program output:\n\n- Use the following test cases, each specified as a pair $(T,\\beta)$:\n  - Case $1$: $(T,\\beta) = (8, 2.0)$.\n  - Case $2$: $(T,\\beta) = (1, 3.0)$.\n  - Case $3$: $(T,\\beta) = (7, 0.0)$.\n  - Case $4$: $(T,\\beta) = (10, 5.0)$.\n\n- For each case, compute two floats: the alternation fidelity $F$ for query $t=0$, and the accuracy $\\mathrm{Acc}$ as a decimal in $[0,1]$. Round both numbers to $6$ decimal places in the final output.\n\n- Your program should produce a single line of output containing all results in order as a comma-separated list enclosed in square brackets, that is,\n`[F_1,Acc_1,F_2,Acc_2,F_3,Acc_3,F_4,Acc_4]`,\nwith each float rounded to $6$ decimal places and with no spaces anywhere in the line.\n\nThe program must be self-contained, require no input, and must deterministically compute the required outputs using only the definitions above and the specified test cases. No randomization is allowed. All trigonometric computations must use radians. The result types must be floats as specified.", "solution": "The user-provided problem statement has been analyzed and validated as scientifically sound, well-posed, and internally consistent.\n\nThe problem requires the design of a specialized single-head self-attention mechanism to solve a parity task on binary sequences. The design must leverage sinusoidal positional encodings to create an alternating attention pattern. We will first construct the required projection matrices, then derive the analytical form of the attention weights and the fidelity metric $F$, and finally analyze the accuracy of the parity prediction mechanism.\n\n### Step 1: Construction of Projection Matrices\n\nThe input embedding for a token $x_p \\in \\{0, 1\\}$ at position $p$ is given by $e_p = [x_p, \\sin(\\theta_p), \\cos(\\theta_p)] \\in \\mathbb{R}^3$, where the positional angle is $\\theta_p = \\pi p$ radians. We need to construct weight matrices $W_Q \\in \\mathbb{R}^{3 \\times 2}$, $W_K \\in \\mathbb{R}^{3 \\times 2}$, and $W_V \\in \\mathbb{R}^{3 \\times 1}$ (since $d_k=2, d_v=1$) subject to several constraints.\n\n1.  The queries $Q_t = e_t W_Q$ and keys $K_s = e_s W_K$ must depend only on the positional components of the embeddings. This implies that the first row of both $W_Q$ and $W_K$ must be zero.\n2.  The values $V_s = e_s W_V$ must depend only on the token component $x_s$. This implies that the second and third rows of $W_V$ must be zero.\n3.  The unnormalized attention score, which is a function of $\\langle Q_t, K_s \\rangle$, should depend on the difference in positions, $\\theta_t - \\theta_s$. The provided trigonometric identity $\\cos(a-b) = \\cos(a)\\cos(b) + \\sin(a)\\sin(b)$ suggests a path.\n\nLet the positional part of the embedding be the vector $\\vec{p}_p = [\\sin(\\theta_p), \\cos(\\theta_p)]$. The dot product $\\langle\\vec{p}_t, \\vec{p}_s\\rangle$ is precisely $\\cos(\\theta_t - \\theta_s)$. We can achieve this if the queries and keys are simply the positional parts of the embeddings. We choose the sub-matrices acting on the positional components to be identity matrices. A simple choice for the weight matrices that satisfies all constraints is:\n$$\nW_Q = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad W_K = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad W_V = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nWith these matrices, we have:\n-   Query at position $t$: $Q_t = e_t W_Q = [\\sin(\\theta_t), \\cos(\\theta_t)]$.\n-   Key at position $s$: $K_s = e_s W_K = [\\sin(\\theta_s), \\cos(\\theta_s)]$.\n-   Value at position $s$: $V_s = e_s W_V = [x_s]$.\n\nThe dot product of the query and key is $\\langle Q_t, K_s \\rangle = \\sin(\\theta_t)\\sin(\\theta_s) + \\cos(\\theta_t)\\cos(\\theta_s) = \\cos(\\theta_t - \\theta_s)$. With $\\theta_p=\\pi p$, this becomes $\\langle Q_t, K_s \\rangle = \\cos(\\pi(t-s))$.\n\n### Step 2: Derivation of Attention Weights and Fidelity\n\nThe problem specifies using temperature-scaled scores $z_{t,s}^{(\\beta)} = \\beta \\cdot \\langle Q_t, K_s \\rangle$ in the softmax function, replacing the standard scaled dot-product formulation. Thus, the unnormalized scores are:\n$$\nz_{t,s}^{(\\beta)} = \\beta \\cos(\\pi(t-s))\n$$\nThe attention weights $\\alpha_{t,s}$ are given by the softmax function applied to these scores. We are interested in the query at position $t=0$:\n$$\n\\alpha_{0,s} = \\frac{\\exp\\left(z_{0,s}^{(\\beta)}\\right)}{\\sum_{u=0}^{T-1} \\exp\\left(z_{0,u}^{(\\beta)}\\right)}\n$$\nFor $t=0$, the scores simplify: $z_{0,s}^{(\\beta)} = \\beta \\cos(-\\pi s) = \\beta \\cos(\\pi s)$. The value of $\\cos(\\pi s)$ is $1$ if $s$ is an even integer and $-1$ if $s$ is an odd integer.\n$$\nz_{0,s}^{(\\beta)} = \\begin{cases} \\beta & \\text{if } s \\text{ is even} \\\\ -\\beta & \\text{if } s \\text{ is odd} \\end{cases}\n$$\nLet $E = \\{s \\in \\{0, \\dots, T-1\\} : s \\text{ is even}\\}$ and $O = \\{s \\in \\{0, \\dots, T-1\\} : s \\text{ is odd}\\}$. Let their sizes be $k_E = |E| = \\lceil T/2 \\rceil$ and $k_O = |O| = \\lfloor T/2 \\rfloor$. The attention weights $\\alpha_{0,s}$ will take on one of two values:\n-   For $s \\in E$: $\\alpha_{0,s} = \\alpha_{\\text{even}} = \\frac{e^{\\beta}}{k_E e^{\\beta} + k_O e^{-\\beta}}$\n-   For $s \\in O$: $\\alpha_{0,s} = \\alpha_{\\text{odd}} = \\frac{e^{-\\beta}}{k_E e^{\\beta} + k_O e^{-\\beta}}$\n\nThe alternation fidelity metric $F$ at $t=0$ is defined as $F = \\sum_{s \\in E} \\alpha_{0,s} - \\sum_{s \\in O} \\alpha_{0,s}$. Substituting the constant weights:\n$$\nF = k_E \\alpha_{\\text{even}} - k_O \\alpha_{\\text{odd}} = \\frac{k_E e^{\\beta} - k_O e^{-\\beta}}{k_E e^{\\beta} + k_O e^{-\\beta}}\n$$\nThis formula can be computed directly for each test case $(T, \\beta)$.\n\n### Step 3: Analysis of Parity Prediction and Accuracy\n\nThe problem defines a procedure to predict the parity of a sequence $x \\in \\{0,1\\}^T$. The true parity is $y(x) = (\\sum_{p=0}^{T-1} x_p) \\pmod 2$. The prediction mechanism uses weighted means of token values over even and odd positions.\n$$\n\\mu_E(x) = \\frac{\\sum_{s \\in E} \\alpha_{0,s} \\, x_s}{\\sum_{s \\in E} \\alpha_{0,s}}, \\quad \\mu_O(x) = \\frac{\\sum_{s \\in O} \\alpha_{0,s} \\, x_s}{\\sum_{s \\in O} \\alpha_{0,s}}\n$$\nSince $\\alpha_{0,s}$ is constant within each group (even/odd), we can simplify these expressions. For $T>0$, $k_E > 0$ and $\\alpha_{\\text{even}} > 0$, so the denominator for $\\mu_E(x)$ is non-zero.\n$$\n\\mu_E(x) = \\frac{\\alpha_{\\text{even}} \\sum_{s \\in E} x_s}{k_E \\alpha_{\\text{even}}} = \\frac{1}{k_E} \\sum_{s \\in E} x_s\n$$\nSimilarly, if $k_O > 0$:\n$$\n\\mu_O(x) = \\frac{\\alpha_{\\text{odd}} \\sum_{s \\in O} x_s}{k_O \\alpha_{\\text{odd}}} = \\frac{1}{k_O} \\sum_{s \\in O} x_s\n$$\nIf $k_O = 0$ (which occurs for $T=1$), the problem convention sets $\\mu_O(x) = 0$.\n\nThe predicted total sum is $\\widehat{S}(x) = k_E \\mu_E(x) + k_O \\mu_O(x)$. Substituting the expressions for the means:\n-   If $k_E > 0$ and $k_O > 0$:\n    $$ \\widehat{S}(x) = k_E \\left(\\frac{1}{k_E} \\sum_{s \\in E} x_s\\right) + k_O \\left(\\frac{1}{k_O} \\sum_{s \\in O} x_s\\right) = \\sum_{s \\in E} x_s + \\sum_{s \\in O} x_s = \\sum_{p=0}^{T-1} x_p $$\n-   If $k_O = 0$ (i.e., $T=1$):\n    $$ \\widehat{S}(x) = k_E \\mu_E(x) + 0 \\cdot \\mu_O(x) = 1 \\cdot \\left(\\frac{1}{1} \\sum_{s \\in E} x_s\\right) + 0 = x_0 = \\sum_{p=0}^{0} x_p $$\nIn all cases where $T>0$, the predicted sum $\\widehat{S}(x)$ is exactly the true sum of the tokens $S(x) = \\sum_{p=0}^{T-1} x_p$.\n\nThe predicted parity is $\\widehat{y}(x) = \\lfloor \\widehat{S}(x) + 0.5 \\rfloor \\pmod 2$. Since $S(x)$ is always an integer (as it is a sum of binary values), we have $\\lfloor S(x) + 0.5 \\rfloor = S(x)$. Therefore:\n$$\n\\widehat{y}(x) = S(x) \\pmod 2 = y(x)\n$$\nThe predicted parity is identical to the true parity for every sequence $x$. This holds true irrespective of the sequence length $T>0$ and the temperature $\\beta$. Consequently, the accuracy $\\mathrm{Acc}$ over the entire dataset $\\{0,1\\}^T$ must be exactly $1.0$.\n\nThe problem requires a numerical implementation of this process. Due to the exact algebraic cancellations, numerical floating-point errors are the only potential source of deviation. However, for standard double-precision arithmetic, these errors will be negligible, and the rounding operation $\\lfloor \\cdot + 0.5 \\rfloor$ ensures robustness, leading to a calculated accuracy of $1.0$.\n\n### Step 4: Numerical Evaluation\n\nWe now apply these formulas to the specified test cases.\n\n-   **Case 1:** $(T, \\beta) = (8, 2.0)$. $k_E=4$, $k_O=4$.\n    $F = \\frac{4e^2 - 4e^{-2}}{4e^2 + 4e^{-2}} = \\frac{e^2 - e^{-2}}{e^2 + e^{-2}} = \\tanh(2.0) \\approx 0.964028$. $\\mathrm{Acc} = 1.0$.\n-   **Case 2:** $(T, \\beta) = (1, 3.0)$. $k_E=1$, $k_O=0$.\n    $F = \\frac{1e^3 - 0}{1e^3 + 0} = 1.0$. $\\mathrm{Acc} = 1.0$.\n-   **Case 3:** $(T, \\beta) = (7, 0.0)$. $k_E=4$, $k_O=3$.\n    $F = \\frac{4e^0 - 3e^{0}}{4e^0 + 3e^{0}} = \\frac{4-3}{4+3} = \\frac{1}{7} \\approx 0.142857$. $\\mathrm{Acc} = 1.0$.\n-   **Case 4:** $(T, \\beta) = (10, 5.0)$. $k_E=5$, $k_O=5$.\n    $F = \\frac{5e^5 - 5e^{-5}}{5e^5 + 5e^{-5}} = \\frac{e^5 - e^{-5}}{e^5 + e^{-5}} = \\tanh(5.0) \\approx 0.999909$. $\\mathrm{Acc} = 1.0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating alternation fidelity and parity prediction accuracy\n    for a series of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement as (T, beta) pairs.\n    test_cases = [\n        (8, 2.0),\n        (1, 3.0),\n        (7, 0.0),\n        (10, 5.0),\n    ]\n\n    results = []\n    \n    for T, beta in test_cases:\n        # Step 1: Calculate alternation fidelity F\n        \n        # Determine the number of even and odd indices in the sequence of length T.\n        # k_E is the size of the set E = {s in {0,...,T-1} : s is even}\n        # k_O is the size of the set O = {s in {0,...,T-1} : s is odd}\n        k_E = int(np.ceil(T / 2.0))\n        k_O = int(np.floor(T / 2.0))\n        \n        # Calculate the numerator and denominator for the fidelity metric F.\n        # F = (k_E * exp(beta) - k_O * exp(-beta)) / (k_E * exp(beta) + k_O * exp(-beta))\n        # This formula is robust. If T=1, k_O=0, denominator is exp(beta) != 0.\n        # If beta=0, denominator is k_E + k_O = T != 0 for T>=1.\n        numerator_F = k_E * np.exp(beta) - k_O * np.exp(-beta)\n        denominator_F = k_E * np.exp(beta) + k_O * np.exp(-beta)\n        \n        fidelity = 0.0\n        if denominator_F != 0:\n            fidelity = numerator_F / denominator_F\n\n        results.append(f\"{fidelity:.6f}\")\n        \n        # Step 2: Calculate classification accuracy Acc\n        \n        # The problem requires a numerical implementation of the accuracy calculation.\n        # We iterate through all 2^T binary sequences.\n        num_sequences = 2**T\n        correct_predictions = 0\n        \n        # Generate indices for even and odd positions\n        even_indices = [i for i in range(T) if i % 2 == 0]\n        odd_indices = [i for i in range(T) if i % 2 != 0]\n\n        for i in range(num_sequences):\n            # Generate the binary sequence x of length T from integer i\n            x = np.array([(i >> p) & 1 for p in range(T)])\n            \n            # True parity y(x)\n            true_sum = np.sum(x)\n            true_parity = true_sum % 2\n            \n            # Predicted parity y_hat(x)\n            # The calculation simplifies as shown in the analytical solution.\n            # Here we implement the full procedure as requested.\n            \n            # Calculate means mu_E and mu_O.\n            # alpha_{0,s} are constant within even/odd groups, so they cancel.\n            sum_x_even = np.sum(x[even_indices]) if len(even_indices) > 0 else 0\n            sum_x_odd = np.sum(x[odd_indices]) if len(odd_indices) > 0 else 0\n            \n            mu_E = sum_x_even / k_E if k_E > 0 else 0.0\n            mu_O = sum_x_odd / k_O if k_O > 0 else 0.0\n            \n            # Calculate predicted total sum S_hat(x)\n            pred_sum = k_E * mu_E + k_O * mu_O\n            \n            # Calculate predicted parity y_hat(x)\n            # Use floor(val + 0.5) for rounding to nearest integer.\n            pred_parity = int(np.floor(pred_sum + 0.5)) % 2\n            \n            if pred_parity == true_parity:\n                correct_predictions += 1\n                \n        accuracy = correct_predictions / num_sequences\n        results.append(f\"{accuracy:.6f}\")\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3192596"}, {"introduction": "While powerful, the self-attention mechanism carries a significant computational cost, scaling quadratically with sequence length $n$. This final practice [@problem_id:3192615] delves into the performance engineering of attention by analyzing the computational and memory trade-offs of its core matrix multiplications. By applying a roofline performance model, you will discover how a simple reordering of operations—leveraging the associativity of matrix multiplication—can lead to dramatic speedups when the sequence length $n$ is much larger than the head dimension $d_h$.", "problem": "In a single-head scaled dot-product self-attention block, the core linear algebraic chain before any nonlinearity can be written as a product of three matrices with compatible dimensions. Consider a batched, multi-head setting where queries, keys, and values are stored as tensors with shape $B \\times H \\times n \\times d_h$, where $B$ is the batch size, $H$ is the number of heads, $n$ is the sequence length, and $d_h$ is the per-head embedding dimension. For each batch-head pair, define matrices $Q, K, V \\in \\mathbb{R}^{n \\times d_h}$. Ignore any scaling factors and nonlinearities, and focus solely on the associative product of three matrices.\n\nYou are tasked with analyzing two mathematically equivalent orders of evaluation for the batched product:\n- Baseline order: $\\left(QK^{\\top}\\right)V$.\n- Reordered (associative) order: $Q\\left(K^{\\top}V\\right)$.\n\nAssume the following foundational facts:\n- A General Matrix–Matrix Multiply (GEMM) of shape $(m \\times k)$ times $(k \\times n)$ performs $2mkn$ floating-point operations.\n- For each GEMM, every input element is read exactly once and every output element is written exactly once. Any intermediate result that is produced by one GEMM and consumed by another must be written to and then read from main memory exactly once. Do not assume any further caching or fusion.\n- The roofline execution time for a computation is the larger of the compute time and the memory time, where compute time equals total floating-point operations divided by the peak floating-point rate, and memory time equals total bytes moved divided by the sustained memory bandwidth.\n\nConsider half-precision floating-point (FP16) tensors with element size $s = 2$ bytes. Let the hardware be a Graphics Processing Unit (GPU) with peak FP16 throughput $P = 1.0 \\times 10^{14}$ floating-point operations per second and sustained memory bandwidth $W = 1.0 \\times 10^{12}$ bytes per second. Take $B = 2$, $H = 8$, $n = 4096$, and $d_h = 64$.\n\nUsing only the principles stated above and standard matrix dimensions, do the following across all $B \\times H$ heads:\n- Derive the total floating-point operations for each order.\n- Derive the total memory traffic in bytes for each order.\n- Compute the roofline-predicted execution time for each order as the maximum of its compute-bound and memory-bound times.\n- Finally, compute the speedup factor $S$, defined as the baseline time divided by the reordered time, as a pure number.\n\nRound your final answer $S$ to four significant figures. Express the final answer as a pure number without units.", "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, and objective. It presents a standard roofline model analysis of two computational strategies for the self-attention mechanism, using clearly defined parameters and an unambiguous model of computation and memory access. The problem is valid, and a solution will be derived.\n\nThe core task is to compare the performance of two evaluation orders for a batched, multi-head self-attention computation, specifically `$(QK^{\\top})V$` (baseline) and `$Q(K^{\\top}V)$` (reordered). We will use the provided roofline model, which defines execution time as the maximum of compute time and memory time.\n\nFirst, we define the given parameters symbolically:\n- Batch size: `$B = 2$`\n- Number of heads: `$H = 8$`\n- Sequence length: `$n = 4096$`\n- Per-head embedding dimension: `$d_h = 64$`\n- Element size for FP16: `$s = 2$` bytes\n- Peak FP16 throughput: `$P = 1.0 \\times 10^{14}$` FLOPS\n- Sustained memory bandwidth: `$W = 1.0 \\times 10^{12}$` bytes/s\n\nThe total number of independent attention calculations is `$N_{ops} = B \\times H = 2 \\times 8 = 16$`.\nFor each calculation, the matrices `$Q, K, V$` have dimensions `$\\mathbb{R}^{n \\times d_h}$`.\nA General Matrix-Matrix Multiply (GEMM) of a matrix of shape `$(m \\times k)$` with a matrix of shape `$(k \\times p)$` requires `$2mkp$` floating-point operations (FLOPs). The total memory traffic for such an operation, accounting for reading two input matrices and writing one output matrix, is `$s(mk + kp + mp)$` bytes.\n\n### Baseline Order Analysis: `$(QK^{\\top})V$`\n\nThis evaluation proceeds in two steps across all `$N_{ops}$` heads.\n\n**Step 1: Compute the attention matrix `$A = QK^{\\top}$`**\nFor each head, this is a product of `$Q \\in \\mathbb{R}^{n \\times d_h}$` and `$K^{\\top} \\in \\mathbb{R}^{d_h \\times n}$`. The resulting matrix `$A$` has shape `$(n \\times n)$`.\n- FLOPs per head: With `$m=n, k=d_h, p=n$`, the FLOPs are `$2 \\times n \\times d_h \\times n = 2n^2d_h$`.\n- Memory traffic per head: The traffic is `$s(nd_h + d_h n + n^2) = s(2nd_h + n^2)$` bytes.\n\n**Step 2: Compute the output `$C = AV$`**\nFor each head, this is a product of `$A \\in \\mathbb{R}^{n \\times n}$` and `$V \\in \\mathbb{R}^{n \\times d_h}$`. The resulting matrix `$C$` has shape `$(n \\times d_h)$`.\n- FLOPs per head: With `$m=n, k=n, p=d_h$`, the FLOPs are `$2 \\times n \\times n \\times d_h = 2n^2d_h$`.\n- Memory traffic per head: The traffic is `$s(n^2 + nd_h + nd_h) = s(n^2 + 2nd_h)$` bytes.\n\n**Total for Baseline Order**\nThe total FLOPs and memory traffic are the sum over the two steps, multiplied by the number of heads `$N_{ops}$`.\n- Total FLOPs, `$F_{base}`$:\n$$F_{base} = N_{ops} (2n^2d_h + 2n^2d_h) = 4BHn^2d_h$$\n$$F_{base} = 4 \\times 2 \\times 8 \\times (4096)^2 \\times 64 = 64 \\times (2^{12})^2 \\times 2^6 = 2^6 \\times 2^{24} \\times 2^6 = 2^{36} \\approx 6.872 \\times 10^{10} \\text{ FLOPs}$$\n- Total Memory Traffic, `$M_{base}`$:\n$$M_{base} = N_{ops} [s(2nd_h + n^2) + s(n^2 + 2nd_h)] = BHs(4nd_h + 2n^2)$$\n$$M_{base} = 16 \\times 2 \\times [4(4096)(64) + 2(4096)^2] = 32 [4 \\cdot 2^{12} \\cdot 2^6 + 2 \\cdot (2^{12})^2]$$\n$$M_{base} = 2^5 [2^2 \\cdot 2^{18} + 2 \\cdot 2^{24}] = 2^5 [2^{20} + 2^{25}] = 2^{25}(1+2^5) = 33 \\times 2^{25} \\approx 1.107 \\times 10^9 \\text{ bytes}$$\n\n**Baseline Roofline Time, `$T_{base}$`**\n- Compute Time: `$T_{compute, base} = F_{base} / P = (2^{36}) / (10^{14}) \\approx 6.872 \\times 10^{-4}$` s.\n- Memory Time: `$T_{memory, base} = M_{base} / W = (33 \\times 2^{25}) / (10^{12}) \\approx 1.107 \\times 10^{-3}$` s.\n- Execution Time: `$T_{base} = \\max(T_{compute, base}, T_{memory, base}) = T_{memory, base} \\approx 1.107 \\times 10^{-3}$` s.\n\n### Reordered Order Analysis: `$Q(K^{\\top}V)$`\n\nThis evaluation also proceeds in two steps across all `$N_{ops}$` heads.\n\n**Step 1: Compute the context matrix `$B = K^{\\top}V$`**\nFor each head, this is a product of `$K^{\\top} \\in \\mathbb{R}^{d_h \\times n}$` and `$V \\in \\mathbb{R}^{n \\times d_h}$`. The resulting matrix `$B$` has shape `$(d_h \\times d_h)$`.\n- FLOPs per head: With `$m=d_h, k=n, p=d_h$`, the FLOPs are `$2 \\times d_h \\times n \\times d_h = 2nd_h^2$`.\n- Memory traffic per head: The traffic is `$s(d_h n + nd_h + d_h^2) = s(2nd_h + d_h^2)$` bytes.\n\n**Step 2: Compute the output `$C = QB$`**\nFor each head, this is a product of `$Q \\in \\mathbb{R}^{n \\times d_h}$` and `$B \\in \\mathbb{R}^{d_h \\times d_h}$`. The resulting matrix `$C$` has shape `$(n \\times d_h)$`.\n- FLOPs per head: With `$m=n, k=d_h, p=d_h$`, the FLOPs are `$2 \\times n \\times d_h \\times d_h = 2nd_h^2$`.\n- Memory traffic per head: The traffic is `$s(nd_h + d_h^2 + nd_h) = s(2nd_h + d_h^2)$` bytes.\n\n**Total for Reordered Order**\n- Total FLOPs, `$F_{reord}`$:\n$$F_{reord} = N_{ops}(2nd_h^2 + 2nd_h^2) = 4BHnd_h^2$$\n$$F_{reord} = 4 \\times 2 \\times 8 \\times 4096 \\times (64)^2 = 64 \\times 2^{12} \\times (2^6)^2 = 2^6 \\times 2^{12} \\times 2^{12} = 2^{30} \\approx 1.074 \\times 10^9 \\text{ FLOPs}$$\n- Total Memory Traffic, `$M_{reord}`$:\n$$M_{reord} = N_{ops} [s(2nd_h + d_h^2) + s(2nd_h + d_h^2)] = BHs(4nd_h + 2d_h^2)$$\n$$M_{reord} = 16 \\times 2 \\times [4(4096)(64) + 2(64)^2] = 32 [4 \\cdot 2^{12} \\cdot 2^6 + 2 \\cdot (2^6)^2]$$\n$$M_{reord} = 2^5 [2^2 \\cdot 2^{18} + 2 \\cdot 2^{12}] = 2^5 [2^{20} + 2^{13}] = 2^{18}(2^7+1) = 129 \\times 2^{18} \\approx 3.382 \\times 10^7 \\text{ bytes}$$\n\n**Reordered Roofline Time, `$T_{reord}`**\n- Compute Time: `$T_{compute, reord} = F_{reord} / P = (2^{30}) / (10^{14}) \\approx 1.074 \\times 10^{-5}$` s.\n- Memory Time: `$T_{memory, reord} = M_{reord} / W = (129 \\times 2^{18}) / (10^{12}) \\approx 3.382 \\times 10^{-5}$` s.\n- Execution Time: `$T_{reord} = \\max(T_{compute, reord}, T_{memory, reord}) = T_{memory, reord} \\approx 3.382 \\times 10^{-5}$` s.\n\n### Speedup Factor `$S$`\n\nBoth evaluation orders are memory-bound, as `$T_{memory} > T_{compute}$` in both cases. The speedup factor `$S$` is the ratio of their execution times.\n$$S = \\frac{T_{base}}{T_{reord}} = \\frac{T_{memory, base}}{T_{memory, reord}} = \\frac{M_{base} / W}{M_{reord} / W} = \\frac{M_{base}}{M_{reord}}$$\nSubstituting the symbolic expressions for memory traffic:\n$$S = \\frac{BHs(4nd_h + 2n^2)}{BHs(4nd_h + 2d_h^2)} = \\frac{2n(2d_h + n)}{2d_h(2n + d_h)}$$\nUsing the calculated values:\n$$S = \\frac{33 \\times 2^{25}}{129 \\times 2^{18}} = \\frac{33}{129} \\times 2^{25-18} = \\frac{3 \\times 11}{3 \\times 43} \\times 2^7 = \\frac{11}{43} \\times 128 = \\frac{1408}{43}$$\n$$S \\approx 32.744186...$$\nRounding to four significant figures, the speedup factor is `$32.74$`.", "answer": "$$\\boxed{32.74}$$", "id": "3192615"}]}