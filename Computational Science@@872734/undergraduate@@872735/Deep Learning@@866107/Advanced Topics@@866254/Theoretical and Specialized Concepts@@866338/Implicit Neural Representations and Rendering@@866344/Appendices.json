{"hands_on_practices": [{"introduction": "A simple multilayer perceptron (MLP) naturally struggles to learn functions with high-frequency details, a phenomenon known as \"spectral bias.\" To overcome this, implicit neural representations like NeRF employ positional encoding to map low-dimensional inputs to a higher-dimensional feature space. This practice allows you to explore this fundamental concept by building a toy model to see firsthand how insufficient positional encoding leads to aliasing, a classic signal processing artifact, providing a tangible link between the Nyquist sampling theorem and the performance of neural networks [@problem_id:3136712].", "problem": "Consider the task of modeling a one-dimensional, band-limited signal using an Implicit Neural Representation (INR) with a restricted Positional Encoding (PE). The input coordinate $x$ lies in the interval $[0,1)$ and angles must be handled in radians. The target signal is a synthetic stripe pattern defined by $f_K(x) = \\cos(2\\pi K x)$, where $K$ is an integer frequency specifying the number of cycles across the unit interval. The INR is constructed as a linear model over a truncated Fourier feature map, which serves as the PE. Specifically, for a chosen cutoff $K_{\\text{cut}} \\in \\mathbb{N}$, the PE is defined by the feature vector\n$$\n\\phi_{K_{\\text{cut}}}(x) = \\left[1, \\cos(2\\pi \\cdot 1 \\cdot x), \\sin(2\\pi \\cdot 1 \\cdot x), \\ldots, \\cos(2\\pi \\cdot k \\cdot x), \\sin(2\\pi \\cdot k \\cdot x), \\ldots, \\cos(2\\pi \\cdot K_{\\text{cut}} \\cdot x), \\sin(2\\pi \\cdot K_{\\text{cut}} \\cdot x)\\right],\n$$\nand the INR predicts\n$$\n\\hat{f}(x) = \\mathbf{w}^{\\top} \\phi_{K_{\\text{cut}}}(x),\n$$\nwhere $\\mathbf{w}$ is obtained by minimizing the squared error on a training set.\n\nThe training set consists of $N$ uniformly spaced samples $x_n = \\frac{n}{N}$ for $n = 0,1,\\ldots,N-1$, with corresponding targets $y_n = f_K(x_n)$. The model is fit by linear least squares on these $N$ samples.\n\nYour task is to build a toy example that demonstrates aliasing due to insufficient positional encoding, and to quantify the phenomenon using this synthetic stripe pattern. Use the following well-tested bases for reasoning: the properties of Fourier series on $[0,1)$, the concept of band-limited representations via truncated Fourier features, and the Nyquist sampling perspective on uniformly sampled sequences. Do not assume any special identities beyond these foundational facts; derive the observed behavior algorithmically.\n\nFor each test case below, you must:\n- Construct the training design matrix with the specified $K_{\\text{cut}}$, fit $\\mathbf{w}$ by least squares, and compute the training Mean Squared Error (MSE) defined as\n$$\n\\text{MSE}_{\\text{train}} = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left(\\hat{f}(x_n) - f_K(x_n)\\right)^2.\n$$\n- Evaluate the fitted model on a dense grid of $M$ points $x_m = \\frac{m}{M}$ for $m = 0,1,\\ldots,M-1$ with $M$ chosen as a power of two, and compute the evaluation MSE\n$$\n\\text{MSE}_{\\text{eval}} = \\frac{1}{M} \\sum_{m=0}^{M-1} \\left(\\hat{f}(x_m) - f_K(x_m)\\right)^2.\n$$\n- Estimate the dominant frequency of the fitted model by applying the Fast Fourier Transform (FFT) on $\\{\\hat{f}(x_m)\\}_{m=0}^{M-1}$ and selecting the index $k^{\\star} \\in \\{1,2,\\ldots,\\lfloor M/2 \\rfloor\\}$ that maximizes the magnitude of the one-sided spectrum. Report $k^{\\star}$ as an integer.\n\nUse the following test suite of parameter values, which is designed to cover different facets of the phenomenon:\n1. Happy path case: $(N, K, K_{\\text{cut}}) = (64, 4, 10)$.\n2. Aliasing case where the target frequency exceeds the PE cutoff but can alias into the representable band on the training grid: $(N, K, K_{\\text{cut}}) = (64, 60, 10)$.\n3. Edge case where the target frequency exceeds the PE cutoff and cannot alias into the representable band, resulting in a low-pass approximation: $(N, K, K_{\\text{cut}}) = (64, 53, 10)$.\n4. Boundary condition at the Nyquist limit of the training grid: $(N, K, K_{\\text{cut}}) = (64, 32, 32)$.\n\nSet the dense evaluation grid size to $M = 2048$. For each test case, produce three quantities: $\\text{MSE}_{\\text{train}}$, $\\text{MSE}_{\\text{eval}}$, and the integer dominant frequency $k^{\\star}$ detected on the dense grid. Express the MSE values as decimal floats rounded to six decimal places. There are no physical units involved; angles are in radians.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by test case and metric as follows:\n$$\n\\left[\\text{MSE}_{\\text{train}}^{(1)}, \\text{MSE}_{\\text{eval}}^{(1)}, k^{\\star (1)}, \\text{MSE}_{\\text{train}}^{(2)}, \\text{MSE}_{\\text{eval}}^{(2)}, k^{\\star (2)}, \\text{MSE}_{\\text{train}}^{(3)}, \\text{MSE}_{\\text{eval}}^{(3)}, k^{\\star (3)}, \\text{MSE}_{\\text{train}}^{(4)}, \\text{MSE}_{\\text{eval}}^{(4)}, k^{\\star (4)}\\right].\n$$\nAll quantities must be computed by your program. No external input or files are permitted. Angles must be treated in radians throughout.", "solution": "The user's request has been assessed and is valid. It presents a well-posed, scientifically grounded problem in the domain of deep learning and signal processing, free of any inconsistencies or ambiguities. We shall proceed with a complete solution.\n\nThe problem requires an analysis of an Implicit Neural Representation (INR) based on a truncated Fourier feature map, also known as Positional Encoding (PE). The goal is to demonstrate aliasing and other representation artifacts by fitting this model to a simple sinusoidal signal, $f_K(x) = \\cos(2\\pi K x)$, under different conditions. The INR is a linear model $\\hat{f}(x) = \\mathbf{w}^{\\top} \\phi_{K_{\\text{cut}}}(x)$, where the weights $\\mathbf{w}$ are determined by linear least squares on a set of $N$ uniform samples.\n\nThe core of the analysis rests on three principles:\n1.  **Representation Capacity**: The PE, $\\phi_{K_{\\text{cut}}}(x)$, consists of sinusoidal basis functions up to a maximum frequency of $K_{\\text{cut}}$. The INR, being a linear combination of these basis functions, is inherently band-limited to the frequency range $[0, K_{\\text{cut}}]$. It cannot perfectly represent any signal containing frequencies greater than $K_{\\text{cut}}$.\n2.  **Least Squares Projection**: The least squares fitting process finds the optimal weights $\\mathbf{w}$ by projecting the sampled target vector $\\mathbf{y} = [y_0, \\ldots, y_{N-1}]^\\top$ onto the subspace spanned by the sampled basis vectors (the columns of the design matrix $\\Phi$). When the basis vectors are orthogonal on the sampling grid, the solution simplifies to independent projections onto each basis vector.\n3.  **Sampling and Aliasing**: According to the Nyquist-Shannon sampling theorem, sampling a continuous signal at a rate of $N$ samples per unit interval can unambiguously capture frequencies only up to the Nyquist frequency, $N/2$. A signal with frequency $K  N/2$ becomes indistinguishable from a lower-frequency alias on the sampling grid. For a pure cosine signal $\\cos(2\\pi K x)$ sampled at $x_n = n/N$, the values $\\cos(2\\pi K n/N)$ are identical to those of $\\cos(2\\pi K' n/N)$ where $K'$ is the frequency of smallest magnitude congruent to $\\pm K$ modulo $N$. That is, $K' = \\min_{j \\in \\mathbb{Z}} |K - jN|$.\n\nWe will now apply this framework to analyze each test case. For all cases, the evaluation grid has size $M=2048$.\n\n**Case 1: $(N, K, K_{\\text{cut}}) = (64, 4, 10)$**\n- **Analysis**: The target signal frequency is $K=4$, and the PE cutoff is $K_{\\text{cut}}=10$. Since $K \\le K_{\\text{cut}}$, the target signal $f_4(x) = \\cos(2\\pi \\cdot 4 \\cdot x)$ is directly representable by the PE basis. The least squares fit will identify the correct non-zero weight for the $\\cos(2\\pi \\cdot 4 \\cdot x)$ basis function and near-zero weights for all others.\n- **Prediction**: The model will perfectly reconstruct the target signal. We expect both $\\text{MSE}_{\\text{train}}$ and $\\text{MSE}_{\\text{eval}}$ to be close to $0$ (within machine precision). The dominant frequency of the fitted model $\\hat{f}(x)$ on the dense grid will be $k^{\\star}=4$.\n\n**Case 2: $(N, K, K_{\\text{cut}}) = (64, 60, 10)$**\n- **Analysis**: The target frequency $K=60$ is much larger than the PE cutoff $K_{\\text{cut}}=10$. However, the training is performed on a grid with $N=64$ samples. The Nyquist frequency is $N/2 = 32$. The frequency $K=60$ will alias. The aliased frequency is $K' = |60 - 64| = 4$. On the training grid, the sampled data from $f_{60}(x)$ is identical to that from $f_4(x)$:\n$$y_n = \\cos\\left(2\\pi \\cdot 60 \\cdot \\frac{n}{64}\\right) = \\cos\\left(2\\pi \\cdot (64-4) \\cdot \\frac{n}{64}\\right) = \\cos\\left(2\\pi n - 2\\pi \\cdot 4 \\cdot \\frac{n}{64}\\right) = \\cos\\left(2\\pi \\cdot 4 \\cdot \\frac{n}{64}\\right)$$\nThe least squares algorithm is thus presented with data that appears to come from a frequency-$4$ signal. Since $K'=4 \\le K_{\\text{cut}}=10$, the model will fit this aliased signal perfectly.\n- **Prediction**: $\\text{MSE}_{\\text{train}}$ will be near $0$. The learned model will be $\\hat{f}(x) \\approx \\cos(2\\pi \\cdot 4 \\cdot x)$. When evaluated on the dense grid, this low-frequency model is compared against the true high-frequency signal $f_{60}(x) = \\cos(2\\pi \\cdot 60 \\cdot x)$. The two are very different, so $\\text{MSE}_{\\text{eval}}$ will be large. The expected value is approximately $\\int_0^1 (\\cos(2\\pi \\cdot 4x) - \\cos(2\\pi \\cdot 60x))^2 dx = \\int_0^1 \\cos^2(2\\pi \\cdot 4x) dx + \\int_0^1 \\cos^2(2\\pi \\cdot 60x) dx = \\frac{1}{2} + \\frac{1}{2} = 1$. The dominant frequency found in $\\hat{f}(x)$ will be that of the alias, so $k^{\\star}=4$.\n\n**Case 3: $(N, K, K_{\\text{cut}}) = (64, 53, 10)$**\n- **Analysis**: The target frequency is $K=53$, with $N=64$ and $K_{\\text{cut}}=10$. The aliased frequency on the training grid is $K' = |53-64| = 11$. The training data thus appears to come from a frequency-$11$ signal. However, this aliased frequency $K'=11$ is *still greater* than the PE cutoff $K_{\\text{cut}}=10$. The model's basis functions (frequencies $0, 1, \\ldots, 10$) cannot represent the frequency-$11$ signal they are being trained on. The discrete Fourier basis functions are orthogonal on the uniform grid. The sampled target vector, corresponding to frequency $11$, is orthogonal to all the basis vectors provided by the PE (frequencies $0$ to $10$). The projection onto this basis space is thus the zero vector.\n- **Prediction**: The least squares fit will result in $\\mathbf{w} \\approx \\mathbf{0}$, meaning $\\hat{f}(x) \\approx 0$ for all $x$. The training MSE will be $\\text{MSE}_{\\text{train}} = \\frac{1}{N}\\sum_{n=0}^{N-1}(0 - y_n)^2 = \\frac{1}{N}\\sum_{n=0}^{N-1}\\cos^2(2\\pi \\cdot 11 \\cdot n/N) \\approx \\frac{1}{2}$. Similarly, $\\text{MSE}_{\\text{eval}} = \\frac{1}{M}\\sum_{m=0}^{M-1}(0-f_{53}(x_m))^2 \\approx \\frac{1}{2}\\int_0^1 \\cos^2(2\\pi \\cdot 53 x) dx = 0.5$. Since $\\hat{f}(x)$ is the zero function, its FFT will be zero everywhere. The `argmax` function will return the first index of the search range $\\{1, \\ldots, \\lfloor M/2 \\rfloor\\}$, so we expect $k^{\\star}=1$.\n\n**Case 4: $(N, K, K_{\\text{cut}}) = (64, 32, 32)$**\n- **Analysis**: The target frequency $K=32$ is exactly the Nyquist frequency for $N=64$. The PE cutoff is $K_{\\text{cut}}=32$, so the basis includes the function $\\cos(2\\pi \\cdot 32 \\cdot x)$. At the sampling points $x_n = n/64$, the target is $y_n = \\cos(2\\pi \\cdot 32 \\cdot n/64) = \\cos(\\pi n) = (-1)^n$. This is exactly the sampled Nyquist-frequency cosine basis function. It is important to note that the corresponding sine term, $\\sin(2\\pi \\cdot 32 \\cdot n/64) = \\sin(\\pi n)$, is zero for all integer $n$, resulting in a column of zeros in the design matrix. This introduces a rank deficiency. However, since the target signal lies perfectly within the span of the remaining basis vectors, a perfect fit is still achieved. `numpy.linalg.lstsq` will correctly find the solution that sets the weight for the $\\cos(2\\pi \\cdot 32 \\cdot x)$ term to $1$ and all other weights to $0$.\n- **Prediction**: The model will perfectly represent the target. Both $\\text{MSE}_{\\text{train}}$ and $\\text{MSE}_{\\text{eval}}$ will be near $0$. The dominant frequency of the fitted model $\\hat{f}(x)=\\cos(2\\pi \\cdot 32 \\cdot x)$ will be $k^{\\star}=32$.\n\nThese analyses lead to the implementation provided in the final answer.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of fitting a band-limited INR to a sinusoidal signal\n    under various conditions of sampling, signal frequency, and PE cutoff.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, K, K_cut)\n        (64, 4, 10),    # Happy path\n        (64, 60, 10),   # Aliasing into representable band\n        (64, 53, 10),   # Aliasing outside representable band\n        (64, 32, 32),   # Nyquist boundary case\n    ]\n    \n    # Dense evaluation grid size\n    M = 2048\n\n    all_results = []\n\n    for N, K, K_cut in test_cases:\n        # 1. Construct the training data\n        x_train = np.arange(N) / N\n        y_train = np.cos(2 * np.pi * K * x_train)\n        \n        # 2. Construct the Positional Encoding (PE) design matrix for training\n        num_features = 1 + 2 * K_cut\n        Phi_train = np.zeros((N, num_features))\n        \n        # DC component (frequency 0)\n        Phi_train[:, 0] = 1.0\n        \n        # Sinusoidal components (frequencies 1 to K_cut)\n        freqs = np.arange(1, K_cut + 1)\n        # Use broadcasting for efficient construction\n        cos_terms = np.cos(2 * np.pi * freqs[None, :] * x_train[:, None])\n        sin_terms = np.sin(2 * np.pi * freqs[None, :] * x_train[:, None])\n        \n        # Interleave cos and sin terms as per the problem definition\n        Phi_train[:, 1::2] = cos_terms\n        Phi_train[:, 2::2] = sin_terms\n        \n        # 3. Fit the model by solving the linear least squares problem\n        # w = (Phi^T Phi)^-1 Phi^T y\n        w = np.linalg.lstsq(Phi_train, y_train, rcond=None)[0]\n        \n        # 4. Compute the training MSE\n        y_hat_train = Phi_train @ w\n        mse_train = np.mean((y_hat_train - y_train) ** 2)\n        \n        # 5. Evaluate the fitted model on a dense grid\n        x_eval = np.arange(M) / M\n        y_eval_true = np.cos(2 * np.pi * K * x_eval)\n        \n        # Construct the PE design matrix for evaluation\n        Phi_eval = np.zeros((M, num_features))\n        Phi_eval[:, 0] = 1.0\n        cos_terms_eval = np.cos(2 * np.pi * freqs[None, :] * x_eval[:, None])\n        sin_terms_eval = np.sin(2 * np.pi * freqs[None, :] * x_eval[:, None])\n        Phi_eval[:, 1::2] = cos_terms_eval\n        Phi_eval[:, 2::2] = sin_terms_eval\n        \n        y_hat_eval = Phi_eval @ w\n        \n        # 6. Compute the evaluation MSE\n        mse_eval = np.mean((y_hat_eval - y_eval_true) ** 2)\n\n        # 7. Estimate the dominant frequency of the fitted model\n        # Apply FFT on the densely sampled predictions\n        fft_vals = np.fft.fft(y_hat_eval)\n        # Get magnitudes of the one-sided spectrum\n        fft_mags = np.abs(fft_vals)\n        \n        # Find the peak frequency index k* in {1, 2, ..., floor(M/2)}\n        # This corresponds to array indices 1 through M//2\n        search_range = fft_mags[1 : M//2 + 1]\n        # argmax returns 0-based index relative to the slice; add 1 for frequency\n        k_star = int(np.argmax(search_range) + 1)\n\n        # 8. Append results for the current test case\n        all_results.append(f\"{mse_train:.6f}\")\n        all_results.append(f\"{mse_eval:.6f}\")\n        all_results.append(str(k_star))\n\n    #\n    # Final print statement in the exact required format.\n    #\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3136712"}, {"introduction": "Successfully representing a 3D scene is only half the battle; we must first reconstruct it from a set of 2D images. The quality of this reconstruction is deeply tied to the principles of multi-view geometry, where the arrangement of cameras plays a critical role. In this exercise, you will investigate the relationship between camera baseline—the distance between two camera centers—and the final reconstruction error, deriving and fitting a model that quantifies this effect, which gives you practical intuition for designing effective data capture setups [@problem_id:3136704].", "problem": "You are investigating the effect of camera baseline on the reconstruction quality of an implicit neural representation, specifically a Neural Radiance Field (NeRF). The goal is to formalize how camera baseline impacts expected training error and to compute the minimal baseline needed to achieve a target error threshold. Your program must fit a curve relating camera baseline to reconstruction error and use the fitted curve to infer the minimal baseline requirement for several test scenarios.\n\nStarting from the pinhole camera model and multi-view geometry, use the following fundamental and widely accepted facts as your derivation base:\n- The depth $Z$ of a point viewed by two cameras with focal length $f$ and baseline $B$ is related to disparity $d$ by the pinhole relation $$Z = \\frac{f B}{d}.$$\n- Propagating small disparity noise $\\sigma_d$ through the above relation yields a depth uncertainty governed by $$\\sigma_Z = \\left|\\frac{\\partial Z}{\\partial d}\\right|\\sigma_d = \\frac{Z^2}{f B}\\,\\sigma_d.$$\n- In implicit neural representation training for a Neural Radiance Field (NeRF), the photometric error, for example Mean Squared Error (MSE), is affected by geometric misregistration. Under small-error assumptions, the expected photometric error contribution caused by depth uncertainty along rays scales linearly with $\\sigma_Z$ after averaging over an appropriate scene distribution, while a nonzero error floor remains due to model bias, occlusions, reflectance complexity, and sensor noise unrelated to geometry.\n\nUsing the above base and without introducing any shortcut formula in the problem statement, derive a parametric functional form for the expected reconstruction error $E(B)$ as a function of baseline $B$, with unknown nonnegative parameters that can be fit from data. Then, given measured error samples at different baselines, fit the parameters of your model. Finally, for a specified target error threshold $\\tau$, infer the minimal baseline $B_{\\min}$ such that the fitted model predicts $E(B_{\\min}) \\le \\tau$. If the target threshold is stricter than the fitted error floor, then the requirement is impossible; in that case, report an infeasible result.\n\nAll baselines must be expressed in meters. All error values are Mean Squared Error (MSE) and unitless. Angles do not appear in this problem; therefore, no angle units are required.\n\nYour program must:\n- Derive the functional dependence of $E(B)$ from the provided principles.\n- Fit the unknown parameters using nonlinear least squares with nonnegativity constraints on the parameters.\n- Compute $$B_{\\min}$$ for each test case as the smallest baseline (in meters) satisfying the inequality with the fitted model parameters.\n- If the target threshold $\\tau$ is not achievable because it is less than the fitted error floor, output a result indicating infeasibility.\n\nTest Suite:\nProvide minimal baselines for the following test cases. Each test case includes camera baselines and measured errors:\n\n1. Happy path with moderate baselines and achievable target:\n   - Baselines (meters): $[0.05, 0.08, 0.12, 0.20, 0.30]$\n   - Measured errors (MSE): $[0.134, 0.091, 0.066, 0.047, 0.036]$\n   - Target threshold: $\\tau = 0.040$\n\n2. Close-range views with small baselines and relaxed target:\n   - Baselines (meters): $[0.02, 0.04, 0.06, 0.08]$\n   - Measured errors (MSE): $[0.158, 0.086, 0.061, 0.046]$\n   - Target threshold: $\\tau = 0.070$\n\n3. Larger baselines approaching an error floor with a stricter target:\n   - Baselines (meters): $[0.10, 0.20, 0.40]$\n   - Measured errors (MSE): $[0.061, 0.041, 0.031]$\n   - Target threshold: $\\tau = 0.028$\n\n4. Strongly strict target, potentially infeasible:\n   - Baselines (meters): $[0.10, 0.15, 0.25, 0.50]$\n   - Measured errors (MSE): $[0.067, 0.052, 0.042, 0.034]$\n   - Target threshold: $\\tau = 0.020$\n\nAnswer specification:\n- For each test case, output the inferred minimal baseline $B_{\\min}$ in meters, rounded to three significant figures. If infeasible, output $+\\infty$ as a float.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $$[B_{\\min,1}, B_{\\min,2}, B_{\\min,3}, B_{\\min,4}].$$", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of multi-view geometry, well-posed with a clear objective and sufficient data, and free of subjective or ambiguous language. The solution proceeds as follows.\n\n### Derivation of the Error Model\n\nThe objective is to derive a functional form for the expected reconstruction error, $E(B)$, as a function of the camera baseline, $B$. The derivation is based on the provided principles.\n\n1.  **Depth Uncertainty**: The problem states that the uncertainty in depth, $\\sigma_Z$, is related to the camera baseline $B$, focal length $f$, scene depth $Z$, and disparity measurement noise $\\sigma_d$ by the formula:\n    $$ \\sigma_Z = \\frac{Z^2}{f B}\\,\\sigma_d $$\n    Here, $B$ is the variable of interest. The other quantities ($Z$, $f$, $\\sigma_d$) depend on scene structure, camera intrinsics, and image feature quality, but not on the baseline $B$.\n\n2.  **Photometric Error Contribution**: The problem posits that the expected photometric error, $E$, consists of two components.\n    -   A geometry-dependent component, let's call it $E_g$, which \"scales linearly with $\\sigma_Z$ after averaging over an appropriate scene distribution\". This can be expressed as $E_g \\propto \\sigma_Z$. Introducing a positive proportionality constant $k$, we have $E_g = k \\sigma_Z$.\n    -   An error floor, let's call it $C$, which is \"unrelated to geometry\". This term is therefore independent of the baseline $B$.\n\n3.  **Functional Form**: Combining these facts, the total expected error $E(B)$ is the sum of the two components:\n    $$ E(B) = E_g(B) + C $$\n    Substituting the expressions for $E_g$ and $\\sigma_Z$:\n    $$ E(B) = k \\left( \\frac{Z^2 \\sigma_d}{f B} \\right) + C $$\n    The problem specifies that this relationship holds \"after averaging over an appropriate scene distribution.\" We can therefore aggregate all terms that are not dependent on $B$ into a single, non-negative parameter $A$.\n    Let $A = \\langle k \\frac{Z^2 \\sigma_d}{f} \\rangle$, where $\\langle \\cdot \\rangle$ denotes the averaging over the scene and viewing statistics. Since $k$, $Z^2$, $\\sigma_d$, and $f$ are all non-negative quantities, the resulting parameter $A$ must also be non-negative, i.e., $A \\ge 0$. The error floor $C$, being a Mean Squared Error, must also be non-negative, $C \\ge 0$.\n    This yields the final parametric functional form for the expected reconstruction error:\n    $$ E(B) = \\frac{A}{B} + C $$\n\n### Parameter Fitting\n\nFor each test case, we are given a set of measurements of error $E_i$ at corresponding baselines $B_i$. The unknown parameters $A$ and $C$ of our derived model $E(B) = \\frac{A}{B} + C$ must be determined from this data. We use the method of nonlinear least squares to find the values of $A$ and $C$ that best fit the data. This involves minimizing the sum of the squared residuals, $S(A, C)$:\n$$ S(A, C) = \\sum_{i} \\left( E_i - \\left(\\frac{A}{B_i} + C\\right) \\right)^2 $$\nThis optimization is performed subject to the non-negativity constraints $A \\ge 0$ and $C \\ge 0$.\n\n### Minimal Baseline Calculation\n\nOnce the parameters $\\hat{A}$ and $\\hat{C}$ are fitted for the model $E(B) = \\frac{\\hat{A}}{B} + \\hat{C}$, we must find the minimal baseline $B_{\\min}$ that achieves a target error threshold $\\tau$. This is the smallest $B  0$ such that:\n$$ E(B) \\le \\tau $$\n$$ \\frac{\\hat{A}}{B} + \\hat{C} \\le \\tau $$\n\nFirst, we analyze the feasibility. The function $E(B)$ is monotonically decreasing with $B$ for $\\hat{A}  0$. As $B \\to \\infty$, the error approaches its minimum possible value, the error floor $\\hat{C}$. Therefore, if the target threshold $\\tau$ is less than this floor (i.e., $\\tau  \\hat{C}$), it is impossible to satisfy the condition. The case where $\\tau = \\hat{C}$ would require an infinite baseline to achieve (for $\\hat{A}  0$). Thus, if $\\tau \\le \\hat{C}$, the task is infeasible, and the result is $B_{\\min} = +\\infty$.\n\nIf the task is feasible ($\\tau  \\hat{C}$), we can solve for $B_{\\min}$. To find the *minimal* baseline, we solve the equality, as any smaller baseline would yield a higher error:\n$$ \\frac{\\hat{A}}{B_{\\min}} + \\hat{C} = \\tau $$\nRearranging the terms to solve for $B_{\\min}$:\n$$ \\frac{\\hat{A}}{B_{\\min}} = \\tau - \\hat{C} $$\n$$ B_{\\min} = \\frac{\\hat{A}}{\\tau - \\hat{C}} $$\nThis formula will be applied to each test case using its specifically fitted parameters $\\hat{A}$ and $\\hat{C}$ and its target $\\tau$. The final result for each case will be rounded to three significant figures.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef solve():\n    \"\"\"\n    Solves for the minimal baseline required to meet an error threshold for a NeRF reconstruction.\n    \n    The process involves:\n    1. Defining an error model E(B) = A/B + C based on first principles.\n    2. Fitting parameters A and C to measured data points (B_i, E_i) using nonlinear least squares.\n    3. Solving for the minimal baseline B_min such that E(B_min) = tau.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"baselines\": np.array([0.05, 0.08, 0.12, 0.20, 0.30]),\n            \"errors\": np.array([0.134, 0.091, 0.066, 0.047, 0.036]),\n            \"tau\": 0.040,\n        },\n        {\n            \"baselines\": np.array([0.02, 0.04, 0.06, 0.08]),\n            \"errors\": np.array([0.158, 0.086, 0.061, 0.046]),\n            \"tau\": 0.070,\n        },\n        {\n            \"baselines\": np.array([0.10, 0.20, 0.40]),\n            \"errors\": np.array([0.061, 0.041, 0.031]),\n            \"tau\": 0.028,\n        },\n        {\n            \"baselines\": np.array([0.10, 0.15, 0.25, 0.50]),\n            \"errors\": np.array([0.067, 0.052, 0.042, 0.034]),\n            \"tau\": 0.020,\n        },\n    ]\n\n    def error_model(B, A, C):\n        \"\"\" The derived functional form for error E as a function of baseline B. \"\"\"\n        return A / B + C\n\n    def format_to_3sf(value):\n        \"\"\" Rounds a float to three significant figures, handling infinity. \"\"\"\n        if np.isinf(value):\n            return float('inf')\n        if value == 0:\n            return 0.0\n        return float(f\"{value:.3g}\")\n\n    results = []\n    for case in test_cases:\n        baselines = case[\"baselines\"]\n        errors = case[\"errors\"]\n        tau = case[\"tau\"]\n\n        # Fit the model E(B) = A/B + C using nonlinear least squares\n        # with non-negativity constraints on A and C.\n        try:\n            popt, _ = curve_fit(\n                error_model,\n                baselines,\n                errors,\n                bounds=([0, 0], [np.inf, np.inf])\n            )\n        except RuntimeError:\n            # In case the fitting algorithm fails, which is unlikely for this model.\n            results.append(float('inf'))\n            continue\n            \n        A_fit, C_fit = popt\n\n        # Check for feasibility. If the target error is lower than or equal to\n        # the asymptotic error floor C, it's impossible to achieve.\n        if tau = C_fit:\n            B_min = float('inf')\n        else:\n            # If feasible, calculate the minimal baseline.\n            B_min = A_fit / (tau - C_fit)\n        \n        # Format result to 3 significant figures.\n        formatted_result = format_to_3sf(B_min)\n        results.append(formatted_result)\n\n    # Print the final output in the specified format. The `str` representation\n    # of float('inf') is 'inf', which is the required output.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3136704"}, {"introduction": "An implicit representation encodes a surface as a continuous field in space, but for many applications, we need an explicit mesh. This is typically achieved by finding a level set of the learned field, a process akin to contouring a map. This hands-on practice guides you through investigating how the choice of the decision threshold $\\tau$ on a learned occupancy field $o_{\\theta}(\\mathbf{x})$ directly impacts the topology and geometric fidelity of the extracted surface, demonstrating the crucial trade-offs between completeness and noise in the final 3D model [@problem_id:3136800].", "problem": "You are given an implicit binary surface represented by a learned occupancy field $o_{\\theta}(\\mathbf{x})$ defined on $\\mathbb{R}^3$. The implicit surface at a threshold $\\tau \\in (0,1)$ is the level set $\\{\\mathbf{x} \\in \\mathbb{R}^3 \\mid o_{\\theta}(\\mathbf{x}) = \\tau\\}$, and the induced solid is the superlevel set $\\{\\mathbf{x} \\in \\mathbb{R}^3 \\mid o_{\\theta}(\\mathbf{x}) \\ge \\tau\\}$. You will study how changing the threshold $\\tau$ affects the topology (number of connected components) and geometric fidelity (a manifold quality proxy) of the recovered surface on a discrete grid. Your program must construct $o_{\\theta}(\\mathbf{x})$ from first principles and evaluate the requested metrics on a fixed grid without reading any input.\n\nStart from the following fundamental base:\n- An implicit occupancy field is a function $o_{\\theta} : \\mathbb{R}^3 \\to [0,1]$; thresholding at $\\tau$ yields a binary classification of space into occupied and free.\n- A Signed Distance Function (SDF) $s(\\mathbf{x})$ satisfies $s(\\mathbf{x}) = 0$ on the true surface, $s(\\mathbf{x}) lt; 0$ inside, and $s(\\mathbf{x}) gt; 0$ outside.\n- The logistic sigmoid function is $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$ and the logit function is $\\operatorname{logit}(\\tau) = \\log\\!\\left(\\dfrac{\\tau}{1 - \\tau}\\right)$. These are inverses, i.e., $\\sigma(\\operatorname{logit}(\\tau)) = \\tau$ and $\\operatorname{logit}(\\sigma(z)) = z$.\n- A discrete connected component on a $3$-dimensional grid with $6$-connectivity is a maximal set of occupied voxels where any two voxels in the set are connected by a path of face-adjacent steps.\n\nConstruct the learned occupancy field by composing an SDF with a logistic link and a small spatially varying bias to simulate learned imperfections. Specifically, define\n$$\no_{\\theta}(\\mathbf{x}) \\;=\\; \\sigma\\!\\Big(-k \\, s(\\mathbf{x}) + b + n(\\mathbf{x})\\Big),\n$$\nwith stiffness $k \\in \\mathbb{R}_{gt;0}$ and bias $b \\in \\mathbb{R}$. The term $n(\\mathbf{x})$ models spatially varying fitting error via\n$$\nn(\\mathbf{x}) \\;=\\; \\varepsilon \\,\\sin(\\omega x)\\,\\sin(\\omega y)\\,\\sin(\\omega z),\n$$\nwhere $\\varepsilon \\in \\mathbb{R}_{\\ge 0}$ and $\\omega \\in \\mathbb{R}_{gt;0}$.\n\nUse two shape families for $s(\\mathbf{x})$:\n- Single sphere centered at the origin with radius $R$: $s(\\mathbf{x}) = \\|\\mathbf{x}\\|_2 - R$.\n- Union of two equal spheres of radius $R$ with centers $\\mathbf{c}_1 = (-d/2,0,0)$, $\\mathbf{c}_2 = (d/2,0,0)$: $s(\\mathbf{x}) = \\min\\!\\big(\\|\\mathbf{x}-\\mathbf{c}_1\\|_2 - R, \\|\\mathbf{x}-\\mathbf{c}_2\\|_2 - R\\big)$.\n\nIgnoring $n(\\mathbf{x})$, setting $o_{\\theta}(\\mathbf{x}) = \\tau$ and inverting the logistic link implies the SDF level $s(\\mathbf{x}) = \\delta(\\tau)$ for some offset $\\delta(\\tau)$ that depends on $\\tau$, $k$, and $b$. You must derive this relation in your solution and then use it to define an analytic target radius $r^{\\star}(\\tau)$ for spheres as $r^{\\star}(\\tau) = R + \\delta(\\tau)$ (for the union case, use the radius with respect to the nearest center). This $r^{\\star}(\\tau)$ provides a geometric fidelity reference independent of sampling.\n\nDiscretize the domain to a regular grid of $N \\times N \\times N$ points with $N = 64$, covering the cube $[-1,1]^3$. For each test case specified below:\n- Evaluate $o_{\\theta}(\\mathbf{x})$ on grid points.\n- Threshold at $\\tau$ to obtain a binary occupancy volume $\\mathcal{V}_{\\tau}$ using the superlevel set condition.\n- Compute the number of connected components $C(\\tau)$ of $\\mathcal{V}_{\\tau}$ using $6$-connectivity.\n- Identify boundary voxels as occupied voxels with at least one face-adjacent neighbor that is unoccupied; compute the mean absolute radial deviation\n$$\nE(\\tau) \\;=\\; \\frac{1}{M}\\sum_{i=1}^{M}\\Big|\\|\\mathbf{p}_i - \\mathbf{c}(\\mathbf{p}_i)\\|_2 - r^{\\star}(\\tau)\\Big|,\n$$\nwhere $\\mathbf{p}_i$ is the coordinate of the $i$-th boundary voxel center, $\\mathbf{c}(\\mathbf{p}_i)$ is the nearest sphere center (for the single sphere, use $\\mathbf{0}$), and $M$ is the number of boundary voxels. If $M = 0$, define $E(\\tau) = 0$.\n\nUse constants $k = 16$, $b = 0$, $R = 0.35$, $\\omega = 12$, and two-sphere center separation $d = 0.8$. The test suite consists of the following $5$ cases, each described by a tuple $(\\text{shape}, \\varepsilon, \\tau)$, where $\\text{shape} \\in \\{1,2\\}$ denotes single sphere ($1$) or union of two spheres ($2$):\n- Case $1$: $(1, 0.0, 0.5)$\n- Case $2$: $(1, 0.12, 0.5)$\n- Case $3$: $(2, 0.0, 0.7)$\n- Case $4$: $(2, 0.0, 0.31)$\n- Case $5$: $(2, 0.05, 0.30)$\n\nYour program must:\n- Implement the above construction.\n- For each case, compute $C(\\tau)$ (integer) and $E(\\tau)$ (float).\n- Round $E(\\tau)$ to $4$ decimal places.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of per-case results, each result being a two-element list $[C(\\tau),E(\\tau)]$, all enclosed in square brackets. For example, an output with three cases should look like $[[1,0.0123],[2,0.0456],[1,0.0034]]$ but with the values for the five cases above.", "solution": "The problem asks for an analysis of an implicit binary surface defined by a learned occupancy field, $o_{\\theta}(\\mathbf{x})$. We are tasked with constructing this field, evaluating it on a discrete grid, and then computing two metrics for several test cases: the number of connected components, $C(\\tau)$, and a geometric fidelity measure, the mean absolute radial deviation, $E(\\tau)$, as a function of an occupancy threshold $\\tau$.\n\nThe problem is well-posed, scientifically grounded in the domain of implicit neural representations, and provides all necessary parameters and definitions for a unique, deterministic solution. We proceed with a principled derivation and an outline of the computational procedure.\n\n**1. Theoretical Formulation**\n\nThe core of the problem is the occupancy field, defined as:\n$$\no_{\\theta}(\\mathbf{x}) \\;=\\; \\sigma\\!\\Big(-k \\, s(\\mathbf{x}) + b + n(\\mathbf{x})\\Big)\n$$\nwhere $\\sigma(z) = (1 + e^{-z})^{-1}$ is the logistic sigmoid function, $s(\\mathbf{x})$ is a Signed Distance Function (SDF), $k$ and $b$ are scalar parameters representing network weights, and $n(\\mathbf{x})$ is a spatially-varying noise term modeling learned imperfections. The choice of $-k \\, s(\\mathbf{x})$ in the argument ensures that points inside the object, where $s(\\mathbf{x})  0$, map to high occupancy values (close to $1$), while points outside, where $s(\\mathbf{x})  0$, map to low occupancy values (close to $0$).\n\nThe problem requires deriving an analytic reference radius, $r^{\\star}(\\tau)$, for the reconstructed surface. This is found by analyzing the ideal, noise-free case where $n(\\mathbf{x}) = 0$. The implicit surface is the level set where $o_{\\theta}(\\mathbf{x}) = \\tau$.\n$$\n\\sigma(-k \\, s(\\mathbf{x}) + b) = \\tau\n$$\nTo solve for $s(\\mathbf{x})$, we apply the inverse of the sigmoid function, the logit function $\\operatorname{logit}(p) = \\log(p/(1-p))$, to both sides:\n$$\n\\operatorname{logit}(\\sigma(-k \\, s(\\mathbf{x}) + b)) = \\operatorname{logit}(\\tau)\n$$\n$$\n-k \\, s(\\mathbf{x}) + b = \\operatorname{logit}(\\tau)\n$$\nSolving for $s(\\mathbf{x})$ yields the specific SDF level, which we denote $\\delta(\\tau)$, corresponding to the occupancy threshold $\\tau$:\n$$\ns(\\mathbf{x}) = \\frac{b - \\operatorname{logit}(\\tau)}{k} = \\delta(\\tau)\n$$\nFor a single sphere centered at the origin with radius $R$, the SDF is $s(\\mathbf{x}) = \\|\\mathbf{x}\\|_2 - R$. The surface equation becomes:\n$$\n\\|\\mathbf{x}\\|_2 - R = \\delta(\\tau) \\quad \\implies \\quad \\|\\mathbf{x}\\|_2 = R + \\delta(\\tau)\n$$\nThis describes a sphere whose radius is shifted from $R$ by the offset $\\delta(\\tau)$. This defines the analytic target radius $r^{\\star}(\\tau)$:\n$$\nr^{\\star}(\\tau) = R + \\delta(\\tau) = R + \\frac{b - \\operatorname{logit}(\\tau)}{k}\n$$\nFor the union of two spheres, this same $r^{\\star}(\\tau)$ represents the target radius of each spherical component, measured from its respective center.\n\n**2. Algorithmic Implementation**\n\nThe problem is solved computationally by discretizing the continuous domain and applying a sequence of defined operations for each test case.\n\n**Step 1: Domain Discretization**\nThe continuous domain $[-1, 1]^3$ is discretized into a uniform grid of $N \\times N \\times N$ voxels, where $N=64$. The center of each voxel $(\\mathbf{p}_{ijk})$ is used as the sample point for evaluating the functions.\n\n**Step 2: Occupancy Volume Generation**\nFor each test case, defined by a shape type, noise amplitude $\\varepsilon$, and threshold $\\tau$, we perform the following:\n- The appropriate SDF, $s(\\mathbf{x})$, is evaluated at every grid point. For shape $1$, this is a single sphere SDF. For shape $2$, it is the SDF of the union of two spheres.\n- The noise field, $n(\\mathbf{x}) = \\varepsilon \\sin(\\omega x)\\sin(\\omega y)\\sin(\\omega z)$, is evaluated at every grid point.\n- The full occupancy field $o_{\\theta}(\\mathbf{x})$ is computed on the grid using the given formula.\n- A binary occupancy volume, $\\mathcal{V}_{\\tau}$, is generated by thresholding the field: a voxel is marked as occupied (value $1$) if its occupancy value is greater than or equal to $\\tau$, and unoccupied (value $0$) otherwise.\n\n**Step 3: Topological Analysis ($C(\\tau)$)**\nThe number of connected components, $C(\\tau)$, in the binary volume $\\mathcal{V}_{\\tau}$ is determined. This is achieved using a 3D connected-component labeling algorithm. The connectivity is defined as $6$-connectivity, meaning voxels are considered connected if they share a face. This is implemented using the `scipy.ndimage.label` function with a structuring element generated by `scipy.ndimage.generate_binary_structure(3, 1)`.\n\n**Step 4: Geometric Fidelity Analysis ($E(\\tau)$)**\nThe a priori computed value $r^{\\star}(\\tau)$ serves as the ground truth radius. The geometric error $E(\\tau)$ is calculated as the mean absolute deviation of the boundary voxels' radial distance from this target.\n- **Boundary Voxel Identification**: A voxel in $\\mathcal{V}_{\\tau}$ is considered a boundary voxel if it is occupied and at least one of its $6$ face-adjacent neighbors is unoccupied. A computationally efficient method to find the set of boundary voxels is to take the set of all occupied voxels and subtract the set of *interior* voxels. The interior voxels are those for which the voxel itself and all its $6$-connected neighbors are occupied. This set can be found using a binary erosion operation (`scipy.ndimage.binary_erosion`) with the same $6$-connectivity structuring element.\n- **Error Calculation**: For each identified boundary voxel center $\\mathbf{p}_i$:\n    1. The nearest sphere center, $\\mathbf{c}(\\mathbf{p}_i)$, is identified. For the single-sphere case, this is always the origin $\\mathbf{0}$. For the two-sphere case, it is the center to which $\\mathbf{p}_i$ is closer in Euclidean distance.\n    2. The radial distance $r_i = \\|\\mathbf{p}_i - \\mathbf{c}(\\mathbf{p}_i)\\|_2$ is computed.\n    3. The absolute deviation from the target radius, $|\\,r_i - r^{\\star}(\\tau)\\,|$, is calculated.\n- The final metric, $E(\\tau)$, is the arithmetic mean of these deviations over all $M$ boundary voxels. If no boundary voxels exist ($M=0$), $E(\\tau)$ is defined as $0$.\n\nThis procedure is repeated for all five test cases, and the resulting pairs $[C(\\tau), E(\\tau)]$ are collected and formatted as specified.", "answer": "```python\nimport numpy as np\nfrom scipy.ndimage import label, generate_binary_structure, binary_erosion\n\ndef solve():\n    \"\"\"\n    Computes topological and geometric metrics for an implicit surface.\n    \"\"\"\n    # Define constants from the problem statement.\n    N = 64\n    k = 16.0\n    b = 0.0\n    R = 0.35\n    omega = 12.0\n    d = 0.8\n\n    # Set up the computational grid.\n    coords_1d = np.linspace(-1.0, 1.0, N)\n    grid_x, grid_y, grid_z = np.meshgrid(coords_1d, coords_1d, coords_1d, indexing='ij')\n\n    # Define the test cases.\n    test_cases = [\n        # (shape, epsilon, tau)\n        # shape=1: single sphere, shape=2: two spheres\n        (1, 0.0, 0.5),\n        (1, 0.12, 0.5),\n        (2, 0.0, 0.7),\n        (2, 0.0, 0.31),\n        (2, 0.05, 0.30)\n    ]\n\n    results = []\n\n    for case in test_cases:\n        shape, epsilon, tau = case\n\n        # --- Step 1: Derive analytic target radius r_star ---\n        # Handle logit(tau) for tau near 0 or 1.\n        if tau = 0.0 or tau = 1.0:\n            # According to the problem tau is in (0,1), but as a safeguard.\n            logit_tau = np.nan\n        else:\n            logit_tau = np.log(tau / (1.0 - tau))\n        \n        delta_tau = (b - logit_tau) / k\n        r_star = R + delta_tau\n\n        # --- Step 2: Evaluate the occupancy field on the grid ---\n        # SDF calculation\n        if shape == 1:\n            c1 = np.array([0.0, 0.0, 0.0])\n            centers = [c1]\n            s_xyz = np.sqrt(grid_x**2 + grid_y**2 + grid_z**2) - R\n        else:  # shape == 2\n            c1 = np.array([-d / 2, 0.0, 0.0])\n            c2 = np.array([d / 2, 0.0, 0.0])\n            centers = [c1, c2]\n            dist1 = np.sqrt((grid_x - c1[0])**2 + (grid_y - c1[1])**2 + (grid_z - c1[2])**2) - R\n            dist2 = np.sqrt((grid_x - c2[0])**2 + (grid_y - c2[1])**2 + (grid_z - c2[2])**2) - R\n            s_xyz = np.minimum(dist1, dist2)\n        \n        # Noise field\n        n_xyz = epsilon * np.sin(omega * grid_x) * np.sin(omega * grid_y) * np.sin(omega * grid_z)\n        \n        # Occupancy field (sigmoid)\n        z_arg = -k * s_xyz + b + n_xyz\n        o_xyz = 1.0 / (1.0 + np.exp(-z_arg))\n        \n        # Threshold to get the binary occupancy volume\n        occupancy_volume = o_xyz = tau\n\n        # --- Step 3: Compute number of connected components C(tau) ---\n        if not np.any(occupancy_volume):\n            C_tau = 0\n        else:\n            # 6-connectivity for 3D\n            structure = generate_binary_structure(3, 1)\n            _, C_tau = label(occupancy_volume, structure=structure)\n\n        # --- Step 4: Compute mean absolute radial deviation E(tau) ---\n        # Identify boundary voxels using erosion\n        structure = generate_binary_structure(3, 1)\n        eroded_volume = binary_erosion(occupancy_volume, structure=structure)\n        boundary_voxels_mask = occupancy_volume  ~eroded_volume\n\n        boundary_indices = np.argwhere(boundary_voxels_mask)\n        M = len(boundary_indices)\n\n        if M == 0:\n            E_tau = 0.0\n        else:\n            # Convert voxel indices to world coordinates\n            boundary_coords = -1.0 + boundary_indices * (2.0 / (N - 1))\n            \n            total_abs_dev = 0.0\n            for p_i in boundary_coords:\n                # Find the nearest sphere center\n                if shape == 1:\n                    nearest_center = centers[0]\n                else:\n                    dist_to_c1 = np.linalg.norm(p_i - centers[0])\n                    dist_to_c2 = np.linalg.norm(p_i - centers[1])\n                    if dist_to_c1  dist_to_c2:\n                        nearest_center = centers[0]\n                    else:\n                        nearest_center = centers[1]\n                \n                # Calculate deviation for this boundary point\n                radial_dist = np.linalg.norm(p_i - nearest_center)\n                abs_dev = np.abs(radial_dist - r_star)\n                total_abs_dev += abs_dev\n            \n            E_tau = total_abs_dev / M\n        \n        # Store the results for the current case\n        results.append([C_tau, round(E_tau, 4)])\n\n    # Format the final output string.\n    # e.g., [[1,0.0123],[2,0.0456]]\n    result_str = \",\".join([f\"[{c},{e}]\" for c, e in results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3136800"}]}