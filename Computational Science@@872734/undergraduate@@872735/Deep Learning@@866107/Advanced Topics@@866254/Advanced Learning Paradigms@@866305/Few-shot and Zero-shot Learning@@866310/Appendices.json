{"hands_on_practices": [{"introduction": "Prototypical networks often convert distances to class probabilities using a softmax function with a temperature parameter $\\tau$. While this temperature is frequently treated as a tunable hyperparameter, this practice reveals its profound connection to the underlying probabilistic nature of the data. By working through this problem [@problem_id:3125741], you will derive the Bayes-optimal value for $\\tau$ under Gaussian assumptions, providing a clear, principled interpretation for this crucial parameter and linking a common deep learning heuristic to fundamental statistical theory.", "problem": "Consider a few-shot classification scenario in deep learning where each class is represented by a prototype in an embedding space. Let the embedding map be denoted by $\\phi(\\cdot)$, and assume there are $C$ classes with class prototypes $\\mu_{1}, \\mu_{2}, \\dots, \\mu_{C}$ in $\\mathbb{R}^{m}$. The classifier assigns a label to an input $x$ by computing a softmax over negative distances to the class prototypes with a temperature parameter $\\tau > 0$, specifically\n$$\np_{\\tau}(y=c \\mid x) \\;=\\; \\frac{\\exp\\!\\big(-\\tau\\, d\\!\\big(\\phi(x), \\mu_{c}\\big)\\big)}{\\sum_{k=1}^{C} \\exp\\!\\big(-\\tau\\, d\\!\\big(\\phi(x), \\mu_{k}\\big)\\big)},\n$$\nwhere $d\\!\\big(\\phi(x), \\mu_{c}\\big) = \\|\\phi(x) - \\mu_{c}\\|_{2}^{2}$ is the squared Euclidean distance. Suppose the class-conditional distribution of embeddings is Gaussian with shared isotropic covariance across classes, namely for each class $c$, the random vector $z = \\phi(x)$ given $y=c$ is distributed as a multivariate normal with mean $\\mu_{c}$ and covariance matrix $\\sigma^{2} I_{m}$, written as $z \\mid y=c \\sim \\mathcal{N}\\!\\big(\\mu_{c}, \\sigma^{2} I_{m}\\big)$, where $\\sigma^{2} > 0$ and $I_{m}$ is the $m \\times m$ identity matrix. Assume equal class priors $p(y=c) = \\frac{1}{C}$ for all $c \\in \\{1, 2, \\dots, C\\}$.\n\nStarting from the definition of the Gaussian probability density function and Bayes' rule, derive the value of the temperature parameter $\\tau$ for the above softmax classifier that makes $p_{\\tau}(y=c \\mid x)$ match the Bayes-optimal posterior $p(y=c \\mid x)$ under the stated assumptions. Express your final answer as a single closed-form symbolic expression in terms of $\\sigma^{2}$ only. No numerical approximation or rounding is required.", "solution": "The objective is to determine the value of the temperature parameter $\\tau$ that makes the given softmax classifier's posterior probability distribution $p_{\\tau}(y=c \\mid x)$ identical to the Bayes-optimal posterior probability $p(y=c \\mid x)$ under the specified modeling assumptions.\n\nFirst, let's derive the Bayes-optimal posterior probability $p(y=c \\mid x)$. Let $z = \\phi(x)$ be the embedding of the input $x$ in $\\mathbb{R}^{m}$. According to Bayes' rule, the posterior probability of class $c$ given the embedding $z$ is:\n$$p(y=c \\mid z) = \\frac{p(z \\mid y=c) p(y=c)}{p(z)}$$\nThe denominator $p(z)$ is the marginal probability density of $z$, which can be expressed using the law of total probability by summing over all possible classes:\n$$p(z) = \\sum_{k=1}^{C} p(z \\mid y=k) p(y=k)$$\nSubstituting this into the Bayes' rule expression gives:\n$$p(y=c \\mid z) = \\frac{p(z \\mid y=c) p(y=c)}{\\sum_{k=1}^{C} p(z \\mid y=k) p(y=k)}$$\n\nThe problem provides the following information:\n1.  The class-conditional distribution of the embeddings $z = \\phi(x)$ for any class $c$ is a multivariate normal distribution with mean $\\mu_{c}$ and covariance matrix $\\sigma^{2} I_{m}$. This is denoted as $z \\mid y=c \\sim \\mathcal{N}(\\mu_{c}, \\sigma^{2} I_{m})$.\n2.  The class priors are uniform, i.e., $p(y=c) = \\frac{1}{C}$ for all $c \\in \\{1, 2, \\dots, C\\}$.\n\nThe probability density function (PDF) of the multivariate normal distribution $\\mathcal{N}(\\mu_{c}, \\sigma^{2} I_{m})$ for a vector $z \\in \\mathbb{R}^{m}$ is:\n$$p(z \\mid y=c) = \\frac{1}{(2\\pi)^{m/2} \\det(\\sigma^{2} I_{m})^{1/2}} \\exp\\left(-\\frac{1}{2} (z - \\mu_{c})^{\\top} (\\sigma^{2} I_{m})^{-1} (z - \\mu_{c})\\right)$$\nLet's simplify the terms in this PDF. The determinant of the covariance matrix is $\\det(\\sigma^{2} I_{m}) = (\\sigma^{2})^{m} \\det(I_{m}) = (\\sigma^{2})^{m}$. The inverse of the covariance matrix is $(\\sigma^{2} I_{m})^{-1} = \\frac{1}{\\sigma^{2}} I_{m}^{-1} = \\frac{1}{\\sigma^{2}} I_{m}$.\nThe quadratic form in the exponent becomes:\n$$(z - \\mu_{c})^{\\top} \\left(\\frac{1}{\\sigma^{2}} I_{m}\\right) (z - \\mu_{c}) = \\frac{1}{\\sigma^{2}} (z - \\mu_{c})^{\\top} (z - \\mu_{c}) = \\frac{1}{\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}$$\nSubstituting these back into the PDF expression, we get:\n$$p(z \\mid y=c) = \\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)$$\n\nNow, we substitute both the PDF and the uniform prior $p(y=c) = \\frac{1}{C}$ into the expression for the Bayes-optimal posterior:\n$$p(y=c \\mid z) = \\frac{\\left(\\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)\\right) \\left(\\frac{1}{C}\\right)}{\\sum_{k=1}^{C} \\left(\\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{k}\\|_{2}^{2}\\right)\\right) \\left(\\frac{1}{C}\\right)}$$\nThe constant terms $\\frac{1}{(2\\pi\\sigma^{2})^{m/2}}$ and $\\frac{1}{C}$ are common to all terms in the numerator and the denominator's sum, so they cancel out. The simplified expression for the Bayes-optimal posterior is:\n$$p(y=c \\mid z) = \\frac{\\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{k}\\|_{2}^{2}\\right)}$$\n\nNext, we compare this derived posterior with the classifier model given in the problem statement. The model's probability distribution is:\n$$p_{\\tau}(y=c \\mid x) = \\frac{\\exp\\big(-\\tau\\, d\\big(\\phi(x), \\mu_{c}\\big)\\big)}{\\sum_{k=1}^{C} \\exp\\big(-\\tau\\, d\\big(\\phi(x), \\mu_{k}\\big)\\big)}$$\nUsing $z = \\phi(x)$ and the definition of the distance metric $d(z, \\mu_c) = \\|z - \\mu_{c}\\|_{2}^{2}$, we can rewrite the classifier's probability as:\n$$p_{\\tau}(y=c \\mid x) = \\frac{\\exp\\left(-\\tau \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\tau \\|z - \\mu_{k}\\|_{2}^{2}\\right)}$$\n\nFor the classifier to be Bayes-optimal, we must have $p_{\\tau}(y=c \\mid x) = p(y=c \\mid z)$ for all $c$ and $z$.\n$$\\frac{\\exp\\left(-\\tau \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\tau \\|z - \\mu_{k}\\|_{2}^{2}\\right)} = \\frac{\\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{k}\\|_{2}^{2}\\right)}$$\nBoth expressions are in the form of a softmax function applied to a set of scores. For the outputs to be identical for any set of prototypes $\\{\\mu_{k}\\}$ and any embedding $z$, the arguments of the corresponding exponential functions in the numerator and denominator must be equal (up to an additive constant that is independent of the class index $c$, which is zero in this case).\nBy comparing the exponents for class $c$, we obtain the equality:\n$$-\\tau \\|z - \\mu_{c}\\|_{2}^{2} = -\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}$$\nSince this must hold for any $z$, and we can choose $z$ such that $\\|z - \\mu_{c}\\|_{2}^{2} \\neq 0$, we can divide both sides by $-\\|z - \\mu_{c}\\|_{2}^{2}$:\n$$\\tau = \\frac{1}{2\\sigma^{2}}$$\nThis provides the value of the temperature parameter $\\tau$ in terms of the variance $\\sigma^{2}$ of the class-conditional embedding distributions. The condition $\\sigma^{2} > 0$ ensures that $\\tau$ is well-defined and positive, as required.", "answer": "$$\\boxed{\\frac{1}{2\\sigma^{2}}}$$", "id": "3125741"}, {"introduction": "While the simple mean is a common way to compute class prototypes, it is highly sensitive to outliers and data dispersion, especially in imbalanced few-shot scenarios. This hands-on exercise [@problem_id:3125726] challenges you to move beyond this simple baseline by first deriving a robust, weighted prototype estimator from the principle of empirical risk minimization. You will then implement and evaluate your derived method, gaining practical experience in building classifiers that are more resilient to the imperfections of real-world data.", "problem": "Consider a two-class few-shot classification task in which a metric-based classifier predicts the class of a query by comparing its embedding to class prototypes. Let the embedding function be denoted by $\\phi:\\mathbb{R}^d\\rightarrow\\mathbb{R}^d$. For a class $c\\in\\{0,1\\}$ with support embeddings $\\{y_{c,i}\\}_{i=1}^{k_c}$ where $y_{c,i}=\\phi(x_{c,i})$, a standard prototype uses the unweighted mean $\\mu_c=\\frac{1}{k_c}\\sum_{i=1}^{k_c}y_{c,i}$. In class-imbalanced few-shot settings where $k_1\\ll k_2$, the unweighted mean can be biased by dispersed points in the majority class. To mitigate this, consider a weighted prototype of the form $\\mu_c=\\sum_{i=1}^{k_c}\\alpha_{c,i}\\,y_{c,i}$, where $\\alpha_{c,i}\\ge 0$ and $\\sum_{i=1}^{k_c}\\alpha_{c,i}=1$.\n\nStarting from foundational probabilistic modeling and the principle of empirical risk minimization, derive a principled rule for choosing the weights $\\alpha_{c,i}$ that reduces the influence of outliers while respecting the constraints $\\alpha_{c,i}\\ge 0$ and $\\sum_{i=1}^{k_c}\\alpha_{c,i}=1$. Your derivation must begin with a well-posed assumption about the data-generating process in the embedding space and proceed step by step to a concrete weighting mechanism without invoking ad hoc heuristics. Then implement the resulting classifier and evaluate its performance under the following conditions.\n\nUse a two-dimensional embedding with a fixed linear embedding function $\\phi(x)=W x$, where\n$$\nW=\\begin{pmatrix}\n1.2 & 0.3\\\\\n-0.4 & 0.8\n\\end{pmatrix}.\n$$\nClassification must be performed by nearest prototype under squared Euclidean distance in the embedding space, with ties broken in favor of the numerically smaller class index. For each test case below, compute two prototypes per class: the unweighted mean and the derived weighted prototype. For each query, predict its class using each prototype set and compute accuracy as the fraction of correctly classified queries expressed as a decimal. For numerical stability in any variance or scale estimate required by your weighting rule, add a small constant $\\varepsilon=10^{-6}$ wherever division by zero might otherwise occur.\n\nTest Suite. For each test case, the support sets and queries are specified in $\\mathbb{R}^2$. All coordinates are in unitless numerical values. The task is to output, for each test case, a single float equal to the accuracy of the weighted-prototype classifier minus the accuracy of the unweighted-mean classifier, rounded to three decimal places.\n\n- Test Case A (moderate imbalance with a dispersed majority cluster):\n    - Class $0$ support ($k_0=3$): $(-0.1,\\,0.2)$, $(0.05,\\,-0.05)$, $(0.1,\\,0.0)$.\n    - Class $1$ support ($k_1=7$): $(3.8,\\,0.1)$, $(4.1,\\,-0.2)$, $(4.0,\\,0.0)$, $(5.5,\\,2.0)$, $(3.9,\\,0.2)$, $(4.2,\\,0.1)$, $(4.1,\\,0.0)$.\n    - Queries and ground-truth labels:\n        - $(-0.05,\\,0.0)\\rightarrow 0$, $(0.2,\\,-0.1)\\rightarrow 0$, $(4.05,\\,0.0)\\rightarrow 1$, $(3.7,\\,0.1)\\rightarrow 1$, $(2.1,\\,0.4)\\rightarrow 1$, $(2.0,\\,0.0)\\rightarrow 0$.\n\n- Test Case B (extreme imbalance $k_0=1\\ll k_1=10$):\n    - Class $0$ support ($k_0=1$): $(0.0,\\,0.0)$.\n    - Class $1$ support ($k_1=10$): $(4.0,\\,0.0)$, $(4.1,\\,0.1)$, $(3.9,\\,-0.1)$, $(4.2,\\,0.0)$, $(3.8,\\,0.2)$, $(4.3,\\,-0.2)$, $(4.05,\\,0.05)$, $(4.2,\\,0.2)$, $(5.0,\\,1.5)$, $(3.7,\\,-0.3)$.\n    - Queries and ground-truth labels:\n        - $(0.1,\\,0.0)\\rightarrow 0$, $(-0.2,\\,0.05)\\rightarrow 0$, $(4.05,\\,0.1)\\rightarrow 1$, $(3.9,\\,-0.05)\\rightarrow 1$, $(2.2,\\,0.2)\\rightarrow 1$, $(-1.0,\\,0.0)\\rightarrow 0$.\n\n- Test Case C (balanced but both classes have a strong outlier):\n    - Class $0$ support ($k_0=5$): $(0.0,\\,0.0)$, $(0.1,\\,-0.1)$, $(-0.05,\\,0.1)$, $(0.2,\\,0.05)$, $(-1.8,\\,-1.2)$.\n    - Class $1$ support ($k_1=5$): $(3.9,\\,0.0)$, $(4.1,\\,0.1)$, $(4.0,\\,-0.1)$, $(4.2,\\,0.0)$, $(5.2,\\,2.5)$.\n    - Queries and ground-truth labels:\n        - $(0.1,\\,0.0)\\rightarrow 0$, $(-0.2,\\,0.15)\\rightarrow 0$, $(4.1,\\,-0.05)\\rightarrow 1$, $(3.95,\\,0.2)\\rightarrow 1$, $(2.3,\\,0.5)\\rightarrow 1$, $(1.0,\\,-0.7)\\rightarrow 0$.\n\nYour program must:\n- Implement the derived weighting rule to compute $\\alpha_{c,i}$ and construct weighted prototypes $\\mu_c$.\n- Compute unweighted prototypes by simple means for comparison.\n- Classify each query by nearest prototype in the embedding space under squared Euclidean distance.\n- Compute the accuracy for the weighted and unweighted cases per test case.\n- Output a single line containing a comma-separated list with brackets of the per-test-case accuracy differences (weighted minus unweighted), each rounded to three decimal places, for Test Cases A, B, and C in that order. For example, the final output format must be exactly of the form $\\left[\\text{result}_A,\\text{result}_B,\\text{result}_C\\right]$.", "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- **Task**: Two-class few-shot classification.\n- **Classifier**: Metric-based, comparing query embedding to class prototypes.\n- **Classes**: $c \\in \\{0, 1\\}$.\n- **Embedding Function**: $\\phi:\\mathbb{R}^d\\rightarrow\\mathbb{R}^d$.\n- **Support Set**: For class $c$, $\\{x_{c,i}\\}_{i=1}^{k_c}$ are the input points.\n- **Support Embeddings**: $y_{c,i} = \\phi(x_{c,i})$.\n- **Unweighted Prototype**: $\\mu_c = \\frac{1}{k_c}\\sum_{i=1}^{k_c}y_{c,i}$.\n- **Weighted Prototype**: $\\mu_c = \\sum_{i=1}^{k_c}\\alpha_{c,i}\\,y_{c,i}$, with constraints $\\alpha_{c,i} \\ge 0$ and $\\sum_{i=1}^{k_c}\\alpha_{c,i}=1$.\n- **Derivation Goal**: Derive a principled rule for weights $\\alpha_{c,i}$ starting from probabilistic modeling and empirical risk minimization.\n- **Embedding Matrix**: For the implementation, the embedding is linear, $\\phi(x) = Wx$, with $d=2$ and\n$$\nW=\\begin{pmatrix}\n1.2 & 0.3\\\\\n-0.4 & 0.8\n\\end{pmatrix}.\n$$\n- **Classification Metric**: Nearest prototype under squared Euclidean distance.\n- **Tie-Breaking Rule**: Ties are broken in favor of the numerically smaller class index (i.e., class $0$).\n- **Numerical Stability**: Add $\\varepsilon=10^{-6}$ to denominators to prevent division by zero.\n- **Test Cases**: Three test cases (A, B, C) are provided, each with specific support sets for class $0$ and class $1$, and a set of queries with ground-truth labels.\n- **Output**: For each test case, compute the accuracy of the weighted-prototype classifier minus the accuracy of the unweighted-mean classifier, rounded to three decimal places. The final output must be a single line: `[result_A,result_B,result_C]`.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria:\n-   **Scientifically Grounded**: The problem is firmly rooted in the established principles of machine learning and statistics. It asks for a derivation based on probabilistic modeling and empirical risk minimization, which are standard, rigorous frameworks for developing algorithms. The use of a weighted mean to create robust estimators is a well-studied concept in robust statistics. The problem is scientifically sound.\n-   **Well-Posed**: The problem is clearly defined and self-contained. It provides all necessary information: the mathematical formulation, the specific embedding function, the classification rule, the tie-breaking rule, a stability constant, and a complete set of data for testing. The objective is unambiguous.\n-   **Objective**: The problem is stated in precise, formal language, free of any subjective or opinion-based claims. All data and conditions are numerical and objective.\n\nThe problem does not exhibit any of the invalidity flaws. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, ill-posed, trivial, or outside the realm of scientific verifiability.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n---\n## Derivation of the Weighting Rule\n\nThe objective is to derive a principled rule for the weights $\\alpha_{c,i}$ in the prototype definition $\\mu_c=\\sum_{i=1}^{k_c}\\alpha_{c,i}\\,y_{c,i}$, starting from a probabilistic model and the principle of empirical risk minimization. The rule should mitigate the influence of outliers.\n\n1.  **Probabilistic Model**: We begin by modeling the support embeddings $\\{y_{c,i}\\}_{i=1}^{k_c}$ of a class $c$ as noisy observations of an unknown true class prototype $\\theta_c$. To account for outliers, we assume that each observation $y_{c,i}$ is generated from a Gaussian distribution centered at $\\theta_c$ but with its own individual variance $\\sigma_{c,i}^2$. That is, $y_{c,i} \\sim \\mathcal{N}(\\theta_c, \\sigma_{c,i}^2 I)$, where $I$ is the identity matrix. A large variance $\\sigma_{c,i}^2$ for a point $y_{c,i}$ signifies low confidence that this point is a representative sample of the true center $\\theta_c$, which provides a natural mechanism for handling outliers.\n\n2.  **Empirical Risk Minimization via Maximum Likelihood**: Under this model, the principle of empirical risk minimization corresponds to finding the prototype $\\theta_c$ that maximizes the log-likelihood of observing the support set. Assuming the observations are independent, the log-likelihood is:\n    $$ \\mathcal{L}(\\theta_c) = \\ln \\prod_{i=1}^{k_c} p(y_{c,i} | \\theta_c, \\sigma_{c,i}^2) = \\sum_{i=1}^{k_c} \\ln p(y_{c,i} | \\theta_c, \\sigma_{c,i}^2) $$\n    For our Gaussian model in a $d$-dimensional space, this becomes:\n    $$ \\mathcal{L}(\\theta_c) = \\sum_{i=1}^{k_c} \\left( -\\frac{\\|y_{c,i} - \\theta_c\\|^2}{2\\sigma_{c,i}^2} - \\frac{d}{2}\\ln(2\\pi\\sigma_{c,i}^2) \\right) $$\n    Maximizing $\\mathcal{L}(\\theta_c)$ with respect to $\\theta_c$ is equivalent to minimizing the negative log-likelihood. Dropping terms that do not depend on $\\theta_c$, we are left with minimizing the following risk function $R(\\theta_c)$:\n    $$ R(\\theta_c) = \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} \\|y_{c,i} - \\theta_c\\|^2 $$\n    This is a weighted least-squares problem where each point's contribution to the loss is inversely proportional to its variance.\n\n3.  **Solving for the Prototype**: To find the optimal prototype (our estimate of $\\theta_c$), we set the gradient of $R(\\theta_c)$ with respect to $\\theta_c$ to zero:\n    $$ \\nabla_{\\theta_c} R(\\theta_c) = \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} \\cdot 2(\\theta_c - y_{c,i}) = 0 $$\n    Solving for $\\theta_c$ yields:\n    $$ \\left( \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} \\right) \\theta_c = \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} y_{c,i} $$\n    The resulting estimate for the prototype, which we denote $\\mu_c$, is a weighted average of the support embeddings:\n    $$ \\mu_c = \\frac{\\sum_{i=1}^{k_c} (1/\\sigma_{c,i}^2) y_{c,i}}{\\sum_{j=1}^{k_c} (1/\\sigma_{c,j}^2)} $$\n    This corresponds to the desired form $\\mu_c = \\sum_{i=1}^{k_c} \\alpha_{c,i} y_{c,i}$, where the weights are $\\alpha_{c,i} = \\frac{w_{c,i}}{\\sum_{j=1}^{k_c} w_{c,j}}$ with $w_{c,i} = 1/\\sigma_{c,i}^2$. These weights automatically satisfy the constraints $\\alpha_{c,i} \\ge 0$ and $\\sum_i \\alpha_{c,i}=1$.\n\n4.  **Deriving the Weighting Rule**: The final step is to define the individual variances $\\sigma_{c,i}^2$, which are unknown. A principled, non-iterative approach is to use a data-driven proxy for the reliability of each point. A point's deviation from a preliminary estimate of the class center is a good measure of its potential \"outlierness\". The standard unweighted mean, $\\mu_{c, \\text{unw}} = \\frac{1}{k_c}\\sum_j y_{c,j}$, is the simplest and most natural choice for this preliminary estimate (it is the maximum likelihood estimate if all variances are assumed to be equal).\n\n    We therefore model each point's variance $\\sigma_{c,i}^2$ as being proportional to its squared Euclidean distance to this unweighted mean:\n    $$ \\sigma_{c,i}^2 \\propto \\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2 $$\n    This provides a concrete rule: points far from the initial \"center of mass\" are deemed less reliable (higher variance) and are consequently assigned smaller weights in the calculation of the final prototype. The unnormalized weights are inversely proportional to this variance:\n    $$ w_{c,i} \\propto \\frac{1}{\\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2} $$\n    To handle the case where a point may land exactly on the mean and to ensure numerical stability, we add the small constant $\\varepsilon=10^{-6}$ as specified. The final, concrete form for the unnormalized weights is:\n    $$ w_{c,i} = \\frac{1}{\\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2 + \\varepsilon} $$\n    These weights are then normalized to obtain $\\alpha_{c,i}$, which are used to compute the final weighted prototype. For the special case $k_c=1$, the distance is $0$, the unnormalized weight is $1/\\varepsilon$, and the normalized weight is $1$, correctly yielding the point itself as the prototype. This completes the derivation.\n\n## Algorithm and Implementation\nThe derived method is implemented as follows:\n1.  For each class $c \\in \\{0, 1\\}$, take the support points $\\{x_{c,i}\\}$ and apply the embedding function $\\phi(x) = Wx$ to get the embeddings $\\{y_{c,i}\\}$.\n2.  **Unweighted Prototypes**: For each class, compute the standard mean of the embeddings: $\\mu_{c, \\text{unw}} = \\frac{1}{k_c}\\sum_{i=1}^{k_c} y_{c,i}$.\n3.  **Weighted Prototypes**: For each class:\n    a. If $k_c = 1$, the weighted prototype is simply the single support embedding.\n    b. If $k_c > 1$, first compute the unweighted prototype $\\mu_{c, \\text{unw}}$.\n    c. For each support embedding $y_{c,i}$, compute an unnormalized weight $w_{c,i} = 1 / (\\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2 + \\varepsilon)$.\n    d. Normalize the weights: $\\alpha_{c,i} = w_{c,i} / \\sum_{j=1}^{k_c} w_{c,j}$.\n    e. Compute the weighted prototype: $\\mu_{c, \\text{w}} = \\sum_{i=1}^{k_c} \\alpha_{c,i} y_{c,i}$.\n4.  **Classification and Evaluation**: For each test case:\n    a. Embed the query points $\\{x_q\\}$.\n    b. For each query embedding $y_q$, calculate its squared Euclidean distance to the prototypes $\\{\\mu_{0, \\text{unw}}, \\mu_{1, \\text{unw}}\\}$ and $\\{\\mu_{0, \\text{w}}, \\mu_{1, \\text{w}}\\}$.\n    c. Assign the class label of the nearest prototype for each method, breaking ties in favor of class $0$.\n    d. Calculate the accuracy for the unweighted and weighted methods by comparing predictions to the ground-truth labels.\n    e. Compute the difference: (weighted accuracy) - (unweighted accuracy).\n5.  Collect the accuracy differences for all test cases and format the output as requested.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the few-shot classification problem by deriving and implementing a\n    principled weighting scheme for robust prototypes.\n    \"\"\"\n\n    # ---- Global parameters from the problem statement ----\n    W = np.array([[1.2, 0.3], [-0.4, 0.8]])\n    epsilon = 1e-6\n\n    # ---- Test cases ----\n    # Each case is a tuple: (support_set_dict, query_points_array, query_labels_array)\n    test_cases = [\n        (\n            {  # Support set for Test Case A\n                0: np.array([[-0.1, 0.2], [0.05, -0.05], [0.1, 0.0]]),\n                1: np.array([(3.8, 0.1), (4.1, -0.2), (4.0, 0.0), (5.5, 2.0), (3.9, 0.2), (4.2, 0.1), (4.1, 0.0)])\n            },\n            np.array([(-0.05, 0.0), (0.2, -0.1), (4.05, 0.0), (3.7, 0.1), (2.1, 0.4), (2.0, 0.0)]),\n            np.array([0, 0, 1, 1, 1, 0])\n        ),\n        (\n            {  # Support set for Test Case B\n                0: np.array([[0.0, 0.0]]),\n                1: np.array([(4.0, 0.0), (4.1, 0.1), (3.9, -0.1), (4.2, 0.0), (3.8, 0.2), (4.3, -0.2), (4.05, 0.05), (4.2, 0.2), (5.0, 1.5), (3.7, -0.3)])\n            },\n            np.array([(0.1, 0.0), (-0.2, 0.05), (4.05, 0.1), (3.9, -0.05), (2.2, 0.2), (-1.0, 0.0)]),\n            np.array([0, 0, 1, 1, 1, 0])\n        ),\n        (\n            {  # Support set for Test Case C\n                0: np.array([(0.0, 0.0), (0.1, -0.1), (-0.05, 0.1), (0.2, 0.05), (-1.8, -1.2)]),\n                1: np.array([(3.9, 0.0), (4.1, 0.1), (4.0, -0.1), (4.2, 0.0), (5.2, 2.5)])\n            },\n            np.array([(0.1, 0.0), (-0.2, 0.15), (4.1, -0.05), (3.95, 0.2), (2.3, 0.5), (1.0, -0.7)]),\n            np.array([0, 0, 1, 1, 1, 0])\n        )\n    ]\n\n    results = []\n\n    # ---- Helper functions ----\n    def embed_points(points, W_matrix):\n        if points.ndim == 1:\n            points = points.reshape(1, -1)\n        return (W_matrix @ points.T).T\n\n    def get_prototypes(support_embeddings, eps):\n        prototypes_unweighted = {}\n        prototypes_weighted = {}\n        \n        for c in sorted(support_embeddings.keys()):\n            points = support_embeddings[c]\n            k_c = len(points)\n            \n            # Unweighted prototype: simple mean\n            mu_unweighted = np.mean(points, axis=0)\n            prototypes_unweighted[c] = mu_unweighted\n            \n            # Weighted prototype: derived from the robust estimation principle\n            if k_c == 1:\n                mu_weighted = points[0]\n            else:\n                dists_sq = np.sum((points - mu_unweighted)**2, axis=1)\n                raw_weights = 1.0 / (dists_sq + eps)\n                normalized_weights = raw_weights / np.sum(raw_weights)\n                mu_weighted = np.sum(points * normalized_weights[:, np.newaxis], axis=0)\n            \n            prototypes_weighted[c] = mu_weighted\n            \n        return prototypes_unweighted, prototypes_weighted\n\n    def classify_queries(prototypes_dict, query_embeddings):\n        # Prototypes are already created for sorted class indices\n        prototypes = [prototypes_dict[i] for i in sorted(prototypes_dict.keys())]\n        predictions = []\n        for q in query_embeddings:\n            dists_sq = [np.sum((q - p)**2) for p in prototypes]\n            # np.argmin breaks ties by choosing the first occurrence, which\n            # corresponds to the smaller class index as required.\n            predictions.append(np.argmin(dists_sq))\n        return np.array(predictions)\n\n    # ---- Main loop for test case evaluation ----\n    for case in test_cases:\n        support_x, query_x, labels = case\n        \n        # 1. Embed all points using the given matrix W\n        support_y = {c: embed_points(pts, W) for c, pts in support_x.items()}\n        query_y = embed_points(query_x, W)\n        \n        # 2. Get both unweighted and weighted prototypes for each class\n        protos_unw, protos_w = get_prototypes(support_y, epsilon)\n        \n        # 3. Classify queries and compute accuracy for the unweighted method\n        preds_unw = classify_queries(protos_unw, query_y)\n        acc_unw = np.mean(preds_unw == labels)\n\n        # 4. Classify queries and compute accuracy for the weighted method\n        preds_w = classify_queries(protos_w, query_y)\n        acc_w = np.mean(preds_w == labels)\n        \n        # 5. Calculate and store the accuracy difference, rounded to 3 decimal places\n        accuracy_diff = acc_w - acc_unw\n        results.append(round(accuracy_diff, 3))\n        \n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3125726"}, {"introduction": "When faced with extreme data scarcity, one powerful strategy is to augment the support set with synthetic examples from a generative model. However, is a \"better\" generator—one with a lower Frechet Inception Distance (FID)—always more helpful? This practice [@problem_id:3125790] guides you through a theoretical and computational analysis to answer this question, deriving the connection between a generator's bias, its FID score, and its \"shot efficiency\" in a few-shot context. This will equip you with a deeper, more critical understanding of the trade-offs involved in using generative models for few-shot augmentation.", "problem": "You will implement and analyze a simplified generative augmentation pipeline to study when Frechet Inception Distance (FID) correlates with shot efficiency in few-shot learning. Consider a conditional generator modeled at the feature level as follows. For a novel class, the true data distribution is assumed to be multivariate Gaussian with mean $\\boldsymbol{\\mu}_{\\text{true}} \\in \\mathbb{R}^d$ and covariance $\\sigma^2 \\mathbf{I}_d$, written as $\\mathcal{N}(\\boldsymbol{\\mu}_{\\text{true}}, \\sigma^2 \\mathbf{I}_d)$. A conditional generator trained on base classes and then fine-tuned only through the novel class embedding with $k$ examples yields a synthetic distribution for the same class that is also modeled as Gaussian, with mean $\\boldsymbol{\\mu}_{\\text{syn}} = \\boldsymbol{\\mu}_{\\text{true}} + \\boldsymbol{b}$ and covariance $\\tau^2 \\mathbf{I}_d$, written as $\\mathcal{N}(\\boldsymbol{\\mu}_{\\text{true}} + \\boldsymbol{b}, \\tau^2 \\mathbf{I}_d)$. Here $\\boldsymbol{b} \\in \\mathbb{R}^d$ is a deterministic bias vector that captures residual mismatch after fine-tuning only the class embedding; you will treat $\\lVert \\boldsymbol{b} \\rVert_2$ as a controllable parameter.\n\nYour task is to derive principled formulas from fundamental facts and then write a program that computes, for several test cases, the Pearson correlation coefficient (PCC) between FID and shot efficiency as the bias magnitude varies. Use only the following foundational bases:\n- For independent and identically distributed Gaussian samples $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_k \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}_d)$, the sample mean $\\bar{\\boldsymbol{x}}$ satisfies $\\mathbb{E}[\\bar{\\boldsymbol{x}}] = \\boldsymbol{\\mu}$ and $\\mathrm{Cov}(\\bar{\\boldsymbol{x}}) = \\frac{\\sigma^2}{k} \\mathbf{I}_d$.\n- For two Gaussians with means $\\boldsymbol{m}_1, \\boldsymbol{m}_2$ and covariances $\\mathbf{C}_1, \\mathbf{C}_2$, the Frechet Inception Distance (FID) is given by\n$$\n\\mathrm{FID} = \\lVert \\boldsymbol{m}_1 - \\boldsymbol{m}_2 \\rVert_2^2 + \\mathrm{Tr}\\!\\left(\\mathbf{C}_1 + \\mathbf{C}_2 - 2\\left(\\mathbf{C}_1^{1/2} \\mathbf{C}_2 \\mathbf{C}_1^{1/2}\\right)^{1/2}\\right).\n$$\n- The Pearson correlation coefficient (PCC) between two real-valued lists $\\{u_i\\}_{i=1}^n$ and $\\{v_i\\}_{i=1}^n$ is\n$$\n\\rho = \\frac{\\sum_{i=1}^n (u_i - \\bar{u})(v_i - \\bar{v})}{\\sqrt{\\sum_{i=1}^n (u_i - \\bar{u})^2} \\sqrt{\\sum_{i=1}^n (v_i - \\bar{v})^2}},\n$$\nwith $\\bar{u}$ and $\\bar{v}$ denoting empirical means.\n\nDefine the following shot-efficiency model. You have $k$ real samples from the true distribution and $n_{\\text{syn}}$ synthetic samples from the generator. Let $\\bar{\\boldsymbol{x}}_{\\text{real}}$ denote the sample mean of the $k$ real samples, and let $\\bar{\\boldsymbol{x}}_{\\text{syn}}$ denote the sample mean of the $n_{\\text{syn}}$ synthetic samples. Assume $\\bar{\\boldsymbol{x}}_{\\text{real}}$ and $\\bar{\\boldsymbol{x}}_{\\text{syn}}$ are independent. Consider linear combination estimators of the form\n$$\n\\hat{\\boldsymbol{\\mu}}(a) = a \\, \\bar{\\boldsymbol{x}}_{\\text{real}} + (1 - a)\\,\\bar{\\boldsymbol{x}}_{\\text{syn}}, \\quad a \\in \\mathbb{R}.\n$$\nDefine the baseline mean-squared error (MSE) using only the real $k$-shot mean as\n$$\n\\mathrm{MSE}_{\\text{base}} = \\mathbb{E}\\left[\\lVert \\bar{\\boldsymbol{x}}_{\\text{real}} - \\boldsymbol{\\mu}_{\\text{true}}\\rVert_2^2\\right].\n$$\nDefine the optimal augmented MSE as\n$$\n\\mathrm{MSE}_{\\text{opt}} = \\min_{a \\in \\mathbb{R}} \\mathbb{E}\\left[\\lVert \\hat{\\boldsymbol{\\mu}}(a) - \\boldsymbol{\\mu}_{\\text{true}}\\rVert_2^2\\right].\n$$\nDefine shot efficiency as the ratio\n$$\n\\mathrm{SE} = \\frac{\\mathrm{MSE}_{\\text{base}}}{\\mathrm{MSE}_{\\text{opt}}}.\n$$\n\nYour program must:\n- For each test case, evaluate a set of bias magnitudes $\\mathcal{B} = \\{\\lVert \\boldsymbol{b} \\rVert_2\\}$, compute the corresponding FID and SE values for each $\\lVert \\boldsymbol{b} \\rVert_2 \\in \\mathcal{B}$, and then compute the Pearson correlation coefficient between the list of FID values and the list of SE values for that test case.\n- Produce a single line of output containing the correlation coefficients for all test cases as a comma-separated list enclosed in square brackets, with each coefficient rounded to $6$ decimals.\n\nAssumptions to use for derivations and implementation:\n- The $k$ real samples are independent draws from $\\mathcal{N}(\\boldsymbol{\\mu}_{\\text{true}}, \\sigma^2 \\mathbf{I}_d)$.\n- The $n_{\\text{syn}}$ synthetic samples are independent draws from $\\mathcal{N}(\\boldsymbol{\\mu}_{\\text{true}} + \\boldsymbol{b}, \\tau^2 \\mathbf{I}_d)$, with $\\boldsymbol{b}$ treated as a fixed bias vector whose magnitude is specified; only $\\lVert \\boldsymbol{b} \\rVert_2$ matters for all required quantities.\n- You must treat $\\bar{\\boldsymbol{x}}_{\\text{real}}$ and $\\bar{\\boldsymbol{x}}_{\\text{syn}}$ as independent random vectors with the appropriate means and isotropic covariances implied by the above.\n\nTest suite of parameter sets to be evaluated (each test case is a tuple $(d, k, n_{\\text{syn}}, \\sigma, \\tau, \\mathcal{B})$):\n- Case $1$: $(8, 5, 100, 1.0, 1.0, \\{0.0, 0.25, 0.5, 1.0, 2.0\\})$.\n- Case $2$: $(16, 5, 20, 1.0, 2.0, \\{0.0, 0.5, 1.0, 1.5, 2.5\\})$.\n- Case $3$: $(8, 1, 50, 1.0, 0.5, \\{0.0, 0.2, 0.7, 1.2, 2.0\\})$.\n- Case $4$: $(32, 100, 1000, 1.0, 1.5, \\{0.0, 0.1, 0.3, 0.8, 1.5\\})$.\n- Case $5$: $(4, 2, 1, 1.0, 3.0, \\{0.0, 0.5, 1.0, 3.0, 5.0\\})$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5]$), where $r_i$ is the Pearson correlation coefficient for test case $i$, rounded to $6$ decimals.", "solution": "We begin from the fundamental assumptions and derive the closed-form expressions needed for implementation.\n\nFirst, consider the two Gaussian distributions. The true distribution is $\\mathcal{N}(\\boldsymbol{\\mu}_{\\text{true}}, \\sigma^2 \\mathbf{I}_d)$ and the synthetic distribution is $\\mathcal{N}(\\boldsymbol{\\mu}_{\\text{true}} + \\boldsymbol{b}, \\tau^2 \\mathbf{I}_d)$. The Frechet Inception Distance (FID) between two Gaussians with means $\\boldsymbol{m}_1, \\boldsymbol{m}_2$ and covariances $\\mathbf{C}_1, \\mathbf{C}_2$ is\n$$\n\\mathrm{FID} = \\lVert \\boldsymbol{m}_1 - \\boldsymbol{m}_2 \\rVert_2^2 + \\mathrm{Tr}\\!\\left(\\mathbf{C}_1 + \\mathbf{C}_2 - 2\\left(\\mathbf{C}_1^{1/2} \\mathbf{C}_2 \\mathbf{C}_1^{1/2}\\right)^{1/2}\\right).\n$$\nSubstitute $\\boldsymbol{m}_1 = \\boldsymbol{\\mu}_{\\text{true}}$, $\\boldsymbol{m}_2 = \\boldsymbol{\\mu}_{\\text{true}} + \\boldsymbol{b}$, $\\mathbf{C}_1 = \\sigma^2 \\mathbf{I}_d$, and $\\mathbf{C}_2 = \\tau^2 \\mathbf{I}_d$. The mean term becomes\n$$\n\\lVert \\boldsymbol{m}_1 - \\boldsymbol{m}_2 \\rVert_2^2 = \\lVert \\boldsymbol{b} \\rVert_2^2.\n$$\nFor the covariance term, note that $\\mathbf{C}_1^{1/2} = \\sigma \\mathbf{I}_d$ and $\\mathbf{C}_1^{1/2} \\mathbf{C}_2 \\mathbf{C}_1^{1/2} = \\sigma \\mathbf{I}_d \\cdot \\tau^2 \\mathbf{I}_d \\cdot \\sigma \\mathbf{I}_d = \\sigma^2 \\tau^2 \\mathbf{I}_d$. Therefore,\n$$\n\\left(\\mathbf{C}_1^{1/2} \\mathbf{C}_2 \\mathbf{C}_1^{1/2}\\right)^{1/2} = \\sigma \\tau \\mathbf{I}_d.\n$$\nHence,\n$$\n\\mathrm{Tr}\\!\\left(\\mathbf{C}_1 + \\mathbf{C}_2 - 2\\left(\\mathbf{C}_1^{1/2} \\mathbf{C}_2 \\mathbf{C}_1^{1/2}\\right)^{1/2}\\right)\n= \\mathrm{Tr}\\!\\left(\\sigma^2 \\mathbf{I}_d + \\tau^2 \\mathbf{I}_d - 2 \\sigma \\tau \\mathbf{I}_d\\right)\n= d(\\sigma - \\tau)^2.\n$$\nPutting these together yields the simplified FID for the isotropic case:\n$$\n\\mathrm{FID}(\\lVert \\boldsymbol{b} \\rVert_2) = \\lVert \\boldsymbol{b} \\rVert_2^2 + d(\\sigma - \\tau)^2.\n$$\n\nSecond, we derive shot efficiency. Let $\\bar{\\boldsymbol{x}}_{\\text{real}}$ be the sample mean of $k$ independent and identically distributed draws from $\\mathcal{N}(\\boldsymbol{\\mu}_{\\text{true}}, \\sigma^2 \\mathbf{I}_d)$. By the properties of the sample mean,\n$$\n\\mathbb{E}[\\bar{\\boldsymbol{x}}_{\\text{real}}] = \\boldsymbol{\\mu}_{\\text{true}}, \\quad \\mathrm{Cov}(\\bar{\\boldsymbol{x}}_{\\text{real}}) = \\frac{\\sigma^2}{k} \\mathbf{I}_d.\n$$\nTherefore,\n$$\n\\mathrm{MSE}_{\\text{base}} = \\mathbb{E}\\left[\\lVert \\bar{\\boldsymbol{x}}_{\\text{real}} - \\boldsymbol{\\mu}_{\\text{true}} \\rVert_2^2\\right] = \\mathrm{Tr}\\!\\left(\\mathrm{Cov}(\\bar{\\boldsymbol{x}}_{\\text{real}})\\right) = d \\cdot \\frac{\\sigma^2}{k}.\n$$\n\nSimilarly, let $\\bar{\\boldsymbol{x}}_{\\text{syn}}$ be the sample mean of $n_{\\text{syn}}$ independent and identically distributed draws from $\\mathcal{N}(\\boldsymbol{\\mu}_{\\text{true}} + \\boldsymbol{b}, \\tau^2 \\mathbf{I}_d)$. Then\n$$\n\\mathbb{E}[\\bar{\\boldsymbol{x}}_{\\text{syn}}] = \\boldsymbol{\\mu}_{\\text{true}} + \\boldsymbol{b}, \\quad \\mathrm{Cov}(\\bar{\\boldsymbol{x}}_{\\text{syn}}) = \\frac{\\tau^2}{n_{\\text{syn}}} \\mathbf{I}_d.\n$$\nConsider the linear estimator\n$$\n\\hat{\\boldsymbol{\\mu}}(a) = a \\, \\bar{\\boldsymbol{x}}_{\\text{real}} + (1 - a)\\,\\bar{\\boldsymbol{x}}_{\\text{syn}}.\n$$\nAssuming independence between $\\bar{\\boldsymbol{x}}_{\\text{real}}$ and $\\bar{\\boldsymbol{x}}_{\\text{syn}}$, the mean-squared error (MSE) is\n$$\n\\mathbb{E}\\left[\\lVert \\hat{\\boldsymbol{\\mu}}(a) - \\boldsymbol{\\mu}_{\\text{true}} \\rVert_2^2\\right]\n= a^2 \\, \\mathbb{E}\\left[\\lVert \\bar{\\boldsymbol{x}}_{\\text{real}} - \\boldsymbol{\\mu}_{\\text{true}} \\rVert_2^2\\right]\n+ (1 - a)^2 \\, \\mathbb{E}\\left[\\lVert \\bar{\\boldsymbol{x}}_{\\text{syn}} - \\boldsymbol{\\mu}_{\\text{true}} \\rVert_2^2\\right],\n$$\nbecause the cross-term has zero expectation due to independence and zero-mean centeredness of the errors from $\\bar{\\boldsymbol{x}}_{\\text{real}}$ and the centered deviations of $\\bar{\\boldsymbol{x}}_{\\text{syn}}$ around its own mean. We now compute each term:\n$$\n\\mathbb{E}\\left[\\lVert \\bar{\\boldsymbol{x}}_{\\text{real}} - \\boldsymbol{\\mu}_{\\text{true}} \\rVert_2^2\\right] = d \\cdot \\frac{\\sigma^2}{k} \\equiv A,\n$$\nand\n$$\n\\mathbb{E}\\left[\\lVert \\bar{\\boldsymbol{x}}_{\\text{syn}} - \\boldsymbol{\\mu}_{\\text{true}} \\rVert_2^2\\right] = \\mathbb{E}\\left[\\lVert (\\bar{\\boldsymbol{x}}_{\\text{syn}} - \\mathbb{E}[\\bar{\\boldsymbol{x}}_{\\text{syn}}]) + (\\mathbb{E}[\\bar{\\boldsymbol{x}}_{\\text{syn}}] - \\boldsymbol{\\mu}_{\\text{true}}) \\rVert_2^2\\right]\n= \\mathrm{Tr}\\!\\left(\\mathrm{Cov}(\\bar{\\boldsymbol{x}}_{\\text{syn}})\\right) + \\lVert \\boldsymbol{b} \\rVert_2^2\n= d \\cdot \\frac{\\tau^2}{n_{\\text{syn}}} + \\lVert \\boldsymbol{b} \\rVert_2^2 \\equiv S.\n$$\nTherefore, the MSE as a function of $a$ is\n$$\n\\mathrm{MSE}(a) = a^2 A + (1 - a)^2 S.\n$$\nThis is a convex quadratic in $a$. Differentiating and setting the derivative to zero yields\n$$\n\\frac{d}{da} \\mathrm{MSE}(a) = 2 a A - 2(1 - a) S = 0 \\;\\;\\Rightarrow\\;\\; a^\\star = \\frac{S}{A + S}.\n$$\nSubstituting $a^\\star$ back into the quadratic (or by completing the square) gives the minimal MSE\n$$\n\\mathrm{MSE}_{\\text{opt}} = \\frac{A S}{A + S}.\n$$\nHence, the shot efficiency is\n$$\n\\mathrm{SE} = \\frac{\\mathrm{MSE}_{\\text{base}}}{\\mathrm{MSE}_{\\text{opt}}} = \\frac{A}{\\frac{A S}{A + S}} = \\frac{A + S}{S} = 1 + \\frac{A}{S},\n$$\nwhere\n$$\nA = d \\cdot \\frac{\\sigma^2}{k}, \\qquad S = d \\cdot \\frac{\\tau^2}{n_{\\text{syn}}} + \\lVert \\boldsymbol{b} \\rVert_2^2.\n$$\n\nThird, to study correlation across bias magnitudes, for each test case we evaluate the list $\\{\\lVert \\boldsymbol{b} \\rVert_2\\} \\in \\mathcal{B}$, compute\n$$\n\\mathrm{FID}(\\lVert \\boldsymbol{b} \\rVert_2) = \\lVert \\boldsymbol{b} \\rVert_2^2 + d(\\sigma - \\tau)^2,\n$$\nand\n$$\n\\mathrm{SE}(\\lVert \\boldsymbol{b} \\rVert_2) = 1 + \\frac{d \\cdot \\sigma^2 / k}{d \\cdot \\tau^2 / n_{\\text{syn}} + \\lVert \\boldsymbol{b} \\rVert_2^2}.\n$$\nWe then compute the Pearson correlation coefficient between the sequences $\\{\\mathrm{FID}(\\lVert \\boldsymbol{b} \\rVert_2)\\}$ and $\\{\\mathrm{SE}(\\lVert \\boldsymbol{b} \\rVert_2)\\}$ for each test case. Intuitively, as $\\lVert \\boldsymbol{b} \\rVert_2$ increases, the FID increases linearly in $\\lVert \\boldsymbol{b} \\rVert_2^2$ while the shot efficiency decreases monotonically because synthetic data becomes less informative due to increased bias; hence we expect a negative correlation, often strongly negative.\n\nFinally, the program implements these formulas exactly for the specified test suite:\n- Case $1$: $(8, 5, 100, 1.0, 1.0, \\{0.0, 0.25, 0.5, 1.0, 2.0\\})$.\n- Case $2$: $(16, 5, 20, 1.0, 2.0, \\{0.0, 0.5, 1.0, 1.5, 2.5\\})$.\n- Case $3$: $(8, 1, 50, 1.0, 0.5, \\{0.0, 0.2, 0.7, 1.2, 2.0\\})$.\n- Case $4$: $(32, 100, 1000, 1.0, 1.5, \\{0.0, 0.1, 0.3, 0.8, 1.5\\})$.\n- Case $5$: $(4, 2, 1, 1.0, 3.0, \\{0.0, 0.5, 1.0, 3.0, 5.0\\})$.\n\nFor each, it computes the PCC between FID and SE across $\\mathcal{B}$ and prints the list $[r_1,r_2,r_3,r_4,r_5]$, with each $r_i$ rounded to $6$ decimals, as a single line.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fid_isotropic(d, sigma, tau, b_mag):\n    # FID between N(mu_true, sigma^2 I) and N(mu_true + b, tau^2 I)\n    # with isotropic covariances and mean offset of magnitude b_mag.\n    return (b_mag ** 2) + d * (sigma - tau) ** 2\n\ndef shot_efficiency(d, k, n_syn, sigma, tau, b_mag):\n    # A = baseline MSE using k real samples\n    A = d * (sigma ** 2) / k\n    # S = MSE component for synthetic sample mean including bias squared\n    S = d * (tau ** 2) / n_syn + (b_mag ** 2)\n    # SE = (A + S) / S = 1 + A/S\n    return 1.0 + A / S\n\ndef pearson_corr(x, y):\n    # Compute Pearson correlation coefficient between two lists/arrays.\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    # Handle constant sequences to avoid division by zero; define corr as 0 in that degenerate case.\n    if np.allclose(x, x[0]) or np.allclose(y, y[0]):\n        return 0.0\n    cmat = np.corrcoef(x, y)\n    return float(cmat[0, 1])\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (d, k, n_syn, sigma, tau, list_of_b_magnitudes)\n    test_cases = [\n        (8, 5, 100, 1.0, 1.0, [0.0, 0.25, 0.5, 1.0, 2.0]),\n        (16, 5, 20, 1.0, 2.0, [0.0, 0.5, 1.0, 1.5, 2.5]),\n        (8, 1, 50, 1.0, 0.5, [0.0, 0.2, 0.7, 1.2, 2.0]),\n        (32, 100, 1000, 1.0, 1.5, [0.0, 0.1, 0.3, 0.8, 1.5]),\n        (4, 2, 1, 1.0, 3.0, [0.0, 0.5, 1.0, 3.0, 5.0]),\n    ]\n\n    results = []\n    for d, k, n_syn, sigma, tau, b_list in test_cases:\n        fids = [fid_isotropic(d, sigma, tau, b) for b in b_list]\n        ses = [shot_efficiency(d, k, n_syn, sigma, tau, b) for b in b_list]\n        r = pearson_corr(fids, ses)\n        # Round to 6 decimals as specified\n        results.append(f\"{r:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3125790"}]}