{"hands_on_practices": [{"introduction": "A central challenge in heatmap-based pose estimation is extracting precise, sub-pixel keypoint coordinates in a fully differentiable manner for end-to-end training. This exercise guides you through the design of a 'soft-argmax' layer, which uses a temperature-controlled softmax function to compute the expected coordinate over the heatmap. By deriving its gradient and analyzing key metrics, you will understand the critical trade-off between localization sharpness and the gradient flow necessary for learning [@problem_id:3139976].", "problem": "You are asked to design and analyze a differentiable non-maximum suppression layer for keypoint detection in pose estimation by replacing hard argmax selection over a heatmap with a smooth soft selection. Begin from the following fundamental bases: the definition of the exponential family normalization known as the softmax, the probabilistic interpretation of weights summing to one, the expected value of a discrete random variable, and the chain rule from calculus.\n\nSuppose you are given a discrete heatmap on a regular grid, represented as a matrix $H \\in \\mathbb{R}^{m \\times n}$ with entries $H_{y,x}$ for row index $y \\in \\{0,\\dots,m-1\\}$ and column index $x \\in \\{0,\\dots,n-1\\}$. Flatten the grid into a list of $N = m \\cdot n$ locations indexed by $i \\in \\{0,\\dots,N-1\\}$, each associated with a pixel coordinate $\\mathbf{p}_i = (x_i, y_i)$ and a heatmap value $H_i$. Define a differentiable soft selection via the softmax weights with temperature $\\tau > 0$:\n$$\ns_i = \\frac{\\exp\\left(\\frac{H_i}{\\tau}\\right)}{\\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right)} \\quad \\text{for } i \\in \\{0,\\dots,N-1\\}.\n$$\nInterpret $\\{s_i\\}$ as a discrete probability distribution over pixel locations. Use these weights to define a differentiable keypoint as the expected coordinate (soft-argmax):\n$$\n\\hat{\\mathbf{p}} = \\sum_{i=0}^{N-1} s_i \\, \\mathbf{p}_i.\n$$\nFrom first principles, derive the gradient of $\\hat{\\mathbf{p}}$ with respect to the heatmap values $\\{H_i\\}$ by applying the chain rule to the softmax definition. Using the derived expression, define a differentiability sensitivity measure as the Frobenius norm of the Jacobian of $\\hat{\\mathbf{p}}$ with respect to $H$, namely:\n$$\nG = \\left\\|\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H}\\right\\|_F = \\sqrt{\\sum_{i=0}^{N-1} \\left\\|\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_i}\\right\\|_2^2}.\n$$\nTo evaluate peak sharpness, define two complementary scalar measures of the distribution $\\{s_i\\}$ relative to $\\hat{\\mathbf{p}}$:\n- The spatial variance around $\\hat{\\mathbf{p}}$:\n$$\nV = \\sum_{i=0}^{N-1} s_i \\, \\left\\|\\mathbf{p}_i - \\hat{\\mathbf{p}}\\right\\|_2^2.\n$$\n- The normalized Shannon entropy:\n$$\nE = -\\frac{1}{\\log N} \\sum_{i=0}^{N-1} s_i \\log s_i,\n$$\nwhich lies in $[0,1]$ where values near $0$ indicate a sharp peak and values near $1$ indicate a diffuse distribution.\n\nYour program must implement the above layer and compute the following outputs for a given $(H,\\tau)$ pair: the soft-argmax coordinate $(\\hat{x}, \\hat{y})$, the spatial variance $V$, the normalized entropy $E$, and the differentiability sensitivity $G$. All computations are unitless. Angles are not involved in this task.\n\nDerivation requirements:\n- Start from the softmax definition, check that $\\sum_i s_i = 1$, and use the expected value definition to define $\\hat{\\mathbf{p}}$.\n- Derive $\\frac{\\partial s_i}{\\partial H_k}$ and then $\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k}$ without invoking pre-known shortcuts beyond the chain rule and the properties of the exponential function.\n- Simplify the expression for $G$ to a form that is directly computable from $\\{s_i\\}$, $\\{\\mathbf{p}_i\\}$, and $\\hat{\\mathbf{p}}$.\n\nImplementation requirements:\n- Use the grid coordinates $\\mathbf{p}_i = (x_i, y_i)$ with $x_i \\in \\{0,\\dots,n-1\\}$ and $y_i \\in \\{0,\\dots,m-1\\}$ mapped by the usual row-major flattening.\n- Implement numerically stable computations; for entropy, ensure the logarithm is evaluated only for strictly positive arguments.\n\nTest suite:\n- All heatmaps are of size $m = 5$, $n = 5$ (that is, $5 \\times 5$), flattened in row-major order. Indexing is $0$-based.\n- Case $1$ (happy path, very sharp single peak): $H$ is all zeros except $H_{2,2} = 10$, with $\\tau = 0.1$.\n- Case $2$ (moderate temperature): same $H$ as Case $1$, with $\\tau = 1.0$.\n- Case $3$ (two symmetric peaks): $H$ is all zeros except $H_{1,1} = 8$ and $H_{3,3} = 8$, with $\\tau = 0.5$.\n- Case $4$ (uniform heatmap, boundary diffuse case): $H$ is all ones, with $\\tau = 2.0$.\n\nFor each case, compute the tuple $(\\hat{x}, \\hat{y}, V, E, G)$ and aggregate all results into a single flat list in the following order: Case $1$ results, followed by Case $2$, Case $3$, and Case $4$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result1},\\text{result2},\\dots]$). All results must be floats. No units are required.", "solution": "The problem is valid as it is mathematically well-posed, scientifically grounded in the principles of deep learning and calculus, and provides a complete and consistent set of definitions and constraints. We will proceed with the derivation and implementation.\n\nOur objective is to design a differentiable soft keypoint selection layer. This involves deriving the gradient of the soft-argmax coordinate with respect to the input heatmap and defining several metrics to analyze the behavior of this layer.\n\n**1. Foundational Definitions**\n\nWe are given a heatmap $H \\in \\mathbb{R}^{m \\times n}$. We flatten this into a vector of $N = m \\cdot n$ values $\\{H_i\\}$, where each index $i$ corresponds to a unique pixel coordinate $\\mathbf{p}_i = (x_i, y_i)$.\n\nThe core of the differentiable selection is the softmax function, which converts the heatmap values into a probability distribution $\\{s_i\\}$ over the pixel locations. With a temperature parameter $\\tau > 0$, the softmax weight for the $i$-th location is:\n$$\ns_i = \\frac{\\exp\\left(\\frac{H_i}{\\tau}\\right)}{\\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right)}\n$$\nThese weights constitute a valid probability distribution because $s_i \\ge 0$ for all $i$ and their sum is unity:\n$$\n\\sum_{i=0}^{N-1} s_i = \\sum_{i=0}^{N-1} \\frac{\\exp\\left(\\frac{H_i}{\\tau}\\right)}{\\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right)} = \\frac{1}{\\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right)} \\sum_{i=0}^{N-1} \\exp\\left(\\frac{H_i}{\\tau}\\right) = 1\n$$\nUsing this probabilistic interpretation, the soft-argmax keypoint coordinate $\\hat{\\mathbf{p}}$ is defined as the expected value of the pixel coordinates $\\mathbf{p}_i$ under the distribution $\\{s_i\\}$:\n$$\n\\hat{\\mathbf{p}} = \\sum_{i=0}^{N-1} s_i \\, \\mathbf{p}_i\n$$\nwhere $\\hat{\\mathbf{p}}$ is a $2$-dimensional vector $(\\hat{x}, \\hat{y})$.\n\n**2. Derivation of the Gradient**\n\nTo ensure this layer can be used in a gradient-based optimization framework (like training a neural network), we must find the derivative of the output $\\hat{\\mathbf{p}}$ with respect to the input heatmap values $\\{H_k\\}$. We apply the chain rule. The derivative of $\\hat{\\mathbf{p}}$ with respect to a single heatmap entry $H_k$ is:\n$$\n\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k} = \\frac{\\partial}{\\partial H_k} \\left( \\sum_{i=0}^{N-1} s_i \\mathbf{p}_i \\right) = \\sum_{i=0}^{N-1} \\left( \\frac{\\partial s_i}{\\partial H_k} \\right) \\mathbf{p}_i\n$$\nThe central task is to compute the derivative of the softmax function, $\\frac{\\partial s_i}{\\partial H_k}$. Let $Z = \\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right)$. Then $s_i = Z^{-1} \\exp\\left(\\frac{H_i}{\\tau}\\right)$. Using the quotient rule (or product rule with $Z^{-1}$):\n$$\n\\frac{\\partial s_i}{\\partial H_k} = \\frac{\\partial}{\\partial H_k} \\left( \\frac{\\exp(H_i/\\tau)}{Z} \\right) = \\frac{ \\left( \\frac{\\partial}{\\partial H_k} \\exp\\left(\\frac{H_i}{\\tau}\\right) \\right) Z - \\exp\\left(\\frac{H_i}{\\tau}\\right) \\left( \\frac{\\partial Z}{\\partial H_k} \\right) }{Z^2}\n$$\nThe derivatives of the components are:\n$$\n\\frac{\\partial}{\\partial H_k} \\exp\\left(\\frac{H_i}{\\tau}\\right) = \\exp\\left(\\frac{H_i}{\\tau}\\right) \\cdot \\frac{\\partial}{\\partial H_k}\\left(\\frac{H_i}{\\tau}\\right) = \\exp\\left(\\frac{H_i}{\\tau}\\right) \\frac{1}{\\tau} \\delta_{ik}\n$$\nwhere $\\delta_{ik}$ is the Kronecker delta.\n$$\n\\frac{\\partial Z}{\\partial H_k} = \\frac{\\partial}{\\partial H_k} \\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right) = \\sum_{j=0}^{N-1} \\frac{\\partial}{\\partial H_k} \\exp\\left(\\frac{H_j}{\\tau}\\right) = \\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right) \\frac{1}{\\tau} \\delta_{jk} = \\frac{1}{\\tau} \\exp\\left(\\frac{H_k}{\\tau}\\right)\n$$\nSubstituting these back into the quotient rule expression:\n$$\n\\frac{\\partial s_i}{\\partial H_k} = \\frac{ \\left( \\frac{1}{\\tau} \\delta_{ik} \\exp\\left(\\frac{H_i}{\\tau}\\right) \\right) Z - \\exp\\left(\\frac{H_i}{\\tau}\\right) \\left( \\frac{1}{\\tau} \\exp\\left(\\frac{H_k}{\\tau}\\right) \\right) }{Z^2}\n$$\n$$\n= \\frac{1}{\\tau} \\left( \\delta_{ik} \\frac{\\exp(H_i/\\tau)}{Z} - \\frac{\\exp(H_i/\\tau)}{Z} \\frac{\\exp(H_k/\\tau)}{Z} \\right)\n$$\nBy recognizing the definitions of $s_i$ and $s_k$, this simplifies to the well-known Jacobian of the softmax:\n$$\n\\frac{\\partial s_i}{\\partial H_k} = \\frac{1}{\\tau} (s_i \\delta_{ik} - s_i s_k)\n$$\nNow, we substitute this into the expression for $\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k}$:\n$$\n\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k} = \\sum_{i=0}^{N-1} \\frac{1}{\\tau} (s_i \\delta_{ik} - s_i s_k) \\mathbf{p}_i = \\frac{1}{\\tau} \\left( \\sum_{i=0}^{N-1} s_i \\delta_{ik} \\mathbf{p}_i - \\sum_{i=0}^{N-1} s_i s_k \\mathbf{p}_i \\right)\n$$\nThe first summation collapses to a single term where $i=k$: $s_k \\mathbf{p}_k$. For the second summation, $s_k$ is a constant factor: $s_k \\sum_{i=0}^{N-1} s_i \\mathbf{p}_i = s_k \\hat{\\mathbf{p}}$.\nThis yields the final simplified expression for the gradient:\n$$\n\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k} = \\frac{1}{\\tau} (s_k \\mathbf{p}_k - s_k \\hat{\\mathbf{p}}) = \\frac{s_k}{\\tau} (\\mathbf{p}_k - \\hat{\\mathbf{p}})\n$$\n\n**3. Derivation of the Differentiability Sensitivity Measure**\n\nThe sensitivity measure $G$ is the Frobenius norm of the Jacobian matrix $\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H}$, whose columns are the vectors $\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k}$.\n$$\nG = \\left\\|\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H}\\right\\|_F = \\sqrt{\\sum_{k=0}^{N-1} \\left\\|\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k}\\right\\|_2^2}\n$$\nUsing our derived gradient:\n$$\n\\left\\|\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k}\\right\\|_2^2 = \\left\\| \\frac{s_k}{\\tau} (\\mathbf{p}_k - \\hat{\\mathbf{p}}) \\right\\|_2^2 = \\left(\\frac{s_k}{\\tau}\\right)^2 \\left\\| \\mathbf{p}_k - \\hat{\\mathbf{p}} \\right\\|_2^2\n$$\nSumming over all $k$:\n$$\nG^2 = \\sum_{k=0}^{N-1} \\frac{s_k^2}{\\tau^2} \\left\\| \\mathbf{p}_k - \\hat{\\mathbf{p}} \\right\\|_2^2 = \\frac{1}{\\tau^2} \\sum_{k=0}^{N-1} s_k^2 \\left\\| \\mathbf{p}_k - \\hat{\\mathbf{p}} \\right\\|_2^2\n$$\nThus, $G$ can be computed as:\n$$\nG = \\frac{1}{\\tau} \\sqrt{\\sum_{i=0}^{N-1} s_i^2 \\left\\|\\mathbf{p}_i - \\hat{\\mathbf{p}}\\right\\|_2^2}\n$$\nThis expression is directly computable from the softmax weights $\\{s_i\\}$, the pixel coordinates $\\{\\mathbf{p}_i\\}$, the soft-argmax coordinate $\\hat{\\mathbf{p}}$, and the temperature $\\tau$.\n\n**4. Metrics for Peak Sharpness**\n\nThe other two metrics are defined in the problem statement and do not require further derivation.\n- The spatial variance $V$ measures the expected squared distance from the mean coordinate $\\hat{\\mathbf{p}}$:\n$$\nV = \\sum_{i=0}^{N-1} s_i \\, \\left\\|\\mathbf{p}_i - \\hat{\\mathbf{p}}\\right\\|_2^2\n$$\n- The normalized Shannon entropy $E$ measures the uncertainty or diffuseness of the distribution $\\{s_i\\}$:\n$$\nE = -\\frac{1}{\\log N} \\sum_{i=0}^{N-1} s_i \\log s_i\n$$\nThe term $s_i \\log s_i$ is taken to be $0$ if $s_i = 0$.\n\n**5. Implementation Plan**\nThe implementation will follow these derived formulas.\n1.  Generate the grid of coordinates $\\mathbf{p}_i = (x_i, y_i)$.\n2.  Given an input heatmap $H$ and temperature $\\tau$, compute the flattened vector of heatmap values $\\{H_i\\}$.\n3.  Compute the softmax weights $\\{s_i\\}$ using the log-sum-exp trick for numerical stability to prevent overflow.\n4.  Calculate the soft-argmax coordinate $\\hat{\\mathbf{p}} = \\sum s_i \\mathbf{p}_i$.\n5.  Calculate the spatial variance $V = \\sum s_i \\|\\mathbf{p}_i - \\hat{\\mathbf{p}}\\|_2^2$.\n6.  Calculate the normalized entropy $E$, ensuring to handle the case where $s_i=0$.\n7.  Calculate the sensitivity $G = \\frac{1}{\\tau} \\sqrt{\\sum s_i^2 \\|\\mathbf{p}_i - \\hat{\\mathbf{p}}\\|_2^2}$.\n8.  Return the computed tuple $(\\hat{x}, \\hat{y}, V, E, G)$ for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_metrics(H: np.ndarray, tau: float) -> tuple[float, float, float, float, float]:\n    \"\"\"\n    Computes soft-argmax coordinates and related metrics for a given heatmap.\n\n    Args:\n        H (np.ndarray): A 2D numpy array representing the heatmap.\n        tau (float): The temperature parameter for the softmax function.\n\n    Returns:\n        tuple[float, float, float, float, float]: A tuple containing\n        (x_hat, y_hat, V, E, G).\n    \"\"\"\n    m, n = H.shape\n    N = float(m * n)\n\n    # 1. Generate grid coordinates p_i = (x_i, y_i)\n    # np.meshgrid creates coordinate matrices from coordinate vectors.\n    # The 'ij' indexing gives row-major compatible grids.\n    x_coords, y_coords = np.meshgrid(np.arange(n), np.arange(m), indexing='xy')\n    \n    # Flatten coordinates into an (N, 2) array of (x, y) pairs\n    # This corresponds to row-major flattening of the heatmap.\n    p = np.stack([x_coords.ravel(), y_coords.ravel()], axis=1)\n\n    # Flatten the heatmap H\n    H_flat = H.ravel()\n\n    # 2. Compute softmax weights s_i with numerical stability (log-sum-exp trick)\n    a = H_flat / tau\n    # Subtracting the max value prevents overflow in exp\n    a_max = np.max(a)\n    exp_a = np.exp(a - a_max)\n    sum_exp_a = np.sum(exp_a)\n    s = exp_a / sum_exp_a\n\n    # 3. Compute soft-argmax p_hat = (x_hat, y_hat)\n    # p_hat is the expected coordinate E[p] under distribution s.\n    # s is (N,), p is (N, 2). s[:, np.newaxis] makes s (N, 1) for broadcasting.\n    p_hat = np.sum(s[:, np.newaxis] * p, axis=0)\n    x_hat, y_hat = p_hat[0], p_hat[1]\n\n    # 4. Compute spatial variance V\n    # diff_p is (N, 2) array of vectors (p_i - p_hat)\n    diff_p = p - p_hat\n    # sq_dist is (N,) array of squared Euclidean distances ||p_i - p_hat||^2\n    sq_dist = np.sum(diff_p**2, axis=1)\n    V = np.sum(s * sq_dist)\n\n    # 5. Compute normalized Shannon entropy E\n    # To avoid log(0), we select only s_i > 0.\n    # The limit of s*log(s) as s->0 is 0.\n    s_positive = s[s > 0]\n    # The entropy is sum(-s_i * log(s_i))\n    entropy = -np.sum(s_positive * np.log(s_positive))\n    E = entropy / np.log(N)\n\n    # 6. Compute differentiability sensitivity G\n    # The formular for G^2 is (1/tau^2) * sum(s_i^2 * ||p_i - p_hat||^2)\n    g_sum_term = np.sum(s**2 * sq_dist)\n    G = (1.0 / tau) * np.sqrt(g_sum_term)\n    \n    return (x_hat, y_hat, V, E, G)\n\ndef solve():\n    \"\"\"\n    Defines test cases, computes metrics for each, and prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    m, n = 5, 5\n\n    # Case 1: Sharp single peak\n    H1 = np.zeros((m, n), dtype=float)\n    H1[2, 2] = 10.0\n    tau1 = 0.1\n\n    # Case 2: Moderate temperature single peak\n    H2 = np.zeros((m, n), dtype=float)\n    H2[2, 2] = 10.0\n    tau2 = 1.0\n\n    # Case 3: Two symmetric peaks\n    H3 = np.zeros((m, n), dtype=float)\n    H3[1, 1] = 8.0\n    H3[3, 3] = 8.0\n    tau3 = 0.5\n\n    # Case 4: Uniform heatmap\n    H4 = np.ones((m, n), dtype=float)\n    tau4 = 2.0\n\n    test_cases = [\n        (H1, tau1),\n        (H2, tau2),\n        (H3, tau3),\n        (H4, tau4),\n    ]\n\n    all_results = []\n    for H, tau in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        results = compute_metrics(H, tau)\n        all_results.extend(results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3139976"}, {"introduction": "Once a model can output coordinates, we must train it to produce high-quality heatmaps. This practice delves into the crucial choice of a loss function by comparing the standard Mean Squared Error ($L_{\\text{MSE}}$) with the more advanced Focal Loss, which is better suited to the sparse nature of heatmap targets. Through a first-principles derivation, you will analyze how their gradients behave near the target location, revealing why Focal Loss can accelerate learning for well-localized peaks [@problem_id:3140025].", "problem": "A single ground-truth keypoint in a human pose heatmap is represented by a per-pixel ground-truth heat value function $H(\\mathbf{x})$ that equals $1$ at the true peak location $\\mathbf{x}^{\\ast}$ and decays away from it. A deep network produces a pre-activation (logit) $z(\\mathbf{x}) \\in \\mathbb{R}$ and a heatmap prediction $\\hat{H}(\\mathbf{x}) \\in (0,1)$ via the sigmoid $\\sigma$, so that $\\hat{H}(\\mathbf{x}) = \\sigma(z(\\mathbf{x}))$, where $\\sigma(u) = \\frac{1}{1 + \\exp(-u)}$. Consider the per-pixel losses at the peak location $\\mathbf{x}^{\\ast}$ with $H(\\mathbf{x}^{\\ast}) = 1$:\n- The mean squared error (MSE) loss $L_{\\text{MSE}} = \\big(H(\\mathbf{x}^{\\ast}) - \\hat{H}(\\mathbf{x}^{\\ast})\\big)^{2}$.\n- The focal heatmap loss $L_{\\text{focal}} = -\\big(1 - \\hat{H}(\\mathbf{x}^{\\ast})\\big)^{\\gamma} \\ln\\big(\\hat{H}(\\mathbf{x}^{\\ast})\\big)$, with focusing parameter $\\gamma > 0$.\n\nAssume the network prediction at the peak is very close to the target, and define the small error parameter $\\varepsilon = 1 - \\hat{H}(\\mathbf{x}^{\\ast})$ with $0 < \\varepsilon \\ll 1$. Let $R(\\varepsilon,\\gamma)$ denote the ratio of the magnitudes of the gradients of the two losses with respect to the logit $z(\\mathbf{x}^{\\ast})$:\n$$\nR(\\varepsilon,\\gamma) \\equiv \\frac{\\left|\\frac{\\partial L_{\\text{MSE}}}{\\partial z(\\mathbf{x}^{\\ast})}\\right|}{\\left|\\frac{\\partial L_{\\text{focal}}}{\\partial z(\\mathbf{x}^{\\ast})}\\right|}.\n$$\nStarting from core definitions (the sigmoid function and the chain rule for differentiation), derive the leading-order asymptotic expression for $R(\\varepsilon,\\gamma)$ as $\\varepsilon \\to 0^{+}$, simplified into a single closed-form expression in terms of $\\varepsilon$ and $\\gamma$. Your final answer must be this expression. No rounding is required, and no units are involved.", "solution": "The user has asked for a derivation of the leading-order asymptotic expression for the ratio of the magnitudes of the gradients of two loss functions, $L_{\\text{MSE}}$ and $L_{\\text{focal}}$, with respect to the network logit $z(\\mathbf{x}^{\\ast})$.\n\nLet's begin by defining the quantities involved. For notational simplicity, we will evaluate all functions at the peak location $\\mathbf{x}^{\\ast}$ and drop the explicit dependence on $\\mathbf{x}^{\\ast}$. Let $H \\equiv H(\\mathbf{x}^{\\ast})$, $\\hat{H} \\equiv \\hat{H}(\\mathbf{x}^{\\ast})$, and $z \\equiv z(\\mathbf{x}^{\\ast})$.\nThe problem provides the following information:\n- The ground-truth value at the peak is $H = 1$.\n- The heatmap prediction is $\\hat{H} = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$.\n- The mean squared error loss is $L_{\\text{MSE}} = (H - \\hat{H})^{2} = (1 - \\hat{H})^{2}$.\n- The focal heatmap loss is $L_{\\text{focal}} = -(1 - \\hat{H})^{\\gamma} \\ln(\\hat{H})$, with $\\gamma > 0$.\n- The small error parameter is $\\varepsilon = 1 - \\hat{H}$, which implies $\\hat{H} = 1 - \\varepsilon$. The analysis is for the limit $\\varepsilon \\to 0^{+}$.\n\nOur goal is to compute the ratio $R(\\varepsilon,\\gamma) \\equiv \\frac{\\left|\\frac{\\partial L_{\\text{MSE}}}{\\partial z}\\right|}{\\left|\\frac{\\partial L_{\\text{focal}}}{\\partial z}\\right|}$.\n\nFirst, we must apply the chain rule to find the derivatives of the losses with respect to the logit $z$. For any loss $L(\\hat{H})$, the derivative is given by:\n$$\n\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial \\hat{H}} \\frac{\\partial \\hat{H}}{\\partial z}\n$$\nWe need the derivative of the sigmoid activation function, $\\hat{H} = \\sigma(z)$, with respect to $z$:\n$$\n\\frac{\\partial \\hat{H}}{\\partial z} = \\frac{d}{dz} \\left( \\frac{1}{1 + \\exp(-z)} \\right) = - \\frac{1}{(1 + \\exp(-z))^2} (-\\exp(-z)) = \\frac{\\exp(-z)}{(1 + \\exp(-z))^2}\n$$\nThis can be expressed compactly in terms of $\\sigma(z)$ itself:\n$$\n\\frac{\\partial \\hat{H}}{\\partial z} = \\frac{1}{1 + \\exp(-z)} \\cdot \\frac{\\exp(-z)}{1 + \\exp(-z)} = \\sigma(z) \\left( \\frac{1 + \\exp(-z) - 1}{1 + \\exp(-z)} \\right) = \\sigma(z) \\left( 1 - \\frac{1}{1 + \\exp(-z)} \\right)\n$$\n$$\n\\frac{\\partial \\hat{H}}{\\partial z} = \\sigma(z) (1 - \\sigma(z)) = \\hat{H}(1 - \\hat{H})\n$$\n\nNow, we compute the gradient for each loss function.\n\n**1. Gradient of the MSE Loss**\n\nFirst, we find the derivative of $L_{\\text{MSE}}$ with respect to $\\hat{H}$:\n$$\nL_{\\text{MSE}} = (1 - \\hat{H})^2\n$$\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial \\hat{H}} = 2(1 - \\hat{H})(-1) = -2(1 - \\hat{H})\n$$\nUsing the chain rule, we find the gradient with respect to $z$:\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial z} = \\frac{\\partial L_{\\text{MSE}}}{\\partial \\hat{H}} \\frac{\\partial \\hat{H}}{\\partial z} = -2(1 - \\hat{H}) \\cdot [\\hat{H}(1 - \\hat{H})] = -2\\hat{H}(1 - \\hat{H})^2\n$$\nIn terms of $\\varepsilon = 1 - \\hat{H}$, this becomes:\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial z} = -2(1 - \\varepsilon)\\varepsilon^2\n$$\nThe magnitude is:\n$$\n\\left|\\frac{\\partial L_{\\text{MSE}}}{\\partial z}\\right| = |-2(1 - \\varepsilon)\\varepsilon^2| = 2(1 - \\varepsilon)\\varepsilon^2\n$$\nAs $\\varepsilon \\to 0^{+}$, the leading-order term of the numerator's magnitude is $2\\varepsilon^2$.\n\n**2. Gradient of the Focal Loss**\n\nNext, we find the derivative of $L_{\\text{focal}}$ with respect to $\\hat{H}$:\n$$\nL_{\\text{focal}} = -(1 - \\hat{H})^{\\gamma} \\ln(\\hat{H})\n$$\nUsing the product rule for differentiation, $(uv)' = u'v + uv'$:\n$$\n\\frac{\\partial L_{\\text{focal}}}{\\partial \\hat{H}} = -\\left[ \\left(\\frac{d}{d\\hat{H}}(1 - \\hat{H})^{\\gamma}\\right)\\ln(\\hat{H}) + (1 - \\hat{H})^{\\gamma}\\left(\\frac{d}{d\\hat{H}}\\ln(\\hat{H})\\right) \\right]\n$$\n$$\n\\frac{\\partial L_{\\text{focal}}}{\\partial \\hat{H}} = -\\left[ \\gamma(1 - \\hat{H})^{\\gamma-1}(-1)\\ln(\\hat{H}) + (1 - \\hat{H})^{\\gamma}\\frac{1}{\\hat{H}} \\right]\n$$\n$$\n\\frac{\\partial L_{\\text{focal}}}{\\partial \\hat{H}} = \\gamma(1 - \\hat{H})^{\\gamma-1}\\ln(\\hat{H}) - \\frac{(1 - \\hat{H})^{\\gamma}}{\\hat{H}}\n$$\nUsing the chain rule, we find the gradient with respect to $z$:\n$$\n\\frac{\\partial L_{\\text{focal}}}{\\partial z} = \\frac{\\partial L_{\\text{focal}}}{\\partial \\hat{H}} \\frac{\\partial \\hat{H}}{\\partial z} = \\left[ \\gamma(1 - \\hat{H})^{\\gamma-1}\\ln(\\hat{H}) - \\frac{(1 - \\hat{H})^{\\gamma}}{\\hat{H}} \\right] \\cdot [\\hat{H}(1 - \\hat{H})]\n$$\nDistributing the $\\hat{H}(1 - \\hat{H})$ term:\n$$\n\\frac{\\partial L_{\\text{focal}}}{\\partial z} = \\gamma \\hat{H}(1 - \\hat{H})^{\\gamma}\\ln(\\hat{H}) - (1 - \\hat{H})^{\\gamma+1}\n$$\nIn terms of $\\varepsilon = 1 - \\hat{H}$:\n$$\n\\frac{\\partial L_{\\text{focal}}}{\\partial z} = \\gamma (1 - \\varepsilon)\\varepsilon^{\\gamma}\\ln(1 - \\varepsilon) - \\varepsilon^{\\gamma+1}\n$$\nFor small $\\varepsilon > 0$, we know $\\ln(1 - \\varepsilon)$ is negative. Both terms are negative, so the entire expression is negative. The magnitude is:\n$$\n\\left|\\frac{\\partial L_{\\text{focal}}}{\\partial z}\\right| = \\left| \\gamma (1 - \\varepsilon)\\varepsilon^{\\gamma}\\ln(1 - \\varepsilon) - \\varepsilon^{\\gamma+1} \\right| = \\varepsilon^{\\gamma+1} - \\gamma(1 - \\varepsilon)\\varepsilon^{\\gamma}\\ln(1 - \\varepsilon)\n$$\n\n**3. Asymptotic Analysis**\n\nTo find the leading-order behavior of this expression as $\\varepsilon \\to 0^{+}$, we use the Taylor series expansion for $\\ln(1-x)$ around $x_0=0$:\n$$\n\\ln(1 - \\varepsilon) = -\\varepsilon - \\frac{\\varepsilon^2}{2} - O(\\varepsilon^3)\n$$\nSubstituting this into the expression for the magnitude of the focal loss gradient:\n$$\n\\left|\\frac{\\partial L_{\\text{focal}}}{\\partial z}\\right| = \\varepsilon^{\\gamma+1} - \\gamma(1 - \\varepsilon)\\varepsilon^{\\gamma}(-\\varepsilon - \\frac{\\varepsilon^2}{2} - \\dots)\n$$\n$$\n= \\varepsilon^{\\gamma+1} + \\gamma(1 - \\varepsilon)\\varepsilon^{\\gamma+1}(1 + \\frac{\\varepsilon}{2} + \\dots)\n$$\n$$\n= \\varepsilon^{\\gamma+1} + \\gamma(\\varepsilon^{\\gamma+1} - \\varepsilon^{\\gamma+2})(1 + \\frac{\\varepsilon}{2} + \\dots)\n$$\nWe only need terms up to the next highest order to confirm the leading term:\n$$\n= \\varepsilon^{\\gamma+1} + \\gamma(\\varepsilon^{\\gamma+1} + \\frac{1}{2}\\varepsilon^{\\gamma+2} - \\varepsilon^{\\gamma+2} - O(\\varepsilon^{\\gamma+3}))\n$$\n$$\n= \\varepsilon^{\\gamma+1} + \\gamma\\varepsilon^{\\gamma+1} - \\frac{\\gamma}{2}\\varepsilon^{\\gamma+2} + O(\\varepsilon^{\\gamma+3})\n$$\n$$\n= (1 + \\gamma)\\varepsilon^{\\gamma+1} - \\frac{\\gamma}{2}\\varepsilon^{\\gamma+2} + O(\\varepsilon^{\\gamma+3})\n$$\nThe leading-order term for the magnitude of the focal loss gradient is $(1 + \\gamma)\\varepsilon^{\\gamma+1}$.\n\n**4. The Ratio**\n\nNow we can construct the ratio $R(\\varepsilon,\\gamma)$ using the leading-order terms for the numerator and denominator.\n$$\nR(\\varepsilon,\\gamma) \\approx \\frac{\\text{leading-order of } \\left|\\frac{\\partial L_{\\text{MSE}}}{\\partial z}\\right|}{\\text{leading-order of } \\left|\\frac{\\partial L_{\\text{focal}}}{\\partial z}\\right|}\n$$\nThe leading-order behavior of the numerator is $\\left|\\frac{\\partial L_{\\text{MSE}}}{\\partial z}\\right| = 2(1-\\varepsilon)\\varepsilon^2 \\approx 2\\varepsilon^2$.\nThe leading-order behavior of the denominator is $\\left|\\frac{\\partial L_{\\text{focal}}}{\\partial z}\\right| \\approx (1+\\gamma)\\varepsilon^{\\gamma+1}$.\n$$\nR(\\varepsilon,\\gamma) \\approx \\frac{2\\varepsilon^2}{(1 + \\gamma)\\varepsilon^{\\gamma+1}}\n$$\nSimplifying the expression by combining the powers of $\\varepsilon$:\n$$\nR(\\varepsilon,\\gamma) \\approx \\frac{2}{1 + \\gamma} \\varepsilon^{2 - (\\gamma+1)} = \\frac{2}{1 + \\gamma} \\varepsilon^{1-\\gamma}\n$$\nThis is the leading-order asymptotic expression for the ratio as $\\varepsilon \\to 0^{+}$.", "answer": "$$\\boxed{\\frac{2}{1+\\gamma}\\varepsilon^{1-\\gamma}}$$", "id": "3140025"}, {"introduction": "A well-trained model must generalize beyond its training data, and a powerful way to achieve this is by teaching it about the world's inherent symmetries. This practice explores how to leverage the left-right anatomical symmetry common in pose estimation tasks to improve prediction consistency and robustness. You will implement a 'flip-and-swap' test-time augmentation strategy and quantify its ability to correct errors by enforcing geometric equivariance [@problem_id:3140014].", "problem": "Consider a keypoint detector that predicts Two-Dimensional (2D) pixel coordinates for a fixed number of keypoints on an image. Let an image be denoted by $I$, and let the detectorâ€™s prediction be $\\hat{K}(I) \\in \\mathbb{R}^{N \\times 2}$, where $N$ is the number of keypoints and each row contains $(x,y)$ pixel coordinates. Let $F$ denote the horizontal flip transform acting on pixel coordinates by\n$$\nF(x,y;w) = (w-1-x, y),\n$$\nwhere $w$ is the image width in pixels and $x,y$ are pixel coordinates. Let $S$ be a permutation on keypoint indices that swaps left and right counterparts, i.e., $S:\\{0,\\dots,N-1\\}\\rightarrow\\{0,\\dots,N-1\\}$ is an involution such that for each left-right pair $(i,j)$, $S(i)=j$ and $S(j)=i$, and for any self-symmetric index $k$, $S(k)=k$. The equivariance condition for horizontal flip with left-right swapping is\n$$\n\\hat{K}(F I) = F \\, S \\, \\hat{K}(I),\n$$\nwhere $F$ acts on coordinates and $S$ acts on indices.\n\nDefine the symmetric error in pixels for a pair of predictions $\\hat{K}(I)$ and $\\hat{K}(F I)$ on the same subject image and its horizontally flipped version as\n$$\nE_{\\text{sym}}(\\hat{K}(I),\\hat{K}(F I); w, S) \\triangleq \\frac{1}{N} \\sum_{i=0}^{N-1} \\left\\| \\hat{K}(F I)_i - \\left(F\\!\\left(\\hat{K}(I)_{S(i)}; w\\right)\\right) \\right\\|_2,\n$$\nwhere $\\|\\cdot\\|_2$ is the Euclidean norm in pixels. Consider a partial enforcement step that updates only the prediction on the original image by projecting it toward the flip-swap consistency:\n$$\nK_{\\text{cons}}(I) \\triangleq \\frac{1}{2}\\left( \\hat{K}(I) + S \\, F^{-1}\\, \\hat{K}(F I) \\right),\n$$\nwith $F^{-1}=F$ for horizontal flips, while leaving $\\hat{K}(F I)$ unchanged. The post-enforcement symmetric error is\n$$\nE_{\\text{sym}}^{\\text{after}} \\triangleq E_{\\text{sym}}\\!\\left(K_{\\text{cons}}(I),\\hat{K}(F I); w, S\\right).\n$$\n\nYour task is to write a complete, runnable program that, for each test case below, computes:\n- the pre-enforcement symmetric error $E_{\\text{sym}}^{\\text{before}} \\triangleq E_{\\text{sym}}\\!\\left(\\hat{K}(I),\\hat{K}(F I); w, S\\right)$, and\n- the post-enforcement symmetric error $E_{\\text{sym}}^{\\text{after}}$ as defined above,\n\nboth expressed in pixels as floating-point values. All angle-related quantities are absent; no angle units are required. The final output must be a single line containing a comma-separated list enclosed in square brackets, with the values for each test case appearing in order as $[E_{\\text{sym}}^{\\text{before}},E_{\\text{sym}}^{\\text{after}}]$ concatenated across test cases. Each number must be rounded to $6$ decimal places.\n\nImplement the following test suite. Each case defines $(w,h,N,S,\\hat{K}(I),\\hat{K}(F I))$ explicitly, where $w$ is the width in pixels, $h$ is the height in pixels, $N$ is the number of keypoints, $S$ is given via its left-right pairs, and the predictions are given as arrays of pixel coordinates.\n\n- Case $1$ (happy path, moderate inconsistency):\n  - $w = 100$, $h = 80$, $N=4$, left-right pairs: $(0,1)$ and $(2,3)$.\n  - $\\hat{K}(I) = \\begin{bmatrix} 20 & 50 \\\\ 80 & 50 \\\\ 25 & 30 \\\\ 75 & 30 \\end{bmatrix}$.\n  - $\\hat{K}(F I) = \\begin{bmatrix} 22 & 52 \\\\ 77 & 49 \\\\ 26 & 31 \\\\ 71 & 29 \\end{bmatrix}$.\n\n- Case $2$ (boundary case, perfect symmetry):\n  - $w = 100$, $h = 80$, $N=4$, left-right pairs: $(0,1)$ and $(2,3)$.\n  - $\\hat{K}(I) = \\begin{bmatrix} 20 & 50 \\\\ 80 & 50 \\\\ 25 & 30 \\\\ 75 & 30 \\end{bmatrix}$.\n  - $\\hat{K}(F I)$ is exactly $F S \\hat{K}(I)$, i.e.,\n    $$\n    \\hat{K}(F I) = \\begin{bmatrix}\n    19 & 50 \\\\\n    79 & 50 \\\\\n    24 & 30 \\\\\n    74 & 30\n    \\end{bmatrix}.\n    $$\n\n- Case $3$ (edge case, indices on symmetry axis):\n  - $w = 101$, $h = 120$, $N=4$, left-right pairs: $(0,1)$ and $(2,3)$.\n  - $\\hat{K}(I) = \\begin{bmatrix} 50 & 10 \\\\ 50 & 12 \\\\ 50 & 70 \\\\ 50 & 68 \\end{bmatrix}$.\n  - $\\hat{K}(F I) = \\begin{bmatrix} 48 & 11 \\\\ 52 & 10 \\\\ 49 & 71 \\\\ 51 & 67 \\end{bmatrix}$.\n\n- Case $4$ (edge case, large asymmetry and wrong swapping):\n  - $w = 64$, $h = 64$, $N=4$, left-right pairs: $(0,1)$ and $(2,3)$.\n  - $\\hat{K}(I) = \\begin{bmatrix} 10 & 20 \\\\ 53 & 19 \\\\ 12 & 40 \\\\ 51 & 41 \\end{bmatrix}$.\n  - $\\hat{K}(F I) = \\begin{bmatrix} 56 & 23 \\\\ 7 & 19 \\\\ 51 & 39 \\\\ 12 & 44 \\end{bmatrix}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as\n$$\n[E_{\\text{sym}}^{\\text{before}}(\\text{Case }1), E_{\\text{sym}}^{\\text{after}}(\\text{Case }1), E_{\\text{sym}}^{\\text{before}}(\\text{Case }2), E_{\\text{sym}}^{\\text{after}}(\\text{Case }2), E_{\\text{sym}}^{\\text{before}}(\\text{Case }3), E_{\\text{sym}}^{\\text{after}}(\\text{Case }3), E_{\\text{sym}}^{\\text{before}}(\\text{Case }4), E_{\\text{sym}}^{\\text{after}}(\\text{Case }4)].\n$$\nAll values must be floats in pixels, rounded to $6$ decimal places. No additional text should be printed.", "solution": "The problem has been validated and found to be scientifically grounded, well-posed, and self-contained. All definitions and data are sufficient for a unique solution.\n\nThe task is to compute two quantities for a series of test cases: the symmetric error before ($E_{\\text{sym}}^{\\text{before}}$) and after ($E_{\\text{sym}}^{\\text{after}}$) a consistency enforcement step for a keypoint detector.\n\nThe fundamental quantities are defined as follows:\n- A keypoint prediction for an image $I$ is a matrix $\\hat{K}(I) \\in \\mathbb{R}^{N \\times 2}$.\n- The horizontal flip transform on a coordinate vector $(x,y)$ is $F(x,y;w) = (w-1-x, y)$, where $w$ is the image width. This transform is its own inverse, $F^{-1}=F$.\n- The permutation $S$ swaps the indices of corresponding left-right keypoints. As it consists of swaps, $S$ is an involution, meaning $S^2$ is the identity permutation.\n- The pre-enforcement symmetric error is given by:\n$$\nE_{\\text{sym}}^{\\text{before}} = E_{\\text{sym}}(\\hat{K}(I),\\hat{K}(F I); w, S) \\triangleq \\frac{1}{N} \\sum_{i=0}^{N-1} \\left\\| \\hat{K}(F I)_i - \\left(F\\!\\left(\\hat{K}(I)_{S(i)}; w\\right)\\right) \\right\\|_2\n$$\nThis measures the deviation from the ideal flip-swap equivariance condition $\\hat{K}(F I) = F \\, S \\, \\hat{K}(I)$, where $F$ acts on coordinates and $S$ acts on the list of keypoint vectors. The term $F(\\hat{K}(I)_{S(i)}; w)$ can be written as the $i$-th row of the matrix $F(S\\hat{K}(I); w)$.\n\n- The consistency enforcement step updates the prediction on the original image:\n$$\nK_{\\text{cons}}(I) \\triangleq \\frac{1}{2}\\left( \\hat{K}(I) + S \\, F^{-1}\\, \\hat{K}(F I) \\right)\n$$\nGiven $F^{-1}=F$, this simplifies to $K_{\\text{cons}}(I) = \\frac{1}{2}\\left( \\hat{K}(I) + S F \\hat{K}(F I) \\right)$.\n\n- The post-enforcement symmetric error is then calculated using the new prediction $K_{\\text{cons}}(I)$ while keeping $\\hat{K}(F I)$ unchanged:\n$$\nE_{\\text{sym}}^{\\text{after}} = E_{\\text{sym}}\\!\\left(K_{\\text{cons}}(I),\\hat{K}(F I); w, S\\right) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\left\\| \\hat{K}(F I)_i - \\left(F\\!\\left((K_{\\text{cons}}(I))_{S(i)}; w\\right)\\right) \\right\\|_2\n$$\n\nA crucial insight can be gained by analyzing the relationship between $E_{\\text{sym}}^{\\text{after}}$ and $E_{\\text{sym}}^{\\text{before}}$. Let us examine the argument of the norm in the expression for $E_{\\text{sym}}^{\\text{after}}$. The term being subtracted from $\\hat{K}(F I)$ is $F S K_{\\text{cons}}(I)$. Let's expand this term:\n$$\nF S K_{\\text{cons}}(I) = F S \\left[ \\frac{1}{2}\\left( \\hat{K}(I) + S F \\hat{K}(F I) \\right) \\right]\n$$\nThe permutation $S$ is a linear operator on the set of $N$ keypoint vectors. The transform $F$ is affine, but it distributes over averaging: $F(\\frac{1}{2}(\\vec{u}+\\vec{v})) = \\frac{1}{2}(F(\\vec{u})+F(\\vec{v}))$. Applying these properties:\n$$\nF S K_{\\text{cons}}(I) = F \\left[ \\frac{1}{2}\\left( S\\hat{K}(I) + S S F \\hat{K}(F I) \\right) \\right]\n$$\nSince $S$ is an involution, $S S$ is the identity operator.\n$$\n= F \\left[ \\frac{1}{2}\\left( S\\hat{K}(I) + F \\hat{K}(F I) \\right) \\right] = \\frac{1}{2} \\left[ F(S\\hat{K}(I)) + F(F(\\hat{K}(F I))) \\right]\n$$\nSince $F$ is its own inverse, $F F$ is the identity operator.\n$$\n= \\frac{1}{2} \\left[ F S \\hat{K}(I) + \\hat{K}(F I) \\right]\n$$\nNow, let's look at the difference vector for the $i$-th keypoint in the post-enforcement error calculation:\n$$\n\\Delta_{i}^{\\text{after}} = \\hat{K}(F I)_i - (F S K_{\\text{cons}}(I))_i = \\hat{K}(F I)_i - \\frac{1}{2} \\left[ (F S \\hat{K}(I))_i + \\hat{K}(F I)_i \\right]\n$$\n$$\n\\Delta_{i}^{\\text{after}} = \\frac{1}{2} \\left[ \\hat{K}(F I)_i - (F S \\hat{K}(I))_i \\right]\n$$\nThis difference vector is precisely half of the original difference vector, $\\Delta_{i}^{\\text{before}} = \\hat{K}(F I)_i - (F S \\hat{K}(I))_i$.\nTherefore, the norm is also halved: $\\|\\Delta_{i}^{\\text{after}}\\|_2 = \\frac{1}{2} \\|\\Delta_{i}^{\\text{before}}\\|_2$. When we average these norms over all $N$ keypoints, the relationship holds for the total error:\n$$\nE_{\\text{sym}}^{\\text{after}} = \\frac{1}{2} E_{\\text{sym}}^{\\text{before}}\n$$\nThis analytical result demonstrates that the chosen update rule projects the inconsistent predictions halfway toward the equivariant manifold. It simplifies the computation, as we only need to calculate $E_{\\text{sym}}^{\\text{before}}$ for each case. However, for rigor, the full computation is implemented.\n\nThe computational procedure for a given test case $(w, N, S, \\hat{K}(I), \\hat{K}(F I))$ is:\n1.  Define the permutation map for $S$ from the given left-right pairs. For pairs $(0,1)$ and $(2,3)$, the map is $[1,0,3,2]$.\n2.  Calculate the ideal transformed prediction, $K_{\\text{target}} = F(S \\hat{K}(I); w)$.\n3.  Compute the per-keypoint error vectors $\\Delta_i = \\hat{K}(F I)_i - (K_{\\text{target}})_i$.\n4.  Calculate $E_{\\text{sym}}^{\\text{before}}$ by taking the mean of the Euclidean norms $\\|\\Delta_i\\|_2$.\n5.  $E_{\\text{sym}}^{\\text{after}}$ is then $\\frac{1}{2}E_{\\text{sym}}^{\\text{before}}$. The implementation confirms this by explicitly calculating $K_{\\text{cons}}(I)$ and then the corresponding error.\n\nApplying this procedure to the test cases yields the following results:\n\n- **Case 1**: $w=100, N=4, S \\leftrightarrow \\{(0,1), (2,3)\\}$.\n  $E_{\\text{sym}}^{\\text{before}} = \\frac{1}{4}(\\sqrt{13} + 2\\sqrt{5} + \\sqrt{10}) \\approx 2.809993$ pixels.\n  $E_{\\text{sym}}^{\\text{after}} = \\frac{1}{2} E_{\\text{sym}}^{\\text{before}} \\approx 1.404996$ pixels.\n\n- **Case 2**: $w=100, N=4, S \\leftrightarrow \\{(0,1), (2,3)\\}$. The predictions are perfectly symmetric, so $K_{target} = \\hat{K}(F I)$.\n  $E_{\\text{sym}}^{\\text{before}} = 0.0$ pixels.\n  $E_{\\text{sym}}^{\\text{after}} = 0.0$ pixels.\n\n- **Case 3**: $w=101, N=4, S \\leftrightarrow \\{(0,1), (2,3)\\}$.\n  $E_{\\text{sym}}^{\\text{before}} = \\frac{1}{4}(2 + \\sqrt{5} + 2\\sqrt{10}) \\approx 2.640156$ pixels.\n  $E_{\\text{sym}}^{\\text{after}} = \\frac{1}{2} E_{\\text{sym}}^{\\text{before}} \\approx 1.320078$ pixels.\n\n- **Case 4**: $w=64, N=4, S \\leftrightarrow \\{(0,1), (2,3)\\}$.\n  $E_{\\text{sym}}^{\\text{before}} = \\frac{1}{4}(\\sqrt{2132} + \\sqrt{2117} + \\sqrt{1525} + \\sqrt{1537}) \\approx 42.610074$ pixels.\n  $E_{\\text{sym}}^{\\text{after}} = \\frac{1}{2} E_{\\text{sym}}^{\\text{before}} \\approx 21.305037$ pixels.\n\nThese values are formatted and presented as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the symmetric error calculation problem for all test cases.\n    \"\"\"\n    # Each case defines (w, h, N, S, K_I, K_FI)\n    # S is given by left-right pairs, which is a fixed (0,1), (2,3) for all cases.\n    # h is not used in calculations.\n    \n    test_cases = [\n        {\n            \"w\": 100, \"N\": 4, \"s_pairs\": [(0, 1), (2, 3)],\n            \"K_I\": np.array([[20., 50.], [80., 50.], [25., 30.], [75., 30.]]),\n            \"K_FI\": np.array([[22., 52.], [77., 49.], [26., 31.], [71., 29.]])\n        },\n        {\n            \"w\": 100, \"N\": 4, \"s_pairs\": [(0, 1), (2, 3)],\n            \"K_I\": np.array([[20., 50.], [80., 50.], [25., 30.], [75., 30.]]),\n            \"K_FI\": np.array([[19., 50.], [79., 50.], [24., 30.], [74., 30.]])\n        },\n        {\n            \"w\": 101, \"N\": 4, \"s_pairs\": [(0, 1), (2, 3)],\n            \"K_I\": np.array([[50., 10.], [50., 12.], [50., 70.], [50., 68.]]),\n            \"K_FI\": np.array([[48., 11.], [52., 10.], [49., 71.], [51., 67.]])\n        },\n        {\n            \"w\": 64, \"N\": 4, \"s_pairs\": [(0, 1), (2, 3)],\n            \"K_I\": np.array([[10., 20.], [53., 19.], [12., 40.], [51., 41.]]),\n            \"K_FI\": np.array([[56., 23.], [7., 19.], [51., 39.], [12., 44.]])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        w = case[\"w\"]\n        N = case[\"N\"]\n        K_I = case[\"K_I\"]\n        K_FI = case[\"K_FI\"]\n\n        # Construct the permutation map from pairs\n        s_map = np.arange(N)\n        for i, j in case[\"s_pairs\"]:\n            s_map[i], s_map[j] = j, i\n\n        # Calculate pre- and post-enforcement errors\n        e_before, e_after = calculate_symmetric_errors(w, N, s_map, K_I, K_FI)\n        \n        results.append(f\"{e_before:.6f}\")\n        results.append(f\"{e_after:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef F_transform(k_matrix, w):\n    \"\"\"\n    Applies the horizontal flip transform F(x,y;w) = (w-1-x, y) to a matrix of keypoints.\n    \n    Args:\n        k_matrix (np.ndarray): An (N, 2) array of (x,y) coordinates.\n        w (int): The image width.\n        \n    Returns:\n        np.ndarray: The transformed (N, 2) array.\n    \"\"\"\n    k_flipped = k_matrix.copy()\n    k_flipped[:, 0] = w - 1 - k_flipped[:, 0]\n    return k_flipped\n\ndef S_permut(k_matrix, s_map):\n    \"\"\"\n    Applies the permutation S to the rows of a keypoint matrix.\n    \n    Args:\n        k_matrix (np.ndarray): An (N, 2) array of keypoints.\n        s_map (np.ndarray): A 1D array representing the permutation of indices.\n        \n    Returns:\n        np.ndarray: The permuted (N, 2) array.\n    \"\"\"\n    return k_matrix[s_map]\n\ndef calculate_symmetric_errors(w, N, s_map, K_I, K_FI):\n    \"\"\"\n    Calculates the pre- and post-enforcement symmetric errors.\n    \n    Args:\n        w (int): Image width.\n        N (int): Number of keypoints.\n        s_map (np.ndarray): Permutation map for S.\n        K_I (np.ndarray): Keypoints from original image.\n        K_FI (np.ndarray): Keypoints from flipped image.\n        \n    Returns:\n        tuple[float, float]: A tuple containing (E_sym_before, E_sym_after).\n    \"\"\"\n    # --- Calculate E_sym_before ---\n    # The ideal prediction for the flipped image, derived from K_I, is F(S(K_I)).\n    target_from_I = F_transform(S_permut(K_I, s_map), w)\n    \n    # Calculate the per-keypoint difference vectors.\n    diff_before = K_FI - target_from_I\n    \n    # Calculate the L2 norm for each difference vector.\n    norms_before = np.linalg.norm(diff_before, axis=1)\n    \n    # The error is the mean of these norms.\n    E_sym_before = np.mean(norms_before)\n    \n    # --- Calculate E_sym_after ---\n    # First, compute the consistency-enforced keypoints K_cons(I).\n    # K_cons(I) = 0.5 * (K_I + S * F^-1 * K_FI), where F^-1 = F.\n    SF_K_FI = S_permut(F_transform(K_FI, w), s_map)\n    K_cons_I = 0.5 * (K_I + SF_K_FI)\n    \n    # The post-enforcement error is E_sym(K_cons_I, K_FI).\n    # The new target is derived from K_cons_I: F(S(K_cons_I)).\n    target_from_cons = F_transform(S_permut(K_cons_I, s_map), w)\n    \n    diff_after = K_FI - target_from_cons\n    norms_after = np.linalg.norm(diff_after, axis=1)\n    E_sym_after = np.mean(norms_after)\n    \n    # As derived in the solution, E_sym_after is analytically half of E_sym_before.\n    # This implementation calculates it explicitly for rigor.\n    \n    return E_sym_before, E_sym_after\n\nsolve()\n```", "id": "3140014"}]}