{"hands_on_practices": [{"introduction": "The genius of EfficientNet lies in its compound scaling rule, but where do the specific scaling coefficients for depth, width, and resolution come from? This first practice invites you to solve this very puzzle. You will step into the role of an architect designing a new family of models by framing the problem as a multi-objective optimization, balancing the competing goals of maximizing accuracy while minimizing latency and memory usage under a fixed computational budget [@problem_id:3119675]. This exercise reveals that the optimal scaling strategy is not universal but depends on the specific constraints of the target hardware.", "problem": "You are tasked with implementing a self-contained program that computes optimal compound scaling coefficients for an EfficientNet-style Convolutional Neural Network (CNN). The optimization must balance model accuracy against latency and memory costs, under a realistic hardware budget constraint. Your program must solve a multi-objective optimization by scalarization.\n\nStart from the following empirically grounded and widely used scaling relationships in convolutional architectures:\n\n1. For a stack of convolutional layers in a CNN, the floating-point compute required scales proportionally to the product of depth, squared width, and squared input resolution. Let the scaling multipliers for depth, width, and resolution be denoted by $\\alpha$, $\\beta$, and $\\gamma$ respectively. Then an idealized latency measure $L$ (in normalized, dimensionless units) is modeled as\n$$\nL(\\alpha,\\beta,\\gamma) = k_L \\, \\alpha \\, \\beta^2 \\, \\gamma^2,\n$$\nwhere $k_L$ is a positive constant capturing baseline compute-to-latency proportionality.\n\n2. The total memory footprint $M$ (in normalized, dimensionless units), combining parameter memory and activation memory, is modeled as\n$$\nM(\\alpha,\\beta,\\gamma) = k_P \\, \\alpha \\, \\beta^2 \\;+\\; k_A \\, \\alpha \\, \\beta \\, \\gamma^2,\n$$\nwhere $k_P$ represents proportionality for parameter memory and $k_A$ for activation memory. These forms follow from parameters scaling roughly with the product of depth and squared width, while activations scale with depth, width, and squared resolution.\n\n3. Accuracy $A$ (in an abstract accuracy score unit that is dimensionless) exhibits diminishing returns with increasing scale. A concave surrogate consistent with empirical scaling laws is\n$$\nA(\\alpha,\\beta,\\gamma) = s_d \\, \\ln(\\alpha) \\;+\\; s_w \\, \\ln(\\beta) \\;+\\; s_r \\, \\ln(\\gamma),\n$$\nwhere $s_d$, $s_w$, and $s_r$ are positive coefficients encoding the relative accuracy sensitivity to depth, width, and resolution.\n\nThe multi-objective optimization is scalarized into\n$$\nJ(\\alpha,\\beta,\\gamma;\\lambda_1,\\lambda_2) \\;=\\; A(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_1 \\, L(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_2 \\, M(\\alpha,\\beta,\\gamma),\n$$\nwhere $\\lambda_1$ and $\\lambda_2$ are nonnegative hardware-specific weights penalizing latency and memory respectively, both in dimensionless normalized units.\n\nOptimization constraints:\n- Compound compute budget:\n$$\n\\alpha \\, \\beta^2 \\, \\gamma^2 \\;\\le\\; B,\n$$\nwith $B \\ge 1$ a dimensionless budget. This inequality reflects that available compute may be underutilized if penalties are high; it must never be exceeded.\n- Lower bounds:\n$$\n\\alpha \\ge 1, \\quad \\beta \\ge 1, \\quad \\gamma \\ge 1.\n$$\nThese lower bounds enforce non-shrinking scaling (no downscaling below baseline).\n- Upper bounds:\n$$\n\\alpha \\le U, \\quad \\beta \\le U, \\quad \\gamma \\le U,\n$$\nwhere $U$ is a fixed upper limit to ensure well-posedness and reflect practical upper bounds on scaling.\n\nConstants to use in all computations:\n- $k_L = 1.0$, $k_P = 1.0$, $k_A = 0.5$,\n- $s_d = 0.20$, $s_w = 0.30$, $s_r = 0.50$,\n- $U = 5.0$.\n\nYour program must numerically maximize $J(\\alpha,\\beta,\\gamma;\\lambda_1,\\lambda_2)$ subject to the constraints. You may solve this by any correct numerical method; a constrained optimizer for differentiable objectives is acceptable. All quantities $L$, $M$, $A$, $J$, $\\alpha$, $\\beta$, $\\gamma$, $\\lambda_1$, $\\lambda_2$, $B$, $k_L$, $k_P$, $k_A$, $s_d$, $s_w$, $s_r$, and $U$ are dimensionless and must be treated as such.\n\nTest Suite:\nRun your solver on the following hardware-specific penalty weights and budgets $(\\lambda_1,\\lambda_2,B)$:\n- Case 1 (balanced, happy path): $(0.02,\\, 0.01,\\, 2.0)$\n- Case 2 (memory-dominated penalty): $(0.0,\\, 0.03,\\, 3.0)$\n- Case 3 (latency-dominated penalty): $(0.05,\\, 0.0,\\, 3.0)$\n- Case 4 (very high penalties, likely minimal scaling): $(0.20,\\, 0.10,\\, 2.0)$\n- Case 5 (boundary budget): $(0.01,\\, 0.005,\\, 1.0)$\n\nRequired final output:\n- For each test case, compute the optimal triple $(\\alpha^\\star,\\beta^\\star,\\gamma^\\star)$.\n- Round each scalar in the triple to $4$ decimal places.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets, where each element is the rounded triple as a Python-style list. For example, the output should look like\n$$\n[\\,[\\alpha_1,\\beta_1,\\gamma_1],\\,[\\alpha_2,\\beta_2,\\gamma_2],\\,\\dots\\,]\n$$\nwith each $\\alpha_i$, $\\beta_i$, and $\\gamma_i$ rounded to $4$ decimals. No additional text should be printed.", "solution": "The user has provided a well-defined multi-objective optimization problem, which I will first validate and then solve.\n\n### Step 1: Extract Givens\n\n**Models:**\n- Latency: $L(\\alpha,\\beta,\\gamma) = k_L \\, \\alpha \\, \\beta^2 \\, \\gamma^2$\n- Memory: $M(\\alpha,\\beta,\\gamma) = k_P \\, \\alpha \\, \\beta^2 \\;+\\; k_A \\, \\alpha \\, \\beta \\, \\gamma^2$\n- Accuracy: $A(\\alpha,\\beta,\\gamma) = s_d \\, \\ln(\\alpha) \\;+\\; s_w \\, \\ln(\\beta) \\;+\\; s_r \\, \\ln(\\gamma)$\n\n**Objective Function:**\n- Scalarized objective to maximize: $J(\\alpha,\\beta,\\gamma;\\lambda_1,\\lambda_2) \\;=\\; A(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_1 \\, L(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_2 \\, M(\\alpha,\\beta,\\gamma)$\n\n**Constraints:**\n- Compute budget: $\\alpha \\, \\beta^2 \\, \\gamma^2 \\;\\le\\; B$, with $B \\ge 1$\n- Lower bounds: $\\alpha \\ge 1, \\quad \\beta \\ge 1, \\quad \\gamma \\ge 1$\n- Upper bounds: $\\alpha \\le U, \\quad \\beta \\le U, \\quad \\gamma \\le U$\n\n**Constants:**\n- $k_L = 1.0$\n- $k_P = 1.0$\n- $k_A = 0.5$\n- $s_d = 0.20$\n- $s_w = 0.30$\n- $s_r = 0.50$\n- $U = 5.0$\n\n**Test Suite:**\n- Case 1: $(\\lambda_1, \\lambda_2, B) = (0.02, 0.01, 2.0)$\n- Case 2: $(\\lambda_1, \\lambda_2, B) = (0.0, 0.03, 3.0)$\n- Case 3: $(\\lambda_1, \\lambda_2, B) = (0.05, 0.0, 3.0)$\n- Case 4: $(\\lambda_1, \\lambda_2, B) = (0.20, 0.10, 2.0)$\n- Case 5: $(\\lambda_1, \\lambda_2, B) = (0.01, 0.005, 1.0)$\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientifically Grounded:** The problem is grounded in the principles of neural network architecture design, specifically relating to compound scaling as popularized by EfficientNet. The functional forms for latency ($L$), memory ($M$), and accuracy ($A$) are simplified but plausible models. The scaling of compute with depth ($\\alpha$), width squared ($\\beta^2$), and resolution squared ($\\gamma^2$) is standard. The memory model correctly distinguishes between parameters (scaling with $\\alpha \\beta^2$) and activations (scaling with $\\alpha \\beta \\gamma^2$). The logarithmic accuracy model reflects the principle of diminishing returns. The formulation is a valid abstraction of a real-world engineering problem in deep learning.\n\n2.  **Well-Posed:** The problem is to maximize a continuous, differentiable function $J$ over a compact (closed and bounded) subset of $\\mathbb{R}^3$. The feasible region is defined by the inequalities $1 \\le \\alpha \\le U$, $1 \\le \\beta \\le U$, $1 \\le \\gamma \\le U$, and $\\alpha \\beta^2 \\gamma^2 \\le B$. By the Extreme Value Theorem, a continuous function on a compact set must attain a maximum. Therefore, a solution is guaranteed to exist. The problem is well-posed.\n\n3.  **Objective:** The problem is stated using precise mathematical definitions and objective, formal language. All constants and variables are clearly defined. There is no ambiguity.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. It is a well-posed, scientifically grounded, and objective constrained non-linear optimization problem. I will proceed with the solution.\n\n### Solution Derivation\n\nThe task is to find the values of $(\\alpha, \\beta, \\gamma)$ that maximize the objective function $J$ subject to the given constraints. This is a constrained non-linear programming problem. Since numerical optimization libraries are typically designed to find minima, we will equivalently minimize the negative of the objective function, $-J$.\n\nLet the vector of variables be $\\mathbf{x} = [\\alpha, \\beta, \\gamma]$. The objective function to minimize is:\n$$\n-J(\\mathbf{x}; \\lambda_1, \\lambda_2) = -A(\\mathbf{x}) + \\lambda_1 L(\\mathbf{x}) + \\lambda_2 M(\\mathbf{x})\n$$\nSubstituting the given expressions and constants:\n$$\n-J(\\alpha, \\beta, \\gamma) = -[s_d \\ln(\\alpha) + s_w \\ln(\\beta) + s_r \\ln(\\gamma)] + \\lambda_1 [k_L \\alpha \\beta^2 \\gamma^2] + \\lambda_2 [k_P \\alpha \\beta^2 + k_A \\alpha \\beta \\gamma^2]\n$$\nGrouping terms by the variables $\\alpha, \\beta, \\gamma$:\n$$\n-J(\\alpha, \\beta, \\gamma) = -s_d \\ln(\\alpha) - s_w \\ln(\\beta) - s_r \\ln(\\gamma) + (\\lambda_1 k_L + \\lambda_2 k_A) \\alpha \\beta^2 \\gamma^2 + \\lambda_2 k_P \\alpha \\beta^2\n$$\nSubstituting the known constant values ($k_L=1.0, k_P=1.0, k_A=0.5, s_d=0.2, s_w=0.3, s_r=0.5$):\n$$\n-J(\\alpha, \\beta, \\gamma) = -0.2 \\ln(\\alpha) - 0.3 \\ln(\\beta) - 0.5 \\ln(\\gamma) + (\\lambda_1 + 0.5 \\lambda_2) \\alpha \\beta^2 \\gamma^2 + \\lambda_2 \\alpha \\beta^2\n$$\nThis is the function we will provide to a numerical optimizer.\n\nThe constraints are:\n1.  **Inequality Constraint:** $\\alpha \\beta^2 \\gamma^2 \\le B$. For standard solvers that require constraints of the form $g(\\mathbf{x}) \\ge 0$, this is written as $B - \\alpha \\beta^2 \\gamma^2 \\ge 0$.\n2.  **Bound Constraints:**\n    $1 \\le \\alpha \\le U$\n    $1 \\le \\beta \\le U$\n    $1 \\le \\gamma \\le U$\n    with $U=5.0$.\n\nWe will use the Sequential Least Squares Programming (`SLSQP`) method from the `scipy.optimize` library, as it is well-suited for non-linear optimization problems with both bound and inequality constraints.\n\nWe will iterate through each test case $(\\lambda_1, \\lambda_2, B)$, defining the objective function and constraints accordingly. A robust initial guess for the optimizer is the baseline model $(\\alpha, \\beta, \\gamma) = (1, 1, 1)$, which is guaranteed to be in the feasible region for any $B \\ge 1$.\n\nFor the special case where $B=1.0$ (Test Case 5), the constraints $\\alpha \\ge 1, \\beta \\ge 1, \\gamma \\ge 1$ and $\\alpha \\beta^2 \\gamma^2 \\le 1$ together force the only possible solution to be $\\alpha=1, \\beta=1, \\gamma=1$. The optimizer should converge to this point.\n\nThe final result for each test case will be the optimal triple $(\\alpha^\\star, \\beta^\\star, \\gamma^\\star)$, with each component rounded to $4$ decimal places, formatted as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Computes optimal compound scaling coefficients for a CNN model by solving\n    a constrained multi-objective optimization problem.\n    \"\"\"\n    # Define constants as specified in the problem statement.\n    # All quantities are dimensionless.\n    k_L = 1.0  # Latency proportionality\n    k_P = 1.0  # Parameter memory proportionality\n    k_A = 0.5  # Activation memory proportionality\n    s_d = 0.20 # Accuracy sensitivity to depth\n    s_w = 0.30 # Accuracy sensitivity to width\n    s_r = 0.50 # Accuracy sensitivity to resolution\n    U = 5.0    # Upper bound for scaling factors\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (lambda1, lambda2, B)\n    test_cases = [\n        (0.02, 0.01, 2.0),   # Case 1: balanced, happy path\n        (0.0, 0.03, 3.0),    # Case 2: memory-dominated penalty\n        (0.05, 0.0, 3.0),    # Case 3: latency-dominated penalty\n        (0.20, 0.10, 2.0),   # Case 4: very high penalties\n        (0.01, 0.005, 1.0),  # Case 5: boundary budget\n    ]\n\n    results = []\n\n    # Define the objective function to be minimized (-J)\n    def objective_function(x, lambda1, lambda2):\n        \"\"\"\n        Calculates the negative of the scalarized objective function J.\n        We minimize -J to maximize J.\n        \n        x: numpy array [alpha, beta, gamma]\n        lambda1: latency penalty weight\n        lambda2: memory penalty weight\n        \"\"\"\n        alpha, beta, gamma = x[0], x[1], x[2]\n        \n        # Guard against log(x) for x<=0, though bounds should prevent this.\n        if alpha <= 0 or beta <= 0 or gamma <= 0:\n            return np.inf\n\n        accuracy_term = s_d * np.log(alpha) + s_w * np.log(beta) + s_r * np.log(gamma)\n        \n        latency_term = k_L * alpha * beta**2 * gamma**2\n        \n        memory_term = k_P * alpha * beta**2 + k_A * alpha * beta * gamma**2\n        \n        penalty = lambda1 * latency_term + lambda2 * memory_term\n        \n        # Return negative J for minimization\n        return -(accuracy_term - penalty)\n\n    for case in test_cases:\n        lambda1, lambda2, B = case\n\n        # Define the inequality constraint: alpha * beta^2 * gamma^2 <= B\n        # Scipy's SLSQP expects constraints in the form g(x) >= 0\n        constraint = {\n            'type': 'ineq',\n            'fun': lambda x: B - x[0] * x[1]**2 * x[2]**2\n        }\n\n        # Define the bounds for alpha, beta, gamma: 1 <= x_i <= U\n        bounds = ((1.0, U), (1.0, U), (1.0, U))\n\n        # Initial guess: the baseline model, which is always feasible.\n        x0 = np.array([1.0, 1.0, 1.0])\n\n        # Perform the constrained optimization\n        opt_result = minimize(\n            fun=objective_function,\n            x0=x0,\n            args=(lambda1, lambda2),\n            method='SLSQP',\n            bounds=bounds,\n            constraints=[constraint],\n            tol=1e-9\n        )\n\n        # Extract the optimal scaling factors\n        optimal_coeffs = opt_result.x\n\n        # Round the results to 4 decimal places\n        rounded_coeffs = np.round(optimal_coeffs, 4).tolist()\n        results.append(rounded_coeffs)\n    \n    # Format the final output string exactly as required\n    output_str = str(results).replace(\" \", \"\")\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```", "id": "3119675"}, {"introduction": "Once a scaling strategy is defined by the coefficients $(\\alpha, \\beta, \\gamma)$, the next step is to implement it and evaluate its effectiveness. This practice guides you through the core mechanics of building and analyzing an EfficientNet-style model stage [@problem_id:3119662]. You will construct a cost model for the fundamental Mobile Inverted Bottleneck Convolution (MBConv) block and apply the compound scaling principle to scale it up. By modeling how real-world training imperfections can affect performance, you will gain a deeper appreciation for the gap between theoretical computational scaling and realized accuracy gains.", "problem": "You are tasked with implementing a principled cost model and scaling procedure for a family of efficient convolutional neural network architectures, focusing on the EfficientNet compound scaling idea and the Mobile Inverted Bottleneck Convolution (MBConv) with Squeeze-and-Excitation (SE). Your program must be fully self-contained and produce quantifiable outputs for a predefined test suite. The goal is to evaluate accuracy recovery under compound scaling from EfficientNet-B0 through EfficientNet-B3 analogs and to quantify deviations attributable to training hyperparameters.\n\nFoundational base you must use:\n\n- For a two-dimensional convolution with kernel size $k \\times k$, input channels $C_{\\text{in}}$, output channels $C_{\\text{out}}$, and spatial resolution $H \\times W$, the multiply–accumulate count (MAC) is $H W C_{\\text{in}} k^2 C_{\\text{out}}$; the parameter count is $C_{\\text{in}} k^2 C_{\\text{out}}$.\n- For a depthwise convolution on $C$ channels with kernel size $k \\times k$ and spatial resolution $H \\times W$, the MAC is $H W C k^2$; the parameter count is $C k^2$.\n- A pointwise ($1 \\times 1$) convolution from $C_{\\text{in}}$ to $C_{\\text{out}}$ at resolution $H \\times W$ has MAC $H W C_{\\text{in}} C_{\\text{out}}$ and parameters $C_{\\text{in}} C_{\\text{out}}$.\n- The Mobile Inverted Bottleneck Convolution (MBConv) block with expansion factor $t$ and kernel size $k$ consists of: an expansion pointwise convolution from $C_{\\text{in}}$ to $t C_{\\text{in}}$, a depthwise convolution with kernel size $k \\times k$ on $t C_{\\text{in}}$ channels, an SE module that reduces channels by a ratio $\\rho_{\\text{se}}$ and then re-expands, and a projection pointwise convolution back to $C_{\\text{out}}$.\n- The Squeeze-and-Excitation (SE) module uses two fully connected or $1 \\times 1$ convolutional transforms: one from $t C_{\\text{in}}$ to $t C_{\\text{in}} \\rho_{\\text{se}}$ and one back from $t C_{\\text{in}} \\rho_{\\text{se}}$ to $t C_{\\text{in}}$. Its parameter count and MACs for these linear transforms are $t C_{\\text{in}} \\cdot (t C_{\\text{in}} \\rho_{\\text{se}}) + (t C_{\\text{in}} \\rho_{\\text{se}}) \\cdot t C_{\\text{in}}$. Global average pooling and elementwise nonlinearities are ignored in MACs.\n- Rounding rule: any channel count must be rounded to the nearest integer that is divisible by $8$.\n- Compound scaling principle: the total compute of a stage with width $w$, depth $d$ repeats, and square resolution $s \\times s$ scales approximately as a product in the form $s^2 w^2 d$ multiplied by constants from the block structure. Imposing a per-increment scaling coefficient $\\phi$ and fixed multipliers $(\\alpha, \\beta, \\gamma)$ that act on $(d, w, s)$ respectively, the doubling-of-compute constraint requires that increasing $\\phi$ by $1$ approximately doubles the compute, which yields a constraint of the form $\\alpha \\beta^2 \\gamma^2 \\approx 2$.\n\nYour program must:\n\n1. Implement an MBConv-with-SE cost model:\n   - Inputs: $C_{\\text{in}}$, $C_{\\text{out}}$, $t$, $k$, $s$, $\\rho_{\\text{se}}$.\n   - Outputs: per-block MACs and parameter count, using the foundational formulas above.\n   - For a stage with $d$ repeats, assume $C_{\\text{in}} = C_{\\text{out}} = w$ for all repeats and compute stage MACs as $d$ times the per-block MACs, applying the rounding rule for all channel counts to be divisible by $8$.\n\n2. Implement compound scaling:\n   - Inputs: baseline $(w_0, d_0, s_0)$, scaling multipliers $(\\alpha, \\beta, \\gamma)$, and scaling coefficient $\\phi$.\n   - Compute scaled $(w, d, s)$ by exponentially scaling each dimension by its respective multiplier for the given $\\phi$ under the doubling-of-compute constraint as described. Apply rounding to $w$ and $t w$ to be divisible by $8$, round $d$ to the nearest positive integer, and round $s$ to the nearest integer.\n\n3. Define a baseline stage (EfficientNet-B0 analog) and compute its stage MACs $N_0$ with MBConv settings:\n   - Baseline: $w_0 = 32$, $d_0 = 4$, $s_0 = 224$, $t = 6$, $k = 3$, $\\rho_{\\text{se}} = 0.25$, channel divisor $8$.\n\n4. For a scaled stage at coefficient $\\phi$, compute:\n   - Scaled stage MACs $N$ using the scaled $(w, d, s)$ from step $2$ and the MBConv settings from step $3$ unless otherwise specified in the test case.\n   - Target doubled-compute $N_{\\text{target}} = N_0 \\cdot 2^{\\phi}$.\n\n5. Define an accuracy proxy from widely observed neural scaling behavior:\n   - Use a normalized compute $x = N / N_0$.\n   - A proxy loss $L(x) = c_0 x^{-p} + c_1$, where $c_0$, $p$, and $c_1$ are positive constants, and the proxy accuracy $A(x) = 1 - L(x)$.\n   - Training hyperparameters induce an effective utilization factor $u = \\min(\\eta / \\eta^{\\ast}, 1) \\cdot \\min(B / B^{\\ast}, 1) \\cdot \\min(E / E^{\\ast}, 1)$, where $(\\eta, B, E)$ are the learning rate, batch size, and epoch count, and $(\\eta^{\\ast}, B^{\\ast}, E^{\\ast})$ are their respective optimal values.\n   - The realized accuracy is $A_{\\text{eff}} = A(x) \\cdot u$. The ideal target accuracy at coefficient $\\phi$ is $A_{\\text{target}} = 1 - \\left(c_0 \\left(2^{\\phi}\\right)^{-p} + c_1\\right)$.\n   - Define deviation $\\Delta = A_{\\text{eff}} - A_{\\text{target}}$.\n\n6. Use the following fixed constants and scaling multipliers:\n   - Scaling multipliers: $\\alpha = 1.2$, $\\beta = 1.1$, $\\gamma = 1.15$.\n   - Accuracy proxy constants: $c_0 = 0.4$, $p = 0.2$, $c_1 = 0.1$.\n   - Optimal hyperparameters: $\\eta^{\\ast} = 0.2$, $B^{\\ast} = 128$, $E^{\\ast} = 350$.\n\n7. Test suite:\n   - Case $1$: $\\phi = 0$, $t = 6$, $\\rho_{\\text{se}} = 0.25$, $B = 128$, $\\eta = 0.2$, $E = 350$.\n   - Case $2$: $\\phi = 1$, $t = 6$, $\\rho_{\\text{se}} = 0.25$, $B = 128$, $\\eta = 0.2$, $E = 350$.\n   - Case $3$: $\\phi = 2$, $t = 6$, $\\rho_{\\text{se}} = 0.25$, $B = 128$, $\\eta = 0.05$, $E = 350$.\n   - Case $4$: $\\phi = 3$, $t = 6$, $\\rho_{\\text{se}} = 0.25$, $B = 32$, $\\eta = 0.2$, $E = 200$.\n   - Case $5$: $\\phi = 1$, $t = 6$, $\\rho_{\\text{se}} = 0.5$, $B = 128$, $\\eta = 0.2$, $E = 350$.\n\nYour program should produce a single line of output containing the deviations $\\Delta$ for the cases in the order above, as a comma-separated list enclosed in square brackets, for example, $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$. No additional text should be printed.", "solution": "The problem requires the implementation of a computational model to analyze the compound scaling of a convolutional neural network (CNN) stage, based on the principles articulated for the EfficientNet family of models. The analysis involves calculating the computational cost (in Multiply-Accumulate operations, or MACs) of a stage, scaling its dimensions, and evaluating the resulting performance against an idealized target using a defined accuracy proxy. This process allows for the quantification of deviations from ideal scaling due to both architectural rounding and suboptimal training hyperparameters.\n\nThe solution is structured upon three foundational pillars: the cost model for the Mobile Inverted Bottleneck Convolution (MBConv) block, the compound scaling law for network dimensions, and an explicit model for accuracy and its deviation from an ideal target.\n\nFirst, we establish the cost model for a single MBConv block within a stage. A stage is defined by its width (number of channels, $w$), depth (number of block repetitions, $d$), and a square spatial resolution of side length $s$. For a block where input and output channels are both $w$, with an expansion factor $t$, a depthwise convolution kernel of size $k \\times k$, and a Squeeze-and-Excitation (SE) module with a reduction ratio $\\rho_{\\text{se}}$, the MAC count is the sum of its constituent operations. The problem specifies a rounding rule where all channel counts must be rounded to the nearest integer divisible by a divisor, given as $8$. Let $\\text{round}_m(x)$ denote the function that rounds $x$ to the nearest multiple of $m$. The channel counts used in the calculation are:\n- Expansion channels: $C_{\\text{exp}} = \\text{round}_8(t \\cdot w)$\n- SE-reduced channels: $C_{\\text{se}} = \\text{round}_8(C_{\\text{exp}} \\cdot \\rho_{\\text{se}})$\n\nThe total MACs for one MBConv block are the sum of the MACs for each component:\n1.  Expansion ($1 \\times 1$ conv): $s^2 \\cdot w \\cdot C_{\\text{exp}}$\n2.  Depthwise ($k \\times k$ conv): $s^2 \\cdot C_{\\text{exp}} \\cdot k^2$\n3.  Squeeze-and-Excitation (two linear layers on pooled features): $2 \\cdot C_{\\text{exp}} \\cdot C_{\\text{se}}$\n4.  Projection ($1 \\times 1$ conv): $s^2 \\cdot C_{\\text{exp}} \\cdot w$\n\nThe total MACs for a single block, $\\text{MACs}_{\\text{block}}$, is the sum of these four terms. The total MACs for the stage, $N$, is then $d \\cdot \\text{MACs}_{\\text{block}}$.\n\nSecond, we implement the compound scaling principle. Given a baseline architecture with depth $d_0$, width $w_0$, and resolution $s_0$, the dimensions are scaled by a coefficient $\\phi$ using specific multipliers $\\alpha$, $\\beta$, and $\\gamma$ for depth, width, and resolution, respectively. The scaled dimensions are given by:\n- $d(\\phi) = d_0 \\cdot \\alpha^{\\phi}$\n- $w(\\phi) = w_0 \\cdot \\beta^{\\phi}$\n- $s(\\phi) = s_0 \\cdot \\gamma^{\\phi}$\n\nThese raw scaled values are then rounded according to the problem's rules: $d$ is rounded to the nearest positive integer, $w$ is rounded to the nearest integer divisible by $8$, and $s$ is rounded to the nearest integer. The constraint $\\alpha \\beta^2 \\gamma^2 \\approx 2$ ensures that increasing $\\phi$ by $1$ approximately doubles the total computational cost, which is proportional to $d \\cdot w^2 \\cdot s^2$.\n\nThird, we define the accuracy and deviation models. The performance of a model is related to its computational budget through a power-law relationship. The proxy accuracy $A(x)$ is defined as $A(x) = 1 - L(x)$, where $L(x) = c_0 x^{-p} + c_1$ is the proxy loss. Here, $x = N/N_0$ is the computational budget of the scaled model normalized by the baseline model's budget $N_0$. The constants $c_0$, $c_1$, and $p$ are given. The ideal or target accuracy for a scaling level $\\phi$, $A_{\\text{target}}$, assumes perfect computational scaling, i.e., $N = N_0 \\cdot 2^\\phi$. Thus, $A_{\\text{target}}(\\phi) = 1 - (c_0 (2^\\phi)^{-p} + c_1)$.\nThe realized accuracy, $A_{\\text{eff}}$, is affected by training hyperparameters. This is modeled by a utilization factor $u$, defined as $u = \\min(\\eta / \\eta^{\\ast}, 1) \\cdot \\min(B / B^{\\ast}, 1) \\cdot \\min(E / E^{\\ast}, 1)$, where $(\\eta, B, E)$ are the learning rate, batch size, and epoch count, and $(\\eta^{\\ast}, B^{\\ast}, E^{\\ast})$ are their optimal values. The effective accuracy is then $A_{\\text{eff}} = A(x) \\cdot u$. The final quantity of interest is the deviation $\\Delta = A_{\\text{eff}} - A_{\\text{target}}$, which measures the difference between the model's realized accuracy and its ideal target.\n\nThe computational procedure for each test case is as follows:\n1.  Calculate the baseline MAC count $N_0$ using the given baseline parameters for $\\phi=0$.\n2.  For each test case with a specific set of $(\\phi, t, \\rho_{\\text{se}}, B, \\eta, E)$:\n    a. Compute the scaled dimensions $(w, d, s)$ using $\\phi$ and the specified rounding rules.\n    b. Calculate the stage MACs, $N$, using these scaled dimensions and the given MBConv parameters $(t, k, \\rho_{\\text{se}})$.\n    c. Compute the normalized compute $x = N/N_0$.\n    d. Calculate the utilization factor $u$ from the training hyperparameters.\n    e. Compute the effective accuracy $A_{\\text{eff}} = (1 - (c_0 x^{-p} + c_1)) \\cdot u$.\n    f. Compute the target accuracy $A_{\\text{target}} = 1 - (c_0 (2^\\phi)^{-p} + c_1)$.\n    g. Calculate and record the deviation $\\Delta = A_{\\text{eff}} - A_{\\text{target}}$.\n\nThe following program implements this logic to compute the deviations for the provided test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a cost and accuracy model for a CNN stage based on EfficientNet principles,\n    calculates performance deviations for a set of test cases.\n    \"\"\"\n\n    # --- Constants and Baseline Definitions ---\n\n    # Scaling multipliers\n    ALPHA = 1.2\n    BETA = 1.1\n    GAMMA = 1.15\n\n    # Accuracy proxy constants\n    C0 = 0.4\n    P = 0.2\n    C1 = 0.1\n\n    # Optimal hyperparameters\n    ETA_STAR = 0.2\n    B_STAR = 128\n    E_STAR = 350\n\n    # Baseline architecture and MBConv settings\n    W0 = 32\n    D0 = 4\n    S0 = 224\n    K_BASE = 3\n    CHANNEL_DIVISOR = 8\n\n    # Test suite\n    test_cases = [\n        # (phi, t, rho_se, B, eta, E)\n        (0, 6, 0.25, 128, 0.2, 350),\n        (1, 6, 0.25, 128, 0.2, 350),\n        (2, 6, 0.25, 128, 0.05, 350),\n        (3, 6, 0.25, 32, 0.2, 200),\n        (1, 6, 0.5, 128, 0.2, 350),\n    ]\n\n    # --- Helper Functions ---\n\n    def round_divisible(n, divisor):\n        \"\"\"Rounds n to the nearest integer divisible by divisor.\"\"\"\n        if divisor == 0:\n            return int(n)\n        return int(np.round(n / divisor) * divisor)\n\n    def calculate_stage_macs(w, d, s, t, k, rho_se, divisor):\n        \"\"\"Calculates total MACs for an MBConv stage.\"\"\"\n        \n        # Apply rounding to all channel counts\n        c_in = w # w is assumed pre-rounded\n        c_expand = round_divisible(t * c_in, divisor)\n        c_se = round_divisible(c_expand * rho_se, divisor)\n        # Ensure c_se is at least divisor if c_expand is not zero to avoid div by zero in some real cases,\n        # but problem says \"round\" not \"round up\". For n < divisor/2, this can become 0.\n        if c_se == 0 and c_expand > 0:\n             # As per strict problem spec, a 0 is possible. In a real net this would be an issue.\n             # e.g., if t*w*rho_se < 4. We will follow spec.\n             pass\n        c_out = w\n\n        s_squared = s * s\n        \n        # MACs for each part of a single MBConv block\n        macs_expansion = s_squared * c_in * c_expand\n        macs_depthwise = s_squared * c_expand * (k * k)\n        macs_se = 2 * c_expand * c_se\n        macs_projection = s_squared * c_expand * c_out\n        \n        macs_per_block = macs_expansion + macs_depthwise + macs_se + macs_projection\n        \n        total_macs = d * macs_per_block\n        return total_macs\n\n    def scale_dimensions(phi, w0, d0, s0, alpha, beta, gamma, divisor):\n        \"\"\"Scales network dimensions based on phi and rounds them.\"\"\"\n        d_scaled = d0 * (alpha ** phi)\n        w_scaled = w0 * (beta ** phi)\n        s_scaled = s0 * (gamma ** phi)\n        \n        # Rounding as per problem specification\n        d = max(1, int(np.round(d_scaled))) # nearest positive integer\n        w = round_divisible(w_scaled, divisor)\n        s = int(np.round(s_scaled))\n        \n        return w, d, s\n\n    # --- Main Calculation Logic ---\n\n    results = []\n\n    # 1. Calculate baseline MACs (N0)\n    w_base, d_base, s_base = scale_dimensions(0, W0, D0, S0, ALPHA, BETA, GAMMA, CHANNEL_DIVISOR)\n    base_case = test_cases[0]\n    n0 = calculate_stage_macs(\n        w=w_base, \n        d=d_base, \n        s=s_base, \n        t=base_case[1], \n        k=K_BASE, \n        rho_se=base_case[2], \n        divisor=CHANNEL_DIVISOR\n    )\n\n    if n0 == 0:\n        raise ValueError(\"Baseline MAC count (N0) is zero, preventing normalization.\")\n\n    # 2. Iterate through test cases\n    for case in test_cases:\n        phi, t, rho_se, b, eta, e = case\n        \n        # a. Compute scaled dimensions\n        w, d, s = scale_dimensions(phi, W0, D0, S0, ALPHA, BETA, GAMMA, CHANNEL_DIVISOR)\n        \n        # b. Calculate actual stage MACs (N)\n        n = calculate_stage_macs(w, d, s, t, K_BASE, rho_se, CHANNEL_DIVISOR)\n        \n        # c. Compute normalized compute (x)\n        x = n / n0\n        \n        # d. Calculate utilization factor (u)\n        u = min(eta / ETA_STAR, 1) * min(b / B_STAR, 1) * min(e / E_STAR, 1)\n        \n        # e. Compute effective accuracy (A_eff)\n        loss_x = C0 * (x ** -P) + C1\n        a_x = 1 - loss_x\n        a_eff = a_x * u\n        \n        # f. Compute target accuracy (A_target)\n        loss_target = C0 * ((2 ** phi) ** -P) + C1\n        a_target = 1 - loss_target\n        \n        # g. Calculate deviation (Delta)\n        delta = a_eff - a_target\n        results.append(delta)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3119662"}, {"introduction": "The standard compound scaling assumes isotropic input images, where height and width are equal. This final practice challenges you to adapt these principles to a more complex, real-world scenario: anisotropic inputs with varying aspect ratios. You will first derive the compute scaling law for this generalized case and then use optimization to determine the best way to adjust network width and depth to maintain the target computational budget [@problem_id:3119513]. This exercise moves beyond simply applying a formula to adapting core principles, a crucial skill for tackling novel problems in model design and deployment.", "problem": "You are asked to formalize and compute the effect of anisotropic input resolution on the compute budget of a convolutional neural network that follows the compound scaling principle popularized by EfficientNet. The goal is to reason from base definitions about the number of multiply–accumulate operations in convolutions and to determine how to minimally adjust width and depth multipliers when the input resolution is scaled anisotropically.\n\nAssumptions and base definitions:\n- A single two-dimensional convolution with kernel size $k_x \\times k_y$, input channels $c_{\\mathrm{in}}$, output channels $c_{\\mathrm{out}}$, and output spatial size $h \\times w$ requires a number of multiply–accumulate operations proportional to $k_x k_y \\, c_{\\mathrm{in}} \\, c_{\\mathrm{out}} \\, h \\, w$.\n- Consider a stack of such convolutions where, under a width multiplier $w$, all channel dimensions are scaled in proportion to $w$ across layers, and under a depth multiplier $d$, the total number of layers is scaled in proportion to $d$.\n- The spatial input resolution to the network is $(r_x, r_y)$, and the spatial resolution within convolution layers scales proportionally with $(r_x, r_y)$ up to constant factors that do not depend on $w$ or $d$.\n\nFrom these base facts, derive a scaling law for the total multiply–accumulate operations of the network as a function of width multiplier $w$, depth multiplier $d$, and input spatial dimensions $(r_x, r_y)$. Do not introduce any formulas that are not derivable directly from the stated base facts.\n\nDefine a baseline resolution $r_0 = 224$ and adopt the following compound scaling parameterization for the nominal (isotropic) family: for a compound exponent $\\phi \\ge 0$ and coefficients $(\\alpha,\\beta,\\gamma)$,\n- width multiplier $w(\\phi) = \\beta^{\\phi}$,\n- depth multiplier $d(\\phi) = \\alpha^{\\phi}$,\n- target isotropic resolution $r(\\phi) = r_0 \\, \\gamma^{\\phi}$.\n\nUse the concrete coefficients $(\\alpha,\\beta,\\gamma) = (1.2, 1.1, 1.15)$.\n\nTask:\n- For a given $\\phi$ and an anisotropic input resolution $(R_x, R_y)$, determine adjusted multipliers $(w_{\\mathrm{adj}}, d_{\\mathrm{adj}})$ such that the total multiply–accumulate operations match the nominal isotropic compute at the same $\\phi$.\n- Among all $(w_{\\mathrm{adj}}, d_{\\mathrm{adj}})$ that satisfy the compute-equality constraint, choose the pair that minimizes the squared log-deviation from the nominal multipliers:\n  minimize $(\\log w_{\\mathrm{adj}} - \\log w(\\phi))^2 + (\\log d_{\\mathrm{adj}} - \\log d(\\phi))^2$ subject to compute equality.\n- Your program should output, for each test case, the relative adjustment factors:\n  $$\n  \\frac{w_{\\mathrm{adj}}}{w(\\phi)} \\quad \\text{and} \\quad \\frac{d_{\\mathrm{adj}}}{d(\\phi)} \\, .\n  $$\n\nTest suite:\nUse $r_0 = 224$, $(\\alpha,\\beta,\\gamma) = (1.2, 1.1, 1.15)$, and the following five cases. In each case, the program must compute the pair $\\left(\\frac{w_{\\mathrm{adj}}}{w(\\phi)}, \\frac{d_{\\mathrm{adj}}}{d(\\phi)}\\right)$.\n\n- Case $1$: $\\phi = 1$, $R_x = 224 \\cdot 1.15$, $R_y = 224 \\cdot 1.15$.\n- Case $2$: $\\phi = 2$, $R_x = 224 \\cdot 1.15^{2} \\cdot 3$, $R_y = 224 \\cdot 1.15^{2} / 3$.\n- Case $3$: $\\phi = 1$, $R_x = 224 \\cdot 1.15$, $R_y = 224 \\cdot 1.15 / 2$.\n- Case $4$: $\\phi = 1$, $R_x = 224 \\cdot 1.15$, $R_y = 224 \\cdot 1.15 \\cdot 2$.\n- Case $5$: $\\phi = 0$, $R_x = 336$, $R_y = 112$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, where each inner list corresponds to a test case in the given order and contains the two floats $\\left[\\frac{w_{\\mathrm{adj}}}{w(\\phi)}, \\frac{d_{\\mathrm{adj}}}{d(\\phi)}\\right]$ rounded to $6$ decimal places.\n- For example, the format must be exactly like:\n  $$\n  [[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4],[a_5,b_5]]\n  $$\n  where each $a_i$ and $b_i$ is a decimal numeral with up to six places after the decimal point.", "solution": "The user has provided a well-posed and scientifically grounded problem that requires the derivation of a scaling law for neural network computational cost and the solution of a constrained optimization problem. The problem is validated as sound and self-contained.\n\n### 1. Derivation of the Compute Scaling Law\nThe total number of multiply-accumulate operations (MACs), a measure of computational cost often referred to as FLOPs, is to be determined as a function of network scaling parameters.\n\nA single 2D convolution's MACs are given as proportional to the product of kernel dimensions ($k_x$, $k_y$), input channels ($c_{\\mathrm{in}}$), output channels ($c_{\\mathrm{out}}$), and output spatial dimensions ($h$, $w$).\n$$\n\\text{MACs}_{\\text{layer}} \\propto k_x k_y c_{\\mathrm{in}} c_{\\mathrm{out}} h w\n$$\nThe problem states how the network architecture scales with a width multiplier $w$, a depth multiplier $d$, and an input resolution $(r_x, r_y)$.\n\n- **Width Scaling**: All channel dimensions ($c_{\\mathrm{in}}$, $c_{\\mathrm{out}}$) are scaled in proportion to the width multiplier $w$. Therefore, the product $c_{\\mathrm{in}} c_{\\mathrm{out}}$ scales with $w^2$.\n- **Resolution Scaling**: The spatial dimensions of feature maps ($h, w$) scale proportionally with the input resolution $(r_x, r_y)$. This means $h \\propto r_y$ and $w \\propto r_x$. Consequently, the product $h w$ scales with $r_x r_y$.\n- **Depth Scaling**: The total number of layers in the network is scaled in proportion to the depth multiplier $d$.\n\nCombining these factors, the MACs for any single layer in the scaled network are proportional to $w^2 r_x r_y$. The total MACs for the entire network, which is a sum over all layers, is proportional to the MACs per layer multiplied by the number of layers.\n$$\n\\text{Total MACs} \\propto (\\text{number of layers}) \\times (\\text{MACs per layer})\n$$\nThus, the total MACs, which we denote as a function $F(w, d, r_x, r_y)$, scales as:\n$$\nF(w, d, r_x, r_y) \\propto d \\cdot w^2 \\cdot r_x r_y\n$$\nWe can express this as an equality with a constant of proportionality $C$ that depends on the specific architecture of the base network but not on the scaling factors:\n$$\nF(w, d, r_x, r_y) = C \\cdot d \\cdot w^2 \\cdot r_x r_y\n$$\nThis derived formula is the scaling law for the network's computational cost.\n\n### 2. Formulation of the Optimization Problem\nWe are given a nominal (isotropic) scaling configuration defined by a compound exponent $\\phi \\ge 0$ and coefficients $(\\alpha, \\beta, \\gamma) = (1.2, 1.1, 1.15)$:\n- Width multiplier: $w(\\phi) = \\beta^\\phi = 1.1^\\phi$\n- Depth multiplier: $d(\\phi) = \\alpha^\\phi = 1.2^\\phi$\n- Isotropic resolution: $r(\\phi) = r_0 \\gamma^\\phi = 224 \\cdot 1.15^\\phi$\n\nThe computational cost for this nominal configuration is:\n$$\nF_{\\mathrm{nom}} = F(w(\\phi), d(\\phi), r(\\phi), r(\\phi)) = C \\cdot d(\\phi) \\cdot [w(\\phi)]^2 \\cdot [r(\\phi)]^2\n$$\nFor a given $\\phi$ and an anisotropic input resolution $(R_x, R_y)$, we need to find adjusted multipliers $(w_{\\mathrm{adj}}, d_{\\mathrm{adj}})$ such that the compute budget is the same. This gives us the constraint:\n$$\nF(w_{\\mathrm{adj}}, d_{\\mathrm{adj}}, R_x, R_y) = F_{\\mathrm{nom}}\n$$\n$$\nC \\cdot d_{\\mathrm{adj}} \\cdot w_{\\mathrm{adj}}^2 \\cdot R_x R_y = C \\cdot d(\\phi) \\cdot [w(\\phi)]^2 \\cdot [r(\\phi)]^2\n$$\nThe constant $C$ cancels out, yielding the constraint equation:\n$$\nd_{\\mathrm{adj}} \\cdot w_{\\mathrm{adj}}^2 = \\frac{d(\\phi) \\cdot [w(\\phi)]^2 \\cdot [r(\\phi)]^2}{R_x R_y}\n$$\nThe objective is to find the pair $(w_{\\mathrm{adj}}, d_{\\mathrm{adj}})$ that satisfies this constraint while minimizing the squared log-deviation from the nominal multipliers:\n$$\n\\text{minimize } J(w_{\\mathrm{adj}}, d_{\\mathrm{adj}}) = (\\log w_{\\mathrm{adj}} - \\log w(\\phi))^2 + (\\log d_{\\mathrm{adj}} - \\log d(\\phi))^2\n$$\n\n### 3. Solving the Optimization Problem\nTo simplify the problem, we work in the logarithmic space of the multipliers. Let:\n- $W = \\log w_{\\mathrm{adj}}$, $D = \\log d_{\\mathrm{adj}}$\n- $W_0 = \\log w(\\phi)$, $D_0 = \\log d(\\phi)$\n\nThe objective function becomes minimizing the squared Euclidean distance between $(W, D)$ and $(W_0, D_0)$:\n$$\n\\text{minimize } (W - W_0)^2 + (D - D_0)^2\n$$\nTaking the logarithm of the constraint equation gives:\n$$\n\\log(d_{\\mathrm{adj}} \\cdot w_{\\mathrm{adj}}^2) = \\log\\left(\\frac{d(\\phi) \\cdot [w(\\phi)]^2 \\cdot [r(\\phi)]^2}{R_x R_y}\\right)\n$$\n$$\nD + 2W = D_0 + 2W_0 + \\log\\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)\n$$\nThis is a linear constraint in $W$ and $D$. The problem is to find the point on this line that is closest to the point $(W_0, D_0)$. We use the method of Lagrange multipliers. Let the Lagrangian be $\\mathcal{L}(W, D, \\lambda)$:\n$$\n\\mathcal{L} = (W - W_0)^2 + (D - D_0)^2 - \\lambda \\left( D + 2W - C' \\right)\n$$\nwhere $C'$ is the constant right-hand side of the log-constraint equation. Taking partial derivatives and setting them to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W} = 2(W - W_0) - 2\\lambda = 0 \\implies W - W_0 = \\lambda\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial D} = 2(D - D_0) - \\lambda = 0 \\implies D - D_0 = \\frac{\\lambda}{2}\n$$\nSubstituting $W = W_0 + \\lambda$ and $D = D_0 + \\lambda/2$ into the constraint $D + 2W = C'$:\n$$\n(D_0 + \\lambda/2) + 2(W_0 + \\lambda) = C' \\implies D_0 + 2W_0 + \\frac{5}{2}\\lambda = C'\n$$\nSolving for $\\lambda$:\n$$\n\\lambda = \\frac{2}{5} (C' - (D_0 + 2W_0)) = \\frac{2}{5} \\log\\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)\n$$\nNow we find the optimal adjustments in log-space:\n$$\n\\log w_{\\mathrm{adj}} = W = W_0 + \\lambda = \\log w(\\phi) + \\frac{2}{5} \\log\\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)\n$$\n$$\n\\log d_{\\mathrm{adj}} = D = D_0 + \\frac{\\lambda}{2} = \\log d(\\phi) + \\frac{1}{5} \\log\\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)\n$$\nExponentiating both sides to find $w_{\\mathrm{adj}}$ and $d_{\\mathrm{adj}}$:\n$$\nw_{\\mathrm{adj}} = w(\\phi) \\cdot \\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)^{2/5}\n$$\n$$\nd_{\\mathrm{adj}} = d(\\phi) \\cdot \\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)^{1/5}\n$$\nThe problem asks for the relative adjustment factors, which are:\n$$\n\\frac{w_{\\mathrm{adj}}}{w(\\phi)} = \\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)^{2/5}\n$$\n$$\n\\frac{d_{\\mathrm{adj}}}{d(\\phi)} = \\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)^{1/5}\n$$\nThese formulas provide the solution to be implemented. The term $\\frac{[r(\\phi)]^2}{R_x R_y}$ quantifies the imbalance in total pixels between the target isotropic resolution and the given anisotropic resolution. The exponents $2/5$ and $1/5$ arise from the relative influence of width ($w^2$) and depth ($d^1$) on the compute budget and the minimization criterion.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the adjusted width and depth multipliers for a CNN under\n    anisotropic input resolution to maintain a constant compute budget,\n    based on the principles of compound scaling.\n    \"\"\"\n    \n    # Define constants from the problem statement.\n    r0 = 224.0\n    alpha = 1.2\n    beta = 1.1\n    gamma = 1.15\n\n    # Define the test cases.\n    test_cases = [\n        # Case 1: phi = 1, Rx = 224 * 1.15, Ry = 224 * 1.15\n        (1.0, r0 * np.power(gamma, 1.0), r0 * np.power(gamma, 1.0)),\n        # Case 2: phi = 2, Rx = 224 * 1.15^2 * 3, Ry = 224 * 1.15^2 / 3\n        (2.0, r0 * np.power(gamma, 2.0) * 3.0, r0 * np.power(gamma, 2.0) / 3.0),\n        # Case 3: phi = 1, Rx = 224 * 1.15, Ry = 224 * 1.15 / 2\n        (1.0, r0 * np.power(gamma, 1.0), r0 * np.power(gamma, 1.0) / 2.0),\n        # Case 4: phi = 1, Rx = 224 * 1.15, Ry = 224 * 1.15 * 2\n        (1.0, r0 * np.power(gamma, 1.0), r0 * np.power(gamma, 1.0) * 2.0),\n        # Case 5: phi = 0, Rx = 336, Ry = 112\n        (0.0, 336.0, 112.0)\n    ]\n\n    results = []\n    for phi, Rx, Ry in test_cases:\n        # Calculate the nominal isotropic resolution for the given phi.\n        r_phi = r0 * np.power(gamma, phi)\n        \n        # Calculate the ratio of nominal squared resolution to the product of\n        # the anisotropic resolutions. This is the core scaling factor.\n        resolution_ratio = np.power(r_phi, 2) / (Rx * Ry)\n        \n        # Calculate the relative adjustment factors.\n        # The exponents 2/5 and 1/5 come from the constrained optimization.\n        # w_adj/w(phi) = (resolution_ratio)^(2/5)\n        # d_adj/d(phi) = (resolution_ratio)^(1/5)\n        width_adj_ratio = np.power(resolution_ratio, 0.4)\n        depth_adj_ratio = np.power(resolution_ratio, 0.2)\n        \n        # Round the results to 6 decimal places as required.\n        w_adj_rounded = round(width_adj_ratio, 6)\n        d_adj_rounded = round(depth_adj_ratio, 6)\n        \n        results.append([w_adj_rounded, d_adj_rounded])\n\n    # Format the final output string to exactly match the problem specification:\n    # \"[[a_1,b_1],[a_2,b_2],...]\"\n    # Using str() on a list of lists and removing spaces is a robust way to achieve this.\n    output_str = str(results).replace(\" \", \"\")\n    \n    print(output_str)\n\nsolve()\n```", "id": "3119513"}]}