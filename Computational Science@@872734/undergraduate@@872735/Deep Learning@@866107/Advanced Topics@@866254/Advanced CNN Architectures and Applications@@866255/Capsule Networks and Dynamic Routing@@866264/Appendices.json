{"hands_on_practices": [{"introduction": "The magic of dynamic routing lies in its ability to learn how parts should form a whole through a competitive process. This practice dives into the core mathematical engine of that competition—the softmax function—by having you derive its gradient from first principles [@problem_id:3104832]. Understanding this derivative is key to grasping how routing-by-agreement is implemented and optimized during training.", "problem": "In a Capsule Network with dynamic routing, for a fixed lower-level capsule indexed by $i$, the routing logits are $b_{ij}$ and the coupling coefficients are obtained by a softmax across the upper-level capsules $j \\in \\{1,\\dots,J\\}$:\n$$\nc_{ij} = \\frac{\\exp(b_{ij})}{\\sum_{m=1}^{J} \\exp(b_{im})}.\n$$\nUsing only this definition and standard rules of calculus, derive from first principles a general expression for the partial derivative $\\frac{\\partial c_{ij}}{\\partial b_{ik}}$ for arbitrary indices $j$ and $k$ in $\\{1,\\dots,J\\}$. Then, specialize to the case $J=3$ with coupling vector for capsule $i$ given by $c_{i1} = 0.6$, $c_{i2} = 0.3$, and $c_{i3} = 0.1$. Compute the value of $\\frac{\\partial c_{i2}}{\\partial b_{i1}}$ for this case. Provide the exact decimal value; no rounding is required. Finally, explain briefly how the sign and magnitude of this derivative affect learning dynamics in the routing procedure when gradients backpropagate through the softmax.", "solution": "The problem requires the derivation of the partial derivative of the softmax function, a specific numerical calculation based on this derivative, and an interpretation of the result in the context of learning dynamics in Capsule Networks.\n\nFirst, we validate the problem.\n**Step 1: Extract Givens**\n- Lower-level capsule indexed by $i$.\n- Upper-level capsules indexed by $j \\in \\{1,\\dots,J\\}$.\n- Routing logits: $b_{ij}$.\n- Coupling coefficients: $c_{ij} = \\frac{\\exp(b_{ij})}{\\sum_{m=1}^{J} \\exp(b_{im})}$.\n- Task 1: Derive the general expression for $\\frac{\\partial c_{ij}}{\\partial b_{ik}}$.\n- Task 2: For $J=3$, $c_{i1} = 0.6$, $c_{i2} = 0.3$, $c_{i3} = 0.1$, compute $\\frac{\\partial c_{i2}}{\\partial b_{i1}}$.\n- Task 3: Explain the role of the derivative's sign and magnitude in learning.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a direct application of calculus to the softmax function as used in the standard dynamic routing algorithm for Capsule Networks. It is well-posed, with all necessary information provided and no contradictions. The given coupling coefficients ($0.6$, $0.3$, $0.1$) sum to $1.0$, which is consistent with a softmax output. The problem is objective and formalizable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\nThe core of the problem is to compute the partial derivative $\\frac{\\partial c_{ij}}{\\partial b_{ik}}$. The function for the coupling coefficient $c_{ij}$ is:\n$$c_{ij} = \\frac{\\exp(b_{ij})}{\\sum_{m=1}^{J} \\exp(b_{im})}$$\nLet us denote the denominator as a sum $S_i = \\sum_{m=1}^{J} \\exp(b_{im})$. The index $i$ is fixed throughout this problem, so we can write $S = S_i$ for simplicity. The function is $c_{ij} = \\frac{\\exp(b_{ij})}{S}$. We will use the quotient rule for derivatives, which states that for a function $f(x) = \\frac{u(x)}{v(x)}$, the derivative is $f'(x) = \\frac{u'(x)v(x) - u(x)v'(x)}{[v(x)]^2}$.\n\nWe must consider two separate cases for the indices $j$ and $k$.\n\n**Case 1: $j=k$**\nIn this case, we are computing the partial derivative of $c_{ij}$ with respect to its own logit, $b_{ij}$.\n$$\\frac{\\partial c_{ij}}{\\partial b_{ij}} = \\frac{\\partial}{\\partial b_{ij}} \\left( \\frac{\\exp(b_{ij})}{S} \\right)$$\nHere, $u = \\exp(b_{ij})$ and $v = S = \\sum_{m=1}^{J} \\exp(b_{im})$. The variable of differentiation is $b_{ij}$.\nThe derivatives of $u$ and $v$ with respect to $b_{ij}$ are:\n$$\\frac{\\partial u}{\\partial b_{ij}} = \\frac{\\partial}{\\partial b_{ij}} \\exp(b_{ij}) = \\exp(b_{ij})$$\n$$\\frac{\\partial v}{\\partial b_{ij}} = \\frac{\\partial}{\\partial b_{ij}} \\left( \\sum_{m=1}^{J} \\exp(b_{im}) \\right) = \\frac{\\partial}{\\partial b_{ij}} \\exp(b_{ij}) = \\exp(b_{ij})$$\n(Note that for $m \\neq j$, $\\frac{\\partial}{\\partial b_{ij}}\\exp(b_{im})=0$.)\n\nApplying the quotient rule:\n$$\\frac{\\partial c_{ij}}{\\partial b_{ij}} = \\frac{(\\exp(b_{ij}))(S) - (\\exp(b_{ij}))(\\exp(b_{ij}))}{S^2}$$\nWe can rewrite this expression by separating the terms:\n$$\\frac{\\partial c_{ij}}{\\partial b_{ij}} = \\frac{\\exp(b_{ij})}{S} - \\left( \\frac{\\exp(b_{ij})}{S} \\right) \\left( \\frac{\\exp(b_{ij})}{S} \\right)$$\nRecognizing that $c_{ij} = \\frac{\\exp(b_{ij})}{S}$, we get:\n$$\\frac{\\partial c_{ij}}{\\partial b_{ij}} = c_{ij} - (c_{ij})^2 = c_{ij}(1 - c_{ij})$$\n\n**Case 2: $j \\neq k$**\nHere, we compute the partial derivative of $c_{ij}$ with respect to a different logit, $b_{ik}$, where $k \\neq j$.\n$$\\frac{\\partial c_{ij}}{\\partial b_{ik}} = \\frac{\\partial}{\\partial b_{ik}} \\left( \\frac{\\exp(b_{ij})}{S} \\right)$$\nHere, $u = \\exp(b_{ij})$ and $v = S = \\sum_{m=1}^{J} \\exp(b_{im})$. The variable of differentiation is $b_{ik}$.\nThe derivatives of $u$ and $v$ with respect to $b_{ik}$ are:\n$$\\frac{\\partial u}{\\partial b_{ik}} = \\frac{\\partial}{\\partial b_{ik}} \\exp(b_{ij}) = 0 \\quad (\\text{since } j \\neq k)$$\n$$\\frac{\\partial v}{\\partial b_{ik}} = \\frac{\\partial}{\\partial b_{ik}} \\left( \\sum_{m=1}^{J} \\exp(b_{im}) \\right) = \\frac{\\partial}{\\partial b_{ik}} \\exp(b_{ik}) = \\exp(b_{ik})$$\nApplying the quotient rule:\n$$\\frac{\\partial c_{ij}}{\\partial b_{ik}} = \\frac{(0)(S) - (\\exp(b_{ij}))(\\exp(b_{ik}))}{S^2}$$\n$$\\frac{\\partial c_{ij}}{\\partial b_{ik}} = - \\frac{\\exp(b_{ij})\\exp(b_{ik})}{S^2} = - \\left( \\frac{\\exp(b_{ij})}{S} \\right) \\left( \\frac{\\exp(b_{ik})}{S} \\right)$$\nRecognizing the definitions of $c_{ij}$ and $c_{ik}$, we get:\n$$\\frac{\\partial c_{ij}}{\\partial b_{ik}} = -c_{ij}c_{ik}$$\n\n**General Expression**\nWe can combine these two results into a single general expression using the Kronecker delta, $\\delta_{jk}$, which is $1$ if $j=k$ and $0$ if $j \\neq k$.\nThe general expression for the partial derivative is:\n$$\\frac{\\partial c_{ij}}{\\partial b_{ik}} = c_{ij}(\\delta_{jk} - c_{ik})$$\n\n**Numerical Calculation**\nThe problem asks for the value of $\\frac{\\partial c_{i2}}{\\partial b_{i1}}$ for the case where $J=3$ and the coupling coefficients are given as $c_{i1} = 0.6$, $c_{i2} = 0.3$, and $c_{i3} = 0.1$.\nHere, the indices are $j=2$ and $k=1$. Since $j \\neq k$, we use the result from Case 2:\n$$\\frac{\\partial c_{i2}}{\\partial b_{i1}} = -c_{i2}c_{i1}$$\nSubstituting the given numerical values:\n$$\\frac{\\partial c_{i2}}{\\partial b_{i1}} = -(0.3)(0.6) = -0.18$$\n\n**Explanation of Learning Dynamics**\nThe derivative $\\frac{\\partial c_{ij}}{\\partial b_{ik}}$ is the Jacobian of the softmax function and plays a crucial role in backpropagation. The chain rule for computing the gradient of a loss function $L$ with respect to a logit $b_{ik}$ involves this term: $\\frac{\\partial L}{\\partial b_{ik}} = \\sum_{j=1}^{J} \\frac{\\partial L}{\\partial c_{ij}}\\frac{\\partial c_{ij}}{\\partial b_{ik}}$. The sign and magnitude of $\\frac{\\partial c_{ij}}{\\partial b_{ik}}$ thus directly influence how gradients flow and how the logits are updated during training.\n\n- **Sign**: The sign of our computed derivative, $\\frac{\\partial c_{i2}}{\\partial b_{i1}} = -0.18$, is negative. This indicates a competitive relationship between the coupling coefficients. An increase in the logit $b_{i1}$ makes its corresponding output $c_{i1}$ larger, but because the outputs must sum to $1$, it forces a decrease in all other outputs, such as $c_{i2}$. During learning, if the optimization process requires increasing the coupling $c_{i1}$ (e.g., to increase agreement with parent capsule $1$), the update to logit $b_{i1}$ will be positive. This positive update to $b_{i1}$ will, via the negative derivative, contribute to decreasing the coupling $c_{i2}$. This mechanism is fundamental to the \"routing-by-agreement\" procedure, where strengthening the connection to one parent capsule inherently weakens the connections to others.\n\n- **Magnitude**: The magnitude of the derivative, $|-0.18| = 0.18$, quantifies the strength of this competitive interaction. The magnitude of the cross-derivative, $|-c_{ij}c_{ik}|$, is largest when both $c_{ij}$ and $c_{ik}$ have significant probability mass. In contrast, the \"self-derivative,\" $\\frac{\\partial c_{i1}}{\\partial b_{i1}} = c_{i1}(1-c_{i1}) = 0.6(1-0.6) = 0.24$, governs how much a logit affects its own output probability. In our case, the cross-effect magnitude ($0.18$) is comparable to the self-effect magnitude ($0.24$), indicating a strong gradient crosstalk between the updates for parent capsule $1$ and parent capsule $2$. This ensures that the dynamic routing procedure can effectively and rapidly shift routing probabilities away from incorrect parent capsules when a better candidate emerges. If the magnitude were very small (e.g., if $c_{i2}$ were near $0$), then changing $b_{i1}$ would have a negligible effect on $c_{i2}$, making the competitive adjustment less effective.", "answer": "$$\\boxed{-0.18}$$", "id": "3104832"}, {"introduction": "Capsule Networks promise to preserve spatial hierarchies, a property known as equivariance, which stands in contrast to the invariance often sought in traditional CNNs. This exercise provides a direct, hands-on comparison by tasking you with a numerical experiment to demonstrate how a capsule's pose vector rotates with the input, while a CNN-like representation remains fixed [@problem_id:3104851]. Implementing this will provide a concrete understanding of one of the most celebrated features of CapsNets.", "problem": "You are asked to implement a self-contained numerical experiment that contrasts the equivariance of Capsule Networks (Capsule Networks (CapsNets)) with the invariance of Convolutional Neural Networks (Convolutional Neural Networks (CNNs)) using a synthetic setup that mimics the effect of an affine transformation on a pose. The experiment must be conducted entirely in vector space with explicit matrices and vectors, without loading any external datasets. You will implement a single higher-level capsule with dynamic routing and a simple CNN-like baseline. The goal is to measure and compare the pose vector alignment error under a rotation transformation and to analyze how it depends on the number of dynamic routing iterations.\n\nFundamental base and definitions to use:\n- A rotation transformation in two dimensions is represented by the matrix\n$$\nT(\\theta) \\;=\\; \\begin{bmatrix}\n\\cos(\\theta)  -\\sin(\\theta)\\\\\n\\sin(\\theta)  \\cos(\\theta)\n\\end{bmatrix},\n$$\nwhich belongs to the special orthogonal group $SO(2)$ and preserves vector norms.\n- The dynamic routing procedure between capsules uses coupling coefficients computed by a softmax over routing logits and the \"squash\" nonlinearity. For a set of \"prediction\" vectors $\\{ \\mathbf{u}_i \\in \\mathbb{R}^2 \\}_{i=1}^m$ and routing logits $\\{ b_i \\in \\mathbb{R} \\}_{i=1}^m$, the coupling coefficients are\n$$\nc_i \\;=\\; \\frac{\\exp(b_i)}{\\sum_{k=1}^{m} \\exp(b_k)},\n$$\nand the pre-activation is\n$$\n\\mathbf{s} \\;=\\; \\sum_{i=1}^{m} c_i \\, \\mathbf{u}_i.\n$$\nThe squash nonlinearity maps $\\mathbf{s}$ to an output pose vector $\\mathbf{v} \\in \\mathbb{R}^2$ via\n$$\n\\mathbf{v} \\;=\\; \\frac{\\lVert \\mathbf{s} \\rVert^2}{1 + \\lVert \\mathbf{s} \\rVert^2} \\cdot \\frac{\\mathbf{s}}{\\lVert \\mathbf{s} \\rVert},\n$$\nwith the convention that if $\\lVert \\mathbf{s} \\rVert = 0$ then $\\mathbf{v} = \\mathbf{0}$.\n- The routing logits are updated by agreement for a specified number of iterations $r \\in \\mathbb{N}_0$ (nonnegative integers) using the dot product (agreement) between each prediction and the current output:\n$$\nb_i \\leftarrow b_i + \\mathbf{u}_i \\cdot \\mathbf{v}.\n$$\n- Equivariance of a representation $\\mathbf{v}$ with respect to $T(\\theta)$ means that if the input is transformed by $T(\\theta)$, then the output pose transforms by the same $T(\\theta)$. In formulas, if $\\mathbf{v}(\\theta)$ is the output pose for a transformed input, equivariance implies $\\mathbf{v}(\\theta) \\approx T(\\theta)\\,\\mathbf{v}(0)$.\n- We measure pose vector alignment error between two nonzero vectors $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^2$ as the angle between them in radians,\n$$\n\\operatorname{ang}(\\mathbf{a}, \\mathbf{b}) \\;=\\; \\arccos\\!\\left( \\frac{\\mathbf{a}^\\top \\mathbf{b}}{\\lVert \\mathbf{a} \\rVert \\,\\lVert \\mathbf{b} \\rVert} \\right).\n$$\n\nSynthetic setup:\n- Let the \"true\" canonical pose be $\\mathbf{p}_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$.\n- For any rotation angle $\\theta \\in \\mathbb{R}$ (in radians), the transformed pose is $\\mathbf{p}(\\theta) = T(\\theta)\\,\\mathbf{p}_0$.\n- There are $m = 5$ primary capsules. Each produces a $2$-dimensional \"vote\" vector that is a scaled and slightly misaligned observation of the transformed pose:\n$$\n\\mathbf{u}_i(\\theta) \\;=\\; s_i \\, R(\\varphi_i)\\, \\mathbf{p}(\\theta),\n$$\nwhere $R(\\varphi)$ is the rotation matrix $T(\\varphi)$ and the constants are\n$$\n[s_1, s_2, s_3, s_4, s_5] \\;=\\; [1.2, 0.9, 1.1, 0.8, 1.0],\n$$\n$$\n[\\varphi_1, \\varphi_2, \\varphi_3, \\varphi_4, \\varphi_5] \\;=\\; [-0.2, 0.1, 0.0, 0.15, -0.1].\n$$\n- The higher-level capsule uses identity transforms (that is, prediction-to-output transforms are the identity in $\\mathbb{R}^2$), so the $\\mathbf{u}_i(\\theta)$ directly participate in routing.\n- The Convolutional Neural Network (CNN)-like baseline discards pose direction by aggregating only magnitudes. Define its representation for angle $\\theta$ as the $2$-vector\n$$\n\\mathbf{v}_{\\mathrm{cnn}}(\\theta) \\;=\\; \\begin{bmatrix} g(\\theta) \\\\ 0 \\end{bmatrix}, \\quad \\text{where} \\quad g(\\theta) \\;=\\; \\sum_{i=1}^{m} \\lVert \\mathbf{u}_i(\\theta) \\rVert.\n$$\nThis baseline is intentionally invariant to $\\theta$ in direction because it aligns along the fixed $x$-axis.\n\nYour tasks:\n1. Implement the dynamic routing algorithm described above for a single higher-level capsule, parameterized by the number of routing iterations $r \\in \\mathbb{N}_0$. Use the initial logits $b_i = 0$ for all $i$. In each iteration, compute the coupling coefficients $c_i$ by the softmax over $\\{b_i\\}$, compute $\\mathbf{s}$ and $\\mathbf{v}$ by the formulas above, and then update $b_i \\leftarrow b_i + \\mathbf{u}_i(\\theta) \\cdot \\mathbf{v}$. The output pose $\\mathbf{v}_{\\mathrm{caps}}(\\theta; r)$ for a given $\\theta$ and $r$ is the $\\mathbf{v}$ computed in the final iteration. For the boundary case $r = 0$, define $\\mathbf{v}_{\\mathrm{caps}}(\\theta; 0)$ as the squash of the uniform average, that is, use $c_i = 1/m$ without any updates.\n2. For the CNN-like baseline, compute $\\mathbf{v}_{\\mathrm{cnn}}(\\theta)$ as defined above.\n3. For a given model that outputs $\\mathbf{v}(\\theta)$, define the equivariance alignment error at angle $\\theta$ as\n$$\n\\varepsilon(\\theta) \\;=\\; \\operatorname{ang}\\!\\left( \\mathbf{v}(\\theta), \\; T(\\theta)\\,\\mathbf{v}(0) \\right),\n$$\nmeasured in radians.\n\nTest suite:\nEvaluate the pair of errors $\\left[ \\varepsilon_{\\mathrm{caps}}(\\theta; r), \\; \\varepsilon_{\\mathrm{cnn}}(\\theta) \\right]$ on the following cases:\n- Case $1$: $\\theta = 0.0$, $r = 0$.\n- Case $2$: $\\theta = 0.5$, $r = 0$.\n- Case $3$: $\\theta = 0.5$, $r = 5$.\n- Case $4$: $\\theta = 1.0$, $r = 0$.\n- Case $5$: $\\theta = 1.0$, $r = 5$.\n- Case $6$: $\\theta = 1.0$, $r = 10$.\n\nAngle units:\n- All angles $\\theta$, $\\varphi_i$, and errors $\\varepsilon$ must be handled and reported in radians.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the six cases as a comma-separated list of six inner lists, each inner list containing two floating-point numbers rounded to exactly six digits after the decimal point, in the order specified above. The output must be enclosed in a single pair of square brackets. For example, the printed structure must look like\n$$\n[ [a_1, b_1], [a_2, b_2], \\ldots, [a_6, b_6] ],\n$$\nbut printed without any spaces after commas to match the exact requirement\n$$\n[[a_1,b_1],[a_2,b_2],\\ldots,[a_6,b_6]].\n$$", "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, objective, and internally consistent. It provides a clear and formalizable numerical experiment to contrast the properties of Capsule Networks (CapsNets) and a simplified Convolutional Neural Network (CNN) baseline with respect to affine transformations, specifically $SO(2)$ rotations. All necessary constants, definitions, and procedures are explicitly provided. We may therefore proceed with a complete solution.\n\nThe core of this problem lies in understanding and quantifying equivariance. A function $f$ is equivariant with respect to a transformation group $G$ if, for any transformation $T \\in G$, applying the transformation to the input and then passing it through the function yields the same result as passing the original input through the function and then applying the transformation to the output. In this context, we are investigating equivariance with respect to the rotation group $SO(2)$, and the condition is $\\mathbf{v}(\\theta) = T(\\theta)\\mathbf{v}(0)$, where $\\mathbf{v}(\\theta)$ is the model's output for an input rotated by $\\theta$. The deviation from this ideal is measured by the equivariance alignment error, $\\varepsilon(\\theta) = \\operatorname{ang}(\\mathbf{v}(\\theta), T(\\theta)\\mathbf{v}(0))$.\n\nWe will first analyze the theoretical behavior of both the CapsNet and CNN models under the problem's synthetic setup. Then, we will implement the numerical computation to verify this analysis.\n\n### Theoretical Analysis\n\n**1. CNN-like Baseline Model**\n\nThe CNN-like baseline model computes a representation $\\mathbf{v}_{\\mathrm{cnn}}(\\theta) = [g(\\theta), 0]^\\top$, where $g(\\theta) = \\sum_{i=1}^{m} \\lVert \\mathbf{u}_i(\\theta) \\rVert$. The prediction vectors are defined as $\\mathbf{u}_i(\\theta) = s_i R(\\varphi_i) \\mathbf{p}(\\theta)$, with $\\mathbf{p}(\\theta) = T(\\theta)\\mathbf{p}_0$. Here, $s_i$ are scalar constants, and $R(\\varphi_i)$ and $T(\\theta)$ are rotation matrices from $SO(2)$. Since rotations are orthogonal transformations, they preserve vector norms.\nThe norm of a prediction vector is:\n$$\n\\lVert \\mathbf{u}_i(\\theta) \\rVert = \\lVert s_i R(\\varphi_i) T(\\theta) \\mathbf{p}_0 \\rVert = |s_i| \\cdot \\lVert R(\\varphi_i) \\rVert \\cdot \\lVert T(\\theta) \\rVert \\cdot \\lVert \\mathbf{p}_0 \\rVert\n$$\nGiven that for any rotation matrix $A$, $\\lVert A\\mathbf{x} \\rVert = \\lVert\\mathbf{x}\\rVert$, and all $s_i  0$, this simplifies to:\n$$\n\\lVert \\mathbf{u}_i(\\theta) \\rVert = s_i \\lVert \\mathbf{p}_0 \\rVert\n$$\nThis result is independent of the rotation angle $\\theta$. Consequently, the aggregated magnitude $g(\\theta)$ is also a constant, independent of $\\theta$:\n$$\ng(\\theta) = \\sum_{i=1}^{m} s_i \\lVert \\mathbf{p}_0 \\rVert = \\left( \\sum_{i=1}^{m} s_i \\right) \\lVert \\mathbf{p}_0 \\rVert = G\n$$\nThe CNN output is therefore $\\mathbf{v}_{\\mathrm{cnn}}(\\theta) = [G, 0]^\\top$ for any $\\theta$. This model is fully invariant; its output never changes.\n\nTo compute the equivariance error $\\varepsilon_{\\mathrm{cnn}}(\\theta)$, we need $\\mathbf{v}_{\\mathrm{cnn}}(0) = [G, 0]^\\top$. The target equivariant vector is $T(\\theta)\\mathbf{v}_{\\mathrm{cnn}}(0)$:\n$$\nT(\\theta)\\mathbf{v}_{\\mathrm{cnn}}(0) = \\begin{bmatrix} \\cos(\\theta)  -\\sin(\\theta) \\\\ \\sin(\\theta)  \\cos(\\theta) \\end{bmatrix} \\begin{bmatrix} G \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} G\\cos(\\theta) \\\\ G\\sin(\\theta) \\end{bmatrix}\n$$\nThe error is the angle between the actual output $\\mathbf{v}_{\\mathrm{cnn}}(\\theta) = [G, 0]^\\top$ and the target vector.\n$$\n\\varepsilon_{\\mathrm{cnn}}(\\theta) = \\operatorname{ang}\\left( \\begin{bmatrix} G \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} G\\cos(\\theta) \\\\ G\\sin(\\theta) \\end{bmatrix} \\right) = \\arccos\\left( \\frac{[G, 0] \\cdot [G\\cos(\\theta), G\\sin(\\theta)]}{G \\cdot G} \\right) = \\arccos(\\cos(\\theta))\n$$\nFor $\\theta$ in the primary range $[0, \\pi]$, which includes all test cases, this simplifies to $\\varepsilon_{\\mathrm{cnn}}(\\theta) = \\theta$. The CNN baseline's failure to rotate results in an error equal to the rotation angle, demonstrating a complete lack of equivariance.\n\n**2. Capsule Network Model**\n\nThe CapsNet model's behavior is fundamentally different. Let's analyze the prediction vectors $\\mathbf{u}_i(\\theta)$:\n$$\n\\mathbf{u}_i(\\theta) = s_i R(\\varphi_i) \\mathbf{p}(\\theta) = s_i T(\\varphi_i) T(\\theta) \\mathbf{p}_0\n$$\nSince rotations in $SO(2)$ commute, $T(\\varphi_i)T(\\theta) = T(\\theta)T(\\varphi_i)$. Thus:\n$$\n\\mathbf{u}_i(\\theta) = s_i T(\\theta) T(\\varphi_i) \\mathbf{p}_0\n$$\nThe prediction vector at $\\theta=0$ is $\\mathbf{u}_i(0) = s_i T(\\varphi_i) \\mathbf{p}_0$. By substitution, we find a direct relationship:\n$$\n\\mathbf{u}_i(\\theta) = T(\\theta) \\mathbf{u}_i(0)\n$$\nEach individual prediction vector is perfectly equivariant with respect to the global rotation $\\theta$.\n\nNow we analyze the dynamic routing process. Let the state of the model (vectors $\\mathbf{s}, \\mathbf{v}$ and scalars $b_i, c_i$) at angle $\\theta$ after $k$ routing iterations be denoted with a superscript $(\\theta, k)$.\n\n**Base Case ($r=0$):**\nThe output is computed using uniform coupling coefficients $c_i = 1/m$. The pre-activation vector $\\mathbf{s}^{(\\theta, 0)}$ is:\n$$\n\\mathbf{s}^{(\\theta, 0)} = \\sum_{i=1}^{m} \\frac{1}{m} \\mathbf{u}_i(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} T(\\theta) \\mathbf{u}_i(0) = T(\\theta) \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\mathbf{u}_i(0) \\right) = T(\\theta) \\mathbf{s}^{(0, 0)}\n$$\nThe pre-activation vector $\\mathbf{s}$ is perfectly equivariant. Now we apply the squash function. A key property of the squash function is its interaction with rotations. Since $\\lVert T(\\theta)\\mathbf{s} \\rVert = \\lVert \\mathbf{s} \\rVert$, we have:\n$$\n\\mathbf{v}^{(\\theta, 0)} = \\operatorname{squash}(\\mathbf{s}^{(\\theta, 0)}) = \\operatorname{squash}(T(\\theta)\\mathbf{s}^{(0, 0)}) = T(\\theta)\\operatorname{squash}(\\mathbf{s}^{(0, 0)}) = T(\\theta)\\mathbf{v}^{(0, 0)}\n$$\nThis demonstrates that for $r=0$, the capsule output is perfectly equivariant. The error $\\varepsilon_{\\mathrm{caps}}(\\theta; 0)$ must be $0$.\n\n**Inductive Step ($r0$):**\nAssume that after $k-1$ iterations, the output is perfectly equivariant: $\\mathbf{v}^{(\\theta, k-1)} = T(\\theta)\\mathbf{v}^{(0, k-1)}$. The routing logits are updated as $b_i \\leftarrow b_i + \\mathbf{u}_i \\cdot \\mathbf{v}$. Let's examine the update term:\n$$\n\\mathbf{u}_i(\\theta) \\cdot \\mathbf{v}^{(\\theta, k-1)} = (T(\\theta)\\mathbf{u}_i(0)) \\cdot (T(\\theta)\\mathbf{v}^{(0, k-1)})\n$$\nThe dot product is invariant under rotations, i.e., $(Ra)\\cdot(Rb) = a \\cdot b$. Therefore:\n$$\n\\mathbf{u}_i(\\theta) \\cdot \\mathbf{v}^{(\\theta, k-1)} = \\mathbf{u}_i(0) \\cdot \\mathbf{v}^{(0, k-1)}\n$$\nThe logit update amount is independent of $\\theta$. By induction, if the initial logits are the same for all $\\theta$ (which they are, $b_i=0$), then the logits $b_i^{(\\theta, k)}$ at any iteration $k$ are independent of $\\theta$: $b_i^{(\\theta, k)} = b_i^{(0, k)}$.\nThis implies the coupling coefficients $c_i^{(\\theta, k)}$ are also independent of $\\theta$.\nThe pre-activation $\\mathbf{s}^{(\\theta, k)}$ for the next iteration is:\n$$\n\\mathbf{s}^{(\\theta, k)} = \\sum_{i=1}^{m} c_i^{(\\theta, k)} \\mathbf{u}_i(\\theta) = \\sum_{i=1}^{m} c_i^{(0, k)} T(\\theta)\\mathbf{u}_i(0) = T(\\theta) \\sum_{i=1}^{m} c_i^{(0, k)} \\mathbf{u}_i(0) = T(\\theta)\\mathbf{s}^{(0, k)}\n$$\nFollowing the same logic as the base case, applying the squash function yields $\\mathbf{v}^{(\\theta, k)} = T(\\theta)\\mathbf{v}^{(0, k)}$.\nThe induction holds: the capsule network output is perfectly equivariant for any number of routing iterations $r$ in this idealized setup. The error $\\varepsilon_{\\mathrm{caps}}(\\theta; r)$ is theoretically $0$ for all test cases. The analysis of its dependence on $r$ is that there is no dependence; the system is perfectly equivariant from the start.\n\n### Numerical Implementation\n\nWe will now implement the algorithms as described to confirm our theoretical analysis. The implementation will follow the specified procedures for both models and calculate the errors for the given test suite. We expect the CapsNet error to be $0$ (or a value on the order of machine epsilon) and the CNN error to be $\\theta$.\n\nThe overall process for each test case $(\\theta, r)$ will be:\n1.  Calculate $\\mathbf{v}_{\\mathrm{caps}}(\\theta; r)$ and $\\mathbf{v}_{\\mathrm{cnn}}(\\theta)$.\n2.  Calculate the reference vectors $\\mathbf{v}_{\\mathrm{caps}}(0; r)$ and $\\mathbf{v}_{\\mathrm{cnn}}(0)$.\n3.  Compute the target vectors $T(\\theta)\\mathbf{v}_{\\mathrm{caps}}(0; r)$ and $T(\\theta)\\mathbf{v}_{\\mathrm{cnn}}(0)$.\n4.  Compute the angles between the actual and target vectors to find $\\varepsilon_{\\mathrm{caps}}$ and $\\varepsilon_{\\mathrm{cnn}}$.\n\nAll calculations will be performed using floating-point arithmetic in Python with the `numpy` library.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a numerical experiment to contrast Capsule Network equivariance\n    with CNN invariance under 2D rotation.\n    \"\"\"\n\n    # --- Problem Constants ---\n    P0 = np.array([2.0, 1.0])\n    S_VALS = np.array([1.2, 0.9, 1.1, 0.8, 1.0])\n    PHI_VALS = np.array([-0.2, 0.1, 0.0, 0.15, -0.1])\n    M = 5\n\n    # --- Helper Functions ---\n    def rotation_matrix(theta: float) - np.ndarray:\n        \"\"\"Computes the 2D rotation matrix T(theta).\"\"\"\n        c, s = np.cos(theta), np.sin(theta)\n        return np.array([[c, -s], [s, c]])\n\n    def squash(s: np.ndarray, epsilon: float = 1e-9) - np.ndarray:\n        \"\"\"Applies the squashing non-linearity to a vector s.\"\"\"\n        s_norm_sq = np.sum(s**2)\n        s_norm = np.sqrt(s_norm_sq)\n        if s_norm  epsilon:\n            return np.zeros_like(s)\n        scale = s_norm_sq / (1.0 + s_norm_sq)\n        return scale * s / s_norm\n\n    def get_votes(theta: float) - np.ndarray:\n        \"\"\"Generates the set of prediction vectors u_i(theta).\"\"\"\n        p_theta = rotation_matrix(theta) @ P0\n        votes = np.zeros((M, 2))\n        for i in range(M):\n            # u_i(theta) = s_i * R(phi_i) * p(theta)\n            votes[i, :] = S_VALS[i] * (rotation_matrix(PHI_VALS[i]) @ p_theta)\n        return votes\n\n    def angle_between(v1: np.ndarray, v2: np.ndarray, epsilon: float = 1e-9) - float:\n        \"\"\"Calculates the angle in radians between two 2D vectors.\"\"\"\n        norm1 = np.linalg.norm(v1)\n        norm2 = np.linalg.norm(v2)\n        if norm1  epsilon or norm2  epsilon:\n            return 0.0\n        \n        dot_product = np.dot(v1, v2)\n        cos_angle = dot_product / (norm1 * norm2)\n        # Clip to handle potential floating point inaccuracies\n        cos_angle = np.clip(cos_angle, -1.0, 1.0)\n        return np.arccos(cos_angle)\n\n    # --- Model Implementations ---\n    def capsule_model(theta: float, r: int) - np.ndarray:\n        \"\"\"\n        Computes the output pose vector v_caps(theta; r) using dynamic routing.\n        \"\"\"\n        votes = get_votes(theta)\n        \n        if r == 0:\n            # Uniform averaging, no routing iterations\n            s = np.mean(votes, axis=0)\n            return squash(s)\n\n        # Dynamic routing for r  0\n        b = np.zeros(M)\n        v = np.zeros(2) # v is initialized implicitly in loop\n        for _ in range(r):\n            # Softmax to get coupling coefficients\n            c = np.exp(b) / np.sum(np.exp(b))\n            # Weighted sum of votes\n            s = np.sum(c[:, np.newaxis] * votes, axis=0)\n            # Squash to get output pose vector\n            v = squash(s)\n            # Update logits by agreement\n            agreement = np.dot(votes, v) # shape (M,)\n            b += agreement\n            \n        return v\n\n    def cnn_model(theta: float) - np.ndarray:\n        \"\"\"Computes the output representation v_cnn(theta).\"\"\"\n        votes = get_votes(theta)\n        g_theta = np.sum(np.linalg.norm(votes, axis=1))\n        return np.array([g_theta, 0.0])\n\n    # --- Test Suite ---\n    test_cases = [\n        # (theta, r)\n        (0.0, 0),  # Case 1\n        (0.5, 0),  # Case 2\n        (0.5, 5),  # Case 3\n        (1.0, 0),  # Case 4\n        (1.0, 5),  # Case 5\n        (1.0, 10), # Case 6\n    ]\n\n    results = []\n    for theta, r in test_cases:\n        # 1. Calculate CapsNet error\n        v_caps_theta = capsule_model(theta, r)\n        v_caps_0 = capsule_model(0.0, r)\n        target_caps = rotation_matrix(theta) @ v_caps_0\n        eps_caps = angle_between(v_caps_theta, target_caps)\n\n        # 2. Calculate CNN error\n        v_cnn_theta = cnn_model(theta)\n        v_cnn_0 = cnn_model(0.0)\n        target_cnn = rotation_matrix(theta) @ v_cnn_0\n        eps_cnn = angle_between(v_cnn_theta, target_cnn)\n\n        results.append(f\"[{eps_caps:.6f},{eps_cnn:.6f}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3104851"}, {"introduction": "While powerful, the routing-by-agreement algorithm can be confounded by ambiguity, especially in highly symmetric scenarios where parts could plausibly belong to multiple wholes. This practice challenges you to first construct such a failure case, demonstrating how routing can fail to make a decisive choice [@problem_id:3104796]. You will then explore how introducing simple architectural priors can break this symmetry and restore correct routing, offering a crucial lesson in algorithmic robustness.", "problem": "Consider a simplified vector-based Capsule Network (CapsNet) routing scenario with two layers: a set of primary capsules (parts) indexed by $i \\in \\{0,1\\}$ and a set of higher-level capsules (wholes) indexed by $j \\in \\{0,1\\}$. Each primary capsule $i$ emits a prediction vector $u_{\\hat{i}j} \\in \\mathbb{R}^d$ for each higher-level capsule $j$. The pose dimensionality is $d = 2$. The routing algorithm to be implemented is the routing-by-agreement scheme defined by the following base components.\n\n- Let $b_{ij}$ be the routing logits. Initialize $b_{ij}$ as needed by any prior you include. At each routing iteration, compute the coupling coefficients $c_{ij}$ by applying the softmax across $j$ for each fixed $i$, namely $c_{ij} = \\exp(b_{ij}) \\big/ \\sum_{j'} \\exp(b_{ij'})$.\n- Compute the pre-activation of each higher-level capsule $j$ as $s_j = \\sum_i c_{ij} u_{\\hat{i}j}$.\n- Apply the standard squash nonlinearity for vector capsules to produce $v_j \\in \\mathbb{R}^d$ via\n$$\nv_j = \\frac{\\lVert s_j \\rVert^2}{1 + \\lVert s_j \\rVert^2} \\cdot \\frac{s_j}{\\lVert s_j \\rVert},\n$$\nwith the understanding that a numerically stable implementation must handle the case $\\lVert s_j \\rVert = 0$.\n- Update the routing logits by agreement: $b_{ij} \\leftarrow b_{ij} + u_{\\hat{i}j} \\cdot v_j$ where $\\cdot$ denotes the Euclidean inner product.\n\nYou will investigate symmetric ambiguity in routing, in which parts can be assigned to incorrect wholes due to symmetry, and then add symmetry-breaking priors to improve routing.\n\nYour task is to implement the above routing algorithm for a fixed number of iterations $r = 3$ and evaluate the sum of coupling mass directed to the ground-truth whole. For a given ground-truth index $j^\\star \\in \\{0,1\\}$, define the evaluation metric\n$$\n\\mathcal{S} = \\sum_{i \\in \\{0,1\\}} c_{i j^\\star}.\n$$\n\nYou must implement two symmetry-breaking priors:\n\n- Parent logit bias prior: a parent-level bias vector $\\beta = (\\beta_0, \\beta_1)$ that is added to the initial logits $b_{ij}$ identically for both parts $i$ (that is, $b_{ij} \\leftarrow b_{ij} + \\beta_j$ before the first softmax).\n- Vote perturbation prior: a small vector $\\Delta = (\\delta_x, \\delta_y)$ that perturbs prediction vectors so that predictions toward the ground-truth whole are shifted as $u_{\\hat{i} j^\\star} \\leftarrow u_{\\hat{i} j^\\star} + \\Delta$, while predictions toward the other whole are shifted oppositely as $u_{\\hat{i} j} \\leftarrow u_{\\hat{i} j} - \\Delta$ for $j \\neq j^\\star$.\n\nImplement the routing and evaluate the metric $\\mathcal{S}$ for each of the following test cases. In all cases use $n=2$ parts, $m=2$ wholes, $d=2$, and $r=3$ iterations. The ground-truth whole is $j^\\star = 0$.\n\n- Test case A (perfect symmetry, no prior):\n  - Predictions: $u_{\\hat{00}} = (1, 0)$, $u_{\\hat{10}} = (1, 0)$, $u_{\\hat{01}} = (0, 1)$, $u_{\\hat{11}} = (0, 1)$.\n  - Priors: $\\beta = (0, 0)$, $\\Delta = (0, 0)$.\n\n- Test case B (perfect symmetry, logit bias prior):\n  - Predictions: as in Test case A.\n  - Priors: $\\beta = (0.05, -0.05)$, $\\Delta = (0, 0)$.\n\n- Test case C (conflicting votes, no prior):\n  - Predictions: $u_{\\hat{00}} = (1, 0)$, $u_{\\hat{10}} = (0, 1)$, $u_{\\hat{01}} = (0, 1)$, $u_{\\hat{11}} = (1, 0)$.\n  - Priors: $\\beta = (0, 0)$, $\\Delta = (0, 0)$.\n\n- Test case D (conflicting votes, logit bias prior):\n  - Predictions: as in Test case C.\n  - Priors: $\\beta = (0.2, -0.2)$, $\\Delta = (0, 0)$.\n\n- Test case E (perfect symmetry, vote perturbation prior):\n  - Predictions: as in Test case A.\n  - Priors: $\\beta = (0, 0)$, $\\Delta = (0.05, 0.00)$.\n\nYour program must compute the metric $\\mathcal{S}$ for each test case in the order A, B, C, D, E, each as a real number. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each real number rounded to exactly $6$ decimal places (e.g., $[1.000000,1.937500,1.000000,1.750000,1.875000]$). No other output should be produced.", "solution": "The user has provided a problem that requires the implementation and evaluation of a simplified dynamic routing algorithm, a core component of Capsule Networks (CapsNets). The task is to compute a specific metric, $\\mathcal{S}$, under five different scenarios designed to test the algorithm's behavior in the presence of symmetry and the effect of symmetry-breaking priors.\n\nFirst, the validity of the problem statement is confirmed. The problem is scientifically grounded in the principles of deep learning, specifically the routing-by-agreement mechanism detailed in the original CapsNet literature. All provided parameters, equations, and conditions are self-contained, mathematically consistent, and well-defined, constituting a well-posed problem. There are no factual errors, ambiguities, or subjective elements.\n\nThe solution involves implementing the iterative routing algorithm as specified. The state of the system is stored in several matrices. The prediction vectors from primary capsules $i$ to higher-level capsules $j$ are given by $u_{\\hat{i}j}$. These are stored in a tensor of size $n \\times m \\times d$, where $n=2$ is the number of primary capsules, $m=2$ is the number of higher-level capsules, and $d=2$ is the vector dimensionality. The routing logits, $b_{ij}$, which dynamically adjust to control the routing, are stored in an $n \\times m$ matrix.\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization**:\n    The routing logits $b_{ij}$ are initialized to $0$. The problem defines two types of priors that modify this initial state.\n    - **Vote Perturbation Prior**: Before the routing process begins, the prediction vectors $u_{\\hat{i}j}$ are perturbed. For the ground-truth capsule $j^\\star=0$, prediction vectors are shifted by a small vector $\\Delta$, i.e., $u_{\\hat{i}0} \\leftarrow u_{\\hat{i}0} + \\Delta$. For the other capsule ($j=1$), they are shifted by $-\\Delta$, i.e., $u_{\\hat{i}1} \\leftarrow u_{\\hat{i}1} - \\Delta$. This is applied to all primary capsules $i \\in \\{0,1\\}$.\n    - **Parent Logit Bias Prior**: A bias vector $\\beta = (\\beta_0, \\beta_1)$ is added to the initial logits. For each primary capsule $i$, the logit for higher-level capsule $j$ is updated as $b_{ij} \\leftarrow b_{ij} + \\beta_j$. This operation is performed before the first iteration of the routing loop.\n\n2.  **Iterative Routing**: The core of the algorithm is a loop that runs for a fixed number of iterations, $r = 3$. Each iteration consists of four steps:\n    a.  **Coupling Coefficients ($c_{ij}$)**: The routing logits $b_{ij}$ are converted into coupling coefficients $c_{ij}$ using the softmax function. The softmax is applied for each primary capsule $i$ across all higher-level capsules $j$:\n        $$c_{ij} = \\frac{\\exp(b_{ij})}{\\sum_{j' \\in \\{0,1\\}} \\exp(b_{ij'})}$$\n        The value $c_{ij}$ represents the probability that primary capsule $i$ should be routed to higher-level capsule $j$.\n\n    b.  **Pre-activation ($s_j$)**: The pre-activation vector $s_j$ for each higher-level capsule $j$ is calculated as a weighted sum of the prediction vectors from all primary capsules. The weights are the coupling coefficients $c_{ij}$:\n        $$s_j = \\sum_{i \\in \\{0,1\\}} c_{ij} u_{\\hat{i}j}$$\n\n    c.  **Activation ($v_j$)**: The pre-activation vector $s_j$ is passed through a non-linear \"squash\" function to produce the final output vector $v_j$ for the higher-level capsule. The squash function scales the vector's magnitude to be between $0$ and $1$, preserving its direction.\n        $$v_j = \\frac{\\lVert s_j \\rVert^2}{1 + \\lVert s_j \\rVert^2} \\frac{s_j}{\\lVert s_j \\rVert}$$\n        A numerically stable implementation is essential, particularly for the case where $\\lVert s_j \\rVert = 0$. In this scenario, $v_j$ must be the zero vector. This is handled by adding a small epsilon to the denominator term $\\lVert s_j \\rVert$ before division.\n\n    d.  **Logit Update ($b_{ij}$)**: The routing logits are updated based on the agreement between the prediction vectors $u_{\\hat{i}j}$ and the resulting higher-level capsule activations $v_j$. The agreement is measured by the Euclidean inner product (dot product).\n        $$b_{ij} \\leftarrow b_{ij} + u_{\\hat{i}j} \\cdot v_j$$\n        This update implements the \"routing-by-agreement\" principle: if a primary capsule's prediction aligns well with the aggregated output of a higher-level capsule, the logit connecting them is increased, reinforcing that routing path in the next iteration.\n\n3.  **Final Evaluation**:\n    After $r=3$ iterations, the final routing logits $b_{ij}$ are obtained. From these, a final set of coupling coefficients $c_{ij}$ is computed using the same softmax function as in step 2a. The evaluation metric $\\mathcal{S}$ is the sum of the coupling coefficients directed to the ground-truth capsule $j^\\star=0$ from all primary capsules:\n    $$\\mathcal{S} = \\sum_{i \\in \\{0,1\\}} c_{i, j^\\star} = c_{00} + c_{10}$$\n    This metric quantifies the total \"routing mass\" assigned to the correct whole, with a maximum possible value of $2$ (if both primary capsules route fully to $j^\\star=0$) and a minimum of $0$. A value of $1$ indicates that, on average, the routing is perfectly ambiguous.\n\nThe implementation will process each of the five test cases by setting the appropriate prediction vectors and prior parameters and executing the described algorithm to calculate the metric $\\mathcal{S}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import softmax\n\ndef squash(s, epsilon=1e-9):\n    \"\"\"\n    Applies the squash nonlinearity to a set of vectors.\n    s: A NumPy array of shape (m, d) representing pre-activation vectors.\n    Returns: A NumPy array of shape (m, d) representing activated vectors.\n    \"\"\"\n    # s_norm_sq has shape (m, 1) to enable broadcasting\n    s_norm_sq = np.sum(s**2, axis=-1, keepdims=True)\n    # scale factor also has shape (m, 1)\n    scale = s_norm_sq / (1.0 + s_norm_sq)\n    # s_norm has shape (m, 1)\n    s_norm = np.sqrt(s_norm_sq)\n    # unit_s is s normalized, with protection against division by zero\n    unit_s = s / (s_norm + epsilon)\n    # The result v is the scaled unit vector\n    v = scale * unit_s\n    return v\n\ndef run_routing(u_in, beta, delta, j_star, r, n, m, d):\n    \"\"\"\n    Executes the dynamic routing algorithm for a given configuration.\n    \n    Args:\n        u_in (np.ndarray): Prediction vectors, shape (n, m, d).\n        beta (np.ndarray): Parent logit bias prior, shape (m,).\n        delta (np.ndarray): Vote perturbation prior, shape (d,).\n        j_star (int): Ground-truth whole index.\n        r (int): Number of routing iterations.\n        n (int): Number of primary capsules.\n        m (int): Number of higher-level capsules.\n        d (int): Dimensionality of pose vectors.\n\n    Returns:\n        float: The evaluation metric S.\n    \"\"\"\n    # Make a copy to avoid modifying the global test case data\n    u = u_in.copy()\n    \n    # 1. Apply vote perturbation prior\n    if np.any(delta):\n        other_j = 1 - j_star\n        u[:, j_star, :] += delta\n        u[:, other_j, :] -= delta\n\n    # 2. Initialize routing logits b_ij\n    b = np.zeros((n, m))\n    \n    # 3. Apply parent logit bias prior\n    if np.any(beta):\n        b += beta # Broadcasting adds beta to each row of b\n\n    # 4. Routing loop for r iterations\n    for _ in range(r):\n        # a. Compute coupling coefficients c_ij by softmax over logits\n        c = softmax(b, axis=1)\n        \n        # b. Compute pre-activations s_j as a weighted sum of predictions\n        s = np.einsum('ij,ijd-jd', c, u)\n        \n        # c. Compute activations v_j using the squash function\n        v = squash(s)\n        \n        # d. Update logits b_ij based on agreement (dot product)\n        agreement = np.einsum('ijd,jd-ij', u, v)\n        b += agreement\n        \n    # 5. Compute final coupling coefficients after all iterations\n    final_c = softmax(b, axis=1)\n    \n    # 6. Compute the metric S: sum of coupling mass to the ground-truth whole\n    metric_s = np.sum(final_c[:, j_star])\n    \n    return metric_s\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the routing algorithm for each, printing the results.\n    \"\"\"\n    # Define constants for all test cases\n    n, m, d = 2, 2, 2\n    r = 3\n    j_star = 0\n    \n    # Define prediction vectors for the test cases\n    u_A = np.array([[[1.0, 0.0], [0.0, 1.0]], \n                    [[1.0, 0.0], [0.0, 1.0]]])\n    \n    u_C = np.array([[[1.0, 0.0], [0.0, 1.0]], \n                    [[0.0, 1.0], [1.0, 0.0]]])\n\n    # Define the 5 test cases from the problem statement\n    test_cases = [\n        # Case A: Perfect symmetry, no prior\n        {'u': u_A, 'beta': np.array([0.0, 0.0]), 'delta': np.array([0.0, 0.0])},\n        # Case B: Perfect symmetry, logit bias prior\n        {'u': u_A, 'beta': np.array([0.05, -0.05]), 'delta': np.array([0.0, 0.0])},\n        # Case C: Conflicting votes, no prior\n        {'u': u_C, 'beta': np.array([0.0, 0.0]), 'delta': np.array([0.0, 0.0])},\n        # Case D: Conflicting votes, logit bias prior\n        {'u': u_C, 'beta': np.array([0.2, -0.2]), 'delta': np.array([0.0, 0.0])},\n        # Case E: Perfect symmetry, vote perturbation prior\n        {'u': u_A, 'beta': np.array([0.0, 0.0]), 'delta': np.array([0.05, 0.0])},\n    ]\n\n    results = []\n    for case in test_cases:\n        s_metric = run_routing(\n            u_in=case['u'],\n            beta=case['beta'],\n            delta=case['delta'],\n            j_star=j_star,\n            r=r, n=n, m=m, d=d\n        )\n        results.append(s_metric)\n\n    # Format the output as a comma-separated list with 6 decimal places, enclosed in brackets.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3104796"}]}