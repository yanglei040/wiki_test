{"hands_on_practices": [{"introduction": "The Temporal-Difference (TD) error is the fundamental signal that drives learning in a Deep Q-Network. To truly grasp how an agent learns, we must first understand the dynamics of this error. This exercise [@problem_id:3113150] provides a focused analytical workout, stripping away environmental complexity to reveal how the statistical properties of the TD error evolve from the initial, ignorant state of the agent to its final, converged state.", "problem": "You are to analyze a simplified off-policy Deep Q-Network (DQN) learning setup with Experience Replay (ER) and a replay buffer warm-up period of length $T_{\\text{burn}}$. The environment is a one-state Markov Decision Process (MDP) with two available actions. Rewards are stochastic and action-dependent, and the next state is always the same state. The agent uses an $\\epsilon$-greedy behavior policy with respect to its current action-value function. Your analysis must start from fundamental definitions of Temporal Difference (TD) learning and the Bellman optimality equation, and must avoid using any shortcut formulas not derived from those bases.\n\nAssumptions and definitions:\n- There is a single state $s$, and two actions $a_1$ and $a_2$. If the agent takes action $a_i$, it receives a reward $R_i$ with conditional distribution $R_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2)$, where $\\mu_i \\in \\mathbb{R}$ and $\\sigma \\ge 0$ are known constants, and transitions deterministically back to $s$.\n- The discount factor is $\\gamma \\in [0,1)$.\n- The replay buffer collects transitions during a warm-up period of length $T_{\\text{burn}}$, during which no parameter updates are performed.\n- The action-value function is initialized at $Q_0(s,a_1) = 0$ and $Q_0(s,a_2) = 0$.\n- The behavior policy is $\\epsilon$-greedy: with probability $1-\\epsilon$ it selects a greedy action (breaking ties uniformly at random), and with probability $\\epsilon$ it selects each action uniformly at random.\n- The Temporal Difference (TD) error at time $t$ is $\\delta_t = r_t + \\gamma \\max_{a'} Q(s, a') - Q(s, a_t)$.\n- Idealized post-burn-in learning assumption: immediately after the burn-in period, the agent updates $Q$ via ER sufficiently to reach the fixed point of the Bellman optimality equation for the one-state MDP. That is, it reaches $Q^*$ satisfying $Q^*(a) = \\mathbb{E}[R_a] + \\gamma \\max_{a'} Q^*(a')$.\n\nTasks:\n1. Using only the above assumptions and the definitions of TD error and the Bellman optimality equation, derive the distribution of the TD error before any learning occurs, that is, while $Q(s,a_1) = Q(s,a_2) = 0$. Specifically, derive closed-form expressions for the expected value and the variance of $\\delta$ aggregated over the behavior policy under $\\epsilon$-greedy exploration while both actions are tied at initialization.\n2. Using the same bases, derive the distribution of the TD error after convergence to $Q^*$ under the idealized post-burn-in learning assumption. Specifically, derive closed-form expressions for the expected value and the variance of $\\delta$ aggregated over the $\\epsilon$-greedy behavior policy at convergence.\n3. Define a measure of early learning speed as the expected cumulative mean squared TD error over the first $N$ environment steps, denoted $C(N, T_{\\text{burn}})$. Under the idealized post-burn-in convergence assumption, derive a closed-form expression for $C(N, T_{\\text{burn}})$ in terms of the pre-convergence mean squared TD error and the post-convergence mean squared TD error. Your expression must handle the boundary case $T_{\\text{burn}} \\ge N$.\n4. Implement a program that, for each test case in the test suite below, computes and returns the tuple of the following five quantities in order:\n   - The expected value of $\\delta$ before learning, denoted $m_{\\text{before}}$.\n   - The variance of $\\delta$ before learning, denoted $v_{\\text{before}}$.\n   - The expected value of $\\delta$ after convergence, denoted $m_{\\text{after}}$.\n   - The variance of $\\delta$ after convergence, denoted $v_{\\text{after}}$.\n   - The expected cumulative mean squared TD error over the first $N$ steps, $C(N, T_{\\text{burn}})$.\n   Your implementation should not simulate; it must compute these values from your derived formulas.\n\nTest suite (each item is a tuple $(\\mu_1, \\mu_2, \\sigma, \\gamma, \\epsilon, T_{\\text{burn}}, N)$):\n- Case A: $(1.0, 0.2, 1.0, 0.9, 0.1, 20, 100)$\n- Case B: $(0.5, 0.5, 0.5, 0.0, 0.5, 0, 50)$\n- Case C: $(2.0, -1.0, 0.0, 0.8, 0.2, 10, 10)$\n- Case D: $(-0.3, -0.1, 0.7, 0.95, 0.05, 100, 30)$\n- Case E: $(0.0, 1.0, 0.3, 0.9, 0.3, 5, 5)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each per-case result is itself a comma-separated list in the same style. For example: \"[[m_b,v_b,m_a,v_a,C],[...],...]\".\n- All outputs must be real numbers; do not include any units or percentage signs and do not print any additional text beyond the single required line.", "solution": "The problem statement is evaluated as scientifically grounded, well-posed, objective, and self-contained. All necessary parameters and definitions for a rigorous analysis are provided. The idealization of instantaneous convergence post-burn-in is a valid theoretical simplification that makes the problem analytically tractable without violating fundamental principles of reinforcement learning. Therefore, the problem is valid.\n\nThe solution proceeds by deriving the requested statistical quantities from first principles for the two specified phases of learning (pre-learning and post-convergence) and then combining them to find the total expected error.\n\n**Task 1: TD Error Distribution Before Learning**\n\nBefore any learning updates, the action-value function is initialized to zero for both actions: $Q_0(s, a_1) = 0$ and $Q_0(s, a_2) = 0$.\n\nThe Temporal Difference (TD) error at a given time step $t$ is defined as $\\delta_t = r_t + \\gamma \\max_{a'} Q(s, a') - Q(s, a_t)$.\nSubstituting the initial Q-values, we get:\n$$ \\delta_t = r_t + \\gamma \\max(0, 0) - Q(s, a_t) = r_t - 0 = r_t $$\nThus, before learning, the TD error is simply the received reward.\n\nThe behavior policy is $\\epsilon$-greedy. At initialization, $Q(s, a_1) = Q(s, a_2)$, so the actions are tied. The policy dictates that ties are broken uniformly at random.\n- With probability $1-\\epsilon$, the agent acts greedily, choosing $a_1$ or $a_2$ with probability $0.5$ each.\n- With probability $\\epsilon$, the agent explores, choosing $a_1$ or $a_2$ with probability $0.5$ each.\nTherefore, the probability of selecting action $a_i$ (for $i \\in \\{1, 2\\}$) is:\n$$ P(A_t = a_i) = (1-\\epsilon) \\times 0.5 + \\epsilon \\times 0.5 = 0.5 $$\nThe distribution of the TD error $\\delta$ is a mixture of the two reward distributions, $R_1 \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ and $R_2 \\sim \\mathcal{N}(\\mu_2, \\sigma^2)$, with equal mixing probability $0.5$.\n\nThe expected value of the TD error, $m_{\\text{before}} = \\mathbb{E}[\\delta]$, is found using the law of total expectation:\n$$ \\mathbb{E}[\\delta] = \\sum_{i=1}^{2} P(A_t=a_i) \\mathbb{E}[\\delta | A_t=a_i] = \\sum_{i=1}^{2} P(A_t=a_i) \\mathbb{E}[R_i] $$\n$$ m_{\\text{before}} = 0.5 \\cdot \\mu_1 + 0.5 \\cdot \\mu_2 = \\frac{\\mu_1 + \\mu_2}{2} $$\nThe variance of the TD error, $v_{\\text{before}} = \\text{Var}(\\delta)$, is found using the law of total variance: $\\text{Var}(\\delta) = \\mathbb{E}[\\text{Var}(\\delta | A_t)] + \\text{Var}(\\mathbb{E}[\\delta | A_t])$.\nThe conditional variance is $\\text{Var}(\\delta | A_t=a_i) = \\text{Var}(R_i) = \\sigma^2$.\n$$ \\mathbb{E}[\\text{Var}(\\delta | A_t)] = \\sum_{i=1}^{2} P(A_t=a_i) \\text{Var}(\\delta | A_t=a_i) = 0.5 \\cdot \\sigma^2 + 0.5 \\cdot \\sigma^2 = \\sigma^2 $$\nThe conditional expectation is $\\mathbb{E}[\\delta | A_t=a_i] = \\mu_i$. The random variable $\\mathbb{E}[\\delta | A_t]$ takes value $\\mu_1$ with probability $0.5$ and $\\mu_2$ with probability $0.5$. The variance of this variable is:\n$$ \\text{Var}(\\mathbb{E}[\\delta | A_t]) = \\mathbb{E}[(\\mathbb{E}[\\delta | A_t])^2] - (\\mathbb{E}[\\mathbb{E}[\\delta | A_t]])^2 $$\n$$ \\text{Var}(\\mathbb{E}[\\delta | A_t]) = (0.5 \\cdot \\mu_1^2 + 0.5 \\cdot \\mu_2^2) - \\left(\\frac{\\mu_1 + \\mu_2}{2}\\right)^2 = \\frac{2\\mu_1^2 + 2\\mu_2^2 - (\\mu_1^2 + 2\\mu_1\\mu_2 + \\mu_2^2)}{4} = \\frac{\\mu_1^2 - 2\\mu_1\\mu_2 + \\mu_2^2}{4} = \\left(\\frac{\\mu_1 - \\mu_2}{2}\\right)^2 $$\nCombining the terms, the total variance is:\n$$ v_{\\text{before}} = \\sigma^2 + \\left(\\frac{\\mu_1 - \\mu_2}{2}\\right)^2 $$\n\n**Task 2: TD Error Distribution After Convergence**\n\nAfter convergence, the action-value function $Q^*$ satisfies the Bellman optimality equation for this single-state MDP: $Q^*(a) = \\mathbb{E}[R_a] + \\gamma \\max_{a'} Q^*(a')$.\nLet $Q^*_i = Q^*(s, a_i)$ and $V^* = \\max_{a'} Q^*(s, a') = \\max(Q^*_1, Q^*_2)$. The equations are:\n$$ Q^*_1 = \\mu_1 + \\gamma V^* $$\n$$ Q^*_2 = \\mu_2 + \\gamma V^* $$\nSubstituting these into the definition of $V^*$:\n$$ V^* = \\max(\\mu_1 + \\gamma V^*, \\mu_2 + \\gamma V^*) = \\max(\\mu_1, \\mu_2) + \\gamma V^* $$\nSolving for $V^*$ for $\\gamma \\in [0,1)$:\n$$ V^*(1-\\gamma) = \\max(\\mu_1, \\mu_2) \\implies V^* = \\frac{\\max(\\mu_1, \\mu_2)}{1-\\gamma} $$\nNow consider the TD error $\\delta_t = r_t + \\gamma \\max_{a'} Q^*(s, a') - Q^*(s, a_t)$. If action $a_i$ is taken, $a_t=a_i$ and $r_t$ is a sample from $R_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2)$. The TD error is:\n$$ \\delta_t(a_i) = R_i + \\gamma V^* - Q^*_i = R_i + \\gamma V^* - (\\mu_i + \\gamma V^*) = R_i - \\mu_i $$\nThis means the TD error, conditioned on action $a_i$, is a random variable with distribution $\\mathcal{N}(0, \\sigma^2)$, since $\\mathbb{E}[R_i - \\mu_i] = \\mu_i - \\mu_i = 0$ and $\\text{Var}(R_i - \\mu_i) = \\text{Var}(R_i) = \\sigma^2$.\n\nThe expected value of the TD error, $m_{\\text{after}} = \\mathbb{E}[\\delta]$, is again found using the law of total expectation. Since the conditional expectation is $0$ for any action, the total expectation is also $0$:\n$$ m_{\\text{after}} = \\mathbb{E}[\\delta] = \\sum_{i=1}^{2} P(A_t=a_i) \\mathbb{E}[\\delta | A_t=a_i] = \\sum_{i=1}^{2} P(A_t=a_i) \\cdot 0 = 0 $$\nThe variance of the TD error, $v_{\\text{after}} = \\text{Var}(\\delta)$, is found using the law of total variance. The conditional variance is $\\text{Var}(\\delta | A_t=a_i) = \\sigma^2$, and the conditional expectation is $\\mathbb{E}[\\delta|A_t=a_i]=0$.\n$$ \\mathbb{E}[\\text{Var}(\\delta | A_t)] = \\sum_{i=1}^{2} P(A_t=a_i) \\cdot \\sigma^2 = \\sigma^2 \\sum_{i=1}^{2} P(A_t=a_i) = \\sigma^2 $$\n$$ \\text{Var}(\\mathbb{E}[\\delta | A_t]) = \\text{Var}(0) = 0 $$\nThus, the total variance after convergence is:\n$$ v_{\\text{after}} = \\sigma^2 + 0 = \\sigma^2 $$\nNote that these results are independent of the action-selection probabilities and thus independent of $\\epsilon$ and which action is optimal.\n\n**Task 3: Expected Cumulative Mean Squared TD Error**\n\nThe expected cumulative mean squared TD error, $C(N, T_{\\text{burn}})$, is interpreted as the sum of the expected squared TD errors over $N$ steps: $C(N, T_{\\text{burn}}) = \\sum_{t=1}^{N} \\mathbb{E}[\\delta_t^2]$.\nThe mean squared error (MSE) of a random variable $X$ is $\\mathbb{E}[X^2] = \\text{Var}(X) + (\\mathbb{E}[X])^2$.\n\nFor the pre-learning phase (from $t=1$ to $t=T_{\\text{burn}}$), the MSE is:\n$$ \\text{MSE}_{\\text{before}} = v_{\\text{before}} + m_{\\text{before}}^2 = \\left(\\sigma^2 + \\left(\\frac{\\mu_1 - \\mu_2}{2}\\right)^2\\right) + \\left(\\frac{\\mu_1 + \\mu_2}{2}\\right)^2 $$\n$$ \\text{MSE}_{\\text{before}} = \\sigma^2 + \\frac{\\mu_1^2 - 2\\mu_1\\mu_2 + \\mu_2^2}{4} + \\frac{\\mu_1^2 + 2\\mu_1\\mu_2 + \\mu_2^2}{4} = \\sigma^2 + \\frac{2\\mu_1^2 + 2\\mu_2^2}{4} = \\sigma^2 + \\frac{\\mu_1^2 + \\mu_2^2}{2} $$\nFor the post-convergence phase (from $t = T_{\\text{burn}}+1$ to $t=N$), the MSE is:\n$$ \\text{MSE}_{\\text{after}} = v_{\\text{after}} + m_{\\text{after}}^2 = \\sigma^2 + 0^2 = \\sigma^2 $$\nThe total duration is $N$ steps. The burn-in period lasts for $T_{\\text{burn}}$ steps.\n- If $T_{\\text{burn}} \\ge N$, all $N$ steps occur in the pre-learning phase.\n- If $T_{\\text{burn}} < N$, the first $T_{\\text{burn}}$ steps are pre-learning, and the remaining $N - T_{\\text{burn}}$ steps are post-convergence.\nThis can be expressed as a single formula:\n$$ C(N, T_{\\text{burn}}) = \\min(N, T_{\\text{burn}}) \\cdot \\text{MSE}_{\\text{before}} + \\max(0, N - T_{\\text{burn}}) \\cdot \\text{MSE}_{\\text{after}} $$\nSubstituting the expressions for the MSEs:\n$$ C(N, T_{\\text{burn}}) = \\min(N, T_{\\text{burn}}) \\left(\\sigma^2 + \\frac{\\mu_1^2 + \\mu_2^2}{2}\\right) + \\max(0, N - T_{\\text{burn}}) \\cdot \\sigma^2 $$\n\n**Task 4: Implementation**\n\nThe derived formulae are implemented in the following Python program to compute the five quantities for each test case. The parameters $\\gamma$ and $\\epsilon$ from the input tuples are not required for the final calculations, as established in the derivations.\n- $m_{\\text{before}} = (\\mu_1 + \\mu_2)/2$\n- $v_{\\text{before}} = \\sigma^2 + ((\\mu_1 - \\mu_2)/2)^2$\n- $m_{\\text{after}} = 0$\n- $v_{\\text{after}} = \\sigma^2$\n- $C(N, T_{\\text{burn}})$ as derived above.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes statistical properties of the TD error for a simplified DQN setup.\n    \"\"\"\n    # Test suite: each item is a tuple (mu1, mu2, sigma, gamma, epsilon, T_burn, N)\n    test_cases = [\n        (1.0, 0.2, 1.0, 0.9, 0.1, 20, 100),\n        (0.5, 0.5, 0.5, 0.0, 0.5, 0, 50),\n        (2.0, -1.0, 0.0, 0.8, 0.2, 10, 10),\n        (-0.3, -0.1, 0.7, 0.95, 0.05, 100, 30),\n        (0.0, 1.0, 0.3, 0.9, 0.3, 5, 5),\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        mu1, mu2, sigma, gamma, epsilon, T_burn, N = case\n\n        # Task 1: Expected value and variance of TD error before learning\n        # m_before = (mu1 + mu2) / 2\n        m_before = 0.5 * (mu1 + mu2)\n        # v_before = sigma^2 + ((mu1 - mu2) / 2)^2\n        v_before = sigma**2 + (0.5 * (mu1 - mu2))**2\n\n        # Task 2: Expected value and variance of TD error after convergence\n        # m_after = 0\n        m_after = 0.0\n        # v_after = sigma^2\n        v_after = sigma**2\n\n        # Task 3: Expected cumulative mean squared TD error\n        # MSE_before = v_before + m_before^2, or sigma^2 + (mu1^2 + mu2^2)/2\n        mse_before = sigma**2 + 0.5 * (mu1**2 + mu2**2)\n        # MSE_after = v_after + m_after^2 = sigma^2\n        mse_after = sigma**2\n        \n        # C(N, T_burn) = min(N, T_burn) * MSE_before + max(0, N - T_burn) * MSE_after\n        num_steps_before = min(N, T_burn)\n        num_steps_after = max(0, N - T_burn)\n        \n        C = num_steps_before * mse_before + num_steps_after * mse_after\n\n        # Store results for this case\n        case_result = [m_before, v_before, m_after, v_after, C]\n        \n        # Format the result list into the required string format \" [v1,v2,...] \"\n        case_str = f\"[{','.join(map(str, case_result))}]\"\n        all_results_str.append(case_str)\n\n    # Final print statement in the exact required format \"[[...],[...],...]\"\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3113150"}, {"introduction": "Prioritized Experience Replay (PER) is a clever enhancement that accelerates learning by focusing on transitions the agent finds \"surprising\"—those with high TD error. However, this focused attention can sometimes be a double-edged sword. Through a hands-on simulation [@problem_id:3113071], we will explore a scenario where strong prioritization can cause the agent to overlook important experiences, a phenomenon known as mode collapse, providing a tangible lesson on the importance of balanced sampling.", "problem": "Construct a minimal Markov Decision Process that reveals how stochastic rewards can induce a bimodal Temporal-Difference (TD) error and how Prioritized Experience Replay can bias sampling toward one mode, potentially collapsing the diversity of sampled transitions. Use the following fundamental base: the Bellman optimality equation and the definition of the Temporal-Difference error, together with the standard definition of prioritized sampling probability.\n\nYou are given a single-state, single-action environment. A Deep Q-Network (DQN) is assumed to be present but held fixed (no learning), so that the action-value estimate remains constant. Let the reward at each time step be drawn independently from a two-point mixture distribution:\n- With probability $p_{\\mathrm{small}}$, the reward is $r_{\\mathrm{small}}$.\n- With probability $p_{\\mathrm{big}} = 1 - p_{\\mathrm{small}}$, the reward is $r_{\\mathrm{big}}$.\n\nLet the discount factor be $\\gamma$, and let the fixed Q-value be $q_0$ for the unique state-action pair. The Temporal-Difference (TD) error is defined by the core Temporal-Difference (TD) learning definition\n$$\n\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a).\n$$\nBecause $Q$ is held fixed at $q_0$ and there is only one action, you must use\n$$\n\\delta = r + \\gamma q_0 - q_0.\n$$\nWhen $q_0 = 0$, this reduces to $\\delta = r$, which directly inherits the bimodality from the two-point reward distribution whenever $\\lvert r_{\\mathrm{small}} \\rvert \\neq \\lvert r_{\\mathrm{big}} \\rvert$.\n\nFor Prioritized Experience Replay, transitions stored in a buffer of size $N$ are sampled with probability proportional to a power of their absolute TD error, using the standard, well-tested formulation:\n- For each transition $i$ with TD error $\\delta_i$, assign an unnormalized priority $w_i = \\left( \\lvert \\delta_i \\rvert + \\varepsilon \\right)^{\\alpha}$ for given $\\alpha \\ge 0$ and small $\\varepsilon \\ge 0$.\n- The sampling probability is $p_i = w_i / \\sum_j w_j$.\n- Samples are drawn with replacement.\n\nYour task is to write a complete, runnable program that:\n1. Constructs the described environment and fills a replay buffer with $N$ independent transitions by drawing rewards from the specified mixture. Use a fixed random seed $s$ for reproducibility.\n2. Labels each stored transition by which mixture mode generated its reward. Define the “minor mode” to be the mode with smaller absolute TD error magnitude $m_{\\mathrm{small}} = \\min\\{\\lvert r_{\\mathrm{small}} + (\\gamma - 1) q_0 \\rvert, \\lvert r_{\\mathrm{big}} + (\\gamma - 1) q_0 \\rvert\\}$, and the “big mode” to be the other one with magnitude $m_{\\mathrm{big}}$.\n3. Performs $M$ prioritized samples with replacement from the buffer using $p_i \\propto (\\lvert \\delta_i \\rvert + \\varepsilon)^{\\alpha}$.\n4. Computes the observed fraction $\\hat{f}_{\\mathrm{minor}}$ of sampled transitions that came from the minor mode.\n5. Reports $\\hat{f}_{\\mathrm{minor}}$ for each test case as a float rounded to exactly $6$ decimal places.\n\nPrinciple-based reasoning target: starting from the TD error definition and the prioritized weighting rule above, reason about how the expected minor-mode sampling fraction depends on the mixture probabilities and the magnitudes of the two modes. Programmatically estimate this quantity by Monte Carlo sampling and report the results.\n\nTest suite and parameterization. Run the program on the following four parameter sets, each specified as a tuple $(N, M, p_{\\mathrm{small}}, r_{\\mathrm{small}}, r_{\\mathrm{big}}, \\alpha, \\varepsilon, \\gamma, q_0, s)$:\n- Case A (boundary, uniform replay): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.2,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 3,\\; \\alpha = \\; 0,\\; \\varepsilon = \\; 0,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$.\n- Case B (happy path, moderate prioritization): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.5,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 5,\\; \\alpha = \\; 1,\\; \\varepsilon = \\; 0,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$.\n- Case C (edge, strong prioritization yielding mode collapse): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.8,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 20,\\; \\alpha = \\; 2,\\; \\varepsilon = \\; 0,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$.\n- Case D (edge, adding $\\varepsilon$ to mitigate collapse): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.8,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 20,\\; \\alpha = \\; 2,\\; \\varepsilon = \\; 20,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$.\n\nNotes:\n- Because $q_0 = 0$ and $\\gamma = 0$ in all cases, the TD error simplifies to $\\delta = r$.\n- Case A tests the boundary $\\alpha = 0$, where sampling must be uniform over the buffer regardless of magnitudes.\n- Case B demonstrates bias toward the larger-magnitude mode under moderate prioritization.\n- Case C demonstrates “mode collapse,” operationalized here as a very small $\\hat{f}_{\\mathrm{minor}}$ when $\\alpha$ is large and the magnitude ratio is large.\n- Case D demonstrates that making $\\varepsilon$ comparable to the larger magnitude can mitigate collapse by flattening priorities.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated Python-style list of the four rounded minor-mode fractions $[\\hat{f}_{\\mathrm{minor}}^{(A)}, \\hat{f}_{\\mathrm{minor}}^{(B)}, \\hat{f}_{\\mathrm{minor}}^{(C)}, \\hat{f}_{\\mathrm{minor}}^{(D)}]$, for Cases A, B, C, and D respectively. For example, an output of the form $[0.200000,0.166700,0.010000,0.524000]$ would be acceptable if those were the computed values.", "solution": "The problem is valid as it presents a well-posed, scientifically grounded simulation task within the domain of deep reinforcement learning. All parameters and definitions are provided, and the objective is clear and unambiguous.\n\nThe objective is to demonstrate the sampling bias induced by Prioritized Experience Replay (PER) in a minimal Markov Decision Process (MDP). This MDP is constructed with a single state and a single action, such that the action-value function $Q(s, a)$ is a fixed constant, $q_0$. The reward $r$ at each step is stochastic, drawn from a two-point mixture distribution: $r = r_{\\mathrm{small}}$ with probability $p_{\\mathrm{small}}$ and $r = r_{\\mathrm{big}}$ with probability $p_{\\mathrm{big}} = 1 - p_{\\mathrm{small}}$.\n\nThe core of the analysis rests on the Temporal-Difference (TD) error, $\\delta$, which is the basis for calculating priorities in PER. For a given transition, the TD error is defined by the Bellman equation:\n$$\n\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\n$$\nIn our simplified one-state MDP, the next state $s'$ is identical to the current state $s$, and there is only one action. The Q-value is fixed at $q_0$. The equation thus simplifies to:\n$$\n\\delta = r + \\gamma q_0 - q_0 = r + (\\gamma - 1)q_0\n$$\nSince the reward $r$ is bimodal, the TD error $\\delta$ is also bimodal, with two possible values:\n$$\n\\delta_{\\mathrm{small}} = r_{\\mathrm{small}} + (\\gamma - 1)q_0\n$$\n$$\n\\delta_{\\mathrm{big}} = r_{\\mathrm{big}} + (\\gamma - 1)q_0\n$$\nThe problem defines two modes based on the absolute magnitude of these TD errors. The \"minor mode\" corresponds to the smaller magnitude, $m_{\\mathrm{minor}} = \\min(|\\delta_{\\mathrm{small}}|, |\\delta_{\\mathrm{big}}|)$, and the \"big mode\" to the larger magnitude, $m_{\\mathrm{major}} = \\max(|\\delta_{\\mathrm{small}}|, |\\delta_{\\mathrm{big}}|)$.\n\nIn PER, transitions are sampled from a replay buffer of size $N$ with a probability proportional to their priority. The priority $w_i$ of a transition $i$ with TD error $\\delta_i$ is given by:\n$$\nw_i = (|\\delta_i| + \\varepsilon)^{\\alpha}\n$$\nwhere $\\alpha \\ge 0$ is the prioritization exponent and $\\varepsilon \\ge 0$ is a small constant to prevent zero priorities. The probability of sampling transition $i$ is:\n$$\np_i = \\frac{w_i}{\\sum_{j=1}^{N} w_j}\n$$\nLet the replay buffer contain $N_{\\mathrm{minor}}$ transitions from the minor mode and $N_{\\mathrm{major}}$ from the major mode, where $N_{\\mathrm{minor}} + N_{\\mathrm{major}} = N$. The priorities for these modes are $w_{\\mathrm{minor}} = (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha}$ and $w_{\\mathrm{major}} = (m_{\\mathrm{major}} + \\varepsilon)^{\\alpha}$. The total sum of priorities in the buffer is $W = N_{\\mathrm{minor}} w_{\\mathrm{minor}} + N_{\\mathrm{major}} w_{\\mathrm{major}}$.\n\nThe theoretical probability of picking any transition from the minor mode in a single draw is the sum of their individual probabilities, which simplifies to:\n$$\nP(\\text{sample is minor}) = \\frac{N_{\\mathrm{minor}} \\cdot w_{\\mathrm{minor}}}{W} = \\frac{N_{\\mathrm{minor}} \\cdot (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha}}{N_{\\mathrm{minor}} \\cdot (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha} + N_{\\mathrm{major}} \\cdot (m_{\\mathrm{major}} + \\varepsilon)^{\\alpha}}\n$$\nThis equation reveals the source of the sampling bias.\n- When $\\alpha = 0$, $w_{\\mathrm{minor}} = w_{\\mathrm{major}} = 1$. The sampling probability becomes $P(\\text{sample is minor}) = N_{\\mathrm{minor}} / (N_{\\mathrm{minor}} + N_{\\mathrm{major}}) = N_{\\mathrm{minor}} / N$. This corresponds to uniform sampling, where the sampling fraction mirrors the composition of the buffer.\n- When $\\alpha > 0$, the ratio of priorities $w_{\\mathrm{major}}/w_{\\mathrm{minor}}$ can become very large if $m_{\\mathrm{major}} \\gg m_{\\mathrm{minor}}$. This heavily biases sampling towards the major mode, even if $N_{\\mathrm{minor}} \\gg N_{\\mathrm{major}}$. This can lead to \"mode collapse,\" where transitions from the minor mode are rarely sampled.\n- A non-zero $\\varepsilon$, especially when comparable to $m_{\\mathrm{major}}$, reduces the ratio of priorities $(m_{\\mathrm{major}} + \\varepsilon)^{\\alpha} / (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha}$, thus mitigating the sampling bias.\n\nTo solve the problem, we perform a Monte Carlo simulation for each test case:\n1.  A replay buffer of $N$ transitions is instantiated. For each transition, a reward is drawn from the specified two-point distribution. We use a fixed random seed $s$ for reproducibility.\n2.  Based on the parameters for each case, we determine which reward ($r_{\\mathrm{small}}$ or $r_{\\mathrm{big}}$) gives rise to the minor TD error mode. Each transition in the buffer is labeled accordingly. For all provided test cases, $\\gamma = 0$ and $q_0 = 0$, so the TD error is identical to the reward, $\\delta = r$. The minor mode is thus simply the one with the smaller absolute reward.\n3.  The priorities for all $N$ transitions are calculated using the formula $w_i = (|\\delta_i| + \\varepsilon)^{\\alpha}$.\n4.  These priorities are normalized to form a sampling probability distribution over the $N$ transitions.\n5.  $M$ samples are drawn with replacement from the buffer according to this distribution.\n6.  The fraction of these $M$ samples that belong to the minor mode, $\\hat{f}_{\\mathrm{minor}}$, is calculated. This fraction serves as the numerical estimate for $P(\\text{sample is minor})$.\n\nThis procedure is systematically applied to all four test cases to quantify the impact of the parameters $\\alpha$ and $\\varepsilon$ on sampling diversity.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a Monte Carlo simulation to demonstrate the effect of Prioritized\n    Experience Replay (PER) on sampling bias in a simplified MDP.\n    \"\"\"\n    test_cases = [\n        # Case A: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.2, -1, 3, 0, 0, 0, 0, 42),\n        # Case B: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.5, -1, 5, 1, 0, 0, 0, 42),\n        # Case C: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.8, -1, 20, 2, 0, 0, 0, 42),\n        # Case D: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.8, -1, 20, 2, 20, 0, 0, 42),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s = case\n\n        # 1. Initialize random number generator for reproducibility.\n        rng = np.random.default_rng(s)\n\n        # 2. Fill the replay buffer with N transitions.\n        # Draw N random numbers to determine the reward for each transition.\n        reward_choices = rng.random(size=N)\n        # Assign rewards based on the mixture probability p_small.\n        rewards = np.where(reward_choices < p_small, r_small, r_big)\n\n        # 3. Calculate TD errors and identify which transitions belong to the minor mode.\n        td_error_for_r_small = r_small + (gamma - 1) * q0\n        td_error_for_r_big = r_big + (gamma - 1) * q0\n\n        # An array indicating which transitions were generated by the r_small component.\n        is_from_small_reward_source = (rewards == r_small)\n\n        # Determine which reward source corresponds to the minor TD error mode.\n        if np.abs(td_error_for_r_small) < np.abs(td_error_for_r_big):\n            # The minor mode corresponds to r_small.\n            is_minor_mode = is_from_small_reward_source\n        else:\n            # The minor mode corresponds to r_big.\n            is_minor_mode = ~is_from_small_reward_source\n\n        # Calculate the TD error for each transition in the buffer.\n        td_errors = rewards + (gamma - 1) * q0\n\n        # 4. Compute sampling priorities and probabilities.\n        priorities = (np.abs(td_errors) + epsilon)**alpha\n        \n        total_priority = np.sum(priorities)\n        \n        if total_priority > 0:\n            sampling_probs = priorities / total_priority\n        else:\n            # Fallback to uniform sampling if all priorities are zero.\n            # This can happen if alpha > 0 and |delta_i| + epsilon is 0 for all i.\n            # Not expected for the given test cases.\n            sampling_probs = np.full(N, 1.0 / N)\n\n        # 5. Perform M prioritized samples with replacement.\n        sampled_indices = rng.choice(N, size=M, replace=True, p=sampling_probs)\n\n        # 6. Compute the observed fraction of sampled transitions from the minor mode.\n        num_minor_sampled = np.sum(is_minor_mode[sampled_indices])\n        f_minor_hat = num_minor_sampled / M\n        \n        results.append(f_minor_hat)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3113071"}, {"introduction": "After analyzing existing mechanisms, the next step is to design new ones. This practice [@problem_id:3113068] challenges you to think like a researcher by proposing and evaluating a novel \"age-aware\" prioritized replay scheme. By calculating key performance metrics related to learning speed and stability, you will gain insight into the complex trade-offs involved in designing effective and robust reinforcement learning algorithms.", "problem": "Consider the setting of Deep Q-Network (DQN) training with Experience Replay (ER). Let a replay buffer contain $N$ transitions indexed by $i \\in \\{1, \\dots, N\\}$, each with a temporal-difference error $\\delta_i$ and an associated age $\\text{age}_i$ measured as the number of environment steps since storage. We propose an age-aware prioritized replay where the unnormalized priority of transition $i$ is given by\n$$\ns_i = \\frac{|\\delta_i|^\\alpha}{1 + \\lambda \\, \\text{age}_i},\n$$\nwith $\\alpha \\ge 0$ controlling the degree of prioritization and $\\lambda \\ge 0$ penalizing older transitions. The sampling probability is\n$$\np_i = \\frac{s_i}{\\sum_{j=1}^{N} s_j},\n$$\nwith the convention that if $\\sum_{j=1}^{N} s_j = 0$ then $p_i = \\frac{1}{N}$ for all $i$.\n\nTo mitigate bias from non-uniform sampling, importance sampling weights are used. Let $\\beta \\in [0,1]$ control the strength of correction. Define the unnormalized weight\n$$\nw_i = (N \\, p_i)^{-\\beta}\n$$\nfor $p_i > 0$, and $w_i = 0$ when $p_i = 0$. For numerical stability and to avoid uncontrolled step size scaling, define a normalized weight\n$$\n\\hat{w}_i = \n\\begin{cases}\n\\frac{w_i}{\\max_{k: p_k > 0} w_k} & \\text{if } \\max_{k: p_k > 0} w_k > 0, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n$$\n\nWe analyze two core aspects:\n- Learning speed proxy $S$, defined as the expected absolute update magnitude under the sampling distribution,\n$$\nS = \\sum_{i=1}^{N} p_i \\, \\hat{w}_i \\, |\\delta_i|.\n$$\n- Stability proxy $V$, defined as the variance of signed updates,\n$$\n\\mu = \\sum_{i=1}^{N} p_i \\, \\hat{w}_i \\, \\delta_i, \\quad\nV = \\sum_{i=1}^{N} p_i \\left(\\hat{w}_i \\, \\delta_i - \\mu \\right)^2.\n$$\n\nAdditionally, report the effective sample size (ESS) and an outlier-sensitivity factor:\n$$\n\\text{ESS} = \\frac{1}{\\sum_{i=1}^{N} p_i^2}, \\quad\nO = \\frac{\\max_{i} \\hat{w}_i}{\\frac{1}{N} \\sum_{i=1}^{N} \\hat{w}_i}.\n$$\nThe ESS quantifies diversity of the sampling distribution, and $O$ quantifies the potential dominance of the largest normalized weight relative to the mean normalized weight.\n\nStarting from the fundamental base that DQN minimizes the mean-squared Bellman error using stochastic gradient descent and that importance sampling corrects the expectation of gradient estimates under non-uniform sampling, derive how $S$ and $V$ relate to $p_i$ and $\\hat{w}_i$. Implement a program that, for each test case below, computes and returns $(S, V, \\text{ESS}, O)$.\n\nTest suite:\n- Case $1$ (general happy path): $N=6$, $\\delta = [\\,0.3,\\,-0.1,\\,0.05,\\,0.8,\\,-0.4,\\,0.2\\,]$, $\\text{age} = [\\,1,\\,5,\\,0,\\,2,\\,10,\\,3\\,]$, $\\alpha = 0.6$, $\\lambda = 0.5$, $\\beta = 0.4$.\n- Case $2$ (uniform baseline): $N=6$, $\\delta = [\\,0.3,\\,-0.1,\\,0.05,\\,0.8,\\,-0.4,\\,0.2\\,]$, $\\text{age} = [\\,1,\\,5,\\,0,\\,2,\\,10,\\,3\\,]$, $\\alpha = 0$, $\\lambda = 0$, $\\beta = 0$.\n- Case $3$ (strong age penalty): $N=6$, $\\delta = [\\,0.3,\\,-0.1,\\,0.05,\\,0.8,\\,-0.4,\\,0.2\\,]$, $\\text{age} = [\\,1,\\,5,\\,0,\\,2,\\,10,\\,3\\,]$, $\\alpha = 0.6$, $\\lambda = 2.0$, $\\beta = 0.4$.\n- Case $4$ (outlier temporal-difference error): $N=6$, $\\delta = [\\,0.01,\\,0.02,\\,0.01,\\,5.0,\\,0.0,\\,-0.01\\,]$, $\\text{age} = [\\,0,\\,1,\\,0,\\,0,\\,50,\\,2\\,]$, $\\alpha = 1.0$, $\\lambda = 0.1$, $\\beta = 0.6$.\n- Case $5$ (degenerate zero priorities): $N=4$, $\\delta = [\\,0,\\,0,\\,0,\\,0\\,]$, $\\text{age} = [\\,0,\\,10,\\,20,\\,30\\,]$, $\\alpha = 0.5$, $\\lambda = 1.0$, $\\beta = 0.5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The order must be $[S_1,V_1,\\text{ESS}_1,O_1,S_2,V_2,\\text{ESS}_2,O_2,S_3,V_3,\\text{ESS}_3,O_3,S_4,V_4,\\text{ESS}_4,O_4,S_5,V_5,\\text{ESS}_5,O_5]$, where the subscript denotes the case index. All values must be floats.", "solution": "The problem presented is a well-posed and scientifically grounded exercise in the analysis of a prioritized experience replay mechanism for deep Q-networks (DQNs). It provides a complete and consistent set of definitions and data, allowing for a unique and verifiable solution. We may therefore proceed with a full solution.\n\nThe core of the problem requires us to understand the roles of the learning speed proxy, $S$, and the stability proxy, $V$, in the context of importance sampling corrections for prioritized replay. To do so, we must first revisit the fundamentals of learning in DQNs.\n\nA DQN is trained to minimize the mean-squared Bellman error (MSBE) over a distribution of transitions $\\mathcal{D}$. The loss function for a single transition $i$, consisting of state $s_i$, action $a_i$, reward $r_i$, and next state $s'_i$, is given by $L_i(\\theta) = \\delta_i^2$, where the temporal-difference (TD) error $\\delta_i = (r_i + \\gamma \\max_{a'} Q(s'_i, a'; \\theta^-)) - Q(s_i, a_i; \\theta)$, with $\\theta$ being the weights of the online network and $\\theta^-$ being the weights of the target network.\n\nTraining proceeds via stochastic gradient descent (SGD). The gradient of the loss with respect to the online network weights $\\theta$ for transition $i$ is:\n$$\n\\nabla_\\theta L_i(\\theta) = \\nabla_\\theta \\left( (r_i + \\gamma \\max_{a'} Q(s'_i, a'; \\theta^-) - Q(s_i, a_i; \\theta))^2 \\right) \\approx -2\\delta_i \\nabla_\\theta Q(s_i, a_i; \\theta)\n$$\nHere, we treat the target term as a constant, a standard practice in Q-learning. The SGD update rule is thus $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta L(\\theta)$, where $\\eta$ is the learning rate. For a single sample $i$, the update is proportional to $\\delta_i \\nabla_\\theta Q(s_i, a_i; \\theta)$.\n\nIf we sample transitions uniformly from the replay buffer of size $N$, the probability of sampling any transition $i$ is $1/N$. The expected gradient over a minibatch is an unbiased estimate of the true gradient over the entire buffer. However, uniform sampling is inefficient as it gives equal importance to all transitions, including those with small errors that contribute little to learning.\n\nPrioritized Experience Replay (PER) addresses this by sampling transitions non-uniformly with probabilities $p_i$ that are monotonically related to the magnitude of their TD errors, $|\\delta_i|$. In this problem, the sampling probability $p_i$ is derived from the score $s_i = \\frac{|\\delta_i|^\\alpha}{1 + \\lambda \\, \\text{age}_i}$. This introduces a bias: the expectation of the gradient under this sampling distribution $p_i$ no longer matches the true gradient expectation.\n$$\n\\mathbb{E}_{i \\sim p}[\\nabla_\\theta L_i] = \\sum_{i=1}^{N} p_i \\nabla_\\theta L_i \\neq \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_\\theta L_i = \\mathbb{E}_{i \\sim U(1/N)}[\\nabla_\\theta L_i]\n$$\nTo correct for this bias, we employ importance sampling (IS). The update for each sample $i$ is weighted by the ratio of the target distribution (uniform) to the sampling distribution, $\\frac{1/N}{p_i}$. The loss for sample $i$ becomes $L_i^{IS}(\\theta) = \\frac{1}{N p_i} \\delta_i^2$. However, using these raw weights can lead to instability. For this reason, two modifications are standard:\n1. The weight is raised to a power $\\beta \\in [0, 1]$, giving $w_i = (N p_i)^{-\\beta}$. This parameter allows for a smooth interpolation between uniform sampling ($\\beta=0$, where $w_i=1$) and full IS correction ($\\beta=1$). This introduces a controlled amount of bias for a significant reduction in variance.\n2. The weights are normalized by their maximum value, $\\hat{w}_i = w_i / \\max_k w_k$. This ensures that the largest weight is capped at $1$, preventing any single transition from dominating the gradient update and destabilizing the learning process.\n\nThe resulting gradient update for a sample $i$ is now proportional to $\\hat{w}_i \\delta_i \\nabla_\\theta Q(s_i, a_i; \\theta)$. The term $\\hat{w}_i \\delta_i$ can be seen as the effective, corrected error signal that drives learning for sample $i$.\n\nWith this foundation, we can now justify the definitions of the proxies $S$ and $V$.\n- **Learning Speed Proxy, $S$**: The magnitude of the parameter update for sample $i$ is proportional to $|\\hat{w}_i \\delta_i| = \\hat{w}_i |\\delta_i|$ (since $\\hat{w}_i \\ge 0$). The expected magnitude of an update step, when sampling with probabilities $p_i$, is the expectation of this quantity over the sampling distribution:\n$$\n\\mathbb{E}_{i \\sim p}[\\text{update magnitude}] \\propto \\sum_{i=1}^N p_i \\hat{w}_i |\\delta_i| = S\n$$\nTherefore, $S$ represents the expected size of a gradient step. A larger value of $S$ suggests that, on average, the network is making larger adjustments to its weights, which is a plausible proxy for accelerated learning.\n\n- **Stability Proxy, $V$**: The stability of SGD is critically dependent on the variance of the gradient estimates. High variance can cause the learning process to oscillate or diverge. The term $\\hat{w}_i \\delta_i$ is the random variable representing the signed corrective signal for a sampled transition $i$. The quantity $V$ is defined as the variance of this random variable under the sampling distribution $p_i$:\n$$\n\\mu = \\mathbb{E}[\\hat{w} \\delta] = \\sum_{i=1}^N p_i (\\hat{w}_i \\delta_i)\n$$\n$$\nV = \\text{Var}(\\hat{w} \\delta) = \\mathbb{E}[(\\hat{w} \\delta - \\mu)^2] = \\sum_{i=1}^N p_i (\\hat{w}_i \\delta_i - \\mu)^2\n$$\nA smaller value of $V$ indicates that the corrective signals are more consistent across different samples, which is a proxy for a more stable and reliable learning process.\n\nThe other two metrics provide additional diagnostics:\n- **Effective Sample Size, ESS**: ESS measures the diversity of the samples. For a uniform distribution ($p_i = 1/N$), $\\text{ESS} = N$. For a highly skewed distribution where one $p_k \\approx 1$, ESS approaches $1$. A low ESS indicates that the replay is repeatedly sampling a small subset of transitions, potentially leading to overfitting and poor generalization.\n- **Outlier-sensitivity Factor, $O$**: This metric quantifies the dominance of the largest normalized weight $\\hat{w}_i$ compared to the average. A high value of $O$ indicates that one or a few samples have a disproportionately large influence on the updates, which can be a source of instability.\n\nThe algorithm to compute these four metrics for a given test case is as follows:\n1.  Given $N, \\{\\delta_i\\}, \\{\\text{age}_i\\}, \\alpha, \\lambda, \\beta$.\n2.  Calculate the unnormalized priority for each transition $i$: $s_i = \\frac{|\\delta_i|^\\alpha}{1 + \\lambda \\, \\text{age}_i}$.\n3.  Calculate the sum of priorities, $S_{tot} = \\sum_{j=1}^N s_j$.\n4.  If $S_{tot} = 0$, set sampling probabilities to uniform: $p_i = 1/N$ for all $i$. Otherwise, normalize: $p_i = s_i / S_{tot}$.\n5.  Calculate unnormalized importance-sampling weights $w_i$. For each $i$ where $p_i > 0$, set $w_i = (N p_i)^{-\\beta}$. For $i$ where $p_i = 0$, set $w_i = 0$.\n6.  Calculate the maximum weight, $w_{max} = \\max_k w_k$.\n7.  If $w_{max} > 0$, calculate normalized weights $\\hat{w}_i = w_i / w_{max}$. Otherwise, set $\\hat{w}_i = 0$ for all $i$.\n8.  Compute the four metrics using their definitions:\n    - $S = \\sum_{i=1}^{N} p_i \\hat{w}_i |\\delta_i|$.\n    - $\\mu = \\sum_{i=1}^{N} p_i \\hat{w}_i \\delta_i$, then $V = \\sum_{i=1}^{N} p_i (\\hat{w}_i \\delta_i - \\mu)^2$.\n    - $\\text{ESS} = 1 / (\\sum_{i=1}^{N} p_i^2)$.\n    - $O = (\\max_i \\hat{w}_i) / (\\frac{1}{N}\\sum_{i=1}^N \\hat{w}_i)$, assuming the denominator is non-zero.\nThis procedure is deterministic and computationally straightforward.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes learning and stability metrics for an age-aware prioritized experience replay scheme.\n    \"\"\"\n    test_cases = [\n        # Case 1: general happy path\n        {'N': 6, 'delta': [0.3, -0.1, 0.05, 0.8, -0.4, 0.2], 'age': [1, 5, 0, 2, 10, 3], 'alpha': 0.6, 'lambda': 0.5, 'beta': 0.4},\n        # Case 2: uniform baseline\n        {'N': 6, 'delta': [0.3, -0.1, 0.05, 0.8, -0.4, 0.2], 'age': [1, 5, 0, 2, 10, 3], 'alpha': 0.0, 'lambda': 0.0, 'beta': 0.0},\n        # Case 3: strong age penalty\n        {'N': 6, 'delta': [0.3, -0.1, 0.05, 0.8, -0.4, 0.2], 'age': [1, 5, 0, 2, 10, 3], 'alpha': 0.6, 'lambda': 2.0, 'beta': 0.4},\n        # Case 4: outlier temporal-difference error\n        {'N': 6, 'delta': [0.01, 0.02, 0.01, 5.0, 0.0, -0.01], 'age': [0, 1, 0, 0, 50, 2], 'alpha': 1.0, 'lambda': 0.1, 'beta': 0.6},\n        # Case 5: degenerate zero priorities\n        {'N': 4, 'delta': [0.0, 0.0, 0.0, 0.0], 'age': [0, 10, 20, 30], 'alpha': 0.5, 'lambda': 1.0, 'beta': 0.5},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = case['N']\n        delta = np.array(case['delta'], dtype=float)\n        age = np.array(case['age'], dtype=float)\n        alpha = case['alpha']\n        lambda_ = case['lambda']\n        beta = case['beta']\n\n        # 1. Calculate unnormalized priorities s_i\n        # np.power(0.0, 0.0) correctly returns 1.0, handling the alpha=0 case.\n        s = np.power(np.abs(delta), alpha) / (1.0 + lambda_ * age)\n\n        # 2. Calculate sampling probabilities p_i\n        sum_s = np.sum(s)\n        if sum_s == 0.0:\n            p = np.full(N, 1.0 / N)\n        else:\n            p = s / sum_s\n\n        # 3. Calculate unnormalized importance sampling weights w_i\n        w = np.zeros(N, dtype=float)\n        positive_p_mask = p > 0.0\n        if np.any(positive_p_mask):\n            w[positive_p_mask] = np.power(N * p[positive_p_mask], -beta)\n            \n        # 4. Calculate normalized importance sampling weights w_hat_i\n        w_hat = np.zeros(N, dtype=float)\n        max_w = np.max(w)\n        if max_w > 0.0:\n            w_hat = w / max_w\n            \n        # 5. Calculate metrics S, V, ESS, O\n        # S: Learning speed proxy\n        S = np.sum(p * w_hat * np.abs(delta))\n\n        # V: Stability proxy (Variance)\n        mu = np.sum(p * w_hat * delta)\n        V = np.sum(p * np.power(w_hat * delta - mu, 2))\n\n        # ESS: Effective Sample Size\n        ESS = 1.0 / np.sum(np.power(p, 2))\n\n        # O: Outlier-sensitivity factor\n        max_w_hat = np.max(w_hat)\n        mean_w_hat = np.mean(w_hat)\n        \n        if mean_w_hat > 0:\n            O = max_w_hat / mean_w_hat\n        else:\n            # This case implies all w_hat are 0, so max_w_hat is also 0.\n            # No single dominant weight exists. A ratio of 1.0 is a reasonable convention.\n            O = 1.0\n\n        results.extend([S, V, ESS, O])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3113068"}]}