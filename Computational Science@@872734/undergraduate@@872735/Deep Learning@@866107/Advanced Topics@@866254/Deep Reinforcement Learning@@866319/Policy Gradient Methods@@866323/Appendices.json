{"hands_on_practices": [{"introduction": "A fundamental step in implementing any policy gradient algorithm is deriving the score function, $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a|s)$. In many practical applications, the set of valid actions changes depending on the state. This exercise provides hands-on practice in deriving the score function for a softmax policy that incorporates action masking, a standard technique for handling state-dependent action spaces. By working through this problem, you will solidify your understanding of how the gradient is computed and why its components for illegal actions correctly become zero.", "problem": "Consider a discrete-action policy in Reinforcement Learning (RL) that uses action masking to forbid illegal actions in a given state. Let the action set be $\\{1,2,3,4\\}$ and let the mask be $m \\in \\{0,1\\}^{4}$, where $m_{i} = 1$ indicates that action $i$ is legal and $m_{i} = 0$ indicates that action $i$ is illegal. The policy is parameterized by a vector of logits $\\theta \\in \\mathbb{R}^{4}$, and the probability assigned to action $a$ in state $s$ under mask $m$ is defined by\n$$\n\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} ,\n$$\nwhich enforces zero probability for illegal actions and normalizes only over legal actions. Assume the chosen action $a$ is legal, so that $\\pi(a \\mid s, m, \\theta)  0$.\n\nStarting from the definition of the score function and basic rules of differentiation, derive the gradient $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$ for a legal chosen action $a$ under arbitrary mask $m$, taking care to justify why any components corresponding to illegal actions vanish. Then, evaluate this gradient at\n$$\n\\theta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad m = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad a = 3 .\n$$\nYour final answer must be the resulting gradient vector expressed exactly in analytic form as a single row matrix. Do not approximate; no rounding is required. No physical units are involved.", "solution": "The user has provided a problem statement that requires validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- Action set: $\\{1, 2, 3, 4\\}$\n- Mask: $m \\in \\{0, 1\\}^4$, where $m_i = 1$ for a legal action and $m_i=0$ for an illegal action.\n- Policy parameters (logits): $\\theta \\in \\mathbb{R}^4$.\n- Policy definition: $\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}$.\n- Condition: The chosen action $a$ is legal, which implies $m_a = 1$.\n- Task 1: Derive the gradient $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$.\n- Task 2: Evaluate the gradient for the specific case:\n  - $\\theta = \\begin{pmatrix} 0  1  2  -1 \\end{pmatrix}^T$\n  - $m = \\begin{pmatrix} 1  0  1  1 \\end{pmatrix}^T$\n  - $a = 3$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n- **Scientifically Grounded:** The problem is firmly rooted in the theory of Reinforcement Learning, specifically concerning policy gradient methods. The formulation of a masked softmax policy and the calculation of its score function ($\\nabla_{\\theta} \\ln \\pi$) are standard and correct techniques in the field.\n- **Well-Posed:** The problem is clearly defined. It provides a specific functional form for the policy, all necessary parameters for evaluation, and a clear objective. The condition that the chosen action $a$ is legal ensures that the policy probability $\\pi(a | \\dots)$ is greater than $0$, and thus its logarithm is well-defined. A unique, stable, and meaningful solution exists.\n- **Objective:** The problem is stated using precise mathematical language, free from any subjectivity or ambiguity.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe primary objective is to derive the gradient of the log-policy, also known as the score function. The policy is given by:\n$$\n\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}\n$$\nTo find the gradient $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$, we first compute the logarithm of the policy probability.\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\ln \\left( \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} \\right)\n$$\nUsing the property of logarithms $\\ln(x/y) = \\ln(x) - \\ln(y)$, we get:\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\ln(\\exp(\\theta_{a}) \\, m_{a}) - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\nThe problem states that the chosen action $a$ is legal, which means $m_a = 1$. This simplifies the first term: $\\ln(\\exp(\\theta_{a}) \\cdot 1) = \\theta_a$.\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\theta_a - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\nNow, we compute the gradient of this expression with respect to the parameter vector $\\theta$. The gradient is a vector whose $j$-th component is the partial derivative with respect to $\\theta_j$.\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_j = \\frac{\\partial}{\\partial \\theta_j} \\left( \\theta_a - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) \\right)\n$$\nWe differentiate term by term:\n$$\n\\frac{\\partial}{\\partial \\theta_j} (\\theta_a) = \\delta_{aj}\n$$\nwhere $\\delta_{aj}$ is the Kronecker delta, which is $1$ if $j=a$ and $0$ otherwise.\n\nFor the second term, we use the chain rule for differentiation ($\\frac{d}{dx}\\ln(f(x)) = \\frac{f'(x)}{f(x)}$):\n$$\n\\frac{\\partial}{\\partial \\theta_j} \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) = \\frac{1}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} \\cdot \\frac{\\partial}{\\partial \\theta_j}\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\nThe derivative of the sum with respect to $\\theta_j$ is:\n$$\n\\frac{\\partial}{\\partial \\theta_j}\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) = \\exp(\\theta_j) \\, m_j\n$$\nCombining these, the derivative of the second term is:\n$$\n\\frac{\\exp(\\theta_j) \\, m_j}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}\n$$\nThis expression is exactly the definition of the policy probability for action $j$, $\\pi(j \\mid s, m, \\theta)$.\n\nSo, the $j$-th component of the gradient is:\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_j = \\delta_{aj} - \\pi(j \\mid s, m, \\theta)\n$$\nThis is the general form of the score function for a masked softmax policy.\n\nThe problem asks to justify why components corresponding to illegal actions vanish. Let $k$ be an index for an illegal action, meaning $m_k=0$. The $k$-th component of the gradient is:\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_k = \\delta_{ak} - \\pi(k \\mid s, m, \\theta)\n$$\nSince $a$ is a legal action ($m_a=1$) and $k$ is an illegal action ($m_k=0$), we must have $a \\neq k$. Therefore, $\\delta_{ak} = 0$.\nThe probability of the illegal action $k$ is:\n$$\n\\pi(k \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{k}) \\, m_{k}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} = \\frac{\\exp(\\theta_{k}) \\cdot 0}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} = 0\n$$\nThus, the $k$-th component of the gradient becomes:\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_k = 0 - 0 = 0\n$$\nThis confirms that the gradient components for all illegal actions are zero.\n\n### Gradient Evaluation\n\nWe are given the following values:\n$$\n\\theta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad m = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad a = 3\n$$\nFirst, let's compute the normalization term, $Z = \\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}$:\n$$\nZ = \\exp(\\theta_1)m_1 + \\exp(\\theta_2)m_2 + \\exp(\\theta_3)m_3 + \\exp(\\theta_4)m_4\n$$\n$$\nZ = \\exp(0) \\cdot 1 + \\exp(1) \\cdot 0 + \\exp(2) \\cdot 1 + \\exp(-1) \\cdot 1\n$$\n$$\nZ = 1 \\cdot 1 + 0 + e^2 \\cdot 1 + e^{-1} \\cdot 1 = 1 + e^2 + e^{-1}\n$$\nNext, we compute the probability vector $\\vec{\\pi}$ whose components are $\\pi(j \\mid s, m, \\theta)$:\n$$\n\\pi(1) = \\frac{\\exp(0) \\cdot 1}{Z} = \\frac{1}{Z}\n$$\n$$\n\\pi(2) = \\frac{\\exp(1) \\cdot 0}{Z} = 0\n$$\n$$\n\\pi(3) = \\frac{\\exp(2) \\cdot 1}{Z} = \\frac{e^2}{Z}\n$$\n$$\n\\pi(4) = \\frac{\\exp(-1) \\cdot 1}{Z} = \\frac{e^{-1}}{Z}\n$$\nThe gradient vector is given by $\\nabla_{\\theta} \\ln \\pi(a=3 \\mid \\dots) = \\mathbf{e}_3 - \\vec{\\pi}$, where $\\mathbf{e}_3$ is the one-hot vector $(0, 0, 1, 0)^T$.\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} \\pi(1) \\\\ \\pi(2) \\\\ \\pi(3) \\\\ \\pi(4) \\end{pmatrix} = \\begin{pmatrix} -\\pi(1) \\\\ -\\pi(2) \\\\ 1 - \\pi(3) \\\\ -\\pi(4) \\end{pmatrix}\n$$\nSubstituting the probabilities:\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ 1 - e^2/Z \\\\ -e^{-1}/Z \\end{pmatrix} = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ (Z - e^2)/Z \\\\ -e^{-1}/Z \\end{pmatrix}\n$$\nSubstitute $Z = 1 + e^2 + e^{-1}$:\n$$\nZ - e^2 = (1 + e^2 + e^{-1}) - e^2 = 1 + e^{-1}\n$$\nSo the gradient vector is:\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ (1+e^{-1})/Z \\\\ -e^{-1}/Z \\end{pmatrix} = \\frac{1}{Z} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1+e^{-1} \\\\ -e^{-1} \\end{pmatrix}\n$$\nTo express this in a more elegant form, we can substitute $Z = 1 + e^2 + e^{-1}$ and clear the negative exponents by multiplying the numerator and denominator by $e$.\n$$\n\\frac{1}{Z} = \\frac{1}{1 + e^2 + e^{-1}} = \\frac{e}{e(1 + e^2 + e^{-1})} = \\frac{e}{e + e^3 + 1}\n$$\nNow multiply this scalar into the vector:\n$$\n\\nabla_{\\theta} \\ln \\pi = \\frac{e}{1+e+e^3} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1+e^{-1} \\\\ -e^{-1} \\end{pmatrix} = \\frac{1}{1+e+e^3} \\begin{pmatrix} -e \\\\ 0 \\\\ e(1+e^{-1}) \\\\ e(-e^{-1}) \\end{pmatrix} = \\frac{1}{1+e+e^3} \\begin{pmatrix} -e \\\\ 0 \\\\ e+1 \\\\ -1 \\end{pmatrix}\n$$\nThe components of the gradient vector are:\n\\begin{itemize}\n    \\item Component $1$: $-\\frac{e}{1+e+e^3}$\n    \\item Component $2$: $0$\n    \\item Component $3$: $\\frac{e+1}{1+e+e^3}$\n    \\item Component $4$: $-\\frac{1}{1+e+e^3}$\n\\end{itemize}\nThe final answer is to be provided as a row matrix.", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{e}{e^3+e+1}  0  \\frac{e+1}{e^3+e+1}  -\\frac{1}{e^3+e+1} \\end{pmatrix}}\n$$", "id": "3158020"}, {"introduction": "While the score-function estimator is unbiased, its major practical drawback is high variance, which can lead to slow and unstable learning. Consequently, variance reduction techniques are not just optional extras but essential components of modern policy gradient methods. This practice offers a clear and quantitative look at one such technique, antithetic sampling, allowing you to derive its effect on variance from first principles. By comparing the variance of a standard estimator with an antithetic-pair estimator, you will gain concrete insight into how correlated samples can improve estimation efficiency.", "problem": "Consider a one-dimensional stochastic bandit with action $a \\in \\mathbb{R}$ drawn from a symmetric Gaussian policy $\\pi_{\\theta}(a) = \\mathcal{N}(\\theta, \\sigma^{2})$, where the variance $\\sigma^{2}  0$ is fixed and only the mean $\\theta \\in \\mathbb{R}$ is the trainable parameter. The reward is linear in the action, $r(a) = k a$, where $k \\in \\mathbb{R}$ is a nonzero constant. The objective is $J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a)]$. Use the likelihood-ratio (score-function) identity $\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a)]$ as the fundamental starting point.\n\nDefine the following unbiased gradient estimators for $\\nabla_{\\theta} J(\\theta)$:\n- A two-sample independent-and-identically-distributed averaged estimator: draw $a_{1}, a_{2} \\stackrel{\\text{i.i.d.}}{\\sim} \\pi_{\\theta}$ and set $\\hat{g}_{\\text{iid}} = \\frac{1}{2}\\big[r(a_{1}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{1}) + r(a_{2}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{2})\\big]$.\n- An antithetic-pair averaged estimator: draw $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$, set $a_{+} = \\theta + \\varepsilon$ and $a_{-} = \\theta - \\varepsilon$, and define $\\hat{g}_{\\text{anti}} = \\frac{1}{2}\\big[r(a_{+}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{+}) + r(a_{-}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{-})\\big]$.\n\nStarting from the definition of $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a)$ for a univariate Gaussian with fixed variance and basic moment identities for a Gaussian random variable, derive closed-form expressions for $\\operatorname{Var}[\\hat{g}_{\\text{iid}}]$ and $\\operatorname{Var}[\\hat{g}_{\\text{anti}}]$. Then, quantify the variance reduction achieved by antithetic sampling under an equal reward-evaluation budget by providing the closed-form analytic expression for the variance-reduction factor\n$$V(\\theta, \\sigma) := \\frac{\\operatorname{Var}[\\hat{g}_{\\text{iid}}]}{\\operatorname{Var}[\\hat{g}_{\\text{anti}}]}.$$\n\nExpress your final result as a single simplified expression in terms of $\\theta$ and $\\sigma$. No numerical rounding is required.", "solution": "The user wants to validate and solve a problem on variance reduction in policy gradient methods.\n\n### Step 1: Extract Givens\n-   **Policy**: $\\pi_{\\theta}(a) = \\mathcal{N}(\\theta, \\sigma^{2})$, a symmetric Gaussian policy with trainable mean $\\theta \\in \\mathbb{R}$ and fixed variance $\\sigma^{2}  0$.\n-   **Action**: $a \\in \\mathbb{R}$.\n-   **Reward Function**: $r(a) = k a$, where $k \\in \\mathbb{R}$ is a nonzero constant.\n-   **Objective Function**: $J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a)]$.\n-   **Gradient Identity**: $\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a)]$.\n-   **I.I.D. Gradient Estimator**: $\\hat{g}_{\\text{iid}} = \\frac{1}{2}\\big[r(a_{1}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{1}) + r(a_{2}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{2})\\big]$, where $a_{1}, a_{2} \\stackrel{\\text{i.i.d.}}{\\sim} \\pi_{\\theta}$.\n-   **Antithetic Gradient Estimator**: $\\hat{g}_{\\text{anti}} = \\frac{1}{2}\\big[r(a_{+}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{+}) + r(a_{-}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{-})\\big]$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$, $a_{+} = \\theta + \\varepsilon$, and $a_{-} = \\theta - \\varepsilon$.\n-   **Target Quantity**: The variance-reduction factor $V(\\theta, \\sigma) := \\frac{\\operatorname{Var}[\\hat{g}_{\\text{iid}}]}{\\operatorname{Var}[\\hat{g}_{\\text{anti}}]}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n-   **Scientific Grounding**: The problem deals with standard concepts in reinforcement learning and computational statistics: policy gradient methods, the score-function (or REINFORCE) estimator, and antithetic sampling as a variance reduction technique. All definitions and principles are standard and correct.\n-   **Well-Posedness**: The problem provides all necessary definitions and asks for the derivation of specific, well-defined quantities ($\\operatorname{Var}[\\hat{g}_{\\text{iid}}]$, $\\operatorname{Var}[\\hat{g}_{\\text{anti}}]$, and their ratio). The setup is self-contained and mathematically consistent, leading to a unique solution.\n-   **Objectivity**: The problem is stated using formal mathematical language and is devoid of any subjective or ambiguous terminology.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the full derivation.\n\n### Solution Derivation\n\nFirst, we establish some preliminary quantities. The probability density function for the policy $\\pi_{\\theta}(a) = \\mathcal{N}(\\theta, \\sigma^2)$ is\n$$p(a; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(a-\\theta)^2}{2\\sigma^2}\\right).$$\nThe log-policy is $\\ln \\pi_{\\theta}(a) = -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(a-\\theta)^2}{2\\sigma^2}$. The score function, which is the gradient of the log-policy with respect to the parameter $\\theta$, is\n$$ \\nabla_{\\theta} \\ln \\pi_{\\theta}(a) = \\frac{\\partial}{\\partial\\theta}\\left(-\\frac{(a-\\theta)^2}{2\\sigma^2}\\right) = -\\frac{2(a-\\theta)(-1)}{2\\sigma^2} = \\frac{a-\\theta}{\\sigma^2}. $$\nLet's define a single-sample gradient estimate as $\\hat{g}(a) = r(a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a)$. Using the given reward and the derived score function, we have\n$$ \\hat{g}(a) = (ka) \\left(\\frac{a-\\theta}{\\sigma^2}\\right) = \\frac{k}{\\sigma^2} a(a-\\theta). $$\nThe true gradient $\\nabla_{\\theta} J(\\theta)$ is the expectation of this estimator. Let $a \\sim \\mathcal{N}(\\theta, \\sigma^2)$. We can write $a = \\theta + z$, where $z \\sim \\mathcal{N}(0, \\sigma^2)$.\n$$ \\nabla_{\\theta} J(\\theta) = \\mathbb{E}[\\hat{g}(a)] = \\mathbb{E}\\left[\\frac{k}{\\sigma^2} a(a-\\theta)\\right] = \\frac{k}{\\sigma^2}\\mathbb{E}[(\\theta+z)(z)] = \\frac{k}{\\sigma^2}\\mathbb{E}[\\theta z + z^2]. $$\nUsing the linearity of expectation and noting that for $z \\sim \\mathcal{N}(0, \\sigma^2)$, we have $\\mathbb{E}[z]=0$ and $\\mathbb{E}[z^2]=\\sigma^2$, we get\n$$ \\nabla_{\\theta} J(\\theta) = \\frac{k}{\\sigma^2}(\\theta \\mathbb{E}[z] + \\mathbb{E}[z^2]) = \\frac{k}{\\sigma^2}(0 + \\sigma^2) = k. $$\nThis confirms that $\\hat{g}(a)$ is an unbiased estimator of the true gradient $k$.\n\nNow, we derive the variance of the two estimators.\n\n**1. Variance of the I.I.D. Estimator, $\\operatorname{Var}[\\hat{g}_{\\text{iid}}]$**\n\nThe estimator $\\hat{g}_{\\text{iid}}$ is the average of two i.i.d. random variables, $\\hat{g}(a_1)$ and $\\hat{g}(a_2)$. Due to independence, the variance is\n$$ \\operatorname{Var}[\\hat{g}_{\\text{iid}}] = \\operatorname{Var}\\left[\\frac{1}{2}(\\hat{g}(a_1) + \\hat{g}(a_2))\\right] = \\frac{1}{4}(\\operatorname{Var}[\\hat{g}(a_1)] + \\operatorname{Var}[\\hat{g}(a_2)]) = \\frac{1}{2}\\operatorname{Var}[\\hat{g}(a)]. $$\nWe need to compute $\\operatorname{Var}[\\hat{g}(a)] = \\mathbb{E}[\\hat{g}(a)^2] - (\\mathbb{E}[\\hat{g}(a)])^2$. We already know $\\mathbb{E}[\\hat{g}(a)] = k$.\nLet's compute the second moment $\\mathbb{E}[\\hat{g}(a)^2]$:\n$$ \\mathbb{E}[\\hat{g}(a)^2] = \\mathbb{E}\\left[\\left(\\frac{k}{\\sigma^2} a(a-\\theta)\\right)^2\\right] = \\frac{k^2}{\\sigma^4} \\mathbb{E}[a^2(a-\\theta)^2]. $$\nSubstitute $a = \\theta + z$ where $z \\sim \\mathcal{N}(0, \\sigma^2)$:\n$$ \\mathbb{E}[a^2(a-\\theta)^2] = \\mathbb{E}[(\\theta+z)^2 z^2] = \\mathbb{E}[(\\theta^2+2\\theta z+z^2)z^2] = \\mathbb{E}[\\theta^2 z^2 + 2\\theta z^3 + z^4]. $$\nUsing the moments of a centered Gaussian, $\\mathbb{E}[z^2]=\\sigma^2$, $\\mathbb{E}[z^3]=0$ (by symmetry), and $\\mathbb{E}[z^4]=3\\sigma^4$:\n$$ \\mathbb{E}[a^2(a-\\theta)^2] = \\theta^2\\mathbb{E}[z^2] + 2\\theta\\mathbb{E}[z^3] + \\mathbb{E}[z^4] = \\theta^2\\sigma^2 + 0 + 3\\sigma^4 = \\sigma^2(\\theta^2 + 3\\sigma^2). $$\nSo, the second moment of $\\hat{g}(a)$ is\n$$ \\mathbb{E}[\\hat{g}(a)^2] = \\frac{k^2}{\\sigma^4}\\sigma^2(\\theta^2 + 3\\sigma^2) = k^2\\left(\\frac{\\theta^2}{\\sigma^2} + 3\\right). $$\nThen, the variance of a single-sample estimator is\n$$ \\operatorname{Var}[\\hat{g}(a)] = \\mathbb{E}[\\hat{g}(a)^2] - (\\mathbb{E}[\\hat{g}(a)])^2 = k^2\\left(\\frac{\\theta^2}{\\sigma^2} + 3\\right) - k^2 = k^2\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right). $$\nFinally, the variance of the two-sample i.i.d. estimator is\n$$ \\operatorname{Var}[\\hat{g}_{\\text{iid}}] = \\frac{1}{2}\\operatorname{Var}[\\hat{g}(a)] = \\frac{k^2}{2}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right). $$\n\n**2. Variance of the Antithetic Estimator, $\\operatorname{Var}[\\hat{g}_{\\text{anti}}]$**\n\nThe antithetic estimator uses a correlated pair of samples. Let's first express the estimator in terms of the random variable $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\nThe two components of the estimator are $\\hat{g}(a_{+})$ and $\\hat{g}(a_{-})$.\nFor $a_{+} = \\theta + \\varepsilon$:\n$$ \\hat{g}(a_{+}) = r(a_{+}) \\nabla_{\\theta}\\ln\\pi_{\\theta}(a_{+}) = k(\\theta+\\varepsilon) \\left(\\frac{(\\theta+\\varepsilon)-\\theta}{\\sigma^2}\\right) = \\frac{k}{\\sigma^2}(\\theta+\\varepsilon)\\varepsilon = \\frac{k}{\\sigma^2}(\\theta\\varepsilon + \\varepsilon^2). $$\nFor $a_{-} = \\theta - \\varepsilon$:\n$$ \\hat{g}(a_{-}) = r(a_{-}) \\nabla_{\\theta}\\ln\\pi_{\\theta}(a_{-}) = k(\\theta-\\varepsilon) \\left(\\frac{(\\theta-\\varepsilon)-\\theta}{\\sigma^2}\\right) = \\frac{k}{\\sigma^2}(\\theta-\\varepsilon)(-\\varepsilon) = \\frac{k}{\\sigma^2}(-\\theta\\varepsilon + \\varepsilon^2). $$\nThe antithetic estimator is the average of these two components:\n$$ \\hat{g}_{\\text{anti}} = \\frac{1}{2}(\\hat{g}(a_{+}) + \\hat{g}(a_{-})) = \\frac{1}{2} \\left[ \\frac{k}{\\sigma^2}(\\theta\\varepsilon + \\varepsilon^2) + \\frac{k}{\\sigma^2}(-\\theta\\varepsilon + \\varepsilon^2) \\right] = \\frac{1}{2} \\frac{k}{\\sigma^2} (2\\varepsilon^2) = \\frac{k}{\\sigma^2}\\varepsilon^2. $$\nThe term dependent on $\\theta$ has been cancelled out. Now we can compute the variance of this simplified expression.\n$$ \\operatorname{Var}[\\hat{g}_{\\text{anti}}] = \\operatorname{Var}\\left[\\frac{k}{\\sigma^2}\\varepsilon^2\\right] = \\left(\\frac{k}{\\sigma^2}\\right)^2 \\operatorname{Var}[\\varepsilon^2] = \\frac{k^2}{\\sigma^4}\\operatorname{Var}[\\varepsilon^2]. $$\nThe variance of $\\varepsilon^2$ is $\\operatorname{Var}[\\varepsilon^2] = \\mathbb{E}[(\\varepsilon^2)^2] - (\\mathbb{E}[\\varepsilon^2])^2 = \\mathbb{E}[\\varepsilon^4] - (\\mathbb{E}[\\varepsilon^2])^2$.\nUsing the moments for $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, $\\mathbb{E}[\\varepsilon^2] = \\sigma^2$ and $\\mathbb{E}[\\varepsilon^4] = 3\\sigma^4$, we have\n$$ \\operatorname{Var}[\\varepsilon^2] = 3\\sigma^4 - (\\sigma^2)^2 = 2\\sigma^4. $$\nSubstituting this back, we find the variance of the antithetic estimator:\n$$ \\operatorname{Var}[\\hat{g}_{\\text{anti}}] = \\frac{k^2}{\\sigma^4}(2\\sigma^4) = 2k^2. $$\n\n**3. Variance Reduction Factor, $V(\\theta, \\sigma)$**\n\nFinally, we compute the ratio of the two variances.\n$$ V(\\theta, \\sigma) = \\frac{\\operatorname{Var}[\\hat{g}_{\\text{iid}}]}{\\operatorname{Var}[\\hat{g}_{\\text{anti}}]} = \\frac{\\frac{k^2}{2}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right)}{2k^2}. $$\nSince $k \\neq 0$, we can cancel the $k^2$ terms:\n$$ V(\\theta, \\sigma) = \\frac{\\frac{1}{2}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right)}{2} = \\frac{1}{4}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right) = \\frac{\\theta^2}{4\\sigma^2} + \\frac{2}{4} = \\frac{\\theta^2}{4\\sigma^2} + \\frac{1}{2}. $$\nTo write this as a single simplified expression, we can use a common denominator:\n$$ V(\\theta, \\sigma) = \\frac{\\theta^2 + 2\\sigma^2}{4\\sigma^2}. $$\nThis is the closed-form analytic expression for the variance-reduction factor.", "answer": "$$\\boxed{\\frac{\\theta^2 + 2\\sigma^2}{4\\sigma^2}}$$", "id": "3158001"}, {"introduction": "The power of policy gradient methods lies in their versatility, extending far beyond simple, discrete action spaces. This advanced exercise challenges you to apply the policy gradient framework to a combinatorial action space, specifically the problem of selecting an ordered slate of items. You will work with the Plackett-Luce model, a natural choice for modeling sequential selections, and derive a complete policy gradient estimator. This involves not only calculating the score function for this sequential policy but also designing an effective, state-dependent baseline to reduce variance, integrating multiple core concepts into a single, practical application.", "problem": "Consider a single-state episodic problem in reinforcement learning (RL), where an agent must select a slate of size $k$ without replacement from a ground set of $n$ items, indexed by $\\{1,\\dots,n\\}$. Let the slate be an ordered sequence $\\mathbf{i}=(i_{1},\\dots,i_{k})$ produced sequentially. At step $t$, the available set is $A_{t}$, defined by $A_{1}=\\{1,\\dots,n\\}$ and $A_{t+1}=A_{t}\\setminus\\{i_{t}\\}$ for $t=1,\\dots,k-1$. The policy is a Plackett–Luce model: at step $t$, the probability of choosing item $i\\in A_{t}$ is\n$$\np_{\\theta}(i\\mid A_{t})=\\frac{\\exp\\big(s_{i}\\big)}{\\sum_{j\\in A_{t}}\\exp\\big(s_{j}\\big)},\n$$\nwith linear scores $s_{i}=\\theta^{\\top}x_{i}$, where $x_{i}\\in\\mathbb{R}^{d}$ is a known feature vector and $\\theta\\in\\mathbb{R}^{d}$ is the parameter to be learned. The joint probability of an ordered slate $\\mathbf{i}$ is the product of the stepwise probabilities. The episodic reward is additive over the chosen items:\n$$\nR(\\mathbf{i})=\\sum_{t=1}^{k}r_{i_{t}},\n$$\nwhere $r_{i}$ is a known real-valued utility associated with item $i$. The learning objective is to maximize the expected return\n$$\nJ(\\theta)=\\mathbb{E}_{\\mathbf{i}\\sim\\pi_{\\theta}}\\big[R(\\mathbf{i})\\big],\n$$\nwhere $\\pi_{\\theta}$ denotes the slate distribution induced by the sequential Plackett–Luce policy.\n\nTasks:\n- Starting from the definition of $J(\\theta)$ as an expectation over slates and using only first principles about differentiating expectations of distributions that depend on parameters, derive an unbiased Monte Carlo policy-gradient estimator that may incorporate per-step baselines $b_{t}(A_{t})$ that can depend on $A_{t}$ but not on the sampled item $i_{t}$. Clearly justify why such baselines do not introduce bias.\n- Specialize the general expression to the given Plackett–Luce policy with linear scores and the additive reward. Propose a low-variance choice of $b_{t}(A_{t})$ that exploits the item-wise decomposition of the reward across steps, and simplify the estimator fully in terms of $\\{x_{i}\\}$, $\\{r_{i}\\}$, and the Plackett–Luce probabilities over $A_{t}$.\n- Provide the final single-slate estimator expression for $\\nabla_{\\theta}J(\\theta)$, expressed in closed form. Your final answer must be a single analytic expression; do not present it as an equation.\n\nNo numerical rounding is required. Express your final answer without units.", "solution": "The user wants me to solve this problem.\n### Step 1: Problem Validation\n\nI will first validate the problem statement.\n\n#### Step 1.1: Extract Givens\n\n-   **Problem Setting**: Single-state episodic reinforcement learning.\n-   **Action**: Select a slate (ordered sequence) $\\mathbf{i}=(i_{1},\\dots,i_{k})$ of size $k$ without replacement from a ground set of $n$ items, $\\{1,\\dots,n\\}$.\n-   **State/Available Set**: $A_{1}=\\{1,\\dots,n\\}$ and $A_{t+1}=A_{t}\\setminus\\{i_{t}\\}$ for $t=1,\\dots,k-1$.\n-   **Policy**: Plackett–Luce model.\n-   **Stepwise Probability**: $p_{\\theta}(i\\mid A_{t})=\\frac{\\exp(s_{i})}{\\sum_{j\\in A_{t}}\\exp(s_{j})}$ for $i\\in A_{t}$.\n-   **Item Scores**: $s_{i}=\\theta^{\\top}x_{i}$, with $x_{i}\\in\\mathbb{R}^{d}$ (known feature vector) and $\\theta\\in\\mathbb{R}^{d}$ (parameters).\n-   **Slate Probability**: $\\pi_{\\theta}(\\mathbf{i}) = \\prod_{t=1}^{k} p_{\\theta}(i_{t}\\mid A_{t})$.\n-   **Reward**: Additive, $R(\\mathbf{i})=\\sum_{t=1}^{k}r_{i_{t}}$, with $r_{i}$ (known real-valued utility).\n-   **Objective Function**: Maximize $J(\\theta)=\\mathbb{E}_{\\mathbf{i}\\sim\\pi_{\\theta}}[R(\\mathbf{i})]$.\n-   **Task Requirements**:\n    1.  Derive a general unbiased Monte Carlo policy-gradient estimator with per-step baselines $b_{t}(A_{t})$. Justify the absence of bias.\n    2.  Specialize the estimator for the given Plackett-Luce policy, linear scores, and additive reward. Propose a specific low-variance baseline $b_{t}(A_{t})$ and simplify the expression.\n    3.  Provide the final single-slate estimator expression for $\\nabla_{\\theta}J(\\theta)$.\n\n#### Step 1.2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is well-established within the field of statistical learning and reinforcement learning. The Plackett–Luce model is a standard choice for modeling distributions over rankings, and policy gradient methods are a cornerstone of RL. The setup is scientifically sound.\n-   **Well-Posed**: The objective function $J(\\theta)$ is well-defined and differentiable with respect to $\\theta$. The task of deriving an estimator for its gradient is a concrete mathematical problem.\n-   **Objective**: The problem is stated using precise mathematical definitions and free of subjective language.\n-   **Incomplete or Contradictory Setup**: The problem is self-contained. All necessary components (policy, reward, objective) are clearly defined.\n-   **Unrealistic or Infeasible**: The setup is a standard model for slate optimization problems, commonly found in applications like recommender systems. It is realistic and feasible.\n-   **Ill-Posed or Poorly Structured**: The problem is structured as a guided derivation, which is clear and logical.\n-   **Other Flaws**: The problem is neither trivial nor pseudo-profound and lies firmly within the domain of scientific inquiry.\n\n#### Step 1.3: Verdict and Action\n\nThe problem is **valid**. I will proceed with the solution.\n\n### Step 2: Derivation and Solution\n\nThe objective is to maximize the expected return $J(\\theta) = \\mathbb{E}_{\\mathbf{i}\\sim\\pi_{\\theta}}[R(\\mathbf{i})]$. We can write this as a sum over all possible slates $\\mathbf{i}$:\n$$\nJ(\\theta) = \\sum_{\\mathbf{i}} \\pi_{\\theta}(\\mathbf{i}) R(\\mathbf{i})\n$$\nThe gradient with respect to the parameters $\\theta$ is:\n$$\n\\nabla_{\\theta} J(\\theta) = \\nabla_{\\theta} \\sum_{\\mathbf{i}} \\pi_{\\theta}(\\mathbf{i}) R(\\mathbf{i}) = \\sum_{\\mathbf{i}} (\\nabla_{\\theta} \\pi_{\\theta}(\\mathbf{i})) R(\\mathbf{i})\n$$\nUsing the log-derivative trick, $\\nabla_{\\theta} \\pi_{\\theta}(\\mathbf{i}) = \\pi_{\\theta}(\\mathbf{i}) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{i})$, we can express the gradient as an expectation:\n$$\n\\nabla_{\\theta} J(\\theta) = \\sum_{\\mathbf{i}} \\pi_{\\theta}(\\mathbf{i}) (\\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{i})) R(\\mathbf{i}) = \\mathbb{E}_{\\mathbf{i}\\sim\\pi_{\\theta}} \\big[ R(\\mathbf{i}) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{i}) \\big]\n$$\nThis gives a general Monte Carlo estimator for a sampled slate $\\mathbf{i}$: $\\hat{g}(\\mathbf{i}) = R(\\mathbf{i}) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{i})$.\n\nThe log-probability of a slate $\\mathbf{i}=(i_{1},\\dots,i_{k})$ decomposes into a sum over the sequential steps:\n$$\n\\log \\pi_{\\theta}(\\mathbf{i}) = \\log \\left( \\prod_{t=1}^{k} p_{\\theta}(i_{t} \\mid A_{t}) \\right) = \\sum_{t=1}^{k} \\log p_{\\theta}(i_{t} \\mid A_{t})\n$$\nTherefore, its gradient is:\n$$\n\\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{i}) = \\sum_{t=1}^{k} \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t})\n$$\nSubstituting this into the expectation for $\\nabla_{\\theta} J(\\theta)$:\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\mathbf{i}\\sim\\pi_{\\theta}} \\left[ R(\\mathbf{i}) \\sum_{t=1}^{k} \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t}) \\right] = \\sum_{t=1}^{k} \\mathbb{E}_{\\mathbf{i}\\sim\\pi_{\\theta}} \\left[ R(\\mathbf{i}) \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t}) \\right]\n$$\nBy causality, decisions at step $t$ can only influence future rewards, not past ones. The reward term $R(\\mathbf{i}) = \\sum_{t'=1}^{k} r_{i_{t'}}$ can be split into past rewards ($\\sum_{t'=1}^{t-1} r_{i_{t'}}$) and future rewards (reward-to-go, $\\sum_{t'=t}^{k} r_{i_{t'}}$). The expectation of the product of past rewards and the gradient term at step $t$ is zero:\n$$\n\\mathbb{E} \\left[ \\left(\\sum_{t'=1}^{t-1} r_{i_{t'}}\\right) \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t}) \\right] = \\mathbb{E}_{\\mathbf{i}_{t}} \\left[ \\left(\\sum_{t'=1}^{t-1} r_{i_{t'}}\\right) \\mathbb{E}_{i_t|A_t} \\left[ \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t}) \\right] \\right] = 0\n$$\nThis is because $\\mathbb{E}_{i_t|A_t} [ \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t}) ] = \\sum_{i \\in A_t} p_{\\theta}(i|A_t) \\frac{\\nabla_{\\theta} p_{\\theta}(i|A_t)}{p_{\\theta}(i|A_t)} = \\nabla_{\\theta} \\sum_{i \\in A_t} p_{\\theta}(i|A_t) = \\nabla_{\\theta}(1) = \\mathbf{0}$.\nThus, we can replace the total reward $R(\\mathbf{i})$ with the reward-to-go from step $t$:\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\mathbf{i}\\sim\\pi_{\\theta}} \\left[ \\sum_{t=1}^{k} \\left( \\sum_{t'=t}^{k} r_{i_{t'}} \\right) \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t}) \\right]\n$$\nTo reduce the variance of this estimator, we can introduce a baseline $b_{t}(A_{t})$ that depends on the available set $A_{t}$ but not on the specific item $i_{t}$ sampled at step $t$. The baseline is subtracted from the reward-to-go. The estimator remains unbiased because the expectation of the baseline term is zero:\n$$\n\\mathbb{E} \\left[ b_{t}(A_{t}) \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t}) \\right] = \\mathbb{E}_{\\mathbf{i}_{t}} \\left[ b_{t}(A_{t}) \\mathbb{E}_{i_t|A_t} \\left[ \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t}) \\right] \\right] = \\mathbb{E}_{\\mathbf{i}_{t}} [b_{t}(A_{t}) \\cdot \\mathbf{0}] = \\mathbf{0}\n$$\nTherefore, the general form of an unbiased policy-gradient estimator with per-step baselines, for a single sampled slate $\\mathbf{i}$, is:\n$$\n\\hat{g}(\\mathbf{i}) = \\sum_{t=1}^{k} \\left( \\left( \\sum_{t'=t}^{k} r_{i_{t'}} \\right) - b_{t}(A_{t}) \\right) \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t})\n$$\n\nNow, we specialize this to the given Plackett-Luce policy with linear scores $s_{i}=\\theta^{\\top}x_{i}$. The log-probability of selecting item $i$ from set $A_{t}$ is:\n$$\n\\log p_{\\theta}(i \\mid A_{t}) = \\log\\left(\\frac{\\exp(\\theta^{\\top}x_{i})}{\\sum_{j\\in A_{t}}\\exp(\\theta^{\\top}x_{j})}\\right) = \\theta^{\\top}x_{i} - \\log\\left(\\sum_{j\\in A_{t}}\\exp(\\theta^{\\top}x_{j})\\right)\n$$\nThe gradient with respect to $\\theta$ is:\n$$\n\\nabla_{\\theta} \\log p_{\\theta}(i \\mid A_{t}) = x_{i} - \\frac{\\sum_{j\\in A_{t}} \\exp(\\theta^{\\top}x_{j}) x_{j}}{\\sum_{j\\in A_{t}}\\exp(\\theta^{\\top}x_{j})} = x_{i} - \\sum_{j\\in A_{t}} p_{\\theta}(j|A_t) x_{j} = x_{i} - \\mathbb{E}_{j \\sim p_{\\theta}(\\cdot|A_t)}[x_{j}]\n$$\nThis is the feature vector of the chosen item, centered by the expected feature vector over the available items.\n\nFor the second task, we propose a low-variance choice for the baseline $b_{t}(A_{t})$. An effective baseline should approximate the expected reward-to-go. Exploiting the item-wise decomposition of the reward, a natural and computationally simple choice is the expected immediate reward at step $t$, conditioned on the available set $A_t$:\n$$\nb_{t}(A_{t}) = \\mathbb{E}_{j \\sim p_{\\theta}(\\cdot|A_{t})}[r_{j}] = \\sum_{j \\in A_{t}} r_{j} p_{\\theta}(j \\mid A_{t})\n$$\nThis baseline is \"low-variance\" as it centers the immediate component $r_{i_t}$ of the reward-to-go, reducing variance compared to a zero baseline. It depends on $A_{t}$ (through the set of items and the probabilities defined over it) but not on the sampled item $i_t$.\n\nSubstituting the specific forms of the score function gradient and the proposed baseline into the general estimator gives the final expression for a single-slate sample $\\mathbf{i}=(i_1, \\dots, i_k)$:\n$$\n\\hat{g}(\\mathbf{i}) = \\sum_{t=1}^{k} \\left( \\left( \\sum_{t'=t}^{k} r_{i_{t'}} \\right) - \\sum_{j \\in A_{t}} r_{j} p_{\\theta}(j|A_t) \\right) \\left( x_{i_{t}} - \\sum_{j \\in A_{t}} x_{j} p_{\\theta}(j|A_t) \\right)\n$$\nwhere $A_t = \\{1, \\dots, n\\} \\setminus \\{i_1, \\dots, i_{t-1}\\}$ and $p_{\\theta}(j|A_t) = \\frac{\\exp(\\theta^\\top x_j)}{\\sum_{l \\in A_t}\\exp(\\theta^\\top x_l)}$. This is the fully simplified expression in terms of the given quantities.\n\nThe final single-slate estimator for $\\nabla_{\\theta} J(\\theta)$ is this expression.", "answer": "$$\\boxed{\\sum_{t=1}^{k} \\left( \\left( \\sum_{t'=t}^{k} r_{i_{t'}} \\right) - \\frac{\\sum_{j \\in A_t} r_j \\exp(\\theta^\\top x_j)}{\\sum_{l \\in A_t} \\exp(\\theta^\\top x_l)} \\right) \\left( x_{i_t} - \\frac{\\sum_{j \\in A_t} x_j \\exp(\\theta^\\top x_j)}{\\sum_{l \\in A_t} \\exp(\\theta^\\top x_l)} \\right)}$$", "id": "3158005"}]}