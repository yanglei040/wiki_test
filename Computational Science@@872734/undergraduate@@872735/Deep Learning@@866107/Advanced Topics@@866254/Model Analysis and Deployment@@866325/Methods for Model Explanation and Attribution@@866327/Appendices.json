{"hands_on_practices": [{"introduction": "This exercise gets to the heart of why simple gradient-based attributions can be misleading. By analyzing a simple Rectified Linear Unit (ReLU) network, you will see firsthand how the \"saturation\" problem can cause gradients to vanish, wrongly suggesting a feature is unimportant. You will then derive Integrated Gradients from first principles to see how this path-based method overcomes saturation to provide more faithful explanations [@problem_id:3150467].", "problem": "You are given a single hidden-unit Rectified Linear Unit (ReLU) model defined by the scalar-valued function $f(\\mathbf{x})=\\max\\left(0,\\mathbf{w}^\\top \\mathbf{x}+b\\right)$, where $\\mathbf{w}\\in\\mathbb{R}^d$, $b\\in\\mathbb{R}$, and $\\mathbf{x}\\in\\mathbb{R}^d$. Consider two attribution methods for a target input $\\mathbf{x}$:\n- Raw gradients: $a_i=\\frac{\\partial f}{\\partial x_i}(\\mathbf{x})$.\n- Integrated gradients (from a baseline $\\mathbf{x}'$ along the straight-line path): for each coordinate $i$, the attribution is\n$$\\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=(x_i-x'_i)\\int_{0}^{1}\\frac{\\partial f}{\\partial x_i}\\big(\\mathbf{x}'+\\alpha(\\mathbf{x}-\\mathbf{x}')\\big)\\,d\\alpha.$$\n\nYour task is to derive, implement, and compare the two attribution methods using only the fundamental definitions of gradients, the ReLU function, and path integrals, without relying on any pre-derived shortcut formulas. Work entirely in purely mathematical terms. No physical units are involved.\n\nRequirements:\n1) Starting from the definitions of gradient, the ReLU function, and the line integral along a straight path, derive a closed-form expression for $\\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')$ specialized to $f(\\mathbf{x})=\\max\\left(0,\\mathbf{w}^\\top \\mathbf{x}+b\\right)$, expressed only in terms of $\\mathbf{w}$, $b$, $\\mathbf{x}$, and $\\mathbf{x}'$. You must reason from first principles about when the derivative of the ReLU is active or inactive along the straight-line path.\n2) Define the raw gradient attribution at $\\mathbf{x}$ by $a(\\mathbf{x})=\\nabla_{\\mathbf{x}} f(\\mathbf{x})$, with the convention that when $\\mathbf{w}^\\top\\mathbf{x}+b=0$ the gradient is taken to be the zero vector for definiteness.\n3) Define two baselines $\\mathbf{x}'$:\n   - Zero baseline $\\mathbf{x}'=\\mathbf{0}$.\n   - A principled baseline $\\mathbf{x}'$ on the ReLU decision boundary: choose the orthogonal projection of $\\mathbf{x}$ onto the hyperplane $\\{\\mathbf{z}:\\mathbf{w}^\\top \\mathbf{z}+b=0\\}$, i.e.,\n   $$\\mathbf{x}'=\\mathbf{x}-\\frac{\\mathbf{w}^\\top \\mathbf{x}+b}{\\lVert \\mathbf{w}\\rVert_2^2}\\,\\mathbf{w},$$\n   whenever $\\lVert \\mathbf{w}\\rVert_2>0$. In the degenerate case $\\lVert \\mathbf{w}\\rVert_2=0$ (so $f(\\mathbf{x})=\\max(0,b)$), define the boundary baseline by $\\mathbf{x}'=\\mathbf{x}$ so that the straight-line path is trivial.\n4) Define a saturation diagnostic at $\\mathbf{x}$ relative to the zero baseline as the Boolean\n$$S(\\mathbf{x})=\\big(\\lVert \\nabla_{\\mathbf{x}} f(\\mathbf{x})\\rVert_2\\le \\varepsilon\\big)\\ \\wedge\\ \\big(\\lVert \\mathrm{IG}(\\mathbf{x},\\mathbf{0})\\rVert_2> \\varepsilon\\big),$$\nwhere $\\varepsilon=10^{-9}$ is a fixed tolerance used for numerical comparisons.\n5) Verify the completeness relation for integrated gradients for both baselines:\n   $$C_0(\\mathbf{x})=\\left|\\sum_{i=1}^d \\mathrm{IG}_i(\\mathbf{x},\\mathbf{0})-\\big(f(\\mathbf{x})-f(\\mathbf{0})\\big)\\right|\\le \\varepsilon,$$\n   $$C_b(\\mathbf{x})=\\left|\\sum_{i=1}^d \\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')-\\big(f(\\mathbf{x})-f(\\mathbf{x}')\\big)\\right|\\le \\varepsilon,$$\n   where $\\mathbf{x}'$ is the boundary baseline as defined in item $3$.\n6) Implement a program that, for each test case below, computes:\n   - The saturation diagnostic $S(\\mathbf{x})$ (Boolean).\n   - The completeness checks $C_0(\\mathbf{x})\\le\\varepsilon$ and $C_b(\\mathbf{x})\\le\\varepsilon$ (Booleans).\n   Use exact closed-form reasoning for the integrated gradients derived in item $1$ rather than numerical quadrature.\n\nTest suite (each case gives $(\\mathbf{w},b,\\mathbf{x})$):\n- Case $1$: $\\mathbf{w}=[1.0,-2.0]$, $b=0.5$, $\\mathbf{x}=[2.0,1.0]$.\n- Case $2$: $\\mathbf{w}=[1.0,1.0]$, $b=1.0$, $\\mathbf{x}=[-2.0,-2.0]$.\n- Case $3$: $\\mathbf{w}=[1.0,3.0]$, $b=-4.0$, $\\mathbf{x}=[1.0,1.0]$.\n- Case $4$: $\\mathbf{w}=[2.0,-2.0]$, $b=1.0$, $\\mathbf{x}=[1.0,1.0]$.\n- Case $5$: $\\mathbf{w}=[0.0,0.0]$, $b=1.5$, $\\mathbf{x}=[3.0,-7.0]$.\n\nAngle units are not relevant. There are no physical units.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each case $k\\in\\{1,2,3,4,5\\}$, output the triple $\\big(S(\\mathbf{x}^{(k)}),\\ C_0(\\mathbf{x}^{(k)})\\le\\varepsilon,\\ C_b(\\mathbf{x}^{(k)})\\le\\varepsilon\\big)$ as three Booleans.\n- Concatenate the triples for all cases into one flat list of length $15$ and print it in a single line, e.g., $[r_1,r_2,\\ldots,r_{15}]$ where each $r_j$ is either $\\mathrm{True}$ or $\\mathrm{False}$.", "solution": "We begin from fundamental definitions. The model is $f(\\mathbf{x})=\\max(0,z(\\mathbf{x}))$ with $z(\\mathbf{x})=\\mathbf{w}^\\top\\mathbf{x}+b$. The gradient of $f$ with respect to $\\mathbf{x}$ exists everywhere except where $z(\\mathbf{x})=0$; we adopt the convention that at $z(\\mathbf{x})=0$ the gradient is the zero vector. Thus,\n$$\n\\nabla_{\\mathbf{x}} f(\\mathbf{x})=\n\\begin{cases}\n\\mathbf{0}, & \\text{if } z(\\mathbf{x})\\le 0,\\\\\n\\mathbf{w}, & \\text{if } z(\\mathbf{x})>0.\n\\end{cases}\n$$\n\nIntegrated gradients (from a straight-line path) are defined by the path integral of the gradient projected onto the input difference. For a baseline $\\mathbf{x}'$ and target $\\mathbf{x}$, let $\\gamma(\\alpha)=\\mathbf{x}'+\\alpha(\\mathbf{x}-\\mathbf{x}')$ for $\\alpha\\in[0,1]$. The $i$-th coordinate integrated gradient is\n$$\n\\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=(x_i-x'_i)\\int_{0}^{1}\\frac{\\partial f}{\\partial x_i}\\big(\\gamma(\\alpha)\\big)\\,d\\alpha.\n$$\nBy the structure of $f$, along the path the derivative toggles between $\\mathbf{0}$ and $\\mathbf{w}$ depending on the sign of $z(\\gamma(\\alpha))$. We compute $z(\\gamma(\\alpha))$:\n$$\nz(\\gamma(\\alpha))=\\mathbf{w}^\\top\\big(\\mathbf{x}'+\\alpha(\\mathbf{x}-\\mathbf{x}')\\big)+b=\\underbrace{\\big(\\mathbf{w}^\\top \\mathbf{x}'+b\\big)}_{z_0}+\\alpha\\underbrace{\\mathbf{w}^\\top(\\mathbf{x}-\\mathbf{x}')}_{c}=z_0+\\alpha c.\n$$\nThus $z(\\gamma(\\alpha))$ is an affine function of $\\alpha$. Define the crossing point (if any) by\n$$\n\\alpha^\\star=-\\frac{z_0}{c},\\quad\\text{when }c\\ne 0.\n$$\nWe analyze cases:\n\n- If $\\lVert \\mathbf{w}\\rVert_2=0$, then $f(\\mathbf{x})=\\max(0,b)$ is constant in $\\mathbf{x}$ and $\\nabla_{\\mathbf{x}}f(\\mathbf{x})=\\mathbf{0}$ for all $\\mathbf{x}$. Therefore $\\mathrm{IG}(\\mathbf{x},\\mathbf{x}')=\\mathbf{0}$ for any $\\mathbf{x},\\mathbf{x}'$, and completeness holds because $f(\\mathbf{x})-f(\\mathbf{x}')=0$ for any $\\mathbf{x},\\mathbf{x}'$ when $b>0$, and equals $0$ as well when $b\\le 0$.\n\n- Assume $\\lVert \\mathbf{w}\\rVert_2>0$. Then along the path, the gradient is $\\mathbf{w}$ on the subset of $\\alpha\\in[0,1]$ where $z(\\gamma(\\alpha))>0$, and $\\mathbf{0}$ where $z(\\gamma(\\alpha))\\le 0$. Since $z(\\gamma(\\alpha))=z_0+\\alpha c$ is affine, the set where it is positive is an interval whose length equals:\n  - If $c=0$, then $z(\\gamma(\\alpha))\\equiv z_0$. The positive-length measure $L$ of the active region is $L=1$ if $z_0>0$ and $L=0$ otherwise.\n  - If $c>0$, then $z(\\gamma(\\alpha))>0$ for $\\alpha>\\alpha^\\star$; the active length is $L=\\begin{cases}0,&\\alpha^\\star\\ge 1,\\\\1,&\\alpha^\\star\\le 0,\\\\1-\\alpha^\\star,&\\text{otherwise.}\\end{cases}$\n  - If $c<0$, then $z(\\gamma(\\alpha))>0$ for $\\alpha<\\alpha^\\star$; the active length is $L=\\begin{cases}0,&\\alpha^\\star\\le 0,\\\\1,&\\alpha^\\star\\ge 1,\\\\\\alpha^\\star,&\\text{otherwise.}\\end{cases}$\n\nSince on the active set the gradient equals $\\mathbf{w}$ and on the inactive set it is $\\mathbf{0}$, we have for each $i$:\n$$\n\\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=(x_i-x'_i)\\cdot w_i \\cdot L.\n$$\nCollecting across coordinates, the vector form is\n$$\n\\mathrm{IG}(\\mathbf{x},\\mathbf{x}')=\\big(\\mathbf{x}-\\mathbf{x}'\\big)\\odot \\mathbf{w}\\cdot L,\n$$\nwhere $\\odot$ denotes elementwise product and $L$ is the active-length scalar determined above.\n\nWhy this is correct: The integral reduces to the measure of the set where the integrand is nonzero because the derivative is piecewise constant along the straight path. This leverages only the definition of the ReLU derivative and the path integral.\n\nCompleteness property: The sum of integrated gradients equals the line integral of the gradient along the path in the direction of $\\mathbf{x}-\\mathbf{x}'$:\n$$\n\\sum_{i=1}^d \\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=\\int_0^1 \\nabla_{\\mathbf{x}} f\\big(\\gamma(\\alpha)\\big)^\\top (\\mathbf{x}-\\mathbf{x}')\\,d\\alpha.\n$$\nSince $\\nabla_{\\mathbf{x}} f(\\gamma(\\alpha))$ is $\\mathbf{w}$ on the active region and $\\mathbf{0}$ otherwise, this integral equals $L\\cdot \\mathbf{w}^\\top(\\mathbf{x}-\\mathbf{x}')=L\\cdot c$. But\n$$\nf(\\mathbf{x})-f(\\mathbf{x}')=\\max(0,z_0+c)-\\max(0,z_0).\n$$\nGiven the affine form and piecewise-constant derivative except possibly at a single $\\alpha^\\star$ of measure zero, the fundamental theorem of calculus along the path implies\n$$\n\\int_0^1 \\frac{d}{d\\alpha} f\\big(\\gamma(\\alpha)\\big)\\,d\\alpha=f(\\gamma(1))-f(\\gamma(0))=f(\\mathbf{x})-f(\\mathbf{x}'),\n$$\nand since $\\frac{d}{d\\alpha} f(\\gamma(\\alpha))=\\nabla_{\\mathbf{x}} f(\\gamma(\\alpha))^\\top (\\mathbf{x}-\\mathbf{x}')$, we obtain\n$$\n\\sum_{i=1}^d \\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=f(\\mathbf{x})-f(\\mathbf{x}').\n$$\nOur explicit $L$-based expression satisfies this equality case-wise, including the degenerate cases $c=0$ and $\\lVert\\mathbf{w}\\rVert_2=0$.\n\nPrincipled baseline on the ReLU boundary: For $\\lVert\\mathbf{w}\\rVert_2>0$, define the boundary baseline as the orthogonal projection of $\\mathbf{x}$ onto the hyperplane $\\mathbf{w}^\\top \\mathbf{z}+b=0$:\n$$\n\\mathbf{x}'=\\mathbf{x}-\\frac{\\mathbf{w}^\\top \\mathbf{x}+b}{\\lVert \\mathbf{w}\\rVert_2^2}\\,\\mathbf{w}.\n$$\nThis choice satisfies $\\mathbf{w}^\\top \\mathbf{x}'+b=0$, hence $f(\\mathbf{x}')=0$. For this baseline, $c=\\mathbf{w}^\\top(\\mathbf{x}-\\mathbf{x}')=\\mathbf{w}^\\top \\mathbf{x}+b$, and $z_0=0$. Therefore $\\alpha^\\star=0$ when $c\\ne 0$. If $c>0$ (the active case at $\\mathbf{x}$), the active length is $L=1$ and\n$$\n\\mathrm{IG}(\\mathbf{x},\\mathbf{x}')=(\\mathbf{x}-\\mathbf{x}')\\odot \\mathbf{w}.\n$$\nSumming yields\n$$\n\\sum_i \\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=\\mathbf{w}^\\top(\\mathbf{x}-\\mathbf{x}')=\\mathbf{w}^\\top\\mathbf{x}+b=f(\\mathbf{x})-f(\\mathbf{x}')=f(\\mathbf{x}),\n$$\nverifying completeness and that attributions exactly account for the output. If $c\\le 0$ (inactive at $\\mathbf{x}$ or on the boundary), $L=0$ and $\\mathrm{IG}(\\mathbf{x},\\mathbf{x}')=\\mathbf{0}$, again matching completeness since $f(\\mathbf{x})=f(\\mathbf{x}')=0$. In the degenerate case $\\lVert\\mathbf{w}\\rVert_2=0$, no such hyperplane exists unless $b=0$. We define $\\mathbf{x}'=\\mathbf{x}$, which yields the trivial path, $\\mathrm{IG}=\\mathbf{0}$, and $f(\\mathbf{x})-f(\\mathbf{x}')=0$.\n\nSaturation diagnosis: Raw gradients can be misleadingly small (or zero) when $z(\\mathbf{x})\\le 0$, even though relative to a baseline such as $\\mathbf{0}$ the integrated gradients can be nonzero and reveal that moving toward the baseline changes the model output. We formalize this via\n$$\nS(\\mathbf{x})=\\big(\\lVert \\nabla_{\\mathbf{x}} f(\\mathbf{x})\\rVert_2\\le \\varepsilon\\big)\\ \\wedge\\ \\big(\\lVert \\mathrm{IG}(\\mathbf{x},\\mathbf{0})\\rVert_2> \\varepsilon\\big),\n$$\nwith $\\varepsilon=10^{-9}$.\n\nAlgorithm summary for each test case:\n- Compute $f(\\mathbf{x})$, $f(\\mathbf{0})$, and the raw gradient at $\\mathbf{x}$ using the definition of the ReLU derivative.\n- Compute the zero-baseline integrated gradients using the closed-form derived above with $L$ determined by $z_0=\\mathbf{w}^\\top \\mathbf{0}+b=b$, $c=\\mathbf{w}^\\top(\\mathbf{x}-\\mathbf{0})=\\mathbf{w}^\\top\\mathbf{x}$, including the $c=0$ and $\\lVert\\mathbf{w}\\rVert_2=0$ cases.\n- Construct the boundary baseline $\\mathbf{x}'$ as the orthogonal projection if $\\lVert\\mathbf{w}\\rVert_2>0$, otherwise set $\\mathbf{x}'=\\mathbf{x}$. Compute the corresponding integrated gradients similarly.\n- Form $S(\\mathbf{x})$ and check completeness $C_0(\\mathbf{x})\\le \\varepsilon$ and $C_b(\\mathbf{x})\\le \\varepsilon$.\n\nApplying this procedure to the provided test suite ensures coverage of: an active case, an inactive case where zero-baseline path crosses into the active region (revealing saturation), a boundary case, a case with a path parallel to the boundary ($c=0$ but active), and a degenerate case with $\\mathbf{w}=\\mathbf{0}$. The final output is the flattened list of Booleans in the specified order.", "answer": "```python\nimport numpy as np\n\ndef relu(z):\n    return np.maximum(0.0, z)\n\ndef f_val(x, w, b):\n    return relu(np.dot(w, x) + b)\n\ndef grad_raw(x, w, b, eps=0.0):\n    # At z > 0: gradient is w; at z <= 0: zero vector (by convention at z=0)\n    z = np.dot(w, x) + b\n    if z > 0:\n        return w.copy()\n    else:\n        return np.zeros_like(w)\n\ndef ig_closed_form(x, x_base, w, b, tol=1e-12):\n    # Integrated gradients along straight-line path from x_base to x\n    # Handle degenerate w=0: IG is zero vector\n    if np.allclose(w, 0.0, atol=tol):\n        return np.zeros_like(w)\n    # Compute z0 and c\n    z0 = np.dot(w, x_base) + b\n    dx = x - x_base\n    c = np.dot(w, dx)\n    # Active length L determination\n    if abs(c) <= tol:\n        # z(alpha) = z0 constant\n        if z0 > 0:\n            L = 1.0\n        else:\n            L = 0.0\n    else:\n        a_star = -z0 / c\n        if c > 0:\n            if a_star >= 1.0:\n                L = 0.0\n            elif a_star <= 0.0:\n                L = 1.0\n            else:\n                L = 1.0 - a_star\n        else:  # c < 0\n            if a_star <= 0.0:\n                L = 0.0\n            elif a_star >= 1.0:\n                L = 1.0\n            else:\n                L = a_star\n    return dx * w * L\n\ndef boundary_baseline(x, w, b, tol=1e-12):\n    # If w is zero vector, return x (trivial path)\n    if np.allclose(w, 0.0, atol=tol):\n        return x.copy()\n    ww = np.dot(w, w)\n    # Projection of x onto the hyperplane w^T z + b = 0\n    factor = (np.dot(w, x) + b) / ww\n    return x - factor * w\n\ndef solve():\n    # Define epsilon for numerical checks\n    eps = 1e-9\n\n    # Test cases: (w, b, x)\n    test_cases = [\n        (np.array([1.0, -2.0]), 0.5, np.array([2.0, 1.0])),     # Case 1\n        (np.array([1.0, 1.0]), 1.0, np.array([-2.0, -2.0])),    # Case 2\n        (np.array([1.0, 3.0]), -4.0, np.array([1.0, 1.0])),     # Case 3\n        (np.array([2.0, -2.0]), 1.0, np.array([1.0, 1.0])),     # Case 4\n        (np.array([0.0, 0.0]), 1.5, np.array([3.0, -7.0])),     # Case 5 (degenerate w=0)\n    ]\n\n    results = []\n    for w, b, x in test_cases:\n        # Raw gradient and norms\n        g = grad_raw(x, w, b)\n        g_norm = np.linalg.norm(g, 2)\n\n        # Zero baseline IG\n        x0 = np.zeros_like(x)\n        ig0 = ig_closed_form(x, x0, w, b)\n        ig0_norm = np.linalg.norm(ig0, 2)\n\n        # Saturation diagnostic\n        saturation = (g_norm <= eps) and (ig0_norm > eps)\n\n        # Boundary baseline and IG\n        xb = boundary_baseline(x, w, b)\n        igb = ig_closed_form(x, xb, w, b)\n\n        # Completeness checks\n        comp0_err = abs(np.sum(ig0) - (f_val(x, w, b) - f_val(x0, w, b)))\n        compb_err = abs(np.sum(igb) - (f_val(x, w, b) - f_val(xb, w, b)))\n        comp0_ok = comp0_err <= eps\n        compb_ok = compb_err <= eps\n\n        # Append booleans in the specified order per case\n        results.extend([saturation, comp0_ok, compb_ok])\n\n    # Final print statement in the exact required format: single-line list\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3150467"}, {"introduction": "Real-world models often learn complex feature interactions, where the effect of one feature depends on the value of another. This practice explores how two foundational attribution methods, SHAP and Integrated Gradients, account for these interaction effects in a simple polynomial model. By deriving the attributions from their core definitions, you will not only verify the celebrated Shapley interaction index but also uncover the deep connection between these two methods [@problem_id:3150523].", "problem": "You are given a two-feature deterministic model defined by the output function $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ with $f(x_1,x_2)=\\beta_1 x_1+\\beta_2 x_2+\\beta_{12} x_1 x_2$. Your task is to, from first principles, implement and compare attributions from Shapley Additive Explanations (SHAP) and Integrated Gradients (IG) under a zero baseline, and to verify the Shapley interaction index using a difference-of-differences computation. Use only the mathematical definitions of SHAP and IG as foundational starting points and do not rely on any pre-derived shortcut formulas.\n\nBegin from the following fundamental bases:\n- The Shapley value from cooperative game theory: it allocates the total value $f(x_1,x_2)-f(0,0)$ among features $x_1$ and $x_2$ by averaging each featureâ€™s marginal contribution over all feature orderings.\n- The Integrated Gradients (IG) method: for a differentiable function $f$, the attribution for feature $x_i$ relative to a baseline $x_i'=0$ is defined by a line integral of the partial derivative of $f$ along the straight-line path from the baseline to $x$, scaled by $(x_i-x_i')$.\n- The Shapley interaction index: for two features, the pairwise interaction is the part of the value that cannot be explained by either feature alone, obtainable via a discrete second-order difference of $f$ evaluated at subsets.\n\nImplement the following, all with baseline $(0,0)$:\n1. Compute the SHAP attributions for $x_1$ and $x_2$, derived from the Shapley value definition applied to the function $f(x_1,x_2)$.\n2. Compute the IG attributions for $x_1$ and $x_2$, derived directly from the line-integral definition along the path $\\alpha\\mapsto(\\alpha x_1,\\alpha x_2)$ for $\\alpha\\in[0,1]$.\n3. Compute the Shapley interaction index for the pair $(x_1,x_2)$ using the discrete second-order difference $f(x_1,x_2)-f(x_1,0)-f(0,x_2)+f(0,0)$.\n\nThen, for each test case, quantify the following checks as non-negative real numbers:\n- The absolute difference between the sum of IG attributions and $f(x_1,x_2)$.\n- The absolute difference between the SHAP attribution for $x_1$ and the IG attribution for $x_1$.\n- The absolute difference between the SHAP attribution for $x_2$ and the IG attribution for $x_2$.\n- The absolute difference between the Shapley interaction index computed via discrete second-order difference and $\\beta_{12} x_1 x_2$.\n- The absolute difference between the portion of interaction allocated to $x_1$ by IG, that is $IG_1-\\beta_1 x_1$, and half of the pairwise interaction, that is $\\tfrac{1}{2}\\beta_{12} x_1 x_2$.\n- The absolute difference between the portion of interaction allocated to $x_2$ by IG, that is $IG_2-\\beta_2 x_2$, and half of the pairwise interaction, that is $\\tfrac{1}{2}\\beta_{12} x_1 x_2$.\n\nUse the following test suite of parameter values $(\\beta_1,\\beta_2,\\beta_{12},x_1,x_2)$:\n- Test $1$: $(1.5,2.0,0.8,1.0,3.0)$.\n- Test $2$: $(1.0,1.0,0.0,2.0,-1.0)$.\n- Test $3$: $(-0.5,0.75,-1.2,1.2,0.5)$.\n- Test $4$: $(3.0,-2.0,0.5,0.0,4.0)$.\n- Test $5$: $(0.0,0.0,5.0,0.2,-0.4)$.\n\nYour program must output a single line containing a comma-separated list enclosed in square brackets, formed by concatenating the six results for Test $1$, then the six results for Test $2$, and so on through Test $5$. The final output is thus a list of $30$ real numbers. No physical units are involved. Angles are not involved. Express all results as decimal real numbers.", "solution": "The problem has been validated and is determined to be sound. It is scientifically grounded in the established theories of Shapley values and Integrated Gradients, is well-posed with all necessary information provided, and is stated objectively. The task is to derive and implement attribution calculations from first principles for a specified polynomial model and verify certain theoretical properties.\n\nThe model is defined by the function $f:\\mathbb{R}^2 \\rightarrow \\mathbb{R}$ as $f(x_1, x_2) = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2$. The baseline for all attributions is the origin, $(0,0)$.\n\n**1. Derivation of SHAP Attributions**\n\nThe Shapley value $\\phi_i$ for a feature $i$ in a set of features $N$ is defined as the weighted average of its marginal contributions to all possible coalitions of features:\n$$ \\phi_i(v) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|! (|N| - |S| - 1)!}{|N|!} [v(S \\cup \\{i\\}) - v(S)] $$\nIn our case, the set of features is $N = \\{1, 2\\}$, so $|N|=2$. The value function $v(S)$ is the model output $f$ evaluated with inputs $x_j$ for features $j \\in S$ and baseline value $0$ for features $j \\notin S$.\nThe relevant function values are:\n- $v(\\emptyset) = f(0, 0) = 0$\n- $v(\\{1\\}) = f(x_1, 0) = \\beta_1 x_1$\n- $v(\\{2\\}) = f(0, x_2) = \\beta_2 x_2$\n- $v(\\{1, 2\\}) = f(x_1, x_2) = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2$\n\nFor feature $x_1$, the SHAP attribution $\\text{SHAP}_1$ is calculated by considering subsets of $N \\setminus \\{1\\} = \\{2\\}$, which are $\\emptyset$ and $\\{2\\}$:\n$$ \\text{SHAP}_1 = \\frac{0!(2-0-1)!}{2!}[v(\\{1\\}) - v(\\emptyset)] + \\frac{1!(2-1-1)!}{2!}[v(\\{1, 2\\}) - v(\\{2\\})] $$\n$$ \\text{SHAP}_1 = \\frac{1}{2}[f(x_1, 0) - f(0, 0)] + \\frac{1}{2}[f(x_1, x_2) - f(0, x_2)] $$\nSubstituting the function values:\n$$ \\text{SHAP}_1 = \\frac{1}{2}[\\beta_1 x_1 - 0] + \\frac{1}{2}[(\\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2) - (\\beta_2 x_2)] $$\n$$ \\text{SHAP}_1 = \\frac{1}{2}(\\beta_1 x_1) + \\frac{1}{2}(\\beta_1 x_1 + \\beta_{12} x_1 x_2) $$\n$$ \\text{SHAP}_1 = \\beta_1 x_1 + \\frac{1}{2}\\beta_{12} x_1 x_2 $$\n\nBy symmetry, for feature $x_2$, the SHAP attribution $\\text{SHAP}_2$ is:\n$$ \\text{SHAP}_2 = \\frac{1}{2}[f(0, x_2) - f(0, 0)] + \\frac{1}{2}[f(x_1, x_2) - f(x_1, 0)] $$\n$$ \\text{SHAP}_2 = \\frac{1}{2}[\\beta_2 x_2 - 0] + \\frac{1}{2}[(\\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2) - (\\beta_1 x_1)] $$\n$$ \\text{SHAP}_2 = \\frac{1}{2}(\\beta_2 x_2) + \\frac{1}{2}(\\beta_2 x_2 + \\beta_{12} x_1 x_2) $$\n$$ \\text{SHAP}_2 = \\beta_2 x_2 + \\frac{1}{2}\\beta_{12} x_1 x_2 $$\n\n**2. Derivation of Integrated Gradients (IG) Attributions**\n\nThe Integrated Gradients attribution for feature $x_i$ with input $x=(x_1, x_2)$ and baseline $x'=(0,0)$ is given by the formula:\n$$ \\text{IG}_i(x) = (x_i - x'_i) \\int_{\\alpha=0}^{1} \\frac{\\partial f}{\\partial x_i}(x' + \\alpha(x - x')) d\\alpha $$\nThe path of integration is $\\gamma(\\alpha) = (0,0) + \\alpha(x_1-0, x_2-0) = (\\alpha x_1, \\alpha x_2)$. The term $(x_i - x'_i)$ is simply $x_i$.\n\nFirst, we compute the partial derivatives of $f$:\n$$ \\frac{\\partial f}{\\partial x_1} = \\beta_1 + \\beta_{12} x_2 $$\n$$ \\frac{\\partial f}{\\partial x_2} = \\beta_2 + \\beta_{12} x_1 $$\nNext, we evaluate these derivatives along the path $\\gamma(\\alpha)$:\n$$ \\frac{\\partial f}{\\partial x_1}(\\gamma(\\alpha)) = \\beta_1 + \\beta_{12}(\\alpha x_2) = \\beta_1 + \\alpha\\beta_{12}x_2 $$\n$$ \\frac{\\partial f}{\\partial x_2}(\\gamma(\\alpha)) = \\beta_2 + \\beta_{12}(\\alpha x_1) = \\beta_2 + \\alpha\\beta_{12}x_1 $$\nNow we compute the integral for $\\text{IG}_1$:\n$$ \\text{IG}_1 = x_1 \\int_{0}^{1} (\\beta_1 + \\alpha\\beta_{12}x_2) d\\alpha = x_1 \\left[ \\beta_1\\alpha + \\frac{\\alpha^2}{2}\\beta_{12}x_2 \\right]_0^1 $$\n$$ \\text{IG}_1 = x_1 \\left( \\beta_1 + \\frac{1}{2}\\beta_{12}x_2 \\right) = \\beta_1 x_1 + \\frac{1}{2}\\beta_{12} x_1 x_2 $$\nSimilarly, for $\\text{IG}_2$:\n$$ \\text{IG}_2 = x_2 \\int_{0}^{1} (\\beta_2 + \\alpha\\beta_{12}x_1) d\\alpha = x_2 \\left[ \\beta_2\\alpha + \\frac{\\alpha^2}{2}\\beta_{12}x_1 \\right]_0^1 $$\n$$ \\text{IG}_2 = x_2 \\left( \\beta_2 + \\frac{1}{2}\\beta_{12}x_1 \\right) = \\beta_2 x_2 + \\frac{1}{2}\\beta_{12} x_1 x_2 $$\nFor this specific model and baseline, the IG and SHAP attributions are identical.\n\n**3. Derivation of the Shapley Interaction Index**\n\nThe Shapley interaction index for the pair $(x_1, x_2)$ is given by the discrete second-order difference:\n$$ \\text{InteractionIndex} = f(x_1, x_2) - f(x_1, 0) - f(0, x_2) + f(0, 0) $$\nSubstituting the function definitions:\n$$ \\text{InteractionIndex} = (\\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2) - (\\beta_1 x_1) - (\\beta_2 x_2) + (0) $$\n$$ \\text{InteractionIndex} = \\beta_{12} x_1 x_2 $$\nThis demonstrates that the interaction index precisely isolates the interaction term of the model.\n\n**4. Analysis of Verification Checks**\n\nBased on the above derivations, we can analyze the six required checks.\n- **Check 1**: $|\\sum \\text{IG}_i - f(x_1, x_2)|$.\nThe sum of IG attributions is $\\text{IG}_1 + \\text{IG}_2 = (\\beta_1 x_1 + \\frac{1}{2}\\beta_{12} x_1 x_2) + (\\beta_2 x_2 + \\frac{1}{2}\\beta_{12} x_1 x_2) = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2 = f(x_1, x_2)$. Thus, the difference is $|f(x_1, x_2) - f(x_1, x_2)| = 0$.\n- **Check 2**: $|\\text{SHAP}_1 - \\text{IG}_1|$. As shown above, $\\text{SHAP}_1 = \\text{IG}_1 = \\beta_1 x_1 + \\frac{1}{2}\\beta_{12} x_1 x_2$. The difference is $0$.\n- **Check 3**: $|\\text{SHAP}_2 - \\text{IG}_2|$. Similarly, $\\text{SHAP}_2 = \\text{IG}_2 = \\beta_2 x_2 + \\frac{1}{2}\\beta_{12} x_1 x_2$. The difference is $0$.\n- **Check 4**: $|\\text{InteractionIndex} - \\beta_{12} x_1 x_2|$. As shown, $\\text{InteractionIndex} = \\beta_{12} x_1 x_2$. The difference is $0$.\n- **Check 5**: $|(\\text{IG}_1 - \\beta_1 x_1) - \\frac{1}{2}\\beta_{12} x_1 x_2|$. Substituting $\\text{IG}_1$, we get $|(\\beta_1 x_1 + \\frac{1}{2}\\beta_{12} x_1 x_2 - \\beta_1 x_1) - \\frac{1}{2}\\beta_{12} x_1 x_2| = |\\frac{1}{2}\\beta_{12} x_1 x_2 - \\frac{1}{2}\\beta_{12} x_1 x_2| = 0$. This confirms that IG allocates exactly half the interaction to feature $1$.\n- **Check 6**: $|(\\text{IG}_2 - \\beta_2 x_2) - \\frac{1}{2}\\beta_{12} x_1 x_2|$. Similarly, substituting $\\text{IG}_2$, we get $|(\\beta_2 x_2 + \\frac{1}{2}\\beta_{12} x_1 x_2 - \\beta_2 x_2) - \\frac{1}{2}\\beta_{12} x_1 x_2| = |\\frac{1}{2}\\beta_{12} x_1 x_2 - \\frac{1}{2}\\beta_{12} x_1 x_2| = 0$.\n\nAll six checks are analytically expected to yield $0$ for all test cases, which the implementation will confirm up to floating-point precision.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares attributions from SHAP and Integrated Gradients (IG)\n    for a two-feature model and verifies the Shapley interaction index.\n    \"\"\"\n    \n    # Test suite of parameter values (beta1, beta2, beta12, x1, x2)\n    test_cases = [\n        (1.5, 2.0, 0.8, 1.0, 3.0),\n        (1.0, 1.0, 0.0, 2.0, -1.0),\n        (-0.5, 0.75, -1.2, 1.2, 0.5),\n        (3.0, -2.0, 0.5, 0.0, 4.0),\n        (0.0, 0.0, 5.0, 0.2, -0.4),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        b1, b2, b12, x1, x2 = case\n\n        # The model function f(x1, x2) = b1*x1 + b2*x2 + b12*x1*x2\n        # A lambda is used for clarity in the interaction index calculation.\n        f = lambda z1, z2: b1 * z1 + b2 * z2 + b12 * z1 * z2\n\n        # 1. Compute SHAP attributions based on the derived formula:\n        # SHAP_i = beta_i * x_i + 0.5 * beta_12 * x_1 * x_2\n        interaction_term_val = b12 * x1 * x2\n        shap1 = b1 * x1 + 0.5 * interaction_term_val\n        shap2 = b2 * x2 + 0.5 * interaction_term_val\n\n        # 2. Compute IG attributions based on the derived formula:\n        # IG_i = beta_i * x_i + 0.5 * beta_12 * x_1 * x_2\n        ig1 = b1 * x1 + 0.5 * interaction_term_val\n        ig2 = b2 * x2 + 0.5 * interaction_term_val\n\n        # 3. Compute Shapley interaction index using the discrete second-order difference\n        # InteractionIndex = f(x1, x2) - f(x1, 0) - f(0, x2) + f(0, 0)\n        interaction_index = f(x1, x2) - f(x1, 0) - f(0, x2) + f(0, 0)\n\n        # Compute the six specified checks\n        \n        # Check 1: Absolute difference between the sum of IG attributions and f(x1, x2)\n        f_val = f(x1, x2)\n        check1 = np.abs((ig1 + ig2) - f_val)\n\n        # Check 2: Absolute difference between SHAP1 and IG1\n        check2 = np.abs(shap1 - ig1)\n\n        # Check 3: Absolute difference between SHAP2 and IG2\n        check3 = np.abs(shap2 - ig2)\n\n        # Check 4: Absolute difference between Shapley interaction index and beta_12 * x_1 * x_2\n        check4 = np.abs(interaction_index - interaction_term_val)\n\n        # Check 5: Absolute difference between IG interaction part for x1 and half the interaction\n        ig1_interaction_part = ig1 - b1 * x1\n        check5 = np.abs(ig1_interaction_part - 0.5 * interaction_term_val)\n\n        # Check 6: Absolute difference between IG interaction part for x2 and half the interaction\n        ig2_interaction_part = ig2 - b2 * x2\n        check6 = np.abs(ig2_interaction_part - 0.5 * interaction_term_val)\n\n        results.extend([check1, check2, check3, check4, check5, check6])\n    \n    # Format the final output as a comma-separated list of real numbers in brackets.\n    # The map to str ensures correct formatting, e.g., 0.0 instead of 0.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3150523"}, {"introduction": "After learning to generate attributions, a critical next step is to ask: can we trust them? This advanced practice challenges the reliability of explanation methods by tasking you with creating an \"adversarial attack\" against them. You will learn to craft a small, targeted perturbation to an input that radically alters its attribution map while leaving the model's output nearly unchanged, revealing the potential fragility of these explanations [@problem_id:3150456].", "problem": "You must write a complete and runnable program that constructs adversarial explainer attacks to change an attribution map while approximately keeping the model output fixed, and then measures the attackability of different explanation methods. The context is a deterministic, fully specified scalar-output neural network. The program must implement the network, three attribution methods, a constrained optimization to perturb the input, and a quantitative metric of attackability. The final output must be a single line containing a list of floating-point values, one per test case, representing the measured attackability in the specified order.\n\nThe model is a two-layer fully connected network with hyperbolic tangent nonlinearity. Let the input dimension be $d = 5$ and the hidden dimension be $h = 3$. The model is\n$$\nf(\\mathbf{x}) = \\mathbf{w}_2^\\top \\tanh(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1) + b_2,\n$$\nwith parameters\n$$\n\\mathbf{W}_1 = \\begin{bmatrix}\n0.8 & -0.4 & 0.1 & 0.0 & 0.5 \\\\\n-0.3 & 0.7 & -0.6 & 0.2 & -0.1 \\\\\n0.2 & 0.1 & 0.9 & -0.5 & 0.3\n\\end{bmatrix},\\quad\n\\mathbf{b}_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix},\\quad\n\\mathbf{w}_2 = \\begin{bmatrix} 0.6 \\\\ -0.4 \\\\ 0.3 \\end{bmatrix},\\quad\nb_2 = 0.0.\n$$\nFor any input $\\mathbf{x} \\in \\mathbb{R}^5$, denote by $\\nabla f(\\mathbf{x})$ the gradient of $f$ at $\\mathbf{x}$. The three explanation methods to be tested are defined as follows:\n- Gradient: $A_{\\mathrm{grad}}(\\mathbf{x}) = \\nabla f(\\mathbf{x})$.\n- Gradient times input: $A_{\\mathrm{gxi}}(\\mathbf{x}) = \\mathbf{x} \\odot \\nabla f(\\mathbf{x})$, where $\\odot$ denotes the element-wise (Hadamard) product.\n- Integrated Gradients (baseline $\\mathbf{0}$): with a positive integer parameter $m$, define\n$$\nA_{\\mathrm{ig}}(\\mathbf{x}; m) \\approx \\mathbf{x} \\odot \\left(\\frac{1}{m} \\sum_{k=1}^{m} \\nabla f\\!\\left(\\alpha_k \\mathbf{x}\\right)\\right), \\quad \\alpha_k = \\frac{k}{m}.\n$$\n\nThe adversarial explainer attack is formulated as follows. For a fixed input $\\mathbf{x}$ and a fixed explanation method $A$, choose a perturbation $\\boldsymbol{\\delta}$ to minimize the similarity between $A(\\mathbf{x} + \\boldsymbol{\\delta})$ and $A(\\mathbf{x})$ while keeping $f(\\mathbf{x} + \\boldsymbol{\\delta})$ close to $f(\\mathbf{x})$ and obeying a bound on $\\|\\boldsymbol{\\delta}\\|_\\infty$. Let the cosine similarity be\n$$\ns(\\mathbf{u}, \\mathbf{v}) = \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{\\|\\mathbf{u}\\|_2 \\, \\|\\mathbf{v}\\|_2 + 10^{-12}}.\n$$\nDefine the attack loss\n$$\n\\mathcal{L}(\\boldsymbol{\\delta}; \\mathbf{x}, A) = s\\!\\left(A(\\mathbf{x} + \\boldsymbol{\\delta}), A(\\mathbf{x})\\right) + \\lambda \\left(f(\\mathbf{x} + \\boldsymbol{\\delta}) - f(\\mathbf{x})\\right)^2,\n$$\nwith penalty weight $\\lambda > 0$, subject to $\\|\\boldsymbol{\\delta}\\|_\\infty \\le \\varepsilon$. The attackability of method $A$ at $\\mathbf{x}$ under budget $\\varepsilon$ is measured as\n$$\n\\mathrm{Attackability} = 1 - s\\!\\left(A(\\mathbf{x} + \\boldsymbol{\\delta}^\\star), A(\\mathbf{x})\\right),\n$$\nwhere $\\boldsymbol{\\delta}^\\star$ approximately minimizes $\\mathcal{L}$ under the $\\ell_\\infty$ constraint.\n\nYour program must:\n- Implement $f(\\mathbf{x})$ and $\\nabla f(\\mathbf{x})$ exactly for the above network.\n- Implement $A_{\\mathrm{grad}}$, $A_{\\mathrm{gxi}}$, and $A_{\\mathrm{ig}}$ as defined above. For $A_{\\mathrm{ig}}$, use a Riemann sum with a given $m$.\n- Optimize $\\boldsymbol{\\delta}$ using projected gradient descent on $\\mathcal{L}$ with respect to $\\boldsymbol{\\delta}$, using central finite differences to approximate the gradient with respect to $\\boldsymbol{\\delta}$. Use a central finite difference step size $h = 10^{-4}$. Perform $T$ iterations with step size $\\eta = 0.2 \\varepsilon$. After each gradient step and projection onto the $\\ell_\\infty$ ball of radius $\\varepsilon$, apply a scalar output-preservation correction using a first-order local linearization:\n$$\n\\boldsymbol{\\delta} \\leftarrow \\boldsymbol{\\delta} - \\frac{f(\\mathbf{x} + \\boldsymbol{\\delta}) - f(\\mathbf{x})}{\\|\\nabla f(\\mathbf{x})\\|_2^2 + 10^{-12}} \\, \\nabla f(\\mathbf{x}),\n$$\nfollowed by reprojection onto the $\\ell_\\infty$ ball. Initialize $\\boldsymbol{\\delta} = \\mathbf{0}$. Use $T = 60$ and $\\lambda = 50.0$.\n- Report the attackability as defined above for each test case, in the specified order.\n\nTest suite. For each case, you are given the explanation method $A$, the input $\\mathbf{x}$, the perturbation budget $\\varepsilon$, and, when applicable, the integrated gradients step count $m$. The five test cases are:\n- Case $1$: $A = A_{\\mathrm{grad}}$, $\\mathbf{x} = \\begin{bmatrix} 0.5 \\\\ -0.3 \\\\ 0.1 \\\\ 0.2 \\\\ 0.0 \\end{bmatrix}$, $\\varepsilon = 0.05$.\n- Case $2$: $A = A_{\\mathrm{gxi}}$, $\\mathbf{x} = \\begin{bmatrix} -0.1 \\\\ 0.4 \\\\ 0.0 \\\\ -0.3 \\\\ 0.2 \\end{bmatrix}$, $\\varepsilon = 0.05$.\n- Case $3$: $A = A_{\\mathrm{ig}}$ with $m = 50$, $\\mathbf{x} = \\begin{bmatrix} 0.2 \\\\ -0.2 \\\\ 0.3 \\\\ -0.1 \\\\ 0.1 \\end{bmatrix}$, $\\varepsilon = 0.05$.\n- Case $4$: $A = A_{\\mathrm{grad}}$, $\\mathbf{x} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$, $\\varepsilon = 0.0$.\n- Case $5$: $A = A_{\\mathrm{ig}}$ with $m = 20$, $\\mathbf{x} = \\begin{bmatrix} 0.7 \\\\ -0.5 \\\\ 0.2 \\\\ 0.0 \\\\ -0.1 \\end{bmatrix}$, $\\varepsilon = 0.2$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above:\n$$\n[\\mathrm{Attackability}_1,\\mathrm{Attackability}_2,\\mathrm{Attackability}_3,\\mathrm{Attackability}_4,\\mathrm{Attackability}_5].\n$$\nEach $\\mathrm{Attackability}_i$ must be a floating-point number. No other text should be printed.", "solution": "The user has provided a well-defined computational problem concerning the adversarial attacking of explanation methods for a neural network. I will first formally restate the problem's components and then detail the algorithmic procedure for its solution.\n\nThe problem is valid as it is scientifically grounded in the domain of trustworthy machine learning, mathematically well-posed, and all parameters and procedures are specified without ambiguity.\n\n### Mathematical and Algorithmic Specification\n\nThe core of the problem involves a fully specified, deterministic neural network, several attribution methods for explaining its predictions, and an optimization procedure to find an adversarial perturbation that minimally changes the model's output while maximally altering its explanation.\n\n**1. Neural Network Model**\n\nThe model is a two-layer fully connected network that maps an input vector $\\mathbf{x} \\in \\mathbb{R}^d$ to a scalar output $f(\\mathbf{x}) \\in \\mathbb{R}$, where the input dimension is $d=5$ and the hidden layer has $h=3$ neurons. The model is defined as:\n$$\nf(\\mathbf{x}) = \\mathbf{w}_2^\\top \\tanh(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1) + b_2\n$$\nThe parameters are given as:\n- Weight matrix for the first layer: $\\mathbf{W}_1 \\in \\mathbb{R}^{h \\times d}$\n- Bias vector for the first layer: $\\mathbf{b}_1 \\in \\mathbb{R}^h$\n- Weight vector for the output layer: $\\mathbf{w}_2 \\in \\mathbb{R}^h$\n- Bias for the output layer: $b_2 \\in \\mathbb{R}$\n\nThe activation function for the hidden layer is the hyperbolic tangent, $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$.\n\n**2. Model Gradient**\n\nTo implement the specified attribution methods and the attack procedure, the gradient of the model output with respect to the input, $\\nabla f(\\mathbf{x})$, is required. Using the chain rule of calculus, we can derive an analytical expression for this gradient. Let $\\mathbf{z}(\\mathbf{x}) = \\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1$ be the pre-activation of the hidden layer and $\\mathbf{h}(\\mathbf{x}) = \\tanh(\\mathbf{z}(\\mathbf{x}))$ be the post-activation. The derivative of the $\\tanh$ function is $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$.\n\nThe gradient is then:\n$$\n\\nabla f(\\mathbf{x}) = \\frac{\\partial f}{\\partial \\mathbf{x}} = \\frac{\\partial f}{\\partial \\mathbf{h}} \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}\n$$\nEach term is a Jacobian matrix:\n- $\\frac{\\partial f}{\\partial \\mathbf{h}} = \\mathbf{w}_2^\\top$\n- $\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}} = \\mathrm{diag}(1 - \\tanh^2(\\mathbf{z}))$, where $\\mathrm{diag}(\\cdot)$ creates a diagonal matrix.\n- $\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} = \\mathbf{W}_1$\n\nCombining these yields the gradient vector:\n$$\n\\nabla f(\\mathbf{x}) = \\mathbf{W}_1^\\top \\left( \\mathbf{w}_2 \\odot (1 - \\tanh^2(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1)) \\right)\n$$\nwhere $\\odot$ denotes the element-wise (Hadamard) product.\n\n**3. Attribution Methods**\n\nThree attribution (explanation) methods are to be evaluated, each producing an attribution map $A(\\mathbf{x}) \\in \\mathbb{R}^d$:\n- **Gradient**: The attribution is simply the gradient of the output with respect to the input.\n  $$\n  A_{\\mathrm{grad}}(\\mathbf{x}) = \\nabla f(\\mathbf{x})\n  $$\n- **Gradient-times-Input**: The attribution is the element-wise product of the input and the gradient.\n  $$\n  A_{\\mathrm{gxi}}(\\mathbf{x}) = \\mathbf{x} \\odot \\nabla f(\\mathbf{x})\n  $$\n- **Integrated Gradients (IG)**: This method computes the average gradient along the straight-line path from a baseline (here, the zero vector $\\mathbf{0}$) to the input $\\mathbf{x}$, and then multiplies by the input. The integral is approximated by a finite Riemann sum with $m$ steps.\n  $$\n  A_{\\mathrm{ig}}(\\mathbf{x}; m) \\approx \\mathbf{x} \\odot \\left(\\frac{1}{m} \\sum_{k=1}^{m} \\nabla f\\!\\left(\\alpha_k \\mathbf{x}\\right)\\right), \\quad \\text{where } \\alpha_k = \\frac{k}{m}\n  $$\n\n**4. Adversarial Attack Formulation**\n\nThe goal is to find a small perturbation $\\boldsymbol{\\delta}$ that, when added to an input $\\mathbf{x}$, changes the resulting attribution map $A(\\mathbf{x}+\\boldsymbol{\\delta})$ as much as possible, while keeping the model output $f(\\mathbf{x}+\\boldsymbol{\\delta})$ close to the original output $f(\\mathbf{x})$. This is formulated as a constrained optimization problem.\n\nThe change in attribution is measured by minimizing the cosine similarity between the original and perturbed attribution maps:\n$$\ns(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}^\\top \\mathbf{v}}{\\|\\mathbf{u}\\|_2 \\, \\|\\mathbf{v}\\|_2 + \\epsilon_{\\text{cos}}}\n$$\nwhere $\\epsilon_{\\text{cos}} = 10^{-12}$ is a small constant for numerical stability.\n\nThe objective is to find $\\boldsymbol{\\delta}^\\star$ that minimizes the following loss function:\n$$\n\\mathcal{L}(\\boldsymbol{\\delta}; \\mathbf{x}, A) = s\\!\\left(A(\\mathbf{x} + \\boldsymbol{\\delta}), A(\\mathbf{x})\\right) + \\lambda \\left(f(\\mathbf{x} + \\boldsymbol{\\delta}) - f(\\mathbf{x})\\right)^2\n$$\nsubject to the constraint $\\|\\boldsymbol{\\delta}\\|_\\infty \\le \\varepsilon$, where $\\varepsilon$ is the perturbation budget. The hyperparameter $\\lambda=50.0$ balances the two objectives.\n\n**5. Optimization Algorithm**\n\nThe problem specifies an iterative algorithm to find an approximate minimizer $\\boldsymbol{\\delta}^\\star$. The algorithm is Projected Gradient Descent (PGD), augmented with a specialized correction step.\n\nStarting with $\\boldsymbol{\\delta}_0 = \\mathbf{0}$, the algorithm iterates for $T=60$ steps. In each step $t$:\n1.  **Gradient Calculation**: The gradient of the loss with respect to the perturbation, $\\nabla_{\\boldsymbol{\\delta}} \\mathcal{L}(\\boldsymbol{\\delta}_t)$, is approximated using central finite differences with a step size of $h=10^{-4}$.\n    $$\n    [\\nabla_{\\boldsymbol{\\delta}} \\mathcal{L}]_i = \\frac{\\mathcal{L}(\\boldsymbol{\\delta}_t + h\\mathbf{e}_i) - \\mathcal{L}(\\boldsymbol{\\delta}_t - h\\mathbf{e}_i)}{2h}\n    $$\n    where $\\mathbf{e}_i$ is the $i$-th standard basis vector.\n2.  **PGD Step**: A gradient descent step is taken, and the result is projected onto the $\\ell_\\infty$-ball of radius $\\varepsilon$. The step size is $\\eta = 0.2\\varepsilon$.\n    $$\n    \\boldsymbol{\\delta}' = \\mathrm{Proj}_{\\|\\cdot\\|_\\infty \\le \\varepsilon} \\left( \\boldsymbol{\\delta}_t - \\eta \\nabla_{\\boldsymbol{\\delta}} \\mathcal{L}(\\boldsymbol{\\delta}_t) \\right)\n    $$\n    The projection operator is a simple element-wise clipping: $\\mathrm{Proj}_{\\|\\cdot\\|_\\infty \\le \\varepsilon}(\\mathbf{v})_i = \\mathrm{clip}(v_i, -\\varepsilon, \\varepsilon)$.\n3.  **Output Preservation Correction**: To further enforce the constraint on the model output, a correction is applied. This step projects $\\boldsymbol{\\delta}'$ towards the hyperplane that is tangent to the level set of $f$ at the original point $\\mathbf{x}$.\n    $$\n    \\boldsymbol{\\delta}'' = \\boldsymbol{\\delta}' - \\frac{f(\\mathbf{x} + \\boldsymbol{\\delta}') - f(\\mathbf{x})}{\\|\\nabla f(\\mathbf{x})\\|_2^2 + \\epsilon_{\\text{num}}} \\, \\nabla f(\\mathbf{x})\n    $$\n    where $\\epsilon_{\\text{num}} = 10^{-12}$ for stability. Note the use of the pre-computed gradient $\\nabla f(\\mathbf{x})$.\n4.  **Re-projection**: The perturbation is projected back into the $\\ell_\\infty$-ball after the correction step.\n    $$\n    \\boldsymbol{\\delta}_{t+1} = \\mathrm{Proj}_{\\|\\cdot\\|_\\infty \\le \\varepsilon} ( \\boldsymbol{\\delta}'' )\n    $$\n\nThe final perturbation after $T$ iterations is denoted $\\boldsymbol{\\delta}^\\star$.\n\n**6. Attackability Metric**\n\nThe success of the attack is quantified by the \"Attackability\" metric, which measures the drop in cosine similarity:\n$$\n\\mathrm{Attackability} = 1 - s\\!\\left(A(\\mathbf{x} + \\boldsymbol{\\delta}^\\star), A(\\mathbf{x})\\right)\n$$\nA value close to $1$ indicates a highly successful attack (low final similarity), while a value close to $0$ indicates failure (high final similarity). For the trivial case where $\\varepsilon=0$, $\\boldsymbol{\\delta}^\\star=\\mathbf{0}$, and the attackability is $0$.\n\nThe program will implement this entire pipeline and compute the attackability for the five specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs adversarial explainer attacks and measures the attackability\n    of different attribution methods on a specified neural network.\n    \"\"\"\n\n    # --- Model and Attack Parameters ---\n    W1 = np.array([\n        [0.8, -0.4, 0.1, 0.0, 0.5],\n        [-0.3, 0.7, -0.6, 0.2, -0.1],\n        [0.2, 0.1, 0.9, -0.5, 0.3]\n    ])\n    b1 = np.array([0.1, -0.2, 0.05])\n    w2 = np.array([0.6, -0.4, 0.3])\n    b2 = 0.0\n\n    # Optimization hyperparameters\n    LAMBDA = 50.0\n    T = 60\n    H_FD = 1e-4  # Finite difference step\n    EPS_NUM = 1e-12 # Numerical stability constant\n\n    # --- Core Functions ---\n\n    def f(x_in):\n        \"\"\"Computes the model's scalar output f(x).\"\"\"\n        z = W1 @ x_in + b1\n        h = np.tanh(z)\n        return w2 @ h + b2\n\n    def grad_f(x_in):\n        \"\"\"Computes the gradient of the model output w.r.t. the input x.\"\"\"\n        z = W1 @ x_in + b1\n        tanh_z = np.tanh(z)\n        # Derivative of tanh is 1 - tanh^2\n        d_tanh_dz = 1.0 - tanh_z**2\n        # Chain rule: grad = W1.T @ (d_f_dh * d_h_dz)\n        # d_f_dh = w2, d_h_dz is diagonal\n        v = w2 * d_tanh_dz\n        return W1.T @ v\n\n    # --- Attribution Methods ---\n    \n    def A_grad(x_in):\n        \"\"\"Gradient attribution method.\"\"\"\n        return grad_f(x_in)\n\n    def A_gxi(x_in):\n        \"\"\"Gradient-times-Input attribution method.\"\"\"\n        return x_in * grad_f(x_in)\n\n    def A_ig(x_in, m):\n        \"\"\"Integrated Gradients attribution method.\"\"\"\n        avg_grad = np.zeros_like(x_in, dtype=float)\n        if np.all(x_in == 0): # Handle zero input case\n            return avg_grad\n\n        for k in range(1, m + 1):\n            alpha_k = k / m\n            x_interp = alpha_k * x_in\n            avg_grad += grad_f(x_interp)\n        \n        avg_grad /= m\n        return x_in * avg_grad\n\n    # --- Attack Infrastructure ---\n    \n    def cosine_similarity(u, v):\n        \"\"\"Computes cosine similarity between two vectors.\"\"\"\n        dot_product = np.dot(u, v)\n        norm_u = np.linalg.norm(u)\n        norm_v = np.linalg.norm(v)\n        return dot_product / (norm_u * norm_v + EPS_NUM)\n\n    def find_delta_star(x_orig, method_name, epsilon, m=None):\n        \"\"\"\n        Finds the optimal perturbation delta_star using PGD.\n        \"\"\"\n        if epsilon == 0.0:\n            return np.zeros_like(x_orig)\n            \n        # Set up attribution function for this attack\n        if method_name == 'ig':\n            A_func = lambda y: A_ig(y, m)\n        elif method_name == 'gxi':\n            A_func = A_gxi\n        else: # 'grad'\n            A_func = A_grad\n\n        # Pre-compute target values that are constant in the loop\n        f_target = f(x_orig)\n        A_target = A_func(x_orig)\n        grad_f_x = grad_f(x_orig)\n        \n        delta = np.zeros_like(x_orig, dtype=float)\n        eta = 0.2 * epsilon\n\n        def loss_L(d_prime):\n            \"\"\"Calculates the attack loss for a given perturbation.\"\"\"\n            x_prime = x_orig + d_prime\n            A_prime = A_func(x_prime)\n            sim = cosine_similarity(A_prime, A_target)\n            f_penalty = (f(x_prime) - f_target)**2\n            return sim + LAMBDA * f_penalty\n\n        for _ in range(T):\n            # 1. Gradient of Loss w.r.t. delta (central finite differences)\n            grad_L_delta = np.zeros_like(delta, dtype=float)\n            for i in range(len(delta)):\n                d_plus = delta.copy()\n                d_minus = delta.copy()\n                d_plus[i] += H_FD\n                d_minus[i] -= H_FD\n                \n                L_plus = loss_L(d_plus)\n                L_minus = loss_L(d_minus)\n                grad_L_delta[i] = (L_plus - L_minus) / (2 * H_FD)\n\n            # 2. PGD step and projection onto L_inf ball\n            delta = delta - eta * grad_L_delta\n            delta = np.clip(delta, -epsilon, epsilon)\n            \n            # 3. Output preservation correction\n            f_current = f(x_orig + delta)\n            correction_numerator = f_current - f_target\n            correction_denominator = np.linalg.norm(grad_f_x)**2 + EPS_NUM\n            correction = (correction_numerator / correction_denominator) * grad_f_x\n            delta = delta - correction\n            \n            # 4. Re-projection onto L_inf ball\n            delta = np.clip(delta, -epsilon, epsilon)\n            \n        return delta\n\n    # --- Test Cases and Execution ---\n    test_cases = [\n        {'method': 'grad', 'x': np.array([0.5, -0.3, 0.1, 0.2, 0.0]), 'eps': 0.05, 'm': None},\n        {'method': 'gxi', 'x': np.array([-0.1, 0.4, 0.0, -0.3, 0.2]), 'eps': 0.05, 'm': None},\n        {'method': 'ig', 'x': np.array([0.2, -0.2, 0.3, -0.1, 0.1]), 'eps': 0.05, 'm': 50},\n        {'method': 'grad', 'x': np.array([0.0, 0.0, 0.0, 0.0, 0.0]), 'eps': 0.0, 'm': None},\n        {'method': 'ig', 'x': np.array([0.7, -0.5, 0.2, 0.0, -0.1]), 'eps': 0.2, 'm': 20},\n    ]\n\n    results = []\n    for case in test_cases:\n        method, x, eps, m_val = case['method'], case['x'], case['eps'], case['m']\n        \n        delta_star = find_delta_star(x, method, eps, m_val)\n\n        # Set up the attribution function to calculate final attackability\n        if method == 'ig':\n            A_func = lambda y: A_ig(y, m_val)\n        elif method == 'gxi':\n            A_func = A_gxi\n        else: # 'grad'\n            A_func = A_grad\n        \n        A_orig = A_func(x)\n        A_adv = A_func(x + delta_star)\n        \n        final_similarity = cosine_similarity(A_adv, A_orig)\n        attackability = 1.0 - final_similarity\n        \n        results.append(attackability)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3150456"}]}