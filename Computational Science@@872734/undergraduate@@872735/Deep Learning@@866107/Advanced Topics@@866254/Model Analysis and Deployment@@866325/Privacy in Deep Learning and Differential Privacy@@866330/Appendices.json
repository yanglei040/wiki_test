{"hands_on_practices": [{"introduction": "Beyond adding noise to numerical queries, a core challenge in privacy is making a discrete selection while preserving confidentiality. The Exponential Mechanism provides an elegant and powerful solution for choosing the \"best\" option from a set of candidates—such as selecting a final model from a group of trained candidates—in a differentially private manner. This practice guides you through the first-principles derivation and implementation of this fundamental mechanism, providing a concrete understanding of how to balance the trade-off between selecting a high-utility option and guaranteeing $\\epsilon$-differential privacy [@problem_id:3165692].", "problem": "You are given the task of constructing a selection mechanism that satisfies Differential Privacy (DP) when choosing a single model from a finite candidate set based on a real-valued utility function. Start from the following foundational basis only: the definition of $\\epsilon$-Differential Privacy (DP) and bounded sensitivity of the utility function. Let a randomized mechanism $\\mathcal{M}$ satisfy $\\epsilon$-Differential Privacy if for all neighboring datasets $D$ and $D'$ that differ in exactly one record, and for all measurable subsets $S$ of outputs, the following holds:\n$$\n\\Pr[\\mathcal{M}(D) \\in S] \\le e^{\\epsilon} \\Pr[\\mathcal{M}(D') \\in S].\n$$\nAssume a real-valued utility function $u(D, r)$ for output $r$ with known global sensitivity $\\Delta u$:\n$$\n\\Delta u = \\max_{D \\sim D'} \\max_{r} |u(D, r) - u(D', r)|.\n$$\nConstruct a mechanism for selecting a single model that is differentially private with respect to $D$ and whose selection probabilities favor higher-utility models. Derive the selection rule for the mechanism using only the above base definitions and then implement it.\n\nYour program must implement the Exponential Mechanism derived from first principles. Given:\n- a list of utilities $[u_0, u_1, \\dots, u_{k-1}]$ for $k$ candidate models,\n- a privacy parameter $\\epsilon > 0$,\n- a utility sensitivity $\\Delta u > 0$,\noutput the normalized selection probabilities over the candidates implied by your mechanism, and then compute specified properties for a set of test cases.\n\nDo not use any randomness in your final outputs; compute exact probabilities deterministically. Use numerically stable computations. Use zero-based indexing for model indices.\n\nTest suite. For each test case below, compute the requested quantity precisely. All tolerances for equality checks are absolute tolerances of $10^{-12}$.\n- Test A (happy path): utilities $[1.0, 2.0, 0.0]$, $\\epsilon = 1.0$, $\\Delta u = 1.0$. Output two quantities in order:\n  1) the probability (as a float) of selecting the model with the largest utility, rounded to $6$ decimal places,\n  2) the zero-based index (as an integer) of the model with the maximum selection probability under your mechanism.\n- Test B (boundary $\\epsilon = 0$): utilities $[3.0, -1.0, 7.0, 7.0]$, $\\epsilon = 0.0$, $\\Delta u = 1.0$. Output a boolean that is true if and only if the resulting distribution is uniform to within tolerance, that is, all probabilities equal $1/k$ within the specified tolerance.\n- Test C (tie fairness): utilities $[0.5, 0.5, 0.0]$, $\\epsilon = 2.0$, $\\Delta u = 1.0$. Output a boolean that is true if and only if tied utilities induce equal selection probabilities for the tied indices to within tolerance.\n- Test D (privacy ratio check on neighbors, two outputs): utilities $u = [0.0, 1.0]$, neighboring utilities $u' = [0.2, 0.8]$, $\\epsilon = 0.5$, $\\Delta u = 0.2$. Since there are exactly $2$ outputs, verifying the inequality on singleton events implies it for all measurable subsets. Output a boolean that is true if and only if\n  $$\n  \\max\\left\\{\\max_{i \\in \\{0,1\\}} \\frac{p_i(u)}{p_i(u')}, \\max_{i \\in \\{0,1\\}} \\frac{p_i(u')}{p_i(u)}\\right\\} \\le e^{\\epsilon} + 10^{-12},\n  $$\n  where $p_i(\\cdot)$ denotes the selection probability of index $i$ under the corresponding utility vector.\n- Test E (large $\\epsilon$ concentration): utilities $[0.0, 2.0, 1.0]$, $\\epsilon = 10.0$, $\\Delta u = 1.0$. Output the probability (as a float) of selecting the model with the largest utility, rounded to $6$ decimal places.\n- Test F (effect of sensitivity): utilities $[0.0, 2.0, 1.0]$, $\\epsilon = 1.0$, compare $\\Delta u_1 = 1.0$ versus $\\Delta u_2 = 2.0$. Output a boolean that is true if and only if the probability of selecting the best-utility model with $\\Delta u_1$ is greater than or equal to that with $\\Delta u_2$ within tolerance.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following order:\n$$\n[\\text{Test A probability}, \\text{Test A index}, \\text{Test B boolean}, \\text{Test C boolean}, \\text{Test D boolean}, \\text{Test E probability}, \\text{Test F boolean}]\n$$\nFor probabilities, apply rounding to $6$ decimal places before outputting. Indices are integers, and booleans are in the programming language’s native boolean format. No external input is required; hard-code the test suite parameters above in your program. No physical units or angles apply in this problem. Express all decimal quantities as decimal numbers, not percentages.", "solution": "The task is to derive and implement a differentially private mechanism for selecting one model from a finite set of candidates, based on a real-valued utility function. The derivation must start from the foundational definitions of $\\epsilon$-Differential Privacy ($\\epsilon$-DP) and global sensitivity.\n\nLet $R$ be the finite set of candidate models. Let $u(D, r)$ be a real-valued utility function that scores a model $r \\in R$ based on a dataset $D$. The global sensitivity of $u$ is given as $\\Delta u = \\max_{D \\sim D'} \\max_{r \\in R} |u(D, r) - u(D', r)|$, where $D$ and $D'$ are neighboring datasets differing by a single record.\n\nWe aim to construct a randomized mechanism $\\mathcal{M}$ that, given a dataset $D$, outputs a model $r \\in R$ with probability $\\Pr[\\mathcal{M}(D) = r]$. This mechanism must satisfy $\\epsilon$-DP, meaning for any pair of neighboring datasets $D, D'$ and any subset of outputs $S \\subseteq R$, we have $\\Pr[\\mathcal{M}(D) \\in S] \\le e^{\\epsilon} \\Pr[\\mathcal{M}(D') \\in S]$. It is sufficient to prove this for all singleton sets $S = \\{r\\}$.\n\nTo favor models with higher utility, we propose that the probability of selecting a model $r$ is proportional to an exponential function of its utility. This is a common choice as it provides a strong preference for higher utility while being mathematically tractable. Let the unnormalized score be proportional to $\\exp(c \\cdot u(D, r))$ for some constant $c > 0$ that we will determine.\n\nTo form a valid probability distribution, we normalize these scores over all models in $R$:\n$$\n\\Pr[\\mathcal{M}(D) = r] = \\frac{\\exp(c \\cdot u(D, r))}{\\sum_{r' \\in R} \\exp(c \\cdot u(D, r'))}\n$$\nNow, we must find the value of $c$ that ensures the mechanism satisfies $\\epsilon$-DP. We analyze the ratio of probabilities for an arbitrary output $r$ on two neighboring datasets $D$ and $D'$:\n$$\n\\frac{\\Pr[\\mathcal{M}(D) = r]}{\\Pr[\\mathcal{M}(D') = r]} = \\frac{\\frac{\\exp(c \\cdot u(D, r))}{\\sum_{r_j \\in R} \\exp(c \\cdot u(D, r_j))}}{\\frac{\\exp(c \\cdot u(D', r))}{\\sum_{r_j \\in R} \\exp(c \\cdot u(D', r_j))}} = \\frac{\\exp(c \\cdot u(D, r))}{\\exp(c \\cdot u(D', r))} \\cdot \\frac{\\sum_{r_j \\in R} \\exp(c \\cdot u(D', r_j))}{\\sum_{r_j \\in R} \\exp(c \\cdot u(D, r_j))}\n$$\nWe bound this ratio using the definition of global sensitivity $\\Delta u$. We can bound the two terms in the product separately.\n\nFor the first term, by definition of sensitivity, $u(D, r) \\le u(D', r) + \\Delta u$. Since $c > 0$, this implies:\n$$\n\\exp(c \\cdot u(D, r)) \\le \\exp(c \\cdot (u(D', r) + \\Delta u)) = \\exp(c \\cdot u(D', r)) \\cdot \\exp(c \\cdot \\Delta u)\n$$\nThus, the first term is bounded by $\\exp(c \\cdot \\Delta u)$.\n\nFor the second term, which is a ratio of normalization constants, we establish a lower bound on its denominator. For any model $r_j \\in R$, we have $u(D, r_j) \\ge u(D', r_j) - \\Delta u$. Therefore:\n$$\n\\exp(c \\cdot u(D, r_j)) \\ge \\exp(c \\cdot (u(D', r_j) - \\Delta u)) = \\exp(c \\cdot u(D', r_j)) \\cdot \\exp(-c \\cdot \\Delta u)\n$$\nSumming over all $r_j \\in R$, we get a bound for the entire sum:\n$$\n\\sum_{r_j \\in R} \\exp(c \\cdot u(D, r_j)) \\ge \\sum_{r_j \\in R} \\left( \\exp(c \\cdot u(D', r_j)) \\cdot \\exp(-c \\cdot \\Delta u) \\right) = \\exp(-c \\cdot \\Delta u) \\sum_{r_j \\in R} \\exp(c \\cdot u(D', r_j))\n$$\nThis gives us an upper bound on the inverse of the sum:\n$$\n\\frac{1}{\\sum_{r_j \\in R} \\exp(c \\cdot u(D, r_j))} \\le \\frac{1}{\\exp(-c \\cdot \\Delta u) \\sum_{r_j \\in R} \\exp(c \\cdot u(D', r_j))}\n$$\nMultiplying this by the numerator of the second term, $\\sum_{r_j \\in R} \\exp(c \\cdot u(D', r_j))$, we find that the second term is bounded by $\\exp(c \\cdot \\Delta u)$.\n\nCombining the bounds for both terms, we get an upper bound on the probability ratio:\n$$\n\\frac{\\Pr[\\mathcal{M}(D) = r]}{\\Pr[\\mathcal{M}(D') = r]} \\le \\exp(c \\cdot \\Delta u) \\cdot \\exp(c \\cdot \\Delta u) = \\exp(2c \\cdot \\Delta u)\n$$\nTo satisfy the $\\epsilon$-DP constraint, this bound must be no greater than $e^\\epsilon$:\n$$ \\exp(2c \\cdot \\Delta u) \\le e^{\\epsilon} \\implies 2c \\cdot \\Delta u \\le \\epsilon $$\nTo provide the strongest utility guarantee (i.e., to make the selection as sensitive to utility as privacy allows), we should choose the largest possible value for $c$, which corresponds to setting the inequality to an equality: $2c \\cdot \\Delta u = \\epsilon$. This yields the value for $c$:\n$$\nc = \\frac{\\epsilon}{2 \\Delta u}\n$$\nSubstituting this back into our probability formula gives the final form of the Exponential Mechanism for model selection:\n$$\np_i = \\Pr[\\mathcal{M}(D) = r_i] = \\frac{\\exp\\left(\\frac{\\epsilon u_i}{2 \\Delta u}\\right)}{\\sum_{j=0}^{k-1} \\exp\\left(\\frac{\\epsilon u_j}{2 \\Delta u}\\right)}\n$$\nwhere $u_i$ is a shorthand for $u(D, r_i)$. For implementation, to avoid numerical overflow when computing the exponentials, we use the log-sum-exp stabilization trick. Letting $s_i = \\frac{\\epsilon u_i}{2 \\Delta u}$ and $s_{\\max} = \\max_j s_j$, the probabilities are computed as:\n$$\np_i = \\frac{\\exp(s_i - s_{\\max})}{\\sum_{j=0}^{k-1} \\exp(s_j - s_{\\max})}\n$$\nThis form is numerically stable as the arguments to the exponential function are all non-positive.", "answer": "```python\nimport numpy as np\n\ndef run_exponential_mechanism(utilities, epsilon, delta_u):\n    \"\"\"\n    Computes the selection probabilities using the Exponential Mechanism.\n    \n    Args:\n        utilities (list or np.ndarray): A list of utility scores for k models.\n        epsilon (float): The privacy parameter epsilon.\n        delta_u (float): The global sensitivity of the utility function.\n        \n    Returns:\n        np.ndarray: An array of k selection probabilities.\n    \"\"\"\n    utilities = np.array(utilities, dtype=np.float64)\n    k = len(utilities)\n    \n    # Handle the boundary case of epsilon = 0, which implies a uniform distribution.\n    if epsilon == 0.0:\n        return np.full(k, 1.0 / k)\n    \n    # The problem specifies epsilon > 0 and delta_u > 0 for the main derivation,\n    # but Test B uses epsilon=0. delta_u=0 would be an issue, but is not tested.\n    if delta_u <= 0:\n        raise ValueError(\"delta_u must be positive.\")\n\n    # Calculate scaled utilities as per the derived formula.\n    scaling_factor = epsilon / (2.0 * delta_u)\n    scaled_utilities = scaling_factor * utilities\n    \n    # Use the log-sum-exp trick for numerical stability.\n    # Subtracting the max value from each scaled utility before exponentiating\n    # prevents overflow and mitigates underflow.\n    max_scaled_utility = np.max(scaled_utilities)\n    exp_utilities = np.exp(scaled_utilities - max_scaled_utility)\n    \n    sum_exp_utilities = np.sum(exp_utilities)\n    \n    probabilities = exp_utilities / sum_exp_utilities\n    return probabilities\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    results = []\n    TOL = 1e-12\n\n    # --- Test A ---\n    u_A = [1.0, 2.0, 0.0]\n    eps_A = 1.0\n    du_A = 1.0\n    probs_A = run_exponential_mechanism(u_A, eps_A, du_A)\n    best_utility_idx_A = np.argmax(u_A)\n    prob_of_best_A = probs_A[best_utility_idx_A]\n    max_prob_idx_A = np.argmax(probs_A)\n    results.append(round(prob_of_best_A, 6))\n    results.append(int(max_prob_idx_A))\n\n    # --- Test B ---\n    u_B = [3.0, -1.0, 7.0, 7.0]\n    eps_B = 0.0\n    du_B = 1.0\n    probs_B = run_exponential_mechanism(u_B, eps_B, du_B)\n    k_B = len(u_B)\n    is_uniform_B = np.all(np.abs(probs_B - 1.0/k_B) < TOL)\n    results.append(is_uniform_B)\n    \n    # --- Test C ---\n    u_C = [0.5, 0.5, 0.0]\n    eps_C = 2.0\n    du_C = 1.0\n    probs_C = run_exponential_mechanism(u_C, eps_C, du_C)\n    # indices 0 and 1 have tied utilities\n    tied_probs_equal_C = np.abs(probs_C[0] - probs_C[1]) < TOL\n    results.append(tied_probs_equal_C)\n\n    # --- Test D ---\n    u_D = [0.0, 1.0]\n    u_prime_D = [0.2, 0.8]\n    eps_D = 0.5\n    du_D = 0.2\n    probs_D = run_exponential_mechanism(u_D, eps_D, du_D)\n    probs_prime_D = run_exponential_mechanism(u_prime_D, eps_D, du_D)\n    \n    # Prevent division by zero, although not expected here\n    # Adding a small constant is one way, but given the problem setup,\n    # probabilities should be non-zero.\n    ratio1 = probs_D / probs_prime_D\n    ratio2 = probs_prime_D / probs_D\n    max_ratio = np.max(np.concatenate([ratio1, ratio2]))\n    \n    dp_holds_D = max_ratio <= np.exp(eps_D) + TOL\n    results.append(dp_holds_D)\n\n    # --- Test E ---\n    u_E = [0.0, 2.0, 1.0]\n    eps_E = 10.0\n    du_E = 1.0\n    probs_E = run_exponential_mechanism(u_E, eps_E, du_E)\n    best_utility_idx_E = np.argmax(u_E)\n    prob_of_best_E = probs_E[best_utility_idx_E]\n    results.append(round(prob_of_best_E, 6))\n\n    # --- Test F ---\n    u_F = [0.0, 2.0, 1.0]\n    eps_F = 1.0\n    du1_F = 1.0\n    du2_F = 2.0\n    \n    probs1_F = run_exponential_mechanism(u_F, eps_F, du1_F)\n    probs2_F = run_exponential_mechanism(u_F, eps_F, du2_F)\n    \n    best_utility_idx_F = np.argmax(u_F)\n    prob_best1_F = probs1_F[best_utility_idx_F]\n    prob_best2_F = probs2_F[best_utility_idx_F]\n    \n    concentration_check_F = prob_best1_F >= prob_best2_F - TOL\n    results.append(concentration_check_F)\n    \n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3165692"}, {"introduction": "Applying differential privacy to machine learning often involves protecting the training process itself. This exercise provides a hands-on introduction to Differentially Private Stochastic Gradient Descent (DP-SGD) by focusing on a specific, critical scenario: protecting sensitive training labels. You will implement a label-private training step and then act as an adversary by launching a model inversion attack to see if sensitive features can still be inferred, making the consequences of privacy leakage tangible. This practice illuminates the core mechanics of gradient analysis, clipping, and noise addition in a simplified and understandable context [@problem_id:3165689].", "problem": "You are given a supervised binary classification problem with a linear model trained by Stochastic Gradient Descent (SGD). The domain contains a designated sensitive feature. Your task is to implement training with privacy applied to labels only, and then perform a model inversion attack to assess leakage of the sensitive feature. The analysis must be framed under $(\\epsilon,\\delta)$-Differential Privacy (DP).\n\nConstruct a synthetic dataset with $N$ samples and $d$ features, where the first feature is the sensitive feature. For each sample index $i$ with feature vector $\\mathbf{x}_i \\in \\mathbb{R}^d$, define the sensitive coordinate as $x_{i,0} \\in \\{-1, +1\\}$ obtained by centering a Bernoulli random variable, and the remaining coordinates $x_{i,1},\\dots,x_{i,d-1}$ as independent Gaussian variables scaled so that the per-sample vector norm is bounded above by a constant $C_x$. Generate labels $y_i \\in \\{0,1\\}$ according to a logistic generative process where the true logit depends strongly on the sensitive coordinate and very weakly on the non-sensitive coordinates, with additive small Gaussian noise. You must ensure that each $\\mathbf{x}_i$ satisfies $\\lVert \\mathbf{x}_i \\rVert_2 \\leq C_x$.\n\nTrain a linear model with parameters $\\mathbf{w} \\in \\mathbb{R}^d$ and bias $b \\in \\mathbb{R}$ using one gradient step of SGD from the zero initialization, with learning rate $\\eta$. Let the model’s predicted probability for input $\\mathbf{x}$ be $\\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)$, where $\\sigma$ is the logistic sigmoid. Compute the per-example residuals as $r_i = \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) - y_i$ and clip each $r_i$ into the interval $[-c, c]$ with $c$ chosen so that changing one label can change $r_i$ by at most $1$. Form the average gradient with respect to $\\mathbf{w}$ as the mean of $\\mathbf{x}_i r_i$ over the batch. To achieve label privacy only, add independent Gaussian noise to the aggregated gradient with variance calibrated by the Gaussian mechanism applied to functions of labels, treating features as fixed and bounded. The sensitivity of the averaged gradient with respect to changing one label must be derived from the bound $\\lVert \\mathbf{x}_i \\rVert_2 \\leq C_x$ and the clipping of $r_i$; calibrate the Gaussian mechanism to guarantee $(\\epsilon,\\delta)$-DP for labels. Apply the same label-private Gaussian mechanism to the bias gradient computed from the averaged $r_i$.\n\nAfter this single noisy gradient step, perform a model inversion attack that seeks an input $\\hat{\\mathbf{x}}$ maximizing the model’s logit while penalizing large inputs via a quadratic regularizer. Specifically, solve for the maximizer of the objective $J(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b - \\lambda \\lVert \\mathbf{x} \\rVert_2^2$ for a chosen $\\lambda > 0$. Use the resulting $\\hat{\\mathbf{x}}$ to define a leakage metric comparing the magnitude of the sensitive coordinate to the magnitudes of the non-sensitive coordinates:\n$$\nR = \\frac{\\left| \\hat{x}_0 \\right|}{\\frac{1}{d-1} \\sum_{j=1}^{d-1} \\left| \\hat{x}_j \\right|}.\n$$\nDeclare that leakage is present if $R > \\tau$, for a chosen threshold $\\tau > 0$, and absent otherwise.\n\nYour program must implement the above pipeline and run the following test suite, each test being specified by the tuple $(N, d, \\epsilon, \\delta, \\lambda, \\tau)$:\n- Test $1$ (happy path, no privacy noise): $(N=\\;200,\\; d=\\;4,\\; \\epsilon=\\;\\text{None},\\; \\delta=\\;10^{-5},\\; \\lambda=\\;0.1,\\; \\tau=\\;1.5)$.\n- Test $2$ (significant privacy, moderate batch): $(N=\\;50,\\; d=\\;4,\\; \\epsilon=\\;0.1,\\; \\delta=\\;10^{-5},\\; \\lambda=\\;0.1,\\; \\tau=\\;1.5)$.\n- Test $3$ (boundary condition, very strong privacy): $(N=\\;20,\\; d=\\;4,\\; \\epsilon=\\;0.01,\\; \\delta=\\;10^{-5},\\; \\lambda=\\;0.1,\\; \\tau=\\;1.5)$.\n\nIn all tests, use a learning rate $\\eta=\\;1.0$, per-example residual clipping parameter $c=\\;0.5$ so that flipping one label changes $r_i$ by at most $1$, and feature norm bound $C_x=\\;1.0$ via per-example clipping of $\\mathbf{x}_i$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the tests above, where each element is a boolean indicating whether leakage is present for that test (e.g., $\\left[\\text{True},\\text{False},\\text{False}\\right]$). No other text should be printed.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of differential privacy and machine learning, is well-posed with a clear objective, and provides a sufficiently complete, consistent, and formalizable set of instructions.\n\nThe solution is implemented by following a sequence of three main steps: synthetic data generation, a single step of label-private Stochastic Gradient Descent (SGD), and a model inversion attack to quantify information leakage.\n\n### Step 1: Synthetic Data Generation\nWe construct a synthetic dataset of $N$ samples, each with $d$ features, represented by a matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$ and a label vector $\\mathbf{y} \\in \\{0, 1\\}^N$.\n\n1.  **Feature Generation**:\n    The first feature, $\\mathbf{x}_{\\cdot, 0}$, is designated as sensitive. Its values are drawn from a centered Bernoulli distribution, i.e., $x_{i,0} \\in \\{-1, +1\\}$ for each sample $i=1, \\dots, N$. The remaining $d-1$ non-sensitive features, $x_{i,j}$ for $j=1, \\dots, d-1$, are drawn from a standard normal distribution $\\mathcal{N}(0,1)$.\n\n2.  **Feature Norm Clipping**:\n    To bound the sensitivity of the gradient in the subsequent privacy analysis, each feature vector $\\mathbf{x}_i$ is normalized to ensure its $L_2$ norm does not exceed a constant $C_x=1.0$. Specifically, if $\\lVert \\mathbf{x}_i \\rVert_2 > C_x$, the vector is scaled by a factor of $C_x / \\lVert \\mathbf{x}_i \\rVert_2$. This operation is stated as per-example clipping in the problem.\n\n3.  **Label Generation**:\n    Labels $y_i \\in \\{0, 1\\}$ are generated via a logistic model. A true weight vector $\\mathbf{w}_{\\text{true}}$ is defined to create a strong dependency on the sensitive feature and a weak dependency on the non-sensitive features. We choose $\\mathbf{w}_{\\text{true}} = [10, 0.1, \\dots, 0.1]^\\top$. The true logit for sample $i$ is computed as $z_i = \\mathbf{w}_{\\text{true}}^\\top \\mathbf{x}_i + n_i$, where $n_i \\sim \\mathcal{N}(0, 0.1^2)$ is a small amount of Gaussian noise. The probability of label $y_i=1$ is given by the logistic sigmoid function, $p_i = \\sigma(z_i) = (1 + e^{-z_i})^{-1}$. The final labels are drawn from a Bernoulli distribution, $y_i \\sim \\text{Bernoulli}(p_i)$.\n\n### Step 2: Differentially Private SGD\nWe perform a single gradient descent step on a linear model, starting from zero initialization, i.e., $\\mathbf{w}_0 = \\mathbf{0}$ and $b_0 = 0$. The privacy guarantee is $(\\epsilon, \\delta)$-Differential Privacy (DP) with respect to the labels.\n\n1.  **Gradient Calculation**: The model's prediction for an input $\\mathbf{x}$ is $\\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)$. For the first step, with $\\mathbf{w}=\\mathbf{w}_0$ and $b=b_0$, the prediction for any input is $\\sigma(0) = 0.5$. The per-example residual is $r_i = \\sigma(\\mathbf{w}_0^\\top \\mathbf{x}_i + b_0) - y_i = 0.5 - y_i$. These residuals are clipped to the range $[-c, c]$ with $c=0.5$. Since $y_i \\in \\{0,1\\}$, $r_i$ is either $0.5$ or $-0.5$, so clipping has no effect. The average gradients with respect to $\\mathbf{w}$ and $b$ are:\n    $$\n    \\nabla_{\\mathbf{w}} L = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{x}_i r_i \\quad , \\quad \\nabla_{b} L = \\frac{1}{N} \\sum_{i=1}^N r_i\n    $$\n\n2.  **Sensitivity Analysis**: For label privacy, we analyze the sensitivity of the average gradients to a change in a single label $y_k$. If $y_k$ flips, the corresponding residual $r_k$ changes by $\\Delta r_k = y_k - y'_k = \\pm 1$. The maximum change in the average gradient for $\\mathbf{w}$ is $\\Delta(\\nabla_{\\mathbf{w}} L) = \\frac{1}{N} \\mathbf{x}_k (\\Delta r_k)$. The $L_2$ sensitivity is the maximum $L_2$ norm of this change:\n    $$\n    \\mathcal{S}_{\\mathbf{w}} = \\sup \\lVert \\Delta(\\nabla_{\\mathbf{w}} L) \\rVert_2 = \\sup \\frac{1}{N} \\lVert \\mathbf{x}_k \\rVert_2 |\\Delta r_k| = \\frac{C_x}{N}\n    $$\n    Similarly, the sensitivity for the bias gradient is:\n    $$\n    \\mathcal{S}_{b} = \\sup |\\Delta(\\nabla_{b} L)| = \\sup \\frac{1}{N} |\\Delta r_k| = \\frac{1}{N}\n    $$\n\n3.  **Gaussian Mechanism**: To ensure $(\\epsilon, \\delta)$-DP, we add Gaussian noise to the gradients. The standard deviation of the noise is calibrated by the sensitivity. For a function with $L_2$ sensitivity $\\mathcal{S}$, the noise standard deviation is $\\sigma_{\\text{noise}} = \\frac{\\mathcal{S} \\sqrt{2 \\ln(1.25/\\delta)}}{\\epsilon}$.\n    - For $\\nabla_{\\mathbf{w}} L$: $\\sigma_{\\mathbf{w}} = \\frac{C_x \\sqrt{2 \\ln(1.25/\\delta)}}{N \\epsilon}$. We add noise $\\mathbf{n}_{\\mathbf{w}} \\sim \\mathcal{N}(0, \\sigma_{\\mathbf{w}}^2 \\mathbf{I})$.\n    - For $\\nabla_{b} L$: $\\sigma_{b} = \\frac{\\sqrt{2 \\ln(1.25/\\delta)}}{N \\epsilon}$. We add noise $n_b \\sim \\mathcal{N}(0, \\sigma_{b}^2)$.\n    For the test case where $\\epsilon$ is `None`, no noise is added ($\\sigma_{\\mathbf{w}}=0, \\sigma_{b}=0$).\n\n4.  **Model Update**: The weights and bias are updated using the noisy gradients and learning rate $\\eta=1.0$:\n    $$\n    \\mathbf{w}_1 = \\mathbf{w}_0 - \\eta(\\nabla_{\\mathbf{w}} L + \\mathbf{n}_{\\mathbf{w}}) = -\\eta(\\nabla_{\\mathbf{w}} L + \\mathbf{n}_{\\mathbf{w}})\n    $$\n    $$\n    b_1 = b_0 - \\eta(\\nabla_{b} L + n_b) = -\\eta(\\nabla_{b} L + n_b)\n    $$\n\n### Step 3: Model Inversion and Leakage Assessment\nAfter the single training step, we perform a model inversion attack to recover information about the training data from the learned parameters $\\mathbf{w}_1$ and $b_1$.\n\n1.  **Attack Formulation**: The attacker seeks an input $\\hat{\\mathbf{x}}$ that maximizes the model's logit, penalized by an $L_2$ regularizer to prevent trivial solutions. The objective function is:\n    $$\n    J(\\mathbf{x}) = \\mathbf{w}_1^\\top \\mathbf{x} + b_1 - \\lambda \\lVert \\mathbf{x} \\rVert_2^2\n    $$\n    To find the maximizer $\\hat{\\mathbf{x}}$, we set the gradient with respect to $\\mathbf{x}$ to zero:\n    $$\n    \\nabla_{\\mathbf{x}} J(\\mathbf{x}) = \\mathbf{w}_1 - 2\\lambda \\mathbf{x} = 0\n    $$\n    Solving for $\\mathbf{x}$ gives the reconstructed input:\n    $$\n    \\hat{\\mathbf{x}} = \\frac{1}{2\\lambda} \\mathbf{w}_1\n    $$\n\n2.  **Leakage Metric**: The reconstructed vector $\\hat{\\mathbf{x}}$ is expected to reflect properties of the training data. Since the labels are strongly correlated with the sensitive feature $x_0$, the first component of the gradient, and thus of $\\mathbf{w}_1$ and $\\hat{\\mathbf{x}}$, should be large in magnitude when no privacy is used. We measure this disparity using the ratio $R$:\n    $$\n    R = \\frac{\\left| \\hat{x}_0 \\right|}{\\frac{1}{d-1} \\sum_{j=1}^{d-1} \\left| \\hat{x}_j \\right|}\n    $$\n    This ratio compares the magnitude of the reconstructed sensitive coordinate to the average magnitude of the non-sensitive coordinates. A large value of $R$ indicates that the model has disproportionately encoded information about the sensitive feature. Leakage is declared if $R > \\tau$.\n\nThe implementation carries out this entire pipeline for each test case. In the absence of privacy noise, $R$ is expected to be large, indicating leakage. With sufficient privacy noise (i.e., small $\\epsilon$), the signal in the gradient is obscured, causing all components of $\\hat{\\mathbf{x}}$ to have similar magnitudes (driven by isotropic noise), resulting in $R \\approx 1$ and no detected leakage.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the label-private SGD and model inversion attack pipeline.\n    \"\"\"\n    # Use a fixed random seed for reproducibility.\n    np.random.seed(42)\n\n    # --- Fixed Parameters ---\n    # Learning rate for SGD\n    ETA = 1.0\n    # Per-example residual clipping parameter\n    C_RESIDUAL = 0.5\n    # Per-example feature norm bound\n    C_X = 1.0\n    \n    # --- Test Suite ---\n    test_cases = [\n        # (N, d, epsilon, delta, lambda, tau)\n        (200, 4, None, 1e-5, 0.1, 1.5),  # Test 1: No privacy\n        (50, 4, 0.1, 1e-5, 0.1, 1.5),    # Test 2: Significant privacy\n        (20, 4, 0.01, 1e-5, 0.1, 1.5),   # Test 3: Very strong privacy\n    ]\n\n    results = []\n\n    for N, d, epsilon, delta, lambda_reg, tau in test_cases:\n        # --- Step 1: Data Generation ---\n        \n        # Sensitive feature: centered Bernoulli\n        sensitive_feature = 2 * np.random.binomial(1, 0.5, size=(N, 1)) - 1\n        \n        # Non-sensitive features: Gaussian\n        other_features = np.random.randn(N, d - 1)\n        \n        # Combine features\n        X = np.hstack([sensitive_feature, other_features])\n        \n        # Clip feature vectors to have L2 norm at most C_X\n        norms = np.linalg.norm(X, axis=1, keepdims=True)\n        # Add a small constant to avoid division by zero for zero-norm vectors\n        X /= np.maximum(1.0, norms / C_X) \n\n        # Generate labels from a logistic model\n        # True weights with strong dependency on the sensitive feature\n        w_true = np.array([10.0] + [0.1] * (d - 1))\n        # Add small Gaussian noise to true logits\n        logits_true = X @ w_true + np.random.normal(0, 0.1, size=N)\n        probs = 1 / (1 + np.exp(-logits_true))\n        y = np.random.binomial(1, probs).astype(float)\n\n        # --- Step 2: Differentially Private SGD Step ---\n        \n        # Start from zero initialization for weights and bias\n        w0 = np.zeros(d)\n        b0 = 0.0\n        \n        # With zero-init model, prediction is sigma(0) = 0.5 for all inputs\n        pred = 0.5\n        \n        # Calculate per-example residuals\n        residuals = pred - y  # Values are either +0.5 or -0.5\n        \n        # The problem requires clipping, but residuals are already within [-0.5, 0.5]\n        # np.clip(residuals, -C_RESIDUAL, C_RESIDUAL, out=residuals)\n        \n        # Calculate average gradients (non-private)\n        grad_w = (X.T @ residuals) / N\n        grad_b = np.mean(residuals)\n        \n        noisy_grad_w = grad_w\n        noisy_grad_b = grad_b\n\n        # Add Gaussian noise for differential privacy if epsilon is specified\n        if epsilon is not None and epsilon > 0:\n            # L2 sensitivity for label privacy\n            sensitivity_w = C_X / N\n            sensitivity_b = 1.0 / N\n            \n            # Calculate noise standard deviation for the Gaussian mechanism\n            # Common factor for both noise scales\n            privacy_term = np.sqrt(2 * np.log(1.25 / delta)) / epsilon\n            \n            sigma_w = sensitivity_w * privacy_term\n            sigma_b = sensitivity_b * privacy_term\n            \n            # Add noise to gradients\n            noise_w = np.random.normal(0, sigma_w, size=d)\n            noise_b = np.random.normal(0, sigma_b)\n            \n            noisy_grad_w += noise_w\n            noisy_grad_b += noise_b\n            \n        # Perform a single SGD update step\n        w1 = w0 - ETA * noisy_grad_w\n        b1 = b0 - ETA * noisy_grad_b\n        \n        # --- Step 3: Model Inversion Attack and Leakage Assessment ---\n        \n        # Attacker reconstructs input by maximizing the regularized logit\n        # J(x) = w1.T @ x + b1 - lambda * ||x||^2\n        # Setting dJ/dx = 0 -> w1 - 2 * lambda * x = 0 -> x = w1 / (2 * lambda)\n        if lambda_reg <= 0:\n            raise ValueError(\"Regularization parameter lambda must be positive.\")\n        x_hat = w1 / (2 * lambda_reg)\n        \n        # Calculate leakage metric R\n        x_hat_0_abs = np.abs(x_hat[0])\n        x_hat_rest_abs_mean = np.mean(np.abs(x_hat[1:]))\n\n        # Handle potential division by zero if d=1, though problem states d=4\n        if x_hat_rest_abs_mean > 1e-12:\n            R = x_hat_0_abs / x_hat_rest_abs_mean\n        else: # If non-sensitive components are all zero, leakage is maximal\n            R = np.inf\n            \n        # Declare leakage if R exceeds the threshold tau\n        leakage_detected = R > tau\n        results.append(leakage_detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3165689"}, {"introduction": "A theoretical privacy guarantee is only as strong as its implementation. This final practice moves from building private systems to auditing them, a crucial step in deploying trustworthy AI. You will learn how to design and execute a quantitative audit that empirically tests if a system's real-world privacy leakage aligns with its advertised $\\epsilon$ value. By simulating a membership inference attack and using statistical tools to compare the empirical attack success against the theoretical privacy bound, you will gain a practical understanding of what $\\epsilon$ means in adversarial terms and how to detect potential flaws in a private system [@problem_id:3165736].", "problem": "You are tasked with implementing a quantitative audit of a differentially private mechanism in a stylized membership-inference setting. The audit should compare an empirical attack success rate against a theoretically justified upper bound implied solely by the privacy accountant’s parameter $ \\epsilon $ and should decide whether there is a detectable discrepancy that suggests a potential implementation error.\n\nStart from the following fundamental base:\n\n- A randomized mechanism $ M $ satisfies $(\\epsilon, 0)$-Differential Privacy (DP) if for all neighboring datasets $ D $ and $ D' $ (differing in one individual) and for all measurable events $ S $, $ \\mathbb{P}(M(D) \\in S) \\leq e^{\\epsilon} \\, \\mathbb{P}(M(D') \\in S) $. \n- Under equal priors, an optimal membership-inference adversary is the Bayes-optimal test that minimizes error, which is given by the likelihood ratio test, and its success probability is $ \\frac{1 + \\mathrm{TV}(P, Q)}{2} $, where $ \\mathrm{TV}(P, Q) $ is the total variation distance between the two output distributions $ P $ and $ Q $.\n- The Laplace mechanism that adds noise $ \\eta \\sim \\mathrm{Laplace}(0, b) $ to a query of global sensitivity $ 1 $ achieves $(\\epsilon, 0)$-DP with $ \\epsilon = \\frac{1}{b} $.\n\nYour program must implement the following audit procedure:\n\n1. For a fixed integer count $ c_{\\mathrm{in}} $ and its neighboring count $ c_{\\mathrm{out}} = c_{\\mathrm{in}} - 1 $, define a mechanism that releases $ Y = c + \\eta $, where $ c $ equals $ c_{\\mathrm{in}} $ if the individual is in the dataset and $ c_{\\mathrm{out}} $ otherwise. The noise $ \\eta $ is drawn from a Laplace distribution with scale $ b = \\frac{\\mathrm{factor}}{\\epsilon} $, where $ \\epsilon $ is the privacy accountant’s parameter and $ \\mathrm{factor} $ is a multiplicative implementation factor. The correct implementation should have $ \\mathrm{factor} = 1 $, while deviations model potential implementation errors.\n\n2. To estimate the empirical attack success rate, simulate $ N $ independent trials with equal prior $ \\frac{1}{2} $ for membership. In each trial, draw a Bernoulli variable with probability $ \\frac{1}{2} $ to decide whether the output comes from the “in” distribution $ \\mathrm{Laplace}(c_{\\mathrm{in}}, b) $ or the “out” distribution $ \\mathrm{Laplace}(c_{\\mathrm{out}}, b) $. Use the Bayes-optimal decision rule specialized to this pair of distributions: decide “in” if $ Y \\geq \\frac{c_{\\mathrm{in}} + c_{\\mathrm{out}}}{2} $ and “out” otherwise. The empirical success rate is the fraction of correct decisions among $ N $ trials.\n\n3. Derive from the $(\\epsilon, 0)$-DP definition a rigorous upper bound on the Bayes-optimal success probability for equal priors as a function of $ \\epsilon $ only, without referencing the mechanism internals. Use this bound as the “accountant-predicted” upper limit.\n\n4. To avoid false positives due to sampling variability, compute a non-asymptotic tolerance using Hoeffding’s inequality for Bernoulli trials. For a user-specified confidence parameter $ \\alpha \\in (0, 1) $, with probability at least $ 1 - \\alpha $, the empirical success rate $ \\hat{p} $ deviates from the true success probability $ p $ by at most\n$$\n\\tau = \\sqrt{\\frac{\\ln\\left(\\frac{2}{\\alpha}\\right)}{2 N}}.\n$$\nFlag a discrepancy if $ \\hat{p} - \\tau $ strictly exceeds the accountant-predicted upper bound.\n\nYour program must implement the above audit for the following test suite, each test case specified as a tuple $ (\\epsilon, \\mathrm{factor}, N, \\alpha, \\mathrm{seed}) $:\n\n- Test $ 1 $: $ (\\epsilon = 1.0, \\mathrm{factor} = 1.0, N = 100000, \\alpha = 10^{-6}, \\mathrm{seed} = 42) $.\n- Test $ 2 $: $ (\\epsilon = 1.0, \\mathrm{factor} = 0.5, N = 50000, \\alpha = 10^{-4}, \\mathrm{seed} = 7) $.\n- Test $ 3 $: $ (\\epsilon = 0.001, \\mathrm{factor} = 1.0, N = 200000, \\alpha = 10^{-6}, \\mathrm{seed} = 2023) $.\n- Test $ 4 $: $ (\\epsilon = 5.0, \\mathrm{factor} = 1.0, N = 50000, \\alpha = 10^{-6}, \\mathrm{seed} = 123456) $.\n- Test $ 5 $: $ (\\epsilon = 1.0, \\mathrm{factor} = 0.8, N = 500, \\alpha = 0.05, \\mathrm{seed} = 99) $.\n\nImplementation details:\n\n- Use a fixed integer $ c_{\\mathrm{in}} = 10 $ and set $ c_{\\mathrm{out}} = 9 $. The midpoint decision threshold is $ \\frac{c_{\\mathrm{in}} + c_{\\mathrm{out}}}{2} = 9.5 $.\n- Use the provided random seeds to ensure reproducibility.\n\nFinal output specification:\n\n- For each test case, output a boolean indicating whether a discrepancy was detected.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $ [\\mathrm{result}_1,\\mathrm{result}_2,\\mathrm{result}_3,\\mathrm{result}_4,\\mathrm{result}_5] $, where each $ \\mathrm{result}_i $ is either $ \\mathrm{True} $ or $ \\mathrm{False} $.", "solution": "The task is to implement a quantitative audit of a differentially private mechanism. The audit compares an empirically measured attack success rate against a theoretical upper bound derived from the privacy parameter $\\epsilon$. A discrepancy is flagged if the empirical rate is statistically significantly higher than the theoretical bound, suggesting a potential implementation error. The process involves three main steps: deriving the theoretical bound, simulating the attack to find an empirical success rate, and applying a statistical test to make a decision.\n\n### 1. Theoretical Upper Bound from Differential Privacy\n\nA mechanism $M$ provides $(\\epsilon, 0)$-Differential Privacy (DP) if for any two neighboring datasets $D$ and $D'$, and any event $S$, the output distributions $P=M(D)$ and $Q=M(D')$ satisfy $\\mathbb{P}(P \\in S) \\leq e^{\\epsilon} \\mathbb{P}(Q \\in S)$.\n\nThe problem states that for equal priors, the success probability of an optimal membership-inference adversary is $p_{succ} = \\frac{1 + \\mathrm{TV}(P, Q)}{2}$, where $\\mathrm{TV}(P, Q)$ is the total variation distance between the output distributions. To find the \"accountant-predicted\" upper bound, we must find the maximum possible value of $\\mathrm{TV}(P, Q)$ under the $(\\epsilon, 0)$-DP constraint, irrespective of the specific mechanism.\n\nIt is a standard result in differential privacy that for any two distributions $P$ and $Q$ satisfying the $(\\epsilon, 0)$-DP condition, their total variation distance is bounded by:\n$$\n\\mathrm{TV}(P, Q) \\leq \\frac{e^\\epsilon - 1}{e^\\epsilon + 1}\n$$\nThis bound is tight; there exists a pair of distributions satisfying $(\\epsilon, 0)$-DP that achieves this maximum TV distance.\n\nSubstituting this maximum possible TV distance into the formula for the adversary's success probability gives the theoretical upper bound, which we denote as $p_{bound}$:\n$$\np_{succ} \\leq \\frac{1}{2} \\left( 1 + \\frac{e^\\epsilon - 1}{e^\\epsilon + 1} \\right)\n$$\nSimplifying this expression yields the accountant-predicted upper bound on success probability:\n$$\np_{bound} = \\frac{1}{2} \\left( \\frac{e^\\epsilon + 1 + e^\\epsilon - 1}{e^\\epsilon + 1} \\right) = \\frac{1}{2} \\left( \\frac{2e^\\epsilon}{e^\\epsilon + 1} \\right) = \\frac{e^\\epsilon}{e^\\epsilon + 1}\n$$\nThis bound depends only on the claimed privacy parameter $\\epsilon$.\n\n### 2. Empirical Attack Success Rate Simulation\n\nThe audit simulates a specific membership inference attack to estimate the mechanism's true attack success probability.\n- The mechanism adds noise from a Laplace distribution, $Y = c + \\eta$, where $\\eta \\sim \\mathrm{Laplace}(0, b)$.\n- The scale parameter is $b = \\frac{\\mathrm{factor}}{\\epsilon}$. For a correct implementation on a query with global sensitivity $1$, we should have $\\mathrm{factor}=1$. The global sensitivity is $\\Delta c = |c_{\\mathrm{in}} - c_{\\mathrm{out}}| = |10 - 9| = 1$.\n- Two distributions are considered: $P_{in}$ corresponding to $c=c_{\\mathrm{in}}=10$, so $Y \\sim \\mathrm{Laplace}(10, b)$, and $P_{out}$ for $c=c_{\\mathrm{out}}=9$, so $Y \\sim \\mathrm{Laplace}(9, b)$.\n- The Bayes-optimal decision rule for distinguishing between these two symmetric distributions with equal priors is to compare the likelihoods, which simplifies to checking which mean the output $Y$ is closer to. The decision boundary is the midpoint $T = \\frac{c_{\\mathrm{in}} + c_{\\mathrm{out}}}{2} = 9.5$. The rule is: decide \"in\" if $Y \\geq 9.5$, and \"out\" otherwise.\n\nThe simulation runs $N$ trials. In each trial:\n1. A true state (\"in\" or \"out\") is chosen with probability $\\frac{1}{2}$.\n2. A noisy output $Y$ is generated from the corresponding distribution, $\\mathrm{Laplace}(c_{\\mathrm{in}}, b)$ or $\\mathrm{Laplace}(c_{\\mathrm{out}}, b)$.\n3. The decision rule is applied to $Y$.\n4. The decision is compared with the true state to check if it was correct.\n\nThe empirical success rate, $\\hat{p}$, is the total number of correct decisions divided by the total number of trials, $N$. This $\\hat{p}$ is an estimate of the true success probability, $p_{true}$, for this specific attack on this specific mechanism.\n\nFor completeness, the analytical value for $p_{true}$ is derived from the success probability of the midpoint classifier on two Laplace distributions. The probability of a correct decision is $p_{true} = \\frac{1}{2} \\mathbb{P}(\\text{correct} | \\text{in}) + \\frac{1}{2} \\mathbb{P}(\\text{correct} | \\text{out})$. By symmetry, the two conditional probabilities are equal: $\\mathbb{P}(Y \\geq T | Y \\sim \\mathrm{Laplace}(c_{\\mathrm{in}}, b)) = \\mathbb{P}(Y < T | Y \\sim \\mathrm{Laplace}(c_{\\mathrm{out}}, b))$. Using the CDF of the Laplace distribution, this probability is $1 - \\frac{1}{2} \\exp\\left(-\\frac{c_{\\mathrm{in}}-T}{b}\\right)$. This results in:\n$$\np_{true} = 1 - \\frac{1}{2} \\exp\\left(-\\frac{\\Delta c}{2b}\\right) = 1 - \\frac{1}{2} \\exp\\left(-\\frac{\\epsilon}{2 \\cdot \\mathrm{factor}}\\right)\n$$\n\n### 3. Statistical Test for Discrepancy Detection\n\nThe empirical rate $\\hat{p}$ is a random variable. To account for sampling variability and avoid false alarms, we use Hoeffding's inequality. For $N$ Bernoulli trials with true probability $p$, the empirical estimate $\\hat{p}$ deviates from $p$ by at most a tolerance $\\tau$ with high probability:\n$$\n\\mathbb{P}(|\\hat{p} - p| \\geq \\tau) \\leq 2e^{-2N\\tau^2}\n$$\nSetting the right-hand side to a small confidence parameter $\\alpha$, we can solve for the one-sided tolerance $\\tau$:\n$$\n\\alpha = 2e^{-2N\\tau^2} \\implies \\frac{\\alpha}{2} = e^{-2N\\tau^2} \\implies \\ln\\left(\\frac{2}{\\alpha}\\right) = 2N\\tau^2 \\implies \\tau = \\sqrt{\\frac{\\ln\\left(\\frac{2}{\\alpha}\\right)}{2 N}}\n$$\nWith probability at least $1-\\alpha$, the true success rate $p_{true}$ is not more than $\\hat{p}+\\tau$. If the implementation is faulty making it less private (e.g., $\\mathrm{factor} < 1$), $p_{true}$ might exceed the accountant's bound $p_{bound}$. We flag a discrepancy if we have strong statistical evidence that $p_{true} > p_{bound}$. This is established if the lower bound of our confidence interval for $p_{true}$, which is $\\hat{p} - \\tau$, is strictly greater than the theoretical maximum $p_{bound}$.\n\nThe final audit condition is to flag a discrepancy if and only if:\n$$\n\\hat{p} - \\tau > p_{bound}\n$$\n\n### Summary of the Audit Algorithm\nFor each test case $(\\epsilon, \\mathrm{factor}, N, \\alpha, \\mathrm{seed})$:\n1.  Calculate the Laplace scale parameter $b = \\frac{\\mathrm{factor}}{\\epsilon}$.\n2.  Set up a random number generator with the given `seed`.\n3.  Simulate $N$ trials of the membership inference attack to compute the empirical success rate $\\hat{p}$.\n4.  Calculate the accountant-predicted upper bound: $p_{bound} = \\frac{e^\\epsilon}{e^\\epsilon + 1}$.\n5.  Calculate the statistical tolerance: $\\tau = \\sqrt{\\frac{\\ln(2/\\alpha)}{2N}}$.\n6.  Compare $\\hat{p} - \\tau$ with $p_{bound}$. If $\\hat{p} - \\tau > p_{bound}$, a discrepancy is detected (True). Otherwise, no discrepancy is detected (False).\n7.  Collect the boolean results for all test cases.", "answer": "```python\nimport numpy as np\n\ndef run_audit(epsilon, factor, N, alpha, seed):\n    \"\"\"\n    Performs a quantitative audit of a differentially private mechanism.\n\n    Args:\n        epsilon (float): The claimed privacy parameter epsilon.\n        factor (float): The implementation factor for the Laplace scale.\n        N (int): The number of simulation trials.\n        alpha (float): The confidence parameter for Hoeffding's inequality.\n        seed (int): The random seed for reproducibility.\n\n    Returns:\n        bool: True if a discrepancy is detected, False otherwise.\n    \"\"\"\n    # Initialize random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # --- Step 1: Mechanism and Attack Setup ---\n    c_in = 10.0\n    c_out = 9.0\n    decision_threshold = (c_in + c_out) / 2.0\n    \n    # The global sensitivity of the count query is |c_in - c_out| = 1.\n    # The Laplace scale b should be GS/epsilon = 1/epsilon for a correct implementation.\n    # A 'factor' is introduced to model implementation errors.\n    if epsilon == 0:\n        # Avoid division by zero, although not in test cases.\n        # Infinite noise means output is always 0, attack success is 0.5.\n        b = float('inf')\n    else:\n        b = factor / epsilon\n\n    # --- Step 2: Estimate Empirical Attack Success Rate ---\n    correct_decisions = 0\n    \n    # Generate all random choices at once for efficiency\n    true_labels_are_in = rng.random(size=N) < 0.5\n    \n    # Generate Laplace noise for both 'in' and 'out' cases\n    # Note: np.random.laplace takes scale parameter b\n    laplace_noise = rng.laplace(loc=0.0, scale=b, size=N)\n\n    # Calculate noisy outputs\n    outputs_in = c_in + laplace_noise\n    outputs_out = c_out + laplace_noise\n    \n    # Combine outputs based on true labels\n    Y = np.where(true_labels_are_in, outputs_in, outputs_out)\n    \n    # Apply the Bayes-optimal decision rule\n    decisions_are_in = Y >= decision_threshold\n    \n    # Count correct decisions\n    correct_decisions = np.sum(decisions_are_in == true_labels_are_in)\n\n    # Empirical success rate\n    p_hat = correct_decisions / N\n\n    # --- Step 3: Theoretical Bound ---\n    # The upper bound on success probability is derived from the TV-distance bound of (eps, 0)-DP\n    # p_bound = (1 + (e^eps - 1)/(e^eps + 1)) / 2 = e^eps / (e^eps + 1)\n    p_bound = np.exp(epsilon) / (np.exp(epsilon) + 1.0)\n\n    # --- Step 4: Statistical Test ---\n    # Calculate the tolerance tau using Hoeffding's inequality\n    tau = np.sqrt(np.log(2.0 / alpha) / (2.0 * N))\n    \n    # A discrepancy is flagged if the empirical rate, adjusted for sampling error,\n    # is strictly greater than the theoretical bound.\n    discrepancy_detected = (p_hat - tau) > p_bound\n\n    return discrepancy_detected\n\ndef solve():\n    \"\"\"\n    Solves the problem by running the audit for each test case.\n    \"\"\"\n    test_cases = [\n        # (epsilon, factor, N, alpha, seed)\n        (1.0, 1.0, 100000, 1e-6, 42),\n        (1.0, 0.5, 50000, 1e-4, 7),\n        (0.001, 1.0, 200000, 1e-6, 2023),\n        (5.0, 1.0, 50000, 1e-6, 123456),\n        (1.0, 0.8, 500, 0.05, 99)\n    ]\n\n    results = []\n    for case in test_cases:\n        epsilon, factor, N, alpha, seed = case\n        result = run_audit(epsilon, factor, N, alpha, seed)\n        results.append(str(result))\n\n    # Print the final output in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3165736"}]}