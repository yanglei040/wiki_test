{"hands_on_practices": [{"introduction": "A core challenge in automated machine learning is discovering architectures that not only fit the training data but also generalize to new, unseen data. This exercise provides a hands-on simulation to explore this very issue in the context of Neural Architecture Search (NAS). By using a simple polynomial model space, you will empirically investigate how a search strategy based on maximizing training accuracy can favor overly complex models, especially when data is limited, leading to poor generalization [@problem_id:3158070]. This practice will help you build a concrete understanding of the generalization gap and the problem of overfitting in NAS.", "problem": "You are asked to implement a small, fully reproducible simulation to study Neural Architecture Search (NAS) under limited training data. The problem focuses on whether NAS that optimizes an empirical objective on the training set implicitly favors over-parameterized models when the number of training samples is small. Use the following precise and self-contained setup.\n\nDefine the architecture space as a set of polynomial models with degree parameter $\\alpha \\in \\mathcal{A}$, where each model uses the feature map $\\phi_{\\alpha}(x) = \\left[x^{0}, x^{1}, \\dots, x^{\\alpha}\\right]$. Treat the degree $\\alpha$ as a surrogate for model capacity and the number of trainable parameters. Consider supervised binary classification with inputs $x \\in [-1,1]$ and labels $y \\in \\{0,1\\}$. Data are generated from a fixed, unknown target function with additive Gaussian noise:\n- Input distribution: $x \\sim \\mathrm{Uniform}([-1,1])$.\n- Latent target: $g(x) = \\sin(3\\pi x) + 0.2 x$.\n- Label noise: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ with $\\sigma = 0.35$.\n- Label: $y = \\mathbb{1}\\{g(x) + \\varepsilon \\ge 0\\}$.\n\nTraining rule for a given $\\alpha$: fit a ridge-regularized least-squares predictor $f_{\\alpha}(x) = w_{\\alpha}^{\\top}\\phi_{\\alpha}(x)$ that minimizes the empirical squared error on the training set of size $n$, with ridge penalty $\\lambda > 0$:\n$$\n\\hat{w}_{\\alpha} = \\arg\\min_{w} \\frac{1}{n}\\sum_{i=1}^{n} \\left(y_{i} - w^{\\top}\\phi_{\\alpha}(x_{i})\\right)^{2} + \\lambda \\lVert w \\rVert_{2}^{2}.\n$$\nUse the closed-form solution\n$$\n\\hat{w}_{\\alpha} = (X_{\\alpha}^{\\top}X_{\\alpha} + \\lambda I)^{-1} X_{\\alpha}^{\\top} y,\n$$\nwhere $X_{\\alpha} \\in \\mathbb{R}^{n \\times (\\alpha+1)}$ is the design matrix with rows $\\phi_{\\alpha}(x_{i})^{\\top}$ and $y \\in \\{0,1\\}^{n}$ is the vector of labels. For evaluation, convert predictions to class labels by thresholding at $0.5$, i.e., $\\hat{y} = \\mathbb{1}\\{f_{\\alpha}(x) \\ge 0.5\\}$, and define accuracy as the fraction of correctly classified points. For each $\\alpha$, define the generalization gap\n$$\n\\Delta(\\alpha) = A_{\\text{train}}(\\alpha) - A_{\\text{val}}(\\alpha),\n$$\nwhere $A_{\\text{train}}(\\alpha)$ is the training accuracy and $A_{\\text{val}}(\\alpha)$ is the accuracy on a large, held-out validation set.\n\nFundamental base: You must base your reasoning on Empirical Risk Minimization (ERM), the definition of a generalization gap as the difference between empirical and population performance, and the well-tested observation that higher-capacity models can fit noise when $n$ is small, inflating $A_{\\text{train}}$ but not $A_{\\text{val}}$. You must not assume any shortcut formulas; derive all steps from these principles.\n\nImplementation details and experimental protocol:\n- Architecture set: $\\mathcal{A} = \\{1, 3, 5, 9, 13, 19, 27, 39\\}$.\n- Ridge parameter: $\\lambda = 10^{-6}$.\n- Training data come from a fixed pool of size $N_{\\text{pool}} = 4096$. Construct this pool once using a single random seed $s_{\\text{pool}} = 12345$, and then form training sets by subsampling prefixes of this pool. This ensures the small-$n$ sets are nested within larger ones. That is, for training size $n$, use the first $n$ points from the pool.\n- Validation set: size $V = 10000$, drawn independently with a fixed seed $s_{\\text{val}} = 2025$.\n- Search rule emulating NAS under two selection criteria for each training size $n$:\n  1. Training-selection (proxy-driven NAS): choose $\\alpha_{\\text{train-sel}} \\in \\arg\\max_{\\alpha \\in \\mathcal{A}} A_{\\text{train}}(\\alpha)$. Tie-breaking rule: pick the largest $\\alpha$ among maximizers.\n  2. Validation-selection (oracle-like NAS): choose $\\alpha_{\\text{val-sel}} \\in \\arg\\max_{\\alpha \\in \\mathcal{A}} A_{\\text{val}}(\\alpha)$. Tie-breaking rule: pick the smallest $\\alpha$ among maximizers.\n- Training sizes to test (the test suite): $n \\in \\{16, 64, 256, 1024\\}$. These values probe a small-sample regime, a moderate regime, and a larger-sample regime.\n\nYour program must:\n- Implement the above data generation, training, evaluation, and selection procedures exactly.\n- For each $n$ in the test suite, compute and record the quadruple $[\\alpha_{\\text{train-sel}}, \\alpha_{\\text{val-sel}}, I, \\Delta(\\alpha_{\\text{train-sel}})]$, where $I = 1$ if $\\alpha_{\\text{train-sel}} > \\alpha_{\\text{val-sel}}$ and $I = 0$ otherwise, and where $\\Delta(\\alpha_{\\text{train-sel}})$ is rounded to four decimals.\n- Use the angle unit-free setting; there are no physical units to report. All numeric results are pure numbers.\n- Final output format: Your program should produce a single line of output containing a list of results, one per $n$ in the specified order $[16, 64, 256, 1024]$. Each result must itself be a list of four numbers as described above. For example, a syntactically valid output looks like $[[a_{1}, b_{1}, c_{1}, d_{1}], [a_{2}, b_{2}, c_{2}, d_{2}], [a_{3}, b_{3}, c_{3}, d_{3}], [a_{4}, b_{4}, c_{4}, d_{4}]]$, where $a_{i}$ and $b_{i}$ are integers in $\\mathcal{A}$, $c_{i}$ is either $0$ or $1$, and $d_{i}$ is a float rounded to four decimals.\n\nWhat you must determine empirically via your implementation and report for each $n$ is whether the training-selected model favors higher capacity than the validation-selected model and how large the overfitting gap $\\Delta(\\alpha)$ is at that selection. The results must follow the exact selection and tie-breaking rules stated above, use the exact seeds and sample sizes stated above, and be reproducible.", "solution": "The problem statement is valid. It outlines a well-defined computational experiment to study a fundamental issue in machine learning and Neural Architecture Search (NAS): the tendency of empirical objectives to favor overly complex models when training data is limited. The setup is scientifically sound, fully specified, and computationally feasible.\n\nThe core of the problem rests on the principle of Empirical Risk Minimization (ERM). In supervised learning, the goal is to find a function $f$ that minimizes the true risk, or expected loss, $R(f) = \\mathbb{E}_{(x,y) \\sim P}[L(f(x), y)]$, where $P$ is the true data distribution. Since $P$ is unknown, we instead minimize the empirical risk on a training set $S = \\{(x_i, y_i)\\}_{i=1}^n$, given by $R_{\\text{emp}}(f) = \\frac{1}{n} \\sum_{i=1}^n L(f(x_i), y_i)$. The problem investigates the discrepancy between the model selected by minimizing empirical risk and the model that truly generalizes best.\n\nThis simulation operationalizes the investigation as follows:\n\n1.  **Architecture Space and Capacity**: The set of polynomial models of varying degree, $\\alpha \\in \\mathcal{A} = \\{1, 3, 5, 9, 13, 19, 27, 39\\}$, serves as the architecture search space. The degree $\\alpha$ acts as a direct proxy for model capacity, as it determines the dimensionality of the feature space spanned by $\\phi_{\\alpha}(x) = [x^{0}, x^{1}, \\dots, x^{\\alpha}]$. A higher $\\alpha$ allows the model to fit more complex functions.\n\n2.  **Data Generation**: The data are generated from a fixed, known process. Inputs $x$ are drawn from $\\mathrm{Uniform}([-1,1])$. The labels $y \\in \\{0, 1\\}$ are determined by a latent function $g(x) = \\sin(3\\pi x) + 0.2 x$ with added Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, where $\\sigma = 0.35$. The rule is $y = \\mathbb{1}\\{g(x) + \\varepsilon \\ge 0\\}$. This setup provides a ground truth against which model performance can be objectively measured, and the noise term $\\varepsilon$ provides an opportunity for models to overfit.\n\n3.  **Training as ERM**: For a given architecture $\\alpha$ and training set of size $n$, the weights $w_{\\alpha}$ are found by minimizing the regularized empirical squared error:\n    $$\n    \\hat{w}_{\\alpha} = \\arg\\min_{w} \\frac{1}{n}\\sum_{i=1}^{n} \\left(y_{i} - w^{\\top}\\phi_{\\alpha}(x_{i})\\right)^{2} + \\lambda \\lVert w \\rVert_{2}^{2}\n    $$\n    This is a classic ERM problem with ridge regularization. The small regularization term $\\lambda = 10^{-6}$ ensures the problem is well-posed, particularly when the number of features $(\\alpha+1)$ is greater than the number of samples $n$. The specified closed-form solution $\\hat{w}_{\\alpha} = (X_{\\alpha}^{\\top}X_{\\alpha} + \\lambda I)^{-1} X_{\\alpha}^{\\top} y$ is a direct consequence of this optimization problem, where $X_{\\alpha}$ is the design matrix with rows $\\phi_{\\alpha}(x_i)^\\top$.\n\n4.  **Simulating NAS and Evaluating Generalization**: The experiment simulates two NAS strategies for different training set sizes $n \\in \\{16, 64, 256, 1024\\}$:\n    -   **Training Selection ($\\alpha_{\\text{train-sel}}$)**: This mimics a practical NAS scenario where the best architecture is chosen based on its performance on the available training data ($A_{\\text{train}}$). This selection is based on an empirical proxy for the true objective. The tie-breaking rule (largest $\\alpha$) reflects a potential implicit bias towards complexity.\n    -   **Validation Selection ($\\alpha_{\\text{val-sel}}$)**: This serves as an \"oracle\" by selecting the architecture that performs best on a large, independent validation set ($A_{\\text{val}}$). This choice is assumed to be close to the one that truly minimizes generalization error. The tie-breaking rule (smallest $\\alpha$) reflects a preference for simplicity (Occam's razor).\n\n    The central hypothesis is that for small $n$, the training data contains sampling noise that high-capacity models can easily fit. This will artificially inflate $A_{\\text{train}}$ for large $\\alpha$, leading to the selection of an overly complex model, i.e., $\\alpha_{\\text{train-sel}} > \\alpha_{\\text{val-sel}}$. This phenomenon is known as overfitting. The generalization gap, $\\Delta(\\alpha) = A_{\\text{train}}(\\alpha) - A_{\\text{val}}(\\alpha)$, quantifies this overfitting. A large positive value for $\\Delta(\\alpha_{\\text{train-sel}})$ indicates that the model chosen via the training-based proxy has learned patterns specific to the training set that do not generalize. As $n$ increases, the training set becomes more representative of the true distribution, the empirical risk becomes a more reliable proxy for the true risk, and we expect $\\alpha_{\\text{train-sel}}$ and $\\alpha_{\\text{val-sel}}$ to converge, with the generalization gap diminishing.\n\nThe implementation will proceed as follows:\n-   First, the training data pool (size $N_{\\text{pool}}=4096$) and the validation set (size $V=10000$) will be generated once using their respective fixed random seeds, $s_{\\text{pool}}=12345$ and $s_{\\text{val}}=2025$.\n-   The program will then iterate through each specified training size $n$. For each $n$, the first $n$ samples from the pool will form the training set.\n-   Within this loop, it will iterate through every architecture degree $\\alpha \\in \\mathcal{A}$. For each $\\alpha$, it will:\n    1.  Construct the Vandermonde-like design matrix $X_\\alpha$.\n    2.  Solve the linear system $(X_{\\alpha}^{\\top}X_{\\alpha} + \\lambda I) w = X_{\\alpha}^{\\top} y$ to find the optimal weights $\\hat{w}_{\\alpha}$.\n    3.  Compute the model's predictions $f_{\\alpha}(x) = X_{\\alpha} \\hat{w}_{\\alpha}$ on both the training and validation sets.\n    4.  Apply the threshold of $0.5$ to get binary labels $\\hat{y} = \\mathbb{1}\\{f_{\\alpha}(x) \\ge 0.5\\}$.\n    5.  Calculate and store the training accuracy $A_{\\text{train}}(\\alpha)$ and validation accuracy $A_{\\text{val}}(\\alpha)$.\n-   After evaluating all architectures for a given $n$, the code will determine $\\alpha_{\\text{train-sel}}$ and $\\alpha_{\\text{val-sel}}$ by finding the maximums of the stored accuracies and applying the specified tie-breaking rules.\n-   Finally, it will compute the indicator $I = \\mathbb{1}\\{\\alpha_{\\text{train-sel}} > \\alpha_{\\text{val-sel}}\\}$ and the generalization gap $\\Delta(\\alpha_{\\text{train-sel}})$, rounded to four decimal places.\n-   The resulting quadruple will be appended to a list, and the final output will be a list of these quadruples for all tested values of $n$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a simulation to study Neural Architecture Search (NAS)\n    under limited training data.\n    \"\"\"\n    # Define experimental parameters from the problem statement.\n    ARCH_SET = [1, 3, 5, 9, 13, 19, 27, 39]\n    LAMBDA = 1e-6\n    N_POOL = 4096\n    S_POOL = 12345\n    V_SIZE = 10000\n    S_VAL = 2025\n    N_TEST_SUITE = [16, 64, 256, 1024]\n    NOISE_STD = 0.35\n\n    def g(x):\n        \"\"\"The latent target function.\"\"\"\n        return np.sin(3 * np.pi * x) + 0.2 * x\n\n    def generate_data(num_samples, seed):\n        \"\"\"Generates input-label pairs based on the problem's specification.\"\"\"\n        rng = np.random.default_rng(seed)\n        x = rng.uniform(-1, 1, size=num_samples)\n        epsilon = rng.normal(0, NOISE_STD, size=num_samples)\n        y = (g(x) + epsilon >= 0).astype(int)\n        return x, y\n\n    def evaluate_model(w, X, y_true):\n        \"\"\"Calculates model accuracy given weights, design matrix, and true labels.\"\"\"\n        y_pred_scores = X @ w\n        y_pred_labels = (y_pred_scores >= 0.5).astype(int)\n        return np.mean(y_pred_labels == y_true)\n\n    # Generate the fixed datasets for the experiment.\n    x_pool, y_pool = generate_data(N_POOL, S_POOL)\n    x_val, y_val = generate_data(V_SIZE, S_VAL)\n\n    # Pre-calculate validation design matrices for efficiency.\n    val_design_matrices = {\n        alpha: np.vander(x_val, N=alpha + 1, increasing=True)\n        for alpha in ARCH_SET\n    }\n\n    all_results = []\n    \n    # Iterate over the different training set sizes.\n    for n in N_TEST_SUITE:\n        x_train = x_pool[:n]\n        y_train = y_pool[:n]\n\n        train_accuracies = {}\n        val_accuracies = {}\n\n        # Iterate over all architectures (polynomial degrees).\n        for alpha in ARCH_SET:\n            # Create design matrix for the current training set and architecture.\n            X_train_alpha = np.vander(x_train, N=alpha + 1, increasing=True)\n            \n            # Train the ridge-regularized least-squares model using the closed-form solution.\n            d = alpha + 1\n            A = X_train_alpha.T @ X_train_alpha + LAMBDA * np.identity(d)\n            b = X_train_alpha.T @ y_train\n            try:\n                # Use np.linalg.solve for numerical stability.\n                w_alpha = np.linalg.solve(A, b)\n            except np.linalg.LinAlgError:\n                # Fallback to pseudoinverse if solve fails (unlikely with ridge penalty).\n                pinv_A = np.linalg.pinv(A)\n                w_alpha = pinv_A @ b\n\n            # Evaluate the trained model on both training and validation sets.\n            train_acc = evaluate_model(w_alpha, X_train_alpha, y_train)\n            train_accuracies[alpha] = train_acc\n\n            X_val_alpha = val_design_matrices[alpha]\n            val_acc = evaluate_model(w_alpha, X_val_alpha, y_val)\n            val_accuracies[alpha] = val_acc\n            \n        # 1. Selection based on training accuracy (proxy-driven NAS)\n        # Find max accuracy, handling potential float precision issues.\n        max_train_acc = max(train_accuracies.values())\n        best_train_alphas = [\n            alpha for alpha, acc in train_accuracies.items() if np.isclose(acc, max_train_acc)\n        ]\n        # Apply tie-breaking rule: pick the largest alpha.\n        alpha_train_sel = max(best_train_alphas)\n\n        # 2. Selection based on validation accuracy (oracle-like NAS)\n        # Find max accuracy.\n        max_val_acc = max(val_accuracies.values())\n        best_val_alphas = [\n            alpha for alpha, acc in val_accuracies.items() if np.isclose(acc, max_val_acc)\n        ]\n        # Apply tie-breaking rule: pick the smallest alpha.\n        alpha_val_sel = min(best_val_alphas)\n        \n        # Calculate indicator I for whether training-selected model is more complex.\n        I = 1 if alpha_train_sel > alpha_val_sel else 0\n        \n        # Calculate the generalization gap for the training-selected model.\n        # Delta(alpha) = A_train(alpha) - A_val(alpha)\n        delta = train_accuracies[alpha_train_sel] - val_accuracies[alpha_train_sel]\n        \n        # Assemble the quadruple of results for the current n.\n        result_for_n = [alpha_train_sel, alpha_val_sel, I, round(delta, 4)]\n        all_results.append(result_for_n)\n\n    # Format the final output as a string representing a list of lists.\n    # The string representation of each inner list will include spaces, e.g., '[1, 2, 3]'.\n    # Joining these with ',' and wrapping with '[]' matches the required format style.\n    # Example: [[list1], [list2]] -> \"[[1, 2, 3],[4, 5, 6]]\"\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n    \n    # Remove spaces to match the stricter template-based format.\n    final_output = output_str.replace(\" \", \"\")\n    print(final_output)\n\nsolve()\n```", "id": "3158070"}, {"introduction": "The previous exercise demonstrated that relying on training accuracy as a performance predictor can be misleading. In practical NAS, we often use cheaper-to-evaluate 'proxies' to estimate an architecture's final performance, but how do we know if a proxy is reliable? This practice introduces you to the standard methods for answering that question [@problem_id:3158046]. You will use rank-based correlation coefficients, such as Spearman's $\\rho$ and Kendall's $\\tau$, to quantify the relationship between proxy scores and true performance, and develop a principled protocol for selecting the most stable and accurate proxy across different scenarios.", "problem": "You are given multiple search spaces in the context of Neural Architecture Search (NAS). For each search space, you have a set of architectures $ \\alpha $ that share a common index ordering across all proxies and the final metric. Each architecture $ \\alpha $ is associated with a cheap proxy score $ Z(\\alpha) $ and a final outcome $ A(\\alpha) $ (for example, validation accuracy). The aim is to quantify the monotonic relationship between proxy scores and final outcomes using rank-based correlations and then to devise a principled protocol to select the most reliable proxy across spaces. The protocol must be derived from first principles and provide a scientifically grounded justification for its use. You must implement the program to compute the required correlations and select the best proxy.\n\nFundamental definitions to be used include:\n- Spearman's rank correlation coefficient $ \\rho $, defined as the Pearson correlation between ranks of two variables.\n- Kendall's rank correlation coefficient $ \\tau $, defined in terms of concordant and discordant pairs, using a tie-adjusted definition consistent with the Kendall $ \\tau_b $ variant.\n\nFor each search space $ s $ and proxy $ p $, compute $ \\rho_{p,s} $ and $ \\tau_{p,s} $ between the sequence $ Z^{(p)}_s(\\alpha) $ and $ A_s(\\alpha) $. If a correlation is undefined due to constant values (for example, all $ Z^{(p)}_s(\\alpha) $ identical), treat the value as $ 0 $.\n\nYou must then devise and implement a principled protocol to select the \"best\" proxy $ p^{\\star} $ across spaces. Your protocol must:\n- Use $ \\rho_{p,s} $ and $ \\tau_{p,s} $ across spaces as the fundamental inputs.\n- Aggregate across spaces in a way that reflects both strength of monotonic alignment and cross-space stability.\n- Output, for each test case, the index of the selected proxy along with aggregated Spearman and Kendall values for the chosen proxy.\n\nAssume proxies are ordered as listed, with indices $ 0,1,2,\\dots $.\n\nTest Suite:\n- Test Case $ 1 $ (three spaces, three proxies):\n  - Space $ 1 $:\n    - $ A_1 = [0.70,0.74,0.76,0.79,0.82,0.85] $\n    - $ Z^{(0)}_1 = [0.68,0.73,0.75,0.80,0.81,0.86] $\n    - $ Z^{(1)}_1 = [0.90,0.86,0.83,0.80,0.77,0.74] $\n    - $ Z^{(2)}_1 = [0.69,0.75,0.74,0.78,0.83,0.84] $\n  - Space $ 2 $:\n    - $ A_2 = [0.60,0.63,0.68,0.71,0.74,0.80] $\n    - $ Z^{(0)}_2 = [0.59,0.64,0.67,0.70,0.73,0.79] $\n    - $ Z^{(1)}_2 = [0.81,0.77,0.72,0.69,0.65,0.61] $\n    - $ Z^{(2)}_2 = [0.58,0.65,0.66,0.69,0.74,0.78] $\n  - Space $ 3 $:\n    - $ A_3 = [0.50,0.55,0.60,0.62,0.66,0.70] $\n    - $ Z^{(0)}_3 = [0.49,0.54,0.59,0.61,0.65,0.69] $\n    - $ Z^{(1)}_3 = [0.71,0.67,0.63,0.62,0.58,0.54] $\n    - $ Z^{(2)}_3 = [0.51,0.57,0.58,0.60,0.64,0.68] $\n- Test Case $ 2 $ (two spaces, ties and constants):\n  - Space $ 1 $:\n    - $ A_1 = [0.70,0.72,0.74,0.76,0.78,0.80] $\n    - $ Z^{(0)}_1 = [0.10,0.10,0.20,0.20,0.30,0.30] $\n    - $ Z^{(1)}_1 = [0.30,0.30,0.20,0.20,0.10,0.10] $\n    - $ Z^{(2)}_1 = [1.00,1.00,1.00,1.00,1.00,1.00] $\n  - Space $ 2 $:\n    - $ A_2 = [0.67,0.69,0.71,0.73,0.74,0.75] $\n    - $ Z^{(0)}_2 = [10.00,10.00,20.00,20.00,30.00,30.00] $\n    - $ Z^{(1)}_2 = [30.00,30.00,20.00,20.00,10.00,10.00] $\n    - $ Z^{(2)}_2 = [5.00,5.00,5.00,5.00,5.00,5.00] $\n- Test Case $ 3 $ (three spaces, stability versus strength):\n  - Space $ 1 $:\n    - $ A_1 = [0.60,0.65,0.70,0.75,0.80,0.85] $\n    - $ Z^{(0)}_1 = [1.00,2.00,3.00,4.00,5.00,6.00] $\n    - $ Z^{(1)}_1 = [0.50,1.20,1.70,2.50,3.00,3.40] $\n    - $ Z^{(2)}_1 = [6.00,5.00,4.00,3.00,2.00,1.00] $\n  - Space $ 2 $:\n    - $ A_2 = [0.50,0.55,0.58,0.62,0.66,0.72] $\n    - $ Z^{(0)}_2 = [6.00,5.00,4.00,3.00,2.00,1.00] $\n    - $ Z^{(1)}_2 = [0.30,0.60,0.90,1.10,1.50,1.80] $\n    - $ Z^{(2)}_2 = [1.00,1.00,1.00,1.00,1.00,1.00] $\n  - Space $ 3 $:\n    - $ A_3 = [0.78,0.79,0.80,0.81,0.82,0.83] $\n    - $ Z^{(0)}_3 = [2.00,3.00,1.00,5.00,6.00,7.00] $\n    - $ Z^{(1)}_3 = [0.10,0.20,0.30,0.40,0.50,0.60] $\n    - $ Z^{(2)}_3 = [10.00,9.00,8.00,7.00,6.00,5.00] $\n- Test Case $ 4 $ (two spaces, small sample size):\n  - Space $ 1 $:\n    - $ A_1 = [0.70,0.75,0.80] $\n    - $ Z^{(0)}_1 = [3.00,2.00,1.00] $\n    - $ Z^{(1)}_1 = [1.00,2.00,3.00] $\n    - $ Z^{(2)}_1 = [1.00,1.00,2.00] $\n  - Space $ 2 $:\n    - $ A_2 = [0.65,0.70,0.76] $\n    - $ Z^{(0)}_2 = [3.00,2.00,1.00] $\n    - $ Z^{(1)}_2 = [0.00,1.00,2.00] $\n    - $ Z^{(2)}_2 = [2.00,2.00,1.00] $\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case, output a nested list of the form $ [p^{\\star},\\overline{\\rho},\\overline{\\tau}] $, where $ p^{\\star} $ is the selected proxy index, $ \\overline{\\rho} $ is the aggregated Spearman value for the selected proxy across spaces, and $ \\overline{\\tau} $ is the aggregated Kendall value for the selected proxy across spaces. Express $ \\overline{\\rho} $ and $ \\overline{\\tau} $ as decimal values rounded to four decimal places.\n- Example of the required single-line format: $ [[p_1,\\overline{\\rho}_1,\\overline{\\tau}_1],[p_2,\\overline{\\rho}_2,\\overline{\\tau}_2],\\dots] $.\n\nNotes:\n- There are no physical units in this problem.\n- If any correlation is undefined due to constant input, treat it as $ 0 $ when aggregating and selecting proxies.", "solution": "The problem requires us to develop a principled protocol for selecting the most reliable proxy for neural architecture performance prediction across multiple search spaces. This is a common challenge in Neural Architecture Search (NAS), where cheap-to-evaluate proxies $Z(\\alpha)$ are used to estimate the final, expensive-to-measure performance $A(\\alpha)$ of an architecture $\\alpha$. A reliable proxy should exhibit a strong and consistent monotonic relationship with the final performance.\n\nThe protocol will be developed in three stages:\n1.  Quantify the monotonic relationship within each search space using rank-based correlation coefficients.\n2.  Define a scoring metric to aggregate these correlations across spaces, rewarding both strength and stability.\n3.  Establish a selection rule to identify the best proxy based on this score.\n\n## 1. Rank-Based Correlation Coefficients\n\nTo measure the monotonic relationship between the proxy scores $Z(\\alpha)$ and the final outcomes $A(\\alpha)$, we use two standard non-parametric rank correlation coefficients: Spearman's $\\rho$ and Kendall's $\\tau$.\n\n### Spearman's Rank Correlation Coefficient ($\\rho$)\n\nSpearman's $\\rho$ is the Pearson correlation coefficient applied to the rank-transformed variables. For two sequences of data $X = \\{x_1, \\dots, x_n\\}$ and $Y = \\{y_1, \\dots, y_n\\}$, let their corresponding rank sequences be $r_X$ and $r_Y$. In cases of tied values, each tied value is assigned the average of the ranks that would have been assigned had they not been tied. The coefficient $\\rho$ is then:\n$$ \\rho = \\frac{\\sum_{i=1}^{n} (r_{X,i} - \\bar{r}_X)(r_{Y,i} - \\bar{r}_Y)}{\\sqrt{\\sum_{i=1}^{n} (r_{X,i} - \\bar{r}_X)^2 \\sum_{i=1}^{n} (r_{Y,i} - \\bar{r}_Y)^2}} $$\nwhere $\\bar{r}_X$ and $\\bar{r}_Y$ are the mean ranks. $\\rho$ ranges from $-1$ (perfect negative monotonic relationship) to $+1$ (perfect positive monotonic relationship).\n\n### Kendall's Rank Correlation Coefficient ($\\tau$)\n\nKendall's $\\tau$ assesses the similarity of the orderings of data when ranked by each of the quantities. A pair of observations $(x_i, y_i)$ and $(x_j, y_j)$ is concordant if the ranks of both elements agree, i.e., if $(x_i > x_j \\text{ and } y_i > y_j)$ or $(x_i < x_j \\text{ and } y_i < y_j)$. The pair is discordant if they disagree. We use the Kendall $\\tau_b$ variant, which adjusts for ties:\n$$ \\tau_b = \\frac{N_c - N_d}{\\sqrt{(N_0 - N_1)(N_0 - N_2)}} $$\nwhere $N_c$ is the number of concordant pairs, $N_d$ is the number of discordant pairs, $N_0 = n(n-1)/2$ is the total number of pairs, $N_1 = \\sum_i t_i(t_i-1)/2$ is the number of pairs tied only in the first variable, and $N_2 = \\sum_j u_j(u_j-1)/2$ is the number of pairs tied only in the second variable. $t_i$ and $u_j$ are the number of tied values in each tie group for the first and second variables, respectively. $\\tau_b$ also ranges from $-1$ to $+1$.\n\nAs per the problem specification, if a correlation is undefined due to one of the input vectors being constant (which results in zero variance of ranks and division by zero), its value will be treated as $0$.\n\n## 2. Principled Protocol for Proxy Selection\n\nA good proxy is not only one that has a high average correlation with the true outcomes but one that is also stable and reliable across different search spaces. A proxy that works perfectly on one space but is uncorrelated or anti-correlated on another is less useful than a proxy that is moderately well-correlated on all spaces. Our protocol must capture this trade-off between strength (average performance) and stability (consistency).\n\nFor each proxy $p$, we compute a set of correlation values across $S$ search spaces: $\\{\\rho_{p,s}\\}_{s=1}^S$ and $\\{\\tau_{p,s}\\}_{s=1}^S$.\n\n### Aggregation and Scoring\n\nTo balance strength and stability, we can formulate a score that rewards high mean correlation and penalizes high variance (or standard deviation) in correlation. A scientifically principled way to do this is to use a metric analogous to a lower confidence bound on the performance.\n\nFor a proxy $p$, we define its stability-adjusted score for Spearman's $\\rho$ as:\n$$ \\mathcal{S}_{\\rho}(p) = \\mu_{\\rho,p} - \\sigma_{\\rho,p} $$\nwhere $\\mu_{\\rho,p}$ is the mean of the Spearman correlations across all spaces, and $\\sigma_{\\rho,p}$ is the population standard deviation. The population standard deviation is used because we are evaluating performance over the complete, given set of spaces, not a sample.\n\nSimilarly, the score for Kendall's $\\tau$ is:\n$$ \\mathcal{S}_{\\tau}(p) = \\mu_{\\tau,p} - \\sigma_{\\tau,p} $$\n\nTo arrive at a single metric for selecting the best proxy, we combine these two scores. Since both $\\rho$ and $\\tau$ measure the same underlying property (monotonic association), it is reasonable to give them equal weight. We define the final score $\\mathcal{S}(p)$ for proxy $p$ as the average of its stability-adjusted scores:\n$$ \\mathcal{S}(p) = \\frac{1}{2} \\left( \\mathcal{S}_{\\rho}(p) + \\mathcal{S}_{\\tau}(p) \\right) = \\frac{1}{2} \\left[ (\\mu_{\\rho,p} - \\sigma_{\\rho,p}) + (\\mu_{\\tau,p} - \\sigma_{\\tau,p}) \\right] $$\n\n### Selection Rule\n\nThe best proxy, denoted $p^\\star$, is the one that maximizes this overall score. This proxy is considered the most reliable because it provides the best-guaranteed level of performance, taking into account its variability across different contexts.\n$$ p^\\star = \\arg\\max_{p} \\mathcal{S}(p) $$\n\nThe final output for each test case will be the index of the selected proxy, $p^\\star$, along with its mean Spearman ($\\overline{\\rho} = \\mu_{\\rho, p^\\star}$) and mean Kendall ($\\overline{\\tau} = \\mu_{\\tau, p^\\star}$) correlations across the spaces, rounded to four decimal places. These mean values represent the \"strength\" component of the chosen proxy's performance.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import spearmanr, kendalltau\n\ndef solve():\n    \"\"\"\n    Solves the Neural Architecture Search (NAS) proxy selection problem.\n    \"\"\"\n    test_cases = [\n        # Test Case 1: Three spaces, three proxies\n        (\n            [ # A_by_space\n                [0.70, 0.74, 0.76, 0.79, 0.82, 0.85],\n                [0.60, 0.63, 0.68, 0.71, 0.74, 0.80],\n                [0.50, 0.55, 0.60, 0.62, 0.66, 0.70],\n            ],\n            [ # Z_by_proxy_then_space\n                [[0.68, 0.73, 0.75, 0.80, 0.81, 0.86], [0.59, 0.64, 0.67, 0.70, 0.73, 0.79], [0.49, 0.54, 0.59, 0.61, 0.65, 0.69]], # Proxy 0\n                [[0.90, 0.86, 0.83, 0.80, 0.77, 0.74], [0.81, 0.77, 0.72, 0.69, 0.65, 0.61], [0.71, 0.67, 0.63, 0.62, 0.58, 0.54]], # Proxy 1\n                [[0.69, 0.75, 0.74, 0.78, 0.83, 0.84], [0.58, 0.65, 0.66, 0.69, 0.74, 0.78], [0.51, 0.57, 0.58, 0.60, 0.64, 0.68]], # Proxy 2\n            ]\n        ),\n        # Test Case 2: Two spaces, ties and constants\n        (\n            [ # A_by_space\n                [0.70, 0.72, 0.74, 0.76, 0.78, 0.80],\n                [0.67, 0.69, 0.71, 0.73, 0.74, 0.75],\n            ],\n            [ # Z_by_proxy_then_space\n                [[0.10, 0.10, 0.20, 0.20, 0.30, 0.30], [10.00, 10.00, 20.00, 20.00, 30.00, 30.00]], # Proxy 0\n                [[0.30, 0.30, 0.20, 0.20, 0.10, 0.10], [30.00, 30.00, 20.00, 20.00, 10.00, 10.00]], # Proxy 1\n                [[1.00, 1.00, 1.00, 1.00, 1.00, 1.00], [5.00, 5.00, 5.00, 5.00, 5.00, 5.00]],       # Proxy 2\n            ]\n        ),\n        # Test Case 3: Three spaces, stability versus strength\n        (\n            [ # A_by_space\n                [0.60, 0.65, 0.70, 0.75, 0.80, 0.85],\n                [0.50, 0.55, 0.58, 0.62, 0.66, 0.72],\n                [0.78, 0.79, 0.80, 0.81, 0.82, 0.83],\n            ],\n            [ # Z_by_proxy_then_space\n                [[1.00, 2.00, 3.00, 4.00, 5.00, 6.00], [6.00, 5.00, 4.00, 3.00, 2.00, 1.00], [2.00, 3.00, 1.00, 5.00, 6.00, 7.00]], # Proxy 0\n                [[0.50, 1.20, 1.70, 2.50, 3.00, 3.40], [0.30, 0.60, 0.90, 1.10, 1.50, 1.80], [0.10, 0.20, 0.30, 0.40, 0.50, 0.60]], # Proxy 1\n                [[6.00, 5.00, 4.00, 3.00, 2.00, 1.00], [1.00, 1.00, 1.00, 1.00, 1.00, 1.00], [10.00, 9.00, 8.00, 7.00, 6.00, 5.00]], # Proxy 2\n            ]\n        ),\n        # Test Case 4: Two spaces, small sample size\n        (\n            [ # A_by_space\n                [0.70, 0.75, 0.80],\n                [0.65, 0.70, 0.76],\n            ],\n            [ # Z_by_proxy_then_space\n                [[3.00, 2.00, 1.00], [3.00, 2.00, 1.00]], # Proxy 0\n                [[1.00, 2.00, 3.00], [0.00, 1.00, 2.00]], # Proxy 1\n                [[1.00, 1.00, 2.00], [2.00, 2.00, 1.00]], # Proxy 2\n            ]\n        ),\n    ]\n\n    all_results = []\n    \n    for A_by_space, Z_by_proxy_then_space in test_cases:\n        num_proxies = len(Z_by_proxy_then_space)\n        num_spaces = len(A_by_space)\n        \n        proxy_scores = []\n        proxy_rhos = []\n        proxy_taus = []\n\n        for p_idx in range(num_proxies):\n            rhos = []\n            taus = []\n            \n            for s_idx in range(num_spaces):\n                A_s = A_by_space[s_idx]\n                Z_ps = Z_by_proxy_then_space[p_idx][s_idx]\n\n                # Calculate Spearman's rho\n                rho, _ = spearmanr(A_s, Z_ps)\n                rho = np.nan_to_num(rho, nan=0.0)\n                rhos.append(rho)\n                \n                # Calculate Kendall's tau_b\n                tau, _ = kendalltau(A_s, Z_ps, variant='b')\n                tau = np.nan_to_num(tau, nan=0.0)\n                taus.append(tau)\n            \n            # Calculate mean and std dev for rho and tau across spaces\n            mu_rho = np.mean(rhos)\n            sigma_rho = np.std(rhos, ddof=0)\n            mu_tau = np.mean(taus)\n            sigma_tau = np.std(taus, ddof=0)\n            \n            # Calculate the stability-adjusted score\n            score = 0.5 * ((mu_rho - sigma_rho) + (mu_tau - sigma_tau))\n            \n            proxy_scores.append(score)\n            proxy_rhos.append(rhos)\n            proxy_taus.append(taus)\n\n        # Select the best proxy\n        p_star = np.argmax(proxy_scores)\n        \n        # Get the aggregated (mean) values for the selected proxy\n        mu_rho_star = np.mean(proxy_rhos[p_star])\n        mu_tau_star = np.mean(proxy_taus[p_star])\n\n        all_results.append([p_star, mu_rho_star, mu_tau_star])\n\n    # Format the final output string\n    formatted_results = []\n    for res in all_results:\n        p, rho, tau = res\n        rho_str = f\"{rho:.4f}\"\n        tau_str = f\"{tau:.4f}\"\n        formatted_results.append(f\"[{p},{rho_str},{tau_str}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3158046"}, {"introduction": "After understanding how to evaluate architectures, the question becomes how to search the vast space of possibilities efficiently. This exercise introduces you to the powerful paradigm of differentiable architecture search, which transforms the discrete search problem into a continuous optimization problem that can be solved with gradient descent. You will implement a key component of this approach, relaxing discrete architectural choices into continuous gates and using an annealing process to guide the search towards a final, discrete architecture [@problem_id:3158131]. This provides a practical look into one of the most influential and efficient search strategies in modern NAS.", "problem": "Consider Neural Architecture Search (NAS) where binary pruning masks are used to select a subset of architectural components. To enable gradient-based optimization, relax each binary mask variable from $m_\\ell \\in \\{0,1\\}$ to a continuous gate $m_\\ell \\in [0,1]$ via the temperature-controlled sigmoid parameterization $m_\\ell(\\mathbf{z}, \\tau) = \\sigma\\!\\left(\\frac{z_\\ell}{\\tau}\\right)$, where $\\sigma(u) = \\frac{1}{1 + e^{-u}}$, $\\mathbf{z} \\in \\mathbb{R}^L$ is the vector of logits, and $\\tau > 0$ is a temperature parameter. Consider the composite objective\n$$\nJ(\\mathbf{z};\\tau) \\;=\\; -\\sum_{\\ell=1}^L b_\\ell \\, m_\\ell(\\mathbf{z}, \\tau) \\;+\\; \\lambda \\sum_{\\ell=1}^L \\big| m_\\ell(\\mathbf{z}, \\tau) \\big|,\n$$\nwhere $b_\\ell \\ge 0$ models the utility of keeping component $\\ell$, and $\\lambda \\ge 0$ controls sparsity via an $\\ell_1$ penalty on the relaxed mask. This setup abstracts the trade-off between validation performance contribution and sparsity pressure, and is a well-used formulation in differentiable pruning and neural architecture search.\n\nStarting from the definitions of the sigmoid function and subgradients of the $\\ell_1$ norm, implement gradient descent on $\\mathbf{z}$ using the chain rule to compute $\\nabla_{\\mathbf{z}} J(\\mathbf{z};\\tau)$. Use an annealing schedule that reduces the temperature $\\tau$ multiplicatively until a minimum temperature is reached. At each temperature level, run a fixed number of gradient descent iterations on $\\mathbf{z}$ with a fixed learning rate. Initialize $\\mathbf{z}$ to the zero vector.\n\nDefine the distance of a relaxed mask to the nearest discrete value by $d(x) = \\min\\{\\,|x-0|,\\;|x-1|\\,\\}$ for $x \\in [0,1]$. For each test case, after completing the annealing schedule and gradient descent, compute the maximum distance to discreteness across coordinates, i.e., $\\max_{\\ell} d\\!\\left(m_\\ell(\\mathbf{z},\\tau_{\\text{final}})\\right)$, where $\\tau_{\\text{final}}$ is the last temperature used by your schedule. This single real number per test case will be the output to report.\n\nYour program must implement the following numerical procedure in a self-contained manner and produce results for all listed test cases:\n\n- The update uses gradient descent on $\\mathbf{z}$ at each annealing stage with temperature $\\tau$, for $T$ steps per stage, with learning rate $\\eta$.\n- The temperature schedule is $\\tau_0, \\tau_1, \\dots$ with $\\tau_{k+1} = \\gamma \\, \\tau_k$ until $\\tau_K \\le \\tau_{\\min}$, where $0 < \\gamma < 1$.\n- Use the subgradient of the $\\ell_1$ term as follows: for $m_\\ell > 0$, take $\\frac{\\partial}{\\partial m_\\ell} |m_\\ell| = 1$; at $m_\\ell = 0$, choose the subgradient $0$. This choice yields a piecewise-constant derivative with respect to $m_\\ell$ and is consistent with standard convex analysis.\n- To ensure numerical stability of $\\sigma\\!\\left(\\frac{z_\\ell}{\\tau}\\right)$, you may clamp logits $z_\\ell$ to a finite range when computing the sigmoid.\n\nTest Suite:\nProvide outputs for the following four test cases. In each case, the dimension is $L$, the utility vector is $\\mathbf{b}$, the sparsity coefficient is $\\lambda$, the initial logits are $\\mathbf{z}_0 = \\mathbf{0}$, the initial temperature is $\\tau_0$, the minimum temperature is $\\tau_{\\min}$, the multiplicative decay factor is $\\gamma$, the number of steps per temperature is $T$, and the learning rate is $\\eta$.\n\n- Case $1$ (happy path favoring discrete selections):\n  - $L = 5$\n  - $\\mathbf{b} = [\\,1.0,\\;0.3,\\;0.8,\\;0.49,\\;2.0\\,]$\n  - $\\lambda = 0.5$\n  - $\\tau_0 = 1.0$\n  - $\\tau_{\\min} = 10^{-3}$\n  - $\\gamma = 0.5$\n  - $T = 200$\n  - $\\eta = 0.5$\n\n- Case $2$ (boundary condition where $b_\\ell = \\lambda$ for all $\\ell$):\n  - $L = 3$\n  - $\\mathbf{b} = [\\,0.5,\\;0.5,\\;0.5\\,]$\n  - $\\lambda = 0.5$\n  - $\\tau_0 = 1.0$\n  - $\\tau_{\\min} = 10^{-3}$\n  - $\\gamma = 0.5$\n  - $T = 200$\n  - $\\eta = 0.5$\n\n- Case $3$ (no sparsity pressure):\n  - $L = 4$\n  - $\\mathbf{b} = [\\,0.2,\\;0.1,\\;0.9,\\;1.5\\,]$\n  - $\\lambda = 0$\n  - $\\tau_0 = 1.0$\n  - $\\tau_{\\min} = 10^{-3}$\n  - $\\gamma = 0.5$\n  - $T = 200$\n  - $\\eta = 0.5$\n\n- Case $4$ (strong sparsity with mixed utilities):\n  - $L = 3$\n  - $\\mathbf{b} = [\\,0.1,\\;3.0,\\;1.9\\,]$\n  - $\\lambda = 2.0$\n  - $\\tau_0 = 1.0$\n  - $\\tau_{\\min} = 10^{-3}$\n  - $\\gamma = 0.5$\n  - $T = 200$\n  - $\\eta = 0.5$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases, where each entry is the computed maximum distance to discreteness for that case (for example, a line like \"[x1,x2,x3,x4]\" where each $x_i$ is a float).", "solution": "The problem requires the implementation of a gradient-based optimization procedure for a relaxed architectural pruning problem in the context of Neural Architecture Search (NAS). We are asked to find the maximum distance to discreteness of the final relaxed masks after an annealing schedule. The problem is scientifically grounded, well-posed, and all necessary parameters are provided for a deterministic numerical simulation.\n\nThe core of the task is to perform gradient descent on the logits $\\mathbf{z}$ to minimize the objective function $J(\\mathbf{z};\\tau)$. The objective function is given by:\n$$\nJ(\\mathbf{z};\\tau) \\;=\\; -\\sum_{\\ell=1}^L b_\\ell \\, m_\\ell(\\mathbf{z}, \\tau) \\;+\\; \\lambda \\sum_{\\ell=1}^L \\big| m_\\ell(\\mathbf{z}, \\tau) \\big|\n$$\nwhere $m_\\ell(\\mathbf{z}, \\tau) = \\sigma(z_\\ell / \\tau)$ is the sigmoid-relaxed binary mask for component $\\ell$, with $\\sigma(u) = (1 + e^{-u})^{-1}$. The parameters $b_\\ell \\ge 0$ represent utility, and $\\lambda \\ge 0$ controls sparsity.\n\nTo perform gradient descent, we must compute the gradient of the objective function with respect to the logits, $\\nabla_{\\mathbf{z}} J(\\mathbf{z};\\tau)$. The objective function is a sum over the components $\\ell$, and the mask $m_\\ell$ depends only on the logit $z_\\ell$. Therefore, the gradient can be computed element-wise. The partial derivative with respect to a single logit $z_\\ell$ is found using the chain rule:\n$$\n\\frac{\\partial J}{\\partial z_\\ell} = \\frac{\\partial J}{\\partial m_\\ell} \\frac{\\partial m_\\ell}{\\partial z_\\ell}\n$$\n\nFirst, we compute the partial derivative of $J$ with respect to the mask $m_\\ell$:\n$$\n\\frac{\\partial J}{\\partial m_\\ell} = \\frac{\\partial}{\\partial m_\\ell} \\left( -b_\\ell m_\\ell + \\lambda |m_\\ell| \\right) = -b_\\ell + \\lambda \\frac{\\partial |m_\\ell|}{\\partial m_\\ell}\n$$\nThe problem specifies the subgradient for the absolute value term. Since the sigmoid function's range is $(0, 1)$, $m_\\ell$ is always strictly positive. Consequently, the derivative $\\frac{\\partial |m_\\ell|}{\\partial m_\\ell}$ is unambiguously $1$. This gives:\n$$\n\\frac{\\partial J}{\\partial m_\\ell} = -b_\\ell + \\lambda\n$$\n\nNext, we compute the partial derivative of the mask $m_\\ell$ with respect to the logit $z_\\ell$:\n$$\nm_\\ell(z_\\ell, \\tau) = \\sigma\\left(\\frac{z_\\ell}{\\tau}\\right)\n$$\nUsing the chain rule and the known derivative of the sigmoid function, $\\sigma'(u) = \\sigma(u)(1 - \\sigma(u))$, we have:\n$$\n\\frac{\\partial m_\\ell}{\\partial z_\\ell} = \\sigma'\\left(\\frac{z_\\ell}{\\tau}\\right) \\cdot \\frac{\\partial}{\\partial z_\\ell}\\left(\\frac{z_\\ell}{\\tau}\\right) = \\sigma\\left(\\frac{z_\\ell}{\\tau}\\right)\\left(1 - \\sigma\\left(\\frac{z_\\ell}{\\tau}\\right)\\right) \\cdot \\frac{1}{\\tau} = \\frac{m_\\ell(1 - m_\\ell)}{\\tau}\n$$\n\nCombining these results, the partial derivative of the objective with respect to the logit $z_\\ell$ is:\n$$\n\\frac{\\partial J}{\\partial z_\\ell} = (-b_\\ell + \\lambda) \\frac{m_\\ell(1 - m_\\ell)}{\\tau}\n$$\nThe full gradient vector $\\nabla_{\\mathbf{z}} J$ is composed of these partial derivatives for each $\\ell = 1, \\dots, L$.\n\nThe specified numerical procedure is as follows:\n1. Initialize the logits $\\mathbf{z} = \\mathbf{0}$ and the temperature $\\tau = \\tau_0$.\n2. Enter an annealing loop that continues as long as the current temperature $\\tau > \\tau_{\\min}$. Let $\\tau_{\\text{final}}$ be the last temperature value for which the loop's body is executed.\n3. Inside the annealing loop, for the current temperature $\\tau$, perform $T$ steps of gradient descent on $\\mathbf{z}$:\n$$\n\\mathbf{z} \\leftarrow \\mathbf{z} - \\eta \\nabla_{\\mathbf{z}} J(\\mathbf{z};\\tau)\n$$\nwhere $\\eta$ is the learning rate.\n4. After the inner loop of $T$ steps, update the temperature for the next stage: $\\tau \\leftarrow \\gamma \\tau$.\n5. After the annealing schedule is complete, using the final logit vector $\\mathbf{z}_{\\text{final}}$ and the last used temperature $\\tau_{\\text{final}}$, compute the final mask values:\n$$\nm_{\\ell, \\text{final}} = \\sigma\\left(\\frac{z_{\\ell, \\text{final}}}{\\tau_{\\text{final}}}\\right)\n$$\nDuring implementation, for numerical stability with small $\\tau$, the argument $z_\\ell/\\tau$ to the sigmoid function can become very large in magnitude. The `scipy.special.expit` function provides a numerically robust implementation of the sigmoid, preventing overflow/underflow issues, and is used in the solution.\n\nFinally, we calculate the distance to discreteness for each component, defined as $d(x) = \\min(|x-0|, |x-1|)$ for $x \\in [0,1]$. Since $m_{\\ell, \\text{final}} \\in (0,1)$, this simplifies to $d(m_{\\ell, \\text{final}}) = \\min(m_{\\ell, \\text{final}}, 1 - m_{\\ell, \\text{final}})$. The value to be reported for each test case is the maximum of these distances over all components $\\ell$:\n$$\n\\max_{\\ell} d(m_{\\ell, \\text{final}})\n$$\nThis procedure is implemented for each of the four test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path favoring discrete selections)\n        {\n            \"L\": 5,\n            \"b\": np.array([1.0, 0.3, 0.8, 0.49, 2.0]),\n            \"lambda\": 0.5,\n            \"tau0\": 1.0,\n            \"tau_min\": 1e-3,\n            \"gamma\": 0.5,\n            \"T\": 200,\n            \"eta\": 0.5,\n        },\n        # Case 2 (boundary condition where b_ell = lambda for all ell)\n        {\n            \"L\": 3,\n            \"b\": np.array([0.5, 0.5, 0.5]),\n            \"lambda\": 0.5,\n            \"tau0\": 1.0,\n            \"tau_min\": 1e-3,\n            \"gamma\": 0.5,\n            \"T\": 200,\n            \"eta\": 0.5,\n        },\n        # Case 3 (no sparsity pressure)\n        {\n            \"L\": 4,\n            \"b\": np.array([0.2, 0.1, 0.9, 1.5]),\n            \"lambda\": 0.0,\n            \"tau0\": 1.0,\n            \"tau_min\": 1e-3,\n            \"gamma\": 0.5,\n            \"T\": 200,\n            \"eta\": 0.5,\n        },\n        # Case 4 (strong sparsity with mixed utilities)\n        {\n            \"L\": 3,\n            \"b\": np.array([0.1, 3.0, 1.9]),\n            \"lambda\": 2.0,\n            \"tau0\": 1.0,\n            \"tau_min\": 1e-3,\n            \"gamma\": 0.5,\n            \"T\": 200,\n            \"eta\": 0.5,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(\n            L=case[\"L\"],\n            b=case[\"b\"],\n            lambda_val=case[\"lambda\"],\n            tau0=case[\"tau0\"],\n            tau_min=case[\"tau_min\"],\n            gamma=case[\"gamma\"],\n            T=case[\"T\"],\n            eta=case[\"eta\"],\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(L, b, lambda_val, tau0, tau_min, gamma, T, eta):\n    \"\"\"\n    Runs the annealing and gradient descent procedure for a single test case.\n\n    Args:\n        L (int): Dimension of the logit vector.\n        b (np.ndarray): Utility vector.\n        lambda_val (float): Sparsity coefficient.\n        tau0 (float): Initial temperature.\n        tau_min (float): Minimum temperature for the schedule.\n        gamma (float): Multiplicative decay factor for temperature.\n        T (int): Number of gradient descent steps per temperature.\n        eta (float): Learning rate for gradient descent.\n\n    Returns:\n        float: The maximum distance to discreteness.\n    \"\"\"\n    z = np.zeros(L)\n    tau = tau0\n    tau_final = tau\n\n    # Annealing loop\n    while tau > tau_min:\n        tau_final = tau\n        # Gradient descent loop for the current temperature\n        for _ in range(T):\n            # Calculate relaxed masks m using a numerically stable sigmoid\n            # m_ell = sigma(z_ell / tau)\n            m = expit(z / tau)\n\n            # Calculate gradient of J w.r.t z\n            # dJ/dm_ell = -b_ell + lambda\n            dJ_dm = -b + lambda_val\n\n            # dm/dz_ell = m_ell * (1 - m_ell) / tau\n            dm_dz = (m * (1 - m)) / tau\n\n            # Gradient via chain rule: dJ/dz = dJ/dm * dm/dz\n            grad_z = dJ_dm * dm_dz\n\n            # Update logits using gradient descent\n            z = z - eta * grad_z\n        \n        # Anneal the temperature\n        tau *= gamma\n    \n    # After all optimization, calculate the final mask with the final z and tau_final\n    final_m = expit(z / tau_final)\n\n    # Calculate the distance to the nearest discrete value (0 or 1) for each component\n    # d(x) = min(|x-0|, |x-1|) which is min(x, 1-x) for x in [0,1]\n    distances = np.minimum(final_m, 1 - final_m)\n\n    # Return the maximum distance across all components\n    return np.max(distances)\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3158131"}]}