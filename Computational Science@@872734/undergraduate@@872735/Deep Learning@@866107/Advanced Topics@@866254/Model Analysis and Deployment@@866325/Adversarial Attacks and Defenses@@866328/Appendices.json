{"hands_on_practices": [{"introduction": "Many successful adversarial attacks, such as the Fast Gradient Sign Method (FGSM), operate by adding high-frequency noise-like patterns to an input. This observation suggests a defense strategy rooted in classic signal processing: simply filter these high frequencies out. In this exercise [@problem_id:3098464], you will implement an ideal low-pass filter as a preprocessing defense, allowing you to empirically measure and analyze the fundamental trade-off between enhancing adversarial robustness and maintaining accuracy on clean, unperturbed data.", "problem": "You are to implement and evaluate a simple preprocessing defense based on ideal low-pass filtering against a first-order adversarial attack for a linear classifier on synthetic one-dimensional signals. The task is to derive the required components from foundational definitions, implement them, and quantify the trade-off between clean accuracy and robust accuracy as the low-pass cutoff frequency varies.\n\nConsider the following setting.\n\n- Data generation: Each input is a one-dimensional discrete signal of length $N$, denoted by $\\mathbf{x} \\in \\mathbb{R}^{N}$. Let labels be $y \\in \\{-1,+1\\}$. For each sample, the signal is generated as\n  $$\\mathbf{x} = y \\left(A_{\\text{low}} \\,\\mathbf{s}_{\\text{low}} + A_{\\text{high}} \\,\\mathbf{s}_{\\text{high}}\\right) + \\boldsymbol{\\epsilon},$$\n  where $\\mathbf{s}_{\\text{low}}$ and $\\mathbf{s}_{\\text{high}}$ are fixed sinusoidal basis vectors of angular frequencies $2\\pi k_{\\text{low}}/N$ and $2\\pi k_{\\text{high}}/N$ respectively, and $\\boldsymbol{\\epsilon}$ is independent and identically distributed Gaussian noise with zero mean and standard deviation $\\sigma$. The parameters $A_{\\text{low}}$, $A_{\\text{high}}$, $k_{\\text{low}}$, $k_{\\text{high}}$, $N$, $\\sigma$, and the number of samples $M$ are specified in the test suite below.\n\n- Classifier: Use a logistic regression model with a fixed weight vector $\\mathbf{w} \\in \\mathbb{R}^{N}$ and bias $b \\in \\mathbb{R}$. The classifier computes the logit $z = \\mathbf{w}^{\\top}\\mathbf{x} + b$ and predicts $\\hat{y} = \\mathrm{sign}(z)$. The loss for an example $(\\mathbf{x}, y)$ is the logistic loss\n  $$\\mathcal{L}(\\mathbf{x}, y) = \\log\\big(1 + \\exp(-y\\,(\\mathbf{w}^{\\top}\\mathbf{x} + b))\\big).$$\n  The weight vector is constructed to emphasize the high-frequency component, as described in the test suite.\n\n- Adversary and attack: Use the Fast Gradient Sign Method (FGSM, Fast Gradient Sign Method) under an $\\ell_{\\infty}$ constraint. The adversarial example is defined by\n  $$\\mathbf{x}_{\\text{adv}} = \\mathbf{x} + \\varepsilon \\,\\mathrm{sign}\\left(\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, y)\\right),$$\n  where $\\varepsilon$ is the perturbation budget. You must derive $\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, y)$ from the above loss using first principles (the chain rule and the derivative of the logistic function) and use that to implement FGSM.\n\n- Defense: Define an ideal low-pass filter as follows. Given a real signal $\\mathbf{x}$ of length $N$ and a cutoff angular frequency $\\omega_{c} \\in [0,\\pi]$ measured in radians per sample, compute its Discrete Fourier Transform (DFT, Discrete Fourier Transform) coefficients $\\{X_{k}\\}$, where the angular frequency associated with bin $k$ is $\\omega_{k} = 2\\pi k/N$. Zero out all frequency components with $\\omega_{k}  \\omega_{c}$ and invert the DFT to obtain the filtered signal. In practice for real signals, you may use the one-sided real DFT and map indices to angular frequencies accordingly. The low-pass defense preprocesses inputs by applying this ideal filter.\n\n- Metrics:\n  1. Clean accuracy for a cutoff $\\omega_{c}$: the fraction of correctly classified clean examples after preprocessing the inputs with the low-pass filter at $\\omega_{c}$.\n  2. Robust accuracy for a cutoff $\\omega_{c}$: the fraction of correctly classified adversarial examples, where each adversarial example is generated from the unfiltered clean input using FGSM and then passed through the low-pass filter at $\\omega_{c}$ before classification.\n\n- Quantities to report for each cutoff $\\omega_{c}$:\n  1. Clean accuracy drop: the difference between the clean accuracy with no filtering (baseline at $\\omega_{c}=\\pi$) and the clean accuracy at the given $\\omega_{c}$.\n  2. Robust accuracy gain: the difference between the robust accuracy at the given $\\omega_{c}$ and the robust accuracy with no filtering (baseline at $\\omega_{c}=\\pi$).\n\nYour program must:\n\n- Generate the dataset, construct the classifier, implement FGSM by deriving and using $\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, y)$, implement the ideal low-pass filter in the frequency domain, and compute the requested metrics.\n- Use the following test suite (angles are in radians per sample, dataset sizes are unitless):\n  - Signal length: $N = 64$.\n  - Number of samples: $M = 400$ with a balanced label set $y \\in \\{-1, +1\\}$.\n  - Frequencies: $k_{\\text{low}} = 2$, $k_{\\text{high}} = 16$.\n  - Amplitudes: $A_{\\text{low}} = 0.8$, $A_{\\text{high}} = 0.7$.\n  - Noise standard deviation: $\\sigma = 0.2$.\n  - Classifier construction: $\\mathbf{w} = \\alpha \\,\\mathbf{s}_{\\text{high}} + \\beta \\,\\mathbf{s}_{\\text{low}}$, then normalized to unit $\\ell_{2}$ norm, with $\\alpha = 1.0$, $\\beta = 0.15$, and $b=0$.\n  - FGSM budget: $\\varepsilon = 0.5$.\n  - Cutoff set (including the baseline): $\\{\\omega_{c}\\} = \\{\\pi, 0.8\\pi, 0.5\\pi, 0.2\\pi\\}$.\n- Round each reported value (clean accuracy drop and robust accuracy gain) to $4$ decimal places.\n\nFinal output format: Your program should produce a single line of output containing the results as a list of two-element lists, one per cutoff in the specified order, where each inner list is $[\\text{clean\\_drop}, \\text{robust\\_gain}]$. For example,\n\"[ [cd_1,rg_1],[cd_2,rg_2],[cd_3,rg_3],[cd_4,rg_4] ]\" but without spaces after commas. The actual numeric values must be decimals (not percentages). Only this single line must be printed.\n\nAngle unit requirement: All angular frequencies must be interpreted in radians per sample. There are no physical units in this problem.", "solution": "The problem is valid as it is scientifically grounded in digital signal processing and machine learning, well-posed with all necessary parameters defined, and objective in its formulation. The task is to implement a simulation to evaluate an ideal low-pass filter as a defense against the Fast Gradient Sign Method (FGSM) attack.\n\nThe solution proceeds in several steps: derivation of the necessary gradient for the attack, definition of the data generation and classification models, implementation of the low-pass filter defense, and execution of the simulation to compute the required performance metrics.\n\n**1. Data Generation and Classifier Model**\n\nThe input signals $\\mathbf{x} \\in \\mathbb{R}^{N}$ are generated based on a label $y \\in \\{-1, +1\\}$. Each signal is a sum of two sinusoids, $\\mathbf{s}_{\\text{low}}$ and $\\mathbf{s}_{\\text{high}}$, scaled by the label, with added Gaussian noise $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2 \\mathbf{I})$:\n$$\n\\mathbf{x} = y \\left(A_{\\text{low}} \\,\\mathbf{s}_{\\text{low}} + A_{\\text{high}} \\,\\mathbf{s}_{\\text{high}}\\right) + \\boldsymbol{\\epsilon}\n$$\nThe basis vectors are defined as $\\mathbf{s}_{\\text{mode}}[n] = \\cos(2\\pi k_{\\text{mode}} n / N)$ for time index $n \\in \\{0, 1, \\dots, N-1\\}$.\n\nThe classifier is a linear model followed by a sign function, $\\hat{y} = \\mathrm{sign}(\\mathbf{w}^{\\top}\\mathbf{x} + b)$. The weight vector $\\mathbf{w}$ is constructed to be a linear combination of the high- and low-frequency basis vectors, $\\mathbf{w} \\propto \\alpha \\mathbf{s}_{\\text{high}} + \\beta \\mathbf{s}_{\\text{low}}$, and is normalized to unit $\\ell_2$ norm. The bias is $b=0$. Given the parameters $\\alpha=1.0$ and $\\beta=0.15$, the classifier is primarily sensitive to the high-frequency component $\\mathbf{s}_{\\text{high}}$.\n\n**2. Adversarial Attack: Fast Gradient Sign Method (FGSM)**\n\nThe FGSM attack crafts an adversarial example $\\mathbf{x}_{\\text{adv}}$ by adding a small perturbation to the original input $\\mathbf{x}$ in the direction that maximizes the loss:\n$$\n\\mathbf{x}_{\\text{adv}} = \\mathbf{x} + \\varepsilon \\,\\mathrm{sign}\\left(\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, y)\\right)\n$$\nwhere $\\mathcal{L}$ is the logistic loss and $\\varepsilon$ is the perturbation budget. To implement this, we must first derive the gradient of the loss with respect to the input, $\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, y)$.\n\nThe logistic loss is $\\mathcal{L}(\\mathbf{x}, y) = \\log\\big(1 + \\exp(-y\\,z)\\big)$, where the logit is $z = \\mathbf{w}^{\\top}\\mathbf{x} + b$.\nUsing the chain rule, $\\nabla_{\\mathbf{x}}\\mathcal{L} = \\frac{\\partial\\mathcal{L}}{\\partial z} \\frac{\\partial z}{\\partial \\mathbf{x}}$.\n\nThe derivative of the logit with respect to the input vector $\\mathbf{x}$ is:\n$$\n\\frac{\\partial z}{\\partial \\mathbf{x}} = \\nabla_{\\mathbf{x}}(\\mathbf{w}^{\\top}\\mathbf{x} + b) = \\mathbf{w}\n$$\nThe derivative of the loss with respect to the logit $z$ is:\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial z} = \\frac{1}{1 + \\exp(-yz)} \\cdot \\frac{\\partial}{\\partial z}\\left(1 + \\exp(-yz)\\right) = \\frac{1}{1 + \\exp(-yz)} \\cdot \\left(-\\exp(-yz) \\cdot y\\right) = \\frac{-y \\exp(-yz)}{1 + \\exp(-yz)}\n$$\nMultiplying the numerator and denominator by $\\exp(yz)$ simplifies this to:\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial z} = \\frac{-y}{ \\exp(yz) + 1}\n$$\nCombining these results, the gradient is:\n$$\n\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, y) = \\left( \\frac{-y}{\\exp(y(\\mathbf{w}^{\\top}\\mathbf{x} + b)) + 1} \\right) \\mathbf{w}\n$$\nFor FGSM, we only need the sign of this gradient. The term in the parenthesis is a scalar. Its denominator, $\\exp(\\cdot) + 1$, is always positive. Therefore, its sign is determined by the numerator, $-y$.\n$$\n\\mathrm{sign}\\left(\\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, y)\\right) = \\mathrm{sign}\\left( \\left( \\frac{-y}{\\exp(\\dots)+1} \\right) \\mathbf{w} \\right) = \\mathrm{sign}(-y) \\cdot \\mathrm{sign}(\\mathbf{w})\n$$\nSince $y \\in \\{-1, +1\\}$, we have $\\mathrm{sign}(-y) = -y$. The perturbation is thus simplified to:\n$$\n\\varepsilon \\cdot \\mathrm{sign}\\left(\\nabla_{\\mathbf{x}}\\mathcal{L}\\right) = - \\varepsilon y \\cdot \\mathrm{sign}(\\mathbf{w})\n$$\nAnd the adversarial example is:\n$$\n\\mathbf{x}_{\\text{adv}} = \\mathbf{x} - \\varepsilon y \\,\\mathrm{sign}(\\mathbf{w})\n$$\nThis simplified form is computationally stable and will be used to generate the adversarial examples. The attack perturbs the input in the direction of $-\\mathrm{sign}(\\mathbf{w})$ for $y=+1$ and $+\\mathrm{sign}(\\mathbf{w})$ for $y=-1$, which maximally reduces the classifier's logit $z=\\mathbf{w}^\\top\\mathbf{x}$.\n\n**3. Defense Mechanism: Ideal Low-Pass Filter**\n\nThe defense preprocesses any input by applying an ideal low-pass filter. This is implemented in the frequency domain using the Discrete Fourier Transform (DFT). For a real-valued signal $\\mathbf{x}$ of length $N$, we use the one-sided Real DFT (`numpy.fft.rfft`), which yields $N/2+1$ complex coefficients $X_k$ for $k \\in \\{0, 1, \\dots, N/2\\}$. The angular frequency corresponding to coefficient $X_k$ is $\\omega_k = 2\\pi k / N$.\n\nGiven a cutoff angular frequency $\\omega_c$, the filter zeroes out all coefficients $X_k$ for which $\\omega_k  \\omega_c$. This condition is equivalent to $k  \\omega_c N / (2\\pi)$. After zeroing out the high-frequency components, the filtered signal is recovered by applying the inverse Real DFT (`numpy.fft.irfft`).\n\n**4. Computational Procedure and Evaluation**\n\nThe simulation is executed as follows:\n1.  **Setup**: The parameters ($N, M, k_{\\text{low}}, k_{\\text{high}}$, etc.) are defined. The basis vectors $\\mathbf{s}_{\\text{low}}, \\mathbf{s}_{\\text{high}}$ and the classifier weight vector $\\mathbf{w}$ are constructed.\n2.  **Data Generation**: A dataset of $M=400$ samples $(\\mathbf{X}, \\mathbf{y})$ is created according to the specified model, with balanced labels.\n3.  **Attack Generation**: For each clean sample $(\\mathbf{x}_i, y_i)$, the corresponding adversarial example $\\mathbf{x}_{\\text{adv}, i}$ is generated using the derived FGSM formula.\n4.  **Evaluation Loop**: The program iterates through the specified set of cutoff frequencies $\\{\\omega_c\\} = \\{\\pi, 0.8\\pi, 0.5\\pi, 0.2\\pi\\}$. For each $\\omega_c$:\n    a. Clean inputs $\\mathbf{X}$ are passed through the low-pass filter. The accuracy of the classifier on these filtered inputs is calculated (clean accuracy).\n    b. Adversarial inputs $\\mathbf{X}_{\\text{adv}}$ are passed through the same low-pass filter. The accuracy on these inputs is calculated (robust accuracy).\n5.  **Metric Calculation**: The baseline accuracies are those at $\\omega_c = \\pi$, which corresponds to no filtering as it is the Nyquist frequency. For each $\\omega_c$, the \"clean accuracy drop\" and \"robust accuracy gain\" are computed relative to this baseline.\n\nThis comprehensive procedure allows for a quantitative assessment of the trade-off introduced by the low-pass filtering defense: its potential to degrade performance on clean data versus its ability to improve robustness to adversarial attacks.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a low-pass filtering defense against an FGSM attack\n    on a linear classifier for 1D signals.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    N = 64\n    M = 400\n    k_low = 2\n    k_high = 16\n    A_low = 0.8\n    A_high = 0.7\n    sigma = 0.2\n    alpha = 1.0\n    beta = 0.15\n    b = 0.0\n    epsilon = 0.5\n    # Cutoff frequencies in radians per sample, in the specified order.\n    omega_cs = [math.pi, 0.8 * math.pi, 0.5 * math.pi, 0.2 * math.pi]\n\n    # Use a fixed random seed for reproducibility of the results.\n    np.random.seed(42)\n\n    # --- 1. Model and Data Component Construction ---\n    # Generate sinusoidal basis vectors.\n    n = np.arange(N)\n    s_low = np.cos(2 * np.pi * k_low * n / N)\n    s_high = np.cos(2 * np.pi * k_high * n / N)\n    \n    # Construct the classifier's weight vector.\n    w_unnormalized = alpha * s_high + beta * s_low\n    w = w_unnormalized / np.linalg.norm(w_unnormalized)\n\n    # --- 2. Dataset Generation ---\n    # Create a balanced set of labels.\n    num_pos = M // 2\n    num_neg = M - num_pos\n    y = np.array([1] * num_pos + [-1] * num_neg)\n    \n    # Create the base signal component (without noise).\n    signal_base = A_low * s_low + A_high * s_high\n    # Scale base signal by label y for each sample.\n    X_signal = np.outer(y, signal_base)\n    \n    # Add independent and identically distributed Gaussian noise.\n    noise = np.random.normal(0, sigma, (M, N))\n    X = X_signal + noise\n\n    # --- 3. Adversarial Attack Generation ---\n    # Use the simplified FGSM formula: x_adv = x - epsilon * y * sign(w)\n    y_reshaped = y[:, np.newaxis]  # For broadcasting\n    sign_w = np.sign(w)\n    \n    perturbation = -epsilon * y_reshaped * sign_w\n    X_adv = X + perturbation\n    \n    # --- 4. Defense and Evaluation ---\n    def ideal_low_pass_filter(signals, wc, N_len):\n        \"\"\"Applies an ideal low-pass filter to a batch of signals.\"\"\"\n        # If cutoff is at or above Nyquist, no filtering is needed.\n        if wc = math.pi:\n            return signals\n\n        # Compute the one-sided real DFT of the signal batch.\n        fft_coeffs = np.fft.rfft(signals, axis=1)\n        \n        # Determine the cutoff index for frequency components.\n        # Angular frequency of bin k is omega_k = 2*pi*k/N.\n        # We zero out where k  wc*N/(2*pi).\n        k_cutoff_float = wc * N_len / (2 * np.pi)\n        \n        num_coeffs = fft_coeffs.shape[1]\n        freq_indices = np.arange(num_coeffs)\n        \n        # Apply the filter by zeroing out coefficients above the cutoff.\n        fft_coeffs[:, freq_indices  k_cutoff_float] = 0\n        \n        # Invert the DFT to get the filtered signals.\n        filtered_signals = np.fft.irfft(fft_coeffs, n=N_len, axis=1)\n        return filtered_signals\n\n    all_accuracies = {}\n    for wc in omega_cs:\n        # Evaluate performance on clean data with filtering.\n        X_filtered = ideal_low_pass_filter(X, wc, N)\n        logits_clean = (X_filtered @ w) + b\n        preds_clean = np.sign(logits_clean)\n        acc_clean = np.mean(preds_clean == y)\n        \n        # Evaluate performance on adversarial data with filtering.\n        X_adv_filtered = ideal_low_pass_filter(X_adv, wc, N)\n        logits_robust = (X_adv_filtered @ w) + b\n        preds_robust = np.sign(logits_robust)\n        acc_robust = np.mean(preds_robust == y)\n        \n        all_accuracies[wc] = {'clean': acc_clean, 'robust': acc_robust}\n\n    # --- 5. Metric Calculation and Final Output ---\n    # Get baseline accuracies (no filtering, wc = pi).\n    baseline_wc = math.pi\n    baseline_acc_clean = all_accuracies[baseline_wc]['clean']\n    baseline_acc_robust = all_accuracies[baseline_wc]['robust']\n    \n    results = []\n    for wc in omega_cs:\n        current_acc_clean = all_accuracies[wc]['clean']\n        current_acc_robust = all_accuracies[wc]['robust']\n        \n        clean_drop = baseline_acc_clean - current_acc_clean\n        robust_gain = current_acc_robust - baseline_acc_robust\n        \n        # Round to 4 decimal places as required.\n        clean_drop = round(clean_drop, 4)\n        robust_gain = round(robust_gain, 4)\n\n        # Ensure -0.0 is formatted as 0.0 for clean output.\n        if clean_drop == -0.0: clean_drop = 0.0\n        if robust_gain == -0.0: robust_gain = 0.0\n            \n        results.append([clean_drop, robust_gain])\n        \n    # Format the final list of lists into the required string format.\n    formatted_results = [f\"[{r[0]},{r[1]}]\" for r in results]\n    final_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_string)\n\nsolve()\n```", "id": "3098464"}, {"introduction": "When evaluating a defense, it is not enough to show that it foils an attack; we must ensure it provides genuine robustness. Some methods achieve an illusion of security by simply obscuring or shattering the loss gradients that white-box attackers rely on, a failure mode known as gradient masking. This practice [@problem_id:3097091] provides a hands-on demonstration of how to detect this phenomenon by testing the transferability of attacks from a surrogate model, a crucial technique for the rigorous evaluation of any proposed defense.", "problem": "Consider a binary classification setting with input vectors $\\mathbf{x} \\in \\mathbb{R}^d$ and labels $y \\in \\{0,1\\}$. The learning process follows empirical risk minimization, where a model $f_\\theta$ with parameters $\\theta$ minimizes an empirical loss over training data. The standard base loss for binary classification is the logistic loss, defined by probabilities $p_\\theta(\\mathbf{x}) \\in (0,1)$ and the cross-entropy $L(\\mathbf{x},y;\\theta) = -y \\log p_\\theta(\\mathbf{x}) - (1-y) \\log(1 - p_\\theta(\\mathbf{x}))$. An adversarial example for a given input-label pair $(\\mathbf{x},y)$ and a norm budget $\\epsilon  0$ is any perturbation $\\delta$ satisfying $\\|\\delta\\|_\\infty \\le \\epsilon$ that increases the loss $L(\\mathbf{x} + \\delta, y; \\theta)$. The adversarial crafting problem can be posed as constrained maximization: find $\\delta^\\star$ to maximize $L(\\mathbf{x} + \\delta, y; \\theta)$ over the set $\\{\\delta : \\|\\delta\\|_\\infty \\le \\epsilon\\}$. Transferability is the property that adversarial examples crafted to increase the loss of one model $f_\\theta$ also cause errors in a different model $g_\\phi$ trained on the same data distribution.\n\nGradient masking is a failure mode where gradients used for optimization or attack generation are rendered uninformative (for example by non-differentiable pre-processing or saturating functions), reducing white-box attack effectiveness without providing true robustness. A principle-based detection method is to compare white-box attack success on a candidate masked model against transfer attack success from a surrogate model whose gradients are informative. If adversarial examples crafted on a surrogate $f_\\theta$ substantially increase error in $g_\\phi$ while white-box attacks crafted on $g_\\phi$ itself have low effectiveness, this indicates gradient masking in $g_\\phi$.\n\nYour task is to write a complete program implementing the following experiment from first principles:\n\n- Construct two models on the same synthetic data:\n  1. A standard logistic regression classifier $f_\\theta$ that maps $\\mathbf{x} \\mapsto p_\\theta(\\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)$, where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, with parameters $\\theta = (\\mathbf{w}, b)$ trained by gradient descent to minimize the empirical cross-entropy.\n  2. A second classifier $g_\\phi$ trained on the same data, with three possible instantiations depending on the test case:\n     - A model with non-differentiable input pre-processing $h(\\mathbf{x})$ (e.g., elementwise sign) followed by logistic regression on the transformed inputs, producing $p_\\phi(\\mathbf{x}) = \\sigma(\\mathbf{u}^\\top h(\\mathbf{x}) + c)$, with parameters $\\phi = (\\mathbf{u}, c)$ trained by gradient descent with respect to $(\\mathbf{u}, c)$ only. The gradient of the loss with respect to the input $\\mathbf{x}$ must be treated as zero for attack generation on $g_\\phi$.\n     - A standard logistic regression trained with adversarial training, where in each training epoch, inputs are replaced by adversarially perturbed inputs within an $\\ell_\\infty$ budget to minimize worst-case empirical loss.\n     - A standard logistic regression trained normally (no masking and no adversarial training).\n\n- Implement an $\\ell_\\infty$-constrained projected gradient ascent attack to craft adversarial examples, using the gradient of the loss with respect to $\\mathbf{x}$. The optimization should proceed by iteratively ascending along the sign of the gradient and projecting back to the $\\ell_\\infty$ ball of radius $\\epsilon$ around the original input. The attack must be implemented for arbitrary $\\epsilon  0$, step size $\\alpha  0$, and a fixed number of steps $T \\in \\mathbb{N}$.\n\n- For each test case, compute two empirical success rates:\n  1. The white-box attack success rate on $g_\\phi$: the fraction of inputs for which $g_\\phi$'s predicted label changes when attacked using $g_\\phi$'s own (possibly masked) gradients.\n  2. The transfer attack success rate from $f_\\theta$ to $g_\\phi$: the fraction of inputs for which $g_\\phi$'s predicted label changes when adversarial examples are crafted using $f_\\theta$'s gradients and then applied to $g_\\phi$.\n\n- Declare gradient masking detected for a test case if and only if the following two conditions both hold: the white-box success rate on $g_\\phi$ is less than a threshold $\\tau \\in (0,1)$ and the transfer success rate from $f_\\theta$ to $g_\\phi$ exceeds the white-box success rate on $g_\\phi$ by at least a margin $\\Delta \\in (0,1)$.\n\nData generation must be fully synthetic, statistically sound, and reproducible. Use a two-dimensional input space $d = 2$ with class-conditional Gaussian distributions: generate $n$ independent samples for each class, where class $y = 1$ has mean $\\boldsymbol{\\mu}_1 \\in \\mathbb{R}^2$ and diagonal covariance $\\operatorname{diag}(\\sigma^2, \\sigma^2)$, and class $y = 0$ has mean $\\boldsymbol{\\mu}_0 \\in \\mathbb{R}^2$ and the same covariance. Draw inputs, clip them to the range $[-1, 1]^2$, and use labels $y \\in \\{0,1\\}$ accordingly. Fix all random seeds inside your program to ensure determinism.\n\nYour program must implement the above and evaluate the following test suite of parameter values:\n\n- Test case $1$ (masked $g_\\phi$):\n  - Data: $n = 200$ per class, $\\boldsymbol{\\mu}_1 = (0.3, 0.3)$, $\\boldsymbol{\\mu}_0 = (-0.3, -0.3)$, $\\sigma = 0.6$.\n  - Attack: $\\epsilon = 1.0$, $\\alpha = 0.2$, $T = 10$.\n  - Model $f_\\theta$: standard logistic regression trained normally.\n  - Model $g_\\phi$: input pre-processing $h(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{x})$ (elementwise), logistic regression on $h(\\mathbf{x})$; gradient with respect to $\\mathbf{x}$ set to $0$ for attacks on $g_\\phi$.\n  - Detection parameters: $\\tau = 0.1$, $\\Delta = 0.2$.\n\n- Test case $2$ (adversarially trained $g_\\phi$):\n  - Data: $n = 200$ per class, $\\boldsymbol{\\mu}_1 = (0.7, 0.7)$, $\\boldsymbol{\\mu}_0 = (-0.7, -0.7)$, $\\sigma = 0.3$.\n  - Attack: $\\epsilon = 0.3$, $\\alpha = 0.05$, $T = 40$.\n  - Model $f_\\theta$: standard logistic regression trained normally.\n  - Model $g_\\phi$: standard logistic regression trained with adversarial training using the same $\\epsilon$ and $\\alpha$ during training for $50$ epochs.\n  - Detection parameters: $\\tau = 0.2$, $\\Delta = 0.2$.\n\n- Test case $3$ (both models normal):\n  - Data: $n = 200$ per class, $\\boldsymbol{\\mu}_1 = (0.2, 0.2)$, $\\boldsymbol{\\mu}_0 = (-0.2, -0.2)$, $\\sigma = 0.5$.\n  - Attack: $\\epsilon = 0.3$, $\\alpha = 0.05$, $T = 20$.\n  - Model $f_\\theta$: standard logistic regression trained normally.\n  - Model $g_\\phi$: standard logistic regression trained normally.\n  - Detection parameters: $\\tau = 0.2$, $\\Delta = 0.2$.\n\nRequired outputs:\n\n- For each test case $i \\in \\{1,2,3\\}$, compute a boolean indicating whether gradient masking is detected in $g_\\phi$ under the stated detection rule.\n- Your program should produce a single line of output containing the three booleans as a comma-separated list enclosed in square brackets, in order of the test cases, for example $[{\\rm True},{\\rm False},{\\rm True}]$.\n\nAll calculations are unitless. Angles are not used. Percentages must not be printed; only booleans are required. Your program must be self-contained, require no user input, and follow the specified output format exactly.", "solution": "The problem requires an implementation of an experiment to detect gradient masking, a failure mode in evaluating the robustness of machine learning models. The solution involves constructing, training, and attacking two binary classifiers, $f_\\theta$ and $g_\\phi$, and evaluating a specific detection criterion.\n\n### 1. Theoretical Framework\n\n**Binary Classification and Logistic Loss:**\nThe setting is a binary classification task with input vectors $\\mathbf{x} \\in \\mathbb{R}^d$ and labels $y \\in \\{0,1\\}$. We use a logistic regression model, which calculates the probability of the positive class ($y=1$) as $p_\\theta(\\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)$, where $\\sigma(z) = (1 + e^{-z})^{-1}$ is the sigmoid function and $\\theta = (\\mathbf{w}, b)$ are the model parameters. The model is trained by minimizing the empirical average of the binary cross-entropy (logistic) loss over a training dataset:\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N L(\\mathbf{x}_i, y_i; \\theta) = \\frac{1}{N} \\sum_{i=1}^N \\left[ -y_i \\log p_\\theta(\\mathbf{x}_i) - (1-y_i) \\log(1 - p_\\theta(\\mathbf{x}_i)) \\right]\n$$\nThis minimization is performed using gradient descent, where parameters are updated iteratively in the opposite direction of the loss gradient:\n$$\n\\nabla_\\mathbf{w} \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N (p_\\theta(\\mathbf{x}_i) - y_i)\\mathbf{x}_i \\quad , \\quad \\nabla_b \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N (p_\\theta(\\mathbf{x}_i) - y_i)\n$$\n\n**Adversarial Attacks and Gradient Masking:**\nAn adversarial example is a slightly perturbed input $\\mathbf{x}' = \\mathbf{x} + \\delta$ designed to fool a model. The perturbation $\\delta$ is constrained, typically within an $\\ell_\\infty$-norm ball: $\\|\\delta\\|_\\infty \\le \\epsilon$. The most effective perturbation within this constraint is found by maximizing the model's loss. A common method for this is Projected Gradient Ascent (PGA), which iteratively updates the input in the direction of the loss gradient with respect to the input:\n$$\n\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\alpha \\cdot \\operatorname{sign}(\\nabla_{\\mathbf{x}^{(t)}} L(\\mathbf{x}^{(t)}, y; \\theta))\n$$\nwhere $\\alpha$ is the step size. The gradient of the loss with respect to a single input $\\mathbf{x}$ is given by:\n$$\n\\nabla_\\mathbf{x} L = (p_\\theta(\\mathbf{x}) - y)\\mathbf{w}\n$$\nAfter each step, the total perturbation is projected (clipped) to satisfy $\\|\\mathbf{x}^{(t+1)} - \\mathbf{x}^{(0)}\\|_\\infty \\le \\epsilon$, and the resulting adversarial example is clipped to the valid input domain (e.g., $[-1, 1]^d$).\n\nGradient masking occurs when a model appears robust to such white-box attacks (where the attacker has full access to the model) because its loss surface is \"flat\" or its gradients are otherwise uninformative. This provides a false sense of security. A key indicator of gradient masking is high transferability: adversarial examples created for a different, non-masked model $f_\\theta$ are still effective against the candidate model $g_\\phi$, even when $g_\\phi$ seems robust to attacks based on its own gradients.\n\n### 2. Experimental Design\n\n**Data Generation:**\nWe generate a synthetic dataset in $\\mathbb{R}^2$ from two class-conditional Gaussian distributions. For a number of samples $n$ per class, class $y=1$ is drawn from $\\mathcal{N}(\\boldsymbol{\\mu}_1, \\sigma^2\\mathbf{I})$ and class $y=0$ from $\\mathcal{N}(\\boldsymbol{\\mu}_0, \\sigma^2\\mathbf{I})$. All generated points are clipped to the hypercube $[-1, 1]^2$. All random processes are seeded for reproducibility.\n\n**Model Configurations:**\n- **Surrogate Model $f_\\theta$**: A standard logistic regression classifier trained via gradient descent on the generated data. This model serves as the source for transfer attacks.\n\n- **Target Model $g_\\phi$**: This model takes one of three forms depending on the test case:\n    1.  **Masked Model**: Employs a non-differentiable pre-processing step $h(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{x})$. The model is $p_\\phi(\\mathbf{x}) = \\sigma(\\mathbf{u}^\\top h(\\mathbf{x}) + c)$. It is trained normally via gradient descent with respect to its parameters $\\phi = (\\mathbf{u}, c)$ on the transformed data. For the purpose of crafting a white-box attack on this model, the problem specifies that the gradient of the loss with respect to the input $\\mathbf{x}$ is treated as zero ($\\nabla_\\mathbf{x} L = \\mathbf{0}$). This directly simulates gradient masking.\n    2.  **Adversarially Trained Model**: A standard logistic regression model trained to achieve true robustness. The training objective is an empirical worst-case loss minimization:\n        $$\n        \\min_{\\phi} \\frac{1}{N} \\sum_{i=1}^N \\max_{\\|\\delta_i\\|_\\infty \\le \\epsilon} L(\\mathbf{x}_i + \\delta_i, y_i; \\phi)\n        $$\n        The inner maximization is approximated using PGA for a fixed number of steps at each epoch of training.\n    3.  **Normal Model**: A standard logistic regression classifier identical in architecture and training to $f_\\theta$.\n\n**Evaluation and Detection Logic:**\nFor each test case, we compute two metrics:\n1.  **White-box Success Rate**: The fraction of samples for which the prediction of $g_\\phi$ changes when attacked using its own gradients.\n    $$\n    R_{\\text{white-box}} = \\frac{1}{N_{\\text{total}}} \\sum_{i=1}^{N_{\\text{total}}} \\mathbb{I} \\left[ g_\\phi(\\mathbf{x}_i^{\\text{adv-g}}) \\neq g_\\phi(\\mathbf{x}_i) \\right]\n    $$\n    where $\\mathbf{x}_i^{\\text{adv-g}}$ is the adversarial example crafted from $\\mathbf{x}_i$ using $g_\\phi$'s gradients.\n2.  **Transfer Success Rate**: The fraction of samples for which the prediction of $g_\\phi$ changes when subjected to adversarial examples crafted using the gradients of the surrogate model $f_\\theta$.\n    $$\n    R_{\\text{transfer}} = \\frac{1}{N_{\\text{total}}} \\sum_{i=1}^{N_{\\text{total}}} \\mathbb{I} \\left[ g_\\phi(\\mathbf{x}_i^{\\text{adv-f}}) \\neq g_\\phi(\\mathbf{x}_i) \\right]\n    $$\n    where $\\mathbf{x}_i^{\\text{adv-f}}$ is crafted from $\\mathbf{x}_i$ using $f_\\theta$'s gradients.\n\nGradient masking is detected if and only if both conditions are met:\n- $R_{\\text{white-box}}  \\tau$\n- $R_{\\text{transfer}}  R_{\\text{white-box}} + \\Delta$\n\n### 3. Expected Outcomes\n\n- **Test Case 1 (Masked $g_\\phi$)**: The white-box attack on $g_\\phi$ will fail because its input gradient is defined as zero, yielding $R_{\\text{white-box}} = 0$. The transfer attack from the standard model $f_\\theta$ is expected to be effective because the perturbations can alter the signs of the input features, changing the output of $g_\\phi$'s pre-processor $h(\\mathbf{x})$. Thus, we expect $R_{\\text{transfer}}  \\Delta$. The conditions for detection should be met, resulting in `True`.\n- **Test Case 2 (Adversarially Trained $g_\\phi$)**: Adversarial training should confer true robustness. We expect $R_{\\text{white-box}}$ to be low, possibly below $\\tau$. However, this robustness should also reduce the effectiveness of transfer attacks, so $R_{\\text{transfer}}$ is expected to be low as well, and likely not exceed $R_{\\text{white-box}}$ by the margin $\\Delta$. The detection should fail, resulting in `False`.\n- **Test Case 3 (Normal $g_\\phi$)**: Both $f_\\theta$ and $g_\\phi$ are standard, vulnerable models. The white-box attack on $g_\\phi$ should be highly effective, so $R_{\\text{white-box}}$ will likely be greater than $\\tau$. This will cause the first condition for detection to fail, resulting in `False`.\n\nThe implementation proceeds by creating a `LogisticRegression` class capable of handling all three model types, a `pga_attack` function, and a main loop to execute the logic for each test case as described.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# No other libraries are permitted, not even scipy.\n\ndef solve():\n    \"\"\"\n    Main function to run the gradient masking detection experiment.\n    \"\"\"\n\n    class LogisticRegression:\n        \"\"\"\n        A logistic regression classifier with configurable training and attack modes.\n        \"\"\"\n        def __init__(self, model_type='normal', preprocessor=None):\n            if model_type not in ['normal', 'masked', 'adversarial']:\n                raise ValueError(\"Invalid model_type\")\n            self.model_type = model_type\n            self.preprocessor = preprocessor\n            self.w = None\n            self.b = None\n\n        def _sigmoid(self, z):\n            # Clip z to prevent overflow in np.exp\n            z = np.clip(z, -500, 500)\n            return 1 / (1 + np.exp(-z))\n\n        def predict_proba(self, X):\n            X_proc = self.preprocessor(X) if self.preprocessor else X\n            z = X_proc @ self.w + self.b\n            return self._sigmoid(z)\n\n        def predict(self, X):\n            return (self.predict_proba(X) = 0.5).astype(int)\n\n        def loss(self, X, y):\n            p = self.predict_proba(X)\n            # Add a small epsilon to avoid log(0)\n            epsilon = 1e-9\n            p = np.clip(p, epsilon, 1 - epsilon)\n            return -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n\n        def fit(self, X, y, lr, epochs, adv_params=None):\n            X_proc = self.preprocessor(X) if self.preprocessor else X\n            n_samples, n_features = X_proc.shape\n            \n            # Initialize weights\n            self.w = np.zeros((n_features, 1))\n            self.b = 0.0\n\n            # Reshape y to be a column vector if it's not\n            if y.ndim == 1:\n                y = y.reshape(-1, 1)\n\n            for epoch in range(epochs):\n                if self.model_type == 'adversarial' and adv_params:\n                    # Adversarial Training: generate adversarial examples for this epoch\n                    X_adv = self._pga_attack(X, y, **adv_params)\n                    X_train = X_adv\n                else:\n                    X_train = X\n\n                # Use preprocessor if the model is 'masked'\n                X_train_proc = self.preprocessor(X_train) if self.preprocessor else X_train\n\n                p = self.predict_proba(X_train) # Probabilities on original X for grad, but update on adversarial\n                \n                # Full-batch gradient descent\n                dw = (1 / n_samples) * X_train_proc.T @ (p - y)\n                db = (1 / n_samples) * np.sum(p - y)\n\n                self.w -= lr * dw\n                self.b -= lr * db\n        \n        def _gradient_wrt_input(self, X, y):\n            \"\"\"Computes the gradient of the loss with respect to the input X.\"\"\"\n            if self.model_type == 'masked':\n                return np.zeros_like(X)\n\n            if y.ndim == 1:\n                y = y.reshape(-1, 1)\n\n            p = self.predict_proba(X)\n            grad = (p - y) @ self.w.T\n            return grad\n\n        def _pga_attack(self, X, y, epsilon, alpha, T):\n            \"\"\"Projected Gradient Ascent attack.\"\"\"\n            X_adv = X.copy()\n            X_orig = X.copy()\n\n            for _ in range(T):\n                grad = self._gradient_wrt_input(X_adv, y)\n                X_adv = X_adv + alpha * np.sign(grad)\n                \n                # Project perturbation back to epsilon-ball\n                delta = X_adv - X_orig\n                delta = np.clip(delta, -epsilon, epsilon)\n                X_adv = X_orig + delta\n                \n                # Clip to valid input range [-1, 1]\n                X_adv = np.clip(X_adv, -1.0, 1.0)\n            \n            return X_adv\n\n    def generate_data(n_per_class, mu1, mu0, sigma, seed):\n        \"\"\"Generates synthetic 2D data from two Gaussian distributions.\"\"\"\n        np.random.seed(seed)\n        cov = np.diag([sigma**2, sigma**2])\n        \n        X1 = np.random.multivariate_normal(mu1, cov, n_per_class)\n        y1 = np.ones(n_per_class)\n        \n        X0 = np.random.multivariate_normal(mu0, cov, n_per_class)\n        y0 = np.zeros(n_per_class)\n        \n        X = np.vstack((X1, X0))\n        y = np.hstack((y1, y0))\n\n        # Clip data to [-1, 1]^2\n        X = np.clip(X, -1.0, 1.0)\n        \n        # Shuffle data\n        perm = np.random.permutation(2 * n_per_class)\n        return X[perm], y[perm]\n\n    test_cases = [\n        # Case 1: Masked g_phi\n        {\n            'data_params': {'n_per_class': 200, 'mu1': (0.3, 0.3), 'mu0': (-0.3, -0.3), 'sigma': 0.6},\n            'attack_params': {'epsilon': 1.0, 'alpha': 0.2, 'T': 10},\n            'g_phi_type': 'masked',\n            'g_phi_preprocessor': np.sign,\n            'detection_params': {'tau': 0.1, 'Delta': 0.2},\n            'train_params': {'lr': 0.01, 'epochs': 1000},\n            'g_phi_adv_train_params': None,\n            'seed': 42\n        },\n        # Case 2: Adversarially trained g_phi\n        {\n            'data_params': {'n_per_class': 200, 'mu1': (0.7, 0.7), 'mu0': (-0.7, -0.7), 'sigma': 0.3},\n            'attack_params': {'epsilon': 0.3, 'alpha': 0.05, 'T': 40},\n            'g_phi_type': 'adversarial',\n            'g_phi_preprocessor': None,\n            'detection_params': {'tau': 0.2, 'Delta': 0.2},\n            'train_params': {'lr': 0.01, 'epochs': 1000}, # For f_theta\n            'g_phi_adv_train_params': {'lr':0.1, 'epochs':50, 'adv_params': {'epsilon': 0.3, 'alpha': 0.05, 'T': 10}},\n            'seed': 43\n        },\n        # Case 3: Both models normal\n        {\n            'data_params': {'n_per_class': 200, 'mu1': (0.2, 0.2), 'mu0': (-0.2, -0.2), 'sigma': 0.5},\n            'attack_params': {'epsilon': 0.3, 'alpha': 0.05, 'T': 20},\n            'g_phi_type': 'normal',\n            'g_phi_preprocessor': None,\n            'detection_params': {'tau': 0.2, 'Delta': 0.2},\n            'train_params': {'lr': 0.01, 'epochs': 1000},\n            'g_phi_adv_train_params': None,\n            'seed': 44\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # 1. Data Generation\n        X, y = generate_data(**case['data_params'], seed=case['seed'])\n        \n        # 2. Train f_theta (always standard)\n        f_theta = LogisticRegression(model_type='normal')\n        f_theta.fit(X, y, **case['train_params'])\n\n        # 3. Train g_phi (type depends on case)\n        g_phi = LogisticRegression(model_type=case['g_phi_type'], preprocessor=case['g_phi_preprocessor'])\n        if g_phi.model_type == 'adversarial':\n            g_phi.fit(X, y, **case['g_phi_adv_train_params'])\n        else:\n            g_phi.fit(X, y, **case['train_params'])\n\n        # 4. Evaluation\n        y_pred_g_orig = g_phi.predict(X)\n\n        # 4a. White-box attack on g_phi\n        X_adv_g = g_phi._pga_attack(X, y, **case['attack_params'])\n        y_pred_g_whitebox = g_phi.predict(X_adv_g)\n        whitebox_rate = np.mean(y_pred_g_orig != y_pred_g_whitebox)\n        \n        # 4b. Transfer attack from f_theta to g_phi\n        X_adv_f = f_theta._pga_attack(X, y, **case['attack_params'])\n        y_pred_g_transfer = g_phi.predict(X_adv_f)\n        transfer_rate = np.mean(y_pred_g_orig != y_pred_g_transfer)\n\n        # 5. Detection Logic\n        tau = case['detection_params']['tau']\n        Delta = case['detection_params']['Delta']\n        is_masked = (whitebox_rate  tau) and (transfer_rate  whitebox_rate + Delta)\n        results.append(is_masked)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(lambda b: str(b).capitalize(), results))}]\")\n\nsolve()\n```", "id": "3097091"}, {"introduction": "Testing a model against a battery of attacks provides empirical evidence of robustness, but it cannot prove that the model is secure against all possible attacks within a certain threat model. For a true guarantee of security, we must turn to methods of formal verification. In this exercise [@problem_id:3098472], you will implement Interval Bound Propagation (IBP), a foundational technique for providing a *certified* radius of robustness, which mathematically proves that no adversarial example within a given $L_{\\infty}$ norm ball can cause a misclassification.", "problem": "You are asked to implement Interval Bound Propagation (IBP) to compute per-layer interval bounds in a feedforward Rectified Linear Unit (ReLU) network and to certify an $L_\\infty$ radius for classification robustness on given samples. Interval Bound Propagation (IBP) is a technique that propagates input intervals through a neural network to obtain output intervals, which can be used to provide a certificate that the predicted class remains unchanged under perturbations bounded in the $L_\\infty$ norm. The derivation must start from first principles. The following are the core definitions and constraints.\n\nThe network is a two-layer feedforward neural network with one hidden layer and a ReLU nonlinearity. Let the input dimension be $d = 4$, the hidden layer dimension be $h = 3$, and the number of output classes be $K = 3$. The first layer applies a linear transformation followed by a Rectified Linear Unit (ReLU); the second layer is linear. The weight matrices and bias vectors are:\n- First layer weight matrix $W_1 \\in \\mathbb{R}^{h \\times d}$:\n  $\\begin{bmatrix}\n  0.5  -0.4  0.3  0.1 \\\\\n  -0.2  0.6  0.1  -0.5 \\\\\n  0.3  0.2  -0.4  0.7\n  \\end{bmatrix}$,\n  and bias vector $b_1 \\in \\mathbb{R}^h$: $[0.1, -0.1, 0.2]$.\n- Second layer weight matrix $W_2 \\in \\mathbb{R}^{K \\times h}$:\n  $\\begin{bmatrix}\n  0.7  -0.3  0.2 \\\\\n  -0.5  0.8  0.1 \\\\\n  0.2  0.1  0.6\n  \\end{bmatrix}$,\n  and bias vector $b_2 \\in \\mathbb{R}^K$: $[0.05, -0.02, 0.01]$.\n\nFundamental base and derivation requirements:\n- The $L_\\infty$ norm of a vector $x \\in \\mathbb{R}^d$ is defined as $\\|x\\|_\\infty = \\max_i |x_i|$. An $L_\\infty$ ball of radius $\\varepsilon$ centered at a point $x_0$ is the set $\\{ x \\in \\mathbb{R}^d \\mid \\|x - x_0\\|_\\infty \\le \\varepsilon \\}$.\n- For a linear transformation $y = W x + b$ and an interval input $x \\in [\\ell, u]$ with lower bound $\\ell \\in \\mathbb{R}^d$ and upper bound $u \\in \\mathbb{R}^d$, the output interval $y \\in [\\ell', u']$ can be derived from first principles: for each component $y_i = \\sum_j W_{ij} x_j + b_i$, the minimum and maximum of $y_i$ over the interval occur at endpoints of each $x_j$. Splitting the matrix into its nonnegative and negative parts, $W^+_{ij} = \\max(W_{ij}, 0)$ and $W^-_{ij} = \\min(W_{ij}, 0)$, yields\n  $$\\ell' = W^+ \\ell + W^- u + b, \\quad u' = W^+ u + W^- \\ell + b.$$\n- The Rectified Linear Unit (ReLU) nonlinearity $\\operatorname{ReLU}(z)$ is defined componentwise by $\\operatorname{ReLU}(z_i) = \\max(0, z_i)$. Given pre-activation intervals $z \\in [\\ell_z, u_z]$, the post-activation intervals are\n  $$\\ell_{\\text{ReLU}} = \\max(0, \\ell_z), \\quad u_{\\text{ReLU}} = \\max(0, u_z),$$\n  where the max is applied elementwise.\n- If the input is an $L_\\infty$ ball of radius $\\varepsilon$ centered at $x_0$, then the input interval is $[\\ell_0, u_0]$ with $\\ell_0 = x_0 - \\varepsilon$ and $u_0 = x_0 + \\varepsilon$.\n\nCertification criterion:\n- Let the network output logits $f(x) \\in \\mathbb{R}^K$. For a sample $x_0$, define the predicted class $c = \\arg\\max_{k \\in \\{0, \\dots, K-1\\}} f_k(x_0)$ at the center point.\n- Using IBP, propagate the input interval $[\\ell_0, u_0]$ through the network to obtain output interval bounds $[\\ell_2, u_2]$ for the logits. A certificate of robustness at radius $\\varepsilon$ is obtained if, for all $k \\ne c$, the lower bound of the predicted logit exceeds the upper bound of every other logit:\n  $$\\ell_{2,c} - u_{2,k}  0 \\quad \\text{for all } k \\ne c.$$\n- The certified $L_\\infty$ radius is the maximum $\\varepsilon \\ge 0$ such that the above inequalities hold under IBP. Because the IBP bounds for linear layers and ReLU mappings are monotone with respect to $\\varepsilon$ (linear transformations expand intervals linearly in $\\varepsilon$ and ReLU is monotone), the certification predicate is monotonically nonincreasing in $\\varepsilon$, making binary search appropriate to compute the maximal $\\varepsilon$ to a specified numerical tolerance.\n\nYour task:\n- Implement IBP for the specified network. For any given $\\varepsilon \\ge 0$, compute the per-layer interval bounds:\n  1. Input interval $[\\ell_0, u_0] = [x_0 - \\varepsilon, x_0 + \\varepsilon]$,\n  2. First-layer linear pre-activation interval $[\\ell_{1,\\text{pre}}, u_{1,\\text{pre}}]$ using the linear interval formulas,\n  3. First-layer post-activation interval $[\\ell_{1}, u_{1}]$ using the ReLU formulas,\n  4. Second-layer linear output interval $[\\ell_2, u_2]$ using the linear interval formulas.\n- Implement a function that, given $x_0$, computes the certified $L_\\infty$ radius using a binary search over $\\varepsilon$ with a termination tolerance of $10^{-6}$, starting from $\\varepsilon = 0$ and expanding the upper search bound geometrically until the certificate fails.\n\nTest suite:\n- Use the following three test samples:\n  - $x^{(1)} = [\\,0.9,\\,-0.3,\\,0.2,\\,0.1\\,]$,\n  - $x^{(2)} = [\\,0.1,\\,0.2,\\,-0.1,\\,0.0\\,]$,\n  - $x^{(3)} = [\\,0.0,\\,0.0,\\,0.0,\\,0.0\\,]$.\n- For each test sample, compute the certified $L_\\infty$ radius as a real number. No physical units apply.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each radius rounded to six decimal places as a decimal. For example: $[r_1,r_2,r_3]$, where $r_i$ is the certified radius for $x^{(i)}$ rounded to six decimals.", "solution": "The objective is to compute the certified $L_\\infty$ robustness radius for a given two-layer Rectified Linear Unit (ReLU) network on specific input samples. The method to be used is Interval Bound Propagation (IBP), a technique that computes bounds on the network's output logits for all inputs within an $L_\\infty$ neighborhood of a given sample $x_0$.\n\nThe network is a feedforward architecture defined by the function $f(x) = z_2$, where the layers are computed as follows:\n1.  First hidden layer pre-activation: $z_1 = W_1 x + b_1$\n2.  First hidden layer post-activation: $a_1 = \\operatorname{ReLU}(z_1)$\n3.  Output layer logits: $z_2 = W_2 a_1 + b_2$\n\nThe dimensions are input dimension $d=4$, hidden layer dimension $h=3$, and output dimension (number of classes) $K=3$. The network parameters are given as:\n- First layer weight matrix $W_1 \\in \\mathbb{R}^{3 \\times 4}$:\n$$W_1 = \\begin{bmatrix}\n0.5  -0.4  0.3  0.1 \\\\\n-0.2  0.6  0.1  -0.5 \\\\\n0.3  0.2  -0.4  0.7\n\\end{bmatrix}$$\n- First layer bias vector $b_1 \\in \\mathbb{R}^3$:\n$$b_1 = \\begin{bmatrix} 0.1 \\\\ -0.1 \\\\ 0.2 \\end{bmatrix}$$\n- Second layer weight matrix $W_2 \\in \\mathbb{R}^{3 \\times 3}$:\n$$W_2 = \\begin{bmatrix}\n0.7  -0.3  0.2 \\\\\n-0.5  0.8  0.1 \\\\\n0.2  0.1  0.6\n\\end{bmatrix}$$\n- Second layer bias vector $b_2 \\in \\mathbb{R}^3$:\n$$b_2 = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.01 \\end{bmatrix}$$\n\nThe input perturbation is defined by an $L_\\infty$ ball of radius $\\varepsilon \\ge 0$ around a central point $x_0 \\in \\mathbb{R}^4$. This set of perturbed inputs is $\\{x \\in \\mathbb{R}^4 \\mid \\|x - x_0\\|_\\infty \\le \\varepsilon\\}$. This can be expressed as an interval where each component $x_i$ lies in $[x_{0,i} - \\varepsilon, x_{0,i} + \\varepsilon]$. We can write this compactly using vector notation as $x \\in [\\ell_0, u_0]$, where $\\ell_0 = x_0 - \\varepsilon \\mathbf{1}$ and $u_0 = x_0 + \\varepsilon \\mathbf{1}$.\n\nThe IBP procedure propagates these interval bounds layer by layer.\n\n**Step 1: Propagation through the First Linear Layer**\nThe pre-activation of the first hidden layer is $z_1 = W_1 x + b_1$. To find the interval bounds $[\\ell_{1,\\text{pre}}, u_{1,\\text{pre}}]$ for $z_1$, we must find the minimum and maximum of each component $z_{1,i} = \\sum_j (W_1)_{ij} x_j + (b_1)_i$ over $x \\in [\\ell_0, u_0]$. Since this is a linear function of the $x_j$, and the variables $x_j$ are independent within their intervals, the minimum and maximum are attained by choosing either $\\ell_{0,j}$ or $u_{0,j}$ for each $x_j$. Specifically, to minimize $z_{1,i}$, we choose $x_j = \\ell_{0,j}$ if $(W_1)_{ij} \\ge 0$ and $x_j = u_{0,j}$ if $(W_1)_{ij}  0$. A compact way to write this is by splitting the weight matrix into its non-negative part, $W_1^+$, and its negative part, $W_1^-$, where $(W_1^+)_{ij} = \\max((W_1)_{ij}, 0)$ and $(W_1^-)_{ij} = \\min((W_1)_{ij}, 0)$.\nThe lower and upper bounds for the pre-activation vector $z_1$ are then:\n$$\\ell_{1,\\text{pre}} = W_1^+ \\ell_0 + W_1^- u_0 + b_1$$\n$$u_{1,\\text{pre}} = W_1^+ u_0 + W_1^- \\ell_0 + b_1$$\n\n**Step 2: Propagation through the ReLU Activation**\nThe activation function is the Rectified Linear Unit, $a_1 = \\operatorname{ReLU}(z_1)$, applied elementwise. Given the pre-activation interval $z_1 \\in [\\ell_{1,\\text{pre}}, u_{1,\\text{pre}}]$, the post-activation interval $[\\ell_1, u_1]$ is determined by the monotonic nature of the ReLU function. The lower bound of the output is the ReLU of the input lower bound, and the upper bound is the ReLU of the input upper bound.\n$$\\ell_1 = \\operatorname{ReLU}(\\ell_{1,\\text{pre}}) = \\max(0, \\ell_{1,\\text{pre}})$$\n$$u_1 = \\operatorname{ReLU}(u_{1,\\text{pre}}) = \\max(0, u_{1,\\text{pre}})$$\nwhere the $\\max$ is applied elementwise.\n\n**Step 3: Propagation through the Second Linear Layer**\nThe final output logits are $z_2 = W_2 a_1 + b_2$. The propagation rule is identical to the first linear layer, but now applied to the post-activation interval $[\\ell_1, u_1]$ from the hidden layer. We define $W_2^+ = \\max(W_2, 0)$ and $W_2^- = \\min(W_2, 0)$. The final output logit interval $[\\ell_2, u_2]$ is:\n$$\\ell_2 = W_2^+ \\ell_1 + W_2^- u_1 + b_2$$\n$$u_2 = W_2^+ u_1 + W_2^- \\ell_1 + b_2$$\n\n**Robustness Certification Criterion**\nFor a given sample $x_0$ and perturbation radius $\\varepsilon$, the network's prediction is robustly certified if the predicted class at the center point $x_0$ remains the highest-scoring class for all $x$ in the $L_\\infty$ ball. Let $c = \\arg\\max_k f_k(x_0)$ be the predicted class for the unperturbed input. The certification condition using IBP is that the lower bound of the logit for class $c$ must be strictly greater than the upper bound of the logit for any other class $k \\neq c$.\n$$\\ell_{2,c}  u_{2,k} \\quad \\text{for all } k \\neq c$$\n\n**Computing the Certified Radius**\nThe goal is to find the maximum $\\varepsilon \\ge 0$ for which this certification condition holds. The bounds $\\ell_2$ and $u_2$ are monotonic functions of $\\varepsilon$. Specifically, the gap $\\ell_{2,c} - u_{2,k}$ is a monotonically non-increasing function of $\\varepsilon$. This property allows us to use binary search to efficiently find the maximum certified radius $\\varepsilon^*$ to a desired numerical tolerance.\n\nThe algorithm is as follows:\n1.  For a given input $x_0$, first determine the predicted class $c = \\arg\\max_k f_k(x_0)$. If there is a tie for the maximum logit, the certified radius is $0$, as the certification condition fails even for $\\varepsilon=0$.\n2.  Establish a search range $[\\varepsilon_{\\text{low}}, \\varepsilon_{\\text{high}}]$ for the binary search. We start with $\\varepsilon_{\\text{low}} = 0$. We find an initial $\\varepsilon_{\\text{high}}$ by starting with a small value (e.g., $0.1$) and geometrically increasing it (e.g., doubling) until the certification check fails. The last value that passed becomes the lower bound of the binary search range, and the first value that failed becomes the upper bound.\n3.  Perform a binary search on $\\varepsilon$ within the range $[\\varepsilon_{\\text{low}}, \\varepsilon_{\\text{high}}]$. In each step, we test the midpoint $\\varepsilon_{\\text{mid}} = (\\varepsilon_{\\text{low}} + \\varepsilon_{\\text{high}}) / 2$.\n    - If the certificate holds for $\\varepsilon_{\\text{mid}}$, it means a radius of at least $\\varepsilon_{\\text{mid}}$ is achievable, so we update $\\varepsilon_{\\text{low}} = \\varepsilon_{\\text{mid}}$.\n    - If the certificate fails, the radius is too large, so we update $\\varepsilon_{\\text{high}} = \\varepsilon_{\\text{mid}}$.\n4.  The search terminates when $\\varepsilon_{\\text{high}} - \\varepsilon_{\\text{low}}$ is smaller than a specified tolerance, here $10^{-6}$. The resulting certified radius is $\\varepsilon_{\\text{low}}$.\n\nThis procedure is applied to each of the three provided test samples to find their respective certified $L_\\infty$ radii.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to compute and print the certified radii\n    for the given test cases.\n    \"\"\"\n    # Define network parameters\n    W1 = np.array([\n        [0.5, -0.4, 0.3, 0.1],\n        [-0.2, 0.6, 0.1, -0.5],\n        [0.3, 0.2, -0.4, 0.7]\n    ], dtype=np.float64)\n\n    b1 = np.array([0.1, -0.1, 0.2], dtype=np.float64)\n\n    W2 = np.array([\n        [0.7, -0.3, 0.2],\n        [-0.5, 0.8, 0.1],\n        [0.2, 0.1, 0.6]\n    ], dtype=np.float64)\n\n    b2 = np.array([0.05, -0.02, 0.01], dtype=np.float64)\n\n    network_params = (W1, b1, W2, b2)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([0.9, -0.3, 0.2, 0.1], dtype=np.float64),\n        np.array([0.1, 0.2, -0.1, 0.0], dtype=np.float64),\n        np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float64),\n    ]\n\n    results = []\n    for x0 in test_cases:\n        radius = compute_certified_radius(x0, network_params)\n        results.append(f\"{radius:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef forward_pass(x, network_params):\n    \"\"\"Computes the network output (logits) for a single input point.\"\"\"\n    W1, b1, W2, b2 = network_params\n    z1 = W1 @ x + b1\n    a1 = np.maximum(0, z1)\n    z2 = W2 @ a1 + b2\n    return z2\n\ndef ibp_propagate(x0, epsilon, network_params):\n    \"\"\"\n    Performs Interval Bound Propagation for a given input x0 and radius epsilon.\n    Returns the final lower and upper bounds on the output logits.\n    \"\"\"\n    W1, b1, W2, b2 = network_params\n\n    # Input interval bounds\n    l0 = x0 - epsilon\n    u0 = x0 + epsilon\n\n    # Layer 1: Linear transformation\n    W1_pos = np.maximum(W1, 0)\n    W1_neg = np.minimum(W1, 0)\n    l1_pre = W1_pos @ l0 + W1_neg @ u0 + b1\n    u1_pre = W1_pos @ u0 + W1_neg @ l0 + b1\n\n    # Layer 1: ReLU activation\n    l1 = np.maximum(0, l1_pre)\n    u1 = np.maximum(0, u1_pre)\n\n    # Layer 2: Linear transformation\n    W2_pos = np.maximum(W2, 0)\n    W2_neg = np.minimum(W2, 0)\n    l2 = W2_pos @ l1 + W2_neg @ u1 + b2\n    u2 = W2_pos @ u1 + W2_neg @ l1 + b2\n\n    return l2, u2\n\ndef check_certificate(x0, epsilon, network_params, c):\n    \"\"\"\n    Checks the robustness certification criterion for a given epsilon.\n    \"\"\"\n    l2, u2 = ibp_propagate(x0, epsilon, network_params)\n    l2_c = l2[c]\n    \n    for k in range(len(l2)):\n        if k == c:\n            continue\n        if l2_c = u2[k]:\n            return False  # Certificate fails\n    \n    return True  # Certificate holds\n\ndef compute_certified_radius(x0, network_params):\n    \"\"\"\n    Computes the maximum certified L-infinity radius using binary search.\n    \"\"\"\n    tolerance = 1e-6\n\n    # Determine predicted class at the center point\n    logits_x0 = forward_pass(x0, network_params)\n    c = np.argmax(logits_x0)\n\n    # Check if robust at epsilon = 0. If not, radius is 0.\n    # This happens if there's a tie for the top logit.\n    if not check_certificate(x0, 0, network_params, c):\n        return 0.0\n\n    # Phase 1: Find an upper bound for the binary search by geometric expansion.\n    eps_low = 0.0\n    eps_high = 0.1  # Initial guess for the upper bound\n    while check_certificate(x0, eps_high, network_params, c):\n        eps_low = eps_high\n        eps_high *= 2.0\n        # Safety break for extremely robust cases to avoid long search\n        if eps_high  100.0:\n            break\n\n    # Phase 2: Binary search in the interval [eps_low, eps_high].\n    while (eps_high - eps_low)  tolerance:\n        eps_mid = (eps_low + eps_high) / 2.0\n        if eps_mid == eps_low or eps_mid == eps_high: # Precision limit reached\n            break\n        if check_certificate(x0, eps_mid, network_params, c):\n            eps_low = eps_mid  # This radius is achievable, try for more.\n        else:\n            eps_high = eps_mid  # This radius is too large.\n\n    return eps_low\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3098472"}]}