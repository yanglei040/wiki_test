{"hands_on_practices": [{"introduction": "To build robust uncertainty estimates, we must first understand how to derive predictive probabilities from a model that outputs not just a single value, but a distribution. This first exercise takes you back to fundamentals, asking you to work with a model where the logits themselves are Gaussian random variables. By deriving the predictive probability from scratch, you will gain a foundational understanding of how uncertainty in a model's internal states propagates to its final predictions. [@problem_id:3179687]", "problem": "Consider a binary deep neural network classifier that, for a specific input $x^{\\star}$, outputs per-class logit means and heteroscedastic logit variances. Denote the latent logit for class $c \\in \\{0, 1\\}$ by $z_{c}$, with $z_{c}$ modeled as a Gaussian random variable with mean $\\mu_{c}$ and variance $\\sigma_{c}^{2}$, that is, $z_{c} \\sim \\mathcal{N}(\\mu_{c}, \\sigma_{c}^{2})$. Assume the latent logits are independent across classes. The predictive decision rule selects the class with the largest latent logit. The predictive probability for class $1$ is therefore the probability of the event $z_{1} > z_{0}$ under the joint distribution of $(z_{0}, z_{1})$.\n\nStarting from the definitions of independent Gaussian random variables and the Probability Density Function (PDF) of the Gaussian distribution, derive the distribution of the difference $d = z_{1} - z_{0}$, express the predictive probability $p(y = 1 \\mid x^{\\star})$ as an integral over the appropriate region, and evaluate it in closed-form using the properties of Gaussian distributions.\n\nFinally, compute the numerical value of $p(y = 1 \\mid x^{\\star})$ for $x^{\\star}$ with model outputs $\\mu_{1} = 1.3$, $\\mu_{0} = 0.6$, $\\sigma_{1} = 0.3$, and $\\sigma_{0} = 0.4$. Round your numerical answer to four significant figures. Express your final answer as a pure number without any unit.", "solution": "The problem statement is critically validated and found to be valid. It is scientifically grounded in probability theory and statistics as applied to uncertainty estimation in deep learning, is well-posed with a unique and stable solution, and is expressed in objective, unambiguous language. All necessary data and conditions for a solution are provided.\n\nThe problem requires us to analyze the predictive probability of a binary classifier whose latent logits, $z_0$ and $z_1$, are modeled as independent Gaussian random variables. We are given:\n$z_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$\n$z_0 \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$\nThe two random variables $z_1$ and $z_0$ are independent. The predictive probability for class $1$ is given by $p(y = 1 \\mid x^{\\star}) = P(z_1 > z_0)$.\n\nFirst, we derive the distribution of the difference $d = z_1 - z_0$. A fundamental property of Gaussian distributions states that a linear combination of independent Gaussian random variables is also a Gaussian random variable. The difference $d$ is a linear combination of $z_1$ and $z_0$ with coefficients $1$ and $-1$, respectively.\n\nThe mean of $d$, denoted $\\mu_d$, is the expectation of the difference:\n$$\n\\mu_d = E[d] = E[z_1 - z_0] = E[z_1] - E[z_0] = \\mu_1 - \\mu_0\n$$\nThe variance of $d$, denoted $\\sigma_d^2$, is the variance of the difference. Due to the independence of $z_1$ and $z_0$, the variance of the sum (or difference) is the sum of the variances:\n$$\n\\sigma_d^2 = \\text{Var}(d) = \\text{Var}(z_1 - z_0) = \\text{Var}(z_1) + \\text{Var}(-1 \\cdot z_0) = \\text{Var}(z_1) + (-1)^2 \\text{Var}(z_0) = \\sigma_1^2 + \\sigma_0^2\n$$\nThus, the distribution of the difference $d$ is a Gaussian distribution with mean $\\mu_1 - \\mu_0$ and variance $\\sigma_1^2 + \\sigma_0^2$:\n$$\nd \\sim \\mathcal{N}(\\mu_1 - \\mu_0, \\sigma_1^2 + \\sigma_0^2)\n$$\nThe Probability Density Function (PDF) of $d$ is given by:\n$$\nf_d(x) = \\frac{1}{\\sqrt{2\\pi(\\sigma_1^2 + \\sigma_0^2)}} \\exp\\left(-\\frac{(x - (\\mu_1 - \\mu_0))^2}{2(\\sigma_1^2 + \\sigma_0^2)}\\right)\n$$\n\nNext, we express the predictive probability $p(y = 1 \\mid x^{\\star})$ as an integral. The event $z_1 > z_0$ is equivalent to the event $z_1 - z_0 > 0$, which is $d > 0$. The probability of this event is the integral of the PDF of $d$ over the region where $x > 0$:\n$$\np(y = 1 \\mid x^{\\star}) = P(d > 0) = \\int_{0}^{\\infty} f_d(x) \\,dx\n$$\nSubstituting the expression for $f_d(x)$:\n$$\np(y = 1 \\mid x^{\\star}) = \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi(\\sigma_1^2 + \\sigma_0^2)}} \\exp\\left(-\\frac{(x - (\\mu_1 - \\mu_0))^2}{2(\\sigma_1^2 + \\sigma_0^2)}\\right) \\,dx\n$$\n\nTo evaluate this integral in closed-form, we perform a change of variables to standardize the distribution. Let $\\mu_d = \\mu_1 - \\mu_0$ and $\\sigma_d = \\sqrt{\\sigma_1^2 + \\sigma_0^2}$. We introduce a new variable $u = \\frac{x - \\mu_d}{\\sigma_d}$. This is a standard normal variable, $u \\sim \\mathcal{N}(0, 1)$. The differential is $du = \\frac{1}{\\sigma_d}dx$, so $dx = \\sigma_d du$. The integration limits must also be transformed:\n- When $x = 0$, the lower limit becomes $u = \\frac{0 - \\mu_d}{\\sigma_d} = -\\frac{\\mu_d}{\\sigma_d}$.\n- When $x \\to \\infty$, the upper limit becomes $u \\to \\infty$.\n\nSubstituting these into the integral gives:\n$$\np(y = 1 \\mid x^{\\star}) = \\int_{-\\mu_d/\\sigma_d}^{\\infty} \\frac{1}{\\sqrt{2\\pi}\\sigma_d} \\exp\\left(-\\frac{u^2}{2}\\right) (\\sigma_d \\,du) = \\int_{-\\mu_d/\\sigma_d}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2}\\right) \\,du\n$$\nThis integral is the definition of the survival function of the standard normal distribution evaluated at $-\\mu_d/\\sigma_d$. It is equivalent to $1 - \\Phi(-\\mu_d/\\sigma_d)$, where $\\Phi(z)$ is the Cumulative Distribution Function (CDF) of the standard normal distribution, $\\Phi(z) = P(Z \\le z)$ for $Z \\sim \\mathcal{N}(0, 1)$. Using the symmetry property of the standard normal distribution, $\\Phi(-z) = 1 - \\Phi(z)$, we have:\n$$\nP(d > 0) = 1 - \\Phi(-\\mu_d/\\sigma_d) = 1 - (1 - \\Phi(\\mu_d/\\sigma_d)) = \\Phi(\\mu_d/\\sigma_d)\n$$\nSubstituting back the expressions for $\\mu_d$ and $\\sigma_d$, the closed-form expression for the predictive probability is:\n$$\np(y = 1 \\mid x^{\\star}) = \\Phi\\left(\\frac{\\mu_1 - \\mu_0}{\\sqrt{\\sigma_1^2 + \\sigma_0^2}}\\right)\n$$\n\nFinally, we compute the numerical value for the given model outputs: $\\mu_1 = 1.3$, $\\mu_0 = 0.6$, $\\sigma_1 = 0.3$, and $\\sigma_0 = 0.4$.\nFirst, we calculate the argument of the $\\Phi$ function:\nThe difference in means is $\\mu_1 - \\mu_0 = 1.3 - 0.6 = 0.7$.\nThe sum of variances is $\\sigma_1^2 + \\sigma_0^2 = (0.3)^2 + (0.4)^2 = 0.09 + 0.16 = 0.25$.\nThe standard deviation of the difference is $\\sqrt{\\sigma_1^2 + \\sigma_0^2} = \\sqrt{0.25} = 0.5$.\nThe argument is therefore:\n$$\n\\frac{\\mu_1 - \\mu_0}{\\sqrt{\\sigma_1^2 + \\sigma_0^2}} = \\frac{0.7}{0.5} = 1.4\n$$\nThe predictive probability is thus:\n$$\np(y = 1 \\mid x^{\\star}) = \\Phi(1.4)\n$$\nUsing a standard normal distribution table or a computational tool, we find the value of the CDF at $z = 1.4$:\n$$\n\\Phi(1.4) \\approx 0.91924334\n$$\nRounding this value to four significant figures gives $0.9192$.", "answer": "$$\n\\boxed{0.9192}\n$$", "id": "3179687"}, {"introduction": "While the previous exercise showed how to derive a perfectly calibrated probability from a probabilistic model, real-world classifiers are often miscalibrated, meaning their confidence scores do not reflect their true accuracy. This practical coding challenge has you implement and compare two common post-hoc calibration techniques, one in logit-space and one in probability-space. You will learn to use standard metrics like Expected Calibration Error ($ECE$) and predictive entropy to diagnose and improve the reliability of a model's uncertainty estimates. [@problem_id:3179715]", "problem": "You are given a binary classification setting where model outputs are pre-sigmoid scores (logits). The goal is to study uncertainty estimation through calibration and entropy, by comparing raw predictions, linear post-hoc calibration in logit space, and linear post-hoc calibration in probability space. Build a program that implements the following computations and outputs specified quantities for a provided test suite.\n\nDefinitions and foundational base:\n- A binary classifier outputs a real-valued logit $z \\in \\mathbb{R}$ for each input. The predicted probability of the positive class is $p = \\sigma(z)$ where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the logistic sigmoid function.\n- The predictive entropy of a Bernoulli distribution with parameter $p$ is $H(p) = - \\left( p \\ln p + (1 - p) \\ln(1 - p) \\right)$ using the natural logarithm.\n- The negative log-likelihood (NLL) for binary labels $y \\in \\{0,1\\}$ given probabilities $p$ is $\\mathrm{NLL}(p,y) = - \\left( y \\ln p + (1 - y) \\ln(1 - p) \\right)$, and the average NLL over a dataset is the arithmetic mean of per-sample NLLs. Minimizing average NLL is a well-established principle for fitting calibration parameters as it corresponds to maximum likelihood estimation under a Bernoulli model.\n- Expected Calibration Error (ECE) assesses calibration by binning predicted confidences and comparing bin-wise average accuracy to bin-wise average confidence. For binary classification, define prediction $\\hat{y} = \\mathbb{I}[p \\geq 0.5]$ and confidence $c = \\max(p, 1 - p)$. Let the confidence interval $[0,1]$ be partitioned into $B$ equal-width bins. For bin $b$, let $n_b$ be the number of samples in that bin, $\\mathrm{acc}_b$ the average of $\\mathbb{I}[\\hat{y} = y]$ over samples in the bin, and $\\mathrm{conf}_b$ the average of $c$ over samples in the bin. The expected calibration error is\n$$\n\\mathrm{ECE} = \\sum_{b=1}^{B} \\frac{n_b}{N} \\left| \\mathrm{acc}_b - \\mathrm{conf}_b \\right|,\n$$\nwhere $N$ is the dataset size. Empty bins are omitted from the sum.\n\nCalibration mappings to compare:\n- Logit-space linear post-hoc calibration (Platt scaling): Given logits $z$, transform $z' = a z + b$ with parameters $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$. The calibrated probability is $p' = \\sigma(z')$. Fit $(a,b)$ by minimizing average NLL on the provided dataset.\n- Probability-space linear post-hoc calibration: Given raw probabilities $p = \\sigma(z)$, transform $p'' = c p + d$ using a monotone linear mapping that preserves the unit interval. Parameterize $d = \\sigma(u)$ and $c = (1 - d)\\, \\sigma(v)$ with unconstrained $u,v \\in \\mathbb{R}$ to ensure $p'' \\in (0,1)$ for all $p \\in [0,1]$. Fit $(u,v)$ by minimizing average NLL on the provided dataset, and compute $p''$ via the implied $(c,d)$.\n\nTasks for each dataset:\n1. Compute raw probabilities $p = \\sigma(z)$.\n2. Fit $(a,b)$ and compute calibrated probabilities $p' = \\sigma(a z + b)$.\n3. Fit $(u,v)$, form $(c,d)$, and compute calibrated probabilities $p'' = c p + d$.\n4. Compute $\\mathrm{ECE}$ for $p$, $p'$, and $p''$ using $B = 10$ equal-width bins on $[0,1]$.\n5. Compute the mean predictive entropy $\\overline{H}$ for $p$, $p'$, and $p''$ using $H(p)$ defined above.\n\nYour program must implement these steps using the test suite below and produce, for each dataset, a list of six floats in the order\n$$\n\\left[ \\mathrm{ECE}(p), \\mathrm{ECE}(p'), \\mathrm{ECE}(p''), \\overline{H}(p), \\overline{H}(p'), \\overline{H}(p'') \\right].\n$$\nAggregate the results for all datasets into a single list of lists.\n\nTest suite (each dataset is a pair $(z,y)$ with logits $z$ and labels $y$; numbers are real-valued for $z$ and binary for $y$):\n- Dataset $1$: $z = \\{-2.0, -0.5, 0.0, 0.5, 1.5, -1.2, 2.3, -2.5, 0.8, -0.8\\}$, $y = \\{0, 0, 1, 1, 1, 0, 1, 0, 1, 0\\}$.\n- Dataset $2$: $z = \\{5.0, -5.0, 4.0, -4.0, 6.0, -6.0, 3.5, -3.5\\}$, $y = \\{1, 0, 0, 1, 1, 0, 0, 1\\}$.\n- Dataset $3$: $z = \\{-0.05, 0.07, -0.12, 0.15, -0.2, 0.22, -0.3, 0.35, -0.4, 0.45\\}$, $y = \\{0, 1, 0, 1, 0, 1, 0, 1, 0, 1\\}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results formatted as a comma-separated list of per-dataset result lists, enclosed in square brackets. For example, the output should look like $[ [r_{11}, r_{12}, \\dots, r_{16}], [r_{21}, \\dots, r_{26}], [r_{31}, \\dots, r_{36}] ]$ where each $r_{ij}$ is a float computed as above. No additional text should be printed.", "solution": "The user has provided a well-defined problem in the domain of machine learning, specifically focusing on the calibration of binary classifiers. The problem is scientifically grounded, internally consistent, and requires the implementation of standard, verifiable algorithms. I will proceed with a solution.\n\nThe task is to compare three types of probability estimates from a binary classifier's logits ($z$): raw probabilities, probabilities from a linearly calibrated logit-space model (Platt scaling), and probabilities from a linearly calibrated probability-space model. For each of these three probability sets, we must compute two metrics: the Expected Calibration Error (ECE) and the mean predictive entropy.\n\nThis process involves three main stages for each dataset provided:\n1.  **Prediction Generation**:\n    *   **Raw Probabilities ($p$)**: These are computed directly from the logits using the logistic sigmoid function, $p_i = \\sigma(z_i) = \\frac{1}{1 + e^{-z_i}}$.\n    *   **Logit-Space Calibrated Probabilities ($p'$)**: This method, known as Platt scaling, finds optimal parameters $(a,b)$ by transforming the logits $z'_i = a z_i + b$ and then applying the sigmoid function, $p'_i = \\sigma(z'_i)$. The parameters $(a, b)$ are optimized by minimizing the average Negative Log-Likelihood (NLL) on the dataset. The NLL for a single prediction $p$ and true label $y \\in \\{0, 1\\}$ is given by $\\mathrm{NLL}(p, y) = -[y \\ln(p) + (1-y)\\ln(1-p)]$. This objective function is convex, guaranteeing a unique global minimum for $(a,b)$.\n    *   **Probability-Space Calibrated Probabilities ($p''$)**: This method transforms the raw probabilities directly via a linear map $p''_i = c p_i + d$. To ensure that $p''_i$ remains a valid probability (i.e., in the interval $(0,1)$), the parameters $(c,d)$ are reparameterized using unconstrained variables $(u,v) \\in \\mathbb{R}^2$ as $d = \\sigma(u)$ and $c = (1-d)\\sigma(v)$. This construction guarantees $c > 0$ and $d \\in (0,1)$, ensuring that the transformation is monotone and maps the interval $[0,1]$ into $(0,1)$. The parameters $(u, v)$ are found by minimizing the average NLL, similar to Platt scaling. We use numerical optimization to find these parameters.\n\n2.  **Metric Calculation**:\n    *   **Expected Calibration Error (ECE)**: ECE measures the discrepancy between a model's confidence and its accuracy. We first calculate the confidence for each prediction, defined as $c_i = \\max(p_i, 1-p_i)$. The predictions' confidences are then binned into $B=10$ equal-width intervals over $[0,1]$. For each bin $b$, we compute the average confidence $\\mathrm{conf}_b$ and the average accuracy $\\mathrm{acc}_b$. The ECE is the weighted average of the absolute differences $|\\mathrm{conf}_b - \\mathrm{acc}_b|$ over all non-empty bins, weighted by the proportion of samples in each bin.\n    *   **Mean Predictive Entropy ($\\overline{H}$)**: This metric quantifies the average uncertainty of the model's predictions. For each probability $p_i$, the predictive entropy is calculated as $H(p_i) = -[p_i \\ln(p_i) + (1-p_i)\\ln(1-p_i)]$. The mean entropy is the arithmetic average of these values over the entire dataset. A higher mean entropy indicates greater overall uncertainty in the predictions.\n\n3.  **Implementation Strategy**:\n    *   Helper functions for the sigmoid, NLL, predictive entropy, and ECE are implemented.\n    - To find the optimal calibration parameters for both Platt scaling and the probability-space model, we define objective functions corresponding to the average NLL. We then use the `scipy.optimize.minimize` function with the `L-BFGS-B` algorithm to find the parameters that minimize these objectives.\n    - The entire process is encapsulated in a main function that iterates through the provided test datasets, performs all computations, and formats the results into the specified nested list structure for the final output. Numerical stability is ensured by clipping probability values to a small positive distance from $0$ and $1$ before applying the logarithm.\n\nThe program will execute these steps for each of the three datasets in the test suite and collate the six required floating-point values ($\\mathrm{ECE}(p)$, $\\mathrm{ECE}(p')$, $\\mathrm{ECE}(p'')$, $\\overline{H}(p)$, $\\overline{H}(p')$, $\\overline{H}(p'')$) for each dataset into a final list of lists.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define a small epsilon for numerical stability in log calculations\nEPSILON = 1e-15\n\ndef sigmoid(z):\n    \"\"\"Computes the logistic sigmoid function.\"\"\"\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef predictive_entropy(p):\n    \"\"\"Computes the predictive entropy for a Bernoulli probability.\"\"\"\n    p = np.clip(p, EPSILON, 1.0 - EPSILON)\n    return -(p * np.log(p) + (1.0 - p) * np.log(1.0 - p))\n\ndef compute_nll(p, y):\n    \"\"\"Computes the negative log-likelihood for binary classification.\"\"\"\n    p = np.clip(p, EPSILON, 1.0 - EPSILON)\n    return -(y * np.log(p) + (1.0 - y) * np.log(1.0 - p))\n\ndef compute_ece(p, y, n_bins=10):\n    \"\"\"Computes the Expected Calibration Error (ECE).\"\"\"\n    if len(p) == 0:\n        return 0.0\n        \n    p = np.asarray(p)\n    y = np.asarray(y)\n\n    predictions = (p >= 0.5).astype(int)\n    confidences = np.maximum(p, 1.0 - p)\n    is_correct = (predictions == y).astype(int)\n\n    # Bin confidences. The last bin includes 1.0.\n    bin_indices = np.minimum(n_bins - 1, np.floor(confidences * n_bins)).astype(int)\n    \n    ece = 0.0\n    total_samples = len(p)\n    \n    for b in range(n_bins):\n        in_bin = (bin_indices == b)\n        num_in_bin = np.sum(in_bin)\n        \n        if num_in_bin > 0:\n            bin_accuracy = np.mean(is_correct[in_bin])\n            bin_confidence = np.mean(confidences[in_bin])\n            ece += (num_in_bin / total_samples) * np.abs(bin_accuracy - bin_confidence)\n            \n    return ece\n\ndef fit_platt_scaling(z, y):\n    \"\"\"Fits Platt scaling parameters (a, b) by minimizing NLL.\"\"\"\n    def objective(params):\n        a, b = params\n        p_calibrated = sigmoid(a * z + b)\n        return np.mean(compute_nll(p_calibrated, y))\n\n    initial_params = np.array([1.0, 0.0])\n    result = minimize(objective, initial_params, method='L-BFGS-B')\n    a_opt, b_opt = result.x\n    return a_opt, b_opt\n\ndef fit_prob_space_calibration(p, y):\n    \"\"\"Fits probability-space calibration parameters (u, v) by minimizing NLL.\"\"\"\n    def objective(params):\n        u, v = params\n        d = sigmoid(u)\n        c = (1.0 - d) * sigmoid(v)\n        p_calibrated = c * p + d\n        return np.mean(compute_nll(p_calibrated, y))\n\n    initial_params = np.array([0.0, 0.0])\n    result = minimize(objective, initial_params, method='L-BFGS-B')\n    u_opt, v_opt = result.x\n    \n    d_opt = sigmoid(u_opt)\n    c_opt = (1.0 - d_opt) * sigmoid(v_opt)\n    return c_opt, d_opt\n\ndef solve():\n    \"\"\"Main function to run the full analysis.\"\"\"\n    test_cases = [\n        (np.array([-2.0, -0.5, 0.0, 0.5, 1.5, -1.2, 2.3, -2.5, 0.8, -0.8]),\n         np.array([0, 0, 1, 1, 1, 0, 1, 0, 1, 0])),\n        (np.array([5.0, -5.0, 4.0, -4.0, 6.0, -6.0, 3.5, -3.5]),\n         np.array([1, 0, 0, 1, 1, 0, 0, 1])),\n        (np.array([-0.05, 0.07, -0.12, 0.15, -0.2, 0.22, -0.3, 0.35, -0.4, 0.45]),\n         np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1]))\n    ]\n\n    all_results = []\n    \n    for z, y in test_cases:\n        # 1. Raw probabilities and their metrics\n        p_raw = sigmoid(z)\n        ece_raw = compute_ece(p_raw, y)\n        h_raw = np.mean(predictive_entropy(p_raw))\n\n        # 2. Logit-space calibration (Platt)\n        a_opt, b_opt = fit_platt_scaling(z, y)\n        p_platt = sigmoid(a_opt * z + b_opt)\n        ece_platt = compute_ece(p_platt, y)\n        h_platt = np.mean(predictive_entropy(p_platt))\n        \n        # 3. Probability-space calibration\n        c_opt, d_opt = fit_prob_space_calibration(p_raw, y)\n        p_prob = c_opt * p_raw + d_opt\n        ece_prob = compute_ece(p_prob, y)\n        h_prob = np.mean(predictive_entropy(p_prob))\n        \n        # 4. Aggregate results for the dataset\n        case_results = [\n            ece_raw, ece_platt, ece_prob,\n            h_raw, h_platt, h_prob\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified\n    inner_strs = [f\"[{','.join(map(str, sublist))}]\" for sublist in all_results]\n    print(f\"[{','.join(inner_strs)}]\")\n\nsolve()\n\n```", "id": "3179715"}, {"introduction": "Our final practice extends these concepts to the more complex setting of ordinal regression, where the goal is to predict an ordered category. Here, you will work with a Bayesian model that explicitly separates two key types of uncertainty: epistemic (model uncertainty) and aleatoric (data uncertainty). This exercise will challenge you to adapt the idea of calibration to cumulative probabilities across ordinal thresholds, providing a deeper and more nuanced perspective on uncertainty quantification. [@problem_id:3179750]", "problem": "You will model ordinal regression using a cumulative link formulation under a latent Gaussian score with both epistemic and aleatoric uncertainty. Consider a model with a latent score defined by $z = \\mathbf{x}^{\\top}\\mathbf{w} + \\varepsilon$, where $\\mathbf{x} \\in \\mathbb{R}^{d}$ is a fixed feature vector, $\\mathbf{w} \\in \\mathbb{R}^{d}$ is a random weight vector with a Gaussian posterior, and $\\varepsilon$ is independent Gaussian noise. The ordinal label $y \\in \\{1,2,\\dots,K\\}$ is obtained by thresholding the latent $z$ via unknown but fixed cut-points $-\\infty = \\tau_{0} < \\tau_{1} < \\dots < \\tau_{K-1} < \\tau_{K} = +\\infty$ using the rule $y = c$ if and only if $\\tau_{c-1} < z \\le \\tau_{c}$. The cumulative link probability for threshold $k$ is defined as $p(y \\le k \\mid \\mathbf{x})$.\n\nBase assumptions and definitions you must use:\n- The posterior over weights is Gaussian: $\\mathbf{w} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{w}, \\boldsymbol{\\Sigma}_{w})$ with known mean $\\boldsymbol{\\mu}_{w}$ and covariance $\\boldsymbol{\\Sigma}_{w}$.\n- The aleatoric noise is $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^{2})$, independent of $\\mathbf{w}$.\n- For a fixed $\\mathbf{x}$, the linear score $s = \\mathbf{x}^{\\top}\\mathbf{w}$ is a linear functional of a Gaussian random vector.\n- Under the probit cumulative link, the cumulative probability at threshold $k$ is given by the Gaussian cumulative distribution function evaluated at an appropriately standardized argument with respect to the predictive distribution of $z$ given $\\mathbf{x}$.\n\nYour tasks:\n1) Starting from the fundamental facts that any linear transformation of a multivariate Gaussian is Gaussian, and that the variance of a sum of independent random variables is the sum of their variances, derive the predictive variance of the latent score $s = \\mathbf{x}^{\\top}\\mathbf{w}$ and of the latent $z = s + \\varepsilon$ for a given $\\mathbf{x}$. Do not assume any shortcut formula; begin from these base definitions.\n2) Using the derived predictive distribution for $z \\mid \\mathbf{x}$, express the cumulative probability $p(y \\le k \\mid \\mathbf{x})$ under the probit link in terms of the Gaussian cumulative distribution function and the predictive mean and variance of $z \\mid \\mathbf{x}$.\n3) Define the calibration of the cumulative events across ordinal thresholds via the Expected Calibration Error (ECE). For each threshold $k \\in \\{1,2,\\dots,K-1\\}$, consider the binary event $b = \\mathbb{I}[y \\le k]$, with predicted probability $p = p(y \\le k \\mid \\mathbf{x})$. Given $N$ samples $(\\mathbf{x}_{i}, y_{i})$, with corresponding $(p_{i}, b_{i})$ for a fixed $k$, partition the unit interval $[0,1]$ into $B$ equal-width bins with edges $0 = e_{0} < e_{1} < \\dots < e_{B} = 1$, where $e_{j} = j/B$. Let $\\mathcal{I}_{j} = \\{ i : e_{j-1} \\le p_{i} < e_{j} \\}$ for $j \\in \\{1,2,\\dots,B-1\\}$ and $\\mathcal{I}_{B} = \\{ i : e_{B-1} \\le p_{i} \\le e_{B} \\}$ so that probability $p = 1$ is included. Define, for each nonempty bin, the empirical accuracy $\\mathrm{acc}_{j} = \\frac{1}{|\\mathcal{I}_{j}|}\\sum_{i \\in \\mathcal{I}_{j}} b_{i}$ and the empirical confidence $\\mathrm{conf}_{j} = \\frac{1}{|\\mathcal{I}_{j}|}\\sum_{i \\in \\mathcal{I}_{j}} p_{i}$. The Expected Calibration Error at threshold $k$ is\n$$\n\\mathrm{ECE}_{k} = \\sum_{j=1}^{B} \\frac{|\\mathcal{I}_{j}|}{N} \\left| \\mathrm{acc}_{j} - \\mathrm{conf}_{j} \\right|,\n$$\nwith empty bins contributing $0$ to the sum. Aggregate across thresholds by the macro-average $\\overline{\\mathrm{ECE}} = \\frac{1}{K-1}\\sum_{k=1}^{K-1} \\mathrm{ECE}_{k}$ and quantify threshold-variability via the range $\\Delta \\mathrm{ECE} = \\max_{k} \\mathrm{ECE}_{k} - \\min_{k} \\mathrm{ECE}_{k}$.\n4) Implement a program that, for each test case provided below, computes: (i) the predictive variance of $z$ at a designated query feature $\\mathbf{x}_{\\star}$, (ii) the macro-average $\\overline{\\mathrm{ECE}}$ across thresholds on the provided dataset, and (iii) the range $\\Delta \\mathrm{ECE}$ across thresholds on the provided dataset. Use $B = 5$ bins.\n\nUse only the values provided in the test suite. Angles are not involved. All numerical outputs must be real numbers and should be rounded to exactly $6$ decimal places.\n\nTest suite:\n- Case A:\n  - Dimension $d = 2$.\n  - Posterior mean $\\boldsymbol{\\mu}_{w} = \\begin{bmatrix} 0.8 \\\\ -0.3 \\end{bmatrix}$.\n  - Posterior covariance $\\boldsymbol{\\Sigma}_{w} = \\begin{bmatrix} 0.10 & 0.02 \\\\ 0.02 & 0.05 \\end{bmatrix}$.\n  - Aleatoric variance $\\sigma_{\\varepsilon}^{2} = 0.20$.\n  - Thresholds for $K = 3$: $\\tau_{1} = -0.5$, $\\tau_{2} = 0.7$.\n  - Query feature $\\mathbf{x}_{\\star} = \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}$.\n  - Dataset of $N = 8$ pairs $(\\mathbf{x}_{i}, y_{i})$:\n    - $(\\mathbf{x}_{1}, y_{1}) = \\left( \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix}, 1 \\right)$,\n    - $(\\mathbf{x}_{2}, y_{2}) = \\left( \\begin{bmatrix} 1.5 \\\\ 0.3 \\end{bmatrix}, 3 \\right)$,\n    - $(\\mathbf{x}_{3}, y_{3}) = \\left( \\begin{bmatrix} -0.7 \\\\ 0.8 \\end{bmatrix}, 1 \\right)$,\n    - $(\\mathbf{x}_{4}, y_{4}) = \\left( \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}, 2 \\right)$,\n    - $(\\mathbf{x}_{5}, y_{5}) = \\left( \\begin{bmatrix} 0.9 \\\\ -1.1 \\end{bmatrix}, 2 \\right)$,\n    - $(\\mathbf{x}_{6}, y_{6}) = \\left( \\begin{bmatrix} -1.2 \\\\ 0.4 \\end{bmatrix}, 1 \\right)$,\n    - $(\\mathbf{x}_{7}, y_{7}) = \\left( \\begin{bmatrix} 0.4 \\\\ 0.6 \\end{bmatrix}, 2 \\right)$,\n    - $(\\mathbf{x}_{8}, y_{8}) = \\left( \\begin{bmatrix} 1.1 \\\\ -0.2 \\end{bmatrix}, 3 \\right)$.\n- Case B:\n  - Dimension $d = 2$.\n  - Posterior mean $\\boldsymbol{\\mu}_{w} = \\begin{bmatrix} 0.3 \\\\ 0.2 \\end{bmatrix}$.\n  - Posterior covariance $\\boldsymbol{\\Sigma}_{w} = \\begin{bmatrix} 0.0 & 0.0 \\\\ 0.0 & 0.0 \\end{bmatrix}$.\n  - Aleatoric variance $\\sigma_{\\varepsilon}^{2} = 0.10$.\n  - Thresholds for $K = 3$: $\\tau_{1} = -0.1$, $\\tau_{2} = 0.4$.\n  - Query feature $\\mathbf{x}_{\\star} = \\begin{bmatrix} -0.5 \\\\ 2.0 \\end{bmatrix}$.\n  - Dataset of $N = 8$ pairs $(\\mathbf{x}_{i}, y_{i})$:\n    - $(\\mathbf{x}_{1}, y_{1}) = \\left( \\begin{bmatrix} -0.2 \\\\ 0.1 \\end{bmatrix}, 2 \\right)$,\n    - $(\\mathbf{x}_{2}, y_{2}) = \\left( \\begin{bmatrix} 0.5 \\\\ -0.3 \\end{bmatrix}, 2 \\right)$,\n    - $(\\mathbf{x}_{3}, y_{3}) = \\left( \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}, 3 \\right)$,\n    - $(\\mathbf{x}_{4}, y_{4}) = \\left( \\begin{bmatrix} -1.0 \\\\ 0.0 \\end{bmatrix}, 1 \\right)$,\n    - $(\\mathbf{x}_{5}, y_{5}) = \\left( \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}, 3 \\right)$,\n    - $(\\mathbf{x}_{6}, y_{6}) = \\left( \\begin{bmatrix} 0.2 \\\\ 0.2 \\end{bmatrix}, 2 \\right)$,\n    - $(\\mathbf{x}_{7}, y_{7}) = \\left( \\begin{bmatrix} -0.6 \\\\ 0.4 \\end{bmatrix}, 1 \\right)$,\n    - $(\\mathbf{x}_{8}, y_{8}) = \\left( \\begin{bmatrix} 0.8 \\\\ 0.0 \\end{bmatrix}, 3 \\right)$.\n- Case C:\n  - Dimension $d = 3$.\n  - Posterior mean $\\boldsymbol{\\mu}_{w} = \\begin{bmatrix} 0.5 \\\\ -0.2 \\\\ 0.1 \\end{bmatrix}$.\n  - Posterior covariance $\\boldsymbol{\\Sigma}_{w} = \\begin{bmatrix} 0.20 & 0.00 & 0.05 \\\\ 0.00 & 0.10 & -0.02 \\\\ 0.05 & -0.02 & 0.15 \\end{bmatrix}$.\n  - Aleatoric variance $\\sigma_{\\varepsilon}^{2} = 0.30$.\n  - Thresholds for $K = 4$: $\\tau_{1} = -0.8$, $\\tau_{2} = 0.2$, $\\tau_{3} = 1.1$.\n  - Query feature $\\mathbf{x}_{\\star} = \\begin{bmatrix} 0.3 \\\\ -0.7 \\\\ 1.2 \\end{bmatrix}$.\n  - Dataset of $N = 10$ pairs $(\\mathbf{x}_{i}, y_{i})$:\n    - $(\\mathbf{x}_{1}, y_{1}) = \\left( \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}, 2 \\right)$,\n    - $(\\mathbf{x}_{2}, y_{2}) = \\left( \\begin{bmatrix} 1.0 \\\\ -1.0 \\\\ 0.5 \\end{bmatrix}, 3 \\right)$,\n    - $(\\mathbf{x}_{3}, y_{3}) = \\left( \\begin{bmatrix} -0.5 \\\\ 1.5 \\\\ -0.7 \\end{bmatrix}, 1 \\right)$,\n    - $(\\mathbf{x}_{4}, y_{4}) = \\left( \\begin{bmatrix} 0.2 \\\\ -0.2 \\\\ 0.2 \\end{bmatrix}, 2 \\right)$,\n    - $(\\mathbf{x}_{5}, y_{5}) = \\left( \\begin{bmatrix} 0.8 \\\\ 0.3 \\\\ -0.1 \\end{bmatrix}, 3 \\right)$,\n    - $(\\mathbf{x}_{6}, y_{6}) = \\left( \\begin{bmatrix} -1.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}, 1 \\right)$,\n    - $(\\mathbf{x}_{7}, y_{7}) = \\left( \\begin{bmatrix} 0.5 \\\\ -0.6 \\\\ 1.0 \\end{bmatrix}, 4 \\right)$,\n    - $(\\mathbf{x}_{8}, y_{8}) = \\left( \\begin{bmatrix} 1.2 \\\\ 0.0 \\\\ 0.8 \\end{bmatrix}, 4 \\right)$,\n    - $(\\mathbf{x}_{9}, y_{9}) = \\left( \\begin{bmatrix} -0.8 \\\\ -0.8 \\\\ 0.8 \\end{bmatrix}, 2 \\right)$,\n    - $(\\mathbf{x}_{10}, y_{10}) = \\left( \\begin{bmatrix} 0.6 \\\\ 1.2 \\\\ -0.4 \\end{bmatrix}, 3 \\right)$.\n\nProgram requirements:\n- For each case, compute the predictive variance of $z$ at $\\mathbf{x}_{\\star}$, the macro-average $\\overline{\\mathrm{ECE}}$ across thresholds on the case’s dataset, and the range $\\Delta \\mathrm{ECE}$ across thresholds on the case’s dataset, using $B = 5$ bins.\n- Your program should produce a single line of output containing all results as a comma-separated list enclosed in square brackets, in the following order:\n  - Case A: predictive variance of $z$ at $\\mathbf{x}_{\\star}$, $\\overline{\\mathrm{ECE}}$, $\\Delta \\mathrm{ECE}$;\n  - Case B: predictive variance of $z$ at $\\mathbf{x}_{\\star}$, $\\overline{\\mathrm{ECE}}$, $\\Delta \\mathrm{ECE}$;\n  - Case C: predictive variance of $z$ at $\\mathbf{x}_{\\star}$, $\\overline{\\mathrm{ECE}}$, $\\Delta \\mathrm{ECE}$.\n- Round every printed number to exactly $6$ decimal places, with no additional text.\n\nYour final program must be complete and runnable as is, require no user input, and use only the standard Python library together with Numerical Python (NumPy) and Scientific Python (SciPy) in the specified versions.", "solution": "The user's problem statement is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n- **Model Definition**: Latent score $z = \\mathbf{x}^{\\top}\\mathbf{w} + \\varepsilon$.\n- **Feature Vector**: $\\mathbf{x} \\in \\mathbb{R}^{d}$ (fixed).\n- **Weight Vector**: $\\mathbf{w} \\in \\mathbb{R}^{d}$ (random).\n- **Noise Term**: $\\varepsilon$ (independent Gaussian noise).\n- **Ordinal Label**: $y \\in \\{1, 2, \\dots, K\\}$.\n- **Cut-points**: $-\\infty = \\tau_{0} < \\tau_{1} < \\dots < \\tau_{K-1} < \\tau_{K} = +\\infty$ (fixed, unknown). The test cases provide specific values.\n- **Labeling Rule**: $y = c \\iff \\tau_{c-1} < z \\le \\tau_{c}$.\n- **Cumulative Probability**: $p(y \\le k \\mid \\mathbf{x})$.\n- **Weight Posterior**: $\\mathbf{w} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{w}, \\boldsymbol{\\Sigma}_{w})$.\n- **Noise Distribution**: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^{2})$.\n- **Independence Assumption**: $\\varepsilon$ is independent of $\\mathbf{w}$.\n- **Linear Score**: $s = \\mathbf{x}^{\\top}\\mathbf{w}$.\n- **Probit Link Model**: The cumulative probability is given by the Gaussian cumulative distribution function (CDF).\n- **Expected Calibration Error (ECE)**:\n  - Binary event for a threshold $k$: $b = \\mathbb{I}[y \\le k]$.\n  - Predicted probability: $p = p(y \\le k \\mid \\mathbf{x})$.\n  - Bins: $B$ equal-width bins on $[0,1]$ with edges $e_j = j/B$. $\\mathcal{I}_{j} = \\{ i : e_{j-1} \\le p_{i} < e_{j} \\}$ for $j < B$, and $\\mathcal{I}_{B} = \\{ i : e_{B-1} \\le p_{i} \\le e_{B} \\}$.\n  - Bin accuracy: $\\mathrm{acc}_{j} = \\frac{1}{|\\mathcal{I}_{j}|}\\sum_{i \\in \\mathcal{I}_{j}} b_{i}$.\n  - Bin confidence: $\\mathrm{conf}_{j} = \\frac{1}{|\\mathcal{I}_{j}|}\\sum_{i \\in \\mathcal{I}_{j}} p_{i}$.\n  - ECE at threshold $k$: $\\mathrm{ECE}_{k} = \\sum_{j=1}^{B} \\frac{|\\mathcal{I}_{j}|}{N} \\left| \\mathrm{acc}_{j} - \\mathrm{conf}_{j} \\right|$.\n- **Aggregate Metrics**:\n  - Macro-average ECE: $\\overline{\\mathrm{ECE}} = \\frac{1}{K-1}\\sum_{k=1}^{K-1} \\mathrm{ECE}_{k}$.\n  - Range of ECE: $\\Delta \\mathrm{ECE} = \\max_{k} \\mathrm{ECE}_{k} - \\min_{k} \\mathrm{ECE}_{k}$.\n- **Specific Parameters**: $B=5$ bins. Three test cases (A, B, C) are provided with all necessary numerical values for $d, \\boldsymbol{\\mu}_w, \\boldsymbol{\\Sigma}_w, \\sigma_\\varepsilon^2, K, \\tau_k, \\mathbf{x}_\\star$, and a dataset $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem describes a standard Bayesian ordinal regression model with a probit link function. The statistical concepts, including the properties of multivariate Gaussian distributions, independence of random variables, and the definition of Expected Calibration Error, are all well-established and correctly stated. The model is a fundamental building block in Bayesian statistics and machine learning.\n- **Well-Posed**: The problem is fully specified. For each task, all necessary numerical values, definitions, and datasets are provided. The objective is to compute specific numerical quantities based on these givens, which leads to a unique, stable, and meaningful solution.\n- **Objective**: The problem is formulated in precise mathematical language, free from ambiguity, subjectivity, or opinion.\n\nThe problem does not exhibit any of the listed flaws:\n1.  **Scientific Unsoundness**: The physics and mathematics are sound. The properties of Gaussian variables under linear transformation and summation are correctly invoked.\n2.  **Non-Formalizable**: The problem is entirely formal and quantitative.\n3.  **Incomplete or Contradictory**: The setup is complete and self-consistent for each test case.\n4.  **Unrealistic or Infeasible**: The parameters and data are numerically reasonable and do not violate physical or mathematical constraints. The covariance matrices are positive semi-definite as required.\n5.  **Ill-Posed**: The problem is well-posed, with a clear path to a unique solution.\n6.  **Trivial or Tautological**: The problem requires non-trivial derivations and a careful, multi-step implementation, testing the understanding of both statistical theory and its computational application.\n7.  **Outside Scientific Verifiability**: The results are computationally deterministic and can be independently verified.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution is provided below.\n\n***\n\n### Part 1: Derivation of Predictive Mean and Variance\nWe are tasked with deriving the predictive distribution of the latent score $z = \\mathbf{x}^{\\top}\\mathbf{w} + \\varepsilon$ for a given feature vector $\\mathbf{x}$. This derivation proceeds in two steps, starting from the base definitions.\n\nFirst, consider the linear score $s = \\mathbf{x}^{\\top}\\mathbf{w}$. This is a linear transformation of the multivariate Gaussian random vector $\\mathbf{w} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{w}, \\boldsymbol{\\Sigma}_{w})$. For any random vector $\\mathbf{y}$ with mean $\\mathbb{E}[\\mathbf{y}] = \\boldsymbol{\\mu}$ and covariance $\\mathrm{Cov}(\\mathbf{y}) = \\boldsymbol{\\Sigma}$, a linear transformation $\\mathbf{A}\\mathbf{y}$ results in a random vector with mean $\\mathbf{A}\\boldsymbol{\\mu}$ and covariance $\\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}^{\\top}$.\nFor the scalar score $s$, the transformation matrix is $\\mathbf{A} = \\mathbf{x}^{\\top}$.\n\nThe mean of $s$ is:\n$$ \\mu_s = \\mathbb{E}[s] = \\mathbb{E}[\\mathbf{x}^{\\top}\\mathbf{w}] = \\mathbf{x}^{\\top}\\mathbb{E}[\\mathbf{w}] = \\mathbf{x}^{\\top}\\boldsymbol{\\mu}_{w} $$\n\nThe variance of $s$ (which represents the epistemic uncertainty) is:\n$$ \\sigma_s^2 = \\mathrm{Var}(s) = \\mathrm{Var}(\\mathbf{x}^{\\top}\\mathbf{w}) = (\\mathbf{x}^{\\top})\\mathrm{Cov}(\\mathbf{w})(\\mathbf{x}^{\\top})^{\\top} = \\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}_{w}\\mathbf{x} $$\nSince a linear transformation of a Gaussian is Gaussian, $s \\mid \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{x}^{\\top}\\boldsymbol{\\mu}_{w}, \\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}_{w}\\mathbf{x})$.\n\nSecond, consider the full latent score $z = s + \\varepsilon$. We are given that $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^{2})$ and that $\\varepsilon$ is independent of $\\mathbf{w}$. Since $s$ is a function of $\\mathbf{w}$, $s$ and $\\varepsilon$ are also independent.\n\nThe mean of $z$ is the sum of the means:\n$$ \\mu_z = \\mathbb{E}[z] = \\mathbb{E}[s + \\varepsilon] = \\mathbb{E}[s] + \\mathbb{E}[\\varepsilon] = \\mu_s + 0 = \\mathbf{x}^{\\top}\\boldsymbol{\\mu}_{w} $$\n\nThe variance of $z$ is the sum of the variances of the independent variables $s$ and $\\varepsilon$:\n$$ \\sigma_z^2 = \\mathrm{Var}(z) = \\mathrm{Var}(s + \\varepsilon) = \\mathrm{Var}(s) + \\mathrm{Var}(\\varepsilon) = \\sigma_s^2 + \\sigma_\\varepsilon^2 = \\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}_{w}\\mathbf{x} + \\sigma_{\\varepsilon}^{2} $$\nThe total variance $\\sigma_z^2$ combines the epistemic uncertainty from the weights ($\\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}_{w}\\mathbf{x}$) and the aleatoric uncertainty from the noise term ($\\sigma_{\\varepsilon}^{2}$). As the sum of two independent Gaussian variables, $z$ is also Gaussian.\n\nTherefore, the predictive distribution of the latent score $z$ for a given $\\mathbf{x}$ is:\n$$ z \\mid \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{x}^{\\top}\\boldsymbol{\\mu}_{w}, \\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}_{w}\\mathbf{x} + \\sigma_{\\varepsilon}^{2}) $$\n\n### Part 2: Expression for the Cumulative Probability\nThe ordinal model defines the label $y=c$ if and only if $\\tau_{c-1} < z \\le \\tau_c$. The cumulative event $y \\le k$ is thus equivalent to the event $z \\le \\tau_k$. The cumulative probability is:\n$$ p(y \\le k \\mid \\mathbf{x}) = p(z \\le \\tau_k \\mid \\mathbf{x}) $$\nGiven that $z \\mid \\mathbf{x} \\sim \\mathcal{N}(\\mu_z, \\sigma_z^2)$, this probability is calculated using the cumulative distribution function (CDF) of the normal distribution. If we let $\\Phi(\\cdot)$ denote the CDF of the standard normal distribution $\\mathcal{N}(0, 1)$, the probability is found by standardizing the variable:\n$$ p(z \\le \\tau_k \\mid \\mathbf{x}) = \\Phi\\left(\\frac{\\tau_k - \\mu_z}{\\sigma_z}\\right) $$\nSubstituting the expressions for $\\mu_z$ and $\\sigma_z$ from Part 1 gives the final form for the cumulative probability under the probit link:\n$$ p(y \\le k \\mid \\mathbf{x}) = \\Phi\\left(\\frac{\\tau_k - \\mathbf{x}^{\\top}\\boldsymbol{\\mu}_{w}}{\\sqrt{\\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}_{w}\\mathbf{x} + \\sigma_{\\varepsilon}^{2}}}\\right) $$\n\n### Part 3: Expected Calibration Error (ECE) Calculation\nThe procedure for calculating the ECE, macro-average ECE, and range of ECE is explicitly defined in the problem statement. For implementation, we follow these steps:\n1.  Iterate through each relevant ordinal threshold $k \\in \\{1, 2, \\dots, K-1\\}$.\n2.  For each threshold $k$, we transform the multiclass problem into a binary one.\n    - For each data point $(\\mathbf{x}_i, y_i)$, calculate the predicted probability $p_i = p(y_i \\le k \\mid \\mathbf{x}_i)$ using the formula from Part 2.\n    - Determine the true binary outcome $b_i = \\mathbb{I}[y_i \\le k]$, which is $1$ if $y_i \\le k$ and $0$ otherwise.\n3.  With the set of pairs $\\{(p_i, b_i)\\}_{i=1}^N$, we compute $\\mathrm{ECE}_k$.\n    - Partition the predictions $\\{p_i\\}$ into $B=5$ bins defined by edges $[0.0, 0.2)$, $[0.2, 0.4)$, $[0.4, 0.6)$, $[0.6, 0.8)$, and $[0.8, 1.0]$. The last bin is inclusive of $1.0$.\n    - For each non-empty bin $j$, calculate the number of points $|\\mathcal{I}_j|$, the average accuracy $\\mathrm{acc}_j = \\frac{1}{|\\mathcal{I}_j|}\\sum_{i \\in \\mathcal{I}_j} b_i$, and the average confidence $\\mathrm{conf}_j = \\frac{1}{|\\mathcal{I}_j|}\\sum_{i \\in \\mathcal{I}_j} p_i$.\n    - Compute $\\mathrm{ECE}_k = \\sum_{j=1}^{B} \\frac{|\\mathcal{I}_j|}{N} |\\mathrm{acc}_j - \\mathrm{conf}_j|$.\n4.  After computing $\\mathrm{ECE}_k$ for all $k$, calculate the aggregate statistics:\n    - $\\overline{\\mathrm{ECE}} = \\frac{1}{K-1}\\sum_{k=1}^{K-1} \\mathrm{ECE}_k$.\n    - $\\Delta \\mathrm{ECE} = \\max_{k} \\mathrm{ECE}_k - \\min_{k} \\mathrm{ECE}_k$.\n\n### Part 4: Implementation\nThe following program implements the calculations for the provided test cases. For each case, it computes:\n- The total predictive variance $\\sigma_z^2 = \\mathbf{x}_{\\star}^{\\top}\\boldsymbol{\\Sigma}_{w}\\mathbf{x}_{\\star} + \\sigma_{\\varepsilon}^{2}$ for the query feature $\\mathbf{x}_{\\star}$.\n- The macro-average ECE, $\\overline{\\mathrm{ECE}}$.\n- The ECE range, $\\Delta \\mathrm{ECE}$.\n\nThe calculations rely on `numpy` for linear algebra and `scipy.stats.norm.cdf` for the standard normal CDF, $\\Phi(\\cdot)$. All results are rounded to $6$ decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\n# language: Python\n# version: 3.12\n# libraries:\n#     - name: numpy, version: 1.23.5\n#     - name: scipy, version: 1.11.4\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {\n            \"d\": 2,\n            \"mu_w\": np.array([0.8, -0.3]),\n            \"Sigma_w\": np.array([[0.10, 0.02], [0.02, 0.05]]),\n            \"sigma_eps_sq\": 0.20,\n            \"K\": 3,\n            \"taus\": {-1: -np.inf, 0: -np.inf, 1: -0.5, 2: 0.7, 3: np.inf},\n            \"x_star\": np.array([1.0, -0.5]),\n            \"dataset_X\": np.array([\n                [0.2, -0.1], [1.5, 0.3], [-0.7, 0.8], [0.0, 0.0],\n                [0.9, -1.1], [-1.2, 0.4], [0.4, 0.6], [1.1, -0.2]\n            ]),\n            \"dataset_y\": np.array([1, 3, 1, 2, 2, 1, 2, 3]),\n        },\n        # Case B\n        {\n            \"d\": 2,\n            \"mu_w\": np.array([0.3, 0.2]),\n            \"Sigma_w\": np.array([[0.0, 0.0], [0.0, 0.0]]),\n            \"sigma_eps_sq\": 0.10,\n            \"K\": 3,\n            \"taus\": {-1: -np.inf, 0: -np.inf, 1: -0.1, 2: 0.4, 3: np.inf},\n            \"x_star\": np.array([-0.5, 2.0]),\n            \"dataset_X\": np.array([\n                [-0.2, 0.1], [0.5, -0.3], [0.0, 1.0], [-1.0, 0.0],\n                [1.0, 1.0], [0.2, 0.2], [-0.6, 0.4], [0.8, 0.0]\n            ]),\n            \"dataset_y\": np.array([2, 2, 3, 1, 3, 2, 1, 3]),\n        },\n        # Case C\n        {\n            \"d\": 3,\n            \"mu_w\": np.array([0.5, -0.2, 0.1]),\n            \"Sigma_w\": np.array([[0.20, 0.00, 0.05], [0.00, 0.10, -0.02], [0.05, -0.02, 0.15]]),\n            \"sigma_eps_sq\": 0.30,\n            \"K\": 4,\n            \"taus\": {-1: -np.inf, 0: -np.inf, 1: -0.8, 2: 0.2, 3: 1.1, 4: np.inf},\n            \"x_star\": np.array([0.3, -0.7, 1.2]),\n            \"dataset_X\": np.array([\n                [0.0, 0.0, 0.0], [1.0, -1.0, 0.5], [-0.5, 1.5, -0.7],\n                [0.2, -0.2, 0.2], [0.8, 0.3, -0.1], [-1.5, 0.5, 0.5],\n                [0.5, -0.6, 1.0], [1.2, 0.0, 0.8], [-0.8, -0.8, 0.8],\n                [0.6, 1.2, -0.4]\n            ]),\n            \"dataset_y\": np.array([2, 3, 1, 2, 3, 1, 4, 4, 2, 3]),\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        mu_w = case[\"mu_w\"]\n        Sigma_w = case[\"Sigma_w\"]\n        sigma_eps_sq = case[\"sigma_eps_sq\"]\n        K = case[\"K\"]\n        taus = case[\"taus\"]\n        x_star = case[\"x_star\"]\n        X, y = case[\"dataset_X\"], case[\"dataset_y\"]\n        N = len(y)\n        B = 5\n\n        # 1. Compute predictive variance of z at x_star\n        epistemic_var_star = x_star.T @ Sigma_w @ x_star\n        pred_var_z_star = epistemic_var_star + sigma_eps_sq\n\n        # 2. Compute ECE metrics\n        all_ece_k = []\n        for k in range(1, K):\n            tau_k = taus[k]\n            \n            p_values = []\n            b_values = []\n            \n            for i in range(N):\n                x_i = X[i]\n                y_i = y[i]\n                \n                mu_z_i = x_i.T @ mu_w\n                var_z_i = x_i.T @ Sigma_w @ x_i + sigma_eps_sq\n                std_z_i = np.sqrt(var_z_i)\n                \n                # Handle case where variance is zero\n                if std_z_i == 0:\n                    p_i = 1.0 if tau_k > mu_z_i else 0.0\n                else:\n                    p_i = norm.cdf((tau_k - mu_z_i) / std_z_i)\n                    \n                b_i = 1.0 if y_i <= k else 0.0\n                \n                p_values.append(p_i)\n                b_values.append(b_i)\n                \n            p_values = np.array(p_values)\n            b_values = np.array(b_values)\n            \n            # Calculate ECE_k\n            ece_k = 0.0\n            bin_edges = np.linspace(0, 1, B + 1)\n            \n            for j in range(B):\n                lower_bound = bin_edges[j]\n                upper_bound = bin_edges[j+1]\n                \n                # The last bin is inclusive of 1.0\n                if j == B - 1:\n                    in_bin_mask = (p_values >= lower_bound) & (p_values <= upper_bound)\n                else:\n                    in_bin_mask = (p_values >= lower_bound) & (p_values < upper_bound)\n                \n                bin_size = np.sum(in_bin_mask)\n                \n                if bin_size > 0:\n                    acc_j = np.mean(b_values[in_bin_mask])\n                    conf_j = np.mean(p_values[in_bin_mask])\n                    ece_k += (bin_size / N) * np.abs(acc_j - conf_j)\n            \n            all_ece_k.append(ece_k)\n        \n        all_ece_k = np.array(all_ece_k)\n        macro_ece = np.mean(all_ece_k)\n        \n        # Handle case where K=2, so only 1 threshold and range is 0\n        if len(all_ece_k) > 1:\n            delta_ece = np.max(all_ece_k) - np.min(all_ece_k)\n        else:\n            delta_ece = 0.0\n            \n        all_results.extend([pred_var_z_star, macro_ece, delta_ece])\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in all_results])}]\")\n\nsolve()\n\n```", "id": "3179750"}]}