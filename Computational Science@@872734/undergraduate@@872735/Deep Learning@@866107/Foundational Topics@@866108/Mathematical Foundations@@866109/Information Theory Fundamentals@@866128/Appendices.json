{"hands_on_practices": [{"introduction": "In deep learning, data augmentation is a powerful technique to improve generalization. However, not all augmentations are created equal. This exercise will help you build a crucial intuition: increasing the complexity or entropy of the input, $H(X)$, does not automatically increase the amount of useful, task-relevant information, $I(X;Y)$. Through a hands-on coding experiment, you will demonstrate cases where an augmentation adds only task-irrelevant randomness, and other cases where it can even corrupt the predictive signal, providing a clear distinction between total information and valuable information [@problem_id:3138094].", "problem": "You are given a directive to demonstrate, using synthetic discrete experiments, that augmentations which increase input entropy $H(X)$ need not increase the mutual information $I(X;Y)$ between inputs and labels. Your task is to construct a complete program that generates data from precisely defined discrete distributions, applies specified augmentations, and computes empirical estimates of Shannon entropy and mutual information. The goal is to separate randomness from task-relevant information using fundamental definitions from information theory.\n\nUse the following foundational base:\n- Shannon entropy $H(X)$ for a discrete random variable $X$ with empirical probabilities $p(x)$ is defined by $H(X) = -\\sum_{x} p(x) \\log_2 p(x)$.\n- Mutual Information (MI) $I(X;Y)$ between discrete random variables $X$ and $Y$ with empirical joint probabilities $p(x,y)$ and marginals $p(x)$ and $p(y)$ is defined by $I(X;Y) = \\sum_{x,y} p(x,y)\\log_2 \\frac{p(x,y)}{p(x)p(y)}$.\n\nAll variables in this problem are discrete and take finitely many values. You will approximate $H(\\cdot)$ and $I(\\cdot\\,;\\cdot)$ using empirical frequencies from Monte Carlo samples of size $n$.\n\nExperimental setup:\n- Let $Y \\in \\{0,1\\}$ be a binary label with $Y \\sim \\text{Bernoulli}(q)$, where $q = \\mathbb{P}(Y=1)$.\n- Let $Z = Y$ represent a task-relevant binary feature.\n- Augmentations introduce additional randomness that is either independent of $Y$ or label-corrupting noise applied to the observed feature.\n- For composite inputs, treat $X$ as a single discrete variable by encoding tuples $(x_1, x_2)$ into an integer category to compute discrete $H(X)$ and $I(X;Y)$ via the definitions above.\n\nYour program must implement empirical estimators for $H(X)$ and $I(X;Y)$ from samples and evaluate the boolean predicate\n$$b = \\left[H\\!\\left(X_{\\text{aug}}\\right) > H\\!\\left(X_{\\text{base}}\\right)\\right] \\wedge \\left[I\\!\\left(X_{\\text{aug}};Y\\right) \\le I\\!\\left(X_{\\text{base}};Y\\right)\\right],$$\nwith a small numerical tolerance to account for sampling error. A true value of $b$ demonstrates that an augmentation increased $H(X)$ while not increasing task-relevant information $I(X;Y)$.\n\nTest suite (each case should be run with the stated parameters and Monte Carlo sample size $n$):\n\n- Case $1$ (happy path, independent randomness added): $q = 0.5$, $n = 200000$. Base input $X_{\\text{base}} = Z$. Augmented input $X_{\\text{aug}} = (Z, N)$ where $N \\sim \\text{Bernoulli}(0.5)$ is independent of $(Z,Y)$.\n- Case $2$ (class imbalance, independent randomness added): $q = 0.1$, $n = 200000$. Base input $X_{\\text{base}} = Z$. Augmented input $X_{\\text{aug}} = (Z, N)$ with $N \\sim \\text{Bernoulli}(0.5)$ independent.\n- Case $3$ (label-independent corrupting noise increases $H(X)$ but reduces $I(X;Y)$): $q = 0.1$, $s = 0.3$, $n = 200000$. Base input $X_{\\text{base}} = Z$. Augmented input $X_{\\text{aug}} = \\tilde{Z}$ where $\\tilde{Z} = Z \\oplus B$ and $B \\sim \\text{Bernoulli}(s)$ is independent of $(Z,Y)$; here $\\oplus$ denotes bitwise exclusive-or.\n- Case $4$ (adding task-relevant information raises $I(X;Y)$): $q = 0.1$, $s = 0.3$, $n = 200000$. Base input $X_{\\text{base}} = \\tilde{Z}$ defined as in Case $3$. Augmented input $X_{\\text{aug}} = (\\tilde{Z}, Y)$.\n- Case $5$ (boundary condition, no corruption): $q = 0.1$, $s = 0.0$, $n = 200000$. Base input $X_{\\text{base}} = Z$. Augmented input $X_{\\text{aug}} = \\tilde{Z}$ with $\\tilde{Z} = Z \\oplus B$ and $B \\sim \\text{Bernoulli}(s)$, i.e., no change when $s = 0.0$.\n\nImplementation requirements:\n- Use a reproducible pseudo-random number generator.\n- Compute empirical $H(X)$ and $I(X;Y)$ for each case directly from samples using the fundamental definitions above.\n- For each case, compute the boolean $b$ given by the predicate, using a tolerance of $10^{-3}$ bits for comparisons of $H(\\cdot)$ and $I(\\cdot\\,;\\cdot)$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\dots,result_5]$), where each $result_i$ is the boolean $b$ for Case $i$ in the order they are listed above.\n\nAll answers are dimensionless and should be reported as booleans as specified. No physical units or angles appear in this problem.", "solution": "The objective is to demonstrate through synthetic experiments that data augmentations can increase the total entropy of the input, $H(X)$, without increasing, and sometimes decreasing, the mutual information $I(X;Y)$ it shares with the target label $Y$. This distinction is fundamental to understanding which augmentations provide task-relevant information versus those that merely add task-irrelevant randomness.\n\nThe methodology involves a series of Monte Carlo simulations. For each case, we generate a large number of samples, $n$, from specified discrete probability distributions for a base input $X_{\\text{base}}$ and an augmented input $X_{\\text{aug}}$, along with a binary label $Y$. From these samples, we compute empirical estimates of Shannon entropy and mutual information to evaluate the predicate $b = \\left[H\\!\\left(X_{\\text{aug}}\\right) > H\\!\\left(X_{\\text{base}}\\right)\\right] \\wedge \\left[I\\!\\left(X_{\\text{aug}};Y\\right) \\le I\\!\\left(X_{\\text{base}};Y\\right)\\right]$. A numerical tolerance of $10^{-3}$ is used to make the comparisons robust to sampling noise.\n\nThe core of the analysis rests on the empirical estimation of two information-theoretic quantities for discrete random variables.\n\nFirst, the Shannon entropy of a variable $X$, which can take values in a set $\\mathcal{X}$, is estimated from its empirical probability mass function $p(x)$. Given $n$ samples, $p(x)$ is the frequency of occurrence of the value $x$. The entropy, measured in bits, is:\n$$H(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x)$$\nwhere the term $p(x) \\log_2 p(x)$ is taken to be $0$ if $p(x)=0$.\n\nSecond, the mutual information between two variables $X$ and $Y$ is a measure of their statistical dependence. For numerical stability and ease of implementation, we use the identity:\n$$I(X;Y) = H(X) + H(Y) - H(X,Y)$$\nwhere $H(X)$ and $H(Y)$ are the marginal entropies, and $H(X,Y)$ is the joint entropy of the pair $(X,Y)$. The joint entropy is calculated from the empirical joint probability distribution $p(x,y)$:\n$$H(X,Y) = -\\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} p(x,y) \\log_2 p(x,y)$$\nWhen an input $X$ is a composite of multiple features, such as $X=(F_1, F_2)$, it is treated as a single discrete variable by encoding the tuple of feature values into a unique integer identifier. This allows the direct application of the above formulae for discrete variables.\n\nThe analysis proceeds by evaluating the predicate $b$ for five distinct cases.\n\n**Case 1: Independent Randomness (Balanced Classes)**\n- Parameters: $q = \\mathbb{P}(Y=1) = 0.5$, $n = 200000$.\n- Base Input: $X_{\\text{base}} = Z$, where $Z=Y$. Here, the input is a perfect copy of the label.\n- Augmented Input: $X_{\\text{aug}} = (Z, N)$, where $N \\sim \\text{Bernoulli}(0.5)$ is a noise variable independent of $Y$.\n- Analysis: $I(X_{\\text{base}};Y) = I(Y;Y) = H(Y) = 1$ bit. Adding the independent noise variable $N$ increases the total entropy, since by independence $H(X_{\\text{aug}}) = H(Z,N) = H(Z) + H(N) = H(Y) + H(N) = 1 + 1 = 2$ bits. The mutual information is unchanged because $N$ is independent of $Y$: $I(X_{\\text{aug}};Y) = I((Z,N);Y) = I(Z;Y) = 1$ bit. Thus, we expect $H(X_{\\text{aug}}) > H(X_{\\text{base}})$ and $I(X_{\\text{aug}};Y) \\le I(X_{\\text{base}};Y)$. The predicate $b$ should evaluate to true.\n\n**Case 2: Independent Randomness (Imbalanced Classes)**\n- Parameters: $q = \\mathbb{P}(Y=1) = 0.1$, $n = 200000$.\n- Setup: Same as Case 1, but with an imbalanced label distribution.\n- Analysis: The reasoning is identical to Case 1. The entropy of the label is $H(Y) = -0.1\\log_2(0.1) - 0.9\\log_2(0.9) \\approx 0.469$ bits. We have $H(X_{\\text{base}}) = H(Y) \\approx 0.469$ bits and $I(X_{\\text{base}};Y) = H(Y) \\approx 0.469$ bits. The augmented entropy is $H(X_{\\text{aug}}) = H(Y) + H(N) \\approx 0.469 + 1 = 1.469$ bits. The mutual information remains $I(X_{\\text{aug}};Y) = I(Y;Y) = H(Y) \\approx 0.469$ bits. Again, we expect $H(X_{\\text{aug}}) > H(X_{\\text{base}})$ and $I(X_{\\text{aug}};Y) \\le I(X_{\\text{base}};Y)$. The predicate $b$ should be true.\n\n**Case 3: Label-Corrupting Noise**\n- Parameters: $q = 0.1$, noise level $s = 0.3$, $n = 200000$.\n- Base Input: $X_{\\text{base}} = Z = Y$.\n- Augmented Input: $X_{\\text{aug}} = \\tilde{Z} = Z \\oplus B$, where $B \\sim \\text{Bernoulli}(s)$ is independent bit-flip noise and $\\oplus$ is the exclusive-or operation. This models a binary symmetric channel.\n- Analysis: $H(X_{\\text{base}}) = H(Y) \\approx 0.469$ bits. The augmented input $\\tilde{Z}$ is a noisy version of $Y$. The probability $\\mathbb{P}(\\tilde{Z}=1) = q(1-s) + (1-q)s = 0.1(0.7) + 0.9(0.3) = 0.34$. The entropy is $H(X_{\\text{aug}}) = H_b(0.34) \\approx 0.923$ bits, which is greater than $H(X_{\\text{base}})$. The noise, however, corrupts the information about $Y$. By the data processing inequality, $I(Y;\\tilde{Z}) \\le I(Y;Z)$. Specifically, $I(X_{\\text{aug}};Y) = H(\\tilde{Z}) - H(\\tilde{Z}|Y) = H_b(0.34) - H_b(s) \\approx 0.923-0.881=0.042$ bits. This is significantly less than $I(X_{\\text{base}};Y) = H(Y) \\approx 0.469$ bits. Both conditions of the predicate are met, so $b$ should be true.\n\n**Case 4: Adding Task-Relevant Information**\n- Parameters: $q=0.1, s=0.3, n=200000$.\n- Base Input: $X_{\\text{base}} = \\tilde{Z}$, the noisy variable from Case 3.\n- Augmented Input: $X_{\\text{aug}} = (\\tilde{Z}, Y)$. The augmentation explicitly adds the true label.\n- Analysis: The base quantities are derived from Case 3: $H(X_{\\text{base}}) \\approx 0.923$ bits and $I(X_{\\text{base}};Y) \\approx 0.042$ bits. The augmented input's entropy is $H(X_{\\text{aug}}) = H(\\tilde{Z}, Y) = H(Y) + H(\\tilde{Z}|Y) = H_b(0.1) + H_b(0.3) \\approx 0.469 + 0.881 = 1.35$ bits, which is greater than $H(X_{\\text{base}})$. However, the mutual information is now $I(X_{\\text{aug}};Y) = I((\\tilde{Z},Y);Y) = H(Y) - H(Y|(\\tilde{Z},Y))$. Since $Y$ is perfectly known given the tuple $(\\tilde{Z},Y)$, the conditional entropy $H(Y|(\\tilde{Z},Y))$ is $0$. Thus, $I(X_{\\text{aug}};Y) = H(Y) \\approx 0.469$ bits. This is much larger than $I(X_{\\text{base}};Y) \\approx 0.042$ bits. The condition $I(X_{\\text{aug}};Y) \\le I(X_{\\text{base}};Y)$ is false. The predicate $b$ should be false.\n\n**Case 5: Boundary Condition (No Corruption)**\n- Parameters: $q=0.1, s=0.0, n=200000$.\n- Setup: Same as Case 3, but with the noise probability $s=0$.\n- Analysis: With $s=0$, the noise variable $B$ is always $0$. Thus, $X_{\\text{aug}} = Z \\oplus 0 = Z = X_{\\text{base}}$. The base and augmented inputs are identical. Consequently, $H(X_{\\text{aug}}) = H(X_{\\text{base}})$ and $I(X_{\\text{aug}};Y) = I(X_{\\text{base}};Y)$. The first part of the predicate, $H(X_{\\text{aug}}) > H(X_{\\text{base}})$, is a strict inequality and will be false (within the numerical tolerance). Therefore, the overall predicate $b$ must be false.\n\nThe implementation will generate samples for each case, compute the information-theoretic quantities, and evaluate the predicate $b$ to confirm these theoretical expectations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_entropy(samples):\n    \"\"\"\n    Computes the Shannon entropy of a discrete random variable from its samples.\n    \"\"\"\n    _, counts = np.unique(samples, return_counts=True)\n    probabilities = counts / len(samples)\n    # The contribution of p*log(p) is 0 for p=0.\n    # We filter to avoid log(0) errors.\n    probabilities = probabilities[probabilities > 0]\n    entropy = -np.sum(probabilities * np.log2(probabilities))\n    return entropy\n\ndef compute_info_metrics(x_samples, y_samples):\n    \"\"\"\n    Computes H(X) and I(X;Y) from samples.\n    I(X;Y) is calculated as H(X) + H(Y) - H(X,Y).\n    \"\"\"\n    # H(X)\n    h_x = _calculate_entropy(x_samples)\n    \n    # H(Y)\n    h_y = _calculate_entropy(y_samples)\n    \n    # H(X, Y) - Joint Entropy\n    # We get joint probabilities by counting unique pairs (rows).\n    joint_samples = np.c_[x_samples, y_samples]\n    _, joint_counts = np.unique(joint_samples, axis=0, return_counts=True)\n    joint_probabilities = joint_counts / len(x_samples)\n    joint_probabilities = joint_probabilities[joint_probabilities > 0]\n    h_xy = -np.sum(joint_probabilities * np.log2(joint_probabilities))\n    \n    # I(X;Y) = H(X) + H(Y) - H(X,Y)\n    i_xy = h_x + h_y - h_xy\n    \n    return h_x, i_xy\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute the boolean predicate for each case.\n    \"\"\"\n    test_cases = [\n        # (case_id, q, s, n)\n        (1, 0.5, None, 200000),\n        (2, 0.1, None, 200000),\n        (3, 0.1, 0.3, 200000),\n        (4, 0.1, 0.3, 200000),\n        (5, 0.1, 0.0, 200000),\n    ]\n\n    results = []\n    rng = np.random.default_rng(seed=123)  # Reproducible pseudo-random number generator\n    tol = 1e-3  # Numerical tolerance in bits\n\n    for case_id, q, s, n in test_cases:\n        # Generate base data\n        y = rng.binomial(1, q, size=n)\n        z = y\n\n        if case_id == 1:\n            # Case 1: happy path, independent randomness added\n            x_base = z\n            n_rv = rng.binomial(1, 0.5, size=n)\n            # Encode tuple (Z, N) into a single integer\n            x_aug = 2 * z + n_rv\n\n        elif case_id == 2:\n            # Case 2: class imbalance, independent randomness added\n            x_base = z\n            n_rv = rng.binomial(1, 0.5, size=n)\n            # Encode tuple (Z, N) into a single integer\n            x_aug = 2 * z + n_rv\n\n        elif case_id == 3:\n            # Case 3: label-independent corrupting noise\n            x_base = z\n            b_rv = rng.binomial(1, s, size=n)\n            z_tilde = z ^ b_rv  # bitwise XOR\n            x_aug = z_tilde\n        \n        elif case_id == 4:\n            # Case 4: adding task-relevant information\n            b_rv = rng.binomial(1, s, size=n)\n            z_tilde = z ^ b_rv\n            x_base = z_tilde\n            # Encode tuple (Z_tilde, Y) into a single integer\n            x_aug = 2 * z_tilde + y\n            \n        elif case_id == 5:\n            # Case 5: boundary condition, no corruption\n            x_base = z\n            b_rv = rng.binomial(1, s, size=n) # s=0.0 means B is all zeros\n            z_tilde = z ^ b_rv\n            x_aug = z_tilde\n        else:\n            continue\n\n        # Compute information-theoretic metrics for base and augmented inputs\n        h_base, i_base = compute_info_metrics(x_base, y)\n        h_aug, i_aug = compute_info_metrics(x_aug, y)\n\n        # Evaluate the boolean predicate b with tolerance\n        # b = [H(X_aug) > H(X_base)] AND [I(X_aug;Y) = I(X_base;Y)]\n        # H_aug > H_base + tol : Guards against noise making things unequal\n        # I_aug = I_base + tol : Guards against noise making things unequal\n        is_h_aug_greater = h_aug > h_base + tol\n        is_i_aug_le_i_base = i_aug = i_base + tol\n        \n        b = is_h_aug_greater and is_i_aug_le_i_base\n        results.append(b)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3138094"}, {"introduction": "Having distinguished useful information from mere randomness, we can now formalize this into a design principle for creating efficient representations. This practice explores the Information Bottleneck (IB) framework, which aims to find a compressed representation $T$ of an input $X$ that maximally preserves information about a target variable $Y$. You will implement an algorithm to find the optimal representation under a given bit-budget and discover the minimum complexity required to achieve a target accuracy, connecting these theoretical concepts to the practical concern of sample complexity [@problem_id:3138091].", "problem": "You are given finite random variables $X$ and $Y$ with known discrete distributions. You must design a discrete representation $T = g(X)$ under an entropy budget $k$ bits and evaluate how many bits suffice to reach a target population accuracy (with a Bayes-optimal decision rule). You must also relate this bit budget to a sample complexity bound for estimating the joint distribution of $(T,Y)$ from independent and identically distributed samples.\n\nFundamental base:\n- Shannon entropy of a discrete random variable $T$ with probability mass function $p(t)$ is $H(T) = -\\sum_{t} p(t) \\log_2 p(t)$.\n- For any finite set, $\\log_2 |T|$ is the maximum possible entropy of $T$ when $p(t)$ is uniform, therefore $H(T) \\le \\log_2 |T|$.\n- A Bayes-optimal classifier for predicting $Y$ from $T$ chooses $\\arg\\max_{y} p(y \\mid t)$ for each $t$, yielding population accuracy $\\sum_{t} p(t) \\max_{y} p(y \\mid t)$.\n- Hoeffding’s inequality for bounded Bernoulli variables states that for an empirical average $\\hat{\\mu}$ of $n$ independent and identically distributed samples of a Bernoulli random variable with mean $\\mu$, $\\Pr(|\\hat{\\mu} - \\mu| \\ge \\epsilon) \\le 2 e^{-2 n \\epsilon^2}$. With a union bound over a finite set of events, this extends to a uniform deviation bound.\n\nProblem tasks:\n1. For each test case, let $X$ take values in a finite set $\\mathcal{X}$ with probabilities $p(x)$ for $x \\in \\mathcal{X}$, and let $Y$ take values in a finite set $\\mathcal{Y}$ with specified conditional probabilities $p(y \\mid x)$ for each $x \\in \\mathcal{X}$ and $y \\in \\mathcal{Y}$. Consider any discrete representation $T = g(X)$ as a deterministic mapping from $\\mathcal{X}$ to a finite code set $\\mathcal{T}$. The entropy budget is constrained by $H(T) \\le k$ bits. Because $H(T) \\le \\log_2 |\\mathcal{T}| \\le k$, it suffices to restrict $|\\mathcal{T}| \\le 2^k$.\n2. For a fixed integer $k \\ge 0$, compute the best achievable population accuracy for predicting $Y$ from $T$ under the constraints above, assuming the Bayes-optimal classifier. This accuracy is\n   $$\\mathrm{Acc}_k^* = \\max_{g: \\mathcal{X} \\to \\mathcal{T},\\ |\\mathcal{T}| \\le 2^k} \\sum_{t \\in \\mathcal{T}} p(t) \\max_{y \\in \\mathcal{Y}} p(y \\mid t),$$\n   where $p(t) = \\sum_{x \\in \\mathcal{X}: g(x)=t} p(x)$ and $p(y \\mid t) = \\frac{\\sum_{x \\in \\mathcal{X}: g(x)=t} p(x) p(y \\mid x)}{p(t)}$ for $p(t) > 0$.\n3. For each test case, given a target accuracy threshold $\\alpha$, determine the minimal integer $k_{\\min}$ such that $\\mathrm{Acc}_k^* \\ge \\alpha$. If no such $k$ exists for $0 \\le k \\le \\lceil \\log_2 |\\mathcal{X}| \\rceil$, set $k_{\\min} = -1$ and define the reported accuracy as $\\max_{0 \\le k \\le \\lceil \\log_2 |\\mathcal{X}| \\rceil} \\mathrm{Acc}_k^*$.\n4. Sample complexity relation: Let $S$ denote the number of distinct codes used by an optimal mapping $g$ achieving $\\mathrm{Acc}_{k_{\\min}}^*$, i.e., $S = |\\{t \\in \\mathcal{T}: p(t)>0\\}|$. To uniformly estimate the joint probabilities $p(t,y)$ for all $(t,y) \\in \\mathcal{T} \\times \\mathcal{Y}$ within an additive error $\\epsilon > 0$ with confidence at least $1-\\delta$ (that is, with probability at least $1-\\delta$, every empirical estimate $\\hat{p}(t,y)$ deviates from $p(t,y)$ by at most $\\epsilon$), Hoeffding’s inequality and a union bound yield the sufficient sample size\n   $$n \\ge \\frac{1}{2\\epsilon^2} \\ln\\!\\left(\\frac{2 S |\\mathcal{Y}|}{\\delta}\\right).$$\n   Report $N_{\\min} = \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\!\\left(\\frac{2 S |\\mathcal{Y}|}{\\delta}\\right) \\right\\rceil$ for each test case.\n\nTest suite:\n- Test case $1$ (binary, deterministic, separable):\n  - $\\mathcal{X} = \\{0,1,2,3\\}$,\n  - $p(x) = [0.4, 0.3, 0.2, 0.1]$,\n  - $\\mathcal{Y} = \\{0,1\\}$,\n  - $p(y \\mid x)$ given by $p(1 \\mid 0) = 1.0$, $p(1 \\mid 1) = 1.0$, $p(0 \\mid 2) = 1.0$, $p(0 \\mid 3) = 1.0$ (and zero otherwise),\n  - target accuracy $\\alpha = 0.95$,\n  - error tolerance $\\epsilon = 0.02$,\n  - confidence slack $\\delta = 0.01$.\n- Test case $2$ (binary, label independent of input):\n  - $\\mathcal{X} = \\{0,1,2\\}$,\n  - $p(x) = [0.5, 0.3, 0.2]$,\n  - $\\mathcal{Y} = \\{0,1\\}$,\n  - $p(1 \\mid x) = 0.6$ and $p(0 \\mid x) = 0.4$ for all $x \\in \\mathcal{X}$,\n  - target accuracy $\\alpha = 0.6$,\n  - error tolerance $\\epsilon = 0.01$,\n  - confidence slack $\\delta = 0.05$.\n- Test case $3$ (three-class, deterministic, requires more than one bit):\n  - $\\mathcal{X} = \\{0,1,2,3\\}$,\n  - $p(x) = [0.5, 0.3, 0.1, 0.1]$,\n  - $\\mathcal{Y} = \\{0,1,2\\}$,\n  - $p(y \\mid x)$ given by $p(0 \\mid 0) = 1.0$, $p(1 \\mid 1) = 1.0$, $p(2 \\mid 2) = 1.0$, $p(2 \\mid 3) = 1.0$ (and zero otherwise),\n  - target accuracy $\\alpha = 0.95$,\n  - error tolerance $\\epsilon = 0.02$,\n  - confidence slack $\\delta = 0.02$.\n\nRequired program behavior:\n- Implement an exhaustive search over mappings $g: \\mathcal{X} \\to \\mathcal{T}$ with $|\\mathcal{T}| \\le 2^k$ for each integer $k$ starting from $0$ up to $\\lceil \\log_2 |\\mathcal{X}| \\rceil$. For each mapping, compute the induced distribution over $T$, evaluate the Bayes-optimal accuracy, and retain the maximum accuracy $\\mathrm{Acc}_k^*$ and the number $S$ of used codes in the optimal mapping for the smallest $k$ meeting $\\alpha$.\n- For each test case, produce the list $[k_{\\min}, \\mathrm{Acc}_{k_{\\min}}^*, N_{\\min}]$ where:\n  - $k_{\\min}$ is an integer (or $-1$ if the target accuracy is unattainable within the search range),\n  - $\\mathrm{Acc}_{k_{\\min}}^*$ is a float,\n  - $N_{\\min}$ is an integer computed using the formula above with $S$ equal to the number of used codes in the optimal mapping at $k_{\\min}$; if $k_{\\min} = -1$, use the $S$ from the mapping achieving the maximum accuracy across the search range.\n- Final output format: Your program should produce a single line of output containing the results for all three test cases as a comma-separated list enclosed in square brackets, where each element is itself a list. For example, the output must look like $[[\\dots],[\\dots],[\\dots]]$ with no spaces or extra text.", "solution": "The user has provided a problem that requires finding an optimal discrete representation of a random variable $X$ to predict another random variable $Y$, under a constraint on the complexity of the representation. The complexity is budgeted in terms of Shannon entropy, simplified to a constraint on the size of the representation's codomain. The tasks involve finding the minimum budget $k$ to achieve a target accuracy, and calculating a corresponding sample complexity bound.\n\n### Problem Validation\nThe problem statement is critically assessed based on the provided methodology.\n\n**Step 1: Extracted Givens**\n- **Variables**: Finite discrete random variables $X$ and $Y$.\n- **Mapping**: A deterministic function $T = g(X)$, where $g: \\mathcal{X} \\to \\mathcal{T}$.\n- **Constraint**: The entropy of the representation $H(T) \\le k$ bits. This is simplified to a cardinality constraint on the codomain, $|\\mathcal{T}| \\le 2^k$.\n- **Objective**: Maximize the Bayes-optimal population accuracy for predicting $Y$ from $T$. The accuracy for a given mapping $g$ is $\\sum_{t} p(t) \\max_{y} p(y \\mid t)$.\n- **Optimization Problem**: $\\mathrm{Acc}_k^* = \\max_{g: |\\mathcal{T}| \\le 2^k} \\sum_{t} p(t) \\max_{y} p(y \\mid t)$.\n- **Task 1**: For a given target accuracy $\\alpha$, find the minimum integer $k_{\\min}$ (in the range $0 \\le k \\le \\lceil \\log_2 |\\mathcal{X}| \\rceil$) such that $\\mathrm{Acc}_{k_{\\min}}^* \\ge \\alpha$. If no such $k$ exists, $k_{\\min} = -1$.\n- **Task 2**: Calculate a sample complexity bound $N_{\\min} = \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\!\\left(\\frac{2 S |\\mathcal{Y}|}{\\delta}\\right) \\right\\rceil$ where $S$ is the number of active codes for an optimal mapping at $k_{\\min}$.\n- **Inputs**: For each test case, the sets $\\mathcal{X}$, $\\mathcal{Y}$, distributions $p(x)$ and $p(y \\mid x)$, and parameters $\\alpha$, $\\epsilon$, $\\delta$ are provided.\n\n**Step 2: Validation Using Extracted Givens**\n- **Scientifically Grounded**: The problem is well-founded in information theory, probability theory, and statistical learning theory. The concepts of Shannon entropy, Bayes-optimal classifiers, and sample complexity via Hoeffding’s inequality are standard and correctly applied. The core task is a form of rate-distortion optimization, specifically related to the Information Bottleneck principle.\n- **Well-Posed**: The problem is well-posed. The objective function (accuracy) is to be maximized over a finite set of possible mappings $g$. A mapping $g: \\mathcal{X} \\to \\mathcal{T}$ with $|\\mathcal{T}| \\le C$ induces a partition of the set $\\mathcal{X}$ into $S \\le C$ non-empty subsets. The number of partitions of a finite set is finite (given by the Bell numbers). Since the search space is finite, a maximum accuracy is guaranteed to exist for each $k$. The subsequent steps are deterministic.\n- **Objective**: The problem is stated in precise mathematical language, free from any subjectivity.\n\nThe problem does not exhibit any of the invalidity flaws. It is scientifically sound, formalizable, complete, feasible for the given test cases, and well-structured.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Principle-Based Solution\nThe problem requires us to find an optimal compression of a random variable $X$ into a new variable $T=g(X)$ that maximally preserves predictive information about a target variable $Y$. The compression is constrained by limiting the number of possible values $T$ can take, $|\\mathcal{T}| \\le 2^k$, where $k$ is a budget in bits.\n\n**1. Formalizing the Optimization**\nA deterministic mapping $g: \\mathcal{X} \\to \\mathcal{T}$ partitions the input space $\\mathcal{X}$ into a set of disjoint subsets $\\Pi = \\{A_t\\}_{t \\in \\mathcal{T}}$, where $A_t = \\{x \\in \\mathcal{X} \\mid g(x)=t\\}$. The number of non-empty subsets, $S = |\\{t \\mid A_t \\neq \\emptyset\\}|$, is the number of \"active codes\" used by the mapping. The constraint $|\\mathcal{T}| \\le 2^k$ implies that we must find the best partition of $\\mathcal{X}$ into $S \\le 2^k$ parts.\n\nFor a given partition $\\Pi = \\{A_1, A_2, \\dots, A_S\\}$, we can calculate the resulting accuracy. Each subset $A_j$ corresponds to a unique code $t_j$.\n- The probability of observing code $t_j$ is the sum of probabilities of the input values that map to it: $p(t_j) = \\sum_{x \\in A_j} p(x)$.\n- The joint probability of code $t_j$ and output $y$ is $p(t_j, y) = \\sum_{x \\in A_j} p(x,y) = \\sum_{x \\in A_j} p(y|x)p(x)$.\n- The Bayes-optimal classifier, given observation $T=t_j$, predicts the label $y^*$ that maximizes the conditional probability $p(y|t_j) = p(t_j, y) / p(t_j)$. The accuracy contribution from this cluster is $p(t_j) \\max_y p(y|t_j) = \\max_y p(t_j,y)$.\n- The total population accuracy for this partition is the sum over all clusters:\n$$ \\mathrm{Acc}(\\Pi) = \\sum_{j=1}^{S} \\max_{y \\in \\mathcal{Y}} \\left( \\sum_{x \\in A_j} p(x,y) \\right) $$\n\nThe optimization problem for a fixed budget $k$ is to find the partition $\\Pi^*$ with at most $2^k$ subsets that maximizes this accuracy:\n$$ \\mathrm{Acc}_k^* = \\max_{\\Pi : |\\Pi| \\le 2^k} \\mathrm{Acc}(\\Pi) $$\n\n**2. Algorithmic Strategy: Exhaustive Search**\nSince the size of the input space $\\mathcal{X}$ is small in the provided test cases ($|\\mathcal{X}| \\le 4$), an exhaustive search over all possible partitions of $\\mathcal{X}$ is computationally feasible. The number of ways to partition a set of size $N$ is the Bell number $B_N$. For $N=3$, $B_3=5$. For $N=4$, $B_4=15$.\n\nThe overall algorithm proceeds as follows:\n1.  **Generate Partitions**: For the given set $\\mathcal{X}$, generate all possible partitions.\n2.  **Evaluate Partitions**: For each partition, calculate its size (number of subsets, $S$) and the corresponding Bayes-optimal accuracy.\n3.  **Determine $\\mathrm{Acc}_k^*$**: For each integer budget $k$ from $0$ to $\\lceil \\log_2 |\\mathcal{X}| \\rceil$:\n    - The maximum number of clusters allowed is $C = 2^k$.\n    - Find the maximum accuracy among all evaluated partitions with $S \\le C$. This gives $\\mathrm{Acc}_k^*$. Keep track of the number of clusters $S_k^*$ for a partition that achieves this accuracy. If there's a tie in accuracy, a deterministic tie-breaking rule (e.g., choosing the partition with the minimum $S$) is used.\n4.  **Find $k_{\\min}$**: Identify the smallest $k$ for which $\\mathrm{Acc}_k^* \\ge \\alpha$. This is $k_{\\min}$. If the condition is never met, $k_{\\min}$ is set to $-1$.\n5.  **Calculate Sample Complexity $N_{\\min}$**:\n    - If $k_{\\min}$ was found, use the accuracy $\\mathrm{Acc}_{k_{\\min}}^*$ and its corresponding cluster count $S_{k_{\\min}}^*$.\n    - If $k_{\\min} = -1$, use the highest accuracy achieved over all $k$, $\\max_k \\mathrm{Acc}_k^*$, and its corresponding cluster count $S^*$.\n    - Substitute the final $S$, $|\\mathcal{Y}|$, $\\epsilon$, and $\\delta$ into the provided formula to calculate $N_{\\min}$:\n      $$N_{\\min} = \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\!\\left(\\frac{2 S |\\mathcal{Y}|}{\\delta}\\right) \\right\\rceil$$\n\nThis systematic process guarantees finding the optimal solution as defined by the problem statement.", "answer": "```python\nimport numpy as np\nimport math\n\ndef generate_all_partitions(elements):\n    \"\"\"\n    Generates all partitions of a set of elements.\n    A partition is a list of lists (subsets).\n    \"\"\"\n    if not elements:\n        yield []\n        return\n\n    first = elements[0]\n    rest = elements[1:]\n\n    for smaller_partition in generate_all_partitions(rest):\n        if not smaller_partition:\n            yield [[first]]\n            continue\n        # Option 1: Add 'first' to an existing part of the smaller partition\n        for i in range(len(smaller_partition)):\n            new_partition = [sublist[:] for sublist in smaller_partition]\n            new_partition[i].append(first)\n            yield new_partition\n        # Option 2: 'first' forms a new part\n        yield smaller_partition + [[first]]\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            'p_x': [0.4, 0.3, 0.2, 0.1],\n            'p_y_given_x': [[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 0.0, 0.0]],\n            'alpha': 0.95,\n            'epsilon': 0.02,\n            'delta': 0.01\n        },\n        # Test case 2\n        {\n            'p_x': [0.5, 0.3, 0.2],\n            'p_y_given_x': [[0.4, 0.4, 0.4], [0.6, 0.6, 0.6]],\n            'alpha': 0.6,\n            'epsilon': 0.01,\n            'delta': 0.05\n        },\n        # Test case 3\n        {\n            'p_x': [0.5, 0.3, 0.1, 0.1],\n            'p_y_given_x': [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 1.0]],\n            'alpha': 0.95,\n            'epsilon': 0.02,\n            'delta': 0.02\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        p_x = np.array(case['p_x'])\n        p_y_given_x = np.array(case['p_y_given_x'])\n        alpha = case['alpha']\n        epsilon = case['epsilon']\n        delta = case['delta']\n\n        num_x = len(p_x)\n        num_y = p_y_given_x.shape[0]\n        \n        # Precompute joint probabilities p(x, y) = p(y|x) * p(x)\n        p_xy = p_y_given_x * p_x[np.newaxis, :]\n\n        # Generate and evaluate all partitions of the input space X\n        elements_to_partition = list(range(num_x))\n        all_partitions = list(generate_all_partitions(elements_to_partition))\n        \n        partition_results = []\n        for p in all_partitions:\n            if not p: continue\n            \n            accuracy = 0.0\n            for cluster in p:\n                if not cluster: continue\n                # Sum p(x,y) over all x in the cluster to get p(t,y)\n                cluster_p_y = np.sum(p_xy[:, cluster], axis=1)\n                # Bayes-optimal decision for this cluster contributes max_y p(t,y)\n                accuracy += np.max(cluster_p_y)\n            \n            partition_results.append({'S': len(p), 'acc': accuracy})\n\n        # Determine k_min and related values\n        max_k = math.ceil(math.log2(num_x)) if num_x > 1 else 0\n        k_min = -1\n        \n        best_acc_overall = -1.0\n        best_S_overall = -1\n        acc_at_k_min = -1.0\n        S_at_k_min = -1\n\n        for k in range(max_k + 1):\n            max_clusters = 2**k\n            \n            acc_k_star = -1.0\n            S_k_star = -1\n            \n            # Find the best accuracy for the current budget k\n            for res in partition_results:\n                if res['S'] = max_clusters:\n                    if res['acc'] > acc_k_star + 1e-12:\n                        acc_k_star = res['acc']\n                        S_k_star = res['S']\n                    elif abs(res['acc'] - acc_k_star)  1e-12:\n                        S_k_star = min(S_k_star, res['S'])\n\n            # Update overall best performance across all k\n            if acc_k_star > best_acc_overall + 1e-12:\n                best_acc_overall = acc_k_star\n                best_S_overall = S_k_star\n            elif abs(acc_k_star - best_acc_overall)  1e-12:\n                best_S_overall = min(best_S_overall, S_k_star)\n\n            # Check if we have met the target accuracy for the first time\n            if k_min == -1 and acc_k_star >= alpha - 1e-12:\n                k_min = k\n                acc_at_k_min = acc_k_star\n                S_at_k_min = S_k_star\n        \n        # Determine final parameters for reporting\n        final_k = k_min\n        if final_k == -1:\n            final_acc = best_acc_overall\n            final_S = best_S_overall\n        else:\n            final_acc = acc_at_k_min\n            final_S = S_at_k_min\n\n        # Calculate N_min using the final selected S\n        if final_S = 0:\n            N_min = 0\n        else:\n            log_term = math.log((2 * final_S * num_y) / delta)\n            N_min = math.ceil((1 / (2 * epsilon**2)) * log_term)\n\n        results.append([final_k, final_acc, N_min])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3138091"}, {"introduction": "Information-theoretic tools are not only for design but are also powerful for diagnostics. In this final practice, we will apply these concepts to analyze and debug generative models, a cornerstone of modern deep learning. You will compute mutual information and Kullback-Leibler (KL) divergence to quantify common failure modes like 'mode collapse' and 'poor coverage' in several toy models, gaining a quantitative understanding of how to diagnose why a generative model might not be learning the true data distribution effectively [@problem_id:3138062].", "problem": "You are given a discrete latent-variable generative model with a latent variable $Z$ taking values in a finite set and an observable variable $X$ taking values in a finite set. The model defines a prior over latents $p(Z)$ and a likelihood $p(X \\mid Z)$, which together induce a model joint distribution $p(Z, X)$ and a model marginal over data $p_{\\text{model}}(X)$. You are also given a target data distribution $p_{\\text{data}}(X)$ over the same support for $X$. Your task is to use the basic discrete definitions from information theory to quantify failure modes that commonly occur in generative modeling. Specifically, you will compute the mutual information between $Z$ and $X$, the entropy of $Z$, and two directions of Kullback-Leibler (KL) divergence between $p_{\\text{data}}(X)$ and $p_{\\text{model}}(X)$.\n\nAll logarithms must be the natural logarithm, so information quantities are expressed in nats. For any summation involving probabilities that are zero, apply the standard discrete convention that any term of the form $0 \\cdot \\log(\\cdot)$ contributes $0$ to the sum. Your program must compute these quantities exactly from the given distributions using the basic definitions, with no approximations other than rounding in the final reported values.\n\nFor each test case below, compute and report the following four quantities in the order listed:\n- The mutual information between $Z$ and $X$ under the model joint, denoted $I(Z; X)$.\n- The entropy of the latent prior, denoted $H(Z)$.\n- The Kullback–Leibler divergence from $p_{\\text{data}}(X)$ to $p_{\\text{model}}(X)$, denoted $\\mathrm{KL}(p_{\\text{data}} \\| p_{\\text{model}})$.\n- The Kullback–Leibler divergence from $p_{\\text{model}}(X)$ to $p_{\\text{data}}(X)$, denoted $\\mathrm{KL}(p_{\\text{model}} \\| p_{\\text{data}})$.\n\nImportant implementation details:\n- Use the standard discrete definitions for the above quantities and the natural logarithm throughout.\n- For each float you output, round to $6$ decimal places.\n- Your program must produce a single line of output containing the results for all test cases as a comma-separated list of lists, enclosed in square brackets, for example $[\\,[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4]\\,]$ where each symbol represents a float rounded to $6$ decimal places.\n\nTest suite (all probabilities are given explicitly and sum to $1$ across their respective supports):\n\nCommon support across all test cases:\n- The latent variable has support $Z \\in \\{0, 1, 2\\}$.\n- The observable variable has support $X \\in \\{0, 1, 2\\}$.\n\nTest case $1$ (balanced, near-ideal coverage):\n- Prior $p(Z) = [\\,1/3,\\, 1/3,\\, 1/3\\,]$, that is $[\\,0.\\overline{3},\\, 0.\\overline{3},\\, 0.\\overline{3}\\,]$.\n- Likelihood $p(X \\mid Z)$ is deterministic and one-to-one: the $3 \\times 3$ matrix has rows $[\\,1,\\,0,\\,0\\,]$, $[\\,0,\\,1,\\,0\\,]$, and $[\\,0,\\,0,\\,1\\,]$ for $Z=0,1,2$ respectively.\n- Data distribution $p_{\\text{data}}(X) = [\\,1/3,\\, 1/3,\\, 1/3\\,]$.\n\nTest case $2$ (mode collapse: identical likelihood across latents):\n- Prior $p(Z) = [\\,1/3,\\, 1/3,\\, 1/3\\,]$.\n- Likelihood rows are identical: each row of $p(X \\mid Z)$ equals $[\\,0.7,\\, 0.2,\\, 0.1\\,]$.\n- Data distribution $p_{\\text{data}}(X) = [\\,0.34,\\, 0.33,\\, 0.33\\,]$.\n\nTest case $3$ (coverage mismatch: overlapping likelihoods and non-uniform prior):\n- Prior $p(Z) = [\\,0.6,\\, 0.3,\\, 0.1\\,]$.\n- Likelihood matrix with rows:\n  - For $Z=0$: $[\\,0.6,\\, 0.25,\\, 0.15\\,]$,\n  - For $Z=1$: $[\\,0.3,\\, 0.4,\\, 0.3\\,]$,\n  - For $Z=2$: $[\\,0.2,\\, 0.3,\\, 0.5\\,]$.\n- Data distribution $p_{\\text{data}}(X) = [\\,0.5,\\, 0.3,\\, 0.2\\,]$.\n\nTest case $4$ (boundary condition: degenerate latent prior):\n- Prior $p(Z) = [\\,1.0,\\, 0.0,\\, 0.0\\,]$.\n- Likelihood rows (note only the first row matters due to the prior): each row equals $[\\,0.6,\\, 0.25,\\, 0.15\\,]$.\n- Data distribution $p_{\\text{data}}(X) = [\\,0.6,\\, 0.25,\\, 0.15\\,]$.\n\nYour program must compute the four requested quantities for each test case using the above inputs and output a single line string representing a list of lists, where each inner list corresponds to a test case in order. Round every reported float to $6$ decimal places and do not include any additional text beyond the specified output format.", "solution": "The problem requires the computation of four information-theoretic quantities for a discrete latent-variable generative model, which are instrumental in diagnosing its performance characteristics, such as mode collapse and coverage. The model is defined by a prior distribution over latent variables, $p(Z)$, and a conditional distribution (likelihood) of observable variables given the latent ones, $p(X \\mid Z)$. These combine to form a joint distribution $p(Z, X) = p(X \\mid Z)p(Z)$. The model's effectiveness is assessed relative to a given target data distribution, $p_{\\text{data}}(X)$.\n\nThe four quantities to be computed are:\n1.  The mutual information between the latent variable $Z$ and the observable variable $X$ under the model's joint distribution, denoted $I(Z; X)$. This measures the reduction in uncertainty about $X$ from knowing $Z$, indicating how much information the latent code carries about the generated output.\n2.  The entropy of the latent prior, $H(Z)$. This quantifies the diversity of the latent codes being used by the model.\n3.  The Kullback-Leibler (KL) divergence from the data distribution to the model distribution, $\\mathrm{KL}(p_{\\text{data}} \\| p_{\\text{model}})$. This measures how well the model approximates the data distribution, and it heavily penalizes the model for failing to generate samples where the true data distribution has support (i.e., poor coverage).\n4.  The KL divergence from the model distribution to the data distribution, $\\mathrm{KL}(p_{\\text{model}} \\| p_{\\text{data}})$. This divergence penalizes the model for generating samples that are unlikely under the true data distribution (i.e., poor precision or mode collapse).\n\nAll computations must adhere to the standard definitions from information theory using the natural logarithm, with results expressed in nats.\n\n**Theoretical Framework and Computational Strategy**\n\nThe computation proceeds by first deriving the necessary distributions from the given model components and then applying the fundamental definitions of the required quantities.\n\n**Step 1: Derive the Model's Marginal Distribution $p_{\\text{model}}(X)$**\nThe model's marginal distribution over the observable variable $X$ is obtained by marginalizing out the latent variable $Z$ from the joint distribution $p(Z, X)$.\nlet $Z$ take values $\\{z_i\\}_{i=1}^{N_Z}$ and $X$ take values $\\{x_j\\}_{j=1}^{N_X}$.\nThe probability of observing $x_j$ under the model is given by the law of total probability:\n$$p_{\\text{model}}(X=x_j) = \\sum_{i=1}^{N_Z} p(Z=z_i, X=x_j) = \\sum_{i=1}^{N_Z} p(X=x_j \\mid Z=z_i) p(Z=z_i)$$\nThis calculation must be performed for each test case as a prerequisite for computing $I(Z; X)$ and the KL divergences.\n\n**Step 2: Compute $H(Z)$**\nThe entropy of the discrete latent prior $p(Z)$ is given by the standard formula:\n$$H(Z) = - \\sum_{i=1}^{N_Z} p(Z=z_i) \\log p(Z=z_i)$$\nThe convention that terms of the form $0 \\cdot \\log(0)$ contribute $0$ to the sum is applied.\n\n**Step 3: Compute $I(Z; X)$**\nThe mutual information $I(Z; X)$ can be computed using several equivalent formulas. A computationally convenient form is $I(Z; X) = H(X) - H(X \\mid Z)$.\nFirst, we compute the entropy of the model's marginal distribution, $H(X)$:\n$$H(X) = - \\sum_{j=1}^{N_X} p_{\\text{model}}(X=x_j) \\log p_{\\text{model}}(X=x_j)$$\nNext, we compute the conditional entropy $H(X \\mid Z)$, which is the expected value of the entropy of the likelihood distribution $p(X \\mid Z=z_i)$, averaged over the prior $p(Z)$:\n$$H(X \\mid Z) = \\sum_{i=1}^{N_Z} p(Z=z_i) H(X \\mid Z=z_i)$$\nwhere the entropy of each conditional distribution is:\n$$H(X \\mid Z=z_i) = - \\sum_{j=1}^{N_X} p(X=x_j \\mid Z=z_i) \\log p(X=x_j \\mid Z=z_i)$$\nThe mutual information is then the difference:\n$$I(Z; X) = H(X) - H(X \\mid Z)$$\n\n**Step 4: Compute KL Divergences**\nThe KL divergence between two discrete probability distributions $P(Y)$ and $Q(Y)$ over the same support is defined as:\n$$\\mathrm{KL}(P \\| Q) = \\sum_{y} P(y) \\log \\frac{P(y)}{Q(y)}$$\nThe requested quantities are instances of this definition:\n-   $\\mathrm{KL}(p_{\\text{data}} \\| p_{\\text{model}}) = \\sum_{j=1}^{N_X} p_{\\text{data}}(X=x_j) \\log \\frac{p_{\\text{data}}(X=x_j)}{p_{\\text{model}}(X=x_j)}$\n-   $\\mathrm{KL}(p_{\\text{model}} \\| p_{\\text{data}}) = \\sum_{j=1}^{N_X} p_{\\text{model}}(X=x_j) \\log \\frac{p_{\\text{model}}(X=x_j)}{p_{\\text{data}}(X=x_j)}$\n\nIn cases where a probability in the denominator is $0$ while the corresponding numerator is non-zero, the KL divergence is infinite. However, the test cases provided do not exhibit this issue. The convention that a term is $0$ if the probability in the numerator is $0$ is also applied.\n\nThis systematic application of fundamental definitions provides a complete and exact method to calculate the required quantities for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef solve():\n    \"\"\"\n    Computes four information-theoretic quantities for a series of\n    latent-variable generative model test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"p_z\": np.array([1/3, 1/3, 1/3]),\n            \"p_x_cond_z\": np.array([[1.0, 0.0, 0.0],\n                                    [0.0, 1.0, 0.0],\n                                    [0.0, 0.0, 1.0]]),\n            \"p_data_x\": np.array([1/3, 1/3, 1/3])\n        },\n        {\n            \"p_z\": np.array([1/3, 1/3, 1/3]),\n            \"p_x_cond_z\": np.array([[0.7, 0.2, 0.1],\n                                    [0.7, 0.2, 0.1],\n                                    [0.7, 0.2, 0.1]]),\n            \"p_data_x\": np.array([0.34, 0.33, 0.33])\n        },\n        {\n            \"p_z\": np.array([0.6, 0.3, 0.1]),\n            \"p_x_cond_z\": np.array([[0.6, 0.25, 0.15],\n                                    [0.3, 0.4, 0.3],\n                                    [0.2, 0.3, 0.5]]),\n            \"p_data_x\": np.array([0.5, 0.3, 0.2])\n        },\n        {\n            \"p_z\": np.array([1.0, 0.0, 0.0]),\n            \"p_x_cond_z\": np.array([[0.6, 0.25, 0.15],\n                                    [0.6, 0.25, 0.15],\n                                    [0.6, 0.25, 0.15]]),\n            \"p_data_x\": np.array([0.6, 0.25, 0.15])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        p_z = case[\"p_z\"]\n        p_x_cond_z = case[\"p_x_cond_z\"]\n        p_data_x = case[\"p_data_x\"]\n\n        # Calculate H(Z), the entropy of the latent prior.\n        h_z = entropy(p_z, base=np.e)\n\n        # Calculate p_model(X), the model's marginal distribution.\n        # This is sum_z p(x|z)p(z), which is a matrix multiplication.\n        p_model_x = p_z @ p_x_cond_z\n\n        # Calculate I(Z; X) = H(X) - H(X|Z)\n        # H(X) is the entropy of the model's marginal.\n        h_x = entropy(p_model_x, base=np.e)\n        \n        # H(X|Z) is the expectation of the entropy of the likelihoods.\n        # E_p(z) [H(p(X|Z=z))]\n        h_x_cond_z_per_z = np.array([entropy(p, base=np.e) for p in p_x_cond_z])\n        h_x_cond_z = np.dot(p_z, h_x_cond_z_per_z)\n        \n        i_zx = h_x - h_x_cond_z\n\n        # Calculate KL(p_data || p_model)\n        # scipy.stats.entropy calculates KL divergence when qk is provided.\n        kl_data_model = entropy(pk=p_data_x, qk=p_model_x, base=np.e)\n\n        # Calculate KL(p_model || p_data)\n        kl_model_data = entropy(pk=p_model_x, qk=p_data_x, base=np.e)\n\n        # Store the four required quantities in order.\n        result_for_case = [i_zx, h_z, kl_data_model, kl_model_data]\n        all_results.append(result_for_case)\n\n    # Format the final output string as a list of lists with 6 decimal places.\n    inner_lists_str = []\n    for r in all_results:\n        # Using f-string formatting to ensure 6 decimal places are shown.\n        r_formatted_str = ','.join([f'{val:.6f}' for val in r])\n        inner_lists_str.append(f\"[{r_formatted_str}]\")\n    \n    final_output_str = f\"[{','.join(inner_lists_str)}]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "3138062"}]}