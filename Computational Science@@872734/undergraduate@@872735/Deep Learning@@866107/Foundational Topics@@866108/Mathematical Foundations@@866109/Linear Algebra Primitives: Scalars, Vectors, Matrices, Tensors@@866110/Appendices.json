{"hands_on_practices": [{"introduction": "Ensuring that tensor shapes align in your code is a necessary, but insufficient, step for correctness. This exercise challenges you to think beyond mere shapes and consider the fundamental mathematical properties of the linear transformations you implement [@problem_id:3143508]. By designing unit tests based on concepts like equivariance and change-of-basis invariance, you will learn to detect subtle but critical bugs that shape-checking alone would miss, building a more robust understanding of what a linear layer truly represents.", "problem": "A data scientist implements a linear layer in a sequence model using tensor contraction. Let $X \\in \\mathbb{R}^{B \\times T \\times D}$ denote a mini-batch ($B$) of sequences over $T$ time steps with $D$ feature channels per time step, and let $W \\in \\mathbb{R}^{D \\times K}$ be a weight matrix mapping features to $K$ outputs. The intended operation is to produce $Y \\in \\mathbb{R}^{B \\times T \\times K}$ via contraction of the last axis of $X$ with the first axis of $W$, i.e., $Y_{b,t,k}$ should equal the sum over the feature index $i$ of $X_{b,t,i} W_{i,k}$ for each $b$, $t$, and $k$. Suppose a bug contracts the wrong axis (for instance, the time axis), which can silently pass shape checks when $T=D$ but yields incorrect semantics.\n\nYou are tasked with selecting a minimal set of unit tests based on invariances and equivariances that would reliably detect axis misalignment, regardless of particular numeric values and even in cases where $T=D$ makes shapes deceptively compatible. Base your choice on fundamental properties of tensor contraction, the identity matrix, permutation matrices, and orthogonal change of basis.\n\nWhich option describes a minimal set of unit tests that would detect an axis misalignment in this scenario?\n\nA. Two tests:\n- Identity-weight test: choose $D=K$ and set $W = I_{D}$; assert $f(X,W) = X$ exactly.\n- Time-permutation equivariance: let $\\Pi \\in \\mathbb{R}^{T \\times T}$ be a permutation matrix; assert $f(X \\Pi^{\\top}, W) = (f(X,W)) \\Pi^{\\top}$.\n\nB. Single test:\n- Feature-axis change-of-basis invariance: choose a random orthogonal matrix $Q \\in \\mathbb{R}^{D \\times D}$ with $Q^{\\top} Q = I_{D}$ and set $W' = Q^{-1} W$; assert $f(X Q, W') = f(X, W)$.\n\nC. Single test:\n- Linearity in $X$: choose $X_{1}, X_{2} \\in \\mathbb{R}^{B \\times T \\times D}$; assert $f(X_{1}+X_{2}, W) = f(X_{1}, W) + f(X_{2}, W)$.\n\nD. Two tests:\n- Batch-permutation equivariance: let $P \\in \\mathbb{R}^{B \\times B}$ be a permutation matrix; assert $f(P X, W) = P f(X, W)$.\n- Positive homogeneity: for any $\\alpha \\in \\mathbb{R}$, assert $f(\\alpha X, W) = \\alpha f(X, W)$.\n\nHere $f$ denotes the implemented tensor contraction that is supposed to realize the described layer. Pick the option that yields the minimal set of tests that would detect the misalignment robustly and does not include redundant checks.", "solution": "The user has requested an exacting, professor-like validation and solution for a problem in linear algebra as applied to deep learning.\n\n### Problem Validation\n\nI will begin by validating the problem statement as per the required protocol.\n\n#### Step 1: Extract Givens\n-   **Input Tensor `X`**: $X \\in \\mathbb{R}^{B \\times T \\times D}$, representing a mini-batch of $B$ sequences, each with $T$ time steps and $D$ features.\n-   **Weight Matrix `W`**: $W \\in \\mathbb{R}^{D \\times K}$, a weight matrix mapping $D$ input features to $K$ output features.\n-   **Output Tensor `Y`**: $Y \\in \\mathbb{R}^{B \\times T \\times K}$.\n-   **Intended Operation `f_correct`**: The output tensor components are given by $Y_{b,t,k} = \\sum_{i=1}^{D} X_{b,t,i} W_{i,k}$. This corresponds to a tensor contraction of the last axis of $X$ (dimension $D$) with the first axis of $W$ (dimension $D$). This operation is applied independently for each batch element $b \\in \\{1, ..., B\\}$ and each time step $t \\in \\{1, ..., T\\}$.\n-   **The Bug**: The implemented function, denoted as $f(X, W)$, contracts along an incorrect axis. The example given is contracting the time axis of $X$ (dimension $T$) with the feature axis of $W$ (dimension $D$). This is possible if $T=D$, and the bug can \"silently pass shape checks,\" meaning the output tensor has the expected shape $\\mathbb{R}^{B \\times T \\times K}$.\n-   **The Goal**: Select a minimal set of unit tests, based on fundamental properties like invariances and equivariances, that can reliably detect such an axis misalignment.\n\n#### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is set in the context of deep learning and uses fundamental concepts from linear algebra, specifically tensor contractions. The notions of equivariance, invariance, permutation matrices, and orthogonal change of basis are all standard, rigorous mathematical concepts. The scenario of an indexing bug in a numerical computation library is both realistic and common. The problem is scientifically and mathematically sound.\n2.  **Well-Posed**: The problem is well-posed. It clearly defines the correct behavior and the nature of the error to be detected. It asks for a \"minimal set\" of tests from a given list of options, which is a clear objective. A unique, best answer among the options can be determined through logical deduction and analysis of the mathematical properties of the operations.\n3.  **Objective**: The language is precise, formal, and free of subjectivity. All terms are standard in mathematics and computer science.\n4.  **Completeness**: The problem statement is self-contained. It provides all necessary definitions of the tensors, their dimensions, the correct operation, and the class of bugs to be detected.\n5.  **No Other Flaws**: The problem is not trivial, as it requires understanding the specific properties (equivariances/invariances) associated with different tensor axes. It is not ill-posed, contradictory, or based on flawed premises.\n\n#### Step 3: Verdict and Action\nThe problem statement is **valid**. I will now proceed with the full solution and analysis.\n\n### Derivation and Option Analysis\n\nLet $f_{correct}(X, W)$ be the intended operation and $f_{bug}(X, W)$ be the buggy implementation.\n\nThe correct operation is given by the formula:\n$$ (f_{correct}(X, W))_{b,t,k} = \\sum_{i=1}^{D} X_{b,t,i} W_{i,k} $$\nThis operation is a linear transformation defined by $W$ applied to each $D$-dimensional feature vector $X_{b,t,:}$ for each batch $b$ and time step $t$.\n\nThe buggy operation, as described, contracts the time axis of $X$ (axis 1, dimension $T$) with the feature axis of $W$ (axis 0, dimension $D$). This is computationally possible only if $T=D$. To ensure the output shape is $B \\times T \\times K$, the implementation must map the remaining axes appropriately. A plausible form for the buggy operation when $T=D$ is:\n$$ (f_{bug}(X, W))_{b,i,k} = \\sum_{t=1}^{T} X_{b,t,i} W_{t,k} $$\nNote the swapping of roles for indices $t$ and $i$. The summation is over the time steps $t$, and the second index of the output corresponds to the feature channel $i$. For the shape to match the expected $B \\times T \\times K$, the implementation might implicitly permute the output axes, for instance, defining the output $Y$ as $Y_{b,t,k} = (f_{bug}(X, W))_{b,k,t}$. However, the fundamental faulty computation remains the summation over the time axis. We will analyze the properties based on the core computation.\n\nWe must find a test that $f_{correct}$ passes but $f_{bug}$ fails. The test set must also be minimal.\n\n#### Evaluation of Option C\n**C. Single test: Linearity in $X$.**\nThis test asserts that $f(X_{1}+X_{2}, W) = f(X_{1}, W) + f(X_{2}, W)$.\n-   For $f_{correct}$:\n    $$ \\sum_{i} (X_{1,b,t,i} + X_{2,b,t,i}) W_{i,k} = \\sum_{i} X_{1,b,t,i} W_{i,k} + \\sum_{i} X_{2,b,t,i} W_{i,k} $$\n    This property holds.\n-   For $f_{bug}$:\n    $$ \\sum_{t} (X_{1,b,t,i} + X_{2,b,t,i}) W_{t,k} = \\sum_{t} X_{1,b,t,i} W_{t,k} + \\sum_{t} X_{2,b,t,i} W_{t,k} $$\n    This property also holds.\nBoth the correct and buggy operations are tensor contractions, which are fundamentally linear operations (or more accurately, multilinear). This test checks for linearity, which both implementations possess. Therefore, it cannot distinguish between them.\n**Verdict: Incorrect.**\n\n#### Evaluation of Option D\n**D. Two tests: Batch-permutation equivariance and Positive homogeneity.**\n-   **Positive homogeneity**: $f(\\alpha X, W) = \\alpha f(X, W)$. This is a direct consequence of linearity, which we established in C holds for both implementations. This test is therefore not discriminative.\n-   **Batch-permutation equivariance**: $f(P X, W) = P f(X, W)$, where $P \\in \\mathbb{R}^{B \\times B}$ is a permutation matrix acting on the batch axis. The operation, whether correct or buggy, is applied independently to each element of the batch. Permuting the input elements along the batch axis will simply result in a corresponding permutation of the output elements. Both $f_{correct}$ and $f_{bug}$ are equivariant with respect to permutations on the batch axis.\nBoth tests in this option check for properties that are too general and are shared by both the correct and buggy implementations.\n**Verdict: Incorrect.**\n\n#### Evaluation of Option A\n**A. Two tests: Identity-weight test and Time-permutation equivariance.**\n1.  **Identity-weight test**: For $D=K$, set $W=I_D$. Assert $f(X, W) = X$.\n    -   For $f_{correct}$: $(f_{correct}(X, I_D))_{b,t,k} = \\sum_{i} X_{b,t,i} (I_D)_{i,k} = \\sum_{i} X_{b,t,i} \\delta_{ik} = X_{b,t,k}$. The test passes.\n    -   For $f_{bug}$ (requires $T=D=K$): $(f_{bug}(X, I_D))_{b,i,k} = \\sum_{t} X_{b,t,i} (I_D)_{t,k} = \\sum_{t} X_{b,t,i} \\delta_{tk} = X_{b,k,i}$. The test asserts that the output is $X$, i.e., $(f(X,I_D))_{b,t,k} = X_{b,t,k}$. The buggy implementation yields an output where the time and feature axes have been swapped: $X_{b,k,t}$. Unless $X$ is symmetric in its last two dimensions (i.e., $X_{b,k,t} = X_{b,t,k}$), this test will fail. This test is effective.\n2.  **Time-permutation equivariance**: Let $\\Pi \\in \\mathbb{R}^{T \\times T}$ be a permutation matrix. Assert $f(X \\Pi^{\\top}, W) = (f(X,W)) \\Pi^{\\top}$. This notation implies that $\\Pi^{\\top}$ permutes the time axis of the tensor it multiplies.\n    -   For $f_{correct}$: The operation is applied independently to each time-slice $X_{b,t,:}$. Permuting the time-slices before the operation is equivalent to applying the operation and then permuting the resulting time-slices. This equivariance holds.\n    -   For $f_{bug}$: The operation involves summing over the time axis ($\\sum_t$). This summation breaks the independence between time steps. If the input time steps are permuted, the resulting sum will change in a way that is not equivalent to simply permuting the output of the original operation. The buggy implementation is not equivariant with respect to time-axis permutations. This test is also effective.\n\nBoth tests are capable of detecting the bug. However, the problem asks for a **minimal set**. Since either test is sufficient on its own, a set containing both is not minimal. Therefore, this option is suboptimal.\n**Verdict: Incorrect.**\n\n#### Evaluation of Option B\n**B. Single test: Feature-axis change-of-basis invariance.**\nThis test asserts that for any orthogonal matrix $Q \\in \\mathbb{R}^{D \\times D}$, $f(X Q, Q^{\\top} W) = f(X, W)$.\n-   For $f_{correct}$: This property describes the behavior of a linear map under a change of basis. We apply a change of basis $Q$ to the input vectors (the feature vectors along the $D$ axis of $X$) and apply the corresponding inverse transformation $Q^{-1}=Q^{\\top}$ to the matrix of the linear map $W$. The result of the overall transformation must be invariant. Let's verify:\n    $$ (f_{correct}(XQ, Q^{\\top}W))_{b,t,k} = \\sum_{i} (XQ)_{b,t,i} (Q^{\\top}W)_{i,k} $$\n    $$ = \\sum_{i} \\left(\\sum_{j} X_{b,t,j} Q_{j,i}\\right) \\left(\\sum_{l} (Q^{\\top})_{i,l} W_{l,k}\\right) $$\n    $$ = \\sum_{j,l} X_{b,t,j} W_{l,k} \\left(\\sum_{i} Q_{j,i} (Q^{\\top})_{i,l}\\right) $$\n    The term $\\sum_{i} Q_{j,i} (Q^{\\top})_{i,l}$ is the $(j,l)$-th element of the matrix product $Q Q^{\\top}$. Since $Q$ is orthogonal, $Q Q^{\\top} = I_D$, so the sum is $\\delta_{jl}$.\n    $$ = \\sum_{j,l} X_{b,t,j} W_{l,k} \\delta_{jl} = \\sum_{j} X_{b,t,j} W_{j,k} = (f_{correct}(X,W))_{b,t,k} $$\n    The test passes.\n-   For $f_{bug}$ (requires $T=D$): The change of basis $Q$ is applied to the feature axis (index $i$) of $X$. The buggy implementation, however, contracts over the time axis (index $t$). The transformation $Q$ and its inverse $Q^{\\top}$ are applied to axes that are not being contracted together.\n    $$ (f_{bug}(XQ, Q^{\\top}W))_{b,i,k} = \\sum_{t} (XQ)_{b,t,i} (Q^{\\top}W)_{t,k} $$\n    $$ = \\sum_{t} \\left(\\sum_{j} X_{b,t,j} Q_{j,i}\\right) \\left(\\sum_{l} (Q^{\\top})_{t,l} W_{l,k}\\right) $$\n    This expression does not simplify to $(f_{bug}(X, W))_{b,i,k} = \\sum_{t} X_{b,t,i} W_{t,k}$. The matrices $Q$ and $Q^\\top$ do not cancel. The test will fail for a generic choice of inputs.\n\nThis single test is sufficient to detect the axis misalignment. It correctly probes the central property of the intended operation: that it is a linear map acting on the $D$-dimensional feature space. Since it is a single, sufficient test, the set containing only this test is minimal.\n**Verdict: Correct.**", "answer": "$$\\boxed{B}$$", "id": "3143508"}, {"introduction": "Once an operation is mathematically correct, the next challenge in deep learning is making it computationally efficient. This practice moves from the abstract world of linear algebra to the concrete realities of hardware performance, exploring how tensors are stored and accessed in memory [@problem_id:3143504]. You will analyze a batched matrix multiplication, the workhorse of many neural networks, and discover how reordering tensor axes can dramatically boost performance by optimizing memory access patterns, a key skill for any practitioner working with large-scale models.", "problem": "A deep learning model performs a batched linear transformation where, for each batch index $b$, the matrix $\\mathbf{A}^{(b)}$ of shape $(n \\times k)$ multiplies the matrix $\\mathbf{B}^{(b)}$ of shape $(k \\times m)$ to produce $\\mathbf{C}^{(b)}$ of shape $(n \\times m)$. In tensor form, the inputs are $\\mathbf{A} \\in \\mathbb{R}^{b \\times n \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{b \\times k \\times m}$, and the output is $\\mathbf{C} \\in \\mathbb{R}^{b \\times n \\times m}$. The matrices are stored in row-major (C-contiguous) layout with $32$-bit floating point elements (that is, $4$ bytes per element). The hardware fetches memory in cache lines of size $64$ bytes. You must represent the batch operation as an Einstein summation contraction via the function $\\mathrm{einsum}$ and propose an optimization that reorders axes to increase arithmetic intensity (Floating Point Operations (FLOPs) per byte moved) by improving spatial locality, justified via tensor stride arithmetic.\n\nWork from first principles:\n- Use the core definition of matrix multiplication, $C_{b,i,j} = \\sum_{t=1}^{k} A_{b,i,t}\\,B_{b,t,j}$, and the definition of Einstein summation convention, where repeated indices indicate summation.\n- Use the definition of row-major strides: for a tensor with shape $(d_0, d_1, d_2)$, the stride in elements along axis $0$ is $d_1 d_2$, along axis $1$ is $d_2$, and along axis $2$ is $1$. The byte stride equals the element stride times $4$.\n- Assume the kernel computes each output element $C_{b,i,j}$ by accumulating over the reduction dimension $k$ as the innermost loop. For spatial locality, contiguous access (stride $1$ in elements) along the innermost loop uses each cache line fully, whereas a stride of at least $16$ elements (because $64$ bytes per line and $4$ bytes per element) causes each access to likely land on a distinct cache line with minimal reuse.\n\nLet the concrete sizes be $b=32$, $n=64$, $k=128$, $m=64$. Choose the option that both:\n(i) provides a correct $\\mathrm{einsum}$ contraction string that computes $\\mathbf{C}$ from $\\mathbf{A}$ and $\\mathbf{B}$, and\n(ii) proposes an axis reordering that maximizes arithmetic intensity by making the reduction dimension $k$ contiguous in memory in both inputs and making output writes contiguous along the innermost produced dimension.\n\nWhich option satisfies the above and achieves the best FLOP-to-memory ratio under the stated model?\n\nA. Use $\\mathrm{einsum}(\\texttt{'bnk,bmk->bnm'})$ after transposing $\\mathbf{B}$ to shape $(b,m,k)$, keep $\\mathbf{A}$ as $(b,n,k)$, and write $\\mathbf{C}$ as $(b,n,m)$.\n\nB. Use $\\mathrm{einsum}(\\texttt{'bnk,bkm->bnm'})$ with the original shapes $\\mathbf{A}$ as $(b,n,k)$ and $\\mathbf{B}$ as $(b,k,m)$, writing $\\mathbf{C}$ as $(b,n,m)$.\n\nC. Use $\\mathrm{einsum}(\\texttt{'bkn,bkm->bmn'})$ after transposing $\\mathbf{A}$ to $(b,k,n)$, keep $\\mathbf{B}$ as $(b,k,m)$, and write $\\mathbf{C}$ as $(b,m,n)$.\n\nD. Use $\\mathrm{einsum}(\\texttt{'bnk,bmk->bmn'})$ after transposing $\\mathbf{B}$ to $(b,m,k)$, keep $\\mathbf{A}$ as $(b,n,k)$, and write $\\mathbf{C}$ as $(b,m,n)$.", "solution": "The user wants to identify the optimal strategy for a batched matrix multiplication, $\\mathbf{C} = \\mathbf{A} \\mathbf{B}$, focusing on performance optimization through memory access patterns.\n\n### Problem Statement Validation\n\n**Step 1: Extract Givens**\n- Batched matrix multiplication: for each batch index $b$, $\\mathbf{C}^{(b)} = \\mathbf{A}^{(b)} \\mathbf{B}^{(b)}$.\n- Input Tensors: $\\mathbf{A} \\in \\mathbb{R}^{b \\times n \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{b \\times k \\times m}$.\n- Output Tensor: $\\mathbf{C} \\in \\mathbb{R}^{b \\times n \\times m}$.\n- Element-wise formula: $C_{b,i,j} = \\sum_{t=1}^{k} A_{b,i,t}\\,B_{b,t,j}$.\n- Data representation: Row-major (C-contiguous) layout, $32$-bit floating point elements ($4$ bytes/element).\n- Hardware constraint: Cache line size is $64$ bytes.\n- Kernel execution model: The innermost loop is over the reduction dimension $k$.\n- Spatial locality model: Access with an element stride of $1$ is contiguous and efficient. Access with a stride of $16$ elements (i.e., $16 \\times 4 = 64$ bytes) or more is inefficient.\n- Concrete dimensions: $b=32$, $n=64$, $k=128$, $m=64$.\n- Task: Find the option with the correct Einstein summation (`einsum`) string and the axis reordering that maximizes arithmetic intensity by (1) making the reduction dimension $k$ contiguous in memory for both inputs and (2) making output writes contiguous.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on fundamental principles of linear algebra (matrix multiplication), computer science (tensor representation, data layouts), and computer architecture (cache locality, memory strides, arithmetic intensity). These are standard and critical concepts in high-performance computing and deep learning.\n- **Well-Posed:** The problem provides a clear objective (find the best option) and a set of quantitative criteria for evaluation (stride arithmetic, correctness of `einsum`). The simplified model of cache behavior allows for a deterministic analysis leading to a unique best answer among the choices.\n- **Objective:** The problem is stated using precise, technical language with no subjective or ambiguous terms.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, objective, and contains all necessary information for a rigorous analysis. I will now proceed with the solution derivation.\n\n### Derivation and Option Analysis\n\n**1. First Principles: Strides and Baseline Performance**\n\nThe problem uses row-major memory layout. For a tensor with shape $(d_0, d_1, ..., d_{N-1})$, the stride in elements for axis $i$ is given by $S_i = \\prod_{j=i+1}^{N-1} d_j$, with $S_{N-1} = 1$. The byte stride is $4 \\times S_i$.\n\nThe operation is the batched matrix multiplication $C_{b,i,j} = \\sum_{t=1}^{k} A_{b,i,t}\\,B_{b,t,j}$. The index $t$ corresponds to the dimension of size $k$.\nThe standard `einsum` representation for this is `'bnk,bkm->bnm'`. This corresponds to Option B, which we will analyze as the baseline.\n\n**Baseline Analysis (Original Layout):**\n- $\\mathbf{A}$ has shape $(b, n, k) = (32, 64, 128)$. Its element strides are $(n \\times k, k, 1) = (64 \\times 128, 128, 1) = (8192, 128, 1)$.\n- $\\mathbf{B}$ has shape $(b, k, m) = (32, 128, 64)$. Its element strides are $(k \\times m, m, 1) = (128 \\times 64, 64, 1) = (8192, 64, 1)$.\n\nThe kernel's innermost loop iterates over the reduction dimension $k$.\n- Accessing $A_{b,i,t}$: As $t$ increments, we access elements along the last dimension of $\\mathbf{A}$. The stride is $1$. This is contiguous and demonstrates excellent spatial locality.\n- Accessing $B_{b,t,j}$: As $t$ increments, we access elements along the middle dimension of $\\mathbf{B}$. The stride is the stride of the $k$-dimension, which is $m=64$ elements. Since $64 > 16$ (the number of elements per cache line), each memory access is likely to be in a different cache line, leading to very poor cache utilization. This is the performance bottleneck.\n\n**2. Optimization Strategy**\n\nTo satisfy the optimization goal, the reduction dimension $k$ must be the last dimension (stride $1$) for both input tensors.\n- $\\mathbf{A}$ already has shape $(b,n,k)$, so its 'k' dimension is already the last one. We keep it as is.\n- $\\mathbf{B}$ has shape $(b,k,m)$. To make 'k' the last dimension, we must transpose the last two axes, creating a new tensor $\\mathbf{B'}$ with shape $(b,m,k)$. This new tensor will be C-contiguous with its new shape.\n- The elements of $\\mathbf{B'}$ are related to $\\mathbf{B}$ by $B'_{b,j,t} = B_{b,t,j}$, where $j$ is the index for dimension $m$ and $t$ is the index for dimension $k$.\n- The multiplication formula becomes: $C_{b,i,j} = \\sum_{t=1}^{k} A_{b,i,t}\\,B'_{b,j,t}$.\n\nThe `einsum` string for this new operation involves inputs with indices corresponding to `bnk` for $\\mathbf{A}$ and `bmk` for $\\mathbf{B'}$. The summation is over `k`, and the output indices are `b`, `n`, `m`. This leads to the string `'bnk,bmk->bnm'`.\n\n**3. Option-by-Option Analysis**\n\n**A. Use $\\mathrm{einsum}(\\texttt{'bnk,bmk->bnm'})$ after transposing $\\mathbf{B}$ to shape $(b,m,k)$, keep $\\mathbf{A}$ as $(b,n,k)$, and write $\\mathbf{C}$ as $(b,n,m)$.**\n- **Correctness:** As derived above, transposing $\\mathbf{B}$ to $\\mathbf{B'}$ of shape $(b,m,k)$ and using the `einsum` string `'bnk,bmk->bnm'` correctly computes $C_{b,i,j} = \\sum_{t} A_{b,i,t}B'_{b,j,t} = \\sum_{t} A_{b,i,t}B_{b,t,j}$. The output shape $(b,n,m)$ is also correct.\n- **Performance:**\n    - Input $\\mathbf{A}$ (shape $(b,n,k)$): Accessing over `k` has stride $1$. Optimal.\n    - Input $\\mathbf{B'}$ (shape $(b,m,k)$): Accessing over `k` has stride $1$. Optimal.\n    - Output $\\mathbf{C}$ (shape $(b,n,m)$): A loop nest `for b, for n, for m, ...` will compute the output. Writing to $C_{b,n,m}$ while incrementing $m$ means accessing memory with stride $1$. Optimal.\n- **Verdict:** **Correct**. This option satisfies all stated requirements and achieves the desired optimization.\n\n**B. Use $\\mathrm{einsum}(\\texttt{'bnk,bkm->bnm'})$ with the original shapes $\\mathbf{A}$ as $(b,n,k)$ and $\\mathbf{B}$ as $(b,k,m)$, writing $\\mathbf{C}$ as $(b,n,m)$.**\n- **Correctness:** The `einsum` string is mathematically correct for the original tensor shapes.\n- **Performance:** This is the baseline case analyzed earlier. The access pattern for $\\mathbf{B}$ involves a large stride of $m=64$, which is highly inefficient. It does not meet the optimization goal.\n- **Verdict:** **Incorrect**. It fails to perform the optimization.\n\n**C. Use $\\mathrm{einsum}(\\texttt{'bkn,bkm->bmn'})$ after transposing $\\mathbf{A}$ to $(b,k,n)$, keep $\\mathbf{B}$ as $(b,k,m)$, and write $\\mathbf{C}$ as $(b,m,n)$.**\n- **Correctness:** This option computes $C'_{b,j,i} = \\sum_t A'_{b,t,i} B_{b,t,j}$. With $A'_{b,t,i} = A_{b,i,t}$, this becomes $C'_{b,j,i} = \\sum_t A_{b,i,t} B_{b,t,j} = C_{b,i,j}$. The operation computes the transpose of the desired matrix, $\\mathbf{C}^T$. This is a valid computational path.\n- **Performance:**\n    - Input $\\mathbf{A'}$ (shape $(b,k,n)$): Strides are $(k \\times n, n, 1)$. Accessing over the reduction dimension `k` (the middle dimension) has a stride of $n=64$. Inefficient.\n    - Input $\\mathbf{B}$ (shape $(b,k,m)$): Strides are $(k \\times m, m, 1)$. Accessing over `k` has a stride of $m=64$. Inefficient.\n- **Verdict:** **Incorrect**. The proposed reordering leads to poor memory access patterns for both inputs.\n\n**D. Use $\\mathrm{einsum}(\\texttt{'bnk,bmk->bmn'})$ after transposing $\\mathbf{B}$ to $(b,m,k)$, keep $\\mathbf{A}$ as $(b,n,k)$, and write $\\mathbf{C}$ as $(b,m,n)$.**\n- **Correctness:** This uses the same optimized inputs as Option A. The `einsum` computes $C'_{b,j,i} = \\sum_t A_{b,i,t}B'_{b,j,t} = C_{b,i,j}$. The output shape $(b,m,n)$ indicates it produces the transpose of the desired matrix, $\\mathbf{C}^T$.\n- **Performance:**\n    - Input access for $\\mathbf{A}$ and $\\mathbf{B'}$ is optimal (stride $1$), same as in Option A.\n    - Output access for $\\mathbf{C'}$ (shape $(b,m,n)$) is also optimal (stride $1$) for a `for m, for n, ...` loop structure.\n- **Comparison to A:** Both A and D have optimal memory access patterns within the compute kernel. However, the problem defines the final output as $\\mathbf{C} \\in \\mathbb{R}^{b \\times n \\times m}$. Option A produces this resultant tensor directly. Option D produces $\\mathbf{C}^T$, which would necessitate an additional transpose operation to obtain the specified output format, incurring extra cost. Therefore, Option A is the more direct and efficient end-to-end solution.\n- **Verdict:** **Incorrect**. While the core computation is optimized, it does not produce the final output in its specified native shape, making it an inferior strategy compared to Option A.\n\n### Conclusion\n\nOption A provides the correct `einsum` string for the optimized memory layout that directly produces the output tensor $\\mathbf{C}$ in its specified shape $(b,n,m)$. It achieves optimal spatial locality for both input reads (stride $1$ on the reduction dimension) and output writes (stride $1$ on the innermost produced dimension).", "answer": "$$\\boxed{A}$$", "id": "3143504"}, {"introduction": "A deep understanding of linear algebra allows us not only to build and train models, but also to probe their vulnerabilities. This exercise demonstrates how the gradient, a fundamental rank-1 tensor, can be wielded to create adversarial examples—inputs intentionally designed to fool a model [@problem_id:3143535]. By solving for the smallest perturbation that maximally alters a model's output, you will engage with the geometric relationship between gradients and different vector norms, revealing fascinating insights into the sensitivity and landscape of high-dimensional functions.", "problem": "You are given a scalar-valued, differentiable function $f:\\mathbb{R}^n\\to\\mathbb{R}$ and a point $x\\in\\mathbb{R}^n$. Let the gradient at $x$ be $g=\\nabla_x f(x)\\in\\mathbb{R}^n$. The first-order Taylor approximation around $x$ is $f(x+\\delta)\\approx f(x)+g^\\top \\delta$ for small $\\delta\\in\\mathbb{R}^n$. Consider constructing adversarial perturbations $\\delta$ that change the linearized output by a prescribed amount $g^\\top \\delta=\\alpha$, where $\\alpha\\in\\mathbb{R}$ is given. You will compare minimal-norm solutions under the Euclidean norm ($L_2$) and the maximum norm ($L_\\infty$), and interpret the direction of steepest change via the Singular Value Decomposition (SVD).\n\nFundamental base for derivation:\n- Norm definition: for $p\\in[1,\\infty]$, the $L_p$ norm is $\\lVert v\\rVert_p=\\left(\\sum_{i=1}^n |v_i|^p\\right)^{1/p}$ for $p<\\infty$, and $\\lVert v\\rVert_\\infty=\\max_i |v_i|$.\n- Dual norm definition: for a given norm $\\lVert\\cdot\\rVert$, its dual norm $\\lVert\\cdot\\rVert_\\ast$ is $\\lVert w\\rVert_\\ast=\\sup\\{w^\\top v:\\lVert v\\rVert\\le 1\\}$.\n- Singular Value Decomposition (SVD): for a matrix $J\\in\\mathbb{R}^{m\\times n}$, the SVD is $J=U\\Sigma V^\\top$, where $U\\in\\mathbb{R}^{m\\times m}$ and $V\\in\\mathbb{R}^{n\\times n}$ are orthogonal, and $\\Sigma\\in\\mathbb{R}^{m\\times n}$ is diagonal with nonnegative singular values in nonincreasing order. For $m=1$ and $J=g^\\top\\in\\mathbb{R}^{1\\times n}$, the leading right singular vector equals the normalized gradient direction.\n\nTask:\n- For each test case defined below, assume the scalar-output, linearized model $f(x+\\delta)\\approx f(x)+g^\\top\\delta$ with $g\\in\\mathbb{R}^n$ given and with target change $\\alpha\\in\\mathbb{R}$ specified.\n- Construct a perturbation $\\delta_2$ that has minimal $L_2$ norm among all $\\delta$ satisfying $g^\\top\\delta=\\alpha$, and a perturbation $\\delta_\\infty$ that has minimal $L_\\infty$ norm among all $\\delta$ satisfying $g^\\top\\delta=\\alpha$.\n- Report the minimal norms $n_2=\\lVert\\delta_2\\rVert_2$ and $n_\\infty=\\lVert\\delta_\\infty\\rVert_\\infty$, and the ratio $r=n_2/n_\\infty$.\n- Verify numerically that $g^\\top\\delta_2$ and $g^\\top\\delta_\\infty$ equal $\\alpha$ up to tolerance $\\varepsilon=10^{-9}$, and report boolean flags indicating whether each equality holds within tolerance.\n- Special cases must be handled as follows:\n  - If $\\alpha=0$, then the zero perturbation is feasible and minimal for both norms; define the ratio $r$ to be not-a-number.\n  - If $\\lVert g\\rVert_2=0$ and $\\alpha\\ne 0$, then no finite perturbation can change the linearized output; define both minimal norms to be infinite, the ratio to be not-a-number, and both verification flags to be false.\n\nTest suite:\n- Case $1$: $g=(3,4)$, $\\alpha=1$.\n- Case $2$: $g=(1,-1,1,-1)$, $\\alpha=2$.\n- Case $3$: $g=(0,0,0)$, $\\alpha=1$.\n- Case $4$: $g=(-2,0.5)$, $\\alpha=0$.\n- Case $5$: $g=(0,-3,4,0,12)$, $\\alpha=5$.\n\nRequired final output format:\n- Your program should produce a single line of output that is a list of results, one per test case, in the same order as above. Each result must itself be a list with five entries: the three floats $[n_2,n_\\infty,r]$ followed by two booleans indicating the verification outcomes for $g^\\top\\delta_2$ and $g^\\top\\delta_\\infty$ respectively, each checked against $\\alpha$ within tolerance $\\varepsilon=10^{-9}$.\n- All float outputs must be rounded to exactly six decimal places; use the canonical string representations \"inf\" and \"nan\" for infinite and not-a-number values. The booleans should appear as either True or False.\n- The final output should therefore look like a single list of lists, for example $[\\,[\\cdots],\\,[\\cdots],\\,\\ldots\\,]$, printed as one line with no extra text.", "solution": "The core of the problem is to find a vector $\\delta \\in \\mathbb{R}^n$ that satisfies the linear constraint $g^\\top \\delta = \\alpha$ for a given gradient vector $g \\in \\mathbb{R}^n$ and a scalar target change $\\alpha \\in \\mathbb{R}$, while minimizing a specified norm of $\\delta$. We will address this for the $L_2$ and $L_\\infty$ norms.\n\nFirst, we address the special cases defined in the problem statement.\n1.  If $\\alpha = 0$, the constraint is $g^\\top \\delta = 0$. The perturbation $\\delta = 0$ satisfies this constraint. Since the norm of the zero vector is $0$ for any $L_p$ norm ($p \\ge 1$), and norms are non-negative, $\\delta=0$ is the unique minimal-norm solution. Therefore, $n_2 = \\lVert 0 \\rVert_2 = 0$ and $n_\\infty = \\lVert 0 \\rVert_\\infty = 0$. The ratio $r = n_2 / n_\\infty = 0/0$ is indeterminate and is defined as not-a-number (NaN).\n2.  If $g = 0$ (i.e., $\\lVert g \\rVert_2 = 0$) and $\\alpha \\ne 0$, the constraint becomes $0^\\top \\delta = 0 = \\alpha$, which is a contradiction. No such perturbation $\\delta$ can exist. In this context, the minimum norm required to satisfy the constraint is infinite. Thus, $n_2 = \\infty$, $n_\\infty = \\infty$, and the ratio $r$ is defined as NaN. The verification fails as the constraint can never be met.\n\nNow, we consider the general case where $g \\ne 0$ and $\\alpha \\ne 0$.\n\n**Minimal $L_2$ Norm Perturbation ($\\delta_2$)**\n\nThe optimization problem is to minimize $\\lVert \\delta \\rVert_2$ subject to $g^\\top \\delta = \\alpha$. Minimizing $\\lVert \\delta \\rVert_2$ is equivalent to minimizing $\\frac{1}{2}\\lVert \\delta \\rVert_2^2 = \\frac{1}{2}\\delta^\\top \\delta$. We use the method of Lagrange multipliers. The Lagrangian function is:\n$$ \\mathcal{L}(\\delta, \\lambda) = \\frac{1}{2}\\delta^\\top\\delta - \\lambda(g^\\top\\delta - \\alpha) $$\nTo find the minimum, we set the gradient of $\\mathcal{L}$ with respect to $\\delta$ to zero:\n$$ \\nabla_\\delta \\mathcal{L} = \\delta - \\lambda g = 0 \\implies \\delta = \\lambda g $$\nThis indicates that the optimal perturbation $\\delta_2$ must be collinear with the gradient vector $g$. This direction is that of the steepest ascent for the linearized function $f(x) + g^\\top\\delta$. As mentioned in the problem's base for derivation, for the Jacobian $J=g^\\top \\in \\mathbb{R}^{1\\times n}$, its single singular value is $\\sigma_1 = \\lVert g \\rVert_2$ and the corresponding right singular vector is $v_1 = g / \\lVert g \\rVert_2$. Thus, the optimal perturbation direction is aligned with the principal right singular vector of the system's Jacobian.\n\nTo find the scalar multiplier $\\lambda$, we substitute $\\delta = \\lambda g$ back into the constraint:\n$$ g^\\top (\\lambda g) = \\alpha \\implies \\lambda (g^\\top g) = \\alpha \\implies \\lambda \\lVert g \\rVert_2^2 = \\alpha $$\nSolving for $\\lambda$ (since $g \\ne 0$, $\\lVert g \\rVert_2^2 > 0$):\n$$ \\lambda = \\frac{\\alpha}{\\lVert g \\rVert_2^2} $$\nThe minimal $L_2$ norm perturbation is therefore:\n$$ \\delta_2 = \\frac{\\alpha}{\\lVert g \\rVert_2^2} g $$\nThe magnitude of this minimal norm, $n_2$, is:\n$$ n_2 = \\lVert \\delta_2 \\rVert_2 = \\left\\lVert \\frac{\\alpha}{\\lVert g \\rVert_2^2} g \\right\\rVert_2 = \\left| \\frac{\\alpha}{\\lVert g \\rVert_2^2} \\right| \\lVert g \\rVert_2 = \\frac{|\\alpha|}{\\lVert g \\rVert_2} $$\n\n**Minimal $L_\\infty$ Norm Perturbation ($\\delta_\\infty$)**\n\nThe optimization problem is to minimize $\\lVert \\delta \\rVert_\\infty$ subject to $g^\\top \\delta = \\alpha$. We can establish a lower bound on $\\lVert \\delta \\rVert_\\infty$ using Hölder's inequality, which states $|w^\\top v| \\le \\lVert w \\rVert_p \\lVert v \\rVert_q$ for dual norms where $1/p + 1/q = 1$. The dual norm of the $L_\\infty$ norm is the $L_1$ norm. Applying this to our constraint:\n$$ |\\alpha| = |g^\\top \\delta| \\le \\lVert g \\rVert_1 \\lVert \\delta \\rVert_\\infty $$\nSince $g \\ne 0$, $\\lVert g \\rVert_1 > 0$. We can rearrange to find a lower bound on $\\lVert \\delta \\rVert_\\infty$:\n$$ \\lVert \\delta \\rVert_\\infty \\ge \\frac{|\\alpha|}{\\lVert g \\rVert_1} $$\nThis lower bound is achievable. Equality in Hölder's inequality for the $(1, \\infty)$ norm pair holds if the perturbation $\\delta$ is chosen appropriately. Specifically, we can construct a perturbation $\\delta_\\infty$ that meets this bound. Let's define $\\delta_\\infty$ as:\n$$ \\delta_\\infty = c \\cdot \\mathrm{sgn}(g) $$\nwhere $\\mathrm{sgn}(g)$ is a vector whose components are the signs of the components of $g$ ($\\mathrm{sgn}(z)=1$ if $z>0$, $-1$ if $z<0$, $0$ if $z=0$). Substituting this into the constraint:\n$$ g^\\top (c \\cdot \\mathrm{sgn}(g)) = \\alpha \\implies c \\sum_{i=1}^n g_i \\mathrm{sgn}(g_i) = \\alpha $$\nSince $g_i \\mathrm{sgn}(g_i) = |g_i|$, this simplifies to:\n$$ c \\sum_{i=1}^n |g_i| = \\alpha \\implies c \\lVert g \\rVert_1 = \\alpha \\implies c = \\frac{\\alpha}{\\lVert g \\rVert_1} $$\nSo, the minimal $L_\\infty$ norm perturbation is:\n$$ \\delta_\\infty = \\frac{\\alpha}{\\lVert g \\rVert_1} \\mathrm{sgn}(g) $$\nThe magnitude of this minimal norm, $n_\\infty$, is:\n$$ n_\\infty = \\lVert \\delta_\\infty \\rVert_\\infty = \\left\\lVert \\frac{\\alpha}{\\lVert g \\rVert_1} \\mathrm{sgn}(g) \\right\\rVert_\\infty = \\left| \\frac{\\alpha}{\\lVert g \\rVert_1} \\right| \\lVert \\mathrm{sgn}(g) \\rVert_\\infty $$\nFor any $g \\ne 0$, at least one component of $\\mathrm{sgn}(g)$ is $\\pm 1$, so $\\lVert \\mathrm{sgn}(g) \\rVert_\\infty = 1$. Thus, the minimal norm is:\n$$ n_\\infty = \\frac{|\\alpha|}{\\lVert g \\rVert_1} $$\n\n**Ratio of Norms**\n\nThe ratio $r$ of the minimal norms is:\n$$ r = \\frac{n_2}{n_\\infty} = \\frac{|\\alpha| / \\lVert g \\rVert_2}{|\\alpha| / \\lVert g \\rVert_1} = \\frac{\\lVert g \\rVert_1}{\\lVert g \\rVert_2} $$\nThis ratio depends only on the properties of the gradient vector $g$.\n\n**Summary for Implementation**\nFor each test case $(g, \\alpha)$:\n1.  If $\\alpha = 0$, set $n_2=0$, $n_\\infty=0$, $r=\\text{NaN}$. The perturbations are $\\delta_2 = \\delta_\\infty = 0$.\n2.  If $\\alpha \\ne 0$, calculate $\\lVert g \\rVert_2$. If $\\lVert g \\rVert_2 = 0$, set $n_2=\\infty$, $n_\\infty=\\infty$, $r=\\text{NaN}$, and verification flags to False.\n3.  If $\\alpha \\ne 0$ and $\\lVert g \\rVert_2 \\ne 0$, calculate $\\lVert g \\rVert_1$. Compute $n_2 = |\\alpha|/\\lVert g \\rVert_2$, $n_\\infty = |\\alpha|/\\lVert g \\rVert_1$, and $r = \\lVert g \\rVert_1/\\lVert g \\rVert_2$. Construct $\\delta_2$ and $\\delta_\\infty$ using the derived formulas to perform verification checks.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes minimal L2 and L_inf norm perturbations for a set of test cases.\n    \"\"\"\n    test_cases = [\n        (np.array([3.0, 4.0]), 1.0),\n        (np.array([1.0, -1.0, 1.0, -1.0]), 2.0),\n        (np.array([0.0, 0.0, 0.0]), 1.0),\n        (np.array([-2.0, 0.5]), 0.0),\n        (np.array([0.0, -3.0, 4.0, 0.0, 12.0]), 5.0),\n    ]\n\n    results = []\n    epsilon = 1e-9\n\n    for g, alpha in test_cases:\n        norm_g_2 = np.linalg.norm(g, 2)\n        \n        # Handle special case: alpha = 0\n        if np.isclose(alpha, 0.0):\n            n2 = 0.0\n            n_inf = 0.0\n            r = np.nan\n            delta2 = np.zeros_like(g, dtype=float)\n            delta_inf = np.zeros_like(g, dtype=float)\n            verif2 = np.isclose(np.dot(g, delta2), alpha, atol=epsilon)\n            verif_inf = np.isclose(np.dot(g, delta_inf), alpha, atol=epsilon)\n\n        # Handle special case: g = 0 and alpha != 0\n        elif np.isclose(norm_g_2, 0.0):\n            n2 = np.inf\n            n_inf = np.inf\n            r = np.nan\n            verif2 = False\n            verif_inf = False\n        \n        # General case: g != 0 and alpha != 0\n        else:\n            norm_g_1 = np.linalg.norm(g, 1)\n            \n            n2 = abs(alpha) / norm_g_2\n            n_inf = abs(alpha) / norm_g_1\n            r = norm_g_1 / norm_g_2\n\n            # Perturbation for L2 minimization\n            delta2 = (alpha / (norm_g_2**2)) * g\n            # Perturbation for L_inf minimization\n            delta_inf = (alpha / norm_g_1) * np.sign(g)\n\n            # Verification\n            verif2 = np.isclose(np.dot(g, delta2), alpha, atol=epsilon)\n            verif_inf = np.isclose(np.dot(g, delta_inf), alpha, atol=epsilon)\n\n        results.append([n2, n_inf, r, bool(verif2), bool(verif_inf)])\n\n    def format_float(val):\n        \"\"\"Formats a float to 6 decimal places, handling inf and nan.\"\"\"\n        if np.isinf(val):\n            return \"inf\"\n        if np.isnan(val):\n            return \"nan\"\n        return f\"{val:.6f}\"\n\n    output_str_parts = []\n    for res in results:\n        n2_str = format_float(res[0])\n        n_inf_str = format_float(res[1])\n        r_str = format_float(res[2])\n        v2_str = str(res[3])\n        v_inf_str = str(res[4])\n        output_str_parts.append(f\"[{n2_str},{n_inf_str},{r_str},{v2_str},{v_inf_str}]\")\n    \n    final_output_str = f\"[{','.join(output_str_parts)}]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "3143535"}]}