{"hands_on_practices": [{"introduction": "In theory, optimization with gradients requires differentiable functions. In practice, many useful operations in deep learning, like quantization, are not. This exercise [@problem_id:3162528] confronts this issue head-on by exploring the Straight-Through Estimator (STE). You will compute both the 'true' mathematical gradient and the STE approximation, revealing precisely how this essential technique creates a useful, albeit biased, learning signal where calculus alone would leave us stranded.", "problem": "Consider a single-parameter linear predictor with a pre-quantized parameter $\\tilde{w} \\in \\mathbb{R}$ and a quantized parameter $w \\in \\mathbb{Z}$ given by $w=\\mathrm{round}(\\tilde{w})$, where $\\mathrm{round}(\\cdot)$ denotes rounding to the nearest integer. The model output for input $x$ is $f(x)=w\\,x$. You train on a single deterministic training pair $(x,y)$ with $x=\\frac{3}{2}$ and $y=\\frac{5}{4}$ using the squared-error loss\n$$\nL(\\tilde{w})=\\big(w\\,x-y\\big)^{2}.\n$$\nDuring backpropagation, adopt the straight-through estimator (STE), that is, approximate the Jacobian $\\frac{\\partial w}{\\partial \\tilde{w}}$ by $1$ wherever needed.\n\nWork at any $\\tilde{w}$ in the open interval $\\big(\\frac{1}{2},\\frac{3}{2}\\big)$, where $w$ is constant. Starting only from the definition of partial derivatives, the chain rule, and the definition of the squared-error loss, do the following:\n\n- Compute the STE-based partial derivative $\\frac{\\partial L}{\\partial \\tilde{w}}$ at such a $\\tilde{w}$.\n- Using the exact, quantized objective $L(\\tilde{w})=\\big(\\mathrm{round}(\\tilde{w})\\,x-y\\big)^{2}$, determine the true partial derivative $\\frac{\\partial L}{\\partial \\tilde{w}}$ at such a $\\tilde{w}$.\n- Define the bias of the STE gradient at such a $\\tilde{w}$ as $B=\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}}-\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}$. Compute $B$ exactly.\n\nYour final answer must be a single exact number for $B$ with no rounding.", "solution": "The problem asks for the computation of the bias of the straight-through estimator (STE) for the gradient of a loss function with respect to a pre-quantized parameter. We are given the loss function $L(\\tilde{w})=\\big(w\\,x-y\\big)^{2}$, where $w = \\mathrm{round}(\\tilde{w})$. The bias is defined as $B=\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}}-\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}$. We must evaluate this for $\\tilde{w} \\in \\big(\\frac{1}{2},\\frac{3}{2}\\big)$, with the specific training data $x=\\frac{3}{2}$ and $y=\\frac{5}{4}$.\n\nFirst, we compute the STE-based partial derivative, $\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}}$.\nThe loss function $L$ is a composition of functions of $\\tilde{w}$. We apply the chain rule to find its derivative with respect to $\\tilde{w}$:\n$$\n\\frac{\\partial L}{\\partial \\tilde{w}} = \\frac{\\partial L}{\\partial w} \\frac{\\partial w}{\\partial \\tilde{w}}\n$$\nLet's compute the partial derivative of $L$ with respect to $w$:\n$$\n\\frac{\\partial L}{\\partial w} = \\frac{\\partial}{\\partial w} \\big((w\\,x-y)^{2}\\big) = 2(w\\,x-y) \\cdot \\frac{\\partial}{\\partial w}(w\\,x-y) = 2(w\\,x-y)x\n$$\nSubstituting this back into the chain rule expression gives:\n$$\n\\frac{\\partial L}{\\partial \\tilde{w}} = 2x(w\\,x-y) \\frac{\\partial w}{\\partial \\tilde{w}}\n$$\nThe straight-through estimator (STE) approximates the derivative of the quantization function, $\\frac{\\partial w}{\\partial \\tilde{w}}$, as $1$.\n$$\n\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}} = 2x(w\\,x-y) \\cdot 1 = 2x(w\\,x-y)\n$$\nThe problem specifies that we evaluate this derivative for any $\\tilde{w}$ in the open interval $\\big(\\frac{1}{2}, \\frac{3}{2}\\big)$, which is the interval $(0.5, 1.5)$. For any value of $\\tilde{w}$ in this interval, the `round` function yields the integer $1$. Thus, $w = \\mathrm{round}(\\tilde{w}) = 1$.\nNow we substitute the given values $x=\\frac{3}{2}$ and $y=\\frac{5}{4}$, along with $w=1$:\n$$\n\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}} = 2\\left(\\frac{3}{2}\\right)\\left( (1)\\left(\\frac{3}{2}\\right) - \\frac{5}{4} \\right) = 3\\left(\\frac{6}{4} - \\frac{5}{4}\\right) = 3\\left(\\frac{1}{4}\\right) = \\frac{3}{4}\n$$\nSo, the STE-based gradient is $\\frac{3}{4}$.\n\nNext, we compute the true partial derivative, $\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}$.\nThe loss function is $L(\\tilde{w})=\\big(\\mathrm{round}(\\tilde{w})\\,x-y\\big)^{2}$.\nAs established before, for any $\\tilde{w}$ in the open interval $\\big(\\frac{1}{2}, \\frac{3}{2}\\big)$, the value of $\\mathrm{round}(\\tilde{w})$ is constant and equal to $1$.\nTherefore, for $\\tilde{w} \\in \\big(\\frac{1}{2}, \\frac{3}{2}\\big)$, the loss function simplifies to:\n$$\nL(\\tilde{w}) = \\big((1)x-y\\big)^{2}\n$$\nSubstituting the constant values $x=\\frac{3}{2}$ and $y=\\frac{5}{4}$:\n$$\nL(\\tilde{w}) = \\left(\\frac{3}{2} - \\frac{5}{4}\\right)^{2} = \\left(\\frac{6}{4} - \\frac{5}{4}\\right)^{2} = \\left(\\frac{1}{4}\\right)^{2} = \\frac{1}{16}\n$$\nIn the specified interval, $L(\\tilde{w})$ is a constant function of $\\tilde{w}$. The derivative of any constant function is $0$.\nTherefore, the true partial derivative is:\n$$\n\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}} = \\frac{\\partial}{\\partial \\tilde{w}}\\left(\\frac{1}{16}\\right) = 0\n$$\nThis is consistent with the fact that the derivative of the `round` function is $0$ everywhere except at the points of discontinuity (i.e., at values $\\tilde{w}=n+0.5$ for any integer $n$), which are not included in the open interval $\\big(\\frac{1}{2}, \\frac{3}{2}\\big)$.\n\nFinally, we compute the bias $B$.\nThe bias is defined as the difference between the STE-based gradient and the true gradient:\n$$\nB = \\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}} - \\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}\n$$\nSubstituting the values we computed:\n$$\nB = \\frac{3}{4} - 0 = \\frac{3}{4}\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "3162528"}, {"introduction": "The gradient vector is the engine of optimization, and the loss function is its fuel. This exercise [@problem_id:3162520] provides a hands-on comparison of two fundamental loss functions: Mean Squared Error ($L_{\\mathrm{MSE}}$) and Mean Absolute Error ($L_{\\mathrm{MAE}}$). By calculating and comparing their gradients in a scenario with an outlier, you will develop a practical intuition for how their different mathematical properties make one sensitive to large errors and the other more robust, a crucial consideration in model design.", "problem": "Consider a single linear neuron used for regression with two real-valued features, defined by the parametric model $\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x}$ where $\\mathbf{w} \\in \\mathbb{R}^{2}$ and $\\mathbf{x} \\in \\mathbb{R}^{2}$. You are given a mini-batch of $3$ training examples:\n- Example $1$: $\\mathbf{x}^{(1)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$ with target $y^{(1)} = 1$,\n- Example $2$: $\\mathbf{x}^{(2)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$ with target $y^{(2)} = 1$,\n- Example $3$ (an outlier in its target): $\\mathbf{x}^{(3)} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ with target $y^{(3)} = -10$.\n\nAssume the current parameter is $\\mathbf{w} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$. Define the mean squared error (MSE) and mean absolute error (MAE) over the mini-batch as\n$$\nL_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^{2}, \n\\quad\nL_{\\mathrm{MAE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left|\\hat{y}^{(i)} - y^{(i)}\\right|.\n$$\nUsing only the base definitions of partial derivatives and the gradient vector, compute the batch gradient with respect to $\\mathbf{w}$ under each loss at the given $\\mathbf{w}$, and then compute the angle between these two gradient vectors. Express the final answer as a single exact closed-form expression for the angle in radians (do not approximate). State your final answer in radians (no degree conversion and no numerical rounding).", "solution": "The model is a linear neuron, so its prediction $\\hat{y}$ for an input vector $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ with weights $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$ is given by $\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x} = w_1 x_1 + w_2 x_2$.\n\nThe task requires computing the gradients of the MSE and MAE loss functions at the specific weight vector $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nFirst, we compute the model's predictions for each example in the mini-batch at $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$:\n$\\hat{y}^{(i)} = \\mathbf{w}^{\\top}\\mathbf{x}^{(i)} = \\begin{pmatrix} 0  0 \\end{pmatrix} \\begin{pmatrix} x_1^{(i)} \\\\ x_2^{(i)} \\end{pmatrix} = 0$ for all $i \\in \\{1, 2, 3\\}$.\n\nNext, we calculate the errors $(\\hat{y}^{(i)} - y^{(i)})$ for each example:\n- Error for example $1$: $e_1 = \\hat{y}^{(1)} - y^{(1)} = 0 - 1 = -1$.\n- Error for example $2$: $e_2 = \\hat{y}^{(2)} - y^{(2)} = 0 - 1 = -1$.\n- Error for example $3$: $e_3 = \\hat{y}^{(3)} - y^{(3)} = 0 - (-10) = 10$.\n\n**1. Gradient of the Mean Squared Error (MSE) Loss**\n\nThe MSE loss is $L_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left(\\mathbf{w}^{\\top}\\mathbf{x}^{(i)} - y^{(i)}\\right)^{2}$.\nThe gradient of $L_{\\mathrm{MSE}}$ with respect to the weight vector $\\mathbf{w}$ is given by:\n$$ \\nabla_{\\mathbf{w}} L_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{\\partial}{\\partial \\mathbf{w}} \\left( \\frac{1}{3}\\sum_{i=1}^{3}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^{2} \\right) = \\frac{1}{3}\\sum_{i=1}^{3} 2\\left(\\hat{y}^{(i)} - y^{(i)}\\right) \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mathbf{w}} $$\nSince $\\hat{y}^{(i)} = \\mathbf{w}^{\\top}\\mathbf{x}^{(i)}$, its gradient with respect to $\\mathbf{w}$ is $\\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mathbf{w}} = \\mathbf{x}^{(i)}$.\nThus, the gradient of the MSE loss is:\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\nabla_{\\mathbf{w}} L_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{2}{3}\\sum_{i=1}^{3}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)\\mathbf{x}^{(i)} $$\nEvaluating this at $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ using the pre-calculated errors:\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\frac{2}{3} \\left( (-1)\\mathbf{x}^{(1)} + (-1)\\mathbf{x}^{(2)} + (10)\\mathbf{x}^{(3)} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\frac{2}{3} \\left( (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (10)\\begin{pmatrix}1 \\\\ 0\\end{pmatrix} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\frac{2}{3} \\left( \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}10 \\\\ 0\\end{pmatrix} \\right) = \\frac{2}{3} \\begin{pmatrix}10 \\\\ -2\\end{pmatrix} = \\begin{pmatrix} \\frac{20}{3} \\\\ -\\frac{4}{3} \\end{pmatrix} $$\n\n**2. Gradient of the Mean Absolute Error (MAE) Loss**\n\nThe MAE loss is $L_{\\mathrm{MAE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left|\\mathbf{w}^{\\top}\\mathbf{x}^{(i)} - y^{(i)}\\right|$.\nThe derivative of the absolute value function $|u|$ is the sign function, $\\text{sgn}(u)$, which is defined for $u \\neq 0$. Since all errors $e_i$ are non-zero ($-1, -1, 10$), the gradient of $L_{\\mathrm{MAE}}$ is well-defined at this point.\nThe gradient of $L_{\\mathrm{MAE}}$ with respect to $\\mathbf{w}$ is:\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\nabla_{\\mathbf{w}} L_{\\mathrm{MAE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3} \\text{sgn}\\left(\\hat{y}^{(i)} - y^{(i)}\\right) \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mathbf{w}} = \\frac{1}{3}\\sum_{i=1}^{3} \\text{sgn}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)\\mathbf{x}^{(i)} $$\nWe need the signs of the errors:\n- $\\text{sgn}(e_1) = \\text{sgn}(-1) = -1$.\n- $\\text{sgn}(e_2) = \\text{sgn}(-1) = -1$.\n- $\\text{sgn}(e_3) = \\text{sgn}(10) = 1$.\nEvaluating the gradient at $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$:\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\frac{1}{3} \\left( (-1)\\mathbf{x}^{(1)} + (-1)\\mathbf{x}^{(2)} + (1)\\mathbf{x}^{(3)} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\frac{1}{3} \\left( (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (1)\\begin{pmatrix}1 \\\\ 0\\end{pmatrix} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\frac{1}{3} \\left( \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix}1 \\\\ -2\\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \\\\ -\\frac{2}{3} \\end{pmatrix} $$\n\n**3. Angle between the Gradient Vectors**\n\nLet $\\theta$ be the angle between the two gradient vectors $\\mathbf{g}_{\\mathrm{MSE}}$ and $\\mathbf{g}_{\\mathrm{MAE}}$. The angle is given by the formula:\n$$ \\theta = \\arccos\\left(\\frac{\\mathbf{g}_{\\mathrm{MSE}} \\cdot \\mathbf{g}_{\\mathrm{MAE}}}{\\|\\mathbf{g}_{\\mathrm{MSE}}\\| \\|\\mathbf{g}_{\\mathrm{MAE}}\\|}\\right) $$\nWe compute the components of this formula:\n- **Dot Product**:\n$$ \\mathbf{g}_{\\mathrm{MSE}} \\cdot \\mathbf{g}_{\\mathrm{MAE}} = \\begin{pmatrix} \\frac{20}{3} \\\\ -\\frac{4}{3} \\end{pmatrix} \\cdot \\begin{pmatrix} \\frac{1}{3} \\\\ -\\frac{2}{3} \\end{pmatrix} = \\left(\\frac{20}{3}\\right)\\left(\\frac{1}{3}\\right) + \\left(-\\frac{4}{3}\\right)\\left(-\\frac{2}{3}\\right) = \\frac{20}{9} + \\frac{8}{9} = \\frac{28}{9} $$\n- **Magnitudes**:\n$$ \\|\\mathbf{g}_{\\mathrm{MSE}}\\| = \\sqrt{\\left(\\frac{20}{3}\\right)^2 + \\left(-\\frac{4}{3}\\right)^2} = \\sqrt{\\frac{400}{9} + \\frac{16}{9}} = \\sqrt{\\frac{416}{9}} = \\frac{\\sqrt{16 \\times 26}}{3} = \\frac{4\\sqrt{26}}{3} $$\n$$ \\|\\mathbf{g}_{\\mathrm{MAE}}\\| = \\sqrt{\\left(\\frac{1}{3}\\right)^2 + \\left(-\\frac{2}{3}\\right)^2} = \\sqrt{\\frac{1}{9} + \\frac{4}{9}} = \\sqrt{\\frac{5}{9}} = \\frac{\\sqrt{5}}{3} $$\n- **Product of Magnitudes**:\n$$ \\|\\mathbf{g}_{\\mathrm{MSE}}\\| \\|\\mathbf{g}_{\\mathrm{MAE}}\\| = \\left(\\frac{4\\sqrt{26}}{3}\\right) \\left(\\frac{\\sqrt{5}}{3}\\right) = \\frac{4\\sqrt{26 \\times 5}}{9} = \\frac{4\\sqrt{130}}{9} $$\n- **Cosine of the angle**:\n$$ \\cos(\\theta) = \\frac{\\frac{28}{9}}{\\frac{4\\sqrt{130}}{9}} = \\frac{28}{4\\sqrt{130}} = \\frac{7}{\\sqrt{130}} $$\n- **Angle $\\theta$**:\nThe angle in radians is:\n$$ \\theta = \\arccos\\left(\\frac{7}{\\sqrt{130}}\\right) $$\nThis is the final exact closed-form expression for the angle.", "answer": "$$\\boxed{\\arccos\\left(\\frac{7}{\\sqrt{130}}\\right)}$$", "id": "3162520"}, {"introduction": "Deep learning models are compositions of functions, making the chain rule our most powerful tool. This practice [@problem_id:3162556] applies it to a cornerstone of modern architectures: Batch Normalization. By deriving the gradient expression from first principles, you will uncover the non-local dependencies inherent in this layer—how the update for one example is influenced by all others in its mini-batch—and gain a deeper appreciation for the intricate flow of gradients in complex networks.", "problem": "Consider a single scalar feature channel processed by Batch Normalization (BN) in training mode over a mini-batch of size $m$, with forward definitions given by the batch mean $\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x_i$, the batch variance $\\sigma^{2} = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu)^{2}$, the standard deviation $\\sigma = \\sqrt{\\sigma^{2} + \\varepsilon}$, the normalized activations $\\hat{x}_i = \\frac{x_i - \\mu}{\\sigma}$, and the BN outputs $y_i = \\gamma \\hat{x}_i + \\beta$. A scalar loss is applied to this BN layer with upstream sensitivities $g_i = \\frac{\\partial L}{\\partial y_i}$.\n\nStarting only from these forward definitions and the chain rule of calculus, derive the general expression for the partial derivative $\\frac{\\partial L}{\\partial x_i}$, making explicit how the dependence of $\\mu$ and $\\sigma$ on all samples in the mini-batch creates non-local terms that couple all $\\{x_j\\}_{j=1}^{m}$ through the gradient. Then, using your derived expression, evaluate $\\frac{\\partial L}{\\partial x_1}$ for the following scientifically plausible configuration that exhibits domination of gradient contributions by a single sample via batch statistics:\n\n- Mini-batch size $m = 3$.\n- Inputs $(x_1, x_2, x_3) = (5, 0, 1)$.\n- BN parameters $\\gamma = 1$, $\\beta = 0$, and $\\varepsilon = 0$.\n- A linear loss $L = \\sum_{i=1}^{m} c_i y_i$ with coefficients $(c_1, c_2, c_3) = (1, 0, 0)$, implying upstream sensitivities $g_i = c_i$.\n\nYour final answer must be the single real value of $\\frac{\\partial L}{\\partial x_1}$ for this configuration. Round your answer to four significant figures.", "solution": "We begin by deriving the general expression for the partial derivative of the loss $L$ with respect to a mini-batch input $x_i$. The loss $L$ is a function of the Batch Normalization (BN) outputs $\\{y_j\\}_{j=1}^m$. The dependence of $L$ on a specific input $x_i$ is established through the chain rule:\n$$\n\\frac{\\partial L}{\\partial x_i} = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i}\n$$\nGiven the upstream sensitivities $g_j = \\frac{\\partial L}{\\partial y_j}$, this becomes:\n$$\n\\frac{\\partial L}{\\partial x_i} = \\sum_{j=1}^{m} g_j \\frac{\\partial y_j}{\\partial x_i}\n$$\nThe BN output $y_j$ is defined as $y_j = \\gamma \\hat{x}_j + \\beta$, where $\\hat{x}_j = \\frac{x_j - \\mu}{\\sigma}$. The parameters $\\gamma$ and $\\beta$ are independent of the inputs $\\{x_k\\}$. Therefore, we must compute $\\frac{\\partial y_j}{\\partial x_i}$:\n$$\n\\frac{\\partial y_j}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left( \\gamma \\frac{x_j - \\mu}{\\sigma} + \\beta \\right) = \\gamma \\frac{\\partial}{\\partial x_i} \\left( \\frac{x_j - \\mu}{\\sigma} \\right)\n$$\nUsing the quotient rule, where both the numerator $N_j = x_j - \\mu$ and the denominator $D = \\sigma$ depend on $x_i$:\n$$\n\\frac{\\partial}{\\partial x_i} \\left( \\frac{N_j}{D} \\right) = \\frac{D \\frac{\\partial N_j}{\\partial x_i} - N_j \\frac{\\partial D}{\\partial x_i}}{D^2} = \\frac{1}{\\sigma} \\frac{\\partial (x_j - \\mu)}{\\partial x_i} - \\frac{x_j - \\mu}{\\sigma^2} \\frac{\\partial \\sigma}{\\partial x_i}\n$$\nWe must find the partial derivatives of the batch statistics $\\mu$ and $\\sigma$ with respect to $x_i$.\nThe batch mean is $\\mu = \\frac{1}{m} \\sum_{k=1}^{m} x_k$. Its partial derivative is:\n$$\n\\frac{\\partial \\mu}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left( \\frac{1}{m} \\sum_{k=1}^{m} x_k \\right) = \\frac{1}{m}\n$$\nThe derivative of the numerator term $N_j = x_j - \\mu$ is:\n$$\n\\frac{\\partial (x_j - \\mu)}{\\partial x_i} = \\frac{\\partial x_j}{\\partial x_i} - \\frac{\\partial \\mu}{\\partial x_i} = \\delta_{ij} - \\frac{1}{m}\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta.\n\nNext, we address the derivative of the standard deviation $\\sigma = \\sqrt{\\sigma^2 + \\varepsilon}$:\n$$\n\\frac{\\partial \\sigma}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\sqrt{\\sigma^2 + \\varepsilon} = \\frac{1}{2\\sqrt{\\sigma^2 + \\varepsilon}} \\frac{\\partial \\sigma^2}{\\partial x_i} = \\frac{1}{2\\sigma} \\frac{\\partial \\sigma^2}{\\partial x_i}\n$$\nThe batch variance is $\\sigma^2 = \\frac{1}{m} \\sum_{k=1}^{m} (x_k - \\mu)^2$. Its partial derivative is:\n$$\n\\frac{\\partial \\sigma^2}{\\partial x_i} = \\frac{1}{m} \\sum_{k=1}^{m} \\frac{\\partial}{\\partial x_i} (x_k - \\mu)^2 = \\frac{1}{m} \\sum_{k=1}^{m} 2(x_k - \\mu) \\frac{\\partial (x_k - \\mu)}{\\partial x_i}\n$$\n$$\n\\frac{\\partial \\sigma^2}{\\partial x_i} = \\frac{2}{m} \\sum_{k=1}^{m} (x_k - \\mu) (\\delta_{ki} - \\frac{1}{m}) = \\frac{2}{m} \\left( (x_i - \\mu) - \\frac{1}{m} \\sum_{k=1}^{m} (x_k - \\mu) \\right)\n$$\nBy definition of the mean, $\\sum_{k=1}^{m} (x_k - \\mu) = 0$. Thus:\n$$\n\\frac{\\partial \\sigma^2}{\\partial x_i} = \\frac{2}{m} (x_i - \\mu)\n$$\nSubstituting this back into the derivative for $\\sigma$:\n$$\n\\frac{\\partial \\sigma}{\\partial x_i} = \\frac{1}{2\\sigma} \\left( \\frac{2}{m} (x_i - \\mu) \\right) = \\frac{x_i - \\mu}{m\\sigma}\n$$\nNow we can assemble the expression for $\\frac{\\partial y_j}{\\partial x_i}$:\n$$\n\\frac{\\partial y_j}{\\partial x_i} = \\gamma \\left( \\frac{1}{\\sigma}(\\delta_{ij} - \\frac{1}{m}) - \\frac{x_j - \\mu}{\\sigma^2} \\frac{x_i - \\mu}{m\\sigma} \\right)\n$$\nUsing the definition of normalized activations, $\\hat{x}_k = \\frac{x_k - \\mu}{\\sigma}$, we can simplify this expression:\n$$\n\\frac{\\partial y_j}{\\partial x_i} = \\frac{\\gamma}{\\sigma} \\left( (\\delta_{ij} - \\frac{1}{m}) - \\frac{\\hat{x}_j \\hat{x}_i}{m} \\right) = \\frac{\\gamma}{\\sigma} \\left( \\delta_{ij} - \\frac{1}{m}(1 + \\hat{x}_i \\hat{x}_j) \\right)\n$$\nFinally, we substitute this into the sum for $\\frac{\\partial L}{\\partial x_i}$:\n$$\n\\frac{\\partial L}{\\partial x_i} = \\sum_{j=1}^{m} g_j \\left[ \\frac{\\gamma}{\\sigma} \\left( \\delta_{ij} - \\frac{1}{m} - \\frac{\\hat{x}_i \\hat{x}_j}{m} \\right) \\right]\n$$\n$$\n\\frac{\\partial L}{\\partial x_i} = \\frac{\\gamma}{\\sigma} \\left( \\sum_{j=1}^{m} g_j \\delta_{ij} - \\frac{1}{m} \\sum_{j=1}^{m} g_j - \\frac{\\hat{x}_i}{m} \\sum_{j=1}^{m} g_j \\hat{x}_j \\right)\n$$\nThis simplifies to the general expression for the gradient with respect to an input feature $x_i$:\n$$\n\\frac{\\partial L}{\\partial x_i} = \\frac{\\gamma}{\\sigma} \\left( g_i - \\frac{1}{m} \\sum_{j=1}^{m} g_j - \\frac{\\hat{x}_i}{m} \\sum_{j=1}^{m} g_j \\hat{x}_j \\right)\n$$\nThis expression demonstrates the non-local coupling: the gradient $\\frac{\\partial L}{\\partial x_i}$ depends not only on the local upstream gradient $g_i$ and local activation $\\hat{x}_i$, but also on terms that are averages over the entire mini-batch ($\\sum g_j$ and $\\sum g_j \\hat{x}_j$).\n\nNow, we evaluate this expression for the specific configuration provided.\nThe givens are:\n- Mini-batch size: $m = 3$\n- Inputs: $(x_1, x_2, x_3) = (5, 0, 1)$\n- BN parameters: $\\gamma = 1$, $\\beta = 0$, $\\varepsilon = 0$\n- Upstream sensitivities: $(g_1, g_2, g_3) = (1, 0, 0)$\n\nFirst, we compute the batch statistics:\n1.  Batch mean: $\\mu = \\frac{1}{3}(5 + 0 + 1) = \\frac{6}{3} = 2$.\n2.  Centered inputs: $(x_1 - \\mu, x_2 - \\mu, x_3 - \\mu) = (5-2, 0-2, 1-2) = (3, -2, -1)$.\n3.  Batch variance: $\\sigma^2 = \\frac{1}{3}((3)^2 + (-2)^2 + (-1)^2) = \\frac{1}{3}(9 + 4 + 1) = \\frac{14}{3}$.\n4.  Standard deviation: $\\sigma = \\sqrt{\\sigma^2 + \\varepsilon} = \\sqrt{\\frac{14}{3} + 0} = \\sqrt{\\frac{14}{3}}$.\n\nNext, we calculate the terms required for the gradient formula for $i=1$:\n- $\\sum_{j=1}^{3} g_j = g_1 + g_2 + g_3 = 1 + 0 + 0 = 1$.\n- We need the normalized activation $\\hat{x}_1$: $\\hat{x}_1 = \\frac{x_1 - \\mu}{\\sigma} = \\frac{3}{\\sqrt{14/3}}$.\n- We need the sum $\\sum_{j=1}^{3} g_j \\hat{x}_j = g_1\\hat{x}_1 + g_2\\hat{x}_2 + g_3\\hat{x}_3 = (1)\\hat{x}_1 + (0)\\hat{x}_2 + (0)\\hat{x}_3 = \\hat{x}_1$.\n\nSubstituting these into the general formula for $\\frac{\\partial L}{\\partial x_1}$:\n$$\n\\frac{\\partial L}{\\partial x_1} = \\frac{\\gamma}{\\sigma} \\left( g_1 - \\frac{1}{m} \\sum_{j=1}^{3} g_j - \\frac{\\hat{x}_1}{m} \\sum_{j=1}^{3} g_j \\hat{x}_j \\right)\n$$\n$$\n\\frac{\\partial L}{\\partial x_1} = \\frac{1}{\\sqrt{14/3}} \\left( 1 - \\frac{1}{3}(1) - \\frac{\\hat{x}_1}{3}(\\hat{x}_1) \\right) = \\sqrt{\\frac{3}{14}} \\left( \\frac{2}{3} - \\frac{\\hat{x}_1^2}{3} \\right)\n$$\nWe calculate $\\hat{x}_1^2$:\n$$\n\\hat{x}_1^2 = \\left( \\frac{3}{\\sqrt{14/3}} \\right)^2 = \\frac{9}{14/3} = \\frac{27}{14}\n$$\nNow substitute this value back:\n$$\n\\frac{\\partial L}{\\partial x_1} = \\sqrt{\\frac{3}{14}} \\left( \\frac{2}{3} - \\frac{1}{3} \\cdot \\frac{27}{14} \\right) = \\sqrt{\\frac{3}{14}} \\left( \\frac{2}{3} - \\frac{9}{14} \\right)\n$$\nTo simplify the term in the parenthesis, we find a common denominator, which is $42$:\n$$\n\\frac{2}{3} - \\frac{9}{14} = \\frac{2 \\cdot 14}{42} - \\frac{9 \\cdot 3}{42} = \\frac{28 - 27}{42} = \\frac{1}{42}\n$$\nTherefore, the exact value of the partial derivative is:\n$$\n\\frac{\\partial L}{\\partial x_1} = \\sqrt{\\frac{3}{14}} \\cdot \\frac{1}{42} = \\frac{\\sqrt{3}}{42\\sqrt{14}} = \\frac{\\sqrt{3}\\sqrt{14}}{42 \\cdot 14} = \\frac{\\sqrt{42}}{588}\n$$\nFinally, we compute the numerical value and round to four significant figures:\n$$\n\\frac{\\partial L}{\\partial x_1} = \\frac{\\sqrt{42}}{588} \\approx \\frac{6.4807407}{588} \\approx 0.01102166785\n$$\nRounding to four significant figures yields $0.01102$.", "answer": "$$\\boxed{0.01102}$$", "id": "3162556"}]}