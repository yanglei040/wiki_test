{"hands_on_practices": [{"introduction": "The Hessian matrix is the cornerstone of the second derivative test in multiple dimensions, allowing us to classify critical points as local minima, maxima, or saddle points. This practice [@problem_id:2215318] provides a direct application of this test in an optimization scenario, where you are given the components of the Hessian at a critical point. Your task is to use these values to determine the nature of the point, reinforcing the connection between the Hessian's properties and the local geometry of a function.", "problem": "An operations research analyst is modeling the weekly profit, $P(x, y)$, of a manufacturing company. The profit is a function of two variables: $x$, the number of hours the primary production line is operational, and $y$, the number of hours the secondary, more specialized line is operational. The analyst has identified that the point $(x_0, y_0) = (40, 15)$ is a critical point for the profit function, meaning that the first partial derivatives of profit with respect to both $x$ and $y$ are zero at this point.\n\nTo determine if this operating schedule represents a local maximum profit, a local minimum profit, or a saddle point, the second derivative test must be applied. Through data analysis, the second-order partial derivatives of the profit function $P(x,y)$ have been evaluated at the critical point $(40, 15)$ with the following results:\n*   $\\frac{\\partial^2 P}{\\partial x^2} \\bigg|_{(40,15)} = -8.0$\n*   $\\frac{\\partial^2 P}{\\partial y^2} \\bigg|_{(40,15)} = -12.0$\n*   $\\frac{\\partial^2 P}{\\partial x \\partial y} \\bigg|_{(40,15)} = 9.0$\n\nBased on this information, classify the nature of the critical point $(40, 15)$.\n\nA) Local maximum\n\nB) Local minimum\n\nC) Saddle point\n\nD) The classification cannot be determined from the information provided.", "solution": "At a critical point, the second derivative test for a function of two variables uses the Hessian matrix\n$$\nH(x,y)=\\begin{pmatrix}\nP_{xx}(x,y)  P_{xy}(x,y) \\\\\nP_{yx}(x,y)  P_{yy}(x,y)\n\\end{pmatrix},\n$$\nand its determinant\n$$\nD(x,y)=P_{xx}(x,y)P_{yy}(x,y)-\\left(P_{xy}(x,y)\\right)^{2}.\n$$\nAt the point $(40,15)$, the given values are $P_{xx}=-8.0$, $P_{yy}=-12.0$, and $P_{xy}=9.0$ (and by equality of mixed partials where applicable, $P_{xy}=P_{yx}$). Compute the determinant:\n$$\nD(40,15)=(-8.0)(-12.0)-(9.0)^{2}=96-81=15.\n$$\nThus $D(40,15)0$. The second derivative test states:\n- If $D0$ and $P_{xx}0$, then the point is a local maximum.\n- If $D0$ and $P_{xx}0$, then the point is a local minimum.\n- If $D0$, then the point is a saddle point.\n- If $D=0$, the test is inconclusive.\n\nHere, $D(40,15)0$ and $P_{xx}(40,15)=-8.00$, so the critical point is a local maximum.", "answer": "$$\\boxed{A}$$", "id": "2215318"}, {"introduction": "While computing the full Hessian is feasible for simple functions, it becomes computationally intractable for deep neural networks with millions of parameters. This practice [@problem_id:3186600] introduces the fundamental technique that makes second-order information accessible in deep learning: the Hessian-vector product, $H\\mathbf{v}$. You will implement this method from scratch, gaining insight into how automatic differentiation can be cleverly extended to compute this product efficiently without ever forming the enormous Hessian matrix itself.", "problem": "You are to derive, implement, and validate the Hessian–vector product for a scalar loss function in a small feedforward neural network, using principles of automatic differentiation. Consider a single-hidden-layer Multi-Layer Perceptron (MLP) with hyperbolic tangent activation mapping inputs in $\\mathbb{R}^d$ to a scalar output. The model parameters are collected into a single vector $\\theta \\in \\mathbb{R}^p$ formed by concatenating the weights and biases from both layers. The network is defined by\n$$\n\\mathbf{a}_1 = \\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1,\\quad\n\\mathbf{h}_1 = \\tanh(\\mathbf{a}_1),\\quad\n\\hat{y} = \\mathbf{W}_2^\\top \\mathbf{h}_1 + b_2,\n$$\nwhere $\\mathbf{W}_1 \\in \\mathbb{R}^{m \\times d}$, $\\mathbf{b}_1 \\in \\mathbb{R}^m$, $\\mathbf{W}_2 \\in \\mathbb{R}^m$, and $b_2 \\in \\mathbb{R}$ are the parameters, $\\mathbf{x} \\in \\mathbb{R}^d$ is the input, and $\\hat{y} \\in \\mathbb{R}$ is the predicted scalar output. For a dataset $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$, the empirical loss is\n$$\nf(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2.\n$$\nLet $\\nabla f(\\theta)$ denote the gradient of $f$ with respect to $\\theta$, and let $\\mathbf{H}(\\theta)$ denote the Hessian matrix of second derivatives. The Hessian–vector product is $\\mathbf{H}(\\theta)\\mathbf{v}$ for a given direction vector $\\mathbf{v}\\in\\mathbb{R}^p$.\n\nStarting from fundamental definitions and rules of differentiation, derive Pearlmutter’s method for computing the Hessian–vector product via automatic differentiation. Specifically, use the definition of the Hessian–vector product as the directional derivative of the gradient and apply the chain rule to construct a forward directional-derivative pass and a backward pass that yields $\\mathbf{H}(\\theta)\\mathbf{v}$ without explicitly forming $\\mathbf{H}(\\theta)$.\n\nImplementation requirements:\n- Use $d=3$, $m=4$, and $n=5$. Construct the dataset deterministically as follows: draw inputs $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$ and targets $\\mathbf{y}\\in\\mathbb{R}^n$ from a standard normal distribution with a fixed seed $42$. Initialize parameters $\\mathbf{W}_1$, $\\mathbf{b}_1$, $\\mathbf{W}_2$, and $b_2$ by drawing from a standard normal distribution with seed $0$ and scaling by $0.1$. The activation function is the hyperbolic tangent $\\tanh$, which is twice differentiable.\n- Implement two functions:\n  1. A function that returns $\\mathbf{H}(\\theta)\\mathbf{v}$ using Pearlmutter’s method with automatic differentiation principles (directional derivatives propagated through the network and reverse-mode accumulation).\n  2. A function that returns a finite-difference approximation of $\\mathbf{H}(\\theta)\\mathbf{v}$ using\n     $$\n     \\frac{\\nabla f(\\theta + \\epsilon \\mathbf{v}) - \\nabla f(\\theta)}{\\epsilon},\n     $$\n     for a given $\\epsilon > 0$, by computing gradients at $\\theta$ and $\\theta+\\epsilon\\mathbf{v}$ via standard backpropagation.\n- Use the derivative $\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x)$ and the second derivative $\\frac{d^2}{dx^2}\\tanh(x) = -2\\,\\tanh(x)\\,\\big(1 - \\tanh^2(x)\\big)$ in your derivation and implementation.\n\nTest suite:\n- For all tests, let $\\mathbf{v}\\in\\mathbb{R}^p$ be a specified direction and $\\epsilon$ be the finite-difference step. Use the following cases:\n  1. A random unit-norm direction with $\\epsilon = 10^{-5}$ and tolerance $10^{-6}$.\n  2. The zero direction $\\mathbf{v}=\\mathbf{0}$ with $\\epsilon = 10^{-5}$ and tolerance $10^{-12}$.\n  3. A basis direction selecting the last parameter (corresponding to $b_2$), i.e., $\\mathbf{v} = \\mathbf{e}_{p}$, with $\\epsilon = 10^{-6}$ and tolerance $10^{-6}$.\n  4. The same random unit-norm direction as case 1 with $\\epsilon = 10^{-9}$ and tolerance $10^{-2}$.\n  5. The same random unit-norm direction as case 1 with $\\epsilon = 10^{-1}$ and tolerance $10^{-3}$.\n- For each test case, compute the relative error\n  $$\n  r = \\frac{\\|\\mathbf{H}(\\theta)\\mathbf{v} - \\text{FD}(\\theta,\\mathbf{v},\\epsilon)\\|_2}{\\max\\left(10^{-12}, \\|\\text{FD}(\\theta,\\mathbf{v},\\epsilon)\\|_2\\right)},\n  $$\n  where $\\text{FD}(\\theta,\\mathbf{v},\\epsilon)$ denotes the finite-difference approximation. The test result is a boolean indicating whether $r$ is less than or equal to the given tolerance.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,True,False]\").\n- No physical units, angles, or percentages appear in this problem; all quantities are dimensionless real numbers.", "solution": "The user wants to derive and implement the Hessian-vector product for a small neural network using Pearlmutter's method.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n\n-   **Network Definition**:\n    -   $\\mathbf{a}_1 = \\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1$\n    -   $\\mathbf{h}_1 = \\tanh(\\mathbf{a}_1)$\n    -   $\\hat{y} = \\mathbf{W}_2^\\top \\mathbf{h}_1 + b_2$\n-   **Parameters and Dimensions**:\n    -   Inputs: $\\mathbf{x} \\in \\mathbb{R}^d$\n    -   Parameters: $\\mathbf{W}_1 \\in \\mathbb{R}^{m \\times d}$, $\\mathbf{b}_1 \\in \\mathbb{R}^m$, $\\mathbf{W}_2 \\in \\mathbb{R}^m$, $b_2 \\in \\mathbb{R}$\n    -   Parameter vector: $\\theta \\in \\mathbb{R}^p$ (concatenation of all parameters)\n    -   Predicted output: $\\hat{y} \\in \\mathbb{R}$\n    -   Constants: $d=3$, $m=4$, $n=5$. The total number of parameters is $p = m \\times d + m + m \\times 1 + 1 = 4 \\times 3 + 4 + 4 + 1 = 21$.\n-   **Loss Function**:\n    -   $f(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2$ for dataset $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$.\n-   **Objective**:\n    -   Compute the Hessian-vector product $\\mathbf{H}(\\theta)\\mathbf{v}$, where $\\mathbf{H}(\\theta)$ is the Hessian of $f(\\theta)$ and $\\mathbf{v} \\in \\mathbb{R}^p$ is a direction vector.\n-   **Methodology**:\n    -   Derive Pearlmutter’s method using automatic differentiation, based on the definition of the Hessian-vector product as the directional derivative of the gradient.\n-   **Data and Parameter Generation**:\n    -   Inputs $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$ and targets $\\mathbf{y}\\in\\mathbb{R}^n$ from a standard normal distribution with seed $42$.\n    -   Parameters $\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, b_2$ from a standard normal distribution with seed $0$, scaled by $0.1$.\n-   **Provided Derivatives**:\n    -   $\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x)$\n    -   $\\frac{d^2}{dx^2}\\tanh(x) = -2\\,\\tanh(x)\\,\\big(1 - \\tanh^2(x)\\big)$\n-   **Validation Method**:\n    -   Finite-difference approximation: $\\frac{\\nabla f(\\theta + \\epsilon \\mathbf{v}) - \\nabla f(\\theta)}{\\epsilon}$.\n-   **Evaluation**:\n    -   Relative error: $r = \\frac{\\|\\mathbf{H}(\\theta)\\mathbf{v} - \\text{FD}(\\theta,\\mathbf{v},\\epsilon)\\|_2}{\\max\\left(10^{-12}, \\|\\text{FD}(\\theta,\\mathbf{v},\\epsilon)\\|_2\\right)}$.\n    -   Test cases specified with $\\mathbf{v}$, $\\epsilon$, and a tolerance for $r$.\n\n#### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is a standard exercise in automatic differentiation applied to neural networks. Pearlmutter's method for computing Hessian-vector products is a well-established and fundamental technique in computational science and machine learning optimization. All mathematical operations are based on standard rules of calculus and linear algebra.\n-   **Well-Posed**: The problem is well-posed. The network and loss function are twice differentiable, ensuring the Hessian exists. The instructions for data generation, parameter initialization, and testing are deterministic and specific, leading to a single, verifiable solution.\n-   **Objective**: The problem is stated using precise, objective mathematical language.\n-   **Completeness**: The problem provides all necessary information, including network architecture, loss function, parameter dimensions, initialization schemes, and explicit formulas for validation and evaluation.\n\n#### Step 3: Verdict and Action\n\nThe problem is valid. It is scientifically sound, well-posed, and complete. A solution will be provided.\n\n---\n\n### Derivation of the Hessian-Vector Product\n\nThe core of Pearlmutter's method is to recognize that the Hessian-vector product $\\mathbf{H}(\\theta)\\mathbf{v}$ can be expressed as a directional derivative of the gradient $\\nabla f(\\theta)$ in the direction of a vector $\\mathbf{v}$. Let's define a directional derivative operator $D_{\\mathbf{v}}[\\cdot]$ for a function $g(\\theta)$ as:\n$$\nD_{\\mathbf{v}}[g(\\theta)] = \\frac{d}{d\\lambda} g(\\theta + \\lambda\\mathbf{v}) \\bigg|_{\\lambda=0}\n$$\nBy the chain rule, this is equivalent to $(\\nabla_\\theta g(\\theta))^\\top \\mathbf{v}$. The Hessian-vector product is then:\n$$\n\\mathbf{H}(\\theta)\\mathbf{v} = \\left(\\nabla_\\theta (\\nabla_\\theta f(\\theta))^\\top \\right) \\mathbf{v} = D_{\\mathbf{v}}[\\nabla_\\theta f(\\theta)]\n$$\nThis insight allows us to compute $\\mathbf{H}(\\theta)\\mathbf{v}$ by applying the $D_{\\mathbf{v}}$ operator to the entire backpropagation algorithm, which computes $\\nabla_\\theta f(\\theta)$. This avoids the explicit, and computationally expensive, formation of the full Hessian matrix $\\mathbf{H}(\\theta)$. The process involves two phases: a \"forward-on-forward\" pass to compute directional derivatives of all intermediate variables, and a \"forward-on-reverse\" pass which is essentially a second backpropagation pass to compute the directional derivatives of the gradients.\n\nFor efficiency, we will use a batch formulation. Let the input data be a matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ and the target outputs be a vector $\\mathbf{y} \\in \\mathbb{R}^n$. The parameters $\\theta$ and the direction vector $\\mathbf{v}$ are un-flattened into their corresponding matrix/vector shapes:\n$\\theta \\rightarrow (\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, b_2)$ and $\\mathbf{v} \\rightarrow (\\mathbf{v}_{W1}, \\mathbf{v}_{b1}, \\mathbf{v}_{W2}, v_{b2})$.\n\n**1. Standard Forward and Backward Pass (for Gradient Calculation)**\n\nFirst, we review the standard forward and backward passes required for computing the gradient $\\nabla f(\\theta)$, which will be needed for the finite-difference check and as a basis for the HVP calculation.\n\n**Forward Pass:**\n-   Compute pre-activations: $\\mathbf{A}_1 = \\mathbf{X} \\mathbf{W}_1^\\top + \\mathbf{b}_1^\\top \\in \\mathbb{R}^{n \\times m}$\n-   Compute hidden layer activations: $\\mathbf{H}_1 = \\tanh(\\mathbf{A}_1) \\in \\mathbb{R}^{n \\times m}$\n-   Compute predictions: $\\hat{\\mathbf{y}} = \\mathbf{H}_1 \\mathbf{W}_2 + b_2 \\in \\mathbb{R}^n$\n\n**Backward Pass (Gradient Calculation):**\nLet $\\bar{z} = \\frac{\\partial f}{\\partial z}$ denote the adjoint (gradient) of the loss $f$ with respect to a variable $z$. The loss is $f = \\frac{1}{2n}\\sum_{i=1}^n (\\hat{y}_i - y_i)^2$.\n-   $\\bar{\\hat{\\mathbf{y}}} = \\frac{\\partial f}{\\partial \\hat{\\mathbf{y}}} = \\frac{1}{n}(\\hat{\\mathbf{y}} - \\mathbf{y})$\n-   $\\bar{b}_2 = \\frac{\\partial f}{\\partial b_2} = \\sum_{i=1}^n \\bar{\\hat{y}}_i$\n-   $\\bar{\\mathbf{W}}_2 = \\frac{\\partial f}{\\partial \\mathbf{W}_2} = \\mathbf{H}_1^\\top \\bar{\\hat{\\mathbf{y}}}$\n-   $\\bar{\\mathbf{H}}_1 = \\frac{\\partial f}{\\partial \\mathbf{H}_1} = \\bar{\\hat{\\mathbf{y}}} \\mathbf{W}_2^\\top$ (using outer product semantics, one row per sample)\n-   $\\bar{\\mathbf{A}}_1 = \\frac{\\partial f}{\\partial \\mathbf{A}_1} = \\bar{\\mathbf{H}}_1 \\odot \\frac{\\partial \\mathbf{H}_1}{\\partial \\mathbf{A}_1} = \\bar{\\mathbf{H}}_1 \\odot (1 - \\mathbf{H}_1^2)$\n-   $\\bar{\\mathbf{b}}_1 = \\frac{\\partial f}{\\partial \\mathbf{b}_1} = \\sum_{i=1}^n (\\bar{\\mathbf{A}}_1)_{i,:}$ (sum over the batch dimension)\n-   $\\bar{\\mathbf{W}}_1 = \\frac{\\partial f}{\\partial \\mathbf{W}_1} = \\bar{\\mathbf{A}}_1^\\top \\mathbf{X}$\n\n**2. Pearlmutter's Method (for Hessian-Vector Product)**\n\nWe now apply the operator $D_{\\mathbf{v}}$ to the computations. Let $\\dot{z} = D_{\\mathbf{v}}[z]$ denote the directional derivative of a variable $z$.\n\n**Phase I: Directional Forward Pass (Forward-on-Forward)**\nWe propagate directional derivatives through the forward computational graph.\n-   $\\dot{\\mathbf{A}}_1 = D_{\\mathbf{v}}[\\mathbf{X} \\mathbf{W}_1^\\top + \\mathbf{b}_1^\\top] = \\mathbf{X} \\mathbf{v}_{W1}^\\top + \\mathbf{v}_{b1}^\\top$\n-   $\\dot{\\mathbf{H}}_1 = D_{\\mathbf{v}}[\\tanh(\\mathbf{A}_1)] = \\tanh'(\\mathbf{A}_1) \\odot \\dot{\\mathbf{A}}_1 = (1 - \\mathbf{H}_1^2) \\odot \\dot{\\mathbf{A}}_1$\n-   $\\dot{\\hat{\\mathbf{y}}} = D_{\\mathbf{v}}[\\mathbf{H}_1 \\mathbf{W}_2 + b_2] = \\dot{\\mathbf{H}}_1 \\mathbf{W}_2 + \\mathbf{H}_1 \\mathbf{v}_{W2} + v_{b2}$\n\n**Phase II: HVP Backward Pass (Forward-on-Reverse)**\nWe apply the $D_{\\mathbf{v}}$ operator to the gradient equations from the standard backward pass. The result, $\\dot{\\bar{\\theta}} = D_{\\mathbf{v}}[\\bar{\\theta}]$, is the desired Hessian-vector product $\\mathbf{H}(\\theta)\\mathbf{v}$. This requires applying the product rule for differentiation, $D_{\\mathbf{v}}[UV] = \\dot{U}V + U\\dot{V}$.\n-   $\\dot{\\bar{\\hat{\\mathbf{y}}}} = D_{\\mathbf{v}}[\\frac{1}{n}(\\hat{\\mathbf{y}} - \\mathbf{y})] = \\frac{1}{n}\\dot{\\hat{\\mathbf{y}}}$\n-   $\\dot{\\bar{b}}_2 = D_{\\mathbf{v}}[\\sum_{i=1}^n \\bar{\\hat{y}}_i] = \\sum_{i=1}^n \\dot{\\bar{\\hat{y}}}_i$\n-   $\\dot{\\bar{\\mathbf{W}}}_2 = D_{\\mathbf{v}}[\\mathbf{H}_1^\\top \\bar{\\hat{\\mathbf{y}}}] = \\dot{\\mathbf{H}}_1^\\top \\bar{\\hat{\\mathbf{y}}} + \\mathbf{H}_1^\\top \\dot{\\bar{\\hat{\\mathbf{y}}}}$\n-   $\\dot{\\bar{\\mathbf{H}}}_1 = D_{\\mathbf{v}}[\\bar{\\hat{\\mathbf{y}}} \\mathbf{W}_2^\\top] = \\dot{\\bar{\\hat{\\mathbf{y}}}} \\mathbf{W}_2^\\top + \\bar{\\hat{\\mathbf{y}}} \\mathbf{v}_{W2}^\\top$\n-   $\\dot{\\bar{\\mathbf{A}}}_1 = D_{\\mathbf{v}}[\\bar{\\mathbf{A}}_1] = D_{\\mathbf{v}}[\\bar{\\mathbf{H}}_1 \\odot \\tanh'(\\mathbf{A}_1)] = \\dot{\\bar{\\mathbf{H}}}_1 \\odot \\tanh'(\\mathbf{A}_1) + \\bar{\\mathbf{H}}_1 \\odot D_{\\mathbf{v}}[\\tanh'(\\mathbf{A}_1)]$.\n    -   $D_{\\mathbf{v}}[\\tanh'(\\mathbf{A}_1)] = \\tanh''(\\mathbf{A}_1) \\odot \\dot{\\mathbf{A}}_1$.\n    -   Using the given derivatives and $\\mathbf{H}_1=\\tanh(\\mathbf{A}_1)$:\n    -   $\\dot{\\bar{\\mathbf{A}}}_1 = \\dot{\\bar{\\mathbf{H}}}_1 \\odot (1 - \\mathbf{H}_1^2) + \\bar{\\mathbf{H}}_1 \\odot (-2\\mathbf{H}_1 \\odot (1 - \\mathbf{H}_1^2)) \\odot \\dot{\\mathbf{A}}_1$\n-   $\\dot{\\bar{\\mathbf{b}}}_1 = D_{\\mathbf{v}}[\\sum_{i=1}^n (\\bar{\\mathbf{A}}_1)_{i,:}] = \\sum_{i=1}^n (\\dot{\\bar{\\mathbf{A}}}_1)_{i,:}$\n-   $\\dot{\\bar{\\mathbf{W}}}_1 = D_{\\mathbf{v}}[\\bar{\\mathbf{A}}_1^\\top \\mathbf{X}] = \\dot{\\bar{\\mathbf{A}}}_1^\\top \\mathbf{X}$\n\nThe resulting quantities $(\\dot{\\bar{\\mathbf{W}}}_1, \\dot{\\bar{\\mathbf{b}}}_1, \\dot{\\bar{\\mathbf{W}}}_2, \\dot{\\bar{b}}_2)$, when concatenated, form the vector $\\mathbf{H}(\\theta)\\mathbf{v}$. This procedure computes the product without forming the $p \\times p$ Hessian, making it efficient for large $p$.\n\n**3. Finite-Difference Approximation**\n\nFor validation, we approximate the Hessian-vector product using a first-order finite difference on the gradient:\n$$\n\\mathbf{H}(\\theta)\\mathbf{v} \\approx \\frac{\\nabla f(\\theta + \\epsilon \\mathbf{v}) - \\nabla f(\\theta)}{\\epsilon}\n$$\nThis requires two full gradient computations using the standard backpropagation algorithm outlined above, one at $\\theta$ and another at the perturbed point $\\theta + \\epsilon \\mathbf{v}$. The error of this approximation is of order $O(\\epsilon)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives, implements, and validates the Hessian-vector product for a small MLP.\n    \"\"\"\n    # Define problem constants and initialize data/parameters\n    d, m, n = 3, 4, 5\n    p = d * m + m + m * 1 + 1\n\n    # Generate dataset\n    rng_data = np.random.default_rng(seed=42)\n    X = rng_data.standard_normal(size=(n, d))\n    y = rng_data.standard_normal(size=n)\n\n    # Initialize parameters\n    rng_params = np.random.default_rng(seed=0)\n    W1_init = rng_params.standard_normal(size=(m, d)) * 0.1\n    b1_init = rng_params.standard_normal(size=m) * 0.1\n    W2_init = rng_params.standard_normal(size=m) * 0.1\n    b2_init = rng_params.standard_normal() * 0.1\n\n    # --- Parameter Flattening and Unflattening Utilities ---\n    def _flatten_params(W1, b1, W2, b2):\n        return np.concatenate([\n            W1.flatten(),\n            b1.flatten(),\n            W2.flatten(),\n            np.array([b2]).flatten()\n        ])\n\n    def _unflatten_params(theta):\n        idx = 0\n        W1 = theta[idx:idx + m * d].reshape(m, d)\n        idx += m * d\n        b1 = theta[idx:idx + m]\n        idx += m\n        W2 = theta[idx:idx + m]\n        idx += m\n        b2 = theta[idx]\n        return W1, b1, W2, b2\n\n    # --- Core Implementation: Gradient and HVP ---\n\n    def _compute_grad(theta, X, y):\n        \"\"\"Computes the gradient of the loss function via backpropagation.\"\"\"\n        W1, b1, W2, b2 = _unflatten_params(theta)\n        num_samples = X.shape[0]\n\n        # Forward pass\n        A1 = X @ W1.T + b1\n        H1 = np.tanh(A1)\n        y_hat = H1 @ W2 + b2\n\n        # Backward pass (gradient calculation)\n        y_hat_bar = (y_hat - y) / num_samples\n        \n        b2_bar = np.sum(y_hat_bar)\n        W2_bar = H1.T @ y_hat_bar\n        H1_bar = np.outer(y_hat_bar, W2)\n        \n        A1_bar = H1_bar * (1 - H1**2)\n        \n        b1_bar = np.sum(A1_bar, axis=0)\n        W1_bar = A1_bar.T @ X\n\n        return _flatten_params(W1_bar, b1_bar, W2_bar, b2_bar)\n\n    def _compute_hvp_pearlmutter(theta, v, X, y):\n        \"\"\"Computes the Hessian-vector product using Pearlmutter's method.\"\"\"\n        W1, b1, W2, b2 = _unflatten_params(theta)\n        v_W1, v_b1, v_W2, v_b2 = _unflatten_params(v)\n        num_samples = X.shape[0]\n\n        # --- Standard Forward Pass ---\n        A1 = X @ W1.T + b1\n        H1 = np.tanh(A1)\n        y_hat = H1 @ W2 + b2\n\n        # --- Phase I: Directional Forward Pass (Forward-on-Forward) ---\n        dot_A1 = X @ v_W1.T + v_b1\n        dot_H1 = (1 - H1**2) * dot_A1\n        dot_y_hat = np.sum(dot_H1 * W2, axis=1) + H1 @ v_W2 + v_b2\n\n        # --- Phase II: HVP Backward Pass (Forward-on-Reverse) ---\n        # Gradients from standard backprop (needed for the second-order terms)\n        y_hat_bar = (y_hat - y) / num_samples\n        H1_bar = np.outer(y_hat_bar, W2)\n\n        # Directional derivatives of gradients\n        dot_y_hat_bar = dot_y_hat / num_samples\n        \n        dot_b2_bar = np.sum(dot_y_hat_bar)\n        dot_W2_bar = dot_H1.T @ y_hat_bar + H1.T @ dot_y_hat_bar\n        \n        dot_H1_bar = np.outer(dot_y_hat_bar, W2) + np.outer(y_hat_bar, v_W2)\n        \n        tanh_prime_A1 = 1 - H1**2\n        tanh_second_A1 = -2 * H1 * tanh_prime_A1\n        \n        dot_A1_bar = dot_H1_bar * tanh_prime_A1 + H1_bar * tanh_second_A1 * dot_A1\n\n        dot_b1_bar = np.sum(dot_A1_bar, axis=0)\n        dot_W1_bar = dot_A1_bar.T @ X\n\n        return _flatten_params(dot_W1_bar, dot_b1_bar, dot_W2_bar, dot_b2_bar)\n\n    def _compute_hvp_finite_diff(theta, v, X, y, epsilon):\n        \"\"\"Approximates the Hessian-vector product using finite differences.\"\"\"\n        grad_pos = _compute_grad(theta + epsilon * v, X, y)\n        grad_neg = _compute_grad(theta, X, y)\n        return (grad_pos - grad_neg) / epsilon\n\n    # --- Test Suite ---\n    theta_init = _flatten_params(W1_init, b1_init, W2_init, b2_init)\n    \n    # Generate a reusable random unit-norm direction for cases 1, 4, 5\n    rng_v = np.random.default_rng(seed=123)\n    v_rand_raw = rng_v.standard_normal(size=p)\n    v_rand_unit = v_rand_raw / np.linalg.norm(v_rand_raw)\n\n    test_cases = [\n        # (name, direction_vector, epsilon, tolerance)\n        (\"random_unit_v_e-5\", v_rand_unit, 1e-5, 1e-6),\n        (\"zero_v\", np.zeros(p), 1e-5, 1e-12),\n        (\"basis_v\", np.eye(1, p, p - 1).flatten(), 1e-6, 1e-6),\n        (\"random_unit_v_e-9\", v_rand_unit, 1e-9, 1e-2),\n        (\"random_unit_v_e-1\", v_rand_unit, 1e-1, 1e-3),\n    ]\n\n    results = []\n    for name, v, epsilon, tolerance in test_cases:\n        # Calculate HVP using both methods\n        hvp_ad = _compute_hvp_pearlmutter(theta_init, v, X, y)\n        hvp_fd = _compute_hvp_finite_diff(theta_init, v, X, y, epsilon)\n\n        # Compute relative error\n        norm_fd = np.linalg.norm(hvp_fd)\n        # The denominator is max(1e-12, norm_fd) to avoid division by zero\n        # when hvp_fd is exactly zero (as in the v=0 case).\n        denominator = max(1e-12, norm_fd)\n        \n        rel_error = np.linalg.norm(hvp_ad - hvp_fd) / denominator\n        \n        results.append(rel_error = tolerance)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3186600"}, {"introduction": "With an efficient method to compute Hessian-vector products, we can now probe the complex loss landscapes of neural networks. A key question is identifying directions of sharpest curvature, which correspond to the largest eigenvalue of the Hessian and have significant implications for optimization. This practice [@problem_id:3186508] guides you through implementing the power iteration algorithm, a classic numerical method that leverages repeated Hessian-vector products to approximate the dominant eigenpair of the Hessian, providing a practical tool for landscape analysis.", "problem": "Consider a differentiable scalar loss function $L(\\mathbf{w})$ used in deep learning, where $\\mathbf{w} \\in \\mathbb{R}^d$ denotes the parameter vector. The Hessian matrix $H(\\mathbf{w})$ at a point $\\mathbf{w}$ is defined as the square matrix of second-order partial derivatives, with entries $H_{ij}(\\mathbf{w}) = \\frac{\\partial^2 L(\\mathbf{w})}{\\partial w_i \\partial w_j}$. For functions with continuous second derivatives, $H(\\mathbf{w})$ is symmetric. The second-order Taylor expansion of $L$ around $\\mathbf{w}$ in a direction $\\mathbf{v} \\in \\mathbb{R}^d$ is given by $L(\\mathbf{w} + \\mathbf{v}) \\approx L(\\mathbf{w}) + \\nabla L(\\mathbf{w})^\\top \\mathbf{v} + \\frac{1}{2} \\mathbf{v}^\\top H(\\mathbf{w}) \\mathbf{v}$. The quantity $\\mathbf{v}^\\top H(\\mathbf{w}) \\mathbf{v}$ characterizes local curvature along $\\mathbf{v}$, and its extremal values under the constraint $\\mathbf{v}^\\top \\mathbf{v} = 1$ are attained at the eigenvectors of $H(\\mathbf{w})$. The associated extremal values are the eigenvalues, and the largest algebraic eigenvalue approximates the maximum local curvature direction at $\\mathbf{w}$.\n\nYour task is to implement a program that approximates the largest algebraic eigenvalue of a given real symmetric matrix $H$ (interpreted as a Hessian) using only repeated Hessian-vector products and normalization, together with the Rayleigh quotient. Do not form or use any closed-form eigen-decomposition. At each iteration, use the current iterate to compute the next direction by applying the matrix $H$ and normalizing the result, then evaluate the Rayleigh quotient to update the eigenvalue estimate. Iterate until the absolute change in the Rayleigh quotient is below a specified tolerance or a maximum iteration cap is reached. If at any step the Hessian-vector product is the zero vector (i.e., its Euclidean norm is zero or below a machine-safe threshold), terminate and return $0$ as the eigenvalue estimate.\n\nUse the following fixed parameters for all test cases:\n- Initial vector $\\mathbf{v}_0$ equal to the all-ones vector in the appropriate dimension, normalized to unit Euclidean norm.\n- Tolerance $10^{-10}$ on the absolute change of the Rayleigh quotient between successive iterations.\n- Maximum number of iterations $1000$.\n- Output each estimated largest algebraic eigenvalue as a float rounded to six decimal places.\n\nTest suite (each $H$ below is symmetric and should be treated as a Hessian at some parameter point):\n1. Happy path, symmetric positive definite $3 \\times 3$:\n   $$H_1 = \\begin{bmatrix}\n   4  1  0 \\\\\n   1  3  0 \\\\\n   0  0  2\n   \\end{bmatrix}.$$\n2. Indefinite $3 \\times 3$ with a dominant positive curvature:\n   $$H_2 = \\begin{bmatrix}\n   2  0.5  0 \\\\\n   0.5  -0.5  0 \\\\\n   0  0  0.2\n   \\end{bmatrix}.$$\n3. Degenerate top curvature (repeated largest eigenvalue) $3 \\times 3$:\n   $$H_3 = \\begin{bmatrix}\n   3  0  0 \\\\\n   0  3  0 \\\\\n   0  0  1\n   \\end{bmatrix}.$$\n4. Boundary case (no curvature) $3 \\times 3$:\n   $$H_4 = \\begin{bmatrix}\n   0  0  0 \\\\\n   0  0  0 \\\\\n   0  0  0\n   \\end{bmatrix}.$$\n5. Ill-conditioned symmetric positive definite $5 \\times 5$:\n   $$H_5 = \\begin{bmatrix}\n   10^{-6}  10^{-5}  0  0  0 \\\\\n   10^{-5}  10^{-2}  10^{-4}  0  0 \\\\\n   0  10^{-4}  10^{-1}  10^{-3}  0 \\\\\n   0  0  10^{-3}  1  10^{-2} \\\\\n   0  0  0  10^{-2}  10\n   \\end{bmatrix}.$$\n\nProgram requirements:\n- For each test matrix $H$, approximate the largest algebraic eigenvalue using the iterative procedure described above.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite $[H_1,H_2,H_3,H_4,H_5]$. Each entry must be the float rounded to six decimal places, with no additional text. For example, an output must look like $[x_1,x_2,x_3,x_4,x_5]$ where each $x_i$ is the rounded float for matrix $H_i$.", "solution": "The problem requires the implementation of an iterative algorithm to find the largest algebraic eigenvalue of a given real symmetric matrix $H$. The physical context provided is the Hessian matrix $H(\\mathbf{w})$ of a loss function $L(\\mathbf{w})$ in deep learning, where the largest eigenvalue corresponds to the direction of maximum local curvature. The specified algorithm is the Power Iteration method, coupled with the Rayleigh quotient for estimating the eigenvalue.\n\nThe method is based on the principle that for a diagonalizable matrix $H$, repeatedly applying it to an arbitrary non-zero vector $\\mathbf{v}_0$ will progressively align the resulting vector with the eigenvector corresponding to the eigenvalue of largest magnitude, denoted $\\lambda_{dom}$. Let the eigenvalues of $H$ be $|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_d|$, with corresponding eigenvectors $\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_d$. An initial vector $\\mathbf{v}_0$ can be expressed as a linear combination of these eigenvectors: $\\mathbf{v}_0 = c_1\\mathbf{u}_1 + c_2\\mathbf{u}_2 + \\dots + c_d\\mathbf{u}_d$. Assuming $c_1 \\neq 0$, applying $H$ repeatedly gives:\n$$H^k\\mathbf{v}_0 = c_1\\lambda_1^k\\mathbf{u}_1 + c_2\\lambda_2^k\\mathbf{u}_2 + \\dots + c_d\\lambda_d^k\\mathbf{u}_d = \\lambda_1^k \\left( c_1\\mathbf{u}_1 + c_2\\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k\\mathbf{u}_2 + \\dots + c_d\\left(\\frac{\\lambda_d}{\\lambda_1}\\right)^k\\mathbf{u}_d \\right)$$\nIf $|\\lambda_1|  |\\lambda_2|$ (a unique dominant eigenvalue in magnitude), the terms $(\\frac{\\lambda_i}{\\lambda_1})^k$ for $i  1$ approach $0$ as $k \\to \\infty$. Thus, the vector $H^k\\mathbf{v}_0$ becomes increasingly parallel to the dominant eigenvector $\\mathbf{u}_1$. To prevent the magnitude of $H^k\\mathbf{v}_0$ from diverging or vanishing, the vector is normalized at each step. This leads to the iterative update rule for the vector:\n$$\\mathbf{v}_{k+1} = \\frac{H\\mathbf{v}_k}{\\|H\\mathbf{v}_k\\|_2}$$\nwhere $\\mathbf{v}_k$ is the normalized vector estimate at iteration $k$.\n\nOnce an estimate for the eigenvector $\\mathbf{v}_k$ is available, the corresponding eigenvalue can be estimated using the Rayleigh quotient, defined for a non-zero vector $\\mathbf{v}$ as:\n$$R_H(\\mathbf{v}) = \\frac{\\mathbf{v}^\\top H \\mathbf{v}}{\\mathbf{v}^\\top \\mathbf{v}}$$\nIf $\\mathbf{v}$ is an exact eigenvector, then $H\\mathbf{v} = \\lambda\\mathbf{v}$, and the Rayleigh quotient yields the exact eigenvalue $\\lambda$. Since our iterates $\\mathbf{v}_k$ are normalized such that $\\mathbf{v}_k^\\top \\mathbf{v}_k = 1$, the expression simplifies to $R_H(\\mathbf{v}_k) = \\mathbf{v}_k^\\top H \\mathbf{v}_k$. As $\\mathbf{v}_k$ converges to the dominant eigenvector $\\mathbf{u}_1$, the Rayleigh quotient $R_H(\\mathbf{v}_k)$ converges to the dominant eigenvalue $\\lambda_1$. For all the test cases provided, the largest algebraic eigenvalue is positive and also has the largest magnitude, so this method will correctly converge to the desired value.\n\nThe algorithm to be implemented, following the problem's specifications, is as follows:\n\n$1$. **Initialization**:\n   - Given a matrix $H \\in \\mathbb{R}^{d \\times d}$.\n   - The initial vector $\\mathbf{v}_0$ is the all-ones vector of dimension $d$, normalized to have a Euclidean norm of $1$: $\\mathbf{v}_0 = \\frac{1}{\\sqrt{d}}[1, 1, \\dots, 1]^\\top$.\n   - The convergence tolerance is set to $\\epsilon = 10^{-10}$.\n   - The maximum number of iterations is $N_{max} = 1000$.\n   - Initialize eigenvalue estimates, for instance $\\lambda_{current} = 0$ and $\\lambda_{previous}$ to a value like $\\infty$ to ensure the first iteration's convergence check does not pass.\n\n$2$. **Iteration**: For $k = 0, 1, 2, \\dots, N_{max}-1$:\n   a. Check for convergence: If $|\\lambda_{current} - \\lambda_{previous}|  \\epsilon$, the algorithm has converged. Terminate and return $\\lambda_{current}$.\n   b. Update the previous eigenvalue estimate: $\\lambda_{previous} \\leftarrow \\lambda_{current}$.\n   c. Compute the next unnormalized vector: $\\mathbf{w} = H \\mathbf{v}_k$.\n   d. Check for the zero-eigenvalue case: if $\\|\\mathbf{w}\\|_2$ is below a machine-safe threshold, it implies $\\mathbf{v}_k$ is in the null space of $H$ (or $H$ is the zero matrix). The corresponding eigenvalue is $0$. Per the instructions, terminate and return $0$.\n   e. Normalize the vector to obtain the next iterate: $\\mathbf{v}_{k+1} = \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|_2}$. Let this new vector be denoted simply as $\\mathbf{v}$ for the next step.\n   f. Update the current eigenvalue estimate by evaluating the Rayleigh quotient with the newly computed vector $\\mathbf{v}$: $\\lambda_{current} = \\mathbf{v}^\\top H \\mathbf{v}$. This requires a second matrix-vector product within the loop but strictly adheres to the sequence of operations described in the problem.\n\n$3$. **Termination**: If the loop completes without convergence (i.e., reaches $N_{max}$ iterations), return the last computed value of $\\lambda_{current}$. The final result for each test case is rounded to six decimal places.\n\nThis procedure constitutes a complete and robust method for approximating the largest algebraic eigenvalue as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_iteration(H: np.ndarray, tol: float = 1e-10, max_iter: int = 1000) -> float:\n    \"\"\"\n    Approximates the largest algebraic eigenvalue of a symmetric matrix H\n    using the power iteration method with the Rayleigh quotient.\n\n    Args:\n        H: The symmetric matrix (as a numpy array).\n        tol: The tolerance for convergence, based on the absolute change\n             in the Rayleigh quotient.\n        max_iter: The maximum number of iterations.\n\n    Returns:\n        The estimated largest algebraic eigenvalue.\n    \"\"\"\n    # Get the dimension of the matrix\n    d = H.shape[0]\n\n    # 1. Initialize the vector v0 to the normalized all-ones vector.\n    v = np.ones(d, dtype=H.dtype)\n    v /= np.linalg.norm(v)\n\n    # Initialize eigenvalue estimates\n    lambda_current = 0.0\n    lambda_previous = np.inf  # Set to infinity to ensure the first diff is large\n\n    # A small number to check for zero vectors\n    machine_eps = np.finfo(H.dtype).eps\n\n    # 2. Iterate until convergence or max iterations\n    for _ in range(max_iter):\n        # Check for convergence based on the change in the eigenvalue estimate\n        if np.abs(lambda_current - lambda_previous)  tol:\n            break\n\n        # Store the previous eigenvalue estimate\n        lambda_previous = lambda_current\n\n        # Compute the Hessian-vector product\n        w = H @ v\n        \n        # Calculate the norm of the resulting vector\n        w_norm = np.linalg.norm(w)\n\n        # Check for the special case where H*v is the zero vector\n        if w_norm  machine_eps:\n            # The eigenvalue is 0. Terminate and return 0.\n            lambda_current = 0.0\n            break\n\n        # Normalize the vector to get the next iterate\n        v = w / w_norm\n\n        # Update the eigenvalue estimate using the Rayleigh quotient with the new vector v.\n        # This requires a second matrix-vector product in the loop as per a\n        # literal interpretation of the problem statement \"compute next direction...,\n        # then evaluate the Rayleigh quotient\".\n        lambda_current = v.T @ (H @ v)\n\n    return lambda_current\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Test suite as specified in the problem statement\n    H1 = np.array([\n        [4.0, 1.0, 0.0],\n        [1.0, 3.0, 0.0],\n        [0.0, 0.0, 2.0]\n    ])\n\n    H2 = np.array([\n        [2.0, 0.5, 0.0],\n        [0.5, -0.5, 0.0],\n        [0.0, 0.0, 0.2]\n    ])\n\n    H3 = np.array([\n        [3.0, 0.0, 0.0],\n        [0.0, 3.0, 0.0],\n        [0.0, 0.0, 1.0]\n    ])\n\n    H4 = np.array([\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0]\n    ])\n    \n    H5 = np.array([\n        [1e-6, 1e-5, 0.0, 0.0, 0.0],\n        [1e-5, 1e-2, 1e-4, 0.0, 0.0],\n        [0.0, 1e-4, 1e-1, 1e-3, 0.0],\n        [0.0, 0.0, 1e-3, 1.0, 1e-2],\n        [0.0, 0.0, 0.0, 1e-2, 10.0]\n    ])\n\n    test_cases = [H1, H2, H3, H4, H5]\n    \n    results = []\n    for H in test_cases:\n        # Calculate the largest eigenvalue for the current matrix\n        largest_eigenvalue = power_iteration(H)\n        \n        # Format the result to six decimal places and append\n        results.append(f\"{largest_eigenvalue:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3186508"}]}