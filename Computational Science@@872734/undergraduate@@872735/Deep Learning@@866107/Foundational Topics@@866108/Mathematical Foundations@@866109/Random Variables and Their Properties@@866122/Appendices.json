{"hands_on_practices": [{"introduction": "The engine of modern deep learning is stochastic optimization, where model parameters are updated using gradients computed on small \"mini-batches\" of data. This exercise treats the loss on each training example as a random variable, making the mini-batch loss a sample mean. By deriving the relationship between batch size $B$ and the variance of this loss estimate, you will gain a fundamental understanding of \"gradient noise\" and the critical trade-off between computational cost and estimator precision that governs the training of neural networks.", "problem": "Consider a deep learning training scenario where each per-example loss is treated as a random variable. Let the per-example loss be denoted by $\\ell$, and assume that $\\ell$ has a finite mean $\\mu = \\mathbb{E}[\\ell]$ and finite variance $\\sigma^{2} = \\mathrm{Var}(\\ell)$. Let $\\{\\ell_{i}\\}_{i=1}^{B}$ be independent and identically distributed draws of the loss from a shuffled dataset in one training step, and define the mini-batch average loss as\n$$\n\\hat{L}_{B} = \\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}.\n$$\nYou conduct a pilot run with batch size $B=1$ over a large number of examples to estimate the variability of $\\ell$, obtaining an unbiased sample variance estimate $\\hat{\\sigma}^{2} = 0.09$ for the per-example loss. You plan to choose a batch size $B$ such that the variance of the mini-batch average $\\hat{L}_{B}$ equals a target fluctuation level $v^{\\star} = 0.001$.\n\nStarting only from core definitions of expectation and variance for independent random variables, derive the variance of $\\hat{L}_{B}$ in terms of $\\sigma^{2}$ and $B$, and determine the batch size $B$ that achieves $\\mathrm{Var}(\\hat{L}_{B}) = v^{\\star}$ when $\\sigma^{2}$ is approximated by $\\hat{\\sigma}^{2}$. Provide your final answer as a single integer. No rounding is required.", "solution": "The problem is first validated for scientific and logical soundness.\n\n### Step 1: Extract Givens\n- The per-example loss is a random variable, $\\ell$.\n- The mean of the loss is finite: $\\mu = \\mathbb{E}[\\ell]$.\n- The variance of the loss is finite: $\\sigma^{2} = \\mathrm{Var}(\\ell)$.\n- A mini-batch consists of $\\{\\ell_{i}\\}_{i=1}^{B}$ independent and identically distributed (i.i.d.) draws of the loss.\n- The mini-batch average loss is defined as $\\hat{L}_{B} = \\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}$.\n- An unbiased sample variance estimate for $\\ell$ is given as $\\hat{\\sigma}^{2} = 0.09$.\n- A target variance for the mini-batch average loss is specified as $v^{\\star} = 0.001$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard, fundamental problem in the statistical analysis of stochastic optimization methods used in deep learning. The concepts of sample mean, population variance, and the variance of the sample mean are cornerstones of statistics. The problem provides all necessary data ($\\hat{\\sigma}^{2}$, $v^{\\star}$) and definitions to arrive at a unique, meaningful solution for the batch size $B$. The provided values are realistic. The reference to the Law of Large Numbers (LLN) and Central Limit Theorem (CLT) is contextually appropriate, as these theorems provide the theoretical foundation for why the mini-batch average loss $\\hat{L}_{B}$ is a good estimator for the true expected loss $\\mu$, and why its variance is a critical quantity to control. The problem is free from any of the invalidating flaws listed in the instructions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Solution Derivation\nThe objective is to derive the variance of the mini-batch average loss, $\\mathrm{Var}(\\hat{L}_{B})$, and then determine the batch size $B$ that satisfies the target condition.\n\nThe mini-batch average loss is defined as:\n$$\n\\hat{L}_{B} = \\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}\n$$\nWe seek its variance, $\\mathrm{Var}(\\hat{L}_{B})$. We start from the core definition of variance and its properties for linear combinations of random variables.\n\nFirst, we use the property that for any random variable $X$ and constant $c$, $\\mathrm{Var}(cX) = c^{2}\\mathrm{Var}(X)$. Here, our random variable is the sum $\\sum_{i=1}^{B} \\ell_{i}$ and the constant is $\\frac{1}{B}$.\n$$\n\\mathrm{Var}(\\hat{L}_{B}) = \\mathrm{Var}\\left(\\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}\\right) = \\left(\\frac{1}{B}\\right)^{2} \\mathrm{Var}\\left(\\sum_{i=1}^{B} \\ell_{i}\\right) = \\frac{1}{B^{2}} \\mathrm{Var}\\left(\\sum_{i=1}^{B} \\ell_{i}\\right)\n$$\nNext, we determine the variance of the sum of the random variables. For any two *independent* random variables $X$ and $Y$, the variance of their sum is the sum of their variances: $\\mathrm{Var}(X+Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)$. The problem states that the per-example losses $\\{\\ell_{i}\\}_{i=1}^{B}$ are independent. By induction, this property extends to the sum of $B$ independent random variables:\n$$\n\\mathrm{Var}\\left(\\sum_{i=1}^{B} \\ell_{i}\\right) = \\sum_{i=1}^{B} \\mathrm{Var}(\\ell_{i})\n$$\nThe problem also states that the losses $\\{\\ell_{i}\\}_{i=1}^{B}$ are identically distributed. This means that each $\\ell_{i}$ has the same variance, $\\mathrm{Var}(\\ell_{i}) = \\sigma^{2}$ for all $i \\in \\{1, 2, \\dots, B\\}$. Therefore, the sum of the variances is:\n$$\n\\sum_{i=1}^{B} \\mathrm{Var}(\\ell_{i}) = \\sum_{i=1}^{B} \\sigma^{2} = B\\sigma^{2}\n$$\nSubstituting this result back into our expression for $\\mathrm{Var}(\\hat{L}_{B})$:\n$$\n\\mathrm{Var}(\\hat{L}_{B}) = \\frac{1}{B^{2}} (B\\sigma^{2}) = \\frac{\\sigma^{2}}{B}\n$$\nThis derivation shows that the variance of the mini-batch average loss is inversely proportional to the batch size $B$. This relationship is fundamental to understanding the trade-off between computational cost and gradient estimate accuracy in deep learning. The LLN guarantees that $\\hat{L}_{B}$ converges to $\\mu$ as $B \\to \\infty$, and the CLT characterizes the distribution of the fluctuations around $\\mu$ for finite $B$, whose scale is governed by $\\mathrm{Var}(\\hat{L}_{B}) = \\frac{\\sigma^{2}}{B}$.\n\nNow, we must find the batch size $B$ such that the variance of the mini-batch average equals the target fluctuation level $v^{\\star}$.\n$$\n\\mathrm{Var}(\\hat{L}_{B}) = v^{\\star}\n$$\nUsing our derived formula:\n$$\n\\frac{\\sigma^{2}}{B} = v^{\\star}\n$$\nThe problem provides an unbiased sample variance estimate $\\hat{\\sigma}^{2} = 0.09$ from a pilot run, which we use as an approximation for the true population variance $\\sigma^{2}$. Substituting the given values:\n$$\n\\frac{\\hat{\\sigma}^{2}}{B} = v^{\\star} \\implies \\frac{0.09}{B} = 0.001\n$$\nSolving for $B$:\n$$\nB = \\frac{0.09}{0.001} = \\frac{9 \\times 10^{-2}}{1 \\times 10^{-3}} = 9 \\times 10^{1} = 90\n$$\nThus, a batch size of $B=90$ is required to achieve the target variance of $v^{\\star} = 0.001$ for the mini-batch average loss, based on the provided estimate of the per-example loss variance.", "answer": "$$\n\\boxed{90}\n$$", "id": "3166774"}, {"introduction": "Real-world datasets are rarely perfect and often contain outliers, which can severely corrupt the training process if not handled carefully. This practice moves from analysis to principled design, challenging you to construct a robust loss function from first principles. By modeling outlier magnitudes as random variables from a heavy-tailed distribution, you will derive the celebrated Huber loss and analyze how its \"influence function\" tames the effect of extreme data points, a crucial skill for building reliable machine learning systems.", "problem": "A scalar regression model in deep learning predicts a response $\\hat{y} = f_{\\theta}(x)$ with parameter vector $\\theta$. For a single training pair $(x,y)$, define the residual $u = y - \\hat{y}$. In the presence of extreme outliers, assume the residuals corresponding to such outliers are positive and their magnitudes follow a heavy-tail law. You will design a robust loss and analyze the random contribution of an extreme outlier to the gradient via its influence function, starting from core definitions in probability and convex analysis.\n\nAssume the following design requirements for a robust per-sample loss $\\rho(u)$:\n- $\\rho(u)$ is symmetric, convex, and continuously differentiable in $u$.\n- There exists a threshold $c0$ such that for $|u| \\leq c$ the loss is quadratic, and for $|u|c$ the loss grows linearly in $|u|$.\n- The function value and the first derivative match continuously at $u=\\pm c$.\n\nDefine the influence function $\\psi(u)$ to be the derivative of $\\rho(u)$ with respect to the residual, that is, $\\psi(u) = \\frac{d}{du}\\rho(u)$. Model extreme outlier magnitudes as a random variable $R$ supported on $[x_{m},\\infty)$ with Pareto density\n$$\nf_{R}(r) = \\alpha x_{m}^{\\alpha} r^{-(\\alpha+1)}, \\quad r \\geq x_{m},\n$$\nfor shape parameter $\\alpha1$ and scale $x_{m}0$. Assume the robust threshold satisfies $c \\geq x_{m}$.\n\nStarting only from the definitions above and standard properties of expectations and integrals of random variables, do the following:\n- Derive the explicit piecewise form of $\\rho(u)$ that meets the design requirements, and its influence function $\\psi(u)$.\n- Using this $\\psi(u)$, compute the closed-form analytic expression of the expectation $\\mathbb{E}[\\psi(R)]$ as a function of $\\alpha$, $c$, and $x_{m}$.\n\nYour final answer must be a single closed-form expression in terms of $\\alpha$, $c$, and $x_{m}$. No numerical approximation or rounding is required. Do not include any units in your answer.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of robust statistics and probability theory, well-posed, and objective. It provides a self-contained set of definitions and constraints to derive a unique and meaningful solution. We may now proceed with the solution.\n\nThe problem asks for two main derivations: first, the explicit form of a robust loss function $\\rho(u)$ and its derivative (the influence function) $\\psi(u)$ based on a set of design requirements; second, the expected value of the influence function $\\mathbb{E}[\\psi(R)]$ for a random variable $R$ following a Pareto distribution, which models the magnitude of extreme outlier residuals.\n\n**Part 1: Derivation of the Loss Function $\\rho(u)$ and Influence Function $\\psi(u)$**\n\nThe design requirements for the loss function $\\rho(u)$ are:\n1.  $\\rho(u)$ is symmetric, i.e., $\\rho(u) = \\rho(-u)$.\n2.  $\\rho(u)$ is convex and continuously differentiable.\n3.  For $|u| \\leq c$ with $c0$, $\\rho(u)$ is quadratic.\n4.  For $|u|  c$, $\\rho(u)$ grows linearly in $|u|$.\n5.  $\\rho(u)$ and its first derivative, $\\psi(u) = \\frac{d\\rho}{du}$, are continuous at $u = \\pm c$.\n\nLet's construct the function based on these requirements. We can focus on the case $u \\geq 0$ due to symmetry.\n\nFor $0 \\leq u \\leq c$, the loss is quadratic. A general symmetric quadratic function centered at $0$ is of the form $A u^2 + B$. For a loss function, it is standard convention that zero error incurs zero loss, so $\\rho(0) = 0$, which implies $B=0$. The problem states the loss is \"quadratic,\" which could imply a form $A u^2$. To align with the standard a robust alternative to the mean squared error (or $L_2$) loss, which is $\\frac{1}{2}u^2$, we will set the quadratic part to be $\\rho(u) = \\frac{1}{2}u^2$. This choice defines a specific scaling for the loss function, which is standard for the Huber loss.\nSo, for $|u| \\leq c$:\n$$\n\\rho(u) = \\frac{1}{2}u^2\n$$\nThe derivative, or influence function $\\psi(u)$, in this region is:\n$$\n\\psi(u) = \\frac{d}{du}\\left(\\frac{1}{2}u^2\\right) = u\n$$\n\nFor $|u|  c$, the loss grows linearly in $|u|$. For $u  c$, this means $\\rho(u)$ is a linear function of $u$, say $\\rho(u) = K u + D$ for some constants $K$ and $D$.\nThe derivative in this region is:\n$$\n\\psi(u) = \\frac{d}{du}(K u + D) = K\n$$\n\nNow we enforce the continuity conditions at $u=c$.\nFirst, the influence function $\\psi(u)$ must be continuous at $u=c$.\n$$\n\\lim_{u \\to c^-} \\psi(u) = \\lim_{u \\to c^+} \\psi(u)\n$$\nFrom our forms, this means:\n$$\nc = K\n$$\nSo, the constant $K$ is equal to $c$. For $u  c$, $\\psi(u) = c$.\n\nSecond, the loss function $\\rho(u)$ must be continuous at $u=c$.\n$$\n\\lim_{u \\to c^-} \\rho(u) = \\lim_{u \\to c^+} \\rho(u)\n$$\nUsing the forms for $\\rho(u)$:\n$$\n\\frac{1}{2}c^2 = K c + D\n$$\nSubstituting $K=c$, we get:\n$$\n\\frac{1}{2}c^2 = c \\cdot c + D \\implies \\frac{1}{2}c^2 = c^2 + D \\implies D = -\\frac{1}{2}c^2\n$$\nSo, for $u  c$, the loss function is $\\rho(u) = cu - \\frac{1}{2}c^2$.\n\nBy symmetry $\\rho(u)=\\rho(-u)$, for $u  -c$, we must have $\\rho(u) = c(-u) - \\frac{1}{2}c^2$. This can be summarized for $|u|c$ as $\\rho(u) = c|u| - \\frac{1}{2}c^2$.\n\nThe explicit piecewise form of $\\rho(u)$, known as the Huber loss, is:\n$$\n\\rho(u) = \\begin{cases} \\frac{1}{2}u^2  \\text{if } |u| \\leq c \\\\ c|u| - \\frac{1}{2}c^2  \\text{if } |u|  c \\end{cases}\n$$\nThe corresponding influence function $\\psi(u) = \\rho'(u)$ is:\n$$\n\\psi(u) = \\begin{cases} u  \\text{if } |u| \\leq c \\\\ c \\cdot \\text{sgn}(u)  \\text{if } |u|  c \\end{cases}\n$$\nwhere $\\text{sgn}(u)$ is the sign function.\n\n**Part 2: Computation of the Expectation $\\mathbb{E}[\\psi(R)]$**\n\nWe need to compute the expectation of $\\psi(R)$, where $R$ is a random variable modeling the magnitude of positive outlier residuals. The probability density function (PDF) of $R$ is given by the Pareto distribution:\n$$\nf_{R}(r) = \\alpha x_{m}^{\\alpha} r^{-(\\alpha+1)}, \\quad \\text{for } r \\geq x_{m}\n$$\nwith parameters $\\alpha  1$ and $x_{m}  0$. We are also given the constraint $c \\geq x_{m}$.\n\nThe expectation $\\mathbb{E}[\\psi(R)]$ is defined by the integral:\n$$\n\\mathbb{E}[\\psi(R)] = \\int_{x_{m}}^{\\infty} \\psi(r) f_{R}(r) dr\n$$\nSince $R$ represents a positive magnitude, its support is $[x_m, \\infty)$, where $x_m  0$. We only need the definition of $\\psi(u)$ for $u0$:\n$$\n\\psi(r) = \\begin{cases} r  \\text{if } 0  r \\leq c \\\\ c  \\text{if } r  c \\end{cases}\n$$\nGiven the constraint $c \\geq x_{m}$, the integration range $[x_{m}, \\infty)$ must be split at the point $r=c$. The integral becomes:\n$$\n\\mathbb{E}[\\psi(R)] = \\int_{x_{m}}^{c} \\psi(r) f_{R}(r) dr + \\int_{c}^{\\infty} \\psi(r) f_{R}(r) dr\n$$\nSubstituting the expressions for $\\psi(r)$ and $f_R(r)$:\n$$\n\\mathbb{E}[\\psi(R)] = \\int_{x_{m}}^{c} r \\left(\\alpha x_{m}^{\\alpha} r^{-(\\alpha+1)}\\right) dr + \\int_{c}^{\\infty} c \\left(\\alpha x_{m}^{\\alpha} r^{-(\\alpha+1)}\\right) dr\n$$\nLet's evaluate each integral separately.\n\nThe first integral, $I_1$:\n$$\nI_1 = \\int_{x_{m}}^{c} \\alpha x_{m}^{\\alpha} r^{-\\alpha} dr = \\alpha x_{m}^{\\alpha} \\int_{x_{m}}^{c} r^{-\\alpha} dr\n$$\nSince $\\alpha  1$, we have $\\alpha \\neq 1$. The antiderivative of $r^{-\\alpha}$ is $\\frac{r^{-\\alpha+1}}{-\\alpha+1}$.\n$$\nI_1 = \\alpha x_{m}^{\\alpha} \\left[ \\frac{r^{1-\\alpha}}{1-\\alpha} \\right]_{x_{m}}^{c} = \\frac{\\alpha x_{m}^{\\alpha}}{1-\\alpha} \\left( c^{1-\\alpha} - x_{m}^{1-\\alpha} \\right)\n$$\nRearranging the term $\\frac{\\alpha}{1-\\alpha}$ to $\\frac{-\\alpha}{\\alpha-1}$ and distributing:\n$$\nI_1 = \\frac{\\alpha}{\\alpha-1} \\left( x_{m}^{\\alpha} x_{m}^{1-\\alpha} - x_{m}^{\\alpha} c^{1-\\alpha} \\right) = \\frac{\\alpha}{\\alpha-1} \\left( x_{m} - x_{m}^{\\alpha} c^{1-\\alpha} \\right)\n$$\n\nThe second integral, $I_2$:\n$$\nI_2 = \\int_{c}^{\\infty} c \\alpha x_{m}^{\\alpha} r^{-(\\alpha+1)} dr = c \\alpha x_{m}^{\\alpha} \\int_{c}^{\\infty} r^{-(\\alpha+1)} dr\n$$\nThe antiderivative of $r^{-(\\alpha+1)}$ is $\\frac{r^{-(\\alpha+1)+1}}{-(\\alpha+1)+1} = \\frac{r^{-\\alpha}}{-\\alpha}$.\n$$\nI_2 = c \\alpha x_{m}^{\\alpha} \\left[ \\frac{r^{-\\alpha}}{-\\alpha} \\right]_{c}^{\\infty} = c \\alpha x_{m}^{\\alpha} \\left( \\lim_{b \\to \\infty} \\frac{b^{-\\alpha}}{-\\alpha} - \\frac{c^{-\\alpha}}{-\\alpha} \\right)\n$$\nSince $\\alpha  1$, we have $\\alpha  0$, so $\\lim_{b \\to \\infty} b^{-\\alpha} = 0$.\n$$\nI_2 = c \\alpha x_{m}^{\\alpha} \\left( 0 - \\frac{c^{-\\alpha}}{-\\alpha} \\right) = c \\alpha x_{m}^{\\alpha} \\frac{c^{-\\alpha}}{\\alpha} = c \\cdot x_{m}^{\\alpha} \\cdot c^{-\\alpha} = c^{1-\\alpha} x_{m}^{\\alpha}\n$$\n\nNow, we sum the two parts to find the total expectation:\n$$\n\\mathbb{E}[\\psi(R)] = I_1 + I_2 = \\frac{\\alpha}{\\alpha-1} \\left( x_{m} - x_{m}^{\\alpha} c^{1-\\alpha} \\right) + x_{m}^{\\alpha} c^{1-\\alpha}\n$$\nLet's simplify this expression:\n$$\n\\mathbb{E}[\\psi(R)] = \\frac{\\alpha x_{m}}{\\alpha-1} - \\frac{\\alpha}{\\alpha-1} x_{m}^{\\alpha} c^{1-\\alpha} + x_{m}^{\\alpha} c^{1-\\alpha}\n$$\nCombine the terms containing $x_{m}^{\\alpha} c^{1-\\alpha}$:\n$$\n\\mathbb{E}[\\psi(R)] = \\frac{\\alpha x_{m}}{\\alpha-1} + \\left(1 - \\frac{\\alpha}{\\alpha-1}\\right) x_{m}^{\\alpha} c^{1-\\alpha}\n$$\nThe term in the parenthesis simplifies to:\n$$\n1 - \\frac{\\alpha}{\\alpha-1} = \\frac{\\alpha-1 - \\alpha}{\\alpha-1} = \\frac{-1}{\\alpha-1}\n$$\nSubstituting this back into the expression for the expectation:\n$$\n\\mathbb{E}[\\psi(R)] = \\frac{\\alpha x_{m}}{\\alpha-1} - \\frac{1}{\\alpha-1} x_{m}^{\\alpha} c^{1-\\alpha}\n$$\nThis gives the final closed-form expression:\n$$\n\\mathbb{E}[\\psi(R)] = \\frac{\\alpha x_{m} - x_{m}^{\\alpha} c^{1-\\alpha}}{\\alpha-1}\n$$\nThis expression is a function of $\\alpha$, $c$, and $x_m$ as required.", "answer": "$$\\boxed{\\frac{\\alpha x_{m} - x_{m}^{\\alpha} c^{1-\\alpha}}{\\alpha-1}}$$", "id": "3166703"}, {"introduction": "For deep learning models to be trustworthy in critical applications, they must not only be accurate but also provide a reliable sense of their own uncertainty. This hands-on coding exercise explores how model ensembles can be used to quantify predictive uncertainty. You will apply the Law of Total Variance, a cornerstone theorem of probability, to decompose the total variance of an ensemble's predictions into two meaningful components: aleatoric uncertainty (inherent data noise) and epistemic uncertainty (model ignorance), providing a powerful tool for diagnosing and understanding model behavior.", "problem": "Consider a supervised deep learning setting where a collection of independently trained Neural Networks (NNs) form an ensemble. For a fixed dataset of inputs, let $N$ denote the number of distinct inputs and let $K$ denote the number of ensemble members. For each input index $i \\in \\{1,\\dots,N\\}$ and ensemble member index $m \\in \\{1,\\dots,K\\}$, denote the scalar prediction by $y_{i,m} \\in \\mathbb{R}$. Define a random input $X$ by selecting one of the $N$ inputs uniformly at random, and define a random prediction $Y$ by selecting one of the $K$ ensemble members uniformly at random and taking its prediction at the selected input. Using only the core definitions of expectation and variance and the definition of conditional expectation, derive the variance decomposition conditioned on $X$, and then apply this decomposition to quantify the two constituent terms empirically from the finite matrix of ensemble predictions $\\{y_{i,m}\\}$.\n\nYou must proceed from the following foundational base:\n- The variance of a random variable $Z$ is defined as $\\mathrm{Var}(Z) = \\mathbb{E}\\big[(Z - \\mathbb{E}[Z])^2\\big] = \\mathbb{E}[Z^2] - \\big(\\mathbb{E}[Z]\\big)^2$.\n- The conditional expectation $\\mathbb{E}[Z \\mid W]$ is defined as a random variable (a function of $W$) satisfying $\\mathbb{E}[Z \\mathbf{1}_A] = \\mathbb{E}\\big[\\mathbb{E}[Z \\mid W] \\mathbf{1}_A\\big]$ for any event $A$ measurable with respect to $W$.\n- The law of total expectation (tower property) states $\\mathbb{E}[Z] = \\mathbb{E}\\big[\\mathbb{E}[Z \\mid W]\\big]$ for any pair of random variables $(Z,W)$.\n\nYour program must:\n- Generate synthetic ensemble predictions $\\{y_{i,m}\\}$ using the following model for each test case. Let $x_i \\in \\mathbb{R}$ denote the scalar input at index $i$. Define a deterministic base function $f(x) = a x + c$, a per-model bias $b_m \\sim \\mathcal{N}(0,\\tau^2)$ independent across $m$, and per-prediction noise $\\epsilon_{i,m} \\sim \\mathcal{N}\\big(0, \\sigma(x_i)^2\\big)$ with heteroscedastic standard deviation $\\sigma(x) = \\sigma_0 + \\sigma_1 |x|$. Then set each prediction as\n$$\ny_{i,m} = f(x_i) + b_m + \\epsilon_{i,m}.\n$$\nWithin each test case, treat the empirical distribution as uniform over all observed inputs and ensemble members. Specifically, use empirical expectations computed as averages over the finite sets with denominators $N$, $K$, and $N K$ (not unbiased corrections):\n- For each input $i$, compute the conditional mean $\\mu_i = \\frac{1}{K} \\sum_{m=1}^{K} y_{i,m}$ and the conditional variance $v_i = \\frac{1}{K} \\sum_{m=1}^{K} \\big(y_{i,m} - \\mu_i\\big)^2$.\n- Compute the empirical analogs of the two decomposition terms and the total variance as\n$$\nA = \\frac{1}{N} \\sum_{i=1}^{N} v_i, \\quad\nB = \\frac{1}{N} \\sum_{i=1}^{N} \\big(\\mu_i - \\bar{\\mu}\\big)^2, \\quad\nC = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\big(y_{i,m} - \\bar{y}\\big)^2,\n$$\nwhere $\\bar{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} \\mu_i$ and $\\bar{y} = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} y_{i,m}$. Also compute the discrepancy\n$$\nD = C - (A + B),\n$$\nwhich must be numerically close to $0$ if the decomposition is correctly implemented with empirical expectations.\n\nImplement and evaluate the following test suite, where each case provides $(N,K,a,c,\\tau,\\sigma_0,\\sigma_1,\\text{seed})$ and inputs $x_i$ are independently drawn from $\\mathcal{N}(0,1)$ using the provided seed:\n- Test case $1$ (happy path, heteroscedastic, moderate ensemble diversity): $N=200$, $K=5$, $a=2.0$, $c=0.5$, $\\tau=0.3$, $\\sigma_0=0.2$, $\\sigma_1=0.5$, $\\text{seed}=1234$.\n- Test case $2$ (boundary, no within-input variability): $N=100$, $K=7$, $a=1.5$, $c=-1.0$, $\\tau=0.0$, $\\sigma_0=0.0$, $\\sigma_1=0.0$, $\\text{seed}=42$.\n- Test case $3$ (boundary, no between-input variability in ensemble mean): $N=150$, $K=4$, $a=0.0$, $c=3.0$, $\\tau=0.8$, $\\sigma_0=0.5$, $\\sigma_1=0.0$, $\\text{seed}=7$.\n- Test case $4$ (edge, single input): $N=1$, $K=10$, $a=0.7$, $c=0.0$, $\\tau=1.0$, $\\sigma_0=0.3$, $\\sigma_1=0.0$, $\\text{seed}=31415$.\n- Test case $5$ (edge, single model): $N=120$, $K=1$, $a=1.0$, $c=0.0$, $\\tau=0.0$, $\\sigma_0=0.2$, $\\sigma_1=0.0$, $\\text{seed}=98765$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, with each test case reported as a sublist $[C,A,B,D]$ of four floats rounded to six decimal places. For example, the output must look like\n$$\n[[C_1,A_1,B_1,D_1],[C_2,A_2,B_2,D_2],\\dots,[C_5,A_5,B_5,D_5]].\n$$\nNo spaces are permitted anywhere in the printed line.", "solution": "The problem requires a derivation of the law of total variance and its application to a finite set of ensemble predictions. The solution is presented in two parts: first, the theoretical derivation of the variance decomposition, followed by a demonstration that this identity holds algebraically for the specific empirical estimators provided.\n\n### Part 1: Derivation of the Law of Total Variance\n\nThe goal is to prove the identity $\\mathrm{Var}(Y) = \\mathbb{E}[\\mathrm{Var}(Y \\mid X)] + \\mathrm{Var}(\\mathbb{E}[Y \\mid X])$. We start from the foundational definitions provided.\n\nThe variance of a random variable $Y$ is defined as:\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2\n$$\nWe apply the law of total expectation, $\\mathbb{E}[Z] = \\mathbb{E}\\big[\\mathbb{E}[Z \\mid W]\\big]$, to each term on the right-hand side, conditioning on the random variable $X$.\n\nFor the term $\\mathbb{E}[Y]$, we have:\n$$\n\\mathbb{E}[Y] = \\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\n$$\nwhere the subscript on the outer expectation, $\\mathbb{E}_X[\\cdot]$, emphasizes that the expectation is taken over the distribution of $X$.\n\nFor the term $\\mathbb{E}[Y^2]$, we similarly have:\n$$\n\\mathbb{E}[Y^2] = \\mathbb{E}_X[\\mathbb{E}[Y^2 \\mid X]]\n$$\nSubstituting these into the variance definition gives:\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_X[\\mathbb{E}[Y^2 \\mid X]] - \\big(\\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\\big)^2\n$$\nNow, we use the definition of conditional variance, $\\mathrm{Var}(Y \\mid X) = \\mathbb{E}[Y^2 \\mid X] - (\\mathbb{E}[Y \\mid X])^2$. This is a random variable that is a function of $X$. We can rearrange this definition to express $\\mathbb{E}[Y^2 \\mid X]$:\n$$\n\\mathbb{E}[Y^2 \\mid X] = \\mathrm{Var}(Y \\mid X) + (\\mathbb{E}[Y \\mid X])^2\n$$\nSubstituting this expression for $\\mathbb{E}[Y^2 \\mid X]$ back into our equation for $\\mathrm{Var}(Y)$:\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_X\\big[ \\mathrm{Var}(Y \\mid X) + (\\mathbb{E}[Y \\mid X])^2 \\big] - \\big(\\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\\big)^2\n$$\nUsing the linearity of expectation for the outer expectation $\\mathbb{E}_X[\\cdot]$, we can split the first term:\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)] + \\mathbb{E}_X\\big[(\\mathbb{E}[Y \\mid X])^2\\big] - \\big(\\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\\big)^2\n$$\nLet us examine the last two terms. If we define a new random variable $Z = \\mathbb{E}[Y \\mid X]$, which is a function of $X$, then these terms are $\\mathbb{E}_X[Z^2] - (\\mathbb{E}_X[Z])^2$. This is precisely the definition of the variance of $Z$, i.e., $\\mathrm{Var}(Z) = \\mathrm{Var}_X(\\mathbb{E}[Y \\mid X])$.\n\nTherefore, we arrive at the final decomposition:\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}[\\mathrm{Var}(Y \\mid X)] + \\mathrm{Var}(\\mathbb{E}[Y \\mid X])\n$$\nThe first term, $\\mathbb{E}[\\mathrm{Var}(Y \\mid X)]$, is the expected conditional variance, often called the \"aleatoric\" or \"irreducible\" uncertainty. It represents the average variance of predictions for a fixed input. The second term, $\\mathrm{Var}(\\mathbb{E}[Y \\mid X])$, is the variance of the conditional expectation, often called the \"epistemic\" uncertainty or model disagreement. It represents the variability of the ensemble's mean prediction across different inputs.\n\n### Part 2: The Empirical Decomposition as an Algebraic Identity\n\nThe problem defines the random variables $X$ and $Y$ based on a finite matrix of predictions $\\{y_{i,m}\\}$ for $i \\in \\{1, \\dots, N\\}$ and $m \\in \\{1, \\dots, K\\}$. The sampling process is uniform, meaning any pair $(i,m)$ is chosen with probability $1/(NK)$. The empirical quantities $A$, $B$, and $C$ are defined as:\n$$\nA = \\frac{1}{N} \\sum_{i=1}^{N} v_i \\quad \\text{where} \\quad v_i = \\frac{1}{K} \\sum_{m=1}^{K} \\big(y_{i,m} - \\mu_i\\big)^2 \\quad \\text{and} \\quad \\mu_i = \\frac{1}{K} \\sum_{m=1}^{K} y_{i,m}\n$$\n$$\nB = \\frac{1}{N} \\sum_{i=1}^{N} \\big(\\mu_i - \\bar{\\mu}\\big)^2 \\quad \\text{where} \\quad \\bar{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} \\mu_i\n$$\n$$\nC = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\big(y_{i,m} - \\bar{y}\\big)^2 \\quad \\text{where} \\quad \\bar{y} = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} y_{i,m}\n$$\nThese quantities are the empirical analogs of the terms in the theoretical decomposition. We show that for these specific definitions, the identity $C = A + B$ holds algebraically.\n\nFirst, note that the overall mean $\\bar{y}$ is identical to the mean of the conditional means $\\bar{\\mu}$:\n$$\n\\bar{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\frac{1}{K} \\sum_{m=1}^{K} y_{i,m}\\right) = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} y_{i,m} = \\bar{y}\n$$\nNow, we expand the expression for the total variance $C$. We add and subtract the conditional mean $\\mu_i$ inside the square:\n$$\nC = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\big( (y_{i,m} - \\mu_i) + (\\mu_i - \\bar{y}) \\big)^2\n$$\nExpanding the square $(p+q)^2 = p^2 + q^2 + 2pq$:\n$$\nC = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\left[ (y_{i,m} - \\mu_i)^2 + (\\mu_i - \\bar{y})^2 + 2(y_{i,m} - \\mu_i)(\\mu_i - \\bar{y}) \\right]\n$$\nWe can split this into three terms by linearity of summation:\n$$\nC = \\frac{1}{NK} \\sum_{i,m} (y_{i,m} - \\mu_i)^2 + \\frac{1}{NK} \\sum_{i,m} (\\mu_i - \\bar{y})^2 + \\frac{2}{NK} \\sum_{i,m} (y_{i,m} - \\mu_i)(\\mu_i - \\bar{y})\n$$\nLet's analyze each term:\n1.  **First term**:\n    $$\n    \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} (y_{i,m} - \\mu_i)^2 = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{1}{K} \\sum_{m=1}^{K} (y_{i,m} - \\mu_i)^2 \\right) = \\frac{1}{N} \\sum_{i=1}^{N} v_i = A\n    $$\n2.  **Second term**: The summand $(\\mu_i - \\bar{y})^2$ does not depend on the index $m$. The sum over $m$ results in a factor of $K$:\n    $$\n    \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} (\\mu_i - \\bar{y})^2 = \\frac{1}{NK} \\sum_{i=1}^{N} K (\\mu_i - \\bar{y})^2 = \\frac{1}{N} \\sum_{i=1}^{N} (\\mu_i - \\bar{y})^2 = B\n    $$\n    (since $\\bar{y}=\\bar{\\mu}$)\n3.  **Third term (cross-term)**: We can factor out the term that does not depend on $m$:\n    $$\n    \\frac{2}{NK} \\sum_{i=1}^{N} (\\mu_i - \\bar{y}) \\left( \\sum_{m=1}^{K} (y_{i,m} - \\mu_i) \\right)\n    $$\n    The inner sum over $m$ is zero by definition of $\\mu_i$:\n    $$\n    \\sum_{m=1}^{K} (y_{i,m} - \\mu_i) = \\left( \\sum_{m=1}^{K} y_{i,m} \\right) - \\left( \\sum_{m=1}^{K} \\mu_i \\right) = (K \\mu_i) - (K \\mu_i) = 0\n    $$\n    Thus, the entire cross-term is zero.\n\nCombining the results, we have proven that for the given empirical estimators:\n$$\nC = A + B\n$$\nThis means that the discrepancy $D = C - (A+B)$ must be exactly $0$. Any non-zero value observed in a numerical implementation will be solely due to floating-point representation errors.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and empirically validates the law of total variance for an ensemble\n    of neural network predictions.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, K, a, c, tau, sigma0, sigma1, seed)\n        (200, 5, 2.0, 0.5, 0.3, 0.2, 0.5, 1234),\n        (100, 7, 1.5, -1.0, 0.0, 0.0, 0.0, 42),\n        (150, 4, 0.0, 3.0, 0.8, 0.5, 0.0, 7),\n        (1, 10, 0.7, 0.0, 1.0, 0.3, 0.0, 31415),\n        (120, 1, 1.0, 0.0, 0.0, 0.2, 0.0, 98765),\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        N, K, a, c, tau, sigma0, sigma1, seed = case\n        \n        # Set up a random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n        \n        # Generate synthetic data according to the model.\n        # 1. Inputs x_i ~ N(0, 1)\n        x = rng.normal(loc=0.0, scale=1.0, size=N)\n        \n        # 2. Per-model biases b_m ~ N(0, tau^2)\n        b = rng.normal(loc=0.0, scale=tau, size=K)\n        \n        # 3. Base function f(x_i)\n        f_x = a * x + c\n        \n        # 4. Heteroscedastic noise standard deviation sigma(x_i)\n        sigma_x = sigma0 + sigma1 * np.abs(x)\n        \n        # 5. Per-prediction noise epsilon_{i,m} ~ N(0, sigma(x_i)^2)\n        # We broadcast sigma_x (N,) to (N, K) to generate the noise matrix.\n        epsilon_scale = sigma_x[:, np.newaxis]\n        epsilon = rng.normal(loc=0.0, scale=epsilon_scale, size=(N, K))\n        \n        # 6. Final predictions y_{i,m}\n        # We use broadcasting to combine f_x (N,), b (K,), and epsilon (N, K).\n        y = f_x[:, np.newaxis] + b[np.newaxis, :] + epsilon\n        \n        # Compute the empirical quantities as defined in the problem.\n        # ddof=0 ensures we divide by n (population variance) instead of n-1.\n        \n        # Conditional mean for each input i: mu_i\n        mu = np.mean(y, axis=1)  # Shape (N,)\n        \n        # Conditional variance for each input i: v_i\n        v = np.var(y, axis=1, ddof=0)  # Shape (N,)\n        \n        # Term A: Expected value of the conditional variance\n        # A = (1/N) * sum(v_i)\n        A = np.mean(v)\n        \n        # Term B: Variance of the conditional expectation\n        # B = (1/N) * sum((mu_i - bar_mu)^2)\n        # np.var calculates variance against the mean of the input array `mu`.\n        B = np.var(mu, ddof=0)\n        \n        # Term C: Total variance\n        # C = (1/NK) * sum((y_{i,m} - bar_y)^2)\n        # np.var applied to the entire matrix calculates this.\n        C = np.var(y, ddof=0)\n        \n        # Discrepancy D = C - (A + B)\n        # This should be zero up to floating-point precision, as shown in the derivation.\n        D = C - (A + B)\n        \n        case_results = [C, A, B, D]\n        # Format the results for this case to 6 decimal places.\n        case_str = f\"[{','.join([f'{val:.6f}' for val in case_results])}]\"\n        all_results_str.append(case_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3166709"}]}