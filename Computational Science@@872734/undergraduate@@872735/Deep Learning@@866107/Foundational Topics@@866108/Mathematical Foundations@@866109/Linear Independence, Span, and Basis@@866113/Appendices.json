{"hands_on_practices": [{"introduction": "Understanding linear algebra often begins with vectors in $\\mathbb{R}^n$, but the concepts of basis and subspace are far more general. This first practice challenges you to work within the vector space of polynomials, a common structure in modeling and function approximation. By finding a basis for a specific subspace of polynomials, you will solidify your grasp on the core definitions of linear independence and spanning in a more abstract context [@problem_id:1868582].", "problem": "Let $P_3$ be the vector space of all polynomials of degree at most 3 with real coefficients. Consider the subspace $W$ of $P_3$ defined by the set $W = \\{ p(x) \\in P_3 \\mid p(1) = 0 \\}$. Which of the following sets of polynomials forms a basis for the subspace $W$?\n\nA. $S_1 = \\{ x-1, x^2-1, x^3-1 \\}$\n\nB. $S_2 = \\{ x-1, x^2-1, x^2-x \\}$\n\nC. $S_3 = \\{ x-1, x^2-1 \\}$\n\nD. $S_4 = \\{ 1, x, x^2 \\}$\n\nE. $S_5 = \\{ x-1, x^2-1, x^3 \\}$", "solution": "Let $P_{3}$ be the $4$-dimensional real vector space of polynomials of degree at most $3$. Define the linear map $T:P_{3}\\to\\mathbb{R}$ by $T(p)=p(1)$. The subspace $W$ is $W=\\ker(T)$. Since $T$ is surjective (for example, $T(1)=1$), by the rank-nullity theorem,\n$$\n\\dim(W)=\\dim(P_{3})-\\dim(\\operatorname{im}T)=4-1=3.\n$$\nEquivalently, by the factor theorem, $p(1)=0$ if and only if $x-1$ divides $p(x)$, so every $p\\in W$ can be written as $p(x)=(x-1)q(x)$ with $q\\in P_{2}$, again giving $\\dim(W)=3$.\n\nWe test each candidate set.\n\nFor $S_{1}=\\{x-1, x^{2}-1, x^{3}-1\\}$:\n- Membership in $W$: $(x-1)(1)=0$, $(x^{2}-1)(1)=1-1=0$, $(x^{3}-1)(1)=1-1=0$, so all three are in $W$.\n- Linear independence: Suppose\n$$\na(x-1)+b(x^{2}-1)+c(x^{3}-1)=0.\n$$\nExpand and collect coefficients:\n$$\nc x^{3}+b x^{2}+a x+(-a-b-c)=0.\n$$\nEquating coefficients gives\n$$\nc=0,\\quad b=0,\\quad a=0,\\quad -a-b-c=0,\n$$\nso only the trivial solution exists. Thus the three vectors are linearly independent. Since $W$ has dimension $3$, $S_{1}$ is a basis for $W$.\n\nFor $S_{2}=\\{x-1, x^{2}-1, x^{2}-x\\}$:\n- Membership in $W$: $(x-1)(1)=0$, $(x^{2}-1)(1)=0$, $(x^{2}-x)(1)=0$.\n- Linear independence: Suppose\n$$\na(x-1)+b(x^{2}-1)+c(x^{2}-x)=0.\n$$\nThen\n$$\n(b+c)x^{2}+(a-c)x+(-a-b)=0,\n$$\nso\n$$\nb+c=0,\\quad a-c=0,\\quad -a-b=0.\n$$\nThese imply $a=c$ and $b=-c$, yielding nontrivial solutions (e.g., take $c\\neq 0$). Hence the set is linearly dependent and not a basis.\n\nFor $S_{3}=\\{x-1, x^{2}-1\\}$:\n- It has only $2$ vectors, but $\\dim(W)=3$, so it cannot span $W$ and is not a basis.\n\nFor $S_{4}=\\{1, x, x^{2}\\}$:\n- Membership: $1\\notin W$ since $1(1)=1\\neq 0$. Thus it is not even a subset of $W$ and cannot be a basis of $W$.\n\nFor $S_{5}=\\{x-1, x^{2}-1, x^{3}\\}$:\n- Membership: $x^{3}\\notin W$ since $x^{3}(1)=1\\neq 0$. Hence not a basis of $W$.\n\nTherefore, the only valid basis among the options is $S_{1}$.", "answer": "$$\\boxed{A}$$", "id": "1868582"}, {"introduction": "Building on the idea of abstract vector spaces, we now turn our attention to matrices, which are central to the formulation of neural networks. Instead of constructing a basis directly as in our previous exercise, this problem encourages a more elegant approach to determine the size of a basis. You will find the dimension of the subspace of traceless matrices by applying the powerful rank-nullity theorem, a fundamental tool for analyzing linear transformations [@problem_id:1868614].", "problem": "Consider the set of all $3 \\times 3$ matrices with real-valued entries. Within this set, we are interested in a special collection of matrices, which we'll call 'zero-sum matrices'. A matrix is defined as a zero-sum matrix if the sum of the elements on its main diagonal is equal to zero. Any zero-sum matrix can be constructed by taking a weighted sum (a linear combination) of a smaller, fundamental set of zero-sum matrices. What is the minimum number of matrices required in such a fundamental set, such that any $3 \\times 3$ zero-sum matrix can be represented as a weighted sum of the matrices in this set?", "solution": "The problem asks for the minimum number of matrices in a \"fundamental set\" that can be used to generate any $3 \\times 3$ \"zero-sum matrix\" through weighted sums. In the language of linear algebra, this is equivalent to finding the dimension of the subspace of traceless $3 \\times 3$ matrices.\n\nLet $V = M_{3}(\\mathbb{R})$ be the vector space of all $3 \\times 3$ matrices with real entries. The dimension of this vector space is $\\dim(V) = 3 \\times 3 = 9$. A standard basis for $V$ consists of the nine matrices $E_{ij}$ (for $i,j \\in \\{1, 2, 3\\}$), where $E_{ij}$ is the matrix with a 1 in the $i$-th row and $j$-th column and zeros elsewhere.\n\nLet $W$ be the set of \"zero-sum matrices\". A matrix $A \\in V$ is in $W$ if its trace is zero. The trace of a matrix is the sum of its diagonal elements. So, for $A = (a_{ij})$, $A \\in W$ if and only if $\\text{tr}(A) = a_{11} + a_{22} + a_{33} = 0$.\n\nFirst, we establish that $W$ is a subspace of $V$.\n1.  The zero matrix, $O$, has a trace of $0+0+0=0$, so $O \\in W$.\n2.  Let $A, B \\in W$. Then $\\text{tr}(A)=0$ and $\\text{tr}(B)=0$. The trace is a linear operation, so $\\text{tr}(A+B) = \\text{tr}(A) + \\text{tr}(B) = 0 + 0 = 0$. Thus, $A+B \\in W$.\n3.  Let $A \\in W$ and $c \\in \\mathbb{R}$ be a scalar. Then $\\text{tr}(cA) = c \\cdot \\text{tr}(A) = c \\cdot 0 = 0$. Thus, $cA \\in W$.\nSince $W$ is closed under addition and scalar multiplication, it is a subspace of $V$. The question asks for the dimension of this subspace, $\\dim(W)$.\n\nWe can find this dimension by using the rank-nullity theorem. Let's define a linear map $T: M_{3}(\\mathbb{R}) \\to \\mathbb{R}$ by $T(A) = \\text{tr}(A)$.\nThis map is a linear transformation (also called a linear functional) because:\n-   $T(A+B) = \\text{tr}(A+B) = \\text{tr}(A) + \\text{tr}(B) = T(A) + T(B)$\n-   $T(cA) = \\text{tr}(cA) = c \\cdot \\text{tr}(A) = c \\cdot T(A)$\n\nThe set $W$ is precisely the set of matrices $A$ for which $T(A) = 0$. This means that $W$ is the kernel (or null space) of the linear transformation $T$. So, $W = \\ker(T)$.\n\nThe rank-nullity theorem states that for a linear map $T: V \\to U$, we have:\n$$ \\dim(V) = \\dim(\\ker(T)) + \\dim(\\text{Im}(T)) $$\nHere, $V = M_{3}(\\mathbb{R})$ and the codomain is $\\mathbb{R}$.\n\nWe already know $\\dim(V) = 9$. We need to find the dimension of the image (or range) of $T$, denoted $\\text{Im}(T)$. The image of $T$ is the set of all possible values that the trace can take. For any real number $r \\in \\mathbb{R}$, we can construct a matrix $A \\in M_{3}(\\mathbb{R})$ such that $\\text{tr}(A)=r$. For example, the matrix\n$$ A = \\begin{pmatrix} r & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\nhas $\\text{tr}(A) = r$. Since the trace can be any real number, the image of $T$ is the entire set of real numbers, $\\mathbb{R}$. The dimension of $\\mathbb{R}$ as a vector space over itself is 1. So, $\\dim(\\text{Im}(T)) = 1$.\n\nNow, we can apply the rank-nullity theorem:\n$$ \\dim(M_{3}(\\mathbb{R})) = \\dim(\\ker(T)) + \\dim(\\text{Im}(T)) $$\n$$ 9 = \\dim(W) + 1 $$\nSolving for $\\dim(W)$, we get:\n$$ \\dim(W) = 9 - 1 = 8 $$\n\nTherefore, the minimum number of matrices required in the fundamental set is 8.", "answer": "$$\\boxed{8}$$", "id": "1868614"}, {"introduction": "The true power of linear algebra is revealed when its concepts are applied to solve problems in other fields. This final exercise bridges the gap between abstract vector spaces and the practical world of multivariable calculus and optimization, which are the engines of deep learning. Here, you will investigate the conditions under which the gradient vectors of two functions are linearly dependent, providing insight into the geometry of function landscapes and optimization paths [@problem_id:1651295].", "problem": "Let $f: \\mathbb{R}^2 \\to \\mathbb{R}$ and $g: \\mathbb{R}^2 \\to \\mathbb{R}$ be two scalar functions defined by $f(x, y) = x^2 + y^2$ and $g(x, y) = \\exp(ax + by)$, where $a$ and $b$ are given non-zero real constants. Find the set of all points $(x,y) \\in \\mathbb{R}^2$ for which the gradient vectors $\\nabla f(x,y)$ and $\\nabla g(x,y)$ are linearly dependent. This set can be described by a single equation of the form $H(x,y,a,b) = 0$. Provide the expression for $H(x,y,a,b)$ as your answer. Note that any non-zero constant multiple of $H(x,y,a,b)$ is also a valid representation; provide the simplest polynomial form.", "solution": "We recall that the gradient of a scalar function $h:\\mathbb{R}^{2}\\to\\mathbb{R}$ is defined as $\\nabla h=(\\partial h/\\partial x,\\partial h/\\partial y)$. For $f(x,y)=x^{2}+y^{2}$, we compute\n$$\n\\nabla f(x,y)=\\left(\\frac{\\partial}{\\partial x}(x^{2}+y^{2}),\\frac{\\partial}{\\partial y}(x^{2}+y^{2})\\right)=(2x,2y).\n$$\nFor $g(x,y)=\\exp(ax+by)$, using the chain rule,\n$$\n\\nabla g(x,y)=\\left(\\frac{\\partial}{\\partial x}\\exp(ax+by),\\frac{\\partial}{\\partial y}\\exp(ax+by)\\right)=\\left(a\\exp(ax+by),\\,b\\exp(ax+by)\\right).\n$$\nTwo vectors in $\\mathbb{R}^{2}$ are linearly dependent if and only if the determinant of the $2\\times 2$ matrix having them as rows (or columns) is zero. Thus the condition for linear dependence is\n$$\n\\det\\begin{pmatrix}\n2x & 2y\\\\\na\\exp(ax+by) & b\\exp(ax+by)\n\\end{pmatrix}=0.\n$$\nEvaluating the determinant gives\n$$\n2x\\cdot b\\exp(ax+by)-2y\\cdot a\\exp(ax+by)=0.\n$$\nFactor out the common nonzero factor $\\exp(ax+by)$:\n$$\n2\\exp(ax+by)\\,(bx-ay)=0.\n$$\nSince $\\exp(ax+by)\\neq 0$ for all $(x,y)\\in\\mathbb{R}^{2}$, the condition reduces to\n$$\nbx-ay=0.\n$$\nTherefore, the set of all points where $\\nabla f$ and $\\nabla g$ are linearly dependent is described by the equation $H(x,y,a,b)=0$ with\n$$\nH(x,y,a,b)=bx-ay.\n$$\nAny nonzero constant multiple would also be valid; the simplest polynomial form is $bx-ay$.", "answer": "$$\\boxed{bx-ay}$$", "id": "1651295"}]}