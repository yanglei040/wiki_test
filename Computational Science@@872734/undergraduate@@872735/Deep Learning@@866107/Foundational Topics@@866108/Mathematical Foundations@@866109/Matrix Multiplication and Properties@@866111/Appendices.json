{"hands_on_practices": [{"introduction": "In modern deep learning, training is often parallelized across many machines to save time. This exercise [@problem_id:3148022] grounds matrix multiplication in this real-world scenario, showing how the aggregation of gradients from multiple workers is elegantly described as a matrix-vector product. By analyzing this operation's communication cost, you will connect the algebraic properties of matrices to the practical performance of large-scale distributed algorithms.", "problem": "In synchronous data-parallel training of a deep neural network with $p$ workers, each worker $i \\in \\{1,\\dots,p\\}$ computes a local gradient vector $g_i \\in \\mathbb{R}^{d}$ for the current minibatch. Stack the local gradients column-wise to form the matrix $G \\in \\mathbb{R}^{d \\times p}$ given by $G = \\begin{bmatrix} g_1  \\cdots  g_p \\end{bmatrix}$. The global gradient to be applied to the model parameters is the column-wise sum $g \\in \\mathbb{R}^{d}$, which equals the matrix-vector product $g = G \\mathbf{1}_p$, where $\\mathbf{1}_p \\in \\mathbb{R}^{p}$ denotes the all-ones vector.\n\nYou will analyze the minimal communication time needed to compute $g$ at a designated root worker using a balanced reduction protocol that respects basic matrix multiplication structure and vector addition associativity. Assume the following:\n\n- The only cost that matters is communication; local arithmetic (including vector additions needed to implement the reduction) takes negligible time compared to communication.\n- Communication follows the standard $\\alpha$–$\\beta$ model: sending a message containing $k$ real numbers takes time $T(k) = \\alpha + k \\beta$, where $\\alpha > 0$ is the latency term and $\\beta > 0$ is the per-number transfer time.\n- The reduction is performed over a perfectly balanced binary tree with $p$ leaves and a single designated root. Assume $p$ is a power of $2$, links are full-duplex and contention-free, and all message exchanges at the same tree level occur concurrently. Each message at every level consists of exactly the entire vector to be summed, consistent with the linearity of the operation $G \\mathbf{1}_p$ and associativity of addition.\n\nUsing only the definition of matrix multiplication and the $\\alpha$–$\\beta$ model stated above, derive a closed-form expression for the total communication time required to compute $g = G \\mathbf{1}_p$ at the root under this balanced binary-tree reduction.\n\nProvide your final answer as a single closed-form symbolic expression in terms of $p$, $d$, $\\alpha$, and $\\beta$. Do not include units, and do not approximate or round.", "solution": "We begin from the definition of matrix multiplication and the structure of $G \\mathbf{1}_p$. Let $G = \\begin{bmatrix} g_1  \\cdots  g_p \\end{bmatrix} \\in \\mathbb{R}^{d \\times p}$ and $\\mathbf{1}_p \\in \\mathbb{R}^{p}$ the all-ones vector. By the definition of matrix-vector multiplication, the product $g = G \\mathbf{1}_p \\in \\mathbb{R}^{d}$ yields\n$$\ng = \\sum_{i=1}^{p} g_i,\n$$\nthat is, each entry of $g$ is the sum of the corresponding entries across the $p$ columns of $G$. This relies on linearity of matrix multiplication in each column and the fact that multiplying by $\\mathbf{1}_p$ weights each column $g_i$ by $1$.\n\nTo realize this computation in a distributed setting, we use a balanced binary-tree reduction. The core algebraic property enabling a tree reduction is the associativity and commutativity of vector addition: for vectors $u,v,w \\in \\mathbb{R}^{d}$, $(u+v)+w = u+(v+w)$ and $u+v = v+u$. Therefore, we can structure the sum $\\sum_{i=1}^{p} g_i$ as a sequence of pairwise additions arranged in levels of a binary tree without changing the result, matching the conceptual computation of $G \\mathbf{1}_p$.\n\nNow consider the communication model. Each pairwise combination at a tree edge requires communicating a full vector of length $d$ between two workers so that one of them can add the received vector to its own partial sum. Under the $\\alpha$–$\\beta$ model, sending $k$ real numbers costs $T(k) = \\alpha + k \\beta$. Here, $k = d$ for every message because each message carries an entire $d$-dimensional vector.\n\nIn a balanced binary tree with $p$ leaves (with $p$ a power of $2$), there are exactly $\\log_{2}(p)$ levels. At each level, workers are paired, and within each pair, one vector of length $d$ is sent across the link and added at the receiver. Because links are assumed full-duplex and contention-free, and all pairs at the same level communicate concurrently, the time for an entire level is the time of a single message: $\\alpha + d \\beta$. Thus, the total time over all levels is the sum of the per-level times across the $\\log_{2}(p)$ levels:\n$$\nT_{\\text{total}} \\;=\\; \\underbrace{\\log_{2}(p)}_{\\text{number of levels}} \\times \\underbrace{(\\alpha + d \\beta)}_{\\text{time per level}} \\;=\\; (\\alpha + d \\beta)\\,\\log_{2}(p).\n$$\n\nTherefore, the minimal communication time to compute $g = G \\mathbf{1}_p$ at the root via a balanced binary-tree reduction, under the stated assumptions and the $\\alpha$–$\\beta$ model, is the closed-form expression\n$$\n(\\alpha + d \\beta)\\,\\log_{2}(p).\n$$", "answer": "$$\\boxed{(\\alpha + d \\beta)\\,\\log_{2}(p)}$$", "id": "3148022"}, {"introduction": "A deep neural network's power comes from stacking multiple layers, but what happens when those layers are purely linear? This practice [@problem_id:3148079] delves into the algebraic structure of networks by exploring when two linear layers commute, a property revealed through the concept of a shared eigenbasis. You will discover how sequences of linear transformations can be functionally redundant and see firsthand why nonlinear activation functions are indispensable for building expressive deep models.", "problem": "Consider two linear layers in a feed-forward neural network, represented by real $3 \\times 3$ matrices $W_1$ and $W_2$. You will analyze when these layers commute by exhibiting a shared eigenbasis (simultaneous diagonalization), and interpret what this implies about functional redundancy in a network with and without a nonlinearity between the layers. Use only the fundamental definitions of matrix multiplication, eigenvalues and eigenvectors, and the elementwise action of nonlinearities.\n\nLet $W_1$ and $W_2$ be given by\n$$\nW_1 = \\begin{pmatrix}\na  0  0 \\\\\n0  b  0 \\\\\n0  0  c\n\\end{pmatrix},\n\\qquad\nW_2 = \\begin{pmatrix}\nd  0  0 \\\\\n0  e  0 \\\\\n0  0  f\n\\end{pmatrix},\n$$\nwhere $a,b,c,d,e,f \\in \\mathbb{R}$. These are already diagonal in the standard basis, which is a candidate shared eigenbasis.\n\nFirst, construct sufficient conditions under which two real matrices $W_1$ and $W_2$ commute by identifying a shared eigenbasis and explaining how simultaneous diagonalization enforces commutativity. Your construction must be based on the definitions of eigenvalues and eigenvectors, and on the properties of diagonal matrices, without assuming any specific shortcut theorems not derived from these.\n\nNext, consider the composition of the two linear layers without any nonlinearity between them, which acts as $x \\mapsto W_2 W_1 x$ for $x \\in \\mathbb{R}^3$. Explain the notion of functional redundancy in this linear case and, for the specific $W_1$ and $W_2$ given above, compute the scalar quantity $\\operatorname{tr}(W_2 W_1)$ as a closed-form analytic expression in $a,b,c,d,e,f$.\n\nFinally, insert a Rectified Linear Unit (ReLU) nonlinearity, defined as the elementwise map $\\sigma(z) = \\max\\{0,z\\}$, between the layers to form $x \\mapsto W_2 \\,\\sigma(W_1 x)$. In the shared-eigenbasis (which is the standard basis here), give the condition on $x$ under which this composition reduces to a single linear map on that input, and interpret what this means for functional redundancy of the two layers when a nonlinearity is present. You do not need to compute any numeric value for this part.\n\nYour final answer must be the single closed-form expression for $\\operatorname{tr}(W_2 W_1)$ in terms of $a,b,c,d,e,f$.", "solution": "We begin from the foundational definitions. Two linear operators represented by matrices $W_1$ and $W_2$ commute if and only if $W_1 W_2 = W_2 W_1$. A matrix $W$ is diagonalizable if there exists an invertible matrix $S$ such that $S^{-1} W S = D$, where $D$ is a diagonal matrix. A diagonal matrix $D$ has the property that $D u$ scales each coordinate of a vector $u$ independently, and diagonal matrices commute because multiplication of diagonal matrices is entrywise and therefore associative and commutative at the level of corresponding diagonal entries.\n\nConstructing sufficient conditions for commutation via simultaneous diagonalization proceeds as follows. Suppose $W_1$ and $W_2$ are diagonalizable and share a complete set of eigenvectors; that is, there exists an invertible matrix $S$ whose columns form a basis of eigenvectors common to both $W_1$ and $W_2$. Then we can write\n$$\nS^{-1} W_1 S = D_1, \\qquad S^{-1} W_2 S = D_2,\n$$\nwhere $D_1$ and $D_2$ are diagonal matrices comprising the eigenvalues of $W_1$ and $W_2$ with respect to the shared eigenbasis. Consider the products:\n$$\nW_1 W_2 = S D_1 S^{-1} S D_2 S^{-1} = S (D_1 D_2) S^{-1},\n$$\nand\n$$\nW_2 W_1 = S D_2 S^{-1} S D_1 S^{-1} = S (D_2 D_1) S^{-1}.\n$$\nBecause diagonal matrices commute, we have $D_1 D_2 = D_2 D_1$, which immediately implies $W_1 W_2 = W_2 W_1$. Hence, a sufficient condition for $W_1$ and $W_2$ to commute is that they be simultaneously diagonalizable by the same invertible matrix $S$ (equivalently, they share a complete eigenbasis). In particular, if $W_1$ and $W_2$ are both diagonal in the same basis, they commute.\n\nFor the specific matrices given,\n$$\nW_1 = \\begin{pmatrix}\na  0  0 \\\\\n0  b  0 \\\\\n0  0  c\n\\end{pmatrix},\n\\qquad\nW_2 = \\begin{pmatrix}\nd  0  0 \\\\\n0  e  0 \\\\\n0  0  f\n\\end{pmatrix},\n$$\nthe standard basis vectors are common eigenvectors, and both matrices are diagonal in that basis. Therefore, $W_1$ and $W_2$ commute. The composition without nonlinearity is the linear operator $W_2 W_1$, which is also diagonal with diagonal entries given by the products of corresponding entries:\n$$\nW_2 W_1 = \\begin{pmatrix}\nad  0  0 \\\\\n0  be  0 \\\\\n0  0  cf\n\\end{pmatrix}.\n$$\nFunctional redundancy in this linear case means that the two-layer linear subnetwork $x \\mapsto W_2 W_1 x$ can be replaced by a single linear layer $x \\mapsto W_{\\text{eff}} x$ with $W_{\\text{eff}} = W_2 W_1$, without changing the input-output mapping. To compute the requested scalar,\n$$\n\\operatorname{tr}(W_2 W_1) = ad + be + cf,\n$$\nby the definition of the trace as the sum of the diagonal entries.\n\nNow consider inserting a Rectified Linear Unit (ReLU) nonlinearity between the layers, forming the map $x \\mapsto W_2 \\,\\sigma(W_1 x)$ with $\\sigma$ applied elementwise: $\\sigma(z) = \\max\\{0,z\\}$. Because $W_1$ is diagonal,\n$$\nW_1 x = \\begin{pmatrix}\na x_1 \\\\ b x_2 \\\\ c x_3\n\\end{pmatrix}, \\qquad \\sigma(W_1 x) = \\begin{pmatrix}\n\\max\\{0, a x_1\\} \\\\ \\max\\{0, b x_2\\} \\\\ \\max\\{0, c x_3\\}\n\\end{pmatrix}.\n$$\nThe subsequent multiplication by $W_2$ scales each coordinate of $\\sigma(W_1 x)$ by $d$, $e$, and $f$ respectively. The composition $x \\mapsto W_2 \\,\\sigma(W_1 x)$ reduces to the single linear map $x \\mapsto W_2 W_1 x$ on a particular input $x$ exactly when $\\sigma$ acts identically on $W_1 x$, which occurs if and only if each coordinate of $W_1 x$ is nonnegative:\n$$\na x_1 \\ge 0, \\quad b x_2 \\ge 0, \\quad c x_3 \\ge 0.\n$$\nUnder these conditions, $\\sigma(W_1 x) = W_1 x$, so $W_2 \\,\\sigma(W_1 x) = W_2 W_1 x$, and the presence of the nonlinearity does not change the function on that input; the two-layer structure is functionally redundant for such $x$. If any of these coordinate-wise inequalities fail, then at least one component is truncated to zero by ReLU, and the overall mapping cannot be represented by a single fixed linear operator for all inputs—functional redundancy is broken by the nonlinearity. In particular, global redundancy (for all $x$) would require that the domain of inputs be restricted to the set where $a x_1 \\ge 0$, $b x_2 \\ge 0$, and $c x_3 \\ge 0$ hold, or that the signs of $a$, $b$, and $c$ and the input coordinates be aligned so that ReLU acts as the identity everywhere in the domain of interest.\n\nTherefore, the requested scalar expression for the linear case is the trace computed above.", "answer": "$$\\boxed{ad + be + cf}$$", "id": "3148079"}, {"introduction": "The behavior of a neural network is profoundly influenced by the \"size\" of its weight matrices, a concept captured mathematically by the spectral norm. This norm is central to both theoretical understanding and practical regularization techniques. In this hands-on coding challenge [@problem_id:3148029], you will implement the power iteration method, a classic and efficient algorithm derived from fundamental matrix properties, to estimate the spectral norm and verify its key characteristics.", "problem": "You are given a real matrix $W \\in \\mathbb{R}^{m \\times n}$ representing a weight matrix from a linear layer in deep learning. The goal is to implement an algorithm that estimates the spectral norm (matrix operator $2$-norm) $\\|W\\|_2$ using only matrix multiplication and properties, grounded in fundamental definitions. The spectral norm is defined from first principles as\n$$\n\\|W\\|_2 = \\sup_{\\|x\\|_2 = 1} \\|W x\\|_2,\n$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm for vectors. The iterative procedure to estimate $\\|W\\|_2$ relies on the following steps: initialize a unit vector $v \\in \\mathbb{R}^{n}$, then repeat the following updates for a specified number of iterations $T$:\n$$\nu \\leftarrow \\frac{W v}{\\|W v\\|_2} \\quad \\text{if } \\|W v\\|_2 \\neq 0,\n$$\n$$\nv \\leftarrow \\frac{W^\\top u}{\\|W^\\top u\\|_2} \\quad \\text{if } \\|W^\\top u\\|_2 \\neq 0,\n$$\nand at the end report the estimate\n$$\n\\hat{\\sigma} = \\|W v\\|_2,\n$$\nwhich, under suitable conditions, approximates $\\|W\\|_2$. If either normalization denominator becomes zero during the process, replace the corresponding vector with a random unit vector of the appropriate dimension and continue.\n\nStarting only from the above definition and iterative procedure, write a complete program that:\n- Implements the described power iteration estimator for $\\|W\\|_2$, using only matrix multiplications and vector normalizations.\n- Uses a reproducible random number generator seeded with $12345$ whenever random unit vectors are needed.\n- Produces results for the following test suite, designed to cover general behavior and edge cases. In all cases, iteration counts $T$ are integers and dimensions are specified by positive integers.\n\nTest suite:\n$1.$ General rectangular case (happy path): Let $W_1 \\in \\mathbb{R}^{5 \\times 3}$ be filled with independent standard normal entries, generated with a fixed seed $12345$. Use $T = 50$. Output the estimated spectral norm $\\hat{\\sigma}_1$ as a floating-point number.\n\n$2.$ Identity boundary case: Let $W_2 = I_4 \\in \\mathbb{R}^{4 \\times 4}$. Use $T = 10$. Output the estimated spectral norm $\\hat{\\sigma}_2$ as a floating-point number.\n\n$3.$ Zero boundary case: Let $W_3 \\in \\mathbb{R}^{3 \\times 2}$ be the zero matrix. Use $T = 10$. Output the estimated spectral norm $\\hat{\\sigma}_3$ as a floating-point number.\n\n$4.$ Diagonal known-values case: Let $W_4 = \\mathrm{diag}(3,1,2) \\in \\mathbb{R}^{3 \\times 3}$. Use $T = 10$. Output a boolean indicating whether the estimated spectral norm is within an absolute tolerance of $10^{-6}$ of $3$, i.e., output\n$$\n\\left|\\hat{\\sigma}_4 - 3\\right| \\leq 10^{-6}.\n$$\n\n$5.$ Constructed rectangular matrix with known singular values: Construct $W_5 \\in \\mathbb{R}^{2 \\times 5}$ as $W_5 = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{2 \\times 2}$ and $V \\in \\mathbb{R}^{5 \\times 5}$ are orthonormal matrices obtained from the $\\mathrm{QR}$ decomposition of random Gaussian matrices (seed $12345$), and\n$$\n\\Sigma = \\begin{bmatrix}\n4  0  0  0  0 \\\\\n0  1  0  0  0\n\\end{bmatrix}.\n$$\nUse $T = 60$. Output the absolute difference $|\\hat{\\sigma}_5 - 4|$ as a floating-point number.\n\n$6.$ Scalar multiplication property check: Let $W_6 \\in \\mathbb{R}^{4 \\times 4}$ be random Gaussian (seed $12345$) and let $\\alpha = 0.5$. Use $T = 60$. Output a boolean indicating whether\n$$\n\\left|\\hat{\\sigma}(\\alpha W_6) - |\\alpha| \\, \\hat{\\sigma}(W_6)\\right| \\leq 10^{-5},\n$$\nwhere $\\hat{\\sigma}(\\cdot)$ denotes the algorithm’s estimate.\n\n$7.$ Submultiplicativity check with a vector: Using the same $W_1$ from the first test, draw a random vector $x \\in \\mathbb{R}^{3}$ (seed $12345$), and compute the exact $\\|W_1\\|_2$ using the singular value decomposition (SVD) for verification only in this property check. Output a boolean indicating whether\n$$\n\\|W_1 x\\|_2 \\leq \\|W_1\\|_2 \\cdot \\|x\\|_2.\n$$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must appear in the order of the tests $1$ through $7$, and each element must be of type float or boolean as specified. For example, your output should look like\n$[\\hat{\\sigma}_1,\\hat{\\sigma}_2,\\hat{\\sigma}_3,\\text{bool}_4,\\text{float}_5,\\text{bool}_6,\\text{bool}_7]$.", "solution": "The problem requires the implementation of an algorithm to estimate the spectral norm of a real matrix $W \\in \\mathbb{R}^{m \\times n}$. The spectral norm, or matrix $2$-norm, is defined as the maximum stretching factor the matrix applies to any unit vector:\n$$\n\\|W\\|_2 = \\sup_{\\|x\\|_2 = 1} \\|W x\\|_2\n$$\nThis norm is equivalent to the largest singular value of the matrix, denoted $\\sigma_{\\max}(W)$. The prescribed algorithm is a variant of the power iteration method, which is designed to find the eigenvector corresponding to the eigenvalue with the largest magnitude.\n\nThe algorithm's connection to the spectral norm can be understood by examining the matrix $A = W^\\top W \\in \\mathbb{R}^{n \\times n}$. This matrix is symmetric and positive semi-definite. Its eigenvalues $\\lambda_i(A)$ are real and non-negative, and they are related to the singular values of $W$ by $\\lambda_i(W^\\top W) = \\sigma_i(W)^2$. Consequently, the largest eigenvalue of $W^\\top W$ is the square of the largest singular value of $W$: $\\lambda_{\\max}(W^\\top W) = \\sigma_{\\max}(W)^2 = \\|W\\|_2^2$.\n\nThe power iteration method, when applied to $A = W^\\top W$, finds an eigenvector $v$ corresponding to $\\lambda_{\\max}(A)$. The iterative step for the power method is $v_k \\propto A v_{k-1} = (W^\\top W) v_{k-1}$. Let us analyze the procedure given in the problem statement:\n$1.$ $u_k \\leftarrow \\frac{W v_{k-1}}{\\|W v_{k-1}\\|_2}$\n$2.$ $v_k \\leftarrow \\frac{W^\\top u_k}{\\|W^\\top u_k\\|_2}$\n\nSubstituting the expression for $u_k$ into the update for $v_k$:\n$$\nv_k \\propto W^\\top u_k \\propto W^\\top \\left( \\frac{W v_{k-1}}{\\|W v_{k-1}\\|_2} \\right) \\propto W^\\top W v_{k-1}\n$$\nThis demonstrates that the sequence of vectors $\\{v_k\\}$ is generated by the power method applied to the matrix $W^\\top W$. Under suitable conditions (specifically, that the initial vector $v_0$ has a non-zero component in the direction of the eigenvector corresponding to $\\lambda_{\\max}(W^\\top W)$, and $\\lambda_{\\max}$ is strictly greater than other eigenvalues), the vector $v_k$ converges to the eigenvector of $W^\\top W$ associated with $\\lambda_{\\max}$. This eigenvector is the right-singular vector of $W$ corresponding to $\\sigma_{\\max}(W)$.\n\nThe final estimate for the spectral norm is $\\hat{\\sigma} = \\|W v_T\\|_2$, where $v_T$ is the vector after $T$ iterations. As $v_T$ approaches the dominant right-singular vector $v_{\\max}$, we have:\n$$\n\\|W v_T\\|_2^2 = v_T^\\top W^\\top W v_T \\xrightarrow{k \\to \\infty} v_{\\max}^\\top (W^\\top W) v_{\\max} = v_{\\max}^\\top (\\lambda_{\\max} v_{\\max}) = \\lambda_{\\max} \\|v_{\\max}\\|_2^2 = \\lambda_{\\max}(W^\\top W)\n$$\nSince $v_{\\max}$ is a unit vector, this gives $\\hat{\\sigma}^2 \\to \\lambda_{\\max}(W^\\top W) = \\sigma_{\\max}(W)^2$. Therefore, the estimate $\\hat{\\sigma} = \\|W v_T\\|_2$ converges to $\\sigma_{\\max}(W) = \\|W\\|_2$.\n\nThe algorithm must handle cases where a normalization factor is zero. If $\\|W v\\|_2 = 0$, it implies $Wv=0$, so $v$ is in the null space of $W$. If $\\|W^\\top u\\|_2 = 0$, then $u$ is in the null space of $W^\\top$. The problem specifies that in such an event, the vector to be normalized (i.e., $u$ or $v$) should be replaced by a random unit vector of the appropriate dimension. This prevents the iteration from getting stuck at the zero vector. A single reproducible random number generator, seeded with $12345$, must be used for all stochastic aspects of the implementation.\n\nThe solution will now proceed by implementing this algorithm and applying it to the seven specified test cases.\n\n$1.$ **General rectangular case**: A random matrix $W_1 \\in \\mathbb{R}^{5 \\times 3}$ is generated from a standard normal distribution. This is the \"happy path\" case where singular values are distinct and non-zero, for which the power method is expected to converge well. We run the iteration $T=50$ times and report the estimate $\\hat{\\sigma}_1$.\n\n$2.$ **Identity boundary case**: For $W_2 = I_4 \\in \\mathbb{R}^{4 \\times 4}$, the spectral norm is known to be $\\|I_4\\|_2 = 1$. For any unit vector $v$, $\\|I_4 v\\|_2 = \\|v\\|_2 = 1$. The algorithm will compute $u = \\frac{Iv}{\\|Iv\\|_2} = \\frac{v}{1} = v$, and then $v_{new} = \\frac{I^\\top u}{\\|I^\\top u\\|_2} = \\frac{I v}{\\|I v\\|_2} = v$. The vectors do not change, and the final estimate $\\|I_4 v\\|_2$ will be $1$ regardless of the initial vector. The result $\\hat{\\sigma}_2$ should be $1.0$.\n\n$3.$ **Zero boundary case**: For $W_3=0 \\in \\mathbb{R}^{3 \\times 2}$, the spectral norm is $\\|0\\|_2 = 0$. For any vector $v$, $W_3 v = 0$, so its norm is $0$. According to the specified procedure, $u$ is reset to a random unit vector. Then, $W_3^\\top u = 0$, so its norm is also $0$. The vector $v$ is then reset to a random unit vector. This process repeats for $T=10$ iterations. The final estimate is $\\hat{\\sigma}_3 = \\|W_3 v_T\\|_2 = \\|0 \\cdot v_T\\|_2 = 0$. The result should be $0.0$.\n\n$4.$ **Diagonal known-values case**: For a diagonal matrix $W_4 = \\mathrm{diag}(3,1,2)$, the spectral norm is the maximum absolute value of its diagonal entries, which is $\\max(|3|, |1|, |2|) = 3$. The algorithm is expected to converge to this value. The test checks if the estimate $\\hat{\\sigma}_4$ is within a tolerance of $10^{-6}$ of the true value $3$.\n\n$5.$ **Constructed rectangular matrix**: A matrix $W_5 \\in \\mathbb{R}^{2 \\times 5}$ is constructed from its Singular Value Decomposition (SVD), $W_5 = U \\Sigma V^\\top$. The matrices $U$ and $V$ are generated as orthonormal bases from random matrices, ensuring generality. The matrix $\\Sigma$ explicitly sets the singular values to $4$ and $1$. The spectral norm is the largest singular value, so $\\|W_5\\|_2 = 4$. The test computes the absolute difference between the estimate $\\hat{\\sigma}_5$ and the true value $4$.\n\n$6.$ **Scalar multiplication property check**: This test verifies the absolute homogeneity property of norms, $\\|\\alpha W\\|_2 = |\\alpha| \\|W\\|_2$. The algorithm is run on a random matrix $W_6$ and on $\\alpha W_6$ with $\\alpha=0.5$. The test verifies if the estimates obey this property, i.e., $|\\hat{\\sigma}(\\alpha W_6) - |\\alpha| \\hat{\\sigma}(W_6)| \\leq 10^{-5}$. Our theoretical analysis above showed that this should hold up to floating-point precision, as the normalization steps cancel out the scalar factor $|\\alpha|$.\n\n$7.$ **Submultiplicativity check**: This test verifies the fundamental definition of an induced operator norm: $\\|W x\\|_2 \\leq \\|W\\|_2 \\|x\\|_2$ for any vector $x$. The check is performed using the matrix $W_1$ from the first test and a newly generated random vector $x$. Crucially, the problem requires using the *exact* spectral norm $\\|W_1\\|_2$, computed via a standard SVD library function for this verification, not the algorithm's estimate. This inequality is a mathematical truth, so the test should result in `True`, serving as a sanity check on the understanding of the concepts and their implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Master random number generator for reproducibility across all tests.\n    rng = np.random.default_rng(12345)\n    \n    # This is the core algorithm as described in the problem.\n    def estimate_spectral_norm(W, T, prng):\n        \"\"\"\n        Estimates the spectral norm of a matrix W using power iteration.\n\n        Args:\n            W (np.ndarray): The matrix for which to estimate the spectral norm.\n            T (int): The number of power iterations.\n            prng (np.random.Generator): A seeded random number generator.\n\n        Returns:\n            float: The estimated spectral norm of W.\n        \"\"\"\n        m, n = W.shape\n        \n        # Initialize v with a random unit vector.\n        v = prng.standard_normal(size=n)\n        v = v / np.linalg.norm(v)\n\n        for _ in range(T):\n            # Update u\n            Wv = W @ v\n            norm_Wv = np.linalg.norm(Wv)\n            if norm_Wv == 0:\n                u = prng.standard_normal(size=m)\n                u = u / np.linalg.norm(u)\n            else:\n                u = Wv / norm_Wv\n            \n            # Update v\n            Wtu = W.T @ u\n            norm_Wtu = np.linalg.norm(Wtu)\n            if norm_Wtu == 0:\n                v = prng.standard_normal(size=n)\n                v = v / np.linalg.norm(v)\n            else:\n                v = Wtu / norm_Wtu\n        \n        # Final estimate\n        sigma_hat = np.linalg.norm(W @ v)\n        return sigma_hat\n\n    results = []\n\n    # Test case 1: General rectangular case\n    W1 = rng.standard_normal(size=(5, 3))\n    sigma1_hat = estimate_spectral_norm(W1, 50, rng)\n    results.append(sigma1_hat)\n\n    # Test case 2: Identity boundary case\n    W2 = np.identity(4)\n    sigma2_hat = estimate_spectral_norm(W2, 10, rng)\n    results.append(sigma2_hat)\n\n    # Test case 3: Zero boundary case\n    W3 = np.zeros((3, 2))\n    sigma3_hat = estimate_spectral_norm(W3, 10, rng)\n    results.append(sigma3_hat)\n\n    # Test case 4: Diagonal known-values case\n    W4 = np.diag([3, 1, 2])\n    sigma4_hat = estimate_spectral_norm(W4, 10, rng)\n    bool4 = np.abs(sigma4_hat - 3) = 1e-6\n    results.append(bool4)\n\n    # Test case 5: Constructed rectangular matrix with known singular values\n    A_U = rng.standard_normal(size=(2, 2))\n    U, _ = np.linalg.qr(A_U)\n    A_V = rng.standard_normal(size=(5, 5))\n    V, _ = np.linalg.qr(A_V)\n    Sigma = np.zeros((2, 5))\n    Sigma[0, 0] = 4\n    Sigma[1, 1] = 1\n    W5 = U @ Sigma @ V.T\n    sigma5_hat = estimate_spectral_norm(W5, 60, rng)\n    float5 = np.abs(sigma5_hat - 4)\n    results.append(float5)\n\n    # Test case 6: Scalar multiplication property check\n    W6 = rng.standard_normal(size=(4, 4))\n    alpha = 0.5\n    sigma_W6_hat = estimate_spectral_norm(W6, 60, rng)\n    sigma_alphaW6_hat = estimate_spectral_norm(alpha * W6, 60, rng)\n    bool6 = np.abs(sigma_alphaW6_hat - np.abs(alpha) * sigma_W6_hat) = 1e-5\n    results.append(bool6)\n    \n    # Test case 7: Submultiplicativity check with a vector\n    # Using W1 from test case 1.\n    x = rng.standard_normal(size=3)\n    norm_W1x = np.linalg.norm(W1 @ x)\n    norm_x = np.linalg.norm(x)\n    # Compute the exact spectral norm using SVD\n    singular_values_W1 = np.linalg.svd(W1, compute_uv=False)\n    true_norm_W1 = singular_values_W1[0]\n    bool7 = norm_W1x = true_norm_W1 * norm_x\n    results.append(bool7)\n\n    # Final print statement in the exact required format.\n    # The map(str,...) converts each element (float, bool) to its string representation\n    # e.g., True - \"True\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3148029"}]}