{"hands_on_practices": [{"introduction": "Understanding the bias-variance tradeoff is fundamental to machine learning, and polynomial regression provides the classic sandbox for its exploration. This exercise moves beyond simple visual inspection of learning curves by challenging you to implement a complete diagnostic pipeline from scratch. By generating synthetic data and fitting models of varying complexity, you will learn to quantify underfitting and overfitting using precise, code-based definitions, including a novel metric based on the frequency content of residuals to detect the characteristic oscillations of an overfit model [@problem_id:3135788].", "problem": "You are given a supervised learning setting for one-dimensional regression modeled in terms of Empirical Risk Minimization (ERM). Let the input be $x \\in [-1,1]$ and the ground-truth target function be a polynomial of known degree $d^\\star$, contaminated by zero-mean Gaussian noise with variance $\\sigma^2$. Specifically, let the data be generated as $y = f^\\star(x) + \\varepsilon$, where $f^\\star(x) = \\sum_{k=0}^{d^\\star} a_k x^k$, $d^\\star = 4$, the coefficients are $a_0 = 0.3$, $a_1 = -0.8$, $a_2 = 0.5$, $a_3 = 0.0$, $a_4 = 0.7$, and the noise is $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ with $\\sigma = 0.1$ (so $\\sigma^2 = 0.01$). The dataset size is $N = 200$ points with inputs $x$ drawn uniformly from $[-1,1]$. Use $N_{\\text{train}} = 120$ samples for training and $N_{\\text{valid}} = 80$ samples for validation. Use a fixed random seed, $42$, to ensure reproducibility.\n\nYour task is to implement polynomial regression with ridge regularization (also known as $\\ell_2$ regularization). For a chosen model degree $d$ and a regularization strength $\\lambda \\ge 0$, construct the design matrix $\\Phi \\in \\mathbb{R}^{m \\times (d+1)}$ with entries $\\Phi_{i,k} = x_i^k$. Given training data $(\\Phi_{\\text{train}}, y_{\\text{train}})$, compute the ridge estimator coefficients $w \\in \\mathbb{R}^{d+1}$ using the closed-form solution\n$$\nw = \\left(\\Phi_{\\text{train}}^\\top \\Phi_{\\text{train}} + \\lambda I\\right)^{-1} \\Phi_{\\text{train}}^\\top y_{\\text{train}},\n$$\nwhere $I$ is the $(d+1) \\times (d+1)$ identity matrix. Use this estimator to obtain predictions on training and validation sets, and compute the residuals $r_{\\text{train}} = y_{\\text{train}} - \\hat{y}_{\\text{train}}$ and $r_{\\text{valid}} = y_{\\text{valid}} - \\hat{y}_{\\text{valid}}$.\n\nFrom first principles, identify underfitting and overfitting using the following definitions and measurements:\n\n- Mean Squared Error (MSE) is defined as $ \\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2 $. Let $\\text{MSE}_{\\text{train}}$ and $\\text{MSE}_{\\text{valid}}$ denote the training and validation mean squared errors, respectively.\n- Oscillation in residuals is quantified in the frequency domain. Compute the Discrete Fourier Transform (DFT) of the validation residual sequence sorted by its corresponding inputs $x$ in ascending order. Use the real-valued DFT $R = \\text{rfft}(r_{\\text{valid-sorted}})$ and define the high-frequency energy ratio as\n$$\n\\rho_{\\text{HF}} = \\frac{\\sum_{k \\in \\mathcal{H}} |R_k|^2}{\\sum_{k \\in \\mathcal{P}} |R_k|^2},\n$$\nwhere $\\mathcal{P}$ indexes all positive-frequency bins excluding the zero-frequency bin and $\\mathcal{H}$ indexes the top quartile of positive-frequency bins (the highest-frequency $25\\%$ of $\\mathcal{P}$). If the denominator is zero, define $\\rho_{\\text{HF}} = 0$.\n\nUse the following classification rules with fixed thresholds to decide whether a model is underfitting, well-fit, or overfitting. Denote the known noise variance by $\\sigma^2 = 0.01$, and let the thresholds be $t_u = 1.3$, $t_o = 0.9$, $t_o' = 1.2$, $h_u = 0.35$, and $h_o = 0.45$.\n\n- Underfitting (code $0$): declare underfitting if either $d  d^\\star$ or if $\\text{MSE}_{\\text{train}} \\ge t_u \\sigma^2$ and $\\text{MSE}_{\\text{valid}} \\ge t_u \\sigma^2$ and $\\rho_{\\text{HF}} \\le h_u$.\n- Overfitting (code $2$): declare overfitting if $d  d^\\star$ and $\\text{MSE}_{\\text{train}} \\le t_o \\sigma^2$ and $\\text{MSE}_{\\text{valid}} \\ge t_o' \\sigma^2$ and $\\rho_{\\text{HF}} \\ge h_o$.\n- Well-fit (code $1$): if neither of the above conditions hold, declare well-fit.\n\nImplement the above and evaluate the following test suite that varies $d$ and $\\lambda$:\n\n- Case $1$: $d = 2$, $\\lambda = 0.001$.\n- Case $2$: $d = 4$, $\\lambda = 0.05$.\n- Case $3$: $d = 12$, $\\lambda = 0.0$.\n- Case $4$: $d = 12$, $\\lambda = 10.0$.\n- Case $5$: $d = 4$, $\\lambda = 0.0$.\n\nYour program must generate the dataset as specified, fit the model for each case, compute the metrics, and output the classification codes for the cases in the given order. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[0,1,2,0,1]$). No physical units, angle units, or percentages are required for this problem. The final output values are integers specified as above. The program must be complete and runnable as is, with no external inputs or files. Use a deterministic seed as specified so results are reproducible for anyone running the program.", "solution": "The problem statement has been meticulously validated and is determined to be valid. It is scientifically sound, self-contained, and well-posed, providing a clear and formalizable task in the domain of computational statistics and machine learning.\n\nThe task is to classify polynomial regression models as underfitting, well-fit, or overfitting based on a set of precise, quantitative criteria. The solution involves data generation, model fitting, metric computation, and classification for several test cases. The entire process is deterministic due to a specified random seed.\n\n### Step 1: Data Generation and Preparation\n\nThe foundation of this regression problem is a synthetic dataset. The ground-truth relationship between the input $x$ and the output $y$ is defined by a known polynomial function $f^\\star(x)$ of degree $d^\\star=4$:\n$$\nf^\\star(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4\n$$\nwith coefficients $a_0 = 0.3$, $a_1 = -0.8$, $a_2 = 0.5$, $a_3 = 0.0$, and $a_4 = 0.7$.\n\nThe observed data are corrupted by additive white Gaussian noise, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, where the noise variance is $\\sigma^2 = 0.01$. Thus, each data point $(x_i, y_i)$ is generated according to the model:\n$$\ny_i = f^\\star(x_i) + \\varepsilon_i\n$$\nA total of $N=200$ data points are created. The input values $x_i$ are drawn from a uniform distribution over the interval $[-1, 1]$. To ensure reproducibility, the random number generator is initialized with a fixed seed of $42$.\n\nThe generated dataset of $N=200$ points is then deterministically shuffled and split into a training set of size $N_{\\text{train}} = 120$ and a validation set of size $N_{\\text{valid}} = 80$. This partitioning allows us to train the model and independently evaluate its generalization performance.\n\n### Step 2: Polynomial Regression with Ridge Regularization\n\nFor each test case, we fit a polynomial model of a specified degree $d$ to the training data. The model hypothesis is of the form:\n$$\n\\hat{y}(x) = \\sum_{k=0}^d w_k x^k = \\mathbf{w}^\\top \\phi(x)\n$$\nwhere $\\mathbf{w} \\in \\mathbb{R}^{d+1}$ is the vector of model coefficients to be learned and $\\phi(x) = [1, x, x^2, \\dots, x^d]^\\top$ is the feature vector.\n\nFor a set of $m$ training samples, we construct the design matrix $\\Phi_{\\text{train}} \\in \\mathbb{R}^{m \\times (d+1)}$, where each entry is $(\\Phi_{\\text{train}})_{i,k} = x_i^k$ for $i \\in \\{1, \\dots, m\\}$ and $k \\in \\{0, \\dots, d\\}$.\n\nThe coefficients $\\mathbf{w}$ are estimated using ridge regression, which minimizes the regularized sum of squared errors:\n$$\n\\mathcal{L}(\\mathbf{w}) = \\|\\mathbf{y}_{\\text{train}} - \\Phi_{\\text{train}}\\mathbf{w}\\|_2^2 + \\lambda \\|\\mathbf{w}\\|_2^2\n$$\nHere, $\\lambda \\ge 0$ is the regularization parameter that controls the penalty on the magnitude of the coefficients. The closed-form solution for the optimal weight vector $\\mathbf{w}$ is given by the normal equations:\n$$\n\\mathbf{w} = \\left(\\Phi_{\\text{train}}^\\top \\Phi_{\\text{train}} + \\lambda I\\right)^{-1} \\Phi_{\\text{train}}^\\top \\mathbf{y}_{\\text{train}}\n$$\nwhere $I$ is the $(d+1) \\times (d+1)$ identity matrix. For numerical stability, this linear system is solved using `numpy.linalg.solve` rather than by computing the matrix inverse explicitly.\n\n### Step 3: Model Evaluation Metrics\n\nOnce the model is trained (i.e., $\\mathbf{w}$ is computed), its performance is evaluated using two key metrics.\n\n**Mean Squared Error (MSE):** The MSE measures the average squared difference between the predicted values $\\hat{y}_i$ and the actual values $y_i$. It is computed for both the training and validation sets:\n$$\n\\text{MSE}_{\\text{train}} = \\frac{1}{N_{\\text{train}}} \\sum_{i=1}^{N_{\\text{train}}} (y_{\\text{train},i} - \\hat{y}_{\\text{train},i})^2\n$$\n$$\n\\text{MSE}_{\\text{valid}} = \\frac{1}{N_{\\text{valid}}} \\sum_{i=1}^{N_{\\text{valid}}} (y_{\\text{valid},i} - \\hat{y}_{\\text{valid},i})^2\n$$\n\n**High-Frequency Energy Ratio ($\\rho_{\\text{HF}}$):** This metric quantifies the oscillatory nature of the model's errors on the validation set, which is a common symptom of overfitting. The procedure is as follows:\n1.  Compute the validation residuals: $\\mathbf{r}_{\\text{valid}} = \\mathbf{y}_{\\text{valid}} - \\hat{\\mathbf{y}}_{\\text{valid}}$.\n2.  Sort these residuals based on the ascending order of their corresponding input values $x_{\\text{valid}}$. Let this sorted sequence be $\\mathbf{r}_{\\text{valid-sorted}}$.\n3.  Compute the real-valued Discrete Fourier Transform (DFT) of the sorted residuals: $R = \\text{rfft}(\\mathbf{r}_{\\text{valid-sorted}})$. For $N_{\\text{valid}} = 80$, the output $R$ is a complex-valued array of length $41$.\n4.  The set of positive-frequency bins, $\\mathcal{P}$, consists of all bins except the zero-frequency (DC) component. For the RFFT output $R$, these correspond to indices $k \\in \\{1, 2, \\dots, 40\\}$.\n5.  The set of high-frequency bins, $\\mathcal{H}$, is defined as the top quartile (highest $25\\%$) of the frequencies in $\\mathcal{P}$. This corresponds to the last $40 \\times 0.25 = 10$ bins, which have indices $k \\in \\{31, 32, \\dots, 40\\}$.\n6.  The high-frequency energy ratio is then the ratio of the energy in $\\mathcal{H}$ to the total energy in $\\mathcal{P}$:\n    $$\n    \\rho_{\\text{HF}} = \\frac{\\sum_{k \\in \\mathcal{H}} |R_k|^2}{\\sum_{k \\in \\mathcal{P}} |R_k|^2}\n    $$\n    If the denominator is zero, $\\rho_{\\text{HF}}$ is defined as $0$.\n\n### Step 4: Classification Logic\n\nThe computed metrics are used to classify each model as underfitting, well-fit, or overfitting according to a fixed set of rules. The ground truth noise variance is $\\sigma^2 = 0.01$, and the thresholds are $t_u = 1.3$, $t_o = 0.9$, $t_o' = 1.2$, $h_u = 0.35$, and $h_o = 0.45$.\n\n- **Underfitting (Code $0$):** A model is declared to be underfitting if its degree $d$ is less than the true degree $d^\\star$, *or* if it exhibits high error on both training and validation sets coupled with low residual oscillation. Formally:\n  $$\n  (d  d^\\star) \\lor (\\text{MSE}_{\\text{train}} \\ge t_u \\sigma^2 \\land \\text{MSE}_{\\text{valid}} \\ge t_u \\sigma^2 \\land \\rho_{\\text{HF}} \\le h_u)\n  $$\n\n- **Overfitting (Code $2$):** A model is declared to be overfitting if its degree $d$ is greater than $d^\\star$ *and* it shows a low training error, a significantly higher validation error, and high-frequency oscillations in its residuals. Formally:\n  $$\n  (d  d^\\star) \\land (\\text{MSE}_{\\text{train}} \\le t_o \\sigma^2 \\land \\text{MSE}_{\\text{valid}} \\ge t_o' \\sigma^2 \\land \\rho_{\\text{HF}} \\ge h_o)\n  $$\n\n- **Well-fit (Code $1$):** If a model meets neither the underfitting nor the overfitting criteria, it is classified as well-fit.\n\nThese rules provide a concrete, algorithmic definition of the bias-variance trade-off concepts. The program implements this logic for each specified test case, generating a final list of classification codes.", "answer": "```python\nimport numpy as np\nimport scipy.fft\n\ndef solve():\n    \"\"\"\n    Main function to execute the polynomial regression analysis and classification.\n    \"\"\"\n    #\n    # Step 0: Define constants and problem parameters\n    #\n    RANDOM_SEED = 42\n    D_STAR = 4\n    A_COEFFS = np.array([0.3, -0.8, 0.5, 0.0, 0.7])\n    SIGMA = 0.1\n    SIGMA_SQUARED = SIGMA**2\n    N_TOTAL = 200\n    N_TRAIN = 120\n    N_VALID = 80\n\n    # Classification thresholds\n    T_U = 1.3\n    T_O = 0.9\n    T_O_PRIME = 1.2\n    H_U = 0.35\n    H_O = 0.45\n\n    # Test cases to evaluate\n    test_cases = [\n        {'d': 2, 'lambda': 0.001},  # Case 1\n        {'d': 4, 'lambda': 0.05},   # Case 2\n        {'d': 12, 'lambda': 0.0},    # Case 3\n        {'d': 12, 'lambda': 10.0},   # Case 4\n        {'d': 4, 'lambda': 0.0},    # Case 5\n    ]\n    \n    #\n    # Step 1: Generate dataset\n    #\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # Generate x values\n    x = rng.uniform(-1, 1, size=N_TOTAL)\n\n    # Generate true function values y_star\n    def f_star(x_in):\n        return A_COEFFS[0] + A_COEFFS[1] * x_in + A_COEFFS[2] * x_in**2 + \\\n               A_COEFFS[3] * x_in**3 + A_COEFFS[4] * x_in**4\n\n    y_star = f_star(x)\n\n    # Add Gaussian noise\n    noise = rng.normal(0, SIGMA, size=N_TOTAL)\n    y = y_star + noise\n\n    # Split into training and validation sets\n    indices = np.arange(N_TOTAL)\n    rng.shuffle(indices)\n    \n    train_indices = indices[:N_TRAIN]\n    valid_indices = indices[N_TRAIN:]\n\n    x_train, y_train = x[train_indices], y[train_indices]\n    x_valid, y_valid = x[valid_indices], y[valid_indices]\n\n    #\n    # Helper functions\n    #\n    def construct_design_matrix(x_data, degree):\n        \"\"\"Constructs the polynomial design matrix Phi.\"\"\"\n        return np.vander(x_data, degree + 1, increasing=True)\n\n    results = []\n\n    #\n    # Step 2-4: Process each test case\n    #\n    for case in test_cases:\n        d = case['d']\n        lambda_reg = case['lambda']\n\n        # Construct design matrices\n        phi_train = construct_design_matrix(x_train, d)\n        phi_valid = construct_design_matrix(x_valid, d)\n\n        # Fit the model using ridge regression (numerically stable)\n        d_plus_1 = d + 1\n        A = phi_train.T @ phi_train + lambda_reg * np.eye(d_plus_1)\n        b = phi_train.T @ y_train\n        w = np.linalg.solve(A, b)\n\n        # Make predictions\n        y_hat_train = phi_train @ w\n        y_hat_valid = phi_valid @ w\n\n        # Calculate metrics\n        # a) MSE\n        mse_train = np.mean((y_train - y_hat_train)**2)\n        mse_valid = np.mean((y_valid - y_hat_valid)**2)\n        \n        # b) High-frequency energy ratio rho_HF\n        residuals_valid = y_valid - y_hat_valid\n        \n        # Sort residuals according to x_valid\n        sort_indices = np.argsort(x_valid)\n        residuals_valid_sorted = residuals_valid[sort_indices]\n        \n        # Compute RFFT\n        R = scipy.fft.rfft(residuals_valid_sorted)\n        \n        # Calculate energies\n        # P: positive frequencies (indices 1 to end)\n        # H: top quartile of P (last 10 for N_valid=80)\n        # N_valid = 80 - rfft length = 41. P_indices = 1..40. H_indices = 31..40.\n        num_positive_freqs = len(R) - 1\n        top_quartile_size = int(np.ceil(0.25 * num_positive_freqs))\n        \n        energy_P = np.sum(np.abs(R[1:])**2)\n        energy_H = np.sum(np.abs(R[-top_quartile_size:])**2)\n        \n        rho_hf = energy_H / energy_P if energy_P  0 else 0.0\n        \n        # Apply classification rules\n        code = 1 # Default to well-fit\n\n        # Underfitting rule\n        is_underfit_by_degree = (d  D_STAR)\n        is_underfit_by_metrics = (mse_train = T_U * SIGMA_SQUARED and \\\n                                  mse_valid = T_U * SIGMA_SQUARED and \\\n                                  rho_hf = H_U)\n        if is_underfit_by_degree or is_underfit_by_metrics:\n            code = 0\n\n        # Overfitting rule\n        is_overfit_by_metrics = (d  D_STAR and \\\n                                 mse_train = T_O * SIGMA_SQUARED and \\\n                                 mse_valid = T_O_PRIME * SIGMA_SQUARED and \\\n                                 rho_hf = H_O)\n        if is_overfit_by_metrics:\n            code = 2\n            \n        results.append(code)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3135788"}, {"introduction": "A well-trained model that performs poorly on new data might be doing more than just fitting random noise; it might be exploiting 'shortcuts.' These are features or correlations in the training data that are highly predictive but not causally related to the outcome, and thus fail to generalize. This practice guides you in constructing a synthetic environment to explore this subtle but critical failure mode, challenging you to diagnose a model that has overfit to non-robust shortcuts by observing how its performance collapses when these shortcuts are experimentally removed [@problem_id:3135726].", "problem": "You must write a complete, runnable program that constructs a synthetic binary classification dataset with a mixture of robust features and shortcut features, trains a one-hidden-layer neural network using empirical risk minimization, evaluates validation accuracy under two controlled conditions (shortcut features visible and shortcut features suppressed), and diagnoses overfitting or underfitting from first principles. The program must implement the following scenario and produce a single line of output containing a list of integers for the specified test suite.\n\nFundamental base. Consider empirical risk minimization over a hypothesis class $\\mathcal{F}$, where a classifier $f \\in \\mathcal{F}$ maps inputs $x \\in \\mathbb{R}^d$ to predicted probabilities $\\hat{y} \\in [0,1]$ for a binary label $y \\in \\{0,1\\}$. Let the empirical risk on a dataset $S = \\{(x_i, y_i)\\}_{i=1}^n$ be\n$$\nR_S(f) = \\frac{1}{n} \\sum_{i=1}^n \\ell(f(x_i), y_i),\n$$\nwith the binary cross-entropy loss $\\ell(\\hat{y}, y) = -\\left[y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})\\right]$. The expected risk under a distribution $\\mathcal{D}$ is $R_{\\mathcal{D}}(f) = \\mathbb{E}_{(x,y)\\sim\\mathcal{D}}[\\ell(f(x),y)]$. The generalization gap is $\\Gamma(f) = R_{\\mathcal{D}}(f) - R_S(f)$. Overfitting is characterized by small training error but large validation error, and underfitting is characterized by large errors across both training and validation due to insufficient model capacity or poor optimization.\n\nData generation with robust and shortcut features. For each sample, let the input be $x = [r; s] \\in \\mathbb{R}^{p+q}$, where $r \\in \\mathbb{R}^p$ are robust features and $s \\in \\mathbb{R}^q$ are shortcut features. Labels are $y \\in \\{0,1\\}$, and denote $y' = 2y - 1 \\in \\{-1, +1\\}$. Robust features follow\n$$\nr \\sim \\mathcal{N}\\left(m \\, y' \\, \\mathbf{1}_p, \\, \\sigma_r^2 I_p\\right),\n$$\nwhile shortcut features follow\n$$\ns \\sim \\mathcal{N}\\left(\\alpha \\, y' \\, \\mathbf{1}_q, \\, \\sigma_s^2 I_q\\right).\n$$\nAn augmentation called shortcut suppression replaces $s$ by noise independent of $y$:\n$$\nA(x) = [r; \\tilde{s}], \\quad \\tilde{s} \\sim \\mathcal{N}\\left(0, \\sigma_s^2 I_q\\right).\n$$\nThis augmentation preserves the robust signal while suppressing shortcuts. When shortcuts are visible, validation is performed on $x$; when suppressed, validation is performed on $A(x)$.\n\nModel and training. Use a one-hidden-layer neural network $f_\\theta$ with parameters $\\theta = (W_1, b_1, W_2, b_2)$,\n$$\nh = \\phi(W_1 x + b_1), \\quad \\hat{y} = \\sigma(W_2^\\top h + b_2),\n$$\nwhere $\\phi$ is a rectified linear unit (ReLU) and $\\sigma$ is the logistic sigmoid. Train using stochastic gradient descent to minimize $R_S(f_\\theta)$ on a training set generated as above; optionally apply shortcut suppression to the training inputs to discourage shortcut learning.\n\nDiagnosis rules. Let validation accuracy when shortcuts are visible be $a_{\\mathrm{vis}}$ and when suppressed be $a_{\\mathrm{sup}}$. Use thresholds $\\tau_h = 0.9$ (high accuracy), $\\tau_\\ell = 0.7$ (low accuracy), and drop threshold $\\Delta = 0.25$. Diagnose:\n- Overfitting to shortcuts if $a_{\\mathrm{vis}} \\ge \\tau_h$, $a_{\\mathrm{sup}} \\le \\tau_\\ell$, and $a_{\\mathrm{vis}} - a_{\\mathrm{sup}} \\ge \\Delta$.\n- Underfitting if $a_{\\mathrm{vis}}  \\tau_\\ell$ and $a_{\\mathrm{sup}}  \\tau_\\ell$.\n- Otherwise, neither underfitting nor overfitting.\n\nEncode the diagnosis as an integer: overfitting $\\to 1$, underfitting $\\to -1$, otherwise $\\to 0$.\n\nProgram requirements. Your program must:\n- Generate independent training and validation datasets for each test case using a fixed random seed per case to ensure reproducibility.\n- Implement training of the specified neural network using binary cross-entropy and stochastic gradient descent.\n- Evaluate $a_{\\mathrm{vis}}$ and $a_{\\mathrm{sup}}$ and apply the diagnosis rules above.\n- Produce a single line of output with the diagnosis results for all test cases as a comma-separated list enclosed in square brackets, e.g., $[1,0,-1,0]$.\n\nTest suite. Use the following test cases, each specified as a tuple $(p, q, m, \\alpha, \\sigma_r, \\sigma_s, H, E, \\eta, \\text{train\\_suppress}, \\text{seed})$ where $p$ and $q$ are feature dimensions, $m$ and $\\alpha$ are signal strengths, $\\sigma_r$ and $\\sigma_s$ are noise scales, $H$ is hidden layer width, $E$ is number of training epochs, $\\eta$ is learning rate, $\\text{train\\_suppress} \\in \\{0,1\\}$ indicates whether shortcut suppression is applied during training, and $\\text{seed}$ is the random seed. The dataset sizes must be fixed: training set size $n_{\\mathrm{train}} = 3000$ and validation set size $n_{\\mathrm{val}} = 3000$.\n\n- Case $1$ (expected overfitting): $(5, 3, 0.2, 4.0, 1.0, 1.0, 32, 50, 0.05, 0, 1337)$.\n- Case $2$ (expected robust generalization): $(5, 3, 0.6, 4.0, 1.0, 1.0, 32, 50, 0.05, 1, 2027)$.\n- Case $3$ (expected underfitting): $(5, 3, 0.6, 4.0, 1.0, 1.0, 2, 5, 0.01, 1, 3037)$.\n- Case $4$ (expected robust generalization even without suppression due to strong robust signal): $(5, 3, 1.5, 1.0, 1.0, 1.0, 32, 50, 0.05, 0, 4047)$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_i \\in \\{-1,0,1\\}$ is the diagnosis for case $i$ according to the rules above. No other text should be printed.", "solution": "The problem requires the implementation of a computational experiment to diagnose overfitting and underfitting in a neural network trained on a synthetic dataset with two types of features: robust and shortcut. The solution involves four main stages: data generation, model implementation, training, and diagnosis.\n\n### 1. Data Generation\n\nFor each test case, a training set of size $n_{\\mathrm{train}} = 3000$ and a validation set of size $n_{\\mathrm{val}} = 3000$ are generated. Each data point $(x, y)$ consists of an input vector $x \\in \\mathbb{R}^{p+q}$ and a binary label $y \\in \\{0,1\\}$. The input vector $x$ is a concatenation of robust features $r \\in \\mathbb{R}^p$ and shortcut features $s \\in \\mathbb{R}^q$, i.e., $x = [r; s]$.\n\nThe labels $y$ are drawn from a Bernoulli distribution with parameter $0.5$. For mathematical convenience, the label is transformed to $y' = 2y - 1$, so $y' \\in \\{-1, +1\\}$. The features are then generated from multivariate normal distributions whose means depend on $y'$:\n-   Robust features: $r \\sim \\mathcal{N}\\left(m \\, y' \\, \\mathbf{1}_p, \\, \\sigma_r^2 I_p\\right)$\n-   Shortcut features: $s \\sim \\mathcal{N}\\left(\\alpha \\, y' \\, \\mathbf{1}_q, \\, \\sigma_s^2 I_q\\right)$\n\nHere, $m$ and $\\alpha$ control the signal strength of robust and shortcut features, respectively. $\\sigma_r^2$ and $\\sigma_s^2$ are their variances. This process is implemented using vectorized `numpy` operations for efficiency. A specific random seed is set for each test case to ensure reproducibility of the data and subsequent results.\n\n### 2. Model Architecture\n\nThe classifier is a one-hidden-layer neural network, $f_\\theta$. The input dimension is $d = p+q$. The architecture is defined as:\n$$\nh = \\phi(W_1 x + b_1)\n$$\n$$\n\\hat{y} = \\sigma(W_2^\\top h + b_2)\n$$\nwhere:\n-   $x \\in \\mathbb{R}^d$ is the input vector.\n-   $W_1 \\in \\mathbb{R}^{H \\times d}$ and $b_1 \\in \\mathbb{R}^{H}$ are the weight matrix and bias vector of the hidden layer, with $H$ being the number of hidden units.\n-   $\\phi(\\cdot)$ is the Rectified Linear Unit (ReLU) activation function, $\\phi(z) = \\max(0, z)$.\n-   $h \\in \\mathbb{R}^{H}$ is the hidden layer activation.\n-   $W_2 \\in \\mathbb{R}^{H}$ and $b_2 \\in \\mathbb{R}$ are the weight vector and scalar bias of the output layer.\n-   $\\sigma(\\cdot)$ is the logistic sigmoid function, $\\sigma(z) = (1 + e^{-z})^{-1}$, which maps the logit to a probability $\\hat{y} \\in [0,1]$.\n-   The parameters of the model are $\\theta = (W_1, b_1, W_2, b_2)$. Weights are initialized with small random values, and biases are initialized to zero.\n\n### 3. Training Procedure\n\nThe model is trained to minimize the empirical risk, which is the average binary cross-entropy loss over the training set $S = \\{(x_i, y_i)\\}_{i=1}^{n_{\\mathrm{train}}}$:\n$$\nR_S(f_\\theta) = \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} \\ell(\\sigma(W_2^\\top \\phi(W_1 x_i + b_1) + b_2), y_i)\n$$\nOptimization is performed using mini-batch stochastic gradient descent (SGD). For each epoch, the training data is shuffled and split into mini-batches. For each mini-batch:\n1.  **Augmentation (Conditional)**: If the `train_suppress` flag is set to $1$, shortcut suppression is applied to the input batch. This involves replacing the shortcut features $s$ with noise drawn from $\\mathcal{N}(0, \\sigma_s^2 I_q)$, effectively forcing the model to rely on robust features.\n2.  **Forward Pass**: The mini-batch is passed through the network to compute the predicted probabilities $\\hat{y}$.\n3.  **Loss Calculation**: The binary cross-entropy loss is computed. A small epsilon term ($10^{-9}$) is added within the logarithm to ensure numerical stability.\n4.  **Backward Pass (Backpropagation)**: The gradients of the loss with respect to all parameters ($\\nabla_{W_1} R_S$, $\\nabla_{b_1} R_S$, $\\nabla_{W_2} R_S$, $\\nabla_{b_2} R_S$) are calculated using the chain rule. The derivative of the ReLU function is $1$ for positive inputs and $0$ otherwise.\n5.  **Parameter Update**: The model parameters are updated by taking a step in the negative gradient direction, scaled by the learning rate $\\eta$:\n    $$\n    \\theta \\leftarrow \\theta - \\eta \\nabla_\\theta R_S(f_\\theta)\n    $$\nThis process is repeated for a specified number of epochs $E$.\n\n### 4. Evaluation and Diagnosis\n\nAfter training, the model's performance is gauged on the validation set under two conditions:\n1.  **Visible Shortcuts ($a_{\\mathrm{vis}}$)**: The model's accuracy is calculated on the original validation set. The predicted label is $1$ if $\\hat{y} \\ge 0.5$ and $0$ otherwise.\n2.  **Suppressed Shortcuts ($a_{\\mathrm{sup}}$)**: A modified validation set is created by applying the shortcut suppression augmentation $A(x) = [r; \\tilde{s}]$ to every sample, where $\\tilde{s} \\sim \\mathcal{N}(0, \\sigma_s^2 I_q)$. The model's accuracy is then calculated on this suppressed dataset.\n\nThe final diagnosis is made based on a set of predefined rules using the thresholds $\\tau_h = 0.9$, $\\tau_\\ell = 0.7$, and $\\Delta = 0.25$:\n-   If $a_{\\mathrm{vis}} \\ge \\tau_h$ AND $a_{\\mathrm{sup}} \\le \\tau_\\ell$ AND $a_{\\mathrm{vis}} - a_{\\mathrm{sup}} \\ge \\Delta$, the model is diagnosed with **overfitting to shortcuts** (encoded as $1$). This indicates high reliance on shortcuts that are not robust.\n-   If $a_{\\mathrm{vis}}  \\tau_\\ell$ AND $a_{\\mathrm{sup}}  \\tau_\\ell$, the model is diagnosed with **underfitting** (encoded as $-1$). This suggests the model failed to learn meaningful patterns from either feature type.\n-   Otherwise, the model's behavior is classified as **neither** of the above, typically indicating robust generalization (encoded as $0$).\n\nThe program iterates through each test case, performs these steps, and collates the integer-encoded diagnoses into a final list.", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit\n\nclass NeuralNetwork:\n    \"\"\"A one-hidden-layer neural network for binary classification.\"\"\"\n    def __init__(self, input_dim, hidden_dim):\n        \"\"\"\n        Initializes the parameters of the neural network.\n        \n        Args:\n            input_dim (int): Dimension of the input features (p+q).\n            hidden_dim (int): Number of units in the hidden layer (H).\n        \"\"\"\n        # Problem statement: W2 is a column vector, so W2.T is a row vector\n        self.W1 = np.random.randn(hidden_dim, input_dim) * 0.01\n        self.b1 = np.zeros((hidden_dim, 1))\n        self.W2 = np.random.randn(hidden_dim, 1) * 0.01\n        self.b2 = np.zeros((1, 1))\n        self.cache = {}\n\n    @staticmethod\n    def relu(z):\n        return np.maximum(0, z)\n\n    def forward(self, X):\n        \"\"\"\n        Performs the forward pass.\n        \n        Args:\n            X (np.ndarray): Input data of shape (input_dim, num_samples).\n        \n        Returns:\n            np.ndarray: Predicted probabilities of shape (1, num_samples).\n        \"\"\"\n        Z1 = self.W1 @ X + self.b1\n        A1 = self.relu(Z1)\n        Z2 = self.W2.T @ A1 + self.b2\n        A2 = expit(Z2) # Numerically stable sigmoid\n\n        self.cache = {'X': X, 'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n        return A2\n\n    def backward(self, Y):\n        \"\"\"\n        Performs the backward pass and computes gradients.\n        \n        Args:\n            Y (np.ndarray): True labels of shape (1, num_samples).\n        \"\"\"\n        num_samples = Y.shape[1]\n        X, A1, A2, Z1 = self.cache['X'], self.cache['A1'], self.cache['A2'], self.cache['Z1']\n\n        dZ2 = A2 - Y\n        self.dW2 = (1 / num_samples) * (A1 @ dZ2.T)\n        self.db2 = (1 / num_samples) * np.sum(dZ2, axis=1, keepdims=True)\n\n        dA1 = self.W2 @ dZ2\n        dZ1 = dA1 * (Z1  0)\n        self.dW1 = (1 / num_samples) * (dZ1 @ X.T)\n        self.db1 = (1 / num_samples) * np.sum(dZ1, axis=1, keepdims=True)\n\n    def update_params(self, learning_rate):\n        \"\"\"Updates parameters using computed gradients.\"\"\"\n        self.W1 -= learning_rate * self.dW1\n        self.b1 -= learning_rate * self.db1\n        self.W2 -= learning_rate * self.dW2\n        self.b2 -= learning_rate * self.db2\n\ndef generate_data(n_samples, p, q, m, alpha, sigma_r, sigma_s):\n    \"\"\"Generates synthetic data with robust and shortcut features.\"\"\"\n    y = np.random.randint(0, 2, size=(n_samples, 1))\n    y_prime = 2 * y - 1\n\n    # Generate robust features\n    mean_r_mat = y_prime @ np.ones((1, p)) * m\n    r = np.random.randn(n_samples, p) * sigma_r + mean_r_mat\n\n    # Generate shortcut features\n    mean_s_mat = y_prime @ np.ones((1, q)) * alpha\n    s = np.random.randn(n_samples, q) * sigma_s + mean_s_mat\n    \n    X = np.hstack((r, s))\n    return X, y\n\ndef apply_shortcut_suppression(X, q, sigma_s):\n    \"\"\"Applies shortcut suppression augmentation to data.\"\"\"\n    n_samples, _ = X.shape\n    p = X.shape[1] - q\n    r_features = X[:, :p]\n    noise = np.random.randn(n_samples, q) * sigma_s\n    return np.hstack((r_features, noise))\n\ndef solve():\n    \"\"\"Main function to run the test suite and produce the final output.\"\"\"\n    test_cases = [\n        (5, 3, 0.2, 4.0, 1.0, 1.0, 32, 50, 0.05, 0, 1337),\n        (5, 3, 0.6, 4.0, 1.0, 1.0, 32, 50, 0.05, 1, 2027),\n        (5, 3, 0.6, 4.0, 1.0, 1.0, 2, 5, 0.01, 1, 3037),\n        (5, 3, 1.5, 1.0, 1.0, 1.0, 32, 50, 0.05, 0, 4047),\n    ]\n\n    n_train = 3000\n    n_val = 3000\n    batch_size = 32\n    \n    tau_h = 0.9\n    tau_l = 0.7\n    delta = 0.25\n\n    results = []\n    \n    for case in test_cases:\n        p, q, m, alpha, sigma_r, sigma_s, H, E, eta, train_suppress, seed = case\n        \n        np.random.seed(seed)\n        \n        # 1. Generate Data\n        X_train, y_train = generate_data(n_train, p, q, m, alpha, sigma_r, sigma_s)\n        X_val, y_val = generate_data(n_val, p, q, m, alpha, sigma_r, sigma_s)\n\n        # 2. Initialize Model\n        input_dim = p + q\n        model = NeuralNetwork(input_dim, H)\n\n        # 3. Train Model\n        n_batches = int(np.ceil(n_train / batch_size))\n        for epoch in range(E):\n            permutation = np.random.permutation(n_train)\n            X_train_shuffled = X_train[permutation, :]\n            y_train_shuffled = y_train[permutation, :]\n\n            for i in range(n_batches):\n                start = i * batch_size\n                end = min(start + batch_size, n_train)\n                X_batch = X_train_shuffled[start:end, :]\n                y_batch = y_train_shuffled[start:end, :]\n                \n                # Apply shortcut suppression if required\n                if train_suppress == 1:\n                    X_batch = apply_shortcut_suppression(X_batch, q, sigma_s)\n                \n                # Transpose for model's expected shape (dim, samples)\n                X_batch_T = X_batch.T\n                y_batch_T = y_batch.T\n\n                # Forward, backward, update\n                _ = model.forward(X_batch_T)\n                model.backward(y_batch_T)\n                model.update_params(eta)\n\n        # 4. Evaluate Model\n        # a_vis: accuracy on validation set with shortcuts visible\n        y_hat_vis = model.forward(X_val.T)\n        predictions_vis = (y_hat_vis = 0.5).astype(int)\n        a_vis = np.mean(predictions_vis == y_val.T)\n\n        # a_sup: accuracy on validation set with shortcuts suppressed\n        X_val_sup = apply_shortcut_suppression(X_val, q, sigma_s)\n        y_hat_sup = model.forward(X_val_sup.T)\n        predictions_sup = (y_hat_sup = 0.5).astype(int)\n        a_sup = np.mean(predictions_sup == y_val.T)\n\n        # 5. Diagnose\n        diagnosis = 0 # Default: Neither\n        if a_vis = tau_h and a_sup = tau_l and (a_vis - a_sup) = delta:\n            diagnosis = 1 # Overfitting to shortcuts\n        elif a_vis  tau_l and a_sup  tau_l:\n            diagnosis = -1 # Underfitting\n            \n        results.append(diagnosis)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3135726"}, {"introduction": "An accurate diagnosis of underfitting or overfitting relies on one crucial assumption: that your validation loss is an unbiased estimate of generalization error. This practice explores a common and insidious pitfall—data leakage—where this assumption is violated, leading to dangerously misleading results. You will analyze a scenario where improper preprocessing artificially lowers the validation loss, making an underfitting model appear to generalize well, and learn to identify the procedural error that caused it [@problem_id:3135777]. Mastering this concept is essential for ensuring the integrity of your machine learning experiments.", "problem": "A team trains a deep neural network for regression on a tabular dataset with $n$ samples and $d$ features. The pipeline includes per-feature standardization and principal component analysis (PCA) before the network. Let $L_{\\text{train}}$ and $L_{\\text{val}}$ denote mean squared error on the training and validation sets, respectively. Consider two experiments on the same train/validation split and the same model architecture and optimizer:\n\n- Experiment $1$ (leaky preprocessing): standardization and PCA are fit once on the entire dataset (training and validation combined) and then applied to both sets. After $50$ epochs, the team records $L_{\\text{train}} \\approx 0.84$ and $L_{\\text{val}} \\approx 0.62$.\n\n- Experiment $2$ (proper preprocessing): standardization and PCA are fit using training data only and then applied to the validation set. After $50$ epochs, the team records $L_{\\text{train}} \\approx 0.84$ and $L_{\\text{val}} \\approx 0.86$.\n\nAssume the data are independently and identically distributed (i.i.d.) draws from an unknown distribution, and the validation set is intended to provide an unbiased estimate of generalization risk when it is not used to fit any part of the pipeline. Assume also that optimization is stable across runs so that observed differences come from preprocessing choices rather than stochastic noise.\n\nBased on the fundamental definitions of empirical risk minimization and the role of a validation set as an independent performance estimator, which option best explains why Experiment $1$ produces the observed $L_{\\text{val}}$ and how this can mask an underfitting regime, and also prescribes a reliable, proactive check to detect the issue before mislabeling the regime?\n\nA. The preprocessing in Experiment $1$ uses information from the validation inputs to set transformation parameters, so the transformation $T_{\\theta}$ depends on the validation features. This violates the independence required for an unbiased validation estimate and induces an optimistic bias that artificially lowers $L_{\\text{val}}$. Because $L_{\\text{train}}$ remains high (around $0.84$) in both experiments, the model class is underfitting, but leakage in Experiment $1$ masks this by making $L_{\\text{val}}$ appear much lower. A reliable check is to enforce that all preprocessing steps are fit strictly on the training data within each split (for example, by encapsulating them in a single training-only fit stage or within-fold cross-validation) and to compare $L_{\\text{val}}$; a marked increase when the fit is restricted to training data is evidence of leakage.\n\nB. The preprocessing in Experiment $1$ mainly reduces $L_{\\text{train}}$ by giving the model partial access to validation targets during optimization, so the gap $L_{\\text{train}} - L_{\\text{val}}$ cannot be trusted. The correct diagnosis is overfitting, which should be addressed by reducing model capacity. A reliable check is to add stronger regularization and see if $L_{\\text{train}}$ increases.\n\nC. The drop in $L_{\\text{val}}$ in Experiment $1$ proves the model generalizes better there; scaling and PCA are unsupervised so they cannot leak information. The correct diagnosis is that the model is not underfitting, and no special check is needed beyond monitoring $L_{\\text{val}}$.\n\nD. Because $L_{\\text{val}}  L_{\\text{train}}$ in Experiment $1$, the model must be overfitting. The appropriate check is to increase early stopping patience so that $L_{\\text{val}}$ rises to match $L_{\\text{train}}$.\n\nE. The preprocessing in Experiment $1$ biases $L_{\\text{val}}$ downward, but this effect can be ruled out by permuting the validation labels and recomputing $L_{\\text{val}}$; if $L_{\\text{val}}$ increases after permutation, there is no leakage. If $L_{\\text{val}}$ is unchanged, then leakage exists; otherwise, the model is not underfitting.", "solution": "Begin from the role of the validation set and empirical risk. Let $(X,Y) \\sim \\mathcal{D}$ be the data-generating distribution. A learning algorithm chooses parameters $\\hat{w}$ by minimizing empirical risk on training data $\\{(x_i,y_i)\\}_{i=1}^{n_{\\text{train}}}$, for example\n$$\n\\hat{w} \\in \\arg\\min_{w} \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} \\ell(f_w(T_{\\theta}(x_i)), y_i),\n$$\nwhere $T_{\\theta}$ denotes a data-dependent preprocessing transform (for example, standardization and principal component analysis) with parameters $\\theta$ estimated from data, $f_w$ denotes the model, and $\\ell$ denotes the loss (mean squared error). The validation loss estimates the generalization risk\n$$\nL_{\\text{val}} \\approx \\mathbb{E}_{(X,Y)\\sim \\mathcal{D}}[\\ell(f_{\\hat{w}}(T_{\\hat{\\theta}}(X)),Y)]\n$$\nif and only if the validation examples are independent of any data-dependent choices, including both $\\hat{w}$ and $\\hat{\\theta}$.\n\nIn Experiment $2$, $\\hat{\\theta}$ is fit on training data only, which preserves the independence of the validation set and yields an approximately unbiased estimate of the risk. Observing $L_{\\text{train}} \\approx 0.84$ and $L_{\\text{val}} \\approx 0.86$ indicates that the hypothesis class or optimization is not fitting the training data well; both losses are high, which is characteristic of underfitting.\n\nIn Experiment $1$, the preprocessing parameters $\\hat{\\theta}$ are estimated using the entire dataset, including validation inputs. This makes $T_{\\hat{\\theta}}$ a function of the validation features, and therefore $L_{\\text{val}}$ is no longer evaluated on data independent of the training pipeline. The dependence introduces optimistic bias: because $T_{\\hat{\\theta}}$ is tuned using information from the validation features, the transformed validation inputs are, on average, better aligned to the model’s fitted representation than they would be under an independent transform fit on training only. As a result, $L_{\\text{val}}$ is artificially reduced (here from approximately $0.86$ to approximately $0.62$), even though $L_{\\text{train}}$ remains approximately $0.84$. This masks the true regime: the high $L_{\\text{train}}$ indicates underfitting, but the biased low $L_{\\text{val}}$ falsely suggests good generalization.\n\nA proactive and reliable check follows directly from the independence requirement: ensure that all preprocessing steps are fit strictly on the training data within each split. Practically, one encapsulates preprocessing and model training in a single pipeline fitted only on training data and, if using cross-validation, fits preprocessing within each fold. If replacing any leaky fit with a training-only fit causes a marked increase in $L_{\\text{val}}$, that is evidence of leakage. A sealed hold-out test set, untouched by any fitting (including preprocessing), serves as an additional safeguard.\n\nOption-by-option analysis:\n\n- Option A: This explanation correctly identifies that $T_{\\theta}$ was fit with validation information, violating independence and biasing $L_{\\text{val}}$ downward. It correctly reads $L_{\\text{train}} \\approx 0.84$ in both experiments as evidence of underfitting and prescribes the correct proactive check: enforce training-only fitting of preprocessing (for example, via within-fold pipelines) and compare $L_{\\text{val}}$. Verdict — Correct.\n\n- Option B: It claims that preprocessing reduced $L_{\\text{train}}$ by providing access to validation targets. The scenario concerns unsupervised preprocessing (standardization and principal component analysis) that does not use targets $Y$; the observed $L_{\\text{train}}$ is unchanged across experiments. It also misdiagnoses overfitting and suggests capacity control rather than addressing leakage. Verdict — Incorrect.\n\n- Option C: It asserts that unsupervised steps cannot leak information. This is false: fitting unsupervised transforms on the entire dataset still uses validation inputs and violates independence, introducing optimistic bias in $L_{\\text{val}}$. The conclusion that no check is needed is therefore invalid. Verdict — Incorrect.\n\n- Option D: It interprets $L_{\\text{val}}  L_{\\text{train}}$ as overfitting. Overfitting is characterized by $L_{\\text{train}}$ being low and $L_{\\text{val}}$ being high relative to $L_{\\text{train}}$, not the reverse; furthermore, the pattern here is explained by leakage. Increasing early stopping patience does not diagnose or resolve preprocessing leakage. Verdict — Incorrect.\n\n- Option E: A label permutation test on the validation set evaluates susceptibility to target leakage or label misuse, not to unsupervised preprocessing leakage. Permuting validation labels will trivially increase loss even without leakage and does not specifically detect that $T_{\\theta}$ used validation inputs. Hence it is not a reliable diagnostic for this scenario. Verdict — Incorrect.", "answer": "$$\\boxed{A}$$", "id": "3135777"}]}