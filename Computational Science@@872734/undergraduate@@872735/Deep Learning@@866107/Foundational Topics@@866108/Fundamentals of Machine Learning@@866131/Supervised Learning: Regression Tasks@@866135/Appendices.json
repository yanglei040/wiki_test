{"hands_on_practices": [{"introduction": "Successful model training hinges on numerical stability, a challenge often encountered when dealing with data at very large scales. This exercise [@problem_id:3178853] provides a controlled environment to witness gradient explosion firsthand, a phenomenon where gradients grow so large they cause numerical overflow. You will construct a scenario that breaks standard gradient descent and then implement two fundamental techniques—target standardization and gradient normalization—to restore stability and build intuition for these crucial methods.", "problem": "You are asked to design and analyze a minimal supervised learning regression experiment that demonstrates how the magnitude of gradients under squared Euclidean loss (also known as $L_2$ loss) can become numerically unstable when target values are extremely large, and how two standard remedies—target standardization and gradient normalization—prevent such instabilities. Work in the following mathematically defined setting.\n\nDataset construction and model:\n- Consider a scalar linear model with parameter $w \\in \\mathbb{R}$ and predictions $\\hat{y}_i = w x_i$.\n- Construct a deterministic dataset of $N$ points with $N = 200$. Let $x_i$ be evenly spaced in the interval $\\left[-5, 5\\right]$ for $i = 1, \\dots, N$.\n- Fix a ground-truth scalar $w_\\star = 2$. For a given scale $s > 0$, define targets by $y_i = s \\cdot w_\\star \\cdot x_i$ for each $i$.\n- Use single-precision floating point ($32$-bit, IEEE $754$) for all training computations to make finite-precision effects observable.\n\nLoss and gradient:\n- Use the mean squared error (squared $L_2$ loss) $$J(w) = \\frac{1}{N}\\sum_{i=1}^{N} \\left(\\hat{y}_i - y_i\\right)^2.$$ \n- Starting from $w^{(0)} = 0$, perform $T$ steps of gradient descent with constant step size (learning rate) $\\alpha$:\n  $$w^{(t+1)} = w^{(t)} - \\alpha \\cdot \\nabla J\\left(w^{(t)}\\right), \\quad t = 0, 1, \\dots, T-1.$$\n- You must begin from the fundamental definitions and derive the analytic expression for $\\nabla J(w)$ for this model before implementing it. Do not assume any “shortcut” formula.\n\nInstability definition and diagnostics:\n- A run is considered unstable if at any iteration any of the following occurs in the training state computed in single precision: a non-finite number (Not-a-Number or infinity) appears in the loss, the parameter, or the gradient. This corresponds to detecting numerical overflow or invalid operations.\n- To assess monotonic improvement without causing overflow in diagnostics, also compute a scale-invariant proxy loss\n  $$\\tilde{J}(w) = \\frac{1}{N}\\sum_{i=1}^{N} \\left(\\frac{\\hat{y}_i - y_i}{M}\\right)^2,$$\n  where $M = \\max_i |y_i|$ is computed in double precision ($64$-bit) prior to training and then treated as a constant; evaluating $\\tilde{J}(w)$ uses single precision for the arithmetic involving $w$, $x_i$, and $y_i$, but divides by the double-precision constant $M$ cast to single precision. This proxy does not alter training and avoids overflow in the diagnostic itself. A run is considered stably improving if it remains finite and $\\tilde{J}(w)$ is nonincreasing (up to a tolerance $\\varepsilon = 10^{-6}$) over iterations and strictly decreases at least once.\n\nRemedies to implement:\n- Target standardization: Compute the standardized targets $y^{\\mathrm{std}}_i = \\left(y_i - \\mu_y\\right)/\\sigma_y$ with $\\mu_y$ the mean of $y_i$ and $\\sigma_y$ the standard deviation of $y_i$. To avoid overflow while computing these statistics for very large $s$, compute $\\mu_y$ and $\\sigma_y$ in double precision ($64$-bit IEEE $754$) and then cast $y^{\\mathrm{std}}_i$ to single precision for training. Train the same scalar linear model on $(x_i, y^{\\mathrm{std}}_i)$ via gradient descent in single precision with the same $\\alpha$ and $T$.\n- Gradient normalization by per-sample clipping: For the original (unstandardized) targets, compute the per-sample contribution to the gradient of the loss with respect to $w$, clip each contribution to a chosen magnitude prior to summation, and then average to form the batch gradient. All operations here, including the clipping and the summation, must be in single precision.\n\nTest suite:\nUse the following parameter values to produce four runs that collectively test unprotected instability, both remedies, and a benign baseline.\n- Dataset size and inputs: $N = 200$, $x_i$ evenly spaced in $\\left[-5, 5\\right]$.\n- Ground truth and initialization: $w_\\star = 2$, $w^{(0)} = 0$.\n- Training hyperparameters: learning rate $\\alpha = 0.05$, iterations $T = 60$, tolerance $\\varepsilon = 10^{-6}$.\n- Large-scale dataset (intentionally hazardous in single precision): $s_{\\mathrm{large}} = 10^{36}$.\n- Small-scale dataset (benign): $s_{\\mathrm{small}} = 1$.\n- Gradient clipping threshold for per-sample normalization: $g_{\\mathrm{clip}} = 10^{34}$.\n\nFor each run, implement full-batch gradient descent under the following four scenarios:\n- Scenario A (Unstandardized, large scale): train on $y_i = s_{\\mathrm{large}} \\cdot w_\\star \\cdot x_i$ with naive full-batch gradient (no normalization).\n- Scenario B (Target standardized, large scale): compute standardized targets from the same large-scale $y_i$ using double-precision statistics, cast to single precision, then train with naive gradient on the standardized targets.\n- Scenario C (Per-sample gradient clipping, large scale): train on the same large-scale $y_i$ with per-sample gradient contributions clipped to magnitude $g_{\\mathrm{clip}}$ in single precision before averaging.\n- Scenario D (Unstandardized, small scale): train on $y_i = s_{\\mathrm{small}} \\cdot w_\\star \\cdot x_i$ with naive full-batch gradient (no normalization).\n\nAnswer specification:\n- For Scenario A, output a boolean indicating whether instability occurred (true if unstable).\n- For Scenario B, output a boolean indicating whether the run is stably improving as defined above (true if stable and improving).\n- For Scenario C, output a boolean indicating whether the run is stably improving as defined above (true if stable and improving).\n- For Scenario D, output a boolean indicating whether the run is stably improving as defined above (true if stable and improving).\n\nFinal output format:\nYour program should produce a single line of output containing the four boolean results in the order $\\left[\\text{A}, \\text{B}, \\text{C}, \\text{D}\\right]$ as a comma-separated list enclosed in square brackets, for example, $[{\\tt True},{\\tt False},{\\tt True},{\\tt True}]$.", "solution": "The problem statement is a well-defined numerical experiment designed to investigate the effects of finite-precision arithmetic on gradient-based optimization in a supervised learning context. It is scientifically grounded, objective, and provides all necessary specifications to be solved. Therefore, it is a valid problem.\n\nWe proceed with a complete solution, starting from first principles.\n\n### Fundamental Definitions and Gradient Derivation\n\nThe analysis is based on a scalar linear model where predictions $\\hat{y}_i$ for inputs $x_i$ are generated by a single parameter $w \\in \\mathbb{R}$:\n$$\n\\hat{y}_i = w x_i\n$$\nThe objective is to minimize the mean squared error (MSE) loss function, $J(w)$, over a dataset of $N$ points $(x_i, y_i)$:\n$$\nJ(w) = \\frac{1}{N}\\sum_{i=1}^{N} \\left(\\hat{y}_i - y_i\\right)^2 = \\frac{1}{N}\\sum_{i=1}^{N} \\left(w x_i - y_i\\right)^2\n$$\nThe parameter $w$ is updated via gradient descent. The gradient, $\\nabla J(w)$, is the derivative of the loss function with respect to the parameter $w$. We derive this from the definition of $J(w)$:\n$$\n\\nabla J(w) = \\frac{dJ}{dw} = \\frac{d}{dw} \\left( \\frac{1}{N}\\sum_{i=1}^{N} (w x_i - y_i)^2 \\right)\n$$\nBy linearity of differentiation, we can move the derivative inside the summation:\n$$\n\\nabla J(w) = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{d}{dw} (w x_i - y_i)^2\n$$\nApplying the chain rule, where the outer function is $u^2$ and the inner function is $u(w) = w x_i - y_i$:\n$$\n\\frac{d}{dw} (w x_i - y_i)^2 = 2(w x_i - y_i) \\cdot \\frac{d}{dw}(w x_i - y_i) = 2(w x_i - y_i) \\cdot x_i\n$$\nSubstituting this back gives the full-batch gradient expression:\n$$\n\\nabla J(w) = \\frac{2}{N}\\sum_{i=1}^{N} (w x_i - y_i) x_i\n$$\nThe gradient descent update rule for $w$ at iteration $t$ with learning rate $\\alpha$ is:\n$$\nw^{(t+1)} = w^{(t)} - \\alpha \\cdot \\nabla J\\left(w^{(t)}\\right)\n$$\nAll training computations are performed using single-precision floating-point numbers ($32$-bit IEEE $754$), which have a finite range and precision. The maximum representable finite value is approximately $3.4 \\times 10^{38}$. Operations resulting in larger values will lead to numerical overflow, producing infinity (`inf`).\n\n### Analysis of Scenarios\n\n**Scenario A (Unstandardized, large scale):**\nHere, the targets are $y_i = s_{\\mathrm{large}} \\cdot w_\\star \\cdot x_i$, with $s_{\\mathrm{large}} = 10^{36}$ and $w_\\star = 2$. The maximum absolute value of $x_i$ is $5$.\nThe maximum magnitude of a target value is $|y_i|_{\\max} = 10^{36} \\cdot 2 \\cdot 5 = 10^{37}$. This value is representable in single precision.\nHowever, instability arises during the computation of the loss and gradient. At the first iteration ($t=0$), $w^{(0)} = 0$. The squared error for a single sample is:\n$$\n\\left(w^{(0)} x_i - y_i\\right)^2 = (-y_i)^2 = y_i^2\n$$\nFor the sample with the largest target magnitude, this becomes $(10^{37})^2 = 10^{74}$. This vastly exceeds the single-precision limit of $\\approx 3.4 \\times 10^{38}$, causing an overflow to `inf`. The loss $J(w^{(0)})$ will therefore be `inf`.\nSimilarly, the per-sample contribution to the unsummed gradient is $2(w^{(0)}x_i - y_i)x_i = -2y_ix_i$. The maximum magnitude is $|-2 \\cdot (10^{36} \\cdot 2 \\cdot 5) \\cdot 5| = 10^{38}$. While this individual term might be representable, summing many such large numbers can also lead to overflow. Crucially, the overflow in the loss calculation is sufficient to classify the run as unstable.\n**Expected Outcome: `True` (instability occurs).**\n\n**Scenario B (Target standardized, large scale):**\nThis remedy preemptively rescales the targets. Standardization is performed using double-precision ($64$-bit) arithmetic to avoid overflow during the computation of statistics. The standardized targets are:\n$$\ny^{\\mathrm{std}}_i = \\frac{y_i - \\mu_y}{\\sigma_y}\n$$\nwhere $\\mu_y$ and $\\sigma_y$ are the mean and standard deviation of the original large-scale targets $y_i$. Since $x_i$ is symmetric around $0$, the mean $\\mu_y = \\mathbb{E}[s w_\\star x_i]$ is $0$. The standard deviation is $\\sigma_y = |s w_\\star| \\sigma_x$.\nThe training is then performed on the new dataset $(x_i, y^{\\mathrm{std}}_i)$. The new target for the parameter $w$ is $w^{\\mathrm{std}}_\\star = (s w_\\star)/\\sigma_y = \\text{sign}(s w_\\star)/\\sigma_x$. With the given $x_i$, $\\sigma_x \\approx 2.89$, so $w^{\\mathrm{std}}_\\star \\approx 1/2.89 \\approx 0.346$.\nAll values in the training process—parameters, targets, predictions, loss, gradients—are now of a small, manageable magnitude. The learning rate $\\alpha=0.05$ is appropriate for this well-conditioned convex problem, ensuring monotonic convergence. The run will be finite. The proxy loss $\\tilde{J}(w)$, which measures performance on the original scale, will be non-increasing and will strictly decrease from its initial value.\n**Expected Outcome: `True` (run is stably improving).**\n\n**Scenario C (Per-sample gradient clipping, large scale):**\nThis remedy addresses the gradient's magnitude directly. The per-sample gradient contribution is $g_i(w) = 2(wx_i - y_i)x_i$.\nAt $t=0$, $w^{(0)}=0$, so $g_i(w^{(0)}) = -2y_ix_i = -2(s w_\\star x_i)x_i = -2 s w_\\star x_i^2$. The magnitude of these contributions can be as large as $|-2 \\cdot 10^{36} \\cdot 2 \\cdot 5^2| = 10^{38}$, which is near the single-precision limit but may not overflow on its own.\nHowever, these values are clipped to the range $[-g_{\\mathrm{clip}}, g_{\\mathrm{clip}}]$, where $g_{\\mathrm{clip}} = 10^{34}$. Since $|g_i(w^{(0)})| > g_{\\mathrm{clip}}$, every non-zero contribution is clipped. The gradient becomes $\\nabla J(w) \\approx -g_{\\mathrm{clip}}$.\nThe first update is $w^{(1)} = w^{(0)} - \\alpha \\cdot \\nabla J(w^{(0)}) \\approx 0 - 0.05 \\cdot (-10^{34}) = 5 \\times 10^{32}$.\nThe parameter $w$ updates remain finite. Although the gradient information is saturated, the direction is correct (increasing $w$ towards the positive target $s w_\\star$). The loss will decrease at each step, albeit sub-optimally. The run will remain finite and the proxy loss will be non-increasing.\n**Expected Outcome: `True` (run is stably improving).**\n\n**Scenario D (Unstandardized, small scale):**\nThis is the benign baseline. The scale is $s_{\\mathrm{small}}=1$. The targets are $y_i = 2x_i$. All values are small. The maximum target magnitude is $10$. There is no risk of numerical overflow. The problem is identical in its geometry to Scenario B, just with a different scaling of the target parameter ($w_\\star=2$ instead of $w^{\\mathrm{std}}_\\star \\approx 0.346$). With a safe learning rate, gradient descent will proceed without issue, exhibiting monotonic convergence.\n**Expected Outcome: `True` (run is stably improving).**", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and runs a supervised learning regression experiment to demonstrate\n    numerical instability and remedies in gradient descent.\n    \"\"\"\n    # Global parameters from the problem\n    N = 200\n    W_STAR = 2.0\n    W0 = 0.0\n    ALPHA = 0.05\n    T = 60\n    EPSILON = 1e-6\n    S_LARGE = 1e36\n    S_SMALL = 1.0\n    G_CLIP = 1e34\n\n    # Data types\n    F32 = np.float32\n    F64 = np.float64\n\n    # Common dataset generation\n    x_64 = np.linspace(-5, 5, N, dtype=F64)\n    x_32 = x_64.astype(F32)\n\n    y_large_64 = S_LARGE * W_STAR * x_64\n    y_large_32 = y_large_64.astype(F32)\n\n    y_small_64 = S_SMALL * W_STAR * x_64\n    y_small_32 = y_small_64.astype(F32)\n\n    def run_scenario_A():\n        w = F32(W0)\n        x = x_32\n        y = y_large_32\n        \n        for _ in range(T):\n            if not np.isfinite(w): return True\n            y_hat = w * x\n            errors = y_hat - y\n            loss = np.mean(errors**2)\n            if not np.isfinite(loss): return True\n            grad = np.mean(F32(2.0) * errors * x)\n            if not np.isfinite(grad): return True\n            w = w - F32(ALPHA) * grad\n        \n        return not np.isfinite(w)\n\n    def run_scenario_B():\n        mu_y_64 = np.mean(y_large_64)\n        sigma_y_64 = np.std(y_large_64)\n        \n        y_std_64 = (y_large_64 - mu_y_64) / sigma_y_64 if sigma_y_64 != 0 else y_large_64 - mu_y_64\n        y_train = y_std_64.astype(F32)\n        x = x_32\n        \n        M_64 = np.max(np.abs(y_large_64))\n        M_32 = F32(M_64)\n        mu_y_32 = F32(mu_y_64)\n        sigma_y_32 = F32(sigma_y_64)\n\n        w = F32(W0)\n        proxy_losses = []\n        is_finite_run = True\n\n        for t in range(T + 1):\n            w_unstd = w * sigma_y_32\n            y_hat_unstd = w_unstd * x + mu_y_32\n            errors_orig_scale = y_hat_unstd - y_large_32\n            \n            proxy_loss = np.mean((errors_orig_scale / M_32)**2) if M_32 != 0 else np.mean(errors_orig_scale**2)\n            \n            if not np.isfinite(proxy_loss):\n                is_finite_run = False\n                break\n            proxy_losses.append(proxy_loss)\n            \n            if t == T: break\n\n            y_hat = w * x\n            errors = y_hat - y_train\n            loss = np.mean(errors**2)\n            grad = np.mean(F32(2.0) * errors * x)\n            \n            if not (np.isfinite(w) and np.isfinite(loss) and np.isfinite(grad)):\n                is_finite_run = False\n                break\n            w = w - F32(ALPHA) * grad\n        \n        if not is_finite_run: return False\n\n        is_non_increasing = all(proxy_losses[i+1] <= proxy_losses[i] + EPSILON for i in range(T))\n        has_decreased = any(proxy_losses[i+1] < proxy_losses[i] for i in range(T))\n            \n        return is_non_increasing and has_decreased\n\n    def run_scenario_C():\n        w = F32(W0)\n        x = x_32\n        y = y_large_32\n        M_64 = np.max(np.abs(y_large_64))\n        M_32 = F32(M_64)\n        clip_val = F32(G_CLIP)\n        \n        proxy_losses = []\n        is_finite_run = True\n\n        for t in range(T + 1):\n            y_hat = w * x\n            errors = y_hat - y\n            proxy_loss = np.mean((errors / M_32)**2) if M_32 != 0 else np.mean(errors**2)\n            if not np.isfinite(proxy_loss):\n                is_finite_run = False\n                break\n            proxy_losses.append(proxy_loss)\n\n            if t == T: break\n\n            loss = np.mean(errors**2)\n            grad_contribs = F32(2.0) * errors * x\n            clipped_contribs = np.clip(grad_contribs, -clip_val, clip_val)\n            grad = np.mean(clipped_contribs)\n\n            if not (np.isfinite(w) and np.isfinite(loss) and np.isfinite(grad)):\n                is_finite_run = False\n                break\n            w = w - F32(ALPHA) * grad\n        \n        if not is_finite_run: return False\n\n        is_non_increasing = all(proxy_losses[i+1] <= proxy_losses[i] + EPSILON for i in range(T))\n        has_decreased = any(proxy_losses[i+1] < proxy_losses[i] for i in range(T))\n            \n        return is_non_increasing and has_decreased\n\n    def run_scenario_D():\n        w = F32(W0)\n        x = x_32\n        y = y_small_32\n        M_64 = np.max(np.abs(y_small_64))\n        M_32 = F32(M_64)\n        \n        proxy_losses = []\n        is_finite_run = True\n\n        for t in range(T + 1):\n            y_hat = w * x\n            errors = y_hat - y\n            proxy_loss = np.mean((errors / M_32)**2) if M_32 != 0 else np.mean(errors**2)\n            if not np.isfinite(proxy_loss):\n                is_finite_run = False\n                break\n            proxy_losses.append(proxy_loss)\n\n            if t == T: break\n            \n            loss = np.mean(errors**2)\n            grad = np.mean(F32(2.0) * errors * x)\n\n            if not (np.isfinite(w) and np.isfinite(loss) and np.isfinite(grad)):\n                is_finite_run = False\n                break\n            w = w - F32(ALPHA) * grad\n\n        if not is_finite_run: return False\n\n        is_non_increasing = all(proxy_losses[i+1] <= proxy_losses[i] + EPSILON for i in range(T))\n        has_decreased = any(proxy_losses[i+1] < proxy_losses[i] for i in range(T))\n        \n        return is_non_increasing and has_decreased\n\n    results = [\n        run_scenario_A(),\n        run_scenario_B(),\n        run_scenario_C(),\n        run_scenario_D(),\n    ]\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3178853"}, {"introduction": "The choice of loss function profoundly impacts a model's resilience to noisy data, as the standard squared-error loss is notoriously sensitive to outliers. This practice [@problem_id:3178857] moves beyond training dynamics to the heart of the objective function itself. You will analytically and numerically compare the robustness of three different losses—the $\\mathcal{L}_1$, Huber, and generalized Charbonnier functions—to understand how their mathematical properties can mitigate the influence of extreme data corruption.", "problem": "Consider a supervised learning regression task under empirical risk minimization, where the objective is to choose a constant prediction $\\hat{y} \\in \\mathbb{R}$ that minimizes the expected loss over a noisy label model. Let the true label be $y = 0$ and the observed label be $y_{\\text{obs}} = y + \\eta$, where the adversarial label noise $\\eta$ is defined by a two-point distribution: with probability $1 - p$, $\\eta = 0$, and with probability $p$, $\\eta = \\delta$. The scalar $p \\in [0,1]$ is a given probability expressed as a decimal or a fraction, and $\\delta > 0$ is a given magnitude of the adversarial shift.\n\nDefine the per-sample residual as $r = y_{\\text{obs}} - \\hat{y}$ and the expected risk as\n$$\nR(\\hat{y};p,\\delta,\\ell) = \\mathbb{E}[\\ell(r)] = (1-p)\\,\\ell(-\\hat{y}) + p\\,\\ell(\\delta - \\hat{y}),\n$$\nwhere $\\ell$ is a chosen loss function. Consider the following three losses:\n1. The absolute error loss (commonly called the $\\mathcal{L}_1$ loss): $\\ell_{\\mathcal{L}_1}(r) = |r|$.\n2. The Huber loss with threshold $\\kappa > 0$: \n$$\n\\ell_{\\text{Huber}}(r;\\kappa) = \n\\begin{cases}\n\\dfrac{1}{2} r^2, & \\text{if } |r| \\le \\kappa,\\\\\n\\kappa\\left(|r| - \\dfrac{1}{2}\\kappa\\right), & \\text{if } |r| > \\kappa.\n\\end{cases}\n$$\nFor this problem use $\\kappa = 10$.\n3. The generalized Charbonnier loss with parameters $\\epsilon > 0$ and $\\beta \\in (0,1)$:\n$$\n\\ell_{\\text{GC}}(r;\\epsilon,\\beta) = \\left(r^2 + \\epsilon^2\\right)^{\\beta}.\n$$\nFor this problem use $\\epsilon = 1$ and $\\beta = 0.45$.\n\nFor each loss, define the optimal constant predictor\n$$\n\\hat{y}^*_{\\ell}(p,\\delta) \\in \\arg\\min_{\\hat{y} \\in \\mathbb{R}} R(\\hat{y};p,\\delta,\\ell),\n$$\nand define the robustness score for the given loss under $(p,\\delta)$ as the absolute bias\n$$\nb_{\\ell}(p,\\delta) = \\left|\\hat{y}^*_{\\ell}(p,\\delta) - y\\right| = \\left|\\hat{y}^*_{\\ell}(p,\\delta)\\right|.\n$$\n\nYour task is to implement a program that, for each $(p,\\delta)$ in the test suite below, computes the three robustness scores $b_{\\mathcal{L}_1}(p,\\delta)$, $b_{\\text{Huber}}(p,\\delta)$, and $b_{\\text{GC}}(p,\\delta)$ corresponding to $\\ell_{\\mathcal{L}_1}$, $\\ell_{\\text{Huber}}$ with $\\kappa = 10$, and $\\ell_{\\text{GC}}$ with $\\epsilon = 1$ and $\\beta = 0.45$. For the $\\mathcal{L}_1$ loss, resolve the tie at $p = 0.5$ by choosing $\\hat{y}^*_{\\mathcal{L}_1}(p,\\delta) = 0$.\n\nTest suite:\n- Case $1$: $p = 0.1$, $\\delta = 20$.\n- Case $2$: $p = 0.4$, $\\delta = 50$.\n- Case $3$: $p = 0.6$, $\\delta = 50$.\n- Case $4$: $p = 0.9$, $\\delta = 100$.\n- Case $5$: $p = 0.0$, $\\delta = 10$.\n- Case $6$: $p = 1.0$, $\\delta = 10$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this outer list corresponds to one test case, ordered as above, and is itself a list of three floating-point numbers $[b_{\\mathcal{L}_1}, b_{\\text{Huber}}, b_{\\text{GC}}]$ for that case. For example, the output format must be\n$$\n[\\,[b_{\\mathcal{L}_1}^{(1)},b_{\\text{Huber}}^{(1)},b_{\\text{GC}}^{(1)}],\\,[b_{\\mathcal{L}_1}^{(2)},b_{\\text{Huber}}^{(2)},b_{\\text{GC}}^{(2)}],\\,\\dots,\\, [b_{\\mathcal{L}_1}^{(6)},b_{\\text{Huber}}^{(6)},b_{\\text{GC}}^{(6)}]\\,].\n$$\nNo physical units apply. Angles are not involved. Probabilities $p$ are given as decimals. The output values are floating-point numbers.", "solution": "The objective is to find the optimal constant predictor $\\hat{y}^*$ that minimizes the expected risk $R(\\hat{y})$. The risk is defined as:\n$$R(\\hat{y};p,\\delta,\\ell) = \\mathbb{E}[\\ell(y_{\\text{obs}} - \\hat{y})] = (1-p)\\,\\ell(-\\hat{y}) + p\\,\\ell(\\delta - \\hat{y})$$\nwhere $y_{\\text{obs}}$ takes the value $0$ with probability $1-p$ and $\\delta$ with probability $p$. The true label is $y=0$. The optimal predictor $\\hat{y}^*$ is a value in $\\mathbb{R}$ that minimizes $R(\\hat{y})$. For the considered loss functions, which are convex, this minimum can be found by setting the derivative of the risk with respect to $\\hat{y}$ to zero.\n\nThe derivative of the risk is obtained using the chain rule:\n$$\\frac{d R}{d \\hat{y}} = (1-p) \\frac{d}{d\\hat{y}}\\ell(-\\hat{y}) + p \\frac{d}{d\\hat{y}}\\ell(\\delta - \\hat{y})$$\n$$\\frac{d R}{d \\hat{y}} = (1-p) \\ell'(-\\hat{y}) \\cdot (-1) + p \\ell'(\\delta - \\hat{y}) \\cdot (-1)$$\n$$\\frac{d R}{d \\hat{y}} = - \\left[ (1-p) \\ell'(-\\hat{y}) + p \\ell'(\\delta - \\hat{y}) \\right]$$\nSetting this derivative to zero gives the optimality condition:\n$$(1-p) \\ell'(-\\hat{y}^*) = -p \\ell'(\\delta - \\hat{y}^*)$$\nWe will solve this equation for $\\hat{y}^*$ for each of the three specified loss functions. The robustness score is then calculated as the absolute bias $b_{\\ell}(p,\\delta) = |\\hat{y}^*_{\\ell}(p,\\delta)|$.\n\n**1. Absolute Error Loss ($\\ell_{\\mathcal{L}_1}(r) = |r|$)**\n\nThe derivative of the absolute error loss is the sign function, $\\ell'_{\\mathcal{L}_1}(r) = \\text{sgn}(r)$. The risk function is $R(\\hat{y}) = (1-p)|-\\hat{y}| + p|\\delta-\\hat{y}|$. The minimizer of this function is the weighted median of the data points $0$ and $\\delta$. A subgradient-based analysis confirms this. The subgradient of $R(\\hat{y})$ is:\n$$\\partial R(\\hat{y}) = (1-p)\\,\\text{sgn}(\\hat{y}) - p\\,\\text{sgn}(\\delta-\\hat{y})$$\nWe seek $\\hat{y}^*$ such that $0 \\in \\partial R(\\hat{y}^*)$. Assuming $0 < \\hat{y}^* < \\delta$, the derivative is $1-2p$.\n- If $p < 0.5$, then $1-2p > 0$. The risk is increasing on $(0, \\delta)$, so the minimum is at $\\hat{y}^*_{\\mathcal{L}_1} = 0$.\n- If $p > 0.5$, then $1-2p < 0$. The risk is decreasing on $(0, \\delta)$, so the minimum is at $\\hat{y}^*_{\\mathcal{L}_1} = \\delta$.\n- If $p = 0.5$, the risk is flat on $[0, \\delta]$, meaning any value in this interval is a minimizer. The problem specifies the tie-breaking rule $\\hat{y}^*_{\\mathcal{L}_1} = 0$.\nTherefore, the optimal predictor is:\n$$\\hat{y}^*_{\\mathcal{L}_1}(p,\\delta) = \\begin{cases} 0 & \\text{if } p \\le 0.5 \\\\ \\delta & \\text{if } p > 0.5 \\end{cases}$$\nThe robustness score is $b_{\\mathcal{L}_1}(p,\\delta) = |\\hat{y}^*_{\\mathcal{L}_1}(p,\\delta)|$.\n\n**2. Huber Loss ($\\ell_{\\text{Huber}}(r; \\kappa)$)**\n\nWith $\\kappa=10$, the derivative of the Huber loss is $\\ell'_{\\text{Huber}}(r;10) = \\text{clip}(r, -10, 10)$. The optimality condition is:\n$$(1-p)\\,\\text{clip}(-\\hat{y}^*, -10, 10) = -p\\,\\text{clip}(\\delta - \\hat{y}^*, -10, 10)$$\nWe analyze this piecewise. We assume $0 < \\hat{y}^* < \\delta$.\n- **Case 1**: Both arguments are in the quadratic region: $|\\hat{y}^*| \\le 10$ and $|\\delta-\\hat{y}^*| \\le 10$.\nThe condition becomes $(1-p)(-\\hat{y}^*) = -p(\\delta - \\hat{y}^*)$, which simplifies to $\\hat{y}^* = p\\delta$. This is valid if $p\\delta \\le 10$ and $\\delta(1-p) \\le 10$.\n\n- **Case 2**: Left term is linear, right is quadratic: $\\hat{y}^* > 10$ and $|\\delta-\\hat{y}^*| \\le 10$.\nThe condition becomes $(1-p)(-10) = -p(\\delta - \\hat{y}^*)$, which solves to $\\hat{y}^* = \\delta - \\frac{10(1-p)}{p}$. This case applies if $p > 0.5$ and the result is consistent with the assumptions, which holds if $p\\delta > 10$.\n\n- **Case 3**: Left term is quadratic, right is linear: $|\\hat{y}^*| \\le 10$ and $\\delta-\\hat{y}^* > 10$.\nThe condition becomes $(1-p)(-\\hat{y}^*) = -p(10)$, which solves to $\\hat{y}^* = \\frac{10p}{1-p}$. This case applies if $p < 0.5$ and $\\delta(1-p)>10$.\n\nCombining these cases gives the solution for $\\hat{y}^*_{\\text{Huber}}$:\n- If $p \\le 0.5$:\n  $$\\hat{y}^*_{\\text{Huber}} = \\begin{cases} p\\delta & \\text{if } \\delta(1-p) \\le 10 \\\\ \\frac{10p}{1-p} & \\text{if } \\delta(1-p) > 10 \\end{cases}$$\n- If $p > 0.5$:\n  $$\\hat{y}^*_{\\text{Huber}} = \\begin{cases} p\\delta & \\text{if } p\\delta \\le 10 \\\\ \\delta - \\frac{10(1-p)}{p} & \\text{if } p\\delta > 10 \\end{cases}$$\nFor the edge cases $p=0$ and $p=1$, this logic correctly yields $\\hat{y}^*=0$ and $\\hat{y}^*=\\delta$, respectively. The robustness score is $b_{\\text{Huber}}(p,\\delta) = |\\hat{y}^*_{\\text{Huber}}(p,\\delta)|$.\n\n**3. Generalized Charbonnier Loss ($\\ell_{\\text{GC}}(r; \\epsilon, \\beta)$)**\n\nWith $\\epsilon=1$ and $\\beta=0.45$, the loss is $\\ell_{\\text{GC}}(r) = (r^2+1)^{0.45}$. Its derivative is $\\ell'_{\\text{GC}}(r) = 2\\beta r (r^2+\\epsilon^2)^{\\beta-1} = 0.9 r (r^2+1)^{-0.55}$.\nThe optimality condition is $(1-p)\\ell'_{\\text{GC}}(-\\hat{y}^*) = -p\\ell'_{\\text{GC}}(\\delta - \\hat{y}^*)$. Substituting the derivative gives:\n$$(1-p) [0.9(-\\hat{y}^*)((-\\hat{y}^*)^2+1)^{-0.55}] = -p [0.9(\\delta-\\hat{y}^*)((\\delta-\\hat{y}^*)^2+1)^{-0.55}]$$\nSimplifying and rearranging terms, we get the equation for $\\hat{y}^*$:\n$$(1-p)\\hat{y}^*(\\hat{y}^{*2}+1)^{-0.55} - p(\\delta-\\hat{y}^*)((\\delta-\\hat{y}^*)^2+1)^{-0.55} = 0$$\nThis is a nonlinear equation that cannot be solved in closed form. We must find the root numerically. Let $g(\\hat{y})$ be the left-hand side of the equation.\n- For $p=0$, the equation becomes $\\hat{y}^*(\\hat{y}^{*2}+1)^{-0.55} = 0$, which implies $\\hat{y}^*_{\\text{GC}} = 0$.\n- For $p=1$, the equation becomes $-(\\delta-\\hat{y}^*)((\\delta-\\hat{y}^*)^2+1)^{-0.55} = 0$, which implies $\\hat{y}^*_{\\text{GC}} = \\delta$.\n- For $p \\in (0,1)$, we can observe that $g(0) < 0$ and $g(\\delta) > 0$. Since $g(\\hat{y})$ is continuous and monotonic between $0$ and $\\delta$, a unique root $\\hat{y}^*_{\\text{GC}} \\in (0, \\delta)$ exists. This root can be found using a numerical method like the bisection or Brent's method.\nThe robustness score is $b_{\\text{GC}}(p,\\delta) = |\\hat{y}^*_{\\text{GC}}(p,\\delta)|$.\n\nThe implementation will compute these three scores for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Computes robustness scores for L1, Huber, and Generalized Charbonnier losses\n    for a set of test cases.\n    \"\"\"\n    \n    # Loss function parameters\n    HUBER_KAPPA = 10.0\n    GC_EPSILON = 1.0\n    GC_BETA = 0.45\n\n    test_cases = [\n        (0.1, 20.0),  # Case 1\n        (0.4, 50.0),  # Case 2\n        (0.6, 50.0),  # Case 3\n        (0.9, 100.0), # Case 4\n        (0.0, 10.0),  # Case 5\n        (1.0, 10.0),  # Case 6\n    ]\n\n    def get_l1_bias(p, delta):\n        \"\"\"Computes the bias for the L1 loss.\"\"\"\n        if p > 0.5:\n            y_star = delta\n        else: # p <= 0.5, including tie-breaking at p = 0.5\n            y_star = 0.0\n        return abs(y_star)\n\n    def get_huber_bias(p, delta, kappa):\n        \"\"\"Computes the bias for the Huber loss.\"\"\"\n        if p <= 0.5:\n            if delta * (1.0 - p) <= kappa:\n                y_star = p * delta\n            else:\n                y_star = (p * kappa) / (1.0 - p)\n        else: # p > 0.5\n            if p * delta <= kappa:\n                y_star = p * delta\n            else:\n                y_star = delta - (kappa * (1.0 - p)) / p\n        return abs(y_star)\n\n    def get_gc_bias(p, delta, epsilon, beta):\n        \"\"\"Computes the bias for the Generalized Charbonnier loss.\"\"\"\n        if p == 0.0:\n            return 0.0\n        if p == 1.0:\n            return float(delta)\n\n        # Define the function whose root we need to find.\n        # This is derived from setting the derivative of the expected risk to zero.\n        def root_function(y_hat):\n            term1 = (1.0 - p) * y_hat * ((y_hat**2 + epsilon**2)**(beta - 1.0))\n            term2 = p * (delta - y_hat) * (((delta - y_hat)**2 + epsilon**2)**(beta - 1.0))\n            return term1 - term2\n\n        # Use Brent's method to find the root in the interval [0, delta].\n        # The function has opposite signs at the interval endpoints for p in (0, 1).\n        y_star = brentq(root_function, 0.0, delta)\n        return abs(y_star)\n\n    results = []\n    for p, delta in test_cases:\n        b_l1 = get_l1_bias(p, delta)\n        b_huber = get_huber_bias(p, delta, HUBER_KAPPA)\n        b_gc = get_gc_bias(p, delta, GC_EPSILON, GC_BETA)\n        \n        results.append([b_l1, b_huber, b_gc])\n\n    # Format the final output string as per requirements.\n    outer_parts = [f\"[{','.join(map(str, inner_list))}]\" for inner_list in results]\n    final_output = f\"[{','.join(outer_parts)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3178857"}, {"introduction": "A model's performance is not fixed after training; we can often enhance its predictions during the inference stage by leveraging data augmentation. This exercise [@problem_id:3178874] introduces you to Test-Time Augmentation (TTA), a powerful technique for improving generalization and robustness. You will apply augmentations like rotation and scaling to a model's input at test time and average the outputs, analyzing how this process can refine predictions and where it might introduce its own systematic biases.", "problem": "Consider a supervised learning regression setting with input vectors $\\mathbf{x} \\in \\mathbb{R}^2$ and the ground-truth target function defined by $y(\\mathbf{x}) = \\|\\mathbf{x}\\|_2^2$. A fixed predictor $f(\\mathbf{x})$ is given by a one-hidden-layer Rectified Linear Unit (ReLU) network (Rectified Linear Unit (ReLU) applies $\\max(0,z)$ elementwise) with weight matrix $W \\in \\mathbb{R}^{3 \\times 2}$, output weights $v \\in \\mathbb{R}^3$, and bias $b \\in \\mathbb{R}$:\n$$\nf(\\mathbf{x}) = v^\\top \\operatorname{ReLU}(W \\mathbf{x}) + b,\n$$\nwhere the numerical parameters are\n$$\nW = \\begin{bmatrix}\n1.0 & -0.5 \\\\\n-1.5 & 2.0 \\\\\n0.3 & 0.8\n\\end{bmatrix}, \\quad\nv = \\begin{bmatrix}\n1.0 \\\\ -0.7 \\\\ 0.5\n\\end{bmatrix}, \\quad\nb = 0.1.\n$$\nAt test-time, Test-Time Augmentation (TTA) is performed by averaging predictions over a set of input augmentations. Two types of augmentations are considered:\n- Rotations $\\mathcal{R}_\\theta$ by angle $\\theta$ (specified in radians), defined for $\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$ as\n$$\n\\mathcal{R}_\\theta(\\mathbf{x}) = \\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{bmatrix} \\mathbf{x}.\n$$\n- Scalings $\\mathcal{S}_s$ by factor $s$, defined as\n$$\n\\mathcal{S}_s(\\mathbf{x}) = s \\mathbf{x}.\n$$\nFor a finite augmentation set $\\{\\mathcal{T}_k\\}_{k=1}^K$, define the TTA prediction and the base prediction as:\n$$\n\\hat{y}_{\\text{tta}}(\\mathbf{x}) = \\frac{1}{K} \\sum_{k=1}^K f(\\mathcal{T}_k(\\mathbf{x})), \\qquad \\hat{y}_{\\text{base}}(\\mathbf{x}) = f(\\mathbf{x}).\n$$\nDefine the base bias and the TTA bias with respect to the ground-truth $y(\\mathbf{x})$ as:\n$$\nb_{\\text{base}}(\\mathbf{x}) = \\hat{y}_{\\text{base}}(\\mathbf{x}) - y(\\mathbf{x}), \\qquad b_{\\text{tta}}(\\mathbf{x}) = \\hat{y}_{\\text{tta}}(\\mathbf{x}) - y(\\mathbf{x}).\n$$\nThe augmentation-induced bias is the change in bias due to TTA:\n$$\n\\Delta b(\\mathbf{x}) = b_{\\text{tta}}(\\mathbf{x}) - b_{\\text{base}}(\\mathbf{x}).\n$$\nYour task is to implement the above definitions and evaluate $\\Delta b(\\mathbf{x})$ across the following test suite of inputs and augmentation sets. Angles must be interpreted in radians.\n\nTest suite (in the listed order):\n1. Input $\\mathbf{x}_1 = \\begin{bmatrix} 1.0 \\\\ -2.0 \\end{bmatrix}$ with rotations $\\theta \\in \\{0, \\pi/2, \\pi, 3\\pi/2\\}$.\n2. Input $\\mathbf{x}_2 = \\begin{bmatrix} 3.0 \\\\ 4.0 \\end{bmatrix}$ with rotations $\\theta \\in \\{0, \\pi/2, \\pi, 3\\pi/2\\}$.\n3. Input $\\mathbf{x}_2 = \\begin{bmatrix} 3.0 \\\\ 4.0 \\end{bmatrix}$ with scalings $s \\in \\{0.5, 1.0, 2.0\\}$.\n4. Input $\\mathbf{x}_3 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$ with scalings $s \\in \\{0.5, 1.0, 2.0\\}$.\n5. Input $\\mathbf{x}_4 = \\begin{bmatrix} 10^{-6} \\\\ -10^{-6} \\end{bmatrix}$ with scalings $s \\in \\{0.1, 10.0\\}$.\n6. Input $\\mathbf{x}_5 = \\begin{bmatrix} 10.0 \\\\ -0.5 \\end{bmatrix}$ with scalings $s \\in \\{0.5, 1.0, 2.0\\}$.\n\nScientific realism notes:\n- The target $y(\\mathbf{x}) = \\|\\mathbf{x}\\|_2^2$ is invariant to rotations but not invariant to scalings (except $s = \\pm 1$), ensuring coverage of both invariant and non-invariant augmentation regimes.\n- The predictor $f(\\mathbf{x})$ is not constrained to be invariant to any augmentation.\n\nYour program must:\n- Implement $f(\\mathbf{x})$, $y(\\mathbf{x})$, $\\mathcal{R}_\\theta$, and $\\mathcal{S}_s$ exactly as specified.\n- For each test case, compute $\\Delta b(\\mathbf{x})$.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as above, with each value rounded to six decimal places, for example $[\\delta_1,\\delta_2,\\dots]$ where $\\delta_i$ corresponds to $\\Delta b(\\mathbf{x}_i)$ for the $i$-th case.\n\nNo user input or external data is permitted; all constants must be hard-coded as specified. The angle unit for rotations must be radians. The outputs are dimensionless real numbers (floats).", "solution": "The user wants to compute the augmentation-induced bias, $\\Delta b(\\mathbf{x})$, for a simple neural network predictor under different Test-Time Augmentation (TTA) schemes. The problem is well-defined, scientifically grounded, and contains all necessary information for a unique solution.\n\nFirst, we will simplify the expression for the quantity of interest, $\\Delta b(\\mathbf{x})$. The definition provided is:\n$$\n\\Delta b(\\mathbf{x}) = b_{\\text{tta}}(\\mathbf{x}) - b_{\\text{base}}(\\mathbf{x})\n$$\nThe components, TTA bias $b_{\\text{tta}}(\\mathbf{x})$ and base bias $b_{\\text{base}}(\\mathbf{x})$, are defined with respect to the ground-truth function $y(\\mathbf{x})$:\n$$\nb_{\\text{tta}}(\\mathbf{x}) = \\hat{y}_{\\text{tta}}(\\mathbf{x}) - y(\\mathbf{x})\n$$\n$$\nb_{\\text{base}}(\\mathbf{x}) = \\hat{y}_{\\text{base}}(\\mathbf{x}) - y(\\mathbf{x})\n$$\nSubstituting these into the expression for $\\Delta b(\\mathbf{x})$ yields:\n$$\n\\Delta b(\\mathbf{x}) = (\\hat{y}_{\\text{tta}}(\\mathbf{x}) - y(\\mathbf{x})) - (\\hat{y}_{\\text{base}}(\\mathbf{x}) - y(\\mathbf{x}))\n$$\nThe ground-truth term $y(\\mathbf{x})$ cancels out, which simplifies the calculation significantly:\n$$\n\\Delta b(\\mathbf{x}) = \\hat{y}_{\\text{tta}}(\\mathbf{x}) - \\hat{y}_{\\text{base}}(\\mathbf{x})\n$$\nNow, we substitute the definitions for the TTA prediction $\\hat{y}_{\\text{tta}}(\\mathbf{x})$ and the base prediction $\\hat{y}_{\\text{base}}(\\mathbf{x})$:\n$$\n\\hat{y}_{\\text{tta}}(\\mathbf{x}) = \\frac{1}{K} \\sum_{k=1}^K f(\\mathcal{T}_k(\\mathbf{x}))\n$$\n$$\n\\hat{y}_{\\text{base}}(\\mathbf{x}) = f(\\mathbf{x})\n$$\nThis gives the final computable expression for the augmentation-induced bias:\n$$\n\\Delta b(\\mathbf{x}) = \\left(\\frac{1}{K} \\sum_{k=1}^K f(\\mathcal{T}_k(\\mathbf{x}))\\right) - f(\\mathbf{x})\n$$\nwhere $\\{\\mathcal{T}_k\\}_{k=1}^K$ is the set of $K$ augmentations. This expression shows that $\\Delta b(\\mathbf{x})$ is the difference between the average prediction over the augmented inputs and the prediction on the original input.\n\nThe core of the problem is to evaluate this expression for each of the specified test cases. This requires implementing the predictor function $f(\\mathbf{x})$ and the augmentation transformations $\\mathcal{R}_\\theta(\\mathbf{x})$ and $\\mathcal{S}_s(\\mathbf{x})$.\n\nThe predictor function is a one-hidden-layer neural network:\n$$\nf(\\mathbf{x}) = v^\\top \\operatorname{ReLU}(W \\mathbf{x}) + b\n$$\nwith parameters:\n$$\nW = \\begin{bmatrix}\n1.0 & -0.5 \\\\\n-1.5 & 2.0 \\\\\n0.3 & 0.8\n\\end{bmatrix}, \\quad\nv = \\begin{bmatrix}\n1.0 \\\\ -0.7 \\\\ 0.5\n\\end{bmatrix}, \\quad\nb = 0.1\n$$\nThe input $\\mathbf{x}$ is a $2$-dimensional vector, which we will treat as a column vector of shape $2 \\times 1$. The matrix $W$ has shape $3 \\times 2$, so the product $W\\mathbf{x}$ is a $3 \\times 1$ vector. The $\\operatorname{ReLU}$ function, $\\max(0,z)$, is applied element-wise. The vector $v$ has shape $3 \\times 1$, so its transpose $v^\\top$ has shape $1 \\times 3$. The dot product $v^\\top \\operatorname{ReLU}(W \\mathbf{x})$ results in a scalar, to which the scalar bias $b$ is added.\n\nThe augmentation transformations are:\n1.  Rotation by angle $\\theta$ (in radians):\n    $$\n    \\mathcal{R}_\\theta(\\mathbf{x}) = R_\\theta \\mathbf{x} = \\begin{bmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix} \\mathbf{x}\n    $$\n2.  Scaling by factor $s$:\n    $$\n    \\mathcal{S}_s(\\mathbf{x}) = s \\mathbf{x}\n    $$\n\nThe algorithm to solve the problem is as follows:\n1.  Define the numerical constants $W$, $v$, and $b$ as matrices and vectors.\n2.  Implement the predictor function $f(\\mathbf{x})$ based on the matrix-vector operations described above.\n3.  For each of the six test cases specified in the problem statement:\n    a. Identify the input vector $\\mathbf{x}$, the type of augmentation (rotation or scaling), and the set of augmentation parameters (angles $\\theta$ or scaling factors $s$).\n    b. Compute the base prediction, $\\hat{y}_{\\text{base}}(\\mathbf{x}) = f(\\mathbf{x})$.\n    c. Initialize a sum for the augmented predictions to zero.\n    d. Iterate through each augmentation parameter in the set:\n        i. Apply the corresponding transformation $\\mathcal{T}_k$ to the input vector $\\mathbf{x}$ to get the augmented vector $\\mathbf{x}'_k = \\mathcal{T}_k(\\mathbf{x})$.\n        ii. Compute the prediction on the augmented vector, $f(\\mathbf{x}'_k)$.\n        iii. Add this prediction to the running sum.\n    e. Divide the sum by the number of augmentations, $K$, to get the TTA prediction, $\\hat{y}_{\\text{tta}}(\\mathbf{x})$.\n    f. Calculate the augmentation-induced bias $\\Delta b(\\mathbf{x}) = \\hat{y}_{\\text{tta}}(\\mathbf{x}) - \\hat{y}_{\\text{base}}(\\mathbf{x})$.\n    g. Store this result.\n4.  After processing all test cases, format the list of results as a comma-separated string, with each value rounded to six decimal places, and enclose the string in square brackets.\n\nThis procedure will be implemented in Python using the `numpy` library for efficient numerical computations.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Test-Time Augmentation problem by calculating the\n    augmentation-induced bias for a series of test cases.\n    \"\"\"\n    # Define the fixed parameters of the neural network predictor.\n    W = np.array([\n        [1.0, -0.5],\n        [-1.5, 2.0],\n        [0.3, 0.8]\n    ])\n    v = np.array([[1.0], [-0.7], [0.5]])\n    b = 0.1\n\n    def f(x_vec):\n        \"\"\"\n        Computes the prediction of the one-hidden-layer ReLU network.\n        f(x) = v^T * ReLU(W*x) + b\n        \n        Args:\n            x_vec (np.ndarray): Input vector of shape (2, 1).\n        \n        Returns:\n            float: The scalar output of the network.\n        \"\"\"\n        # Ensure x is a column vector\n        if x_vec.shape != (2, 1):\n            x_vec = x_vec.reshape(2, 1)\n        \n        z = W @ x_vec\n        h = np.maximum(0, z)\n        output = v.T @ h + b\n        return output.item()\n\n    # Define the test suite. Each case is a dictionary specifying the input\n    # vector, augmentation type, and augmentation parameters.\n    test_cases = [\n        {'x': np.array([1.0, -2.0]), 'type': 'rotation', 'params': [0, np.pi/2, np.pi, 3*np.pi/2]},\n        {'x': np.array([3.0, 4.0]), 'type': 'rotation', 'params': [0, np.pi/2, np.pi, 3*np.pi/2]},\n        {'x': np.array([3.0, 4.0]), 'type': 'scaling', 'params': [0.5, 1.0, 2.0]},\n        {'x': np.array([0.0, 0.0]), 'type': 'scaling', 'params': [0.5, 1.0, 2.0]},\n        {'x': np.array([1e-6, -1e-6]), 'type': 'scaling', 'params': [0.1, 10.0]},\n        {'x': np.array([10.0, -0.5]), 'type': 'scaling', 'params': [0.5, 1.0, 2.0]},\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        x_original = case['x'].reshape(2, 1)\n        aug_type = case['type']\n        aug_params = case['params']\n        K = len(aug_params)\n\n        # Calculate the base prediction on the original input\n        y_base = f(x_original)\n\n        # Calculate the TTA prediction by averaging over augmentations\n        y_tta_sum = 0.0\n        for param in aug_params:\n            if aug_type == 'rotation':\n                theta = param\n                rot_matrix = np.array([\n                    [np.cos(theta), -np.sin(theta)],\n                    [np.sin(theta), np.cos(theta)]\n                ])\n                x_augmented = rot_matrix @ x_original\n            elif aug_type == 'scaling':\n                s = param\n                x_augmented = s * x_original\n            \n            y_tta_sum += f(x_augmented)\n            \n        y_tta = y_tta_sum / K\n\n        # Calculate the augmentation-induced bias\n        delta_b = y_tta - y_base\n        results.append(delta_b)\n\n    # Format the final output string as required.\n    formatted_results = [\"{:.6f}\".format(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3178874"}]}