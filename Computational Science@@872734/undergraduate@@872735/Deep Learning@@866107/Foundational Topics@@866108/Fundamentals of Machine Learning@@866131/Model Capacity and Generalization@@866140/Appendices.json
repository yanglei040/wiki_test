{"hands_on_practices": [{"introduction": "Understanding the relationship between model capacity and performance is a cornerstone of machine learning. A model that is too simple may fail to capture the underlying patterns in the data (high bias), while a model that is too complex might fit the training data's noise instead of its signal (high variance). This exercise provides a hands-on exploration of this fundamental bias-variance trade-off by analyzing polynomial regression models of varying complexity, challenging you to identify the optimal model degree that balances underfitting and overfitting [@problem_id:3107026].", "problem": "You are given observations from polynomial regression models of degree $d$ fitted to the same dataset. For each test case, you are provided, for a set of degrees $d \\in \\mathbb{N}$, the empirical training Mean Squared Error (Mean Squared Error (MSE)) $E_{\\text{train}}(d)$ and the empirical cross-validation MSE $E_{\\text{val}}(d)$. Your task is to predict overfitting risk by regressing the generalization gap $g(d)$ versus $d$, and to classify the optimal degree that minimizes an estimate of the expected generalization error.\n\nFundamental base and definitions:\n- The generalization error at complexity $d$ can be expressed as $E_{\\text{gen}}(d) = E_{\\text{train}}(d) + g(d)$, where $g(d)$ is the generalization gap defined by $g(d) = E_{\\text{test}}(d) - E_{\\text{train}}(d)$. In practice, $E_{\\text{test}}(d)$ is unknown and cross-validation is used as a well-tested proxy, so we estimate the gap as $g(d) \\approx E_{\\text{val}}(d) - E_{\\text{train}}(d)$.\n- Overfitting risk increasing with model complexity is reflected by a positive slope of $g(d)$ as a function of $d$.\n\nAlgorithmic requirement:\n- For each test case, compute $g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$ at each observed degree $d_k$.\n- Fit a straight line $g(d) \\approx a + b\\,d$ to the observed pairs $\\{(d_k, g(d_k))\\}$ using Ordinary Least Squares (Ordinary Least Squares (OLS)).\n- Form the estimated expected generalization error at each observed degree as $\\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}\\,d_k$.\n- Classify the optimal degree $\\widehat{d}^{\\star}$ as the integer degree from the provided set that minimizes $\\widehat{E}_{\\text{gen}}(d_k)$. If multiple degrees attain the same minimum within numerical tolerance, choose the smallest such degree (prefer lower complexity).\n- Report two quantities per test case: the estimated slope $\\widehat{b}$ of the gap-versus-degree regression, rounded to $4$ decimal places, and the classified optimal degree $\\widehat{d}^{\\star}$ as an integer.\n\nTest suite:\n- Test case $1$: $d = [1,2,3,4,5]$, $E_{\\text{train}} = [9.0,6.0,4.5,4.0,3.8]$, $E_{\\text{val}} = [10.0,7.0,5.0,5.2,6.0]$.\n- Test case $2$: $d = [1,2,3,4]$, $E_{\\text{train}} = [5.0,4.2,3.6,3.3]$, $E_{\\text{val}} = [5.0,4.2,3.6,3.3]$.\n- Test case $3$: $d = [1,2,3,4,5,6]$, $E_{\\text{train}} = [12.0,8.0,6.0,5.2,5.0,4.9]$, $E_{\\text{val}} = [11.5,7.3,5.1,4.1,3.7,3.4]$.\n- Test case $4$: $d = [1,10]$, $E_{\\text{train}} = [5.0,0.1]$, $E_{\\text{val}} = [5.1,6.0]$.\n- Test case $5$: $d = [2,3,4]$, $E_{\\text{train}} = [8.0,6.0,5.0]$, $E_{\\text{val}} = [8.7,6.6,5.4]$.\n\nOutput specification:\n- For each test case, output a two-element list $[\\widehat{b}, \\widehat{d}^{\\star}]$, where $\\widehat{b}$ is the slope rounded to $4$ decimal places, and $\\widehat{d}^{\\star}$ is an integer.\n- Your program should produce a single line of output containing one list that aggregates the per-test-case outputs in order, with no spaces. Each element must itself be a two-element list as described. The final printout must be exactly one line.\n\nNo physical units are involved. All angles, if any, are irrelevant here. No percentages are required.\n\nYour program must be a complete, runnable program that defines the test cases above internally and produces the specified single-line output.", "solution": "### Step 1: Extract Givens\n- **Models**: Polynomial regression models of degree $d \\in \\mathbb{N}$.\n- **Data per test case**: A set of degrees $d_k$, corresponding training Mean Squared Error (MSE) $E_{\\text{train}}(d_k)$, and cross-validation MSE $E_{\\text{val}}(d_k)$.\n- **Definitions**:\n    - Generalization error: $E_{\\text{gen}}(d) = E_{\\text{train}}(d) + g(d)$.\n    - Generalization gap: $g(d) = E_{\\text{test}}(d) - E_{\\text{train}}(d)$.\n    - Empirical generalization gap estimate: $g(d) \\approx E_{\\text{val}}(d) - E_{\\text{train}}(d)$.\n- **Algorithmic Requirements**:\n    1.  Compute the empirical gap for each degree: $g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$.\n    2.  Fit a straight line $g(d) \\approx a + b\\,d$ to the data points $\\{(d_k, g(d_k))\\}$ using Ordinary Least Squares (OLS) to find the estimated intercept $\\widehat{a}$ and slope $\\widehat{b}$.\n    3.  Estimate the expected generalization error at each degree: $\\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}\\,d_k$.\n    4.  Classify the optimal degree $\\widehat{d}^{\\star}$ as the integer degree from the provided set that minimizes $\\widehat{E}_{\\text{gen}}(d_k)$. The tie-breaking rule is to choose the smallest degree if multiple degrees yield the same minimum.\n    5.  Report the slope $\\widehat{b}$ rounded to $4$ decimal places and the integer optimal degree $\\widehat{d}^{\\star}$.\n- **Test Suite**:\n    - **Test case 1**: $d = [1,2,3,4,5]$, $E_{\\text{train}} = [9.0,6.0,4.5,4.0,3.8]$, $E_{\\text{val}} = [10.0,7.0,5.0,5.2,6.0]$.\n    - **Test case 2**: $d = [1,2,3,4]$, $E_{\\text{train}} = [5.0,4.2,3.6,3.3]$, $E_{\\text{val}} = [5.0,4.2,3.6,3.3]$.\n    - **Test case 3**: $d = [1,2,3,4,5,6]$, $E_{\\text{train}} = [12.0,8.0,6.0,5.2,5.0,4.9]$, $E_{\\text{val}} = [11.5,7.3,5.1,4.1,3.7,3.4]$.\n    - **Test case 4**: $d = [1,10]$, $E_{\\text{train}} = [5.0,0.1]$, $E_{\\text{val}} = [5.1,6.0]$.\n    - **Test case 5**: $d = [2,3,4]$, $E_{\\text{train}} = [8.0,6.0,5.0]$, $E_{\\text{val}} = [8.7,6.6,5.4]$.\n- **Output Specification**: A single-line list of two-element lists $[\\widehat{b}, \\widehat{d}^{\\star}]$ for each test case, with no spaces in the final output string.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is based on fundamental concepts in statistical learning theory, including the bias-variance trade-off, overfitting, training error, validation error, and generalization error. The use of Ordinary Least Squares to model the relationship between model complexity and the generalization gap is a standard and sound analytical technique.\n- **Well-Posed**: The problem is well-posed. For each test case, the number of distinct data points for the OLS regression is at least $2$, ensuring a unique solution for the line parameters. The search for the optimal degree is a minimization over a finite set, which is guaranteed to have a solution. The tie-breaking rule ensures this solution is unique.\n- **Objective**: The problem is stated using precise mathematical definitions and a clear, unambiguous algorithmic procedure. The inputs are numerical, and the required outputs are well-defined.\n- **Flaw Checklist**: The problem does not violate any of the specified flaws. It is scientifically sound, formalizable, complete, realistic, and well-posed. It requires non-trivial computation and reasoning.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe problem requires an analysis of model performance as a function of complexity, specifically the degree $d$ of a polynomial regression model. The core of the analysis lies in understanding and modeling the generalization gap, $g(d)$, which represents the difference between a model's performance on unseen data (approximated by validation error, $E_{\\text{val}}$) and its performance on the training data ($E_{\\text{train}}$). A growing gap with increasing complexity is a hallmark of overfitting.\n\nThe specified algorithm aims to create a smoothed estimate of the generalization error to make a more robust model selection decision than simply picking the model with the lowest validation error. The validation error itself can be noisy, and modeling the trend of the generalization gap can help filter out this noise.\n\nThe procedure for each test case is as follows:\n\n1.  **Compute the Empirical Generalization Gap**: For each given polynomial degree $d_k$, we calculate the empirical generalization gap, $g(d_k)$, using the provided training and validation errors:\n    $$g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$$\n\n2.  **Model the Generalization Gap via OLS**: We hypothesize a linear relationship between the model complexity $d$ and the generalization gap $g(d)$. We fit a line, $\\widehat{g}(d) = \\widehat{a} + \\widehat{b}d$, to the set of observed points $\\{(d_k, g(d_k))\\}$ using Ordinary Least Squares (OLS). The slope $\\widehat{b}$ and intercept $\\widehat{a}$ are chosen to minimize the sum of squared differences $\\sum_k (g(d_k) - (\\widehat{a} + \\widehat{b}d_k))^2$. For a set of $n$ points, the OLS estimators are given by:\n    $$ \\widehat{b} = \\frac{\\sum_{k=1}^{n} (d_k - \\bar{d})(g_k - \\bar{g})}{\\sum_{k=1}^{n} (d_k - \\bar{d})^2} $$\n    $$ \\widehat{a} = \\bar{g} - \\widehat{b}\\bar{d} $$\n    where $\\bar{d} = \\frac{1}{n}\\sum_k d_k$ and $\\bar{g} = \\frac{1}{n}\\sum_k g(d_k)$ are the sample means. The slope $\\widehat{b}$ serves as a direct measure of overfitting risk; a positive $\\widehat{b}$ indicates that the gap between training and validation performance widens as model complexity increases.\n\n3.  **Estimate the Generalization Error**: Using the linear model for the gap, we construct a smoothed estimate of the true generalization error, $\\widehat{E}_{\\text{gen}}(d_k)$:\n    $$ \\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{g}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}d_k $$\n    This estimate combines the directly observed training error, which reflects how well the model fits the data, with a regularized estimate of the generalization penalty, which accounts for complexity.\n\n4.  **Determine the Optimal Degree**: The optimal degree, $\\widehat{d}^{\\star}$, is selected as the degree from the given set $\\{d_k\\}$ that minimizes our estimated generalization error, $\\widehat{E}_{\\text{gen}}(d_k)$.\n    $$ \\widehat{d}^{\\star} = \\underset{d_k}{\\arg\\min} \\{ \\widehat{E}_{\\text{gen}}(d_k) \\} $$\n    The problem specifies that if a tie occurs, the smallest degree among those that achieve the minimum error should be chosen. This reflects the principle of parsimony (Occam's razor): when all else is equal, prefer the simpler model.\n\nFinally, for each test case, we report the calculated slope $\\widehat{b}$ (a measure of overfitting risk) and the determined optimal degree $\\widehat{d}^{\\star}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {'d': [1, 2, 3, 4, 5], \n         'E_train': [9.0, 6.0, 4.5, 4.0, 3.8], \n         'E_val': [10.0, 7.0, 5.0, 5.2, 6.0]},\n        # Test case 2\n        {'d': [1, 2, 3, 4], \n         'E_train': [5.0, 4.2, 3.6, 3.3], \n         'E_val': [5.0, 4.2, 3.6, 3.3]},\n        # Test case 3\n        {'d': [1, 2, 3, 4, 5, 6], \n         'E_train': [12.0, 8.0, 6.0, 5.2, 5.0, 4.9], \n         'E_val': [11.5, 7.3, 5.1, 4.1, 3.7, 3.4]},\n        # Test case 4\n        {'d': [1, 10], \n         'E_train': [5.0, 0.1], \n         'E_val': [5.1, 6.0]},\n        # Test case 5\n        {'d': [2, 3, 4], \n         'E_train': [8.0, 6.0, 5.0], \n         'E_val': [8.7, 6.6, 5.4]},\n    ]\n\n    results_as_strings = []\n    \n    for case in test_cases:\n        # Extract and convert data to numpy arrays for vectorized operations\n        d = np.array(case['d'], dtype=float)\n        E_train = np.array(case['E_train'], dtype=float)\n        E_val = np.array(case['E_val'], dtype=float)\n        \n        # Step 1: Compute the empirical generalization gap\n        g = E_val - E_train\n        \n        # Step 2: Fit a straight line g(d) = a + b*d using OLS.\n        # np.polyfit with degree 1 returns the coefficients [b, a] for slope and intercept.\n        b_hat, a_hat = np.polyfit(d, g, 1)\n        \n        # Step 3: Form the estimated expected generalization error\n        # E_gen_hat(d) = E_train(d) + (a_hat + b_hat*d)\n        E_gen_hat = E_train + a_hat + b_hat * d\n        \n        # Step 4: Classify the optimal degree d_star\n        # np.argmin finds the index of the first occurrence of the minimum value.\n        # Since the input degrees 'd' are sorted, this satisfies the tie-breaking rule\n        # of choosing the smallest degree.\n        min_error_index = np.argmin(E_gen_hat)\n        d_star = int(d[min_error_index])\n        \n        # Format the result for this case as a string \"[b,d_star]\" with no spaces\n        # and b rounded to 4 decimal places.\n        case_result_str = f\"[{b_hat:.4f},{d_star}]\"\n        results_as_strings.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    # The output is a single list containing the results for all test cases.\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```", "id": "3107026"}, {"introduction": "Classical wisdom suggests that test error follows a U-shaped curve as model capacity increases, but modern deep learning has revealed a more surprising picture. This practice guides you through a simulation of the \"double descent\" phenomenon, where test error, after peaking at the point of data interpolation, can decrease again as the model becomes even more overparameterized. By varying the bottleneck size of a linear autoencoder, you will quantitatively map out this intriguing behavior and see how model capacity's relationship with generalization extends beyond the classical view [@problem_id:3183618].", "problem": "You will implement a simulation to study the double descent phenomenon in a linear autoencoder by varying the bottleneck size. The goal is to connect training reconstruction error and test-time generalization to how model capacity crosses the interpolation threshold and approaches the identity mapping. Use a purely mathematical setup grounded in Empirical Risk Minimization (ERM), Principal Component Analysis (PCA), and orthogonal projection properties, and design a program that produces quantifiable outputs for multiple test cases.\n\nConsider a linear autoencoder with tied weights, where the input is a vector $x \\in \\mathbb{R}^d$, the encoder is $z = W^\\top x$ with $W \\in \\mathbb{R}^{d \\times m}$, and the decoder reconstructs $\\hat{x} = W z = W W^\\top x$. Let the training data matrix be $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$ and the test data matrix be $X_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}} \\times d}$. Center the training data by its empirical mean $\\mu_{\\text{train}} \\in \\mathbb{R}^d$, and center the test data by the same $\\mu_{\\text{train}}$ to evaluate generalization for a fixed estimator. For a given bottleneck size $m$ with $1 \\le m \\le d$, define the reconstruction matrix $P_m = W_m W_m^\\top$, where $W_m$ has orthonormal columns spanning an $m$-dimensional subspace of $\\mathbb{R}^d$. The training reconstruction error at bottleneck $m$ is\n$$\nE_{\\text{train}}(m) = \\frac{1}{n} \\sum_{i=1}^n \\left\\| x_i - P_m x_i \\right\\|_2^2,\n$$\nand the test reconstruction error is\n$$\nE_{\\text{test}}(m) = \\frac{1}{n_{\\text{test}}} \\sum_{j=1}^{n_{\\text{test}}} \\left\\| x^{\\text{test}}_j - P_m x^{\\text{test}}_j \\right\\|_2^2,\n$$\nwhere all $x_i$ and $x^{\\text{test}}_j$ are centered by $\\mu_{\\text{train}}$.\n\nFundamental base and modeling assumptions:\n- Empirical Risk Minimization (ERM): The estimator minimizes the average training reconstruction error $E_{\\text{train}}(m)$ over $W$ subject to orthonormal columns.\n- Principal Component Analysis (PCA): The ERM solution for the linear autoencoder with squared error and tied weights yields $W_m$ whose columns are the top $m$ right singular vectors of the centered training matrix (equivalently, the top $m$ eigenvectors of the empirical covariance), thus $P_m$ is the orthogonal projector onto that $m$-dimensional subspace.\n- Orthogonal projections: For any $m$, $P_m$ is an idempotent symmetric matrix with rank $m$, and $\\left\\| x - P_m x \\right\\|_2^2$ is the squared distance from $x$ to the subspace spanned by $W_m$.\n\nCapacity crossing and identity mapping:\n- Interpolation threshold: Let $r = \\operatorname{rank}(X_{\\text{train}})$. When $m \\ge r$, there exist solutions with $E_{\\text{train}}(m) = 0$ because the projector can span the training data subspace. The smallest such $m$ is denoted $m_{\\text{interp}}$.\n- Identity mapping: When $m = d$ and $W_d$ has $d$ orthonormal columns, $P_d = I_d$, which yields $E_{\\text{train}}(d) = 0$ and $E_{\\text{test}}(d) = 0$, connecting capacity to the identity mapping.\n\nDouble descent investigation:\n- As $m$ increases from $1$ to $d$, the test error $E_{\\text{test}}(m)$ may exhibit an initial descent (adding signal-aligned directions), then an ascent near $m \\approx r$ (overfitting to the training subspace), followed by a second descent as $m \\to d$ (approaching the identity mapping). This shape is referred to as double descent.\n\nImplementation requirements:\n- Construct $X_{\\text{train}}$ and $X_{\\text{test}}$ by sampling independently from a zero-mean multivariate normal distribution $\\mathcal{N}(0, \\Sigma)$ in $\\mathbb{R}^d$, where $\\Sigma$ is built by selecting a random orthonormal basis and placing specified eigenvalues on the diagonal. Let $Q \\in \\mathbb{R}^{d \\times d}$ be orthonormal, and set $\\Sigma = Q \\operatorname{diag}(\\lambda_1,\\ldots,\\lambda_d) Q^\\top$ with user-specified $\\lambda_i$ values that reflect a few large \"signal\" variances and smaller \"noise\" variances.\n- Use the training mean $\\mu_{\\text{train}}$ to center both $X_{\\text{train}}$ and $X_{\\text{test}}$.\n- Compute the Singular Value Decomposition (SVD) of the centered training matrix to obtain an orthonormal basis $V \\in \\mathbb{R}^{d \\times d}$ of right singular vectors. For each $m$, set $W_m$ to be the first $m$ columns of $V$, and $P_m = W_m W_m^\\top$.\n- For each $m \\in \\{1, 2, \\ldots, d\\}$, compute $E_{\\text{train}}(m)$ and $E_{\\text{test}}(m)$.\n- Let $m_{\\text{peak}}$ be the index $m$ that maximizes $E_{\\text{test}}(m)$ (use the smallest such $m$ in case of ties).\n- Let $m_{\\text{interp}}$ be the smallest $m$ such that $E_{\\text{train}}(m) \\le \\tau$, with tolerance $\\tau = 10^{-12}$.\n- Define a boolean $b_{\\text{dd}}$ indicating whether double descent is observed, using the following criterion:\n    - Let $k = m_{\\text{peak}}$, and consider the sequences before and after the peak. Let $E_{\\text{pre}} = \\{E_{\\text{test}}(1), \\ldots, E_{\\text{test}}(k-1)\\}$ and $E_{\\text{post}} = \\{E_{\\text{test}}(k+1), \\ldots, E_{\\text{test}}(d)\\}$ (empty sequences imply $b_{\\text{dd}} = \\text{False}$).\n    - Double descent is declared if both $\\min(E_{\\text{pre}})  E_{\\text{test}}(k)$ and $\\min(E_{\\text{post}})  E_{\\text{test}}(k)$, and there is at least one strict decrease somewhere in $E_{\\text{pre}}$.\n- For each test case, output the list $[m_{\\text{peak}}, \\text{round}(E_{\\text{test}}(m_{\\text{peak}}), 6), m_{\\text{interp}}, b_{\\text{dd}}]$.\n\nTest suite:\nFor each case, parameters are $(d, n, n_{\\text{test}}, s, \\sigma_{\\text{signal}}^2, \\sigma_{\\text{noise}}^2, \\text{seed})$ where $d$ is the dimensionality, $n$ is the number of training samples, $n_{\\text{test}}$ is the number of test samples, $s$ is the number of signal dimensions, $\\sigma_{\\text{signal}}^2$ is the signal variance, $\\sigma_{\\text{noise}}^2$ is the noise variance, and $\\text{seed}$ sets the random number generator. Construct eigenvalues by setting the first $s$ to $\\sigma_{\\text{signal}}^2$ and the remaining $d - s$ to $\\sigma_{\\text{noise}}^2$.\n- Case $1$: $(d, n, n_{\\text{test}}, s, \\sigma_{\\text{signal}}^2, \\sigma_{\\text{noise}}^2, \\text{seed}) = (\\,60,\\,40,\\,2000,\\,12,\\,5.0,\\,0.5,\\,1\\,)$.\n- Case $2$: $(\\,60,\\,58,\\,2000,\\,12,\\,4.0,\\,0.6,\\,2\\,)$.\n- Case $3$: $(\\,30,\\,15,\\,3000,\\,0,\\,0.0,\\,1.0,\\,3\\,)$.\n- Case $4$: $(\\,20,\\,5,\\,5000,\\,8,\\,3.0,\\,0.3,\\,4\\,)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the form $[m_{\\text{peak}}, \\text{round}(E_{\\text{test}}(m_{\\text{peak}}), 6), m_{\\text{interp}}, b_{\\text{dd}}]$. Floats must be rounded to $6$ decimal places. For example, the output should look like $[[1,0.123456,1,True],[\\ldots],[\\ldots],[\\ldots]]$.", "solution": "The user's request is to validate and solve a computational problem involving the simulation of the double descent phenomenon in a linear autoencoder.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Model**: Linear autoencoder with input $x \\in \\mathbb{R}^d$, encoder $z = W^\\top x$, decoder $\\hat{x} = W W^\\top x$, where $W \\in \\mathbb{R}^{d \\times m}$ has orthonormal columns. The reconstruction matrix is $P_m = W W^\\top$.\n-   **Data**: Training set $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$, test set $X_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}} \\times d}$. Data is sampled from $\\mathcal{N}(0, \\Sigma)$.\n-   **Covariance**: $\\Sigma = Q \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_d) Q^\\top$, where $Q$ is a random orthonormal matrix. The first $s$ eigenvalues $\\{\\lambda_i\\}$ are $\\sigma_{\\text{signal}}^2$, and the remaining $d-s$ are $\\sigma_{\\text{noise}}^2$.\n-   **Centering**: Both $X_{\\text{train}}$ and $X_{\\text{test}}$ are centered using the empirical mean of the training data, $\\mu_{\\text{train}}$.\n-   **Optimization**: The model parameters $W_m$ are chosen to minimize the training reconstruction error (Empirical Risk Minimization), which is equivalent to Principal Component Analysis (PCA). The columns of $W_m$ are the top $m$ right singular vectors of the centered training matrix.\n-   **Error Metrics**:\n    -   Training error: $E_{\\text{train}}(m) = \\frac{1}{n} \\sum_{i=1}^n \\left\\| x_i - P_m x_i \\right\\|_2^2$\n    -   Test error: $E_{\\text{test}}(m) = \\frac{1}{n_{\\text{test}}} \\sum_{j=1}^{n_{\\text{test}}} \\left\\| x^{\\text{test}}_j - P_m x^{\\text{test}}_j \\right\\|_2^2$\n-   **Quantities to Compute**:\n    -   $m_{\\text{peak}}$: The smallest bottleneck size $m$ that maximizes $E_{\\text{test}}(m)$.\n    -   $m_{\\text{interp}}$: The smallest $m$ for which $E_{\\text{train}}(m) \\le \\tau$, with $\\tau = 10^{-12}$.\n    -   $b_{\\text{dd}}$: A boolean indicating if double descent is observed, based on the shape of the $E_{\\text{test}}(m)$ curve around $m_{\\text{peak}}$.\n-   **Test Cases**: Four specific parameter sets for $(d, n, n_{\\text{test}}, s, \\sigma_{\\text{signal}}^2, \\sigma_{\\text{noise}}^2, \\text{seed})$ are provided.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the validation criteria:\n\n-   **Scientifically Grounded**: The problem is based on well-established principles. The equivalence of ERM for a linear autoencoder and PCA is a standard result in machine learning, rooted in the Eckart-Young-Mirsky theorem. The simulation of double descent by varying model capacity ($m$) is a standard technique used in contemporary research to understand generalization in overparameterized models. The data generation process is a conventional method for creating structured synthetic data.\n-   **Well-Posed**: The problem is clearly defined with no ambiguity. All parameters, methodologies for data generation and analysis (SVD), and definitions for the output metrics ($m_{\\text{peak}}$, $m_{\\text{interp}}$, $b_{\\text{dd}}$) are specified precisely. This ensures that a unique and stable solution can be computed for each test case given the random seed.\n-   **Objective**: The language is formal and objective. The criteria for analysis are quantitative and free of subjective interpretation.\n-   **Completeness and Consistency**: The problem provides all necessary information (hyperparameters, seeds, tolerances) and contains no internal contradictions.\n-   **Feasibility**: The specified dimensions ($d \\le 60, n \\le 58$) are computationally manageable, making the required SVD and matrix operations feasible on standard hardware.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. It is a well-posed, scientifically sound, and computationally feasible task that directly addresses a relevant topic in modern machine learning theory. The solution will proceed as requested.\n\n### Solution\n\nThe simulation will be implemented by adhering to the theoretical principles and computational steps outlined in the problem statement. The core of the analysis is to relate the model's capacity, controlled by the bottleneck dimension $m$, to its performance on training and test data.\n\n1.  **Theoretical Framework**: The problem specifies a linear autoencoder with tied weights, where the decoder weight matrix is the transpose of the encoder's. The reconstruction is given by $\\hat{x} = W_m W_m^\\top x$. The matrix $P_m = W_m W_m^\\top$ is an orthogonal projector onto the $m$-dimensional subspace spanned by the columns of $W_m$. The principle of Empirical Risk Minimization (ERM) dictates that we must choose $W_m$ to minimize the average training reconstruction error, $E_{\\text{train}}(m)$. For the squared $\\ell_2$-norm loss, this optimization problem is equivalent to Principal Component Analysis (PCA). The optimal subspace is the one spanned by the first $m$ principal components of the centered training data. These components are the right singular vectors of the centered training data matrix, $X_c = X_{\\text{train}} - \\mu_{\\text{train}}$.\n\n2.  **Simulation Algorithm**:\n    -   **Data Generation**: For each test case, we first establish a random number generator with the given seed. We construct the $d \\times d$ covariance matrix $\\Sigma = Q \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_d) Q^\\top$. The eigenvalues $\\lambda_i$ are set to create a distinction between \"signal\" and \"noise\" dimensions, and $Q$ is a random orthonormal matrix generated using routines from `scipy.stats`. We then sample $n$ training vectors and $n_{\\text{test}}$ test vectors from the multivariate normal distribution $\\mathcal{N}(0, \\Sigma)$.\n    -   **Data Centering**: The training data is centered by subtracting its empirical mean, $\\mu_{\\text{train}} = \\frac{1}{n} \\sum_{i=1}^n x_i$, to yield $X_c$. Crucially, the test data is also centered using this same $\\mu_{\\text{train}}$ to evaluate how the learned model generalizes to unseen data from the same distribution.\n    -   **Principal Component Basis**: We compute the Singular Value Decomposition (SVD) of the centered training matrix: $X_c = U S V^\\top$. The columns of $V$ (or rows of $V^\\top$) form an orthonormal basis of principal components for the training data, ordered by the corresponding singular values in $S$.\n    -   **Error Curve Computation**: We iterate through the model capacity $m$ from $1$ to $d$. In each step:\n        -   The projector $P_m$ is formed from the first $m$ principal vectors: $W_m = V_{:, 1:m}$, $P_m = W_m W_m^\\top$.\n        -   The test reconstruction error, $E_{\\text{test}}(m)$, is calculated directly using its definition. This can be computed efficiently as $E_{\\text{test}}(m) = \\frac{1}{n_{\\text{test}}} \\|X_{test, c} (I_d - P_m)\\|_F^2$, where $\\|\\cdot\\|_F$ is the Frobenius norm.\n        -   The training reconstruction error, $E_{\\text{train}}(m)$, can be computed even more efficiently. Due to the properties of SVD and PCA, the total reconstruction error on the training set using the top $m$ components is the sum of the squares of the remaining singular values. That is, $E_{\\text{train}}(m) = \\frac{1}{n} \\sum_{k=m+1}^{\\text{rank}} s_k^2$, where $s_k$ are the singular values of $X_c$.\n\n3.  **Analysis and Metric Extraction**: After computing the full error curves $E_{\\text{train}}(m)$ and $E_{\\text{test}}(m)$ for $m=1, \\ldots, d$:\n    -   $m_{\\text{peak}}$ is identified as the smallest $m$ that maximizes the test error curve $E_{\\text{test}}$. This point often signals the onset of overfitting, where the model capacity is just enough to fit the training data but captures spurious correlations that hurt generalization.\n    -   $m_{\\text{interp}}$ is found by locating the first $m$ at which the training error $E_{\\text{train}}(m)$ drops below a numerical tolerance $\\tau = 10^{-12}$. This marks the interpolation threshold, where the model capacity is sufficient to perfectly memorize the training data ($m \\ge \\operatorname{rank}(X_c)$).\n    -   $b_{\\text{dd}}$ is determined by a specific criterion that checks for the characteristic \"U-shape\" before the peak ($m_{\\text{peak}}$) and a subsequent decline in test error, which constitutes the \"second descent\". This second descent occurs as the model capacity approaches $d$, and the projector $P_m$ approaches the identity matrix, which trivially achieves zero error on any data.\n\nThis comprehensive procedure allows for a quantitative investigation of the double descent phenomenon within a controlled and theoretically grounded linear model.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import ortho_group\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (60, 40, 2000, 12, 5.0, 0.5, 1),\n        (60, 58, 2000, 12, 4.0, 0.6, 2),\n        (30, 15, 3000, 0, 0.0, 1.0, 3),\n        (20, 5, 5000, 8, 3.0, 0.3, 4),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(*params)\n        results.append(result)\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(d, n, n_test, s, sigma_signal_sq, sigma_noise_sq, seed):\n    \"\"\"\n    Runs the double descent simulation for a single set of parameters.\n\n    Args:\n        d (int): Dimensionality of the data space.\n        n (int): Number of training samples.\n        n_test (int): Number of test samples.\n        s (int): Number of signal dimensions.\n        sigma_signal_sq (float): Signal variance.\n        sigma_noise_sq (float): Noise variance.\n        seed (int): Random seed.\n\n    Returns:\n        list: A list containing [m_peak, E_test_at_peak, m_interp, b_dd].\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct the covariance matrix Sigma\n    lambdas = np.array([sigma_signal_sq] * s + [sigma_noise_sq] * (d - s))\n    Q = ortho_group.rvs(dim=d, random_state=rng)\n    Sigma = Q @ np.diag(lambdas) @ Q.T\n\n    # 2. Generate training and test data\n    mean = np.zeros(d)\n    X_train = rng.multivariate_normal(mean=mean, cov=Sigma, size=n)\n    X_test = rng.multivariate_normal(mean=mean, cov=Sigma, size=n_test)\n\n    # 3. Center data using the training mean\n    mu_train = np.mean(X_train, axis=0)\n    X_train_centered = X_train - mu_train\n    X_test_centered = X_test - mu_train\n\n    # 4. Compute SVD of the centered training data\n    # U, S, Vt where V are the right singular vectors (principal components)\n    U, S_vals, Vt = np.linalg.svd(X_train_centered, full_matrices=False)\n    V = Vt.T\n\n    E_train_curve = []\n    E_test_curve = []\n\n    # Pad singular values array with zeros up to dimension d if necessary\n    S_sq_full = np.zeros(d)\n    S_sq_full[:len(S_vals)] = S_vals**2\n\n    # 5. Iterate over bottleneck size m from 1 to d\n    for m in range(1, d + 1):\n        # Efficiently compute training error using singular values\n        # E_train(m) = (1/n) * sum_{k=m to rank-1} s_k^2\n        train_error = np.sum(S_sq_full[m:]) / n\n        E_train_curve.append(train_error)\n\n        # Compute test error via projection\n        W_m = V[:, :m]\n        P_m = W_m @ W_m.T\n        I = np.identity(d)\n        \n        # Error matrix for test set: X_test_c - X_test_c @ P_m = X_test_c @ (I - P_m)\n        error_matrix_test = X_test_centered @ (I - P_m)\n        # Sum of squared L2 norms is the squared Frobenius norm\n        test_error = np.sum(error_matrix_test**2) / n_test\n        E_test_curve.append(test_error)\n\n    # 6. Calculate the required metrics\n    E_test_curve = np.array(E_test_curve)\n    E_train_curve = np.array(E_train_curve)\n\n    # m_peak: smallest m that maximizes test error\n    m_peak = np.argmax(E_test_curve) + 1\n    e_test_at_peak = E_test_curve[m_peak - 1]\n\n    # m_interp: smallest m where training error is effectively zero\n    tau = 1e-12\n    interp_indices = np.where(E_train_curve = tau)[0]\n    # The problem setup guarantees interpolation happens\n    m_interp = interp_indices[0] + 1 if len(interp_indices) > 0 else d + 1\n\n    # b_dd: boolean for double descent\n    k = m_peak\n    E_pre = E_test_curve[:k-1]\n    E_post = E_test_curve[k:] # from index k to the end\n\n    b_dd = False\n    # Check for empty sequences before proceeding\n    if E_pre.size > 0 and E_post.size > 0:\n        cond1 = np.min(E_pre)  e_test_at_peak\n        cond2 = np.min(E_post)  e_test_at_peak\n        \n        # \"at least one strict decrease somewhere in E_pre\"\n        # Implemented as not being monotonically non-decreasing\n        # Or more directly, there is at least one adjacent pair with a decrease\n        # This correctly handles sequences of length 1 (returns False)\n        cond3 = any(E_pre[i] > E_pre[i+1] for i in range(len(E_pre)-1))\n\n        if cond1 and cond2 and cond3:\n            b_dd = True\n            \n    return [m_peak, round(e_test_at_peak, 6), m_interp, b_dd]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3183618"}, {"introduction": "The double descent phenomenon shows that overparameterized models *can* generalize well, but it doesn't fully explain *why*. This practice delves into the mechanisms behind this success by comparing a narrow and a wide model that compute the exact same function. You will investigate how the \"wider\" parameterization leads to a larger classification margin and a flatter loss landscape, providing a concrete link between these geometric properties and improved generalization performance [@problem_id:3152424].", "problem": "You are given a controlled comparison scenario in deep learning to examine when wider models can generalize better than narrower ones, even when both achieve the same training error. The setting considers binary classification with two parametric model families evaluated through empirical margin and loss flatness metrics derived from the fundamental principles of Empirical Risk Minimization (ERM) and stability-based generalization reasoning.\n\nFundamental base for this problem:\n- Empirical Risk Minimization (ERM) defines the empirical loss over a training set $\\{(x_i,y_i)\\}_{i=1}^n$ with labels $y_i \\in \\{-1,+1\\}$ and model score function $f_\\theta(x)$ as\n$$\n\\widehat{L}(\\theta) \\;=\\; \\frac{1}{n} \\sum_{i=1}^n \\ell\\big(y_i f_\\theta(x_i)\\big),\n$$\nwhere the logistic loss is given by\n$$\n\\ell(t) \\;=\\; \\log\\!\\big(1 + e^{-t}\\big).\n$$\n- The empirical margin of a classifier $f_\\theta$ on the training set is defined as\n$$\n\\gamma(\\theta) \\;=\\; \\min_{i \\in \\{1,\\dots,n\\}} y_i f_\\theta(x_i).\n$$\n- A flatness surrogate (sharpness) is captured by the expected increase in empirical loss under a small multiplicative perturbation of the parameters $\\theta \\mapsto \\theta'$, defined numerically by\n$$\n\\Delta(\\theta;\\alpha) \\;=\\; \\mathbb{E}\\big[\\,\\widehat{L}(\\theta') - \\widehat{L}(\\theta)\\,\\big],\n$$\nwhere each parameter $\\theta_k$ is perturbed independently as $\\theta_k' \\;=\\; \\theta_k \\cdot (1 + \\alpha z_k)$ with $z_k \\sim \\mathcal{N}(0,1)$ and small noise scale $\\alpha$.\n\nTask:\nImplement two classifiers and evaluate their empirical margin and flatness on a fixed training set, and their generalization performance on a synthetic test distribution.\n- Model $\\mathsf{A}$ (narrow): A linear classifier in $\\mathbb{R}^2$ with unit-norm weight vector\n$$\nw_{\\mathsf{A}} \\;=\\; \\begin{bmatrix} \\cos(\\theta) \\\\ \\sin(\\theta) \\end{bmatrix},\n$$\nand score $f_{\\mathsf{A}}(x) = w_{\\mathsf{A}}^\\top x$. The angle $\\theta$ is provided in radians.\n- Model $\\mathsf{B}$ (wide): A two-layer Rectified Linear Unit (ReLU) network with $2m$ hidden units that realizes the score\n$$\nf_{\\mathsf{B}}(x) \\;=\\; \\sum_{j=1}^{m} \\frac{1}{m}\\,\\max\\!\\big(0,\\,u^\\top x\\big) \\;+\\; \\sum_{j=1}^{m} \\left(-\\frac{1}{m}\\right)\\,\\max\\!\\big(0,\\,-u^\\top x\\big),\n$$\nwhere $u = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ and $\\max(0,\\cdot)$ is the ReLU activation. Note that $f_{\\mathsf{B}}(x)$ equals $u^\\top x$ exactly, but the parameterization is wider due to the duplication across $m$ pairs.\n\nTraining set:\nUse the deterministic training set with four points\n$$\nX_{\\text{train}} = \\Big\\{ (1,\\,r),\\; (1,\\,-r),\\; (-1,\\,r),\\; (-1,\\,-r) \\Big\\},\n$$\nwith labels $y=+1$ for points with first coordinate $x_1 = 1$ and $y=-1$ for points with first coordinate $x_1 = -1$. The scalar $r0$ is provided.\n\nTest distribution:\nGenerate a balanced test set of size $N$ by sampling $N/2$ points from the positive class distribution $\\mathcal{N}\\!\\left(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix},\\,\\sigma^2 I\\right)$ with label $+1$, and $N/2$ points from the negative class distribution $\\mathcal{N}\\!\\left(\\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix},\\,\\sigma^2 I\\right)$ with label $-1$. Here $\\sigma0$ and $I$ is the $2\\times 2$ identity matrix. The angle unit is radians. The test error must be expressed as a decimal fraction in $[0,1]$.\n\nMetrics to compute for each test case:\n- Empirical margin on the training set for both models, $\\gamma_{\\mathsf{A}}$ and $\\gamma_{\\mathsf{B}}$.\n- Flatness surrogate for both models at noise scale $\\alpha$, defined as the average increase in empirical logistic loss under multiplicative Gaussian perturbations of all parameters, $\\Delta_{\\mathsf{A}}(\\alpha)$ and $\\Delta_{\\mathsf{B}}(\\alpha)$.\n- Test error for both models on the generated test set, $e_{\\mathsf{A}}$ and $e_{\\mathsf{B}}$, computed as the fraction of misclassified test samples.\n\nDecision logic:\nFor each test case, output three booleans indicating whether the wider model exhibits the desired properties:\n- $b_1$: $\\gamma_{\\mathsf{B}}  \\gamma_{\\mathsf{A}}$,\n- $b_2$: $\\Delta_{\\mathsf{B}}(\\alpha)  \\Delta_{\\mathsf{A}}(\\alpha)$,\n- $b_3$: $e_{\\mathsf{B}}  e_{\\mathsf{A}}$.\n\nTest suite:\nRun your program on the following parameter sets to cover different facets:\n- Case $1$ (happy path): $\\theta = 0.3$, $m = 50$, $r = 0.2$, $\\sigma = 0.4$, $\\alpha = 0.05$, $N = 5000$.\n- Case $2$ (boundary condition with small spread): $\\theta = 0.6$, $m = 100$, $r = 0.1$, $\\sigma = 0.15$, $\\alpha = 0.05$, $N = 5000$.\n- Case $3$ (edge case with moderate width and larger spread): $\\theta = 0.4$, $m = 10$, $r = 0.25$, $\\sigma = 0.5$, $\\alpha = 0.05$, $N = 5000$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of three booleans $[b_1,b_2,b_3]$ in order. For example, a valid output format is\n[[True,False,True],[True,True,True],[False,True,False]].\nNo additional text should be printed.", "solution": "The user-provided problem is valid. It is scientifically grounded in the principles of statistical learning theory and deep learning, specifically concerning model capacity, generalization, and the role of over-parameterization. The problem is well-posed, with all necessary parameters and definitions provided to arrive at a unique, meaningful solution. It presents a controlled, formalizable experiment to test a specific hypothesis about the relationship between model width, the geometry of the loss landscape (flatness), and generalization performance (test error), which is a central topic of modern deep learning research.\n\nThe solution proceeds by implementing the two specified models, computing the required metrics for each, and then performing the requested comparisons.\n\n**1. Model and Metric Definitions**\n\nThe problem defines two models for a binary classification task on data $x \\in \\mathbb{R}^2$ with labels $y \\in \\{-1, +1\\}$.\n\n- **Model $\\mathsf{A}$ (Narrow)** is a linear classifier with score function $f_{\\mathsf{A}}(x) = w_{\\mathsf{A}}^\\top x$. Its weight vector $w_{\\mathsf{A}} = [\\cos(\\theta), \\sin(\\theta)]^\\top$ is parameterized by a single scalar angle $\\theta$.\n\n- **Model $\\mathsf{B}$ (Wide)** is a two-layer ReLU network. Its score function is given by\n$$\nf_{\\mathsf{B}}(x) \\;=\\; \\sum_{j=1}^{m} \\frac{1}{m}\\,\\max\\!\\big(0,\\,u^\\top x\\big) \\;+\\; \\sum_{j=1}^{m} \\left(-\\frac{1}{m}\\right)\\,\\max\\!\\big(0,\\,-u^\\top x\\big),\n$$\nwhere $u = [1, 0]^\\top$. Using the identity $\\max(0, z) - \\max(0, -z) = z$, this function simplifies to an exact linear function:\n$$\nf_{\\mathsf{B}}(x) = \\frac{m}{m} \\max(0, u^\\top x) - \\frac{m}{m} \\max(0, -u^\\top x) = u^\\top x = x_1.\n$$\nDespite its simple functional form, its parameterization involves $2m$ weights in the second layer: $m$ weights with value $1/m$ and $m$ weights with value $-1/m$. These $2m$ weights are the parameters subject to perturbation for the flatness calculation.\n\nThe training set is fixed, containing $n=4$ points: $X_{\\text{train}} = \\{ (1, r), (1, -r), (-1, r), (-1, -r) \\}$ with corresponding labels $Y_{\\text{train}} = \\{+1, +1, -1, -1\\}$.\n\nThree metrics are computed:\n- **Empirical Margin ($\\gamma$)**: The minimum score on a correctly classified training point, $\\gamma(\\theta) = \\min_{i} y_i f_\\theta(x_i)$.\n- **Flatness Surrogate ($\\Delta$)**: The expected increase in logistic loss under multiplicative parameter perturbation, $\\Delta(\\theta;\\alpha) = \\mathbb{E}[\\,\\widehat{L}(\\theta') - \\widehat{L}(\\theta)\\,]$, where $\\theta_k' = \\theta_k(1+\\alpha z_k)$ for $z_k \\sim \\mathcal{N}(0,1)$. The logistic loss is $\\ell(t) = \\log(1+e^{-t})$.\n- **Test Error ($e$)**: The misclassification rate on a large, synthetically generated test set.\n\n**2. Calculation of Metrics for Model $\\mathsf{A}$**\n\n- **Empirical Margin $\\gamma_{\\mathsf{A}}$**:\nThe scores on the training set for Model $\\mathsf{A}$ are $y_i f_{\\mathsf{A}}(x_i)$. For the four points, these are:\n$y_1 f_{\\mathsf{A}}(x_1) = 1 \\cdot (\\cos\\theta + r\\sin\\theta)$\n$y_2 f_{\\mathsf{A}}(x_2) = 1 \\cdot (\\cos\\theta - r\\sin\\theta)$\n$y_3 f_{\\mathsf{A}}(x_3) = -1 \\cdot (-\\cos\\theta + r\\sin\\theta) = \\cos\\theta - r\\sin\\theta$\n$y_4 f_{\\mathsf{A}}(x_4) = -1 \\cdot (-\\cos\\theta - r\\sin\\theta) = \\cos\\theta + r\\sin\\theta$\nSince the test case parameters ensure $\\theta > 0$ and $r>0$, we have $\\sin\\theta > 0$. The margin is the minimum of these values:\n$\\gamma_{\\mathsf{A}} = \\min(\\cos\\theta + r\\sin\\theta, \\cos\\theta - r\\sin\\theta) = \\cos\\theta - r\\sin\\theta$.\n\n- **Flatness Surrogate $\\Delta_{\\mathsf{A}}(\\alpha)$**:\nThe parameter for Model $\\mathsf{A}$ is $\\theta$. The empirical loss is $\\widehat{L}_{\\mathsf{A}}(\\phi) = \\frac{1}{2}[\\ell(\\cos\\phi + r\\sin\\phi) + \\ell(\\cos\\phi - r\\sin\\phi)]$. The flatness is estimated via a Monte Carlo average. We generate $K$ samples $z_k \\sim \\mathcal{N}(0,1)$, compute the perturbed parameters $\\theta_k' = \\theta(1+\\alpha z_k)$, and then average the resulting change in loss:\n$\\Delta_{\\mathsf{A}}(\\alpha) \\approx \\frac{1}{K} \\sum_{k=1}^K \\widehat{L}_{\\mathsf{A}}(\\theta_k') - \\widehat{L}_{\\mathsf{A}}(\\theta)$.\n\n- **Test Error $e_{\\mathsf{A}}$**:\nThe test error is computed by generating a test set of size $N$ according to the problem specification and counting the fraction of samples where $\\text{sign}(f_{\\mathsf{A}}(x)) \\neq y$.\n\n**3. Calculation of Metrics for Model $\\mathsf{B}$**\n\n- **Empirical Margin $\\gamma_{\\mathsf{B}}$**:\nThe score function is $f_{\\mathsf{B}}(x) = x_1$. The products $y_i f_{\\mathsf{B}}(x_i)$ for the four training points are:\n$y_1 f_{\\mathsf{B}}(x_1) = 1 \\cdot 1 = 1$\n$y_2 f_{\\mathsf{B}}(x_2) = 1 \\cdot 1 = 1$\n$y_3 f_{\\mathsf{B}}(x_3) = -1 \\cdot (-1) = 1$\n$y_4 f_{\\mathsf{B}}(x_4) = -1 \\cdot (-1) = 1$\nThe margin is therefore constant: $\\gamma_{\\mathsf{B}} = 1$.\n\n- **Flatness Surrogate $\\Delta_{\\mathsf{B}}(\\alpha)$**:\nThe parameters are the $2m$ second-layer weights. Let $v_j$ be these weights. The perturbed score function on a training point $x_i$ depends on the sum of perturbed weights. For training points with $x_{i,1}=1$, the score is $S_1 = \\sum_{j=1}^m \\frac{1}{m}(1+\\alpha z_j) = 1 + \\frac{\\alpha}{m}\\sum_{j=1}^m z_j$. For points with $x_{i,1}=-1$, the score is $-S_2$, where $S_2 = 1 + \\frac{\\alpha}{m}\\sum_{j=m+1}^{2m} z_j$. By the Central Limit Theorem, for large $m$, $S_1, S_2$ are approximately normally distributed. Precisely, as sums of scaled normal variables, $S_1 \\sim \\mathcal{N}(1, \\alpha^2/m)$ and $S_2 \\sim \\mathcal{N}(1, \\alpha^2/m)$.\nThe loss on the perturbed model becomes $\\frac{1}{2}[\\ell(S_1) + \\ell(S_2)]$. The unperturbed loss is $\\ell(1)$. The flatness is:\n$\\Delta_{\\mathsf{B}}(\\alpha) = \\mathbb{E}_{S_1, S_2}\\left[ \\frac{1}{2}(\\ell(S_1) + \\ell(S_2)) \\right] - \\ell(1) = \\mathbb{E}_{S \\sim \\mathcal{N}(1, \\alpha^2/m)}[\\ell(S)] - \\ell(1)$.\nThis expectation is computed efficiently via Monte Carlo simulation by sampling directly from the distribution of $S$. The variance of the effective perturbation scales inversely with the width $m$, suggesting wider models are flatter.\n\n- **Test Error $e_{\\mathsf{B}}$**:\nThe test error is computed on the same generated test set. The decision rule is $\\text{sign}(x_1)$. This corresponds to the Bayes optimal classifier for the specified test distribution, which is centered at $[1,0]^\\top$ and $[-1,0]^\\top$, making it the ideal benchmark.\n\n**4. Implementation and Decision Logic**\n\nThe solution is implemented in Python using the `numpy` library. A single random number generator is used for both Monte Carlo estimations and test set generation for each case to ensure consistency. A large number of Monte Carlo samples ($K=20000$) is used to ensure stable estimates of the flatness metrics. For each of the three test cases, we compute $(\\gamma_{\\mathsf{A}}, \\Delta_{\\mathsf{A}}, e_{\\mathsf{A}})$ and $(\\gamma_{\\mathsf{B}}, \\Delta_{\\mathsf{B}}, e_{\\mathsf{B}})$ and then evaluate the three boolean conditions:\n- $b_1: \\gamma_{\\mathsf{B}} > \\gamma_{\\mathsf{A}}$ (Is the wide model's margin larger?)\n- $b_2: \\Delta_{\\mathsf{B}}(\\alpha)  \\Delta_{\\mathsf{A}}(\\alpha)$ (Is the wide model's loss landscape flatter?)\n- $b_3: e_{\\mathsf{B}}  e_{\\mathsf{A}}$ (Does the wide model generalize better?)\n\nBased on the derivations, we expect $b_1$ to be `True` since $\\gamma_{\\mathsf{A}}  1 = \\gamma_{\\mathsf{B}}$, $b_2$ to be `True` as the perturbation variance for Model $\\mathsf{B}$ is reduced by its width $m$, and $b_3$ to be `True` because Model $\\mathsf{B}$ implements the optimal classifier for the test data while Model $\\mathsf{A}$'s classifier is slightly misaligned due to $\\theta \\neq 0$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating and comparing metrics for two classifier models\n    across three different test cases, as specified in the problem description.\n    \"\"\"\n\n    # --- Static Configurations ---\n    # Number of Monte Carlo samples for flatness calculation\n    K_SAMPLES = 20000\n\n    # --- Helper Functions ---\n    def logistic_loss(t):\n        \"\"\"Computes the logistic loss log(1 + exp(-t)) in a numerically stable way.\"\"\"\n        return np.logaddexp(0, -t)\n\n    def get_training_set(r):\n        \"\"\"Returns the fixed training set points and labels.\"\"\"\n        X_train = np.array([\n            [1.0, r], [1.0, -r], [-1.0, r], [-1.0, -r]\n        ])\n        y_train = np.array([1.0, 1.0, -1.0, -1.0])\n        return X_train, y_train\n\n    def generate_test_set(N, sigma, rng):\n        \"\"\"Generates a balanced test set of size N.\"\"\"\n        n_half = N // 2\n        cov = np.eye(2) * (sigma ** 2)\n\n        mean_pos = np.array([1.0, 0.0])\n        X_pos = rng.multivariate_normal(mean_pos, cov, n_half)\n        y_pos = np.ones(n_half)\n\n        mean_neg = np.array([-1.0, 0.0])\n        X_neg = rng.multivariate_normal(mean_neg, cov, n_half)\n        y_neg = -np.ones(n_half)\n\n        X_test = np.vstack((X_pos, X_neg))\n        y_test = np.concatenate((y_pos, y_neg))\n        return X_test, y_test\n\n    def calculate_model_A_metrics(theta, r, alpha, X_test, y_test, rng):\n        \"\"\"Calculates margin, flatness, and test error for Model A.\"\"\"\n        # 1. Empirical Margin (gamma_A)\n        gamma_A = np.cos(theta) - r * np.sin(theta)\n\n        # 2. Flatness Surrogate (Delta_A)\n        def empirical_loss_A(phi):\n            t1 = np.cos(phi) + r * np.sin(phi)\n            t2 = np.cos(phi) - r * np.sin(phi)\n            return 0.5 * (logistic_loss(t1) + logistic_loss(t2))\n\n        base_loss_A = empirical_loss_A(theta)\n        \n        z_samples = rng.standard_normal(K_SAMPLES)\n        perturbed_thetas = theta * (1.0 + alpha * z_samples)\n        \n        perturbed_losses = np.array([empirical_loss_A(p_t) for p_t in perturbed_thetas])\n        delta_A = np.mean(perturbed_losses) - base_loss_A\n\n        # 3. Test Error (e_A)\n        w_A = np.array([np.cos(theta), np.sin(theta)])\n        scores_A = X_test @ w_A\n        predictions_A = np.sign(scores_A)\n        predictions_A[predictions_A == 0] = 1.0  # Break ties\n        e_A = np.mean(predictions_A != y_test)\n\n        return gamma_A, delta_A, e_A\n\n    def calculate_model_B_metrics(m, alpha, X_test, y_test, rng):\n        \"\"\"Calculates margin, flatness, and test error for Model B.\"\"\"\n        # 1. Empirical Margin (gamma_B)\n        # For Model B, f(x) = x_1. For all training points, y_i * f(x_i) = 1.\n        gamma_B = 1.0\n\n        # 2. Flatness Surrogate (Delta_B)\n        base_loss_B = logistic_loss(1.0)\n        \n        zeta_samples = rng.standard_normal(K_SAMPLES)\n        S_samples = 1.0 + (alpha / np.sqrt(m)) * zeta_samples\n        \n        perturbed_losses = logistic_loss(S_samples)\n        delta_B = np.mean( perturbed_losses) - base_loss_B\n\n        # 3. Test Error (e_B)\n        scores_B = X_test[:, 0]\n        predictions_B = np.sign(scores_B)\n        predictions_B[predictions_B == 0] = 1.0  # Break ties\n        e_B = np.mean(predictions_B != y_test)\n\n        return gamma_B, delta_B, e_B\n\n    # --- Main Execution Logic ---\n    test_cases = [\n        # Case 1: theta=0.3, m=50, r=0.2, sigma=0.4, alpha=0.05, N=5000\n        (0.3, 50, 0.2, 0.4, 0.05, 5000),\n        # Case 2: theta=0.6, m=100, r=0.1, sigma=0.15, alpha=0.05, N=5000\n        (0.6, 100, 0.1, 0.15, 0.05, 5000),\n        # Case 3: theta=0.4, m=10, r=0.25, sigma=0.5, alpha=0.05, N=5000\n        (0.4, 10, 0.25, 0.5, 0.05, 5000),\n    ]\n\n    results = []\n    # Use a single random number generator for all random processes.\n    # No seed is specified, so results may vary slightly on different runs,\n    # but the conclusions should be stable due to large N and K.\n    rng = np.random.default_rng()\n\n    for case in test_cases:\n        theta, m, r, sigma, alpha, N = case\n\n        # Generate a single test set to be used by both models for this case\n        X_test, y_test = generate_test_set(N, sigma, rng)\n\n        # Calculate metrics for both models\n        gamma_A, delta_A, e_A = calculate_model_A_metrics(theta, r, alpha, X_test, y_test, rng)\n        gamma_B, delta_B, e_B = calculate_model_B_metrics(m, alpha, X_test, y_test, rng)\n        \n        # Perform the comparisons to get the boolean results\n        b1 = gamma_B > gamma_A\n        b2 = delta_B  delta_A\n        b3 = e_B  e_A\n        \n        results.append([bool(b1), bool(b2), bool(b3)])\n\n    # Format the final output string exactly as required\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3152424"}]}