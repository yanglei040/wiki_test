{"hands_on_practices": [{"introduction": "In standard classification, misclassifying a \"cat\" as a \"dog\" is penalized the same as misclassifying it as a \"car,\" even though the first error is taxonomically less severe. This exercise challenges that flat-world view by exploring the paradigm of hierarchy-aware risk, where you will implement and compare loss functions that explicitly incorporate the relationships between classes. By designing objectives that understand the structure of the problem, we can guide models to make more sensible errors and better reflect our domain knowledge [@problem_id:3160884].", "problem": "Consider a hierarchical classification task in which classes are leaves of a taxonomy tree. Let the taxonomy be a rooted tree with a single root and internal nodes representing categories. The learning paradigm is empirical risk minimization: given an input, a model outputs a vector of real-valued scores (logits), which are transformed into a probability distribution over leaf classes via the Softmax function. The objective is to evaluate how the choice of loss shaping reflects the taxonomy and to compare it to flat Cross-Entropy, thereby analyzing a hierarchy-aware risk.\n\nFundamental base:\n- Empirical risk is defined as the expectation of a loss function over the data distribution. For a single example with true leaf class index $y$ and predicted class probabilities $p(k)$ over leaf classes $k$, the instantaneous risk is the chosen loss $\\ell(y, p)$.\n- Softmax converts logits $z(k)$ into probabilities $p(k) = \\exp(z(k)) \\big/ \\sum_{j} \\exp(z(j))$ using the natural logarithm base $e$.\n- Flat Cross-Entropy (defined at the leaf level) is $\\ell_{\\mathrm{CE}}(y, p) = -\\log p(y)$.\n- A hierarchy-aware expected cost is constructed from a taxonomy distance function $d(y, k)$ between leaf classes, yielding $\\ell_{\\mathrm{HEC}}(y, p) = \\sum_{k} p(k) \\, d(y, k)$.\n- A hierarchy-aware path Cross-Entropy shapes the loss to allocate probability mass along the path from the root to the true leaf. Let $P(n)$ be the probability mass of internal or leaf node $n$ defined as the sum of probabilities of its descendant leaves. Let $\\mathrm{path}(y)$ denote the set of nodes on the unique path from the root to $y$. Excluding the root (whose probability mass is identically $1$), define $\\ell_{\\mathrm{HPCE}}(y, p) = -\\sum_{n \\in \\mathrm{path}(y) \\setminus \\{\\mathrm{root}\\}} \\log P(n)$. This penalizes predictions that assign mass outside the correct branch more severely than those that confuse siblings within the correct branch.\n\nTaxonomy:\n- The taxonomy has a root and two internal nodes, each with two leaves:\n  - Internal node $A$ with leaves $A1$ and $A2$.\n  - Internal node $B$ with leaves $B1$ and $B2$.\n- Indexing of leaves is:\n  - $0 \\leftrightarrow A1$, $1 \\leftrightarrow A2$, $2 \\leftrightarrow B1$, $3 \\leftrightarrow B2$.\n- The taxonomy distance $d(i, j)$ between leaves $i$ and $j$ is the length of the shortest path in the tree between the two leaves (counted in edges). Hence:\n  - $d(i, i) = 0$ for all $i$.\n  - $d(0, 1) = 2$ and $d(2, 3) = 2$ for sibling leaves under the same internal node.\n  - $d(0, 2) = 4$, $d(0, 3) = 4$, $d(1, 2) = 4$, $d(1, 3) = 4$ for leaves under different internal nodes.\n\nTest suite:\nEach test case consists of a pair $(\\text{logits}, y)$ where logits is a list of $4$ real numbers and $y$ is the true leaf index. The Softmax probabilities $p(k)$ must be computed from logits before evaluating the losses. Use the natural logarithm and compute all losses as real numbers. Round each loss to $6$ decimal places.\n\n- Case $1$ (happy path, strong correct leaf): logits $[4, 1, -1, -2]$, $y = 0$.\n- Case $2$ (sibling confusion within $A$): logits $[1, 3.5, -1, -2]$, $y = 0$.\n- Case $3$ (cross-branch confusion into $B$): logits $[-1, -1.5, 3.0, 2.5]$, $y = 0$.\n- Case $4$ (uninformative uniform logits): logits $[0, 0, 0, 0]$, $y = 1$.\n- Case $5$ (extreme wrong branch concentration): logits $[10, -10, -10, -10]$, $y = 3$.\n\nFor each test case, compute:\n- Flat Cross-Entropy $\\ell_{\\mathrm{CE}}(y, p)$.\n- Hierarchy-aware expected cost $\\ell_{\\mathrm{HEC}}(y, p)$.\n- Hierarchy-aware path Cross-Entropy $\\ell_{\\mathrm{HPCE}}(y, p)$.\n\nFinal output format:\nYour program should produce a single line of output containing a comma-separated list of results for all test cases, enclosed in square brackets. Each test case result must itself be a list of three floats in the order $[\\ell_{\\mathrm{CE}}, \\ell_{\\mathrm{HEC}}, \\ell_{\\mathrm{HPCE}}]$, each rounded to $6$ decimal places. The final output must look like:\n$[[\\ell_{\\mathrm{CE}}^{(1)}, \\ell_{\\mathrm{HEC}}^{(1)}, \\ell_{\\mathrm{HPCE}}^{(1)}],[\\ell_{\\mathrm{CE}}^{(2)}, \\ell_{\\mathrm{HEC}}^{(2)}, \\ell_{\\mathrm{HPCE}}^{(2)}],\\dots]$.", "solution": "The problem requires the computation and comparison of three distinct loss functions for a hierarchical classification task. The task is defined on a specific tree-structured taxonomy. The three loss functions are the standard Flat Cross-Entropy ($\\ell_{\\mathrm{CE}}$), a Hierarchy-aware Expected Cost ($\\ell_{\\mathrm{HEC}}$), and a Hierarchy-aware Path Cross-Entropy ($\\ell_{\\mathrm{HPCE}}$). The objective is to evaluate these losses for five different scenarios represented by test cases, each consisting of a vector of logits and a true class label.\n\nFirst, we formalize the components of the problem. Given a vector of logits $z = [z(0), z(1), z(2), z(3)]$, the probability distribution over the $4$ leaf classes $p = [p(0), p(1), p(2), p(3)]$ is obtained using the Softmax function:\n$$ p(k) = \\frac{\\exp(z(k))}{\\sum_{j=0}^{3} \\exp(z(j))} \\quad \\text{for } k \\in \\{0, 1, 2, 3\\} $$\nThe natural logarithm, $\\log$, is used throughout the calculations.\n\nThe taxonomy is a rooted tree with two levels of branching. The root has two children, internal nodes $A$ and $B$. Node $A$ has two leaf children, $A1$ (index $0$) and $A2$ (index $1$). Node $B$ has two leaf children, $B1$ (index $2$) and $B2$ (index $3$).\n\nThe three loss functions are defined as follows for a given true leaf index $y$ and predicted probability distribution $p$:\n\n$1$. **Flat Cross-Entropy ($\\ell_{\\mathrm{CE}}$)**: This is the negative log-likelihood of the true class. It treats all classes as independent and does not consider the hierarchy.\n$$ \\ell_{\\mathrm{CE}}(y, p) = -\\log p(y) $$\n\n$2$. **Hierarchy-aware Expected Cost ($\\ell_{\\mathrm{HEC}}$)**: This loss function defines the risk as the expected distance between the true class and the predicted class, where the expectation is taken over the model's predicted probability distribution.\n$$ \\ell_{\\mathrm{HEC}}(y, p) = \\sum_{k=0}^{3} p(k) \\, d(y, k) $$\nThe distance $d(i, j)$ is the number of edges on the shortest path between leaf $i$ and leaf $j$. For our taxonomy, the distance matrix $D$ where $D_{ij}=d(i,j)$ is:\n$$\nD = \\begin{pmatrix}\n0 & 2 & 4 & 4 \\\\\n2 & 0 & 4 & 4 \\\\\n4 & 4 & 0 & 2 \\\\\n4 & 4 & 2 & 0\n\\end{pmatrix}\n$$\nThis loss penalizes misclassifications based on their distance in the hierarchy, with sibling confusions (e.g., $A1$ vs. $A2$, distance $2$) penalized less severely than cross-branch confusions (e.g., $A1$ vs. $B1$, distance $4$).\n\n$3$. **Hierarchy-aware Path Cross-Entropy ($\\ell_{\\mathrm{HPCE}}$)**: This loss encourages the model to assign probability mass along the correct path from the root to the true leaf. It is computed by summing the negative log probabilities of all nodes on this path (excluding the root).\n$$ \\ell_{\\mathrm{HPCE}}(y, p) = -\\sum_{n \\in \\mathrm{path}(y) \\setminus \\{\\mathrm{root}\\}} \\log P(n) $$\n$P(n)$ is the total probability mass of a node $n$, calculated by summing the probabilities of all leaf descendants of $n$. For our taxonomy:\n-   Probability of internal node $A$: $P(A) = p(0) + p(1)$.\n-   Probability of internal node $B$: $P(B) = p(2) + p(3)$.\n-   The probability of a leaf node, e.g., $P(A1)$, is simply its own probability, $p(0)$.\n\nThe specific formulas for $\\ell_{\\mathrm{HPCE}}$ for each true class $y$ are:\n-   If $y=0$ (leaf $A1$): $\\mathrm{path}(0) = \\{\\text{root}, A, A1\\}$.\n    $\\ell_{\\mathrm{HPCE}}(0, p) = -[\\log P(A) + \\log P(A1)] = -[\\log(p(0)+p(1)) + \\log p(0)]$.\n-   If $y=1$ (leaf $A2$): $\\mathrm{path}(1) = \\{\\text{root}, A, A2\\}$.\n    $\\ell_{\\mathrm{HPCE}}(1, p) = -[\\log P(A) + \\log P(A2)] = -[\\log(p(0)+p(1)) + \\log p(1)]$.\n-   If $y=2$ (leaf $B1$): $\\mathrm{path}(2) = \\{\\text{root}, B, B1\\}$.\n    $\\ell_{\\mathrm{HPCE}}(2, p) = -[\\log P(B) + \\log P(B1)] = -[\\log(p(2)+p(3)) + \\log p(2)]$.\n-   If $y=3$ (leaf $B2$): $\\mathrm{path}(3) = \\{\\text{root}, B, B2\\}$.\n    $\\ell_{\\mathrm{HPCE}}(3, p) = -[\\log P(B) + \\log P(B2)] = -[\\log(p(2)+p(3)) + \\log p(3)]$.\n\nLet's walk through the calculation for Case $1$: logits $[4, 1, -1, -2]$, $y = 0$.\n-   **Step 1: Compute Probabilities.**\n    The exponents of the logits are $[\\exp(4), \\exp(1), \\exp(-1), \\exp(-2)] \\approx [54.598, 2.718, 0.368, 0.135]$.\n    The sum is $\\approx 57.820$.\n    The probability vector is $p \\approx [54.598/57.820, 2.718/57.820, 0.368/57.820, 0.135/57.820] \\approx [0.944285, 0.047012, 0.006363, 0.002341]$.\n-   **Step 2: Compute Losses.**\n    -   $\\ell_{\\mathrm{CE}}(0, p) = -\\log(p(0)) = -\\log(0.944285) \\approx 0.057310$.\n    -   $\\ell_{\\mathrm{HEC}}(0, p) = \\sum_k p(k) d(0, k) = p(0) \\cdot 0 + p(1) \\cdot 2 + p(2) \\cdot 4 + p(3) \\cdot 4 \\approx (0.047012 \\cdot 2) + (0.006363 \\cdot 4) + (0.002341 \\cdot 4) \\approx 0.094024 + 0.025452 + 0.009364 \\approx 0.128840$.\n    -   $\\ell_{\\mathrm{HPCE}}(0, p) = -[\\log(p(0)+p(1)) + \\log(p(0))] = -[\\log(0.944285+0.047012) + \\log(0.944285)] = -[\\log(0.991297) - 0.057310] \\approx -[-0.008745 - 0.057310] \\approx 0.066055$.\n\nThe results demonstrate the properties of each loss. For instance, in Case $2$ (sibling confusion), $\\ell_{\\mathrm{HEC}}$ is significantly lower than in Case $3$ (cross-branch confusion), reflecting the smaller taxonomic distance of the error. $\\ell_{\\mathrm{HPCE}}$ is more forgiving than $\\ell_{\\mathrm{CE}}$ in Case $2$ because most probability is kept within the correct superclass, but it is high in Case $3$ where probability mass has leaked to a completely different branch of the taxonomy. The following Python code systematically applies this methodology to all test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes three different loss functions for hierarchical classification\n    based on a defined taxonomy and a set of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ([4, 1, -1, -2], 0),   # Case 1: Happy path\n        ([1, 3.5, -1, -2], 0), # Case 2: Sibling confusion\n        ([-1, -1.5, 3.0, 2.5], 0), # Case 3: Cross-branch confusion\n        ([0, 0, 0, 0], 1),      # Case 4: Uninformative uniform\n        ([10, -10, -10, -10], 3)    # Case 5: Extreme wrong branch\n    ]\n\n    # Taxonomy distance matrix d(i, j)\n    # Leaves: 0=A1, 1=A2, 2=B1, 3=B2\n    # d(i,i)=0\n    # d(0,1)=2, d(2,3)=2\n    # d(0,2)=4, d(0,3)=4, d(1,2)=4, d(1,3)=4\n    distance_matrix = np.array([\n        [0, 2, 4, 4],\n        [2, 0, 4, 4],\n        [4, 4, 0, 2],\n        [4, 4, 2, 0]\n    ])\n\n    results = []\n    \n    for logits, y in test_cases:\n        # Convert logits to numpy array for vectorized operations\n        z = np.array(logits, dtype=np.float64)\n\n        # Compute probabilities using the Softmax function\n        # A numerically stable version of Softmax is used: exp(z - max(z))\n        exps = np.exp(z - np.max(z))\n        p = exps / np.sum(exps)\n\n        # --- Loss Calculation ---\n\n        # 1. Flat Cross-Entropy (l_ce)\n        l_ce = -np.log(p[y])\n\n        # 2. Hierarchy-aware Expected Cost (l_hec)\n        l_hec = np.sum(p * distance_matrix[y])\n\n        # 3. Hierarchy-aware Path Cross-Entropy (l_hpce)\n        # Taxonomy: root -> {A, B}; A -> {0, 1}; B -> {2, 3}\n        if y in [0, 1]:  # True class is in group A\n            # P(A) = p(0) + p(1)\n            p_group = p[0] + p[1]\n            # l_hpce = -(log(P(A)) + log(P(leaf)))\n            l_hpce = -(np.log(p_group) + np.log(p[y]))\n        else:  # True class is in group B (y in [2, 3])\n            # P(B) = p(2) + p(3)\n            p_group = p[2] + p[3]\n            # l_hpce = -(log(P(B)) + log(P(leaf)))\n            l_hpce = -(np.log(p_group) + np.log(p[y]))\n            \n        # Round results to 6 decimal places as required\n        case_result = [\n            round(l_ce, 6),\n            round(l_hec, 6),\n            round(l_hpce, 6)\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) correctly formats the inner lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3160884"}, {"introduction": "Data augmentation is a cornerstone of modern deep learning, used to improve generalization by creating new training examples. However, what if an augmentation inadvertently breaks the very logic the model is supposed to learn? This exercise uses a carefully constructed thought experiment to reveal the potential dangers of enforcing invariance to transformations that don't preserve the label, pushing the model to rely on spurious correlations. It encourages a shift in paradigm towards causally-guided augmentation, where we critically assess which invariances are truly beneficial [@problem_id:3160908].", "problem": "Consider a binary image classification task in deep learning where the input $X$ consists of two discrete features: a causal feature $C \\in \\{-1, +1\\}$ indicating arrow direction (left if $C=-1$, right if $C=+1$), and a spurious feature $S \\in \\{-1, +1\\}$ indicating background texture. The label is $Y = \\mathbb{I}[C=+1] \\in \\{0,1\\}$. Assume $P(C=+1)=P(C=-1)=1/2$ and that $S$ is correlated with $C$ via $P(S=C) = p$ where $p \\in (1/2, 1]$. At test time, the environment shifts so that $P_{\\text{test}}(S=C) = q$ with $q \\in [0,1]$ and possibly $q < p$.\n\nTraining is carried out by Empirical Risk Minimization (ERM), which minimizes the empirical average of a loss function $\\ell$ over the training samples. Formally, ERM seeks a predictor $f$ to minimize $\\widehat{R}(f) = \\frac{1}{n}\\sum_{i=1}^{n} \\ell\\big(f(X_i), Y_i\\big)$, and the population risk is $R(f) = \\mathbb{E}[\\ell(f(X), Y)]$. The Bayes optimal classifier for $0\\text{-}1$ loss is $f^{*}(x) = \\arg\\max_{y \\in \\{0,1\\}} P(Y=y \\mid X=x)$.\n\nA data augmentation transformation $T$ is applied during training that horizontally flips images. By construction, $T$ maps $(C,S)$ to $(-C, S)$, because flipping reverses the arrow direction but leaves background texture unchanged. The training pipeline uses standard augmentation where each original example $(X, Y)$ is paired with $(T(X), Y)$, i.e., the label $Y$ is kept unchanged under $T$. In addition, some training objectives enforce invariance $f(X) = f(T(X))$.\n\nAnswer the following multiple-choice question. Select all options that are correct.\n\nA. If a transformation $T$ changes the causal feature that determines the label so that $P(Y \\mid X) \\neq P(Y \\mid T(X))$, then constraining $f$ to be invariant under $T$ (i.e., $f(X)=f(T(X))$) generally increases population risk $R(f)$ relative to the unconstrained Bayes optimal classifier on the original distribution.\n\nB. In the described setup, duplicating each sample with $(T(X), Y)$ obliterates the learnability of the causal signal from $C$: specifically, the correlation $\\mathbb{E}[Y C]$ in the augmented training distribution becomes $0$, which makes learning any classifier with nontrivial generalization impossible even with infinite data.\n\nC. A causally guided selective augmentation paradigm recommends either applying augmentations only to nuisance features like $S$ that do not change $Y$, or composing $T$ with an appropriate label transform $g$ to use $(T(X), g(Y))$, thereby preserving $P(Y \\mid X)$; this avoids harmful invariance while improving robustness to spurious correlations.\n\nD. When the test-time environment reduces the spurious correlation from $p$ to $q$ (with $q < p$), enforcing invariance to flips $T$ while keeping labels fixed guarantees higher accuracy on the test distribution compared to training without augmentation.\n\nE. The necessary and sufficient condition for label-preserving augmentation is that $T$ be measure-preserving with respect to $P(X)$; if $T$ is measure-preserving on $X$, then labels can be kept unchanged without harming risk.", "solution": "The problem investigates the effect of a label-breaking data augmentation on a model's ability to learn causal vs. spurious features. The core of the problem lies in the fact that the horizontal flip transformation $T$ changes the causal feature ($C \\to -C$) but the augmentation scheme incorrectly keeps the label $Y$ (which depends on $C$) fixed.\n\n**Core Analysis:**\n-   **Causal Feature:** $C \\in \\{-1, +1\\}$.\n-   **Label:** $Y = \\mathbb{I}[C=+1]$. The label is a direct function of the causal feature. The Bayes optimal classifier is $f^*(X) = Y$, which has zero error.\n-   **Spurious Feature:** $S \\in \\{-1, +1\\}$, correlated with $C$ via $P(S=C)=p > 1/2$.\n-   **Transformation:** $T(C,S) = (-C, S)$.\n-   **Faulty Augmentation:** The training data includes pairs $(X,Y)$ and $(T(X),Y)$. This means for an original sample $((C,S), Y)$, an augmented sample $((-C,S), Y)$ is added. The correct label for the transformed input would be $Y' = \\mathbb{I}[-C=+1] = 1-Y$. By using $Y$, the augmentation introduces mislabeled data with respect to the causal feature. A model trained on this data will be incentivized to find a feature that is invariant under the transformation and still predictive of the (incorrectly shared) label. The only such feature is $S$.\n\n**Option-by-Option Analysis:**\n\n**A. Correct.** A classifier $f$ invariant under $T$ must satisfy $f(C,S) = f(-C,S)$. This means $f$ cannot depend on $C$ and must be a function of $S$ only, i.e., $f(X) = h(S)$. The best such classifier is one that relies on the spurious correlation: $f_{inv}(X) = \\mathbb{I}[S=+1]$. The risk of this classifier is the probability that it's wrong, which is $P(S \\neq C) = 1-p$. Since $p1$, this risk is greater than the zero risk of the unconstrained Bayes optimal classifier $f^*(X) = \\mathbb{I}[C=+1]$. Therefore, enforcing this harmful invariance increases the population risk.\n\n**B. Incorrect.** The augmentation creates an augmented distribution where the input's causal feature $\\tilde{C}$ and the label $\\tilde{Y}$ are uncorrelated (the expectation $\\mathbb{E}_{aug}[\\tilde{Y}\\tilde{C}]$ is indeed 0). This makes the causal feature useless for predicting the provided labels in the augmented dataset. However, a model can still learn a non-trivial classifier by exploiting the spurious correlation with $S$. The optimal classifier on the augmented data becomes $f(X) = \\mathbb{I}[S=+1]$, which achieves an accuracy of $p > 1/2$. This classifier has non-trivial generalization on the test set (accuracy $q$). The claim that learning is \"impossible\" is too strong.\n\n**C. Correct.** This option describes the core principle of causally-aware data augmentation. To fix the issue, one should either (1) only augment features that are known to be irrelevant to the label (nuisance variables), or (2) if a causal feature is transformed, the label must also be transformed accordingly. In this case, that means pairing $T(X)$ with the new label $g(Y) = 1-Y$. This procedure correctly populates the training set with valid examples like $((C,S), Y)$ and $((-C,S), 1-Y)$, which breaks the model's reliance on the spurious feature $S$ (since $S$ is seen with both labels) and forces it to learn the true causal feature $C$.\n\n**D. Incorrect.** Enforcing invariance to flips $T$ forces the model to rely entirely on the spurious feature $S$, resulting in the classifier $f_{inv}(X) = \\mathbb{I}[S=+1]$. The test accuracy of this model is exactly $q$. A model trained without augmentation could, in principle, learn the true causal feature $C$, achieving a perfect test accuracy of 1. Even a non-ideal model trained without augmentation would likely learn some combination of $C$ and $S$, and its test accuracy would be between $q$ and $1$. Therefore, the invariant model's accuracy is at best equal to, and generally worse than, the non-augmented model's accuracy. It is not guaranteed to be higher.\n\n**E. Incorrect.** \"Measure-preserving\" means that $X$ and $T(X)$ have the same probability distribution. \"Label-preserving\" means that the true label for $X$ is the same as the true label for $T(X)$. These are different concepts. A transformation can be measure-preserving but not label-preserving (e.g., swapping two i.i.d. features $X_1, X_2$ when the label depends only on $X_1$). Conversely, a transformation can be label-preserving without being measure-preserving (e.g., if the label only depends on a feature untouched by a non-measure-preserving transformation). The statement incorrectly equates these two distinct properties.", "answer": "$$\\boxed{AC}$$", "id": "3160908"}, {"introduction": "How can we learn meaningful representations from the vast unlabeled data available in the world? This hands-on practice delves into self-supervised learning, a paradigm that generates its own supervision signal by creating \"pretext\" tasks, such as predicting image rotations. Through a precise mathematical model, you will compare different pretext tasks and quantify how their design choices determine which information is preserved for a downstream objective. This exercise provides a formal understanding of how implicit invariances are learned and why aligning the pretext task with the final goal is critical [@problem_id:3160860].", "problem": "You will analyze two self-supervised learning paradigms in deep learning through a mathematically precise surrogate. The latent variable is a continuous planar orientation angle $\\theta \\in [0,2\\pi)$ measured in radians. The downstream task is to estimate $\\theta$ using only the information preserved by a pretext task. You will compare two pretext tasks that define different implicit invariances and quantify which aligns better with downstream geometry.\n\nDefinitions and setup:\n- Let $\\theta$ be a random variable with a uniform distribution on an interval $[a,b] \\subseteq [0,2\\pi)$ (angles are in radians).\n- Let $p$ be a discrete permutation label that is independent of $\\theta$.\n- The rotation prediction pretext creates a label $Y_{\\mathrm{rot}}$ by binning $\\theta$ into $K$ equal-width classes over $[0,2\\pi)$: $Y_{\\mathrm{rot}} = \\left\\lfloor K \\cdot \\theta / (2\\pi) \\right\\rfloor \\in \\{0,1,\\dots,K-1\\}$.\n- The jigsaw pretext creates a label $Y_{\\mathrm{jig}} = p$ that is independent of $\\theta$.\n\nFundamental bases to use:\n- Under squared error loss, the Bayes estimator of a target variable $Z$ given an observed variable $Y$ is the conditional mean $\\hat{Z}(Y) = \\mathbb{E}[Z \\mid Y]$, and the corresponding minimum mean squared error (Bayes risk) is $\\mathcal{R}^\\star(Y) = \\mathbb{E}\\big[(Z - \\mathbb{E}[Z \\mid Y])^2\\big] = \\mathbb{E}\\big[\\mathrm{Var}(Z \\mid Y)\\big]$.\n- Mutual Information (MI) is defined as $I(U;V) = \\mathbb{E}\\left[\\log \\frac{p_{U,V}(U,V)}{p_U(U) p_V(V)}\\right]$ and satisfies the Data Processing Inequality. Angles must be handled in radians.\n\nYour goal:\n- For each test case below, compute the following two quantities:\n  1. The alignment score $\\Delta_{\\mathrm{MSE}} = \\mathcal{R}^\\star(Y_{\\mathrm{jig}}) - \\mathcal{R}^\\star(Y_{\\mathrm{rot}})$, where $\\mathcal{R}^\\star(Y)$ is the Bayes-optimal mean squared error for estimating $\\theta$ using only $Y$. A positive value indicates that rotation prediction preserves more information about $\\theta$ than jigsaw for squared error estimation.\n  2. The mutual information $I(\\theta; Y_{\\mathrm{rot}})$ in nats.\n- Use only the mathematical principles stated above. Angles must be in radians.\n\nTest suite (angles in radians):\n- Case $1$: $K=4$, $[a,b]=[0,2\\pi)$.\n- Case $2$: $K=8$, $[a,b]=[0,2\\pi)$.\n- Case $3$: $K=4$, $[a,b]=[0,\\pi)$.\n- Case $4$: $K=1$, $[a,b]=[0,2\\pi)$.\n- Case $5$: $K=4$, $[a,b]=[0,\\pi/8)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of pairs, each pair in the form $[\\Delta_{\\mathrm{MSE}}, I(\\theta; Y_{\\mathrm{rot}})]$, in the order of the cases above, all enclosed in a single outer pair of square brackets. For example, the output should look like [[x1,y1],[x2,y2],...,[x5,y5]] with floating-point values for each $x_i$ and $y_i$.", "solution": "The problem asks for the computation of two quantities, an alignment score $\\Delta_{\\mathrm{MSE}}$ and a mutual information value $I(\\theta; Y_{\\mathrm{rot}})$, to compare two self-supervised learning pretext tasks. The target variable for estimation is a uniformly distributed angle $\\theta \\in [a,b]$.\n\n**1. Calculation of $\\mathcal{R}^\\star(Y_{\\mathrm{jig}})$**\nThe Bayes risk for the jigsaw pretext is $\\mathcal{R}^\\star(Y_{\\mathrm{jig}}) = \\mathbb{E}[\\mathrm{Var}(\\theta \\mid Y_{\\mathrm{jig}})]$. The problem states that the jigsaw label $Y_{\\mathrm{jig}}$ is independent of $\\theta$. Conditioning on an independent variable provides no information, so the conditional distribution of $\\theta$ given $Y_{\\mathrm{jig}}$ is identical to its marginal distribution, $p(\\theta \\mid Y_{\\mathrm{jig}}) = p(\\theta)$. Therefore, the conditional variance is equal to the marginal variance:\n$$ \\mathrm{Var}(\\theta \\mid Y_{\\mathrm{jig}}) = \\mathrm{Var}(\\theta) $$\nThe variance of a continuous uniform distribution on an interval $[a,b]$ is $\\frac{(b-a)^2}{12}$. Thus, the Bayes risk is the total variance of $\\theta$:\n$$ \\mathcal{R}^\\star(Y_{\\mathrm{jig}}) = \\mathrm{Var}(\\theta) = \\frac{(b-a)^2}{12} $$\nThis term represents the maximum possible estimation error, as the jigsaw pretext preserves no information about $\\theta$.\n\n**2. Calculation of $\\mathcal{R}^\\star(Y_{\\mathrm{rot}})$**\nThe Bayes risk for the rotation pretext is $\\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\mathbb{E}[\\mathrm{Var}(\\theta \\mid Y_{\\mathrm{rot}})]$. This can be computed using the law of total expectation:\n$$ \\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\sum_{k=0}^{K-1} P(Y_{\\mathrm{rot}}=k) \\cdot \\mathrm{Var}(\\theta \\mid Y_{\\mathrm{rot}}=k) $$\nThe condition $Y_{\\mathrm{rot}}=k$ implies that $\\theta$ falls into the bin $B_k = [\\frac{2\\pi k}{K}, \\frac{2\\pi (k+1)}{K})$. Given that $\\theta$ is originally uniform on $[a,b]$, the conditional distribution of $\\theta$ given $Y_{\\mathrm{rot}}=k$ is uniform over the intersection interval $I_k = [a,b] \\cap B_k$.\n\nLet $L_k$ be the length of the interval $I_k$. The probability of this event is $P(Y_{\\mathrm{rot}}=k) = \\frac{L_k}{b-a}$. The variance of a uniform distribution over an interval of length $L_k$ is $\\frac{L_k^2}{12}$. So, $\\mathrm{Var}(\\theta \\mid Y_{\\mathrm{rot}}=k) = \\frac{L_k^2}{12}$.\nSubstituting these into the formula for the risk:\n$$ \\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\sum_{k=0}^{K-1} \\left( \\frac{L_k}{b-a} \\right) \\left( \\frac{L_k^2}{12} \\right) = \\frac{1}{12(b-a)} \\sum_{k=0}^{K-1} L_k^3 $$\n\n**3. Calculation of $\\Delta_{\\mathrm{MSE}}$**\nThe alignment score is the difference between the two risks:\n$$ \\Delta_{\\mathrm{MSE}} = \\mathcal{R}^\\star(Y_{\\mathrm{jig}}) - \\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\frac{(b-a)^2}{12} - \\frac{1}{12(b-a)} \\sum_{k=0}^{K-1} L_k^3 $$\n\n**4. Calculation of $I(\\theta; Y_{\\mathrm{rot}})$**\nThe mutual information $I(\\theta; Y_{\\mathrm{rot}})$ is given by $H(Y_{\\mathrm{rot}}) - H(Y_{\\mathrm{rot}} \\mid \\theta)$. Since $Y_{\\mathrm{rot}}$ is a deterministic function of $\\theta$, the conditional entropy $H(Y_{\\mathrm{rot}} \\mid \\theta) = 0$. Thus, the mutual information simplifies to the entropy of the label $Y_{\\mathrm{rot}}$:\n$$ I(\\theta; Y_{\\mathrm{rot}}) = H(Y_{\\mathrm{rot}}) = -\\sum_{k \\text{ s.t. } p_k>0} p_k \\log(p_k) $$\nwhere $p_k = P(Y_{\\mathrm{rot}}=k) = \\frac{L_k}{b-a}$ and the logarithm is the natural log (units are nats).\n\n**Applying to Test Cases**\n- **Case 1  2**: $\\theta$ is uniform on $[0, 2\\pi)$. All $K$ bins have equal length $L_k = 2\\pi/K$ and equal probability $p_k=1/K$. The risk is reduced, and $\\Delta_{\\mathrm{MSE}} > 0$. The mutual information is $I = \\log(K)$.\n- **Case 3**: $\\theta$ is uniform on $[0, \\pi)$. Only the first two of the four bins have non-zero probability. $p_0=p_1=1/2$. The risk is reduced, and $I = \\log(2)$.\n- **Case 4**: $K=1$. There is only one bin. $Y_{\\mathrm{rot}}$ is always 0, providing no information. The risk reduction is zero ($\\Delta_{\\mathrm{MSE}}=0$) and the mutual information is zero.\n- **Case 5**: $K=4$, but the range of $\\theta$ is $[0, \\pi/8)$, which is entirely inside the first bin. Again, $Y_{\\mathrm{rot}}$ is always 0, providing no information. $\\Delta_{\\mathrm{MSE}}=0$ and $I=0$.\n\nThe provided Python code implements these calculations for each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_metrics(K, a, b):\n    \"\"\"\n    Calculates the alignment score Delta_MSE and mutual information I(theta; Y_rot).\n\n    Args:\n        K (int): The number of bins for rotation prediction.\n        a (float): The start of the uniform interval for theta.\n        b (float): The end of the uniform interval for theta.\n\n    Returns:\n        list: A list containing [Delta_MSE, I(theta; Y_rot)].\n    \"\"\"\n    # Edge case: if a=b, theta is a constant. All variances and MIs are 0.\n    if np.isclose(a, b):\n        return [0.0, 0.0]\n    \n    b_minus_a = b - a\n\n    # Calculate Risk for Jigsaw Pretext\n    # R_jig = Var(theta) for theta ~ U(a, b)\n    risk_jig = (b_minus_a)**2 / 12.0\n\n    # Calculate Risk for Rotation Pretext\n    sum_L_cubed = 0.0\n    L_values = []\n    \n    for k in range(K):\n        bin_start = 2.0 * np.pi * k / K\n        bin_end = 2.0 * np.pi * (k + 1.0) / K\n        \n        # Calculate intersection of [a, b] and [bin_start, bin_end)\n        intersect_start = max(a, bin_start)\n        intersect_end = min(b, bin_end)\n        \n        Lk = max(0.0, intersect_end - intersect_start)\n        \n        sum_L_cubed += Lk**3\n        L_values.append(Lk)\n\n    # R_rot = E[Var(theta | Y_rot)]\n    risk_rot = sum_L_cubed / (12.0 * b_minus_a)\n    \n    # Calculate Delta_MSE\n    delta_mse = risk_jig - risk_rot\n\n    # Calculate Mutual Information I(theta; Y_rot)\n    # I(theta; Y_rot) = H(Y_rot)\n    mutual_info = 0.0\n    for Lk in L_values:\n        if Lk > 1e-9: # Use a small epsilon to avoid log(0) from float precision issues\n            pk = Lk / b_minus_a\n            # Ensure pk is not slightly > 1 due to float precision\n            pk = min(pk, 1.0) \n            mutual_info -= pk * np.log(pk)\n\n    return [delta_mse, mutual_info]\n\n\ndef solve():\n    \"\"\"\n    Defines test cases, computes results, and prints them in the specified format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (4, 0.0, 2.0 * np.pi),  # Case 1\n        (8, 0.0, 2.0 * np.pi),  # Case 2\n        (4, 0.0, np.pi),        # Case 3\n        (1, 0.0, 2.0 * np.pi),  # Case 4\n        (4, 0.0, np.pi / 8.0)    # Case 5\n    ]\n\n    results_str = []\n    for case in test_cases:\n        K, a, b = case\n        result = calculate_metrics(K, a, b)\n        # Format each pair as [x,y] without spaces\n        results_str.append(f\"[{result[0]},{result[1]}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3160860"}]}