{"hands_on_practices": [{"introduction": "Feature engineering is the art and science of translating domain knowledge into numerical representations that a machine learning model can understand. This process often begins with raw, structured data from a specific field, such as biology or finance. In this first practice, you will step into the role of a computational biologist to engineer a feature that captures a fundamental biophysical property of proteins. By implementing a program to calculate the ratio of charged to hydrophobic residues on a protein's surface, you will gain hands-on experience in parsing domain-specific data, applying thresholds, and handling the practical edge cases that arise when creating meaningful features from scratch [@problem_id:2389794].", "problem": "You are given the task of engineering a scalar feature for proteins that captures the ratio of charged residues to hydrophobic residues on the solvent-exposed surface. Use the following definitions and conventions.\n\n- The set of charged residues is $\\mathcal{C}=\\{R,K,D,E\\}$.\n- The set of hydrophobic residues is $\\mathcal{H}=\\{A,V,L,I,P,F,M,W\\}$.\n- A protein is represented by a one-letter amino acid sequence $s$ of length $n$ and a corresponding vector of relative solvent accessibility values $a=(a_1,\\dots,a_n)$, where each $a_i$ is the relative solvent accessibility (RSA) in the closed interval $[0,1]$. Relative solvent accessibility is defined as the solvent accessible surface area (SASA) of a residue divided by the maximum SASA of that residue type.\n- A residue at index $i$ is considered surface-exposed if $a_i \\ge \\tau$, where $\\tau \\in [0,1]$ is a given threshold.\n\nLet the set of surface indices be $S=\\{i \\in \\{1,\\dots,n\\}\\,:\\,a_i \\ge \\tau\\}$. Define the numerator $n_c$ and denominator $n_h$ as\n$$\nn_c = \\sum_{i \\in S} \\mathbf{1}\\{s_i \\in \\mathcal{C}\\}, \\quad\nn_h = \\sum_{i \\in S} \\mathbf{1}\\{s_i \\in \\mathcal{H}\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The desired feature is the ratio\n$$\nr =\n\\begin{cases}\n\\frac{n_c}{n_h}, & \\text{if } n_h > 0,\\\\\n+\\infty, & \\text{if } n_h = 0 \\text{ and } n_c > 0,\\\\\n0, & \\text{if } n_h = 0 \\text{ and } n_c = 0.\n\\end{cases}\n$$\nAny residue symbol not in the standard $20$ amino acids, or ambiguous/unknown symbols (for example, $X$, $B$, $Z$, $J$), must be ignored for the purpose of counting; such residues do not contribute to $n_c$ or $n_h$ regardless of their relative solvent accessibility.\n\nImplement a program that, for each test case below, computes $r$ as defined. For finite $r$, round to six decimal places. If $r=+\\infty$, output it as the string inf. The final output must be a single line containing the results for all test cases in order, formatted as a comma-separated list enclosed in square brackets, for example, $[x_1,x_2,x_3]$.\n\nTest suite:\n- Case $1$: $s=$ \"ACDEFGHIKLMNPQ\", $a=$ [$0.8$, $0.1$, $0.9$, $0.6$, $0.3$, $0.2$, $0.7$, $0.75$, $0.05$, $0.4$, $0.9$, $0.12$, $0.8$, $0.2$], $\\tau = 0.5$.\n- Case $2$: $s=$ \"RKDEAVILMWFP\", $a=$ [$0.9$, $0.8$, $0.7$, $0.31$, $0.1$, $0.2$, $0.29$, $0.0$, $0.05$, $0.2$, $0.1$, $0.28$], $\\tau = 0.3$.\n- Case $3$: $s=$ \"AVLIWFPAMGTC\", $a=$ [$0.8$, $0.6$, $0.7$, $0.9$, $0.2$, $0.55$, $0.52$, $0.1$, $0.51$, $0.05$, $0.2$, $0.49$], $\\tau = 0.5$.\n- Case $4$: $s=$ \"RKDEAVXGHNQ\", $a=$ [$0.1$, $0.2$, $0.3$, $0.4$, $0.6$, $0.5$, $1.0$, $0.8$, $0.9$, $0.94$, $0.2$], $\\tau = 0.95$.\n- Case $5$: $s=$ \"BEZJRPFA\", $a=$ [$0.5$, $0.5$, $0.5$, $0.5$, $0.5$, $0.5$, $0.5$, $0.5$], $\\tau = 0.5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ corresponds to the ratio for Case $i$ in the given order. For finite values, print exactly six digits after the decimal point; for infinite values, print inf.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n-   The set of charged residues is $\\mathcal{C}=\\{R,K,D,E\\}$.\n-   The set of hydrophobic residues is $\\mathcal{H}=\\{A,V,L,I,P,F,M,W\\}$.\n-   A protein is described by a sequence $s$ of length $n$ and a vector of relative solvent accessibility (RSA) values $a=(a_1,\\dots,a_n)$, where $a_i \\in [0,1]$.\n-   A residue at index $i$ is surface-exposed if $a_i \\ge \\tau$, where $\\tau \\in [0,1]$ is a threshold.\n-   The set of surface-exposed indices is $S=\\{i \\in \\{1,\\dots,n\\}\\,:\\,a_i \\ge \\tau\\}$.\n-   The count of surface-exposed charged residues is $n_c = \\sum_{i \\in S} \\mathbf{1}\\{s_i \\in \\mathcal{C}\\}$.\n-   The count of surface-exposed hydrophobic residues is $n_h = \\sum_{i \\in S} \\mathbf{1}\\{s_i \\in \\mathcal{H}\\}$.\n-   The feature ratio $r$ is defined as:\n    $$\n    r =\n    \\begin{cases}\n    \\frac{n_c}{n_h}, & \\text{if } n_h > 0,\\\\\n    +\\infty, & \\text{if } n_h = 0 \\text{ and } n_c > 0,\\\\\n    0, & \\text{if } n_h = 0 \\text{ and } n_c = 0.\n    \\end{cases}\n    $$\n-   Any residue symbol not in the standard $20$ amino acids, or ambiguous symbols (e.g., $X, B, Z, J$), are ignored.\n-   Test cases are provided with sequences $s$, RSA vectors $a$, and thresholds $\\tau$.\n-   Final-value formatting requires rounding to six decimal places for finite values and representing $+\\infty$ as the string `inf`.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is evaluated against the established criteria:\n-   **Scientifically Grounded**: The problem is based on fundamental concepts in structural bioinformatics. The classification of amino acids into charged and hydrophobic groups is standard. Relative Solvent Accessibility (RSA) is a well-established measure for quantifying residue exposure. Using a threshold on RSA to distinguish surface residues from buried ones is a common and valid simplification. The engineered feature, a ratio of charged to hydrophobic residues on the surface, is a plausible proxy for certain biophysical properties of a protein, such as its interaction with a polar solvent. The problem is scientifically sound.\n-   **Well-Posed**: The problem is stated with mathematical precision. All inputs ($s, a, \\tau$) are clearly specified for each test case. The procedure for calculating the counts $n_c$ and $n_h$ is unambiguous. The definition of the ratio $r$, including the conditional logic for handling the case when the denominator $n_h$ is zero, is exhaustive and ensures that a unique, stable, and meaningful solution exists for any valid input.\n-   **Objective**: The problem description uses objective, formal language. All terms are explicitly defined. There are no subjective claims or opinions.\n\nThe problem is determined to be complete, consistent, realistic, and well-structured. It does not exhibit any of the flaws that would render it invalid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A reasoned solution will be provided.\n\n**Solution Procedure**\n\nThe objective is to compute a scalar feature $r$ representing the ratio of charged to hydrophobic residues on a protein's solvent-exposed surface. The procedure for any given test case, consisting of a sequence $s$, an RSA vector $a$, and a threshold $\\tau$, is as follows.\n\n1.  **Identify Surface-Exposed Residues**: First, we identify the set of indices $S$ corresponding to residues on the protein surface. An amino acid at index $i$ (using $1$-based indexing as in the problem text) is considered surface-exposed if its RSA value $a_i$ is greater than or equal to the given threshold $\\tau$. That is, $S = \\{i \\in \\{1, \\dots, n\\} \\mid a_i \\ge \\tau\\}$, where $n$ is the length of the sequence $s$.\n\n2.  **Count Residue Types**: Next, we iterate through the indices in the set $S$. For each such index $i$, we examine the amino acid $s_i$.\n    -   We initialize two counters: $n_c = 0$ for charged residues and $n_h = 0$ for hydrophobic residues.\n    -   The set of charged residues is $\\mathcal{C} = \\{R, K, D, E\\}$.\n    -   The set of hydrophobic residues is $\\mathcal{H} = \\{A, V, L, I, P, F, M, W\\}$.\n    -   For each $i \\in S$, if the residue $s_i \\in \\mathcal{C}$, we increment $n_c$. If $s_i \\in \\mathcal{H}$, we increment $n_h$. Residues not belonging to either set, including ambiguous or non-standard symbols like $X$, $B$, $Z$, and $J$, are ignored and do not contribute to either count.\n\n3.  **Calculate the Ratio $r$**: Finally, we compute the ratio $r$ based on the values of $n_c$ and $n_h$, following the specified tripartite logic:\n    -   If $n_h > 0$, the ratio is $r = \\frac{n_c}{n_h}$.\n    -   If $n_h = 0$ and $n_c > 0$, the ratio is defined as $r = +\\infty$.\n    -   If $n_h = 0$ and $n_c = 0$, the ratio is defined as $r = 0$.\n\nThis procedure is applied to each test case.\n\n**Analysis of Test Cases**\n\n-   **Case 1**: $s=$ \"ACDEFGHIKLMNPQ\", $a=$ [$0.8$, $0.1$, $0.9$, $0.6$, $0.3$, $0.2$, $0.7$, $0.75$, $0.05$, $0.4$, $0.9$, $0.12$, $0.8$, $0.2$], $\\tau = 0.5$.\n    -   Surface indices $S$ (indices $i$ where $a_i \\ge 0.5$): {$1, 3, 4, 7, 8, 11, 13$}.\n    -   Surface residues: $s_1=A, s_3=D, s_4=E, s_7=H, s_8=I, s_{11}=M, s_{13}=P$.\n    -   Charged residues on surface ($s_i \\in \\mathcal{C}$): $D, E$. Thus, $n_c = 2$.\n    -   Hydrophobic residues on surface ($s_i \\in \\mathcal{H}$): $A, I, M, P$. Thus, $n_h = 4$.\n    -   Since $n_h > 0$, $r = \\frac{n_c}{n_h} = \\frac{2}{4} = 0.5$.\n\n-   **Case 2**: $s=$ \"RKDEAVILMWFP\", $a=$ [$0.9$, $0.8$, $0.7$, $0.31$, $0.1$, $0.2$, $0.29$, $0.0$, $0.05$, $0.2$, $0.1$, $0.28$], $\\tau = 0.3$.\n    -   Surface indices $S$ (indices $i$ where $a_i \\ge 0.3$): {$1, 2, 3, 4$}.\n    -   Surface residues: $s_1=R, s_2=K, s_3=D, s_4=E$.\n    -   Charged residues on surface: $R, K, D, E$. Thus, $n_c = 4$.\n    -   Hydrophobic residues on surface: None. Thus, $n_h = 0$.\n    -   Since $n_h = 0$ and $n_c > 0$, $r = +\\infty$.\n\n-   **Case 3**: $s=$ \"AVLIWFPAMGTC\", $a=$ [$0.8$, $0.6$, $0.7$, $0.9$, $0.2$, $0.55$, $0.52$, $0.1$, $0.51$, $0.05$, $0.2$, $0.49$], $\\tau = 0.5$.\n    -   Surface indices $S$ (indices $i$ where $a_i \\ge 0.5$): {$1, 2, 3, 4, 6, 7, 9$}.\n    -   Surface residues: $s_1=A, s_2=V, s_3=L, s_4=I, s_6=F, s_7=P, s_9=A$.\n    -   Charged residues on surface: None. Thus, $n_c = 0$.\n    -   Hydrophobic residues on surface: $A, V, L, I, F, P, A$. Thus, $n_h = 7$.\n    -   Since $n_h > 0$, $r = \\frac{n_c}{n_h} = \\frac{0}{7} = 0$.\n\n-   **Case 4**: $s=$ \"RKDEAVXGHNQ\", $a=$ [$0.1$, $0.2$, $0.3$, $0.4$, $0.6$, $0.5$, $1.0$, $0.8$, $0.9$, $0.94$, $0.2$], $\\tau = 0.95$.\n    -   Surface indices $S$ (indices $i$ where $a_i \\ge 0.95$): {$7$}.\n    -   Surface residue: $s_7=X$.\n    -   The residue $X$ is non-standard and is ignored. It contributes to neither $n_c$ nor $n_h$.\n    -   Charged residues on surface: None. Thus, $n_c = 0$.\n    -   Hydrophobic residues on surface: None. Thus, $n_h = 0$.\n    -   Since $n_h = 0$ and $n_c = 0$, $r = 0$.\n\n-   **Case 5**: $s=$ \"BEZJRPFA\", $a=$ [$0.5$, $0.5$, $0.5$, $0.5$, $0.5$, $0.5$, $0.5$, $0.5$], $\\tau = 0.5$.\n    -   Surface indices $S$ (indices $i$ where $a_i \\ge 0.5$): {$1, 2, 3, 4, 5, 6, 7, 8$}. All residues are on the surface.\n    -   Surface residues: $B, E, Z, J, R, P, F, A$.\n    -   The ambiguous/non-standard residues $B, Z, J$ are ignored.\n    -   Charged residues on surface from the remaining set $\\{E, R, P, F, A\\}$: $E, R$. Thus, $n_c = 2$.\n    -   Hydrophobic residues on surface from the remaining set: $P, F, A$. Thus, $n_h = 3$.\n    -   Since $n_h > 0$, $r = \\frac{n_c}{n_h} = \\frac{2}{3} \\approx 0.666667$.\n\nThe implementation will follow this logic precisely.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating the ratio of charged to hydrophobic \n    residues on the solvent-exposed surface of proteins for a given test suite.\n    \"\"\"\n    \n    test_cases = [\n        {\"s\": \"ACDEFGHIKLMNPQ\", \"a\": [0.8, 0.1, 0.9, 0.6, 0.3, 0.2, 0.7, 0.75, 0.05, 0.4, 0.9, 0.12, 0.8, 0.2], \"tau\": 0.5},\n        {\"s\": \"RKDEAVILMWFP\", \"a\": [0.9, 0.8, 0.7, 0.31, 0.1, 0.2, 0.29, 0.0, 0.05, 0.2, 0.1, 0.28], \"tau\": 0.3},\n        {\"s\": \"AVLIWFPAMGTC\", \"a\": [0.8, 0.6, 0.7, 0.9, 0.2, 0.55, 0.52, 0.1, 0.51, 0.05, 0.2, 0.49], \"tau\": 0.5},\n        {\"s\": \"RKDEAVXGHNQ\", \"a\": [0.1, 0.2, 0.3, 0.4, 0.6, 0.5, 1.0, 0.8, 0.9, 0.94, 0.2], \"tau\": 0.95},\n        {\"s\": \"BEZJRPFA\", \"a\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], \"tau\": 0.5},\n    ]\n\n    charged_residues = {'R', 'K', 'D', 'E'}\n    hydrophobic_residues = {'A', 'V', 'L', 'I', 'P', 'F', 'M', 'W'}\n\n    results = []\n    \n    for case in test_cases:\n        s = case[\"s\"]\n        a = np.array(case[\"a\"])\n        tau = case[\"tau\"]\n        \n        n_c = 0.0  # Numerator: Count of charged surface residues\n        n_h = 0.0  # Denominator: Count of hydrophobic surface residues\n        \n        # Identify surface-exposed indices\n        surface_indices = np.where(a >= tau)[0]\n        \n        for i in surface_indices:\n            residue = s[i]\n            if residue in charged_residues:\n                n_c += 1\n            elif residue in hydrophobic_residues:\n                n_h += 1\n        \n        if n_h > 0:\n            ratio = n_c / n_h\n            results.append(f\"{ratio:.6f}\")\n        elif n_h == 0 and n_c > 0:\n            results.append(\"inf\")\n        elif n_h == 0 and n_c == 0:\n            ratio = 0.0\n            results.append(f\"{ratio:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2389794"}, {"introduction": "Once features are engineered, their statistical properties can dramatically affect the performance and stability of a model. Raw features, especially those from real-world measurements, often contain extreme values or follow heavy-tailed distributions that can mislead feature selection methods like those based on Pearson correlation. This exercise provides a hands-on simulation to explore this very issue. You will learn how to implement quantile normalization—a powerful rank-based transformation—to make your features more robust, and you will quantitatively measure how this technique improves the stability of your feature selection process in the presence of outliers [@problem_id:3124173].", "problem": "You are asked to design and implement a complete program that examines how quantile normalization of features affects feature selection in the presence of heavy-tailed distributions and injected outliers. The program must generate synthetic data, perform feature selection using empirical Pearson correlation, apply quantile normalization as a rank-based transform to a standard normal distribution, and measure two quantities for each test case: the shift in the selected feature set caused by quantile normalization under outliers and the gain in robustness to outliers due to quantile normalization, quantified by changes in set similarity with respect to a clean baseline.\n\nThe fundamental base for this task consists of the following well-tested definitions and facts:\n\n- Empirical Pearson correlation between two real-valued variables is a measure of linear association defined from observations and is sensitive to extreme values.\n- Rank-based quantile normalization uses empirical ranks to transform each feature to a target distribution via the inverse cumulative distribution function, reducing the leverage of extreme values while preserving order relations.\n- Heavy-tailed distributions generate extreme observations more frequently than light-tailed distributions, altering empirical moments in finite samples. The Student's t-distribution with low degrees of freedom is a canonical heavy-tailed model.\n- Set similarity can be quantified using the Jaccard index, which compares intersection and union cardinalities and yields a real number in the interval $[0,1]$.\n\nYou must adhere to the following mathematically precise specification:\n\n1. Data generation:\n   - Let $n$ denote the number of samples and let $d$ denote the number of features. Draw the feature matrix $X \\in \\mathbb{R}^{n \\times d}$ with entries independently sampled from a Student's t-distribution with degrees of freedom $\\nu$. This distribution is heavy-tailed for small $\\nu$.\n   - Choose a subset of indices $S_{\\text{true}} \\subset \\{0,1,\\dots,d-1\\}$ of size $s$ to be the signal features.\n   - Let $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{s}$ denote fixed coefficients sampled once for each test case.\n   - Define the target as $y = X_{:,S_{\\text{true}}}\\boldsymbol{\\alpha} + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ independently of $X$.\n   - Produce two versions of the feature matrix: a clean version $X_{\\text{clean}}$ and an outlier-corrupted version $X_{\\text{out}}$. For outlier corruption, for each entry independently, with probability $p$ add a perturbation of the form $m \\cdot u$, where $m > 0$ is the outlier magnitude and $u \\in \\{-1,+1\\}$ is a random sign.\n\n2. Quantile normalization:\n   - For each feature (each column of $X$), compute empirical ranks $r_i \\in \\{1,2,\\dots,n\\}$ of its values. Map each rank to a probability $q_i = (r_i - 0.5)/n \\in (0,1)$.\n   - Transform to a standard normal distribution by $z_i = \\Phi^{-1}(q_i)$ where $\\Phi^{-1}$ is the inverse cumulative distribution function of the standard normal distribution. Replace the original feature values with $z_i$ for that feature. Apply this transform independently to each feature.\n\n3. Feature selection:\n   - For a specified integer $k$ with $1 \\le k \\le d$, compute the absolute empirical Pearson correlation $\\rho_j = \\left|\\text{corr}(X_{:,j}, y)\\right|$ for each feature index $j \\in \\{0,1,\\dots,d-1\\}$.\n   - Select the set $S_k$ of $k$ feature indices corresponding to the largest $k$ values of $\\rho_j$, with ties broken by increasing index.\n\n4. Evaluation metrics:\n   - Define the shift due to quantile normalization under outliers as the size of the symmetric difference between the sets selected from outlier-corrupted data without and with quantile normalization, namely\n     $$\\Delta = \\left|S_k(X_{\\text{out}}) \\triangle S_k(\\text{QN}(X_{\\text{out}}))\\right|,$$\n     where $\\text{QN}(\\cdot)$ denotes per-feature quantile normalization and $\\triangle$ denotes symmetric difference.\n   - Define robustness to outliers via the Jaccard index between the selected set from clean data and from outlier data. Compute\n     $$J_{\\text{raw}} = \\frac{\\left|S_k(X_{\\text{clean}}) \\cap S_k(X_{\\text{out}})\\right|}{\\left|S_k(X_{\\text{clean}}) \\cup S_k(X_{\\text{out}})\\right|}, \\quad J_{\\text{qn}} = \\frac{\\left|S_k(\\text{QN}(X_{\\text{clean}})) \\cap S_k(\\text{QN}(X_{\\text{out}}))\\right|}{\\left|S_k(\\text{QN}(X_{\\text{clean}})) \\cup S_k(\\text{QN}(X_{\\text{out}}))\\right|}.$$\n     Report the robustness gain $G = J_{\\text{qn}} - J_{\\text{raw}}$.\n\n5. Program output:\n   - For each test case, output a list $[\\Delta, G]$ containing the symmetric difference size $\\Delta$ as an integer and the robustness gain $G$ as a float.\n   - Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets. Each element of this list must be the two-element list $[\\Delta, G]$ for one test case, e.g., $\\left[[\\Delta_1,G_1],[\\Delta_2,G_2],\\dots\\right]$.\n\nUse the following test suite. For all test cases, ensure that the random generation is reproducible by using the provided seeds. All quantities are dimensionless, and angles are not involved.\n\n- Test case $1$ (general case):\n  - $n=200$, $d=20$, $k=5$, $\\nu=3$, $p=0.05$, $m=25.0$, $\\sigma=1.0$, $s=5$, seed $=42$.\n- Test case $2$ (heavier tails and more outliers):\n  - $n=200$, $d=20$, $k=5$, $\\nu=1$, $p=0.10$, $m=50.0$, $\\sigma=1.0$, $s=5$, seed $=123$.\n- Test case $3$ (no outliers for baseline comparison):\n  - $n=200$, $d=20$, $k=5$, $\\nu=3$, $p=0.0$, $m=0.0$, $\\sigma=1.0$, $s=5$, seed $=7$.\n- Test case $4$ (small sample size and pronounced outliers):\n  - $n=50$, $d=20$, $k=5$, $\\nu=2$, $p=0.10$, $m=40.0$, $\\sigma=1.0$, $s=5$, seed $=2024$.\n- Test case $5$ (higher dimensional feature space):\n  - $n=300$, $d=100$, $k=10$, $\\nu=5$, $p=0.05$, $m=30.0$, $\\sigma=1.0$, $s=8$, seed $=314159$.\n\nYour program must implement the steps above precisely and output a single line of the form $\\left[[\\Delta_1,G_1],[\\Delta_2,G_2],[\\Delta_3,G_3],[\\Delta_4,G_4],[\\Delta_5,G_5]\\right]$ with numerical values computed from your implementation.", "solution": "The problem is valid. It presents a well-defined and scientifically grounded task in computational statistics to evaluate the impact of quantile normalization on feature selection robustness. The problem is self-contained, with all parameters and procedures specified, allowing for a unique and reproducible solution. The minor ambiguity regarding the sampling of signal coefficients and selection of true feature indices is resolved by adopting standard practices in synthetic data generation, which does not compromise the problem's integrity. These assumptions are that the signal coefficients $\\boldsymbol{\\alpha}$ are drawn from a standard normal distribution and the set of true signal features $S_{\\text{true}}$ is chosen uniformly at random.\n\nThe solution is implemented by following the sequence of steps specified in the problem statement. The process for each test case involves data generation, feature transformation, feature selection, and the calculation of evaluation metrics.\n\n**1. Data Generation**\n\nThe foundation of this analysis is a synthetic dataset designed to model the challenges of real-world data, namely heavy-tailed distributions and the presence of outliers. For each test case, we generate a feature matrix $X \\in \\mathbb{R}^{n \\times d}$, where $n$ is the number of samples and $d$ is the number of features.\n\n-   **Base Feature Distribution**: The entries of the clean feature matrix, $X_{\\text{clean}}$, are independent and identically distributed samples from a Student's t-distribution with $\\nu$ degrees of freedom. This distribution is chosen for its heavy tails, which, for small $\\nu$, produce extreme values more frequently than a normal distribution, thus challenging statistical methods sensitive to such values.\n-   **Target Variable**: A ground-truth relationship between features and a target variable $y$ is established. A subset of $s$ features, indexed by the set $S_{\\text{true}}$, is randomly chosen to be the \"signal\" features. Their corresponding coefficients, $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{s}$, are sampled from a standard normal distribution $\\mathcal{N}(0, 1)$. The target $y$ is then constructed as a linear combination of these signal features, with additive Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$:\n    $$y = X_{\\text{clean},:,S_{\\text{true}}}\\boldsymbol{\\alpha} + \\varepsilon$$\n-   **Outlier Corruption**: To simulate data corruption, an outlier-injected matrix $X_{\\text{out}}$ is created from $X_{\\text{clean}}$. Each element of $X_{\\text{clean}}$ is corrupted with a probability $p$. A corrupted element has an outlier term $m \\cdot u$ added to it, where $m > 0$ is a large magnitude and $u$ is a random sign, either $+1$ or $-1$. This process introduces sparse, high-leverage outliers that can severely distort statistical measures like Pearson correlation.\n\n**2. Quantile Normalization**\n\nQuantile normalization is a non-linear transformation designed to mitigate the influence of a feature's original distribution, particularly extreme values. The procedure is applied independently to each feature column.\n\n1.  For each feature vector, the ranks $r_i$ of its $n$ data points are computed, ranging from $1$ to $n$.\n2.  These ranks are converted into empirical quantiles using the formula $q_i = \\frac{r_i - 0.5}{n}$. This maps the ranks to the interval $(0, 1)$.\n3.  Each quantile $q_i$ is then mapped to a value $z_i$ from a target distribution, which in this case is the standard normal distribution $\\mathcal{N}(0, 1)$. This is achieved by applying the inverse cumulative distribution function (CDF) of the standard normal distribution, also known as the probit function: $z_i = \\Phi^{-1}(q_i)$.\n\nThe resulting feature vector consists of values $\\{z_1, z_2, \\dots, z_n\\}$, which follow a standard normal distribution by construction. This rank-based approach preserves the order of the data points but replaces their magnitudes, effectively neutralizing the impact of outliers.\n\n**3. Feature Selection**\n\nThe feature selection method is based on the empirical Pearson correlation coefficient, which measures linear association.\n\n-   For a given feature matrix (e.g., $X_{\\text{out}}$ or $\\text{QN}(X_{\\text{out}})$) and the fixed target $y$, the absolute Pearson correlation $\\rho_j = |\\text{corr}(X_{:,j}, y)|$ is calculated for each feature $j \\in \\{0, 1, \\dots, d-1\\}$.\n-   The $k$ features with the highest $\\rho_j$ values are selected. To ensure a deterministic outcome, ties in correlation values are resolved by selecting the feature with the smaller index. This is implemented using a lexicographical sort on the primary key (descending absolute correlation) and the secondary key (ascending feature index).\n\n**4. Evaluation Metrics**\n\nTwo metrics are computed to quantify the effect of quantile normalization.\n\n-   **Shift due to Normalization ($\\Delta$)**: This metric captures how much the set of selected features changes when quantile normalization is applied to the outlier-corrupted data. It is the size of the symmetric difference between the feature set selected from the raw outlier data, $S_k(X_{\\text{out}})$, and the set selected from the normalized outlier data, $S_k(\\text{QN}(X_{\\text{out}}))$:\n    $$\\Delta = \\left|S_k(X_{\\text{out}}) \\triangle S_k(\\text{QN}(X_{\\text{out}}))\\right|$$\n-   **Robustness Gain ($G$)**: This metric assesses whether quantile normalization improves the stability of the feature selection process in the presence of outliers. It compares the consistency of feature selection between clean and outlier-corrupted data, with and without normalization. This consistency is measured by the Jaccard index, $J(A, B) = |A \\cap B| / |A \\cup B|$.\n    -   The Jaccard index for raw data is $J_{\\text{raw}} = J(S_k(X_{\\text{clean}}), S_k(X_{\\text{out}}))$.\n    -   The Jaccard index for quantile-normalized data is $J_{\\text{qn}} = J(S_k(\\text{QN}(X_{\\text{clean}})), S_k(\\text{QN}(X_{\\text{out}})))$.\n    -   The robustness gain is the difference: $G = J_{\\text{qn}} - J_{\\text{raw}}$. A positive value of $G$ indicates that quantile normalization makes the feature selection process more robust to outliers.\n\nThe overall program iterates through each test case, performs these steps using a specified random seed for reproducibility, and collects the resulting $[\\Delta, G]$ pairs for final output.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t, norm, rankdata\n\ndef quantile_normalize(X):\n    \"\"\"\n    Applies quantile normalization to each column of X, mapping to a standard\n    normal distribution.\n    \"\"\"\n    n_samples = X.shape[0]\n    ranks = rankdata(X, axis=0)  # Ranks are from 1 to n\n    quantiles = (ranks - 0.5) / n_samples\n    X_norm = norm.ppf(quantiles)\n    return X_norm\n\ndef select_features(X, y, k):\n    \"\"\"\n    Selects top k features based on absolute Pearson correlation with y.\n    Ties are broken by selecting the smaller feature index.\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    # Vectorized computation of Pearson correlation\n    X_c = X - np.mean(X, axis=0)\n    y_c = y - np.mean(y)\n\n    # (n-1) * cov = sum((X_i - mu_X) * (y - mu_y)) = X_c.T @ y_c\n    cov_xy = X_c.T @ y_c\n    \n    std_X = np.std(X, axis=0, ddof=1)\n    std_y = np.std(y, ddof=1)\n    \n    corr = np.zeros(n_features)\n    # Mask to avoid division by zero for constant features or if y is constant\n    valid_mask = (std_X > 0) & (std_y > 0)\n    if np.any(valid_mask):\n      denominator = std_X[valid_mask] * std_y\n      corr[valid_mask] = (cov_xy[valid_mask] / (n_samples - 1)) / denominator\n    \n    abs_corr = np.abs(corr)\n    \n    # Tie-breaking: sort by descending abs_corr, then ascending index.\n    # np.lexsort sorts by the last key first.\n    indices = np.arange(n_features)\n    sorted_indices = np.lexsort((indices, -abs_corr))\n    \n    top_k_indices = sorted_indices[:k]\n    \n    return set(top_k_indices)\n\ndef jaccard_index(set1, set2):\n    \"\"\"Computes the Jaccard index between two sets.\"\"\"\n    intersection_size = len(set1.intersection(set2))\n    union_size = len(set1.union(set2))\n    if union_size == 0:\n        return 1.0\n    return intersection_size / union_size\n\ndef run_simulation(n, d, k, nu, p, m, sigma, s, seed):\n    \"\"\"\n    Runs one full simulation for a given set of parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Data Generation\n    # Generate clean feature matrix X from Student's t-distribution\n    X_clean = t.rvs(df=nu, size=(n, d), random_state=rng)\n    \n    # Choose true signal features and coefficients\n    S_true_indices = rng.choice(d, size=s, replace=False)\n    alpha_coeffs = rng.standard_normal(size=s)\n    \n    # Generate target variable y\n    signal = X_clean[:, S_true_indices] @ alpha_coeffs\n    noise = rng.normal(loc=0.0, scale=sigma, size=n)\n    y = signal + noise\n    \n    # Generate outlier-corrupted data\n    X_out = X_clean.copy()\n    outlier_mask = rng.random(size=(n, d)) < p\n    signs = rng.choice([-1, 1], size=(n, d))\n    X_out[outlier_mask] += signs[outlier_mask] * m\n\n    # 2. Quantile Normalization\n    X_clean_qn = quantile_normalize(X_clean)\n    X_out_qn = quantile_normalize(X_out)\n\n    # 3. Feature Selection for all four data versions\n    S_clean = select_features(X_clean, y, k)\n    S_out = select_features(X_out, y, k)\n    S_clean_qn = select_features(X_clean_qn, y, k)\n    S_out_qn = select_features(X_out_qn, y, k)\n\n    # 4. Evaluation Metrics\n    # Shift Delta\n    delta = len(S_out.symmetric_difference(S_out_qn))\n    \n    # Robustness Gain G\n    J_raw = jaccard_index(S_clean, S_out)\n    J_qn = jaccard_index(S_clean_qn, S_out_qn)\n    G = J_qn - J_raw\n    \n    return [delta, G]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (n, d, k, nu, p, m, sigma, s, seed)\n        (200, 20, 5, 3, 0.05, 25.0, 1.0, 5, 42),\n        (200, 20, 5, 1, 0.10, 50.0, 1.0, 5, 123),\n        (200, 20, 5, 3, 0.0, 0.0, 1.0, 5, 7),\n        (50, 20, 5, 2, 0.10, 40.0, 1.0, 5, 2024),\n        (300, 100, 10, 5, 0.05, 30.0, 1.0, 8, 314159),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(*params)\n        results.append(result)\n\n    # Format the output string precisely as [[d1,g1],[d2,g2],...]\n    formatted_results = [f\"[{d},{g}]\" for d, g in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3124173"}, {"introduction": "Perhaps the most deceptive pitfall in feature engineering is label leakage, where information about the target variable unintentionally contaminates the features. This leads to models that appear highly accurate during training but fail catastrophically on new data. This final practice is a crucial lesson in vigilance and diagnostics, guiding you through a controlled experiment to understand and combat this problem. You will learn to use noise sentinel features and permutation importance to create a safeguard system that can automatically flag and penalize suspiciously predictive features, providing a powerful tool for building more reliable and generalizable models [@problem_id:3124151].", "problem": "You will implement a complete, deterministic procedure to detect label leakage in feature engineering for a binary classifier by intentionally adding leaked features, performing feature selection, computing permutation importances, and applying a safeguard that penalizes suspiciously high importances. The program must be self-contained and produce results for a given test suite. All mathematical symbols in this problem are specified in LaTeX, and every symbol, variable, function, operator, and number is written using inline math mode.\n\nThe foundational base of this problem is the definition of Empirical Risk Minimization (ERM) for training, Binary Cross-Entropy (BCE) for measuring classification error, and permutation-based importance for feature relevance, all of which are well-tested principles in machine learning.\n\nConstruct a synthetic binary classification dataset as follows. For each test case, there are $n$ samples and three feature blocks:\n- Base features of dimension $d_b$ that are informative for the label.\n- Noise sentinel features of dimension $d_n$ that are independent of the label.\n- Leaked features of dimension $d_\\ell$ that are intentionally constructed to carry direct information about the label.\n\nData generation:\n1. Draw base features $\\mathbf{x}_i^b \\in \\mathbb{R}^{d_b}$ independently as $\\mathbf{x}_i^b \\sim \\mathcal{N}(\\mathbf{0}, I)$ for each sample index $i \\in \\{1,\\dots,n\\}$.\n2. Draw a ground-truth weight vector $\\mathbf{w}^\\star \\in \\mathbb{R}^{d_b}$ and bias $b^\\star \\in \\mathbb{R}$ deterministically from a pseudorandom seed, and define the logit $z_i = \\mathbf{w}^{\\star \\top}\\mathbf{x}_i^b + b^\\star$. Define the label probability $p_i = \\sigma(z_i)$ where $\\sigma(u) = \\frac{1}{1 + e^{-u}}$, and sample the binary label $y_i \\in \\{0,1\\}$ from a Bernoulli distribution with parameter $p_i$.\n3. Draw noise sentinel features $\\mathbf{x}_i^n \\in \\mathbb{R}^{d_n}$ independently as $\\mathbf{x}_i^n \\sim \\mathcal{N}(\\mathbf{0}, I)$, all independent of $\\mathbf{x}_i^b$ and $y_i$.\n4. Construct leaked features $\\mathbf{x}_i^\\ell \\in \\mathbb{R}^{d_\\ell}$ as $x_{i,j}^\\ell = y_i + \\eta_{i,j}$ for each leaked feature index $j \\in \\{1,\\dots,d_\\ell\\}$, where $\\eta_{i,j} \\sim \\mathcal{N}(0, s_\\ell^2)$ and $s_\\ell$ is a provided leak noise standard deviation. Smaller $s_\\ell$ yields stronger leakage.\n5. Form the complete feature vector $\\mathbf{x}_i = [\\mathbf{x}_i^b, \\mathbf{x}_i^n, \\mathbf{x}_i^\\ell] \\in \\mathbb{R}^{d}$ with $d = d_b + d_n + d_\\ell$.\n\nSplit the dataset into training and validation sets, using a training fraction of $0.7$ so that the training set has $\\lfloor 0.7n \\rfloor$ samples and the validation set has $n - \\lfloor 0.7n \\rfloor$ samples. Standardize each feature dimension using the training set statistics: for each feature index $j \\in \\{1,\\dots,d\\}$, compute the training mean $m_j$ and standard deviation $s_j$, then transform both training and validation features by $\\tilde{x}_{i,j} = \\frac{x_{i,j} - m_j}{s_j}$.\n\nTrain a feedforward Artificial Neural Network (ANN) classifier with one hidden layer:\n- Hidden layer width is a fixed constant $h$.\n- Hidden activation is the Rectified Linear Unit (ReLU), $\\text{ReLU}(u) = \\max(0,u)$.\n- Output is a single logit with a logistic activation $\\sigma$ to produce $\\hat{y}_i \\in (0,1)$.\n- Minimize the Binary Cross-Entropy (BCE) empirical risk over the training set:\n$$\n\\mathcal{L}_{\\text{BCE}} = -\\frac{1}{n_{\\text{train}}}\\sum_{i=1}^{n_{\\text{train}}}\\left( y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i) \\right),\n$$\nusing full-batch gradient descent for a fixed number of epochs and a fixed learning rate, both specified in the program.\n\nCompute permutation importance on the validation set to quantify reliance on each feature coordinate $j$:\n1. Compute the baseline validation loss $L_0$ using the trained ANN and the unpermuted validation features.\n2. For each feature index $j \\in \\{1,\\dots,d\\}$, independently permute column $j$ of the validation features across samples to produce a perturbed validation set, compute the loss $L_{(j)}$, and define the importance:\n$$\nI_j = L_{(j)} - L_0.\n$$\nLarger $I_j$ indicates greater reliance on feature $j$.\n\nDefine a safeguard against leakage using the noise sentinel features as a reference null distribution:\n- Let $J_n \\subset \\{1,\\dots,d\\}$ denote the set of indices corresponding to the noise sentinel features (the block following the base features).\n- Compute the noise-reference mean and standard deviation of importances:\n$$\n\\mu_n = \\frac{1}{|J_n|}\\sum_{j \\in J_n} I_j,\n\\qquad\n\\sigma_n = \\sqrt{\\frac{1}{|J_n|}\\sum_{j \\in J_n}(I_j - \\mu_n)^2}.\n$$\n- Define a threshold $\\tau = \\mu_n + z \\sigma_n$ with a fixed $z$ and a penalty coefficient $\\lambda > 0$.\n- Define the penalized importance for each feature index $j$:\n$$\nP_j = I_j - \\lambda \\max(0, I_j - \\tau).\n$$\n\nPerform selection with and without the safeguard:\n- Unpenalized selection chooses the top-$k$ indices by descending $I_j$.\n- Penalized selection chooses the top-$k$ indices by descending $P_j$.\n- A feature index $j$ is flagged as suspicious if $I_j > \\tau$.\n- Define leakage detection as a boolean that is true if at least one suspiciously flagged feature would be selected by the unpenalized selection but is excluded by the penalized selection:\n$$\n\\text{detected} = \\left( \\left\\{ j \\mid I_j > \\tau \\right\\} \\cap \\text{TopK}(I) \\right) \\setminus \\text{TopK}(P) \\neq \\emptyset,\n$$\nwhere $\\text{TopK}(I)$ is the set of indices of the $k$ largest entries of $I$, and similarly for $\\text{TopK}(P)$.\n\nYour program must implement the above procedure and evaluate the following test suite. Each test case is a tuple containing $(\\text{seed}, n, d_b, d_n, d_\\ell, s_\\ell, k)$:\n- Happy path with no leak: $(42, 600, 5, 5, 0, 0.0, 3)$\n- Mild leak: $(43, 600, 5, 5, 1, 0.9, 3)$\n- Strong leak: $(44, 600, 5, 5, 2, 0.05, 3)$\n- Boundary case (smaller sample, more noise sentinels, single strong leak): $(45, 200, 5, 10, 1, 0.1, 3)$\n\nUse fixed hyperparameters for the ANN: hidden width $h = 12$, learning rate $\\alpha = 0.1$, number of epochs $300$. Use $z = 3$ and $\\lambda = 0.8$ for the safeguard. All random operations must use the provided seed per test case to ensure determinism.\n\nFinal output format:\nYour program should produce a single line of output containing the detection booleans for the four test cases as a comma-separated list enclosed in square brackets (e.g., $[b_1,b_2,b_3,b_4]$), where each $b_i$ is either $True$ or $False$.", "solution": "The user-provided problem has been critically validated and is determined to be sound. It is scientifically grounded in established machine learning principles, is well-posed with a clear, deterministic procedure, and is expressed objectively. Therefore, a complete solution is provided.\n\nThe objective is to construct a deterministic program to detect label leakage in feature engineering. This is achieved by creating a synthetic dataset containing intentionally leaked features, training a neural network classifier, evaluating feature importance using permutations, and applying a statistical safeguard to identify and penalize features with suspiciously high importance relative to known-uninformative features.\n\nThe process is structured as follows:\n\nFirst, a synthetic binary classification dataset is generated for each test case. Each test case is defined by a tuple of parameters: $(\\text{seed}, n, d_b, d_n, d_\\ell, s_\\ell, k)$, which control the random seed, number of samples, and dimensions of the feature blocks. All random operations are seeded to ensure determinism.\n-   Informative base features, $\\mathbf{x}_i^b \\in \\mathbb{R}^{d_b}$, are drawn from a standard normal distribution, $\\mathcal{N}(\\mathbf{0}, I)$.\n-   A ground-truth linear model, $z_i = \\mathbf{w}^{\\star \\top}\\mathbf{x}_i^b + b^\\star$, is defined using a deterministically generated weight vector $\\mathbf{w}^\\star$ and bias $b^\\star$. The true probability for the positive class is given by the logistic function, $p_i = \\sigma(z_i) = (1 + e^{-z_i})^{-1}$. Binary labels $y_i \\in \\{0, 1\\}$ are then sampled from a Bernoulli distribution with parameter $p_i$.\n-   Uninformative noise sentinel features, $\\mathbf{x}_i^n \\in \\mathbb{R}^{d_n}$, are drawn from $\\mathcal{N}(\\mathbf{0}, I)$, and are statistically independent of both the base features and the labels.\n-   Potentially leaked features, $\\mathbf{x}_i^\\ell \\in \\mathbb{R}^{d_\\ell}$, are constructed by directly corrupting the label with Gaussian noise: $x_{i,j}^\\ell = y_i + \\eta_{i,j}$, where $\\eta_{i,j} \\sim \\mathcal{N}(0, s_\\ell^2)$. The parameter $s_\\ell$ controls the strength of the leak; a smaller $s_\\ell$ implies a stronger, more obvious leak. If $d_\\ell=0$, this block is empty.\n-   The final feature vector for each sample $i$ is the concatenation $\\mathbf{x}_i = [\\mathbf{x}_i^b, \\mathbf{x}_i^n, \\mathbf{x}_i^\\ell] \\in \\mathbb{R}^{d}$, where the total dimension is $d = d_b + d_n + d_\\ell$.\n\nNext, the data is preprocessed. The dataset is chronologically split into a training set containing the first $\\lfloor 0.7n \\rfloor$ samples and a validation set with the remaining $n - \\lfloor 0.7n \\rfloor$ samples. To prevent information leakage from the validation set, feature standardization is performed by computing the mean $m_j$ and standard deviation $s_j$ for each feature $j$ on the training set *only*. These statistics are then used to transform both the training and validation data according to $\\tilde{x}_{i,j} = (x_{i,j} - m_j) / s_j$. A small epsilon is added to $s_j$ to prevent division by zero for constant features.\n\nAn Artificial Neural Network (ANN) is then trained on the standardized training data. The network has a single hidden layer of a fixed width $h=12$ with the Rectified Linear Unit (ReLU) activation function, $\\text{ReLU}(u) = \\max(0,u)$. The output layer consists of a single neuron with a logistic activation function $\\sigma(u)$ to predict the probability $\\hat{y}_i$. The model is trained to minimize the Binary Cross-Entropy (BCE) loss over the training set:\n$$\n\\mathcal{L}_{\\text{BCE}} = -\\frac{1}{n_{\\text{train}}}\\sum_{i=1}^{n_{\\text{train}}}\\left( y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i) \\right).\n$$\nTraining is performed using full-batch gradient descent for $300$ epochs with a learning rate of $\\alpha = 0.1$.\n\nAfter training, feature importance is quantified using the permutation importance method on the validation set. This method measures the model's reliance on each feature.\n-   First, the baseline loss, $L_0$, is computed on the original (unpermuted) validation set.\n-   Then, for each feature index $j \\in \\{1, \\dots, d\\}$, the values in the $j$-th column of the validation feature matrix are randomly permuted. The loss $L_{(j)}$ is re-computed on this perturbed dataset.\n-   The permutation importance for feature $j$ is defined as the increase in loss: $I_j = L_{(j)} - L_0$. A large positive value of $I_j$ indicates that the model relies heavily on feature $j$ to make accurate predictions.\n\nA statistical safeguard is implemented to distinguish legitimate high-importance features from suspiciously important ones that may indicate leakage. This safeguard uses the noise sentinel features as a null reference.\n-   The mean $\\mu_n$ and standard deviation $\\sigma_n$ of the importances are calculated exclusively from the set of noise sentinel features:\n$$\n\\mu_n = \\frac{1}{|J_n|}\\sum_{j \\in J_n} I_j, \\qquad \\sigma_n = \\sqrt{\\frac{1}{|J_n|}\\sum_{j \\in J_n}(I_j - \\mu_n)^2},\n$$\nwhere $J_n$ is the set of indices for the noise features.\n-   A statistical threshold $\\tau$ is defined as $\\tau = \\mu_n + z \\sigma_n$, using a z-score multiplier of $z=3$. Any feature $j$ with an importance $I_j > \\tau$ is flagged as \"suspicious,\" as its importance is significantly greater than would be expected for an uninformative feature.\n-   A penalized importance score, $P_j$, is then computed for each feature to discount the importance of any suspicious feature:\n$$\nP_j = I_j - \\lambda \\max(0, I_j - \\tau),\n$$\nwith a penalty coefficient of $\\lambda = 0.8$. This penalty reduces the score of features whose importance exceeds the threshold $\\tau$, making them less likely to be selected.\n\nFinally, the leakage detection logic is applied by comparing feature selections made with and without the safeguard.\n-   The unpenalized selection, $\\text{TopK}(I)$, is the set of indices of the top-$k$ features as ranked by the original importances $I_j$.\n-   The penalized selection, $\\text{TopK}(P)$, is the set of indices of the top-$k$ features as ranked by the penalized importances $P_j$.\n-   Leakage is detected (evaluates to True) if there exists at least one feature that is (a) flagged as suspicious (i.e., $I_j > \\tau$), (b) is part of the unpenalized top-$k$ selection, and (c) is excluded from the penalized top-$k$ selection. Formally, detection occurs if the following set is non-empty:\n$$\n\\left( \\left\\{ j \\mid I_j > \\tau \\right\\} \\cap \\text{TopK}(I) \\right) \\setminus \\text{TopK}(P) \\neq \\emptyset.\n$$\nThe program executes this entire procedure for each test case in the suite and reports the final boolean detection result.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a deterministic procedure to detect label leakage in feature engineering.\n    \"\"\"\n\n    test_cases = [\n        # (seed, n, d_b, d_n, d_ell, s_ell, k)\n        (42, 600, 5, 5, 0, 0.0, 3),   # Happy path with no leak\n        (43, 600, 5, 5, 1, 0.9, 3),   # Mild leak\n        (44, 600, 5, 5, 2, 0.05, 3),  # Strong leak\n        (45, 200, 5, 10, 1, 0.1, 3), # Boundary case\n    ]\n\n    results = []\n    \n    # Fixed hyperparameters\n    h = 12\n    learning_rate = 0.1\n    epochs = 300\n    z_score_threshold = 3.0\n    penalty_lambda = 0.8\n    train_frac = 0.7\n\n    for case in test_cases:\n        seed, n, d_b, d_n, d_ell, s_ell, k = case\n        \n        # Set seed for determinism in all random operations\n        np.random.seed(seed)\n        \n        # --- 1. Data Generation ---\n        # Ground-truth weights and bias\n        w_star = np.random.randn(d_b, 1)\n        b_star = np.random.randn(1)\n        \n        # Base features\n        X_b = np.random.randn(n, d_b)\n        \n        # Labels\n        z_logits = X_b @ w_star + b_star\n        p_labels = 1 / (1 + np.exp(-z_logits))\n        y = (np.random.rand(n, 1) < p_labels).astype(float)\n        \n        # Noise sentinel features\n        X_n = np.random.randn(n, d_n)\n        \n        # Leaked features\n        if d_ell > 0:\n            eta = np.random.randn(n, d_ell) * s_ell\n            X_ell = y + eta\n        else:\n            X_ell = np.empty((n, 0))\n            \n        # Concatenate all feature blocks\n        X = np.hstack([X_b, X_n, X_ell])\n        d = d_b + d_n + d_ell\n\n        # --- 2. Data Preprocessing ---\n        n_train = int(np.floor(train_frac * n))\n        X_train, X_val = X[:n_train], X[n_train:]\n        y_train, y_val = y[:n_train], y[n_train:]\n\n        train_mean = np.mean(X_train, axis=0)\n        train_std = np.std(X_train, axis=0)\n        # Add epsilon for numerical stability if a feature has zero standard deviation\n        train_std[train_std == 0] = 1e-8\n        \n        X_train_std = (X_train - train_mean) / train_std\n        X_val_std = (X_val - train_mean) / train_std\n\n        # --- 3. ANN Model and Training ---\n        # Initialize weights with Xavier/Glorot scaling\n        W1 = np.random.randn(d, h) * np.sqrt(2.0 / (d + h))\n        b1 = np.zeros(h)\n        W2 = np.random.randn(h, 1) * np.sqrt(2.0 / (h + 1))\n        b2 = np.zeros(1)\n\n        def bce_loss(y_true, y_pred, epsilon=1e-9):\n            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n            return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\n        # Full-batch gradient descent\n        for _ in range(epochs):\n            # Forward pass\n            z1 = X_train_std @ W1 + b1\n            a1 = np.maximum(0, z1)\n            z2 = a1 @ W2 + b2\n            y_hat = 1 / (1 + np.exp(-z2))\n\n            # Backward pass\n            d_z2 = (y_hat - y_train) / n_train\n            d_W2 = a1.T @ d_z2\n            d_b2 = np.sum(d_z2, axis=0)\n            \n            d_a1 = d_z2 @ W2.T\n            d_z1 = d_a1 * (z1 > 0)\n            d_W1 = X_train_std.T @ d_z1\n            d_b1 = np.sum(d_z1, axis=0)\n\n            # Update weights\n            W1 -= learning_rate * d_W1\n            b1 -= learning_rate * d_b1\n            W2 -= learning_rate * d_W2\n            b2 -= learning_rate * d_b2\n        \n        def predict(X_in):\n            z1 = X_in @ W1 + b1\n            a1 = np.maximum(0, z1)\n            z2 = a1 @ W2 + b2\n            return 1 / (1 + np.exp(-z2))\n\n        # --- 4. Permutation Importance ---\n        baseline_loss = bce_loss(y_val, predict(X_val_std))\n        importances = np.zeros(d)\n        \n        for j in range(d):\n            X_val_perm = X_val_std.copy()\n            # Shuffle in place using the global RNG stream\n            np.random.shuffle(X_val_perm[:, j])\n            permuted_loss = bce_loss(y_val, predict(X_val_perm))\n            importances[j] = permuted_loss - baseline_loss\n\n        # --- 5. Safeguard and Detection Logic ---\n        I = importances\n        noise_indices = range(d_b, d_b + d_n)\n        \n        if d_n > 1:\n            I_noise = I[noise_indices]\n            mu_n = np.mean(I_noise)\n            sigma_n = np.std(I_noise)\n        elif d_n == 1:\n            mu_n = I[noise_indices[0]]\n            sigma_n = 0.0 # Std of a single point is 0\n        else: # d_n == 0\n            mu_n = 0.0\n            sigma_n = 0.0 # Cannot compute threshold without sentinels\n\n        tau = mu_n + z_score_threshold * sigma_n\n        \n        # Penalized importance\n        P = I - penalty_lambda * np.maximum(0, I - tau)\n        \n        # Get top-k indices\n        topk_I_indices = np.argsort(I)[-k:][::-1]\n        topk_P_indices = np.argsort(P)[-k:][::-1]\n        \n        # Convert to sets for formal comparison\n        set_topk_I = set(topk_I_indices)\n        set_topk_P = set(topk_P_indices)\n        \n        # Identify suspicious features\n        suspicious_indices = {j for j, imp in enumerate(I) if imp > tau}\n        \n        # Find suspicious features that are selected by I but deselected by P\n        suspiciously_selected_by_I = suspicious_indices.intersection(set_topk_I)\n        deselected_by_P = suspiciously_selected_by_I.difference(set_topk_P)\n        \n        is_detected = len(deselected_by_P) > 0\n        results.append(is_detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3124151"}]}