{"hands_on_practices": [{"introduction": "The canonical partition function, $Q$, is the cornerstone that connects the microscopic quantum world to macroscopic thermodynamics. This exercise guides you through computing $Q$ for one of the simplest quantum systems—a particle in a one-dimensional box—by directly summing over its discrete energy levels. By then deriving and computing the classical partition function, you will witness the correspondence principle in action and explore the conditions under which the discrete quantum sum beautifully converges to its continuous classical analogue [@problem_id:2463653].", "problem": "Consider a single non-interacting quantum particle confined to a one-dimensional (1D) infinite potential well of length $L$ with perfectly rigid walls. In the canonical ensemble at temperature $T$, the microscopic states have energies labeled by a positive integer $n$. Your task is to connect the microscopic energy spectrum to macroscopic thermodynamic behavior by computing the canonical partition function and comparing the exact quantum result to its classical (high-temperature) limit.\n\nRequirements and foundational base:\n- Use the Boltzmann distribution in the canonical ensemble, where the canonical partition function is defined as $Q = \\sum_{i} e^{-\\beta E_{i}}$ with $\\beta = 1/(k_B T)$, and $k_B$ is the Boltzmann constant.\n- For a particle in a one-dimensional infinite potential well of length $L$, the energy eigenvalues are a well-tested result: $E_{n} = \\dfrac{n^{2} h^{2}}{8 m L^{2}}$ for $n = 1, 2, 3, \\dots$, where $h$ is Planck’s constant and $m$ is the particle mass.\n- Derive the classical counterpart of the partition function from first principles by starting from the phase-space integral over position and momentum with the standard quantum of phase-space cell volume $h$, i.e., $Q_{\\mathrm{cl}} = \\dfrac{1}{h} \\int_{0}^{L} \\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\beta \\dfrac{p^{2}}{2m}\\right) \\, \\mathrm{d}p \\, \\mathrm{d}x$. Do not approximate the quantum spectrum when setting up this integral; only apply continuum methods appropriate for the classical limit.\n\nAlgorithmic tasks your program must perform:\n1. Implement a numerically stable computation of the quantum partition function\n   $$Q_{\\mathrm{q}}(m,L,T) = \\sum_{n=1}^{\\infty} \\exp\\!\\left(-\\beta \\dfrac{n^{2} h^{2}}{8 m L^{2}}\\right).$$\n   Use an adaptive truncation of the infinite sum by adding terms until the next term is smaller than a prescribed tolerance. Ensure your truncation logic is justified by monotonic decay of the positive summands and that it prevents unnecessary computation for small or large $T$.\n2. From the classical phase-space integral stated above, carry out the derivation symbolically to obtain a closed-form expression for the classical partition function $Q_{\\mathrm{cl}}(m,L,T)$ that your program will evaluate numerically. Use the International System of Units (SI) consistently: $m$ in $\\mathrm{kg}$, $L$ in $\\mathrm{m}$, $T$ in $\\mathrm{K}$, $h$ in $\\mathrm{J\\,s}$, and $k_B$ in $\\mathrm{J/K}$. The partition functions are dimensionless.\n3. For each test case, compute the relative deviation\n   $$\\delta = \\dfrac{Q_{\\mathrm{q}} - Q_{\\mathrm{cl}}}{Q_{\\mathrm{cl}}}.$$\n   Report $\\delta$ as a floating-point number rounded to $6$ decimal places.\n\nConstants:\n- Use $k_B = 1.380649\\times 10^{-23}\\ \\mathrm{J/K}$ and $h = 6.62607015\\times 10^{-34}\\ \\mathrm{J\\,s}$.\n\nTest suite (use exactly these values):\n- Case $1$ (happy path, moderate quantization): $m = 9.1093837015\\times 10^{-31}\\ \\mathrm{kg}$, $L = 1.0\\times 10^{-9}\\ \\mathrm{m}$, $T = 300\\ \\mathrm{K}$.\n- Case $2$ (high-temperature classical limit): $m = 9.1093837015\\times 10^{-31}\\ \\mathrm{kg}$, $L = 1.0\\times 10^{-9}\\ \\mathrm{m}$, $T = 1.0\\times 10^{7}\\ \\mathrm{K}$.\n- Case $3$ (heavier particle, closer to classical at ambient conditions): $m = 6.6335209\\times 10^{-26}\\ \\mathrm{kg}$, $L = 1.0\\times 10^{-9}\\ \\mathrm{m}$, $T = 300\\ \\mathrm{K}$.\n- Case $4$ (smaller box, stronger quantization at elevated temperature): $m = 9.1093837015\\times 10^{-31}\\ \\mathrm{kg}$, $L = 2.0\\times 10^{-10}\\ \\mathrm{m}$, $T = 1000\\ \\mathrm{K}$.\n\nAdditional numerical requirements:\n- Implement an absolute tolerance for truncating the quantum sum; take the summation cutoff so that the first neglected term is strictly less than $10^{-12}$.\n- Guard against numerical underflow by recognizing that terms with exponent less than $-700$ in the natural exponential are effectively zero in double precision.\n- All computations must be performed in SI units as stated above.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry being the value of $\\delta$ for the corresponding test case, rounded to $6$ decimal places. For example, the format must be $[x_{1},x_{2},x_{3},x_{4}]$ with each $x_{i}$ a decimal number with $6$ digits after the decimal point.", "solution": "We begin from the canonical ensemble within statistical mechanics, which is foundational in computational chemistry for connecting microscopic energy levels to macroscopic thermodynamic properties. The Boltzmann distribution assigns probability $p_{i} = e^{-\\beta E_{i}}/Q$ to microstate $i$, with the canonical partition function defined by\n$$\nQ = \\sum_{i} e^{-\\beta E_{i}}, \\quad \\beta = \\dfrac{1}{k_B T}.\n$$\nThe partition function is dimensionless and provides the central link to macroscopic quantities such as the Helmholtz free energy $F = -k_B T \\ln Q$.\n\nMicroscopic model: a one-dimensional (1D) infinite potential well of length $L$ with perfectly rigid walls has stationary energy eigenvalues\n$$\nE_{n} = \\dfrac{n^{2} h^{2}}{8 m L^{2}}, \\quad n = 1, 2, 3, \\dots,\n$$\na standard result from solving the time-independent Schrödinger equation with Dirichlet boundary conditions. The quantum canonical partition function is therefore\n$$\nQ_{\\mathrm{q}}(m,L,T) = \\sum_{n=1}^{\\infty} \\exp\\!\\left(-\\beta \\dfrac{n^{2} h^{2}}{8 m L^{2}}\\right).\n$$\nDefine $a = \\beta h^{2}/(8 m L^{2})$. The summand is $e^{-a n^{2}}$, which is strictly positive and strictly decreasing in $n$ for $a > 0$. This monotonicity justifies truncating the infinite sum when the first neglected term is below a specified tolerance $\\varepsilon$, because the tail sum is then bounded by an integral test:\n$$\n\\sum_{n=N+1}^{\\infty} e^{-a n^{2}} \\le \\int_{N}^{\\infty} e^{-a x^{2}} \\, \\mathrm{d}x = \\dfrac{1}{2}\\sqrt{\\dfrac{\\pi}{a}}\\, \\mathrm{erfc}\\!\\big(\\sqrt{a}\\, N\\big),\n$$\nso choosing $N$ such that $e^{-a N^{2}}  \\varepsilon$ ensures a small tail. An efficient practical cutoff is\n$$\nN = \\left\\lceil \\sqrt{\\dfrac{\\ln(1/\\varepsilon)}{a}} \\right\\rceil,\n$$\nwhich guarantees $e^{-a N^{2}} \\le \\varepsilon$. In floating point arithmetic, terms with exponent less than $-700$ are effectively zero in double precision, so we also treat those as vanishing.\n\nClassical limit: In the high-temperature limit, the discrete quantum levels become densely populated and the classical partition function emerges from the phase-space integral with the Liouville measure and the quantum of phase-space cell volume $h$:\n$$\nQ_{\\mathrm{cl}} = \\dfrac{1}{h} \\int_{0}^{L} \\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\beta \\dfrac{p^{2}}{2m}\\right) \\, \\mathrm{d}p \\, \\mathrm{d}x.\n$$\nThe position integral yields $\\int_{0}^{L} \\mathrm{d}x = L$. The momentum integral is a standard Gaussian integral:\n$$\n\\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\beta \\dfrac{p^{2}}{2m}\\right) \\, \\mathrm{d}p = \\sqrt{\\dfrac{2 \\pi m}{\\beta}} = \\sqrt{2 \\pi m k_B T}.\n$$\nTherefore the classical translational partition function in one dimension is\n$$\nQ_{\\mathrm{cl}}(m,L,T) = \\dfrac{L}{h} \\sqrt{2 \\pi m k_B T}.\n$$\nThis result is valid in the regime where quantum level spacing is negligible compared to $k_B T$. The microscopic-to-macroscopic connection is explicit: $Q_{\\mathrm{q}}$ is a sum over microscopic energy levels, while $Q_{\\mathrm{cl}}$ is a continuum phase-space measure that approximates the sum when $T$ is large or when $m$ and $L$ make the level spacing small.\n\nAsymptotic consistency: The sum $\\sum_{n=1}^{\\infty} e^{-a n^{2}}$ is related to the Jacobi theta function. Using the Poisson summation formula or the modular property of the theta function, one finds the high-temperature (small $a$) asymptotic expansion\n$$\n\\sum_{n=1}^{\\infty} e^{-a n^{2}} = \\dfrac{1}{2} \\sqrt{\\dfrac{\\pi}{a}} - \\dfrac{1}{2} + \\mathcal{O}\\!\\left(e^{-\\pi^{2}/a}\\right).\n$$\nWith $a = \\beta h^{2}/(8 m L^{2})$, the leading term gives\n$$\n\\sum_{n=1}^{\\infty} e^{-a n^{2}} \\sim \\dfrac{1}{2} \\sqrt{\\dfrac{\\pi}{a}} = \\dfrac{L}{h} \\sqrt{2 \\pi m k_B T} = Q_{\\mathrm{cl}}(m,L,T),\n$$\nso $Q_{\\mathrm{q}} \\to Q_{\\mathrm{cl}}$ as $T \\to \\infty$, validating the microscopic-to-macroscopic transition.\n\nNumerical algorithm design:\n- Inputs per case: $(m, L, T)$ in SI units. Constants: $k_B = 1.380649\\times 10^{-23}\\ \\mathrm{J/K}$, $h = 6.62607015\\times 10^{-34}\\ \\mathrm{J\\,s}$.\n- Compute $\\beta = 1/(k_B T)$ and $a = \\beta h^{2}/(8 m L^{2})$.\n- Choose tolerance $\\varepsilon = 10^{-12}$. Set\n  $$\n  N = \\max\\!\\left(1,\\ \\left\\lceil \\sqrt{\\dfrac{\\ln(1/\\varepsilon)}{a}} \\right\\rceil \\right),\n  $$\n  which ensures the first neglected term is below tolerance. Optionally cap $N$ if needed; for the provided test suite, $N$ remains modest (on the order of hundreds).\n- Compute\n  $$\n  Q_{\\mathrm{q}} = \\sum_{n=1}^{N} e^{-a n^{2}},\n  $$\n  skipping terms with exponent below $-700$ to avoid underflow.\n- Compute\n  $$\n  Q_{\\mathrm{cl}} = \\dfrac{L}{h} \\sqrt{2 \\pi m k_B T}.\n  $$\n- Compute the relative deviation\n  $$\n  \\delta = \\dfrac{Q_{\\mathrm{q}} - Q_{\\mathrm{cl}}}{Q_{\\mathrm{cl}}}.\n  $$\n- Round $\\delta$ to $6$ decimal places.\n- Repeat for the four test cases given.\n\nTest suite rationale:\n- Case $1$ tests a moderately quantum regime at ambient temperature for an electron in a nanometer box.\n- Case $2$ probes the high-temperature limit to confirm $Q_{\\mathrm{q}} \\approx Q_{\\mathrm{cl}}$.\n- Case $3$ uses a heavier particle (argon atom) where classical behavior emerges more readily at the same $L$ and $T$.\n- Case $4$ employs a smaller box and elevated temperature to test strong quantization and numerical stability of the summation.\n\nThe final program will compute and print a single line in the format $[\\delta_{1},\\delta_{2},\\delta_{3},\\delta_{4}]$, each rounded to $6$ decimals, where each $\\delta_{i}$ is the relative deviation for the $i$th test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Physical constants (SI)\nk_B = 1.380649e-23      # Boltzmann constant in J/K\nh = 6.62607015e-34      # Planck constant in J*s\n\ndef quantum_partition_1d_box(m, L, T, tol=1e-12):\n    \"\"\"\n    Compute the quantum canonical partition function for a particle in a 1D infinite well:\n        Z_q = sum_{n=1}^\\infty exp(-beta * n^2 * h^2 / (8 m L^2))\n    using an adaptive truncation where the first neglected term is  tol.\n\n    Parameters:\n        m : mass in kg\n        L : length in m\n        T : temperature in K\n        tol : absolute tolerance for truncation\n\n    Returns:\n        Z_q : float\n    \"\"\"\n    if T = 0.0:\n        # Physically undefined; here return 0 to avoid division by zero in beta\n        return 0.0\n\n    beta = 1.0 / (k_B * T)\n    a = beta * h * h / (8.0 * m * L * L)\n\n    if a = 0.0:\n        # Degenerate case; treat as zero spacing - diverging sum; not expected in test suite\n        return np.inf\n\n    # Determine cutoff N such that exp(-a * N^2)  tol\n    # If ln(1/tol)/a is very small, ensure N at least 1\n    target = np.log(1.0 / tol) / a\n    if target = 1.0:\n        N = 1\n    else:\n        N = int(np.ceil(np.sqrt(target)))\n\n    # Safety cap to prevent accidental huge loops (not expected to trigger for given cases)\n    N = min(N, 2_000_000)\n\n    # Sum terms, skipping those that underflow in double precision (exp(-x) ~ 0 for x  ~ 700)\n    Z_q = 0.0\n    # For efficiency, sum in blocks using vectorization when N is moderate\n    # But also handle underflow to avoid unnecessary exponent evaluations\n    n_values = np.arange(1, N + 1, dtype=np.float64)\n    exponents = -a * n_values * n_values\n    # Mask out underflow-prone terms\n    mask = exponents  -700.0\n    if np.any(mask):\n        Z_q = float(np.exp(exponents[mask]).sum())\n    else:\n        Z_q = 0.0\n\n    return Z_q\n\ndef classical_partition_1d(m, L, T):\n    \"\"\"\n    Classical 1D translational partition function:\n        Z_cl = (L/h) * sqrt(2 * pi * m * k_B * T)\n    \"\"\"\n    if T = 0.0:\n        return 0.0\n    return (L / h) * np.sqrt(2.0 * np.pi * m * k_B * T)\n\ndef relative_deviation(m, L, T, tol=1e-12):\n    Z_q = quantum_partition_1d_box(m, L, T, tol=tol)\n    Z_cl = classical_partition_1d(m, L, T)\n    if Z_cl == 0.0:\n        # Avoid division by zero; define deviation as 0 if both zero else +/-inf\n        if Z_q == 0.0:\n            return 0.0\n        else:\n            return float('inf') if Z_q  0.0 else float('-inf')\n    return (Z_q - Z_cl) / Z_cl\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (m [kg], L [m], T [K])\n    test_cases = [\n        (9.1093837015e-31, 1.0e-9, 300.0),        # Case 1\n        (9.1093837015e-31, 1.0e-9, 1.0e7),        # Case 2\n        (6.6335209e-26,   1.0e-9, 300.0),         # Case 3\n        (9.1093837015e-31, 2.0e-10, 1000.0),      # Case 4\n    ]\n\n    results = []\n    for m, L, T in test_cases:\n        delta = relative_deviation(m, L, T, tol=1e-12)\n        # Round to 6 decimal places as required\n        if np.isfinite(delta):\n            results.append(f\"{delta:.6f}\")\n        else:\n            # Represent infinities explicitly if they occur (not expected here)\n            results.append(\"inf\" if delta  0 else \"-inf\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2463653"}, {"introduction": "Beyond the partition function, the Boltzmann distribution provides a direct path to calculating other thermodynamic quantities, such as entropy. This practice challenges you to compute the configurational entropy of a simple liquid by analyzing a set of microscopic particle positions from a simulation. By discretizing the simulation box into bins and tallying particle occupancies, you will apply the Gibbs entropy formula to connect a spatial probability distribution to a fundamental macroscopic property [@problem_id:2463576].", "problem": "A two-dimensional coarse-grained description of a simple liquid at thermal equilibrium relates the microscopic distribution of particle positions to a macroscopic configurational entropy through the Boltzmann framework. Consider a square or rectangular simulation box with side lengths $L_x$ and $L_y$ (in $\\mathrm{nm}$), partitioned into a uniform grid of $n_x \\times n_y$ rectangular bins. A simulation provides $F$ snapshots (frames), each containing $N$ particle positions $\\{(x_{j,f}, y_{j,f})\\}$ for $j \\in \\{1,\\dots,N\\}$ and $f \\in \\{1,\\dots,F\\}$, with $x_{j,f} \\in [0,L_x]$ and $y_{j,f} \\in [0,L_y]$ given in $\\mathrm{nm}$. Assume indistinguishable particles, equilibrium sampling, and independence at the level of the single-particle positional distribution such that many-body correlations are neglected for the purpose of estimating a coarse-grained, single-particle-based configurational entropy. Periodic Boundary Conditions (PBC) are imposed.\n\nDefine the empirical single-particle positional probability mass $p_i$ for each bin $i$ by counting how many particle instances across all frames fall into bin $i$ and dividing by the total number of instances $N F$. Under PBC and bin discretization, assign a particle at position $(x,y)$ to bin indices $(i_x,i_y)$ using bin widths $\\Delta x = L_x/n_x$ and $\\Delta y = L_y/n_y$ according to:\n- $i_x = \\lfloor x / \\Delta x \\rfloor$ and $i_y = \\lfloor y / \\Delta y \\rfloor$,\n- if $x = L_x$ then set $i_x = n_x - 1$; if $y = L_y$ then set $i_y = n_y - 1$,\n- for $x$ or $y$ exactly on an internal bin edge (that is, $x = m \\Delta x$ with integer $m$ and $0  x  L_x$, or analogously for $y$), the point belongs to the higher-index bin along that axis,\n- $i_x \\in \\{0,\\dots,n_x-1\\}$, $i_y \\in \\{0,\\dots,n_y-1\\}$.\n\nUsing Boltzmann’s constant $k_B = 1.380649 \\times 10^{-23}\\ \\mathrm{J/K}$ and natural logarithms, compute the coarse-grained single-particle configurational entropy of the $N$-particle system from first principles, expressed in $\\mathrm{J/K}$. Treat bins with $p_i = 0$ as contributing $0$ to the sum. All results must be reported in $\\mathrm{J/K}$.\n\nYour program must evaluate the entropy for each of the following test cases. In all cases, coordinates and box lengths are in $\\mathrm{nm}$; angles are not involved. For each case, $F = 1$ unless otherwise stated, and all frames contain the same $N$.\n\n- Test Case 1 (uniform occupation across bins):\n  - $L_x = 2.0$, $L_y = 2.0$, $n_x = 2$, $n_y = 2$, $F = 1$, $N = 4$.\n  - Frame $1$ positions: $(0.5, 0.5)$, $(1.5, 0.5)$, $(0.5, 1.5)$, $(1.5, 1.5)$.\n\n- Test Case 2 (all particles in one bin):\n  - $L_x = 2.0$, $L_y = 2.0$, $n_x = 2$, $n_y = 2$, $F = 1$, $N = 4$.\n  - Frame $1$ positions: $(0.25, 0.25)$, $(0.25, 0.25)$, $(0.25, 0.25)$, $(0.25, 0.25)$.\n\n- Test Case 3 (two bins equally likely):\n  - $L_x = 2.0$, $L_y = 2.0$, $n_x = 2$, $n_y = 2$, $F = 1$, $N = 4$.\n  - Frame $1$ positions: $(0.25, 0.25)$, $(0.25, 0.25)$, $(1.75, 1.75)$, $(1.75, 1.75)$.\n\n- Test Case 4 (bin-edge and boundary handling under PBC):\n  - $L_x = 1.0$, $L_y = 1.0$, $n_x = 2$, $n_y = 2$, $F = 1$, $N = 4$.\n  - Frame $1$ positions: $(0.5, 0.25)$, $(0.25, 0.5)$, $(1.0, 0.75)$, $(0.75, 1.0)$.\n\n- Test Case 5 (rectangular box, non-square grid, uneven occupancy):\n  - $L_x = 3.0$, $L_y = 1.0$, $n_x = 3$, $n_y = 1$, $F = 1$, $N = 3$.\n  - Frame $1$ positions: $(0.25, 0.5)$, $(0.75, 0.5)$, $(1.25, 0.5)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the entropies for the five test cases, as a comma-separated list enclosed in square brackets, for example, $[e_1,e_2,e_3,e_4,e_5]$.\n- Each $e_i$ must be a floating-point number in scientific notation with exactly six digits after the decimal point, using a lowercase $e$ exponent, and must represent the entropy in $\\mathrm{J/K}$.\n- Example of required formatting for one value: $7.000000\\mathrm{e}{-23}$.\n\nThe required outputs are real numbers (floats).", "solution": "The problem presented is a valid exercise in computational statistical mechanics. It requests the calculation of the coarse-grained configurational entropy of a simple liquid from simulated particle coordinates. All necessary parameters and a clear, albeit simplified, physical model are provided. The problem is scientifically grounded, well-posed, and objective. I will now provide the solution.\n\nThe fundamental principle is the connection between the macroscopic thermodynamic property of entropy, $S$, and the microscopic states of a system, as established by Ludwig Boltzmann and generalized by J. Willard Gibbs. The entropy of a system is a measure of the number of accessible microscopic states corresponding to a given macroscopic state. For a system at thermal equilibrium, the probability of finding it in a particular microstate is given by the Boltzmann distribution. The Gibbs entropy formula provides a general expression for any probability distribution over a set of states $\\{i\\}$ with probabilities $P_i$:\n$$S = -k_B \\sum_{i} P_i \\ln P_i$$\nwhere $k_B$ is the Boltzmann constant, with value $k_B = 1.380649 \\times 10^{-23}\\ \\mathrm{J/K}$.\n\nIn this problem, we are not treating the full microscopic phase space. Instead, we use a coarse-grained representation where the system's state is described by the distribution of particles among a finite number of spatial bins. The problem simplifies the model further by assuming that the particles are independent. This is a crucial simplification, meaning we neglect explicit many-body correlations. Under this assumption, the total configurational entropy of the $N$-particle system, $S_N$, can be approximated as the sum of the single-particle entropies. Since all particles are indistinguishable, they share the same single-particle probability distribution, and thus the same single-particle entropy, $S_1$. Therefore, the total entropy of the system is $N$ times the single-particle entropy:\n$$S_N = N \\cdot S_1 = -N k_B \\sum_{i} p_i \\ln p_i$$\nHere, the sum is over all spatial bins $i$, and $p_i$ is the probability of finding a single, specific particle in bin $i$. The problem states that bins with zero probability ($p_i = 0$) contribute nothing to the sum, which is consistent with the mathematical limit $\\lim_{p\\to 0} p \\ln p = 0$.\n\nThe algorithmic procedure to solve this problem is as follows:\n\n1.  **System Discretization**: For each test case, the simulation box of size $L_x \\times L_y$ is partitioned into an $n_x \\times n_y$ grid. The dimensions of each bin are $\\Delta x = L_x / n_x$ and $\\Delta y = L_y / n_y$.\n\n2.  **Data Tallying**: A two-dimensional array, or histogram, of size $n_x \\times n_y$ must be initialized with zeros to store the particle counts for each bin. We iterate through all $F$ frames and all $N$ particles within each frame. For each particle position $(x, y)$, we determine its corresponding bin indices $(i_x, i_y)$ using the specified rules:\n    - The general rule is based on the floor function: $i_x = \\lfloor x / \\Delta x \\rfloor$ and $i_y = \\lfloor y / \\Delta y \\rfloor$.\n    - This rule correctly handles internal bin edges, as a point on the boundary $x=m\\Delta x$ is mapped to bin $m$, which is the higher index compared to bin $m-1$.\n    - The special boundary conditions at $x=L_x$ and $y=L_y$ must be handled to prevent out-of-bounds indices. A robust implementation is to cap the index at its maximum value: $i_x = \\min(n_x-1, \\lfloor x / \\Delta x \\rfloor)$ and $i_y = \\min(n_y-1, \\lfloor y / \\Delta y \\rfloor)$.\n    - The count for the identified bin $(i_x, i_y)$ is then incremented.\n\n3.  **Probability Calculation**: After processing all particle positions, the count in each bin represents the total number of particle observations in that region of space. The empirical probability $p_i$ for a single particle to be in bin $i$ is calculated by dividing the bin's count by the total number of particle observations, which is $N_{\\text{total}} = N \\times F$.\n\n4.  **Entropy Computation**: With the set of probabilities $\\{p_i\\}$ for all bins, we calculate the sum $\\sum_i p_i \\ln p_i$, where only terms with $p_i  0$ contribute. The final configurational entropy for the $N$-particle system is then calculated using the formula $S_N = -N k_B \\sum_{i} p_i \\ln p_i$.\n\nLet us apply this procedure to an example, Test Case 1:\n- Parameters: $L_x = 2.0$, $L_y = 2.0$, $n_x = 2$, $n_y = 2$, $F = 1$, $N = 4$.\n- Bin dimensions: $\\Delta x = 2.0/2 = 1.0$, $\\Delta y = 2.0/2 = 1.0$.\n- Positions: $(0.5, 0.5)$, $(1.5, 0.5)$, $(0.5, 1.5)$, $(1.5, 1.5)$.\n- Binning:\n    - $(0.5, 0.5) \\rightarrow i_x=0, i_y=0$.\n    - $(1.5, 0.5) \\rightarrow i_x=1, i_y=0$.\n    - $(0.5, 1.5) \\rightarrow i_x=0, i_y=1$.\n    - $(1.5, 1.5) \\rightarrow i_x=1, i_y=1$.\n- Counts: Each of the four bins has a count of $1$.\n- Probabilities: Total observations $N \\times F = 4$. Each bin has $p_i = 1/4$.\n- Entropy: There are $4$ bins with non-zero probability.\n    $$S = -4 k_B \\sum_{i=1}^{4} \\left(\\frac{1}{4} \\ln \\frac{1}{4}\\right) = -4 k_B \\left(4 \\cdot \\frac{1}{4} \\cdot (-\\ln 4)\\right) = 4 k_B \\ln 4$$\n    $$S = 4 \\times (1.380649 \\times 10^{-23}) \\times \\ln(4) \\approx 7.656360 \\times 10^{-23}\\ \\mathrm{J/K}$$\n\nThis systematic application of first principles yields the required results for all specified test cases. The following program implements this logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the formatted output.\n    \"\"\"\n    # Define Boltzmann's constant\n    K_B = 1.380649e-23  # J/K\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"Lx\": 2.0, \"Ly\": 2.0, \"nx\": 2, \"ny\": 2, \"N\": 4, \"F\": 1,\n            \"positions\": [(0.5, 0.5), (1.5, 0.5), (0.5, 1.5), (1.5, 1.5)]\n        },\n        {\n            \"Lx\": 2.0, \"Ly\": 2.0, \"nx\": 2, \"ny\": 2, \"N\": 4, \"F\": 1,\n            \"positions\": [(0.25, 0.25), (0.25, 0.25), (0.25, 0.25), (0.25, 0.25)]\n        },\n        {\n            \"Lx\": 2.0, \"Ly\": 2.0, \"nx\": 2, \"ny\": 2, \"N\": 4, \"F\": 1,\n            \"positions\": [(0.25, 0.25), (0.25, 0.25), (1.75, 1.75), (1.75, 1.75)]\n        },\n        {\n            \"Lx\": 1.0, \"Ly\": 1.0, \"nx\": 2, \"ny\": 2, \"N\": 4, \"F\": 1,\n            \"positions\": [(0.5, 0.25), (0.25, 0.5), (1.0, 0.75), (0.75, 1.0)]\n        },\n        {\n            \"Lx\": 3.0, \"Ly\": 1.0, \"nx\": 3, \"ny\": 1, \"N\": 3, \"F\": 1,\n            \"positions\": [(0.25, 0.5), (0.75, 0.5), (1.25, 0.5)]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        Lx, Ly, nx, ny = case[\"Lx\"], case[\"Ly\"], case[\"nx\"], case[\"ny\"]\n        N, F = case[\"N\"], case[\"F\"]\n        positions = case[\"positions\"]\n\n        # Calculate bin widths\n        dx = Lx / nx\n        dy = Ly / ny\n\n        # Initialize histogram for bin counts\n        counts = np.zeros((nx, ny), dtype=int)\n\n        # Populate histogram based on particle positions\n        for x, y in positions:\n            # The binning rule is robustly implemented by capping the index at nx-1 (or ny-1).\n            # This handles both internal edges (via floor) and the box boundary x=Lx (via min).\n            ix = min(nx - 1, int(np.floor(x / dx)))\n            iy = min(ny - 1, int(np.floor(y / dy)))\n            counts[ix, iy] += 1\n        \n        # Total number of particle instances across all frames\n        total_particle_instances = N * F\n        \n        # If there are no particles, entropy is zero.\n        if total_particle_instances == 0:\n            entropy = 0.0\n        else:\n            # Flatten the counts array and calculate probabilities\n            probabilities = counts.flatten() / total_particle_instances\n\n            # Calculate the sum part of the entropy formula: sum(p_i * ln(p_i))\n            # Only consider non-zero probabilities as lim_{p-0} p*ln(p) = 0\n            entropy_sum = np.sum(probabilities[probabilities  0] * np.log(probabilities[probabilities  0]))\n\n            # Calculate the final N-particle system entropy\n            # S = -N * k_B * sum(p_i * ln(p_i))\n            entropy = -N * K_B * entropy_sum\n\n        results.append(entropy)\n\n    # Format the results according to the specified output format.\n    formatted_results = [f\"{r:.6e}\" for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2463576"}, {"introduction": "Molecular dynamics simulations are powerful tools for generating configurations that obey the laws of statistical mechanics, but how do they achieve this? This advanced exercise puts you in control of a Nosé-Hoover thermostat, a deterministic algorithm designed to maintain a constant average temperature and produce a canonical ensemble. By integrating the equations of motion and analyzing the resulting trajectory, you will empirically verify that this simulation technique correctly reproduces the statistical properties, like energy distributions and variances, predicted by the Boltzmann distribution [@problem_id:2463631].", "problem": "You are asked to write a complete program that simulates a one-dimensional harmonic oscillator coupled to a single-variable Nosé–Hoover thermostat and demonstrates that the generated configurations are consistent with the canonical ensemble at temperature $T$ in reduced units. All quantities are in reduced units with particle mass $m=1$ and Boltzmann constant $k_B=1$, so that temperature $T$ has the same units as energy. No physical units beyond these reduced units are required.\n\nThe system has coordinate $x$, velocity $v$, and thermostat friction variable $\\zeta$. The potential energy is $U(x) = \\tfrac{1}{2} k x^2$ with spring constant $k$. The equations of motion for a single Nosé–Hoover thermostat with thermostat mass parameter $Q$ in these reduced units are:\n- $\\dot{x} = v$,\n- $\\dot{v} = -k x - \\zeta v$,\n- $\\dot{\\zeta} = \\dfrac{v^2 - T}{Q}$.\n\nStarting from an out-of-equilibrium initial condition $x(0)=5$, $v(0)=0$, $\\zeta(0)=0$, you must integrate these equations numerically using a fixed time step $dt$ and a $4$th-order Runge–Kutta method. Discard the first $20\\%$ of steps as burn-in. From the remaining trajectory, sample every $10$th step to form arrays of sampled positions and velocities. Denote the sampled arrays by $\\{x_i\\}$ and $\\{v_i\\}$.\n\nYour program must, for each parameter set in the test suite below, compute the following diagnostics that connect microscopic sampling to macroscopic ensemble properties expected for a canonical distribution:\n- The sample variance of positions, $\\mathrm{Var}(x)$, and the sample variance of velocities, $\\mathrm{Var}(v)$. From the canonical ensemble for a harmonic oscillator in these reduced units, the theoretical variances implied by the Boltzmann distribution are $\\sigma_x^2 = T/k$ and $\\sigma_v^2 = T$. Define the relative errors $e_x = \\left|\\mathrm{Var}(x)-\\sigma_x^2\\right|/\\sigma_x^2$ and $e_v = \\left|\\mathrm{Var}(v)-\\sigma_v^2\\right|/\\sigma_v^2$.\n- The empirical Pearson correlation coefficient between $x$ and $v$, which should be close to $0$ under the canonical factorization of position and velocity.\n- A one-sample Kolmogorov–Smirnov statistic for positions: normalize positions by the theoretical standard deviation, $z_i = x_i/\\sqrt{T/k}$, and compute the Kolmogorov–Smirnov statistic\n$$\nD = \\max\\left\\{\\max_{1\\le i\\le n}\\left(\\frac{i}{n} - \\Phi(z_{(i)})\\right),\\ \\max_{1\\le i\\le n}\\left(\\Phi(z_{(i)}) - \\frac{i-1}{n}\\right)\\right\\},\n$$\nwhere $z_{(i)}$ are the sorted normalized positions and $\\Phi$ is the cumulative distribution function of the standard normal distribution. This tests consistency of the sampled $x$-marginal with the Gaussian predicted by the canonical ensemble.\n\nFor each parameter set, return a boolean indicating whether all three conditions hold simultaneously:\n- $e_x  0.15$ and $e_v  0.15$,\n- $|{\\rm Corr}(x,v)|  0.10$,\n- $D  0.12$.\n\nImplement the $4$th-order Runge–Kutta method explicitly and do not use any stochastic thermostat or velocity-rescaling; only the deterministic Nosé–Hoover equations above.\n\nTest Suite:\nProvide results for the following parameter sets $(T, k, Q, dt, \\text{steps})$:\n- Case $1$: $(1.0, 1.0, 1.0, 0.005, 60000)$,\n- Case $2$: $(0.5, 2.0, 1.0, 0.005, 60000)$,\n- Case $3$: $(1.5, 0.7, 1.0, 0.004, 60000)$.\n\nNumerical details and conventions to follow:\n- Use initial conditions $x(0)=5$, $v(0)=0$, $\\zeta(0)=0$ for all cases.\n- Burn-in fraction is $0.20$ of the total number of steps; sample every $10$th step thereafter.\n- Compute sample means and variances using the unbiased definitions based on the data arrays after burn-in and downsampling.\n- Angles are not used in this problem.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and with no spaces, in the order of the test suite cases. For example, if all three cases pass, the output must be exactly\n[True,True,True]\nwith the appropriate boolean values for your results.", "solution": "The problem requires the validation of a Nosé–Hoover thermostat as a method for generating configurations consistent with the canonical ensemble for a one-dimensional harmonic oscillator. This is a fundamental exercise in computational statistical mechanics. The procedure involves numerically integrating the equations of motion for an extended system and then calculating specific statistical diagnostics from the resulting trajectory to compare against theoretical predictions from the Boltzmann distribution.\n\nThe state of the extended system is described by the vector $\\mathbf{y}(t) = [x(t), v(t), \\zeta(t)]^T$, where $x$ is the position, $v$ is the velocity, and $\\zeta$ is the thermostat's friction parameter. The dynamics are governed by a system of first-order ordinary differential equations (ODEs). In the specified reduced units ($m=1$, $k_B=1$), these equations are:\n$$\n\\begin{cases}\n\\dot{x} = v \\\\\n\\dot{v} = -k x - \\zeta v \\\\\n\\dot{\\zeta} = \\dfrac{v^2 - T}{Q}\n\\end{cases}\n$$\nHere, $k$ is the spring constant of the harmonic potential $U(x) = \\frac{1}{2} k x^2$, $T$ is the target temperature, and $Q$ is the thermostat mass parameter, which controls the timescale of the thermostat's response.\n\nTo solve this system numerically, we employ the $4$th-order Runge–Kutta (RK4) method, a robust and widely-used numerical integrator. For a generic ODE system $\\dot{\\mathbf{y}} = \\mathbf{f}(\\mathbf{y}, t)$, the update from a state $\\mathbf{y}_n$ at time $t_n$ to $\\mathbf{y}_{n+1}$ at time $t_{n+1} = t_n + dt$ is given by:\n$$\n\\begin{align*}\n\\mathbf{k}_1 = \\mathbf{f}(\\mathbf{y}_n, t_n) \\\\\n\\mathbf{k}_2 = \\mathbf{f}(\\mathbf{y}_n + \\frac{dt}{2} \\mathbf{k}_1, t_n + \\frac{dt}{2}) \\\\\n\\mathbf{k}_3 = \\mathbf{f}(\\mathbf{y}_n + \\frac{dt}{2} \\mathbf{k}_2, t_n + \\frac{dt}{2}) \\\\\n\\mathbf{k}_4 = \\mathbf{f}(\\mathbf{y}_n + dt \\mathbf{k}_3, t_n + dt) \\\\\n\\mathbf{y}_{n+1} = \\mathbf{y}_n + \\frac{dt}{6} (\\mathbf{k}_1 + 2\\mathbf{k}_2 + 2\\mathbf{k}_3 + \\mathbf{k}_4)\n\\end{align*}\n$$\nIn our specific case, the function $\\mathbf{f}$ does not depend explicitly on time $t$. The simulation starts from a specified out-of-equilibrium state: $x(0)=5$, $v(0)=0$, $\\zeta(0)=0$.\n\nThe simulation is run for a total number of steps. The initial $20\\%$ of the trajectory constitutes the \"burn-in\" or equilibration phase, during which the system evolves from its initial state to the target canonical distribution. These initial steps are discarded. From the remaining $80\\%$ of the trajectory, we sample the state every $10$ steps to reduce serial correlation between data points, yielding sample sets $\\{x_i\\}$ and $\\{v_i\\}$.\n\nTo verify that the sampled data conform to the canonical ensemble at temperature $T$, we compute three diagnostics:\n\n1.  **Variances and the Equipartition Theorem**: For a classical system in thermal equilibrium, the equipartition theorem states that each quadratic degree of freedom in the Hamiltonian contributes $\\frac{1}{2}k_BT$ to the average internal energy. For our harmonic oscillator, the kinetic energy is $\\frac{1}{2}mv^2$ and the potential energy is $\\frac{1}{2}kx^2$. In reduced units ($m=1, k_B=1$), this implies $\\langle \\frac{1}{2}v^2 \\rangle = \\frac{1}{2}T$ and $\\langle \\frac{1}{2}kx^2 \\rangle = \\frac{1}{2}T$. Since the mean position and velocity must be zero, $\\langle x \\rangle = 0$ and $\\langle v \\rangle = 0$, the theoretical variances are $\\sigma_x^2 = \\langle x^2 \\rangle = T/k$ and $\\sigma_v^2 = \\langle v^2 \\rangle = T$. We compute the unbiased sample variances, $\\mathrm{Var}(x)$ and $\\mathrm{Var}(v)$, and their relative errors, $e_x = |\\mathrm{Var}(x) - \\sigma_x^2| / \\sigma_x^2$ and $e_v = |\\mathrm{Var}(v) - \\sigma_v^2| / \\sigma_v^2$. Successful thermalization requires these errors to be small.\n\n2.  **Position-Velocity Correlation**: The canonical probability density function for a Hamiltonian of the form $H(x,v) = U(x) + K(v)$ is $p(x,v) \\propto e^{-\\beta U(x)} e^{-\\beta K(v)}$, where $\\beta = 1/(k_BT)$. The distribution function factorizes, implying that position and velocity are statistically independent variables. Consequently, their theoretical correlation coefficient is zero. We compute the empirical Pearson correlation coefficient $\\mathrm{Corr}(x,v)$ from the samples; its magnitude should be close to zero.\n\n3.  **Position Distribution**: The marginal probability distribution for the position $x$ is a Gaussian (normal) distribution with mean $0$ and variance $\\sigma_x^2 = T/k$. To test this hypothesis, we normalize the sampled positions to create a new variable $z_i = x_i / \\sigma_x$, which should follow a standard normal distribution, $\\mathcal{N}(0, 1)$. We use the Kolmogorov–Smirnov (KS) test to compare the empirical cumulative distribution function (CDF) of the $\\{z_i\\}$ data against the CDF of the standard normal distribution, $\\Phi(z)$. The KS statistic $D$ is the maximum absolute difference between these two CDFs. The specific computational formula provided is:\n    $$\n    D = \\max\\left\\{\\max_{1\\le i\\le n}\\left(\\frac{i}{n} - \\Phi(z_{(i)})\\right),\\ \\max_{1\\le i\\le n}\\left(\\Phi(z_{(i)}) - \\frac{i-1}{n}\\right)\\right\\}\n    $$\n    where $z_{(i)}$ are the sorted normalized positions and $n$ is the number of samples. A small value of $D$ indicates good agreement.\n\nThe program will iterate through the provided test cases. For each case, it will perform the simulation, sampling, and diagnostic calculations. A final boolean result is produced based on whether all three specified numerical thresholds are met simultaneously: $e_x, e_v  0.15$, $|\\mathrm{Corr}(x,v)|  0.10$, and $D  0.12$. The implementation will use `numpy` for numerical arrays and operations and `scipy.stats.norm.cdf` for the standard normal CDF, $\\Phi$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (T, k, Q, dt, steps)\n        (1.0, 1.0, 1.0, 0.005, 60000),\n        (0.5, 2.0, 1.0, 0.005, 60000),\n        (1.5, 0.7, 1.0, 0.004, 60000),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation_and_analyze(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef derivatives(state, k, T, Q):\n    \"\"\"\n    Computes the time derivatives for the Nosé–Hoover system.\n    state: numpy array [x, v, zeta]\n    k: spring constant\n    T: target temperature\n    Q: thermostat mass parameter\n    Returns: numpy array [dx/dt, dv/dt, dzeta/dt]\n    \"\"\"\n    x, v, zeta = state\n    dxdt = v\n    dvdt = -k * x - zeta * v\n    dzetadt = (v**2 - T) / Q\n    return np.array([dxdt, dvdt, dzetadt])\n\ndef rk4_step(state, dt, k, T, Q):\n    \"\"\"\n    Performs a single 4th-order Runge–Kutta step.\n    state: current state vector [x, v, zeta]\n    dt: time step\n    k, T, Q: system parameters\n    Returns: new state vector\n    \"\"\"\n    k1 = derivatives(state, k, T, Q)\n    k2 = derivatives(state + 0.5 * dt * k1, k, T, Q)\n    k3 = derivatives(state + 0.5 * dt * k2, k, T, Q)\n    k4 = derivatives(state + dt * k3, k, T, Q)\n    new_state = state + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n    return new_state\n\ndef run_simulation_and_analyze(params):\n    \"\"\"\n    Runs a full simulation for one parameter set and computes diagnostics.\n    params: tuple (T, k, Q, dt, steps)\n    Returns: boolean indicating if all conditions are met.\n    \"\"\"\n    T, k, Q, dt, total_steps = params\n    \n    # Initial conditions\n    x0, v0, z0 = 5.0, 0.0, 0.0\n    state = np.array([x0, v0, z0])\n    \n    # Store trajectory history\n    x_history = np.zeros(total_steps)\n    v_history = np.zeros(total_steps)\n    \n    # Simulation loop\n    for i in range(total_steps):\n        x_history[i] = state[0]\n        v_history[i] = state[1]\n        state = rk4_step(state, dt, k, T, Q)\n        \n    # Apply burn-in and sampling\n    burn_in_steps = int(0.20 * total_steps)\n    sample_stride = 10\n    \n    x_samples = x_history[burn_in_steps::sample_stride]\n    v_samples = v_history[burn_in_steps::sample_stride]\n    \n    # --- Diagnostic Calculations ---\n    \n    # 1. Sample Variances and Relative Errors\n    var_x = np.var(x_samples, ddof=1)  # Unbiased sample variance\n    var_v = np.var(v_samples, ddof=1)\n    \n    theory_var_x = T / k\n    theory_var_v = T\n    \n    e_x = np.abs(var_x - theory_var_x) / theory_var_x\n    e_v = np.abs(var_v - theory_var_v) / theory_var_v\n    \n    # 2. Pearson Correlation Coefficient\n    corr_xv = np.corrcoef(x_samples, v_samples)[0, 1]\n    \n    # 3. Kolmogorov–Smirnov Statistic\n    n_samples = len(x_samples)\n    sigma_x_theory = np.sqrt(theory_var_x)\n    z_scores = x_samples / sigma_x_theory\n    z_sorted = np.sort(z_scores)\n    \n    cdf_empirical_upper = np.arange(1, n_samples + 1) / n_samples\n    cdf_empirical_lower = np.arange(0, n_samples) / n_samples\n    cdf_theoretical = norm.cdf(z_sorted)\n    \n    d_plus = np.max(cdf_empirical_upper - cdf_theoretical)\n    d_minus = np.max(cdf_theoretical - cdf_empirical_lower)\n    D_ks = np.max([d_plus, d_minus])\n\n    # --- Validation Check ---\n    \n    cond1 = (e_x  0.15) and (e_v  0.15)\n    cond2 = np.abs(corr_xv)  0.10\n    cond3 = D_ks  0.12\n    \n    return cond1 and cond2 and cond3\n\nsolve()\n```", "id": "2463631"}]}