{"hands_on_practices": [{"introduction": "At the heart of every Canonical Monte Carlo simulation is the Metropolis acceptance criterion, which determines whether a random trial move is accepted or rejected. This decision-making process is what guides the simulation towards the correct thermodynamic equilibrium. This first exercise provides a foundational check of your understanding by presenting a simplified thought experiment: a simulation of an ideal gas where the potential energy $U$ is always zero [@problem_id:2451884]. By analyzing this limiting case, you can isolate the core logic of the acceptance rule and see precisely how it behaves when energy changes are absent, building a solid foundation before tackling more complex systems.", "problem": "You run a standard Metropolis Monte Carlo (MMC) simulation in the canonical ensemble with fixed particle number $N$, volume $V$, and temperature $T$ (the $NVT$ ensemble). The system is an ideal gas of $N$ point particles in a cubic box with periodic boundary conditions, so the configurational potential energy is $U(\\mathbf{r}^N)=0$ for all configurations $\\mathbf{r}^N$. Trial moves are single-particle displacements drawn from a symmetric proposal distribution in real space, and momenta are not sampled because configuration Monte Carlo targets the configurational distribution. Assume proposals always map to valid configurations under periodic boundary conditions.\n\nFrom the fundamental requirement that the stationary distribution in the canonical ensemble is proportional to $\\exp(-\\beta U)$, where $\\beta=1/(k_{\\mathrm{B}}T)$ and $k_{\\mathrm{B}}$ is Boltzmann’s constant, and that the acceptance rule must enforce detailed balance with this target distribution, which of the following best describes the long-time average acceptance rate of proposed particle displacements in this simulation?\n\nA. Always $1$, independent of step size and temperature, given symmetric proposals and periodic boundary conditions.\n\nB. Less than $1$ and decreasing with larger step size, because larger moves tend to increase the energy even for an ideal gas.\n\nC. Equal to the average of $\\exp(-\\beta \\Delta U)$ over proposed moves, which is less than $1$ unless $T \\to \\infty$.\n\nD. Zero, because there is no interaction energy to drive acceptance in an ideal gas.", "solution": "The problem statement must first be validated for scientific and logical integrity.\n\n### Step 1: Extract Givens\nThe problem provides the following explicit information:\n- **Simulation Method**: Standard Metropolis Monte Carlo ($MMC$).\n- **Ensemble**: Canonical ensemble ($NVT$), with fixed particle number $N$, volume $V$, and temperature $T$.\n- **System**: An ideal gas of $N$ point particles.\n- **System Geometry**: Cubic box with periodic boundary conditions ($PBC$).\n- **Potential Energy**: The configurational potential energy is $U(\\mathbf{r}^N) = 0$ for all configurations $\\mathbf{r}^N$.\n- **Trial Moves**: Single-particle displacements drawn from a symmetric proposal distribution.\n- **Simulation Detail**: Momenta are not sampled. Proposals are assumed to always map to valid configurations.\n- **Fundamental Principle 1**: The stationary distribution is proportional to $\\exp(-\\beta U)$, where $\\beta = 1/(k_{\\mathrm{B}}T)$.\n- **Fundamental Principle 2**: The acceptance rule must enforce detailed balance.\n- **Question**: What is the long-time average acceptance rate of proposed particle displacements?\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation against established criteria.\n- **Scientific Grounding**: The problem describes a canonical Monte Carlo simulation, a standard technique in computational statistical mechanics. The model system is an ideal gas ($U=0$), a fundamental concept in physics. The Metropolis algorithm, detailed balance, and the canonical ensemble are all core, well-established principles. The problem is scientifically sound.\n- **Well-Posedness**: The problem is well-posed. It asks for a specific, computable quantity (the average acceptance rate) under a clearly defined set of physical and algorithmic conditions. A unique solution exists and can be derived from the given principles.\n- **Objectivity**: The problem is stated in precise, objective, and technical language, free from ambiguity or subjective interpretation.\n\nNo flaws are found. The problem is self-contained, consistent, and scientifically valid.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. Proceeding to solution derivation.\n\n### Derivation of the Correct Answer\n\nThe Metropolis Monte Carlo algorithm generates a sequence of configurations that sample the canonical probability distribution, $\\pi(\\mathbf{r}^N) \\propto \\exp(-\\beta U(\\mathbf{r}^N))$. The transition from an old configuration, denoted by $o$, to a new trial configuration, $n$, is accepted with a probability $A(o \\to n)$. The detailed balance condition, which ensures that the simulation converges to the target distribution, is given by:\n$$ \\pi(o) P(o \\to n) = \\pi(n) P(n \\to o) $$\nwhere $P(o \\to n)$ is the total transition probability from state $o$ to $n$. This probability can be decomposed into a proposal probability $g(o \\to n)$ and an acceptance probability $A(o \\to n)$, such that $P(o \\to n) = g(o \\to n) A(o \\to n)$. The detailed balance equation becomes:\n$$ \\pi(o) g(o \\to n) A(o \\to n) = \\pi(n) g(n \\to o) A(n \\to o) $$\nThis leads to the Metropolis-Hastings acceptance ratio:\n$$ \\frac{A(o \\to n)}{A(n \\to o)} = \\frac{\\pi(n) g(n \\to o)}{\\pi(o) g(o \\to n)} $$\nThe standard choice for the acceptance probability that satisfies this is:\n$$ A(o \\to n) = \\min\\left(1, \\frac{\\pi(n) g(n \\to o)}{\\pi(o) g(o \\to n)}\\right) $$\nThe problem states that the proposal distribution is symmetric, which means the probability of proposing a move from $o$ to $n$ is the same as proposing the reverse move from $n$ to $o$. Mathematically, $g(o \\to n) = g(n \\to o)$. This simplifies the acceptance probability to the original Metropolis form:\n$$ A(o \\to n) = \\min\\left(1, \\frac{\\pi(n)}{\\pi(o)}\\right) $$\nSubstituting the canonical distribution $\\pi(\\mathbf{r}^N) \\propto \\exp(-\\beta U(\\mathbf{r}^N))$:\n$$ A(o \\to n) = \\min\\left(1, \\frac{\\exp(-\\beta U(n))}{\\exp(-\\beta U(o))}\\right) = \\min\\left(1, \\exp(-\\beta [U(n) - U(o)])\\right) $$\nLet $\\Delta U = U(n) - U(o)$ be the change in potential energy for the move. The acceptance probability is:\n$$ A(o \\to n) = \\min(1, \\exp(-\\beta \\Delta U)) $$\nThe problem specifies an ideal gas, for which the configurational potential energy $U(\\mathbf{r}^N) = 0$ for *all* possible particle configurations $\\mathbf{r}^N$. Therefore, for any trial move from any configuration $o$ to any other configuration $n$, the potential energies are:\n$$ U(o) = 0 $$\n$$ U(n) = 0 $$\nThe change in potential energy is consequently always zero:\n$$ \\Delta U = U(n) - U(o) = 0 - 0 = 0 $$\nSubstituting this result into the acceptance probability formula yields:\n$$ A(o \\to n) = \\min(1, \\exp(-\\beta \\cdot 0)) = \\min(1, \\exp(0)) = \\min(1, 1) = 1 $$\nThis demonstrates that for an ideal gas with $U=0$, every proposed Monte Carlo move is accepted with a probability of $1$. This result is independent of the temperature $T$ (since $\\beta$ is multiplied by $\\Delta U=0$) and the size of the trial displacement (as long as the proposal is valid, which is guaranteed by the problem statement).\n\nThe long-time average acceptance rate is the average of the acceptance probability over all proposed moves. Since the acceptance probability for every single move is exactly $1$, the average must also be $1$.\n\n### Option-by-Option Analysis\n\n**A. Always $1$, independent of step size and temperature, given symmetric proposals and periodic boundary conditions.**\nAs derived above, the acceptance probability for any move is $A = \\min(1, \\exp(-\\beta \\cdot 0)) = 1$. This result does not depend on the temperature $T$ (and thus $\\beta$) or the step size of the move. The conditions of symmetric proposals and periodic boundary conditions are premises used to arrive at this conclusion. This statement is perfectly consistent with our derivation.\n**Verdict: Correct**\n\n**B. Less than $1$ and decreasing with larger step size, because larger moves tend to increase the energy even for an ideal gas.**\nThis statement is fundamentally flawed. The premise \"larger moves tend to increase the energy even for an ideal gas\" is scientifically incorrect. By definition, an ideal gas has no interparticle interactions, so its configurational potential energy is constant, typically defined as $U=0$. No displacement, large or small, can change the potential energy. Consequently, the acceptance rate is not less than $1$ and does not depend on the step size.\n**Verdict: Incorrect**\n\n**C. Equal to the average of $\\exp(-\\beta \\Delta U)$ over proposed moves, which is less than $1$ unless $T \\to \\infty$.**\nThis statement contains two errors. First, the acceptance probability is $\\min(1, \\exp(-\\beta \\Delta U))$, not simply $\\exp(-\\beta \\Delta U)$. The minimum function is crucial. Second, for the ideal gas system in question, $\\Delta U = 0$ for all moves. Thus, $\\exp(-\\beta \\Delta U) = \\exp(0) = 1$ for all moves, and the average is $1$. The claim that the average is less than $1$ is false for this specific system.\n**Verdict: Incorrect**\n\n**D. Zero, because there is no interaction energy to drive acceptance in an ideal gas.**\nThis statement misinterprets the Metropolis algorithm. The lack of an energy penalty ($\\Delta U \\le 0$) leads to maximum acceptance, not zero acceptance. A move with $\\Delta U = 0$ has an acceptance probability of $\\min(1, \\exp(0)) = 1$. The algorithm is not \"driven\" by interaction energy; rather, it biases the random walk towards lower-energy configurations while still allowing exploration of higher-energy states. In the absence of any energy differences, the random walk is unbiased, and every step is accepted.\n**Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "2451884"}, {"introduction": "Moving from abstract rules to practical implementation, we must consider how particles are represented in a computer simulation. To approximate an infinite system, we confine particles to a simulation box with periodic boundary conditions (PBC). Calculating the interaction energy, and thus the change $\\Delta U$ needed for the Metropolis criterion, requires finding the distance to the *closest* periodic image of each neighboring particle—a procedure known as the minimum image convention (MIC) [@problem_id:2451882]. This practice is a crucial coding exercise that challenges you to implement the MIC for the most general case of a triclinic (non-orthogonal) cell, a fundamental skill for developing or understanding modern simulation software.", "problem": "You are given a three-dimensional periodic simulation cell defined by three non-coplanar basis vectors $\\mathbf{a}$, $\\mathbf{b}$, and $\\mathbf{c}$ that form a triclinic Bravais lattice. Let the cell matrix be $\\mathbf{A} = [\\mathbf{a}\\,\\,\\mathbf{b}\\,\\,\\mathbf{c}]$, where the columns are the basis vectors in Cartesian coordinates. For any two particle position vectors $\\mathbf{r}_i$ and $\\mathbf{r}_j$ in Cartesian coordinates, the minimum-image distance is defined as the Euclidean norm\n$$\nd(\\mathbf{r}_i,\\mathbf{r}_j) \\equiv \\min_{\\mathbf{n} \\in \\mathbb{Z}^3} \\left\\| \\left(\\mathbf{r}_j - \\mathbf{r}_i\\right) - \\mathbf{A}\\,\\mathbf{n} \\right\\|_2.\n$$\nAll lengths are in the same arbitrary but consistent unit as the components of $\\mathbf{a}$, $\\mathbf{b}$, and $\\mathbf{c}$. Your task is to compute $d(\\mathbf{r}_i,\\mathbf{r}_j)$ for each of the following independent test cases.\n\nUse the following test suite. For each test case, the simulation cell is given by its basis vectors and the two particle positions are given explicitly.\n\n- Test case $1$ (happy path, orthorhombic cell with simple wrap):\n  - Cell basis vectors: $\\mathbf{a} = (10, 0, 0)$, $\\mathbf{b} = (0, 10, 0)$, $\\mathbf{c} = (0, 0, 10)$.\n  - Positions: $\\mathbf{r}_i = (1, 1, 1)$, $\\mathbf{r}_j = (9, 1, 1)$.\n\n- Test case $2$ (triclinic skewed cell requiring a non-axis-aligned minimum image):\n  - Cell basis vectors: $\\mathbf{a} = (10, 0, 0)$, $\\mathbf{b} = (3, 8, 0)$, $\\mathbf{c} = (1, 2, 6)$.\n  - Positions: $\\mathbf{r}_i = (11.95, 7.8, 0.6)$, $\\mathbf{r}_j = (2.05, 2.2, 5.4)$.\n\n- Test case $3$ (edge case, coincident positions):\n  - Cell basis vectors: $\\mathbf{a} = (7, 0, 0)$, $\\mathbf{b} = (1, 7, 0)$, $\\mathbf{c} = (0.5, 0.5, 7)$.\n  - Positions: $\\mathbf{r}_i = (3.5, 2.0, 1.0)$, $\\mathbf{r}_j = (3.5, 2.0, 1.0)$.\n\n- Test case $4$ (boundary case, exactly half cell along one axis):\n  - Cell basis vectors: $\\mathbf{a} = (10, 0, 0)$, $\\mathbf{b} = (0, 10, 0)$, $\\mathbf{c} = (0, 0, 10)$.\n  - Positions: $\\mathbf{r}_i = (0, 0, 0)$, $\\mathbf{r}_j = (5, 0, 0)$.\n\nRequirements:\n- Compute the scalar minimum-image distance $d(\\mathbf{r}_i,\\mathbf{r}_j)$ for each test case, as defined above by the minimization over all integer triplets. Express each distance in the same unit as the basis vector components, rounded to six decimal places.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list of the four rounded floating-point distances, enclosed in square brackets, for example, $\\texttt{[d1,d2,d3,d4]}$, where each $d_k$ is rounded to six decimal places.", "solution": "The problem statement is subjected to validation.\n\nGivens are extracted verbatim.\n- Definition of minimum-image distance: $d(\\mathbf{r}_i,\\mathbf{r}_j) \\equiv \\min_{\\mathbf{n} \\in \\mathbb{Z}^3} \\left\\| \\left(\\mathbf{r}_j - \\mathbf{r}_i\\right) - \\mathbf{A}\\,\\mathbf{n} \\right\\|_2$.\n- Cell matrix: $\\mathbf{A} = [\\mathbf{a}\\,\\,\\mathbf{b}\\,\\,\\mathbf{c}]$, where $\\mathbf{a}$, $\\mathbf{b}$, $\\mathbf{c}$ are non-coplanar basis vectors forming a triclinic Bravais lattice.\n- Integer vector: $\\mathbf{n} \\in \\mathbb{Z}^3$.\n- Test case $1$: $\\mathbf{a} = (10, 0, 0)$, $\\mathbf{b} = (0, 10, 0)$, $\\mathbf{c} = (0, 0, 10)$; $\\mathbf{r}_i = (1, 1, 1)$, $\\mathbf{r}_j = (9, 1, 1)$.\n- Test case $2$: $\\mathbf{a} = (10, 0, 0)$, $\\mathbf{b} = (3, 8, 0)$, $\\mathbf{c} = (1, 2, 6)$; $\\mathbf{r}_i = (11.95, 7.8, 0.6)$, $\\mathbf{r}_j = (2.05, 2.2, 5.4)$.\n- Test case $3$: $\\mathbf{a} = (7, 0, 0)$, $\\mathbf{b} = (1, 7, 0)$, $\\mathbf{c} = (0.5, 0.5, 7)$; $\\mathbf{r}_i = (3.5, 2.0, 1.0)$, $\\mathbf{r}_j = (3.5, 2.0, 1.0)$.\n- Test case $4$: $\\mathbf{a} = (10, 0, 0)$, $\\mathbf{b} = (0, 10, 0)$, $\\mathbf{c} = (0, 0, 10)$; $\\mathbf{r}_i = (0, 0, 0)$, $\\mathbf{r}_j = (5, 0, 0)$.\n- Output requirement: A single line with a comma-separated list of four distances, each rounded to six decimal places, enclosed in square brackets.\n\nValidation verdict: The problem is valid. It is scientifically grounded in the principles of computational physics and chemistry, specifically regarding periodic boundary conditions. It is well-posed, with all necessary data provided for each test case to find a unique solution. The language is objective and precise. The problem is a standard computational task that is both verifiable and meaningful.\n\nProceeding with the solution.\n\nThe problem is to compute the minimum-image distance between two points $\\mathbf{r}_i$ and $\\mathbf{r}_j$ in a three-dimensional periodic system with a generally triclinic unit cell. The distance is defined as the minimum Euclidean norm of the vector connecting $\\mathbf{r}_j$ to any periodic image of $\\mathbf{r}_i$.\n\nLet the displacement vector in Cartesian coordinates be $\\Delta\\mathbf{r} = \\mathbf{r}_j - \\mathbf{r}_i$. The vector from $\\mathbf{r}_j$ to a periodic image of $\\mathbf{r}_i$ is given by $\\Delta\\mathbf{r} + \\mathbf{A}\\mathbf{n}$, where $\\mathbf{n} = (n_a, n_b, n_c)^T$ is a vector of integers. The problem asks for the minimum distance, which is equivalent to finding the vector of minimum length among all possible image vectors. Note that a translation of the entire system does not change the distance, so the problem is equivalent to finding the length of the shortest vector connecting the origin to any point in the lattice of points defined by $\\Delta\\mathbf{r} + \\mathbf{A}\\mathbf{n}$. We seek to compute:\n$$\nd = \\min_{\\mathbf{n} \\in \\mathbb{Z}^3} \\| \\Delta\\mathbf{r} - \\mathbf{A}\\mathbf{n} \\|_2\n$$\nThis is a statement of the Closest Vector Problem (CVP) for the lattice generated by the basis vectors $\\mathbf{a}$, $\\mathbf{b}$, and $\\mathbf{c}$. The vector we are trying to approximate with a lattice vector is $\\Delta\\mathbf{r}$.\n\nA robust method to solve this involves changing from Cartesian coordinates to fractional (or reduced) coordinates, which are expressed in the basis of the cell vectors. Any Cartesian vector $\\mathbf{v}$ can be written in terms of fractional coordinates $\\mathbf{s}=(s_a, s_b, s_c)^T$ as $\\mathbf{v} = s_a\\mathbf{a} + s_b\\mathbf{b} + s_c\\mathbf{c} = \\mathbf{A}\\mathbf{s}$. The transformation is therefore $\\mathbf{s} = \\mathbf{A}^{-1}\\mathbf{v}$, where $\\mathbf{A}^{-1}$ is the inverse of the cell matrix.\n\nThe algorithm proceeds as follows:\n$1$. Compute the Cartesian displacement vector $\\Delta\\mathbf{r} = \\mathbf{r}_j - \\mathbf{r}_i$.\n$2$. Construct the cell matrix $\\mathbf{A} = [\\mathbf{a} \\,\\, \\mathbf{b} \\,\\, \\mathbf{c}]$ and compute its inverse $\\mathbf{A}^{-1}$.\n$3$. Convert the Cartesian displacement vector $\\Delta\\mathbf{r}$ into fractional coordinates: $\\Delta\\mathbf{s} = \\mathbf{A}^{-1}\\Delta\\mathbf{r}$. The components of $\\Delta\\mathbf{s}$ represent the displacement in units of the basis vectors $\\mathbf{a}$, $\\mathbf{b}$, and $\\mathbf{c}$.\n$4$. Our goal is to find an integer vector $\\mathbf{n}$ that, when subtracted from $\\Delta\\mathbf{s}$, results in a new fractional vector $\\Delta\\mathbf{s}_{\\text{mic}} = \\Delta\\mathbf{s} - \\mathbf{n}$ corresponding to the shortest possible Cartesian vector $\\Delta\\mathbf{r}_{\\text{mic}} = \\mathbf{A}\\Delta\\mathbf{s}_{\\text{mic}}$.\n$5$. For an orthorhombic cell, simply rounding each component of $\\Delta\\mathbf{s}$ to the nearest integer gives the correct $\\mathbf{n}$. That is, $\\mathbf{n} = \\text{round}(\\Delta\\mathbf{s})$. For a general, highly skewed triclinic cell, this simple rounding is not guaranteed to yield the shortest vector. The vector produced by this rounding lies in the primitive cell centered at the origin (fractional coordinates in $[-0.5, 0.5]$), but the shortest vector may lie in an adjacent cell image.\n$6$. To guarantee finding the minimum for any triclinic cell, we must search a small space of integer vectors $\\mathbf{n}$ around a central estimate. The central estimate is obtained by rounding: $\\mathbf{n}_{\\text{center}} = \\text{round}(\\Delta\\mathbf{s})$. We then search all integer vectors $\\mathbf{n}$ in a $3 \\times 3 \\times 3$ cube centered on $\\mathbf{n}_{\\text{center}}$. That is, we test all $\\mathbf{n} = \\mathbf{n}_{\\text{center}} + \\delta\\mathbf{n}$, where $\\delta\\mathbf{n}$ is a vector with components from $\\{-1, 0, 1\\}$. This search space of $27$ candidates is sufficient because the Wigner-Seitz cell, which by definition contains all points closer to the origin than any other lattice point, is guaranteed to be contained within the volume spanned by these $27$ cell images.\n$7$. The search procedure:\n    a. Initialize a minimum squared distance, $d^2_{\\min}$, to a very large number.\n    b. Iterate through all $27$ candidate integer vectors $\\mathbf{n}_{ijk} = \\mathbf{n}_{\\text{center}} + (i, j, k)^T$ for $i,j,k \\in \\{-1, 0, 1\\}$.\n    c. For each $\\mathbf{n}_{ijk}$, calculate the corresponding image vector in Cartesian coordinates: $\\Delta\\mathbf{r}_{\\text{image}} = \\Delta\\mathbf{r} - \\mathbf{A}\\mathbf{n}_{ijk}$.\n    d. Compute its squared norm: $d^2_{\\text{image}} = \\|\\Delta\\mathbf{r}_{\\text{image}}\\|^2_2$.\n    e. Update the minimum: $d^2_{\\min} = \\min(d^2_{\\min}, d^2_{\\text{image}})$.\n$8$. After checking all $27$ candidates, the minimum-image distance is $d = \\sqrt{d^2_{\\min}}$.\n\nThis algorithm will be applied to each test case. For numerical stability and efficiency, we minimize the squared distance to avoid repeated square root calculations inside the loop.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the minimum-image distance for several test cases in triclinic\n    periodic boundary conditions.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"cell_vectors\": np.array([[10, 0, 0], [0, 10, 0], [0, 0, 10]]).T,\n            \"r_i\": np.array([1, 1, 1]),\n            \"r_j\": np.array([9, 1, 1])\n        },\n        {\n            \"cell_vectors\": np.array([[10, 0, 0], [3, 8, 0], [1, 2, 6]]).T,\n            \"r_i\": np.array([11.95, 7.8, 0.6]),\n            \"r_j\": np.array([2.05, 2.2, 5.4])\n        },\n        {\n            \"cell_vectors\": np.array([[7, 0, 0], [1, 7, 0], [0.5, 0.5, 7]]).T,\n            \"r_i\": np.array([3.5, 2.0, 1.0]),\n            \"r_j\": np.array([3.5, 2.0, 1.0])\n        },\n        {\n            \"cell_vectors\": np.array([[10, 0, 0], [0, 10, 0], [0, 0, 10]]).T,\n            \"r_i\": np.array([0, 0, 0]),\n            \"r_j\": np.array([5, 0, 0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"cell_vectors\"]\n        r_i = case[\"r_i\"]\n        r_j = case[\"r_j\"]\n\n        # Step 1: Compute Cartesian displacement vector\n        delta_r = r_j - r_i\n        \n        # If positions are coincident, distance is zero.\n        if np.allclose(delta_r, 0.0):\n            results.append(0.0)\n            continue\n\n        # Step 2: Compute inverse of cell matrix\n        A_inv = np.linalg.inv(A)\n\n        # Step 3: Convert to fractional coordinates\n        delta_s = A_inv @ delta_r\n\n        # Step 4  5: Get central integer vector guess and search neighbors\n        n_center = np.round(delta_s)\n        \n        min_dist_sq = np.inf\n\n        # Step 6  7: Search the 3x3x3 block of images around the central guess\n        for i in [-1, 0, 1]:\n            for j in [-1, 0, 1]:\n                for k in [-1, 0, 1]:\n                    # Integer vector for the current periodic image\n                    n = n_center + np.array([i, j, k])\n                    \n                    # Cartesian vector to the periodic image\n                    image_vector = delta_r - A @ n\n                    \n                    # Squared distance\n                    dist_sq = np.dot(image_vector, image_vector)\n                    \n                    if dist_sq  min_dist_sq:\n                        min_dist_sq = dist_sq\n\n        # Step 8: Final distance is the square root of the minimum squared distance\n        final_distance = np.sqrt(min_dist_sq)\n        results.append(final_distance)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2451882"}, {"introduction": "Once a simulation is complete, it generates a long trajectory of system states. A critical, and often overlooked, step is the statistical analysis of this output. The sequence of configurations from a Markov chain is not a series of independent random samples; rather, each state is correlated with the ones that came before it. This final practice delves into the essential task of analyzing this time-series data [@problem_id:2451857]. By calculating the autocorrelation function, the integrated autocorrelation time $\\tau_{\\mathrm{int}}$, and the effective sample size $N_{\\mathrm{eff}}$, you will learn how to quantify the efficiency of your simulation and determine how to correctly calculate statistical averages and their associated errors.", "problem": "You are given the task of determining an appropriate sampling frequency for recording potential energy from a Markov Chain Monte Carlo trajectory in the canonical (NVT) ensemble. Consider a discrete-time, stationary series of potential energies $\\{U_t\\}_{t=0}^{N-1}$ measured in kilojoules per mole (kJ/mol) at unit time intervals of one Monte Carlo step. Define the sample mean $\\hat{\\mu}$, the unbiased sample autocovariance $\\hat{C}(k)$, and the normalized autocorrelation function $\\hat{\\rho}(k)$ by\n$$\n\\hat{\\mu} = \\frac{1}{N} \\sum_{t=0}^{N-1} U_t,\n\\quad\n\\hat{C}(k) = \\frac{1}{N-k} \\sum_{t=0}^{N-k-1} (U_t - \\hat{\\mu})(U_{t+k} - \\hat{\\mu}) \\quad \\text{for } k = 0,1,\\dots,N-1,\n$$\n$$\n\\hat{\\rho}(k) = \\frac{\\hat{C}(k)}{\\hat{C}(0)} \\quad \\text{for } k = 0,1,\\dots,N-1,\n$$\nwhere $N \\ge 2$ and $\\hat{C}(0)  0$. The integrated autocorrelation time $\\tau_{\\mathrm{int}}$ (in Monte Carlo steps) is defined as\n$$\n\\tau_{\\mathrm{int}} = \\frac{1}{2} + \\sum_{k=1}^{K} \\hat{\\rho}(k),\n$$\nwhere $K$ is the largest nonnegative integer such that $\\hat{\\rho}(k)  0$ for all $k \\in \\{1,2,\\dots,K\\}$ (that is, the sum is truncated at the first nonpositive value of $\\hat{\\rho}(k)$, excluding $k=0$). The effective sample size is defined as\n$$\nN_{\\mathrm{eff}} = \\frac{N}{2 \\, \\tau_{\\mathrm{int}}}.\n$$\nGiven a user-specified correlation threshold $c \\in (0,1)$, define the minimal sampling interval $s_c$ (in Monte Carlo steps) as the smallest positive integer $k \\ge 1$ such that $|\\hat{\\rho}(k)| \\le c$. If no such $k$ exists for $k \\in \\{1,2,\\dots,N-1\\}$, define $s_c = N-1$. The sampling frequency is then\n$$\nf_c = \\frac{1}{s_c} \\quad \\text{in samples per Monte Carlo step}.\n$$\n\nTest suite and data generation. For each test case below, generate a synthetic potential energy trajectory $\\{U_t\\}$ using a discrete-time Ornstein–Uhlenbeck process (equivalently, a stationary autoregressive process of order $1$) with parameters $(N, \\mu, \\sigma, \\tau_c, \\text{seed})$ as follows:\n- Let $\\phi = \\exp(-1/\\tau_c)$.\n- Initialize $U_0 = \\mu$.\n- For $t = 0,1,\\dots,N-2$, update\n$$\nU_{t+1} = \\mu + \\phi \\left(U_t - \\mu \\right) + \\eta_t,\n$$\nwhere $\\eta_t \\sim \\mathcal{N}\\!\\left(0, \\sigma_\\eta^2\\right)$ are independent Gaussian increments with variance $\\sigma_\\eta^2 = \\sigma^2 \\left(1 - \\phi^2\\right)$. Here, $\\mu$ is the stationary mean (in kJ/mol), $\\sigma$ is the stationary standard deviation (in kJ/mol), and $\\tau_c$ is the correlation time in Monte Carlo steps. Use the given integer seed to initialize a pseudorandom number generator in a reproducible manner.\n\nYour program must, for each test case, generate $\\{U_t\\}$, compute $\\hat{\\rho}(k)$, compute $\\tau_{\\mathrm{int}}$, determine $s_c$ for the given threshold $c$, compute $f_c$, and compute $N_{\\mathrm{eff}}$.\n\nUnit and output requirements.\n- Report $s_c$ as an integer number of Monte Carlo steps.\n- Report $f_c$ in samples per Monte Carlo step, rounded to six decimal places.\n- Report $\\tau_{\\mathrm{int}}$ in Monte Carlo steps, rounded to three decimal places.\n- Report $N_{\\mathrm{eff}}$ as an integer, rounded to the nearest integer.\n- Angles do not appear in this problem.\n- Percentages do not appear in this problem.\n\nTest suite:\n- Case A: $(N, \\mu, \\sigma, \\tau_c, c, \\text{seed}) = (20000, -10.0, 2.0, 20.0, 0.1, 12345)$.\n- Case B: $(N, \\mu, \\sigma, \\tau_c, c, \\text{seed}) = (10000, 0.0, 1.0, 2.0, 0.05, 24680)$.\n- Case C: $(N, \\mu, \\sigma, \\tau_c, c, \\text{seed}) = (50000, -50.0, 5.0, 100.0, 0.2, 13579)$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list $[s_c, f_c, \\tau_{\\mathrm{int}}, N_{\\mathrm{eff}}]$ in that order. For example, the output must look like\n$[[s_1,f_1,\\tau_1,N_{\\mathrm{eff},1}],[s_2,f_2,\\tau_2,N_{\\mathrm{eff},2}],[s_3,f_3,\\tau_3,N_{\\mathrm{eff},3}]]$,\nwith all quantities expressed in the units specified above and rounded as required.", "solution": "The problem statement is assessed to be valid. It presents a well-posed, scientifically grounded problem in computational statistics, pertinent to the analysis of simulation data in computational chemistry and physics. The initial description mentions a \"canonical Monte Carlo (NVE) setting\", which is a minor terminological incongruity; canonical Monte Carlo simulations are performed in the $NVT$ ensemble (constant number of particles $N$, volume $V$, and temperature $T$), while $NVE$ simulations are characteristic of microcanonical molecular dynamics. However, this does not affect the core task, which is the statistical analysis of a synthetically generated time series using a precisely defined procedure. The provided model, a discrete-time Ornstein-Uhlenbeck process, is a standard and appropriate choice for this purpose.\n\nThe solution proceeds by systematically implementing the specified definitions and algorithms for each test case. The overall procedure is as follows:\n\n1.  **Data Generation**: For each test case defined by parameters $(N, \\mu, \\sigma, \\tau_c, c, \\text{seed})$, a potential energy time series $\\{U_t\\}_{t=0}^{N-1}$ is generated. This is accomplished using the specified autoregressive process of order $1$ (AR($1$)):\n    $$\n    U_{t+1} = \\mu + \\phi \\left(U_t - \\mu \\right) + \\eta_t,\n    $$\n    with the initial condition $U_0 = \\mu$. The autoregressive parameter $\\phi$ is determined by the correlation time $\\tau_c$ as $\\phi = \\exp(-1/\\tau_c)$. The noise term $\\eta_t$ is drawn from a Gaussian distribution with mean $0$ and variance $\\sigma_\\eta^2 = \\sigma^2(1-\\phi^2)$, ensuring the process has a stationary standard deviation of $\\sigma$. A distinct pseudorandom number generator is initialized for each test case using the provided `seed` to guarantee reproducibility.\n\n2.  **Autocorrelation Analysis**: Once the time series $\\{U_t\\}$ is generated, its statistical properties are estimated.\n    - First, the sample mean $\\hat{\\mu} = \\frac{1}{N} \\sum_{t=0}^{N-1} U_t$ is computed.\n    - The unbiased sample autocovariance function, $\\hat{C}(k)$, is then calculated for lags $k = 0, 1, \\dots, N-1$:\n      $$\n      \\hat{C}(k) = \\frac{1}{N-k} \\sum_{t=0}^{N-k-1} (U_t - \\hat{\\mu})(U_{t+k} - \\hat{\\mu}).\n      $$\n      Computationally, this is performed efficiently. The sums $\\sum (U_t - \\hat{\\mu})(U_{t+k} - \\hat{\\mu})$ for all $k$ are calculated in a single vectorized operation, equivalent to a discrete convolution, and then each sum is divided by the appropriate normalization factor $N-k$.\n    - The normalized autocorrelation function (ACF), $\\hat{\\rho}(k)$, is obtained by normalizing the autocovariance by its value at lag $0$:\n      $$\n      \\hat{\\rho}(k) = \\frac{\\hat{C}(k)}{\\hat{C}(0)}.\n      $$\n\n3.  **Integrated Autocorrelation Time Calculation**: The integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, quantifies the number of simulation steps after which the system's \"memory\" of its previous state has decayed. It is estimated using the formula:\n    $$\n    \\tau_{\\mathrm{int}} = \\frac{1}{2} + \\sum_{k=1}^{K} \\hat{\\rho}(k).\n    $$\n    The summation window $K$ is determined by a standard heuristic to avoid accumulating noise from the tail of the ACF. The sum is truncated just before the first non-positive value of $\\hat{\\rho}(k)$ for $k \\ge 1$. Specifically, $K$ is the largest integer such that $\\hat{\\rho}(k)  0$ for all $k$ from $1$ to $K$. If all $\\hat{\\rho}(k)$ for $k \\ge 1$ are positive, the sum extends to $k=N-1$.\n\n4.  **Effective Sample Size**: The presence of correlation in the data means that not all $N$ samples are statistically independent. The effective number of independent samples, $N_{\\mathrm{eff}}$, is estimated as:\n    $$\n    N_{\\mathrm{eff}} = \\frac{N}{2 \\, \\tau_{\\mathrm{int}}}.\n    $$\n    This quantity is crucial for estimating the statistical error of any average property computed from the correlated time series. A small $N_{\\mathrm{eff}}$ relative to $N$ indicates high correlation and, consequently, high uncertainty in the calculated averages.\n\n5.  **Sampling Interval and Frequency**: To collect a dataset of nearly uncorrelated samples for future analysis, one might sample the trajectory at an interval $s$ greater than $1$ step. The problem defines a minimal sampling interval, $s_c$, based on a user-specified correlation threshold $c \\in (0,1)$. $s_c$ is the smallest integer lag $k \\ge 1$ at which the absolute value of the ACF drops to or below the threshold $c$:\n    $$\n    s_c = \\min \\{k \\in \\{1, \\dots, N-1\\} \\mid |\\hat{\\rho}(k)| \\le c \\}.\n    $$\n    If the ACF never drops below this threshold, $s_c$ is set to its maximum possible value, $N-1$. The corresponding sampling frequency, in units of samples per Monte Carlo step, is simply the reciprocal of the interval:\n    $$\n    f_c = \\frac{1}{s_c}.\n    $$\n\nThese five steps are algorithmically implemented and executed for each of the three test cases provided. The final numerical results for $s_c$, $f_c$, $\\tau_{\\mathrm{int}}$, and $N_{\\mathrm{eff}}$ are then rounded according to the specified precision and formatted into the required output structure.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    def calculate_metrics(N, mu, sigma, tau_c, c, seed):\n        \"\"\"\n        Generates time series data and computes specified metrics for a single case.\n        \n        Args:\n            N (int): Length of the time series.\n            mu (float): Stationary mean of the process.\n            sigma (float): Stationary standard deviation of the process.\n            tau_c (float): Correlation time in Monte Carlo steps.\n            c (float): Correlation threshold for determining sampling interval.\n            seed (int): Seed for the random number generator.\n\n        Returns:\n            list: A list containing [s_c, f_c, tau_int, N_eff] with specified rounding.\n        \"\"\"\n        # 1. Data Generation\n        # Use the modern, recommended method for reproducible random number generation.\n        rng = np.random.default_rng(seed)\n        \n        # Calculate AR(1) process parameters.\n        phi = np.exp(-1.0 / tau_c) if tau_c  0 else 0.0\n        \n        sigma_eta_sq = sigma**2 * (1 - phi**2)\n        # Handle potential floating point precision issues.\n        sigma_eta = np.sqrt(max(0.0, sigma_eta_sq))\n        \n        # Generate the time series U_t.\n        U = np.zeros(N)\n        U[0] = mu\n        # Pre-generating noise variates is more efficient than calling in a loop.\n        noise_terms = rng.normal(loc=0.0, scale=sigma_eta, size=N - 1)\n        \n        for t in range(N - 1):\n            U[t+1] = mu + phi * (U[t] - mu) + noise_terms[t]\n\n        # 2. Autocorrelation Analysis\n        mu_hat = np.mean(U)\n        U_demeaned = U - mu_hat\n        \n        # Calculate unnormalized autocovariance sum using np.correlate.\n        # The segment `[N-1:]` corresponds to non-negative lags k=0, 1, ..., N-1.\n        unnormalized_C = np.correlate(U_demeaned, U_demeaned, mode='full')[N-1:]\n        \n        # Normalization factors are (N-k) for k=0, ..., N-1.\n        norm_factors = np.arange(N, 0, -1)\n        C_hat = unnormalized_C / norm_factors\n        \n        # The problem states C(0)0, so we assume C_hat[0] is strictly positive.\n        rho_hat = C_hat / C_hat[0]\n\n        # 3. Integrated Autocorrelation Time\n        positive_lags_rho = rho_hat[1:]\n        non_positive_indices = np.where(positive_lags_rho = 0)[0]\n        \n        if len(non_positive_indices)  0:\n            # Truncate sum at the first non-positive ACF value.\n            K = non_positive_indices[0]\n            sum_rho = np.sum(positive_lags_rho[:K])\n        else:\n            # If all ACF values for k0 are positive, sum all of them.\n            sum_rho = np.sum(positive_lags_rho)\n            \n        tau_int = 0.5 + sum_rho\n\n        # 4. Effective Sample Size\n        # tau_int is guaranteed to be = 0.5 given the definition.\n        N_eff = N / (2 * tau_int)\n\n        # 5. Sampling Interval and Frequency\n        abs_rho_hat_lags = np.abs(rho_hat[1:])\n        below_threshold_indices = np.where(abs_rho_hat_lags = c)[0]\n        \n        if len(below_threshold_indices)  0:\n            # The minimal sampling interval s_c is the smallest k = 1.\n            s_c = below_threshold_indices[0] + 1\n        else:\n            # If correlation never drops below threshold, use N-1.\n            s_c = N - 1\n\n        f_c = 1.0 / s_c\n        \n        # Rounding according to problem specifications.\n        s_c_out = int(s_c)\n        f_c_out = round(f_c, 6)\n        tau_int_out = round(tau_int, 3)\n        N_eff_out = int(round(N_eff, 0))\n    \n        return [s_c_out, f_c_out, tau_int_out, N_eff_out]\n\n    # Test cases from the problem statement.\n    test_cases = [\n        # (N, mu, sigma, tau_c, c, seed)\n        (20000, -10.0, 2.0, 20.0, 0.1, 12345),  # Case A\n        (10000, 0.0, 1.0, 2.0, 0.05, 24680),   # Case B\n        (50000, -50.0, 5.0, 100.0, 0.2, 13579), # Case C\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = calculate_metrics(*case_params)\n        results.append(result)\n\n    # Final print statement must be in the exact required format.\n    # The format is a string representation of a list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2451857"}]}