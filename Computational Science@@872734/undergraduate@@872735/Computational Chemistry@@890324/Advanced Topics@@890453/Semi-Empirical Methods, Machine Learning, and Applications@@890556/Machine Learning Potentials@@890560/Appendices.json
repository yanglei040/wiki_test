{"hands_on_practices": [{"introduction": "A powerful machine learning potential must represent atomic environments in a way that respects fundamental physical symmetries like translation, rotation, and permutation. This exercise provides direct, hands-on experience in building a core component of modern MLPs: an atom-centered symmetry function. By implementing a Behler-Parrinello-style descriptor from first principles, you will bridge the gap between abstract invariance requirements and concrete, computable features that enable a neural network to learn a potential energy surface [@problem_id:2457438].", "problem": "Implement a program that derives and computes a basic two-body Behler–Parrinello-style symmetry function for Argon (Ar) atoms and evaluates it on a small test suite of geometries. The purpose is to connect invariance requirements of machine-learned interatomic potentials to a concrete descriptor and to demonstrate numerical behavior under different parameter choices. The overall context is that the total potential energy surface of a system can be approximated as a sum of atomic contributions, each depending on a localized, symmetry-invariant representation of its neighborhood, as in High-Dimensional Neural Network Potentials (HDNNP). Your task is to derive, from invariance principles, a two-body radial symmetry function and implement it.\n\nStart from the following fundamental base:\n- Translational and rotational invariance of a scalar potential energy imply that a local descriptor for an atom must be constructed from internal coordinates such as interatomic distances.\n- For finite-range interactions and locality in the learned mapping, impose a smooth cutoff at a finite radius so that distant atoms beyond a cutoff do not contribute and forces remain well-behaved.\n- To resolve the radial distribution around an atom, use a radial basis with tunable width and center parameters so that the representation can distinguish environments at different length scales.\n\nFrom these principles, derive and then implement a two-body radial symmetry function $G^2$ for a chosen central atom $i$ of the form\n- a sum over neighbor atoms $j \\neq i$,\n- a smooth, finite-range cutoff function that is $C^1$-continuous and equals zero at the cutoff radius,\n- and a localized radial weight that can shift and sharpen around a chosen distance.\n\nConcretely specify and use the following forms in your derivation and implementation:\n- Use the cosine cutoff\n$$\nf_c(r; R_c) = \n\\begin{cases}\n\\dfrac{1}{2}\\left[\\cos\\!\\left(\\dfrac{\\pi r}{R_c}\\right) + 1\\right], & r \\le R_c,\\\\\n0, & r > R_c,\n\\end{cases}\n$$\nwith the cosine argument in radians.\n- Use a Gaussian-like radial basis\n$$\n\\exp\\!\\left[-\\eta\\,(r - R_s)^2\\right],\n$$\nwith width parameter $\\eta$ and shift $R_s$.\n- Combine them in the two-body symmetry function\n$$\nG_i^{2}(\\eta, R_s, R_c) = \\sum_{j \\ne i} \\exp\\!\\left[-\\eta\\,(r_{ij} - R_s)^2\\right]\\, f_c(r_{ij}; R_c),\n$$\nwhere $r_{ij}$ is the Euclidean distance between atoms $i$ and $j$.\n\nAll atoms are Argon (Ar), treated as a single chemical species, so no species-dependent weighting is required. Distances $r_{ij}$, cutoff $R_c$, and shift $R_s$ must be expressed in Ångström, and $\\eta$ in $\\text{Å}^{-2}$. The cosine function must take its argument in radians.\n\nProgram requirements:\n- Implement a function that, given a set of Cartesian coordinates (in Ångström), an index $i$ for the central atom, and parameters $(\\eta, R_s, R_c)$, computes $G_i^{2}(\\eta, R_s, R_c)$ using the formulas above.\n- Use standard three-dimensional Euclidean distance. Do not apply periodic boundary conditions.\n- Numerical stability: Exclude self-interaction ($j = i$). Distances $r_{ij}$ are strictly nonnegative; do not special-case $r_{ij} = 0$ beyond excluding self-interaction.\n\nTest suite:\nEvaluate $G_i^{2}$ for each of the following five cases. Each case specifies $(\\text{positions}, i, R_c, \\eta, R_s)$, with all distances in Ångström and $\\eta$ in $\\text{Å}^{-2}$:\n- Case A:\n  - positions: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 0.5$\n  - $R_s = 0.0$\n- Case B:\n  - positions: $\\big[(0,0,0)\\big]$\n  - $i = 0$\n  - $R_c = 3.0$\n  - $\\eta = 1.0$\n  - $R_s = 0.0$\n- Case C:\n  - positions: $\\big[(0,0,0),(5.0,0,0),(-5.0,0,0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 1.0$\n  - $R_s = 0.0$\n- Case D:\n  - positions: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 2.0$\n  - $R_s = 2.5$\n- Case E:\n  - positions: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 1$\n  - $R_c = 5.0$\n  - $\\eta = 0.5$\n  - $R_s = 0.0$\n\nOutput specification:\n- For each case, compute a single floating-point value $G_i^{2}$.\n- Round each result to exactly $6$ decimal places using standard rounding.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order A, B, C, D, E. For example, an output with generic placeholders should look like \"[0.123456,0.000000,0.000000,1.234567,0.654321]\".", "solution": "The problem presented is valid, scientifically sound, and well-posed. It requires the derivation and implementation of a two-body radial symmetry function, a fundamental component of modern machine-learned interatomic potentials such as High-Dimensional Neural Network Potentials (HDNNPs). We shall first deduce the form of this function from first principles, and then detail the algorithm for its computation.\n\nThe potential energy $E$ of a system of atoms is a scalar quantity. For it to be physically meaningful, it must be invariant under translation and rotation of the entire system, as well as under the permutation of identical atoms. In the HDNNP scheme, the total energy is decomposed into atomic contributions $E_i$, where $E = \\sum_i E_i$. Each atomic energy $E_i$ is a function of the local environment of atom $i$, characterized by a set of descriptors or \"symmetry functions,\" $\\{G_i\\}$. Therefore, these symmetry functions must themselves be invariant to the aforementioned transformations.\n\n1.  **Translational and Rotational Invariance**: These symmetries dictate that the descriptors for atom $i$ must depend only on the internal coordinates of its local environment, not on the absolute Cartesian coordinates of the atoms in a global frame. The simplest set of internal coordinates consists of the scalar distances $r_{ij}$ between the central atom $i$ and its neighbors $j$. Any function of these distances, $G_i = F(\\{r_{ij}\\}_{j \\neq i})$, is automatically invariant to rigid translation and rotation of the atomic assembly.\n\n2.  **Permutational Invariance**: The energy contribution of atom $i$ must not depend on the arbitrary labeling of its identical neighbors. If atoms $j$ and $k$ are of the same species, swapping them must not change the value of the descriptor. The simplest mathematical construct that satisfies this is a sum over all neighbors. Thus, we propose a descriptor of the form $G_i = \\sum_{j \\neq i} g(r_{ij})$, where $g$ is some function of the interatomic distance. This form is the basis of the two-body symmetry function.\n\n3.  **Locality and Smoothness**: Physical interactions are local in nature; the influence of very distant atoms is negligible. To model this, we introduce a smooth cutoff function, $f_c(r_{ij}; R_c)$, which multiplies the contribution of each neighbor. This function must be equal to $1$ for small distances, and smoothly go to $0$ as the distance $r_{ij}$ approaches a cutoff radius $R_c$. For distances $r_{ij} > R_c$, the contribution is exactly zero. The requirement for smoothness, specifically $C^1$-continuity (continuous first derivative), is critical. The forces on atoms are calculated as the negative gradient of the potential energy, $\\mathbf{F}_k = -\\nabla_{\\mathbf{r}_k} E$. Discontinuities in the first derivative of the energy would lead to unphysical, infinite forces. The provided cosine cutoff function is:\n    $$\n    f_c(r; R_c) = \n    \\begin{cases}\n    \\frac{1}{2}\\left[\\cos\\left(\\frac{\\pi r}{R_c}\\right) + 1\\right], & r \\le R_c,\\\\\n    0, & r > R_c.\n    \\end{cases}\n    $$\n    At the cutoff radius $r = R_c$, the function value is $f_c(R_c; R_c) = \\frac{1}{2}[\\cos(\\pi) + 1] = \\frac{1}{2}[-1 + 1] = 0$, ensuring continuity. Its derivative is $f'_c(r; R_c) = -\\frac{\\pi}{2R_c}\\sin(\\frac{\\pi r}{R_c})$. At $r = R_c$, the derivative is $f'_c(R_c; R_c) = -\\frac{\\pi}{2R_c}\\sin(\\pi) = 0$, which matches the derivative of the zero function for $r > R_c$. Thus, the function is $C^1$-continuous as required.\n\n4.  **Radial Resolution**: A simple sum of cutoff functions would only provide a weighted count of neighbors within the cutoff sphere. To create a descriptor that can distinguish different radial structures, we introduce a radial basis function. The specified Gaussian form, $\\exp[-\\eta(r_{ij} - R_s)^2]$, serves this purpose. This function is centered at a distance $R_s$ and has a characteristic width controlled by the parameter $\\eta$. A larger $\\eta$ corresponds to a narrower, more sharply peaked Gaussian. By using a set of these functions with different parameters $(\\eta, R_s)$, one can resolve the radial distribution of neighbors around the central atom $i$.\n\nCombining these four principles—invariance from using distances, permutation symmetry from summation, locality from a smooth cutoff, and resolution from a radial basis—we arrive at the specified two-body radial symmetry function, designated as $G_i^2$:\n$$\nG_i^{2}(\\eta, R_s, R_c) = \\sum_{j \\ne i} \\exp\\!\\left[-\\eta\\,(r_{ij} - R_s)^2\\right]\\, f_c(r_{ij}; R_c)\n$$\nThe sum is over all atoms $j$ in the system, excluding the central atom $i$. For each neighbor $j$, we calculate its contribution only if its distance $r_{ij}$ from atom $i$ is less than or equal to the cutoff radius $R_c$.\n\nThe computational procedure is as follows:\nGiven a set of Cartesian coordinates for $N$ atoms, $\\{\\mathbf{r}_k\\}_{k=0,..,N-1}$, a central atom index $i$, and parameters $\\eta$, $R_s$, and $R_c$:\n1.  Initialize the symmetry function value, $G_i^2$, to $0$.\n2.  Identify the coordinate vector of the central atom, $\\mathbf{r}_i$.\n3.  Iterate through all other atoms $j$ where $j \\in \\{0, 1, ..., N-1\\}$ and $j \\neq i$.\n4.  For each neighbor $j$, compute the Euclidean distance $r_{ij} = ||\\mathbf{r}_j - \\mathbf{r}_i|| = \\sqrt{(x_j-x_i)^2 + (y_j-y_i)^2 + (z_j-z_i)^2}$.\n5.  Check if $r_{ij} \\le R_c$. If not, the contribution from atom $j$ is $0$, and we proceed to the next neighbor.\n6.  If $r_{ij} \\le R_c$, calculate the two components of the term:\n    -   The radial basis term: $T_{\\text{rad}} = \\exp[-\\eta(r_{ij} - R_s)^2]$.\n    -   The cutoff function term: $T_{\\text{cut}} = \\frac{1}{2}[\\cos(\\frac{\\pi r_{ij}}{R_c}) + 1]$.\n7.  Add the product of these terms, $T_{\\text{rad}} \\times T_{\\text{cut}}$, to the running sum for $G_i^2$.\n8.  After iterating through all neighbors $j$, the final sum is the value of the symmetry function for atom $i$.\n\nThis procedure will now be implemented and applied to the five specified test cases. All units must be consistent; distances ($r_{ij}$, $R_s$, $R_c$) are in Ångström ($\\text{Å}$), and the parameter $\\eta$ is in $\\text{Å}^{-2}$, ensuring the argument of the exponential is dimensionless.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the Behler-Parrinello G2 symmetry function\n    for a series of test cases.\n    \"\"\"\n\n    def compute_g2(positions, i, R_c, eta, R_s):\n        \"\"\"\n        Computes the G2 symmetry function for a central atom i.\n        \n        Args:\n            positions (np.ndarray): Array of shape (N, 3) with Cartesian coordinates.\n            i (int): Index of the central atom.\n            R_c (float): Cutoff radius in Angstrom.\n            eta (float): Width parameter in Angstrom^-2.\n            R_s (float): Shift parameter in Angstrom.\n        \n        Returns:\n            float: The computed value of the G2 symmetry function.\n        \"\"\"\n        if positions.shape[0] = 1:\n            return 0.0\n\n        central_atom_pos = positions[i]\n        g2_value = 0.0\n\n        for j in range(positions.shape[0]):\n            if i == j:\n                continue\n\n            neighbor_pos = positions[j]\n            # Calculate Euclidean distance\n            r_ij = np.linalg.norm(central_atom_pos - neighbor_pos)\n\n            # Apply the cutoff condition\n            if r_ij = R_c:\n                # Cosine cutoff function\n                fc = 0.5 * (np.cos(np.pi * r_ij / R_c) + 1.0)\n                \n                # Gaussian-like radial basis function\n                radial_term = np.exp(-eta * (r_ij - R_s)**2)\n                \n                # Add contribution to the sum\n                g2_value += radial_term * fc\n        \n        return g2_value\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 0.5, 'R_s': 0.0},\n        # Case B\n        {'positions': np.array([[0.0, 0.0, 0.0]]),\n         'i': 0, 'R_c': 3.0, 'eta': 1.0, 'R_s': 0.0},\n        # Case C\n        {'positions': np.array([[0.0, 0.0, 0.0], [5.0, 0.0, 0.0], [-5.0, 0.0, 0.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 1.0, 'R_s': 0.0},\n        # Case D\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 2.0, 'R_s': 2.5},\n        # Case E\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 1, 'R_c': 5.0, 'eta': 0.5, 'R_s': 0.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_g2(\n            positions=case['positions'],\n            i=case['i'],\n            R_c=case['R_c'],\n            eta=case['eta'],\n            R_s=case['R_s']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format string \"{:.6f}\" handles rounding to 6 decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2457438"}, {"introduction": "After implementing a symmetry-aware descriptor, it is vital to understand precisely why such constructions are indispensable. This thought experiment [@problem_id:2457453] challenges you to confront the consequences of ignoring these physical principles, specifically the invariance to the permutation of identical atoms. By analyzing a scenario with a naive model applied to a symmetric molecule, you will gain a deeper appreciation for the foundational architectural choices that prevent unphysical behavior and make MLPs reliable tools for chemical simulation.", "problem": "Consider the planar benzene molecule $C_6H_6$ in its ideal $D_{6h}$ geometry, with all carbon–carbon and carbon–hydrogen bond lengths equal and all atoms lying in the plane $z=0$. Let the nuclear positions be $\\{\\mathbf{R}_i\\}_{i=1}^{12}$ with atomic numbers $\\{Z_i\\}_{i=1}^{12}$, where $Z_i \\in \\{6,1\\}$ denotes carbon or hydrogen, respectively. Under the Born–Oppenheimer (BO) approximation, the exact potential energy $E(\\{\\mathbf{R}_i,Z_i\\})$ of this system is a scalar that is invariant under any permutation of indices among nuclei that share the same atomic number.\n\nSuppose we train a machine learning potential (MLP) for the energy, denoted $E_\\theta(\\mathbf{X})$, where $\\mathbf{X} \\in \\mathbb{R}^{36}$ is formed by concatenating the Cartesian coordinates of the $12$ atoms in a fixed input order as $(x_1,y_1,z_1,\\dots,x_{12},y_{12},z_{12})$. The model $E_\\theta$ is a generic feedforward neural network acting directly on $\\mathbf{X}$ and is trained by minimizing the mean squared error to reference energies $E^\\ast$ on a dataset of $N$ benzene geometries that all use the same conventional atom indexing.\n\nAfter training, the model is evaluated on two inputs that represent the same physical geometry of benzene: $\\mathbf{X}^{(1)}$ uses the conventional indexing, while $\\mathbf{X}^{(2)}$ is obtained from $\\mathbf{X}^{(1)}$ by applying a cyclic permutation $\\pi$ of the six carbon indices and the corresponding six hydrogen indices, that is, $\\pi$ maps $(\\mathrm{C}_1,\\mathrm{C}_2,\\dots,\\mathrm{C}_6)$ to $(\\mathrm{C}_2,\\mathrm{C}_3,\\dots,\\mathrm{C}_1)$ and similarly $(\\mathrm{H}_1,\\mathrm{H}_2,\\dots,\\mathrm{H}_6)$ to $(\\mathrm{H}_2,\\mathrm{H}_3,\\dots,\\mathrm{H}_1)$, without changing the Cartesian coordinates themselves. Denote by $P_\\pi$ the linear operator that permutes the components of $\\mathbf{X}$ accordingly so that $\\mathbf{X}^{(2)}=P_\\pi \\mathbf{X}^{(1)}$.\n\nWhich of the following statements about this scenario are correct?\n\nA. Even if $\\mathbf{X}^{(1)}$ and $\\mathbf{X}^{(2)}$ correspond to the same physical geometry, a model $E_\\theta$ that operates on a fixed-order concatenation of coordinates and is trained only on one indexing can in general yield $E_\\theta(\\mathbf{X}^{(1)}) \\ne E_\\theta(\\mathbf{X}^{(2)})$, thereby violating permutational invariance among identical atoms.\n\nB. Defining a symmetrized predictor $E^{\\mathrm{sym}}_\\theta(\\mathbf{X}) = \\frac{1}{|\\mathcal{G}|}\\sum_{\\pi \\in \\mathcal{G}} E_\\theta(P_\\pi \\mathbf{X})$, where $\\mathcal{G}$ is the permutation group over the six carbons and the six hydrogens (permuted within types), enforces exact permutational invariance of $E^{\\mathrm{sym}}_\\theta$ for any input $\\mathbf{X}$.\n\nC. Rotating the benzene coordinates in the plane by $60^\\circ$ while keeping the original atom indexing produces a configuration that is equivalent to a pure permutation of indices, so any rotation-invariant model is automatically permutation-invariant among identical atoms.\n\nD. Using an architecture or descriptor that aggregates over atoms or neighbors by a commutative operation such as summation (for example, atom-centered symmetry functions combined with a sum over atomic contributions, or a message-passing graph neural network with sum pooling) yields predictions that are invariant to permutations of identical atoms by construction.\n\nE. Although it is possible that $E_\\theta(\\mathbf{X}^{(1)}) \\ne E_\\theta(\\mathbf{X}^{(2)})$, the corresponding forces $\\mathbf{F}_i(\\mathbf{X}) = -\\nabla_{\\mathbf{R}_i} E_\\theta(\\mathbf{X})$ must still respect permutation symmetry across the six equivalent carbons at the $D_{6h}$ geometry, so molecular dynamics driven by $E_\\theta$ remains physically correct in this symmetric case.", "solution": "The problem statement is scientifically valid and well-posed. It addresses the fundamental requirement of permutation invariance for potential energy surfaces in physics and chemistry, and correctly frames a scenario to test the properties of a machine learning potential that lacks this symmetry by construction. We will now proceed to evaluate each statement.\n\nThe core of the problem lies in the fact that the exact potential energy, $E(\\{\\mathbf{R}_i,Z_i\\})$, is a function of a *set* of particle coordinates, where the ordering of identical particles does not matter. However, the proposed machine learning potential, $E_\\theta(\\mathbf{X})$, is a function of an *ordered vector* $\\mathbf{X}$, where the position of each element in the vector is distinct. A generic feedforward neural network, by its nature, applies different weights to different input neurons, meaning it is not designed to be invariant to the permutation of its inputs.\n\nA. **Even if $\\mathbf{X}^{(1)}$ and $\\mathbf{X}^{(2)}$ correspond to the same physical geometry, a model $E_\\theta$ that operates on a fixed-order concatenation of coordinates and is trained only on one indexing can in general yield $E_\\theta(\\mathbf{X}^{(1)}) \\ne E_\\theta(\\mathbf{X}^{(2)})$, thereby violating permutational invariance among identical atoms.**\n\nThis statement addresses the central flaw of the naive MLP architecture described. The model $E_\\theta$ is a generic function of a $36$-dimensional vector $\\mathbf{X}$. Let a simple model be $E_\\theta(\\mathbf{X}) = \\sigma(\\mathbf{W}\\mathbf{X} + \\mathbf{b})$, where $\\mathbf{W}$ is a weight matrix. The inputs $\\mathbf{X}^{(1)}$ and $\\mathbf{X}^{(2)}$ are related by a permutation, $\\mathbf{X}^{(2)} = P_\\pi \\mathbf{X}^{(1)}$, where $P_\\pi$ is a matrix that reorders the elements of the vector. For a generic, non-linear function $E_\\theta$, there is no mathematical reason to expect $E_\\theta(\\mathbf{X}^{(1)})$ to be equal to $E_\\theta(P_\\pi \\mathbf{X}^{(1)})$. The network learns specific correlations between the fixed input slots and the target energy. Permuting the inputs effectively presents the network with a completely new data point, for which its prediction is not constrained to be the same. The training protocol, using only one conventional indexing, does not provide the network with any information that would allow it to learn this symmetry. Therefore, in general, $E_\\theta(\\mathbf{X}^{(1)}) \\ne E_\\theta(\\mathbf{X}^{(2)})$. This is a direct violation of the physical principle of permutation invariance.\n\n**Verdict: Correct.**\n\nB. **Defining a symmetrized predictor $E^{\\mathrm{sym}}_\\theta(\\mathbf{X}) = \\frac{1}{|\\mathcal{G}|}\\sum_{\\pi \\in \\mathcal{G}} E_\\theta(P_\\pi \\mathbf{X})$, where $\\mathcal{G}$ is the permutation group over the six carbons and the six hydrogens (permuted within types), enforces exact permutational invariance of $E^{\\mathrm{sym}}_\\theta$ for any input $\\mathbf{X}$.**\n\nThis statement proposes a method to enforce the required symmetry by averaging over the group of all relevant permutations, $\\mathcal{G}$. Let us test the invariance of $E^{\\mathrm{sym}}_\\theta$ under the action of an arbitrary permutation $\\sigma \\in \\mathcal{G}$. We apply the operator $P_\\sigma$ to the input $\\mathbf{X}$ and evaluate the function:\n$$E^{\\mathrm{sym}}_\\theta(P_\\sigma \\mathbf{X}) = \\frac{1}{|\\mathcal{G}|}\\sum_{\\pi \\in \\mathcal{G}} E_\\theta(P_\\pi (P_\\sigma \\mathbf{X}))$$\nThe composition of two permutation operators $P_\\pi$ and $P_\\sigma$ corresponds to the permutation operator of the group product, $P_{\\pi\\sigma}$. Thus, we have:\n$$E^{\\mathrm{sym}}_\\theta(P_\\sigma \\mathbf{X}) = \\frac{1}{|\\mathcal{G}|}\\sum_{\\pi \\in \\mathcal{G}} E_\\theta(P_{\\pi\\sigma} \\mathbf{X})$$\nAccording to the rearrangement theorem of group theory, if $\\sigma$ is a fixed element of $\\mathcal{G}$ and $\\pi$ runs over all elements of $\\mathcal{G}$, then the product $\\pi' = \\pi\\sigma$ also runs over all elements of $\\mathcal{G}$ exactly once. We can therefore change the summation variable from $\\pi$ to $\\pi'$:\n$$E^{\\mathrm{sym}}_\\theta(P_\\sigma \\mathbf{X}) = \\frac{1}{|\\mathcal{G}|}\\sum_{\\pi' \\in \\mathcal{G}} E_\\theta(P_{\\pi'} \\mathbf{X}) = E^{\\mathrm{sym}}_\\theta(\\mathbf{X})$$\nThis shows that $E^{\\mathrm{sym}}_\\theta(\\mathbf{X})$ is indeed invariant under the action of any permutation $\\sigma \\in \\mathcal{G}$. This technique is a standard method for projecting a function onto the totally symmetric subspace of a group's representation.\n\n**Verdict: Correct.**\n\nC. **Rotating the benzene coordinates in the plane by $60^\\circ$ while keeping the original atom indexing produces a configuration that is equivalent to a pure permutation of indices, so any rotation-invariant model is automatically permutation-invariant among identical atoms.**\n\nThis statement contains a logical fallacy. The first part is correct for the specific case mentioned: rotating an ideal $D_{6h}$ benzene molecule by $60^\\circ$ (a $C_6$ rotation) maps the molecule onto itself, such that the new coordinates of atom $i$ are identical to the old coordinates of atom $j$, where $j$ is the atom that was originally at the new position of $i$. This mapping is equivalent to the permutation $\\pi$ described in the problem. However, the conclusion drawn from this specific observation is incorrect. Rotational invariance and permutational invariance are fundamentally different symmetries. Rotational invariance implies that for any rotation operator $R$, $E(\\{\\mathbf{R}_i\\}) = E(\\{R\\mathbf{R}_i\\})$. Permutational invariance implies that for any permutation $\\pi$ of identical atoms, $E(\\{\\mathbf{R}_i\\}) = E(\\{\\mathbf{R}_{\\pi(i)}\\})$. A general permutation (e.g., swapping two non-adjacent carbon atoms) cannot be represented by a simple rotation of the entire molecule. Conversely, a general rotation of a non-symmetric configuration of atoms does not correspond to any permutation. A model can be invariant to one type of symmetry without being invariant to the other. For example, a potential that depends only on the distances between atoms, $E = \\sum_{ij} f(Z_i, Z_j, ||\\mathbf{R}_i - \\mathbf{R}_j||)$, is rotation- and translation-invariant but is only permutation-invariant if the function $f$ is symmetric with respect to its atomic type arguments, which is not guaranteed. More pointedly, the argument tries to prove a general property from a single, highly symmetric special case, which is invalid reasoning.\n\n**Verdict: Incorrect.**\n\nD. **Using an architecture or descriptor that aggregates over atoms or neighbors by a commutative operation such as summation (for example, atom-centered symmetry functions combined with a sum over atomic contributions, or a message-passing graph neural network with sum pooling) yields predictions that are invariant to permutations of identical atoms by construction.**\n\nThis statement accurately describes the foundational design principle of modern, successful machine learning potentials. These models avoid the fixed-ordering problem by representing the total energy as a sum over atomic contributions:\n$$E = \\sum_{i=1}^{N_{\\text{atoms}}} E_i$$\nHere, the atomic energy $E_i$ is computed by a function (e.g., a neural network) that depends on the atomic number $Z_i$ of atom $i$ and a description of its local environment. Crucially, the function used to compute $E_i$ is the same for all atoms of the same type. For example, all carbon atoms use one network, $f_C$, and all hydrogen atoms use another, $f_H$. The description of the local environment is itself constructed to be invariant to the permutation of neighboring atoms, often through another summation or commutative operation. Because the outer summation $\\sum_i$ is a commutative operation, swapping the indices of any two identical atoms (e.g., carbon $j$ and carbon $k$) merely changes the order of two identical terms ($E_j$ and $E_k$ are both computed using $f_C$) in the sum, leaving the total value of $E$ unchanged. Architectures like Behler-Parrinello potentials and many graph neural networks are built on this principle.\n\n**Verdict: Correct.**\n\nE. **Although it is possible that $E_\\theta(\\mathbf{X}^{(1)}) \\ne E_\\theta(\\mathbf{X}^{(2)})$, the corresponding forces $\\mathbf{F}_i(\\mathbf{X}) = -\\nabla_{\\mathbf{R}_i} E_\\theta(\\mathbf{X})$ must still respect permutation symmetry across the six equivalent carbons at the $D_{6h}$ geometry, so molecular dynamics driven by $E_\\theta$ remains physically correct in this symmetric case.**\n\nThis statement is critically incorrect. Forces are the negative gradient of the potential energy: $\\mathbf{F}(\\mathbf{X}) = -\\nabla E_\\theta(\\mathbf{X})$. If the energy $E_\\theta$ is not invariant under a symmetry operation (like permutation), its gradient (the forces) will not be equivariant. Equivariance of forces means that if the atomic coordinates are permuted, $\\mathbf{X}' = P_\\pi \\mathbf{X}$, then the new force vector is the permutation of the old force vector, $\\mathbf{F}(\\mathbf{X}') = P_\\pi \\mathbf{F}(\\mathbf{X})$. A non-invariant potential does not satisfy this. For the $D_{6h}$ geometry, the true physical forces are zero on all atoms. A well-trained model might predict near-zero forces for the input $\\mathbf{X}^{(1)}$ that it saw during training. However, for the permuted input $\\mathbf{X}^{(2)}$, which represents the same physical state, the non-invariant model $E_\\theta$ will produce a different energy $E_\\theta(\\mathbf{X}^{(2)}) \\ne E_\\theta(\\mathbf{X}^{(1)})$. This means that $\\mathbf{X}^{(2)}$ is not a stationary point of the potential $E_\\theta$. Consequently, the forces $\\mathbf{F}(\\mathbf{X}^{(2)}) = -\\nabla E_\\theta(\\mathbf{X}^{(2)})$ will be non-zero. This is an unphysical result: the model predicts that the equilibrium geometry should experience forces that would tear it apart, simply because the atoms were relabeled. Molecular dynamics driven by such a potential would be unphysical and unstable.\n\n**Verdict: Incorrect.**", "answer": "$$\\boxed{ABD}$$", "id": "2457453"}, {"introduction": "An MLP built with proper invariances is a robust tool, but its predictive power is fundamentally tied to the data it has seen. This practical exercise [@problem_id:2457471] demonstrates the critical concept of *domain mismatch*, a universal challenge in machine learning. By training a potential on synthetic data representing a \"bulk\" liquid environment and then evaluating it on an isolated dimer in a \"vacuum\", you will uncover how and why an MLP's accuracy can degrade when it is forced to extrapolate beyond its training distribution, a crucial lesson in model validation and application.", "problem": "You will implement a complete, runnable program that constructs a simple one-dimensional machine learning potential trained on synthetic \"periodic bulk water\" data and then evaluates its ability to predict the vacuum potential energy curve of a water dimer as a function of the oxygen–oxygen separation. The goal is to demonstrate, through explicit numerical evaluation, how a model trained on periodic bulk data can fail when applied to the out-of-domain vacuum dimer potential energy curve.\n\nFoundational starting point: use a physically motivated pair-interaction representation for the vacuum dimer potential energy that captures the competition between short-range Pauli repulsion and long-range dispersion attraction. Use a kernel ridge regression model to represent a one-dimensional machine learning potential, with a Gaussian kernel. The program must be fully self-contained.\n\nDefinitions and specifications:\n\n1) Vacuum water dimer potential energy model. For an oxygen–oxygen separation $R$ in Ångström, define the vacuum reference energy $E_{\\mathrm{vac}}(R)$ by\n$$\nE_{\\mathrm{vac}}(R) \\equiv A_{\\mathrm{rep}} \\, e^{-b_{\\mathrm{rep}} R} \\;-\\; \\frac{C_6}{R^6},\n$$\nwith parameters\n- $A_{\\mathrm{rep}} = 295400$ in $\\mathrm{kJ/mol}$,\n- $b_{\\mathrm{rep}} = 3.2$ in $\\mathrm{\\AA^{-1}}$,\n- $C_6 = 25000$ in $\\mathrm{kJ \\, \\AA^6/mol}$.\nAll energies must be expressed in $\\mathrm{kJ/mol}$ and all distances $R$ in $\\mathrm{\\AA}$.\n\n2) Emulating periodic bulk training data. To emulate that the training data originates from periodic bulk water (and thus includes many-body environmental stabilization absent in vacuum), define an environmental stabilization term\n$$\n\\Delta_{\\mathrm{env}}(R) \\equiv -A_{\\mathrm{env}} \\exp\\!\\left(-\\frac{(R - R_{\\mathrm{env}})^2}{2 s_{\\mathrm{env}}^2}\\right),\n$$\nwith\n- $A_{\\mathrm{env}} = 8.0$ in $\\mathrm{kJ/mol}$,\n- $R_{\\mathrm{env}} = 2.75$ in $\\mathrm{\\AA}$,\n- $s_{\\mathrm{env}} = 0.15$ in $\\mathrm{\\AA}$.\nThe \"bulk-labeled\" training energies are\n$$\nE_{\\mathrm{bulk}}(R) \\equiv E_{\\mathrm{vac}}(R) + \\Delta_{\\mathrm{env}}(R).\n$$\n\n3) Training set construction. Construct a training set $\\{(R_i, y_i)\\}_{i=1}^{N}$ by sampling $N$ points uniformly in the interval $[R_{\\min}, R_{\\max}]$ with\n- $R_{\\min} = 2.5$ in $\\mathrm{\\AA}$,\n- $R_{\\max} = 3.0$ in $\\mathrm{\\AA}$,\n- $N = 40$,\nand set $y_i \\equiv E_{\\mathrm{bulk}}(R_i)$.\n\n4) Machine Learning Potential (Kernel Ridge Regression). Use kernel ridge regression with a Gaussian kernel to obtain a model $\\widehat{E}(R)$ trained on the above data. Specifically, define the kernel\n$$\nK(R, R') \\equiv \\exp\\!\\left(-\\frac{(R - R')^2}{2 \\ell^2}\\right),\n$$\nwith length scale\n- $\\ell = 0.25$ in $\\mathrm{\\AA}$,\nand ridge regularization parameter\n- $\\lambda = 10^{-6}$ (dimensionless).\nLet $\\mathbf{K} \\in \\mathbb{R}^{N \\times N}$ be the Gram matrix with entries $K_{ij} \\equiv K(R_i, R_j)$. The kernel ridge regression solution for the coefficient vector $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{N}$ is\n$$\n\\boldsymbol{\\alpha} = \\left(\\mathbf{K} + \\lambda \\mathbf{I}\\right)^{-1} \\mathbf{y},\n$$\nwhere $\\mathbf{y} = (y_1, \\dots, y_N)^\\top$. The model prediction at a new separation $R$ is\n$$\n\\widehat{E}(R) = \\sum_{j=1}^{N} \\alpha_j \\, K(R, R_j).\n$$\n\n5) Evaluation against vacuum reference. Evaluate the model $\\widehat{E}(R)$ against the vacuum reference $E_{\\mathrm{vac}}(R)$ for a specified set of test separations and also by comparing minima locations on a grid. All energies must be reported in $\\mathrm{kJ/mol}$ and distances in $\\mathrm{\\AA}$.\n\nTest Suite:\n\n- Pointwise tests at the oxygen–oxygen separations\n$$\nR_{\\mathrm{test}} \\in \\{ 2.4, \\; 2.85, \\; 3.4, \\; 5.0 \\} \\; \\mathrm{\\AA}.\n$$\nFor each $R$ in the set, compute the absolute energy error\n$$\n\\varepsilon(R) \\equiv \\left| \\widehat{E}(R) - E_{\\mathrm{vac}}(R) \\right|\n$$\nin $\\mathrm{kJ/mol}$.\n\n- Minima-location test on a grid. On a uniform grid\n$$\nR \\in [2.2, \\; 5.0] \\; \\mathrm{\\AA}\n$$\nwith grid spacing\n$$\n\\Delta R = 0.001 \\; \\mathrm{\\AA},\n$$\ncompute\n$$\nR_{\\min}^{\\mathrm{vac}} \\equiv \\operatorname*{arg\\,min}_{R} E_{\\mathrm{vac}}(R), \\quad\nR_{\\min}^{\\mathrm{ml}} \\equiv \\operatorname*{arg\\,min}_{R} \\widehat{E}(R),\n$$\nand report the signed deviation\n$$\n\\Delta R_{\\min} \\equiv R_{\\min}^{\\mathrm{ml}} - R_{\\min}^{\\mathrm{vac}}\n$$\nin $\\mathrm{\\AA}$.\n\nRequired final output format:\n\n- Your program must produce a single line of output containing a list with $5$ entries:\n  1) $\\varepsilon(2.4)$,\n  2) $\\varepsilon(2.85)$,\n  3) $\\varepsilon(3.4)$,\n  4) $\\varepsilon(5.0)$,\n  5) $\\Delta R_{\\min}$.\n- The first four entries must be floats in $\\mathrm{kJ/mol}$ rounded to exactly $3$ decimal places. The last entry must be a float in $\\mathrm{\\AA}$ rounded to exactly $3$ decimal places.\n- The output must be a comma-separated list enclosed in square brackets, for example\n$$\n[\\varepsilon(2.4),\\varepsilon(2.85),\\varepsilon(3.4),\\varepsilon(5.0),\\Delta R_{\\min}]\n$$\nwith all five numbers shown as decimals having exactly three digits after the decimal point.\n\nConstraints and notes:\n\n- The program must implement the definitions above exactly, without any external data.\n- All energies must be expressed in $\\mathrm{kJ/mol}$, all distances in $\\mathrm{\\AA}$.\n- No random numbers are needed.\n- Angles are not used in this reduced one-dimensional model; all angle quantities are irrelevant.\n- Your code must be standalone and must not read any input.", "solution": "The problem is subjected to validation.\n\n**Step 1: Extract Givens**\n\nThe problem provides the following definitions, parameters, and conditions:\n\n1.  **Vacuum Water Dimer Potential Energy Model**:\n    The potential energy is $E_{\\mathrm{vac}}(R) \\equiv A_{\\mathrm{rep}} \\, e^{-b_{\\mathrm{rep}} R} - \\frac{C_6}{R^6}$, where $R$ is the oxygen–oxygen separation.\n    Parameters:\n    $A_{\\mathrm{rep}} = 295400 \\, \\mathrm{kJ/mol}$\n    $b_{\\mathrm{rep}} = 3.2 \\, \\mathrm{\\AA^{-1}}$\n    $C_6 = 25000 \\, \\mathrm{kJ \\, \\AA^6/mol}$\n\n2.  **Environmental Stabilization Term**:\n    The term is defined as $\\Delta_{\\mathrm{env}}(R) \\equiv -A_{\\mathrm{env}} \\exp\\!\\left(-\\frac{(R - R_{\\mathrm{env}})^2}{2 s_{\\mathrm{env}}^2}\\right)$.\n    Parameters:\n    $A_{\\mathrm{env}} = 8.0 \\, \\mathrm{kJ/mol}$\n    $R_{\\mathrm{env}} = 2.75 \\, \\mathrm{\\AA}$\n    $s_{\\mathrm{env}} = 0.15 \\, \\mathrm{\\AA}$\n\n3.  **Bulk-Labeled Training Energy**:\n    The energy is $E_{\\mathrm{bulk}}(R) \\equiv E_{\\mathrm{vac}}(R) + \\Delta_{\\mathrm{env}}(R)$.\n\n4.  **Training Set**:\n    A set of $N=40$ points $\\{(R_i, y_i)\\}_{i=1}^{N}$.\n    Sampling interval: $R_i \\in [R_{\\min}, R_{\\max}]$, with $R_{\\min} = 2.5 \\, \\mathrm{\\AA}$ and $R_{\\max} = 3.0 \\, \\mathrm{\\AA}$.\n    Sampling method: uniform sampling.\n    Training targets: $y_i \\equiv E_{\\mathrm{bulk}}(R_i)$.\n\n5.  **Machine Learning Potential (Kernel Ridge Regression)**:\n    Kernel function: $K(R, R') \\equiv \\exp\\!\\left(-\\frac{(R - R')^2}{2 \\ell^2}\\right)$.\n    Kernel length scale: $\\ell = 0.25 \\, \\mathrm{\\AA}$.\n    Regularization parameter: $\\lambda = 10^{-6}$.\n    Coefficient vector solution: $\\boldsymbol{\\alpha} = (\\mathbf{K} + \\lambda \\mathbf{I})^{-1} \\mathbf{y}$.\n    Prediction function: $\\widehat{E}(R) = \\sum_{j=1}^{N} \\alpha_j \\, K(R, R_j)$.\n\n6.  **Evaluation Tasks**:\n    Pointwise tests: for $R_{\\mathrm{test}} \\in \\{ 2.4, 2.85, 3.4, 5.0 \\} \\, \\mathrm{\\AA}$, compute the absolute error $\\varepsilon(R) \\equiv | \\widehat{E}(R) - E_{\\mathrm{vac}}(R) |$.\n    Minima-location test: on a grid $R \\in [2.2, 5.0] \\, \\mathrm{\\AA}$ with spacing $\\Delta R = 0.001 \\, \\mathrm{\\AA}$, find $R_{\\min}^{\\mathrm{vac}} \\equiv \\operatorname*{arg\\,min}_{R} E_{\\mathrm{vac}}(R)$ and $R_{\\min}^{\\mathrm{ml}} \\equiv \\operatorname*{arg\\,min}_{R} \\widehat{E}(R)$, and compute the deviation $\\Delta R_{\\min} \\equiv R_{\\min}^{\\mathrm{ml}} - R_{\\min}^{\\mathrm{vac}}$.\n\n7.  **Output Format**:\n    A single-line list `[val1, val2, val3, val4, val5]` with results rounded to $3$ decimal places.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the validation criteria.\n\n-   **Scientifically Grounded**: The problem is sound. The form of $E_{\\mathrm{vac}}(R)$ is a simplified but standard representation of intermolecular interactions (Pauli repulsion and dispersion). The concept of an environmental stabilization term $\\Delta_{\\mathrm{env}}(R)$ to distinguish bulk from vacuum environments is physically motivated. Kernel ridge regression is a standard non-parametric regression technique. The overall setup correctly formalizes a common challenge in machine learning for physical sciences: the problem of *distributional shift* or *domain mismatch*, where a model is trained on data from one physical regime (periodic bulk) and evaluated on another (vacuum dimer).\n\n-   **Well-Posed**: The problem is well-posed. The kernel ridge regression model has a unique solution. The Gram matrix $\\mathbf{K}$ is positive semi-definite. The regularization term $\\lambda \\mathbf{I}$ is positive definite for $\\lambda = 10^{-6}  0$. Their sum, $\\mathbf{K} + \\lambda \\mathbf{I}$, is therefore positive definite and invertible, guaranteeing a unique solution for the coefficient vector $\\boldsymbol{\\alpha}$. All subsequent calculations are deterministic.\n\n-   **Objective**: The problem is stated with complete objectivity. All terms are defined by precise mathematical formulas, and all parameters are given as specific numerical values. There is no ambiguity or subjective language.\n\nThe problem statement does not exhibit any of the listed flaws (e.g., scientific unsoundness, incompleteness, contradiction, infeasibility, or poor structure).\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be provided.\n\n**Solution Procedure**\n\nThe objective is to construct a one-dimensional machine learning potential $\\widehat{E}(R)$ trained on synthetic \"bulk\" data and evaluate its predictive accuracy for the \"vacuum\" potential energy curve $E_{\\mathrm{vac}}(R)$. The core of the problem lies in the discrepancy between the training data distribution, which includes an environmental stabilization term $\\Delta_{\\mathrm{env}}(R)$, and the target test distribution, which does not.\n\n1.  **Define Potential Energy Functions**: First, we implement the analytical forms of the potentials. The vacuum potential $E_{\\mathrm{vac}}(R)$ is defined as\n    $$\n    E_{\\mathrm{vac}}(R) = A_{\\mathrm{rep}} \\, e^{-b_{\\mathrm{rep}} R} - \\frac{C_6}{R^6}\n    $$\n    with the given parameters $A_{\\mathrm{rep}} = 295400 \\, \\mathrm{kJ/mol}$, $b_{\\mathrm{rep}} = 3.2 \\, \\mathrm{\\AA^{-1}}$, and $C_6 = 25000 \\, \\mathrm{kJ \\, \\AA^6/mol}$.\n    The environmental stabilization term, representing the effect of a condensed-phase environment, is\n    $$\n    \\Delta_{\\mathrm{env}}(R) = -A_{\\mathrm{env}} \\exp\\!\\left(-\\frac{(R - R_{\\mathrm{env}})^2}{2 s_{\\mathrm{env}}^2}\\right)\n    $$\n    with parameters $A_{\\mathrm{env}} = 8.0 \\, \\mathrm{kJ/mol}$, $R_{\\mathrm{env}} = 2.75 \\, \\mathrm{\\AA}$, and $s_{\\mathrm{env}} = 0.15 \\, \\mathrm{\\AA}$.\n    The training data is generated using the \"bulk\" potential, which is the sum of these two components:\n    $$\n    E_{\\mathrm{bulk}}(R) = E_{\\mathrm{vac}}(R) + \\Delta_{\\mathrm{env}}(R)\n    $$\n    This term $\\Delta_{\\mathrm{env}}(R)$ introduces a stabilization (a negative energy contribution) centered at $R = 2.75 \\, \\mathrm{\\AA}$, which is the physically correct location for the first peak of the oxygen-oxygen radial distribution function in liquid water. The machine learning model will learn this feature from the training data.\n\n2.  **Construct Training Set**: A training set $\\{ (R_i, y_i) \\}_{i=1}^{N}$ is generated. The input points $R_i$ are $N=40$ uniformly spaced separations in the interval $[R_{\\min}, R_{\\max}] = [2.5, 3.0] \\, \\mathrm{\\AA}$. The target values are the corresponding \"bulk\" energies, $y_i = E_{\\mathrm{bulk}}(R_i)$. This training region, $[2.5, 3.0] \\, \\mathrm{\\AA}$, is narrow and centered around the environmental stabilization minimum at $R=2.75 \\, \\mathrm{\\AA}$.\n\n3.  **Train the Kernel Ridge Regression Model**: We employ kernel ridge regression (KRR) with a Gaussian kernel to model the potential energy surface. The Gaussian kernel is given by\n    $$\n    K(R, R') = \\exp\\!\\left(-\\frac{(R - R')^2}{2 \\ell^2}\\right)\n    $$\n    with length scale $\\ell = 0.25 \\, \\mathrm{\\AA}$. The KRR model predicts the energy at a new point $R$ as a linear combination of kernel functions centered at the training points $R_j$:\n    $$\n    \\widehat{E}(R) = \\sum_{j=1}^{N} \\alpha_j K(R, R_j)\n    $$\n    The coefficients $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_N)^\\top$ are found by solving the regularized linear system:\n    $$\n    (\\mathbf{K} + \\lambda \\mathbf{I}) \\boldsymbol{\\alpha} = \\mathbf{y}\n    $$\n    Here, $\\mathbf{K}$ is the $N \\times N$ Gram matrix with entries $K_{ij} = K(R_i, R_j)$, $\\mathbf{y} = (y_1, \\dots, y_N)^\\top$ is the vector of training targets, $\\mathbf{I}$ is the identity matrix, and $\\lambda = 10^{-6}$ is the Tikhonov regularization parameter. This linear system is solved numerically.\n\n4.  **Evaluate the Model**: The trained model $\\widehat{E}(R)$ is now evaluated against the true vacuum potential $E_{\\mathrm{vac}}(R)$, not the bulk potential it was trained on. This tests its ability to generalize outside its training distribution.\n\n    a.  **Pointwise Error**: The absolute error $\\varepsilon(R) = | \\widehat{E}(R) - E_{\\mathrm{vac}}(R) |$ is computed at four specified test points: $R_{\\mathrm{test}} \\in \\{ 2.4, 2.85, 3.4, 5.0 \\} \\, \\mathrm{\\AA}$. Points such as $R=2.4 \\, \\mathrm{\\AA}$ and $R=3.4 \\, \\mathrm{\\AA}$ are outside the training interval $[2.5, 3.0] \\, \\mathrm{\\AA}$, testing extrapolation. The point $R=2.85 \\, \\mathrm{\\AA}$ is inside the interval, testing interpolation. The point $R=5.0 \\, \\mathrm{\\AA}$ tests behavior at long range. We expect significant errors because the model $\\widehat{E}(R)$ has learned the stabilization feature from $\\Delta_{\\mathrm{env}}(R)$, which is absent in the target function $E_{\\mathrm{vac}}(R)$.\n\n    b.  **Minima Location Error**: To assess the model's ability to predict structural properties, we compare the location of the potential energy minimum. We define a fine grid of $R$ values from $2.2 \\, \\mathrm{\\AA}$ to $5.0 \\, \\mathrm{\\AA}$ with a step of $\\Delta R = 0.001 \\, \\mathrm{\\AA}$. We evaluate both $E_{\\mathrm{vac}}(R)$ and $\\widehat{E}(R)$ on this grid and find their respective minima, $R_{\\min}^{\\mathrm{vac}}$ and $R_{\\min}^{\\mathrm{ml}}$. The deviation $\\Delta R_{\\min} = R_{\\min}^{\\mathrm{ml}} - R_{\\min}^{\\mathrm{vac}}$ is calculated. Since the training data includes an artefactual stabilization centered at $R = 2.75 \\, \\mathrm{\\AA}$, the learned potential $\\widehat{E}(R)$ will have its minimum shifted towards this value compared to the true vacuum minimum of $E_{\\mathrm{vac}}(R)$, resulting in a non-zero $\\Delta R_{\\min}$.\n\nThe final implementation will carry out these steps numerically and format the results as requested.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and evaluates a 1D machine learning potential for a water dimer.\n    The model is trained on synthetic \"bulk\" data and tested against the \"vacuum\" potential,\n    demonstrating the effect of distributional shift.\n    \"\"\"\n\n    # --- 1. Definitions and Parameters ---\n\n    # Parameters for the vacuum potential E_vac(R)\n    A_rep = 295400.0  # kJ/mol\n    b_rep = 3.2      # A^-1\n    C6 = 25000.0     # kJ * A^6 / mol\n\n    # Parameters for the environmental stabilization term Delta_env(R)\n    A_env = 8.0      # kJ/mol\n    R_env = 2.75     # A\n    s_env = 0.15     # A\n\n    # Training set parameters\n    N = 40\n    R_min = 2.5      # A\n    R_max = 3.0      # A\n\n    # Kernel Ridge Regression parameters\n    length_scale = 0.25  # A\n    lambda_reg = 1e-6    # dimensionless\n\n    # --- 2. Potential Energy Functions ---\n\n    def E_vac(R):\n        \"\"\"Calculates the vacuum reference energy E_vac(R).\"\"\"\n        return A_rep * np.exp(-b_rep * R) - C6 / (R**6)\n\n    def Delta_env(R):\n        \"\"\"Calculates the environmental stabilization term Delta_env(R).\"\"\"\n        return -A_env * np.exp(-((R - R_env)**2) / (2 * s_env**2))\n\n    def E_bulk(R):\n        \"\"\"Calculates the 'bulk-labeled' training energy E_bulk(R).\"\"\"\n        return E_vac(R) + Delta_env(R)\n\n    # --- 3. Training Set Construction ---\n\n    R_train = np.linspace(R_min, R_max, N)\n    y_train = E_bulk(R_train)\n\n    # --- 4. Machine Learning Potential (Kernel Ridge Regression) ---\n\n    def gaussian_kernel(R1, R2, l):\n        \"\"\"\n        Computes the Gaussian kernel between two sets of points.\n        Handles broadcasting for vector-matrix operations.\n        \"\"\"\n        # Ensure R1 and R2 are numpy arrays for broadcasting\n        R1 = np.asarray(R1)\n        R2 = np.asarray(R2)\n        \n        # If R1 is a vector and R2 is a vector, we want a matrix of distances\n        if R1.ndim == 1 and R2.ndim == 1:\n            R1 = R1[:, np.newaxis] # Makes R1 a column vector\n        \n        dist_sq = (R1 - R2)**2\n        return np.exp(-dist_sq / (2 * l**2))\n\n    # Construct the Gram matrix K\n    K_matrix = gaussian_kernel(R_train, R_train, length_scale)\n\n    # Solve for the KRR coefficients alpha\n    # (K + lambda*I) * alpha = y  =>  alpha = solve(K + lambda*I, y)\n    A = K_matrix + lambda_reg * np.eye(N)\n    alpha = np.linalg.solve(A, y_train)\n\n    def E_hat(R):\n        \"\"\"\n        Predicts the energy using the trained KRR model.\n        R can be a single value or a numpy array.\n        \"\"\"\n        kernel_vec = gaussian_kernel(R, R_train, length_scale)\n        return kernel_vec @ alpha\n\n    # --- 5. Evaluation against Vacuum Reference ---\n\n    # a) Pointwise tests\n    R_test = np.array([2.4, 2.85, 3.4, 5.0])\n    E_hat_test = E_hat(R_test)\n    E_vac_test = E_vac(R_test)\n    errors = np.abs(E_hat_test - E_vac_test)\n    \n    # b) Minima-location test\n    R_grid_step = 0.001\n    R_grid = np.arange(2.2, 5.0 + R_grid_step, R_grid_step)\n\n    # Evaluate both potentials on the grid\n    E_vac_grid = E_vac(R_grid)\n    E_hat_grid = E_hat(R_grid)\n\n    # Find the arguments of the minima\n    idx_min_vac = np.argmin(E_vac_grid)\n    R_min_vac = R_grid[idx_min_vac]\n\n    idx_min_ml = np.argmin(E_hat_grid)\n    R_min_ml = R_grid[idx_min_ml]\n\n    # Calculate the signed deviation\n    delta_R_min = R_min_ml - R_min_vac\n\n    # --- 6. Final Output Formatting ---\n    results = [\n        errors[0],      # epsilon(2.4)\n        errors[1],      # epsilon(2.85)\n        errors[2],      # epsilon(3.4)\n        errors[3],      # epsilon(5.0)\n        delta_R_min     # Delta_R_min\n    ]\n\n    # Format output according to the problem specification\n    formatted_results = [f\"{res:.3f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "2457471"}]}