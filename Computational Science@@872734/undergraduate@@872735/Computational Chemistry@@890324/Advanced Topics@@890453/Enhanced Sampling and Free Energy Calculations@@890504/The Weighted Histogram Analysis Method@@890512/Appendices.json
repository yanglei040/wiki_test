{"hands_on_practices": [{"introduction": "The success of a WHAM calculation depends critically on the quality of the input data from umbrella sampling simulations. Before committing significant computational resources, it is essential to design the experiment thoughtfully. This first practice challenges you to think like a computational scientist and determine the optimal strategy for placing sampling windows to efficiently map out a potential of mean force, balancing the need for good statistical overlap with the need to sample challenging high-energy regions. [@problem_id:2465769]", "problem": "A ligand unbinding potential of mean force along a one-dimensional reaction coordinate $\\xi$ is to be estimated with the Weighted Histogram Analysis Method (WHAM) from umbrella sampling data. The coordinate spans $\\xi \\in [0,L]$, where $\\xi \\approx 0$ is a tightly bound basin, $\\xi \\approx \\xi^{\\ddagger}$ is a transition state region with a free-energy barrier, and $\\xi > \\xi^{\\ddagger}$ approaches a flat, unbound plateau. You will place $N$ harmonic umbrellas with identical spring constant $k$ and centers $\\{\\xi_i\\}_{i=1}^N$, and you must decide how to space the centers given a fixed total simulation time $T$ that will be divided equally across windows.\n\nUse only the following foundational principles to reason your choice:\n\n- At equilibrium, the unbiased marginal distribution of $\\xi$ is $p(\\xi) \\propto \\exp\\!\\big(-\\beta F(\\xi)\\big)$, where $F(\\xi)$ is the potential of mean force and $\\beta = 1/(k_{\\mathrm{B}}T)$ with $k_{\\mathrm{B}}$ the Boltzmann constant and $T$ the absolute temperature.\n- In a window with harmonic bias $U_i(\\xi) = \\tfrac{1}{2} k (\\xi - \\xi_i)^2$, the sampled distribution along $\\xi$ is $p_i(\\xi) \\propto \\exp\\!\\big(-\\beta[F(\\xi) + U_i(\\xi)]\\big)$.\n- Near a given center $\\xi_i$, a quadratic expansion $F(\\xi) \\approx F(\\xi_i) + \\tfrac{1}{2}\\kappa_i(\\xi - \\xi_i)^2$ with curvature $\\kappa_i = \\tfrac{\\partial^2 F}{\\partial \\xi^2}\\big|_{\\xi_i}$ is appropriate if the window confines sampling locally.\n- The statistical efficiency of WHAM improves when adjacent windows have sufficient histogram overlap and when regions of low unbiased probability density $p(\\xi)$ are sampled by multiple windows so that many independent samples contribute after reweighting.\n\nWhich spacing strategy is most appropriate to minimize the variance of the WHAM estimate of $F(\\xi)$ across the entire path, under the stated constraints?\n\nA. Place the $N$ umbrella centers with uniform spacing over $[0,L]$ and divide time $T$ equally among them.\n\nB. Place the $N$ umbrella centers more densely near the bound basin ($\\xi \\approx 0$) and near the transition state region ($\\xi \\approx \\xi^{\\ddagger}$), with sparser spacing on the flat unbound plateau; keep $k$ the same and divide time $T$ equally.\n\nC. Place the $N$ umbrella centers more densely only in the unbound plateau ($\\xi > \\xi^{\\ddagger}$), where the free energy is flat, to maximize adjacent-window overlap; keep $k$ the same and divide time $T$ equally.\n\nD. Keep the umbrella centers uniformly spaced, but increase $k$ near the bound basin and the transition state to constrain sampling; keep equal time $T/N$ per window and identical center spacing everywhere.", "solution": "The user has requested a critical evaluation and solution of the provided problem statement. The validation and solution are presented below.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **System**: A ligand unbinding process described by a potential of mean force (PMF), $F(\\xi)$, along a one-dimensional reaction coordinate $\\xi$.\n-   **Reaction Coordinate Range**: $\\xi \\in [0,L]$.\n-   **PMF Features**:\n    -   $\\xi \\approx 0$: a tightly bound basin.\n    -   $\\xi \\approx \\xi^{\\ddagger}$: a transition state region with a free-energy barrier.\n    -   $\\xi > \\xi^{\\ddagger}$: a flat, unbound plateau.\n-   **Sampling Method**: Weighted Histogram Analysis Method (WHAM) applied to umbrella sampling data.\n-   **Sampling Parameters**:\n    -   Number of umbrellas: $N$.\n    -   Spring constant: $k$, identical for all umbrellas.\n    -   Umbrella centers: $\\{\\xi_i\\}_{i=1}^N$.\n    -   Total simulation time: $T$, divided equally across the $N$ windows.\n-   **Foundational Principles**:\n    1.  Unbiased marginal distribution: $p(\\xi) \\propto \\exp(-\\beta F(\\xi))$, with $\\beta = 1/(k_{\\mathrm{B}}T)$.\n    2.  Biased distribution in window $i$: $p_i(\\xi) \\propto \\exp(-\\beta[F(\\xi) + U_i(\\xi)])$, where the bias potential is $U_i(\\xi) = \\tfrac{1}{2} k (\\xi - \\xi_i)^2$.\n    3.  Local quadratic approximation of the PMF: $F(\\xi) \\approx F(\\xi_i) + \\tfrac{1}{2}\\kappa_i(\\xi - \\xi_i)^2$ with curvature $\\kappa_i = \\tfrac{\\partial^2 F}{\\partial \\xi^2}\\big|_{\\xi_i}$.\n    4.  Condition for WHAM efficiency: Sufficient histogram overlap between adjacent windows and enhanced sampling of low unbiased probability regions.\n-   **Objective**: Determine the spacing strategy for $\\{\\xi_i\\}_{i=1}^N$ that is most appropriate to minimize the variance of the WHAM estimate of $F(\\xi)$ across the entire path $[0,L]$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem statement is examined for validity:\n\n-   **Scientifically Grounded**: The problem is firmly rooted in the principles of statistical mechanics and computational chemistry. The concepts of potential of mean force, umbrella sampling, and the Weighted Histogram Analysis Method are standard and well-established in the field. All provided principles are correct statements of theory. The description of the ligand-unbinding PMF is canonical.\n-   **Well-Posed**: The problem is well-posed. It asks for the most appropriate strategy among a set of choices to achieve a clearly defined goal (minimizing PMF variance) under specified constraints (fixed $N$, $T$, and $k$). A unique qualitative answer can be derived from the provided principles.\n-   **Objective**: The problem is stated in objective, scientific language. It does not contain subjective claims or ambiguities.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, well-posed, objective, and complete. I will proceed with the derivation of the solution.\n\n### Solution Derivation\n\nThe objective is to minimize the statistical variance of the estimated potential of mean force, $F(\\xi)$, over its entire domain. The total simulation effort, characterized by the total time $T$ distributed among $N$ windows, is fixed. The spring constant $k$ is also fixed for the primary comparison of spacing strategies.\n\nAccording to Principle 4, the statistical efficiency of WHAM depends on two key factors: (i) sufficient overlap between the sampled distributions of adjacent windows, and (ii) adequate sampling of regions where the unbiased probability density, $p(\\xi)$, is low.\n\nLet us analyze the implications of these factors for different regions of the reaction coordinate $\\xi$.\n\n1.  **Analysis of Low-Probability Regions**:\n    From Principle 1, $p(\\xi) \\propto \\exp(-\\beta F(\\xi))$. This implies that regions of high free energy $F(\\xi)$ correspond to regions of low sampling probability in an unbiased simulation. The problem describes a high free-energy barrier at the transition state, $\\xi \\approx \\xi^{\\ddagger}$. Therefore, $p(\\xi)$ is at a minimum in this region. Principle 4 dictates that to minimize the variance of the PMF estimate, such low-probability regions must be sampled thoroughly. Since the time per window, $T/N$, is fixed, the only way to increase the total number of samples collected in the vicinity of $\\xi^{\\ddagger}$ is to place multiple windows in this region, i.e., to make the density of umbrella centers $\\{\\xi_i\\}$ high.\n\n2.  **Analysis of Histogram Overlap**:\n    The distribution sampled in window $i$, centered at $\\xi_i$, is given by Principle 2: $p_i(\\xi) \\propto \\exp(-\\beta[F(\\xi) + U_i(\\xi)])$. Using the local quadratic approximation from Principle 3, $F(\\xi) \\approx F(\\xi_i) + \\frac{1}{2}\\kappa_i(\\xi - \\xi_i)^2$, we can approximate the sampled distribution near its peak:\n    $$p_i(\\xi) \\propto \\exp\\left(-\\beta\\left[\\left(F(\\xi_i) + \\frac{1}{2}\\kappa_i(\\xi - \\xi_i)^2\\right) + \\frac{1}{2} k (\\xi - \\xi_i)^2\\right]\\right)$$\n    $$p_i(\\xi) \\propto \\exp\\left(-\\frac{\\beta}{2}(k + \\kappa_i)(\\xi - \\xi_i)^2\\right)$$\n    This shows that the local sampling distribution is approximately Gaussian with a variance $\\sigma_i^2 = (\\beta(k + \\kappa_i))^{-1}$. The width of the sampled histogram is therefore $\\sigma_i = 1 / \\sqrt{\\beta(k + \\kappa_i)}$. Good overlap between adjacent windows $i$ and $i+1$ (with centers $\\xi_i$ and $\\xi_{i+1}$) requires that their spacing, $|\\xi_{i+1} - \\xi_i|$, be on the order of the sampling width, $\\sigma_i$.\n\n    Let us apply this to the specific regions of the PMF:\n    -   **Bound Basin ($\\xi \\approx 0$)**: This is a deep well, meaning the PMF is steep on its sides. The curvature $\\kappa_i = F''(\\xi_i)$ is large and positive. Consequently, the effective spring constant $k + \\kappa_i$ is large, and the sampling width $\\sigma_i$ is small. To maintain sufficient overlap, the distance between adjacent umbrella centers must be small. Thus, a high density of windows is required in this region.\n    -   **Transition State Region ($\\xi \\approx \\xi^{\\ddagger}$)**: As established, this is a high-energy region requiring dense sampling. The curvature $\\kappa_i = F''(\\xi_i)$ is negative at the barrier crest. This makes the effective spring constant $k+\\kappa_i$ smaller than $k$, leading to a wider sampling distribution $\\sigma_i$. While this might suggest wider spacing is acceptable, the dominant requirement for this region is the need to overcome the extremely low unbiased probability $p(\\xi)$ by concentrating sampling effort. Thus, dense window placement is necessary.\n    -   **Unbound Plateau ($\\xi > \\xi^{\\ddagger}$)**: Here, the PMF is described as flat. This means the curvature $\\kappa_i \\approx 0$. The sampling width is approximately constant, $\\sigma_i \\approx 1/\\sqrt{\\beta k}$. Since the underlying PMF is flat, there are no inherent sampling difficulties due to high energy barriers or steep gradients. Therefore, this region requires the least sampling effort, and windows can be placed more sparsely.\n\n**Conclusion**: To minimize the overall variance of the estimated PMF, the density of umbrella windows should be proportional to the \"difficulty\" of sampling. This means placing windows more densely in regions of high free energy (the transition state) and high PMF curvature (the walls of the bound basin). Spacing can be much sparser on the flat, unbound plateau.\n\n### Option-by-Option Analysis\n\n-   **A. Place the $N$ umbrella centers with uniform spacing over $[0,L]$ and divide time $T$ equally among them.**\n    This strategy allocates sampling resources uniformly across the entire reaction coordinate. It fails to concentrate effort in the statistically challenging regions (the barrier and the steep basin) and wastes resources on the easy-to-sample flat plateau. This will result in high statistical uncertainty in the estimated PMF, particularly at the barrier.\n    **Verdict: Incorrect.**\n\n-   **B. Place the $N$ umbrella centers more densely near the bound basin ($\\xi \\approx 0$) and near the transition state region ($\\xi \\approx \\xi^{\\ddagger}$), with sparser spacing on the flat unbound plateau; keep $k$ the same and divide time $T$ equally.**\n    This strategy directly implements the conclusions derived from first principles. It increases the density of windows in the bound basin to ensure overlap where the PMF is steep, and it increases density at the transition state to adequately sample this high-energy region. It correctly allocates fewer resources to the trivial plateau region. This adaptive spacing is the standard and most appropriate approach to optimize WHAM calculations under the given constraints.\n    **Verdict: Correct.**\n\n-   **C. Place the $N$ umbrella centers more densely only in the unbound plateau ($\\xi > \\xi^{\\ddagger}$), where the free energy is flat, to maximize adjacent-window overlap; keep $k$ the same and divide time $T$ equally.**\n    This strategy is the antithesis of the optimal approach. It concentrates sampling on the easiest part of the PMF while neglecting the most difficult regions. Placing windows more densely on a flat potential does not \"maximize overlap\" in any meaningful way beyond what is necessary; it is simply wasteful. This would lead to extremely poor statistics and high variance in the critical basin and barrier regions.\n    **Verdict: Incorrect.**\n\n-   **D. Keep the umbrella centers uniformly spaced, but increase $k$ near the bound basin and the transition state to constrain sampling; keep equal time $T/N$ per window and identical center spacing everywhere.**\n    This option proposes a different approach by varying the spring constant $k$. Increasing $k$ in a region makes the sampling distribution narrower, as $\\sigma_i \\approx 1/\\sqrt{\\beta(k+\\kappa_i)}$. If the window centers remain uniformly spaced, increasing $k$ would *decrease* the overlap between adjacent windows, thereby degrading the quality of the WHAM calculation. To compensate for a larger $k$, one would need to place windows *closer together*, which contradicts the premise of uniform spacing. Therefore, this strategy as stated is flawed.\n    **Verdict: Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "2465769"}, {"introduction": "Real-world scientific work rarely proceeds without encountering unexpected issues, and computational studies are no exception. Data can be lost due to hardware failure, or simulations may terminate unexpectedly. This exercise presents a realistic scenario where crucial data from your umbrella sampling series is missing, creating a gap in your coverage of the reaction coordinate. [@problem_id:2465737] Your task is to evaluate the best course of action, a decision that hinges on a deep understanding of WHAM's core requirement for statistical connectivity between windows.", "problem": "You perform umbrella sampling along a one-dimensional reaction coordinate $x$ for a molecular system at temperature $T = 300\\,\\mathrm{K}$. There are $30$ windows centered at $x_i = (i-1)\\times 0.10\\,\\mathrm{nm}$ for $i \\in \\{1,2,\\dots,30\\}$, each with a harmonic umbrella bias $w_i(x) = \\tfrac{1}{2}k (x - x_i)^2$ and force constant $k = 1000\\,\\mathrm{kJ\\,mol^{-1}\\,nm^{-2}}$. The unbiased potential of mean force (PMF) along $x$ is to be estimated with the Weighted Histogram Analysis Method (WHAM). The time series data for windows $\\#8$ and $\\#21$ are lost and cannot be recovered or re-simulated under the current time constraints. You may assume that within each window the local variation of the underlying unbiased potential $U_0(x)$ near $x_i$ is modest compared to the umbrella curvature, so that the biased sampling of $x$ is dominated by the umbrella and can be approximated as locally Gaussian with variance $\\sigma^2 \\approx k_B T / k$, where $k_B$ is the Boltzmann constant.\n\nWhich strategy below will produce the best possible PMF under these constraints, and what defect(s) should you expect in the result?\n\nA. Fill the missing histograms by linear interpolation of neighboring histograms along $x$, then run WHAM on all $30$ windows to enforce continuity. The PMF will be continuous with uncertainties comparable to neighboring regions.\n\nB. WHAM requires complete, continuous coverage; without the missing windows, WHAM cannot be used at all. The only viable option is to discard the entire dataset until windows $\\#8$ and $\\#21$ are re-simulated.\n\nC. Apply WHAM (or Multistate Bennett Acceptance Ratio (MBAR)) to the available $28$ windows, first checking statistical overlap along $x$. Treat the data as up to three connected components separated by the gaps at $x_{\\#8}$ and $x_{\\#21}$. Extract piecewise PMFs on each connected component and report that (i) the relative free-energy offsets between disconnected segments are undetermined, and (ii) uncertainties inflate and potential bias increases near the edges adjacent to the missing windows. Optionally, use coarser $x$-bins to stabilize noise, but do not attempt to bridge the gaps without new data.\n\nD. Reweight the trajectories to effectively stronger umbrellas by post-processing to increase $k$ for all windows, thereby broadening effective coverage and restoring overlap across the gaps. WHAM will then reconstruct a single continuous PMF with at most a uniform additive constant error.\n\nE. Switch from WHAM to umbrella integration of the mean force $\\langle \\partial w_i/\\partial x \\rangle$ over $x$ using the available windows only. Because force integrates to the PMF, gaps in $x$ do not affect the integral, and the PMF will be correct up to one global additive constant.", "solution": "The problem statement is first subjected to a rigorous validation.\n\nStep 1: Extract Givens\n- Method: Umbrella sampling and Weighted Histogram Analysis Method (WHAM).\n- Reaction coordinate: A one-dimensional coordinate $x$.\n- Temperature: $T = 300\\,\\mathrm{K}$.\n- Number of windows: $30$.\n- Window centers: $x_i = (i-1)\\times 0.10\\,\\mathrm{nm}$ for $i \\in \\{1,2,\\dots,30\\}$.\n- Umbrella bias potential: $w_i(x) = \\tfrac{1}{2}k (x - x_i)^2$.\n- Force constant: $k = 1000\\,\\mathrm{kJ\\,mol^{-1}\\,nm^{-2}}$.\n- Objective: Estimate the unbiased potential of mean force (PMF), $U_0(x)$.\n- Data loss: Time series for windows $\\#8$ and $\\#21$ are lost.\n- Assumption: The local variation of the underlying unbiased potential $U_0(x)$ near $x_i$ is modest compared to the umbrella curvature.\n- Approximation: The distribution of $x$ sampled in window $i$ is locally Gaussian with variance $\\sigma^2 \\approx k_B T / k$.\n- Constant: Boltzmann constant $k_B$.\n\nStep 2: Validate Using Extracted Givens\nThe problem describes a common scenario in computational chemistry involving umbrella sampling simulations and PMF reconstruction using WHAM. All concepts, including the umbrella potential form, WHAM, PMF, and the Gaussian approximation, are standard and scientifically sound. The provided numerical values for temperature, force constant, and window spacing are physically realistic for such a simulation. The core of the problem is to determine the best-practice methodology for handling a specific, and realistic, type of data loss. The problem is well-posed, as it asks for the most scientifically defensible strategy and the expected defects, rather than an unobtainable exact solution. The language is objective and precise. The problem does not violate any criteria for validity.\n\nStep 3: Verdict and Action\nThe problem statement is declared valid. A solution will be derived based on the fundamental principles of statistical mechanics and the WHAM formalism.\n\nThe primary purpose of WHAM is to combine data from multiple biased simulations (windows) to obtain a globally optimal estimate of the unbiased probability distribution $P_0(x)$ along a coordinate $x$. The PMF is then given by $U_0(x) = -k_B T \\ln P_0(x) + C$, where $C$ is an arbitrary constant. The WHAM equations are a set of self-consistent equations that solve for $P_0(x)$ and the relative free energies $F_i$ of biasing each window:\n$$ P_0(x) \\propto \\frac{\\sum_{i=1}^{M} N_i(x)}{\\sum_{j=1}^{M} n_j \\exp[-(w_j(x) - F_j)/(k_B T)]} $$\n$$ \\exp(-F_i / (k_B T)) = \\int P_0(x') \\exp(-w_i(x') / (k_B T)) dx' $$\nHere, $M$ is the number of windows, $N_i(x)$ is the histogram of coordinate $x$ from window $i$, and $n_i = \\int N_i(x')dx'$ is the total number of samples in that window.\n\nA critical requirement for the WHAM equations to yield a single, continuous PMF over the entire range of $x$ is statistical overlap between adjacent windows. This overlap allows the free energy differences between windows, encapsulated in the $F_i$ terms, to be determined, thus connecting the entire series of simulations.\n\nLet us evaluate the overlap based on the provided information. The problem states that the biased probability distribution in each window is approximately a Gaussian with variance $\\sigma^2 \\approx k_B T / k$.\nUsing $k_B \\approx 8.314 \\times 10^{-3} \\,\\mathrm{kJ\\,mol^{-1}\\,K^{-1}}$, $T = 300\\,\\mathrm{K}$, and $k = 1000\\,\\mathrm{kJ\\,mol^{-1}\\,nm^{-2}}$:\n$$ \\sigma^2 \\approx \\frac{(8.314 \\times 10^{-3} \\,\\mathrm{kJ\\,mol^{-1}\\,K^{-1}}) \\times (300\\,\\mathrm{K})}{1000\\,\\mathrm{kJ\\,mol^{-1}\\,nm^{-2}}} \\approx 2.494 \\times 10^{-3}\\,\\mathrm{nm^2} $$\nThe standard deviation is therefore:\n$$ \\sigma \\approx \\sqrt{2.494 \\times 10^{-3}\\,\\mathrm{nm^2}} \\approx 0.0499\\,\\mathrm{nm} \\approx 0.05\\,\\mathrm{nm} $$\nThe spacing between adjacent window centers is $\\Delta x = 0.10\\,\\mathrm{nm}$. This corresponds to a separation of $0.10\\,\\mathrm{nm} / 0.05\\,\\mathrm{nm} = 2\\sigma$. This separation is standard and ensures good statistical overlap between adjacent windows.\n\nHowever, with windows $\\#8$ and $\\#21$ missing, we have two gaps in the data.\n1.  Between window $\\#7$ (center $x_7 = 0.70\\,\\mathrm{nm}$) and window $\\#9$ (center $x_9 = 0.80\\,\\mathrm{nm}$). The separation is $0.20\\,\\mathrm{nm}$, which is approximately $4\\sigma$.\n2.  Between window $\\#20$ (center $x_{20} = 1.90\\,\\mathrm{nm}$) and window $\\#22$ (center $x_{22} = 2.10\\,\\mathrm{nm}$). The separation is again $0.20\\,\\mathrm{nm}$, approximately $4\\sigma$.\n\nThe overlap between two Gaussian distributions separated by $4\\sigma$ is exceedingly small. The probability density of one Gaussian at the mean of the other is proportional to $\\exp(- (4\\sigma)^2 / (2\\sigma^2)) = \\exp(-8)$, a very small number. This near-zero overlap means that there is no statistically meaningful way to connect the free energies across these gaps. The WHAM procedure will effectively treat the data as three disconnected sets of simulations (or \"components\"):\n-   Component 1: Windows $\\#1$ through $\\#7$.\n-   Component 2: Windows $\\#9$ through $\\#20$.\n-   Component 3: Windows $\\#22$ through $\\#30$.\n\nWHAM (or its more general variant, MBAR) can be applied to each component individually to produce a continuous PMF segment. However, the relative free energy offsets between these three segments will be undetermined. We can find the shape of the PMF within $[x_1, x_7]$, $[x_9, x_{20}]$, and $[x_{22}, x_{30}]$, but we cannot know how to shift these three curves vertically to align them into a single global PMF. Additionally, near the edges of these components (e.g., the PMF derived from windows $\\#7$, $\\#9$, $\\#20$, and $\\#22$), the statistical quality of the estimate will be degraded. The WHAM estimator for $P_0(x)$ benefits from contributions from all windows that sample region $x$. Near the gap at window $\\#8$, the data is missing a key contributor, leading to higher statistical uncertainty and potential for bias in the PMF estimate around $x_7$ and $x_9$.\n\nWith this understanding, we evaluate each option.\n\nA. Fill the missing histograms by linear interpolation of neighboring histograms along $x$, then run WHAM on all $30$ windows to enforce continuity. The PMF will be continuous with uncertainties comparable to neighboring regions.\nThis strategy is fundamentally flawed. A histogram is a result of a stochastic process, not a smooth deterministic function. Interpolating histograms amounts to fabricating data. This approach has no basis in statistical mechanics and will introduce uncontrolled and unquantifiable systematic errors. Any features in the true PMF within the gaps (e.g., barriers or wells) would be artificially smoothed over. The claim that uncertainties would be \"comparable to neighboring regions\" is false, as the error properties of the fabricated data are unknown. Verdict: **Incorrect**.\n\nB. WHAM requires complete, continuous coverage; without the missing windows, WHAM cannot be used at all. The only viable option is to discard the entire dataset until windows $\\#8$ and $\\#21$ are re-simulated.\nThis statement is too strong and impractical. WHAM is a robust method that can be applied to any set of states (windows), connected or not. If the states are disconnected, the result is a set of PMFs with unknown relative offsets, which is still valuable information. Discarding $28$ valid simulations is an enormous waste of computational resources and data. Extracting partial information is far superior to extracting no information. Verdict: **Incorrect**.\n\nC. Apply WHAM (or Multistate Bennett Acceptance Ratio (MBAR)) to the available $28$ windows, first checking statistical overlap along $x$. Treat the data as up to three connected components separated by the gaps at $x_{\\#8}$ and $x_{\\#21}$. Extract piecewise PMFs on each connected component and report that (i) the relative free-energy offsets between disconnected segments are undetermined, and (ii) uncertainties inflate and potential bias increases near the edges adjacent to the missing windows. Optionally, use coarser $x$-bins to stabilize noise, but do not attempt to bridge the gaps without new data.\nThis strategy is the only one that is statistically rigorous and scientifically honest. It correctly identifies the consequence of poor overlap: the dataset partitions into disconnected components. It correctly diagnoses the main defect: the relative free energies between these components are undetermined. It also correctly predicts the secondary defect: increased uncertainty and potential bias at the boundaries of the gaps due to missing information. Mentioning MBAR is appropriate as it is a related, more general method that would face the same limitation. This approach makes the best possible use of the available data while transparently reporting the limitations of the resulting PMF. Verdict: **Correct**.\n\nD. Reweight the trajectories to effectively stronger umbrellas by post-processing to increase $k$ for all windows, thereby broadening effective coverage and restoring overlap across the gaps.\nThis proposal is based on a misunderstanding of both reweighting and the effect of the force constant $k$. Increasing $k$ makes the harmonic restraint *stronger* or *stiffer*, which leads to *narrower* sampling distributions, not broader ones. This would worsen the overlap, not restore it. To broaden sampling, one would need to reweight to a smaller $k$. However, reweighting is only effective when the target and sampled distributions have significant overlap. Attempting to reweight data from window $\\#7$ to predict behavior in the region of window $\\#8$ (a gap of $2\\sigma$ away from the center of window $\\#7$) is already a stretch, and bridging the full $4\\sigma$ gap between the means of windows $\\#7$ and $\\#9$ by reweighting is statistically hopeless. It would result in huge statistical errors, as the weights would be dominated by a few unrepresentative data points. Verdict: **Incorrect**.\n\nE. Switch from WHAM to umbrella integration of the mean force $\\langle \\partial w_i/\\partial x \\rangle$ over $x$ using the available windows only. Because force integrates to the PMF, gaps in $x$ do not affect the integral, and the PMF will be correct up to one global additive constant.\nThis option is incorrect. Umbrella integration reconstructs the PMF by integrating the unbiased mean force, $U_0(x) = -\\int \\langle F_{unbiased}(x') \\rangle dx'$. The unbiased mean force $\\langle F_{unbiased}(x') \\rangle$ itself must be estimated at all values of $x'$ across the integration range. This is typically done by combining data from all windows. With windows $\\#8$ and $\\#21$ missing, there are regions of $x$ where we have no data to estimate $\\langle F_{unbiased}(x') \\rangle$. The statement that \"gaps in $x$ do not affect the integral\" is false. An integral requires the integrand to be known across the domain of integration. Without data in the gaps, the integral cannot be performed across them, and the PMF cannot be connected. This method suffers from the same fundamental problem as WHAM. Verdict: **Incorrect**.", "answer": "$$\\boxed{C}$$", "id": "2465737"}, {"introduction": "Having explored high-level experimental design and data integrity, we now dive into the numerical heart of the WHAM algorithm itself. One of the most critical user-defined parameters in any histogram-based method is the bin width, $\\Delta x$. This coding-based practice guides you through a numerical experiment to investigate the fundamental bias-variance trade-off, where choosing too small a bin width increases statistical noise, while choosing too large a bin width washes out important features of the PMF. [@problem_id:2465743]", "problem": "You will implement a numerical study of the Weighted Histogram Analysis Method (WHAM) to quantify, from first principles, the trade-off between systematic bias and statistical variance in the reconstructed potential of mean force (PMF) as a function of the histogram bin width $\\,\\Delta x\\,$. Your implementation must adhere to the following mathematical model and tasks.\n\nContext and core definitions:\n- In one dimension, the unbiased equilibrium probability density along a reaction coordinate $\\,x\\,$ is $\\,p(x) \\propto \\exp\\!\\left(-\\beta U(x)\\right)\\,$, where $\\,U(x)\\,$ is the potential energy and $\\,\\beta = 1/(k_\\mathrm{B} T)\\,$. In this problem, work in reduced units where $\\,\\beta = 1\\,$ so that energies are in units of $\\,k_\\mathrm{B}T\\,$.\n- The potential of mean force (PMF) is defined (up to an additive constant) as $\\,F(x) = -\\ln p(x)\\,$. With $\\,\\beta = 1\\,$ and $\\,p(x) \\propto \\exp(-U(x))\\,$, the true PMF equals $\\,U(x)\\,$ plus a constant.\n- Umbrella sampling measures data under $\\,K\\,$ biased potentials $\\,U_k^\\text{tot}(x) = U(x) + w_k(x)\\,$, where $\\,w_k(x)\\,$ is a known bias potential, here harmonic: $\\,w_k(x) = \\tfrac{1}{2}\\kappa\\,(x - x_k)^2\\,$ with spring constant $\\,\\kappa > 0\\,$ and window center $\\,x_k\\,$.\n- A histogram estimator partitions the domain into bins of width $\\,\\Delta x\\,$ and uses bin counts. The Weighted Histogram Analysis Method (WHAM) combines histograms from multiple biased windows to estimate the unbiased density, and thus the PMF, by solving a self-consistent system for the density and the window free-energy offsets.\n\nYour tasks:\n1) Synthetic data generation from first principles. Let the true potential be the symmetric double-well\n$$\nU(x) \\;=\\; \\alpha \\,\\bigl(x^2 - c^2\\bigr)^2,\n$$\nwith $\\,\\alpha > 0\\,$ and $\\,c > 0\\,$, and consider the domain $\\,x \\in [-L, L]\\,$. For each umbrella window $\\,k \\in \\{1,\\dots,K\\}\\,$ with harmonic bias $\\,w_k(x) = \\tfrac{1}{2}\\kappa\\,(x-x_k)^2\\,$ and a specified total sample size $\\,N_k\\,$, generate synthetic histogram counts by:\n- Computing the exact biased bin probabilities from the continuous densities $\\,p_k(x) \\propto \\exp\\!\\bigl(-U(x) - w_k(x)\\bigr)\\,$ via numerical quadrature on a sufficiently fine grid over $\\,[-L,L]\\,$.\n- Drawing counts per bin for window $\\,k\\,$ from a multinomial distribution with total $\\,N_k\\,$ and the computed bin probabilities. This ensures scientific realism by directly reflecting the Boltzmann distribution under the bias, without relying on shortcuts or closed-form sampling.\n\n2) WHAM reconstruction. For a given $\\,\\Delta x\\,$, implement WHAM to estimate the unbiased density at the bin centers. Your implementation must use a fixed-point iteration that enforces proper normalization of the density and determines the free-energy offsets for each window. The PMF estimate is $\\,\\widehat{F}(x_b) = -\\ln \\widehat{p}(x_b)\\,$ up to an additive constant; for numerical comparison, choose the constant such that $\\,\\min_b \\widehat{F}(x_b) = 0\\,$, and likewise set the true PMF $\\,F_\\text{true}(x)=U(x)\\,$ to have $\\,\\min_b F_\\text{true}(x_b) = 0\\,$ when evaluated on the same bin centers.\n\n3) Bias–variance analysis as a function of $\\,\\Delta x\\,$. For each candidate $\\,\\Delta x\\,$, repeat the synthetic dataset generation and WHAM reconstruction for $\\,R\\,$ independent replicates to empirically estimate:\n- The pointwise mean PMF $\\,\\overline{F}_{\\Delta x}(x_b)\\,$ and variance $\\,\\operatorname{Var}_{\\Delta x}\\!\\bigl[F(x_b)\\bigr]\\,$ across replicates.\n- The integrated squared bias\n$$\nB^2(\\Delta x) \\;=\\; \\sum_b \\bigl(\\,\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}(x_b)\\,\\bigr)^2 \\,\\Delta x,\n$$\nand the integrated variance\n$$\nV(\\Delta x) \\;=\\; \\sum_b \\operatorname{Var}_{\\Delta x}\\!\\bigl[F(x_b)\\bigr] \\,\\Delta x.\n$$\n- The mean integrated squared error (MISE) $\\,\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)\\,$, which reflects the fundamental trade-off: increasing $\\,\\Delta x\\,$ increases bin-averaging bias while reducing statistical variance; decreasing $\\,\\Delta x\\,$ reduces bias while increasing variance due to fewer counts per bin.\n\nTest suite:\nAdopt the following fixed physical and numerical parameters in reduced units:\n- Double-well potential: $\\,\\alpha = 2\\,$, $\\,c = 1\\,$, domain $\\,[-L,L] = [-2,2]\\,$.\n- Number of umbrellas: $\\,K = 7\\,$ with centers $\\,x_k \\in \\{-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5\\}\\,$ and spring constant $\\,\\kappa = 20\\,$.\n- Number of independent replicates for bias–variance estimation: $\\,R = 40\\,$.\n- For all cases, assume equal counts per window $\\,N_k = N\\,$.\n\nDefine three test cases that explore variance-dominated, balanced, and bias-dominated regimes by varying $\\,N\\,$ and candidate bin widths:\n- Case $\\,1\\,$ (moderate sampling): $\\,N = 2000\\,$ and candidate $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$.\n- Case $\\,2\\,$ (low sampling): $\\,N = 500\\,$ and candidate $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$.\n- Case $\\,3\\,$ (high sampling): $\\,N = 10000\\,$ and candidate $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$.\n\nWhat your program must do:\n- For each test case, and for each candidate $\\,\\Delta x\\,$, perform the replicate-based bias–variance analysis described above and compute $\\,\\mathrm{MISE}(\\Delta x)\\,$.\n- For each test case, select the $\\,\\Delta x\\,$ that minimizes $\\,\\mathrm{MISE}(\\Delta x)\\,$. If there is a tie, choose the smallest $\\,\\Delta x\\,$ among the minimizers.\n\nFinal output format:\n- Your program should produce a single line of output containing the selected optimal bin widths for the three test cases as a comma-separated list enclosed in square brackets, for example `[0.05,0.10,0.05]`. No additional text should be printed.\n\nImportant notes:\n- All energies are in units of $\\,k_\\mathrm{B}T\\,$ and $\\,x\\,$ is dimensionless.\n- Angles are not involved.\n- Randomness must be handled internally and reproducibly; fix a random seed.\n- The implementation must be self-contained and must not read or write any files or require user input.", "solution": "The problem requires a numerical investigation of the bias-variance trade-off for the Weighted Histogram Analysis Method (WHAM) as a function of histogram bin width, $\\Delta x$. The problem is scientifically valid, well-posed, and all necessary parameters are provided. It represents a standard task in computational chemistry and statistical mechanics, based on established principles. I will proceed with a full solution.\n\nThe solution is implemented in three main stages: synthetic data generation, PMF reconstruction using WHAM, and a bias-variance analysis over multiple replicates.\n\n### 1. Synthetic Data Generation\n\nTo ensure a scientifically realistic analysis, synthetic data must be generated directly from the underlying Boltzmann distribution. The true, unbiased potential is a symmetric double-well potential given by $U(x) = \\alpha(x^2 - c^2)^2$, with $\\alpha=2$ and $c=1$. The domain is $x \\in [-2, 2]$. All calculations are performed in reduced units where $\\beta = (k_B T)^{-1} = 1$.\n\nIn umbrella sampling, the system is simulated under $K$ different biasing potentials, $w_k(x)$, to enhance sampling of high-energy regions. Here, harmonic biases $w_k(x) = \\frac{1}{2}\\kappa(x - x_k)^2$ are used, with $\\kappa=20$ and $K=7$ windows centered at $x_k \\in \\{-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5\\}$.\n\nFor each window $k$, the total potential is $U_k^\\text{tot}(x) = U(x) + w_k(x)$, and the corresponding equilibrium probability density is $p_k(x) \\propto \\exp(-U_k^\\text{tot}(x))$. To generate histogram counts for a given bin width $\\Delta x$, we first partition the domain $[-L, L]$ into discrete bins. The exact probability of observing a sample in bin $b$ (from $x_b^{\\text{start}}$ to $x_b^{\\text{end}}$) for window $k$ is given by the integral of the normalized density:\n$$\nP_{k,b} = \\frac{\\int_{x_b^{\\text{start}}}^{x_b^{\\text{end}}} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}{\\int_{-L}^{L} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}\n$$\nThese integrals are computed via numerical quadrature over a fine grid. A high-resolution grid (with spacing significantly smaller than any $\\Delta x$) is used to approximate the integrals by a discrete sum (trapezoidal rule).\n\nWith the vector of bin probabilities $\\{P_{k,b}\\}$ for each window $k$, synthetic histogram counts $\\{N_{k,b}\\}$ are drawn from a multinomial distribution with $N_k$ total trials (samples). This process accurately models the statistical nature of sampling in a molecular simulation.\n\n### 2. WHAM Reconstruction\n\nWHAM provides a way to combine data from multiple biased simulations to compute the optimal estimate of the unbiased probability distribution, $p(x)$, and thus the potential of mean force (PMF), $F(x) = -\\ln p(x)$. The method solves a set of self-consistent equations for the unbiased probabilities $p_b$ at each bin center $x_b$ and the dimensionless free energies $f_k$ of each simulation window. With $\\beta=1$, the WHAM equations are:\n$$\np_b = \\frac{\\sum_{k=1}^K N_{k,b}}{\\sum_{k=1}^K N_k \\exp(f_k - w_k(x_b))}\n$$\n$$\n\\exp(-f_k) = \\sum_b p_b \\exp(-w_k(x_b))\n$$\nHere, $p_b$ are unnormalized probabilities proportional to the true density. These equations are solved using a fixed-point iteration:\n1. Initialize all free energies $f_k = 0$.\n2. Repeatedly compute the probabilities $p_b$ using the current $f_k$.\n3. Use the new $p_b$ to compute updated free energies $f_k^{\\text{new}}$.\n4. To prevent drift, a constraint is applied, such as fixing one free energy (e.g., $f_1=0$).\n5. The iteration continues until the free energies converge to a specified tolerance.\n\nAfter convergence, the final probabilities $p_b$ are used to compute the PMF estimate for the given dataset: $\\widehat{F}(x_b) = -\\ln p_b$. For comparison, the estimated PMF is shifted by an additive constant such that its minimum value is zero. The true PMF, $F_{\\text{true}}(x_b) = U(x_b)$, is also normalized to have a minimum of zero over the same set of bin centers.\n\nA critical issue arises when a bin $b$ has zero counts across all windows, i.e., $\\sum_k N_{k,b} = 0$. In this case, $p_b=0$ and the estimated PMF, $\\widehat{F}(x_b)$, is infinite. This represents a catastrophic failure of the estimator for that bin.\n\n### 3. Bias-Variance Analysis\n\nThe core of the problem is to quantify the trade-off between systematic error (bias) and statistical error (variance) as a function of bin width $\\Delta x$. For each candidate $\\Delta x$, we perform $R=40$ independent replicates of the data generation and WHAM reconstruction process.\n\nAcross the $R$ replicates, we compute:\n- The mean estimated PMF: $\\overline{F}_{\\Delta x}(x_b) = \\frac{1}{R} \\sum_{r=1}^R \\widehat{F}_r(x_b)$.\n- The sample variance of the PMF: $\\operatorname{Var}_{\\Delta x}[F(x_b)] = \\frac{1}{R-1} \\sum_{r=1}^R (\\widehat{F}_r(x_b) - \\overline{F}_{\\Delta x}(x_b))^2$.\n\nThese are then integrated over the domain to yield the integrated squared bias $B^2(\\Delta x)$ and integrated variance $V(\\Delta x)$:\n$$\nB^2(\\Delta x) = \\sum_b \\left(\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}}(x_b)\\right)^2 \\Delta x\n$$\n$$\nV(\\Delta x) = \\sum_b \\operatorname{Var}_{\\Delta x}[F(x_b)] \\Delta x\n$$\nThe Mean Integrated Squared Error (MISE) is the sum: $\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)$.\n\nThe choice of $\\Delta x$ governs the trade-off:\n- **Small $\\Delta x$**: Low bias, as bin-averaging error is minimal. However, with fewer samples per bin, the statistical variance is high. This increases the probability of empty bins, which results in an infinite PMF estimate. If any replicate yields an infinite PMF for any bin, the mean PMF for that bin will also be infinite, leading to an infinite MISE.\n- **Large $\\Delta x$**: Low variance, as more samples are pooled into each bin, reducing the chance of empty bins. However, bias increases due to averaging the potential over a wider region.\n\nOur implementation handles the infinite PMF values by assigning an infinite MISE to any $\\Delta x$ that results in one or more empty bins in any of the $R$ replicates. The optimal $\\Delta x$ is then the one that minimizes the MISE among the choices that produce a finite MISE. This correctly penalizes bin widths that are too small for the given sample size. The final algorithm iterates through the candidate bin widths for each test case, calculates the MISE, and selects the optimal $\\Delta x$ according to the specified criteria.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the WHAM bias-variance analysis for all test cases.\n    \"\"\"\n    \n    # --- Fixed Physical and Numerical Parameters ---\n    ALPHA = 2.0\n    C = 1.0\n    L = 2.0\n    K_UMBRELLA = 7\n    X_CENTERS = np.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5])\n    KAPPA = 20.0\n    R_REPLICATES = 40\n    \n    # --- WHAM Solver Parameters ---\n    WHAM_MAX_ITER = 10000\n    WHAM_TOL = 1e-8\n    \n    # --- Data Generation Parameters ---\n    FINE_GRID_FACTOR = 100\n    RANDOM_SEED = 1234\n    \n    RNG = np.random.default_rng(RANDOM_SEED)\n\n    # --- Test Cases ---\n    test_cases = [\n        # Case 1 (moderate sampling)\n        (2000, [0.02, 0.05, 0.10, 0.20]),\n        # Case 2 (low sampling)\n        (500, [0.02, 0.05, 0.10, 0.20]),\n        # Case 3 (high sampling)\n        (10000, [0.02, 0.05, 0.10, 0.20]),\n    ]\n    \n    # --- Helper Functions ---\n\n    def true_potential(x, alpha, c):\n        return alpha * (x**2 - c**2)**2\n\n    def bias_potential(x, x_k, kappa):\n        return 0.5 * kappa * (x - x_k)**2\n\n    def generate_synthetic_data(n_samples, delta_x):\n        \"\"\"\n        Generates one set of synthetic histogram data for all K windows.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        n_bins = len(bins) - 1\n        \n        fine_x = np.linspace(-L, L, n_bins * FINE_GRID_FACTOR)\n        fine_dx = fine_x[1] - fine_x[0]\n\n        U_fine = true_potential(fine_x, ALPHA, C)\n        \n        hist_counts = np.zeros((K_UMBRELLA, n_bins), dtype=np.int32)\n        \n        for k in range(K_UMBRELLA):\n            w_k_fine = bias_potential(fine_x, X_CENTERS[k], KAPPA)\n            U_tot_k_fine = U_fine + w_k_fine\n            \n            # Numerically integrate to get bin probabilities\n            unnorm_p_density = np.exp(-U_tot_k_fine)\n            Z_k = np.sum(unnorm_p_density) * fine_dx\n            \n            p_k_bins = np.zeros(n_bins)\n            for b in range(n_bins):\n                bin_mask = (fine_x >= bins[b])  (fine_x  bins[b+1])\n                p_k_bins[b] = np.sum(unnorm_p_density[bin_mask]) * fine_dx / Z_k\n            \n            # Ensure probabilities sum to 1 due to potential floating point errors\n            p_k_bins /= np.sum(p_k_bins)\n            \n            # Generate counts from multinomial distribution\n            hist_counts[k, :] = RNG.multinomial(n_samples, p_k_bins)\n            \n        return hist_counts\n\n    def run_wham(hist_counts, delta_x, n_samples_vec):\n        \"\"\"\n        Implements the WHAM fixed-point iteration to find the PMF.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n\n        # Pre-compute biases at bin centers\n        w_kb = np.zeros((K_UMBRELLA, n_bins))\n        for k in range(K_UMBRELLA):\n            w_kb[k, :] = bias_potential(bin_centers, X_CENTERS[k], KAPPA)\n\n        f_k = np.zeros(K_UMBRELLA)\n        \n        for _ in range(WHAM_MAX_ITER):\n            f_k_old = f_k.copy()\n            \n            # Equation for probabilities p_b\n            numer = np.sum(hist_counts, axis=0) # N_b\n            log_denom_terms = f_k[:, np.newaxis] - w_kb\n            max_log = np.max(log_denom_terms, axis=0)\n            denom_arg = n_samples_vec[:, np.newaxis] * np.exp(log_denom_terms - max_log)\n            denom = np.exp(max_log) * np.sum(denom_arg, axis=0)\n\n            # p_b are unnormalized probabilities\n            p_b = np.zeros_like(numer, dtype=float)\n            non_zero_denom = denom > 1e-15 # Avoid division by zero\n            p_b[non_zero_denom] = numer[non_zero_denom] / denom[non_zero_denom]\n\n            # Equation for free energies f_k\n            exp_neg_w_kb = np.exp(-w_kb)\n            f_k_new_arg = np.sum(p_b[np.newaxis, :] * exp_neg_w_kb, axis=1)\n\n            # Avoid log(0) for windows with no overlap with data\n            f_k = -np.log(f_k_new_arg, where=f_k_new_arg > 0, out=np.full_like(f_k_new_arg, np.inf))\n            \n            # Shift to set f_1 = 0\n            f_k -= f_k[0]\n            \n            if np.linalg.norm(f_k - f_k_old)  WHAM_TOL:\n                break\n        \n        # Final PMF calculation\n        pmf = -np.log(p_b, where=p_b > 0, out=np.full_like(p_b, np.inf))\n        \n        if not np.all(np.isfinite(pmf)):\n             return pmf # Return with inf values\n        \n        pmf -= np.min(pmf)\n        return pmf\n\n    def analyze_bias_variance(n_samples, delta_x):\n        \"\"\"\n        Performs the full bias-variance analysis for a given N and delta_x.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n        \n        # True PMF on bin centers, normalized\n        F_true = true_potential(bin_centers, ALPHA, C)\n        F_true -= np.min(F_true)\n        \n        all_pmfs = np.zeros((R_REPLICATES, n_bins))\n        n_samples_vec = np.full(K_UMBRELLA, n_samples)\n        \n        for r in range(R_REPLICATES):\n            hist_counts = generate_synthetic_data(n_samples, delta_x)\n            pmf_estimate = run_wham(hist_counts, delta_x, n_samples_vec)\n            all_pmfs[r, :] = pmf_estimate\n\n        # If any replicate resulted in an empty bin, MISE is infinite\n        if np.isinf(all_pmfs).any():\n            return np.inf\n            \n        # Calculate mean and variance of PMF estimates\n        mean_pmf = np.mean(all_pmfs, axis=0)\n        var_pmf = np.var(all_pmfs, axis=0, ddof=1) # Sample variance\n        \n        # Integrated squared bias\n        bias_sq = np.sum((mean_pmf - F_true)**2) * delta_x\n        \n        # Integrated variance\n        variance = np.sum(var_pmf) * delta_x\n        \n        return bias_sq + variance\n\n    # --- Main Loop ---\n    \n    optimal_dx_results = []\n    \n    for n_samples, dx_candidates in test_cases:\n        mises = []\n        for dx in dx_candidates:\n            mise = analyze_bias_variance(n_samples, dx)\n            mises.append(mise)\n        \n        mises_array = np.array(mises)\n        \n        # Find the minimum finite MISE\n        min_mise = np.min(mises_array[np.isfinite(mises_array)]) if np.any(np.isfinite(mises_array)) else np.inf\n\n        if not np.isfinite(min_mise):\n             # This case should not happen with the given parameters\n             # If all MISE are inf, pick the largest dx as it's most robust\n            optimal_dx = dx_candidates[-1]\n        else:\n            # Find all indices matching the minimum MISE\n            best_indices = np.where(mises_array == min_mise)[0]\n            # Tie-breaking rule: choose smallest dx\n            optimal_dx = dx_candidates[best_indices[0]]\n\n        optimal_dx_results.append(optimal_dx)\n\n    # Format the final output\n    print(f\"[{','.join(f'{x:.2f}' for x in optimal_dx_results)}]\")\n\n\nsolve()\n```", "id": "2465743"}]}