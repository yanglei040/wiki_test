{"hands_on_practices": [{"introduction": "In many real-world inverse problems, the process of generating observations from a system's state is not a simple, direct measurement. Instead, it can be a complex chain of operations, involving physical effects like instrument smoothing, nonlinear sensor responses, and direct dependencies on the model parameters. This exercise ([@problem_id:3419121]) provides essential practice in systematically deriving the adjoint system for such a composite observation operator by carefully applying the chain rule for Fréchet derivatives, a fundamental skill for correctly formulating gradient-based optimization in sophisticated data assimilation systems.", "problem": "Consider the following parameter-to-state map and hybrid observation operator composed of linear and nonlinear elements. Let the spatial domain be $\\Omega = (0,1)$ with homogeneous Dirichlet boundary conditions. For a parameter field $m \\in L^{2}(\\Omega)$, the state $u \\in H_{0}^{1}(\\Omega)$ is determined by the boundary value problem\n$$\n- \\frac{d^{2}u}{dx^{2}} + u = m \\quad \\text{for } x \\in (0,1), \\quad u(0) = 0, \\quad u(1) = 0.\n$$\nDefine a smoothing operator $S: L^{2}(\\Omega) \\to L^{2}(\\Omega)$ by\n$$\n(Su)(x) = \\int_{0}^{1} s(x,\\xi)\\, u(\\xi)\\, d\\xi,\n$$\nwhere $s \\in L^{2}(\\Omega \\times \\Omega)$ is a given symmetric kernel. Define a pointwise nonlinearity $\\Phi: L^{2}(\\Omega) \\to L^{2}(\\Omega)$ by\n$$\n(\\Phi v)(x) = \\exp(\\alpha\\, v(x)) - 1,\n$$\nwhere $\\alpha > 0$ is a fixed constant. Let $W: L^{2}(\\Omega) \\to \\mathbb{R}^{K}$ and $E: L^{2}(\\Omega) \\to \\mathbb{R}^{K}$ be linear mappings defined componentwise by\n$$\n(Wv)_{i} = \\int_{0}^{1} w_{i}(x)\\, v(x)\\, dx, \\qquad (Em)_{i} = \\int_{0}^{1} e_{i}(x)\\, m(x)\\, dx,\n$$\nfor $i = 1, \\dots, K$, where $w_{i}, e_{i} \\in L^{2}(\\Omega)$ are given functions. Given observations $y \\in \\mathbb{R}^{K}$ and a symmetric positive definite weighting matrix $R \\in \\mathbb{R}^{K \\times K}$, define the hybrid observation operator by\n$$\nH(u,m) = W\\big(\\Phi(Su)\\big) + E m,\n$$\nand the data assimilation objective functional by\n$$\nJ(m) = \\frac{1}{2}\\big(H(u(m),m) - y\\big)^{\\top} R^{-1} \\big(H(u(m),m) - y\\big) + \\frac{\\lambda}{2} \\int_{0}^{1} m(x)^{2}\\, dx,\n$$\nwith $\\lambda > 0$.\n\nStarting from the definitions of the Fréchet derivative and the adjoint-state method based on a Lagrangian construction, derive the $L^{2}(\\Omega)$-gradient of $J$ with respect to $m$. Your derivation must explicitly account for the composition of the hybrid observation operator $H$, including the linear operator $W$, the nonlinear pointwise map $\\Phi$, and the smoothing operator $S$, as well as the direct dependence of $H$ on $m$ through $E$. Express the final result as a single closed-form analytical expression for the gradient $g(x)$ in terms of $m$, $u$, an adjoint state $p$, and the given operators and functions. You may introduce auxiliary quantities defined from $R^{-1}$ and the residual, but the final answer must be a single expression for $g(x)$. No numerical approximation is required.", "solution": "The objective is to derive the $L^{2}(\\Omega)$-gradient of the functional $J(m)$ with respect to the parameter field $m$. We will employ the adjoint-state method using a Lagrangian formulation. The problem is to find the function $g \\in L^{2}(\\Omega)$ such that for any perturbation $\\delta m \\in L^{2}(\\Omega)$, the Fréchet derivative of $J$ is given by\n$$\ndJ(m)[\\delta m] = \\langle g, \\delta m \\rangle_{L^{2}} = \\int_{0}^{1} g(x)\\delta m(x)\\, dx.\n$$\nThe state variable $u$ is a function of the parameter $m$, denoted by $u(m)$, defined by the state equation:\n$$\n- \\frac{d^{2}u}{dx^{2}} + u = m, \\quad u(0)=0, u(1)=0.\n$$\nThis constitutes a constraint in the optimization problem. We introduce the Lagrangian functional $\\mathcal{L}(u,m,p)$ by augmenting the objective functional $J(m)$ with the weak form of the state equation, weighted by a Lagrange multiplier $p(x)$, which will serve as the adjoint state. The space for the adjoint state $p$ will be determined during the derivation, but we anticipate $p \\in H_{0}^{1}(\\Omega)$.\n\nThe Lagrangian is defined as:\n$$\n\\mathcal{L}(u, m, p) = J(u,m) + \\int_{0}^{1} p(x) \\left(-\\frac{d^{2}u}{dx^{2}} + u(x) - m(x)\\right) dx.\n$$\nThe objective functional $J$, which implicitly depends on $m$ through $u(m)$, is written as $J(u,m)$:\n$$\nJ(u,m) = \\frac{1}{2}\\big(H(u,m) - y\\big)^{\\top} R^{-1} \\big(H(u,m) - y\\big) + \\frac{\\lambda}{2} \\int_{0}^{1} m(x)^{2}\\, dx.\n$$\nTo eliminate the second derivative of $u$, we integrate the term involving $p \\frac{d^{2}u}{dx^{2}}$ by parts twice.\n\\begin{align*}\n\\int_{0}^{1} -p \\frac{d^{2}u}{dx^{2}} dx &= \\left[-p \\frac{du}{dx}\\right]_{0}^{1} + \\int_{0}^{1} \\frac{dp}{dx} \\frac{du}{dx} dx \\\\\n&= \\left[-p \\frac{du}{dx}\\right]_{0}^{1} + \\left[\\frac{dp}{dx} u\\right]_{0}^{1} - \\int_{0}^{1} u \\frac{d^{2}p}{dx^{2}} dx.\n\\end{align*}\nSince $u \\in H_{0}^{1}(\\Omega)$, we have $u(0)=u(1)=0$. To ensure the boundary terms vanish for any $u$, we require the adjoint state $p$ to also be in $H_{0}^{1}(\\Omega)$, which implies $p(0)=p(1)=0$. With this choice, the Lagrangian becomes:\n$$\n\\mathcal{L}(u, m, p) = J(u,m) + \\int_{0}^{1} u(x)\\left(-\\frac{d^{2}p}{dx^{2}} + p(x)\\right) dx - \\int_{0}^{1} p(x)m(x)\\, dx.\n$$\nThe gradient of $J(m)$ is found by considering the total variation of $\\mathcal{L}$ with respect to $m$. Since $u=u(m)$ is determined by the state equation, we have $\\mathcal{L}(u(m),m,p) = J(m)$ for any $p$. Thus, the total Fréchet derivative of $J$ with respect to $m$ equals that of $\\mathcal{L}$:\n$$\ndJ(m)[\\delta m] = d\\mathcal{L}(u(m),m,p)[\\delta m] = \\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] + \\frac{\\partial \\mathcal{L}}{\\partial m}[\\delta m].\n$$\nHere, $\\delta u$ is the change in the state corresponding to a change $\\delta m$ in the parameter, governed by the linearized state equation: $-\\frac{d^{2}(\\delta u)}{dx^{2}} + \\delta u = \\delta m$.\n\nThe adjoint-state method circumvents the need to compute $\\delta u$ by choosing $p$ such that the term involving $\\delta u$ vanishes. We set the partial Fréchet derivative of $\\mathcal{L}$ with respect to $u$ to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = \\frac{\\partial J}{\\partial u}[\\delta u] + \\int_{0}^{1} \\delta u(x)\\left(-\\frac{d^{2}p}{dx^{2}} + p(x)\\right) dx = 0.\n$$\nThis must hold for all admissible variations $\\delta u$. Let's compute the Gâteaux derivative $\\frac{\\partial J}{\\partial u}[\\delta u]$. Let $\\mathbf{r}(u,m) = H(u,m) - y$.\n$$\n\\frac{\\partial J}{\\partial u}[\\delta u] = \\left\\langle R^{-1}\\mathbf{r}(u,m), \\frac{\\partial H}{\\partial u}[\\delta u] \\right\\rangle_{\\mathbb{R}^{K}}.\n$$\nThe operator $H(u,m) = W(\\Phi(Su)) + Em$ depends on $u$ through the term $W(\\Phi(Su))$. Its Fréchet derivative with respect to $u$ in the direction $\\delta u$ is, by the chain rule:\n$$\n\\frac{\\partial H}{\\partial u}[\\delta u] = W\\left(\\Phi'(Su)[S\\delta u]\\right).\n$$\nThe derivative of the nonlinearity $\\Phi(v)(x) = \\exp(\\alpha v(x)) - 1$ is the multiplication operator $(\\Phi'(v)[\\delta v])(x) = \\alpha \\exp(\\alpha v(x))\\delta v(x)$. Thus,\n$$\n\\frac{\\partial H}{\\partial u}[\\delta u] = W\\left(\\alpha \\exp(\\alpha Su) \\cdot (S\\delta u)\\right).\n$$\nThe term $\\frac{\\partial J}{\\partial u}[\\delta u]$ can be written as an inner product in $L^{2}(\\Omega)$:\n$$\n\\left\\langle R^{-1}\\mathbf{r}, \\frac{\\partial H}{\\partial u}[\\delta u] \\right\\rangle_{\\mathbb{R}^{K}} = \\left\\langle \\left(\\frac{\\partial H}{\\partial u}\\right)^{*} [R^{-1}\\mathbf{r}], \\delta u \\right\\rangle_{L^{2}(\\Omega)}.\n$$\nThe adjoint operator $(\\frac{\\partial H}{\\partial u})^{*}$ is the composition of the adjoints of the operators in reverse order: $(\\frac{\\partial H}{\\partial u})^{*} = S^{*} \\circ (\\Phi'(Su))^{*} \\circ W^{*}$.\n-   The adjoint of $W: L^2(\\Omega) \\to \\mathbb{R}^K$ is $W^{*}: \\mathbb{R}^K \\to L^2(\\Omega)$, given by $(W^{*}\\mathbf{z})(x) = \\sum_{i=1}^{K} z_{i}w_{i}(x)$.\n-   The derivative $\\Phi'(Su)$ is a self-adjoint multiplication operator.\n-   The operator $S$ is an integral operator with a symmetric kernel $s(x, \\xi)$, so it is self-adjoint: $S^{*} = S$.\nLet $\\mathbf{z} = R^{-1}\\mathbf{r}$. The adjoint operator applied to $\\mathbf{z}$ is:\n$$\n\\left(\\frac{\\partial H}{\\partial u}\\right)^{*}[\\mathbf{z}] = S\\left(\\alpha \\exp(\\alpha Su) \\cdot (W^{*}\\mathbf{z})\\right).\n$$\nThe expression for the variation with respect to $u$ becomes:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = \\left\\langle \\left(\\frac{\\partial H}{\\partial u}\\right)^{*}[\\mathbf{z}], \\delta u \\right\\rangle_{L^{2}} + \\left\\langle -\\frac{d^{2}p}{dx^{2}} + p, \\delta u \\right\\rangle_{L^{2}} = 0.\n$$\nSince this holds for all $\\delta u \\in H_0^1(\\Omega)$, we obtain the strong form of the **adjoint equation** for $p \\in H_0^1(\\Omega)$:\n$$\n-\\frac{d^{2}p}{dx^{2}} + p = - \\left(\\frac{\\partial H}{\\partial u}\\right)^{*}[R^{-1}(H(u,m)-y)], \\quad p(0)=0, p(1)=0.\n$$\nWith $p$ defined as the solution to this equation, the derivative of the Lagrangian simplifies to:\n$$\ndJ(m)[\\delta m] = \\frac{\\partial \\mathcal{L}}{\\partial m}[\\delta m] = \\frac{\\partial J}{\\partial m}[\\delta m] - \\int_{0}^{1} p(x)\\delta m(x)\\, dx.\n$$\nWe now compute the partial Gâteaux derivative of $J$ with respect to $m$ (holding $u$ fixed):\n$$\n\\frac{\\partial J}{\\partial m}[\\delta m] = \\left\\langle R^{-1}\\mathbf{r}(u,m), \\frac{\\partial H}{\\partial m}[\\delta m] \\right\\rangle_{\\mathbb{R}^{K}} + \\lambda \\int_{0}^{1} m(x)\\delta m(x)\\, dx.\n$$\nThe partial derivative of $H(u,m)$ with respect to $m$ is simply $\\frac{\\partial H}{\\partial m}[\\delta m] = E\\delta m$.\nThus,\n\\begin{align*}\n\\frac{\\partial J}{\\partial m}[\\delta m] &= \\left\\langle R^{-1}\\mathbf{r}, E\\delta m \\right\\rangle_{\\mathbb{R}^{K}} + \\lambda \\langle m, \\delta m \\rangle_{L^{2}} \\\\\n&= \\left\\langle E^{*}[R^{-1}\\mathbf{r}], \\delta m \\right\\rangle_{L^{2}} + \\langle \\lambda m, \\delta m \\rangle_{L^{2}} \\\\\n&= \\left\\langle E^{*}[R^{-1}(H(u,m)-y)] + \\lambda m, \\delta m \\right\\rangle_{L^{2}}.\n\\end{align*}\nThe adjoint operator $E^{*}: \\mathbb{R}^{K} \\to L^{2}(\\Omega)$ is given by $(E^{*}\\mathbf{z})(x) = \\sum_{i=1}^{K} z_{i}e_{i}(x)$.\nSubstituting this into the expression for $dJ(m)[\\delta m]$:\n$$\ndJ(m)[\\delta m] = \\left\\langle E^{*}[R^{-1}(H(u,m)-y)] + \\lambda m - p, \\delta m \\right\\rangle_{L^{2}}.\n$$\nFrom the definition of the $L^{2}$-gradient, $dJ(m)[\\delta m] = \\langle g, \\delta m \\rangle_{L^{2}}$, we can identify the gradient $g(x)$:\n$$\ng(x) = \\lambda m(x) - p(x) + \\left(E^{*}\\left[R^{-1}(H(u(m),m)-y)\\right]\\right)(x).\n$$\nExpanding the term with $E^{*}$ gives the final expression. Let the vector $\\mathbf{z} = R^{-1}(H(u(m),m)-y) \\in \\mathbb{R}^K$. Then,\n$$\ng(x) = \\lambda m(x) - p(x) + \\sum_{i=1}^{K} z_{i} e_{i}(x).\n$$\nThis expression provides the gradient $g(x)$ in terms of the parameter $m$, the state $u$ (via $H$), and the adjoint state $p$, as required.", "answer": "$$\n\\boxed{\ng(x) = \\lambda m(x) - p(x) + \\sum_{i=1}^{K} \\left[R^{-1}\\left(H(u(m),m) - y\\right)\\right]_{i} e_{i}(x)\n}\n$$", "id": "3419121"}, {"introduction": "Calculating the gradient of a cost function is only the first step; a deeper understanding comes from analyzing the sensitivity operator itself. The Fréchet derivative, represented by the Jacobian matrix in a discretized setting, holds the key to understanding which aspects of a model's parameters can actually be determined from the available data. This hands-on practice ([@problem_id:3419154]) guides you through deriving this Jacobian and its adjoint, and then uses it to explore the concept of the identifiability nullspace—the space of parameter changes that are invisible to the observation network.", "problem": "Consider a one-dimensional steady-state diffusion model on the closed interval $[0,1]$ with homogeneous Dirichlet boundary conditions. Let the interior of the interval be discretized into $n$ equally spaced points with spacing $h = 1/(n+1)$, and approximate the differential operator mapping $x$ to $-x''$ by the standard second-order finite-difference matrix $A \\in \\mathbb{R}^{n \\times n}$ defined by $A_{ii} = 2/h^2$ and $A_{i,i\\pm 1} = -1/h^2$ for valid indices. Let the parameter vector $m \\in \\mathbb{R}^n$ represent a discretized source term, and let the state vector $x \\in \\mathbb{R}^n$ solve the linear system\n$$\nA x = m.\n$$\nLet $S \\in \\mathbb{R}^{k \\times n}$ be a linear observation operator that selects a subset of coordinates of $x$ corresponding to $k$ sensor locations among the $n$ interior grid points. The forward map $F : \\mathbb{R}^n \\to \\mathbb{R}^k$ is defined by\n$$\nF(m) = S x(m) = S A^{-1} m.\n$$\nDefine the data misfit cost functional\n$$\n\\Phi(m) = \\tfrac{1}{2} \\| F(m) - d \\|_2^2,\n$$\nwhere $d \\in \\mathbb{R}^k$ is a given data vector and $\\|\\cdot\\|_2$ denotes the Euclidean norm. All quantities in this problem are dimensionless; no physical units are required.\n\nYour tasks are:\n\n1. Starting from the definition of the Fréchet derivative in finite-dimensional real Hilbert spaces, derive the expression for the Fréchet derivative $F'(m)[h]$ of the forward map $F$ at $m$ in the direction $h \\in \\mathbb{R}^n$. Express your derivation directly in terms of the linearized state equation $A x_h = h$ for the perturbation $x_h$ and the observation operator $S$.\n\n2. Using the adjoint-state method via the Lagrangian framework, derive the gradient of $\\Phi(m)$ with respect to $m$. Introduce an adjoint variable $\\lambda \\in \\mathbb{R}^n$, set up the Lagrangian\n$$\n\\mathcal{L}(x,m,\\lambda) = \\tfrac{1}{2} \\| S x - d \\|_2^2 + \\lambda^\\top (A x - m),\n$$\nand derive the adjoint equation and the final expression for $\\nabla \\Phi(m)$ in terms of $\\lambda$. Carefully justify each step from first principles and specify the boundary conditions encoded by the matrix $A$.\n\n3. Provide a precise characterization of the identifiability nullspace\n$$\n\\mathcal{N} = \\{ h \\in \\mathbb{R}^n : F'(m)[h] = 0 \\}.\n$$\nShow that $\\mathcal{N}$ is independent of $m$ and equals the nullspace of the matrix $S A^{-1}$. Prove the following constructive characterization: if there exists a vector $v \\in \\mathbb{R}^n$ that is zero at all sensor locations (i.e., $S v = 0$), then the perturbation $h = A v$ belongs to $\\mathcal{N}$. Explain how this relates to identifiability and the role of the adjoint operator in detecting unobservable directions.\n\n4. Design an algorithm to compute the dimension of the identifiability nullspace $\\dim(\\mathcal{N})$ by evaluating the numerical rank of the Jacobian matrix $J = S A^{-1}$ via singular value decomposition. Justify a thresholding rule of the form: count singular values strictly greater than $10^{-10}$ times the largest singular value as contributing to the rank, and define the nullspace dimension as $n$ minus this numerical rank.\n\n5. Implement a complete, runnable program that:\n   - Constructs the matrix $A$ for a given $n$ and the selection matrix $S$ for specified sensor indices.\n   - Computes the Jacobian matrix $J = S A^{-1}$ and its singular values.\n   - Computes the nullspace dimension via the thresholding rule described above.\n   - Demonstrates the constructive characterization from task 3 by building a vector $v$ that is identically zero at sensor locations and forming $h = A v$, and then computing $\\| J h \\|_2$.\n   - Aggregates results for a test suite and prints them in the exact specified format.\n\nThe test suite must cover the following cases:\n\n- Case 1 (Happy path): $n = 8$, sensors at all interior grid points. This corresponds to $k = n$ and $S$ equal to the identity matrix. Output the integer $\\dim(\\mathcal{N})$.\n- Case 2 (No data edge case): $n = 8$, no sensors selected. This corresponds to $k = 0$ and $S$ equal to the zero-row matrix. Output the integer $\\dim(\\mathcal{N})$.\n- Case 3 (Partial coverage and constructive nullspace): $n = 10$, sensors at indices $\\{2,5,8\\}$ (zero-based indexing among interior points). Construct $v \\in \\mathbb{R}^{10}$ such that $v_i = 0$ for the sensor indices and $v_i$ is nonzero elsewhere, set $h = A v$, and output the float $\\| J h \\|_2$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, namely $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$, where $\\text{result}_1$ and $\\text{result}_2$ are integers and $\\text{result}_3$ is a float. No additional text should be printed.", "solution": "The problem statement is a valid exercise in the theory and computation of sensitivities for an inverse problem governed by a linear elliptic partial differential equation. It is scientifically sound, mathematically well-posed, and all necessary information for its resolution is provided.\n\nThe problem asks for a multi-part analysis of a one-dimensional steady-state diffusion problem from an inverse problems perspective. We will address each task in sequence.\n\nThe discretized state equation is given by the linear system $A x = m$, where $x, m \\in \\mathbb{R}^n$. The matrix $A \\in \\mathbb{R}^{n \\times n}$ is the finite-difference approximation of the negative second derivative operator $-d^2/ds^2$ on an interval $[0,1]$ with homogeneous Dirichlet boundary conditions. This matrix is symmetric and positive definite, and thus invertible. The forward map $F(m)$ from the source parameter $m$ to the observations is $F(m) = S A^{-1} m$, where $S \\in \\mathbb{R}^{k \\times n}$ is a linear observation operator.\n\n### 1. Fréchet Derivative of the Forward Map\n\nThe Fréchet derivative of a map $F$ at a point $m$ is a linear operator $F'(m)$ such that\n$$ F(m+h) = F(m) + F'(m)[h] + o(\\|h\\|) $$\nfor a small perturbation $h \\in \\mathbb{R}^n$.\n\nThe forward map is given by $F(m) = S A^{-1} m$. Since $A^{-1}$ and $S$ are constant matrices, the map $F$ is linear in $m$. For a linear operator $L$, its derivative is the operator itself, i.e., $L'(m)[h] = L(h)$. Therefore,\n$$ F'(m)[h] = S A^{-1} h. $$\nTo express this in terms of the linearized state equation as requested, let us consider the effect of the perturbation $h$ on the state $x$. The original state $x(m)$ satisfies $A x(m) = m$. The perturbed state $x(m+h)$ satisfies $A x(m+h) = m+h$.\n\nLet $x_h = x(m+h) - x(m)$ be the perturbation in the state. By the linearity of the matrix-vector product,\n$$ A x_h = A(x(m+h) - x(m)) = A x(m+h) - A x(m) = (m+h) - m = h. $$\nThis gives the linearized state equation for the state perturbation $x_h$:\n$$ A x_h = h. $$\nSince $A$ is invertible, $x_h = A^{-1} h$.\n\nNow, we examine the perturbation in the observation:\n$$ F(m+h) - F(m) = S x(m+h) - S x(m) = S(x(m+h) - x(m)) = S x_h. $$\nSubstituting the expression for $x_h$, we have\n$$ F(m+h) - F(m) = S (A^{-1} h). $$\nThe term $o(\\|h\\|)$ is zero because the relationship is exactly linear. Thus, the Fréchet derivative of $F$ at $m$ applied to the direction $h$ is:\n$$ F'(m)[h] = S x_h, \\quad \\text{where} \\quad A x_h = h. $$\nThis expression shows that the sensitivity of the observation to a perturbation $h$ in the source is given by observing the resulting state perturbation $x_h$ with the operator $S$. Notably, because $F$ is linear, its derivative $F'(m)[\\cdot] = SA^{-1}(\\cdot)$ is a constant operator, independent of the point $m$ at which it is evaluated.\n\n### 2. Gradient of the Cost Functional via the Adjoint-State Method\n\nWe seek the gradient of the cost functional $\\Phi(m) = \\frac{1}{2} \\| S x - d \\|_2^2$ subject to the state equation constraint $A x = m$. We use the Lagrangian framework. The Lagrangian is given as:\n$$ \\mathcal{L}(x, m, \\lambda) = \\tfrac{1}{2} \\| S x - d \\|_2^2 + \\lambda^\\top (A x - m), $$\nwhere $\\lambda \\in \\mathbb{R}^n$ is the adjoint state or Lagrange multiplier.\n\nTo find the gradient of the reduced functional $\\Phi(m)$, we find the stationary point of $\\mathcal{L}$ with respect to its arguments. The gradient $\\nabla \\Phi(m)$ is then given by the partial derivative of $\\mathcal{L}$ with respect to $m$, evaluated at the solution of the state and adjoint equations.\n\n1.  **Derivative with respect to $\\lambda$**: Taking the derivative of $\\mathcal{L}$ with respect to $\\lambda$ and setting it to zero recovers the state equation.\n    $$ \\nabla_\\lambda \\mathcal{L}(x, m, \\lambda) = A x - m = 0 \\implies A x = m. $$\n\n2.  **Derivative with respect to $x$**: We compute the directional derivative of $\\mathcal{L}$ with respect to $x$ in an arbitrary direction $\\delta x$.\n    $$ \\delta_x \\mathcal{L} = \\frac{d}{d\\epsilon} \\mathcal{L}(x+\\epsilon\\delta x, m, \\lambda) \\Big|_{\\epsilon=0}. $$\n    $$ \\delta_x \\mathcal{L} = \\frac{d}{d\\epsilon} \\left( \\tfrac{1}{2} (S(x+\\epsilon\\delta x) - d)^\\top (S(x+\\epsilon\\delta x) - d) + \\lambda^\\top (A(x+\\epsilon\\delta x) - m) \\right) \\Big|_{\\epsilon=0} $$\n    $$ = (S x - d)^\\top S \\delta x + \\lambda^\\top A \\delta x = (S^\\top(S x - d) + A^\\top \\lambda)^\\top \\delta x. $$\n    For this to be zero for all $\\delta x$, we must have $\\nabla_x \\mathcal{L} = S^\\top(S x - d) + A^\\top \\lambda = 0$. This gives the **adjoint equation**:\n    $$ A^\\top \\lambda = -S^\\top(S x - d). $$\n    The matrix $A$ as defined is symmetric ($A_{ij} = A_{ji}$), so $A^\\top = A$. The adjoint equation simplifies to:\n    $$ A \\lambda = -S^\\top(S x - d). $$\n    The structure of the matrix $A$ implies homogeneous Dirichlet boundary conditions for the underlying continuous problem. Since the adjoint operator is again $A$, the adjoint variable $\\lambda$ satisfies the same boundary conditions.\n\n3.  **Derivative with respect to $m$**: We compute the directional derivative of $\\mathcal{L}$ with respect to $m$ in an arbitrary direction $\\delta m$.\n    $$ \\delta_m \\mathcal{L} = \\frac{d}{d\\epsilon} \\mathcal{L}(x, m+\\epsilon\\delta m, \\lambda) \\Big|_{\\epsilon=0}. $$\n    $$ \\delta_m \\mathcal{L} = \\frac{d}{d\\epsilon} \\left( \\tfrac{1}{2} \\| S x - d \\|_2^2 + \\lambda^\\top (A x - (m+\\epsilon\\delta m)) \\right) \\Big|_{\\epsilon=0} = -\\lambda^\\top \\delta m. $$\n    The gradient of the reduced functional $\\Phi(m)$ is given by $\\nabla_m \\mathcal{L}$, which is the vector that satisfies $(\\nabla_m \\mathcal{L})^\\top \\delta m = -\\lambda^\\top \\delta m$ for all $\\delta m$.\n    Thus, the gradient is:\n    $$ \\nabla \\Phi(m) = -\\lambda. $$\n\nIn summary, the gradient of $\\Phi(m)$ is computed via the following three steps:\n(i) Solve the state equation for $x$: $A x = m$.\n(ii) Solve the adjoint equation for $\\lambda$: $A \\lambda = -S^\\top(S x - d)$.\n(iii) The gradient is $\\nabla \\Phi(m) = -\\lambda$.\n\n### 3. Identifiability Nullspace\n\nThe identifiability nullspace $\\mathcal{N}$ is the set of parameter perturbations $h$ that are unobservable, i.e., produce no change in the output of the forward map. It is defined as:\n$$ \\mathcal{N} = \\{ h \\in \\mathbb{R}^n : F'(m)[h] = 0 \\}. $$\nFrom Task 1, we have $F'(m)[h] = S A^{-1} h$. The condition $F'(m)[h] = 0$ is therefore $S A^{-1} h = 0$. This is the definition of the nullspace (or kernel) of the matrix $J = S A^{-1}$.\n$$ \\mathcal{N} = \\text{ker}(S A^{-1}). $$\nSince the matrices $S$ and $A$ are constant, the Jacobian matrix $J = S A^{-1}$ is also constant. It does not depend on the point $m$. Consequently, its nullspace $\\mathcal{N}$ is independent of $m$.\n\n**Constructive Characterization:**\nWe are asked to prove that if a vector $v \\in \\mathbb{R}^n$ exists such that $S v = 0$, then the perturbation $h = A v$ belongs to $\\mathcal{N}$.\nA vector $v$ with $S v = 0$ corresponds to a state that is zero at all sensor locations.\nLet's test if $h = A v$ is in the nullspace $\\mathcal{N}$ by applying the operator $J = S A^{-1}$ to it:\n$$ J h = (S A^{-1}) h = (S A^{-1}) (A v) = S (A^{-1} A) v = S I v = S v. $$\nBy our premise, $S v = 0$. Therefore, $J h = 0$, which means $h \\in \\text{ker}(J) = \\mathcal{N}$. This completes the proof.\n\nThis characterization provides a way to construct elements of the nullspace. Any source term $h$ that generates a state $v$ that is \"invisible\" to the sensors ($S v = 0$) is itself an unobservable parameter perturbation. An optimization algorithm based on the gradient will not be able to recover or correct components of the parameter $m$ that lie in this nullspace $\\mathcal{N}$. This is because the gradient $\\nabla \\Phi(m) = -\\lambda = A^{-1} S^\\top (S x - d)$ belongs to the range of the adjoint operator $J^\\top = (S A^{-1})^\\top = (A^{-1})^\\top S^\\top = A^{-1} S^\\top$. By the fundamental theorem of linear algebra, the range of the adjoint is the orthogonal complement of the nullspace of the original operator: $\\text{range}(J^\\top) \\perp \\text{ker}(J)$. Hence, gradient-based updates are always orthogonal to the unobservable directions.\n\n### 4. Algorithm for Computing Nullspace Dimension\n\nThe dimension of the identifiability nullspace, $\\dim(\\mathcal{N})$, is the nullity of the Jacobian matrix $J = S A^{-1}$. By the rank-nullity theorem for a matrix $J \\in \\mathbb{R}^{k \\times n}$:\n$$ \\text{rank}(J) + \\text{nullity}(J) = n. $$\nTherefore, $\\dim(\\mathcal{N}) = \\text{nullity}(J) = n - \\text{rank}(J)$.\n\nThe rank of a matrix can be computed reliably from its singular values. The rank is the number of non-zero singular values. In numerical computation, we must use a threshold to distinguish non-zero from zero singular values due to floating-point inaccuracies. The problem provides a specific thresholding rule.\n\nThe algorithm is as follows:\n1.  **Construct Matrices**: For a given grid size $n$ and a set of sensor locations, construct the finite-difference matrix $A \\in \\mathbb{R}^{n \\times n}$ and the observation matrix $S \\in \\mathbb{R}^{k \\times n}$.\n2.  **Compute Jacobian**: Compute the Jacobian matrix $J = S A^{-1}$. Numerically, this can be done by first computing $A^{-1}$ using `numpy.linalg.inv` and then multiplying by $S$.\n3.  **Compute Singular Values**: Compute the singular value decomposition (SVD) of $J$. Let the singular values be $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_p \\ge 0$, where $p = \\min(k, n)$.\n4.  **Determine Numerical Rank**: Find the largest singular value $\\sigma_1$. Apply the thresholding rule: the numerical rank, $\\text{rank}_{\\text{num}}(J)$, is the count of singular values $\\sigma_i$ such that $\\sigma_i > 10^{-10} \\sigma_1$. If $J$ has no non-zero singular values (e.g., if it is a zero matrix), its rank is $0$.\n5.  **Compute Nullspace Dimension**: The dimension of the nullspace is $\\dim(\\mathcal{N}) = n - \\text{rank}_{\\text{num}}(J)$.\n\n### 5. Implementation\nThe final step is to implement this algorithm in Python for the specified test cases. The code will construct the matrices, compute the Jacobian, its singular values, and then determine the nullspace dimension or test the constructive characterization as required by each case.", "answer": "```python\nimport numpy as np\n\ndef construct_A(n):\n    \"\"\"Constructs the finite-difference matrix A for a given n.\"\"\"\n    if n == 0:\n        return np.array([[]])\n    h = 1.0 / (n + 1)\n    h2_inv = 1.0 / (h * h)\n    \n    A = np.zeros((n, n))\n    \n    # Fill diagonal\n    np.fill_diagonal(A, 2.0 * h2_inv)\n    \n    # Fill off-diagonals\n    if n > 1:\n        diag_indices = np.arange(n - 1)\n        A[diag_indices, diag_indices + 1] = -1.0 * h2_inv\n        A[diag_indices + 1, diag_indices] = -1.0 * h2_inv\n        \n    return A\n\ndef construct_S(n, sensor_indices):\n    \"\"\"Constructs the observation matrix S for given n and sensor indices.\"\"\"\n    k = len(sensor_indices)\n    if k == 0:\n        return np.zeros((0, n))\n    \n    S = np.zeros((k, n))\n    for i, sensor_idx in enumerate(sensor_indices):\n        if 0 <= sensor_idx < n:\n            S[i, sensor_idx] = 1.0\n    return S\n\ndef get_nullspace_dim(n, sensor_indices):\n    \"\"\"Computes the dimension of the identifiability nullspace.\"\"\"\n    A = construct_A(n)\n    S = construct_S(n, sensor_indices)\n\n    if n == 0:\n        return 0\n    if S.shape[0] == 0: # No sensors\n        return n\n    \n    A_inv = np.linalg.inv(A)\n    J = S @ A_inv\n    \n    # singular values of J\n    singular_values = np.linalg.svd(J, compute_uv=False)\n    \n    if len(singular_values) == 0:\n        numerical_rank = 0\n    else:\n        sigma_max = np.max(singular_values)\n        if sigma_max == 0:\n            numerical_rank = 0\n        else:\n            threshold = 1e-10 * sigma_max\n            numerical_rank = np.sum(singular_values > threshold)\n\n    nullity = n - numerical_rank\n    return nullity\n\ndef solve():\n    \"\"\"\n    Solves the problem for the three specified test cases and prints the results.\n    \"\"\"\n    results = []\n\n    # Case 1: n = 8, sensors at all points\n    n1 = 8\n    sensor_indices1 = list(range(n1))\n    dim_N1 = get_nullspace_dim(n1, sensor_indices1)\n    results.append(int(dim_N1))\n\n    # Case 2: n = 8, no sensors\n    n2 = 8\n    sensor_indices2 = []\n    dim_N2 = get_nullspace_dim(n2, sensor_indices2)\n    results.append(int(dim_N2))\n\n    # Case 3: n = 10, sensors at {2, 5, 8}, constructive nullspace check\n    n3 = 10\n    sensor_indices3 = [2, 5, 8]\n    \n    A3 = construct_A(n3)\n    S3 = construct_S(n3, sensor_indices3)\n    \n    A3_inv = np.linalg.inv(A3)\n    J3 = S3 @ A3_inv\n    \n    # Construct vector v that is zero at sensor locations\n    v3 = np.ones(n3)\n    v3[sensor_indices3] = 0\n    \n    # Construct h = Av\n    h3 = A3 @ v3\n    \n    # Compute the norm ||Jh||_2\n    norm_Jh = np.linalg.norm(J3 @ h3)\n    results.append(float(norm_Jh))\n\n    # Print a single line of output in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3419154"}, {"introduction": "While steady-state models provide a clean setting for learning adjoints, many critical applications, from weather prediction to reservoir simulation, involve time-dependent systems. The adjoint method for these problems presents a major computational hurdle: the need to store the entire history of the model's state for use in the backward-in-time adjoint integration. This practice ([@problem_id:3419136]) addresses this challenge directly, introducing the indispensable technique of checkpointing to balance computational cost against memory limitations, making large-scale time-dependent data assimilation feasible.", "problem": "Consider the discrete-time, time-dependent linear state evolution model defined for a state vector $x_n \\in \\mathbb{R}^d$ with index $n \\in \\{0,1,\\dots,N\\}$ by the explicit Euler time-stepping rule\n$$\nx_{n+1} = x_n + \\Delta t \\left( A x_n + \\theta B x_n + s \\right),\n$$\nwhere $A \\in \\mathbb{R}^{d \\times d}$ and $B \\in \\mathbb{R}^{d \\times d}$ are fixed matrices, $s \\in \\mathbb{R}^d$ is a fixed source term, $\\Delta t > 0$ is a fixed time step, and $\\theta \\in \\mathbb{R}$ is a scalar parameter. The initial condition is $x_0 = 0$. Let the observation operator be $C \\in \\mathbb{R}^{d \\times d}$, and let observations $y_n \\in \\mathbb{R}^d$ for $n \\in \\{0,1,\\dots,N\\}$ be generated by the same model with a fixed \"true\" parameter $\\theta_{\\mathrm{true}}$ and the same initial condition, with no noise:\n$$\ny_n = C x_n(\\theta_{\\mathrm{true}}).\n$$\nDefine the data misfit at each time as\n$$\n\\ell_n(x_n) = \\tfrac{1}{2} \\left\\| C x_n - y_n \\right\\|_2^2,\n$$\nand the total objective function as\n$$\nJ(\\theta) = \\sum_{n=0}^{N} \\ell_n(x_n(\\theta)) + \\tfrac{\\alpha}{2} \\theta^2,\n$$\nwhere $\\alpha \\ge 0$ is a fixed regularization weight.\n\nYou will compute the sensitivity of $J$ with respect to $\\theta$ via the adjoint-state method in a memory-constrained setting using checkpointing, and you will verify the correctness against a finite-difference approximation.\n\nFoundational base definitions and assumptions for derivation:\n- The Fréchet derivative of a functional $\\Phi$ at an argument $u$ in direction $h$ is the linear map $D\\Phi(u)[h]$ satisfying $\\Phi(u+h) - \\Phi(u) = D\\Phi(u)[h] + o(\\|h\\|)$ as $\\|h\\|\\to 0$.\n- For the discrete-time map $F_\\theta(x) = x + \\Delta t \\left( A x + \\theta B x + s \\right)$, the Jacobian with respect to the state is $\\partial F_\\theta / \\partial x = I + \\Delta t (A + \\theta B)$, and the partial derivative with respect to the parameter is $\\partial F_\\theta / \\partial \\theta (x) = \\Delta t\\, B x$.\n- The adjoint recursion arises by enforcing the stationarity of the Lagrangian formed by the objective $J$ and the discrete dynamics constraints, introducing adjoint variables $\\lambda_n \\in \\mathbb{R}^d$.\n\nCheckpointing and memory:\n- When running the adjoint backward in time from $n = N$ to $n = 0$, one needs access to the forward states $x_n$ to evaluate the gradient contributions and the adjoint updates. Storing all $x_n$ may be infeasible when $N$ is large.\n- A checkpointing policy stores a selected subset of states $\\{x_{n_k}\\}$ at indices $\\{n_k\\}$ subject to a memory limit $\\mathsf{M}$ (the maximum number of stored states). When a needed state $x_n$ is not stored, it is recomputed on demand by re-running the forward model from the nearest preceding stored checkpoint index $n_k \\le n$ up to $n$. The number of extra forward steps performed beyond the original forward pass is tracked as the recomputation count.\n\nTasks:\n1. Derive, from the provided base definitions, the discrete adjoint recursion for $\\lambda_n$ and the expression for the gradient $\\mathrm{d}J/\\mathrm{d}\\theta$ in terms of the forward states, adjoint variables, and the model derivatives.\n2. Implement a program that:\n   - Generates observations $y_n$ by running the forward model at $\\theta_{\\mathrm{true}}$.\n   - Computes $\\mathrm{d}J/\\mathrm{d}\\theta$ using the adjoint-state method under a uniform checkpointing policy that stores $\\mathsf{M}$ states at evenly spaced time indices from $0$ to $N$, inclusive, subject to deduplication when spacing is not exact. When a needed $x_n$ is not stored, recompute it from the nearest preceding checkpoint.\n   - Counts the total number of additional forward steps performed during adjoint-time recomputation beyond the initial forward run of $N$ steps.\n   - Computes a central finite-difference approximation to $\\mathrm{d}J/\\mathrm{d}\\theta$ using a small step $\\varepsilon$:\n     $$\n     \\mathrm{d}J/\\mathrm{d}\\theta \\approx \\frac{J(\\theta+\\varepsilon) - J(\\theta-\\varepsilon)}{2\\varepsilon}.\n     $$\n   - Reports the absolute error between the adjoint-based gradient and the finite-difference gradient, the recomputation count, the number of stored states, and a boolean indicating whether the absolute error is below a specified tolerance.\n3. Use the following fixed model settings:\n   - Dimension $d = 3$.\n   - Time step $\\Delta t = 0.1$.\n   - Matrices\n     $$\n     A = \\begin{bmatrix}\n     -0.1 & 0.2 & 0 \\\\\n     -0.2 & -0.3 & 0.1 \\\\\n     0 & -0.1 & -0.2\n     \\end{bmatrix},\\quad\n     B = \\begin{bmatrix}\n     0.5 & 0 & 0 \\\\\n     0 & 0.1 & 0 \\\\\n     0 & 0 & 0.3\n     \\end{bmatrix},\\quad\n     C = I_3,\n     $$\n     and source\n     $$\n     s = \\begin{bmatrix} 0.1 \\\\ -0.05 \\\\ 0.2 \\end{bmatrix}.\n     $$\n   - Initial condition $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n   - True parameter $\\theta_{\\mathrm{true}} = 0.3$.\n4. Implement the following test suite (each test case provides $N$, $\\mathsf{M}$, $\\theta$, $\\alpha$, and $\\varepsilon$):\n   - Test Case 1 (general case): $N = 50$, $\\mathsf{M} = 6$, $\\theta = 0.2$, $\\alpha = 0.01$, $\\varepsilon = 10^{-6}$.\n   - Test Case 2 (full-memory boundary): $N = 50$, $\\mathsf{M} = 51$, $\\theta = 0.2$, $\\alpha = 0.01$, $\\varepsilon = 10^{-6}$.\n   - Test Case 3 (minimal memory edge case): $N = 50$, $\\mathsf{M} = 1$, $\\theta = 0.2$, $\\alpha = 0.01$, $\\varepsilon = 10^{-6}$.\n   - Test Case 4 (short horizon boundary): $N = 1$, $\\mathsf{M} = 1$, $\\theta = 0.2$, $\\alpha = 0.01$, $\\varepsilon = 10^{-8}$.\n5. For each test case, your program must produce a result as a list in the format\n   $$\n   [g_{\\mathrm{adj}}, g_{\\mathrm{fd}}, \\lvert g_{\\mathrm{adj}} - g_{\\mathrm{fd}} \\rvert, \\text{recompute\\_count}, \\text{memory\\_used}, \\text{match}],\n   $$\n   where $g_{\\mathrm{adj}}$ is the adjoint-based gradient, $g_{\\mathrm{fd}}$ is the finite-difference gradient, $\\text{recompute\\_count}$ is an integer, $\\text{memory\\_used}$ is an integer number of stored states, and $\\text{match}$ is a boolean indicating whether the absolute error is less than $10^{-8}$ for Test Cases 1–3 and less than $10^{-10}$ for Test Case 4.\n6. Final output format requirement: Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, with each test case result itself formatted as the list specified in item 5. For example, a valid shape is\n   $$\n   [[\\cdots],[\\cdots],[\\cdots],[\\cdots]].\n   $$\n\nNo physical units or angle units are involved in this problem; all quantities are unitless real numbers. The output values must be floating-point numbers, integers, or booleans as specified.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the established theory of optimal control and sensitivity analysis (specifically, the adjoint-state method), is well-posed with a clear and objective formulation, and provides all necessary data and definitions for a unique solution.\n\n### 1. Derivation of the Adjoint Equations and Gradient\n\nThe goal is to compute the gradient $\\mathrm{d}J/\\mathrm{d}\\theta$ of the objective function:\n$$\nJ(\\theta) = \\sum_{n=0}^{N} \\ell_n(x_n(\\theta)) + \\tfrac{\\alpha}{2} \\theta^2 = \\sum_{n=0}^{N} \\tfrac{1}{2} \\left\\| C x_n(\\theta) - y_n \\right\\|_2^2 + \\tfrac{\\alpha}{2} \\theta^2\n$$\nThe state vector $x_n$ evolves according to the discrete-time dynamics, which act as constraints on the optimization problem:\n$$\nx_{n+1} = F_\\theta(x_n) = x_n + \\Delta t \\left( A x_n + \\theta B x_n + s \\right) \\quad \\text{for } n = 0, \\dots, N-1\n$$\nwith initial condition $x_0=0$.\n\nWe use the method of Lagrange multipliers to derive the adjoint equations. The Lagrangian $\\mathcal{L}$ is constructed by augmenting the objective function with the constraints, weighted by the adjoint variables (Lagrange multipliers) $\\lambda_{n+1} \\in \\mathbb{R}^d$:\n$$\n\\mathcal{L}(x_1, \\dots, x_N, \\theta, \\lambda_1, \\dots, \\lambda_N) = J(\\theta) - \\sum_{n=0}^{N-1} \\lambda_{n+1}^T \\left( x_{n+1} - F_\\theta(x_n) \\right)\n$$\nSubstituting the expression for $J(\\theta)$ and noting that $x_0=0$ is a fixed condition:\n$$\n\\mathcal{L} = \\left(\\sum_{n=0}^{N} \\ell_n(x_n)\\right) + \\tfrac{\\alpha}{2} \\theta^2 - \\sum_{n=0}^{N-1} \\lambda_{n+1}^T \\left( x_{n+1} - F_\\theta(x_n) \\right)\n$$\nFor a trajectory $\\{x_n\\}$ that satisfies the state equations, the gradient $\\mathrm{d}J/\\mathrm{d}\\theta$ is the partial derivative of the Lagrangian with respect to $\\theta$, provided the adjoint variables satisfy the stationarity conditions $\\partial \\mathcal{L} / \\partial x_n = 0$ for each state variable $x_n$ ($n=1, \\dots, N$).\n\n**Adjoint Equations:**\nThe stationarity conditions define the adjoint recursion.\nFor the final state $x_N$ ($n=N$):\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_N} = \\nabla_{x_N} \\ell_N(x_N) - \\lambda_N^T = 0 \\implies \\lambda_N = (\\nabla_{x_N} \\ell_N(x_N))^T\n$$\nFor intermediate states $x_n$ ($n=1, \\dots, N-1$):\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_n} = \\nabla_{x_n} \\ell_n(x_n) - \\lambda_n^T + \\lambda_{n+1}^T \\frac{\\partial F_\\theta(x_n)}{\\partial x_n} = 0\n$$\nThis yields the backward recursion for the adjoint variables:\n$$\n\\lambda_n = \\left(\\frac{\\partial F_\\theta(x_n)}{\\partial x_n}\\right)^T \\lambda_{n+1} + (\\nabla_{x_n} \\ell_n(x_n))^T\n$$\nLet's identify the required derivatives:\n- The gradient of the misfit term $\\ell_n(x_n) = \\frac{1}{2}(Cx_n-y_n)^T(Cx_n-y_n)$ is $\\nabla_{x_n}\\ell_n(x_n) = (Cx_n-y_n)^T C$. Its transpose is $(\\nabla_{x_n}\\ell_n(x_n))^T = C^T(Cx_n-y_n)$.\n- The Jacobian of the state transition map $F_\\theta$ with respect to the state is $\\frac{\\partial F_\\theta(x_n)}{\\partial x_n} = I + \\Delta t(A + \\theta B)$. Let's denote this matrix as $M_\\theta$.\n\nThe full adjoint system is defined by a backward time evolution:\nTerminal condition at $n=N$:\n$$\n\\lambda_N = C^T (C x_N - y_N)\n$$\nAdjoint recursion for $n = N-1, \\dots, 0$:\n$$\n\\lambda_n = (I + \\Delta t(A + \\theta B))^T \\lambda_{n+1} + C^T (C x_n - y_n)\n$$\nNote that we include the recursion down to $n=0$, providing $\\lambda_0$, which represents the sensitivity of $J$ to the initial condition $x_0$. While not strictly needed for the parameter gradient, this defines the complete adjoint state trajectory.\n\n**Gradient Expression:**\nThe gradient $\\mathrm{d}J/\\mathrm{d}\\theta$ is found by differentiating the Lagrangian with respect to $\\theta$:\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta} = \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{\\partial}{\\partial\\theta} \\left( \\tfrac{\\alpha}{2}\\theta^2 \\right) + \\sum_{n=0}^{N-1} \\lambda_{n+1}^T \\frac{\\partial F_\\theta(x_n)}{\\partial \\theta}\n$$\nThe partial derivative of $F_\\theta(x_n)$ with respect to $\\theta$ is:\n$$\n\\frac{\\partial F_\\theta(x_n)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left( x_n + \\Delta t (Ax_n + \\theta Bx_n + s) \\right) = \\Delta t B x_n\n$$\nSubstituting this into the gradient expression gives the final formula for the adjoint-based gradient:\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta} = \\alpha \\theta + \\Delta t \\sum_{n=0}^{N-1} \\lambda_{n+1}^T B x_n\n$$\n\n### 2. Computational Algorithm\n\nThe computation proceeds as follows:\n1.  **Generate Observations:** The \"true\" observations $y_n$ are generated by running the forward model with the parameter $\\theta_{\\mathrm{true}}$.\n    - Set $x_0 = 0$.\n    - For $n=0, \\dots, N-1$: $x_{n+1} = (I + \\Delta t(A+\\theta_{\\mathrm{true}} B))x_n + \\Delta t s$.\n    - For $n=0, \\dots, N$: $y_n = C x_n$.\n\n2.  **Finite-Difference Gradient:** To verify correctness, a central finite-difference approximation $g_{\\mathrm{fd}}$ is calculated:\n    - Define a function `compute_J(theta_eval)` that runs the forward model with `theta_eval` to get the trajectory $\\{x_n(\\theta_{\\mathrm{eval}})\\}$ and then computes $J(\\theta_{\\mathrm{eval}})$.\n    - Compute $g_{\\mathrm{fd}} = \\frac{J(\\theta+\\varepsilon) - J(\\theta-\\varepsilon)}{2\\varepsilon}$.\n\n3.  **Adjoint-State Gradient with Checkpointing:**\n    a.  **Forward Pass and Checkpointing:**\n        - Given parameters $N$ and $\\mathsf{M}$, determine the checkpoint indices. A uniform policy stores states at indices `k` given by `np.linspace(0, N, M, dtype=int)`, with duplicates removed. Let this set be $\\mathcal{K}$.\n        - Run the forward model from $n=0$ to $N$ for a given $\\theta$. Store the state vector $x_n$ in memory if and only if $n \\in \\mathcal{K}$. This constitutes the initial forward pass of $N$ steps.\n\n    b.  **Backward Pass and Gradient Assembly:**\n        - Initialize the gradient $g_{\\mathrm{adj}} = \\alpha \\theta$ and the recomputation step counter to $0$.\n        - To start the recursion, obtain $x_N$. If $N \\in \\mathcal{K}$, retrieve it from memory. Otherwise, find the largest checkpoint index $n_k \\in \\mathcal{K}$ such that $n_k < N$, retrieve $x_{n_k}$, and re-run the forward model from $n_k$ to $N$. The number of recomputation steps, $N-n_k$, is added to the count.\n        - Initialize the adjoint variable: $\\lambda_N = C^T(Cx_N - y_N)$.\n        - Iterate backwards from $n=N$ down to $1$:\n            i. At step $n$, we have the adjoint state $\\lambda_n$.\n            ii. Obtain the state $x_{n-1}$. If $n-1 \\notin \\mathcal{K}$, find the nearest preceding checkpoint $n_k \\le n-1$, retrieve $x_{n_k}$, and re-run the forward model for $(n-1) - n_k$ steps. Add this number to the recomputation counter.\n            iii. Update the gradient: $g_{\\mathrm{adj}} \\leftarrow g_{\\mathrm{adj}} + \\Delta t \\lambda_n^T B x_{n-1}$.\n            iv. Update the adjoint variable for the next iteration: $\\lambda_{n-1} = (I + \\Delta t(A+\\theta B))^T \\lambda_n + C^T(Cx_{n-1} - y_{n-1})$.\n        - The final value $g_{\\mathrm{adj}}$ is the desired gradient.\n\n    c.  **Output:** Report the computed gradients $g_{\\mathrm{adj}}$ and $g_{\\mathrm{fd}}$, their absolute difference, the total recomputation count, the number of stored states (`memory_used`), and a boolean indicating if the error is below the specified tolerance.\n\nThis procedure correctly implements the adjoint-state method under memory constraints, using checkpointing to manage the storage of the forward trajectory required during the backward pass.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases for the adjoint-state method problem.\n    \"\"\"\n\n    # Fixed model settings\n    d = 3\n    dt = 0.1\n    A = np.array([\n        [-0.1, 0.2, 0.0],\n        [-0.2, -0.3, 0.1],\n        [0.0, -0.1, -0.2]\n    ])\n    B = np.array([\n        [0.5, 0.0, 0.0],\n        [0.0, 0.1, 0.0],\n        [0.0, 0.0, 0.3]\n    ])\n    C = np.eye(d)\n    s = np.array([0.1, -0.05, 0.2])\n    x0 = np.zeros(d)\n    theta_true = 0.3\n\n    # Test suite\n    test_cases = [\n        # (N, M, theta, alpha, epsilon, tol)\n        (50, 6, 0.2, 0.01, 1e-6, 1e-8),\n        (50, 51, 0.2, 0.01, 1e-6, 1e-8),\n        (50, 1, 0.2, 0.01, 1e-6, 1e-8),\n        (1, 1, 0.2, 0.01, 1e-8, 1e-10),\n    ]\n\n    all_results = []\n\n    for N, M_mem, theta, alpha, epsilon, tol in test_cases:\n\n        # Step 1: Generate observations y_n using theta_true\n        y_obs = []\n        x_true_traj = [x0]\n        x = x0.copy()\n        \n        M_true = np.eye(d) + dt * (A + theta_true * B)\n\n        for _ in range(N):\n            x = M_true @ x + dt * s\n            x_true_traj.append(x)\n        \n        for x_val in x_true_traj:\n            y_obs.append(C @ x_val)\n\n        # Helper function to run the forward model for a given theta\n        def run_forward(theta_eval):\n            x_traj = [x0]\n            x = x0.copy()\n            M_eval = np.eye(d) + dt * (A + theta_eval * B)\n            for _ in range(N):\n                x = M_eval @ x + dt * s\n                x_traj.append(x)\n            return x_traj\n\n        # Helper function to compute the objective function J(theta)\n        def compute_J(theta_eval):\n            x_traj = run_forward(theta_eval)\n            misfit = 0.0\n            for n in range(N + 1):\n                misfit += 0.5 * np.linalg.norm(C @ x_traj[n] - y_obs[n])**2\n            return misfit + 0.5 * alpha * theta_eval**2\n\n        # Step 2: Compute finite-difference gradient\n        J_plus = compute_J(theta + epsilon)\n        J_minus = compute_J(theta - epsilon)\n        g_fd = (J_plus - J_minus) / (2 * epsilon)\n\n        # Step 3: Compute adjoint-state gradient with checkpointing\n        \n        # 3a: Forward pass and checkpointing\n        checkpoint_indices = sorted(list(set(np.linspace(0, N, M_mem, dtype=int))))\n        memory_used = len(checkpoint_indices)\n        checkpoints = {}\n        \n        x_fwd_traj = [x0]\n        x = x0.copy()\n        if 0 in checkpoint_indices:\n            checkpoints[0] = x0.copy()\n            \n        M_theta = np.eye(d) + dt * (A + theta * B)\n\n        for n in range(N):\n            x = M_theta @ x + dt * s\n            x_fwd_traj.append(x)\n            if (n + 1) in checkpoint_indices:\n                checkpoints[n + 1] = x.copy()\n        \n        recompute_count = 0\n\n        # Helper for on-demand state recomputation\n        memoized_states = {}\n        def get_state(n):\n            nonlocal recompute_count\n            if n in memoized_states:\n                return memoized_states[n]\n            if n in checkpoints:\n                memoized_states[n] = checkpoints[n]\n                return checkpoints[n]\n\n            # Find nearest preceding checkpoint\n            start_n = 0\n            for k in checkpoint_indices:\n                if k < n:\n                    start_n = k\n                else:\n                    break\n            \n            x_re = checkpoints[start_n].copy()\n            for i in range(start_n, n):\n                x_re = M_theta @ x_re + dt * s\n                recompute_count += 1\n            \n            memoized_states[n] = x_re\n            return x_re\n\n        # 3b: Backward pass\n        g_adj = alpha * theta\n        M_theta_T = M_theta.T\n\n        # Initialize adjoint at n=N\n        xN = get_state(N)\n        lam = C.T @ (C @ xN - y_obs[N])\n\n        # Backward recursion from n=N down to 1\n        for n in range(N, 0, -1):\n            # At start of loop, lam is lambda_n\n            \n            # Get state x_{n-1} for gradient and adjoint update\n            x_prev = get_state(n-1)\n\n            # Update gradient: term is dt * lambda_n^T * B * x_{n-1}\n            g_adj += dt * (lam.T @ B @ x_prev)\n            \n            # Update adjoint for next iteration: compute lambda_{n-1}\n            lam = M_theta_T @ lam + C.T @ (C @ x_prev - y_obs[n-1])\n\n        abs_err = abs(g_adj - g_fd)\n        match = abs_err < tol\n\n        all_results.append([g_adj, g_fd, abs_err, recompute_count, memory_used, match])\n\n    # Final print statement\n    result_str = ','.join([str(res) for res in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3419136"}]}