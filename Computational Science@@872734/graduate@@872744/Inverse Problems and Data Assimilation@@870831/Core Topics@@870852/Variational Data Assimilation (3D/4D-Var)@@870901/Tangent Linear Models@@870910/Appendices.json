{"hands_on_practices": [{"introduction": "The concept of a tangent linear model (TLM) generalizes the familiar derivative from single-variable calculus to more complex operators. This first exercise grounds the abstract definition of the TLM in this fundamental idea. By deriving the TLM for a simple scalar function, analyzing its error, and performing a numerical check, you will build a solid intuition for how linearization works and verify the core principle that $M(x+\\delta x) \\approx M(x) + L_x \\delta x$ for small perturbations [@problem_id:3424235].", "problem": "Consider the scalar nonlinear forward model mapping a scalar state to a scalar observable, defined by $M:\\mathbb{R}\\to\\mathbb{R}$ with $M(x)=\\sin(x^2)$. In variational data assimilation and inverse problems, the local linearization of $M$ about a base state $x$ is represented by the Tangent Linear Model (TLM), whose operator $L_x$ is the Fréchet derivative of $M$ at $x$. The TLM is defined via the relation $M(x+\\delta x)=M(x)+L_x\\,\\delta x+r(\\delta x)$ with $\\lim_{\\delta x\\to 0}\\frac{|r(\\delta x)|}{|\\delta x|}=0$.\n\nTasks:\n1) Starting only from the fundamental definition of the derivative and the chain rule of calculus, derive the explicit analytic expression for the tangent linear operator $L_x$ for $M(x)=\\sin(x^2)$ at an arbitrary $x\\in\\mathbb{R}$.\n\n2) Using Taylor’s theorem with Lagrange remainder, deduce the order of the truncation error in the first-order linearization and identify the leading-order term symbolically in terms of derivatives of $M$.\n\n3) Take $x_0=\\sqrt{2}$ and a perturbation $\\delta x=10^{-3}$. Numerically verify the first-order approximation by computing the relative error\n$$\n\\mathrm{RE}=\\frac{\\left|M(x_0+\\delta x)-M(x_0)-L_{x_0}\\,\\delta x\\right|}{\\left|M(x_0+\\delta x)-M(x_0)\\right|},\n$$\nand round your numerical result to four significant figures.\n\nImportant: Provide only the analytical expression for $L_x$ as your final boxed answer. The numerical verification is required in your working but should not be included in the boxed final answer.", "solution": "The problem statement will first be validated against the required criteria.\n\n### Step 1: Extract Givens\n-   **Model definition**: A scalar nonlinear forward model $M:\\mathbb{R}\\to\\mathbb{R}$ given by $M(x)=\\sin(x^2)$.\n-   **Tangent Linear Model (TLM) operator**: Denoted by $L_x$, defined as the Fréchet derivative of $M$ at a base state $x$.\n-   **Linearization relation**: $M(x+\\delta x)=M(x)+L_x\\,\\delta x+r(\\delta x)$, where $r(\\delta x)$ is the remainder term satisfying $\\lim_{\\delta x\\to 0}\\frac{|r(\\delta x)|}{|\\delta x|}=0$.\n-   **Task 1**: Derive the analytic expression for $L_x$ using the fundamental definition of the derivative and the chain rule.\n-   **Task 2**: Use Taylor’s theorem with Lagrange remainder to find the order of the truncation error and its leading-order term.\n-   **Task 3**: For $x_0=\\sqrt{2}$ and $\\delta x=10^{-3}$, numerically compute the relative error $\\mathrm{RE}=\\frac{\\left|M(x_0+\\delta x)-M(x_0)-L_{x_0}\\,\\delta x\\right|}{\\left|M(x_0+\\delta x)-M(x_0)\\right|}$ and round to four significant figures.\n-   **Final Answer Requirement**: Provide only the analytical expression for $L_x$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Grounding**: The problem is grounded in fundamental calculus (derivatives, Taylor's theorem) and its application to linearization, a standard technique in numerical analysis and data assimilation. The function $M(x)=\\sin(x^2)$ is a well-behaved, infinitely differentiable function. The problem is scientifically and mathematically sound.\n2.  **Well-Posedness**: The function $M(x)$ is defined and smooth for all $x \\in \\mathbb{R}$. Its derivatives exist and are unique. The tasks are specific and lead to a unique, meaningful solution. The problem is well-posed.\n3.  **Objectivity**: The problem is stated using precise mathematical language and is free from subjective claims.\n4.  **Completeness and Consistency**: All necessary information, including the function, definitions, and values for numerical computation, is provided. There are no internal contradictions in the problem statement.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution\n\n**Task 1: Derivation of the Tangent Linear Operator $L_x$**\n\nThe tangent linear operator $L_x$ is defined as the Fréchet derivative of the model $M$ at the point $x$. For a scalar function of a scalar variable, $M: \\mathbb{R} \\to \\mathbb{R}$, the Fréchet derivative is the ordinary derivative, $M'(x)$. The action of the operator $L_x$ on a perturbation $\\delta x$ is given by multiplication: $L_x \\delta x = M'(x) \\delta x$. Therefore, the operator $L_x$ can be identified with the scalar value $M'(x)$.\n\nThe problem specifies using the chain rule to find the derivative of $M(x) = \\sin(x^2)$. We can represent $M(x)$ as a composition of two functions: $f(u) = \\sin(u)$ and $g(x) = x^2$, such that $M(x) = f(g(x))$.\n\nAccording to the chain rule, the derivative of a composite function is given by:\n$$\nM'(x) = \\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)\n$$\nFirst, we find the derivatives of $f(u)$ and $g(x)$:\n$$\nf'(u) = \\frac{d}{du}(\\sin(u)) = \\cos(u)\n$$\n$$\ng'(x) = \\frac{d}{dx}(x^2) = 2x\n$$\nSubstituting these into the chain rule formula:\n$$\nM'(x) = \\cos(g(x)) \\cdot (2x) = \\cos(x^2) \\cdot 2x = 2x\\cos(x^2)\n$$\nSince the operator $L_x$ is identified with $M'(x)$, the explicit analytic expression for the tangent linear operator is:\n$$\nL_x = 2x\\cos(x^2)\n$$\n\n**Task 2: Truncation Error Analysis**\n\nThe first-order linearization of $M(x)$ around $x$ is $M(x+\\delta x) \\approx M(x) + L_x \\delta x = M(x) + M'(x)\\delta x$. The truncation error is the remainder term $r(\\delta x)$ from the definition $M(x+\\delta x)=M(x)+M'(x)\\delta x+r(\\delta x)$.\n\nTo analyze this error, we use Taylor's theorem with the Lagrange remainder. For a function $M$ that is twice differentiable on an interval containing $x$ and $x+\\delta x$, there exists some $\\xi$ between $x$ and $x+\\delta x$ such that:\n$$\nM(x+\\delta x) = M(x) + M'(x)\\delta x + \\frac{M''(\\xi)}{2!}(\\delta x)^2\n$$\nComparing this with the linearization, the truncation error is exactly:\n$$\nr(\\delta x) = \\frac{M''(\\xi)}{2}(\\delta x)^2\n$$\nThe order of the truncation error is determined by the power of $\\delta x$, which is 2. Thus, the error is of order $O((\\delta x)^2)$.\n\nTo find the leading-order term symbolically, we consider the limit as $\\delta x \\to 0$. In this limit, $\\xi \\to x$. The leading-order term of the error is therefore $\\frac{M''(x)}{2}(\\delta x)^2$. We need to compute the second derivative, $M''(x)$:\n$$\nM''(x) = \\frac{d}{dx} M'(x) = \\frac{d}{dx} \\left(2x\\cos(x^2)\\right)\n$$\nUsing the product rule and the chain rule:\n$$\nM''(x) = \\left(\\frac{d}{dx}(2x)\\right)\\cos(x^2) + 2x\\left(\\frac{d}{dx}\\cos(x^2)\\right)\n$$\n$$\nM''(x) = 2\\cos(x^2) + 2x(-\\sin(x^2) \\cdot 2x)\n$$\n$$\nM''(x) = 2\\cos(x^2) - 4x^2\\sin(x^2)\n$$\nThe symbolic leading-order term of the truncation error is:\n$$\n\\frac{M''(x)}{2}(\\delta x)^2 = \\frac{1}{2} (2\\cos(x^2) - 4x^2\\sin(x^2))(\\delta x)^2 = (\\cos(x^2) - 2x^2\\sin(x^2))(\\delta x)^2\n$$\n\n**Task 3: Numerical Verification**\n\nWe are given $x_0 = \\sqrt{2}$ and $\\delta x = 10^{-3}$. We are to compute the relative error:\n$$\n\\mathrm{RE}=\\frac{\\left|M(x_0+\\delta x)-M(x_0)-L_{x_0}\\,\\delta x\\right|}{\\left|M(x_0+\\delta x)-M(x_0)\\right|}\n$$\nLet's compute each term.\n-   $x_0 = \\sqrt{2}$, so $x_0^2 = 2$.\n-   $M(x_0) = M(\\sqrt{2}) = \\sin(2)$.\n-   $L_{x_0} = 2x_0\\cos(x_0^2) = 2\\sqrt{2}\\cos(2)$.\n-   $L_{x_0}\\delta x = (2\\sqrt{2}\\cos(2)) \\cdot 10^{-3}$.\n-   $x_0+\\delta x = \\sqrt{2} + 10^{-3}$.\n-   $M(x_0+\\delta x) = \\sin\\left(\\left(\\sqrt{2} + 10^{-3}\\right)^2\\right) = \\sin(2 + 2\\sqrt{2} \\cdot 10^{-3} + 10^{-6})$.\n\nNow, we evaluate the numerator and denominator using high-precision calculations.\nThe numerator is the absolute error of the linear approximation:\n$$\n\\mathrm{Numerator} = \\left|\\sin(2 + 2\\sqrt{2} \\cdot 10^{-3} + 10^{-6}) - \\sin(2) - (2\\sqrt{2}\\cos(2)) \\cdot 10^{-3}\\right|\n$$\nUsing numerical computation:\n$\\sin(2) \\approx 0.9092974268$\n$\\cos(2) \\approx -0.4161468365$\n$\\sqrt{2} \\approx 1.4142135624$\nArgument of $\\sin$: $2 + 2(1.4142135624)(0.001) + 0.000001 = 2.0028294271$\n$M(x_0+\\delta x) = \\sin(2.0028294271) \\approx 0.9081216641$\n$M(x_0) = \\sin(2) \\approx 0.9092974268$\n$L_{x_0}\\delta x = 2(1.4142135624)(-0.4161468365)(0.001) \\approx -0.0011769852$\nTrue change, $\\Delta M = M(x_0+\\delta x) - M(x_0) \\approx 0.9081216641 - 0.9092974268 = -0.0011757627$.\nThe error in the linear approximation of the change is:\n$\\mathrm{Numerator} = |\\Delta M - L_{x_0}\\delta x| \\approx |-0.0011757627 - (-0.0011769852)| = |0.0000012225| = 1.2225 \\times 10^{-6}$.\n\nThe denominator is the magnitude of the true change:\n$$\n\\mathrm{Denominator} = |M(x_0+\\delta x)-M(x_0)| = |\\Delta M| \\approx |-0.0011757627| = 1.1757627 \\times 10^{-3}\n$$\n\nThe relative error is:\n$$\n\\mathrm{RE} = \\frac{1.2225 \\times 10^{-6}}{1.1757627 \\times 10^{-3}} \\approx 0.00103975\n$$\nRounding to four significant figures, we get:\n$$\n\\mathrm{RE} \\approx 1.040 \\times 10^{-3}\n$$\nThis small relative error indicates that the first-order approximation is accurate for the given perturbation, thus numerically verifying the approximation.\nThe final answer required is only the analytical expression for $L_x$.", "answer": "$$ \\boxed{2x\\cos(x^2)} $$", "id": "3424235"}, {"introduction": "In practice, tangent linear models are applied to large, complex numerical simulations, not simple analytic functions. A critical skill is verifying that the implemented TLM code is correct. This practice introduces the \"Taylor test,\" an indispensable method for verifying that the error of the linear approximation scales quadratically with the perturbation size, as predicted by theory. By implementing this test for the classic Lorenz-63 chaotic system, you will gain hands-on experience with the gold standard for code verification in computational science [@problem_id:3424290].", "problem": "Implement a complete, runnable program that performs a Taylor test to verify the correctness of a Tangent Linear Model (TLM) produced by Automatic Differentiation (AD) for a discrete nonlinear model map. The verification must be done by measuring the residual $r(\\alpha) = \\lVert M(x + \\alpha \\,\\delta) - M(x) - \\alpha \\, L_x \\,\\delta \\rVert_2$ across a range of perturbation magnitudes $\\alpha$ and demonstrating $O(\\alpha^2)$ scaling of $r(\\alpha)$ for sufficiently small $\\alpha$. Your implementation must start from fundamental definitions and well-tested formulas, as specified below, and may not assume shortcut identities not derived from these bases.\n\nDefinitions and fundamental bases to use:\n- Let $M: \\mathbb{R}^n \\to \\mathbb{R}^n$ be a discrete-time model map produced by numerically integrating a smooth Ordinary Differential Equation (ODE) using a deterministic scheme. Assume that $M$ is twice continuously differentiable in a neighborhood of any $x \\in \\mathbb{R}^n$.\n- The Tangent Linear Model (TLM) at $x$, denoted $L_x: \\mathbb{R}^n \\to \\mathbb{R}^n$, is the Fréchet derivative of $M$ at $x$, i.e., the unique linear map satisfying the first-order Taylor approximation $M(x + \\delta) \\approx M(x) + L_x \\delta$ with an $o(\\lVert \\delta \\rVert_2)$ remainder as $\\lVert \\delta \\rVert_2 \\to 0$.\n- Taylor’s theorem for vector-valued functions implies that if $M$ is twice continuously differentiable in a neighborhood of $x$, then there exists a constant $C_x > 0$ such that, for all sufficiently small $\\alpha > 0$ and any $\\delta \\in \\mathbb{R}^n$, the residual satisfies $\\lVert M(x + \\alpha \\,\\delta) - M(x) - \\alpha \\, L_x \\,\\delta \\rVert_2 \\le C_x \\,\\alpha^2 \\,\\lVert \\delta \\rVert_2^2$.\n\nModel specification to implement:\n- Use the Lorenz–$63$ system as the continuous-time state evolution model:\n  - State dimension is $n = 3$, with state $u(t) = (x(t), y(t), z(t))^\\top \\in \\mathbb{R}^3$.\n  - Parameters are $\\sigma = 10$, $\\rho = 28$, and $\\beta = 8/3$.\n  - The right-hand side is $f(u) = \\begin{bmatrix} \\sigma (y - x) \\\\ x(\\rho - z) - y \\\\ x y - \\beta z \\end{bmatrix}$.\n  - The Jacobian of $f$ at $u = (x,y,z)$ is $J(u) = \\begin{bmatrix} -\\sigma & \\sigma & 0 \\\\ \\rho - z & -1 & -x \\\\ y & x & -\\beta \\end{bmatrix}$.\n- Define $M$ as the time-$T$ flow map obtained by $K$ fixed steps of the classical fourth-order Runge–Kutta method (RK4) with step size $\\Delta t$, i.e., $T = K \\,\\Delta t$. For any $x \\in \\mathbb{R}^3$, $M(x)$ is the state after $K$ RK4 steps starting from initial condition $u(0) = x$.\n- Define the TLM $L_x$ as the Jacobian–vector product of the discrete map $M$ at $x$, applied to a perturbation direction $\\delta \\in \\mathbb{R}^3$. To compute $L_x \\delta$, implement the discrete TLM of RK4 by coupling the RK4 stages for the base trajectory with their linearizations. For one RK4 step from $u$ to $u^+$ with step size $\\Delta t$, define\n  - $k_1 = f(u)$, $k_2 = f(u + \\tfrac{\\Delta t}{2} k_1)$, $k_3 = f(u + \\tfrac{\\Delta t}{2} k_2)$, $k_4 = f(u + \\Delta t \\, k_3)$, and $u^+ = u + \\tfrac{\\Delta t}{6}(k_1 + 2 k_2 + 2 k_3 + k_4)$.\n  - Let $A_1 = J(u)$, $A_2 = J(u + \\tfrac{\\Delta t}{2} k_1)$, $A_3 = J(u + \\tfrac{\\Delta t}{2} k_2)$, $A_4 = J(u + \\Delta t \\, k_3)$.\n  - For a tangent vector $\\delta$, define $K_1 = A_1 \\delta$, $K_2 = A_2 \\left(\\delta + \\tfrac{\\Delta t}{2} K_1\\right)$, $K_3 = A_3 \\left(\\delta + \\tfrac{\\Delta t}{2} K_2\\right)$, $K_4 = A_4 \\left(\\delta + \\Delta t \\, K_3\\right)$, and $\\delta^+ = \\delta + \\tfrac{\\Delta t}{6}(K_1 + 2 K_2 + 2 K_3 + K_4)$.\n  - Repeating this coupled update for $K$ steps yields $M(x)$ and $L_x \\delta$.\n\nWhat to compute:\n- For each provided test case, do the following:\n  1. Normalize the perturbation direction $\\delta$ to unit Euclidean norm, i.e., replace $\\delta$ by $\\delta / \\lVert \\delta \\rVert_2$.\n  2. Compute $M(x)$ and $L_x \\delta$ by $K$ RK4 steps of size $\\Delta t$.\n  3. For each $\\alpha$ in a prescribed list, compute the residual $r(\\alpha) = \\lVert M(x + \\alpha \\,\\delta) - M(x) - \\alpha \\, L_x \\,\\delta \\rVert_2$.\n  4. Estimate the scaling exponent $s$ by ordinary least squares fitting of the model $\\log r(\\alpha) \\approx a + s \\log \\alpha$ over all $\\alpha$ for which $r(\\alpha) > 0$ and both $\\log \\alpha$ and $\\log r(\\alpha)$ are finite. Explicitly, if $\\{(\\alpha_i, r_i)\\}_{i=1}^m$ is the filtered set, compute $s = \\dfrac{\\sum_{i=1}^m (\\log \\alpha_i - \\overline{\\log \\alpha})(\\log r_i - \\overline{\\log r})}{\\sum_{i=1}^m (\\log \\alpha_i - \\overline{\\log \\alpha})^2}$, where bars denote sample means.\n  5. Report the slope $s$ and whether it is within a tolerance of the expected value $2$, i.e., report a Boolean that is true if $|s - 2|  0.1$ and false otherwise.\n\nTest suite:\n- Use $\\alpha$ values $\\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}\\}$ for all cases.\n- Case $1$ (baseline): $x = (1, 1, 1)^\\top$, $\\delta = (1, -1, 2)^\\top$, $\\Delta t = 10^{-2}$, $K = 10$.\n- Case $2$ (increased nonlinearity through longer horizon): $x = (5, -5, 20)^\\top$, $\\delta = (-2, 1, 0.5)^\\top$, $\\Delta t = 10^{-2}$, $K = 50$.\n- Case $3$ (different geometry and horizon): $x = (-8, 7, 27)^\\top$, $\\delta = (0.3, -0.4, 0.5)^\\top$, $\\Delta t = 5 \\times 10^{-3}$, $K = 200$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, consisting of two lists:\n  - The first list contains the three slope estimates $s$ (one per test case) as decimal numbers.\n  - The second list contains the three Boolean pass/fail results (true if $|s - 2|  0.1$, false otherwise), in the same order as the slopes.\n- For example, the output must have the form $[[s_1,s_2,s_3],[b_1,b_2,b_3]]$ with no spaces anywhere in the line.\n\nNo external input should be read. All computations are nondimensional and do not involve physical units. Angles do not appear. The program must be fully deterministic and self-contained, using only the specified libraries and the language/runtime stated later.", "solution": "The problem requires the implementation of a Taylor test to verify a tangent linear model (TLM) for a discrete-time map derived from the Lorenz–$63$ system. This verification is based on the fundamental properties of Taylor's theorem for vector-valued functions.\n\nThe core principle is that for a twice continuously differentiable function $M: \\mathbb{R}^n \\to \\mathbb{R}^n$, its value at a perturbed point $x + \\alpha \\delta$ can be approximated by its first-order Taylor expansion around $x$. The expansion is given by:\n$$\nM(x + \\alpha \\delta) = M(x) + \\alpha L_x \\delta + O(\\alpha^2)\n$$\nwhere $L_x$ is the Fréchet derivative (or Jacobian) of $M$ at $x$, and $\\delta \\in \\mathbb{R}^n$ is a perturbation direction. The term $O(\\alpha^2)$ indicates that the error of this linear approximation is of the order of $\\alpha^2$ for sufficiently small $\\alpha$.\n\nThe objective is to numerically verify this scaling property. We define a residual $r(\\alpha)$ as the Euclidean norm of the error in the first-order approximation:\n$$\nr(\\alpha) = \\lVert M(x + \\alpha \\delta) - M(x) - \\alpha L_x \\delta \\rVert_2\n$$\nAccording to Taylor's theorem, there exists a constant $C_x  0$ such that $r(\\alpha) \\le C_x \\alpha^2 \\lVert \\delta \\rVert_2^2$. For a fixed, normalized perturbation $\\delta$ (i.e., $\\lVert \\delta \\rVert_2 = 1$), this implies $r(\\alpha) \\propto \\alpha^2$ for small $\\alpha$.\n\nTo test this relationship, we can analyze the behavior of $r(\\alpha)$ on a log-log scale. Taking the natural logarithm of the proportionality $r(\\alpha) \\approx C \\alpha^2$ yields:\n$$\n\\log r(\\alpha) \\approx \\log C + 2 \\log \\alpha\n$$\nThis is a linear relationship between $\\log r(\\alpha)$ and $\\log \\alpha$ with a theoretical slope of $s=2$. We can estimate this slope from numerical data by computing $r(\\alpha)$ for a sequence of decreasing $\\alpha$ values and then performing an ordinary least-squares (OLS) linear regression on the log-transformed data. The problem provides the standard formula for the slope $s$ of the best-fit line for data points $(\\log \\alpha_i, \\log r_i)$:\n$$\ns = \\frac{\\sum_{i=1}^m (\\log \\alpha_i - \\overline{\\log \\alpha})(\\log r_i - \\overline{\\log r})}{\\sum_{i=1}^m (\\log \\alpha_i - \\overline{\\log \\alpha})^2}\n$$\nwhere the bar denotes the sample mean. A computed slope $s$ close to $2$ provides strong evidence that the implementation of the TLM, $L_x \\delta$, is correct.\n\nThe implementation consists of the following components:\n\n1.  **The Continuous Model**: The Lorenz–$63$ system is defined by the ODE $\\frac{d u}{d t} = f(u)$ where $u = (x, y, z)^\\top \\in \\mathbb{R}^3$ and the right-hand side $f(u)$ is:\n    $$\n    f(u) = \\begin{bmatrix} \\sigma (y - x) \\\\ x(\\rho - z) - y \\\\ x y - \\beta z \\end{bmatrix}\n    $$\n    with parameters $\\sigma = 10$, $\\rho = 28$, and $\\beta = 8/3$. The Jacobian of $f$, denoted $J(u)$, is required for the TLM and is given by:\n    $$\n    J(u) = \\frac{\\partial f}{\\partial u}(u) = \\begin{bmatrix} -\\sigma  \\sigma  0 \\\\ \\rho - z  -1  -x \\\\ y  x  -\\beta \\end{bmatrix}\n    $$\n\n2.  **The Discrete Model Map $M$**: The map $M$ advances an initial state $x$ over a time interval $T = K \\Delta t$ by applying the fourth-order Runge–Kutta (RK4) method for $K$ steps of size $\\Delta t$. A single RK4 step from state $u$ to $u^+$ is:\n    $$\n    \\begin{aligned}\n    k_1 = f(u) \\\\\n    k_2 = f(u + \\frac{\\Delta t}{2} k_1) \\\\\n    k_3 = f(u + \\frac{\\Delta t}{2} k_2) \\\\\n    k_4 = f(u + \\Delta t k_3) \\\\\n    u^+ = u + \\frac{\\Delta t}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n    \\end{aligned}\n    $$\n\n3.  **The Tangent Linear Model $L_x \\delta$**: The action of the TLM on a perturbation $\\delta$ is found by differentiating the discrete map $M$. This is accomplished by differentiating each RK4 step and composing the results. For a single step, the perturbation $\\delta$ is advanced to $\\delta^+$ using the chain rule, resulting in a coupled system. The update for $\\delta^+$ is given by:\n    $$\n    \\begin{aligned}\n    A_1 = J(u), \\quad K_1 = A_1 \\delta \\\\\n    A_2 = J(u + \\frac{\\Delta t}{2} k_1), \\quad K_2 = A_2 (\\delta + \\frac{\\Delta t}{2} K_1) \\\\\n    A_3 = J(u + \\frac{\\Delta t}{2} k_2), \\quad K_3 = A_3 (\\delta + \\frac{\\Delta t}{2} K_2) \\\\\n    A_4 = J(u + \\Delta t k_3), \\quad K_4 = A_4 (\\delta + \\Delta t K_3) \\\\\n    \\delta^+ = \\delta + \\frac{\\Delta t}{6}(K_1 + 2K_2 + 2K_3 + K_4)\n    \\end{aligned}\n    $$\n    To compute $M(x)$ and $L_x \\delta$, we start with $(u_0, \\delta_0) = (x, \\delta)$ and iteratively apply the coupled RK4 update $K$ times.\n\nThe program implements these functions and carries out the Taylor test for three distinct test cases. For each case, it normalizes the initial perturbation $\\delta$, computes $M(x)$ and $L_x \\delta$, calculates the residuals $r(\\alpha)$ for a list of $\\alpha$ values, estimates the slope $s$ from the log-log data, and finally checks if the slope is within a tolerance of $\\lvert s - 2 \\rvert  0.1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global constants for the Lorenz-63 system\nSIGMA = 10.0\nRHO = 28.0\nBETA = 8.0 / 3.0\n\ndef lorenz_rhs(u: np.ndarray) - np.ndarray:\n    \"\"\"\n    Computes the right-hand side of the Lorenz-63 system.\n    \"\"\"\n    x, y, z = u\n    dx_dt = SIGMA * (y - x)\n    dy_dt = x * (RHO - z) - y\n    dz_dt = x * y - BETA * z\n    return np.array([dx_dt, dy_dt, dz_dt])\n\ndef lorenz_jacobian(u: np.ndarray) - np.ndarray:\n    \"\"\"\n    Computes the Jacobian of the Lorenz-63 RHS function.\n    \"\"\"\n    x, y, z = u\n    jac = np.array([\n        [-SIGMA, SIGMA, 0.0],\n        [RHO - z, -1.0, -x],\n        [y, x, -BETA]\n    ])\n    return jac\n\ndef propagate_model_and_tlm(x0: np.ndarray, delta0: np.ndarray, K: int, dt: float) - tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Propagates the state and a perturbation forward in time for K steps\n    using the coupled RK4 and its Tangent Linear Model.\n    \n    Returns M(x0) and L_x0(delta0).\n    \"\"\"\n    u = x0.copy()\n    delta = delta0.copy()\n\n    for _ in range(K):\n        # RK4 stages for the nonlinear state\n        k1 = lorenz_rhs(u)\n        u_k2 = u + 0.5 * dt * k1\n        k2 = lorenz_rhs(u_k2)\n        u_k3 = u + 0.5 * dt * k2\n        k3 = lorenz_rhs(u_k3)\n        u_k4 = u + dt * k3\n        k4 = lorenz_rhs(u_k4)\n        \n        # Jacobian evaluations at intermediate points\n        A1 = lorenz_jacobian(u)\n        A2 = lorenz_jacobian(u_k2)\n        A3 = lorenz_jacobian(u_k3)\n        A4 = lorenz_jacobian(u_k4)\n\n        # RK4 stages for the tangent linear perturbation\n        K1 = A1 @ delta\n        K2 = A2 @ (delta + 0.5 * dt * K1)\n        K3 = A3 @ (delta + 0.5 * dt * K2)\n        K4 = A4 @ (delta + dt * K3)\n        \n        # Update state and perturbation\n        u += (dt / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n        delta += (dt / 6.0) * (K1 + 2.0 * K2 + 2.0 * K3 + K4)\n\n    return u, delta\n\ndef propagate_model(x0: np.ndarray, K: int, dt: float) - np.ndarray:\n    \"\"\"\n    Propagates the state forward for K steps using RK4.\n    \n    Returns M(x0).\n    \"\"\"\n    u = x0.copy()\n    for _ in range(K):\n        k1 = lorenz_rhs(u)\n        k2 = lorenz_rhs(u + 0.5 * dt * k1)\n        k3 = lorenz_rhs(u + 0.5 * dt * k2)\n        k4 = lorenz_rhs(u + dt * k3)\n        u += (dt / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n    return u\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'x': np.array([1.0, 1.0, 1.0]), 'delta': np.array([1.0, -1.0, 2.0]), 'dt': 1e-2, 'K': 10},\n        {'x': np.array([5.0, -5.0, 20.0]), 'delta': np.array([-2.0, 1.0, 0.5]), 'dt': 1e-2, 'K': 50},\n        {'x': np.array([-8.0, 7.0, 27.0]), 'delta': np.array([0.3, -0.4, 0.5]), 'dt': 5e-3, 'K': 200}\n    ]\n\n    alpha_vals = np.array([1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6])\n    \n    slopes = []\n    checks = []\n\n    for case in test_cases:\n        x = case['x']\n        delta_orig = case['delta']\n        dt = case['dt']\n        K = case['K']\n        \n        # Step 1: Normalize the perturbation direction\n        delta_norm = np.linalg.norm(delta_orig)\n        delta = delta_orig / delta_norm if delta_norm  0 else delta_orig\n\n        # Step 2: Compute M(x) and L_x(delta)\n        M_x, L_x_delta = propagate_model_and_tlm(x, delta, K, dt)\n        \n        residuals = []\n        valid_alphas = []\n        \n        # Step 3: For each alpha, compute the residual r(alpha)\n        for alpha in alpha_vals:\n            x_pert = x + alpha * delta\n            M_x_pert = propagate_model(x_pert, K, dt)\n            residual_vec = M_x_pert - M_x - alpha * L_x_delta\n            r_alpha = np.linalg.norm(residual_vec)\n            \n            # Filter for positive residuals where log is defined\n            if r_alpha  0:\n                residuals.append(r_alpha)\n                valid_alphas.append(alpha)\n\n        # Step 4: Estimate the scaling exponent s by OLS\n        log_residuals = np.log(np.array(residuals))\n        log_alphas = np.log(np.array(valid_alphas))\n        \n        if len(log_alphas)  2:\n            s = np.nan\n        else:\n            mean_log_alpha = np.mean(log_alphas)\n            mean_log_residual = np.mean(log_residuals)\n            \n            numerator = np.sum((log_alphas - mean_log_alpha) * (log_residuals - mean_log_residual))\n            denominator = np.sum((log_alphas - mean_log_alpha)**2)\n            \n            s = numerator / denominator if denominator != 0 else np.nan\n        \n        # Step 5: Report slope and check against tolerance\n        is_correct = abs(s - 2.0)  0.1\n        \n        slopes.append(round(s, 8))\n        checks.append(is_correct)\n        \n    # Final print statement in the exact required format.\n    slopes_str = ','.join(map(str, slopes))\n    checks_str = ','.join(map(str, checks)).lower()\n    print(f\"[[{slopes_str}],[{checks_str}]]\")\n\nsolve()\n```", "id": "3424290"}, {"introduction": "The tangent linear model is rarely used in isolation; it is fundamentally linked to its dual, the adjoint model, which is essential for variational data assimilation and optimization. This relationship is defined through an inner product, $\\langle L \\delta x, \\delta y \\rangle_W = \\langle \\delta x, L^* \\delta y \\rangle_W$, where $L$ is the TLM operator and $L^*$ is the adjoint. This exercise makes this abstract \"adjoint-tangent\" identity concrete, allowing you to numerically verify it and explore how its implementation depends critically on the choice of the inner product weighting matrix $W$ [@problem_id:3424230].", "problem": "You are given a nonlinear discrete-time map constructed by one forward Euler time step applied to an ordinary differential equation, defined as follows. Let the state be $x \\in \\mathbb{R}^{3}$, the time step be $dt \\in \\mathbb{R}$, and the nonlinear right-hand side be $f:\\mathbb{R}^{3}\\to\\mathbb{R}^{3}$ with components\n$$\nf_1(x) = \\sin(x_1) + x_2^2,\\quad\nf_2(x) = e^{x_1}\\,x_2 + x_3,\\quad\nf_3(x) = \\tanh(x_3) - x_1 x_2.\n$$\nThe one-step map is $M(x) = x + dt\\,f(x)$. Consider the tangent linear operator $L$ that linearizes $M$ at a given base point $x_0$, namely $L = I + dt\\,J_f(x_0)$, where $J_f(x_0)$ is the Jacobian matrix of $f$ evaluated at $x_0$.\n\nLet $\\langle a,b\\rangle_W := a^{\\top} W b$ denote a weighted inner product on $\\mathbb{R}^3$ for any symmetric positive definite weight matrix $W \\in \\mathbb{R}^{3\\times 3}$ and any $a,b \\in \\mathbb{R}^{3}$. The adjoint operator $L^{*}$ is defined (by the inner product) to be the unique linear map satisfying $\\langle L\\,\\delta x, \\delta y \\rangle_W = \\langle \\delta x, L^{*}\\,\\delta y \\rangle_W$ for all $\\delta x,\\delta y \\in \\mathbb{R}^{3}$.\n\nYour task is to numerically verify the adjoint–tangent inner product identity\n$$\n\\langle L\\,\\delta x, \\delta y \\rangle_W = \\langle \\delta x, L^{*}\\,\\delta y \\rangle_W,\n$$\nfor a collection of test cases, and to quantify discrepancies that arise under inconsistent implementations. To do so, implement:\n- The nonlinear map $M(x) = x + dt\\,f(x)$.\n- The exact tangent linear operator $L = I + dt\\,J_f(x_0)$ using the analytic Jacobian\n$$\nJ_f(x) =\n\\begin{bmatrix}\n\\cos(x_1)  2x_2  0 \\\\\ne^{x_1}x_2  e^{x_1}  1 \\\\\n-x_2  -x_1  1-\\tanh^2(x_3)\n\\end{bmatrix}.\n$$\n- A finite-difference approximation of $L$ using forward differences on $M$ with a specified step $h>0$, i.e., the $i$-th column of $L_{\\mathrm{fd}}$ is $(M(x_0 + h e_i) - M(x_0))/h$, where $e_i$ is the $i$-th standard basis vector.\n- The weighted inner product $\\langle a,b\\rangle_W = a^{\\top} W b$.\n- The adjoint $L^{*}$ with respect to the weighted inner product, computed so that it satisfies the defining identity for the given $W$ and $L$.\n- The scalar residual\n$$\nr = \\big|\\langle L\\,\\delta x, \\delta y \\rangle_W - \\langle \\delta x, L^{*}\\,\\delta y \\rangle_W\\big|.\n$$\n\nUse the following fixed perturbations for all test cases:\n- $\\delta x = [\\,0.3,\\,-0.5,\\,0.8\\,]^{\\top}$,\n- $\\delta y = [\\,-0.2,\\,0.4,\\,0.1\\,]^{\\top}$.\n\nImplement the five test cases below. For each case, compute $r$ as described.\n\n- Test case A (consistent, Euclidean inner product):\n  - $dt = 0.1$,\n  - $x_0 = [\\,0.2,\\,-0.4,\\,0.6\\,]^{\\top}$,\n  - $W = \\mathrm{diag}([\\,1.0,\\,1.0,\\,1.0\\,])$,\n  - $L$ computed exactly from the analytic Jacobian,\n  - $L^{*}$ computed consistently with the weighted inner product and $L$.\n\n- Test case B (consistent, weighted inner product):\n  - $dt = 0.2$,\n  - $x_0 = [\\,-0.3,\\,0.7,\\,-0.5\\,]^{\\top}$,\n  - $W = \\mathrm{diag}([\\,1.0,\\,2.0,\\,3.0\\,])$,\n  - $L$ computed exactly from the analytic Jacobian,\n  - $L^{*}$ computed consistently with the weighted inner product and $L$.\n\n- Test case C (mismatch: tangent linear via finite differences vs adjoint from exact Jacobian):\n  - $dt = 0.05$,\n  - $x_0 = [\\,1.0,\\,-1.0,\\,2.0\\,]^{\\top}$,\n  - $W = \\mathrm{diag}([\\,1.0,\\,1.0,\\,1.0\\,])$,\n  - $L$ approximated by forward finite differences with step $h = 10^{-4}$,\n  - $L^{*}$ computed from the exact $L$ (using the analytic Jacobian), creating an intentional inconsistency.\n\n- Test case D (mismatch: wrong adjoint ignores weighting):\n  - $dt = 0.15$,\n  - $x_0 = [\\,0.25,\\,-0.75,\\,1.25\\,]^{\\top}$,\n  - $W = \\mathrm{diag}([\\,10.0,\\,0.1,\\,1.0\\,])$,\n  - $L$ computed exactly from the analytic Jacobian,\n  - $L^{*}$ incorrectly taken as the Euclidean-adjoint $L^{\\top}$, ignoring the weighting.\n\n- Test case E (consistent but ill-conditioned weighting):\n  - $dt = 0.1$,\n  - $x_0 = [\\,2.0,\\,-2.0,\\,0.0\\,]^{\\top}$,\n  - $W = \\mathrm{diag}([\\,10^{-8},\\,1.0,\\,10^{8}\\,])$,\n  - $L$ computed exactly from the analytic Jacobian,\n  - $L^{*}$ computed consistently with the weighted inner product and $L$.\n\nYour program must compute the residual $r$ for each test case in the order A, B, C, D, E. The final output format must be a single line containing a Python-style list of the five residuals in this order, with each residual as a floating-point number. For example, the output must look like\n\"[r_A,r_B,r_C,r_D,r_E]\".\n\nThere are no physical units or angles in this problem. All computations are real-valued.\n\nThe test suite exercises correctness under consistent implementations, sensitivity to finite-difference approximations, the effect of using an incorrect inner product in the adjoint, and numerical conditioning effects due to a highly ill-conditioned weight matrix. The results are to be reported as real numbers.", "solution": "The central task is to numerically verify the defining identity of an adjoint operator in the context of a weighted inner product and to investigate the consequences of inconsistent implementations. The identity states that for a linear operator $L$, its adjoint $L^*$, and any two vectors $\\delta x, \\delta y$ in the state space, the following equality holds: $\\langle L\\,\\delta x, \\delta y \\rangle_W = \\langle \\delta x, L^{*}\\,\\delta y \\rangle_W$. Here, $\\langle a, b \\rangle_W = a^\\top W b$ is the inner product weighted by a symmetric positive definite matrix $W$.\n\nFirst, we must establish the explicit form of the adjoint operator $L^*$. The defining relation can be written in matrix notation as:\n$$\n(L\\,\\delta x)^{\\top} W (\\delta y) = (\\delta x)^{\\top} W (L^{*}\\,\\delta y)\n$$\nUsing the property that $(AB)^\\top = B^\\top A^\\top$, the left-hand side becomes:\n$$\n(\\delta x)^{\\top} L^{\\top} W \\delta y = (\\delta x)^{\\top} (W L^{*}) \\delta y\n$$\nSince this equality must hold for all vectors $\\delta x, \\delta y \\in \\mathbb{R}^3$, the matrices within the quadratic forms must be equal:\n$$\nL^{\\top} W = W L^{*}\n$$\nGiven that $W$ is a symmetric positive definite matrix, it is invertible. We can therefore solve for $L^*$ by left-multiplying by $W^{-1}$:\n$$\nW^{-1} L^{\\top} W = W^{-1} W L^{*}\n$$\n$$\nW^{-1} L^{\\top} W = I L^{*}\n$$\nThis yields the formula for the adjoint operator with respect to the weighted inner product:\n$$\nL^{*} = W^{-1} L^{\\top} W\n$$\nThis formula is the cornerstone of a correct and consistent implementation. It shows that the adjoint operator depends on both the forward operator $L$ and the weighting matrix $W$ of the inner product space. A common error, explored in test case D, is to assume $L^* = L^\\top$, which is only valid for the standard Euclidean inner product where $W=I$ (the identity matrix).\n\nThe overall numerical procedure involves the following steps for each test case:\n1.  Define the parameters for the case: the time step $dt$, the linearization point $x_0$, and the weighting matrix $W$.\n2.  Construct the tangent linear operator $L$. This can be done either exactly using the provided analytical Jacobian $J_f(x_0)$ via $L = I + dt\\,J_f(x_0)$, or approximately using a finite-difference scheme. The forward-difference approximation for the $i$-th column of $L$ is given by $(M(x_0 + h e_i) - M(x_0))/h$, where $M(x) = x + dt f(x)$ and $e_i$ is the $i$-th standard basis vector.\n3.  Construct the corresponding adjoint operator $L^*$. For a consistent implementation, this is done using the derived formula $L^* = W^{-1}L^\\top W$. The test cases also explore inconsistent constructions to highlight potential pitfalls.\n4.  Calculate the two scalar values of the inner product identity: $S_1 = \\langle L\\,\\delta x, \\delta y \\rangle_W = (L\\delta x)^\\top W \\delta y$ and $S_2 = \\langle \\delta x, L^{*}\\,\\delta y \\rangle_W = (\\delta x)^\\top W (L^*\\delta y)$, using the given perturbations $\\delta x$ and $\\delta y$.\n5.  Compute the scalar residual $r = |S_1 - S_2|$.\n\nThe test cases are designed to probe different aspects of this framework:\n-   Cases A and B test the fundamental correctness of the implementation. With a consistently derived $L$ and $L^*$, the residual $r$ should be zero, up to machine precision. Case A uses the Euclidean inner product ($W=I$), while Case B uses a non-trivial weighting matrix, confirming the generality of the formula $L^* = W^{-1}L^\\top W$.\n-   Case C introduces a deliberate inconsistency. $L$ is approximated by finite differences ($L_{\\mathrm{fd}}$), but $L^*$ is derived from the *exact* analytical $L$. The resulting non-zero residual $r$ quantifies the error introduced by the finite-difference approximation, specifically how it breaks the adjoint identity when paired with an adjoint from a different operator. The magnitude of this residual is related to the truncation error of the finite-difference scheme, which is typically proportional to the step size $h$.\n-   Case D highlights a common conceptual error. $L$ is computed exactly, but $L^*$ is incorrectly assumed to be the simple transpose $L^\\top$, which is only the adjoint for the unweighted Euclidean inner product. Since $W \\neq I$, this creates a mismatch, and the residual $r$ will be non-zero, measuring the magnitude of this modeling error.\n-   Case E examines numerical stability. The implementation is theoretically consistent, but the weighting matrix $W$ is highly ill-conditioned (its diagonal entries span $16$ orders of magnitude). The computation of $L^*$, which involves both $W$ and its inverse $W^{-1}$, can be sensitive to floating-point representation errors. While the theoretical residual is zero, the numerical residual may be larger than machine epsilon, illustrating how poor conditioning can degrade computational accuracy.\n\nBy implementing these steps and analyzing the resulting residuals, one can numerically validate the adjoint relationship and gain practical insight into sources of error in numerical models used in data assimilation and optimization.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the verification of the adjoint-tangent inner product identity\n    for a series of test cases.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    dx = np.array([0.3, -0.5, 0.8])\n    dy = np.array([-0.2, 0.4, 0.1])\n\n    # --- Core Function Definitions ---\n\n    def f(x_vec):\n        \"\"\"Computes the nonlinear right-hand side f(x).\"\"\"\n        x1, x2, x3 = x_vec\n        f1 = np.sin(x1) + x2**2\n        f2 = np.exp(x1) * x2 + x3\n        f3 = np.tanh(x3) - x1 * x2\n        return np.array([f1, f2, f3])\n\n    def M(x_vec, dt):\n        \"\"\"Computes the one-step map M(x).\"\"\"\n        return x_vec + dt * f(x_vec)\n\n    def analytic_jacobian(x_vec):\n        \"\"\"Computes the analytic Jacobian matrix J_f(x).\"\"\"\n        x1, x2, x3 = x_vec\n        J = np.zeros((3, 3))\n        J[0, 0] = np.cos(x1)\n        J[0, 1] = 2 * x2\n        J[0, 2] = 0.0\n        J[1, 0] = np.exp(x1) * x2\n        J[1, 1] = np.exp(x1)\n        J[1, 2] = 1.0\n        J[2, 0] = -x2\n        J[2, 1] = -x1\n        J[2, 2] = 1.0 - np.tanh(x3)**2\n        return J\n\n    def inner_product(a, b, W):\n        \"\"\"Computes the weighted inner product a, b_W = a^T W b.\"\"\"\n        return a.T @ W @ b\n\n    def compute_residual(L_op, L_star_op, dx_vec, dy_vec, W_mat):\n        \"\"\"Computes the residual |L dx, dy_W - dx, L* dy_W|.\"\"\"\n        lhs = inner_product(L_op @ dx_vec, dy_vec, W_mat)\n        rhs = inner_product(dx_vec, L_star_op @ dy_vec, W_mat)\n        return np.abs(lhs - rhs)\n\n    # --- Test Case Definitions ---\n    # Each tuple contains: (dt, x0, W_diag, L_method, L_star_method, h)\n    # L_method: 'exact' or 'fd'\n    # L_star_method: 'consistent', 'from_exact', 'transpose'\n\n    test_cases = [\n        # Case A: Consistent, Euclidean inner product\n        (0.1, np.array([0.2, -0.4, 0.6]), np.array([1.0, 1.0, 1.0]), 'exact', 'consistent', None),\n        # Case B: Consistent, weighted inner product\n        (0.2, np.array([-0.3, 0.7, -0.5]), np.array([1.0, 2.0, 3.0]), 'exact', 'consistent', None),\n        # Case C: Mismatch (FD L vs anaytic L*)\n        (0.05, np.array([1.0, -1.0, 2.0]), np.array([1.0, 1.0, 1.0]), 'fd', 'from_exact', 1e-4),\n        # Case D: Mismatch (wrong adjoint ignores weighting)\n        (0.15, np.array([0.25, -0.75, 1.25]), np.array([10.0, 0.1, 1.0]), 'exact', 'transpose', None),\n        # Case E: Consistent but ill-conditioned weighting\n        (0.1, np.array([2.0, -2.0, 0.0]), np.array([1e-8, 1.0, 1e8]), 'exact', 'consistent', None),\n    ]\n\n    results = []\n    I = np.identity(3)\n\n    for case in test_cases:\n        dt, x0, W_diag, L_method, L_star_method, h = case\n        W = np.diag(W_diag)\n\n        # 1. Compute the Tangent Linear Operator L\n        if L_method == 'exact':\n            J = analytic_jacobian(x0)\n            L = I + dt * J\n        elif L_method == 'fd':\n            L = np.zeros((3, 3))\n            M_x0 = M(x0, dt)\n            for i in range(3):\n                e_i = np.zeros(3)\n                e_i[i] = 1.0\n                M_perturbed = M(x0 + h * e_i, dt)\n                L[:, i] = (M_perturbed - M_x0) / h\n        else:\n            raise ValueError(f\"Unknown L_method: {L_method}\")\n\n        # 2. Compute the Adjoint Operator L*\n        if L_star_method == 'consistent':\n            W_inv = np.linalg.inv(W)\n            L_star = W_inv @ L.T @ W\n        elif L_star_method == 'from_exact':\n            # This case uses L from FD but L* from the exact L, as per problem\n            J_exact = analytic_jacobian(x0)\n            L_exact = I + dt * J_exact\n            W_inv = np.linalg.inv(W)\n            L_star = W_inv @ L_exact.T @ W\n        elif L_star_method == 'transpose':\n            # Incorrectly assumes Euclidean inner product\n            L_star = L.T\n        else:\n            raise ValueError(f\"Unknown L_star_method: {L_star_method}\")\n\n        # 3. Compute the residual\n        residual = compute_residual(L, L_star, dx, dy, W)\n        results.append(residual)\n\n    # Final output formatting\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "3424230"}]}