{"hands_on_practices": [{"introduction": "Optimal Interpolation is not merely an algebraic recipe; it is deeply rooted in Bayesian probability theory. This exercise demonstrates this fundamental connection by guiding you from first principles. By starting with Bayes' theorem for a simple linear-Gaussian system, you will derive the posterior distribution of the state, revealing that the familiar Optimal Interpolation equations for the analysis mean and covariance emerge naturally from this rigorous statistical foundation [@problem_id:3407601].", "problem": "Consider a linear-Gaussian data assimilation setting for a two-dimensional state vector $x \\in \\mathbb{R}^{2}$ with a Gaussian prior and a single linear observation. The prior distribution is $x \\sim \\mathcal{N}(x_{b}, B)$, where the prior mean is $x_{b} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ and the prior covariance is $B = \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix}$. The observation model is $y = H x + v$, where $H = \\begin{pmatrix} 1  2 \\end{pmatrix}$, the observation error $v$ is Gaussian with distribution $v \\sim \\mathcal{N}(0, R)$, and $R = 1$. A single observation $y$ is recorded with the value $y = 3$.\n\nUsing Bayes' theorem and properties of the multivariate normal distribution, derive the posterior distribution $p(x \\mid y)$ by completing the square in the exponent of the product of the prior and likelihood. Express the posterior as a Gaussian distribution with mean $x_{a}$ and covariance $A$ in closed form. Then, using foundational linear algebra identities, verify analytically that the resulting posterior mean and covariance can be rewritten in the standard gain-based forms used in Optimal Interpolation (OI), without invoking any memorized formula as a starting point. Conclude by evaluating $x_{a}$ and $A$ numerically for the provided $x_{b}$, $B$, $H$, $R$, and $y$.\n\nFor grading, report the value of the first component of the analysis mean $x_{a}$, expressed exactly as a rational number. No rounding is required, and no physical units are involved.", "solution": "The linear-Gaussian data assimilation model specifies a Gaussian prior and a Gaussian likelihood. By Bayes' theorem, the posterior density is proportional to the product of the prior and likelihood:\n$$\np(x \\mid y) \\propto p(y \\mid x) p(x).\n$$\nWith $x \\sim \\mathcal{N}(x_{b}, B)$ and $y \\mid x \\sim \\mathcal{N}(H x, R)$, we have\n$$\np(x) \\propto \\exp\\!\\left( -\\tfrac{1}{2} (x - x_{b})^{\\top} B^{-1} (x - x_{b}) \\right),\n\\quad\np(y \\mid x) \\propto \\exp\\!\\left( -\\tfrac{1}{2} (y - H x)^{\\top} R^{-1} (y - H x) \\right).\n$$\nThus,\n$$\np(x \\mid y) \\propto \\exp\\!\\left( -\\tfrac{1}{2} \\left[ (x - x_{b})^{\\top} B^{-1} (x - x_{b}) + (y - H x)^{\\top} R^{-1} (y - H x) \\right] \\right).\n$$\nExpanding the quadratic form and collecting terms in $x$, we obtain the canonical quadratic form\n$$\n-\\tfrac{1}{2}\\left[ x^{\\top} \\left( B^{-1} + H^{\\top} R^{-1} H \\right) x - 2 x^{\\top} \\left( B^{-1} x_{b} + H^{\\top} R^{-1} y \\right) + \\text{const} \\right].\n$$\nCompleting the square yields a Gaussian posterior with precision\n$$\nS \\equiv B^{-1} + H^{\\top} R^{-1} H,\n$$\nposterior covariance\n$$\nA = S^{-1},\n$$\nand posterior mean\n$$\nx_{a} = A \\left( B^{-1} x_{b} + H^{\\top} R^{-1} y \\right).\n$$\nThese expressions follow directly from completing the square in the exponent of the Gaussian and represent the Bayesian solution.\n\nNext, we show how these expressions can be rewritten in the gain-based form associated with Optimal Interpolation (OI). Consider the matrix inversion lemma (also known as the Woodbury identity):\n$$\n\\left( B^{-1} + H^{\\top} R^{-1} H \\right)^{-1}\n= B - B H^{\\top} \\left( R + H B H^{\\top} \\right)^{-1} H B.\n$$\nDefine the gain\n$$\nK \\equiv B H^{\\top} \\left( H B H^{\\top} + R \\right)^{-1}.\n$$\nThen\n$$\nA = \\left( B^{-1} + H^{\\top} R^{-1} H \\right)^{-1} = B - B H^{\\top} \\left( R + H B H^{\\top} \\right)^{-1} H B = \\left( I - K H \\right) B.\n$$\nFor the mean, using $x_{a} = A \\left( B^{-1} x_{b} + H^{\\top} R^{-1} y \\right)$ and $A = \\left( I - K H \\right) B$, we have\n\\begin{align*}\nx_{a}\n= \\left( I - K H \\right) B \\left( B^{-1} x_{b} + H^{\\top} R^{-1} y \\right) \\\\\n= \\left( I - K H \\right) x_{b} + \\left( I - K H \\right) B H^{\\top} R^{-1} y.\n\\end{align*}\nBecause $K = B H^{\\top} \\left( H B H^{\\top} + R \\right)^{-1}$ and $R$ is symmetric positive definite, one can use algebra to show that\n$$\n\\left( I - K H \\right) B H^{\\top} R^{-1} = K,\n$$\nwhich implies\n$$\nx_{a} = x_{b} + K \\left( y - H x_{b} \\right).\n$$\nThis is the gain-based form associated with Optimal Interpolation (OI), derived from the canonical Bayesian expressions without invoking any shortcut formulas at the outset.\n\nWe now evaluate $x_{a}$ and $A$ numerically for the given $x_{b}$, $B$, $H$, $R$, and $y$.\n\nFirst compute\n$$\nB^{-1} = \\begin{pmatrix} \\tfrac{1}{4}  0 \\\\ 0  1 \\end{pmatrix}, \\quad\nH^{\\top} R^{-1} H = H^{\\top} H = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\begin{pmatrix} 1  2 \\end{pmatrix}\n= \\begin{pmatrix} 1  2 \\\\ 2  4 \\end{pmatrix}.\n$$\nTherefore,\n$$\nS = B^{-1} + H^{\\top} R^{-1} H\n= \\begin{pmatrix} \\tfrac{1}{4}  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} 1  2 \\\\ 2  4 \\end{pmatrix}\n= \\begin{pmatrix} \\tfrac{5}{4}  2 \\\\ 2  5 \\end{pmatrix}.\n$$\nThe determinant of $S$ is\n$$\n\\det(S) = \\left( \\tfrac{5}{4} \\right) \\cdot 5 - 2 \\cdot 2 = \\tfrac{25}{4} - 4 = \\tfrac{9}{4}.\n$$\nHence,\n$$\nA = S^{-1} = \\frac{1}{\\det(S)} \\begin{pmatrix} 5  -2 \\\\ -2  \\tfrac{5}{4} \\end{pmatrix}\n= \\frac{4}{9} \\begin{pmatrix} 5  -2 \\\\ -2  \\tfrac{5}{4} \\end{pmatrix}\n= \\begin{pmatrix} \\tfrac{20}{9}  -\\tfrac{8}{9} \\\\ -\\tfrac{8}{9}  \\tfrac{5}{9} \\end{pmatrix}.\n$$\nNext compute the posterior mean:\n$$\nB^{-1} x_{b} = \\begin{pmatrix} \\tfrac{1}{4}  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n= \\begin{pmatrix} \\tfrac{1}{4} \\\\ -1 \\end{pmatrix}, \\quad\nH^{\\top} R^{-1} y = H^{\\top} y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\cdot 3 = \\begin{pmatrix} 3 \\\\ 6 \\end{pmatrix}.\n$$\nThus\n$$\nB^{-1} x_{b} + H^{\\top} R^{-1} y = \\begin{pmatrix} \\tfrac{1}{4} + 3 \\\\ -1 + 6 \\end{pmatrix} = \\begin{pmatrix} \\tfrac{13}{4} \\\\ 5 \\end{pmatrix}.\n$$\nMultiplying by $A$ gives\n\\begin{align*}\nx_{a}\n= A \\begin{pmatrix} \\tfrac{13}{4} \\\\ 5 \\end{pmatrix}\n= \\begin{pmatrix} \\tfrac{20}{9}  -\\tfrac{8}{9} \\\\ -\\tfrac{8}{9}  \\tfrac{5}{9} \\end{pmatrix}\n\\begin{pmatrix} \\tfrac{13}{4} \\\\ 5 \\end{pmatrix} \\\\\n= \\begin{pmatrix}\n\\tfrac{20}{9} \\cdot \\tfrac{13}{4} - \\tfrac{8}{9} \\cdot 5 \\\\\n- \\tfrac{8}{9} \\cdot \\tfrac{13}{4} + \\tfrac{5}{9} \\cdot 5\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\tfrac{260}{36} - \\tfrac{40}{9} \\\\\n- \\tfrac{104}{36} + \\tfrac{25}{9}\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\tfrac{65}{9} - \\tfrac{40}{9} \\\\\n- \\tfrac{26}{9} + \\tfrac{25}{9}\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\tfrac{25}{9} \\\\\n- \\tfrac{1}{9}\n\\end{pmatrix}.\n\\end{align*}\nAlternatively, in the gain-based form, compute\n$$\nH B H^{\\top} = H \\left( B H^{\\top} \\right) = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix} = 8,\n\\quad\nK = B H^{\\top} \\left( H B H^{\\top} + R \\right)^{-1} = \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix} \\cdot \\frac{1}{9} = \\begin{pmatrix} \\tfrac{4}{9} \\\\ \\tfrac{2}{9} \\end{pmatrix},\n$$\nand the innovation\n$$\nd = y - H x_{b} = 3 - \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 3 - (1 - 2) = 4.\n$$\nThen\n$$\nx_{a} = x_{b} + K d = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} \\tfrac{4}{9} \\\\ \\tfrac{2}{9} \\end{pmatrix} \\cdot 4 = \\begin{pmatrix} 1 + \\tfrac{16}{9} \\\\ -1 + \\tfrac{8}{9} \\end{pmatrix} = \\begin{pmatrix} \\tfrac{25}{9} \\\\ - \\tfrac{1}{9} \\end{pmatrix},\n$$\nwhich matches the Bayesian-completed-square result. For the covariance,\n$$\nA = \\left( I - K H \\right) B\n= \\left( I - \\frac{1}{9} \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix} \\begin{pmatrix} 1  2 \\end{pmatrix} \\right) \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix}\n= \\left( I - \\frac{1}{9} \\begin{pmatrix} 4  8 \\\\ 2  4 \\end{pmatrix} \\right) \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 1 - \\tfrac{4}{9}  - \\tfrac{8}{9} \\\\ - \\tfrac{2}{9}  1 - \\tfrac{4}{9} \\end{pmatrix} \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix}\n= \\begin{pmatrix} \\tfrac{5}{9}  - \\tfrac{8}{9} \\\\ - \\tfrac{2}{9}  \\tfrac{5}{9} \\end{pmatrix} \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix}\n= \\begin{pmatrix} \\tfrac{20}{9}  - \\tfrac{8}{9} \\\\ - \\tfrac{8}{9}  \\tfrac{5}{9} \\end{pmatrix},\n$$\nwhich equals $S^{-1}$ computed above. This verifies analytically that the posterior mean and covariance derived from Bayesian principles coincide with the Optimal Interpolation (OI) gain-based forms.\n\nThe requested final quantity is the first component of $x_{a}$, which is $\\tfrac{25}{9}$, expressed exactly as a rational number.", "answer": "$$\\boxed{\\frac{25}{9}}$$", "id": "3407601"}, {"introduction": "To build a strong intuition for how Optimal Interpolation works, it is invaluable to analyze its behavior in the simplest settings. This practice strips the problem down to a scalar case, allowing you to derive the optimal gain, $k$, from scratch by directly minimizing the analysis error variance. By examining the limiting behaviors of $k$ as the background and observation uncertainties change, you will develop a clear understanding of how the gain intelligently weights information from the forecast and the measurements [@problem_id:3407588].", "problem": "Consider a linear-Gaussian data assimilation setting for Optimal Interpolation (OI). Let the unknown state be $x \\in \\mathbb{R}^{n}$ with a background (prior) $x \\sim \\mathcal{N}(x_{b}, B)$, where $x_{b} \\in \\mathbb{R}^{n}$ is the background mean and $B \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite background-error covariance. Observations are given by $y = H x + \\varepsilon$, where $H \\in \\mathbb{R}^{m \\times n}$ is a known linear observation operator and $\\varepsilon \\sim \\mathcal{N}(0, R)$ with $R \\in \\mathbb{R}^{m \\times m}$ symmetric positive definite, independent of the background error. Consider the affine family of linear estimators of the form $x_{a} = x_{b} + K \\left(y - H x_{b}\\right)$, where $K \\in \\mathbb{R}^{n \\times m}$ is a gain matrix to be determined by minimizing the mean-squared analysis error under the Gaussian assumptions.\n\n1. Specialize to a scalar setting $n = m = 1$ with $H = 1$, $B = b \\in \\mathbb{R}_{0}$, and $R = r \\in \\mathbb{R}_{0}$. Derive from first principles (starting from the definitions of the analysis error and its variance) the unique value $k \\in \\mathbb{R}$ that minimizes the expected squared analysis error, expressed explicitly in terms of $b$ and $r$. Then analyze, in this scalar case, the limiting behaviors of $k$ as $b \\to \\infty$ (non-informative prior) and as $r \\to 0$ (noiseless observations), explaining the epistemic meaning of each limit.\n\n2. Now consider the diagonal-matrix case with $H = I_{n}$, $B = \\operatorname{diag}(b_{1}, \\dots, b_{n})$ and $R = \\operatorname{diag}(r_{1}, \\dots, r_{n})$ with all $b_{i}, r_{i} \\in \\mathbb{R}_{0}$. Under the same optimality criterion, derive the explicit diagonal gain $K \\in \\mathbb{R}^{n \\times n}$ and analyze its limiting behaviors as $b_{i} \\to \\infty$ for all $i$ and as $r_{i} \\to 0$ for all $i$. Interpret the epistemic consequences of each limit.\n\nProvide your final answer as a single row matrix with six entries in the following order:\n- the scalar optimal gain $k$,\n- the limit $\\lim_{b \\to \\infty} k$,\n- the limit $\\lim_{r \\to 0} k$,\n- the diagonal optimal gain $K$,\n- the limit $\\lim_{b_{i} \\to \\infty \\ \\forall i} K$,\n- the limit $\\lim_{r_{i} \\to 0 \\ \\forall i} K$.\n\nNo numerical rounding is required. Do not include units. Express your final answer only in symbolic closed form.", "solution": "The problem statement is evaluated as valid. It is a well-posed, scientifically grounded, and objective problem formulation rooted in the fundamental principles of Optimal Interpolation within the field of data assimilation. All necessary data, definitions, and conditions are provided, and there are no internal contradictions or ambiguities. We may therefore proceed with the solution.\n\nThe problem asks for the optimal gain matrix $K$ that minimizes the mean-squared analysis error. The analysis state $x_a$ is an affine estimator given by $x_{a} = x_{b} + K \\left(y - H x_{b}\\right)$. The true state is $x$, with the background state $x_b$ representing a prior estimate, where the background error $e_b = x_b - x$ has zero mean and covariance $B = \\mathbb{E}[e_b e_b^T]$. The observations are $y = H x + \\varepsilon$, where the observation error $\\varepsilon$ has zero mean and covariance $R = \\mathbb{E}[\\varepsilon \\varepsilon^T]$. The background and observation errors are uncorrelated.\n\nThe analysis error is defined as $e_a = x_a - x$. We can express it in terms of the background and observation errors:\n$$e_a = \\left(x_b + K(y - Hx_b)\\right) - x = (x_b - x) + K(Hx + \\varepsilon - Hx_b) = (x_b - x) - KH(x_b - x) + K\\varepsilon$$\n$$e_a = (I - KH)e_b + K\\varepsilon$$\nThe analysis error covariance matrix $A$ is given by $A = \\mathbb{E}[e_a e_a^T]$. Since $e_b$ and $\\varepsilon$ are uncorrelated, the cross-terms vanish:\n$$A = \\mathbb{E}[((I - KH)e_b + K\\varepsilon)((I - KH)e_b + K\\varepsilon)^T] = (I - KH)\\mathbb{E}[e_b e_b^T](I - KH)^T + K\\mathbb{E}[\\varepsilon\\varepsilon^T]K^T$$\n$$A = (I - KH)B(I - KH)^T + KRK^T$$\nThe objective is to minimize the mean-squared analysis error, which is the trace of the analysis error covariance matrix, $J(K) = \\operatorname{tr}(A)$. The gain matrix $K$ that minimizes this cost function is given by the well-known Kalman gain formula:\n$$K = BH^T(HBH^T + R)^{-1}$$\n\n**Part 1: Scalar Case**\n\nIn this part, we specialize to a scalar setting where $n=m=1$, $H=1$, $B=b \\in \\mathbb{R}_{0}$, and $R=r \\in \\mathbb{R}_{0}$. The gain matrix $K$ becomes a scalar $k \\in \\mathbb{R}$. We are asked to derive the solution from first principles.\n\nThe scalar analysis error $e_a$ is given by $e_a = (1-k)e_b + k\\varepsilon$, where $e_b$ is the scalar background error and $\\varepsilon$ is the scalar observation error. The variances are $\\mathbb{E}[e_b^2] = b$ and $\\mathbb{E}[\\varepsilon^2] = r$.\nThe expected squared analysis error, which is the analysis error variance $a$, is the cost function to minimize:\n$$J(k) = a = \\mathbb{E}[e_a^2] = \\mathbb{E}[((1-k)e_b + k\\varepsilon)^2]$$\nExpanding and using the independence of $e_b$ and $\\varepsilon$ ($\\mathbb{E}[e_b\\varepsilon] = 0$), we get:\n$$J(k) = (1-k)^2\\mathbb{E}[e_b^2] + k^2\\mathbb{E}[\\varepsilon^2] = (1-k)^2 b + k^2 r$$\nTo find the optimal gain $k$ that minimizes this variance, we differentiate $J(k)$ with respect to $k$ and set the result to zero:\n$$\\frac{dJ}{dk} = \\frac{d}{dk}((1-2k+k^2)b + k^2r) = -2b + 2kb + 2kr = 2k(b+r) - 2b$$\nSetting the derivative to zero:\n$$2k(b+r) - 2b = 0 \\implies k(b+r) = b \\implies k = \\frac{b}{b+r}$$\n\nNow, we analyze the limiting behaviors of $k$:\n1.  As $b \\to \\infty$ (non-informative prior):\n    $$\\lim_{b \\to \\infty} k = \\lim_{b \\to \\infty} \\frac{b}{b+r} = \\lim_{b \\to \\infty} \\frac{1}{1 + \\frac{r}{b}} = \\frac{1}{1+0} = 1$$\n    **Epistemic Meaning:** When the background error variance $b$ is infinite, the prior information $x_b$ is considered completely unreliable. The optimal gain $k$ approaches $1$. The analysis update becomes $x_a = x_b + 1(y-x_b) = y$. This signifies that the background is entirely discarded, and the analysis is determined solely by the observation $y$.\n\n2.  As $r \\to 0$ (noiseless observations):\n    $$\\lim_{r \\to 0} k = \\lim_{r \\to 0} \\frac{b}{b+r} = \\frac{b}{b+0} = 1$$\n    **Epistemic Meaning:** When the observation error variance $r$ is zero, the observation $y$ is perfectly accurate. The optimal gain $k$ again approaches $1$, leading to $x_a = y$. This signifies that the perfect observation is trusted completely, overriding any uncertain information from the background.\n\n**Part 2: Diagonal-Matrix Case**\n\nHere, we consider $H=I_n$, $B = \\operatorname{diag}(b_{1}, \\dots, b_{n})$, and $R = \\operatorname{diag}(r_{1}, \\dots, r_{n})$. We use the general formula for the optimal gain $K$:\n$$K = B H^T (H B H^T + R)^{-1}$$\nSubstituting $H=I_n$:\n$$K = B I_n^T (I_n B I_n^T + R)^{-1} = B(B+R)^{-1}$$\nThe matrices $B$ and $R$ are diagonal, so their sum is also a diagonal matrix:\n$$B+R = \\operatorname{diag}(b_1, \\dots, b_n) + \\operatorname{diag}(r_1, \\dots, r_n) = \\operatorname{diag}(b_1+r_1, \\dots, b_n+r_n)$$\nThe inverse of a diagonal matrix is the diagonal matrix of the reciprocals of its diagonal elements:\n$$(B+R)^{-1} = \\operatorname{diag}\\left(\\frac{1}{b_1+r_1}, \\dots, \\frac{1}{b_n+r_n}\\right)$$\nFinally, we compute $K$ by multiplying the two diagonal matrices:\n$$K = \\operatorname{diag}(b_1, \\dots, b_n) \\operatorname{diag}\\left(\\frac{1}{b_1+r_1}, \\dots, \\frac{1}{b_n+r_n}\\right)$$\n$$K = \\operatorname{diag}\\left(\\frac{b_1}{b_1+r_1}, \\frac{b_2}{b_2+r_2}, \\dots, \\frac{b_n}{b_n+r_n}\\right)$$\nThe problem decouples into $n$ independent scalar problems, one for each component of the state vector.\n\nNow, we analyze the limiting behaviors of $K$:\n1.  As $b_i \\to \\infty$ for all $i$:\n    For each diagonal element $K_{ii} = \\frac{b_i}{b_i+r_i}$, the limit is $\\lim_{b_i \\to \\infty} \\frac{b_i}{b_i+r_i} = 1$. Therefore, the limit of the matrix is:\n    $$\\lim_{b_i \\to \\infty \\ \\forall i} K = \\operatorname{diag}(1, 1, \\dots, 1) = I_n$$\n    **Epistemic Meaning:** When the background uncertainties become infinite for all state components, the prior is non-informative. The gain matrix becomes the identity matrix $I_n$. The analysis update becomes $x_a = x_b + I_n(y - x_b) = y$. The analysis state is set equal to the observation vector, completely ignoring the unreliable background.\n\n2.  As $r_i \\to 0$ for all $i$:\n    For each diagonal element $K_{ii} = \\frac{b_i}{b_i+r_i}$, the limit is $\\lim_{r_i \\to 0} \\frac{b_i}{b_i+r_i} = \\frac{b_i}{b_i} = 1$. The limit of the matrix is:\n    $$\\lim_{r_i \\to 0 \\ \\forall i} K = \\operatorname{diag}(1, 1, \\dots, 1) = I_n$$\n    **Epistemic Meaning:** When the observation errors become zero for all components, the observations are perfect. The gain matrix becomes the identity matrix $I_n$, and the analysis update is $x_a = y$. Again, the analysis state is determined entirely by the observations, as they are known with absolute certainty.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{b}{b+r}  1  1  \\operatorname{diag}\\left(\\frac{b_1}{b_1+r_1}, \\frac{b_2}{b_2+r_2}, \\dots, \\frac{b_n}{b_n+r_n}\\right)  I_n  I_n\n\\end{pmatrix}\n}\n$$", "id": "3407588"}, {"introduction": "Applying data assimilation to a real-world problem requires translating physical measurements into a mathematical framework. A key step is constructing the observation operator, $H$, which maps the model's state space to the observation space. This exercise provides a concrete scenario involving point sensors on a grid, challenging you to build the matrix $H$ using linear interpolation before proceeding to calculate the full analysis update [@problem_id:3407564]. This practice bridges the gap between abstract theory and practical application.", "problem": "Consider a one-dimensional spatial domain with uniformly spaced grid nodes at positions $x_{0}=0$, $x_{1}=1$, $x_{2}=2$, and $x_{3}=3$. The model state is the vector $x \\in \\mathbb{R}^{4}$ whose components are the field values at these grid nodes. Two point sensors measure the field at positions $s_{1}=0.4$ and $s_{2}=2.3$. Each observation equals the true field at the sensor position plus an independent, zero-mean Gaussian error. Assume linear interpolation between neighboring grid nodes represents the continuous field implied by the discrete state $x$.\n\n1. Using the physical definition of point measurement and the standard linear interpolation rule on a uniform grid, define the linear observation operator $H$ that maps $x$ to the observation vector $y \\in \\mathbb{R}^{2}$. Write $H$ explicitly for the given sensor positions $s_{1}$ and $s_{2}$.\n\n2. In the linear-Gaussian optimal interpolation framework, assume a background error covariance $B=\\sigma_{b}^{2} I_{4}$ with $\\sigma_{b}=1.2$, and an observation error covariance $R=\\operatorname{diag}(\\sigma_{o,1}^{2},\\sigma_{o,2}^{2})$ with $\\sigma_{o,1}^{2}=0.04$ and $\\sigma_{o,2}^{2}=0.09$. Starting from the minimum-variance linear estimator for the analysis $x^{a}$ of the form $x^{a}=x^{b}+K\\left(y-H x^{b}\\right)$, derive the expression for the gain $K$ and the associated observation-space influence matrix $S$, defined as the linear map from observation innovations to analysis-equivalent innovations in observation space, $S=H K$. Then, evaluate the $(1,1)$ entry of $S$ for the concrete $H$, $B$, and $R$ obtained in this configuration.\n\nReport the numerical value of the $(1,1)$ entry of $S$ and round your answer to four significant figures.", "solution": "The problem is found to be valid as it is scientifically grounded, self-contained, and well-posed within the established framework of optimal interpolation.\n\nThe solution proceeds in two parts as requested. First, we derive the observation operator $H$. Second, we derive the optimal gain $K$ and the influence matrix $S$, and compute the specific entry $S_{11}$.\n\nPart 1: Derivation of the Observation Operator $H$\n\nThe model state is a vector $x \\in \\mathbb{R}^{4}$, where the components $x_i$ represent the field value at grid node $x_i$ for $i \\in \\{0, 1, 2, 3\\}$. The grid nodes are at positions $x_0=0$, $x_1=1$, $x_2=2$, and $x_3=3$. The grid spacing is uniform, $\\Delta x = 1$. The problem states that the continuous field is represented by linear interpolation between neighboring grid nodes.\n\nFor a point $s$ located between two grid nodes $x_i$ and $x_{i+1}$, the interpolated value $v(s)$ is a weighted average of the field values at these nodes, $x(x_i)$ and $x(x_{i+1})$. The general formula for linear interpolation is:\n$$v(s) = x(x_i) \\frac{x_{i+1} - s}{x_{i+1} - x_i} + x(x_{i+1}) \\frac{s - x_i}{x_{i+1} - x_i}$$\nGiven $\\Delta x = x_{i+1} - x_i = 1$, this simplifies to:\n$$v(s) = x(x_i) (x_{i+1} - s) + x(x_{i+1}) (s - x_i)$$\nThe observation operator $H$ is a $2 \\times 4$ matrix that maps the state vector $x = [x(x_0), x(x_1), x(x_2), x(x_3)]^T$ to the vector of observations $y = [v(s_1), v(s_2)]^T$. Each row of $H$ corresponds to one sensor.\n\nThe first sensor is at $s_1 = 0.4$. This position is between grid nodes $x_0 = 0$ and $x_1 = 1$. Applying the interpolation formula:\n$$v(s_1) = x(x_0) (1 - 0.4) + x(x_1) (0.4 - 0) = 0.6 \\cdot x(x_0) + 0.4 \\cdot x(x_1)$$\nThe other components of the state vector, $x(x_2)$ and $x(x_3)$, have no influence on this observation. Thus, the first row of $H$ is $[0.6, 0.4, 0, 0]$.\n\nThe second sensor is at $s_2 = 2.3$. This position is between grid nodes $x_2 = 2$ and $x_3 = 3$. Applying the interpolation formula:\n$$v(s_2) = x(x_2) (3 - 2.3) + x(x_3) (2.3 - 2) = 0.7 \\cdot x(x_2) + 0.3 \\cdot x(x_3)$$\nThe components $x(x_0)$ and $x(x_1)$ have no influence. Thus, the second row of $H$ is $[0, 0, 0.7, 0.3]$.\n\nCombining these rows, the linear observation operator $H$ is:\n$$H = \\begin{pmatrix} 0.6  0.4  0  0 \\\\ 0  0  0.7  0.3 \\end{pmatrix}$$\n\nPart 2: Derivation of $K$, $S$, and evaluation of $S_{11}$\n\nThe analysis state $x^a$ is given by the optimal interpolation equation:\n$$x^{a}=x^{b}+K(y-H x^{b})$$\nwhere $x^b$ is the background state. The optimal gain matrix $K$ that minimizes the analysis error variance is given by:\n$$K = B H^T (H B H^T + R)^{-1}$$\nHere, $B$ is the background error covariance matrix and $R$ is the observation error covariance matrix.\n\nThe observation-space influence matrix $S$ is defined as $S=HK$. Substituting the expression for $K$:\n$$S = H \\left( B H^T (H B H^T + R)^{-1} \\right) = (H B H^T) (H B H^T + R)^{-1}$$\n\nWe are given the following covariance matrices:\n- Background error covariance: $B = \\sigma_b^2 I_4$, with $\\sigma_b = 1.2$. So, $B = 1.2^2 I_4 = 1.44 I_4$.\n- Observation error covariance: $R = \\operatorname{diag}(\\sigma_{o,1}^2, \\sigma_{o,2}^2) = \\operatorname{diag}(0.04, 0.09)$, which is the matrix $\\begin{pmatrix} 0.04  0 \\\\ 0  0.09 \\end{pmatrix}$.\n\nTo calculate $S$, we first compute the matrix product $H B H^T$.\nSince $B = 1.44 I_4$, where $I_4$ is the $4 \\times 4$ identity matrix, we have:\n$$H B H^T = H (1.44 I_4) H^T = 1.44 H H^T$$\nLet's compute $H H^T$:\n$$H H^T = \\begin{pmatrix} 0.6  0.4  0  0 \\\\ 0  0  0.7  0.3 \\end{pmatrix} \\begin{pmatrix} 0.6  0 \\\\ 0.4  0 \\\\ 0  0.7 \\\\ 0  0.3 \\end{pmatrix}$$\n$$H H^T = \\begin{pmatrix} (0.6)^2 + (0.4)^2  0 \\\\ 0  (0.7)^2 + (0.3)^2 \\end{pmatrix} = \\begin{pmatrix} 0.36 + 0.16  0 \\\\ 0  0.49 + 0.09 \\end{pmatrix} = \\begin{pmatrix} 0.52  0 \\\\ 0  0.58 \\end{pmatrix}$$\nNow, we find $H B H^T$:\n$$H B H^T = 1.44 \\begin{pmatrix} 0.52  0 \\\\ 0  0.58 \\end{pmatrix} = \\begin{pmatrix} 1.44 \\times 0.52  0 \\\\ 0  1.44 \\times 0.58 \\end{pmatrix} = \\begin{pmatrix} 0.7488  0 \\\\ 0  0.8352 \\end{pmatrix}$$\nNext, we compute the matrix in the inverse, $H B H^T + R$:\n$$H B H^T + R = \\begin{pmatrix} 0.7488  0 \\\\ 0  0.8352 \\end{pmatrix} + \\begin{pmatrix} 0.04  0 \\\\ 0  0.09 \\end{pmatrix} = \\begin{pmatrix} 0.7888  0 \\\\ 0  0.9252 \\end{pmatrix}$$\nSince this matrix is diagonal, its inverse is straightforward to compute:\n$$(H B H^T + R)^{-1} = \\begin{pmatrix} \\frac{1}{0.7888}  0 \\\\ 0  \\frac{1}{0.9252} \\end{pmatrix}$$\nFinally, we compute $S$:\n$$S = (H B H^T) (H B H^T + R)^{-1} = \\begin{pmatrix} 0.7488  0 \\\\ 0  0.8352 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{0.7888}  0 \\\\ 0  \\frac{1}{0.9252} \\end{pmatrix}$$\n$$S = \\begin{pmatrix} \\frac{0.7488}{0.7888}  0 \\\\ 0  \\frac{0.8352}{0.9252} \\end{pmatrix}$$\nThe problem asks for the $(1,1)$ entry of $S$, which is $S_{11}$:\n$$S_{11} = \\frac{0.7488}{0.7888} \\approx 0.949289934...$$\nRounding this value to four significant figures gives $0.9493$.\nThe entry $S_{11}$ represents the fraction of the innovation from the first observation that is projected back onto the analysis field at the first observation location. A value close to $1$ implies that the background has relatively large error compared to the observation, so the analysis at that location strongly trusts the observation.", "answer": "$$\\boxed{0.9493}$$", "id": "3407564"}]}