{"hands_on_practices": [{"introduction": "The ability to correctly derive an adjoint model is the foundational skill for sensitivity analysis. This exercise provides crucial practice by applying the Lagrange multiplier method to a general two-stage Runge-Kutta scheme, a common class of numerical integrators [@problem_id:3363656]. Successfully working through this derivation builds the mechanical and conceptual understanding required to construct discrete adjoints for a wide variety of time-stepping models.", "problem": "Consider the parameter-dependent ordinary differential equation (ODE) initial value problem for a state $y \\in \\mathbb{R}^{d}$ and a parameter $m \\in \\mathbb{R}^{p}$,\n$$\n\\frac{d y(t)}{d t} = f(y(t), m, t), \\quad y(0) = y_{0},\n$$\nwhere $f:\\mathbb{R}^{d} \\times \\mathbb{R}^{p} \\times \\mathbb{R} \\to \\mathbb{R}^{d}$ is continuously differentiable in its arguments. Let the cost functional be a terminal misfit,\n$$\nJ(y_{N}) = \\frac{1}{2} \\left\\| H y_{N} - d \\right\\|_{R^{-1}}^{2} = \\frac{1}{2} (H y_{N} - d)^{\\top} R^{-1} (H y_{N} - d),\n$$\nwhere $H \\in \\mathbb{R}^{q \\times d}$, $d \\in \\mathbb{R}^{q}$, and $R \\in \\mathbb{R}^{q \\times q}$ is symmetric positive definite. Discretize the time interval $[0, T]$ into $N$ equal steps of size $\\Delta t = T/N$ with grid points $t_{n} = n \\Delta t$ for $n \\in \\{0, 1, \\ldots, N\\}$.\n\nUse a general two-stage Runge–Kutta (RK) scheme with Butcher coefficients $a_{ij}$, $b_{i}$, and $c_{i}$ for $i, j \\in \\{1, 2\\}$, not necessarily explicit, to advance $y_{n}$ to $y_{n+1}$. Define stage states $Y_{n}^{(i)}$ and stage right-hand sides $k_{n}^{(i)}$ at times $t_{n} + c_{i} \\Delta t$.\n\nTasks:\n- Write the two-stage RK stage equations that define $Y_{n}^{(i)}$ and $k_{n}^{(i)}$, and the state update equation for $y_{n+1}$ from $y_{n}$.\n- Form the discrete Lagrangian by introducing Lagrange multipliers for the state update and for each stage equation, treating $Y_{n}^{(i)}$ and $k_{n}^{(i)}$ as independent variables constrained by the RK relations and by $k_{n}^{(i)} - f(Y_{n}^{(i)}, m, t_{n} + c_{i} \\Delta t) = 0$.\n- Derive, by stationarity of the Lagrangian, the backward-in-time adjoint recurrence for the step multipliers associated with $y_{n}$ and the algebraic adjoint stage equations that couple the stage multipliers at each time step.\n- Finally, derive the discrete adjoint-based gradient with respect to the parameter $m$ as a single symbolic analytic expression in terms of the adjoint stage multipliers and $\\partial_{m} f$.\n\nAssume all necessary derivatives exist and are continuous, and assume $y_{0}$ does not depend on $m$. Express your final answer as a single symbolic analytic expression for the gradient with respect to $m$. No numerical evaluation is required. Do not include any units. No rounding is needed.", "solution": "The problem is well-posed, scientifically grounded in the field of inverse problems and optimal control, and contains all necessary information for a formal derivation. The task is to derive the discrete adjoint model and the corresponding gradient for a parameter-dependent ordinary differential equation (ODE) discretized by a general two-stage Runge-Kutta (RK) method.\n\nThe forward problem is defined by the ODE initial value problem\n$$\n\\frac{d y(t)}{d t} = f(y(t), m, t), \\quad y(0) = y_{0}\n$$\nfor a state $y \\in \\mathbb{R}^{d}$ and a parameter $m \\in \\mathbb{R}^{p}$. The cost functional to be minimized is\n$$\nJ(y_{N}) = \\frac{1}{2} (H y_{N} - d)^{\\top} R^{-1} (H y_{N} - d)\n$$\nwhere $y_N$ is the state at the final time $T$.\n\nFirst, we write the equations for the general two-stage Runge-Kutta scheme used to discretize the ODE over the time interval $[0, T]$ with time step $\\Delta t$. For each time step from $t_n$ to $t_{n+1}$ for $n \\in \\{0, 1, \\ldots, N-1\\}$, the scheme is defined by stage equations and a state update equation.\n\nThe stage equations define the stage states $Y_{n}^{(i)} \\in \\mathbb{R}^{d}$ and the stage right-hand sides $k_{n}^{(i)} \\in \\mathbb{R}^{d}$ for stages $i \\in \\{1, 2\\}$:\n$$\nY_{n}^{(i)} = y_n + \\Delta t \\sum_{j=1}^{2} a_{ij} k_n^{(j)}\n$$\n$$\nk_{n}^{(i)} = f(Y_n^{(i)}, m, t_n + c_i \\Delta t)\n$$\nThe state vector $y_n$ is then advanced to $y_{n+1}$ using the state update equation:\n$$\ny_{n+1} = y_n + \\Delta t \\sum_{i=1}^{2} b_i k_n^{(i)}\n$$\nwhere $y_n$ is the numerical approximation to $y(t_n)$.\n\nTo derive the adjoint model using the Lagrange multiplier method, we formulate the Lagrangian $\\mathcal{L}$. The objective is to minimize $J(y_N)$ subject to the constraints imposed by the RK scheme at each time step $n \\in \\{0, 1, \\ldots, N-1\\}$. We treat $y_n$, $Y_{n}^{(i)}$, and $k_{n}^{(i)}$ as independent variables. The constraints are:\n1. State update: $y_{n+1} - y_n - \\Delta t \\sum_{i=1}^{2} b_i k_n^{(i)} = 0$\n2. Stage state definition: $Y_n^{(i)} - y_n - \\Delta t \\sum_{j=1}^{2} a_{ij} k_n^{(j)} = 0$ for $i \\in \\{1, 2\\}$\n3. Stage RHS definition: $k_n^{(i)} - f(Y_n^{(i)}, m, t_n + c_i \\Delta t) = 0$ for $i \\in \\{1, 2\\}$\n\nWe introduce Lagrange multipliers (adjoint variables) for each constraint:\n- $\\lambda_{n+1} \\in \\mathbb{R}^{d}$ for the state update constraint at step $n$.\n- $\\mu_{n}^{(i)} \\in \\mathbb{R}^{d}$ for the stage state constraint for stage $i$ at step $n$.\n- $\\nu_{n}^{(i)} \\in \\mathbb{R}^{d}$ for the stage RHS constraint for stage $i$ at step $n$.\n\nThe discrete Lagrangian $\\mathcal{L}$ is the sum of the objective functional and the inner products of the multipliers with their respective constraints, summed over all time steps:\n$$\n\\mathcal{L} = J(y_N) + \\sum_{n=0}^{N-1} \\left[ \\lambda_{n+1}^{\\top} \\left(y_{n+1} - y_n - \\Delta t \\sum_{i=1}^{2} b_i k_n^{(i)}\\right) + \\sum_{i=1}^{2} (\\mu_n^{(i)})^{\\top} \\left(Y_n^{(i)} - y_n - \\Delta t \\sum_{j=1}^{2} a_{ij} k_n^{(j)}\\right) + \\sum_{i=1}^{2} (\\nu_n^{(i)})^{\\top} \\left(k_n^{(i)} - f(Y_n^{(i)}, m, t_n + c_i \\Delta t)\\right) \\right]\n$$\n\nThe adjoint equations are derived by enforcing stationarity of the Lagrangian with respect to the state variables $y_n$, $Y_n^{(i)}$, and $k_n^{(j)}$. The partial derivatives of $\\mathcal{L}$ with respect to these variables must be zero.\n\n1.  Derivative with respect to $y_N$:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial y_N} = \\frac{\\partial J}{\\partial y_N} + \\lambda_N^{\\top} = 0 \\implies \\lambda_N = -\\nabla_{y_N} J\n    $$\n    The gradient of the cost functional is $\\nabla_{y_N} J = H^{\\top} R^{-1} (H y_N - d)$. This gives the terminal condition for the adjoint state $\\lambda_N$:\n    $$\n    \\lambda_N = -H^{\\top} R^{-1} (H y_N - d)\n    $$\n\n2.  Derivative with respect to $y_n$ for $n \\in \\{1, \\ldots, N-1\\}$:\n    The terms involving $y_n$ appear in the summation at indices $n$ and $n-1$.\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial y_n} = \\lambda_n^{\\top} - \\lambda_{n+1}^{\\top} - \\sum_{i=1}^{2} (\\mu_n^{(i)})^{\\top} = 0\n    $$\n    This yields the backward recurrence for the adjoint state $\\lambda_n$, which is the multiplier associated with $y_n$:\n    $$\n    \\lambda_n = \\lambda_{n+1} + \\sum_{i=1}^{2} \\mu_n^{(i)}, \\quad \\text{for } n = N-1, \\ldots, 1\n    $$\n\n3.  Derivatives with respect to stage variables $Y_n^{(i)}$ and $k_n^{(j)}$ give the algebraic adjoint stage equations for each time step $n \\in \\{0, \\ldots, N-1\\}$.\n    Derivative with respect to $Y_n^{(i)}$ for $i \\in \\{1, 2\\}$:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial Y_n^{(i)}} = (\\mu_n^{(i)})^{\\top} - (\\nu_n^{(i)})^{\\top} \\frac{\\partial f}{\\partial y}(Y_n^{(i)}, m, t_n + c_i \\Delta t) = 0\n    $$\n    $$\n    \\mu_n^{(i)} = \\left(\\frac{\\partial f}{\\partial y}(Y_n^{(i)}, m, t_n + c_i \\Delta t)\\right)^{\\top} \\nu_n^{(i)}\n    $$\n    Derivative with respect to $k_n^{(j)}$ for $j \\in \\{1, 2\\}$:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial k_n^{(j)}} = -\\lambda_{n+1}^{\\top} \\Delta t \\, b_j - \\sum_{i=1}^{2} (\\mu_n^{(i)})^{\\top} \\Delta t \\, a_{ij} + (\\nu_n^{(j)})^{\\top} = 0\n    $$\n    $$\n    \\nu_n^{(j)} = \\Delta t \\left( b_j \\lambda_{n+1} + \\sum_{i=1}^{2} a_{ij} \\mu_n^{(i)} \\right)\n    $$\n    These two sets of equations at each step $n$ form a coupled linear system for the stage multipliers $\\mu_n^{(i)}$ and $\\nu_n^{(i)}$, which can be solved given $\\lambda_{n+1}$ from the previous (backward) step.\n\nFinally, we derive the expression for the gradient of the cost functional with respect to the parameter $m$. According to the adjoint method, if the forward and adjoint equations are satisfied, the total derivative of $J$ with respect to $m$ is equal to the partial derivative of the Lagrangian with respect to $m$. Since $y_0$ is assumed to be independent of $m$, we have:\n$$\n\\frac{d J}{d m} = \\frac{\\partial \\mathcal{L}}{\\partial m}\n$$\nThe only term in the Lagrangian that explicitly depends on $m$ is $f$.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial m} = \\sum_{n=0}^{N-1} \\sum_{i=1}^{2} (\\nu_n^{(i)})^{\\top} \\left( -\\frac{\\partial f}{\\partial m}(Y_n^{(i)}, m, t_n + c_i \\Delta t) \\right) = - \\sum_{n=0}^{N-1} \\sum_{i=1}^{2} (\\nu_n^{(i)})^{\\top} \\partial_m f(Y_n^{(i)}, \\ldots)\n$$\nThe gradient $\\nabla_m J$ is a column vector, which is the transpose of the row vector $dJ/dm$. Let $\\partial_m f$ denote the matrix of partial derivatives of $f$ with respect to the components of $m$.\n$$\n\\nabla_m J = \\left(\\frac{\\partial \\mathcal{L}}{\\partial m}\\right)^{\\top} = \\left( - \\sum_{n=0}^{N-1} \\sum_{i=1}^{2} (\\nu_n^{(i)})^{\\top} \\partial_m f(Y_n^{(i)}, \\ldots) \\right)^{\\top}\n$$\nThis results in the final expression for the gradient:\n$$\n\\nabla_m J = - \\sum_{n=0}^{N-1} \\sum_{i=1}^{2} \\left[ \\partial_m f(Y_n^{(i)}, m, t_n + c_i \\Delta t) \\right]^{\\top} \\nu_n^{(i)}\n$$\nHere, $\\nu_n^{(i)}$ are the adjoint stage multipliers obtained by solving the adjoint equations backward in time from $n=N-1$ to $n=0$.", "answer": "$$\n\\boxed{- \\sum_{n=0}^{N-1} \\sum_{i=1}^{2} \\left[ \\partial_m f(Y_n^{(i)}, m, t_n + c_i \\Delta t) \\right]^{\\top} \\nu_n^{(i)}}\n$$", "id": "3363656"}, {"introduction": "An implemented adjoint model is only useful if it is correct, and verification is a non-negotiable step in the development workflow. The \"dot-product test\" is a powerful and direct method to confirm that a coded operator satisfies the mathematical definition of an adjoint, $\\langle Lu, v \\rangle = \\langle u, L^*v \\rangle$ [@problem_id:3363680]. This practice focuses on formulating a robust test, especially when dealing with the weighted inner products frequently encountered in data assimilation and inverse problems.", "problem": "Consider a differentiable discrete forward model $F:\\mathbb{R}^{n}\\to\\mathbb{R}^{m}$ and a fixed state $x\\in\\mathbb{R}^{n}$. Let $X=\\mathbb{R}^{n}$ and $Y=\\mathbb{R}^{m}$ be endowed with weighted inner products defined by symmetric positive definite (SPD) matrices $W_{x}\\in\\mathbb{R}^{n\\times n}$ and $W_{y}\\in\\mathbb{R}^{m\\times m}$, respectively:\n$$\\langle u,v\\rangle_{X}=u^{\\top}W_{x}v,\\quad \\langle p,q\\rangle_{Y}=p^{\\top}W_{y}q.$$\nAssume that a tangent-linear implementation of the Jacobian $F'(x)$ is available to compute $F'(x)\\delta x$ for any perturbation $\\delta x\\in\\mathbb{R}^{n}$, and suppose you have implemented a candidate adjoint operator $A_{\\mathrm{adj}}:\\mathbb{R}^{m}\\to\\mathbb{R}^{n}$ intended to represent the adjoint of $F'(x)$ with respect to the given inner products on $X$ and $Y$. You wish to verify the correctness of $A_{\\mathrm{adj}}$ through a numerical test that uses randomly generated perturbations $\\delta x\\in\\mathbb{R}^{n}$ and $\\delta y\\in\\mathbb{R}^{m}$ and accounts for finite precision arithmetic via an appropriate tolerance that scales with the magnitudes of the quantities being compared.\n\nWhich option correctly formulates a robust dot-product test to validate $A_{\\mathrm{adj}}$ against $F'(x)$ under the weighted inner products on $X$ and $Y$?\n\nA. Draw independent random $\\delta x\\in\\mathbb{R}^{n}$ and $\\delta y\\in\\mathbb{R}^{m}$ from a standard normal distribution. Compute $s_{1}=\\delta y^{\\top}\\big(F'(x)\\delta x\\big)$ and $s_{2}=\\delta x^{\\top}\\big(F'(x)^{\\top}\\delta y\\big)$. Declare the adjoint implementation correct if $\\big|s_{1}-s_{2}\\big|\\leq \\varepsilon\\,\\|\\delta x\\|_{2}\\,\\|\\delta y\\|_{2}$ for a small tolerance $\\varepsilon$.\n\nB. Draw independent random $\\delta x\\in\\mathbb{R}^{n}$ and $\\delta y\\in\\mathbb{R}^{m}$ from a standard normal distribution. Compute $s_{1}=\\big(F'(x)\\delta x\\big)^{\\top}W_{y}\\delta y$ and $s_{2}=\\delta x^{\\top}W_{x}\\big(A_{\\mathrm{adj}}\\delta y\\big)$. Repeat for several trials and declare the adjoint implementation correct if the relative discrepancy satisfies\n$$\\frac{\\big|s_{1}-s_{2}\\big|}{\\big|s_{1}\\big|+\\big|s_{2}\\big|}\\leq \\varepsilon,$$\nwhere $\\varepsilon$ is chosen on the order of machine precision scaled by problem-dependent factors, and the inequality holds across trials.\n\nC. Draw independent random $\\delta x\\in\\mathbb{R}^{n}$ and $\\delta y\\in\\mathbb{R}^{m}$. Test $\\big\\|F'(x)\\delta x-A_{\\mathrm{adj}}\\delta y\\big\\|_{2}\\leq \\varepsilon$ for a small tolerance $\\varepsilon$, and declare success if the inequality holds across trials.\n\nD. Choose a single random $\\delta x\\in\\mathbb{R}^{n}$ and set $\\delta y=F'(x)\\delta x$. Test $\\big(F'(x)\\delta x\\big)^{\\top}\\delta y=\\delta x^{\\top}\\big(A_{\\mathrm{adj}}\\delta y\\big)$ using the standard Euclidean inner product and declare success if the equality holds within a small absolute tolerance $\\varepsilon$.", "solution": "The user wants me to validate a candidate adjoint implementation, $A_{\\mathrm{adj}}$, against a tangent-linear model, $F'(x)$.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n- A differentiable discrete forward model $F:\\mathbb{R}^{n}\\to\\mathbb{R}^{m}$.\n- A fixed state $x\\in\\mathbb{R}^{n}$.\n- The state space is $X=\\mathbb{R}^{n}$, equipped with the inner product $\\langle u,v\\rangle_{X}=u^{\\top}W_{x}v$.\n- The observation space is $Y=\\mathbb{R}^{m}$, equipped with the inner product $\\langle p,q\\rangle_{Y}=p^{\\top}W_{y}q$.\n- $W_{x}\\in\\mathbb{R}^{n\\times n}$ and $W_{y}\\in\\mathbb{R}^{m\\times m}$ are symmetric positive definite (SPD) matrices.\n- A tangent-linear implementation provides the action of the Jacobian, $F'(x)$, on a vector: $F'(x)\\delta x$.\n- A candidate adjoint operator implementation, $A_{\\mathrm{adj}}:\\mathbb{R}^{m}\\to\\mathbb{R}^{n}$, is available.\n- The objective is to formulate a robust numerical test, using random perturbations and a scaled tolerance, to verify that $A_{\\mathrm{adj}}$ is the correct adjoint of $F'(x)$ with respect to the given inner products.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Groundedness**: The problem is based on fundamental concepts of linear algebra and functional analysis, specifically the definition of an adjoint operator with respect to general inner products. The context is that of inverse problems and data assimilation, where tangent-linear and adjoint models are standard computational tools. This is a very common and practical problem in scientific computing. The problem is scientifically and mathematically sound.\n2.  **Well-Posedness**: The problem asks for the correct formulation of a numerical test. The definition of an adjoint operator is unique and unambiguous, leading to a single correct principle for the test. The problem is well-posed.\n3.  **Objectivity**: The language is formal, precise, and uses standard mathematical notation. It is free of any subjective or ambiguous terminology.\n4.  **Incomplete or Contradictory Setup**: The problem provides all necessary definitions and constraints to formulate the required test. The information is self-contained and consistent.\n5.  **Unrealistic or Infeasible**: The scenario described—verifying an adjoint code via a \"dot-product test\"—is a standard and essential procedure in the development of numerical models for optimization and data assimilation. It is highly realistic.\n6.  **Ill-Posed or Poorly Structured**: The question is clearly structured and asks for a specific outcome: a valid testing procedure.\n7.  **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial, as it requires the correct application of the definition of the adjoint in the presence of non-standard (weighted) inner products and an understanding of robust numerical comparison techniques.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will now proceed with the solution.\n\n### Principle-Based Derivation\nThe adjoint of a linear operator is defined by its behavior within the inner product of the relevant spaces. Let $L: X \\to Y$ be a linear operator between two inner product spaces $X$ and $Y$. The adjoint operator, denoted $L^*: Y \\to X$, is uniquely defined by the relation:\n$$ \\langle Lu, v \\rangle_Y = \\langle u, L^*v \\rangle_X, \\quad \\text{for all } u \\in X, v \\in Y $$\nIn our problem, the linear operator is the Jacobian (the tangent-linear model) $L = F'(x)$. The vector spaces are $X = \\mathbb{R}^n$ and $Y = \\mathbb{R}^m$. The vectors are perturbations $u = \\delta x \\in \\mathbb{R}^n$ and $v = \\delta y \\in \\mathbb{R}^m$. The inner products are the specified weighted inner products:\n- $\\langle \\cdot, \\cdot \\rangle_X = \\langle \\cdot, \\cdot \\rangle_{\\mathbb{R}^n, W_x}$\n- $\\langle \\cdot, \\cdot \\rangle_Y = \\langle \\cdot, \\cdot \\rangle_{\\mathbb{R}^m, W_y}$\n\nSubstituting these into the defining relation, the adjoint of $F'(x)$, denoted $(F'(x))^*$, must satisfy:\n$$ \\langle F'(x)\\delta x, \\delta y \\rangle_Y = \\langle \\delta x, (F'(x))^*\\delta y \\rangle_X $$\nfor all $\\delta x \\in \\mathbb{R}^n$ and $\\delta y \\in \\mathbb{R}^m$.\n\nNow, we express the inner products using their given matrix-vector definitions:\n- The left-hand side (LHS) is: $\\langle F'(x)\\delta x, \\delta y \\rangle_Y = \\big(F'(x)\\delta x\\big)^{\\top} W_y \\delta y$.\n- The right-hand side (RHS) is: $\\langle \\delta x, (F'(x))^*\\delta y \\rangle_X = \\delta x^{\\top} W_x \\big((F'(x))^*\\delta y\\big)$.\n\nTo verify that our candidate implementation $A_{\\mathrm{adj}}$ is correct, we must test if $A_{\\mathrm{adj}} \\approx (F'(x))^*$. This is done by choosing random vectors $\\delta x$ and $\\delta y$ and checking if the defining equality holds when $A_{\\mathrm{adj}}$ is substituted for $(F'(x))^*$. Let's define the two scalar quantities to be compared:\n$$ s_1 = \\big(F'(x)\\delta x\\big)^{\\top} W_y \\delta y $$\n$$ s_2 = \\delta x^{\\top} W_x \\big(A_{\\mathrm{adj}}\\delta y\\big) $$\nA numerical test must verify that $s_1 \\approx s_2$. In finite precision arithmetic, an exact equality $s_1 = s_2$ is not expected. The comparison must use a tolerance that is robust to the scale of $s_1$ and $s_2$. An absolute tolerance $|s_1 - s_2| \\leq \\varepsilon$ is not robust. A relative tolerance is required. A particularly robust form of relative error, which is well-behaved even when $s_1$ or $s_2$ is near zero, is\n$$ \\frac{|s_1 - s_2|}{|s_1| + |s_2|} \\leq \\varepsilon $$\nwhere $\\varepsilon$ is a small tolerance, typically on the order of machine precision. This is the foundation of a correct and robust \"dot-product test\" for the adjoint.\n\n### Option-by-Option Analysis\n\n**A. Draw independent random $\\delta x\\in\\mathbb{R}^{n}$ and $\\delta y\\in\\mathbb{R}^{m}$ from a standard normal distribution. Compute $s_{1}=\\delta y^{\\top}\\big(F'(x)\\delta x\\big)$ and $s_{2}=\\delta x^{\\top}\\big(F'(x)^{\\top}\\delta y\\big)$. Declare the adjoint implementation correct if $\\big|s_{1}-s_{2}\\big|\\leq \\varepsilon\\,\\|\\delta x\\|_{2}\\,\\|\\delta y\\|_{2}$ for a small tolerance $\\varepsilon$.**\n\nThis option has two fundamental flaws. First, it uses the standard Euclidean inner product (dot product), for which $W_x=I$ and $W_y=I$. The problem explicitly specifies weighted inner products with SPD matrices $W_x$ and $W_y$. Second, it tests the correctness of the matrix transpose $F'(x)^{\\top}$ as the adjoint, not the user-provided implementation $A_{\\mathrm{adj}}$. The goal is to validate the code $A_{\\mathrm{adj}}$.\n\n**Verdict: Incorrect.**\n\n**B. Draw independent random $\\delta x\\in\\mathbb{R}^{n}$ and $\\delta y\\in\\mathbb{R}^{m}$ from a standard normal distribution. Compute $s_{1}=\\big(F'(x)\\delta x\\big)^{\\top}W_{y}\\delta y$ and $s_{2}=\\delta x^{\\top}W_{x}\\big(A_{\\mathrm{adj}}\\delta y\\big)$. Repeat for several trials and declare the adjoint implementation correct if the relative discrepancy satisfies $\\frac{\\big|s_{1}-s_{2}\\big|}{\\big|s_{1}\\big|+\\big|s_{2}\\big|}\\leq \\varepsilon$, where $\\varepsilon$ is chosen on the order of machine precision scaled by problem-dependent factors, and the inequality holds across trials.**\n\nThis option correctly formulates the two sides of the adjoint identity using the specified weighted inner products.\n- $s_1 = \\big(F'(x)\\delta x\\big)^{\\top}W_{y}\\delta y = \\langle F'(x)\\delta x, \\delta y \\rangle_Y$.\n- $s_2 = \\delta x^{\\top}W_{x}\\big(A_{\\mathrm{adj}}\\delta y\\big) = \\langle \\delta x, A_{\\mathrm{adj}}\\delta y \\rangle_X$.\nComparing $s_1$ and $s_2$ tests the defining property of the adjoint for the candidate $A_{\\mathrm{adj}}$. It uses a robust relative error metric that scales with the magnitude of the computed values, as required for a robust numerical test. Using independent random vectors and repeating for several trials is standard best practice.\n\n**Verdict: Correct.**\n\n**C. Draw independent random $\\delta x\\in\\mathbb{R}^{n}$ and $\\delta y\\in\\mathbb{R}^{m}$. Test $\\big\\|F'(x)\\delta x-A_{\\mathrm{adj}}\\delta y\\big\\|_{2}\\leq \\varepsilon$ for a small tolerance $\\varepsilon$, and declare success if the inequality holds across trials.**\n\nThis comparison is mathematically and dimensionally nonsensical. The vector $F'(x)\\delta x$ is in $\\mathbb{R}^m$ (the observation space), while the vector $A_{\\mathrm{adj}}\\delta y$ is in $\\mathbb{R}^n$ (the state space). Unless $n=m$, the subtraction is not defined. Even if $n=m$, there is no principle stating that these two vectors should be close in any norm. The tangent-linear and adjoint operators map between different spaces.\n\n**Verdict: Incorrect.**\n\n**D. Choose a single random $\\delta x\\in\\mathbb{R}^{n}$ and set $\\delta y=F'(x)\\delta x$. Test $\\big(F'(x)\\delta x\\big)^{\\top}\\delta y=\\delta x^{\\top}\\big(A_{\\mathrm{adj}}\\delta y\\big)$ using the standard Euclidean inner product and declare success if the equality holds within a small absolute tolerance $\\varepsilon$.**\n\nThis option has multiple flaws. First, it uses the standard Euclidean inner product, ignoring the specified weighted inner products with matrices $W_x$ and $W_y$. Second, it does not use independent random vectors; instead, it creates a dependency $\\delta y = F'(x)\\delta x$. The adjoint property must hold for all pairs of vectors, not just this specific, highly constrained subset. Testing on such a small subspace is not a general or robust verification. Third, it uses a simple absolute tolerance, which is less robust than a relative tolerance.\n\n**Verdict: Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "3363680"}, {"introduction": "Modern applications in machine learning and data assimilation often involve models that run for very long time horizons, where standard adjoint methods become memory-prohibitive. This hands-on coding exercise addresses this challenge by implementing a checkpointing scheme, a technique that trades increased computation for significant memory savings [@problem_id:3363611]. By building and comparing a checkpointed adjoint for a Long Short-Term Memory (LSTM) network against standard backpropagation, you will gain practical insight into managing the critical memory-time trade-off in large-scale gradient computations.", "problem": "Consider a time-discrete surrogate model based on Long Short-Term Memory (LSTM), used inside gradient-based data assimilation loops such as four-dimensional variational assimilation. The LSTM cell updates a hidden state and cell state using standard gating. Let the input at time step $t$ be $x_t \\in \\mathbb{R}^m$, the hidden state be $h_t \\in \\mathbb{R}^n$, and the cell state be $c_t \\in \\mathbb{R}^n$. The trainable parameters are $\\theta$, consisting of affine maps for the four gates and an affine output map. The LSTM update is defined by\n$$\na^{(i)}_t = W^{(i)} x_t + U^{(i)} h_{t-1} + b^{(i)}, \\quad i_t = \\sigma\\!\\left(a^{(i)}_t\\right),\n$$\n$$\na^{(f)}_t = W^{(f)} x_t + U^{(f)} h_{t-1} + b^{(f)}, \\quad f_t = \\sigma\\!\\left(a^{(f)}_t\\right),\n$$\n$$\na^{(o)}_t = W^{(o)} x_t + U^{(o)} h_{t-1} + b^{(o)}, \\quad o_t = \\sigma\\!\\left(a^{(o)}_t\\right),\n$$\n$$\na^{(g)}_t = W^{(g)} x_t + U^{(g)} h_{t-1} + b^{(g)}, \\quad g_t = \\tanh\\!\\left(a^{(g)}_t\\right),\n$$\n$$\nc_t = f_t \\odot c_{t-1} + i_t \\odot g_t,\\quad h_t = o_t \\odot \\tanh\\!\\left(c_t\\right),\n$$\nwhere $\\sigma(\\cdot)$ is the logistic sigmoid, $\\tanh(\\cdot)$ is the hyperbolic tangent, and $\\odot$ denotes element-wise multiplication. The model output is\n$$\ny_t = V h_t + b_y,\n$$\nwith $V \\in \\mathbb{R}^{p \\times n}$ and $b_y \\in \\mathbb{R}^p$ included in $\\theta$. The assimilation cost over a horizon of $T$ steps is\n$$\nJ(\\theta) = \\frac{1}{2} \\sum_{t=1}^{T} \\left\\|y_t - y^{\\mathrm{obs}}_t\\right\\|_2^2,\n$$\nwhere $y^{\\mathrm{obs}}_t \\in \\mathbb{R}^p$ are given observations. Angles, if any, are not used in this problem. All quantities are unitless in this problem; do not introduce any physical units.\n\nTask: Starting only from the chain rule for discrete-time systems and the definitions above, derive the discrete adjoint recursion for the LSTM cell and the gradient $\\nabla_{\\theta} J$. Next, design and implement two gradient computation strategies:\n(1) standard Backpropagation Through Time (BPTT), which stores all intermediate states needed for the backward sweep; and\n(2) a checkpointed discrete adjoint with uniform segment length $s$, which stores states only at segment boundaries and recomputes forward states within segments during the backward sweep.\n\nYou must implement both strategies in a single runnable program that:\n(i) computes $\\nabla_{\\theta} J$ using BPTT,\n(ii) computes $\\nabla_{\\theta} J$ using the checkpointed adjoint, and\n(iii) reports the norm of the difference between the two gradient vectors for each test case. Additionally, for each test case, you must report theoretical memory and time costs for both strategies under the following normalized metrics:\n- Memory cost is the integer number of float scalars stored specifically for forward-state retention during the backward pass. For BPTT, assume storing the gate outputs $i_t$, $f_t$, $o_t$, $g_t$, and the states $c_t$, $h_t$ for all time steps, so the per-step storage is $6n$. For checkpointing, assume storing those quantities only for the current segment of length $s$ plus storing checkpoint states $(h,c)$ at each segment boundary and at the initial time. Therefore, memory is $6ns + 2n(S+1)$, where $S = \\lceil T/s \\rceil$.\n- Time cost is the integer number of LSTM cell forward/backward evaluations counted in units of per-step cell evaluations. For BPTT, count $2T$ (one forward and one backward sweep). For the checkpointed adjoint, count $3T$ (one initial forward sweep, one recomputation forward of length $T$ across all segments, and one backward sweep).\n\nYour program must execute the following test suite and aggregate the results:\n- Case $1$ (happy path): $T=64$, $m=4$, $n=3$, $p=2$, $s=16$.\n- Case $2$ (boundary where $s=T$): $T=64$, $m=4$, $n=3$, $p=2$, $s=64$.\n- Case $3$ (edge where $s=1$): $T=64$, $m=4$, $n=3$, $p=2$, $s=1$.\n\nFor each case, generate a fixed random input sequence $\\{x_t\\}_{t=1}^T$ and fixed observations $\\{y^{\\mathrm{obs}}_t\\}_{t=1}^T$ using a separate set of \"true\" parameters to ensure deterministic reproducibility. Initialize the assimilation model with a different set of parameters to create a nontrivial gradient. Use initial states $h_0 = 0$ and $c_0 = 0$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a five-element list $[\\Delta, M_{\\mathrm{BPTT}}, M_{\\mathrm{CHK}}, T_{\\mathrm{BPTT}}, T_{\\mathrm{CHK}}]$, where $\\Delta$ is the floating-point norm of the difference between the two gradient vectors, $M_{\\mathrm{BPTT}}$ and $M_{\\mathrm{CHK}}$ are integers, and $T_{\\mathrm{BPTT}}$ and $T_{\\mathrm{CHK}}$ are integers. The results for the three cases must be aggregated into a single list of lists with no additional text, for example: $[[\\Delta_1,M_1^{\\mathrm{B}},M_1^{\\mathrm{C}},T_1^{\\mathrm{B}},T_1^{\\mathrm{C}}],[\\Delta_2,\\dots],[\\Delta_3,\\dots]]$.", "solution": "The task is to derive the discrete adjoint model for a standard Long Short-Term Memory (LSTM) cell and to compute the gradient of a cost function, $\\nabla_{\\theta} J$, with respect to the model parameters $\\theta$. We will then implement two strategies for this computation: standard Backpropagation Through Time (BPTT) and a checkpointed adjoint method.\n\nLet us first formally state the forward propagation model and the cost function.\nThe state of the LSTM cell at time step $t$ is described by the hidden state $h_t \\in \\mathbb{R}^n$ and the cell state $c_t \\in \\mathbb{R}^n$. The model is driven by an external input $x_t \\in \\mathbb{R}^m$. The update from time $t-1$ to $t$ is given by the following set of equations:\n$$a^{(i)}_t = W^{(i)} x_t + U^{(i)} h_{t-1} + b^{(i)}$$\n$$i_t = \\sigma(a^{(i)}_t) \\quad (\\text{input gate})$$\n$$a^{(f)}_t = W^{(f)} x_t + U^{(f)} h_{t-1} + b^{(f)}$$\n$$f_t = \\sigma(a^{(f)}_t) \\quad (\\text{forget gate})$$\n$$a^{(o)}_t = W^{(o)} x_t + U^{(o)} h_{t-1} + b^{(o)}$$\n$$o_t = \\sigma(a^{(o)}_t) \\quad (\\text{output gate})$$\n$$a^{(g)}_t = W^{(g)} x_t + U^{(g)} h_{t-1} + b^{(g)}$$\n$$g_t = \\tanh(a^{(g)}_t) \\quad (\\text{cell input/candidate})$$\nThe cell and hidden states are then updated as:\n$$c_t = f_t \\odot c_{t-1} + i_t \\odot g_t$$\n$$h_t = o_t \\odot \\tanh(c_t)$$\nwhere $\\sigma(\\cdot)$ is the logistic sigmoid function, $\\tanh(\\cdot)$ is the hyperbolic tangent function, and $\\odot$ denotes element-wise (Hadamard) product. The model parameters $\\theta$ consist of the weight matrices $\\{W^{(k)}, U^{(k)}\\}_{k \\in \\{i,f,o,g\\}}$, bias vectors $\\{b^{(k)}\\}_{k \\in \\{i,f,o,g\\}}$, and the parameters of the output layer, $V \\in \\mathbb{R}^{p \\times n}$ and $b_y \\in \\mathbb{R}^p$.\n\nThe model output at time $t$ is an affine transformation of the hidden state:\n$$y_t = V h_t + b_y$$\nThe cost function $J(\\theta)$ to be minimized is the sum of squared errors over a time horizon of $T$ steps:\n$$J(\\theta) = \\frac{1}{2} \\sum_{t=1}^{T} \\|y_t - y^{\\mathrm{obs}}_t\\|_2^2$$\nwhere $y^{\\mathrm{obs}}_t \\in \\mathbb{R}^p$ are the observations. Initial states are given as $h_0 = 0$ and $c_0 = 0$.\n\nThe gradient $\\nabla_{\\theta} J$ is derived using the adjoint method, which is an application of the chain rule. Let $\\lambda_z \\equiv \\nabla_z J$ denote the adjoint (gradient) of the cost function $J$ with respect to any variable $z$. The adjoint variables propagate backward in time.\n\nThe derivatives of the activation functions are required:\n$$\\frac{d\\sigma(z)}{dz} = \\sigma(z)(1-\\sigma(z))$$\n$$\\frac{d\\tanh(z)}{dz} = 1 - \\tanh^2(z)$$\nFor a vector argument $z$, the Jacobian is a diagonal matrix. If $q = \\sigma(z)$, then $\\nabla_z q = \\mathrm{diag}(\\sigma(z) \\odot (1-\\sigma(z)))$. For brevity and since all operations are element-wise, we will use $\\sigma'(z)$ to denote the vector of element-wise derivatives.\n\nThe adjoint method proceeds backwards from $t=T$ to $t=1$. At each time step $t$, we compute the gradients with respect to the parameters at that step and then propagate the adjoint state to time $t-1$. Let $\\lambda_{h,t}^{\\text{next}}$ and $\\lambda_{c,t}^{\\text{next}}$ be the adjoints of $h_t$ and $c_t$ passed from the computations at step $t+1$. For the final step $t=T$, these are zero: $\\lambda_{h,T}^{\\text{next}} = 0$ and $\\lambda_{c,T}^{\\text{next}} = 0$.\n\nThe procedure at each time step $t$ (from $t=T$ down to $1$) is as follows:\n\n1.  **Compute total adjoints for $h_t$ and $c_t$**:\n    The cost function $J$ depends on $h_t$ directly through the term $\\|y_t - y_t^{\\mathrm{obs}}\\|_2^2$ and indirectly through the states at $t+1$.\n    The adjoint of the cost function with respect to the output $y_t$ is $\\lambda_{y_t} = \\nabla_{y_t} J = y_t - y_t^{\\mathrm{obs}}$.\n    The total adjoint for $h_t$, denoted $\\lambda_{h,t}$, combines the local gradient from the cost at time $t$ and the propagated gradient from time $t+1$:\n    $$\\lambda_{h,t} = (\\nabla_{h_t} y_t)^T \\lambda_{y_t} + \\lambda_{h,t}^{\\text{next}} = V^T(y_t - y_t^{\\mathrm{obs}}) + \\lambda_{h,t}^{\\text{next}}$$\n    The state $c_t$ influences $J$ through $h_t$ and through the recurrence for $c_{t+1}$. The total adjoint for $c_t$, denoted $\\lambda_{c,t}$, is:\n    $$\\lambda_{c,t} = (\\nabla_{c_t} h_t)^T \\lambda_{h,t} + \\lambda_{c,t}^{\\text{next}} = \\lambda_{h,t} \\odot o_t \\odot (1 - \\tanh^2(c_t)) + \\lambda_{c,t}^{\\text{next}}$$\n\n2.  **Compute adjoints for gates and pre-activations**:\n    Using the chain rule, we backpropagate through the cell structure:\n    $$\\lambda_{o_t} = (\\nabla_{o_t} h_t)^T \\lambda_{h,t} = \\lambda_{h,t} \\odot \\tanh(c_t)$$\n    $$\\lambda_{g_t} = (\\nabla_{g_t} c_t)^T \\lambda_{c_t} = \\lambda_{c_t} \\odot i_t$$\n    $$\\lambda_{i_t} = (\\nabla_{i_t} c_t)^T \\lambda_{c_t} = \\lambda_{c_t} \\odot g_t$$\n    $$\\lambda_{f_t} = (\\nabla_{f_t} c_t)^T \\lambda_{c_t} = \\lambda_{c_t} \\odot c_{t-1}$$\n    Then, backpropagate through the activation functions to the pre-activation variables $a_t$:\n    $$\\lambda_{a^{(o)}_t} = \\lambda_{o_t} \\odot \\sigma'(a^{(o)}_t) = \\lambda_{o_t} \\odot o_t \\odot (1 - o_t)$$\n    $$\\lambda_{a^{(g)}_t} = \\lambda_{g_t} \\odot \\tanh'(a^{(g)}_t) = \\lambda_{g_t} \\odot (1 - g_t^2)$$\n    $$\\lambda_{a^{(i)}_t} = \\lambda_{i_t} \\odot \\sigma'(a^{(i)}_t) = \\lambda_{i_t} \\odot i_t \\odot (1 - i_t)$$\n    $$\\lambda_{a^{(f)}_t} = \\lambda_{f_t} \\odot \\sigma'(a^{(f)}_t) = \\lambda_{f_t} \\odot f_t \\odot (1 - f_t)$$\n\n3.  **Compute parameter gradients at step $t$**:\n    The gradients of the cost function with respect to the parameters are accumulated over all time steps. The contribution from step $t$ is:\n    For $k \\in \\{i,f,o,g\\}$:\n    $$\\nabla_{W^{(k)}_t} J = \\lambda_{a^{(k)}_t} x_t^T$$\n    $$\\nabla_{U^{(k)}_t} J = \\lambda_{a^{(k)}_t} h_{t-1}^T$$\n    $$\\nabla_{b^{(k)}_t} J = \\lambda_{a^{(k)}_t}$$\n    For the output layer:\n    $$\\nabla_{V_t} J = (y_t - y_t^{\\mathrm{obs}}) h_t^T$$\n    $$\\nabla_{b_{y,t}} J = y_t - y_t^{\\mathrm{obs}}$$\n    The total gradient is $\\nabla_{\\theta} J = \\sum_{t=1}^T \\nabla_{\\theta_t} J$.\n\n4.  **Propagate adjoints to time $t-1$**:\n    Finally, we compute the adjoints for $h_{t-1}$ and $c_{t-1}$ to be passed to the previous step.\n    $$\\lambda_{h,t-1}^{\\text{next}} = \\sum_{k \\in \\{i,f,o,g\\}} (\\nabla_{h_{t-1}} a^{(k)}_t)^T \\lambda_{a^{(k)}_t} = \\sum_{k \\in \\{i,f,o,g\\}} (U^{(k)})^T \\lambda_{a^{(k)}_t}$$\n    $$\\lambda_{c,t-1}^{\\text{next}} = (\\nabla_{c_{t-1}} c_t)^T \\lambda_{c_t} = \\lambda_{c_t} \\odot f_t$$\n    These become the `next` values for the $t-1$ iteration.\n\n**Implementation Strategies:**\n\n**1. Backpropagation Through Time (BPTT):**\nThis is the standard algorithm for training recurrent neural networks.\n- **Forward Pass:** Iterate from $t=1$ to $T$. At each step, compute the LSTM states and outputs. All intermediate variables ($x_t, h_{t-1}, c_{t-1}, i_t, f_t, o_t, g_t, c_t, h_t$) required for the backward pass are stored in memory for every time step.\n- **Backward Pass:** Iterate from $t=T$ to $1$. At each step, retrieve the stored forward-pass variables. Apply the adjoint equations derived above to compute parameter gradients and propagate the adjoint states $\\lambda_h$ and $\\lambda_c$ to the previous time step. The parameter gradients are accumulated.\n\n**2. Checkpointed Adjoint:**\nThis strategy reduces memory usage at the cost of increased computation. It is a form of recursive BPTT.\n- **Initial Forward Pass:** The model is run from $t=1$ to $T$. However, only the states $(h_t, c_t)$ at specific intervals, called checkpoints, are stored. Segments are of length $s$. Checkpoints are stored at times $t_k = k \\cdot s$.\n- **Backward Pass with Recomputation:** The backward pass proceeds segment by segment, from the last segment to the first. For each segment $[t_k+1, t_{k+1}]$:\n    a. The state $(h_{t_k}, c_{t_k})$ is loaded from the checkpoint memory.\n    b. A forward recomputation is performed over the segment to regenerate all the necessary intermediate variables. These are stored temporarily, for this segment only.\n    c. A backward pass is performed over the segment using the recomputed intermediates and the incoming adjoints from the next segment. This computes the parameter gradients for this segment and the adjoints $(\\lambda_{h,t_k}, \\lambda_{c,t_k})$ at the start of the segment.\n    d. The temporary storage for the segment is discarded.\nThis process avoids storing the entire history of the forward pass simultaneously.\nThe provided theoretical cost metrics are used for analysis, representing a trade-off: checkpointing uses less memory ($M_{\\mathrm{CHK}}  M_{\\mathrm{BPTT}}$ for small $s$) but more computation time ($T_{\\mathrm{CHK}} > T_{\\mathrm{BPTT}}$).\n\nThe implementation will construct both algorithms and verify that they produce numerically identical gradients, confirming the correctness of both the derivation and the implementation.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    class LSTM_Adjoint_Problem:\n        \"\"\"\n        Encapsulates the LSTM model, data, and gradient computation methods.\n        \"\"\"\n        def __init__(self, T, m, n, p, s, seed=123):\n            self.T, self.m, self.n, self.p, self.s = T, m, n, p, s\n            self.rng = np.random.default_rng(seed)\n            \n            self.true_params = self._init_params()\n            self.init_params = self._init_params()\n            \n            self._param_keys, self._param_shapes, self._param_sizes = self._get_param_spec(self.init_params)\n\n            self.x_seq = self.rng.standard_normal(size=(T, m))\n            self.y_obs_seq = self._generate_observations()\n            \n            self.h0 = np.zeros(n)\n            self.c0 = np.zeros(n)\n\n        def _init_params(self):\n            n, m, p = self.n, self.m, self.p\n            # Use Xavier/Glorot initialization for stability\n            return {\n                'W_i': self.rng.normal(0, 1/np.sqrt(m), (n, m)), 'U_i': self.rng.normal(0, 1/np.sqrt(n), (n, n)), 'b_i': np.zeros(n),\n                'W_f': self.rng.normal(0, 1/np.sqrt(m), (n, m)), 'U_f': self.rng.normal(0, 1/np.sqrt(n), (n, n)), 'b_f': np.zeros(n),\n                'W_o': self.rng.normal(0, 1/np.sqrt(m), (n, m)), 'U_o': self.rng.normal(0, 1/np.sqrt(n), (n, n)), 'b_o': np.zeros(n),\n                'W_g': self.rng.normal(0, 1/np.sqrt(m), (n, m)), 'U_g': self.rng.normal(0, 1/np.sqrt(n), (n, n)), 'b_g': np.zeros(n),\n                'V'  : self.rng.normal(0, 1/np.sqrt(n), (p, n)), 'b_y': np.zeros(p)\n            }\n\n        def _get_param_spec(self, params_dict):\n            keys = sorted(params_dict.keys())\n            shapes = {k: params_dict[k].shape for k in keys}\n            sizes = {k: np.prod(shapes[k]) for k in keys}\n            return keys, shapes, sizes\n\n        def _pack(self, params_dict):\n            return np.concatenate([params_dict[k].flatten() for k in self._param_keys])\n\n        def _generate_observations(self):\n            h, c = np.zeros(self.n), np.zeros(self.n)\n            y_obs_seq = []\n            for t in range(self.T):\n                h, c, _ = self._forward_step(self.true_params, self.x_seq[t], h, c)\n                y_t = self.true_params['V'] @ h + self.true_params['b_y']\n                y_obs_seq.append(y_t)\n            return np.array(y_obs_seq)\n\n        @staticmethod\n        def _sigmoid(x):\n            return 1 / (1 + np.exp(-x))\n\n        def _forward_step(self, params, x_t, h_prev, c_prev):\n            a_i = params['W_i'] @ x_t + params['U_i'] @ h_prev + params['b_i']\n            i_t = self._sigmoid(a_i)\n            a_f = params['W_f'] @ x_t + params['U_f'] @ h_prev + params['b_f']\n            f_t = self._sigmoid(a_f)\n            a_o = params['W_o'] @ x_t + params['U_o'] @ h_prev + params['b_o']\n            o_t = self._sigmoid(a_o)\n            a_g = params['W_g'] @ x_t + params['U_g'] @ h_prev + params['b_g']\n            g_t = np.tanh(a_g)\n            \n            c_t = f_t * c_prev + i_t * g_t\n            h_t = o_t * np.tanh(c_t)\n            \n            cache = (x_t, h_prev, c_prev, i_t, f_t, o_t, g_t, c_t, h_t)\n            return h_t, c_t, cache\n\n        def _backward_step(self, params, cache, grad_h_next, grad_c_next, y_obs_t):\n            x_t, h_prev, c_prev, i_t, f_t, o_t, g_t, c_t, h_t = cache\n            \n            y_t = params['V'] @ h_t + params['b_y']\n            grad_y = y_t - y_obs_t\n            \n            grad_h = params['V'].T @ grad_y + grad_h_next\n            tanh_c = np.tanh(c_t)\n            grad_c = grad_h * o_t * (1 - tanh_c**2) + grad_c_next\n            \n            grad_o = grad_h * tanh_c\n            grad_g = grad_c * i_t\n            grad_i = grad_c * g_t\n            grad_f = grad_c * c_prev\n\n            grad_a_o = grad_o * o_t * (1 - o_t)\n            grad_a_g = grad_g * (1 - g_t**2)\n            grad_a_i = grad_i * i_t * (1 - i_t)\n            grad_a_f = grad_f * f_t * (1 - f_t)\n\n            d_params = {\n                'W_i': np.outer(grad_a_i, x_t), 'U_i': np.outer(grad_a_i, h_prev), 'b_i': grad_a_i,\n                'W_f': np.outer(grad_a_f, x_t), 'U_f': np.outer(grad_a_f, h_prev), 'b_f': grad_a_f,\n                'W_o': np.outer(grad_a_o, x_t), 'U_o': np.outer(grad_a_o, h_prev), 'b_o': grad_a_o,\n                'W_g': np.outer(grad_a_g, x_t), 'U_g': np.outer(grad_a_g, h_prev), 'b_g': grad_a_g,\n                'V':   np.outer(grad_y, h_t), 'b_y': grad_y\n            }\n            \n            grad_h_prev = (params['U_i'].T @ grad_a_i + params['U_f'].T @ grad_a_f + \n                           params['U_o'].T @ grad_a_o + params['U_g'].T @ grad_a_g)\n            grad_c_prev = grad_c * f_t\n            \n            return d_params, grad_h_prev, grad_c_prev\n\n        def compute_gradient_bptt(self):\n            caches = []\n            h, c = self.h0, self.c0\n            for t in range(self.T):\n                h, c, cache = self._forward_step(self.init_params, self.x_seq[t], h, c)\n                caches.append(cache)\n            \n            grads = {k: np.zeros_like(v) for k, v in self.init_params.items()}\n            grad_h_next, grad_c_next = np.zeros(self.n), np.zeros(self.n)\n            for t in reversed(range(self.T)):\n                d_params, grad_h_next, grad_c_next = self._backward_step(\n                    self.init_params, caches[t], grad_h_next, grad_c_next, self.y_obs_seq[t])\n                for k in grads:\n                    grads[k] += d_params[k]\n            return grads\n            \n        def compute_gradient_checkpointing(self):\n            params = self.init_params\n            s, T, n = self.s, self.T, self.n\n            \n            checkpoints = {0: (self.h0, self.c0)}\n            h, c = self.h0, self.c0\n            num_segments = math.ceil(T / s)\n            for i in range(1, num_segments + 1):\n                t_start_loop = (i-1)*s\n                t_end_loop = min(i*s, T)\n                for t in range(t_start_loop, t_end_loop):\n                    h, c, _ = self._forward_step(params, self.x_seq[t], h, c)\n                if t_end_loop  T:\n                    checkpoints[t_end_loop] = (h,c)\n\n            grads = {k: np.zeros_like(v) for k, v in params.items()}\n            grad_h_next, grad_c_next = np.zeros(n), np.zeros(n)\n            for i in reversed(range(1, num_segments + 1)):\n                t_start = (i - 1) * s\n                t_end = min(i * s, T)\n                \n                h, c = checkpoints[t_start]\n                segment_caches = []\n                for t in range(t_start, t_end):\n                    h, c, cache = self._forward_step(params, self.x_seq[t], h, c)\n                    segment_caches.append(cache)\n\n                for t_idx, t_glob in enumerate(reversed(range(t_start, t_end))):\n                    cache_idx = t_end - t_start - 1 - t_idx\n                    d_params, grad_h_next, grad_c_next = self._backward_step(\n                        params, segment_caches[cache_idx], grad_h_next, grad_c_next, self.y_obs_seq[t_glob])\n                    for k in grads:\n                        grads[k] += d_params[k]\n            return grads\n\n    test_cases = [\n        (64, 4, 3, 2, 16),\n        (64, 4, 3, 2, 64),\n        (64, 4, 3, 2, 1),\n    ]\n\n    results = []\n    for T, m, n, p, s in test_cases:\n        problem = LSTM_Adjoint_Problem(T, m, n, p, s)\n        \n        grads_bptt_dict = problem.compute_gradient_bptt()\n        grads_chk_dict = problem.compute_gradient_checkpointing()\n        \n        flat_grads_bptt = problem._pack(grads_bptt_dict)\n        flat_grads_chk = problem._pack(grads_chk_dict)\n        \n        grad_diff_norm = np.linalg.norm(flat_grads_bptt - flat_grads_chk)\n        \n        M_bptt = 6 * n * T\n        S = math.ceil(T / s)\n        M_chk = 6 * n * s + 2 * n * (S + 1)\n        T_bptt = 2 * T\n        T_chk = 3 * T\n        \n        results.append([grad_diff_norm, M_bptt, M_chk, T_bptt, T_chk])\n\n    inner_parts = []\n    for res in results:\n        inner_parts.append(f\"[{','.join(f'{v:.8e}' if isinstance(v, float) else str(v) for v in res)}]\")\n    final_str = f\"[{','.join(inner_parts)}]\"\n    print(final_str)\n\nsolve()\n```", "id": "3363611"}]}