{"hands_on_practices": [{"introduction": "The engine of variational data assimilation is gradient-based optimization. However, this engine is useless—and can even be misleading—if the gradient itself is calculated incorrectly. This practice introduces the Taylor remainder test, an essential numerical tool for verifying the implementation of the adjoint model used to compute the gradient [@problem_id:3408591]. By examining the convergence rate of the Taylor series remainder, you will learn to distinguish a correctly implemented gradient, which exhibits second-order convergence, from a faulty one.", "problem": "Construct a complete, runnable program that verifies the second-order consistency of an adjoint-derived gradient for a variational data assimilation objective using Taylor remainder tests. Begin from the fundamental definition of a strong-constraint variational data assimilation objective and Taylor’s theorem for scalar fields. You must derive the algorithmic form of the gradient using only core definitions and the chain rule, and then design numerical experiments to verify the expected second-order behavior of the Taylor remainder when the gradient is correct, and to detect its loss when the gradient is intentionally perturbed.\n\nYou are given the following setting. Let the state dimension be $n=5$ and the observation dimension be $m=3$. Define the background term with background state $x_b \\in \\mathbb{R}^n$ and symmetric positive-definite background covariance $B \\in \\mathbb{R}^{n \\times n}$. Define the forward model (single-step model operator) $M:\\mathbb{R}^n \\to \\mathbb{R}^n$ and the observation operator $H:\\mathbb{R}^n \\to \\mathbb{R}^m$. Let the observation $y \\in \\mathbb{R}^m$ and the symmetric positive-definite observation covariance $R \\in \\mathbb{R}^{m \\times m}$. Consider the objective\n$$\nJ(x) = \\tfrac{1}{2}\\,(x - x_b)^\\top B^{-1} (x - x_b) \\;+\\; \\tfrac{1}{2}\\,\\big(H(M(x)) - y\\big)^\\top R^{-1} \\big(H(M(x)) - y\\big).\n$$\nUse the following concrete, fully specified ingredients:\n\n- Dimensions: $n=5$, $m=3$.\n- Time step: $\\Delta t = 0.1$.\n- Background and observation covariances: construct $B$ and $R$ deterministically as follows. Let $L_B \\in \\mathbb{R}^{n \\times n}$ be the lower-triangular matrix with diagonal entries $[1.0,\\,1.2,\\,1.4,\\,1.6,\\,1.8]$ and sub-diagonal entries $0.05$ (all other entries $0$). Set $B = L_B L_B^\\top$. Let $L_R \\in \\mathbb{R}^{m \\times m}$ be the lower-triangular matrix with diagonal entries $[0.7,\\,0.9,\\,1.1]$ and sub-diagonal entries $0.02$, and set $R = L_R L_R^\\top$.\n- Background state: $x_b = [0.5,\\,-0.3,\\,0.8,\\,-1.0,\\,0.2]^\\top$.\n- Truth and observations: set $x_{\\text{true}} = [-0.4,\\,0.7,\\,-0.2,\\,0.3,\\,-0.6]^\\top$, define $y = H(M(x_{\\text{true}}))$ with no added noise.\n- Forward model: for any $x \\in \\mathbb{R}^n$, define $f(x) = \\sin(x) + 0.1\\,x \\odot x$ where $\\sin(\\cdot)$ and $\\odot$ act elementwise, and set $M(x) = x + \\Delta t\\, f(x)$.\n- Observation operator: let $C \\in \\mathbb{R}^{m \\times n}$ be\n$$\nC = \\begin{bmatrix}\n1 & 0 & 0.2 & 0 & 0\\\\\n0 & 0.5 & 0 & 0.1 & 0\\\\\n0 & 0 & 0 & 0.3 & 1\n\\end{bmatrix},\n$$\nand define the observation operator $H: \\mathbb{R}^n \\to \\mathbb{R}^m$ component-wise for $i=1, \\dots, m$ as $(H(z))_i = (C z)_i + 0.05 z_i^2$.\n- Evaluation state: define $x_0 = [0.1,\\, -0.2,\\, 0.3,\\, -0.4,\\, 0.5]^\\top$.\n- Direction vector: let $p$ be a deterministic unit vector defined as $p = \\frac{1}{\\|v\\|}v$ where $v = [1.0,\\, -2.0,\\, 3.0,\\, -4.0,\\, 5.0]^\\top$.\n\nUse basic matrix calculus and the chain rule to express the gradient $\\nabla J(x)$ in terms of $B^{-1}$, $R^{-1}$, and the Jacobians of $M$ and $H$. Then, implement the Taylor remainder test based on the identity from Taylor’s theorem for scalar fields:\n$$\nJ(x + \\epsilon p) - J(x) \\;=\\; \\epsilon\\, \\nabla J(x)^\\top p \\;+\\; \\mathcal{O}(\\epsilon^2),\n$$\nfor small $|\\epsilon|$, using a sequence of $\\epsilon$ values. For each test, compute the first-order remainder\n$$\nr_1(\\epsilon) \\;=\\; \\left|J(x + \\epsilon p) - J(x) - \\epsilon\\, \\nabla J(x)^\\top p\\right|.\n$$\nEstimate the empirical convergence rate $\\alpha$ by fitting a straight line to $\\log r_1(\\epsilon)$ versus $\\log \\epsilon$ and reporting the slope $\\alpha$, which should be approximately $2$ for a correct gradient.\n\nDesign and execute the following test suite, which must be built into the program with no user input:\n\n- Test A (nonlinear, correct gradient): use the $M$ and $H$ defined above and the analytically derived correct $\\nabla J(x)$. Evaluate the slope $\\alpha_A$ at $x=x_0$ along direction $p$, using a set of decreasing $\\epsilon$ values in $[10^{-1},\\, 10^{-5}]$ that includes both $10^{-k}$ and approximately geometric intermediates such as $3\\times 10^{-k}$ for integer $k$ where applicable.\n- Test B (nonlinear, intentionally faulty gradient): repeat Test A but replace the adjoint-propagation step by a deliberately incorrect variant that uses $M'(x)$ where $M'(x)^\\top$ is required in the gradient assembly. Denote the resulting slope by $\\alpha_B$; it should be close to $1$ rather than $2$.\n- Test C (linear-quadratic reference): define a linear model and observation by $M_{\\text{lin}}(x) = A x$ and $H_{\\text{lin}}(z) = C z$, where $A \\in \\mathbb{R}^{n \\times n}$ is\n$$\nA = I + \\Delta t\\, \\mathrm{diag}([0.2,\\,-0.1,\\,0.05,\\,-0.2,\\,0.1]),\n$$\nwith $I$ the identity and $\\mathrm{diag}(\\cdot)$ the diagonal matrix with the given entries. Use the same $C$, $B$, $R$, $x_b$, $x_0$, and $p$, and define $y_{\\text{lin}} = H_{\\text{lin}}(M_{\\text{lin}}(x_{\\text{true}}))$. Compute the slope $\\alpha_C$ using the exact gradient in this linear-quadratic setting; it should be near $2$ until roundoff dominates.\n- Test D (orthogonal direction): return to the nonlinear $M$ and $H$ and construct a unit direction $p_\\perp$ such that $\\nabla J(x_0)^\\top p_\\perp = 0$ numerically by projecting $p$ to be orthogonal to $\\nabla J(x_0)$ and normalizing. Compute the slope $\\alpha_D$ using $r_1(\\epsilon)$; it should be near $2$ because the first-order term vanishes in this direction.\n\nYour program must:\n\n- Implement $J(x)$ and its gradient using only basic linear algebra and the chain rule applied to the specified $M$ and $H$.\n- Implement the faulty gradient for Test B by replacing $M'(x)^\\top$ with $M'(x)$ in the adjoint accumulation.\n- Use a fixed sequence of $\\epsilon$ values containing $[10^{-1},\\,3\\cdot 10^{-2},\\,10^{-2},\\,3\\cdot 10^{-3},\\,10^{-3},\\,3\\cdot 10^{-4},\\,10^{-4},\\,3\\cdot 10^{-5},\\,10^{-5}]$.\n- Estimate slopes $\\alpha_A, \\alpha_B, \\alpha_C, \\alpha_D$ by least-squares linear regression of $\\log r_1(\\epsilon)$ against $\\log \\epsilon$ over the entire sequence, excluding any $\\epsilon$ for which $r_1(\\epsilon)$ underflows to zero in floating point.\n\nFinal Output Format: Your program should produce a single line of output containing the four estimated slopes, rounded to three decimal places, as a comma-separated list enclosed in square brackets, in the order $[\\alpha_A,\\alpha_B,\\alpha_C,\\alpha_D]$. No physical units are involved. All angles, if any, must be considered dimensionless real values. The outputs are real numbers.", "solution": "The problem requires the construction of a numerical experiment to verify the correctness of an adjoint-derived gradient for a variational data assimilation objective function. The verification is to be performed using a Taylor remainder test, which relies on the second-order behavior of the remainder term in a first-order Taylor expansion. The analysis begins with the formal derivation of the gradient and proceeds to the design and implementation of four specific numerical tests.\n\nFirst, we establish the theoretical foundation. The objective function $J(x)$ is given as a sum of a background term $J_b(x)$ and an observation term $J_o(x)$:\n$$\nJ(x) = J_b(x) + J_o(x) = \\tfrac{1}{2}\\,(x - x_b)^\\top B^{-1} (x - x_b) \\;+\\; \\tfrac{1}{2}\\,\\big(H(M(x)) - y\\big)^\\top R^{-1} \\big(H(M(x)) - y\\big)\n$$\nwhere $x \\in \\mathbb{R}^n$ is the control variable (the model state). The other quantities are defined in the problem statement: $x_b$ is the background state, $B$ is the background error covariance, $y$ is the observation vector, $R$ is the observation error covariance, $M$ is the forward model, and $H$ is the observation operator.\n\nThe verification method is based on Taylor's theorem for a scalar field $J:\\mathbb{R}^n \\to \\mathbb{R}$. Expanding $J(x)$ around a point $x_0$ along a direction $p \\in \\mathbb{R}^n$:\n$$\nJ(x_0 + \\epsilon p) = J(x_0) + \\epsilon\\, \\nabla J(x_0)^\\top p + \\tfrac{\\epsilon^2}{2} p^\\top \\nabla^2 J(x_0) p + \\mathcal{O}(\\epsilon^3)\n$$\nwhere $\\epsilon$ is a small scalar, $\\nabla J(x_0)$ is the gradient of $J$ at $x_0$, and $\\nabla^2 J(x_0)$ is the Hessian. If our computed gradient is correct, the first-order remainder, defined as\n$$\nr_1(\\epsilon) \\;=\\; \\left|J(x_0 + \\epsilon p) - J(x_0) - \\epsilon\\, \\nabla J(x_0)^\\top p\\right|\n$$\nmust be dominated by the second-order term:\n$$\nr_1(\\epsilon) = \\left|\\tfrac{\\epsilon^2}{2} p^\\top \\nabla^2 J(x_0) p + \\mathcal{O}(\\epsilon^3)\\right| \\approx C \\epsilon^2\n$$\nfor some constant $C$ (assuming $p^\\top \\nabla^2 J(x_0) p \\neq 0$). Taking the logarithm, we find $\\log r_1(\\epsilon) \\approx \\log C + 2 \\log \\epsilon$. This implies that a plot of $\\log r_1(\\epsilon)$ versus $\\log \\epsilon$ should be a straight line with a slope of approximately $2$. If the computed gradient is incorrect, the term $\\epsilon\\, \\nabla J(x_0)^\\top p$ will not correctly cancel the first-order variation in $J$, and the remainder $r_1(\\epsilon)$ will be of order $\\mathcal{O}(\\epsilon)$, leading to a slope of approximately $1$.\n\nNext, we derive the gradient $\\nabla J(x)$. The gradient of the background term $J_b(x)$ is standard for a quadratic form:\n$$\n\\nabla J_b(x) = B^{-1}(x - x_b)\n$$\nFor the observation term $J_o(x)$, we apply the chain rule. Let $d(x) = H(M(x)) - y$. The Jacobian of $d(x)$ with respect to $x$ is, by the chain rule, the product of the Jacobians of $H$ (evaluated at $M(x)$) and $M$ (evaluated at $x$):\n$$\n\\frac{\\partial d}{\\partial x} = H'(M(x)) M'(x)\n$$\nThe gradient of $J_o(x) = \\frac{1}{2} d(x)^\\top R^{-1} d(x)$ is then:\n$$\n\\nabla J_o(x) = \\left(\\frac{\\partial d}{\\partial x}\\right)^\\top R^{-1} d(x) = \\big(H'(M(x)) M'(x)\\big)^\\top R^{-1} \\big(H(M(x)) - y\\big)\n$$\nUsing the property $(AB)^\\top = B^\\top A^\\top$, we obtain the adjoint formulation:\n$$\n\\nabla J_o(x) = (M'(x))^\\top (H'(M(x)))^\\top R^{-1} \\big(H(M(x)) - y\\big)\n$$\nThe total gradient is the sum of the two components:\n$$\n\\nabla J(x) = B^{-1}(x - x_b) + (M'(x))^\\top (H'(M(x)))^\\top R^{-1} \\big(H(M(x)) - y\\big)\n$$\nTo implement this, we need the Jacobians of the specific model functions.\nThe forward model is $M(x) = x + \\Delta t\\, f(x)$ with $f(x)_i = \\sin(x_i) + 0.1\\, x_i^2$. Since $f$ acts elementwise, its Jacobian $f'(x)$ is a diagonal matrix with entries $\\frac{\\partial f_i}{\\partial x_i} = \\cos(x_i) + 0.2\\, x_i$. The Jacobian of $M(x)$ is therefore:\n$$\nM'(x) = I + \\Delta t\\, \\mathrm{diag}\\big(\\cos(x) + 0.2\\, x\\big)\n$$\nThe observation operator is given by $H(z)_i = \\sum_{k=1}^n C_{ik} z_k + 0.05 z_i^2$ for $i=1, \\dots, m$. Its Jacobian $H'(z) \\in \\mathbb{R}^{m \\times n}$ has entries:\n$$\n[H'(z)]_{ij} = \\frac{\\partial H_i}{\\partial z_j} = C_{ij} + 0.1 z_i \\delta_{ij}\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. This means $H'(z)$ is the matrix $C$ plus a diagonal matrix containing $0.1 z_i$ on its first $m$ diagonal entries.\n\nThe problem specifies four tests:\n**Test A (Nonlinear, Correct Gradient):** This test implements the derived gradient $\\nabla J(x_0)$ and is expected to yield a convergence rate $\\alpha_A \\approx 2$.\n\n**Test B (Nonlinear, Faulty Gradient):** This test is designed to use an incorrect gradient by replacing $(M'(x))^\\top$ with $M'(x)$. However, a critical observation is that the specified model $M(x)$ leads to a symmetric Jacobian, $M'(x) = (M'(x))^\\top$, because it is the sum of the identity matrix and a diagonal matrix. Consequently, the \"faulty\" gradient is identical to the correct one. The expectation stated in the problem that $\\alpha_B \\approx 1$ will not be met; instead, we must predict $\\alpha_B \\approx 2$. This outcome highlights an important aspect of verification: a test may not reveal an error if the specific data or model possesses symmetries that mask the bug.\n\n**Test C (Linear-Quadratic Reference):** For the linear model $M_{\\text{lin}}(x)=Ax$ and operator $H_{\\text{lin}}(z)=Cz$, the objective function is purely quadratic. The Taylor expansion is exact to second order, meaning $r_1(\\epsilon) = |\\frac{\\epsilon^2}{2} p^\\top \\nabla^2 J p|$. The relationship $\\log r_1(\\epsilon)$ vs. $\\log \\epsilon$ should be perfectly linear with a slope of exactly $2$, limited only by floating-point precision. This provides a baseline for a \"perfect\" second-order convergence.\n\n**Test D (Orthogonal Direction):** In this test, the direction vector $p_\\perp$ is constructed to be orthogonal to the gradient, i.e., $\\nabla J(x_0)^\\top p_\\perp = 0$. The Taylor expansion then simplifies to $J(x_0 + \\epsilon p_\\perp) - J(x_0) = \\mathcal{O}(\\epsilon^2)$, and the remainder becomes $r_1(\\epsilon) = |J(x_0 + \\epsilon p_\\perp) - J(x_0)|$. This remainder is still of order $\\mathcal{O}(\\epsilon^2)$, so the slope $\\alpha_D$ is expected to be near $2$. This test confirms that the quadratic behavior of the function is correctly captured even when the first-order term is identically zero.\n\nThe program implements these derivations and tests by defining functions for $J$, $\\nabla J$, the models, and their Jacobians. It then iterates through the specified $\\epsilon$ values for each of the four tests, calculates the remainders, and uses `numpy.polyfit` to perform a least-squares linear regression on the logarithms of the data to estimate the convergence slopes $\\alpha_A, \\alpha_B, \\alpha_C$, and $\\alpha_D$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not strictly needed as numpy.polyfit is sufficient.\n\ndef solve():\n    \"\"\"\n    Constructs and executes a test suite to verify an adjoint-derived gradient\n    for a variational data assimilation objective function using Taylor remainder tests.\n    \"\"\"\n    # --- Problem Setup: Constants and Givens ---\n    n = 5  # State dimension\n    m = 3  # Observation dimension\n    delta_t = 0.1\n\n    # Background state\n    xb = np.array([0.5, -0.3, 0.8, -1.0, 0.2])\n    # True state for generating observations\n    xtrue = np.array([-0.4, 0.7, -0.2, 0.3, -0.6])\n    # State for evaluation\n    x0 = np.array([0.1, -0.2, 0.3, -0.4, 0.5])\n\n    # Direction vector p\n    v = np.array([1.0, -2.0, 3.0, -4.0, 5.0])\n    p = v / np.linalg.norm(v)\n\n    # Covariance matrices B and R\n    L_B = np.diag([1.0, 1.2, 1.4, 1.6, 1.8])\n    for i in range(1, n):\n        L_B[i, i - 1] = 0.05\n    B = L_B @ L_B.T\n    B_inv = np.linalg.inv(B)\n\n    L_R = np.diag([0.7, 0.9, 1.1])\n    for i in range(1, m):\n        L_R[i, i - 1] = 0.02\n    R = L_R @ L_R.T\n    R_inv = np.linalg.inv(R)\n\n    # Observation operator matrix C\n    C = np.array([\n        [1, 0, 0.2, 0, 0],\n        [0, 0.5, 0, 0.1, 0],\n        [0, 0, 0, 0.3, 1]\n    ])\n\n    # Linear model matrix A for Test C\n    A = np.eye(n) + delta_t * np.diag([0.2, -0.1, 0.05, -0.2, 0.1])\n    \n    # Epsilon values for Taylor test\n    epsilons = np.array([\n        1e-1, 3e-2, 1e-2, 3e-3, 1e-3, 3e-4, 1e-4, 3e-5, 1e-5\n    ])\n\n    # --- Model and Operator Definitions ---\n\n    # Nonlinear forward model M(x)\n    def M_nl(x):\n        f = np.sin(x) + 0.1 * x**2\n        return x + delta_t * f\n\n    # Nonlinear observation operator H(z)\n    def H_nl(z):\n        h = C @ z\n        h += 0.05 * z[:m]**2\n        return h\n\n    # Jacobian of M_nl(x)\n    def M_nl_jac(x):\n        diag_f_prime = np.cos(x) + 0.2 * x\n        return np.eye(n) + delta_t * np.diag(diag_f_prime)\n\n    # Jacobian of H_nl(z)\n    def H_nl_jac(z):\n        jac = np.copy(C)\n        jac[:m, :m] += np.diag(0.1 * z[:m])\n        return jac\n\n    # Linear model and operator for Test C\n    def M_lin(x):\n        return A @ x\n    def H_lin(z):\n        return C @ z\n\n    # --- Objective Function and Gradient ---\n\n    def objective_J(x, y, M_func, H_func, B_inv, R_inv, xb):\n        J_b = 0.5 * (x - xb).T @ B_inv @ (x - xb)\n        hx = H_func(M_func(x))\n        J_o = 0.5 * (hx - y).T @ R_inv @ (hx - y)\n        return J_b + J_o\n\n    def gradient_J(x, y, M_func, H_func, B_inv, R_inv, xb, M_jac_T_provider, H_jac_func):\n        # Background gradient component\n        grad_b = B_inv @ (x - xb)\n        \n        # Observation gradient component (adjoint formulation)\n        z = M_func(x)\n        hx = H_func(z)\n        H_jac_val = H_jac_func(z) # H'(M(x))\n        M_jac_T_val = M_jac_T_provider(x) # M'(x)^T (or faulty version)\n        \n        # dJ/dH * dH/dz * dz/dx\n        # Adjoint variable: (H'(z))^T * R^-1 * (H(z) - y)\n        adj = H_jac_val.T @ (R_inv @ (hx - y))\n        # Propagate adjoint variable back through model: (M'(x))^T * adj\n        grad_o = M_jac_T_val @ adj\n        \n        return grad_b + grad_o\n\n    # --- Test Execution ---\n    \n    def run_taylor_test(x0_test, p_test, grad_j_val, M_func, H_func, y_test, B_inv, R_inv, xb):\n        J0 = objective_J(x0_test, y_test, M_func, H_func, B_inv, R_inv, xb)\n        gradJ_p = grad_j_val.T @ p_test\n        \n        log_eps, log_remainders = [], []\n        for eps in epsilons:\n            J_eps = objective_J(x0_test + eps * p_test, y_test, M_func, H_func, B_inv, R_inv, xb)\n            remainder = abs(J_eps - J0 - eps * gradJ_p)\n            if remainder > 0:\n                log_eps.append(np.log(eps))\n                log_remainders.append(np.log(remainder))\n        \n        # Perform linear regression to find slope\n        if len(log_eps) > 1:\n            slope, _ = np.polyfit(log_eps, log_remainders, 1)\n            return slope\n        return np.nan\n\n    all_slopes = []\n\n    # Generate observation data `y` for nonlinear tests\n    y_nl = H_nl(M_nl(xtrue))\n\n    # --- Test A: Nonlinear, Correct Gradient ---\n    grad_A = gradient_J(x0, y_nl, M_nl, H_nl, B_inv, R_inv, xb, \n                        lambda x: M_nl_jac(x).T, H_nl_jac)\n    alpha_A = run_taylor_test(x0, p, grad_A, M_nl, H_nl, y_nl, B_inv, R_inv, xb)\n    all_slopes.append(alpha_A)\n\n    # --- Test B: Nonlinear, Faulty Gradient ---\n    # Faulty implementation using M'(x) instead of M'(x)^T\n    # Note: For this problem's M, M'(x) is symmetric, so this \"faulty\"\n    # gradient is identical to the correct one. Expected slope is ~2.\n    grad_B = gradient_J(x0, y_nl, M_nl, H_nl, B_inv, R_inv, xb, \n                        lambda x: M_nl_jac(x), H_nl_jac) # Using jacobian, not its transpose\n    alpha_B = run_taylor_test(x0, p, grad_B, M_nl, H_nl, y_nl, B_inv, R_inv, xb)\n    all_slopes.append(alpha_B)\n\n    # --- Test C: Linear-Quadratic Reference ---\n    y_lin = H_lin(M_lin(xtrue))\n    \n    def M_lin_jac_T(x): # M_lin'(x) = A, so M_lin'(x)^T = A^T\n        return A.T\n    def H_lin_jac(z): # H_lin'(z) = C\n        return C\n        \n    grad_C = gradient_J(x0, y_lin, M_lin, H_lin, B_inv, R_inv, xb, M_lin_jac_T, H_lin_jac)\n    alpha_C = run_taylor_test(x0, p, grad_C, M_lin, H_lin, y_lin, B_inv, R_inv, xb)\n    all_slopes.append(alpha_C)\n    \n    # --- Test D: Orthogonal Direction ---\n    g_nl = grad_A # Use correct nonlinear gradient\n    # Project p to be orthogonal to the gradient g_nl\n    p_ortho = p - (g_nl.T @ p / (g_nl.T @ g_nl)) * g_nl\n    p_ortho_norm = p_ortho / np.linalg.norm(p_ortho)\n    \n    alpha_D = run_taylor_test(x0, p_ortho_norm, g_nl, M_nl, H_nl, y_nl, B_inv, R_inv, xb)\n    all_slopes.append(alpha_D)\n\n    # --- Final Output ---\n    formatted_slopes = [f\"{s:.3f}\" for s in all_slopes]\n    print(f\"[{','.join(formatted_slopes)}]\")\n\nsolve()\n```", "id": "3408591"}, {"introduction": "Once we have confidence in our gradient calculation, the next step is to use it to solve the optimization problem. In large-scale systems like incremental 4D-Var, the resulting quadratic minimization subproblems are solved using iterative methods, with the Conjugate Gradient (CG) algorithm being a popular choice. This practice focuses on implementing a \"matrix-free\" CG step, where the system matrix is never explicitly formed; instead, its action is computed by sequentially applying the forward model, observation operator, and their adjoints [@problem_id:3408588]. This technique is fundamental to the efficiency of modern data assimilation systems.", "problem": "You are given a linearized incremental Four-Dimensional Variational (4D-Var) data assimilation subproblem at a single inner-loop iteration in preconditioned control variables. Let the state increment be $\\,\\delta x \\in \\mathbb{R}^n\\,$ and the preconditioned control variable be $\\,v \\in \\mathbb{R}^n\\,$ defined by $\\,\\delta x = B^{1/2} v\\,$ where $\\,B \\in \\mathbb{R}^{n \\times n}\\,$ is the background-error covariance and $\\,B^{1/2}\\,$ is its symmetric positive-definite square root. Let $\\,M \\in \\mathbb{R}^{n \\times n}\\,$ denote the tangent linear model operator over the data assimilation window, $\\,H \\in \\mathbb{R}^{m \\times n}\\,$ the linearized observation operator, and $\\,R \\in \\mathbb{R}^{m \\times m}\\,$ the observation-error covariance with symmetric positive-definite square root $\\,R^{1/2}\\,$ and inverse square root $\\,R^{-1/2}\\,$. The background term in preconditioned variables is $\\,\\tfrac{1}{2}\\lVert v - v_b \\rVert_2^2\\,$ and the observation misfit is $\\,\\tfrac{1}{2}\\lVert R^{-1/2}(H M B^{1/2} v - d)\\rVert_2^2\\,$ where $\\,v_b \\in \\mathbb{R}^n\\,$ and $\\,d \\in \\mathbb{R}^m\\,$ are given.\n\nStarting from the quadratic objective in $\\,v\\,$, the first-order optimality (normal) equations for the inner-loop linear system take the form\n$$\nA v = b,\n$$\nwhere\n$$\nA \\equiv I + Z^\\top Z,\\quad Z \\equiv R^{-1/2} H M B^{1/2},\\quad b \\equiv v_b + Z^\\top R^{-1/2} d,\n$$\nand $\\,I\\,$ is the identity on $\\,\\mathbb{R}^n\\,$. The matrix $\\,A\\,$ is symmetric positive definite by construction.\n\nYour task is to write a program that performs one step of the Conjugate Gradient (CG) method for solving the symmetric positive-definite linear system $\\,A v = b\\,$ using only operator actions constructed from the given operators $\\,B^{-1/2}, M, H, R^{-1/2}\\,$ and the implied inverse $\\,B^{1/2} = \\left(B^{-1/2}\\right)^{-1}\\,$ (you may compute $\\,B^{1/2}\\,$ numerically by inverting $\\,B^{-1/2}\\,$ since the test cases are low-dimensional). Specifically, given an initial guess $\\,v_0\\,$, perform the following single CG iteration:\n- Compute the initial residual $\\,r_0 = b - A v_0\\,$.\n- Set the initial search direction $\\,p_0 = r_0\\,$.\n- Compute the step size\n$$\n\\alpha_0 = \\frac{r_0^\\top r_0}{p_0^\\top A p_0}.\n$$\n- Update the iterate $\\,v_1 = v_0 + \\alpha_0 p_0\\,$.\n- Update the residual $\\,r_1 = r_0 - \\alpha_0 A p_0\\,$.\nReport the Euclidean norm $\\,\\lVert r_1 \\rVert_2\\,$ after this single CG step for each test case.\n\nOperator implementation requirement: you must not assemble $\\,A\\,$ explicitly. Instead, implement the action $\\,y = A x\\,$ for an arbitrary $\\,x \\in \\mathbb{R}^n\\,$ using the compositions induced by $\\,Z\\,$ and $\\,Z^\\top\\,$, i.e.,\n$$\nA x = x + Z^\\top (Z x),\\quad Z x = R^{-1/2} H M B^{1/2} x,\\quad Z^\\top y = B^{1/2} M^\\top H^\\top R^{-1/2} y.\n$$\nAll transposes are the usual Euclidean adjoints.\n\nTest suite. For each case below, $\\,n\\,$ and $\\,m\\,$ denote state and observation dimensions, respectively. All matrices are small, real-valued, and the specified square-root matrices are symmetric positive definite. Use exactly the parameters given. Compute and return the single-step residual norm $\\,\\lVert r_1 \\rVert_2\\,$ for each test as a floating-point number.\n\n- Case $1$ (happy path, mixed-strength observations):\n  - $n = 3$, $m = 2$.\n  - $B^{-1/2} = \\mathrm{diag}(2, 1, 1/2)$.\n  - $M = \\begin{bmatrix}1 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 1\\end{bmatrix}$.\n  - $H = \\begin{bmatrix}1 & 0 & 0\\\\ 0 & 1 & 1\\end{bmatrix}$.\n  - $R^{-1/2} = \\mathrm{diag}(1, 2)$.\n  - $v_b = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$.\n  - $d = \\begin{bmatrix}1\\\\-1\\end{bmatrix}$.\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$.\n\n- Case $2$ (no observations; background-only):\n  - $n = 3$, $m = 2$.\n  - $B^{-1/2} = \\mathrm{diag}(1, 1, 1)$.\n  - $M = I$ of size $3$.\n  - $H = \\begin{bmatrix}0 & 0 & 0\\\\ 0 & 0 & 0\\end{bmatrix}$.\n  - $R^{-1/2} = I$ of size $2$.\n  - $v_b = \\begin{bmatrix}1\\\\-1\\\\1/2\\end{bmatrix}$.\n  - $d = \\begin{bmatrix}3\\\\-2\\end{bmatrix}$.\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$.\n\n- Case $3$ (very weak observations; nearly background-only):\n  - $n = 3$, $m = 2$.\n  - $B^{-1/2} = \\mathrm{diag}(4/5, 6/5, 1)$.\n  - $M = I$ of size $3$.\n  - $H = \\begin{bmatrix}1 & 0 & 0\\\\ 0 & 0 & 1\\end{bmatrix}$.\n  - $R^{-1/2} = \\mathrm{diag}(10^{-3}, 2 \\cdot 10^{-3})$.\n  - $v_b = \\begin{bmatrix}1/5\\\\-1/10\\\\1/20\\end{bmatrix}$.\n  - $d = \\begin{bmatrix}1/2\\\\-1\\end{bmatrix}$.\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$.\n\n- Case $4$ (dynamics and full observations):\n  - $n = 3$, $m = 3$.\n  - $B^{-1/2} = \\mathrm{diag}(3/2, 3/4, 1/2)$.\n  - $M = \\begin{bmatrix}1 & 1 & 0\\\\ 0 & 1 & 1\\\\ 0 & 0 & 1\\end{bmatrix}$.\n  - $H = I$ of size $3$.\n  - $R^{-1/2} = \\mathrm{diag}(1, 1/2, 1/4)$.\n  - $v_b = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$.\n  - $d = \\begin{bmatrix}1/2\\\\-1/2\\\\1\\end{bmatrix}$.\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$.\n\n- Case $5$ (general case with nonzero initial guess):\n  - $n = 3$, $m = 2$.\n  - $B^{-1/2} = \\mathrm{diag}(6/5, 9/10, 7/10)$.\n  - $M = \\begin{bmatrix}1 & 1/5 & 0\\\\ 0 & 1 & 3/10\\\\ 0 & 0 & 1\\end{bmatrix}$.\n  - $H = \\begin{bmatrix}1/2 & -1/2 & 0\\\\ 0 & 1 & -1\\end{bmatrix}$.\n  - $R^{-1/2} = \\mathrm{diag}(7/10, 13/10)$.\n  - $v_b = \\begin{bmatrix}1/10\\\\-1/5\\\\3/10\\end{bmatrix}$.\n  - $d = \\begin{bmatrix}-1/2\\\\4/5\\end{bmatrix}$.\n  - $v_0 = \\begin{bmatrix}1/20\\\\1/20\\\\-1/10\\end{bmatrix}$.\n\nYour program must:\n- Construct $\\,B^{1/2} = \\left(B^{-1/2}\\right)^{-1}\\,$ for each case numerically.\n- Implement $\\,y = A x\\,$ using only operator compositions with $\\,B^{1/2}, M, H, R^{-1/2}\\,$ and their transposes as needed.\n- For each case, perform exactly one Conjugate Gradient step as specified above and compute $\\,\\lVert r_1 \\rVert_2\\,$.\n- Output a single line containing the list of the five residual norms as a comma-separated list enclosed in square brackets, e.g., $[a_1,a_2,a_3,a_4,a_5]$ where each $\\,a_i\\,$ is a floating-point number.\n\nNo physical units or angles are involved in this problem. The final outputs are real numbers without units.", "solution": "The user has provided a valid problem statement.\n\n### Step 1: Extract Givens\n- **Problem Domain**: A linearized incremental Four-Dimensional Variational (4D-Var) data assimilation subproblem.\n- **System to Solve**: A symmetric positive-definite linear system $A v = b$, where $v \\in \\mathbb{R}^n$ is the preconditioned control variable.\n- **Matrix and Vector Definitions**:\n  - $A \\equiv I + Z^\\top Z$, where $I$ is the identity matrix on $\\mathbb{R}^n$.\n  - $Z \\equiv R^{-1/2} H M B^{1/2}$.\n  - $b \\equiv v_b + Z^\\top R^{-1/2} d$.\n- **Constituent Operators**:\n  - $B^{1/2} \\in \\mathbb{R}^{n \\times n}$: Symmetric positive-definite square root of the background-error covariance $B$. The problem provides its inverse, $B^{-1/2}$.\n  - $M \\in \\mathbb{R}^{n \\times n}$: Tangent linear model operator.\n  - $H \\in \\mathbb{R}^{m \\times n}$: Linearized observation operator.\n  - $R^{-1/2} \\in \\mathbb{R}^{m \\times m}$: Symmetric positive-definite inverse square root of the observation-error covariance $R$.\n- **Given Data**:\n  - $v_b \\in \\mathbb{R}^n$: A vector related to the background state.\n  - $d \\in \\mathbb{R}^m$: A vector related to the observation-model misfit (innovation vector).\n  - $v_0 \\in \\mathbb{R}^n$: The initial guess for the solution $v$.\n- **Task**: Perform a single step of the Conjugate Gradient (CG) algorithm.\n  1. Compute initial residual: $r_0 = b - A v_0$.\n  2. Set initial search direction: $p_0 = r_0$.\n  3. Compute step size: $\\alpha_0 = \\frac{r_0^\\top r_0}{p_0^\\top A p_0}$.\n  4. Update the iterate: $v_1 = v_0 + \\alpha_0 p_0$.\n  5. Update the residual: $r_1 = r_0 - \\alpha_0 A p_0$.\n- **Output Requirement**: Report the Euclidean norm of the updated residual, $\\|r_1\\|_2$, for each of the five provided test cases.\n- **Implementation Constraint**: The matrix $A$ must not be assembled explicitly. The action of $A$ on a vector $x$, i.e., $A x$, must be computed using compositions of the given operators: $A x = x + Z^\\top (Z x)$, where $Z x = R^{-1/2} H M B^{1/2} x$ and $Z^\\top y = B^{1/2} M^\\top H^\\top R^{-1/2} y$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded (Critical)**: The problem is a standard formulation in numerical weather prediction and geophysical data assimilation. The use of preconditioned CG to solve the normal equations of a quadratic cost function is a cornerstone technique in this field. All concepts are based on established principles of linear algebra, optimization, and inverse problem theory. The problem is scientifically sound.\n- **Well-Posed**: The system matrix $A = I + Z^\\top Z$ is symmetric and positive definite. The matrix $Z^\\top Z$ is symmetric positive semi-definite, and the addition of the identity matrix $I$ ensures that $A$ is strictly positive definite. Therefore, the linear system $Av=b$ has a unique solution. The Conjugate Gradient algorithm is a well-defined and convergent method for such systems. The problem is well-posed.\n- **Objective (Critical)**: The problem is specified with precise mathematical notation and provides explicit numerical data for all test cases. There is no ambiguity or subjectivity.\n- **Incomplete or Contradictory Setup**: The problem is self-contained. All matrices, vectors, and initial conditions required to perform the calculation are provided for each test case. Dimensions of all matrices and vectors are consistent. The derivation of $A$ and $b$ is consistent with the minimization of the standard 4D-Var quadratic cost function.\n- **Unrealistic or Infeasible**: The dimensions and values are chosen for computational convenience but are not physically impossible or scientifically implausible in the context of a discretized model.\n- **Conclusion**: The problem is valid, well-posed, and all necessary information for its solution is provided.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Principle-Based Design\nThe core of this problem is to solve the linear system $A v = b$, which represents the first-order optimality condition (or normal equations) for a quadratic minimization problem arising in 4D-Var data assimilation. The Conjugate Gradient (CG) method is an iterative algorithm ideally suited for solving large-scale symmetric positive-definite (SPD) systems, especially when the system matrix $A$ is not explicitly available, but its action on a vector can be computed efficiently. This is known as a \"matrix-free\" approach.\n\nOur solution proceeds by implementing a single step of the CG algorithm, strictly adhering to the matrix-free constraint.\n\n**1. Operator Definition and Action**\n\nThe system matrix is defined as $A \\equiv I + Z^\\top Z$, with $Z \\equiv R^{-1/2} H M B^{1/2}$. Instead of forming the dense matrix $A$, we implement a function that computes the product $y = Ax$ for any given vector $x$. This is achieved by composing the actions of the constituent operators:\n- The action of $Z$ on a vector $x$ is computed as a sequence of matrix-vector products: $Z x = R^{-1/2}(H(M(B^{1/2} x)))$.\n- The action of the adjoint $Z^\\top$ on a vector $y$ is similarly computed: $Z^\\top y = B^{1/2}(M^\\top(H^\\top(R^{-1/2} y)))$. Note that since $B^{1/2}$ and $R^{-1/2}$ are symmetric, their transposes are themselves.\n- The full action of $A$ on $x$ is then $A x = x + Z^\\top(Z x)$.\n\nThe matrix $B^{1/2}$ is required for these operations. It is computed once per test case by numerically inverting the given matrix $B^{-1/2}$, i.e., $B^{1/2} = (B^{-1/2})^{-1}$.\n\n**2. Right-Hand Side Construction**\n\nThe right-hand side vector $b$ is defined as $b \\equiv v_b + Z^\\top R^{-1/2} d$. This is also computed using operator actions. First, the vector $y_d = R^{-1/2} d$ is calculated. Then, the action of $Z^\\top$ is applied to $y_d$, and the result is added to $v_b$:\n$$b = v_b + Z^\\top(y_d) = v_b + B^{1/2}(M^\\top(H^\\top(R^{-1/2} (R^{-1/2} d))))$$\n\n**3. Single CG Iteration**\n\nWith the ability to compute $Ax$ and $b$, we perform exactly one iteration of the CG algorithm starting from the initial guess $v_0$.\n\n- **Initial Residual**: The algorithm begins by calculating the initial residual, which measures how far the initial guess is from satisfying the equation:\n$$r_0 = b - A v_0$$\nThe product $A v_0$ is computed using the matrix-free function described above.\n\n- **Initial Search Direction**: In the CG algorithm, the first search direction is simply the initial residual:\n$$p_0 = r_0$$\n\n- **Optimal Step Size**: We determine how far to move along the search direction $p_0$ by computing the step size $\\alpha_0$ that minimizes the residual in the A-norm. The formula is:\n$$\\alpha_0 = \\frac{r_0^\\top r_0}{p_0^\\top A p_0}$$\nThe denominator term $p_0^\\top A p_0$ is computed by first finding the vector $w = A p_0$ and then taking the dot product $p_0^\\top w$.\n\n- **Update Iterate and Residual**: Finally, we update the solution vector and the residual. The problem only asks for the norm of the new residual, $r_1$. The formula for updating the residual is computationally cheaper than re-evaluating $r_1 = b - A v_1$:\n$$r_1 = r_0 - \\alpha_0 A p_0$$\n\n- **Final Result**: The Euclidean norm of the new residual, $\\|r_1\\|_2 = \\sqrt{r_1^\\top r_1}$, is computed and reported for each test case. This value indicates the error reduction achieved in a single CG step.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_single_cg_step(case_data):\n    \"\"\"\n    Performs one step of the Conjugate Gradient method for a given test case.\n\n    Args:\n        case_data (dict): A dictionary containing all matrices and vectors for the test case.\n\n    Returns:\n        float: The Euclidean norm of the residual after one CG step, ||r_1||_2.\n    \"\"\"\n    # Unpack all data for the current case\n    B_inv_sqrt = case_data[\"B_inv_sqrt\"]\n    M = case_data[\"M\"]\n    H = case_data[\"H\"]\n    R_inv_sqrt = case_data[\"R_inv_sqrt\"]\n    v_b = case_data[\"v_b\"]\n    d = case_data[\"d\"]\n    v0 = case_data[\"v0\"]\n\n    # 1. Compute B_sqrt and required transposes\n    # As per the problem, B^{1/2} = (B^{-1/2})^{-1}\n    B_sqrt = np.linalg.inv(B_inv_sqrt)\n    M_T = M.T\n    H_T = H.T\n    # R_inv_sqrt is symmetric, so its transpose is itself.\n\n    # 2. Define the matrix-free operator actions\n    def Z_action(x):\n        \"\"\"Computes Zx = R^{-1/2} H M B^{1/2} x.\"\"\"\n        res = B_sqrt @ x\n        res = M @ res\n        res = H @ res\n        res = R_inv_sqrt @ res\n        return res\n\n    def Z_transpose_action(y):\n        \"\"\"Computes Z^T y = B^{1/2} M^T H^T R^{-1/2} y.\"\"\"\n        res = R_inv_sqrt @ y\n        res = H_T @ res\n        res = M_T @ res\n        res = B_sqrt @ res\n        return res\n\n    def A_action(x):\n        \"\"\"Computes Ax = (I + Z^T Z)x = x + Z^T(Z(x)).\"\"\"\n        return x + Z_transpose_action(Z_action(x))\n\n    # 3. Compute the right-hand side vector b\n    # b = v_b + Z^T (R^{-1/2} d)\n    term_in_Z_T = R_inv_sqrt @ d\n    b = v_b + Z_transpose_action(term_in_Z_T)\n\n    # 4. Perform one step of the Conjugate Gradient algorithm\n    # Compute the initial residual: r_0 = b - A v_0\n    Av0 = A_action(v0)\n    r0 = b - Av0\n    \n    # If the initial residual is zero, the initial guess is the exact solution.\n    # The next residual will also be zero.\n    if np.linalg.norm(r0) < 1e-15:\n        return 0.0\n\n    # Set the initial search direction: p_0 = r_0\n    p0 = r0\n    \n    # Compute the matrix-vector product A*p_0 needed for alpha_0 and r_1\n    Ap0 = A_action(p0)\n    \n    # Compute the step size: alpha_0 = (r_0^T r_0) / (p_0^T A p_0)\n    r0_dot_r0 = np.dot(r0, r0)\n    p0_dot_Ap0 = np.dot(p0, Ap0)\n\n    # Since A is SPD, p0_dot_Ap0 is zero iff p0 is zero.\n    # This is handled by the initial check on norm(r0).\n    # This check is for robustness.\n    if abs(p0_dot_Ap0) < 1e-15:\n        alpha0 = 0.0\n    else:\n        alpha0 = r0_dot_r0 / p0_dot_Ap0\n    \n    # Update the residual: r_1 = r_0 - alpha_0 * A * p_0\n    r1 = r0 - alpha0 * Ap0\n    \n    # 5. Compute and return the Euclidean norm of the new residual, ||r_1||\n    norm_r1 = np.linalg.norm(r1)\n    \n    return norm_r1\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, mixed-strength observations):\n        {\n            \"B_inv_sqrt\": np.diag([2.0, 1.0, 0.5]),\n            \"M\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 1.0]]),\n            \"R_inv_sqrt\": np.diag([1.0, 2.0]),\n            \"v_b\": np.array([0.0, 0.0, 0.0]),\n            \"d\": np.array([1.0, -1.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 2 (no observations; background-only):\n        {\n            \"B_inv_sqrt\": np.diag([1.0, 1.0, 1.0]),\n            \"M\": np.identity(3),\n            \"H\": np.zeros((2, 3)),\n            \"R_inv_sqrt\": np.identity(2),\n            \"v_b\": np.array([1.0, -1.0, 0.5]),\n            \"d\": np.array([3.0, -2.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 3 (very weak observations; nearly background-only):\n        {\n            \"B_inv_sqrt\": np.diag([4.0/5.0, 6.0/5.0, 1.0]),\n            \"M\": np.identity(3),\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"R_inv_sqrt\": np.diag([1e-3, 2e-3]),\n            \"v_b\": np.array([1.0/5.0, -1.0/10.0, 1.0/20.0]),\n            \"d\": np.array([0.5, -1.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 4 (dynamics and full observations):\n        {\n            \"B_inv_sqrt\": np.diag([3.0/2.0, 3.0/4.0, 1.0/2.0]),\n            \"M\": np.array([[1.0, 1.0, 0.0], [0.0, 1.0, 1.0], [0.0, 0.0, 1.0]]),\n            \"H\": np.identity(3),\n            \"R_inv_sqrt\": np.diag([1.0, 0.5, 0.25]),\n            \"v_b\": np.array([0.0, 0.0, 0.0]),\n            \"d\": np.array([0.5, -0.5, 1.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 5 (general case with nonzero initial guess):\n        {\n            \"B_inv_sqrt\": np.diag([6.0/5.0, 9.0/10.0, 7.0/10.0]),\n            \"M\": np.array([[1.0, 1.0/5.0, 0.0], [0.0, 1.0, 3.0/10.0], [0.0, 0.0, 1.0]]),\n            \"H\": np.array([[0.5, -0.5, 0.0], [0.0, 1.0, -1.0]]),\n            \"R_inv_sqrt\": np.diag([7.0/10.0, 13.0/10.0]),\n            \"v_b\": np.array([1.0/10.0, -2.0/10.0, 3.0/10.0]),\n            \"d\": np.array([-0.5, 4.0/5.0]),\n            \"v0\": np.array([1.0/20.0, 1.0/20.0, -1.0/10.0]),\n        }\n    ]\n\n    results = []\n    for case_data in test_cases:\n        norm_r1 = run_single_cg_step(case_data)\n        results.append(norm_r1)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3408588"}, {"introduction": "When developing an adjoint model for a system described by partial differential equations, two competing philosophies emerge: Discretize-Then-Optimize (DTO) and Optimize-Then-Discretize (OTD). While the DTO approach guarantees that you are computing the exact gradient of your discretized cost function, the OTD approach can inadvertently introduce inconsistencies between the discrete forward and adjoint operators. This practice explores the practical consequences of such an inconsistency, demonstrating how a seemingly minor mismatch can corrupt the gradient, hinder convergence, and ultimately lead to a biased solution [@problem_id:3408562].", "problem": "Consider a one-dimensional semilinear advection–diffusion–reaction state model and a distributed-parameter variational data assimilation problem. Let the state be a function $u: [0,1] \\to \\mathbb{R}$ and the control (unknown) be a function $m: [0,1] \\to \\mathbb{R}$. For given advection coefficient $b \\in \\mathbb{R}$ and nonlinearity coefficient $c \\ge 0$, the state $u$ is constrained by the semilinear boundary value problem\n$$\n- \\frac{d^2 u}{dx^2} + b \\frac{d u}{dx} + c\\, u^3 = m \\quad \\text{in } (0,1), \\qquad u(0)=0,\\; u(1)=0.\n$$\nAssume full-field observations with additive noise, $y(x) = u(x) + \\varepsilon(x)$, and the Tikhonov-regularized least-squares objective\n$$\nJ(m) = \\frac{1}{2} \\int_0^1 \\big(u(x) - y(x)\\big)^2 \\, dx + \\frac{\\alpha}{2} \\int_0^1 \\big(m(x) - m_{\\text{prior}}(x)\\big)^2 \\, dx,\n$$\nwith $\\alpha > 0$ and a given prior $m_{\\text{prior}}$. The forward map $m \\mapsto u$ is defined implicitly by the state equation.\n\nTwo algorithmic variants to compute gradients of the reduced objective $j(m) := J(u(m), m)$ are considered:\n\n- Discretize-Then-Optimize (DTO): Discretize the state equation and the objective first on a uniform grid, then derive the discrete adjoint and the discrete gradient of the discrete reduced objective.\n\n- Optimize-Then-Discretize (OTD): Derive the continuous adjoint and continuous $L^2$ gradient first, then discretize these optimality conditions to obtain a computable gradient approximation.\n\nYour task is to implement both variants and quantify the discretization-induced gradient mismatch and its effect on convergence and estimator bias, under a scientifically consistent and fully specified discretization. The implementation must adhere to the following requirements.\n\n1) Spatial discretization and quadrature:\n- Use a uniform grid with $n$ interior points, spacing $h = 1/(n+1)$, and interior nodes $x_i = i h$, for $i=1,\\dots,n$.\n- Discretize $-u''$ by the standard second-order central difference, i.e., the stiffness matrix $K \\in \\mathbb{R}^{n \\times n}$ with main diagonal $2/h^2$ and off-diagonals $-1/h^2$.\n- Discretize $b u'$ in the forward model using the first-order upwind difference for $b > 0$:\n$$\n\\left.\\frac{d u}{dx}\\right|_{x_i} \\approx \\frac{u_i - u_{i-1}}{h}, \\quad \\text{with } u_0 = 0,\n$$\nwhich yields a strictly lower-bidiagonal matrix $D_{\\text{up}} \\in \\mathbb{R}^{n \\times n}$ whose diagonal entries are $1/h$ and subdiagonal entries are $-1/h$, with the subdiagonal absent in the first row.\n- Discretize the cubic nonlinearity pointwise, i.e., define $(u^3)_i := u_i^3$.\n- Approximate all integrals in $J$ by the composite trapezoidal rule on the interior nodes, which in this setting reduces to the mass matrix $M = h I \\in \\mathbb{R}^{n \\times n}$ acting on interior vectors.\n\n2) Discrete forward problem:\n- For any control vector $m \\in \\mathbb{R}^n$, define the discrete forward residual $F(u, m) := (K + b D_{\\text{up}}) u + c\\, u^{\\odot 3} - m$, where $u^{\\odot 3}$ denotes the elementwise cube of $u$.\n- Compute $u(m) \\in \\mathbb{R}^n$ as the discrete state by solving $F(u, m) = 0$ using a Newton method with backtracking line search. The Jacobian with respect to $u$ is $J(u) := K + b D_{\\text{up}} + 3 c \\,\\mathrm{diag}(u^{\\odot 2})$.\n\n3) DTO adjoint and gradient:\n- Let $r(u, y) := u - y$ and the discrete objective be $J_h(u, m) := \\tfrac{1}{2} r^\\top M r + \\tfrac{\\alpha}{2} (m - m_{\\text{prior}})^\\top M (m - m_{\\text{prior}})$.\n- The discrete adjoint $\\lambda \\in \\mathbb{R}^n$ is defined by the transpose Jacobian system\n$$\nJ(u)^\\top \\lambda = M\\, r(u, y).\n$$\n- The DTO reduced gradient with respect to the Euclidean inner product on $\\mathbb{R}^n$ is\n$$\ng_{\\text{DTO}}(m) = \\alpha\\, M (m - m_{\\text{prior}}) - \\lambda.\n$$\n\n4) OTD adjoint and gradient:\n- The continuous adjoint $p$ solves $-p'' - b\\, p' + 3 c\\, u^2 p = u - y$ in $(0,1)$ with $p(0)=0,\\; p(1)=0$.\n- Discretize this continuous adjoint by replacing $-p''$ with $K p$, $-b p'$ by $-b D_{\\text{up}} p$ (i.e., reuse the same upwind operator as in the forward model instead of its transpose), and $3 c u^2 p$ by $3 c\\, \\mathrm{diag}(u^{\\odot 2}) p$, to obtain the linear system\n$$\n\\big(K - b D_{\\text{up}} + 3 c\\, \\mathrm{diag}(u^{\\odot 2})\\big) p = M\\, r(u, y).\n$$\n- Define the OTD reduced gradient as\n$$\ng_{\\text{OTD}}(m) = \\alpha\\, M (m - m_{\\text{prior}}) - p.\n$$\nNote: The only difference between DTO and OTD here is the treatment of the advection term in the adjoint. The DTO adjoint uses $J(u)^\\top$ which contains $+ b D_{\\text{up}}^\\top$, whereas OTD discretizes $-b p'$ using the same $D_{\\text{up}}$ as in the forward discretization, which is not the transpose. This intentional inconsistency induces a discretization-dependent gradient mismatch.\n\n5) Metrics to compute:\n- Gradient mismatch at the prior: For the initial guess $m_0 := m_{\\text{prior}}$, compute the relative Euclidean norm\n$$\n\\eta := \\frac{\\| g_{\\text{OTD}}(m_0) - g_{\\text{DTO}}(m_0) \\|_2}{\\| g_{\\text{DTO}}(m_0) \\|_2}.\n$$\n- One-step descent flags at the prior: For a trial step length $s_0 = 1$, evaluate the discrete objective $J_h$ at $m_0$ and at $m_0 - s_0 g_{\\text{DTO}}(m_0)$ and $m_0 - s_0 g_{\\text{OTD}}(m_0)$. Report two boolean flags indicating whether $J_h$ decreased in each case.\n- Convergence and bias: Starting from $m_0$, run steepest descent with Armijo backtracking line search for at most $K_{\\max} = 20$ iterations, once using $g_{\\text{DTO}}$ and once using $g_{\\text{OTD}}$. In each iteration, recompute the forward state and the corresponding gradient. Use Armijo parameters $c_{\\text{A}} = 10^{-4}$ and backtracking factor $\\beta = 1/2$, with initial trial step $s=1$. Terminate early if the relative gradient norm $\\|g\\|_2/(\\|g\\|_2 + 1) < 10^{-8}$; otherwise stop at the iteration limit.\nFor each run, report:\n    - The final discrete objective value $J_h$.\n    - The $L^2$ mean-squared error of the estimated control relative to the ground truth $m_{\\text{true}}$:\n$$\n\\mathrm{MSE} := \\int_0^1 \\big(m(x) - m_{\\text{true}}(x)\\big)^2 \\, dx \\approx (m - m_{\\text{true}})^\\top M (m - m_{\\text{true}}).\n$$\n\n6) Synthetic truth and observations:\n- For each test, define the ground-truth control $m_{\\text{true}}(x) := \\sin(2 \\pi x) + 0.5 \\cos(\\pi x)$ sampled at the interior nodes.\n- Define $m_{\\text{prior}}(x) \\equiv 0$.\n- Generate synthetic observations by solving the forward problem at $m_{\\text{true}}$ to obtain $u_{\\text{true}}$, then set $y = u_{\\text{true}} + \\varepsilon$, where $\\varepsilon$ is independent and identically distributed Gaussian noise with zero mean and standard deviation $10^{-3}$ times the empirical standard deviation of $u_{\\text{true}}$ on the grid. Use a fixed random seed to ensure reproducibility.\n\n7) Test suite:\nImplement and evaluate the following four parameter cases, each specified by the tuple $(n, b, c, \\alpha)$:\n- Case A (happy path, moderate advection): $(64, 5.0, 1.0, 10^{-2})$.\n- Case B (boundary condition with no advection, gradients should match to solver tolerance): $(64, 0.0, 1.0, 10^{-2})$.\n- Case C (edge case with strong advection on a coarser grid): $(32, 20.0, 1.0, 5 \\cdot 10^{-3})$.\n- Case D (refinement study, mismatch should be reduced): $(256, 5.0, 1.0, 10^{-2})$.\n\n8) Required final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output a list with seven entries in the following order:\n    - The gradient mismatch $\\eta$ as a float.\n    - The one-step descent flag for DTO at $m_0$ using $s_0=1$ as an integer ($1$ for true, $0$ for false).\n    - The one-step descent flag for OTD at $m_0$ using $s_0=1$ as an integer ($1$ for true, $0$ for false).\n    - The final $\\mathrm{MSE}$ from the DTO-based optimization as a float.\n    - The final $\\mathrm{MSE}$ from the OTD-based optimization as a float.\n    - The final discrete objective value $J_h$ from the DTO-based optimization as a float.\n    - The final discrete objective value $J_h$ from the OTD-based optimization as a float.\nAggregate the four case results into a single list in the order A, B, C, D. For example, the output must look like\n$[ [\\dots], [\\dots], [\\dots], [\\dots] ]$,\nwith numeric entries only and no additional text.", "solution": "The problem posed requires a comparative analysis of two common strategies in PDE-constrained optimization, specifically for a variational data assimilation problem: Discretize-Then-Optimize (DTO) and Optimize-Then-Discretize (OTD). We are tasked with implementing both approaches for a one-dimensional semilinear advection-diffusion-reaction equation, quantifying the discrepancy between the computed gradients, and evaluating the impact on an iterative optimization algorithm.\n\nThe core of the problem lies in the mathematical formulation of finding an unknown control parameter function $m(x)$ that best explains a set of noisy observations $y(x)$ of a state $u(x)$, where $u$ and $m$ are linked by a boundary value problem (BVP).\n\nThe state equation is a semilinear BVP:\n$$\n- \\frac{d^2 u}{dx^2} + b \\frac{d u}{dx} + c\\, u^3 = m \\quad \\text{for } x \\in (0,1),\n$$\nwith homogeneous Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$. This equation models stationary phenomena involving diffusion ($-u''$), advection ($b u'$), and a nonlinear reaction or source term ($c u^3$), driven by a source $m$.\n\nThe goal is to minimize a Tikhonov-regularized least-squares objective functional, which balances data fidelity and prior knowledge:\n$$\nJ(m) = \\frac{1}{2} \\int_0^1 \\big(u(x) - y(x)\\big)^2 \\, dx + \\frac{\\alpha}{2} \\int_0^1 \\big(m(x) - m_{\\text{prior}}(x)\\big)^2 \\, dx.\n$$\nThe first term measures the misfit between the model-predicted state $u(m)$ and the observations $y$, while the second term regularizes the problem by penalizing deviations of the control $m$ from a prior estimate $m_{\\text{prior}}$. The parameter $\\alpha > 0$ controls the strength of this regularization.\n\nTo solve this minimization problem using a gradient-based method (like steepest descent), we need the gradient of the reduced objective $j(m) = J(u(m), m)$. A direct computation would require the derivative of the map $m \\mapsto u$, which is computationally prohibitive. The adjoint method provides an efficient alternative. Here, the distinction between DTO and OTD arises.\n\n**1. Discretize-Then-Optimize (DTO)**\n\nIn the DTO approach, we first discretize all continuous entities. The spatial domain $[0,1]$ is replaced by a uniform grid of $n$ interior points. The state $u(x)$ and control $m(x)$ become vectors $u, m \\in \\mathbb{R}^n$. The differential operators are replaced by matrices. For an interior grid spacing $h=1/(n+1)$:\n- The diffusion operator $-d^2/dx^2$ is discretized using a second-order central difference matrix, the discrete Laplacian $K \\in \\mathbb{R}^{n \\times n}$.\n- The advection operator $b\\, d/dx$ (for $b>0$) is discretized using a first-order upwind scheme, yielding a matrix $b D_{\\text{up}}$. This choice is motivated by the stability of the numerical scheme for advection-dominated problems.\n- Integrals are approximated using the trapezoidal rule, which corresponds to an inner product weighted by the mass matrix $M=hI$.\n\nThe continuous BVP becomes a system of nonlinear algebraic equations:\n$$\nF(u, m) := (K + b D_{\\text{up}}) u + c\\, u^{\\odot 3} - m = 0,\n$$\nwhere $u^{\\odot 3}$ is the elementwise cube. This equation is solved for $u$ given $m$ using a Newton method. The discrete objective function is:\n$$\nJ_h(u, m) = \\frac{1}{2} (u - y)^\\top M (u - y) + \\frac{\\alpha}{2} (m - m_{\\text{prior}})^\\top M (m - m_{\\text{prior}}).\n$$\nThe DTO gradient of the reduced objective $j_h(m) = J_h(u(m), m)$ is found by applying the chain rule to the finite-dimensional problem. This leads to the discrete adjoint equation for an adjoint state vector $\\lambda \\in \\mathbb{R}^n$:\n$$\n\\left(\\frac{\\partial F}{\\partial u}\\right)^\\top \\lambda = -\\left(\\frac{\\partial J_h}{\\partial u}\\right)^\\top \\implies J(u)^\\top \\lambda = M(u-y),\n$$\nwhere $J(u) = K + b D_{\\text{up}} + 3c\\,\\mathrm{diag}(u^{\\odot 2})$ is the Jacobian of $F$ with respect to $u$. Once $\\lambda$ is found, the gradient is given by:\n$$\ng_{\\text{DTO}}(m) = \\frac{\\partial J_h}{\\partial m} + \\left(\\frac{\\partial F}{\\partial m}\\right)^\\top \\lambda = \\alpha M(m - m_{\\text{prior}}) - \\lambda.\n$$\nCrucially, $g_{\\text{DTO}}$ is the exact gradient of the discrete functional $J_h$. Thus, it is guaranteed to be a descent direction. The adjoint operator $J(u)^\\top$ contains the term $(b D_{\\text{up}})^\\top = b D_{\\text{up}}^\\top$, which represents a downwind discretization.\n\n**2. Optimize-Then-Discretize (OTD)**\n\nIn the OTD approach, we first derive the optimality conditions in the continuous function space using calculus of variations. This yields a continuous adjoint equation for an adjoint function $p(x)$:\n$$\n-p'' - (bp)' + 3 c\\, u^2 p = u - y \\quad \\text{in } (0,1),\n$$\nwith boundary conditions $p(0)=0, p(1)=0$. Assuming constant $b$, this simplifies to $-p'' - b p' + 3 c\\, u^2 p = u - y$. The continuous gradient is then $g(x) = \\alpha(m(x)-m_{\\text{prior}}(x)) - p(x)$.\n\nNext, we discretize this system. The forward BVP is discretized as before. The OTD approach becomes problematic if the discretization of the adjoint equation is not consistent with the forward discretization. The problem statement intentionally introduces such an inconsistency for the advection term. The continuous adjoint operator $-b\\, d/dx$ is discretized using the same upwind matrix, $-b D_{\\text{up}}$, as in the forward problem, instead of its transpose (a downwind scheme). This leads to the discrete OTD adjoint system:\n$$\n\\left(K - b D_{\\text{up}} + 3 c\\, \\mathrm{diag}(u^{\\odot 2})\\right) p = M(u-y).\n$$\nThe OTD gradient is then $g_{\\text{OTD}}(m) = \\alpha M(m - m_{\\text{prior}}) - p$.\n\n**3. The Mismatch and its Consequences**\n\nThe only difference between the DTO and OTD adjoint systems lies in the advection term's matrix: $b D_{\\text{up}}^\\top$ for DTO versus $-b D_{\\text{up}}$ for OTD. This discrepancy, $b(D_{\\text{up}}^\\top + D_{\\text{up}})$, is non-zero if $b \\neq 0$ and introduces a mismatch between the gradients, $\\eta = \\|g_{\\text{OTD}}-g_{\\text{DTO}}\\|_2 / \\|g_{\\text{DTO}}\\|_2$.\n\nThis mismatch has two main consequences:\na) **Convergence**: The OTD gradient is not the true gradient of the discrete functional $J_h$. Therefore, it is not guaranteed to be a descent direction, which can hinder or completely stall the convergence of the optimization algorithm.\nb) **Bias**: If the optimization using the OTD gradient converges, it converges to a point where $\\|g_{\\text{OTD}}\\| \\approx 0$. Since $g_{\\text{OTD}} \\neq g_{\\text{DTO}}$, this point is not a true stationary point of $J_h$ (where $\\|g_{\\text{DTO}}\\| \\approx 0$). This results in a biased estimate for the control $m$, which is systematically different from the optimal solution of the discrete problem.\n\nThe provided test cases are designed to explore these effects. Case B with $b=0$ serves as a sanity check, as the advection term vanishes and the DTO and OTD gradients should become identical. Cases with larger $b$ or coarser grids (larger $h$) are expected to show a larger mismatch, while grid refinement should reduce it, as the discretizations become more accurate. The implementation will solve the forward problem with a Newton-Raphson scheme and the optimization with a steepest descent algorithm equipped with an Armijo line search.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that runs all test cases and prints the results.\n    \"\"\"\n    _RANDOM_SEED = 12345\n    \n    test_cases = {\n        \"A\": (64, 5.0, 1.0, 1e-2),\n        \"B\": (64, 0.0, 1.0, 1e-2),\n        \"C\": (32, 20.0, 1.0, 5e-3),\n        \"D\": (256, 5.0, 1.0, 1e-2),\n    }\n\n    all_results = []\n    \n    class ProblemSolver:\n        def __init__(self, n, b, c, alpha, seed):\n            # 1) Spatial discretization and quadrature\n            self.n = n\n            self.b = b\n            self.c = c\n            self.alpha = alpha\n            self.h = 1.0 / (n + 1)\n            self.x_nodes = np.linspace(self.h, 1.0 - self.h, n)\n\n            # Stiffness matrix K for -u''\n            diag_K = 2.0 / self.h**2 * np.ones(n)\n            offdiag_K = -1.0 / self.h**2 * np.ones(n - 1)\n            self.K = np.diag(diag_K) + np.diag(offdiag_K, k=1) + np.diag(offdiag_K, k=-1)\n            \n            # Advection matrix D_up for u' (upwind)\n            diag_Dup = 1.0 / self.h * np.ones(n)\n            subdiag_Dup = -1.0 / self.h * np.ones(n - 1)\n            self.D_up = np.diag(diag_Dup) + np.diag(subdiag_Dup, k=-1)\n            \n            # Mass matrix M for integration (h*I)\n            self.M = self.h * np.eye(n)\n            \n            # 6) Synthetic truth and observations\n            self.m_true = np.sin(2 * np.pi * self.x_nodes) + 0.5 * np.cos(np.pi * self.x_nodes)\n            self.m_prior = np.zeros(n)\n            \n            self.rng = np.random.default_rng(seed)\n            self.y = None # Observations to be generated\n\n        def solve_forward(self, m, u_guess=None):\n            # 2) Discrete forward problem solver using Newton's method\n            u = np.zeros(self.n) if u_guess is None else u_guess.copy()\n            \n            for _ in range(20): # Newton iterations\n                res = (self.K @ u) + self.b * (self.D_up @ u) + self.c * u**3 - m\n                norm_res = np.linalg.norm(res)\n                if norm_res < 1e-10:\n                    break\n                \n                J_u = self.K + self.b * self.D_up + 3.0 * self.c * np.diag(u**2)\n                delta_u = np.linalg.solve(J_u, -res)\n                \n                # Backtracking line search for Newton step\n                s_newton = 1.0\n                for _ in range(10):\n                    u_new = u + s_newton * delta_u\n                    res_new = (self.K @ u_new) + self.b * (self.D_up @ u_new) + self.c * u_new**3 - m\n                    if np.linalg.norm(res_new) < norm_res:\n                        break\n                    s_newton *= 0.5\n                u = u_new\n            return u\n\n        def generate_observations(self):\n            u_true = self.solve_forward(self.m_true)\n            u_std = np.std(u_true)\n            noise_std = 1e-3 * u_std\n            noise = self.rng.normal(0, noise_std, self.n)\n            self.y = u_true + noise\n\n        def compute_objective(self, m, u):\n            r = u - self.y\n            m_res = m - self.m_prior\n            cost = 0.5 * self.h * np.dot(r, r) + 0.5 * self.alpha * self.h * np.dot(m_res, m_res)\n            return cost\n\n        def get_gradients(self, m, u):\n            # 3) DTO adjoint and gradient & 4) OTD adjoint and gradient\n            r = u - self.y\n            rhs = self.h * r\n            \n            J_u = self.K + self.b * self.D_up + 3.0 * self.c * np.diag(u**2)\n            \n            # DTO\n            J_u_T = J_u.T\n            lambda_dto = np.linalg.solve(J_u_T, rhs)\n            g_dto = self.alpha * self.h * (m - self.m_prior) - lambda_dto\n            \n            # OTD\n            J_otd = self.K - self.b * self.D_up + 3.0 * self.c * np.diag(u**2)\n            p_otd = np.linalg.solve(J_otd, rhs)\n            g_otd = self.alpha * self.h * (m - self.m_prior) - p_otd\n            \n            return g_dto, g_otd\n\n        def run_optimization(self, grad_type):\n            # 5) Convergence and bias\n            m = self.m_prior.copy()\n            \n            k_max = 20\n            c_armijo = 1e-4\n            beta_armijo = 0.5\n            \n            u = self.solve_forward(m)\n            jh = self.compute_objective(m, u)\n\n            for _ in range(k_max):\n                g_dto, g_otd = self.get_gradients(m, u)\n                g = g_dto if grad_type == 'dto' else g_otd\n                \n                # Termination condition\n                rel_grad_norm = np.linalg.norm(g) / (np.linalg.norm(g) + 1.0)\n                if rel_grad_norm < 1e-8:\n                    break\n                \n                # Armijo backtracking line search\n                s = 1.0\n                g_dot_g = np.dot(g, g)\n                for _ in range(10): # Max backtracking steps\n                    m_new = m - s * g\n                    u_new = self.solve_forward(m_new)\n                    jh_new = self.compute_objective(m_new, u_new)\n                    \n                    if jh_new <= jh - c_armijo * s * g_dot_g:\n                        break\n                    s *= beta_armijo\n                \n                m = m_new\n                u = u_new\n                jh = jh_new\n            \n            mse = self.h * np.sum((m - self.m_true)**2)\n            return mse, jh\n\n    for case_name in [\"A\", \"B\", \"C\", \"D\"]:\n        params = test_cases[case_name]\n        prob = ProblemSolver(*params, seed=_RANDOM_SEED)\n        prob.generate_observations()\n        \n        # Calculations at the prior\n        m0 = prob.m_prior\n        u0 = prob.solve_forward(m0)\n        jh_at_prior = prob.compute_objective(m0, u0)\n        g_dto_prior, g_otd_prior = prob.get_gradients(m0, u0)\n        \n        # 5) Gradient mismatch\n        norm_g_dto = np.linalg.norm(g_dto_prior)\n        if norm_g_dto == 0:\n            eta = 0.0\n        else:\n            eta = np.linalg.norm(g_otd_prior - g_dto_prior) / norm_g_dto\n\n        # 5) One-step descent flags\n        s0 = 1.0\n        # DTO step\n        m_test_dto = m0 - s0 * g_dto_prior\n        u_test_dto = prob.solve_forward(m_test_dto)\n        jh_test_dto = prob.compute_objective(m_test_dto, u_test_dto)\n        dto_desc_flag = 1 if jh_test_dto < jh_at_prior else 0\n        \n        # OTD step\n        m_test_otd = m0 - s0 * g_otd_prior\n        u_test_otd = prob.solve_forward(m_test_otd)\n        jh_test_otd = prob.compute_objective(m_test_otd, u_test_otd)\n        otd_desc_flag = 1 if jh_test_otd < jh_at_prior else 0\n        \n        # 5) Convergence and bias\n        dto_mse, dto_jh_final = prob.run_optimization('dto')\n        otd_mse, otd_jh_final = prob.run_optimization('otd')\n        \n        case_results = [\n            eta,\n            dto_desc_flag,\n            otd_desc_flag,\n            dto_mse,\n            otd_mse,\n            dto_jh_final,\n            otd_jh_final\n        ]\n        all_results.append(case_results)\n    \n    # 8) Required final output format\n    print(str(all_results).replace(\"'\", \"\"))\n\nsolve()\n```", "id": "3408562"}]}