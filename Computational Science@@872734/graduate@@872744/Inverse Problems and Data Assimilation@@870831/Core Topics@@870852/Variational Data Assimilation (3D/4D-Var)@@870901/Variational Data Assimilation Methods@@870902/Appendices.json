{"hands_on_practices": [{"introduction": "At the heart of variational data assimilation lies the adjoint model, a powerful tool for efficiently computing the gradient of a cost function with respect to a large number of control variables. Since the entire optimization process depends on this gradient, its correctness is paramount. This practice introduces the \"dot-product test,\" a fundamental numerical procedure used to verify that an implemented adjoint model is indeed the true adjoint of the tangent linear model, ensuring the foundation of your assimilation system is sound. [@problem_id:3430448]", "problem": "You are asked to implement and verify a discrete dot-product adjoint test for variational data assimilation using a set of finite-dimensional models. The goal is to check numerically that, for a nonlinear mapping between Euclidean spaces endowed with possibly weighted inner products, the adjoint of the tangent linear operator satisfies the defining equality of adjoints to machine precision.\n\nStart from the following fundamental base:\n- The directional (Gateaux) derivative of a mapping between finite-dimensional vector spaces, the definition of Jacobian matrices, and their action on perturbations.\n- The definition of an adjoint operator with respect to an inner product: given inner products on the domain and codomain, the adjoint is the unique linear map that satisfies the inner-product equality for all directions.\n- Linear algebra facts about symmetric positive definite matrices and weighted inner products in finite-dimensional spaces.\n\nLet $M:\\mathbb{R}^n\\to\\mathbb{R}^m$ be a differentiable mapping, and let $x^\\ast\\in\\mathbb{R}^n$ be a reference state. Let $M'(x^\\ast)$ denote the Jacobian (tangent linear) operator evaluated at $x^\\ast$. Equip the domain $\\mathbb{R}^n$ with an inner product $\\langle u,v\\rangle_X = u^\\top W_X v$ and the codomain $\\mathbb{R}^m$ with an inner product $\\langle y,z\\rangle_Y = y^\\top W_Y z$, where $W_X\\in\\mathbb{R}^{n\\times n}$ and $W_Y\\in\\mathbb{R}^{m\\times m}$ are symmetric positive definite matrices. The adjoint of $M'(x^\\ast)$ with respect to these inner products, denoted $M'(x^\\ast)^\\star:\\mathbb{R}^m\\to\\mathbb{R}^n$, is defined as the unique linear map that satisfies\n$$\n\\langle M'(x^\\ast)\\,\\delta x, \\delta y \\rangle_Y \\;=\\; \\langle \\delta x, M'(x^\\ast)^\\star \\,\\delta y \\rangle_X\n$$\nfor all $\\delta x\\in\\mathbb{R}^n$ and $\\delta y\\in\\mathbb{R}^m$.\n\nYour task is to implement a program that, for each of the test cases below, computes a numerical estimate of the mismatch\n$$\n\\varepsilon \\;=\\; \\frac{\\left|\\langle M'(x^\\ast)\\,\\delta x, \\delta y \\rangle_Y \\;-\\; \\langle \\delta x, M'(x^\\ast)^\\star \\,\\delta y \\rangle_X\\right|}{\\max\\!\\left(1,\\left|\\langle M'(x^\\ast)\\,\\delta x, \\delta y \\rangle_Y\\right|,\\left|\\langle \\delta x, M'(x^\\ast)^\\star \\,\\delta y \\rangle_X\\right|\\right)}\\,,\n$$\nand verifies that $\\varepsilon$ is at the level of machine precision. You must derive the adjoint action $M'(x^\\ast)^\\star\\,\\delta y$ directly from the adjoint definition and the weighted inner products, using only the fundamental base above, without relying on any pre-given formula.\n\nImplement the following models and test cases. All random quantities must be generated deterministically using the specified seeds. Angles are not involved. There are no physical units to report.\n\nDefinitions:\n- The elementwise hyperbolic tangent is denoted by $\\tanh(\\cdot)$, the elementwise sine by $\\sin(\\cdot)$, the elementwise cosine by $\\cos(\\cdot)$, and the elementwise hyperbolic secant squared by $\\operatorname{sech}^2(z)=1-\\tanh^2(z)$.\n- For any vector $a\\in\\mathbb{R}^k$, $\\operatorname{diag}(a)$ denotes the $k\\times k$ diagonal matrix with the entries of $a$ on its diagonal.\n\nModels:\n- Linear model: $M(x)=A\\,x$ with $A\\in\\mathbb{R}^{m\\times n}$.\n- Nonlinear model: $M(x)=\\tanh(L\\,x + b)+ D\\,\\sin(x)$ with $L\\in\\mathbb{R}^{m\\times n}$, $b\\in\\mathbb{R}^m$, $D\\in\\mathbb{R}^{m\\times n}$, where all functions are applied elementwise.\n\nTangent linear operators:\n- For the linear model, $M'(x^\\ast)\\,\\delta x = A\\,\\delta x$.\n- For the nonlinear model, define $z=L\\,x^\\ast + b$ and $g=\\operatorname{sech}^2(z)$. Then\n$$\nM'(x^\\ast)\\,\\delta x \\;=\\; \\operatorname{diag}(g)\\,L\\,\\delta x \\;+\\; D\\,\\operatorname{diag}(\\cos(x^\\ast))\\,\\delta x\\,.\n$$\n\nAdjoint operators:\n- You must derive and implement $M'(x^\\ast)^\\star$ for each case under the given inner products, starting from the adjoint definition above. Do not assume any pre-remembered shortcut formula; instead, derive the algebraic form from first principles.\n\nTest suite:\n- Case $\\mathbf{A}$ (linear, Euclidean inner products):\n  - Dimensions: $n=5$, $m=4$.\n  - Random seeds: matrix $A$ from seed $s_A=0$; perturbations $\\delta x$ and $\\delta y$ from seeds $s_x=1$ and $s_y=2$.\n  - Weights: $W_X=I_n$, $W_Y=I_m$, where $I_k$ is the $k\\times k$ identity.\n- Case $\\mathbf{B}$ (linear, weighted inner products):\n  - Dimensions: $n=6$, $m=6$.\n  - Random seeds: matrix $A$ from seed $s_A=3$; perturbations $\\delta x$ and $\\delta y$ from seeds $s_x=6$ and $s_y=7$.\n  - Weights: $W_X=R_X^\\top R_X+\\alpha I_n$ and $W_Y=R_Y^\\top R_Y+\\beta I_m$, with $R_X$ from seed $s_{RX}=4$, $R_Y$ from seed $s_{RY}=5$, and scalars $\\alpha=n$, $\\beta=m$.\n- Case $\\mathbf{C}$ (nonlinear, Euclidean inner products):\n  - Dimensions: $n=8$, $m=7$.\n  - Random seeds: matrices $L$ and $D$ from seeds $s_L=10$ and $s_D=11$; vector $b$ from seed $s_b=12$; reference state $x^\\ast$ from seed $s_{x^\\ast}=13$; perturbations $\\delta x$ and $\\delta y$ from seeds $s_x=14$ and $s_y=15$.\n  - Weights: $W_X=I_n$, $W_Y=I_m$.\n- Case $\\mathbf{D}$ (edge case with zero perturbation):\n  - Dimensions: $n=4$, $m=3$.\n  - Random seeds: matrix $A$ from seed $s_A=8$; perturbations $\\delta x=\\mathbf{0}$ (the zero vector) and $\\delta y$ from seed $s_y=9$.\n  - Weights: $W_X=I_n$, $W_Y=I_m$.\n\nFor each case, compute the mismatch $\\varepsilon$ as defined above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $\\left[\\varepsilon_A,\\varepsilon_B,\\varepsilon_C,\\varepsilon_D\\right]$. The entries must be floating-point numbers. No other text should be printed. The numerical results must be consistent with machine precision for the test to be considered passed.", "solution": "The objective is to perform a numerical verification of the adjoint property for several discrete models, a procedure commonly known as the dot-product test. This test is a fundamental debugging and validation tool in fields that rely on adjoint models, such as variational data assimilation and large-scale optimization. The verification must confirm that for a given linear operator and specified inner products on its domain and codomain, the defining equality for the adjoint holds to within machine precision.\n\nLet $M: \\mathbb{R}^n \\to \\mathbb{R}^m$ be a differentiable mapping. Its linearization at a reference state $x^\\ast \\in \\mathbb{R}^n$ is the tangent linear operator, which is a linear map represented by the Jacobian matrix $K = M'(x^\\ast) \\in \\mathbb{R}^{m \\times n}$. The domain $\\mathbb{R}^n$ and codomain $\\mathbb{R}^m$ are endowed with weighted inner products:\n$$ \\langle u, v \\rangle_X = u^\\top W_X v \\quad \\text{for } u, v \\in \\mathbb{R}^n $$\n$$ \\langle y, z \\rangle_Y = y^\\top W_Y z \\quad \\text{for } y, z \\in \\mathbb{R}^m $$\nwhere $W_X \\in \\mathbb{R}^{n \\times n}$ and $W_Y \\in \\mathbb{R}^{m \\times m}$ are symmetric positive definite matrices.\n\nThe adjoint of the operator $K$, denoted $K^\\star: \\mathbb{R}^m \\to \\mathbb{R}^n$, is uniquely defined by the relation:\n$$ \\langle K \\delta x, \\delta y \\rangle_Y = \\langle \\delta x, K^\\star \\delta y \\rangle_X $$\nwhich must hold for all perturbations $\\delta x \\in \\mathbb{R}^n$ and $\\delta y \\in \\mathbb{R}^m$.\n\nTo implement the dot-product test, we must first derive an explicit expression for the action of the adjoint, $K^\\star \\delta y$. We begin by substituting the definitions of the inner products into the defining relation.\n\nThe left-hand side (LHS) is:\n$$ \\langle K \\delta x, \\delta y \\rangle_Y = (K \\delta x)^\\top W_Y \\delta y $$\nUsing the transpose property $(AB)^\\top = B^\\top A^\\top$, this becomes:\n$$ \\text{LHS} = (\\delta x)^\\top K^\\top W_Y \\delta y $$\n\nThe right-hand side (RHS) is:\n$$ \\langle \\delta x, K^\\star \\delta y \\rangle_X = (\\delta x)^\\top W_X (K^\\star \\delta y) $$\n\nEquating the two expressions, we obtain:\n$$ (\\delta x)^\\top K^\\top W_Y \\delta y = (\\delta x)^\\top W_X (K^\\star \\delta y) $$\nThis equality must hold for any choice of vector $\\delta x$. This implies that the vectors multiplying $(\\delta x)^\\top$ on both sides must be identical:\n$$ K^\\top W_Y \\delta y = W_X (K^\\star \\delta y) $$\nSince the weight matrix $W_X$ is symmetric positive definite, it is invertible. We can therefore solve for the action $K^\\star \\delta y$ by pre-multiplying by the inverse of $W_X$, denoted $W_X^{-1}$:\n$$ K^\\star \\delta y = W_X^{-1} K^\\top W_Y \\delta y $$\nThis derived expression provides the concrete computational formula for the action of the adjoint operator. Numerically, calculating this action is best performed by solving the linear system $W_X z = v$, where $z = K^\\star \\delta y$ and $v = K^\\top W_Y \\delta y$, thus avoiding the potential numerical instability of explicit matrix inversion.\n\nFor the special case of standard Euclidean inner products, the weight matrices are identity matrices: $W_X = I_n$ and $W_Y = I_m$. In this configuration, the formula for the adjoint action simplifies considerably:\n$$ K^\\star \\delta y = (I_n)^{-1} K^\\top I_m \\delta y = K^\\top \\delta y $$\nThus, for Euclidean inner products, the adjoint operator is simply the transpose of the original linear operator.\n\nThe problem specifies two models for which this verification must be carried out.\n1.  **Linear Model**: $M(x) = Ax$. The tangent linear operator is constant and independent of the reference state $x^\\ast$, i.e., $K = M'(x^\\ast) = A$.\n2.  **Nonlinear Model**: $M(x) = \\tanh(Lx + b) + D\\sin(x)$. The tangent linear operator, evaluated at $x^\\ast$, is given by $K = \\operatorname{diag}(g)L + D\\operatorname{diag}(\\cos(x^\\ast))$, where $g = \\operatorname{sech}^2(Lx^\\ast + b) = 1 - \\tanh^2(Lx^\\ast + b)$.\n\nThe numerical verification consists of computing both sides of the adjoint definition and evaluating their difference. A normalized error metric, $\\varepsilon$, is used to quantify the mismatch:\n$$ \\varepsilon = \\frac{|\\text{LHS} - \\text{RHS}|}{\\max(1, |\\text{LHS}|, |\\text{RHS}|)} = \\frac{\\left|\\langle K \\delta x, \\delta y \\rangle_Y - \\langle \\delta x, K^\\star \\delta y \\rangle_X\\right|}{\\max(1, |\\langle K \\delta x, \\delta y \\rangle_Y|, |\\langle \\delta x, K^\\star \\delta y \\rangle_X|)} $$\nFor a correct implementation, the value of $\\varepsilon$ should be on the order of machine precision (typically around $10^{-15}$ to $10^{-16}$ for double-precision floating-point arithmetic), provided the operands are not zero. For the edge case where one of the perturbations is a zero vector (e.g., $\\delta x = \\mathbf{0}$), both the LHS and RHS evaluate to exactly zero, resulting in $\\varepsilon=0$. The implementation will proceed with these derived formulas to compute $\\varepsilon$ for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs a discrete dot-product adjoint test for a set of\n    finite-dimensional models to verify the adjoint property numerically.\n    \"\"\"\n\n    test_cases = [\n        {\n            'case_id': 'A', 'model': 'linear', 'n': 5, 'm': 4,\n            's_A': 0, 's_x': 1, 's_y': 2, 'weights': 'euclidean'\n        },\n        {\n            'case_id': 'B', 'model': 'linear', 'n': 6, 'm': 6,\n            's_A': 3, 's_x': 6, 's_y': 7, 'weights': 'weighted',\n            's_RX': 4, 's_RY': 5\n        },\n        {\n            'case_id': 'C', 'model': 'nonlinear', 'n': 8, 'm': 7,\n            's_L': 10, 's_D': 11, 's_b': 12, 's_x_star': 13,\n            's_x': 14, 's_y': 15, 'weights': 'euclidean'\n        },\n        {\n            'case_id': 'D', 'model': 'linear', 'n': 4, 'm': 3,\n            's_A': 8, 's_y': 9, 'weights': 'euclidean', 'dx_zero': True\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        n, m = case['n'], case['m']\n\n        # Generate perturbations\n        rng_y = np.random.default_rng(case['s_y'])\n        delta_y = rng_y.standard_normal((m, 1))\n\n        if case.get('dx_zero', False):\n            delta_x = np.zeros((n, 1))\n        else:\n            rng_x = np.random.default_rng(case['s_x'])\n            delta_x = rng_x.standard_normal((n, 1))\n\n        # Construct weight matrices\n        if case['weights'] == 'euclidean':\n            W_X = np.eye(n)\n            W_Y = np.eye(m)\n        else: # weighted\n            alpha, beta = n, m\n            \n            rng_RX = np.random.default_rng(case['s_RX'])\n            R_X = rng_RX.standard_normal((n, n))\n            W_X = R_X.T @ R_X + alpha * np.eye(n)\n\n            rng_RY = np.random.default_rng(case['s_RY'])\n            R_Y = rng_RY.standard_normal((m, m))\n            W_Y = R_Y.T @ R_Y + beta * np.eye(m)\n            \n        # Construct tangent linear operator K\n        if case['model'] == 'linear':\n            rng_A = np.random.default_rng(case['s_A'])\n            A = rng_A.standard_normal((m, n))\n            K = A\n        else: # nonlinear\n            rng_L = np.random.default_rng(case['s_L'])\n            L = rng_L.standard_normal((m, n))\n            \n            rng_D = np.random.default_rng(case['s_D'])\n            D = rng_D.standard_normal((m, n))\n\n            rng_b = np.random.default_rng(case['s_b'])\n            b = rng_b.standard_normal((m, 1))\n\n            rng_x_star = np.random.default_rng(case['s_x_star'])\n            x_star = rng_x_star.standard_normal((n, 1))\n            \n            z = L @ x_star + b\n            tanh_z = np.tanh(z)\n            g = 1 - tanh_z**2\n            cos_x_star = np.cos(x_star)\n            \n            K = np.diag(g.flatten()) @ L + D @ np.diag(cos_x_star.flatten())\n            \n        # Calculate LHS: <K*delta_x, delta_y>_Y\n        lhs_val = (K @ delta_x).T @ W_Y @ delta_y\n        lhs = lhs_val.item()\n\n        # Calculate RHS: <delta_x, K_star*delta_y>_X\n        # K_star*delta_y = inv(W_X) * K.T * W_Y * delta_y\n        v = K.T @ W_Y @ delta_y\n        adj_action = np.linalg.solve(W_X, v)\n        rhs_val = delta_x.T @ W_X @ adj_action\n        rhs = rhs_val.item()\n\n        # Calculate mismatch epsilon\n        numerator = abs(lhs - rhs)\n        denominator = max(1, abs(lhs), abs(rhs))\n        epsilon = numerator / denominator\n        \n        results.append(epsilon)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3430448"}, {"introduction": "When dealing with dynamical systems described by differential equations, we face a fundamental choice in how we derive gradients for optimization. We can either find the continuous adjoint equations first and then discretize them, or discretize the forward model and then derive the exact discrete adjoint. This exercise provides a hands-on exploration of these two paradigms, revealing the subtle yet significant inconsistencies that can arise and highlighting why the \"discretize-then-optimize\" approach is generally preferred for ensuring gradient consistency. [@problem_id:3430507]", "problem": "Consider a scalar parameter estimation problem posed in the framework of variational data assimilation for an ordinary differential equation (ODE). The state $x(t)$ evolves according to the nonlinear model $x'(t) = f(x(t),\\theta)$ over the time interval $t \\in [0,T]$, with initial condition $x(0) = x_0$. The performance metric is a tracking-type cost functional defined by an integral-in-time misfit to a reference trajectory $y(t)$ and a quadratic regularization on the parameter $\\theta$. The goal is to analyze, quantitatively, how time discretization impacts the accuracy of the adjoint-based gradient by comparing the gradient computed using adjoints derived from the continuous-time formulation against the gradient computed using adjoints derived from the discrete-time time-stepping scheme. Your program must produce the discrepancy between these two gradients for several test cases.\n\nFundamental setup:\n- The nonlinear model is $f(x,\\theta) = \\theta\\,x\\,(1-x)$, the classical logistic right-hand side. The initial condition is $x(0) = x_0 \\in (0,1)$ and the assimilation window is $[0,T]$.\n- The cost functional is\n$$\nJ(\\theta) = \\frac{1}{2}\\int_0^T \\big(x(t) - y(t)\\big)^2 \\, dt \\;+\\; \\frac{1}{2}\\,\\beta\\,\\big(\\theta - \\theta_{\\mathrm{ref}}\\big)^2,\n$$\nwhere $y(t)$ is a given reference trajectory generated by the same model but with a known \"true\" parameter $\\theta_{\\mathrm{true}}$, and $\\beta \\ge 0$ is a regularization weight. There is no terminal misfit term, so $J(\\theta)$ contains only the integral term in $t$ and the parameter regularization. No physical units are required for the output since this is a purely mathematical setup.\n\nDiscretization and adjoints to be compared:\n- Continuous-time adjoint: Derive the adjoint from the continuous-time constrained optimization problem using the calculus of variations and Lagrange multipliers. Then, approximate its backward-in-time integration numerically to compute the gradient $\\frac{dJ}{d\\theta}$.\n- Discrete-time adjoint: Derive the adjoint from the discrete-time constrained optimization problem associated with the chosen time-stepping scheme. Use reverse-time recursion consistent with the discrete scheme to compute the gradient $\\frac{dJ}{d\\theta}$.\n\nBoth computations must be performed for the same forward trajectory $x(t)$ generated by the explicit Euler method applied to the ODE $x'(t) = \\theta x (1-x)$ with a user-specified time step. The reference trajectory $y(t)$ must be generated with a high-fidelity fourth-order Runge–Kutta method using a time step that is at least ten times finer than the explicit Euler step, and then sampled to the coarse grid to define $y(t_n)$ at the explicit Euler grid points.\n\nTarget quantity:\n- For each test case, compute the absolute discrepancy\n$$\n\\Delta = \\left| \\left.\\frac{dJ}{d\\theta}\\right|_{\\text{continuous-adjoint}} \\;-\\; \\left.\\frac{dJ}{d\\theta}\\right|_{\\text{discrete-adjoint}} \\right|.\n$$\n\nImplementation requirements:\n- Forward model for the computed trajectory must use explicit Euler with $N$ steps on $[0,T]$, i.e., $\\Delta t = T/N$ and $t_n = n\\,\\Delta t$ for $n=0,\\dots,N$.\n- The continuous-adjoint gradient must be obtained by numerically integrating the continuous adjoint backward in time over the same coarse grid and then approximating the gradient integral using a consistent Riemann sum on that grid.\n- The discrete-adjoint gradient must be obtained by reverse accumulation on the coarse grid based on the explicit Euler scheme and a left Riemann sum for the integral part of the cost.\n- The reference trajectory $y(t)$ must be computed by fourth-order Runge–Kutta with a time step $\\delta t = \\Delta t/10$ (i.e., $10N$ fine steps over $[0,T]$) using the \"true\" parameter $\\theta_{\\mathrm{true}}$, and then sampled to the coarse grid points $t_n$.\n\nTest suite:\nProvide results for the following parameter sets. In each bullet, the tuple is $(T,N,\\theta,\\beta,\\theta_{\\mathrm{ref}},x_0,\\theta_{\\mathrm{true}})$.\n- Case $1$: $(2.0, 200, 1.0, 0.05, 0.8, 0.2, 1.2)$\n- Case $2$: $(2.0, 20, 1.0, 0.05, 0.8, 0.2, 1.2)$\n- Case $3$: $(2.0, 1000, 1.0, 0.05, 0.8, 0.2, 1.2)$\n- Case $4$: $(2.0, 200, -0.8, 0.05, 0.0, 0.2, 1.2)$\n- Case $5$: $(2.0, 200, 1.0, 0.05, 0.8, 0.99, 1.2)$\n\nOutput specification:\n- Your program must produce a single line of output containing the discrepancies for the test suite as a comma-separated list enclosed in square brackets.\n- All results must be real numbers and must be rounded to $8$ decimal places.\n- The final output format must be exactly like $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$ where $\\Delta_i$ corresponds to Case $i$ in the order listed above.", "solution": "The user wants to solve a problem in variational data assimilation, specifically to quantify the discrepancy between two different methods of computing the gradient of a cost functional. The two methods are based on the \"optimize-then-discretize\" and \"discretize-then-optimize\" paradigms, which lead to different adjoint equations and gradient expressions.\n\n### Step 1: Extract Givens\n- **State Model:** Ordinary Differential Equation (ODE) $x'(t) = f(x(t),\\theta)$ with $f(x,\\theta) = \\theta\\,x\\,(1-x)$.\n- **Time Interval:** $t \\in [0,T]$.\n- **Initial Condition:** $x(0) = x_0$.\n- **Cost Functional:** $J(\\theta) = \\frac{1}{2}\\int_0^T \\big(x(t) - y(t)\\big)^2 \\, dt + \\frac{1}{2}\\,\\beta\\,\\big(\\theta - \\theta_{\\mathrm{ref}}\\big)^2$.\n- **Reference Trajectory:** $y(t)$ is generated by the same model $x'(t) = f(x(t), \\theta_{\\mathrm{true}})$ with a \"true\" parameter $\\theta_{\\mathrm{true}}$ and initial condition $x_0$.\n- **Target Quantity:** The absolute discrepancy $\\Delta = \\left| \\left.\\frac{dJ}{d\\theta}\\right|_{\\text{continuous-adjoint}} - \\left.\\frac{dJ}{d\\theta}\\right|_{\\text{discrete-adjoint}} \\right|$.\n- **Numerical Methods:**\n    - **Forward model for state $x(t)$:** Explicit Euler with $N$ steps, time step $\\Delta t = T/N$.\n    - **Reference trajectory $y(t)$:** Fourth-order Runge-Kutta (RK4) with time step $\\delta t = \\Delta t/10$, sampled at the coarse grid points $t_n = n\\,\\Delta t$.\n    - **Continuous-adjoint gradient:** Based on numerical integration of the continuous adjoint ODE and the gradient integral.\n    - **Discrete-adjoint gradient:** Based on reverse accumulation using the adjoint of the discrete (Explicit Euler) scheme.\n    - **Cost functional integral quadrature:** Left Riemann sum, $\\int_0^T g(t) dt \\approx \\sum_{n=0}^{N-1} g(t_n) \\Delta t$.\n- **Test Cases:** The tuples $(T,N,\\theta,\\beta,\\theta_{\\mathrm{ref}},x_0,\\theta_{\\mathrm{true}})$ are:\n    1. $(2.0, 200, 1.0, 0.05, 0.8, 0.2, 1.2)$\n    2. $(2.0, 20, 1.0, 0.05, 0.8, 0.2, 1.2)$\n    3. $(2.0, 1000, 1.0, 0.05, 0.8, 0.2, 1.2)$\n    4. $(2.0, 200, -0.8, 0.05, 0.0, 0.2, 1.2)$\n    5. $(2.0, 200, 1.0, 0.05, 0.8, 0.99, 1.2)$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, objective, and self-contained.\n- **Scientific Soundness:** The problem is a standard exercise in the field of optimal control and data assimilation for dynamical systems. The model (logistic equation) and cost functional (tracking-type) are canonical. The comparison between \"discretize-then-optimize\" and \"optimize-then-discretize\" is a fundamental topic in numerical optimization.\n- **Well-Posedness:** All necessary parameters, initial conditions, and numerical schemes are precisely specified, leading to a unique and computable result for each test case.\n- **Objectivity:** The problem is stated using precise mathematical language, free from any subjectivity or ambiguity.\n- **Completeness:** All required information for implementation is provided. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will now be provided.\n\n### Principle-Based Design and Derivations\n\nThe core of the problem lies in the non-commutativity of discretization and optimization. We will derive the gradient expressions for the two specified approaches. Let the partial derivatives of the model function $f(x,\\theta)=\\theta x(1-x)$ be denoted as $\\frac{\\partial f}{\\partial x} = \\theta(1-2x)$ and $\\frac{\\partial f}{\\partial\\theta} = x(1-x)$. The time grid is $t_n=n\\Delta t$ for $n=0, \\dots, N$. The forward state, denoted $x_n \\approx x(t_n)$, is computed for both approaches using the explicit Euler scheme:\n$$ x_{n+1} = x_n + \\Delta t \\, f(x_n, \\theta) = x_n + \\Delta t \\, \\theta x_n (1-x_n) $$\nwith initial condition $x_0$.\n\n**1. Continuous-Adjoint Approach (Optimize-then-Discretize)**\n\nFirst, we derive the adjoint system in the continuous-time setting. The Lagrangian is:\n$$ \\mathcal{L}(x, \\theta, p) = \\frac{1}{2}\\int_0^T (x - y)^2 dt + \\frac{1}{2}\\beta(\\theta - \\theta_{\\mathrm{ref}})^2 + \\int_0^T p(t) [f(x, \\theta) - x'(t)] dt $$\nSetting the first variation with respect to $x(t)$ to zero yields the adjoint ODE and the transversality condition. After integration by parts of the term $\\int p(-x')dt$, the adjoint ODE is:\n$$ -p'(t) = \\frac{\\partial f}{\\partial x}(x(t),\\theta) \\, p(t) + (x(t)-y(t)) $$\n$$ -p'(t) = \\theta(1-2x(t))\\,p(t) + (x(t)-y(t)) $$\nThe transversality condition at $t=T$ is $p(T)=0$ because there is no terminal cost on $x(T)$. The gradient of the cost functional is the partial derivative of the Lagrangian with respect to $\\theta$:\n$$ \\frac{dJ}{d\\theta} = \\beta(\\theta - \\theta_{\\mathrm{ref}}) + \\int_0^T p(t) \\frac{\\partial f}{\\partial \\theta}(x(t), \\theta) dt = \\beta(\\theta - \\theta_{\\mathrm{ref}}) + \\int_0^T p(t) x(t)(1-x(t)) dt $$\nNext, we discretize this system. We use a backward-in-time explicit Euler scheme for the adjoint ODE $p' = -\\theta(1-2x)p - (x-y)$, resulting in:\n$$ \\frac{p_n - p_{n+1}}{-\\Delta t} = -\\theta(1-2x_{n+1})p_{n+1} - (x_{n+1}-y_{n+1}) $$\n$$ p_n = p_{n+1} + \\Delta t \\left[ \\theta(1-2x_{n+1})p_{n+1} + (x_{n+1}-y_{n+1}) \\right] = p_{n+1}\\left(1+\\Delta t \\frac{\\partial f}{\\partial x}\\Big|_{x_{n+1}}\\right) + \\Delta t (x_{n+1}-y_{n+1}) $$\nThis recurrence is solved for $n = N-1, \\dots, 0$, starting with $p_N = 0$. The gradient integral is approximated using a left Riemann sum, consistent with the cost function specification:\n$$ \\left.\\frac{dJ}{d\\theta}\\right|_{\\text{continuous-adjoint}} = \\beta(\\theta - \\theta_{\\mathrm{ref}}) + \\Delta t \\sum_{n=0}^{N-1} p_n x_n(1-x_n) $$\n\n**2. Discrete-Adjoint Approach (Discretize-then-Optimize)**\n\nFirst, we discretize the entire problem. The explicit Euler scheme for the state is $x_{n+1} = x_n + \\Delta t f(x_n, \\theta) =: M(x_n, \\theta)$. Using a left Riemann sum for the integral term, the discrete cost functional is:\n$$ J_d(\\theta) = \\frac{\\Delta t}{2} \\sum_{n=0}^{N-1} (x_n - y_n)^2 + \\frac{\\beta}{2}(\\theta - \\theta_{\\mathrm{ref}})^2 $$\nWe form the discrete Lagrangian with multipliers $p_1, \\dots, p_N$:\n$$ \\mathcal{L}_d = J_d(\\theta) + \\sum_{n=0}^{N-1} p_{n+1} [M(x_n, \\theta) - x_{n+1}] $$\nSetting $\\frac{\\partial \\mathcal{L}_d}{\\partial x_n} = 0$ for each state variable $x_n, n=1, \\dots, N$:\nFor $n \\in \\{1, \\dots, N-1\\}$:\n$$ \\frac{\\partial \\mathcal{L}_d}{\\partial x_n} = \\Delta t (x_n - y_n) - p_n + p_{n+1} \\frac{\\partial M}{\\partial x_n}(x_n, \\theta) = 0 $$\n$$ p_n = p_{n+1} \\frac{\\partial M}{\\partial x_n}(x_n, \\theta) + \\Delta t (x_n - y_n) = p_{n+1}\\left(1+\\Delta t \\frac{\\partial f}{\\partial x}\\Big|_{x_n}\\right) + \\Delta t (x_n - y_n) $$\nFor $n=N$, the cost term does not contain $x_N$, so $\\frac{\\partial J_d}{\\partial x_N}=0$. From the constraint sum, we get $-p_N$. Thus, $\\frac{\\partial \\mathcal{L}_d}{\\partial x_N} = -p_N = 0$. This gives the terminal condition $p_N=0$. The adjoint state is computed via the recurrence for $n=N-1, \\dots, 1$. The gradient is $\\frac{dJ_d}{d\\theta} = \\frac{\\partial \\mathcal{L}_d}{\\partial \\theta}$:\n$$ \\left.\\frac{dJ}{d\\theta}\\right|_{\\text{discrete-adjoint}} = \\beta(\\theta - \\theta_{\\mathrm{ref}}) + \\sum_{n=0}^{N-1} p_{n+1} \\frac{\\partial M}{\\partial \\theta}(x_n, \\theta) $$\nwhere $\\frac{\\partial M}{\\partial \\theta} = \\Delta t \\frac{\\partial f}{\\partial \\theta}$. This gives:\n$$ \\left.\\frac{dJ}{d\\theta}\\right|_{\\text{discrete-adjoint}} = \\beta(\\theta - \\theta_{\\mathrm{ref}}) + \\Delta t \\sum_{n=0}^{N-1} p_{n+1} x_n(1-x_n) $$\n\nThe discrepancy arises because the adjoint recurrence relations and the final gradient summations are different in the two approaches, reflecting the non-commutativity of discretization and optimization.\n\n**Implementation Summary**\n\nFor each test case, the program will:\n1.  Generate the high-fidelity reference trajectory $\\{y_n\\}_{n=0}^N$ using RK4 with $\\theta_{\\mathrm{true}}$ and a fine time step, then sample it onto the coarse grid.\n2.  Compute the forward state trajectory $\\{x_n\\}_{n=0}^N$ using explicit Euler with the given $\\theta$.\n3.  Compute the continuous-adjoint gradient using its specific backward recurrence and gradient sum formula.\n4.  Compute the discrete-adjoint gradient using its specific backward recurrence and gradient sum formula.\n5.  Calculate the absolute difference of the two gradients.\nFinal results are collected and formatted as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f_model(x, theta):\n    \"\"\"The logistic model ODE right-hand side.\"\"\"\n    return theta * x * (1.0 - x)\n\ndef df_dx(x, theta):\n    \"\"\"Partial derivative of f_model with respect to x.\"\"\"\n    return theta * (1.0 - 2.0 * x)\n\ndef df_dtheta(x, theta):\n    \"\"\"Partial derivative of f_model with respect to theta.\"\"\"\n    return x * (1.0 - x)\n\ndef compute_discrepancy(T, N, theta, beta, theta_ref, x0, theta_true):\n    \"\"\"\n    Computes the discrepancy between continuous-adjoint and discrete-adjoint gradients.\n    \"\"\"\n    # Time discretization parameters for the coarse grid\n    dt = T / N\n\n    # --- 1. Generate Reference Trajectory y ---\n    # The reference is computed on a fine grid (10x finer) using RK4\n    N_fine = 10 * N\n    dt_fine = T / N_fine\n    \n    x_fine = np.zeros(N_fine + 1)\n    x_fine[0] = x0\n    \n    # RK4 integration for the reference trajectory\n    for i in range(N_fine):\n        k1 = f_model(x_fine[i], theta_true)\n        k2 = f_model(x_fine[i] + 0.5 * dt_fine * k1, theta_true)\n        k3 = f_model(x_fine[i] + 0.5 * dt_fine * k2, theta_true)\n        k4 = f_model(x_fine[i] + dt_fine * k3, theta_true)\n        x_fine[i+1] = x_fine[i] + (dt_fine / 6.0) * (k1 + 2.0*k2 + 2.0*k3 + k4)\n        \n    # Sample the fine-grid trajectory to get the coarse-grid reference y\n    y = x_fine[::10]\n\n    # --- 2. Generate Forward Trajectory x ---\n    # The state trajectory is computed on the coarse grid using explicit Euler\n    x = np.zeros(N + 1)\n    x[0] = x0\n    \n    for n in range(N):\n        x[n+1] = x[n] + dt * f_model(x[n], theta)\n\n    # --- 3. Continuous-Adjoint Gradient (Optimize-then-Discretize) ---\n    p_C = np.zeros(N + 1)\n    p_C[N] = 0.0\n    \n    # Backward integration of the continuous adjoint ODE using backward explicit Euler\n    # p_n = p_{n+1} * (1 + dt*df/dx(x_{n+1})) + dt*(x_{n+1}-y_{n+1})\n    for n in range(N - 1, -1, -1):\n        p_C[n] = p_C[n+1] * (1.0 + dt * df_dx(x[n+1], theta)) + dt * (x[n+1] - y[n+1])\n        \n    # Compute the gradient integral using a left Riemann sum\n    grad_integral_C = 0.0\n    for n in range(N):\n        grad_integral_C += p_C[n] * df_dtheta(x[n], theta)\n    grad_integral_C *= dt\n    \n    grad_J_C = beta * (theta - theta_ref) + grad_integral_C\n\n    # --- 4. Discrete-Adjoint Gradient (Discretize-then-Optimize) ---\n    p_D = np.zeros(N + 1)\n    p_D[N] = 0.0\n    \n    # Backward recursion for the discrete adjoint variables\n    # p_n = p_{n+1} * (1 + dt*df/dx(x_n)) + dt*(x_n - y_n)\n    for n in range(N - 1, 0, -1): # n from N-1 down to 1\n        p_D[n] = p_D[n+1] * (1.0 + dt * df_dx(x[n], theta)) + dt * (x[n] - y[n])\n        \n    # Compute the gradient using the discrete-adjoint formula\n    grad_integral_D = 0.0\n    for n in range(N):\n        grad_integral_D += p_D[n+1] * df_dtheta(x[n], theta)\n    grad_integral_D *= dt\n\n    grad_J_D = beta * (theta - theta_ref) + grad_integral_D\n\n    # --- 5. Compute the Absolute Discrepancy ---\n    discrepancy = np.abs(grad_J_C - grad_J_D)\n    \n    return discrepancy\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (T, N, theta, beta, theta_ref, x0, theta_true)\n    test_cases = [\n        (2.0, 200, 1.0, 0.05, 0.8, 0.2, 1.2),\n        (2.0, 20, 1.0, 0.05, 0.8, 0.2, 1.2),\n        (2.0, 1000, 1.0, 0.05, 0.8, 0.2, 1.2),\n        (2.0, 200, -0.8, 0.05, 0.0, 0.2, 1.2),\n        (2.0, 200, 1.0, 0.05, 0.8, 0.99, 1.2),\n    ]\n\n    results = []\n    for case in test_cases:\n        discrepancy = compute_discrepancy(*case)\n        # Format result to 8 decimal places as required.\n        results.append(f\"{discrepancy:.8f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3430507"}, {"introduction": "Solving the large-scale nonlinear least-squares problem of 4D-Var requires more sophisticated techniques than simple gradient descent. This practice delves into the trust-region Gauss–Newton method, a powerful and robust algorithm commonly used in operational data assimilation systems. You will build a quadratic model of the cost function and implement a practical strategy for selecting the trust-region radius, gaining insight into how these methods balance rapid convergence with stability. [@problem_id:3430474]", "problem": "Consider the incremental form of four-dimensional variational (4D-Var) data assimilation. Let the state increment be denoted by $\\delta x \\in \\mathbb{R}^n$. The background error covariance is $\\mathbf{B} \\in \\mathbb{R}^{n \\times n}$, assumed symmetric positive definite. For time indices $i \\in \\{1,\\dots, m\\}$, let the tangent-linear model mapping from the initial time to time $i$ be $\\mathbf{M}_{0,i} \\in \\mathbb{R}^{n \\times n}$, the linearized observation operator be $\\mathbf{H}_i \\in \\mathbb{R}^{p_i \\times n}$, the observation error covariance be $\\mathbf{R}_i \\in \\mathbb{R}^{p_i \\times p_i}$, symmetric positive definite, and the observed innovation be $d_i \\in \\mathbb{R}^{p_i}$. The incremental cost function is\n$$\nJ(\\delta x) = \\tfrac{1}{2}\\,\\|\\delta x\\|_{\\mathbf{B}^{-1}}^2 + \\tfrac{1}{2}\\,\\sum_{i=1}^m \\|\\mathbf{H}_i \\mathbf{M}_{0,i}\\,\\delta x - d_i\\|_{\\mathbf{R}_i^{-1}}^2,\n$$\nwith $\\|u\\|_{\\mathbf{W}}^2 = u^\\top \\mathbf{W}\\, u$ for a symmetric positive definite matrix $\\mathbf{W}$. At the current linearization point $\\delta x = 0$, define the Gauss–Newton quadratic model\n$$\nq(s) = J(0) + \\nabla J(0)^\\top s + \\tfrac{1}{2}\\, s^\\top \\mathbf{H}_{\\text{GN}}\\, s,\n$$\nwhere the Gauss–Newton Hessian is\n$$\n\\mathbf{H}_{\\text{GN}} = \\mathbf{B}^{-1} + \\sum_{i=1}^m (\\mathbf{H}_i \\mathbf{M}_{0,i})^\\top \\mathbf{R}_i^{-1} (\\mathbf{H}_i \\mathbf{M}_{0,i}),\n$$\nand the gradient at $\\delta x = 0$ is\n$$\n\\nabla J(0) = \\mathbf{B}^{-1}\\, 0 + \\sum_{i=1}^m (\\mathbf{H}_i \\mathbf{M}_{0,i})^\\top \\mathbf{R}_i^{-1}\\,(\\mathbf{H}_i \\mathbf{M}_{0,i}\\,0 - d_i) = - \\sum_{i=1}^m (\\mathbf{H}_i \\mathbf{M}_{0,i})^\\top \\mathbf{R}_i^{-1}\\, d_i.\n$$\nYou will construct a trust-region Gauss–Newton method for the quadratic model $q(s)$ with a radius $\\Delta$ selected by a predicted-reduction criterion that uses the Gauss–Newton Hessian. Specifically:\n- The Gauss–Newton step $s_{\\text{GN}}$ is the minimizer of $q(s)$ over $\\mathbb{R}^n$.\n- The model-predicted reduction for a step $s$ is defined by $\\mathrm{pred}(s) = q(0) - q(s)$.\n- The steepest-descent (Cauchy) direction at $s=0$ is $- \\nabla J(0)$, and for a given radius $r > 0$, the Cauchy step constrained to the ball of radius $r$ is $s_C(r) = - \\alpha(r)\\, \\nabla J(0)$ with $\\alpha(r) = r / \\|\\nabla J(0)\\|_2$.\n\nTrust-region radius selection by predicted reduction:\n- Given a target fraction $\\gamma \\in (0,1]$ and a maximum radius $\\Delta_{\\max} > 0$, select $\\Delta$ to be the smallest radius $r \\in (0, \\Delta_{\\max}]$ such that the predicted reduction along the Cauchy step at radius $r$ equals $\\gamma$ times the predicted reduction achieved by the Gauss–Newton step, i.e., enforce $\\mathrm{pred}(s_C(r)) = \\gamma \\,\\mathrm{pred}(s_{\\text{GN}})$, whenever such an $r$ exists. If no such $r$ exists (because the maximum achievable predicted reduction along the Cauchy direction is smaller than $\\gamma \\,\\mathrm{pred}(s_{\\text{GN}})$), then choose $r$ to be the radius that maximizes the predicted reduction along the Cauchy direction, and finally set $\\Delta = \\min\\{r, \\Delta_{\\max}\\}$.\n\nAnalysis goal:\n- Using only fundamental definitions and linear algebra, determine, for each test case below, whether the unconstrained Gauss–Newton step $s_{\\text{GN}}$ lies strictly outside the selected trust region, i.e., whether $\\|s_{\\text{GN}}\\|_2 > \\Delta$. Report a boolean indicator for this event, and also report the norm $\\|s_{\\text{GN}}\\|_2$, the selected radius $\\Delta$, the predicted reduction $\\mathrm{pred}(s_{\\text{GN}})$, the predicted reduction $\\mathrm{pred}(s_C(\\Delta))$, and the ratio $\\mathrm{pred}(s_C(\\Delta))\\,/\\,\\mathrm{pred}(s_{\\text{GN}})$.\n\nYour program must implement the following steps from first principles:\n- Assemble $\\mathbf{H}_{\\text{GN}}$ and $\\nabla J(0)$ from the given $\\mathbf{B}$, $\\mathbf{M}_{0,i}$, $\\mathbf{H}_i$, $\\mathbf{R}_i$, and $d_i$.\n- Compute the Gauss–Newton step $s_{\\text{GN}}$ as the unique global minimizer of $q(s)$ in $\\mathbb{R}^n$.\n- Compute the model-predicted reductions $\\mathrm{pred}(s_{\\text{GN}})$ and $\\mathrm{pred}(s_C(r))$ for a general radius $r$.\n- Implement the radius-selection rule stated above to obtain $\\Delta$ based on the target fraction $\\gamma$ and the cap $\\Delta_{\\max}$, using only the definitions of predicted reduction and the Cauchy step.\n- Decide whether $\\|s_{\\text{GN}}\\|_2 > \\Delta$.\n\nTest suite:\nFor each case, $n$ is the state dimension, $m$ is the number of observation times, and all matrices and vectors are given numerically. Use the exact values listed here.\n\nCase $1$ (boundary case with isotropic curvature):\n- $n = 2$, $m = 1$.\n- $\\mathbf{B} = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n- $\\mathbf{M}_{0,1} = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n- $\\mathbf{H}_1 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n- $\\mathbf{R}_1 = \\begin{bmatrix} 0.1 & 0.0 \\\\ 0.0 & 0.1 \\end{bmatrix}$.\n- $d_1 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$.\n- $\\gamma = 1.0$, $\\Delta_{\\max} = 10.0$.\n\nCase $2$ (leaving due to strict fraction):\n- Same as Case $1$ but with $\\gamma = 0.25$, $\\Delta_{\\max} = 10.0$.\n\nCase $3$ (ill-conditioned, multi-time, anisotropic):\n- $n = 2$, $m = 2$.\n- $\\mathbf{B} = \\begin{bmatrix} 100.0 & 0.0 \\\\ 0.0 & 0.1 \\end{bmatrix}$.\n- $\\mathbf{M}_{0,1} = \\begin{bmatrix} 1.0 & 2.0 \\\\ 0.01 & 1.0 \\end{bmatrix}$, $\\mathbf{M}_{0,2} = \\begin{bmatrix} 0.5 & -0.3 \\\\ 0.2 & 1.5 \\end{bmatrix}$.\n- $\\mathbf{H}_1 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$, $\\mathbf{H}_2 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n- $\\mathbf{R}_1 = \\begin{bmatrix} 0.5 & 0.0 \\\\ 0.0 & 2.0 \\end{bmatrix}$, $\\mathbf{R}_2 = \\begin{bmatrix} 0.1 & 0.0 \\\\ 0.0 & 0.2 \\end{bmatrix}$.\n- $d_1 = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$, $d_2 = \\begin{bmatrix} 0.2 \\\\ -0.5 \\end{bmatrix}$.\n- $\\gamma = 1.0$, $\\Delta_{\\max} = 10.0$.\n\nCase $4$ (capped radius):\n- Same as Case $1$ but with $\\gamma = 1.0$, $\\Delta_{\\max} = 0.5$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no whitespace. Each test case contributes a list of six entries in the order $[\\text{leave}, \\|s_{\\text{GN}}\\|_2, \\Delta, \\mathrm{pred}(s_{\\text{GN}}), \\mathrm{pred}(s_C(\\Delta)), \\mathrm{pred}(s_C(\\Delta))/\\mathrm{pred}(s_{\\text{GN}})]$, where $\\text{leave}$ is $1$ if $\\|s_{\\text{GN}}\\|_2 > \\Delta$ and $0$ otherwise. The overall output is thus a list of lists, for example, $[[\\dots],[\\dots],[\\dots],[\\dots]]$.", "solution": "The user-provided problem is a well-posed and scientifically grounded exercise in numerical optimization, specifically concerning the application of a trust-region Gauss-Newton method to the incremental 4D-Var data assimilation cost function. The problem asks for the implementation and analysis of a specific trust-region radius selection strategy based on the predicted reduction. We will proceed by first deriving the general formulas for the quantities of interest, followed by their application to the specific test cases provided.\n\nThe core of the problem lies in minimizing the Gauss-Newton quadratic model of the cost function $J(\\delta x)$ around the point $\\delta x = 0$. The model is given by:\n$$\nq(s) = J(0) + \\nabla J(0)^\\top s + \\tfrac{1}{2}\\, s^\\top \\mathbf{H}_{\\text{GN}}\\, s\n$$\nFor brevity, let us denote the gradient at $\\delta x = 0$ as $g \\equiv \\nabla J(0)$ and the Gauss-Newton Hessian as $H \\equiv \\mathbf{H}_{\\text{GN}}$. The quadratic model becomes:\n$$\nq(s) = J(0) + g^\\top s + \\tfrac{1}{2}\\, s^\\top H s\n$$\nThe gradient $g$ and Hessian $H$ are assembled from the problem components as follows:\n$$\ng = - \\sum_{i=1}^m (\\mathbf{H}_i \\mathbf{M}_{0,i})^\\top \\mathbf{R}_i^{-1}\\, d_i\n$$\n$$\nH = \\mathbf{B}^{-1} + \\sum_{i=1}^m (\\mathbf{H}_i \\mathbf{M}_{0,i})^\\top \\mathbf{R}_i^{-1} (\\mathbf{H}_i \\mathbf{M}_{0,i})\n$$\nSince $\\mathbf{B}$ and all $\\mathbf{R}_i$ are symmetric positive definite, their inverses $\\mathbf{B}^{-1}$ and $\\mathbf{R}_i^{-1}$ are also symmetric positive definite. The Hessian $H$ is a sum of a symmetric positive definite matrix ($\\mathbf{B}^{-1}$) and one or more symmetric positive semi-definite matrices ($(\\mathbf{H}_i \\mathbf{M}_{0,i})^\\top \\mathbf{R}_i^{-1} (\\mathbf{H}_i \\mathbf{M}_{0,i})$). Consequently, $H$ is symmetric positive definite, which guarantees that the quadratic model $q(s)$ is strictly convex and has a unique global minimizer.\n\nThe first step is to compute the unconstrained Gauss-Newton step, $s_{\\text{GN}}$. This step minimizes $q(s)$ over $\\mathbb{R}^n$ and is found by setting the gradient of $q(s)$ to zero:\n$$\n\\nabla q(s) = g + H s = 0 \\implies s_{\\text{GN}} = -H^{-1} g\n$$\n\nNext, we evaluate the model-predicted reduction, $\\mathrm{pred}(s)$, which measures the decrease in the model value $q(s)$ from its value at $s=0$:\n$$\n\\mathrm{pred}(s) = q(0) - q(s) = q(0) - (J(0) + g^\\top s + \\tfrac{1}{2}\\, s^\\top H s) = -g^\\top s - \\tfrac{1}{2}\\, s^\\top H s\n$$\nFor the Gauss-Newton step $s_{\\text{GN}}$, the predicted reduction is:\n$$\n\\mathrm{pred}(s_{\\text{GN}}) = -g^\\top(-H^{-1}g) - \\tfrac{1}{2}(-H^{-1}g)^\\top H (-H^{-1}g) = g^\\top H^{-1} g - \\tfrac{1}{2}g^\\top H^{-1} H H^{-1} g = \\tfrac{1}{2}g^\\top H^{-1} g\n$$\nAn equivalent expression, often useful for computation, is $\\mathrm{pred}(s_{\\text{GN}}) = -\\frac{1}{2}g^\\top s_{\\text{GN}}$.\n\nThe trust-region method constrains the minimization of $q(s)$ to a ball of radius $\\Delta$. The radius selection rule depends on the behavior of the model along the steepest-descent (Cauchy) direction, which is $p = -g$. A step along this direction with length $r$ is $s_C(r) = \\alpha(r) p = - \\alpha(r) g$. To have $\\|s_C(r)\\|_2 = r$, we must choose $\\alpha(r) = r / \\|g\\|_2$. Thus,\n$$\ns_C(r) = -\\frac{r}{\\|g\\|_2} g\n$$\nThe predicted reduction for this Cauchy step is a function of the radius $r$:\n$$\n\\mathrm{pred}(s_C(r)) = -g^\\top \\left(-\\frac{r}{\\|g\\|_2} g\\right) - \\tfrac{1}{2} \\left(-\\frac{r}{\\|g\\|_2} g\\right)^\\top H \\left(-\\frac{r}{\\|g\\|_2} g\\right)\n$$\n$$\n\\mathrm{pred}(s_C(r)) = \\frac{r}{\\|g\\|_2} (g^\\top g) - \\frac{1}{2} \\frac{r^2}{\\|g\\|_2^2} (g^\\top H g) = r \\|g\\|_2 - \\frac{g^\\top H g}{2 \\|g\\|_2^2} r^2\n$$\nLet's denote this quadratic function of $r$ as $P(r)$. The trust-region radius $\\Delta$ is selected by finding the smallest radius $r \\in (0, \\Delta_{\\max}]$ that satisfies the condition:\n$$\nP(r) = \\gamma \\,\\mathrm{pred}(s_{\\text{GN}})\n$$\nLet $C_{\\text{target}} = \\gamma \\,\\mathrm{pred}(s_{\\text{GN}})$. We must solve the quadratic equation for $r$:\n$$\n\\left(\\frac{g^\\top H g}{2 \\|g\\|_2^2}\\right) r^2 - \\|g\\|_2 r + C_{\\text{target}} = 0\n$$\nThe discriminant of this equation is $\\mathcal{D} = (\\|g\\|_2)^2 - 4 \\left(\\frac{g^\\top H g}{2 \\|g\\|_2^2}\\right) C_{\\text{target}} = \\|g\\|_2^2 - 2 \\frac{g^\\top H g}{\\|g\\|_2^2} C_{\\text{target}}$.\n- If $\\mathcal{D} \\ge 0$, real solutions for $r$ exist. Since the quadratic term's coefficient is positive and the linear term's coefficient is negative, and $C_{\\text{target}} > 0$ (assuming $g \\neq 0$), both roots are positive. We choose the smaller root:\n  $$\n  r^* = \\frac{\\|g\\|_2 - \\sqrt{\\mathcal{D}}}{g^\\top H g / \\|g\\|_2^2}\n  $$\n- If $\\mathcal{D} < 0$, no real solution exists. This physically means that the target reduction $C_{\\text{target}}$ is greater than the maximum achievable reduction along the Cauchy direction. The problem statement specifies that in this case, we should choose the radius $r$ that maximizes $P(r)$. This radius is found by setting $P'(r) = 0$:\n  $$\n  P'(r) = \\|g\\|_2 - r \\frac{g^\\top H g}{\\|g\\|_2^2} = 0 \\implies r^* = \\frac{\\|g\\|_2^3}{g^\\top H g}\n  $$\n  This radius $r^*$ corresponds to the length of the \"Cauchy point\".\n\nAfter determining the candidate radius $r^*$, the final trust-region radius $\\Delta$ is capped by $\\Delta_{\\max}$:\n$$\n\\Delta = \\min(r^*, \\Delta_{\\max})\n$$\nFinally, for each test case, we compute the required quantities: $\\|s_{\\text{GN}}\\|_2$, $\\Delta$, $\\mathrm{pred}(s_{\\text{GN}})$, and $\\mathrm{pred}(s_C(\\Delta))$, and determine if the Gauss-Newton step lies outside the trust region, i.e., if $\\|s_{\\text{GN}}\\|_2 > \\Delta$. The ratio $\\mathrm{pred}(s_C(\\Delta)) / \\mathrm{pred}(s_{\\text{GN}})$ is also computed. The implementation will perform these calculations for each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the trust-region subproblem for each test case.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: boundary case with isotropic curvature\n        {\n            \"n\": 2, \"m\": 1,\n            \"B\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"M0_list\": [np.array([[1.0, 0.0], [0.0, 1.0]])],\n            \"H_list\": [np.array([[1.0, 0.0], [0.0, 1.0]])],\n            \"R_list\": [np.array([[0.1, 0.0], [0.0, 0.1]])],\n            \"d_list\": [np.array([1.0, 1.0])],\n            \"gamma\": 1.0,\n            \"delta_max\": 10.0\n        },\n        # Case 2: leaving due to strict fraction\n        {\n            \"n\": 2, \"m\": 1,\n            \"B\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"M0_list\": [np.array([[1.0, 0.0], [0.0, 1.0]])],\n            \"H_list\": [np.array([[1.0, 0.0], [0.0, 1.0]])],\n            \"R_list\": [np.array([[0.1, 0.0], [0.0, 0.1]])],\n            \"d_list\": [np.array([1.0, 1.0])],\n            \"gamma\": 0.25,\n            \"delta_max\": 10.0\n        },\n        # Case 3: ill-conditioned, multi-time, anisotropic\n        {\n            \"n\": 2, \"m\": 2,\n            \"B\": np.array([[100.0, 0.0], [0.0, 0.1]]),\n            \"M0_list\": [np.array([[1.0, 2.0], [0.01, 1.0]]), np.array([[0.5, -0.3], [0.2, 1.5]])],\n            \"H_list\": [np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[1.0, 0.0], [0.0, 1.0]])],\n            \"R_list\": [np.array([[0.5, 0.0], [0.0, 2.0]]), np.array([[0.1, 0.0], [0.0, 0.2]])],\n            \"d_list\": [np.array([1.0, -1.0]), np.array([0.2, -0.5])],\n            \"gamma\": 1.0,\n            \"delta_max\": 10.0\n        },\n        # Case 4: capped radius\n        {\n            \"n\": 2, \"m\": 1,\n            \"B\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"M0_list\": [np.array([[1.0, 0.0], [0.0, 1.0]])],\n            \"H_list\": [np.array([[1.0, 0.0], [0.0, 1.0]])],\n            \"R_list\": [np.array([[0.1, 0.0], [0.0, 0.1]])],\n            \"d_list\": [np.array([1.0, 1.0])],\n            \"gamma\": 1.0,\n            \"delta_max\": 0.5\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(case)\n        all_results.append(result)\n\n    # Format the final output string\n    # e.g. [[1,0.632,0.531,0.823,0.700,0.850]]\n    output_str = \"[\"\n    for i, res in enumerate(all_results):\n        leave_val = 1 if res['leave'] else 0\n        res_str = f\"[{leave_val},{res['norm_s_gn']:.7f},{res['delta']:.7f},{res['pred_s_gn']:.7f},{res['pred_s_c_delta']:.7f},{res['ratio']:.7f}]\"\n        output_str += res_str\n        if i < len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\n\ndef process_case(case_data):\n    \"\"\"\n    Computes all required quantities for a single test case.\n    \"\"\"\n    B = case_data['B']\n    M0_list = case_data['M0_list']\n    H_list = case_data['H_list']\n    R_list = case_data['R_list']\n    d_list = case_data['d_list']\n    gamma = case_data['gamma']\n    delta_max = case_data['delta_max']\n    n = case_data['n']\n    m = case_data['m']\n\n    # Assemble gradient and Hessian\n    B_inv = np.linalg.inv(B)\n    g = np.zeros(n)\n    H_gn = np.copy(B_inv)\n\n    for i in range(m):\n        M0i = M0_list[i]\n        Hi = H_list[i]\n        Ri = R_list[i]\n        di = d_list[i]\n        \n        Ri_inv = np.linalg.inv(Ri)\n        HiM0i = Hi @ M0i\n\n        g -= (HiM0i.T @ Ri_inv @ di)\n        H_gn += (HiM0i.T @ Ri_inv @ HiM0i)\n\n    # Compute Gauss-Newton step and its norm\n    s_gn = np.linalg.solve(H_gn, -g)\n    norm_s_gn = np.linalg.norm(s_gn)\n\n    # Compute predicted reduction for GN step\n    pred_s_gn = -0.5 * g.T @ s_gn\n\n    # Radius selection logic\n    if np.linalg.norm(g) < 1e-12:\n        # If gradient is zero, optimal step is zero, delta is arbitrary but > 0\n        r_star = delta_max \n    else:\n        C_target = gamma * pred_s_gn\n        norm_g = np.linalg.norm(g)\n        norm_g_sq = norm_g**2\n        gHg = g.T @ H_gn @ g\n\n        # We solve a*r^2 + b*r + c = 0\n        # where (gHg / (2*norm_g_sq)) r^2 - norm_g r + C_target = 0\n        a = 0.5 * gHg / norm_g_sq\n        b = -norm_g\n        c = C_target\n        \n        discriminant = b**2 - 4 * a * c\n        \n        if discriminant >= 0:\n            # Smallest positive root\n            r_star = (-b - np.sqrt(discriminant)) / (2 * a)\n        else:\n            # Maximizer of pred(s_C(r))\n            r_star = norm_g**3 / gHg\n\n    delta = min(r_star, delta_max)\n\n    # Final calculations\n    leave = norm_s_gn > delta\n    \n    # Predicted reduction for Cauchy step of length delta\n    norm_g = np.linalg.norm(g)\n    norm_g_sq = norm_g**2\n    gHg = g.T @ H_gn @ g\n    pred_s_c_delta = delta * norm_g - 0.5 * delta**2 * gHg / norm_g_sq\n\n    # Ratio of predicted reductions\n    if abs(pred_s_gn) < 1e-15:\n        # Avoid division by zero, although not expected in these cases\n        ratio = 0.0 if abs(pred_s_c_delta) < 1e-15 else np.inf\n    else:\n        ratio = pred_s_c_delta / pred_s_gn\n\n    return {\n        \"leave\": leave,\n        \"norm_s_gn\": norm_s_gn,\n        \"delta\": delta,\n        \"pred_s_gn\": pred_s_gn,\n        \"pred_s_c_delta\": pred_s_c_delta,\n        \"ratio\": ratio\n    }\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3430474"}]}