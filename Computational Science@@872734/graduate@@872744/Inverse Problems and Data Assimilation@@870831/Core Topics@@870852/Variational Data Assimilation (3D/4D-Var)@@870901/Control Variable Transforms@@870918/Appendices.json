{"hands_on_practices": [{"introduction": "The primary goal of control variable transforms in variational data assimilation is often to improve the conditioning of the underlying optimization problem. This exercise provides a concrete numerical demonstration of this preconditioning effect [@problem_id:3372107]. By explicitly computing and comparing the condition numbers of the Hessian matrix before and after applying the standard $B^{1/2}$ transform, you will gain a quantitative understanding of how this technique accelerates the convergence of optimization algorithms.", "problem": "Consider a one-dimensional variational data assimilation problem with a linear observation operator. The cost function is given by the standard quadratic form in variational data assimilation:\n$$\nJ(x) = \\frac{1}{2} \\left\\| x - x_b \\right\\|_{B^{-1}}^2 + \\frac{1}{2} \\left\\| y - H x \\right\\|_{R^{-1}}^2,\n$$\nwhere $x \\in \\mathbb{R}^n$ is the state vector, $x_b \\in \\mathbb{R}^n$ is the background state, $y \\in \\mathbb{R}^n$ are observations, $B \\in \\mathbb{R}^{n \\times n}$ is the background error covariance matrix, $H \\in \\mathbb{R}^{n \\times n}$ is the linear observation operator, and $R \\in \\mathbb{R}^{n \\times n}$ is the observation error covariance matrix. Assume $H = I$ and $R = \\sigma_o^2 I$, where $I$ is the identity matrix and $\\sigma_o^2  0$ is the observation error variance.\n\nThe Gauss-Newton approximation of the Hessian of $J(x)$ at any point is\n$$\nK = B^{-1} + H^\\top R^{-1} H.\n$$\nWith $H = I$ and $R = \\sigma_o^2 I$, this simplifies to\n$$\nK = B^{-1} + \\frac{1}{\\sigma_o^2} I.\n$$\n\nIntroduce the control variable transform defined by $x = x_b + L v$, where $L \\in \\mathbb{R}^{n \\times n}$ is chosen such that $L L^\\top = B$ (so $L$ is a matrix square root of $B$), and $v \\in \\mathbb{R}^n$ is the control variable. In $v$-space, the Gauss-Newton Hessian becomes\n$$\nK_v = I + L^\\top H^\\top R^{-1} H L.\n$$\nFor $H = I$ and $R = \\sigma_o^2 I$, this is\n$$\nK_v = I + \\frac{1}{\\sigma_o^2} L^\\top L = I + \\frac{1}{\\sigma_o^2} B.\n$$\n\nLet the background error covariance $B$ be defined by an exponential correlation on a uniform one-dimensional grid of $n$ points over the unit interval $[0,1]$. The grid points are $x_i = \\frac{i}{n-1}$ for $i = 0, 1, \\dots, n-1$. The entries of $B$ are defined as\n$$\nB_{ij} = \\sigma_b^2 \\exp\\left(-\\frac{|x_i - x_j|}{\\ell}\\right),\n$$\nwhere $\\sigma_b^2  0$ is a background variance and $\\ell  0$ is a correlation length scale, both dimensionless.\n\nYour task is to:\n- Construct $B$ for $n = 64$, $\\sigma_b^2 = 1$, and $\\ell = 0.2$.\n- Compute $L = B^{1/2}$ using an eigenvalue decomposition of $B$.\n- For each specified value of $\\sigma_o^2$, compute the $2$-norm condition number (Spectral Norm (2-norm) condition number) of the untransformed Hessian $K$ and the transformed Hessian $K_v$. The $2$-norm condition number of a symmetric positive definite matrix $A$ is defined as\n$$\n\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)},\n$$\nwhere $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ are the largest and smallest eigenvalues of $A$, respectively.\n\nProvide numerical results for the following test suite of observation error variances:\n- $\\sigma_o^2 = 10^{-8}$ (edge case: extremely accurate observations),\n- $\\sigma_o^2 = 10^{-4}$,\n- $\\sigma_o^2 = 10^{-2}$,\n- $\\sigma_o^2 = 10^{0}$ (general case),\n- $\\sigma_o^2 = 10^{2}$ (edge case: extremely noisy observations).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of the list should itself be a two-element list containing the pair $[\\kappa_2(K), \\kappa_2(K_v)]$ for the corresponding $\\sigma_o^2$. For example, the output format must be\n$$\n[\\,[\\kappa_2(K\\_1),\\kappa_2(K\\_{v,1})],\\,[\\kappa_2(K\\_2),\\kappa_2(K\\_{v,2})],\\,\\dots\\,],\n$$\nprinted as a single line in standard Python list syntax. No physical units are involved; all quantities are dimensionless.", "solution": "The problem statement is a well-posed and scientifically grounded exercise in numerical linear algebra applied to variational data assimilation. It asks for the computation of condition numbers for two different formulations of the Hessian matrix in a standard data assimilation cost function. All parameters and definitions are provided, and the problem is self-contained and free of contradictions or ambiguities. It is a valid problem.\n\nThe core task is to compute the spectral condition number, $\\kappa_2(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$, for two matrices: the untransformed Hessian $K$ and the transformed Hessian $K_v$.\nThe provided definitions are:\n1.  The untransformed Hessian: $K = B^{-1} + \\frac{1}{\\sigma_o^2} I$\n2.  The transformed Hessian: $K_v = I + \\frac{1}{\\sigma_o^2} B$\n\nHere, $B$ is the background error covariance matrix, $I$ is the identity matrix, and $\\sigma_o^2$ is the observation error variance. To compute the condition numbers, we must find the minimum and maximum eigenvalues of $K$ and $K_v$. These can be directly related to the eigenvalues of $B$.\n\nLet $\\{\\lambda_i(B)\\}$ be the set of eigenvalues of the matrix $B$. Since $B$ is constructed to be symmetric and positive definite, its eigenvalues are real and positive. Let $\\lambda_{\\min}(B)$ and $\\lambda_{\\max}(B)$ denote the smallest and largest eigenvalues of $B$, respectively.\n\n**Eigenvalues of $K$**\n\nThe eigenvalues of the inverse matrix $B^{-1}$ are the reciprocals of the eigenvalues of $B$, i.e., $\\{1/\\lambda_i(B)\\}$. The ordering is reversed, so the largest eigenvalue of $B^{-1}$ is $1/\\lambda_{\\min}(B)$, and the smallest is $1/\\lambda_{\\max}(B)$.\nAdding a scaled identity matrix $\\alpha I$ to any matrix shifts its eigenvalues by $\\alpha$. In our case, $\\alpha = 1/\\sigma_o^2$.\nTherefore, the extremal eigenvalues of $K = B^{-1} + \\frac{1}{\\sigma_o^2} I$ are:\n$$\n\\lambda_{\\max}(K) = \\lambda_{\\max}(B^{-1}) + \\frac{1}{\\sigma_o^2} = \\frac{1}{\\lambda_{\\min}(B)} + \\frac{1}{\\sigma_o^2}\n$$\n$$\n\\lambda_{\\min}(K) = \\lambda_{\\min}(B^{-1}) + \\frac{1}{\\sigma_o^2} = \\frac{1}{\\lambda_{\\max}(B)} + \\frac{1}{\\sigma_o^2}\n$$\nThe condition number of $K$ is then:\n$$\n\\kappa_2(K) = \\frac{\\lambda_{\\max}(K)}{\\lambda_{\\min}(K)} = \\frac{\\frac{1}{\\lambda_{\\min}(B)} + \\frac{1}{\\sigma_o^2}}{\\frac{1}{\\lambda_{\\max}(B)} + \\frac{1}{\\sigma_o^2}}\n$$\n\n**Eigenvalues of $K_v$**\n\nThe eigenvalues of the matrix $\\frac{1}{\\sigma_o^2}B$ are $\\{\\frac{\\lambda_i(B)}{\\sigma_o^2}\\}$. Adding the identity matrix $I$ shifts these eigenvalues by $1$.\nTherefore, the extremal eigenvalues of $K_v = I + \\frac{1}{\\sigma_o^2} B$ are:\n$$\n\\lambda_{\\max}(K_v) = 1 + \\frac{\\lambda_{\\max}(B)}{\\sigma_o^2}\n$$\n$$\n\\lambda_{\\min}(K_v) = 1 + \\frac{\\lambda_{\\min}(B)}{\\sigma_o^2}\n$$\nThe condition number of $K_v$ is then:\n$$\n\\kappa_2(K_v) = \\frac{\\lambda_{\\max}(K_v)}{\\lambda_{\\min}(K_v)} = \\frac{1 + \\frac{\\lambda_{\\max}(B)}{\\sigma_o^2}}{1 + \\frac{\\lambda_{\\min}(B)}{\\sigma_o^2}}\n$$\n\nThis analytical approach reveals that we only need to compute the extremal eigenvalues of the matrix $B$. The explicit formation of $B^{-1}$ or the matrix square root $L=B^{1/2}$ is not required for the calculation of the condition numbers.\n\nThe numerical implementation plan is as follows:\n1.  Define the problem parameters: grid size $n = 64$, background variance $\\sigma_b^2 = 1$, and correlation length $\\ell = 0.2$.\n2.  Create the one-dimensional grid of $n$ points over the interval $[0, 1]$.\n3.  Construct the $n \\times n$ background error covariance matrix $B$ using its definition: $B_{ij} = \\sigma_b^2 \\exp\\left(-\\frac{|x_i - x_j|}{\\ell}\\right)$.\n4.  Compute the eigenvalues of the real symmetric matrix $B$. A specialized algorithm for symmetric matrices, such as `numpy.linalg.eigvalsh`, is appropriate as it is efficient and numerically stable. This function returns sorted eigenvalues.\n5.  Extract the minimum and maximum eigenvalues, $\\lambda_{\\min}(B)$ and $\\lambda_{\\max}(B)$.\n6.  Iterate through the given list of observation error variances $\\sigma_o^2 = \\{10^{-8}, 10^{-4}, 10^{-2}, 10^{0}, 10^{2}\\}$.\n7.  In each iteration, calculate $\\kappa_2(K)$ and $\\kappa_2(K_v)$ using the derived formulas.\n8.  Store each pair $[\\kappa_2(K), \\kappa_2(K_v)]$ and format the final list of pairs into the required single-line string output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the condition numbers of the untransformed (K) and transformed (Kv)\n    Hessians for a 1D variational data assimilation problem.\n    \"\"\"\n    # 1. Define problem parameters\n    n = 64\n    sigma_b_sq = 1.0\n    l_corr = 0.2\n    \n    test_cases_sigma_o_sq = [1e-8, 1e-4, 1e-2, 1.0, 1e2]\n\n    # 2. Create the grid\n    x_grid = np.linspace(0.0, 1.0, n)\n\n    # 3. Construct the background error covariance matrix B\n    # Use broadcasting for an efficient construction of the distance matrix\n    dist_matrix = np.abs(x_grid[:, np.newaxis] - x_grid)\n    B = sigma_b_sq * np.exp(-dist_matrix / l_corr)\n    \n    # 4. Compute eigenvalues of B\n    # B is a real symmetric matrix. numpy.linalg.eigvalsh is efficient and\n    # returns sorted eigenvalues in ascending order.\n    eigvals_B = np.linalg.eigvalsh(B)\n    \n    # 5. Extract minimum and maximum eigenvalues\n    lambda_min_B = eigvals_B[0]\n    lambda_max_B = eigvals_B[-1]\n    \n    results = []\n    \n    # 6. Loop through test cases for sigma_o^2\n    for s2o in test_cases_sigma_o_sq:\n        # 7. Calculate condition number for K = B^-1 + (1/s2o) * I\n        lambda_max_K = 1.0 / lambda_min_B + 1.0 / s2o\n        lambda_min_K = 1.0 / lambda_max_B + 1.0 / s2o\n        cond_K = lambda_max_K / lambda_min_K\n        \n        # Calculate condition number for Kv = I + (1/s2o) * B\n        lambda_max_Kv = 1.0 + (1.0 / s2o) * lambda_max_B\n        lambda_min_Kv = 1.0 + (1.0 / s2o) * lambda_min_B\n        cond_Kv = lambda_max_Kv / lambda_min_Kv\n        \n        results.append([cond_K, cond_Kv])\n\n    # 8. Format the final output string as specified\n    string_parts = []\n    for res_pair in results:\n        string_parts.append(f\"[{res_pair[0]},{res_pair[1]}]\")\n    final_string = f\"[{','.join(string_parts)}]\"\n    \n    print(final_string)\n\nsolve()\n```", "id": "3372107"}, {"introduction": "Beyond preconditioning, control variable transforms are essential for enforcing physical constraints, such as positivity. The choice of transform is not arbitrary, as it re-shapes the optimization landscape, and this practice delves into this critical aspect by comparing two popular positivity-enforcing transforms: the exponential map and the softplus function [@problem_id:3372041]. You will analytically and numerically investigate how each transform affects the gradient and curvature of the objective function, revealing important trade-offs related to optimization stability and performance.", "problem": "Consider a scalar positive state variable $x \\in (0,\\infty)$ to be estimated in a nonlinear variational Data Assimilation (DA) problem. Let $z \\in \\mathbb{R}$ be an unconstrained control variable related to $x$ through a control variable transform $x = \\tau(z)$. The variational objective is defined by a background term and a single nonlinear observation term with Gaussian errors:\n$$\nJ(z) = \\frac{1}{2}\\frac{\\left(x(z) - x_b\\right)^2}{\\sigma_b^2} + \\frac{1}{2}\\frac{\\left(h\\big(x(z)\\big) - y\\right)^2}{\\sigma_o^2},\n$$\nwhere $x_b \\in (0,\\infty)$ is the background (prior) mean, $\\sigma_b^2  0$ is the background variance, $y \\in \\mathbb{R}$ is the observation, $\\sigma_o^2  0$ is the observation-error variance, and $h:(0,\\infty)\\to\\mathbb{R}$ is a twice-differentiable observation operator. In this problem, use the specific nonlinear observation operator $h(x) = \\sqrt{x}$.\n\nTwo different positivity-enforcing control variable transforms are to be compared:\n- Softplus transform: $x(z) = \\log\\!\\left(1 + e^{z}\\right)$.\n- Exponential log-transform: $x(z) = e^{z}$.\n\nTasks:\n1. Starting from the definitions of $J(z)$, $h(x)$, and the two transforms above, and using only standard rules of differential calculus (including the chain rule), derive for each transform explicit expressions for the first derivative $\\frac{dJ}{dz}$ and the second derivative $\\frac{d^2J}{dz^2}$, both evaluated at an arbitrary $z \\in \\mathbb{R}$.\n2. Implement these expressions in a program and evaluate them for each transform on the test suite below. For each test case, compute the following two scalar diagnostics:\n   - The gradient saturation ratio $r_g = \\frac{\\left|\\frac{dJ}{dz}\\right|_{\\text{softplus}}}{\\left|\\frac{dJ}{dz}\\right|_{\\exp}}$.\n   - The curvature ratio $r_H = \\frac{\\left.\\frac{d^2J}{dz^2}\\right|_{\\text{softplus}}}{\\left.\\frac{d^2J}{dz^2}\\right|_{\\exp}}$.\n3. Report all results as a single flat list in the specified order.\n\nTest suite (each case is a tuple $(z, x_b, \\sigma_b^2, y, \\sigma_o^2)$):\n- Case 1 (general): $(0.0, 1.0, 0.25, 1.2, 0.04)$.\n- Case 2 (strongly negative control, small state): $(-20.0, 1.0, 0.25, 0.8, 0.04)$.\n- Case 3 (strongly positive control, large state under exponential): $(10.0, 2.0, 0.25, 100.0, 0.04)$.\n- Case 4 (moderately negative control, curvature near small state): $(-5.0, 0.5, 0.25, 0.1, 0.04)$.\n\nYour program must output a single line containing a flat list with eight floating-point numbers in the order\n$$\n\\left[r_g^{(1)}, r_H^{(1)}, r_g^{(2)}, r_H^{(2)}, r_g^{(3)}, r_H^{(3)}, r_g^{(4)}, r_H^{(4)}\\right],\n$$\nwhere the superscripts refer to the test case index. Round each numeric output to six decimal places. The output must be a single line, representing the list in standard bracketed form with comma-separated values (for example, $[0.123456,0.654321,\\dots]$). No physical units are involved; all quantities are nondimensional real numbers. Angles do not appear anywhere in the problem, so no angle unit is required.", "solution": "The problem is valid. It is scientifically grounded in the principles of variational data assimilation and differential calculus, is well-posed, and all necessary information is provided.\n\nThe objective is to derive the first and second derivatives of a variational cost function $J(z)$ with respect to a control variable $z$, for two different control variable transforms $x(z)$, and then to evaluate these derivatives and their ratios for a set of test cases.\n\nThe cost function is given by:\n$$\nJ(z) = \\frac{1}{2}\\frac{\\left(x(z) - x_b\\right)^2}{\\sigma_b^2} + \\frac{1}{2}\\frac{\\left(h\\big(x(z)\\big) - y\\right)^2}{\\sigma_o^2}\n$$\nwhere $x$ is the state variable, $z$ is the control variable, $x_b$ is the background state, $y$ is the observation, $\\sigma_b^2$ and $\\sigma_o^2$ are variances, and $h(x)$ is the observation operator. For this problem, $h(x) = \\sqrt{x}$.\n\nThe derivatives of $J$ with respect to $z$ are found using the chain rule:\n$$\n\\frac{dJ}{dz} = \\frac{dJ}{dx} \\frac{dx}{dz}\n$$\n$$\n\\frac{d^2J}{dz^2} = \\frac{d}{dz}\\left(\\frac{dJ}{dx} \\frac{dx}{dz}\\right) = \\frac{d}{dx}\\left(\\frac{dJ}{dx}\\right) \\left(\\frac{dx}{dz}\\right) \\frac{dx}{dz} + \\frac{dJ}{dx} \\frac{d^2x}{dz^2} = \\frac{d^2J}{dx^2}\\left(\\frac{dx}{dz}\\right)^2 + \\frac{dJ}{dx}\\frac{d^2x}{dz^2}\n$$\n\nFirst, we compute the derivatives of $J$ with respect to $x$. These terms are common to both transforms.\nThe first derivative of $h(x) = x^{1/2}$ is $h'(x) = \\frac{d h}{d x} = \\frac{1}{2}x^{-1/2} = \\frac{1}{2\\sqrt{x}}$.\nThe second derivative is $h''(x) = \\frac{d^2 h}{d x^2} = -\\frac{1}{4}x^{-3/2} = -\\frac{1}{4x\\sqrt{x}}$.\n\nThe first derivative of $J$ with respect to $x$ is:\n$$\n\\frac{dJ}{dx} = \\frac{x - x_b}{\\sigma_b^2} + \\frac{h(x) - y}{\\sigma_o^2} h'(x) = \\frac{x - x_b}{\\sigma_b^2} + \\frac{\\sqrt{x} - y}{\\sigma_o^2} \\frac{1}{2\\sqrt{x}} = \\frac{x - x_b}{\\sigma_b^2} + \\frac{1}{2\\sigma_o^2}\\left(1 - \\frac{y}{\\sqrt{x}}\\right)\n$$\nThe second derivative of $J$ with respect to $x$ is:\n$$\n\\frac{d^2J}{dx^2} = \\frac{d}{dx}\\left(\\frac{dJ}{dx}\\right) = \\frac{1}{\\sigma_b^2} + \\frac{d}{dx}\\left(\\frac{1}{2\\sigma_o^2}\\left(1 - yx^{-1/2}\\right)\\right) = \\frac{1}{\\sigma_b^2} - \\frac{y}{2\\sigma_o^2} \\left(-\\frac{1}{2}x^{-3/2}\\right) = \\frac{1}{\\sigma_b^2} + \\frac{y}{4\\sigma_o^2 x^{3/2}} = \\frac{1}{\\sigma_b^2} + \\frac{y}{4x\\sqrt{x}\\sigma_o^2}\n$$\n\nNow we derive the expressions for each specific transform.\n\n**1. Exponential Log-Transform: $x(z) = e^z$**\n\nThe derivatives of the transform with respect to $z$ are:\n$$\n\\frac{dx}{dz} = e^z = x\n$$\n$$\n\\frac{d^2x}{dz^2} = e^z = x\n$$\nSubstituting these into the chain rule formulas:\n\nThe first derivative $\\frac{dJ}{dz}$ is:\n$$\n\\left.\\frac{dJ}{dz}\\right|_{\\exp} = \\left( \\frac{x - x_b}{\\sigma_b^2} + \\frac{1}{2\\sigma_o^2}\\left(1 - \\frac{y}{\\sqrt{x}}\\right) \\right) x\n$$\nwhere $x = e^z$.\n\nThe second derivative $\\frac{d^2J}{dz^2}$ is:\n$$\n\\left.\\frac{d^2J}{dz^2}\\right|_{\\exp} = \\left( \\frac{1}{\\sigma_b^2} + \\frac{y}{4x\\sqrt{x}\\sigma_o^2} \\right) \\left(x\\right)^2 + \\left( \\frac{x - x_b}{\\sigma_b^2} + \\frac{1}{2\\sigma_o^2}\\left(1 - \\frac{y}{\\sqrt{x}}\\right) \\right) x\n$$\nwhere $x = e^z$. Note that the second term is equal to $\\left.\\frac{dJ}{dz}\\right|_{\\exp}$.\n\n**2. Softplus Transform: $x(z) = \\log(1+e^z)$**\n\nThe derivatives of the transform with respect to $z$ are:\n$$\n\\frac{dx}{dz} = \\frac{e^z}{1+e^z}\n$$\nThis is the logistic sigmoid function, often denoted $\\sigma(z)$. It can also be written as $(1+e^{-z})^{-1}$.\n\n$$\n\\frac{d^2x}{dz^2} = \\frac{d}{dz}\\left(\\frac{e^z}{1+e^z}\\right) = \\frac{e^z(1+e^z) - e^z(e^z)}{(1+e^z)^2} = \\frac{e^z}{(1+e^z)^2}\n$$\nThis can be written compactly as $\\frac{dx}{dz}\\left(1 - \\frac{dx}{dz}\\right)$.\n\nSubstituting these into the chain rule formulas:\n\nThe first derivative $\\frac{dJ}{dz}$ is:\n$$\n\\left.\\frac{dJ}{dz}\\right|_{\\text{softplus}} = \\left( \\frac{x - x_b}{\\sigma_b^2} + \\frac{1}{2\\sigma_o^2}\\left(1 - \\frac{y}{\\sqrt{x}}\\right) \\right) \\left(\\frac{e^z}{1+e^z}\\right)\n$$\nwhere $x = \\log(1+e^z)$.\n\nThe second derivative $\\frac{d^2J}{dz^2}$ is:\n$$\n\\left.\\frac{d^2J}{dz^2}\\right|_{\\text{softplus}} = \\left( \\frac{1}{\\sigma_b^2} + \\frac{y}{4x\\sqrt{x}\\sigma_o^2} \\right) \\left(\\frac{e^z}{1+e^z}\\right)^2 + \\left( \\frac{x - x_b}{\\sigma_b^2} + \\frac{1}{2\\sigma_o^2}\\left(1 - \\frac{y}{\\sqrt{x}}\\right) \\right) \\left(\\frac{e^z}{(1+e^z)^2}\\right)\n$$\nwhere $x = \\log(1+e^z)$.\n\nThese explicit expressions are implemented in the provided Python code to compute the gradient saturation ratio $r_g$ and the curvature ratio $r_H$ for the given test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving and evaluating derivatives for two control\n    variable transforms and computing their ratios.\n    \"\"\"\n    test_cases = [\n        # (z, x_b, sigma_b^2, y, sigma_o^2)\n        (0.0, 1.0, 0.25, 1.2, 0.04),\n        (-20.0, 1.0, 0.25, 0.8, 0.04),\n        (10.0, 2.0, 0.25, 100.0, 0.04),\n        (-5.0, 0.5, 0.25, 0.1, 0.04),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        z, xb, s2b, y, s2o = case\n\n        # --- Calculations for Exponential Transform ---\n        # x(z) and its derivatives\n        x_exp = np.exp(z)\n        dxdz_exp = x_exp\n        d2xdz2_exp = x_exp\n\n        # Derivatives of J w.r.t x, evaluated at x_exp\n        sqrt_x_exp = np.sqrt(x_exp)\n        dJdx_exp = (x_exp - xb) / s2b + (1.0 - y / sqrt_x_exp) / (2.0 * s2o)\n        d2Jdx2_exp = 1.0 / s2b + y / (4.0 * s2o * x_exp * sqrt_x_exp)\n\n        # Derivatives of J w.r.t z using the chain rule\n        dJdz_exp = dJdx_exp * dxdz_exp\n        d2Jdz2_exp = d2Jdx2_exp * (dxdz_exp**2) + dJdx_exp * d2xdz2_exp\n\n        # --- Calculations for Softplus Transform ---\n        # x(z) and its derivatives (using numerically stable forms)\n        if z  0:\n            x_soft = z + np.log(1.0 + np.exp(-z))\n        else:\n            x_soft = np.log(1.0 + np.exp(z))\n        \n        dxdz_soft = 1.0 / (1.0 + np.exp(-z))  # Stable sigmoid\n        d2xdz2_soft = dxdz_soft * (1.0 - dxdz_soft)\n        \n        # Derivatives of J w.r.t x, evaluated at x_soft\n        sqrt_x_soft = np.sqrt(x_soft)\n        dJdx_soft = (x_soft - xb) / s2b + (1.0 - y / sqrt_x_soft) / (2.0 * s2o)\n        d2Jdx2_soft = 1.0 / s2b + y / (4.0 * s2o * x_soft * sqrt_x_soft)\n\n        # Derivatives of J w.r.t z using the chain rule\n        dJdz_soft = dJdx_soft * dxdz_soft\n        d2Jdz2_soft = d2Jdx2_soft * (dxdz_soft**2) + dJdx_soft * d2xdz2_soft\n\n        # --- Compute Ratios ---\n        # Gradient saturation ratio\n        rg = np.abs(dJdz_soft) / np.abs(dJdz_exp)\n        \n        # Curvature ratio\n        rH = d2Jdz2_soft / d2Jdz2_exp\n\n        results.extend([rg, rH])\n\n    # Format the final output string\n    formatted_results = \",\".join([f\"{val:.6f}\" for val in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```", "id": "3372041"}, {"introduction": "In large-scale data assimilation, the direct construction and application of the transform matrix $L$ can be computationally prohibitive, so this final practice explores how mathematical structure in the background covariance matrix $B$ can be exploited for highly efficient implementations [@problem_id:3372128]. By considering a common scenario where $B$ is circulant, you will design an algorithm that applies the transform $L$ and its transpose $L^T$ using the Fast Fourier Transform (FFT). This analysis of computational complexity is fundamental to building scalable data assimilation systems and highlights the profound link between statistical modeling and efficient scientific computing.", "problem": "Consider a one-dimensional periodic discretization with grid size $N$ and the discrete Fourier transform (DFT) matrix $F \\in \\mathbb{C}^{N \\times N}$ taken to be unitary, so that $F^{-1} = F^{*}$. In incremental four-dimensional variational data assimilation (incremental $4$D-Var), a common control variable transform uses a symmetric, positive-definite background covariance matrix $B \\in \\mathbb{R}^{N \\times N}$ that is circulant. The circulant structure implies that $B$ is diagonalized by the DFT: $B = F^{*} \\Lambda F$, where $\\Lambda = \\operatorname{diag}(\\lambda_{0},\\ldots,\\lambda_{N-1})$ with $\\lambda_{k} \\ge 0$ for all $k$. A control variable transform factor $L$ satisfying $B = L L^{T}$ can then be constructed spectrally as\n$$\nL \\;=\\; F^{*} \\, \\operatorname{diag}(d_{0},\\ldots,d_{N-1}) \\, F, \\quad d_{k} \\;=\\; \\sqrt{\\lambda_{k}} \\in \\mathbb{R}_{\\ge 0}.\n$$\nYou are asked to design an algorithm to apply $L$ and $L^{T}$ to arbitrary vectors $x \\in \\mathbb{R}^{N}$ using Fast Fourier Transforms (FFTs), and to derive the computational complexity and memory requirements as functions of $N$.\n\nYour derivation must begin from the following fundamental bases:\n- The convolution theorem and the diagonalizability of circulant matrices by the DFT: application of a circulant operator is equivalent to multiplication by its Fourier symbol in the spectral domain.\n- The definition of the adjoint with respect to the Euclidean inner product and the unitarity of $F$.\n- A standard cost model for radix-$2$ Cooleyâ€“Tukey FFTs:\n  - A length-$N$ complex-to-complex FFT costs $5 N \\log_2 N$ floating-point operations (flops) to leading order.\n  - A length-$N$ inverse complex-to-complex FFT also costs $5 N \\log_2 N$ flops to leading order.\n  - A pointwise multiplication of a complex number by a real scalar costs $2$ flops to leading order.\n\nDesign an explicit algorithm that applies $L$ and $L^{T}$ using FFTs without overwriting input or output arrays, assuming:\n- You precompute and store the spectral amplitudes $d_{k}$ for $k=0,\\ldots,N-1$ as real numbers.\n- You use a separate complex work buffer of length $N$ (i.e., $N$ complex numbers) to hold Fourier coefficients.\n- The input and output arrays are real-valued length-$N$ arrays stored separately.\n\nFrom first principles, derive:\n- The leading-order total flop count to apply both $L$ and $L^{T}$ once each to arbitrary real inputs under the cost model above.\n- The peak auxiliary memory required, measured in the number of stored real floating-point numbers, counting the precomputed $d_{k}$ and the complex work buffer, but excluding the storage for the input and output arrays.\n\nProvide the final answer as a single analytic expression collecting both quantities as a two-entry row vector, with the first entry equal to the flop count function of $N$ and the second entry equal to the auxiliary memory function of $N$. No numerical substitution or rounding is required. Express logarithms explicitly as $\\log_2(\\cdot)$ and count memory strictly in units of reals, not bytes or words.", "solution": "The problem requires the design of an algorithm to apply a control variable transform $L$ and its transpose $L^T$, and to determine the computational complexity and memory requirements of this application. The problem is valid as it is scientifically grounded in the principles of linear algebra and data assimilation, is well-posed, objective, and internally consistent.\n\nFirst, we analyze the structure of the transform matrix $L$ and its transpose $L^T$. The matrix $L$ is defined as\n$$\nL = F^{*} D F\n$$\nwhere $F$ is the unitary Discrete Fourier Transform (DFT) matrix, and $D = \\operatorname{diag}(d_{0}, d_{1}, \\ldots, d_{N-1})$ is a diagonal matrix of non-negative real numbers $d_k = \\sqrt{\\lambda_k}$. The values $\\lambda_k$ are the eigenvalues of the background covariance matrix $B$. The problem states that $B$ is a real, symmetric, positive-definite, and circulant matrix.\n\nLet us determine the transpose of $L$:\n$$\nL^T = (F^{*} D F)^T = F^T D^T (F^*)^T\n$$\nSince $D$ is a real diagonal matrix, $D^T = D$. The unitary DFT matrix $F$ with entries $F_{jk} = \\frac{1}{\\sqrt{N}} \\exp(-2\\pi i jk / N)$ is a symmetric matrix, i.e., $F^T = F$. The transpose of the conjugate transpose of $F$ is $(F^*)^T = \\overline{F}$, where $\\overline{F}$ is the element-wise complex conjugate of $F$. Thus,\n$$\nL^T = F D \\overline{F}\n$$\nThe matrix $B$ is a real symmetric circulant matrix. A real circulant matrix is symmetric if and only if its generating first row $(b_0, b_1, \\ldots, b_{N-1})$ satisfies $b_j = b_{N-j}$ for $j=1, \\ldots, N-1$. The eigenvalues of such a matrix are real and satisfy the symmetry property $\\lambda_k = \\lambda_{N-k}$ (with index taken modulo $N$). Consequently, the spectral values $d_k = \\sqrt{\\lambda_k}$ also satisfy $d_k = d_{N-k}$. This symmetry property of the diagonal matrix $D$ implies that $L$ is itself a symmetric matrix. To show this, we check if $L = L^T$:\n$$\nL = L^T \\iff F^* D F = F D \\overline{F}\n$$\nThe DFT matrix satisfies $F^* = \\overline{F}$. Substituting this into the right side gives $F D F^*$. So the condition for symmetry is $F^* D F = F D F^*$. This is equivalent to commutativity, $D (F^* F) = (F^* F) D$? No. Let's pre-multiply by $F$ and post-multiply by $F^*$:\n$$\nF(F^* D F)F^* = F(F D F^*)F^* \\implies (FF^*) D (FF^*) = (FF) D (F^*F^*)\n$$\nSince $F$ is unitary, $FF^* = I$. So we have $I D I = F^2 D (F^*)^2$. Let $P = F^2$. The matrix $P$ is the parity flip permutation matrix, $(Px)_j = x_{-j \\pmod N}$. The condition becomes $D = P D P^{-1}$, which means $D$ must commute with $P$. This requires $d_j = d_{-j \\pmod N} = d_{N-j}$ for all $j$. As established, this condition holds for a symmetric circulant matrix. Therefore, $L$ is a symmetric matrix, $L=L^T$.\nThe condition $B=LL^T$ is also satisfied: $LL^T=L^2 = (F^* D F)(F^* D F) = F^* D (FF^*) D F = F^* D I D F = F^* D^2 F = F^* \\Lambda F = B$. The formulation is consistent.\n\nSince $L=L^T$, the algorithm to apply $L$ is identical to the algorithm to apply $L^T$. We design an algorithm to compute $y = Lx$ for an arbitrary real input vector $x \\in \\mathbb{R}^N$. The operation $y = (F^* D F)x$ can be decomposed into three steps:\n1.  Apply $F$ to $x$: $\\hat{x} = Fx$. This is a forward DFT.\n2.  Apply $D$ to $\\hat{x}$: $\\hat{y} = D\\hat{x}$. This is a pointwise multiplication, $\\hat{y}_k = d_k \\hat{x}_k$.\n3.  Apply $F^*$ to $\\hat{y}$: $y = F^*\\hat{y}$. This is an inverse DFT.\n\nThe algorithm, using the specified work buffers, is as follows:\nInput: Real vector $x \\in \\mathbb{R}^N$.\nPrecomputed storage: Real vector $d = (d_0, \\ldots, d_{N-1})$.\nAuxiliary storage: Complex vector $\\texttt{work} \\in \\mathbb{C}^N$.\nOutput: Real vector $y \\in \\mathbb{R}^N$.\n\nAlgorithm to compute $y = Lx$:\n1.  Initialize the work buffer: For $k=0, \\ldots, N-1$, set $\\texttt{work}_k \\leftarrow x_k + 0i$.\n2.  Forward Fourier Transform: Apply a complex-to-complex FFT to $\\texttt{work}$. Let the result be $\\hat{x}_{\\texttt{work}}$. $\\texttt{work} \\leftarrow \\text{FFT}(\\texttt{work})$.\n3.  Spectral Multiplication: For $k=0, \\ldots, N-1$, multiply the elements of the work buffer by the corresponding spectral amplitudes: $\\texttt{work}_k \\leftarrow d_k \\times \\texttt{work}_k$.\n4.  Inverse Fourier Transform: Apply a complex-to-complex inverse FFT to $\\texttt{work}$. Let the result be $y_{\\texttt{work}}$. $\\texttt{work} \\leftarrow \\text{IFFT}(\\texttt{work})$.\n5.  Finalize output: For $k=0, \\ldots, N-1$, copy the real part of the work buffer to the output vector: $y_k \\leftarrow \\operatorname{Re}(\\texttt{work}_k)$.\n\nNow, we derive the computational complexity and memory requirements based on this algorithm and the provided cost model.\n\n**Computational Complexity (Flop Count)**\nWe calculate the number of floating-point operations (flops) for a single application of $L$.\n- Step 1 (Initialization): This is a data copy operation and does not involve floating-point arithmetic. Cost = $0$ flops.\n- Step 2 (FFT): A length-$N$ complex-to-complex FFT costs $5 N \\log_2 N$ flops to leading order.\n- Step 3 (Spectral Multiplication): This step involves $N$ multiplications of a complex number by a real scalar. Each such multiplication costs $2$ flops. The total cost for this step is $2N$ flops.\n- Step 4 (IFFT): A length-$N$ inverse complex-to-complex FFT costs $5 N \\log_2 N$ flops to leading order.\n- Step 5 (Finalization): This is a data copy, costing $0$ flops.\n\nThe total flop count for one application of $L$ is the sum of the costs of these steps:\n$$\nC_L = (5 N \\log_2 N) + (2N) + (5 N \\log_2 N) = 10 N \\log_2 N + 2N\n$$\nThe problem asks for the total flop count to apply both $L$ and $L^T$ once each. As $L=L^T$, the cost for applying $L^T$ is the same, $C_{L^T} = C_L$. The total cost is:\n$$\nC_{\\text{total}} = C_L + C_{L^T} = 2 \\times (10 N \\log_2 N + 2N) = 20 N \\log_2 N + 4N\n$$\nThis expression represents the leading-order terms of the total flop count, as derived from the provided cost model.\n\n**Auxiliary Memory Requirements**\nThe problem asks for the peak auxiliary memory, excluding the storage for input and output arrays. The auxiliary memory consists of the storage for the precomputed spectral amplitudes $d_k$ and the complex work buffer.\n1.  **Storage for $d_k$**: The vector $d = (d_0, \\ldots, d_{N-1})$ contains $N$ real numbers. The memory required is $N$ real floating-point numbers.\n2.  **Complex work buffer**: The buffer is a complex array of length $N$. Each complex number requires storage for one real part and one imaginary part. Therefore, the buffer requires storage for $N \\times 2 = 2N$ real floating-point numbers.\n\nThe total auxiliary memory required is the sum of these two components:\n$$\nM_{\\text{aux}} = N + 2N = 3N\n$$\nThe memory is counted in units of real floating-point numbers.\n\nCombining the results for the total flop count and auxiliary memory, we can present the final answer as a two-entry row vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} 20 N \\log_{2}(N) + 4N  3N \\end{pmatrix}}\n$$", "id": "3372128"}]}