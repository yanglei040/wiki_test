{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we will ground the abstract concept of generalized Tikhonov regularization in a concrete, low-dimensional calculation. This first exercise [@problem_id:1031979] asks you to find the minimizer of the functional $J(\\mathbf{u}) = \\| A \\mathbf{u} - \\mathbf{b} \\|^2 + \\lambda \\| L \\mathbf{u} \\|^2$, where $L$ is a simple first-derivative operator. By working through this problem, you will build fundamental intuition for how the regularization term balances data fidelity with a penalty on the solution's smoothness, and see directly how the parameter $\\lambda$ controls this trade-off.", "problem": "Consider the overdetermined linear system $A \\mathbf{u} = \\mathbf{b}$, where $A$ is the $3 \\times 2$ matrix  \n$$  \nA = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix},  \n$$  \nand $\\mathbf{b}$ is the vector $\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$. The generalized Tikhonov regularization functional incorporates a derivative operator $L$ and is defined as  \n$$  \nJ(\\mathbf{u}) = \\| A \\mathbf{u} - \\mathbf{b} \\|^2 + \\lambda \\| L \\mathbf{u} \\|^2,  \n$$  \nwhere $\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} \\in \\mathbb{R}^2$, $\\lambda > 0$ is a regularization parameter, and $L$ is the first-order derivative operator given by the $1 \\times 2$ matrix $L = \\begin{bmatrix} -1 & 1 \\end{bmatrix}$. Compute the minimum value of $J(\\mathbf{u})$ over all $\\mathbf{u} \\in \\mathbb{R}^2$.", "solution": "1. Write the functional  \n$$\nJ(u_1,u_2)=(u_1-1)^2+u_2^2+(u_1+u_2)^2+\\lambda(u_2-u_1)^2.\n$$  \n2. Expand and collect terms. Let $a=2+\\lambda$, $b=2-2\\lambda$. Then  \n$$\nJ=(2+\\lambda)(u_1^2+u_2^2)+(2-2\\lambda)u_1u_2-2u_1+1\n=a\\,(u_1^2+u_2^2)+b\\,u_1u_2-2u_1+1.\n$$  \n3. Stationarity conditions  \n$$\n\\frac{\\partial J}{\\partial u_1}=2a\\,u_1+b\\,u_2-2=0,\\qquad\n\\frac{\\partial J}{\\partial u_2}=2a\\,u_2+b\\,u_1=0.\n$$  \n4. Solve for $(u_1,u_2)$. The solution is  \n$$\nu_1=\\frac{2+\\lambda}{3(1+2\\lambda)},\\quad\nu_2=-\\frac{1-\\lambda}{3(1+2\\lambda)}.\n$$  \n5. Substitute back into $J$: after algebra one finds  \n$$\nJ_{\\min}\n=\\frac{1+7\\lambda+10\\lambda^2}{3(1+2\\lambda)^2}\n=\\frac{(1+5\\lambda)(1+2\\lambda)}{3(1+2\\lambda)^2}\n=\\frac{5\\lambda+1}{3(2\\lambda+1)}.\n$$  \nHence the minimum of $J$ is $(5\\lambda+1)/[3(2\\lambda+1)]\\,$.", "answer": "$$\\boxed{\\frac{5\\lambda+1}{3(2\\lambda+1)}}$$", "id": "1031979"}, {"introduction": "Having computed a regularized solution, we now turn to the crucial theoretical questions of existence and uniqueness. This practice [@problem_id:3283916] investigates the structure of the solution set when the regularization operator, denoted here as $\\Gamma$, may be singular (rank-deficient). By analyzing the problem through the lens of linear algebra, you will discover the fundamental condition for uniqueness: the Tikhonov minimizer is unique if and only if the null spaces of the forward operator $A$ and the regularization operator $\\Gamma$ have a trivial intersection, i.e., $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) = \\{0\\}$.", "problem": "Consider the Tikhonov regularization problem for a possibly ill-conditioned linear model: given $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $\\Gamma \\in \\mathbb{R}^{p \\times n}$, and a regularization parameter $\\alpha > 0$, define the objective\n$$\nJ_{\\alpha}(x) \\;=\\; \\lVert A x - b \\rVert_{2}^{2} \\;+\\; \\alpha \\,\\lVert \\Gamma x \\rVert_{2}^{2}, \\quad x \\in \\mathbb{R}^{n}.\n$$\nAssume that $\\Gamma$ may be singular (rank-deficient). Let $\\mathcal{N}(M) = \\{x \\in \\mathbb{R}^{n} : M x = 0\\}$ denote the null space of a matrix $M$, and let $\\mathcal{R}(M)$ denote its range (column space). Analyze existence, uniqueness, and the structure of the minimizer(s) when $\\Gamma$ is singular, using only fundamental linear algebra facts about least squares and properties of null spaces and ranges.\n\nWhich of the following statements are correct?\n\nA. For every $\\alpha > 0$ and every $A$, $b$, and singular $\\Gamma$, the minimization problem for $J_{\\alpha}$ has at least one solution $x_{\\alpha}$.\n\nB. The minimizer is unique if and only if $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) = \\{0\\}$.\n\nC. If $\\Gamma$ is singular, then the quadratic form associated with $J_{\\alpha}$ is singular for all $\\alpha > 0$, hence the minimizer is never unique.\n\nD. If $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) \\neq \\{0\\}$, then the set of minimizers is an affine subspace of the form $x_{\\alpha}^{\\star} + \\big(\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma)\\big)$ for some particular minimizer $x_{\\alpha}^{\\star}$.\n\nE. If $A$ has full column rank but $\\Gamma$ is singular, then there exists some $b$ for which the Tikhonov minimizer does not exist.", "solution": "The problem statement is first validated for correctness and clarity.\n\n### Step 1: Extract Givens\n- Matrices: $A \\in \\mathbb{R}^{m \\times n}$, $\\Gamma \\in \\mathbb{R}^{p \\times n}$.\n- Vectors: $b \\in \\mathbb{R}^{m}$, $x \\in \\mathbb{R}^{n}$.\n- Scalar: Regularization parameter $\\alpha > 0$.\n- Objective Function: $J_{\\alpha}(x) = \\lVert A x - b \\rVert_{2}^{2} + \\alpha \\lVert \\Gamma x \\rVert_{2}^{2}$.\n- Condition: The matrix $\\Gamma$ may be singular.\n- Notation: Null space $\\mathcal{N}(M) = \\{x \\in \\mathbb{R}^{n} : M x = 0\\}$, Range $\\mathcal{R}(M)$.\n- Task: Analyze existence, uniqueness, and structure of the minimizer(s) of $J_{\\alpha}(x)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, well-posed, and objective. It presents the standard formulation for generalized Tikhonov regularization, a fundamental topic in numerical methods and scientific computing. The objective function is clearly defined, and the task is a standard mathematical analysis of an optimization problem. The language is precise and unambiguous. All necessary components are provided, and there are no internal contradictions.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will proceed with a full derivation and analysis of each option.\n\n### Derivation\nThe objective function $J_{\\alpha}(x)$ can be rewritten as the squared Euclidean norm of a stacked system.\n$$\nJ_{\\alpha}(x) = \\lVert A x - b \\rVert_{2}^{2} + \\alpha \\lVert \\Gamma x \\rVert_{2}^{2} = \\lVert A x - b \\rVert_{2}^{2} + \\lVert \\sqrt{\\alpha} \\Gamma x - 0 \\rVert_{2}^{2}\n$$\nThis is equivalent to minimizing the least squares problem $\\lVert A_{\\alpha} x - b_{\\alpha} \\rVert_{2}^{2}$, where $A_{\\alpha}$ and $b_{\\alpha}$ are the augmented matrix and vector:\n$$\nA_{\\alpha} = \\begin{pmatrix} A \\\\ \\sqrt{\\alpha} \\Gamma \\end{pmatrix} \\in \\mathbb{R}^{(m+p) \\times n}, \\quad b_{\\alpha} = \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^{(m+p)}\n$$\nThe minimizers of this least squares problem are the solutions to the normal equations:\n$$\nA_{\\alpha}^{T} A_{\\alpha} x = A_{\\alpha}^{T} b_{\\alpha}\n$$\nLet us compute the components of the normal equations.\nThe matrix is:\n$$\nA_{\\alpha}^{T} A_{\\alpha} = \\begin{pmatrix} A^T & \\sqrt{\\alpha} \\Gamma^T \\end{pmatrix} \\begin{pmatrix} A \\\\ \\sqrt{\\alpha} \\Gamma \\end{pmatrix} = A^T A + \\alpha \\Gamma^T \\Gamma\n$$\nThe right-hand side vector is:\n$$\nA_{\\alpha}^{T} b_{\\alpha} = \\begin{pmatrix} A^T & \\sqrt{\\alpha} \\Gamma^T \\end{pmatrix} \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} = A^T b\n$$\nThus, the normal equations are:\n$$\n(A^T A + \\alpha \\Gamma^T \\Gamma) x = A^T b\n$$\nLet's denote the Hessian matrix (of the quadratic form) as $H_{\\alpha} = A^T A + \\alpha \\Gamma^T \\Gamma$. The objective function is $J_{\\alpha}(x) = x^T H_{\\alpha} x - 2x^T A^T b + b^T b$. Since $A^T A$ and $\\Gamma^T \\Gamma$ are positive semidefinite, and $\\alpha > 0$, $H_{\\alpha}$ is also positive semidefinite. This confirms that $J_{\\alpha}(x)$ is a convex function.\n\n### Option-by-Option Analysis\n\n**A. For every $\\alpha > 0$ and every $A$, $b$, and singular $\\Gamma$, the minimization problem for $J_{\\alpha}$ has at least one solution $x_{\\alpha}$.**\n\nThe problem of minimizing $J_{\\alpha}(x)$ is a linear least squares problem. A fundamental theorem of linear algebra states that a solution to any linear least squares problem $\\min_{x \\in \\mathbb{R}^n} \\lVert Kx - d \\rVert_2^2$ always exists. This is because the set of solutions is equivalent to the set of vectors $x$ satisfying $Kx = \\text{proj}_{\\mathcal{R}(K)}(d)$, which is non-empty because the projection of $d$ onto the column space of $K$ always exists.\nAlternatively, the solution set is identical to the solution set of the normal equations $K^T K x = K^T d$. A linear system $Mx=c$ has a solution if and only if $c \\in \\mathcal{R}(M)$. Here, $M = K^T K$ and $c = K^T d$. Since $\\mathcal{R}(K^T K) = \\mathcal{R}(K^T)$ for any matrix $K$, and $c = K^T d$ is by definition in $\\mathcal{R}(K^T)$, the system is always consistent and has at least one solution.\nThe existence is unconditional for any $A, b, \\Gamma$ and any $\\alpha > 0$. The fact that $\\Gamma$ is singular has no bearing on the existence of a minimizer.\n\nVerdict: **Correct**.\n\n**B. The minimizer is unique if and only if $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) = \\{0\\}$.**\n\nThe minimizer of the least squares problem $\\min_{x} \\lVert A_{\\alpha} x - b_{\\alpha} \\rVert_{2}^{2}$ is unique if and only if the matrix $A_{\\alpha}$ has full column rank, which is equivalent to its null space being trivial, $\\mathcal{N}(A_{\\alpha}) = \\{0\\}$.\nLet's characterize this null space. A vector $x \\in \\mathbb{R}^n$ is in $\\mathcal{N}(A_{\\alpha})$ if and only if $A_{\\alpha} x = 0$.\n$$\nA_{\\alpha} x = \\begin{pmatrix} A x \\\\ \\sqrt{\\alpha} \\Gamma x \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis is true if and only if both $A x = 0$ and $\\sqrt{\\alpha} \\Gamma x = 0$. Since $\\alpha > 0$, the second condition is equivalent to $\\Gamma x = 0$.\nSo, $x \\in \\mathcal{N}(A_{\\alpha})$ if and only if $x \\in \\mathcal{N}(A)$ and $x \\in \\mathcal{N}(\\Gamma)$. This means:\n$$\n\\mathcal{N}(A_{\\alpha}) = \\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma)\n$$\nTherefore, the minimizer is unique if and only if $\\mathcal{N}(A_{\\alpha}) = \\{0\\}$, which is equivalent to the condition $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) = \\{0\\}$.\n\nVerdict: **Correct**.\n\n**C. If $\\Gamma$ is singular, then the quadratic form associated with $J_{\\alpha}$ is singular for all $\\alpha > 0$, hence the minimizer is never unique.**\n\nThe quadratic form is associated with the Hessian matrix $H_{\\alpha} = A^T A + \\alpha \\Gamma^T \\Gamma$. This matrix is singular if and only if its null space is non-trivial. As shown in the analysis for B, $\\mathcal{N}(H_{\\alpha}) = \\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma)$.\nThe statement claims that if $\\Gamma$ is singular (i.e., $\\mathcal{N}(\\Gamma) \\neq \\{0\\}$), then $H_{\\alpha}$ is always singular (i.e., $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) \\neq \\{0\\}$). This is not true. It is possible for $\\mathcal{N}(\\Gamma)$ to be a non-trivial subspace while its intersection with $\\mathcal{N}(A)$ is trivial.\nConsider the following counterexample. Let $n=2$, $A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$, and $\\Gamma = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}$.\nThen $\\mathcal{N}(A) = \\text{span}\\left\\{ \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right\\}$ and $\\mathcal{N}(\\Gamma) = \\text{span}\\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\}$.\nHere, $\\Gamma$ is singular. However, $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) = \\{0\\}$.\nAccording to the result from B, the minimizer is unique in this case. The matrix $H_{\\alpha}$ is $A^T A + \\alpha \\Gamma^T \\Gamma = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\alpha \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\alpha \\end{pmatrix}$. Since $\\alpha > 0$, $H_\\alpha$ is invertible.\nThe statement is false.\n\nVerdict: **Incorrect**.\n\n**D. If $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) \\neq \\{0\\}$, then the set of minimizers is an affine subspace of the form $x_{\\alpha}^{\\star} + \\big(\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma)\\big)$ for some particular minimizer $x_{\\alpha}^{\\star}$.**\n\nThe set of minimizers is the solution set of the normal equations $H_{\\alpha} x = A^T b$, where $H_{\\alpha} = A^T A + \\alpha \\Gamma^T \\Gamma$. From linear algebra, the general solution to a consistent linear system $Mx=c$ is given by $x = x_p + z$, where $x_p$ is any particular solution and $z$ is any vector in the null space of $M$, $\\mathcal{N}(M)$. The set of solutions is the affine subspace $x_p + \\mathcal{N}(M)$.\nIn our case, we have established that the problem always has a solution (so the system is consistent), and that the null space of the relevant matrix is $\\mathcal{N}(H_{\\alpha}) = \\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma)$.\nTherefore, if $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) \\neq \\{0\\}$, the solution is not unique, and the set of all minimizers is precisely the affine subspace formed by adding any particular minimizer $x_{\\alpha}^{\\star}$ to the null space $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma)$. The statement accurately describes this structure.\n\nVerdict: **Correct**.\n\n**E. If $A$ has full column rank but $\\Gamma$ is singular, then there exists some $b$ for which the Tikhonov minimizer does not exist.**\n\nThis statement is fundamentally incorrect. As established in the analysis of option A, a minimizer *always* exists for any choice of $A$, $b$, $\\Gamma$, and for any $\\alpha > 0$. The existence is unconditional.\nFurthermore, the premise of this statement leads to the conclusion of a unique solution, which is the opposite of non-existence. If $A$ has full column rank, then $\\mathcal{N}(A) = \\{0\\}$. This implies $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) = \\{0\\} \\cap \\mathcal{N}(\\Gamma) = \\{0\\}$. By the result from B, the minimizer is always unique under this condition, regardless of whether $\\Gamma$ is singular. Uniqueness implies existence. The statement is therefore false.\n\nVerdict: **Incorrect**.\n\nFinal summary of correct options: A, B, D.", "answer": "$$\\boxed{ABD}$$", "id": "3283916"}, {"introduction": "Our final practice addresses one of the most critical practical challenges in regularization: how to choose an optimal value for the hyperparameter $\\lambda$. This advanced coding exercise [@problem_id:3427389] guides you through implementing a bilevel optimization scheme, where $\\lambda$ is tuned by minimizing a validation error on a held-out dataset. You will derive and implement the gradient of the validation risk with respect to $\\lambda$ by applying the chain rule and implicit differentiation to the Tikhonov normal equations, a powerful technique central to hyperparameter optimization in both inverse problems and machine learning.", "problem": "You are tasked with implementing a bilevel optimization procedure for selecting the hyperparameter $\\lambda$ in generalized Tikhonov regularization by minimizing a validation risk on held-out data. The inner problem is a generalized Tikhonov estimator defined by minimizing the objective $\\|A x - y\\|^2 + \\lambda^2 \\|L x\\|^2$ with respect to $x$, where $A \\in \\mathbb{R}^{m \\times n}$ is the training design matrix, $y \\in \\mathbb{R}^m$ is the training observation vector, and $L \\in \\mathbb{R}^{p \\times n}$ is a linear regularization operator. The outer problem selects $\\lambda \\ge 0$ to minimize the validation risk $R_{\\mathrm{val}}(x_\\lambda) = \\|A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}}\\|^2$, where $A_{\\mathrm{val}} \\in \\mathbb{R}^{m_{\\mathrm{val}} \\times n}$ and $y_{\\mathrm{val}} \\in \\mathbb{R}^{m_{\\mathrm{val}}}$ represent held-out validation data, and $x_\\lambda$ denotes the unique minimizer of the inner problem for a given $\\lambda$. Your implementation must be based on first principles and must derive and use the gradient $\\partial x_\\lambda / \\partial \\lambda$ to perform hyperparameter tuning on $\\lambda$ via projected gradient descent onto $\\mathbb{R}_{\\ge 0}$.\n\nStarting point: Use the fundamental least-squares normal equations that characterize the minimizer of a convex quadratic objective. For the inner problem, the minimizer $x_\\lambda$ satisfies the first-order optimality (normal) equations $(A^\\top A + \\lambda^2 L^\\top L) x_\\lambda = A^\\top y$, assuming $A^\\top A + \\lambda^2 L^\\top L$ is invertible. The outer problem objective is the validation risk $R_{\\mathrm{val}}(x_\\lambda) = \\|A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}}\\|^2$. From these bases, derive $\\partial x_\\lambda / \\partial \\lambda$ and implement the gradient of the validation risk with respect to $\\lambda$, $\\mathrm{d}R_{\\mathrm{val}}/\\mathrm{d}\\lambda$, without using pre-derived shortcut formulas.\n\nYour program should:\n- Construct synthetic training and validation datasets $(A, y)$ and $(A_{\\mathrm{val}}, y_{\\mathrm{val}})$ for each test case using fixed random seeds and a known ground-truth vector $x^\\star$.\n- Implement the inner solver for $x_\\lambda$ using the normal equations.\n- Derive and implement the sensitivity $\\partial x_\\lambda / \\partial \\lambda$ via implicit differentiation of the normal equations.\n- Compute the gradient of the validation risk and run projected gradient descent on $\\lambda$ (projection onto $\\mathbb{R}_{\\ge 0}$) for a fixed number of iterations with a specified step size.\n- For the gradient verification test, compare the analytic gradient against a central finite-difference approximation and report the relative discrepancy.\n\nUnits and representation:\n- There are no physical units in this problem.\n- All angles, if any, are to be interpreted in radians; however, no trigonometric angles of measurement are required in the final outputs.\n\nTest Suite Specification:\nImplement the following four test cases. In all cases, generate $A$ and $A_{\\mathrm{val}}$ with independent entries drawn from the standard normal distribution $\\mathcal{N}(0,1)$ using the specified random seed, and let the ground-truth vector $x^\\star \\in \\mathbb{R}^n$ be defined component-wise by $x^\\star_i = \\sin\\left(\\frac{\\pi i}{n}\\right) + \\frac{1}{2} \\cos\\left(\\frac{2 \\pi i}{n}\\right)$ for $i = 1, \\dots, n$. For a given noise standard deviation $\\sigma$, generate $y = A x^\\star + \\varepsilon_{\\mathrm{train}}$ and $y_{\\mathrm{val}} = A_{\\mathrm{val}} x^\\star + \\varepsilon_{\\mathrm{val}}$ with independent $\\varepsilon_{\\mathrm{train}} \\sim \\mathcal{N}(0, \\sigma^2 I)$ and $\\varepsilon_{\\mathrm{val}} \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n\n- Test Case $1$ (Happy path; generalized $L$ as first-difference):\n  - Dimensions: $m_{\\mathrm{train}} = 40$, $m_{\\mathrm{val}} = 45$, $n = 20$.\n  - Random seed: $1$.\n  - Regularization operator $L \\in \\mathbb{R}^{(n-1) \\times n}$ is the first-difference operator: $L_{i,i} = -1$, $L_{i,i+1} = 1$ for $i = 1, \\dots, n-1$, with all other entries $0$.\n  - Noise standard deviation: $\\sigma = 10^{-2}$.\n  - Initialization and optimization: $\\lambda_0 = 10^{-1}$, step size $\\alpha = 10^{-2}$, number of iterations $200$.\n\n  Output the final tuned $\\lambda$ as a float.\n\n- Test Case $2$ (Boundary condition; $\\lambda$ initialized at zero with $L = I$):\n  - Dimensions: $m_{\\mathrm{train}} = 50$, $m_{\\mathrm{val}} = 55$, $n = 25$.\n  - Random seed: $3$.\n  - Regularization operator $L \\in \\mathbb{R}^{n \\times n}$ is the identity matrix.\n  - Noise standard deviation: $\\sigma = 5 \\cdot 10^{-3}$.\n  - Initialization and optimization: $\\lambda_0 = 0$, step size $\\alpha = 10^{-2}$, number of iterations $50$.\n\n  Output the final tuned $\\lambda$ as a float.\n\n- Test Case $3$ (Edge case; singular $L$ as second-difference):\n  - Dimensions: $m_{\\mathrm{train}} = 60$, $m_{\\mathrm{val}} = 65$, $n = 30$.\n  - Random seed: $4$.\n  - Regularization operator $L \\in \\mathbb{R}^{(n-2) \\times n}$ is the second-difference operator: $L_{i,i} = 1$, $L_{i,i+1} = -2$, $L_{i,i+2} = 1$ for $i = 1, \\dots, n-2$, with all other entries $0$.\n  - Noise standard deviation: $\\sigma = 10^{-2}$.\n  - Initialization and optimization: $\\lambda_0 = 5 \\cdot 10^{-2}$, step size $\\alpha = 2 \\cdot 10^{-2}$, number of iterations $300$.\n\n  Output the final tuned $\\lambda$ as a float.\n\n- Test Case $4$ (Gradient verification via finite differences):\n  - Dimensions: $m_{\\mathrm{train}} = 6$, $m_{\\mathrm{val}} = 7$, $n = 5$.\n  - Random seed: $2$.\n  - Regularization operator $L \\in \\mathbb{R}^{n \\times n}$ is the identity matrix.\n  - Noise standard deviation: $\\sigma = 10^{-3}$.\n  - Evaluate at a fixed $\\lambda = 0.15$.\n  - Compute the analytic gradient $\\mathrm{d}R_{\\mathrm{val}}/\\mathrm{d}\\lambda$ at $\\lambda$ and compare against the central finite-difference approximation with step $h = 10^{-6}$:\n    $$\\frac{R_{\\mathrm{val}}(\\lambda + h) - R_{\\mathrm{val}}(\\lambda - h)}{2 h}.$$\n  - Output the relative discrepancy defined as\n    $$\\frac{\\left|\\left(\\frac{R_{\\mathrm{val}}(\\lambda + h) - R_{\\mathrm{val}}(\\lambda - h)}{2 h}\\right) - \\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda}\\right|}{\\max\\left(10^{-12}, \\left|\\frac{R_{\\mathrm{val}}(\\lambda + h) - R_{\\mathrm{val}}(\\lambda - h)}{2 h}\\right|\\right)}$$\n    as a float.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Test Cases $1$ through $4$. For example, the format must be like $[r_1,r_2,r_3,r_4]$, where $r_1$, $r_2$, and $r_3$ are floats for the tuned $\\lambda$ values of Test Cases $1$–$3$, and $r_4$ is the float for the relative discrepancy in Test Case $4$.", "solution": "The problem statement is assessed to be valid. It presents a well-posed and scientifically sound task in the field of numerical optimization and inverse problems. All necessary parameters, definitions, and conditions are provided, and the problem is free of contradictions, ambiguities, or factual errors. The task is to implement a bilevel optimization scheme for hyperparameter selection in generalized Tikhonov regularization, which is a standard and important problem.\n\nThe solution proceeds in two stages. First, we derive the analytical gradient of the validation risk with respect to the regularization hyperparameter $\\lambda$. Second, we implement this gradient within a projected gradient descent algorithm to solve the specified test cases.\n\n**1. Derivation of the Gradient for Bilevel Optimization**\n\nThe problem is structured as a bilevel optimization problem. The inner problem finds the regularized solution $x_\\lambda$ for a given hyperparameter $\\lambda$, and the outer problem optimizes $\\lambda$ by minimizing a validation metric.\n\n**Inner Problem:** The generalized Tikhonov regularized solution, $x_\\lambda$, is the unique minimizer of the objective function:\n$$ J(x; \\lambda) = \\|A x - y\\|^2_2 + \\lambda^2 \\|L x\\|^2_2 $$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is the training design matrix, $y \\in \\mathbb{R}^m$ is the training data, $L \\in \\mathbb{R}^{p \\times n}$ is the regularization operator, and $\\lambda \\ge 0$ is the regularization hyperparameter. The objective $J(x; \\lambda)$ is a convex quadratic function of $x$. Its minimizer is found by setting its gradient with respect to $x$ to zero:\n$$ \\nabla_x J(x; \\lambda) = 2 A^\\top (A x - y) + 2 \\lambda^2 L^\\top L x = 0 $$\nRearranging these terms gives the normal equations for the regularized least-squares problem:\n$$ (A^\\top A + \\lambda^2 L^\\top L) x_\\lambda = A^\\top y $$\nWe denote the system matrix as $M_\\lambda = A^\\top A + \\lambda^2 L^\\top L$ and the right-hand side as $b = A^\\top y$. The solution $x_\\lambda$ is thus given by $x_\\lambda = M_\\lambda^{-1} b$, assuming $M_\\lambda$ is invertible. This is guaranteed if $\\lambda > 0$ and the null spaces of $A$ and $L$ intersect only at the origin, i.e., $\\ker(A) \\cap \\ker(L) = \\{0\\}$, a standard condition. For $\\lambda = 0$, invertibility requires $A^\\top A$ to be invertible, which is true with probability $1$ for the randomly generated matrices in this problem where $m \\ge n$.\n\n**Outer Problem:** The outer problem is to find the optimal $\\lambda$ that minimizes the validation risk, defined as the squared error on a held-out validation dataset $(A_{\\mathrm{val}}, y_{\\mathrm{val}})$:\n$$ R_{\\mathrm{val}}(\\lambda) = \\|A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}}\\|^2_2 $$\nTo optimize $R_{\\mathrm{val}}(\\lambda)$ using gradient descent, we must compute its derivative with respect to $\\lambda$, $\\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda}$.\n\n**Gradient Derivation:** We start by applying the chain rule to $R_{\\mathrm{val}}(\\lambda)$:\n$$ R_{\\mathrm{val}}(\\lambda) = (A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}})^\\top (A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}}) $$\n$$ \\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda} = 2 (A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}})^\\top \\frac{\\mathrm{d}}{\\mathrm{d}\\lambda}(A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}}) = 2 (A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}})^\\top A_{\\mathrm{val}} \\frac{\\mathrm{d}x_\\lambda}{\\mathrm{d}\\lambda} $$\nThe core of the derivation is to find the sensitivity vector $\\frac{\\mathrm{d}x_\\lambda}{\\mathrm{d}\\lambda}$. This is achieved by implicitly differentiating the normal equation $M_\\lambda x_\\lambda = b$ with respect to $\\lambda$:\n$$ \\frac{\\mathrm{d}}{\\mathrm{d}\\lambda} (M_\\lambda x_\\lambda) = \\frac{\\mathrm{d}b}{\\mathrm{d}\\lambda} $$\nThe right-hand side is zero, as $b=A^\\top y$ is independent of $\\lambda$. Applying the product rule to the left-hand side yields:\n$$ \\left( \\frac{\\mathrm{d}M_\\lambda}{\\mathrm{d}\\lambda} \\right) x_\\lambda + M_\\lambda \\left( \\frac{\\mathrm{d}x_\\lambda}{\\mathrm{d}\\lambda} \\right) = 0 $$\nThe derivative of the system matrix $M_\\lambda$ is:\n$$ \\frac{\\mathrm{d}M_\\lambda}{\\mathrm{d}\\lambda} = \\frac{\\mathrm{d}}{\\mathrm{d}\\lambda} (A^\\top A + \\lambda^2 L^\\top L) = 2\\lambda L^\\top L $$\nSubstituting this back, we get:\n$$ (2\\lambda L^\\top L) x_\\lambda + M_\\lambda \\left( \\frac{\\mathrm{d}x_\\lambda}{\\mathrm{d}\\lambda} \\right) = 0 $$\nSolving for the sensitivity $\\frac{\\mathrm{d}x_\\lambda}{\\mathrm{d}\\lambda}$:\n$$ \\frac{\\mathrm{d}x_\\lambda}{\\mathrm{d}\\lambda} = -M_\\lambda^{-1} (2\\lambda L^\\top L x_\\lambda) = -2\\lambda (A^\\top A + \\lambda^2 L^\\top L)^{-1} (L^\\top L) x_\\lambda $$\nFinally, we substitute this expression for $\\frac{\\mathrm{d}x_\\lambda}{\\mathrm{d}\\lambda}$ into the equation for $\\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda}$:\n$$ \\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda} = 2 (A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}})^\\top A_{\\mathrm{val}} \\left( -2\\lambda (A^\\top A + \\lambda^2 L^\\top L)^{-1} (L^\\top L) x_\\lambda \\right) $$\n$$ \\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda} = -4\\lambda (A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}})^\\top A_{\\mathrm{val}} (A^\\top A + \\lambda^2 L^\\top L)^{-1} (L^\\top L) x_\\lambda $$\nThis is the analytical gradient of the validation risk. Note that if $\\lambda = 0$, the gradient is $0$. This is expected, as the objective function is defined in terms of $\\lambda^2$, making $R_{\\mathrm{val}}(\\lambda)$ an even function of $\\lambda$ whose derivative at $0$ must be $0$.\n\n**2. Algorithm and Implementation**\n\nTo compute the gradient efficiently, we avoid explicit matrix inversion. The computation proceeds as follows for a given $\\lambda$:\n1.  Pre-compute constant matrices: $A^\\top A$, $A^\\top y$, and $L^\\top L$.\n2.  Form the matrix $M_\\lambda = A^\\top A + \\lambda^2 L^\\top L$.\n3.  Solve the linear system $M_\\lambda x_\\lambda = A^\\top y$ to find $x_\\lambda$.\n4.  Compute the vector $v = (L^\\top L) x_\\lambda$.\n5.  Solve the linear system $M_\\lambda w = v$ to find $w = M_\\lambda^{-1} v$. This reuses the matrix $M_\\lambda$ (or its factorization).\n6.  The gradient is then calculated as $\\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda} = -4\\lambda (A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}})^\\top A_{\\mathrm{val}} w$.\n\nThe hyperparameter $\\lambda$ is updated via projected gradient descent. For a given step size $\\alpha > 0$, the update rule is:\n$$ \\lambda_{k+1} = \\lambda_k - \\alpha \\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda}\\bigg|_{\\lambda=\\lambda_k} $$\n$$ \\lambda_{k+1} = \\max(0, \\lambda_{k+1}) $$\nThe projection step $\\max(0, \\cdot)$ ensures that $\\lambda$ remains non-negative. This iterative process is repeated for a fixed number of iterations as specified in each test case.\n\nFor the gradient verification in Test Case $4$, the analytical gradient is compared to a central finite-difference approximation:\n$$ \\left. \\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda} \\right|_{\\mathrm{FD}} = \\frac{R_{\\mathrm{val}}(\\lambda + h) - R_{\\mathrm{val}}(\\lambda - h)}{2h} $$\nfor a small step $h$. The relative discrepancy is then computed as specified. The data synthesis, operator definitions, and algorithm parameters for all test cases are implemented precisely as described in the problem statement.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"case_id\": 1, \"m_train\": 40, \"m_val\": 45, \"n\": 20, \"seed\": 1,\n            \"L_type\": \"first_diff\", \"sigma\": 1e-2, \"lambda_0\": 1e-1,\n            \"alpha\": 1e-2, \"n_iter\": 200, \"mode\": \"optimize\"\n        },\n        {\n            \"case_id\": 2, \"m_train\": 50, \"m_val\": 55, \"n\": 25, \"seed\": 3,\n            \"L_type\": \"identity\", \"sigma\": 5e-3, \"lambda_0\": 0.0,\n            \"alpha\": 1e-2, \"n_iter\": 50, \"mode\": \"optimize\"\n        },\n        {\n            \"case_id\": 3, \"m_train\": 60, \"m_val\": 65, \"n\": 30, \"seed\": 4,\n            \"L_type\": \"second_diff\", \"sigma\": 1e-2, \"lambda_0\": 5e-2,\n            \"alpha\": 2e-2, \"n_iter\": 300, \"mode\": \"optimize\"\n        },\n        {\n            \"case_id\": 4, \"m_train\": 6, \"m_val\": 7, \"n\": 5, \"seed\": 2,\n            \"L_type\": \"identity\", \"sigma\": 1e-3, \"lambda_val\": 0.15,\n            \"h\": 1e-6, \"mode\": \"verify_gradient\"\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        result = handle_case(params)\n        results.append(result)\n\n    print(f\"[{','.join(f'{r:.12g}' for r in results)}]\")\n\ndef handle_case(params):\n    \"\"\"Handles a single test case, either optimization or gradient verification.\"\"\"\n    # Generate data\n    rng = np.random.default_rng(params[\"seed\"])\n    n = params[\"n\"]\n    m_train = params[\"m_train\"]\n    m_val = params[\"m_val\"]\n    sigma = params[\"sigma\"]\n\n    A = rng.normal(size=(m_train, n))\n    A_val = rng.normal(size=(m_val, n))\n\n    i_vals = np.arange(1, n + 1)\n    x_star = np.sin(np.pi * i_vals / n) + 0.5 * np.cos(2 * np.pi * i_vals / n)\n\n    y_train = A @ x_star + sigma * rng.normal(size=m_train)\n    y_val = A_val @ x_star + sigma * rng.normal(size=m_val)\n\n    if params[\"L_type\"] == \"identity\":\n        L = np.identity(n)\n    elif params[\"L_type\"] == \"first_diff\":\n        L = np.zeros((n - 1, n))\n        np.fill_diagonal(L, -1)\n        np.fill_diagonal(L[:, 1:], 1)\n    elif params[\"L_type\"] == \"second_diff\":\n        L = np.zeros((n - 2, n))\n        np.fill_diagonal(L, 1)\n        np.fill_diagonal(L[:, 1:], -2)\n        np.fill_diagonal(L[:, 2:], 1)\n\n    # Pre-compute constant matrices\n    AtA = A.T @ A\n    LTL = L.T @ L\n    Aty = A.T @ y_train\n\n    data = (AtA, Aty, LTL, A_val, y_val)\n\n    if params[\"mode\"] == \"optimize\":\n        lam = params[\"lambda_0\"]\n        alpha = params[\"alpha\"]\n        n_iter = params[\"n_iter\"]\n        for _ in range(n_iter):\n            grad = compute_gradient(lam, data)\n            lam -= alpha * grad\n            lam = max(0.0, lam)\n        return lam\n    elif params[\"mode\"] == \"verify_gradient\":\n        lam = params[\"lambda_val\"]\n        h = params[\"h\"]\n        analytic_grad = compute_gradient(lam, data)\n\n        def R_val(lambda_val, data_tuple):\n            AtA_loc, Aty_loc, LTL_loc, A_val_loc, y_val_loc = data_tuple\n            x_lam = solve_inner(lambda_val, AtA_loc, Aty_loc, LTL_loc)\n            return np.linalg.norm(A_val_loc @ x_lam - y_val_loc)**2\n\n        R_plus = R_val(lam + h, data)\n        R_minus = R_val(lam - h, data)\n        fd_approx = (R_plus - R_minus) / (2 * h)\n\n        discrepancy = np.abs(fd_approx - analytic_grad) / np.maximum(1e-12, np.abs(fd_approx))\n        return discrepancy\n\ndef solve_inner(lam, AtA, Aty, LTL):\n    \"\"\"Solves the inner problem for x_lambda.\"\"\"\n    M = AtA + lam**2 * LTL\n    x_lam = np.linalg.solve(M, Aty)\n    return x_lam\n\ndef compute_gradient(lam, data):\n    \"\"\"Computes the analytical gradient of the validation risk.\"\"\"\n    AtA, Aty, LTL, A_val, y_val = data\n    \n    if lam == 0.0:\n        return 0.0\n\n    # 1. Solve for x_lambda\n    x_lam = solve_inner(lam, AtA, Aty, LTL)\n\n    # 2. Form M_lambda\n    M = AtA + lam**2 * LTL\n    \n    # 3. Solve for sensitivity component w\n    # We want to compute dR/dλ = -4λ (e_val)ᵀ A_val (M⁻¹ LᵀL x_λ)\n    # Let v = LᵀL x_λ. Let w = M⁻¹ v. Then dR/dλ = -4λ (e_val)ᵀ A_val w.\n    v = LTL @ x_lam\n    w = np.linalg.solve(M, v)\n\n    # 4. Compute final gradient\n    e_val = A_val @ x_lam - y_val\n    grad = -4 * lam * (e_val.T @ A_val @ w)\n    \n    return grad\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3427389"}]}