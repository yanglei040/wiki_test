{"hands_on_practices": [{"introduction": "Understanding a statistical estimator begins with building it from the ground up. This exercise guides you through the derivation of the Unbiased Predictive Risk Estimator (UPRE) from the fundamental definition of predictive risk in a simple, multi-channel linear model. By applying the derived UPRE to compare a single-parameter regularization scheme against a more flexible two-parameter approach, you will gain concrete insight into how UPRE can be used to select and justify more nuanced models, directly demonstrating its utility in practical scenarios [@problem_id:3429048].", "problem": "Consider a linear inverse problem with two independent observation channels of a common state vector. Let the forward model be $A = I_{2}$ and the regularization operator be $L = I_{2}$. The true noise-free data are $y_{\\text{true}} = x_{\\text{true}} \\in \\mathbb{R}^{2}$, and the observed data are $y = y_{\\text{true}} + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$ with diagonal covariance $\\Sigma = \\operatorname{diag}(\\sigma_{1}^{2}, \\sigma_{2}^{2})$. In this problem, take the observed data and noise levels to be\n$$\ny = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}, \\quad \\sigma_{1}^{2} = \\frac{1}{4}, \\quad \\sigma_{2}^{2} = 1.\n$$\nWe use Tikhonov-regularized estimators of the form $x_{\\lambda} = \\arg\\min_{x \\in \\mathbb{R}^{2}} \\|A x - y\\|_{2}^{2} + \\lambda \\|L x\\|_{2}^{2}$, which reduce here to linear shrinkage estimators $x_{\\lambda} = S y$, where $S$ depends on the chosen regularization parameters. We will compare two schemes:\n- Single-parameter scheme: a common $\\lambda \\ge 0$ applied across both channels, so $S = s I_{2}$ with $s = 1/(1+\\lambda)$.\n- Two-parameter scheme: independent $(\\lambda_{1}, \\lambda_{2}) \\ge 0$ across channels, so $S = \\operatorname{diag}(s_{1}, s_{2})$ with $s_{j} = 1/(1+\\lambda_{j})$ for $j \\in \\{1,2\\}$.\n\nFor each scheme, consider the predictive risk for the noise-free predictions $y_{\\text{true}}$, namely $\\mathcal{R}(S) = \\mathbb{E}\\big[\\|S y - y_{\\text{true}}\\|_{2}^{2}\\big]$, where the expectation is over the measurement noise. The Unbiased Predictive Risk Estimator (UPRE) is any function of the data $y$ and the parameters that is an unbiased estimator of $\\mathcal{R}(S)$, and it can be used to select the parameters by minimizing it with respect to $S$.\n\nTasks:\n1. Starting from the linear-Gaussian data model, derive an explicit expression for the Unbiased Predictive Risk Estimator (UPRE) for this setting in terms of $y$, $\\sigma_{1}^{2}$, $\\sigma_{2}^{2}$, and $S = \\operatorname{diag}(s_{1}, s_{2})$. Your derivation must begin from the definition $\\mathcal{R}(S) = \\mathbb{E}\\big[\\|S y - y_{\\text{true}}\\|_{2}^{2}\\big]$ and properties of expectations under Gaussian noise, without assuming any pre-existing UPRE formulas.\n2. Under the two-parameter scheme, minimize the joint UPRE over $(s_{1}, s_{2})$ and hence obtain $(\\lambda_{1}^{\\star}, \\lambda_{2}^{\\star})$ that minimize the joint UPRE. Evaluate the minimal UPRE for this scheme.\n3. Under the single-parameter scheme, minimize the UPRE over $s$ (equivalently $\\lambda$) and hence obtain $\\lambda^{\\star}$ that minimizes the UPRE. Evaluate the minimal UPRE for this scheme.\n4. Using the results of parts 2 and 3, compute the exact difference\n$$\n\\Delta \\;=\\; \\big(\\min_{\\lambda \\ge 0} \\operatorname{UPRE}_{\\text{single}}(\\lambda)\\big) \\;-\\; \\big(\\min_{\\lambda_{1}, \\lambda_{2} \\ge 0} \\operatorname{UPRE}_{\\text{two}}(\\lambda_{1}, \\lambda_{2})\\big).\n$$\nExpress your final answer as an exact rational number. This quantifies how tuning a single $\\lambda$ across channels via the Unbiased Predictive Risk Estimator (UPRE) can be suboptimal in predictive risk relative to a two-parameter scheme selected by minimizing the joint UPRE.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the theory of inverse problems and statistical estimation, specifically concerning Tikhonov regularization and unbiased risk estimation. The problem is well-posed, providing all necessary data and definitions for a unique solution. The language is objective and mathematically precise. Therefore, a full solution is warranted.\n\n### Task 1: Derivation of the Unbiased Predictive Risk Estimator (UPRE)\n\nThe predictive risk is defined as $\\mathcal{R}(S) = \\mathbb{E}\\big[\\|S y - y_{\\text{true}}\\|_{2}^{2}\\big]$, where the expectation is over the noise distribution $\\varepsilon$. The data model is $y = y_{\\text{true}} + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$.\n\nWe begin by expanding the risk definition, substituting $y = y_{\\text{true}} + \\varepsilon$:\n$$\n\\mathcal{R}(S) = \\mathbb{E}\\big[\\|S (y_{\\text{true}} + \\varepsilon) - y_{\\text{true}}\\|_{2}^{2}\\big] = \\mathbb{E}\\big[\\|(S - I)y_{\\text{true}} + S\\varepsilon\\|_{2}^{2}\\big]\n$$\nwhere $I$ is the identity matrix. Expanding the squared Euclidean norm:\n$$\n\\mathcal{R}(S) = \\mathbb{E}\\big[ \\big( (S-I)y_{\\text{true}} \\big)^{T} \\big( (S-I)y_{\\text{true}} \\big) + 2 \\big( (S-I)y_{\\text{true}} \\big)^{T} (S\\varepsilon) + (S\\varepsilon)^{T} (S\\varepsilon) \\big]\n$$\nBy linearity of expectation, we can distribute the expectation operator:\n$$\n\\mathcal{R}(S) = \\mathbb{E}\\big[ \\|(S-I)y_{\\text{true}}\\|_{2}^{2} \\big] + 2 \\mathbb{E}\\big[ y_{\\text{true}}^{T}(S-I)^{T}S\\varepsilon \\big] + \\mathbb{E}\\big[ \\|S\\varepsilon\\|_{2}^{2} \\big]\n$$\nThe first term involves only deterministic quantities, so $\\mathbb{E}\\big[ \\|(S-I)y_{\\text{true}}\\|_{2}^{2} \\big] = \\|(S-I)y_{\\text{true}}\\|_{2}^{2}$.\nFor the second term, since $y_{\\text{true}}$ and $S$ are deterministic, we have $2 y_{\\text{true}}^{T}(S-I)^{T}S \\, \\mathbb{E}[\\varepsilon]$. Since $\\mathbb{E}[\\varepsilon]=0$, this cross-term vanishes.\nThe risk is thus simplified to:\n$$\n\\mathcal{R}(S) = \\|(S-I)y_{\\text{true}}\\|_{2}^{2} + \\mathbb{E}\\big[ \\|S\\varepsilon\\|_{2}^{2} \\big]\n$$\nThe second term can be evaluated using the trace trick: $\\mathbb{E}[\\|S\\varepsilon\\|_{2}^{2}] = \\mathbb{E}[\\varepsilon^{T}S^{T}S\\varepsilon] = \\mathbb{E}[\\operatorname{tr}(\\varepsilon^{T}S^{T}S\\varepsilon)] = \\mathbb{E}[\\operatorname{tr}(S^{T}S\\varepsilon\\varepsilon^{T})] = \\operatorname{tr}(S^{T}S\\mathbb{E}[\\varepsilon\\varepsilon^{T}]) = \\operatorname{tr}(S^{T}S\\Sigma)$.\nSo, the true risk is:\n$$\n\\mathcal{R}(S) = \\|(S-I)y_{\\text{true}}\\|_{2}^{2} + \\operatorname{tr}(S^{T}S\\Sigma)\n$$\nThis expression depends on the unknown $y_{\\text{true}}$. An Unbiased Predictive Risk Estimator, $\\operatorname{UPRE}(y, S)$, must be a function of the observed data $y$ and satisfy $\\mathbb{E}[\\operatorname{UPRE}(y, S)] = \\mathcal{R}(S)$.\n\nTo construct such an estimator, we find an unbiased estimator for the unknown term $\\|(S-I)y_{\\text{true}}\\|_{2}^{2}$. Let's consider the quantity $\\|(S-I)y\\|_{2}^{2}$:\n$$\n\\|(S-I)y\\|_{2}^{2} = \\|(S-I)(y_{\\text{true}} + \\varepsilon)\\|_{2}^{2} = \\|(S-I)y_{\\text{true}} + (S-I)\\varepsilon\\|_{2}^{2}\n$$\nExpanding this squared norm gives:\n$$\n\\|(S-I)y\\|_{2}^{2} = \\|(S-I)y_{\\text{true}}\\|_{2}^{2} + 2y_{\\text{true}}^{T}(S-I)^{T}(S-I)\\varepsilon + \\|(S-I)\\varepsilon\\|_{2}^{2}\n$$\nTaking the expectation of this expression:\n$$\n\\mathbb{E}\\big[\\|(S-I)y\\|_{2}^{2}\\big] = \\|(S-I)y_{\\text{true}}\\|_{2}^{2} + \\mathbb{E}\\big[\\|(S-I)\\varepsilon\\|_{2}^{2}\\big]\n$$\nThe cross-term expectation is again zero. Using the trace trick for the second term, we get $\\mathbb{E}\\big[\\|(S-I)\\varepsilon\\|_{2}^{2}\\big] = \\operatorname{tr}\\big((S-I)^{T}(S-I)\\Sigma\\big)$.\nThus, we have the relationship:\n$$\n\\|(S-I)y_{\\text{true}}\\|_{2}^{2} = \\mathbb{E}\\big[\\|(S-I)y\\|_{2}^{2}\\big] - \\operatorname{tr}\\big((S-I)^{T}(S-I)\\Sigma\\big)\n$$\nThis implies that the quantity $\\|(S-I)y\\|_{2}^{2} - \\operatorname{tr}\\big((S-I)^{T}(S-I)\\Sigma\\big)$ is an unbiased estimator of $\\|(S-I)y_{\\text{true}}\\|_{2}^{2}$. Substituting this into the expression for $\\mathcal{R}(S)$, we obtain our UPRE function:\n$$\n\\operatorname{UPRE}(y, S) = \\left( \\|(S-I)y\\|_{2}^{2} - \\operatorname{tr}\\big((S-I)^{T}(S-I)\\Sigma\\big) \\right) + \\operatorname{tr}(S^{T}S\\Sigma)\n$$\nThe matrix $S$ is symmetric, so $S^T=S$. Thus $(S-I)^{T}(S-I) = (S-I)^2 = S^2 - 2S + I$.\n$$\n\\operatorname{UPRE}(y, S) = \\|(S-I)y\\|_{2}^{2} - \\operatorname{tr}\\big((S^2 - 2S + I)\\Sigma\\big) + \\operatorname{tr}(S^2\\Sigma)\n$$\nUsing the linearity of the trace:\n$$\n\\operatorname{UPRE}(y, S) = \\|(S-I)y\\|_{2}^{2} - \\operatorname{tr}(S^2\\Sigma) + 2\\operatorname{tr}(S\\Sigma) - \\operatorname{tr}(\\Sigma) + \\operatorname{tr}(S^2\\Sigma)\n$$\n$$\n\\operatorname{UPRE}(y, S) = \\|(S-I)y\\|_{2}^{2} + 2\\operatorname{tr}(S\\Sigma) - \\operatorname{tr}(\\Sigma)\n$$\nFor the specific case where $S = \\operatorname{diag}(s_{1}, s_{2})$, $\\Sigma = \\operatorname{diag}(\\sigma_{1}^{2}, \\sigma_{2}^{2})$, and $y = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}$:\n$\\|(S-I)y\\|_{2}^{2} = \\| \\operatorname{diag}(s_1-1, s_2-1)y \\|_{2}^{2} = (s_1-1)^2 y_1^2 + (s_2-1)^2 y_2^2$.\n$\\operatorname{tr}(S\\Sigma) = \\operatorname{tr}(\\operatorname{diag}(s_1\\sigma_1^2, s_2\\sigma_2^2)) = s_1\\sigma_1^2 + s_2\\sigma_2^2$.\n$\\operatorname{tr}(\\Sigma) = \\sigma_1^2 + \\sigma_2^2$.\nSubstituting these into the general UPRE formula gives the explicit expression:\n$$\n\\operatorname{UPRE}(s_1, s_2) = (s_1-1)^2 y_1^2 + (s_2-1)^2 y_2^2 + 2(s_1\\sigma_1^2 + s_2\\sigma_2^2) - (\\sigma_1^2 + \\sigma_2^2)\n$$\n\n### Task 2: Two-parameter scheme optimization\n\nThe UPRE expression is separable in $s_1$ and $s_2$. We can minimize it by minimizing each component function independently. For $j \\in \\{1,2\\}$, we minimize $f_j(s_j) = (s_j-1)^2 y_j^2 + 2s_j\\sigma_j^2$.\nTaking the derivative with respect to $s_j$ and setting it to zero:\n$$\n\\frac{df_j}{ds_j} = 2(s_j-1)y_j^2 + 2\\sigma_j^2 = 0 \\implies s_j y_j^2 - y_j^2 + \\sigma_j^2 = 0 \\implies s_j = \\frac{y_j^2 - \\sigma_j^2}{y_j^2} = 1 - \\frac{\\sigma_j^2}{y_j^2}\n$$\nThe constraint $\\lambda_j \\ge 0$ implies $s_j = 1/(1+\\lambda_j) \\in (0, 1]$. Since $y_j^2 > 0$ and $\\sigma_j^2 > 0$, the unconstrained minimum $s_j^\\star$ is always less than $1$. If $s_j^\\star \\le 0$, the minimum on $(0,1]$ is at the boundary, but typically the shrinkage factors are allowed to be in $[0,1]$ corresponding to $\\lambda_j \\in [0, \\infty]$, in which case the optimal value is $s_j^{\\star} = \\max(0, 1-\\sigma_j^2/y_j^2)$.\nGiven data: $y_1=3$, $y_2=2$, $\\sigma_1^2 = 1/4$, $\\sigma_2^2 = 1$.\n$$\ns_1^{\\star} = 1 - \\frac{1/4}{3^2} = 1 - \\frac{1}{36} = \\frac{35}{36}\n$$\n$$\ns_2^{\\star} = 1 - \\frac{1}{2^2} = 1 - \\frac{1}{4} = \\frac{3}{4}\n$$\nBoth values lie in $(0, 1]$, so they are the valid minima.\nThe corresponding optimal regularization parameters are $\\lambda_j^{\\star} = 1/s_j^{\\star} - 1$:\n$$\n\\lambda_1^{\\star} = \\frac{36}{35} - 1 = \\frac{1}{35}, \\quad \\lambda_2^{\\star} = \\frac{4}{3} - 1 = \\frac{1}{3}\n$$\nTo find the minimum UPRE value, we substitute $s_j^{\\star}$ back into the UPRE formula. The minimum value for each component is $f_j(s_j^{\\star}) = (\\sigma_j^4/y_j^4)y_j^2 + 2(1-\\sigma_j^2/y_j^2)\\sigma_j^2 = 2\\sigma_j^2 - \\sigma_j^4/y_j^2$.\nThus, the total minimal UPRE is:\n$$\n\\min_{\\lambda_1, \\lambda_2} \\operatorname{UPRE}_{\\text{two}} = \\sum_{j=1}^2 \\left( 2\\sigma_j^2 - \\frac{\\sigma_j^4}{y_j^2} \\right) - (\\sigma_1^2+\\sigma_2^2) = \\sum_{j=1}^2 \\left( \\sigma_j^2 - \\frac{\\sigma_j^4}{y_j^2} \\right)\n$$\nPlugging in the values:\n$$\n\\min \\operatorname{UPRE}_{\\text{two}} = \\left(\\frac{1}{4} - \\frac{(1/4)^2}{3^2}\\right) + \\left(1 - \\frac{1^2}{2^2}\\right) = \\left(\\frac{1}{4} - \\frac{1}{144}\\right) + \\left(1 - \\frac{1}{4}\\right) = 1 - \\frac{1}{144} = \\frac{143}{144}\n$$\n\n### Task 3: Single-parameter scheme optimization\n\nIn this scheme, $s_1 = s_2 = s$. The UPRE becomes:\n$$\n\\operatorname{UPRE}_{\\text{single}}(s) = (s-1)^2 y_1^2 + (s-1)^2 y_2^2 + 2s\\sigma_1^2 + 2s\\sigma_2^2 - (\\sigma_1^2+\\sigma_2^2)\n$$\n$$\n\\operatorname{UPRE}_{\\text{single}}(s) = (s-1)^2 (y_1^2+y_2^2) + 2s(\\sigma_1^2+\\sigma_2^2) - (\\sigma_1^2+\\sigma_2^2)\n$$\nMinimizing with respect to $s$:\n$$\n\\frac{d}{ds}\\operatorname{UPRE}_{\\text{single}} = 2(s-1)(y_1^2+y_2^2) + 2(\\sigma_1^2+\\sigma_2^2) = 0\n$$\n$$\ns^{\\star} = \\frac{(y_1^2+y_2^2) - (\\sigma_1^2+\\sigma_2^2)}{y_1^2+y_2^2} = 1 - \\frac{\\sigma_1^2+\\sigma_2^2}{y_1^2+y_2^2}\n$$\nNumerical values: $y_1^2+y_2^2 = 3^2+2^2 = 13$ and $\\sigma_1^2+\\sigma_2^2 = 1/4+1 = 5/4$.\n$$\ns^{\\star} = 1 - \\frac{5/4}{13} = 1 - \\frac{5}{52} = \\frac{47}{52}\n$$\nThis value lies in $(0, 1]$. The optimal parameter $\\lambda^\\star$ is:\n$$\n\\lambda^{\\star} = \\frac{1}{s^{\\star}} - 1 = \\frac{52}{47} - 1 = \\frac{5}{47}\n$$\nThe minimum UPRE value is:\n$$\n\\min_{\\lambda} \\operatorname{UPRE}_{\\text{single}} = (s^{\\star}-1)^2(y_1^2+y_2^2) + 2s^{\\star}(\\sigma_1^2+\\sigma_2^2) - (\\sigma_1^2+\\sigma_2^2)\n$$\nThe general minimal value is $(\\sigma_1^2+\\sigma_2^2) - \\frac{(\\sigma_1^2+\\sigma_2^2)^2}{y_1^2+y_2^2}$.\n$$\n\\min \\operatorname{UPRE}_{\\text{single}} = \\frac{5}{4} - \\frac{(5/4)^2}{13} = \\frac{5}{4} - \\frac{25/16}{13} = \\frac{5}{4} - \\frac{25}{208} = \\frac{5 \\times 52}{208} - \\frac{25}{208} = \\frac{260 - 25}{208} = \\frac{235}{208}\n$$\n\n### Task 4: Calculation of the difference $\\Delta$\n\nThe difference is defined as $\\Delta = \\big(\\min_{\\lambda} \\operatorname{UPRE}_{\\text{single}}(\\lambda)\\big) - \\big(\\min_{\\lambda_{1}, \\lambda_{2}} \\operatorname{UPRE}_{\\text{two}}(\\lambda_{1}, \\lambda_{2})\\big)$.\nUsing the results from Tasks 2 and 3:\n$$\n\\Delta = \\frac{235}{208} - \\frac{143}{144}\n$$\nTo subtract these fractions, we find a common denominator. The prime factorizations are $208 = 2^4 \\times 13$ and $144 = 2^4 \\times 3^2$.\nThe least common multiple is $\\operatorname{lcm}(208, 144) = 2^4 \\times 3^2 \\times 13 = 16 \\times 9 \\times 13 = 1872$.\n$$\n\\Delta = \\frac{235 \\times 9}{1872} - \\frac{143 \\times 13}{1872}\n$$\nThe numerators are:\n$235 \\times 9 = 2115$\n$143 \\times 13 = 1859$\n$$\n\\Delta = \\frac{2115 - 1859}{1872} = \\frac{256}{1872}\n$$\nTo simplify the fraction, we use the prime factorizations: $256 = 2^8$ and $1872=2^4 \\times 117$.\n$$\n\\Delta = \\frac{2^8}{2^4 \\times 117} = \\frac{2^4}{117} = \\frac{16}{117}\n$$\nThe positive difference quantifies the performance gain, in terms of estimated predictive risk, of using a more flexible two-parameter regularization scheme over a single-parameter scheme.", "answer": "$$\n\\boxed{\\frac{16}{117}}\n$$", "id": "3429048"}, {"introduction": "Once an objective function like UPRE is formulated, the next practical step is to find the parameter that minimizes it. This practice moves from the definition of UPRE to the mechanics of its optimization. You will use the Singular Value Decomposition (SVD) to express the UPRE function and its derivatives in a simple scalar form, and then use these results to construct a one-step update for Newton's method, a powerful algorithm for finding the optimal regularization parameter $\\lambda$ [@problem_id:3429134]. This exercise connects the statistical theory of risk estimation with the numerical practice of efficient optimization.", "problem": "Consider the discrete linear inverse problem with additive Gaussian noise, where the observed data vector $y \\in \\mathbb{R}^{m}$ is modeled as $y = A x_{\\star} + \\varepsilon$, with $A \\in \\mathbb{R}^{m \\times n}$, $x_{\\star} \\in \\mathbb{R}^{n}$ the unknown truth, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$ a zero-mean Gaussian noise with variance $\\sigma^{2} > 0$. We consider zero-order Tikhonov regularization with regularization parameter $\\lambda > 0$, defined by\n$$\n\\widehat{x}_{\\lambda} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\|A x - y\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2} \\right\\}.\n$$\nThe corresponding linear estimator can be written as $\\widehat{x}_{\\lambda} = S(\\lambda) y$ with $S(\\lambda) = (A^{\\top} A + \\lambda^{2} I_{n})^{-1} A^{\\top}$. Define the predictive risk for the data space as $R_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[\\|A \\widehat{x}_{\\lambda} - A x_{\\star}\\|_{2}^{2}\\right]$. The Unbiased Predictive Risk Estimator (UPRE) for $R_{\\mathrm{pred}}(\\lambda)$ under the Gaussian model is the function\n$$\nU(\\lambda) \\equiv \\mathrm{UPRE}(\\lambda) = \\|(I_{m} - A S(\\lambda)) y\\|_{2}^{2} + 2 \\sigma^{2} \\,\\mathrm{tr}(A S(\\lambda)) - m \\sigma^{2}.\n$$\nLet the Singular Value Decomposition (SVD) of $A$ be $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ has diagonal entries $\\{\\sigma_{i}\\}_{i=1}^{r}$ on its leading $r \\times r$ block with $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge \\sigma_{r} > 0$ and $r = \\mathrm{rank}(A) \\le \\min\\{m, n\\}$. Denote the data coefficients by $w_{i} = u_{i}^{\\top} y$ for $i \\in \\{1, \\dots, m\\}$, where $u_{i}$ is the $i$-th column of $U$.\n\nTasks:\n- Starting from the definition of $U(\\lambda)$ and the SVD of $A$, express $U(\\lambda)$ entirely in terms of $\\{\\sigma_{i}\\}_{i=1}^{r}$, $\\{w_{i}\\}_{i=1}^{m}$, and $\\lambda$, by eliminating matrix traces and norms. Then differentiate to obtain explicit formulas for the first derivative $U'(\\lambda)$ and the second derivative $U''(\\lambda)$.\n- State the first-order optimality condition for minimizing $U(\\lambda)$ and the associated second-order condition in terms of $U'(\\lambda)$ and $U''(\\lambda)$.\n- Design a Newton or quasi-Newton iteration for $\\lambda$ that uses only SVD-based scalar quantities. Provide the exact one-step Newton update formula $\\lambda_{\\mathrm{new}} = \\lambda - U'(\\lambda)/U''(\\lambda)$ as a closed-form analytic expression in terms of $\\{\\sigma_{i}\\}_{i=1}^{r}$, $\\{w_{i}\\}_{i=1}^{r}$, $\\sigma^{2}$, and $\\lambda$.\n\nAnswer format requirement:\n- Your final answer must be a single closed-form analytic expression for the one-step Newton update $\\lambda_{\\mathrm{new}}$ in terms of $\\{\\sigma_{i}\\}_{i=1}^{r}$, $\\{w_{i}\\}_{i=1}^{r}$, $\\sigma^{2}$, and $\\lambda$.\n- No numerical evaluation is required.\n- Do not include any units in the final answer.", "solution": "The problem is first validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   The data model is $y = A x_{\\star} + \\varepsilon$, where $y \\in \\mathbb{R}^{m}$, $A \\in \\mathbb{R}^{m \\times n}$, $x_{\\star} \\in \\mathbb{R}^{n}$, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$ with $\\sigma^{2} > 0$.\n-   The Tikhonov regularized solution is $\\widehat{x}_{\\lambda} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\|A x - y\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2} \\right\\}$ for $\\lambda > 0$.\n-   The estimator is linear: $\\widehat{x}_{\\lambda} = S(\\lambda) y$, with $S(\\lambda) = (A^{\\top} A + \\lambda^{2} I_{n})^{-1} A^{\\top}$.\n-   The predictive risk is $R_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[\\|A \\widehat{x}_{\\lambda} - A x_{\\star}\\|_{2}^{2}\\right]$.\n-   The Unbiased Predictive Risk Estimator (UPRE) is given by $U(\\lambda) = \\|(I_{m} - A S(\\lambda)) y\\|_{2}^{2} + 2 \\sigma^{2} \\,\\mathrm{tr}(A S(\\lambda)) - m \\sigma^{2}$.\n-   The Singular Value Decomposition (SVD) of $A$ is $A = U \\Sigma V^{\\top}$, with $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ being orthogonal matrices. $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix with non-zero singular values $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge \\sigma_{r} > 0$, where $r = \\mathrm{rank}(A)$.\n-   The data coefficients in the SVD basis are $w_{i} = u_{i}^{\\top} y$ for $i \\in \\{1, \\dots, m\\}$, where $u_{i}$ is the $i$-th column of $U$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is set within the standard framework of linear inverse problems and regularization theory. All concepts, including Tikhonov regularization, SVD, and UPRE, are well-established and fundamental to the field.\n-   **Well-Posed:** The problem is clearly stated, providing all necessary definitions and relationships between quantities. The tasks require specific, derivable analytical expressions, indicating a unique solution path exists.\n-   **Objective:** The language is formal, mathematical, and free of any subjective or ambiguous terminology.\n\nA detailed check against the list of flaws confirms the problem's validity. It is scientifically sound, formally stated, complete, and poses a non-trivial but solvable challenge in mathematical analysis.\n\n**Step 3: Verdict and Action**\n-   **Verdict:** The problem is valid.\n-   **Action:** Proceed with a full solution.\n\n### Solution\n\nThe solution proceeds by first expressing the UPRE function $U(\\lambda)$ and its derivatives in terms of the SVD components of the matrix $A$.\n\n**1. Express $U(\\lambda)$ in SVD coordinates**\n\nWe begin by analyzing the matrix product $A S(\\lambda)$ using the SVD of $A = U \\Sigma V^{\\top}$.\nFirst, consider the term $A^{\\top} A + \\lambda^{2} I_{n}$:\n$$A^{\\top} A + \\lambda^{2} I_{n} = (U \\Sigma V^{\\top})^{\\top} (U \\Sigma V^{\\top}) + \\lambda^{2} I_{n} = V \\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top} + \\lambda^{2} V V^{\\top} = V (\\Sigma^{\\top} \\Sigma + \\lambda^{2} I_{n}) V^{\\top}$$\nThe inverse is:\n$$(A^{\\top} A + \\lambda^{2} I_{n})^{-1} = V (\\Sigma^{\\top} \\Sigma + \\lambda^{2} I_{n})^{-1} V^{\\top}$$\nNow, we can write the solution operator $S(\\lambda)$:\n$$S(\\lambda) = (A^{\\top} A + \\lambda^{2} I_{n})^{-1} A^{\\top} = V (\\Sigma^{\\top} \\Sigma + \\lambda^{2} I_{n})^{-1} V^{\\top} (V \\Sigma^{\\top} U^{\\top}) = V (\\Sigma^{\\top} \\Sigma + \\lambda^{2} I_{n})^{-1} \\Sigma^{\\top} U^{\\top}$$\nThe central object $A S(\\lambda)$, often called the influence matrix, becomes:\n$$A S(\\lambda) = (U \\Sigma V^{\\top}) S(\\lambda) = U \\Sigma V^{\\top} V (\\Sigma^{\\top} \\Sigma + \\lambda^{2} I_{n})^{-1} \\Sigma^{\\top} U^{\\top} = U \\left( \\Sigma (\\Sigma^{\\top} \\Sigma + \\lambda^{2} I_{n})^{-1} \\Sigma^{\\top} \\right) U^{\\top}$$\nThe matrix $\\Sigma (\\Sigma^{\\top} \\Sigma + \\lambda^{2} I_{n})^{-1} \\Sigma^{\\top}$ is an $m \\times m$ diagonal matrix. Its diagonal entries, known as filter factors, are:\n$$f_{i} = \\begin{cases} \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}} & \\text{for } i = 1, \\dots, r \\\\ 0 & \\text{for } i = r+1, \\dots, m \\end{cases}$$\nLet's denote this diagonal matrix by $\\Lambda_{f}$. Then $A S(\\lambda) = U \\Lambda_{f} U^{\\top}$.\n\nNow we can rewrite the terms in $U(\\lambda)$:\n-   The residual norm term:\n    $$\\|(I_{m} - A S(\\lambda)) y\\|_{2}^{2} = \\|(I_{m} - U \\Lambda_{f} U^{\\top}) y\\|_{2}^{2} = \\|U (I_{m} - \\Lambda_{f}) U^{\\top} y\\|_{2}^{2}$$\n    Since $U$ is orthogonal, this is equal to $\\|(I_{m} - \\Lambda_{f}) (U^{\\top} y)\\|_{2}^{2}$. Let $w = U^{\\top} y$, so $w_{i} = u_i^\\top y$. The components of $(I_{m} - \\Lambda_{f}) w$ are $(1-f_{i})w_{i}$. The squared norm is:\n    $$\\sum_{i=1}^{m} (1 - f_{i})^{2} w_{i}^{2} = \\sum_{i=1}^{r} \\left(1 - \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}}\\right)^{2} w_{i}^{2} + \\sum_{i=r+1}^{m} (1-0)^{2} w_{i}^{2} = \\sum_{i=1}^{r} \\left(\\frac{\\lambda^{2}}{\\sigma_{i}^{2} + \\lambda^{2}}\\right)^{2} w_{i}^{2} + \\sum_{i=r+1}^{m} w_{i}^{2}$$\n-   The trace term:\n    $$\\mathrm{tr}(A S(\\lambda)) = \\mathrm{tr}(U \\Lambda_{f} U^{\\top}) = \\mathrm{tr}(\\Lambda_{f} U^{\\top} U) = \\mathrm{tr}(\\Lambda_{f}) = \\sum_{i=1}^{m} f_{i} = \\sum_{i=1}^{r} \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}}$$\nCombining these gives the full expression for $U(\\lambda)$:\n$$U(\\lambda) = \\sum_{i=1}^{r} \\frac{\\lambda^{4}}{(\\sigma_{i}^{2} + \\lambda^{2})^{2}} w_{i}^{2} + \\sum_{i=r+1}^{m} w_{i}^{2} + 2 \\sigma^{2} \\sum_{i=1}^{r} \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}} - m \\sigma^{2}$$\n\n**2. First and Second Derivatives of $U(\\lambda)$**\n\nTo find the minimum of $U(\\lambda)$, we compute its derivatives with respect to $\\lambda$. The terms $\\sum_{i=r+1}^{m} w_{i}^{2}$ and $-m \\sigma^{2}$ are constant with respect to $\\lambda$ and vanish upon differentiation.\n\nFor the first derivative, $U'(\\lambda)$:\n$$ \\frac{d}{d\\lambda} \\left( \\frac{\\lambda^{4}}{(\\sigma_i^2+\\lambda^2)^2} \\right) = \\frac{4\\lambda^3(\\sigma_i^2+\\lambda^2)^2 - \\lambda^4 \\cdot 2(\\sigma_i^2+\\lambda^2)(2\\lambda)}{(\\sigma_i^2+\\lambda^2)^4} = \\frac{4\\lambda^3(\\sigma_i^2+\\lambda^2) - 4\\lambda^5}{(\\sigma_i^2+\\lambda^2)^3} = \\frac{4\\lambda^3\\sigma_i^2}{(\\sigma_i^2+\\lambda^2)^3} $$\n$$ \\frac{d}{d\\lambda} \\left( \\frac{\\sigma_i^2}{\\sigma_i^2+\\lambda^2} \\right) = \\sigma_i^2 \\frac{-1}{(\\sigma_i^2+\\lambda^2)^2} (2\\lambda) = \\frac{-2\\lambda\\sigma_i^2}{(\\sigma_i^2+\\lambda^2)^2} $$\nCombining these results, we get $U'(\\lambda)$:\n$$U'(\\lambda) = \\sum_{i=1}^{r} \\frac{4\\lambda^3\\sigma_i^2 w_i^2}{(\\sigma_i^2 + \\lambda^2)^3} + 2\\sigma^2 \\sum_{i=1}^{r} \\frac{-2\\lambda\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2} = \\sum_{i=1}^{r} \\frac{4\\lambda^3\\sigma_i^2 w_i^2}{(\\sigma_i^2 + \\lambda^2)^3} - \\sum_{i=1}^{r} \\frac{4\\lambda\\sigma^2\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2}$$\n\nFor the second derivative, $U''(\\lambda)$, we differentiate $U'(\\lambda)$:\nFor the first term in $U'(\\lambda)$:\n$$ \\frac{d}{d\\lambda} \\left( \\frac{4\\lambda^3\\sigma_i^2}{(\\sigma_i^2+\\lambda^2)^3} \\right) = 4\\sigma_i^2 \\frac{3\\lambda^2(\\sigma_i^2+\\lambda^2)^3 - \\lambda^3 \\cdot 3(\\sigma_i^2+\\lambda^2)^2(2\\lambda)}{(\\sigma_i^2+\\lambda^2)^6} = 4\\sigma_i^2 \\frac{3\\lambda^2(\\sigma_i^2+\\lambda^2) - 6\\lambda^4}{(\\sigma_i^2+\\lambda^2)^4} = \\frac{12\\lambda^2\\sigma_i^2(\\sigma_i^2-\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^4} $$\nFor the second term in $U'(\\lambda)$:\n$$ \\frac{d}{d\\lambda} \\left( \\frac{-4\\lambda\\sigma^2\\sigma_i^2}{(\\sigma_i^2+\\lambda^2)^2} \\right) = -4\\sigma^2\\sigma_i^2 \\frac{1(\\sigma_i^2+\\lambda^2)^2 - \\lambda \\cdot 2(\\sigma_i^2+\\lambda^2)(2\\lambda)}{(\\sigma_i^2+\\lambda^2)^4} = -4\\sigma^2\\sigma_i^2 \\frac{\\sigma_i^2+\\lambda^2 - 4\\lambda^2}{(\\sigma_i^2+\\lambda^2)^3} = \\frac{-4\\sigma^2\\sigma_i^2(\\sigma_i^2-3\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^3} $$\nCombining these, we get $U''(\\lambda)$:\n$$U''(\\lambda) = \\sum_{i=1}^{r} \\frac{12\\lambda^2\\sigma_i^2(\\sigma_i^2-\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^4} w_i^2 - \\sum_{i=1}^{r} \\frac{4\\sigma^2\\sigma_i^2(\\sigma_i^2-3\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^3}$$\n\n**3. Optimality Conditions**\n\nTo find the value of $\\lambda > 0$ that minimizes $U(\\lambda)$, we use standard calculus conditions for an extremum.\n-   **First-order necessary condition:** A candidate minimum $\\lambda^{\\ast}$ must be a stationary point, i.e., it must satisfy $U'(\\lambda^{\\ast}) = 0$.\n-   **Second-order sufficient condition:** For the stationary point to be a local minimum, the second derivative must be positive, i.e., $U''(\\lambda^{\\ast}) > 0$.\n\n**4. Newton's Method for Minimizing $U(\\lambda)$**\n\nMinimizing $U(\\lambda)$ is equivalent to finding a root of its derivative, $U'(\\lambda)=0$. Newton's method for finding a root of a function $f(x)=0$ provides the iterative update $x_{k+1} = x_k - f(x_k)/f'(x_k)$. Applying this to $f(\\lambda) = U'(\\lambda)$, we get $f'(\\lambda) = U''(\\lambda)$, and the update rule for $\\lambda$ is:\n$$\\lambda_{\\mathrm{new}} = \\lambda - \\frac{U'(\\lambda)}{U''(\\lambda)}$$\nSubstituting the expressions for $U'(\\lambda)$ and $U''(\\lambda)$ yields the one-step Newton update formula:\n$$\\lambda_{\\mathrm{new}} = \\lambda - \\frac{\\sum_{i=1}^{r} \\frac{4\\lambda^3\\sigma_i^2 w_i^2}{(\\sigma_i^2 + \\lambda^2)^3} - \\sum_{i=1}^{r} \\frac{4\\lambda\\sigma^2\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2}}{\\sum_{i=1}^{r} \\frac{12\\lambda^2\\sigma_i^2(\\sigma_i^2-\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^4} w_i^2 - \\sum_{i=1}^{r} \\frac{4\\sigma^2\\sigma_i^2(\\sigma_i^2-3\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^3}}$$\nWe can cancel the common factor of $4$ to obtain the final expression.\n$$\\lambda_{\\mathrm{new}} = \\lambda - \\frac{\\sum_{i=1}^{r} \\frac{\\lambda^3\\sigma_i^2 w_i^2}{(\\sigma_i^2 + \\lambda^2)^3} - \\sum_{i=1}^{r} \\frac{\\lambda\\sigma^2\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2}}{\\sum_{i=1}^{r} \\frac{3\\lambda^2\\sigma_i^2 w_i^2(\\sigma_i^2-\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^4} - \\sum_{i=1}^{r} \\frac{\\sigma^2\\sigma_i^2(\\sigma_i^2-3\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^3}}$$\nThis expression is in the required terms: $\\{\\sigma_i\\}_{i=1}^r$, $\\{w_i\\}_{i=1}^r$, $\\sigma^2$, and the current iterate $\\lambda$.", "answer": "$$\\boxed{\\lambda - \\frac{\\sum_{i=1}^{r} \\frac{\\lambda^{3}\\sigma_{i}^{2} w_{i}^{2}}{(\\sigma_{i}^{2} + \\lambda^{2})^{3}} - \\sum_{i=1}^{r} \\frac{\\lambda\\sigma^{2}\\sigma_{i}^{2}}{(\\sigma_{i}^{2} + \\lambda^{2})^{2}}}{\\sum_{i=1}^{r} \\frac{3\\lambda^{2}\\sigma_{i}^{2}w_{i}^{2}(\\sigma_{i}^{2}-\\lambda^{2})}{(\\sigma_{i}^{2}+\\lambda^{2})^{4}} - \\sum_{i=1}^{r} \\frac{\\sigma^{2}\\sigma_{i}^{2}(\\sigma_{i}^{2}-3\\lambda^{2})}{(\\sigma_{i}^{2}+\\lambda^{2})^{3}}}}$$", "id": "3429134"}, {"introduction": "In many large-scale applications, the trace term in the UPRE formula is computationally intractable and must be approximated, often with Monte Carlo methods. This introduces a new source of randomness into our estimate of the risk, and it is crucial to understand its impact. This advanced exercise guides you through analyzing the stability of the UPRE by deriving an expression for its variance, distinguishing the uncertainty arising from measurement noise from that of the randomized trace estimator [@problem_id:3429085]. This allows you to develop a quantitative criterion to assess the reliability of a parameter selected using this practical, yet approximate, method.", "problem": "Consider the linear inverse problem with additive Gaussian noise $y \\in \\mathbb{R}^{m}$ given by $y = A x_{\\star} + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$, $x_{\\star} \\in \\mathbb{R}^{n}$ is fixed but unknown, and $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2} I_{m})$ with known noise variance $\\sigma^{2} > 0$. For a fixed regularization parameter $\\lambda > 0$, define the Tikhonov-regularized estimate \n$$\nx_{\\lambda} = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\|A x - y\\|_{2}^{2} + \\lambda^{2} \\|L x\\|_{2}^{2},\n$$\nwhere $L \\in \\mathbb{R}^{q \\times n}$ has full row rank. Let the predictive (smoothing) matrix be\n$$\nS_{\\lambda} = A\\left(A^{\\top} A + \\lambda^{2} L^{\\top} L\\right)^{-1} A^{\\top} \\in \\mathbb{R}^{m \\times m},\n$$\nand the residual operator $R_{\\lambda} = I_{m} - S_{\\lambda}$. The Unbiased Predictive Risk Estimator (UPRE) for the predictive risk at $\\lambda$ is\n$$\n\\widehat{U}(\\lambda) = \\|R_{\\lambda} y\\|_{2}^{2} + 2 \\sigma^{2} \\operatorname{tr}(S_{\\lambda}) - m \\sigma^{2}.\n$$\nAssume that $\\operatorname{tr}(S_{\\lambda})$ is approximated via a Monte Carlo trace estimator using $p \\in \\mathbb{N}$ independent probe vectors $z^{(k)} \\sim \\mathcal{N}(0,I_{m})$, $k \\in \\{1,\\dots,p\\}$, independent of $y$, so that\n$$\n\\widehat{\\operatorname{tr}}(S_{\\lambda}) = \\frac{1}{p} \\sum_{k=1}^{p} \\left(z^{(k)}\\right)^{\\top} S_{\\lambda} z^{(k)},\n\\quad\\text{and}\\quad\n\\widehat{U}_{\\mathrm{MC}}(\\lambda) = \\|R_{\\lambda} y\\|_{2}^{2} + 2 \\sigma^{2} \\widehat{\\operatorname{tr}}(S_{\\lambda}) - m \\sigma^{2}.\n$$\n\nUsing only core facts about Gaussian quadratic forms and independence of the two randomness sources, derive a closed-form expression for the variance of $\\widehat{U}_{\\mathrm{MC}}(\\lambda)$ at a fixed $\\lambda$ by decomposing the contribution due to the measurement noise $\\varepsilon$ and the contribution due to the randomized trace estimation. Express your final result in terms of $\\sigma^{2}$, $p$, $\\mu = A x_{\\star}$, $S_{\\lambda}$, and $R_{\\lambda}$. You may use $\\| \\cdot \\|_{F}$ for the Frobenius norm and $\\operatorname{tr}(\\cdot)$ for the trace.\n\nAdditionally, propose a quantitative criterion, based on your variance expression, to assess the reliability of the $\\lambda$ selected by minimizing $\\widehat{U}_{\\mathrm{MC}}(\\lambda)$, explaining the rationale of your criterion. Your final numeric result must be a single analytic expression for $\\operatorname{Var}\\!\\left[\\widehat{U}_{\\mathrm{MC}}(\\lambda)\\right]$; no rounding is required.", "solution": "The problem as stated is a standard derivation in the field of inverse problems and statistical learning, concerning the properties of a regularized solution estimator.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n-   **Model:** Linear inverse problem with additive Gaussian noise: $y = A x_{\\star} + \\varepsilon$, for $y \\in \\mathbb{R}^{m}$, $A \\in \\mathbb{R}^{m \\times n}$, $x_{\\star} \\in \\mathbb{R}^{n}$.\n-   **Noise:** $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2} I_{m})$, with known variance $\\sigma^{2} > 0$.\n-   **Regularized Estimate:** $x_{\\lambda} = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\|A x - y\\|_{2}^{2} + \\lambda^{2} \\|L x\\|_{2}^{2}$ for a fixed $\\lambda > 0$, where $L \\in \\mathbb{R}^{q \\times n}$ has full row rank.\n-   **Smoothing Matrix:** $S_{\\lambda} = A\\left(A^{\\top} A + \\lambda^{2} L^{\\top} L\\right)^{-1} A^{\\top} \\in \\mathbb{R}^{m \\times m}$.\n-   **Residual Operator:** $R_{\\lambda} = I_{m} - S_{\\lambda}$.\n-   **Unbiased Predictive Risk Estimator (UPRE):** $\\widehat{U}(\\lambda) = \\|R_{\\lambda} y\\|_{2}^{2} + 2 \\sigma^{2} \\operatorname{tr}(S_{\\lambda}) - m \\sigma^{2}$.\n-   **Monte Carlo Trace Estimator:** $\\widehat{\\operatorname{tr}}(S_{\\lambda}) = \\frac{1}{p} \\sum_{k=1}^{p} \\left(z^{(k)}\\right)^{\\top} S_{\\lambda} z^{(k)}$, where $z^{(k)} \\sim \\mathcal{N}(0,I_{m})$ for $k \\in \\{1,\\dots,p\\}$ are independent probe vectors, also independent of $y$.\n-   **Monte Carlo UPRE:** $\\widehat{U}_{\\mathrm{MC}}(\\lambda) = \\|R_{\\lambda} y\\|_{2}^{2} + 2 \\sigma^{2} \\widehat{\\operatorname{tr}}(S_{\\lambda}) - m \\sigma^{2}$.\n-   **Definitions:** $\\mu = A x_{\\star}$. $\\| \\cdot \\|_{F}$ for Frobenius norm, $\\operatorname{tr}(\\cdot)$ for trace.\n-   **Objective:** Derive a closed-form expression for $\\operatorname{Var}\\!\\left[\\widehat{U}_{\\mathrm{MC}}(\\lambda)\\right]$ and propose a reliability criterion for the choice of $\\lambda$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is firmly rooted in the theory of Tikhonov regularization for linear inverse problems, a cornerstone of numerical analysis and statistics. The UPRE is a well-established method for parameter selection, and its Monte Carlo approximation is a standard computational technique. All premises are factually and mathematically sound.\n-   **Well-Posed:** The problem is well-defined. The assumption that $L$ has full row rank and $\\lambda > 0$ ensures, under the common implicit condition that $\\ker(A) \\cap \\ker(L) = \\{0\\}$, that the matrix $A^{\\top} A + \\lambda^{2} L^{\\top} L$ is invertible, making $S_{\\lambda}$ well-defined. The task is to derive a specific quantity, which is a soluble, unambiguous mathematical exercise.\n-   **Objective:** The problem is stated in precise, objective mathematical language, free of any subjective or speculative content.\n-   The problem does not exhibit any of the flaws listed in the instructions (e.g., incompleteness, contradiction, infeasibility, ambiguity).\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed with the derivation.\n\nThe quantity whose variance we seek is $\\widehat{U}_{\\mathrm{MC}}(\\lambda) = \\|R_{\\lambda} y\\|_{2}^{2} + 2 \\sigma^{2} \\widehat{\\operatorname{tr}}(S_{\\lambda}) - m \\sigma^{2}$.\nIt has two sources of randomness: the measurement noise $\\varepsilon$ in $y$, and the probe vectors $z^{(k)}$ in $\\widehat{\\operatorname{tr}}(S_{\\lambda})$. As these sources are independent, we can decompose the total variance into a sum of variances from each source.\nLet $V_y = \\|R_{\\lambda} y\\|_{2}^{2}$ and $V_z = 2 \\sigma^{2} \\widehat{\\operatorname{tr}}(S_{\\lambda})$. The constant term $-m\\sigma^2$ has zero variance. Since $V_y$ and $V_z$ are independent, the total variance is:\n$$\n\\operatorname{Var}\\!\\left[\\widehat{U}_{\\mathrm{MC}}(\\lambda)\\right] = \\operatorname{Var}\\!\\left[V_y + V_z - m\\sigma^2\\right] = \\operatorname{Var}[V_y] + \\operatorname{Var}[V_z]\n$$\nWe will compute these two terms separately.\n\n**1. Variance from Measurement Noise, $\\operatorname{Var}[V_y]$**\n\nThe term $V_y$ is $V_y = \\|R_{\\lambda} y\\|_{2}^{2}$. The randomness in $y = \\mu + \\varepsilon$ comes from $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2} I_{m})$.\nThe matrices $S_{\\lambda}$ and $R_{\\lambda} = I_m - S_{\\lambda}$ are symmetric. Thus, $R_{\\lambda}^{\\top}=R_{\\lambda}$.\nWe can expand $V_y$ as a quadratic form in $\\varepsilon$:\n$$\nV_y = \\|R_{\\lambda}(\\mu + \\varepsilon)\\|_{2}^{2} = \\|R_{\\lambda} \\mu + R_{\\lambda} \\varepsilon\\|_{2}^{2} = (R_{\\lambda} \\mu + R_{\\lambda} \\varepsilon)^{\\top}(R_{\\lambda} \\mu + R_{\\lambda} \\varepsilon)\n$$\n$$\nV_y = \\mu^{\\top} R_{\\lambda}^2 \\mu + 2 \\mu^{\\top} R_{\\lambda}^{2} \\varepsilon + \\varepsilon^{\\top} R_{\\lambda}^{2} \\varepsilon\n$$\nThis expression is of the form $c + b^{\\top}\\varepsilon + \\varepsilon^{\\top}M\\varepsilon$, where $c=\\|R_{\\lambda}\\mu\\|_2^2$ is a constant, $b = 2 R_{\\lambda}^{2} \\mu$ is a constant vector, and $M = R_{\\lambda}^{2}$ is a symmetric matrix.\nThe variance is $\\operatorname{Var}[V_y] = \\operatorname{Var}[b^{\\top}\\varepsilon + \\varepsilon^{\\top}M\\varepsilon]$.\nFor a centered Gaussian vector $\\varepsilon$, the linear form $b^{\\top}\\varepsilon$ and the quadratic form $\\varepsilon^{\\top}M\\varepsilon$ are uncorrelated. This is a known property, verifiable through Isserlis' theorem, which states that the expectation of the product of three centered Gaussian random variables is zero.\nThus, $\\operatorname{Cov}(b^{\\top}\\varepsilon, \\varepsilon^{\\top}M\\varepsilon) = 0$.\nThe required variance is the sum of the variances of the linear and quadratic parts:\n$$\n\\operatorname{Var}[V_y] = \\operatorname{Var}[b^{\\top}\\varepsilon] + \\operatorname{Var}[\\varepsilon^{\\top}M\\varepsilon]\n$$\nThe variance of the linear term is:\n$$\n\\operatorname{Var}[b^{\\top}\\varepsilon] = b^{\\top} \\operatorname{Cov}(\\varepsilon) b = (2 R_{\\lambda}^{2} \\mu)^{\\top} (\\sigma^{2} I_m) (2 R_{\\lambda}^{2} \\mu) = 4\\sigma^2 \\mu^{\\top} (R_{\\lambda}^2)^{\\top} R_{\\lambda}^2 \\mu = 4\\sigma^2 \\mu^{\\top} R_{\\lambda}^4 \\mu = 4\\sigma^2 \\|R_{\\lambda}^{2} \\mu\\|_{2}^{2}\n$$\nThe variance of the quadratic form $\\varepsilon^{\\top}M\\varepsilon$ where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ and $M$ is symmetric is given by the formula $2\\operatorname{tr}((M\\Sigma)^2)$, where $\\Sigma = \\sigma^2 I_m$.\n$$\n\\operatorname{Var}[\\varepsilon^{\\top}M\\varepsilon] = 2\\operatorname{tr}((R_{\\lambda}^{2} \\sigma^2 I_m)^2) = 2\\sigma^4 \\operatorname{tr}( (R_{\\lambda}^{2})^2 ) = 2\\sigma^4 \\operatorname{tr}(R_{\\lambda}^{4})\n$$\nNote that for a symmetric matrix $B$, $\\operatorname{tr}(B^2) = \\|B\\|_F^2$. So $\\operatorname{tr}(R_{\\lambda}^4) = \\|R_{\\lambda}^2\\|_F^2$.\nCombining these results, the variance contribution from the measurement noise is:\n$$\n\\operatorname{Var}[V_y] = 4\\sigma^2 \\|R_{\\lambda}^{2} \\mu\\|_{2}^{2} + 2\\sigma^4 \\operatorname{tr}(R_{\\lambda}^{4})\n$$\n\n**2. Variance from Monte Carlo Estimation, $\\operatorname{Var}[V_z]$**\n\nThe term $V_z$ is $V_z = 2 \\sigma^{2} \\widehat{\\operatorname{tr}}(S_{\\lambda}) = 2\\sigma^2 \\frac{1}{p} \\sum_{k=1}^{p} \\left(z^{(k)}\\right)^{\\top} S_{\\lambda} z^{(k)}$.\nThe probe vectors $z^{(k)}$ are i.i.d. random variables with $z^{(k)} \\sim \\mathcal{N}(0, I_m)$. Let $W_k = \\left(z^{(k)}\\right)^{\\top} S_{\\lambda} z^{(k)}$. The $W_k$ are i.i.d. random variables.\n$$\n\\operatorname{Var}[V_z] = \\operatorname{Var}\\left[\\frac{2\\sigma^2}{p} \\sum_{k=1}^{p} W_k\\right] = \\left(\\frac{2\\sigma^2}{p}\\right)^2 \\operatorname{Var}\\left[\\sum_{k=1}^{p} W_k\\right]\n$$\nDue to independence, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}[V_z] = \\frac{4\\sigma^4}{p^2} \\sum_{k=1}^{p} \\operatorname{Var}[W_k] = \\frac{4\\sigma^4}{p^2} (p \\operatorname{Var}[W_1]) = \\frac{4\\sigma^4}{p} \\operatorname{Var}[W_1]\n$$\nWe compute the variance of a single quadratic form $W_1 = z^{\\top} S_{\\lambda} z$ with $z \\sim \\mathcal{N}(0, I_m)$. Using the same formula for the variance of a quadratic form, with $M=S_{\\lambda}$ (which is symmetric) and $\\Sigma=I_m$:\n$$\n\\operatorname{Var}[W_1] = 2\\operatorname{tr}((S_{\\lambda} I_m)^2) = 2\\operatorname{tr}(S_{\\lambda}^2)\n$$\nSubstituting this back, we get the variance contribution from randomization:\n$$\n\\operatorname{Var}[V_z] = \\frac{4\\sigma^4}{p} (2\\operatorname{tr}(S_{\\lambda}^2)) = \\frac{8\\sigma^4}{p} \\operatorname{tr}(S_{\\lambda}^2)\n$$\n\n**3. Total Variance and Final Expression**\n\nSumming the two components gives the total variance of $\\widehat{U}_{\\mathrm{MC}}(\\lambda)$:\n$$\n\\operatorname{Var}\\!\\left[\\widehat{U}_{\\mathrm{MC}}(\\lambda)\\right] = \\operatorname{Var}[V_y] + \\operatorname{Var}[V_z] = 4\\sigma^2 \\|R_{\\lambda}^{2} \\mu\\|_{2}^{2} + 2\\sigma^4 \\operatorname{tr}(R_{\\lambda}^{4}) + \\frac{8\\sigma^4}{p} \\operatorname{tr}(S_{\\lambda}^2)\n$$\nThis expression decomposes the variance into a term depending on the measurement noise $\\varepsilon$ (the first two terms) and a term depending on the Monte Carlo sampling for the trace estimation (the third term).\n\n**4. Quantitative Criterion for Reliability**\n\nThe goal of minimizing $\\widehat{U}_{\\mathrm{MC}}(\\lambda)$ is to find a $\\lambda$ that approximates the minimizer of the true predictive risk, $U(\\lambda) = \\mathbb{E}_{\\varepsilon}[\\|S_{\\lambda}y - \\mu\\|_2^2] = \\|R_{\\lambda}\\mu\\|_2^2 + \\sigma^2 \\operatorname{tr}(S_{\\lambda}^2)$. The estimator $\\widehat{U}_{\\mathrm{MC}}(\\lambda)$ is an unbiased estimator of $U(\\lambda)$.\nThe reliability of a $\\lambda_{\\mathrm{MC}}$ chosen by minimizing $\\widehat{U}_{\\mathrm{MC}}(\\lambda)$ depends on how much the random function $\\widehat{U}_{\\mathrm{MC}}(\\cdot)$ fluctuates around its mean $U(\\cdot)$. A large variance implies that the minimizer of the random function could be far from the minimizer of its mean.\nA natural quantitative measure for this is the coefficient of variation (CV) of the estimator, defined as the ratio of its standard deviation to its mean:\n$$\n\\text{CV}(\\lambda) = \\frac{\\sqrt{\\operatorname{Var}\\!\\left[\\widehat{U}_{\\mathrm{MC}}(\\lambda)\\right]}}{\\mathbb{E}\\!\\left[\\widehat{U}_{\\mathrm{MC}}(\\lambda)\\right]} = \\frac{\\sqrt{\\operatorname{Var}\\!\\left[\\widehat{U}_{\\mathrm{MC}}(\\lambda)\\right]}}{U(\\lambda)}\n$$\nA small CV at the chosen $\\lambda_{\\mathrm{MC}}$ suggests the estimator is stable and a reliable proxy for the true risk.\n\nA practical criterion must be computable from data, meaning it cannot depend on the unknown $\\mu = Ax_{\\star}$. We must therefore estimate the terms in the variance and mean expressions.\nAn unbiased estimator for $\\|R_{\\lambda}^2 \\mu\\|_2^2$ can be constructed. We have $\\mathbb{E}[\\|R_{\\lambda}^2 y\\|_2^2] = \\mathbb{E}[\\|R_{\\lambda}^2(\\mu+\\varepsilon)\\|_2^2] = \\|R_{\\lambda}^2\\mu\\|_2^2 + \\mathbb{E}[\\varepsilon^{\\top}R_{\\lambda}^4\\varepsilon] = \\|R_{\\lambda}^2\\mu\\|_2^2 + \\sigma^2 \\operatorname{tr}(R_{\\lambda}^4)$.\nThus, an unbiased estimator for $\\|R_{\\lambda}^2 \\mu\\|_2^2$ is $\\|R_{\\lambda}^2 y\\|_2^2 - \\sigma^2 \\operatorname{tr}(R_{\\lambda}^4)$.\nSubstituting this into our variance expression gives an unbiased estimator for the variance:\n$$\n\\widehat{\\operatorname{Var}}\\!\\left[\\widehat{U}_{\\mathrm{MC}}(\\lambda)\\right] = 4\\sigma^2 \\left( \\|R_{\\lambda}^{2} y\\|_{2}^{2} - \\sigma^2 \\operatorname{tr}(R_{\\lambda}^{4}) \\right) + 2\\sigma^4 \\operatorname{tr}(R_{\\lambda}^{4}) + \\frac{8\\sigma^4}{p} \\operatorname{tr}(S_{\\lambda}^2)\n$$\n$$\n\\widehat{\\operatorname{Var}}\\!\\left[\\widehat{U}_{\\mathrm{MC}}(\\lambda)\\right] = 4\\sigma^2 \\|R_{\\lambda}^{2} y\\|_{2}^{2} - 2\\sigma^4 \\operatorname{tr}(R_{\\lambda}^{4}) + \\frac{8\\sigma^4}{p} \\operatorname{tr}(S_{\\lambda}^2)\n$$\nThe mean $U(\\lambda)$ is estimated by $\\widehat{U}_{\\mathrm{MC}}(\\lambda)$ itself. Thus, we can compute an estimated coefficient of variation $\\widehat{\\text{CV}}(\\lambda)$.\n\n**Criterion:** Let $\\lambda_{\\mathrm{MC}} = \\operatorname{argmin}_{\\lambda} \\widehat{U}_{\\mathrm{MC}}(\\lambda)$. The choice $\\lambda_{\\mathrm{MC}}$ is deemed reliable if the estimated coefficient of variation at this value is small, i.e.,\n$$\n\\widehat{\\text{CV}}(\\lambda_{\\mathrm{MC}}) = \\frac{\\sqrt{\\max(0, \\widehat{\\operatorname{Var}}\\!\\left[\\widehat{U}_{\\mathrm{MC}}(\\lambda_{\\mathrm{MC}})\\right])}}{|\\widehat{U}_{\\mathrm{MC}}(\\lambda_{\\mathrm{MC}})|} < C\n$$\nfor some user-defined threshold $C$, for instance, $C = 0.1$. The $\\max(0, \\cdot)$ operation ensures the numerator is real, as the unbiased variance estimator can be negative. This criterion is fully computable from the data $y$, the noise level $\\sigma^2$, and the problem-defining matrices $A$ and $L$.\nThe rationale is that a small relative standard deviation implies that the value of the empirical risk curve at its minimum is a precise estimate of the true risk, lending confidence to the location of the minimum itself.", "answer": "$$\n\\boxed{4 \\sigma^2 \\|R_{\\lambda}^2 \\mu\\|_{2}^{2} + 2 \\sigma^4 \\operatorname{tr}\\!\\left(R_{\\lambda}^4\\right) + \\frac{8 \\sigma^4}{p} \\operatorname{tr}\\!\\left(S_{\\lambda}^2\\right)}\n$$", "id": "3429085"}]}