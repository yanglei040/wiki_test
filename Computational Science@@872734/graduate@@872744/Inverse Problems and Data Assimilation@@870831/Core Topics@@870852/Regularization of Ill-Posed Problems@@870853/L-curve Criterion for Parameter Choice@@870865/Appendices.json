{"hands_on_practices": [{"introduction": "Our exploration of the L-curve begins with its foundational building block: the Tikhonov-regularized solution itself. This first exercise guides you through deriving the explicit form of the solution, $x_{\\lambda}$, from the optimality conditions. By confirming that the solution is a smooth function of the regularization parameter $\\lambda$, we establish the mathematical groundwork that ensures the L-curve is a well-defined, continuous path, a necessary condition for any analysis based on its geometry. [@problem_id:3394293]", "problem": "Consider the Tikhonov-regularized least-squares problem in linear inverse problems, which seeks $x_{\\lambda}$ as the minimizer of the functional $J_{\\lambda}(x) = \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|L x\\|_{2}^{2}$, where $A \\in \\mathbb{R}^{m \\times n}$ with full column rank, $b \\in \\mathbb{R}^{m}$, $\\lambda > 0$ is the regularization parameter, and $L \\in \\mathbb{R}^{p \\times n}$ is a regularization operator. The necessary optimality condition yields the normal equations $(A^\\top A + \\lambda^{2} L^\\top L) x_{\\lambda} = A^\\top b$. In the standard case $L = I$, the L-curve criterion for parameter choice analyzes the parametric curve $\\left(\\ln \\|A x_{\\lambda} - b\\|_{2}, \\ln \\|x_{\\lambda}\\|_{2}\\right)$ as a function of $\\lambda$, which requires that $x_{\\lambda}$ depend smoothly on $\\lambda$.\n\n(a) Starting from the optimality condition above, and under the assumptions that $L = I$ and $A$ has full column rank, derive the expression for $x_{\\lambda}$ in terms of $A$, $b$, and $\\lambda$. Justify that the mapping $\\lambda \\mapsto x_{\\lambda}$ is smooth for all real $\\lambda$.\n\n(b) Using the result of part (a), consider the specific full-column-rank matrix $A \\in \\mathbb{R}^{3 \\times 2}$ and vector $b \\in \\mathbb{R}^{3}$ given by\n$$\nA = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1\n\\end{pmatrix}, \n\\qquad\nb = \\begin{pmatrix}\n1 \\\\\n2 \\\\\n0\n\\end{pmatrix}.\n$$\nCompute $x_{\\lambda}$ explicitly as a closed-form function of $\\lambda$ for this $(A,b)$.\n\nYour final answer should be the expression for $x_{\\lambda}$ obtained in part (b), written as a single row using the `pmatrix` environment. No numerical approximation is required, and no units are involved.", "solution": "The problem statement is evaluated as valid. It is a well-posed problem in numerical linear algebra and inverse problems, grounded in established mathematical principles. The problem is self-contained, with all necessary data and definitions provided. The given matrices and vectors are consistent, and the assumptions (e.g., $A$ having full column rank) are appropriate for ensuring a unique, meaningful solution.\n\n(a) We begin with the Tikhonov-regularized normal equations for the specified case where the regularization operator is the identity matrix, $L = I$. The functional to be minimized is $J_{\\lambda}(x) = \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}$. The necessary optimality condition $\\nabla_x J_{\\lambda}(x) = 0$ yields the normal equations:\n$$\n(A^\\top A + \\lambda^{2} I) x_{\\lambda} = A^\\top b\n$$\nHere, $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $x_{\\lambda} \\in \\mathbb{R}^{n}$, and $I$ is the $n \\times n$ identity matrix.\n\nTo derive an expression for $x_{\\lambda}$, we must show that the matrix $M(\\lambda) = A^\\top A + \\lambda^{2} I$ is invertible. We are given that $A$ has full column rank. This means that its columns are linearly independent, and for any non-zero vector $v \\in \\mathbb{R}^{n}$, the product $Av$ is a non-zero vector in $\\mathbb{R}^{m}$. Consequently, the squared Euclidean norm $\\|Av\\|_{2}^{2} = v^\\top A^\\top A v$ is strictly positive for any $v \\neq 0$. This is the definition of $A^\\top A$ being a symmetric positive definite (SPD) matrix.\n\nNow, consider the matrix $M(\\lambda)$ for any real $\\lambda$. For any non-zero vector $v \\in \\mathbb{R}^{n}$, the quadratic form is:\n$$\nv^\\top M(\\lambda) v = v^\\top (A^\\top A + \\lambda^{2} I) v = v^\\top A^\\top A v + \\lambda^{2} v^\\top I v = \\|Av\\|_{2}^{2} + \\lambda^{2} \\|v\\|_{2}^{2}\n$$\nSince $A$ has full column rank, $\\|Av\\|_{2}^{2} > 0$. Since $v \\neq 0$, $\\|v\\|_{2}^{2} > 0$. For any real $\\lambda$, $\\lambda^2 \\ge 0$. Therefore, the sum $\\|Av\\|_{2}^{2} + \\lambda^{2} \\|v\\|_{2}^{2}$ is strictly positive. This shows that $M(\\lambda)$ is positive definite for all real $\\lambda$. A positive definite matrix is always invertible.\n\nSince $A^\\top A + \\lambda^2 I$ is invertible for all real $\\lambda$, we can uniquely solve for $x_{\\lambda}$:\n$$\nx_{\\lambda} = (A^\\top A + \\lambda^2 I)^{-1} A^\\top b\n$$\nTo justify that the mapping $\\lambda \\mapsto x_{\\lambda}$ is smooth for all real $\\lambda$, we examine the structure of $x_{\\lambda}(\\lambda)$. The vector $x_\\lambda$ is the product of the matrix $(A^\\top A + \\lambda^2 I)^{-1}$ and the constant vector $A^\\top b$. The entries of the matrix $A^\\top A + \\lambda^2 I$ are polynomial functions of $\\lambda$ (at most of degree $2$).\n\nThe inverse of a matrix $C$ can be expressed as $C^{-1} = \\frac{1}{\\det(C)} \\text{adj}(C)$, where $\\text{adj}(C)$ is the adjugate matrix.\n1. The determinant, $\\det(A^\\top A + \\lambda^2 I)$, is a polynomial in $\\lambda$. As we have shown that $A^\\top A + \\lambda^2 I$ is positive definite for all real $\\lambda$, all its eigenvalues are positive. The determinant, being the product of eigenvalues, is therefore strictly positive for all real $\\lambda$. Thus, the polynomial $\\det(A^\\top A + \\lambda^2 I)$ has no real roots.\n2. The entries of the adjugate matrix, $\\text{adj}(A^\\top A + \\lambda^2 I)$, are cofactors, which are determinants of submatrices. Since the entries of $A^\\top A + \\lambda^2 I$ are polynomials in $\\lambda$, the entries of the adjugate are also polynomials in $\\lambda$.\n\nEach entry of the inverse matrix $(A^\\top A + \\lambda^2 I)^{-1}$ is a ratio of a polynomial in $\\lambda$ to another polynomial in $\\lambda$ (the determinant). Since the denominator polynomial has no real roots, each entry is a rational function that is defined and infinitely differentiable (i.e., smooth) for all real $\\lambda$.\n\nThe vector $x_{\\lambda}$ is obtained by multiplying this matrix of smooth functions by the constant vector $A^\\top b$. Each component of $x_{\\lambda}$ is a linear combination of these smooth functions, and is therefore itself a smooth function of $\\lambda$. Thus, the mapping $\\lambda \\mapsto x_{\\lambda}$ is smooth for all real $\\lambda$.\n\n(b) We are given the specific matrix $A \\in \\mathbb{R}^{3 \\times 2}$ and vector $b \\in \\mathbb{R}^{3}$:\n$$\nA = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1\n\\end{pmatrix}, \n\\qquad\nb = \\begin{pmatrix}\n1 \\\\\n2 \\\\\n0\n\\end{pmatrix}\n$$\nThe columns of $A$ are clearly linearly independent, so $A$ has full column rank. First, we compute $A^\\top A$ and $A^\\top b$.\n$$\nA^\\top = \\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n$$\n$$\nA^\\top A = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1(1)+0(0)+1(1) & 1(0)+0(1)+1(1) \\\\ 0(1)+1(0)+1(1) & 0(0)+1(1)+1(1) \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\n$$\nA^\\top b = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1(1)+0(2)+1(0) \\\\ 0(1)+1(2)+1(0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\nNext, we form the matrix $M(\\lambda) = A^\\top A + \\lambda^2 I$:\n$$\nM(\\lambda) = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} + \\lambda^2 \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2+\\lambda^2 & 1 \\\\ 1 & 2+\\lambda^2 \\end{pmatrix}\n$$\nWe find the inverse of this matrix. Its determinant is:\n$$\n\\det(M(\\lambda)) = (2+\\lambda^2)^2 - 1^2 = 4 + 4\\lambda^2 + \\lambda^4 - 1 = \\lambda^4 + 4\\lambda^2 + 3\n$$\nThis can be factored as $(\\lambda^2+1)(\\lambda^2+3)$. The inverse matrix is:\n$$\nM(\\lambda)^{-1} = \\frac{1}{(\\lambda^2+1)(\\lambda^2+3)} \\begin{pmatrix} 2+\\lambda^2 & -1 \\\\ -1 & 2+\\lambda^2 \\end{pmatrix}\n$$\nFinally, we compute $x_{\\lambda} = M(\\lambda)^{-1} (A^\\top b)$:\n$$\nx_{\\lambda} = \\frac{1}{(\\lambda^2+1)(\\lambda^2+3)} \\begin{pmatrix} 2+\\lambda^2 & -1 \\\\ -1 & 2+\\lambda^2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\n$$\nx_{\\lambda} = \\frac{1}{(\\lambda^2+1)(\\lambda^2+3)} \\begin{pmatrix} 1(2+\\lambda^2) - 1(2) \\\\ -1(1) + 2(2+\\lambda^2) \\end{pmatrix}\n$$\n$$\nx_{\\lambda} = \\frac{1}{(\\lambda^2+1)(\\lambda^2+3)} \\begin{pmatrix} 2+\\lambda^2 - 2 \\\\ -1 + 4 + 2\\lambda^2 \\end{pmatrix}\n$$\n$$\nx_{\\lambda} = \\frac{1}{(\\lambda^2+1)(\\lambda^2+3)} \\begin{pmatrix} \\lambda^2 \\\\ 2\\lambda^2+3 \\end{pmatrix}\n$$\nThis gives the closed-form expression for the components of $x_{\\lambda}$:\n$$\nx_{\\lambda} = \\begin{pmatrix} \\frac{\\lambda^2}{(\\lambda^2+1)(\\lambda^2+3)} \\\\ \\frac{2\\lambda^2+3}{(\\lambda^2+1)(\\lambda^2+3)} \\end{pmatrix}\n$$\nThis is the required explicit expression for $x_{\\lambda}$ as a function of $\\lambda$. The final answer should be presented as a row matrix containing these two components.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\lambda^2}{(\\lambda^2+1)(\\lambda^2+3)} & \\frac{2\\lambda^2+3}{(\\lambda^2+1)(\\lambda^2+3)} \\end{pmatrix}}\n$$", "id": "3394293"}, {"introduction": "To gain a deeper intuition for the L-curve's characteristic shape, we turn to the Singular Value Decomposition (SVD), the most powerful analytical tool for linear inverse problems. This practice involves re-casting the residual and solution norms—the very axes of the L-curve—in terms of the singular value spectrum. This perspective reveals precisely how Tikhonov regularization acts as a spectral filter and provides a clear rationale for the trade-off that the L-curve visualizes. [@problem_id:3394301]", "problem": "Consider the linear inverse problem $A x = b$ with $A \\in \\mathbb{R}^{m \\times n}$, and the zero-order Tikhonov-regularized solution\n$$\nx_{\\lambda} \\in \\arg \\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2} \\right\\},\n$$\nwhere $\\lambda > 0$ is the regularization parameter. Let the Singular Value Decomposition (SVD) of $A$ be $A = U \\Sigma V^{\\top}$ with singular values $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r} > 0$ on the diagonal of $\\Sigma \\in \\mathbb{R}^{m \\times n}$, left singular vectors $\\{u_{i}\\}_{i=1}^{m}$ (columns of $U$), and right singular vectors $\\{v_{i}\\}_{i=1}^{n}$ (columns of $V$). Decompose the data as $b = \\sum_{i=1}^{r} (u_{i}^{\\top} b) u_{i} + b_{\\perp}$, where $b_{\\perp}$ is the component orthogonal to the range of $A$. Define the residual norm $r(\\lambda) := \\|A x_{\\lambda} - b\\|_{2}$ and the solution seminorm $p(\\lambda) := \\|x_{\\lambda}\\|_{2}$.\n\nTasks:\n1) Starting from the definition of $x_{\\lambda}$ and the SVD of $A$, derive the representations\n$$\nr(\\lambda)^{2} \\;=\\; \\sum_{i=1}^{r} \\left(\\frac{\\lambda^{2}}{\\sigma_{i}^{2} + \\lambda^{2}}\\right)^{2} (u_{i}^{\\top} b)^{2} \\;+\\; \\|b_{\\perp}\\|_{2}^{2},\n\\quad\np(\\lambda)^{2} \\;=\\; \\sum_{i=1}^{r} \\frac{\\sigma_i^2 (u_i^{\\top}b)^2}{(\\sigma_i^2 + \\lambda^2)^2}.\n$$\n2) Analyze how $r(\\lambda)$ and $p(\\lambda)$ depend on $\\lambda$ and on the decay rate of the data coefficients $(u_{i}^{\\top} b)$ relative to the singular values $\\sigma_{i}$, including their limits as $\\lambda \\to 0^{+}$ and $\\lambda \\to \\infty$.\n3) Define the L-curve as the parametric curve $L(\\lambda) = \\big(\\ln r(\\lambda), \\ln p(\\lambda)\\big)$. The curvature of a planar parametric curve $(x(\\lambda), y(\\lambda))$ is given by\n$$\n\\kappa(\\lambda) \\;=\\; \\frac{|x'(\\lambda)\\, y''(\\lambda) \\;-\\; y'(\\lambda)\\, x''(\\lambda)|}{\\big(x'(\\lambda)^{2} + y'(\\lambda)^{2}\\big)^{3/2}}.\n$$\nFor the specific $3$-dimensional problem with singular values $\\sigma_{1} = 10$, $\\sigma_{2} = 1$, $\\sigma_{3} = 0.1$, data coefficients $u_{1}^{\\top} b = 1$, $u_{2}^{\\top} b = 0.01$, $u_{3}^{\\top} b = 0.0001$, and $b_{\\perp} = 0$, compute the curvature $\\kappa(1)$ of the L-curve at $\\lambda = 1$. Round your final answer to $4$ significant figures. The final answer is unitless and must be given as a single real number.", "solution": "The problem requires a three-part analysis of the Tikhonov-regularized solution $x_{\\lambda}$ to an inverse problem. First, we derive expressions for the residual and solution norms. Second, we analyze their behavior. Third, we compute the curvature of the L-curve for a specific case.\n\n**Part 1: Derivation of $r(\\lambda)^2$ and $p(\\lambda)^2$**\n\nThe Tikhonov-regularized solution $x_{\\lambda}$ minimizes the functional $J(x) = \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}$. The minimum is found by setting the gradient of $J(x)$ with respect to $x$ to zero:\n$$\n\\nabla_{x} J(x) = 2 A^{\\top}(A x - b) + 2 \\lambda^{2} x = 0.\n$$\nThis leads to the normal equations:\n$$\n(A^{\\top}A + \\lambda^{2} I) x_{\\lambda} = A^{\\top}b.\n$$\nThe solution is thus $x_{\\lambda} = (A^{\\top}A + \\lambda^{2} I)^{-1} A^{\\top}b$. We substitute the Singular Value Decomposition (SVD) $A = U \\Sigma V^{\\top}$.\n$A^{\\top}A = (V \\Sigma^{\\top} U^{\\top})(U \\Sigma V^{\\top}) = V \\Sigma^{\\top}\\Sigma V^{\\top}$. The matrix $\\Sigma^{\\top}\\Sigma$ is a diagonal $n \\times n$ matrix with entries $\\sigma_i^2$ for $i=1, \\dots, r$ and zeros elsewhere. We denote this as $\\Sigma_n^2$.\nSo, $A^{\\top}A = V \\Sigma_n^2 V^{\\top}$.\nAlso, $A^{\\top}b = V \\Sigma^{\\top} U^{\\top}b$.\nSubstituting these into the expression for $x_{\\lambda}$:\n$$\nx_{\\lambda} = (V \\Sigma_n^2 V^{\\top} + \\lambda^{2} V V^{\\top})^{-1} V \\Sigma^{\\top} U^{\\top}b = (V(\\Sigma_n^2 + \\lambda^{2} I)V^{\\top})^{-1} V \\Sigma^{\\top} U^{\\top}b.\n$$\nUsing the property $(XYZ)^{-1} = Z^{-1}Y^{-1}X^{-1}$ and $V^{-1}=V^{\\top}$, we get:\n$$\nx_{\\lambda} = V(\\Sigma_n^2 + \\lambda^{2} I)^{-1}V^{\\top} V \\Sigma^{\\top} U^{\\top}b = V (\\Sigma_n^2 + \\lambda^{2} I)^{-1} \\Sigma^{\\top} U^{\\top}b.\n$$\nThe matrix $(\\Sigma_n^2 + \\lambda^{2} I)^{-1} \\Sigma^{\\top}$ is an $n \\times m$ matrix. Its non-zero elements are on the diagonal, with values $\\frac{\\sigma_i}{\\sigma_i^2 + \\lambda^2}$ for $i=1, \\dots, r$.\nLet $\\hat{b}_i = u_i^{\\top}b$. The vector $U^{\\top}b$ contains these coefficients.\nThe action of $(\\Sigma_n^2 + \\lambda^2 I)^{-1} \\Sigma^{\\top}$ on $U^{\\top}b$ produces a vector in $\\mathbb{R}^n$ whose $i$-th component is $\\frac{\\sigma_i \\hat{b}_i}{\\sigma_i^2 + \\lambda^2}$ for $i=1, \\dots, r$ and zero otherwise.\nMultiplying by $V$ expresses this vector in the basis of right singular vectors $\\{v_i\\}$:\n$$\nx_{\\lambda} = \\sum_{i=1}^{r} \\frac{\\sigma_i (u_i^{\\top}b)}{\\sigma_i^2 + \\lambda^2} v_i.\n$$\nNow we compute the solution seminorm $p(\\lambda) = \\|x_{\\lambda}\\|_2$. Since the vectors $\\{v_i\\}$ are orthonormal, the squared norm is the sum of squares of the coefficients:\n$$\np(\\lambda)^2 = \\|x_{\\lambda}\\|_2^2 = \\sum_{i=1}^{r} \\left(\\frac{\\sigma_i (u_i^{\\top}b)}{\\sigma_i^2 + \\lambda^2}\\right)^2 = \\sum_{i=1}^{r} \\frac{\\sigma_i^2 (u_i^{\\top}b)^2}{(\\sigma_i^2 + \\lambda^2)^2}.\n$$\nThis confirms the second formula.\n\nNext, we compute the residual norm $r(\\lambda) = \\|A x_{\\lambda} - b\\|_2$.\nFirst, find $A x_{\\lambda}$:\n$$\nA x_{\\lambda} = (U \\Sigma V^{\\top}) \\left( \\sum_{j=1}^{r} \\frac{\\sigma_j (u_j^{\\top}b)}{\\sigma_j^2 + \\lambda^2} v_j \\right) = \\sum_{j=1}^{r} \\frac{\\sigma_j (u_j^{\\top}b)}{\\sigma_j^2 + \\lambda^2} (U \\Sigma V^{\\top} v_j).\n$$\nSince $V^{\\top}v_j=e_j$ (the $j$-th standard basis vector) and $\\Sigma e_j = \\sigma_j e_j$, we have $U\\Sigma V^{\\top}v_j = U (\\sigma_j e_j) = \\sigma_j u_j$.\n$$\nA x_{\\lambda} = \\sum_{j=1}^{r} \\frac{\\sigma_j^2 (u_j^{\\top}b)}{\\sigma_j^2 + \\lambda^2} u_j.\n$$\nThe residual vector is $A x_{\\lambda} - b$. Using the given decomposition $b = \\sum_{i=1}^{r} (u_i^{\\top}b)u_i + b_{\\perp}$:\n$$\nA x_{\\lambda} - b = \\sum_{i=1}^{r} \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} - 1\\right) (u_i^{\\top}b) u_i - b_{\\perp} = \\sum_{i=1}^{r} \\left(\\frac{-\\lambda^2}{\\sigma_i^2 + \\lambda^2}\\right) (u_i^{\\top}b) u_i - b_{\\perp}.\n$$\nThe vectors $\\{u_i\\}$ are orthonormal and are orthogonal to $b_{\\perp}$. Therefore, the squared norm is:\n$$\nr(\\lambda)^2 = \\|A x_{\\lambda} - b\\|_2^2 = \\sum_{i=1}^{r} \\left(\\frac{-\\lambda^2}{\\sigma_i^2 + \\lambda^2}\\right)^2 (u_i^{\\top}b)^2 + \\|b_{\\perp}\\|_2^2 = \\sum_{i=1}^{r} \\left(\\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2}\\right)^2 (u_i^{\\top}b)^2 + \\|b_{\\perp}\\|_2^2.\n$$\nThis confirms the first formula.\n\n**Part 2: Analysis of $r(\\lambda)$ and $p(\\lambda)$**\n\nThe functions $p(\\lambda)$ and $r(\\lambda)$ exhibit monotonic behavior with respect to $\\lambda > 0$.\nThe term $\\frac{\\sigma_i^2}{\\sigma_i^2+\\lambda^2}$ is a decreasing function of $\\lambda$. As it appears in the sum for $p(\\lambda)^2$, $p(\\lambda)$ must be a strictly decreasing function of $\\lambda$.\nThe term $\\frac{\\lambda^2}{\\sigma_i^2+\\lambda^2} = 1 - \\frac{\\sigma_i^2}{\\sigma_i^2+\\lambda^2}$ is an increasing function of $\\lambda$. As it appears squared in the sum for $r(\\lambda)^2$, $r(\\lambda)$ must be a strictly increasing function of $\\lambda$.\n\nLimits as $\\lambda \\to 0^{+}$:\nAs $\\lambda \\to 0^{+}$, $\\frac{\\sigma_i^2}{\\sigma_i^2+\\lambda^2} \\to 1$. Thus, $p(\\lambda)^2 \\to \\sum_{i=1}^{r} \\frac{(u_i^{\\top}b)^2}{\\sigma_i^2}$. This is the squared norm of the Moore-Penrose pseudoinverse solution $x^{\\dagger} = \\sum_{i=1}^r \\frac{u_i^{\\top}b}{\\sigma_i}v_i$. If the discrete Picard condition is not satisfied (i.e., $|u_i^{\\top}b|$ decays slower than $\\sigma_i$), this norm can be very large or infinite in the continuous case.\nAs $\\lambda \\to 0^{+}$, $\\frac{\\lambda^2}{\\sigma_i^2+\\lambda^2} \\to 0$. Thus, $r(\\lambda)^2 \\to \\|b_{\\perp}\\|_2^2$. This is the norm of the data component orthogonal to the range of $A$, representing the minimum possible residual (the least-squares residual).\n\nLimits as $\\lambda \\to \\infty$:\nAs $\\lambda \\to \\infty$, $\\frac{\\sigma_i^2}{\\sigma_i^2+\\lambda^2} \\sim \\frac{\\sigma_i^2}{\\lambda^2} \\to 0$. Thus, $p(\\lambda)^2 \\to 0$. The solution is heavily damped and approaches the zero vector.\nAs $\\lambda \\to \\infty$, $\\frac{\\lambda^2}{\\sigma_i^2+\\lambda^2} \\to 1$. Thus, $r(\\lambda)^2 \\to \\sum_{i=1}^r(u_i^{\\top}b)^2 + \\|b_{\\perp}\\|_2^2 = \\|b\\|_2^2$. As $x_\\lambda \\to 0$, the residual $Ax_\\lambda-b$ approaches $-b$, so its norm approaches $\\|b\\|_2$.\n\nThe L-curve, which plots $\\ln p(\\lambda)$ against $\\ln r(\\lambda)$, is typically L-shaped due to this behavior. Small $\\lambda$ values correspond to the vertical arm (small residual, large solution norm), while large $\\lambda$ values correspond to the horizontal arm (large residual, small solution norm).\n\n**Part 3: Curvature Calculation**\n\nWe are given the L-curve $L(\\lambda) = (x(\\lambda), y(\\lambda)) = (\\ln r(\\lambda), \\ln p(\\lambda))$ and asked to compute its curvature $\\kappa(1)$ for the specific case.\nIt is more convenient to work with the squared norms $\\rho(\\lambda) = r(\\lambda)^2$ and $\\pi(\\lambda) = p(\\lambda)^2$. The curve coordinates are then $x(\\lambda) = \\frac{1}{2}\\ln \\rho(\\lambda)$ and $y(\\lambda) = \\frac{1}{2}\\ln \\pi(\\lambda)$.\nWe need the first and second derivatives of $x(\\lambda)$ and $y(\\lambda)$ evaluated at $\\lambda = 1$.\n$x'(\\lambda) = \\frac{\\rho'(\\lambda)}{2\\rho(\\lambda)}$, $x''(\\lambda) = \\frac{\\rho''(\\lambda)\\rho(\\lambda) - (\\rho'(\\lambda))^2}{2\\rho(\\lambda)^2}$.\n$y'(\\lambda) = \\frac{\\pi'(\\lambda)}{2\\pi(\\lambda)}$, $y''(\\lambda) = \\frac{\\pi''(\\lambda)\\pi(\\lambda) - (\\pi'(\\lambda))^2}{2\\pi(\\lambda)^2}$.\n\nThe problem specifies $\\sigma_1 = 10, \\sigma_2 = 1, \\sigma_3 = 0.1$, and $b_{\\perp}=0$. The data coefficients squared are $\\beta_1 = (u_1^{\\top}b)^2=1^2=1$, $\\beta_2 = (u_2^{\\top}b)^2=(0.01)^2=10^{-4}$, $\\beta_3 = (u_3^{\\top}b)^2=(0.0001)^2=10^{-8}$.\nThe functions are:\n$\\rho(\\lambda) = \\sum_{i=1}^{3} \\left(\\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2}\\right)^{2} \\beta_i$, and $\\pi(\\lambda) = \\sum_{i=1}^{3} \\frac{\\sigma_i^2 \\beta_i}{(\\sigma_i^2 + \\lambda^2)^2}$.\nTheir derivatives with respect to $\\lambda$ are:\n$\\rho'(\\lambda) = 4\\lambda^3 \\sum_{i=1}^{3} \\frac{\\sigma_i^2 \\beta_i}{(\\sigma_i^2 + \\lambda^2)^3}$.\n$\\pi'(\\lambda) = -4\\lambda \\sum_{i=1}^{3} \\frac{\\sigma_i^2 \\beta_i}{(\\sigma_i^2 + \\lambda^2)^3}$.\nAt $\\lambda=1$, we have $\\rho'(1) = -\\pi'(1)$.\n$\\rho''(\\lambda)=12\\lambda^2 \\sum_{i=1}^{3} \\frac{\\beta_i \\sigma_i^2(\\sigma_i^2 - \\lambda^2)}{(\\sigma_i^2 + \\lambda^2)^4}$.\n$\\pi''(\\lambda) = \\sum_{i=1}^{3} \\frac{\\beta_i \\sigma_i^2 (20\\lambda^2 - 4\\sigma_i^2)}{(\\sigma_i^2 + \\lambda^2)^4}$.\n\nAt $\\lambda=1$:\n$\\rho(1) = (\\frac{1}{101})^2(1) + (\\frac{1}{2})^2(10^{-4}) + (\\frac{1}{1.01})^2(10^{-8}) \\approx 1.2304 \\times 10^{-4}$.\n$\\pi(1) = \\frac{100(1)}{101^2} + \\frac{1(10^{-4})}{2^2} + \\frac{0.01(10^{-8})}{1.01^2} \\approx 9.8280 \\times 10^{-3}$.\n$\\rho'(1) = 4(\\frac{100}{101^3} + \\frac{10^{-4}}{2^3} + \\frac{0.001}{1.01^3}) \\approx 4.3824 \\times 10^{-4}$.\nThus $\\pi'(1) \\approx -4.3824 \\times 10^{-4}$.\n$\\rho''(1) = 12(\\frac{100(99)}{101^4} + \\frac{1(10^{-4})(0)}{2^4} + \\frac{0.01(10^{-8})(-0.99)}{1.01^4}) \\approx 1.1416 \\times 10^{-3}$.\n$\\pi''(1) = \\frac{100(20-400)}{101^4} + \\frac{10^{-4}(20-4)}{2^4} + \\frac{0.01(10^{-8})(20-0.04)}{1.01^4} \\approx -2.6517 \\times 10^{-4}$.\n\nNow we compute the derivatives for the L-curve coordinates:\n$x'(1) = \\frac{\\rho'(1)}{2\\rho(1)} \\approx \\frac{4.3824 \\times 10^{-4}}{2(1.2304 \\times 10^{-4})} \\approx 1.7809$.\n$y'(1) = \\frac{\\pi'(1)}{2\\pi(1)} \\approx \\frac{-4.3824 \\times 10^{-4}}{2(9.8280 \\times 10^{-3})} \\approx -0.022293$.\n$x''(1) = \\frac{\\rho''(1)\\rho(1) - \\rho'(1)^2}{2\\rho(1)^2} \\approx \\frac{(1.1416 \\times 10^{-3})(1.2304 \\times 10^{-4}) - (4.3824 \\times 10^{-4})^2}{2(1.2304 \\times 10^{-4})^2} \\approx -1.7036$.\n$y''(1) = \\frac{\\pi''(1)\\pi(1) - \\pi'(1)^2}{2\\pi(1)^2} \\approx \\frac{(-2.6517 \\times 10^{-4})(9.8280 \\times 10^{-3}) - (-4.3824 \\times 10^{-4})^2}{2(9.8280 \\times 10^{-3})^2} \\approx -0.014484$.\n\nFinally, substitute these into the curvature formula $\\kappa(\\lambda) = \\frac{|x' y'' - y' x''|}{(x'^2 + y'^2)^{3/2}}$:\nNumerator:\n$|x'(1)y''(1) - y'(1)x''(1)| \\approx |(1.7809)(-0.014484) - (-0.022293)(-1.7036)|$\n$\\approx |-0.025795 - 0.037976| = |-0.063771| = 0.063771$.\nDenominator:\n$(x'(1)^2+y'(1)^2)^{3/2} \\approx ((1.7809)^2 + (-0.022293)^2)^{3/2}$\n$\\approx (3.1716 + 0.000497)^{3/2} = (3.1721)^{3/2} \\approx 5.6482$.\n\n$\\kappa(1) \\approx \\frac{0.063771}{5.6482} \\approx 0.0112905$.\nRounding to 4 significant figures, the curvature is $0.01129$.", "answer": "$$\\boxed{0.01129}$$", "id": "3394301"}, {"introduction": "A critical question for any practical method is its robustness to arbitrary choices, such as the scaling of variables or the units of measurement. This exercise investigates the fundamental invariance properties of the L-curve under such transformations. By analyzing how the curve's coordinates and curvature behave under scaling of the operator, data, and regularization term, you will uncover how the geometric shape of the L-curve remains fundamentally unchanged, a property that underscores the intrinsic nature of the trade-off it represents. [@problem_id:3394265]", "problem": "Consider the linear Tikhonov regularization problem in a finite-dimensional Euclidean space: for given matrices $A \\in \\mathbb{R}^{m \\times n}$ and $L \\in \\mathbb{R}^{p \\times n}$, and a data vector $b \\in \\mathbb{R}^{m}$, define for each regularization parameter $\\lambda > 0$ the Tikhonov solution $x_{\\lambda} \\in \\mathbb{R}^{n}$ as the unique minimizer of the strictly convex functional\n$$\nJ_{\\lambda}(x) \\equiv \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|L x\\|_{2}^{2},\n$$\nwhere it is assumed that $A^{\\top} A + \\lambda^{2} L^{\\top} L$ is invertible for all $\\lambda > 0$ (for instance, $\\mathrm{null}(A) \\cap \\mathrm{null}(L) = \\{0\\}$). The L-curve is the parametric curve $\\lambda \\mapsto \\big(\\xi(\\lambda), \\nu(\\lambda)\\big)$ on the plane, with coordinates\n$$\n\\xi(\\lambda) \\equiv \\ln\\big(\\rho(\\lambda)\\big), \\quad \\nu(\\lambda) \\equiv \\ln\\big(\\eta(\\lambda)\\big),\n$$\nwhere $\\rho(\\lambda) \\equiv \\|A x_{\\lambda} - b\\|_{2}$ and $\\eta(\\lambda) \\equiv \\|L x_{\\lambda}\\|_{2}$. The curvature of this plane curve, viewed as a function of the parameter $\\lambda$, is defined by the standard formula\n$$\n\\kappa(\\lambda) \\equiv \\frac{\\big|\\xi'(\\lambda)\\,\\nu''(\\lambda) - \\nu'(\\lambda)\\,\\xi''(\\lambda)\\big|}{\\big(\\big(\\xi'(\\lambda)\\big)^{2} + \\big(\\nu'(\\lambda)\\big)^{2}\\big)^{3/2}}.\n$$\nNow consider the scaled problem obtained by the transformations $A \\mapsto \\alpha A$, $b \\mapsto \\beta b$, $L \\mapsto \\gamma L$, with $\\alpha, \\beta, \\gamma \\in \\mathbb{R}_{>0}$. For the same value of the parameter $\\lambda > 0$, define $x^{\\alpha,\\beta,\\gamma}_{\\lambda}$ as the unique minimizer of\n$$\nJ^{\\alpha,\\beta,\\gamma}_{\\lambda}(x) \\equiv \\|\\alpha A x - \\beta b\\|_{2}^{2} + \\lambda^{2} \\|\\gamma L x\\|_{2}^{2},\n$$\nand define the corresponding L-curve coordinates and curvature by\n$$\n\\xi_{\\alpha,\\beta,\\gamma}(\\lambda) \\equiv \\ln\\big(\\rho_{\\alpha,\\beta,\\gamma}(\\lambda)\\big), \\quad \\nu_{\\alpha,\\beta,\\gamma}(\\lambda) \\equiv \\ln\\big(\\eta_{\\alpha,\\beta,\\gamma}(\\lambda)\\big), \\quad \\kappa_{\\alpha,\\beta,\\gamma}(\\lambda) \\equiv \\frac{\\big|\\xi'_{\\alpha,\\beta,\\gamma}(\\lambda)\\,\\nu''_{\\alpha,\\beta,\\gamma}(\\lambda) - \\nu'_{\\alpha,\\beta,\\gamma}(\\lambda)\\,\\xi''_{\\alpha,\\beta,\\gamma}(\\lambda)\\big|}{\\big(\\big(\\xi'_{\\alpha,\\beta,\\gamma}(\\lambda)\\big)^{2} + \\big(\\nu'_{\\alpha,\\beta,\\gamma}(\\lambda)\\big)^{2}\\big)^{3/2}},\n$$\nwhere $\\rho_{\\alpha,\\beta,\\gamma}(\\lambda) \\equiv \\|\\alpha A x^{\\alpha,\\beta,\\gamma}_{\\lambda} - \\beta b\\|_{2}$ and $\\eta_{\\alpha,\\beta,\\gamma}(\\lambda) \\equiv \\|\\gamma L x^{\\alpha,\\beta,\\gamma}_{\\lambda}\\|_{2}$.\n\nStarting from the definitions above and basic properties of linear systems and norms, derive the effect of the scaling parameters $\\alpha$, $\\beta$, and $\\gamma$ on the L-curve coordinates in the $(\\xi,\\nu)$-plane, and on the curvature as a function of $\\lambda$. In particular, show how the curvature of the scaled L-curve relates to the curvature of the unscaled L-curve through a reparameterization of $\\lambda$. Your final answer must be the analytical expression for $\\kappa_{\\alpha,\\beta,\\gamma}(\\lambda)$ in terms of $\\kappa(\\cdot)$, $\\alpha$, $\\beta$, and $\\gamma$. No numerical evaluation is required, and no rounding is needed.", "solution": "The problem is well-posed and concerns a fundamental property of the L-curve criterion for Tikhonov regularization. We may proceed with a formal derivation.\n\nThe unscaled Tikhonov problem seeks to minimize the functional\n$$J_{\\lambda}(x) = \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|L x\\|_{2}^{2}$$\nThe unique minimizer, $x_{\\lambda}$, satisfies the normal equations, which are obtained by setting the gradient of $J_{\\lambda}(x)$ with respect to $x$ to zero:\n$$\\nabla_{x} J_{\\lambda}(x) = 2 A^{\\top}(A x - b) + 2 \\lambda^{2} L^{\\top}L x = 0$$\nThis yields the linear system:\n$$(A^{\\top}A + \\lambda^{2} L^{\\top}L) x_{\\lambda} = A^{\\top}b$$\nThe solution is thus formally given by $x_{\\lambda} = (A^{\\top}A + \\lambda^{2} L^{\\top}L)^{-1} A^{\\top}b$. The invertibility of the matrix $A^{\\top}A + \\lambda^{2} L^{\\top}L$ for $\\lambda > 0$ is given as a premise.\n\nSimilarly, for the scaled problem, we consider the functional\n$$J^{\\alpha,\\beta,\\gamma}_{\\lambda}(x) = \\|\\alpha A x - \\beta b\\|_{2}^{2} + \\lambda^{2} \\|\\gamma L x\\|_{2}^{2}$$\nThe minimizer, $x^{\\alpha,\\beta,\\gamma}_{\\lambda}$, satisfies the corresponding normal equations:\n$$\\nabla_{x} J^{\\alpha,\\beta,\\gamma}_{\\lambda}(x) = 2 (\\alpha A)^{\\top}(\\alpha A x - \\beta b) + 2 (\\lambda \\gamma L)^{\\top}(\\lambda \\gamma L x) = 0$$\n$$\\alpha^{2} A^{\\top}(A x - (\\beta/\\alpha) b) + \\lambda^{2} \\gamma^{2} L^{\\top}L x = 0$$\nRearranging terms, we obtain the linear system for the scaled problem:\n$$(\\alpha^{2} A^{\\top}A + \\lambda^{2} \\gamma^{2} L^{\\top}L) x^{\\alpha,\\beta,\\gamma}_{\\lambda} = \\alpha \\beta A^{\\top}b$$\nTo relate this solution to the unscaled one, we manipulate the matrix on the left-hand side:\n$$\\alpha^{2} \\left( A^{\\top}A + \\frac{\\lambda^{2} \\gamma^{2}}{\\alpha^{2}} L^{\\top}L \\right) x^{\\alpha,\\beta,\\gamma}_{\\lambda} = \\alpha \\beta A^{\\top}b$$\n$$\\alpha^{2} \\left( A^{\\top}A + \\left(\\frac{\\lambda \\gamma}{\\alpha}\\right)^{2} L^{\\top}L \\right) x^{\\alpha,\\beta,\\gamma}_{\\lambda} = \\alpha \\beta A^{\\top}b$$\nLet us define a re-scaled regularization parameter $\\tilde{\\lambda}$ as a function of $\\lambda$:\n$$\\tilde{\\lambda} \\equiv \\frac{\\lambda \\gamma}{\\alpha}$$\nSubstituting $\\tilde{\\lambda}$ into the equation gives:\n$$\\alpha^{2} (A^{\\top}A + \\tilde{\\lambda}^{2} L^{\\top}L) x^{\\alpha,\\beta,\\gamma}_{\\lambda} = \\alpha \\beta A^{\\top}b$$\nSolving for $x^{\\alpha,\\beta,\\gamma}_{\\lambda}$:\n$$x^{\\alpha,\\beta,\\gamma}_{\\lambda} = \\frac{\\alpha \\beta}{\\alpha^{2}} (A^{\\top}A + \\tilde{\\lambda}^{2} L^{\\top}L)^{-1} A^{\\top}b = \\frac{\\beta}{\\alpha} (A^{\\top}A + \\tilde{\\lambda}^{2} L^{\\top}L)^{-1} A^{\\top}b$$\nWe recognize the expression $(A^{\\top}A + \\tilde{\\lambda}^{2} L^{\\top}L)^{-1} A^{\\top}b$ as the definition of the unscaled solution $x_{\\tilde{\\lambda}}$. Therefore, we have established the key relationship between the scaled and unscaled solutions:\n$$x^{\\alpha,\\beta,\\gamma}_{\\lambda} = \\frac{\\beta}{\\alpha} x_{\\tilde{\\lambda}} = \\frac{\\beta}{\\alpha} x_{\\frac{\\lambda \\gamma}{\\alpha}}$$\n\nNext, we analyze the effect of scaling on the L-curve coordinates. We first relate the scaled norms, $\\rho_{\\alpha,\\beta,\\gamma}(\\lambda)$ and $\\eta_{\\alpha,\\beta,\\gamma}(\\lambda)$, to their unscaled counterparts, $\\rho(\\cdot)$ and $\\eta(\\cdot)$.\nFor the residual norm:\n$$\\rho_{\\alpha,\\beta,\\gamma}(\\lambda) \\equiv \\|\\alpha A x^{\\alpha,\\beta,\\gamma}_{\\lambda} - \\beta b\\|_{2} = \\left\\|\\alpha A \\left(\\frac{\\beta}{\\alpha} x_{\\tilde{\\lambda}}\\right) - \\beta b\\right\\|_{2} = \\|\\beta A x_{\\tilde{\\lambda}} - \\beta b\\|_{2}$$\nSince $\\beta > 0$, we can factor it out of the norm:\n$$\\rho_{\\alpha,\\beta,\\gamma}(\\lambda) = \\beta \\|A x_{\\tilde{\\lambda}} - b\\|_{2} = \\beta \\rho(\\tilde{\\lambda})$$\nFor the solution semi-norm:\n$$\\eta_{\\alpha,\\beta,\\gamma}(\\lambda) \\equiv \\|\\gamma L x^{\\alpha,\\beta,\\gamma}_{\\lambda}\\|_{2} = \\left\\|\\gamma L \\left(\\frac{\\beta}{\\alpha} x_{\\tilde{\\lambda}}\\right)\\right\\|_{2} = \\left\\|\\frac{\\gamma \\beta}{\\alpha} L x_{\\tilde{\\lambda}}\\right\\|_{2}$$\nSince $\\alpha, \\beta, \\gamma > 0$, the factor $\\frac{\\gamma \\beta}{\\alpha}$ is positive and can be factored out:\n$$\\eta_{\\alpha,\\beta,\\gamma}(\\lambda) = \\frac{\\gamma \\beta}{\\alpha} \\|L x_{\\tilde{\\lambda}}\\|_{2} = \\frac{\\gamma \\beta}{\\alpha} \\eta(\\tilde{\\lambda})$$\n\nNow we can determine the relationship between the log-log coordinates of the L-curves.\nFor the abscissa:\n$$\\xi_{\\alpha,\\beta,\\gamma}(\\lambda) \\equiv \\ln\\big(\\rho_{\\alpha,\\beta,\\gamma}(\\lambda)\\big) = \\ln\\big(\\beta \\rho(\\tilde{\\lambda})\\big) = \\ln(\\beta) + \\ln\\big(\\rho(\\tilde{\\lambda})\\big) = \\ln(\\beta) + \\xi(\\tilde{\\lambda})$$\nFor the ordinate:\n$$\\nu_{\\alpha,\\beta,\\gamma}(\\lambda) \\equiv \\ln\\big(\\eta_{\\alpha,\\beta,\\gamma}(\\lambda)\\big) = \\ln\\left(\\frac{\\gamma \\beta}{\\alpha} \\eta(\\tilde{\\lambda})\\right) = \\ln\\left(\\frac{\\gamma \\beta}{\\alpha}\\right) + \\ln\\big(\\eta(\\tilde{\\lambda})\\big) = \\ln\\left(\\frac{\\gamma \\beta}{\\alpha}\\right) + \\nu(\\tilde{\\lambda})$$\nThese relations show that for any given $\\lambda$, the point on the scaled L-curve is obtained by taking the point on the original L-curve corresponding to $\\tilde{\\lambda} = \\lambda\\gamma/\\alpha$ and translating it by the constant vector $\\left(\\ln(\\beta), \\ln\\left(\\frac{\\gamma \\beta}{\\alpha}\\right)\\right)$. A translation does not change the curvature of a curve. However, the parameterization has changed from $\\tilde{\\lambda}$ to $\\lambda$, which will affect the calculation of curvature as a function of the parameter.\n\nTo find the curvature $\\kappa_{\\alpha,\\beta,\\gamma}(\\lambda)$, we must compute the first and second derivatives of $\\xi_{\\alpha,\\beta,\\gamma}(\\lambda)$ and $\\nu_{\\alpha,\\beta,\\gamma}(\\lambda)$ with respect to $\\lambda$. Let $C \\equiv \\frac{\\gamma}{\\alpha}$, a positive constant. Then $\\tilde{\\lambda} = C\\lambda$. We use the chain rule, where primes on $\\xi$ and $\\nu$ denote differentiation with respect to their arguments.\nFirst derivatives:\n$$\\xi'_{\\alpha,\\beta,\\gamma}(\\lambda) = \\frac{d}{d\\lambda} \\left[ \\ln(\\beta) + \\xi(C\\lambda) \\right] = \\xi'(C\\lambda) \\cdot \\frac{d(C\\lambda)}{d\\lambda} = C \\xi'(\\tilde{\\lambda})$$\n$$\\nu'_{\\alpha,\\beta,\\gamma}(\\lambda) = \\frac{d}{d\\lambda} \\left[ \\ln\\left(\\frac{\\gamma \\beta}{\\alpha}\\right) + \\nu(C\\lambda) \\right] = \\nu'(C\\lambda) \\cdot \\frac{d(C\\lambda)}{d\\lambda} = C \\nu'(\\tilde{\\lambda})$$\nSecond derivatives:\n$$\\xi''_{\\alpha,\\beta,\\gamma}(\\lambda) = \\frac{d}{d\\lambda} \\left[ C \\xi'(C\\lambda) \\right] = C \\cdot \\xi''(C\\lambda) \\cdot \\frac{d(C\\lambda)}{d\\lambda} = C^{2} \\xi''(\\tilde{\\lambda})$$\n$$\\nu''_{\\alpha,\\beta,\\gamma}(\\lambda) = \\frac{d}{d\\lambda} \\left[ C \\nu'(C\\lambda) \\right] = C \\cdot \\nu''(C\\lambda) \\cdot \\frac{d(C\\lambda)}{d\\lambda} = C^{2} \\nu''(\\tilde{\\lambda})$$\nNow we substitute these into the formula for curvature $\\kappa_{\\alpha,\\beta,\\gamma}(\\lambda)$:\n$$\\kappa_{\\alpha,\\beta,\\gamma}(\\lambda) = \\frac{\\big|\\xi'_{\\alpha,\\beta,\\gamma}(\\lambda)\\,\\nu''_{\\alpha,\\beta,\\gamma}(\\lambda) - \\nu'_{\\alpha,\\beta,\\gamma}(\\lambda)\\,\\xi''_{\\alpha,\\beta,\\gamma}(\\lambda)\\big|}{\\big(\\big(\\xi'_{\\alpha,\\beta,\\gamma}(\\lambda)\\big)^{2} + \\big(\\nu'_{\\alpha,\\beta,\\gamma}(\\lambda)\\big)^{2}\\big)^{3/2}}$$\nThe numerator of the expression is:\n$$\\big| \\left(C \\xi'(\\tilde{\\lambda})\\right) \\left(C^{2} \\nu''(\\tilde{\\lambda})\\right) - \\left(C \\nu'(\\tilde{\\lambda})\\right) \\left(C^{2} \\xi''(\\tilde{\\lambda})\\right) \\big| = \\big| C^{3} \\left( \\xi'(\\tilde{\\lambda})\\nu''(\\tilde{\\lambda}) - \\nu'(\\tilde{\\lambda})\\xi''(\\tilde{\\lambda}) \\right) \\big|$$\nSince $C > 0$, this simplifies to $C^{3} \\big| \\xi'(\\tilde{\\lambda})\\nu''(\\tilde{\\lambda}) - \\nu'(\\tilde{\\lambda})\\xi''(\\tilde{\\lambda}) \\big|$.\nThe denominator of the expression is:\n$$\\big( \\left(C \\xi'(\\tilde{\\lambda})\\right)^{2} + \\left(C \\nu'(\\tilde{\\lambda})\\right)^{2} \\big)^{3/2} = \\big( C^{2} \\left(\\xi'(\\tilde{\\lambda})^{2} + \\nu'(\\tilde{\\lambda})^{2}\\right) \\big)^{3/2}$$\nUsing the property $(ab)^{n} = a^{n}b^{n}$ and noting $(C^{2})^{3/2} = |C|^{3} = C^{3}$ as $C > 0$:\n$$C^{3} \\big( \\xi'(\\tilde{\\lambda})^{2} + \\nu'(\\tilde{\\lambda})^{2} \\big)^{3/2}$$\nCombining the numerator and denominator:\n$$\\kappa_{\\alpha,\\beta,\\gamma}(\\lambda) = \\frac{C^{3} \\big| \\xi'(\\tilde{\\lambda})\\nu''(\\tilde{\\lambda}) - \\nu'(\\tilde{\\lambda})\\xi''(\\tilde{\\lambda}) \\big|}{C^{3} \\big( \\xi'(\\tilde{\\lambda})^{2} + \\nu'(\\tilde{\\lambda})^{2} \\big)^{3/2}} = \\frac{\\big| \\xi'(\\tilde{\\lambda})\\nu''(\\tilde{\\lambda}) - \\nu'(\\tilde{\\lambda})\\xi''(\\tilde{\\lambda}) \\big|}{\\big( \\xi'(\\tilde{\\lambda})^{2} + \\nu'(\\tilde{\\lambda})^{2} \\big)^{3/2}}$$\nThis is precisely the definition of the curvature of the original L-curve, $\\kappa(\\cdot)$, evaluated at the parameter value $\\tilde{\\lambda}$.\nThus, we have the final relationship:\n$$\\kappa_{\\alpha,\\beta,\\gamma}(\\lambda) = \\kappa(\\tilde{\\lambda}) = \\kappa\\left(\\frac{\\lambda \\gamma}{\\alpha}\\right)$$\nThis result signifies that the scaling of the matrices $A$ and $L$ by factors $\\alpha$ and $\\gamma$ respectively causes a simple scaling of the regularization parameter axis for the curvature function, while the scaling of the data vector $b$ by $\\beta$ has no effect on the curvature. The shape of the L-curve is invariant under scaling, only its position in the plane and its parameterization are affected. The location of the \"corner\" of the L-curve, as determined by the maximum of the curvature function, will shift from $\\lambda^{*}_{\\text{orig}}$ to a new value $\\lambda^{*}_{\\text{scaled}}$ such that $\\frac{\\lambda^{*}_{\\text{scaled}} \\gamma}{\\alpha} = \\lambda^{*}_{\\text{orig}}$, or $\\lambda^{*}_{\\text{scaled}} = \\frac{\\alpha}{\\gamma}\\lambda^{*}_{\\text{orig}}$.", "answer": "$$\\boxed{\\kappa_{\\alpha,\\beta,\\gamma}(\\lambda) = \\kappa\\left(\\lambda \\frac{\\gamma}{\\alpha}\\right)}$$", "id": "3394265"}]}