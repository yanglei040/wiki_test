{"hands_on_practices": [{"introduction": "Before tackling complex, high-dimensional inverse problems, it's essential to understand the core mechanism of the Landweber iteration. This first exercise grounds the method in its simplest form: a scalar equation. By computing a single iteration step, you will see how the update rule works to reduce the residual, providing a concrete intuition for its role as a gradient descent step on the data-misfit functional [@problem_id:539080].", "problem": "Consider the Landweber iteration, an iterative regularization method used to solve linear equations of the form $A\\mathbf{x} = \\mathbf{b}$. For a scalar equation $a x = b$ with $a, b \\in \\mathbb{R}$, the iteration scheme is defined as:\n$$\nx_{k+1} = x_k + \\tau a (b - a x_k),\n$$\nwhere $\\tau$ is the step size and $x_k$ denotes the $k$-th iterate.  \n\nGiven the scalar equation $2x = 3$, compute the first iterate $x_1$ using the Landweber iteration with step size $\\tau = \\frac{1}{4}$ and initial guess $x_0 = 0$. Provide the exact value of $x_1$.", "solution": "1. The Landweber iteration for the scalar problem $a x = b$ is\n$$\nx_{k+1} \\;=\\; x_k \\;+\\; \\tau\\,a\\,(b - a\\,x_k).\n$$\n2. For $a=2$, $b=3$, $\\tau=\\tfrac14$, and $x_0=0$, we have\n$$\nb - a\\,x_0 = 3 - 2\\cdot 0 = 3.\n$$\n3. Hence\n$$\nx_1 = x_0 + \\tau\\,a\\,(b - a\\,x_0)\n    = 0 + \\frac14\\cdot 2\\cdot 3\n    = \\frac{6}{4}\n    = \\frac{3}{2}.\n$$", "answer": "$$\\boxed{3/2}$$", "id": "539080"}, {"introduction": "Moving from a single scalar equation to realistic matrix-based problems requires a robust implementation. This comprehensive exercise challenges you to not only code the standard fixed-step Landweber iteration but also to derive and implement a more adaptive version using an exact line search. By testing these methods on problems with varying degrees of ill-conditioning, you will gain practical insight into algorithm performance and the crucial role of step-size selection [@problem_id:3395628].", "problem": "Consider the linear inverse problem in a finite-dimensional real Hilbert space, where given a matrix $A \\in \\mathbb{R}^{m \\times n}$ and noisy data $y^\\delta \\in \\mathbb{R}^m$, one seeks to minimize the quadratic data misfit functional $J(x) = \\tfrac{1}{2}\\lVert A x - y^\\delta \\rVert_2^2$ with respect to $x \\in \\mathbb{R}^n$. The Landweber iteration is the gradient descent method for this quadratic objective, based on the fundamental identity that the gradient of $J$ is $\\nabla J(x) = A^\\ast (A x - y^\\delta)$, where $A^\\ast$ denotes the adjoint (transpose) of $A$. Starting from an initial guess $x_0$, the classical Landweber update with a fixed step size $\\omega$ is $x_{k+1} = x_k - \\omega A^\\ast (A x_k - y^\\delta)$, and a standard sufficient condition for convergence is $0  \\omega  \\tfrac{2}{\\lVert A \\rVert_2^2}$, where $\\lVert A \\rVert_2$ is the spectral norm (largest singular value). An alternative is to use an exact line-search step, which at each iteration chooses $\\omega_k$ to minimize the one-dimensional function $\\phi_k(\\omega) = \\lVert A(x_k - \\omega A^\\ast r_k) - y^\\delta \\rVert_2^2$, where $r_k = A x_k - y^\\delta$.\n\nYour tasks are:\n- Derive from first principles the exact line-search step choice $\\omega_k = \\arg\\min_{\\omega \\in \\mathbb{R}} \\lVert A(x_k - \\omega A^\\ast r_k) - y^\\delta \\rVert_2^2$ for the Landweber iteration. Implement this exact line-search Landweber iteration and compare its practical behavior against the fixed-step Landweber method.\n- In your implementation, for the fixed-step method use $\\omega_{\\mathrm{fix}} = c \\, \\lVert A \\rVert_2^{-2}$ for a given constant $c$, and verify the theoretical stability condition by checking whether $\\omega_{\\mathrm{fix}}  \\tfrac{2}{\\lVert A \\rVert_2^2}$ holds.\n- For each test case below, use the initial guess $x_0 = 0$ and run the specified number of iterations. Report the final data misfit norm $\\lVert A x_K - y^\\delta \\rVert_2$ for both methods. All numerical outputs must be rounded to six decimal places.\n\nThe test suite is defined as follows. All matrices and data are to be generated deterministically as specified, with all random number generators seeded as indicated, and all linear algebra performed in real numbers:\n\n- Test Case 1 (well-conditioned square system):\n  - Dimensions: $m = 30$, $n = 30$.\n  - Matrix: Entries of $A$ drawn independently from a standard normal distribution with seed $0$.\n  - Ground truth: $x^\\dagger \\in \\mathbb{R}^{30}$ with components $x^\\dagger_i = i^{-2}$ for $i = 1, \\dots, 30$.\n  - Data: $y = A x^\\dagger$.\n  - Noise: Additive noise $e$ with entries drawn independently from a standard normal distribution with seed $1$, scaled to satisfy $\\lVert e \\rVert_2 = \\varepsilon \\lVert y \\rVert_2$ with $\\varepsilon = 10^{-3}$, giving $y^\\delta = y + e$.\n  - Iterations: $K = 100$.\n  - Fixed step factor: $c = 1.0$, so $\\omega_{\\mathrm{fix}} = \\lVert A \\rVert_2^{-2}$.\n\n- Test Case 2 (ill-conditioned tall system):\n  - Dimensions: $m = 40$, $n = 30$.\n  - Matrix: Construct $A = U \\Sigma V^\\top$ with $U \\in \\mathbb{R}^{40 \\times 40}$ and $V \\in \\mathbb{R}^{30 \\times 30}$ orthonormal from a reduced $QR$ factorization of standard normal matrices with seed $2$ for $U$ and seed $3$ for $V$. Use $r = \\min(m,n) = 30$ singular values $\\sigma_j$ logarithmically spaced between $1$ and $10^{-6}$ in descending order, so that $\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_{30})$, and set $A = U_{:,1:r} \\Sigma V_{:,1:r}^\\top$.\n  - Ground truth: $x^\\dagger \\in \\mathbb{R}^{30}$ with components $x^\\dagger_i = i^{-2}$.\n  - Data and noise: As in Test Case $1$, with $y = A x^\\dagger$ and relative noise level $\\varepsilon = 10^{-3}$, using seed $4$ for the noise.\n  - Iterations: $K = 200$.\n  - Fixed step factor: $c = 1.8$, so $\\omega_{\\mathrm{fix}} = 1.8 \\, \\lVert A \\rVert_2^{-2}$.\n\n- Test Case 3 (rank-deficient underdetermined system):\n  - Dimensions: $m = 30$, $n = 40$, target rank $r = 20$.\n  - Matrix: Construct $A = U \\Sigma V^\\top$ with $U \\in \\mathbb{R}^{30 \\times 30}$ and $V \\in \\mathbb{R}^{40 \\times 40}$ orthonormal from a reduced $QR$ factorization of standard normal matrices with seed $5$ for $U$ and seed $6$ for $V$. Use $r = 20$ singular values $\\sigma_j$ logarithmically spaced between $1$ and $10^{-4}$ in descending order, and define $A = U_{:,1:r} \\Sigma V_{:,1:r}^\\top$, which has rank $r$.\n  - Ground truth: $x^\\dagger \\in \\mathbb{R}^{40}$ with components $x^\\dagger_i = i^{-2}$.\n  - Data and noise: As in Test Case $1$, with $y = A x^\\dagger$ and relative noise level $\\varepsilon = 10^{-3}$, using seed $7$ for the noise.\n  - Iterations: $K = 200$.\n  - Fixed step factor: $c = 1.0$, so $\\omega_{\\mathrm{fix}} = \\lVert A \\rVert_2^{-2}$.\n\n- Test Case 4 (stability boundary check with a diagonal system):\n  - Dimensions: $m = n = 3$.\n  - Matrix: $A = \\mathrm{diag}(3.0, 1.0, 0.5)$.\n  - Ground truth: $x^\\dagger = [1, -2, 3]^\\top$.\n  - Data: $y^\\delta = y = A x^\\dagger$ (no noise).\n  - Iterations: $K = 25$.\n  - Fixed step factor: $c = 2.2$, so $\\omega_{\\mathrm{fix}} = 2.2 \\, \\lVert A \\rVert_2^{-2}$, which exceeds the theoretical upper bound $\\tfrac{2}{\\lVert A \\rVert_2^2}$.\n\nImplementation requirements:\n- Implement both the fixed-step Landweber and the exact line-search Landweber methods. For exact line-search, derive and implement a closed-form expression for the minimizer $\\omega_k$ of $\\phi_k(\\omega)$ at each iteration $k$, based only on $A$, $r_k$, and $A^\\ast r_k$.\n- For Test Cases $1$–$3$, report for each case:\n  1. The final data misfit norm $\\lVert A x_K^{\\mathrm{els}} - y^\\delta \\rVert_2$ using exact line-search Landweber (rounded to six decimals).\n  2. The final data misfit norm $\\lVert A x_K^{\\mathrm{fix}} - y^\\delta \\rVert_2$ using fixed-step Landweber (rounded to six decimals).\n  3. A boolean indicating whether the theoretical fixed-step stability condition $\\omega_{\\mathrm{fix}}  \\tfrac{2}{\\lVert A \\rVert_2^2}$ holds.\n- For Test Case $4$, report:\n  1. A boolean indicating whether $\\omega_{\\mathrm{fix}}  \\tfrac{2}{\\lVert A \\rVert_2^2}$ holds.\n  2. A boolean indicating whether the fixed-step Landweber residual norms $\\lVert A x_k^{\\mathrm{fix}} - y^\\delta \\rVert_2$ fail to be monotonically nonincreasing over the $K$ iterations (this should be true when the step size is above the bound).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must aggregate the results for all test cases in the following order:\n  - Test Case $1$: $[\\lVert A x_K^{\\mathrm{els}} - y^\\delta \\rVert_2,\\ \\lVert A x_K^{\\mathrm{fix}} - y^\\delta \\rVert_2,\\ \\mathrm{stable}]$\n  - Test Case $2$: $[\\lVert A x_K^{\\mathrm{els}} - y^\\delta \\rVert_2,\\ \\lVert A x_K^{\\mathrm{fix}} - y^\\delta \\rVert_2,\\ \\mathrm{stable}]$\n  - Test Case $3$: $[\\lVert A x_K^{\\mathrm{els}} - y^\\delta \\rVert_2,\\ \\lVert A x_K^{\\mathrm{fix}} - y^\\delta \\rVert_2,\\ \\mathrm{stable}]$\n  - Test Case $4$: $[\\mathrm{above\\_bound},\\ \\mathrm{nonmonotone}]$\n- The final list is flattened into a single list of $11$ entries: three floats and one boolean per Test Case $1$–$3$, and two booleans for Test Case $4$. All floats must be rounded to six decimals. For example: $[f_{1,\\mathrm{els}}, f_{1,\\mathrm{fix}}, \\mathrm{stable}_1, f_{2,\\mathrm{els}}, f_{2,\\mathrm{fix}}, \\mathrm{stable}_2, f_{3,\\mathrm{els}}, f_{3,\\mathrm{fix}}, \\mathrm{stable}_3, \\mathrm{above\\_bound}_4, \\mathrm{nonmonotone}_4]$.", "solution": "The problem asks for the derivation and implementation of an exact line-search Landweber iteration for solving linear inverse problems, and its comparison with the fixed-step variant.\n\nThe problem is formulated as minimizing the data misfit functional $J(x) = \\frac{1}{2}\\lVert A x - y^\\delta \\rVert_2^2$, where $x \\in \\mathbb{R}^n$ is the parameter vector to be recovered, $A \\in \\mathbb{R}^{m \\times n}$ is the forward operator (matrix), and $y^\\delta \\in \\mathbb{R}^m$ is the noisy data. The Landweber iteration is a gradient descent method applied to this objective. The gradient of $J(x)$ with respect to $x$ is given by $\\nabla J(x) = A^\\ast (A x - y^\\delta)$, where $A^\\ast$ is the adjoint (transpose) of $A$.\n\nThe general form of the iteration is $x_{k+1} = x_k - \\omega_k \\nabla J(x_k)$, where $\\omega_k$ is the step size at iteration $k$. Let $r_k = A x_k - y^\\delta$ be the residual at iteration $k$. The gradient can be written as $\\nabla J(x_k) = A^\\ast r_k$. The Landweber update is thus $x_{k+1} = x_k - \\omega_k A^\\ast r_k$.\n\nTwo choices for the step size $\\omega_k$ are considered:\n1.  **Fixed-step Landweber**: $\\omega_k = \\omega$ is a constant. For the iteration to converge to a minimizer of $J(x)$, a sufficient condition on the step size is $0  \\omega  \\frac{2}{\\lVert A \\rVert_2^2}$, where $\\lVert A \\rVert_2$ is the spectral norm of $A$. The problem specifies using $\\omega_{\\mathrm{fix}} = c \\, \\lVert A \\rVert_2^{-2}$ for a given constant $c$. The stability condition is then equivalent to $0  c  2$.\n\n2.  **Exact Line-Search Landweber**: At each iteration $k$, the step size $\\omega_k$ is chosen to minimize the objective functional along the search direction $- \\nabla J(x_k)$. That is, we find $\\omega_k$ that minimizes $J(x_{k+1}) = J(x_k - \\omega A^\\ast r_k)$. This is equivalent to minimizing the one-dimensional function $\\phi_k(\\omega)$ as defined in the problem statement:\n    $$\n    \\omega_k = \\arg\\min_{\\omega \\in \\mathbb{R}} \\phi_k(\\omega) = \\arg\\min_{\\omega \\in \\mathbb{R}} \\lVert A(x_k - \\omega A^\\ast r_k) - y^\\delta \\rVert_2^2\n    $$\n\n**Derivation of the Exact Line-Search Step Size $\\omega_k$**\n\nTo find the optimal $\\omega_k$, we solve the minimization problem for $\\phi_k(\\omega)$.\nFirst, we rewrite the expression inside the norm by using the definition of the residual, $r_k = A x_k - y^\\delta$:\n$$\nA(x_k - \\omega A^\\ast r_k) - y^\\delta = (A x_k - y^\\delta) - \\omega A A^\\ast r_k = r_k - \\omega A A^\\ast r_k\n$$\nSo, we need to minimize $\\phi_k(\\omega) = \\lVert r_k - \\omega A A^\\ast r_k \\rVert_2^2$.\nThis is a standard linear least-squares problem for the scalar variable $\\omega$. We can find the minimum by expanding the squared norm and setting its derivative with respect to $\\omega$ to zero. The squared norm is the inner product of the vector with itself:\n$$\n\\phi_k(\\omega) = \\langle r_k - \\omega A A^\\ast r_k, r_k - \\omega A A^\\ast r_k \\rangle\n$$\nExpanding the inner product:\n$$\n\\phi_k(\\omega) = \\langle r_k, r_k \\rangle - 2\\omega \\langle r_k, A A^\\ast r_k \\rangle + \\omega^2 \\langle A A^\\ast r_k, A A^\\ast r_k \\rangle\n$$\nUsing the property of the adjoint, $\\langle u, Av \\rangle = \\langle A^\\ast u, v \\rangle$, the middle term can be simplified:\n$$\n\\langle r_k, A A^\\ast r_k \\rangle = \\langle A^\\ast r_k, A^\\ast r_k \\rangle = \\lVert A^\\ast r_k \\rVert_2^2\n$$\nThe expression for $\\phi_k(\\omega)$ becomes a quadratic in $\\omega$:\n$$\n\\phi_k(\\omega) = \\lVert r_k \\rVert_2^2 - 2\\omega \\lVert A^\\ast r_k \\rVert_2^2 + \\omega^2 \\lVert A A^\\ast r_k \\rVert_2^2\n$$\nTo find the minimum, we differentiate with respect to $\\omega$ and set the derivative to zero:\n$$\n\\frac{d\\phi_k}{d\\omega} = -2 \\lVert A^\\ast r_k \\rVert_2^2 + 2\\omega \\lVert A A^\\ast r_k \\rVert_2^2 = 0\n$$\nSolving for $\\omega$ yields the optimal step size $\\omega_k$:\n$$\n\\omega_k = \\frac{\\lVert A^\\ast r_k \\rVert_2^2}{\\lVert A A^\\ast r_k \\rVert_2^2}\n$$\nThis is the closed-form expression for the exact line-search step size. The denominator is zero if and only if $A A^\\ast r_k = 0$. This implies $\\langle A A^\\ast r_k, r_k \\rangle = \\lVert A^\\ast r_k \\rVert_2^2 = 0$, meaning the numerator is also zero. This occurs when the gradient $\\nabla J(x_k) = A^\\ast r_k$ is zero, indicating that $x_k$ is already a stationary point (a solution to the normal equations) and the iteration has converged. In a numerical implementation, if the denominator is zero (or numerically close to it), the step size can be set to $0$.\n\n**Implementation Strategy**\nThe solution is implemented in Python using the `numpy` library. The procedure for each test case is as follows:\n1.  **Problem Setup**: The matrices $A$, ground truth vectors $x^\\dagger$, and data $y^\\delta$ are generated according to the specifications for each test case. This involves creating random matrices with specific seeds, constructing matrices from a given singular value decomposition, generating noise, and scaling it to the desired relative level.\n2.  **Algorithm Implementation**:\n    - **Fixed-Step Landweber**: An initial guess $x_0 = 0$ is used. The fixed step size $\\omega_{\\mathrm{fix}} = c / \\lVert A \\rVert_2^2$ is computed, where $\\lVert A \\rVert_2$ is determined using `np.linalg.norm(A, 2)`. The iteration $x_{k+1} = x_k - \\omega_{\\mathrm{fix}} A^\\top (A x_k - y^\\delta)$ is then run for the specified number of iterations, $K$.\n    - **Exact Line-Search Landweber**: Starting from $x_0 = 0$, the iteration $x_{k+1} = x_k - \\omega_k A^\\top (A x_k - y^\\delta)$ is performed for $K$ steps. At each step $k$, the optimal step size $\\omega_k$ is computed using the derived formula $\\omega_k = \\lVert A^\\top r_k \\rVert_2^2 / \\lVert A A^\\top r_k \\rVert_2^2$.\n3.  **Result Evaluation**:\n    - For Test Cases 1-3, the final data misfit norms $\\lVert A x_K - y^\\delta \\rVert_2$ are calculated for both methods. The stability condition for the fixed-step method, $c  2$, is evaluated.\n    - For Test Case 4, the fixed-step method is tested with a step-size factor $c  2$, which violates the stability condition. The boolean condition $c  2$ is checked. The sequence of residual norms $\\{\\lVert A x_k - y^\\delta \\rVert_2\\}_{k=0}^K$ is stored, and it is checked whether this sequence fails to be monotonically non-increasing, i.e., if $\\lVert r_{k+1} \\rVert_2  \\lVert r_k \\rVert_2$ for any $k \\in \\{0, \\dots, K-1\\}$.\n4.  **Output Formatting**: The results for all test cases are collected into a single list. The floating-point numbers are formatted to six decimal places, and the list is printed in the specified comma-separated format.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares fixed-step and exact line-search Landweber iterations\n    for several linear inverse problems.\n    \"\"\"\n    \n    test_cases_params = [\n        # Test Case 1: well-conditioned square system\n        {'m': 30, 'n': 30, 'seed_A': 0, 'seed_noise': 1, 'eps': 1e-3, 'K': 100, 'c': 1.0, 'type': 'randn'},\n        # Test Case 2: ill-conditioned tall system\n        {'m': 40, 'n': 30, 'r': 30, 'sv_range': (1.0, 1e-6), 'seed_U': 2, 'seed_V': 3, 'seed_noise': 4, 'eps': 1e-3, 'K': 200, 'c': 1.8, 'type': 'svd'},\n        # Test Case 3: rank-deficient underdetermined system\n        {'m': 30, 'n': 40, 'r': 20, 'sv_range': (1.0, 1e-4), 'seed_U': 5, 'seed_V': 6, 'seed_noise': 7, 'eps': 1e-3, 'K': 200, 'c': 1.0, 'type': 'svd'},\n        # Test Case 4: stability boundary check\n        {'m': 3, 'n': 3, 'A': np.diag([3.0, 1.0, 0.5]), 'x_dagger': np.array([1.0, -2.0, 3.0]), 'eps': 0.0, 'K': 25, 'c': 2.2, 'type': 'diag'}\n    ]\n\n    all_results = []\n\n    for i, params in enumerate(test_cases_params):\n        m, n, K, c, eps = params['m'], params['n'], params['K'], params['c'], params['eps']\n        \n        # Generate problem data (A, x_dagger, y_delta)\n        if params['type'] == 'randn':\n            rng_A = np.random.default_rng(params['seed_A'])\n            A = rng_A.standard_normal((m, n))\n            x_dagger = 1.0 / np.arange(1, n + 1)**2\n            y = A @ x_dagger\n            if eps > 0:\n                rng_noise = np.random.default_rng(params['seed_noise'])\n                e = rng_noise.standard_normal(m)\n                y_delta = y + e * (eps * np.linalg.norm(y) / np.linalg.norm(e))\n            else:\n                y_delta = y\n        elif params['type'] == 'svd':\n            r = params['r']\n            sv_start, sv_end = params['sv_range']\n            \n            rng_U = np.random.default_rng(params['seed_U'])\n            U, _ = np.linalg.qr(rng_U.standard_normal((m, m)))\n            \n            rng_V = np.random.default_rng(params['seed_V'])\n            V, _ = np.linalg.qr(rng_V.standard_normal((n, n)))\n            \n            sigmas = np.logspace(np.log10(sv_start), np.log10(sv_end), r)\n            Sigma = np.diag(sigmas)\n            \n            A = U[:, :r] @ Sigma @ V[:, :r].T\n\n            x_dagger = 1.0 / np.arange(1, n + 1)**2\n            y = A @ x_dagger\n            if eps > 0:\n                rng_noise = np.random.default_rng(params['seed_noise'])\n                e = rng_noise.standard_normal(m)\n                y_delta = y + e * (eps * np.linalg.norm(y) / np.linalg.norm(e))\n            else:\n                y_delta = y\n        elif params['type'] == 'diag':\n            A = params['A']\n            x_dagger = params['x_dagger']\n            y_delta = A @ x_dagger\n            \n        # Spectral norm of A\n        if params['type'] == 'svd':\n            norm_A = params['sv_range'][0]\n        else:\n            norm_A = np.linalg.norm(A, 2)\n\n        x0 = np.zeros(n)\n\n        # ----- Fixed-step Landweber -----\n        x_fix = np.copy(x0)\n        omega_fix = c / norm_A**2\n        \n        residuals_fix_norms = []\n        if i == 3: # For Test Case 4\n            residuals_fix_norms.append(np.linalg.norm(A @ x_fix - y_delta))\n        \n        for _ in range(K):\n            r_fix = A @ x_fix - y_delta\n            x_fix = x_fix - omega_fix * (A.T @ r_fix)\n            if i == 3:\n                residuals_fix_norms.append(np.linalg.norm(A @ x_fix - y_delta))\n\n        # ----- Exact Line-Search Landweber -----\n        x_els = np.copy(x0)\n        for _ in range(K):\n            r_els = A @ x_els - y_delta\n            grad = A.T @ r_els\n            A_grad = A @ grad\n            \n            num = np.dot(grad, grad)\n            den = np.dot(A_grad, A_grad)\n            \n            omega_els = num / den if den > 1e-15 else 0.0\n            \n            x_els = x_els - omega_els * grad\n\n        # Collect and format results for the current test case\n        if i  3: # Test Cases 1, 2, 3\n            misfit_els = np.linalg.norm(A @ x_els - y_delta)\n            misfit_fix = np.linalg.norm(A @ x_fix - y_delta)\n            stable = (c  2.0)\n            all_results.extend([misfit_els, misfit_fix, stable])\n        else: # Test Case 4\n            above_bound = (c > 2.0)\n            is_nonmonotone = False\n            for j in range(len(residuals_fix_norms) - 1):\n                if residuals_fix_norms[j+1] > residuals_fix_norms[j]:\n                    is_nonmonotone = True\n                    break\n            all_results.extend([above_bound, is_nonmonotone])\n    \n    # Format the final output string\n    final_output_list = []\n    for item in all_results:\n        if isinstance(item, float):\n            final_output_list.append(f\"{item:.6f}\")\n        else:\n            final_output_list.append(str(item))\n            \n    print(f\"[{','.join(final_output_list)}]\")\n\nsolve()\n```", "id": "3395628"}, {"introduction": "An iterative method is only as good as its stopping rule, especially when dealing with the noisy data characteristic of inverse problems. Running the iteration for too few steps leads to an overly smoothed solution (high bias), while too many steps amplify noise (high variance). This final exercise moves from calculation to a critical conceptual comparison, asking you to analyze how two landmark *a posteriori* stopping rules—Morozov’s discrepancy principle and Lepskii’s balancing principle—navigate this crucial trade-off [@problem_id:3395655].", "problem": "Consider a linear inverse problem on Hilbert spaces with a bounded linear operator $A : X \\to Y$, true solution $x^\\dagger \\in X$, and noisy data $y^\\delta \\in Y$ satisfying $\\|y^\\delta - y\\| \\le \\delta$ where $y = A x^\\dagger$ and $\\delta  0$ denotes the noise level. The Landweber iteration with a fixed stepsize $\\tau$ chosen such that $0  \\tau  2/\\|A\\|^2$ is defined by\n$$\nx_{k+1} = x_k + \\tau A^*\\big(y^\\delta - A x_k\\big), \\quad k = 0, 1, 2, \\dots\n$$\nwith some initial guess $x_0 \\in X$, where $A^*$ denotes the adjoint of $A$. Let $r_k = y^\\delta - A x_k$ denote the residual at iteration $k$. Two principled ways to choose the stopping index $k$ in iterative regularization are:\n\n(i) Morozov’s discrepancy principle: choose the smallest $k$ such that $\\|r_k\\| \\le \\rho \\delta$ for a fixed safety factor $\\rho  1$.\n\n(ii) Lepskii’s balancing principle: choose the smallest $k$ such that, for all $j \\ge k$ within a prescribed index set, the differences $\\|x_j - x_k\\|$ are bounded by a noise envelope proportional to the propagated noise at scale $j$, thereby balancing approximation error and noise amplification via comparisons of iterates.\n\nStarting from the standard gradient-descent interpretation of Landweber iteration as minimizing the data misfit $\\|A x - y^\\delta\\|^2$, and the spectral characterization of the iteration as a filter acting on $A^* A$, derive the bias–variance trade-off in $\\|x_k - x^\\dagger\\|$ and then compare how the two principles above select $k$ in the presence of unknown smoothness of $x^\\dagger$ and unknown or misspecified noise level $\\delta$. Which statement below most accurately characterizes the adaptivity of the two principles to unknown smoothness and noise?\n\nA. Morozov’s discrepancy principle is simultaneously rate-optimal and fully adaptive to both unknown smoothness and unknown noise magnitude; Lepskii’s balancing principle requires knowing both the smoothness and the noise level and is therefore less adaptive.\n\nB. Both Morozov’s discrepancy principle and Lepskii’s balancing principle require prior knowledge of the smoothness of $x^\\dagger$, so neither is adaptive in that sense; only the spectrum of $A$ matters.\n\nC. Morozov’s discrepancy principle does not depend on the noise level $\\delta$ and therefore better adapts to unknown noise; Lepskii’s balancing principle always requires exact knowledge of $\\delta$.\n\nD. Lepskii’s balancing principle adapts to unknown smoothness by comparing the stability of successive iterates and thus balancing bias and variance without requiring the smoothness parameter; it typically uses an estimate of the noise level to set thresholds but is more robust than Morozov’s discrepancy principle to misspecification of $\\delta$, whereas Morozov’s rule hinges directly on $\\delta$ and does not provide additional adaptation to unknown smoothness beyond its residual criterion.\n\nE. Morozov’s discrepancy principle minimizes the residual norm by construction and therefore achieves minimax-optimal error rates uniformly across all source conditions without using the noise level, making it strictly superior to Lepskii’s principle in adaptation.", "solution": "The problem statement is a valid and well-posed question within the mathematical field of inverse problems and regularization theory. It accurately describes the Landweber iteration and two canonical *a posteriori* parameter choice rules, Morozov's discrepancy principle and Lepskii's balancing principle, and asks for a conceptual comparison of their adaptive properties. We shall proceed to a full analysis.\n\nFirst, we analyze the structure of the error in the Landweber iteration. The iteration is given by\n$$\nx_{k+1} = x_k + \\tau A^*(y^\\delta - A x_k) = (I - \\tau A^*A)x_k + \\tau A^*y^\\delta.\n$$\nLet $e_k = x_k - x^\\dagger$ be the error at step $k$. The error propagation is\n\\begin{align*}\ne_{k+1} = x_{k+1} - x^\\dagger \\\\\n= (I - \\tau A^*A)x_k + \\tau A^*y^\\delta - x^\\dagger \\\\\n= (I - \\tau A^*A)(e_k + x^\\dagger) + \\tau A^*(y + (y^\\delta - y)) - x^\\dagger \\\\\n= (I - \\tau A^*A)e_k + (I - \\tau A^*A)x^\\dagger - x^\\dagger + \\tau A^*y + \\tau A^*(y^\\delta - y) \\\\\n= (I - \\tau A^*A)e_k -\\tau A^*A x^\\dagger + \\tau A^*(A x^\\dagger) + \\tau A^*(y^\\delta - y) \\\\\n= (I - \\tau A^*A)e_k + \\tau A^*(y^\\delta - y).\n\\end{align*}\nBy unrolling this recurrence relation, we find the error at step $k$ to be\n$$\ne_k = (I - \\tau A^*A)^k e_0 + \\sum_{j=0}^{k-1} (I - \\tau A^*A)^{k-1-j} \\tau A^*(y^\\delta - y).\n$$\nwhere $e_0 = x_0 - x^\\dagger$ is the initial error. The total error $\\|x_k - x^\\dagger\\| = \\|e_k\\|$ is composed of two main components:\n\n1.  **Approximation Error (Bias):** The term $e_k^{\\text{approx}} = (I - \\tau A^*A)^k e_0$. This term represents how well the exact solution $x^\\dagger$ can be approximated by the regularized solution after $k$ steps in the absence of noise. The norm $\\|e_k^{\\text{approx}}\\|$ is a monotonically decreasing function of $k$. The rate of decrease depends on the \"smoothness\" of the initial error $e_0$, often characterized by a source condition like $e_0 = (A^*A)^\\mu w$ for some $\\mu  0$ and $w \\in X$. Under such a condition, $\\|e_k^{\\text{approx}}\\| = \\mathcal{O}(k^{-\\mu})$.\n\n2.  **Propagated Data Error (Variance):** The term $e_k^{\\text{noise}} = \\sum_{j=0}^{k-1} (I - \\tau A^*A)^{k-1-j} \\tau A^*(y^\\delta - y)$. This term represents the amplification of the noise in the data $y^\\delta - y$. Using the noise model $\\|y^\\delta - y\\| \\le \\delta$, the norm of this term can be bounded. Its norm, $\\|e_k^{\\text{noise}}\\|$, is a monotonically increasing function of $k$, and for Landweber iteration, its behavior is typically $\\|e_k^{\\text{noise}}\\| \\approx \\mathcal{O}(\\delta \\sqrt{k})$.\n\nThe total error is bounded by the sum of the norms of these two terms: $\\|e_k\\| \\le \\|e_k^{\\text{approx}}\\| + \\|e_k^{\\text{noise}}\\|$. This reveals the classic **bias-variance trade-off**: for small $k$, the bias is large and noise error is small; for large $k$, the bias is small but the noise error dominates. The goal of a stopping rule is to find an index $k$ that optimally balances these two terms. The optimal choice $k^*$ depends on the smoothness parameter $\\mu$ and the noise level $\\delta$.\n\nNow we analyze the two stopping principles.\n\n**(i) Morozov’s Discrepancy Principle:** This rule prescribes stopping at the smallest index $k$ for which the residual norm $\\|r_k\\| = \\|y^\\delta - A x_k\\|$ falls below a threshold proportional to the noise level $\\delta$: $\\|r_k\\| \\le \\rho \\delta$ for some safety factor $\\rho  1$.\nThe logic is to iterate as long as the residual is significantly larger than the noise in the data, implying that the signal has not yet been sufficiently recovered. Once the residual is of the same order of magnitude as the data noise, further iterations risk \"fitting the noise,\" leading to large oscillations in the solution.\n- **Adaptivity to smoothness:** The rate at which the residual of the noiseless iteration, $\\|y - A x_k^{\\text{exact}}\\|$, decreases depends on the smoothness $\\mu$. A smoother solution (larger $\\mu$) leads to a faster residual decrease, which in turn means the discrepancy principle will be satisfied at a larger iteration number $k$. It has been proven that this allows the method to achieve (asymptotically) the minimax-optimal convergence rate for the corresponding smoothness class without prior knowledge of $\\mu$. Thus, it is adaptive to unknown smoothness.\n- **Dependence on noise level:** The rule explicitly and critically depends on the noise level $\\delta$. If $\\delta$ is unknown or a poor estimate is used, the principle fails. If the estimate $\\hat{\\delta}$ is too large, the iteration stops too early (under-smoothing). If $\\hat{\\delta}$ is too small, the iteration stops too late (over-smoothing and noise amplification). It is not adaptive to unknown or misspecified noise.\n\n**(ii) Lepskii’s Balancing Principle:** This principle is specifically designed to adapt to an unknown smoothness parameter. It works by generating a sequence of solutions $\\{x_k\\}$ for an increasing sequence of iteration numbers (e.g., $k \\in \\{k_0, k_1, \\dots, k_M\\}$). It then selects the largest index $k_i$ such that for all subsequent indices $k_j$ ($j  i$), the difference between the iterates is controlled by the propagated noise at the higher index: $\\|x_{k_j} - x_{k_i}\\| \\le C \\cdot \\text{noise_bound}(k_j)$. For Landweber, this becomes $\\|x_{k_j} - x_{k_i}\\| \\le C \\delta \\sqrt{k_j}$.\nThe principle seeks the finest resolution (largest $k$) that is still \"stable\" with respect to solutions at even finer resolutions. The term $\\|x_{k_j} - x_{k_i}\\|$ depends on both the change in the bias part and the change in the noise part. When $k_i$ is in the \"bias-dominated\" regime, the term $\\|x_{k_j}^{\\text{approx}} - x_{k_i}^{\\text{approx}}\\|$ is large, and the condition will be violated. The condition is satisfied only when $k_i$ is large enough that the error becomes dominated by noise, and the subsequent change $\\|x_{k_j} - x_{k_i}\\|$ is governed by noise propagation.\n- **Adaptivity to smoothness:** This mechanism explicitly balances bias and variance without needing to know the rate of decay of the bias (i.e., without knowing $\\mu$). This is the key feature and purpose of the balancing principle, making it highly adaptive to unknown smoothness.\n- **Dependence on noise level:** The standard formulation uses $\\delta$ to define the threshold. However, because it relies on a *comparison* between iterates, it is more robust to a misspecification of $\\delta$ (e.g., an error by a constant factor) than Morozov's principle, which uses $\\delta$ as an absolute target. Advanced versions can be formulated to work without knowledge of $\\delta$.\n\nNow we evaluate the given options.\n\n**A. Morozov’s discrepancy principle is simultaneously rate-optimal and fully adaptive to both unknown smoothness and unknown noise magnitude; Lepskii’s balancing principle requires knowing both the smoothness and the noise level and is therefore less adaptive.**\nThis statement is incorrect. Morozov's principle is not adaptive to unknown noise magnitude; it requires a good estimate of $\\delta$. Lepskii's principle is specifically designed to be adaptive to unknown smoothness and does not require knowing it.\n\n**B. Both Morozov’s discrepancy principle and Lepskii’s balancing principle require prior knowledge of the smoothness of $x^\\dagger$, so neither is adaptive in that sense; only the spectrum of $A$ matters.**\nThis statement is incorrect. The main purpose of these *a posteriori* rules is to provide adaptivity to unknown smoothness. Both methods achieve this, albeit through different mechanisms.\n\n**C. Morozov’s discrepancy principle does not depend on the noise level $\\delta$ and therefore better adapts to unknown noise; Lepskii’s balancing principle always requires exact knowledge of $\\delta$.**\nThis statement is incorrect. It reverses the roles. Morozov's principle is $\\|r_k\\| \\le \\rho \\delta$, which directly depends on $\\delta$. Lepskii's principle is more robust to misspecification of $\\delta$.\n\n**D. Lepskii’s balancing principle adapts to unknown smoothness by comparing the stability of successive iterates and thus balancing bias and variance without requiring the smoothness parameter; it typically uses an estimate of the noise level to set thresholds but is more robust than Morozov’s discrepancy principle to misspecification of $\\delta$, whereas Morozov’s rule hinges directly on $\\delta$ and does not provide additional adaptation to unknown smoothness beyond its residual criterion.**\nThis statement is a correct and nuanced summary. It accurately describes that Lepskii's principle's mechanism is to compare iterates to adapt to smoothness. It correctly states that while the standard form uses $\\delta$, it is more robust to errors in $\\delta$ than Morozov's principle. It also correctly states that Morozov's rule depends directly on $\\delta$. The final clause, while slightly imprecisely worded (\"does not provide additional adaptation\"), correctly contrasts the sophisticated, multi-scale comparison mechanism of Lepskii's with the simpler, single-threshold mechanism of Morozov's.\n\n**E. Morozov’s discrepancy principle minimizes the residual norm by construction and therefore achieves minimax-optimal error rates uniformly across all source conditions without using the noise level, making it strictly superior to Lepskii’s principle in adaptation.**\nThis statement is incorrect. It does not minimize the residual norm; it stops when the norm crosses a threshold. Landweber iteration itself is a descent method for the residual, but Morozov's principle is a rule for when to stop the descent. Furthermore, it explicitly uses the noise level $\\delta$. It is not strictly superior to Lepskii's principle in adaptation; in fact, Lepskii's principle is often considered superior for its built-in mechanism for smoothness adaptation.\n\nBased on this analysis, statement D provides the most accurate characterization.", "answer": "$$\\boxed{D}$$", "id": "3395655"}]}