{"hands_on_practices": [{"introduction": "The Landweber iteration is a foundational gradient-based method for solving inverse problems, and its convergence speed is critically dependent on the choice of the step size, $\\omega$. This exercise guides you through the process of deriving the optimal step size by analyzing the spectral properties of the iteration operator, providing a crucial insight into the mechanics of iterative methods [@problem_id:3392775]. Mastering this derivation will build a solid theoretical understanding of how to analyze and tune iterative schemes for best performance in idealized, noise-free settings.", "problem": "Consider a linear inverse problem on finite-dimensional real Hilbert spaces, where the exact data model is $y = A x^{\\dagger}$ with a bounded linear operator $A \\in \\mathbb{R}^{m \\times n}$ and the unknown $x^{\\dagger} \\in \\mathbb{R}^{n}$. Let $L = \\|A\\|$ denote the operator norm of $A$ induced by the Euclidean norm. Assume that the initial guess $x_{0}$ lies in the orthogonal complement of the null space of $A$, i.e., $x_{0} \\in \\mathcal{N}(A)^{\\perp}$, so that the iteration proceeds on the subspace where $A$ has strictly positive singular values. Further assume that the nonzero singular values of $A$ are bounded below by a known constant $\\underline{\\sigma} > 0$, so that all relevant singular values satisfy $\\sigma_{i} \\in [\\underline{\\sigma}, L]$.\n\nConsider the Landweber iteration with a constant step size $\\omega > 0$,\n$$\nx_{k+1} = x_{k} + \\omega A^{\\ast} \\big( y - A x_{k} \\big),\n$$\nwhere $A^{\\ast}$ denotes the transpose (adjoint) of $A$. Let the error be $e_{k} = x_{k} - x^{\\dagger}$, and measure convergence in the singular vector basis of $A$.\n\nStarting from foundational definitions of the singular value decomposition and the induced operator norm, derive the evolution of the error components under the iteration. Using only the bounds $\\underline{\\sigma}$ and $L$, determine the constant step size $\\omega^{\\star}$ that maximizes the asymptotic linear convergence rate in the sense of minimizing the worst-case per-iteration contraction over all singular components. Then quantify the resulting per-iteration contraction factor for each singular component in the singular vector basis under $\\omega^{\\star}$.\n\nYour final answer must be a single analytical expression and include:\n- The optimal constant step size $\\omega^{\\star}$ expressed in terms of $\\underline{\\sigma}$ and $L$.\n- The per-iteration contraction factor for a singular component with singular value $\\sigma \\in [\\underline{\\sigma}, L]$ under $\\omega^{\\star}$.\n\nNo numerical approximation or rounding is required.", "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in the field of inverse problems concerning the convergence analysis of the Landweber iteration. All necessary information is provided, and the terminology is standard and unambiguous.\n\nWe begin by analyzing the error propagation of the Landweber iteration. The iteration is given by\n$$\nx_{k+1} = x_{k} + \\omega A^{\\ast} ( y - A x_{k} ),\n$$\nwhere $y = A x^{\\dagger}$ is the exact data, $x^{\\dagger}$ is the true solution, $A^{\\ast}$ is the adjoint (transpose) of $A$, and $\\omega > 0$ is a constant step size. The error at iteration $k$ is defined as $e_k = x_k - x^{\\dagger}$.\n\nTo derive the evolution of the error, we subtract $x^{\\dagger}$ from both sides of the iteration equation:\n$$\nx_{k+1} - x^{\\dagger} = x_{k} - x^{\\dagger} + \\omega A^{\\ast} ( y - A x_{k} ).\n$$\nSubstituting the definitions of the error $e_{k+1}$ and $e_k$, and the model for the data $y = A x^{\\dagger}$, we get:\n$$\ne_{k+1} = e_k + \\omega A^{\\ast} ( A x^{\\dagger} - A x_{k} ) = e_k - \\omega A^{\\ast} A (x_k - x^{\\dagger}) = e_k - \\omega A^{\\ast} A e_k.\n$$\nThis can be written as the linear error update equation:\n$$\ne_{k+1} = (I - \\omega A^{\\ast} A) e_k,\n$$\nwhere $I$ is the identity operator.\n\nTo analyze the convergence, we study the spectral properties of the iteration operator $R(\\omega) = I - \\omega A^{\\ast} A$. This is best done in the basis of the singular value decomposition (SVD) of $A$. Let the SVD of $A$ be $A = U \\Sigma V^{\\ast}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix containing the singular values $\\sigma_i \\ge 0$. The columns of $V$, denoted by $v_i$, are the right singular vectors of $A$ and form an orthonormal basis for $\\mathbb{R}^n$.\n\nThe operator $A^{\\ast}A$ can be expressed in terms of the SVD as:\n$$\nA^{\\ast}A = (V \\Sigma^{\\ast} U^{\\ast}) (U \\Sigma V^{\\ast}) = V \\Sigma^{\\ast} \\Sigma V^{\\ast}.\n$$\nThe matrix $\\Sigma^{\\ast} \\Sigma$ is an $n \\times n$ diagonal matrix with entries $\\sigma_i^2$. The right singular vectors $v_i$ are the eigenvectors of $A^{\\ast}A$:\n$$\nA^{\\ast} A v_i = (V \\Sigma^{\\ast} \\Sigma V^{\\ast}) v_i = \\sigma_i^2 v_i.\n$$\nThus, the singular vectors $v_i$ are also the eigenvectors of the error propagation operator $R(\\omega)$:\n$$\nR(\\omega) v_i = (I - \\omega A^{\\ast} A) v_i = v_i - \\omega (\\sigma_i^2 v_i) = (1 - \\omega \\sigma_i^2) v_i.\n$$\nThe corresponding eigenvalues of $R(\\omega)$ are $\\lambda_i = 1 - \\omega \\sigma_i^2$.\n\nThe problem states that the iteration is restricted to $\\mathcal{N}(A)^{\\perp}$, which is the span of the right singular vectors $v_i$ corresponding to non-zero singular values $\\sigma_i > 0$. Let the error be expanded in this basis: $e_k = \\sum_{i} c_{k,i} v_i$. The evolution of the coefficients is given by:\n$$\ne_{k+1} = \\sum_{i} c_{k+1,i} v_i = R(\\omega) e_k = \\sum_{i} c_{k,i} R(\\omega) v_i = \\sum_{i} c_{k,i} (1 - \\omega \\sigma_i^2) v_i.\n$$\nTherefore, the component of the error along each singular vector $v_i$ is scaled at each iteration by its corresponding eigenvalue: $c_{k+1,i} = (1 - \\omega \\sigma_i^2) c_{k,i}$. The per-iteration contraction factor for the $i$-th component is $\\rho_i = |1 - \\omega \\sigma_i^2|$.\n\nFor the iteration to converge for all components, we require $|\\rho_i|  1$ for all relevant $i$. This means we need $|1 - \\omega \\sigma^2|  1$ for all singular values $\\sigma$ in the given range $[\\underline{\\sigma}, L]$, where $L = \\|A\\| = \\max_i \\sigma_i$ and $\\underline{\\sigma} > 0$ is the lower bound for the non-zero singular values. This condition is equivalent to $-1  1 - \\omega \\sigma^2  1$, which simplifies to $0  \\omega \\sigma^2  2$, or $0  \\omega  \\frac{2}{\\sigma^2}$. To satisfy this for all $\\sigma \\in [\\underline{\\sigma}, L]$, we must satisfy it for the most restrictive case, which is $\\sigma = L$. Thus, we must have $0  \\omega  \\frac{2}{L^2}$.\n\nThe goal is to find the optimal constant step size $\\omega^{\\star}$ that maximizes the asymptotic convergence rate. This is equivalent to minimizing the worst-case (maximum) contraction factor over all possible modes, i.e., all $\\sigma \\in [\\underline{\\sigma}, L]$. We must solve the following min-max problem:\n$$\n\\omega^{\\star} = \\arg\\min_{\\omega > 0} \\left( \\max_{\\sigma \\in [\\underline{\\sigma}, L]} |1 - \\omega \\sigma^2| \\right).\n$$\nLet $s = \\sigma^2$. The problem becomes finding $\\omega^{\\star}$ that minimizes the function $g(\\omega) = \\max_{s \\in [\\underline{\\sigma}^2, L^2]} |1 - \\omega s|$. The function $f(s) = 1 - \\omega s$ is a line with negative slope. The maximum of its absolute value $|f(s)|$ over the interval $[\\underline{\\sigma}^2, L^2]$ must occur at one of the endpoints. Therefore,\n$$\ng(\\omega) = \\max \\{ |1 - \\omega \\underline{\\sigma}^2|, |1 - \\omega L^2| \\}.\n$$\nThe minimum of $g(\\omega)$ is achieved when the values at the two endpoints are equal in magnitude:\n$$\n|1 - \\omega \\underline{\\sigma}^2| = |1 - \\omega L^2|.\n$$\nSince $L > \\underline{\\sigma}$ and $\\omega > 0$, we have $1 - \\omega \\underline{\\sigma}^2 > 1 - \\omega L^2$. For the magnitudes to be equal, we must have one value be the negative of the other:\n$$\n1 - \\omega \\underline{\\sigma}^2 = -(1 - \\omega L^2) = \\omega L^2 - 1.\n$$\nSolving for $\\omega$:\n$$\n2 = \\omega L^2 + \\omega \\underline{\\sigma}^2 = \\omega(L^2 + \\underline{\\sigma}^2).\n$$\nThis yields the optimal step size:\n$$\n\\omega^{\\star} = \\frac{2}{L^2 + \\underline{\\sigma}^2}.\n$$\nWith this optimal step size, we can determine the per-iteration contraction factor for a singular component with singular value $\\sigma \\in [\\underline{\\sigma}, L]$. This factor, which we denote by $\\rho(\\sigma)$, is:\n$$\n\\rho(\\sigma) = |1 - \\omega^{\\star} \\sigma^2| = \\left| 1 - \\frac{2\\sigma^2}{L^2 + \\underline{\\sigma}^2} \\right|.\n$$\nThis expression gives the contraction factor for any mode $\\sigma$ in the specified range when using the optimal constant step size $\\omega^{\\star}$. The worst-case contraction factor occurs at the boundaries $\\sigma = \\underline{\\sigma}$ and $\\sigma = L$, and its value is $\\frac{L^2 - \\underline{\\sigma}^2}{L^2 + \\underline{\\sigma}^2}$.\n\nThe two quantities requested are the optimal step size $\\omega^{\\star}$ and the general contraction factor $\\rho(\\sigma)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{L^2 + \\underline{\\sigma}^2}  \\left| 1 - \\frac{2 \\sigma^2}{L^2 + \\underline{\\sigma}^2} \\right| \\end{pmatrix}}\n$$", "id": "3392775"}, {"introduction": "In real-world applications, inverse problems are invariably contaminated by noise, and running an iterative solver for too many steps can lead to catastrophic noise amplification. This practice demonstrates the core concept of iterative regularization: stopping the iteration early to prevent \"overfitting\" to the noise. By implementing the discrepancy principle—a data-driven stopping rule—and comparing it against an unconstrained iteration in an adversarial noise scenario, you will gain a concrete understanding of the semi-convergence phenomenon and the necessity of regularization [@problem_id:3392768].", "problem": "Consider the linear inverse problem in finite dimensions with additive noise. Let $A \\in \\mathbb{R}^{n \\times n}$ be a square, ill-conditioned matrix, $x^\\dagger \\in \\mathbb{R}^n$ the exact solution, and $y = A x^\\dagger$ the noise-free data. The noisy data is $y^\\delta = y + e^\\delta$ with $\\lVert e^\\delta \\rVert_2 = \\delta$. You will study the behavior of two iterative methods: (i) Landweber iteration with discrepancy-principle stopping, and (ii) unconstrained gradient descent continued for a large, fixed number of iterations. Your goal is to construct an adversarial noise scenario and quantify how discrepancy-based stopping avoids overfitting relative to unconstrained descent.\n\nFundamental basis and definitions:\n- The singular value decomposition (SVD) of $A$ is $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma = \\mathrm{diag}(\\sigma_1,\\ldots,\\sigma_n)$ with $\\sigma_1 \\ge \\cdots \\ge \\sigma_n > 0$.\n- The Landweber iteration for minimizing the data misfit $\\frac{1}{2} \\lVert A x - y^\\delta \\rVert_2^2$ with step size $\\omega \\in (0, 2/\\lVert A \\rVert_2^2)$ is $x_{k+1}^\\delta = x_k^\\delta + \\omega A^\\top (y^\\delta - A x_k^\\delta)$, with $x_0^\\delta = 0$.\n- The discrepancy principle with parameter $\\tau > 1$ stops at the first index $k_\\ast$ such that $\\lVert A x_{k_\\ast}^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$.\n- Unconstrained gradient descent here is the same Landweber update but continued for a prescribed large number of iterations $K$ without using $\\delta$.\n\nAdversarial noise construction:\n- You will construct $A$ with prescribed singular value decay via an SVD model: $A = U \\Sigma V^\\top$, where $U$ and $V$ are random orthogonal matrices, and $\\Sigma = \\mathrm{diag}(\\sigma_i)$ with $\\sigma_i = i^{-p}$ for an exponent $p > 0$.\n- You will construct $x^\\dagger$ by assigning coefficients in the right singular vector basis as $(\\alpha_i)_{i=1}^n$ with $\\alpha_i = i^{-q}$ for an exponent $q > 0$, and then normalizing so that $\\lVert x^\\dagger \\rVert_2 = 1$, that is $x^\\dagger = V \\alpha / \\lVert \\alpha \\rVert_2$.\n- You will choose adversarial noise aligned with the left singular vector corresponding to the smallest singular value: $e^\\delta = \\delta \\, u_n$, where $u_n$ is the $n$-th column of $U$. This makes $\\lVert e^\\delta \\rVert_2 = \\delta$ and concentrates the noise in the most amplified solution component.\n\nNumerical tasks to implement for each test case:\n1. Build $A = U \\Sigma V^\\top$ as described, with $U$ and $V$ obtained by orthonormalizing random Gaussian matrices using a fixed seed.\n2. Construct $x^\\dagger$ and $y = A x^\\dagger$, as described.\n3. Construct the adversarial noise $e^\\delta = \\delta u_n$ and the noisy data $y^\\delta = y + e^\\delta$.\n4. Choose the Landweber step size $\\omega = \\frac{1.9}{\\sigma_1^2}$, where $\\sigma_1$ is the largest singular value in $\\Sigma$.\n5. Run Landweber iteration with discrepancy-principle stopping: stop at the first index $k_\\ast$ such that $\\lVert A x_{k_\\ast}^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$, with a sufficiently large safety cap on the number of iterations to ensure termination if possible.\n6. Run the same iteration for a fixed large number of iterations $K$ (unconstrained gradient descent).\n7. Compute the relative reconstruction errors $\\varepsilon_{\\mathrm{reg}} = \\lVert x_{k_\\ast}^\\delta - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$ and $\\varepsilon_{\\mathrm{unc}} = \\lVert x_{K}^\\delta - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$, and report the ratio $r = \\varepsilon_{\\mathrm{unc}} / \\varepsilon_{\\mathrm{reg}}$.\n\nTest suite:\nFor each tuple $(n, p, q, \\delta, \\tau, \\mathrm{seed}, K)$ below, carry out the steps above and return the ratio $r$.\n\n- Case A (happy path, moderate ill-posedness): $(n, p, q, \\delta, \\tau, \\mathrm{seed}, K) = (50, 1.5, 1.0, 10^{-3}, 1.1, 0, 20000)$.\n- Case B (severe ill-posedness): $(50, 2.5, 1.0, 10^{-3}, 1.1, 1, 40000)$.\n- Case C (larger noise level and milder decay): $(80, 1.2, 0.5, 10^{-2}, 1.05, 2, 15000)$.\n- Case D (very severe ill-posedness, small noise): $(50, 3.0, 1.5, 10^{-4}, 1.2, 3, 60000)$.\n\nAnswer specification and output format:\n- For each case, the result is a single float $r$.\n- Your program must produce a single line of output containing the list of the four ratios $[r_A, r_B, r_C, r_D]$ as a comma-separated list enclosed in square brackets. Each ratio must be rounded to six decimal places.\n- No physical units are involved. All angles, if any, are irrelevant here.\n\nYour program must be self-contained and produce the exact specified final output format without any additional text.", "solution": "The user-provided problem is valid. It is a well-posed numerical experiment in the field of inverse problems, designed to illustrate the concept of iterative regularization and the phenomenon of overfitting. The problem is scientifically grounded in the theory of linear inverse problems and numerical optimization, with all parameters and procedures clearly and objectively defined.\n\nThe problem investigates a linear inverse problem of the form $y^\\delta = A x^\\dagger + e^\\delta$, where the goal is to recover the true solution $x^\\dagger \\in \\mathbb{R}^n$ from noisy data $y^\\delta \\in \\mathbb{R}^n$. The matrix $A \\in \\mathbb{R}^{n \\times n}$ is ill-conditioned, meaning its singular values decay rapidly. This ill-conditioning is a hallmark of inverse problems, where small perturbations in the data can lead to large errors in a naive solution. The noise term $e^\\delta$ has a known norm $\\lVert e^\\delta \\rVert_2 = \\delta$.\n\nWe use the singular value decomposition (SVD) of $A$, $A = U \\Sigma V^\\top$, as the fundamental tool for analysis. Here, $U = [u_1, \\dots, u_n]$ and $V = [v_1, \\dots, v_n]$ are orthogonal matrices whose columns are the left and right singular vectors, respectively. The matrix $\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$ contains the singular values, ordered non-increasingly: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n > 0$. The ill-conditioning of $A$ implies that the ratio $\\sigma_1/\\sigma_n$ is large. A naive solution, $x_{\\mathrm{naive}} = A^{-1} y^\\delta$, would amplify the noise. Using the SVD, this is $x_{\\mathrm{naive}} = V \\Sigma^{-1} U^\\top y^\\delta$. The noise component in the direction of $u_n$, where the singular value $\\sigma_n$ is smallest, gets amplified by a factor of $1/\\sigma_n$. The problem constructs an adversarial noise $e^\\delta = \\delta u_n$ precisely to maximize this effect.\n\nThe problem compares two iterative approaches to solve for $x$. Both are based on the Landweber iteration, which is a form of gradient descent applied to minimize the data misfit functional $J(x) = \\frac{1}{2} \\lVert Ax - y^\\delta \\rVert_2^2$. The iteration is given by:\n$$x_{k+1}^\\delta = x_k^\\delta - \\omega \\nabla J(x_k^\\delta) = x_k^\\delta + \\omega A^\\top (y^\\delta - A x_k^\\delta)$$\nstarting from $x_0^\\delta = 0$. The step size $\\omega$ must be chosen in $(0, 2/\\lVert A \\rVert_2^2) = (0, 2/\\sigma_1^2)$ to ensure convergence. The problem specifies $\\omega = 1.9/\\sigma_1^2$.\n\nIn the SVD basis, the Landweber iterate can be expressed as $x_k^\\delta = \\sum_{i=1}^n c_{k,i} v_i$. The coefficients evolve according to:\n$$c_{k,i} = \\frac{\\langle y^\\delta, u_i \\rangle}{\\sigma_i} \\left(1 - (1 - \\omega \\sigma_i^2)^k\\right)$$\nAs the number of iterations $k \\to \\infty$, the filter factors $(1 - (1 - \\omega \\sigma_i^2)^k)$ approach $1$. The solution coefficients $c_{k,i}$ converge to $\\langle y^\\delta, u_i \\rangle / \\sigma_i$, which are the coefficients of the naive solution $A^{-1} y^\\delta$. For the component $i=n$, the noise $e^\\delta = \\delta u_n$ results in $\\langle y^\\delta, u_n \\rangle = \\langle y, u_n \\rangle + \\delta$. The coefficient $c_{k,n}$ converges to $\\frac{\\langle y, u_n \\rangle}{\\sigma_n} + \\frac{\\delta}{\\sigma_n}$. The term $\\delta/\\sigma_n$ is enormous due to the smallness of $\\sigma_n$, leading to catastrophic noise amplification. This is what happens in the \"unconstrained gradient descent\" case, where the iteration is run for a large, fixed number of steps $K$, causing it to \"overfit\" to the noise. The resulting error $\\varepsilon_{\\mathrm{unc}}$ is expected to be large.\n\nThe second approach uses the Landweber iteration but incorporates a stopping rule known as the discrepancy principle. This principle, a form of regularization, dictates that the iteration should be stopped as soon as the data misfit of the iterate becomes comparable to the noise level. Specifically, we stop at the first iteration $k_\\ast$ such that:\n$$\\lVert A x_{k_\\ast}^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$$\nwhere $\\tau > 1$ is a safety factor. This stopping rule prevents the iteration from proceeding long enough to significantly amplify the high-frequency noise components. The filter factors $(1 - (1 - \\omega \\sigma_i^2)^{k_\\ast})$ for small $\\sigma_i$ (i.e., high-frequency components) will be close to $0$ because $k_\\ast$ is relatively small. This damps the noisy components, especially the one corresponding to $u_n$, resulting in a stable, regularized solution $x_{k_\\ast}^\\delta$ with a much smaller error $\\varepsilon_{\\mathrm{reg}}$.\n\nThe numerical task is to implement this scenario for four different parameter sets. For each case, we construct the matrix $A$, the true solution $x^\\dagger$, and the noisy data $y^\\delta$ according to the SVD-based model. Then we run the two iterative schemes: one stopped by the discrepancy principle and one run for a fixed large number of iterations $K$. Finally, we compute the relative reconstruction errors, $\\varepsilon_{\\mathrm{reg}}$ and $\\varepsilon_{\\mathrm{unc}}$, and their ratio $r = \\varepsilon_{\\mathrm{unc}} / \\varepsilon_{\\mathrm{reg}}$, which quantifies the benefit of using a proper regularization strategy. This ratio is expected to be significantly larger than $1$.\n\nThe implementation will proceed as follows:\n1.  A function will be defined to handle a single test case, taking the parameters $(n, p, q, \\delta, \\tau, \\mathrm{seed}, K)$ as input.\n2.  Inside this function, a random number generator seed is set for reproducibility.\n3.  Random orthogonal matrices $U$ and $V$ of size $n \\times n$ are generated by applying QR decomposition to random Gaussian matrices.\n4.  The diagonal matrix of singular values $\\Sigma$ is constructed with entries $\\sigma_i = i^{-p}$ for $i \\in \\{1, \\dots, n\\}$.\n5.  The matrix $A$ is assembled as $A = U \\Sigma V^\\top$.\n6.  The true solution $x^\\dagger$ is constructed from coefficients $\\alpha_i = i^{-q}$, normalized such that $\\lVert x^\\dagger \\rVert_2 = 1$, and transformed into the standard basis: $x^\\dagger = V (\\alpha / \\lVert \\alpha \\rVert_2)$. The noise-free data is $y = A x^\\dagger$.\n7.  The adversarial noise $e^\\delta = \\delta u_n$ is created, where $u_n$ is the last column of $U$. The noisy data becomes $y^\\delta = y + e^\\delta$.\n8.  The Landweber step size is set to $\\omega = 1.9 / \\sigma_1^2$.\n9.  The Landweber iteration with the discrepancy principle is run. Starting with $x_0^\\delta = 0$, the iteration proceeds until $\\lVert A x_k^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$. The resulting solution is $x_{\\mathrm{reg}} = x_{k_\\ast}^\\delta$.\n10. The unconstrained Landweber iteration is run for a fixed $K$ steps, yielding the solution $x_{\\mathrm{unc}} = x_K^\\delta$.\n11. The relative errors $\\varepsilon_{\\mathrm{reg}} = \\lVert x_{\\mathrm{reg}} - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$ and $\\varepsilon_{\\mathrm{unc}} = \\lVert x_{\\mathrm{unc}} - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$ are calculated.\n12. The ratio $r = \\varepsilon_{\\mathrm{unc}} / \\varepsilon_{\\mathrm{reg}}$ is computed and returned.\n\nThis procedure will be repeated for all four test cases, and the final list of ratios will be formatted as requested.", "answer": "```python\nimport numpy as np\n\ndef run_case(n, p, q, delta, tau, seed, K):\n    \"\"\"\n    Solves a single test case for the iterative regularization problem.\n    \"\"\"\n    # 1. Build A = U Sigma V^T\n    np.random.seed(seed)\n    \n    # Generate random orthogonal matrices U and V\n    H1 = np.random.randn(n, n)\n    U, _ = np.linalg.qr(H1)\n    \n    H2 = np.random.randn(n, n)\n    V, _ = np.linalg.qr(H2)\n    \n    # Generate singular values and Sigma matrix\n    i_vals = np.arange(1, n + 1)\n    sigma_vals = i_vals**(-p)\n    Sigma = np.diag(sigma_vals)\n    \n    A = U @ Sigma @ V.T\n\n    # 2. Construct x_dagger and y\n    alpha_coeffs = i_vals**(-q)\n    alpha_norm_factor = np.linalg.norm(alpha_coeffs)\n    alpha_normalized = alpha_coeffs / alpha_norm_factor\n    \n    x_dagger = V @ alpha_normalized\n    y = A @ x_dagger\n\n    # 3. Construct adversarial noise and noisy data\n    u_n = U[:, -1]\n    e_delta = delta * u_n\n    y_delta = y + e_delta\n\n    # 4. Choose Landweber step size\n    sigma_1 = sigma_vals[0]\n    omega = 1.9 / (sigma_1**2)\n\n    # 5. Run Landweber with discrepancy principle\n    x_k_reg = np.zeros(n)\n    # Use a large enough safety cap for iterations\n    max_iter_reg = max(100000, 2 * K)\n    \n    for _ in range(max_iter_reg):\n        residual = y_delta - A @ x_k_reg\n        residual_norm = np.linalg.norm(residual)\n        \n        if residual_norm = tau * delta:\n            break\n        \n        x_k_reg = x_k_reg + omega * (A.T @ residual)\n    else:\n        # This part should not be reached if the problem is well-posed.\n        # It indicates failure of the discrepancy principle to stop.\n        # Assign NaN or raise an error to signal failure.\n        x_k_reg = np.full(n, np.nan)\n    \n    x_reg = x_k_reg\n\n    # 6. Run unconstrained Landweber for K iterations\n    x_k_unc = np.zeros(n)\n    for _ in range(K):\n        residual = y_delta - A @ x_k_unc\n        x_k_unc = x_k_unc + omega * (A.T @ residual)\n        \n    x_unc = x_k_unc\n\n    # 7. Compute errors and ratio\n    norm_x_dagger = np.linalg.norm(x_dagger) # Should be 1.0 by construction\n    \n    err_reg = np.linalg.norm(x_reg - x_dagger)\n    err_unc = np.linalg.norm(x_unc - x_dagger)\n    \n    eps_reg = err_reg / norm_x_dagger\n    eps_unc = err_unc / norm_x_dagger\n    \n    ratio = eps_unc / eps_reg\n    return ratio\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, p, q, delta, tau, seed, K)\n        (50, 1.5, 1.0, 10**-3, 1.1, 0, 20000),  # Case A\n        (50, 2.5, 1.0, 10**-3, 1.1, 1, 40000),  # Case B\n        (80, 1.2, 0.5, 10**-2, 1.05, 2, 15000), # Case C\n        (50, 3.0, 1.5, 10**-4, 1.2, 3, 60000),  # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        n, p, q, delta, tau, seed, K = case\n        result = run_case(n, p, q, delta, tau, seed, K)\n        results.append(result)\n\n    # Format output as required\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3392768"}, {"introduction": "While the Landweber iteration is conceptually simple, its convergence can be impractically slow. This hands-on exercise introduces a more powerful alternative, the Conjugate Gradient for Least Squares (CGLS) method, which belongs to the class of Krylov subspace methods. You will implement both Landweber and CGLS to solve the same ill-posed problem and compare their semi-convergence behavior [@problem_id:3392772]. This comparison highlights the dramatic acceleration that sophisticated methods like CGLS provide, offering a practical perspective on choosing the right algorithm for a given problem.", "problem": "Consider the linear inverse problem defined by a square diagonal matrix $A \\in \\mathbb{R}^{50 \\times 50}$ with $A = \\mathrm{diag}(\\sigma_j)$ where $\\sigma_j = 10^{-j}$ for $j \\in \\{1,\\dots,50\\}$. Let the exact data be $b^\\dagger = A x^\\dagger$ for an exact solution $x^\\dagger \\in \\mathbb{R}^{50}$. The measured data are $b^\\delta = b^\\dagger + e^\\delta$, where the noise vector $e^\\delta \\in \\mathbb{R}^{50}$ satisfies $\\lVert e^\\delta \\rVert_2 = \\delta$ and is constructed deterministically as\n$$\nu_j = (-1)^j \\, j,\\quad j \\in \\{1,\\dots,50\\},\\qquad e^\\delta = \\delta \\, \\frac{u}{\\lVert u \\rVert_2}.\n$$\nYou will study iterative regularization via two methods: the Conjugate Gradient for Least Squares (CGLS) method and the Landweber iteration. The CGLS method is defined as the Conjugate Gradient algorithm applied to the normal equations $A^\\top A x = A^\\top b^\\delta$ starting from $x_0 = 0$, with the canonical residuals and search directions determined by the method. The Landweber iteration is defined as gradient descent on the least-squares objective $\\frac{1}{2}\\lVert A x - b^\\delta \\rVert_2^2$, with iteration\n$$\nx_{k+1} = x_k + \\tau A^\\top \\left(b^\\delta - A x_k\\right),\n$$\nstarting from $x_0 = 0$ and a fixed stepsize $\\tau$ chosen to satisfy $0  \\tau  \\frac{2}{\\lVert A \\rVert_2^2}$.\n\nYour task is to:\n- Implement the first $10$ iterations of the CGLS method and the Landweber iteration for the same computational cost, where computational cost is measured by the total number of applications of $A$ and $A^\\top$. Use $x_0 = 0$ for both methods and the same number of iterations for both methods.\n- Set the Landweber stepsize to $\\tau = \\frac{1}{\\lVert A \\rVert_2^2}$, where $\\lVert A \\rVert_2$ is the spectral norm of $A$.\n- For each iteration $k \\in \\{1,\\dots,10\\}$ and each method, compute the relative error $\\frac{\\lVert x_k - x^\\dagger \\rVert_2}{\\lVert x^\\dagger \\rVert_2}$.\n- For each method, determine the iteration index $k$ in $\\{1,\\dots,10\\}$ that minimizes the relative error and record that index and the corresponding minimal relative error. This demonstrates semi-convergence behavior.\n\nUse the following test suite of exact solutions $x^\\dagger$ and noise levels $\\delta$:\n1. Case A: $x^\\dagger_j = \\frac{1}{j}$ for $j \\in \\{1,\\dots,50\\}$, $\\delta = 10^{-3}$.\n2. Case B: $x^\\dagger_j = 1$ for $j \\in \\{1,\\dots,50\\}$, $\\delta = 10^{-3}$.\n3. Case C: $x^\\dagger_j = \\frac{1}{j}$ for $j \\in \\{1,\\dots,50\\}$, $\\delta = 10^{-6}$.\n4. Case D: $x^\\dagger_j = 1$ for $j \\in \\{1,\\dots,50\\}$, $\\delta = 10^{-2}$.\n\nConstraints and definitions to use:\n- The Landweber stepsize is fixed to $\\tau = \\frac{1}{\\lVert A \\rVert_2^2}$, where $\\lVert A \\rVert_2 = \\max_{j} \\sigma_j$.\n- The CGLS method must be implemented directly as Conjugate Gradient on the normal equations without any external black-box solver.\n- The initial iterate for all methods is $x_0 = 0$.\n- All norms are the Euclidean norm $\\lVert \\cdot \\rVert_2$.\n- Angles do not appear; no angle units are needed. No physical units are involved.\n\nFor each case in the test suite, produce a result of the form $[k_{\\mathrm{CGLS}}, \\mathrm{err}_{\\mathrm{CGLS}}, k_{\\mathrm{LW}}, \\mathrm{err}_{\\mathrm{LW}}]$, where $k_{\\mathrm{CGLS}}$ and $k_{\\mathrm{LW}}$ are integers in $\\{1,\\dots,10\\}$, and $\\mathrm{err}_{\\mathrm{CGLS}}$ and $\\mathrm{err}_{\\mathrm{LW}}$ are floats equal to the minimal relative errors over the first $10$ iterations for the respective methods.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results for the $4$ cases as a comma-separated list of lists, enclosed in square brackets. For example: $[[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]]$.\n- Each float must be formatted in scientific notation with six digits after the decimal point using a lower-case $e$ (for example, $1.234567e-04$).\n- The final printed line must be exactly in this format, with no additional text.\n\nThis problem is purely mathematical and algorithmic; no physical units are involved. Ensure all calculations use real arithmetic with full double precision.", "solution": "The problem presented is a well-posed numerical exercise in the field of inverse problems, specifically focusing on the comparison of two iterative regularization methods: Landweber iteration and the Conjugate Gradient for Least Squares (CGLS) method. The problem is mathematically sound, self-contained, and all parameters and objectives are clearly defined. I will therefore proceed with a complete solution.\n\n### 1. Theoretical Framework\n\nThe core of the problem is the linear system $A x = b^\\delta$, where we seek to find a stable and meaningful approximation of an unknown true solution $x^\\dagger$ given noisy data $b^\\delta$. The matrix $A$ is a diagonal matrix with entries $\\sigma_j = 10^{-j}$ for $j \\in \\{1, \\dots, 50\\}$. These values, which are the singular values of $A$, decay rapidly towards zero. This property makes the inverse problem severely ill-posed: small perturbations in the data $b^\\delta$ (due to noise $e^\\delta$) can lead to large, unphysical oscillations in the naively computed solution, $x = A^{-1} b^\\delta$. This is because the inverse matrix $A^{-1}$ has singular values $1/\\sigma_j = 10^j$, which amplify noise components aligned with the corresponding singular vectors.\n\nRegularization methods are designed to counteract this instability. Iterative methods, such as Landweber and CGLS, provide regularization by projecting the problem onto a sequence of subspaces of increasing dimension. The iteration count $k$ acts as the regularization parameter. By stopping the iteration early (i.e., before the algorithm starts to fit the noise), we can obtain a stable approximation of $x^\\dagger$. This phenomenon, where the error $\\lVert x_k - x^\\dagger \\rVert_2$ first decreases and then increases with $k$, is known as semi-convergence. The task is to find the optimal iteration count $k$ within the first $10$ iterations that minimizes this error.\n\nThe problem uses a fixed problem size $N=50$. The matrix $A$ is diagonal, so $A = A^\\top$ and its spectral norm is $\\lVert A \\rVert_2 = \\max_j |\\sigma_j| = \\sigma_1 = 10^{-1}$.\n\n### 2. Method Implementation\n\nWe will implement two iterative methods starting with the initial guess $x_0 = 0$. For each method and each test case, we will compute the iterates $x_1, \\dots, x_{10}$ and their corresponding relative errors $\\frac{\\lVert x_k - x^\\dagger \\rVert_2}{\\lVert x^\\dagger \\rVert_2}$.\n\n#### 2.1. Landweber Iteration\n\nThe Landweber iteration is a form of gradient descent applied to the least-squares functional $J(x) = \\frac{1}{2}\\lVert Ax - b^\\delta \\rVert_2^2$. The gradient is $\\nabla J(x) = A^\\top(Ax - b^\\delta)$. The iterative update rule is:\n$$\nx_{k+1} = x_k - \\tau \\nabla J(x_k) = x_k + \\tau A^\\top(b^\\delta - Ax_k)\n$$\nThe problem specifies a stepsize $\\tau = \\frac{1}{\\lVert A \\rVert_2^2}$. With $\\lVert A \\rVert_2 = 10^{-1}$, we have $\\tau = \\frac{1}{(10^{-1})^2} = 100$. Since $A$ is a real diagonal matrix, $A^\\top = A$. The iteration starting from $x_0=0$ is implemented by repeatedly applying this formula for $k=0, 1, \\dots, 9$.\n\n#### 2.2. Conjugate Gradient for Least Squares (CGLS)\n\nThe CGLS method applies the Conjugate Gradient (CG) algorithm to solve the normal equations $A^\\top A x = A^\\top b^\\delta$. This is significantly more efficient than simple gradient descent. The standard algorithm, adapted for stability and starting with $x_0 = 0$, is as follows:\n\n1.  Initialize:\n    $x_0 = 0$\n    $r_0 = b^\\delta - A x_0 = b^\\delta$\n    $g_0 = A^\\top r_0$\n    $p_0 = g_0$\n\n2.  Iterate for $k = 0, 1, \\dots, 9$:\n    $q_k = A p_k$\n    $\\alpha_k = \\frac{\\lVert g_k \\rVert_2^2}{\\lVert q_k \\rVert_2^2}$\n    $x_{k+1} = x_k + \\alpha_k p_k$\n    $r_{k+1} = r_k - \\alpha_k q_k$\n    $g_{k+1} = A^\\top r_{k+1}$\n    $\\beta_k = \\frac{\\lVert g_{k+1} \\rVert_2^2}{\\lVert g_k \\rVert_2^2}$\n    $p_{k+1} = g_{k+1} + \\beta_k p_k$\n\nEach iteration of both Landweber and CGLS requires one application of $A$ and one of $A^\\top$, making their per-iteration computational cost comparable, as specified in the problem statement.\n\n### 3. Execution Plan\n\nThe overall procedure for each of the four test cases is:\n\n1.  **Setup**: Construct the vector of singular values $\\sigma \\in \\mathbb{R}^{50}$ where $\\sigma_j = 10^{-j}$. Construct the specified true solution $x^\\dagger$ and calculate its norm $\\lVert x^\\dagger \\rVert_2$. Compute the true data $b^\\dagger = A x^\\dagger$. Construct the deterministic noise vector $e^\\delta$ with norm $\\delta$. Compute the noisy data $b^\\delta = b^\\dagger + e^\\delta$.\n\n2.  **Iteration and Error Calculation**:\n    -   Run the Landweber iteration for $10$ steps, storing the relative error $\\frac{\\lVert x_k - x^\\dagger \\rVert_2}{\\lVert x^\\dagger \\rVert_2}$ for each iterate $x_k$, $k \\in \\{1, \\dots, 10\\}$.\n    -   Run the CGLS iteration for $10$ steps, storing the relative error for each iterate $x_k$, $k \\in \\{1, \\dots, 10\\}$.\n\n3.  **Result Extraction**: For each method, find the minimum relative error among the $10$ computed values and the corresponding iteration index $k \\in \\{1, \\dots, 10\\}$.\n\n4.  **Formatting**: The final results for each test case, $[k_{\\mathrm{CGLS}}, \\mathrm{err}_{\\mathrm{CGLS}}, k_{\\mathrm{LW}}, \\mathrm{err}_{\\mathrm{LW}}]$, are assembled into a list for final output according to the specified format. The Python code in the `final_answer` section implements this logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_one_case(x_dagger_type: str, delta: float):\n    \"\"\"\n    Runs one test case for the inverse problem, comparing Landweber and CGLS.\n    \"\"\"\n    N = 50\n    # Use 1-based indexing for correspondence with problem statement\n    j_indices = np.arange(1, N + 1)\n\n    # Setup matrix A (as a vector of singular values)\n    sigma = 10.0**(-j_indices)\n\n    # Setup exact solution x_dagger\n    if x_dagger_type == '1/j':\n        x_dagger = 1.0 / j_indices\n    elif x_dagger_type == '1':\n        x_dagger = np.ones(N)\n    else:\n        raise ValueError(\"Unknown x_dagger_type\")\n    \n    norm_x_dagger = np.linalg.norm(x_dagger)\n\n    # Setup data b_dagger and b_delta\n    b_dagger = sigma * x_dagger\n    \n    u = ((-1.0)**j_indices) * j_indices\n    norm_u = np.linalg.norm(u)\n    e_delta = delta * u / norm_u\n    \n    b_delta = b_dagger + e_delta\n\n    # --- Landweber Iteration ---\n    errors_lw = []\n    x_lw = np.zeros(N)\n    # Spectral norm ||A||_2 is max(sigma) = 10^-1 = 0.1\n    # tau = 1 / ||A||_2^2 = 1 / (0.1)^2 = 100\n    tau = 100.0\n    \n    for _ in range(10):\n        residual = b_delta - sigma * x_lw\n        # Since A is diagonal real, A^T = A\n        grad = sigma * residual\n        x_lw = x_lw + tau * grad\n        \n        rel_error = np.linalg.norm(x_lw - x_dagger) / norm_x_dagger\n        errors_lw.append(rel_error)\n\n    min_err_lw = min(errors_lw)\n    k_lw = errors_lw.index(min_err_lw) + 1\n\n    # --- CGLS Iteration ---\n    errors_cgls = []\n    x_cgls = np.zeros(N)\n    r = b_delta  # r_0 = b - A*x_0 = b since x_0=0\n    g = sigma * r  # g_0 = A^T * r_0\n    p = g        # p_0 = g_0\n    norm_g_sq = np.dot(g, g)\n\n    for _ in range(10):\n        q = sigma * p # q_k = A*p_k\n        alpha = norm_g_sq / np.dot(q, q)\n        \n        x_cgls = x_cgls + alpha * p\n        \n        r = r - alpha * q\n        g_next = sigma * r\n        norm_g_next_sq = np.dot(g_next, g_next)\n        \n        beta = norm_g_next_sq / norm_g_sq\n        p = g_next + beta * p\n        \n        g = g_next\n        norm_g_sq = norm_g_next_sq\n        \n        rel_error = np.linalg.norm(x_cgls - x_dagger) / norm_x_dagger\n        errors_cgls.append(rel_error)\n    \n    min_err_cgls = min(errors_cgls)\n    k_cgls = errors_cgls.index(min_err_cgls) + 1\n    \n    return [k_cgls, min_err_cgls, k_lw, min_err_lw]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'x_spec': '1/j', 'delta': 1e-3}, # Case A\n        {'x_spec': '1',   'delta': 1e-3}, # Case B\n        {'x_spec': '1/j', 'delta': 1e-6}, # Case C\n        {'x_spec': '1',   'delta': 1e-2}, # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_one_case(case['x_spec'], case['delta'])\n        results.append(result)\n\n    # Format the final output string\n    formatted_results = []\n    for res in results:\n        k_cgls, err_cgls, k_lw, err_lw = res\n        s = f\"[{k_cgls},{err_cgls:.6e},{k_lw},{err_lw:.6e}]\"\n        formatted_results.append(s)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3392772"}]}