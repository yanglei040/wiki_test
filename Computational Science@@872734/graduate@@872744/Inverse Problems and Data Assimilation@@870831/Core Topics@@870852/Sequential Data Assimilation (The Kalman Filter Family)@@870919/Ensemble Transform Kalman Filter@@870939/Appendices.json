{"hands_on_practices": [{"introduction": "This practice addresses a fundamental challenge in ensemble filtering: the tendency for the ensemble to underestimate the true forecast error variance, a phenomenon known as ensemble collapse. A common remedy is covariance inflation, and this exercise introduces a practical scheme called Relaxation-to-Prior-Spread (RTPS). By working through this problem [@problem_id:3379787], you will derive the key relaxation parameter $\\gamma$, gaining hands-on insight into how inflation schemes are designed to maintain a healthy and realistic ensemble spread.", "problem": "Consider the Ensemble Transform Kalman Filter (ETKF), where the ensemble anomalies are defined to have zero ensemble mean. Let there be a single scalar state variable and an ensemble of size $m \\geq 3$. Denote the forecast anomalies across ensemble members by the row vector $X^{f} \\in \\mathbb{R}^{1 \\times m}$ and the analysis anomalies by $X^{a} \\in \\mathbb{R}^{1 \\times m}$. Define the sample variance and covariance across the ensemble by\n- $v_{f} = \\frac{1}{m-1} \\sum_{j=1}^{m} \\left(X^{f}_{1j}\\right)^{2}$,\n- $v_{a} = \\frac{1}{m-1} \\sum_{j=1}^{m} \\left(X^{a}_{1j}\\right)^{2}$,\n- $c = \\frac{1}{m-1} \\sum_{j=1}^{m} X^{a}_{1j} X^{f}_{1j}$,\nwhere the subscript $1j$ refers to the scalar state (row $1$) and ensemble member $j$, and the anomalies satisfy $\\sum_{j=1}^{m} X^{f}_{1j} = 0$ and $\\sum_{j=1}^{m} X^{a}_{1j} = 0$.\n\nIn the Relaxation to Prior Spread (RTPS) scheme, the relaxed analysis anomalies $X^{r}$ are obtained by a convex combination\n$$\nX^{r} \\leftarrow \\gamma X^{a} + \\left(1 - \\gamma\\right) X^{f},\n$$\nwith relaxation parameter $\\gamma \\in \\mathbb{R}$. Suppose the aim is to achieve a target analysis variance fraction $\\alpha \\in (0,1)$ relative to the forecast variance, meaning that the sample variance of $X^{r}$ equals $\\alpha v_{f}$.\n\nUsing only the fundamental definitions of sample variance and covariance given above, derive the scalar equation that $\\gamma$ must satisfy, solve it to obtain a closed-form expression for $\\gamma$ in terms of $v_{f}$, $v_{a}$, $c$, and $\\alpha$, and then evaluate $\\gamma$ for the specific values $v_{f} = 9$, $v_{a} = 4$, $c = 5$, and $\\alpha = \\frac{1}{2}$. Report the value of $\\gamma$ that lies in the interval $[0,1]$. Round your final numerical answer to $4$ significant figures.", "solution": "The problem is valid as it is scientifically grounded in the theory of data assimilation, is mathematically well-posed, and provides a complete and consistent set of definitions and values.\n\nThe first step is to derive an expression for the sample variance of the relaxed analysis anomalies, $X^{r}$. The relaxed anomalies are defined as a convex combination of the analysis anomalies $X^{a}$ and the forecast anomalies $X^{f}$:\n$$\nX^{r} = \\gamma X^{a} + (1 - \\gamma) X^{f}\n$$\nwhere $\\gamma$ is the relaxation parameter. For a single scalar state variable, the $j$-th component of this vector equation is:\n$$\nX^{r}_{1j} = \\gamma X^{a}_{1j} + (1 - \\gamma) X^{f}_{1j}\n$$\nThe problem states that the forecast and analysis anomalies have a zero ensemble mean, i.e., $\\sum_{j=1}^{m} X^{f}_{1j} = 0$ and $\\sum_{j=1}^{m} X^{a}_{1j} = 0$. We first verify that the relaxed anomalies $X^{r}$ also have a zero ensemble mean:\n$$\n\\sum_{j=1}^{m} X^{r}_{1j} = \\sum_{j=1}^{m} \\left( \\gamma X^{a}_{1j} + (1 - \\gamma) X^{f}_{1j} \\right) = \\gamma \\left(\\sum_{j=1}^{m} X^{a}_{1j}\\right) + (1 - \\gamma) \\left(\\sum_{j=1}^{m} X^{f}_{1j}\\right) = \\gamma(0) + (1 - \\gamma)(0) = 0\n$$\nSince the mean is zero, the sample variance of $X^{r}$, which we denote as $v_{r}$, is given by:\n$$\nv_{r} = \\frac{1}{m-1} \\sum_{j=1}^{m} \\left(X^{r}_{1j}\\right)^{2}\n$$\nSubstituting the expression for $X^{r}_{1j}$:\n$$\nv_{r} = \\frac{1}{m-1} \\sum_{j=1}^{m} \\left( \\gamma X^{a}_{1j} + (1 - \\gamma) X^{f}_{1j} \\right)^{2}\n$$\nWe expand the squared term:\n$$\nv_{r} = \\frac{1}{m-1} \\sum_{j=1}^{m} \\left[ \\gamma^{2} \\left(X^{a}_{1j}\\right)^{2} + (1 - \\gamma)^{2} \\left(X^{f}_{1j}\\right)^{2} + 2\\gamma(1 - \\gamma) X^{a}_{1j} X^{f}_{1j} \\right]\n$$\nBy distributing the summation and the factor $\\frac{1}{m-1}$, we can identify the given definitions for the sample variances and covariance:\n$$\nv_{r} = \\gamma^{2} \\left(\\frac{1}{m-1} \\sum_{j=1}^{m} \\left(X^{a}_{1j}\\right)^{2}\\right) + (1 - \\gamma)^{2} \\left(\\frac{1}{m-1} \\sum_{j=1}^{m} \\left(X^{f}_{1j}\\right)^{2}\\right) + 2\\gamma(1 - \\gamma) \\left(\\frac{1}{m-1} \\sum_{j=1}^{m} X^{a}_{1j} X^{f}_{1j}\\right)\n$$\nUsing the provided definitions $v_{a} = \\frac{1}{m-1} \\sum_{j=1}^{m} \\left(X^{a}_{1j}\\right)^{2}$, $v_{f} = \\frac{1}{m-1} \\sum_{j=1}^{m} \\left(X^{f}_{1j}\\right)^{2}$, and $c = \\frac{1}{m-1} \\sum_{j=1}^{m} X^{a}_{1j} X^{f}_{1j}$, the expression for $v_{r}$ becomes:\n$$\nv_{r} = \\gamma^{2} v_{a} + (1 - \\gamma)^{2} v_{f} + 2\\gamma(1 - \\gamma) c\n$$\nThe problem requires the variance of the relaxed anomalies to be a fraction $\\alpha$ of the forecast variance, i.e., $v_{r} = \\alpha v_{f}$. This gives the scalar equation that $\\gamma$ must satisfy:\n$$\n\\alpha v_{f} = \\gamma^{2} v_{a} + (1 - \\gamma)^{2} v_{f} + 2\\gamma(1 - \\gamma) c\n$$\nTo solve for $\\gamma$, we rearrange this equation into a standard quadratic form $A\\gamma^{2} + B\\gamma + C = 0$.\n$$\n\\alpha v_{f} = \\gamma^{2} v_{a} + (1 - 2\\gamma + \\gamma^{2}) v_{f} + (2\\gamma - 2\\gamma^{2}) c\n$$\n$$\n\\alpha v_{f} = \\gamma^{2} v_{a} + v_{f} - 2\\gamma v_{f} + \\gamma^{2} v_{f} + 2\\gamma c - 2\\gamma^{2} c\n$$\nCollecting terms based on powers of $\\gamma$:\n$$\n\\gamma^{2}(v_{a} + v_{f} - 2c) + \\gamma(-2v_{f} + 2c) + (v_{f} - \\alpha v_{f}) = 0\n$$\n$$\n(v_{f} + v_{a} - 2c)\\gamma^{2} + 2(c - v_{f})\\gamma + (1 - \\alpha)v_{f} = 0\n$$\nThe solution to this quadratic equation is given by the quadratic formula, which provides the closed-form expression for $\\gamma$:\n$$\n\\gamma = \\frac{-2(c - v_{f}) \\pm \\sqrt{(2(c - v_{f}))^{2} - 4(v_{f} + v_{a} - 2c)(1 - \\alpha)v_{f}}}{2(v_{f} + v_{a} - 2c)}\n$$\n$$\n\\gamma = \\frac{v_{f} - c \\pm \\sqrt{(v_{f} - c)^{2} - (v_{f} + v_{a} - 2c)(1 - \\alpha)v_{f}}}{v_{f} + v_{a} - 2c}\n$$\nNow we substitute the given numerical values: $v_{f} = 9$, $v_{a} = 4$, $c = 5$, and $\\alpha = \\frac{1}{2}$. First, we compute the coefficients of the quadratic equation:\n$A = v_{f} + v_{a} - 2c = 9 + 4 - 2(5) = 13 - 10 = 3$.\n$B = 2(c - v_{f}) = 2(5 - 9) = 2(-4) = -8$.\n$C = (1 - \\alpha)v_{f} = (1 - \\frac{1}{2})(9) = \\frac{9}{2} = 4.5$.\nThe quadratic equation for $\\gamma$ is:\n$$\n3\\gamma^{2} - 8\\gamma + 4.5 = 0\n$$\nUsing the quadratic formula to find the roots:\n$$\n\\gamma = \\frac{-(-8) \\pm \\sqrt{(-8)^{2} - 4(3)(4.5)}}{2(3)}\n$$\n$$\n\\gamma = \\frac{8 \\pm \\sqrt{64 - 54}}{6}\n$$\n$$\n\\gamma = \\frac{8 \\pm \\sqrt{10}}{6}\n$$\nThis yields two possible values for $\\gamma$:\n$$\n\\gamma_{1} = \\frac{8 + \\sqrt{10}}{6} \\quad \\text{and} \\quad \\gamma_{2} = \\frac{8 - \\sqrt{10}}{6}\n$$\nWe evaluate these to determine which one lies in the interval $[0,1]$. Using the approximation $\\sqrt{10} \\approx 3.16227766$:\n$$\n\\gamma_{1} \\approx \\frac{8 + 3.16227766}{6} = \\frac{11.16227766}{6} \\approx 1.860379\n$$\n$$\n\\gamma_{2} \\approx \\frac{8 - 3.16227766}{6} = \\frac{4.83772234}{6} \\approx 0.806287\n$$\nThe first root, $\\gamma_{1}$, is greater than $1$ and thus outside the required interval. The second root, $\\gamma_{2}$, is within the interval $[0,1]$. Therefore, the correct value for the relaxation parameter is $\\gamma = \\frac{8 - \\sqrt{10}}{6}$.\nThe problem asks for this value rounded to $4$ significant figures.\n$\\gamma \\approx 0.806287...$\nThe first four significant figures are $8$, $0$, $6$, $2$. The fifth significant digit is $8$, which is $5$ or greater, so we round up the fourth significant digit.\n$\\gamma \\approx 0.8063$.", "answer": "$$\n\\boxed{0.8063}\n$$", "id": "3379787"}, {"introduction": "Building on the concept of covariance inflation, this practice moves from a prescribed scheme to an adaptive one where the optimal inflation factor is determined from the data itself. You will use the powerful statistical principle of maximum likelihood to derive an update rule for a multiplicative inflation factor, $\\alpha$. This exercise [@problem_id:3379826] demonstrates how to formally connect the ETKF to statistical estimation theory, creating a filter that can diagnose and correct its own variance deficiencies in real-time.", "problem": "Consider a single linear-Gaussian data assimilation cycle for a state-space model with observation operator $H \\in \\mathbb{R}^{m \\times n}$, an ensemble forecast mean $\\bar{x}^f \\in \\mathbb{R}^n$, and an observation $y \\in \\mathbb{R}^m$. The observation error covariance is $R \\in \\mathbb{R}^{m \\times m}$, symmetric positive definite. Denote the innovation by $v = y - H \\bar{x}^f$ and the whitened innovation by $z = R^{-1/2} v$. Let $P^f \\in \\mathbb{R}^{n \\times n}$ denote the forecast error covariance represented by the ensemble. In the Ensemble Transform Kalman Filter (ETKF), a multiplicative inflation factor $\\alpha > 0$ scales the forecast covariance prior to analysis, $P^f \\mapsto \\alpha P^f$, equivalently scaling its projection in observation space.\n\nAssume that at this cycle the whitened forecast-observation covariance in observation space is isotropic, i.e.,\n$$\nA \\equiv R^{-1/2} H P^f H^{\\top} R^{-1/2} = c I_m,\n$$\nfor a known scalar $c > 0$ and identity $I_m$. Under multiplicative inflation $\\alpha$, the predicted whitened innovation covariance is\n$$\nS(\\alpha) = I_m + \\alpha A = \\left(1 + \\alpha c\\right) I_m.\n$$\nTreat the whitened innovation $z$ as Gaussian with mean $0$ and covariance $S(\\alpha)$, and consider the Gaussian log-likelihood\n$$\n\\ell(\\alpha \\, ; \\, z) = -\\tfrac{1}{2}\\Big( m \\ln(2\\pi) + \\ln \\det S(\\alpha) + z^{\\top} S(\\alpha)^{-1} z \\Big).\n$$\nDesign an adaptive multiplicative inflation $\\alpha$ by maximizing the expected log-likelihood of innovations $z$ under the above Gaussian model, and derive a closed-form update for $\\alpha$ that is compatible with the ETKF transform (that is, it ensures the symmetric matrix to be square-rooted in the ETKF analysis step remains positive definite). Then evaluate this update numerically for the following concrete instance:\n- Observation dimension $m = 3$.\n- Observation error covariance $R = \\mathrm{diag}(1, 4, 9)$.\n- Innovation $v = \\begin{pmatrix} 1.0 \\\\ 2.0 \\\\ 5.020 \\end{pmatrix}$.\n- Isotropy level $c = \\tfrac{1}{2}$, so that $A = \\tfrac{1}{2} I_3$.\n\nAdditionally, test the stability of your maximizer by verifying that the second derivative of the log-likelihood at your solution is negative and that the ETKF transform remains real-valued. Report only the numerical value of the inflation factor $\\alpha$ for this instance. Round your answer to three significant figures. The inflation factor is dimensionless; do not include units in your final answer.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the theory of data assimilation and the Ensemble Transform Kalman Filter (ETKF), is mathematically well-posed, objective, and self-contained. The objective of deriving an adaptive inflation factor by maximizing a likelihood function is a standard procedure in statistical estimation. All necessary data and definitions are provided.\n\nThe objective is to find the multiplicative inflation factor $\\alpha > 0$ that maximizes the Gaussian log-likelihood $\\ell(\\alpha \\, ; \\, z)$ for a given whitened innovation $z$. The log-likelihood function is given by:\n$$\n\\ell(\\alpha \\, ; \\, z) = -\\frac{1}{2}\\Big( m \\ln(2\\pi) + \\ln \\det S(\\alpha) + z^{\\top} S(\\alpha)^{-1} z \\Big)\n$$\nThe problem specifies the structure of the predicted whitened innovation covariance $S(\\alpha)$ under the assumption of an isotropic whitened forecast-observation covariance $A = c I_m$:\n$$\nS(\\alpha) = I_m + \\alpha A = I_m + \\alpha (c I_m) = (1 + \\alpha c) I_m\n$$\nwhere $I_m$ is the $m \\times m$ identity matrix.\n\nWe can now express the terms in the log-likelihood as functions of $\\alpha$.\nThe determinant term is:\n$$\n\\ln \\det S(\\alpha) = \\ln \\det((1 + \\alpha c) I_m) = \\ln((1 + \\alpha c)^m) = m \\ln(1 + \\alpha c)\n$$\nThe inverse of $S(\\alpha)$ is:\n$$\nS(\\alpha)^{-1} = ((1 + \\alpha c) I_m)^{-1} = \\frac{1}{1 + \\alpha c} I_m\n$$\nThe quadratic form in the log-likelihood becomes:\n$$\nz^{\\top} S(\\alpha)^{-1} z = z^{\\top} \\left(\\frac{1}{1 + \\alpha c} I_m\\right) z = \\frac{1}{1 + \\alpha c} z^{\\top}z\n$$\nSubstituting these into the log-likelihood expression, we get $\\ell$ as a function of $\\alpha$:\n$$\n\\ell(\\alpha) = -\\frac{1}{2}\\left( m \\ln(2\\pi) + m \\ln(1 + \\alpha c) + \\frac{z^{\\top}z}{1 + \\alpha c} \\right)\n$$\nTo maximize $\\ell(\\alpha)$, we take its derivative with respect to $\\alpha$ and set it to zero. For the maximization to be well-defined, we must have $1+\\alpha c > 0$. Given $\\alpha>0$ and $c>0$, this condition is satisfied.\n$$\n\\frac{d\\ell}{d\\alpha} = -\\frac{1}{2} \\left( m \\frac{c}{1 + \\alpha c} - \\frac{c(z^{\\top}z)}{(1 + \\alpha c)^2} \\right)\n$$\nSetting the derivative to zero:\n$$\n-\\frac{1}{2} \\left( \\frac{mc}{1 + \\alpha c} - \\frac{c(z^{\\top}z)}{(1 + \\alpha c)^2} \\right) = 0\n$$\nSince $c > 0$, we can simplify:\n$$\n\\frac{m}{1 + \\alpha c} = \\frac{z^{\\top}z}{(1 + \\alpha c)^2}\n$$\nMultiplying both sides by $(1 + \\alpha c)^2$ (which is non-zero) yields:\n$$\nm(1 + \\alpha c) = z^{\\top}z\n$$\nSolving for $\\alpha$:\n$$\n1 + \\alpha c = \\frac{z^{\\top}z}{m}\n$$\n$$\n\\alpha c = \\frac{z^{\\top}z}{m} - 1\n$$\n$$\n\\alpha = \\frac{1}{c} \\left( \\frac{z^{\\top}z}{m} - 1 \\right)\n$$\nThis is the closed-form update for $\\alpha$ derived by maximizing the log-likelihood.\n\nWe must verify the stability of this solution by checking the second derivative.\n$$\n\\frac{d^2\\ell}{d\\alpha^2} = -\\frac{1}{2} \\frac{d}{d\\alpha} \\left( \\frac{mc}{1 + \\alpha c} - \\frac{c(z^{\\top}z)}{(1 + \\alpha c)^2} \\right) = -\\frac{1}{2} \\left( -\\frac{mc^2}{(1 + \\alpha c)^2} + \\frac{2c^2(z^{\\top}z)}{(1 + \\alpha c)^3} \\right)\n$$\n$$\n\\frac{d^2\\ell}{d\\alpha^2} = \\frac{c^2}{2(1 + \\alpha c)^3} \\left( m(1 + \\alpha c) - 2z^{\\top}z \\right)\n$$\nAt the solution point, we have $m(1 + \\alpha c) = z^{\\top}z$. Substituting this into the second derivative:\n$$\n\\frac{d^2\\ell}{d\\alpha^2}\\Big|_{\\alpha=\\alpha^*} = \\frac{c^2}{2(1 + \\alpha^* c)^3} \\left( z^{\\top}z - 2z^{\\top}z \\right) = -\\frac{c^2 z^{\\top}z}{2(1 + \\alpha^* c)^3}\n$$\nAt the solution, we also have $1 + \\alpha^* c = z^{\\top}z/m$. For any non-zero innovation vector $z$, $z^{\\top}z > 0$, which implies $1 + \\alpha^* c > 0$. Since $c > 0$, we have $c^2 > 0$. Therefore, the second derivative is negative, confirming that the solution is indeed a local maximum.\n\nThe derived update is compatible with the ETKF transform if the matrix that is square-rooted, $(I_m + \\alpha A)^{-1/2}$, is real. This requires $I_m + \\alpha A$ to be positive definite. In this case, $I_m + \\alpha A = (1+\\alpha c)I_m$, which is positive definite if $1+\\alpha c > 0$. At our solution, $1+\\alpha c = z^{\\top}z/m$. Since $z$ is a real vector, $z^{\\top}z \\ge 0$. As long as $z$ is not the zero vector, $z^{\\top}z > 0$ and the condition is satisfied.\nThe problem also specifies $\\alpha>0$. From our formula, this requires $\\frac{z^{\\top}z}{m} - 1 > 0$, or $z^{\\top}z > m$. This means inflation is applied only when the observed whitened innovation variance, represented by $z^{\\top}z$, exceeds its expected value under the uninflated model ($\\mathbb{E}[z^{\\top}z] = \\text{tr}(I_m) = m$).\n\nNow, we evaluate this update for the given numerical instance:\n- $m = 3$\n- $R = \\mathrm{diag}(1, 4, 9)$\n- $v = \\begin{pmatrix} 1.0 \\\\ 2.0 \\\\ 5.020 \\end{pmatrix}$\n- $c = \\frac{1}{2}$\n\nFirst, we compute the whitened innovation $z = R^{-1/2} v$. The matrix $R^{-1/2}$ is the diagonal matrix whose entries are the reciprocal of the square roots of the entries of $R$:\n$$\nR^{1/2} = \\mathrm{diag}(\\sqrt{1}, \\sqrt{4}, \\sqrt{9}) = \\mathrm{diag}(1, 2, 3)\n$$\n$$\nR^{-1/2} = \\mathrm{diag}(1^{-1}, 2^{-1}, 3^{-1}) = \\mathrm{diag}(1, \\tfrac{1}{2}, \\tfrac{1}{3}) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 \\\\ 0 & 0 & \\frac{1}{3} \\end{pmatrix}\n$$\nNow we compute $z$:\n$$\nz = R^{-1/2} v = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 \\\\ 0 & 0 & \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} 1.0 \\\\ 2.0 \\\\ 5.020 \\end{pmatrix} = \\begin{pmatrix} 1.0 \\times 1 \\\\ 2.0 \\times \\frac{1}{2} \\\\ 5.020 \\times \\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix} 1.0 \\\\ 1.0 \\\\ \\frac{5.020}{3} \\end{pmatrix}\n$$\nNext, we compute the squared norm $z^{\\top}z$:\n$$\nz^{\\top}z = (1.0)^2 + (1.0)^2 + \\left(\\frac{5.020}{3}\\right)^2 = 1.0 + 1.0 + \\frac{25.2004}{9} = 2.0 + \\frac{25.2004}{9}\n$$\n$$\nz^{\\top}z = \\frac{18.0}{9} + \\frac{25.2004}{9} = \\frac{43.2004}{9}\n$$\nThe value is approximately $z^{\\top}z \\approx 4.800044...$\n\nNow we substitute the values of $m$, $c$, and $z^{\\top}z$ into the formula for $\\alpha$:\n$$\n\\alpha = \\frac{1}{c} \\left( \\frac{z^{\\top}z}{m} - 1 \\right) = \\frac{1}{1/2} \\left( \\frac{43.2004/9}{3} - 1 \\right)\n$$\n$$\n\\alpha = 2 \\left( \\frac{43.2004}{27} - 1 \\right) = 2 \\left( \\frac{43.2004 - 27}{27} \\right) = 2 \\left( \\frac{16.2004}{27} \\right)\n$$\n$$\n\\alpha = \\frac{32.4008}{27} \\approx 1.2000296...\n$$\nThe condition $\\alpha > 0$ is satisfied, as $z^{\\top}z \\approx 4.8 > m=3$.\nRounding the answer to three significant figures, we get $1.20$.", "answer": "$$\\boxed{1.20}$$", "id": "3379826"}, {"introduction": "In high-dimensional systems, a finite-sized ensemble inevitably generates spurious correlations between distant variables, which can degrade the filter's performance. Covariance localization is the essential technique used to mitigate this problem by tapering the influence of observations based on distance. This exercise [@problem_id:3379789] provides a concrete, hands-on derivation of how localization is implemented through a local observation operator, $H_{\\text{loc}}$, and how it directly modifies the ETKF transform matrix, $T$.", "problem": "Consider the Ensemble Transform Kalman Filter (ETKF), which is a square-root implementation of the Kalman filter update in the ensemble subspace. You are given a one-dimensional grid consisting of three equally spaced points at positions $x = -\\Delta$, $x = 0$, and $x = \\Delta$. A single scalar observation is located at $x = 0$, and the instrument has a Gaussian footprint with localization radius $r$, such that only the nearest neighbors contribute. The effective local observation operator $H_{\\text{loc}}$ maps the state $x \\in \\mathbb{R}^{3}$ to the observation by a normalized Gaussian weighting over these three grid points:\n$$\nw_{-} = \\exp\\!\\left(-\\frac{\\Delta^{2}}{2 r^{2}}\\right), \\quad w_{0} = \\exp(0) = 1, \\quad w_{+} = \\exp\\!\\left(-\\frac{\\Delta^{2}}{2 r^{2}}\\right),\n$$\nand\n$$\nH_{\\text{loc}} = \\frac{1}{w_{0} + w_{-} + w_{+}}\\begin{pmatrix} w_{-} & w_{0} & w_{+} \\end{pmatrix}.\n$$\nYou have an ensemble of $m = 3$ forecast members. Let $X^{f} \\in \\mathbb{R}^{3 \\times 3}$ denote the forecast anomalies (columns sum to the zero vector), and the scaled anomalies matrix used by ETKF be $A^{f} = X^{f}/\\sqrt{m-1}$. The observational error covariance is scalar $R = \\sigma^{2}$. Assume the forecast anomalies are\n$$\nX^{f} = \\begin{pmatrix}\n1 & -1 & 0 \\\\\n0 & 1 & -1 \\\\\n-1 & 0 & 1\n\\end{pmatrix}.\n$$\nStarting from first principles of the Kalman filter and the definition of the ensemble transform, do the following:\n\n1. Derive the explicit dependence of $H_{\\text{loc}}$ on the localization radius $r$ and grid spacing $\\Delta$.\n2. Compute the local projected ensemble anomalies in observation space,\n$$\nY^{f} = H_{\\text{loc}}\\,A^{f} \\in \\mathbb{R}^{1 \\times 3}.\n$$\n3. Derive the symmetric ETKF transform $T \\in \\mathbb{R}^{3 \\times 3}$ in the ensemble space that maps forecast anomalies to analysis anomalies, by enforcing that the analysis covariance in the ensemble subspace matches the Kalman posterior covariance under the given single observation and error covariance $R$.\n\nYour final answer must be a single closed-form analytic expression for the determinant of the transform matrix $T$ as a function of $r$, $\\Delta$, and $\\sigma$. No rounding is required. Express the final answer without units.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Grid points: $x = -\\Delta$, $x = 0$, $x = \\Delta$.\n- State vector: $x \\in \\mathbb{R}^{3}$.\n- Single scalar observation at $x=0$.\n- Gaussian localization footprint with radius $r$.\n- Weights for the local observation operator: $w_{-} = \\exp(-\\frac{\\Delta^{2}}{2 r^{2}})$, $w_{0} = 1$, $w_{+} = \\exp(-\\frac{\\Delta^{2}}{2 r^{2}})$.\n- Local observation operator: $H_{\\text{loc}} = \\frac{1}{w_{0} + w_{-} + w_{+}}\\begin{pmatrix} w_{-} & w_{0} & w_{+} \\end{pmatrix}$.\n- Ensemble size: $m = 3$.\n- Forecast anomalies matrix: $X^{f} \\in \\mathbb{R}^{3 \\times 3}$, $X^{f} = \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\\\ -1 & 0 & 1 \\end{pmatrix}$.\n- Scaled forecast anomalies matrix: $A^{f} = X^{f}/\\sqrt{m-1}$.\n- Observational error covariance: $R = \\sigma^{2}$.\n- Task: Derive the symmetric Ensemble Transform Kalman Filter (ETKF) transform matrix $T$ and find its determinant, $\\det(T)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem uses standard definitions and concepts from data assimilation, specifically the ETKF with localization. The mathematical setup is consistent with established literature.\n- **Well-Posed:** All necessary information is provided. The number of ensemble members ($m=3$) equals the state dimension, and the anomaly matrix $X^f$ is correctly specified such that its columns sum to the zero vector, which is a requirement for anomaly matrices. The problem requests a specific, derivable quantity.\n- **Objective:** The problem is stated using precise mathematical language, free from ambiguity or subjective elements.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution Derivation**\n\nThe solution proceeds in three parts as requested by the problem statement.\n\n**1. Explicit form of $H_{\\text{loc}}$**\n\nThe local observation operator $H_{\\text{loc}}$ is defined by a set of normalized weights. Let's define the common weight for the off-center points as\n$$w = w_{-} = w_{+} = \\exp\\left(-\\frac{\\Delta^{2}}{2 r^{2}}\\right).$$\nThe center weight is $w_{0} = 1$. The sum of the weights, which serves as the normalization factor, is\n$$S = w_{0} + w_{-} + w_{+} = 1 + 2w = 1 + 2\\exp\\left(-\\frac{\\Delta^{2}}{2 r^{2}}\\right).$$\nThus, the local observation operator $H_{\\text{loc}} \\in \\mathbb{R}^{1 \\times 3}$ has the explicit form:\n$$H_{\\text{loc}} = \\frac{1}{S} \\begin{pmatrix} w & 1 & w \\end{pmatrix} = \\frac{1}{1 + 2\\exp\\left(-\\frac{\\Delta^{2}}{2 r^{2}}\\right)} \\begin{pmatrix} \\exp\\left(-\\frac{\\Delta^{2}}{2 r^{2}}\\right) & 1 & \\exp\\left(-\\frac{\\Delta^{2}}{2 r^{2}}\\right) \\end{pmatrix}.$$\n\n**2. Computation of Projected Ensemble Anomalies $Y^{f}$**\n\nFirst, we compute the scaled forecast anomalies matrix $A^{f}$. Given the ensemble size $m=3$, the scaling factor is $1/\\sqrt{m-1} = 1/\\sqrt{2}$.\n$$A^{f} = \\frac{X^{f}}{\\sqrt{m-1}} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\\\ -1 & 0 & 1 \\end{pmatrix}.$$\nNext, we project these scaled anomalies into observation space using $H_{\\text{loc}}$ to get $Y^{f} = H_{\\text{loc}} A^{f} \\in \\mathbb{R}^{1 \\times 3}$.\n$$Y^{f} = \\frac{1}{S\\sqrt{2}} \\begin{pmatrix} w & 1 & w \\end{pmatrix} \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\\\ -1 & 0 & 1 \\end{pmatrix}.$$\nPerforming the matrix multiplication:\n$$Y^{f} = \\frac{1}{S\\sqrt{2}} \\begin{pmatrix} (w)(1) + (1)(0) + (w)(-1) & (w)(-1) + (1)(1) + (w)(0) & (w)(0) + (1)(-1) + (w)(1) \\end{pmatrix}$$\n$$Y^{f} = \\frac{1}{S\\sqrt{2}} \\begin{pmatrix} 0 & 1-w & w-1 \\end{pmatrix}.$$\n\n**3. Derivation of the Transform Matrix $T$ and its Determinant**\n\nThe ETKF updates the scaled forecast anomalies $A^{f}$ to the scaled analysis anomalies $A^{a}$ via a linear transformation in the ensemble space: $A^{a} = A^{f} T$. The symmetric transform matrix $T \\in \\mathbb{R}^{3 \\times 3}$ is chosen such that the analysis covariance $P^{a} = A^{a}(A^{a})^{T}$ matches the theoretical Kalman posterior covariance.\n\nThe analysis error covariance in the ensemble space, denoted $P^{a}_{\\text{ens}}$, is given by the formula:\n$$P^{a}_{\\text{ens}} = \\left( I + (Y^{f})^{T} R^{-1} Y^{f} \\right)^{-1},$$\nwhere $I$ is the $m \\times m$ identity matrix ($3 \\times 3$ here). The symmetric transform matrix $T$ is the principal square root of $P^{a}_{\\text{ens}}$:\n$$T = (P^{a}_{\\text{ens}})^{1/2} = \\left( I + (Y^{f})^{T} R^{-1} Y^{f} \\right)^{-1/2}.$$\nOur goal is to find $\\det(T)$. Using the property $\\det(M^{a}) = (\\det(M))^{a}$:\n$$\\det(T) = \\det\\left( \\left( I + (Y^{f})^{T} R^{-1} Y^{f} \\right)^{-1/2} \\right) = \\left( \\det\\left(I + (Y^{f})^{T} R^{-1} Y^{f} \\right) \\right)^{-1/2}.$$\nLet's compute the matrix $M = (Y^{f})^{T} R^{-1} Y^{f}$. The observation error covariance is $R=\\sigma^2$, so its inverse is $R^{-1} = 1/\\sigma^2$.\n$$(Y^{f})^{T} = \\frac{1}{S\\sqrt{2}} \\begin{pmatrix} 0 \\\\ 1-w \\\\ w-1 \\end{pmatrix}.$$\n$$M = \\left( \\frac{1}{S\\sqrt{2}} \\begin{pmatrix} 0 \\\\ 1-w \\\\ w-1 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sigma^2} \\right) \\left( \\frac{1}{S\\sqrt{2}} \\begin{pmatrix} 0 & 1-w & w-1 \\end{pmatrix} \\right)$$\n$$M = \\frac{1}{2S^2\\sigma^2} \\begin{pmatrix} 0 \\\\ 1-w \\\\ w-1 \\end{pmatrix} \\begin{pmatrix} 0 & 1-w & w-1 \\end{pmatrix} = \\frac{1}{2S^2\\sigma^2} \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & (1-w)^2 & -(1-w)^2 \\\\ 0 & -(1-w)^2 & (1-w)^2 \\end{pmatrix}.$$\nLet's define a scalar constant $\\alpha$ to simplify the notation:\n$$\\alpha = \\frac{(1-w)^2}{2S^2\\sigma^2}.$$\nThe matrix $M$ can be written as:\n$$M = \\alpha \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix}.$$\nNow we compute the determinant of $I+M$:\n$$I+M = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & \\alpha & -\\alpha \\\\ 0 & -\\alpha & \\alpha \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1+\\alpha & -\\alpha \\\\ 0 & -\\alpha & 1+\\alpha \\end{pmatrix}.$$\n$$\\det(I+M) = 1 \\cdot \\det\\begin{pmatrix} 1+\\alpha & -\\alpha \\\\ -\\alpha & 1+\\alpha \\end{pmatrix} - 0 + 0 = (1+\\alpha)^2 - (-\\alpha)^2 = 1+2\\alpha+\\alpha^2-\\alpha^2 = 1+2\\alpha.$$\nNow we find $\\det(T)$:\n$$\\det(T) = (1+2\\alpha)^{-1/2}.$$\nSubstituting the expression for $\\alpha$:\n$$1+2\\alpha = 1 + 2\\left(\\frac{(1-w)^2}{2S^2\\sigma^2}\\right) = 1 + \\frac{(1-w)^2}{S^2\\sigma^2}.$$\nFinally, we substitute the expressions for $w$ and $S$ in terms of the problem parameters $r$, $\\Delta$, and $\\sigma$:\n$$w = \\exp\\left(-\\frac{\\Delta^{2}}{2r^{2}}\\right)$$\n$$S = 1+2w = 1+2\\exp\\left(-\\frac{\\Delta^{2}}{2r^{2}}\\right)$$\nThe determinant of the transform matrix $T$ is:\n$$\\det(T) = \\left( 1 + \\frac{\\left(1-\\exp\\left(-\\frac{\\Delta^{2}}{2r^{2}}\\right)\\right)^2}{\\left(1+2\\exp\\left(-\\frac{\\Delta^{2}}{2r^{2}}\\right)\\right)^2 \\sigma^2} \\right)^{-1/2}.$$\nThis is the required closed-form analytic expression for the determinant of $T$.", "answer": "$$\n\\boxed{\\left( 1 + \\frac{\\left(1-\\exp\\left(-\\frac{\\Delta^{2}}{2r^{2}}\\right)\\right)^{2}}{\\left(1+2\\exp\\left(-\\frac{\\Delta^{2}}{2r^{2}}\\right)\\right)^{2} \\sigma^{2}} \\right)^{-1/2}}\n$$", "id": "3379789"}]}