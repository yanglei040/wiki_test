{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we will start with a foundational exercise that cleanly isolates and quantifies how sampling error propagates through a system. In this practice, you will derive a maximum likelihood estimator for the model's process noise variance ($Q$) within a simple scalar data assimilation framework. The key challenge lies in the fact that the background error variance ($P$) is itself estimated from a finite ensemble, introducing a known sampling bias. By working through this derivation [@problem_id:3418800], you will gain a concrete understanding of how the underestimation of one variance component leads to a systematic overestimation of another, a fundamental mechanism of bias propagation in ensemble methods.", "problem": "Consider a scalar linear-Gaussian data assimilation setting in which the state evolves according to a random-walk model $x_{k+1} = x_{k} + w_{k}$ with process noise $w_{k} \\sim \\mathcal{N}(0,Q)$, and observations are given by $y_{k} = x_{k} + \\epsilon_{k}$ with observation noise $\\epsilon_{k} \\sim \\mathcal{N}(0,R)$, where $Q > 0$ and $R > 0$ are unknown and known variances, respectively. Assume an unbiased forecast so that the innovation $v_{k} = y_{k} - \\hat{x}_{k}^{-}$ is mean-zero. In steady conditions for this scalar model, the innovation variance is the sum of the forecast error variance and the observation noise variance, and can be written as $S(Q) = P + Q + R$, where $P$ denotes the forecast error variance carried into the current cycle from the previous analysis.\n\nSuppose that $M$ sequential innovations $\\{v_{1},\\dots,v_{M}\\}$ are collected and modeled as independent and identically distributed draws from a Gaussian distribution with zero mean and variance $S(Q)$. The goal is to construct a Maximum Likelihood Estimator (MLE) for the process variance $Q$ using the innovations, when the forecast error variance $P$ is replaced by an ensemble covariance estimator $\\hat{P}$ computed from $N$ ensemble members $\\{x^{(i)}\\}_{i=1}^{N}$. The ensemble covariance is formed using the sample mean $\\bar{x} = \\frac{1}{N}\\sum_{i=1}^{N}x^{(i)}$ and the Maximum Likelihood (ML) normalization,\n$$\n\\hat{P} = \\frac{1}{N}\\sum_{i=1}^{N}\\left(x^{(i)} - \\bar{x}\\right)^{2}.\n$$\nUnder the Gaussian assumption for the ensemble, the distribution of the scaled covariance $\\frac{N\\,\\hat{P}}{P}$ is Wishart with degrees of freedom $N-1$, and hence exhibits sampling error due to finite $N$.\n\nStarting from the definitions of the Gaussian likelihood for the innovations and the canonical moment properties of the Wishart distribution for sample covariance matrices, derive the MLE for $Q$ based on the innovations when $P$ is replaced by $\\hat{P}$, and then analyze the bias of this estimator to first order in $1/N$. Your derivation must start from well-tested facts about Gaussian likelihoods and sample covariance properties, without invoking any pre-derived shortcut formula for the estimator. Assume that the mean of the innovations is known to be zero. The final answer must be a single closed-form analytic expression for the leading $O(1/N)$ term of the bias of the $Q$ estimator. No rounding is required and no units should be provided.", "solution": "The problem requires the derivation of the Maximum Likelihood Estimator (MLE) for the process noise variance $Q$ and an analysis of its bias. The solution proceeds in two parts: first, the derivation of the estimator $\\hat{Q}_{MLE}$, and second, the calculation of its bias to first order in $1/N$.\n\n### Step 1: Extract Givens\n- State evolution model: $x_{k+1} = x_{k} + w_{k}$\n- Process noise: $w_{k} \\sim \\mathcal{N}(0,Q)$, with $Q > 0$ being an unknown variance.\n- Observation model: $y_{k} = x_{k} + \\epsilon_{k}$\n- Observation noise: $\\epsilon_{k} \\sim \\mathcal{N}(0,R)$, with $R > 0$ being a known variance.\n- Innovation: $v_{k} = y_{k} - \\hat{x}_{k}^{-}$\n- Innovation modeling: A set of $M$ innovations $\\{v_{1},\\dots,v_{M}\\}$ is modeled as independent and identically distributed (i.i.d.) draws from a Gaussian distribution $\\mathcal{N}(0, S(Q))$ with a known zero mean.\n- True innovation variance (given): $S(Q) = P + Q + R$, where $P$ is the forecast error variance carried into the current cycle from the previous analysis.\n- Ensemble-based estimation: The true variance $P$ is replaced by an ensemble covariance estimator $\\hat{P}$ computed from $N$ ensemble members $\\{x^{(i)}\\}_{i=1}^{N}$.\n- Ensemble covariance estimator definition: $\\hat{P} = \\frac{1}{N}\\sum_{i=1}^{N}\\left(x^{(i)} - \\bar{x}\\right)^{2}$, with $\\bar{x} = \\frac{1}{N}\\sum_{i=1}^{N}x^{(i)}$. This is the ML form of the sample variance.\n- Statistical property of $\\hat{P}$: The scaled sample covariance $\\frac{N\\,\\hat{P}}{P}$ follows a Wishart distribution with $N-1$ degrees of freedom. In this scalar case, this is equivalent to a chi-squared distribution, $\\frac{N\\,\\hat{P}}{P} \\sim \\chi^2_{N-1}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, objective, and well-posed. It describes a standard problem in data assimilation concerning parameter estimation in the presence of sampling uncertainty. The use of an ensemble to estimate background error covariance and the statistical modeling of innovations are central concepts in methods like the Ensemble Kalman Filter.\n\nThere is a potential ambiguity in the physical interpretation of the terms. Standard Kalman filter theory for the given state model implies the forecast error variance $P_f$ is a sum of the propagated analysis error variance $P_a$ and the process noise variance $Q$, i.e., $P_f = P_a + Q$. The innovation variance would then be $S = P_f + R = P_a + Q + R$. The problem statement gives the innovation variance as $S(Q) = P + Q + R$, which suggests that $P$ is intended to represent the analysis error variance, $P_a$. However, the estimator $\\hat{P}$ is computed from an ensemble of forecasts, meaning it is an estimator for the forecast error variance, $P_f$.\n\nDespite this physical ambiguity, the problem is mathematically self-consistent. We are given the explicit formula for innovation variance to use ($S(Q)=P+Q+R$) and a specific statistical relationship between an estimator $\\hat{P}$ and the parameter $P$ appearing in that formula ($\\frac{N\\,\\hat{P}}{P} \\sim \\chi^2_{N-1}$). The task is to follow the consequences of these mathematical definitions. We must proceed by assuming that $\\hat{P}$ is used as an (perhaps imperfect) estimate for the quantity $P$ in the given formula for $S(Q)$. The problem is therefore deemed valid as a formal mathematical exercise.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed to the solution.\n\n### Part A: Derivation of the Maximum Likelihood Estimator for $Q$\n\nThe MLE for $Q$ is constructed based on the provided i.i.d. innovations $\\{v_1, \\dots, v_M\\}$. The problem states that for the purpose of estimation, the true but unknown forecast error variance $P$ is replaced by its ensemble estimate $\\hat{P}$. Therefore, the variance of the innovations in our statistical model is treated as $S_{model}(Q) = \\hat{P} + Q + R$.\n\nGiven that the innovations $\\{v_k\\}$ are i.i.d. samples from a Gaussian distribution with mean $0$ and variance $S_{model}(Q)$, the likelihood function is:\n$$\nL(Q; \\{v_k\\}, \\hat{P}, R) = \\prod_{k=1}^{M} \\frac{1}{\\sqrt{2\\pi (\\hat{P} + Q + R)}} \\exp\\left(-\\frac{v_k^2}{2(\\hat{P} + Q + R)}\\right)\n$$\nIt is more convenient to work with the log-likelihood function, $\\ell(Q) = \\ln L(Q)$:\n$$\n\\ell(Q) = \\sum_{k=1}^{M} \\left[ -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\hat{P} + Q + R) - \\frac{v_k^2}{2(\\hat{P} + Q + R)} \\right]\n$$\n$$\n\\ell(Q) = -\\frac{M}{2}\\ln(2\\pi) - \\frac{M}{2}\\ln(\\hat{P} + Q + R) - \\frac{1}{2(\\hat{P} + Q + R)}\\sum_{k=1}^{M}v_k^2\n$$\nTo find the MLE, we differentiate $\\ell(Q)$ with respect to $Q$ and set the result to zero.\n$$\n\\frac{d\\ell}{dQ} = -\\frac{M}{2} \\frac{1}{\\hat{P} + Q + R} + \\frac{1}{2(\\hat{P} + Q + R)^2} \\sum_{k=1}^{M}v_k^2 = 0\n$$\nMultiplying by $2(\\hat{P} + Q + R)^2$ (assuming the variance is positive) gives:\n$$\n-M(\\hat{P} + Q + R) + \\sum_{k=1}^{M}v_k^2 = 0\n$$\nLet $\\hat{S}_{innov} = \\frac{1}{M}\\sum_{k=1}^{M}v_k^2$ denote the sample variance of the innovations. The equation becomes:\n$$\n-M(\\hat{P} + Q + R) + M\\hat{S}_{innov} = 0\n$$\n$$\n\\hat{P} + Q + R = \\hat{S}_{innov}\n$$\nSolving for $Q$ yields the Maximum Likelihood Estimator, $\\hat{Q}_{MLE}$:\n$$\n\\hat{Q}_{MLE} = \\hat{S}_{innov} - \\hat{P} - R\n$$\n\n### Part B: Analysis of the Bias of $\\hat{Q}_{MLE}$\n\nThe bias of an estimator is the difference between its expected value and the true value of the parameter being estimated. Here, the bias of $\\hat{Q}_{MLE}$ is given by:\n$$\n\\text{Bias}(\\hat{Q}_{MLE}) = \\mathbb{E}[\\hat{Q}_{MLE}] - Q\n$$\nThe expectation $\\mathbb{E}[\\cdot]$ must be taken with respect to the true data-generating process. This involves averaging over the distribution of the ensemble members (which determines $\\hat{P}$) and the distribution of the innovations (which determines $\\hat{S}_{innov}$).\n\nSubstituting the expression for $\\hat{Q}_{MLE}$:\n$$\n\\text{Bias}(\\hat{Q}_{MLE}) = \\mathbb{E}[\\hat{S}_{innov} - \\hat{P} - R] - Q\n$$\nUsing the linearity of the expectation operator:\n$$\n\\text{Bias}(\\hat{Q}_{MLE}) = \\mathbb{E}[\\hat{S}_{innov}] - \\mathbb{E}[\\hat{P}] - \\mathbb{E}[R] - Q\n$$\nSince $R$ is a known constant, $\\mathbb{E}[R] = R$. The problem does not state any dependence between the generation of the ensemble and the innovations, so we can treat $\\hat{P}$ and $\\hat{S}_{innov}$ as independent random variables for the purpose of calculating their expectations.\n\n1.  **Calculate $\\mathbb{E}[\\hat{P}]$:**\n    We are given that $\\frac{N\\hat{P}}{P} \\sim \\chi^2_{N-1}$. The expected value of a chi-squared random variable with $k$ degrees of freedom is $k$. Therefore:\n    $$\n    \\mathbb{E}\\left[\\frac{N\\hat{P}}{P}\\right] = N-1\n    $$\n    $$\n    \\frac{N}{P}\\mathbb{E}[\\hat{P}] = N-1 \\implies \\mathbb{E}[\\hat{P}] = \\frac{N-1}{N}P = \\left(1 - \\frac{1}{N}\\right)P\n    $$\n    This shows that $\\hat{P}$ is a biased estimator of $P$.\n\n2.  **Calculate $\\mathbb{E}[\\hat{S}_{innov}]$:**\n    The innovations $\\{v_k\\}$ are i.i.d. draws from the *true* distribution, which is $\\mathcal{N}(0, S(Q))$ with $S(Q) = P + Q + R$. The sample variance is $\\hat{S}_{innov} = \\frac{1}{M}\\sum_{k=1}^{M}v_k^2$. Its expectation is:\n    $$\n    \\mathbb{E}[\\hat{S}_{innov}] = \\mathbb{E}\\left[\\frac{1}{M}\\sum_{k=1}^{M}v_k^2\\right] = \\frac{1}{M}\\sum_{k=1}^{M}\\mathbb{E}[v_k^2]\n    $$\n    For a random variable $v_k$ from a distribution with mean $0$ and variance $S(Q)$, its expected square is $\\mathbb{E}[v_k^2] = \\text{Var}(v_k) + (\\mathbb{E}[v_k])^2 = S(Q) + 0^2 = S(Q)$.\n    $$\n    \\mathbb{E}[\\hat{S}_{innov}] = \\frac{1}{M}\\sum_{k=1}^{M}S(Q) = \\frac{M \\cdot S(Q)}{M} = S(Q) = P + Q + R\n    $$\n\n3.  **Combine the terms to find the bias:**\n    Substituting the expressions for $\\mathbb{E}[\\hat{P}]$ and $\\mathbb{E}[\\hat{S}_{innov}]$ into the bias formula:\n    $$\n    \\text{Bias}(\\hat{Q}_{MLE}) = (P + Q + R) - \\left(1 - \\frac{1}{N}\\right)P - R - Q\n    $$\n    $$\n    \\text{Bias}(\\hat{Q}_{MLE}) = P + Q + R - P + \\frac{P}{N} - R - Q\n    $$\n    The terms cancel out, leaving:\n    $$\n    \\text{Bias}(\\hat{Q}_{MLE}) = \\frac{P}{N}\n    $$\nThis expression gives the exact bias of the estimator under the problem's assumptions. As it is already a term of order $O(1/N)$, this is the leading order term requested. The bias is positive, meaning the estimator $\\hat{Q}_{MLE}$ tends to overestimate the true process variance $Q$. This overestimation is a direct consequence of the underestimation of $P$ by the ML sample variance estimator $\\hat{P}$ which has a negative sign in the expression for $\\hat{Q}_{MLE}$.", "answer": "$$\n\\boxed{\\frac{P}{N}}\n$$", "id": "3418800"}, {"introduction": "Building on the principle of bias propagation, we now turn our attention to its impact on the iterative algorithms at the heart of many modern ensemble-based inverse modeling techniques. This practice focuses on methods like Ensemble Randomized Maximum Likelihood (EnRML), which use a Gauss-Newton optimization framework. You will analytically derive the bias introduced into the Gauss-Newton normal matrix when the system's Jacobian is estimated from a finite ensemble [@problem_id:3418799]. This exercise reveals how sampling error does not merely affect a static estimate but can systematically alter the search direction and convergence rate of a sophisticated optimization algorithm.", "problem": "Consider a linear inverse problem with observation model $y = J x + \\varepsilon$, where $J \\in \\mathbb{R}^{m \\times n}$ is the Jacobian (or sensitivity) matrix, $x \\in \\mathbb{R}^{n}$ is the state, $y \\in \\mathbb{R}^{m}$ is the observation, and $\\varepsilon \\sim \\mathcal{N}(0, R)$ with a known, symmetric positive-definite observation-error covariance $R \\in \\mathbb{R}^{m \\times m}$. At a given Gauss–Newton iteration, let the residual be $r \\in \\mathbb{R}^{m}$, treated as deterministic. In ensemble implementations such as Ensemble Randomized Maximum Likelihood (EnRML) and Ensemble Smoother with Multiple Data Assimilation (ES-MDA), the Jacobian $J$ is estimated by linear regression from an ensemble of size $N$ as follows. Draw state perturbations as columns $\\{x^{(i)}\\}_{i=1}^{N}$, independently from $\\mathcal{N}(0, C)$ with a known, symmetric positive-definite prior covariance $C \\in \\mathbb{R}^{n \\times n}$, and form the anomaly matrix $X = [x^{(1)}, \\dots, x^{(N)}] \\in \\mathbb{R}^{n \\times N}$. Generate corresponding observation anomalies $Y = [y^{(1)}, \\dots, y^{(N)}] \\in \\mathbb{R}^{m \\times N}$ by $Y = J X + E$, where the noise matrix $E = [\\varepsilon^{(1)}, \\dots, \\varepsilon^{(N)}] \\in \\mathbb{R}^{m \\times N}$ has independent columns $\\varepsilon^{(i)} \\sim \\mathcal{N}(0, R)$ and is independent of $X$. The ensemble-regression estimator of $J$ is $\\widehat{J} = Y X^{\\top} (X X^{\\top})^{-1}$. Define the normal matrix and right-hand side for the Gauss–Newton step as $A = J^{\\top} R^{-1} J$ and $b = J^{\\top} R^{-1} r$, and their ensemble-regression counterparts $\\widehat{A} = \\widehat{J}^{\\top} R^{-1} \\widehat{J}$ and $\\widehat{b} = \\widehat{J}^{\\top} R^{-1} r$. Assume $N > n + 1$ so that $(X X^{\\top})^{-1}$ exists almost surely and the expectation of its inverse exists.\n\nUsing only foundational probabilistic facts about multivariate Gaussian variables and the Wishart distribution, derive from first principles the expected bias in $\\widehat{A}$ and $\\widehat{b}$ due to finite $N$. Then, consider a Gauss–Newton step that solves $(\\widehat{A}) s = b$ (i.e., uses the ensemble-regression normal matrix but the exact right-hand side) for a linear least-squares objective without additional regularization. For the linear model, the error propagation from one iteration to the next can be written in terms of the error $e = x - x^{\\star}$ with the true solution $x^{\\star}$ as $e^{+} = M e$, where $M$ is a contraction matrix that depends on the choice of $A$ and the bias in $\\widehat{A}$. Under the commutativity and isotropy assumption $A = \\lambda C^{-1}$ for some scalar $\\lambda > 0$ (equivalently, the whitened sensitivity $C^{1/2} A C^{1/2}$ is proportional to the identity), compute the expected spectral radius of $M$ (which equals the expected linear convergence factor) in closed form as a function of $m$, $n$, $N$, and $\\lambda$. Provide your final answer as a single simplified analytic expression. No rounding is required, and no units are involved.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the theory of data assimilation and inverse problems, is mathematically well-posed with sufficient and consistent definitions, and is stated in objective, formal language. We may therefore proceed with the solution.\n\nThe problem is divided into two parts: first, deriving the expected bias in the ensemble-estimated Gauss-Newton normal matrix $\\widehat{A}$ and right-hand side $\\widehat{b}$, and second, computing the expected convergence factor for a specific iterative scheme.\n\n**Part 1: Derivation of Expected Bias**\n\nWe are given the ensemble-regression estimator for the Jacobian matrix $J$:\n$$ \\widehat{J} = Y X^{\\top} (X X^{\\top})^{-1} $$\nSubstituting the definition $Y = JX + E$, we get:\n$$ \\widehat{J} = (JX + E) X^{\\top} (X X^{\\top})^{-1} = JX X^{\\top} (X X^{\\top})^{-1} + E X^{\\top} (X X^{\\top})^{-1} = J + E X^{\\top} (X X^{\\top})^{-1} $$\nTo find the expected value of $\\widehat{J}$, we take the expectation over the distributions of $X$ and $E$. Since the state perturbations $X$ and observation errors $E$ are independent, and the columns of $E$ are drawn from a zero-mean distribution ($\\mathbb{E}[E]=0$), we can use the law of total expectation, conditioning on $X$:\n$$ \\mathbb{E}[\\widehat{J}] = \\mathbb{E}[J + E X^{\\top} (X X^{\\top})^{-1}] = J + \\mathbb{E}_{X}[\\mathbb{E}_{E}[E X^{\\top} (X X^{\\top})^{-1} | X]] $$\nThe inner expectation is $\\mathbb{E}_{E}[E] X^{\\top} (X X^{\\top})^{-1} = 0 \\cdot X^{\\top} (X X^{\\top})^{-1} = 0$.\nThus, $\\mathbb{E}[\\widehat{J}] = J$. The estimator $\\widehat{J}$ is unbiased.\n\nNow, we compute the expectation of the estimated right-hand side vector $\\widehat{b} = \\widehat{J}^{\\top} R^{-1} r$. Since the residual $r$ is treated as deterministic:\n$$ \\mathbb{E}[\\widehat{b}] = \\mathbb{E}[\\widehat{J}^{\\top} R^{-1} r] = \\mathbb{E}[\\widehat{J}]^{\\top} R^{-1} r = J^{\\top} R^{-1} r = b $$\nTherefore, the estimator $\\widehat{b}$ is also unbiased, and its expected bias, $\\mathbb{E}[\\widehat{b}-b]$, is zero.\n\nNext, we compute the expectation of the estimated normal matrix $\\widehat{A} = \\widehat{J}^{\\top} R^{-1} \\widehat{J}$:\n$$ \\widehat{A} = (J + E X^{\\top} (X X^{\\top})^{-1})^{\\top} R^{-1} (J + E X^{\\top} (X X^{\\top})^{-1}) $$\nExpanding this expression gives:\n$$ \\widehat{A} = J^{\\top}R^{-1}J + J^{\\top}R^{-1}E X^{\\top}(XX^{\\top})^{-1} + ((XX^{\\top})^{-1})^{\\top}X E^{\\top}R^{-1}J + ((XX^{\\top})^{-1})^{\\top}X E^{\\top}R^{-1}E X^{\\top}(XX^{\\top})^{-1} $$\nTaking the expectation, the two cross-terms vanish because $\\mathbb{E}[E] = 0$ and $E$ is independent of $X$. We are left with:\n$$ \\mathbb{E}[\\widehat{A}] = J^{\\top}R^{-1}J + \\mathbb{E}[ (X X^{\\top})^{-1} X E^{\\top} R^{-1} E X^{\\top} (X X^{\\top})^{-1}] $$\nNote that $XX^\\top$ is symmetric, so the transpose on the inverse is redundant. Using the law of total expectation, we first condition on $X$:\n$$ \\mathbb{E}[\\widehat{A}] = A + \\mathbb{E}_{X}[ (X X^{\\top})^{-1} X \\mathbb{E}_{E}[E^{\\top} R^{-1} E | X] X^{\\top} (X X^{\\top})^{-1}] $$\nThe matrix inner product is $E^{\\top} R^{-1} E$, which is an $N \\times N$ matrix. Its $(i, j)$-th entry is $(\\varepsilon^{(i)})^{\\top} R^{-1} \\varepsilon^{(j)}$. The columns $\\varepsilon^{(i)}$ of $E$ are independent and drawn from $\\mathcal{N}(0, R)$. For $i \\neq j$, $\\mathbb{E}[(\\varepsilon^{(i)})^{\\top} R^{-1} \\varepsilon^{(j)}] = \\mathbb{E}[(\\varepsilon^{(i)})^{\\top}] R^{-1} \\mathbb{E}[\\varepsilon^{(j)}] = 0$. For $i = j$, we have a quadratic form. For a random vector $z \\sim \\mathcal{N}(\\mu, \\Sigma)$, $\\mathbb{E}[z^{\\top}Qz] = \\mathrm{tr}(Q\\Sigma) + \\mu^{\\top}Q\\mu$. Here, $z=\\varepsilon^{(i)}$, $\\mu=0$, $\\Sigma=R$, and $Q=R^{-1}$. So, $\\mathbb{E}[(\\varepsilon^{(i)})^{\\top} R^{-1} \\varepsilon^{(i)}] = \\mathrm{tr}(R^{-1}R) = \\mathrm{tr}(I_m) = m$.\nTherefore, $\\mathbb{E}_{E}[E^{\\top} R^{-1} E] = m I_N$. Substituting this back:\n$$ \\mathbb{E}[\\widehat{A}] = A + \\mathbb{E}_{X}[ (X X^{\\top})^{-1} X (m I_N) X^{\\top} (X X^{\\top})^{-1}] $$\n$$ \\mathbb{E}[\\widehat{A}] = A + m \\, \\mathbb{E}_{X}[ (X X^{\\top})^{-1} (X X^{\\top}) (X X^{\\top})^{-1}] = A + m \\, \\mathbb{E}[(X X^{\\top})^{-1}] $$\nThe matrix $S_X = X X^{\\top} = \\sum_{i=1}^N x^{(i)} (x^{(i)})^\\top$. Since $x^{(i)} \\sim \\mathcal{N}(0, C)$ are i.i.d., $S_X$ follows a Wishart distribution, $S_X \\sim \\mathcal{W}_n(C, N)$. For a Wishart matrix $S \\sim \\mathcal{W}_p(\\Sigma, k)$ with $k > p+1$, its inverse has expectation $\\mathbb{E}[S^{-1}] = \\frac{1}{k-p-1} \\Sigma^{-1}$. Here, $p=n$ and $k=N$, and the condition $N > n+1$ is given.\nHence, $\\mathbb{E}[(X X^{\\top})^{-1}] = \\frac{1}{N-n-1} C^{-1}$.\nThe expected value of $\\widehat{A}$ is:\n$$ \\mathbb{E}[\\widehat{A}] = A + \\frac{m}{N-n-1} C^{-1} $$\nThe expected bias in $\\widehat{A}$ is $\\mathbb{E}[\\widehat{A} - A] = \\frac{m}{N-n-1} C^{-1}$.\n\n**Part 2: Expected Convergence Factor**\n\nThe problem states that the error $e=x-x^\\star$ propagates via $e^+ = Me$. This corresponds to a fixed-point iteration $x_{k+1} = T x_k + c'$, whose error propagation is $e_{k+1} = T e_k$. The Gauss-Newton update is $x_{k+1} = x_k + s$, where $\\widehat{A} s = b$. If this is a step in solving the linear system $Ax=b_{true}$, the right-hand side would be $b = J^\\top R^{-1}(y-Jx_k) = b_{true} - Ax_k$. The update becomes $x_{k+1} = x_k + \\widehat{A}^{-1}(b_{true}-Ax_k)$. The error $e_k=x_k-x^\\star$ then updates as $e_{k+1} = (I - \\widehat{A}^{-1}A)e_k$. The propagation matrix is thus $M = I - \\widehat{A}^{-1}A$.\n\nWe need to compute the expected spectral radius, $\\mathbb{E}[\\rho(M)]$. A direct computation is generally intractable. In the context of ensemble methods, this quantity is typically approximated by the spectral radius of the expected operator, $\\rho(\\mathbb{E}[M])$, which is also known as the expected linear convergence factor. We proceed with this standard identification.\n$$ \\mathbb{E}[M] = \\mathbb{E}[I - \\widehat{A}^{-1}A] = I - \\mathbb{E}[\\widehat{A}^{-1}]A $$\nThe expectation of the inverse matrix $\\mathbb{E}[\\widehat{A}^{-1}]$ is also difficult to compute. We use the first-order approximation $\\mathbb{E}[\\widehat{A}^{-1}] \\approx (\\mathbb{E}[\\widehat{A}])^{-1}$, which is accurate when the variance of $\\widehat{A}$ is small compared to its mean (e.g., for large $N$).\n$$ \\mathbb{E}[M] \\approx I - (\\mathbb{E}[\\widehat{A}])^{-1} A $$\nUsing our result for $\\mathbb{E}[\\widehat{A}]$ and the given isotropy assumption $A = \\lambda C^{-1}$ (which implies $C^{-1} = \\frac{1}{\\lambda}A$):\n$$ \\mathbb{E}[\\widehat{A}] = A + \\frac{m}{N-n-1} C^{-1} = A + \\frac{m}{N-n-1} \\left(\\frac{1}{\\lambda}A\\right) = \\left(1 + \\frac{m}{\\lambda(N-n-1)}\\right)A $$\nNow we invert this expected matrix:\n$$ (\\mathbb{E}[\\widehat{A}])^{-1} = \\left(1 + \\frac{m}{\\lambda(N-n-1)}\\right)^{-1} A^{-1} $$\nSubstituting this into the expression for $\\mathbb{E}[M]$:\n$$ \\mathbb{E}[M] \\approx I - \\left(1 + \\frac{m}{\\lambda(N-n-1)}\\right)^{-1} A^{-1} A = \\left(1 - \\left(1 + \\frac{m}{\\lambda(N-n-1)}\\right)^{-1}\\right)I $$\nLet $\\gamma = \\frac{m}{\\lambda(N-n-1)}$.\n$$ \\mathbb{E}[M] \\approx \\left(1 - \\frac{1}{1+\\gamma}\\right)I = \\left(\\frac{1+\\gamma-1}{1+\\gamma}\\right)I = \\frac{\\gamma}{1+\\gamma}I $$\nThe spectral radius is the largest absolute eigenvalue. Since $\\lambda, m > 0$ and $N > n+1$, $\\gamma > 0$, so the eigenvalue is positive.\n$$ \\rho(\\mathbb{E}[M]) \\approx \\frac{\\gamma}{1+\\gamma} = \\frac{\\frac{m}{\\lambda(N-n-1)}}{1 + \\frac{m}{\\lambda(N-n-1)}} $$\nMultiplying the numerator and denominator by $\\lambda(N-n-1)$ gives the simplified expression:\n$$ \\rho(\\mathbb{E}[M]) \\approx \\frac{m}{m + \\lambda(N-n-1)} $$\nThis is the expected linear convergence factor.", "answer": "$$\\boxed{\\frac{m}{m + \\lambda(N - n - 1)}}$$", "id": "3418799"}, {"introduction": "Having analyzed the direct effects of sampling error, we now investigate the trade-offs associated with one of its primary mitigation techniques: covariance localization. While localization is essential for suppressing spurious long-range correlations in high-dimensional systems, it can also introduce its own form of systematic error. In this computational practice, you will implement a scenario with known long-range correlations and quantify how applying Schur-product localization distorts the power spectrum of the background error covariance [@problem_id:3418750]. This exercise provides critical insight into the practical challenge of tuning localization, demonstrating that an improperly configured filter can degrade the analysis by altering the physically meaningful structures it was intended to preserve.", "problem": "Consider a one-dimensional periodic grid with $n$ equally spaced points indexed by $i \\in \\{0,1,\\dots,n-1\\}$. Define the periodic distance between indices $i$ and $j$ by $d(i,j) = \\min\\{|i-j|, n-|i-j|\\}$. Let the forecast error be a zero-mean, second-order stationary Gaussian random field with covariance matrix $P^f \\in \\mathbb{R}^{n \\times n}$ that is circulant and specified by a correlation function $p(r)$ depending only on $r = d(i,j)$. Assume a class of teleconnected forecast covariances given by\n$$\np(r) = \\exp\\!\\left(-\\frac{r}{L_c}\\right)\\,\\Big((1-a) + a \\cos\\!\\big(2\\pi \\frac{k_0}{n} r\\big)\\Big),\n$$\nwith parameters $L_c > 0$ (correlation length in grid steps), $a \\in [0,1)$ (teleconnection amplitude), and integer $k_0 \\in \\{0,1,\\dots,\\lfloor n/2 \\rfloor\\}$ (teleconnection wavenumber). Observations are taken at all grid points with identity observation operator $H = I_n$ and independent Gaussian noise with variance $\\sigma^2$, i.e., $R = \\sigma^2 I_n$.\n\nSchur-product localization forms a localized prior covariance $P^L = P^f \\circ \\rho$, where $\\circ$ denotes the Hadamard (elementwise) product, and $\\rho \\in \\mathbb{R}^{n \\times n}$ is a correlation matrix specified by a stationary, circulant localization function $\\rho(r)$. Use the Gaussian localization\n$$\n\\rho(r) = \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{r}{\\ell}\\right)^2\\right),\n$$\nwith localization half-width $\\ell > 0$ in grid steps. Let $\\Phi(k)$ denote the discrete power spectrum of $P^f$ and $\\Phi_L(k)$ the discrete power spectrum of $P^L$ (both defined as the discrete Fourier transform of the first row of the corresponding circulant matrix). Define the spectral distortion of localization by\n$$\nS(k) = \\frac{\\Phi_L(k)}{\\Phi(k)},\n$$\nfor those modes $k$ where $\\Phi(k)$ is non-negligible.\n\nThe Kalman gain constructed from a covariance matrix $Q$ with $H = I_n$ and $R = \\sigma^2 I_n$ is defined by $K(Q) = Q\\,(Q + R)^{-1}$. The optimal analysis obtained from the true $P^f$ uses $K(P^f)$, while a localized analysis uses $K(P^L)$. Consider the linear analysis estimator $\\hat{x}^a = \\hat{x}^f + K(y - \\hat{x}^f)$ with $y = x + \\varepsilon$, where $x$ is the true state, $\\varepsilon$ is the observation noise, and $\\hat{x}^f$ is the prior estimate. The mean-squared analysis error covariance achieved by using any linear gain $K$ with $H=I_n$ and $R=\\sigma^2 I_n$ is defined by the fundamental relation\n$$\n\\Sigma^a(K) = (I_n - K)\\,P^f\\,(I_n - K)^\\top + K\\,R\\,K^\\top.\n$$\n\nYour tasks are:\n- Starting only from the core definitions above (stationarity and circulant structure, the definitions of the discrete Fourier transform, the Hadamard product, and the mean-squared analysis error covariance for a linear estimator), derive mode-wise expressions that allow you to compute:\n  1. The discrete spectra $\\Phi(k)$ and $\\Phi_L(k)$.\n  2. The spectral distortion $S(k)$ at each mode $k$ where $\\Phi(k)$ is above a small threshold $\\tau$.\n  3. The optimal mode-wise analysis error variances that result from using $K(P^f)$.\n  4. The actually achieved mode-wise analysis error variances that result from using the localized gain $K(P^L)$ while the true prior covariance remains $P^f$.\n- Using these expressions, define two aggregate scalars for each parameter set:\n  1. The root-mean-square spectral distortion across significant modes,\n     $$\n     D_{\\mathrm{rms}} = \\sqrt{\\frac{1}{|\\mathcal{K}|} \\sum_{k \\in \\mathcal{K}} \\big(S(k) - 1\\big)^2},\n     $$\n     where $\\mathcal{K} = \\{k : \\Phi(k) \\ge \\tau\\}$.\n  2. The increase in average analysis error variance per grid point due to localization:\n     $$\n     \\Delta \\bar{\\sigma}^2 = \\frac{1}{n} \\sum_{k=0}^{n-1} \\left(\\sigma^2_{a,\\mathrm{loc}}(k) - \\sigma^2_{a,\\mathrm{opt}}(k)\\right),\n     $$\n     where $\\sigma^2_{a,\\mathrm{loc}}(k)$ uses $K(P^L)$ and $\\sigma^2_{a,\\mathrm{opt}}(k)$ uses $K(P^f)$, both evaluated mode-wise.\n- Additionally, report a boolean flag indicating whether the relative degradation is significant, defined by\n  $$\n  \\frac{\\Delta \\bar{\\sigma}^2}{\\frac{1}{n}\\sum_{k=0}^{n-1} \\sigma^2_{a,\\mathrm{opt}}(k)} > 0.05.\n  $$\n\nImplement a program that computes $(D_{\\mathrm{rms}}, \\Delta \\bar{\\sigma}^2, \\text{significant})$ for each of the following test parameter sets, with all parameters specified in grid-step units:\n- Test A (strong teleconnections, narrow localization):\n  $$\n  n=64,\\quad L_c=12,\\quad a=0.8,\\quad k_0=4,\\quad \\ell=2.5,\\quad \\sigma^2=0.2,\\quad \\tau=10^{-8}.\n  $$\n- Test B (strong teleconnections, broad localization):\n  $$\n  n=64,\\quad L_c=12,\\quad a=0.8,\\quad k_0=4,\\quad \\ell=10.0,\\quad \\sigma^2=0.2,\\quad \\tau=10^{-8}.\n  $$\n- Test C (no effective localization):\n  $$\n  n=64,\\quad L_c=12,\\quad a=0.8,\\quad k_0=4,\\quad \\ell=10^{6},\\quad \\sigma^2=0.2,\\quad \\tau=10^{-8}.\n  $$\n- Test D (higher-frequency teleconnections, narrow localization):\n  $$\n  n=64,\\quad L_c=8,\\quad a=0.9,\\quad k_0=16,\\quad \\ell=3.0,\\quad \\sigma^2=0.5,\\quad \\tau=10^{-8}.\n  $$\n\nFinal output format requirement:\n- Your program must produce a single line of output containing a list of four entries, one per test case, in the order $(\\text{A},\\text{B},\\text{C},\\text{D})$.\n- Each entry must be a list of the form $[D_{\\mathrm{rms}},\\,\\Delta \\bar{\\sigma}^2,\\,\\text{significant}]$, where $D_{\\mathrm{rms}}$ and $\\Delta \\bar{\\sigma}^2$ are decimal numbers rounded to six digits after the decimal point, and $\\text{significant}$ is a boolean value.\n- The overall output must thus look like\n  $$\n  \\big[\\,[d_A,\\delta_A,s_A],\\,[d_B,\\delta_B,s_B],\\,[d_C,\\delta_C,s_C],\\,[d_D,\\delta_D,s_D]\\,\\big],\n  $$\n  printed on a single line with no extra characters.", "solution": "We begin from the fundamental structures and definitions. On a one-dimensional periodic grid of size $n$, a stationary covariance $P^f$ is circulant: its $(i,j)$ entry depends only on $r = d(i,j)$, where $d$ is the periodic distance. The first row $p(r)$, $r \\in \\{0,1,\\dots,n-1\\}$, fully determines $P^f$. For the teleconnected prior, we are given\n$$\np(r) = \\exp\\!\\left(-\\frac{r}{L_c}\\right)\\,\\Big((1-a) + a \\cos\\!\\big(2\\pi \\frac{k_0}{n} r\\big)\\Big),\n$$\nwhich encodes long-range teleconnections through the oscillatory factor with wavenumber $k_0$ and amplitude $a$, modulated by exponential decay with correlation length $L_c$. The Schur-product localization constructs $P^L = P^f \\circ \\rho$, where $\\rho$ is circulant with first row\n$$\n\\rho(r) = \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{r}{\\ell}\\right)^2\\right).\n$$\n\nKey spectral facts for circulant matrices (derivable from the definition of the discrete Fourier transform) are:\n- The eigenvectors of any circulant matrix are the discrete Fourier modes. If $c(r)$ is the first row of a circulant matrix $C$, then its eigenvalues $\\Lambda_C(k)$ are given by the discrete Fourier transform\n$$\n\\Lambda_C(k) = \\sum_{r=0}^{n-1} c(r)\\, \\exp\\!\\left(-2\\pi i \\frac{k r}{n}\\right), \\quad k = 0,1,\\dots,n-1.\n$$\nFor a covariance, these eigenvalues are real and nonnegative due to positive semidefiniteness, and we can refer to them as the discrete power spectrum. Denote $\\Phi(k) = \\Lambda_{P^f}(k)$ and $\\Phi_L(k) = \\Lambda_{P^L}(k)$.\n\n- The Hadamard product in physical space corresponds to circular convolution in the frequency domain. Specifically, if $P^L = P^f \\circ \\rho$, then\n$$\n\\Phi_L(k) = \\sum_{m=0}^{n-1} \\Phi(m)\\, \\widehat{\\rho}(k - m) \\quad \\text{with indices modulo } n,\n$$\nwhere $\\widehat{\\rho}$ is the discrete Fourier transform of the first row of $\\rho$ and the sum is a circular convolution. This expresses the spectral leakage caused by localization: sharp localization in space (small $\\ell$) broadens the spectrum of $\\rho$ and thus mixes the modes of $\\Phi$.\n\nDefine the spectral distortion pointwise by\n$$\nS(k) = \\frac{\\Phi_L(k)}{\\Phi(k)},\n$$\nfor modes $k$ where $\\Phi(k)$ is not negligible. Let $\\tau > 0$ be a threshold; we consider the significant set $\\mathcal{K} = \\{ k : \\Phi(k) \\ge \\tau \\}$ and define the root-mean-square distortion\n$$\nD_{\\mathrm{rms}} = \\sqrt{\\frac{1}{|\\mathcal{K}|} \\sum_{k \\in \\mathcal{K}} \\big(S(k) - 1\\big)^2}.\n$$\n\nNext, we characterize the analysis error under linear estimation. The linear analysis estimator with gain $K$ and $H = I_n$ is $\\hat{x}^a = \\hat{x}^f + K(y - \\hat{x}^f)$ with $y = x + \\varepsilon$. The fundamental expression for the achieved analysis error covariance is\n$$\n\\Sigma^a(K) = (I_n - K)\\,P^f\\,(I_n - K)^\\top + K\\,R\\,K^\\top,\n$$\nwith $R = \\sigma^2 I_n$. For $H=I_n$, the optimal (minimum mean-squared error) gain computed from a covariance $Q$ is the Wiener filter\n$$\nK(Q) = Q\\,(Q + R)^{-1},\n$$\na result obtained by minimizing $\\mathrm{trace}(\\Sigma^a(K))$ with respect to $K$ using standard quadratic forms, or by the classical Gauss–Markov argument. The usual Kalman filter uses $Q = P^f$ and achieves the optimal posterior covariance $( (P^f)^{-1} + R^{-1} )^{-1}$. However, we are interested in the suboptimal analysis that uses the gain $K(P^L)$ computed from the localized covariance $P^L$ while the true prior covariance is still $P^f$.\n\nBecause $P^f$, $P^L$, and $R$ are all circulant, they are diagonalizable in the same Fourier basis. Let $\\Phi(k)$ and $\\Phi_L(k)$ be as above. In the Fourier domain, $K(P^L)$ acts diagonally with mode-wise gains\n$$\ng_{\\mathrm{loc}}(k) = \\frac{\\Phi_L(k)}{\\Phi_L(k) + \\sigma^2}.\n$$\nIf we instead used the true prior, the optimal mode-wise gains are\n$$\ng_{\\mathrm{opt}}(k) = \\frac{\\Phi(k)}{\\Phi(k) + \\sigma^2}.\n$$\nPlugging $K = K(P^L)$ into the fundamental expression for $\\Sigma^a(K)$ and using simultaneous diagonalization yields, for each mode $k$,\n$$\n\\sigma^2_{a,\\mathrm{loc}}(k) = \\big(1 - g_{\\mathrm{loc}}(k)\\big)^2\\,\\Phi(k) + g_{\\mathrm{loc}}(k)^2\\,\\sigma^2,\n$$\nwhile the optimal mode-wise posterior variance is the well-known identity\n$$\n\\sigma^2_{a,\\mathrm{opt}}(k) = \\frac{\\Phi(k)\\,\\sigma^2}{\\Phi(k) + \\sigma^2},\n$$\nwhich also follows by substituting $g_{\\mathrm{opt}}(k)$ into the same expression, or by inverting $(\\Phi(k)^{-1} + \\sigma^{-2})$ per mode. The average analysis error variance per grid point is the mean of the mode variances,\n$$\n\\bar{\\sigma}^2_{a,\\mathrm{loc}} = \\frac{1}{n} \\sum_{k=0}^{n-1} \\sigma^2_{a,\\mathrm{loc}}(k), \\quad\n\\bar{\\sigma}^2_{a,\\mathrm{opt}} = \\frac{1}{n} \\sum_{k=0}^{n-1} \\sigma^2_{a,\\mathrm{opt}}(k),\n$$\nand the degradation due to localization is\n$$\n\\Delta \\bar{\\sigma}^2 = \\bar{\\sigma}^2_{a,\\mathrm{loc}} - \\bar{\\sigma}^2_{a,\\mathrm{opt}} \\ge 0,\n$$\nwith equality only if $K(P^L) = K(P^f)$ in the sense of equal mode-wise gains. A practically significant degradation flag can be defined as\n$$\n\\frac{\\Delta \\bar{\\sigma}^2}{\\bar{\\sigma}^2_{a,\\mathrm{opt}}} > 0.05.\n$$\n\nAlgorithmic procedure for each test case:\n1. Construct the first row $p(r)$ of $P^f$ for $r=0,\\dots,n-1$ using the given $n$, $L_c$, $a$, $k_0$.\n2. Construct the first row $\\rho(r)$ of the localization matrix using $\\ell$.\n3. Compute the localized first row $p_L(r) = p(r)\\,\\rho(r)$.\n4. Compute the discrete spectra $\\Phi(k)$ and $\\Phi_L(k)$ as the real parts of the discrete Fourier transform of $p(r)$ and $p_L(r)$, respectively. Enforce nonnegativity by clipping small negative numerical round-off to zero.\n5. With a threshold $\\tau$, define $\\mathcal{K} = \\{k : \\Phi(k) \\ge \\tau\\}$ and compute $S(k) = \\Phi_L(k)/\\Phi(k)$ on $\\mathcal{K}$, then compute $D_{\\mathrm{rms}}$.\n6. Compute mode-wise gains $g_{\\mathrm{loc}}(k)$ and the achieved mode-wise analysis variances $\\sigma^2_{a,\\mathrm{loc}}(k)$. Compute the optimal $\\sigma^2_{a,\\mathrm{opt}}(k)$. Average over $k$ to obtain $\\Delta \\bar{\\sigma}^2$ and the relative degradation, and determine the boolean flag by comparing to $0.05$.\n\nConditions where localization degrades analyses follow immediately from the spectral viewpoint: if $\\ell$ is small compared to the teleconnection wavelength (set by $k_0$) and the correlation length $L_c$ is large, the spectrum $\\widehat{\\rho}$ is broad and mixes separated teleconnection peaks in $\\Phi$, producing significant $S(k) \\ne 1$ across many $k$. This mis-specifies the gains $g_{\\mathrm{loc}}(k)$ relative to $g_{\\mathrm{opt}}(k)$ and increases $\\bar{\\sigma}^2_{a}$. Conversely, if $\\ell$ is large (so that $\\rho \\approx 1$), then $S(k) \\approx 1$ and $\\Delta \\bar{\\sigma}^2 \\approx 0$.\n\nWe implement this procedure for the four specified test cases:\n- Test A: $n=64$, $L_c=12$, $a=0.8$, $k_0=4$, $\\ell=2.5$, $\\sigma^2=0.2$, $\\tau=10^{-8}$.\n- Test B: $n=64$, $L_c=12$, $a=0.8$, $k_0=4$, $\\ell=10.0$, $\\sigma^2=0.2$, $\\tau=10^{-8}$.\n- Test C: $n=64$, $L_c=12$, $a=0.8$, $k_0=4$, $\\ell=10^{6}$, $\\sigma^2=0.2$, $\\tau=10^{-8}$.\n- Test D: $n=64$, $L_c=8$, $a=0.9$, $k_0=16$, $\\ell=3.0$, $\\sigma^2=0.5$, $\\tau=10^{-8}$.\n\nThe program computes $(D_{\\mathrm{rms}}, \\Delta \\bar{\\sigma}^2, \\text{significant})$ for each case, rounding the first two to six decimal places, and prints them as a single-line list in the required order.", "answer": "```python\nimport numpy as np\n\ndef periodic_distance_vector(n: int) -> np.ndarray:\n    \"\"\"\n    Return the vector of periodic distances r = d(0,j) for j=0..n-1 on a 1D circle of length n.\n    \"\"\"\n    r = np.arange(n)\n    r = np.minimum(r, n - r)\n    return r.astype(float)\n\ndef build_forecast_first_row(n: int, Lc: float, a: float, k0: int) -> np.ndarray:\n    \"\"\"\n    Build the first row p(r) of the circulant forecast covariance P^f for a teleconnected process.\n    p(r) = exp(-r/Lc) * ((1 - a) + a * cos(2*pi*k0*r/n))\n    \"\"\"\n    r = periodic_distance_vector(n)\n    base = np.exp(-r / Lc)\n    tele = (1.0 - a) + a * np.cos(2.0 * np.pi * k0 * r / n)\n    p = base * tele\n    return p\n\ndef build_localization_first_row(n: int, ell: float) -> np.ndarray:\n    \"\"\"\n    Build the first row rho(r) of the Gaussian localization matrix.\n    rho(r) = exp(-0.5 * (r/ell)^2)\n    \"\"\"\n    r = periodic_distance_vector(n)\n    if ell = 0:\n        # Degenerate case: no localization beyond self; set rho(0)=1 and others ~0\n        rho = np.zeros_like(r)\n        rho[0] = 1.0\n        return rho\n    # To allow \"no localization\" with very large ell, this remains numerically ~1\n    rho = np.exp(-0.5 * (r / ell) ** 2)\n    return rho\n\ndef circulant_spectrum(first_row: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the discrete spectrum (eigenvalues) of a circulant matrix from its first row via DFT.\n    Ensure numerical nonnegativity by clipping tiny negative values to zero.\n    \"\"\"\n    spec = np.fft.fft(first_row).real\n    # Numerical safety: small negative due to rounding set to zero\n    spec = np.where(spec  0, np.maximum(spec, 0.0), spec)\n    return spec\n\ndef compute_metrics(n: int, Lc: float, a: float, k0: int, ell: float, sigma2: float, tau: float):\n    \"\"\"\n    Compute:\n    - D_rms: RMS spectral distortion over modes with Phi >= tau\n    - delta_avg: increase in average analysis error variance per grid point due to localization\n    - significant: boolean flag if delta_avg / avg_opt > 0.05\n    \"\"\"\n    # Build first rows\n    p = build_forecast_first_row(n, Lc, a, k0)\n    rho = build_localization_first_row(n, ell)\n    pL = p * rho\n\n    # Spectra\n    Phi = circulant_spectrum(p)\n    PhiL = circulant_spectrum(pL)\n\n    # Avoid division by tiny values; define mask\n    mask = Phi >= tau\n    if not np.any(mask):\n        Drms = 0.0\n    else:\n        S = np.zeros_like(Phi)\n        S[mask] = PhiL[mask] / Phi[mask]\n        Drms = np.sqrt(np.mean((S[mask] - 1.0) ** 2))\n\n    # Mode-wise gains and analysis variances\n    # Regularization to avoid numerical issues if PhiL + sigma2 is zero (shouldn't happen with sigma2>0)\n    g_loc = PhiL / (PhiL + sigma2 + 1e-15)\n    # Achieved with localized gain but true Phi:\n    sigma2_a_loc = (1.0 - g_loc) ** 2 * Phi + (g_loc ** 2) * sigma2\n\n    # Optimal\n    sigma2_a_opt = (Phi * sigma2) / (Phi + sigma2 + 1e-15)\n\n    avg_loc = np.mean(sigma2_a_loc)\n    avg_opt = np.mean(sigma2_a_opt)\n    delta_avg = avg_loc - avg_opt\n    # Numerical clipping of tiny negatives due to roundoff\n    if delta_avg  0 and abs(delta_avg)  1e-14:\n        delta_avg = 0.0\n    rel = delta_avg / avg_opt if avg_opt > 1e-12 else np.inf\n    significant = bool(rel > 0.05)\n\n    return Drms, delta_avg, significant\n\ndef solve():\n    # Define the test cases as specified\n    test_cases = [\n        # Test A\n        {\"n\": 64, \"Lc\": 12.0, \"a\": 0.8, \"k0\": 4, \"ell\": 2.5, \"sigma2\": 0.2, \"tau\": 1e-8},\n        # Test B\n        {\"n\": 64, \"Lc\": 12.0, \"a\": 0.8, \"k0\": 4, \"ell\": 10.0, \"sigma2\": 0.2, \"tau\": 1e-8},\n        # Test C\n        {\"n\": 64, \"Lc\": 12.0, \"a\": 0.8, \"k0\": 4, \"ell\": 1e6, \"sigma2\": 0.2, \"tau\": 1e-8},\n        # Test D\n        {\"n\": 64, \"Lc\": 8.0, \"a\": 0.9, \"k0\": 16, \"ell\": 3.0, \"sigma2\": 0.5, \"tau\": 1e-8},\n    ]\n\n    outputs = []\n    for case in test_cases:\n        Drms, delta_avg, significant = compute_metrics(**case)\n        outputs.append(f\"[{Drms:.6f},{delta_avg:.6f},{str(significant).lower()}]\")\n\n    print(f\"[{','.join(outputs)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```\n\n**Output:**\n\n[[0.584310,0.016335,true],[0.096735,0.000451,false],[0.000000,0.000000,false],[0.636402,0.053154,true]]", "id": "3418750"}]}