{"hands_on_practices": [{"introduction": "In practice, it can be tempting to simplify the likelihood function by dropping terms that appear to be 'just' normalization constants. However, this is only permissible if those terms do not depend on the parameters we seek to estimate. This exercise [@problem_id:3397329] serves as a critical cautionary tale, demonstrating through a simple but powerful example that ignoring parameter-dependent normalization factors can lead to fundamentally incorrect and inconsistent estimators.", "problem": "Consider a simple scalar observation model commonly used to motivate likelihood construction in inverse problems and data assimilation. Let $\\{y_i\\}_{i=1}^{n}$ be independent and identically distributed (i.i.d.) draws from a Gaussian measurement model with parameter-dependent mean and variance given by $y_i \\mid \\theta \\sim \\mathcal{N}(\\theta, \\theta)$, where $\\theta \\in (0,\\infty)$ is the unknown physical parameter to be estimated.\n\nYou are told that the scientifically correct likelihood for a Gaussian model must include the normalization factor that depends on the covariance determinant. In this one-dimensional setting, the normalization factor is a function of $\\theta$. An analyst, however, mistakenly constructs an improper pseudo-likelihood by deleting every $\\theta$-dependent normalization factor and retaining only the exponential term. This is equivalent to maximizing the likelihood built solely from the negative quadratic form in the exponent.\n\nUsing only first principles, namely the definition of the likelihood for independent Gaussian data and standard limit theorems for sample averages, carry out the following steps:\n\n1. Starting from the definition of the Gaussian density and the independence of $\\{y_i\\}_{i=1}^{n}$, write down the correct log-likelihood and the improper pseudo-log-likelihood (that omits every $\\theta$-dependent normalization factor). Carefully identify which term is dropped in forming the improper pseudo-log-likelihood.\n2. Derive the maximizer $\\tilde{\\theta}_n$ of the improper pseudo-log-likelihood over $\\theta \\in (0,\\infty)$.\n3. Compute the probability limit (in the sense of convergence in probability) of $\\tilde{\\theta}_n$ as $n \\to \\infty$ under the true data-generating parameter $\\theta_0 \\in (0,\\infty)$, expressing your answer as a closed-form function of $\\theta_0$.\n\nYour final answer must be a single closed-form analytic expression for the probability limit. No rounding is required, and no units are involved.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- The data $\\{y_i\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.) draws.\n- The data-generating model is a Gaussian distribution where the mean and variance are both equal to an unknown parameter $\\theta$: $y_i \\mid \\theta \\sim \\mathcal{N}(\\theta, \\theta)$.\n- The parameter space for $\\theta$ is $(0, \\infty)$.\n- An improper pseudo-likelihood is constructed by deleting every $\\theta$-dependent normalization factor from the correct likelihood, which is equivalent to building the likelihood from the negative quadratic form in the exponent.\n- The true, data-generating parameter value is denoted by $\\theta_0 \\in (0, \\infty)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is based on the established principles of statistical inference, namely maximum likelihood estimation for a Gaussian model. The chosen model, $\\mathcal{N}(\\theta, \\theta)$, is a valid statistical model that provides a non-trivial but tractable setting to explore the consequences of model misspecification. The problem is mathematically and scientifically sound.\n- **Well-Posed**: The problem is well-posed. It provides all necessary information: the statistical model, the definition of the correct and improper likelihoods, and a clear objective. The sequence of tasks leads to a unique and meaningful solution.\n- **Objective**: The problem is stated in precise, objective language, free of subjective claims or ambiguity.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, missing information, or ill-posed structure. It is a standard, valid problem in theoretical statistics, relevant to the fields of inverse problems and data assimilation where correct likelihood specification is critical.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution Derivation\n\nThe solution proceeds by following the three steps outlined in the problem statement.\n\n**1. Correct and Improper Log-Likelihoods**\n\nThe probability density function (PDF) for a single observation $y_i$ from a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is given by $f(y_i \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right)$.\nIn this problem, the mean is $\\mu = \\theta$ and the variance is $\\sigma^2 = \\theta$. Substituting these into the PDF gives:\n$$f(y_i \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\theta}} \\exp\\left(-\\frac{(y_i - \\theta)^2}{2\\theta}\\right)$$\nSince the observations $\\{y_i\\}_{i=1}^{n}$ are i.i.d., the total likelihood function $L(\\theta \\mid \\{y_i\\})$ is the product of the individual PDFs:\n$$L(\\theta) = \\prod_{i=1}^{n} f(y_i \\mid \\theta) = \\prod_{i=1}^{n} \\frac{1}{(2\\pi\\theta)^{1/2}} \\exp\\left(-\\frac{(y_i - \\theta)^2}{2\\theta}\\right)$$\n$$L(\\theta) = (2\\pi\\theta)^{-n/2} \\exp\\left(-\\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i - \\theta)^2\\right)$$\nThe correct log-likelihood function, $\\ell(\\theta)$, is the natural logarithm of $L(\\theta)$:\n$$\\ell(\\theta) = \\ln(L(\\theta)) = \\ln\\left((2\\pi\\theta)^{-n/2}\\right) + \\ln\\left(\\exp\\left(-\\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i - \\theta)^2\\right)\\right)$$\n$$\\ell(\\theta) = -\\frac{n}{2}\\ln(2\\pi\\theta) - \\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i - \\theta)^2$$\n$$\\ell(\\theta) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\theta) - \\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i - \\theta)^2$$\nThe problem states that the improper pseudo-likelihood is formed by deleting every $\\theta$-dependent normalization factor. In the full likelihood $L(\\theta)$, the term $(2\\pi\\theta)^{-n/2}$ contains the normalization factors. The $\\theta$-dependent part is $\\theta^{-n/2}$. In the log-likelihood $\\ell(\\theta)$, this corresponds to the term $-\\frac{n}{2}\\ln(\\theta)$. Dropping this term (and the constant term $-\\frac{n}{2}\\ln(2\\pi)$, which does not affect maximization) yields the improper pseudo-log-likelihood, $\\tilde{\\ell}(\\theta)$:\n$$\\tilde{\\ell}(\\theta) = -\\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i - \\theta)^2$$\nThis aligns with the problem's description of retaining only the logarithm of the exponential term. The term dropped from the correct log-likelihood to obtain the kernel of the improper one is precisely $-\\frac{n}{2}\\ln(\\theta)$.\n\n**2. Derivation of the Improper Maximizer $\\tilde{\\theta}_n$**\n\nTo find the maximizer $\\tilde{\\theta}_n$ of the improper pseudo-log-likelihood $\\tilde{\\ell}(\\theta)$, we first expand the expression and then differentiate with respect to $\\theta$.\n$$\\tilde{\\ell}(\\theta) = -\\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i^2 - 2y_i\\theta + \\theta^2) = -\\frac{1}{2\\theta}\\left(\\sum_{i=1}^{n}y_i^2 - 2\\theta\\sum_{i=1}^{n}y_i + n\\theta^2\\right)$$\n$$\\tilde{\\ell}(\\theta) = -\\frac{1}{2\\theta}\\sum_{i=1}^{n}y_i^2 + \\sum_{i=1}^{n}y_i - \\frac{n\\theta}{2}$$\nNow, we compute the first derivative with respect to $\\theta$ and set it to zero to find the critical points:\n$$\\frac{d\\tilde{\\ell}}{d\\theta} = \\frac{d}{d\\theta}\\left(-\\frac{1}{2}\\theta^{-1}\\sum_{i=1}^{n}y_i^2 + \\sum_{i=1}^{n}y_i - \\frac{n\\theta}{2}\\right) = \\frac{1}{2}\\theta^{-2}\\sum_{i=1}^{n}y_i^2 - \\frac{n}{2}$$\nSetting the derivative to zero:\n$$\\frac{1}{2\\theta^2}\\sum_{i=1}^{n}y_i^2 - \\frac{n}{2} = 0 \\implies \\frac{1}{\\theta^2}\\sum_{i=1}^{n}y_i^2 = n$$\n$$\\theta^2 = \\frac{1}{n}\\sum_{i=1}^{n}y_i^2$$\nSince the parameter space is $\\theta \\in (0,\\infty)$, we take the positive square root. The maximizer $\\tilde{\\theta}_n$ is:\n$$\\tilde{\\theta}_n = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}y_i^2}$$\nTo confirm this is a maximum, we check the second derivative:\n$$\\frac{d^2\\tilde{\\ell}}{d\\theta^2} = \\frac{d}{d\\theta}\\left(\\frac{1}{2}\\theta^{-2}\\sum_{i=1}^{n}y_i^2 - \\frac{n}{2}\\right) = -\\theta^{-3}\\sum_{i=1}^{n}y_i^2 = -\\frac{1}{\\theta^3}\\sum_{i=1}^{n}y_i^2$$\nFor $\\theta > 0$ and assuming not all $y_i$ are zero (a probability-zero event), $\\sum y_i^2 > 0$, so $\\frac{d^2\\tilde{\\ell}}{d\\theta^2}  0$. This confirms that $\\tilde{\\theta}_n$ is indeed a local maximum. As it's the only critical point in $(0, \\infty)$, it is the global maximizer.\n\n**3. Probability Limit of $\\tilde{\\theta}_n$**\n\nWe need to compute the probability limit of $\\tilde{\\theta}_n$ as $n \\to \\infty$. The true data are generated i.i.d. from $\\mathcal{N}(\\theta_0, \\theta_0)$.\n$$\\text{plim}_{n\\to\\infty} \\tilde{\\theta}_n = \\text{plim}_{n\\to\\infty} \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}y_i^2}$$\nThe expression inside the square root, $\\frac{1}{n}\\sum_{i=1}^{n}y_i^2$, is the sample mean of the random variables $Z_i = y_i^2$. By the Law of Large Numbers, the sample mean of i.i.d. random variables converges in probability to their expected value, provided the expectation exists.\n$$\\frac{1}{n}\\sum_{i=1}^{n}y_i^2 \\xrightarrow{p} E[y^2]$$\nwhere the expectation is taken with respect to the true data-generating distribution $y \\sim \\mathcal{N}(\\theta_0, \\theta_0)$. The second moment $E[y^2]$ can be calculated from the mean $E[y]$ and variance $\\text{Var}(y)$ using the formula $E[y^2] = \\text{Var}(y) + (E[y])^2$. For our distribution, we have:\n$$E[y] = \\theta_0$$\n$$\\text{Var}(y) = \\theta_0$$\nSubstituting these into the formula for the second moment gives:\n$$E[y^2] = \\theta_0 + (\\theta_0)^2 = \\theta_0 + \\theta_0^2$$\nNow, we apply the Continuous Mapping Theorem. Since the square root function $g(x)=\\sqrt{x}$ is continuous for $x > 0$, and since $\\theta_0 > 0$ implies $\\theta_0 + \\theta_0^2 > 0$, we can pass the probability limit inside the function:\n$$\\text{plim}_{n\\to\\infty} \\tilde{\\theta}_n = \\sqrt{\\text{plim}_{n\\to\\infty} \\frac{1}{n}\\sum_{i=1}^{n}y_i^2} = \\sqrt{E[y^2]}$$\nTherefore, the probability limit of the improper estimator is:\n$$\\text{plim}_{n\\to\\infty} \\tilde{\\theta}_n = \\sqrt{\\theta_0 + \\theta_0^2}$$\nThis result shows that the estimator derived from the improper pseudo-likelihood is inconsistent, as it does not converge to the true parameter value $\\theta_0$.", "answer": "$$\\boxed{\\sqrt{\\theta_0 + \\theta_0^2}}$$", "id": "3397329"}, {"introduction": "Many scientific measurements come in the form of counts rather than continuous values, requiring a departure from the ubiquitous Gaussian error model. This practice [@problem_id:3397372] delves into modeling count data using the Poisson distribution, a cornerstone of generalized linear models. You will explore not only the construction of the Poisson log-likelihood but also discover how the choice of a 'canonical' link function is key to ensuring the convexity of the resulting optimization problem, a highly desirable property for finding a unique solution.", "problem": "Consider a data assimilation setting in which a parameter vector $\\,\\theta \\in \\mathbb{R}^{p}\\,$ is mapped by a forward operator $\\,H:\\mathbb{R}^{p}\\to\\mathbb{R}^{n}\\,$ to a predictor field. Observations are counts $\\,d=\\{d_{i}\\}_{i=1}^{n}\\,$, where each $\\,d_{i}\\,$ is a realization of a counting process at sensor $\\,i\\,$. Conditioned on $\\,\\theta\\,$, assume the observations are mutually independent and follow the Poisson distribution with rate $\\,\\lambda_{i}(\\theta)0\\,$, so that the probability mass function for each $\\,d_{i}\\,$ is given by the definition $\\,\\mathbb{P}(D_{i}=d_{i}\\mid\\theta)=\\exp(-\\lambda_{i}(\\theta))\\,\\lambda_{i}(\\theta)^{d_{i}}/(d_{i}!)\\,$. The likelihood is defined as the joint probability of the data viewed as a function of $\\,\\theta\\,$.\n\na) Starting from the stated definition of the Poisson probability mass function and the independence assumption, derive the likelihood $\\,L(\\theta;d)\\,$ and the log-likelihood $\\,\\ell(\\theta;d)=\\ln L(\\theta;d)\\,$ in terms of $\\,\\{\\lambda_{i}(\\theta)\\}_{i=1}^{n}\\,$ and $\\,\\{d_{i}\\}_{i=1}^{n}\\,$.\n\nb) Now impose the canonical link of the exponential family for count data by defining $\\,\\eta_{i}(\\theta)=\\ln\\lambda_{i}(\\theta)\\,$, and suppose the forward operator is linear in parameters so that $\\,\\eta(\\theta)=X\\theta\\,$ with a known design matrix $\\,X\\in\\mathbb{R}^{n\\times p}\\,$. Define the negative log-likelihood $\\,J(\\theta)=-\\ell(\\theta;d)\\,$. Using only first principles of differentiation and the chain rule, derive $\\,\\nabla J(\\theta)\\,$ and $\\,\\nabla^{2}J(\\theta)\\,$, and state a condition under which $\\,J(\\theta)\\,$ is convex in $\\,\\theta\\,$. Explain briefly how the canonical link $\\,\\ln\\lambda\\,$ is implicated in this convexity.\n\nc) Under the assumptions in part b), provide a single closed-form analytic expression for the log-likelihood $\\,\\ell(\\theta;d)\\,$ in terms of $\\,X\\,$, $\\,\\theta\\,$, and $\\,d\\,$. Your final reported answer must be this expression. Do not omit any term that depends on $\\,\\theta\\,$ or $\\,d\\,$. No rounding is required, and no physical units apply.", "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, and objective. It presents a standard derivation in the context of generalized linear models, specifically Poisson regression, which is a fundamental topic in statistical modeling, inverse problems, and data assimilation. All terms are well-defined, and the premises are internally consistent and sufficient to derive a unique solution.\n\na) Derivation of the likelihood and log-likelihood functions.\n\nThe likelihood function, $L(\\theta;d)$, is defined as the joint probability of observing the data $d = \\{d_i\\}_{i=1}^n$, viewed as a function of the parameter vector $\\theta \\in \\mathbb{R}^p$. Given that the observations $\\{d_i\\}_{i=1}^n$ are assumed to be mutually independent conditioned on $\\theta$, the joint probability is the product of the individual probabilities:\n$$\nL(\\theta;d) = \\mathbb{P}(D_1=d_1, D_2=d_2, \\dots, D_n=d_n \\mid \\theta) = \\prod_{i=1}^{n} \\mathbb{P}(D_i=d_i \\mid \\theta)\n$$\nThe problem states that each observation $d_i$ follows a Poisson distribution with rate $\\lambda_i(\\theta) > 0$. The probability mass function (PMF) for a single observation is given as:\n$$\n\\mathbb{P}(D_i = d_i \\mid \\theta) = \\frac{\\exp(-\\lambda_i(\\theta)) \\, \\lambda_i(\\theta)^{d_i}}{d_i!}\n$$\nSubstituting this PMF into the product for the likelihood function yields:\n$$\nL(\\theta;d) = \\prod_{i=1}^{n} \\left( \\frac{\\exp(-\\lambda_i(\\theta)) \\, \\lambda_i(\\theta)^{d_i}}{d_i!} \\right)\n$$\nThis can be rewritten by separating the products:\n$$\nL(\\theta;d) = \\left( \\prod_{i=1}^{n} \\exp(-\\lambda_i(\\theta)) \\right) \\left( \\prod_{i=1}^{n} \\lambda_i(\\theta)^{d_i} \\right) \\left( \\prod_{i=1}^{n} \\frac{1}{d_i!} \\right) = \\exp\\left(-\\sum_{i=1}^{n} \\lambda_i(\\theta)\\right) \\left( \\prod_{i=1}^{n} \\lambda_i(\\theta)^{d_i} \\right) \\left( \\frac{1}{\\prod_{i=1}^{n} d_i!} \\right)\n$$\nThe log-likelihood, $\\ell(\\theta;d)$, is the natural logarithm of the likelihood function, $\\ell(\\theta;d) = \\ln L(\\theta;d)$. Applying the logarithm to the product form of $L(\\theta;d)$ converts the product into a sum:\n$$\n\\ell(\\theta;d) = \\ln \\left( \\prod_{i=1}^{n} \\frac{\\exp(-\\lambda_i(\\theta)) \\, \\lambda_i(\\theta)^{d_i}}{d_i!} \\right) = \\sum_{i=1}^{n} \\ln \\left( \\frac{\\exp(-\\lambda_i(\\theta)) \\, \\lambda_i(\\theta)^{d_i}}{d_i!} \\right)\n$$\nUsing the properties of logarithms ($\\ln(ab) = \\ln a + \\ln b$, $\\ln(a/b) = \\ln a - \\ln b$, and $\\ln(a^c) = c \\ln a$), we can expand the term inside the summation:\n$$\n\\ell(\\theta;d) = \\sum_{i=1}^{n} \\left[ \\ln(\\exp(-\\lambda_i(\\theta))) + \\ln(\\lambda_i(\\theta)^{d_i}) - \\ln(d_i!) \\right]\n$$\n$$\n\\ell(\\theta;d) = \\sum_{i=1}^{n} \\left[ -\\lambda_i(\\theta) + d_i \\ln(\\lambda_i(\\theta)) - \\ln(d_i!) \\right]\n$$\nThis is the expression for the log-likelihood function.\n\nb) Derivation of the gradient and Hessian of the negative log-likelihood, and analysis of convexity.\n\nThe negative log-likelihood is defined as $J(\\theta) = -\\ell(\\theta;d)$. Using the result from part (a):\n$$\nJ(\\theta) = - \\sum_{i=1}^{n} \\left[ -\\lambda_i(\\theta) + d_i \\ln(\\lambda_i(\\theta)) - \\ln(d_i!) \\right] = \\sum_{i=1}^{n} \\left[ \\lambda_i(\\theta) - d_i \\ln(\\lambda_i(\\theta)) + \\ln(d_i!) \\right]\n$$\nThe term $\\sum_{i=1}^n \\ln(d_i!)$ is a constant with respect to $\\theta$ and will not affect the derivatives.\n\nWe now incorporate the assumptions that $\\eta_i(\\theta) = \\ln \\lambda_i(\\theta)$ and $\\eta(\\theta) = X\\theta$. The latter implies that for each component $i$, $\\eta_i(\\theta) = (X\\theta)_i = \\sum_{j=1}^{p} X_{ij}\\theta_j$. From the link function definition, we have $\\lambda_i(\\theta) = \\exp(\\eta_i(\\theta)) = \\exp((X\\theta)_i)$. Substituting this into $J(\\theta)$:\n$$\nJ(\\theta) = \\sum_{i=1}^{n} \\left[ \\exp((X\\theta)_i) - d_i (X\\theta)_i \\right] + C\n$$\nwhere $C = \\sum_{i=1}^n \\ln(d_i!)$ is the constant term.\n\nTo find the gradient $\\nabla J(\\theta)$, we compute the partial derivative with respect to each component $\\theta_k$ for $k=1, \\dots, p$:\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} \\sum_{i=1}^{n} \\left[ \\exp\\left(\\sum_{j=1}^{p} X_{ij}\\theta_j\\right) - d_i \\sum_{j=1}^{p} X_{ij}\\theta_j \\right]\n$$\nBy linearity of differentiation:\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\sum_{i=1}^{n} \\left[ \\frac{\\partial}{\\partial \\theta_k} \\exp\\left(\\sum_{j=1}^{p} X_{ij}\\theta_j\\right) - \\frac{\\partial}{\\partial \\theta_k} \\left(d_i \\sum_{j=1}^{p} X_{ij}\\theta_j\\right) \\right]\n$$\nUsing the chain rule, $\\frac{\\partial}{\\partial \\theta_k} \\exp((X\\theta)_i) = \\exp((X\\theta)_i) \\cdot \\frac{\\partial (X\\theta)_i}{\\partial \\theta_k} = \\exp((X\\theta)_i) \\cdot X_{ik}$. The derivative of the linear term is $\\frac{\\partial}{\\partial \\theta_k} (d_i \\sum_j X_{ij}\\theta_j) = d_i X_{ik}$.\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\sum_{i=1}^{n} \\left[ \\exp((X\\theta)_i) X_{ik} - d_i X_{ik} \\right] = \\sum_{i=1}^{n} X_{ik} (\\exp((X\\theta)_i) - d_i)\n$$\nThis is the $k$-th component of the gradient vector. In matrix notation, recognizing that $X_{ik}$ is the $(k,i)$ element of $X^T$, this can be written as:\n$$\n\\nabla J(\\theta) = X^T (\\exp(X\\theta) - d)\n$$\nwhere $\\exp(X\\theta)$ is the vector with components $\\exp((X\\theta)_i) = \\lambda_i(\\theta)$ and $d$ is the vector of observations $d_i$.\n\nTo find the Hessian matrix $\\nabla^2 J(\\theta)$, we take the partial derivative of each component of the gradient with respect to $\\theta_l$ for $l=1, \\dots, p$:\n$$\n[\\nabla^2 J(\\theta)]_{kl} = \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_l \\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_l} \\left( \\sum_{i=1}^{n} X_{ik} (\\exp((X\\theta)_i) - d_i) \\right)\n$$\n$$\n[\\nabla^2 J(\\theta)]_{kl} = \\sum_{i=1}^{n} X_{ik} \\frac{\\partial}{\\partial \\theta_l} (\\exp((X\\theta)_i) - d_i) = \\sum_{i=1}^{n} X_{ik} \\left( \\exp((X\\theta)_i) \\cdot X_{il} \\right)\n$$\n$$\n[\\nabla^2 J(\\theta)]_{kl} = \\sum_{i=1}^{n} X_{ik} \\lambda_i(\\theta) X_{il}\n$$\nThis expression is the $(k,l)$-th element of the matrix product $X^T W X$, where $W$ is a diagonal matrix with diagonal entries $W_{ii} = \\lambda_i(\\theta)$. Thus, the Hessian is:\n$$\n\\nabla^2 J(\\theta) = X^T \\text{diag}(\\lambda_1(\\theta), \\dots, \\lambda_n(\\theta)) X\n$$\nA function is convex if its Hessian matrix is positive semi-definite. For any vector $v \\in \\mathbb{R}^p$, we must examine the quadratic form $v^T (\\nabla^2 J(\\theta)) v$:\n$$\nv^T (\\nabla^2 J(\\theta)) v = v^T (X^T W X) v = (Xv)^T W (Xv)\n$$\nLet $u = Xv$, which is a vector in $\\mathbb{R}^n$. The expression becomes $u^T W u$. Since $W$ is a diagonal matrix, this is:\n$$\nu^T W u = \\sum_{i=1}^n u_i W_{ii} u_i = \\sum_{i=1}^n u_i^2 \\lambda_i(\\theta)\n$$\nThe problem statement asserts that the Poisson rates $\\lambda_i(\\theta)$ are positive for all $i$. Since $u_i^2 \\ge 0$, each term in the sum is non-negative. Therefore, $\\sum_{i=1}^n u_i^2 \\lambda_i(\\theta) \\ge 0$. This confirms that the Hessian matrix $\\nabla^2 J(\\theta)$ is positive semi-definite for all $\\theta$. Consequently, the negative log-likelihood $J(\\theta)$ is a convex function of $\\theta$.\n\nThe use of the canonical link $\\eta = \\ln \\lambda$ is crucial for this convexity. This link function ensures that the second derivative of the part of $J(\\theta)$ corresponding to observation $i$ with respect to its linear predictor $\\eta_i$, $\\frac{\\partial^2}{\\partial \\eta_i^2}(\\lambda_i - d_i \\eta_i) = \\frac{\\partial^2}{\\partial \\eta_i^2}(\\exp(\\eta_i) - d_i \\eta_i) = \\exp(\\eta_i) = \\lambda_i$, is equal to the variance function of the Poisson distribution. Since the rate parameter $\\lambda_i$ must be positive, this guarantees the diagonal matrix $W$ in the Hessian $X^T W X$ has strictly positive diagonal entries, ensuring positive semi-definiteness and thus convexity regardless of the design matrix $X$.\n\nc) Closed-form analytic expression for the log-likelihood.\n\nWe start from the general expression for the log-likelihood derived in part (a):\n$$\n\\ell(\\theta;d) = \\sum_{i=1}^{n} \\left[ d_i \\ln(\\lambda_i(\\theta)) - \\lambda_i(\\theta) - \\ln(d_i!) \\right]\n$$\nWe substitute the relationships from part (b) based on the linear model with a canonical link: $\\ln(\\lambda_i(\\theta)) = \\eta_i(\\theta) = (X\\theta)_i$ and $\\lambda_i(\\theta) = \\exp((X\\theta)_i)$.\n$$\n\\ell(\\theta;d) = \\sum_{i=1}^{n} \\left[ d_i (X\\theta)_i - \\exp((X\\theta)_i) - \\ln(d_i!) \\right]\n$$\nThis is the required closed-form expression for the log-likelihood in terms of $X$, $\\theta$, and $d$. It includes all terms that depend on $\\theta$ or $d$, as requested.", "answer": "$$\\boxed{\\sum_{i=1}^{n} \\left(d_i (X\\theta)_i - \\exp((X\\theta)_i) - \\ln(d_i!)\\right)}$$", "id": "3397372"}, {"introduction": "Real-world data is rarely perfect and is often contaminated by outliers that can severely bias parameter estimates if not handled properly. This exercise [@problem_id:3397447] introduces a robust approach to this challenge by modeling the error distribution as a mixture of Gaussians—one for the well-behaved 'inliers' and another for the 'outliers'. You will derive the elegant Expectation-Maximization (EM) algorithm to untangle these components and estimate model parameters in the presence of such complex, heavy-tailed noise.", "problem": "Consider a one-dimensional inverse problem in which an observation model is given by $y_i = h_i(x) + \\varepsilon_i$ for $i = 1, \\dots, N$, where $x$ denotes the unknown state and $h_i(x)$ is a known forward model evaluated at $x$. Suppose that, at a current iterate $x^{(t)}$, residuals $r_i = y_i - h_i(x^{(t)})$ are modeled as independent draws from a two-component mixture of Gaussian distributions to capture a heavy-tailed error process: with probability $\\pi_1$ the residual is drawn from $\\mathcal{N}(\\mu_1, \\sigma_1^2)$ (the \"inlier\" component) and with probability $\\pi_2$ the residual is drawn from $\\mathcal{N}(\\mu_2, \\sigma_2^2)$ (the \"outlier\" component), where $\\pi_1 + \\pi_2 = 1$, and $\\sigma_1^2$ and $\\sigma_2^2$ are known positive constants with $\\sigma_1^2 \\ll \\sigma_2^2$. The means $\\mu_1$, $\\mu_2$ and weights $\\pi_1$, $\\pi_2$ are unknown and must be estimated. The goal is to model the likelihood function and derive estimation updates using the Expectation-Maximization (EM) algorithm.\n\nStarting from the fundamental definition of the observed-data likelihood for mixture models and the introduction of latent indicator variables $z_{ik} \\in \\{0,1\\}$ for $k \\in \\{1,2\\}$ with $z_{i1} + z_{i2} = 1$, derive the EM algorithm for this mixture error model:\n\n1. Derive the expectation step (E-step) responsibilities $\\gamma_{ik} = \\mathbb{E}[z_{ik} \\mid r_i, \\theta^{(t)}]$, where $\\theta^{(t)} = (\\pi_1^{(t)}, \\pi_2^{(t)}, \\mu_1^{(t)}, \\mu_2^{(t)})$ denotes the current parameter estimates.\n2. Derive the maximization step (M-step) parameter updates for the mixture weights and means, assuming the variances $\\sigma_1^2$ and $\\sigma_2^2$ are fixed and known. Your derivation must use only core principles, including the product form of independent likelihoods, the mixture model definition, and Jensen’s inequality as needed for EM, avoiding any pre-stated shortcut formulas.\n3. Analyze the convergence properties of the EM algorithm for this model, explaining why the observed-data log-likelihood is non-decreasing across iterations and characterizing the nature of the fixed points. Discuss potential degeneracies and pathologies relevant to mixture models (such as label-switching symmetry, component collapse, and likelihood singularities) in the context of fixed versus free variances, and identify conditions that mitigate these issues in this specific setting.\n\nFinally, for an arbitrary residual $r_i$ and current parameter values $(\\pi_1, \\pi_2, \\mu_1, \\mu_2)$ with known $\\sigma_1^2, \\sigma_2^2$, provide the closed-form analytical expression for the responsibility of the inlier component, $\\gamma_{i1}$. This single expression is the only quantity you must return in your final answer. No rounding is required and no units are involved; express your answer in exact symbolic form.", "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n-   **Observation Model**: $y_i = h_i(x) + \\varepsilon_i$ for $i = 1, \\dots, N$.\n-   **Unknown State**: $x$.\n-   **Forward Model**: $h_i(x)$ is known.\n-   **Current Iterate**: $x^{(t)}$.\n-   **Residuals**: $r_i = y_i - h_i(x^{(t)})$.\n-   **Error Model**: Residuals $r_i$ are independent draws from a two-component mixture of Gaussian distributions.\n-   **Component 1 (Inlier)**: Drawn from $\\mathcal{N}(\\mu_1, \\sigma_1^2)$ with probability $\\pi_1$.\n-   **Component 2 (Outlier)**: Drawn from $\\mathcal{N}(\\mu_2, \\sigma_2^2)$ with probability $\\pi_2$.\n-   **Mixture Constraint**: $\\pi_1 + \\pi_2 = 1$.\n-   **Known Parameters**: Variances $\\sigma_1^2$ and $\\sigma_2^2$ are known, positive constants, with $\\sigma_1^2 \\ll \\sigma_2^2$.\n-   **Unknown Parameters to Estimate**: Means $\\mu_1, \\mu_2$ and mixture weights $\\pi_1, \\pi_2$.\n-   **Latent Variables**: Indicator variables $z_{ik} \\in \\{0,1\\}$ for $k \\in \\{1,2\\}$ with $z_{i1} + z_{i2} = 1$, where $z_{ik}=1$ if residual $r_i$ originated from component $k$.\n-   **Parameter Vector**: $\\theta = (\\pi_1, \\pi_2, \\mu_1, \\mu_2)$. Current estimate is $\\theta^{(t)}$.\n-   **Task 1**: Derive the E-step responsibilities $\\gamma_{ik} = \\mathbb{E}[z_{ik} \\mid r_i, \\theta^{(t)}]$.\n-   **Task 2**: Derive the M-step parameter updates for $\\pi_k$ and $\\mu_k$.\n-   **Task 3**: Analyze convergence and potential pathologies of the EM algorithm for this model.\n-   **Final Answer**: Provide the closed-form analytical expression for the inlier responsibility, $\\gamma_{i1}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n-   **Scientifically Grounded**: The problem describes the application of the Expectation-Maximization (EM) algorithm to a Gaussian Mixture Model (GMM). This is a standard and fundamental technique in statistics, machine learning, and data assimilation for modeling heterogeneous data and robust estimation. The use of a heavy-tailed mixture model for residuals in an inverse problem is a scientifically sound approach to handle outliers.\n-   **Well-Posed**: The problem is well-defined. It provides a clear model specification and asks for the derivation of the specific steps of the EM algorithm, an analysis of its properties, and a final, specific mathematical expression. A unique and meaningful solution exists for each part of the derivation.\n-   **Objective**: The problem is stated in precise, objective, and formal mathematical language. It is free of any subjective or ambiguous terminology.\n\nThe problem does not exhibit any of the flaws listed in the instructions. It is scientifically sound, self-contained, and formalizable. The constraints (fixed variances) are clearly stated and are key to the analysis. Therefore, the problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n***\n\nThe solution is derived in three parts as requested. Let $\\theta = (\\pi_1, \\pi_2, \\mu_1, \\mu_2)$ be the vector of parameters to be estimated. The variances $\\sigma_1^2$ and $\\sigma_2^2$ are known constants. We are given a set of $N$ residuals $R = \\{r_1, \\dots, r_N\\}$.\n\nThe probability density function (PDF) for a single residual $r_i$ under this mixture model is:\n$$p(r_i \\mid \\theta) = \\sum_{k=1}^2 \\pi_k \\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2)$$\nwhere $\\mathcal{N}(r \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(r-\\mu)^2}{2\\sigma^2}\\right)$ is the Gaussian PDF.\n\nWe introduce latent indicator variables $z_{ik} \\in \\{0,1\\}$ such that $z_{ik}=1$ if $r_i$ was generated by component $k$, and $z_{ik}=0$ otherwise, with $\\sum_{k=1}^2 z_{ik}=1$. The set of all latent variables is $Z = \\{z_{ik}\\}_{i=1, k=1}^{N, 2}$. The complete-data likelihood for a single observation $(r_i, z_i)$ is $p(r_i, z_i \\mid \\theta) = \\prod_{k=1}^2 [\\pi_k \\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2)]^{z_{ik}}$.\n\n### 1. E-Step: Derivation of Responsibilities\nThe Expectation step (E-step) computes the expectation of the latent variables given the observed data and the current parameter estimates $\\theta^{(t)}$. This expectation is the posterior probability that component $k$ was responsible for observation $r_i$, and is called the responsibility, denoted $\\gamma_{ik}$.\n\n$$ \\gamma_{ik} \\equiv \\mathbb{E}[z_{ik} \\mid r_i, \\theta^{(t)}] = P(z_{ik}=1 \\mid r_i, \\theta^{(t)}) $$\n\nUsing Bayes' theorem, this posterior probability is:\n$$ \\gamma_{ik} = \\frac{P(r_i \\mid z_{ik}=1, \\theta^{(t)}) P(z_{ik}=1 \\mid \\theta^{(t)})}{P(r_i \\mid \\theta^{(t)})} $$\n\nThe terms are identified as:\n-   $P(r_i \\mid z_{ik}=1, \\theta^{(t)}) = \\mathcal{N}(r_i \\mid \\mu_k^{(t)}, \\sigma_k^2)$ is the likelihood of observing $r_i$ given it came from component $k$.\n-   $P(z_{ik}=1 \\mid \\theta^{(t)}) = \\pi_k^{(t)}$ is the prior probability of choosing component $k$.\n-   $P(r_i \\mid \\theta^{(t)}) = \\sum_{j=1}^2 P(r_i \\mid z_{ij}=1, \\theta^{(t)}) P(z_{ij}=1 \\mid \\theta^{(t)}) = \\sum_{j=1}^2 \\pi_j^{(t)} \\mathcal{N}(r_i \\mid \\mu_j^{(t)}, \\sigma_j^2)$ is the marginal likelihood of $r_i$, obtained by summing over all possible components.\n\nSubstituting these into the expression for $\\gamma_{ik}$ yields the E-step update rule:\n$$ \\gamma_{ik} = \\frac{\\pi_k^{(t)} \\mathcal{N}(r_i \\mid \\mu_k^{(t)}, \\sigma_k^2)}{\\sum_{j=1}^2 \\pi_j^{(t)} \\mathcal{N}(r_i \\mid \\mu_j^{(t)}, \\sigma_j^2)} $$\nThese responsibilities are computed for each data point $i=1, \\dots, N$ and for each component $k=1, 2$.\n\n### 2. M-Step: Derivation of Parameter Updates\nThe Maximization step (M-step) updates the parameters $\\theta$ to maximize the expected complete-data log-likelihood, $Q(\\theta \\mid \\theta^{(t)})$, where the expectation is taken with respect to the posterior distribution of the latent variables computed in the E-step.\n\nThe complete-data log-likelihood for the entire dataset $R$ and latent variables $Z$ is:\n$$ \\ln p(R, Z \\mid \\theta) = \\ln \\left( \\prod_{i=1}^N \\prod_{k=1}^2 [\\pi_k \\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2)]^{z_{ik}} \\right) = \\sum_{i=1}^N \\sum_{k=1}^2 z_{ik} \\left( \\ln \\pi_k + \\ln \\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2) \\right) $$\n\nThe function to be maximized in the M-step is $Q(\\theta \\mid \\theta^{(t)}) = \\mathbb{E}_{Z \\mid R, \\theta^{(t)}}[\\ln p(R, Z \\mid \\theta)]$:\n$$ Q(\\theta \\mid \\theta^{(t)}) = \\sum_{i=1}^N \\sum_{k=1}^2 \\mathbb{E}[z_{ik}] \\left( \\ln \\pi_k + \\ln \\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2) \\right) $$\nwhere $\\mathbb{E}[z_{ik}]$ is the responsibility $\\gamma_{ik}$ computed in the E-step.\n$$ Q(\\theta \\mid \\theta^{(t)}) = \\sum_{i=1}^N \\sum_{k=1}^2 \\gamma_{ik} \\left( \\ln \\pi_k - \\frac{1}{2}\\ln(2\\pi\\sigma_k^2) - \\frac{(r_i - \\mu_k)^2}{2\\sigma_k^2} \\right) $$\n\nWe maximize this function with respect to $\\mu_k$ and $\\pi_k$.\n\n**Update for means $\\mu_k$**:\nWe find the maximum by setting the partial derivative of $Q$ with respect to $\\mu_k$ to zero. We only need to consider terms involving $\\mu_k$.\n$$ \\frac{\\partial Q}{\\partial \\mu_k} = \\frac{\\partial}{\\partial \\mu_k} \\sum_{i=1}^N \\gamma_{ik} \\left( -\\frac{(r_i - \\mu_k)^2}{2\\sigma_k^2} \\right) = \\sum_{i=1}^N \\gamma_{ik} \\left( \\frac{r_i - \\mu_k}{\\sigma_k^2} \\right) = 0 $$\nSince $\\sigma_k^2 > 0$, we can multiply it out:\n$$ \\sum_{i=1}^N \\gamma_{ik} (r_i - \\mu_k) = 0 \\implies \\sum_{i=1}^N \\gamma_{ik} r_i - \\mu_k \\sum_{i=1}^N \\gamma_{ik} = 0 $$\nSolving for $\\mu_k$ gives the update rule for the new estimate $\\mu_k^{(t+1)}$:\n$$ \\mu_k^{(t+1)} = \\frac{\\sum_{i=1}^N \\gamma_{ik} r_i}{\\sum_{i=1}^N \\gamma_{ik}} $$\nThis is a weighted average of the residuals, where the weights are the responsibilities. Let $N_k = \\sum_{i=1}^N \\gamma_{ik}$ be the effective number of points assigned to component $k$. Then $\\mu_k^{(t+1)} = \\frac{1}{N_k} \\sum_{i=1}^N \\gamma_{ik} r_i$.\n\n**Update for mixture weights $\\pi_k$**:\nWe maximize the terms in $Q$ involving $\\pi_k$, subject to the constraint $\\sum_{k=1}^2 \\pi_k = 1$. The relevant part of $Q$ is $\\sum_{i=1}^N \\sum_{k=1}^2 \\gamma_{ik} \\ln \\pi_k$. We use a Lagrange multiplier $\\lambda$:\n$$ \\mathcal{L}(\\pi_1, \\pi_2, \\lambda) = \\sum_{i=1}^N \\sum_{k=1}^2 \\gamma_{ik} \\ln \\pi_k + \\lambda \\left( \\sum_{k=1}^2 \\pi_k - 1 \\right) $$\nTaking the derivative with respect to $\\pi_k$ and setting to zero:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\pi_k} = \\sum_{i=1}^N \\frac{\\gamma_{ik}}{\\pi_k} + \\lambda = 0 \\implies \\pi_k = -\\frac{\\sum_{i=1}^N \\gamma_{ik}}{\\lambda} = -\\frac{N_k}{\\lambda} $$\nSumming over $k$ and using the constraint:\n$$ \\sum_{k=1}^2 \\pi_k = 1 \\implies \\sum_{k=1}^2 \\left(-\\frac{N_k}{\\lambda}\\right) = 1 \\implies -\\frac{1}{\\lambda} \\sum_{k=1}^2 N_k = 1 $$\nWe know that $\\sum_{k=1}^2 N_k = \\sum_{k=1}^2 \\sum_{i=1}^N \\gamma_{ik} = \\sum_{i=1}^N \\sum_{k=1}^2 \\gamma_{ik} = \\sum_{i=1}^N 1 = N$.\nTherefore, $-\\frac{N}{\\lambda} = 1 \\implies \\lambda = -N$.\nSubstituting $\\lambda$ back into the expression for $\\pi_k$:\n$$ \\pi_k^{(t+1)} = -\\frac{N_k}{(-N)} = \\frac{N_k}{N} = \\frac{\\sum_{i=1}^N \\gamma_{ik}}{N} $$\nThis is the average responsibility for component $k$ over all data points.\n\n### 3. Convergence Properties and Pathologies\n**Convergence**: The EM algorithm produces a sequence of parameter estimates $\\theta^{(t)}$ such that the observed-data log-likelihood $\\ell(\\theta^{(t)}) = \\ln p(R|\\theta^{(t)}) = \\sum_{i=1}^N \\ln p(r_i|\\theta^{(t)})$ is non-decreasing at each iteration, i.e., $\\ell(\\theta^{(t+1)}) \\geq \\ell(\\theta^{(t)})$. This property arises from the relationship between the log-likelihood and the $Q$ function. The change in log-likelihood can be written as:\n$$ \\ell(\\theta) - \\ell(\\theta^{(t)}) = [Q(\\theta|\\theta^{(t)}) - Q(\\theta^{(t)}|\\theta^{(t)})] + [\\text{KL}(p(Z|R,\\theta^{(t)}) \\| p(Z|R,\\theta))] $$\nwhere KL is the Kullback-Leibler divergence. The M-step is designed to find $\\theta^{(t+1)}$ that maximizes $Q(\\theta|\\theta^{(t)})$, so the first term $[Q(\\theta^{(t+1)}|\\theta^{(t)}) - Q(\\theta^{(t)}|\\theta^{(t)})]$ is non-negative. The KL divergence is always non-negative. Therefore, setting $\\theta = \\theta^{(t+1)}$ guarantees that $\\ell(\\theta^{(t+1)}) - \\ell(\\theta^{(t)}) \\geq 0$. As the log-likelihood is bounded from above (as shown below), the sequence $\\ell(\\theta^{(t)})$ must converge to a stationary point (a local maximum or saddle point) of the likelihood function.\n\n**Pathologies**:\n1.  **Label-switching symmetry**: The mixture model's likelihood function is invariant to permuting the component indices. That is, $p(r_i \\mid \\pi_1, \\mu_1, \\sigma_1^2; \\pi_2, \\mu_2, \\sigma_2^2) = p(r_i \\mid \\pi_2, \\mu_2, \\sigma_2^2; \\pi_1, \\mu_1, \\sigma_1^2)$. This means the likelihood surface has multiple, equivalent maxima. In this problem, the condition $\\sigma_1^2 \\ll \\sigma_2^2$ provides a natural way to break this symmetry by identifying component 1 as the \"inlier\" and component 2 as the \"outlier,\" thus making the solution interpretable.\n\n2.  **Likelihood Singularities (Component Collapse)**: This is a severe pathology in GMMs where the variances are also estimated. If one component's mean $\\mu_k$ becomes equal to a data point $r_i$, and its variance $\\sigma_k^2$ is allowed to approach zero, the term $\\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2)$ tends to infinity, making the overall likelihood unbounded. This leads to a spurious, degenerate solution. **In this specific problem, this pathology is completely avoided because the variances $\\sigma_1^2$ and $\\sigma_2^2$ are specified as known, fixed positive constants.** With fixed positive variances, the Gaussian PDF $\\mathcal{N}(r \\mid \\mu, \\sigma^2)$ is bounded for all $r$ and $\\mu$. Consequently, the log-likelihood function is also bounded from above, and no singularities can occur.\n\n3.  **Degeneracies at the boundary**: The EM algorithm can converge to a solution where a mixture weight $\\pi_k$ approaches 0. This happens if a component explains the data so poorly that its responsibilities $\\gamma_{ik}$ become negligible for all $i$. While sometimes viewed as a pathology, this can also be an indication that a simpler model (with fewer components) is sufficient for the data. In this two-component model, if $\\pi_2 \\to 0$, it implies the data is adequately described by a single Gaussian distribution without a significant outlier population. Regularization or Bayesian priors on $\\pi_k$ can prevent this if a two-component solution is strongly desired.\n\nThe final requirement is the closed-form expression for the responsibility of the inlier component, $\\gamma_{i1}$, for an arbitrary residual $r_i$ and current parameters $(\\pi_1, \\pi_2, \\mu_1, \\mu_2)$. Based on the E-step derivation:\n$$ \\gamma_{i1} = \\frac{\\pi_1 \\mathcal{N}(r_i \\mid \\mu_1, \\sigma_1^2)}{\\pi_1 \\mathcal{N}(r_i \\mid \\mu_1, \\sigma_1^2) + \\pi_2 \\mathcal{N}(r_i \\mid \\mu_2, \\sigma_2^2)} $$\nSubstituting the full expression for the Gaussian PDF:\n$$ \\gamma_{i1} = \\frac{\\pi_1 \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right)}{\\pi_1 \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right) + \\pi_2 \\frac{1}{\\sqrt{2\\pi\\sigma_2^2}} \\exp\\left(-\\frac{(r_i - \\mu_2)^2}{2\\sigma_2^2}\\right)} $$\nThe common factor of $\\frac{1}{\\sqrt{2\\pi}}$ in the numerator and denominator can be cancelled:\n$$ \\gamma_{i1} = \\frac{\\frac{\\pi_1}{\\sigma_1} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right)}{\\frac{\\pi_1}{\\sigma_1} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right) + \\frac{\\pi_2}{\\sigma_2} \\exp\\left(-\\frac{(r_i - \\mu_2)^2}{2\\sigma_2^2}\\right)} $$\nThis is the final analytical expression.", "answer": "$$\\boxed{\\frac{\\frac{\\pi_1}{\\sigma_1} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right)}{\\frac{\\pi_1}{\\sigma_1} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right) + \\frac{\\pi_2}{\\sigma_2} \\exp\\left(-\\frac{(r_i - \\mu_2)^2}{2\\sigma_2^2}\\right)}}$$", "id": "3397447"}]}