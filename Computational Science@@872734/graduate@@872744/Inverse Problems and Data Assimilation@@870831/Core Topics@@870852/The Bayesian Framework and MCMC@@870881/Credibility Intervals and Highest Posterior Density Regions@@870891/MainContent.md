## Introduction
In Bayesian inference, the complete answer to an [inverse problem](@entry_id:634767) is the posterior probability distribution, which encapsulates all information about unknown parameters given the observed data. However, for practical communication, interpretation, and decision-making, it is essential to distill this often complex, high-dimensional distribution into concise and meaningful summaries. This raises a fundamental question: how can we best characterize the uncertainty encoded in the posterior? This article addresses this by providing a comprehensive exploration of **[credible intervals](@entry_id:176433)** and, more specifically, **Highest Posterior Density (HPD) regions**—the standard for optimal Bayesian uncertainty reporting.

This article will guide you from foundational theory to practical application across three distinct chapters. The first chapter, **Principles and Mechanisms**, establishes the formal definitions of credible sets, critically distinguishes them from frequentist [confidence intervals](@entry_id:142297), and examines the properties and subtleties of HPD regions. Following this, the **Applications and Interdisciplinary Connections** chapter demonstrates their use in diverse fields like [data assimilation](@entry_id:153547) and for complex, multimodal posteriors arising from nonlinear systems. Finally, the **Hands-On Practices** chapter provides concrete exercises to solidify your understanding by constructing and analyzing HPD regions from posterior samples. By the end, you will have a robust theoretical and practical grasp of these essential tools for Bayesian uncertainty quantification.

## Principles and Mechanisms

In the Bayesian paradigm, all knowledge and uncertainty regarding a set of unknown parameters, denoted by the vector $\theta$, is encoded in the posterior probability distribution, $\pi(\theta \mid y)$, conditioned on observed data $y$. While the full posterior distribution is the complete answer to the inverse problem, for communication and decision-making, it is often necessary to summarize this distribution. This chapter details the principles and mechanisms behind the construction of such summaries, known as **credible sets**, with a particular focus on the widely used **Highest Posterior Density (HPD) regions**.

### Foundational Concepts of Credible Sets

A credible set is a region in the [parameter space](@entry_id:178581) that is likely to contain the true value of the parameter, according to the posterior distribution. Its definition is fundamentally probabilistic and direct.

#### Formal Definition of a Credible Set

Formally, a **$(1-\alpha)$ credible set** for a parameter $\theta$ is any measurable subset $C$ of the [parameter space](@entry_id:178581) $\Theta$ such that the [posterior probability](@entry_id:153467) of $\theta$ lying in $C$ is $1-\alpha$:

$$ P(\theta \in C \mid y) = \int_C \pi(\theta \mid y) \, d\theta = 1-\alpha $$

where $\alpha \in (0,1)$ is the level of incredibility, typically a small number like $0.05$. The requirement that $C$ be a [measurable set](@entry_id:263324) is crucial for the integral, and thus the probability, to be well-defined.

In advanced data assimilation and inverse problems, particularly those where the parameter $\theta$ is a function residing in an [infinite-dimensional space](@entry_id:138791), a rigorous measure-theoretic foundation is essential. To ensure the existence of a well-defined [posterior probability](@entry_id:153467) measure $\Pi(\cdot \mid y)$ for almost every realization of data $y$, we rely on a framework where the parameter space $(\Theta, \mathcal{B}_\Theta)$ and data space $(\mathcal{Y}, \mathcal{B}_\mathcal{Y})$ are assumed to be **standard Borel spaces** (the [measurable spaces](@entry_id:189701) associated with Polish spaces). Within this framework, the likelihood can be described by a **probability kernel**, which guarantees the existence of a **regular [conditional probability](@entry_id:151013)** that serves as the posterior. This ensures that the concept of a credible set is well-posed even in these complex settings [@problem_id:3373826].

#### Credible Intervals versus Confidence Intervals

It is of paramount importance to distinguish the Bayesian credible interval from the **frequentist confidence interval**. While both provide intervals to characterize uncertainty, their interpretation and probabilistic guarantees are fundamentally different.

A $(1-\alpha)$ **Bayesian [credible interval](@entry_id:175131)** is a statement about the parameter $\theta$, given the observed data $y$. For an interval $[a, b]$, the statement is: "Given our data $y$, there is a $(1-\alpha)$ probability that the true parameter $\theta$ lies between $a$ and $b$." Here, $\theta$ is treated as a random variable, while the interval $[a, b]$, being a function of the fixed data $y$, is fixed.

In contrast, a $(1-\alpha)$ **frequentist confidence interval** is a statement about the interval itself, not the parameter. The parameter $\theta$ is considered a fixed, unknown constant. The interval, $[L(Y), U(Y)]$, is a function of the random data $Y$. The guarantee is: "If we were to repeat the experiment many times, generating new datasets $Y$ and constructing a new interval each time, $(1-\alpha)$ of those random intervals would contain the true, fixed parameter $\theta$." The probability statement is about the procedure that generates the intervals, over the distribution of hypothetical repeated data.

These differing philosophies can lead to numerically different intervals, even for the same data. Consider estimating a Poisson rate $\lambda$ from a single count $y=0$. Using a standard non-informative Jeffreys prior, $\pi(\lambda) \propto \lambda^{-1/2}$, the posterior for $\lambda$ given $y=0$ is a Gamma distribution, $\text{Gamma}(\frac{1}{2}, 1)$. A $95\%$ [credible interval](@entry_id:175131) can be constructed from this posterior. For instance, an upper-tailed interval would be $(0, 2.996)$. The corresponding exact $95\%$ frequentist [confidence interval](@entry_id:138194) for this observation is $(0, 3.689)$ [@problem_id:3373838]. The Bayesian interval provides a direct statement of posterior belief based on the observed data, while the frequentist interval adheres to a procedure with long-run coverage guarantees. In [data assimilation](@entry_id:153547), where we typically have only one realization of the "true" system's data, the direct probabilistic interpretation of the credible interval is often considered more natural.

### Highest Posterior Density (HPD) Regions

For any given credibility level $1-\alpha$, there are infinitely many possible credible sets. For example, in one dimension, any interval $[a,b]$ containing $95\%$ of the [posterior probability](@entry_id:153467) is a valid $95\%$ credible interval. This ambiguity motivates the search for an "optimal" credible set. The **Highest Posterior Density (HPD) region** is the standard choice for such an optimal set.

An HPD region is a credible set with the additional property that the posterior density of any point inside the region is greater than or equal to the posterior density of any point outside it. That is, for a given $1-\alpha$, the $(1-\alpha)$ HPD region $C_{\text{HPD}}$ is the [level set](@entry_id:637056) of the posterior density:

$$ C_{\text{HPD}} = \{ \theta \in \Theta : \pi(\theta \mid y) \ge \lambda_\alpha \} $$

where the threshold $\lambda_\alpha$ is chosen such that $\int_{C_{\text{HPD}}} \pi(\theta \mid y) \, d\theta = 1-\alpha$. For a unimodal posterior in one dimension, the HPD region is the shortest possible [credible interval](@entry_id:175131). This property of "smallest size" (e.g., length in 1D, area in 2D, volume in nD) makes HPD regions an attractive summary of the posterior.

#### Transformation Properties and Invariance

A crucial property of any statistical summary is its behavior under [reparameterization](@entry_id:270587). If we define a new parameter $\phi = g(\theta)$, how does the credible set for $\theta$ relate to that for $\phi$?

For a general credible set $C$ with posterior probability $1-\alpha$, its image $g(C)$ will also have [posterior probability](@entry_id:153467) $1-\alpha$ provided the transformation $g$ is **injective** (one-to-one) [@problem_id:3373856]. If $g$ is many-to-one, the probability can "fold over," and the probability of $g(C)$ will generally be greater than or equal to $1-\alpha$. A particularly useful invariant credible set is the **[equal-tailed interval](@entry_id:164843)**, defined by [posterior quantiles](@entry_id:753635). For a strictly monotone transformation $g$, an [equal-tailed interval](@entry_id:164843) for $\theta$ maps directly to the corresponding [equal-tailed interval](@entry_id:164843) for $\phi$ [@problem_id:3373856].

In stark contrast, **HPD regions are not invariant under general nonlinear reparameterizations**. The shape of an HPD region is tied to the [level sets](@entry_id:151155) of a specific density function. When the [parameterization](@entry_id:265163) changes, the [change of variables](@entry_id:141386) formula introduces a Jacobian determinant term that re-weights the density, altering its level sets. For example, if $H_\theta$ is an HPD region for a parameter $\theta$, its image $g(H_\theta)$ under a logarithmic transform $\phi = \ln(\theta)$ is generally not an HPD region for $\phi$. This lack of invariance is a significant conceptual drawback of HPD regions, as the choice of [parameterization](@entry_id:265163) can sometimes be arbitrary.

#### Subtleties: Non-Uniqueness of HPD Regions

While often treated as unique, the HPD region can be non-unique in specific, albeit important, theoretical cases. Non-uniqueness arises if the posterior density $\pi(\theta \mid y)$ has a "plateau"—a region of positive measure where the density is constant—that coincides with the threshold $\lambda_\alpha$. In such a scenario, any HPD region must contain all points with density greater than $\lambda_\alpha$, but there is freedom in choosing a portion of the plateau region to include to bring the total probability up to exactly $1-\alpha$. Since many different subsets of the plateau may have the required measure, the HPD region is not uniquely defined [@problem_id:3373832].

This ambiguity can be resolved by introducing a tie-breaking rule. A mathematically elegant rule is to select the set that minimizes its boundary measure (or perimeter) among all admissible candidates. This invokes the classical **[isoperimetric problem](@entry_id:199163)**: for a fixed volume, the shape with the minimum surface area is a ball. Thus, if the plateau region is large enough, this tie-breaking rule would select a ball-shaped subset, restoring a form of uniqueness [@problem_id:3373832]. While a theoretical point, this illustrates the deep geometric underpinnings of statistical concepts.

### HPD Regions in Gaussian and Near-Gaussian Models

In many [data assimilation](@entry_id:153547) applications, posteriors are, or are approximated as, multivariate Gaussian distributions. This situation is particularly tractable and provides significant insight into the structure of posterior uncertainty.

#### The Linear-Gaussian Case: Ellipsoidal HPD Regions

Consider the common linear inverse problem with additive Gaussian noise: $y = Gx + \eta$, where $\eta \sim \mathcal{N}(0, \Gamma_{\text{obs}})$. If we also assume a Gaussian prior for the state $x \sim \mathcal{N}(m_0, \Gamma_{\text{prior}})$, the resulting posterior distribution is also Gaussian, $\pi(x \mid y) \sim \mathcal{N}(m_{\text{post}}, \Gamma_{\text{post}})$.

In this case, the HPD regions are the level sets of the Gaussian density, which are **ellipsoids** centered at the posterior mean $m_{\text{post}}$. A $(1-\alpha)$ HPD region is given by the set:

$$ C_{\text{HPD}} = \{ x \in \mathbb{R}^n : (x - m_{\text{post}})^\top \Gamma_{\text{post}}^{-1} (x - m_{\text{post}}) \le c_\alpha \} $$

The quadratic form $(x - m_{\text{post}})^\top \Gamma_{\text{post}}^{-1} (x - m_{\text{post}})$ is the squared **Mahalanobis distance** from $x$ to the [posterior mean](@entry_id:173826). For a Gaussian posterior, this quadratic form follows a **chi-squared distribution** with $n$ degrees of freedom ($\chi^2_n$), where $n$ is the dimension of the parameter space. The boundary of the HPD region is thus defined by setting the constant $c_\alpha$ to be the $(1-\alpha)$-quantile of the $\chi^2_n$ distribution.

The geometry of this posterior [ellipsoid](@entry_id:165811) is deeply connected to the properties of the forward model $G$. The posterior precision (inverse covariance) matrix is given by $\Gamma_{\text{post}}^{-1} = \Gamma_{\text{prior}}^{-1} + G^\top \Gamma_{\text{obs}}^{-1} G$. The principal axes of the [ellipsoid](@entry_id:165811) are aligned with the eigenvectors of $\Gamma_{\text{post}}$, and the lengths of these axes are inversely related to the eigenvalues. When the problem is **ill-posed**, the forward operator $G$ has small singular values in certain directions. These directions correspond to eigenvectors of $G^\top G$ with small eigenvalues. Consequently, in these directions, the data term $G^\top \Gamma_{\text{obs}}^{-1} G$ contributes little to the posterior precision, and the posterior uncertainty is large (dominated by the prior). This manifests as long axes in the posterior ellipsoid, graphically illustrating which combinations of parameters are poorly constrained by the data [@problem_id:3373893]. Conversely, directions with large singular values are well-informed by the data, leading to high posterior precision and short axes in the [ellipsoid](@entry_id:165811).

#### Connection to Variational Methods and the Laplace Approximation

There is a profound connection between the Bayesian HPD region and the [uncertainty quantification](@entry_id:138597) arising from [variational methods](@entry_id:163656) like Tikhonov-[regularized least squares](@entry_id:754212). The Tikhonov objective function to be minimized is:

$$ \mathcal{J}(x) = \|y - F(x)\|_{\Gamma_{\text{obs}}^{-1}}^2 + \|x - m_0\|_{\Gamma_{\text{prior}}^{-1}}^2 $$

Minimizing this function is equivalent to finding the **maximum a posteriori (MAP)** estimate under the assumptions of a linear forward model $F(x)=Gx$, Gaussian noise, and a Gaussian prior. The Hessian of this objective function, which characterizes the shape of the minimum, is precisely the posterior [precision matrix](@entry_id:264481) $\Gamma_{\text{post}}^{-1}$. Therefore, the "uncertainty ellipse" derived from the Hessian in a Tikhonov framework is identical to the Bayesian HPD ellipsoid under these specific assumptions [@problem_id:3373875].

For **nonlinear** forward models $F(x)$, the posterior is generally non-Gaussian. A standard and powerful technique to obtain an approximate HPD region is the **Laplace approximation**. This method approximates the posterior density with a Gaussian centered at the MAP estimate $\hat{\theta}$. The covariance of this approximating Gaussian is taken to be the inverse of the Hessian of the negative log-posterior evaluated at the mode, $H = -\nabla^2 \log \pi(\hat{\theta} \mid y)$. The resulting approximate HPD region is an ellipsoid given by $\{ \theta : (\theta - \hat{\theta})^\top H (\theta - \hat{\theta}) \le \chi^2_{d, 1-\alpha} \}$, where $d$ is the dimension of $\theta$ [@problem_id:3373834]. This provides a computationally feasible way to characterize uncertainty for a wide class of problems, though it's important to remember it is a local, [quadratic approximation](@entry_id:270629) that may fail to capture global features of the posterior [@problem_id:3373875].

### Characterizing Complex and Multimodal Posteriors

The assumption of a unimodal, near-Gaussian posterior is often a convenient simplification. In many challenging [inverse problems](@entry_id:143129), the posterior distribution can be multimodal, exhibiting multiple, distinct regions of high probability.

When the prior is a mixture of Gaussians and the likelihood is Gaussian, the posterior is also a mixture of Gaussians, whose component weights, means, and variances can be derived analytically [@problem_id:3373874]. Such posteriors can easily be multimodal. In these scenarios, simple [point estimates](@entry_id:753543) like the MAP or the posterior mean can be highly misleading. The MAP estimator, by definition, finds the point of highest density, but this peak might be extremely narrow and contain very little of the total probability mass. An analysis could thus focus on a region of the parameter space that is, in an integrated sense, very unlikely [@problem_id:3373882]. Similarly, the posterior mean might fall in a region of very low probability density between two modes.

Here, HPD regions become an indispensable tool. By construction, an HPD region identifies the set of most plausible parameter values, regardless of whether they form a single connected region. For a [multimodal posterior](@entry_id:752296), the HPD region will be a **disjoint union of intervals** (or regions in higher dimensions). This explicitly reveals the existence of multiple, competing solutions that are compatible with the data and prior knowledge. The very **topology** of the HPD set—specifically, its number of [connected components](@entry_id:141881)—is a crucial piece of information. As the credibility level $1-\alpha$ is varied, the threshold $\lambda_\alpha$ changes, and the number of components in the HPD set can change as intervals emerge around modes or merge as the threshold drops below intervening valleys [@problem_id:3373874]. Analyzing the HPD set's topology as a function of $\alpha$ provides a powerful diagnostic for understanding complex posterior landscapes.

### Uncertainty for Functions: Pointwise Intervals and Simultaneous Bands

In many [data assimilation](@entry_id:153547) contexts, the unknown parameter is not a finite-dimensional vector but a function or field, such as a temperature field over a spatial domain. In a Bayesian setting, this is naturally handled using a **Gaussian Process (GP)** prior, which leads to a GP posterior. We must then summarize uncertainty about an entire function.

A common but often misinterpreted approach is to construct **pointwise [credible intervals](@entry_id:176433)**. For each point $x$ in the domain, one can calculate a $(1-\alpha)$ [credible interval](@entry_id:175131) for the value of the function $u(x)$. While valid for each point individually, this collection of intervals does **not** constitute a $(1-\alpha)$ credible set for the function as a whole. The probability that the *entire function* lies within these pointwise intervals is typically much lower than $1-\alpha$. This is a manifestation of the **[multiple comparisons problem](@entry_id:263680)**: with many points in the domain, there are many opportunities for the function to exit the pointwise bounds at some location [@problem_id:3373822]. For a discretized grid of $K$ independent points, the actual simultaneous coverage of the set of pointwise $(1-\alpha)$ intervals is only $(1-\alpha)^K$, which rapidly approaches zero as $K$ increases.

To make a probabilistic statement about the function as a whole, one must construct a **simultaneous $(1-\alpha)$ credible band**. This is a band around the [posterior mean](@entry_id:173826) function, $m(x) \pm c \cdot \sigma(x)$, where $\sigma(x)$ is the pointwise posterior standard deviation, and the constant $c$ is chosen to guarantee $(1-\alpha)$ coverage for the [entire function](@entry_id:178769). This constant $c$ is determined by the distribution of the supremum of the standardized posterior process, $Z(x) = (u(x) - m(x))/\sigma(x)$. Specifically, $c$ must be the $(1-\alpha)$-quantile of the random variable $\sup_x |Z(x)|$ [@problem_id:3373822]. This constant $c$ will be larger than the corresponding quantile from a [standard normal distribution](@entry_id:184509) used for a pointwise interval.

Interestingly, the severity of the [multiple comparisons problem](@entry_id:263680), and thus the required width of the band, is mitigated by positive correlation in the posterior process. If the function values are strongly correlated, they tend to move together, reducing the "effective number" of independent locations where an extremum could occur. Consequently, a more strongly correlated process requires a smaller constant $c$ to achieve the same level of simultaneous coverage [@problem_id:3373822].

Finally, it is worth noting that a simultaneous credible band represents a different type of credible set than an HPD region. When discretized on a grid, the simultaneous band corresponds to a hyperrectangular region in the [parameter space](@entry_id:178581), whereas the HPD region for a multivariate normal posterior is an [ellipsoid](@entry_id:165811). These shapes are generally different, reflecting the fact that the band controls the maximum pointwise deviation, while the HPD region identifies the region of highest joint density [@problem_id:3373822].