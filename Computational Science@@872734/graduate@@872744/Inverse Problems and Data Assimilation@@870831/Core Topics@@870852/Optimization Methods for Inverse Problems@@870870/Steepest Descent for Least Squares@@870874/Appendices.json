{"hands_on_practices": [{"introduction": "The steepest descent method, while conceptually simple, can exhibit surprisingly slow convergence on certain problems. This first exercise dives into the root cause: the sensitivity of the algorithm to the scaling of the problem variables. Through a carefully constructed analytical example [@problem_id:3422227], you will explore how disparate scales in the forward operator lead to an ill-conditioned optimization landscape, and you will see how a simple variable transformation can begin to remedy this fundamental issue.", "problem": "Consider a linear inverse problem with a two-parameter state vector $x \\in \\mathbb{R}^{2}$ and a measurement operator $A \\in \\mathbb{R}^{2 \\times 2}$ relating $x$ to measured data $b \\in \\mathbb{R}^{2}$ via $y = A x$. Adopt the Least Squares (LS) objective $F(x) = \\frac{1}{2} \\|A x - b\\|_{2}^{2}$ and a Steepest Descent (SD) iteration that, from a current $x$, moves along the negative gradient direction with an exact line search along that direction. It is known in inverse problems and data assimilation that SD can be sensitive to the scaling of the components of $x$.\n\nYou are asked to analyze sensitivity to scaling via a concrete example and a variable transformation. Let\n$$\nA(\\epsilon) = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix}, \\qquad b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix},\n$$\nwhere $\\epsilon \\in (0,1)$ represents a strong scale disparity of the columns of $A(\\epsilon)$.\n\nTasks:\n1. Starting from the definitions of the LS objective and the gradient, explain why steepest descent is sensitive to the scaling of $x$-components when the columns of $A(\\epsilon)$ have very different Euclidean norms.\n2. Introduce a diagonal scaling matrix $S \\in \\mathbb{R}^{2 \\times 2}$ and the variable transformation $y = S x$. Show how the forward operator maps to $A' = A(\\epsilon) S^{-1}$ and argue for choosing $S$ via the Euclidean column norms of $A(\\epsilon)$ to equilibrate those columns.\n3. For the unscaled variables (i.e., with $S$ equal to the identity), starting at $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, perform one steepest descent step with an exact line search along the negative gradient direction. Denote the resulting iterate by $x_{1}$ and compute the LS objective value $F(x_{1})$ as a closed-form function of $\\epsilon$.\n4. Express your final answer as a single closed-form analytic expression in $\\epsilon$. No rounding is required.\n\nYour derivations must start from core definitions and well-tested facts (the LS objective, its gradient, and exact line search), without using any pre-stated shortcut formulas.", "solution": "The problem asks for an analysis of the steepest descent method's sensitivity to scaling for a linear least-squares problem, followed by a specific calculation for one iteration. We will first establish the general framework, then discuss the scaling sensitivity, and finally perform the detailed calculation.\n\nThe least-squares objective function is given by $F(x) = \\frac{1}{2} \\|Ax - b\\|_{2}^{2}$. We can write this in vector-transpose notation as $F(x) = \\frac{1}{2} (Ax - b)^T (Ax - b)$. Expanding this expression gives:\n$$\nF(x) = \\frac{1}{2} (x^T A^T - b^T)(Ax - b) = \\frac{1}{2} (x^T A^T A x - x^T A^T b - b^T A x + b^T b)\n$$\nSince $b^T A x$ is a scalar, it is equal to its transpose $(b^T A x)^T = x^T A^T b$. Thus, the objective function is:\n$$\nF(x) = \\frac{1}{2} (x^T A^T A x - 2 b^T A x + b^T b)\n$$\nTo find the direction of steepest descent, we compute the gradient of $F(x)$ with respect to $x$. Using standard rules of vector calculus ($\\nabla_x(x^T M x) = (M+M^T)x$ and $\\nabla_x(c^T x) = c$), and noting that $A^T A$ is symmetric, we get:\n$$\n\\nabla F(x) = \\frac{1}{2} (2 A^T A x - 2 A^T b) = A^T(Ax - b)\n$$\nThe steepest descent method updates the current estimate $x_k$ via the iteration $x_{k+1} = x_k + \\alpha_k p_k$, where the search direction $p_k$ is the negative gradient, $p_k = -\\nabla F(x_k)$, and $\\alpha_k$ is a step size.\n\nTask 1: Sensitivity to scaling.\nThe convergence rate of the steepest descent method is governed by the conditioning of the problem. The geometry of the level sets of the objective function $F(x)$ is determined by its Hessian matrix, $H_F = \\nabla^2 F(x)$. For the least-squares problem, the Hessian is constant:\n$$\nH_F = \\nabla (A^T(Ax-b)) = A^T A\n$$\nThe eigenvalues of the Hessian matrix $H_F$ determine the lengths of the principal axes of the ellipsoidal level sets of $F(x)$. If the eigenvalues are of vastly different magnitudes, the level sets are highly elongated (eccentric). The steepest descent direction, being normal to the level set, will be almost perpendicular to the direction of the minimum, leading to a characteristic zig-zagging pattern and slow convergence. The ratio of the largest to the smallest eigenvalue of $H_F$, $\\kappa(H_F) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$, is the condition number. A large condition number indicates poor scaling and slow convergence.\n\nFor the given problem, $A(\\epsilon) = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix}$. The Hessian is:\n$$\nH_F = A(\\epsilon)^T A(\\epsilon) = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon^2 \\end{pmatrix}\n$$\nThe eigenvalues of this diagonal matrix are $\\lambda_1 = 1$ and $\\lambda_2 = \\epsilon^2$. The condition number is $\\kappa(H_F) = \\frac{1}{\\epsilon^2}$. Since $\\epsilon \\in (0,1)$, as $\\epsilon \\to 0$, the condition number $\\kappa(H_F) \\to \\infty$. This extreme ill-conditioning, arising from the disparate Euclidean norms of the columns of $A(\\epsilon)$ ($\\|a_1\\|_2=1$, $\\|a_2\\|_2=\\epsilon$), is the reason for the method's sensitivity.\n\nTask 2: Variable transformation.\nTo mitigate this, we introduce a diagonal scaling matrix $S$ and a new variable $y$ such that $x = S^{-1}y$. The forward model $Ax=b$ becomes $A S^{-1} y = b$. The new forward operator is $A' = A S^{-1}$, and the new least-squares problem is to minimize $F'(y) = \\frac{1}{2} \\|A' y - b\\|_2^2$. The Hessian of this new problem is $H_{F'} = (A')^T A' = (S^{-1})^T A^T A S^{-1}$. The goal is to choose $S$ to make $H_{F'}$ well-conditioned, ideally with $\\kappa(H_{F'}) \\approx 1$. A standard choice is to scale the columns of the operator to have unit norm. Let $A = [a_1, a_2]$ and $S = \\text{diag}(s_1, s_2)$. Then $A' = A S^{-1} = [s_1^{-1}a_1, s_2^{-1}a_2]$. To make the new columns have unit norm, we must choose $s_i = \\|a_i\\|_2$. For $A(\\epsilon)$, we have $\\|a_1\\|_2 = 1$ and $\\|a_2\\|_2 = \\epsilon$. Thus, an optimal scaling matrix is $S = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix}$. With this choice, $A' = A(\\epsilon) S^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon^{-1} \\end{pmatrix} = I$, the identity matrix. The new Hessian would be $I^T I = I$, which is perfectly conditioned.\n\nTask 3  4: One steepest descent step for the unscaled problem.\nWe now perform the calculation for the original, unscaled problem, starting from $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe initial residual is $r_0 = Ax_0 - b = -b = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$.\nThe gradient at $x_0$ is $\\nabla F(x_0) = A^T r_0 = A^T(-b)$:\n$$\n\\nabla F(x_0) = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\epsilon \\end{pmatrix}\n$$\nThe search direction is the negative gradient:\n$$\np_0 = -\\nabla F(x_0) = \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix}\n$$\nThe next iterate is $x_1 = x_0 + \\alpha p_0 = \\alpha p_0$. We find the optimal step size $\\alpha$ by minimizing $F(x_0 + \\alpha p_0)$ with respect to $\\alpha$. Let $g(\\alpha) = F(x_0 + \\alpha p_0)$. The minimum is found where $\\frac{dg}{d\\alpha} = 0$.\n$$\n\\frac{dg}{d\\alpha} = \\frac{d}{d\\alpha} F(x_0+\\alpha p_0) = \\nabla F(x_0 + \\alpha p_0)^T p_0 = 0\n$$\nUsing the gradient formula, $\\nabla F(x_0+\\alpha p_0) = A^T(A(x_0+\\alpha p_0)-b) = A^T(Ax_0-b) + \\alpha A^T A p_0 = \\nabla F(x_0) + \\alpha A^T A p_0$.\nSubstituting this into the condition for the minimum:\n$$\n(\\nabla F(x_0) + \\alpha A^T A p_0)^T p_0 = 0 \\implies \\nabla F(x_0)^T p_0 + \\alpha p_0^T A^T A p_0 = 0\n$$\nUsing $p_0 = -\\nabla F(x_0)$, we get $-p_0^T p_0 + \\alpha p_0^T A^T A p_0 = 0$.\nSolving for $\\alpha$ gives the exact line search step size:\n$$\n\\alpha = \\frac{p_0^T p_0}{p_0^T A^T A p_0} = \\frac{\\|p_0\\|_2^2}{\\|A p_0\\|_2^2}\n$$\nWe now compute the terms in this expression for our specific problem.\nThe numerator is $\\|p_0\\|_2^2 = 1^2 + \\epsilon^2 = 1 + \\epsilon^2$.\nFor the denominator, we first compute $A p_0$:\n$$\nA p_0 = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\epsilon^2 \\end{pmatrix}\n$$\nThen, $\\|A p_0\\|_2^2 = 1^2 + (\\epsilon^2)^2 = 1 + \\epsilon^4$.\nThe step size is therefore:\n$$\n\\alpha = \\frac{1 + \\epsilon^2}{1 + \\epsilon^4}\n$$\nThe new iterate $x_1$ is:\n$$\nx_1 = x_0 + \\alpha p_0 = \\frac{1 + \\epsilon^2}{1 + \\epsilon^4} \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix} = \\begin{pmatrix} \\frac{1 + \\epsilon^2}{1 + \\epsilon^4} \\\\ \\frac{\\epsilon(1 + \\epsilon^2)}{1 + \\epsilon^4} \\end{pmatrix}\n$$\nFinally, we compute the objective function value at this new point, $F(x_1) = \\frac{1}{2} \\|A x_1 - b\\|_2^2$.\nThe new residual is $r_1 = A x_1 - b = A(\\alpha p_0) - b = \\alpha (A p_0) - b$:\n$$\nr_1 = \\frac{1 + \\epsilon^2}{1 + \\epsilon^4} \\begin{pmatrix} 1 \\\\ \\epsilon^2 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1 + \\epsilon^2}{1 + \\epsilon^4} - 1 \\\\ \\frac{\\epsilon^2(1 + \\epsilon^2)}{1 + \\epsilon^4} - 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1 + \\epsilon^2 - (1 + \\epsilon^4)}{1 + \\epsilon^4} \\\\ \\frac{\\epsilon^2 + \\epsilon^4 - (1 + \\epsilon^4)}{1 + \\epsilon^4} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\epsilon^2 - \\epsilon^4}{1 + \\epsilon^4} \\\\ \\frac{\\epsilon^2 - 1}{1 + \\epsilon^4} \\end{pmatrix}\n$$\nNow we find the squared norm of this residual:\n$$\n\\|r_1\\|_2^2 = \\left( \\frac{\\epsilon^2(1 - \\epsilon^2)}{1 + \\epsilon^4} \\right)^2 + \\left( \\frac{-(1 - \\epsilon^2)}{1 + \\epsilon^4} \\right)^2 = \\frac{\\epsilon^4(1 - \\epsilon^2)^2}{(1 + \\epsilon^4)^2} + \\frac{(1 - \\epsilon^2)^2}{(1 + \\epsilon^4)^2}\n$$\nFactoring out the common terms:\n$$\n\\|r_1\\|_2^2 = \\frac{(1 - \\epsilon^2)^2 (\\epsilon^4 + 1)}{(1 + \\epsilon^4)^2} = \\frac{(1 - \\epsilon^2)^2}{1 + \\epsilon^4}\n$$\nThe value of the objective function at $x_1$ is half of this value:\n$$\nF(x_1) = \\frac{1}{2}\\|r_1\\|_2^2 = \\frac{(1 - \\epsilon^2)^2}{2(1 + \\epsilon^4)}\n$$\nThis is the final closed-form expression for the objective value after one step of steepest descent.", "answer": "$$\n\\boxed{\\frac{(1 - \\epsilon^2)^2}{2(1 + \\epsilon^4)}}\n$$", "id": "3422227"}, {"introduction": "In practice, iterative algorithms require robust stopping criteria to terminate effectively. These criteria must handle various scenarios, such as reaching a desired solution accuracy, encountering numerical stagnation, or respecting computational budgets. This coding exercise [@problem_id:3422232] moves from theory to practice, challenging you to implement steepest descent with a hierarchy of stopping rules. You will confront scenarios involving noisy data, which calls for the Discrepancy Principle, and well-posed problems where convergence is determined by the gradient's magnitude, solidifying your understanding of how to build a practical and reliable solver.", "problem": "Consider the linear inverse problem in a finite-dimensional real vector space, where an unknown parameter vector $x \\in \\mathbb{R}^{n}$ is estimated from measurements $y \\in \\mathbb{R}^{m}$ through a known matrix $A \\in \\mathbb{R}^{m \\times n}$. Assume $y$ is contaminated by additive noise. The estimator is computed by minimizing the least-squares objective $J(x)$ defined by\n$$\nJ(x) = \\tfrac{1}{2} \\|A x - y\\|_{2}^{2}.\n$$\nImplement the steepest descent algorithm for minimizing $J(x)$, using the negative gradient direction and an exact line search along that direction at each iteration. Use the following three stopping criteria, prioritized in the order listed:\n\n- Discrepancy principle: stop at the first iteration index $k$ where the residual norm satisfies\n$$\n\\|A x_{k} - y\\|_{2} \\le \\tau \\, \\delta,\n$$\nwhere $\\delta$ is the known noise level and $\\tau  1$ is a user-chosen tolerance multiplier.\n\n- Relative gradient norm threshold: stop at the first iteration index $k$ where\n$$\n\\|g_{k}\\|_{2} \\le \\epsilon \\, \\|g_{0}\\|_{2},\n$$\nwith $g_{k}$ denoting the gradient of $J$ at $x_{k}$ and $\\epsilon  0$ a user-chosen threshold.\n\n- Maximum iteration cap: stop when the iteration count reaches a specified maximum $k_{\\max}$.\n\nAt the start of the algorithm, evaluate the discrepancy principle and the relative gradient norm threshold at $k = 0$ before performing any update.\n\nFor each test case in the suite below, initialize with $x_{0} = 0$ (the zero vector of appropriate dimension). Your program must compute the steepest descent iterations with exact line search and produce, for each test case, a list containing four entries:\n$$\n[k, \\|A x_{k} - y\\|_{2}, \\|g_{k}\\|_{2}, c],\n$$\nwhere $k$ is the total number of iterations performed, $\\|A x_{k} - y\\|_{2}$ is the final residual norm, $\\|g_{k}\\|_{2}$ is the final gradient norm, and $c$ is an integer stop reason code defined as $c = 0$ for the discrepancy principle, $c = 1$ for the relative gradient norm threshold, and $c = 2$ for the maximum iteration cap.\n\nThe final output must be a single line containing a comma-separated list of the four result lists enclosed in square brackets, for example, $\\texttt{[[k_1,r_1,g_1,c_1],[k_2,r_2,g_2,c_2],[k_3,r_3,g_3,c_3],[k_4,r_4,g_4,c_4]]}$, with actual numeric values replacing the symbols.\n\nTest Suite:\n\n- Case $1$ (overdetermined system, moderate noise; expected to trigger the discrepancy principle):\n  - $m = 5$, $n = 3$.\n  - $A = \\begin{bmatrix}\n  2  -1  0 \\\\\n  0  1  3 \\\\\n  1  0  -2 \\\\\n  3  1  1 \\\\\n  0  2  -1\n  \\end{bmatrix}$.\n  - $x^{\\star} = \\begin{bmatrix} 1 \\\\ -2 \\\\ 0.5 \\end{bmatrix}$, $\\nu = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\\\ 0.0 \\\\ -0.15 \\end{bmatrix}$, $y = A x^{\\star} + \\nu$, $\\delta = \\|\\nu\\|_{2}$.\n  - $\\tau = 1.2$, $\\epsilon = 10^{-10}$, $k_{\\max} = 500$.\n\n- Case $2$ (underdetermined system, no noise; expected to trigger the relative gradient norm threshold):\n  - $m = 3$, $n = 4$.\n  - $A = \\begin{bmatrix}\n  1  0  0  0 \\\\\n  0  1  0  1 \\\\\n  0  0  1  -1\n  \\end{bmatrix}$.\n  - $x^{\\star} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$, $\\nu = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $y = A x^{\\star}$, $\\delta = 0$.\n  - $\\tau = 1.0$, $\\epsilon = 10^{-8}$, $k_{\\max} = 10000$.\n\n- Case $3$ (square, severely ill-conditioned; expected to hit the iteration cap):\n  - $m = 4$, $n = 4$.\n  - $A = \\mathrm{diag}\\left(1, 10^{-3}, 10^{-6}, 10^{-9}\\right)$.\n  - $x^{\\star} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $\\nu = \\begin{bmatrix} 10^{-3} \\\\ -10^{-3} \\\\ 0 \\\\ 5 \\cdot 10^{-4} \\end{bmatrix}$, $y = A x^{\\star} + \\nu$, $\\delta = \\|\\nu\\|_{2}$.\n  - $\\tau = 1.01$, $\\epsilon = 10^{-12}$, $k_{\\max} = 5$.\n\n- Case $4$ (immediate discrepancy satisfaction at initialization):\n  - $m = 2$, $n = 2$.\n  - $A = \\begin{bmatrix}\n  1  0 \\\\\n  0  1\n  \\end{bmatrix}$.\n  - $x^{\\star} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$, $\\nu = \\begin{bmatrix} 10^{-2} \\\\ -2 \\cdot 10^{-2} \\end{bmatrix}$, $y = A x^{\\star} + \\nu$, $\\delta = \\|\\nu\\|_{2}$.\n  - $\\tau = 10^{5}$, $\\epsilon = 10^{-12}$, $k_{\\max} = 1000$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each result formatted as $[k,\\|A x_{k} - y\\|_{2},\\|g_{k}\\|_{2},c]$, for the four test cases in the exact order listed above.", "solution": "The user-provided problem statement is assessed to be valid. It is scientifically grounded in the principles of numerical linear algebra and optimization, specifically concerning the minimization of a least-squares objective function. The problem is well-posed, providing all necessary matrices, vectors, and parameters for each test case. The algorithmic specification, including the steepest descent method with exact line search and a prioritized set of stopping criteria, is complete and unambiguous. There are no contradictions, factual errors, or subjective elements.\n\nThe problem is to find a numerical estimate for an unknown parameter vector $x \\in \\mathbb{R}^{n}$ from a measurement vector $y \\in \\mathbb{R}^{m}$ related by the linear model $y = Ax + \\nu$, where $A \\in \\mathbb{R}^{m \\times n}$ is a known matrix and $\\nu$ represents additive noise. This is achieved by minimizing the least-squares objective function $J(x)$:\n$$\nJ(x) = \\tfrac{1}{2} \\|Ax - y\\|_{2}^{2}\n$$\nThe minimization is performed using the steepest descent iterative method. The core components of the algorithm are the gradient of the objective function, the search direction, and the step size determined by an exact line search.\n\nFirst, we derive the gradient of $J(x)$. The squared L2-norm can be written as an inner product:\n$$\nJ(x) = \\tfrac{1}{2} (Ax - y)^{T}(Ax - y) = \\tfrac{1}{2} (x^{T}A^{T} - y^{T})(Ax - y) = \\tfrac{1}{2} (x^{T}A^{T}Ax - 2y^{T}Ax + y^{T}y)\n$$\nTaking the gradient of this quadratic form with respect to $x$ yields:\n$$\ng(x) = \\nabla J(x) = \\tfrac{1}{2} (2A^{T}Ax - 2A^{T}y) = A^{T}(Ax - y)\n$$\nIf we define the residual vector as $r(x) = Ax - y$, the gradient can be compactly written as $g(x) = A^{T}r(x)$.\n\nThe steepest descent algorithm generates a sequence of estimates $\\{x_k\\}$ starting from an initial guess $x_0$. At each iteration $k$, the next estimate $x_{k+1}$ is found by moving from $x_k$ in the direction of the negative gradient, which is the direction of steepest descent. The update rule is:\n$$\nx_{k+1} = x_k + \\alpha_k p_k, \\quad \\text{where the search direction is } p_k = -g_k = -\\nabla J(x_k)\n$$\nThe step size $\\alpha_k  0$ is chosen to minimize the objective function along the search direction. This is known as an exact line search. We need to find the value of $\\alpha$ that minimizes the single-variable function $\\phi(\\alpha) = J(x_k + \\alpha p_k)$:\n$$\n\\phi(\\alpha) = \\tfrac{1}{2} \\|A(x_k + \\alpha p_k) - y\\|_{2}^{2} = \\tfrac{1}{2} \\|(Ax_k - y) + \\alpha A p_k\\|_{2}^{2} = \\tfrac{1}{2} \\|r_k + \\alpha A p_k\\|_{2}^{2}\n$$\nwhere $r_k = r(x_k)$. To find the minimum, we set the derivative of $\\phi(\\alpha)$ with respect to $\\alpha$ to zero:\n$$\n\\frac{d\\phi}{d\\alpha} = (r_k + \\alpha A p_k)^{T}(A p_k) = r_k^{T} A p_k + \\alpha (A p_k)^{T}(A p_k) = 0\n$$\nSolving for $\\alpha$, we get the optimal step size $\\alpha_k$:\n$$\n\\alpha_k = - \\frac{r_k^{T} A p_k}{(A p_k)^{T}(A p_k)} = - \\frac{r_k^{T} A p_k}{\\|A p_k\\|_{2}^{2}}\n$$\nSubstituting $p_k = -g_k$ and recalling that $g_k = A^T r_k$, the numerator becomes:\n$$\nr_k^{T} A p_k = r_k^{T} A (-g_k) = -(r_k^{T} A) g_k = -(A^{T} r_k)^{T} g_k = -g_k^{T} g_k = -\\|g_k\\|_{2}^{2}\n$$\nThus, the expression for $\\alpha_k$ simplifies to a numerically favorable form:\n$$\n\\alpha_k = \\frac{\\|g_k\\|_{2}^{2}}{\\|A p_k\\|_{2}^{2}} = \\frac{\\|g_k\\|_{2}^{2}}{\\|A g_k\\|_{2}^{2}}\n$$\n\nThe overall algorithm proceeds as follows:\n1.  Initialize the iteration counter $k=0$ and the parameter vector $x_0=0$.\n2.  Compute the initial residual $r_0 = Ax_0 - y = -y$ and gradient $g_0 = A^T r_0 = -A^T y$.\n3.  Calculate the initial residual norm $\\|r_0\\|_2$ and gradient norm $\\|g_0\\|_2$. Store $\\|g_0\\|_2$ as a reference for the relative gradient stopping criterion.\n4.  Check the stopping criteria at iteration $k=0$ in the specified order:\n    a. Discrepancy Principle: If $\\|r_0\\|_2 \\le \\tau \\delta$, stop with reason code $c=0$.\n    b. Relative Gradient Norm: If $\\|g_0\\|_2 \\le \\epsilon \\|g_0\\|_2$, stop with reason code $c=1$. This condition is met if $g_0$ is the zero vector or if $\\epsilon \\ge 1$.\n5.  Begin the main loop for iterations $k = 0, 1, \\dots, k_{\\max}-1$:\n    a. Compute the search direction $p_k = -g_k$.\n    b. Compute the optimal step size $\\alpha_k = \\|g_k\\|_{2}^{2} / \\|Ap_k\\|_{2}^{2}$. If $\\|g_k\\|_2=0$, the algorithm has converged and should terminate.\n    c. Update the parameter vector: $x_{k+1} = x_k + \\alpha_k p_k$.\n    d. Update the residual efficiently: $r_{k+1} = r_k + \\alpha_k A p_k$.\n    e. Compute the new gradient: $g_{k+1} = A^T r_{k+1}$.\n    f. Calculate the new norms $\\|r_{k+1}\\|_2$ and $\\|g_{k+1}\\|_2$.\n    g. Check the stopping criteria for iteration $k+1$:\n        i. Discrepancy Principle: If $\\|r_{k+1}\\|_2 \\le \\tau \\delta$, stop with $k+1$ iterations and reason code $c=0$.\n        ii. Relative Gradient Norm: If $\\|g_{k+1}\\|_2 \\le \\epsilon \\|g_0\\|_2$, stop with $k+1$ iterations and reason code $c=1$.\n6.  If the loop completes without any of the above criteria being met, the maximum number of iterations $k_{\\max}$ has been reached. Stop with reason code $c=2$.\n\nThis complete algorithm is implemented to process the four test cases provided. For each case, the initial data ($A$, $x^{\\star}$, $\\nu$) are used to construct the measurement vector $y$ and the noise level $\\delta$, and the algorithm is executed with the specified parameters ($\\tau, \\epsilon, k_{\\max}$). The final state $[k, \\|A x_{k} - y\\|_{2}, \\|g_{k}\\|_{2}, c]$ is recorded for each case.", "answer": "```python\nimport numpy as np\n\ndef solve_case(A, y, tau, delta, epsilon, k_max):\n    \"\"\"\n    Implements the steepest descent algorithm for least-squares minimization.\n\n    Args:\n        A (np.ndarray): The forward model matrix.\n        y (np.ndarray): The measurement vector.\n        tau (float): Tolerance multiplier for the discrepancy principle.\n        delta (float): The known noise level (L2-norm of the noise).\n        epsilon (float): Threshold for the relative gradient norm stopping criterion.\n        k_max (int): The maximum number of iterations.\n\n    Returns:\n        list: A list containing [k, residual_norm, gradient_norm, stop_code].\n    \"\"\"\n    m, n = A.shape\n    \n    # Initialization at k=0\n    k = 0\n    x_k = np.zeros(n)\n    \n    # Using np.dot for matrix-vector products\n    r_k = np.dot(A, x_k) - y\n    g_k = np.dot(A.T, r_k)\n    \n    res_norm = np.linalg.norm(r_k, 2)\n    grad_norm = np.linalg.norm(g_k, 2)\n    \n    g0_norm = grad_norm\n    \n    disc_thresh = tau * delta\n    grad_thresh = epsilon * g0_norm\n    \n    # Check stopping criteria at k=0, before any iterations\n    if res_norm = disc_thresh:\n        c = 0\n        return [k, res_norm, grad_norm, c]\n    \n    # This condition is typically only met if g0_norm is zero.\n    if g0_norm = grad_thresh:\n        c = 1\n        return [k, res_norm, grad_norm, c]\n        \n    # Iteration loop\n    for iter_count in range(k_max):\n        # Check for convergence (gradient is zero)\n        if grad_norm == 0:\n            # We are at a minimum. The relative gradient norm check should\n            # have already caught this, but this is a safeguard.\n            c = 1 \n            return [iter_count, res_norm, grad_norm, c]\n\n        # Search direction\n        p_k = -g_k\n        \n        # Exact line search step size\n        Apk = np.dot(A, p_k)\n        Apk_norm_sq = np.dot(Apk, Apk)\n        if Apk_norm_sq == 0: # This happens if p_k is in the nullspace of A\n            c = 1 # Gradient is non-zero, but we cannot improve. Converged to a LS solution.\n            return [iter_count, res_norm, grad_norm, c]\n        \n        alpha_k = (grad_norm**2) / Apk_norm_sq\n        \n        # Update step\n        x_k = x_k + alpha_k * p_k\n        # Efficient residual update\n        r_k = r_k + alpha_k * Apk\n        # Recompute gradient from the updated residual\n        g_k = np.dot(A.T, r_k)\n        \n        # Update norms for the new state\n        res_norm = np.linalg.norm(r_k, 2)\n        grad_norm = np.linalg.norm(g_k, 2)\n        \n        # Current iteration number is iter_count + 1\n        current_k = iter_count + 1\n        \n        # Check stopping criteria after the update\n        if res_norm = disc_thresh:\n            c = 0\n            return [current_k, res_norm, grad_norm, c]\n        if grad_norm = grad_thresh:\n            c = 1\n            return [current_k, res_norm, grad_norm, c]\n            \n    # Maximum iterations reached\n    c = 2\n    return [k_max, res_norm, grad_norm, c]\n\ndef solve():\n    # Case 1\n    A1 = np.array([\n        [2., -1., 0.],\n        [0., 1., 3.],\n        [1., 0., -2.],\n        [3., 1., 1.],\n        [0., 2., -1.]\n    ])\n    x_star1 = np.array([1., -2., 0.5])\n    nu1 = np.array([0.1, -0.2, 0.05, 0.0, -0.15])\n    y1 = np.dot(A1, x_star1) + nu1\n    delta1 = np.linalg.norm(nu1, 2)\n    params1 = {'A': A1, 'y': y1, 'tau': 1.2, 'delta': delta1, 'epsilon': 1e-10, 'k_max': 500}\n\n    # Case 2\n    A2 = np.array([\n        [1., 0., 0., 0.],\n        [0., 1., 0., 1.],\n        [0., 0., 1., -1.]\n    ])\n    x_star2 = np.array([1., 2., 3., 4.])\n    nu2 = np.array([0., 0., 0.])\n    y2 = np.dot(A2, x_star2) + nu2\n    delta2 = np.linalg.norm(nu2, 2)\n    params2 = {'A': A2, 'y': y2, 'tau': 1.0, 'delta': delta2, 'epsilon': 1e-8, 'k_max': 10000}\n\n    # Case 3\n    A3 = np.diag([1., 1e-3, 1e-6, 1e-9])\n    x_star3 = np.array([1., 1., 1., 1.])\n    nu3 = np.array([1e-3, -1e-3, 0., 5e-4])\n    y3 = np.dot(A3, x_star3) + nu3\n    delta3 = np.linalg.norm(nu3, 2)\n    params3 = {'A': A3, 'y': y3, 'tau': 1.01, 'delta': delta3, 'epsilon': 1e-12, 'k_max': 5}\n    \n    # Case 4\n    A4 = np.array([\n      [1., 0.],\n      [0., 1.]\n    ])\n    x_star4 = np.array([1., -1.])\n    nu4 = np.array([1e-2, -2e-2])\n    y4 = np.dot(A4, x_star4) + nu4\n    delta4 = np.linalg.norm(nu4, 2)\n    params4 = {'A': A4, 'y': y4, 'tau': 1e5, 'delta': delta4, 'epsilon': 1e-12, 'k_max': 1000}\n\n    test_cases = [params1, params2, params3, params4]\n    \n    results = []\n    for params in test_cases:\n        result = solve_case(**params)\n        results.append(result)\n    \n    # Custom string formatting to avoid spaces after commas\n    result_strings = []\n    for res in results:\n        res_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]}]\"\n        result_strings.append(res_str)\n    \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n\n```", "id": "3422232"}, {"introduction": "Many scientific problems, like data assimilation, can be framed as generalized least-squares problems that incorporate prior information. In this context, the background-error covariance matrix, denoted $B$, not only defines the structure of the problem but also provides a powerful, physically motivated preconditioner where the preconditioning matrix $P$ is set to $B$. This advanced practice [@problem_id:3422256] synthesizes the previous concepts within the framework of 3D-Var data assimilation. You will derive the components of a more complex objective function and witness firsthand how a well-chosen preconditioner, tied to the physics of the problem, dramatically improves the conditioning and reduces the computational effort required by the steepest descent method.", "problem": "Consider the Three-Dimensional Variational (3D-Var) data assimilation objective function\n$$\nf(x) = \\frac{1}{2}\\|H x - y\\|_{R^{-1}}^2 + \\frac{1}{2}\\|x - x_b\\|_{B^{-1}}^2,\n$$\nwhere $x \\in \\mathbb{R}^n$ is the state to be estimated, $x_b \\in \\mathbb{R}^n$ is the background state, $y \\in \\mathbb{R}^m$ are the observations, $H \\in \\mathbb{R}^{m \\times n}$ is the linear observation operator, $R \\in \\mathbb{R}^{m \\times m}$ is the observation-error covariance (assumed symmetric positive definite), and $B \\in \\mathbb{R}^{n \\times n}$ is the background-error covariance (assumed symmetric positive definite). The weighted norm $\\|v\\|_{M}^2$ is defined by $\\|v\\|_{M}^2 = v^\\top M v$ for any symmetric positive definite matrix $M$.\n\nStarting from the core definitions of weighted least squares and the derivative of quadratic forms, derive the steepest descent algorithm for minimizing $f(x)$ both without preconditioning and with the preconditioner $P = B$. Use exact line search at each iteration, computed from first principles (do not use canned formulas). Use the background state $x_b$ as the initial guess. Implement both algorithms and compare their behavior.\n\nYour program must, for each test case specified below, compute the following quantities:\n- The two-norm condition number of the symmetric positive definite matrix $A$ that acts as the Hessian of $f(x)$.\n- The two-norm condition number of the symmetrically preconditioned operator $B^{1/2} A B^{1/2}$, where $B^{1/2}$ is the unique symmetric square root of $B$.\n- The number of iterations required by unpreconditioned steepest descent (direction $-g$) to achieve $\\|x_k - x^\\star\\|_2 / \\|x^\\star\\|_2 \\le \\varepsilon$ with exact line search, where $x^\\star$ is the unique minimizer and $\\varepsilon = 10^{-8}$.\n- The number of iterations required by preconditioned steepest descent (direction $-B g$) to achieve the same tolerance with exact line search.\n- A boolean indicating whether preconditioning achieves strictly fewer iterations than the unpreconditioned method.\n\nUse real-valued computations and do not introduce any arbitrary unit conversions. Angles are not involved. All outputs should be numeric or boolean types. The test suite is as follows, with all matrices and vectors given explicitly:\n\n- Test case 1 (structure matched to the background):\n  - $n = 3$, $m = 3$.\n  - $B = \\mathrm{diag}(9, 1, 0.25)$.\n  - $R = I_3$.\n  - $H = 2 B^{-1/2} = \\mathrm{diag}\\left(\\frac{2}{3}, 2, 4\\right)$.\n  - $x_b = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$, $y = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 0.5 \\end{bmatrix}$.\n\n- Test case 2 (identity observation operator with heterogeneous observation variances):\n  - $n = 3$, $m = 3$.\n  - $B = \\mathrm{diag}(1, 100, 10000)$.\n  - $R = \\mathrm{diag}(1, 0.01, 100)$.\n  - $H = I_3$.\n  - $x_b = \\begin{bmatrix} 1.0 \\\\ -1.0 \\\\ 0.0 \\end{bmatrix}$, $y = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$.\n\n- Test case 3 (low-rank observation operator):\n  - $n = 3$, $m = 2$.\n  - $B = \\mathrm{diag}(4, 0.04, 25)$.\n  - $R = \\mathrm{diag}(0.5, 0.1)$.\n  - $H = \\begin{bmatrix} 1  0  0 \\\\ 0  0  1 \\end{bmatrix}$.\n  - $x_b = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 1.0 \\end{bmatrix}$, $y = \\begin{bmatrix} 1.0 \\\\ -2.0 \\end{bmatrix}$.\n\nYour implementation must:\n- Construct the quadratic form of $f(x)$ and its gradient from the definitions, identifying the symmetric positive definite matrix $A$ and the vector $b$ such that $\\nabla f(x) = A x - b$ and $x^\\star$ solves $A x^\\star = b$.\n- Use exact line search at each iteration, derived from first principles, for both the unpreconditioned ($P = I$) and preconditioned ($P = B$) steepest descent directions.\n- Compute the two-norm condition numbers by eigenvalue analysis for symmetric positive definite matrices.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one entry per test case. Each entry should itself be a list $[\\kappa(A), \\kappa(B^{1/2} A B^{1/2}), N_{\\text{unprec}}, N_{\\text{prec}}, \\text{better}]$, where $\\kappa(\\cdot)$ denotes the two-norm condition number, $N_{\\text{unprec}}$ and $N_{\\text{prec}}$ are integers, and $\\text{better}$ is a boolean. For example, the final output format must be\n$$\n[\\,[\\kappa_1,\\kappa_{\\text{pre},1},N_{\\text{unprec},1},N_{\\text{prec},1},\\text{better}_1],\\,[\\kappa_2,\\kappa_{\\text{pre},2},N_{\\text{unprec},2},N_{\\text{prec},2},\\text{better}_2],\\,[\\kappa_3,\\kappa_{\\text{pre},3},N_{\\text{unprec},3},N_{\\text{prec},3},\\text{better}_3]\\,].\n$$", "solution": "The problem requires the derivation and implementation of the steepest descent algorithm to minimize the Three-Dimensional Variational (3D-Var) data assimilation objective function, both with and without preconditioning.\n\nThe objective function is given by\n$$\nf(x) = \\frac{1}{2}\\|H x - y\\|_{R^{-1}}^2 + \\frac{1}{2}\\|x - x_b\\|_{B^{-1}}^2\n$$\nwhere $x \\in \\mathbb{R}^n$ is the state vector, $x_b \\in \\mathbb{R}^n$ is the background state, $y \\in \\mathbb{R}^m$ are observations, $H \\in \\mathbb{R}^{m \\times n}$ is the linear observation operator, and $R \\in \\mathbb{R}^{m \\times m}$ and $B \\in \\mathbb{R}^{n \\times n}$ are the symmetric positive definite (SPD) observation-error and background-error covariance matrices, respectively. The weighted norm is defined as $\\|v\\|_M^2 = v^\\top M v$.\n\nFirst, we express the objective function in a standard quadratic form. By expanding the weighted norms, we get:\n$$\nf(x) = \\frac{1}{2}(H x - y)^\\top R^{-1} (H x - y) + \\frac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b)\n$$\nExpanding both terms yields:\n$$\nf(x) = \\frac{1}{2}(x^\\top H^\\top R^{-1}Hx - x^\\top H^\\top R^{-1}y - y^\\top R^{-1}Hx + y^\\top R^{-1}y) + \\frac{1}{2}(x^\\top B^{-1}x - x^\\top B^{-1}x_b - x_b^\\top B^{-1}x + x_b^\\top B^{-1}x_b)\n$$\nSince $R$ and $B$ are symmetric, so are their inverses. Scalar transposition properties ($u^\\top M v = v^\\top M^\\top u$) allow us to combine terms involving $x$:\n$$\nf(x) = \\frac{1}{2}x^\\top (H^\\top R^{-1} H + B^{-1})x - x^\\top (H^\\top R^{-1}y + B^{-1}x_b) + C\n$$\nwhere $C = \\frac{1}{2}y^\\top R^{-1}y + \\frac{1}{2}x_b^\\top B^{-1}x_b$ is a constant that does not depend on $x$. This is the standard quadratic form $f(x) = \\frac{1}{2}x^\\top A x - x^\\top b + C$, where the Hessian matrix $A$ and the vector $b$ are identified as:\n$$\nA = H^\\top R^{-1} H + B^{-1}\n$$\n$$\nb = H^\\top R^{-1}y + B^{-1}x_b\n$$\nThe matrix $R^{-1}$ is SPD because $R$ is SPD. The matrix $H^\\top R^{-1} H$ is symmetric positive semi-definite. The matrix $B^{-1}$ is SPD because $B$ is SPD. The sum of a symmetric positive semi-definite matrix and a symmetric positive definite matrix is symmetric positive definite. Therefore, the Hessian $A$ is SPD, which guarantees that $f(x)$ is strictly convex and possesses a unique minimizer, denoted $x^\\star$.\n\nThe gradient of the objective function is:\n$$\n\\nabla f(x) = g(x) = A x - b\n$$\nThe minimizer $x^\\star$ is found by setting the gradient to zero, $\\nabla f(x^\\star) = 0$, which leads to the linear system:\n$$\nA x^\\star = b\n$$\n\nThe steepest descent algorithm is an iterative method for finding $x^\\star$. Starting from an initial guess $x_0 = x_b$, the iterates are updated according to $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is a search direction and $\\alpha_k$ is the step size. For an exact line search, $\\alpha_k$ is chosen to minimize $f(x_k + \\alpha p_k)$. The derivative of $\\phi(\\alpha) = f(x_k + \\alpha p_k)$ with respect to $\\alpha$ is $\\phi'(\\alpha) = \\nabla f(x_k + \\alpha p_k)^\\top p_k$. Setting this to zero gives:\n$$\n(A(x_k + \\alpha_k p_k) - b)^\\top p_k = 0 \\implies (A x_k - b + \\alpha_k A p_k)^\\top p_k = 0 \\implies (g_k + \\alpha_k A p_k)^\\top p_k = 0\n$$\nSolving for $\\alpha_k$ yields the general formula for the optimal step size:\n$$\n\\alpha_k = -\\frac{g_k^\\top p_k}{p_k^\\top A p_k}\n$$\n\n**1. Unpreconditioned Steepest Descent**\nThe search direction is the negative gradient, $p_k = -g_k$. Substituting this into the step size formula:\n$$\n\\alpha_k = -\\frac{g_k^\\top (-g_k)}{(-g_k)^\\top A (-g_k)} = \\frac{g_k^\\top g_k}{g_k^\\top A g_k}\n$$\nThe update rule is $x_{k+1} = x_k - \\alpha_k g_k$.\n\n**2. Preconditioned Steepest Descent with $P=B$**\nThe preconditioner $P=B$ is used to define the search direction $p_k = -P g_k = -B g_k$. Substituting this new direction into the formula for $\\alpha_k$:\n$$\n\\alpha_k = -\\frac{g_k^\\top (-B g_k)}{(-B g_k)^\\top A (-B g_k)} = \\frac{g_k^\\top B g_k}{g_k^\\top B^\\top A B g_k}\n$$\nSince $B$ is symmetric ($B^\\top = B$), this simplifies to:\n$$\n\\alpha_k = \\frac{g_k^\\top B g_k}{g_k^\\top B A B g_k}\n$$\nThe update rule is $x_{k+1} = x_k - \\alpha_k B g_k$.\n\n**Convergence Analysis**\nThe convergence rate of the steepest descent method for a quadratic function with an SPD Hessian $A$ is determined by the two-norm condition number of $A$, $\\kappa_2(A)$. For an SPD matrix $M$, $\\kappa_2(M) = \\lambda_{\\max}(M) / \\lambda_{\\min}(M)$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the maximum and minimum eigenvalues. A larger condition number implies slower convergence.\n\nPreconditioning aims to transform the problem into one with a lower condition number. Using a symmetric preconditioner $P=B$ is equivalent to applying standard steepest descent to a transformed problem. With the change of variables $\\hat{x} = B^{-1/2}x$, the objective function becomes $\\hat{f}(\\hat{x}) = \\frac{1}{2}\\hat{x}^\\top (B^{1/2} A B^{1/2}) \\hat{x} - \\hat{x}^\\top (B^{1/2} b) + C$. The convergence rate of the preconditioned algorithm is therefore governed by the condition number of the symmetrically preconditioned Hessian, $\\kappa_2(B^{1/2} A B^{1/2})$. An effective preconditioner results in $\\kappa_2(B^{1/2} A B^{1/2}) \\ll \\kappa_2(A)$. The implementation will compute these condition numbers and count the iterations required for convergence for both methods.", "answer": "```python\nimport numpy as np\n\ndef steepest_descent(A, b, x_star, x_b, B=None, method='unpreconditioned', epsilon=1e-8, max_iter=500000):\n    \"\"\"\n    Performs steepest descent for the quadratic problem 1/2*x'Ax - b'x.\n    \"\"\"\n    x = x_b.copy()\n    \n    x_star_norm = np.linalg.norm(x_star)\n    \n    if x_star_norm  1e-12:\n        # If the true solution is the zero vector, use absolute error\n        error_func = lambda x_k: np.linalg.norm(x_k - x_star)\n    else:\n        # Otherwise, use relative error\n        error_func = lambda x_k: np.linalg.norm(x_k - x_star) / x_star_norm\n\n    for k in range(max_iter + 1):\n        if error_func(x) = epsilon:\n            return k\n\n        g = A @ x - b\n        \n        # In case the initial guess is the solution\n        if np.linalg.norm(g)  1e-15:\n            return k\n\n        if method == 'unpreconditioned':\n            # Search direction p_k = -g_k\n            alpha_num = g.T @ g\n            alpha_den = g.T @ A @ g\n            if alpha_den == 0: break\n            alpha = alpha_num / alpha_den\n            x = x - alpha * g\n        elif method == 'preconditioned':\n            # Search direction p_k = -B @ g_k\n            p = B @ g\n            alpha_num = g.T @ p\n            alpha_den = p.T @ A @ p\n            if alpha_den == 0: break\n            alpha = alpha_num / alpha_den\n            x = x - alpha * p\n        else:\n            raise ValueError(\"Invalid method specified.\")\n            \n    return max_iter # Return max_iter to indicate non-convergence\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and generate the final output.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            'n': 3, 'm': 3,\n            'B': np.diag([9.0, 1.0, 0.25]),\n            'R': np.diag([1.0, 1.0, 1.0]),\n            'H': np.diag([2.0/3.0, 2.0, 4.0]),\n            'x_b': np.array([0.1, -0.2, 0.3]),\n            'y': np.array([1.0, -2.0, 0.5])\n        },\n        # Test case 2\n        {\n            'n': 3, 'm': 3,\n            'B': np.diag([1.0, 100.0, 10000.0]),\n            'R': np.diag([1.0, 0.01, 100.0]),\n            'H': np.diag([1.0, 1.0, 1.0]),\n            'x_b': np.array([1.0, -1.0, 0.0]),\n            'y': np.array([0.0, 0.0, 0.0])\n        },\n        # Test case 3\n        {\n            'n': 3, 'm': 2,\n            'B': np.diag([4.0, 0.04, 25.0]),\n            'R': np.diag([0.5, 0.1]),\n            'H': np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),\n            'x_b': np.array([0.5, -0.5, 1.0]),\n            'y': np.array([1.0, -2.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        H, R, B, x_b, y = case['H'], case['R'], case['B'], case['x_b'], case['y']\n        \n        # Invert R and B\n        R_inv = np.linalg.inv(R)\n        B_inv = np.linalg.inv(B)\n        \n        # Formulate the Hessian A and vector b for the system Ax = b\n        A = H.T @ R_inv @ H + B_inv\n        b = H.T @ R_inv @ y + B_inv @ x_b\n        \n        # Calculate the true solution x_star\n        x_star = np.linalg.solve(A, b)\n        \n        # --- Condition number calculations ---\n        # 1. Unpreconditioned Hessian A\n        eigvals_A = np.linalg.eigvalsh(A)\n        kappa_A = np.max(eigvals_A) / np.min(eigvals_A)\n        \n        # 2. Symmetrically preconditioned Hessian\n        B_sqrt = np.sqrt(B) # B is diagonal, so sqrt is element-wise\n        A_pre = B_sqrt @ A @ B_sqrt\n        eigvals_A_pre = np.linalg.eigvalsh(A_pre)\n        kappa_A_pre = np.max(eigvals_A_pre) / np.min(eigvals_A_pre)\n        \n        # --- Iteration counts ---\n        # 3. Unpreconditioned steepest descent\n        N_unprec = steepest_descent(A, b, x_star, x_b, method='unpreconditioned')\n        \n        # 4. Preconditioned steepest descent\n        N_prec = steepest_descent(A, b, x_star, x_b, B=B, method='preconditioned')\n        \n        # 5. Comparison\n        better = N_prec  N_unprec\n        \n        results.append([kappa_A, kappa_A_pre, N_unprec, N_prec, better])\n\n    # Format the final output string precisely\n    output_strings = []\n    for res in results:\n        # Convert boolean to lowercase 'true'/'false' for consistent output format\n        res_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{str(res[4]).lower()}]\"\n        output_strings.append(res_str)\n        \n    print(f\"[{','.join(output_strings)}]\")\n\nsolve()\n```", "id": "3422256"}]}