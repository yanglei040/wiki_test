{"hands_on_practices": [{"introduction": "This first exercise explores the fundamental geometry of linear least squares. Before diving into complex applications, it is crucial to understand when a problem has a perfect fit and what the residual signifies. By determining the conditions for a zero residual, you will practice connecting the algebraic statement $b \\in \\mathcal{R}(A)$ to the geometric concept of projecting a data vector onto a subspace, a core idea in data assimilation and inverse problems [@problem_id:3398138].", "problem": "Consider a linear inverse problem in which the observation operator is represented by a real matrix $A \\in \\mathbb{R}^{4 \\times 2}$ and the data vector is $b(\\alpha) \\in \\mathbb{R}^{4}$. Let $A$ be constructed with the two columns $c_{1}$ and $c_{2}$ given by\n$$\nc_{1} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\qquad c_{2} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix},\n$$\nso that\n$$\nA = \\begin{pmatrix}\n1  1 \\\\\n1  0 \\\\\n0  1 \\\\\n1  -1\n\\end{pmatrix}.\n$$\nLet the data vector depend on a real parameter $\\alpha \\in \\mathbb{R}$ as\n$$\nb(\\alpha) = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\alpha \\end{pmatrix}.\n$$\nIn the sense of linear Least Squares (LS), the residual is defined as $r(\\alpha) = b(\\alpha) - A x(\\alpha)$ for some $x(\\alpha) \\in \\mathbb{R}^{2}$ chosen to minimize the Euclidean norm $ \\| r(\\alpha) \\|_{2}$. Denote by $P$ the orthogonal projector onto the column space $\\mathcal{R}(A)$ of $A$, and by $I$ the identity on $\\mathbb{R}^{4}$. Your tasks are:\n- Using only foundational definitions of linear least squares and the normal equations, determine whether there exist values of $\\alpha$ for which the LS residual can be zero. Equivalently, ascertain for which $\\alpha$ we have $b(\\alpha) \\in \\mathcal{R}(A)$.\n- Compute the orthogonal projection $P\\,b(\\alpha)$ and the orthogonal complement $(I-P)\\,b(\\alpha)$ explicitly as functions of $\\alpha$.\n- Finally, report the unique real value of $\\alpha$ (if any) for which the LS residual is zero.\n\nThe final answer must be the single value of $\\alpha$. No rounding is required, and no units are involved.", "solution": "**Problem Validation**\n\n**Step 1: Extract Givens**\n- The observation operator is a real matrix $A \\in \\mathbb{R}^{4 \\times 2}$.\n- The columns of $A$ are $c_{1} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$ and $c_{2} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}$.\n- The matrix $A$ is given by $A = \\begin{pmatrix} 1  1 \\\\ 1  0 \\\\ 0  1 \\\\ 1  -1 \\end{pmatrix}$.\n- The data vector is $b(\\alpha) = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\alpha \\end{pmatrix}$, where $\\alpha \\in \\mathbb{R}$.\n- The residual is defined as $r(\\alpha) = b(\\alpha) - A x(\\alpha)$, where $x(\\alpha) \\in \\mathbb{R}^{2}$ is chosen to minimize the Euclidean norm $\\| r(\\alpha) \\|_{2}$.\n- $P$ is the orthogonal projector onto the column space $\\mathcal{R}(A)$ of $A$.\n- $I$ is the identity matrix on $\\mathbb{R}^{4}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a standard problem in linear algebra concerning linear least squares. It is well-posed, providing all necessary information (the matrix $A$ and the parameterized vector $b(\\alpha)$) to determine a unique solution for the requested quantities. The problem is expressed in objective and precise mathematical language. The columns of $A$ are linearly independent, as one is not a scalar multiple of the other, which ensures that the matrix $A^T A$ is invertible and a unique least squares solution exists for any given $b(\\alpha)$. The problem is fully self-contained and analytically solvable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution**\n\nThe problem asks for the value of the parameter $\\alpha$ for which the linear system $A x = b(\\alpha)$ has an exact solution, which is equivalent to the least squares residual being zero. This occurs if and only if the data vector $b(\\alpha)$ lies in the column space of the matrix $A$, denoted $\\mathcal{R}(A)$. The column space $\\mathcal{R}(A)$ is the set of all linear combinations of the columns of $A$.\n\nLet $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^2$. The condition $b(\\alpha) \\in \\mathcal{R}(A)$ means there exist scalars $x_1$ and $x_2$ such that $x_1 c_1 + x_2 c_2 = b(\\alpha)$. This translates to the following system of linear equations:\n$$\nx_1 \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 1 \\end{pmatrix} + x_2 \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\alpha \\end{pmatrix}\n$$\nThis vector equation is equivalent to a system of four scalar equations:\n$1.$ $x_1 + x_2 = 2$\n$2.$ $x_1 + 0 \\cdot x_2 = 1 \\implies x_1 = 1$\n$3.$ $0 \\cdot x_1 + x_2 = 1 \\implies x_2 = 1$\n$4.$ $x_1 - x_2 = \\alpha$\n\nFrom the second and third equations, we immediately find that $x_1 = 1$ and $x_2 = 1$. We must check if these values are consistent with the first equation. Substituting into the first equation gives $1 + 1 = 2$, which is true. Thus, the first three equations are consistent and uniquely determine $x_1=1$ and $x_2=1$.\nFor the entire system to be consistent, these values must also satisfy the fourth equation. Substituting $x_1 = 1$ and $x_2 = 1$ into the fourth equation yields:\n$$\n1 - 1 = \\alpha \\implies \\alpha = 0\n$$\nTherefore, the vector $b(\\alpha)$ is in the column space of $A$ if and only if $\\alpha = 0$. This is the unique value for which the least squares residual is zero.\n\nThe second part of the task is to compute the orthogonal projection $P\\,b(\\alpha)$ and its complement $(I-P)\\,b(\\alpha)$. The orthogonal projector onto the column space $\\mathcal{R}(A)$ is given by the formula $P = A(A^T A)^{-1}A^T$. First, we compute the matrix $A^T A$:\n$$\nA^T A = \\begin{pmatrix} 1  1  0  1 \\\\ 1  0  1  -1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  0 \\\\ 0  1 \\\\ 1  -1 \\end{pmatrix} = \\begin{pmatrix} 1\\cdot1+1\\cdot1+0\\cdot0+1\\cdot1  1\\cdot1+1\\cdot0+0\\cdot1+1\\cdot(-1) \\\\ 1\\cdot1+0\\cdot1+1\\cdot0+(-1)\\cdot1  1\\cdot1+0\\cdot0+1\\cdot1+(-1)\\cdot(-1) \\end{pmatrix}\n$$\n$$\nA^T A = \\begin{pmatrix} 3  0 \\\\ 0  3 \\end{pmatrix} = 3I_2\n$$\nwhere $I_2$ is the $2 \\times 2$ identity matrix. The fact that $A^T A$ is a diagonal matrix confirms that the columns of $A$ are orthogonal. The inverse is easily found:\n$$\n(A^T A)^{-1} = \\frac{1}{3} I_2 = \\begin{pmatrix} \\frac{1}{3}  0 \\\\ 0  \\frac{1}{3} \\end{pmatrix}\n$$\nThe projection of $b(\\alpha)$ onto $\\mathcal{R}(A)$ is the vector $A x(\\alpha)$, where $x(\\alpha)$ is the solution to the normal equations $A^T A x(\\alpha) = A^T b(\\alpha)$.\nLet's solve for $x(\\alpha)$:\n$$\n3I_2 x(\\alpha) = A^T b(\\alpha) \\implies x(\\alpha) = \\frac{1}{3} A^T b(\\alpha)\n$$\nWe compute $A^T b(\\alpha)$:\n$$\nA^T b(\\alpha) = \\begin{pmatrix} 1  1  0  1 \\\\ 1  0  1  -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\alpha \\end{pmatrix} = \\begin{pmatrix} 1\\cdot2+1\\cdot1+0\\cdot1+1\\cdot\\alpha \\\\ 1\\cdot2+0\\cdot1+1\\cdot1+(-1)\\cdot\\alpha \\end{pmatrix} = \\begin{pmatrix} 3+\\alpha \\\\ 3-\\alpha \\end{pmatrix}\n$$\nSo, the least squares solution vector is:\n$$\nx(\\alpha) = \\frac{1}{3} \\begin{pmatrix} 3+\\alpha \\\\ 3-\\alpha \\end{pmatrix} = \\begin{pmatrix} 1+\\frac{\\alpha}{3} \\\\ 1-\\frac{\\alpha}{3} \\end{pmatrix}\n$$\nNow we can compute the projection $P\\,b(\\alpha) = A x(\\alpha)$:\n$$\nP\\,b(\\alpha) = \\begin{pmatrix} 1  1 \\\\ 1  0 \\\\ 0  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} 1+\\frac{\\alpha}{3} \\\\ 1-\\frac{\\alpha}{3} \\end{pmatrix} = \\begin{pmatrix} (1+\\frac{\\alpha}{3}) + (1-\\frac{\\alpha}{3}) \\\\ 1+\\frac{\\alpha}{3} \\\\ 1-\\frac{\\alpha}{3} \\\\ (1+\\frac{\\alpha}{3}) - (1-\\frac{\\alpha}{3}) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1+\\frac{\\alpha}{3} \\\\ 1-\\frac{\\alpha}{3} \\\\ \\frac{2\\alpha}{3} \\end{pmatrix}\n$$\nThe orthogonal complement projection $(I-P)b(\\alpha)$ is the residual vector $r(\\alpha)$, given by $r(\\alpha) = b(\\alpha) - P\\,b(\\alpha)$:\n$$\n(I-P)\\,b(\\alpha) = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\alpha \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 1+\\frac{\\alpha}{3} \\\\ 1-\\frac{\\alpha}{3} \\\\ \\frac{2\\alpha}{3} \\end{pmatrix} = \\begin{pmatrix} 2-2 \\\\ 1-(1+\\frac{\\alpha}{3}) \\\\ 1-(1-\\frac{\\alpha}{3}) \\\\ \\alpha - \\frac{2\\alpha}{3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -\\frac{\\alpha}{3} \\\\ \\frac{\\alpha}{3} \\\\ \\frac{\\alpha}{3} \\end{pmatrix}\n$$\nThe LS residual is zero when $(I-P)b(\\alpha)$ is the zero vector.\n$$\n\\begin{pmatrix} 0 \\\\ -\\frac{\\alpha}{3} \\\\ \\frac{\\alpha}{3} \\\\ \\frac{\\alpha}{3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis requires $-\\frac{\\alpha}{3} = 0$ and $\\frac{\\alpha}{3} = 0$, both of which imply $\\alpha=0$. This result confirms our initial finding.\nThe unique real value of $\\alpha$ for which the LS residual is zero is $0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "3398138"}, {"introduction": "The elegance of the normal equations can be deceptive, as they conceal numerical pitfalls. This exercise illuminates one of the most significant practical challenges: ill-conditioning. You will investigate a scenario where nearly collinear columns in the observation matrix lead to a nearly singular matrix $A^T A$ in the normal equations, severely amplifying numerical errors. Calculating the condition number provides a concrete measure of this instability and highlights why alternative, more robust solution methods are often necessary in practice [@problem_id:2218032].", "problem": "In scientific computing, the normal equations method is a common approach to solving linear least squares problems of the form $A\\mathbf{x} \\approx \\mathbf{b}$. This method transforms the problem into solving a square linear system $(A^T A)\\mathbf{x} = A^T\\mathbf{b}$. However, this approach can suffer from numerical instability if the matrix $M = A^T A$ is ill-conditioned. The ill-conditioning is often exacerbated when the columns of the matrix $A$ are nearly linearly dependent.\n\nConsider an experiment where a physical quantity $y$ is modeled as a linear function of time $t$, such that $y(t) = c_0 + c_1 t$. To determine the coefficients $c_0$ and $c_1$, three measurements of $y$ are taken at times $t_1 = 100.0$, $t_2 = 101.0$, and $t_3 = 102.0$. The corresponding least squares matrix $A$ for this problem is given by:\n$$A = \\begin{pmatrix} 1  t_1 \\\\ 1  t_2 \\\\ 1  t_3 \\end{pmatrix}$$\nThe numerical stability of solving the normal equations is related to the condition number of the matrix $M = A^T A$. For a symmetric positive definite matrix such as $M$, the spectral condition number, denoted $\\kappa_2(M)$, is defined as the ratio of its largest eigenvalue to its smallest eigenvalue:\n$$\\kappa_2(M) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$$\nA large condition number indicates that the matrix is ill-conditioned.\n\nYour task is to compute the spectral condition number $\\kappa_2(A^T A)$ for the matrix $A$ defined by the measurement times above. Round your final answer to four significant figures.", "solution": "We are given $A=\\begin{pmatrix}1  t_{1}\\\\ 1  t_{2}\\\\ 1  t_{3}\\end{pmatrix}$ with $t_{1}=100$, $t_{2}=101$, $t_{3}=102$. The normal equations matrix is\n$$\nA^{T}A=\\begin{pmatrix}\n\\sum_{i=1}^{3}1  \\sum_{i=1}^{3}t_{i}\\\\\n\\sum_{i=1}^{3}t_{i}  \\sum_{i=1}^{3}t_{i}^{2}\n\\end{pmatrix}.\n$$\nCompute the sums:\n$$\n\\sum_{i=1}^{3}1=3,\\quad \\sum_{i=1}^{3}t_{i}=100+101+102=303,\\quad \\sum_{i=1}^{3}t_{i}^{2}=10000+10201+10404=30605.\n$$\nHence\n$$\nA^{T}A=\\begin{pmatrix}3  303\\\\ 303  30605\\end{pmatrix}.\n$$\nFor a symmetric $2\\times 2$ matrix $\\begin{pmatrix}a  b\\\\ b  c\\end{pmatrix}$, the eigenvalues are\n$$\n\\lambda_{\\pm}=\\frac{(a+c)\\pm\\sqrt{(a-c)^{2}+4b^{2}}}{2}.\n$$\nEquivalently, with trace $\\tau=a+c$ and determinant $\\det=ac-b^{2}$, we can write\n$$\n\\lambda_{\\pm}=\\frac{\\tau\\pm\\sqrt{\\tau^{2}-4\\det}}{2}.\n$$\nHere $a=3$, $b=303$, $c=30605$, so\n$$\n\\tau=3+30605=30608,\\qquad \\det=3\\cdot 30605-303^{2}=91815-91809=6,\n$$\nand\n$$\nD=\\tau^{2}-4\\det=30608^{2}-24=936849640.\n$$\nThus\n$$\n\\lambda_{\\max}=\\frac{\\tau+\\sqrt{D}}{2},\\qquad \\lambda_{\\min}=\\frac{\\tau-\\sqrt{D}}{2}.\n$$\nThe spectral condition number is\n$$\n\\kappa_{2}(A^{T}A)=\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}=\\frac{\\tau+\\sqrt{D}}{\\tau-\\sqrt{D}}.\n$$\nTo avoid subtractive cancellation, rewrite\n$$\n\\kappa_{2}(A^{T}A)=\\frac{(\\tau+\\sqrt{D})^{2}}{\\tau^{2}-D}=\\frac{(\\tau+\\sqrt{D})^{2}}{4\\det}=\\frac{(30608+\\sqrt{936849640})^{2}}{24}.\n$$\nCompute $\\sqrt{936849640}$ accurately. Note that $30608^{2}=936849664$, so\n$$\n\\sqrt{936849640}=\\sqrt{30608^{2}-24}\\approx 30608-\\frac{24}{2\\cdot 30608}=30608-\\frac{3}{7652}\\approx 30607.9996079456.\n$$\nHence\n$$\n\\tau+\\sqrt{D}\\approx 61215.9996079456,\n$$\nso\n$$\n(\\tau+\\sqrt{D})^{2}\\approx 3{,}747{,}398{,}608,\n$$\nand therefore\n$$\n\\kappa_{2}(A^{T}A)\\approx \\frac{3{,}747{,}398{,}608}{24}=156{,}141{,}608.666\\ldots\n$$\nRounding to four significant figures gives\n$$\n\\kappa_{2}(A^{T}A)\\approx 1.561\\times 10^{8}.\n$$", "answer": "$$\\boxed{1.561 \\times 10^{8}}$$", "id": "2218032"}, {"introduction": "Real-world inverse problems often involve not just fitting data, but also adhering to background knowledge or physical laws expressed as constraints. This advanced exercise demonstrates how to incorporate such information by solving a constrained least squares problem. You will apply the null-space method to transform the problem, effectively reducing it to an unconstrained minimization in a smaller parameter space, showcasing a powerful and widely used technique in geophysical and engineering applications [@problem_id:3398146].", "problem": "In a linear data assimilation setting, consider a state vector $x \\in \\mathbb{R}^{3}$ representing analysis increments at three spatial locations. A set of $4$ linear observations is modeled by a linear operator $A \\in \\mathbb{R}^{4 \\times 3}$ and a data vector $b \\in \\mathbb{R}^{4}$. The analysis increment is required to satisfy a linear balance constraint represented by $C \\in \\mathbb{R}^{1 \\times 3}$ and $d \\in \\mathbb{R}^{1}$. The objective is to find the unique vector $x^{\\star}$ that minimizes the squared two-norm of the observation misfit subject to the balance constraint:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|A x - b\\|_{2}^{2} \\quad \\text{subject to} \\quad C x = d.\n$$\nAssume all errors are uncorrelated with equal variance so that no weighting other than the two-norm is required. The matrices and vectors are\n$$\nA = \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  1  0 \\\\\n2  1  1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n2 \\\\ 2 \\\\ 1 \\\\ 4\n\\end{pmatrix}, \\quad\nC = \\begin{pmatrix}\n1  -1  0\n\\end{pmatrix}, \\quad\nd = \\begin{pmatrix}\n1\n\\end{pmatrix}.\n$$\nStarting from first principles (the definition of constrained least squares and basic properties of linear subspaces), parameterize the feasible set defined by the linear constraint using a basis for the nullspace of $C$ together with a particular feasible vector, and reduce the constrained problem to an unconstrained least-squares problem in the reduced coordinates. Then, using the normal equations associated with the reduced problem, compute the unique constrained minimizer $x^{\\star}$.\n\nReport only the second component $x^{\\star}_{2}$ of the constrained least-squares solution. Express your final answer as an exact fraction. No units are required.", "solution": "The problem is to find the vector $x^{\\star} \\in \\mathbb{R}^{3}$ that solves the constrained linear least-squares problem:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|A x - b\\|_{2}^{2} \\quad \\text{subject to} \\quad C x = d\n$$\nwhere the matrices $A$, $C$ and vectors $b$, $d$ are given as:\n$$\nA = \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  1  0 \\\\\n2  1  1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n2 \\\\ 2 \\\\ 1 \\\\ 4\n\\end{pmatrix}, \\quad\nC = \\begin{pmatrix}\n1  -1  0\n\\end{pmatrix}, \\quad\nd = \\begin{pmatrix}\n1\n\\end{pmatrix}.\n$$\nThe problem requires a solution method based on the parameterization of the feasible set. The feasible set is the set of all vectors $x \\in \\mathbb{R}^{3}$ that satisfy the linear constraint $C x = d$. This set forms an affine subspace, which can be expressed as the sum of a particular solution to the non-homogeneous equation and the general solution to the corresponding homogeneous equation.\n\nLet $x \\in \\mathbb{R}^{3}$ be a vector in the feasible set. It can be written as $x = x_p + x_h$, where $x_p$ is any particular solution satisfying $C x_p = d$, and $x_h$ is a vector in the nullspace of $C$, satisfying $C x_h = 0$.\n\nFirst, we find a particular solution $x_p$. The constraint is $x_1 - x_2 = 1$. A simple choice for $x_p$ is to set $x_1 = 1$ and $x_2 = 0$, with $x_3$ arbitrarily set to $0$.\n$$\nx_p = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nWe verify that $C x_p = \\begin{pmatrix} 1  -1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1$, which matches $d$.\n\nNext, we characterize the nullspace of $C$, which is the set of all vectors $x_h = \\begin{pmatrix} x_{h1} \\\\ x_{h2} \\\\ x_{h3} \\end{pmatrix}$ such that $C x_h = 0$. This gives the equation $x_{h1} - x_{h2} = 0$, or $x_{h1} = x_{h2}$. The vectors in the nullspace are of the form $\\begin{pmatrix} s \\\\ s \\\\ t \\end{pmatrix}$ for arbitrary scalars $s, t \\in \\mathbb{R}$. A basis for this two-dimensional nullspace can be formed by the vectors corresponding to $(s=1, t=0)$ and $(s=0, t=1)$:\n$$\nz_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad z_2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\nAny vector $x_h$ in the nullspace of $C$ can be written as a linear combination of these basis vectors: $x_h = w_1 z_1 + w_2 z_2 = Z w$, where $w = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$ is the vector of reduced coordinates and $Z$ is the matrix whose columns are the basis vectors:\n$$\nZ = \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix}.\n$$\nThus, any feasible vector $x$ can be parameterized as:\n$$\nx = x_p + Z w = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}.\n$$\nSubstituting this parameterization into the objective function $\\|A x - b\\|_{2}^{2}$, we obtain a reduced, unconstrained least-squares problem in terms of $w$:\n$$\n\\min_{w \\in \\mathbb{R}^{2}} \\|A (x_p + Z w) - b\\|_{2}^{2} = \\min_{w \\in \\mathbb{R}^{2}} \\|(A Z) w - (b - A x_p)\\|_{2}^{2}.\n$$\nLet $\\hat{A} = A Z$ and $\\hat{b} = b - A x_p$. We compute these terms.\n$$\n\\hat{A} = AZ = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  0 \\\\ 2  1  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1(1)+0(1)+1(0)  1(0)+0(0)+1(1) \\\\ 0(1)+1(1)+1(0)  0(0)+1(0)+1(1) \\\\ 1(1)+1(1)+0(0)  1(0)+1(0)+0(1) \\\\ 2(1)+1(1)+1(0)  2(0)+1(0)+1(1) \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 1  1 \\\\ 2  0 \\\\ 3  1 \\end{pmatrix}.\n$$\nNext, we compute $A x_p$:\n$$\nA x_p = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  0 \\\\ 2  1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{pmatrix}.\n$$\nNow we compute $\\hat{b}$:\n$$\n\\hat{b} = b - A x_p = \\begin{pmatrix} 2 \\\\ 2 \\\\ 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 2 \\end{pmatrix}.\n$$\nThe solution $w^{\\star}$ to the unconstrained problem $\\min_{w} \\|\\hat{A} w - \\hat{b}\\|_{2}^{2}$ is found by solving the normal equations:\n$$\n(\\hat{A}^T \\hat{A}) w^{\\star} = \\hat{A}^T \\hat{b}.\n$$\nWe compute the matrix $\\hat{A}^T \\hat{A}$ and the vector $\\hat{A}^T \\hat{b}$.\n$$\n\\hat{A}^T \\hat{A} = \\begin{pmatrix} 1  1  2  3 \\\\ 1  1  0  1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  1 \\\\ 2  0 \\\\ 3  1 \\end{pmatrix} = \\begin{pmatrix} 1+1+4+9  1+1+0+3 \\\\ 1+1+0+3  1+1+0+1 \\end{pmatrix} = \\begin{pmatrix} 15  5 \\\\ 5  3 \\end{pmatrix}.\n$$\n$$\n\\hat{A}^T \\hat{b} = \\begin{pmatrix} 1  1  2  3 \\\\ 1  1  0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1(1)+1(2)+2(0)+3(2) \\\\ 1(1)+1(2)+0(0)+1(2) \\end{pmatrix} = \\begin{pmatrix} 1+2+0+6 \\\\ 1+2+0+2 \\end{pmatrix} = \\begin{pmatrix} 9 \\\\ 5 \\end{pmatrix}.\n$$\nThe normal equations are a $2 \\times 2$ system for $w^{\\star}$:\n$$\n\\begin{pmatrix} 15  5 \\\\ 5  3 \\end{pmatrix} w^{\\star} = \\begin{pmatrix} 9 \\\\ 5 \\end{pmatrix}.\n$$\nTo solve for $w^{\\star}$, we find the inverse of the matrix $\\hat{A}^T \\hat{A}$. The determinant is $\\det(\\hat{A}^T \\hat{A}) = (15)(3) - (5)(5) = 45 - 25 = 20$.\nThe inverse is:\n$$\n(\\hat{A}^T \\hat{A})^{-1} = \\frac{1}{20} \\begin{pmatrix} 3  -5 \\\\ -5  15 \\end{pmatrix}.\n$$\nNow we can solve for $w^{\\star}$:\n$$\nw^{\\star} = \\frac{1}{20} \\begin{pmatrix} 3  -5 \\\\ -5  15 \\end{pmatrix} \\begin{pmatrix} 9 \\\\ 5 \\end{pmatrix} = \\frac{1}{20} \\begin{pmatrix} 3(9) - 5(5) \\\\ -5(9) + 15(5) \\end{pmatrix} = \\frac{1}{20} \\begin{pmatrix} 27 - 25 \\\\ -45 + 75 \\end{pmatrix} = \\frac{1}{20} \\begin{pmatrix} 2 \\\\ 30 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{20} \\\\ \\frac{30}{20} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix}.\n$$\nSo, $w_1^{\\star} = \\frac{1}{10}$ and $w_2^{\\star} = \\frac{3}{2}$.\nFinally, we compute the constrained minimizer $x^{\\star}$ using the optimal reduced coordinates $w^{\\star}$:\n$$\nx^{\\star} = x_p + Z w^{\\star} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{1}{10} \\\\ \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{11}{10} \\\\ \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix}.\n$$\nThe solution vector is $x^{\\star} = \\begin{pmatrix} \\frac{11}{10} \\\\ \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix}$. The problem asks for the second component of this vector, $x^{\\star}_{2}$.\n$$\nx^{\\star}_{2} = \\frac{1}{10}.\n$$", "answer": "$$ \\boxed{\\frac{1}{10}} $$", "id": "3398146"}]}