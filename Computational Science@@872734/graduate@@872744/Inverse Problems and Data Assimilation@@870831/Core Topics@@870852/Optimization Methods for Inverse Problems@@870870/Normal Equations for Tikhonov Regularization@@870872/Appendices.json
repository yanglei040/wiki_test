{"hands_on_practices": [{"introduction": "To truly grasp the origin and structure of the normal equations, our first practice involves deriving them from first principles. This exercise [@problem_id:3405686] will guide you through minimizing the Tikhonov functional to arrive at the linear system we must solve. Crucially, you will also prove the system's unique solvability by analyzing the nullspaces of the forward and regularization operators, a key step in understanding why Tikhonov regularization is effective for ill-posed problems.", "problem": "Consider a linear inverse model with unknown state vector $x \\in \\mathbb{R}^{3}$, observation operator $A \\in \\mathbb{R}^{2 \\times 3}$, and data $b \\in \\mathbb{R}^{2}$. Let\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & -1 & 0 \\\\\n0 & 1 & -1\n\\end{pmatrix}, \\qquad\nb \\;=\\; \\begin{pmatrix}\n2 \\\\\n1\n\\end{pmatrix},\n$$\nand suppose we regularize with a linear operator $L \\in \\mathbb{R}^{2 \\times 3}$ given by\n$$\nL \\;=\\; \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & -1\n\\end{pmatrix}.\n$$\nThe matrix $A$ has a nontrivial nullspace and the matrix $L$ penalizes directions complementary to that nullspace. For a fixed regularization parameter $\\lambda > 0$, consider the Tikhonov functional\n$$\nJ(x) \\;=\\; \\|A x - b\\|_{2}^{2} \\;+\\; \\lambda \\,\\|L x\\|_{2}^{2}.\n$$\nStarting from first principles in linear algebra and calculus, and without invoking any pre-stated optimality formulas, perform the following:\n- Derive the first-order optimality condition for the minimizer $x_{\\lambda}$ by computing the gradient $\\nabla J(x)$ and setting it to zero. Argue that the resulting linear system is nonsingular for all $\\lambda > 0$ by analyzing the nullspaces of $A$ and $L$.\n- Using that optimality condition, solve explicitly for $x_{\\lambda}$ as a closed-form expression in $\\lambda$.\n\nYour final answer must be the explicit expression for $x_{\\lambda}$ written as a single row vector. No rounding is required, and no units apply. Express the final answer in exact symbolic form.", "solution": "The Tikhonov functional is given by\n$$ J(x) = \\|A x - b\\|_{2}^{2} + \\lambda \\|L x\\|_{2}^{2} $$\nwhere $x \\in \\mathbb{R}^{3}$, $A \\in \\mathbb{R}^{2 \\times 3}$, $b \\in \\mathbb{R}^{2}$, $L \\in \\mathbb{R}^{2 \\times 3}$, and $\\lambda > 0$. We are given the specific matrices and vector\n$$\nA = \\begin{pmatrix}\n1 & -1 & 0 \\\\\n0 & 1 & -1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n2 \\\\\n1\n\\end{pmatrix}, \\quad\nL = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & -1\n\\end{pmatrix}.\n$$\n\nTo find the minimizer $x_{\\lambda}$ of $J(x)$, we first derive the first-order optimality condition, which is found by setting the gradient of $J(x)$ with respect to $x$ to zero. We begin by expanding the functional using the definition of the squared Euclidean norm, $\\|v\\|_2^2 = v^T v$.\n$$ J(x) = (Ax - b)^T (Ax - b) + \\lambda (Lx)^T (Lx) $$\nUsing the properties of the transpose, $(MN)^T=N^T M^T$, we expand the terms:\n$$ J(x) = (x^T A^T - b^T)(Ax - b) + \\lambda (x^T L^T L x) $$\n$$ J(x) = x^T A^T A x - x^T A^T b - b^T A x + b^T b + \\lambda x^T L^T L x $$\nSince $b^T A x$ is a scalar, it is equal to its transpose $(b^T A x)^T = x^T A^T b$. Thus, we can combine the linear terms:\n$$ J(x) = x^T (A^T A + \\lambda L^T L) x - 2 b^T A x + b^T b $$\nThis is a quadratic form in $x$. To find the gradient $\\nabla J(x) = \\frac{dJ}{dx}$, we use standard results from vector calculus: $\\nabla_x (x^T M x) = (M + M^T)x$ and $\\nabla_x (c^T x) = c$.\nThe matrix $H = A^T A + \\lambda L^T L$ is symmetric, since $(A^T A)^T = A^T (A^T)^T = A^T A$ and $(L^T L)^T = L^T (L^T)^T = L^T L$. Therefore, the gradient of the quadratic term is $2(A^T A + \\lambda L^T L)x$.\nThe linear term can be written as $-2(A^T b)^T x$, so its gradient is $-2A^T b$.\nThe term $b^T b$ is constant with respect to $x$, so its gradient is zero.\nCombining these results, the gradient of the functional is:\n$$ \\nabla J(x) = 2(A^T A + \\lambda L^T L)x - 2A^T b $$\nThe first-order optimality condition is $\\nabla J(x) = 0$:\n$$ 2(A^T A + \\lambda L^T L)x - 2A^T b = 0 $$\n$$ (A^T A + \\lambda L^T L)x = A^T b $$\nThis is the system of normal equations for the Tikhonov-regularized problem.\n\nNext, we must show that the system matrix $H(\\lambda) = A^T A + \\lambda L^T L$ is nonsingular for any $\\lambda > 0$. A matrix is nonsingular if and only if its nullspace (or kernel) contains only the zero vector. Let $v$ be a vector in the nullspace of $H(\\lambda)$, so $H(\\lambda)v = 0$.\n$$ (A^T A + \\lambda L^T L)v = 0 $$\nMultiplying from the left by $v^T$:\n$$ v^T(A^T A + \\lambda L^T L)v = v^T 0 = 0 $$\n$$ v^T A^T A v + \\lambda v^T L^T L v = 0 $$\n$$ (Av)^T(Av) + \\lambda (Lv)^T(Lv) = 0 $$\n$$ \\|Av\\|_2^2 + \\lambda \\|Lv\\|_2^2 = 0 $$\nSince norms are non-negative and we are given $\\lambda > 0$, this sum can be zero only if both terms are individually zero:\n$$ \\|Av\\|_2^2 = 0 \\quad \\text{and} \\quad \\|Lv\\|_2^2 = 0 $$\nThis implies that $Av = 0$ and $Lv = 0$. A vector $v$ in the nullspace of $H(\\lambda)$ must therefore be in the nullspace of $A$ and in the nullspace of $L$. In other words, $\\ker(H(\\lambda)) = \\ker(A) \\cap \\ker(L)$.\nLet's find these nullspaces.\nFor $\\ker(A)$, we solve $Ax=0$ for $x = (x_1, x_2, x_3)^T$:\n$$ \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nThis gives $x_1 - x_2 = 0 \\implies x_1 = x_2$ and $x_2 - x_3 = 0 \\implies x_2 = x_3$. Thus, $x_1=x_2=x_3$. Any vector in $\\ker(A)$ is of the form $c(1, 1, 1)^T$ for some scalar $c$. So, $\\ker(A) = \\text{span}\\left\\{(1, 1, 1)^T\\right\\}$.\n\nFor $\\ker(L)$, we solve $Lx=0$:\n$$ \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nThis gives $x_1=0$ and $x_2 - x_3 = 0 \\implies x_2=x_3$. Any vector in $\\ker(L)$ is of the form $c(0, 1, 1)^T$ for some scalar $c$. So, $\\ker(L) = \\text{span}\\left\\{(0, 1, 1)^T\\right\\}$.\n\nNow we find the intersection $\\ker(A) \\cap \\ker(L)$. A vector $v$ in the intersection must be a scalar multiple of $(1, 1, 1)^T$ and simultaneously a scalar multiple of $(0, 1, 1)^T$. Let $v = c_1(1, 1, 1)^T = c_2(0, 1, 1)^T$. Comparing the first components, we get $c_1 \\cdot 1 = c_2 \\cdot 0$, which implies $c_1=0$. If $c_1=0$, then $v = 0 \\cdot (1, 1, 1)^T = (0, 0, 0)^T$. The intersection contains only the zero vector: $\\ker(A) \\cap \\ker(L) = \\{0\\}$.\nSince $\\ker(H(\\lambda))=\\{0\\}$, the matrix $H(\\lambda)$ is nonsingular for all $\\lambda > 0$, and a unique solution $x_{\\lambda}$ exists.\n\nFinally, we solve for $x_{\\lambda}$ by solving the linear system. First, we compute the matrices $A^T A$ and $L^T L$, and the vector $A^T b$.\n$$ A^T = \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\\\ 0 & -1 \\end{pmatrix}, \\quad L^T = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & -1 \\end{pmatrix} $$\n$$ A^T A = \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix} $$\n$$ L^T L = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix} $$\nThe system matrix is:\n$$ H(\\lambda) = A^T A + \\lambda L^T L = \\begin{pmatrix} 1+\\lambda & -1 & 0 \\\\ -1 & 2+\\lambda & -1-\\lambda \\\\ 0 & -1-\\lambda & 1+\\lambda \\end{pmatrix} $$\nThe right-hand side is:\n$$ A^T b = \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ -1 \\end{pmatrix} $$\nThe system to solve for $x_{\\lambda} = (x_1, x_2, x_3)^T$ is:\n$$ \\begin{pmatrix} 1+\\lambda & -1 & 0 \\\\ -1 & 2+\\lambda & -1-\\lambda \\\\ 0 & -1-\\lambda & 1+\\lambda \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ -1 \\end{pmatrix} $$\nThis corresponds to the system of equations:\n1) $(1+\\lambda)x_1 - x_2 = 2$\n2) $-x_1 + (2+\\lambda)x_2 - (1+\\lambda)x_3 = -1$\n3) $-(1+\\lambda)x_2 + (1+\\lambda)x_3 = -1$\n\nFrom equation (3), since $\\lambda > 0$, $1+\\lambda \\ne 0$, so we can divide by it:\n$$ -x_2 + x_3 = -\\frac{1}{1+\\lambda} \\implies x_3 = x_2 - \\frac{1}{1+\\lambda} $$\nSubstitute this expression for $x_3$ into equation (2):\n$$ -x_1 + (2+\\lambda)x_2 - (1+\\lambda)\\left(x_2 - \\frac{1}{1+\\lambda}\\right) = -1 $$\n$$ -x_1 + (2+\\lambda)x_2 - (1+\\lambda)x_2 + 1 = -1 $$\n$$ -x_1 + (2+\\lambda - 1-\\lambda)x_2 = -2 $$\n$$ -x_1 + x_2 = -2 $$\nNow we have a system of two equations for $x_1$ and $x_2$:\n(a) $-x_1 + x_2 = -2$\n(b) $(1+\\lambda)x_1 - x_2 = 2$ (from eq. 1)\n\nAdding equations (a) and (b):\n$$ (-x_1 + x_2) + ((1+\\lambda)x_1 - x_2) = -2 + 2 $$\n$$ -x_1 + (1+\\lambda)x_1 = 0 $$\n$$ \\lambda x_1 = 0 $$\nSince $\\lambda > 0$, we must have $x_1 = 0$.\n\nSubstitute $x_1=0$ back into equation (a):\n$$ -0 + x_2 = -2 \\implies x_2 = -2 $$\nFinally, substitute $x_2=-2$ into the expression for $x_3$:\n$$ x_3 = -2 - \\frac{1}{1+\\lambda} = \\frac{-2(1+\\lambda) - 1}{1+\\lambda} = \\frac{-2-2\\lambda-1}{1+\\lambda} = -\\frac{2\\lambda+3}{\\lambda+1} $$\nThe solution vector is:\n$$ x_{\\lambda} = \\begin{pmatrix} 0 \\\\ -2 \\\\ -\\frac{2\\lambda+3}{\\lambda+1} \\end{pmatrix} $$", "answer": "$$ \\boxed{ \\begin{pmatrix} 0 & -2 & -\\frac{2\\lambda+3}{\\lambda+1} \\end{pmatrix} } $$", "id": "3405686"}, {"introduction": "Regularization is not a free lunch; it introduces a trade-off between the stability of the solution and its fidelity to the data. This exercise [@problem_id:3283983] explores this fundamental compromise by examining a simplified, noiseless scenario, revealing the inherent bias introduced by the Tikhonov penalty term. Understanding this \"regularization bias\" is critical for appreciating the role of the regularization parameter $\\lambda$ and the nature of the regularized solution.", "problem": "Consider the linear inverse problem with noiseless data modeled by $y = A x_{\\text{true}}$, where $A \\in \\mathbb{R}^{n \\times n}$ and $x_{\\text{true}} \\in \\mathbb{R}^{n}$. The Tikhonov-regularized estimate $x_{\\lambda}$ is defined as the minimizer of the objective\n$$\nJ(x) = \\lVert A x - y \\rVert_2^2 + \\lambda \\lVert x \\rVert_2^2,\n$$\nwhere $\\lambda > 0$ is the regularization parameter and $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm. Assume the special case $A = I_n$ (the $n \\times n$ identity matrix), so $y = x_{\\text{true}}$ exactly.\n\nThe question concerns whether and how Tikhonov regularization can fail when $\\lVert x_{\\text{true}} \\rVert_2$ is very large, even in the absence of noise. Which of the following statements are correct?\n\nA) In this setting, $x_{\\lambda} = x_{\\text{true}}$ for all $\\lambda > 0$, so Tikhonov does not introduce bias.\n\nB) In this setting, $x_{\\lambda}$ differs from $x_{\\text{true}}$ by a bias vector $b_{\\lambda}$ whose norm satisfies $\\lVert b_{\\lambda} \\rVert_2 = \\dfrac{\\lambda}{1+\\lambda} \\lVert x_{\\text{true}} \\rVert_2$, hence the absolute error grows linearly with $\\lVert x_{\\text{true}} \\rVert_2$.\n\nC) For fixed $\\lambda > 0$, as $\\lVert x_{\\text{true}} \\rVert_2 \\to \\infty$, the relative error $\\dfrac{\\lVert x_{\\lambda} - x_{\\text{true}} \\rVert_2}{\\lVert x_{\\text{true}} \\rVert_2}$ tends to $0$, so Tikhonov becomes exact for large signals.\n\nD) If $A$ has orthonormal columns (so $A^{\\mathsf T} A = I_n$), then even with noiseless data $y = A x_{\\text{true}}$, the Tikhonov estimate equals the unregularized least-squares solution, independent of $\\lambda$.\n\nE) In the setting $A = I_n$, the Tikhonov estimate shrinks each component of $y$ by the same factor, and this shrinkage causes underestimation that does not vanish even as noise goes to zero.\n\nSelect all correct statements.", "solution": "### Problem Validation\n\n**Step 1: Extract Givens**\n- The problem is a linear inverse problem with noiseless data modeled by $y = A x_{\\text{true}}$.\n- The matrix $A$ is in $\\mathbb{R}^{n \\times n}$ and the true solution $x_{\\text{true}}$ is in $\\mathbb{R}^{n}$.\n- The Tikhonov-regularized estimate $x_{\\lambda}$ minimizes the objective function:\n$$\nJ(x) = \\lVert A x - y \\rVert_2^2 + \\lambda \\lVert x \\rVert_2^2\n$$\n- The regularization parameter $\\lambda$ is positive, i.e., $\\lambda > 0$.\n- The symbol $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm.\n- A special case is assumed for the main question: $A = I_n$ (the $n \\times n$ identity matrix).\n- Under this special case, the data is exactly the true solution: $y = x_{\\text{true}}$.\n- The question is about the behavior of Tikhonov regularization when $\\lVert x_{\\text{true}} \\rVert_2$ is very large, in the absence of noise.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded (Critical)**: The problem is rooted in the well-established mathematical field of inverse problems and regularization theory. Tikhonov regularization is a standard and fundamental technique. All concepts are based on linear algebra and calculus. The problem is scientifically and mathematically sound.\n- **Well-Posed**: The problem is well-posed. The objective function $J(x)$ is strictly convex for $\\lambda > 0$, guaranteeing a unique minimizer $x_{\\lambda}$. The question asks for an analysis of this unique solution under specific conditions.\n- **Objective (Critical)**: The language is precise, quantitative, and free of subjective or ambiguous terminology. All terms like \"Euclidean norm\", \"identity matrix\", and the objective function are defined formally.\n- **Incomplete or Contradictory Setup**: The setup is complete and self-consistent. The assumptions $A = I_n$ and $y=x_{\\text{true}}$ are clearly stated for the specific context of the question.\n- **Other Flaws**: The problem does not exhibit any other flaws such as being unrealistic, ill-posed, trivial, or unverifiable. It is a standard conceptual problem in numerical analysis.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the solution derivation.\n\n### Solution Derivation\n\nThe Tikhonov-regularized solution $x_{\\lambda}$ is the vector $x$ that minimizes the objective function:\n$$\nJ(x) = \\lVert A x - y \\rVert_2^2 + \\lambda \\lVert x \\rVert_2^2\n$$\nTo find the minimizer, we compute the gradient of $J(x)$ with respect to $x$ and set it to zero.\n$$\nJ(x) = (Ax - y)^{\\mathsf T}(Ax - y) + \\lambda x^{\\mathsf T}x = x^{\\mathsf T}A^{\\mathsf T}Ax - 2y^{\\mathsf T}Ax + y^{\\mathsf T}y + \\lambda x^{\\mathsf T}x\n$$\nThe gradient is:\n$$\n\\nabla_x J(x) = 2A^{\\mathsf T}Ax - 2A^{\\mathsf T}y + 2\\lambda x\n$$\nSetting the gradient to zero, $\\nabla_x J(x) = 0$, gives the normal equations for Tikhonov regularization:\n$$\n(A^{\\mathsf T}A + \\lambda I)x = A^{\\mathsf T}y\n$$\nwhere $I$ is the identity matrix of the appropriate size. The solution is thus:\n$$\nx_{\\lambda} = (A^{\\mathsf T}A + \\lambda I)^{-1} A^{\\mathsf T}y\n$$\nThe problem specifies the special case where $A = I_n$ and, consequently, $y = A x_{\\text{true}} = I_n x_{\\text{true}} = x_{\\text{true}}$. Substituting these into the general solution:\n$$\nA^{\\mathsf T}A + \\lambda I = I_n^{\\mathsf T}I_n + \\lambda I_n = I_n + \\lambda I_n = (1+\\lambda)I_n\n$$\n$$\nA^{\\mathsf T}y = I_n^{\\mathsf T}x_{\\text{true}} = x_{\\text{true}}\n$$\nThe normal equations become:\n$$\n(1+\\lambda)I_n x = x_{\\text{true}}\n$$\nSolving for $x$, which is our Tikhonov estimate $x_{\\lambda}$, we get:\n$$\nx_{\\lambda} = \\frac{1}{1+\\lambda} x_{\\text{true}}\n$$\nThis expression forms the basis for evaluating each of the provided options.\n\n### Option-by-Option Analysis\n\n**A) In this setting, $x_{\\lambda} = x_{\\text{true}}$ for all $\\lambda > 0$, so Tikhonov does not introduce bias.**\nFrom our derivation, $x_{\\lambda} = \\frac{1}{1+\\lambda} x_{\\text{true}}$. Since the problem states $\\lambda > 0$, the scalar factor $\\frac{1}{1+\\lambda}$ is strictly less than $1$. Therefore, $x_{\\lambda}$ is not equal to $x_{\\text{true}}$ unless $x_{\\text{true}}=0$. The difference between the estimate $x_\\lambda$ and the true value $x_{\\text{true}}$ is the bias. In this case, the bias is $x_{\\lambda} - x_{\\text{true}} = (\\frac{1}{1+\\lambda} - 1)x_{\\text{true}} = \\frac{-\\lambda}{1+\\lambda}x_{\\text{true}}$, which is non-zero. Thus, Tikhonov regularization introduces a bias.\n**Verdict: Incorrect.**\n\n**B) In this setting, $x_{\\lambda}$ differs from $x_{\\text{true}}$ by a bias vector $b_{\\lambda}$ whose norm satisfies $\\lVert b_{\\lambda} \\rVert_2 = \\dfrac{\\lambda}{1+\\lambda} \\lVert x_{\\text{true}} \\rVert_2$, hence the absolute error grows linearly with $\\lVert x_{\\text{true}} \\rVert_2$.**\nThe bias vector is defined as $b_{\\lambda} = x_{\\lambda} - x_{\\text{true}}$. As calculated for option A,\n$$\nb_{\\lambda} = \\frac{1}{1+\\lambda}x_{\\text{true}} - x_{\\text{true}} = \\left(\\frac{1}{1+\\lambda} - \\frac{1+\\lambda}{1+\\lambda}\\right)x_{\\text{true}} = \\frac{-\\lambda}{1+\\lambda}x_{\\text{true}}\n$$\nThe norm of this bias vector is the absolute error:\n$$\n\\lVert b_{\\lambda} \\rVert_2 = \\left\\lVert \\frac{-\\lambda}{1+\\lambda}x_{\\text{true}} \\right\\rVert_2 = \\left| \\frac{-\\lambda}{1+\\lambda} \\right| \\lVert x_{\\text{true}} \\rVert_2\n$$\nSince $\\lambda > 0$, we have $\\left| \\frac{-\\lambda}{1+\\lambda} \\right| = \\frac{\\lambda}{1+\\lambda}$. So,\n$$\n\\lVert b_{\\lambda} \\rVert_2 = \\frac{\\lambda}{1+\\lambda} \\lVert x_{\\text{true}} \\rVert_2\n$$\nThis equation shows that for a fixed $\\lambda > 0$, the absolute error $\\lVert b_{\\lambda} \\rVert_2$ is directly proportional to $\\lVert x_{\\text{true}} \\rVert_2$. This constitutes linear growth.\n**Verdict: Correct.**\n\n**C) For fixed $\\lambda > 0$, as $\\lVert x_{\\text{true}} \\rVert_2 \\to \\infty$, the relative error $\\dfrac{\\lVert x_{\\lambda} - x_{\\text{true}} \\rVert_2}{\\lVert x_{\\text{true}} \\rVert_2}$ tends to $0$, so Tikhonov becomes exact for large signals.**\nThe relative error is the ratio of the absolute error to the norm of the true solution. Using the result from option B, for $x_{\\text{true}} \\neq 0$:\n$$\n\\text{Relative Error} = \\frac{\\lVert x_{\\lambda} - x_{\\text{true}} \\rVert_2}{\\lVert x_{\\text{true}} \\rVert_2} = \\frac{\\frac{\\lambda}{1+\\lambda} \\lVert x_{\\text{true}} \\rVert_2}{\\lVert x_{\\text{true}} \\rVert_2} = \\frac{\\lambda}{1+\\lambda}\n$$\nFor a fixed $\\lambda > 0$, this value is a positive constant. It does not depend on $\\lVert x_{\\text{true}} \\rVert_2$ and therefore does not tend to $0$ as $\\lVert x_{\\text{true}} \\rVert_2 \\to \\infty$. The statement is false.\n**Verdict: Incorrect.**\n\n**D) If $A$ has orthonormal columns (so $A^{\\mathsf T} A = I_n$), then even with noiseless data $y = A x_{\\text{true}}$, the Tikhonov estimate equals the unregularized least-squares solution, independent of $\\lambda$.**\nThis option presents a different scenario from the main question. Here, $A$ is an $n \\times n$ matrix with orthonormal columns, which means $A$ is an orthogonal matrix, so $A^{\\mathsf T}A = I_n$.\nThe Tikhonov estimate is $x_{\\lambda} = (A^{\\mathsf T}A + \\lambda I_n)^{-1}A^{\\mathsf T}y$. Substituting $A^{\\mathsf T}A = I_n$:\n$$\nx_{\\lambda} = (I_n + \\lambda I_n)^{-1}A^{\\mathsf T}y = ((1+\\lambda)I_n)^{-1}A^{\\mathsf T}y = \\frac{1}{1+\\lambda} A^{\\mathsf T}y\n$$\nThe unregularized least-squares solution, $x_{LS}$, minimizes $\\lVert Ax-y \\rVert_2^2$. The corresponding normal equations are $A^{\\mathsf T}Ax = A^{\\mathsf T}y$. Substituting $A^{\\mathsf T}A = I_n$:\n$$\nI_n x_{LS} = A^{\\mathsf T}y \\implies x_{LS} = A^{\\mathsf T}y\n$$\nComparing the two solutions, we see that $x_{\\lambda} = \\frac{1}{1+\\lambda} x_{LS}$. Since $\\lambda > 0$, the factor $\\frac{1}{1+\\lambda}$ is not equal to $1$. Thus, $x_{\\lambda} \\neq x_{LS}$ (unless $x_{LS}=0$). The Tikhonov estimate is dependent on $\\lambda$.\n**Verdict: Incorrect.**\n\n**E) In the setting $A = I_n$, the Tikhonov estimate shrinks each component of $y$ by the same factor, and this shrinkage causes underestimation that does not vanish even as noise goes to zero.**\nThe setting is $A=I_n$, so $y=x_{\\text{true}}$. Our derived solution is $x_{\\lambda} = \\frac{1}{1+\\lambda} x_{\\text{true}}$. This can be rewritten as $x_{\\lambda} = \\frac{1}{1+\\lambda} y$.\nThis equation means that for any component $i$, $(x_{\\lambda})_i = \\frac{1}{1+\\lambda} y_i$. This is a uniform shrinkage of every component of the vector $y$ by the factor $\\frac{1}{1+\\lambda}$. Since $\\lambda>0$, this factor is less than $1$, causing the magnitude of the estimate $\\lVert x_{\\lambda} \\rVert_2$ to be smaller than the magnitude of the true solution $\\lVert x_{\\text{true}} \\rVert_2$, which is a form of underestimation. The problem is formulated in a noiseless setting. The bias, or \"underestimation\", is $b_{\\lambda} = -\\frac{\\lambda}{1+\\lambda}x_{\\text{true}}$. This bias is inherent to the regularization and exists even with zero noise. It is a function of $\\lambda$ and $x_{\\text{true}}$, not of noise. Therefore, this underestimation does not vanish as noise goes to zero (it is present at zero noise).\n**Verdict: Correct.**", "answer": "$$\\boxed{BE}$$", "id": "3283983"}, {"introduction": "Having established the normal equations and the concept of regularization bias, we now turn to the deeper question of how regularization achieves stability. By leveraging the Singular Value Decomposition (SVD), this practice [@problem_id:3405692] deconstructs the Tikhonov solution to reveal its role as a \"spectral filter.\" You will derive the famous filter factors that show how regularization selectively dampens solution components tied to small singular values, providing stability in the face of ill-conditioning and noise.", "problem": "Consider a linear inverse problem in data assimilation where the observed data vector $\\mathbf{y} \\in \\mathbb{R}^{m}$ is modeled as $\\mathbf{y} = A \\mathbf{x}_{\\mathrm{true}} + \\boldsymbol{\\varepsilon}$ with $A \\in \\mathbb{R}^{m \\times n}$, $\\mathbf{x}_{\\mathrm{true}} \\in \\mathbb{R}^{n}$, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{m}$ representing observational noise. To stabilize the inversion, use zero-order Tikhonov regularization and define the estimator $\\mathbf{x}_{\\lambda}$ as the minimizer of the functional\n$$\nJ(\\mathbf{x}) = \\|A \\mathbf{x} - \\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{2}^{2},\n$$\nwith $\\lambda > 0$. Starting from core definitions, derive the normal equations associated with this minimization and then express the solution in the basis of the Singular Value Decomposition (SVD), where $A = U \\Sigma V^{\\top}$, $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative diagonal entries $\\sigma_{1}, \\dots, \\sigma_{r}$, where $r = \\operatorname{rank}(A)$ and $\\sigma_{i} > 0$ for $i \\le r$. Assume the data $\\mathbf{y}$ projects entirely onto a single left singular vector $u_{i}$, i.e., $\\mathbf{y} = \\beta_{i} u_{i}$ for some index $i \\in \\{1, \\dots, r\\}$ and scalar $\\beta_{i} \\in \\mathbb{R}$, with all other components zero. Let $\\sigma_{i}$ be strictly positive and small compared to the largest singular value of $A$.\n\nDefine the attenuation factor $f(\\sigma_{i}, \\lambda)$ as the ratio between the coefficient of $v_{i}$ in $\\mathbf{x}_{\\lambda}$ and the data coefficient $\\beta_{i}$. Compute $f(\\sigma_{i}, \\lambda)$ explicitly as a closed-form analytic expression in terms of $\\sigma_{i}$ and $\\lambda$. The final answer must be a single analytic expression. No rounding is required and no units should be reported.", "solution": "The problem is to find a closed-form expression for an attenuation factor related to the Tikhonov-regularized solution of a linear inverse problem. The process begins by validating the problem statement.\n\n### Step 1: Extract Givens\n-   **Linear Model**: The observed data vector $\\mathbf{y} \\in \\mathbb{R}^{m}$ is related to the true state $\\mathbf{x}_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ by $\\mathbf{y} = A \\mathbf{x}_{\\mathrm{true}} + \\boldsymbol{\\varepsilon}$, where $A \\in \\mathbb{R}^{m \\times n}$ is the forward operator and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{m}$ is observational noise.\n-   **Tikhonov Functional**: The regularized solution $\\mathbf{x}_{\\lambda}$ is the minimizer of the functional $J(\\mathbf{x}) = \\|A \\mathbf{x} - \\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{2}^{2}$ for a regularization parameter $\\lambda > 0$.\n-   **Singular Value Decomposition (SVD)**: The matrix $A$ has an SVD given by $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices. The matrix $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with non-negative entries $\\sigma_{1}, \\dots, \\sigma_{r}$ on its main diagonal, where $r = \\operatorname{rank}(A)$ and $\\sigma_{i} > 0$ for $i \\le r$.\n-   **Data Condition**: The data vector is specified as $\\mathbf{y} = \\beta_{i} u_{i}$ for some index $i \\in \\{1, \\dots, r\\}$ and a scalar coefficient $\\beta_{i} \\in \\mathbb{R}$. Here, $u_i$ is the $i$-th column of $U$.\n-   **Singular Value Condition**: The singular value $\\sigma_{i}$ corresponding to the data is strictly positive and small compared to the largest singular value of $A$.\n-   **Definition of Attenuation Factor**: The factor $f(\\sigma_{i}, \\lambda)$ is defined as the ratio of the coefficient of $v_{i}$ in the solution $\\mathbf{x}_{\\lambda}$ to the data coefficient $\\beta_{i}$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is a standard exercise in the theory of inverse problems, specifically Tikhonov regularization. The use of SVD is the canonical way to analyze such problems. The setup is fundamentally sound.\n-   **Well-Posed**: The functional $J(\\mathbf{x})$ is a sum of two squared norms. Since $A^{\\top}A$ is positive semi-definite, the matrix $A^{\\top}A + \\lambda I$ is positive definite for any $\\lambda > 0$. This guarantees that $J(\\mathbf{x})$ is strictly convex and has a unique minimum. The problem is well-posed.\n-   **Objective**: The problem is stated in precise mathematical language with all terms and variables clearly defined.\n-   **Completeness**: All necessary information to derive the solution is provided. The simplification of the data vector to a single SVD component is a standard technique to isolate and analyze the regularizer's effect, not a flaw.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed to the solution.\n\nThe Tikhonov regularized solution $\\mathbf{x}_{\\lambda}$ is the vector $\\mathbf{x}$ that minimizes the functional:\n$$J(\\mathbf{x}) = \\|A \\mathbf{x} - \\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{2}^{2}$$\nThis can be written using the inner product as:\n$$J(\\mathbf{x}) = (A \\mathbf{x} - \\mathbf{y})^{\\top}(A \\mathbf{x} - \\mathbf{y}) + \\lambda \\mathbf{x}^{\\top}\\mathbf{x}$$\nExpanding the terms, we get:\n$$J(\\mathbf{x}) = \\mathbf{x}^{\\top}A^{\\top}A\\mathbf{x} - 2\\mathbf{y}^{\\top}A\\mathbf{x} + \\mathbf{y}^{\\top}\\mathbf{y} + \\lambda \\mathbf{x}^{\\top}\\mathbf{x}$$\nTo find the minimum, we compute the gradient of $J(\\mathbf{x})$ with respect to $\\mathbf{x}$ and set it to zero. The gradient is:\n$$\\nabla_{\\mathbf{x}} J(\\mathbf{x}) = 2A^{\\top}A\\mathbf{x} - 2A^{\\top}\\mathbf{y} + 2\\lambda\\mathbf{x}$$\nSetting $\\nabla_{\\mathbf{x}} J(\\mathbf{x}) = \\mathbf{0}$ gives:\n$$A^{\\top}A\\mathbf{x}_{\\lambda} - A^{\\top}\\mathbf{y} + \\lambda\\mathbf{x}_{\\lambda} = \\mathbf{0}$$\nRearranging the terms, we obtain the normal equations for Tikhonov regularization:\n$$(A^{\\top}A + \\lambda I)\\mathbf{x}_{\\lambda} = A^{\\top}\\mathbf{y}$$\nwhere $I$ is the $n \\times n$ identity matrix. Since $\\lambda > 0$, the matrix $(A^{\\top}A + \\lambda I)$ is invertible. The solution is formally given by:\n$$\\mathbf{x}_{\\lambda} = (A^{\\top}A + \\lambda I)^{-1}A^{\\top}\\mathbf{y}$$\nTo analyze this solution, we introduce the SVD of $A$, which is $A = U \\Sigma V^{\\top}$. The transpose is $A^{\\top} = V \\Sigma^{\\top} U^{\\top}$. We substitute these into the expression for $\\mathbf{x}_{\\lambda}$.\nFirst, we compute the term $A^{\\top}A$:\n$$A^{\\top}A = (V \\Sigma^{\\top} U^{\\top})(U \\Sigma V^{\\top}) = V \\Sigma^{\\top} (U^{\\top}U) \\Sigma V^{\\top} = V (\\Sigma^{\\top}\\Sigma) V^{\\top}$$\nusing the orthogonality of $U$ ($U^{\\top}U = I_m$).\nThe normal equations become:\n$$(V \\Sigma^{\\top}\\Sigma V^{\\top} + \\lambda I)\\mathbf{x}_{\\lambda} = V \\Sigma^{\\top} U^{\\top}\\mathbf{y}$$\nSince $I = VV^{\\top}$, we can factor out $V$ and $V^{\\top}$:\n$$V(\\Sigma^{\\top}\\Sigma + \\lambda I)V^{\\top}\\mathbf{x}_{\\lambda} = V \\Sigma^{\\top} U^{\\top}\\mathbf{y}$$\nLeft-multiplying by $V^{\\top}$ and using $V^{\\top}V = I_n$:\n$$(\\Sigma^{\\top}\\Sigma + \\lambda I)V^{\\top}\\mathbf{x}_{\\lambda} = \\Sigma^{\\top} U^{\\top}\\mathbf{y}$$\nThe matrix $(\\Sigma^{\\top}\\Sigma + \\lambda I)$ is a diagonal matrix of size $n \\times n$. Its diagonal entries are $(\\sigma_{j}^{2} + \\lambda)$ for $j=1, \\dots, r$ and $\\lambda$ for $j > r$. This matrix is invertible.\nLet us define the solution and data in the SVD bases: $\\hat{\\mathbf{x}} = V^{\\top}\\mathbf{x}_{\\lambda}$ and $\\hat{\\mathbf{y}} = U^{\\top}\\mathbf{y}$. The equation becomes:\n$$(\\Sigma^{\\top}\\Sigma + \\lambda I)\\hat{\\mathbf{x}} = \\Sigma^{\\top}\\hat{\\mathbf{y}}$$\nThe $j$-th component of this vector equation is:\n$$(\\sigma_{j}^{2} + \\lambda)\\hat{x}_{j} = \\sigma_{j}\\hat{y}_{j}$$\nwhere $\\hat{x}_j$ and $\\hat{y}_j$ are the $j$-th components of $\\hat{\\mathbf{x}}$ and $\\hat{\\mathbf{y}}$ respectively, and we define $\\sigma_j=0$ for $j > r$. This gives the component-wise solution:\n$$\\hat{x}_{j} = \\frac{\\sigma_{j}}{\\sigma_{j}^{2} + \\lambda}\\hat{y}_{j} = \\frac{\\sigma_{j}}{\\sigma_{j}^{2} + \\lambda}(u_{j}^{\\top}\\mathbf{y})$$\nThe full solution $\\mathbf{x}_{\\lambda}$ can be reconstructed by transforming back from the $V$ basis:\n$$\\mathbf{x}_{\\lambda} = V\\hat{\\mathbf{x}} = \\sum_{j=1}^{n} \\hat{x}_{j} v_{j} = \\sum_{j=1}^{r} \\left(\\frac{\\sigma_{j}}{\\sigma_{j}^{2} + \\lambda}(u_{j}^{\\top}\\mathbf{y})\\right)v_{j}$$\nNow we use the specific form of the data vector given in the problem: $\\mathbf{y} = \\beta_{i} u_{i}$ for a fixed index $i \\in \\{1, \\dots, r\\}$.\nWe compute the inner product $u_{j}^{\\top}\\mathbf{y}$:\n$$u_{j}^{\\top}\\mathbf{y} = u_{j}^{\\top}(\\beta_{i}u_{i}) = \\beta_{i}(u_{j}^{\\top}u_{i})$$\nDue to the orthogonality of the columns of $U$, we have $u_{j}^{\\top}u_{i} = \\delta_{ij}$, the Kronecker delta. Thus, the inner product is non-zero only for $j = i$:\n$$u_{j}^{\\top}\\mathbf{y} = \\beta_{i}\\delta_{ij}$$\nSubstituting this into the expression for $\\mathbf{x}_{\\lambda}$:\n$$\\mathbf{x}_{\\lambda} = \\sum_{j=1}^{r} \\left(\\frac{\\sigma_{j}}{\\sigma_{j}^{2} + \\lambda}(\\beta_{i}\\delta_{ij})\\right)v_{j}$$\nThe Kronecker delta collapses the sum to a single term where $j=i$:\n$$\\mathbf{x}_{\\lambda} = \\left(\\frac{\\sigma_{i}\\beta_{i}}{\\sigma_{i}^{2} + \\lambda}\\right)v_{i}$$\nThe problem defines the attenuation factor $f(\\sigma_{i}, \\lambda)$ as the ratio between the coefficient of $v_{i}$ in $\\mathbf{x}_{\\lambda}$ and the data coefficient $\\beta_{i}$.\nFrom the expression for $\\mathbf{x}_{\\lambda}$, the coefficient of $v_{i}$ is $\\frac{\\sigma_{i}\\beta_{i}}{\\sigma_{i}^{2} + \\lambda}$.\nThe data coefficient is given as $\\beta_{i}$.\nThe ratio is:\n$$f(\\sigma_{i}, \\lambda) = \\frac{\\text{coefficient of } v_{i}}{\\text{data coefficient } \\beta_{i}} = \\frac{\\frac{\\sigma_{i}\\beta_{i}}{\\sigma_{i}^{2} + \\lambda}}{\\beta_{i}}$$\nAssuming $\\beta_{i} \\neq 0$ (otherwise the problem is trivial), we cancel $\\beta_{i}$ from the numerator and denominator:\n$$f(\\sigma_{i}, \\lambda) = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda}$$\nThis is the final closed-form analytic expression for the specified attenuation factor.", "answer": "$$\\boxed{\\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda}}$$", "id": "3405692"}]}