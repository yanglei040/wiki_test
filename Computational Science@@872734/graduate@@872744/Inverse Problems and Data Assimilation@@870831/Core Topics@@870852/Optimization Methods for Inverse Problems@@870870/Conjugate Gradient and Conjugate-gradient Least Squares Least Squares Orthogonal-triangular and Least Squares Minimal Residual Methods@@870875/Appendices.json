{"hands_on_practices": [{"introduction": "This first practice takes you back to basics. By manually executing the first few iterations of the Conjugate Gradient algorithm on a small, well-defined problem, you will gain an intimate understanding of its inner workings and verify the fundamental orthogonality properties that make it so effective. [@problem_id:3371322] This foundational exercise is crucial for building intuition before moving on to more complex implementations and applications.", "problem": "Consider a linear Gaussian inverse problem in which the Maximum A Posteriori (MAP) estimate is obtained by minimizing the strictly convex quadratic objective $J(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$ with respect to $x \\in \\mathbb{R}^{3}$. The information (Hessian) matrix $A$ is symmetric positive definite, with known eigenvalues $\\{1, 2, 4\\}$, and is given explicitly by\n$$\nA \\;=\\; \\begin{pmatrix}\n\\frac{3}{2} & -\\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & \\frac{3}{2} & 0 \\\\\n0 & 0 & 4\n\\end{pmatrix}.\n$$\nThe data vector is $b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$, and the initial guess is $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nStarting from the core definitions that characterize the Conjugate Gradient (CG) method for symmetric positive definite systems and its interpretation as steepest descent with respect to the $A$-inner product on Krylov subspaces generated by the initial residual, perform exactly three CG iterations to minimize $J(x)$, constructing the search directions from the orthogonalization conditions intrinsic to CG. Compute all quantities exactly (no floating-point rounding), and at each iteration:\n1. Determine the step size by minimizing $J$ along the current search direction.\n2. Update the approximate solution, residual, and next search direction derived from the fundamental orthogonality relations.\n3. Verify the $A$-conjugacy of search directions, that is, $p_{i}^{\\top} A p_{j} = 0$ for $i \\neq j$, and the mutual orthogonality of residuals, that is, $r_{i}^{\\top} r_{j} = 0$ for $i \\neq j$, for the three iterations you carry out.\n\nReport the final approximate solution after three iterations, $x_{3}$, as a single row vector. Express your answer exactly; do not round. Your final answer must be a single mathematical object.", "solution": "The problem requires the application of the Conjugate Gradient (CG) method to minimize the quadratic objective function $J(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$. The minimizer of $J(x)$ is the solution to the linear system $Ax = b$. The CG method is an iterative algorithm for solving such systems where $A$ is symmetric and positive definite.\n\nThe core formulas for the Conjugate Gradient algorithm, starting from an initial guess $x_0$, are as follows:\nInitial residual: $r_0 = b - A x_0$\nInitial search direction: $p_0 = r_0$\n\nFor each iteration $k = 0, 1, 2, \\dots$:\nStep size: $\\alpha_k = \\frac{r_k^{\\top} r_k}{p_k^{\\top} A p_k}$\nSolution update: $x_{k+1} = x_k + \\alpha_k p_k$\nResidual update: $r_{k+1} = r_k - \\alpha_k A p_k$\nSearch direction parameter: $\\beta_k = \\frac{r_{k+1}^{\\top} r_{k+1}}{r_k^{\\top} r_k}$\nSearch direction update: $p_{k+1} = r_{k+1} + \\beta_k p_k$\n\nLet us apply this algorithm for the given $A$, $b$, and $x_0$.\nGiven:\n$$\nA = \\begin{pmatrix}\n\\frac{3}{2} & -\\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & \\frac{3}{2} & 0 \\\\\n0 & 0 & 4\n\\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n\n**Initialization ($k=0$):**\nThe initial residual is:\n$$\nr_0 = b - A x_0 = b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe first search direction is the initial residual:\n$$\np_0 = r_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\n\n**Iteration 1 ($k=0$):**\nWe first compute the product $A p_0$:\n$$\nA p_0 = \\begin{pmatrix}\n\\frac{3}{2} & -\\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & \\frac{3}{2} & 0 \\\\\n0 & 0 & 4\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} - \\frac{1}{2} \\\\ -\\frac{1}{2} + \\frac{3}{2} \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix}\n$$\nThe step size $\\alpha_0$ is calculated by minimizing $J(x_0+\\alpha p_0)$:\n$$\nr_0^{\\top} r_0 = 1^2 + 1^2 + 1^2 = 3\n$$\n$$\np_0^{\\top} A p_0 = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix} = 1 + 1 + 4 = 6\n$$\n$$\n\\alpha_0 = \\frac{r_0^{\\top} r_0}{p_0^{\\top} A p_0} = \\frac{3}{6} = \\frac{1}{2}\n$$\nUpdate the solution:\n$$\nx_1 = x_0 + \\alpha_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix}\n$$\nUpdate the residual:\n$$\nr_1 = r_0 - \\alpha_0 A p_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1 \\end{pmatrix}\n$$\nUpdate the search direction:\n$$\nr_1^{\\top} r_1 = \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 + (-1)^2 = \\frac{1}{4} + \\frac{1}{4} + 1 = \\frac{3}{2}\n$$\n$$\n\\beta_0 = \\frac{r_1^{\\top} r_1}{r_0^{\\top} r_0} = \\frac{3/2}{3} = \\frac{1}{2}\n$$\n$$\np_1 = r_1 + \\beta_0 p_0 = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1/2 \\end{pmatrix}\n$$\n\n**Iteration 2 ($k=1$):**\nCompute the product $A p_1$:\n$$\nA p_1 = \\begin{pmatrix}\n\\frac{3}{2} & -\\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & \\frac{3}{2} & 0 \\\\\n0 & 0 & 4\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1/2 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} - \\frac{1}{2} \\\\ -\\frac{1}{2} + \\frac{3}{2} \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix}\n$$\nCalculate the step size $\\alpha_1$:\n$$\np_1^{\\top} A p_1 = \\begin{pmatrix} 1 & 1 & -1/2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix} = 1 + 1 + 1 = 3\n$$\n$$\n\\alpha_1 = \\frac{r_1^{\\top} r_1}{p_1^{\\top} A p_1} = \\frac{3/2}{3} = \\frac{1}{2}\n$$\nUpdate the solution:\n$$\nx_2 = x_1 + \\alpha_1 p_1 = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1/2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1/4 \\end{pmatrix}\n$$\nUpdate the residual:\n$$\nr_2 = r_1 - \\alpha_1 A p_1 = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nSince $r_2=0$, the algorithm has converged to the exact solution $x_2$. The CG method is guaranteed to find the exact solution in at most $n$ iterations for an $n \\times n$ system; in this case, convergence occurred in $2$ iterations because the initial residual $r_0$ lies in a two-dimensional Krylov subspace (spanned by the eigenvectors corresponding to eigenvalues $1$ and $4$).\n\n**Iteration 3 ($k=2$):**\nThe problem requests \"exactly three CG iterations\". Standard implementations of CG would terminate upon finding a zero residual. To fulfill the request, we proceed formally.\nUpdate the search direction:\n$$\nr_2^{\\top} r_2 = 0^2 + 0^2 + 0^2 = 0\n$$\n$$\n\\beta_1 = \\frac{r_2^{\\top} r_2}{r_1^{\\top} r_1} = \\frac{0}{3/2} = 0\n$$\n$$\np_2 = r_2 + \\beta_1 p_1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + 0 \\cdot \\begin{pmatrix} 1 \\\\ 1 \\\\ -1/2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe next step would be to calculate $\\alpha_2 = \\frac{r_2^{\\top} r_2}{p_2^{\\top} A p_2} = \\frac{0}{0}$, which is indeterminate. However, the update to the solution is $x_3 = x_2 + \\alpha_2 p_2 = x_2 + \\alpha_2 \\cdot 0 = x_2$. Any choice of $\\alpha_2$ would lead to $x_3=x_2$. Therefore, the state of the system does not change.\n$$\nx_3 = x_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1/4 \\end{pmatrix}\n$$\nThe residual would also remain zero: $r_3 = r_2 - \\alpha_2 A p_2 = 0 - \\alpha_2 \\cdot 0 = 0$.\n\n**Verification of Orthogonality Properties:**\nThe problem asks to verify the $A$-conjugacy of search directions and the orthogonality of residuals for the three iterations completed. The vectors generated are:\n$p_0 = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}^{\\top}$, $p_1 = \\begin{pmatrix} 1 & 1 & -1/2 \\end{pmatrix}^{\\top}$, $p_2 = \\begin{pmatrix} 0 & 0 & 0 \\end{pmatrix}^{\\top}$\n$r_0 = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}^{\\top}$, $r_1 = \\begin{pmatrix} 1/2 & 1/2 & -1 \\end{pmatrix}^{\\top}$, $r_2 = \\begin{pmatrix} 0 & 0 & 0 \\end{pmatrix}^{\\top}$\n\n1.  **$A$-conjugacy of search directions ($p_i^{\\top} A p_j = 0$ for $i \\neq j$):**\n    -   $p_0^{\\top} A p_1$:\n        $$p_0^{\\top} (A p_1) = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix} = 1 + 1 - 2 = 0$$\n    -   $p_0^{\\top} A p_2$:\n        $$p_0^{\\top} (A p_2) = p_0^{\\top} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0$$\n    -   $p_1^{\\top} A p_2$:\n        $$p_1^{\\top} (A p_2) = p_1^{\\top} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0$$\n    The search directions are $A$-conjugate.\n\n2.  **Orthogonality of residuals ($r_i^{\\top} r_j = 0$ for $i \\neq j$):**\n    -   $r_0^{\\top} r_1$:\n        $$r_0^{\\top} r_1 = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1 \\end{pmatrix} = \\frac{1}{2} + \\frac{1}{2} - 1 = 0$$\n    -   $r_0^{\\top} r_2$:\n        $$r_0^{\\top} r_2 = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0$$\n    -   $r_1^{\\top} r_2$:\n        $$r_1^{\\top} r_2 = \\begin{pmatrix} 1/2 & 1/2 & -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0$$\n    The residuals are mutually orthogonal.\n\nThe final approximate solution after three iterations is $x_3$. As demonstrated, $x_3 = x_2$.\n$$\nx_3 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1/4 \\end{pmatrix}\n$$\nWe can verify this is the exact solution $x = A^{-1}b$:\n$$\nA^{-1} = \\begin{pmatrix} \\frac{3}{4} & \\frac{1}{4} & 0 \\\\ \\frac{1}{4} & \\frac{3}{4} & 0 \\\\ 0 & 0 & \\frac{1}{4} \\end{pmatrix}\n$$\n$$\nx = A^{-1}b = \\begin{pmatrix} \\frac{3}{4} & \\frac{1}{4} & 0 \\\\ \\frac{1}{4} & \\frac{3}{4} & 0 \\\\ 0 & 0 & \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{4} + \\frac{1}{4} \\\\ \\frac{1}{4} + \\frac{3}{4} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\frac{1}{4} \\end{pmatrix}\n$$\nThe result is correct. The final answer is requested as a single row vector.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 1 & \\frac{1}{4}\n\\end{pmatrix}\n}\n$$", "id": "3371322"}, {"introduction": "While different Krylov subspace methods may seem distinct on the surface, they are often deeply interconnected. This practice challenges you to first derive the formal relationship between the Golub-Kahan bidiagonalization (the engine of LSQR) and the Lanczos process on the normal equations (the engine of CGLS). [@problem_id:3371365] You will then implement both algorithms to numerically demonstrate their theoretical equivalence, solidifying your understanding that they are two different paths to the same solution.", "problem": "Consider a real rectangular matrix $A \\in \\mathbb{R}^{m \\times n}$ and a right-hand side vector $b \\in \\mathbb{R}^{m}$. Let the Conjugate Gradient Least Squares (CGLS) algorithm denote the application of the Conjugate Gradient method to the normal equations $A^{\\top} A x = A^{\\top} b$ with initial guess $x_{0} = 0$, and let the Least Squares QR (LSQR) algorithm denote the method based on Golub–Kahan bidiagonalization and orthogonal transformations to solve the least squares problem $\\min_{x \\in \\mathbb{R}^{n}} \\lVert A x - b \\rVert_{2}$. Let Golub–Kahan bidiagonalization (GKB) started from $u_{1} = b / \\lVert b \\rVert_{2}$ produce orthonormal bases $U_{k+1} \\in \\mathbb{R}^{m \\times (k+1)}$ and $V_{k} \\in \\mathbb{R}^{n \\times k}$, along with an upper bidiagonal matrix $B_{k} \\in \\mathbb{R}^{(k+1) \\times k}$ and positive scalars $\\{\\alpha_{j}\\}$ and $\\{\\beta_{j}\\}$, such that $A V_{k} = U_{k+1} B_{k}$. Let the Lanczos process applied to the symmetric matrix $A^{\\top} A$ with starting vector $q_{1} = A^{\\top} b / \\lVert A^{\\top} b \\rVert_{2}$ produce an orthonormal basis $Q_{k} \\in \\mathbb{R}^{n \\times k}$ and a symmetric tridiagonal matrix $T_{k} \\in \\mathbb{R}^{k \\times k}$ satisfying $A^{\\top} A Q_{k} = Q_{k} T_{k} + \\gamma_{k+1} q_{k+1} e_{k}^{\\top}$ for some scalar $\\gamma_{k+1}$.\n\nTasks:\n- Starting from the basic definitions of Krylov subspaces and the orthogonality properties of Golub–Kahan bidiagonalization and the Lanczos process, derive the relation between the two processes that connects the projected normal matrix $V_{k}^{\\top} A^{\\top} A V_{k}$ and the GKB bidiagonal $B_{k}$. Your derivation must begin from first principles, namely the defining relations of Golub–Kahan bidiagonalization and Lanczos, the orthonormality of the basis vectors, and the definition of the Krylov subspace $\\mathcal{K}_{k}(M, v) = \\mathrm{span}\\{v, M v, \\dots, M^{k-1} v\\}$ for a symmetric matrix $M$. Do not use any unproven shortcut results.\n- Using the same principles, justify why, in exact arithmetic, the $k$-th iterate $x_{k}^{\\mathrm{LSQR}}$ produced by LSQR and the $k$-th iterate $x_{k}^{\\mathrm{CGLS}}$ produced by CGLS are identical for every $k \\in \\{1, 2, \\dots\\}$, provided both are started from $x_{0} = 0$ and driven by the same pair $(A, b)$.\n- Design a computational experiment that demonstrates these theoretical equivalences numerically. The experiment must:\n  - Implement both LSQR and CGLS to record the entire sequence of iterates $\\{x_{k}\\}_{k=1}^{K}$, where $K$ is a prescribed iteration cap. If either algorithm terminates early due to a structural breakdown (for example, a zero denominator in an update caused by exact attainment of the solution), then for reporting purposes, the sequence must be extended to length $K$ by repeating the last available iterate.\n  - Implement Golub–Kahan bidiagonalization to compute $V_{k}$ and $B_{k}$ for a given $k$, and then form and compare the two small matrices $V_{k}^{\\top} A^{\\top} A V_{k}$ and $B_{k}^{\\top} B_{k}$ using the Frobenius norm.\n  - For each test case, report two boolean values:\n    - The first boolean indicates whether the maximum over $j \\in \\{1, \\dots, K\\}$ of the Euclidean norm $\\lVert x_{j}^{\\mathrm{LSQR}} - x_{j}^{\\mathrm{CGLS}} \\rVert_{2}$ is less than or equal to a prescribed tolerance $\\varepsilon_{x}$.\n    - The second boolean indicates whether the Frobenius norm $\\lVert V_{K}^{\\top} A^{\\top} A V_{K} - B_{K}^{\\top} B_{K} \\rVert_{F}$ is less than or equal to a prescribed tolerance $\\varepsilon_{T}$.\n- Use the following test suite of matrices and right-hand sides, with iteration caps, all expressed deterministically:\n  - Test case $1$ (tall, full column rank, moderately ill-conditioned): $m = 8$, $n = 5$, $A \\in \\mathbb{R}^{8 \\times 5}$ is the rectangular diagonal matrix with entries $A_{i,i} = s_{i}$ for $i \\in \\{1, \\dots, 5\\}$ and $A_{i,j} = 0$ otherwise, where $(s_{1}, s_{2}, s_{3}, s_{4}, s_{5}) = (10, 3, 1, 0.1, 0.01)$. The right-hand side is $b \\in \\mathbb{R}^{8}$ with entries $b_{j} = \\sin(j)$ for $j \\in \\{1, \\dots, 8\\}$. Use $K = 5$.\n  - Test case $2$ (tall, rank-deficient): $m = 8$, $n = 5$, $A \\in \\mathbb{R}^{8 \\times 5}$ is the rectangular diagonal matrix with $(s_{1}, s_{2}, s_{3}, s_{4}, s_{5}) = (10, 3, 1, 0, 0)$. The right-hand side is $b \\in \\mathbb{R}^{8}$ with entries $b_{j} = \\cos(j)$ for $j \\in \\{1, \\dots, 8\\}$. Use $K = 3$.\n  - Test case $3$ (square, symmetric positive definite): $m = n = 6$, $A \\in \\mathbb{R}^{6 \\times 6}$ is diagonal with diagonal entries $(5, 4, 3, 2, 1, 0.5)$. The right-hand side is $b \\in \\mathbb{R}^{6}$ with entries $b_{j} = \\sin(j)$ for $j \\in \\{1, \\dots, 6\\}$. Use $K = 6$.\n  - Test case $4$ (zero right-hand side): $A$ as in test case $1$, $b = 0 \\in \\mathbb{R}^{8}$. Use $K = 3$.\n- Use tolerances $\\varepsilon_{x} = 10^{-10}$ and $\\varepsilon_{T} = 10^{-12}$.\n- Final output format requirement: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of test cases $1$ through $4$, where each test case contributes two boolean entries: first the iterate equivalence boolean, then the projected-matrix equivalence boolean. For example, the output must have the form $[b_{1,1}, b_{1,2}, b_{2,1}, b_{2,2}, b_{3,1}, b_{3,2}, b_{4,1}, b_{4,2}]$ where each $b_{i,j}$ is either $\\mathrm{True}$ or $\\mathrm{False}$.", "solution": "We are asked to derive the relationship between $V_{k}^{\\top} A^{\\top} A V_{k}$ and the GKB bidiagonal matrix $B_{k}$ from first principles. The Golub-Kahan bidiagonalization (GKB) process, after $k$ steps, generates orthonormal sets of vectors $\\{u_j\\}_{j=1}^{k+1}$ and $\\{v_j\\}_{j=1}^{k}$, collected as columns of matrices $U_{k+1} \\in \\mathbb{R}^{m \\times (k+1)}$ and $V_{k} \\in \\mathbb{R}^{n \\times k}$, respectively. These matrices are related to a lower bidiagonal matrix $B_k \\in \\mathbb{R}^{(k+1) \\times k}$ by the equation:\n$$\nA V_k = U_{k+1} B_k\n$$\nThe core properties of the matrices $U_{k+1}$ and $V_k$ are their orthonormality:\n$$\nU_{k+1}^{\\top} U_{k+1} = I_{k+1} \\in \\mathbb{R}^{(k+1) \\times (k+1)}\n$$\n$$\nV_{k}^{\\top} V_{k} = I_{k} \\in \\mathbb{R}^{k \\times k}\n$$\nOur goal is to analyze the expression $V_{k}^{\\top} A^{\\top} A V_{k}$. We start with the fundamental GKB relation $A V_k = U_{k+1} B_k$. Taking the transpose of this equation, we get $V_k^{\\top} A^{\\top} = B_k^{\\top} U_{k+1}^{\\top}$. Now, we can substitute this expression for $V_k^{\\top} A^{\\top}$ into the term of interest:\n$$\nV_{k}^{\\top} A^{\\top} A V_{k} = (V_k^{\\top} A^{\\top})(A V_k) = (B_k^{\\top} U_{k+1}^{\\top})(U_{k+1} B_k)\n$$\nBy associativity of matrix multiplication and using the orthonormality property $U_{k+1}^{\\top} U_{k+1} = I_{k+1}$, this simplifies to:\n$$\nV_{k}^{\\top} A^{\\top} A V_{k} = B_k^{\\top} (U_{k+1}^{\\top} U_{k+1}) B_k = B_k^{\\top} I_{k+1} B_k = B_k^{\\top} B_k\n$$\nThis completes the derivation. The resulting matrix $T_k = B_k^{\\top} B_k$ is a $k \\times k$ symmetric tridiagonal matrix, showing that the projection of the normal matrix $A^{\\top}A$ onto the Krylov subspace implicitly generated by GKB is directly available from the bidiagonal matrix $B_k$.\n\nNext, we show that for any step $k$, the iterates produced by LSQR and CGLS are identical, given a zero initial guess $x_0=0$.\nThe CGLS algorithm applies the Conjugate Gradient (CG) method to solve $A^{\\top} A x = A^{\\top} b$. With $x_0 = 0$, the $k$-th iterate, $x_k^{\\mathrm{CGLS}}$, is the unique vector within the Krylov subspace $\\mathcal{K}_k(A^{\\top}A, A^{\\top} b)$ that satisfies the Galerkin condition: the new residual $A^{\\top}b - A^{\\top}A x_k^{\\mathrm{CGLS}}$ is orthogonal to the search subspace.\nThe LSQR method's $k$-th iterate, $x_k^{\\mathrm{LSQR}}$, is constructed within the subspace spanned by the GKB vectors, $\\mathrm{span}(V_k)$. It can be shown by induction on the GKB recurrences that this subspace is identical to the CGLS Krylov subspace, $\\mathcal{K}_k(A^{\\top}A, A^{\\top}b)$. The LSQR iterate is given by $x_k^{\\mathrm{LSQR}} = V_k y_k$, where $y_k$ solves the smaller projected problem $\\min_{y} \\lVert B_k y - \\beta_1 e_1 \\rVert_2$, with $\\beta_1 = \\lVert b \\rVert_2$. The normal equations for this small problem are $B_k^{\\top} B_k y_k = B_k^{\\top} (\\beta_1 e_1)$.\nTo prove equivalence, we show that $x_k^{\\mathrm{LSQR}}$ also satisfies the CGLS Galerkin condition, which is $V_k^{\\top} (A^{\\top}b - A^{\\top}A x_k^{\\mathrm{LSQR}}) = 0$. Substituting $x_k^{\\mathrm{LSQR}} = V_k y_k$ and using our previous result $V_k^{\\top}A^{\\top}AV_k = B_k^{\\top}B_k$, the condition becomes:\n$$\nV_k^{\\top} A^{\\top} b - (B_k^{\\top} B_k) y_k = 0\n$$\nFrom the LSQR normal equations, we can substitute for $(B_k^{\\top} B_k) y_k$, yielding the condition $V_k^{\\top} A^{\\top} b - B_k^{\\top} (\\beta_1 e_1) = 0$.\nWe evaluate both terms. From the GKB recurrences, $V_k^{\\top} A^{\\top} b = V_k^{\\top} A^{\\top} (\\beta_1 u_1) = \\beta_1 V_k^{\\top} (\\alpha_1 v_1) = \\alpha_1 \\beta_1 V_k^{\\top} v_1 = \\alpha_1 \\beta_1 e_1$. For the second term, since the first row of $B_k$ is $[\\alpha_1, 0, \\dots, 0]$, we have $B_k^{\\top} e_1 = \\alpha_1 e_1 \\in \\mathbb{R}^k$. Thus, $B_k^{\\top} (\\beta_1 e_1) = \\alpha_1 \\beta_1 e_1$.\nSince both terms are equal, the CGLS Galerkin condition holds for the LSQR iterate. As both iterates lie in the same uniquely defining subspace and satisfy the same optimality condition, they must be identical: $x_k^{\\mathrm{LSQR}} = x_k^{\\mathrm{CGLS}}$ for all $k$.", "answer": "```python\nimport numpy as np\n\ndef cgls(A, b, K, tol=1e-15):\n    \"\"\"\n    Implements the Conjugate Gradient Least Squares (CGLS) algorithm.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    \n    # Handle the b=0 case\n    if np.linalg.norm(b) == 0:\n        return [np.zeros(n) for _ in range(K)]\n\n    r = A.T @ b - A.T @ (A @ x)\n    p = r.copy()\n    \n    x_hist = []\n    rs_old = r.T @ r\n    \n    if np.sqrt(rs_old) < tol:\n        for _ in range(K):\n            x_hist.append(x.copy())\n        return x_hist\n\n    for k in range(K):\n        Ap = A @ p\n        alpha = rs_old / (Ap.T @ Ap)\n        x = x + alpha * p\n        r = r - alpha * (A.T @ Ap)\n        \n        x_hist.append(x.copy())\n        \n        rs_new = r.T @ r\n        if np.sqrt(rs_new) < tol:\n            # Algorithm converged, pad the history\n            for _ in range(k + 1, K):\n                x_hist.append(x.copy())\n            break\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return x_hist\n\ndef gkb(A, b, K, tol=1e-15):\n    \"\"\"\n    Implements k-step Golub-Kahan Bidiagonalization.\n    \"\"\"\n    m, n = A.shape\n    \n    U_cols = []\n    V_cols = []\n    alphas = []\n    betas = []\n\n    u = b.copy()\n    beta = np.linalg.norm(u)\n    betas.append(beta)\n\n    if beta < tol:\n        # Breakdown, b=0\n        B = np.zeros((K + 1, K))\n        V = np.empty((n, 0))\n        return V, B\n\n    u = u / beta\n    U_cols.append(u)\n    \n    for k in range(K):\n        v = A.T @ U_cols[k]\n        if k > 0:\n            v = v - betas[k] * V_cols[k-1]\n        \n        alpha = np.linalg.norm(v)\n        alphas.append(alpha)\n\n        if alpha < tol:\n            # Breakdown\n            v = np.zeros_like(v)\n            V_cols.append(v)\n            u = np.zeros_like(u)\n            U_cols.append(u)\n            betas.append(0.0)\n            break\n        \n        v = v / alpha\n        V_cols.append(v)\n        \n        u = A @ v - alphas[k] * U_cols[k]\n        beta = np.linalg.norm(u)\n        betas.append(beta)\n\n        if beta < tol:\n            # Breakdown\n            u = np.zeros_like(u)\n            U_cols.append(u)\n            break\n            \n        u = u / beta\n        U_cols.append(u)\n\n    # Form V and B matrices\n    num_steps = len(alphas)\n    V = np.array(V_cols[:num_steps]).T\n    B = np.zeros((num_steps + 1, num_steps))\n    \n    for i in range(num_steps):\n        B[i, i] = alphas[i]\n        B[i + 1, i] = betas[i + 1]\n        \n    # If K > num_steps, the matrices are smaller\n    if K > V.shape[1]:\n        V_padded = np.zeros((n, K))\n        V_padded[:, :V.shape[1]] = V\n        V = V_padded\n        B_padded = np.zeros((K + 1, K))\n        B_padded[:B.shape[0], :B.shape[1]] = B\n        B = B_padded\n\n    return V, B\n\ndef lsqr(A, b, K, tol=1e-15):\n    \"\"\"\n    Implements LSQR by repeatedly solving the projected problem.\n    \"\"\"\n    m, n = A.shape\n    x_hist = []\n    \n    u = b.copy()\n    beta1 = np.linalg.norm(u)\n\n    if beta1 < tol:\n        return [np.zeros(n) for _ in range(K)]\n\n    U_cols = []\n    V_cols = []\n    alphas = []\n    betas = [beta1]\n    \n    u = u / beta1\n    U_cols.append(u)\n\n    last_x = np.zeros(n)\n\n    for k in range(1, K + 1):\n        # Perform one step of GKB\n        # v_k\n        v_prev = V_cols[-1] if k > 1 else np.zeros(n)\n        v = A.T @ U_cols[k-1] - betas[k-1] * v_prev\n        alpha_k = np.linalg.norm(v)\n        if alpha_k < tol:\n            x_hist.append(last_x.copy())\n            continue\n\n        v = v / alpha_k\n        V_cols.append(v)\n        alphas.append(alpha_k)\n        \n        # u_{k+1}\n        u = A @ v - alpha_k * U_cols[k-1]\n        beta_kp1 = np.linalg.norm(u)\n        if beta_kp1 < tol:\n            u = np.zeros(m)\n        else:\n            u = u / beta_kp1\n        \n        U_cols.append(u)\n        betas.append(beta_kp1)\n        \n        # At step k, form and solve the small problem\n        V_k = np.array(V_cols).T\n        B_k = np.zeros((k + 1, k))\n        for i in range(k):\n            B_k[i, i] = alphas[i]\n            B_k[i + 1, i] = betas[i + 1]\n\n        b_small = np.zeros(k + 1)\n        b_small[0] = beta1\n        \n        y_k, _, _, _ = np.linalg.lstsq(B_k, b_small, rcond=None)\n        \n        x_k = V_k @ y_k\n        x_hist.append(x_k)\n        last_x = x_k\n\n    # Pad if breakdown occurred\n    while len(x_hist) < K:\n        x_hist.append(last_x.copy())\n\n    return x_hist\n\n\ndef solve():\n    \"\"\"\n    Main function to run the computational experiment.\n    \"\"\"\n    \n    # Tolerances\n    eps_x = 1e-10\n    eps_T = 1e-12\n\n    # Test Cases\n    test_cases = []\n    \n    # Case 1\n    m1, n1, K1 = 8, 5, 5\n    s1 = np.array([10.0, 3.0, 1.0, 0.1, 0.01])\n    A1 = np.zeros((m1, n1))\n    np.fill_diagonal(A1, s1)\n    b1 = np.sin(np.arange(1, m1 + 1))\n    test_cases.append({'A': A1, 'b': b1, 'K': K1})\n\n    # Case 2\n    m2, n2, K2 = 8, 5, 3\n    s2 = np.array([10.0, 3.0, 1.0, 0.0, 0.0])\n    A2 = np.zeros((m2, n2))\n    np.fill_diagonal(A2, s2)\n    b2 = np.cos(np.arange(1, m2 + 1))\n    test_cases.append({'A': A2, 'b': b2, 'K': K2})\n\n    # Case 3\n    m3, n3, K3 = 6, 6, 6\n    s3 = np.array([5.0, 4.0, 3.0, 2.0, 1.0, 0.5])\n    A3 = np.diag(s3)\n    b3 = np.sin(np.arange(1, m3 + 1))\n    test_cases.append({'A': A3, 'b': b3, 'K': K3})\n\n    # Case 4\n    K4 = 3\n    b4 = np.zeros(m1)\n    test_cases.append({'A': A1, 'b': b4, 'K': K4})\n\n    results = []\n    for case in test_cases:\n        A, b, K = case['A'], case['b'], case['K']\n        \n        # Run CGLS and LSQR\n        x_cgls_hist = cgls(A, b, K)\n        x_lsqr_hist = lsqr(A, b, K)\n        \n        # Check iterate equivalence\n        diff_norms = [np.linalg.norm(xc - xl) for xc, xl in zip(x_cgls_hist, x_lsqr_hist)]\n        max_diff_norm = np.max(diff_norms) if diff_norms else 0.0\n        bool1 = max_diff_norm <= eps_x\n        results.append(bool1)\n        \n        # Run GKB for K steps\n        V_K, B_K = gkb(A, b, K)\n        \n        # Check matrix equivalence\n        if V_K.shape[1] == 0: # b=0 case\n            frob_norm = 0.0\n        else:\n            M1 = V_K.T @ A.T @ A @ V_K\n            M2 = B_K.T @ B_K\n            frob_norm = np.linalg.norm(M1 - M2, 'fro')\n\n        bool2 = frob_norm <= eps_T\n        results.append(bool2)\n\n    # Final print statement\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3371365"}, {"introduction": "Perhaps the most powerful feature of iterative methods in inverse problems is their inherent ability to manage noise. This practice guides you through a computational experiment to demonstrate the concept of \"iterative regularization,\" where stopping an algorithm early prevents the disastrous amplification of noise common in ill-posed problems. [@problem_id:3371338] By comparing the results of a direct QR-based solution with a carefully stopped LSQR implementation, you will witness firsthand how \"less is more\" when seeking a stable solution from noisy data.", "problem": "Consider solving a linear inverse problem of the form $A x \\approx b$ with $A \\in \\mathbb{R}^{m \\times n}$, where the singular values of $A$ decay slowly and the data $b$ are contaminated with strong additive noise. The goal is to examine how the orthogonal-triangular factorization (QR) approach to least squares behaves without explicit regularization, and to contrast it with the Least Squares with QR-like iterations (LSQR) method that exhibits iterative regularization when stopped by a principled rule. The context is inverse problems and data assimilation, where ill-posedness and noise amplification are central concerns.\n\nUse the following fundamental base:\n- The singular value decomposition (SVD): For a matrix $A$, write $A = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthogonal and $\\Sigma$ is diagonal with nonnegative entries $\\sigma_i$.\n- Least squares orthogonal-triangular: Solve $\\min_{x} \\lVert A x - b \\rVert_2$ by computing an orthogonal-triangular factorization $A = Q R$ with $Q$ orthogonal and $R$ upper triangular, then solve $R x = Q^{\\top} b$.\n- Iterative regularization: In algorithms such as Conjugate-Gradient Least Squares (CGLS) and Least Squares with QR-like iterations (LSQR), early termination can act as a regularization mechanism that suppresses the influence of small singular values, which otherwise amplify noise.\n\nYou must implement a self-contained program that constructs synthetic test problems where $A$ has slowly decaying singular values and $b$ contains strong noise, then demonstrates that:\n- Unregularized QR least squares overfits the noise and yields a higher reconstruction error compared to LSQR stopped by a discrepancy principle.\n- Fully iterated LSQR (run to the maximum practical number of iterations) also overfits compared to suitably early stopping.\n\nYou must proceed purely in mathematical terms:\n1. For each test case, let $m = n$ and define $A$ to be diagonal with entries $\\sigma_i = (i+1)^{-p}$ for $i = 0, 1, \\dots, n-1$, where $p$ is a positive exponent controlling the slow decay of singular values. This choice makes $A$ a compact operator with slowly decaying singular values, a canonical mild ill-posedness scenario.\n2. Define a ground-truth vector $x_{\\text{true}} \\in \\mathbb{R}^n$ by $x_{\\text{true},i} = (-1)^i (i+1)^{-1}$, which is a smooth signal with decaying coefficients.\n3. Form the clean right-hand side $b_{\\text{clean}} = A x_{\\text{true}}$.\n4. Add strong noise: let $\\eta$ be a random vector with independent standard normal entries, and scale it so that $\\lVert \\eta \\rVert_2 = \\delta \\lVert b_{\\text{clean}} \\rVert_2$ for a prescribed relative noise level $\\delta > 0$. Set $b = b_{\\text{clean}} + \\eta$. All norms are the Euclidean norm.\n5. Compute the unregularized QR least squares solution $x_{\\text{QR}}$ to $\\min_x \\lVert A x - b \\rVert_2$ by orthogonal-triangular factorization and back-substitution.\n6. Implement LSQR iteration (Paige and Saunders) starting from $x_0 = 0$. Use the Morozov discrepancy principle with parameter $\\tau > 1$ to choose the stopping index $k_{\\text{stop}}$ as the smallest $k$ for which $\\lVert A x_k - b \\rVert_2 \\le \\tau \\lVert \\eta \\rVert_2$, where $x_k$ is the $k$-th LSQR iterate. Also compute a “fully iterated” LSQR solution $x_{\\text{LSQR-full}}$ by running LSQR for a fixed maximum number of iterations (equal to $n$).\n7. Quantify reconstruction errors as relative errors $E(\\hat{x}) = \\lVert \\hat{x} - x_{\\text{true}} \\rVert_2 / \\lVert x_{\\text{true}} \\rVert_2$ for $\\hat{x} \\in \\{ x_{\\text{QR}}, x_{k_{\\text{stop}}}, x_{\\text{LSQR-full}} \\}$.\n8. Your program must return, for each test case, two boolean values:\n   - Whether $E(x_{k_{\\text{stop}}}) < E(x_{\\text{QR}})$.\n   - Whether $E(x_{k_{\\text{stop}}}) < E(x_{\\text{LSQR-full}})$.\nThese booleans test the claim that unregularized QR overfits unless regularized and that LSQR’s iterative regularization succeeds with suitable stopping.\n\nNo physical units or angles are involved; all quantities are dimensionless.\n\nTest Suite:\nProvide three test cases $(n, p, \\delta, \\tau)$ that probe different regimes:\n- Case $1$: $n = 80$, $p = 0.3$, $\\delta = 0.2$, $\\tau = 1.1$ (strong noise, slow decay).\n- Case $2$: $n = 80$, $p = 0.5$, $\\delta = 0.1$, $\\tau = 1.05$ (moderate noise, slower decay).\n- Case $3$: $n = 50$, $p = 0.2$, $\\delta = 0.3$, $\\tau = 1.1$ (very strong noise, very slow decay).\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain six boolean values in the order of the test cases, where for each case you output first the comparison with QR and then the comparison with fully iterated LSQR. For example, the output format is $[\\text{b}_1,\\text{b}_2,\\text{b}_3,\\text{b}_4,\\text{b}_5,\\text{b}_6]$, where each $\\text{b}_i$ is either True or False.", "solution": "The problem requires an analysis and comparison of two methods for solving a linear system $A x \\approx b$ characteristic of discrete ill-posed inverse problems. The matrix $A \\in \\mathbb{R}^{n \\times n}$ is ill-conditioned, specifically having singular values that decay slowly to zero, and the data vector $b \\in \\mathbb{R}^{n}$ is contaminated with significant additive noise. We will contrast the direct, unregularized least squares solution via orthogonal-triangular (QR) factorization with an iterative method, LSQR, which exhibits regularization properties when stopped early.\n\nThe core of the problem lies in the amplification of noise by the \"inverse\" of an ill-conditioned operator. A linear system is ill-posed if small perturbations in the data $b$ can lead to large changes in the solution $x$. For a matrix $A$ with singular value decomposition (SVD) $A = U \\Sigma V^{\\top}$, the solution to the least squares problem $\\min_{x} \\lVert A x - b \\rVert_2$ is formally $x = A^{\\dagger} b = V \\Sigma^{\\dagger} U^{\\top} b$, where $\\dagger$ denotes the pseudoinverse. If the singular values $\\sigma_i$ of $A$ decay to zero without a significant gap, the corresponding singular values of $A^{\\dagger}$, which are $1/\\sigma_i$, become very large. When the data is noisy, $b = b_{\\text{true}} + \\eta$, the solution becomes $x = A^{\\dagger}b_{\\text{true}} + A^{\\dagger}\\eta = x_{\\text{true}} + A^{\\dagger}\\eta$. The term $A^{\\dagger}\\eta$ represents the propagated noise. Its components associated with small $\\sigma_i$ are massively amplified, rendering the solution meaningless.\n\nThe problem is constructed to exhibit this exact behavior. The matrix $A$ is defined to be diagonal, so its diagonal entries are its singular values, $\\sigma_i = (i+1)^{-p}$ for $i = 0, 1, \\dots, n-1$. With $p > 0$, these values decay slowly. The ground truth signal $x_{\\text{true}}$ is smooth. Noise $\\eta$ is added to the clean data $b_{\\text{clean}} = A x_{\\text{true}}$ to form $b = b_{\\text{clean}} + \\eta$.\n\n**1. Unregularized QR Least Squares Solution ($x_{\\text{QR}}$)**\n\nThe first method is the direct solution of the least squares problem. For a square, invertible matrix $A$, this is equivalent to solving $A x = b$. The problem specifies using a QR factorization, $A=QR$, where $Q$ is orthogonal and $R$ is upper triangular. The least squares solution is found by solving the triangular system $R x = Q^{\\top} b$. For an ill-conditioned matrix $A$, this procedure is numerically equivalent to direct inversion and does not mitigate noise amplification. The computed solution, $x_{\\text{QR}}$, will be close to the true least squares solution $A^{-1}b$:\n$$x_{\\text{QR}} \\approx A^{-1}(A x_{\\text{true}} + \\eta) = x_{\\text{true}} + A^{-1} \\eta$$\nThe error term, $A^{-1}\\eta$, contains components $(A^{-1}\\eta)_i = \\sigma_i^{-1} \\eta_i = (i+1)^p \\eta_i$. Since $p>0$, this term explodes for larger $i$, and the solution $x_{\\text{QR}}$ becomes dominated by amplified noise, leading to a large reconstruction error $E(x_{\\text{QR}})$. This is a phenomenon known as \"overfitting\" the noise.\n\n**2. LSQR with Iterative Regularization ($x_{k_{\\text{stop}}}$)**\n\nLSQR is an iterative algorithm for solving least squares problems, mathematically equivalent to the conjugate gradient method on the normal equations $A^{\\top}A x = A^{\\top}b$ but with superior numerical stability. A key property of such iterative methods is that the solution estimate at iteration $k$, denoted $x_k$, primarily captures information related to the largest singular values of $A$. Components of the solution corresponding to small singular values, which are most susceptible to noise amplification, are only incorporated in later iterations.\n\nThis behavior allows for \"iterative regularization\": by stopping the algorithm early, we effectively filter out a significant portion of the amplified noise. The solution $x_k$ is constrained to the Krylov subspace $\\mathcal{K}_k(A^\\top A, A^\\top b)$, which acts as a low-dimensional projection that regularizes the problem. The dimension of this subspace, $k$, is the regularization parameter.\n\n**3. The Morozov Discrepancy Principle**\n\nTo make iterative regularization effective, a principled stopping rule is needed. The Morozov discrepancy principle provides such a rule. It is based on the idea that one should not seek a solution that fits the data more closely than the level of noise in the data. Given a noise level estimate, in this case the known norm of the noise $\\lVert \\eta \\rVert_2$, we stop at the first iteration $k$ for which the residual norm falls below a threshold related to this noise level:\n$$\\lVert A x_k - b \\rVert_2 \\le \\tau \\lVert \\eta \\rVert_2$$\nHere, $\\tau > 1$ is a safety factor. Stopping at this index $k_{\\text{stop}}$ yields a regularized solution $x_{k_{\\text{stop}}}$ that balances fidelity to the data with suppression of noise.\n\n**4. Fully Iterated LSQR ($x_{\\text{LSQR-full}}$)**\n\nIf LSQR is run for a sufficient number of iterations (e.g., $n$ for an $n \\times n$ non-singular matrix), it will converge to the unregularized least squares solution. Therefore, the \"fully iterated\" solution $x_{\\text{LSQR-full}}$ will be approximately the same as $x_{\\text{QR}}$ and will also suffer from severe noise amplification.\n\n**Synthesis**\n\nThe numerical experiment is designed to demonstrate that:\n-   $E(x_{k_{\\text{stop}}})  E(x_{\\text{QR}}):$ The regularized LSQR solution is superior to the unregularized direct solution because early stopping prevents noise amplification.\n-   $E(x_{k_{\\text{stop}}})  E(x_{\\text{LSQR-full}}):$ The same early stopping provides a better solution than letting the LSQR algorithm run to convergence, which effectively removes the regularization effect.\n\nThe implementation will construct the specified matrices and vectors, run the QR and LSQR solvers, apply the discrepancy principle, and compute the reconstruction errors to verify these inequalities for the given test cases. We expect both boolean comparisons to be `True` for all test cases, confirming the effectiveness of iterative regularization for this class of problems.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy.linalg\n\ndef lsqr_custom(A, b, max_iter, tau, norm_eta):\n    \"\"\"\n    Implements the LSQR algorithm by Paige and Saunders (1982).\n\n    This implementation finds the solution x_k_stop regularized by the\n    discrepancy principle and the fully iterated solution x_full.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    \n    x_stop = None\n    k_stop = -1\n    stop_threshold = tau * norm_eta\n\n    # Initialize\n    beta = np.linalg.norm(b)\n    u = b / beta if beta > 0 else np.zeros(m)\n    \n    Atu = A.T @ u\n    alpha = np.linalg.norm(Atu)\n    v = Atu / alpha if alpha > 0 else np.zeros(n)\n\n    w = v.copy()\n    phi_bar = beta\n    rho_bar = alpha\n\n    # Main iteration loop\n    for k in range(max_iter):\n        # Bidiagonalization\n        Av = A @ v\n        u_hat = Av - alpha * u\n        beta = np.linalg.norm(u_hat)\n        \n        if beta > 1e-15:\n            u = u_hat / beta\n            Atu = A.T @ u\n            v_hat = Atu - beta * v\n            alpha = np.linalg.norm(v_hat)\n            if alpha > 1e-15:\n                v = v_hat / alpha\n            else:\n                v = np.zeros_like(v_hat) # Should not happen in this problem\n        else:\n            beta = 0.0\n            alpha = 0.0 # Should not happen\n\n        # Plane rotation\n        rho = np.sqrt(rho_bar**2 + beta**2)\n        c = rho_bar / rho\n        s = beta / rho\n        \n        theta = s * alpha\n        rho_bar = -c * alpha\n        phi = c * phi_bar\n        phi_bar = s * phi_bar\n        \n        # Update solution and search direction\n        x = x + (phi / rho) * w\n        w = v - (theta / rho) * w\n        \n        # Check discrepancy principle\n        residual_norm = np.abs(phi_bar)\n        if residual_norm = stop_threshold and k_stop == -1:\n            x_stop = x.copy()\n            k_stop = k\n\n    # If the stopping condition was never met, something is wrong,\n    # but for robustness we assign the full solution.\n    if x_stop is None:\n        x_stop = x.copy()\n\n    return x_stop, x\n\ndef run_experiment(n, p, delta, tau):\n    \"\"\"\n    Runs one full test case for the given parameters.\n    \"\"\"\n    # 1. Construct the problem (A, xtrue, b)\n    i_vals = np.arange(n)\n    \n    # Ground truth vector xtrue\n    xtrue = ((-1.0)**i_vals) * ((i_vals + 1.0)**(-1.0))\n    \n    # Diagonal matrix A with slowly decaying singular values\n    singular_values = (i_vals + 1.0)**(-p)\n    A = np.diag(singular_values)\n    \n    # Clean data b_clean\n    b_clean = A @ xtrue\n    \n    # Add strong noise\n    eta_raw = np.random.randn(n)\n    norm_b_clean = np.linalg.norm(b_clean)\n    norm_eta_raw = np.linalg.norm(eta_raw)\n    \n    # Scale noise vector to have the prescribed relative norm\n    eta = eta_raw * (delta * norm_b_clean / norm_eta_raw)\n    norm_eta = np.linalg.norm(eta)\n    \n    # Noisy data b\n    b = b_clean + eta\n\n    # 2. Compute unregularized QR least squares solution\n    Q, R = np.linalg.qr(A)\n    # Solve Rx = Q^T b\n    x_QR = scipy.linalg.solve_triangular(R, Q.T @ b, lower=False)\n    \n    # 3. Compute LSQR solutions\n    max_iterations = n\n    x_lsqr_stop, x_lsqr_full = lsqr_custom(A, b, max_iterations, tau, norm_eta)\n    \n    # 4. Quantify reconstruction errors\n    norm_xtrue = np.linalg.norm(xtrue)\n    \n    err_qr = np.linalg.norm(x_QR - xtrue) / norm_xtrue\n    err_lsqr_stop = np.linalg.norm(x_lsqr_stop - xtrue) / norm_xtrue\n    err_lsqr_full = np.linalg.norm(x_lsqr_full - xtrue) / norm_xtrue\n    \n    # 5. Return boolean comparisons\n    bool1 = err_lsqr_stop  err_qr\n    bool2 = err_lsqr_stop  err_lsqr_full\n    \n    return bool1, bool2\n\ndef solve():\n    # Set a fixed seed for reproducibility of random noise\n    np.random.seed(0)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, p, delta, tau)\n        (80, 0.3, 0.2, 1.1),\n        (80, 0.5, 0.1, 1.05),\n        (50, 0.2, 0.3, 1.1),\n    ]\n\n    results = []\n    for case in test_cases:\n        b1, b2 = run_experiment(*case)\n        results.extend([b1, b2])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3371338"}]}