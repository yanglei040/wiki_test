{"hands_on_practices": [{"introduction": "In many scientific applications, we encounter systems where there are fewer observations than unknown parameters, leading to an infinite number of possible solutions. This exercise explores how to find a unique, meaningful solution to such an underdetermined system by seeking the one with the minimum Euclidean norm. You will use the orthogonal-triangular factorization of the matrix transpose, $H^\\top$, to derive a stable and efficient computational procedure for finding this minimum-norm solution [@problem_id:3408919].", "problem": "In a linear inverse problem motivated by data assimilation, consider a linear observation operator $H \\in \\mathbb{R}^{m \\times n}$ with $m < n$ and full row rank, mapping a state increment $x \\in \\mathbb{R}^{n}$ to observations $y \\in \\mathbb{R}^{m}$ via $H x = y$. The minimum-Euclidean-norm analysis increment is defined as the unique vector $x^{\\star}$ that minimizes $\\|x\\|_{2}$ subject to the constraint $H x = y$. One class of computational strategies for such underdetermined systems uses orthogonal-triangular factorizations tailored to $m < n$, such as the $RQ$ factorization (upper-triangular times orthogonal) or the $LQ$ factorization (lower-triangular times orthogonal), or equivalently a $QR$ factorization of $H^{\\top}$ (orthogonal times upper-triangular).\n\nStarting only from the defining optimality statement “minimize $\\|x\\|_{2}$ subject to $H x = y$,” Euclidean norm invariance under orthogonal transformations, and the structural property that triangular systems can be solved by back/forward substitution, derive a computational procedure that solves for $x^{\\star}$ using an orthogonal-triangular approach appropriate to $m < n$. Then, apply your derivation to the specific instance with $m = 2$, $n = 3$,\n$$\nH \\;=\\; \\begin{pmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix}, \n\\qquad\ny \\;=\\; \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.\n$$\nCompute the third component of the minimum-Euclidean-norm solution $x^{\\star}$, denoted $x^{\\star}_{3}$, and report its exact value. Do not round your answer.", "solution": "The problem is to find the minimum-Euclidean-norm solution $x^{\\star}$ to an underdetermined system of linear equations. This is a constrained optimization problem formulated as:\n$$\n\\text{minimize } \\|x\\|_{2} \\quad \\text{subject to} \\quad Hx = y\n$$\nwhere $x \\in \\mathbb{R}^{n}$, $y \\in \\mathbb{R}^{m}$, and $H \\in \\mathbb{R}^{m \\times n}$ with $m < n$ and full row rank (rank $m$).\n\nWe will first derive a general computational procedure using an orthogonal-triangular factorization, as requested, and then apply it to the specific numerical instance.\n\n**General Derivation**\n\nThe derivation will be based on three principles given in the problem: the optimality statement, the invariance of the Euclidean norm under orthogonal transformations, and the solvability of triangular systems.\n\nThe matrix $H$ is an $m \\times n$ matrix with $m < n$ and full row rank. Its transpose, $H^{\\top}$, is an $n \\times m$ matrix with full column rank. A matrix with full column rank has a unique (thin) QR factorization. Let us compute this factorization for $H^{\\top}$:\n$$\nH^{\\top} = QR\n$$\nwhere $Q \\in \\mathbb{R}^{n \\times m}$ is a matrix with orthonormal columns (i.e., $Q^{\\top}Q = I_m$, where $I_m$ is the $m \\times m$ identity matrix), and $R \\in \\mathbb{R}^{m \\times m}$ is an upper triangular matrix. Since $H^{\\top}$ has full column rank, $R$ is invertible.\n\nFrom this factorization, we can express $H$ as $H = (QR)^{\\top} = R^{\\top}Q^{\\top}$. The constraint $Hx=y$ becomes:\n$$\nR^{\\top}Q^{\\top}x = y\n$$\nTo utilize the norm invariance property, we perform a change of variables. The columns of $Q$ form an orthonormal basis for the column space of $H^{\\top}$ (which is the row space of $H$). We can extend this set of columns to form a full orthonormal basis for $\\mathbb{R}^n$ by finding a matrix $Q_{\\perp} \\in \\mathbb{R}^{n \\times (n-m)}$ whose columns form an orthonormal basis for the null space of $H$. Let $\\bar{Q} = \\begin{pmatrix} Q & Q_{\\perp} \\end{pmatrix} \\in \\mathbb{R}^{n \\times n}$. By construction, $\\bar{Q}$ is an orthogonal matrix, so $\\bar{Q}^{\\top}\\bar{Q} = \\bar{Q}\\bar{Q}^{\\top} = I_n$.\n\nAny vector $x \\in \\mathbb{R}^n$ can be expressed in this new basis as $x = \\bar{Q}w$ for a unique vector of coordinates $w \\in \\mathbb{R}^n$. Let's partition $w$ as $w = \\begin{pmatrix} w_{1} \\\\ w_{2} \\end{pmatrix}$, where $w_1 \\in \\mathbb{R}^m$ and $w_2 \\in \\mathbb{R}^{n-m}$. So, $x = Qw_1 + Q_{\\perp}w_2$.\n\nThe objective function, $\\|x\\|_2$, can be transformed using the property that orthogonal transformations preserve the Euclidean norm:\n$$\n\\|x\\|_{2} = \\|\\bar{Q}w\\|_{2} = \\|w\\|_{2}\n$$\nThe optimization problem is now transformed into an equivalent problem in terms of $w$:\n$$\n\\text{minimize } \\|w\\|_{2} \\quad \\text{subject to} \\quad H\\bar{Q}w = y\n$$\nLet's analyze the new constraint matrix $H\\bar{Q}$:\n$$\nH\\bar{Q} = R^{\\top}Q^{\\top} \\begin{pmatrix} Q & Q_{\\perp} \\end{pmatrix} = \\begin{pmatrix} R^{\\top}Q^{\\top}Q & R^{\\top}Q^{\\top}Q_{\\perp} \\end{pmatrix}\n$$\nSince $Q^{\\top}Q = I_m$ and the columns of $Q_{\\perp}$ are orthogonal to the columns of $Q$, we have $Q^{\\top}Q_{\\perp} = 0$. The expression simplifies to:\n$$\nH\\bar{Q} = \\begin{pmatrix} R^{\\top}I_m & 0 \\end{pmatrix} = \\begin{pmatrix} R^{\\top} & 0 \\end{pmatrix}\n$$\nThe constraint on $w$ is then:\n$$\n\\begin{pmatrix} R^{\\top} & 0 \\end{pmatrix} \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix} = y \\quad \\implies \\quad R^{\\top}w_1 = y\n$$\nThe objective is to minimize $\\|w\\|_{2}^{2} = \\|w_1\\|_{2}^{2} + \\|w_2\\|_{2}^{2}$. The component $w_1$ is completely determined by the constraint equation $R^{\\top}w_1 = y$. Since $R$ is invertible, so is $R^{\\top}$. Thus, $w_1 = (R^{\\top})^{-1}y$ is uniquely fixed. To minimize the total norm, we must minimize the contribution from $w_2$, which is $\\|w_2\\|_2^2$. The minimum is achieved when $w_2=0$.\n\nThe optimal coordinate vector is $w^{\\star} = \\begin{pmatrix} (R^{\\top})^{-1}y \\\\ 0 \\end{pmatrix}$.\nFinally, we transform back to find the optimal solution $x^{\\star}$:\n$$\nx^{\\star} = \\bar{Q}w^{\\star} = \\begin{pmatrix} Q & Q_{\\perp} \\end{pmatrix} \\begin{pmatrix} w_1 \\\\ 0 \\end{pmatrix} = Qw_1 = Q(R^{\\top})^{-1}y\n$$\nThis leads to the following computational procedure:\n1.  Form the transpose matrix $H^{\\top}$.\n2.  Compute the thin QR factorization $H^{\\top} = QR$.\n3.  Solve the lower-triangular system $R^{\\top}z = y$ for the vector $z \\in \\mathbb{R}^m$ via forward substitution. Here, we use $z$ instead of $w_1$ for notational clarity.\n4.  Compute the minimum-norm solution as $x^{\\star} = Qz$.\n\n**Application to the Specific Instance**\n\nWe are given:\n$$\nH = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}, \\qquad y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\n\n1.  **Form $H^{\\top}$**:\n    $$\n    H^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\n    $$\n\n2.  **Compute the QR factorization of $H^{\\top}$**: We apply the Gram-Schmidt process to the columns of $H^\\top$, denoted $a_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $a_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n    The first column of $R$ is $R_{11} = \\|a_1\\|_2 = \\sqrt{1^2+1^2+0^2} = \\sqrt{2}$.\n    The first column of $Q$ is $q_1 = \\frac{a_1}{R_{11}} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\n    Next, $R_{12} = q_1^{\\top}a_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}$.\n    The orthogonal component is $u_2 = a_2 - R_{12}q_1 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{\\sqrt{2}} \\left( \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right) = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1/2 \\\\ 1/2 \\\\ 1 \\end{pmatrix}$.\n    Then, $R_{22} = \\|u_2\\|_2 = \\sqrt{(-1/2)^2 + (1/2)^2 + 1^2} = \\sqrt{\\frac{1}{4} + \\frac{1}{4} + 1} = \\sqrt{\\frac{3}{2}}$.\n    The second column of $Q$ is $q_2 = \\frac{u_2}{R_{22}} = \\frac{1}{\\sqrt{3/2}} \\begin{pmatrix} -1/2 \\\\ 1/2 \\\\ 1 \\end{pmatrix} = \\sqrt{\\frac{2}{3}} \\begin{pmatrix} -1/2 \\\\ 1/2 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{6}}\\begin{pmatrix} -1 \\\\ 1 \\\\ 2 \\end{pmatrix}$.\n    Thus, the factorization is:\n    $$\n    Q = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{-1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ 0 & \\frac{2}{\\sqrt{6}} \\end{pmatrix}, \\qquad R = \\begin{pmatrix} \\sqrt{2} & \\frac{1}{\\sqrt{2}} \\\\ 0 & \\sqrt{\\frac{3}{2}} \\end{pmatrix}\n    $$\n\n3.  **Solve $R^{\\top}z = y$**:\n    The system is $\\begin{pmatrix} \\sqrt{2} & 0 \\\\ \\frac{1}{\\sqrt{2}} & \\sqrt{\\frac{3}{2}} \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n    Using forward substitution:\n    From the first row: $\\sqrt{2} z_1 = 1 \\implies z_1 = \\frac{1}{\\sqrt{2}}$.\n    From the second row: $\\frac{1}{\\sqrt{2}} z_1 + \\sqrt{\\frac{3}{2}} z_2 = 2$.\n    Substituting $z_1$: $\\frac{1}{\\sqrt{2}} \\left(\\frac{1}{\\sqrt{2}}\\right) + \\sqrt{\\frac{3}{2}} z_2 = 2 \\implies \\frac{1}{2} + \\sqrt{\\frac{3}{2}} z_2 = 2$.\n    $\\sqrt{\\frac{3}{2}} z_2 = \\frac{3}{2} \\implies z_2 = \\frac{3/2}{\\sqrt{3/2}} = \\sqrt{\\frac{3}{2}}$.\n    So, $z = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\sqrt{\\frac{3}{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{\\sqrt{6}}{2} \\end{pmatrix}$.\n\n4.  **Compute $x^{\\star} = Qz$**:\n    We only need the third component, $x^{\\star}_{3}$.\n    $$\n    x^{\\star}_{3} = (\\text{third row of } Q) \\cdot z = \\begin{pmatrix} 0 & \\frac{2}{\\sqrt{6}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{\\sqrt{6}}{2} \\end{pmatrix}\n    $$\n    $$\n    x^{\\star}_{3} = 0 \\cdot \\frac{1}{\\sqrt{2}} + \\frac{2}{\\sqrt{6}} \\cdot \\frac{\\sqrt{6}}{2} = 1\n    $$\nThe third component of the minimum-Euclidean-norm solution is $1$. For completeness, the full solution is $x^{\\star} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$.", "answer": "$$\\boxed{1}$$", "id": "3408919"}, {"introduction": "Real-world inverse problems often require us to find a solution that not only fits observed data in a least-squares sense but also perfectly satisfies a set of linear equality constraints, such as known physical laws or boundary conditions. This practice introduces the null-space method, a powerful technique that elegantly handles such constraints by decomposing the problem. You will use an orthogonal-triangular factorization to construct a basis for the null space of the constraint matrix, transforming the constrained problem into a simpler, unconstrained one [@problem_id:3408887].", "problem": "Consider the equality-constrained linear least squares problem in the context of inverse problems and data assimilation: minimize the Euclidean norm of residuals subject to linear equality constraints. Specifically, let $A \\in \\mathbb{R}^{5 \\times 4}$, $b \\in \\mathbb{R}^{5}$, and $C \\in \\mathbb{R}^{2 \\times 4}$ be given by\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 1 & 0 \\\\\n1 & 1 & 0 & 2 \\\\\n0 & 1 & 1 & 2 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 2 & 1\n\\end{pmatrix},\\quad\nb \\;=\\;\n\\begin{pmatrix}\n2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 2\n\\end{pmatrix},\\quad\nC \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0\n\\end{pmatrix},\\quad\nd \\;=\\;\n\\begin{pmatrix}\n1 \\\\ -1\n\\end{pmatrix}.\n$$\nThe goal is to find $x \\in \\mathbb{R}^{4}$ that minimizes $\\|A x - b\\|_{2}$ subject to $C x = d$. Start from the fundamental definitions of the Euclidean norm, orthogonality, and the orthogonal-triangular (QR) factorization, and proceed as follows.\n\n1. Using the null-space approach, rewrite the constraint $C x = d$ by introducing a particular solution $x_{p}$ satisfying $C x_{p} = d$ and a basis $N$ for the null space $\\ker(C)$ so that $x = x_{p} + N y$ with $y \\in \\mathbb{R}^{k}$ for an appropriate $k$. Construct $N$ by computing the orthogonal-triangular factorization of $C^{\\top}$, namely $C^{\\top} = Q_{c} R_{c}$, where $Q_{c}$ is orthogonal and $R_{c}$ is upper triangular, and then extracting from $Q_{c}$ an orthonormal basis for $\\ker(C)$.\n\n2. Compute $C^{\\top} = Q_{c} R_{c}$ explicitly for the given $C$, determine $N$, and choose the minimum Euclidean norm particular solution $x_{p}$ that satisfies $C x_{p} = d$.\n\n3. Form the reduced unconstrained problem $\\min_{y} \\|A N y - (b - A x_{p})\\|_{2}$ and solve it using an orthogonal-triangular factorization of $A N$, that is, compute $A N = Q R$ with $Q$ having orthonormal columns and $R$ upper triangular, and use this factorization to obtain the minimizer $y^{\\star}$ and hence the constrained minimizer $x^{\\star} = x_{p} + N y^{\\star}$.\n\n4. Let $\\kappa_{2}(\\cdot)$ denote the spectral condition number in the Euclidean norm, defined for a full column rank matrix $M$ as the ratio of its largest to smallest singular values. Compute $\\kappa_{2}(R)$ and $\\kappa_{2}(R_{c})$ by deriving them from first principles using the properties of the orthogonal-triangular factorization, and provide the exact closed-form expression for the ratio $\\kappa_{2}(R) / \\kappa_{2}(R_{c})$.\n\nReport your final answer as this exact ratio. No rounding is required, and no units are involved. The final answer must be a single real-valued closed-form expression.", "solution": "The problem is to find $x \\in \\mathbb{R}^{4}$ that solves the equality-constrained least squares problem:\n$$\n\\min_{x \\in \\mathbb{R}^{4}} \\|A x - b\\|_{2} \\quad \\text{subject to} \\quad C x = d\n$$\nwhere the matrices and vectors are given as\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 1 & 0 \\\\\n1 & 1 & 0 & 2 \\\\\n0 & 1 & 1 & 2 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 2 & 1\n\\end{pmatrix},\\quad\nb \\;=\\;\n\\begin{pmatrix}\n2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 2\n\\end{pmatrix},\\quad\nC \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0\n\\end{pmatrix},\\quad\nd \\;=\\;\n\\begin{pmatrix}\n1 \\\\ -1\n\\end{pmatrix}.\n$$\n\nStep 1: Null-space representation of the solution.\nAny vector $x$ that satisfies the linear constraint $C x = d$ can be written as the sum of a particular solution $x_{p}$ (which satisfies $C x_p = d$) and a vector from the null space of $C$, $\\ker(C)$. Let $N$ be a matrix whose columns form an orthonormal basis for $\\ker(C)$. Then any vector in $\\ker(C)$ can be expressed as $Ny$ for some vector $y \\in \\mathbb{R}^{k}$, where $k = \\dim(\\ker(C))$. The general solution to $C x = d$ is thus $x = x_p + Ny$.\n\nThe problem states that $N$ is to be found from the full orthogonal-triangular ($QR$) factorization of $C^{\\top} \\in \\mathbb{R}^{4 \\times 2}$, which is given by $C^{\\top} = Q_{c} R_{c}$, where $Q_{c} \\in \\mathbb{R}^{4 \\times 4}$ is an orthogonal matrix and $R_{c} \\in \\mathbb{R}^{4 \\times 2}$ is an upper triangular matrix. The columns of $Q_{c}$ form an orthonormal basis for $\\mathbb{R}^{4}$. We can partition $Q_{c}$ as $Q_{c} = [Q_{c1} \\ Q_{c2}]$, where the columns of $Q_{c1} \\in \\mathbb{R}^{4 \\times 2}$ form an orthonormal basis for the range of $C^{\\top}$, $\\text{range}(C^{\\top})$, and the columns of $Q_{c2} \\in \\mathbb{R}^{4 \\times 2}$ form an orthonormal basis for the orthogonal complement of the range, $(\\text{range}(C^{\\top}))^{\\perp}$. By the fundamental theorem of linear algebra, $(\\text{range}(C^{\\top}))^{\\perp} = \\ker(C)$. Therefore, the required matrix $N$ is $Q_{c2}$.\n\nStep 2: Explicit computation of $N$ and $x_p$.\nFirst, we compute the $QR$ factorization of $C^{\\top}$.\n$$\nC^{\\top} \\;=\\;\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 0 \\\\\n0 & 0\n\\end{pmatrix}\n$$\nThe columns of $C^{\\top}$ are the standard basis vectors $e_1$ and $e_2$ of $\\mathbb{R}^{4}$. They are already orthonormal. To form a full $QR$ factorization, we can extend this set to an orthonormal basis for $\\mathbb{R}^{4}$ by appending $e_3$ and $e_4$. This choice gives $Q_{c}$ as the identity matrix $I_4$.\n$$\nQ_{c} \\;=\\; I_{4} \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n$$\nThen, $R_{c}$ is found from $R_c = Q_{c}^{\\top} C^{\\top} = I_4 C^{\\top} = C^{\\top}$.\n$$\nR_{c} \\;=\\;\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 0 \\\\\n0 & 0\n\\end{pmatrix}\n$$\nThis $R_c$ is an upper triangular matrix of size $4 \\times 2$.\nWe partition $Q_{c} = [Q_{c1} \\ Q_{c2}]$ where $Q_{c1}$ consists of the first two columns and $Q_{c2}$ the last two.\n$$\nQ_{c1} \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\quad Q_{c2} \\;=\\; N \\;=\\; \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe columns of $N$ are indeed $e_3$ and $e_4$, which span $\\ker(C)$.\nNext, we find the minimum Euclidean norm particular solution $x_p$. This solution is unique and lies in $\\text{range}(C^{\\top})$. Thus, $x_p = C^{\\top} \\lambda$ for some $\\lambda \\in \\mathbb{R}^{2}$. Substituting into the constraint gives $C(C^{\\top} \\lambda) = d$.\n$$\nCC^{\\top} \\;=\\; \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\;=\\; I_2\n$$\nSo, $I_2 \\lambda = d$, which implies $\\lambda = d = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\nThe particular solution is $x_p = C^{\\top} d = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nStep 3: Solve the reduced unconstrained problem.\nSubstituting $x = x_p + Ny$ into the objective function leads to minimizing $\\|A(x_p + Ny) - b\\|_{2} = \\|ANy - (b - Ax_p)\\|_{2}$ over $y \\in \\mathbb{R}^{2}$.\nLet $M = AN$ and $z = b - Ax_p$.\n$$\nM = AN = \\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 1 & 1 & 0 & 2 \\\\ 0 & 1 & 1 & 2 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\\\ 1 & 2 \\\\ 0 & 1 \\\\ 2 & 1 \\end{pmatrix}\n$$\n$$\nAx_p = \\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 1 & 1 & 0 & 2 \\\\ 0 & 1 & 1 & 2 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\nz = b - Ax_p = \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 2 \\end{pmatrix}\n$$\nThe reduced problem is $\\min_{y} \\|My-z\\|_{2}$. We observe that $z$ is identical to the first column of $M$. Let $m_1, m_2$ be the columns of $M$, so $M=[m_1 \\ m_2]$. The problem is $\\min_{y_1, y_2} \\|y_1 m_1 + y_2 m_2 - m_1\\|_{2} = \\min_{y_1, y_2} \\|(y_1-1)m_1 + y_2 m_2\\|_{2}$. Since the columns $m_1$ and $m_2$ are linearly independent, the norm is minimized (to $0$) when the coefficients are zero: $y_1-1=0 \\implies y_1=1$ and $y_2=0$.\nSo the minimizer is $y^{\\star} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe constrained minimizer is $x^{\\star} = x_p + N y^{\\star} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nTo find the matrix $R$ for the next step, we compute the thin $QR$ factorization of $M = AN$. Let $M=QR$, where $Q \\in \\mathbb{R}^{5 \\times 2}$ has orthonormal columns and $R \\in \\mathbb{R}^{2 \\times 2}$ is upper triangular. Using the Gram-Schmidt process on the columns of $M$:\n$m_1 = (1, 0, 1, 0, 2)^{\\top}$. $\\|m_1\\|_{2} = \\sqrt{1^2+0^2+1^2+0^2+2^2} = \\sqrt{6}$.\n$R_{11} = \\sqrt{6}$. $q_1 = \\frac{1}{\\sqrt{6}} m_1$.\n$m_2 = (0, 2, 2, 1, 1)^{\\top}$.\n$R_{12} = q_1^{\\top} m_2 = \\frac{1}{\\sqrt{6}}(1 \\cdot 0 + 0 \\cdot 2 + 1 \\cdot 2 + 0 \\cdot 1 + 2 \\cdot 1) = \\frac{4}{\\sqrt{6}}$.\nThe component of $m_2$ orthogonal to $q_1$ is $m_2' = m_2 - R_{12} q_1 = m_2 - (q_1^{\\top} m_2) q_1 = m_2 - \\frac{4}{6}m_1 = \\begin{pmatrix} 0 \\\\ 2 \\\\ 2 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{2}{3}\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} -2 \\\\ 6 \\\\ 4 \\\\ 3 \\\\ -1 \\end{pmatrix}$.\n$R_{22} = \\|m_2'\\|_{2} = \\frac{1}{3}\\sqrt{(-2)^2+6^2+4^2+3^2+(-1)^2} = \\frac{1}{3}\\sqrt{4+36+16+9+1} = \\frac{\\sqrt{66}}{3}$.\nThe matrix $R$ is:\n$$\nR \\;=\\; \\begin{pmatrix} R_{11} & R_{12} \\\\ 0 & R_{22} \\end{pmatrix} \\;=\\; \\begin{pmatrix} \\sqrt{6} & \\frac{4}{\\sqrt{6}} \\\\ 0 & \\frac{\\sqrt{66}}{3} \\end{pmatrix}\n$$\n\nStep 4: Compute condition numbers and their ratio.\nThe spectral condition number $\\kappa_2(M)$ of a matrix $M$ with full column rank is the ratio of its largest to its smallest singular values, $\\kappa_2(M) = \\sigma_{\\max}(M)/\\sigma_{\\min}(M)$. The singular values of $M$ are the square roots of the eigenvalues of $M^{\\top}M$.\nFor the factorization $M=QR$, $M^{\\top}M = (QR)^{\\top}(QR) = R^{\\top}Q^{\\top}QR = R^{\\top}R$. Thus, the singular values of $M$ are the same as the singular values of $R$, and $\\kappa_2(M) = \\kappa_2(R)$.\nWe compute $M^{\\top}M$:\n$$\nM^{\\top}M = \\begin{pmatrix} 1 & 0 & 1 & 0 & 2 \\\\ 0 & 2 & 2 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\\\ 1 & 2 \\\\ 0 & 1 \\\\ 2 & 1 \\end{pmatrix} = \\begin{pmatrix} 6 & 4 \\\\ 4 & 10 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of $M^{\\top}M$ satisfy the characteristic equation $\\det(M^{\\top}M - \\lambda I)=0$:\n$$\n(6-\\lambda)(10-\\lambda) - 16 = 0 \\implies \\lambda^2 - 16\\lambda + 60 - 16 = 0 \\implies \\lambda^2 - 16\\lambda + 44 = 0\n$$\nThe eigenvalues are $\\lambda = \\frac{16 \\pm \\sqrt{16^2 - 4(44)}}{2} = \\frac{16 \\pm \\sqrt{256 - 176}}{2} = \\frac{16 \\pm \\sqrt{80}}{2} = 8 \\pm 2\\sqrt{5}$.\nThe singular values of $R$ are $\\sigma_{\\max} = \\sqrt{8+2\\sqrt{5}}$ and $\\sigma_{\\min} = \\sqrt{8-2\\sqrt{5}}$.\nThe condition number $\\kappa_2(R)$ is:\n$$\n\\kappa_2(R) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}} = \\sqrt{\\frac{8+2\\sqrt{5}}{8-2\\sqrt{5}}} = \\sqrt{\\frac{4+\\sqrt{5}}{4-\\sqrt{5}}} = \\sqrt{\\frac{(4+\\sqrt{5})^2}{16-5}} = \\frac{4+\\sqrt{5}}{\\sqrt{11}}\n$$\nNext, we compute $\\kappa_2(R_c)$. The matrix $R_c \\in \\mathbb{R}^{4 \\times 2}$ is $R_c = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}$. Its columns are linearly independent, so it has full column rank. Its singular values are the square roots of the eigenvalues of $R_c^{\\top}R_c$.\n$$\nR_c^{\\top}R_c = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2\n$$\nThe eigenvalues are $\\lambda_1 = 1$ and $\\lambda_2 = 1$. The singular values are $\\sigma_{\\max}(R_c)=1$ and $\\sigma_{\\min}(R_c)=1$.\nTherefore, $\\kappa_2(R_c) = \\frac{1}{1}=1$.\nThe required ratio is:\n$$\n\\frac{\\kappa_2(R)}{\\kappa_2(R_c)} = \\frac{(4+\\sqrt{5})/\\sqrt{11}}{1} = \\frac{4+\\sqrt{5}}{\\sqrt{11}}\n$$", "answer": "$$\n\\boxed{\\frac{4+\\sqrt{5}}{\\sqrt{11}}}\n$$", "id": "3408887"}, {"introduction": "Having seen how to apply orthogonal-triangular factorization, we now turn to the practical details of its implementation. This hands-on coding exercise challenges you to build and compare two fundamental algorithms for computing the QR factorization: Givens rotations and Householder reflections. By implementing a hybrid scheme that chooses between them based on matrix sparsity, you will gain direct experience with the trade-offs between computational efficiency and structure preservation, a critical consideration in large-scale scientific computing [@problem_id:3408903].", "problem": "You are given a sequence of linear least-squares problems of the form $\\min_{x \\in \\mathbb{R}^n} \\lVert A x - b \\rVert_2$ arising from discrete Partial Differential Equation (PDE) inverse problems in data assimilation, where $A \\in \\mathbb{R}^{m \\times n}$ is tall and sparse, and $m \\ge n$. The goal is to design and compare orthogonal-triangular factorizations for least squares based on both Givens rotations and Householder reflections, and to construct a sparsity-aware hybrid scheme that chooses between these two at the column level, motivated by preserving sparsity and minimizing fill-in. You must simulate a distributed assimilation setting by partitioning the rows of $A$ into contiguous blocks and processing each column by traversing the partitions in order when using Givens rotations.\n\nFundamental base and definitions to begin your derivation and design:\n- Orthogonal-triangular factorization for least squares: If $A = Q R$ with $Q \\in \\mathbb{R}^{m \\times m}$ orthogonal and $R \\in \\mathbb{R}^{m \\times n}$ upper-triangular in the leading $n \\times n$ block (denoted $R_{11}$), then the minimizer $x^\\star$ of $\\lVert A x - b \\rVert_2$ satisfies $R_{11} x^\\star = (Q^\\top b)_{1:n}$ and the minimal residual satisfies $\\lVert A x^\\star - b \\rVert_2 = \\lVert (Q^\\top b)_{n+1:m} \\rVert_2$.\n- Givens rotation: For $a, \\tilde{a} \\in \\mathbb{R}$, a rotation with cosine $c$ and sine $s$ satisfies $\\begin{bmatrix} c & s \\\\ -s & c \\end{bmatrix} \\begin{bmatrix} a \\\\ \\tilde{a} \\end{bmatrix} = \\begin{bmatrix} r \\\\ 0 \\end{bmatrix}$ with $r = \\sqrt{a^2 + \\tilde{a}^2}$ and $(c,s) = (a/r, \\tilde{a}/r)$ if $r \\ne 0$. Applying such rotations on pairs of rows can eliminate sub-diagonal entries while preserving orthogonality.\n- Householder reflection: Given $x \\in \\mathbb{R}^k$, the reflector $H = I - 2 u u^\\top$ with $u = v / \\lVert v \\rVert_2$ and $v = x + \\operatorname{sign}(x_1) \\lVert x \\rVert_2 e_1$ maps $x$ to $\\pm \\lVert x \\rVert_2 e_1$ and is orthogonal and symmetric.\n\nDesign requirements:\n- Implement three schemes that construct $Q$ implicitly by left-multiplying $A$ and $b$ with a sequence of orthogonal transformations, proceeding strictly column by column $k = 0, 1, \\dots, n-1$ without pivoting:\n  1. Pure Givens rotations: For each column $k$, traverse the partitions in order and, within each partition, for each row index $i > k$ having a nonzero $A_{i,k}$, apply a single Givens rotation on rows $(k,i)$ to annihilate $A_{i,k}$. Each annihilation counts as one orthogonal operation. Only columns $j \\ge k$ are updated at each step to preserve triangular structure and exploit sparsity. Apply the same rotation to the vector $b$.\n  2. Pure Householder reflections: For each column $k$, compute a single Householder reflector acting on rows $k:k{:}m-1$ to annihilate all sub-diagonal entries in column $k$ at once. Count one orthogonal operation per column. Apply the same reflector to $b$.\n  3. Hybrid sparsity-aware scheme: For each column $k$, decide between Givens and Householder using the rule: if the number of nonzeros below the diagonal in column $k$, i.e., $\\#\\{ i > k : A_{i,k} \\ne 0 \\}$, is less than or equal to a threshold $\\tau$, then use Givens rotations on that column as in item $1$; otherwise, use one Householder reflection on that column as in item $2$.\n\n- Residual reduction per orthogonal operation: Let $r_0 = \\lVert b \\rVert_2$ be the initial residual norm corresponding to the zero vector $x = 0$. At the end of each complete factorization (after all columns are processed), let $x^\\star$ be computed by solving $R_{11} x^\\star = (Q^\\top b)_{1:n}$, and let $r_\\text{final} = \\lVert A x^\\star - b \\rVert_2$. Define the average residual reduction per orthogonal operation as\n  $$ \\Delta_\\text{avg} \\;=\\; \\frac{r_0 - r_\\text{final}}{N_\\text{ops}}, $$\n  where $N_\\text{ops}$ is the total count of orthogonal operations (Givens rotations or Householder reflections) used by the scheme.\n\n- Distributed assimilation simulation: Given a partition count $P$, split $\\{0,1,\\dots,m-1\\}$ into $P$ contiguous row blocks differing in size by at most one. For Givens and the Givens part of the hybrid scheme, the annihilations in each column must be performed by sweeping the partitions in increasing order of block index and considering rows within the current block only, updating the global matrix and vector consistently. For Householder, apply the single column reflector globally.\n\n- Numerical stability: Use the stable formulas for Givens and Householder described above. Do not use column pivoting.\n\n- Output specification: For each test case, compute and report three floating-point numbers:\n  - The average residual reduction per orthogonal operation for the Givens scheme, denoted $\\Delta_\\text{avg}^\\text{G}$.\n  - The average residual reduction per orthogonal operation for the Householder scheme, denoted $\\Delta_\\text{avg}^\\text{H}$.\n  - The average residual reduction per orthogonal operation for the hybrid scheme, denoted $\\Delta_\\text{avg}^\\text{Hy}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists, each inner list corresponding to one test case in order and containing $[\\Delta_\\text{avg}^\\text{G}, \\Delta_\\text{avg}^\\text{H}, \\Delta_\\text{avg}^\\text{Hy}]$. For example, a valid format is $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$.\n\nTest suite:\n- Case A (PDE-like sparse banded, two partitions):\n  - Dimensions: $m = 8$, $n = 4$, partitions $P = 2$, threshold $\\tau = 2$.\n  - Matrix construction: Initialize $A \\in \\mathbb{R}^{8 \\times 4}$ with zeros. For each column $j \\in \\{0,1,2,3\\}$, let $r = 2 j$. If $0 \\le r-1 < 8$, set $A_{r-1,j} \\leftarrow A_{r-1,j} - 1$. If $0 \\le r < 8$, set $A_{r,j} \\leftarrow A_{r,j} + 2$. If $0 \\le r+1 < 8$, set $A_{r+1,j} \\leftarrow A_{r+1,j} - 1$.\n  - Right-hand side: $b \\in \\mathbb{R}^8$ with entries $b_i = \\sin(i+1)$ for $i \\in \\{0,\\dots,7\\}$, where the angle is in radians.\n- Case B (moderately dense, three partitions):\n  - Dimensions: $m = 9$, $n = 4$, partitions $P = 3$, threshold $\\tau = 2$.\n  - Matrix construction: Let $A$ have entries $A_{i,j}$ drawn from a standard normal distribution with a fixed seed $\\sigma = 42$ to ensure reproducibility, then add $0.1$ to the diagonal entries $A_{j,j}$ for $j \\in \\{0,1,2,3\\}$ to promote full column rank. All randomness must be deterministic and reproducible under the seed $\\sigma$.\n  - Right-hand side: $b$ from the same seed $\\sigma$ as independent standard normal entries.\n- Case C (very sparse, two partitions):\n  - Dimensions: $m = 10$, $n = 4$, partitions $P = 2$, threshold $\\tau = 1$.\n  - Matrix construction: Initialize $A$ with zeros. For rows $i \\in \\{0,1,2,3\\}$, set $A_{i,i} = 1$. For rows $i \\in \\{4,5,6,7,8,9\\}$, set $A_{i, (i \\bmod 4)} = 0.5 + 0.1 \\cdot (i - 4)$.\n  - Right-hand side: $b \\in \\mathbb{R}^{10}$ with entries $b_i = 1$ for all $i$.\n\nAngle unit: All trigonometric functions, if any, must use radians.\n\nFinal output format: Your program should produce a single line containing a single string representation of a Python-style list of lists $[[\\Delta_\\text{avg}^\\text{G}, \\Delta_\\text{avg}^\\text{H}, \\Delta_\\text{avg}^\\text{Hy}], \\dots]$ for the three test cases, in the order Case A, Case B, Case C, using default floating-point formatting with no additional text.", "solution": "The problem requires the design, implementation, and comparison of three distinct schemes for solving linear least-squares problems of the form $\\min_{x} \\lVert A x - b \\rVert_2$ using orthogonal-triangular ($QR$) factorization. The matrix $A \\in \\mathbb{R}^{m \\times n}$ is tall ($m \\ge n$) and potentially sparse. The comparison is based on a custom metric: the average residual reduction per orthogonal operation.\n\nThe foundational principle of solving least-squares via $QR$ factorization is to transform the original problem into an equivalent, but simpler one. If $A = QR$, where $Q \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix and $R \\in \\mathbb{R}^{m \\times n}$ is an upper-triangular matrix, the norm of the residual is preserved under multiplication by $Q^\\top$:\n$$ \\lVert A x - b \\rVert_2^2 = \\lVert Q^\\top (A x - b) \\rVert_2^2 = \\lVert Q^\\top A x - Q^\\top b \\rVert_2^2 = \\lVert R x - Q^\\top b \\rVert_2^2 $$\nWe partition $R$ and $Q^\\top b$ according to the dimensions of $A$:\n$$ R = \\begin{bmatrix} R_{11} \\\\ 0 \\end{bmatrix}, \\quad Q^\\top b = \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix} $$\nwhere $R_{11} \\in \\mathbb{R}^{n \\times n}$ is upper-triangular, $c_1 \\in \\mathbb{R}^n$, and $c_2 \\in \\mathbb{R}^{m-n}$. The minimization problem then becomes:\n$$ \\min_{x} \\left\\lVert \\begin{bmatrix} R_{11} \\\\ 0 \\end{bmatrix} x - \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix} \\right\\rVert_2^2 = \\min_{x} \\left( \\lVert R_{11} x - c_1 \\rVert_2^2 + \\lVert -c_2 \\rVert_2^2 \\right) $$\nThe minimizer $x^\\star$ is found by solving the upper-triangular system $R_{11} x^\\star = c_1$, which makes the first term zero. The minimal residual norm is then simply $\\lVert c_2 \\rVert_2 = \\lVert (Q^\\top b)_{n+1:m} \\rVert_2$.\n\nThe three schemes construct the orthogonal matrix $Q$ implicitly as a sequence of elementary orthogonal transformations applied to both $A$ and $b$. The updated $A$ becomes $R$, and the updated $b$ becomes $Q^\\top b$.\n\n**1. Pure Givens Rotations Scheme**\n\nA Givens rotation is an orthogonal transformation that operates in a two-dimensional subspace (a plane) and is designed to introduce a single zero into a vector or matrix. To zero out an element $A_{i,k}$ using the diagonal element $A_{k,k}$ (where $i > k$), a rotation is applied to rows $k$ and $i$. The algorithm proceeds column by column ($k=0, 1, \\dots, n-1$). For each column $k$, it systematically eliminates all sub-diagonal non-zeros $A_{i,k}$ for $i > k$ by applying a series of Givens rotations. Each rotation is counted as one orthogonal operation ($N_\\text{ops}$). This method is very targeted and is efficient for matrices where columns have few non-zeros below the diagonal, as it only modifies two rows at a time and can preserve existing sparsity. The problem specifies a \"distributed\" simulation where, for each column, the eliminations are performed by sweeping through predefined row partitions, which dictates the order of operations.\n\n**2. Pure Householder Reflections Scheme**\n\nA Householder reflection is an orthogonal transformation that reflects a vector across a hyperplane. It is more powerful than a Givens rotation, as a single reflection can zero out multiple elements of a vector simultaneously. For each column $k$, one Householder reflection is constructed based on the vector $A_{k:m,k}$ to eliminate all its elements below the first one. This single transformation is applied to the sub-matrix $A_{k:m, k:n}$ and the corresponding sub-vector of $b$. While powerful, a Householder reflection is a dense operation that typically affects all rows and columns of the active sub-matrix, which can destroy sparsity by introducing many new non-zero elements (fill-in). For this scheme, each column's transformation is counted as a single orthogonal operation, so $N_\\text{ops} = n$ (assuming no columns are skipped).\n\n**3. Hybrid Sparsity-Aware Scheme**\n\nThis scheme aims to be the best of both worlds. It makes a strategic choice for each column based on its local sparsity pattern. Before processing column $k$, it counts the number of non-zeros below the diagonal, denoted by $\\#\\{ i > k : A_{i,k} \\ne 0 \\}$.\n- If this count is less than or equal to a given threshold $\\tau$, the column is considered \"sparse enough\". The algorithm proceeds with the more delicate Givens rotation method for this column, as the cost of multiple targeted rotations is presumed to be lower and better for preserving sparsity. The operation count for this column is the number of rotations performed.\n- If the count exceeds $\\tau$, the column is considered \"dense enough\" that a single, powerful Householder reflection is more computationally efficient, despite the risk of fill-in. The operation count for this column is $1$.\nThis hybrid approach uses the threshold $\\tau$ to balance the trade-off between the fine-grained nature of Givens rotations and the broad-stroke efficiency of Householder reflections.\n\n**Metric Calculation**\n\nFor each scheme, the performance is quantified by $\\Delta_\\text{avg} = (r_0 - r_\\text{final}) / N_\\text{ops}$. Here, $r_0 = \\lVert b \\rVert_2$ is the initial residual norm (for a trial solution $x=0$), $r_\\text{final}$ is the final minimal residual norm $\\lVert (Q^\\top b)_{n+1:m} \\rVert_2$, and $N_\\text{ops}$ is the total count of orthogonal operations (Givens rotations or Householder reflections). This metric evaluates the efficiency of each scheme in reducing the residual norm per fundamental computational unit, providing a basis for comparison.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# A small tolerance for floating-point comparisons.\nTOL = 1e-15\n\ndef get_partitions(m, P):\n    \"\"\"Computes P contiguous row partitions for m rows.\"\"\"\n    if P <= 0:\n        return []\n    q = m // P\n    r = m % P\n    partitions = []\n    current_row = 0\n    for i in range(P):\n        size = q + 1 if i < r else q\n        partitions.append(range(current_row, current_row + size))\n        current_row += size\n    return partitions\n\ndef solve_givens(A_orig, b_orig, P):\n    \"\"\"\n    Solves the least-squares problem using Givens rotations, respecting partitions.\n    \"\"\"\n    A = A_orig.copy().astype(float)\n    b = b_orig.copy().astype(float)\n    m, n = A.shape\n    \n    n_ops = 0\n    r0_norm = np.linalg.norm(b)\n    partitions = get_partitions(m, P)\n\n    for k in range(n):\n        for p_range in partitions:\n            for i in p_range:\n                if i > k:\n                    if not np.isclose(A[i, k], 0, atol=TOL):\n                        n_ops += 1\n                        a = A[k, k]\n                        atilde = A[i, k]\n                        \n                        r = np.hypot(a, atilde)\n                        c = a / r\n                        s = atilde / r\n                        \n                        R_k = A[k, k:].copy()\n                        R_i = A[i, k:].copy()\n                        A[k, k:] = c * R_k + s * R_i\n                        A[i, k:] = -s * R_k + c * R_i\n                        \n                        b_k = b[k]\n                        b_i = b[i]\n                        b[k] = c * b_k + s * b_i\n                        b[i] = -s * b_k + c * b_i\n\n    r_final_norm = np.linalg.norm(b[n:])\n    \n    if n_ops == 0:\n        return 0.0\n    return (r0_norm - r_final_norm) / n_ops\n\ndef solve_householder(A_orig, b_orig):\n    \"\"\"\n    Solves the least-squares problem using Householder reflections.\n    \"\"\"\n    A = A_orig.copy().astype(float)\n    b = b_orig.copy().astype(float)\n    m, n = A.shape\n    \n    n_ops = 0\n    r0_norm = np.linalg.norm(b)\n\n    for k in range(n):\n        # The vector to be transformed\n        x = A[k:m, k]\n        \n        # Only apply reflection if there are subdiagonal elements to annihilate\n        if x.size > 1 and not np.isclose(np.linalg.norm(x[1:]), 0, atol=TOL):\n            n_ops += 1\n            \n            # Stable Householder vector calculation\n            sigma = np.copysign(np.linalg.norm(x), x[0])\n            v = x.copy()\n            v[0] += sigma\n            \n            beta = 2.0 / (v.T @ v)\n            \n            # Apply reflection to submatrix of A and subvector of b\n            w_A = beta * (A[k:m, k:n].T @ v)\n            A[k:m, k:n] -= np.outer(v, w_A)\n            \n            w_b = beta * (b[k:m].T @ v)\n            b[k:m] -= v * w_b\n\n    r_final_norm = np.linalg.norm(b[n:])\n    \n    if n_ops == 0:\n        return 0.0\n    return (r0_norm - r_final_norm) / n_ops\n\ndef solve_hybrid(A_orig, b_orig, P, tau):\n    \"\"\"\n    Solves the least-squares problem using the hybrid Givens-Householder scheme.\n    \"\"\"\n    A = A_orig.copy().astype(float)\n    b = b_orig.copy().astype(float)\n    m, n = A.shape\n\n    n_ops = 0\n    r0_norm = np.linalg.norm(b)\n    partitions = get_partitions(m, P)\n    \n    for k in range(n):\n        sub_col = A[k+1:m, k]\n        nnz = np.count_nonzero(np.abs(sub_col) > TOL)\n\n        if 0 < nnz <= tau:\n            # Use Givens rotations\n            n_ops += nnz\n            for p_range in partitions:\n                for i in p_range:\n                    if i > k:\n                        if not np.isclose(A[i, k], 0, atol=TOL):\n                            a = A[k, k]\n                            atilde = A[i, k]\n                            r = np.hypot(a, atilde)\n                            c = a / r\n                            s = atilde / r\n                            \n                            R_k = A[k, k:].copy()\n                            R_i = A[i, k:].copy()\n                            A[k, k:] = c * R_k + s * R_i\n                            A[i, k:] = -s * R_k + c * R_i\n                            \n                            b_k = b[k]\n                            b_i = b[i]\n                            b[k] = c * b_k + s * b_i\n                            b[i] = -s * b_k + c * b_i\n        elif nnz > tau:\n            # Use Householder reflection\n            n_ops += 1\n            x = A[k:m, k]\n            \n            sigma = np.copysign(np.linalg.norm(x), x[0])\n            v = x.copy()\n            v[0] += sigma\n            \n            beta = 2.0 / (v.T @ v)\n\n            w_A = beta * (A[k:m, k:n].T @ v)\n            A[k:m, k:n] -= np.outer(v, w_A)\n\n            w_b = beta * (b[k:m].T @ v)\n            b[k:m] -= v * w_b\n\n    r_final_norm = np.linalg.norm(b[n:])\n\n    if n_ops == 0:\n        return 0.0\n    return (r0_norm - r_final_norm) / n_ops\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = []\n\n    # Case A\n    m, n, P, tau = 8, 4, 2, 2\n    A_A = np.zeros((m, n))\n    for j in range(n):\n        r_idx = 2 * j\n        if 0 <= r_idx - 1 < m: A_A[r_idx-1, j] += -1\n        if 0 <= r_idx < m: A_A[r_idx, j] += 2\n        if 0 <= r_idx + 1 < m: A_A[r_idx+1, j] += -1\n    b_A = np.sin(np.arange(m) + 1)\n    test_cases.append({'A': A_A, 'b': b_A, 'P': P, 'tau': tau})\n\n    # Case B\n    m, n, P, tau = 9, 4, 3, 2\n    rng = np.random.default_rng(seed=42)\n    A_B = rng.standard_normal((m, n))\n    for j in range(n):\n        A_B[j, j] += 0.1\n    b_B = rng.standard_normal(m)\n    test_cases.append({'A': A_B, 'b': b_B, 'P': P, 'tau': tau})\n\n    # Case C\n    m, n, P, tau = 10, 4, 2, 1\n    A_C = np.zeros((m, n))\n    for i in range(n):\n        A_C[i, i] = 1.0\n    for i in range(4, 10):\n        A_C[i, i % n] = 0.5 + 0.1 * (i - 4)\n    b_C = np.ones(m)\n    test_cases.append({'A': A_C, 'b': b_C, 'P': P, 'tau': tau})\n\n    results = []\n    for case in test_cases:\n        A, b, P, tau = case['A'], case['b'], case['P'], case['tau']\n        \n        delta_G = solve_givens(A, b, P)\n        delta_H = solve_householder(A, b)\n        delta_Hy = solve_hybrid(A, b, P, tau)\n        \n        results.append([delta_G, delta_H, delta_Hy])\n\n    # Format the results into the required string representation\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3408903"}]}