{"hands_on_practices": [{"introduction": "The foundation of data assimilation in linear-Gaussian systems lies in the Bayesian update, where prior knowledge is combined with new observations to yield a refined posterior estimate. This exercise solidifies the core mechanics of this process by guiding you through the analytical derivation of the posterior mean and covariance from first principles. By applying these formulas to a concrete numerical example [@problem_id:3384489], you will gain a practical command of the fundamental equations that govern Gaussian data assimilation.", "problem": "Consider a linear inverse problem under the linear-Gaussian data assimilation framework. Let the unknown parameter vector be $m \\in \\mathbb{R}^{2}$ with a Gaussian prior $m \\sim \\mathcal{N}(m_{0}, C_{0})$, and let the observation model be $y = G m + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\Gamma)$ is independent of $m$. You are given\n$$\nm_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad\nC_{0} = \\begin{pmatrix} 1.2  0 \\\\ 0  0.8 \\end{pmatrix}, \\quad\nG = \\begin{pmatrix} 1  -0.5 \\\\ 0.2  1.5 \\end{pmatrix}, \\quad\n\\Gamma = \\begin{pmatrix} 0.5  0.1 \\\\ 0.1  0.2 \\end{pmatrix}, \\quad\ny = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nStarting from Bayes’ theorem and the quadratic form of Gaussian densities, derive the posterior distribution $m \\mid y \\sim \\mathcal{N}(m_{\\text{post}}, C_{\\text{post}})$ by completing the square in the negative log-posterior. Then, using the provided numerical matrices, compute $m_{\\text{post}}$ and $C_{\\text{post}}$ explicitly. Verify that $C_{\\text{post}}$ is symmetric and symmetric positive definite (SPD), for example by checking positivity of its leading principal minors. Finally, report the scalar\n$$\ns \\equiv \\ln\\!\\big(\\det(C_{\\text{post}})\\big).\n$$\nRound your final reported value of $s$ to $4$ significant figures. Express the final answer as a pure number with no units.", "solution": "### Derivation of the Posterior Distribution\nAccording to Bayes' theorem, the posterior probability density function (PDF) $p(m|y)$ is proportional to the product of the likelihood $p(y|m)$ and the prior $p(m)$:\n$$p(m|y) \\propto p(y|m) p(m)$$\nThe prior is given as $m \\sim \\mathcal{N}(m_0, C_0)$, so its PDF has the form:\n$$p(m) \\propto \\exp\\left(-\\frac{1}{2}(m-m_0)^T C_0^{-1} (m-m_0)\\right)$$\nThe observation model $y = Gm + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, \\Gamma)$ implies that the conditional distribution of $y$ given $m$ is $y|m \\sim \\mathcal{N}(Gm, \\Gamma)$. The likelihood function is thus:\n$$p(y|m) \\propto \\exp\\left(-\\frac{1}{2}(y-Gm)^T \\Gamma^{-1} (y-Gm)\\right)$$\nCombining these, the posterior PDF is:\n$$p(m|y) \\propto \\exp\\left(-\\frac{1}{2}(y-Gm)^T \\Gamma^{-1} (y-Gm) - \\frac{1}{2}(m-m_0)^T C_0^{-1} (m-m_0)\\right)$$\nThe posterior is also Gaussian, so its PDF will be of the form $p(m|y) \\propto \\exp\\left(-\\frac{1}{2}(m-m_{\\text{post}})^T C_{\\text{post}}^{-1} (m-m_{\\text{post}})\\right)$. The negative log-posterior, up to an additive constant, is the quadratic form $J(m)$:\n$$J(m) = \\frac{1}{2}(y-Gm)^T \\Gamma^{-1} (y-Gm) + \\frac{1}{2}(m-m_0)^T C_0^{-1} (m-m_0)$$\nWe expand the terms in $J(m)$ and collect terms quadratic and linear in $m$:\n$$2J(m) = (y^T - m^T G^T) \\Gamma^{-1} (y - Gm) + (m^T - m_0^T) C_0^{-1} (m - m_0)$$\n$$2J(m) = y^T \\Gamma^{-1} y - y^T \\Gamma^{-1} Gm - m^T G^T \\Gamma^{-1} y + m^T G^T \\Gamma^{-1} Gm + m^T C_0^{-1} m - m^T C_0^{-1} m_0 - m_0^T C_0^{-1} m + m_0^T C_0^{-1} m_0$$\nRecognizing that the scalar terms $y^T \\Gamma^{-1} Gm$ and $m^T C_0^{-1} m_0$ are equal to their transposes, and grouping terms:\n$$2J(m) = m^T (G^T \\Gamma^{-1} G + C_0^{-1}) m - 2 m^T (G^T \\Gamma^{-1} y + C_0^{-1} m_0) + \\text{const.}$$\nBy completing the square, we can identify the posterior precision matrix $C_{\\text{post}}^{-1}$ and the posterior mean $m_{\\text{post}}$. Comparing with the general quadratic form $ (m-m_{\\text{post}})^T C_{\\text{post}}^{-1} (m-m_{\\text{post}}) = m^T C_{\\text{post}}^{-1} m - 2m^T C_{\\text{post}}^{-1} m_{\\text{post}} + \\text{const.}$, we find:\n$$C_{\\text{post}}^{-1} = C_0^{-1} + G^T \\Gamma^{-1} G$$\n$$C_{\\text{post}}^{-1} m_{\\text{post}} = C_0^{-1} m_0 + G^T \\Gamma^{-1} y$$\nFrom these, we obtain the posterior covariance matrix and mean vector:\n$$C_{\\text{post}} = (C_0^{-1} + G^T \\Gamma^{-1} G)^{-1}$$\n$$m_{\\text{post}} = C_{\\text{post}} (C_0^{-1} m_0 + G^T \\Gamma^{-1} y)$$\n\n### Numerical Computation\nWe are given:\n$m_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $C_{0} = \\begin{pmatrix} 1.2  0 \\\\ 0  0.8 \\end{pmatrix}$, $G = \\begin{pmatrix} 1  -0.5 \\\\ 0.2  1.5 \\end{pmatrix}$, $\\Gamma = \\begin{pmatrix} 0.5  0.1 \\\\ 0.1  0.2 \\end{pmatrix}$, $y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nFirst, we compute the inverse matrices $C_0^{-1}$ and $\\Gamma^{-1}$:\n$$C_0^{-1} = \\begin{pmatrix} \\frac{1}{1.2}  0 \\\\ 0  \\frac{1}{0.8} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{6}  0 \\\\ 0  \\frac{5}{4} \\end{pmatrix}$$\n$$\\det(\\Gamma) = (0.5)(0.2) - (0.1)^2 = 0.1 - 0.01 = 0.09 = \\frac{9}{100}$$\n$$\\Gamma^{-1} = \\frac{1}{0.09} \\begin{pmatrix} 0.2  -0.1 \\\\ -0.1  0.5 \\end{pmatrix} = \\frac{100}{9} \\begin{pmatrix} \\frac{2}{10}  -\\frac{1}{10} \\\\ -\\frac{1}{10}  \\frac{5}{10} \\end{pmatrix} = \\frac{10}{9} \\begin{pmatrix} 2  -1 \\\\ -1  5 \\end{pmatrix} = \\begin{pmatrix} \\frac{20}{9}  -\\frac{10}{9} \\\\ -\\frac{10}{9}  \\frac{50}{9} \\end{pmatrix}$$\nNext, we compute the Hessian term $G^T\\Gamma^{-1}G$:\n$$G^T = \\begin{pmatrix} 1  0.2 \\\\ -0.5  1.5 \\end{pmatrix} = \\begin{pmatrix} 1  \\frac{1}{5} \\\\ -\\frac{1}{2}  \\frac{3}{2} \\end{pmatrix}$$\n$$G^T \\Gamma^{-1} = \\begin{pmatrix} 1  \\frac{1}{5} \\\\ -\\frac{1}{2}  \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{20}{9}  -\\frac{10}{9} \\\\ -\\frac{10}{9}  \\frac{50}{9} \\end{pmatrix} = \\begin{pmatrix} \\frac{20}{9} - \\frac{2}{9}  -\\frac{10}{9} + \\frac{10}{9} \\\\ -\\frac{10}{9} - \\frac{15}{9}  \\frac{5}{9} + \\frac{75}{9} \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{9}  0 \\\\ -\\frac{25}{9}  \\frac{80}{9} \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ -\\frac{25}{9}  \\frac{80}{9} \\end{pmatrix}$$\n$$G^T \\Gamma^{-1} G = \\begin{pmatrix} 2  0 \\\\ -\\frac{25}{9}  \\frac{80}{9} \\end{pmatrix} \\begin{pmatrix} 1  -\\frac{1}{2} \\\\ \\frac{1}{5}  \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} 2  -1 \\\\ -\\frac{25}{9} + \\frac{16}{9}  \\frac{25}{18} + \\frac{120}{9} \\end{pmatrix} = \\begin{pmatrix} 2  -1 \\\\ -\\frac{9}{9}  \\frac{25}{18} + \\frac{240}{18} \\end{pmatrix} = \\begin{pmatrix} 2  -1 \\\\ -1  \\frac{265}{18} \\end{pmatrix}$$\nNow we compute the posterior precision matrix $C_{\\text{post}}^{-1}$:\n$$C_{\\text{post}}^{-1} = C_0^{-1} + G^T \\Gamma^{-1} G = \\begin{pmatrix} \\frac{5}{6}  0 \\\\ 0  \\frac{5}{4} \\end{pmatrix} + \\begin{pmatrix} 2  -1 \\\\ -1  \\frac{265}{18} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{6} + \\frac{12}{6}  -1 \\\\ -1  \\frac{45}{36} + \\frac{530}{36} \\end{pmatrix} = \\begin{pmatrix} \\frac{17}{6}  -1 \\\\ -1  \\frac{575}{36} \\end{pmatrix}$$\nTo find $C_{\\text{post}}$, we invert $C_{\\text{post}}^{-1}$:\n$$\\det(C_{\\text{post}}^{-1}) = \\left(\\frac{17}{6}\\right)\\left(\\frac{575}{36}\\right) - (-1)^2 = \\frac{9775}{216} - 1 = \\frac{9775 - 216}{216} = \\frac{9559}{216}$$\n$$C_{\\text{post}} = \\frac{1}{\\det(C_{\\text{post}}^{-1})} \\begin{pmatrix} \\frac{575}{36}  1 \\\\ 1  \\frac{17}{6} \\end{pmatrix} = \\frac{216}{9559} \\begin{pmatrix} \\frac{575}{36}  1 \\\\ 1  \\frac{17}{6} \\end{pmatrix} = \\frac{1}{9559} \\begin{pmatrix} 6 \\cdot 575  216 \\\\ 216  36 \\cdot 17 \\end{pmatrix}$$\n$$C_{\\text{post}} = \\frac{1}{9559} \\begin{pmatrix} 3450  216 \\\\ 216  612 \\end{pmatrix}$$\nNow we compute $m_{\\text{post}}$. Since $m_0 = 0$, the formula simplifies to $m_{\\text{post}} = C_{\\text{post}} (G^T \\Gamma^{-1} y)$.\n$$G^T \\Gamma^{-1} y = \\begin{pmatrix} 2  0 \\\\ -\\frac{25}{9}  \\frac{80}{9} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{25}{9} \\end{pmatrix}$$\n$$m_{\\text{post}} = C_{\\text{post}} \\begin{pmatrix} 2 \\\\ -\\frac{25}{9} \\end{pmatrix} = \\frac{1}{9559} \\begin{pmatrix} 3450  216 \\\\ 216  612 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -\\frac{25}{9} \\end{pmatrix} = \\frac{1}{9559} \\begin{pmatrix} 3450(2) + 216(-\\frac{25}{9}) \\\\ 216(2) + 612(-\\frac{25}{9}) \\end{pmatrix}$$\n$$m_{\\text{post}} = \\frac{1}{9559} \\begin{pmatrix} 6900 - 24(25) \\\\ 432 - 68(25) \\end{pmatrix} = \\frac{1}{9559} \\begin{pmatrix} 6900 - 600 \\\\ 432 - 1700 \\end{pmatrix} = \\frac{1}{9559} \\begin{pmatrix} 6300 \\\\ -1268 \\end{pmatrix}$$\nIn decimal form, this is $m_{\\text{post}} \\approx \\begin{pmatrix} 0.65906 \\\\ -0.13265 \\end{pmatrix}$.\n\n### Verification of $C_{\\text{post}}$\n- **Symmetry:** The matrix $C_{\\text{post}} = \\frac{1}{9559} \\begin{pmatrix} 3450  216 \\\\ 216  612 \\end{pmatrix}$ is clearly symmetric as its off-diagonal entries are equal.\n- **Positive Definiteness:** We check the leading principal minors.\nThe first minor is $C_{\\text{post},11} = \\frac{3450}{9559} > 0$.\nThe second minor is $\\det(C_{\\text{post}})$. We know that $\\det(C_{\\text{post}}) = (\\det(C_{\\text{post}}^{-1}))^{-1}$. Since $\\det(C_{\\text{post}}^{-1}) = \\frac{9559}{216} > 0$, we have $\\det(C_{\\text{post}}) = \\frac{216}{9559} > 0$.\nSince all leading principal minors are positive, $C_{\\text{post}}$ is symmetric positive definite.\n\n### Calculation of $s$\nThe final scalar to compute is $s \\equiv \\ln(\\det(C_{\\text{post}}))$.\n$$s = \\ln\\left(\\frac{216}{9559}\\right)$$\nUsing a calculator:\n$$s \\approx \\ln(0.0225965059...) \\approx -3.78994114...$$\nRounding to $4$ significant figures, we get:\n$$s \\approx -3.790$$", "answer": "$$\\boxed{-3.790}$$", "id": "3384489"}, {"introduction": "In practical applications, the true error characteristics of a system are complex and often simplified for computational or modeling convenience. This practice explores the direct consequences of one such common simplification: ignoring cross-correlations in observation errors. By comparing the posterior estimate obtained using the full error covariance matrix with one that assumes independent errors [@problem_id:3384513], you will quantify the systematic bias introduced by this assumption, developing a crucial understanding of model fidelity and its impact on the reliability of your results.", "problem": "You are given a linear inverse problem with a multivariate Gaussian prior and a multivariate Gaussian likelihood. The forward map is linear, the state is finite-dimensional, and the observation errors exhibit known cross-correlations. You must derive the posterior distribution in two ways: (i) using the full observation-error covariance with off-diagonal entries and (ii) using the same covariance but with all off-diagonal entries set to zero (diagonal-only). Then, you must quantify the systematic bias introduced by ignoring the off-diagonal terms.\n\nBegin from the following fundamental base:\n- The prior on the unknown state vector $x \\in \\mathbb{R}^{n}$ is $\\mathcal{N}(m_0, C_0)$, where $m_0 \\in \\mathbb{R}^{n}$ and $C_0 \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite.\n- The data model (likelihood) is $y = H x + \\varepsilon$ with $H \\in \\mathbb{R}^{m \\times n}$ a known matrix and $\\varepsilon \\sim \\mathcal{N}(0, \\Gamma)$, where $\\Gamma \\in \\mathbb{R}^{m \\times m}$ is symmetric positive definite and encodes known sensor error cross-correlations.\n- Bayes’ rule for Gaussian priors and Gaussian likelihoods, and the fact that completing the square yields a Gaussian posterior.\n\nTasks:\n1. Derive, from first principles, the posterior mean and covariance for the linear-Gaussian inverse problem using the full observation-error covariance $\\Gamma$ (with off-diagonal entries).\n2. Repeat the derivation for the case where the same covariance is replaced by its diagonal-only version $\\Gamma_{\\mathrm{diag}}$, formed by zeroing all off-diagonal entries while retaining the original diagonal entries.\n3. Define a fixed ground-truth state $x^{\\ast} \\in \\mathbb{R}^{n}$. Define the systematic bias induced by ignoring off-diagonal terms as the difference between the expected posterior means (expectation taken with respect to the observational noise $\\varepsilon$, conditioned on $x^{\\ast}$):\n$$\nb \\equiv \\mathbb{E}\\big[m_{\\mathrm{post}}^{\\mathrm{diag}}(y) \\,\\big|\\, x^{\\ast} \\big] - \\mathbb{E}\\big[m_{\\mathrm{post}}^{\\mathrm{full}}(y) \\,\\big|\\, x^{\\ast} \\big] \\in \\mathbb{R}^{n}.\n$$\nYou must compute the Euclidean norm $\\|b\\|_{2}$ for each test case below. For the purpose of this computation, note that $\\mathbb{E}[y \\mid x^{\\ast}] = H x^{\\ast}$.\n\nTest suite:\nProvide numerical answers for the following four test cases. In every case, form the diagonal-only covariance as $\\Gamma_{\\mathrm{diag}} = \\operatorname{diag}(\\Gamma)$ by zeroing off-diagonals and keeping the original variances on the diagonal.\n\n- Case A:\n  - State dimension $n = 2$, observation dimension $m = 2$.\n  - Forward operator:\n    $$\n    H = \\begin{bmatrix}\n    1.0  0.5 \\\\\n    0.2  1.0\n    \\end{bmatrix}.\n    $$\n  - Prior mean and covariance:\n    $$\n    m_0 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix}, \\quad\n    C_0 = \\begin{bmatrix} 1.0  0.3 \\\\ 0.3  1.5 \\end{bmatrix}.\n    $$\n  - Ground truth:\n    $$\n    x^{\\ast} = \\begin{bmatrix} 0.7 \\\\ -1.2 \\end{bmatrix}.\n    $$\n  - Observation-error covariance (uncorrelated):\n    $$\n    \\Gamma = \\begin{bmatrix} 0.04  0.0 \\\\ 0.0  0.09 \\end{bmatrix}.\n    $$\n\n- Case B:\n  - Same $n$, $m$, $H$, $m_0$, $C_0$, and $x^{\\ast}$ as in Case A.\n  - Observation-error covariance (positively correlated sensors):\n    $$\n    \\Gamma = \\begin{bmatrix} 0.04  0.048 \\\\ 0.048  0.09 \\end{bmatrix}.\n    $$\n\n- Case C:\n  - Same $n$, $m$, $H$, $m_0$, $C_0$, and $x^{\\ast}$ as in Case A.\n  - Observation-error covariance (strongly correlated sensors):\n    $$\n    \\Gamma = \\begin{bmatrix} 0.04  0.0594 \\\\ 0.0594  0.09 \\end{bmatrix}.\n    $$\n\n- Case D:\n  - State dimension $n = 2$, observation dimension $m = 3$.\n  - Forward operator:\n    $$\n    H = \\begin{bmatrix}\n    1.0  0.1 \\\\\n    0.5  1.0 \\\\\n    -0.2  0.7\n    \\end{bmatrix}.\n    $$\n  - Prior mean and covariance:\n    $$\n    m_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}, \\quad\n    C_0 = \\begin{bmatrix} 2.0  0.4 \\\\ 0.4  0.5 \\end{bmatrix}.\n    $$\n  - Ground truth:\n    $$\n    x^{\\ast} = \\begin{bmatrix} 1.5 \\\\ -0.5 \\end{bmatrix}.\n    $$\n  - Observation-error covariance with Toeplitz correlation:\n    Let the standard deviations be $\\sigma = \\begin{bmatrix} 0.05  0.08  0.06 \\end{bmatrix}^{\\top}$ and the correlation matrix be\n    $$\n    R = \\begin{bmatrix}\n    1  \\rho  \\rho^{2} \\\\\n    \\rho  1  \\rho \\\\\n    \\rho^{2}  \\rho  1\n    \\end{bmatrix}, \\quad \\text{with } \\rho = 0.6.\n    $$\n    Then\n    $$\n    \\Gamma = \\operatorname{diag}(\\sigma) \\, R \\, \\operatorname{diag}(\\sigma).\n    $$\n\nImplementation and output requirements:\n- For each case, compute the posterior using the full $\\Gamma$ and the posterior using $\\Gamma_{\\mathrm{diag}}$, then compute the bias vector $b$ as defined above using $\\mathbb{E}[y \\mid x^{\\ast}] = H x^{\\ast}$, and finally compute $\\|b\\|_{2}$.\n- Your program must output a single line with the results for the four cases in order A, B, C, D as a comma-separated list enclosed in square brackets, with each value rounded to $10$ decimal places, for example:\n  $$\n  [\\text{value\\_A}, \\text{value\\_B}, \\text{value\\_C}, \\text{value\\_D}].\n  $$\n\nNo physical units are involved. Angles are not used. Percentages are not used. The final outputs are real numbers (floats).", "solution": "The core of the problem lies in deriving and comparing posterior distributions arising from a linear-Gaussian model. The posterior probability density function, $p(x|y)$, is given by Bayes' rule:\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\nwhere $p(y|x)$ is the likelihood and $p(x)$ is the prior.\n\nThe prior on the state vector $x \\in \\mathbb{R}^{n}$ is given as a multivariate Gaussian distribution, $\\mathcal{N}(m_0, C_0)$:\n$$\np(x) \\propto \\exp\\left(-\\frac{1}{2}(x - m_0)^\\top C_0^{-1} (x - m_0)\\right)\n$$\nThe data model is $y = Hx + \\varepsilon$, where the noise $\\varepsilon$ is Gaussian with zero mean and covariance $\\Gamma$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\Gamma)$. This implies that the likelihood of observing $y$ given a state $x$ is:\n$$\np(y|x) = \\mathcal{N}(Hx, \\Gamma) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^\\top \\Gamma^{-1} (y - Hx)\\right)\n$$\nSince both the prior and the likelihood are Gaussian, the posterior will also be Gaussian. To find its parameters, we combine the exponents of the prior and likelihood. The negative logarithm of the posterior is proportional to the cost function $J(x)$:\n$$\nJ(x) = \\frac{1}{2}(x - m_0)^\\top C_0^{-1} (x - m_0) + \\frac{1}{2}(y - Hx)^\\top \\Gamma^{-1} (y - Hx)\n$$\nWe expand the quadratic forms in $J(x)$ to identify the posterior mean and covariance by the method of \"completing the square\".\n$$\n2J(x) = (x - m_0)^\\top C_0^{-1} (x - m_0) + (y - Hx)^\\top \\Gamma^{-1} (y - Hx)\n$$\n$$\n2J(x) = x^\\top C_0^{-1} x - 2x^\\top C_0^{-1} m_0 + m_0^\\top C_0^{-1} m_0 + y^\\top \\Gamma^{-1} y - 2y^\\top \\Gamma^{-1} Hx + x^\\top H^\\top \\Gamma^{-1} Hx\n$$\nGrouping terms by powers of $x$:\n$$\n2J(x) = x^\\top (C_0^{-1} + H^\\top \\Gamma^{-1} H) x - 2x^\\top (C_0^{-1} m_0 + H^\\top \\Gamma^{-1} y) + (\\text{terms not involving } x)\n$$\nThe posterior distribution $p(x|y)$ is of the form $\\mathcal{N}(m_{\\mathrm{post}}, C_{\\mathrm{post}})$, whose exponent is $\\frac{1}{2}(x - m_{\\mathrm{post}})^\\top C_{\\mathrm{post}}^{-1} (x - m_{\\mathrm{post}})$. Expanding this gives:\n$$\n(x - m_{\\mathrm{post}})^\\top C_{\\mathrm{post}}^{-1} (x - m_{\\mathrm{post}}) = x^\\top C_{\\mathrm{post}}^{-1} x - 2x^\\top C_{\\mathrm{post}}^{-1} m_{\\mathrm{post}} + m_{\\mathrm{post}}^\\top C_{\\mathrm{post}}^{-1} m_{\\mathrm{post}}\n$$\nBy comparing the quadratic and linear terms in $x$ from $2J(x)$ and the posterior exponent, we identify the inverse posterior covariance and the posterior mean.\n\n**1. Derivation of Posterior for the Full Model (Task 1)**\n\nThe posterior parameters for the model using the full observation-error covariance $\\Gamma$ are found by direct comparison.\nThe quadratic term gives the inverse posterior covariance, $C_{\\mathrm{post}}^{\\mathrm{full}}{}^{-1}$:\n$$\nC_{\\mathrm{post}}^{\\mathrm{full}}{}^{-1} = C_0^{-1} + H^\\top \\Gamma^{-1} H\n$$\nThus, the posterior covariance is:\n$$\nC_{\\mathrm{post}}^{\\mathrm{full}} = (C_0^{-1} + H^\\top \\Gamma^{-1} H)^{-1}\n$$\nThe linear term gives the posterior mean, $m_{\\mathrm{post}}^{\\mathrm{full}}$:\n$$\nC_{\\mathrm{post}}^{\\mathrm{full}}{}^{-1} m_{\\mathrm{post}}^{\\mathrm{full}} = C_0^{-1} m_0 + H^\\top \\Gamma^{-1} y\n$$\n$$\nm_{\\mathrm{post}}^{\\mathrm{full}}(y) = C_{\\mathrm{post}}^{\\mathrm{full}} (C_0^{-1} m_0 + H^\\top \\Gamma^{-1} y)\n$$\n\n**2. Derivation of Posterior for the Diagonal-Only Model (Task 2)**\n\nFor the simplified model, we replace the true covariance $\\Gamma$ with its diagonal approximation $\\Gamma_{\\mathrm{diag}}$, which is formed by setting all off-diagonal elements of $\\Gamma$ to zero. The derivation is identical, with $\\Gamma$ replaced by $\\Gamma_{\\mathrm{diag}}$.\nThe posterior covariance is:\n$$\nC_{\\mathrm{post}}^{\\mathrm{diag}} = (C_0^{-1} + H^\\top \\Gamma_{\\mathrm{diag}}^{-1} H)^{-1}\n$$\nThe posterior mean is:\n$$\nm_{\\mathrm{post}}^{\\mathrm{diag}}(y) = C_{\\mathrm{post}}^{\\mathrm{diag}} (C_0^{-1} m_0 + H^\\top \\Gamma_{\\mathrm{diag}}^{-1} y)\n$$\n\n**3. Derivation of the Systematic Bias (Task 3)**\n\nThe systematic bias $b$ is defined as the difference between the expected posterior means, where the expectation is taken with respect to the distribution of observations $y$ conditioned on a ground-truth state $x^{\\ast}$.\n$$\nb \\equiv \\mathbb{E}\\big[m_{\\mathrm{post}}^{\\mathrm{diag}}(y) \\,\\big|\\, x^{\\ast} \\big] - \\mathbb{E}\\big[m_{\\mathrm{post}}^{\\mathrm{full}}(y) \\,\\big|\\, x^{\\ast} \\big]\n$$\nThe posterior means are linear functions of the observation $y$. The expectation of $y$ conditioned on $x^{\\ast}$ is given as $\\mathbb{E}[y \\mid x^{\\ast}] = Hx^{\\ast}$. Let's denote this expected observation as $y^{\\ast} = Hx^{\\ast}$. By linearity of expectation:\n$$\n\\mathbb{E}\\big[m_{\\mathrm{post}}(y) \\mid x^{\\ast}\\big] = m_{\\mathrm{post}}(\\mathbb{E}[y \\mid x^{\\ast}]) = m_{\\mathrm{post}}(y^{\\ast})\n$$\nApplying this to our expressions for the posterior means:\n$$\n\\mathbb{E}\\big[m_{\\mathrm{post}}^{\\mathrm{full}}(y) \\mid x^{\\ast}\\big] = C_{\\mathrm{post}}^{\\mathrm{full}} (H^\\top \\Gamma^{-1} y^{\\ast} + C_0^{-1} m_0)\n$$\n$$\n\\mathbb{E}\\big[m_{\\mathrm{post}}^{\\mathrm{diag}}(y) \\mid x^{\\ast}\\big] = C_{\\mathrm{post}}^{\\mathrm{diag}} (H^\\top \\Gamma_{\\mathrm{diag}}^{-1} y^{\\ast} + C_0^{-1} m_0)\n$$\nThe bias vector $b$ is therefore:\n$$\nb = C_{\\mathrm{post}}^{\\mathrm{diag}} (H^\\top \\Gamma_{\\mathrm{diag}}^{-1} Hx^{\\ast} + C_0^{-1} m_0) - C_{\\mathrm{post}}^{\\mathrm{full}} (H^\\top \\Gamma^{-1} Hx^{\\ast} + C_0^{-1} m_0)\n$$\nAn alternative, computationally more stable expression for $b$ can be derived using the Kalman gain formulation. The posterior mean can be written as $m_{\\mathrm{post}}(y) = m_0 + K(y - Hm_0)$, where $K = C_0 H^\\top (H C_0 H^\\top + \\Gamma)^{-1}$ is the Kalman gain. The expected posterior mean becomes $m_0 + K(Hx^{\\ast} - Hm_0)$. The bias is then:\n$$\nb = (K_{\\mathrm{diag}} - K_{\\mathrm{full}})(Hx^{\\ast} - Hm_0)\n$$\nwhere $K_{\\mathrm{full}} = C_0 H^\\top (H C_0 H^\\top + \\Gamma)^{-1}$ and $K_{\\mathrm{diag}} = C_0 H^\\top (H C_0 H^\\top + \\Gamma_{\\mathrm{diag}})^{-1}$. This formulation avoids multiple inversions of $n \\times n$ matrices and is used in the implementation. The final required quantity is the Euclidean norm of this bias vector, $\\|b\\|_2$.\nFor Case A, $\\Gamma$ is already diagonal, so $\\Gamma = \\Gamma_{\\mathrm{diag}}$. This implies $K_{\\mathrm{full}} = K_{\\mathrm{diag}}$, leading to a bias $b=0$, which serves as a successful sanity check for the formulation. For cases B, C, and D, where $\\Gamma \\neq \\Gamma_{\\mathrm{diag}}$, a non-zero bias is expected.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_bias_norm(H, m_0, C_0, x_star, Gamma):\n    \"\"\"\n    Computes the Euclidean norm of the systematic bias vector.\n\n    The bias is induced by ignoring off-diagonal terms in the observation-error\n    covariance matrix Gamma. This function uses the Kalman gain formulation for\n    numerical stability and efficiency.\n\n    Args:\n        H (np.ndarray): The forward operator matrix.\n        m_0 (np.ndarray): The prior mean vector.\n        C_0 (np.ndarray): The prior covariance matrix.\n        x_star (np.ndarray): The ground-truth state vector.\n        Gamma (np.ndarray): The observation-error covariance matrix.\n\n    Returns:\n        float: The Euclidean norm of the bias vector b.\n    \"\"\"\n    # 1. Form the diagonal-only version of the observation-error covariance\n    Gamma_diag = np.diag(np.diag(Gamma))\n\n    # 2. Compute the Kalman gains for both the full and diagonal models\n    # Innovation covariance S = H * C_0 * H^T + Gamma\n    S_full = H @ C_0 @ H.T + Gamma\n    S_diag = H @ C_0 @ H.T + Gamma_diag\n\n    # Kalman gain K = C_0 * H^T * S^-1\n    # We use np.linalg.solve for better numerical stability than np.linalg.inv\n    # K_full = C_0 @ H.T @ np.linalg.inv(S_full)\n    # K_diag = C_0 @ H.T @ np.linalg.inv(S_diag)\n    \n    # Let's solve K = (S^T \\ (H C_0^T))^T = (S^T \\ H C_0)^T because C0 is symmetric\n    # This avoids forming the inverse explicitly.\n    term_to_solve = H @ C_0\n    K_full = np.linalg.solve(S_full.T, term_to_solve).T\n    K_diag = np.linalg.solve(S_diag.T, term_to_solve).T\n\n    # 3. Calculate the expected innovation term based on the ground truth x_star\n    # This is E[y - H*m_0] = H*x_star - H*m_0 = H * (x_star - m_0)\n    innovation_term = H @ (x_star - m_0)\n\n    # 4. Calculate the bias vector b = (K_diag - K_full) * innovation\n    b = (K_diag - K_full) @ innovation_term\n\n    # 5. Return the Euclidean norm of the bias vector\n    return np.linalg.norm(b)\n\ndef solve():\n    \"\"\"\n    Defines the four test cases from the problem statement and computes the\n    bias norm for each, printing the results in the required format.\n    \"\"\"\n    \n    # --- Case A ---\n    H_A = np.array([[1.0, 0.5], [0.2, 1.0]])\n    m0_A = np.array([0.1, -0.2])\n    C0_A = np.array([[1.0, 0.3], [0.3, 1.5]])\n    x_star_A = np.array([0.7, -1.2])\n    Gamma_A = np.array([[0.04, 0.0], [0.0, 0.09]])\n    \n    # --- Case B ---\n    # Parameters are the same as Case A, except for Gamma\n    Gamma_B = np.array([[0.04, 0.048], [0.048, 0.09]])\n    \n    # --- Case C ---\n    # Parameters are the same as Case A, except for Gamma\n    Gamma_C = np.array([[0.04, 0.0594], [0.0594, 0.09]])\n\n    # --- Case D ---\n    H_D = np.array([[1.0, 0.1], [0.5, 1.0], [-0.2, 0.7]])\n    m0_D = np.array([0.0, 0.0])\n    C0_D = np.array([[2.0, 0.4], [0.4, 0.5]])\n    x_star_D = np.array([1.5, -0.5])\n    sigma_D = np.array([0.05, 0.08, 0.06])\n    rho_D = 0.6\n    R_D = np.array([[1.0, rho_D, rho_D**2],\n                    [rho_D, 1.0, rho_D],\n                    [rho_D**2, rho_D, 1.0]])\n    Gamma_D = np.diag(sigma_D) @ R_D @ np.diag(sigma_D)\n\n    test_cases = [\n        (H_A, m0_A, C0_A, x_star_A, Gamma_A),\n        (H_A, m0_A, C0_A, x_star_A, Gamma_B),\n        (H_A, m0_A, C0_A, x_star_A, Gamma_C),\n        (H_D, m0_D, C0_D, x_star_D, Gamma_D)\n    ]\n\n    results = []\n    for case in test_cases:\n        H, m_0, C_0, x_star, Gamma = case\n        bias_norm = compute_bias_norm(H, m_0, C_0, x_star, Gamma)\n        results.append(f\"{bias_norm:.10f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3384513"}, {"introduction": "A key component of the scientific method is model validation. After computing a posterior estimate, it is essential to assess whether the underlying model is consistent with the observed data. This exercise introduces a powerful diagnostic tool, the normalized misfit, whose statistical properties are known under the model assumptions. By comparing the observed misfit of your solution to its expected value [@problem_id:3384530], you will learn to diagnose common modeling failures such as underfitting or overfitting, a critical skill for any practitioner of inverse problems and data assimilation.", "problem": "Consider the linear inverse problem in a data assimilation setting where the goal is to infer an unknown parameter vector $m \\in \\mathbb{R}^p$ from observations $y \\in \\mathbb{R}^n$. Assume a linear observation operator $G \\in \\mathbb{R}^{n \\times p}$, a Gaussian prior $m \\sim \\mathcal{N}(m_{\\text{prior}}, C_{\\text{prior}})$ with mean $m_{\\text{prior}} \\in \\mathbb{R}^p$ and covariance $C_{\\text{prior}} \\in \\mathbb{R}^{p \\times p}$, and a Gaussian likelihood model $y \\mid m \\sim \\mathcal{N}(G m, \\Gamma)$ with observation noise covariance $\\Gamma \\in \\mathbb{R}^{n \\times n}$. All covariance matrices are Symmetric Positive Definite (SPD).\n\nThe normalized misfit is defined as the squared norm of the posterior residual in the observation noise metric, namely\n$$\n\\|y - G m_{\\text{post}}\\|_{\\Gamma^{-1}}^2 = (y - G m_{\\text{post}})^\\top \\Gamma^{-1} (y - G m_{\\text{post}}),\n$$\nwhere $m_{\\text{post}}$ denotes the posterior mean of $m$ under the model assumptions above.\n\nThe expected value of the normalized misfit under the linear Gaussian model provides a reference to assess model fit quality. Comparing the observed normalized misfit to its expected value allows one to flag potential overfitting or underfitting. Your task is to derive the posterior mean $m_{\\text{post}}$ and the expected value of the normalized misfit using accepted identities of multivariate Gaussian distributions and linear operators, starting only from the model assumptions stated above. Then, implement a program that:\n\n1. Computes $m_{\\text{post}}$.\n2. Computes the normalized misfit $\\|y - G m_{\\text{post}}\\|_{\\Gamma^{-1}}^2$.\n3. Computes the expected value of the normalized misfit predicted by the model.\n4. Classifies the fit as:\n   - $-1$ for overfitting (observed misfit significantly below the expected level),\n   - $0$ for adequate fit,\n   - $1$ for underfitting (observed misfit significantly above the expected level).\n   Use a two-standard-deviation band around the expected value as the decision threshold, where the standard deviation is derived under the same linear Gaussian assumptions.\n\nNo physical units are involved in this problem.\n\nUse the following test suite of three cases, each specified by $(G, C_{\\text{prior}}, m_{\\text{prior}}, \\Gamma, y)$:\n\n- Test Case 1 (balanced, happy path):\n  - $n = 4$, $p = 3$,\n  - $G = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ 1  1  1 \\end{bmatrix}$,\n  - $C_{\\text{prior}} = \\operatorname{diag}(1.0, 0.5, 2.0)$,\n  - $m_{\\text{prior}} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\Gamma = \\operatorname{diag}(0.5, 0.5, 0.5, 0.5)$,\n  - $y = \\begin{bmatrix} 1.0 \\\\ -1.0 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$.\n\n- Test Case 2 (boundary, zero innovation leading to potential overfitting flag):\n  - $n = 4$, $p = 3$,\n  - $G = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ 1  1  1 \\end{bmatrix}$,\n  - $C_{\\text{prior}} = \\operatorname{diag}(1.0, 1.0, 1.0)$,\n  - $m_{\\text{prior}} = \\begin{bmatrix} 0.2 \\\\ -0.3 \\\\ 0.1 \\end{bmatrix}$,\n  - $\\Gamma = \\operatorname{diag}(1.0, 1.0, 1.0, 1.0)$,\n  - $y = \\begin{bmatrix} 0.2 \\\\ -0.3 \\\\ 0.1 \\\\ 0.0 \\end{bmatrix}$.\n\n- Test Case 3 (edge case, strong prior and moderate noise leading to potential underfitting flag):\n  - $n = 4$, $p = 3$,\n  - $G = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ 1  1  1 \\end{bmatrix}$,\n  - $C_{\\text{prior}} = \\operatorname{diag}(10^{-3}, 10^{-3}, 10^{-3})$,\n  - $m_{\\text{prior}} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\Gamma = \\operatorname{diag}(0.3, 0.3, 0.3, 0.3)$,\n  - $y = \\begin{bmatrix} 5.0 \\\\ -4.0 \\\\ 3.0 \\\\ 10.0 \\end{bmatrix}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one entry per test case. Each entry must be a triple $[\\text{misfit}, \\text{expected}, \\text{classification}]$, where $\\text{misfit}$ and $\\text{expected}$ are real numbers (floats) and $\\text{classification}$ is an integer in $\\{-1, 0, 1\\}$. For example, the output format must be of the form:\n$$\n[[\\text{misfit}_1,\\text{expected}_1,\\text{classification}_1],[\\text{misfit}_2,\\text{expected}_2,\\text{classification}_2],[\\text{misfit}_3,\\text{expected}_3,\\text{classification}_3]].\n$$", "solution": "### Theoretical Derivation\n\nThe modeling framework is defined by a prior distribution on the parameters $m$ and a likelihood function for the observations $y$ given $m$.\n\n1.  **Prior Model**: The parameter vector $m \\in \\mathbb{R}^p$ is assumed to follow a Gaussian distribution, $m \\sim \\mathcal{N}(m_{\\text{prior}}, C_{\\text{prior}})$. The probability density function (PDF) is given by:\n    $$\n    p(m) \\propto \\exp\\left(-\\frac{1}{2} (m - m_{\\text{prior}})^\\top C_{\\text{prior}}^{-1} (m - m_{\\text{prior}})\\right)\n    $$\n    where $m_{\\text{prior}} \\in \\mathbb{R}^p$ is the prior mean and $C_{\\text{prior}} \\in \\mathbb{R}^{p \\times p}$ is the symmetric positive definite (SPD) prior covariance matrix.\n\n2.  **Likelihood Model**: The observations $y \\in \\mathbb{R}^n$ are related to the parameters $m$ through a linear operator $G \\in \\mathbb{R}^{n \\times p}$ and are corrupted by additive Gaussian noise. The conditional distribution of $y$ given $m$ is $y \\mid m \\sim \\mathcal{N}(G m, \\Gamma)$. The likelihood function is:\n    $$\n    p(y \\mid m) \\propto \\exp\\left(-\\frac{1}{2} (y - G m)^\\top \\Gamma^{-1} (y - G m)\\right)\n    $$\n    where $\\Gamma \\in \\mathbb{R}^{n \\times n}$ is the SPD observation noise covariance matrix.\n\n#### 1. Posterior Distribution and Posterior Mean\n\nAccording to Bayes' theorem, the posterior distribution of $m$ given $y$ is proportional to the product of the likelihood and the prior, $p(m \\mid y) \\propto p(y \\mid m) p(m)$.\nThe posterior PDF is therefore proportional to the exponential of a quadratic form in $m$, which implies that the posterior is also Gaussian, $m \\mid y \\sim \\mathcal{N}(m_{\\text{post}}, C_{\\text{post}})$. The negative logarithm of the posterior PDF is (up to an additive constant):\n$$\nJ(m) = \\frac{1}{2} (m - m_{\\text{prior}})^\\top C_{\\text{prior}}^{-1} (m - m_{\\text{prior}}) + \\frac{1}{2} (y - G m)^\\top \\Gamma^{-1} (y - G m)\n$$\nThe posterior mean, $m_{\\text{post}}$, is the value of $m$ that maximizes the posterior PDF, which is equivalent to minimizing the quadratic cost function $J(m)$. We find this minimum by setting the gradient of $J(m)$ with respect to $m$ to zero:\n$$\n\\nabla_m J(m) = C_{\\text{prior}}^{-1} (m - m_{\\text{prior}}) - G^\\top \\Gamma^{-1} (y - G m) = 0\n$$\n$$\nC_{\\text{prior}}^{-1} m - C_{\\text{prior}}^{-1} m_{\\text{prior}} - G^\\top \\Gamma^{-1} y + G^\\top \\Gamma^{-1} G m = 0\n$$\nRearranging the terms to solve for $m$:\n$$\n(C_{\\text{prior}}^{-1} + G^\\top \\Gamma^{-1} G) m = C_{\\text{prior}}^{-1} m_{\\text{prior}} + G^\\top \\Gamma^{-1} y\n$$\nThe term $(C_{\\text{prior}}^{-1} + G^\\top \\Gamma^{-1} G)$ is the Hessian of $J(m)$, which is the inverse of the posterior covariance matrix, $C_{\\text{post}}^{-1}$. Since $C_{\\text{prior}}$ and $\\Gamma$ are SPD, $C_{\\text{post}}^{-1}$ is also SPD and thus invertible.\n$$\nC_{\\text{post}} = (C_{\\text{prior}}^{-1} + G^\\top \\Gamma^{-1} G)^{-1}\n$$\nThe posterior mean $m_{\\text{post}}$ is then:\n$$\nm_{\\text{post}} = C_{\\text{post}} (C_{\\text{prior}}^{-1} m_{\\text{prior}} + G^\\top \\Gamma^{-1} y)\n$$\nThis expression is computationally efficient when the parameter dimension $p$ is small, as it requires the inversion of a $p \\times p$ matrix.\n\n#### 2. Expected Value of the Normalized Misfit\n\nThe normalized misfit is defined as $\\chi^2 = \\|y - G m_{\\text{post}}\\|_{\\Gamma^{-1}}^2$. We seek its expected value, $\\mathbb{E}[\\chi^2]$, under the prior predictive distribution of the data. The data $y$ is a random variable whose distribution is obtained by marginalizing the joint distribution $p(y, m) = p(y|m)p(m)$.\nThe mean of $y$ is $\\mathbb{E}[y] = \\mathbb{E}[\\mathbb{E}[y|m]] = \\mathbb{E}[Gm] = G m_{\\text{prior}}$.\nThe covariance of $y$ is $\\text{Cov}(y) = \\mathbb{E}[\\text{Cov}(y|m)] + \\text{Cov}(\\mathbb{E}[y|m]) = \\Gamma + G C_{\\text{prior}} G^\\top$.\nLet us define the innovation vector $d = y - G m_{\\text{prior}}$. Under the model, $d$ is a zero-mean Gaussian random variable, $d \\sim \\mathcal{N}(0, S)$, where $S = \\Gamma + G C_{\\text{prior}} G^\\top$ is the innovation covariance matrix.\n\nWe can express the posterior mean using the Kalman gain formulation: $m_{\\text{post}} = m_{\\text{prior}} + K d$, where $K = C_{\\text{prior}} G^\\top S^{-1}$ is the Kalman gain.\nThe posterior residual is $r = y - G m_{\\text{post}} = (d + G m_{\\text{prior}}) - G(m_{\\text{prior}} + K d) = (I_n - GK)d$, where $I_n$ is the $n \\times n$ identity matrix.\nThe misfit becomes a quadratic form in $d$:\n$$\n\\chi^2 = r^\\top \\Gamma^{-1} r = d^\\top (I_n - GK)^\\top \\Gamma^{-1} (I_n - GK) d\n$$\nThe expected value of a quadratic form $x^\\top A x$ for a random vector $x$ with mean $\\mu_x$ and covariance $\\Sigma_x$ is $\\mathbb{E}[x^\\top A x] = \\text{Tr}(A \\Sigma_x) + \\mu_x^\\top A \\mu_x$. Since $\\mu_d = 0$, we have:\n$$\n\\mathbb{E}[\\chi^2] = \\text{Tr}\\left( (I_n - GK)^\\top \\Gamma^{-1} (I_n - GK) S \\right)\n$$\nUsing the identity $(I_n - GK)S = \\Gamma$ (which can be verified by substituting the expressions for $K$ and $S$), the expression simplifies:\n$$\n\\mathbb{E}[\\chi^2] = \\text{Tr}\\left( (I_n - GK)^\\top \\Gamma^{-1} \\Gamma \\right) = \\text{Tr}\\left( (I_n - GK)^\\top \\right) = \\text{Tr}(I_n - GK)\n$$\n$$\n\\mathbb{E}[\\chi^2] = n - \\text{Tr}(GK)\n$$\nFor computational purposes, we can express $GK$ using $C_{\\text{post}}$:\n$K = C_{\\text{post}} G^\\top \\Gamma^{-1}$, which gives $GK = G C_{\\text{post}} G^\\top \\Gamma^{-1}$.\nThus, the expected misfit is:\n$$\n\\mathbb{E}[\\chi^2] = n - \\text{Tr}(G C_{\\text{post}} G^\\top \\Gamma^{-1})\n$$\n\n#### 3. Standard Deviation of the Normalized Misfit\n\nTo establish the classification threshold, we need the variance of the misfit, $\\text{Var}(\\chi^2)$. For a zero-mean Gaussian vector $d \\sim \\mathcal{N}(0, S)$, the variance of the quadratic form $\\chi^2 = d^\\top A d$ where $A$ is a symmetric matrix is $\\text{Var}(\\chi^2) = 2 \\text{Tr}((AS)^2)$.\nThe matrix of our quadratic form is $A_0 = (I_n - GK)^\\top \\Gamma^{-1} (I_n - GK)$. Symmetrizing it yields $A = (A_0 + A_0^\\top)/2$.\nAs shown before, $(I_n - GK)S = \\Gamma$, so $(I_n-GK) = \\Gamma S^{-1}$.\n$A_0 = (\\Gamma S^{-1})^\\top \\Gamma^{-1} (\\Gamma S^{-1}) = (S^{-1})^\\top \\Gamma^\\top \\Gamma^{-1} \\Gamma S^{-1} = S^{-1} \\Gamma S^{-1}$ (since $\\Gamma$ and $S$ are symmetric). This matrix $A$ is already symmetric.\nTherefore, $A = S^{-1}\\Gamma S^{-1}$.\nThe term $AS$ becomes $AS = (S^{-1} \\Gamma S^{-1}) S = S^{-1} \\Gamma$.\nThe variance is then:\n$$\n\\text{Var}(\\chi^2) = 2 \\text{Tr}((S^{-1} \\Gamma)^2)\n$$\nFor computation, we express $S^{-1}\\Gamma$ in terms of $C_{\\text{post}}$. Using the Woodbury matrix identity, $S^{-1} = (\\Gamma + G C_{\\text{prior}} G^\\top)^{-1} = \\Gamma^{-1} - \\Gamma^{-1} G C_{\\text{post}} G^\\top \\Gamma^{-1}$.\nThen, $S^{-1}\\Gamma = (\\Gamma^{-1} - \\Gamma^{-1} G C_{\\text{post}} G^\\top \\Gamma^{-1})\\Gamma = I_n - \\Gamma^{-1} G C_{\\text{post}} G^\\top$.\nLet $M = I_n - \\Gamma^{-1} G C_{\\text{post}} G^\\top$. The variance is:\n$$\n\\text{Var}(\\chi^2) = 2 \\text{Tr}(M^2)\n$$\nThe standard deviation is $\\sigma_{\\chi^2} = \\sqrt{\\text{Var}(\\chi^2)}$.\n\n#### 4. Classification Criterion\n\nThe classification of the model fit is based on comparing the observed misfit $\\chi^2_{\\text{obs}}$ with a confidence interval around its expected value:\n- Overfitting ($-1$): $\\chi^2_{\\text{obs}}  \\mathbb{E}[\\chi^2] - 2\\sigma_{\\chi^2}$\n- Underfitting ($1$): $\\chi^2_{\\text{obs}}  \\mathbb{E}[\\chi^2] + 2\\sigma_{\\chi^2}$\n- Adequate fit ($0$): Otherwise.\n\nThis completes the theoretical framework needed to solve the problem.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the linear inverse problem for three test cases, computing\n    the posterior mean, normalized misfit, its expected value, and a\n    fit classification.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1: balanced, happy path\n        {\n            \"G\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]),\n            \"C_prior\": np.diag([1.0, 0.5, 2.0]),\n            \"m_prior\": np.array([0.0, 0.0, 0.0]),\n            \"Gamma\": np.diag([0.5, 0.5, 0.5, 0.5]),\n            \"y\": np.array([1.0, -1.0, 0.5, 0.5]),\n        },\n        # Test Case 2: boundary, zero innovation leading to potential overfitting flag\n        {\n            \"G\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]),\n            \"C_prior\": np.diag([1.0, 1.0, 1.0]),\n            \"m_prior\": np.array([0.2, -0.3, 0.1]),\n            \"Gamma\": np.diag([1.0, 1.0, 1.0, 1.0]),\n            \"y\": np.array([0.2, -0.3, 0.1, 0.0]),\n        },\n        # Test Case 3: edge case, strong prior and moderate noise - underfitting\n        {\n            \"G\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]),\n            \"C_prior\": np.diag([1e-3, 1e-3, 1e-3]),\n            \"m_prior\": np.array([0.0, 0.0, 0.0]),\n            \"Gamma\": np.diag([0.3, 0.3, 0.3, 0.3]),\n            \"y\": np.array([5.0, -4.0, 3.0, 10.0]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        G = case[\"G\"]\n        C_prior = case[\"C_prior\"]\n        m_prior = case[\"m_prior\"]\n        Gamma = case[\"Gamma\"]\n        y = case[\"y\"]\n\n        n, p = G.shape\n\n        # Matrix inversions. For diagonal matrices, this is trivial, but\n        # the general formulas are implemented for correctness.\n        try:\n            C_prior_inv = np.linalg.inv(C_prior)\n            Gamma_inv = np.linalg.inv(Gamma)\n        except np.linalg.LinAlgError:\n            # Handle singular matrices if they occurred, though not expected here.\n            results.append([float('nan'), float('nan'), 0])\n            continue\n\n        # 1. Compute posterior covariance C_post\n        # C_post_inv = C_prior_inv + G^T Gamma_inv G\n        # This involves a p x p inversion, which is efficient for p  n.\n        C_post_inv = C_prior_inv + G.T @ Gamma_inv @ G\n        C_post = np.linalg.inv(C_post_inv)\n\n        # 2. Compute posterior mean m_post\n        # m_post = C_post (C_prior_inv m_prior + G^T Gamma_inv y)\n        m_post = C_post @ (C_prior_inv @ m_prior + G.T @ Gamma_inv @ y)\n\n        # 3. Compute the observed normalized misfit\n        # misfit = (y - G m_post)^T Gamma_inv (y - G m_post)\n        residual = y - G @ m_post\n        misfit = residual.T @ Gamma_inv @ residual\n        \n        # 4. Compute the expected value of the misfit\n        # E[misfit] = n - Tr(G C_post G^T Gamma_inv)\n        trace_matrix_exp = G @ C_post @ G.T @ Gamma_inv\n        expected_misfit = n - np.trace(trace_matrix_exp)\n\n        # 5. Compute the standard deviation of the misfit\n        # Var[misfit] = 2 * Tr((I - Gamma_inv G C_post G^T)^2)\n        M = np.identity(n) - Gamma_inv @ G @ C_post @ G.T\n        var_misfit = 2 * np.trace(M @ M)\n        std_misfit = np.sqrt(var_misfit)\n\n        # 6. Classify the fit\n        lower_bound = expected_misfit - 2 * std_misfit\n        upper_bound = expected_misfit + 2 * std_misfit\n\n        classification = 0\n        if misfit  lower_bound:\n            classification = -1  # Overfitting\n        elif misfit  upper_bound:\n            classification = 1   # Underfitting\n\n        results.append([misfit, expected_misfit, classification])\n    \n    # Format the final output string as per the problem specification\n    # [[misfit_1,expected_1,classification_1],[misfit_2,expected_2,classification_2],...]\n    list_of_strings = []\n    for res in results:\n        list_of_strings.append(f\"[{res[0]},{res[1]},{res[2]}]\")\n    final_output = f\"[{','.join(list_of_strings)}]\"\n    \n    print(final_output)\n\n\nsolve()\n\n```", "id": "3384530"}]}