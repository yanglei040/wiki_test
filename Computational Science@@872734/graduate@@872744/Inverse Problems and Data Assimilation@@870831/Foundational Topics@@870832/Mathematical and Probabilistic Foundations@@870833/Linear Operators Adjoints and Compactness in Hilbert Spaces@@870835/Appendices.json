{"hands_on_practices": [{"introduction": "A cornerstone of functional analysis is the concept of the adjoint operator, which generalizes the notion of a conjugate transpose of a matrix to infinite-dimensional spaces. This exercise provides fundamental practice in deriving the adjoint of a second-order differential operator, a common operator in models of diffusion and potential fields. By applying integration by parts, you will uncover not only the form of the adjoint but also the crucial role that boundary conditions play in determining an operator's properties, such as self-adjointness [@problem_id:3398466].", "problem": "Let $H$ denote the Hilbert space $L^{2}(0,1)$ with the standard inner product $\\langle u, v \\rangle_{L^{2}}=\\int_{0}^{1}u(x)\\,v(x)\\,dx$. Let $a:(0,1)\\to\\mathbb{R}$ be a function such that $a\\in C^{1}([0,1])$ and $a(x)\\geq a_{0}>0$ for all $x\\in[0,1]$. Consider the differential operator $T$ defined by\n$$\n(Tu)(x)\\;=\\;-\\frac{d}{dx}\\Big(a(x)\\,\\frac{du}{dx}(x)\\Big),\n$$\nwith domain\n$$\nD(T)\\;=\\;H^{1}_{0}(0,1)\\cap H^{2}(0,1).\n$$\nUsing only the definition of the adjoint in a Hilbert space and integration by parts as the fundamental tool, determine the adjoint operator $T^{\\ast}$ of $T$ with respect to the $L^{2}(0,1)$ inner product. In particular, determine the boundary terms that arise and identify the trace conditions on the adjoint variable that ensure the boundary terms vanish for all $u\\in D(T)$, thereby yielding self-adjointness of $T$. Provide your final answer as the analytic expression for $T^{\\ast}v$ acting on a suitable $v$ in its domain, expressed in terms of $a$ and derivatives of $v$. No numerical evaluation is required.", "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in functional analysis, specifically regarding the theory of linear operators on Hilbert spaces. All terms are well-defined, and the necessary information is provided.\n\nWe are tasked with finding the adjoint operator $T^{\\ast}$ of the operator\n$$\n(Tu)(x) = -\\frac{d}{dx}\\Big(a(x)\\,\\frac{du}{dx}(x)\\Big)\n$$\ndefined on the domain $D(T) = H^{1}_{0}(0,1)\\cap H^{2}(0,1)$. The Hilbert space is $H = L^{2}(0,1)$ with the inner product $\\langle u, v \\rangle_{L^{2}} = \\int_{0}^{1}u(x)v(x)dx$.\n\nThe adjoint operator $T^{\\ast}$ and its domain $D(T^{\\ast})$ are defined by the relation\n$$\n\\langle Tu,v \\rangle_{L^2} = \\langle u, T^{\\ast}v \\rangle_{L^2}\n$$\nfor all $u \\in D(T)$ and all $v \\in D(T^{\\ast})$. The domain $D(T^{\\ast})$ is the set of all $v \\in H$ for which there exists a $w \\in H$ such that $\\langle Tu,v \\rangle_{L^2} = \\langle u, w \\rangle_{L^2}$ for all $u \\in D(T)$; if such a $w$ exists, it is unique, and we define $T^{\\ast}v = w$.\n\nWe start by computing the left-hand side of the defining relation for an arbitrary $u \\in D(T)$ and a sufficiently smooth function $v$.\n$$\n\\langle Tu,v \\rangle_{L^2} = \\int_{0}^{1} (Tu)(x) v(x) dx = \\int_{0}^{1} \\left( -\\frac{d}{dx}\\Big(a(x)\\,\\frac{du}{dx}(x)\\Big) \\right) v(x) dx\n$$\nWe use integration by parts to move the derivative from the term in parentheses onto $v(x)$. Let $F(x) = -a(x)\\frac{du}{dx}(x)$ and $G(x) = v(x)$. The integration by parts formula is $\\int_{0}^{1} F'(x)G(x)dx = [F(x)G(x)]_{0}^{1} - \\int_{0}^{1}F(x)G'(x)dx$. This requires $F$ and $G$ to be sufficiently regular. Since $u \\in H^2(0,1)$ and $a \\in C^1([0,1])$, the term $a(x)\\frac{du}{dx}(x)$ is in $H^1(0,1)$, and its derivative is in $L^2(0,1)$. For $v$ to be in the domain of the adjoint, we can anticipate it will require some regularity, so we assume $v \\in H^1(0,1)$ for this step.\n\nApplying integration by parts:\n$$\n\\langle Tu,v \\rangle_{L^2} = \\left[ -a(x)\\frac{du}{dx}(x)v(x) \\right]_{0}^{1} - \\int_{0}^{1} \\left( -a(x)\\frac{du}{dx}(x) \\right) \\frac{dv}{dx}(x) dx\n$$\n$$\n\\langle Tu,v \\rangle_{L^2} = \\left[ -a(x)\\frac{du}{dx}(x)v(x) \\right]_{0}^{1} + \\int_{0}^{1} a(x)\\frac{du}{dx}(x)\\frac{dv}{dx}(x) dx\n$$\nNow, we apply integration by parts to the remaining integral term to move the derivative from $u$ to $v$. Let $F(x) = u(x)$ and $G(x) = a(x)\\frac{dv}{dx}(x)$. For this to be well-defined, we need $a(x)\\frac{dv}{dx}(x)$ to be in $H^1(0,1)$, which suggests $v$ should be in $H^2(0,1)$.\n$$\n\\int_{0}^{1} \\Big(a(x)\\frac{dv}{dx}(x)\\Big) \\frac{du}{dx}(x) dx = \\left[ u(x) a(x) \\frac{dv}{dx}(x) \\right]_{0}^{1} - \\int_{0}^{1} u(x) \\frac{d}{dx}\\Big( a(x)\\frac{dv}{dx}(x) \\Big) dx\n$$\nSubstituting this back into the expression for $\\langle Tu,v \\rangle_{L^2}$:\n$$\n\\langle Tu,v \\rangle_{L^2} = \\left[ -a(x)\\frac{du}{dx}(x)v(x) \\right]_{0}^{1} + \\left[ u(x) a(x) \\frac{dv}{dx}(x) \\right]_{0}^{1} - \\int_{0}^{1} u(x) \\frac{d}{dx}\\Big( a(x)\\frac{dv}{dx}(x) \\Big) dx\n$$\nWe can rearrange this to match the form $\\langle u, T^{\\ast}v \\rangle_{L^2}$:\n$$\n\\langle Tu,v \\rangle_{L^2} = \\int_{0}^{1} u(x) \\left( -\\frac{d}{dx}\\Big( a(x)\\frac{dv}{dx}(x) \\Big) \\right) dx + \\left[ a(x)u(x)\\frac{dv}{dx}(x) - a(x)v(x)\\frac{du}{dx}(x) \\right]_{0}^{1}\n$$\nThe integral term suggests that the formal adjoint operator is given by $(T^{\\ast}v)(x) = -\\frac{d}{dx}( a(x)\\frac{dv}{dx}(x) )$, which has the same form as $T$. For the equality $\\langle Tu, v \\rangle = \\langle u, T^{\\ast}v \\rangle$ to hold, the boundary terms must vanish for every $u \\in D(T)$. Let's denote the boundary term by $B(u,v)$:\n$$\nB(u,v) = \\left[ a(x)u(x)\\frac{dv}{dx}(x) - a(x)v(x)\\frac{du}{dx}(x) \\right]_{x=0}^{x=1}\n$$\n$$\nB(u,v) = \\left( a(1)u(1)\\frac{dv}{dx}(1) - a(1)v(1)\\frac{du}{dx}(1) \\right) - \\left( a(0)u(0)\\frac{dv}{dx}(0) - a(0)v(0)\\frac{du}{dx}(0) \\right)\n$$\nThe domain of $T$ is $D(T) = H^{1}_{0}(0,1) \\cap H^{2}(0,1)$. A key property of the space $H^{1}_{0}(0,1)$ is that for any function $u \\in H^{1}_{0}(0,1)$, the trace operator is zero at the boundaries, meaning $u(0)=0$ and $u(1)=0$.\nSubstituting these conditions for $u$ into the boundary term $B(u,v)$:\n$$\nB(u,v) = \\left( a(1) \\cdot 0 \\cdot \\frac{dv}{dx}(1) - a(1)v(1)\\frac{du}{dx}(1) \\right) - \\left( a(0) \\cdot 0 \\cdot \\frac{dv}{dx}(0) - a(0)v(0)\\frac{du}{dx}(0) \\right)\n$$\n$$\nB(u,v) = -a(1)v(1)\\frac{du}{dx}(1) + a(0)v(0)\\frac{du}{dx}(0)\n$$\nFor $v$ to be in the domain of the adjoint, $D(T^{\\ast})$, this boundary term $B(u,v)$ must be zero for all $u \\in D(T)$. The space $D(T) = H^{1}_{0}(0,1) \\cap H^{2}(0,1)$ is rich enough that we can find functions $u_1, u_2 \\in D(T)$ such that $\\frac{du_1}{dx}(0) \\neq 0$ while $\\frac{du_2}{dx}(1) = 0$, and vice versa. For the expression $a(0)v(0)\\frac{du}{dx}(0) - a(1)v(1)\\frac{du}{dx}(1)$ to be zero for all choices of $\\frac{du}{dx}(0)$ and $\\frac{du}{dx}(1)$ that are possible for functions in $D(T)$, the coefficients of these derivative terms must be zero independently.\nThis leads to the following conditions on $v$:\n$$\na(0)v(0) = 0 \\quad \\text{and} \\quad -a(1)v(1) = 0\n$$\nThe problem states that $a(x) \\geq a_0 > 0$ for all $x \\in [0,1]$. This implies that $a(0) > 0$ and $a(1) > 0$. Therefore, the conditions on $v$ become:\n$$\nv(0) = 0 \\quad \\text{and} \\quad v(1) = 0\n$$\nThese are the trace conditions on the adjoint variable $v$ that ensure the boundary terms vanish. With these conditions, the identity $\\langle Tu, v \\rangle = \\langle u, T^{\\ast}v \\rangle$ is satisfied, and we can identify the expression for $T^{\\ast}v$.\nThe expression for the adjoint operator is:\n$$\n(T^{\\ast}v)(x) = -\\frac{d}{dx}\\left(a(x)\\frac{dv}{dx}(x)\\right)\n$$\nThe domain $D(T^{\\ast})$ consists of functions $v \\in L^2(0,1)$ such that the expression for $T^{\\ast}v$ is in $L^2(0,1)$ and the boundary conditions $v(0)=0, v(1)=0$ are met. The condition that both $v$ and $-\\frac{d}{dx}(a(x)\\frac{dv}{dx}(x))$ are in $L^2(0,1)$, combined with the fact that $a \\in C^1$ and $a(x)>0$, implies by elliptic regularity theory that $v \\in H^2(0,1)$. The boundary conditions $v(0)=0$ and $v(1)=0$ mean that $v \\in H^1_0(0,1)$. Therefore, the domain of the adjoint is $D(T^{\\ast}) = H^{1}_{0}(0,1) \\cap H^{2}(0,1)$.\nSince the analytical expression for $T^{\\ast}$ is identical to that of $T$, and their domains are identical ($D(T^{\\ast}) = D(T)$), the operator $T$ is self-adjoint.\n\nThe question asks for the analytic expression for $T^{\\ast}v$. As derived, this is identical in form to $Tu$.", "answer": "$$\n\\boxed{-\\frac{d}{dx}\\left(a(x)\\frac{dv}{dx}(x)\\right)}\n$$", "id": "3398466"}, {"introduction": "The property of compactness has profound consequences for the solvability of linear operator equations, as formalized by the Fredholm alternative. This practice moves from abstract theory to a concrete application, using a simple rank-one integral operator that models an averaging process. You will verify its compactness, determine its adjoint, and use the null spaces of both the operator and its adjoint to derive a precise condition for when the equation $(I - K) x = y$ has a solution, illustrating a key principle in the analysis of inverse problems [@problem_id:3398478].", "problem": "Let $H = L^{2}(0,1)$ be the real Hilbert space of square-integrable functions on the interval $(0,1)$ with the inner product defined by $\\langle f,g \\rangle = \\int_{0}^{1} f(s) g(s) \\, ds$. Consider the linear operator $K : H \\to H$ defined by the rank-one integral operator\n$$\n(K x)(s) = \\int_{0}^{1} x(t) \\, dt,\n$$\nwhich maps any $x \\in H$ to the constant function equal to its mean. In the context of inverse problems and data assimilation, such an operator models a spatial averaging effect that introduces a low-rank bias component into the observation model. Starting only from the core definitions of compact operators, adjoints in Hilbert spaces, and the foundational Fredholm alternative for compact perturbations of the identity, perform the following tasks:\n\n- Verify that $K$ is a compact operator on $H$ and identify its adjoint $K^{*}$.\n- Show that the identity-minus-operator $I - K$ fails to be invertible by explicitly identifying a nonzero $x \\in H$ such that $(I - K) x = 0$.\n- Derive the precise solvability condition for the linear equation $(I - K) x = y$ in terms of the adjoint operator and the geometry of the Hilbert space $H$.\n- In a data assimilation setting, interpret the solvability condition as a consistency constraint on the assimilated datum $y \\in H$. For the specific datum $y(s) = 2 s - 1$, compute the scalar functional $S(y)$ that appears in the solvability condition for $(I - K) x = y$.\n\nYour final answer must be the value of $S(y)$, expressed as a single real number. No rounding is required.", "solution": "The problem is validated as being self-contained, mathematically sound, and well-posed. We proceed with a step-by-step solution.\n\nThe Hilbert space is $H = L^{2}(0,1)$ with the inner product $\\langle f,g \\rangle = \\int_{0}^{1} f(s) g(s) \\, ds$. The linear operator $K : H \\to H$ is given by $(K x)(s) = \\int_{0}^{1} x(t) \\, dt$.\n\nFirst, we verify that $K$ is a compact operator and identify its adjoint $K^{*}$.\nLet $u(s) = 1$ for all $s \\in (0,1)$. The function $u$ is an element of $H$ since $\\int_{0}^{1} u(s)^2 \\, ds = \\int_{0}^{1} 1^2 \\, ds = 1 < \\infty$. We can write the operator $K$ using this function and the inner product. For any $x \\in H$, we have:\n$$\n(K x)(s) = \\int_{0}^{1} x(t) \\cdot 1 \\, dt = \\int_{0}^{1} x(t) u(t) \\, dt = \\langle x, u \\rangle.\n$$\nThus, the action of $K$ on $x$ produces the constant function whose value is the scalar $\\langle x, u \\rangle$. We can write this as $(K x)(s) = \\langle x, u \\rangle u(s)$. This shows that the range of $K$, $\\text{Ran}(K)$, is the one-dimensional subspace of $H$ spanned by the function $u$. An operator whose range is finite-dimensional is called a finite-rank operator. Every finite-rank operator on a Hilbert space is a compact operator. Therefore, $K$ is a compact operator.\n\nTo find the adjoint operator $K^{*}$, we use the defining relation $\\langle Kx, y \\rangle = \\langle x, K^{*}y \\rangle$ for all $x, y \\in H$.\nLet's compute the left-hand side:\n$$\n\\langle Kx, y \\rangle = \\int_{0}^{1} (Kx)(s) y(s) \\, ds = \\int_{0}^{1} \\left( \\int_{0}^{1} x(t) \\, dt \\right) y(s) \\, ds.\n$$\nSince the integral with respect to $dt$ is a scalar constant with respect to $s$, we can move it outside the integral with respect to $ds$:\n$$\n\\langle Kx, y \\rangle = \\left( \\int_{0}^{1} x(t) \\, dt \\right) \\left( \\int_{0}^{1} y(s) \\, ds \\right).\n$$\nUsing the inner product notation with the function $u(s)=1$, this is $\\langle x,u \\rangle \\langle y,u \\rangle$.\nThe right-hand side of the adjoint definition is $\\langle x, K^{*}y \\rangle$. We require $\\langle x, K^{*}y \\rangle = \\langle x,u \\rangle \\langle y,u \\rangle$ for all $x \\in H$.\nWe can write this as $\\langle x, K^{*}y \\rangle = \\langle x, u \\rangle \\langle u, y \\rangle = \\langle x, \\langle u, y \\rangle u \\rangle$.\nThis equality must hold for all $x \\in H$, which implies that $K^{*}y = \\langle u, y \\rangle u = \\langle y, u \\rangle u$.\nIn integral form, this is $(K^{*}y)(s) = \\left( \\int_{0}^{1} y(t) \\, dt \\right) \\cdot 1$. This is the same definition as the operator $K$. Thus, $K^{*} = K$, and the operator is self-adjoint.\n\nSecond, we show that the operator $I - K$ is not invertible. An operator is invertible if and only if it is both injective and surjective. We will show it is not injective by finding a non-zero element in its null space (or kernel). Let $x \\in \\ker(I - K)$. By definition, $(I - K)x = 0$, which implies $x - Kx = 0$, or $x = Kx$.\nWriting this out, we have:\n$$\nx(s) = (Kx)(s) = \\int_{0}^{1} x(t) \\, dt.\n$$\nThe right-hand side is a constant, independent of $s$. Let this constant be $c$. Then, the equation implies that any solution must be a constant function, $x(s) = c$. We substitute this form back into the equation to find the value of $c$:\n$$\nc = \\int_{0}^{1} c \\, dt = c \\int_{0}^{1} 1 \\, dt = c \\cdot 1 = c.\n$$\nThis equation, $c=c$, is satisfied for any constant $c \\in \\mathbb{R}$. To find a non-zero element of the null space, we can choose any non-zero value for $c$, for example $c=1$. The function $x(s) = 1$ is a non-zero element of $H$ and satisfies $(I-K)x = 0$. Since the null space $\\ker(I-K)$ is non-trivial (it contains all constant functions), the operator $I-K$ is not injective and therefore not invertible. This also shows that $\\lambda=1$ is an eigenvalue of $K$.\n\nThird, we derive the solvability condition for the equation $(I - K)x = y$.\nAccording to the Fredholm Alternative theorem for compact operators, the equation $(I - K)x = y$ has a solution for $x \\in H$ if and only if the vector $y$ is orthogonal to the null space of the adjoint operator, $(I - K)^{*}$.\nThe adjoint of $I-K$ is $(I - K)^{*} = I^{*} - K^{*}$. Since the identity operator $I$ is self-adjoint ($I^{*}=I$) and we have shown $K$ is self-adjoint ($K^{*}=K$), we have $(I - K)^{*} = I - K$.\nTherefore, the solvability condition is that $y$ must be orthogonal to the null space of $I-K$, i.e., $y \\in [\\ker(I-K)]^{\\perp}$.\nWe have already determined that $\\ker(I-K)$ is the one-dimensional subspace of all constant functions. This subspace is spanned by the function $u(s) = 1$.\nThe orthogonality condition is therefore that $y$ must be orthogonal to every function in $\\ker(I-K)$. It is sufficient to require that $y$ is orthogonal to the basis vector $u(s) = 1$:\n$$\n\\langle y, u \\rangle = 0.\n$$\nIn integral form, the precise solvability condition is:\n$$\n\\int_{0}^{1} y(s) \\cdot 1 \\, ds = 0.\n$$\n\nFourth, we interpret this condition and compute the specified scalar functional. In a data assimilation context, the operator $K$ introduces a constant bias, equal to the mean value. The equation $(I-K)x=y$ asks for a state $x$ whose deviation from its mean value equals the datum $y$. The solvability condition $\\int_0^1 y(s) ds = 0$ signifies that the datum $y$ must have a zero mean. This is a consistency constraint: the datum $y$ cannot itself contain a constant component, as this component lives in the null space of the forward model relating the state deviation to the observation.\n\nThe problem asks to compute the scalar functional $S(y)$ that appears in the solvability condition for the specific datum $y(s) = 2s - 1$. The functional from the condition is $S(y) = \\int_{0}^{1} y(s) \\, ds$. We compute its value for the given $y$:\n$$\nS(y) = \\int_{0}^{1} (2s - 1) \\, ds.\n$$\nThe integration is straightforward:\n$$\nS(y) = \\left[ s^2 - s \\right]_{0}^{1} = (1^2 - 1) - (0^2 - 0) = (1 - 1) - 0 = 0.\n$$\nThe value of the scalar functional for the given datum is $0$.", "answer": "$$\\boxed{0}$$", "id": "3398478"}, {"introduction": "To truly understand compactness, it is instructive to examine its signature characteristics in a clear, discrete setting. This problem explores one of the most canonical examples of a compact operator: a diagonal operator on the Hilbert space of square-summable sequences, $\\ell^2$. By computing its operator norm and analyzing its behavior on the standard orthonormal basis, you will see directly how compactness is related to the decay of the operator's \"action\" at high frequencies and its approximability by finite-rank operators [@problem_id:3398465].", "problem": "Consider the Hilbert space $\\ell^{2}$ of square-summable real sequences equipped with the standard inner product, and the linear operator $T:\\ell^{2}\\to\\ell^{2}$ defined by $(Tx)_{n}=\\frac{x_{n}}{n}$ for every $x=\\{x_{n}\\}_{n\\in\\mathbb{N}}\\in\\ell^{2}$ and every $n\\in\\mathbb{N}$. In the context of inverse problems and data assimilation, such diagonal damping operators arise when weighting high-frequency modes to improve stability of reconstructions. Starting only from core definitions in Hilbert space theory—specifically, the definition of operator norm $\\|T\\|=\\sup_{x\\neq 0}\\frac{\\|Tx\\|}{\\|x\\|}$ and the definition of compactness (an operator is compact if it maps bounded sets into relatively compact sets)—compute the operator norm of $T$. Then, use first principles to determine whether $T$ is compact by analyzing its action on bounded sets and by constructing finite-rank approximations in operator norm from core definitions.\n\nReport your final results as a row matrix whose first entry is the value of $\\|T\\|$, and whose second entry is the value of $\\lim_{n\\to\\infty}\\|Te_{n}\\|$, where $\\{e_{n}\\}_{n\\in\\mathbb{N}}$ is the canonical orthonormal basis of $\\ell^{2}$ defined by $(e_{n})_{k}=\\delta_{nk}$. No numerical rounding is required.", "solution": "We begin from the core definitions in Hilbert space theory. The operator norm of a bounded linear operator $T:\\ell^{2}\\to\\ell^{2}$ is defined by\n$$\n\\|T\\|=\\sup_{x\\in\\ell^{2},\\,x\\neq 0}\\frac{\\|Tx\\|_{\\ell^{2}}}{\\|x\\|_{\\ell^{2}}}.\n$$\nThe space $\\ell^{2}$ is the Hilbert space of all real sequences $x=\\{x_{n}\\}_{n\\in\\mathbb{N}}$ such that $\\sum_{n=1}^{\\infty}|x_{n}|^{2}<\\infty$, with norm $\\|x\\|_{\\ell^{2}}=\\left(\\sum_{n=1}^{\\infty}|x_{n}|^{2}\\right)^{1/2}$.\n\nThe operator $T$ acts diagonally relative to the canonical orthonormal basis $\\{e_{n}\\}_{n\\in\\mathbb{N}}$, with $(Te_{n})_{k}=\\frac{(e_{n})_{k}}{k}=\\frac{\\delta_{nk}}{k}$, so $Te_{n}=\\frac{1}{n}e_{n}$. From this, we compute the squared norm of $Tx$ for an arbitrary $x\\in\\ell^{2}$:\n$$\n\\|Tx\\|_{\\ell^{2}}^{2}=\\sum_{n=1}^{\\infty}\\left|\\frac{x_{n}}{n}\\right|^{2}=\\sum_{n=1}^{\\infty}\\frac{|x_{n}|^{2}}{n^{2}}.\n$$\nUsing the inequality $\\frac{1}{n^{2}}\\leq 1$ for all $n\\in\\mathbb{N}$, we obtain\n$$\n\\|Tx\\|_{\\ell^{2}}^{2}=\\sum_{n=1}^{\\infty}\\frac{|x_{n}|^{2}}{n^{2}}\\leq\\sum_{n=1}^{\\infty}|x_{n}|^{2}=\\|x\\|_{\\ell^{2}}^{2},\n$$\nhence\n$$\n\\|Tx\\|_{\\ell^{2}}\\leq\\|x\\|_{\\ell^{2}} \\quad \\text{for all } x\\in\\ell^{2}.\n$$\nTherefore,\n$$\n\\|T\\|\\leq 1.\n$$\nTo determine whether equality holds, we exhibit a sequence $x$ for which $\\|Tx\\|_{\\ell^{2}}=\\|x\\|_{\\ell^{2}}$. Consider $x=e_{1}$, the first canonical basis vector. Then $Te_{1}=\\frac{1}{1}e_{1}=e_{1}$, so\n$$\n\\frac{\\|Te_{1}\\|_{\\ell^{2}}}{\\|e_{1}\\|_{\\ell^{2}}}=\\frac{1}{1}=1.\n$$\nThus $\\|T\\|\\geq 1$, and together with $\\|T\\|\\leq 1$, we conclude\n$$\n\\|T\\|=1.\n$$\n\nNext, we determine compactness from first principles. An operator $K:\\ell^{2}\\to\\ell^{2}$ is compact if it maps bounded subsets of $\\ell^{2}$ into relatively compact subsets, equivalently, the image of the unit ball under $K$ has compact closure in $\\ell^{2}$.\n\nThere are two standard routes from fundamental definitions to compactness for diagonal operators:\n\n1. Behavior on the canonical orthonormal basis. If $K$ is compact, then for any bounded sequence $\\{x^{(m)}\\}$ in $\\ell^{2}$, the sequence $\\{Kx^{(m)}\\}$ must have a convergent subsequence. In particular, take the bounded sequence $x^{(m)}=e_{m}$. For our operator $T$, we have $Te_{m}=\\frac{1}{m}e_{m}$. Compute the norms:\n$$\n\\|Te_{m}\\|_{\\ell^{2}}=\\left\\|\\frac{1}{m}e_{m}\\right\\|_{\\ell^{2}}=\\frac{1}{m}.\n$$\nThe sequence of norms $\\{\\|Te_{m}\\|_{\\ell^{2}}\\}_{m}$ tends to $0$ as $m\\to\\infty$. Moreover, the sequence $\\{Te_{m}\\}$ converges to $0$ in $\\ell^{2}$ because\n$$\n\\|Te_{m}-0\\|_{\\ell^{2}}=\\|Te_{m}\\|_{\\ell^{2}}=\\frac{1}{m}\\to 0.\n$$\nThis behavior is necessary for compactness and also suggests that $T$ can be approximated by finite-rank operators.\n\n2. Approximation by finite-rank operators in operator norm. Define, for each $N\\in\\mathbb{N}$, a finite-rank operator $T_{N}:\\ell^{2}\\to\\ell^{2}$ by\n$$\n(T_{N}x)_{n}=\\begin{cases}\n\\frac{x_{n}}{n}, & 1\\leq n\\leq N,\\\\\n0, & n>N.\n\\end{cases}\n$$\nEach $T_{N}$ has finite-dimensional range spanned by $\\{e_{1},\\dots,e_{N}\\}$ and is therefore compact. Compute the operator norm of the tail $T-T_{N}$:\n$$\n\\|(T-T_{N})x\\|_{\\ell^{2}}^{2}=\\sum_{n=N+1}^{\\infty}\\left|\\frac{x_{n}}{n}\\right|^{2}\\leq\\left(\\sup_{n>N}\\frac{1}{n^{2}}\\right)\\sum_{n=N+1}^{\\infty}|x_{n}|^{2}\\leq\\left(\\sup_{n>N}\\frac{1}{n^{2}}\\right)\\|x\\|_{\\ell^{2}}^{2}.\n$$\nHence\n$$\n\\|T-T_{N}\\|\\leq\\sup_{n>N}\\frac{1}{n}=\\frac{1}{N+1}.\n$$\nTherefore $\\|T-T_{N}\\|\\to 0$ as $N\\to\\infty$, showing that $T$ is the operator-norm limit of finite-rank (hence compact) operators, which implies that $T$ is compact.\n\nEither route is compatible with the core definition of compactness: the image of the unit ball under $T$ is totally bounded because one can approximate $Tx$ arbitrarily well by $T_{N}x$ uniformly over $x$ in the unit ball, and $\\{T_{N}x:\\|x\\|\\leq 1\\}$ lies in a finite-dimensional space where closed and bounded sets are compact.\n\nAs requested, we provide the two reported quantities. First, the operator norm $\\|T\\|=1$. Second, the limit $\\lim_{n\\to\\infty}\\|Te_{n}\\|=\\lim_{n\\to\\infty}\\frac{1}{n}=0$. The latter, combined with the approximation argument, confirms that $T$ is compact.\n\nThus the final row matrix is $(1,\\,0)$.", "answer": "$$\\boxed{\\begin{pmatrix}1 & 0\\end{pmatrix}}$$", "id": "3398465"}]}