{"hands_on_practices": [{"introduction": "A fundamental choice in solving variational inverse problems is the order of operations: do we first derive the continuous optimality conditions and then discretize, or discretize the functional and then optimize? This exercise [@problem_id:3376894] explores these two paths, \"regularize-then-discretize\" and \"discretize-then-regularize,\" for a Total Variation (TV) regularized problem. By deriving and comparing the resulting discrete equations, you will see firsthand that these strategies are not always equivalent, a crucial insight for developing and analyzing numerical inversion algorithms.", "problem": "Consider a one-dimensional inverse problem on the domain $[0,1]$ with a linear identity forward operator and quadratic data misfit, regularized by a differentiable approximation of the Total Variation (TV) seminorm. The continuous objective functional is\n$$\n\\mathcal{J}(u) \\;=\\; \\frac{1}{2}\\int_{0}^{1} \\left(u(x)-y(x)\\right)^{2}\\,dx \\;+\\; \\lambda \\int_{0}^{1} \\sqrt{\\left(u'(x)\\right)^{2} + \\varepsilon^{2}}\\,dx,\n$$\nwhere $u$ is the unknown state, $y$ is the observation, $\\lambda>0$ is the regularization weight, and $\\varepsilon>0$ is a differentiability parameter. To avoid the so-called inverse crime, when generating synthetic data one typically employs a discretization different from the one used in the inversion; here, however, you will derive and compare the discrete Euler–Lagrange equations that arise from two distinct discretization strategies for the inversion functional itself, as a study in how discretization choices affect the first-order optimality conditions.\n\nTask 1 (regularize-then-discretize). Starting from the first variation of $\\mathcal{J}(u)$, derive the continuous Euler–Lagrange equation for a stationary point $u$ under homogeneous Dirichlet boundary conditions $u(0)=u(1)=0$. Then, discretize this differential equation on a uniform grid $x_{i}=ih$ for $i=0,1,\\dots,N$ with step $h=1/N$ as follows: use the midpoint flux approximation\n$$\np_{i+\\frac{1}{2}} \\;=\\; \\frac{\\frac{u_{i+1}-u_{i}}{h}}{\\sqrt{\\left(\\frac{u_{i+1}-u_{i}}{h}\\right)^{2} + \\varepsilon^{2}}},\n$$\nand approximate the divergence at node $i$ by\n$$\n\\left(\\nabla\\cdot p\\right)_{i} \\;\\approx\\; \\frac{p_{i+\\frac{1}{2}} - p_{i-\\frac{1}{2}}}{h},\n$$\nleading to a discrete Euler–Lagrange residual of the form\n$$\nr^{\\mathrm{RtD}}_{i}(u) \\;=\\; \\left(u_{i} - y_{i}\\right) \\;-\\; \\lambda\\,\\frac{p_{i+\\frac{1}{2}} - p_{i-\\frac{1}{2}}}{h}, \\quad i=1,\\dots,N-1,\n$$\nwith $u_{0}=u_{N}=0$.\n\nTask 2 (discretize-then-regularize). Discretize the functional $\\mathcal{J}(u)$ directly on the same uniform grid using the Riemann sum approximation for the integral terms, namely\n$$\n\\mathcal{J}_{h}(u) \\;=\\; \\frac{1}{2}\\,h\\,\\sum_{i=1}^{N-1} \\left(u_{i}-y_{i}\\right)^{2} \\;+\\; \\lambda\\,\\sum_{i=0}^{N-1} \\sqrt{\\left(u_{i+1}-u_{i}\\right)^{2} + \\left(\\varepsilon h\\right)^{2}},\n$$\nwith $u_{0}=u_{N}=0$. Compute the gradient of $\\mathcal{J}_{h}(u)$ with respect to the interior degrees of freedom to obtain the discrete Euler–Lagrange residual\n$$\ng^{\\mathrm{DtR}}_{i}(u) \\;=\\; h\\left(u_{i} - y_{i}\\right) \\;+\\; \\lambda \\left[\\frac{u_{i}-u_{i-1}}{\\sqrt{\\left(u_{i}-u_{i-1}\\right)^{2} + \\left(\\varepsilon h\\right)^{2}}} \\;-\\; \\frac{u_{i+1}-u_{i}}{\\sqrt{\\left(u_{i+1}-u_{i}\\right)^{2} + \\left(\\varepsilon h\\right)^{2}}}\\right], \\quad i=1,\\dots,N-1,\n$$\nwith $u_{0}=u_{N}=0$.\n\nTask 3 (quantitative comparison on a specific instance). Consider $N=4$, so that $h=1/4$, choose $\\lambda=1$ and $\\varepsilon=1$, impose $u_{0}=u_{4}=0$, and take the interior grid values $u_{1}=1$, $u_{2}=0$, $u_{3}=-1$. Take the observation values $y_{i}=0$ for $i=1,2,3$. Using the formulas derived in Tasks 1 and 2, compute the ratio\n$$\n\\frac{r^{\\mathrm{RtD}}_{1}(u)}{g^{\\mathrm{DtR}}_{1}(u)}.\n$$\n\nYour final answer must be a single real number. No rounding is required and no units are needed.", "solution": "The problem asks for the derivation and comparison of two different discretization strategies for a one-dimensional variational problem, followed by a quantitative evaluation for a specific instance. The two strategies are \"regularize-then-discretize\" (RtD) and \"discretize-then-regularize\" (DtR). Note that the problem uses the term \"discretize-then-regularize\", but a more conventional term for this approach is \"discretize-then-optimize\", as regularization is already present in the continuous functional. We will adhere to the problem's terminology.\n\nThe continuous objective functional is given by\n$$\n\\mathcal{J}(u) \\;=\\; \\frac{1}{2}\\int_{0}^{1} \\left(u(x)-y(x)\\right)^{2}\\,dx \\;+\\; \\lambda \\int_{0}^{1} \\sqrt{\\left(u'(x)\\right)^{2} + \\varepsilon^{2}}\\,dx\n$$\nWe address each of the three tasks sequentially.\n\n**Task 1: Regularize-then-Discretize (RtD)**\n\nIn the RtD approach, we first find the continuous first-order optimality condition, which is the Euler-Lagrange equation, and then discretize this equation. The integrand of the functional $\\mathcal{J}(u)$ is the Lagrangian, $\\mathcal{L}(x, u, u')$.\n$$\n\\mathcal{L}(x, u(x), u'(x)) \\;=\\; \\frac{1}{2}\\left(u(x)-y(x)\\right)^{2} \\;+\\; \\lambda \\sqrt{\\left(u'(x)\\right)^{2} + \\varepsilon^{2}}\n$$\nThe Euler-Lagrange equation for a minimizer $u(x)$ is given by\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u} - \\frac{d}{dx}\\left(\\frac{\\partial \\mathcal{L}}{\\partial u'}\\right) \\;=\\; 0\n$$\nWe compute the partial derivatives of $\\mathcal{L}$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u} \\;=\\; u(x) - y(x)\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u'} \\;=\\; \\lambda \\cdot \\frac{1}{2\\sqrt{(u'(x))^2 + \\varepsilon^2}} \\cdot 2u'(x) \\;=\\; \\lambda \\frac{u'(x)}{\\sqrt{(u'(x))^2 + \\varepsilon^2}}\n$$\nSubstituting these into the Euler-Lagrange equation yields the continuous optimality condition:\n$$\n\\left(u(x) - y(x)\\right) - \\lambda \\frac{d}{dx}\\left(\\frac{u'(x)}{\\sqrt{\\left(u'(x)\\right)^{2} + \\varepsilon^{2}}}\\right) \\;=\\; 0\n$$\nNow, we discretize this differential equation on a uniform grid $x_i = ih$ for $i=0, \\dots, N$, with $h=1/N$. Let $u_i \\approx u(x_i)$. The equation at node $x_i$ is discretized as\n$$\n(u_i - y_i) - \\lambda \\left[\\frac{d}{dx}(\\cdot)\\right]_i \\;\\approx\\; 0\n$$\nThe problem specifies a finite volume-style discretization. Let $p(x) = \\frac{u'(x)}{\\sqrt{(u'(x))^2 + \\varepsilon^2}}$. The differential equation is $u(x)-y(x)-\\lambda p'(x) = 0$. We approximate the divergence term $p'(x_i)$ using a central difference on fluxes defined at cell midpoints:\n$$\np'(x_i) \\;\\approx\\; \\frac{p(x_{i+1/2}) - p(x_{i-1/2})}{h} \\;=\\; \\frac{p_{i+1/2} - p_{i-1/2}}{h}\n$$\nThe flux $p_{i+1/2}$ itself is approximated by evaluating its arguments at the midpoint $x_{i+1/2}$. The derivative $u'(x_{i+1/2})$ is approximated by a central difference on the interval $[x_i, x_{i+1}]$:\n$$\nu'(x_{i+1/2}) \\;\\approx\\; \\frac{u_{i+1}-u_i}{h}\n$$\nSubstituting this into the expression for $p$ gives the midpoint flux:\n$$\np_{i+1/2} \\;\\approx\\; \\frac{\\frac{u_{i+1}-u_i}{h}}{\\sqrt{\\left(\\frac{u_{i+1}-u_i}{h}\\right)^2 + \\varepsilon^2}}\n$$\nThis matches the expression provided in the problem. The discrete residual for the Euler-Lagrange equation at an interior node $i \\in \\{1, \\dots, N-1\\}$ is therefore\n$$\nr^{\\mathrm{RtD}}_{i}(u) \\;=\\; (u_{i} - y_{i}) - \\lambda\\,\\frac{p_{i+\\frac{1}{2}} - p_{i-\\frac{1}{2}}}{h}\n$$\nThis confirms the formula given in the problem statement for Task 1.\n\n**Task 2: Discretize-then-Regularize (DtR)**\n\nIn the DtR approach, we first discretize the functional $\\mathcal{J}(u)$ and then compute the gradient of the resulting discrete function. The discretized functional $\\mathcal{J}_h(u)$ is given as\n$$\n\\mathcal{J}_{h}(u) \\;=\\; \\frac{1}{2}\\,h\\,\\sum_{j=1}^{N-1} \\left(u_{j}-y_{j}\\right)^{2} \\;+\\; \\lambda\\,\\sum_{j=0}^{N-1} \\sqrt{\\left(u_{j+1}-u_{j}\\right)^{2} + \\left(\\varepsilon h\\right)^{2}}\n$$\nThe discrete Euler-Lagrange equations are found by setting the partial derivatives with respect to the interior degrees of freedom $u_i$ (for $i=1, \\dots, N-1$) to zero. The residual $g^{\\mathrm{DtR}}_{i}(u)$ is defined as this partial derivative, $\\frac{\\partial \\mathcal{J}_h}{\\partial u_i}$.\n\nWe differentiate $\\mathcal{J}_h(u)$ with respect to a specific $u_i$:\nThe data misfit term (the first sum) only depends on $u_i$ when the summation index $j=i$. Its derivative is:\n$$\n\\frac{\\partial}{\\partial u_i}\\left(\\frac{1}{2}\\,h\\,\\left(u_{i}-y_{i}\\right)^{2}\\right) \\;=\\; h(u_i - y_i)\n$$\nThe regularization term (the second sum) depends on $u_i$ in two consecutive terms of the sum: for $j=i-1$ and $j=i$.\nLet $R_j = \\sqrt{(u_{j+1}-u_j)^2 + (\\varepsilon h)^2}$. We need to compute $\\frac{\\partial}{\\partial u_i} (\\lambda R_{i-1} + \\lambda R_i)$.\nFor the term $j=i-1$, we have $R_{i-1} = \\sqrt{(u_i - u_{i-1})^2 + (\\varepsilon h)^2}$. Its derivative is:\n$$\n\\frac{\\partial}{\\partial u_i} (\\lambda R_{i-1}) = \\lambda \\frac{2(u_i - u_{i-1})}{2\\sqrt{(u_i - u_{i-1})^2 + (\\varepsilon h)^2}} = \\lambda \\frac{u_i - u_{i-1}}{\\sqrt{(u_i-u_{i-1})^2 + (\\varepsilon h)^2}}\n$$\nFor the term $j=i$, we have $R_i = \\sqrt{(u_{i+1} - u_i)^2 + (\\varepsilon h)^2}$. Its derivative is:\n$$\n\\frac{\\partial}{\\partial u_i} (\\lambda R_i) = \\lambda \\frac{2(u_{i+1} - u_i)(-1)}{2\\sqrt{(u_{i+1} - u_i)^2 + (\\varepsilon h)^2}} = - \\lambda \\frac{u_{i+1} - u_i}{\\sqrt{(u_{i+1} - u_i)^2 + (\\varepsilon h)^2}}\n$$\nCombining all parts, the gradient is:\n$$\ng^{\\mathrm{DtR}}_{i}(u) \\;=\\; \\frac{\\partial \\mathcal{J}_h}{\\partial u_i} \\;=\\; h\\left(u_{i} - y_{i}\\right) \\;+\\; \\lambda \\left[\\frac{u_{i}-u_{i-1}}{\\sqrt{\\left(u_{i}-u_{i-1}\\right)^{2} + \\left(\\varepsilon h\\right)^{2}}} \\;-\\; \\frac{u_{i+1}-u_{i}}{\\sqrt{\\left(u_{i+1}-u_{i}\\right)^{2} + \\left(\\varepsilon h\\right)^{2}}}\\right]\n$$\nThis confirms the formula given in the problem statement for Task 2.\n\n**Task 3: Quantitative Comparison**\n\nWe are asked to compute the ratio $\\frac{r^{\\mathrm{RtD}}_{1}(u)}{g^{\\mathrm{DtR}}_{1}(u)}$ for $i=1$ with the following parameters:\n- $N=4$, so the grid spacing is $h=1/N=1/4$.\n- Regularization parameters: $\\lambda=1$, $\\varepsilon=1$.\n- State vector (interior nodes): $u_1=1$, $u_2=0$, $u_3=-1$.\n- Boundary conditions: $u_0=0$, $u_4=0$.\n- Observation vector (interior nodes): $y_1=0$, $y_2=0$, $y_3=0$.\n\nFirst, we compute the residual $r^{\\mathrm{RtD}}_{1}(u)$:\n$$\nr^{\\mathrm{RtD}}_{1}(u) \\;=\\; (u_1 - y_1) - \\lambda \\frac{p_{1+1/2} - p_{1-1/2}}{h}\n$$\nWith the given values: $u_1=1$, $y_1=0$, $\\lambda=1$, $h=1/4$.\n$$\nr^{\\mathrm{RtD}}_{1}(u) \\;=\\; (1-0) - 1 \\cdot \\frac{p_{3/2} - p_{1/2}}{1/4} \\;=\\; 1 - 4(p_{3/2} - p_{1/2})\n$$\nWe need to calculate the fluxes $p_{1/2}$ and $p_{3/2}$.\nFor $p_{1/2}$, we use nodes $u_0=0$ and $u_1=1$:\n$$\n\\frac{u_1-u_0}{h} = \\frac{1-0}{1/4} = 4\n$$\n$$\np_{1/2} = \\frac{4}{\\sqrt{4^2 + \\varepsilon^2}} = \\frac{4}{\\sqrt{16 + 1^2}} = \\frac{4}{\\sqrt{17}}\n$$\nFor $p_{3/2}$, we use nodes $u_1=1$ and $u_2=0$:\n$$\n\\frac{u_2-u_1}{h} = \\frac{0-1}{1/4} = -4\n$$\n$$\np_{3/2} = \\frac{-4}{\\sqrt{(-4)^2 + \\varepsilon^2}} = \\frac{-4}{\\sqrt{16 + 1^2}} = \\frac{-4}{\\sqrt{17}}\n$$\nSubstituting these fluxes back into the expression for $r^{\\mathrm{RtD}}_{1}(u)$:\n$$\nr^{\\mathrm{RtD}}_{1}(u) \\;=\\; 1 - 4\\left(\\frac{-4}{\\sqrt{17}} - \\frac{4}{\\sqrt{17}}\\right) \\;=\\; 1 - 4\\left(\\frac{-8}{\\sqrt{17}}\\right) \\;=\\; 1 + \\frac{32}{\\sqrt{17}}\n$$\nNext, we compute the residual $g^{\\mathrm{DtR}}_{1}(u)$:\n$$\ng^{\\mathrm{DtR}}_{1}(u) = h\\left(u_{1} - y_{1}\\right) + \\lambda \\left[\\frac{u_{1}-u_{0}}{\\sqrt{\\left(u_{1}-u_{0}\\right)^{2} + \\left(\\varepsilon h\\right)^{2}}} - \\frac{u_{2}-u_{1}}{\\sqrt{\\left(u_{2}-u_{1}\\right)^{2} + \\left(\\varepsilon h\\right)^{2}}}\\right]\n$$\nWith the given values: $\\varepsilon h = 1 \\cdot (1/4) = 1/4$.\n$$\ng^{\\mathrm{DtR}}_{1}(u) = \\frac{1}{4}(1-0) + 1 \\cdot \\left[\\frac{1-0}{\\sqrt{(1-0)^2 + (1/4)^2}} - \\frac{0-1}{\\sqrt{(0-1)^2 + (1/4)^2}}\\right]\n$$\n$$\ng^{\\mathrm{DtR}}_{1}(u) = \\frac{1}{4} + \\left[\\frac{1}{\\sqrt{1 + 1/16}} - \\frac{-1}{\\sqrt{1 + 1/16}}\\right] = \\frac{1}{4} + \\frac{1 - (-1)}{\\sqrt{17/16}}\n$$\n$$\ng^{\\mathrm{DtR}}_{1}(u) = \\frac{1}{4} + \\frac{2}{\\sqrt{17}/4} = \\frac{1}{4} + \\frac{8}{\\sqrt{17}}\n$$\nFinally, we compute the required ratio:\n$$\n\\frac{r^{\\mathrm{RtD}}_{1}(u)}{g^{\\mathrm{DtR}}_{1}(u)} = \\frac{1 + \\frac{32}{\\sqrt{17}}}{\\frac{1}{4} + \\frac{8}{\\sqrt{17}}}\n$$\nWe can factor out a $4$ from the numerator:\n$$\n1 + \\frac{32}{\\sqrt{17}} = 4\\left(\\frac{1}{4} + \\frac{8}{\\sqrt{17}}\\right)\n$$\nThus, the ratio simplifies to:\n$$\n\\frac{r^{\\mathrm{RtD}}_{1}(u)}{g^{\\mathrm{DtR}}_{1}(u)} = \\frac{4\\left(\\frac{1}{4} + \\frac{8}{\\sqrt{17}}\\right)}{\\frac{1}{4} + \\frac{8}{\\sqrt{17}}} = 4\n$$\nThis reveals that for the chosen discretizations, the two residuals are related by a simple scaling factor $1/h$. Specifically, $r^{\\mathrm{RtD}}_i = (1/h) g^{\\mathrm{DtR}}_i$.", "answer": "$$\n\\boxed{4}\n$$", "id": "3376894"}, {"introduction": "To avoid inverse crime, the discrete forward model must faithfully represent the entire data acquisition process. This practice [@problem_id:3376940] examines a deconvolution problem where the signal is also downsampled, forcing a careful construction of the forward operator. By deriving the structure of the normal operator from first principles, you will uncover how explicitly modeling the sampling process creates aliasing in the frequency domain, fundamentally altering the spectral properties of the inverse problem.", "problem": "Consider a one-dimensional periodic deconvolution problem on the cyclic group $\\mathbb{Z}_N$ with $N \\in \\mathbb{N}$, where the unknown signal $x \\in \\mathbb{C}^N$ is mapped to noiseless data by convolution with a known kernel $k \\in \\mathbb{C}^N$. The forward map is $A = S C_k$, where $C_k$ is the circulant convolution operator generated by $k$, and $S$ samples every $m$-th entry with $m \\in \\mathbb{N}$ dividing $N$. To avoid the so-called inverse crime, the sampling operator $S$ must be explicitly included in the discrete forward model rather than treated implicitly or ignored.\n\nYou are to work from first principles based on the definitions of circular convolution and the Discrete Fourier Transform (DFT). Let $F_N$ denote the unitary Discrete Fourier Transform (DFT) matrix with entries $(F_N)_{p,j} = \\frac{1}{\\sqrt{N}}\\exp(-2\\pi i pj/N)$ for $0 \\le p,j \\le N-1$. Let $C_k$ denote circular convolution by $k$ so that $C_k = F_N^{*}\\Lambda F_N$, where $\\Lambda = \\mathrm{diag}(K[0],\\dots,K[N-1])$ and $K[p] = \\sum_{n=0}^{N-1} k[n]\\exp(-2\\pi i pn/N)$ are the eigenvalues of the circulant operator. The sampling operator $S:\\mathbb{C}^N \\to \\mathbb{C}^{N/m}$ is defined by $(S z)[n] = z[m n]$ for $0 \\le n \\le N/m - 1$, with adjoint $S^{*}$ equal to zero-insertion at indices divisible by $m$.\n\n(a) Using only these definitions and the fact that uniform downsampling in the spatial domain corresponds to aliasing in the frequency domain, derive an explicit expression for the normal operator $A^{*}A = C_k^{*}S^{*}S C_k$ in the DFT basis. Show that in the DFT domain the operator $A^{*}A$ is block diagonal with $N/m$ blocks of size $m \\times m$, each block corresponding to one aliasing class $\\{r + \\ell(N/m)\\}_{\\ell=0}^{m-1}$, and identify the unique nonzero eigenvalue in each block in terms of the convolution eigenvalues $\\{K[p]\\}_{p=0}^{N-1}$. Your derivation must start from the definitions of $F_N$, $C_k$, and $S$, and must not assume any pre-packaged spectral aliasing formulas.\n\n(b) Specialize to $N=12$ and $m=3$. Let the kernel be the nearest-neighbor smoothing stencil on $\\mathbb{Z}_{12}$ given by $k[0]=1$, $k[1]=k[11]=\\frac{1}{2}$, and $k[n]=0$ otherwise. Compute the minimal nonzero eigenvalue of $A^{*}A$ exactly (no rounding required). State your final answer as a single real number without units.", "solution": "This problem consists of two parts. Part (a) requires the derivation of the structure of the normal operator $A^*A$ in the Discrete Fourier Transform (DFT) basis. Part (b) requires the computation of a specific value based on the results of part (a).\n\n### Part (a): Structure of the Normal Operator in the DFT Basis\n\nThe forward operator is given by $A = S C_k$, where $C_k$ is the circulant convolution operator and $S$ is the downsampling operator. The normal operator is thus $A^*A = (S C_k)^* (S C_k) = C_k^* S^* S C_k$. We want to find the representation of this operator in the DFT basis. This corresponds to computing the matrix $\\mathcal{A} = F_N (A^*A) F_N^*$, where $F_N$ is the unitary DFT matrix.\n\nUsing the given property $C_k = F_N^* \\Lambda F_N$, where $\\Lambda = \\mathrm{diag}(K[p])$, we can express $C_k^*$ as:\n$C_k^* = (F_N^* \\Lambda F_N)^* = F_N^* \\Lambda^* (F_N^*)^* = F_N^* \\Lambda^* F_N$.\nHere $\\Lambda^*$ is the conjugate transpose of $\\Lambda$, so $\\Lambda^* = \\mathrm{diag}(\\overline{K[p]})$.\n\nNow we substitute these into the expression for $\\mathcal{A}$:\n$$ \\mathcal{A} = F_N (F_N^* \\Lambda^* F_N) (S^*S) (F_N^* \\Lambda F_N) F_N^* $$\nSince $F_N$ is unitary, $F_N F_N^* = I$, where $I$ is the identity matrix. The expression simplifies to:\n$$ \\mathcal{A} = (F_N F_N^*) \\Lambda^* (F_N S^*S F_N^*) \\Lambda (F_N F_N^*) $$\n$$ \\mathcal{A} = \\Lambda^* (F_N S^*S F_N^*) \\Lambda $$\nThe central part of this expression is the operator $M = F_N S^*S F_N^*$, which represents the sampling-and-reconstruction process in the frequency domain. Let's find its matrix elements $M_{p,q}$.\n\nThe operator $S^*S$ acts on a vector $z \\in \\mathbb{C}^N$. The operator $S$ selects entries at indices which are multiples of $m$. The adjoint $S^*$ inserts zeros. The composite operator $S^*S$ is a projection. Specifically, $(S^*S z)[j] = z[j]$ if $j$ is a multiple of $m$, and $0$ otherwise. As a matrix, $S^*S$ is a diagonal matrix whose $j$-th diagonal entry is $1$ if $j \\equiv 0 \\pmod{m}$ and $0$ otherwise. Let's denote this indicator function as $\\mathbf{1}_{j \\equiv 0 \\pmod m}$.\n\nThe matrix elements of $M = F_N (S^*S) F_N^*$ are given by:\n$$ M_{p,q} = (F_N (S^*S) F_N^*)_{p,q} = \\sum_{j=0}^{N-1} \\sum_{l=0}^{N-1} (F_N)_{p,j} (S^*S)_{j,l} (F_N^*)_{l,q} $$\nSince $(S^*S)_{j,l} = \\delta_{j,l} \\mathbf{1}_{j \\equiv 0 \\pmod m}$:\n$$ M_{p,q} = \\sum_{j=0}^{N-1} (F_N)_{p,j} \\mathbf{1}_{j \\equiv 0 \\pmod m} (F_N^*)_{j,q} $$\nSubstituting the definitions of the DFT matrix entries $(F_N)_{p,j} = \\frac{1}{\\sqrt{N}}\\exp(-2\\pi i pj/N)$ and $(F_N^*)_{j,q} = \\frac{1}{\\sqrt{N}}\\exp(2\\pi i qj/N)$:\n$$ M_{p,q} = \\sum_{j=0}^{N-1} \\left( \\frac{1}{\\sqrt{N}}\\exp(-2\\pi i pj/N) \\right) \\mathbf{1}_{j \\equiv 0 \\pmod m} \\left( \\frac{1}{\\sqrt{N}}\\exp(2\\pi i qj/N) \\right) $$\nThe sum is non-zero only for indices $j$ that are multiples of $m$. We can write $j=nm$ for $n \\in \\{0, 1, \\dots, N/m-1\\}$.\n$$ M_{p,q} = \\frac{1}{N} \\sum_{n=0}^{N/m-1} \\exp(-2\\pi i p(nm)/N) \\exp(2\\pi i q(nm)/N) $$\n$$ M_{p,q} = \\frac{1}{N} \\sum_{n=0}^{N/m-1} \\exp\\left(-\\frac{2\\pi i (p-q)n}{N/m}\\right) $$\nThis is a geometric series sum. The sum is equal to $N/m$ if the argument of the exponent is a multiple of $2\\pi i$, which means that $(p-q)/(N/m)$ is an integer. This is equivalent to $p \\equiv q \\pmod{N/m}$. If $p-q$ is not a multiple of $N/m$, the sum is zero.\nTherefore,\n$$ M_{p,q} = \\begin{cases} \\frac{1}{N} \\cdot \\frac{N}{m} = \\frac{1}{m}  \\text{if } p \\equiv q \\pmod{N/m} \\\\ 0  \\text{otherwise} \\end{cases} $$\nThis shows that uniform downsampling in the spatial domain corresponds to aliasing in the frequency domain, coupling frequencies that have the same remainder modulo $N/m$.\n\nNow we can determine the elements of $\\mathcal{A} = \\Lambda^* M \\Lambda$:\n$$ \\mathcal{A}_{p,q} = \\sum_{a=0}^{N-1} \\sum_{b=0}^{N-1} (\\Lambda^*)_{p,a} M_{a,b} \\Lambda_{b,q} $$\nSince $\\Lambda$ is diagonal, $(\\Lambda^*)_{p,a} = \\overline{K[p]}\\delta_{p,a}$ and $\\Lambda_{b,q} = K[b]\\delta_{b,q}$.\n$$ \\mathcal{A}_{p,q} = \\overline{K[p]} M_{p,q} K[q] $$\nSubstituting the expression for $M_{p,q}$:\n$$ \\mathcal{A}_{p,q} = \\begin{cases} \\frac{1}{m} \\overline{K[p]} K[q]  \\text{if } p \\equiv q \\pmod{N/m} \\\\ 0  \\text{otherwise} \\end{cases} $$\nThis expression shows that $\\mathcal{A}$ is a block diagonal matrix. The frequency indices $\\{0, \\dots, N-1\\}$ are partitioned into $N/m$ disjoint sets, known as aliasing classes. Let $r \\in \\{0, 1, \\dots, N/m - 1\\}$. The $r$-th aliasing class is the set of indices $I_r = \\{r + \\ell(N/m) \\mid \\ell=0, \\dots, m-1\\}$.\nThe matrix $\\mathcal{A}$ has non-zero entries only if both $p$ and $q$ belong to the same class $I_r$. Thus, $\\mathcal{A}$ decomposes into $N/m$ blocks of size $m \\times m$.\n\nLet's analyze the $r$-th block, $\\mathcal{A}^{(r)}$, corresponding to the indices $p_\\ell = r + \\ell(N/m)$ for $\\ell \\in \\{0, \\dots, m-1\\}$. The entries of this block are given by:\n$$ (\\mathcal{A}^{(r)})_{\\ell, \\ell'} = \\mathcal{A}_{p_\\ell, p_{\\ell'}} = \\frac{1}{m} \\overline{K[p_\\ell]} K[p_{\\ell'}] $$\nfor $\\ell, \\ell' \\in \\{0, \\dots, m-1\\}$. This is a rank-one matrix. A rank-one matrix of the form $uv^*$ has one non-zero eigenvalue equal to $v^*u$, and the rest are zero. In our case, let's define a vector $w \\in \\mathbb{C}^m$ with components $w_\\ell = K[p_\\ell]$. The block matrix entries are $(\\mathcal{A}^{(r)})_{\\ell, \\ell'} = \\frac{1}{m} \\overline{w_\\ell} w_{\\ell'}$. Let us verify the eigenvector property directly. Consider a vector $v$ with components $v_{\\ell'} = \\overline{w_{\\ell'}}$.\n$$ (\\mathcal{A}^{(r)} v)_\\ell = \\sum_{\\ell'=0}^{m-1} (\\mathcal{A}^{(r)})_{\\ell, \\ell'} v_{\\ell'} = \\sum_{\\ell'=0}^{m-1} \\left(\\frac{1}{m} \\overline{w_\\ell} w_{\\ell'}\\right) \\overline{w_{\\ell'}} = \\frac{1}{m} \\overline{w_\\ell} \\sum_{\\ell'=0}^{m-1} |w_{\\ell'}|^2 $$\nThis can be written as $(\\mathcal{A}^{(r)} v)_\\ell = \\lambda_r v_\\ell$, where the eigenvalue $\\lambda_r$ is:\n$$ \\lambda_r = \\frac{1}{m} \\sum_{\\ell'=0}^{m-1} |w_{\\ell'}|^2 = \\frac{1}{m} \\sum_{\\ell=0}^{m-1} |K[p_\\ell]|^2 = \\frac{1}{m} \\sum_{\\ell=0}^{m-1} |K[r + \\ell(N/m)]|^2 $$\nThe eigenvector is $v$ with entries $\\overline{K[p_\\ell]}$. The other $m-1$ eigenvalues of the block are zero. Thus, for each block $r \\in \\{0, \\dots, N/m-1\\}$, there is a unique nonzero eigenvalue given by the expression for $\\lambda_r$.\n\n### Part (b): Minimal Nonzero Eigenvalue Calculation\n\nWe are given $N=12$, $m=3$, so $N/m=4$. The kernel is $k[0]=1$, $k[1]=k[11]=1/2$, and $k[n]=0$ for other $n$. Since we work on $\\mathbb{Z}_{12}$, $k[11]$ is equivalent to $k[-1]$.\n\nFirst, we compute the DFT of the kernel, $K[p]$ for $p \\in \\{0, \\dots, 11\\}$:\n$$ K[p] = \\sum_{n=0}^{11} k[n] \\exp\\left(-\\frac{2\\pi i pn}{12}\\right) = k[0] + k[1]\\exp\\left(-\\frac{2\\pi i p}{12}\\right) + k[11]\\exp\\left(-\\frac{2\\pi i p(11)}{12}\\right) $$\nUsing $11 \\equiv -1 \\pmod{12}$, this simplifies:\n$$ K[p] = 1 + \\frac{1}{2}\\exp\\left(-\\frac{i \\pi p}{6}\\right) + \\frac{1}{2}\\exp\\left(\\frac{i \\pi p}{6}\\right) = 1 + \\cos\\left(\\frac{\\pi p}{6}\\right) $$\nSince $K[p]$ is real, $|K[p]|^2 = (K[p])^2 = (1 + \\cos(\\pi p/6))^2$.\n\nThe nonzero eigenvalues of $A^*A$ are given by $\\lambda_r$ for $r \\in \\{0, 1, 2, 3\\}$.\n$$ \\lambda_r = \\frac{1}{3} \\sum_{\\ell=0}^{2} (K[r+4\\ell])^2 = \\frac{1}{3} \\left( K[r]^2 + K[r+4]^2 + K[r+8]^2 \\right) $$\n\nWe compute these four eigenvalues:\n\nFor $r=0$:\n$K[0] = 1+\\cos(0) = 2$.\n$K[4] = 1+\\cos(4\\pi/6) = 1+\\cos(2\\pi/3) = 1-1/2 = 1/2$.\n$K[8] = 1+\\cos(8\\pi/6) = 1+\\cos(4\\pi/3) = 1-1/2 = 1/2$.\n$\\lambda_0 = \\frac{1}{3}(2^2 + (1/2)^2 + (1/2)^2) = \\frac{1}{3}(4 + 1/4 + 1/4) = \\frac{1}{3}(4.5) = 1.5 = \\frac{3}{2}$.\n\nFor $r=1$:\n$K[1] = 1+\\cos(\\pi/6) = 1+\\sqrt{3}/2$.\n$K[5] = 1+\\cos(5\\pi/6) = 1-\\sqrt{3}/2$.\n$K[9] = 1+\\cos(9\\pi/6) = 1+\\cos(3\\pi/2) = 1+0 = 1$.\n$\\lambda_1 = \\frac{1}{3}((1+\\sqrt{3}/2)^2 + (1-\\sqrt{3}/2)^2 + 1^2) = \\frac{1}{3}((1+\\sqrt{3}+3/4) + (1-\\sqrt{3}+3/4) + 1) = \\frac{1}{3}(7/4+7/4+1) = \\frac{1}{3}(7/2+1) = \\frac{1}{3}(9/2) = 1.5 = \\frac{3}{2}$.\n\nFor $r=2$:\n$K[2] = 1+\\cos(2\\pi/6) = 1+\\cos(\\pi/3) = 1+1/2 = 3/2$.\n$K[6] = 1+\\cos(6\\pi/6) = 1+\\cos(\\pi) = 1-1 = 0$.\n$K[10] = 1+\\cos(10\\pi/6) = 1+\\cos(5\\pi/3) = 1+1/2 = 3/2$.\n$\\lambda_2 = \\frac{1}{3}((3/2)^2 + 0^2 + (3/2)^2) = \\frac{1}{3}(9/4 + 9/4) = \\frac{1}{3}(9/2) = 1.5 = \\frac{3}{2}$.\n\nFor $r=3$:\n$K[3] = 1+\\cos(3\\pi/6) = 1+\\cos(\\pi/2) = 1+0 = 1$.\n$K[7] = 1+\\cos(7\\pi/6) = 1-\\sqrt{3}/2$.\n$K[11] = 1+\\cos(11\\pi/6) = 1+\\sqrt{3}/2$.\n$\\lambda_3 = \\frac{1}{3}(1^2 + (1-\\sqrt{3}/2)^2 + (1+\\sqrt{3}/2)^2)$. This is the same sum as for $\\lambda_1$.\n$\\lambda_3 = 1.5 = \\frac{3}{2}$.\n\nAll four nonzero eigenvalues of $A^*A$ are equal to $3/2$. Therefore, the minimal nonzero eigenvalue is $3/2$.", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "3376940"}, {"introduction": "Theoretical analysis must be paired with rigorous numerical validation to ensure a method is sound. This computational exercise [@problem_id:3376948] guides you through a complete mesh refinement study, a cornerstone of numerical code verification. By manufacturing a true solution and generating data from the exact continuous model, you will learn how to properly test a discrete inversion scheme, avoid the \"inverse crime,\" and numerically confirm that the reconstruction error converges at the theoretically predicted rate.", "problem": "Design a mesh refinement study for a linear inverse problem that demonstrates how to avoid the inverse crime and verifies the order of convergence of discretization error in the reconstruction. Consider the following continuous model: recover the unknown function $x(t)$ on the interval $[0,1]$ from exact data $y(s)$ generated by the linear integral operator\n$$\ny(s) = \\int_{0}^{1} K(s,t)\\,x(t)\\,dt,\\quad s\\in[0,1],\\quad K(s,t)=\\min(s,t).\n$$\nTo avoid the inverse crime (using the exact same discrete model to both synthesize and invert data), the data must be synthesized analytically from the continuum model, not via the same discrete operator used to reconstruct.\n\nUse a manufactured smooth truth\n$$\nx_{\\mathrm{true}}(t) = \\sin(2\\pi t) + \\tfrac{1}{2}\\cos(3\\pi t),\n$$\nand derive the exact data $y(s)$ using the identity\n$$\ny(s) = \\int_{0}^{s} t\\,x_{\\mathrm{true}}(t)\\,dt + s\\int_{s}^{1} x_{\\mathrm{true}}(t)\\,dt,\n$$\nso that $y$ is computed from the continuum formula. For a uniform mesh with mesh size $h=1/N$ and cell intervals $I_j=[(j-1)h,jh]$, represent the discrete unknown by cellwise constants $x_h\\in\\mathbb{R}^N$ located at cell midpoints $t_j=(j-\\tfrac{1}{2})h$. Let the measurement points be $s_i=t_i$, $i=1,\\dots,N$.\n\nConstruct two discrete forward operators $A_h\\in\\mathbb{R}^{N\\times N}$:\n- Exact cell-integrated assembly (no quadrature error on the subspace): for each $i,j$, set\n$$\n(A_h)_{ij} = \\int_{I_j} \\min(s_i,t)\\,dt\n=\n\\begin{cases}\ns_i h,  \\text{if } s_i \\le (j-1)h,\\\\\n\\tfrac{1}{2}\\!\\left((jh)^2-((j-1)h)^2\\right),  \\text{if } s_i \\ge jh,\\\\\n\\tfrac{1}{2}\\!\\left(s_i^2-((j-1)h)^2\\right) + s_i\\left(jh-s_i\\right),  \\text{if } s_i\\in\\left((j-1)h,jh\\right).\n\\end{cases}\n$$\n- Midpoint quadrature assembly (first-order quadrature): for each $i,j$, set\n$$(A_h^{\\mathrm{mid}})_{ij} = K\\big(s_i,t_j\\big)\\,h = \\min\\big(s_i,t_j\\big)\\,h.$$\n\nForm reconstructions $x_h^\\ast$ as the least squares (LS) minimizer\n$$\nx_h^\\ast = \\operatorname*{arg\\,min}_{x\\in\\mathbb{R}^N}\\,\\|A_h x - y_h\\|_2,\n$$\nand similarly for $A_h^{\\mathrm{mid}}$, where $y_h\\in\\mathbb{R}^N$ is the vector of exact data sampled at the measurement points $s_i$. Here $\\|\\cdot\\|_2$ denotes the Euclidean norm. Compute the reconstruction error in the discrete $L^2$ norm\n$$\nE_h = \\left(\\sum_{j=1}^N h\\,\\big(x_h^\\ast(j)-x_{\\mathrm{true}}(t_j)\\big)^2\\right)^{1/2} = \\sqrt{h}\\,\\|x_h^\\ast - x_{\\mathrm{true}}(t_\\cdot)\\|_2.\n$$\n\nFrom first principles of approximation by piecewise constants and the smoothing nature of the integral operator with kernel $K(s,t)=\\min(s,t)$ (the Green’s kernel of a second-order elliptic operator), the best-approximation error of smooth $x_{\\mathrm{true}}$ in the space of cellwise constants behaves like $\\mathcal{O}(h)$. When the data are exact and the discrete forward map is exact on the approximation space (the exact cell-integrated $A_h$), the reconstruction error is expected to exhibit order $p=1$ as $h\\to 0$. With midpoint quadrature, the operator discretization incurs an additional modeling error of order $\\mathcal{O}(h)$, so the overall order remains $p=1$.\n\nYour program must:\n- Use the exact analytical formula for $y(s)$ to avoid the inverse crime. Using the same discrete operator to generate $y$ is forbidden.\n- Assemble $A_h$ both by exact cell integrals and by midpoint quadrature.\n- Solve the LS problems for $x_h^\\ast$ using a numerically stable solver.\n- Compute $E_h$ over a refinement sequence.\n\nTest suite:\n- Case $1$ (happy path): $N\\in\\{16,32,64,128\\}$ using exact cell-integrated $A_h$. Compute the observed order $p_{\\mathrm{obs}}$ from a linear regression of $\\log(E_h)$ versus $\\log(h)$ over the finest three meshes.\n- Case $2$ (consistency under quadrature): $N\\in\\{16,32,64,128\\}$ using midpoint $A_h^{\\mathrm{mid}}$. Compute the observed order $p_{\\mathrm{obs}}^{\\mathrm{mid}}$ in the same way.\n- Case $3$ (edge case monotonicity): verify that $E_h$ is strictly decreasing as $h$ decreases for $N\\in\\{8,16,32,64,128\\}$ with exact $A_h$.\n\nQuantifiable outputs:\n- For Case $1$, return the float $p_{\\mathrm{obs}}$ rounded to three decimals, and a boolean indicating whether $|p_{\\mathrm{obs}}-1|\\le 0.15$.\n- For Case $2$, return the float $p_{\\mathrm{obs}}^{\\mathrm{mid}}$ rounded to three decimals, and a boolean indicating whether $|p_{\\mathrm{obs}}^{\\mathrm{mid}}-1|\\le 0.15$.\n- For Case $3$, return a boolean indicating strict monotonic decrease of $E_h$ with refinement.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"). The required output order is:\n$$\n\\big[ p_{\\mathrm{obs}},\\ \\text{is\\_first\\_order\\_exact},\\ p_{\\mathrm{obs}}^{\\mathrm{mid}},\\ \\text{is\\_first\\_order\\_mid},\\ \\text{is\\_monotone} \\big].\n$$\nNo physical units or angles are involved, so no unit conversion is required. All numbers should be expressed as plain floats or booleans as specified.", "solution": "We formulate the inverse problem from the continuous operator\n$$\n(\\mathcal{K}x)(s) = \\int_0^1 K(s,t)\\,x(t)\\,dt,\\quad K(s,t)=\\min(s,t).\n$$\nThe kernel $K$ is the Green’s kernel for the one-dimensional second-order operator with homogeneous Dirichlet boundary conditions, which maps $L^2(0,1)$ boundedly into $H^2(0,1)$, a smoothing of order $2$. The reconstruction from exact data is ill-posed in infinite dimensions, but when we constrain $x$ to a finite-dimensional space of piecewise constants $\\mathbb{V}_h$, the discrete forward map restricted to $\\mathbb{V}_h$ with exact cell integration is exact on that subspace, thereby isolating the subspace approximation error.\n\nAvoiding the inverse crime requires generating the data $y$ in a way not tied to the discrete operator. We therefore synthesize $y$ using the continuous identity\n$$\ny(s) = \\int_0^s t\\,x_{\\mathrm{true}}(t)\\,dt + s\\int_s^1 x_{\\mathrm{true}}(t)\\,dt,\n$$\nand we choose a smooth manufactured truth\n$$\nx_{\\mathrm{true}}(t)=\\sin(2\\pi t) + \\tfrac{1}{2}\\cos(3\\pi t).\n$$\nFor this $x_{\\mathrm{true}}$, both integrals are computable analytically by elementary calculus. Let $a=2\\pi$ and $b=3\\pi$. We use the identities\n$$\n\\int t\\sin(a t)\\,dt = -\\frac{t\\cos(a t)}{a} + \\frac{\\sin(a t)}{a^2},\\quad\n\\int \\sin(a t)\\,dt = -\\frac{\\cos(a t)}{a},\n$$\n$$\n\\int t\\cos(b t)\\,dt = \\frac{t\\sin(b t)}{b} + \\frac{\\cos(b t)}{b^2},\\quad\n\\int \\cos(b t)\\,dt = \\frac{\\sin(b t)}{b}.\n$$\nThus, for any $s\\in[0,1]$,\n$$\ny(s) = \\left(-\\frac{s\\cos(a s)}{a} + \\frac{\\sin(a s)}{a^2}\\right)\n+ \\tfrac{1}{2}\\left(\\frac{s\\sin(b s)}{b} + \\frac{\\cos(b s)}{b^2} - \\frac{1}{b^2}\\right)\n+ s\\left[ \\left(-\\frac{\\cos(a)}{a}+\\frac{\\cos(a s)}{a}\\right)\n+ \\tfrac{1}{2}\\left(\\frac{\\sin(b)}{b} - \\frac{\\sin(b s)}{b}\\right)\\right].\n$$\nThis $y$ is exact with respect to the continuous model, thereby avoiding the inverse crime.\n\nWe discretize $[0,1]$ with a uniform mesh of size $h=1/N$, cells $I_j=[(j-1)h,jh]$, midpoints $t_j=(j-\\tfrac{1}{2})h$, and set measurement points $s_i=t_i$. The exact cell-integrated forward operator is assembled by the closed-form integral\n$$\n(A_h)_{ij} = \\int_{I_j} \\min(s_i,t)\\,dt =\n\\begin{cases}\ns_i h,  \\text{if } s_i \\le (j-1)h,\\\\\n\\tfrac{1}{2}\\left((jh)^2-((j-1)h)^2\\right),  \\text{if } s_i \\ge jh,\\\\\n\\tfrac{1}{2}\\left(s_i^2-((j-1)h)^2\\right) + s_i\\left(jh - s_i\\right),  \\text{if } s_i\\in((j-1)h,jh).\n\\end{cases}\n$$\nThe midpoint quadrature assembly is\n$$\n(A_h^{\\mathrm{mid}})_{ij} = \\min(s_i,t_j)\\,h.\n$$\nFor each assembly, we solve the least squares (LS) problem\n$$\nx_h^\\ast = \\operatorname*{arg\\,min}_{x\\in\\mathbb{R}^N}\\|A_h x - y_h\\|_2,\n$$\nwhere $y_h=(y(s_1),\\dots,y(s_N))^\\top$ is the exact data sampled at the measurement points. We use a robust numerical LS solver, for example via the Singular Value Decomposition (SVD), as implemented by standard linear algebra routines.\n\nWe then compute the discrete $L^2$ error\n$$\nE_h = \\sqrt{h}\\,\\|x_h^\\ast - x_{\\mathrm{true}}(t_\\cdot)\\|_2.\n$$\nFor a smooth $x_{\\mathrm{true}}$, the best approximation by piecewise constants satisfies\n$$\n\\inf_{v_h\\in\\mathbb{V}_h}\\|x_{\\mathrm{true}}-v_h\\|_{L^2(0,1)} \\le C\\,h\\,\\|x_{\\mathrm{true}}'\\|_{L^2(0,1)},\n$$\nso we expect $E_h=\\mathcal{O}(h)$. With exact cell integration, the discrete forward operator is exact on $\\mathbb{V}_h$, so the reconstruction converges at the same rate as the best approximation. With midpoint quadrature, the operator is additionally perturbed by $\\mathcal{O}(h)$, which does not change the overall order for smooth data: the dominant rate remains $p=1$.\n\nTo verify, we run mesh refinements $N\\in\\{16,32,64,128\\}$ for both assemblies, fit a line to $(\\log(h),\\log(E_h))$ over the finest three meshes, and report the observed slopes $p_{\\mathrm{obs}}$ and $p_{\\mathrm{obs}}^{\\mathrm{mid}}$, which should be close to $1$. As an edge case, we test monotonic decrease of $E_h$ for $N\\in\\{8,16,32,64,128\\}$.\n\nThe program outputs a single line\n$$\n\\big[ p_{\\mathrm{obs}},\\ \\text{is\\_first\\_order\\_exact},\\ p_{\\mathrm{obs}}^{\\mathrm{mid}},\\ \\text{is\\_first\\_order\\_mid},\\ \\text{is\\_monotone} \\big],\n$$\nwhere $p_{\\mathrm{obs}}$ and $p_{\\mathrm{obs}}^{\\mathrm{mid}}$ are rounded to three decimals, $\\text{is\\_first\\_order\\_exact}$ is true if $| p_{\\mathrm{obs}}-1|\\le 0.15$, $\\text{is\\_first\\_order\\_mid}$ is true if $| p_{\\mathrm{obs}}^{\\mathrm{mid}}-1|\\le 0.15$, and $\\text{is\\_monotone}$ is true if the error decreases strictly with refinement for the exact assembly. This confirms the expected order and demonstrates inverse crime avoidance by using analytically synthesized $y$ rather than the discrete operator.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef x_true(t):\n    # x_true(t) = sin(2*pi*t) + 0.5*cos(3*pi*t)\n    return np.sin(2*np.pi*t) + 0.5*np.cos(3*np.pi*t)\n\ndef y_exact(s):\n    # Exact y(s) from analytical integration for x_true(t) = sin(2πt) + 0.5 cos(3πt)\n    a = 2*np.pi\n    b = 3*np.pi\n    s = np.asarray(s)\n    # Components for sin(a t)\n    I1 = -s*np.cos(a*s)/a + np.sin(a*s)/(a*a)\n    J1 = (-np.cos(a)/a) + np.cos(a*s)/a\n    # Components for cos(b t)\n    I2 = s*np.sin(b*s)/b + np.cos(b*s)/(b*b) - 1.0/(b*b)\n    J2 = np.sin(b)/b - np.sin(b*s)/b\n    y = I1 + 0.5*I2 + s*(J1 + 0.5*J2)\n    return y\n\ndef assemble_A_exact(s_points, N):\n    # Assemble A_h with exact cell integrals of min(s_i, t) over each cell I_j.\n    s_points = np.asarray(s_points)\n    h = 1.0 / N\n    A = np.zeros((len(s_points), N), dtype=float)\n    # Cell edges\n    edges = np.linspace(0.0, 1.0, N+1)\n    for i, s in enumerate(s_points):\n        for j in range(N):\n            a = edges[j]\n            b = edges[j+1]\n            if s = a:  # cell entirely to the right of s\n                A[i, j] = s * (b - a)\n            elif s = b:  # cell entirely to the left of s\n                A[i, j] = 0.5 * (b*b - a*a)\n            else:\n                # s is inside the cell (a  s  b)\n                A[i, j] = 0.5*(s*s - a*a) + s*(b - s)\n    return A\n\ndef assemble_A_midpoint(s_points, N):\n    # Assemble A_h^{mid} with midpoint quadrature of min(s_i, t) over each cell I_j.\n    s_points = np.asarray(s_points)\n    h = 1.0 / N\n    mids = (np.arange(N) + 0.5) * h\n    # Compute min(s_i, t_j_mid) for all pairs\n    S = s_points[:, None]\n    T = mids[None, :]\n    A = np.minimum(S, T) * h\n    return A\n\ndef reconstruct_ls(A, y):\n    # Solve least squares min ||A x - y||_2 using robust solver\n    # rcond=None selects machine precision cutoff as default\n    x, *_ = np.linalg.lstsq(A, y, rcond=None)\n    return x\n\ndef compute_error(N, assembly='exact'):\n    # Build s_i = t_i midpoints, assemble A, synthesize y exactly, reconstruct and compute L2 error\n    h = 1.0 / N\n    mids = (np.arange(N) + 0.5) * h\n    y = y_exact(mids)  # exact data sampled at s_i\n    if assembly == 'exact':\n        A = assemble_A_exact(mids, N)\n    elif assembly == 'mid':\n        A = assemble_A_midpoint(mids, N)\n    else:\n        raise ValueError(\"Unknown assembly type\")\n    x_rec = reconstruct_ls(A, y)\n    x_ref = x_true(mids)\n    err = np.sqrt(h) * np.linalg.norm(x_rec - x_ref)\n    return err\n\ndef observed_rate(hs, errs):\n    # Fit slope p in log(err) ~ p*log(h) + c using least squares\n    logh = np.log(hs)\n    loge = np.log(errs)\n    # p = cov(logh, loge)/var(logh)\n    A = np.vstack([logh, np.ones_like(logh)]).T\n    p, c = np.linalg.lstsq(A, loge, rcond=None)[0]\n    return p\n\ndef solve():\n    # Test cases as specified in the problem statement\n    Ns_rate = [16, 32, 64, 128]\n    hs_rate = [1.0/n for n in Ns_rate]\n\n    # Case 1: exact assembly\n    errs_exact = [compute_error(N, 'exact') for N in Ns_rate]\n    # Use finest three meshes for rate\n    hs_exact_fit = np.array(hs_rate[-3:])\n    errs_exact_fit = np.array(errs_exact[-3:])\n    p_exact = observed_rate(hs_exact_fit, errs_exact_fit)\n\n    # Case 2: midpoint assembly\n    errs_mid = [compute_error(N, 'mid') for N in Ns_rate]\n    hs_mid_fit = np.array(hs_rate[-3:])\n    errs_mid_fit = np.array(errs_mid[-3:])\n    p_mid = observed_rate(hs_mid_fit, errs_mid_fit)\n\n    # Case 3: monotonicity with exact assembly\n    Ns_mono = [8, 16, 32, 64, 128]\n    errs_mono = [compute_error(N, 'exact') for N in Ns_mono]\n    is_monotone = all(errs_mono[k+1]  errs_mono[k] for k in range(len(errs_mono)-1))\n\n    # Checks against expected first-order\n    tol = 0.15\n    is_first_order_exact = abs(p_exact - 1.0) = tol\n    is_first_order_mid = abs(p_mid - 1.0) = tol\n\n    results = [\n        round(float(p_exact), 3),\n        bool(is_first_order_exact),\n        round(float(p_mid), 3),\n        bool(is_first_order_mid),\n        bool(is_monotone),\n    ]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3376948"}]}