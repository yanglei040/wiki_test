{"hands_on_practices": [{"introduction": "The Singular Value Decomposition (SVD) is an indispensable tool for analyzing linear inverse problems in finite-dimensional spaces. It diagonalizes the forward operator, breaking down a complex coupled system into a simple set of scalar multiplications. This practice guides you through deriving the minimum-norm solution from first principles, revealing precisely how the operator's singular values govern the stability of the solution. By completing this exercise [@problem_id:3387743], you will gain a fundamental understanding of how small singular values act as amplifiers for noise in the data, a key mechanism behind ill-posedness.", "problem": "Let $K \\in \\mathbb{R}^{m \\times n}$ be a real matrix with rank $r \\leq \\min\\{m,n\\}$, and let its singular value decomposition (SVD) be $K = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative entries ordered as $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r} > 0$ followed by zeros. Consider the linear inverse problem $K x = y$ with $y \\in \\mathbb{R}^{m}$ and the minimum-norm solution $x^{\\dagger} := \\arg\\min \\{\\|x\\|_{2} : K x = y\\}$.\n\nStarting only from core definitions (orthogonality, the SVD, the Euclidean $2$-norm, and the definition of the minimum-norm solution), perform the following:\n\n- Derive the representation of $x^{\\dagger}$ in terms of the singular triplets $\\{(u_{i}, \\sigma_{i}, v_{i})\\}_{i=1}^{r}$.\n- Using the Hadamard criteria (existence, uniqueness, and continuous dependence on data), determine conditions on $y$ and on $\\sigma_{r}$ under which the mapping from data to solution is well-posed. Then, for perturbations $y^{\\delta} = y + \\varepsilon$, derive the Lipschitz constant $L$ of the data-to-solution map $y \\mapsto x^{\\dagger}$ with respect to the Euclidean $2$-norm, restricted to $y \\in \\operatorname{range}(K)$.\n- Explain, in the right singular vector basis, how each coefficient of $x^{\\dagger}$ depends on the ratio $1/\\sigma_{i}$ and why the smallest positive singular value $\\sigma_{r}$ governs worst-case noise amplification.\n\nExpress your final answer as the exact analytical expression for the Lipschitz constant $L$ in terms of the singular values. No rounding is required. Report only the requested expression for $L$ as your final answer.", "solution": "The problem is valid. We proceed with the derivation and analysis as requested.\n\nThe task is to analyze the linear inverse problem $Kx=y$ where $K \\in \\mathbb{R}^{m \\times n}$ is a real matrix with $\\operatorname{rank}(K) = r \\leq \\min\\{m,n\\}$. We are given the singular value decomposition (SVD) of $K$ as $K = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix of singular values $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r} > 0$ and $\\sigma_i = 0$ for $i > r$. The columns of $U$ are denoted by $\\{u_i\\}_{i=1}^m$ and the columns of $V$ by $\\{v_i\\}_{i=1}^n$.\n\n### Derivation of the Minimum-Norm Solution $x^{\\dagger}$\n\nWe seek the solution $x^{\\dagger}$ defined as $x^{\\dagger} := \\arg\\min \\{\\|x\\|_{2} : K x = y\\}$. This definition requires that the set of solutions $\\{x : Kx = y\\}$ is non-empty, which means we assume $y \\in \\operatorname{range}(K)$.\n\nWe start by substituting the SVD into the equation $Kx = y$:\n$$U \\Sigma V^{\\top} x = y$$\nSince $U$ is an orthogonal matrix, its inverse is its transpose, $U^{-1} = U^{\\top}$. We multiply both sides on the left by $U^{\\top}$:\n$$U^{\\top} (U \\Sigma V^{\\top} x) = U^{\\top} y$$\n$$(U^{\\top} U) \\Sigma V^{\\top} x = U^{\\top} y$$\n$$I_m \\Sigma V^{\\top} x = U^{\\top} y$$\n$$\\Sigma (V^{\\top} x) = U^{\\top} y$$\nLet's introduce a change of basis for the solution $x \\in \\mathbb{R}^n$ and the data $y \\in \\mathbb{R}^m$. We define new coordinates $\\alpha \\in \\mathbb{R}^n$ and $\\beta \\in \\mathbb{R}^m$ as:\n$$\\alpha = V^{\\top} x \\quad (\\text{so } x = V \\alpha \\text{ since } V \\text{ is orthogonal})$$\n$$\\beta = U^{\\top} y \\quad (\\text{so } y = U \\beta \\text{ since } U \\text{ is orthogonal})$$\nThe components of these vectors are $\\alpha_i = v_i^{\\top} x$ and $\\beta_i = u_i^{\\top} y$. The equation $\\Sigma (V^{\\top} x) = U^{\\top} y$ simplifies to:\n$$\\Sigma \\alpha = \\beta$$\nIn component form, this equation reads $\\sigma_i \\alpha_i = \\beta_i$ for $i=1, \\dots, \\min\\{m, n\\}$.\n\nWe must analyze two cases for the indices $i$:\n1.  For $i=1, \\dots, r$, the singular values are positive, $\\sigma_i > 0$. Thus, the coefficients $\\alpha_i$ are uniquely determined:\n    $$\\alpha_i = \\frac{\\beta_i}{\\sigma_i} = \\frac{u_i^{\\top} y}{\\sigma_i}$$\n2.  For $i > r$, the singular values are zero, $\\sigma_i = 0$. The equations become $0 \\cdot \\alpha_i = \\beta_i$.\n    For a solution to exist, it is necessary that $\\beta_i = u_i^{\\top} y = 0$ for all $i > r$. This condition is equivalent to stating that $y$ must be orthogonal to the vectors $\\{u_{r+1}, \\dots, u_m\\}$, which form a basis for the null space of $K^{\\top}$, $\\operatorname{null}(K^{\\top})$. This is the same as the condition $y \\in \\operatorname{range}(K)$, as we initially assumed.\n    For these indices $i > r$, the coefficients $\\alpha_i$ are not determined by the equation and can be any real numbers.\n\nAny solution $x$ can be written in the basis $\\{v_i\\}$ as $x = \\sum_{i=1}^{n} \\alpha_i v_i$. Substituting the derived coefficients gives the general form of a solution:\n$$x = \\sum_{i=1}^{r} \\left(\\frac{u_i^{\\top} y}{\\sigma_i}\\right) v_i + \\sum_{i=r+1}^{n} \\alpha_i v_i$$\nThe second sum represents an arbitrary vector from the null space of $K$, since for $i>r$, $K v_i = \\sigma_i u_i = 0$.\n\nNow, we find the minimum-norm solution $x^{\\dagger}$ by minimizing $\\|x\\|_2$. Since $V$ is an orthogonal matrix, it preserves the Euclidean norm:\n$$\\|x\\|_{2}^{2} = \\|V\\alpha\\|_{2}^{2} = \\alpha^{\\top}V^{\\top}V\\alpha = \\alpha^{\\top}\\alpha = \\|\\alpha\\|_{2}^{2}$$\nThe squared norm is:\n$$\\|x\\|_{2}^{2} = \\sum_{i=1}^{n} \\alpha_i^2 = \\sum_{i=1}^{r} \\left(\\frac{u_i^{\\top} y}{\\sigma_i}\\right)^2 + \\sum_{i=r+1}^{n} \\alpha_i^2$$\nTo minimize this norm, we must choose the arbitrary coefficients $\\alpha_i$ for $i > r$ to be zero. That is, $\\alpha_i = 0$ for $i=r+1, \\dots, n$. This choice eliminates any component from the null space of $K$, making the solution orthogonal to $\\operatorname{null}(K)$.\nThe minimum-norm solution is therefore:\n$$x^{\\dagger} = \\sum_{i=1}^{r} \\frac{u_i^{\\top} y}{\\sigma_i} v_i$$\nThis is the representation of $x^{\\dagger}$ in terms of the singular triplets $\\{(u_{i}, \\sigma_{i}, v_{i})\\}_{i=1}^{r}$.\n\n### Hadamard Well-Posedness and Lipschitz Constant\n\nThe Hadamard criteria for a well-posed problem are existence, uniqueness, and continuous dependence of the solution on the data.\n1.  **Existence**: A solution to $Kx=y$ exists if and only if $y \\in \\operatorname{range}(K)$. For an arbitrary $y \\in \\mathbb{R}^m$, a solution may not exist. Thus, the problem is ill-posed in existence.\n2.  **Uniqueness**: If $\\operatorname{rank}(K) = r  n$, the null space of $K$ is non-trivial. In this case, if a solution exists, there are infinitely many solutions. Thus, the problem is ill-posed in uniqueness.\n\nHowever, if we reformulate the problem as finding the unique minimum-norm solution $x^{\\dagger}$ for any data $y \\in \\mathbb{R}^m$, this new problem has a unique solution given by $x^{\\dagger} = K^{\\dagger} y$, where $K^{\\dagger} = V \\Sigma^{\\dagger} U^{\\top}$ is the Moore-Penrose pseudoinverse, and $\\Sigma^{\\dagger}$ is an $n \\times m$ matrix with entries $1/\\sigma_i$ for $i=1,\\dots,r$ and zeros elsewhere. Our derived formula for $x^{\\dagger}$ is equivalent to this for $y \\in \\operatorname{range}(K)$. For $y \\notin \\operatorname{range}(K)$, $K^{\\dagger}y$ yields the minimum-norm least-squares solution. The problem of finding $x^{\\dagger}$ is always well-posed in existence and uniqueness.\n\n3.  **Continuous Dependence (Stability)**: We examine the stability of the mapping $G: y \\mapsto x^{\\dagger}$ for $y \\in \\operatorname{range}(K)$. Let $y_1, y_2 \\in \\operatorname{range}(K)$ and let $x_1^{\\dagger} = G(y_1)$, $x_2^{\\dagger} = G(y_2)$. The map is linear, as $x^{\\dagger}$ is a linear function of $y$. We want to find the Lipschitz constant $L$ of this map, which for a linear map is its operator norm.\n$$L = \\sup_{y \\in \\operatorname{range}(K), y \\neq 0} \\frac{\\|G(y)\\|_2}{\\|y\\|_2} = \\sup_{y \\in \\operatorname{range}(K), y \\neq 0} \\frac{\\|x^{\\dagger}\\|_2}{\\|y\\|_2}$$\nUsing the expressions for $x^{\\dagger}$ and $y$ in their respective singular bases:\n$x^{\\dagger} = \\sum_{i=1}^{r} \\frac{u_i^{\\top} y}{\\sigma_i} v_i$. Since $\\{v_i\\}$ are orthonormal, $\\|x^{\\dagger}\\|_2^2 = \\sum_{i=1}^{r} \\left(\\frac{u_i^{\\top} y}{\\sigma_i}\\right)^2$.\nSince $y \\in \\operatorname{range}(K)$, $y$ can be written as $y = \\sum_{i=1}^{r} (u_i^{\\top} y) u_i$. Since $\\{u_i\\}_{i=1}^r$ are orthonormal, $\\|y\\|_2^2 = \\sum_{i=1}^{r} (u_i^{\\top} y)^2$.\n\nThe ratio of squared norms is:\n$$\\frac{\\|x^{\\dagger}\\|_2^2}{\\|y\\|_2^2} = \\frac{\\sum_{i=1}^{r} (u_i^{\\top} y)^2 / \\sigma_i^2}{\\sum_{j=1}^{r} (u_j^{\\top} y)^2}$$\nSince $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r > 0$, we have $1/\\sigma_i^2 \\leq 1/\\sigma_r^2$ for all $i=1, \\dots, r$.\n$$\\frac{\\sum_{i=1}^{r} (u_i^{\\top} y)^2 / \\sigma_i^2}{\\sum_{j=1}^{r} (u_j^{\\top} y)^2} \\leq \\frac{\\sum_{i=1}^{r} (u_i^{\\top} y)^2 / \\sigma_r^2}{\\sum_{j=1}^{r} (u_j^{\\top} y)^2} = \\frac{1}{\\sigma_r^2} \\frac{\\sum_{i=1}^{r} (u_i^{\\top} y)^2}{\\sum_{j=1}^{r} (u_j^{\\top} y)^2} = \\frac{1}{\\sigma_r^2}$$\nThis shows that $\\frac{\\|x^{\\dagger}\\|_2}{\\|y\\|_2} \\leq \\frac{1}{\\sigma_r}$. To show that the supremum is exactly $1/\\sigma_r$, we must find a vector $y$ for which this bound is achieved. Let us choose $y = u_r$. This vector is in $\\operatorname{range}(K)$. For this choice:\n$u_i^{\\top} y = u_i^{\\top} u_r = \\delta_{ir}$ (the Kronecker delta).\nThen $\\|y\\|_2^2 = \\|u_r\\|_2^2 = 1$.\nAnd $\\|x^{\\dagger}\\|_2^2 = \\sum_{i=1}^{r} (\\delta_{ir}/\\sigma_i)^2 = (1/\\sigma_r)^2$.\nThus, for $y = u_r$, we have $\\frac{\\|x^{\\dagger}\\|_2}{\\|y\\|_2} = \\frac{1/\\sigma_r}{1} = \\frac{1}{\\sigma_r}$.\nSince we found a vector that achieves the upper bound, the supremum is this value. The Lipschitz constant of the data-to-solution map $y \\mapsto x^{\\dagger}$ restricted to $y \\in \\operatorname{range}(K)$ is:\n$$L = \\frac{1}{\\sigma_r}$$\nThe map is continuous as long as $\\sigma_r > 0$. However, if $\\sigma_r$ is very small, the Lipschitz constant $L$ is very large, indicating that small perturbations in the data $y$ can lead to large changes in the solution $x^{\\dagger}$. This is the characteristic feature of an ill-posed (or, more accurately, ill-conditioned) problem.\n\n### Noise Amplification Analysis\n\nThe solution $x^{\\dagger}$ is expressed in the basis of right singular vectors $\\{v_1, \\dots, v_n\\}$ as:\n$$x^{\\dagger} = \\sum_{i=1}^{r} \\alpha_i v_i, \\quad \\text{where the coefficients are } \\alpha_i = \\frac{u_i^{\\top} y}{\\sigma_i}$$\nEach coefficient $\\alpha_i$ of the solution is determined by the projection of the data $y$ onto the corresponding left singular vector $u_i$, scaled by the inverse of the singular value, $1/\\sigma_i$.\n\nNow, consider perturbed data $y^{\\delta} = y + \\varepsilon$, where $\\varepsilon$ is some noise or error. The new minimum-norm solution is $x^{\\delta\\dagger} = K^{\\dagger} y^{\\delta}$. The error in the solution is $\\Delta x = x^{\\delta\\dagger} - x^{\\dagger} = K^{\\dagger}(y+\\varepsilon) - K^{\\dagger}y = K^{\\dagger}\\varepsilon$.\nUsing the formula for the pseudoinverse action:\n$$\\Delta x = \\sum_{i=1}^{r} \\frac{u_i^{\\top} \\varepsilon}{\\sigma_i} v_i$$\nThe coefficient of the solution error along the direction $v_i$ is $\\frac{u_i^{\\top} \\varepsilon}{\\sigma_i}$. This shows that the component of the data noise in the direction $u_i$, which is $u_i^{\\top} \\varepsilon$, is amplified by the factor $1/\\sigma_i$ to produce the corresponding component of the solution error in the direction $v_i$.\n\nThe amplification factors are $\\{1/\\sigma_1, 1/\\sigma_2, \\dots, 1/\\sigma_r\\}$. Since $\\sigma_1 \\geq \\dots \\geq \\sigma_r > 0$, the largest of these factors is $1/\\sigma_r$. Therefore, the smallest positive singular value $\\sigma_r$ governs the worst-case noise amplification. Any noise component $\\varepsilon$ that has a non-zero projection onto $u_r$ will be amplified by the largest factor $1/\\sigma_r$. If $\\sigma_r$ is close to zero, this amplification can be enormous, destabilizing the solution completely. This demonstrates why the magnitude of the smallest non-zero singular value is a critical measure of the ill-conditioning of the inverse problem.", "answer": "$$\\boxed{\\frac{1}{\\sigma_{r}}}$$", "id": "3387743"}, {"introduction": "While the SVD provides a complete picture for matrices, many scientific problems are naturally posed in infinite-dimensional function spaces. This exercise presents a classic example of an ill-posed problem involving a simple multiplication operator on the space $L^2(0,1)$. By constructing a specific sequence of functions [@problem_id:3387707], you will demonstrate concretely how the solution's norm can grow without bound even as the corresponding data norm vanishes. This vividly illustrates that for an inverse problem to be stable, its inverse operator must be bounded—a condition that is not guaranteed by injectivity or a dense range.", "problem": "Consider the linear inverse problem $T f = g$ posed on the Lebesgue space $L^{2}(0,1)$, where the forward operator $T : L^{2}(0,1) \\to L^{2}(0,1)$ is defined by $(T f)(x) = x f(x)$ for $x \\in (0,1)$. In the sense of Hadamard well-posedness, stability refers to continuous dependence of the solution on the data, which for a linear inverse problem is equivalent to the boundedness of the inverse operator $T^{-1}$ on $\\operatorname{Ran}(T)$, the range of $T$. \n\nStarting from the core definitions of injectivity, dense range, and stability, and using only standard properties of $L^{2}(0,1)$ and the $L^{2}$-norm, do the following:\n\n- Establish that $T$ is injective on $L^{2}(0,1)$.\n- Establish that $\\operatorname{Ran}(T)$ is dense in $L^{2}(0,1)$.\n- Demonstrate that stability fails by constructing a concrete sequence $\\{f_{n}\\}_{n\\in\\mathbb{N}} \\subset L^{2}(0,1)$ for which the associated data $g_{n} = T f_{n}$ satisfies $\\|g_{n}\\|_{L^{2}(0,1)} \\to 0$ while simultaneously $\\|f_{n}\\|_{L^{2}(0,1)} \\to \\infty$. Your construction must be explicit and justified from first principles.\n\nFor the specific sequence given by $f_{n}(x) = n \\,\\chi_{(0,1/n)}(x)$, where $\\chi_{(0,1/n)}$ is the indicator function of the interval $(0,1/n)$, compute the exact analytic expression for the amplification factor\n$$\nA_{n} \\equiv \\frac{\\|f_{n}\\|_{L^{2}(0,1)}}{\\|T f_{n}\\|_{L^{2}(0,1)}}\n$$\nas a closed form in $n$. Provide the expression for $A_{n}$; no rounding is required.", "solution": "The problem asks for an analysis of the linear operator $T: L^{2}(0,1) \\to L^{2}(0,1)$ defined by $(Tf)(x) = xf(x)$ for $x \\in (0,1)$. The space $L^{2}(0,1)$ is the space of square-integrable functions on the interval $(0,1)$, equipped with the inner product $\\langle f,g \\rangle = \\int_0^1 f(x)\\overline{g(x)}dx$ and the associated norm $\\|f\\|_{L^2} = \\left(\\int_0^1 |f(x)|^2 dx\\right)^{1/2}$.\n\nFirst, we establish that the operator $T$ is injective. An operator $T$ is injective if its null space (or kernel) contains only the zero element. For a linear operator, this is equivalent to showing that $Tf = 0$ implies $f = 0$.\nLet $f \\in L^2(0,1)$ be such that $Tf = 0$. By definition, this means $(Tf)(x) = 0$ for almost every $x \\in (0,1)$.\nSubstituting the definition of the operator $T$, we have $x f(x) = 0$ for almost every $x \\in (0,1)$.\nFor any $x$ in the open interval $(0,1)$, we have $x \\neq 0$. Therefore, for the product $xf(x)$ to be zero, it must be that $f(x)=0$. This holds for almost every $x \\in (0,1)$. A function that is zero almost everywhere on its domain is the zero element in the space $L^2(0,1)$. Thus, $f = 0$ in $L^2(0,1)$.\nThe null space of $T$ is $\\ker(T) = \\{0\\}$, which proves that $T$ is injective.\n\nSecond, we establish that the range of $T$, denoted $\\operatorname{Ran}(T)$, is dense in $L^2(0,1)$. To prove this, we must show that for any arbitrary function $h \\in L^2(0,1)$ and any $\\epsilon  0$, there exists a function $g \\in \\operatorname{Ran}(T)$ such that $\\|h - g\\|_{L^2}  \\epsilon$.\nLet $h \\in L^2(0,1)$ be arbitrary. For any $\\delta \\in (0,1)$, let us define a function $g_{\\delta}$ as $g_{\\delta}(x) = h(x) \\chi_{(\\delta,1)}(x)$, where $\\chi_{(\\delta,1)}$ is the indicator function for the interval $(\\delta,1)$.\nWe wish to show that $g_{\\delta} \\in \\operatorname{Ran}(T)$. This requires finding a function $f_{\\delta} \\in L^2(0,1)$ such that $(T f_{\\delta})(x) = g_{\\delta}(x)$, which means $x f_{\\delta}(x) = g_{\\delta}(x)$. We can define $f_{\\delta}(x) = \\frac{g_{\\delta}(x)}{x} = \\frac{h(x)}{x} \\chi_{(\\delta,1)}(x)$.\nWe must verify that this $f_{\\delta}$ is indeed in $L^2(0,1)$. We compute its norm:\n$$\n\\|f_{\\delta}\\|_{L^2}^2 = \\int_0^1 |f_{\\delta}(x)|^2 dx = \\int_0^1 \\left|\\frac{h(x)}{x} \\chi_{(\\delta,1)}(x)\\right|^2 dx = \\int_{\\delta}^1 \\frac{|h(x)|^2}{x^2} dx\n$$\nOn the interval of integration $[\\delta,1]$, we have $x \\ge \\delta  0$, which implies $\\frac{1}{x^2} \\le \\frac{1}{\\delta^2}$. Therefore,\n$$\n\\|f_{\\delta}\\|_{L^2}^2 \\le \\int_{\\delta}^1 \\frac{|h(x)|^2}{\\delta^2} dx = \\frac{1}{\\delta^2} \\int_{\\delta}^1 |h(x)|^2 dx \\le \\frac{1}{\\delta^2} \\int_0^1 |h(x)|^2 dx = \\frac{1}{\\delta^2} \\|h\\|_{L^2}^2\n$$\nSince $h \\in L^2(0,1)$, its norm $\\|h\\|_{L^2}$ is finite. For any fixed $\\delta  0$, $\\|f_{\\delta}\\|_{L^2}^2$ is finite, so $f_{\\delta} \\in L^2(0,1)$. This confirms that $g_{\\delta} \\in \\operatorname{Ran}(T)$ for any $\\delta \\in (0,1)$.\nNow we examine the distance between $h$ and our approximating function $g_{\\delta}$:\n$$\n\\|h - g_{\\delta}\\|_{L^2}^2 = \\int_0^1 |h(x) - g_{\\delta}(x)|^2 dx = \\int_0^1 |h(x) - h(x)\\chi_{(\\delta,1)}(x)|^2 dx = \\int_0^1 |h(x)(1 - \\chi_{(\\delta,1)}(x))|^2 dx\n$$\nSince $1 - \\chi_{(\\delta,1)}(x) = \\chi_{[0,\\delta]}(x)$ for $x \\in [0,1]$, we have:\n$$\n\\|h - g_{\\delta}\\|_{L^2}^2 = \\int_0^1 |h(x)\\chi_{[0,\\delta]}(x)|^2 dx = \\int_0^{\\delta} |h(x)|^2 dx\n$$\nThe function $F(y) = \\int_0^y |h(x)|^2 dx$ is an absolutely continuous function of $y$. Because $|h|^2$ is an integrable function (since $h \\in L^2$), we have $\\lim_{\\delta \\to 0^+} \\int_0^{\\delta} |h(x)|^2 dx = 0$.\nThus, for any given $\\epsilon  0$, we can choose a $\\delta  0$ small enough such that $\\|h - g_{\\delta}\\|_{L^2}^2  \\epsilon^2$, which means $\\|h - g_{\\delta}\\|_{L^2}  \\epsilon$. We have found an element $g_{\\delta}$ in $\\operatorname{Ran}(T)$ that is arbitrarily close to $h$. This proves that $\\operatorname{Ran}(T)$ is dense in $L^2(0,1)$.\n\nThird, we demonstrate that the inverse problem lacks stability. Stability in this context means the inverse operator $T^{-1}: \\operatorname{Ran}(T) \\to L^2(0,1)$ is bounded. An operator is bounded if there exists a constant $C$ such that $\\|T^{-1}g\\|_{L^2} \\le C \\|g\\|_{L^2}$ for all $g \\in \\operatorname{Ran}(T)$. We will show $T^{-1}$ is unbounded by constructing a sequence of functions $\\{f_n\\}_{n\\in\\mathbb{N}}$ such that the ratio $\\frac{\\|f_n\\|_{L^2}}{\\|Tf_n\\|_{L^2}}$ is not bounded.\nLet $f_n(x) = n \\chi_{(0,1/n)}(x)$, where $n \\in \\mathbb{N}$ and $n \\ge 2$.\nWe compute the $L^2$-norm of $f_n$:\n$$\n\\|f_n\\|_{L^2}^2 = \\int_0^1 |f_n(x)|^2 dx = \\int_0^{1/n} n^2 dx = n^2 [x]_0^{1/n} = n^2 \\left(\\frac{1}{n}\\right) = n\n$$\nSo, $\\|f_n\\|_{L^2} = \\sqrt{n}$. As $n \\to \\infty$, $\\|f_n\\|_{L^2} \\to \\infty$.\n\nNext, we compute the corresponding data $g_n = Tf_n$ and its norm.\n$$\ng_n(x) = (Tf_n)(x) = x f_n(x) = x(n \\chi_{(0,1/n)}(x)) = nx \\chi_{(0,1/n)}(x)\n$$\nThe $L^2$-norm of $g_n$ is:\n$$\n\\|g_n\\|_{L^2}^2 = \\|Tf_n\\|_{L^2}^2 = \\int_0^1 |g_n(x)|^2 dx = \\int_0^{1/n} (nx)^2 dx = n^2 \\int_0^{1/n} x^2 dx\n$$\n$$\n\\|Tf_n\\|_{L^2}^2 = n^2 \\left[\\frac{x^3}{3}\\right]_0^{1/n} = n^2 \\left(\\frac{(1/n)^3}{3}\\right) = n^2 \\left(\\frac{1}{3n^3}\\right) = \\frac{1}{3n}\n$$\nSo, $\\|Tf_n\\|_{L^2} = \\sqrt{\\frac{1}{3n}} = \\frac{1}{\\sqrt{3n}}$. As $n \\to \\infty$, $\\|Tf_n\\|_{L^2} \\to 0$.\n\nWe have constructed a sequence $\\{f_n\\}$ whose norm tends to infinity, while the norm of the data $\\{Tf_n\\}$ tends to zero. This demonstrates that small perturbations in the data can lead to arbitrarily large perturbations in the solution, which is the hallmark of instability. The operator $T^{-1}$ is unbounded because the ratio $\\frac{\\|f_n\\|_{L^2}}{\\|Tf_n\\|_{L^2}}$ grows without bound as $n \\to \\infty$.\n\nFinally, we compute the exact analytic expression for the amplification factor $A_n$:\n$$\nA_n \\equiv \\frac{\\|f_n\\|_{L^2}}{\\|T f_n\\|_{L^2}}\n$$\nUsing our previously computed norms:\n$$\nA_n = \\frac{\\sqrt{n}}{\\frac{1}{\\sqrt{3n}}} = \\sqrt{n} \\cdot \\sqrt{3n} = \\sqrt{3n^2} = n\\sqrt{3}\n$$\nAs $n \\to \\infty$, $A_n \\to \\infty$, explicitly confirming the unboundedness of the inverse operator.", "answer": "$$\n\\boxed{n\\sqrt{3}}\n$$", "id": "3387707"}, {"introduction": "A profound challenge in computational science is that a theoretically well-posed continuous problem can become unstable upon discretization. This hands-on practice explores this phenomenon within the context of a saddle-point problem, a structure that is central to constrained optimization, mixed finite element methods, and data assimilation. You will analyze a system that is well-posed in its continuous form but becomes ill-posed due to an incompatible choice of discrete subspaces [@problem_id:3387748]. Calculating the discrete inf-sup (LBB) constant will provide a definitive diagnosis of this instability, underscoring the critical importance of choosing stable numerical discretizations.", "problem": "Consider the following data assimilation-inspired linear saddle-point formulation posed on finite-dimensional Hilbert spaces. Let $V = \\mathbb{R}^{2}$ and $Q = \\mathbb{R}^{2}$ be equipped with the Euclidean norm and inner product. Define the symmetric bilinear form $a : V \\times V \\to \\mathbb{R}$ by $a(u,v) = u^{\\top} v$, and the coupling bilinear form $b : V \\times Q \\to \\mathbb{R}$ by $b(v,q) = v^{\\top} q$. The associated continuous saddle-point problem is: given data functionals $f \\in V^{\\ast}$ and $g \\in Q^{\\ast}$, find $(u,p) \\in V \\times Q$ such that\n$$\na(u,v) + b(v,p) = f(v) \\quad \\text{for all } v \\in V, \\qquad b(u,q) = g(q) \\quad \\text{for all } q \\in Q.\n$$\nThis abstract setting represents, for example, a linearized constrained inverse problem in which $u$ denotes the control/state increment and $p$ enforces a linear constraint via a Lagrange multiplier.\n\nYou will analyze well-posedness in the sense of Jacques Hadamard (existence, uniqueness, and continuous dependence on data) using the Ladyzhenskaya–Babuška–Brezzi (LBB) inf-sup condition. The continuous inf-sup constant is defined by\n$$\n\\beta := \\inf_{0 \\neq q \\in Q} \\ \\sup_{0 \\neq v \\in V} \\ \\frac{b(v,q)}{\\|v\\|_{V} \\, \\|q\\|_{Q}}.\n$$\n\nNow consider a Galerkin discretization with trial and test subspaces $V_{h} \\subset V$ and $Q_{h} \\subset Q$ given by\n$$\nV_{h} = \\operatorname{span}\\{v_{1}\\}, \\quad \\text{where } v_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\qquad Q_{h} = \\operatorname{span}\\{q_{1}, q_{2}\\}, \\quad \\text{where } q_{1} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\ q_{2} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nThe discrete LBB constant is defined by\n$$\n\\beta_{h} := \\inf_{0 \\neq q \\in Q_{h}} \\ \\sup_{0 \\neq v \\in V_{h}} \\ \\frac{b(v,q)}{\\|v\\|_{V} \\, \\|q\\|_{Q}}.\n$$\n\nStarting from the fundamental definitions of Hadamard well-posedness and the LBB inf-sup condition, justify why the continuous problem is well-posed and then determine the discrete inf-sup constant $\\beta_{h}$ for the specified subspaces. Express your final answer as a single real number. No rounding is required.", "solution": "The problem asks for an analysis of the well-posedness of a continuous saddle-point problem and the computation of the discrete inf-sup constant for a given Galerkin discretization.\n\nFirst, we address the well-posedness of the continuous problem. The problem is to find $(u,p) \\in V \\times Q$ such that\n$$\na(u,v) + b(v,p) = f(v) \\quad \\text{for all } v \\in V,\n$$\n$$\nb(u,q) = g(q) \\quad \\text{for all } q \\in Q.\n$$\nThe well-posedness in the sense of Hadamard (existence, uniqueness, and continuous dependence on data) for this class of problems is established by the Ladyzhenskaya–Babuška–Brezzi (LBB) theory, also known as Brezzi's Theorem. This theorem requires three conditions to be met.\n\n1.  **Continuity of the bilinear forms $a(\\cdot, \\cdot)$ and $b(\\cdot, \\cdot)$**:\n    The forms must be bounded. That is, there must exist positive constants $C_a$ and $C_b$ such that\n    $$\n    |a(u,v)| \\le C_a \\|u\\|_{V} \\|v\\|_{V} \\quad \\text{for all } u, v \\in V,\n    $$\n    $$\n    |b(v,q)| \\le C_b \\|v\\|_{V} \\|q\\|_{Q} \\quad \\text{for all } v \\in V, q \\in Q.\n    $$\n    In this problem, $V = \\mathbb{R}^2$ and $Q = \\mathbb{R}^2$ are equipped with the Euclidean norm $\\| \\cdot \\|$, and the bilinear forms are defined as the standard dot product, $a(u,v) = u^{\\top}v$ and $b(v,q) = v^{\\top}q$. By the Cauchy-Schwarz inequality, we have:\n    $$\n    |a(u,v)| = |u^{\\top}v| \\le \\|u\\|_{V} \\|v\\|_{V},\n    $$\n    $$\n    |b(v,q)| = |v^{\\top}q| \\le \\|v\\|_{V} \\|q\\|_{Q}.\n    $$\n    These inequalities hold with continuity constants $C_a = 1$ and $C_b = 1$. Thus, both bilinear forms are continuous.\n\n2.  **Coercivity of $a(\\cdot, \\cdot)$ on the kernel of the operator associated with $b(\\cdot, \\cdot)$**:\n    Let $Z$ be the kernel of the operator $B: V \\to Q^{\\ast}$ defined by $(Bv)(q) = b(v,q)$. The kernel is $Z = \\{v \\in V \\mid b(v,q) = 0 \\text{ for all } q \\in Q\\}$. We require that $a(\\cdot, \\cdot)$ be coercive on this subspace $Z$. That is, there must exist a constant $\\alpha > 0$ such that\n    $$\n    a(v,v) \\ge \\alpha \\|v\\|_{V}^{2} \\quad \\text{for all } v \\in Z.\n    $$\n    For the given problem, $b(v,q) = v^{\\top}q = 0$ for all $q \\in Q = \\mathbb{R}^2$. This implies that the vector $v$ must be orthogonal to every vector in $\\mathbb{R}^2$. The only vector with this property is the zero vector, $v=0$. Therefore, the kernel is the trivial subspace $Z = \\{0\\}$. The coercivity condition is trivially satisfied for the only element $v=0$ for any choice of $\\alpha > 0$.\n\n3.  **The continuous inf-sup (LBB) condition**:\n    The bilinear form $b(\\cdot, \\cdot)$ must satisfy the inf-sup condition. That is, the constant\n    $$\n    \\beta := \\inf_{0 \\neq q \\in Q} \\ \\sup_{0 \\neq v \\in V} \\frac{b(v,q)}{\\|v\\|_{V} \\|q\\|_{Q}}\n    $$\n    must be strictly positive.\n    For the given problem, we have:\n    $$\n    \\beta = \\inf_{0 \\neq q \\in \\mathbb{R}^2} \\ \\sup_{0 \\neq v \\in \\mathbb{R}^2} \\frac{v^{\\top}q}{\\|v\\| \\|q\\|}.\n    $$\n    For any fixed non-zero $q \\in \\mathbb{R}^2$, the expression inside the supremum is maximized when $v$ is chosen to be collinear with $q$. Let us choose $v=q$. The expression becomes:\n    $$\n    \\frac{q^{\\top}q}{\\|q\\| \\|q\\|} = \\frac{\\|q\\|^2}{\\|q\\|^2} = 1.\n    $$\n    By the Cauchy-Schwarz inequality, this is the maximum possible value. Therefore, for any non-zero $q$, the supremum is $1$.\n    $$\n    \\sup_{0 \\neq v \\in V} \\frac{v^{\\top}q}{\\|v\\| \\|q\\|} = 1.\n    $$\n    The infimum over all non-zero $q$ is then $\\beta = \\inf_{0 \\neq q \\in Q} (1) = 1$.\n    Since $\\beta = 1 > 0$, the LBB condition is satisfied.\n\nSince all three conditions of the Brezzi theorem are satisfied, the continuous saddle-point problem is well-posed. This guarantees the existence of a unique solution $(u,p)$ for any given data $(f,g)$, and the solution depends continuously on the data.\n\nNext, we determine the discrete inf-sup constant $\\beta_{h}$. The discrete problem is posed on the subspaces $V_{h} \\subset V$ and $Q_{h} \\subset Q$. The discrete inf-sup constant is defined as:\n$$\n\\beta_{h} := \\inf_{0 \\neq q_{h} \\in Q_{h}} \\ \\sup_{0 \\neq v_{h} \\in V_{h}} \\frac{b(v_{h},q_{h})}{\\|v_{h}\\|_{V} \\|q_{h}\\|_{Q}}.\n$$\nThe subspaces are given by:\n$$\nV_{h} = \\operatorname{span}\\{v_{1}\\}, \\quad \\text{where } v_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix},\n$$\n$$\nQ_{h} = \\operatorname{span}\\{q_{1}, q_{2}\\}, \\quad \\text{where } q_{1} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\ q_{2} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nAny vector $v_h \\in V_h$ can be written as $v_h = c v_1$ for some scalar $c \\in \\mathbb{R}$. The vectors $q_1$ and $q_2$ are linearly independent and span $\\mathbb{R}^2$, so $Q_h = \\mathbb{R}^2 = Q$.\n\nLet's first evaluate the supremum for a fixed non-zero $q_h \\in Q_h$:\n$$\n\\sup_{0 \\neq v_{h} \\in V_{h}} \\frac{b(v_{h},q_{h})}{\\|v_{h}\\|_{V} \\|q_{h}\\|_{Q}} = \\sup_{c \\neq 0} \\frac{b(c v_{1}, q_{h})}{\\|c v_{1}\\|_{V} \\|q_{h}\\|_{Q}} = \\sup_{c \\neq 0} \\frac{c \\, b(v_{1}, q_{h})}{|c| \\, \\|v_{1}\\|_{V} \\|q_{h}\\|_{Q}}.\n$$\nSince $\\sup_{c \\neq 0} \\frac{c}{|c|}$ would yield $1$, we can simplify this to:\n$$\n\\frac{|b(v_{1}, q_{h})|}{\\|v_{1}\\|_{V} \\|q_{h}\\|_{Q}} = \\frac{|v_{1}^{\\top} q_{h}|}{\\|v_{1}\\|_{V} \\|q_{h}\\|_{Q}}.\n$$\nThe norm of $v_1$ is $\\|v_1\\|_V = \\sqrt{1^2 + 1^2} = \\sqrt{2}$.\n\nNow, we must compute the infimum of this quantity over all non-zero $q_h \\in Q_h$:\n$$\n\\beta_{h} = \\inf_{0 \\neq q_{h} \\in Q_{h}} \\frac{|v_{1}^{\\top} q_{h}|}{\\sqrt{2} \\, \\|q_{h}\\|_{Q}}.\n$$\nThe infimum will be zero if there exists a non-zero vector $q_h \\in Q_h$ that is orthogonal to $v_1$, i.e., for which $v_1^{\\top} q_h = 0$.\nThe condition $v_1^{\\top} q_h = 0$ is:\n$$\n\\begin{pmatrix} 1  1 \\end{pmatrix} q_h = 0.\n$$\nThis means $q_h$ lies in the orthogonal complement of the space spanned by $v_1$. The space of vectors orthogonal to $(1,1)^\\top$ in $\\mathbb{R}^2$ is spanned by the vector $(1,-1)^\\top$.\n\nWe must check if any such non-zero vector exists in the subspace $Q_h$. The subspace $Q_h$ is spanned by $q_1 = (1,-1)^\\top$ and $q_2 = (1,1)^\\top$.\nThe vector $q_1 = (1,-1)^\\top$ is itself a non-zero vector in $Q_h$. Let's choose $q_h = q_1$.\nFor this choice, we compute $v_1^{\\top} q_h$:\n$$\nv_1^{\\top} q_1 = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 1 \\cdot 1 + 1 \\cdot (-1) = 0.\n$$\nSince we have found a non-zero vector $q_h = q_1 \\in Q_h$ for which the numerator $|v_1^{\\top} q_h|$ is zero, the value of the fraction $\\frac{|v_1^{\\top} q_h|}{\\sqrt{2} \\|q_h\\|_Q}$ is $0$.\nThe infimum of a set of non-negative real numbers that contains $0$ must be $0$.\nTherefore, the discrete inf-sup constant is $\\beta_h = 0$.\nA value of $\\beta_h=0$ signifies that the discrete LBB condition is not satisfied, and the discrete problem is not well-posed. This particular combination of discrete spaces $V_h$ and $Q_h$ is unstable.", "answer": "$$\\boxed{0}$$", "id": "3387748"}]}