{"hands_on_practices": [{"introduction": "Before tackling complex regularization methods, it is crucial to understand why solutions to inverse problems can be non-unique. This exercise ([@problem_id:3419607]) provides a clear and intuitive entry point by considering a finite-rank operator with a nontrivial nullspace. By working through this example, you will explicitly characterize the entire set of solutions to an equation $Tx=y$ and see why selecting the unique solution with the minimum norm is a fundamental and powerful principle.", "problem": "Consider the separable Hilbert space $H = \\ell^{2}(\\mathbb{N})$ with its canonical orthonormal basis $\\{e_{k}\\}_{k=1}^{\\infty}$. Let $T : H \\to H$ be a compact linear operator with a singular system $\\{(\\sigma_{k}, u_{k}, v_{k})\\}_{k=1}^{\\infty}$, where $u_{k} = e_{k}$ and $v_{k} = e_{k}$ for all $k$, and singular values defined by\n$$\n\\sigma_{1} = 1,\\quad \\sigma_{2} = \\frac{1}{2},\\quad \\sigma_{3} = \\frac{1}{3},\\quad \\sigma_{k} = 0 \\text{ for all } k \\geq 4.\n$$\nAssume $T$ acts by $T v_{k} = \\sigma_{k} u_{k}$ for all $k$ and that $\\{u_{k}\\}$ and $\\{v_{k}\\}$ are orthonormal bases. The operator $T$ is thus finite rank and compact, with a nontrivial nullspace. The Picard condition, in the context of this singular system, stipulates that a datum $y \\in H$ belongs to the range of $T$ if and only if $y$ is orthogonal to all $u_{k}$ for which $\\sigma_{k} = 0$ and the series\n$$\n\\sum_{k=1}^{\\infty} \\frac{|\\langle y, u_{k} \\rangle|^{2}}{\\sigma_{k}^{2}}\n$$\nis finite, where $\\langle \\cdot, \\cdot \\rangle$ denotes the inner product in $H$.\n\nConstruct a specific right-hand side $y \\in H$ by prescribing its coefficients in the $\\{u_{k}\\}$-basis as\n$$\n\\beta_{k} := \\langle y, u_{k} \\rangle =\n\\begin{cases}\n1,  k = 1,\\\n$$4pt]\n\\frac{1}{2},  k = 2,\\\n$$4pt]\n\\frac{1}{3},  k = 3,\\\n$$4pt]\n0,  k \\geq 4,\n\\end{cases}\n$$\nso that $y = \\sum_{k=1}^{\\infty} \\beta_{k} u_{k}$.\n\nStarting from the fundamental definitions of compact operators on Hilbert spaces, singular systems, and the Picard condition, do the following:\n\n- Verify that the constructed $y$ satisfies the Picard condition for $T$.\n- Characterize the solution set $\\{x \\in H : T x = y\\}$ using the singular system, and explain why the presence of the nullspace of $T$ implies that non-minimal-norm solutions can exhibit unbounded growth in certain components.\n- Derive, from first principles, the minimal-norm solution $x_{\\min}$ in the right singular vector basis.\n\nYou must provide the minimal-norm solution $x_{\\min}$ as a single closed-form analytic expression in terms of $\\{v_{k}\\}$ as your final reported answer. No numerical rounding is required and no physical units are involved. The final answer must be a single expression only, not an equation or inequality.", "solution": "The problem is valid. It is a well-posed mathematical problem in the field of functional analysis and inverse problems, containing all necessary information and being free of contradictions or scientific flaws.\n\nWe are tasked with solving the linear operator equation $Tx = y$ for a compact operator $T$ on the Hilbert space $H = \\ell^{2}(\\mathbb{N})$. The operator $T$ is defined via its singular system $\\{(\\sigma_{k}, u_{k}, v_{k})\\}_{k=1}^{\\infty}$, where $\\{u_k\\}$ and $\\{v_k\\}$ are orthonormal bases for $H$, and $T$ acts as $Tv = \\sum_{k=1}^{\\infty} \\sigma_k \\langle v, v_k \\rangle u_k$ for any $v \\in H$. In this specific problem, we are given $u_k = v_k = e_k$ (the canonical basis) for all $k \\geq 1$. The singular values are $\\sigma_{1} = 1$, $\\sigma_{2} = \\frac{1}{2}$, $\\sigma_{3} = \\frac{1}{3}$, and $\\sigma_{k} = 0$ for $k \\geq 4$. The right-hand side $y \\in H$ is given by its components in the $\\{u_k\\}$ basis, $\\beta_k = \\langle y, u_k \\rangle$, as $\\beta_1=1$, $\\beta_2=1/2$, $\\beta_3=1/3$, and $\\beta_k=0$ for $k \\geq 4$. Thus, $y = \\sum_{k=1}^{3} \\beta_k u_k = 1 \\cdot u_1 + \\frac{1}{2} u_2 + \\frac{1}{3} u_3$.\n\nFirst, we verify that the given $y$ satisfies the Picard condition. The condition has two parts:\n1. Orthogonality to the nullspace of the adjoint: The datum $y$ must be in the closure of the range of $T$, which is equivalent to $y$ being orthogonal to the nullspace of the adjoint operator $T^*$. In terms of the singular system, this means $\\langle y, u_k \\rangle = 0$ for all $k$ such that $\\sigma_k = 0$.\nIn our case, $\\sigma_k=0$ for all $k \\geq 4$. We are given the coefficients of $y$ as $\\beta_k = \\langle y, u_k \\rangle$. By construction, $\\beta_k = 0$ for all $k \\ge 4$. Therefore, this part of the condition is satisfied.\n2. Finite weighted norm: The series $\\sum_{k=1}^{\\infty} \\frac{|\\langle y, u_k \\rangle|^2}{\\sigma_k^2}$ must be finite. This sum is understood to be over indices $k$ for which $\\sigma_k \\neq 0$.\nIn our case, $\\sigma_k \\neq 0$ only for $k \\in \\{1, 2, 3\\}$. We compute the sum:\n$$\n\\sum_{k=1}^{3} \\frac{|\\langle y, u_k \\rangle|^2}{\\sigma_k^2} = \\frac{|\\beta_1|^2}{\\sigma_1^2} + \\frac{|\\beta_2|^2}{\\sigma_2^2} + \\frac{|\\beta_3|^2}{\\sigma_3^2} = \\frac{|1|^2}{1^2} + \\frac{|\\frac{1}{2}|^2}{(\\frac{1}{2})^2} + \\frac{|\\frac{1}{3}|^2}{(\\frac{1}{3})^2} = 1 + 1 + 1 = 3.\n$$\nSince $3$ is a finite value, the second part of the Picard condition is also satisfied. Thus, the problem $Tx=y$ has at least one solution.\n\nNext, we characterize the solution set. Let an arbitrary $x \\in H$ be expanded in the orthonormal basis $\\{v_k\\}$:\n$$\nx = \\sum_{j=1}^{\\infty} \\alpha_j v_j, \\quad \\text{where } \\alpha_j = \\langle x, v_j \\rangle.\n$$\nApplying the operator $T$ and using its linearity and continuity, we get:\n$$\nTx = T \\left( \\sum_{j=1}^{\\infty} \\alpha_j v_j \\right) = \\sum_{j=1}^{\\infty} \\alpha_j T v_j.\n$$\nBy the definition of the action of $T$ on its singular vectors, $Tv_j = \\sigma_j u_j$, so\n$$\nTx = \\sum_{j=1}^{\\infty} \\alpha_j \\sigma_j u_j.\n$$\nWe set this equal to $y = \\sum_{k=1}^{\\infty} \\beta_k u_k$:\n$$\n\\sum_{k=1}^{\\infty} \\alpha_k \\sigma_k u_k = \\sum_{k=1}^{\\infty} \\beta_k u_k.\n$$\nSince $\\{u_k\\}$ is an orthonormal basis, we can equate the coefficients for each $k$:\n$$\n\\alpha_k \\sigma_k = \\beta_k \\quad \\text{for all } k \\geq 1.\n$$\nFor $k \\in \\{1, 2, 3\\}$, $\\sigma_k \\neq 0$, so we can uniquely determine the coefficients $\\alpha_k$:\n$\\alpha_1 = \\frac{\\beta_1}{\\sigma_1} = \\frac{1}{1} = 1$.\n$\\alpha_2 = \\frac{\\beta_2}{\\sigma_2} = \\frac{1/2}{1/2} = 1$.\n$\\alpha_3 = \\frac{\\beta_3}{\\sigma_3} = \\frac{1/3}{1/3} = 1$.\nFor $k \\geq 4$, $\\sigma_k = 0$ and $\\beta_k = 0$. The equation becomes $\\alpha_k \\cdot 0 = 0$. This equation holds for any value of $\\alpha_k$.\nThe solution $x$ must be an element of $H = \\ell^2(\\mathbb{N})$, which means its norm must be finite: $\\|x\\|^2 = \\sum_{k=1}^\\infty |\\alpha_k|^2  \\infty$.\nThe general solution is therefore of the form:\n$$\nx = \\alpha_1 v_1 + \\alpha_2 v_2 + \\alpha_3 v_3 + \\sum_{k=4}^{\\infty} \\alpha_k v_k = (v_1 + v_2 + v_3) + \\sum_{k=4}^{\\infty} \\alpha_k v_k,\n$$\nwhere the coefficients $\\alpha_k$ for $k \\geq 4$ are arbitrary, subject to the constraint $\\sum_{k=4}^{\\infty} |\\alpha_k|^2  \\infty$.\n\nThe set of vectors $z = \\sum_{k=4}^{\\infty} \\alpha_k v_k$ with $\\sum_{k=4}^{\\infty} |\\alpha_k|^2  \\infty$ is precisely the nullspace of $T$, $\\ker(T)$, since $Tz = \\sum_{k=4}^{\\infty} \\alpha_k \\sigma_k u_k = \\sum_{k=4}^{\\infty} \\alpha_k \\cdot 0 \\cdot u_k = 0$. The solution set is thus the affine subspace $x_{\\text{part}} + \\ker(T)$, where $x_{\\text{part}} = v_1 + v_2 + v_3$ is a particular solution.\nThe \"unbounded growth\" of components in non-minimal-norm solutions arises because $\\ker(T)$ is an infinite-dimensional subspace. While any individual element $z \\in \\ker(T)$ must have a finite norm, a sequence of its components $\\alpha_k$ must converge to zero. However, there is no upper bound on the magnitude of any single component across the entire solution set. For any real number $M > 0$ and any integer $k_0 \\geq 4$, the vector $x = (v_1+v_2+v_3) + M v_{k_0}$ is a valid solution to $Tx=y$. Its $k_0$-th component is $M$, which can be arbitrarily large. This illustrates that while the full vector of coefficients $\\{\\alpha_k\\}_{k=4}^\\infty$ must be square-summable, individual coefficients are not bounded.\n\nFinally, we derive the minimal-norm solution $x_{\\min}$. The norm squared of a general solution $x$ is:\n$$\n\\|x\\|^2 = \\left\\| (v_1 + v_2 + v_3) + \\sum_{k=4}^{\\infty} \\alpha_k v_k \\right\\|^2.\n$$\nThe vector $x_{\\text{part}} = v_1+v_2+v_3$ is in the subspace spanned by $\\{v_1, v_2, v_3\\}$, which is $(\\ker T)^\\perp = \\mathcal{R}(T^*)$. The vector $z = \\sum_{k=4}^{\\infty} \\alpha_k v_k$ is in $\\ker(T)$. These two subspaces are orthogonal. By the Pythagorean theorem in Hilbert spaces:\n$$\n\\|x\\|^2 = \\|v_1 + v_2 + v_3\\|^2 + \\left\\| \\sum_{k=4}^{\\infty} \\alpha_k v_k \\right\\|^2.\n$$\nUsing the orthonormality of $\\{v_k\\}$:\n$$\n\\|x\\|^2 = (|1|^2 + |1|^2 + |1|^2) + \\sum_{k=4}^{\\infty} |\\alpha_k|^2 = 3 + \\sum_{k=4}^{\\infty} |\\alpha_k|^2.\n$$\nTo minimize $\\|x\\|^2$, we must minimize the non-negative term $\\sum_{k=4}^{\\infty} |\\alpha_k|^2$. Its minimum value is $0$, which is achieved if and only if $\\alpha_k = 0$ for all $k \\geq 4$.\nThis choice corresponds to selecting the solution component in the nullspace to be the zero vector. The resulting minimal-norm solution is:\n$$\nx_{\\min} = v_1 + v_2 + v_3 + \\sum_{k=4}^{\\infty} 0 \\cdot v_k = v_1 + v_2 + v_3.\n$$\nThis solution is the unique solution that lies entirely in the orthogonal complement of the nullspace, $(\\ker T)^\\perp$.", "answer": "$$\n\\boxed{v_1 + v_2 + v_3}\n$$", "id": "3419607"}, {"introduction": "The stability of an inverse problem is dictated by the interplay between the decay of the operator's singular values and the smoothness of the data. This practice ([@problem_id:3419623]) explores a case of severe ill-posedness where singular values decay exponentially, making direct inversion highly unstable even for smooth data. You will first verify the failure of the Picard condition and then quantify its practical consequence by determining the maximum number of singular components that can be recovered while keeping the noise-induced error within a tolerable limit.", "problem": "Consider the separable Hilbert space $\\ell^{2}$ of square-summable real sequences, and a compact linear operator $A : \\ell^{2} \\to \\ell^{2}$ that admits a singular system $\\{(\\sigma_{i}, u_{i}, v_{i})\\}_{i \\geq 1}$ with orthonormal bases $\\{u_{i}\\}_{i \\geq 1}$ and $\\{v_{i}\\}_{i \\geq 1}$ such that the singular values are given by $\\sigma_{i} = \\exp(-\\alpha i)$ for a fixed $\\alpha > 0$. Let the exact data $y \\in \\ell^{2}$ have expansion coefficients in the left singular basis that decay polynomially,\n$$\\langle y, u_{i} \\rangle = i^{-p}, \\quad i \\geq 1,$$\nfor a fixed $p > \\tfrac{1}{2}$, so that $y \\in \\ell^{2}$ is well-defined. You are given noisy data $y^{\\delta} = y + \\eta$ with $\\|\\eta\\|_{\\ell^{2}} \\leq \\delta$, where $\\delta > 0$ is a known noise level. The truncated singular value decomposition (TSVD) estimator with truncation level $k \\in \\mathbb{N}$ is\n$$x_{k}^{\\delta} = \\sum_{i=1}^{k} \\frac{\\langle y^{\\delta}, u_{i} \\rangle}{\\sigma_{i}} \\, v_{i} = \\sum_{i=1}^{k} \\frac{\\langle y, u_{i} \\rangle + \\langle \\eta, u_{i} \\rangle}{\\sigma_{i}} \\, v_{i}.$$\nFirst, justify from core definitions why the Picard condition fails for the exact data $y$ relative to the operator $A$ and therefore some form of regularization is necessary to stabilize inversion. Then, focusing on noise-induced error only, define the noise-free truncated estimate $x_{k}^{0} := \\sum_{i=1}^{k} \\frac{\\langle y, u_{i} \\rangle}{\\sigma_{i}} \\, v_{i}$ and consider the worst-case deviation $\\|x_{k}^{\\delta} - x_{k}^{0}\\|_{\\ell^{2}}$ over all admissible noises with $\\|\\eta\\|_{\\ell^{2}} \\leq \\delta$. For a prescribed reconstruction tolerance $E > 0$, determine the minimal truncation level $k^{\\star}(\\delta)$ that maximizes information use (i.e., is the largest integer $k$ permitted) while guaranteeing\n$$\\sup_{\\|\\eta\\|_{\\ell^{2}} \\leq \\delta} \\|x_{k}^{\\delta} - x_{k}^{0}\\|_{\\ell^{2}} \\leq E.$$\nAssume the noise level satisfies $E/\\delta > \\exp(\\alpha)$ so that $k^{\\star}(\\delta) \\geq 1$ is nontrivial. Provide $k^{\\star}(\\delta)$ as a single closed-form analytic expression in terms of $\\alpha$, $\\delta$, and $E$. No numerical evaluation is required, and no units are involved.", "solution": "We begin by recalling two foundational elements for inverse problems with compact operators and singular systems. First, a compact operator $A : \\ell^{2} \\to \\ell^{2}$ admits a singular system $\\{(\\sigma_{i}, u_{i}, v_{i})\\}_{i \\geq 1}$ with singular values $\\sigma_{i} \\downarrow 0$ and orthonormal bases $\\{u_{i}\\}$ and $\\{v_{i}\\}$ such that for any $x \\in \\ell^{2}$,\n$$A x = \\sum_{i=1}^{\\infty} \\sigma_{i} \\langle x, v_{i} \\rangle \\, u_{i}.$$\nSecond, the Picard condition states that the data $y$ are compatible with a stable inverse if and only if\n$$\\sum_{i=1}^{\\infty} \\frac{|\\langle y, u_{i} \\rangle|^{2}}{\\sigma_{i}^{2}}  \\infty,$$\nwhich ensures that the formal inverse $x^{\\dagger} = \\sum_{i=1}^{\\infty} \\frac{\\langle y, u_{i} \\rangle}{\\sigma_{i}} v_{i}$ defines an element of $\\ell^{2}$.\n\nIn our teaching example, the singular values decay exponentially, $\\sigma_{i} = \\exp(-\\alpha i)$ with $\\alpha  0$, while the data coefficients decay polynomially, $\\langle y, u_{i} \\rangle = i^{-p}$ with $p > \\tfrac{1}{2}$. Then the Picard series becomes\n$$\\sum_{i=1}^{\\infty} \\frac{|\\langle y, u_{i} \\rangle|^{2}}{\\sigma_{i}^{2}} = \\sum_{i=1}^{\\infty} \\frac{i^{-2p}}{\\exp(-2\\alpha i)} = \\sum_{i=1}^{\\infty} i^{-2p} \\exp(2\\alpha i).$$\nBecause $\\exp(2\\alpha i)$ grows exponentially and $i^{-2p}$ decays only polynomially, the general term $i^{-2p} \\exp(2\\alpha i)$ does not tend to zero and the series diverges. Therefore, the Picard condition fails, meaning that direct inversion is unstable and regularization is necessary.\n\nWe now analyze the noise-induced error introduced by the truncated singular value decomposition (TSVD). The TSVD estimator with truncation $k$ is\n$$x_{k}^{\\delta} = \\sum_{i=1}^{k} \\frac{\\langle y^{\\delta}, u_{i} \\rangle}{\\sigma_{i}} \\, v_{i} = \\sum_{i=1}^{k} \\frac{\\langle y, u_{i} \\rangle + \\langle \\eta, u_{i} \\rangle}{\\sigma_{i}} \\, v_{i},$$\nand the corresponding noise-free truncated estimator is\n$$x_{k}^{0} = \\sum_{i=1}^{k} \\frac{\\langle y, u_{i} \\rangle}{\\sigma_{i}} \\, v_{i}.$$\nTheir difference isolates the contribution of noise:\n$$x_{k}^{\\delta} - x_{k}^{0} = \\sum_{i=1}^{k} \\frac{\\langle \\eta, u_{i} \\rangle}{\\sigma_{i}} \\, v_{i}.$$\nTaking the norm in $\\ell^{2}$ and using orthonormality of $\\{v_{i}\\}$ yields\n$$\\|x_{k}^{\\delta} - x_{k}^{0}\\|_{\\ell^{2}}^{2} = \\sum_{i=1}^{k} \\left|\\frac{\\langle \\eta, u_{i} \\rangle}{\\sigma_{i}}\\right|^{2} = \\sum_{i=1}^{k} |\\langle \\eta, u_{i} \\rangle|^{2} \\exp(2\\alpha i).$$\nWe seek the worst-case deviation over all $\\eta$ with $\\|\\eta\\|_{\\ell^{2}} \\leq \\delta$. Writing $a_{i} := |\\langle \\eta, u_{i} \\rangle|^{2}$, we have $a_{i} \\geq 0$ and $\\sum_{i=1}^{\\infty} a_{i} \\leq \\delta^{2}$. The quantity to maximize is a weighted sum\n$$\\sum_{i=1}^{k} a_{i} \\exp(2\\alpha i),$$\nsubject to the budget $\\sum_{i=1}^{\\infty} a_{i} \\leq \\delta^{2}$. Since the weights $\\exp(2\\alpha i)$ are increasing in $i$, the supremum is achieved by concentrating all the available budget on the largest weight within the truncated set, i.e., at index $i = k$. Therefore,\n$$\\sup_{\\|\\eta\\|_{\\ell^{2}} \\leq \\delta} \\|x_{k}^{\\delta} - x_{k}^{0}\\|_{\\ell^{2}}^{2} = \\delta^{2} \\exp(2\\alpha k), \\quad \\text{and hence} \\quad \\sup_{\\|\\eta\\|_{\\ell^{2}} \\leq \\delta} \\|x_{k}^{\\delta} - x_{k}^{0}\\|_{\\ell^{2}} = \\delta \\exp(\\alpha k).$$\n\nImposing the reconstruction tolerance $E  0$ requires\n$$\\delta \\exp(\\alpha k) \\leq E.$$\nSolving this inequality for $k$ gives\n$$\\exp(\\alpha k) \\leq \\frac{E}{\\delta} \\quad \\Longrightarrow \\quad \\alpha k \\leq \\ln\\!\\left(\\frac{E}{\\delta}\\right) \\quad \\Longrightarrow \\quad k \\leq \\frac{1}{\\alpha} \\ln\\!\\left(\\frac{E}{\\delta}\\right).$$\nTo maximize information use while meeting the bound (i.e., to choose the largest admissible truncation level), we take the integer part:\n$$k^{\\star}(\\delta) = \\left\\lfloor \\frac{1}{\\alpha} \\ln\\!\\left(\\frac{E}{\\delta}\\right) \\right\\rfloor.$$\nUnder the stated assumption $E/\\delta > \\exp(\\alpha)$, this yields $k^{\\star}(\\delta) \\geq 1$ and thus a nontrivial truncation.\n\nThis $k^{\\star}(\\delta)$ quantifies, in closed form, the minimal TSVD truncation level in the sense of the largest number of singular components that can be retained while ensuring the worst-case noise-induced error remains bounded by $E$ across all noise realizations satisfying $\\|\\eta\\|_{\\ell^{2}} \\leq \\delta$. It explicitly reflects the exponential decay of $\\sigma_{i}$ via the factor $\\alpha$ and captures how stricter noise levels $\\delta$ permit deeper truncations $k^{\\star}(\\delta)$.", "answer": "$$\\boxed{\\left\\lfloor \\frac{1}{\\alpha} \\ln\\!\\left(\\frac{E}{\\delta}\\right) \\right\\rfloor}$$", "id": "3419623"}, {"introduction": "In practice, inverse problems are solved using noisy data, which makes choosing the right amount of regularization essential. This exercise ([@problem_id:3419572]) guides you through the derivation of a fundamental data-driven parameter choice rule, a variant of the discrepancy principle. You will learn to balance the competing demands of data fidelity and solution stability by equating the data residual with the expected error propagation due to noise, a core technique in designing robust inversion algorithms.", "problem": "Consider a bounded linear compact operator $T:\\mathcal{X}\\to\\mathcal{Y}$ between separable Hilbert spaces with a singular value decomposition (SVD), i.e., there exist singular values $\\{\\sigma_{n}\\}_{n\\geq 1}$ with $\\sigma_{1}\\geq \\sigma_{2}\\geq \\cdots \\searrow 0$, and singular vectors $\\{u_{n}\\}_{n\\geq 1}\\subset\\mathcal{Y}$, $\\{v_{n}\\}_{n\\geq 1}\\subset\\mathcal{X}$, forming orthonormal bases, such that $T v_{n}=\\sigma_{n} u_{n}$ and $T^{*} u_{n}=\\sigma_{n} v_{n}$ for all $n\\geq 1$. Let the exact data be $y=T x$, where $x\\in\\mathcal{X}$ is the exact solution satisfying the Picard condition $\\sum_{n=1}^{\\infty}|\\langle y,u_{n}\\rangle|^{2}\\sigma_{n}^{-2}\\infty$. The observed data are noisy: $y^{\\delta}=y+\\eta$, where the noise $\\eta$ is mean-zero white Gaussian with covariance $\\delta^{2} I_{\\mathcal{Y}}$, so that $\\mathbb{E}|\\langle \\eta,u_{n}\\rangle|^{2}=\\delta^{2}$ for all $n$.\n\nDefine the spectral cutoff estimator $x_{k}^{\\delta}=\\sum_{n=1}^{k}\\sigma_{n}^{-1}\\langle y^{\\delta},u_{n}\\rangle v_{n}$. Starting from the properties of the singular value decomposition (SVD) and the orthonormality of $\\{u_{n}\\}$ and $\\{v_{n}\\}$, derive the exact expression of the residual $\\|T x_{k}^{\\delta}-y^{\\delta}\\|^{2}$ in terms of the noisy data coefficients $\\langle y^{\\delta},u_{n}\\rangle$. Then, using the definition of $x_{k}^{\\delta}$ and the white-noise model, compute the expected value of the noise propagation into the solution, $\\mathbb{E}\\|x_{k}^{\\delta}-x_{k}\\|^{2}$, where $x_{k}=\\sum_{n=1}^{k}\\sigma_{n}^{-1}\\langle y,u_{n}\\rangle v_{n}$ is the truncated exact solution. Finally, by equating the residual with a multiple of the expected noise propagation, derive a stopping rule $k(\\delta)$ that balances these two effects. Your stopping rule must be provided as a single closed-form analytic expression that depends only on $\\{\\sigma_{n}\\}$, $\\{u_{n}\\}$, $y^{\\delta}$, and $\\delta$, and it may include a fixed positive tuning parameter $\\alpha>0$.\n\nNo numerical rounding is required and no physical units are involved. Express your final stopping rule $k(\\delta)$ as a single analytic expression.", "solution": "The task is to derive a stopping rule for the spectral cutoff estimator $x_{k}^{\\delta}$ of an ill-posed linear inverse problem. The derivation proceeds in three steps: first, computing the data-fit residual; second, computing the expected noise propagation in the solution; and third, equating these two quantities to define a rule for choosing the regularization parameter $k$.\n\nLet the operator be $T:\\mathcal{X}\\to\\mathcal{Y}$ with singular value decomposition (SVD) given by the orthonormal bases $\\{u_{n}\\}_{n\\geq 1} \\subset \\mathcal{Y}$ and $\\{v_{n}\\}_{n\\geq 1} \\subset \\mathcal{X}$, and singular values $\\sigma_{1}\\geq \\sigma_{2}\\geq \\cdots  0$ such that $T v_{n}=\\sigma_{n} u_{n}$. The noisy data are $y^{\\delta} = y + \\eta = T x + \\eta$. The spectral cutoff estimator is $x_{k}^{\\delta}=\\sum_{n=1}^{k}\\sigma_{n}^{-1}\\langle y^{\\delta},u_{n}\\rangle v_{n}$.\n\n**Step 1: Derivation of the Residual $\\|T x_{k}^{\\delta}-y^{\\delta}\\|^{2}$**\n\nFirst, we apply the operator $T$ to the estimator $x_{k}^{\\delta}$. By linearity of $T$, we have:\n$$\nT x_{k}^{\\delta} = T \\left( \\sum_{n=1}^{k}\\sigma_{n}^{-1}\\langle y^{\\delta},u_{n}\\rangle v_{n} \\right) = \\sum_{n=1}^{k}\\sigma_{n}^{-1}\\langle y^{\\delta},u_{n}\\rangle (T v_{n})\n$$\nUsing the SVD property $T v_{n} = \\sigma_{n} u_{n}$, we simplify the expression:\n$$\nT x_{k}^{\\delta} = \\sum_{n=1}^{k}\\sigma_{n}^{-1}\\langle y^{\\delta},u_{n}\\rangle (\\sigma_{n} u_{n}) = \\sum_{n=1}^{k}\\langle y^{\\delta},u_{n}\\rangle u_{n}\n$$\nThis expression represents the orthogonal projection of the data $y^{\\delta}$ onto the subspace spanned by the first $k$ singular vectors $\\{u_n\\}_{n=1}^k$.\n\nNext, we express the data vector $y^{\\delta}$ in the orthonormal basis $\\{u_{n}\\}_{n\\geq 1}$ of $\\mathcal{Y}$:\n$$\ny^{\\delta} = \\sum_{n=1}^{\\infty}\\langle y^{\\delta},u_{n}\\rangle u_{n}\n$$\nThe residual vector is the difference between the projected data and the full data:\n$$\nT x_{k}^{\\delta}-y^{\\delta} = \\left( \\sum_{n=1}^{k}\\langle y^{\\delta},u_{n}\\rangle u_{n} \\right) - \\left( \\sum_{n=1}^{\\infty}\\langle y^{\\delta},u_{n}\\rangle u_{n} \\right) = - \\sum_{n=k+1}^{\\infty}\\langle y^{\\delta},u_{n}\\rangle u_{n}\n$$\nThe squared norm of the residual is calculated using the orthonormality of the basis $\\{u_{n}\\}_{n\\geq 1}$. The norm of a sum of orthogonal vectors is the square root of the sum of the squares of their individual norms (Pythagorean theorem generalized via Parseval's identity):\n$$\n\\|T x_{k}^{\\delta}-y^{\\delta}\\|^{2} = \\left\\| - \\sum_{n=k+1}^{\\infty}\\langle y^{\\delta},u_{n}\\rangle u_{n} \\right\\|^{2} = \\sum_{n=k+1}^{\\infty} | \\langle y^{\\delta},u_{n}\\rangle |^{2} \\|u_n\\|^2 = \\sum_{n=k+1}^{\\infty} | \\langle y^{\\delta},u_{n}\\rangle |^{2}\n$$\nThis is the exact expression for the residual.\n\n**Step 2: Computation of the Expected Noise Propagation $\\mathbb{E}\\|x_{k}^{\\delta}-x_{k}\\|^{2}$**\n\nThe noise propagation term measures how a perturbation in the data (the noise $\\eta$) affects the solution. We are interested in its expected value. Let $x_k$ be the truncated exact solution:\n$$\nx_{k} = \\sum_{n=1}^{k}\\sigma_{n}^{-1}\\langle y,u_{n}\\rangle v_{n}\n$$\nThe difference between the noisy and exact truncated solutions is:\n$$\nx_{k}^{\\delta}-x_{k} = \\sum_{n=1}^{k}\\sigma_{n}^{-1}\\langle y^{\\delta},u_{n}\\rangle v_{n} - \\sum_{n=1}^{k}\\sigma_{n}^{-1}\\langle y,u_{n}\\rangle v_{n} = \\sum_{n=1}^{k}\\sigma_{n}^{-1} (\\langle y^{\\delta},u_{n}\\rangle - \\langle y,u_{n}\\rangle) v_{n}\n$$\nUsing $y^{\\delta} = y + \\eta$ and the linearity of the inner product, we have $\\langle y^{\\delta},u_{n}\\rangle - \\langle y,u_{n}\\rangle = \\langle y+\\eta,u_{n}\\rangle - \\langle y,u_{n}\\rangle = \\langle \\eta, u_{n} \\rangle$. Substituting this into the difference gives:\n$$\nx_{k}^{\\delta}-x_{k} = \\sum_{n=1}^{k}\\sigma_{n}^{-1} \\langle \\eta, u_{n} \\rangle v_{n}\n$$\nThe squared norm of this difference, using the orthonormality of $\\{v_n\\}_{n\\geq 1}$, is:\n$$\n\\|x_{k}^{\\delta}-x_{k}\\|^{2} = \\left\\| \\sum_{n=1}^{k}\\sigma_{n}^{-1} \\langle \\eta, u_{n} \\rangle v_{n} \\right\\|^{2} = \\sum_{n=1}^{k} \\left| \\sigma_{n}^{-1} \\langle \\eta, u_{n} \\rangle \\right|^{2} = \\sum_{n=1}^{k} \\sigma_{n}^{-2} |\\langle \\eta,u_{n}\\rangle|^{2}\n$$\nNow, we take the expectation. The expectation operator $\\mathbb{E}$ is linear, so it distributes over the sum:\n$$\n\\mathbb{E}\\|x_{k}^{\\delta}-x_{k}\\|^{2} = \\mathbb{E}\\left[ \\sum_{n=1}^{k} \\sigma_{n}^{-2} |\\langle \\eta,u_{n}\\rangle|^{2} \\right] = \\sum_{n=1}^{k} \\sigma_{n}^{-2} \\mathbb{E}[|\\langle \\eta,u_{n}\\rangle|^{2}]\n$$\nUsing the given property of the white noise, $\\mathbb{E}|\\langle \\eta,u_{n}\\rangle|^{2} = \\delta^{2}$ for all $n$, we obtain the final expression for the expected noise propagation:\n$$\n\\mathbb{E}\\|x_{k}^{\\delta}-x_{k}\\|^{2} = \\sum_{n=1}^{k} \\sigma_{n}^{-2} \\delta^{2} = \\delta^{2} \\sum_{n=1}^{k} \\sigma_{n}^{-2}\n$$\n\n**Step 3: Derivation of the Stopping Rule $k(\\delta)$**\n\nThe stopping rule is derived by balancing the data-fit residual against the noise propagation effect. The problem asks to equate the residual with a multiple $\\alpha > 0$ of the expected noise propagation:\n$$\n\\|T x_{k}^{\\delta}-y^{\\delta}\\|^{2} = \\alpha \\, \\mathbb{E}\\|x_{k}^{\\delta}-x_{k}\\|^{2}\n$$\nSubstituting the expressions derived in the previous steps, we get the balancing equation for the parameter $k$:\n$$\n\\sum_{n=k+1}^{\\infty} | \\langle y^{\\delta},u_{n}\\rangle |^{2} = \\alpha \\delta^{2} \\sum_{n=1}^{k} \\sigma_{n}^{-2}\n$$\nThis is an implicit equation for the integer parameter $k$. To formulate a stopping rule, we must provide a clear procedure to choose $k$. Let $L(k) = \\sum_{n=k+1}^{\\infty} | \\langle y^{\\delta},u_{n}\\rangle |^{2}$ and $R(k) = \\alpha \\delta^{2} \\sum_{n=1}^{k} \\sigma_{n}^{-2}$. The function $L(k)$ is a monotonically non-increasing function of $k$, while $R(k)$ is a monotonically non-decreasing function of $k$. A common approach for such parameter choice rules, often called discrepancy principles, is to choose the smallest integer $k$ for which the residual term $L(k)$ falls below the error-related term $R(k)$. This strategy seeks to regularize just enough to suppress the noise without overly smoothing the solution.\n\nTherefore, the stopping rule $k(\\delta)$ is defined as the smallest non-negative integer $k$ for which the inequality $L(k) \\le R(k)$ holds. This provides a single, well-defined analytic expression for the function $k(\\delta)$:\n$$\nk(\\delta) = \\min \\left\\{ k \\in \\{0, 1, 2, \\dots\\} \\mid \\sum_{n=k+1}^{\\infty} | \\langle y^{\\delta},u_{n}\\rangle |^{2} \\le \\alpha \\delta^{2} \\sum_{n=1}^{k} \\sigma_{n}^{-2} \\right\\}\n$$\nThis expression constitutes the desired stopping rule.", "answer": "$$\n\\boxed{k(\\delta) = \\min \\left\\{ k \\in \\{0, 1, 2, \\dots\\} \\mid \\sum_{n=k+1}^{\\infty} |\\langle y^{\\delta}, u_{n} \\rangle|^{2} \\le \\alpha \\delta^{2} \\sum_{n=1}^{k} \\sigma_{n}^{-2} \\right\\}}\n$$", "id": "3419572"}]}