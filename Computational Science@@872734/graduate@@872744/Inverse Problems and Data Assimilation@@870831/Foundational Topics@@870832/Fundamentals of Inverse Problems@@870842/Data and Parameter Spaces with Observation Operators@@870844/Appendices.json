{"hands_on_practices": [{"introduction": "Understanding the stability of an inverse problem is paramount, as it determines whether small errors in measurements will lead to bounded or catastrophic errors in the estimated parameters. This first exercise provides a foundational analysis of stability for linear systems. By deriving the relationship between the stability constant and the singular values of the observation operator, you will gain a concrete, quantitative understanding of how an operator's structure governs the conditioning of the inverse problem [@problem_id:3374182].", "problem": "Consider a linear inverse problem in which the parameter space is $\\mathbb{R}^{n}$ and the data space is $\\mathbb{R}^{m}$, with a linear observation operator $\\mathcal{O} \\in \\mathbb{R}^{m \\times n}$. Observations are modeled as $y = \\mathcal{O} x$ for $x \\in \\mathbb{R}^{n}$ and $y \\in \\mathbb{R}^{m}$. Assume $\\mathcal{O}$ has full column rank so that the map from data to parameters defined by the least-squares solution is unique and given by the Moore–Penrose pseudoinverse $\\mathcal{O}^{+}$. Using the Euclidean $2$-norm on both spaces, define stability of the inverse problem as the Lipschitz continuity of the mapping $y \\mapsto x^{\\star}(y)$, where $x^{\\star}(y) = \\mathcal{O}^{+} y$, i.e., there exists a constant $C > 0$ such that $\\|x^{\\star}(y_{1}) - x^{\\star}(y_{2})\\|_{2} \\leq C \\|y_{1} - y_{2}\\|_{2}$ for all $y_{1}, y_{2} \\in \\mathbb{R}^{m}$.\n\nStarting only from fundamental definitions of the operator norm, the Singular Value Decomposition (SVD), and the Moore–Penrose pseudoinverse, derive the tightest possible bound constant $C$ in terms of intrinsic spectral properties of $\\mathcal{O}$. Then, for the specific observation operator\n$$\n\\mathcal{O} \\;=\\; \\begin{pmatrix}\n3 & 1 \\\\\n0 & 2 \\\\\n1 & 1\n\\end{pmatrix},\n$$\ncompute the exact value of the optimal stability constant $C$ for the map $y \\mapsto \\mathcal{O}^{+} y$. Your final answer must be a single closed-form expression. If a numerical approximation is used, round your answer to four significant figures; otherwise provide the exact analytical form. No units are required.", "solution": "The stability of the inverse problem is characterized by the Lipschitz continuity of the map $y \\mapsto x^{\\star}(y)$. The condition is given by\n$$\n\\|x^{\\star}(y_{1}) - x^{\\star}(y_{2})\\|_{2} \\leq C \\|y_{1} - y_{2}\\|_{2}\n$$\nfor all $y_{1}, y_{2} \\in \\mathbb{R}^{m}$. Substituting the definition $x^{\\star}(y) = \\mathcal{O}^{+} y$ and using the linearity of the pseudoinverse operator $\\mathcal{O}^{+}$, we have\n$$\n\\|\\mathcal{O}^{+} y_{1} - \\mathcal{O}^{+} y_{2}\\|_{2} = \\|\\mathcal{O}^{+}(y_{1} - y_{2})\\|_{2} \\leq C \\|y_{1} - y_{2}\\|_{2}.\n$$\nLet $z = y_{1} - y_{2}$. Since $y_{1}$ and $y_{2}$ can be any vectors in $\\mathbb{R}^{m}$, $z$ can be any vector in $\\mathbb{R}^{m}$. The inequality becomes\n$$\n\\|\\mathcal{O}^{+} z\\|_{2} \\leq C \\|z\\|_{2}\n$$\nfor all $z \\in \\mathbb{R}^{m}$. The tightest possible constant $C$ that satisfies this inequality for all non-zero $z$ is, by definition, the operator norm of $\\mathcal{O}^{+}$ induced by the Euclidean $2$-norm:\n$$\nC = \\sup_{z \\neq 0} \\frac{\\|\\mathcal{O}^{+} z\\|_{2}}{\\|z\\|_{2}} = \\|\\mathcal{O}^{+}\\|_{2}.\n$$\nThe operator $2$-norm of a matrix is equal to its largest singular value. To find the singular values of $\\mathcal{O}^{+}$, we first consider the Singular Value Decomposition (SVD) of the operator $\\mathcal{O}$. For $\\mathcal{O} \\in \\mathbb{R}^{m \\times n}$, its SVD is given by\n$$\n\\mathcal{O} = U \\Sigma V^T,\n$$\nwhere $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix containing the singular values of $\\mathcal{O}$. Let the rank of $\\mathcal{O}$ be $r$. The singular values $\\sigma_{i}$ are ordered such that $\\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{r} > 0$, with the remaining $n-r$ singular values being zero (if $m \\ge n$).\n\nThe Moore-Penrose pseudoinverse $\\mathcal{O}^{+}$ is defined from the SVD as\n$$\n\\mathcal{O}^{+} = V \\Sigma^{+} U^T,\n$$\nwhere $\\Sigma^{+} \\in \\mathbb{R}^{n \\times m}$ is the pseudoinverse of $\\Sigma$. If $\\Sigma$ has the form $\\begin{pmatrix} D & 0 \\\\ 0 & 0 \\end{pmatrix}$ with $D = \\text{diag}(\\sigma_{1}, \\dots, \\sigma_{r})$, then $\\Sigma^{+}$ has the form $\\begin{pmatrix} D^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}$ with $D^{-1} = \\text{diag}(1/\\sigma_{1}, \\dots, 1/\\sigma_{r})$.\n\nThe singular values of $\\mathcal{O}^{+}$ are the non-zero diagonal entries of $\\Sigma^{+}$, which are the reciprocals of the non-zero singular values of $\\mathcal{O}$. Specifically, the singular values of $\\mathcal{O}^{+}$ are $\\{1/\\sigma_{r}, \\dots, 1/\\sigma_{1}\\}$.\n\nThe largest singular value of $\\mathcal{O}^{+}$ is therefore $1/\\sigma_{r}$, which is the reciprocal of the smallest non-zero singular value of $\\mathcal{O}$. We denote this smallest non-zero singular value as $\\sigma_{\\min}(\\mathcal{O})$.\nThus, the tightest stability constant $C$ is\n$$\nC = \\|\\mathcal{O}^{+}\\|_{2} = \\frac{1}{\\sigma_{\\min}(\\mathcal{O})}.\n$$\nThe problem states that $\\mathcal{O}$ has full column rank, which for an $m \\times n$ matrix means the rank $r$ is equal to $n$. Consequently, all $n$ singular values of $\\mathcal{O}$ are positive: $\\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{n} > 0$. In this case, the smallest non-zero singular value is simply the smallest singular value, $\\sigma_{n}$. The optimal stability constant is therefore\n$$\nC = \\frac{1}{\\sigma_{n}}.\n$$\n\n**Computation for the Specific Operator**\n\nWe are given the operator\n$$\n\\mathcal{O} = \\begin{pmatrix}\n3 & 1 \\\\\n0 & 2 \\\\\n1 & 1\n\\end{pmatrix}.\n$$\nThis is a $3 \\times 2$ matrix, so $m=3$ and $n=2$. The singular values of $\\mathcal{O}$ are the square roots of the eigenvalues of the matrix $\\mathcal{O}^T \\mathcal{O}$. We compute this matrix product:\n$$\n\\mathcal{O}^T \\mathcal{O} = \\begin{pmatrix}\n3 & 0 & 1 \\\\\n1 & 2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n3 & 1 \\\\\n0 & 2 \\\\\n1 & 1\n\\end{pmatrix} =\n\\begin{pmatrix}\n(3)(3)+(0)(0)+(1)(1) & (3)(1)+(0)(2)+(1)(1) \\\\\n(1)(3)+(2)(0)+(1)(1) & (1)(1)+(2)(2)+(1)(1)\n\\end{pmatrix} =\n\\begin{pmatrix}\n10 & 4 \\\\\n4 & 6\n\\end{pmatrix}.\n$$\nNext, we find the eigenvalues $\\lambda$ of $\\mathcal{O}^T \\mathcal{O}$ by solving the characteristic equation $\\det(\\mathcal{O}^T \\mathcal{O} - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix}\n10-\\lambda & 4 \\\\\n4 & 6-\\lambda\n\\end{pmatrix} = 0\n$$\n$$\n(10-\\lambda)(6-\\lambda) - (4)(4) = 0\n$$\n$$\n60 - 16\\lambda + \\lambda^2 - 16 = 0\n$$\n$$\n\\lambda^2 - 16\\lambda + 44 = 0.\n$$\nUsing the quadratic formula, the eigenvalues are:\n$$\n\\lambda = \\frac{-(-16) \\pm \\sqrt{(-16)^2 - 4(1)(44)}}{2(1)} = \\frac{16 \\pm \\sqrt{256 - 176}}{2} = \\frac{16 \\pm \\sqrt{80}}{2}.\n$$\nSimplifying the square root, $\\sqrt{80} = \\sqrt{16 \\times 5} = 4\\sqrt{5}$.\n$$\n\\lambda = \\frac{16 \\pm 4\\sqrt{5}}{2} = 8 \\pm 2\\sqrt{5}.\n$$\nThe two eigenvalues of $\\mathcal{O}^T \\mathcal{O}$ are $\\lambda_{1} = 8 + 2\\sqrt{5}$ and $\\lambda_{2} = 8 - 2\\sqrt{5}$. Both are positive, as expected. The corresponding singular values of $\\mathcal{O}$ are $\\sigma_{1} = \\sqrt{\\lambda_{1}}$ and $\\sigma_{2} = \\sqrt{\\lambda_{2}}$.\nWe need the smallest singular value, which is $\\sigma_{n} = \\sigma_{2}$:\n$$\n\\sigma_{2} = \\sqrt{8 - 2\\sqrt{5}}.\n$$\nThe optimal stability constant $C$ is the reciprocal of this value:\n$$\nC = \\frac{1}{\\sigma_{2}} = \\frac{1}{\\sqrt{8 - 2\\sqrt{5}}}.\n$$\nTo provide a simplified closed-form expression, we rationalize the denominator by considering $C^2$:\n$$\nC^2 = \\frac{1}{8 - 2\\sqrt{5}} = \\frac{1}{8 - 2\\sqrt{5}} \\cdot \\frac{8 + 2\\sqrt{5}}{8 + 2\\sqrt{5}} = \\frac{8 + 2\\sqrt{5}}{8^2 - (2\\sqrt{5})^2} = \\frac{8 + 2\\sqrt{5}}{64 - 20} = \\frac{8 + 2\\sqrt{5}}{44} = \\frac{2(4 + \\sqrt{5})}{44} = \\frac{4 + \\sqrt{5}}{22}.\n$$\nTaking the square root gives the final expression for $C$:\n$$\nC = \\sqrt{\\frac{4 + \\sqrt{5}}{22}}.\n$$\nThis is the exact analytical form for the optimal stability constant.", "answer": "$$\n\\boxed{\\sqrt{\\frac{4+\\sqrt{5}}{22}}}\n$$", "id": "3374182"}, {"introduction": "While linear models offer clarity, most real-world systems are nonlinear. This practice extends our analysis to the nonlinear domain, where sensitivity and uncertainty are local concepts described by the curvature of the data misfit functional. You will compute the Gauss-Newton approximation to the Hessian, a key quantity in optimization, and uncover its profound identity with the Fisher Information Matrix, thus linking the geometric landscape of the problem to the statistical information provided by the data [@problem_id:3374134].", "problem": "Consider a data assimilation setting with parameter space $\\mathcal{X}=\\mathbb{R}$ and data space $\\mathcal{Y}=\\mathbb{R}^{3}$. Let the observation operator $\\mathcal{O}:\\mathcal{X}\\to\\mathcal{Y}$ be given componentwise by $\\mathcal{O}_{1}(x)=\\exp(x)$, $\\mathcal{O}_{2}(x)=\\sin(2x)$, and $\\mathcal{O}_{3}(x)=\\frac{x}{1+x^{2}}$. Assume the observation model $y=\\mathcal{O}(x_{\\text{true}})+\\eta$, where $\\eta$ is a zero-mean Gaussian random vector with covariance matrix $\\Gamma=\\mathrm{diag}(0.09,\\,0.04,\\,0.25)$ and the data misfit functional is\n$$\nJ(x)=\\frac{1}{2}\\left\\|\\Gamma^{-1/2}\\left(\\mathcal{O}(x)-y\\right)\\right\\|_{2}^{2}.\n$$\nStarting from the Gaussian likelihood and the definition of $J(x)$, derive the Gauss–Newton approximation to the Hessian of $J(x)$ at a general $x$ by applying first principles (Fréchet differentiation and the chain rule), and interpret its role in quantifying the local curvature of the data misfit. Then evaluate this Gauss–Newton Hessian approximation at the point $x^{\\star}=0.5$ for the given $\\mathcal{O}$ and $\\Gamma$, and present the final value as a single closed-form analytic expression. Finally, explain how the derived quantity relates to the Fisher Information Matrix (FIM) for the Gaussian observation model. Express your final answer as a single closed-form analytic expression.", "solution": "The data misfit functional is given by\n$$\nJ(x)=\\frac{1}{2}\\left\\|\\Gamma^{-1/2}\\left(\\mathcal{O}(x)-y\\right)\\right\\|_{2}^{2}\n$$\nwhere $x \\in \\mathcal{X}=\\mathbb{R}$, $y \\in \\mathcal{Y}=\\mathbb{R}^3$, $\\mathcal{O}:\\mathbb{R} \\to \\mathbb{R}^3$ is the observation operator, and $\\Gamma$ is the $3 \\times 3$ positive-definite covariance matrix of the observation noise. This expression is a weighted Euclidean norm squared, which can be written as a quadratic form:\n$$\nJ(x) = \\frac{1}{2} \\left( \\mathcal{O}(x) - y \\right)^T \\left(\\Gamma^{-1/2}\\right)^T \\Gamma^{-1/2} \\left( \\mathcal{O}(x) - y \\right) = \\frac{1}{2} \\left( \\mathcal{O}(x) - y \\right)^T \\Gamma^{-1} \\left( \\mathcal{O}(x) - y \\right)\n$$\nsince $\\Gamma$, and therefore $\\Gamma^{-1/2}$, is symmetric.\n\n**Part 1: Derivation of the Gauss-Newton Hessian**\n\nTo find the Hessian of $J(x)$, we first compute its gradient (its Fréchet derivative). Since $x$ is a scalar, the Fréchet derivative of $\\mathcal{O}(x)$ is its Jacobian matrix with respect to $x$, which we denote by $O'(x)$. This is a $3 \\times 1$ column vector:\n$$\nO'(x) = \\frac{d\\mathcal{O}}{dx}(x) = \\begin{pmatrix} \\frac{d\\mathcal{O}_1}{dx}(x) \\\\ \\frac{d\\mathcal{O}_2}{dx}(x) \\\\ \\frac{d\\mathcal{O}_3}{dx}(x) \\end{pmatrix}\n$$\nUsing the chain rule, the gradient of the scalar functional $J(x)$ is:\n$$\n\\nabla J(x) = \\frac{dJ}{dx}(x) = \\left( O'(x) \\right)^T \\Gamma^{-1} \\left( \\mathcal{O}(x) - y \\right)\n$$\nThis is a $1 \\times 1$ scalar, as expected.\n\nNext, we differentiate $\\nabla J(x)$ with respect to $x$ to find the Hessian, $H(x) = \\nabla^2 J(x)$. We apply the product rule:\n$$\nH(x) = \\frac{d}{dx} \\left[ \\left( O'(x) \\right)^T \\Gamma^{-1} \\left( \\mathcal{O}(x) - y \\right) \\right]\n$$\n$$\nH(x) = \\left( \\frac{d}{dx} \\left( O'(x) \\right)^T \\right) \\Gamma^{-1} \\left( \\mathcal{O}(x) - y \\right) + \\left( O'(x) \\right)^T \\Gamma^{-1} \\left( \\frac{d}{dx} \\left( \\mathcal{O}(x) - y \\right) \\right)\n$$\nLet $O''(x)$ denote the $3 \\times 1$ vector of second derivatives of the components of $\\mathcal{O}(x)$. The derivative of the transposed Jacobian is $(O''(x))^T$. The derivative of the residual term is simply $O'(x)$. Substituting these into the expression for $H(x)$ gives the exact Hessian:\n$$\nH(x) = (O'(x))^T \\Gamma^{-1} O'(x) + (O''(x))^T \\Gamma^{-1} (\\mathcal{O}(x) - y)\n$$\nThe Gauss-Newton method provides an approximation to the Hessian by neglecting the second term. This approximation is justified when the model is nearly linear (i.e., $O''(x)$ is small) or when the model provides a good fit to the data (i.e., the residual $\\mathcal{O}(x) - y$ is small). The resulting Gauss-Newton Hessian, $H_{GN}(x)$, is:\n$$\nH_{GN}(x) = (O'(x))^T \\Gamma^{-1} O'(x)\n$$\nThis approximation has the practical advantage of always being positive semi-definite, which is beneficial for optimization algorithms that rely on this property to ensure descent directions.\n\nThe Hessian of a function describes its local curvature. For the misfit functional $J(x)$, $H_{GN}(x)$ thus approximates the curvature of the misfit landscape around a point $x$. A large value of $H_{GN}(x)$ implies a sharp, well-defined minimum, indicating that the data are highly sensitive to the parameter $x$ and can constrain it well. A small value implies a flat, wide minimum, indicating low sensitivity and poor constraint.\n\n**Part 2: Evaluation at $x^{\\star}=0.5$**\n\nWe must evaluate $H_{GN}(x^{\\star}) = (O'(x^{\\star}))^T \\Gamma^{-1} O'(x^{\\star})$ at $x^{\\star}=0.5$.\nFirst, we find the components of the Jacobian $O'(x)$:\n$$\n\\mathcal{O}_{1}(x)=\\exp(x) \\implies \\frac{d\\mathcal{O}_{1}}{dx}(x) = \\exp(x)\n$$\n$$\n\\mathcal{O}_{2}(x)=\\sin(2x) \\implies \\frac{d\\mathcal{O}_{2}}{dx}(x) = 2\\cos(2x)\n$$\n$$\n\\mathcal{O}_{3}(x)=\\frac{x}{1+x^{2}} \\implies \\frac{d\\mathcal{O}_{3}}{dx}(x) = \\frac{(1)(1+x^2) - x(2x)}{(1+x^2)^2} = \\frac{1-x^2}{(1+x^2)^2}\n$$\nAt $x^{\\star}=0.5 = 1/2$:\n$$\n\\frac{d\\mathcal{O}_{1}}{dx}(0.5) = \\exp(0.5)\n$$\n$$\n\\frac{d\\mathcal{O}_{2}}{dx}(0.5) = 2\\cos(2 \\cdot 0.5) = 2\\cos(1)\n$$\n$$\n\\frac{d\\mathcal{O}_{3}}{dx}(0.5) = \\frac{1-(0.5)^2}{(1+(0.5)^2)^2} = \\frac{1-1/4}{(1+1/4)^2} = \\frac{3/4}{(5/4)^2} = \\frac{3/4}{25/16} = \\frac{3}{4} \\cdot \\frac{16}{25} = \\frac{12}{25}\n$$\nThe given covariance matrix is $\\Gamma=\\mathrm{diag}(0.09, 0.04, 0.25)$. Its inverse is:\n$$\n\\Gamma^{-1} = \\mathrm{diag}\\left(\\frac{1}{0.09}, \\frac{1}{0.04}, \\frac{1}{0.25}\\right) = \\mathrm{diag}\\left(\\frac{100}{9}, \\frac{100}{4}, \\frac{100}{25}\\right) = \\mathrm{diag}\\left(\\frac{100}{9}, 25, 4\\right)\n$$\nThe Gauss-Newton Hessian is a scalar sum:\n$$\nH_{GN}(x) = \\sum_{i=1}^3 \\frac{1}{\\Gamma_{ii}} \\left( \\frac{d\\mathcal{O}_i}{dx}(x) \\right)^2\n$$\nEvaluating at $x^{\\star}=0.5$:\n$$\nH_{GN}(0.5) = \\frac{100}{9} (\\exp(0.5))^2 + 25 (2\\cos(1))^2 + 4 \\left(\\frac{12}{25}\\right)^2\n$$\n$$\nH_{GN}(0.5) = \\frac{100}{9} \\exp(1) + 25 (4\\cos^2(1)) + 4 \\left(\\frac{144}{625}\\right)\n$$\n$$\nH_{GN}(0.5) = \\frac{100}{9} \\exp(1) + 100\\cos^2(1) + \\frac{576}{625}\n$$\nThis is the final closed-form analytic expression for the Gauss-Newton Hessian at $x^{\\star}=0.5$.\n\n**Part 3: Relation to the Fisher Information Matrix (FIM)**\n\nFor an observation model $y = \\mathcal{O}(x) + \\eta$ with Gaussian noise $\\eta \\sim \\mathcal{N}(0, \\Gamma)$, the likelihood function $p(y|x)$ is:\n$$\np(y|x) = \\frac{1}{\\sqrt{(2\\pi)^3 \\det(\\Gamma)}} \\exp\\left( -\\frac{1}{2} (\\mathcal{O}(x)-y)^T \\Gamma^{-1} (\\mathcal{O}(x)-y) \\right)\n$$\nThe log-likelihood, $\\ell(x; y) = \\ln(p(y|x))$, is:\n$$\n\\ell(x; y) = -\\frac{1}{2} \\ln((2\\pi)^3 \\det(\\Gamma)) - \\frac{1}{2} (\\mathcal{O}(x)-y)^T \\Gamma^{-1} (\\mathcal{O}(x)-y) = -\\text{const} - J(x)\n$$\nThe Fisher Information Matrix (a scalar in this case since $x \\in \\mathbb{R}$) is defined as the negative expected value of the second derivative of the log-likelihood with respect to the parameter $x$:\n$$\nI(x) = -\\mathbb{E}_{y|x} \\left[ \\frac{\\partial^2 \\ell(x;y)}{\\partial x^2} \\right]\n$$\nThe second derivative of the log-likelihood is:\n$$\n\\frac{\\partial^2 \\ell(x;y)}{\\partial x^2} = -\\frac{\\partial^2 J(x)}{\\partial x^2} = -H(x) = -\\left( (O'(x))^T \\Gamma^{-1} O'(x) + (O''(x))^T \\Gamma^{-1} (\\mathcal{O}(x) - y) \\right)\n$$\nNow we take the expectation over all possible observations $y$, given $x$. This means $y$ is treated as a random variable $Y = \\mathcal{O}(x) + \\eta$.\n$$\nI(x) = - \\mathbb{E}_{Y|x} \\left[ -H(x) \\right] = \\mathbb{E}_{Y|x} \\left[ (O'(x))^T \\Gamma^{-1} O'(x) + (O''(x))^T \\Gamma^{-1} (\\mathcal{O}(x) - Y) \\right]\n$$\nUsing the linearity of expectation:\n$$\nI(x) = (O'(x))^T \\Gamma^{-1} O'(x) + \\mathbb{E}_{Y|x} \\left[ (O''(x))^T \\Gamma^{-1} (\\mathcal{O}(x) - Y) \\right]\n$$\nThe term $(\\mathcal{O}(x) - Y)$ is equal to $(\\mathcal{O}(x) - (\\mathcal{O}(x) + \\eta)) = -\\eta$. The expectation of this term is:\n$$\n\\mathbb{E}_{Y|x} [-\\eta] = - \\mathbb{E}[\\eta] = 0\n$$\nSince the noise vector $\\eta$ has zero mean. Therefore, the second term in the expression for $I(x)$ vanishes. This leaves:\n$$\nI(x) = (O'(x))^T \\Gamma^{-1} O'(x)\n$$\nThis demonstrates that for a model with additive Gaussian noise, the Fisher Information Matrix is exactly equal to the Gauss-Newton approximation of the Hessian of the data misfit functional. The FIM quantifies the information that the data provides about the parameter $x$, and its inverse gives the Cramér-Rao lower bound on the variance of any unbiased estimator for $x$.", "answer": "$$\n\\boxed{\\frac{100}{9}\\exp(1) + 100\\cos^2(1) + \\frac{576}{625}}\n$$", "id": "3374134"}, {"introduction": "Moving from finite-dimensional parameters to functions, such as fields in a partial differential equation, introduces deeper questions about the mathematical validity of the problem itself. This exercise challenges the assumption that an observation operator is always well-behaved, forcing a rigorous examination of the interplay between the parameter space's regularity and the nature of the observation. By applying Sobolev trace theorems, you will determine the precise conditions required for an observation operator to be bounded, a prerequisite for a well-posed Bayesian inverse problem [@problem_id:3374201].", "problem": "Consider a bounded Lipschitz domain $\\Omega \\subset \\mathbb{R}^d$ with $d \\in \\{2,3\\}$ and let $\\{\\gamma_j\\}_{j=1}^M$ be $M \\in \\mathbb{N}$ fixed $C^1$ curves contained in $\\Omega$, each endowed with arc-length measure $ds$ and finite length. Define the observation operator $H$ by\n$$\nH(u)_j \\;=\\; \\int_{\\gamma_j} u \\, ds \\quad \\text{for } j=1,\\dots,M,\n$$\nso that $H(u) \\in \\mathbb{R}^M$ for any function $u$ for which the line integrals are meaningful. Suppose the parameter space is the Sobolev space $H^1(\\Omega)$ and that the prior on $u$ is a Gaussian measure on $H^1(\\Omega)$ with full support. Observations are given by the additive-noise model\n$$\ny \\;=\\; H(u) + \\eta,\n$$\nwhere $y \\in \\mathbb{R}^M$ and $\\eta \\sim \\mathcal{N}(0,\\Gamma)$ with $\\Gamma \\in \\mathbb{R}^{M \\times M}$ symmetric positive definite. In the Bayesian formulation, the posterior $\\mu^y$ on $u$ is defined via the likelihood potential\n$$\n\\Phi(u;y) \\;=\\; \\tfrac{1}{2}\\,\\| \\Gamma^{-1/2}(H(u)-y)\\|_{\\mathbb{R}^M}^2,\n$$\nprovided $H(u)$ is well-defined and the mapping $H:H^1(\\Omega)\\to\\mathbb{R}^M$ is suitably bounded to ensure the posterior is well-defined and stable. Using only fundamental properties of Sobolev spaces, trace theorems onto lower-dimensional manifolds, and the definition of bounded linear operators between Banach spaces, determine which statements about regularity requirements on $u$ and the consequences for posterior well-posedness are correct.\n\nChoose all that apply:\n\nA. If $d=2$ and each $\\gamma_j$ is $C^1$, then the trace of $u \\in H^1(\\Omega)$ onto $\\gamma_j$ is well-defined and belongs to $L^2(\\gamma_j)$; consequently, each functional $u \\mapsto \\int_{\\gamma_j} u\\,ds$ is bounded on $H^1(\\Omega)$, $H:H^1(\\Omega)\\to\\mathbb{R}^M$ is bounded linear, and with $\\eta \\sim \\mathcal{N}(0,\\Gamma)$ the Bayesian posterior is well-defined and stable with respect to data perturbations in the Hellinger Distance (HD).\n\nB. If $d=3$ and the prior is supported on $H^1(\\Omega)$, then $H:H^1(\\Omega)\\to\\mathbb{R}^M$ is bounded by the trace theorem onto curves, and therefore the posterior is well-posed under additive Gaussian noise.\n\nC. If $d=3$ and the parameter space is upgraded to $H^s(\\Omega)$ with $s>1$, with a Gaussian prior supported on $H^s(\\Omega)$, then $H:H^s(\\Omega)\\to\\mathbb{R}^M$ is bounded; with $\\eta \\sim \\mathcal{N}(0,\\Gamma)$, the Bayesian posterior is well-defined and stable under small data perturbations in HD.\n\nD. For any $d\\ge 2$, $H:L^2(\\Omega)\\to\\mathbb{R}^M$ is bounded because $\\int_{\\gamma_j} u\\,ds \\le \\|u\\|_{L^2(\\Omega)}\\,\\|\\mathbf{1}_{\\gamma_j}\\|_{L^2(\\Omega)}$ by the Cauchy–Schwarz inequality, so no additional regularity beyond $L^2(\\Omega)$ is needed for posterior well-posedness.", "solution": "The core of the problem is to determine the conditions under which the observation operator $H: X \\to \\mathbb{R}^M$ is a bounded linear operator, where $X$ is the parameter space (a Sobolev space). The posterior distribution $\\mu^y$ is well-defined and stable with respect to perturbations in the data $y$ (in the Hellinger distance, for instance) if the forward operator $H$ is bounded and linear, given the Gaussian prior and noise structure. The operator $H(u) = (\\int_{\\gamma_1} u \\, ds, \\dots, \\int_{\\gamma_M} u \\, ds)$ is linear by the linearity of the integral. For it to be bounded, each component functional $H_j(u) = \\int_{\\gamma_j} u \\, ds$ must be a bounded linear functional on the parameter space $X$.\n\nBy the Cauchy-Schwarz inequality on $L^2(\\gamma_j)$, we have:\n$$\n|H_j(u)| = \\left| \\int_{\\gamma_j} u \\cdot 1 \\, ds \\right| \\le \\|u|_{\\gamma_j}\\|_{L^2(\\gamma_j)} \\|1\\|_{L^2(\\gamma_j)}\n$$\nSince each curve $\\gamma_j$ has finite length $L_j$, $\\|1\\|_{L^2(\\gamma_j)} = (\\int_{\\gamma_j} 1^2 \\, ds)^{1/2} = \\sqrt{L_j}$, which is a finite constant. Therefore, the functional $H_j$ is bounded on a Sobolev space $X=H^s(\\Omega)$ if and only if the trace operator $u \\mapsto u|_{\\gamma_j}$ is a bounded operator from $H^s(\\Omega)$ to $L^2(\\gamma_j)$.\n\nThe general trace theorem for restriction to a $k$-dimensional manifold $\\Sigma \\subset \\Omega \\subset \\mathbb{R}^d$ states that the trace operator $T_\\Sigma: H^s(\\Omega) \\to L^2(\\Sigma)$ is bounded if and only if $s > \\frac{d-k}{2}$. In our problem, the curves $\\gamma_j$ are $1$-dimensional manifolds, so $k=1$. The condition for a bounded trace from $H^s(\\Omega)$ to $L^2(\\gamma_j)$ is thus $s > \\frac{d-1}{2}$.\n\nWe now evaluate each option based on this criterion.\n\n**A. If $d=2$ and each $\\gamma_j$ is $C^1$, then the trace of $u \\in H^1(\\Omega)$ onto $\\gamma_j$ is well-defined and belongs to $L^2(\\gamma_j)$; consequently, each functional $u \\mapsto \\int_{\\gamma_j} u\\,ds$ is bounded on $H^1(\\Omega)$, $H:H^1(\\Omega)\\to\\mathbb{R}^M$ is bounded linear, and with $\\eta \\sim \\mathcal{N}(0,\\Gamma)$ the Bayesian posterior is well-defined and stable with respect to data perturbations in the Hellinger Distance (HD).**\n\nHere, the dimension of the domain is $d=2$ and the parameter space is $H^1(\\Omega)$, so the Sobolev exponent is $s=1$. The condition for a bounded trace is $s > \\frac{d-1}{2}$. Substituting the values, we check if $1 > \\frac{2-1}{2}$, which simplifies to $1 > \\frac{1}{2}$. This inequality is true.\nTherefore, the trace operator from $H^1(\\Omega)$ to $L^2(\\gamma_j)$ is bounded. This implies that for any $u \\in H^1(\\Omega)$, its trace $u|_{\\gamma_j}$ is in $L^2(\\gamma_j)$ and there exists a constant $C_j$ such that $\\|u|_{\\gamma_j}\\|_{L^2(\\gamma_j)} \\le C_j \\|u\\|_{H^1(\\Omega)}$.\nConsequently, $|\\int_{\\gamma_j} u \\, ds| \\le \\|u|_{\\gamma_j}\\|_{L^2(\\gamma_j)} \\sqrt{L_j} \\le C_j\\sqrt{L_j}\\|u\\|_{H^1(\\Omega)}$, which means each functional $H_j$ is bounded on $H^1(\\Omega)$.\nThis establishes that $H: H^1(\\Omega) \\to \\mathbb{R}^M$ is a bounded linear operator. For a Bayesian inverse problem with a Gaussian prior on a Hilbert space and a bounded linear forward map, the posterior measure is well-defined (being absolutely continuous with respect to the prior) and stable with respect to perturbations in data $y$. The statement is entirely correct.\n\n**Verdict: Correct.**\n\n**B. If $d=3$ and the prior is supported on $H^1(\\Omega)$, then $H:H^1(\\Omega)\\to\\mathbb{R}^M$ is bounded by the trace theorem onto curves, and therefore the posterior is well-posed under additive Gaussian noise.**\n\nHere, the dimension of the domain is $d=3$ and the parameter space is $H^1(\\Omega)$, so $s=1$. The condition for a bounded trace is $s > \\frac{d-1}{2}$. Substituting the values, we check if $1 > \\frac{3-1}{2}$, which simplifies to $1 > 1$. This inequality is false.\nThis is the critical case where the trace theorem fails. The trace operator from $H^1(\\Omega)$ to $L^2(\\gamma_j)$ is not bounded for $d=3$. Functions in $H^1(\\mathbb{R}^3)$ are not necessarily continuous, and their restriction to a line is not well-defined in $L^2$ of the line. Therefore, the operator $H:H^1(\\Omega) \\to \\mathbb{R}^M$ is not bounded. The premise of the statement is incorrect, which invalidates the conclusion about posterior well-posedness based on this reasoning. The posterior is not guaranteed to be well-defined in this setting.\n\n**Verdict: Incorrect.**\n\n**C. If $d=3$ and the parameter space is upgraded to $H^s(\\Omega)$ with $s>1$, with a Gaussian prior supported on $H^s(\\Omega)$, then $H:H^s(\\Omega)\\to\\mathbb{R}^M$ is bounded; with $\\eta \\sim \\mathcal{N}(0,\\Gamma)$, the Bayesian posterior is well-defined and stable under small data perturbations in HD.**\n\nHere, the dimension of the domain is $d=3$ and the parameter space is $H^s(\\Omega)$ with the explicit condition $s>1$. The condition for a bounded trace is $s > \\frac{d-1}{2}$, which is $s > \\frac{3-1}{2}$, or $s > 1$. The problem's assumption $s>1$ exactly matches the requirement of the trace theorem.\nThus, for any $s>1$, the trace operator from $H^s(\\Omega)$ to $L^2(\\gamma_j)$ is bounded. Following the same logic as in option A, this ensures that the forward operator $H:H^s(\\Omega) \\to \\mathbb{R}^M$ is a bounded linear operator. With this property, and given the Gaussian prior on the Hilbert space $H^s(\\Omega)$, the Bayesian posterior is well-defined and stable. This statement accurately describes how increasing the regularity of the parameter space (i.e., choosing $s>1$) remedies the ill-posedness found in the $d=3, s=1$ case.\n\n**Verdict: Correct.**\n\n**D. For any $d\\ge 2$, $H:L^2(\\Omega)\\to\\mathbb{R}^M$ is bounded because $\\int_{\\gamma_j} u\\,ds \\le \\|u\\|_{L^2(\\Omega)}\\,\\|\\mathbf{1}_{\\gamma_j}\\|_{L^2(\\Omega)}$ by the Cauchy–Schwarz inequality, so no additional regularity beyond $L^2(\\Omega)$ is needed for posterior well-posedness.**\n\nThis statement contains a fundamental mathematical error in its application of the Cauchy-Schwarz inequality. The inequality $|\\langle f, g \\rangle| \\le \\|f\\| \\|g\\|$ applies to an inner product on a single space. The expression presented attempts to mix an integral over a $1$-dimensional manifold $\\gamma_j$ with norms defined over a $d$-dimensional domain $\\Omega$. The correct Cauchy-Schwarz inequality on $L^2(\\Omega)$ is $|\\int_{\\Omega} f g \\, dV| \\le \\|f\\|_{L^2(\\Omega)} \\|g\\|_{L^2(\\Omega)}$, where the integration domain is $\\Omega$.\nFurthermore, the term $\\|\\mathbf{1}_{\\gamma_j}\\|_{L^2(\\Omega)}$ is the $L^2$-norm of the indicator function of the curve $\\gamma_j$ over the domain $\\Omega$. Since a $C^1$ curve is a set of Lebesgue measure zero in $\\mathbb{R}^d$ for $d \\ge 2$, we have $\\|\\mathbf{1}_{\\gamma_j}\\|_{L^2(\\Omega)} = \\left(\\int_{\\Omega} (\\mathbf{1}_{\\gamma_j})^2 dV\\right)^{1/2} = (\\text{meas}(\\gamma_j))^{1/2} = 0$. The proposed inequality would then incorrectly imply $\\int_{\\gamma_j} u\\,ds \\le 0$ for any function $u$, which is absurd.\nA function in $L^2(\\Omega)$ has no well-defined trace on a lower-dimensional manifold like a curve. One can easily construct a sequence of functions $\\{u_n\\} \\subset C_c^\\infty(\\Omega)$ such that $\\|u_n\\|_{L^2(\\Omega)}$ is constant, but $\\int_{\\gamma_j} u_n \\, ds \\to \\infty$. This demonstrates the operator $H$ is not well-defined, let alone bounded, on $L^2(\\Omega)$. Regularity beyond $L^2(\\Omega)$ is essential.\n\n**Verdict: Incorrect.**", "answer": "$$\\boxed{AC}$$", "id": "3374201"}]}