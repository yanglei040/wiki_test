{"hands_on_practices": [{"introduction": "A key intuition behind M-estimators is their ability to automatically down-weight outliers that would otherwise corrupt a standard least-squares estimate. This practice provides a concrete look at this mechanism through the lens of the Iteratively Reweighted Least Squares (IRLS) algorithm. By deriving and calculating the weights for the celebrated Huber M-estimator, you will see exactly how the influence of an observation is adjusted based on its residual, connecting the mathematical form of the loss function to the practical outcome of robustness [@problem_id:3418133].", "problem": "Consider a linear inverse problem in data assimilation where one seeks to estimate a state vector $x$ from observations $y$ via a linear observation operator $H$, so that the residuals are $r=y-Hx$. To improve robustness against outliers, suppose the objective is to minimize the sum of a robust loss over residuals, specifically the Huber loss with tuning constant $c0$. The Huber loss is defined by\n$$\n\\rho_{c}(r)=\\begin{cases}\n\\frac{1}{2}r^{2},  |r|\\leq c,\\\\\nc|r|-\\frac{1}{2}c^{2},  |r|c.\n\\end{cases}\n$$\nIn the Iteratively Reweighted Least Squares (IRLS) scheme for robust $M$-estimation, the gradient of the robust objective $\\sum_{i}\\rho_{c}(r_{i})$ with respect to $x$ is computed using the score function $\\psi_{c}(r)=\\frac{d}{dr}\\rho_{c}(r)$, and one seeks a weighted least-squares system whose normal equations match those of the robust objective near a given iterate. Starting from these fundamental definitions, derive the IRLS weight function $w(r)$ associated with the Huber loss by equating the gradients of the robust objective and a weighted least-squares objective. Then, for the residuals $r=\\begin{pmatrix}-5  -1  0.2  3\\end{pmatrix}$ and tuning constant $c=1.5$, compute the Huber IRLS weights $w(r_{i})$ for each residual. Finally, briefly interpret the effect of these weights on the contribution of each observation to the update in a Gauss–Newton step. Express the final weights as a row vector. No rounding is required; provide exact values.", "solution": "The problem asks for the derivation of the Iteratively Reweighted Least Squares (IRLS) weight function for the Huber loss, the calculation of these weights for a specific set of residuals, and an interpretation of the results.\n\nFirst, we derive the IRLS weight function, $w(r)$, associated with the Huber loss, $\\rho_c(r)$. The M-estimation seeks to minimize the objective function $J_{robust}(x) = \\sum_{i} \\rho_{c}(r_i)$, where $r = y - Hx$ is the vector of residuals. The gradient of this objective function with respect to the state vector $x$ is found using the chain rule:\n$$ \\nabla_x J_{robust}(x) = \\sum_i \\frac{d\\rho_c(r_i)}{dr_i} \\nabla_x r_i $$\nThe derivative of the residual component $r_i = y_i - \\sum_k H_{ik} x_k$ with respect to $x$ is $\\nabla_x r_i = -h_i^T$, where $h_i$ is the $i$-th row of the matrix $H$. The derivative of the loss function, $\\frac{d\\rho_c(r)}{dr}$, is defined as the score function, $\\psi_c(r)$. Let us compute $\\psi_c(r)$ from the definition of the Huber loss:\n$$ \\rho_{c}(r)=\\begin{cases} \\frac{1}{2}r^{2},  |r|\\leq c\\\\ c|r|-\\frac{1}{2}c^{2},  |r|c \\end{cases} $$\nFor $|r| \\leq c$, the derivative is $\\psi_c(r) = \\frac{d}{dr} \\left( \\frac{1}{2}r^2 \\right) = r$.\nFor $|r|  c$, we consider two cases. If $r  c$, $\\rho_c(r) = cr - \\frac{1}{2}c^2$, so $\\psi_c(r) = c$. If $r  -c$, $\\rho_c(r) = -cr - \\frac{1}{2}c^2$, so $\\psi_c(r) = -c$. In both cases, $\\psi_c(r) = c \\cdot \\text{sgn}(r)$.\nCombining these, the score function is:\n$$ \\psi_c(r) = \\begin{cases} r,  |r| \\leq c \\\\ c \\cdot \\text{sgn}(r),  |r|  c \\end{cases} $$\nThe gradient of the robust objective function is therefore $\\nabla_x J_{robust}(x) = -\\sum_i h_i^T \\psi_c(r_i) = -H^T \\vec{\\psi}_c(r)$, where $\\vec{\\psi}_c(r)$ is the column vector with elements $\\psi_c(r_i)$. The necessary condition for a minimum is that the gradient is zero, which gives the normal equations for the M-estimator:\n$$ H^T \\vec{\\psi}_c(r) = 0 $$\nThe IRLS algorithm solves this nonlinear system by iteratively solving a weighted least-squares problem. The objective function for a weighted least-squares problem is $J_{WLS}(x) = \\frac{1}{2} \\sum_i w_i r_i^2 = \\frac{1}{2}(y-Hx)^T W (y-Hx)$, where $W$ is a diagonal matrix of weights $w_i$. The gradient of this objective is:\n$$ \\nabla_x J_{WLS}(x) = -H^T W (y-Hx) = -H^T W r $$\nThe corresponding normal equations are:\n$$ H^T W r = 0 $$\nThe core principle of IRLS is to define the weights $w_i$ such that the normal equations of the WLS problem match those of the M-estimator. By comparing the two sets of normal equations, we equate the terms multiplying $H^T$:\n$$ W r = \\vec{\\psi}_c(r) $$\nThis equivalence must hold for each component $i$, so $w_i r_i = \\psi_c(r_i)$. The weight function $w(r)$ can thus be defined as:\n$$ w(r) = \\frac{\\psi_c(r)}{r} $$\nSubstituting the expression for $\\psi_c(r)$:\nFor $|r| \\leq c$ (and $r \\neq 0$), $w(r) = \\frac{r}{r} = 1$. For $r=0$, we can define $w(0) = \\lim_{r \\to 0} \\frac{r}{r} = 1$.\nFor $|r|  c$, $w(r) = \\frac{c \\cdot \\text{sgn}(r)}{r} = \\frac{c}{|r|}$.\nTherefore, the IRLS weight function for the Huber loss is:\n$$ w(r) = \\begin{cases} 1,  |r| \\leq c \\\\ \\frac{c}{|r|},  |r|  c \\end{cases} $$\nThis can be written more compactly as $w(r) = \\min(1, \\frac{c}{|r|})$.\n\nNext, we compute the weights for the given residuals $r=\\begin{pmatrix}-5  -1  0.2  3\\end{pmatrix}$ and tuning constant $c=1.5$. We apply the derived weight function to each residual component $r_i$.\n\nFor $r_1 = -5$: The absolute value is $|r_1|=5$. Since $5  1.5$, this is an outlier.\n$w(r_1) = \\frac{c}{|r_1|} = \\frac{1.5}{5} = \\frac{3/2}{5} = \\frac{3}{10}$.\n\nFor $r_2 = -1$: The absolute value is $|r_2|=1$. Since $1 \\leq 1.5$, this is an inlier.\n$w(r_2) = 1$.\n\nFor $r_3 = 0.2$: The absolute value is $|r_3|=0.2$. Since $0.2 \\leq 1.5$, this is an inlier.\n$w(r_3) = 1$.\n\nFor $r_4 = 3$: The absolute value is $|r_4|=3$. Since $3  1.5$, this is an outlier.\n$w(r_4) = \\frac{c}{|r_4|} = \\frac{1.5}{3} = \\frac{1}{2}$.\n\nThe computed weights are $w(r_1)=\\frac{3}{10}$, $w(r_2)=1$, $w(r_3)=1$, and $w(r_4)=\\frac{1}{2}$.\n\nFinally, for the interpretation of these weights: The IRLS algorithm iteratively approximates the solution to the M-estimation problem by solving a sequence of weighted least-squares problems. The weights $w_i$ determine the influence of each observation $y_i$ on the solution update at each step.\nObservations with small residuals ($|r_i| \\le c$) are considered inliers. For these, the Huber weights are $w_i=1$, meaning they contribute to the least-squares fit with their full strength, just as in a standard (unweighted) least-squares problem. In our case, the observations corresponding to residuals $r_2=-1$ and $r_3=0.2$ are treated as inliers.\nObservations with large residuals ($|r_i|  c$) are flagged as outliers. Their weights are set to $w_i = c/|r_i|  1$. This down-weights their contribution to the cost function, reducing their influence on the solution for $x$. The larger the residual, the smaller the weight. For our data, the observation with residual $r_4=3$ is moderately down-weighted to $\\frac{1}{2}$, while the observation with the largest residual $r_1=-5$ is more significantly down-weighted to $\\frac{3}{10}$. This mechanism ensures that potential outliers do not dominate the estimation process, making the final estimate of $x$ more robust to gross errors in the data $y$.", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{3}{10}  1  1  \\frac{1}{2} \\end{pmatrix} } $$", "id": "3418133"}, {"introduction": "While some M-estimators like Huber's only reduce the influence of outliers, a class of \"redescending\" estimators goes a step further by completely rejecting observations with very large residuals. This powerful feature, however, can introduce a significant challenge: non-convexity in the objective function. This hands-on coding exercise allows you to visualize this trade-off directly, demonstrating how redescending behavior can create spurious local minima and lead to a complete breakdown of the estimate under heavy contamination [@problem_id:3418043].", "problem": "Consider a one-dimensional inverse problem with a nonlinear forward model, where the observation operator is the hyperbolic tangent $F(x) = \\tanh(x)$. Let the data consist of $N_{\\mathrm{in}}$ inliers generated from a true parameter $x^\\star$ without noise, so that each inlier observation equals $y_i = F(x^\\star)$, and $N_{\\mathrm{out}}$ adversarially placed contaminated points, each equaling $y_j = F(x^{\\dagger})$ for some adversarial parameter $x^{\\dagger}$ selected by a contamination process. Assume a robust $M$-estimator (maximum likelihood type estimator) with an $M$-estimation loss $\\rho$ that is continuously differentiable and redescending (its influence function goes to zero for large residuals). Let the objective be the sum of losses,\n$$\nJ(x) = \\sum_{k=1}^{N} \\rho\\!\\left(\\frac{y_k - F(x)}{s}\\right) s^2,\n$$\nwhere $N = N_{\\mathrm{in}} + N_{\\mathrm{out}}$, $s  0$ is a fixed robust scale, and $\\rho$ is Tukey’s bisquare (also known as Tukey’s biweight), defined for a standardized residual $t$ and threshold $c  0$ by\n$$\n\\rho(t) =\n\\begin{cases}\n\\dfrac{c^2}{6}\\left[1 - \\left(1 - \\left(\\dfrac{t}{c}\\right)^2\\right)^3\\right],  \\text{if } |t| \\le c, \\\\\n\\dfrac{c^2}{6},  \\text{if } |t|  c.\n\\end{cases}\n$$\nYou will investigate when a redescending loss produces spurious local minima and when identifiability is lost. The fundamental starting points you should use are (i) the definition of the $M$-estimator as a minimizer of the sum of a prescribed loss of residuals, (ii) the definition and properties of redescending losses, and (iii) the deterministic forward mapping $F(x) = \\tanh(x)$ for the observation operator.\n\nTasks:\n1. Construct a counterexample dataset where a redescending $\\rho$ produces spurious local minima. Use the forward model $F(x) = \\tanh(x)$, an $M$-estimator objective $J(x)$ as above, with fixed scale $s$ and Tukey’s bisquare with threshold $c$. Consider a contamination scheme where $N_{\\mathrm{in}}$ inliers equal $F(x^\\star)$ and $N_{\\mathrm{out}}$ outliers equal $F(x^{\\dagger})$. Explain why, for sufficiently separated $F(x^\\star)$ and $F(x^{\\dagger})$ relative to the threshold $c s$, the objective $J(x)$ can develop at least two local minima near $x^\\star$ and near $x^{\\dagger}$. Your program must detect local minima by evaluating $J(x)$ on a uniform grid and counting points that are strict local minima, and it must report the approximate global minimizer.\n2. Characterize conditions on $F$ and the contamination pattern under which identifiability is lost. Starting from the definitions, argue in terms of the separation $|F(x^\\star) - F(x^{\\dagger})|$ versus the cutoff $c s$, and the sample sizes $N_{\\mathrm{in}}$ and $N_{\\mathrm{out}}$, when the global minimizer of $J(x)$ will concentrate near $x^\\star$ or near $x^{\\dagger}$. Your program must calculate and report a boolean that indicates whether the separation condition $|F(x^\\star) - F(x^{\\dagger})|  c s$ holds, and a boolean that indicates whether identifiability is lost as judged by the location of the global minimizer relative to $x^\\star$.\n\nNumerical instructions:\n- Use a search domain $x \\in [-6, 6]$ sampled on a uniform grid sufficiently fine to detect distinct minima.\n- Implement $F(x) = \\tanh(x)$ exactly.\n- Implement Tukey’s bisquare $\\rho$ as given above, applied to standardized residuals $(y - F(x))/s$, and multiply by $s^2$ inside the sum to put the objective in the original residual units.\n- The detection of local minima should be based on strict inequalities $J(x_{i-1})  J(x_i)  J(x_{i+1})$ on the grid.\n- Determine the approximate global minimizer $\\hat{x}$ as the grid point with the smallest objective value. Use a tolerance $\\tau = 0.5$ to judge proximity: identifiability is considered lost if $|\\hat{x} - x^\\star|  \\tau$.\n\nTest suite:\nEvaluate the following four cases. In each case, use Tukey’s threshold $c = 4.685$, and report results in the order and types specified below.\n- Case 1 (happy path, no contamination): $x^\\star = 3.0$, $x^{\\dagger} = -3.0$, $N_{\\mathrm{in}} = 30$, $N_{\\mathrm{out}} = 0$, $s = 0.1$.\n- Case 2 (spurious local minima but correct global minimizer): $x^\\star = 3.0$, $x^{\\dagger} = -3.0$, $N_{\\mathrm{in}} = 30$, $N_{\\mathrm{out}} = 5$, $s = 0.1$.\n- Case 3 (identifiability lost under heavy contamination): $x^\\star = 3.0$, $x^{\\dagger} = -3.0$, $N_{\\mathrm{in}} = 10$, $N_{\\mathrm{out}} = 25$, $s = 0.1$.\n- Case 4 (boundary regime with reduced separation relative to cutoff): $x^\\star = 3.0$, $x^{\\dagger} = -3.0$, $N_{\\mathrm{in}} = 20$, $N_{\\mathrm{out}} = 20$, $s = 0.5$.\n\nRequired outputs:\nFor each case, your program must compute and collect the following four quantities in order:\n- The approximate global minimizer $\\hat{x}$ as a float, rounded to three decimals.\n- The number of strict local minima on the grid as an integer.\n- A boolean indicating whether identifiability is lost, defined by $|\\hat{x} - x^\\star|  \\tau$ with $\\tau = 0.5$.\n- A boolean indicating whether the separation condition $|F(x^\\star) - F(x^{\\dagger})|  c s$ holds.\n\nFinal output format:\nYour program should produce a single line of output containing the results aggregated for the four cases, in a comma-separated list enclosed in square brackets, in the following order:\n$$\n[\\hat{x}_1, \\text{nmin}_1, \\text{lost}_1, \\text{sep}_1, \\hat{x}_2, \\text{nmin}_2, \\text{lost}_2, \\text{sep}_2, \\hat{x}_3, \\text{nmin}_3, \\text{lost}_3, \\text{sep}_3, \\hat{x}_4, \\text{nmin}_4, \\text{lost}_4, \\text{sep}_4].\n$$\nNo additional text should be printed. All angles are dimensionless, and there are no physical units in this problem. Floats must be rounded to three decimals as specified; booleans must be literal logical values.", "solution": "The validity of the problem statement is confirmed. It is a well-posed, scientifically grounded problem in the field of robust statistics and inverse problems. All necessary definitions, parameters, and numerical instructions are provided, and there are no contradictions or ambiguities.\n\nThe problem investigates the behavior of a robust $M$-estimator for a one-dimensional inverse problem with a nonlinear forward model $F(x) = \\tanh(x)$. The core of the analysis lies in the properties of the objective function $J(x)$, which is constructed using a redescending loss function, specifically Tukey's bisquare loss, $\\rho(t)$.\n\nThe objective function to be minimized is given by:\n$$\nJ(x) = \\sum_{k=1}^{N} \\rho\\!\\left(\\frac{y_k - F(x)}{s}\\right) s^2\n$$\nwhere $y_k$ are the observations, $F(x) = \\tanh(x)$ is the forward model, and $s  0$ is a fixed scale parameter. The sum is over all $N = N_{\\mathrm{in}} + N_{\\mathrm{out}}$ data points, comprising $N_{\\mathrm{in}}$ inliers with value $y_i = F(x^\\star)$ and $N_{\\mathrm{out}}$ outliers with value $y_j = F(x^{\\dagger})$.\n\nThe Tukey bisquare loss function $\\rho(t)$ for a standardized residual $t$ is defined with a tuning constant $c  0$ as:\n$$\n\\rho(t) =\n\\begin{cases}\n\\dfrac{c^2}{6}\\left[1 - \\left(1 - \\left(\\dfrac{t}{c}\\right)^2\\right)^3\\right],  \\text{if } |t| \\le c, \\\\\n\\dfrac{c^2}{6},  \\text{if } |t|  c.\n\\end{cases}\n$$\nA key feature of this loss function is that it is \"redescending.\" This means its corresponding influence function, $\\psi(t) = \\rho'(t)$, is zero for large residuals. Specifically, for $|t|  c$, the loss $\\rho(t)$ becomes constant at its maximum value, $\\frac{c^2}{6}$. Consequently, its derivative $\\psi(t)$ is zero for $|t|  c$. This property allows the estimator to completely reject observations that are sufficiently far from the model prediction. An observation $y_k$ is rejected when its standardized residual $t_k = (y_k - F(x))/s$ satisfies $|t_k|  c$, or equivalently, when the absolute residual $|y_k - F(x)|$ exceeds the cutoff distance $cs$.\n\n**1. Emergence of Spurious Local Minima**\n\nSpurious local minima arise directly from the redescending nature of the loss function when the data is clustered into well-separated groups. Let's analyze the objective function $J(x)$ for a candidate solution $x$ that is close to the true parameter $x^\\star$.\n\nThe data consists of two groups: inliers at $y^\\star = F(x^\\star)$ and outliers at $y^{\\dagger} = F(x^{\\dagger})$.\nWhen the candidate solution $x$ is near $x^\\star$, the residuals for the inliers, $|y^\\star - F(x)|$, are small. In contrast, the residuals for the outliers are approximately $|y^{\\dagger} - F(x)| \\approx |F(x^{\\dagger}) - F(x^\\star)|$.\n\nIf the separation between the inlier and outlier observations is greater than the cutoff distance, i.e., if the condition $|F(x^\\star) - F(x^{\\dagger})|  cs$ holds, then for any $x$ sufficiently close to $x^\\star$, the residuals corresponding to the outliers will satisfy $|F(x^{\\dagger}) - F(x)|  cs$. For these outliers, the loss function $\\rho$ saturates to its constant maximum value, $\\frac{c^2}{6}$.\n\nUnder this separation condition, the objective function in a neighborhood of $x^\\star$ can be approximated as:\n$$\nJ(x) \\approx \\sum_{i=1}^{N_{\\mathrm{in}}} \\rho\\left(\\frac{F(x^\\star) - F(x)}{s}\\right)s^2 + \\sum_{j=1}^{N_{\\mathrm{out}}} \\left(\\frac{c^2}{6}\\right)s^2\n$$\nThe second term is a constant. The first term is minimized when $F(x)$ is closest to $F(x^\\star)$, i.e., at $x = x^\\star$. Thus, a local minimum of $J(x)$ exists near $x^\\star$.\n\nBy a symmetric argument, if we consider a candidate solution $x$ near the adversarial parameter $x^{\\dagger}$, the inliers now appear as outliers. If the separation condition $|F(x^\\star) - F(x^{\\dagger})|  cs$ holds, then for $x$ close to $x^{\\dagger}$, the residuals for the inliers will satisfy $|F(x^\\star) - F(x)|  cs$. These inliers are now rejected. The objective function in a neighborhood of $x^{\\dagger}$ is approximately:\n$$\nJ(x) \\approx \\sum_{i=1}^{N_{\\mathrm{in}}} \\left(\\frac{c^2}{6}\\right)s^2 + \\sum_{j=1}^{N_{\\mathrm{out}}} \\rho\\left(\\frac{F(x^{\\dagger}) - F(x)}{s}\\right)s^2\n$$\nThis creates a second local minimum near $x = x^{\\dagger}$. Therefore, when the data groups are separated by more than the robust cutoff distance $cs$, the objective function develops at least two local minima, one corresponding to each data cluster. This is the mechanism for the creation of spurious minima.\n\n**2. Loss of Identifiability**\n\nIdentifiability is the ability to uniquely determine the true parameter $x^\\star$ from the data. In this context, we consider identifiability to be lost if the global minimizer of $J(x)$, denoted $\\hat{x}$, is located far from $x^\\star$ and instead converges to the adversarial location $x^{\\dagger}$.\n\nAssuming the separation condition $|F(x^\\star) - F(x^{\\dagger})|  cs$ holds, we have established the existence of local minima near $x^\\star$ and $x^{\\dagger}$. To determine which is the global minimum, we compare the values of the objective function at these two points.\n\nThe value of the objective function at the local minimum near $x^\\star$ is approximately:\n$$\nJ(x^\\star) = \\sum_{i=1}^{N_{\\mathrm{in}}} \\rho(0)s^2 + \\sum_{j=1}^{N_{\\mathrm{out}}} \\rho\\left(\\frac{F(x^{\\dagger}) - F(x^\\star)}{s}\\right)s^2 \\approx 0 + N_{\\mathrm{out}} \\left(\\frac{c^2}{6}\\right)s^2\n$$\nsince $\\rho(0)=0$ and the residuals for the outliers exceed the cutoff.\n\nSimilarly, the value at the local minimum near $x^{\\dagger}$ is approximately:\n$$\nJ(x^{\\dagger}) = \\sum_{i=1}^{N_{\\mathrm{in}}} \\rho\\left(\\frac{F(x^\\star) - F(x^{\\dagger})}{s}\\right)s^2 + \\sum_{j=1}^{N_{\\mathrm{out}}} \\rho(0)s^2 \\approx N_{\\mathrm{in}} \\left(\\frac{c^2}{6}\\right)s^2 + 0\n$$\n\nThe global minimizer $\\hat{x}$ will be near $x^\\star$ if $J(x^\\star)  J(x^{\\dagger})$, which implies $N_{\\mathrm{out}}  N_{\\mathrm{in}}$.\nConversely, the global minimizer will be near $x^{\\dagger}$ if $J(x^\\star)  J(x^{\\dagger})$, which implies $N_{\\mathrm{out}}  N_{\\mathrm{in}}$.\n\nTherefore, under the stated separation condition, identifiability is lost when the number of outliers exceeds the number of inliers ($N_{\\mathrm{out}}  N_{\\mathrm{in}}$). The estimator, in its attempt to fit the majority of the data, discards the true inlier cluster and locks onto the larger outlier cluster. The breakdown point of this estimator—the fraction of contamination it can tolerate—is effectively $50\\%$.\n\nWhen the separation condition $|F(x^\\star) - F(x^{\\dagger})|  cs$ does not hold (e.g., if the scale $s$ is large), the two basins of attraction merge. The outliers are not fully rejected and exert a continuous \"pull\" on the solution. The location of the minimum will then depend on a more complex interplay between the locations and relative weights of the data clusters, and the simple comparison above is no longer valid. In such cases, the global minimum might be located somewhere between $x^\\star$ and $x^{\\dagger}$, and may still be far from $x^\\star$, leading to a loss of identifiability according to the problem's criterion $|\\hat{x} - x^\\star|  \\tau$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the robust estimation problem for the four specified test cases.\n    \"\"\"\n\n    def F(x):\n        \"\"\"The forward model F(x) = tanh(x).\"\"\"\n        return np.tanh(x)\n\n    def tukey_bisquare_rho(t, c):\n        \"\"\"\n        Computes Tukey's bisquare loss function rho(t) for a given constant c.\n        This function is vectorized to work with numpy arrays.\n        \"\"\"\n        c_squared_over_6 = c**2 / 6.0\n        abs_t = np.abs(t)\n        # Initialize rho values to the constant part for |t| > c\n        rho_values = np.full_like(t, c_squared_over_6, dtype=float)\n        # Identify elements where |t| = c\n        mask = abs_t = c\n        # Compute rho for these elements\n        t_scaled = t[mask] / c\n        rho_values[mask] = c_squared_over_6 * (1.0 - (1.0 - t_scaled**2)**3)\n        return rho_values\n\n    def objective_function(x, y_data, s, c):\n        \"\"\"\n        Computes the M-estimator objective function J(x).\n        \"\"\"\n        residuals = y_data - F(x)\n        standardized_residuals = residuals / s\n        rho_vals = tukey_bisquare_rho(standardized_residuals, c)\n        return np.sum(rho_vals * s**2)\n\n    # Problem parameters\n    test_cases = [\n        # (x_star, x_dagger, N_in, N_out, s)\n        (3.0, -3.0, 30, 0, 0.1),  # Case 1\n        (3.0, -3.0, 30, 5, 0.1),  # Case 2\n        (3.0, -3.0, 10, 25, 0.1), # Case 3\n        (3.0, -3.0, 20, 20, 0.5), # Case 4\n    ]\n    \n    C_TUKEY = 4.685\n    TAU = 0.5\n    GRID_MIN = -6.0\n    GRID_MAX = 6.0\n    GRID_POINTS = 2001\n    \n    x_grid = np.linspace(GRID_MIN, GRID_MAX, GRID_POINTS)\n    \n    all_results = []\n\n    for case in test_cases:\n        x_star, x_dagger, N_in, N_out, s = case\n        \n        # 1. Construct the dataset\n        y_inliers = np.full(N_in, F(x_star))\n        y_outliers = np.full(N_out, F(x_dagger))\n        y_data = np.concatenate((y_inliers, y_outliers))\n        \n        # 2. Evaluate objective function on the grid\n        J_values = np.array([objective_function(xi, y_data, s, C_TUKEY) for xi in x_grid])\n        \n        # 3. Compute the required outputs\n        \n        # Find approximate global minimizer\n        min_idx = np.argmin(J_values)\n        x_hat = x_grid[min_idx]\n        \n        # Count strict local minima\n        num_local_minima = 0\n        for i in range(1, len(J_values) - 1):\n            if J_values[i-1] > J_values[i] and J_values[i]  J_values[i+1]:\n                num_local_minima += 1\n        \n        # Check if identifiability is lost\n        identifiability_lost = np.abs(x_hat - x_star) > TAU\n        \n        # Check separation condition\n        separation_holds = np.abs(F(x_star) - F(x_dagger)) > C_TUKEY * s\n        \n        # Aggregate results for this case\n        all_results.extend([\n            round(x_hat, 3),\n            num_local_minima,\n            identifiability_lost,\n            separation_holds\n        ])\n\n    # Final print statement in the exact required format.\n    # The format requires a string representation of the list.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3418043"}, {"introduction": "Solving robust estimation problems, especially large-scale ones that include regularization, requires efficient and scalable numerical algorithms. This exercise moves beyond conceptual understanding to algorithmic implementation by introducing a powerful modern optimization framework. You will derive the steps for the Alternating Direction Method of Multipliers (ADMM) applied to a regularized M-estimation problem, revealing the central role of the proximal operator in handling the robust loss term—a cornerstone technique in modern data science and inverse problems [@problem_id:3418091].", "problem": "Consider a linear inverse problem in data assimilation where the state vector $x \\in \\mathbb{R}^{n}$ is inferred from noisy observations $y \\in \\mathbb{R}^{m}$ through a known linear observation operator $A \\in \\mathbb{R}^{m \\times n}$. To promote robustness against outliers in the observation errors, the data misfit is modeled by a robust loss function from the class of maximum-likelihood type estimators (M-estimators), denoted by a convex, proper, lower semicontinuous function $\\rho:\\mathbb{R} \\to \\mathbb{R}$ applied elementwise. A quadratic regularization is enforced through a linear regularization operator $L \\in \\mathbb{R}^{p \\times n}$ with strength $\\lambda  0$. The robust formulation reads\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\sum_{i=1}^{m} \\rho\\!\\left((A x - y)_{i}\\right) + \\lambda \\|L x\\|_{2}^{2}.\n$$\nIntroduce a splitting variable $z \\in \\mathbb{R}^{m}$ to represent the residual, constrained by $z = A x - y$. The Alternating Direction Method of Multipliers (ADMM) is to be applied to the constrained reformulation. Use the scaled-dual ADMM with penalty parameter $\\mu  0$ and scaled dual variable $u \\in \\mathbb{R}^{m}$ associated with the constraint $A x - y - z = 0$.\n\nStarting from the core definitions of the augmented Lagrangian for equality-constrained convex optimization and the proximal operator of a proper, convex, lower semicontinuous function, derive the explicit ADMM iteration. Your derivation must:\n- Set up the scaled augmented Lagrangian corresponding to the constraint $A x - y - z = 0$.\n- Obtain the $x$-update by minimizing the augmented Lagrangian with respect to $x$, expressed as a closed-form solution of a linear system.\n- Obtain the $z$-update by minimizing with respect to $z$ and express it using the proximal operator of $\\rho$.\n- State the scaled dual variable update.\n\nExpress the three updates in terms of $A$, $L$, $y$, $\\lambda$, $\\mu$, and the current iterates $(x^{k}, z^{k}, u^{k})$. Your final answer must be a single closed-form analytic expression that compactly lists the right-hand sides of the $x$-, $z$-, and scaled-dual updates as a row vector. No numerical approximation is required, and no units are involved. Do not include any equations or inequalities in your final boxed answer; present only the expressions for the updates as requested.", "solution": "We begin from the equality-constrained reformulation of the original robust objective by introducing the auxiliary variable $z \\in \\mathbb{R}^{m}$ to represent the observation residual. The problem becomes\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{m}} \\left[ \\sum_{i=1}^{m} \\rho(z_{i}) + \\lambda \\|L x\\|_{2}^{2} \\right] \\quad \\text{subject to} \\quad A x - y - z = 0.\n$$\nDefine the functions $f(x) = \\lambda \\|L x\\|_{2}^{2}$ and $g(z) = \\sum_{i=1}^{m} \\rho(z_{i})$. The Alternating Direction Method of Multipliers (ADMM) in its scaled-dual form introduces a scaled dual variable $u \\in \\mathbb{R}^{m}$ associated with the linear constraint, and uses an augmented Lagrangian penalty parameter $\\mu  0$. The scaled augmented Lagrangian is constructed using well-tested convex optimization principles:\n$$\n\\mathcal{L}_{\\mu}^{\\text{scaled}}(x, z, u) = f(x) + g(z) + \\frac{\\mu}{2} \\left\\|A x - y - z + u \\right\\|_{2}^{2} - \\frac{\\mu}{2} \\|u\\|_{2}^{2}.\n$$\nThe ADMM iteration alternates between minimizing $\\mathcal{L}_{\\mu}^{\\text{scaled}}$ with respect to $x$ and $z$, followed by a scaled dual variable update.\n\nFor the $x$-update, we minimize with respect to $x$ while keeping $(z^{k}, u^{k})$ fixed:\n$$\nx^{k+1} \\in \\arg\\min_{x} \\left[ \\lambda \\|L x\\|_{2}^{2} + \\frac{\\mu}{2} \\left\\|A x - y - z^{k} + u^{k} \\right\\|_{2}^{2} \\right].\n$$\nBoth terms are convex and differentiable in $x$. The first-order optimality condition (obtained by setting the gradient to zero) yields\n$$\n\\nabla_{x} \\left[ \\lambda \\|L x\\|_{2}^{2} \\right] + \\nabla_{x} \\left[ \\frac{\\mu}{2} \\left\\|A x - y - z^{k} + u^{k} \\right\\|_{2}^{2} \\right] = 0,\n$$\nwhich simplifies to\n$$\n2 \\lambda L^{\\top} L x + \\mu A^{\\top} \\left( A x - y - z^{k} + u^{k} \\right) = 0.\n$$\nCollecting terms in $x$, we obtain the normal equations\n$$\n\\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right) x = \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right).\n$$\nAssuming the matrix $\\mu A^{\\top} A + 2 \\lambda L^{\\top} L$ is invertible, the $x$-update is given in closed form by\n$$\nx^{k+1} = \\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right)^{-1} \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right).\n$$\n\nFor the $z$-update, we minimize with respect to $z$ while keeping $(x^{k+1}, u^{k})$ fixed:\n$$\nz^{k+1} \\in \\arg\\min_{z} \\left[ \\sum_{i=1}^{m} \\rho(z_{i}) + \\frac{\\mu}{2} \\left\\|A x^{k+1} - y - z + u^{k} \\right\\|_{2}^{2} \\right].\n$$\nDenote $v^{k} = A x^{k+1} - y + u^{k} \\in \\mathbb{R}^{m}$. The problem decouples across coordinates because $g(z) = \\sum_{i=1}^{m} \\rho(z_{i})$ is separable and the quadratic penalty is separable. By the definition of the proximal operator of a proper, convex, lower semicontinuous function $h:\\mathbb{R}^{m} \\to \\mathbb{R}$,\n$$\n\\operatorname{prox}_{\\tau h}(v) := \\arg\\min_{z} \\left\\{ h(z) + \\frac{1}{2 \\tau} \\|z - v\\|_{2}^{2} \\right\\},\n$$\nwe can rewrite the $z$-update as a proximal step with step size $\\tau = \\frac{1}{\\mu}$:\n$$\nz^{k+1} = \\operatorname{prox}_{\\frac{1}{\\mu} g}\\!\\left( v^{k} \\right) = \\operatorname{prox}_{\\frac{1}{\\mu} \\sum_{i=1}^{m} \\rho}\\!\\left( A x^{k+1} - y + u^{k} \\right).\n$$\nBecause $g$ is separable, this proximal operator acts elementwise and is equivalently expressed using the scalar proximal $\\operatorname{prox}_{\\frac{1}{\\mu}\\rho}$ applied coordinatewise:\n$$\nz^{k+1} = \\left[ \\operatorname{prox}_{\\frac{1}{\\mu}\\rho}\\!\\left( (A x^{k+1} - y + u^{k})_{i} \\right) \\right]_{i=1}^{m}.\n$$\n\nFinally, the scaled dual variable update for $u$ follows directly from the standard scaled ADMM definition for the equality constraint $A x - y - z = 0$:\n$$\nu^{k+1} = u^{k} + \\left( A x^{k+1} - y - z^{k+1} \\right).\n$$\n\nCollecting the three updates, we obtain the ADMM iteration written compactly in terms of the current iterates $(x^{k}, z^{k}, u^{k})$, the operators $A$, $L$, the data $y$, and the parameters $(\\lambda, \\mu)$:\n- $x$-update:\n$$\nx^{k+1} = \\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right)^{-1} \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right),\n$$\n- $z$-update:\n$$\nz^{k+1} = \\operatorname{prox}_{\\frac{1}{\\mu}\\rho}\\!\\left( A x^{k+1} - y + u^{k} \\right),\n$$\n- scaled dual update:\n$$\nu^{k+1} = u^{k} + A x^{k+1} - y - z^{k+1}.\n$$\nThese formulas complete the derivation of ADMM for the robust M-estimator data misfit with quadratic regularization under the splitting $z = A x - y$.", "answer": "$$\\boxed{\\begin{pmatrix}\n\\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right)^{-1} \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right)  \\operatorname{prox}_{\\frac{1}{\\mu}\\rho}\\!\\left( A x^{k+1} - y + u^{k} \\right)  u^{k} + A x^{k+1} - y - z^{k+1}\n\\end{pmatrix}}$$", "id": "3418091"}]}