{"hands_on_practices": [{"introduction": "The Orthogonal Matching Pursuit (OMP) algorithm builds a sparse solution iteratively. To truly grasp its mechanics, there is no substitute for performing the calculations by hand. This first practice exercise guides you through the critical first iteration of OMP, from calculating initial correlations with the measurement vector to selecting the best-matching atom and updating the residual through orthogonal projection.", "problem": "Consider the linear inverse problem $y = A x^{\\star} + \\eta$ with a known sensing matrix $A \\in \\mathbb{R}^{3 \\times 4}$ whose columns are unit-norm atoms and a measurement $y \\in \\mathbb{R}^{3}$. Assume the absence of measurement noise so that $\\eta = 0$. The greedy Orthogonal Matching Pursuit (OMP) algorithm constructs a sparse estimate of $x^{\\star}$ by iteratively selecting atoms with the largest absolute inner product with the current residual and then orthogonally projecting the measurement onto the span of the selected atoms. In the first iteration, the residual is equal to the measurement.\n\nYou are given the matrix\n$$\nA = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{3}} \\\\\n\\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{3}} \\\\\n0  0  1  \\frac{1}{\\sqrt{3}}\n\\end{pmatrix},\n$$\nand the measurement\n$$\ny = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{2}\n\\end{pmatrix}.\n$$\n\nStarting from the definitions of the Euclidean inner product and orthogonal projection in least squares, perform the first iteration of Orthogonal Matching Pursuit: compute all correlations between the residual and the atoms, select the atom with the largest absolute correlation, compute the corresponding least-squares coefficient for that atom, and update the residual. Report the squared Euclidean norm of the updated residual after this first iteration as a single closed-form analytic expression. Do not round; provide the exact value.", "solution": "The problem requires performing the first iteration of the Orthogonal Matching Pursuit (OMP) algorithm to find the squared Euclidean norm of the updated residual.\n\nThe Orthogonal Matching Pursuit algorithm is an iterative greedy procedure. In each step $k$, it performs three main operations:\n1.  **Identification:** Find the column (atom) of the matrix $A$ that is most correlated with the current residual $r_{k-1}$.\n2.  **Update:** Add the index of the selected atom to the support set $\\mathcal{I}_k$.\n3.  **Residual Calculation:** Compute the new residual $r_k$ as the component of the original measurement $y$ that is orthogonal to the subspace spanned by the atoms in the updated support set.\n\nLet the columns of the matrix $A$ be denoted by $a_j$ for $j \\in \\{1, 2, 3, 4\\}$. The matrix $A$ is given as:\n$$ A = \\begin{pmatrix} a_1  a_2  a_3  a_4 \\end{pmatrix} = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{3}} \\\\\n\\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{3}} \\\\\n0  0  1  \\frac{1}{\\sqrt{3}}\n\\end{pmatrix} $$\nThe problem states that the columns are unit-norm, which can be verified. For example, for $a_1$:\n$$ \\|a_1\\|_2^2 = \\left(\\frac{1}{\\sqrt{2}}\\right)^2 + \\left(\\frac{1}{\\sqrt{2}}\\right)^2 + 0^2 = \\frac{1}{2} + \\frac{1}{2} = 1 $$\nThe measurement vector is:\n$$ y = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{2}\n\\end{pmatrix} $$\nThe problem assumes no measurement noise, so $\\eta = 0$.\n\n**Iteration 1:**\nThe initial residual is the measurement itself, $r_0 = y$. The initial support set is empty, $\\mathcal{I}_0 = \\emptyset$.\n\n**Step 1: Identification**\nWe must find the atom $a_j$ that maximizes the absolute value of the inner product with the current residual $r_0$. This is equivalent to finding $j_1 = \\arg\\max_{j \\in \\{1,2,3,4\\}} |a_j^T r_0|$.\nLet's compute the correlations $c_j = a_j^T r_0$ for each atom $a_j$:\n\\begin{itemize}\n    \\item $c_1 = a_1^T r_0 = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{2} \\end{pmatrix} = \\left(\\frac{1}{\\sqrt{2}}\\right)\\left(\\frac{1}{\\sqrt{2}}\\right) + \\left(\\frac{1}{\\sqrt{2}}\\right)\\left(\\frac{1}{\\sqrt{2}}\\right) + (0)\\left(\\frac{1}{2}\\right) = \\frac{1}{2} + \\frac{1}{2} = 1$\n    \\item $c_2 = a_2^T r_0 = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}}  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{2} \\end{pmatrix} = \\left(\\frac{1}{\\sqrt{2}}\\right)\\left(\\frac{1}{\\sqrt{2}}\\right) + \\left(-\\frac{1}{\\sqrt{2}}\\right)\\left(\\frac{1}{\\sqrt{2}}\\right) + (0)\\left(\\frac{1}{2}\\right) = \\frac{1}{2} - \\frac{1}{2} = 0$\n    \\item $c_3 = a_3^T r_0 = \\begin{pmatrix} 0  0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{2} \\end{pmatrix} = (0)\\left(\\frac{1}{\\sqrt{2}}\\right) + (0)\\left(\\frac{1}{\\sqrt{2}}\\right) + (1)\\left(\\frac{1}{2}\\right) = \\frac{1}{2}$\n    \\item $c_4 = a_4^T r_0 = \\begin{pmatrix} \\frac{1}{\\sqrt{3}}  \\frac{1}{\\sqrt{3}}  \\frac{1}{\\sqrt{3}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{2} \\end{pmatrix} = \\left(\\frac{1}{\\sqrt{3}}\\right)\\left(\\frac{1}{\\sqrt{2}}\\right) + \\left(\\frac{1}{\\sqrt{3}}\\right)\\left(\\frac{1}{\\sqrt{2}}\\right) + \\left(\\frac{1}{\\sqrt{3}}\\right)\\left(\\frac{1}{2}\\right) = \\frac{2}{\\sqrt{6}} + \\frac{1}{2\\sqrt{3}} = \\frac{2\\sqrt{6}}{6} + \\frac{\\sqrt{3}}{6} = \\frac{2\\sqrt{6}+\\sqrt{3}}{6}$\n\\end{itemize}\nNow we compare the absolute values: $|c_1| = 1$, $|c_2| = 0$, $|c_3| = \\frac{1}{2} = 0.5$, and $|c_4| = \\frac{2\\sqrt{6}+\\sqrt{3}}{6}$. To compare $|c_1|$ and $|c_4|$, we assess if $\\frac{2\\sqrt{6}+\\sqrt{3}}{6} > 1$. This is equivalent to $2\\sqrt{6}+\\sqrt{3} > 6$, or $2\\sqrt{6} > 6 - \\sqrt{3}$. Since both sides are positive, we can square them: $(2\\sqrt{6})^2 = 24$ and $(6-\\sqrt{3})^2 = 36 - 12\\sqrt{3} + 3 = 39 - 12\\sqrt{3}$. The inequality becomes $24 > 39 - 12\\sqrt{3}$, or $12\\sqrt{3} > 15$, which simplifies to $4\\sqrt{3} > 5$. Squaring again gives $48 > 25$, which is true. Thus, $|c_4|$ is the largest correlation.\n\nThe first selected index is $j_1=4$.\n\n**Step 2: Update Support Set**\nThe support set is updated to $\\mathcal{I}_1 = \\{4\\}$.\n\n**Step 3: Residual Calculation**\nThe new residual, $r_1$, is calculated by projecting the measurement $y$ onto the subspace spanned by the selected atom $a_4$ and subtracting this projection from $y$. The projection of $y$ onto the span of $a_4$ is given by $p_1 = A_{\\mathcal{I}_1}(A_{\\mathcal{I}_1}^T A_{\\mathcal{I}_1})^{-1} A_{\\mathcal{I}_1}^T y$.\nHere, $A_{\\mathcal{I}_1} = a_4$. Since $a_4$ is a unit-norm vector, $A_{\\mathcal{I}_1}^T A_{\\mathcal{I}_1} = a_4^T a_4 = \\|a_4\\|_2^2 = 1$.\nThe projection simplifies to $p_1 = a_4(a_4^T y) = a_4 c_4$.\nThe new residual is $r_1 = y - p_1 = y - a_4(a_4^T y)$.\nWe are asked to find the squared Euclidean norm of this new residual, $\\|r_1\\|_2^2$.\nBy construction, $r_1$ is orthogonal to $p_1$. Therefore, by the Pythagorean theorem, $\\|y\\|_2^2 = \\|p_1\\|_2^2 + \\|r_1\\|_2^2$.\nThis gives a direct way to compute $\\|r_1\\|_2^2$:\n$$ \\|r_1\\|_2^2 = \\|y\\|_2^2 - \\|p_1\\|_2^2 $$\nLet's compute the components:\nFirst, $\\|y\\|_2^2$:\n$$ \\|y\\|_2^2 = \\left(\\frac{1}{\\sqrt{2}}\\right)^2 + \\left(\\frac{1}{\\sqrt{2}}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{2} + \\frac{1}{2} + \\frac{1}{4} = 1 + \\frac{1}{4} = \\frac{5}{4} $$\nNext, $\\|p_1\\|_2^2$. Since $p_1 = a_4 c_4$, its norm is $\\|p_1\\|_2 = \\|a_4 c_4\\|_2 = |c_4| \\|a_4\\|_2$. As $\\|a_4\\|_2 = 1$, we have $\\|p_1\\|_2^2 = c_4^2$.\n$$ c_4^2 = \\left(a_4^T y\\right)^2 = \\left(\\frac{2\\sqrt{6}+\\sqrt{3}}{6}\\right)^2 = \\frac{(2\\sqrt{6})^2 + 2(2\\sqrt{6})(\\sqrt{3}) + (\\sqrt{3})^2}{36} $$\n$$ = \\frac{24 + 4\\sqrt{18} + 3}{36} = \\frac{27 + 4(3\\sqrt{2})}{36} = \\frac{27 + 12\\sqrt{2}}{36} = \\frac{9 + 4\\sqrt{2}}{12} $$\nNow, we can compute the squared norm of the residual $r_1$:\n$$ \\|r_1\\|_2^2 = \\|y\\|_2^2 - c_4^2 = \\frac{5}{4} - \\frac{9 + 4\\sqrt{2}}{12} $$\nTo subtract, we find a common denominator:\n$$ \\|r_1\\|_2^2 = \\frac{3 \\cdot 5}{3 \\cdot 4} - \\frac{9 + 4\\sqrt{2}}{12} = \\frac{15}{12} - \\frac{9 + 4\\sqrt{2}}{12} = \\frac{15 - (9 + 4\\sqrt{2})}{12} $$\n$$ \\|r_1\\|_2^2 = \\frac{15 - 9 - 4\\sqrt{2}}{12} = \\frac{6 - 4\\sqrt{2}}{12} $$\nSimplifying the fraction by dividing the numerator and denominator by $2$:\n$$ \\|r_1\\|_2^2 = \\frac{3 - 2\\sqrt{2}}{6} $$\nThis is the squared Euclidean norm of the updated residual after the first iteration.", "answer": "$$ \\boxed{\\frac{3 - 2\\sqrt{2}}{6}} $$", "id": "3387244"}, {"introduction": "While the iterative process of OMP is straightforward, its success heavily depends on the properties of the dictionary matrix $A$. One crucial property is mutual coherence, which measures the maximum similarity between any two distinct atoms. This exercise will help you connect theory to practice by calculating the coherence of a given dictionary and using it to determine the theoretical limit on signal sparsity for which OMP is guaranteed to find the exact solution.", "problem": "Consider a noiseless linear inverse problem in data assimilation where measurements $y \\in \\mathbb{R}^{m}$ are related to a state vector $x_{0} \\in \\mathbb{R}^{n}$ by $y = A x_{0}$, with $x_{0}$ known to be $k$-sparse. Let the dictionary (design matrix) $A \\in \\mathbb{R}^{3 \\times 3}$ have columns $a_{1}$, $a_{2}$, and $a_{3}$ given by\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1  0  \\frac{1}{4} \\\\\n0  1  \\frac{1}{4} \\\\\n0  0  \\sqrt{\\frac{7}{8}}\n\\end{pmatrix}.\n$$\nDefine the mutual coherence $\\mu(A)$ of $A$ by\n$$\n\\mu(A) \\;=\\; \\max_{i \\neq j} \\frac{\\left| a_{i}^{\\top} a_{j} \\right|}{\\|a_{i}\\|_{2} \\, \\|a_{j}\\|_{2}}.\n$$\nOrthogonal Matching Pursuit (OMP) is used as the greedy sparse recovery method. Using only the mutual coherence-based exact recovery criterion for OMP in the noiseless case, determine the largest integer sparsity level $k$ for which exact recovery of any $k$-sparse $x_{0}$ is guaranteed. Your final answer must be a single integer with no units.", "solution": "The task is to find the largest integer sparsity level $k$ for which the Orthogonal Matching Pursuit (OMP) algorithm is guaranteed to exactly recover any $k$-sparse vector $x_{0} \\in \\mathbb{R}^{n}$ from the noiseless measurements $y = A x_{0}$. The guarantee is to be based on the mutual coherence $\\mu(A)$ of the dictionary matrix $A$.\n\nThe sufficient condition for OMP to exactly recover any $k$-sparse signal in the noiseless case is given by the inequality:\n$$\nk  \\frac{1}{2} \\left( 1 + \\frac{1}{\\mu(A)} \\right)\n$$\nwhere $\\mu(A)$ is the mutual coherence of the matrix $A$. The mutual coherence is defined as:\n$$\n\\mu(A) = \\max_{i \\neq j} \\frac{| a_{i}^{\\top} a_{j} |}{\\|a_{i}\\|_{2} \\, \\|a_{j}\\|_{2}}\n$$\nHere, $a_i$ and $a_j$ are distinct columns of the matrix $A$.\n\nFirst, we must calculate the mutual coherence $\\mu(A)$ for the given matrix:\n$$\nA =\n\\begin{pmatrix}\n1  0  \\frac{1}{4} \\\\\n0  1  \\frac{1}{4} \\\\\n0  0  \\sqrt{\\frac{7}{8}}\n\\end{pmatrix}\n$$\nThe columns of $A$ are:\n$$\na_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\na_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\na_{3} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\sqrt{\\frac{7}{8}} \\end{pmatrix}\n$$\nNext, we calculate the L2-norm of each column vector:\n$$\n\\|a_{1}\\|_{2} = \\sqrt{1^{2} + 0^{2} + 0^{2}} = \\sqrt{1} = 1\n$$\n$$\n\\|a_{2}\\|_{2} = \\sqrt{0^{2} + 1^{2} + 0^{2}} = \\sqrt{1} = 1\n$$\n$$\n\\|a_{3}\\|_{2} = \\sqrt{\\left(\\frac{1}{4}\\right)^{2} + \\left(\\frac{1}{4}\\right)^{2} + \\left(\\sqrt{\\frac{7}{8}}\\right)^{2}} = \\sqrt{\\frac{1}{16} + \\frac{1}{16} + \\frac{7}{8}} = \\sqrt{\\frac{2}{16} + \\frac{14}{16}} = \\sqrt{\\frac{16}{16}} = \\sqrt{1} = 1\n$$\nSince all columns are unit vectors (i.e., $\\|a_i\\|_2 = 1$ for $i=1, 2, 3$), the denominator in the mutual coherence formula is always $1$. The formula simplifies to:\n$$\n\\mu(A) = \\max_{i \\neq j} | a_{i}^{\\top} a_{j} |\n$$\nNow, we compute the absolute values of the dot products between distinct columns:\nFor $i=1, j=2$:\n$$\n|a_{1}^{\\top} a_{2}| = \\left| \\begin{pmatrix} 1  0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right| = |(1)(0) + (0)(1) + (0)(0)| = |0| = 0\n$$\nFor $i=1, j=3$:\n$$\n|a_{1}^{\\top} a_{3}| = \\left| \\begin{pmatrix} 1  0  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\sqrt{\\frac{7}{8}} \\end{pmatrix} \\right| = \\left|(1)\\left(\\frac{1}{4}\\right) + (0)\\left(\\frac{1}{4}\\right) + (0)\\left(\\sqrt{\\frac{7}{8}}\\right)\\right| = \\left|\\frac{1}{4}\\right| = \\frac{1}{4}\n$$\nFor $i=2, j=3$:\n$$\n|a_{2}^{\\top} a_{3}| = \\left| \\begin{pmatrix} 0  1  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\sqrt{\\frac{7}{8}} \\end{pmatrix} \\right| = \\left|(0)\\left(\\frac{1}{4}\\right) + (1)\\left(\\frac{1}{4}\\right) + (0)\\left(\\sqrt{\\frac{7}{8}}\\right)\\right| = \\left|\\frac{1}{4}\\right| = \\frac{1}{4}\n$$\nThe mutual coherence is the maximum of these values:\n$$\n\\mu(A) = \\max \\left\\{ 0, \\frac{1}{4}, \\frac{1}{4} \\right\\} = \\frac{1}{4}\n$$\nNow we substitute this value into the OMP recovery condition:\n$$\nk  \\frac{1}{2} \\left( 1 + \\frac{1}{\\mu(A)} \\right)\n$$\n$$\nk  \\frac{1}{2} \\left( 1 + \\frac{1}{1/4} \\right)\n$$\n$$\nk  \\frac{1}{2} (1 + 4)\n$$\n$$\nk  \\frac{5}{2}\n$$\n$$\nk  2.5\n$$\nThe problem asks for the largest integer sparsity level $k$ that satisfies this condition. The integers smaller than $2.5$ are $2, 1, 0, \\dots$. The largest of these is $2$.\nThus, the largest integer sparsity level $k$ for which exact recovery of any $k$-sparse vector is guaranteed by this criterion is $2$.", "answer": "$$\\boxed{2}$$", "id": "3387260"}, {"introduction": "Theoretical guarantees, like the one based on mutual coherence [@problem_id:3387260], often feature strict inequalities that define a 'safe' region of operation. This practice explores the razor's edge of this guarantee by examining a scenario where the signal sparsity is exactly at the boundary of the condition. You will demonstrate how this borderline case can lead to ambiguity in the atom selection process, providing a concrete example of why the strict inequality is necessary for the guarantee to hold.", "problem": "Consider a linear inverse problem with a sensing matrix (dictionary) $A \\in \\mathbb{R}^{3 \\times 4}$ whose columns are unit-norm atoms $a_{1}, a_{2}, a_{3}, a_{4} \\in \\mathbb{R}^{3}$ given explicitly by\n$$\na_{1} = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}1 \\\\ 1 \\\\ 1\\end{pmatrix}, \\quad\na_{2} = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}1 \\\\ -1 \\\\ -1\\end{pmatrix}, \\quad\na_{3} = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}-1 \\\\ 1 \\\\ -1\\end{pmatrix}, \\quad\na_{4} = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}-1 \\\\ -1 \\\\ 1\\end{pmatrix}.\n$$\nDefine the mutual coherence $\\mu(A)$ of the dictionary $A$ as\n$$\n\\mu(A) = \\max_{i \\neq j} \\left| \\langle a_{i}, a_{j} \\rangle \\right|.\n$$\nLet $k$ be the borderline value\n$$\nk = \\frac{1}{2}\\left(1 + \\frac{1}{\\mu(A)}\\right),\n$$\nand consider a $k$-sparse signal $x \\in \\mathbb{R}^{4}$ supported on the first two atoms, with nonzero entries $x_{1} = 1$ and $x_{2} = 1$ and $x_{3} = x_{4} = 0$. The observation is noiseless: $y = A x$. Orthogonal Matching Pursuit (OMP) is a greedy sparse recovery algorithm that at each iteration selects the atom with maximal absolute correlation with the current residual.\n\nStarting from first principles and core definitions (unit-norm atoms, mutual coherence, and the OMP selection rule), compute the absolute correlations $\\left| \\langle y, a_{j} \\rangle \\right|$ for $j = 1,2,3,4$, and demonstrate that at the borderline $k$ they are all equal, so that OMP can fail due to tie-breaking selecting a wrong atom. Your final answer must be the common value of these absolute correlations written as an exact fraction. No rounding is required.", "solution": "The goal is to compute the initial correlations for the Orthogonal Matching Pursuit (OMP) algorithm and show that they are all equal, leading to a potential failure of the algorithm at a specific sparsity level.\n\nFirst, we verify that the columns of the sensing matrix $A$, denoted by $a_{j}$, are unit-norm vectors as stated. The squared Euclidean norm of each atom is calculated.\nFor $a_{1}$:\n$$\n\\| a_{1} \\|_{2}^{2} = \\left\\langle a_{1}, a_{1} \\right\\rangle = \\left(\\frac{1}{\\sqrt{3}}\\right)^{2} (1^{2} + 1^{2} + 1^{2}) = \\frac{1}{3}(1+1+1) = 1.\n$$\nThe norms for $a_{2}$, $a_{3}$, and $a_{4}$ are computed similarly:\n$$\n\\| a_{2} \\|_{2}^{2} = \\frac{1}{3}(1^{2} + (-1)^{2} + (-1)^{2}) = 1. \\\\\n\\| a_{3} \\|_{2}^{2} = \\frac{1}{3}((-1)^{2} + 1^{2} + (-1)^{2}) = 1. \\\\\n\\| a_{4} \\|_{2}^{2} = \\frac{1}{3}((-1)^{2} + (-1)^{2} + 1^{2}) = 1.\n$$\nSince $\\|a_{j}\\|_{2}^{2} = 1$ for all $j \\in \\{1, 2, 3, 4\\}$, the atoms are indeed unit-norm.\n\nNext, we compute the mutual coherence $\\mu(A)$, which is defined as the maximum absolute value of the off-diagonal entries of the Gram matrix $A^{T}A$. This requires computing all pairwise inner products $\\langle a_{i}, a_{j} \\rangle$ for $i \\neq j$.\n$$\n\\langle a_{1}, a_{2} \\rangle = \\frac{1}{\\sqrt{3}}\\frac{1}{\\sqrt{3}} (1 \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot (-1)) = \\frac{1}{3}(1 - 1 - 1) = -\\frac{1}{3}.\n$$\n$$\n\\langle a_{1}, a_{3} \\rangle = \\frac{1}{3} (1 \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot (-1)) = \\frac{1}{3}(-1 + 1 - 1) = -\\frac{1}{3}.\n$$\n$$\n\\langle a_{1}, a_{4} \\rangle = \\frac{1}{3} (1 \\cdot (-1) + 1 \\cdot (-1) + 1 \\cdot 1) = \\frac{1}{3}(-1 - 1 + 1) = -\\frac{1}{3}.\n$$\n$$\n\\langle a_{2}, a_{3} \\rangle = \\frac{1}{3} (1 \\cdot (-1) + (-1) \\cdot 1 + (-1) \\cdot (-1)) = \\frac{1}{3}(-1 - 1 + 1) = -\\frac{1}{3}.\n$$\n$$\n\\langle a_{2}, a_{4} \\rangle = \\frac{1}{3} (1 \\cdot (-1) + (-1) \\cdot (-1) + (-1) \\cdot 1) = \\frac{1}{3}(-1 + 1 - 1) = -\\frac{1}{3}.\n$$\n$$\n\\langle a_{3}, a_{4} \\rangle = \\frac{1}{3} ((-1) \\cdot (-1) + 1 \\cdot (-1) + (-1) \\cdot 1) = \\frac{1}{3}(1 - 1 - 1) = -\\frac{1}{3}.\n$$\nThe absolute value of all these inner products is $|\\frac{-1}{3}| = \\frac{1}{3}$. Therefore, the mutual coherence is:\n$$\n\\mu(A) = \\max_{i \\neq j} |\\langle a_{i}, a_{j} \\rangle| = \\frac{1}{3}.\n$$\n\nNow, we compute the borderline sparsity value $k$ using the given formula:\n$$\nk = \\frac{1}{2}\\left(1 + \\frac{1}{\\mu(A)}\\right) = \\frac{1}{2}\\left(1 + \\frac{1}{1/3}\\right) = \\frac{1}{2}(1 + 3) = \\frac{4}{2} = 2.\n$$\nThe problem considers a signal $x \\in \\mathbb{R}^{4}$ which is $k$-sparse, i.e., $2$-sparse. The signal is specified to have nonzero entries $x_{1} = 1$ and $x_{2} = 1$, and $x_{3}=x_{4}=0$. So, the signal vector is $x = [1, 1, 0, 0]^{T}$. The support of this signal is $\\{1, 2\\}$, and its sparsity is indeed $\\|x\\|_{0} = 2 = k$.\n\nThe observation vector $y$ is obtained from the noiseless model $y = Ax$.\n$$\ny = A x = \\sum_{j=1}^{4} x_{j} a_{j} = x_{1} a_{1} + x_{2} a_{2} + x_{3} a_{3} + x_{4} a_{4} = 1 \\cdot a_{1} + 1 \\cdot a_{2} = a_{1} + a_{2}.\n$$\n$$\ny = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}1 \\\\ 1 \\\\ 1\\end{pmatrix} + \\frac{1}{\\sqrt{3}}\\begin{pmatrix}1 \\\\ -1 \\\\ -1\\end{pmatrix} = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}2 \\\\ 0 \\\\ 0\\end{pmatrix}.\n$$\nThe first step of the OMP algorithm involves identifying the atom $a_{j}$ that is most correlated with the observation $y$ (which is the initial residual $r_{0}$). This is done by computing $\\left| \\langle y, a_{j} \\rangle \\right|$ for all $j=1,2,3,4$ and finding the maximum.\n\nFor $j=1$:\n$$\n\\langle y, a_{1} \\rangle = \\langle a_{1} + a_{2}, a_{1} \\rangle = \\langle a_{1}, a_{1} \\rangle + \\langle a_{2}, a_{1} \\rangle = \\|a_{1}\\|_{2}^{2} + \\langle a_{1}, a_{2} \\rangle = 1 + \\left(-\\frac{1}{3}\\right) = \\frac{2}{3}.\n$$\nSo, $|\\langle y, a_{1} \\rangle| = \\frac{2}{3}$.\n\nFor $j=2$:\n$$\n\\langle y, a_{2} \\rangle = \\langle a_{1} + a_{2}, a_{2} \\rangle = \\langle a_{1}, a_{2} \\rangle + \\langle a_{2}, a_{2} \\rangle = \\langle a_{1}, a_{2} \\rangle + \\|a_{2}\\|_{2}^{2} = -\\frac{1}{3} + 1 = \\frac{2}{3}.\n$$\nSo, $|\\langle y, a_{2} \\rangle| = \\frac{2}{3}$.\n\nFor $j=3$:\n$$\n\\langle y, a_{3} \\rangle = \\langle a_{1} + a_{2}, a_{3} \\rangle = \\langle a_{1}, a_{3} \\rangle + \\langle a_{2}, a_{3} \\rangle = -\\frac{1}{3} + \\left(-\\frac{1}{3}\\right) = -\\frac{2}{3}.\n$$\nSo, $|\\langle y, a_{3} \\rangle| = \\left|-\\frac{2}{3}\\right| = \\frac{2}{3}$.\n\nFor $j=4$:\n$$\n\\langle y, a_{4} \\rangle = \\langle a_{1} + a_{2}, a_{4} \\rangle = \\langle a_{1}, a_{4} \\rangle + \\langle a_{2}, a_{4} \\rangle = -\\frac{1}{3} + \\left(-\\frac{1}{3}\\right) = -\\frac{2}{3}.\n$$\nSo, $|\\langle y, a_{4} \\rangle| = \\left|-\\frac{2}{3}\\right| = \\frac{2}{3}$.\n\nAll four absolute correlations are equal:\n$$\n|\\langle y, a_{1} \\rangle| = |\\langle y, a_{2} \\rangle| = |\\langle y, a_{3} \\rangle| = |\\langle y, a_{4} \\rangle| = \\frac{2}{3}.\n$$\nThis demonstrates that at the first step of OMP, there is a four-way tie. OMP must select one atom to add to its active set. The correct atoms to identify are $a_{1}$ and $a_{2}$. If the tie-breaking rule leads to the selection of either $a_{3}$ or $a_{4}$ in the first step, the algorithm will have selected an atom outside the true support of $x$, and will thus fail to correctly recover $x$. This situation exemplifies why the strict inequality, sparsity $ \\frac{1}{2}(1 + 1/\\mu)$, is a sufficient condition for the success of OMP. The problem occurs at the boundary of this condition.\n\nThe common value of the absolute correlations is $\\frac{2}{3}$.", "answer": "$$\n\\boxed{\\frac{2}{3}}\n$$", "id": "3387223"}]}