{"hands_on_practices": [{"introduction": "The theory of Iteratively Reweighted Least Squares (IRLS) becomes tangible when applied to a concrete example. This first exercise provides a foundational, step-by-step walkthrough of a single IRLS iteration. By manually calculating the residuals, weights, and updated solution for a small linear system contaminated by a significant outlier, you will gain direct insight into how IRLS achieves robustness by systematically reducing the influence of corrupt data points [@problem_id:3393330].", "problem": "Consider a linear data assimilation setting with a forward operator represented by a matrix $A \\in \\mathbb{R}^{3 \\times 2}$ and observations $b \\in \\mathbb{R}^{3}$, where the third observation is a large outlier relative to the first two. The goal is to robustly estimate the state $x \\in \\mathbb{R}^{2}$ in the presence of this outlier using one iteration of Iteratively Reweighted Least Squares (IRLS) based on the Huber loss. Let\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n1  1\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1 \\\\ -1 \\\\ 20\n\\end{pmatrix},\n\\qquad\nx^{(0)} \\;=\\; \\begin{pmatrix}\n0 \\\\ 0\n\\end{pmatrix}.\n$$\nThe Huber loss with threshold parameter $\\delta0$ is defined by the piecewise function\n$$\n\\rho_{\\delta}(r) \\;=\\;\n\\begin{cases}\n\\frac{1}{2} r^{2},  \\text{if } |r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2},  \\text{if } |r|  \\delta.\n\\end{cases}\n$$\nUse $\\delta = 2$. Starting from the definition of $\\rho_{\\delta}(r)$ above and the principle of robust M-estimation, perform one IRLS iteration as follows:\n1. Compute the residuals at $x^{(0)}$, denoted $r^{(0)} \\in \\mathbb{R}^{3}$, using the misfit $r = Ax - b$.\n2. Derive the corresponding IRLS weights $w_{i}$ from the Huber influence function associated with $\\rho_{\\delta}(r)$, and assemble the diagonal weight matrix $W = \\operatorname{diag}(w_{1}, w_{2}, w_{3})$.\n3. Form the weighted normal equations $(A^{\\top} W A) x^{(1)} = A^{\\top} W b$ and solve the resulting $2 \\times 2$ linear system to obtain $x^{(1)}$.\n\nReport only the value of the second component $x_{2}^{(1)}$ of the updated estimate. No rounding is required, and no physical units are involved.", "solution": "We begin from the robust M-estimation formulation in inverse problems, where the estimate $x \\in \\mathbb{R}^{2}$ minimizes the sum of robust penalties applied to data misfits. For residuals $r_{i} = a_{i}^{\\top} x - b_{i}$, the Huber loss with threshold parameter $\\delta  0$ is defined by\n$$\n\\rho_{\\delta}(r) \\;=\\;\n\\begin{cases}\n\\frac{1}{2} r^{2},  \\text{if } |r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2},  \\text{if } |r|  \\delta.\n\\end{cases}\n$$\nThe corresponding influence function (the derivative with respect to $r$) is\n$$\n\\psi_{\\delta}(r) \\;=\\; \\frac{\\mathrm{d}}{\\mathrm{d}r}\\rho_{\\delta}(r) \\;=\\;\n\\begin{cases}\nr,  \\text{if } |r| \\leq \\delta, \\\\\n\\delta \\,\\operatorname{sign}(r),  \\text{if } |r|  \\delta.\n\\end{cases}\n$$\nIn Iteratively Reweighted Least Squares (IRLS), weights are constructed as $w_{i} = \\frac{\\psi_{\\delta}(r_{i})}{r_{i}}$ for $r_{i} \\neq 0$; this yields\n$$\nw_{i} \\;=\\;\n\\begin{cases}\n1,  \\text{if } |r_{i}| \\leq \\delta, \\\\\n\\frac{\\delta}{|r_{i}|},  \\text{if } |r_{i}|  \\delta,\n\\end{cases}\n$$\nand $w_{i} = 1$ if $r_{i} = 0$ by continuity.\n\nStep 1: Compute residuals at $x^{(0)}$ using $r = A x - b$. With\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n1  1\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1 \\\\ -1 \\\\ 20\n\\end{pmatrix},\n\\qquad\nx^{(0)} \\;=\\; \\begin{pmatrix}\n0 \\\\ 0\n\\end{pmatrix},\n$$\nwe obtain\n$$\nr^{(0)} \\;=\\; A x^{(0)} - b \\;=\\; \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}1 \\\\ -1 \\\\ 20\\end{pmatrix} \\;=\\; \\begin{pmatrix} -1 \\\\ 1 \\\\ -20 \\end{pmatrix}.\n$$\n\nStep 2: Derive IRLS weights from the Huber influence function with $\\delta = 2$. The magnitudes of the residuals are $|r_{1}^{(0)}| = 1$, $|r_{2}^{(0)}| = 1$, and $|r_{3}^{(0)}| = 20$. Hence,\n- For $i = 1$: $|r_{1}^{(0)}| = 1 \\leq \\delta$ implies $w_{1} = 1$.\n- For $i = 2$: $|r_{2}^{(0)}| = 1 \\leq \\delta$ implies $w_{2} = 1$.\n- For $i = 3$: $|r_{3}^{(0)}| = 20  \\delta$ implies $w_{3} = \\frac{\\delta}{|r_{3}^{(0)}|} = \\frac{2}{20} = 0.1$.\n\nTherefore,\n$$\nW \\;=\\; \\operatorname{diag}(1,\\, 1,\\, 0.1).\n$$\n\nStep 3: Form and solve the weighted normal equations $(A^{\\top} W A) x^{(1)} = A^{\\top} W b$.\n\nFirst compute $A^{\\top} W A$. Denote the rows of $A$ by $a_{1}^{\\top} = (1, 0)$, $a_{2}^{\\top} = (0, 1)$, $a_{3}^{\\top} = (1, 1)$. Then\n$$\nA^{\\top} W A \\;=\\; \\sum_{i=1}^{3} w_{i} \\, a_{i} a_{i}^{\\top}.\n$$\nWe have\n$$\nw_{1} a_{1} a_{1}^{\\top} \\;=\\; 1 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\begin{pmatrix} 1  0 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix},\n$$\n$$\nw_{2} a_{2} a_{2}^{\\top} \\;=\\; 1 \\cdot \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\begin{pmatrix} 0  1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix},\n$$\n$$\nw_{3} a_{3} a_{3}^{\\top} \\;=\\; 0.1 \\cdot \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\begin{pmatrix} 1  1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 0.1  0.1 \\\\ 0.1  0.1 \\end{pmatrix}.\n$$\nSumming,\n$$\nA^{\\top} W A \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} 0.1  0.1 \\\\ 0.1  0.1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 1.1  0.1 \\\\ 0.1  1.1 \\end{pmatrix}.\n$$\n\nNext compute $A^{\\top} W b$. Note that $W b = \\begin{pmatrix} 1 \\cdot 1 \\\\ 1 \\cdot (-1) \\\\ 0.1 \\cdot 20 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}$. Then\n$$\nA^{\\top} W b \\;=\\; A^{\\top} \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1 \\cdot 1 + 0 \\cdot (-1) + 1 \\cdot 2 \\\\\n0 \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot 2\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\n\nWe must solve the $2 \\times 2$ linear system\n$$\n\\begin{pmatrix} 1.1  0.1 \\\\ 0.1  1.1 \\end{pmatrix}\n\\begin{pmatrix} x_{1}^{(1)} \\\\ x_{2}^{(1)} \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\nThe determinant of the coefficient matrix is\n$$\n\\det \\;=\\; 1.1 \\cdot 1.1 \\;-\\; 0.1 \\cdot 0.1 \\;=\\; 1.21 - 0.01 \\;=\\; 1.20.\n$$\nThe inverse is\n$$\n\\left(\\begin{pmatrix} 1.1  0.1 \\\\ 0.1  1.1 \\end{pmatrix}\\right)^{-1}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n1.1  -0.1 \\\\\n-0.1  1.1\n\\end{pmatrix}.\n$$\nThus\n$$\n\\begin{pmatrix} x_{1}^{(1)} \\\\ x_{2}^{(1)} \\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n1.1  -0.1 \\\\\n-0.1  1.1\n\\end{pmatrix}\n\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n1.1 \\cdot 3 + (-0.1) \\cdot 1 \\\\\n(-0.1) \\cdot 3 + 1.1 \\cdot 1\n\\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n3.3 - 0.1 \\\\\n-0.3 + 1.1\n\\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n3.2 \\\\\n0.8\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{3.2}{1.2} \\\\\n\\frac{0.8}{1.2}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{8}{3} \\\\\n\\frac{2}{3}\n\\end{pmatrix}.\n$$\n\nTherefore, the requested value is the second component $x_{2}^{(1)} = \\frac{2}{3}$.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "3393330"}, {"introduction": "While the iterative reweighting scheme is intuitive, its power stems from a deeper connection to a class of optimization strategies known as half-quadratic minimization. This practice demonstrates that IRLS is not merely a heuristic but a principled algorithm for minimizing complex objective functions by solving a sequence of simple quadratic problems. You will derive this equivalence and apply it to a geophysical inversion problem using the Geman-McClure penalty, showcasing the method's flexibility beyond the Huber loss [@problem_id:3605213].", "problem": "Consider a robust linearized inversion problem arising in magnetotellurics, where the real and imaginary parts of the impedance at multiple frequencies are concatenated into a single real-valued data vector. Let $m \\in \\mathbb{R}^{n}$ be a vector of log-resistivity perturbations (dimensionless), and let $d \\in \\mathbb{R}^{N}$ be the stacked data vector (units of ohms, denoted $\\Omega$). Assume a fixed linearized sensitivity matrix $G \\in \\mathbb{R}^{N \\times n}$ that maps model perturbations to predicted data. Let $L \\in \\mathbb{R}^{p \\times n}$ be a discrete first-difference operator enforcing smoothness of $m$. We seek to minimize the robust regularized objective\n$$\nJ(m) \\;=\\; \\sum_{i=1}^{N} \\rho\\!\\left(r_i(m)\\right) \\;+\\; \\frac{\\lambda}{2} \\,\\|L m\\|_2^2,\n$$\nwhere $r(m) = G m - d$, and $\\rho$ is a robust data misfit penalty chosen as the Geman–McClure function with scale $c  0$,\n$$\n\\rho(r) \\;=\\; \\frac{1}{2}\\,\\frac{c^2\\, r^2}{r^2 + c^2}.\n$$\nYou are to work in a setting where $G$ is regarded as the Jacobian from a local linearization of the magnetotelluric (MT) forward map about a background model; the inversion is therefore a single robust update step in a Gauss–Newton framework.\n\nTask A (derivation): Starting from the definitions of the least squares normal equations and the concept of convex conjugates, show that if $\\rho$ is differentiable, even, nonnegative, and increasing in $\\lvert r \\rvert$, then there exists a function $\\phi$ such that\n$$\n\\rho(r) \\;=\\; \\min_{w \\ge 0} \\; \\frac{1}{2}\\, w\\, r^2 \\;+\\; \\phi(w),\n$$\nand that the minimizer satisfies\n$$\nw^\\star(r) \\;=\\; \\begin{cases}\n\\dfrac{\\rho'(r)}{r},  r \\neq 0, \\\\[6pt]\n\\rho''(0),  r = 0.\n\\end{cases}\n$$\nConclude that Iteratively Reweighted Least Squares (IRLS), which alternates between updating $w_i \\leftarrow \\rho'\\!\\left(r_i\\right)/r_i$ and solving the weighted least squares problem, is equivalent to the half-quadratic minimization that alternates between minimizing in $w$ and minimizing in $m$ with the same quadratic surrogate, in the sense that both generate the same iterates for $m$ provided the same initialization. Specialize your expression for $w^\\star(r)$ to the Geman–McClure penalty above and simplify as a function of $r$ and $c$.\n\nTask B (algorithm design): Using only linear algebra and the definition of weighted least squares normal equations, derive the linear system to be solved in each iteration for $m$ given diagonal weights $W = \\mathrm{diag}(w_1,\\dots,w_N)$ and regularization parameter $\\lambda  0$. Explain how to handle the $r = 0$ case in the weight update in a numerically robust way.\n\nTask C (implementation and test suite): Implement two solvers for the stated problem, one based on Iteratively Reweighted Least Squares and one based on half-quadratic minimization, both using the weight update you derived for the Geman–McClure penalty. Both solvers must:\n- initialize with $m^{(0)} = 0$,\n- iterate weight updates and weighted normal equation solves until either the relative change in $\\|m\\|_2$ is less than $10^{-8}$ or a maximum of $200$ iterations is reached,\n- use a small ridge term $\\epsilon I$ with $\\epsilon = 10^{-8}$ added to the normal equations to ensure numerical stability,\n- define the objective as $J(m) = \\sum_i \\rho(r_i(m)) + \\frac{\\lambda}{2}\\|L m\\|_2^2$ and monitor convergence.\n\nFor magnetotelluric realism, interpret $N = 2 N_f$ as the number of real-valued data obtained by stacking $N_f$ frequencies’ real and imaginary impedance components. For the test suite below, you will generate the matrices and vectors deterministically using pseudorandom numbers with specified seeds. Resistivity is represented in log perturbations so $m$ is dimensionless; the data $d$ are in $\\Omega$, but your final reported metrics will be dimensionless differences so no unit conversion is needed.\n\nConstruct the following four test cases, each yielding one scalar result that quantifies the equivalence of the two methods:\n\n- Test $1$ (happy path, mild outliers): Take $n = 5$, $N = 20$ (so $N_f = 10$). Generate $G$ by sampling independent standard normal entries with seed $13$, and scale its rows so that the average row $2$-norm equals $1$. Let $L$ be the first-difference operator of size $(n-1) \\times n$. Define a “true” model $m_{\\mathrm{true}}$ by sampling independent normal entries with mean $0$ and standard deviation $0.2$ using seed $17$. Set $d_{\\mathrm{clean}} = G m_{\\mathrm{true}}$. Add independent Gaussian noise with standard deviation $0.05$ using seed $19$, and then add two outliers of amplitude $+5$ at indices $3$ and $12$ (zero-based indexing) to form $d$. Use $c = 1$ and $\\lambda = 10$. Run both solvers and report the maximum absolute difference between their final $m$ vectors.\n\n- Test $2$ (boundary case, exact data): Use the same $G$ and $L$ as in Test $1$, set $d = G m_{\\mathrm{true}}$ with $m_{\\mathrm{true}} = 0$, $c = 1$, and $\\lambda = 1$. Run both solvers and report the maximum absolute difference between their final $m$ vectors.\n\n- Test $3$ (strong outliers): Use the same $G$ and $L$ as in Test $1$, construct $d$ as in Test $1$ but add three additional large outliers of amplitude $+20$ at indices $1$, $7$, and $18$ (zero-based indexing). Use $c = 1$ and $\\lambda = 5$. Run both solvers and report the maximum absolute difference between their final $m$ vectors.\n\n- Test $4$ (small scale parameter): Repeat Test $1$ but with $c = 0.1$ and $\\lambda = 10$. Run both solvers and report the maximum absolute difference between their final $m$ vectors.\n\nAngle units do not apply. No other physical units are to be reported in the outputs. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Tests $1$ through $4$, for example, $[r_1,r_2,r_3,r_4]$, where each $r_k$ is a floating-point number equal to the maximum absolute difference between the Iteratively Reweighted Least Squares and half-quadratic final $m$ vectors in Test $k$.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and provides a complete and consistent setup for a standard problem in computational geophysics and numerical optimization.\n\n### Task A: Derivation of Equivalence and Weight Function\n\nThis task requires demonstrating the equivalence between Iteratively Reweighted Least Squares (IRLS) and half-quadratic minimization for a class of robust objective functions, and deriving the specific weighting function for the Geman–McClure penalty.\n\nLet $\\rho(r)$ be a function that is differentiable, even, non-negative, and increasing for non-negative arguments. Let us define a new function $g(x) = \\rho(\\sqrt{2x})$ for $x \\ge 0$. The properties of $\\rho$ ensure that $g$ is non-negative and monotonically increasing. For many common robust estimators, including Geman-McClure, $g(x)$ is also a concave function.\n\nA concave function $g(x)$ can be represented as the infimum of a family of affine functions (its supporting hyperplanes):\n$$\ng(x) = \\min_{w \\ge 0} \\{wx + \\phi(w)\\}\n$$\nfor some function $\\phi(w)$. The term $wx + \\phi(w)$ is the half-quadratic surrogate for $g(x)$. Substituting $x = r^2/2$, where $r$ is the residual, gives the half-quadratic representation for $\\rho(r)$:\n$$\n\\rho(r) = g\\left(\\frac{r^2}{2}\\right) = \\min_{w \\ge 0} \\left\\{\\frac{1}{2} w r^2 + \\phi(w)\\right\\}\n$$\nThis is the desired form. To find the optimal weight $w^\\star$ that achieves this minimum for a given $r$ (or $x = r^2/2$), we can differentiate the expression inside the minimum with respect to $w$ and set it to zero, but it is more direct to use properties of concave functions. At the point of tangency, the slope of the affine function $wx + \\phi(w)$ must match the slope of $g(x)$. Thus, the minimizer $w^\\star$ must satisfy:\n$$\nw^\\star(x) = g'(x)\n$$\nWe can relate $g'(x)$ back to the derivatives of $\\rho(r)$. Using the chain rule on $g(x) = \\rho(\\sqrt{2x})$ with $r = \\sqrt{2x}$:\n$$\ng'(x) = \\frac{d}{dx} \\rho(\\sqrt{2x}) = \\rho'(\\sqrt{2x}) \\cdot \\frac{d}{dx}\\sqrt{2x} = \\rho'(\\sqrt{2x}) \\cdot \\frac{1}{2} (2x)^{-1/2} \\cdot 2 = \\frac{\\rho'(\\sqrt{2x})}{\\sqrt{2x}}\n$$\nSubstituting $r = \\sqrt{2x}$, we get for $r \\neq 0$:\n$$\nw^\\star(r) = \\frac{\\rho'(r)}{r}\n$$\nFor the case $r=0$, which corresponds to $x=0$, we have $w^\\star(0) = g'(0) = \\lim_{x \\to 0^+} g'(x) = \\lim_{r \\to 0} \\frac{\\rho'(r)}{r}$. Since $\\rho(r)$ is an even function, its derivative $\\rho'(r)$ is an odd function, meaning $\\rho'(0)=0$. The limit is an indeterminate form $0/0$. Applying L'Hôpital's rule:\n$$\n\\lim_{r \\to 0} \\frac{\\rho'(r)}{r} = \\lim_{r \\to 0} \\frac{\\rho''(r)}{1} = \\rho''(0)\n$$\nThus, the complete expression for the optimal auxiliary weight variable is:\n$$\nw^\\star(r) = \\begin{cases}\n\\dfrac{\\rho'(r)}{r},  r \\neq 0, \\\\\n\\rho''(0),  r = 0.\n\\end{cases}\n$$\n\nNow, we can establish the equivalence between IRLS and half-quadratic minimization. The original objective function is:\n$$\nJ(m) = \\sum_{i=1}^{N} \\rho(r_i(m)) + \\frac{\\lambda}{2} \\|L m\\|_2^2\n$$\nUsing the half-quadratic representation for each $\\rho(r_i)$:\n$$\nJ(m) = \\sum_{i=1}^{N} \\min_{w_i \\ge 0} \\left\\{\\frac{1}{2} w_i r_i(m)^2 + \\phi(w_i)\\right\\} + \\frac{\\lambda}{2} \\|L m\\|_2^2\n$$\nBy swapping the sum and min, we can equivalently minimize a joint objective function over both $m$ and the auxiliary weights $w = (w_1, \\dots, w_N)$:\n$$\nJ_{HQ}(m, w) = \\sum_{i=1}^{N} \\left(\\frac{1}{2} w_i r_i(m)^2 + \\phi(w_i)\\right) + \\frac{\\lambda}{2} \\|L m\\|_2^2\n$$\nThe half-quadratic minimization algorithm alternates between minimizing $J_{HQ}$ with respect to $w$ (holding $m$ fixed) and with respect to $m$ (holding $w$ fixed).\n1.  **Minimize w.r.t. $w$ (for fixed $m$):** At iteration $k$, given $m^{(k)}$, we compute residuals $r^{(k)} = G m^{(k)} - d$. The minimization of $J_{HQ}(m^{(k)}, w)$ decouples for each $w_i$. The minimum for each $i$ is achieved at $w_i^{(k+1)} = w^\\star(r_i^{(k)}) = \\rho'(r_i^{(k)})/r_i^{(k)}$.\n2.  **Minimize w.r.t. $m$ (for fixed $w$):** Given $w^{(k+1)}$, we minimize $J_{HQ}(m, w^{(k+1)})$ with respect to $m$. This is equivalent to minimizing the weighted least squares objective: $$m^{(k+1)} = \\arg\\min_m \\sum_{i=1}^{N} \\frac{1}{2} w_i^{(k+1)} r_i(m)^2 + \\frac{\\lambda}{2} \\|L m\\|_2^2$$\n\nThe IRLS algorithm, by definition, is a procedure that, at iteration $k$, computes residuals $r_i^{(k)} = r_i(m^{(k)})$, defines weights as $w_i^{(k+1)} = \\rho'(r_i^{(k)})/r_i^{(k)}$, and then finds the next iterate $m^{(k+1)}$ by solving the same weighted least squares problem. Since the update rules for both $w$ and $m$ are identical, and assuming the same initialization $m^{(0)}$, IRLS and half-quadratic minimization generate the exact same sequence of iterates $\\{m^{(k)}\\}$. They are algorithmically equivalent.\n\nFinally, we specialize $w^\\star(r)$ for the Geman–McClure penalty:\n$$\n\\rho(r) = \\frac{1}{2}\\,\\frac{c^2\\, r^2}{r^2 + c^2}\n$$\nIts first derivative is:\n$$\n\\rho'(r) = \\frac{d}{dr} \\left( \\frac{1}{2} c^2 \\frac{r^2}{r^2+c^2} \\right) = \\frac{c^2}{2} \\left( \\frac{2r(r^2+c^2) - r^2(2r)}{(r^2+c^2)^2} \\right) = \\frac{c^2}{2} \\frac{2rc^2}{(r^2+c^2)^2} = \\frac{c^4 r}{(r^2+c^2)^2}\n$$\nFor $r \\neq 0$, the weight is:\n$$\nw^\\star(r) = \\frac{\\rho'(r)}{r} = \\frac{c^4}{(r^2+c^2)^2}\n$$\nTo find the weight at $r=0$, we can evaluate $\\rho''(0)$. The second derivative is:\n$$\n\\rho''(r) = \\frac{d}{dr} \\left( \\frac{c^4 r}{(r^2+c^2)^2} \\right) = c^4 \\frac{(r^2+c^2)^2 - r \\cdot 2(r^2+c^2)(2r)}{(r^2+c^2)^4} = c^4 \\frac{(r^2+c^2) - 4r^2}{(r^2+c^2)^3} = \\frac{c^4(c^2-3r^2)}{(r^2+c^2)^3}\n$$\nEvaluating at $r=0$:\n$$\nw^\\star(0) = \\rho''(0) = \\frac{c^4(c^2)}{(c^2)^3} = \\frac{c^6}{c^6} = 1\n$$\nNotably, if we take the limit of the expression for $w^\\star(r)$ as $r \\to 0$, we get $\\lim_{r\\to 0} \\frac{c^4}{(r^2+c^2)^2} = \\frac{c^4}{(c^2)^2} = 1$. Since the expression is continuous and well-defined at $r=0$, for the Geman-McClure penalty, the weight can be computed for all $r$ using the single, simple formula:\n$$\nw(r) = \\frac{c^4}{(r^2+c^2)^2}\n$$\n\n### Task B: Algorithm Design\n\nAt each iteration of the IRLS/half-quadratic algorithm, we must solve a weighted regularized least squares problem for the model update $m$. The objective function to be minimized with respect to $m$, for a fixed set of weights $w_i$, is:\n$$\nF(m) = \\sum_{i=1}^{N} \\frac{1}{2} w_i \\left( (Gm)_i - d_i \\right)^2_2 + \\frac{\\lambda}{2} \\|L m\\|_2^2\n$$\nLet $W$ be the diagonal matrix with entries $w_i$ on the diagonal, $W = \\mathrm{diag}(w_1, \\dots, w_N)$. We can write $F(m)$ in matrix-vector notation:\n$$\nF(m) = \\frac{1}{2} (Gm - d)^T W (Gm-d) + \\frac{\\lambda}{2} m^T L^T L m\n$$\nThis is a quadratic function of $m$. To find the minimum, we compute its gradient with respect to $m$ and set it to zero.\n$$\n\\nabla_m F(m) = G^T W (Gm - d) + \\lambda L^T L m\n$$\nSetting $\\nabla_m F(m) = 0$:\n$$\nG^T W G m - G^T W d + \\lambda L^T L m = 0\n$$\nRearranging the terms to form a linear system of the form $Ax=b$:\n$$\n\\left( G^T W G + \\lambda L^T L \\right) m = G^T W d\n$$\nThis is the system of normal equations. The problem specifies adding a small ridge term $\\epsilon I$ (where $I$ is the identity matrix of size $n \\times n$ and $\\epsilon=10^{-8}$) to the system matrix to guarantee numerical stability, particularly if the problem is ill-conditioned. The final linear system to be solved for $m$ in each iteration is:\n$$\n\\left( G^T W G + \\lambda L^T L + \\epsilon I \\right) m = G^T W d\n$$\nRegarding the handling of the $r=0$ case for the weight update $w_i \\leftarrow \\rho'(r_i)/r_i$: for a general penalty function $\\rho$, this expression is numerically unstable for $r_i \\approx 0$. A robust implementation would check if $|r_i|$ is below a small machine-epsilon-like tolerance. If it is, the weight should be set to its limiting value, $w_i \\leftarrow \\rho''(0)$. However, as derived in Task A, for the Geman–McClure penalty, the simplified expression $w(r) = c^4 / (r^2+c^2)^2$ is numerically stable and valid for all $r$, including $r=0$. Therefore, no special conditional logic is required; this single formula can be used to compute all weights.\n\n### Task C: Implementation and Test Suite\nThe implementation will consist of two identical solver functions, `solve_irls` and `solve_hq`, reflecting the two conceptual frameworks, which will be applied to four test cases. The core of each solver will be an iterative loop that updates weights and solves the linear system derived in Task B. The maximum absolute difference between the final model vectors from the two solvers will be computed for each test case. As the algorithms are identical, this difference is expected to be $0.0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are used.\n\ndef _generate_test_data(n, N, seed_G, seed_m_true, seed_noise, std_noise, outliers, m_true_is_zero):\n    \"\"\"Generates a single test case dataset.\"\"\"\n    # Construct the first-difference operator L\n    L = np.zeros((n - 1, n))\n    for i in range(n - 1):\n        L[i, i] = -1.0\n        L[i, i + 1] = 1.0\n\n    # Generate G matrix\n    rng_G = np.random.default_rng(seed=seed_G)\n    G = rng_G.standard_normal((N, n))\n    \n    # Scale G rows so that the average row 2-norm is 1\n    row_norms = np.linalg.norm(G, axis=1)\n    scale_factor = 1.0 / np.mean(row_norms)\n    G = G * scale_factor\n\n    # Generate true model m_true\n    if m_true_is_zero:\n        m_true = np.zeros(n)\n    else:\n        rng_m_true = np.random.default_rng(seed=seed_m_true)\n        m_true = rng_m_true.normal(loc=0.0, scale=0.2, size=n)\n\n    # Generate data vector d\n    d_clean = G @ m_true\n    \n    if std_noise  0:\n        rng_noise = np.random.default_rng(seed=seed_noise)\n        noise = rng_noise.normal(loc=0.0, scale=std_noise, size=N)\n        d = d_clean + noise\n    else:\n        d = d_clean\n\n    for index, amplitude in outliers:\n        d[index] += amplitude\n        \n    return G, L, d\n\ndef _solver_core(G, L, d, c, lambda_reg, epsilon, m_init, max_iter, tol):\n    \"\"\"The core iterative solver logic.\"\"\"\n    n = G.shape[1]\n    m = m_init.copy()\n    m_prev_norm = 0.0\n    \n    # Store L.T @ L as it's constant\n    LtL = L.T @ L\n\n    for _ in range(max_iter):\n        # 1. Calculate residuals\n        r = G @ m - d\n        \n        # 2. Update weights (for Geman-McClure)\n        # w_i = c^4 / (r_i^2 + c^2)^2\n        w = c**4 / (r**2 + c**2)**2\n        W = np.diag(w)\n        \n        # 3. Form and solve the linear system\n        # (G.T @ W @ G + lambda * L.T @ L + epsilon * I) m = G.T @ W @ d\n        A = G.T @ W @ G + lambda_reg * LtL + epsilon * np.identity(n)\n        b = G.T @ W @ d\n        \n        m_new = np.linalg.solve(A, b)\n        m = m_new\n\n        # 4. Check for convergence\n        m_norm = np.linalg.norm(m)\n        if m_prev_norm  0:\n            rel_change = np.abs(m_norm - m_prev_norm) / m_prev_norm\n            if rel_change  tol:\n                break\n        m_prev_norm = m_norm\n        \n    return m\n\ndef solve_irls(G, L, d, c, lambda_reg, epsilon, m_init, max_iter, tol):\n    \"\"\"Solver based on the Iteratively Reweighted Least Squares perspective.\"\"\"\n    return _solver_core(G, L, d, c, lambda_reg, epsilon, m_init, max_iter, tol)\n\ndef solve_hq(G, L, d, c, lambda_reg, epsilon, m_init, max_iter, tol):\n    \"\"\"Solver based on the Half-Quadratic Minimization perspective.\"\"\"\n    return _solver_core(G, L, d, c, lambda_reg, epsilon, m_init, max_iter, tol)\n\ndef solve():\n    \"\"\"Main function to run test suite and print results.\"\"\"\n    # Common parameters\n    n = 5\n    N = 20\n    max_iter = 200\n    tol = 1e-8\n    epsilon = 1e-8\n    m_init = np.zeros(n)\n\n    # Define the test cases from the problem statement.\n    test_cases_params = [\n        # Test 1 (happy path, mild outliers)\n        {'seed_G': 13, 'seed_m_true': 17, 'seed_noise': 19, 'std_noise': 0.05, \n         'outliers': [(3, 5.0), (12, 5.0)], 'm_true_is_zero': False, \n         'c': 1.0, 'lambda_reg': 10.0},\n        \n        # Test 2 (boundary case, exact data)\n        {'seed_G': 13, 'seed_m_true': 17, 'seed_noise': 19, 'std_noise': 0.0,\n         'outliers': [], 'm_true_is_zero': True, \n         'c': 1.0, 'lambda_reg': 1.0},\n        \n        # Test 3 (strong outliers)\n        {'seed_G': 13, 'seed_m_true': 17, 'seed_noise': 19, 'std_noise': 0.05,\n         'outliers': [(3, 5.0), (12, 5.0), (1, 20.0), (7, 20.0), (18, 20.0)], \n         'm_true_is_zero': False, 'c': 1.0, 'lambda_reg': 5.0},\n        \n        # Test 4 (small scale parameter)\n        {'seed_G': 13, 'seed_m_true': 17, 'seed_noise': 19, 'std_noise': 0.05,\n         'outliers': [(3, 5.0), (12, 5.0)], 'm_true_is_zero': False, \n         'c': 0.1, 'lambda_reg': 10.0}\n    ]\n\n    results = []\n    for params in test_cases_params:\n        # Generate data for the current test case\n        G, L, d = _generate_test_data(n=n, N=N, seed_G=params['seed_G'], \n                                    seed_m_true=params['seed_m_true'],\n                                    seed_noise=params['seed_noise'], \n                                    std_noise=params['std_noise'],\n                                    outliers=params['outliers'], \n                                    m_true_is_zero=params['m_true_is_zero'])\n        \n        c = params['c']\n        lambda_reg = params['lambda_reg']\n        \n        # Run both solvers\n        m_irls = solve_irls(G, L, d, c, lambda_reg, epsilon, m_init, max_iter, tol)\n        m_hq = solve_hq(G, L, d, c, lambda_reg, epsilon, m_init, max_iter, tol)\n        \n        # Calculate and store the maximum absolute difference\n        max_abs_diff = np.max(np.abs(m_irls - m_hq))\n        results.append(max_abs_diff)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3605213"}, {"introduction": "Beyond robust regression, IRLS is a key algorithm for promoting sparsity in inverse problems, often by employing nonconvex regularizers like the $L_p$ norm with $p \\lt 1$. This advanced practice explores the critical challenges of nonconvexity, where standard IRLS can converge to suboptimal local minima instead of the desired sparse solution. By analyzing a stylized example, you will diagnose this failure and devise a continuation strategy—a powerful technique to guide the optimization towards the global minimum in a complex energy landscape [@problem_id:3605298].", "problem": "Consider a stylized amplitude-fitting inverse problem drawn from seismic amplitude inversion, where the forward operator is the identity mapping. A single unknown scalar reflectivity amplitude $m \\in \\mathbb{R}$ is to be estimated from data $a \\in \\mathbb{R}$ by minimizing a nonconvex sparsity-promoting objective. The objective function is\n$$\nJ(m) = \\frac{1}{2}\\,|m - a|^{2} + \\lambda\\,|m|^{p},\n$$\nwith $0p1$, data amplitude $a0$, and regularization parameter $\\lambda0$. The nonconvexity reflects a preference for sparse reflectivity typical in computational geophysics. Iteratively Reweighted Least Squares (IRLS) constructs a quadratic surrogate of $|m|^{p}$ that is reweighted at each iteration to majorize the nonconvex term, often stabilized by a small differentiability parameter $\\epsilon0$.\n\nTasks:\n1. Starting from the definition of $J(m)$ and the properties of the function $|m|^{p}$ for $0p1$, use calculus to determine the behavior of $J(m)$ in a neighborhood of $m=0$. Based on first-order analysis from the left and right of $m=0$, argue whether $m=0$ is a local extremum and classify it.\n2. Derive, from the concavity of $x \\mapsto (x+\\epsilon)^{\\frac{p}{2}}$ in $x=m^{2}$ for $0p1$ and $\\epsilon0$, a quadratic majorizer of $|m|^{p}$ at a current iterate $m^{(k)}$. Show that a valid surrogate of the form\n$$\n|m|^{p} \\;\\le\\; w\\!\\left(m^{(k)}\\right)\\,m^{2} + c\\!\\left(m^{(k)}\\right)\n$$\ncan be constructed, and obtain the corresponding weight $w(m^{(k)})$ by differentiating the concave function with respect to $m^{2}$ at $m^{(k)}$.\n3. Using the surrogate in Task 2, write down and solve the scalar quadratic subproblem that defines the IRLS update $m^{(k+1)}$ in closed form, as a function of $a$, $\\lambda$, $p$, $\\epsilon$, and $m^{(k)}$.\n4. Specialize to the parameter choice $a=1$, $\\lambda=0.1$, $p=\\frac{1}{2}$, $\\epsilon=10^{-8}$, initialized at $m^{(0)}=0$. Analyze the resulting fixed-point equation and show that IRLS admits a stable fixed point near $m=0$. Compute the closed-form value of the IRLS fixed point under the assumption that $m^{(k)}$ remains sufficiently small that the stabilizer $\\epsilon$ dominates the weight formula.\n5. For the same parameters, determine (numerically or by qualitative monotonicity arguments) whether there exists a positive stationary point $m^{\\star}0$ of $J(m)$ solving the first-order necessary condition for optimality. Compare $J(m^{\\star})$ with $J(0)$ and with $J$ at the IRLS fixed point found in Task 4 to confirm that the IRLS fixed point is suboptimal.\n6. Propose a scientifically motivated continuation strategy over the nonconvexity (for example, on $p$ and/or $\\lambda$) that reduces the risk of converging to the suboptimal local minimum near $m=0$ in this and related geophysical inverse problems. Clearly state the scheduling principle and stopping criterion you would use.\n\nExpress the final numerical answer for Task 4 as a pure number, rounded to four significant figures, using scientific notation.", "solution": "The provided problem is a well-posed and scientifically grounded exercise in nonconvex optimization, pertinent to computational geophysics. It requires the analysis of an objective function with an $L_p$ sparsity-promoting regularizer for $0p1$, the derivation and analysis of an Iteratively Reweighted Least Squares (IRLS) algorithm, and a discussion of strategies to mitigate convergence to suboptimal local minima. The problem statement is complete, consistent, and relies on established mathematical principles. Therefore, it is deemed valid.\n\nThe objective function to be minimized is:\n$$\nJ(m) = \\frac{1}{2}(m - a)^{2} + \\lambda|m|^{p}\n$$\nwith given parameters $m, a \\in \\mathbb{R}$, $0  p  1$, $a  0$, and $\\lambda  0$.\n\n### Task 1: Behavior of $J(m)$ near $m=0$\n\nTo analyze the behavior of $J(m)$ near $m=0$, we examine its first derivative. The function $|m|^p$ is not differentiable at $m=0$ for $0p1$, so we must consider one-sided derivatives.\n\nFor $m  0$, the objective function is $J(m) = \\frac{1}{2}(m-a)^2 + \\lambda m^p$. Its derivative is:\n$$\nJ'(m) = (m-a) + \\lambda p m^{p-1}\n$$\nThe right-hand derivative at $m=0$ is the limit of $J'(m)$ as $m \\to 0^{+}$:\n$$\nJ'_{+}(0) = \\lim_{m \\to 0^{+}} \\left( m-a + \\lambda p m^{p-1} \\right)\n$$\nSince $0  p  1$, the exponent $p-1$ is negative. Given that $\\lambda  0$ and $p  0$, the term $\\lambda p m^{p-1}$ approaches $+\\infty$ as $m \\to 0^{+}$. Thus,\n$$\nJ'_{+}(0) = -a + \\lim_{m \\to 0^{+}} (\\lambda p m^{p-1}) = +\\infty\n$$\n\nFor $m  0$, the objective function is $J(m) = \\frac{1}{2}(m-a)^2 + \\lambda (-m)^p$. Its derivative is:\n$$\nJ'(m) = (m-a) + \\lambda \\frac{d}{dm}(-m)^p = (m-a) + \\lambda p(-m)^{p-1}(-1) = m-a - \\lambda p(-m)^{p-1}\n$$\nThe left-hand derivative at $m=0$ is the limit of $J'(m)$ as $m \\to 0^{-}$:\n$$\nJ'_{-}(0) = \\lim_{m \\to 0^{-}} \\left( m-a - \\lambda p (-m)^{p-1} \\right)\n$$\nAs $m \\to 0^{-}$, $-m \\to 0^{+}$, so $(-m)^{p-1}$ approaches $+\\infty$. Thus,\n$$\nJ'_{-}(0) = -a - \\lim_{m \\to 0^{-}} (\\lambda p (-m)^{p-1}) = -\\infty\n$$\nThe derivative of $J(m)$ changes sign from negative (approaching from the left of $0$) to positive (approaching from the right of $0$). Specifically, the slope approaches $-\\infty$ from the left and $+\\infty$ from the right, forming a cusp. According to the first derivative test, $m=0$ is a local minimum of $J(m)$.\n\n### Task 2: Derivation of the Quadratic Majorizer\n\nWe need to find a quadratic majorizer for $|m|^p$ of the form $w(m^{(k)})m^2 + c(m^{(k)})$. We use the concavity of a related function. Let $x=m^2$ and consider the function $\\phi(x) = (x+\\epsilon)^{p/2}$ for $x \\ge 0$, where $\\epsilon0$. Its derivatives with respect to $x$ are:\n$$\n\\phi'(x) = \\frac{p}{2}(x+\\epsilon)^{\\frac{p}{2}-1}\n$$\n$$\n\\phi''(x) = \\frac{p}{2}\\left(\\frac{p}{2}-1\\right)(x+\\epsilon)^{\\frac{p}{2}-2}\n$$\nSince $0p1$, we have $p/2  0$ and $p/2-1  0$. Therefore, $\\phi''(x)  0$ for all $x \\ge 0$, which confirms that $\\phi(x)$ is a strictly concave function.\n\nFor any concave function, the tangent line at a point $x_k$ provides an upper bound (a majorizer) for the function:\n$$\n\\phi(x) \\le \\phi(x_k) + \\phi'(x_k)(x - x_k)\n$$\nLet's choose $x_k = (m^{(k)})^2$ and $x=m^2$. This gives a majorizer for $(m^2+\\epsilon)^{p/2}$:\n$$\n(m^2+\\epsilon)^{p/2} \\le ((m^{(k)})^2+\\epsilon)^{p/2} + \\left[\\frac{p}{2}((m^{(k)})^2+\\epsilon)^{\\frac{p}{2}-1}\\right](m^2 - (m^{(k)})^2)\n$$\nSince $|m|^p = (m^2)^{p/2} \\le (m^2+\\epsilon)^{p/2}$ for any $\\epsilon0$, the right-hand side also majorizes $|m|^p$. Rearranging the expression into the desired form $w m^2 + c$:\n$$\n|m|^p \\le \\left[\\frac{p}{2}((m^{(k)})^2+\\epsilon)^{\\frac{p}{2}-1}\\right]m^2 + \\left[((m^{(k)})^2+\\epsilon)^{p/2} - \\frac{p}{2}((m^{(k)})^2+\\epsilon)^{\\frac{p}{2}-1}(m^{(k)})^2\\right]\n$$\nThis inequality is of the form $|m|^p \\le w(m^{(k)})m^2 + c(m^{(k)})$, where the weight $w(m^{(k)})$ is the coefficient of the $m^2$ term:\n$$\nw(m^{(k)}) = \\frac{p}{2}\\left((m^{(k)})^2+\\epsilon\\right)^{\\frac{p-2}{2}}\n$$\n\n### Task 3: IRLS Update Rule\n\nThe IRLS algorithm solves a sequence of quadratic subproblems.\nAt iteration $k$, we minimize a surrogate objective function $J_{surr}(m; m^{(k)})$ which majorizes the true objective $J(m)$.\n$$\nJ_{surr}(m; m^{(k)}) = \\frac{1}{2}(m-a)^2 + \\lambda \\left( w(m^{(k)})m^2 + c(m^{(k)}) \\right)\n$$\nTo find the minimizer $m^{(k+1)}$, we differentiate $J_{surr}$ with respect to $m$ and set the result to zero. The term $c(m^{(k)})$ is constant with respect to $m$ and does not affect the location of the minimum.\n$$\n\\frac{\\partial J_{surr}}{\\partial m} = (m-a) + 2\\lambda w(m^{(k)})m = 0\n$$\nSolving for $m$:\n$$\nm(1 + 2\\lambda w(m^{(k)})) = a\n$$\n$$\nm^{(k+1)} = \\frac{a}{1 + 2\\lambda w(m^{(k)})}\n$$\nSubstituting the expression for $w(m^{(k)})$ from Task 2:\n$$\nm^{(k+1)} = \\frac{a}{1 + 2\\lambda \\left( \\frac{p}{2}((m^{(k)})^2+\\epsilon)^{\\frac{p-2}{2}} \\right)} = \\frac{a}{1 + p\\lambda((m^{(k)})^2+\\epsilon)^{\\frac{p-2}{2}}}\n$$\nThis is the closed-form IRLS update rule.\n\n### Task 4: Fixed-Point Analysis\n\nWe are given $a=1$, $\\lambda=0.1$, $p=1/2$, $\\epsilon=10^{-8}$, and $m^{(0)}=0$. A fixed point $m^*$ of the IRLS iteration satisfies the equation $m^* = F(m^*)$, where $F(m)$ is the update map from Task 3:\n$$\nm^* = \\frac{1}{1 + (1/2)(0.1)((m^*)^2+10^{-8})^{\\frac{1/2-2}{2}}} = \\frac{1}{1 + 0.05((m^*)^2+10^{-8})^{-3/4}}\n$$\nWe are asked to analyze the fixed point near $m=0$. This implies $(m^*)^2 \\ll \\epsilon$. Under this assumption, the fixed-point equation simplifies to:\n$$\nm^* \\approx \\frac{1}{1 + 0.05(\\epsilon)^{-3/4}} = \\frac{1}{1 + 0.05(10^{-8})^{-3/4}} = \\frac{1}{1 + 0.05 \\times 10^{6}} = \\frac{1}{1 + 50000} = \\frac{1}{50001}\n$$\nLet's check the validity of the assumption $(m^*)^2 \\ll \\epsilon$. We have $m^* \\approx 1/50001 \\approx 2 \\times 10^{-5}$. Then $(m^*)^2 \\approx 4 \\times 10^{-10}$. Since $4 \\times 10^{-10} \\ll 10^{-8}$, the assumption is self-consistent.\n\nTo show the fixed point is stable, we analyze the derivative of the update map $F(m) = a/(1+p\\lambda(m^2+\\epsilon)^{(p-2)/2})$.\n$$\nF'(m) = -a p \\lambda (p-2) m (m^2+\\epsilon)^{\\frac{p-4}{2}} \\left[1+p\\lambda(m^2+\\epsilon)^{\\frac{p-2}{2}}\\right]^{-2}\n$$\nAt $m=0$, $F'(0) = 0$. Since the derivative is continuous, for a fixed point $m^*$ very close to $0$, we will have $|F'(m^*)| \\ll 1$, which is the condition for a stable fixed point. The iteration will converge to this point if started sufficiently close, which is the case for $m^{(0)}=0$.\n\nThe numerical value of the IRLS fixed point is $m^* = 1/50001 \\approx 1.9999600 \\times 10^{-5}$. Rounded to four significant figures, this is $2.000 \\times 10^{-5}$.\n\n### Task 5: Comparison with Stationary Points\n\nA stationary point $m^\\star  0$ of the original objective function $J(m)$ satisfies $J'(m^\\star) = 0$:\n$$\nm^\\star - a + \\lambda p (m^\\star)^{p-1} = 0\n$$\nUsing the given parameters, we have:\n$$\nm^\\star - 1 + (0.1)(1/2)(m^\\star)^{-1/2} = 0 \\implies m^\\star - 1 + 0.05(m^\\star)^{-1/2} = 0\n$$\nMultiplying by $(m^\\star)^{1/2}$ gives $ (m^\\star)^{3/2} - (m^\\star)^{1/2} + 0.05 = 0$. Let $u = (m^\\star)^{1/2}$. The equation becomes a cubic polynomial in $u$:\n$$\nu^3 - u + 0.05 = 0\n$$\nThe positive roots of this polynomial can be found numerically. They are $u_1 \\approx 0.05006$ and $u_2 \\approx 0.9756$. These correspond to stationary points $m_1=u_1^2 \\approx 0.0025$ and $m_2=u_2^2 \\approx 0.9518$. Analysis of the second derivative $J''(m) = 1 + \\lambda p(p-1)m^{p-2} = 1 - 0.0125 m^{-3/2}$ reveals that $m_1$ is a local maximum ($J''(m_1)0$) and $m_2$ is a local minimum ($J''(m_2)0$). Thus, the relevant positive stationary point is $m^\\star = m_2 \\approx 0.9518$. No stationary points exist for $m0$.\n\nWe now compare the objective function values at the three points of interest:\n1.  The local minimum at $m=0$: $J(0) = \\frac{1}{2}(0-1)^2 + 0.1\\sqrt{0} = 0.5$.\n2.  The IRLS fixed point $m_{IRLS} = 1/50001$:\n    $J(m_{IRLS}) = \\frac{1}{2}\\left(\\frac{1}{50001}-1\\right)^2 + 0.1\\sqrt{\\frac{1}{50001}} \\approx \\frac{1}{2}(1)^2 + \\frac{0.1}{\\sqrt{50001}} \\approx 0.5 + \\frac{0.1}{223.6} \\approx 0.5 + 0.000447 = 0.500447$.\n3.  The global minimum $m^\\star \\approx 0.9518$:\n    $J(m^\\star) = \\frac{1}{2}(0.9518-1)^2 + 0.1\\sqrt{0.9518} \\approx \\frac{1}{2}(-0.0482)^2 + 0.1(0.9756) \\approx 0.00116 + 0.09756 = 0.09872$.\n\nThe values are $J(m^\\star) \\approx 0.09872$, $J(0)=0.5$, and $J(m_{IRLS}) \\approx 0.500447$. The ordering $J(m^\\star)  J(0)  J(m_{IRLS})$ confirms that the IRLS fixed point is suboptimal. It represents convergence to a local minimum of the stabilized objective function, which is not a minimum of the original function and is inferior to both the local minimum at $m=0$ and especially the global minimum at $m^\\star$.\n\n### Task 6: Continuation Strategy\n\nThe failure of IRLS to find the global minimum is due to the nonconvexity of the objective function, which creates a strong local minimum at $m=0$ that traps the algorithm when initialized at or near zero. To overcome this, a continuation (or homotopy) method can be employed. This strategy starts with a simpler, convex version of the problem and gradually deforms it into the target nonconvex problem, guiding the solution towards the global minimum.\n\nA scientifically well-motivated strategy is continuation on the regularization exponent $p$.\n\n**Principle:**\n1.  **Start with a Convex Problem:** Begin with an exponent $p_0 \\ge 1$, which makes the regularizer $|m|^{p_0}$ convex. A standard choice is $p_0=1$. The problem becomes $\\min_m \\frac{1}{2}(m-a)^2 + \\lambda |m|$, which is a convex L1-regularization problem (LASSO). Its unique global minimum is given by the soft-thresholding operator: $m_0^* = \\text{sgn}(a)(|a|-\\lambda)_+$. With the given parameters, $m_0^* = (1-0.1)_+ = 0.9$. This provides an excellent initial guess, far from the basin of attraction of the zero minimum.\n2.  **Gradual Annealing of $p$:** Define a sequence of exponents $p_k$ decreasing from $p_0=1$ to the target value $p_{final}=1/2$. A simple schedule is an arithmetic progression: $p_k = 1 - k \\cdot \\Delta p$ for a small step $\\Delta p$.\n3.  **Iterative Refinement:** For each $k=1, 2, \\dots, N$, solve the subproblem $\\min_m \\left\\{ \\frac{1}{2}(m-a)^2 + \\lambda|m|^{p_k} \\right\\}$ using an iterative method like IRLS. Crucially, the initial guess for solving the subproblem at step $k$ is the solution from the previous step, $m_{k-1}^*$. This ensures the solution path is tracked continuously as the landscape deforms.\n4.  **Final Solution:** The final output of the continuation procedure, $m_N^*$, is the estimate for the global minimum of the original problem with $p=1/2$.\n\n**Stopping Criterion:** The main continuation loop terminates when the exponent $p_k$ reaches its target value $p_{final}=1/2$. Each inner IRLS solver for a fixed $p_k$ is run until a standard convergence criterion is met, for example, when the relative change in successive iterates falls below a predefined tolerance (e.g., $\\|m^{(j+1)} - m^{(j)}\\| / \\|m^{(j)}\\|  \\delta$). This strategy effectively avoids the suboptimal minimum by starting in a region of the parameter space where the solution is unique and easy to find, and then carefully tracking this solution as the problem's complexity (nonconvexity) is gradually introduced.", "answer": "$$\\boxed{2.000 \\times 10^{-5}}$$", "id": "3605298"}]}