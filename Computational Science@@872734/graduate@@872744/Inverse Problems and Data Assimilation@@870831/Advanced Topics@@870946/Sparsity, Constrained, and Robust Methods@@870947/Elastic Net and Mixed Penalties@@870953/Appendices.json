{"hands_on_practices": [{"introduction": "To appreciate the power of regularization, one must first grasp the fundamental bias-variance tradeoff. This exercise strips the elastic net down to its quadratic Ridge penalty to provide a clear, calculable demonstration of this core concept. By working through a simple two-dimensional example, you will derive how increasing the regularization parameter $\\lambda_2$ systematically trades a small amount of solution bias for a significant reduction in variance, leading to more stable predictions in the presence of noise [@problem_id:3377910].", "problem": "Consider a linear inverse problem in two dimensions with an identity observation operator. Let the data model be $y = x^{\\star} + \\varepsilon$, where $x^{\\star} \\in \\mathbb{R}^{2}$ is the unknown state, $y \\in \\mathbb{R}^{2}$ is the observed data, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{2})$ is additive Gaussian noise with zero mean and covariance $\\sigma^{2} I_{2}$. We perform estimation using the Elastic Net with the mixed penalty specialized to isolate the effect of the quadratic penalty by setting the $\\ell_{1}$ parameter to zero, yielding the ridge-regularized estimator\n$$\n\\widehat{x}_{\\lambda_{2}} \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2} \\| y - x \\|_{2}^{2} + \\frac{\\lambda_{2}}{2} \\| x \\|_{2}^{2} \\right\\}, \\quad \\lambda_{2} \\ge 0.\n$$\nDefine the in-sample prediction as $\\widehat{y}_{\\lambda_{2}} = \\widehat{x}_{\\lambda_{2}}$ and the noiseless truth as $y^{\\star} = x^{\\star}$. Predictive stability is quantified by the variance term in the error decomposition of the expected in-sample prediction error $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$ into its squared bias and variance components. \n\nStarting from the data model and the definition of $\\widehat{x}_{\\lambda_{2}}$, derive the bias-variance decomposition of $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$ as an explicit function of $\\lambda_{2}$, $x^{\\star}$, and $\\sigma^{2}$. Then, instantiate the example with $x^{\\star} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ and $\\sigma^{2} = 1$ to demonstrate that increasing $\\lambda_{2}$ from $0$ to $1$ improves predictive stability (variance reduction) at the cost of increased bias. Quantify this tradeoff by computing the ratio\n$$\nR \\equiv \\frac{\\text{reduction in variance when } \\lambda_{2}: 0 \\to 1}{\\text{increase in squared bias when } \\lambda_{2}: 0 \\to 1}.\n$$\nProvide the value of $R$ as a single exact number. Do not include units. Do not provide intermediate values. The final answer must be a single real number.", "solution": "The analysis begins by validating the problem statement.\n\n### Step 1: Extract Givens\n- Data model: $y = x^{\\star} + \\varepsilon$, where $x^{\\star} \\in \\mathbb{R}^{2}$ is the unknown true state, $y \\in \\mathbb{R}^{2}$ is the observation.\n- Noise model: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{2})$, where $I_{2}$ is the $2 \\times 2$ identity matrix.\n- Estimator: $\\widehat{x}_{\\lambda_{2}} \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2} \\| y - x \\|_{2}^{2} + \\frac{\\lambda_{2}}{2} \\| x \\|_{2}^{2} \\right\\}$, for $\\lambda_{2} \\ge 0$.\n- In-sample prediction: $\\widehat{y}_{\\lambda_{2}} = \\widehat{x}_{\\lambda_{2}}$.\n- Noiseless truth: $y^{\\star} = x^{\\star}$.\n- Quantity to analyze: The expected in-sample prediction error, $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$, and its bias-variance decomposition.\n- Specific parameters for instantiation: $x^{\\star} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$, $\\sigma^{2} = 1$.\n- Required calculation: The ratio $R \\equiv \\frac{\\text{reduction in variance when } \\lambda_{2}: 0 \\to 1}{\\text{increase in squared bias when } \\lambda_{2}: 0 \\to 1}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, being a standard formulation of ridge regression (a special case of the elastic net) in statistical learning theory. It is well-posed, as the cost function is strictly convex, guaranteeing a unique minimum. The problem is objective, using precise mathematical language. All data and definitions required for a unique solution are provided, and there are no contradictions. The problem asks for a rigorous derivation and a specific calculation, which is a standard exercise in the field of inverse problems. It is not trivial and is scientifically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\nThe first step is to find an explicit closed-form expression for the estimator $\\widehat{x}_{\\lambda_{2}}$. The cost function to be minimized is\n$$\nJ(x) = \\frac{1}{2} \\| y - x \\|_{2}^{2} + \\frac{\\lambda_{2}}{2} \\| x \\|_{2}^{2}\n$$\nThis function is strictly convex for $\\lambda_{2} \\ge 0$. The minimum is found by setting the gradient with respect to $x$ to zero.\n$$\n\\nabla_{x} J(x) = \\nabla_{x} \\left( \\frac{1}{2} (y - x)^{T}(y - x) + \\frac{\\lambda_{2}}{2} x^{T}x \\right) = -(y - x) + \\lambda_{2} x\n$$\nSetting the gradient to zero:\n$$\n-(y - \\widehat{x}_{\\lambda_{2}}) + \\lambda_{2} \\widehat{x}_{\\lambda_{2}} = 0\n$$\n$$\n-y + \\widehat{x}_{\\lambda_{2}} + \\lambda_{2} \\widehat{x}_{\\lambda_{2}} = 0\n$$\n$$\n(1 + \\lambda_{2}) \\widehat{x}_{\\lambda_{2}} = y\n$$\nSolving for $\\widehat{x}_{\\lambda_{2}}$ yields:\n$$\n\\widehat{x}_{\\lambda_{2}} = \\frac{1}{1 + \\lambda_{2}} y\n$$\nThe in-sample prediction is $\\widehat{y}_{\\lambda_{2}} = \\widehat{x}_{\\lambda_{2}}$. The noiseless truth is $y^{\\star} = x^{\\star}$. The expected prediction error is $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$. We decompose this error into its squared bias and variance components:\n$$\n\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right] = \\underbrace{\\left\\| \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] - y^{\\star} \\right\\|_{2}^{2}}_{\\text{Squared Bias}} + \\underbrace{\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] \\|_{2}^{2}\\right]}_{\\text{Variance}}\n$$\nFirst, we compute the expected value of the prediction. The expectation is taken over the distribution of the noise $\\varepsilon$.\n$$\n\\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] = \\mathbb{E}\\left[ \\frac{1}{1 + \\lambda_{2}} y \\right] = \\frac{1}{1 + \\lambda_{2}} \\mathbb{E}[y]\n$$\nUsing the data model $y = x^{\\star} + \\varepsilon$ and the fact that $\\mathbb{E}[\\varepsilon] = 0$:\n$$\n\\mathbb{E}[y] = \\mathbb{E}[x^{\\star} + \\varepsilon] = x^{\\star} + \\mathbb{E}[\\varepsilon] = x^{\\star}\n$$\nThus, the expected prediction is:\n$$\n\\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] = \\frac{1}{1 + \\lambda_{2}} x^{\\star}\n$$\nNow, we calculate the squared bias, denoted $B^2(\\lambda_2)$:\n$$\nB^2(\\lambda_2) = \\left\\| \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] - y^{\\star} \\right\\|_{2}^{2} = \\left\\| \\frac{1}{1 + \\lambda_{2}} x^{\\star} - x^{\\star} \\right\\|_{2}^{2} = \\left\\| \\left(\\frac{1}{1 + \\lambda_{2}} - 1\\right) x^{\\star} \\right\\|_{2}^{2} = \\left\\| \\frac{-\\lambda_{2}}{1 + \\lambda_{2}} x^{\\star} \\right\\|_{2}^{2}\n$$\n$$\nB^2(\\lambda_2) = \\left( \\frac{\\lambda_{2}}{1 + \\lambda_{2}} \\right)^{2} \\|x^{\\star}\\|_{2}^{2}\n$$\nNext, we compute the variance, denoted $V(\\lambda_2)$:\n$$\nV(\\lambda_2) = \\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] \\|_{2}^{2}\\right] = \\mathbb{E}\\left[ \\left\\| \\frac{1}{1 + \\lambda_{2}} y - \\frac{1}{1 + \\lambda_{2}} x^{\\star} \\right\\|_{2}^{2} \\right]\n$$\n$$\nV(\\lambda_2) = \\mathbb{E}\\left[ \\left\\| \\frac{1}{1 + \\lambda_{2}} (y - x^{\\star}) \\right\\|_{2}^{2} \\right] = \\mathbb{E}\\left[ \\left\\| \\frac{1}{1 + \\lambda_{2}} \\varepsilon \\right\\|_{2}^{2} \\right] = \\frac{1}{(1 + \\lambda_{2})^{2}} \\mathbb{E}[\\|\\varepsilon\\|_{2}^{2}]\n$$\nThe noise vector is $\\varepsilon = (\\varepsilon_1, \\varepsilon_2)^T$, where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ are independent. The expected squared norm of $\\varepsilon$ is:\n$$\n\\mathbb{E}[\\|\\varepsilon\\|_{2}^{2}] = \\mathbb{E}[\\varepsilon_1^2 + \\varepsilon_2^2] = \\mathbb{E}[\\varepsilon_1^2] + \\mathbb{E}[\\varepsilon_2^2]\n$$\nFor any random variable $Z$ with mean $\\mu_Z$ and variance $\\sigma_Z^2$, $\\mathbb{E}[Z^2] = \\text{Var}(Z) + (\\mathbb{E}[Z])^2$. Here, $\\mathbb{E}[\\varepsilon_i] = 0$ and $\\text{Var}(\\varepsilon_i) = \\sigma^2$. So, $\\mathbb{E}[\\varepsilon_i^2] = \\sigma^2$.\n$$\n\\mathbb{E}[\\|\\varepsilon\\|_{2}^{2}] = \\sigma^2 + \\sigma^2 = 2\\sigma^2\n$$\nSubstituting this back into the variance expression:\n$$\nV(\\lambda_2) = \\frac{2\\sigma^2}{(1 + \\lambda_{2})^{2}}\n$$\nThe full bias-variance decomposition is:\n$$\n\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right] = \\left( \\frac{\\lambda_{2}}{1 + \\lambda_{2}} \\right)^{2} \\|x^{\\star}\\|_{2}^{2} + \\frac{2\\sigma^2}{(1 + \\lambda_{2})^{2}}\n$$\nNow, we instantiate this with the given parameters: $x^{\\star} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ and $\\sigma^{2} = 1$.\nFirst, calculate $\\|x^{\\star}\\|_{2}^{2}$:\n$$\n\\|x^{\\star}\\|_{2}^{2} = 2^2 + 1^2 = 4 + 1 = 5\n$$\nWe evaluate the squared bias and variance for $\\lambda_{2} = 0$ and $\\lambda_{2} = 1$.\n\nFor $\\lambda_{2} = 0$:\nSquared Bias: $B^2(0) = \\left( \\frac{0}{1 + 0} \\right)^{2} \\times 5 = 0$.\nVariance: $V(0) = \\frac{2(1)}{(1 + 0)^{2}} = 2$.\n\nFor $\\lambda_{2} = 1$:\nSquared Bias: $B^2(1) = \\left( \\frac{1}{1 + 1} \\right)^{2} \\times 5 = \\left(\\frac{1}{2}\\right)^{2} \\times 5 = \\frac{5}{4}$.\nVariance: $V(1) = \\frac{2(1)}{(1 + 1)^{2}} = \\frac{2}{4} = \\frac{1}{2}$.\n\nThe increase in squared bias when $\\lambda_2$ goes from $0$ to $1$ is:\n$$\n\\Delta B^2 = B^2(1) - B^2(0) = \\frac{5}{4} - 0 = \\frac{5}{4}\n$$\nThe reduction in variance when $\\lambda_2$ goes from $0$ to $1$ is:\n$$\n\\Delta V = V(0) - V(1) = 2 - \\frac{1}{2} = \\frac{3}{2}\n$$\nThe problem asks for the ratio $R$:\n$$\nR = \\frac{\\text{reduction in variance}}{\\text{increase in squared bias}} = \\frac{\\Delta V}{\\Delta B^2} = \\frac{3/2}{5/4}\n$$\n$$\nR = \\frac{3}{2} \\times \\frac{4}{5} = \\frac{12}{10} = \\frac{6}{5}\n$$", "answer": "$$\n\\boxed{\\frac{6}{5}}\n$$", "id": "3377910"}, {"introduction": "While the $\\ell_1$ component of the elastic net is celebrated for its ability to induce sparsity, its interplay with the $\\ell_2$ penalty can lead to more complex outcomes. This practice explores a critical scenario where this interaction, known as the \"grouping effect,\" causes the estimator to fail at recovering the true sparse support of a signal. Through a carefully designed counterexample, you will use first-order optimality conditions and geometric reasoning to understand why a strong $\\ell_2$ penalty can overwhelm the sparsity-promoting pressure of the $\\ell_1$ norm, a vital lesson for correctly applying and interpreting elastic net solutions [@problem_id:3377903].", "problem": "Consider the linear inverse problem with a single scalar observation modeled as $y = A x^{\\star}$, where $A \\in \\mathbb{R}^{1 \\times 2}$, $x^{\\star} \\in \\mathbb{R}^{2}$, and $y \\in \\mathbb{R}$. Let $A = [\\,1 \\;\\; 1\\,]$, $x^{\\star} = (2,\\,0)^{\\top}$, and consequently $y = 2$. An estimator $x \\in \\mathbb{R}^{2}$ is obtained by minimizing the elastic net objective\n$$\nJ(x) \\;=\\; \\frac{1}{2}\\,\\|y - A x\\|_{2}^{2} \\;+\\; \\lambda_{1}\\,\\|x\\|_{1} \\;+\\; \\frac{\\lambda_{2}}{2}\\,\\|x\\|_{2}^{2},\n$$\nwith $\\lambda_{1} = 1$ and $\\lambda_{2} = 10$. Starting from the core definition of the elastic net objective and basic convex optimization principles (including first-order optimality and convexity arguments), derive the exact minimizer $\\hat{x}$ and compute its value. Then, explain—using geometric reasoning grounded in the geometry of level sets and normal cones in convex analysis—why this construction provides a counterexample to support recovery when $\\lambda_{2}$ is large relative to $\\lambda_{1}$.\n\nYour final numeric answer should be the minimizer vector $\\hat{x}$ expressed as a single row matrix. Do not round; provide the exact values.", "solution": "The user wants me to solve for the minimizer of an elastic net objective function and provide a geometric explanation for a specific property of the solution.\n\n### Step 1: Problem Validation\n\n**Extracted Givens:**\n- Linear model: $y = A x^{\\star}$, where $A \\in \\mathbb{R}^{1 \\times 2}$, $x^{\\star} \\in \\mathbb{R}^{2}$, $y \\in \\mathbb{R}$.\n- Data: $A = [\\,1 \\;\\; 1\\,]$, $x^{\\star} = (2,\\,0)^{\\top}$, $y = 2$.\n- Elastic net objective function: $J(x) = \\frac{1}{2}\\,\\|y - A x\\|_{2}^{2} + \\lambda_{1}\\,\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\,\\|x\\|_{2}^{2}$.\n- Regularization parameters: $\\lambda_{1} = 1$, $\\lambda_{2} = 10$.\n- Task 1: Derive the exact minimizer $\\hat{x}$.\n- Task 2: Explain geometrically why the result provides a counterexample to support recovery.\n\n**Validation:**\n1.  **Scientific Grounding:** The problem is a standard application of elastic net regularization, a well-established technique in convex optimization, statistics, and machine learning. All concepts are scientifically sound.\n2.  **Well-Posedness:** The objective function $J(x)$ is a sum of three convex functions: a convex quadratic loss, a convex $L_1$ norm, and a strictly convex $L_2$-squared norm (since $\\lambda_2 = 10 > 0$). The sum is therefore strictly convex. A strictly convex function over a compact set or over $\\mathbb{R}^n$ (if coercive, which it is) admits a unique minimizer. The problem is well-posed.\n3.  **Objectivity:** The problem is stated in precise mathematical language, free from subjectivity.\n4.  **Completeness:** All necessary values ($A, y, \\lambda_1, \\lambda_2$) are provided and are consistent ($A x^{\\star} = [1\\;1](2,0)^\\top = 2 = y$).\n\n**Verdict:** The problem is valid and all its components are well-defined.\n\n---\n\n### Step 2: Solution Derivation\n\nThe objective function to minimize is $J(x) = \\frac{1}{2}\\,\\|y - A x\\|_{2}^{2} + \\lambda_{1}\\,\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\,\\|x\\|_{2}^{2}$.\nLet $x = (x_1, x_2)^{\\top}$. Substituting the given values $y=2$, $A = [\\,1 \\;\\; 1\\,]$, $\\lambda_1=1$, and $\\lambda_2=10$:\n$$\nJ(x_1, x_2) = \\frac{1}{2}(2 - (x_1 + x_2))^2 + |x_1| + |x_2| + \\frac{10}{2}(x_1^2 + x_2^2)\n$$\nThis function is convex but non-differentiable due to the absolute value terms. A point $\\hat{x}$ is the unique minimizer of $J(x)$ if and only if the zero vector is an element of the subdifferential of $J$ at $\\hat{x}$, i.e., $0 \\in \\partial J(\\hat{x})$.\n\nThe objective function can be split into a differentiable part, $f(x) = \\frac{1}{2}(y - Ax)^2 + \\frac{\\lambda_2}{2}\\|x\\|_2^2$, and a non-differentiable part, $g(x) = \\lambda_1 \\|x\\|_1$. The subdifferential is $\\partial J(x) = \\nabla f(x) + \\partial g(x)$. The first-order optimality condition is $0 \\in \\nabla f(\\hat{x}) + \\lambda_1 \\partial \\|\\hat{x}\\|_1$.\n\nThe gradient of the smooth part $f(x)$ is:\n$$\n\\nabla f(x) = -A^{\\top}(y - Ax) + \\lambda_2 x\n$$\nSubstituting the given values:\n$$\n\\nabla f(x) = -\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}(2 - (x_1 + x_2)) + 10 \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} -(2 - x_1 - x_2) + 10x_1 \\\\ -(2 - x_1 - x_2) + 10x_2 \\end{pmatrix} = \\begin{pmatrix} 11x_1 + x_2 - 2 \\\\ x_1 + 11x_2 - 2 \\end{pmatrix}\n$$\nThe subdifferential of the $L_1$ norm is $\\partial \\|x\\|_1 = \\bigotimes_{i=1}^2 \\partial |x_i|$, where $\\partial |z| = \\text{sign}(z)$ if $z \\neq 0$ and $\\partial |z| = [-1, 1]$ if $z=0$.\n\nThe optimality condition $0 \\in \\nabla f(\\hat{x}) + \\lambda_1 \\partial \\|\\hat{x}\\|_1$ with $\\lambda_1=1$ becomes:\n$$\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\in \\begin{pmatrix} 11\\hat{x}_1 + \\hat{x}_2 - 2 \\\\ \\hat{x}_1 + 11\\hat{x}_2 - 2 \\end{pmatrix} + \\begin{pmatrix} \\partial |\\hat{x}_1| \\\\ \\partial |\\hat{x}_2| \\end{pmatrix}\n$$\nThis gives a system of two subdifferential inclusions:\n1. $-(11\\hat{x}_1 + \\hat{x}_2 - 2) \\in \\partial |\\hat{x}_1|$\n2. $-(\\hat{x}_1 + 11\\hat{x}_2 - 2) \\in \\partial |\\hat{x}_2|$\n\nDue to the symmetry of the problem setup ($A=[1,1]$), the objective function $J(x_1, x_2)$ is symmetric with respect to swapping $x_1$ and $x_2$. Since $J(x)$ is strictly convex and thus has a unique minimizer, this minimizer must also be symmetric, i.e., $\\hat{x}_1 = \\hat{x}_2$. Let $\\hat{x}_1 = \\hat{x}_2 = \\alpha$. The solution cannot be $(0,0)^{\\top}$ because $J(0,0)=2$, while a small perturbation, e.g., $x=(\\epsilon, \\epsilon)$ for small $\\epsilon>0$, gives $J(\\epsilon, \\epsilon) \\approx \\frac{1}{2}(2-2\\epsilon)^2 + 2\\epsilon \\approx 2 - 4\\epsilon + 2\\epsilon^2 + 2\\epsilon = 2-2\\epsilon < 2$. Thus, $\\alpha \\neq 0$.\n\nLet's assume $\\alpha > 0$. Then $\\partial |\\hat{x}_1| = \\{1\\}$ and $\\partial |\\hat{x}_2| = \\{1\\}$. The inclusions become equations:\n1. $-(11\\alpha + \\alpha - 2) = 1 \\implies -12\\alpha + 2 = 1 \\implies 12\\alpha = 1 \\implies \\alpha = \\frac{1}{12}$\n2. $-(\\alpha + 11\\alpha - 2) = 1 \\implies -12\\alpha + 2 = 1 \\implies 12\\alpha = 1 \\implies \\alpha = \\frac{1}{12}$\n\nBoth equations consistently yield $\\alpha = 1/12$. Since this value is positive, our assumption $\\alpha > 0$ is valid. We have found a point that satisfies the first-order optimality conditions. Due to strict convexity, this point is the unique global minimizer.\nThe minimizer is $\\hat{x} = (\\frac{1}{12}, \\frac{1}{12})^{\\top}$.\n\n### Step 3: Geometric Explanation for Failure of Support Recovery\n\n**Support Recovery:** The true sparse vector is $x^{\\star} = (2, 0)^{\\top}$. Its support (the set of indices of non-zero elements) is $\\{1\\}$. The estimated vector is $\\hat{x} = (\\frac{1}{12}, \\frac{1}{12})^{\\top}$. Its support is $\\{1, 2\\}$. Since the supports do not match, the elastic net estimator fails to perform support recovery in this case. This provides a counterexample.\n\n**Geometric and Analytical Explanation:**\nThe elastic net estimator $\\hat{x}$ is the point of tangency between a level set of the smooth part of the objective, $f(x) = \\frac{1}{2}\\|y-Ax\\|^2_2 + \\frac{\\lambda_2}{2}\\|x\\|^2_2$, and a level set of the non-smooth part, $g(x) = \\lambda_1\\|x\\|_1$. Formally, this is expressed by the optimality condition $-\\nabla f(\\hat{x}) \\in \\partial g(\\hat{x})$, which states that the negative gradient of the smooth part must lie in the normal cone of the $L_1$-ball at the point $\\hat{x}$.\n\nLet's investigate if a sparse solution of the form $\\hat{x} = (c, 0)^{\\top}$ with $c>0$ could be optimal. For such a point, the normal cone to the $L_1$-ball scaled by $\\lambda_1$ is $\\lambda_1 \\left( \\{1\\} \\times [-1, 1] \\right)$. The optimality condition $-\\nabla f(c,0) \\in \\lambda_1 \\left( \\{1\\} \\times [-1, 1] \\right)$ becomes:\n$$\n\\begin{pmatrix} -(11c + 0 - 2) \\\\ -(c + 0 - 2) \\end{pmatrix} = \\begin{pmatrix} 2 - 11c \\\\ 2 - c \\end{pmatrix} \\in \\begin{pmatrix} 1 \\cdot \\{1\\} \\\\ 1 \\cdot [-1, 1] \\end{pmatrix} = \\begin{pmatrix} \\{1\\} \\\\ [-1, 1] \\end{pmatrix}\n$$\nThis yields two conditions:\n1. $2 - 11c = 1 \\implies 11c = 1 \\implies c = \\frac{1}{11}$.\n2. $|2-c| \\le 1$.\n\nSubstituting $c=1/11$ from the first condition into the second:\n$$\n|2 - \\frac{1}{11}| = |\\frac{22-1}{11}| = \\frac{21}{11}\n$$\nThe condition becomes $\\frac{21}{11} \\le 1$, which is false.\n\nGeometrically, this means that at the point $(1/11, 0)^{\\top}$ on the $x_1$-axis, the negative gradient vector $-\\nabla f(c,0)$ points outside the normal cone of the $L_1$-ball. The second component of the gradient, $-(c-2)$, is too large, indicating that a descent in the objective function can be achieved by making $x_2$ non-zero. Thus, no sparse solution of the form $(c,0)^{\\top}$ can be optimal.\n\nThe role of a large $\\lambda_2$ is crucial here. A large $\\lambda_2$ guarantees failure of support recovery. This failure is a manifestation of the \"grouping effect\" of the elastic net. The predictors (columns of $A = [\\,1 \\;\\; 1\\,]$) are perfectly correlated. The Ridge penalty term, $\\frac{\\lambda_2}{2}\\|x\\|_2^2$, encourages correlated predictors to have similar coefficient values. When $\\lambda_2$ is large relative to $\\lambda_1$, this grouping effect dominates the sparsity-inducing pressure of the $L_1$ penalty. It forces $\\hat{x}_1$ and $\\hat{x}_2$ to be close to each other, pulling $\\hat{x}_2$ away from zero and thus destroying the sparsity of the true solution. The symmetry of the problem ($A_1=A_2=1$) makes this effect absolute, forcing $\\hat{x}_1 = \\hat{x}_2$ and making support recovery impossible for any $\\lambda_1, \\lambda_2 > 0$ (unless the solution is trivial at the origin).", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{12} & \\frac{1}{12} \\end{pmatrix}}\n$$", "id": "3377903"}, {"introduction": "Bridging the gap between theoretical models and practical computation often involves clever algebraic reformulations. This exercise introduces a widely used \"augmentation trick\" that recasts the elastic net objective into an equivalent problem that has the structure of a standard LASSO or analysis-form sparsity problem. By deriving this transformation, you will learn how to adapt a broader range of existing, highly-optimized solvers to handle elastic net regularization, a valuable skill for tackling large-scale inverse problems efficiently [@problem_id:3377866].", "problem": "Consider a linear inverse problem in which an unknown state vector $x \\in \\mathbb{R}^{n}$ is related to observed data $y \\in \\mathbb{R}^{m}$ by a known linear forward operator $A \\in \\mathbb{R}^{m \\times n}$ through $y = A x + \\varepsilon$, where $\\varepsilon$ represents additive noise. A common approach in inverse problems and data assimilation is to estimate $x$ by minimizing a composite objective that balances fidelity to the data with regularization. Suppose the objective function is\n$$\nJ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + g(x),\n$$\nwhere the analysis elastic net penalty is defined by\n$$\ng(x) = \\lambda_{1}\\|W x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2},\n$$\nwith $W \\in \\mathbb{R}^{p \\times n}$ a fixed analysis operator (for example, a discrete gradient or wavelet transform), $\\lambda_{1} > 0$, and $\\lambda_{2} \\ge 0$. Starting from the fundamental properties of the Euclidean norm and standard linear algebra identities (specifically, that for any vectors $u$ and $v$ of compatible dimensions, $\\|[u; v]\\|_{2}^{2} = \\|u\\|_{2}^{2} + \\|v\\|_{2}^{2}$), derive an equivalent formulation of the minimization problem in which the quadratic part of $g(x)$ is absorbed into the data misfit via an augmentation of the forward operator and observation vector. More precisely, show that there exist an augmented operator $\\tilde{A} \\in \\mathbb{R}^{(m+n) \\times n}$ and an augmented observation vector $\\tilde{b} \\in \\mathbb{R}^{m+n}$ such that\n$$\nJ(x) = \\frac{1}{2}\\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2} + \\lambda_{1}\\|W x\\|_{1},\n$$\nand write $\\tilde{A}$ and $\\tilde{b}$ explicitly in terms of $A$, $y$, $\\lambda_{2}$, and $I \\in \\mathbb{R}^{n \\times n}$, the identity matrix. Assume $\\lambda_{2} \\ge 0$ and take the principal square root when forming any scalar square roots. Your final answer must be the explicit expressions for $\\tilde{A}$ and $\\tilde{b}$ as a single analytical expression. No numerical approximation or rounding is required.", "solution": "The problem is valid as it is scientifically grounded in standard optimization and linear algebra principles, is well-posed with a unique answer, and is stated using objective, formal mathematical language.\n\nThe objective function to be minimized is given by:\n$$\nJ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + g(x)\n$$\nwhere a mixed elastic net penalty is defined as:\n$$\ng(x) = \\lambda_{1}\\|W x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}\n$$\nThe parameters $\\lambda_{1}$ and $\\lambda_{2}$ are non-negative, with $\\lambda_{1} > 0$ and $\\lambda_{2} \\ge 0$. Substituting the expression for $g(x)$ into $J(x)$, we have:\n$$\nJ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda_{1}\\|W x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}\n$$\nThe goal is to show that this can be reformulated into the form:\n$$\nJ(x) = \\frac{1}{2}\\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2} + \\lambda_{1}\\|W x\\|_{1}\n$$\nfor some augmented operator $\\tilde{A}$ and augmented vector $\\tilde{b}$. This requires us to combine the two quadratic terms, $\\frac{1}{2}\\|A x - y\\|_{2}^{2}$ and $\\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}$, into a single squared Euclidean norm. The term $\\lambda_{1}\\|W x\\|_{1}$ is to be left unchanged.\n\nLet's focus on the sum of the quadratic terms:\n$$\nQ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}\n$$\nWe can rewrite the second term, $\\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}$, by bringing the scalar coefficient inside the norm. Since $\\lambda_{2} \\ge 0$, its principal square root, $\\sqrt{\\lambda_{2}}$, is a real, non-negative number. The squared Euclidean norm of a vector $v$ is $\\|v\\|_{2}^{2} = v^{T}v$. Thus, $\\|x\\|_{2}^{2}$ can be manipulated as follows:\n$$\n\\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} = \\frac{1}{2} (\\sqrt{\\lambda_{2}})^{2} \\|x\\|_{2}^{2} = \\frac{1}{2} \\|\\sqrt{\\lambda_{2}} x\\|_{2}^{2}\n$$\nWe can express the vector $\\sqrt{\\lambda_{2}}x$ as a matrix-vector product using the identity matrix $I \\in \\mathbb{R}^{n \\times n}$, which gives $\\sqrt{\\lambda_{2}}x = (\\sqrt{\\lambda_{2}}I)x$. To match the structure of the first data-fitting term, we can introduce a zero vector $0_n \\in \\mathbb{R}^n$:\n$$\n\\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} = \\frac{1}{2} \\|(\\sqrt{\\lambda_{2}}I)x\\|_{2}^{2} = \\frac{1}{2} \\|(\\sqrt{\\lambda_{2}}I)x - 0_n\\|_{2}^{2}\n$$\nNow, substitute this back into the expression for $Q(x)$:\n$$\nQ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{1}{2}\\|(\\sqrt{\\lambda_{2}}I)x - 0_n\\|_{2}^{2}\n$$\nFactoring out the $\\frac{1}{2}$ term, we get:\n$$\nQ(x) = \\frac{1}{2} \\left( \\|A x - y\\|_{2}^{2} + \\|(\\sqrt{\\lambda_{2}}I)x - 0_n\\|_{2}^{2} \\right)\n$$\nThe problem statement provides the identity for the squared norm of a vertically concatenated vector: for any vectors $u$ and $v$ of compatible dimensions, $\\|[u; v]\\|_{2}^{2} = \\|u\\|_{2}^{2} + \\|v\\|_{2}^{2}$. Let's apply this identity in reverse.\nLet $u = Ax - y$ and $v = (\\sqrt{\\lambda_{2}}I)x - 0_n$. The vector $u \\in \\mathbb{R}^m$ and the vector $v \\in \\mathbb{R}^n$. The sum of their squared norms is equal to the squared norm of their vertical concatenation:\n$$\nQ(x) = \\frac{1}{2} \\left\\| \\begin{pmatrix} Ax - y \\\\ (\\sqrt{\\lambda_{2}}I)x - 0_n \\end{pmatrix} \\right\\|_{2}^{2}\n$$\nThis concatenated vector can be rewritten by separating the terms involving $x$ from the constant terms using block matrix algebra:\n$$\n\\begin{pmatrix} Ax - y \\\\ (\\sqrt{\\lambda_{2}}I)x - 0_n \\end{pmatrix} = \\begin{pmatrix} Ax \\\\ (\\sqrt{\\lambda_{2}}I)x \\end{pmatrix} - \\begin{pmatrix} y \\\\ 0_n \\end{pmatrix}\n$$\nFactoring the vector $x$ out of the first block vector gives:\n$$\n\\begin{pmatrix} A \\\\ \\sqrt{\\lambda_{2}}I \\end{pmatrix} x - \\begin{pmatrix} y \\\\ 0_n \\end{pmatrix}\n$$\nBy comparing this expression with the target form $\\tilde{A}x - \\tilde{b}$, we can identify the augmented operator $\\tilde{A}$ and the augmented observation vector $\\tilde{b}$.\nThe augmented operator $\\tilde{A}$ is a block matrix formed by stacking $A$ on top of $\\sqrt{\\lambda_{2}}I$. Since $A \\in \\mathbb{R}^{m \\times n}$ and $I \\in \\mathbb{R}^{n \\times n}$, the resulting matrix $\\tilde{A}$ has dimensions $(m+n) \\times n$:\n$$\n\\tilde{A} = \\begin{pmatrix} A \\\\ \\sqrt{\\lambda_{2}}I \\end{pmatrix}\n$$\nThe augmented vector $\\tilde{b}$ is formed by stacking the original observation vector $y$ on top of the zero vector $0_n$. Since $y \\in \\mathbb{R}^m$ and $0_n \\in \\mathbb{R}^n$, the resulting vector $\\tilde{b}$ has dimensions $(m+n) \\times 1$:\n$$\n\\tilde{b} = \\begin{pmatrix} y \\\\ 0_n \\end{pmatrix}\n$$\nSubstituting these definitions back, we have successfully shown that:\n$$\n\\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} = \\frac{1}{2} \\left\\| \\begin{pmatrix} A \\\\ \\sqrt{\\lambda_{2}}I \\end{pmatrix} x - \\begin{pmatrix} y \\\\ 0_n \\end{pmatrix} \\right\\|_{2}^{2} = \\frac{1}{2}\\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2}\n$$\nTherefore, the original objective function $J(x)$ is equivalent to:\n$$\nJ(x) = \\frac{1}{2}\\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2} + \\lambda_{1}\\|W x\\|_{1}\n$$\nThe expressions for $\\tilde{A}$ and $\\tilde{b}$ are thus derived as required.", "answer": "$$\n\\boxed{\\tilde{A} = \\begin{pmatrix} A \\\\ \\sqrt{\\lambda_2} I \\end{pmatrix}, \\quad \\tilde{b} = \\begin{pmatrix} y \\\\ 0_n \\end{pmatrix}}\n$$", "id": "3377866"}]}