{"hands_on_practices": [{"introduction": "The power of FISTA and other proximal gradient methods lies in their ability to handle composite objective functions with a non-smooth part, like the $\\ell_1$-norm. This is achieved through the proximal operator, which acts as a generalized projection. This first exercise [@problem_id:3446917] isolates this core building block, allowing you to master the mechanics of the soft-thresholding operation, which is the specific proximal operator for the $\\ell_1$-norm.", "problem": "Consider the composite convex optimization problem of minimizing the function $g(x) = f(x) + \\lambda \\|x\\|_{1}$, where $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ and $\\|x\\|_{1} = \\sum_{i} |x_{i}|$. In the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), one iteration uses the proximal mapping of the scaled $\\ell_{1}$-norm. Assume a current gradient step has produced the vector $z \\in \\mathbb{R}^{7}$, and you are to compute the proximal point\n$$\nx = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}(z),\n$$\nwhich is defined as the unique minimizer of the function\n$$\nx \\mapsto \\tau \\lambda \\|x\\|_{1} + \\frac{1}{2}\\|x - z\\|_{2}^{2}.\n$$\nLet the given data be\n$$\nz = \\left(1, -\\frac{1}{4}, \\frac{2}{5}, -\\frac{2}{5}, \\frac{7}{15}, -\\frac{9}{10}, 0\\right), \\quad \\lambda = \\frac{3}{5}, \\quad \\tau = \\frac{2}{3}.\n$$\nCompute the exact vector $x = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}(z)$ in simplest rational form. Provide your final answer as a single row vector. Do not approximate; no rounding is required.", "solution": "The problem asks for the computation of the proximal point $x = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}(z)$, which is defined as the unique minimizer of the optimization problem:\n$$\n\\min_{x \\in \\mathbb{R}^{7}} \\left( \\tau \\lambda \\|x\\|_{1} + \\frac{1}{2}\\|x - z\\|_{2}^{2} \\right)\n$$\nThe objective function can be written in terms of the components of the vectors $x = (x_1, \\dots, x_7)$ and $z = (z_1, \\dots, z_7)$:\n$$\n\\min_{x_1, \\dots, x_7} \\left( \\tau \\lambda \\sum_{i=1}^{7} |x_i| + \\frac{1}{2} \\sum_{i=1}^{7} (x_i - z_i)^2 \\right)\n$$\nThis objective function is separable, meaning it can be decomposed into a sum of functions, each depending on only one component $x_i$. Therefore, we can minimize the function with respect to each component $x_i$ independently:\n$$\nx_i = \\arg\\min_{u \\in \\mathbb{R}} \\left( \\tau \\lambda |u| + \\frac{1}{2}(u - z_i)^2 \\right) \\quad \\text{for } i = 1, \\dots, 7\n$$\nThis is the well-known soft-thresholding operator. Let $\\alpha = \\tau\\lambda$. The solution to the one-dimensional problem\n$$\n\\min_{u} \\left( \\alpha|u| + \\frac{1}{2}(u - v)^2 \\right)\n$$\nis given by the soft-thresholding function $S_{\\alpha}(v)$, which has the closed-form solution:\n$$\nS_{\\alpha}(v) = \\operatorname{sign}(v) \\max(|v| - \\alpha, 0)\n$$\nThis can be expressed piecewise as:\n$$\nS_{\\alpha}(v) = \\begin{cases} v - \\alpha  \\text{if } v  \\alpha \\\\ 0  \\text{if } |v| \\le \\alpha \\\\ v + \\alpha  \\text{if } v  -\\alpha \\end{cases}\n$$\nFirst, we must calculate the threshold parameter $\\alpha$ using the given values for $\\lambda$ and $\\tau$:\n$$\n\\lambda = \\frac{3}{5}, \\quad \\tau = \\frac{2}{3}\n$$\nThe threshold $\\alpha$ is:\n$$\n\\alpha = \\tau \\lambda = \\left(\\frac{2}{3}\\right) \\left(\\frac{3}{5}\\right) = \\frac{6}{15} = \\frac{2}{5}\n$$\nNow, we apply the soft-thresholding operator $S_{2/5}$ to each component of the vector $z$:\n$$\nz = \\left(1, -\\frac{1}{4}, \\frac{2}{5}, -\\frac{2}{5}, \\frac{7}{15}, -\\frac{9}{10}, 0\\right)\n$$\nLet's compute each component $x_i = S_{2/5}(z_i)$:\n\nFor $i=1$: $z_1 = 1$. The threshold is $\\alpha = \\frac{2}{5}$. Since $z_1 = 1  \\frac{2}{5}$, we have:\n$x_1 = z_1 - \\alpha = 1 - \\frac{2}{5} = \\frac{5}{5} - \\frac{2}{5} = \\frac{3}{5}$.\n\nFor $i=2$: $z_2 = -\\frac{1}{4}$. We compare $|z_2| = \\frac{1}{4}$ to $\\alpha = \\frac{2}{5}$. Converting to a common denominator of $20$, we have $\\frac{1}{4} = \\frac{5}{20}$ and $\\frac{2}{5} = \\frac{8}{20}$. Since $\\frac{5}{20}  \\frac{8}{20}$, $|z_2|  \\alpha$. Thus:\n$x_2 = 0$.\n\nFor $i=3$: $z_3 = \\frac{2}{5}$. We have $|z_3| = \\frac{2}{5}$, which is equal to the threshold $\\alpha = \\frac{2}{5}$. Since $|z_3| \\le \\alpha$:\n$x_3 = 0$.\n\nFor $i=4$: $z_4 = -\\frac{2}{5}$. We have $|z_4| = \\frac{2}{5}$, which is equal to the threshold $\\alpha = \\frac{2}{5}$. Since $|z_4| \\le \\alpha$:\n$x_4 = 0$.\n\nFor $i=5$: $z_5 = \\frac{7}{15}$. We compare $z_5$ to $\\alpha = \\frac{2}{5}$. Converting to a common denominator of $15$, we have $\\frac{2}{5} = \\frac{6}{15}$. Since $z_5 = \\frac{7}{15}  \\frac{6}{15} = \\alpha$, we have:\n$x_5 = z_5 - \\alpha = \\frac{7}{15} - \\frac{2}{5} = \\frac{7}{15} - \\frac{6}{15} = \\frac{1}{15}$.\n\nFor $i=6$: $z_6 = -\\frac{9}{10}$. We compare $z_6$ to $-\\alpha = -\\frac{2}{5}$. Converting to a common denominator of $10$, we have $-\\frac{2}{5} = -\\frac{4}{10}$. Since $z_6 = -\\frac{9}{10}  -\\frac{4}{10} = -\\alpha$, we have:\n$x_6 = z_6 + \\alpha = -\\frac{9}{10} + \\frac{2}{5} = -\\frac{9}{10} + \\frac{4}{10} = -\\frac{5}{10} = -\\frac{1}{2}$.\n\nFor $i=7$: $z_7 = 0$. We have $|z_7| = 0$, and $0 \\le \\frac{2}{5} = \\alpha$. Thus:\n$x_7 = 0$.\n\nCombining these components, we obtain the resulting vector $x$:\n$$\nx = \\left(\\frac{3}{5}, 0, 0, 0, \\frac{1}{15}, -\\frac{1}{2}, 0\\right)\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{3}{5}  0  0  0  \\frac{1}{15}  -\\frac{1}{2}  0 \\end{pmatrix}}\n$$", "id": "3446917"}, {"introduction": "With the proximal operator in hand, we can now assemble a complete FISTA iteration. This exercise [@problem_id:3446895] guides you through a single cycle of the algorithm, demonstrating how the Nesterov acceleration step combines with the classic proximal gradient update. By working through this one-dimensional example, you will gain a concrete understanding of how the iterates $x^k$, the momentum term $t_k$, and the extrapolated point $y^k$ interact to produce the next, improved estimate.", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem in one dimension, where the objective is to minimize the composite function $f(x) = g(x) + h(x)$ with $g(x) = \\frac{1}{2}\\|Ax - b\\|_{2}^{2}$ and $h(x) = \\lambda |x|$. Let the measurement matrix be scalar $A = a$ with $a = 2$, let $b = 3$, and let $\\lambda = 1$. The Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) initializes with iterate values $x^{k-1} = 0$, $x^{k} = 1$, and acceleration parameter $t_{k} = 2$. Use the following scientifically grounded bases:\n\n- The gradient of the smooth term $g$ is defined as $\\nabla g(x) = A^{\\top}(Ax - b)$.\n- The Lipschitz constant $L$ of the gradient $\\nabla g$ is the squared spectral norm of $A$, which in one dimension is $L = a^{2}$.\n- The proximal operator of $h$ with parameter $\\tau  0$ is defined by $\\operatorname{prox}_{\\tau h}(z) = \\arg\\min_{x}\\left\\{\\frac{1}{2}(x - z)^{2} + \\tau \\lambda |x|\\right\\}$.\n- The canonical Nesterov acceleration sequence for FISTA satisfies the implicit relation $t_{k+1}^{2} - t_{k+1} = t_{k}^{2}$.\n\nImplement one full FISTA iteration for this LASSO instance as follows: first determine $t_{k+1}$ from the implicit relation above; then form the extrapolated point $y^{k}$ using $x^{k}$, $x^{k-1}$, $t_{k}$, and $t_{k+1}$; compute the gradient $\\nabla g(y^{k})$; perform the gradient step $z^{k} = y^{k} - \\frac{1}{L}\\nabla g(y^{k})$; and apply the proximal operator with parameter $\\tau = \\frac{1}{L}$ to obtain $x^{k+1} = \\operatorname{prox}_{\\frac{1}{L}h}(z^{k})$. Express your final results exactly (no rounding) for the four quantities in the order $y^{k}$, $\\nabla g(y^{k})$, $x^{k+1}$, and $t_{k+1}$.", "solution": "We perform one full FISTA iteration according to the steps outlined in the problem. First, we compute the Lipschitz constant $L$ for the gradient of $g(x)$. Given the scalar matrix $A=a=2$, the Lipschitz constant is $L = a^2 = 2^2 = 4$. The step size for the gradient and proximal steps will be $1/L = 1/4$.\n\n**1. Determine the next acceleration parameter $t_{k+1}$**\nThe parameter $t_{k+1}$ is determined from the relation $t_{k+1}^{2} - t_{k+1} = t_{k}^{2}$. With the given $t_{k} = 2$, we have:\n$$t_{k+1}^{2} - t_{k+1} = 2^{2} = 4 \\implies t_{k+1}^{2} - t_{k+1} - 4 = 0$$\nWe solve this quadratic equation for $t_{k+1}$ using the quadratic formula, taking the positive root since the sequence $\\{t_k\\}$ is positive:\n$$t_{k+1} = \\frac{-(-1) + \\sqrt{(-1)^{2} - 4(1)(-4)}}{2(1)} = \\frac{1 + \\sqrt{1 + 16}}{2} = \\frac{1 + \\sqrt{17}}{2}$$\n\n**2. Form the extrapolated point $y^{k}$**\nThe extrapolated point $y^{k}$ is computed using the formula $y^{k} = x^{k} + \\frac{t_{k}-1}{t_{k+1}}(x^{k}-x^{k-1})$. Substituting the known values $x^{k}=1$, $x^{k-1}=0$, $t_{k}=2$, and our computed $t_{k+1}$:\n$$y^{k} = 1 + \\frac{2-1}{\\frac{1 + \\sqrt{17}}{2}}(1 - 0) = 1 + \\frac{1}{\\frac{1 + \\sqrt{17}}{2}} = 1 + \\frac{2}{1 + \\sqrt{17}}$$\nTo simplify, we rationalize the denominator:\n$$y^{k} = 1 + \\frac{2(1 - \\sqrt{17})}{(1 + \\sqrt{17})(1 - \\sqrt{17})} = 1 + \\frac{2(1 - \\sqrt{17})}{1 - 17} = 1 + \\frac{2(1 - \\sqrt{17})}{-16} = 1 + \\frac{\\sqrt{17} - 1}{8}$$\n$$y^{k} = \\frac{8}{8} + \\frac{\\sqrt{17} - 1}{8} = \\frac{7 + \\sqrt{17}}{8}$$\n\n**3. Compute the gradient $\\nabla g(y^{k})$**\nThe gradient of the smooth term $g(x) = \\frac{1}{2}(ax - b)^2$ is $\\nabla g(x) = a(ax - b)$. With $a=2$ and $b=3$, we have $\\nabla g(x) = 2(2x - 3)$. Evaluating at $y^{k}$:\n$$\\nabla g(y^{k}) = 2\\left(2\\left(\\frac{7 + \\sqrt{17}}{8}\\right) - 3\\right) = 2\\left(\\frac{7 + \\sqrt{17}}{4} - \\frac{12}{4}\\right) = 2\\left(\\frac{\\sqrt{17} - 5}{4}\\right) = \\frac{\\sqrt{17} - 5}{2}$$\n\n**4. Perform the gradient step to find $z^{k}$**\nThe gradient step updates $y^k$ to find an intermediate point $z^k$:\n$$z^{k} = y^{k} - \\frac{1}{L}\\nabla g(y^{k}) = \\frac{7 + \\sqrt{17}}{8} - \\frac{1}{4}\\left(\\frac{\\sqrt{17} - 5}{2}\\right) = \\frac{7 + \\sqrt{17}}{8} - \\frac{\\sqrt{17} - 5}{8}$$\n$$z^{k} = \\frac{7 + \\sqrt{17} - \\sqrt{17} + 5}{8} = \\frac{12}{8} = \\frac{3}{2}$$\n\n**5. Apply the proximal operator to obtain $x^{k+1}$**\nThe final step is to apply the proximal operator for $h(x) = \\lambda|x|$ to $z^k$. This is the soft-thresholding operator $S_{\\theta}(z) = \\operatorname{sign}(z)\\max(|z| - \\theta, 0)$. The threshold is $\\theta = \\frac{\\lambda}{L} = \\frac{1}{4}$.\n$$x^{k+1} = \\operatorname{prox}_{\\frac{1}{L}h}(z^{k}) = S_{1/4}\\left(\\frac{3}{2}\\right)$$\nSince $\\frac{3}{2} > \\frac{1}{4}$, the result is non-zero:\n$$x^{k+1} = \\frac{3}{2} - \\frac{1}{4} = \\frac{6}{4} - \\frac{1}{4} = \\frac{5}{4}$$\n\nThe four quantities requested are:\n- $y^{k} = \\frac{7 + \\sqrt{17}}{8}$\n- $\\nabla g(y^{k}) = \\frac{\\sqrt{17} - 5}{2}$\n- $x^{k+1} = \\frac{5}{4}$\n- $t_{k+1} = \\frac{1 + \\sqrt{17}}{2}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7 + \\sqrt{17}}{8}  \\frac{\\sqrt{17} - 5}{2}  \\frac{5}{4}  \\frac{1 + \\sqrt{17}}{2}\n\\end{pmatrix}\n}\n$$", "id": "3446895"}, {"introduction": "In practical applications, the Lipschitz constant $L$ of the gradient is often unknown or too expensive to compute. A robust FISTA implementation therefore requires a backtracking line search to adaptively determine the step size at each iteration. This problem [@problem_id:3446952] shifts the focus from calculation to algorithmic design, testing your understanding of the theoretical condition that guarantees convergence and how to structure the backtracking loop correctly.", "problem": "Consider the composite convex optimization problem of minimizing $f(x) = g(x) + h(x)$, where $g:\\mathbb{R}^n \\to \\mathbb{R}$ is closed, proper, convex, and differentiable with an $L$-Lipschitz continuous gradient, and $h:\\mathbb{R}^n \\to \\mathbb{R}\\cup\\{+\\infty\\}$ is closed, proper, and convex. The Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) aims to compute a sequence $\\{x^k\\}$ that decreases $f$ by repeated proximal-gradient steps with Nesterov-type extrapolation. A backtracking strategy is used to adaptively choose a local quadratic upper model for $g$ so that each accepted proximal step is computed with a curvature parameter $L_k$ for which the quadratic model majorizes $g$ at the trial point.\n\nAs foundational facts, use only the following:\n- The $L$-Lipschitz continuity of $\\nabla g$ implies that for any $y,u\\in\\mathbb{R}^n$ and any $L_k\\geq L$, the inequality $g(u) \\leq g(y) + \\nabla g(y)^\\top (u - y) + \\tfrac{L_k}{2}\\|u - y\\|^2$ holds.\n- The proximal operator of $h$ at parameter $\\lambda0$ is defined by $\\operatorname{prox}_{\\lambda h}(v) \\triangleq \\arg\\min_{u}\\big\\{h(u) + \\tfrac{1}{2\\lambda}\\|u - v\\|^2\\big\\}$.\n\nStarting from these bases and no others, determine which of the following backtracking schemes correctly implements FISTA with backtracking that tests the local quadratic upper bound for $g$ and adjusts $L_k$ until the bound holds. Assume an initial point $x^1$, set $t_1 = 1$, pick $\\eta1$, and let $y^1=x^1$. In all options, the extrapolation parameters $\\{t_k\\}$ and points $\\{y^k\\}$ are updated once a step is accepted.\n\nWhich option is correct?\n\nA. Initialize $L_k$ by $L_k \\leftarrow L_{k-1}$ for $k\\geq 2$ and pick any positive $L_1$. At iteration $k$, repeat until acceptance:\n- Compute the proximal-gradient trial point $u = \\operatorname{prox}_{h/L_k}\\big(y^k - \\tfrac{1}{L_k}\\nabla g(y^k)\\big)$.\n- If $g(u) \\leq g(y^k) + \\nabla g(y^k)^\\top (u - y^k) + \\tfrac{L_k}{2}\\|u - y^k\\|^2$, accept; otherwise set $L_k \\leftarrow \\eta L_k$ and repeat.\nUpon acceptance, set $x^{k+1}=u$, update $t_{k+1}=\\tfrac{1+\\sqrt{1+4t_k^2}}{2}$, and set $y^{k+1}=x^{k+1} + \\tfrac{t_k-1}{t_{k+1}}\\big(x^{k+1}-x^k\\big)$.\n\nB. Initialize $L_k$ by $L_k \\leftarrow L_{k-1}$ for $k\\geq 2$ and pick any positive $L_1$. At iteration $k$, repeat until acceptance:\n- Compute the trial point $u = \\operatorname{prox}_{L_k h}\\big(y^k - \\tfrac{1}{L_k}\\nabla g(u)\\big)$.\n- If $g(u) \\geq g(y^k) + \\nabla g(u)^\\top (u - y^k) + \\tfrac{L_k}{2}\\|u - y^k\\|^2$, accept; otherwise set $L_k \\leftarrow \\tfrac{1}{\\eta} L_k$ and repeat.\nUpon acceptance, set $x^{k+1}=u$, update $t_{k+1}=\\tfrac{1+\\sqrt{1+4t_k^2}}{2}$, and set $y^{k+1}=x^{k+1} - \\tfrac{t_k-1}{t_{k+1}}\\big(x^{k+1}-x^k\\big)$.\n\nC. Initialize $L_k$ by $L_k \\leftarrow L_{k-1}$ for $k\\geq 2$ and pick any positive $L_1$. At iteration $k$, repeat until acceptance:\n- Compute the trial point $u = \\operatorname{prox}_{h/L_k}\\big(y^k - \\tfrac{1}{L_k}\\nabla g(y^k)\\big)$.\n- If $g(u) \\leq g(y^k) + \\nabla g(y^k)^\\top (u - y^k) + L_k\\|u - y^k\\|^2$, accept; otherwise set $L_k \\leftarrow \\eta L_k$ and repeat.\nUpon acceptance, set $x^{k+1}=u$, update $t_{k+1}=\\tfrac{1+\\sqrt{1+4t_k^2}}{2}$, and set $y^{k+1}=x^{k+1} + \\tfrac{t_{k+1}-1}{t_k}\\big(x^{k+1}-x^k\\big)$.\n\nD. Initialize $L_k$ by $L_k \\leftarrow L_{k-1}$ for $k\\geq 2$ and pick any positive $L_1$. At iteration $k$, repeat until acceptance:\n- Compute the trial point $u = \\operatorname{prox}_{h/L_k}\\big(y^k - \\tfrac{1}{L_k}\\nabla g(y^k)\\big)$.\n- If $f(u) \\leq g(y^k) + \\nabla g(y^k)^\\top (u - y^k) + \\tfrac{L_k}{2}\\|u - y^k\\|^2 + h(y^k)$, accept; otherwise set $L_k \\leftarrow \\eta L_k$ and repeat.\nUpon acceptance, set $x^{k+1}=u$, update $t_{k+1}=\\tfrac{1+\\sqrt{1+4t_k^2}}{2}$, and set $y^{k+1}=x^{k+1} + \\tfrac{t_k-1}{t_{k+1}}\\big(x^{k+1}-x^k\\big)$.\n\nSelect the option that is fully consistent with the stated bases and yields a valid backtracking FISTA scheme that tests the quadratic upper bound on $g$ and increases $L_k$ until the bound holds. Provide only the letter of the correct option.", "solution": "The problem asks to identify the correct algorithm for FISTA with a backtracking line search, based on the specific foundational principles provided. The key is to find an adaptive step size $1/L_k$ that satisfies a majorization condition for the smooth function $g$ at each iteration.\n\nLet's break down the components of a correct backtracking FISTA iteration:\n1.  **Trial Point Calculation:** The next iterate is found by a proximal gradient step from the extrapolated point $y^k$. For a given step size $1/L_k$, the trial point $u$ is computed as:\n    $$u = \\operatorname{prox}_{h/L_k}\\left(y^k - \\frac{1}{L_k}\\nabla g(y^k)\\right)$$\n2.  **Backtracking Condition:** The problem explicitly states that the backtracking scheme \"tests the local quadratic upper bound for $g$.\" This bound is given as a foundational fact:\n    $$g(u) \\leq g(y^k) + \\nabla g(y^k)^\\top (u - y^k) + \\frac{L_k}{2}\\|u - y^k\\|^2$$\n    This is the condition that must be checked. If it fails, it means our current guess for $L_k$ is too small, and the quadratic model does not upper-bound the function $g$ at the point $u$.\n3.  **Action on Failure:** If the backtracking condition is not met, $L_k$ must be increased to make the majorization more likely to hold. The standard way to do this is to multiply it by a factor $\\eta > 1$, i.e., $L_k \\leftarrow \\eta L_k$.\n4.  **Updates on Acceptance:** Once an $L_k$ is found that satisfies the condition, the trial point $u$ is accepted as the next iterate, $x^{k+1} = u$. Then, the FISTA momentum parameters are updated according to the standard formulas:\n    $$t_{k+1} = \\frac{1+\\sqrt{1+4t_k^2}}{2}$$\n    $$y^{k+1} = x^{k+1} + \\frac{t_k-1}{t_{k+1}}(x^{k+1}-x^k)$$\n\nNow we evaluate the options based on these criteria:\n\n-   **Option A:**\n    -   Trial point calculation is correct.\n    -   Backtracking condition matches the required majorization inequality for $g$ precisely.\n    -   Action on failure ($L_k \\leftarrow \\eta L_k$) is correct.\n    -   Updates for $x^{k+1}$, $t_{k+1}$, and $y^{k+1}$ are all correct.\n    -   This option correctly describes the standard FISTA with backtracking.\n\n-   **Option B:**\n    -   The trial point calculation is incorrect; it uses $\\nabla g(u)$ making the update implicit, and the scaling in the proximal operator is wrong.\n    -   The backtracking condition is incorrect (inequality reversed, wrong gradient).\n    -   The action on failure is incorrect (decreases $L_k$).\n    -   The extrapolation update has the wrong sign.\n\n-   **Option C:**\n    -   The backtracking condition is incorrect. It is missing the factor of $\\frac{1}{2}$ in the quadratic term, i.e., it uses $L_k\\|u-y^k\\|^2$ instead of $\\frac{L_k}{2}\\|u-y^k\\|^2$.\n    -   The extrapolation update for $y^{k+1}$ is incorrect; the coefficient should be $\\frac{t_k-1}{t_{k+1}}$.\n\n-   **Option D:**\n    -   The backtracking condition tests an inequality on the full objective function $f(u)$, not just the smooth part $g(u)$. While this is a valid descent condition used in some proximal algorithms, it does not match the problem's specific requirement to \"test the local quadratic upper bound for $g$\".\n\nBased on this analysis, Option A is the only one that is fully consistent with all the foundational facts and requirements stated in the problem.", "answer": "$$\\boxed{A}$$", "id": "3446952"}]}