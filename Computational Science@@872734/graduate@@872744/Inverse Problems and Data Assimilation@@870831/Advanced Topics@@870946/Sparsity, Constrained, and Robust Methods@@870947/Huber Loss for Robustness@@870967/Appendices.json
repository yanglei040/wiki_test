{"hands_on_practices": [{"introduction": "Understanding the theoretical benefits of Huber loss is best complemented by a concrete, hands-on calculation. This first practice exercise strips the problem down to its simplest formâ€”a scalar state with one clean observation and one gross outlier. By manually deriving and computing the estimates using $L_2$, $L_1$, and Huber loss functions, you will gain a direct, quantitative intuition for how the Huber loss function provides a robust compromise, pulling the solution away from the outlier without completely ignoring its presence [@problem_id:3389451].", "problem": "Consider a scalar inverse problem in the context of Data Assimilation (DA) where the unknown state is a single real parameter $x \\in \\mathbb{R}$. Two independent point observations are available through the identity observation operator, so each observation directly measures $x$. One observation is clean and one is contaminated by a large error. Numerically, the observations are $y_{1} = 1.2$ (clean) and $y_{2} = 9.0$ (contaminated). To reflect differing trust, assign observation weights $w_{1} = 2$ to $y_{1}$ and $w_{2} = 0.5$ to $y_{2}$. Let the Huber loss threshold be $\\delta = 1$.\n\nDefine the residuals for the two observations as $r_{1}(x) = x - y_{1}$ and $r_{2}(x) = x - y_{2}$. Consider three estimators corresponding to three data misfit models:\n- The $L_{2}$ (least squares) estimator minimizes the weighted quadratic misfit $\\sum_{i=1}^{2} w_{i} \\, r_{i}(x)^{2}$.\n- The $L_{1}$ (least absolute deviations) estimator minimizes the weighted absolute misfit $\\sum_{i=1}^{2} w_{i} \\, |r_{i}(x)|$.\n- The Huber estimator minimizes the weighted Huber loss $\\sum_{i=1}^{2} w_{i} \\, \\rho_{\\delta}(r_{i}(x))$ with threshold $\\delta$, where $\\rho_{\\delta}$ is the Huber loss.\n\nStarting from the core definitions of these loss functions and first-order optimality conditions for convex minimization, derive each estimator for this two-observation scalar problem. Compute the three estimates explicitly for the given numerical values, and compare them numerically. Report only the numerical value of the Huber estimate $\\hat{x}_{\\mathrm{Huber}}$. No rounding is required. The unknown state $x$ is dimensionless, so express the final answer as a pure number without units.", "solution": "The problem statement is critically evaluated for validity before attempting a solution.\n\n### Step 1: Extract Givens\n- Unknown state: a single real parameter $x \\in \\mathbb{R}$.\n- Observations: $y_{1} = 1.2$ (clean), $y_{2} = 9.0$ (contaminated).\n- Observation operator: Identity.\n- Observation weights: $w_{1} = 2$, $w_{2} = 0.5$.\n- Huber loss threshold: $\\delta = 1$.\n- Residuals: $r_{1}(x) = x - y_{1}$, $r_{2}(x) = x - y_{2}$.\n- $L_{2}$ estimator minimizes: $J_2(x) = \\sum_{i=1}^{2} w_{i} \\, r_{i}(x)^{2}$.\n- $L_{1}$ estimator minimizes: $J_1(x) = \\sum_{i=1}^{2} w_{i} \\, |r_{i}(x)|$.\n- Huber estimator minimizes: $J_H(x) = \\sum_{i=1}^{2} w_{i} \\, \\rho_{\\delta}(r_{i}(x))$, where $\\rho_{\\delta}$ is the Huber loss function.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed based on the predefined criteria:\n- **Scientifically Grounded:** The problem is a standard exercise in statistical estimation and optimization, specifically in the context of robust statistics as applied to inverse problems and data assimilation. The concepts of $L_1$, $L_2$, and Huber loss are fundamental in these fields. The setup is scientifically and mathematically sound.\n- **Well-Posed:** The cost functions defined ($L_1$, $L_2$, Huber) are all convex for a scalar variable $x$. A convex function on $\\mathbb{R}$ has a non-empty set of global minimizers, and for these strictly convex (or piecewise strictly convex) functions, the minimizer is unique. The problem is well-posed.\n- **Objective:** The problem is stated using precise mathematical definitions and numerical values. There are no subjective or ambiguous terms.\n- **Completeness and Consistency:** All necessary data ($y_1, y_2, w_1, w_2, \\delta$) and definitions are provided. The setup is self-contained and free of contradictions.\n- **Realism and Feasibility:** The scenario of having a mix of clean and contaminated data is a canonical problem in data analysis, making the problem realistic. The numerical values are chosen to clearly illustrate the differing behaviors of the estimators.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-posed, scientifically grounded problem that can be solved using established mathematical principles. I will now proceed with deriving the solution.\n\nThe goal is to find the estimates $\\hat{x}_{L_2}$, $\\hat{x}_{L_1}$, and $\\hat{x}_{\\text{Huber}}$ that minimize their respective cost functions, and then report the numerical value for $\\hat{x}_{\\text{Huber}}$.\n\n**1. The $L_2$ (Least Squares) Estimator**\nThe $L_2$ cost function is given by:\n$$J_2(x) = w_1 (x-y_1)^2 + w_2 (x-y_2)^2$$\nThis is a quadratic function of $x$, and its minimum can be found by setting its first derivative with respect to $x$ to zero.\n$$\\frac{dJ_2(x)}{dx} = 2w_1(x-y_1) + 2w_2(x-y_2) = 0$$\nSolving for $x$:\n$$w_1 x - w_1 y_1 + w_2 x - w_2 y_2 = 0$$\n$$(w_1 + w_2)x = w_1 y_1 + w_2 y_2$$\nThe $L_2$ estimator, $\\hat{x}_{L_2}$, is the weighted average of the observations:\n$$\\hat{x}_{L_2} = \\frac{w_1 y_1 + w_2 y_2}{w_1 + w_2}$$\nSubstituting the given numerical values: $y_1=1.2$, $y_2=9.0$, $w_1=2$, and $w_2=0.5$.\n$$\\hat{x}_{L_2} = \\frac{2(1.2) + 0.5(9.0)}{2 + 0.5} = \\frac{2.4 + 4.5}{2.5} = \\frac{6.9}{2.5} = 2.76$$\n\n**2. The $L_1$ (Least Absolute Deviations) Estimator**\nThe $L_1$ cost function is:\n$$J_1(x) = w_1|x-y_1| + w_2|x-y_2|$$\nThis function is convex but not differentiable at $x=y_1$ and $x=y_2$. The minimizer is the weighted median of the observations. The first-order optimality condition is $0 \\in \\partial J_1(x)$, where $\\partial J_1(x)$ is the subgradient. The derivative of $J_1(x)$, where it exists, is:\n$$\\frac{dJ_1(x)}{dx} = w_1 \\text{sgn}(x-y_1) + w_2 \\text{sgn}(x-y_2)$$\nLet's analyze the derivative based on the value of $x$ relative to $y_1=1.2$ and $y_2=9.0$:\n- For $x  1.2$: $\\frac{dJ_1}{dx} = -w_1 - w_2 = -2 - 0.5 = -2.5  0$. The function is decreasing.\n- For $1.2  x  9.0$: $\\frac{dJ_1}{dx} = w_1 - w_2 = 2 - 0.5 = 1.5  0$. The function is increasing.\n- For $x  9.0$: $\\frac{dJ_1}{dx} = w_1 + w_2 = 2 + 0.5 = 2.5  0$. The function is increasing.\nThe derivative changes sign from negative to positive at $x = y_1 = 1.2$. Thus, the minimum of $J_1(x)$ occurs at this point.\n$$\\hat{x}_{L_1} = y_1 = 1.2$$\n\n**3. The Huber Estimator**\nThe Huber cost function is given by $J_H(x) = w_1 \\rho_{\\delta}(x-y_1) + w_2 \\rho_{\\delta}(x-y_2)$, with the Huber loss function $\\rho_{\\delta}(r)$ defined as:\n$$\\rho_{\\delta}(r) = \\begin{cases} \\frac{1}{2}r^2  \\text{if } |r| \\le \\delta \\\\ \\delta|r| - \\frac{1}{2}\\delta^2  \\text{if } |r|  \\delta \\end{cases}$$\nThis function is convex and continuously differentiable. Its derivative is the Huber score function, $\\psi_{\\delta}(r) = \\rho'_{\\delta}(r)$:\n$$\\psi_{\\delta}(r) = \\begin{cases} r  \\text{if } |r| \\le \\delta \\\\ \\delta \\cdot \\text{sgn}(r)  \\text{if } |r|  \\delta \\end{cases} $$\nThe minimum of $J_H(x)$ is found by setting its derivative to zero:\n$$\\frac{dJ_H(x)}{dx} = w_1 \\psi_{\\delta}(x-y_1) + w_2 \\psi_{\\delta}(x-y_2) = 0$$\nWith the given values $w_1=2$, $w_2=0.5$, $y_1=1.2$, $y_2=9.0$, and $\\delta=1$, the equation becomes:\n$$2 \\cdot \\psi_{1}(x-1.2) + 0.5 \\cdot \\psi_{1}(x-9.0) = 0$$\nBased on the other estimates and the weights, we expect the solution $\\hat{x}_{\\text{Huber}}$ to be close to $y_1=1.2$ but not identical to it. This suggests that the residual for the first observation, $r_1 = x-y_1$, is small, while the residual for the second, $r_2 = x-y_2$, is large. Let us hypothesize that $|x-y_1| \\le \\delta$ and $|x-y_2|  \\delta$.\n- If $|r_1| = |x-1.2| \\le 1$, then $\\psi_{1}(x-1.2) = x-1.2$.\n- If $|r_2| = |x-9.0|  1$, then $\\psi_{1}(x-9.0) = 1 \\cdot \\text{sgn}(x-9.0)$. Since we expect $x$ to be near $1.2$, $x-9.0$ will be negative, so $\\text{sgn}(x-9.0)=-1$.\nSubstituting these into the optimality condition:\n$$2(x-1.2) + 0.5(-1) = 0$$\n$$2x - 2.4 - 0.5 = 0$$\n$$2x = 2.9$$\n$$x = 1.45$$\nWe must now verify if this solution is consistent with our initial hypothesis.\n- For $x=1.45$, the first residual is $r_1 = 1.45 - 1.2 = 0.25$. We check if $|r_1| \\le \\delta$: $|0.25| \\le 1$. This is true.\n- The second residual is $r_2 = 1.45 - 9.0 = -7.55$. We check if $|r_2|  \\delta$: $|-7.55|  1$. This is also true.\nThe hypothesis holds, and thus the Huber estimate is:\n$$\\hat{x}_{\\text{Huber}} = 1.45$$\n\n**Comparison of Estimates**\n- $\\hat{x}_{L_2} = 2.76$: This estimate is strongly influenced by the outlier $y_2=9.0$. The quadratic penalty on the large residual $r_2$ pulls the estimate significantly away from the clean observation $y_1=1.2$, demonstrating the non-robustness of least squares.\n- $\\hat{x}_{L_1} = 1.2$: This estimate, the weighted median, completely discards the outlier information and snaps to the clean observation because its weight is larger. This demonstrates extreme robustness.\n- $\\hat{x}_{\\text{Huber}} = 1.45$: This estimate represents a compromise. It is close to the clean observation $y_1=1.2$, showing robustness to the outlier $y_2=9.0$. However, unlike the $L_1$ estimate, it is still influenced slightly by the outlier, as the linear penalty part of the Huber loss still allows the outlier to have a small, constant gradient contribution. It lies between the $L_1$ and $L_2$ estimates.\n\nThe problem asks for the numerical value of the Huber estimate only.", "answer": "$$\n\\boxed{1.45}\n$$", "id": "3389451"}, {"introduction": "While direct minimization is feasible for simple problems, realistic inverse problems require more sophisticated optimization strategies. This exercise introduces the Alternating Direction Method of Multipliers (ADMM), a powerful framework for solving complex, structured optimization problems by breaking them into smaller, manageable subproblems. You will derive the ADMM update steps for a Huber-based objective, a procedure that illuminates the interplay between the data fidelity term, the model constraints, and the dual variables that enforce them [@problem_id:3389428].", "problem": "Consider a linear inverse problem in which a parameter vector $\\theta \\in \\mathbb{R}^{n}$ is to be estimated from data $b \\in \\mathbb{R}^{m}$ and a known forward operator $A \\in \\mathbb{R}^{m \\times n}$ that maps parameters to predicted data. To achieve robustness against outliers in the data, assume the data misfit is penalized by the Huber loss with threshold $\\delta  0$, defined for each scalar $u \\in \\mathbb{R}$ by\n$$\n\\rho_{\\delta}(u) := \n\\begin{cases}\n\\frac{1}{2} u^{2},  \\text{if } |u| \\le \\delta, \\\\\n\\delta \\left( |u| - \\frac{1}{2} \\delta \\right),  \\text{if } |u|  \\delta.\n\\end{cases}\n$$\nThe robust estimation problem is\n$$\n\\min_{\\theta \\in \\mathbb{R}^{n}} \\sum_{i=1}^{m} \\rho_{\\delta}\\!\\left( \\left( A \\theta - b \\right)_{i} \\right).\n$$\nIntroduce auxiliary residuals $r \\in \\mathbb{R}^{m}$ with the equality constraint $r = A \\theta - b$ and consider solving the constrained problem by the Alternating Direction Method of Multipliers (ADMM). Let $\\mu  0$ be the penalty parameter of the augmented Lagrangian and let the dual variable (Lagrange multiplier) be $y \\in \\mathbb{R}^{m}$. Assume $A^{\\top} A$ is invertible.\n\nStarting only from the definitions above and the standard equality-constrained augmented Lagrangian construction, derive the complete ADMM splitting and closed-form iteration updates for $\\theta$, $r$, and $y$ at iteration $k+1$ in terms of $A$, $b$, $\\delta$, $\\mu$, and the previous iterates $\\theta^{k}$, $r^{k}$, and $y^{k}$. Your derivation should produce an explicit elementwise piecewise expression for the $r$-update that depends on $\\delta$ and $\\mu$, and should present the $\\theta$- and $y$-updates in closed form. Express your final answer as analytic expressions; no numerical evaluation is required.", "solution": "The user wants to solve a problem. The user wants me to be a meticulous and exacting professor in the STEM fields. I will now begin the validation and solution process.\n\n### Step 1: Extract Givens\n-   The problem is a linear inverse problem to estimate a parameter vector $\\theta \\in \\mathbb{R}^{n}$ from data $b \\in \\mathbb{R}^{m}$.\n-   The forward operator is a known matrix $A \\in \\mathbb{R}^{m \\times n}$.\n-   The data misfit is penalized by the Huber loss, $\\rho_{\\delta}(u)$, defined for a scalar $u \\in \\mathbb{R}$ and a threshold $\\delta  0$ as:\n    $$\n    \\rho_{\\delta}(u) := \n    \\begin{cases}\n    \\frac{1}{2} u^{2},  \\text{if } |u| \\le \\delta, \\\\\n    \\delta \\left( |u| - \\frac{1}{2} \\delta \\right),  \\text{if } |u|  \\delta.\n    \\end{cases}\n    $$\n-   The optimization problem is:\n    $$\n    \\min_{\\theta \\in \\mathbb{R}^{n}} \\sum_{i=1}^{m} \\rho_{\\delta}\\!\\left( \\left( A \\theta - b \\right)_{i} \\right).\n    $$\n-   An auxiliary residual vector $r \\in \\mathbb{R}^{m}$ is introduced with the equality constraint $r = A \\theta - b$.\n-   The problem is to be solved using the Alternating Direction Method of Multipliers (ADMM).\n-   The ADMM penalty parameter is $\\mu  0$.\n-   The dual variable (Lagrange multiplier) is $y \\in \\mathbb{R}^{m}$.\n-   It is assumed that the matrix $A^{\\top} A$ is invertible.\n-   The starting point for the derivation is the standard equality-constrained augmented Lagrangian.\n-   The desired output is the complete ADMM splitting and closed-form iteration updates for $\\theta^{k+1}$, $r^{k+1}$, and $y^{k+1}$ in terms of $A$, $b$, $\\delta$, $\\mu$, and the previous iterates $\\theta^{k}$, $r^{k}$, and $y^{k}$.\n-   The $r$-update must be an explicit elementwise piecewise expression.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is critically evaluated.\n\n-   **Scientifically Grounded**: The problem is a standard formulation for robust regression and inverse problems. The Huber loss is a cornerstone of robust statistics, and ADMM is a widely-used, powerful optimization algorithm. The entire setup is firmly rooted in established principles of numerical optimization and statistics. The problem is scientifically sound.\n-   **Well-Posed**: The problem is well-defined. The objective function, being a sum of convex Huber loss functions, is convex. The constraint is affine. This forms a convex optimization problem. The assumption that $A^{\\top} A$ is invertible guarantees that the least-squares subproblem for $\\theta$ has a unique solution. The problem is well-posed.\n-   **Objective**: The language is precise, formal, and mathematical. All terms are unambiguously defined. There are no subjective or opinion-based statements. The problem is objective.\n-   **Completeness and Consistency**: All necessary information (functions, variables, constraints, and assumptions) is provided to derive the ADMM iterations. The setup is internally consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with the derivation of the solution.\n\nThe problem is first rewritten using the auxiliary variable $r$ as a constrained optimization problem:\n$$\n\\min_{\\theta, r} \\sum_{i=1}^{m} \\rho_{\\delta}(r_i) \\quad \\text{subject to} \\quad A\\theta - b - r = 0.\n$$\nThe augmented Lagrangian $L_{\\mu}(\\theta, r, y)$ for this problem is constructed by adding the dual term and a quadratic penalty for the constraint violation:\n$$\nL_{\\mu}(\\theta, r, y) = \\sum_{i=1}^{m} \\rho_{\\delta}(r_i) + y^{\\top}(A\\theta - b - r) + \\frac{\\mu}{2} \\|A\\theta - b - r\\|_{2}^{2}.\n$$\nThe ADMM algorithm performs a sequence of minimizations over $\\theta$ and $r$, followed by an update of the dual variable $y$. The updates at iteration $k+1$ are given by:\n1.  $\\theta^{k+1} = \\arg\\min_{\\theta} L_{\\mu}(\\theta, r^{k}, y^{k})$\n2.  $r^{k+1} = \\arg\\min_{r} L_{\\mu}(\\theta^{k+1}, r, y^{k})$\n3.  $y^{k+1} = y^k + \\mu(A\\theta^{k+1} - b - r^{k+1})$\n\nWe now derive the closed-form expression for each step.\n\n**1. $\\theta$-Update**\nTo find $\\theta^{k+1}$, we minimize $L_{\\mu}(\\theta, r^{k}, y^{k})$ with respect to $\\theta$. We only need to consider terms that depend on $\\theta$:\n$$\n\\theta^{k+1} = \\arg\\min_{\\theta} \\left( (y^k)^{\\top}(A\\theta - b - r^k) + \\frac{\\mu}{2} \\|A\\theta - b - r^k\\|_{2}^{2} \\right).\n$$\nThis is a standard least-squares problem. The minimum is found by setting the gradient with respect to $\\theta$ to zero. The gradient is:\n$$\n\\nabla_{\\theta} L_{\\mu}(\\theta, r^k, y^k) = A^{\\top}y^k + \\mu A^{\\top}(A\\theta - b - r^k).\n$$\nSetting the gradient to zero at $\\theta = \\theta^{k+1}$:\n$$\nA^{\\top}y^k + \\mu A^{\\top}(A\\theta^{k+1} - b - r^k) = 0\n$$\n$$\n\\mu A^{\\top}A\\theta^{k+1} = \\mu A^{\\top}(b + r^k) - A^{\\top}y^k\n$$\nDividing by $\\mu$ and rearranging:\n$$\nA^{\\top}A\\theta^{k+1} = A^{\\top}(r^k + b - \\frac{1}{\\mu}y^k).\n$$\nSince $A^{\\top} A$ is assumed to be invertible, we can solve for $\\theta^{k+1}$:\n$$\n\\theta^{k+1} = (A^{\\top} A)^{-1} A^{\\top} \\left(r^k + b - \\frac{1}{\\mu}y^k\\right).\n$$\n\n**2. $r$-Update**\nTo find $r^{k+1}$, we minimize $L_{\\mu}(\\theta^{k+1}, r, y^{k})$ with respect to $r$.\n$$\nr^{k+1} = \\arg\\min_{r} \\left( \\sum_{i=1}^{m} \\rho_{\\delta}(r_i) - (y^k)^{\\top}r + \\frac{\\mu}{2} \\|A\\theta^{k+1} - b - r\\|_{2}^{2} \\right).\n$$\nThis minimization problem is separable with respect to the components $r_i$ of the vector $r$. We can solve for each component $r_i$ independently:\n$$\nr_i^{k+1} = \\arg\\min_{r_i} \\left( \\rho_{\\delta}(r_i) - y_i^k r_i + \\frac{\\mu}{2} \\left( (A\\theta^{k+1} - b)_i - r_i \\right)^{2} \\right).\n$$\nLet's complete the square for the terms involving $r_i$. The objective for $r_i$ is equivalent to minimizing:\n$$\n\\rho_{\\delta}(r_i) + \\frac{\\mu}{2} \\left\\| r_i - \\left( (A\\theta^{k+1} - b)_i + \\frac{y_i^k}{\\mu} \\right) \\right\\|_{2}^{2}.\n$$\nLet $c_i = (A\\theta^{k+1} - b)_i + \\frac{y_i^k}{\\mu}$. The problem for each $r_i$ is to solve $\\min_{r_i} \\left(\\rho_{\\delta}(r_i) + \\frac{\\mu}{2}(r_i - c_i)^2\\right)$. This is the proximal operator of the function $\\frac{1}{\\mu}\\rho_\\delta$ applied to $c_i$.\nThe function $\\rho_\\delta(u)$ is convex and continuously differentiable. Its derivative is $\\rho'_{\\delta}(u) = \\text{sat}_{[-\\delta, \\delta]}(u) = \\min(\\delta, \\max(-\\delta, u))$.\nThe optimality condition is found by setting the derivative of the objective with respect to $r_i$ to zero:\n$$\n\\rho'_{\\delta}(r_i) + \\mu(r_i - c_i) = 0 \\implies \\mu(c_i - r_i) = \\rho'_{\\delta}(r_i).\n$$\nWe analyze this equation in three cases based on the value of the solution $r_i$:\n-   Case 1: $|r_i| \\le \\delta$. Then $\\rho'_{\\delta}(r_i) = r_i$. The condition becomes $\\mu(c_i - r_i) = r_i$, which gives $(\\mu+1)r_i = \\mu c_i$, so $r_i = \\frac{\\mu}{\\mu+1}c_i$. This solution is valid if $|\\frac{\\mu}{\\mu+1}c_i| \\le \\delta$, which is equivalent to $|c_i| \\le \\delta\\frac{\\mu+1}{\\mu} = \\delta(1+\\frac{1}{\\mu})$.\n-   Case 2: $r_i  \\delta$. Then $\\rho'_{\\delta}(r_i) = \\delta$. The condition becomes $\\mu(c_i - r_i) = \\delta$, which gives $r_i = c_i - \\frac{\\delta}{\\mu}$. This solution is valid if $r_i  \\delta$, which means $c_i - \\frac{\\delta}{\\mu}  \\delta$, or $c_i  \\delta(1+\\frac{1}{\\mu})$.\n-   Case 3: $r_i  -\\delta$. Then $\\rho'_{\\delta}(r_i) = -\\delta$. The condition becomes $\\mu(c_i - r_i) = -\\delta$, which gives $r_i = c_i + \\frac{\\delta}{\\mu}$. This solution is valid if $r_i  -\\delta$, which means $c_i + \\frac{\\delta}{\\mu}  -\\delta$, or $c_i  -\\delta(1+\\frac{1}{\\mu})$.\n\nCombining these cases, the update for each component of $r^{k+1}$ is given by the following piecewise expression, where $c_i^{k+1} = (A\\theta^{k+1} - b)_i + \\frac{1}{\\mu}y_i^k$:\n$$\nr_i^{k+1} = \n\\begin{cases}\nc_i^{k+1} - \\frac{\\delta}{\\mu}  \\text{if } c_i^{k+1}  \\delta\\left(1+\\frac{1}{\\mu}\\right) \\\\\n\\frac{\\mu}{1+\\mu} c_i^{k+1}  \\text{if } |c_i^{k+1}| \\le \\delta\\left(1+\\frac{1}{\\mu}\\right) \\\\\nc_i^{k+1} + \\frac{\\delta}{\\mu}  \\text{if } c_i^{k+1}  -\\delta\\left(1+\\frac{1}{\\mu}\\right)\n\\end{cases}\n$$\n\n**3. $y$-Update**\nThe dual variable update is a standard gradient ascent step on the dual problem. For the constraint $A\\theta - b - r = 0$, the update is:\n$$\ny^{k+1} = y^k + \\mu (A\\theta^{k+1} - b - r^{k+1}).\n$$\nHere, $(A\\theta^{k+1} - b - r^{k+1})$ is the primal residual at iteration $k+1$.\n\nThe three derived steps constitute the complete ADMM iteration for the given problem.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\theta^{k+1} = (A^{\\top} A)^{-1} A^{\\top} (r^k + b - \\frac{1}{\\mu} y^k)\n\n(r^{k+1})_i = \n\\begin{cases}\nc_i - \\frac{\\delta}{\\mu}  \\text{if } c_i  \\delta(1+\\frac{1}{\\mu}) \\\\\n\\frac{\\mu}{1+\\mu} c_i  \\text{if } |c_i| \\le \\delta(1+\\frac{1}{\\mu}) \\\\\nc_i + \\frac{\\delta}{\\mu}  \\text{if } c_i  -\\delta(1+\\frac{1}{\\mu})\n\\end{cases}\n\\text{ for } i=1,\\dots,m, \\text{ where } c_i \\equiv (A\\theta^{k+1} - b + \\frac{1}{\\mu}y^k)_i\n\ny^{k+1} = y^k + \\mu (A\\theta^{k+1} - r^{k+1} - b)\n\\end{pmatrix}\n}\n$$", "id": "3389428"}, {"introduction": "Transitioning from analytical derivations to reliable computational tools is a critical step in data assimilation. This practice focuses on the implementation and verification of the Huber loss gradient within an adjoint modeling framework, which is standard for large-scale inverse problems. You will not only implement the adjoint-based gradient but also develop a robust gradient-checking procedure to verify its correctness, ensuring that the code accurately reflects the underlying mathematics, especially the crucial \"saturation\" behavior of the Huber score function [@problem_id:3389414].", "problem": "Consider a linear observation operator in inverse problems and data assimilation, where the unknown state vector is $x \\in \\mathbb{R}^n$, the observation vector is $y \\in \\mathbb{R}^m$, and the forward operator is the matrix $A \\in \\mathbb{R}^{m \\times n}$. The observation-model residual is defined by $r(x) = A x - y$. To achieve robustness to outliers, the data misfit is measured using the Huber loss with threshold $\\delta  0$, defined by the function $\\rho_{\\delta} : \\mathbb{R} \\to \\mathbb{R}$ that coincides with a quadratic loss for small residuals and transitions to a linear loss for large residuals. The total objective is $J(x) = \\sum_{i=1}^m \\rho_{\\delta}(r_i(x))$, where $r_i(x)$ denotes the $i$-th component of the residual.\n\nYour task is to implement an adjoint-based gradient of $J(x)$ with respect to $x$ using the chain rule starting from the definition of $r(x)$ and the Huber loss $\\rho_{\\delta}$, and to verify its correctness via a robust gradient-checking procedure based on central finite differences. In addition, you must explicitly verify the saturation behavior expected of the derivative of the Huber loss with respect to residual components: for residual components satisfying $|r_i(x)|  \\delta$, the corresponding derivative with respect to $r_i$ must take the constant magnitude $\\delta$ with sign matching $\\operatorname{sign}(r_i)$; for residual components with $|r_i(x)| \\le \\delta$, the derivative should coincide with the residual itself. Your program must evaluate these properties on synthetic cases that include moderate residuals, boundary residuals exactly at $\\pm \\delta$, and severe outliers.\n\nStarting from the fundamental definitions of residual $r(x) = A x - y$ and Huber loss $\\rho_{\\delta}$ (quadratic inside the threshold and linear outside), derive and implement:\n- The adjoint gradient of $J(x)$ with respect to $x$ via the chain rule, expressed in terms of $A$ and the derivative of the Huber loss with respect to $r$.\n- A central-difference gradient checker using step size $\\varepsilon  0$ that computes the finite-difference approximation of each component of the gradient of $J(x)$.\n- A saturation verifier that, for each residual component $r_i(x)$, checks whether the derivative with respect to $r_i$ saturates at $\\pm \\delta$ when $|r_i(x)|  \\delta$, equals $r_i(x)$ when $|r_i(x)|  \\delta$, and equals $\\pm \\delta$ at the boundary when $r_i(x) = \\pm \\delta$.\n\nNumerically decide gradient correctness by requiring that the maximum absolute difference between the adjoint gradient and the central-difference gradient is less than a given tolerance $\\tau  0$. The saturation verifier should use a strict numerical check for equality at the boundary and a tolerance-based check elsewhere as appropriate.\n\nImplement the following test suite, computing the boolean result for each case as the logical conjunction of:\n- the gradient check passing under the tolerance criterion, and\n- all saturation checks passing for the residual components.\n\nTest cases:\n1. Happy path with identity forward operator:\n   - Dimensions: $n = 5$, $m = 5$.\n   - $A = I_5$ (the $5 \\times 5$ identity matrix).\n   - $x = [0.0, 1.0, -2.0, 0.5, -0.5]$.\n   - $y = [0.0, -1.0, 10.0, 0.4, -0.9]$.\n   - $\\delta = 0.7$.\n   - Finite-difference step: $\\varepsilon = 10^{-6}$.\n   - Gradient-check tolerance: $\\tau = 10^{-7}$.\n\n2. Boundary case at the kink:\n   - Dimensions: $n = 5$, $m = 5$.\n   - $A = I_5$.\n   - Choose residuals $r = [0.5, -0.5, 0.4999999, -0.5000001, 10.0]$ and let $x = [0, 0, 0, 0, 0]$, $y = -r$ so that $r(x) = A x - y = r$ exactly.\n   - $\\delta = 0.5$.\n   - $\\varepsilon = 10^{-8}$.\n   - $\\tau = 10^{-6}$.\n\n3. General non-identity forward operator:\n   - Dimensions: $n = 4$, $m = 4$.\n   - $A = \\begin{bmatrix} 1.0  0.2  0.0  -0.1 \\\\ 0.0  1.5  -0.5  0.0 \\\\ 0.0  0.0  -1.0  2.0 \\\\ 0.3  0.0  0.0  0.7 \\end{bmatrix}$.\n   - $x = [0.2, -0.3, 1.0, -1.2]$.\n   - $y = [2.0, -1.0, 10.0, 0.0]$.\n   - $\\delta = 1.0$.\n   - $\\varepsilon = 10^{-6}$.\n   - $\\tau = 10^{-6}$.\n\nFor numerical checks involving equalities at the boundary $r_i(x) = \\pm \\delta$ in Test Case $2$, treat equality exactly as defined (no tolerance), and for the other saturation checks, use a small numerical tolerance of $10^{-12}$ to account for floating point arithmetic.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is a boolean indicating whether both the gradient-check and saturation-check conditions passed for the corresponding test case. No external input is allowed, and no physical units appear in this problem; angles are not used and thus require no unit specification. The final output must strictly follow the specified format in a single print statement.", "solution": "The problem requires the derivation and implementation of the adjoint-based gradient for an objective function based on the Huber loss, followed by a numerical verification of its correctness and saturation properties. The solution proceeds in three stages: mathematical derivation, description of the numerical verification procedures, and implementation details.\n\nFirst, we formalize the problem. The state vector is $x \\in \\mathbb{R}^n$, the observation vector is $y \\in \\mathbb{R}^m$, and the linear forward operator is $A \\in \\mathbb{R}^{m \\times n}$. The residual vector is defined as $r(x) = A x - y$. The objective function $J(x)$ is the sum of Huber losses applied to each component of the residual:\n$$ J(x) = \\sum_{i=1}^m \\rho_{\\delta}(r_i(x)) $$\nThe Huber loss function $\\rho_{\\delta}(z)$ with threshold $\\delta  0$ provides a transition between a quadratic ($L_2$) loss for small values and a linear ($L_1$) loss for large values, enhancing robustness to outliers. Its standard definition, which ensures continuity of the function and its first derivative, is:\n$$ \\rho_{\\delta}(z) = \\begin{cases} \\frac{1}{2} z^2  \\text{if } |z| \\le \\delta \\\\ \\delta |z| - \\frac{1}{2} \\delta^2  \\text{if } |z|  \\delta \\end{cases} $$\nThe derivative of the Huber loss with respect to its argument, denoted $\\psi(z) = \\frac{d\\rho_{\\delta}}{dz}(z)$, is a key component for gradient computation. It is given by:\n$$ \\psi(z) = \\begin{cases} z  \\text{if } |z| \\le \\delta \\\\ \\delta \\cdot \\operatorname{sign}(z)  \\text{if } |z|  \\delta \\end{cases} $$\nThis function is also known as a soft-thresholding or shrinkage function.\n\nThe primary task is to find the gradient of $J(x)$ with respect to $x$, denoted $\\nabla_x J(x)$. We apply the chain rule. The $j$-th component of the gradient is:\n$$ (\\nabla_x J(x))_j = \\frac{\\partial J}{\\partial x_j} = \\sum_{i=1}^m \\frac{d \\rho_{\\delta}}{d r_i}(r_i(x)) \\frac{\\partial r_i}{\\partial x_j} $$\nThe partial derivative of the $i$-th component of the residual, $r_i(x) = (Ax - y)_i = \\sum_{k=1}^n A_{ik} x_k - y_i$, with respect to $x_j$ is:\n$$ \\frac{\\partial r_i}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\sum_{k=1}^n A_{ik} x_k - y_i \\right) = \\sum_{k=1}^n A_{ik} \\frac{\\partial x_k}{\\partial x_j} = \\sum_{k=1}^n A_{ik} \\delta_{kj} = A_{ij} $$\nwhere $\\delta_{kj}$ is the Kronecker delta.\nSubstituting this into the gradient expression and using the definition of $\\psi$, we have:\n$$ (\\nabla_x J(x))_j = \\sum_{i=1}^m \\psi(r_i(x)) A_{ij} = \\sum_{i=1}^m (A^T)_{ji} \\psi(r_i(x)) $$\nThis equation represents the $j$-th component of the matrix-vector product $A^T \\psi(r(x))$, where $\\psi(r(x))$ is a vector whose components are $\\psi(r_i(x))$. Therefore, the full gradient vector is:\n$$ \\nabla_x J(x) = A^T \\psi(A x - y) $$\nThis is the adjoint-based gradient. It is computationally efficient as it requires one matrix-vector product with $A$ to compute the residual, followed by applying the element-wise function $\\psi$, and then one matrix-vector product with the transpose (adjoint) matrix $A^T$.\n\nTo verify the correctness of this analytically derived gradient, we employ a central finite-difference approximation. For each component $j \\in \\{1, \\dots, n\\}$, the partial derivative $\\frac{\\partial J}{\\partial x_j}$ is approximated as:\n$$ (\\nabla_x J(x))_j \\approx \\frac{J(x + \\varepsilon e_j) - J(x - \\varepsilon e_j)}{2\\varepsilon} $$\nwhere $e_j$ is the $j$-th canonical basis vector and $\\varepsilon$ is a small step size. A robust implementation requires evaluating the full objective function $J$ twice for each component of the gradient. The check for gradient correctness consists of comparing the adjoint-based gradient with this numerical approximation. The check passes if the maximum absolute difference between the two gradient vectors is below a specified tolerance $\\tau$:\n$$ \\max_{j} \\left| (\\nabla_x J(x))_{\\text{adjoint}, j} - (\\nabla_x J(x))_{\\text{fd}, j} \\right|  \\tau $$\nEven though the derivative of $J(x)$ is not defined at points where $|r_i(x)| = \\delta$ for some $i$, the central difference approximation remains well-behaved and provides a good estimate of a valid subgradient, which in our formulation corresponds to the analytical gradient.\n\nFinally, we must verify the saturation behavior of the Huber derivative $\\psi(r_i)$. This involves checking if the computed values of $\\psi(r_i)$ conform to its piecewise definition for all residual components $r_i$. The checks are:\n1. For residual components in the linear region, $|r_i|  \\delta$, we verify that $\\psi(r_i)$ equals $\\delta \\cdot \\operatorname{sign}(r_i)$, within a small numerical tolerance of $10^{-12}$.\n2. For residual components in the quadratic region, $|r_i|  \\delta$, we verify that $\\psi(r_i)$ equals $r_i$, within the same numerical tolerance.\n3. For residual components exactly at the boundary, $|r_i| = \\delta$, we perform a strict equality check that $\\psi(r_i)$ equals $r_i$ (i.e., $\\pm\\delta$). This is valid because our definition of $\\psi(z)$ is continuous and includes the endpoints in the quadratic case where $\\psi(z)=z$.\n\nThe overall result for each test case is a boolean value representing the logical conjunction of the gradient check passing and all saturation checks passing, thus providing a comprehensive validation of the implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# language: Python\n# version: 3.12\n# libraries:\n#     - name: numpy\n#       version: 1.23.5\n#     - name: scipy\n#       version: 1.11.4\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving, implementing, and verifying the adjoint-based gradient \n    of a Huber loss objective function for a set of predefined test cases.\n    \"\"\"\n\n    def huber_objective(x_vec, A_mat, y_vec, delta):\n        \"\"\"Computes the total Huber loss J(x).\"\"\"\n        residuals = A_mat @ x_vec - y_vec\n        loss = 0.0\n        for r_i in residuals:\n            abs_r_i = np.abs(r_i)\n            if abs_r_i = delta:\n                loss += 0.5 * r_i**2\n            else:\n                loss += delta * abs_r_i - 0.5 * delta**2\n        return loss\n\n    def huber_derivative(residuals, delta):\n        \"\"\"Computes the derivative of the Huber loss with respect to residuals, psi(r).\"\"\"\n        psi = np.zeros_like(residuals, dtype=float)\n        for i, r_i in enumerate(residuals):\n            if np.abs(r_i) = delta:\n                psi[i] = r_i\n            else:\n                psi[i] = delta * np.sign(r_i)\n        return psi\n\n    def run_test_case(params):\n        \"\"\"\n        Runs a single test case, performing adjoint gradient calculation, saturation checks, \n        and finite-difference gradient verification.\n        \"\"\"\n        A = params[\"A\"]\n        x = params[\"x\"]\n        y = params[\"y\"]\n        delta = params[\"delta\"]\n        eps = params[\"eps\"]\n        tau = params[\"tau\"]\n        sat_tol = 1e-12\n\n        # 1. Adjoint-based gradient calculation\n        residuals = A @ x - y\n        psi_r = huber_derivative(residuals, delta)\n        grad_adj = A.T @ psi_r\n\n        # 2. Saturation verifier\n        saturation_ok = True\n        for i in range(len(residuals)):\n            r_i = residuals[i]\n            psi_i = psi_r[i]\n            \n            check_passed = False\n            # Strict equality check for boundary cases, as required\n            if r_i == delta:\n                if psi_i == delta:\n                    check_passed = True\n            elif r_i == -delta:\n                if psi_i == -delta:\n                    check_passed = True\n            # Tolerance-based checks for non-boundary cases\n            elif np.abs(r_i)  delta:\n                if np.abs(psi_i - delta * np.sign(r_i))  sat_tol:\n                    check_passed = True\n            elif np.abs(r_i)  delta:\n                if np.abs(psi_i - r_i)  sat_tol:\n                    check_passed = True\n            \n            if not check_passed:\n                saturation_ok = False\n                break\n        \n        # 3. Central finite-difference gradient checker\n        n = len(x)\n        grad_fd = np.zeros(n)\n        for j in range(n):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[j] += eps\n            x_minus[j] -= eps\n            \n            J_plus = huber_objective(x_plus, A, y, delta)\n            J_minus = huber_objective(x_minus, A, y, delta)\n            \n            grad_fd[j] = (J_plus - J_minus) / (2 * eps)\n            \n        # 4. Gradient correctness check\n        max_abs_diff = np.max(np.abs(grad_adj - grad_fd))\n        gradient_ok = max_abs_diff  tau\n        \n        # 5. Final result is a conjunction of both checks\n        return gradient_ok and saturation_ok\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.identity(5),\n            \"x\": np.array([0.0, 1.0, -2.0, 0.5, -0.5]),\n            \"y\": np.array([0.0, -1.0, 10.0, 0.4, -0.9]),\n            \"delta\": 0.7,\n            \"eps\": 1e-6,\n            \"tau\": 1e-7,\n        },\n        {\n            \"A\": np.identity(5),\n            \"x\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"y\": -np.array([0.5, -0.5, 0.4999999, -0.5000001, 10.0]),\n            \"delta\": 0.5,\n            \"eps\": 1e-8,\n            \"tau\": 1e-6,\n        },\n        {\n            \"A\": np.array([\n                [1.0, 0.2, 0.0, -0.1],\n                [0.0, 1.5, -0.5, 0.0],\n                [0.0, 0.0, -1.0, 2.0],\n                [0.3, 0.0, 0.0, 0.7]\n            ]),\n            \"x\": np.array([0.2, -0.3, 1.0, -1.2]),\n            \"y\": np.array([2.0, -1.0, 10.0, 0.0]),\n            \"delta\": 1.0,\n            \"eps\": 1e-6,\n            \"tau\": 1e-6,\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(case)\n        results.append(str(result).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solution\nsolve()\n```", "id": "3389414"}]}