{"hands_on_practices": [{"introduction": "The Bregman iteration is built upon the concept of the Bregman distance. Before we can use the method, we must first build an intuition for this core component. This foundational exercise ([@problem_id:3369778]) provides a direct, hands-on calculation of the Bregman distance for the Total Variation (TV) functional, a cornerstone of modern signal processing. By working through the definitions of the TV functional, its subgradient, and the Bregman distance formula, you will solidify your understanding of these abstract concepts in a concrete numerical setting.", "problem": "Consider discrete one-dimensional signals of length $n=5$, with $u,v \\in \\mathbb{R}^{5}$ given by\n$$\nu=\\begin{pmatrix}2\\\\0\\\\4\\\\2\\\\1\\end{pmatrix}, \\qquad v=\\begin{pmatrix}0\\\\2\\\\5\\\\1\\\\3\\end{pmatrix}.\n$$\nDefine the anisotropic total variation (TV) of a discrete signal $w \\in \\mathbb{R}^{5}$ by the convex functional\n$$\n\\mathrm{TV}(w)=\\sum_{i=1}^{4} |w_{i+1}-w_{i}|.\n$$\nLet $D:\\mathbb{R}^{5}\\to\\mathbb{R}^{4}$ be the forward finite-difference operator defined by $(Dw)_{i}=w_{i+1}-w_{i}$ for $i=1,2,3,4$. For a convex functional $J$, the Bregman distance with respect to a subgradient $p \\in \\partial J(v)$ is defined by\n$$\nD_{J}^{p}(u,v)=J(u)-J(v)-\\langle p, u-v\\rangle,\n$$\nwhere $\\langle \\cdot,\\cdot\\rangle$ denotes the standard Euclidean inner product. Use the characterization of the subdifferential of $\\mathrm{TV}$ via the chain rule for subgradients: there exists $s\\in\\mathbb{R}^{4}$ with $s_{i}\\in \\operatorname{sign}\\big((Dv)_{i}\\big)$ such that $p=D^{\\top}s$, where $D^{\\top}$ is the adjoint of $D$ under the Euclidean inner product. Here $\\operatorname{sign}(t)=1$ if $t>0$, $\\operatorname{sign}(t)=-1$ if $t<0$, and $\\operatorname{sign}(0)=[-1,1]$.\n\nCompute the Bregman distance $D_{\\mathrm{TV}}^{p}(u,v)$ using the choice $s_{i}=\\operatorname{sign}\\big((Dv)_{i}\\big)$ for $i=1,2,3,4$ and $p=D^{\\top}s$. Your final answer must be a single real number. No rounding is required.", "solution": "The problem asks for the computation of the Bregman distance $D_{\\mathrm{TV}}^{p}(u,v)$ for given discrete signals $u,v \\in \\mathbb{R}^{5}$. The formula for the Bregman distance with respect to a convex functional $J$ and a subgradient $p \\in \\partial J(v)$ is given by:\n$$\nD_{J}^{p}(u,v) = J(u) - J(v) - \\langle p, u-v \\rangle\n$$\nIn this problem, the functional $J$ is the anisotropic total variation, $J=\\mathrm{TV}$. Thus, we need to compute:\n$$\nD_{\\mathrm{TV}}^{p}(u,v) = \\mathrm{TV}(u) - \\mathrm{TV}(v) - \\langle p, u-v \\rangle\n$$\nWe will compute each term separately. The vectors are given as $u=\\begin{pmatrix}2\\\\0\\\\4\\\\2\\\\1\\end{pmatrix}$ and $v=\\begin{pmatrix}0\\\\2\\\\5\\\\1\\\\3\\end{pmatrix}$.\n\nFirst, we calculate the total variation of $u$. The definition is $\\mathrm{TV}(w) = \\sum_{i=1}^{4} |w_{i+1}-w_{i}|$.\nFor $u$, the differences are:\n$u_2 - u_1 = 0-2 = -2$\n$u_3 - u_2 = 4-0 = 4$\n$u_4 - u_3 = 2-4 = -2$\n$u_5 - u_4 = 1-2 = -1$\nThe total variation of $u$ is the sum of the absolute values of these differences:\n$$\n\\mathrm{TV}(u) = |-2| + |4| + |-2| + |-1| = 2 + 4 + 2 + 1 = 9\n$$\n\nSecond, we calculate the total variation of $v$. For $v$, the differences are:\n$v_2 - v_1 = 2-0 = 2$\n$v_3 - v_2 = 5-2 = 3$\n$v_4 - v_3 = 1-5 = -4$\n$v_5 - v_4 = 3-1 = 2$\nThe total variation of $v$ is:\n$$\n\\mathrm{TV}(v) = |2| + |3| + |-4| + |2| = 2 + 3 + 4 + 2 = 11\n$$\n\nThird, we compute the subgradient $p \\in \\partial \\mathrm{TV}(v)$. The problem states that $p = D^{\\top}s$, where $s \\in \\mathbb{R}^4$ is determined by the specific choice $s_{i}=\\operatorname{sign}\\big((Dv)_{i}\\big)$. The operator $D:\\mathbb{R}^{5}\\to\\mathbb{R}^{4}$ is the forward finite-difference operator, $(Dw)_i = w_{i+1}-w_i$.\nThe vector $Dv$ contains the differences we computed for $\\mathrm{TV}(v)$:\n$$\nDv = \\begin{pmatrix} v_2 - v_1 \\\\ v_3 - v_2 \\\\ v_4 - v_3 \\\\ v_5 - v_4 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\\\ -4 \\\\ 2 \\end{pmatrix}\n$$\nNow, we find the vector $s$ by taking the sign of each component of $Dv$. The sign function is defined as $\\operatorname{sign}(t)=1$ for $t>0$ and $\\operatorname{sign}(t)=-1$ for $t<0$. Since no component of $Dv$ is zero, we have a unique $s$:\n$$\ns = \\begin{pmatrix} \\operatorname{sign}(2) \\\\ \\operatorname{sign}(3) \\\\ \\operatorname{sign}(-4) \\\\ \\operatorname{sign}(2) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{pmatrix}\n$$\nNext, we determine the action of the adjoint operator $D^{\\top}:\\mathbb{R}^4 \\to \\mathbb{R}^5$. The defining property of the adjoint is $\\langle Dw, x \\rangle = \\langle w, D^{\\top}x \\rangle$ for all $w \\in \\mathbb{R}^5, x \\in \\mathbb{R}^4$.\n$$\n\\langle Dw, x \\rangle = \\sum_{i=1}^{4} (w_{i+1}-w_i)x_i = \\sum_{i=1}^{4} w_{i+1}x_i - \\sum_{i=1}^{4} w_i x_i\n$$\nBy re-indexing and grouping terms by $w_i$, we get:\n$$\n\\langle Dw, x \\rangle = -w_1 x_1 + w_2(x_1-x_2) + w_3(x_2-x_3) + w_4(x_3-x_4) + w_5 x_4\n$$\nFrom this, we can identify the components of $D^{\\top}x$:\n$(D^{\\top}x)_1 = -x_1$\n$(D^{\\top}x)_2 = x_1 - x_2$\n$(D^{\\top}x)_3 = x_2 - x_3$\n$(D^{\\top}x)_4 = x_3 - x_4$\n$(D^{\\top}x)_5 = x_4$\nApplying this to our vector $s = (1, 1, -1, 1)^{\\top}$, we find the subgradient $p=D^{\\top}s$:\n$p_1 = -s_1 = -1$\n$p_2 = s_1 - s_2 = 1 - 1 = 0$\n$p_3 = s_2 - s_3 = 1 - (-1) = 2$\n$p_4 = s_3 - s_4 = -1 - 1 = -2$\n$p_5 = s_4 = 1$\nSo, the subgradient is $p = \\begin{pmatrix} -1 \\\\ 0 \\\\ 2 \\\\ -2 \\\\ 1 \\end{pmatrix}$.\n\nFourth, we calculate the inner product $\\langle p, u-v \\rangle$. We first find the vector difference $u-v$:\n$$\nu-v = \\begin{pmatrix} 2-0 \\\\ 0-2 \\\\ 4-5 \\\\ 2-1 \\\\ 1-3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\\\ -1 \\\\ 1 \\\\ -2 \\end{pmatrix}\n$$\nNow we compute the inner product:\n$$\n\\langle p, u-v \\rangle = p_1(u_1-v_1) + p_2(u_2-v_2) + p_3(u_3-v_3) + p_4(u_4-v_4) + p_5(u_5-v_5)\n$$\n$$\n\\langle p, u-v \\rangle = (-1)(2) + (0)(-2) + (2)(-1) + (-2)(1) + (1)(-2) = -2 + 0 - 2 - 2 - 2 = -8\n$$\n\nFinally, we substitute all the calculated values back into the Bregman distance formula:\n$$\nD_{\\mathrm{TV}}^{p}(u,v) = \\mathrm{TV}(u) - \\mathrm{TV}(v) - \\langle p, u-v \\rangle = 9 - 11 - (-8)\n$$\n$$\nD_{\\mathrm{TV}}^{p}(u,v) = -2 - (-8) = -2 + 8 = 6\n$$\nThe resulting Bregman distance is a non-negative value, as expected from its definition for a convex functional.", "answer": "$$\n\\boxed{6}\n$$", "id": "3369778"}, {"introduction": "With a grasp of the Bregman distance, we can now explore the dynamics of the iteration itself. To see the mechanism clearly, it helps to strip away the complexity of high-dimensional problems and focus on the simplest possible case. This practice ([@problem_id:3369784]) presents a scalar data assimilation problem, allowing you to derive and implement two Bregman iterations from first principles. By observing the evolution of the state variable, you will gain a tangible understanding of how the algorithm iteratively balances fidelity to a prior belief against the enforcement of a measurement constraint.", "problem": "Consider a one-dimensional static data assimilation problem. The unknown state is a scalar $x \\in \\mathbb{R}$. The observation operator is a scalar $H \\in \\mathbb{R}$ and the observation is a scalar $y \\in \\mathbb{R}$. The prior information for the state is encoded by a convex quadratic functional $\\phi(x) = \\tfrac{\\alpha}{2}(x - x_b)^2$ where $\\alpha \\in \\mathbb{R}_{>0}$ is a prior weight and $x_b \\in \\mathbb{R}$ is the prior mean. The assimilation is posed as the convex optimization problem of minimizing $\\phi(x)$ subject to the equality constraint $H x = y$.\n\nStarting from first principles of convex analysis and equality-constrained optimization, derive the two-step Bregman iterative scheme that enforces the constraint $H x = y$ while respecting the prior $\\phi(x)$. Begin from the foundational definition of the Bregman iteration for minimizing a convex functional $\\phi(x)$ subject to a linear equality constraint and use differentiation rules valid for twice differentiable convex functions to obtain the scalar update for $x$ and the corresponding update for the dual variable. You must not use or assume any specialized shortcut formulas beyond these bases. Define $x^{(0)}$ as the minimizer of $\\phi(x)$ and initialize the dual variable consistently with the subgradient at $x^{(0)}$.\n\nImplement exactly two Bregman iterations numerically for each test case, starting from $x^{(0)}$ and the initial dual variable. For each test case, report two quantities after the second iteration:\n- the absolute constraint residual $|H x^{(2)} - y|$,\n- the absolute prior deviation $|x^{(2)} - x_b|$.\n\nThese two quantities together illustrate the trade-off between constraint satisfaction and prior adherence after two iterations.\n\nNo physical units are involved. Report angle-free scalar values. The final numerical outputs must be represented as decimal numbers.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of the outer list corresponds to one test case and must itself be a two-element list in the order specified above. For example, the output format must be of the form $[[r_1,d_1],[r_2,d_2],\\dots]$, where each $r_i$ and $d_i$ are decimal numbers. Round each decimal number to six digits after the decimal point.\n\nUse the following test suite covering diverse scenarios:\n- Test case 1 (baseline): $H = 1.0$, $y = 3.0$, $x_b = 0.0$, $\\alpha = 1.0$, $\\lambda = 1.0$.\n- Test case 2 (strong prior): $H = 1.0$, $y = 3.0$, $x_b = 0.0$, $\\alpha = 100.0$, $\\lambda = 1.0$.\n- Test case 3 (strong constraint penalty): $H = 1.0$, $y = 3.0$, $x_b = 0.0$, $\\alpha = 1.0$, $\\lambda = 100.0$.\n- Test case 4 (weak sensitivity): $H = 0.1$, $y = 3.0$, $x_b = 0.0$, $\\alpha = 1.0$, $\\lambda = 1.0$.\n- Test case 5 (prior equals observation): $H = 1.0$, $y = 2.0$, $x_b = 2.0$, $\\alpha = 1.0$, $\\lambda = 1.0$.\n\nIn all cases, $\\lambda \\in \\mathbb{R}_{>0}$ is the penalty parameter used in the Bregman iteration. Your implementation must be deterministic and self-contained; it must not read any input. The single line output must be the exact numerical list in the specified format with six decimal places for each number.", "solution": "We start from the convex optimization problem of minimizing the convex functional $\\phi(x)$ subject to a linear equality constraint. The foundational setup is\n$$\n\\min_{x \\in \\mathbb{R}} \\; \\phi(x) \\quad \\text{subject to} \\quad H x = y,\n$$\nwith $\\phi(x) = \\tfrac{\\alpha}{2} (x - x_b)^2$, where $\\alpha \\in \\mathbb{R}_{>0}$ and $x_b \\in \\mathbb{R}$ are given. The observation operator $H \\in \\mathbb{R}$ and observation $y \\in \\mathbb{R}$ define the constraint. The function $\\phi$ is strictly convex and twice continuously differentiable, so its gradient is well-defined for all $x$.\n\nThe Bregman iteration for minimizing a convex functional subject to a linear equality constraint uses the principle of repeatedly minimizing the original convex functional linearized by the current subgradient, coupled with a quadratic penalty enforcing the constraint, and updating a dual-like variable with the residual. The fundamental base includes:\n- Properties of convex functions and their gradients: for twice differentiable $\\phi$, $\\nabla \\phi(x)$ exists and is unique.\n- First-order optimality conditions for unconstrained minimization of differentiable functions: the minimizer is characterized by the gradient equaling zero.\n- Linear constraint residual updates consistent with a gradient descent step in the dual variable.\n\nLet $p^{(k)}$ denote the dual-like variable associated to the Bregman iteration, initialized from the subgradient at the initial state. For twice differentiable $\\phi$, the subgradient at $x$ is simply the gradient $\\nabla \\phi(x)$. Define $x^{(0)}$ as the minimizer of $\\phi(x)$, which is obtained by solving $\\nabla \\phi(x) = 0$. Since\n$$\n\\nabla \\phi(x) = \\alpha (x - x_b),\n$$\nthe unique minimizer satisfies $\\alpha (x^{(0)} - x_b) = 0$, hence $x^{(0)} = x_b$. The initial dual variable is chosen as $p^{(0)} \\in \\partial \\phi(x^{(0)}) = \\{\\nabla \\phi(x^{(0)})\\}$, giving $p^{(0)} = \\alpha (x^{(0)} - x_b) = 0$.\n\nAt iteration $k$, the Bregman minimization step considers the following penalized functional:\n$$\nJ_k(x) = \\phi(x) - \\langle p^{(k)}, x \\rangle + \\frac{\\lambda}{2} \\|H x - y\\|^2,\n$$\nwhere $\\lambda \\in \\mathbb{R}_{>0}$ is a penalty parameter. In one dimension, the inner product reduces to multiplication. The minimizer $x^{(k+1)}$ is obtained by setting the derivative of $J_k$ with respect to $x$ to zero. Using the chain rule and linearity,\n$$\n\\frac{d}{dx} J_k(x) = \\nabla \\phi(x) - p^{(k)} + \\lambda H \\cdot (H x - y).\n$$\nIn detail, we have $\\nabla \\phi(x) = \\alpha (x - x_b)$, and the derivative of $\\tfrac{\\lambda}{2} (H x - y)^2$ with respect to $x$ is $\\lambda H (H x - y)$, since $\\frac{d}{dx}(H x - y)^2 = 2 (H x - y) H$ and the factor $\\tfrac{1}{2}$ cancels the $2$. Setting the derivative to zero yields the first-order optimality condition:\n$$\n\\alpha (x^{(k+1)} - x_b) - p^{(k)} + \\lambda H (H x^{(k+1)} - y) = 0.\n$$\nRearranging terms to solve for $x^{(k+1)}$ gives\n$$\n\\alpha x^{(k+1)} - \\alpha x_b - p^{(k)} + \\lambda H^2 x^{(k+1)} - \\lambda H y = 0,\n$$\nwhich collects into\n$$\n(\\alpha + \\lambda H^2) x^{(k+1)} = \\alpha x_b + p^{(k)} + \\lambda H y,\n$$\nand therefore the explicit scalar update\n$$\nx^{(k+1)} = \\frac{\\alpha x_b + p^{(k)} + \\lambda H y}{\\alpha + \\lambda H^2}.\n$$\n\nThe dual-like variable update is driven by the residual of the constraint, consistent with gradient descent in the dual variable:\n$$\np^{(k+1)} = p^{(k)} - \\lambda H \\left(H x^{(k+1)} - y\\right).\n$$\nThis follows directly from the fundamental iteration in Bregman methods that accumulates constraint residuals scaled by the penalty and sensitivity.\n\nThe algorithm, specialized to one dimension with the quadratic prior, is thus:\n- Initialization: $x^{(0)} = x_b$, $p^{(0)} = 0$.\n- Iterative updates for $k = 0, 1, \\dots$:\n  $$\n  x^{(k+1)} = \\frac{\\alpha x_b + p^{(k)} + \\lambda H y}{\\alpha + \\lambda H^2}, \\quad r^{(k+1)} = H x^{(k+1)} - y, \\quad p^{(k+1)} = p^{(k)} - \\lambda H r^{(k+1)}.\n  $$\n\nWe execute exactly two iterations, obtaining $x^{(1)}$ and $x^{(2)}$. After the second iteration, we compute:\n- the absolute constraint residual $|H x^{(2)} - y|$,\n- the absolute prior deviation $|x^{(2)} - x_b|$.\n\nThese two scalars quantify the trade-off: with larger $\\lambda$ or larger $H$, the residual $|H x^{(2)} - y|$ diminishes rapidly, indicating stronger constraint satisfaction; with larger $\\alpha$, the deviation $|x^{(2)} - x_b|$ diminishes, indicating stronger prior adherence. In contrast, small $H$ reduces the sensitivity and slows constraint enforcement, keeping the state closer to the prior within the same number of iterations.\n\nApplying the algorithm to the test suite:\n- Test case 1: $H = 1.0$, $y = 3.0$, $x_b = 0.0$, $\\alpha = 1.0$, $\\lambda = 1.0$.\n  Initialization: $x^{(0)} = 0$, $p^{(0)} = 0$.\n  First update: $x^{(1)} = \\frac{1 \\cdot 0 + 0 + 1 \\cdot 1 \\cdot 3}{1 + 1 \\cdot 1^2} = \\frac{3}{2} = 1.5$, $r^{(1)} = 1 \\cdot 1.5 - 3 = -1.5$, $p^{(1)} = 0 - 1 \\cdot 1 \\cdot (-1.5) = 1.5$.\n  Second update: $x^{(2)} = \\frac{1 \\cdot 0 + 1.5 + 1 \\cdot 1 \\cdot 3}{1 + 1 \\cdot 1^2} = \\frac{4.5}{2} = 2.25$, $r^{(2)} = 2.25 - 3 = -0.75$.\n  Metrics: $|H x^{(2)} - y| = 0.75$, $|x^{(2)} - x_b| = 2.25$.\n- Test case 2: $H = 1.0$, $y = 3.0$, $x_b = 0.0$, $\\alpha = 100.0$, $\\lambda = 1.0$.\n  Initialization: $x^{(0)} = 0$, $p^{(0)} = 0$.\n  First update: $x^{(1)} = \\frac{100 \\cdot 0 + 0 + 1 \\cdot 1 \\cdot 3}{100 + 1 \\cdot 1^2} = \\frac{3}{101} \\approx 0.029703$, $r^{(1)} \\approx -2.970297$, $p^{(1)} \\approx 2.970297$.\n  Second update: $x^{(2)} = \\frac{100 \\cdot 0 + 2.970297 + 3}{101} \\approx 0.059104$, $r^{(2)} \\approx -2.940896$.\n  Metrics: $|H x^{(2)} - y| \\approx 2.940896$, $|x^{(2)} - x_b| \\approx 0.059104$.\n- Test case 3: $H = 1.0$, $y = 3.0$, $x_b = 0.0$, $\\alpha = 1.0$, $\\lambda = 100.0$.\n  Initialization: $x^{(0)} = 0$, $p^{(0)} = 0$.\n  First update: $x^{(1)} = \\frac{1 \\cdot 0 + 0 + 100 \\cdot 1 \\cdot 3}{1 + 100 \\cdot 1^2} = \\frac{300}{101} \\approx 2.970297$, $r^{(1)} \\approx -0.029703$, $p^{(1)} = 0 - 100 \\cdot 1 \\cdot (-0.029703) \\approx 2.970297$.\n  Second update: $x^{(2)} = \\frac{0 + 2.970297 + 300}{101} \\approx 3.000695$, $r^{(2)} \\approx 0.000695$.\n  Metrics: $|H x^{(2)} - y| \\approx 0.000695$, $|x^{(2)} - x_b| \\approx 3.000695$.\n- Test case 4: $H = 0.1$, $y = 3.0$, $x_b = 0.0$, $\\alpha = 1.0$, $\\lambda = 1.0$.\n  Initialization: $x^{(0)} = 0$, $p^{(0)} = 0$.\n  First update: $x^{(1)} = \\frac{0 + 0 + 1 \\cdot 0.1 \\cdot 3}{1 + 1 \\cdot 0.1^2} = \\frac{0.3}{1.01} \\approx 0.297030$, $r^{(1)} \\approx -2.970297$, $p^{(1)} \\approx 0.297030$.\n  Second update: $x^{(2)} = \\frac{0 + 0.297030 + 0.3}{1.01} \\approx 0.591089$, $r^{(2)} \\approx -2.940891$.\n  Metrics: $|H x^{(2)} - y| \\approx 2.940891$, $|x^{(2)} - x_b| \\approx 0.591089$.\n- Test case 5: $H = 1.0$, $y = 2.0$, $x_b = 2.0$, $\\alpha = 1.0$, $\\lambda = 1.0$.\n  Initialization: $x^{(0)} = 2$, $p^{(0)} = 0$.\n  First update: $x^{(1)} = \\frac{2 + 2}{2} = 2$, $r^{(1)} = 0$, $p^{(1)} = 0$.\n  Second update: $x^{(2)} = 2$, $r^{(2)} = 0$.\n  Metrics: $|H x^{(2)} - y| = 0$, $|x^{(2)} - x_b| = 0$.\n\nThese results confirm the qualitative trade-off: stronger constraint penalties ($\\lambda$ large) or higher sensitivity ($H$ large) rapidly reduce the constraint residual, possibly at the expense of larger deviation from the prior after only two steps, while a strong prior ($\\alpha$ large) limits deviation from $x_b$ but makes constraint satisfaction slower within the same iteration budget. When the prior and observation agree, both metrics are zero and the iteration remains at the prior mean.\n\nThe implementation follows exactly the derived scalar updates and returns the requested metrics rounded to six decimal places in the specified output format.", "answer": "```python\nimport numpy as np\n\ndef bregman_two_steps(H, y, x_b, alpha, lam):\n    \"\"\"\n    Perform two Bregman iterations for the 1D convex prior phi(x) = (alpha/2)*(x - x_b)^2\n    with equality constraint H*x = y.\n\n    Returns:\n        (residual_abs, prior_dev_abs) after two iterations.\n    \"\"\"\n    # Initialization: minimize phi -> x0 = x_b, and p0 = grad phi(x0) = alpha*(x0 - x_b) = 0\n    x = x_b\n    p = 0.0\n\n    # Perform two iterations\n    for _ in range(2):\n        denom = alpha + lam * (H ** 2)\n        # Update x^(k+1)\n        x = (alpha * x_b + p + lam * H * y) / denom\n        # Residual r = H*x - y\n        r = H * x - y\n        # Update p^(k+1)\n        p = p - lam * H * r\n\n    residual_abs = abs(H * x - y)\n    prior_dev_abs = abs(x - x_b)\n    return residual_abs, prior_dev_abs\n\ndef solve():\n    # Define the test cases: (H, y, x_b, alpha, lam)\n    test_cases = [\n        (1.0, 3.0, 0.0, 1.0, 1.0),      # Test case 1 (baseline)\n        (1.0, 3.0, 0.0, 100.0, 1.0),    # Test case 2 (strong prior)\n        (1.0, 3.0, 0.0, 1.0, 100.0),    # Test case 3 (strong constraint penalty)\n        (0.1, 3.0, 0.0, 1.0, 1.0),      # Test case 4 (weak sensitivity)\n        (1.0, 2.0, 2.0, 1.0, 1.0),      # Test case 5 (prior equals observation)\n    ]\n\n    results = []\n    for H, y, x_b, alpha, lam in test_cases:\n        res_abs, dev_abs = bregman_two_steps(H, y, x_b, alpha, lam)\n        # Round to six decimal places as required\n        res_abs = round(res_abs, 6)\n        dev_abs = round(dev_abs, 6)\n        results.append([res_abs, dev_abs])\n\n    # Format the output exactly as a single line list of lists\n    def format_list_of_lists(lst):\n        inner = []\n        for pair in lst:\n            inner.append(f\"[{pair[0]},{pair[1]}]\")\n        return f\"[{','.join(inner)}]\"\n\n    print(format_list_of_lists(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3369784"}, {"introduction": "Having explored the fundamental definition and the iterative dynamics in a simplified setting, we are now prepared to tackle a realistic application. The Split Bregman method is a powerful and widely used algorithm equivalent to the Alternating Direction Method of Multipliers (ADMM). This exercise ([@problem_id:3369799]) guides you through implementing one full iteration of the Split Bregman algorithm for the classic problem of one-dimensional Total Variation (TV) denoising. This practice bridges theory and application, demonstrating how the method's subproblems are solved using standard techniques like the Fast Fourier Transform and soft-thresholding.", "problem": "Consider the one-dimensional Total Variation (TV) denoising problem for an identity forward operator within the framework of inverse problems and data assimilation. Let $A$ denote the forward operator, and suppose $A = I$ (the identity). Given a data vector $g \\in \\mathbb{R}^n$, we pose the Rudin–Osher–Fatemi TV denoising objective as minimizing the sum of a data fidelity term and a TV regularization term, namely the functional whose value at a candidate signal $u \\in \\mathbb{R}^n$ is the sum of the squared data misfit and the $\\ell_1$-norm of the discrete gradient of $u$, scaled by the regularization parameter. The discrete gradient is defined using finite differences under periodic boundary conditions.\n\nDefine the finite difference operator $D : \\mathbb{R}^n \\to \\mathbb{R}^n$ by the component-wise rule\n$$(D u)_i = u_{(i+1) \\bmod n} - u_i \\quad \\text{for } i \\in \\{0,1,\\dots,n-1\\}.$$\nAssume the Split Bregman method is used to solve the denoising problem. In the Split Bregman algorithm, the iteration state comprises the primal variable $u \\in \\mathbb{R}^n$, the split variable $d \\in \\mathbb{R}^n$ that enforces $d \\approx D u$, and the Bregman variable $b \\in \\mathbb{R}^n$. The iteration uses a penalty parameter $\\lambda > 0$ and a TV regularization parameter $\\mu > 0$.\n\nImplement one full Split Bregman iteration starting from the initial state $u^{0} = 0$, $d^{0} = 0$, and $b^{0} = 0$ (the zero vectors in $\\mathbb{R}^n$). Use the principle-based derivation from the augmented Lagrangian viewpoint to compute the update of $u$ and the update of $d$ at the first iteration. The discrete operators must be implemented exactly as specified, and all computations must be carried out in double-precision floating point arithmetic.\n\nYour program must produce, for each specified test case, the vectors $u^{1}$ and $d^{1}$ yielded by a single Split Bregman iteration. Represent vectors as lists of floating-point numbers in the program output.\n\nUse the following test suite, which covers typical behavior, near-constant data, a small problem size, and a higher-penalty regime:\n\n- Test case $1$: $n = 6$, $g = [\\,1.0,\\,1.2,\\,0.9,\\,-0.5,\\,-0.4,\\,0.0\\,]$, $\\mu = 0.6$, $\\lambda = 2.0$.\n- Test case $2$: $n = 6$, $g = [\\,2.0,\\,2.0,\\,2.0,\\,2.0,\\,2.0,\\,2.0\\,]$, $\\mu = 1.5$, $\\lambda = 1.0$.\n- Test case $3$: $n = 3$, $g = [\\,0.0,\\,10.0,\\,-10.0\\,]$, $\\mu = 0.4$, $\\lambda = 0.5$.\n- Test case $4$: $n = 8$, $g = [\\,0.0,\\,0.5,\\,1.0,\\,1.5,\\,1.25,\\,0.8,\\,0.3,\\,-0.2\\,]$, $\\mu = 2.0$, $\\lambda = 3.0$.\n\nFor each test case, compute $u^{1}$ and $d^{1}$ using one full Split Bregman iteration with the finite difference operator $D$ as defined above under periodic boundary conditions.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The $i$-th element of this list must itself be a list of two lists, where the first inner list is $u^{1}$ and the second inner list is $d^{1}$ for the $i$-th test case. For example, the output format should be\n$$[\\, [\\,[u^{1}_{(1)}],\\,[d^{1}_{(1)}]\\,],\\,[\\,[u^{1}_{(2)}],\\,[d^{1}_{(2)}]\\,],\\,\\dots\\, ]$$\nwith no spaces in the printed output.", "solution": "The problem is valid and well-posed. It describes a standard application of the Split Bregman algorithm to the Total Variation (TV) denoising problem, providing all necessary definitions, parameters, and initial conditions.\n\nThe one-dimensional Rudin–Osher–Fatemi (ROF) TV denoising problem seeks to find a signal $u \\in \\mathbb{R}^n$ that minimizes the following objective functional for a given noisy signal $g \\in \\mathbb{R}^n$:\n$$ E(u) = \\frac{1}{2} \\|u-g\\|_2^2 + \\mu \\|Du\\|_1 $$\nHere, $\\mu > 0$ is the regularization parameter that controls the strength of the denoising. The operator $D: \\mathbb{R}^n \\to \\mathbb{R}^n$ is the discrete gradient, defined with periodic boundary conditions as $(Du)_i = u_{(i+1) \\bmod n} - u_i$.\n\nThe $\\ell_1$-norm term $\\|Du\\|_1$ is non-differentiable, making direct minimization difficult. The Split Bregman method addresses this by introducing a splitting variable $d \\in \\mathbb{R}^n$ and converting the problem into a constrained optimization problem:\n$$ \\min_{u,d} \\frac{1}{2} \\|u-g\\|_2^2 + \\mu \\|d\\|_1 \\quad \\text{subject to} \\quad d = Du $$\n\nThis constrained problem is solved by minimizing an associated augmented Lagrangian, which leads to an iterative scheme. The Split Bregman iteration updates the primal variable $u$, the split variable $d$, and a Bregman (dual) variable $b \\in \\mathbb{R}^n$ in an alternating fashion. The iterative scheme, for iteration $k+1$, is given by:\n1.  Update $u$:\n    $$ u^{k+1} = \\arg\\min_u \\left\\{ \\frac{1}{2} \\|u-g\\|_2^2 + \\frac{\\lambda}{2} \\|Du - d^k + b^k\\|_2^2 \\right\\} $$\n2.  Update $d$:\n    $$ d^{k+1} = \\arg\\min_d \\left\\{ \\mu \\|d\\|_1 + \\frac{\\lambda}{2} \\|Du^{k+1} - d + b^k\\|_2^2 \\right\\} $$\n3.  Update $b$:\n    $$ b^{k+1} = b^k + (Du^{k+1} - d^{k+1}) $$\nThe parameter $\\lambda > 0$ is a penalty parameter for the constraint $d=Du$.\n\nWe are tasked with computing one full iteration, yielding $u^1$ and $d^1$, starting from the initial state $u^0 = 0$, $d^0 = 0$, and $b^0 = 0$.\n\n**Step 1: The $u$-Subproblem for $u^1$**\n\nWe substitute $k=0$ along with the initial conditions $d^0=0$ and $b^0=0$ into the $u$-update rule:\n$$ u^1 = \\arg\\min_u \\left\\{ \\frac{1}{2} \\|u-g\\|_2^2 + \\frac{\\lambda}{2} \\|Du - 0 + 0\\|_2^2 \\right\\} = \\arg\\min_u \\left\\{ \\frac{1}{2} \\|u-g\\|_2^2 + \\frac{\\lambda}{2} \\|Du\\|_2^2 \\right\\} $$\nThis is a quadratic minimization problem, which can be solved by setting the gradient of the objective with respect to $u$ to zero. The gradient of $\\frac{1}{2}\\|u-g\\|_2^2$ is $u-g$. The gradient of $\\frac{\\lambda}{2}\\|Du\\|_2^2 = \\frac{\\lambda}{2} u^T D^T D u$ is $\\lambda D^T D u$. Setting the sum of gradients to zero gives the normal equations:\n$$ (u^1 - g) + \\lambda D^T D u^1 = 0 $$\n$$ (I + \\lambda D^T D) u^1 = g $$\nwhere $I$ is the $n \\times n$ identity matrix and $D^T$ is the transpose of $D$. The matrix $C = I + \\lambda D^T D$ is circulant because $D$ (forward difference with periodic boundaries) is a circulant matrix. A circulant linear system can be solved efficiently in the Fourier domain. Let $\\mathcal{F}$ denote the Discrete Fourier Transform (DFT). Applying the DFT to the system yields:\n$$ \\mathcal{F}((I + \\lambda D^T D) u^1) = \\mathcal{F}(g) $$\n$$ \\mathcal{F}(I + \\lambda D^T D) \\mathcal{F}(u^1) = \\mathcal{F}(g) $$\nThe term $\\mathcal{F}(I + \\lambda D^T D)$ represents the eigenvalues of the circulant matrix $C$. These eigenvalues can be found by taking the DFT of the first row of $C$. The operator $D^T D$ is the negative of the 1D discrete Laplacian with periodic boundaries, $(D^T D u)_i = -u_{(i+1) \\bmod n} + 2u_i - u_{(i-1) \\bmod n}$. Thus, the first row of $C$ is $c = [1+2\\lambda, -\\lambda, 0, \\dots, 0, -\\lambda]$.\nThe DFT of $u^1$ is then:\n$$ \\mathcal{F}(u^1)_k = \\frac{\\mathcal{F}(g)_k}{ \\mathcal{F}(c)_k } $$\nFinally, we obtain $u^1$ by applying the inverse DFT:\n$$ u^1 = \\mathcal{F}^{-1}\\left( \\frac{\\mathcal{F}(g)}{\\mathcal{F}(c)} \\right) $$\n\n**Step 2: The $d$-Subproblem for $d^1$**\n\nNext, we compute $d^1$ using the newly computed $u^1$ and the previous $b^0=0$:\n$$ d^1 = \\arg\\min_d \\left\\{ \\mu \\|d\\|_1 + \\frac{\\lambda}{2} \\|Du^1 - d + 0\\|_2^2 \\right\\} = \\arg\\min_d \\left\\{ \\mu \\|d\\|_1 + \\frac{\\lambda}{2} \\|d - Du^1\\|_2^2 \\right\\} $$\nThis problem is separable, meaning we can solve for each component $d_i$ independently:\n$$ d^1_i = \\arg\\min_{d_i} \\left\\{ \\mu |d_i| + \\frac{\\lambda}{2} (d_i - (Du^1)_i)^2 \\right\\} $$\nThis is a standard problem in convex optimization, and its solution is given by the soft-shrinkage operator. Let $v_i = (Du^1)_i$ and the threshold be $T = \\mu/\\lambda$. The solution is:\n$$ d^1_i = \\text{shrink}(v_i, T) = \\text{sign}(v_i) \\max(|v_i| - T, 0) $$\nThis can be computed for all components $i=0, \\dots, n-1$ to obtain the vector $d^1$.\n\n**Summary of the Algorithm for One Iteration**\nGiven $n$, $g$, $\\mu$, and $\\lambda$:\n1.  Construct the first row of the circulant matrix $C = I + \\lambda D^T D$: $c = [1+2\\lambda, -\\lambda, 0, \\dots, 0, -\\lambda]$.\n2.  Compute the DFT of $c$ to get the eigenvalues of $C$, let this be $\\hat{c} = \\mathcal{F}(c)$.\n3.  Compute the DFT of the data vector $g$, let this be $\\hat{g} = \\mathcal{F}(g)$.\n4.  Compute the DFT of the solution $u^1$ by element-wise division: $\\hat{u}^1_k = \\hat{g}_k / \\hat{c}_k$.\n5.  Compute $u^1$ by taking the inverse DFT of $\\hat{u}^1$: $u^1 = \\mathcal{F}^{-1}(\\hat{u}^1)$.\n6.  Compute the discrete gradient of $u^1$: $v = Du^1$.\n7.  Apply the soft-shrinkage operator to $v$ with threshold $T=\\mu/\\lambda$ to obtain $d^1$.\nAll computations are performed using double-precision floating-point arithmetic.", "answer": "```python\nimport numpy as np\nimport json\n\ndef run_split_bregman_iteration(n: int, g: np.ndarray, mu: float, lambda_param: float) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Computes one full Split Bregman iteration for 1D TV denoising.\n\n    Args:\n        n (int): The size of the signal.\n        g (np.ndarray): The observed (noisy) signal vector.\n        mu (float): The TV regularization parameter.\n        lambda_param (float): The Bregman penalty parameter.\n\n    Returns:\n        tuple[list[float], list[float]]: A tuple containing the updated vectors u^1 and d^1 as lists.\n    \"\"\"\n    # Step 1: Compute u^1 by solving (I + lambda * D^T D)u = g\n    # This is a circulant system, solved efficiently via FFT.\n\n    # First, construct the first row of the circulant matrix C = I + lambda * D^T * D\n    c_first_row = np.zeros(n, dtype=np.float64)\n    c_first_row[0] = 1.0 + 2.0 * lambda_param\n    c_first_row[1] = -lambda_param\n    c_first_row[n-1] = -lambda_param\n\n    # Eigenvalues of C are the FFT of its first row.\n    c_eig = np.fft.fft(c_first_row)\n\n    # FFT of the data vector g\n    g_hat = np.fft.fft(g)\n\n    # Solve for u^1 in the Fourier domain\n    u1_hat = g_hat / c_eig\n\n    # Inverse FFT to get u^1 in the spatial domain.\n    # The result should be real; take the real part to discard negligible imaginary components due to floating point error.\n    u1 = np.fft.ifft(u1_hat).real\n\n    # Step 2: Compute d^1 using the soft-shrinkage operator\n    # d^1 = shrink(Du^1, mu/lambda)\n\n    # Compute the discrete gradient Du^1 with periodic boundary conditions\n    # (Du)_i = u_{i+1} - u_i\n    v = np.roll(u1, -1) - u1\n\n    # Apply the soft-shrinkage operator\n    threshold = mu / lambda_param\n    d1 = np.sign(v) * np.maximum(np.abs(v) - threshold, 0.0)\n\n    return u1.tolist(), d1.tolist()\n\ndef solve():\n    \"\"\"\n    Runs the Split Bregman iteration for the test cases specified in the problem and prints the results.\n    \"\"\"\n    test_cases = [\n        # (n, g, mu, lambda)\n        (6, [1.0, 1.2, 0.9, -0.5, -0.4, 0.0], 0.6, 2.0),\n        (6, [2.0, 2.0, 2.0, 2.0, 2.0, 2.0], 1.5, 1.0),\n        (3, [0.0, 10.0, -10.0], 0.4, 0.5),\n        (8, [0.0, 0.5, 1.0, 1.5, 1.25, 0.8, 0.3, -0.2], 2.0, 3.0),\n    ]\n\n    results = []\n    for n, g_list, mu, lambda_param in test_cases:\n        g_np = np.array(g_list, dtype=np.float64)\n        u1, d1 = run_split_bregman_iteration(n, g_np, mu, lambda_param)\n        results.append([u1, d1])\n\n    # The output format requires a json-like string with no spaces.\n    # json.dumps with custom separators is the most reliable way to achieve this.\n    print(json.dumps(results, separators=(',', ':')))\n\nsolve()\n```", "id": "3369799"}]}