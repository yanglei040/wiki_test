{"hands_on_practices": [{"introduction": "The Iterative Soft-Thresholding Algorithm is built upon two key operations: a gradient descent step on the smooth data-fit term and a proximal step on the non-smooth regularization term. This first exercise focuses entirely on the second operation, the heart of sparsity promotion in ISTA. By deriving the soft-thresholding operator from the fundamental definition of a proximal map, you will gain a concrete understanding of how it shrinks large coefficients and eliminates small ones, thereby inducing a sparse solution [@problem_id:3392980].", "problem": "Consider the canonical sparse linear inverse problem formulated as minimizing the sum of a quadratic data misfit and an elementwise sparsity-promoting penalty: find $x \\in \\mathbb{R}^{5}$ that minimizes $f(x) + g(x)$, where $f(x) = \\tfrac{1}{2}\\|A x - y\\|_{2}^{2}$ and $g(x) = \\lambda \\|x\\|_{1}$. In the iterative soft-thresholding algorithm (ISTA), a forward (gradient) step on $f$ at an iterate $x^{k}$ with step size $1/L$ is followed by a proximal step of $g$. Suppose that at a certain iteration, after the forward step, the intermediate vector is $u \\in \\mathbb{R}^{5}$, and the scalar threshold used in the proximal step is $\\tau  0$. The proximal map of the scaled $\\ell_{1}$-norm at $u$ is denoted by $S_{\\tau}(u)$.\n\nFor the specific values $u = (\\,3,\\,-1,\\,\\tfrac{3}{2},\\,-4,\\,\\tfrac{1}{2}\\,)^{\\top} \\in \\mathbb{R}^{5}$ and $\\tau = \\tfrac{3}{2}$, compute $S_{\\tau}(u)$ exactly, and explain, on the basis of first principles from convex analysis and properties of the proximal operator, which entries are set to zero and which are shrunk (and in what direction). Express your final answer as a single row vector with exact rational entries. Do not round.", "solution": "The problem is valid. It is well-posed, scientifically grounded in the principles of convex optimization and inverse problems, and provides all necessary information for a unique, verifiable solution.\n\nThe problem asks for the computation of the proximal operator of the scaled $\\ell_1$-norm, commonly known as the soft-thresholding operator, denoted by $S_{\\tau}(u)$. This operator is central to the Iterative Soft-Thresholding Algorithm (ISTA) used for solving sparse optimization problems.\n\nFrom first principles of convex analysis, the proximal operator of a function $h(x)$ with a scaling parameter $\\gamma  0$ applied to a vector $v$ is defined as the unique minimizer of the following objective function:\n$$\n\\text{prox}_{\\gamma h}(v) = \\arg\\min_{z} \\left( \\gamma h(z) + \\frac{1}{2} \\|z - v\\|_{2}^{2} \\right)\n$$\nIn the context of this problem, the function is the $\\ell_1$-norm, and the operator is denoted $S_{\\tau}(u)$. This implies we are solving for $x = S_{\\tau}(u)$ where:\n$$\nx = \\arg\\min_{x \\in \\mathbb{R}^{5}} \\left( \\tau \\|x\\|_{1} + \\frac{1}{2} \\|x - u\\|_{2}^{2} \\right)\n$$\nThe objective function to be minimized is $J(x) = \\tau \\|x\\|_{1} + \\frac{1}{2} \\|x - u\\|_{2}^{2}$. A key property of this objective function is its separability across the components of the vector $x$. We can rewrite the norms as sums over the components:\n$$\nJ(x) = \\tau \\sum_{i=1}^{5} |x_i| + \\frac{1}{2} \\sum_{i=1}^{5} (x_i - u_i)^2 = \\sum_{i=1}^{5} \\left( \\tau |x_i| + \\frac{1}{2} (x_i - u_i)^2 \\right)\n$$\nSince the overall minimization problem is a sum of terms that each depend on only one component $x_i$, we can minimize the entire sum by minimizing each term independently for each component $i \\in \\{1, 2, 3, 4, 5\\}$. Thus, for each $i$, we solve the scalar minimization problem:\n$$\nx_i = \\arg\\min_{\\xi \\in \\mathbb{R}} \\left( \\tau |\\xi| + \\frac{1}{2} (\\xi - u_i)^2 \\right)\n$$\nLet $J_i(\\xi) = \\tau |\\xi| + \\frac{1}{2} (\\xi - u_i)^2$. Since $J_i(\\xi)$ is a convex function, its minimum is attained where its subgradient contains zero. The subdifferential of $J_i$ at $\\xi$ is given by:\n$$\n\\partial J_i(\\xi) = \\tau \\partial|\\xi| + (\\xi - u_i)\n$$\nThe subdifferential of the absolute value function, $\\partial|\\xi|$, is:\n$$\n\\partial|\\xi| = \\begin{cases} \\{1\\}  \\text{if } \\xi  0 \\\\ \\{-1\\}  \\text{if } \\xi  0 \\\\ [-1, 1]  \\text{if } \\xi = 0 \\end{cases}\n$$\nThe optimality condition is $0 \\in \\partial J_i(x_i)$. We analyze this condition by cases for the optimal value $x_i$:\n\nCase 1: $x_i  0$.\nThe subgradient is $\\partial J_i(x_i) = \\{\\tau + (x_i - u_i)\\}$. Setting this to zero gives $\\tau + x_i - u_i = 0$, which implies $x_i = u_i - \\tau$. For this solution to be consistent with the assumption $x_i  0$, we must have $u_i - \\tau  0$, or $u_i  \\tau$.\n\nCase 2: $x_i  0$.\nThe subgradient is $\\partial J_i(x_i) = \\{-\\tau + (x_i - u_i)\\}$. Setting this to zero gives $-\\tau + x_i - u_i = 0$, which implies $x_i = u_i + \\tau$. For this solution to be consistent with the assumption $x_i  0$, we must have $u_i + \\tau  0$, or $u_i  -\\tau$.\n\nCase 3: $x_i = 0$.\nThe optimality condition becomes $0 \\in \\tau [-1, 1] + (0 - u_i)$, which simplifies to $u_i \\in \\tau [-1, 1]$. This is equivalent to $-\\tau \\le u_i \\le \\tau$, or $|u_i| \\le \\tau$.\n\nCombining these three cases gives the explicit formula for the soft-thresholding operator on a scalar $u_i$:\n$$\nx_i = S_{\\tau}(u_i) = \\begin{cases} u_i - \\tau  \\text{if } u_i  \\tau \\\\ 0  \\text{if } |u_i| \\le \\tau \\\\ u_i + \\tau  \\text{if } u_i  -\\tau \\end{cases}\n$$\nThis rule explains the behavior of the operator. If the magnitude of a component $u_i$ is below or at the threshold $\\tau$, it is set to zero, thus promoting sparsity. If its magnitude exceeds the threshold, it is \"shrunk\" towards zero by an amount $\\tau$ while preserving its sign.\n\nWe now apply this rule to the given vector $u = (\\,3,\\,-1,\\,\\frac{3}{2},\\,-4,\\,\\frac{1}{2}\\,)^{\\top}$ with the threshold $\\tau = \\frac{3}{2}$.\n\nFor the first component, $u_1 = 3$:\nSince $u_1 = 3  \\tau = \\frac{3}{2}$, this component is shrunk.\n$x_1 = u_1 - \\tau = 3 - \\frac{3}{2} = \\frac{6}{2} - \\frac{3}{2} = \\frac{3}{2}$.\n\nFor the second component, $u_2 = -1$:\nSince $|u_2| = |-1| = 1 \\le \\tau = \\frac{3}{2}$, this component is set to zero.\n$x_2 = 0$.\n\nFor the third component, $u_3 = \\frac{3}{2}$:\nSince $|u_3| = |\\frac{3}{2}| = \\frac{3}{2} \\le \\tau = \\frac{3}{2}$, this component is set to zero (it falls on the boundary of the thresholding region).\n$x_3 = 0$.\n\nFor the fourth component, $u_4 = -4$:\nSince $u_4 = -4  -\\tau = -\\frac{3}{2}$, this component is shrunk.\n$x_4 = u_4 + \\tau = -4 + \\frac{3}{2} = -\\frac{8}{2} + \\frac{3}{2} = -\\frac{5}{2}$.\n\nFor the fifth component, $u_5 = \\frac{1}{2}$:\nSince $|u_5| = |\\frac{1}{2}| = \\frac{1}{2} \\le \\tau = \\frac{3}{2}$, this component is set to zero.\n$x_5 = 0$.\n\nCombining these results, the final vector is $S_{\\tau}(u) = (\\frac{3}{2}, 0, 0, -\\frac{5}{2}, 0)^{\\top}$. As requested, this is expressed as a single row vector.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{2}  0  0  -\\frac{5}{2}  0\n\\end{pmatrix}\n}\n$$", "id": "3392980"}, {"introduction": "The performance of any regularization method is critically dependent on the choice of its parameters. This practice explores the delicate role of the regularization parameter $\\lambda$ in ISTA, demonstrating how an overly large value can cause the algorithm to stagnate at a trivial (and uninformative) zero solution. You will then derive a principled, universal threshold based on the statistical properties of the noise, providing a robust strategy to set $\\lambda$ that avoids this pitfall while effectively suppressing noise [@problem_id:3392955].", "problem": "Consider the linear inverse problem with unknown vector $x^{\\star} \\in \\mathbb{R}^{n}$, sensing matrix $A \\in \\mathbb{R}^{n \\times n}$, and observed data $b \\in \\mathbb{R}^{n}$ generated by $b = A x^{\\star} + \\eta$, where $\\eta$ is an additive noise vector. The Iterative Soft-Thresholding Algorithm (ISTA) for the $\\ell_{1}$-regularized least squares problem $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$ is defined by the iterative map $x^{k+1} = \\mathcal{S}_{\\lambda \\tau}\\!\\left(x^{k} + \\tau A^{\\top}(b - A x^{k})\\right)$, where $\\tau  0$ is a step size satisfying a standard Lipschitz condition and $\\mathcal{S}_{\\theta}$ denotes the componentwise soft-thresholding operator $\\big(\\mathcal{S}_{\\theta}(z)\\big)_{i} = \\mathrm{sign}(z_{i}) \\max\\{|z_{i}| - \\theta, 0\\}$.\n\n(a) Construct a counterexample demonstrating stagnation (plateau at the zero vector) under naive threshold selection when the regularization parameter $\\lambda$ is too large relative to the data term. Specifically, take $n = 3$, $A = I_{3}$, $\\tau = 1$, and $b = (0.12,\\,-0.09,\\,0)^{\\top}$. Starting from the definitions above and no further shortcuts, use first-principles reasoning to show precisely why the ISTA iterates $x^{k}$ become identically zero for all $k \\geq 1$ whenever $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$ in this configuration.\n\n(b) In a noisy data assimilation setting, suppose the noise $\\eta$ has independent and identically distributed (i.i.d.) Gaussian entries with zero mean and variance $\\sigma^{2}$, i.e., $\\eta \\sim \\mathcal{N}(0,\\,\\sigma^{2} I_{n})$. Derive, from a probabilistic argument based on the distribution of the maximum of $n$ i.i.d. Gaussian variables and without invoking pre-stated shortcuts, an analytically justified scaling for $\\lambda$ as a function of $\\sigma$ and $n$ that guards against noise-dominated updates while avoiding trivial zero solutions for signals with entries exceeding the typical noise-driven plateaus. Use the resulting scaling to compute a recommended $\\lambda$ for $n = 1024$ and $\\sigma = 0.03$.\n\nRound your final numerical value of $\\lambda$ to four significant figures. Express your answer as a pure number without units.", "solution": "The problem presents two distinct tasks related to the Iterative Soft-Thresholding Algorithm (ISTA). Part (a) requires a demonstration of iterate stagnation under specific conditions, while Part (b) calls for the derivation of a principled scaling for the regularization parameter $\\lambda$ in a stochastic setting.\n\n### Part (a): Stagnation at the Zero Vector\n\nWe are given the ISTA update rule for the minimization problem $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$:\n$$\nx^{k+1} = \\mathcal{S}_{\\lambda \\tau}\\!\\left(x^{k} + \\tau A^{\\top}(b - A x^{k})\\right)\n$$\nThe problem specifies the parameters for this part as $n = 3$, the identity matrix $A = I_{3}$, a step size $\\tau = 1$, and the data vector $b = (0.12,\\,-0.09,\\,0)^{\\top}$.\n\nSubstituting $A=I_{3}$ and $\\tau=1$ into the update rule yields a significant simplification. For any iterate $x^k$:\n$$\nx^{k+1} = \\mathcal{S}_{\\lambda \\cdot 1}\\!\\left(x^{k} + 1 \\cdot I_{3}^{\\top}(b - I_{3} x^{k})\\right) = \\mathcal{S}_{\\lambda}\\!\\left(x^{k} + (b - x^{k})\\right) = \\mathcal{S}_{\\lambda}(b)\n$$\nThis result shows that for any $k \\geq 0$, the next iterate $x^{k+1}$ is solely determined by the action of the soft-thresholding operator $\\mathcal{S}_{\\lambda}$ on the data vector $b$. It is independent of the current iterate $x^k$. Consequently, all iterates from $k=1$ onwards are identical:\n$$\nx^{1} = \\mathcal{S}_{\\lambda}(b)\n$$\n$$\nx^{2} = \\mathcal{S}_{\\lambda}(b)\n$$\n$$\n\\vdots\n$$\n$$\nx^{k} = \\mathcal{S}_{\\lambda}(b) \\quad \\text{for all } k \\geq 1.\n$$\nThe problem asks us to show that these iterates become identically zero, i.e., $x^k = 0$ for all $k \\geq 1$, under the condition $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$.\n\nFor our specific configuration where $A=I_3$, the condition is $\\lambda \\geq \\|I_3^{\\top} b\\|_{\\infty} = \\|b\\|_{\\infty}$. The $\\ell_{\\infty}$-norm of the vector $b=(0.12,\\,-0.09,\\,0)^{\\top}$ is:\n$$\n\\|b\\|_{\\infty} = \\max\\big(|0.12|, |-0.09|, |0|\\big) = 0.12\n$$\nSo the condition becomes $\\lambda \\geq 0.12$.\n\nWe now analyze the expression for the iterates, $x^k = \\mathcal{S}_{\\lambda}(b)$, under this condition. The soft-thresholding operator $\\mathcal{S}_{\\theta}$ is defined componentwise as:\n$$\n\\big(\\mathcal{S}_{\\theta}(z)\\big)_{i} = \\mathrm{sign}(z_{i}) \\max\\{|z_{i}| - \\theta, 0\\}\n$$\nFor an entire vector $\\mathcal{S}_{\\theta}(z)$ to be the zero vector, it is necessary and sufficient that for every component $i$, $\\max\\{|z_{i}| - \\theta, 0\\} = 0$. This, in turn, is equivalent to the condition $|z_i| \\leq \\theta$ for all $i$. This can be expressed compactly using the $\\ell_{\\infty}$-norm as $\\|z\\|_{\\infty} \\leq \\theta$.\n\nApplying this to our problem, the condition for $x^k = \\mathcal{S}_{\\lambda}(b)$ to be the zero vector is that $\\|b\\|_{\\infty} \\leq \\lambda$. This is precisely the condition $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$ given in the problem statement for our choice of $A=I_3$.\n\nTo be explicit, with $\\lambda \\geq 0.12$:\nFor the first component, $b_1 = 0.12$: $|b_1|=0.12$. Since $\\lambda \\geq 0.12$, $|b_1| - \\lambda \\leq 0$, so $\\max\\{|b_1|-\\lambda, 0\\}=0$.\nFor the second component, $b_2 = -0.09$: $|b_2|=0.09$. Since $\\lambda \\geq 0.12$, $|b_2| - \\lambda  0$, so $\\max\\{|b_2|-\\lambda, 0\\}=0$.\nFor the third component, $b_3 = 0$: $|b_3|=0$. Since $\\lambda \\geq 0.12$, $|b_3| - \\lambda  0$, so $\\max\\{|b_3|-\\lambda, 0\\}=0$.\n\nSince all components of $\\mathcal{S}_{\\lambda}(b)$ are zero, we have $\\mathcal{S}_{\\lambda}(b) = 0$.\nTherefore, for any choice of initial vector $x^0$, the first iterate is $x^1 = \\mathcal{S}_{\\lambda}(b) = 0$, and all subsequent iterates $x^k$ for $k \\geq 1$ remain at the zero vector. This demonstrates the specified stagnation.\n\n### Part (b): Probabilistic Scaling of the Regularization Parameter\n\nIn this part, we are to derive a scaling for $\\lambda$ that prevents the algorithm from being dominated by noise. The model is $b = Ax^\\star + \\eta$, where $\\eta$ is a vector of i.i.d. Gaussian noise components, $\\eta_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nA key principle in regularization is to choose $\\lambda$ large enough to suppress noise but not so large that it erroneously eliminates true signal components. A common strategy is to choose $\\lambda$ to be just above the level of noise that one would expect to see in the term being thresholded.\n\nConsider the first iteration of ISTA starting from the natural initial guess $x^0 = 0$. The argument of the soft-thresholding operator is $x^0 + \\tau A^{\\top}(b - A x^0) = \\tau A^\\top b$. If we consider a scenario with no true signal ($x^\\star=0$), then $b=\\eta$, and this argument becomes $\\tau A^\\top \\eta$. To prevent the algorithm from fitting to noise, we want this term to be thresholded to zero. This requires that every component of $\\tau A^\\top\\eta$ has a magnitude less than or equal to the threshold $\\lambda\\tau$. This can be written as:\n$$\n\\|\\tau A^\\top \\eta\\|_{\\infty} \\leq \\lambda\\tau \\implies \\|A^\\top \\eta\\|_{\\infty} \\leq \\lambda\n$$\nThis gives us a criterion for choosing $\\lambda$: it should be an upper bound on the likely values of $\\|A^\\top \\eta\\|_{\\infty}$.\n\nTo proceed with an analytical derivation, we must characterize the distribution of the vector $v = A^\\top \\eta$. This distribution depends on the matrix $A$. A standard assumption made for this type of universal threshold derivation is that the sensing matrix $A$ is orthonormal, i.e., $A^\\top A = I_n$. Under this assumption, the covariance of $v$ is:\n$$\n\\mathrm{Cov}(v) = E[v v^\\top] = E[A^\\top \\eta \\eta^\\top A] = A^\\top E[\\eta \\eta^\\top] A = A^\\top (\\sigma^2 I_n) A = \\sigma^2 A^\\top A = \\sigma^2 I_n\n$$\nSince $E[v] = A^\\top E[\\eta] = 0$, the components $v_i$ of $v = A^\\top \\eta$ are i.i.d. Gaussian random variables with mean $0$ and variance $\\sigma^2$, just like the original noise components $\\eta_i$.\n\nOur task now reduces to finding a high-probability upper bound on the random variable $M_n = \\|v\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |v_i|$. Let $Z_i = v_i/\\sigma$ be i.i.d. standard normal variables, $Z_i \\sim \\mathcal{N}(0, 1)$. We seek an estimate for $\\max_i |Z_i|$, and then scale the result by $\\sigma$.\n\nLet $Y = \\max_{1 \\leq i \\leq n} |Z_i|$. The cumulative distribution function (CDF) of $|Z_i|$ is $F_{|Z|}(y) = P(|Z_i| \\leq y) = 2\\Phi(y)-1$ for $y \\geq 0$, where $\\Phi$ is the CDF of the standard normal distribution. Since the $|Z_i|$ are i.i.d., the CDF of their maximum is:\n$$\nF_Y(y) = P(Y \\leq y) = [F_{|Z|}(y)]^n = (2\\Phi(y)-1)^n\n$$\nWe want to find a threshold $y_n$ such that $P(Y  y_n)$ is small for large $n$. We can use the union bound as an approximation for this tail probability:\n$$\nP(Y  y) = P(\\exists i: |Z_i|  y) \\leq \\sum_{i=1}^n P(|Z_i|y) = n P(|Z_1|y)\n$$\nThe probability $P(|Z_1|y)$ is $2(1-\\Phi(y))$. For large $y$, we can use the standard Gaussian tail approximation:\n$$\n1-\\Phi(y) \\approx \\frac{\\phi(y)}{y} = \\frac{1}{\\sqrt{2\\pi}y}\\exp\\left(-\\frac{y^2}{2}\\right)\n$$\nThus, $P(Yy) \\approx 2n(1-\\Phi(y)) \\approx \\frac{2n}{\\sqrt{2\\pi}y}\\exp\\left(-\\frac{y^2}{2}\\right)$. We seek a scaling for $y$ with $n$ such that this probability vanishes as $n \\to \\infty$. Let us test the candidate scaling $y = \\sqrt{2\\ln n}$:\n$$\nP(Y  \\sqrt{2\\ln n}) \\approx \\frac{2n}{\\sqrt{2\\pi}\\sqrt{2\\ln n}}\\exp\\left(-\\frac{( \\sqrt{2\\ln n} )^2}{2}\\right) = \\frac{2n}{\\sqrt{4\\pi\\ln n}}\\exp(-\\ln n) = \\frac{2n}{2\\sqrt{\\pi\\ln n}} \\left(\\frac{1}{n}\\right) = \\frac{1}{\\sqrt{\\pi\\ln n}}\n$$\nAs $n \\to \\infty$, this probability tends to $0$. This indicates that for large $n$, it is highly probable that the maximum of $n$ i.i.d. $|Z_i|$ variables will not exceed $\\sqrt{2\\ln n}$. This justifies choosing this value as a threshold.\n\nScaling back by $\\sigma$, we obtain the analytically justified scaling for $\\lambda$:\n$$\n\\lambda = \\sigma \\sqrt{2 \\ln n}\n$$\nThis is famously known as the universal threshold. It provides a parameter choice that adapts to the problem dimension $n$ and the noise level $\\sigma$.\n\nNow, we compute the recommended $\\lambda$ for $n = 1024$ and $\\sigma = 0.03$.\nFirst, calculate $\\ln(1024)$:\n$$\n\\ln(1024) = \\ln(2^{10}) = 10 \\ln(2)\n$$\nUsing a standard value for $\\ln(2) \\approx 0.69314718$:\n$$\n\\ln(1024) \\approx 6.9314718\n$$\nNow, substitute this into the formula for $\\lambda$:\n$$\n\\lambda = 0.03 \\times \\sqrt{2 \\times 6.9314718} = 0.03 \\times \\sqrt{13.8629436} \\approx 0.03 \\times 3.7232974\n$$\n$$\n\\lambda \\approx 0.1116989\n$$\nRounding this value to four significant figures gives:\n$$\n\\lambda \\approx 0.1117\n$$", "answer": "$$\n\\boxed{0.1117}\n$$", "id": "3392955"}, {"introduction": "An effective iterative algorithm requires not only a valid update rule but also a reliable stopping criterion. This advanced exercise bridges theory and practice by tasking you with the design of a robust termination condition for ISTA based on the primal-dual gap. By deriving the dual of the LASSO problem and constructing a feasible dual point from any primal iterate, you will implement a criterion that provides a rigorous, computable upper bound on the solution's suboptimality, ensuring both efficiency and accuracy in your solver [@problem_id:3392931].", "problem": "Consider the convex composite optimization problem that is standard in sparse inverse problems and data assimilation: minimize a smooth data-fit term plus a nonsmooth sparsity-promoting penalty. Specifically, consider the Least Absolute Shrinkage and Selection Operator (LASSO) objective, given a matrix $A \\in \\mathbb{R}^{m \\times n}$, a data vector $b \\in \\mathbb{R}^{m}$, and a regularization parameter $\\lambda  0$, where the primal objective is\n$$\nP(x) \\equiv \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1, \\quad x \\in \\mathbb{R}^n.\n$$\nYou will design a practical stopping criterion for the Iterative Soft-Thresholding Algorithm (ISTA) grounded in a primal-dual gap that upper-bounds the primal suboptimality, by constructing a dual feasible point from a given primal iterate and evaluating a computable upper bound on suboptimality.\n\nProblem requirements:\n\n1) Starting from fundamental definitions of convex conjugates, Fenchel duality, and subgradients for norms, derive from first principles the dual problem associated with the primal objective $P(x)$ above. Then, using weak duality, derive a computable primal-dual gap expression $G(x,\\nu)$ that is nonnegative for any primal point $x$ and any dual feasible point $\\nu$.\n\n2) From any primal iterate $x$, construct a dual feasible point $\\nu \\in \\mathbb{R}^m$ using only quantities available during ISTA iterations (e.g., the residual $Ax - b$ and matrix-vector products with $A$ and $A^\\top$). Your construction must guarantee feasibility for the dual constraint, without requiring knowledge of the exact solution.\n\n3) Using the standard proximal-gradient structure of ISTA for the smooth term $\\frac{1}{2}\\|Ax-b\\|_2^2$ (whose gradient is Lipschitz continuous with an appropriate constant determined by $A$) and the nonsmooth term $\\lambda \\|x\\|_1$, specify a termination rule that uses the primal-dual gap to decide convergence within a requested tolerance $\\varepsilon  0$. Prove that the resulting gap at termination is an upper bound on the actual primal suboptimality $P(x) - P(x^\\star)$, where $x^\\star$ denotes a minimizer.\n\n4) Implement a program that:\n   - Computes the spectral norm of $A$ to determine a valid fixed stepsize for ISTA.\n   - Runs ISTA from the zero vector, constructs at each iteration a dual feasible point from the current primal iterate, evaluates the primal-dual gap, and terminates when the gap is at most $\\varepsilon$ or when a maximum number of iterations is reached.\n   - For each test case, returns the final primal-dual gap as a floating-point number.\n\nUse the following test suite. All random generations must use the specified seeds for reproducibility.\n\n- Test case $1$ (happy path, overparameterized, sparse signal with noise):\n  - Dimensions: $m = 40$, $n = 60$.\n  - Random seed: $123$.\n  - Generate $A$ with entries i.i.d. normal with mean $0$ and variance $1/m$.\n  - Generate a ground-truth sparse $x_{\\mathrm{true}} \\in \\mathbb{R}^n$ with exactly $k = 6$ nonzeros at uniformly random positions, with nonzero values i.i.d. standard normal.\n  - Generate noise $\\eta \\in \\mathbb{R}^m$ with entries i.i.d. normal with mean $0$ and standard deviation $\\sigma = 10^{-2}$.\n  - Set $b = A x_{\\mathrm{true}} + \\eta$.\n  - Set $\\lambda = 0.1 \\cdot \\|A^\\top b\\|_\\infty$.\n  - Tolerance $\\varepsilon = 10^{-6}$, maximum iterations $10{,}000$.\n\n- Test case $2$ (boundary case, identity operator):\n  - Dimensions: $m = 30$, $n = 30$.\n  - Matrix $A = I$ (the $30 \\times 30$ identity).\n  - Random seed: $2024$.\n  - Generate $b$ with entries i.i.d. standard normal.\n  - Set $\\lambda = \\|A^\\top b\\|_\\infty$.\n  - Tolerance $\\varepsilon = 10^{-12}$, maximum iterations $1{,}000$.\n\n- Test case $3$ (underdetermined system):\n  - Dimensions: $m = 30$, $n = 80$.\n  - Random seed: $321$.\n  - Generate $A$ with entries i.i.d. normal with mean $0$ and variance $1/m$.\n  - Generate $b$ with entries i.i.d. standard normal.\n  - Set $\\lambda = 0.01 \\cdot \\|A^\\top b\\|_\\infty$.\n  - Tolerance $\\varepsilon = 10^{-5}$, maximum iterations $20{,}000$.\n\n- Test case $4$ (ill-conditioned operator):\n  - Dimensions: $m = 50$, $n = 50$.\n  - Random seed: $999$.\n  - Generate a base matrix with entries i.i.d. normal with mean $0$ and variance $1/m$, then multiply column $j$ by a scaling factor $s_j = 10^{4 \\cdot (j-1)/(n-1)}$ for $j \\in \\{1,\\dots,n\\}$ to induce a condition number approximately $10^4$.\n  - Generate $b$ with entries i.i.d. standard normal.\n  - Set $\\lambda = 0.05 \\cdot \\|A^\\top b\\|_\\infty$.\n  - Tolerance $\\varepsilon = 10^{-5}$, maximum iterations $20{,}000$.\n\nYour program must produce a single line of output containing the results as a comma-separated list of the final primal-dual gaps for the four test cases, enclosed in square brackets, in the order of the test cases above (for example, $[g_1,g_2,g_3,g_4]$). The outputs must be floating-point numbers. No physical units or angle units are involved in this problem.", "solution": "The problem requires the design and implementation of a stopping criterion for the Iterative Soft-Thresholding Algorithm (ISTA) for solving the LASSO optimization problem, based on a primal-dual gap. The solution will be presented in three parts as requested: derivation of the dual problem and gap, construction of a dual feasible point, and specification of the termination rule, followed by the implementation details.\n\n### Part 1: Derivation of the Fenchel Dual and Primal-Dual Gap\n\nThe primal LASSO problem is given by:\n$$\nP(x) \\equiv \\min_{x \\in \\mathbb{R}^n} \\left( \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1 \\right)\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $\\lambda  0$.\n\nTo derive the dual problem, we first reformulate the primal objective by introducing an auxiliary variable $r \\in \\mathbb{R}^m$ representing the residual. A common choice for the residual variable is $r = Ax - b$, but for a more convenient dual form, we define it as $r = b - Ax$. The problem becomes a constrained optimization problem:\n$$\n\\min_{x \\in \\mathbb{R}^n, r \\in \\mathbb{R}^m} \\frac{1}{2}\\|r\\|_2^2 + \\lambda \\|x\\|_1 \\quad \\text{subject to} \\quad r + Ax - b = 0.\n$$\nWe form the Lagrangian by introducing a dual variable (Lagrange multiplier) $\\nu \\in \\mathbb{R}^m$ for the equality constraint:\n$$\nL(x, r, \\nu) = \\frac{1}{2}\\|r\\|_2^2 + \\lambda \\|x\\|_1 + \\nu^\\top (r + Ax - b)\n$$\nThe Lagrange dual function, $D(\\nu)$, is the infimum of the Lagrangian over the primal variables $x$ and $r$:\n$$\nD(\\nu) = \\inf_{x, r} L(x, r, \\nu) = \\inf_{r} \\left( \\frac{1}{2}\\|r\\|_2^2 + \\nu^\\top r \\right) + \\inf_{x} \\left( \\lambda \\|x\\|_1 + \\nu^\\top Ax \\right) - \\nu^\\top b\n$$\nThe two infima can be computed separately.\nThe first term is an unconstrained quadratic minimization over $r$. The minimum is achieved when the gradient is zero: $\\nabla_r (\\frac{1}{2}r^\\top r + \\nu^\\top r) = r + \\nu = 0$, which implies $r = -\\nu$. Substituting this back gives the minimum value: $\\frac{1}{2}(-\\nu)^\\top(-\\nu) + \\nu^\\top(-\\nu) = \\frac{1}{2}\\|\\nu\\|_2^2 - \\|\\nu\\|_2^2 = -\\frac{1}{2}\\|\\nu\\|_2^2$.\n\nThe second term involves the Fenchel conjugate of the $\\ell_1$-norm. Specifically, $\\inf_{x} (\\lambda \\|x\\|_1 + (A^\\top\\nu)^\\top x) = -\\sup_{x} ((-A^\\top\\nu)^\\top x - \\lambda\\|x\\|_1)$. The supremum is the definition of the conjugate of $g(x)=\\lambda\\|x\\|_1$, evaluated at $-A^\\top\\nu$. The conjugate function $g^*(w) = \\sup_x (w^\\top x - \\lambda\\|x\\|_1)$ is an indicator function:\n$$\ng^*(w) = \\begin{cases} 0  \\text{if } \\|w\\|_\\infty \\le \\lambda \\\\ +\\infty  \\text{otherwise} \\end{cases}\n$$\nTherefore, $-\\sup_{x} ((-A^\\top\\nu)^\\top x - \\lambda\\|x\\|_1) = -g^*(-A^\\top\\nu)$. This term is $0$ if $\\|-A^\\top\\nu\\|_\\infty \\le \\lambda$ (i.e., $\\|A^\\top\\nu\\|_\\infty \\le \\lambda$) and $-\\infty$ otherwise.\n\nCombining the terms, the dual function is:\n$$\nD(\\nu) = -\\frac{1}{2}\\|\\nu\\|_2^2 - \\nu^\\top b, \\quad \\text{provided } \\|A^\\top\\nu\\|_\\infty \\le \\lambda.\n$$\nThe dual problem is to maximize this function over $\\nu$:\n$$\n\\max_{\\nu \\in \\mathbb{R}^m} D(\\nu) \\equiv \\max_{\\nu \\in \\mathbb{R}^m} \\left( -\\frac{1}{2}\\|\\nu\\|_2^2 - b^\\top\\nu \\right) \\quad \\text{subject to} \\quad \\|A^\\top\\nu\\|_\\infty \\le \\lambda.\n$$\nBy weak duality, for any primal feasible point $x$ and any dual feasible point $\\nu$ (i.e., a $\\nu$ satisfying the constraint $\\|A^\\top\\nu\\|_\\infty \\le \\lambda$), we have $P(x) \\ge D(\\nu)$. The primal-dual gap $G(x, \\nu)$ is defined as their difference, which is always non-negative:\n$$\nG(x, \\nu) = P(x) - D(\\nu) = \\left(\\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1\\right) - \\left(-\\frac{1}{2}\\|\\nu\\|_2^2 - b^\\top\\nu\\right)\n$$\n$$\nG(x, \\nu) = \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1 + \\frac{1}{2}\\|\\nu\\|_2^2 + b^\\top\\nu \\ge 0.\n$$\nThis expression provides a computable upper bound on the primal suboptimality, as will be shown in Part 3.\n\n### Part 2: Construction of a Dual Feasible Point\n\nDuring the execution of ISTA, we have a sequence of primal iterates $\\{x_k\\}$. For each iterate $x_k$, we need to construct a corresponding dual feasible point $\\nu_k$ that can be used to evaluate the gap. A dual feasible point must satisfy $\\|A^\\top\\nu_k\\|_\\infty \\le \\lambda$.\n\nThe Karush-Kuhn-Tucker (KKT) conditions for optimality suggest a form for the optimal dual variable. At an optimal primal-dual pair $(x^\\star, \\nu^\\star)$, we have $\\nu^\\star = b-Ax^\\star$. This motivates constructing a dual feasible point $\\nu_k$ for a non-optimal iterate $x_k$ by scaling the vector $b-Ax_k$.\n\nLet's test a candidate point of the form $\\nu_k = c_k(b-Ax_k)$ for some scalar $c_k  0$. For this point to be dual feasible, we must have:\n$$\n\\|A^\\top(c_k(b-Ax_k))\\|_\\infty \\le \\lambda \\implies c_k \\|A^\\top(b-Ax_k)\\|_\\infty \\le \\lambda\n$$\nIf $A^\\top(b-Ax_k) = 0$, any $c_k$ works. Otherwise, we must have $c_k \\le \\frac{\\lambda}{\\|A^\\top(b-Ax_k)\\|_\\infty}$. To make the dual objective $D(\\nu_k)$ as large as possible (and thus the gap as small as possible), we want $\\nu_k$ to be \"large\", so we choose the largest possible scaling factor $c_k$.\n\nHowever, if $\\|A^\\top(b-Ax_k)\\|_\\infty  \\lambda$, this would allow $c_k  1$. At the optimum $x^\\star$, we expect $c_{k} \\to 1$. A robust choice that handles all cases is to clamp the scaling factor. We define the primal gradient of the smooth term as $g(x) = \\nabla (\\frac{1}{2}\\|Ax-b\\|_2^2) = A^\\top(Ax-b)$. Then $A^\\top(b-Ax) = -g(x)$. The constraint is $c_k\\|g(x_k)\\|_\\infty \\le \\lambda$. We construct $\\nu_k$ as follows:\n$$\n\\nu_k = \\frac{\\lambda}{\\max(\\lambda, \\|A^\\top(b-Ax_k)\\|_\\infty)} \\cdot (b-Ax_k)\n$$\nLet's verify the feasibility of this construction. Let $C_k = \\max(\\lambda, \\|A^\\top(b-Ax_k)\\|_\\infty)$. Then $\\nu_k = \\frac{\\lambda}{C_k}(b-Ax_k)$.\n$$\n\\|A^\\top\\nu_k\\|_\\infty = \\left\\| A^\\top \\left( \\frac{\\lambda}{C_k}(b-Ax_k) \\right) \\right\\|_\\infty = \\frac{\\lambda}{C_k} \\|A^\\top(b-Ax_k)\\|_\\infty.\n$$\nBy definition of $C_k$, we have $\\|A^\\top(b-Ax_k)\\|_\\infty \\le C_k$. Thus,\n$$\n\\|A^\\top\\nu_k\\|_\\infty \\le \\frac{\\lambda}{C_k} \\cdot C_k = \\lambda.\n$$\nThis construction guarantees that $\\nu_k$ is a dual feasible point for any primal iterate $x_k$. All quantities needed, namely the residual $Ax_k-b$ and matrix-vector products with $A^\\top$, are readily available within an ISTA iteration.\n\n### Part 3: ISTA Termination Rule and Proof of Suboptimality Bound\n\nThe ISTA algorithm is a proximal gradient method. For the LASSO objective $P(x) = f(x) + h(x)$, where $f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2$ is the smooth part and $h(x) = \\lambda\\|x\\|_1$ is the nonsmooth part, the update rule is:\n$$\nx_{k+1} = \\text{prox}_{\\alpha h}(x_k - \\alpha \\nabla f(x_k))\n$$\nHere, $\\nabla f(x) = A^\\top(Ax-b)$, and the proximal operator of $h(x)$ is the soft-thresholding operator, $\\mathcal{S}_{\\tau}(z)_i = \\text{sign}(z_i)\\max(|z_i|-\\tau, 0)$. The ISTA update is therefore:\n$$\nx_{k+1} = \\mathcal{S}_{\\alpha\\lambda}(x_k - \\alpha A^\\top(Ax_k-b))\n$$\nFor convergence, the stepsize $\\alpha$ must satisfy $0  \\alpha \\le 1/L$, where $L$ is the Lipschitz constant of $\\nabla f(x)$. The Lipschitz constant is $L = \\|A^\\top A\\|_2 = \\|A\\|_2^2$, where $\\|A\\|_2$ is the spectral norm of $A$. A valid fixed stepsize is $\\alpha = 1/L$.\n\nThe proposed termination rule is as follows:\nAt each iteration $k$ of ISTA, starting from an initial point $x_0$:\n1.  Compute the current primal iterate $x_k$.\n2.  Construct the dual feasible point $\\nu_k$ using the method from Part 2.\n3.  Evaluate the primal-dual gap $G(x_k, \\nu_k) = P(x_k) - D(\\nu_k)$.\n4.  If $G(x_k, \\nu_k) \\le \\varepsilon$ for a given tolerance $\\varepsilon  0$, terminate the algorithm. Otherwise, compute $x_{k+1}$ and continue.\n\n**Proof of Suboptimality Bound:**\nLet $x^\\star$ be a minimizer of the primal problem $P(x)$, so $P(x^\\star) = \\min_x P(x)$. The primal suboptimality of an iterate $x_k$ is the non-negative quantity $P(x_k) - P(x^\\star)$.\nBy weak duality, for any primal point $x$ and any dual feasible point $\\nu$, we have $P(x) \\ge D(\\nu)$.\nSince strong duality holds for the LASSO problem, we have $P(x^\\star) = \\max_\\nu D(\\nu)$.\nLet $\\nu_k$ be the dual feasible point constructed from the primal iterate $x_k$. Because $\\nu_k$ is dual feasible, it must hold that $D(\\nu_k) \\le \\max_\\nu D(\\nu) = P(x^\\star)$.\nTherefore, we have the following inequalities:\n$$\nD(\\nu_k) \\le P(x^\\star) \\le P(x_k)\n$$\nThe primal suboptimality is $P(x_k) - P(x^\\star)$. Using the inequality $D(\\nu_k) \\le P(x^\\star)$, we can write:\n$$\nP(x_k) - P(x^\\star) \\le P(x_k) - D(\\nu_k)\n$$\nThe term on the right is precisely our computable primal-dual gap, $G(x_k, \\nu_k)$. Thus, we have shown:\n$$\nP(x_k) - P(x^\\star) \\le G(x_k, \\nu_k)\n$$\nThis proves that the primal-dual gap $G(x_k, \\nu_k)$ is an upper bound on the true primal suboptimality. Consequently, if the algorithm terminates when $G(x_k, \\nu_k) \\le \\varepsilon$, we are guaranteed that the primal suboptimality $P(x_k) - P(x^\\star)$ is also at most $\\varepsilon$. This validates the proposed stopping criterion.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the ISTA algorithm with a primal-dual gap stopping criterion\n    on a suite of test cases for the LASSO problem.\n    \"\"\"\n\n    def soft_threshold(z, tau):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n    def run_ista(A, b, lam, tol, max_iter):\n        \"\"\"\n        Solves the LASSO problem using ISTA with a primal-dual gap stopping criterion.\n\n        Args:\n            A (np.ndarray): The measurement matrix.\n            b (np.ndarray): The data vector.\n            lam (float): The regularization parameter lambda.\n            tol (float): The tolerance for the primal-dual gap.\n            max_iter (int): The maximum number of iterations.\n\n        Returns:\n            float: The final primal-dual gap.\n        \"\"\"\n        m, n = A.shape\n        \n        # Determine a valid stepsize alpha\n        L = np.linalg.norm(A, 2)**2\n        # A lipshitz constant of 0 can occur if A is a zero matrix.\n        # In this case, x=0 is the solution, and the gradient is constant. Any positive step size will work.\n        # We set alpha=1.0 for this edge case.\n        if L == 0:\n            alpha = 1.0\n        else:\n            alpha = 1.0 / L\n\n        # Initialization\n        x = np.zeros(n)\n        \n        final_gap = np.inf\n\n        for k in range(max_iter):\n            # Primal objective P(x) = 0.5 * ||Ax - b||^2 + lambda * ||x||_1\n            residual = A @ x - b\n            primal_obj = 0.5 * np.linalg.norm(residual)**2 + lam * np.linalg.norm(x, 1)\n\n            # Construct dual feasible point nu\n            # Dual D(nu) = -0.5 * ||nu||^2 - b.T @ nu, s.t. ||A.T @ nu||_inf = lambda\n            # Construction: nu = c * (b - Ax), where c is a scaling factor.\n            # We use c = lambda / max(lambda, ||A.T @ (b-Ax)||_inf)\n            b_minus_Ax = -residual\n            At_b_minus_Ax = A.T @ b_minus_Ax\n            norm_At_b_minus_Ax_inf = np.linalg.norm(At_b_minus_Ax, np.inf)\n\n            # Epsilon to avoid division by zero in the theoretical case where max is zero.\n            # max(lambda, norm) should be non-zero since lambda  0.\n            c = lam / np.maximum(lam, norm_At_b_minus_Ax_inf)\n            nu = c * b_minus_Ax\n\n            # Dual objective D(nu)\n            dual_obj = -0.5 * np.linalg.norm(nu)**2 - b.T @ nu\n\n            # Primal-dual gap G(x, nu) = P(x) - D(nu)\n            gap = primal_obj - dual_obj\n            final_gap = gap\n\n            if gap = tol:\n                break\n\n            # ISTA update\n            grad_f = A.T @ residual\n            x = soft_threshold(x - alpha * grad_f, alpha * lam)\n        \n        return final_gap\n\n    test_cases = [\n        {'m': 40, 'n': 60, 'seed': 123, 'k': 6, 'sigma': 1e-2, 'lam_factor': 0.1, 'tol': 1e-6, 'max_iter': 10000, 'name': 'overparameterized_sparse'},\n        {'m': 30, 'n': 30, 'seed': 2024, 'lam_factor': 1.0, 'tol': 1e-12, 'max_iter': 1000, 'name': 'identity'},\n        {'m': 30, 'n': 80, 'seed': 321, 'lam_factor': 0.01, 'tol': 1e-5, 'max_iter': 20000, 'name': 'underdetermined'},\n        {'m': 50, 'n': 50, 'seed': 999, 'lam_factor': 0.05, 'tol': 1e-5, 'max_iter': 20000, 'name': 'ill_conditioned'}\n    ]\n\n    results = []\n    for case in test_cases:\n        m, n, seed = case['m'], case['n'], case['seed']\n        rng = np.random.default_rng(seed)\n\n        if case['name'] == 'identity':\n            A = np.eye(m)\n            b = rng.standard_normal(size=m)\n        elif case['name'] == 'ill_conditioned':\n            A_base = rng.normal(0, 1.0 / np.sqrt(m), size=(m, n))\n            scaling_factors = 10**(4.0 * np.arange(n) / (n - 1))\n            A = A_base * scaling_factors # Broadcasting column-wise\n            b = rng.standard_normal(size=m)\n        else: # Default case for 'overparameterized_sparse' and 'underdetermined'\n            A = rng.normal(0, 1.0 / np.sqrt(m), size=(m, n))\n            if case['name'] == 'overparameterized_sparse':\n                k, sigma = case['k'], case['sigma']\n                x_true = np.zeros(n)\n                indices = rng.choice(n, k, replace=False)\n                x_true[indices] = rng.standard_normal(size=k)\n                noise = rng.normal(0, sigma, size=m)\n                b = A @ x_true + noise\n            else: # 'underdetermined'\n                b = rng.standard_normal(size=m)\n\n        lam = case['lam_factor'] * np.linalg.norm(A.T @ b, np.inf)\n        # Ensure lambda is strictly positive\n        if lam == 0:\n            lam = 1e-10\n\n        final_gap = run_ista(A, b, lam, case['tol'], case['max_iter'])\n        results.append(final_gap)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3392931"}]}