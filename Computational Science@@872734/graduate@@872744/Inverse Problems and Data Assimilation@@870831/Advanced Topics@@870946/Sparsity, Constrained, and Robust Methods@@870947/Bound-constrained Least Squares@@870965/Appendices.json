{"hands_on_practices": [{"introduction": "A key operation in solving bound-constrained problems is projecting a candidate solution onto the feasible set to enforce the bounds. This exercise provides a concrete, hands-on calculation of this projection for a vector and a set of box constraints. Mastering this separable, component-wise operation is the first step toward building and understanding iterative solvers for more complex inverse problems. [@problem_id:3369454]", "problem": "Let $y \\in \\mathbb{R}^{6}$ be given by $y = (3.7,\\,-5.2,\\,-3.0,\\,9.1,\\,-2.3,\\,14.8)^{\\top}$. Consider the bound-constrained least-squares projection of $y$ onto the box $[l,u] := \\{x \\in \\mathbb{R}^{6} : l \\le x \\le u\\}$, where the bounds are specified componentwise by\n$l = (0,\\,-\\infty,\\,-1,\\,4,\\,-\\infty,\\,10)^{\\top}$ and $u = (2,\\,0,\\,\\infty,\\,\\infty,\\,1,\\,12)^{\\top}$. Here $-\\infty$ and $\\infty$ denote the absence of a lower or upper bound, respectively.\n\nThe Euclidean projection $P_{[l,u]}(y)$ is defined as the unique minimizer of the strictly convex problem\n$\\min\\{ \\tfrac{1}{2}\\|x - y\\|_{2}^{2} : l \\le x \\le u \\}$.\n\nTasks:\n1. Compute $P_{[l,u]}(y)$.\n2. Starting from the definition of the projection as the unique minimizer of the above problem and using only fundamental properties of convexity and first-order optimality, verify that the variational inequality characterization holds: for all $z \\in [l,u]$,\n$\\langle y - P_{[l,u]}(y),\\, z - P_{[l,u]}(y) \\rangle \\le 0$.\n3. Report the exact value of the squared Euclidean distance $\\|y - P_{[l,u]}(y)\\|_{2}^{2}$.\n\nProvide your final numerical answer to Task 3 as an exact value (no rounding).", "solution": "The user-provided problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- The vector $y \\in \\mathbb{R}^{6}$ is given as $y = (3.7,\\,-5.2,\\,-3.0,\\,9.1,\\,-2.3,\\,14.8)^{\\top}$.\n- The feasible set is a box $[l,u] := \\{x \\in \\mathbb{R}^{6} : l \\le x \\le u\\}$.\n- The lower bounds are $l = (0,\\,-\\infty,\\,-1,\\,4,\\,-\\infty,\\,10)^{\\top}$.\n- The upper bounds are $u = (2,\\,0,\\,\\infty,\\,\\infty,\\,1,\\,12)^{\\top}$.\n- The Euclidean projection $P_{[l,u]}(y)$ is defined as the unique minimizer of the problem $\\min\\{ \\tfrac{1}{2}\\|x - y\\|_{2}^{2} : l \\le x \\le u \\}$.\n- The variational inequality to be verified is $\\langle y - P_{[l,u]}(y),\\, z - P_{[l,u]}(y) \\rangle \\le 0$ for all $z \\in [l,u]$.\n- The final task is to report the exact value of $\\|y - P_{[l,u]}(y)\\|_{2}^{2}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is a standard exercise in convex optimization, specifically concerning projection onto a convex set (a box). All concepts are rooted in well-established mathematical principles of analysis and optimization. The problem is scientifically sound.\n2.  **Well-Posed:** The objective function $f(x) = \\frac{1}{2}\\|x - y\\|_{2}^{2}$ is strictly convex and continuous. The feasible set $[l,u]$ is a closed and convex set (a hyperrectangle). A continuous strictly convex function attains a unique minimum over a non-empty closed convex set. The problem is well-posed.\n3.  **Objective:** The problem is stated using precise mathematical language and definitions. There are no subjective or opinion-based statements.\n4.  **Complete and Consistent:** All necessary data ($y$, $l$, $u$) and definitions are provided. There are no contradictions in the setup.\nThe problem is deemed **valid**.\n\n### Solution\nThe problem asks for three tasks to be completed. Let $x^* = P_{[l,u]}(y)$.\n\n**Task 1: Compute $P_{[l,u]}(y)$**\n\nThe objective function to be minimized is $L(x) = \\frac{1}{2} \\|x - y\\|_{2}^{2} = \\frac{1}{2} \\sum_{i=1}^{6} (x_i - y_i)^2$. The constraints are $l_i \\le x_i \\le u_i$ for each component $i=1, \\dots, 6$. Since both the objective function and the constraints are separable, the $6$-dimensional optimization problem decouples into $6$ independent $1$-dimensional problems:\n$$\n\\min_{x_i} \\frac{1}{2} (x_i - y_i)^2 \\quad \\text{subject to} \\quad l_i \\le x_i \\le u_i\n$$\nfor each $i \\in \\{1, 2, 3, 4, 5, 6\\}$.\n\nThe solution to each $1$D problem is the projection of the scalar $y_i$ onto the interval $[l_i, u_i]$. This is given by the formula:\n$$\nx_i^* = \\max(l_i, \\min(u_i, y_i))\n$$\n\nWe apply this formula for each component:\n- For $i=1$: $y_1=3.7$, $[l_1, u_1] = [0, 2]$.\n  $x_1^* = \\max(0, \\min(2, 3.7)) = \\max(0, 2) = 2$.\n- For $i=2$: $y_2=-5.2$, $[l_2, u_2] = [-\\infty, 0]$.\n  $x_2^* = \\max(-\\infty, \\min(0, -5.2)) = \\min(0, -5.2) = -5.2$.\n- For $i=3$: $y_3=-3.0$, $[l_3, u_3] = [-1, \\infty]$.\n  $x_3^* = \\max(-1, \\min(\\infty, -3.0)) = \\max(-1, -3.0) = -1$.\n- For $i=4$: $y_4=9.1$, $[l_4, u_4] = [4, \\infty]$.\n  $x_4^* = \\max(4, \\min(\\infty, 9.1)) = \\max(4, 9.1) = 9.1$.\n- For $i=5$: $y_5=-2.3$, $[l_5, u_5] = [-\\infty, 1]$.\n  $x_5^* = \\max(-\\infty, \\min(1, -2.3)) = \\min(1, -2.3) = -2.3$.\n- For $i=6$: $y_6=14.8$, $[l_6, u_6] = [10, 12]$.\n  $x_6^* = \\max(10, \\min(12, 14.8)) = \\max(10, 12) = 12$.\n\nCombining these components, the projection is:\n$$\nx^* = P_{[l,u]}(y) = (2, -5.2, -1, 9.1, -2.3, 12)^{\\top}\n$$\n\n**Task 2: Verify the variational inequality**\n\nThe problem is to minimize a differentiable convex function $f(x) = \\frac{1}{2}\\|x-y\\|_2^2$ over a convex set $C = [l,u]$. A fundamental result in convex analysis states that a point $x^* \\in C$ is a minimizer of $f$ over $C$ if and only if it satisfies the first-order optimality condition:\n$$\n\\langle \\nabla f(x^*), z - x^* \\rangle \\ge 0, \\quad \\forall z \\in C\n$$\nThe gradient of $f(x)$ is $\\nabla f(x) = x - y$. Substituting this into the optimality condition, we get:\n$$\n\\langle x^* - y, z - x^* \\rangle \\ge 0, \\quad \\forall z \\in C\n$$\nMultiplying the inner product by $-1$ reverses the inequality:\n$$\n\\langle -(x^* - y), z - x^* \\rangle \\le 0\n$$\n$$\n\\langle y - x^*, z - x^* \\rangle \\le 0, \\quad \\forall z \\in C\n$$\nThis is precisely the variational inequality characterization of the projection. This derivation from first-order principles verifies that the characterization holds for any projection onto a convex set.\n\nWe can also verify this component-wise for the specific projection onto a box. The inequality is $\\sum_{i=1}^{6} (y_i - x_i^*)(z_i - x_i^*) \\le 0$. We show that each term in the sum is non-positive. For any $i$, consider the term $(y_i - x_i^*)(z_i - x_i^*)$, where $z_i \\in [l_i, u_i]$.\n- Case 1: $l_i  y_i  u_i$. The point $y_i$ is in the interior of the interval. The projection is $x_i^* = y_i$. The term is $(y_i - y_i)(z_i - y_i) = 0$.\n- Case 2: $y_i \\le l_i$. The projection is $x_i^* = l_i$. The factor $y_i - x_i^* = y_i - l_i \\le 0$. For any $z_i \\in [l_i, u_i]$, we have $z_i \\ge l_i$, so the factor $z_i - x_i^* = z_i - l_i \\ge 0$. The product is non-positive.\n- Case 3: $y_i \\ge u_i$. The projection is $x_i^* = u_i$. The factor $y_i - x_i^* = y_i - u_i \\ge 0$. For any $z_i \\in [l_i, u_i]$, we have $z_i \\le u_i$, so the factor $z_i - x_i^* = z_i - u_i \\le 0$. The product is non-positive.\nIn all cases, $(y_i - x_i^*)(z_i - x_i^*) \\le 0$. Summing over all $i$ confirms the variational inequality.\n\n**Task 3: Report the squared Euclidean distance**\n\nWe need to compute $\\|y - x^*\\|_{2}^{2}$. First, we compute the difference vector $d = y - x^*$:\n$d = (3.7 - 2, -5.2 - (-5.2), -3.0 - (-1), 9.1 - 9.1, -2.3 - (-2.3), 14.8 - 12)^{\\top}$\n$d = (1.7, 0, -2.0, 0, 0, 2.8)^{\\top}$\n\nThe squared Euclidean distance is the sum of the squares of the components of $d$:\n$$\n\\|y - x^*\\|_{2}^{2} = d_1^2 + d_2^2 + d_3^2 + d_4^2 + d_5^2 + d_6^2\n$$\n$$\n\\|y - x^*\\|_{2}^{2} = (1.7)^2 + (0)^2 + (-2.0)^2 + (0)^2 + (0)^2 + (2.8)^2\n$$\nWe compute the individual squared terms:\n- $(1.7)^2 = 2.89$\n- $(0)^2 = 0$\n- $(-2.0)^2 = 4.0$\n- $(2.8)^2 = 7.84$\n\nSumming these values:\n$$\n\\|y - x^*\\|_{2}^{2} = 2.89 + 0 + 4.0 + 0 + 0 + 7.84 = 14.73\n$$\nThe exact value of the squared Euclidean distance is $14.73$.", "answer": "$$\n\\boxed{14.73}\n$$", "id": "3369454"}, {"introduction": "With the projection operator as a tool, we can construct iterative algorithms to find the constrained minimum. This practice guides you through one full iteration of the projected gradient method, a foundational algorithm for constrained optimization. You will combine a standard gradient descent step with a projection and an Armijo line search, learning how to systematically improve a solution while rigorously respecting the physical or numerical bounds of the problem. [@problem_id:3369390]", "problem": "Consider a linear data assimilation setting with a state vector $\\boldsymbol{x} \\in \\mathbb{R}^{3}$ and a linear observation operator $\\boldsymbol{A} \\in \\mathbb{R}^{3 \\times 3}$, where the observations are modeled as $\\boldsymbol{y} \\approx \\boldsymbol{A}\\boldsymbol{x}$ with observed data $\\boldsymbol{b} \\in \\mathbb{R}^{3}$. We seek to minimize the least-squares misfit subject to bound constraints. The objective function is\n$$\nf(\\boldsymbol{x}) = \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{b}\\|^{2},\n$$\nand the feasible set is the box $\\mathcal{B} = \\{\\boldsymbol{x} \\in \\mathbb{R}^{3} : \\boldsymbol{l} \\le \\boldsymbol{x} \\le \\boldsymbol{u}\\}$ with componentwise bounds.\n\nStarting strictly from the definition of $f(\\boldsymbol{x})$ and the rules of differentiation for compositions of linear maps and quadratic forms, derive the gradient $\\nabla f(\\boldsymbol{x})$. Then, using the Projected Gradient (PG) method with an Armijo backtracking line search, perform one iteration from the current state $\\boldsymbol{x}^{(0)}$ with the following data:\n$$\n\\boldsymbol{A} = \\begin{pmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  3 \\end{pmatrix}, \\quad\n\\boldsymbol{b} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 3 \\end{pmatrix}, \\quad\n\\boldsymbol{x}^{(0)} = \\begin{pmatrix} 0.8 \\\\ -0.5 \\\\ 1.8 \\end{pmatrix},\n$$\nand bounds\n$$\n\\boldsymbol{l} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0.5 \\end{pmatrix}, \\quad\n\\boldsymbol{u} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}.\n$$\nDefine the projection onto the box $\\mathcal{B}$ componentwise by $P_{[l,u]}(\\boldsymbol{z}) = \\min(\\max(\\boldsymbol{z}, \\boldsymbol{l}), \\boldsymbol{u})$, where the minimum and maximum are taken componentwise. Use an initial step size $\\alpha_{0} = 1$, a backtracking factor $\\gamma = \\frac{1}{2}$, and an Armijo constant $\\sigma = \\frac{1}{10}$. At a current iterate $\\boldsymbol{x}$ with gradient $\\boldsymbol{g} = \\nabla f(\\boldsymbol{x})$, define the trial point for step size $\\alpha$ by\n$$\n\\boldsymbol{y}(\\alpha) = P_{[l,u]}\\big(\\boldsymbol{x} - \\alpha \\boldsymbol{g}\\big).\n$$\nAccept a step size $\\alpha$ if the Armijo condition\n$$\nf\\big(\\boldsymbol{y}(\\alpha)\\big) \\le f(\\boldsymbol{x}) + \\sigma\\, \\nabla f(\\boldsymbol{x})^{\\top}\\big(\\boldsymbol{y}(\\alpha) - \\boldsymbol{x}\\big)\n$$\nis satisfied; otherwise replace $\\alpha \\leftarrow \\gamma \\alpha$ and repeat until acceptance. Carry out this procedure for one iteration starting from $\\boldsymbol{x}^{(0)}$ with the numerical values above and report the resulting next iterate $\\boldsymbol{x}^{(1)} = \\boldsymbol{y}(\\alpha)$ as your final answer. No rounding is required. Express the final answer as a row vector.", "solution": "The problem asks for one iteration of the Projected Gradient (PG) method for a bound-constrained least-squares problem. The process involves first deriving the gradient of the objective function, then applying the PG algorithm with an Armijo backtracking line search.\n\nFirst, we derive the gradient of the objective function $f(\\boldsymbol{x}) = \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{b}\\|^{2}$.\nThe squared Euclidean norm can be written as an inner product:\n$$\nf(\\boldsymbol{x}) = \\frac{1}{2} (\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{b})^{\\top}(\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{b})\n$$\nExpanding this expression gives:\n$$\nf(\\boldsymbol{x}) = \\frac{1}{2} \\left( (\\boldsymbol{A}\\boldsymbol{x})^{\\top} - \\boldsymbol{b}^{\\top} \\right) (\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{b}) = \\frac{1}{2} \\left( \\boldsymbol{x}^{\\top}\\boldsymbol{A}^{\\top}\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{x}^{\\top}\\boldsymbol{A}^{\\top}\\boldsymbol{b} - \\boldsymbol{b}^{\\top}\\boldsymbol{A}\\boldsymbol{x} + \\boldsymbol{b}^{\\top}\\boldsymbol{b} \\right)\n$$\nSince $\\boldsymbol{b}^{\\top}\\boldsymbol{A}\\boldsymbol{x}$ is a scalar, it is equal to its own transpose, $(\\boldsymbol{b}^{\\top}\\boldsymbol{A}\\boldsymbol{x})^{\\top} = \\boldsymbol{x}^{\\top}\\boldsymbol{A}^{\\top}\\boldsymbol{b}$. Thus, the expression simplifies to:\n$$\nf(\\boldsymbol{x}) = \\frac{1}{2} \\boldsymbol{x}^{\\top}\\boldsymbol{A}^{\\top}\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{b}^{\\top}\\boldsymbol{A}\\boldsymbol{x} + \\frac{1}{2} \\boldsymbol{b}^{\\top}\\boldsymbol{b}\n$$\nTo find the gradient $\\nabla f(\\boldsymbol{x})$, we differentiate with respect to $\\boldsymbol{x}$. Using the rules for vector calculus, specifically $\\nabla_{\\boldsymbol{x}}(\\boldsymbol{x}^{\\top}\\boldsymbol{Q}\\boldsymbol{x}) = (\\boldsymbol{Q} + \\boldsymbol{Q}^{\\top})\\boldsymbol{x}$ and $\\nabla_{\\boldsymbol{x}}(\\boldsymbol{c}^{\\top}\\boldsymbol{x}) = \\boldsymbol{c}$, we get:\n$$\n\\nabla f(\\boldsymbol{x}) = \\frac{1}{2} (\\boldsymbol{A}^{\\top}\\boldsymbol{A} + (\\boldsymbol{A}^{\\top}\\boldsymbol{A})^{\\top})\\boldsymbol{x} - (\\boldsymbol{b}^{\\top}\\boldsymbol{A})^{\\top}\n$$\nThe matrix $\\boldsymbol{A}^{\\top}\\boldsymbol{A}$ is symmetric, so $(\\boldsymbol{A}^{\\top}\\boldsymbol{A})^{\\top} = \\boldsymbol{A}^{\\top}\\boldsymbol{A}$. Also, $(\\boldsymbol{b}^{\\top}\\boldsymbol{A})^{\\top} = \\boldsymbol{A}^{\\top}\\boldsymbol{b}$. The gradient is therefore:\n$$\n\\nabla f(\\boldsymbol{x}) = \\frac{1}{2} (2\\boldsymbol{A}^{\\top}\\boldsymbol{A})\\boldsymbol{x} - \\boldsymbol{A}^{\\top}\\boldsymbol{b} = \\boldsymbol{A}^{\\top}\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{A}^{\\top}\\boldsymbol{b} = \\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{b})\n$$\nThis is the required expression for the gradient.\n\nNow, we perform one iteration of the PG method starting from $\\boldsymbol{x}^{(0)}$.\nThe current iterate is $\\boldsymbol{x}^{(0)} = \\begin{pmatrix} 0.8 \\\\ -0.5 \\\\ 1.8 \\end{pmatrix}$. The matrices and vectors are:\n$$\n\\boldsymbol{A} = \\begin{pmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  3 \\end{pmatrix}, \\quad \\boldsymbol{b} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 3 \\end{pmatrix}, \\quad \\boldsymbol{l} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0.5 \\end{pmatrix}, \\quad \\boldsymbol{u} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}\n$$\nFirst, we compute the gradient $\\boldsymbol{g}^{(0)} = \\nabla f(\\boldsymbol{x}^{(0)})$.\nThe residual is $\\boldsymbol{A}\\boldsymbol{x}^{(0)} - \\boldsymbol{b}$:\n$$\n\\boldsymbol{A}\\boldsymbol{x}^{(0)} - \\boldsymbol{b} = \\begin{pmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} 0.8 \\\\ -0.5 \\\\ 1.8 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1.6 \\\\ -0.5 \\\\ 5.4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0.6 \\\\ 0.5 \\\\ 2.4 \\end{pmatrix}\n$$\nThe matrix $\\boldsymbol{A}$ is diagonal and thus symmetric, so $\\boldsymbol{A}^{\\top} = \\boldsymbol{A}$.\n$$\n\\boldsymbol{g}^{(0)} = \\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{x}^{(0)} - \\boldsymbol{b}) = \\begin{pmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} 0.6 \\\\ 0.5 \\\\ 2.4 \\end{pmatrix} = \\begin{pmatrix} 1.2 \\\\ 0.5 \\\\ 7.2 \\end{pmatrix}\n$$\nNext, we perform the Armijo backtracking line search, starting with $\\alpha_0 = 1$. The trial point is $\\boldsymbol{y}(\\alpha) = P_{[\\boldsymbol{l},\\boldsymbol{u}]}(\\boldsymbol{x}^{(0)} - \\alpha \\boldsymbol{g}^{(0)})$. We must check if the Armijo condition is satisfied for $\\alpha = 1$:\n$$\nf\\big(\\boldsymbol{y}(1)\\big) \\le f(\\boldsymbol{x}^{(0)}) + \\sigma\\, (\\boldsymbol{g}^{(0)})^{\\top}\\big(\\boldsymbol{y}(1) - \\boldsymbol{x}^{(0)}\\big)\n$$\nLet's evaluate each term for $\\alpha = 1$. First, the trial point $\\boldsymbol{y}(1)$:\nThe unprojected point is $\\boldsymbol{x}^{(0)} - 1 \\cdot \\boldsymbol{g}^{(0)}$:\n$$\n\\begin{pmatrix} 0.8 \\\\ -0.5 \\\\ 1.8 \\end{pmatrix} - \\begin{pmatrix} 1.2 \\\\ 0.5 \\\\ 7.2 \\end{pmatrix} = \\begin{pmatrix} -0.4 \\\\ -1.0 \\\\ -5.4 \\end{pmatrix}\n$$\nProjecting this point onto the box $\\mathcal{B}$ defined by $\\boldsymbol{l}$ and $\\boldsymbol{u}$:\n$$\n\\boldsymbol{y}(1) = P_{[\\boldsymbol{l},\\boldsymbol{u}]}\\left(\\begin{pmatrix} -0.4 \\\\ -1.0 \\\\ -5.4 \\end{pmatrix}\\right) = \\begin{pmatrix} \\min(\\max(-0.4, 0), 1) \\\\ \\min(\\max(-1.0, -1), 2) \\\\ \\min(\\max(-5.4, 0.5), 2) \\end{pmatrix} = \\begin{pmatrix} \\min(0, 1) \\\\ \\min(-1, 2) \\\\ \\min(0.5, 2) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0.5 \\end{pmatrix}\n$$\nNow, we evaluate the two sides of the Armijo inequality.\nFor the left-hand side (LHS), we compute $f(\\boldsymbol{y}(1))$:\n$$\n\\boldsymbol{A}\\boldsymbol{y}(1) - \\boldsymbol{b} = \\begin{pmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\\\ 0.5 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1.5 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\\\ -1.5 \\end{pmatrix}\n$$\n$$\nf(\\boldsymbol{y}(1)) = \\frac{1}{2}\\left\\| \\begin{pmatrix} -1 \\\\ 0 \\\\ -1.5 \\end{pmatrix} \\right\\|^2 = \\frac{1}{2}((-1)^2 + 0^2 + (-1.5)^2) = \\frac{1}{2}(1 + 2.25) = \\frac{1}{2}(3.25) = 1.625\n$$\nFor the right-hand side (RHS), we first need $f(\\boldsymbol{x}^{(0)})$ and the directional derivative term.\n$$\nf(\\boldsymbol{x}^{(0)}) = \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x}^{(0)} - \\boldsymbol{b}\\|^2 = \\frac{1}{2}\\left\\| \\begin{pmatrix} 0.6 \\\\ 0.5 \\\\ 2.4 \\end{pmatrix} \\right\\|^2 = \\frac{1}{2}(0.6^2 + 0.5^2 + 2.4^2) = \\frac{1}{2}(0.36 + 0.25 + 5.76) = \\frac{1}{2}(6.37) = 3.185\n$$\nThe directional term is $\\sigma\\, (\\boldsymbol{g}^{(0)})^{\\top}(\\boldsymbol{y}(1) - \\boldsymbol{x}^{(0)})$. The vector difference is:\n$$\n\\boldsymbol{y}(1) - \\boldsymbol{x}^{(0)} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0.5 \\end{pmatrix} - \\begin{pmatrix} 0.8 \\\\ -0.5 \\\\ 1.8 \\end{pmatrix} = \\begin{pmatrix} -0.8 \\\\ -0.5 \\\\ -1.3 \\end{pmatrix}\n$$\nThe dot product with the gradient is:\n$$\n(\\boldsymbol{g}^{(0)})^{\\top}(\\boldsymbol{y}(1) - \\boldsymbol{x}^{(0)}) = \\begin{pmatrix} 1.2  0.5  7.2 \\end{pmatrix} \\begin{pmatrix} -0.8 \\\\ -0.5 \\\\ -1.3 \\end{pmatrix} = (1.2)(-0.8) + (0.5)(-0.5) + (7.2)(-1.3) = -0.96 - 0.25 - 9.36 = -10.57\n$$\nThe RHS of the Armijo inequality is, with $\\sigma = \\frac{1}{10} = 0.1$:\n$$\nf(\\boldsymbol{x}^{(0)}) + \\sigma\\, (\\boldsymbol{g}^{(0)})^{\\top}(\\boldsymbol{y}(1) - \\boldsymbol{x}^{(0)}) = 3.185 + 0.1(-10.57) = 3.185 - 1.057 = 2.128\n$$\nNow we check the condition: Is $f(\\boldsymbol{y}(1)) \\le 2.128$?\n$$\n1.625 \\le 2.128\n$$\nThe inequality holds. Therefore, the step size $\\alpha=1$ is accepted. The next iterate is $\\boldsymbol{x}^{(1)} = \\boldsymbol{y}(1)$.\n$$\n\\boldsymbol{x}^{(1)} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0.5 \\end{pmatrix}\n$$\nThe problem asks for the result as a row vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0  -1  0.5 \\end{pmatrix}}\n$$", "id": "3369390"}, {"introduction": "Once an algorithm converges to a solution, it is crucial to analyze its properties and understand the impact of the constraints. This exercise explores the relationship between the bound-constrained solution and the ideal, unconstrained least-squares solution. You will derive and compute the \"residual inflation,\" a term that quantifies the increase in data misfit that is paid as the price for enforcing the bounds, providing deep insight into the trade-off between model constraints and data fidelity. [@problem_id:3369377]", "problem": "Consider the bound-constrained linear least-squares problem: minimize the squared Euclidean residual subject to simple bounds,\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|A x - b\\|_{2}^{2} \\quad \\text{subject to} \\quad l \\leq x \\leq u,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ with full column rank, $b \\in \\mathbb{R}^{m}$, and $l,u \\in \\mathbb{R}^{n}$ satisfy $l_{i}  u_{i}$ for all $i$. Let $x^{\\dagger}$ denote the unique unconstrained least-squares minimizer, defined as the unique solution to the normal equations $A^{\\top} A x = A^{\\top} b$. Assume that $A$ has full column rank so that $A^{\\top} A$ is positive definite and the Moore–Penrose pseudoinverse satisfies $x^{\\dagger} = (A^{\\top} A)^{-1} A^{\\top} b$.\n\nTasks:\n- Using only the core definitions of least squares, the normal equations for unconstrained least squares, and the Karush–Kuhn–Tucker (KKT) conditions for convex quadratic programs with bound constraints, establish necessary and sufficient conditions on $(A,b,l,u)$ under which the unconstrained minimizer satisfies the strict componentwise inequalities $l  x^{\\dagger}  u$ and hence is also the unique constrained minimizer.\n- Starting from the orthogonal decomposition of $b$ into a component in the column space of $A$ and its orthogonal complement, and without invoking any pre-packaged projection formula, derive an exact expression for the “residual inflation”\n$$\n\\|A x^{\\star} - b\\|_{2}^{2} - \\|A x^{\\dagger} - b\\|_{2}^{2},\n$$\nwhere $x^{\\star}$ is the constrained minimizer, in terms of $A$, $x^{\\dagger}$, and $x^{\\star}$.\n\nThen, consider the explicit instance with\n$$\nA \\,=\\, \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix}, \\quad\nb \\,=\\, \\begin{pmatrix} 3 \\\\ -1 \\\\ 4 \\end{pmatrix}, \\quad\nl \\,=\\, \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix}, \\quad\nu \\,=\\, \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}.\n$$\nCompute the residual inflation $\\|A x^{\\star} - b\\|_{2}^{2} - \\|A x^{\\dagger} - b\\|_{2}^{2}$ for this instance. Express your final answer as an exact real number. No rounding is required.", "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and contains sufficient information for a unique solution. The problem is a standard bound-constrained quadratic program. The objective function $f(x) = \\|Ax - b\\|_{2}^{2}$ is strictly convex because matrix $A$ has full column rank, which implies $A^{\\top}A$ is positive definite. The feasible region, defined by the hyperrectangle $l \\leq x \\leq u$, is a non-empty, closed, and convex set. A strictly convex function over a non-empty, closed, convex set has a unique minimum. Therefore, the problem is well-posed and a unique constrained minimizer $x^{\\star}$ exists. The provided data for the specific instance is dimensionally consistent ($m=3, n=2$). The problem is valid.\n\n### Part 1: Conditions for the unconstrained minimizer to be the constrained minimizer\n\nThe optimization problem is:\n$$ \\min_{x \\in \\mathbb{R}^{n}} f(x) = \\|Ax - b\\|_{2}^{2} \\quad \\text{subject to} \\quad l \\leq x \\leq u $$\nThis can be written as a quadratic program by expanding the objective:\n$$ f(x) = x^{\\top}A^{\\top}Ax - 2b^{\\top}Ax + b^{\\top}b $$\nThe gradient of the objective function is $\\nabla f(x) = 2A^{\\top}Ax - 2A^{\\top}b$. The unconstrained minimizer $x^{\\dagger}$ is found by setting the gradient to zero, which yields the normal equations $A^{\\top}A x^{\\dagger} = A^{\\top}b$. By definition, $\\nabla f(x^{\\dagger}) = 0$.\n\nThe constraints can be written as $g_i(x) = l_i - x_i \\leq 0$ and $h_i(x) = x_i - u_i \\leq 0$ for $i=1, \\dots, n$. The Karush-Kuhn-Tucker (KKT) conditions for a point $x^{\\star}$ to be the minimizer are:\n1.  **Stationarity:** $\\nabla f(x^{\\star}) - \\mu^{\\star} + \\lambda^{\\star} = 0$, where $\\mu^{\\star}, \\lambda^{\\star} \\in \\mathbb{R}^{n}$ are vectors of Lagrange multipliers.\n2.  **Primal Feasibility:** $l \\leq x^{\\star} \\leq u$.\n3.  **Dual Feasibility:** $\\mu_i^{\\star} \\geq 0$ and $\\lambda_i^{\\star} \\geq 0$ for all $i$.\n4.  **Complementary Slackness:** $\\mu_i^{\\star}(l_i - x_i^{\\star}) = 0$ and $\\lambda_i^{\\star}(x_i^{\\star} - u_i) = 0$ for all $i$.\n\nWe want to find the conditions under which the unconstrained minimizer $x^{\\dagger}$ is also the unique constrained minimizer and satisfies the strict inequalities $l  x^{\\dagger}  u$. Let's assume $x^{\\star} = x^{\\dagger}$ and $l  x^{\\dagger}  u$.\n\nFrom the Primal Feasibility condition, $l \\leq x^{\\dagger} \\leq u$ is satisfied by our assumption.\n\nFrom the Stationarity condition, using $\\nabla f(x^{\\dagger})=0$, we have:\n$$ 0 - \\mu^{\\star} + \\lambda^{\\star} = 0 \\implies \\mu^{\\star} = \\lambda^{\\star} $$\nThe Lagrange multipliers for the lower and upper bounds must be equal, component-wise.\n\nFrom the Complementary Slackness condition, for each component $i$:\nSince $l_i  x^{\\dagger}_i  u_i$, we have $l_i - x^{\\dagger}_i  0$ and $x^{\\dagger}_i - u_i  0$.\nThe conditions $\\mu_i^{\\star}(l_i - x^{\\dagger}_i) = 0$ and $\\lambda_i^{\\star}(x^{\\dagger}_i - u_i) = 0$ imply that $\\mu_i^{\\star}=0$ and $\\lambda_i^{\\star}=0$ for all $i=1, \\dots, n$. Thus, $\\mu^{\\star} = 0$ and $\\lambda^{\\star} = 0$.\n\nThis choice of multipliers satisfies the Stationarity condition ($\\mu^{\\star}=\\lambda^{\\star}$) and the Dual Feasibility condition ($\\mu^{\\star} \\geq 0, \\lambda^{\\star} \\geq 0$). Since all KKT conditions are satisfied, and the problem is strictly convex, $x^{\\dagger}$ is indeed the unique constrained minimizer.\n\nThe assumption we made, $l  x^{\\dagger}  u$, is therefore the condition itself. The unconstrained minimizer is the unique constrained minimizer and satisfies the strict inequalities if and only if it lies strictly inside the feasible box. Expressed in terms of the problem data $(A, b, l, u)$, this condition is:\n$$ l  (A^{\\top}A)^{-1}A^{\\top}b  u \\quad (\\text{component-wise}) $$\n\n### Part 2: Derivation of the Residual Inflation\n\nWe want to derive an expression for $\\|A x^{\\star} - b\\|_{2}^{2} - \\|A x^{\\dagger} - b\\|_{2}^{2}$.\nThe vector $b$ can be decomposed into a component in the column space of $A$, $\\mathcal{R}(A)$, and a component in its orthogonal complement, $\\mathcal{R}(A)^{\\perp}$. The orthogonal projector onto $\\mathcal{R}(A)$ is $P_A = A(A^{\\top}A)^{-1}A^{\\top}$.\nThe component of $b$ in $\\mathcal{R}(A)$ is $P_A b = A(A^{\\top}A)^{-1}A^{\\top}b = A x^{\\dagger}$.\nThe component of $b$ in $\\mathcal{R}(A)^{\\perp}$ is $(I-P_A)b = b - Ax^{\\dagger} = -(Ax^{\\dagger} - b)$.\nThe vector $Ax^{\\dagger}-b$ is the residual of the unconstrained problem.\n\nLet's analyze the residual of an arbitrary vector $x$: $Ax - b$. We can rewrite it by adding and subtracting $Ax^{\\dagger}$:\n$$ Ax - b = (Ax - Ax^{\\dagger}) + (Ax^{\\dagger} - b) = A(x-x^{\\dagger}) + (Ax^{\\dagger} - b) $$\nThe term $A(x-x^{\\dagger})$ is in $\\mathcal{R}(A)$ because it is a linear combination of the columns of $A$. The term $(Ax^{\\dagger} - b)$ is in $\\mathcal{R}(A)^{\\perp}$.\nBy the Pythagorean theorem, for any two orthogonal vectors $v_1, v_2$, $\\|v_1+v_2\\|^2 = \\|v_1\\|^2 + \\|v_2\\|^2$. Applying this here:\n$$ \\|Ax - b\\|_{2}^{2} = \\|A(x-x^{\\dagger})\\|_{2}^{2} + \\|Ax^{\\dagger} - b\\|_{2}^{2} $$\nThis identity holds for any $x \\in \\mathbb{R}^n$. In particular, it holds for the constrained minimizer $x^{\\star}$:\n$$ \\|A x^{\\star} - b\\|_{2}^{2} = \\|A(x^{\\star}-x^{\\dagger})\\|_{2}^{2} + \\|A x^{\\dagger} - b\\|_{2}^{2} $$\nRearranging this equation gives the desired expression for the residual inflation:\n$$ \\|A x^{\\star} - b\\|_{2}^{2} - \\|A x^{\\dagger} - b\\|_{2}^{2} = \\|A(x^{\\star} - x^{\\dagger})\\|_{2}^{2} $$\n\n### Part 3: Computation for the specific instance\n\nWe are given:\n$$ A = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 3 \\\\ -1 \\\\ 4 \\end{pmatrix}, \\quad l = \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix}, \\quad u = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} $$\n\n**Step 1: Compute the unconstrained minimizer $x^{\\dagger}$.**\nWe first compute $A^{\\top}A$ and $A^{\\top}b$:\n$$ A^{\\top}A = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I_2 $$\n$$ A^{\\top}b = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix} $$\nThe normal equations are $I_2 x = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}$, which gives the unconstrained solution:\n$$ x^{\\dagger} = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix} $$\n\n**Step 2: Compute the constrained minimizer $x^{\\star}$.**\nThe objective function to minimize is:\n$$ \\|Ax - b\\|_{2}^{2} = \\left\\| \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ -1 \\\\ 4 \\end{pmatrix} \\right\\|_{2}^{2} = \\left\\| \\begin{pmatrix} x_1 - 3 \\\\ x_2 + 1 \\\\ -4 \\end{pmatrix} \\right\\|_{2}^{2} = (x_1 - 3)^{2} + (x_2 + 1)^{2} + 16 $$\nThe constraints are $0 \\leq x_1 \\leq 2$ and $-2 \\leq x_2 \\leq 0$.\nThe problem is separable, meaning we can minimize with respect to $x_1$ and $x_2$ independently.\n- For $x_1$: Minimize $g_1(x_1) = (x_1 - 3)^{2}$ subject to $0 \\leq x_1 \\leq 2$. The unconstrained minimizer is $x_1 = 3$, which is outside the interval $[0, 2]$. The function $g_1(x_1)$ is decreasing on $[0, 2]$. Thus, the minimum is at the right endpoint, $x^{\\star}_1 = 2$.\n- For $x_2$: Minimize $g_2(x_2) = (x_2 + 1)^{2}$ subject to $-2 \\leq x_2 \\leq 0$. The unconstrained minimizer is $x_2 = -1$, which lies within the interval $[-2, 0]$. Thus, the minimum is at $x^{\\star}_2 = -1$.\n\nThe constrained minimizer is:\n$$ x^{\\star} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} $$\n\n**Step 3: Compute the residual inflation.**\nUsing the derived formula, the residual inflation is $\\|A(x^{\\star} - x^{\\dagger})\\|_{2}^{2}$.\nFirst, find the difference vector:\n$$ x^{\\star} - x^{\\dagger} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} $$\nNext, multiply by $A$:\n$$ A(x^{\\star} - x^{\\dagger}) = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nFinally, compute the squared Euclidean norm:\n$$ \\|A(x^{\\star} - x^{\\dagger})\\|_{2}^{2} = (-1)^{2} + 0^{2} + 0^{2} = 1 $$\nThe residual inflation is $1$.", "answer": "$$\\boxed{1}$$", "id": "3369377"}]}