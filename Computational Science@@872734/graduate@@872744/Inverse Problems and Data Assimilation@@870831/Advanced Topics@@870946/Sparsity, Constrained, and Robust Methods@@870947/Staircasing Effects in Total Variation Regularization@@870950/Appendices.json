{"hands_on_practices": [{"introduction": "To build a solid intuition for the staircasing effect, we begin with a foundational exercise. Here, we model the quintessential case of Total Variation (TV) regularization being applied to a smooth, linear ramp. By restricting the solution to a simple two-plateau function, this problem allows you to derive the optimal piecewise-constant approximation from first principles, explicitly demonstrating how the TV penalty encourages the formation of steps even when the underlying data is perfectly smooth [@problem_id:3420945].", "problem": "Consider a one-dimensional data assimilation setting on the interval $[0,1]$ with a deterministic observation $f(x) = x$. We seek a staircasing solution with two plateaus, meaning a piecewise-constant function $u$ that takes the form\n$$\nu(x) = \\begin{cases}\nc_{1}, & x \\in [0,s),\\\\\nc_{2}, & x \\in [s,1],\n\\end{cases}\n$$\nfor some break point $s \\in (0,1)$ and plateau values $c_{1}, c_{2} \\in \\mathbb{R}$. The estimator is selected by minimizing the standard Tikhonov-type $L^{2}$ fidelity with Total Variation (TV) regularization used in inverse problems:\n$$\n\\min_{c_{1},c_{2},s} \\ \\frac{1}{2} \\int_{0}^{1} \\big(u(x)-f(x)\\big)^{2} \\,\\mathrm{d}x \\ + \\ \\lambda \\, TV(u),\n$$\nwhere $TV$ denotes the Total Variation, and for such a piecewise-constant $u$ with a single jump at $x=s$, one has $TV(u) = |c_{2} - c_{1}|$. The constant $\\lambda>0$ is the regularization parameter.\n\nStarting only from the fundamental definitions of the $L^{2}$ norm and Total Variation, derive the optimal plateau values $c_{1}^{\\star}$ and $c_{2}^{\\star}$ and the optimal break point $s^{\\star}$ that jointly minimize\n$$\n\\frac{1}{2}\\left(\\int_{0}^{s} (c_{1}-x)^{2}\\,\\mathrm{d}x + \\int_{s}^{1} (c_{2}-x)^{2}\\,\\mathrm{d}x\\right) + \\lambda |c_{2}-c_{1}|,\n$$\nunder the assumption $0<\\lambda<\\frac{1}{8}$ so that the minimizer exhibits two distinct plateaus. Your final answer must be a single closed-form analytic expression giving $c_{1}^{\\star}$, $c_{2}^{\\star}$, and $s^{\\star}$.", "solution": "The problem is to find the optimal parameters $c_1$, $c_2$, and $s$ that minimize the objective functional $J(c_1, c_2, s)$. The functional consists of an $L^2$ data fidelity term and a Total Variation (TV) regularization term. The observation is $f(x) = x$ on the interval $[0,1]$. The sought-after solution $u(x)$ is a piecewise-constant function with two plateaus, $c_1$ and $c_2$, with a break point at $s$.\n\nThe objective functional is given by:\n$$\nJ(c_1, c_2, s) = \\frac{1}{2} \\int_{0}^{1} \\big(u(x)-f(x)\\big)^{2} \\,\\mathrm{d}x \\ + \\ \\lambda \\, TV(u)\n$$\nSubstituting the given forms for $u(x)$, $f(x)$, and $TV(u)$, we have:\n$$\nJ(c_1, c_2, s) = \\frac{1}{2}\\left(\\int_{0}^{s} (c_{1}-x)^{2}\\,\\mathrm{d}x + \\int_{s}^{1} (c_{2}-x)^{2}\\,\\mathrm{d}x\\right) + \\lambda |c_{2}-c_{1}|\n$$\nThis minimization is performed with respect to $c_1 \\in \\mathbb{R}$, $c_2 \\in \\mathbb{R}$, and $s \\in (0,1)$. The problem statement specifies we seek a solution with two distinct plateaus, which implies $c_1 \\neq c_2$. The data function $f(x)=x$ is strictly increasing. It is reasonable to expect the optimal approximation $u(x)$ to be non-decreasing, which means we can assume $c_1 \\le c_2$. The condition $0 < \\lambda < 1/8$ ensures that $c_1  c_2$, so we can replace $|c_2 - c_1|$ with $c_2 - c_1$. The objective functional becomes:\n$$\nJ(c_1, c_2, s) = \\frac{1}{2}\\left(\\int_{0}^{s} (c_{1}-x)^{2}\\,\\mathrm{d}x + \\int_{s}^{1} (c_{2}-x)^{2}\\,\\mathrm{d}x\\right) + \\lambda (c_{2}-c_{1})\n$$\nTo find the optimal values $(c_1^{\\star}, c_2^{\\star}, s^{\\star})$, we use the first-order necessary conditions for a minimum. This involves setting the partial derivatives of $J$ with respect to $c_1$, $c_2$, and $s$ to zero.\n\nFirst, for a fixed value of $s$, we find the optimal $c_1$ and $c_2$. We compute the partial derivatives of $J$ with respect to $c_1$ and $c_2$:\n$$\n\\frac{\\partial J}{\\partial c_1} = \\frac{\\partial}{\\partial c_1} \\left[ \\frac{1}{2} \\int_{0}^{s} (c_{1}-x)^{2}\\,\\mathrm{d}x - \\lambda c_1 \\right] = \\int_{0}^{s} (c_1 - x)\\,\\mathrm{d}x - \\lambda\n$$\n$$\n\\frac{\\partial J}{\\partial c_2} = \\frac{\\partial}{\\partial c_2} \\left[ \\frac{1}{2} \\int_{s}^{1} (c_{2}-x)^{2}\\,\\mathrm{d}x + \\lambda c_2 \\right] = \\int_{s}^{1} (c_2 - x)\\,\\mathrm{d}x + \\lambda\n$$\nSetting these derivatives to zero yields:\n$$\n\\int_{0}^{s} (c_1 - x)\\,\\mathrm{d}x = \\lambda \\implies \\left[ c_1 x - \\frac{x^2}{2} \\right]_0^s = \\lambda \\implies c_1 s - \\frac{s^2}{2} = \\lambda\n$$\n$$\n\\int_{s}^{1} (c_2 - x)\\,\\mathrm{d}x = -\\lambda \\implies \\left[ c_2 x - \\frac{x^2}{2} \\right]_s^1 = -\\lambda \\implies (c_2 - \\frac{1}{2}) - (c_2 s - \\frac{s^2}{2}) = -\\lambda\n$$\nFrom the first equation, we solve for $c_1$ as a function of $s$:\n$$\nc_1(s) = \\frac{s}{2} + \\frac{\\lambda}{s}\n$$\nFrom the second equation, we solve for $c_2$ as a function of $s$:\n$$\nc_2(1-s) - \\frac{1-s^2}{2} = -\\lambda \\implies c_2(1-s) = \\frac{(1-s)(1+s)}{2} - \\lambda \\implies c_2(s) = \\frac{1+s}{2} - \\frac{\\lambda}{1-s}\n$$\nThese expressions give the optimal plateau values for any given break point $s$.\n\nNext, we find the optimal break point $s^{\\star}$. We substitute $c_1(s)$ and $c_2(s)$ into $J$ to obtain a function of $s$ alone, and then minimize it. A more direct approach is to find the partial derivative of $J(c_1, c_2, s)$ with respect to $s$ and set it to zero, using the values of $c_1$ and $c_2$ we just found. According to the envelope theorem, for the optimal functions $c_1(s)$ and $c_2(s)$, the total derivative $\\frac{dJ}{ds}$ simplifies to the partial derivative $\\frac{\\partial J}{\\partial s}$.\n$$\n\\frac{dJ}{ds} = \\frac{\\partial J}{\\partial c_1}\\frac{dc_1}{ds} + \\frac{\\partial J}{\\partial c_2}\\frac{dc_2}{ds} + \\frac{\\partial J}{\\partial s}\n$$\nSince $c_1(s)$ and $c_2(s)$ are defined such that $\\frac{\\partial J}{\\partial c_1} = 0$ and $\\frac{\\partial J}{\\partial c_2} = 0$, this simplifies to $\\frac{dJ}{ds} = \\frac{\\partial J}{\\partial s}$.\n\nUsing the Leibniz integral rule, the partial derivative of $J$ with respect to $s$ is:\n$$\n\\frac{\\partial J}{\\partial s} = \\frac{1}{2}(c_1-s)^2 - \\frac{1}{2}(c_2-s)^2\n$$\nSetting this to zero to find the optimal $s$:\n$$\n\\frac{1}{2}(c_1-s)^2 - \\frac{1}{2}(c_2-s)^2 = 0 \\implies (c_1-s)^2 = (c_2-s)^2\n$$\nThis implies $c_1-s = \\pm(c_2-s)$.\nCase 1: $c_1-s = c_2-s \\implies c_1=c_2$. This would mean there is no jump, a single plateau solution. This occurs when $\\lambda \\ge 1/8$, but the problem specifies $\\lambda  1/8$. So we discard this case.\nCase 2: $c_1-s = -(c_2-s) \\implies c_1-s = s-c_2 \\implies c_1+c_2 = 2s$.\nThis condition means the optimal break point $s$ is the arithmetic mean of the plateau values $c_1$ and $c_2$.\n\nWe now have a system of three equations for the three unknowns $c_1, c_2, s$:\n1. $c_1 = \\frac{s}{2} + \\frac{\\lambda}{s}$\n2. $c_2 = \\frac{1+s}{2} - \\frac{\\lambda}{1-s}$\n3. $c_1+c_2 = 2s$\n\nSubstitute (1) and (2) into (3):\n$$\n\\left(\\frac{s}{2} + \\frac{\\lambda}{s}\\right) + \\left(\\frac{1+s}{2} - \\frac{\\lambda}{1-s}\\right) = 2s\n$$\n$$\ns + \\frac{1}{2} + \\lambda\\left(\\frac{1}{s} - \\frac{1}{1-s}\\right) = 2s\n$$\n$$\n\\frac{1}{2} - s + \\lambda\\left(\\frac{1-s-s}{s(1-s)}\\right) = 0\n$$\n$$\n\\frac{1-2s}{2} + \\lambda\\frac{1-2s}{s(1-s)} = 0\n$$\n$$\n(1-2s)\\left(\\frac{1}{2} + \\frac{\\lambda}{s(1-s)}\\right) = 0\n$$\nThis equation has two possible solutions.\nPossibility A: $1-2s=0 \\implies s = 1/2$. This solution lies in the valid range $(0,1)$.\nPossibility B: $\\frac{1}{2} + \\frac{\\lambda}{s(1-s)} = 0 \\implies s(1-s) = -2\\lambda$. This is the quadratic equation $s^2-s-2\\lambda=0$. The solutions are $s = \\frac{1 \\pm \\sqrt{1+8\\lambda}}{2}$. For $\\lambda>0$, $\\sqrt{1+8\\lambda}>1$. Thus, the root $\\frac{1+\\sqrt{1+8\\lambda}}{2} > 1$, and the root $\\frac{1-\\sqrt{1+8\\lambda}}{2}  0$. Neither root is in the interval $(0,1)$.\n\nTherefore, the only valid optimal break point is $s^{\\star} = 1/2$.\n\nFinally, we find the optimal plateau values $c_1^{\\star}$ and $c_2^{\\star}$ by substituting $s^{\\star}=1/2$ into their expressions:\n$$\nc_1^{\\star} = c_1(1/2) = \\frac{1/2}{2} + \\frac{\\lambda}{1/2} = \\frac{1}{4} + 2\\lambda\n$$\n$$\nc_2^{\\star} = c_2(1/2) = \\frac{1+1/2}{2} - \\frac{\\lambda}{1-1/2} = \\frac{3/2}{2} - \\frac{\\lambda}{1/2} = \\frac{3}{4} - 2\\lambda\n$$\nWe check our initial assumption that $c_1  c_2$:\n$$\nc_2^{\\star} - c_1^{\\star} = \\left(\\frac{3}{4} - 2\\lambda\\right) - \\left(\\frac{1}{4} + 2\\lambda\\right) = \\frac{1}{2} - 4\\lambda\n$$\nThe condition $c_2^{\\star}  c_1^{\\star}$ requires $\\frac{1}{2} - 4\\lambda  0 \\implies \\lambda  \\frac{1}{8}$. This is consistent with the problem's assumption, which justifies our replacement of $|c_2-c_1|$ with $c_2-c_1$.\n\nThe optimal parameters are:\n$c_1^{\\star} = \\frac{1}{4} + 2\\lambda$\n$c_2^{\\star} = \\frac{3}{4} - 2\\lambda$\n$s^{\\star} = \\frac{1}{2}$", "answer": "$$\n\\boxed{\\begin{pmatrix} c_1^\\star  c_2^\\star  s^\\star \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} + 2\\lambda  \\frac{3}{4} - 2\\lambda  \\frac{1}{2} \\end{pmatrix}}\n$$", "id": "3420945"}, {"introduction": "This next practice explores the key trade-off inherent in TV regularization: its simultaneous strength in preserving sharp edges and its weakness in representing smooth regions. You will analyze a signal that contains both a large, sharp jump and gentle, sloping ramps. The task is to derive the structure of the TV-regularized solution, quantifying both its success in maintaining the important edge and its tendency to flatten the smooth ramps into spurious plateaus [@problem_id:3420951]. This exercise is crucial for understanding the practical consequences and visual artifacts of staircasing in real-world applications.", "problem": "Consider one-dimensional Total Variation (TV) regularization in the context of state estimation for inverse problems and data assimilation. Let the unknown signal be a vector $u \\in \\mathbb{R}^{6}$ reconstructed from observations $f \\in \\mathbb{R}^{6}$ by minimizing the Rudin–Osher–Fatemi (ROF) functional\n$$\nJ(u) \\;=\\; \\frac{1}{2}\\sum_{i=1}^{6}\\big(u_{i}-f_{i}\\big)^{2} \\;+\\; \\lambda\\sum_{i=1}^{5}\\big|u_{i+1}-u_{i}\\big|,\n$$\nwhere $\\lambda0$ is the regularization parameter and the one-dimensional discrete Total Variation (TV) is $\\sum_{i=1}^{5}\\big|u_{i+1}-u_{i}\\big|$. This framework is widely used to preserve sharp features while suppressing oscillations.\n\nLet the observation vector encode a strong edge with gentle ramps away from it:\n$$\nf \\;=\\; \\big(0,\\;\\varepsilon,\\;2\\varepsilon,\\;M,\\;M+\\varepsilon,\\;M+2\\varepsilon\\big),\n$$\nwith parameters $M0$ and $\\varepsilon0$. Assume the regularization parameter obeys\n$$\n3\\varepsilon \\;\\; \\lambda \\;\\; \\frac{3}{2}M,\n$$\nand adopt the discrete ROF model specified above.\n\nStarting from first principles of convex optimization and the subdifferential characterization of the discrete Total Variation, derive the structure of the minimizer that preserves the strong edge at the interface between indices $3$ and $4$ while producing spurious plateaus away from that interface. Use these principles to compute, in closed form, the following two quantities:\n\n- The edge-preservation factor defined by\n$$\nE \\;=\\; \\frac{\\text{jump of the reconstructed signal at the edge}}{\\text{jump in the observations at the edge}} \\;=\\; \\frac{u_{4}-u_{3}}{f_{4}-f_{3}}.\n$$\n\n- A quantitative measure of staircasing away from the edge defined as the mean squared flattening on the left plateau,\n$$\nP \\;=\\; \\frac{1}{3}\\sum_{i=1}^{3}\\big(u_{i}-f_{i}\\big)^{2}.\n$$\n\nExpress your final answer as a single closed-form expression in terms of $M$, $\\varepsilon$, and $\\lambda$. No numerical approximation is required, and no units are involved. Present both $E$ and $P$ together as a single row matrix.", "solution": "The minimizer $u$ of the convex functional $J(u)$ is characterized by the first-order optimality condition $0 \\in \\partial J(u)$, where $\\partial J(u)$ is the subdifferential of $J(u)$. This condition can be written as a system of equations involving the subgradient of the absolute value function.\nThe problem statement suggests a solution with plateaus away from the main edge. We hypothesize that the solution has the structure of two plateaus: $u_1=u_2=u_3=c_1$ and $u_4=u_5=u_6=c_2$ for some constants $c_1$ and $c_2$.\nThis hypothesis implies that the jumps $u_{i+1}-u_i$ are zero for $i \\in \\{1,2,4,5\\}$. The only potential non-zero jump is $u_4-u_3$.\nThe optimality conditions can be summed over the hypothesized plateaus. Summing the conditions for $i=1,2,3$ gives:\n$$\n\\sum_{i=1}^{3} (u_i - f_i) - \\lambda \\, \\text{sgn}(u_4-u_3) = 0\n$$\nSumming the conditions for $i=4,5,6$ gives:\n$$\n\\sum_{i=4}^{6} (u_i - f_i) + \\lambda \\, \\text{sgn}(u_4-u_3) = 0\n$$\nGiven the data $f$, we expect $u_4 > u_3$, so we set $\\text{sgn}(u_4-u_3) = 1$. Substituting $u_1=u_2=u_3=c_1$ and the corresponding values from $f$ into the first summed equation:\n$$\n(c_1 - 0) + (c_1 - \\varepsilon) + (c_1 - 2\\varepsilon) - \\lambda = 0 \\implies 3c_1 - 3\\varepsilon - \\lambda = 0\n$$\nSolving for $c_1$ gives:\n$$ c_1 = \\varepsilon + \\frac{\\lambda}{3} $$\nSimilarly, substituting $u_4=u_5=u_6=c_2$ into the second summed equation:\n$$\n(c_2 - M) + (c_2 - (M+\\varepsilon)) + (c_2 - (M+2\\varepsilon)) + \\lambda = 0 \\implies 3c_2 - (3M+3\\varepsilon) + \\lambda = 0\n$$\nSolving for $c_2$ gives:\n$$ c_2 = M + \\varepsilon - \\frac{\\lambda}{3} $$\nWe must verify this solution structure is consistent with the subgradient conditions for the given range of $\\lambda$. The condition $u_4 > u_3$ requires $c_2 > c_1$, which means $M + \\varepsilon - \\frac{\\lambda}{3} > \\varepsilon + \\frac{\\lambda}{3}$, simplifying to $M > \\frac{2\\lambda}{3}$ or $\\lambda  \\frac{3}{2}M$. This is given in the problem. The conditions for the zero jumps within the plateaus require that the corresponding subgradient variables are in $[-1,1]$. This analysis leads to the condition $\\lambda > 3\\varepsilon$, which is also given. Thus, our hypothesized solution is correct.\n\nNow, we compute the required quantities $E$ and $P$.\n\n1.  **Edge-preservation factor $E$**:\n    $E = \\frac{u_{4}-u_{3}}{f_{4}-f_{3}}$\n    The jump in the reconstructed signal is $u_4-u_3 = c_2 - c_1 = (M + \\varepsilon - \\frac{\\lambda}{3}) - (\\varepsilon + \\frac{\\lambda}{3}) = M - \\frac{2\\lambda}{3}$.\n    The jump in the observations is $f_4-f_3 = M - 2\\varepsilon$.\n    $$ E = \\frac{M - \\frac{2\\lambda}{3}}{M - 2\\varepsilon} $$\n\n2.  **Staircasing measure $P$**:\n    $P = \\frac{1}{3}\\sum_{i=1}^{3}(u_{i}-f_{i})^{2}$\n    Since $u_1=u_2=u_3=c_1=\\varepsilon+\\frac{\\lambda}{3}$, we compute the residuals:\n    $u_1-f_1 = c_1 - 0 = \\varepsilon + \\frac{\\lambda}{3}$\n    $u_2-f_2 = c_1 - \\varepsilon = \\frac{\\lambda}{3}$\n    $u_3-f_3 = c_1 - 2\\varepsilon = \\frac{\\lambda}{3} - \\varepsilon$\n    Substituting these into the expression for $P$:\n    $$ P = \\frac{1}{3}\\left[ \\left(\\varepsilon + \\frac{\\lambda}{3}\\right)^2 + \\left(\\frac{\\lambda}{3}\\right)^2 + \\left(\\frac{\\lambda}{3} - \\varepsilon\\right)^2 \\right] $$\n    $$ P = \\frac{1}{3}\\left[ \\left(\\varepsilon^2 + \\frac{2\\varepsilon\\lambda}{3} + \\frac{\\lambda^2}{9}\\right) + \\frac{\\lambda^2}{9} + \\left(\\frac{\\lambda^2}{9} - \\frac{2\\varepsilon\\lambda}{3} + \\varepsilon^2\\right) \\right] $$\n    $$ P = \\frac{1}{3}\\left[ 2\\varepsilon^2 + 3 \\frac{\\lambda^2}{9} \\right] = \\frac{1}{3}\\left[ 2\\varepsilon^2 + \\frac{\\lambda^2}{3} \\right] $$\n    $$ P = \\frac{2}{3}\\varepsilon^2 + \\frac{\\lambda^2}{9} $$\nThe final answer is the pair $(E, P)$ presented as a row matrix.", "answer": "$$ \\boxed{ \\begin{pmatrix} E  P \\end{pmatrix} = \\begin{pmatrix} \\frac{M - \\frac{2\\lambda}{3}}{M - 2\\varepsilon}  \\frac{2}{3}\\varepsilon^{2} + \\frac{\\lambda^{2}}{9} \\end{pmatrix} } $$", "id": "3420951"}, {"introduction": "Having explored the \"what\" and \"why\" of staircasing, we now delve into the mathematical mechanics of \"how much.\" This advanced problem challenges you to analyze the behavior of the discrete Rudin-Osher-Fatemi (ROF) model as a function of the regularization parameter, $\\lambda$. Using the powerful taut string characterization, you will determine the precise thresholds of $\\lambda$ at which distinct signal features merge into larger plateaus, effectively reducing the number of \"stairs\" [@problem_id:3420941]. This provides a deep, quantitative understanding of how $\\lambda$ acts as a knob to control the coarseness of the solution.", "problem": "Consider a one-dimensional discrete denoising problem on the interval $[0,1]$ with a uniform grid of $n=4$ points at locations $t_{i} \\in \\{0, \\tfrac{1}{3}, \\tfrac{2}{3}, 1\\}$. The observed data are noisy step values\n$$\nf_{1} = 0.02,\\quad f_{2} = 0.08,\\quad f_{3} = 0.95,\\quad f_{4} = 1.10.\n$$\nFor a regularization parameter $\\lambda \\ge 0$, consider the one-dimensional Rudin–Osher–Fatemi (ROF) total variation (TV) regularization problem\n$$\n\\min_{u \\in \\mathbb{R}^{4}} \\ \\frac{1}{2}\\sum_{i=1}^{4} (u_{i} - f_{i})^{2} \\ + \\ \\lambda \\sum_{i=1}^{3} |u_{i+1} - u_{i}|.\n$$\nIt is known from the taut string characterization of one-dimensional total variation denoising that the solution can be obtained by taking the discrete derivative of a shortest path that lies within a strip of half-width $\\lambda$ around the cumulative sum of the data, connecting the endpoints of that cumulative curve. Using only fundamental principles (convex optimality and subgradient conditions for the TV term) and the taut string characterization as a definition, do the following:\n\n- Explicitly construct, for each regime of $\\lambda$, the taut string corresponding to the cumulative data $F_{k} := \\sum_{i=1}^{k} f_{i}$, $k=0,1,2,3,4$, with $F_{0} := 0$, and hence obtain the TV-denoised solution $u_{\\lambda}$.\n- Determine, as a function of $\\lambda$, the number of plateau regions (maximal contiguous index sets on which $u_{\\lambda}$ is constant) and their locations (which indices belong to each plateau).\n\nFinally, let $N(\\lambda)$ denote the number of plateau regions in $u_{\\lambda}$. Provide $N(\\lambda)$ as a single piecewise-defined analytic expression in $\\lambda$. No rounding is required.", "solution": "The number of plateaus in the solution $u_\\lambda$ is determined by the number of indices $i$ for which the jump $u_{i+1}-u_i$ is non-zero. This behavior is governed by a dual variable $p \\in \\mathbb{R}^3$, which is the solution to the dual problem:\n$$\n\\min_{p \\in \\mathbb{R}^3} \\frac{1}{2}\\|f - D^T p\\|^2 \\quad \\text{subject to} \\quad \\|p\\|_\\infty \\le \\lambda,\n$$\nwhere $D$ is the first-difference operator. The primal solution $u$ is then recovered as $u = f - D^T p$. A key property of this duality is that a plateau forms between indices $i$ and $i+1$ (i.e., $u_{i+1}=u_i$) if and only if the corresponding dual variable is in the interior of its constraint set, $|p_i|  \\lambda$. A jump occurs ($u_{i+1} \\neq u_i$) if and only if the constraint is active, $|p_i|=\\lambda$.\n\nThe number of plateaus $N(\\lambda)$ changes only at critical values of $\\lambda$ where a component of the dual solution $p(\\lambda)$ transitions between the boundary and the interior of $[-\\lambda, \\lambda]$. These transitions are controlled by the unconstrained minimizer of the dual objective, denoted $p^*$. This is found by solving the linear system that arises from setting the gradient of the objective to zero: $DD^T p = Df$.\n\nLet $d = Df$, so $d_i = f_{i+1}-f_i$. With the given data, we have:\n$d_1 = 0.08 - 0.02 = 0.06$\n$d_2 = 0.95 - 0.08 = 0.87$\n$d_3 = 1.10 - 0.95 = 0.15$\nThe matrix $A = DD^T$ is the tridiagonal matrix with $(2, -1)$ entries. The system $Ap=d$ is:\n$$\n\\begin{aligned}\n2 p_{1} - p_{2} = 0.06,\\\\\n-p_{1} + 2 p_{2} - p_{3} = 0.87,\\\\\n-p_{2} + 2 p_{3} = 0.15.\n\\end{aligned}\n$$\nSolving this system yields the unconstrained dual solution:\n$$\np^{\\star} = \\begin{pmatrix} 0.5175 \\\\ 0.975 \\\\ 0.5625 \\end{pmatrix}.\n$$\nAs $\\lambda$ increases from 0, the solution $p(\\lambda)$ moves from the origin towards $p^*$. The path is piecewise linear, with breaks occurring when the set of active constraints $\\{i : |p_i(\\lambda)|=\\lambda\\}$ changes. The thresholds are determined by a detailed analysis of the KKT conditions. A more intuitive view is that a plateau forms once $\\lambda$ is large enough to \"absorb\" a feature. The sequence of plateau mergers is determined by the structure of the dual problem.\n\n1.  For $0 \\le \\lambda  0.06$: All constraints are active ($|p_i|=\\lambda$ for all $i$), meaning all jumps are preserved. The solution $u_\\lambda$ has four distinct values. Thus, $N(\\lambda)=4$. The first component to leave the boundary is related to the smallest \"effort\" required to flatten a feature. This happens at $\\lambda = 0.06$.\n\n2.  For $0.06 \\le \\lambda  0.15$: The first plateau forms, merging indices $\\{1,2\\}$. This corresponds to the condition $|p_1|  \\lambda$ becoming true. The jumps at indices 2 and 3 are preserved, meaning $|p_2|=\\lambda$ and $|p_3|=\\lambda$. The solution has plateaus on index sets $\\{1,2\\}, \\{3\\}, \\{4\\}$. Thus, $N(\\lambda)=3$. The next transition occurs at $\\lambda=0.15$.\n\n3.  For $0.15 \\le \\lambda  0.975$: A second plateau forms, merging indices $\\{3,4\\}$, corresponding to $|p_3|  \\lambda$. The jump between indices 2 and 3 is preserved, meaning $|p_2|=\\lambda$. The solution now has two plateaus on index sets $\\{1,2\\}$ and $\\{3,4\\}$. Thus, $N(\\lambda)=2$. The final transition occurs when the last active constraint, $|p_2|=\\lambda$, is relaxed. This happens when $\\lambda$ becomes larger than the corresponding component of the unconstrained solution, which is $|p^*_2|=0.975$.\n\n4.  For $\\lambda \\ge 0.975$: The box $[-\\lambda, \\lambda]^3$ is large enough that $p(\\lambda)=p^*$. Since all components of $p^*$ have magnitudes less than or equal to $\\lambda$, all conditions $|p_i|  \\lambda$ become true (at $\\lambda=0.975$, $p_2$ is still on the boundary, but for any $\\lambda > 0.975$ it is interior). All jumps are removed, and the solution $u_\\lambda$ becomes a single constant value (the mean of the original data $f$). Thus, $N(\\lambda)=1$.\n\nCombining these regimes gives the number of plateaus as a function of $\\lambda$.", "answer": "$$\\boxed{N(\\lambda)=\\begin{cases}\n4,  0 \\le \\lambda  0.06,\\\\\n3,  0.06 \\le \\lambda  0.15,\\\\\n2,  0.15 \\le \\lambda  0.975,\\\\\n1,  \\lambda \\ge 0.975.\n\\end{cases}}$$", "id": "3420941"}]}