{"hands_on_practices": [{"introduction": "This first practice provides a foundational understanding of weak-constraint 4D-Var by deriving the analytical solution for a simple linear system. By solving for the optimal initial state and model errors directly, you will see how the analysis is a weighted average of information from the background, the model dynamics, and the observations, with weights determined by their respective error covariances. This exercise [@problem_id:3431128] builds crucial intuition for the structure of the Bayesian posterior mean in a Gaussian context.", "problem": "Consider a scalar linear dynamical system over two successive model steps in a weak-constraint Four-Dimensional Variational (4D-Var) data assimilation formulation. Let the initial state be $x_0$, and define the one-step linear model operator $M \\in \\mathbb{R}$, applied identically at both steps, with additive model errors $\\eta_0$ and $\\eta_1$. The state evolves according to\n$$\nx_1 = M x_0 + \\eta_0, \\quad x_2 = M x_1 + \\eta_1.\n$$\nA single scalar observation $y \\in \\mathbb{R}$ is available at the final time, with a scalar linear observation operator $H \\in \\mathbb{R}$ and observation-error variance $R > 0$, so that the observation equation is\n$$\ny = H x_2 + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, R).\n$$\nThe background (prior) on the initial condition is $x_b \\in \\mathbb{R}$ with background-error variance $B > 0$. The additive model errors at both steps are independently distributed as $\\eta_k \\sim \\mathcal{N}(0, Q)$ with $Q > 0$. In the weak-constraint 4D-Var setting, the control variables are the triple $(x_0, \\eta_0, \\eta_1)$. The standard quadratic cost functional is\n$$\nJ(x_0, \\eta_0, \\eta_1) = \\frac{1}{2} \\left[ \\frac{(x_0 - x_b)^2}{B} + \\frac{\\eta_0^2}{Q} + \\frac{\\eta_1^2}{Q} + \\frac{\\left(y - H x_2\\right)^2}{R} \\right],\n$$\nwhere $x_2 = M^2 x_0 + M \\eta_0 + \\eta_1$.\n\nStarting from the fundamental principles of linear least-squares estimation under Gaussian errors, derive the closed-form expressions for the minimizer $(x_0^\\ast, \\eta_0^\\ast, \\eta_1^\\ast)$ of $J$ as analytic functions of the scalars $B$, $Q$, $R$, $M$, $H$, $x_b$, and $y$. Express your final answer as a single $1 \\times 3$ row matrix in the order $(x_0^\\ast, \\eta_0^\\ast, \\eta_1^\\ast)$. Provide exact symbolic expressions; do not perform any numerical rounding.", "solution": "The problem presents a standard weak-constraint four-dimensional variational (4D-Var) data assimilation problem for a scalar, linear system. The task is to find the optimal values of the control variables—the initial state $x_0$ and the model errors at two time steps, $\\eta_0$ and $\\eta_1$—that minimize a given quadratic cost functional $J(x_0, \\eta_0, \\eta_1)$.\n\nFirst, we perform a validation of the problem statement.\n\n### Step 1: Extract Givens\n- State evolution model: $x_1 = M x_0 + \\eta_0$, $x_2 = M x_1 + \\eta_1$, which implies $x_2 = M^2 x_0 + M \\eta_0 + \\eta_1$. All variables are scalars.\n- Observation model: $y = H x_2 + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, R)$ with $R > 0$.\n- Prior information: The initial state $x_0$ has a background estimate $x_b$ with error variance $B > 0$.\n- Model error statistics: The model errors $\\eta_0$ and $\\eta_1$ are independent and identically distributed as $\\mathcal{N}(0, Q)$ with $Q > 0$.\n- Control variables: $(x_0, \\eta_0, \\eta_1)$.\n- Cost functional: $J(x_0, \\eta_0, \\eta_1) = \\frac{1}{2} \\left[ \\frac{(x_0 - x_b)^2}{B} + \\frac{\\eta_0^2}{Q} + \\frac{\\eta_1^2}{Q} + \\frac{\\left(y - H x_2\\right)^2}{R} \\right]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, being a canonical example of linear-quadratic estimation in data assimilation, which is equivalent to Bayesian inference under Gaussian assumptions. It is well-posed; the cost functional $J$ is a strictly convex quadratic function of the control variables (since $B, Q, R > 0$), which guarantees the existence of a unique minimum. The problem statement is objective, complete, and mathematically consistent. No flaws are detected.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the derivation of the solution.\n\nThe minimizer $(x_0^\\ast, \\eta_0^\\ast, \\eta_1^\\ast)$ of the cost functional $J$ is found at the stationary point where the gradient of $J$ with respect to the control variables is zero. We must solve the system of equations:\n$$\n\\frac{\\partial J}{\\partial x_0} = 0, \\quad \\frac{\\partial J}{\\partial \\eta_0} = 0, \\quad \\frac{\\partial J}{\\partial \\eta_1} = 0.\n$$\nFirst, we substitute the expression for $x_2$ into the cost functional:\n$$\nJ(x_0, \\eta_0, \\eta_1) = \\frac{1}{2} \\left[ \\frac{(x_0 - x_b)^2}{B} + \\frac{\\eta_0^2}{Q} + \\frac{\\eta_1^2}{Q} + \\frac{\\left(y - H(M^2 x_0 + M \\eta_0 + \\eta_1)\\right)^2}{R} \\right].\n$$\nNow, we compute the partial derivatives with respect to each control variable.\n\n1.  Derivative with respect to $x_0$:\n    $$\n    \\frac{\\partial J}{\\partial x_0} = \\frac{x_0 - x_b}{B} + \\frac{1}{R}\\left(y - H(M^2 x_0 + M \\eta_0 + \\eta_1)\\right) \\cdot (-H M^2) = 0.\n    $$\n\n2.  Derivative with respect to $\\eta_0$:\n    $$\n    \\frac{\\partial J}{\\partial \\eta_0} = \\frac{\\eta_0}{Q} + \\frac{1}{R}\\left(y - H(M^2 x_0 + M \\eta_0 + \\eta_1)\\right) \\cdot (-H M) = 0.\n    $$\n\n3.  Derivative with respect to $\\eta_1$:\n    $$\n    \\frac{\\partial J}{\\partial \\eta_1} = \\frac{\\eta_1}{Q} + \\frac{1}{R}\\left(y - H(M^2 x_0 + M \\eta_0 + \\eta_1)\\right) \\cdot (-H) = 0.\n    $$\n\nLet the solution be denoted by $(x_0^\\ast, \\eta_0^\\ast, \\eta_1^\\ast)$, and let $x_2^\\ast = M^2 x_0^\\ast + M \\eta_0^\\ast + \\eta_1^\\ast$. The system of equations at the minimum can be rewritten in a more insightful form. Let us define the residual term $\\delta^\\ast = y - H x_2^\\ast$. The equations become:\n$$\n\\frac{x_0^\\ast - x_b}{B} = \\frac{H M^2}{R} \\delta^\\ast \\quad (1)\n$$\n$$\n\\frac{\\eta_0^\\ast}{Q} = \\frac{H M}{R} \\delta^\\ast \\quad (2)\n$$\n$$\n\\frac{\\eta_1^\\ast}{Q} = \\frac{H}{R} \\delta^\\ast \\quad (3)\n$$\nFrom these equations, we can express the optimal control variables as functions of the unknown residual $\\delta^\\ast$:\n$$\nx_0^\\ast = x_b + \\frac{B H M^2}{R} \\delta^\\ast\n$$\n$$\n\\eta_0^\\ast = \\frac{Q H M}{R} \\delta^\\ast\n$$\n$$\n\\eta_1^\\ast = \\frac{Q H}{R} \\delta^\\ast\n$$\nTo find $\\delta^\\ast$, we substitute these expressions back into its definition, $\\delta^\\ast = y - H x_2^\\ast = y - H(M^2 x_0^\\ast + M \\eta_0^\\ast + \\eta_1^\\ast)$:\n$$\n\\delta^\\ast = y - H \\left[ M^2\\left(x_b + \\frac{B H M^2}{R} \\delta^\\ast\\right) + M\\left(\\frac{Q H M}{R} \\delta^\\ast\\right) + \\left(\\frac{Q H}{R} \\delta^\\ast\\right) \\right]\n$$\nNow, we rearrange the equation to solve for $\\delta^\\ast$:\n$$\n\\delta^\\ast = y - H M^2 x_b - H \\left[ \\frac{B H M^4}{R} + \\frac{Q H M^2}{R} + \\frac{Q H}{R} \\right] \\delta^\\ast\n$$\n$$\n\\delta^\\ast = y - H M^2 x_b - \\frac{H^2}{R} \\left( B M^4 + Q M^2 + Q \\right) \\delta^\\ast\n$$\n$$\n\\delta^\\ast \\left[ 1 + \\frac{H^2}{R} \\left( B M^4 + Q M^2 + Q \\right) \\right] = y - H M^2 x_b\n$$\n$$\n\\delta^\\ast \\left[ \\frac{R + H^2 (B M^4 + Q M^2 + Q)}{R} \\right] = y - H M^2 x_b\n$$\nSolving for $\\delta^\\ast$ yields:\n$$\n\\delta^\\ast = \\frac{R (y - H M^2 x_b)}{R + H^2(B M^4 + Q M^2 + Q)}\n$$\nThe term $y - H M^2 x_b$ is the innovation, i.e., the difference between the observation and the forecast from the background mean. The denominator represents the total variance in observation space, comprising the observation error variance $R$ and the propagated prior variances from $x_0$, $\\eta_0$, and $\\eta_1$.\n\nFinally, we substitute this expression for $\\delta^\\ast$ back into the equations for $x_0^\\ast$, $\\eta_0^\\ast$, and $\\eta_1^\\ast$.\n\nFor $x_0^\\ast$:\n$$\nx_0^\\ast = x_b + \\frac{B H M^2}{R} \\left[ \\frac{R (y - H M^2 x_b)}{R + H^2(B M^4 + Q M^2 + Q)} \\right] = x_b + \\frac{B H M^2 (y - H M^2 x_b)}{R + H^2(B M^4 + Q M^2 + Q)}\n$$\n\nFor $\\eta_0^\\ast$:\n$$\n\\eta_0^\\ast = \\frac{Q H M}{R} \\left[ \\frac{R (y - H M^2 x_b)}{R + H^2(B M^4 + Q M^2 + Q)} \\right] = \\frac{Q H M (y - H M^2 x_b)}{R + H^2(B M^4 + Q M^2 + Q)}\n$$\n\nFor $\\eta_1^\\ast$:\n$$\n\\eta_1^\\ast = \\frac{Q H}{R} \\left[ \\frac{R (y - H M^2 x_b)}{R + H^2(B M^4 + Q M^2 + Q)} \\right] = \\frac{Q H (y - H M^2 x_b)}{R + H^2(B M^4 + Q M^2 + Q)}\n$$\nThese are the closed-form expressions for the minimizer $(x_0^\\ast, \\eta_0^\\ast, \\eta_1^\\ast)$. We present them in the required matrix format.", "answer": "$$\n\\boxed{\\begin{pmatrix} x_b + \\frac{B H M^2 (y - H M^2 x_b)}{R + H^2(B M^4 + Q M^2 + Q)} & \\frac{Q H M (y - H M^2 x_b)}{R + H^2(B M^4 + Q M^2 + Q)} & \\frac{Q H (y - H M^2 x_b)}{R + H^2(B M^4 + Q M^2 + Q)} \\end{pmatrix}}\n$$", "id": "3431128"}, {"introduction": "Building on the direct minimization approach, this exercise introduces the powerful Lagrangian and adjoint framework for solving the weak-constraint 4D-Var problem. You will use the method of Lagrange multipliers to enforce the model equations as constraints, leading to the Karush-Kuhn-Tucker (KKT) optimality conditions. This practice [@problem_id:3431086] is essential for understanding how gradients are computed efficiently in large-scale numerical systems, where the adjoint model plays a central role.", "problem": "Consider weak-constraint Four-Dimensional Variational (4D-Var) assimilation for a two-step linear dynamical model with scalar state. Let the dynamics be given by the forward model with additive model error,\n$$\nx_{1} = m_{0}\\,x_{0} + \\eta_{0}, \\qquad x_{2} = m_{1}\\,x_{1} + \\eta_{1},\n$$\nand the observations be linear with identity operator,\n$$\ny_{1} = x_{1} + \\epsilon_{1}, \\qquad y_{2} = x_{2} + \\epsilon_{2}.\n$$\nAssume the weak-constraint 4D-Var cost function,\n$$\nJ(x_{0},\\eta_{0},\\eta_{1}) = \\frac{1}{2}(x_{0}-x_{b})^{2}B^{-1} + \\frac{1}{2}(y_{1}-x_{1})^{2}R_{1}^{-1} + \\frac{1}{2}(y_{2}-x_{2})^{2}R_{2}^{-1} + \\frac{1}{2}\\eta_{0}^{2}Q_{0}^{-1} + \\frac{1}{2}\\eta_{1}^{2}Q_{1}^{-1},\n$$\nwhere all covariances are positive scalars. Take the specific, scientifically consistent values\n$$\nm_{0} = 2, \\quad m_{1} = 1, \\quad x_{b} = 0, \\quad B = 1, \\quad R_{1} = 1, \\quad R_{2} = 1, \\quad Q_{0} = 1, \\quad Q_{1} = 2, \\quad y_{1} = 3, \\quad y_{2} = 1.\n$$\nFormulate the constrained optimization by introducing Lagrange multipliers for the model equations and derive the Karush–Kuhn–Tucker (KKT) stationarity equations by first principles. Compute explicitly:\n- the adjoint variables (Lagrange multipliers) $\\lambda_{1}$ and $\\lambda_{2}$ at the minimizing trajectory,\n- the gradient of the reduced cost with respect to the controls, namely $g_{x_{0}}$, $g_{\\eta_{0}}$, and $g_{\\eta_{1}}$, evaluated at the minimizing trajectory.\n\nYou must verify that the KKT optimality equations hold exactly for the computed values. Report your final answer as a single row matrix in the order\n$$\n\\left(x_{0}^{\\ast},\\,\\eta_{0}^{\\ast},\\,\\eta_{1}^{\\ast},\\,\\lambda_{1}^{\\ast},\\,\\lambda_{2}^{\\ast},\\,g_{x_{0}},\\,g_{\\eta_{0}},\\,g_{\\eta_{1}}\\right),\n$$\nusing exact rational values. No rounding is required. No units are associated with the quantities.", "solution": "The problem is subjected to validation prior to any solution attempt.\n\n### Step 1: Extract Givens\nThe given information is as follows:\n- **Dynamical Model**:\n  $x_{1} = m_{0}\\,x_{0} + \\eta_{0}$\n  $x_{2} = m_{1}\\,x_{1} + \\eta_{1}$\n- **Observation Model**:\n  $y_{1} = x_{1} + \\epsilon_{1}$\n  $y_{2} = x_{2} + \\epsilon_{2}$\n- **Cost Function for Weak-Constraint 4D-Var**:\n  $J(x_{0},\\eta_{0},\\eta_{1}) = \\frac{1}{2}(x_{0}-x_{b})^{2}B^{-1} + \\frac{1}{2}(y_{1}-x_{1})^{2}R_{1}^{-1} + \\frac{1}{2}(y_{2}-x_{2})^{2}R_{2}^{-1} + \\frac{1}{2}\\eta_{0}^{2}Q_{0}^{-1} + \\frac{1}{2}\\eta_{1}^{2}Q_{1}^{-1}$\n- **Parameter Values**:\n  $m_{0} = 2$, $m_{1} = 1$\n  $x_{b} = 0$, $B = 1$\n  $R_{1} = 1$, $R_{2} = 1$\n  $Q_{0} = 1$, $Q_{1} = 2$\n- **Observation Values**:\n  $y_{1} = 3$, $y_{2} = 1$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard, well-defined exercise in data assimilation using the weak-constraint 4D-Var method.\n- **Scientifically Grounded**: The formulation is based on established principles of variational data assimilation and optimization theory. The use of a quadratic cost function corresponds to assumptions of Gaussian-distributed errors, a standard practice in the field.\n- **Well-Posed**: The problem is a convex optimization problem (minimizing a strictly convex quadratic function subject to linear constraints), which guarantees the existence of a unique solution. All necessary parameters and data are provided, making the problem self-contained. The slight ambiguity in the cost function notation $J(x_0, \\eta_0, \\eta_1)$ is resolved by its definition, which includes terms depending on $x_1$ and $x_2$. The instruction to use Lagrange multipliers clarifies the intended \"all-at-once\" solution approach where $(x_0, x_1, x_2, \\eta_0, \\eta_1)$ are treated as independent variables subject to constraints.\n- **Objective**: The problem is stated using precise mathematical terminology and is devoid of any subjective or opinion-based content.\n\nThe problem does not violate any of the invalidity criteria. It is scientifically sound, fully specified, objective, and represents a non-trivial application of core principles in its field.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution\nThe problem is to find the minimum of the cost function $J$ subject to the constraints imposed by the dynamical model. This is a constrained optimization problem. We employ the method of Lagrange multipliers. The optimization variables are the initial state $x_0$, the model errors $\\eta_0$ and $\\eta_1$, and the subsequent states $x_1$ and $x_2$.\n\nThe cost function, as a function of the full state and error vectors, is:\n$$\nJ(x_0, x_1, x_2, \\eta_0, \\eta_1) = \\frac{1}{2}(x_{0}-x_{b})^{2}B^{-1} + \\frac{1}{2}(y_{1}-x_{1})^{2}R_{1}^{-1} + \\frac{1}{2}(y_{2}-x_{2})^{2}R_{2}^{-1} + \\frac{1}{2}\\eta_{0}^{2}Q_{0}^{-1} + \\frac{1}{2}\\eta_{1}^{2}Q_{1}^{-1}\n$$\nThe constraints are the model equations:\n$$\nc_1(x_0, x_1, \\eta_0) = x_1 - m_0 x_0 - \\eta_0 = 0\n$$\n$$\nc_2(x_1, x_2, \\eta_1) = x_2 - m_1 x_1 - \\eta_1 = 0\n$$\nWe form the Lagrangian $\\mathcal{L}$ by adjoining the constraints to the cost function using Lagrange multipliers, which are the adjoint variables $\\lambda_1$ and $\\lambda_2$:\n$$\n\\mathcal{L}(x_0, x_1, x_2, \\eta_0, \\eta_1, \\lambda_1, \\lambda_2) = J + \\lambda_1 (x_1 - m_0 x_0 - \\eta_0) + \\lambda_2 (x_2 - m_1 x_1 - \\eta_1)\n$$\nThe Karush-Kuhn-Tucker (KKT) stationarity conditions are found by setting the partial derivatives of $\\mathcal{L}$ with respect to all its variables to zero.\n\n1.  $\\frac{\\partial\\mathcal{L}}{\\partial x_0} = (x_0-x_b)B^{-1} - \\lambda_1 m_0 = 0$\n2.  $\\frac{\\partial\\mathcal{L}}{\\partial x_1} = -(y_1-x_1)R_1^{-1} + \\lambda_1 - \\lambda_2 m_1 = 0$\n3.  $\\frac{\\partial\\mathcal{L}}{\\partial x_2} = -(y_2-x_2)R_2^{-1} + \\lambda_2 = 0$\n4.  $\\frac{\\partial\\mathcal{L}}{\\partial \\eta_0} = \\eta_0 Q_0^{-1} - \\lambda_1 = 0$\n5.  $\\frac{\\partial\\mathcal{L}}{\\partial \\eta_1} = \\eta_1 Q_1^{-1} - \\lambda_2 = 0$\n6.  $\\frac{\\partial\\mathcal{L}}{\\partial \\lambda_1} = x_1 - m_0 x_0 - \\eta_0 = 0$\n7.  $\\frac{\\partial\\mathcal{L}}{\\partial \\lambda_2} = x_2 - m_1 x_1 - \\eta_1 = 0$\n\nEquations $(6)$ and $(7)$ are the original forward model equations. The set of seven equations forms a linear system for the optimal variables $(x_0^*, x_1^*, x_2^*, \\eta_0^*, \\eta_1^*, \\lambda_1^*, \\lambda_2^*)$.\n\nWe substitute the given numerical values: $m_0 = 2$, $m_1 = 1$, $x_b = 0$, $B = 1$, $y_1 = 3$, $y_2 = 1$, $R_1 = 1$, $R_2 = 1$, $Q_0 = 1$, $Q_1 = 2$.\nThe system becomes:\n1.  $(x_0-0)(1)^{-1} - \\lambda_1 (2) = 0 \\implies x_0 - 2\\lambda_1 = 0$\n2.  $-(3-x_1)(1)^{-1} + \\lambda_1 - \\lambda_2 (1) = 0 \\implies x_1 - 3 + \\lambda_1 - \\lambda_2 = 0$\n3.  $-(1-x_2)(1)^{-1} + \\lambda_2 = 0 \\implies x_2 - 1 + \\lambda_2 = 0$\n4.  $\\eta_0 (1)^{-1} - \\lambda_1 = 0 \\implies \\eta_0 - \\lambda_1 = 0$\n5.  $\\eta_1 (2)^{-1} - \\lambda_2 = 0 \\implies \\frac{1}{2}\\eta_1 - \\lambda_2 = 0$\n6.  $x_1 - 2x_0 - \\eta_0 = 0$\n7.  $x_2 - x_1 - \\eta_1 = 0$\n\nWe solve this system. From equations $(1)$, $(3)$, $(4)$, and $(5)$, we can express the state and error variables in terms of the adjoint variables:\nFrom $(1)$: $x_0 = 2\\lambda_1$\nFrom $(4)$: $\\eta_0 = \\lambda_1$\nFrom $(5)$: $\\eta_1 = 2\\lambda_2$\nFrom $(3)$: $x_2 = 1-\\lambda_2$\n\nSubstitute these into the model equations $(6)$ and $(7)$:\nFrom $(6)$: $x_1 - 2(2\\lambda_1) - \\lambda_1 = 0 \\implies x_1 = 5\\lambda_1$\nFrom $(7)$: $(1-\\lambda_2) - x_1 - (2\\lambda_2) = 0 \\implies x_1 = 1 - 3\\lambda_2$\n\nEquating the two expressions for $x_1$:\n$$\n5\\lambda_1 = 1 - 3\\lambda_2 \\implies 5\\lambda_1 + 3\\lambda_2 = 1 \\quad (\\text{Eq. A})\n$$\nNow, substitute $x_1 = 5\\lambda_1$ into equation $(2)$:\n$$\n(5\\lambda_1) - 3 + \\lambda_1 - \\lambda_2 = 0 \\implies 6\\lambda_1 - \\lambda_2 = 3 \\quad (\\text{Eq. B})\n$$\nWe now have a $2 \\times 2$ system for $(\\lambda_1, \\lambda_2)$:\nA: $5\\lambda_1 + 3\\lambda_2 = 1$\nB: $6\\lambda_1 - \\lambda_2 = 3$\n\nFrom Eq. B, we express $\\lambda_2$ in terms of $\\lambda_1$: $\\lambda_2 = 6\\lambda_1 - 3$.\nSubstitute this into Eq. A:\n$$\n5\\lambda_1 + 3(6\\lambda_1 - 3) = 1\n$$\n$$\n5\\lambda_1 + 18\\lambda_1 - 9 = 1\n$$\n$$\n23\\lambda_1 = 10 \\implies \\lambda_1^* = \\frac{10}{23}\n$$\nNow, we find $\\lambda_2^*$:\n$$\n\\lambda_2^* = 6\\lambda_1^* - 3 = 6\\left(\\frac{10}{23}\\right) - 3 = \\frac{60}{23} - \\frac{69}{23} = -\\frac{9}{23}\n$$\nThese are the optimal values for the adjoint variables (Lagrange multipliers).\n\nNext, we compute the optimal values for the states and model errors:\n$x_0^* = 2\\lambda_1^* = 2\\left(\\frac{10}{23}\\right) = \\frac{20}{23}$\n$\\eta_0^* = \\lambda_1^* = \\frac{10}{23}$\n$\\eta_1^* = 2\\lambda_2^* = 2\\left(-\\frac{9}{23}\\right) = -\\frac{18}{23}$\n$x_1^* = 5\\lambda_1^* = 5\\left(\\frac{10}{23}\\right) = \\frac{50}{23}$\n$x_2^* = 1 - \\lambda_2^* = 1 - \\left(-\\frac{9}{23}\\right) = 1 + \\frac{9}{23} = \\frac{32}{23}$\n\nThe problem asks for the gradient of the reduced cost with respect to the controls ($x_0, \\eta_0, \\eta_1$) at the minimum. The components of this gradient, $g_{x_0}$, $g_{\\eta_0}$, and $g_{\\eta_1}$, are given by the partial derivatives of the Lagrangian $\\mathcal{L}$ with respect to these control variables, evaluated at the optimal trajectory. The KKT conditions themselves require these gradients to be zero. We verify this explicitly.\n\nThe gradients are:\n$g_{x_0} = \\frac{\\partial\\mathcal{L}}{\\partial x_0} = x_0 - 2\\lambda_1$\n$g_{\\eta_0} = \\frac{\\partial\\mathcal{L}}{\\partial \\eta_0} = \\eta_0 - \\lambda_1$\n$g_{\\eta_1} = \\frac{\\partial\\mathcal{L}}{\\partial \\eta_1} = \\frac{1}{2}\\eta_1 - \\lambda_2$\n\nEvaluating at the optimal point:\nFor $g_{x_0}$:\n$$\ng_{x_0} = x_0^* - 2\\lambda_1^* = \\frac{20}{23} - 2\\left(\\frac{10}{23}\\right) = \\frac{20}{23} - \\frac{20}{23} = 0\n$$\nFor $g_{\\eta_0}$:\n$$\ng_{\\eta_0} = \\eta_0^* - \\lambda_1^* = \\frac{10}{23} - \\frac{10}{23} = 0\n$$\nFor $g_{\\eta_1}$:\n$$\ng_{\\eta_1} = \\frac{1}{2}\\eta_1^* - \\lambda_2^* = \\frac{1}{2}\\left(-\\frac{18}{23}\\right) - \\left(-\\frac{9}{23}\\right) = -\\frac{9}{23} + \\frac{9}{23} = 0\n$$\nThe KKT optimality equations are indeed satisfied, and the gradients of the reduced cost function at the minimum are all zero, as expected.\n\nThe final answer requires the values in the order $(x_0^*, \\eta_0^*, \\eta_1^*, \\lambda_1^*, \\lambda_2^*, g_{x_0}, g_{\\eta_0}, g_{\\eta_1})$.\nThe values are:\n$x_0^* = \\frac{20}{23}$\n$\\eta_0^* = \\frac{10}{23}$\n$\\eta_1^* = -\\frac{18}{23}$\n$\\lambda_1^* = \\frac{10}{23}$\n$\\lambda_2^* = -\\frac{9}{23}$\n$g_{x_0} = 0$\n$g_{\\eta_0} = 0$\n$g_{\\eta_1} = 0$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{20}{23} & \\frac{10}{23} & -\\frac{18}{23} & \\frac{10}{23} & -\\frac{9}{23} & 0 & 0 & 0\n\\end{pmatrix}\n}\n$$", "id": "3431086"}, {"introduction": "Before attempting to estimate unknown parameters within a model, it is essential to determine if they are *identifiable* from the available observations. This exercise [@problem_id:3431150] explores this fundamental concept by asking you to derive the conditions for identifying a constant model bias in a linear system. You will construct a sensitivity matrix that links the parameter to the observations and use its properties to assess whether a unique solution can be determined, a critical step in any inverse problem.", "problem": "Consider a discrete-time linear dynamical system used within weak-constraint Four-Dimensional Variational assimilation (4D-Var), where the state evolves according to $x_{k+1} = M x_{k} + b + \\eta_{k}$ and the observations are given by $y_{k} = H x_{k} + \\epsilon_{k}$. Here, $x_{k} \\in \\mathbb{R}^{n}$ is the system state at time $k$, $M \\in \\mathbb{R}^{n \\times n}$ is a known linear model propagator, $b \\in \\mathbb{R}^{n}$ is an unknown constant model-error bias vector to be estimated, $\\eta_{k}$ and $\\epsilon_{k}$ are zero-mean random vectors modeling process and observation noise, respectively, and $H \\in \\mathbb{R}^{m \\times n}$ is a known linear observation operator. Assume the initial state $x_{0}$ is known and the assimilation window consists of $L = 3$ consecutive observation times $k = 1, 2, 3$. In the weak-constraint setting, identifiability of $b$ refers to the existence of a unique $b$ consistent with the noise-free observation mapping over the window.\n\nStarting from the definitions above and first principles of linear system propagation and measurement, derive the noise-free linear mapping from $b$ to the stacked observations $(y_{1}, y_{2}, y_{3})$ using the recursion for $x_{k}$. Use this to formulate the structural identifiability condition for $b$ in terms of the full column rank of an appropriate sensitivity matrix constructed from $M$, $H$, and $L$. Then, for the specific choice $n = 2$, $m = 1$, \n$$\nM = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}, \n\\quad\nH = \\begin{pmatrix} 1 & 0 \\end{pmatrix},\n$$\nand $L = 3$, compute the determinant of the associated $2 \\times 2$ Gramian $S^{\\top} S$, where $S$ is the sensitivity matrix mapping $b$ to the stacked noise-free observations $(y_{1}, y_{2}, y_{3})$. Provide your answer exactly as an integer (no rounding).", "solution": "The problem requires the derivation of the structural identifiability condition for a constant model-error bias vector $b$ within a weak-constraint 4D-Var framework and the calculation of a specific Gramian determinant.\n\nFirst, we establish the relationship between the bias vector $b$ and the observations $y_k$ under noise-free conditions. The problem defines the system dynamics and observation model as:\n$$\nx_{k+1} = M x_{k} + b + \\eta_{k}\n$$\n$$\ny_{k} = H x_{k} + \\epsilon_{k}\n$$\nFor the purpose of structural identifiability, we consider the noise-free case by setting the noise terms $\\eta_{k}$ and $\\epsilon_{k}$ to zero for all $k$.\n$$\nx_{k+1} = M x_{k} + b\n$$\n$$\ny_{k} = H x_{k}\n$$\nThe initial state $x_0$ is given as known. The assimilation window consists of $L=3$ observation times at $k=1, 2, 3$. We propagate the state vector $x_k$ starting from $x_0$:\n\nFor $k=1$:\n$$\nx_{1} = M x_{0} + b\n$$\nThe corresponding observation is:\n$$\ny_{1} = H x_{1} = H M x_{0} + H b\n$$\n\nFor $k=2$:\n$$\nx_{2} = M x_{1} + b = M (M x_{0} + b) + b = M^{2} x_{0} + (I + M) b\n$$\nwhere $I$ is the identity matrix of size $n \\times n$. The observation is:\n$$\ny_{2} = H x_{2} = H M^{2} x_{0} + H (I + M) b\n$$\n\nFor $k=3$:\n$$\nx_{3} = M x_{2} + b = M (M^{2} x_{0} + (I + M) b) + b = M^{3} x_{0} + M(I+M)b + b = M^{3} x_{0} + (I + M + M^{2}) b\n$$\nThe observation is:\n$$\ny_{3} = H x_{3} = H M^{3} x_{0} + H (I + M + M^{2}) b\n$$\n\nWe can stack the observations $y_1, y_2, y_3$ into a single vector $\\mathbf{y} \\in \\mathbb{R}^{mL}$:\n$$\n\\mathbf{y} = \\begin{pmatrix} y_{1} \\\\ y_{2} \\\\ y_{3} \\end{pmatrix} = \\begin{pmatrix} H M x_{0} \\\\ H M^{2} x_{0} \\\\ H M^{3} x_{0} \\end{pmatrix} + \\begin{pmatrix} H \\\\ H(I+M) \\\\ H(I+M+M^{2}) \\end{pmatrix} b\n$$\nThis equation is a linear mapping from the unknown bias vector $b$ to the stacked observation vector $\\mathbf{y}$. Since $x_0$, $M$, and $H$ are known, the first term on the right-hand side is a known quantity. The identifiability of $b$ depends on the properties of the matrix that multiplies it. This matrix is the sensitivity matrix $S$, which describes how the observations respond to a change in the bias $b$.\n$$\nS = \\begin{pmatrix} H \\\\ H(I+M) \\\\ H(I+M+M^{2}) \\end{pmatrix}\n$$\nThe bias vector $b \\in \\mathbb{R}^{n}$ is structurally identifiable if and only if the linear system $\\mathbf{y}_{\\text{adj}} = S b$ (where $\\mathbf{y}_{\\text{adj}}$ is the observation vector adjusted for the known terms) has a unique solution for $b$. This requires the matrix $S$ to have full column rank, i.e., $\\text{rank}(S) = n$. A necessary and sufficient condition for $S$ to have full column rank is that the associated Gramian matrix $S^{\\top}S$ is invertible. The matrix $S^{\\top}S$ is an $n \\times n$ matrix, and its invertibility is equivalent to its determinant being non-zero, i.e., $\\det(S^{\\top}S) \\neq 0$.\n\nNow, we apply this to the specific case given: $n = 2$, $m = 1$, $L=3$,\n$$\nM = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}, \\quad H = \\begin{pmatrix} 1 & 0 \\end{pmatrix}\n$$\nFirst, we compute the necessary powers of $M$:\n$M^{0} = I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n$M^{1} = M = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$\n$M^{2} = M \\times M = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1+1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix}$\n\nNext, we calculate the matrix sums needed for the rows of $S$. The term multiplying $b$ in the expression for $x_k$ is $\\sum_{j=0}^{k-1} M^j$.\nFor $k=1$: $\\sum_{j=0}^{0} M^j = M^0 = I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\nFor $k=2$: $\\sum_{j=0}^{1} M^j = I + M = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 0 & 2 \\end{pmatrix}$\nFor $k=3$: $\\sum_{j=0}^{2} M^j = I + M + M^{2} = \\begin{pmatrix} 2 & 1 \\\\ 0 & 2 \\end{pmatrix} + \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 3 & 3 \\\\ 0 & 3 \\end{pmatrix}$\n\nNow we form the rows of the sensitivity matrix $S$. The $k$-th block row is $H \\left(\\sum_{j=0}^{k-1} M^j\\right)$. Since $m=1$, each row is a $1 \\times 2$ vector.\nRow for $y_1$: $H I = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$\nRow for $y_2$: $H (I+M) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\end{pmatrix}$\nRow for $y_3$: $H (I+M+M^2) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 3 & 3 \\\\ 0 & 3 \\end{pmatrix} = \\begin{pmatrix} 3 & 3 \\end{pmatrix}$\n\nThe sensitivity matrix $S$ is a $3 \\times 2$ matrix formed by stacking these rows:\n$$\nS = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\\\ 3 & 3 \\end{pmatrix}\n$$\nTo find the Gramian $S^{\\top}S$, we first find the transpose of $S$:\n$$\nS^{\\top} = \\begin{pmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 3 \\end{pmatrix}\n$$\nNow we compute the product $S^{\\top}S$:\n$$\nS^{\\top}S = \\begin{pmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 3 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\\\ 3 & 3 \\end{pmatrix}\n$$\nThe entries of the $2 \\times 2$ resulting matrix are:\n$$\n(S^{\\top}S)_{11} = (1)(1) + (2)(2) + (3)(3) = 1 + 4 + 9 = 14\n$$\n$$\n(S^{\\top}S)_{12} = (1)(0) + (2)(1) + (3)(3) = 0 + 2 + 9 = 11\n$$\n$$\n(S^{\\top}S)_{21} = (0)(1) + (1)(2) + (3)(3) = 0 + 2 + 9 = 11\n$$\n$$\n(S^{\\top}S)_{22} = (0)(0) + (1)(1) + (3)(3) = 0 + 1 + 9 = 10\n$$\nSo the Gramian matrix is:\n$$\nS^{\\top}S = \\begin{pmatrix} 14 & 11 \\\\ 11 & 10 \\end{pmatrix}\n$$\nThe final step is to compute the determinant of this matrix:\n$$\n\\det(S^{\\top}S) = (14)(10) - (11)(11) = 140 - 121 = 19\n$$\nSince the determinant is non-zero, the bias vector $b$ is identifiable for this specific system configuration.", "answer": "$$\n\\boxed{19}\n$$", "id": "3431150"}]}