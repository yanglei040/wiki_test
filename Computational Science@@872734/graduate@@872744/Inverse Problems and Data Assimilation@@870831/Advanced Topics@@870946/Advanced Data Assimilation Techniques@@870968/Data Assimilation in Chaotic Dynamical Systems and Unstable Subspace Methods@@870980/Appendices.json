{"hands_on_practices": [{"introduction": "To build a solid foundation, we begin with the simplest possible representation of a chaotic system's local dynamics: a single, unstable scalar mode. This exercise [@problem_id:3374492] challenges you to apply the Kalman filter, the optimal linear estimator, to this system. By deriving the steady-state error variance in closed form, you will gain direct insight into the fundamental tug-of-war between error growth, driven by the system's instability ($|a|  1$), and error reduction, provided by incoming observations.", "problem": "Consider a scalar, discrete-time, linear-Gaussian dynamical system intended to represent a single locally unstable direction of a chaotic flow. The state evolves according to $x_{k+1}=a x_k+\\eta_k$ with $|a|1$, where $\\{\\eta_k\\}$ is a sequence of independent and identically distributed zero-mean Gaussian random variables with variance $Q0$. Observations are given by $y_k=x_k+\\epsilon_k$, where $\\{\\epsilon_k\\}$ is a sequence of independent and identically distributed zero-mean Gaussian random variables with variance $R0$. Assume that $\\{\\eta_k\\}$, $\\{\\epsilon_k\\}$, and the initial state $x_0$ are mutually independent, and that one assimilates data by the optimal linear minimum-variance estimator under Gaussian assumptions (Kalman filter). Let $P_k$ denote the posterior (after-update) error variance of the state estimate at time $k$. Under the assumption that the estimator reaches a steady regime, define the steady-state posterior error variance $P=\\lim_{k\\to\\infty}P_k$.\n\nStarting from the laws of conditional Gaussians and the innovation-based Bayesian update for linear systems, derive the fixed-point equation that characterizes $P$ and solve it in closed form to obtain $P$ explicitly as a function of $a$, $Q$, and $R$. Then, analyze the dependence of $P$ on $a$, $Q$, and $R$ by discussing limiting behaviors as $R\\to 0$, $Q\\to 0$, and $|a|\\to\\infty$, and by explaining whether $P$ increases or decreases with each parameter. Express the final $P$ in a single closed-form analytic expression in terms of $a$, $Q$, and $R$. No numerical evaluation is required, and no rounding is needed. The final answer must be the single analytic expression for $P$.", "solution": "The user wants to solve for the steady-state posterior error variance of a Kalman filter for a one-dimensional, unstable, linear-Gaussian system.\n\nThe problem statement is first validated.\n\n### Step 1: Extract Givens\n- State evolution model: $x_{k+1}=a x_k+\\eta_k$, where $|a|1$.\n- State noise: $\\{\\eta_k\\}$ is a sequence of i.i.d. random variables with $\\eta_k \\sim \\mathcal{N}(0, Q)$, where $Q0$.\n- Observation model: $y_k=x_k+\\epsilon_k$.\n- Observation noise: $\\{\\epsilon_k\\}$ is a sequence of i.i.d. random variables with $\\epsilon_k \\sim \\mathcal{N}(0, R)$, where $R0$.\n- Independence: $\\{\\eta_k\\}$, $\\{\\epsilon_k\\}$, and $x_0$ are mutually independent.\n- Estimator: Optimal linear minimum-variance estimator (Kalman filter).\n- Definition: $P_k$ is the posterior (after-update) error variance of the state estimate at time $k$.\n- Definition: $P = \\lim_{k\\to\\infty} P_k$ is the steady-state posterior error variance.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes a standard discrete-time Kalman filter, a fundamental tool in data assimilation, estimation theory, and control. The system is linear and the noise is Gaussian, which are the classic assumptions under which the Kalman filter is optimal. The focus on an unstable mode ($|a|1$) and its steady-state error variance is a canonical problem in the study of chaotic systems and data assimilation. The problem is scientifically and mathematically sound.\n- **Well-Posed**: The problem is well-posed. The existence of a unique, positive, steady-state solution to the algebraic Riccati equation for a time-invariant, detectable, and stabilizable system is a standard result. Here, the system is scalar, with observation matrix $H=1$ (so it is always observable, hence detectable) and process noise $Q0$ (so it is stabilizable). A unique, stable, and meaningful solution for $P$ exists.\n- **Objective**: The problem is stated in precise, objective mathematical language.\n- **Completeness**: All necessary parameters ($a, Q, R$) and statistical properties are defined. The problem is self-contained.\n- **No other flaws are detected.** The problem is non-trivial, requiring derivation and algebraic manipulation.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe Kalman filter operates in a two-step cycle: prediction (forecast) and update (analysis). Let $x_k^a$ be the state estimate after assimilating observation $y_k$ (the posterior estimate), and let $P_k$ be its error variance. Let $x_k^f$ be the forecast of the state at time $k$ based on information up to time $k-1$, and let $P_k^f$ be its error variance.\n\n**1. Prediction Step:**\nThe state is propagated forward in time using the dynamical model.\nThe state forecast is $x_k^f = a x_{k-1}^a$.\nThe forecast error variance is propagated as:\n$$P_k^f = a^2 P_{k-1} + Q$$\nHere, we use the notation $P_{k-1}$ for the posterior variance at time $k-1$.\n\n**2. Update Step:**\nThe problem asks to start from the laws of conditional Gaussians. The forecast provides a Gaussian prior for the state $x_k$, $p(x_k|y_{1:k-1}) = \\mathcal{N}(x_k; x_k^f, P_k^f)$. The observation $y_k$ provides a likelihood, $p(y_k|x_k) = \\mathcal{N}(y_k; x_k, R)$. The posterior distribution $p(x_k|y_{1:k})$ is proportional to the product of the prior and the likelihood. For Gaussians, this Bayesian update is conveniently expressed in terms of precisions (inverse variances). The posterior precision is the sum of the prior precision and the data precision (from the likelihood):\n$$(P_k)^{-1} = (P_k^f)^{-1} + R^{-1}$$\nThis is the update equation for the error variance in the information filter form. It is equivalent to the more standard covariance update form $P_k = (1 - K_k)P_k^f$ where $K_k = P_k^f(P_k^f+R)^{-1}$ is the Kalman gain.\n\n**3. Riccati Recurrence Relation:**\nCombining the prediction and update steps, we obtain a recurrence relation for the posterior variance $P_k$:\n$$P_k = \\left( (P_k^f)^{-1} + R^{-1} \\right)^{-1} = \\left( (a^2 P_{k-1} + Q)^{-1} + R^{-1} \\right)^{-1}$$\nThis is the discrete-time algebraic Riccati equation for this scalar system.\n\n**4. Steady-State Fixed-Point Equation:**\nIn the steady-state regime, the variance becomes constant: $\\lim_{k\\to\\infty} P_k = \\lim_{k\\to\\infty} P_{k-1} = P$. The recurrence relation becomes a fixed-point equation for $P$:\n$$P = \\left( (a^2 P + Q)^{-1} + R^{-1} \\right)^{-1}$$\nTo solve for $P$, we can invert both sides:\n$$P^{-1} = (a^2 P + Q)^{-1} + R^{-1}$$\n$$\\frac{1}{P} = \\frac{1}{a^2 P + Q} + \\frac{1}{R}$$\nWe rearrange the terms to solve for $P$:\n$$\\frac{1}{P} - \\frac{1}{R} = \\frac{1}{a^2 P + Q}$$\n$$\\frac{R - P}{PR} = \\frac{1}{a^2 P + Q}$$\n$$(R - P)(a^2 P + Q) = PR$$\n$$a^2 RP + QR - a^2 P^2 - QP = PR$$\nRearranging into a standard quadratic form $A P^2 + B P + C = 0$:\n$$-a^2 P^2 + (a^2 R - Q - R)P + QR = 0$$\n$$a^2 P^2 - (a^2 R - R - Q)P - QR = 0$$\n$$a^2 P^2 + (Q + R - a^2 R)P - QR = 0$$\nThis is the fixed-point equation that characterizes $P$.\n\n**5. Solving the Quadratic Equation:**\nWe solve for $P$ using the quadratic formula $P = \\frac{-B \\pm \\sqrt{B^2 - 4AC}}{2A}$, with coefficients:\n$A = a^2$\n$B = Q + (1-a^2)R$\n$C = -QR$\nThe discriminant $\\Delta$ is:\n$$\\Delta = B^2 - 4AC = (Q + (1-a^2)R)^2 - 4(a^2)(-QR) = (Q + (1-a^2)R)^2 + 4a^2QR$$\nThe solutions for $P$ are:\n$$P = \\frac{-(Q + (1-a^2)R) \\pm \\sqrt{(Q + (1-a^2)R)^2 + 4a^2QR}}{2a^2}$$\n$$P = \\frac{(a^2-1)R - Q \\pm \\sqrt{\\Delta}}{2a^2}$$\nSince $P$ is an error variance, it must be a positive real number, $P > 0$. The denominator $2a^2$ is positive. We must analyze the numerator. Let $Y = (a^2-1)R - Q$. The numerator is $Y \\pm \\sqrt{(Q - Y)^2+4a^2QR} = Y \\pm \\sqrt{Y^2+4a^2QR}$. Because $a^20$, $Q0$, and $R0$, the term $4a^2QR$ is strictly positive. Thus, $\\sqrt{Y^2+4a^2QR} > \\sqrt{Y^2} = |Y|$.\n- If we choose the '$-$' sign, the numerator is $Y - \\sqrt{Y^2+4a^2QR}$, which is always negative. This would lead to a negative $P$, which is physically impossible.\n- Therefore, we must choose the '$+$' sign.\n\nThe unique physically meaningful solution is:\n$$P = \\frac{(a^2-1)R - Q + \\sqrt{(Q + (1-a^2)R)^2 + 4a^2QR}}{2a^2}$$\nThis is the closed-form analytic expression for the steady-state posterior error variance $P$. The expression depends on $a^2$, so it is independent of the sign of $a$, only its magnitude.\n\n**6. Analysis of Dependencies:**\n- **As $R \\to 0$ (perfect observations):**\n  $P \\to \\frac{-Q + \\sqrt{Q^2}}{2a^2} = \\frac{-Q+Q}{2a^2} = 0$.\n  This is expected, as perfect observations ($R=0$) mean the state is known exactly, so its error variance is zero.\n- **As $Q \\to 0$ (perfect model):**\n  $P \\to \\frac{(a^2-1)R + \\sqrt{((1-a^2)R)^2}}{2a^2} = \\frac{(a^2-1)R + \\sqrt{(a^2-1)^2R^2}}{2a^2}$.\n  Since $|a|1$, $a^2-10$, so $\\sqrt{(a^2-1)^2R^2} = (a^2-1)R$.\n  $P \\to \\frac{(a^2-1)R + (a^2-1)R}{2a^2} = \\frac{2(a^2-1)R}{2a^2} = (1 - \\frac{1}{a^2})R$.\n  The filter reaches a non-zero error variance because the unstable dynamics amplify any initial uncertainty, which is then counteracted by the observations.\n- **As $|a| \\to \\infty$ (highly unstable system):**\n  We divide the numerator and denominator by $a^2$:\n  $$P = \\frac{(1-1/a^2)R - Q/a^2 + \\sqrt{(Q/a^2 + (1/a^2-1)R)^2 + 4QR/a^2}}{2}$$\n  As $a^2 \\to \\infty$, the terms with $a^2$ in the denominator go to zero:\n  $$P \\to \\frac{(1-0)R - 0 + \\sqrt{((0-1)R)^2 + 0}}{2} = \\frac{R + \\sqrt{R^2}}{2} = \\frac{R+R}{2} = R$$\n  If the system is extremely unstable, the forecast error $P_k^f$ becomes very large, making the prior information negligible. The posterior estimate is almost entirely determined by the observation, so its error variance approaches the observation error variance $R$.\n- **Dependence on parameters:**\n  One can show through implicit differentiation of the quadratic equation that $\\frac{\\partial P}{\\partial a^2}  0$, $\\frac{\\partial P}{\\partial Q}  0$, and $\\frac{\\partial P}{\\partial R}  0$. This means the posterior error variance $P$ increases with the instability $|a|$, the model noise variance $Q$, and the observation noise variance $R$. This is intuitively correct, as each of these factors increases the uncertainty in the system.", "answer": "$$\\boxed{\\frac{(a^2-1)R - Q + \\sqrt{(Q + (1-a^2)R)^2 + 4a^2QR}}{2a^2}}$$", "id": "3374492"}, {"introduction": "Moving from a single dimension to a more realistic multi-dimensional state space introduces new challenges. In many practical applications, we can only observe a fraction of the system's state variables. This exercise [@problem_id:3374524] explores the crucial concept of observability, investigating whether the system's internal dynamics can reveal information about initially unobserved components over time. By constructing and analyzing the observability matrix for a simple 3D system, you will determine precisely which parts of the state can be constrained by a sequence of observations and which remain hidden.", "problem": "Consider the discrete-time tangent linear model for a chaotic dynamical system along a given trajectory,\n$$\nx_{k+1} = A x_k, \\quad y_k = H x_k,\n$$\nwhere $x_k \\in \\mathbb{R}^3$ and $y_k \\in \\mathbb{R}$. The state transition matrix $A$ captures one unstable, one neutral, and one stable direction and is given by\n$$\nA = \\begin{pmatrix}\n1.5  1  0 \\\\\n0  1  0.5 \\\\\n0  0  0.3\n\\end{pmatrix},\n$$\nand the observation operator is\n$$\nH = \\begin{pmatrix} 0  1  0 \\end{pmatrix}.\n$$\nAssume that the scalar observation $y_k$ is collected at times $k = 0, 1, 2$. Using the definition of the observability matrix over $n$ steps,\n$$\n\\mathcal{O}_n = \\begin{pmatrix}\nH \\\\\nH A \\\\\n\\vdots \\\\\nH A^{n-1}\n\\end{pmatrix},\n$$\nform $\\mathcal{O}_3$ and, starting from the fundamental definition of observability for linear time-invariant systems, compute $\\operatorname{rank}(\\mathcal{O}_3)$. Then, based solely on this rank and the definition of the unobservable subspace as the null space of $\\mathcal{O}_3$, determine whether any initially unobserved state components are identifiable from the observations over $3$ steps. Provide as your final numeric answer the rank $\\operatorname{rank}(\\mathcal{O}_3)$.", "solution": "The problem requires the validation of a given linear time-invariant system's observability over a finite time window. We are given the state-space model:\n$$\nx_{k+1} = A x_k\n$$\n$$\ny_k = H x_k\n$$\nwhere the state vector $x_k \\in \\mathbb{R}^3$ and the scalar observation $y_k \\in \\mathbb{R}$. The state transition matrix $A$ and the observation operator $H$ are provided as:\n$$\nA = \\begin{pmatrix}\n1.5  1  0 \\\\\n0  1  0.5 \\\\\n0  0  0.3\n\\end{pmatrix}, \\quad H = \\begin{pmatrix} 0  1  0 \\end{pmatrix}\n$$\nThe problem asks to form the observability matrix $\\mathcal{O}_3$ for an observation window of $n=3$ steps (at times $k=0, 1, 2$), compute its rank, and discuss the implications for the identifiability of the initial state components $x_0$.\n\nThe observability matrix for $n=3$ is defined as:\n$$\n\\mathcal{O}_3 = \\begin{pmatrix}\nH \\\\\nHA \\\\\nHA^2\n\\end{pmatrix}\n$$\nWe proceed to compute the rows of this matrix.\n\nThe first row is simply the observation operator $H$:\n$$\nH = \\begin{pmatrix} 0  1  0 \\end{pmatrix}\n$$\n\nThe second row is the product $HA$:\n$$\nHA = \\begin{pmatrix} 0  1  0 \\end{pmatrix} \\begin{pmatrix}\n1.5  1  0 \\\\\n0  1  0.5 \\\\\n0  0  0.3\n\\end{pmatrix} = \\begin{pmatrix}\n(0)(1.5) + (1)(0) + (0)(0)  (0)(1) + (1)(1) + (0)(0)  (0)(0) + (1)(0.5) + (0)(0.3)\n\\end{pmatrix}\n$$\n$$\nHA = \\begin{pmatrix} 0  1  0.5 \\end{pmatrix}\n$$\n\nThe third row is $HA^2$. First, we compute $A^2$:\n$$\nA^2 = A \\cdot A = \\begin{pmatrix}\n1.5  1  0 \\\\\n0  1  0.5 \\\\\n0  0  0.3\n\\end{pmatrix} \\begin{pmatrix}\n1.5  1  0 \\\\\n0  1  0.5 \\\\\n0  0  0.3\n\\end{pmatrix}\n$$\n$$\nA^2 = \\begin{pmatrix}\n(1.5)(1.5) + (1)(0) + (0)(0)  (1.5)(1) + (1)(1) + (0)(0)  (1.5)(0) + (1)(0.5) + (0)(0.3) \\\\\n(0)(1.5) + (1)(0) + (0.5)(0)  (0)(1) + (1)(1) + (0.5)(0)  (0)(0) + (1)(0.5) + (0.5)(0.3) \\\\\n(0)(1.5) + (0)(0) + (0.3)(0)  (0)(1) + (0)(1) + (0.3)(0)  (0)(0) + (0)(0.5) + (0.3)(0.3)\n\\end{pmatrix}\n$$\n$$\nA^2 = \\begin{pmatrix}\n2.25  1.5 + 1  0.5 \\\\\n0  1  0.5 + 0.15 \\\\\n0  0  0.09\n\\end{pmatrix} = \\begin{pmatrix}\n2.25  2.5  0.5 \\\\\n0  1  0.65 \\\\\n0  0  0.09\n\\end{pmatrix}\n$$\nNow, we can compute $HA^2$:\n$$\nHA^2 = \\begin{pmatrix} 0  1  0 \\end{pmatrix} \\begin{pmatrix}\n2.25  2.5  0.5 \\\\\n0  1  0.65 \\\\\n0  0  0.09\n\\end{pmatrix} = \\begin{pmatrix}\n(0)(2.25) + (1)(0) + (0)(0)  (0)(2.5) + (1)(1) + (0)(0)  (0)(0.5) + (1)(0.65) + (0)(0.09)\n\\end{pmatrix}\n$$\n$$\nHA^2 = \\begin{pmatrix} 0  1  0.65 \\end{pmatrix}\n$$\nNow we assemble the observability matrix $\\mathcal{O}_3$:\n$$\n\\mathcal{O}_3 = \\begin{pmatrix}\n0  1  0 \\\\\n0  1  0.5 \\\\\n0  1  0.65\n\\end{pmatrix}\n$$\nTo compute the rank of $\\mathcal{O}_3$, we can use Gaussian elimination to find its row echelon form. Let $R_1, R_2, R_3$ denote the rows of $\\mathcal{O}_3$.\nThe matrix is $\\begin{pmatrix} 0  1  0 \\\\ 0  1  0.5 \\\\ 0  1  0.65 \\end{pmatrix}$.\nPerform the row operation $R_2 \\leftarrow R_2 - R_1$:\n$$\n\\begin{pmatrix} 0  1  0 \\\\ 0  1-1  0.5-0 \\\\ 0  1  0.65 \\end{pmatrix} = \\begin{pmatrix} 0  1  0 \\\\ 0  0  0.5 \\\\ 0  1  0.65 \\end{pmatrix}\n$$\nPerform the row operation $R_3 \\leftarrow R_3 - R_1$:\n$$\n\\begin{pmatrix} 0  1  0 \\\\ 0  0  0.5 \\\\ 0  1-1  0.65-0 \\end{pmatrix} = \\begin{pmatrix} 0  1  0 \\\\ 0  0  0.5 \\\\ 0  0  0.65 \\end{pmatrix}\n$$\nFinally, perform the row operation $R_3 \\leftarrow R_3 - (\\frac{0.65}{0.5})R_2 = R_3 - 1.3 R_2$:\n$$\n\\begin{pmatrix} 0  1  0 \\\\ 0  0  0.5 \\\\ 0  0  0.65 - 1.3(0.5) \\end{pmatrix} = \\begin{pmatrix} 0  1  0 \\\\ 0  0  0.5 \\\\ 0  0  0.65 - 0.65 \\end{pmatrix} = \\begin{pmatrix} 0  1  0 \\\\ 0  0  0.5 \\\\ 0  0  0 \\end{pmatrix}\n$$\nThe row echelon form has two non-zero rows. Therefore, the rank of the matrix is $2$.\n$$\n\\operatorname{rank}(\\mathcal{O}_3) = 2\n$$\nAccording to the fundamental definition of observability, a linear time-invariant system with state dimension $n$ is observable if and only if its observability matrix $\\mathcal{O}_n$ has full column rank, i.e., $\\operatorname{rank}(\\mathcal{O}_n) = n$. In this case, the state dimension is $n=3$, but we found that $\\operatorname{rank}(\\mathcal{O}_3) = 2$. Since $2  3$, the system is not fully observable over the $3$-step observation window.\n\nThe unobservable subspace is the null space of $\\mathcal{O}_3$. The dimension of this subspace is given by the rank-nullity theorem: $\\dim(\\operatorname{null}(\\mathcal{O}_3)) = n - \\operatorname{rank}(\\mathcal{O}_3) = 3 - 2 = 1$. This means there is a one-dimensional subspace of initial states $x_0$ that produce zero observations for all time, and thus cannot be determined from the measurements.\nLet's find the null space by solving $\\mathcal{O}_3 x_0 = 0$ for $x_0 = (x_{0,1}, x_{0,2}, x_{0,3})^T$:\n$$\n\\begin{pmatrix} 0  1  0 \\\\ 0  1  0.5 \\\\ 0  1  0.65 \\end{pmatrix} \\begin{pmatrix} x_{0,1} \\\\ x_{0,2} \\\\ x_{0,3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis yields the system of equations: $x_{0,2} = 0$, $x_{0,2} + 0.5 x_{0,3} = 0$, and $x_{0,2} + 0.65 x_{0,3} = 0$.\nThe first equation gives $x_{0,2} = 0$. Substituting this into the second and third equations gives $0.5 x_{0,3} = 0$ and $0.65 x_{0,3} = 0$, which both imply $x_{0,3} = 0$. The variable $x_{0,1}$ is unconstrained. Thus, the null space is the set of all vectors of the form $(c, 0, 0)^T$ for any scalar $c \\in \\mathbb{R}$. The unobservable subspace is $\\operatorname{span}\\{(1, 0, 0)^T\\}$.\n\nThe question asks whether any *initially unobserved* state components are identifiable. At time $k=0$, the observation is $y_0 = H x_0 = x_{0,2}$. This means components $x_{0,1}$ and $x_{0,3}$ are not directly observed at the initial time. The sequence of observations relates to the initial state via $Y = \\mathcal{O}_3 x_0$, where $Y = (y_0, y_1, y_2)^T$.\n$$\n\\begin{pmatrix} y_0 \\\\ y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} x_{0,2} \\\\ x_{0,2} + 0.5 x_{0,3} \\\\ x_{0,2} + 0.65 x_{0,3} \\end{pmatrix}\n$$\nFrom this system of equations, we can determine $x_{0,2}$ directly from $y_0$. Then, using $y_1$, we can determine $x_{0,3}$ as $x_{0,3} = (y_1 - x_{0,2})/0.5 = 2(y_1 - y_0)$. However, there is no information in the observations that allows for the determination of $x_{0,1}$.\n\nTherefore, one of the initially unobserved components, $x_{0,3}$, becomes identifiable from the sequence of observations, while the other, $x_{0,1}$, remains unobservable. The existence of the one-dimensional unobservable subspace, as determined from the rank of $\\mathcal{O}_3$, confirms that not all components are identifiable.\n\nThe final numeric answer is the rank itself.", "answer": "$$\\boxed{2}$$", "id": "3374524"}, {"introduction": "Unstable subspace methods are powerful, but their effectiveness hinges on an accurate estimate of the system's unstable directions. This advanced practice [@problem_id:3374551] delves into a critical practical issue: what happens when our estimate of the unstable subspace is wrong? By modeling this as a rotational error in a 2D system, you will derive an analytic expression for the resulting amplification of analysis error. This exercise provides a quantitative framework for understanding the sensitivity of assimilation performance to the accuracy of the subspace estimate, a key consideration for real-world implementations.", "problem": "Consider a two-dimensional linearized chaotic dynamical system near an attractor whose tangent dynamics admit one unstable direction and one stable direction. Let the true unstable unit vector be $e_{u} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ and the stable unit vector be $e_{s} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$. Assume a forecast (background) error $e^{f} \\in \\mathbb{R}^{2}$ that is zero-mean Gaussian with covariance aligned to the true Lyapunov directions,\n$$\n\\operatorname{Cov}(e^{f}) = S = \\begin{pmatrix} \\sigma_{u}^{2}  0 \\\\ 0  \\sigma_{s}^{2} \\end{pmatrix},\n$$\nwith $\\sigma_{u}^{2}  \\sigma_{s}^{2}  0$. Suppose we assimilate a full observation $y = x + v$, where the observation noise $v$ is zero-mean Gaussian, independent of $e^{f}$, with covariance $R = r I_{2}$, where $r  0$ and $I_{2}$ is the $2 \\times 2$ identity matrix.\n\nAn unstable-subspace analysis update is applied as a linear estimator,\n$$\nx^{a} = x^{f} + K \\left(y - x^{f}\\right),\n$$\nwith gain restricted to an estimated one-dimensional unstable subspace. The estimated unstable unit vector is a rotation of the true unstable vector by an angle $\\theta$ (in radians),\n$$\n\\hat{e}_{u}(\\theta) = \\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix},\n$$\nand the associated projector is $P_{u}(\\theta) = \\hat{e}_{u}(\\theta)\\,\\hat{e}_{u}(\\theta)^{\\top}$. The gain is $K = \\gamma P_{u}(\\theta)$ with a fixed scalar $\\gamma \\in \\mathbb{R}$ satisfying $0  \\gamma  2$. Define the analysis error $e^{a} = x^{a} - x^{\\text{true}}$.\n\nUsing only the fundamental definitions given above, and assuming Euclidean norm on $\\mathbb{R}^{2}$, derive the closed-form analytic expression for the error amplification factor as a function of the rotation angle $\\theta$,\n$$\nA(\\theta) = \\frac{\\mathbb{E}\\!\\left[\\|e^{a}(\\theta)\\|^{2}\\right]}{\\mathbb{E}\\!\\left[\\|e^{a}(0)\\|^{2}\\right]},\n$$\nwhere the expectation is over the joint distribution of $e^{f}$ and $v$. Express your final result in terms of $\\sigma_{u}^{2}$, $\\sigma_{s}^{2}$, $r$, $\\gamma$, and $\\theta$. Angles must be expressed in radians. Your final answer must be a single closed-form analytic expression. No units are required.", "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and contains all necessary information for a unique solution.\n\n### Step 1: Extract Givens\n-   System: Two-dimensional linearized chaotic dynamical system.\n-   True unstable unit vector: $e_{u} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$.\n-   True stable unit vector: $e_{s} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$.\n-   Forecast error: $e^{f} \\in \\mathbb{R}^{2}$, a zero-mean Gaussian random vector.\n-   Forecast error covariance: $\\operatorname{Cov}(e^{f}) = S = \\begin{pmatrix} \\sigma_{u}^{2}  0 \\\\ 0  \\sigma_{s}^{2} \\end{pmatrix}$, with $\\sigma_{u}^{2}  \\sigma_{s}^{2}  0$.\n-   Observation: $y = x^{\\text{true}} + v$.\n-   Observation noise: $v$, a zero-mean Gaussian random vector, independent of $e^{f}$.\n-   Observation noise covariance: $R = r I_{2}$, where $r  0$ and $I_{2}$ is the $2 \\times 2$ identity matrix.\n-   Analysis update formula: $x^{a} = x^{f} + K \\left(y - x^{f}\\right)$.\n-   Estimated unstable unit vector: $\\hat{e}_{u}(\\theta) = \\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix}$.\n-   Projector onto the estimated unstable subspace: $P_{u}(\\theta) = \\hat{e}_{u}(\\theta)\\,\\hat{e}_{u}(\\theta)^{\\top}$.\n-   Gain matrix: $K = \\gamma P_{u}(\\theta)$, with a fixed scalar $\\gamma \\in \\mathbb{R}$ satisfying $0  \\gamma  2$.\n-   Analysis error: $e^{a} = x^{a} - x^{\\text{true}}$.\n-   Objective: Derive the error amplification factor $A(\\theta) = \\frac{\\mathbb{E}\\!\\left[\\|e^{a}(\\theta)\\|^{2}\\right]}{\\mathbb{E}\\!\\left[\\|e^{a}(0)\\|^{2}\\right]}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, rooted in the established principles of data assimilation and linear estimation theory. The scenario describes a simplified but conceptually sound model of an unstable-subspace data assimilation method. The problem is well-posed, providing all necessary definitions, distributions, and constraints to derive the requested quantity. The language is objective and mathematically precise. All parameters are clearly defined. The problem is self-contained and does not violate any physical or mathematical principles.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full, reasoned solution will be provided.\n\n### Derivation of the Solution\nThe analysis error, $e^{a}$, is defined as $e^{a} = x^{a} - x^{\\text{true}}$. We first express $e^{a}$ in terms of the forecast error, $e^{f} = x^{f} - x^{\\text{true}}$, and the observation noise, $v$.\nSubstituting the analysis update formula into the definition of $e^{a}$:\n$$e^{a} = x^{f} + K(y - x^{f}) - x^{\\text{true}}$$\nUsing the definition of the observation, $y = x^{\\text{true}} + v$:\n$$e^{a} = x^{f} + K(x^{\\text{true}} + v - x^{f}) - x^{\\text{true}}$$\nRearranging terms:\n$$e^{a} = (I_{2} - K)(x^{f} - x^{\\text{true}}) + K v$$\nwhere $I_2$ is the $2 \\times 2$ identity matrix. This simplifies to:\n$$e^{a} = (I_{2} - K)e^{f} + K v$$\nThe quantity of interest is the expected squared Euclidean norm of the analysis error, $\\mathbb{E}\\!\\left[\\|e^{a}\\|^{2}\\right]$. This is equivalent to the trace of the analysis error covariance matrix, $P^{a} = \\mathbb{E}[e^{a}(e^{a})^{\\top}]$.\n$$P^{a} = \\mathbb{E}\\![((I_{2} - K)e^{f} + K v)((I_{2} - K)e^{f} + K v)^{\\top}]$$\nExpanding the expression:\n$$P^{a} = \\mathbb{E}\\![ (I_{2} - K)e^{f}(e^{f})^{\\top}(I_{2} - K)^{\\top} + (I_{2} - K)e^{f}v^{\\top}K^{\\top} + K v (e^{f})^{\\top}(I_{2} - K)^{\\top} + K v v^{\\top}K^{\\top} ]$$\nSince $e^{f}$ and $v$ are independent and have zero mean, the expectation of the cross-terms is zero. Thus, we have:\n$$P^{a} = (I_{2} - K)\\mathbb{E}[e^{f}(e^{f})^{\\top}](I_{2} - K)^{\\top} + K\\mathbb{E}[v v^{\\top}]K^{\\top}$$\nSubstituting the given covariance matrices $S = \\mathbb{E}[e^{f}(e^{f})^{\\top}]$ and $R = \\mathbb{E}[v v^{\\top}]$:\n$$P^{a} = (I_{2} - K)S(I_{2} - K)^{\\top} + KRK^{\\top}$$\nThe gain matrix is $K = \\gamma P_{u}(\\theta)$. The projector $P_{u}(\\theta)$ is symmetric ($P_{u}(\\theta)^{\\top} = P_{u}(\\theta)$), so $K$ is also symmetric ($K^{\\top} = K$). Substituting $K$ and $K^{\\top}$:\n$$P^{a} = (I_{2} - \\gamma P_{u}(\\theta))S(I_{2} - \\gamma P_{u}(\\theta)) + (\\gamma P_{u}(\\theta))R(\\gamma P_{u}(\\theta))$$\n$$P^{a} = S - \\gamma P_{u}(\\theta)S - \\gamma S P_{u}(\\theta) + \\gamma^{2} P_{u}(\\theta)SP_{u}(\\theta) + \\gamma^{2} P_{u}(\\theta)RP_{u}(\\theta)$$\nThe expected squared error is $J(\\theta) = \\mathbb{E}\\!\\left[\\|e^{a}(\\theta)\\|^{2}\\right] = \\operatorname{trace}(P^{a})$. Using the linearity and cyclic property of the trace operator ($\\operatorname{trace}(AB) = \\operatorname{trace}(BA)$):\n$$J(\\theta) = \\operatorname{trace}(S) - 2\\gamma\\operatorname{trace}(P_{u}(\\theta)S) + \\gamma^{2}\\operatorname{trace}(P_{u}(\\theta)SP_{u}(\\theta)) + \\gamma^{2}\\operatorname{trace}(P_{u}(\\theta)RP_{u}(\\theta))$$\nWe use the properties of the projector $P_{u}(\\theta) = \\hat{e}_{u}(\\theta)\\hat{e}_{u}(\\theta)^{\\top}$. For any matrix $M$:\n$\\operatorname{trace}(P_{u}(\\theta)M) = \\operatorname{trace}(\\hat{e}_{u}(\\theta)\\hat{e}_{u}(\\theta)^{\\top}M) = \\operatorname{trace}(\\hat{e}_{u}(\\theta)^{\\top}M\\hat{e}_{u}(\\theta)) = \\hat{e}_{u}(\\theta)^{\\top}M\\hat{e}_{u}(\\theta)$.\nAlso, $\\operatorname{trace}(P_{u}(\\theta)MP_{u}(\\theta)) = \\operatorname{trace}(P_{u}(\\theta)M) = \\hat{e}_{u}(\\theta)^{\\top}M\\hat{e}_{u}(\\theta)$, since $P_{u}(\\theta)^{2}=P_{u}(\\theta)$.\nApplying these properties:\n$$J(\\theta) = \\operatorname{trace}(S) - (2\\gamma - \\gamma^{2})\\hat{e}_{u}(\\theta)^{\\top}S\\hat{e}_{u}(\\theta) + \\gamma^{2}\\hat{e}_{u}(\\theta)^{\\top}R\\hat{e}_{u}(\\theta)$$\nNow, we evaluate the quadratic forms:\n$\\hat{e}_{u}(\\theta)^{\\top}S\\hat{e}_{u}(\\theta) = \\begin{pmatrix} \\cos\\theta  \\sin\\theta \\end{pmatrix} \\begin{pmatrix} \\sigma_{u}^{2}  0 \\\\ 0  \\sigma_{s}^{2} \\end{pmatrix} \\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix} = \\sigma_{u}^{2}\\cos^{2}\\theta + \\sigma_{s}^{2}\\sin^{2}\\theta$.\n$\\hat{e}_{u}(\\theta)^{\\top}R\\hat{e}_{u}(\\theta) = \\hat{e}_{u}(\\theta)^{\\top}(rI_{2})\\hat{e}_{u}(\\theta) = r\\,\\hat{e}_{u}(\\theta)^{\\top}\\hat{e}_{u}(\\theta) = r\\|\\hat{e}_{u}(\\theta)\\|^{2} = r$.\nAnd $\\operatorname{trace}(S) = \\sigma_{u}^{2} + \\sigma_{s}^{2}$.\nSubstituting these into the expression for $J(\\theta)$:\n$$J(\\theta) = (\\sigma_{u}^{2} + \\sigma_{s}^{2}) - (2\\gamma - \\gamma^{2})(\\sigma_{u}^{2}\\cos^{2}\\theta + \\sigma_{s}^{2}\\sin^{2}\\theta) + \\gamma^{2}r$$\nThis is the numerator of the amplification factor. The denominator is $J(0)$, which corresponds to the case with no angular error, $\\theta = 0$. At $\\theta=0$, $\\cos(0)=1$ and $\\sin(0)=0$:\n$$J(0) = (\\sigma_{u}^{2} + \\sigma_{s}^{2}) - (2\\gamma - \\gamma^{2})\\sigma_{u}^{2} + \\gamma^{2}r$$\nWe can simplify $J(0)$ further:\n$$J(0) = \\sigma_{u}^{2} - 2\\gamma\\sigma_{u}^{2} + \\gamma^{2}\\sigma_{u}^{2} + \\sigma_{s}^{2} + \\gamma^{2}r = (1 - 2\\gamma + \\gamma^{2})\\sigma_{u}^{2} + \\sigma_{s}^{2} + \\gamma^{2}r = (1-\\gamma)^{2}\\sigma_{u}^{2} + \\sigma_{s}^{2} + \\gamma^{2}r$$\nTo simplify the ratio $A(\\theta) = J(\\theta)/J(0)$, we rewrite $J(\\theta)$ in terms of $J(0)$.\n$$J(\\theta) = (\\sigma_{u}^{2} + \\sigma_{s}^{2}) - (2\\gamma - \\gamma^{2})(\\sigma_{u}^{2}(1-\\sin^{2}\\theta) + \\sigma_{s}^{2}\\sin^{2}\\theta) + \\gamma^{2}r$$\n$$J(\\theta) = (\\sigma_{u}^{2} + \\sigma_{s}^{2}) - (2\\gamma - \\gamma^{2})\\sigma_{u}^{2} + (2\\gamma - \\gamma^{2})\\sigma_{u}^{2}\\sin^{2}\\theta - (2\\gamma - \\gamma^{2})\\sigma_{s}^{2}\\sin^{2}\\theta + \\gamma^{2}r$$\nGroup the terms to isolate $J(0)$:\n$$J(\\theta) = \\left[(\\sigma_{u}^{2} + \\sigma_{s}^{2}) - (2\\gamma - \\gamma^{2})\\sigma_{u}^{2} + \\gamma^{2}r\\right] + (2\\gamma - \\gamma^{2})(\\sigma_{u}^{2} - \\sigma_{s}^{2})\\sin^{2}\\theta$$\n$$J(\\theta) = J(0) + (2\\gamma - \\gamma^{2})(\\sigma_{u}^{2} - \\sigma_{s}^{2})\\sin^{2}\\theta$$\nThe error amplification factor $A(\\theta)$ is the ratio $J(\\theta)/J(0)$:\n$$A(\\theta) = \\frac{J(0) + (2\\gamma - \\gamma^{2})(\\sigma_{u}^{2} - \\sigma_{s}^{2})\\sin^{2}\\theta}{J(0)} = 1 + \\frac{(2\\gamma - \\gamma^{2})(\\sigma_{u}^{2} - \\sigma_{s}^{2})\\sin^{2}\\theta}{J(0)}$$\nSubstituting the simplified expression for $J(0)$:\n$$A(\\theta) = 1 + \\frac{(2\\gamma - \\gamma^{2})(\\sigma_{u}^{2} - \\sigma_{s}^{2})\\sin^{2}\\theta}{(1-\\gamma)^{2}\\sigma_{u}^{2} + \\sigma_{s}^{2} + \\gamma^{2}r}$$\nThis is the final closed-form analytic expression for the error amplification factor.", "answer": "$$\n\\boxed{1 + \\frac{(2\\gamma - \\gamma^{2})(\\sigma_{u}^{2} - \\sigma_{s}^{2})\\sin^{2}\\theta}{(1-\\gamma)^{2}\\sigma_{u}^{2} + \\sigma_{s}^{2} + \\gamma^{2}r}}\n$$", "id": "3374551"}]}