{"hands_on_practices": [{"introduction": "To truly master the information filter, it's essential to understand its origins in Bayesian inference. This first exercise takes you back to first principles, guiding you to derive the core update equations for the information matrix and vector directly from the Gaussian likelihood and prior. By working through both a sequential update and a batch update for the same set of observations, you will concretely verify the equivalence of these two approaches, a fundamental property of Bayesian estimation.", "problem": "Consider a linear Gaussian inverse problem in information form for a state vector $x \\in \\mathbb{R}^{2}$ with prior density proportional to $\\exp\\!\\big(-\\tfrac{1}{2}\\,x^{\\top} Y x + y^{\\top} x\\big)$, where the prior information matrix and prior information vector are\n$$\nY \\,=\\, \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}, \\qquad y \\,=\\, \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}.\n$$\nYou receive two independent scalar observations of the form $z_{i} = h_{i}^{\\top} x + v_{i}$ with $v_{i} \\sim \\mathcal{N}(0, r_{i})$, with the following data:\n$$\nh_{1} \\,=\\, \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad r_{1} \\,=\\, \\tfrac{1}{2}, \\quad z_{1} \\,=\\, \\tfrac{3}{2}; \n\\qquad\nh_{2} \\,=\\, \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}, \\quad r_{2} \\,=\\, 2, \\quad z_{2} \\,=\\, -\\tfrac{1}{2}.\n$$\nStarting from Bayes’ rule and the Gaussian likelihood, and using only fundamental algebraic manipulations (e.g., expansion and completion of squares) as your base, do the following:\n\n1. Derive the information-form posterior update for a single scalar observation $z = h^{\\top} x + v$ with $v \\sim \\mathcal{N}(0, r)$, expressed as an updated information matrix $Y^{+}$ and updated information vector $y^{+}$ in terms of $Y$, $y$, $h$, $r$, and $z$.\n\n2. Apply your scalar update sequentially for the two observations given above to obtain $Y_{\\mathrm{seq}}$ and $y_{\\mathrm{seq}}$.\n\n3. Now form the multiple-observation model $z = H x + v$ with\n$$\nH \\,=\\, \\begin{pmatrix} h_{1}^{\\top} \\\\ h_{2}^{\\top} \\end{pmatrix} \\,=\\, \\begin{pmatrix} 1 & 1 \\\\ 2 & -1 \\end{pmatrix}, \n\\qquad \nR \\,=\\, \\operatorname{diag}(r_{1}, r_{2}) \\,=\\, \\begin{pmatrix} \\tfrac{1}{2} & 0 \\\\ 0 & 2 \\end{pmatrix}, \n\\qquad \nz \\,=\\, \\begin{pmatrix} z_{1} \\\\ z_{2} \\end{pmatrix} \\,=\\, \\begin{pmatrix} \\tfrac{3}{2} \\\\ -\\tfrac{1}{2} \\end{pmatrix}.\n$$\nDerive, again from the Gaussian likelihood and completion of squares, the batch information-form posterior update to obtain $Y_{\\mathrm{bat}}$ and $y_{\\mathrm{bat}}$.\n\nFinally, define the scalar\n$$\nS \\,=\\, \\big\\|\\, Y_{\\mathrm{seq}} - Y_{\\mathrm{bat}} \\,\\big\\|_{F}^{2} \\;+\\; \\big\\|\\, y_{\\mathrm{seq}} - y_{\\mathrm{bat}} \\,\\big\\|_{2}^{2},\n$$\nwhere $\\|\\cdot\\|_{F}$ is the Frobenius norm and $\\|\\cdot\\|_{2}$ is the Euclidean norm. Compute $S$ exactly. Provide your final result as a single real number. No rounding is required and no units are needed.", "solution": "The problem requires the derivation and application of information-form updates for a linear Gaussian system, both sequentially and in batch form, followed by a comparison of the results. The state vector is $x \\in \\mathbb{R}^{2}$.\n\nThe analysis begins with Bayes' rule in logarithmic form, which states that the log-posterior is proportional to the sum of the log-likelihood and the log-prior, up to an additive constant:\n$$\n\\ln p(x|\\text{data}) = \\ln p(\\text{data}|x) + \\ln p(x) + C\n$$\nThe prior probability density $p(x)$ is given in information form, proportional to $\\exp(-\\frac{1}{2}x^{\\top} Y x + y^{\\top} x)$. The argument of the exponential, which we will denote as $J_{\\text{prior}}(x)$, is:\n$$\nJ_{\\text{prior}}(x) = -\\frac{1}{2}x^{\\top} Y x + y^{\\top} x\n$$\n\n1.  Derivation of the scalar update rule.\n\nFor a single scalar observation $z = h^{\\top} x + v$ where the noise $v$ is distributed as $\\mathcal{N}(0, r)$, the likelihood function $p(z|x)$ is a Gaussian:\n$$\np(z|x) = \\frac{1}{\\sqrt{2\\pi r}} \\exp\\left(-\\frac{1}{2r}(z - h^{\\top} x)^2\\right)\n$$\nThe log-likelihood, ignoring constants independent of $x$, is proportional to the argument of the exponential, which we call $J_{\\text{like}}(x)$:\n$$\nJ_{\\text{like}}(x) = -\\frac{1}{2r}(z - h^{\\top} x)^2\n$$\nExpanding this quadratic form:\n$$\nJ_{\\text{like}}(x) = -\\frac{1}{2r}(z^2 - 2z h^{\\top} x + (h^{\\top} x)^2) = -\\frac{1}{2r}(z^2 - 2z x^{\\top} h + x^{\\top} h h^{\\top} x)\n$$\nDropping the term $-\\frac{z^2}{2r}$ which is independent of $x$, we have:\n$$\nJ_{\\text{like}}(x) = \\frac{z}{r} x^{\\top} h - \\frac{1}{2r} x^{\\top} h h^{\\top} x = -\\frac{1}{2} x^{\\top} \\left( \\frac{1}{r} h h^{\\top} \\right) x + \\left( \\frac{z}{r} h \\right)^{\\top} x\n$$\nThe posterior log-density argument $J_{\\text{post}}(x)$ is the sum of the prior and likelihood arguments:\n$$\nJ_{\\text{post}}(x) = J_{\\text{prior}}(x) + J_{\\text{like}}(x) = \\left(-\\frac{1}{2}x^{\\top} Y x + y^{\\top} x\\right) + \\left(-\\frac{1}{2} x^{\\top} \\left(\\frac{1}{r} h h^{\\top}\\right) x + \\left(\\frac{z}{r} h\\right)^{\\top} x\\right)\n$$\nGrouping the quadratic and linear terms in $x$:\n$$\nJ_{\\text{post}}(x) = -\\frac{1}{2} x^{\\top} \\left(Y + \\frac{1}{r} h h^{\\top}\\right) x + \\left(y + \\frac{z}{r} h\\right)^{\\top} x\n$$\nBy comparing this to the general information form $-\\frac{1}{2}x^{\\top} Y^{+} x + (y^{+})^{\\top} x$, we identify the updated information matrix $Y^{+}$ and information vector $y^{+}$:\n$$\nY^{+} = Y + \\frac{1}{r} h h^{\\top}\n$$\n$$\ny^{+} = y + \\frac{z}{r} h\n$$\n\n2.  Sequential application of the scalar update.\n\nWe start with the prior information $Y = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}$ and $y = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$.\n\nFirst observation update: $h_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, r_{1} = \\frac{1}{2}, z_{1} = \\frac{3}{2}$.\nThe information contribution from this observation is $\\frac{1}{r_1}h_{1}h_{1}^{\\top}$ and $\\frac{z_1}{r_1}h_1$.\n$$\n\\frac{1}{r_1}h_{1}h_{1}^{\\top} = \\frac{1}{1/2}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\begin{pmatrix} 1 & 1 \\end{pmatrix} = 2\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 2 \\\\ 2 & 2 \\end{pmatrix}\n$$\n$$\n\\frac{z_1}{r_1}h_1 = \\frac{3/2}{1/2}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 3\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}\n$$\nThe intermediate posterior information matrix $Y_{1}$ and vector $y_{1}$ are:\n$$\nY_{1} = Y + \\frac{1}{r_1}h_{1}h_{1}^{\\top} = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix} + \\begin{pmatrix} 2 & 2 \\\\ 2 & 2 \\end{pmatrix} = \\begin{pmatrix} 5 & 3 \\\\ 3 & 4 \\end{pmatrix}\n$$\n$$\ny_{1} = y + \\frac{z_1}{r_1}h_1 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} + \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}\n$$\nSecond observation update: $h_{2} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}, r_{2} = 2, z_{2} = -\\frac{1}{2}$.\nThis update is applied to the intermediate posterior $(Y_1, y_1)$.\n$$\n\\frac{1}{r_2}h_{2}h_{2}^{\\top} = \\frac{1}{2}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\\begin{pmatrix} 2 & -1 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} 4 & -2 \\\\ -2 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & -1 \\\\ -1 & \\frac{1}{2} \\end{pmatrix}\n$$\n$$\n\\frac{z_2}{r_2}h_2 = \\frac{-1/2}{2}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = -\\frac{1}{4}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}\n$$\nThe final sequential posterior information matrix $Y_{\\mathrm{seq}}$ and vector $y_{\\mathrm{seq}}$ are:\n$$\nY_{\\mathrm{seq}} = Y_{1} + \\frac{1}{r_2}h_{2}h_{2}^{\\top} = \\begin{pmatrix} 5 & 3 \\\\ 3 & 4 \\end{pmatrix} + \\begin{pmatrix} 2 & -1 \\\\ -1 & \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 7 & 2 \\\\ 2 & \\frac{9}{2} \\end{pmatrix}\n$$\n$$\ny_{\\mathrm{seq}} = y_{1} + \\frac{z_2}{r_2}h_2 = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ \\frac{5}{4} \\end{pmatrix}\n$$\n\n3.  Derivation and application of the batch update.\n\nFor the multiple-observation model $z = Hx + v$ with $v \\sim \\mathcal{N}(0, R)$, the log-likelihood argument, ignoring constants, is:\n$$\nJ_{\\text{like}}(x) = -\\frac{1}{2}(z - Hx)^{\\top} R^{-1} (z - Hx)\n$$\nExpanding this expression:\n$$\nJ_{\\text{like}}(x) = -\\frac{1}{2}(z^{\\top}R^{-1}z - z^{\\top}R^{-1}Hx - x^{\\top}H^{\\top}R^{-1}z + x^{\\top}H^{\\top}R^{-1}Hx)\n$$\nDropping the term $-\\frac{1}{2}z^{\\top}R^{-1}z$ and combining the two linear terms (which are scalars and transposes of each other):\n$$\nJ_{\\text{like}}(x) = -\\frac{1}{2}x^{\\top}(H^{\\top}R^{-1}H)x + (H^{\\top}R^{-1}z)^{\\top}x\n$$\nThe batch posterior exponent is $J_{\\text{post}}(x) = J_{\\text{prior}}(x) + J_{\\text{like}}(x)$:\n$$\nJ_{\\text{post}}(x) = \\left(-\\frac{1}{2}x^{\\top} Y x + y^{\\top} x\\right) + \\left(-\\frac{1}{2}x^{\\top}(H^{\\top}R^{-1}H)x + (H^{\\top}R^{-1}z)^{\\top}x\\right)\n$$\n$$\nJ_{\\text{post}}(x) = -\\frac{1}{2}x^{\\top}(Y + H^{\\top}R^{-1}H)x + (y + H^{\\top}R^{-1}z)^{\\top}x\n$$\nThis gives the batch update rules:\n$$\nY_{\\mathrm{bat}} = Y + H^{\\top}R^{-1}H\n$$\n$$\ny_{\\mathrm{bat}} = y + H^{\\top}R^{-1}z\n$$\nNow we apply this using the given batch data: $H = \\begin{pmatrix} 1 & 1 \\\\ 2 & -1 \\end{pmatrix}$, $R = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & 2 \\end{pmatrix}$, $z = \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix}$.\nThe inverse of the covariance matrix is $R^{-1} = \\begin{pmatrix} 2 & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix}$.\nFirst, we compute the information contribution matrix $H^{\\top}R^{-1}H$:\n$$\nH^{\\top}R^{-1}H = \\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 2 & -1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 2 & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 2 & -1 \\end{pmatrix} = \\begin{pmatrix} 4 & 1 \\\\ 1 & \\frac{5}{2} \\end{pmatrix}\n$$\nThen, we compute the information contribution vector $H^{\\top}R^{-1}z$:\n$$\nH^{\\top}R^{-1}z = \\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 3-\\frac{1}{2} \\\\ 3+\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{2} \\\\ \\frac{13}{4} \\end{pmatrix}\n$$\nNow, we find the batch posterior information matrix $Y_{\\mathrm{bat}}$ and vector $y_{\\mathrm{bat}}$:\n$$\nY_{\\mathrm{bat}} = Y + H^{\\top}R^{-1}H = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix} + \\begin{pmatrix} 4 & 1 \\\\ 1 & \\frac{5}{2} \\end{pmatrix} = \\begin{pmatrix} 7 & 2 \\\\ 2 & \\frac{9}{2} \\end{pmatrix}\n$$\n$$\ny_{\\mathrm{bat}} = y + H^{\\top}R^{-1}z = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} + \\begin{pmatrix} \\frac{5}{2} \\\\ \\frac{13}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{2+5}{2} \\\\ \\frac{-8+13}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ \\frac{5}{4} \\end{pmatrix}\n$$\n\nFinally, we compute the scalar $S$.\nBy comparing the results from the sequential and batch updates, we find that they are identical:\n$$\nY_{\\mathrm{seq}} = \\begin{pmatrix} 7 & 2 \\\\ 2 & \\frac{9}{2} \\end{pmatrix} = Y_{\\mathrm{bat}}\n$$\n$$\ny_{\\mathrm{seq}} = \\begin{pmatrix} \\frac{7}{2} \\\\ \\frac{5}{4} \\end{pmatrix} = y_{\\mathrm{bat}}\n$$\nTherefore, the differences are the zero matrix and zero vector:\n$$\nY_{\\mathrm{seq}} - Y_{\\mathrm{bat}} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\n$$\ny_{\\mathrm{seq}} - y_{\\mathrm{bat}} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe scalar $S$ is defined as $S = \\big\\|\\, Y_{\\mathrm{seq}} - Y_{\\mathrm{bat}} \\,\\big\\|_{F}^{2} \\;+\\; \\big\\|\\, y_{\\mathrm{seq}} - y_{\\mathrm{bat}} \\,\\big\\|_{2}^{2}$.\nThe Frobenius norm squared of the zero matrix is:\n$$\n\\big\\|\\, Y_{\\mathrm{seq}} - Y_{\\mathrm{bat}} \\,\\big\\|_{F}^{2} = 0^2 + 0^2 + 0^2 + 0^2 = 0\n$$\nThe Euclidean norm squared of the zero vector is:\n$$\n\\big\\|\\, y_{\\mathrm{seq}} - y_{\\mathrm{bat}} \\,\\big\\|_{2}^{2} = 0^2 + 0^2 = 0\n$$\nThus, the value of $S$ is:\n$$\nS = 0 + 0 = 0\n$$\nThis result demonstrates the fundamental property that for linear Gaussian systems, sequential Bayesian updates are equivalent to a single batch update incorporating all data simultaneously.", "answer": "$$\\boxed{0}$$", "id": "3390747"}, {"introduction": "A key advantage of the information filter becomes apparent when dealing with large-scale systems with inherent structure, such as time-series or spatial models. This practice problem explores a one-dimensional random-walk model, a classic example of a system with local dependencies. Your task is to construct the prior information matrix, which reflects the Markov structure of the model, and then see how local observations preserve the matrix's sparse, banded form, showcasing a major computational benefit of the information-based approach.", "problem": "Consider a one-dimensional chain of $n=5$ latent states $x_1, x_2, x_3, x_4, x_5$. Assume a random-walk Gaussian prior defined by the transition model $x_{i+1} \\mid x_i \\sim \\mathcal{N}(x_i, q)$ for $i=1,2,3,4$ with process variance $q>0$, and an anchor prior $x_1 \\sim \\mathcal{N}(0, \\sigma_0^2)$ with variance $\\sigma_0^2>0$. This prior induces a joint Gaussian distribution over $\\mathbf{x} = (x_1, x_2, x_3, x_4, x_5)^{\\top}$ whose precision (information) matrix is tridiagonal.\n\nYou collect three local observations with a banded linear observation operator and independent Gaussian noise: $y_1 = x_1 + v_1$, $y_3 = x_3 + v_3$, and $y_{45} = x_4 - x_5 + v_{45}$, where $v_1 \\sim \\mathcal{N}(0, r_1)$, $v_3 \\sim \\mathcal{N}(0, r_3)$, and $v_{45} \\sim \\mathcal{N}(0, r_{45})$, with $r_1>0$, $r_3>0$, and $r_{45}>0$. The observation operator $H$ and noise covariance $R$ are thus\n$$\nH \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & -1\n\\end{pmatrix},\n\\quad\nR \\;=\\; \\operatorname{diag}(r_1, r_3, r_{45}).\n$$\n\nStarting from first principles of Gaussian models and the definition of the precision (information) matrix as the quadratic form in the negative log-density, derive the prior precision matrix for the random-walk prior on $\\mathbf{x}$ and then derive the posterior precision matrix after assimilating the three observations. Express your final result as a single explicit analytic expression for the posterior precision matrix in terms of $q$, $\\sigma_0^2$, $r_1$, $r_3$, and $r_{45}$.", "solution": "The posterior probability density function $p(\\mathbf{x}|\\mathbf{y})$ is, by Bayes' theorem, proportional to the product of the likelihood $p(\\mathbf{y}|\\mathbf{x})$ and the prior $p(\\mathbf{x})$. For joint Gaussian distributions, this relationship is most conveniently expressed in terms of their negative logarithms, which are quadratic forms. The posterior precision (or information) matrix, denoted $\\Lambda_{\\text{post}}$, is the sum of the prior precision matrix, $\\Lambda_{\\text{prior}}$, and the precision matrix associated with the likelihood, $\\Lambda_{\\text{likelihood}}$.\n$$\n\\Lambda_{\\text{post}} = \\Lambda_{\\text{prior}} + \\Lambda_{\\text{likelihood}}\n$$\nWe derive each term separately.\n\nFirst, we derive the prior precision matrix $\\Lambda_{\\text{prior}}$. The prior distribution over the state vector $\\mathbf{x} = (x_1, x_2, x_3, x_4, x_5)^{\\top}$ is given by the chain rule:\n$$\np(\\mathbf{x}) = p(x_1) \\prod_{i=1}^{4} p(x_{i+1}|x_i)\n$$\nThe problem specifies the distributions: $x_1 \\sim \\mathcal{N}(0, \\sigma_0^2)$ and $x_{i+1}|x_i \\sim \\mathcal{N}(x_i, q)$. The negative log-prior, up to an additive constant, is:\n$$\n-\\ln p(\\mathbf{x}) = -\\ln p(x_1) - \\sum_{i=1}^{4} \\ln p(x_{i+1}|x_i) = \\frac{1}{2\\sigma_0^2}x_1^2 + \\sum_{i=1}^{4}\\frac{1}{2q}(x_{i+1}-x_i)^2 + \\text{const.}\n$$\nThis expression is a quadratic form in $\\mathbf{x}$, which can be written as $\\frac{1}{2}\\mathbf{x}^{\\top}\\Lambda_{\\text{prior}}\\mathbf{x}$. By expanding the sum and collecting terms, we can identify the elements of the symmetric matrix $\\Lambda_{\\text{prior}}$.\n$$\n\\frac{1}{2}\\mathbf{x}^{\\top}\\Lambda_{\\text{prior}}\\mathbf{x} = \\frac{1}{2\\sigma_0^2}x_1^2 + \\frac{1}{2q}\\left( (x_2-x_1)^2 + (x_3-x_2)^2 + (x_4-x_3)^2 + (x_5-x_4)^2 \\right)\n$$\nExpanding the squares:\n$$\n= \\frac{1}{2\\sigma_0^2}x_1^2 + \\frac{1}{2q}\\left( (x_1^2 - 2x_1x_2 + x_2^2) + (x_2^2 - 2x_2x_3 + x_3^2) + (x_3^2 - 2x_3x_4 + x_4^2) + (x_4^2 - 2x_4x_5 + x_5^2) \\right)\n$$\nCollecting coefficients for each quadratic term $\\frac{1}{2}x_i x_j$:\n\\begin{itemize}\n    \\item $x_1^2$: $(\\frac{1}{\\sigma_0^2} + \\frac{1}{q})$\n    \\item $x_2^2$: $(\\frac{1}{q} + \\frac{1}{q}) = \\frac{2}{q}$\n    \\item $x_3^2$: $(\\frac{1}{q} + \\frac{1}{q}) = \\frac{2}{q}$\n    \\item $x_4^2$: $(\\frac{1}{q} + \\frac{1}{q}) = \\frac{2}{q}$\n    \\item $x_5^2$: $\\frac{1}{q}$\n    \\item $x_1x_2$: $-\\frac{2}{q}$\n    \\item $x_2x_3$: $-\\frac{2}{q}$\n    \\item $x_3x_4$: $-\\frac{2}{q}$\n    \\item $x_4x_5$: $-\\frac{2}{q}$\n\\end{itemize}\nSince the total quadratic form is $\\frac{1}{2}\\sum_{i,j}(\\Lambda_{\\text{prior}})_{ij}x_i x_j$, the diagonal elements $(\\Lambda_{\\text{prior}})_{ii}$ are the coefficients of $x_i^2$, and the off-diagonal elements $(\\Lambda_{\\text{prior}})_{ij}$ are the coefficients of $x_i x_j$ for $i \\neq j$. This yields the tridiagonal prior precision matrix:\n$$\n\\Lambda_{\\text{prior}} =\n\\begin{pmatrix}\n\\frac{1}{\\sigma_0^2} + \\frac{1}{q} & -\\frac{1}{q} & 0 & 0 & 0 \\\\\n-\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} & 0 & 0 \\\\\n0 & -\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} & 0 \\\\\n0 & 0 & -\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} \\\\\n0 & 0 & 0 & -\\frac{1}{q} & \\frac{1}{q}\n\\end{pmatrix}\n$$\n\nNext, we derive the likelihood precision matrix $\\Lambda_{\\text{likelihood}}$. The observation model is $\\mathbf{y} = H\\mathbf{x} + \\mathbf{v}$, where $\\mathbf{v} \\sim \\mathcal{N}(\\mathbf{0}, R)$. The likelihood function is $p(\\mathbf{y}|\\mathbf{x}) \\sim \\mathcal{N}(H\\mathbf{x}, R)$. The negative log-likelihood, up to a constant, defines the information contributed by the observations:\n$$\n-\\ln p(\\mathbf{y}|\\mathbf{x}) = \\frac{1}{2}(\\mathbf{y} - H\\mathbf{x})^{\\top}R^{-1}(\\mathbf{y} - H\\mathbf{x}) + \\text{const.}\n$$\nThe quadratic part in $\\mathbf{x}$ is $\\frac{1}{2}\\mathbf{x}^{\\top}H^{\\top}R^{-1}H\\mathbf{x}$. Therefore, the likelihood precision matrix is given by:\n$$\n\\Lambda_{\\text{likelihood}} = H^{\\top}R^{-1}H\n$$\nWe are given $H$ and $R$. The inverse of the diagonal noise covariance matrix $R$ is:\n$$\nR^{-1} = \\operatorname{diag}(r_1, r_3, r_{45})^{-1} = \\operatorname{diag}\\left(\\frac{1}{r_1}, \\frac{1}{r_3}, \\frac{1}{r_{45}}\\right)\n$$\nThe transpose of the observation operator $H$ is:\n$$\nH^{\\top} =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & -1\n\\end{pmatrix}\n$$\nNow we compute the product $H^{\\top}R^{-1}H$:\n$$\nH^{\\top}R^{-1}H =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{1}{r_1} & 0 & 0 \\\\\n0 & \\frac{1}{r_3} & 0 \\\\\n0 & 0 & \\frac{1}{r_{45}}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & -1\n\\end{pmatrix}\n$$\n$$\n=\n\\begin{pmatrix}\n\\frac{1}{r_1} & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & \\frac{1}{r_3} & 0 \\\\\n0 & 0 & \\frac{1}{r_{45}} \\\\\n0 & 0 & -\\frac{1}{r_{45}}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & -1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{1}{r_1} & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\frac{1}{r_3} & 0 & 0 \\\\\n0 & 0 & 0 & \\frac{1}{r_{45}} & -\\frac{1}{r_{45}} \\\\\n0 & 0 & 0 & -\\frac{1}{r_{45}} & \\frac{1}{r_{45}}\n\\end{pmatrix}\n$$\n\nFinally, the posterior precision matrix $\\Lambda_{\\text{post}}$ is the sum of $\\Lambda_{\\text{prior}}$ and $\\Lambda_{\\text{likelihood}}$:\n$$\n\\Lambda_{\\text{post}} =\n\\begin{pmatrix}\n\\frac{1}{\\sigma_0^2} + \\frac{1}{q} & -\\frac{1}{q} & 0 & 0 & 0 \\\\\n-\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} & 0 & 0 \\\\\n0 & -\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} & 0 \\\\\n0 & 0 & -\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} \\\\\n0 & 0 & 0 & -\\frac{1}{q} & \\frac{1}{q}\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n\\frac{1}{r_1} & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\frac{1}{r_3} & 0 & 0 \\\\\n0 & 0 & 0 & \\frac{1}{r_{45}} & -\\frac{1}{r_{45}} \\\\\n0 & 0 & 0 & -\\frac{1}{r_{45}} & \\frac{1}{r_{45}}\n\\end{pmatrix}\n$$\n$$\n\\Lambda_{\\text{post}} =\n\\begin{pmatrix}\n\\frac{1}{\\sigma_0^2} + \\frac{1}{q} + \\frac{1}{r_1} & -\\frac{1}{q} & 0 & 0 & 0 \\\\\n-\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} & 0 & 0 \\\\\n0 & -\\frac{1}{q} & \\frac{2}{q} + \\frac{1}{r_3} & -\\frac{1}{q} & 0 \\\\\n0 & 0 & -\\frac{1}{q} & \\frac{2}{q} + \\frac{1}{r_{45}} & -\\frac{1}{q} - \\frac{1}{r_{45}} \\\\\n0 & 0 & 0 & -\\frac{1}{q} - \\frac{1}{r_{45}} & \\frac{1}{q} + \\frac{1}{r_{45}}\n\\end{pmatrix}\n$$\nThis is the final expression for the posterior precision matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{\\sigma_0^{2}} + \\frac{1}{q} + \\frac{1}{r_1} & -\\frac{1}{q} & 0 & 0 & 0 \\\\\n-\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} & 0 & 0 \\\\\n0 & -\\frac{1}{q} & \\frac{2}{q} + \\frac{1}{r_3} & -\\frac{1}{q} & 0 \\\\\n0 & 0 & -\\frac{1}{q} & \\frac{2}{q} + \\frac{1}{r_{45}} & -\\frac{1}{q} - \\frac{1}{r_{45}} \\\\\n0 & 0 & 0 & -\\frac{1}{q} - \\frac{1}{r_{45}} & \\frac{1}{q} + \\frac{1}{r_{45}}\n\\end{pmatrix}\n}\n$$", "id": "3390748"}, {"introduction": "The theoretical elegance of the information filter relies on the correctness of the underlying statistical model. This exercise serves as a critical case study on the dangers of model misspecification, specifically when correlations in observation errors are incorrectly ignored. By comparing the posterior uncertainty derived from the true error model versus a simplified diagonal approximation, you will quantify how such an oversight leads to overconfidence—a smaller-than-warranted posterior variance—a crucial lesson for any practitioner.", "problem": "Consider a one-dimensional linear-Gaussian data assimilation setting with a scalar state variable $x \\in \\mathbb{R}$. Let the prior (background) distribution be $x \\sim \\mathcal{N}(x_b, \\sigma_b^{2})$ with $\\sigma_b^{2} = 1$. Two sensors observe the same state $x$ through a linear observation operator $H = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, producing an observation vector $y \\in \\mathbb{R}^{2}$ according to $y = H x + \\varepsilon$, where the observation error $\\varepsilon$ is jointly Gaussian with zero mean.\n\nIn reality, the observation errors are correlated with covariance\n$$\nR_{\\text{true}} = \\sigma_o^{2} \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix},\n$$\nwhere $\\sigma_o^{2} = 1$ and the correlation coefficient is $\\rho = \\frac{1}{2}$. However, suppose an analyst incorrectly assumes uncorrelated errors and uses the diagonal approximation\n$$\nR_{\\text{diag}} = \\sigma_o^{2} I_{2} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}.\n$$\n\nStarting from Bayes’ rule for Gaussian variables and the definition of the posterior in the linear-Gaussian model, derive the posterior covariance in the correctly specified case (using $R_{\\text{true}}$) and in the misspecified case (using $R_{\\text{diag}}$). From these, obtain the corresponding posterior information matrices (the inverse posterior covariances).\n\nThen compute the following two quantities that together constitute a concrete counterexample:\n\n1. The ratio of the misspecified posterior variance to the correctly specified posterior variance, i.e., $\\frac{P_{\\text{diag}}}{P_{\\text{true}}}$.\n2. The difference of the misspecified posterior information and the correctly specified posterior information, i.e., $J_{\\text{diag}} - J_{\\text{true}}$.\n\nExpress your final answer as a single row matrix using exact values (no rounding). No units are required.", "solution": "In a linear-Gaussian model, the posterior probability density function for the state $x$ given an observation $y$, $p(x|y)$, is derived from Bayes' rule: $p(x|y) \\propto p(y|x)p(x)$. The prior $p(x)$ and the likelihood $p(y|x)$ are Gaussian:\n$$\np(x) \\propto \\exp\\left(-\\frac{1}{2}(x - x_b)^T P_b^{-1} (x - x_b)\\right)\n$$\n$$\np(y|x) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\\right)\n$$\nwhere $P_b$ is the prior (background) covariance and $R$ is the observation error covariance. Since our state $x$ is a scalar, $P_b = \\sigma_b^2$.\n\nThe posterior is also Gaussian, $p(x|y) \\propto \\exp(-\\frac{1}{2}(x - x_a)^T P_a^{-1} (x - x_a))$, where $P_a$ is the posterior (analysis) covariance. By combining the exponents, the inverse of the posterior covariance, known as the posterior information matrix $J_a$, is given by the sum of the prior information and the observation information:\n$$\nJ_a = P_a^{-1} = P_b^{-1} + H^T R^{-1} H\n$$\nThe problem asks for a comparison between the posterior statistics derived using the true covariance $R_{\\text{true}}$ and the misspecified diagonal covariance $R_{\\text{diag}}$.\n\nThe givens are:\n- Prior information: $P_b^{-1} = (\\sigma_b^2)^{-1} = 1^{-1} = 1$.\n- Observation operator: $H = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, so its transpose is $H^T = \\begin{pmatrix} 1 & 1 \\end{pmatrix}$.\n\n**1. Correctly Specified Case (using $R_{\\text{true}}$)**\n\nFirst, we compute the true observation error covariance $R_{\\text{true}}$ and its inverse.\nWith $\\sigma_o^2 = 1$ and $\\rho = \\frac{1}{2}$,\n$$\nR_{\\text{true}} = \\begin{pmatrix} 1 & \\frac{1}{2} \\\\ \\frac{1}{2} & 1 \\end{pmatrix}\n$$\nThe inverse of a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ is $\\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\nThe determinant of $R_{\\text{true}}$ is $\\det(R_{\\text{true}}) = (1)(1) - (\\frac{1}{2})(\\frac{1}{2}) = 1 - \\frac{1}{4} = \\frac{3}{4}$.\nThe inverse is:\n$$\nR_{\\text{true}}^{-1} = \\frac{1}{\\frac{3}{4}} \\begin{pmatrix} 1 & -\\frac{1}{2} \\\\ -\\frac{1}{2} & 1 \\end{pmatrix} = \\frac{4}{3} \\begin{pmatrix} 1 & -\\frac{1}{2} \\\\ -\\frac{1}{2} & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{3} & -\\frac{2}{3} \\\\ -\\frac{2}{3} & \\frac{4}{3} \\end{pmatrix}\n$$\nNext, we compute the observation information term $H^T R_{\\text{true}}^{-1} H$:\n$$\nH^T R_{\\text{true}}^{-1} H = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{4}{3} & -\\frac{2}{3} \\\\ -\\frac{2}{3} & \\frac{4}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n= \\begin{pmatrix} \\frac{4}{3}-\\frac{2}{3} & -\\frac{2}{3}+\\frac{4}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n= \\begin{pmatrix} \\frac{2}{3} & \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n= \\frac{2}{3} + \\frac{2}{3} = \\frac{4}{3}\n$$\nThe correctly specified posterior information, $J_{\\text{true}}$, is:\n$$\nJ_{\\text{true}} = P_{\\text{true}}^{-1} = P_b^{-1} + H^T R_{\\text{true}}^{-1} H = 1 + \\frac{4}{3} = \\frac{7}{3}\n$$\nThe correctly specified posterior variance, $P_{\\text{true}}$, is the inverse of the information:\n$$\nP_{\\text{true}} = J_{\\text{true}}^{-1} = \\left(\\frac{7}{3}\\right)^{-1} = \\frac{3}{7}\n$$\n\n**2. Misspecified Case (using $R_{\\text{diag}}$)**\n\nThe analyst incorrectly uses $R_{\\text{diag}}$.\nWith $\\sigma_o^2=1$:\n$$\nR_{\\text{diag}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2\n$$\nThe inverse is simply $R_{\\text{diag}}^{-1} = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nThe observation information term, as calculated by the analyst, is $H^T R_{\\text{diag}}^{-1} H$:\n$$\nH^T R_{\\text{diag}}^{-1} H = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n= \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n= 1 + 1 = 2\n$$\nThe misspecified posterior information, $J_{\\text{diag}}$, is:\n$$\nJ_{\\text{diag}} = P_{\\text{diag}}^{-1} = P_b^{-1} + H^T R_{\\text{diag}}^{-1} H = 1 + 2 = 3\n$$\nThe misspecified posterior variance, $P_{\\text{diag}}$, is the inverse of this information:\n$$\nP_{\\text{diag}} = J_{\\text{diag}}^{-1} = 3^{-1} = \\frac{1}{3}\n$$\n\n**3. Final Computations**\n\nNow we compute the two quantities requested by the problem.\n\n1.  The ratio of the misspecified posterior variance to the correctly specified posterior variance:\n    $$\n    \\frac{P_{\\text{diag}}}{P_{\\text{true}}} = \\frac{\\frac{1}{3}}{\\frac{3}{7}} = \\frac{1}{3} \\cdot \\frac{7}{3} = \\frac{7}{9}\n    $$\n2.  The difference of the misspecified posterior information and the correctly specified posterior information:\n    $$\n    J_{\\text{diag}} - J_{\\text{true}} = 3 - \\frac{7}{3} = \\frac{9}{3} - \\frac{7}{3} = \\frac{2}{3}\n    $$\nThe problem illustrates that incorrectly assuming uncorrelated errors (ignoring positive correlation) leads the analyst to be overconfident. The calculated variance $P_{\\text{diag}} = \\frac{1}{3} \\approx 0.333$ is smaller than the true variance $P_{\\text{true}} = \\frac{3}{7} \\approx 0.429$. Correspondingly, the analyst believes they have more information ($J_{\\text{diag}} = 3$) than is truly provided by the data ($J_{\\text{true}} = \\frac{7}{3} \\approx 2.333$).\n\nThe final answer is a row matrix containing these two results.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{7}{9} & \\frac{2}{3} \\end{pmatrix}}\n$$", "id": "3390743"}]}