{"hands_on_practices": [{"introduction": "To master an iterative method, we must first understand its behavior in the simplest possible setting. This first practice dissects the Ensemble Kalman Inversion (EKI) update for a linear inverse problem, revealing its fundamental connection to preconditioned gradient-based optimization [@problem_id:3379127]. By analyzing the algorithm's one-step dynamics, you will derive a strict stability condition on the step size, providing a crucial lesson in how the properties of the forward operator can dictate the convergence or divergence of the inversion.", "problem": "Consider a linear inverse problem with forward operator $A \\in \\mathbb{R}^{2 \\times 2}$, unknown parameter $u \\in \\mathbb{R}^{2}$, and observed data $y \\in \\mathbb{R}^{2}$ obeying $y = A u + \\eta$, where $\\eta$ is mean-zero Gaussian observational noise with covariance $\\Gamma \\in \\mathbb{R}^{2 \\times 2}$. The data misfit objective is the weighted least-squares functional\n$$\n\\Phi(u) = \\frac{1}{2} \\left\\| \\Gamma^{-1/2} (A u - y) \\right\\|_{2}^{2}.\n$$\nIn Ensemble Kalman Inversion (EKI), in the mean-field limit and for a single explicit Euler step of the preconditioned gradient flow associated with $\\Phi$, the ensemble mean update takes the form\n$$\nu_{1} = u_{0} - \\Delta t \\, C_{0}^{uu} A^{\\top} \\Gamma^{-1} \\big(A u_{0} - y\\big),\n$$\nwhere $C_{0}^{uu} \\in \\mathbb{R}^{2 \\times 2}$ is the empirical covariance of the initial ensemble and $\\Delta t > 0$ is the chosen step size. The preconditioning by $C_{0}^{uu}$ embodies the regularization in regularized iterative ensemble methods.\n\nConstruct a specific $2 \\times 2$ example with an ill-conditioned forward operator and identity noise covariance by taking\n$$\nA = \\begin{pmatrix} 100  0 \\\\ 0  0.1 \\end{pmatrix}, \\qquad \\Gamma = I_{2}, \\qquad C_{0}^{uu} = I_{2}.\n$$\nDefine the whitened misfit $z = \\Gamma^{-1/2} (A u - y) \\in \\mathbb{R}^{2}$ and let $S = \\Gamma^{-1/2} A C_{0}^{uu} A^{\\top} \\Gamma^{-1/2}$. Starting from first principles (the gradient of $\\Phi$ and the preconditioned gradient flow), derive the one-step misfit update and the condition on $\\Delta t$ that ensures a monotone decrease of the whitened misfit norm $\\|z\\|_{2}$. Compute the exact largest admissible step size $\\Delta t_{\\ast}$ for the constructed example such that, for any initial whitened misfit $z_{0} \\in \\mathbb{R}^{2}$, the one-step updated misfit $z_{1}$ satisfies $\\|z_{1}\\|_{2}  \\|z_{0}\\|_{2}$.\n\nYour final answer must be a single exact value for $\\Delta t_{\\ast}$, presented as a reduced fraction or an equivalent closed-form expression with no units. No rounding is required.", "solution": "The problem asks for the derivation of the one-step misfit update for an Ensemble Kalman Inversion (EKI) process, the condition on the step size $\\Delta t$ for a monotone decrease in the misfit norm, and the computation of the largest admissible step size $\\Delta t_{\\ast}$ for a specific example.\n\nFirst, we derive the update rule for the whitened misfit $z$. The data misfit objective function is given by\n$$\n\\Phi(u) = \\frac{1}{2} \\left\\| \\Gamma^{-1/2} (A u - y) \\right\\|_{2}^{2}.\n$$\nLet the whitened misfit be $z(u) = \\Gamma^{-1/2} (A u - y)$. Then the objective function is $\\Phi(u) = \\frac{1}{2} z(u)^{\\top} z(u)$.\nThe gradient of $\\Phi(u)$ with respect to $u$ is found using the chain rule. The Jacobian of $z(u)$ with respect to $u$ is $\\nabla_u z(u) = \\Gamma^{-1/2} A$.\nThus, the gradient of $\\Phi(u)$ is\n$$\n\\nabla_u \\Phi(u) = (\\nabla_u z(u))^{\\top} z(u) = (\\Gamma^{-1/2} A)^{\\top} \\Gamma^{-1/2} (A u - y).\n$$\nSince $\\Gamma$ is a covariance matrix, it is symmetric, so $\\Gamma^{-1/2}$ is also symmetric. Therefore, $(\\Gamma^{-1/2})^{\\top} = \\Gamma^{-1/2}$.\nThe gradient becomes\n$$\n\\nabla_u \\Phi(u) = A^{\\top} \\Gamma^{-1/2} \\Gamma^{-1/2} (A u - y) = A^{\\top} \\Gamma^{-1} (A u - y).\n$$\nThe single-step EKI update for the ensemble mean is given as\n$$\nu_{1} = u_{0} - \\Delta t \\, C_{0}^{uu} \\nabla_u \\Phi(u_0) = u_{0} - \\Delta t \\, C_{0}^{uu} A^{\\top} \\Gamma^{-1} (A u_{0} - y).\n$$\nThis confirms that the given update rule corresponds to a preconditioned gradient descent step on $\\Phi(u)$.\n\nNow, we derive the update for the whitened misfit, $z_{1} = z(u_1)$.\n$$\nz_{1} = \\Gamma^{-1/2} (A u_{1} - y).\n$$\nSubstituting the expression for $u_{1}$:\n$$\nz_{1} = \\Gamma^{-1/2} \\left( A \\left( u_{0} - \\Delta t \\, C_{0}^{uu} A^{\\top} \\Gamma^{-1} (A u_{0} - y) \\right) - y \\right).\n$$\nDistributing the terms:\n$$\nz_{1} = \\Gamma^{-1/2} (A u_{0} - y) - \\Delta t \\, \\Gamma^{-1/2} A C_{0}^{uu} A^{\\top} \\Gamma^{-1} (A u_{0} - y).\n$$\nWe recognize the initial misfit $z_{0} = \\Gamma^{-1/2} (A u_{0} - y)$. We can also write $\\Gamma^{-1} (A u_{0} - y) = \\Gamma^{-1/2} z_0$. Substituting these into the equation for $z_1$:\n$$\nz_{1} = z_{0} - \\Delta t \\, \\Gamma^{-1/2} A C_{0}^{uu} A^{\\top} \\Gamma^{-1/2} z_{0}.\n$$\nUsing the definition $S = \\Gamma^{-1/2} A C_{0}^{uu} A^{\\top} \\Gamma^{-1/2}$, the one-step misfit update is\n$$\nz_{1} = (I - \\Delta t \\, S) z_{0}.\n$$\n\nNext, we find the condition on $\\Delta t > 0$ that ensures a monotone decrease of the whitened misfit norm, i.e., $\\|z_{1}\\|_{2}  \\|z_{0}\\|_{2}$ for any non-zero initial misfit $z_{0} \\in \\mathbb{R}^{2}$. This is equivalent to $\\|z_{1}\\|_{2}^{2}  \\|z_{0}\\|_{2}^{2}$.\n$$\n\\|(I - \\Delta t \\, S) z_{0}\\|_{2}^{2}  \\|z_{0}\\|_{2}^{2}.\n$$\nExpanding the left side:\n$$\nz_{0}^{\\top} (I - \\Delta t \\, S)^{\\top} (I - \\Delta t \\, S) z_{0}  z_{0}^{\\top} z_{0}.\n$$\nThe matrix $S$ is symmetric, since $C_0^{uu}$ and $\\Gamma$ are symmetric:\n$$\nS^{\\top} = (\\Gamma^{-1/2} A C_{0}^{uu} A^{\\top} \\Gamma^{-1/2})^{\\top} = \\Gamma^{-1/2} A (C_{0}^{uu})^{\\top} A^{\\top} \\Gamma^{-1/2} = S.\n$$\nTherefore, the update operator $M = I - \\Delta t \\, S$ is also symmetric. The condition becomes\n$$\nz_{0}^{\\top} (I - \\Delta t \\, S)^{2} z_{0}  z_{0}^{\\top} z_{0}.\n$$\nThis inequality must hold for all $z_{0} \\neq 0$. This is equivalent to requiring that the operator norm of $M = I - \\Delta t \\, S$ is strictly less than $1$, i.e., $\\|M\\|_{2}  1$. For a symmetric matrix, the operator norm (or spectral norm) is the maximum absolute value of its eigenvalues. Let $\\lambda_i(S)$ be the eigenvalues of $S$. The eigenvalues of $M$ are $\\mu_i = 1 - \\Delta t \\, \\lambda_i(S)$.\nThus, we require $|1 - \\Delta t \\, \\lambda_i(S)|  1$ for all eigenvalues $\\lambda_i(S)$. This is equivalent to:\n$$\n-1  1 - \\Delta t \\, \\lambda_i(S)  1.\n$$\nThe matrix $S$ is symmetric positive semi-definite (SPSD), as it can be written as $S = K K^\\top$ with $K = \\Gamma^{-1/2} A (C_{0}^{uu})^{1/2}$. Hence, its eigenvalues are non-negative, $\\lambda_i(S) \\geq 0$.\nThe right-hand side of the inequality, $1 - \\Delta t \\, \\lambda_i(S)  1$, implies $-\\Delta t \\, \\lambda_i(S)  0$. Since $\\Delta t > 0$, this requires $\\lambda_i(S) > 0$. If an eigenvalue is zero, the norm of the corresponding eigenvector component of $z_0$ will not decrease. For a strict decrease for *any* non-zero $z_0$, we must have all $\\lambda_i(S) > 0$, meaning $S$ is positive definite.\nThe left-hand side of the inequality, $-1  1 - \\Delta t \\, \\lambda_i(S)$, implies $\\Delta t \\, \\lambda_i(S)  2$, or $\\Delta t  \\frac{2}{\\lambda_i(S)}$.\nThis condition must hold for all eigenvalues. To ensure this, $\\Delta t$ must be smaller than the minimum of these upper bounds:\n$$\n\\Delta t  \\min_i \\left( \\frac{2}{\\lambda_i(S)} \\right) = \\frac{2}{\\max_i(\\lambda_i(S))} = \\frac{2}{\\lambda_{\\max}(S)}.\n$$\nThe set of admissible step sizes is the interval $(0, \\frac{2}{\\lambda_{\\max}(S)})$. The largest admissible step size $\\Delta t_{\\ast}$ is the supremum of this set.\n$$\n\\Delta t_{\\ast} = \\frac{2}{\\lambda_{\\max}(S)}.\n$$\n\nNow we apply this to the specific example:\n$$\nA = \\begin{pmatrix} 100  0 \\\\ 0  0.1 \\end{pmatrix}, \\qquad \\Gamma = I_{2} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\qquad C_{0}^{uu} = I_{2} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}.\n$$\nWe compute the matrix $S$:\n$$\nS = \\Gamma^{-1/2} A C_{0}^{uu} A^{\\top} \\Gamma^{-1/2} = I_{2}^{-1/2} A I_{2} A^{\\top} I_{2}^{-1/2} = I_{2} A I_{2} A^{\\top} I_{2} = A A^{\\top}.\n$$\nSince $A$ is a diagonal matrix, it is symmetric ($A=A^{\\top}$), so $S=A^2$.\n$$\nS = \\begin{pmatrix} 100  0 \\\\ 0  0.1 \\end{pmatrix} \\begin{pmatrix} 100  0 \\\\ 0  0.1 \\end{pmatrix} = \\begin{pmatrix} 100^2  0 \\\\ 0  (0.1)^2 \\end{pmatrix} = \\begin{pmatrix} 10000  0 \\\\ 0  0.01 \\end{pmatrix}.\n$$\nThe matrix $S$ is diagonal, so its eigenvalues are its diagonal entries: $\\lambda_1 = 10000$ and $\\lambda_2 = 0.01$. Both are positive, so $S$ is positive definite.\nThe maximum eigenvalue of $S$ is $\\lambda_{\\max}(S) = 10000$.\nFinally, we compute the largest admissible step size $\\Delta t_{\\ast}$:\n$$\n\\Delta t_{\\ast} = \\frac{2}{\\lambda_{\\max}(S)} = \\frac{2}{10000} = \\frac{1}{5000}.\n$$", "answer": "$$\n\\boxed{\\frac{1}{5000}}\n$$", "id": "3379127"}, {"introduction": "Moving from the idealized linear world to the more realistic domain of nonlinear inverse problems introduces new challenges to algorithmic stability. This exercise demonstrates that a naive application of EKI can easily fail to reduce the data misfit and may even diverge when the forward map $G(u)$ is nonconvex [@problem_id:3379112]. You will implement a robust solution combining Levenberg-Marquardt style regularization with a backtracking line search, a critical technique for ensuring your inversion makes steady, monotonic progress toward a solution.", "problem": "Consider an inverse problem with unknown parameter vector $u \\in \\mathbb{R}^n$, a nonlinear forward map $G : \\mathbb{R}^n \\to \\mathbb{R}^m$, and observed data $y \\in \\mathbb{R}^m$ modeled by $y = G(u^\\star) + \\eta$, where $u^\\star$ is the unknown truth and $\\eta$ is observational noise. Assume $\\eta$ is Gaussian with mean zero and positive-definite covariance matrix $\\Gamma \\in \\mathbb{R}^{m \\times m}$. Define the data misfit function\n$$\n\\Phi(u) = \\frac{1}{2}\\left\\|\\Gamma^{-1/2}\\left(y - G(u)\\right)\\right\\|_2^2,\n$$\nand the residual $r(u) = y - G(u)$.\n\nAn Ensemble Kalman Inversion (EKI) method evolves an ensemble $\\{u_k^j\\}_{j=1}^J$ across iterations $k = 0,1,2,\\dots$, and defines an ensemble mean $\\bar u_k = \\frac{1}{J}\\sum_{j=1}^J u_k^j$. At iteration $k$, define ensemble outputs $G(u_k^j)$ and their mean $\\overline{G}_k = \\frac{1}{J}\\sum_{j=1}^J G(u_k^j)$. The sample cross-covariance $C_{uG}^{(k)} \\in \\mathbb{R}^{n \\times m}$ and output covariance $C_{GG}^{(k)} \\in \\mathbb{R}^{m \\times m}$ are\n$$\nC_{uG}^{(k)} = \\frac{1}{J-1}\\sum_{j=1}^J \\left(u_k^j - \\bar u_k\\right)\\left(G(u_k^j) - \\overline{G}_k\\right)^\\top, \\quad\nC_{GG}^{(k)} = \\frac{1}{J-1}\\sum_{j=1}^J \\left(G(u_k^j) - \\overline{G}_k\\right)\\left(G(u_k^j) - \\overline{G}_k\\right)^\\top.\n$$\nA regularized EKI update uses a Levenberg–Marquardt (LM) style regularization with parameter $\\lambda_k  0$ and a step scaling $\\alpha_k \\in (0,1]$:\n$$\nK_k = C_{uG}^{(k)}\\left(C_{GG}^{(k)} + \\lambda_k \\Gamma\\right)^{-1}, \\quad u_{k+1}^j = u_k^j + \\alpha_k K_k \\left(y - G(u_k^j)\\right), \\quad j=1,\\dots,J.\n$$\nThis couples the ensemble to data via a gain $K_k$ that depends on sample covariances and regularization by $\\lambda_k$. We seek to enforce monotonic decrease of the mean misfit, i.e.,\n$$\n\\left\\|y - G(\\bar u_{k+1})\\right\\|_2 \\le \\left\\|y - G(\\bar u_k)\\right\\|_2 \\quad \\text{for all iterations } k.\n$$\n\nStarting from the definition of $\\Phi(u)$, smoothness of $G$ (existence of the Jacobian $J_G(u)$), and the LM regularization principle, derive a sufficient monotonicity condition that relates $\\lambda_k$ and $\\alpha_k$ to ensure that the misfit decreases for the mean, and explain how to enforce it algorithmically through a backtracking strategy on $\\alpha_k$ and $\\lambda_k$. The derivation must be principle-based, beginning from the definition of $\\Phi(u)$ and properties of $G$, and should not assume convexity of $G$; the forward maps below are nonconvex.\n\nThen, implement a program that:\n- Constructs ensembles and applies the regularized EKI update with dynamic tuning of $\\lambda_k$ and step scaling $\\alpha_k$ at each iteration via backtracking, in order to strictly enforce\n$$\n\\left\\|y - G(\\bar u_{k+1})\\right\\|_2 \\le \\left\\|y - G(\\bar u_k)\\right\\|_2.\n$$\n- Uses the following test suite of four cases, each defining $(n, m)$, a nonconvex forward map $G$, the truth $u^\\star$, the noise covariance $\\Gamma$, the ensemble size $J$, the number of iterations $K$, and a Gaussian prior for the initial ensemble $\\{u_0^j\\}$ with mean $m_0$ and diagonal covariance $C_0$:\n    1. Case A (happy path): $n=2$, $m=2$, $G(u) = \\begin{bmatrix}\\sin(u_1) + 0.1 u_2^2 \\\\ u_1^3 - u_2 + 0.5\\cos(u_2)\\end{bmatrix}$, $u^\\star = \\begin{bmatrix}1.2 \\\\ -0.7\\end{bmatrix}$, $\\Gamma = \\mathrm{diag}(0.05, 0.05)$, $J=20$, $K=10$, prior mean $m_0 = \\begin{bmatrix}0.5 \\\\ -0.5\\end{bmatrix}$, prior covariance $C_0 = \\mathrm{diag}(0.5, 0.5)$.\n    2. Case B (boundary: minimal ensemble): $n=2$, $m=2$, $G(u) = \\begin{bmatrix}\\sin(u_1) + 0.1 u_2^2 \\\\ u_1^3 - u_2 + 0.5\\cos(u_2)\\end{bmatrix}$, $u^\\star = \\begin{bmatrix}1.2 \\\\ -0.7\\end{bmatrix}$, $\\Gamma = \\mathrm{diag}(0.05, 0.05)$, $J=3$, $K=8$, prior mean $m_0 = \\begin{bmatrix}1.5 \\\\ -1.5\\end{bmatrix}$, prior covariance $C_0 = \\mathrm{diag}(10^{-3}, 10^{-3})$.\n    3. Case C (nonconvex forward map with strong nonlinearity): $n=2$, $m=2$, $G(u) = \\begin{bmatrix}\\tanh(u_1) + \\sin(2u_1) + u_2^2 \\\\ u_1 u_2 + \\sin(u_2)\\end{bmatrix}$, $u^\\star = \\begin{bmatrix}-0.8 \\\\ 1.1\\end{bmatrix}$, $\\Gamma = \\mathrm{diag}(0.08, 0.04)$, $J=15$, $K=12$, prior mean $m_0 = \\begin{bmatrix}-1.0 \\\\ 1.0\\end{bmatrix}$, prior covariance $C_0 = \\mathrm{diag}(1.0, 0.3)$.\n    4. Case D (edge: nearly degenerate initial output covariance): $n=3$, $m=3$, $G(u) = \\begin{bmatrix}\\sin(u_1) + 0.3 u_2^2 - 0.2\\cos(u_3) \\\\ u_1^2 - u_2^3 + 0.1\\sin(u_1 u_3) \\\\ \\tanh(u_3) + u_1 u_2\\end{bmatrix}$, $u^\\star = \\begin{bmatrix}0.7 \\\\ -0.4 \\\\ 0.9\\end{bmatrix}$, $\\Gamma = \\mathrm{diag}(0.06, 0.06, 0.06)$, $J=10$, $K=6$, prior mean $m_0 = \\begin{bmatrix}0.2 \\\\ -0.2 \\\\ 0.3\\end{bmatrix}$, prior covariance $C_0 = \\mathrm{diag}(10^{-8}, 10^{-8}, 10^{-8})$.\n\nFor all cases, generate the observed data $y$ using $y = G(u^\\star) + \\eta$ with $\\eta \\sim \\mathcal{N}(0, \\Gamma)$ drawn once per case, using a deterministic pseudorandom generator with fixed seed for reproducibility.\n\nThe implementation requirements are:\n- At each iteration $k$, compute $C_{uG}^{(k)}$ and $C_{GG}^{(k)}$, propose an update with an initial $\\lambda_k$ and $\\alpha_k = 1$, and accept only if the mean misfit decreases. If the mean misfit does not decrease, increase $\\lambda_k$ and reduce $\\alpha_k$ via backtracking until the monotonicity condition is satisfied. If necessary, allow $\\alpha_k$ to shrink to zero, which yields no change and trivially enforces monotonicity.\n- After $K$ iterations for each case, report whether the entire sequence $\\left\\|y - G(\\bar u_k)\\right\\|_2$ for $k=0,1,\\dots,K$ is nonincreasing.\n\nYour program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets (e.g., \"[True,True,False,True]\"). Each entry must be a boolean indicating whether monotonic decrease was maintained over all iterations for the case.", "solution": "The objective is to derive a sufficient condition for ensuring monotonic decrease of the unweighted L2-norm misfit of the ensemble mean, $\\left\\|y - G(\\bar u_{k+1})\\right\\|_2 \\le \\left\\|y - G(\\bar u_k)\\right\\|_2$, within a regularized Ensemble Kalman Inversion (EKI) framework, and to implement an algorithm that enforces this condition.\n\n### Derivation of the Monotonicity Condition and Algorithmic Enforcement\n\nLet the unweighted squared misfit for the ensemble mean be denoted by $f(\\bar{u}) = \\left\\|y - G(\\bar{u})\\right\\|_2^2$. We want to ensure that for each iteration $k$, $f(\\bar{u}_{k+1}) \\le f(\\bar{u}_k)$.\n\nThe update rule for each ensemble member $j$ is given by\n$$u_{k+1}^j = u_k^j + \\alpha_k K_k \\left(y - G(u_k^j)\\right)$$\nwhere the Kalman gain $K_k$ is a function of the regularization parameter $\\lambda_k$:\n$$K_k(\\lambda_k) = C_{uG}^{(k)}\\left(C_{GG}^{(k)} + \\lambda_k \\Gamma\\right)^{-1}$$\nAveraging the update rule over the ensemble of size $J$ gives the update for the ensemble mean $\\bar{u}_k$:\n$$\n\\bar{u}_{k+1} = \\frac{1}{J}\\sum_{j=1}^J u_{k+1}^j = \\frac{1}{J}\\sum_{j=1}^J \\left(u_k^j + \\alpha_k K_k(\\lambda_k) (y - G(u_k^j))\\right) = \\bar{u}_k + \\alpha_k K_k(\\lambda_k) \\left(y - \\overline{G}_k\\right)\n$$\nwhere $\\overline{G}_k = \\frac{1}{J}\\sum_{j=1}^J G(u_k^j)$. Let the proposed step for the mean be $\\Delta \\bar{u}_k(\\lambda_k) = K_k(\\lambda_k) \\left(y - \\overline{G}_k\\right)$. Then the updated mean is $\\bar{u}_{k+1} = \\bar{u}_k + \\alpha_k \\Delta \\bar{u}_k(\\lambda_k)$.\n\nThe EKI update is related to the Gauss-Newton method for minimizing the weighted data misfit function $\\Phi(u) = \\frac{1}{2}\\left\\|\\Gamma^{-1/2}\\left(y - G(u)\\right)\\right\\|_2^2$. The gradient of this function is $\\nabla_u \\Phi(u) = -J_G(u)^\\top \\Gamma^{-1} (y - G(u))$, where $J_G(u)$ is the Jacobian of $G(u)$. The EKI step for the mean can be seen as an approximate preconditioned gradient descent step for $\\Phi(u)$. Specifically, in the limit of large $\\lambda_k$, the gain becomes $K_k(\\lambda_k) \\approx \\frac{1}{\\lambda_k} C_{uG}^{(k)} \\Gamma^{-1}$. The step direction $\\Delta \\bar{u}_k$ then approximates a descent direction for the *weighted* misfit $\\Phi(u)$ at $\\bar{u}_k$. This is the core of the Levenberg-Marquardt (LM) principle: for a sufficiently large $\\lambda_k$, the step becomes a small step along an approximate (preconditioned) negative gradient, which guarantees a local decrease in the objective function $\\Phi(u)$, provided a sufficiently small step size $\\alpha_k$ is used.\n\nHowever, the problem requires monotonicity for the *unweighted* misfit $f(u)$. A descent direction for $\\Phi(u)$ is not, in general, a descent direction for $f(u)$ unless the noise covariance $\\Gamma$ is a scalar multiple of the identity matrix. Thus, there is no simple analytical condition on $\\lambda_k$ and $\\alpha_k$ that *a priori* guarantees descent for $f(u)$ based on matrix properties alone.\n\nThe sufficient condition must therefore be enforced algorithmically. The key insight is that for any proposed update direction $\\Delta \\bar{u}_k(\\lambda_k)$, the updated mean is $\\bar{u}_{k+1}(\\alpha_k) = \\bar{u}_k + \\alpha_k \\Delta \\bar{u}_k(\\lambda_k)$. Since the forward map $G$ is assumed smooth, it is continuous. Consequently, the misfit function $f(\\bar{u})$ is also continuous. In the limit as the step scaling $\\alpha_k$ approaches zero, we have:\n$$ \\lim_{\\alpha_k \\to 0} \\bar{u}_{k+1}(\\alpha_k) = \\bar{u}_k $$\n$$ \\lim_{\\alpha_k \\to 0} f(\\bar{u}_{k+1}(\\alpha_k)) = f(\\lim_{\\alpha_k \\to 0} \\bar{u}_{k+1}(\\alpha_k)) = f(\\bar{u}_k) $$\nThis continuity implies that we can always find parameters $(\\alpha_k, \\lambda_k)$ that satisfy the monotonicity condition $f(\\bar{u}_{k+1}) \\le f(\\bar{u}_k)$. This can be achieved via a backtracking procedure.\nIf a proposed step $(\\alpha_k, \\lambda_k)$ fails (i.e., $f(\\bar{u}_{k+1}) > f(\\bar{u}_k)$), the algorithm can adjust these parameters.\n1.  **Reduce Step Scaling $\\alpha_k$**: The primary strategy is to reduce $\\alpha_k$ (e.g., by a factor of $0.5$). For a descent direction, a sufficiently small $\\alpha_k > 0$ will ensure misfit reduction. If the direction is not a descent direction, reducing $\\alpha_k$ will still make the step smaller, bringing $f(\\bar{u}_{k+1})$ closer to $f(\\bar{u}_k)$.\n2.  **Increase Regularization $\\lambda_k$**: If reducing $\\alpha_k$ alone is not effective (e.g., it becomes smaller than a prescribed tolerance, suggesting the search direction itself is poor), the regularization parameter $\\lambda_k$ can be increased. This modifies the search direction $\\Delta\\bar{u}_k$, typically making it more aligned with a gradient-based direction, which often improves the chance of finding a productive step. After increasing $\\lambda_k$, $\\alpha_k$ is reset to its initial value (e.g., $1$).\n3.  **Worst-Case Guarantee**: In the most extreme case where no combination of $\\lambda_k > 0$ and $\\alpha_k > 0$ yields a misfit decrease (which could happen if $\\bar{u}_k$ is at a local minimum or in a very challenging region of the parameter space), the algorithm can set $\\alpha_k=0$. This results in $\\bar{u}_{k+1} = \\bar{u}_k$. The monotonicity condition is then trivially satisfied as an equality, $f(\\bar{u}_k) \\le f(\\bar{u}_k)$.\n\nThis backtracking search for acceptable $(\\alpha_k, \\lambda_k)$ constitutes the algorithmic enforcement of the monotonicity condition. The existence of a valid (if unproductive) choice, $\\alpha_k=0$, guarantees that the condition can always be met.\n\n### Algorithmic Backtracking Strategy\n\nFor each iteration $k$:\n1.  Initialize trial parameters, e.g., $\\lambda_k \\leftarrow \\lambda_{init}$ and $\\alpha_k \\leftarrow 1$.\n2.  Compute the current misfit $f_{curr} = \\|y - G(\\bar u_k)\\|_2$.\n3.  Enter a loop to find acceptable parameters:\n    a. Compute the gain $K_k(\\lambda_k)$ and the proposed next mean state $\\bar{u}_{trial} = \\bar{u}_k + \\alpha_k K_k(\\lambda_k)(y-\\overline{G}_k)$.\n    b. Evaluate the trial misfit $f_{trial} = \\|y - G(\\bar{u}_{trial})\\|_2$.\n    c. If $f_{trial} \\le f_{curr}$, the step is accepted. The ensemble is updated using these parameters, and the loop terminates.\n    d. If $f_{trial} > f_{curr}$, the step is rejected. The parameters are adjusted. A common strategy is to first reduce $\\alpha_k$. If after several reductions of $\\alpha_k$ no progress is made, reset $\\alpha_k$ and increase $\\lambda_k$. Repeat from step 3a.\n4.  If the search loop exhausts a predefined budget of attempts, set $\\alpha_k=0$ to enforce monotonicity trivially for the current iteration.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import inv\n\ndef solve():\n    \"\"\"\n    Main solver function that runs the four test cases for regularized EKI\n    with backtracking to enforce monotonic misfit decrease.\n    \"\"\"\n    \n    # Set a fixed seed for reproducibility of random noise generation\n    np.random.seed(42)\n\n    # --- Define Forward Models G(u) for each case ---\n    def G_A(u): # n=2, m=2\n        u1, u2 = u\n        return np.array([np.sin(u1) + 0.1 * u2**2, u1**3 - u2 + 0.5 * np.cos(u2)])\n\n    def G_C(u): # n=2, m=2\n        u1, u2 = u\n        return np.array([np.tanh(u1) + np.sin(2 * u1) + u2**2, u1 * u2 + np.sin(u2)])\n\n    def G_D(u): # n=3, m=3\n        u1, u2, u3 = u\n        return np.array([\n            np.sin(u1) + 0.3 * u2**2 - 0.2 * np.cos(u3),\n            u1**2 - u2**3 + 0.1 * np.sin(u1 * u3),\n            np.tanh(u3) + u1 * u2\n        ])\n\n    test_cases = [\n        # Case A: Happy path\n        {\n            \"n\": 2, \"m\": 2, \"G_func\": G_A, \"u_star\": np.array([1.2, -0.7]),\n            \"Gamma\": np.diag([0.05, 0.05]), \"J\": 20, \"K\": 10,\n            \"m0\": np.array([0.5, -0.5]), \"C0\": np.diag([0.5, 0.5])\n        },\n        # Case B: Minimal ensemble size\n        {\n            \"n\": 2, \"m\": 2, \"G_func\": G_A, \"u_star\": np.array([1.2, -0.7]),\n            \"Gamma\": np.diag([0.05, 0.05]), \"J\": 3, \"K\": 8,\n            \"m0\": np.array([1.5, -1.5]), \"C0\": np.diag([1e-3, 1e-3])\n        },\n        # Case C: Strong nonlinearity\n        {\n            \"n\": 2, \"m\": 2, \"G_func\": G_C, \"u_star\": np.array([-0.8, 1.1]),\n            \"Gamma\": np.diag([0.08, 0.04]), \"J\": 15, \"K\": 12,\n            \"m0\": np.array([-1.0, 1.0]), \"C0\": np.diag([1.0, 0.3])\n        },\n        # Case D: Nearly degenerate initial output covariance\n        {\n            \"n\": 3, \"m\": 3, \"G_func\": G_D, \"u_star\": np.array([0.7, -0.4, 0.9]),\n            \"Gamma\": np.diag([0.06, 0.06, 0.06]), \"J\": 10, \"K\": 6,\n            \"m0\": np.array([0.2, -0.2, 0.3]), \"C0\": np.diag([1e-8, 1e-8, 1e-8])\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # --- Unpack parameters and generate data for the current case ---\n        n, m, G_func = case[\"n\"], case[\"m\"], case[\"G_func\"]\n        u_star, Gamma = case[\"u_star\"], case[\"Gamma\"]\n        J, K, m0, C0 = case[\"J\"], case[\"K\"], case[\"m0\"], case[\"C0\"]\n\n        # Generate observed data y = G(u*) + eta\n        eta = np.random.multivariate_normal(np.zeros(m), Gamma)\n        y = G_func(u_star) + eta\n\n        # --- Initialize ensemble and misfit tracking ---\n        ensemble_u = np.random.multivariate_normal(m0, C0, size=J)\n        misfit_history = []\n        \n        # Initial misfit at k=0\n        u_bar_0 = np.mean(ensemble_u, axis=0)\n        initial_misfit = np.linalg.norm(y - G_func(u_bar_0))\n        misfit_history.append(initial_misfit)\n\n        # --- Main EKI Iteration Loop ---\n        for k in range(K):\n            u_bar_k = np.mean(ensemble_u, axis=0)\n            \n            # Evaluate G for each ensemble member\n            ensemble_G = np.array([G_func(u_j) for u_j in ensemble_u])\n            G_bar_k = np.mean(ensemble_G, axis=0)\n\n            # Calculate sample covariances\n            dev_u = ensemble_u - u_bar_k      # Shape (J, n)\n            dev_G = ensemble_G - G_bar_k    # Shape (J, m)\n            \n            C_uG_k = (dev_u.T @ dev_G) / (J - 1) # Shape (n, m)\n            C_GG_k = (dev_G.T @ dev_G) / (J - 1) # Shape (m, m)\n            \n            # --- Backtracking Loop for lambda and alpha ---\n            current_misfit = misfit_history[-1]\n            lambda_k = 1.0\n            \n            # Budget for backtracking attempts\n            max_outer_loops = 6  # Tries with increasing lambda\n            max_inner_loops = 8  # Tries with decreasing alpha\n\n            accepted_step = False\n            final_alpha = 0.0 # Default to a null step\n            K_k = None\n\n            for i in range(max_outer_loops):\n                try:\n                    # Calculate gain matrix K_k\n                    inv_term = inv(C_GG_k + lambda_k * Gamma)\n                    K_k_trial = C_uG_k @ inv_term\n                except np.linalg.LinAlgError:\n                    # If matrix is singular, increase regularization\n                    lambda_k *= 10.0\n                    continue\n\n                alpha_k = 1.0 # Reset alpha for each new lambda\n                for j in range(max_inner_loops):\n                    # Propose the next ensemble\n                    update_term = (K_k_trial @ (y - ensemble_G).T).T\n                    u_next_ensemble = ensemble_u + alpha_k * update_term\n                    \n                    # Evaluate new mean misfit\n                    u_bar_next = np.mean(u_next_ensemble, axis=0)\n                    new_misfit = np.linalg.norm(y - G_func(u_bar_next))\n\n                    if new_misfit = current_misfit:\n                        final_alpha = alpha_k\n                        K_k = K_k_trial\n                        accepted_step = True\n                        break # Exit inner (alpha) loop\n                    else:\n                        alpha_k /= 2.0\n                \n                if accepted_step:\n                    break # Exit outer (lambda) loop\n                else:\n                    # Alpha reduction failed for this lambda, increase lambda\n                    lambda_k *= 2.0\n            \n            # Update ensemble based on accepted alpha (or alpha=0 if none found)\n            if final_alpha > 0 and K_k is not None:\n                update_term = (K_k @ (y - ensemble_G).T).T\n                ensemble_u = ensemble_u + final_alpha * update_term\n\n            # Record the resulting misfit for this iteration\n            final_u_bar = np.mean(ensemble_u, axis=0)\n            final_misfit = np.linalg.norm(y - G_func(final_u_bar))\n            misfit_history.append(final_misfit)\n\n        # --- Check if monotonicity was maintained throughout ---\n        is_monotonic = True\n        for i in range(1, len(misfit_history)):\n            # Use a small tolerance for floating point comparisons\n            if misfit_history[i] > misfit_history[i-1] + 1e-9:\n                is_monotonic = False\n                break\n        results.append(is_monotonic)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3379112"}, {"introduction": "This final practice tackles one of EKI's most significant inherent limitations: the tendency for the ensemble to lose diversity and \"collapse\" before adequately exploring the parameter space. You will design and test an advanced, adaptive inflation technique that counteracts this premature collapse by diagnosing the problem in real time [@problem_id:3379138]. By dynamically inflating the ensemble covariance based on the geometric alignment between the ensemble subspace and the misfit gradient, you will implement a state-of-the-art method for enhancing the power and reliability of ensemble-based inversion.", "problem": "Consider a deterministic inverse problem with a parameter vector $u \\in \\mathbb{R}^n$ and a forward map $G:\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$. Let the observed data be $y \\in \\mathbb{R}^m$ and assume additive Gaussian noise with covariance matrix $\\Gamma \\in \\mathbb{R}^{m \\times m}$. Define the least-squares data misfit functional\n$$\n\\Phi(u) = \\frac{1}{2}\\left\\| \\Gamma^{-1/2}\\left(G(u)-y\\right)\\right\\|_2^2.\n$$\nAssume $G$ is differentiable and denote its Jacobian at $u$ by $J(u) \\in \\mathbb{R}^{m \\times n}$. The Gauss–Newton method uses the gradient $\\nabla \\Phi(u) = J(u)^\\top \\Gamma^{-1}\\left(G(u)-y\\right)$ and a normal matrix approximation $J(u)^\\top \\Gamma^{-1} J(u)$. The Ensemble Kalman Inversion (EKI) approximates the Gauss–Newton step by replacing the Hessian-related term with an ensemble covariance $C^{uu}_k \\in \\mathbb{R}^{n \\times n}$ at iteration $k$, producing a gain\n$$\nK_k = C^{uu}_k\\, J(\\bar u_k)^\\top \\left(J(\\bar u_k)\\, C^{uu}_k\\, J(\\bar u_k)^\\top + \\alpha_k\\, \\Gamma\\right)^{-1},\n$$\nwhere $\\bar u_k$ is the ensemble mean and $\\alpha_k  0$ is a Tikhonov regularization parameter. The regularized iterative ensemble Kalman method updates ensemble members $\\{u_j^k\\}_{j=1}^E$ via\n$$\nu_j^{k+1} = u_j^k + K_k \\left(y - G(u_j^k)\\right), \\qquad j=1,\\dots,E,\n$$\nwith $E$ the ensemble size. Let the ensemble anomalies be $A_k = [u_1^k-\\bar u_k,\\dots,u_E^k-\\bar u_k] \\in \\mathbb{R}^{n \\times E}$ and the sample covariance be $C^{uu}_k = \\frac{1}{E-1} A_k A_k^\\top$. Premature collapse is the undesired rapid shrinkage of $C^{uu}_k$ that occurs before the discrepancy principle target on the misfit is achieved; here we declare a premature collapse event at iteration $k$ if\n$$\n\\frac{\\operatorname{tr}(C^{uu}_k)}{\\operatorname{tr}(C^{uu}_0)}  \\epsilon\n$$\noccurs before the mean misfit achieves the discrepancy target\n$$\n\\Phi(\\bar u_k) \\le \\tau \\cdot \\frac{m}{2},\n$$\nwhere $(m/2)$ is the expected value of $\\Phi$ under the noise model (since if $G(u)=y+$ noise with identity covariance, then $2\\Phi$ is approximately chi-square with $m$ degrees of freedom), and $\\tau \\ge 1$ is a tolerance factor.\n\nDesign and implement a principled covariance inflation rule that uses the angle between the ensemble subspace and the gradient $\\nabla \\Phi(\\bar u_k)$. Let $P_k$ be the orthogonal projector onto the column space of $A_k$, defined by\n$$\nP_k = A_k \\left(A_k^\\top A_k\\right)^\\dagger A_k^\\top,\n$$\nwhere $(\\cdot)^\\dagger$ denotes the Moore–Penrose pseudoinverse. Define the cosine of the angle by\n$$\n\\cos(\\theta_k) = \\frac{\\left\\|P_k \\nabla \\Phi(\\bar u_k)\\right\\|_2}{\\left\\|\\nabla \\Phi(\\bar u_k)\\right\\|_2},\n$$\nwith the convention that $\\cos(\\theta_k)=1$ if $\\nabla \\Phi(\\bar u_k)=0$. Use this to construct an inflation parameter\n$$\n\\delta_k = \\min\\left\\{\\rho \\left(1 - \\cos(\\theta_k)\\right), \\, \\delta_{\\max}\\right\\},\n$$\nwith $\\rho  0$ and $\\delta_{\\max}  0$ prescribed constants. Apply inflation by scaling the covariance in the gain computation:\n$$\nC^{uu}_k \\leftarrow (1+\\delta_k)\\, C^{uu}_k,\n$$\nonly within the gain $K_k$, leaving the ensemble anomalies as they are.\n\nYour task is to:\n- Implement the regularized iterative ensemble Kalman method with the inflation rule above.\n- Compare two configurations: baseline without inflation (set $\\delta_k \\equiv 0$) and angle-based inflation as defined.\n- Detect premature collapse according to the criterion given, and test whether angle-based inflation avoids premature collapse while achieving the discrepancy target.\n\nUse the following forward model and settings to ensure scientific realism. Let $G(u)$ be a mildly nonlinear mapping:\n$$\nG(u) = H u + b \\cdot \\sin(C u),\n$$\nwhere $H \\in \\mathbb{R}^{m \\times n}$ is a known matrix, $b  0$ is a scalar nonlinearity amplitude, $C \\in \\mathbb{R}^{m \\times n}$ is a known matrix, and the sine is applied component-wise to the vector $C u \\in \\mathbb{R}^m$. The Jacobian is\n$$\nJ(u) = H + b \\cdot \\operatorname{diag}\\left(\\cos(C u)\\right)\\, C.\n$$\nAssume $\\Gamma = \\sigma^2 I_m$ for a scalar $\\sigma  0$ and identity $I_m \\in \\mathbb{R}^{m \\times m}$. Generate synthetic data by drawing a ground truth parameter $u^\\star$ and setting $y = G(u^\\star) + \\eta$ with $\\eta \\sim \\mathcal{N}(0,\\Gamma)$. Initialize the ensemble by sampling from a Gaussian prior with mean $\\mu_0$ and covariance $\\beta^2 I_n$.\n\nFix the following algorithmic constants for all tests:\n- Regularization parameter $\\alpha_k \\equiv \\alpha$ with $\\alpha = 0.05$.\n- Discrepancy tolerance factor $\\tau = 1.5$.\n- Collapse threshold $\\epsilon = 0.05$.\n- Inflation strength $\\rho = 2.0$ and cap $\\delta_{\\max} = 3.0$.\n- Maximum number of iterations $K_{\\max} = 20$.\n- Use the mean-based gradient $\\nabla \\Phi(\\bar u_k) = J(\\bar u_k)^\\top \\Gamma^{-1}\\left(G(\\bar u_k)-y\\right)$.\n\nTest Suite. Implement and run the method on each of the following parameter sets, ensuring reproducibility by the provided random seeds. In each case, construct $H$ to meet the stated conditioning and set $C$ as a dense random matrix:\n- Case $1$: $n=8$, $m=5$, ensemble size $E=6$, noise level $\\sigma=0.05$, nonlinearity amplitude $b=0.1$, well-conditioned $H$; random seed $1$.\n- Case $2$: $n=8$, $m=5$, ensemble size $E=3$, noise level $\\sigma=0.05$, nonlinearity amplitude $b=0.1$, well-conditioned $H$; random seed $2$.\n- Case $3$: $n=10$, $m=6$, ensemble size $E=5$, noise level $\\sigma=0.05$, nonlinearity amplitude $b=0.2$, nearly rank-deficient $H$ (with rapidly decaying singular values); random seed $3$.\n\nConstruction of $H$: For well-conditioned $H$, draw a random Gaussian matrix and orthonormalize via singular value decomposition, then set singular values uniformly in $[0.8,1.2]$. For nearly rank-deficient $H$, set singular values geometrically decaying, e.g., $s_i = 10^{-i/(m)}$ for $i=1,\\dots,m$.\n\nFor each case, run both the baseline (no inflation) and the angle-inflated configuration. Declare a boolean success for the case if and only if:\n- The baseline configuration incurs premature collapse at some iteration, and\n- The angle-inflated configuration does not incur premature collapse, and\n- The angle-inflated configuration achieves the discrepancy target within $K_{\\max}$ iterations.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the $i$-th entry is the boolean success for Case $i$, in the order of the test suite. For example, the output must be of the form $[\\texttt{result1},\\texttt{result2},\\texttt{result3}]$ where each $\\texttt{resulti}$ is either $\\texttt{True}$ or $\\texttt{False}$.", "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, and objective. It presents a clear and formal task within the discipline of inverse problems and data assimilation, specifically focusing on the Ensemble Kalman Inversion (EKI) method. The provided definitions, equations, and constants are self-contained and mathematically consistent. The problem requests the implementation of a specific covariance inflation technique designed to mitigate premature ensemble collapse, a known issue in ensemble methods. The test cases are designed to challenge the algorithm under conditions prone to this failure mode, such as small ensemble sizes and ill-conditioned forward operators. The success criterion is unambiguously defined. The minor lack of specification for the distributions of the true parameter $u^\\star$ and the initial ensemble mean $\\mu_0$ and variance scale $\\beta^2$ will be addressed by using standard and reasonable choices: $u^\\star \\sim N(0, I_n)$, $\\mu_0=0_n$, and $\\beta=1$.\n\nThe solution proceeds by first implementing the regularized iterative Ensemble Kalman method and then incorporating the specified angle-based covariance inflation rule. We will then execute this method on the three provided test cases, comparing the performance of the baseline algorithm (without inflation) against the algorithm with angle-based inflation.\n\n### Theoretical Framework\n\nThe Ensemble Kalman Inversion (EKI) is an iterative, derivative-free method for solving inverse problems, which can be interpreted as an ensemble-based approximation of the Gauss–Newton optimization method. The goal is to find a parameter vector $u$ that minimizes the data misfit functional:\n$$\n\\Phi(u) = \\frac{1}{2}\\left\\| \\Gamma^{-1/2}\\left(G(u)-y\\right)\\right\\|_2^2\n$$\nThe update rule for an ensemble of parameters $\\{u_j^k\\}_{j=1}^E$ at iteration $k$ is given by:\n$$\nu_j^{k+1} = u_j^k + K_k \\left(y - G(u_j^k)\\right)\n$$\nThe key component is the Kalman-like gain matrix $K_k$, which in regularized EKI is:\n$$\nK_k = C^{uu}_k J(\\bar u_k)^\\top \\left(J(\\bar u_k) C^{uu}_k J(\\bar u_k)^\\top + \\alpha_k \\Gamma\\right)^{-1}\n$$\nHere, $C^{uu}_k$ is the sample covariance of the ensemble, which approximates the parameter covariance, and $J(\\bar u_k)$ is the Jacobian of the forward map $G$ evaluated at the ensemble mean $\\bar u_k$. The term $\\alpha_k \\Gamma$ provides Tikhonov regularization.\n\nA critical failure mode of EKI is **premature collapse**, where the ensemble variance, measured by $\\operatorname{tr}(C^{uu}_k)$, shrinks to a negligible fraction of its initial value before the parameters have converged to a region of low data misfit. This happens because the update step projects all ensemble members onto a low-dimensional subspace, causing them to lose diversity. When the ensemble subspace becomes orthogonal to the true descent direction (the gradient direction $\\nabla\\Phi$), the method stagnates.\n\n### Angle-Based Covariance Inflation\n\nThe proposed solution to this problem is a principled covariance inflation scheme. The core idea is to detect when the ensemble is failing to span the gradient direction and, in response, to inflate the ensemble covariance. The alignment between the ensemble subspace and the gradient is measured by the cosine of the angle $\\theta_k$ between $\\nabla \\Phi(\\bar u_k)$ and its projection onto the ensemble anomaly subspace, $\\operatorname{span}(A_k)$.\n$$\n\\cos(\\theta_k) = \\frac{\\left\\|P_k \\nabla \\Phi(\\bar u_k)\\right\\|_2}{\\left\\|\\nabla \\Phi(\\bar u_k)\\right\\|_2}\n$$\nwhere $P_k$ is the orthogonal projector onto the column space of the anomaly matrix $A_k$. A value of $\\cos(\\theta_k) \\approx 0$ indicates that the gradient is nearly orthogonal to the subspace spanned by the ensemble, signaling that the ensemble update will be ineffective. In this situation, the ensemble needs more diversity to explore the search space in the direction of the gradient.\n\nThe inflation parameter $\\delta_k$ is constructed to be large when $\\cos(\\theta_k)$ is small:\n$$\n\\delta_k = \\min\\left\\{\\rho \\left(1 - \\cos(\\theta_k)\\right), \\, \\delta_{\\max}\\right\\}\n$$\nThis inflation is then applied selectively to the covariance matrix used in the gain computation, $C^{uu}_k \\leftarrow (1+\\delta_k) C^{uu}_k$. This intervention increases the magnitude of the update step without altering the underlying ensemble members, effectively \"pushing\" the ensemble further along the directions it can represent, and helping it to re-align with the gradient in subsequent steps.\n\n### Implementation Details\n\nThe solution is implemented in Python using the `numpy` library.\n\n1.  **Setup and Data Generation**: For each test case, a random number generator is seeded for reproducibility. The matrices $H$ and $C$ are constructed according to the specifications. A ground truth parameter $u^\\star$ is drawn from a standard normal distribution, and synthetic data $y$ are generated by evaluating the forward model $G(u^\\star)$ and adding Gaussian noise with covariance $\\Gamma = \\sigma^2 I_m$. The initial ensemble $\\{u_j^0\\}$ is drawn from a standard normal prior distribution.\n\n2.  **EKI Solver**: A function implements the iterative EKI loop. In each iteration $k$:\n    a. The ensemble mean $\\bar u_k$ and anomaly matrix $A_k$ are computed.\n    b. The sample covariance $C^{uu}_k$ is formed.\n    c. The premature collapse condition, $\\operatorname{tr}(C^{uu}_k) / \\operatorname{tr}(C^{uu}_0)  \\epsilon$, is checked. If it is met and the discrepancy target has not been reached, the run is flagged for premature collapse.\n    d. The discrepancy stopping criterion, $\\Phi(\\bar u_k) \\le \\tau \\cdot m/2$, is checked. If met, the run is flagged as converged.\n    e. If `use_inflation` is true, the inflation factor $\\delta_k$ is computed. This involves calculating the gradient $\\nabla \\Phi(\\bar u_k)$, the projector $P_k$ (implicitly, by operating on the gradient), and $\\cos(\\theta_k)$.\n    f. An inflated covariance, $C^{uu, \\text{inflated}}_k = (1+\\delta_k)C^{uu}_k$, is used to compute the gain matrix $K_k$. For the baseline case, $\\delta_k=0$.\n    g. Each ensemble member $u_j^k$ is updated to $u_j^{k+1}$ using the gain $K_k$ and the residual $(y - G(u_j^k))$.\n\n3.  **Evaluation**: For each test case, the EKI solver is run twice: once for the baseline configuration (no inflation) and once for the angle-inflated configuration. The boolean success for the case is determined by verifying three conditions simultaneously, as required by the problem:\n    1. The baseline run resulted in 'premature_collapse'.\n    2. The inflated run did *not* result in 'premature_collapse'.\n    3. The inflated run resulted in 'converged' (i.e., it met the discrepancy target).\n\nThe final output is a list of these boolean success flags, one for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the final results.\n    \"\"\"\n    test_cases = [\n        {'n': 8, 'm': 5, 'E': 6, 'sigma': 0.05, 'b': 0.1, 'h_type': 'well', 'seed': 1},\n        {'n': 8, 'm': 5, 'E': 3, 'sigma': 0.05, 'b': 0.1, 'h_type': 'well', 'seed': 2},\n        {'n': 10, 'm': 6, 'E': 5, 'sigma': 0.05, 'b': 0.2, 'h_type': 'rank_def', 'seed': 3},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_one_case(**params)\n        results.append(result)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\ndef G(u, H, C, b):\n    \"\"\"Forward model G(u) = H*u + b*sin(C*u).\"\"\"\n    # Ensure u is a 1D vector for consistent matrix-vector products\n    if u.ndim > 1:\n        u = u.flatten()\n    return H @ u + b * np.sin(C @ u)\n\ndef J(u, H, C, b):\n    \"\"\"Jacobian of the forward model J(u).\"\"\"\n    if u.ndim > 1:\n        u = u.flatten()\n    return H + b * np.diag(np.cos(C @ u)) @ C\n\ndef phi(G_u, y, sigma_sq):\n    \"\"\"Data misfit functional Phi(u).\"\"\"\n    residual = G_u - y\n    return 0.5 * np.dot(residual, residual) / sigma_sq\n\ndef eki_solver(u_ens_initial, y, H, C, b, sigma, alpha, tau, epsilon, K_max, use_inflation, rho, delta_max):\n    \"\"\"\n    Implements the Ensemble Kalman Inversion algorithm.\n\n    Returns a tuple of (status, final_iteration_count).\n    Status can be 'converged', 'premature_collapse', or 'max_iter'.\n    \"\"\"\n    u_ens = u_ens_initial.copy()\n    n, E = u_ens.shape\n    m = y.shape[0]\n\n    sigma_sq = sigma**2\n    gamma_inv = (1.0 / sigma_sq) * np.eye(m)\n    gamma = sigma_sq * np.eye(m)\n    discrepancy_target = tau * m / 2.0\n\n    A0 = u_ens - u_ens.mean(axis=1, keepdims=True)\n    C_uu_0 = (1.0 / (E - 1)) * (A0 @ A0.T) if E > 1 else np.zeros((n,n))\n    trace_C0 = np.trace(C_uu_0)\n\n    if trace_C0  1e-15:\n        return 'error_zero_initial_trace', 0\n\n    for k in range(K_max):\n        u_mean = u_ens.mean(axis=1)\n        A_k = u_ens - u_mean[:, np.newaxis]\n        C_uu_k = (1.0 / (E - 1)) * (A_k @ A_k.T) if E > 1 else np.zeros((n, n))\n\n        # 1. Check for premature collapse\n        trace_Ck = np.trace(C_uu_k)\n        if trace_Ck / trace_C0  epsilon:\n            G_mean = G(u_mean, H, C, b)\n            misfit_val = phi(G_mean, y, sigma_sq)\n            if misfit_val > discrepancy_target:\n                return 'premature_collapse', k\n\n        # 2. Check for convergence\n        G_mean = G(u_mean, H, C, b)\n        misfit_val = phi(G_mean, y, sigma_sq)\n        if misfit_val = discrepancy_target:\n            return 'converged', k\n            \n        # 3. Calculate inflation\n        delta_k = 0.0\n        J_mean = J(u_mean, H, C, b)\n        if use_inflation:\n            grad = J_mean.T @ gamma_inv @ (G_mean - y)\n            norm_grad = np.linalg.norm(grad)\n            \n            if norm_grad > 1e-12:\n                # Project grad onto span(A_k): P_k * grad = A_k * pinv(A_k.T @ A_k) @ A_k.T @ grad\n                # We use pinv for stability, though inv might work if A_k.T @ A_k is full rank.\n                AT_grad = A_k.T @ grad\n                AT_A = A_k.T @ A_k\n                proj_grad = A_k @ (np.linalg.pinv(AT_A) @ AT_grad)\n                cos_theta_k = np.linalg.norm(proj_grad) / norm_grad\n            else:\n                cos_theta_k = 1.0\n            \n            delta_k = min(rho * (1.0 - cos_theta_k), delta_max)\n        \n        C_k_for_gain = (1.0 + delta_k) * C_uu_k\n        \n        # 4. Calculate Gain K_k\n        term = J_mean @ C_k_for_gain @ J_mean.T + alpha * gamma\n        K_k = C_k_for_gain @ J_mean.T @ np.linalg.inv(term)\n\n        # 5. Update ensemble\n        u_ens_new = np.zeros_like(u_ens)\n        for j in range(E):\n            G_j = G(u_ens[:, j], H, C, b)\n            u_ens_new[:, j] = u_ens[:, j] + K_k @ (y - G_j)\n        u_ens = u_ens_new\n    \n    # Check misfit one last time after max iterations\n    u_mean = u_ens.mean(axis=1)\n    G_mean = G(u_mean, H, C, b)\n    misfit_val = phi(G_mean, y, sigma_sq)\n    if misfit_val = discrepancy_target:\n        return 'converged', K_max\n    \n    return 'max_iter', K_max\n\ndef run_one_case(n, m, E, sigma, b, h_type, seed):\n    \"\"\"\n    Sets up and runs a single test case, returning the boolean success criteria.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct matrices H and C\n    A_rand = rng.standard_normal(size=(m, n))\n    U, s_vals, Vt = np.linalg.svd(A_rand, full_matrices=False)\n    \n    if h_type == 'well':\n        s_new = rng.uniform(0.8, 1.2, size=m)\n    else: # 'rank_def'\n        s_new = 10.0**(-np.arange(1, m + 1) / m)\n    \n    H = U @ np.diag(s_new) @ Vt\n    C = rng.standard_normal(size=(m, n))\n\n    # 2. Generate synthetic data\n    u_star = rng.standard_normal(size=n)\n    y = G(u_star, H, C, b) + rng.normal(0, sigma, size=m)\n\n    # 3. Generate initial ensemble (mean 0, variance 1)\n    mu0 = np.zeros(n)\n    beta = 1.0\n    u_ens_initial = rng.multivariate_normal(mu0, beta**2 * np.eye(n), size=E).T\n\n    # 4. Define constants\n    alpha = 0.05\n    tau = 1.5\n    epsilon = 0.05\n    rho = 2.0\n    delta_max = 3.0\n    K_max = 20\n\n    # 5. Run simulations\n    base_status, _ = eki_solver(u_ens_initial, y, H, C, b, sigma, alpha, tau, epsilon, K_max, use_inflation=False, rho=rho, delta_max=delta_max)\n    infl_status, _ = eki_solver(u_ens_initial, y, H, C, b, sigma, alpha, tau, epsilon, K_max, use_inflation=True, rho=rho, delta_max=delta_max)\n    \n    # 6. Evaluate success criterion\n    baseline_collapsed = (base_status == 'premature_collapse')\n    inflated_ok = (infl_status != 'premature_collapse')\n    inflated_converged = (infl_status == 'converged')\n\n    return baseline_collapsed and inflated_ok and inflated_converged\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3379138"}]}