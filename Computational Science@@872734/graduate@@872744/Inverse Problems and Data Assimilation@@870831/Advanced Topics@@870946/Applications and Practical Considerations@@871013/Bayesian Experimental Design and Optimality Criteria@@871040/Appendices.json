{"hands_on_practices": [{"introduction": "A foundational question in experimental design is whether it is better to repeat an existing experiment to improve precision or to conduct a new, different experiment to explore the parameter space more broadly. This exercise guides you through a direct comparison of these two strategies—replication versus diversification—within a clear linear-Gaussian framework [@problem_id:3367120]. By deriving the expected information gain for each case, you will develop a concrete intuition for why exploring different aspects of a system is often more valuable than repeatedly measuring just one.", "problem": "Consider a linear inverse problem with a two-dimensional parameter vector $\\theta \\in \\mathbb{R}^{2}$, modeled with a Gaussian prior $p(\\theta) = \\mathcal{N}(0,\\Sigma_{0})$ where $\\Sigma_{0} = \\mathrm{diag}(\\alpha,\\beta)$. You can perform $N$ experiments, each producing a scalar observation modeled by $y = X \\theta + \\varepsilon$, where $X \\in \\mathbb{R}^{N \\times 2}$ is the design matrix whose rows are the chosen design vectors, and $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2} I)$ is independent Gaussian noise. Two admissible designs are under consideration:\n\n- Design $\\mathrm{A}$ (replication): choose the same design vector $v_{1} = e_{1}$ twice, so $X_{\\mathrm{A}}$ has rows $e_{1}^{\\top}$ and $e_{1}^{\\top}$.\n- Design $\\mathrm{B}$ (diversification): choose orthogonal design vectors $v_{1} = e_{1}$ and $v_{2} = e_{2}$, so $X_{\\mathrm{B}}$ has rows $e_{1}^{\\top}$ and $e_{2}^{\\top}$.\n\nAdopt as the Bayesian experimental design utility the expected Kullback–Leibler divergence (KLD) from the posterior distribution to the prior distribution of $\\theta$, that is, the mutual information between $\\theta$ and the data $y$ under the specified linear-Gaussian model. Starting from first principles and core definitions, derive the utility for each of the two designs and compute the difference\n$$\\Delta U = U_{\\mathrm{B}} - U_{\\mathrm{A}}$$\nfor the specific parameter and noise settings $\\alpha = 9$, $\\beta = 1$, and $\\sigma^{2} = 1$. Express your final answer as a single closed-form analytic expression involving the natural logarithm, with no rounding and no units.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, self-contained, and consistent. It represents a standard problem in Bayesian experimental design for linear inverse problems. We proceed to the solution.\n\nThe utility of an experimental design, denoted by the design matrix $X$, is given by the expected Kullback–Leibler divergence (KLD) from the posterior distribution of the parameters $\\theta$ to their prior distribution. This quantity is equivalent to the mutual information between the parameters $\\theta$ and the data $y$.\nFor a given design $X$, the utility is $U(X) = I(\\theta; y | X)$.\nThe mutual information can be expressed using differential entropies as $I(\\theta; y) = H(\\theta) - H(\\theta|y)$.\n\nThe prior distribution for $\\theta \\in \\mathbb{R}^{k}$ is given as $p(\\theta) = \\mathcal{N}(\\mu_0, \\Sigma_0)$. The differential entropy of a multivariate Gaussian distribution is $H(\\theta) = \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma_0)$. In this problem, the dimension of $\\theta$ is $k=2$ and the prior mean is $\\mu_0 = 0$.\n\nThe observation model is $y = X\\theta + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$. The likelihood function is $p(y|\\theta) = \\mathcal{N}(y | X\\theta, \\sigma^2 I)$.\nUnder this linear-Gaussian model, the posterior distribution $p(\\theta|y)$ is also Gaussian, $p(\\theta|y) = \\mathcal{N}(\\mu_{post}, \\Sigma_{post})$. The posterior covariance matrix $\\Sigma_{post}$ is given by the inverse of the sum of the inverse prior covariance and the Fisher information from the data:\n$$ \\Sigma_{post}^{-1} = \\Sigma_0^{-1} + \\frac{1}{\\sigma^2} X^{\\top}X $$\nCrucially, $\\Sigma_{post}$ does not depend on the specific data realization $y$. The entropy of the posterior distribution is $H(p(\\theta|y)) = \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma_{post})$.\nThe conditional entropy $H(\\theta|y)$ is the expectation of $H(p(\\theta|y))$ over all possible data $y$:\n$$ H(\\theta|y) = \\mathbb{E}_{y}[H(p(\\theta|y))] $$\nSince $H(p(\\theta|y))$ is independent of $y$, the expectation is trivial, and $H(\\theta|y) = \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma_{post})$.\n\nSubstituting the entropies into the mutual information formula:\n$$ U(X) = H(\\theta) - H(\\theta|y) = \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma_0) - \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma_{post}) $$\n$$ U(X) = \\frac{1}{2} \\left[ \\ln(\\det(\\Sigma_0)) - \\ln(\\det(\\Sigma_{post})) \\right] = \\frac{1}{2} \\ln\\left(\\frac{\\det(\\Sigma_0)}{\\det(\\Sigma_{post})}\\right) = \\frac{1}{2} \\ln(\\det(\\Sigma_0 \\Sigma_{post}^{-1})) $$\nSubstituting the expression for $\\Sigma_{post}^{-1}$:\n$$ U(X) = \\frac{1}{2} \\ln\\left(\\det\\left(\\Sigma_0 \\left(\\Sigma_0^{-1} + \\frac{1}{\\sigma^2} X^{\\top}X\\right)\\right)\\right) = \\frac{1}{2} \\ln\\left(\\det\\left(I + \\frac{1}{\\sigma^2} \\Sigma_0 X^{\\top}X\\right)\\right) $$\nThis expression provides the utility for any design $X$. We now apply this formula to the two specified designs using the given parameter values: $\\Sigma_0 = \\mathrm{diag}(\\alpha, \\beta) = \\mathrm{diag}(9, 1)$ and $\\sigma^2=1$. The utility simplifies to:\n$$ U(X) = \\frac{1}{2} \\ln(\\det(I + \\Sigma_0 X^{\\top}X)) $$\n\nFor Design A (replication), the design matrix $X_{\\mathrm{A}}$ has two identical rows $e_1^{\\top} = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$.\n$$ X_{\\mathrm{A}} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\end{pmatrix} $$\nWe compute the matrix product $X_{\\mathrm{A}}^{\\top}X_{\\mathrm{A}}$:\n$$ X_{\\mathrm{A}}^{\\top}X_{\\mathrm{A}} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\nNow we can compute the utility $U_{\\mathrm{A}}$:\n$$ U_{\\mathrm{A}} = \\frac{1}{2} \\ln\\left(\\det\\left( \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 9 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 0 \\end{pmatrix} \\right)\\right) $$\n$$ U_{\\mathrm{A}} = \\frac{1}{2} \\ln\\left(\\det\\left( \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 18 & 0 \\\\ 0 & 0 \\end{pmatrix} \\right)\\right) $$\n$$ U_{\\mathrm{A}} = \\frac{1}{2} \\ln\\left(\\det\\begin{pmatrix} 19 & 0 \\\\ 0 & 1 \\end{pmatrix}\\right) = \\frac{1}{2} \\ln(19) $$\n\nFor Design B (diversification), the design matrix $X_{\\mathrm{B}}$ has rows $e_1^{\\top} = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$ and $e_2^{\\top} = \\begin{pmatrix} 0 & 1 \\end{pmatrix}$.\n$$ X_{\\mathrm{B}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2 $$\nThe matrix product $X_{\\mathrm{B}}^{\\top}X_{\\mathrm{B}}$ is:\n$$ X_{\\mathrm{B}}^{\\top}X_{\\mathrm{B}} = I_2^{\\top} I_2 = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\nNow we can compute the utility $U_{\\mathrm{B}}$:\n$$ U_{\\mathrm{B}} = \\frac{1}{2} \\ln\\left(\\det\\left( I_2 + \\Sigma_0 I_2 \\right)\\right) = \\frac{1}{2} \\ln(\\det(I_2 + \\Sigma_0)) $$\n$$ U_{\\mathrm{B}} = \\frac{1}{2} \\ln\\left(\\det\\left( \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 9 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right)\\right) $$\n$$ U_{\\mathrm{B}} = \\frac{1}{2} \\ln\\left(\\det\\begin{pmatrix} 10 & 0 \\\\ 0 & 2 \\end{pmatrix}\\right) = \\frac{1}{2} \\ln(20) $$\n\nFinally, we compute the difference in utility $\\Delta U = U_{\\mathrm{B}} - U_{\\mathrm{A}}$:\n$$ \\Delta U = \\frac{1}{2} \\ln(20) - \\frac{1}{2} \\ln(19) $$\nUsing the property of logarithms $\\ln(a) - \\ln(b) = \\ln(a/b)$, we get:\n$$ \\Delta U = \\frac{1}{2} \\left( \\ln(20) - \\ln(19) \\right) = \\frac{1}{2} \\ln\\left(\\frac{20}{19}\\right) $$\nThis is the final closed-form analytical expression.", "answer": "$$\\boxed{\\frac{1}{2} \\ln\\left(\\frac{20}{19}\\right)}$$", "id": "3367120"}, {"introduction": "In any practical setting, experimental resources are finite. A key task is to decide when the cost of an additional measurement outweighs the expected informational benefit. This practice introduces the crucial concept of diminishing returns, a cornerstone of efficient experimental design [@problem_id:3367052]. By calculating the second derivative, or curvature, of the information gain objective function with respect to experimental effort $u$, you will quantify how the value of new data decreases as our knowledge grows, providing a formal basis for resource allocation and cost-benefit analysis.", "problem": "Consider a linear-Gaussian Bayesian inverse problem with parameter vector $\\theta \\in \\mathbb{R}^{d}$ endowed with a Gaussian prior $\\theta \\sim \\mathcal{N}(0, \\Gamma_{\\text{prior}})$, and a linear observation model $y = H \\theta + \\eta$. The observational noise is Gaussian with covariance $\\Gamma_{\\eta}/u$, where $u > 0$ is a scalar effort parameter (for example, proportional to the number of independent replicates), and $\\eta \\sim \\mathcal{N}(0, \\Gamma_{\\eta}/u)$. Under this model, the Expected Information Gain (Kullback–Leibler divergence from the posterior to the prior) for a given effort $u$ can be expressed in terms of the ratio of posterior and prior covariance determinants. In this problem, use the following data:\n- Dimension $d = 2$.\n- Prior covariance $\\Gamma_{\\text{prior}} = \\begin{pmatrix} 2 & 0.6 \\\\ 0.6 & 1.2 \\end{pmatrix}$.\n- Forward model matrix $H = \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix}$.\n- Noise covariance $\\Gamma_{\\eta} = \\begin{pmatrix} 1.0 & 0.2 \\\\ 0.2 & 1.5 \\end{pmatrix}$.\n\nStarting from core definitions of Gaussian conditioning and the log-determinant identity for mutual information, derive the scalar D-optimal design objective $J(u)$ as a function of $u$ and compute its second derivative with respect to $u$ to characterize curvature (which captures diminishing returns). Then, for the specific value $u = 1$, evaluate the curvature $J''(1)$ numerically using the provided matrices.\n\nRound your answer to $4$ significant figures. Express the final answer as a dimensionless real number.", "solution": "The problem requires the derivation and evaluation of the curvature of the D-optimal design objective function, $J(u)$, for a linear-Gaussian Bayesian inverse problem. The objective function is the Expected Information Gain (EIG), which for this model is the Kullback-Leibler divergence from the posterior to the prior, averaged over all possible data.\n\nThe prior distribution for the parameter vector $\\theta \\in \\mathbb{R}^{d}$ is given as $\\theta \\sim \\mathcal{N}(0, \\Gamma_{\\text{prior}})$. The observation model is $y = H \\theta + \\eta$, where the noise $\\eta$ is distributed as $\\eta \\sim \\mathcal{N}(0, \\Gamma_{\\eta}/u)$. Here, $u$ is a scalar effort parameter.\n\nThe posterior distribution of $\\theta$ given an observation $y$ is also a Gaussian, $p(\\theta|y) = \\mathcal{N}(\\mu_{\\text{post}}, \\Gamma_{\\text{post}})$. The posterior precision (inverse covariance) matrix is the sum of the prior precision and the data-dependent precision from the likelihood:\n$$ \\Gamma_{\\text{post}}^{-1} = \\Gamma_{\\text{prior}}^{-1} + H^T (\\Gamma_{\\eta}/u)^{-1} H = \\Gamma_{\\text{prior}}^{-1} + u H^T \\Gamma_{\\eta}^{-1} H $$\n\nThe EIG, which we denote as $J(u)$, is the mutual information between the parameters $\\theta$ and the data $y$. For a linear-Gaussian system, this quantity is independent of the specific observation $y$ and can be expressed in terms of the prior and posterior covariances:\n$$ J(u) = \\frac{1}{2} \\ln \\left( \\frac{\\det(\\Gamma_{\\text{prior}})}{\\det(\\Gamma_{\\text{post}})} \\right) = \\frac{1}{2} \\ln \\left( \\det(\\Gamma_{\\text{post}}^{-1} \\Gamma_{\\text{prior}}) \\right) $$\nSubstituting the expression for $\\Gamma_{\\text{post}}^{-1}$:\n$$ J(u) = \\frac{1}{2} \\ln \\left( \\det\\left( (\\Gamma_{\\text{prior}}^{-1} + u H^T \\Gamma_{\\eta}^{-1} H) \\Gamma_{\\text{prior}} \\right) \\right) = \\frac{1}{2} \\ln \\left( \\det(I + u H^T \\Gamma_{\\eta}^{-1} H \\Gamma_{\\text{prior}}) \\right) $$\nUsing the Sylvester determinant identity, $\\det(I + AB) = \\det(I + BA)$, we can write this as:\n$$ J(u) = \\frac{1}{2} \\ln \\left( \\det(I + u \\Gamma_{\\text{prior}} H^T \\Gamma_{\\eta}^{-1} H) \\right) $$\nLet us define the matrix $\\mathcal{I} = \\Gamma_{\\text{prior}} H^T \\Gamma_{\\eta}^{-1} H$. While $\\mathcal{I}$ is not generally symmetric, its eigenvalues are real, non-negative, and identical to those of the symmetric matrix $\\mathcal{I}_s = \\Gamma_{\\text{prior}}^{1/2} H^T \\Gamma_{\\eta}^{-1} H \\Gamma_{\\text{prior}}^{1/2}$. Let the eigenvalues of $\\mathcal{I}$ (and $\\mathcal{I}_s$) be $\\{\\lambda_i\\}_{i=1}^{d}$. Then the determinant can be expressed as a product over eigenvalues:\n$$ J(u) = \\frac{1}{2} \\ln \\left( \\prod_{i=1}^{d} (1 + u \\lambda_i) \\right) = \\frac{1}{2} \\sum_{i=1}^{d} \\ln(1 + u \\lambda_i) $$\nThis is the derived objective function.\n\nTo find the curvature, we compute the second derivative of $J(u)$ with respect to $u$. The first derivative is:\n$$ J'(u) = \\frac{dJ}{du} = \\frac{1}{2} \\sum_{i=1}^{d} \\frac{\\lambda_i}{1 + u \\lambda_i} $$\nThe second derivative, which represents the curvature, is:\n$$ J''(u) = \\frac{d^2J}{du^2} = \\frac{1}{2} \\sum_{i=1}^{d} \\frac{-(\\lambda_i)(\\lambda_i)}{(1 + u \\lambda_i)^2} = -\\frac{1}{2} \\sum_{i=1}^{d} \\left( \\frac{\\lambda_i}{1 + u \\lambda_i} \\right)^2 $$\nThis expression for $J''(u)$ is always non-positive, which correctly captures the principle of diminishing returns on investment of effort $u$.\n\nNow we evaluate $J''(1)$ using the provided matrices. The dimension is $d=2$.\n- $\\Gamma_{\\text{prior}} = \\begin{pmatrix} 2 & 0.6 \\\\ 0.6 & 1.2 \\end{pmatrix}$\n- $H = \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix}$\n- $\\Gamma_{\\eta} = \\begin{pmatrix} 1.0 & 0.2 \\\\ 0.2 & 1.5 \\end{pmatrix}$\n\nFirst, we compute the matrix $\\mathcal{I} = \\Gamma_{\\text{prior}} H^T \\Gamma_{\\eta}^{-1} H$ and find its eigenvalues.\n\n1.  Calculate $\\Gamma_{\\eta}^{-1}$:\n    $\\det(\\Gamma_{\\eta}) = (1.0)(1.5) - (0.2)(0.2) = 1.5 - 0.04 = 1.46$.\n    $$ \\Gamma_{\\eta}^{-1} = \\frac{1}{1.46} \\begin{pmatrix} 1.5 & -0.2 \\\\ -0.2 & 1.0 \\end{pmatrix} $$\n\n2.  Calculate the Fisher information matrix $H^T \\Gamma_{\\eta}^{-1} H$:\n    Since $H$ is symmetric, $H^T = H$.\n    $$ H^T \\Gamma_{\\eta}^{-1} H = \\frac{1}{1.46} \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1.5 & -0.2 \\\\ -0.2 & 1.0 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix} $$\n    $$ = \\frac{1}{1.46} \\begin{pmatrix} 1.3 & 0.8 \\\\ 1.1 & 1.8 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{1.46} \\begin{pmatrix} 2.1 & 2.9 \\\\ 2.9 & 4.7 \\end{pmatrix} $$\n\n3.  Calculate $\\mathcal{I} = \\Gamma_{\\text{prior}} (H^T \\Gamma_{\\eta}^{-1} H)$:\n    $$ \\mathcal{I} = \\begin{pmatrix} 2 & 0.6 \\\\ 0.6 & 1.2 \\end{pmatrix} \\left( \\frac{1}{1.46} \\begin{pmatrix} 2.1 & 2.9 \\\\ 2.9 & 4.7 \\end{pmatrix} \\right) $$\n    $$ \\mathcal{I} = \\frac{1}{1.46} \\begin{pmatrix} (2)(2.1) + (0.6)(2.9) & (2)(2.9) + (0.6)(4.7) \\\\ (0.6)(2.1) + (1.2)(2.9) & (0.6)(2.9) + (1.2)(4.7) \\end{pmatrix} $$\n    $$ \\mathcal{I} = \\frac{1}{1.46} \\begin{pmatrix} 4.2 + 1.74 & 5.8 + 2.82 \\\\ 1.26 + 3.48 & 1.74 + 5.64 \\end{pmatrix} = \\frac{1}{1.46} \\begin{pmatrix} 5.94 & 8.62 \\\\ 4.74 & 7.38 \\end{pmatrix} $$\n\n4.  Find the eigenvalues of $\\mathcal{I}$. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(\\mathcal{I} - \\lambda I) = 0$. Let $A = \\begin{pmatrix} 5.94 & 8.62 \\\\ 4.74 & 7.38 \\end{pmatrix}$, so the eigenvalues of $\\mathcal{I}$ are $\\frac{1}{1.46}$ times the eigenvalues of $A$. The characteristic equation for $A$ is $\\Lambda^2 - \\text{tr}(A)\\Lambda + \\det(A) = 0$.\n    $\\text{tr}(A) = 5.94 + 7.38 = 13.32$.\n    $\\det(A) = (5.94)(7.38) - (8.62)(4.74) = 43.8372 - 40.8588 = 2.9784$.\n    So, $\\Lambda^2 - 13.32 \\Lambda + 2.9784 = 0$.\n    Using the quadratic formula, $\\Lambda = \\frac{13.32 \\pm \\sqrt{13.32^2 - 4(2.9784)}}{2} = \\frac{13.32 \\pm \\sqrt{165.5088}}{2}$.\n    $\\sqrt{165.5088} \\approx 12.864983$.\n    $\\Lambda_1 = \\frac{13.32 + 12.864983}{2} \\approx 13.0924915$.\n    $\\Lambda_2 = \\frac{13.32 - 12.864983}{2} \\approx 0.2275085$.\n    The eigenvalues of $\\mathcal{I}$ are:\n    $\\lambda_1 = \\Lambda_1 / 1.46 \\approx 13.0924915 / 1.46 \\approx 8.967460$.\n    $\\lambda_2 = \\Lambda_2 / 1.46 \\approx 0.2275085 / 1.46 \\approx 0.155828$.\n\n5.  Evaluate $J''(u)$ at $u=1$:\n    $$ J''(1) = -\\frac{1}{2} \\left[ \\left( \\frac{\\lambda_1}{1 + \\lambda_1} \\right)^2 + \\left( \\frac{\\lambda_2}{1 + \\lambda_2} \\right)^2 \\right] $$\n    $$ J''(1) \\approx -\\frac{1}{2} \\left[ \\left( \\frac{8.967460}{1 + 8.967460} \\right)^2 + \\left( \\frac{0.155828}{1 + 0.155828} \\right)^2 \\right] $$\n    $$ J''(1) \\approx -\\frac{1}{2} \\left[ \\left( \\frac{8.967460}{9.967460} \\right)^2 + \\left( \\frac{0.155828}{1.155828} \\right)^2 \\right] $$\n    $$ J''(1) \\approx -\\frac{1}{2} \\left[ (0.899679)^2 + (0.134819)^2 \\right] $$\n    $$ J''(1) \\approx -\\frac{1}{2} [ 0.809422 + 0.018176 ] $$\n    $$ J''(1) \\approx -\\frac{1}{2} [ 0.827598 ] \\approx -0.413799 $$\n\nRounding to $4$ significant figures, the curvature is $-0.4138$.", "answer": "$$\\boxed{-0.4138}$$", "id": "3367052"}, {"introduction": "Many modern experimental design problems, such as selecting optimal locations for a limited number of sensors, are combinatorial challenges that are computationally intractable to solve by exhaustion. This exercise transitions from pure theory to algorithmic implementation, tackling the sensor selection problem using a powerful greedy algorithm [@problem_id:3367093]. You will derive the information-theoretic objective function and discover the property of submodularity, which explains why a simple, iterative approach can produce a provably near-optimal design, bridging the gap between theoretical criteria and practical, large-scale application.", "problem": "Consider a linear-Gaussian data assimilation model where the unknown state is a vector $x \\in \\mathbb{R}^d$ with a Gaussian prior $x \\sim \\mathcal{N}(0, \\Sigma_{\\text{prior}})$. There are $m$ candidate sensors, each represented by a vector $h_i \\in \\mathbb{R}^d$ for $i \\in \\{0,1,\\dots,m-1\\}$. A selected set of sensors $S \\subseteq \\{0,1,\\dots,m-1\\}$ produces measurements $y_S \\in \\mathbb{R}^{|S|}$ according to the linear model $y_S = H_S x + \\eta_S$, where $H_S \\in \\mathbb{R}^{|S| \\times d}$ has rows $h_i^\\top$ for $i \\in S$, and $\\eta_S \\sim \\mathcal{N}(0, \\sigma^2 I_{|S|})$ with known noise variance $\\sigma^2 > 0$. The Bayesian design objective considered here is the expected information gain from the prior to the posterior over $x$ induced by selecting the sensors in $S$, which can be characterized using the Kullback–Leibler divergence (KLD). Your task is to start from the definitions of Gaussian distributions, linear measurements, and mutual information, and derive an implementable expression for the expected information gain objective as a set function $f(S)$ of the selected sensors $S$. The expected information gain for the linear-Gaussian case can be expressed in terms of the prior covariance and the sensor geometry without requiring any knowledge of the realization of $x$ or $y_S$.\n\nSubmodularity is a key concept for optimal design under cardinality constraints. A set function $f:2^{\\{0,\\dots,m-1\\}} \\to \\mathbb{R}$ is submodular if for any sets $A \\subseteq B \\subseteq \\{0,\\dots,m-1\\}$ and any element $e \\in \\{0,\\dots,m-1\\} \\setminus B$, the diminishing returns property holds: $f(A \\cup \\{e\\}) - f(A) \\ge f(B \\cup \\{e\\}) - f(B)$. In this problem, you will numerically verify the submodularity of the derived $f(S)$ on specified finite instances, and implement a greedy sensor selection algorithm under a cardinality constraint.\n\nStarting from the fundamental definitions of Gaussian distributions, linear measurements, covariance, precision, and mutual information, derive an expression for the expected information gain objective $f(S)$ as a function of $\\Sigma_{\\text{prior}}$, $\\sigma^2$, and $\\{h_i\\}_{i \\in S}$. You must not assume any specific parameterized formula without derivation; begin from the base definitions of mutual information and properties of Gaussian random variables and their linear transforms. Use the derived expression to implement:\n\n- A function that computes $f(S)$ for any subset $S \\subseteq \\{0,\\dots,m-1\\}$.\n- A greedy algorithm that selects $k$ sensors by iteratively adding the sensor with the largest marginal increase in $f$ at each step, with ties broken by the smallest index.\n- An exhaustive search to compute the optimal set of size $k$ for small $m$.\n\nYou will also numerically verify submodularity by checking the diminishing returns inequality for all pairs $(A,B,e)$ with $A \\subseteq B \\subseteq \\{0,\\dots,m-1\\}$ and $e \\notin B$ on the specified instances, using a numerical tolerance $\\varepsilon = 10^{-9}$: declare the inequality satisfied if $f(A \\cup \\{e\\}) - f(A) + \\varepsilon \\ge f(B \\cup \\{e\\}) - f(B)$ for all such triples.\n\nImplement your program to produce results on the following test suite. All indices are zero-based (that is, the first sensor has index $0$). There are no physical units in this problem.\n\n- Test Case $1$ (general linear-Gaussian instance):\n  - State dimension: $d = 3$.\n  - Prior covariance: $\\Sigma_{\\text{prior}} = I_3$.\n  - Noise variance: $\\sigma^2 = 0.10$.\n  - Candidate sensors: $m = 5$ with\n    - $h_0 = [1.0, 0.0, 0.0]$,\n    - $h_1 = [1.0, 0.0, 0.0]$,\n    - $h_2 = [0.0, 1.0, 0.0]$,\n    - $h_3 = [0.0, 0.0, 1.0]$,\n    - $h_4 = [1.0, 1.0, 0.0]$.\n  - Cardinality: $k = 2$.\n  - Required output for this case: the greedy–optimal gap as a real number $g_1$, where $g_1 = f(S_{\\text{greedy}}) - f(S_{\\text{optimal}})$, rounded to $8$ decimal places.\n\n- Test Case $2$ (submodularity verification on Test Case $1$):\n  - Use the same parameters as Test Case $1$.\n  - Required output: a boolean $b_2$ that is $\\text{True}$ if the diminishing returns inequality holds for all $(A,B,e)$ with tolerance $\\varepsilon = 10^{-9}$, and $\\text{False}$ otherwise.\n\n- Test Case $3$ (boundary condition):\n  - State dimension: $d = 2$.\n  - Prior covariance: $\\Sigma_{\\text{prior}} = I_2$.\n  - Noise variance: $\\sigma^2 = 1.0$.\n  - Candidate sensors: $m = 3$ with\n    - $h_0 = [1.0, 0.0]$,\n    - $h_1 = [0.0, 1.0]$,\n    - $h_2 = [1.0, 1.0]$.\n  - Cardinality: $k = 0$.\n  - Required output: a boolean $b_3$ that is $\\text{True}$ if $f(\\emptyset) = 0$ (within tolerance $\\varepsilon = 10^{-12}$) and the greedy algorithm returns the empty set, and $\\text{False}$ otherwise.\n\n- Test Case $4$ (non-identity prior covariance and richer sensor geometry):\n  - State dimension: $d = 4$.\n  - Prior covariance: $\\Sigma_{\\text{prior}} \\in \\mathbb{R}^{4 \\times 4}$ with entries $[\\Sigma_{\\text{prior}}]_{ij} = \\exp\\left(-\\frac{(i-j)^2}{2 \\ell^2}\\right)$ for grid indices $i,j \\in \\{0,1,2,3\\}$ and lengthscale $\\ell = 1.0$.\n  - Noise variance: $\\sigma^2 = 0.50$.\n  - Candidate sensors: $m = 6$ with\n    - $h_0 = [1.0, 0.0, 0.0, 0.0]$,\n    - $h_1 = [0.0, 1.0, 0.0, 0.0]$,\n    - $h_2 = [0.0, 0.0, 1.0, 0.0]$,\n    - $h_3 = [0.0, 0.0, 0.0, 1.0]$,\n    - $h_4 = \\frac{1}{\\sqrt{2}} [1.0, 1.0, 0.0, 0.0]$,\n    - $h_5 = \\frac{1}{\\sqrt{2}} [0.0, 1.0, 1.0, 0.0]$.\n  - Cardinality: $k = 3$.\n  - Required output: the greedy-selected sensor indices $S_{\\text{greedy}}$ as a list of integers in ascending order.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order $[g_1, b_2, b_3, S_{\\text{greedy}}]$, where $g_1$ is a real number rounded to eight decimal places, $b_2$ and $b_3$ are booleans, and $S_{\\text{greedy}}$ is a list of integers. For example, a valid output would look like $[0.00000000,True,True,[0,2,5]]$.", "solution": "The user wants to solve a Bayesian experimental design problem involving sensor selection. The task involves deriving the objective function, which is the expected information gain, and then implementing algorithms to perform sensor selection and verify a mathematical property of the objective function.\n\n### Part 1: Derivation of the Expected Information Gain Objective Function $f(S)$\n\nThe objective is to find a computationally tractable expression for the expected information gain obtained by selecting a set of sensors $S$. This quantity is equivalent to the mutual information $I(x; y_S)$ between the unknown state vector $x$ and the measurement vector $y_S$.\n\nThe definition of mutual information is given by:\n$$I(x; y_S) = H(y_S) - H(y_S | x)$$\nwhere $H(y_S)$ is the entropy of the measurement vector $y_S$, and $H(y_S | x)$ is the conditional entropy of $y_S$ given $x$. We will derive expressions for each of these terms based on the linear-Gaussian model provided.\n\nThe state $x$ and noise $\\eta_S$ are given as zero-mean Gaussian random variables:\n-   Prior on state: $x \\sim \\mathcal{N}(0, \\Sigma_{\\text{prior}})$\n-   Measurement noise: $\\eta_S \\sim \\mathcal{N}(0, \\sigma^2 I_{|S|})$, where $|S|$ is the number of sensors in set $S$.\n\nThe measurement model is linear:\n$$y_S = H_S x + \\eta_S$$\nwhere $H_S$ is the measurement matrix corresponding to the sensor set $S$.\n\n**1. Conditional Entropy $H(y_S | x)$**\n\nGiven a specific realization of the state vector $x$, the measurement vector $y_S$ is a sum of a constant vector $H_S x$ and a Gaussian random vector $\\eta_S$. The resulting conditional distribution of $y_S$ given $x$ is also Gaussian:\n$$y_S | x \\sim \\mathcal{N}(H_S x, \\sigma^2 I_{|S|})$$\nThe differential entropy of a $k$-dimensional Gaussian random variable $z \\sim \\mathcal{N}(\\mu, \\Sigma)$ is given by the formula:\n$$H(z) = \\frac{1}{2} \\ln \\det(2 \\pi e \\Sigma) = \\frac{k}{2} (1 + \\ln(2\\pi)) + \\frac{1}{2} \\ln \\det(\\Sigma)$$\nApplying this to $y_S | x$, where the dimension is $k = |S|$ and the covariance is $\\sigma^2 I_{|S|}$, we get:\n$$H(y_S | x) = \\frac{1}{2} \\ln \\det(2 \\pi e (\\sigma^2 I_{|S|}))$$\nNote that this expression is independent of the specific value of $x$. Therefore, the conditional entropy, which is the expectation of $H(y_S | x)$ over all possible $x$, is simply:\n$$H(y_S | x) = \\mathbb{E}_{x}[H(y_S | x)] = \\frac{1}{2} \\ln \\det(2 \\pi e \\sigma^2 I_{|S|})$$\n\n**2. Entropy of Measurements $H(y_S)$**\n\nTo find the entropy of $y_S$, we first need to determine its marginal distribution. Since $x$ and $\\eta_S$ are both zero-mean Gaussian random variables and $y_S$ is a linear combination of them, $y_S$ is also a zero-mean Gaussian. We need to find its covariance matrix, $\\Sigma_{Y_S}$.\n\nThe mean is:\n$$\\mathbb{E}[y_S] = \\mathbb{E}[H_S x + \\eta_S] = H_S \\mathbb{E}[x] + \\mathbb{E}[\\eta_S] = H_S \\cdot 0 + 0 = 0$$\n\nThe covariance is:\n$$\\Sigma_{Y_S} = \\text{Cov}(y_S) = \\mathbb{E}[y_S y_S^\\top] = \\mathbb{E}[(H_S x + \\eta_S)(H_S x + \\eta_S)^\\top]$$\nExpanding this expression gives:\n$$\\Sigma_{Y_S} = \\mathbb{E}[H_S x x^\\top H_S^\\top + H_S x \\eta_S^\\top + \\eta_S x^\\top H_S^\\top + \\eta_S \\eta_S^\\top]$$\nSince $x$ and $\\eta_S$ are independent, the cross-terms have zero expectation: $\\mathbb{E}[H_S x \\eta_S^\\top] = H_S \\mathbb{E}[x] \\mathbb{E}[\\eta_S^\\top] = 0$.\nThe expression simplifies to:\n$$\\Sigma_{Y_S} = H_S \\mathbb{E}[x x^\\top] H_S^\\top + \\mathbb{E}[\\eta_S \\eta_S^\\top] = H_S \\Sigma_{\\text{prior}} H_S^\\top + \\sigma^2 I_{|S|}$$\nSo, the marginal distribution of the measurements is:\n$$y_S \\sim \\mathcal{N}(0, H_S \\Sigma_{\\text{prior}} H_S^\\top + \\sigma^2 I_{|S|})$$\nThe entropy of $y_S$ is therefore:\n$$H(y_S) = \\frac{1}{2} \\ln \\det(2 \\pi e (H_S \\Sigma_{\\text{prior}} H_S^\\top + \\sigma^2 I_{|S|}))$$\n\n**3. Mutual Information $I(x; y_S)$**\n\nNow we combine the expressions for $H(y_S)$ and $H(y_S | x)$ to find the mutual information:\n$$I(x; y_S) = H(y_S) - H(y_S | x)$$\n$$I(x; y_S) = \\frac{1}{2} \\ln \\det(2 \\pi e (H_S \\Sigma_{\\text{prior}} H_S^\\top + \\sigma^2 I_{|S|})) - \\frac{1}{2} \\ln \\det(2 \\pi e \\sigma^2 I_{|S|})$$\nUsing the properties of logarithms, $\\ln a - \\ln b = \\ln(a/b)$, and determinants, $\\ln \\det(A) - \\ln \\det(B) = \\ln \\det(AB^{-1})$:\n$$I(x; y_S) = \\frac{1}{2} \\ln \\left( \\frac{\\det(2 \\pi e (H_S \\Sigma_{\\text{prior}} H_S^\\top + \\sigma^2 I_{|S|}))}{\\det(2 \\pi e \\sigma^2 I_{|S|})} \\right)$$\nThe term $2 \\pi e$ cancels out:\n$$I(x; y_S) = \\frac{1}{2} \\ln \\left( \\frac{\\det(H_S \\Sigma_{\\text{prior}} H_S^\\top + \\sigma^2 I_{|S|})}{\\det(\\sigma^2 I_{|S|})} \\right)$$\n$$I(x; y_S) = \\frac{1}{2} \\ln \\det \\left( (\\sigma^2 I_{|S|})^{-1} (H_S \\Sigma_{\\text{prior}} H_S^\\top + \\sigma^2 I_{|S|}) \\right)$$\n$$I(x; y_S) = \\frac{1}{2} \\ln \\det \\left( \\frac{1}{\\sigma^2} H_S \\Sigma_{\\text{prior}} H_S^\\top + I_{|S|} \\right)$$\nThis gives us the final, implementable expression for the objective function $f(S)$:\n$$f(S) = \\frac{1}{2} \\ln \\det \\left( I_{|S|} + \\frac{1}{\\sigma^2} H_S \\Sigma_{\\text{prior}} H_S^\\top \\right)$$\nThis function depends only on the sensor geometry (through $H_S$), the prior uncertainty ($\\Sigma_{\\text{prior}}$), and the noise level ($\\sigma^2$), as required.\n\nFor the special case where $S = \\emptyset$ (no sensors are selected), $|S|=0$. The matrix $H_S$ is a $0 \\times d$ matrix. The product $H_S \\Sigma_{\\text{prior}} H_S^\\top$ is a $0 \\times 0$ matrix, which is considered to be the scalar $0$. The identity matrix $I_0$ is a $0 \\times 0$ matrix with determinant $1$. Therefore,\n$$f(\\emptyset) = \\frac{1}{2} \\ln \\det(I_0) = \\frac{1}{2} \\ln(1) = 0$$\nThis correctly reflects that no information is gained if no measurements are taken.\n\nAn alternative but equivalent form can be derived using the posterior covariance $\\Sigma_{\\text{post}}$. The information gain is the reduction in entropy from prior to posterior: $I(x; y_S) = H(x) - \\mathbb{E}_{y_S}[H(x|y_S)]$. The posterior precision is $\\Sigma_{\\text{post}}^{-1} = \\Sigma_{\\text{prior}}^{-1} + \\frac{1}{\\sigma^2} H_S^\\top H_S$. This leads to $f(S) = \\frac{1}{2} \\ln \\det (I_d + \\frac{1}{\\sigma^2} \\Sigma_{\\text{prior}} H_S^\\top H_S)$. By Sylvester's determinant identity, these two forms are equivalent. For implementation, we will use the first form, as the matrix size is $|S| \\times |S|$, which is typically smaller than $d \\times d$.\n\n### Part 2: Algorithm Implementation Strategy\n\nThe derived formula for $f(S)$ will be used to implement the required components.\n\n-   **`calculate_f` Function:** This function takes a set of sensor indices $S$ and the problem parameters. It constructs the matrix $H_S$ by stacking the corresponding sensor vectors $h_i^\\top$. It then computes $I_{|S|} + \\frac{1}{\\sigma^2} H_S \\Sigma_{\\text{prior}} H_S^\\top$ and returns half the natural logarithm of its determinant. `numpy.linalg.slogdet` is used for numerical stability.\n\n-   **Greedy Algorithm:** The algorithm iteratively builds a set $S_{\\text{greedy}}$ of size $k$. In each of the $k$ steps, it evaluates the marginal gain $f(S_{\\text{current}} \\cup \\{e\\}) - f(S_{\\text{current}})$ for every available sensor $e$ not in $S_{\\text{current}}$. The sensor providing the largest marginal gain is added to the set. Ties are broken by choosing the sensor with the smallest index. This is implemented by finding all gains, then selecting the best choice based on a primary sort on gain (descending) and a secondary sort on index (ascending).\n\n-   **Exhaustive Search:** This algorithm finds the true optimal set $S_{\\text{optimal}}$ of size $k$ by exploring all possible subsets of size $k$. `itertools.combinations` is used to generate all $\\binom{m}{k}$ subsets. For each subset, $f(S)$ is computed, and the subset maximizing this value is retained.\n\n-   **Submodularity Verification:** To verify the diminishing returns property, $f(A \\cup \\{e\\}) - f(A) \\ge f(B \\cup \\{e\\}) - f(B)$ for all $A \\subseteq B$ and $e \\notin B$, the program will iterate through all such triples $(A, B, e)$. For a universe of $m$ sensors, all subsets are generated. Then, for each subset $B$, all of its own subsets $A$ are generated. For each pair $(A,B)$, all elements $e$ not in $B$ are tested against the inequality, using the specified tolerance $\\varepsilon = 10^{-9}$.\n\nThe code will be structured to handle each test case by calling these generic functions with the specific parameters provided. The final results are collected and printed in the required format.", "answer": "```python\nimport numpy as np\nfrom itertools import combinations\nimport math\n\ndef calculate_f(S, sensors, Sigma_prior, sigma2):\n    \"\"\"\n    Computes the expected information gain f(S) for a given set of sensors S.\n    f(S) = 0.5 * ln(det(I + (1/sigma2) * H_S @ Sigma_prior @ H_S.T))\n    \n    Args:\n        S (tuple or list): A set of sensor indices.\n        sensors (list): A list of all candidate sensor vectors h_i.\n        Sigma_prior (np.ndarray): The prior covariance matrix.\n        sigma2 (float): The noise variance.\n\n    Returns:\n        float: The value of the objective function f(S).\n    \"\"\"\n    if not S:\n        return 0.0\n    \n    s_list = list(S)\n    num_sensors_in_set = len(s_list)\n    d = sensors[0].shape[0]\n    \n    H_S = np.zeros((num_sensors_in_set, d))\n    for i, sensor_idx in enumerate(s_list):\n        H_S[i, :] = sensors[sensor_idx]\n    \n    M = H_S @ Sigma_prior @ H_S.T\n    I = np.identity(num_sensors_in_set)\n    matrix_to_det = I + (1 / sigma2) * M\n    \n    sign, log_det = np.linalg.slogdet(matrix_to_det)\n    \n    if sign <= 0:\n        # This case implies the matrix is not positive definite, which shouldn't happen\n        # for this problem formulation, but is good practice to handle.\n        return -np.inf\n        \n    return 0.5 * log_det\n\ndef greedy_search(k, sensors, Sigma_prior, sigma2):\n    \"\"\"\n    Performs greedy sensor selection to maximize f(S) under cardinality constraint |S|=k.\n    \"\"\"\n    if k == 0:\n        return []\n\n    m = len(sensors)\n    all_sensor_indices = set(range(m))\n    S_greedy = set()\n    \n    for _ in range(k):\n        available_sensors = all_sensor_indices - S_greedy\n        \n        gains = []\n        current_f = calculate_f(tuple(S_greedy), sensors, Sigma_prior, sigma2)\n        \n        for e in available_sensors:\n            new_set = S_greedy.union({e})\n            gain = calculate_f(tuple(sorted(list(new_set))), sensors, Sigma_prior, sigma2) - current_f\n            gains.append((gain, e))\n        \n        # Sort by gain (descending), then by index (ascending) to handle ties\n        gains.sort(key=lambda x: (-x[0], x[1]))\n        \n        best_e = gains[0][1]\n        S_greedy.add(best_e)\n        \n    return sorted(list(S_greedy))\n\ndef exhaustive_search(k, sensors, Sigma_prior, sigma2):\n    \"\"\"\n    Finds the optimal sensor set of size k by checking all combinations.\n    \"\"\"\n    m = len(sensors)\n    all_sensor_indices = range(m)\n    \n    best_S = None\n    max_f = -np.inf\n\n    if k == 0:\n        return [], 0.0\n\n    for S_tuple in combinations(all_sensor_indices, k):\n        f_S = calculate_f(S_tuple, sensors, Sigma_prior, sigma2)\n        if f_S > max_f:\n            max_f = f_S\n            best_S = S_tuple\n            \n    return sorted(list(best_S)), max_f\n\ndef check_submodularity(sensors, Sigma_prior, sigma2, tol):\n    \"\"\"\n    Numerically verifies the submodularity property for the objective function f.\n    f(A U {e}) - f(A) >= f(B U {e}) - f(B) for A subset B, e not in B.\n    \"\"\"\n    m = len(sensors)\n    universe = set(range(m))\n    \n    all_subsets = []\n    for i in range(m + 1):\n        for subset in combinations(universe, i):\n            all_subsets.append(frozenset(subset))\n\n    for B in all_subsets:\n        subsets_of_B = []\n        B_list = list(B)\n        for i in range(len(B) + 1):\n            for subset in combinations(B_list, i):\n                subsets_of_B.append(frozenset(subset))\n        \n        elements_outside_B = universe - B\n        \n        for A in subsets_of_B:\n            # A is guaranteed to be a subset of B\n            f_A = calculate_f(tuple(A), sensors, Sigma_prior, sigma2)\n            f_B = calculate_f(tuple(B), sensors, Sigma_prior, sigma2)\n            \n            for e in elements_outside_B:\n                A_plus_e = A.union({e})\n                B_plus_e = B.union({e})\n                \n                f_A_plus_e = calculate_f(tuple(sorted(list(A_plus_e))), sensors, Sigma_prior, sigma2)\n                f_B_plus_e = calculate_f(tuple(sorted(list(B_plus_e))), sensors, Sigma_prior, sigma2)\n                \n                gain_A = f_A_plus_e - f_A\n                gain_B = f_B_plus_e - f_B\n                \n                if not (gain_A + tol >= gain_B):\n                    return False\n    return True\n\ndef solve():\n    \"\"\"\n    Solves all test cases and prints the final result.\n    \"\"\"\n    results = []\n\n    # Test Case 1 & 2\n    d1 = 3\n    Sigma_prior1 = np.identity(d1)\n    sigma2_1 = 0.10\n    sensors1 = [\n        np.array([1.0, 0.0, 0.0]),\n        np.array([1.0, 0.0, 0.0]),\n        np.array([0.0, 1.0, 0.0]),\n        np.array([0.0, 0.0, 1.0]),\n        np.array([1.0, 1.0, 0.0]),\n    ]\n    k1 = 2\n    \n    # Case 1: Greedy-optimal gap\n    S_greedy1 = greedy_search(k1, sensors1, Sigma_prior1, sigma2_1)\n    f_greedy1 = calculate_f(tuple(S_greedy1), sensors1, Sigma_prior1, sigma2_1)\n    \n    S_optimal1, f_optimal1 = exhaustive_search(k1, sensors1, Sigma_prior1, sigma2_1)\n    \n    g1 = f_greedy1 - f_optimal1\n    results.append(f\"{g1:.8f}\")\n\n    # Case 2: Submodularity verification\n    tol_submod = 1e-9\n    b2 = check_submodularity(sensors1, Sigma_prior1, sigma2_1, tol_submod)\n    results.append(b2)\n\n    # Test Case 3\n    d3 = 2\n    Sigma_prior3 = np.identity(d3)\n    sigma2_3 = 1.0\n    sensors3 = [\n        np.array([1.0, 0.0]),\n        np.array([0.0, 1.0]),\n        np.array([1.0, 1.0]),\n    ]\n    k3 = 0\n    tol_zero = 1e-12\n    \n    f_empty_val = calculate_f((), sensors3, Sigma_prior3, sigma2_3)\n    S_greedy3 = greedy_search(k3, sensors3, Sigma_prior3, sigma2_3)\n    \n    b3 = math.isclose(f_empty_val, 0.0, abs_tol=tol_zero) and (S_greedy3 == [])\n    results.append(b3)\n\n    # Test Case 4\n    d4 = 4\n    ell4 = 1.0\n    Sigma_prior4 = np.zeros((d4, d4))\n    for i in range(d4):\n        for j in range(d4):\n            Sigma_prior4[i,j] = np.exp(-((i-j)**2) / (2 * ell4**2))\n    \n    sigma2_4 = 0.50\n    sensors4 = [\n        np.array([1.0, 0.0, 0.0, 0.0]),\n        np.array([0.0, 1.0, 0.0, 0.0]),\n        np.array([0.0, 0.0, 1.0, 0.0]),\n        np.array([0.0, 0.0, 0.0, 1.0]),\n        np.array([1.0 / np.sqrt(2), 1.0 / np.sqrt(2), 0.0, 0.0]),\n        np.array([0.0, 1.0 / np.sqrt(2), 1.0 / np.sqrt(2), 0.0]),\n    ]\n    k4 = 3\n    \n    S_greedy4 = greedy_search(k4, sensors4, Sigma_prior4, sigma2_4)\n    results.append(S_greedy4)\n\n    # Format output\n    g1_str = results[0]\n    b2_str = str(results[1])\n    b3_str = str(results[2])\n    S_greedy4_str = str(results[3]).replace(\" \", \"\")\n    \n    print(f\"[{g1_str},{b2_str},{b3_str},{S_greedy4_str}]\")\n\nsolve()\n```", "id": "3367093"}]}