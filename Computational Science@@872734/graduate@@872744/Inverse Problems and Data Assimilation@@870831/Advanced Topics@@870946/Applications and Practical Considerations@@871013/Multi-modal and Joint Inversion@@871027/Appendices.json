{"hands_on_practices": [{"introduction": "Joint inversion begins with a fundamental question: how do we combine misfit terms from different physical measurements into a single objective function? A simple sum is often inadequate, as it fails to account for differences in units, measurement scales, and data quality. The statistically robust approach is to weight each data misfit term by the inverse of its error covariance matrix. This practice provides a concrete numerical example of this principle, demonstrating how proper covariance weighting normalizes the contributions of different data modalities to create a balanced and meaningful objective function. [@problem_id:3404765]", "problem": "Consider a joint inversion problem with two data modalities indexed by $i \\in \\{1,2\\}$. The forward maps are $F_1(m)$ and $F_2(m)$, composed with linear observation operators $H_1$ and $H_2$, yielding residuals $r_i(m) = H_i F_i(m) - d_i$. Assume additive, mean-zero Gaussian data errors for each modality with covariance matrices $C_1$ and $C_2$, respectively, that are known and positive definite. Under this assumption, the data misfit part of the joint objective is constructed using proper covariance weighting implied by the Gaussian error model. \n\nAt a particular candidate model $m^\\star$, suppose that the covariances are $C_1 = 9\\,I$ and $C_2 = I$, where $I$ denotes identity matrices of compatible dimensions, and that the Euclidean residual norms are $\\|r_1(m^\\star)\\|_2 = 3$ and $\\|r_2(m^\\star)\\|_2 = 1$. Using the Gaussian-noise-based construction of the data misfit objective with a common scalar prefactor across modalities, compute the relative contributions of each modality to the total data misfit at $m^\\star$, namely the fractions $J_1(m^\\star)/J_{\\mathrm{data}}(m^\\star)$ and $J_2(m^\\star)/J_{\\mathrm{data}}(m^\\star)$, where $J_{\\mathrm{data}}(m^\\star) = J_1(m^\\star) + J_2(m^\\star)$ denotes the total data misfit.\n\nReport your final answer as a two-entry row vector containing the two relative contributions in that order. No units are required. No rounding is necessary.", "solution": "The problem requires the calculation of the relative contributions of two data modalities to a total data misfit objective function at a specific model candidate, $m^\\star$. The construction of the objective function is based on the assumption of additive, mean-zero Gaussian errors with known covariance matrices.\n\nThe probability density function for the data $d_i$ for modality $i$, given a model $m$, is based on the Gaussian distribution of the error term, which is identified with the residual $r_i(m) = H_i F_i(m) - d_i$. The likelihood function for modality $i$ is given by:\n$$L_i(m) \\propto \\exp\\left(-\\frac{1}{2} r_i(m)^T C_i^{-1} r_i(m)\\right)$$\nwhere $C_i$ is the data error covariance matrix.\n\nFor a joint inversion problem with two statistically independent data modalities, the total likelihood is the product of the individual likelihoods:\n$$L_{\\mathrm{total}}(m) = L_1(m) \\cdot L_2(m) \\propto \\exp\\left(-\\frac{1}{2} r_1(m)^T C_1^{-1} r_1(m) - \\frac{1}{2} r_2(m)^T C_2^{-1} r_2(m)\\right)$$\n\nThe data misfit objective function, $J_{\\mathrm{data}}(m)$, is typically derived from the negative log-likelihood, ignoring any constant terms that do not depend on the model $m$. This yields a sum of quadratic forms:\n$$J_{\\mathrm{data}}(m) = \\frac{1}{2} r_1(m)^T C_1^{-1} r_1(m) + \\frac{1}{2} r_2(m)^T C_2^{-1} r_2(m)$$\nThe problem statement notes \"a common scalar prefactor\", which is consistent with the factor of $\\frac{1}{2}$ appearing in both terms. We can define the individual misfit contributions as:\n$$J_1(m) = \\frac{1}{2} r_1(m)^T C_1^{-1} r_1(m)$$\n$$J_2(m) = \\frac{1}{2} r_2(m)^T C_2^{-1} r_2(m)$$\nsuch that $J_{\\mathrm{data}}(m) = J_1(m) + J_2(m)$.\n\nWe are asked to evaluate these contributions at a specific model $m^\\star$. The givens for this evaluation are:\n- Covariance for modality $1$: $C_1 = 9\\,I$\n- Covariance for modality $2$: $C_2 = I$\n- Residual norm for modality $1$: $\\|r_1(m^\\star)\\|_2 = 3$\n- Residual norm for modality $2$: $\\|r_2(m^\\star)\\|_2 = 1$\n\nFirst, we find the inverse covariance matrices:\n$$C_1^{-1} = (9\\,I)^{-1} = \\frac{1}{9}I^{-1} = \\frac{1}{9}I$$\n$$C_2^{-1} = I^{-1} = I$$\n\nNow, we compute the value of $J_1(m^\\star)$. We substitute $C_1^{-1}$ into the expression for $J_1(m)$:\n$$J_1(m^\\star) = \\frac{1}{2} r_1(m^\\star)^T \\left(\\frac{1}{9}I\\right) r_1(m^\\star)$$\nThe term $r_1(m^\\star)^T I r_1(m^\\star)$ is equivalent to the dot product $r_1(m^\\star) \\cdot r_1(m^\\star)$, which is the squared Euclidean norm, $\\|r_1(m^\\star)\\|_2^2$.\n$$J_1(m^\\star) = \\frac{1}{2} \\cdot \\frac{1}{9} \\|r_1(m^\\star)\\|_2^2 = \\frac{1}{18} \\|r_1(m^\\star)\\|_2^2$$\nSubstituting the given value $\\|r_1(m^\\star)\\|_2 = 3$:\n$$J_1(m^\\star) = \\frac{1}{18} (3^2) = \\frac{9}{18} = \\frac{1}{2}$$\n\nNext, we compute the value of $J_2(m^\\star)$. We substitute $C_2^{-1}$ into the expression for $J_2(m)$:\n$$J_2(m^\\star) = \\frac{1}{2} r_2(m^\\star)^T (I) r_2(m^\\star)$$\nThis simplifies to:\n$$J_2(m^\\star) = \\frac{1}{2} \\|r_2(m^\\star)\\|_2^2$$\nSubstituting the given value $\\|r_2(m^\\star)\\|_2 = 1$:\n$$J_2(m^\\star) = \\frac{1}{2} (1^2) = \\frac{1}{2}$$\n\nThe total data misfit at $m^\\star$ is the sum of the individual contributions:\n$$J_{\\mathrm{data}}(m^\\star) = J_1(m^\\star) + J_2(m^\\star) = \\frac{1}{2} + \\frac{1}{2} = 1$$\n\nFinally, we compute the relative contributions of each modality to the total data misfit.\nThe relative contribution of modality $1$ is:\n$$\\frac{J_1(m^\\star)}{J_{\\mathrm{data}}(m^\\star)} = \\frac{1/2}{1} = \\frac{1}{2}$$\nThe relative contribution of modality $2$ is:\n$$\\frac{J_2(m^\\star)}{J_{\\mathrm{data}}(m^\\star)} = \\frac{1/2}{1} = \\frac{1}{2}$$\n\nThe problem asks for the answer as a two-entry row vector containing these two fractions in order. The result demonstrates the principle of inverse-covariance weighting: the raw residual for modality $1$ is larger, but because its associated data errors have a larger variance ($9$ compared to $1$), its contribution to the objective function is down-weighted accordingly, resulting in an equal contribution from both modalities at the point $m^\\star$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix}}$$", "id": "3404765"}, {"introduction": "Once a joint objective function is defined, we need an efficient method to find the model $m$ that minimizes it, especially for high-dimensional problems. Gradient-based optimization is the standard approach, which requires computing the derivative of the objective functional with respect to the model parameters. This exercise guides you through the derivation of this gradient for a general multi-modal objective function using the powerful adjoint-state method, a computationally essential technique that reveals how each data modality contributes to the direction of model updates. [@problem_id:3404713]", "problem": "Consider a joint inversion setting with multiple data modalities indexed by $k \\in \\{1,\\dots,K\\}$. Let $\\mathcal{M}$ be a Hilbert space of model parameters $m \\in \\mathcal{M}$. For each modality $k$, let $\\mathcal{U}_k$ denote a Hilbert space of forward-model states and $\\mathcal{Y}_k$ a Hilbert space of data. Assume:\n- A differentiable forward map $F_k:\\mathcal{M} \\to \\mathcal{U}_k$ with Fréchet derivative $F_k^{\\prime}(m):\\mathcal{M} \\to \\mathcal{U}_k$.\n- A bounded linear observation map $H_k:\\mathcal{U}_k \\to \\mathcal{Y}_k$ with Hilbert-adjoint $H_k^{*}:\\mathcal{Y}_k \\to \\mathcal{U}_k$.\n- A symmetric positive-definite data misfit weight (noise covariance) $C_k:\\mathcal{Y}_k \\to \\mathcal{Y}_k$ with $C_k^{-1}$ and $C_k^{-1/2}$ well defined and bounded.\n- Observed data $d_k \\in \\mathcal{Y}_k$.\n\nDefine the joint objective functional by\n$$\nJ(m) \\;=\\; \\frac{1}{2} \\sum_{k=1}^{K} \\big\\|\\, C_k^{-1/2} \\big(H_k F_k(m) - d_k\\big) \\big\\|_{\\mathcal{Y}_k}^{2}.\n$$\nAssume all spaces are real Hilbert spaces with their canonical inner products, and that adjoints are taken with respect to these inner products. The Gâteaux derivative of $J$ in the direction $\\delta m \\in \\mathcal{M}$ is defined by\n$$\n\\delta J[m;\\delta m] \\;=\\; \\left.\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon}\\, J(m+\\epsilon\\,\\delta m)\\right|_{\\epsilon=0}.\n$$\n\nStarting strictly from the definitions of the Gâteaux derivative, the chain rule for Fréchet derivatives, properties of Hilbert space adjoints, and the polarization identity relating norms and inner products, derive an adjoint-state expression for the Gâteaux derivative $\\delta J[m;\\delta m]$ that isolates $\\delta m$ in a single inner product on $\\mathcal{M}$. In your derivation, identify explicitly the adjoint sources for each modality in terms of the residuals and the given operators. Your final expression for $\\delta J[m;\\delta m]$ must be written in terms of $F_k^{\\prime}(m)^{*}$, $H_k^{*}$, $C_k^{-1}$, $H_kF_k(m)$, and $d_k$ only, with all operators and residuals clearly combined into a single inner product on $\\mathcal{M}$.\n\nYour final answer must be a single closed-form analytic expression for $\\delta J[m;\\delta m]$. No numerical evaluation is required.", "solution": "The problem statement provides a well-posed mathematical question within the field of inverse problems. All terms are clearly defined, the premises are scientifically and mathematically sound, and the goal is to perform a standard derivation using the provided definitions and properties. The problem is therefore valid.\n\nWe are tasked with deriving an adjoint-state expression for the Gâteaux derivative of the joint objective functional $J(m)$. The functional is given by\n$$\nJ(m) = \\frac{1}{2} \\sum_{k=1}^{K} \\big\\|\\, C_k^{-1/2} \\big(H_k F_k(m) - d_k\\big) \\big\\|_{\\mathcal{Y}_k}^{2}\n$$\nwhere $m \\in \\mathcal{M}$ is the model parameter, and for each modality $k$, $F_k$ is the forward model, $H_k$ is the observation operator, $d_k \\in \\mathcal{Y}_k$ is the observed data, and $C_k$ is the symmetric positive-definite data misfit weight, with $\\mathcal{M}$ and $\\mathcal{Y}_k$ being real Hilbert spaces.\n\nFirst, we express the norm-squared term using the inner product on the Hilbert space $\\mathcal{Y}_k$. For any vector $y \\in \\mathcal{Y}_k$, the norm is defined by $\\|y\\|_{\\mathcal{Y}_k}^2 = \\langle y, y \\rangle_{\\mathcal{Y}_k}$.\nApplying this to the objective functional gives:\n$$\nJ(m) = \\frac{1}{2} \\sum_{k=1}^{K} \\big\\langle C_k^{-1/2} (H_k F_k(m) - d_k), C_k^{-1/2} (H_k F_k(m) - d_k) \\big\\rangle_{\\mathcal{Y}_k}\n$$\nThe operator $C_k$ is given as symmetric positive-definite, which implies that its inverse $C_k^{-1}$ and square root $C_k^{-1/2}$ are also symmetric positive-definite and thus self-adjoint in the real Hilbert space $\\mathcal{Y}_k$. That is, $(C_k^{-1/2})^{*} = C_k^{-1/2}$. Using the definition of the adjoint operator $\\langle u, Av \\rangle = \\langle A^*u, v \\rangle$, we can move one $C_k^{-1/2}$ operator to the first argument of the inner product:\n$$\nJ(m) = \\frac{1}{2} \\sum_{k=1}^{K} \\big\\langle (C_k^{-1/2})^{*} C_k^{-1/2} (H_k F_k(m) - d_k), (H_k F_k(m) - d_k) \\big\\rangle_{\\mathcal{Y}_k}\n$$\n$$\nJ(m) = \\frac{1}{2} \\sum_{k=1}^{K} \\big\\langle C_k^{-1} (H_k F_k(m) - d_k), (H_k F_k(m) - d_k) \\big\\rangle_{\\mathcal{Y}_k}\n$$\nFor convenience, let the data residual for modality $k$ be denoted by $r_k(m) = H_k F_k(m) - d_k$. The functional becomes:\n$$\nJ(m) = \\frac{1}{2} \\sum_{k=1}^{K} \\langle C_k^{-1} r_k(m), r_k(m) \\rangle_{\\mathcal{Y}_k}\n$$\n\nThe Gâteaux derivative of $J$ at $m$ in the direction $\\delta m \\in \\mathcal{M}$ is defined as:\n$$\n\\delta J[m;\\delta m] = \\left.\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon}\\, J(m+\\epsilon\\,\\delta m)\\right|_{\\epsilon=0}\n$$\nSubstituting the inner product form of $J$:\n$$\n\\delta J[m;\\delta m] = \\left.\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon}\\, \\left( \\frac{1}{2} \\sum_{k=1}^{K} \\langle C_k^{-1} r_k(m+\\epsilon\\,\\delta m), r_k(m+\\epsilon\\,\\delta m) \\rangle_{\\mathcal{Y}_k} \\right) \\right|_{\\epsilon=0}\n$$\nDue to the linearity of the derivative and the finite sum, we can interchange them:\n$$\n\\delta J[m;\\delta m] = \\frac{1}{2} \\sum_{k=1}^{K} \\left.\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon}\\, \\langle C_k^{-1} r_k(m+\\epsilon\\,\\delta m), r_k(m+\\epsilon\\,\\delta m) \\rangle_{\\mathcal{Y}_k} \\right|_{\\epsilon=0}\n$$\nWe apply the product rule for differentiating an inner product $\\langle u(\\epsilon), v(\\epsilon) \\rangle$:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} \\langle u, v \\rangle = \\langle \\frac{\\mathrm{d}u}{\\mathrm{d}\\epsilon}, v \\rangle + \\langle u, \\frac{\\mathrm{d}v}{\\mathrm{d}\\epsilon} \\rangle\n$$\nHere, $u(\\epsilon) = C_k^{-1} r_k(m+\\epsilon\\,\\delta m)$ and $v(\\epsilon) = r_k(m+\\epsilon\\,\\delta m)$. The operator $C_k^{-1}$ is constant with respect to $\\epsilon$. Let $r_k'(\\epsilon)$ denote $\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} r_k(m+\\epsilon\\,\\delta m)$. The derivative of the inner product term is:\n$$\n\\langle C_k^{-1} r_k'(\\epsilon), r_k(m+\\epsilon\\,\\delta m) \\rangle_{\\mathcal{Y}_k} + \\langle C_k^{-1} r_k(m+\\epsilon\\,\\delta m), r_k'(\\epsilon) \\rangle_{\\mathcal{Y}_k}\n$$\nSince we are in a real Hilbert space, the inner product is symmetric ($\\langle a,b \\rangle = \\langle b,a \\rangle$). Furthermore, $C_k^{-1}$ is self-adjoint. Thus, the second term can be rewritten as:\n$$\n\\langle C_k^{-1} r_k(m+\\epsilon\\,\\delta m), r_k'(\\epsilon) \\rangle_{\\mathcal{Y}_k} = \\langle r_k'(\\epsilon), (C_k^{-1})^{*} r_k(m+\\epsilon\\,\\delta m) \\rangle_{\\mathcal{Y}_k} = \\langle r_k'(\\epsilon), C_k^{-1} r_k(m+\\epsilon\\,\\delta m) \\rangle_{\\mathcal{Y}_k}\n$$\nThe two terms are identical. The sum is therefore:\n$$\n2 \\langle C_k^{-1} r_k'(\\epsilon), r_k(m+\\epsilon\\,\\delta m) \\rangle_{\\mathcal{Y}_k}\n$$\nSubstituting this back into the expression for $\\delta J[m;\\delta m]$ yields:\n$$\n\\delta J[m;\\delta m] = \\sum_{k=1}^{K} \\left. \\langle C_k^{-1} \\left(\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} r_k(m+\\epsilon\\,\\delta m)\\right), r_k(m+\\epsilon\\,\\delta m) \\rangle_{\\mathcal{Y}_k} \\right|_{\\epsilon=0}\n$$\nEvaluating at $\\epsilon=0$:\n$$\n\\delta J[m;\\delta m] = \\sum_{k=1}^{K} \\langle C_k^{-1} \\left( \\left.\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} r_k(m+\\epsilon\\,\\delta m)\\right|_{\\epsilon=0} \\right), r_k(m) \\rangle_{\\mathcal{Y}_k}\n$$\nNext, we compute the derivative of the residual $r_k(m+\\epsilon\\,\\delta m) = H_k F_k(m+\\epsilon\\,\\delta m) - d_k$. Using the chain rule for Fréchet derivatives, and noting that $H_k$ is a bounded linear operator and $d_k$ is constant:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} r_k(m+\\epsilon\\,\\delta m) = H_k \\left( \\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} F_k(m+\\epsilon\\,\\delta m) \\right) = H_k \\left( F_k^{\\prime}(m+\\epsilon\\,\\delta m)[\\delta m] \\right)\n$$\nEvaluating at $\\epsilon=0$, we get the Gâteaux derivative of the composite forward operator $H_k F_k$:\n$$\n\\left.\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} r_k(m+\\epsilon\\,\\delta m)\\right|_{\\epsilon=0} = H_k (F_k^{\\prime}(m)[\\delta m])\n$$\nSubstituting this result into the expression for $\\delta J[m;\\delta m]$:\n$$\n\\delta J[m;\\delta m] = \\sum_{k=1}^{K} \\langle C_k^{-1} (H_k F_k^{\\prime}(m)[\\delta m]), H_k F_k(m) - d_k \\rangle_{\\mathcal{Y}_k}\n$$\nThe problem requires an expression that isolates $\\delta m$ in a single inner product on $\\mathcal{M}$. We achieve this by systematically applying the definition of the Hilbert-adjoint of an operator. For any linear operator $A:X \\to Y$ between Hilbert spaces $X, Y$, its adjoint $A^*:Y \\to X$ satisfies $\\langle Ax, y \\rangle_Y = \\langle x, A^*y \\rangle_X$ for all $x \\in X, y \\in Y$.\nFirst, we use the self-adjoint property of $C_k^{-1}$:\n$$\n\\delta J[m;\\delta m] = \\sum_{k=1}^{K} \\langle H_k F_k^{\\prime}(m)[\\delta m], C_k^{-1} (H_k F_k(m) - d_k) \\rangle_{\\mathcal{Y}_k}\n$$\nThe operator acting on $\\delta m$ is the composition $H_k F_k^{\\prime}(m)$, which maps $\\mathcal{M} \\to \\mathcal{Y}_k$. Its adjoint is $(H_k F_k^{\\prime}(m))^* = (F_k^{\\prime}(m))^* H_k^*$, which maps $\\mathcal{Y}_k \\to \\mathcal{M}$. Moving this composite operator to the other side of the inner product:\n$$\n\\delta J[m;\\delta m] = \\sum_{k=1}^{K} \\langle \\delta m, (H_k F_k^{\\prime}(m))^* [C_k^{-1} (H_k F_k(m) - d_k)] \\rangle_{\\mathcal{M}}\n$$\n$$\n\\delta J[m;\\delta m] = \\sum_{k=1}^{K} \\langle \\delta m, (F_k^{\\prime}(m))^* H_k^* [C_k^{-1} (H_k F_k(m) - d_k)] \\rangle_{\\mathcal{M}}\n$$\nBy linearity of the inner product in its second argument, we can bring the summation inside:\n$$\n\\delta J[m;\\delta m] = \\bigg\\langle \\delta m, \\sum_{k=1}^{K} (F_k^{\\prime}(m))^* H_k^* [C_k^{-1} (H_k F_k(m) - d_k)] \\bigg\\rangle_{\\mathcal{M}}\n$$\nFinally, since we are in a real Hilbert space, the inner product is symmetric. We write the expression in the conventional form $\\langle \\nabla J(m), \\delta m \\rangle_{\\mathcal{M}}$:\n$$\n\\delta J[m;\\delta m] = \\bigg\\langle \\sum_{k=1}^{K} (F_k^{\\prime}(m))^{*} H_k^{*} C_k^{-1} (H_k F_k(m) - d_k), \\delta m \\bigg\\rangle_{\\mathcal{M}}\n$$\nThis is the final adjoint-state expression for the Gâteaux derivative. The term $s_k = C_k^{-1} (H_k F_k(m) - d_k) \\in \\mathcal{Y}_k$ is the weighted residual, which serves as the \"adjoint source\" for modality $k$. The adjoint operators $(F_k^{\\prime}(m))^{*} H_k^{*}$ propagate this source from the data space $\\mathcal{Y}_k$ back to the model space $\\mathcal{M}$ to form the gradient of the objective functional.", "answer": "$$\n\\boxed{\\bigg\\langle \\sum_{k=1}^{K} (F_k^{\\prime}(m))^{*} H_k^{*} C_k^{-1} \\big(H_k F_k(m) - d_k\\big), \\delta m \\bigg\\rangle_{\\mathcal{M}}}\n$$", "id": "3404713"}, {"introduction": "While many real-world inverse problems are nonlinear and require iterative optimization, the linear-Gaussian case offers profound insights and admits a closed-form analytical solution. This problem frames joint inversion as a data assimilation task, where a prior belief about a system state $x$ is updated using new observations from multiple sensors. By working through a concrete numerical example, you will see how Bayesian principles translate into matrix operations and how the celebrated Kalman gain emerges as the optimal mechanism for fusing prior knowledge with new data. [@problem_id:3404715]", "problem": "In a joint inversion setting for two complementary sensing modalities observing a common latent geophysical state, assume a linear-Gaussian model. Let the latent state be $x \\in \\mathbb{R}^{2}$ with Gaussian prior $p(x) = \\mathcal{N}(x_{0}, P_{0})$, where $x_{0} \\in \\mathbb{R}^{2}$ and $P_{0} \\in \\mathbb{R}^{2 \\times 2}$ are given. Two independent sensing modalities produce observations $y_{1} \\in \\mathbb{R}$ and $y_{2} \\in \\mathbb{R}$ with linear models $y_{1} = H_{1} x + v_{1}$ and $y_{2} = H_{2} x + v_{2}$, where $v_{1} \\sim \\mathcal{N}(0, R_{1})$, $v_{2} \\sim \\mathcal{N}(0, R_{2})$, and $v_{1}$ and $v_{2}$ are independent conditioned on $x$. Define the stacked observation model $y = \\begin{pmatrix} y_{1} \\\\ y_{2} \\end{pmatrix} = H x + v$ with $H = \\begin{pmatrix} H_{1} \\\\ H_{2} \\end{pmatrix}$, $v = \\begin{pmatrix} v_{1} \\\\ v_{2} \\end{pmatrix}$, and block-diagonal noise covariance $R = \\begin{pmatrix} R_{1}  0 \\\\ 0  R_{2} \\end{pmatrix}$. Starting from Bayes’ rule and the properties of multivariate Gaussian distributions, derive from first principles the closed-form expression for the posterior mean $\\hat{x}$ and the corresponding optimal linear update gain (often termed the Kalman gain in linear-Gaussian settings) that fuses both modalities in the stacked form.\n\nThen, evaluate your expressions for the following numerically specified instance:\n- Prior mean $x_{0} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$ and prior covariance $P_{0} = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix}$.\n- Observation operators $H_{1} = \\begin{pmatrix} 1  2 \\end{pmatrix}$ and $H_{2} = \\begin{pmatrix} 2  -1 \\end{pmatrix}$, hence $H = \\begin{pmatrix} 1  2 \\\\ 2  -1 \\end{pmatrix}$.\n- Noise covariances $R_{1} = 1$ and $R_{2} = 4$, hence $R = \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix}$.\n- Observations $y_{1} = 5$ and $y_{2} = -1$, hence $y = \\begin{pmatrix} 5 \\\\ -1 \\end{pmatrix}$.\n\nProvide the exact joint gain matrix and the updated posterior mean as rational numbers. Do not round. The final answer must be given as a single expression containing both the gain matrix and the updated mean, with no units, and expressed exactly (no approximation or rounding).", "solution": "The problem requires the derivation of the posterior mean and the optimal linear update gain for a linear-Gaussian system, followed by a numerical evaluation. The derivation will start from Bayes' rule.\n\n### Part 1: Derivation from First Principles\n\nAccording to Bayes' rule, the posterior probability density function (PDF) $p(x|y)$ is proportional to the product of the likelihood $p(y|x)$ and the prior $p(x)$:\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\nThe problem states a Gaussian prior for the latent state $x \\in \\mathbb{R}^2$:\n$$\np(x) = \\mathcal{N}(x_0, P_0) \\propto \\exp\\left(-\\frac{1}{2}(x - x_0)^T P_0^{-1} (x - x_0)\\right)\n$$\nThe observation model is linear with additive Gaussian noise, $y = Hx + v$, where $v \\sim \\mathcal{N}(0, R)$. This defines the likelihood function $p(y|x)$ as a Gaussian distribution for $y$ with mean $Hx$ and covariance $R$:\n$$\np(y|x) = \\mathcal{N}(Hx, R) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\\right)\n$$\nCombining these, the posterior PDF is:\n$$\np(x|y) \\propto \\exp\\left(-\\frac{1}{2}(x - x_0)^T P_0^{-1} (x - x_0)\\right) \\exp\\left(-\\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\\right)\n$$\n$$\np(x|y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ (x - x_0)^T P_0^{-1} (x - x_0) + (y - Hx)^T R^{-1} (y - Hx) \\right]\\right)\n$$\nThe term in the exponent, which we denote as the cost function $J(x)$, is a quadratic function of $x$. This implies that the posterior distribution is also Gaussian, say $p(x|y) = \\mathcal{N}(\\hat{x}, \\hat{P})$, which has a PDF proportional to $\\exp\\left(-\\frac{1}{2}(x-\\hat{x})^T \\hat{P}^{-1} (x-\\hat{x})\\right)$. To find the posterior mean $\\hat{x}$ and covariance $\\hat{P}$, we expand $J(x)$ and collect terms in powers of $x$.\n\n$J(x) = (x^T P_0^{-1} x - 2x^T P_0^{-1} x_0 + x_0^T P_0^{-1} x_0) + ((Hx)^T R^{-1} (Hx) - 2(Hx)^T R^{-1} y + y^T R^{-1} y)$\n$J(x) = x^T P_0^{-1} x - 2x^T P_0^{-1} x_0 + x^T H^T R^{-1} H x - 2x^T H^T R^{-1} y + (\\text{terms not dependent on } x)$\nGrouping terms:\n$J(x) = x^T (P_0^{-1} + H^T R^{-1} H) x - 2x^T (P_0^{-1} x_0 + H^T R^{-1} y) + \\text{const.}$\n\nThe canonical form for the exponent of a Gaussian $\\mathcal{N}(\\hat{x}, \\hat{P})$ is $(x - \\hat{x})^T \\hat{P}^{-1} (x - \\hat{x}) = x^T \\hat{P}^{-1} x - 2x^T \\hat{P}^{-1} \\hat{x} + \\hat{x}^T \\hat{P}^{-1} \\hat{x}$. By comparing the quadratic and linear terms in $x$ with $J(x)$, we can identify the posterior inverse covariance $\\hat{P}^{-1}$ and the posterior mean $\\hat{x}$:\n\n1.  From the quadratic term ($x^T(\\dots)x$):\n    $$\n    \\hat{P}^{-1} = P_0^{-1} + H^T R^{-1} H\n    $$\n    So the posterior covariance is $\\hat{P} = (P_0^{-1} + H^T R^{-1} H)^{-1}$.\n\n2.  From the linear term ($-2x^T(\\dots)$):\n    $$\n    \\hat{P}^{-1} \\hat{x} = P_0^{-1} x_0 + H^T R^{-1} y\n    $$\n    Solving for the posterior mean $\\hat{x}$:\n    $$\n    \\hat{x} = (\\hat{P}^{-1})^{-1} (P_0^{-1} x_0 + H^T R^{-1} y) = \\hat{P} (P_0^{-1} x_0 + H^T R^{-1} y)\n    $$\nTo derive the update form involving a gain matrix, we use the matrix inversion lemma (specifically, the Woodbury identity) to express $\\hat{P}$ in a different form:\n$\\hat{P} = P_0 - P_0 H^T (H P_0 H^T + R)^{-1} H P_0$.\nLet's define the optimal linear update gain matrix $K$ (also known as the Kalman gain) as:\n$$\nK = P_0 H^T (H P_0 H^T + R)^{-1}\n$$\nWith this definition, the posterior covariance becomes $\\hat{P} = P_0 - K H P_0 = (I - KH)P_0$.\n\nNow we rearrange the expression for $\\hat{x}$ to match the update form $\\hat{x} = x_0 + K(y - Hx_0)$.\nStarting from $I = \\hat{P} \\hat{P}^{-1} = \\hat{P} (P_0^{-1} + H^T R^{-1} H) = \\hat{P} P_0^{-1} + \\hat{P} H^T R^{-1} H$, we can write $\\hat{P}P_0^{-1} = I - \\hat{P} H^T R^{-1} H$.\nAn alternative expression for the gain $K$ is $K = \\hat{P} H^T R^{-1}$.\nSubstituting this gives $\\hat{P}P_0^{-1} = I - K H$.\nNow, we rewrite the posterior mean expression:\n$\\hat{x} = \\hat{P} (P_0^{-1} x_0 + H^T R^{-1} y) = (\\hat{P} P_0^{-1}) x_0 + (\\hat{P} H^T R^{-1}) y$\nSubstituting the identities we just found:\n$\\hat{x} = (I - K H) x_0 + K y = x_0 - K H x_0 + K y$\n$$\n\\hat{x} = x_0 + K(y - Hx_0)\n$$\nThis is the desired closed-form expression for the posterior mean. The residual term $(y - Hx_0)$ is called the innovation. The gain $K$ optimally blends this new information with the prior estimate $x_0$.\n\n### Part 2: Numerical Evaluation\n\nWe are given the following numerical values:\n- Prior mean: $x_{0} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$\n- Prior covariance: $P_{0} = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix}$\n- Observation operator: $H = \\begin{pmatrix} 1  2 \\\\ 2  -1 \\end{pmatrix}$\n- Noise covariance: $R = \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix}$\n- Observation vector: $y = \\begin{pmatrix} 5 \\\\ -1 \\end{pmatrix}$\n\nWe will compute the gain $K = P_0 H^T (H P_0 H^T + R)^{-1}$ and the posterior mean $\\hat{x} = x_0 + K(y - Hx_0)$.\n\n1.  **Compute the innovation covariance $S = H P_0 H^T + R$:**\n    First, calculate $P_0 H^T$:\n    $$\n    P_0 H^T = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1  2 \\\\ 2  -1 \\end{pmatrix}^T = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1  2 \\\\ 2  -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 1 + 1 \\cdot 2  2 \\cdot 2 + 1 \\cdot (-1) \\\\ 1 \\cdot 1 + 3 \\cdot 2  1 \\cdot 2 + 3 \\cdot (-1) \\end{pmatrix} = \\begin{pmatrix} 4  3 \\\\ 7  -1 \\end{pmatrix}\n    $$\n    Next, calculate $H P_0 H^T$:\n    $$\n    H (P_0 H^T) = \\begin{pmatrix} 1  2 \\\\ 2  -1 \\end{pmatrix} \\begin{pmatrix} 4  3 \\\\ 7  -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 4 + 2 \\cdot 7  1 \\cdot 3 + 2 \\cdot (-1) \\\\ 2 \\cdot 4 + (-1) \\cdot 7  2 \\cdot 3 + (-1) \\cdot (-1) \\end{pmatrix} = \\begin{pmatrix} 18  1 \\\\ 1  7 \\end{pmatrix}\n    $$\n    Now, add $R$:\n    $$\n    S = H P_0 H^T + R = \\begin{pmatrix} 18  1 \\\\ 1  7 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix} = \\begin{pmatrix} 19  1 \\\\ 1  11 \\end{pmatrix}\n    $$\n\n2.  **Compute $S^{-1}$:**\n    The determinant of $S$ is $\\det(S) = (19)(11) - (1)(1) = 209 - 1 = 208$.\n    The inverse is:\n    $$\n    S^{-1} = \\frac{1}{208} \\begin{pmatrix} 11  -1 \\\\ -1  19 \\end{pmatrix}\n    $$\n\n3.  **Compute the gain matrix $K = P_0 H^T S^{-1}$:**\n    $$\n    K = \\begin{pmatrix} 4  3 \\\\ 7  -1 \\end{pmatrix} \\frac{1}{208} \\begin{pmatrix} 11  -1 \\\\ -1  19 \\end{pmatrix} = \\frac{1}{208} \\begin{pmatrix} 4 \\cdot 11 + 3 \\cdot (-1)  4 \\cdot (-1) + 3 \\cdot 19 \\\\ 7 \\cdot 11 + (-1) \\cdot (-1)  7 \\cdot (-1) + (-1) \\cdot 19 \\end{pmatrix}\n    $$\n    $$\n    K = \\frac{1}{208} \\begin{pmatrix} 44 - 3  -4 + 57 \\\\ 77 + 1  -7 - 19 \\end{pmatrix} = \\frac{1}{208} \\begin{pmatrix} 41  53 \\\\ 78  -26 \\end{pmatrix}\n    $$\n    Simplifying the fractions:\n    $$\n    K = \\begin{pmatrix} \\frac{41}{208}  \\frac{53}{208} \\\\ \\frac{78}{208}  \\frac{-26}{208} \\end{pmatrix} = \\begin{pmatrix} \\frac{41}{208}  \\frac{53}{208} \\\\ \\frac{3}{8}  -\\frac{1}{8} \\end{pmatrix}\n    $$\n\n4.  **Compute the posterior mean $\\hat{x} = x_0 + K(y - Hx_0)$:**\n    First, compute the innovation $(y - Hx_0)$:\n    $$\n    H x_0 = \\begin{pmatrix} 1  2 \\\\ 2  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 2 \\cdot (-2) \\\\ 2 \\cdot 1 + (-1) \\cdot (-2) \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ 4 \\end{pmatrix}\n    $$\n    $$\n    y - Hx_0 = \\begin{pmatrix} 5 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} -3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ -5 \\end{pmatrix}\n    $$\n    Next, compute the update term $K(y - Hx_0)$:\n    $$\n    K(y - Hx_0) = \\begin{pmatrix} \\frac{41}{208}  \\frac{53}{208} \\\\ \\frac{3}{8}  -\\frac{1}{8} \\end{pmatrix} \\begin{pmatrix} 8 \\\\ -5 \\end{pmatrix} = \\begin{pmatrix} \\frac{41}{208} \\cdot 8 + \\frac{53}{208} \\cdot (-5) \\\\ \\frac{3}{8} \\cdot 8 + (-\\frac{1}{8}) \\cdot (-5) \\end{pmatrix} = \\begin{pmatrix} \\frac{328 - 265}{208} \\\\ 3 + \\frac{5}{8} \\end{pmatrix} = \\begin{pmatrix} \\frac{63}{208} \\\\ \\frac{29}{8} \\end{pmatrix}\n    $$\n    Finally, compute the posterior mean $\\hat{x}$:\n    $$\n    \\hat{x} = x_0 + K(y - Hx_0) = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} + \\begin{pmatrix} \\frac{63}{208} \\\\ \\frac{29}{8} \\end{pmatrix} = \\begin{pmatrix} \\frac{208}{208} + \\frac{63}{208} \\\\ -\\frac{16}{8} + \\frac{29}{8} \\end{pmatrix} = \\begin{pmatrix} \\frac{271}{208} \\\\ \\frac{13}{8} \\end{pmatrix}\n    $$\n\nThe joint gain matrix is $K = \\begin{pmatrix} \\frac{41}{208}  \\frac{53}{208} \\\\ \\frac{3}{8}  -\\frac{1}{8} \\end{pmatrix}$ and the updated posterior mean is $\\hat{x} = \\begin{pmatrix} \\frac{271}{208} \\\\ \\frac{13}{8} \\end{pmatrix}$.\nThe problem asks to present the result as a single expression. A common way to do this is to form a block matrix where the first two columns represent the gain matrix and the third column represents the posterior mean vector.\n$$\n\\begin{pmatrix}\nK  \\hat{x}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{41}{208}  \\frac{53}{208}  \\frac{271}{208} \\\\\n\\frac{3}{8}  -\\frac{1}{8}  \\frac{13}{8}\n\\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{41}{208}  \\frac{53}{208}  \\frac{271}{208} \\\\\n\\frac{3}{8}  -\\frac{1}{8}  \\frac{13}{8}\n\\end{pmatrix}\n}\n$$", "id": "3404715"}]}