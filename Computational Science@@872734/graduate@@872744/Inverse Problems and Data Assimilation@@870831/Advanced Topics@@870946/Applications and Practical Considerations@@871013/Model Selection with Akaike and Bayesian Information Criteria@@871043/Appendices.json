{"hands_on_practices": [{"introduction": "The Bayesian Information Criterion (BIC) is more than just a formula; it is a large-sample approximation to the logarithm of the model evidence. This exercise [@problem_id:3403899] provides a foundational practice by guiding you through the derivation of BIC from the Laplace approximation of the evidence integral. By applying this to a numerical example, you will not only calculate BIC values but also connect them directly to the Bayes factor, reinforcing the criterion's interpretation as a tool for approximating Bayesian model comparison.", "problem": "Consider an inverse modeling and data assimilation setting in which an analyst must select between two nested Gaussian linear observation models for assimilating a fixed batch of measurements. Let $\\mathcal{M}_1$ be a restricted model with $k_1$ free parameters and $\\mathcal{M}_2$ be an extended model with $k_2$ free parameters, where the nesting implies that the parameter space of $\\mathcal{M}_1$ is a linear subspace of that of $\\mathcal{M}_2$. The data consist of $n$ independent draws, and the observation errors are Gaussian with zero mean and unknown variance. The maximum likelihood fits for the two models yield maximized log-likelihood values $\\ell_1$ and $\\ell_2$.\n\nStarting from the Bayesian definition of model evidence (marginal likelihood) for a given model and the Laplace approximation under standard regularity conditions, derive a large-sample criterion that penalizes model complexity through a term involving the sample size $n$ and the number of free parameters $k$. Use this criterion to compute the values for $\\mathcal{M}_1$ and $\\mathcal{M}_2$ from the reported $\\ell_1$ and $\\ell_2$, and then connect the difference of these criterion values to the logarithm of the Bayes factor in favor of $\\mathcal{M}_2$ against $\\mathcal{M}_1$.\n\nYou are provided with the following numerical outputs from the maximum likelihood fits:\n- Sample size: $n = 150$.\n- Parameter counts: $k_1 = 4$ for $\\mathcal{M}_1$ and $k_2 = 9$ for $\\mathcal{M}_2$.\n- Maximized log-likelihoods: $\\ell_1 = -210.7$ for $\\mathcal{M}_1$ and $\\ell_2 = -195.2$ for $\\mathcal{M}_2$.\n\nAssume that all regularity conditions for the Laplace approximation hold, the models are correctly specified up to parameter values, the priors are positive and sufficiently smooth near the maximum likelihood estimates, and use natural logarithms. Compute the Bayesian Information Criterion (BIC) for both models and then compute the Laplace-approximated Bayes factor in favor of $\\mathcal{M}_2$ against $\\mathcal{M}_1$ by relating the BIC difference to the marginal likelihood ratio. Provide the Bayes factor value as your final answer, rounded to four significant figures. No units are required for the final answer.", "solution": "The problem requires the derivation of a large-sample model selection criterion from the Bayesian definition of model evidence, its application to a specific case, and the calculation of a Bayes factor.\n\nLet $\\mathcal{M}$ be a model with a $k$-dimensional parameter vector $\\boldsymbol{\\theta}$. The evidence for model $\\mathcal{M}$, given the data $\\mathbf{y}$, is the marginal likelihood, defined as the integral of the likelihood over the prior distribution of the parameters:\n$$ p(\\mathbf{y} | \\mathcal{M}) = \\int p(\\mathbf{y} | \\boldsymbol{\\theta}, \\mathcal{M}) p(\\boldsymbol{\\theta} | \\mathcal{M}) d\\boldsymbol{\\theta} $$\nHere, $p(\\mathbf{y} | \\boldsymbol{\\theta}, \\mathcal{M})$ is the likelihood function $L(\\boldsymbol{\\theta})$, and $p(\\boldsymbol{\\theta} | \\mathcal{M})$ is the prior probability distribution of the parameters. The integral can be rewritten as:\n$$ p(\\mathbf{y} | \\mathcal{M}) = \\int \\exp\\left( \\ln\\left[ L(\\boldsymbol{\\theta}) p(\\boldsymbol{\\theta} | \\mathcal{M}) \\right] \\right) d\\boldsymbol{\\theta} = \\int \\exp\\left( \\ell(\\boldsymbol{\\theta}) + \\ln p(\\boldsymbol{\\theta} | \\mathcal{M}) \\right) d\\boldsymbol{\\theta} $$\nwhere $\\ell(\\boldsymbol{\\theta}) = \\ln L(\\boldsymbol{\\theta})$ is the log-likelihood.\n\nWe can approximate this integral using the Laplace method. The method is based on a second-order Taylor expansion of the exponent around its maximum. Let $f(\\boldsymbol{\\theta}) = \\ell(\\boldsymbol{\\theta}) + \\ln p(\\boldsymbol{\\theta} | \\mathcal{M})$. The maximum of $f(\\boldsymbol{\\theta})$ occurs at the maximum a posteriori (MAP) estimate, $\\hat{\\boldsymbol{\\theta}}_{\\text{MAP}}$. For a large sample size $n$, the likelihood term $\\ell(\\boldsymbol{\\theta})$ dominates the prior term $\\ln p(\\boldsymbol{\\theta} | \\mathcal{M})$, assuming a sufficiently diffuse prior. Consequently, $\\hat{\\boldsymbol{\\theta}}_{\\text{MAP}}$ is well approximated by the maximum likelihood estimate (MLE), $\\hat{\\boldsymbol{\\theta}}$, which maximizes $\\ell(\\boldsymbol{\\theta})$.\n\nThe Taylor expansion of $f(\\boldsymbol{\\theta})$ around $\\hat{\\boldsymbol{\\theta}}$ is:\n$$ f(\\boldsymbol{\\theta}) \\approx f(\\hat{\\boldsymbol{\\theta}}) + (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}})^T \\nabla f(\\hat{\\boldsymbol{\\theta}}) + \\frac{1}{2} (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}})^T \\mathbf{H}(\\hat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}}) $$\nwhere $\\mathbf{H}(\\hat{\\boldsymbol{\\theta}})$ is the Hessian matrix of $f$ evaluated at $\\hat{\\boldsymbol{\\theta}}$. By definition of the MLE, $\\nabla \\ell(\\hat{\\boldsymbol{\\theta}}) = \\boldsymbol{0}$. Thus, $\\nabla f(\\hat{\\boldsymbol{\\theta}}) = \\nabla \\ell(\\hat{\\boldsymbol{\\theta}}) + \\nabla \\ln p(\\hat{\\boldsymbol{\\theta}}) = \\nabla \\ln p(\\hat{\\boldsymbol{\\theta}})$. If the prior is flat near $\\hat{\\boldsymbol{\\theta}}$, this gradient term is negligible. The Hessian $\\mathbf{H}(\\hat{\\boldsymbol{\\theta}})$ is also dominated by the Hessian of the log-likelihood, and we can approximate $\\mathbf{H}(\\hat{\\boldsymbol{\\theta}}) \\approx \\nabla^2 \\ell(\\hat{\\boldsymbol{\\theta}})$. The negative of this matrix is the observed Fisher information matrix, $\\mathcal{J}(\\hat{\\boldsymbol{\\theta}}) = -\\nabla^2 \\ell(\\hat{\\boldsymbol{\\theta}})$.\n\nThe integral for the evidence becomes:\n$$ p(\\mathbf{y} | \\mathcal{M}) \\approx \\int \\exp\\left( f(\\hat{\\boldsymbol{\\theta}}) - \\frac{1}{2} (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}})^T \\mathcal{J}(\\hat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}}) \\right) d\\boldsymbol{\\theta} $$\n$$ p(\\mathbf{y} | \\mathcal{M}) \\approx \\exp(f(\\hat{\\boldsymbol{\\theta}})) \\int \\exp\\left( - \\frac{1}{2} (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}})^T \\mathcal{J}(\\hat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}}) \\right) d\\boldsymbol{\\theta} $$\nThe integral is the unnormalized form of a $k$-variate Gaussian distribution, which evaluates to $(2\\pi)^{k/2} |\\mathcal{J}(\\hat{\\boldsymbol{\\theta}})|^{-1/2}$. Substituting this and $f(\\hat{\\boldsymbol{\\theta}}) \\approx \\ell(\\hat{\\boldsymbol{\\theta}}) = \\ell_{\\text{max}}$ (ignoring the prior term $\\ln p(\\hat{\\boldsymbol{\\theta}})$ as a lower-order term), we get:\n$$ p(\\mathbf{y} | \\mathcal{M}) \\approx \\exp(\\ell_{\\text{max}}) (2\\pi)^{k/2} |\\mathcal{J}(\\hat{\\boldsymbol{\\theta}})|^{-1/2} $$\nTaking the natural logarithm yields the log-evidence:\n$$ \\ln p(\\mathbf{y} | \\mathcal{M}) \\approx \\ell_{\\text{max}} + \\frac{k}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln |\\mathcal{J}(\\hat{\\boldsymbol{\\theta}})| $$\nFor large $n$, the Fisher information matrix is asymptotically proportional to the sample size, i.e., $|\\mathcal{J}(\\hat{\\boldsymbol{\\theta}})|$ is of order $O(n^k)$. Thus, $\\ln|\\mathcal{J}(\\hat{\\boldsymbol{\\theta}})| \\approx k \\ln n + C$, where $C$ is a constant independent of $n$. Dropping all terms that do not grow with $n$, we obtain the large-sample approximation:\n$$ \\ln p(\\mathbf{y} | \\mathcal{M}) \\approx \\ell_{\\text{max}} - \\frac{k}{2} \\ln n $$\nThe Bayesian Information Criterion (BIC) is conventionally defined by multiplying this quantity by $-2$ to create a loss function to be minimized:\n$$ \\text{BIC} = -2 \\ln p(\\mathbf{y} | \\mathcal{M}) \\approx -2 \\ell_{\\text{max}} + k \\ln n $$\nThis is the desired large-sample criterion. A lower BIC value indicates stronger evidence for the model.\n\nWe now apply this to the two models, $\\mathcal{M}_1$ and $\\mathcal{M}_2$, using the provided data:\nSample size: $n = 150$.\nFor $\\mathcal{M}_1$: number of parameters $k_1 = 4$, maximized log-likelihood $\\ell_1 = -210.7$.\nFor $\\mathcal{M}_2$: number of parameters $k_2 = 9$, maximized log-likelihood $\\ell_2 = -195.2$.\n\nFirst, we compute the value of $\\ln(150)$:\n$$ \\ln(150) \\approx 5.010635 $$\nNow we compute the BIC for each model:\n$$ \\text{BIC}_1 = -2 \\ell_1 + k_1 \\ln n = -2(-210.7) + 4 \\ln(150) = 421.4 + 4(5.010635) = 421.4 + 20.04254 = 441.44254 $$\n$$ \\text{BIC}_2 = -2 \\ell_2 + k_2 \\ln n = -2(-195.2) + 9 \\ln(150) = 390.4 + 9(5.010635) = 390.4 + 45.095715 = 435.495715 $$\nSince $\\text{BIC}_2  \\text{BIC}_1$, the BIC favors the more complex model $\\mathcal{M}_2$.\n\nThe final step is to compute the Bayes factor, $B_{21}$, in favor of $\\mathcal{M}_2$ against $\\mathcal{M}_1$. The Bayes factor is the ratio of the marginal likelihoods:\n$$ B_{21} = \\frac{p(\\mathbf{y} | \\mathcal{M}_2)}{p(\\mathbf{y} | \\mathcal{M}_1)} $$\nThe logarithm of the Bayes factor is related to the difference in log-evidence:\n$$ \\ln B_{21} = \\ln p(\\mathbf{y} | \\mathcal{M}_2) - \\ln p(\\mathbf{y} | \\mathcal{M}_1) $$\nUsing our large-sample approximation for the log-evidence:\n$$ \\ln B_{21} \\approx \\left(\\ell_2 - \\frac{k_2}{2} \\ln n \\right) - \\left(\\ell_1 - \\frac{k_1}{2} \\ln n \\right) = (\\ell_2 - \\ell_1) - \\frac{k_2 - k_1}{2} \\ln n $$\nThis can also be expressed in terms of the BIC values:\n$$ \\ln B_{21} \\approx \\frac{1}{2} \\left[ (-2\\ell_1 + k_1 \\ln n) - (-2\\ell_2 + k_2 \\ln n) \\right] = \\frac{\\text{BIC}_1 - \\text{BIC}_2}{2} $$\nUsing the BIC values we computed:\n$$ \\ln B_{21} \\approx \\frac{441.44254 - 435.495715}{2} = \\frac{5.946825}{2} = 2.9734125 $$\nTo calculate the Bayes factor $B_{21}$, we exponentiate this result:\n$$ B_{21} = \\exp(\\ln B_{21}) \\approx \\exp(2.9734125) \\approx 19.5583 $$\nAlternatively, and for better precision, we use the direct formula:\n$$ \\ln B_{21} \\approx (\\ell_2 - \\ell_1) - \\frac{k_2 - k_1}{2} \\ln n $$\nSubstituting the given values:\n$$ \\ell_2 - \\ell_1 = -195.2 - (-210.7) = 15.5 $$\n$$ k_2 - k_1 = 9 - 4 = 5 $$\n$$ \\ln B_{21} \\approx 15.5 - \\frac{5}{2} \\ln(150) = 15.5 - 2.5(5.01063529) = 15.5 - 12.52658823 = 2.97341177 $$\n$$ B_{21} = \\exp(2.97341177) \\approx 19.55829 $$\nRounding the result to four significant figures gives $19.56$. This value indicates very strong evidence in favor of model $\\mathcal{M}_2$ compared to model $\\mathcal{M}_1$, according to standard interpretations of the Bayes factor (e.g., Jeffreys' scale).", "answer": "$$\\boxed{19.56}$$", "id": "3403899"}, {"introduction": "The penalty term in information criteria, which accounts for model complexity, depends critically on the number of estimated parameters. This practice [@problem_id:3403909] explores this subtlety by asking you to compute the Akaike Information Criterion (AIC) under two different scenarios: one where the observation error variance is known and another where it must be estimated. This comparison will sharpen your intuition about what constitutes an \"estimated parameter\" and how this choice can significantly impact model selection outcomes.", "problem": "Consider a data assimilation setting in which a forecast model is calibrated to a fixed set of $n$ observations by minimizing the sum of squared innovations (residuals). Two candidate models, $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$, are fit to the same $n$ innovations under the assumption of independent Gaussian measurement errors. Let the residual sequences for the $n=10$ assimilation cycles be\n$$\n\\boldsymbol{r}_{1} = \\big(1,\\,-2,\\,2,\\,1,\\,3,\\,-2,\\,1,\\,-1,\\,2,\\,-1\\big), \\quad \\boldsymbol{r}_{2} = \\big(2,\\,-1,\\,1,\\,2,\\,-2,\\,1,\\,0,\\,-1,\\,2,\\,-1\\big).\n$$\nModel $\\mathcal{M}_{1}$ estimates a parameter vector of dimension $p_{1}=3$, and model $\\mathcal{M}_{2}$ estimates a parameter vector of dimension $p_{2}=5$. Assume the residuals arise from a Gaussian error model with variance $\\sigma^{2}$ that is either known or unknown, as specified below.\n\nStarting only from the Gaussian likelihood for independent errors and the definition of the Akaike Information Criterion (AIC), perform the following:\n\n1. Compute the residual sums of squares $RSS_{1} = \\sum_{i=1}^{n} r_{1,i}^{2}$ and $RSS_{2} = \\sum_{i=1}^{n} r_{2,i}^{2}$.\n\n2. Under the scenario where the error variance is known and equal to $\\sigma^{2} = 2$, derive the AIC for each model and report the difference $AIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1})$.\n\n3. Under the scenario where the error variance $\\sigma^{2}$ is unknown and estimated by maximum likelihood, derive the AIC for each model and report the difference $AIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1})$.\n\nBriefly discuss, based on your calculations and without using any pre-stated formulas, how the change from known $\\sigma^{2}$ to unknown $\\sigma^{2}$ affects the model preference, and summarize the intuition in relation to the Bayesian Information Criterion (BIC).\n\nYour final reported answer must be the pair of differences $\\big(AIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1})\\big)$ under known and unknown variance, expressed exactly as a row vector using the $\\mathrm{pmatrix}$ environment. No rounding is required.", "solution": "The fundamental definition of AIC is $AIC = -2 \\ln(\\mathcal{L}_{max}) + 2k$, where $\\mathcal{L}_{max}$ is the value of the likelihood function maximized over the model parameters, and $k$ is the total number of estimated parameters in the model.\n\nThe likelihood function for $n$ independent and identically distributed Gaussian residuals $r_i$ with mean $0$ and variance $\\sigma^2$ is given by:\n$$\n\\mathcal{L}(\\theta, \\sigma^2 | \\boldsymbol{r}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r_i^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} r_i^2\\right)\n$$\nThe corresponding log-likelihood is:\n$$\n\\ln(\\mathcal{L}) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{RSS}{2\\sigma^2}\n$$\nwhere $RSS = \\sum_{i=1}^{n} r_i^2$ is the residual sum of squares.\n\n**1. Compute the Residual Sums of Squares (RSS)**\n\nFor model $\\mathcal{M}_{1}$, with $n=10$ and residual vector $\\boldsymbol{r}_{1} = \\big(1,\\,-2,\\,2,\\,1,\\,3,\\,-2,\\,1,\\,-1,\\,2,\\,-1\\big)$:\n$$\nRSS_{1} = \\sum_{i=1}^{10} r_{1,i}^{2} = 1^2 + (-2)^2 + 2^2 + 1^2 + 3^2 + (-2)^2 + 1^2 + (-1)^2 + 2^2 + (-1)^2\n$$\n$$\nRSS_{1} = 1 + 4 + 4 + 1 + 9 + 4 + 1 + 1 + 4 + 1 = 30\n$$\nFor model $\\mathcal{M}_{2}$, with $n=10$ and residual vector $\\boldsymbol{r}_{2} = \\big(2,\\,-1,\\,1,\\,2,\\,-2,\\,1,\\,0,\\,-1,\\,2,\\,-1\\big)$:\n$$\nRSS_{2} = \\sum_{i=1}^{10} r_{2,i}^{2} = 2^2 + (-1)^2 + 1^2 + 2^2 + (-2)^2 + 1^2 + 0^2 + (-1)^2 + 2^2 + (-1)^2\n$$\n$$\nRSS_{2} = 4 + 1 + 1 + 4 + 4 + 1 + 0 + 1 + 4 + 1 = 21\n$$\n\n**2. AIC with Known Error Variance**\n\nIn this scenario, the error variance is known and fixed at $\\sigma^2 = 2$. It is not an estimated parameter. The number of estimated parameters for each model is solely the dimension of its internal parameter vector.\n- For model $\\mathcal{M}_{1}$, the number of estimated parameters is $k_1 = p_1 = 3$.\n- For model $\\mathcal{M}_{2}$, the number of estimated parameters is $k_2 = p_2 = 5$.\n\nThe log-likelihood for a model $j$ is evaluated with the given $RSS_j$ and the known $\\sigma^2=2$. This value represents the maximized likelihood because the residuals are the result of a precedent optimization process.\n$$\n\\ln(\\mathcal{L}_{max, j}) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{RSS_j}{2\\sigma^2}\n$$\nThe AIC for model $j$ is:\n$$\nAIC(\\mathcal{M}_{j}) = -2 \\left( -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{RSS_j}{2\\sigma^2} \\right) + 2k_j = n\\ln(2\\pi\\sigma^2) + \\frac{RSS_j}{\\sigma^2} + 2k_j\n$$\nFor $\\mathcal{M}_{1}$:\n$$\nAIC(\\mathcal{M}_{1}) = 10\\ln(2\\pi \\cdot 2) + \\frac{30}{2} + 2(3) = 10\\ln(4\\pi) + 15 + 6 = 10\\ln(4\\pi) + 21\n$$\nFor $\\mathcal{M}_{2}$:\n$$\nAIC(\\mathcal{M}_{2}) = 10\\ln(2\\pi \\cdot 2) + \\frac{21}{2} + 2(5) = 10\\ln(4\\pi) + \\frac{21}{2} + 10 = 10\\ln(4\\pi) + \\frac{41}{2}\n$$\nThe difference is:\n$$\nAIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1}) = \\left(10\\ln(4\\pi) + \\frac{41}{2}\\right) - (10\\ln(4\\pi) + 21) = \\frac{41}{2} - 21 = \\frac{41 - 42}{2} = -\\frac{1}{2}\n$$\n\n**3. AIC with Unknown Error Variance**\n\nIn this scenario, the error variance $\\sigma^2$ is unknown and must be estimated from the data for each model. This adds one estimated parameter to each model's count.\n- For model $\\mathcal{M}_{1}$, the number of estimated parameters is $k_1 = p_1 + 1 = 3 + 1 = 4$.\n- For model $\\mathcal{M}_{2}$, the number of estimated parameters is $k_2 = p_2 + 1 = 5 + 1 = 6$.\n\nTo find the maximized log-likelihood, we first find the maximum likelihood estimate (MLE) of $\\sigma^2$ for a given $RSS$. We differentiate the log-likelihood with respect to $\\sigma^2$ and set the result to zero:\n$$\n\\frac{\\partial}{\\partial (\\sigma^2)} \\ln(\\mathcal{L}) = \\frac{\\partial}{\\partial (\\sigma^2)} \\left( -\\frac{n}{2}\\ln(2\\pi) -\\frac{n}{2}\\ln(\\sigma^2) - \\frac{RSS}{2\\sigma^2} \\right) = -\\frac{n}{2\\sigma^2} + \\frac{RSS}{2(\\sigma^2)^2}\n$$\nSetting this to zero yields the MLE for $\\sigma^2$:\n$$\n\\frac{n}{2\\hat{\\sigma}^2} = \\frac{RSS}{2(\\hat{\\sigma}^2)^2} \\implies \\hat{\\sigma}^2 = \\frac{RSS}{n}\n$$\nNow we substitute this $\\hat{\\sigma}^2$ back into the log-likelihood expression to get the maximized value, $\\ln(\\mathcal{L}_{max})$:\n$$\n\\ln(\\mathcal{L}_{max}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{RSS}{n}\\right) - \\frac{RSS}{2(RSS/n)} = -\\frac{n}{2}\\left( \\ln(2\\pi) + \\ln\\left(\\frac{RSS}{n}\\right) + 1 \\right)\n$$\nThe AIC for model $j$ is then:\n$$\nAIC(\\mathcal{M}_{j}) = -2\\ln(\\mathcal{L}_{max, j}) + 2k_j = n\\left(\\ln(2\\pi) + \\ln\\left(\\frac{RSS_j}{n}\\right) + 1\\right) + 2k_j\n$$\nFor $\\mathcal{M}_{1}$: $RSS_1 = 30$, $k_1 = 4$.\n$$\nAIC(\\mathcal{M}_{1}) = 10\\left(\\ln(2\\pi) + \\ln\\left(\\frac{30}{10}\\right) + 1\\right) + 2(4) = 10(\\ln(2\\pi) + \\ln(3) + 1) + 8 = 10\\ln(6\\pi) + 10 + 8 = 10\\ln(6\\pi) + 18\n$$\nFor $\\mathcal{M}_{2}$: $RSS_2 = 21$, $k_2 = 6$.\n$$\nAIC(\\mathcal{M}_{2}) = 10\\left(\\ln(2\\pi) + \\ln\\left(\\frac{21}{10}\\right) + 1\\right) + 2(6) = 10(\\ln(2\\pi) + \\ln(2.1) + 1) + 12 = 10\\ln(4.2\\pi) + 10 + 12 = 10\\ln\\left(\\frac{21\\pi}{5}\\right) + 22\n$$\nThe difference is:\n$$\nAIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1}) = \\left(10\\ln\\left(\\frac{21\\pi}{5}\\right) + 22\\right) - (10\\ln(6\\pi) + 18)\n$$\n$$\n= 10\\left(\\ln\\left(\\frac{21\\pi}{5}\\right) - \\ln(6\\pi)\\right) + 4 = 10\\ln\\left(\\frac{21\\pi/5}{6\\pi}\\right) + 4 = 10\\ln\\left(\\frac{21}{30}\\right) + 4\n$$\n$$\n= 10\\ln\\left(\\frac{7}{10}\\right) + 4\n$$\n\n**Discussion**\n\nWhen the error variance $\\sigma^2$ is known, $AIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1}) = -1/2$. A lower AIC value indicates a better model, so $\\mathcal{M}_{2}$ is preferred.\n\nWhen $\\sigma^2$ is unknown and estimated, $AIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1}) = 4 + 10\\ln(7/10)$. Since $\\ln(7/10)  0$, we can evaluate this. $\\ln(0.7) \\approx -0.3567$, so the difference is approximately $4 - 3.567 = 0.433  0$. In this case, $\\mathcal{M}_{1}$ is preferred.\n\nThe model preference switches from the more complex model ($\\mathcal{M}_{2}$) to the simpler model ($\\mathcal{M}_{1}$) when the variance becomes an estimated parameter. The reason lies in the goodness-of-fit term of the AIC.\n- For known $\\sigma^2$, the log-likelihood term in the $\\Delta AIC$ scales as $(RSS_2 - RSS_1)/\\sigma^2 = (21-30)/2 = -4.5$. This is a linear measure of the improvement in fit.\n- For unknown $\\sigma^2$, the log-likelihood term scales as $n\\ln(RSS_2/RSS_1) = 10\\ln(21/30) = 10\\ln(0.7) \\approx -3.57$. The logarithmic scale dampens the reward for reducing the RSS.\n\nWhen each model is allowed to estimate its own optimal variance ($\\hat{\\sigma}^2 = RSS/n$), the advantage of a lower $RSS$ is partially absorbed into the claim of a lower intrinsic error level. This reduces the effective \"credit\" the model gets for its better fit compared to the case where both models are judged against a common, fixed variance. Consequently, the penalty for higher complexity ($2k$) becomes more influential, leading to the selection of the simpler model $\\mathcal{M}_{1}$.\n\nThis effect has a conceptual parallel to the Bayesian Information Criterion (BIC), defined as $BIC = -2 \\ln(\\mathcal{L}_{max}) + k \\ln(n)$. For $n=10$, $\\ln(10) \\approx 2.3$, so the BIC complexity penalty, $k\\ln(10)$, is stronger than the AIC penalty, $2k$. BIC inherently favors simpler models more strongly than AIC. The shift in preference towards the simpler model $\\mathcal{M}_{1}$ when moving to the unknown-variance case for AIC is thus a move in the direction that BIC would likely favor from the start.", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{1}{2}  4 + 10\\ln\\left(\\frac{7}{10}\\right) \\end{pmatrix}}\n$$", "id": "3403909"}, {"introduction": "Moving from manual calculation to computational implementation is a crucial step in applying model selection methods to real-world data. This coding exercise [@problem_id:3403751] challenges you to build a program that uses the BIC to automatically decide whether a time series of observations contains a change-point in its noise variance. You will implement the BIC calculation for competing models and use it to solve a practical problem relevant to monitoring and quality control in data assimilation systems.", "problem": "Consider the inverse problem of identifying the appropriate observation error model in a one-dimensional data assimilation setting. You observe a sequence $\\{y_t\\}_{t=1}^n$ that represents measurements of a static latent state with zero mean and additive Gaussian observation noise. The competing models specify the structure of the observation noise variance over time:\n\n- Model $\\mathcal{M}_0$ (zero change-point): a single constant variance $\\sigma^2$ for all $t$.\n- Model $\\mathcal{M}_1$ (one change-point): two piecewise-constant variances $\\sigma_1^2$ for $t \\le \\tau$ and $\\sigma_2^2$ for $t  \\tau$, with a single unknown change-point index $\\tau$.\n\nAssume the latent state is zero and the observation noise is Gaussian with zero mean. The Maximum Likelihood Estimate (MLE) for the variance in each segment equals the empirical mean of squared observations in that segment. The Bayesian Information Criterion (BIC) for a model with $k$ parameters and MLE log-likelihood $\\log \\hat{L}$ is defined as $\\mathrm{BIC} = -2 \\log \\hat{L} + k \\log n$, where $n$ is the total number of observations. The change-point index $\\tau$ in $\\mathcal{M}_1$ is treated as a parameter contributing a penalty of $\\log n$, so $\\mathcal{M}_0$ has $k=1$ and $\\mathcal{M}_1$ has $k=3$.\n\nGiven the sequence $\\{y_t\\}_{t=1}^n$, the MLE log-likelihood under $\\mathcal{M}_0$ is\n$$\n\\log \\hat{L}_0 = -\\frac{n}{2}\\left[\\log\\left(2\\pi \\hat{\\sigma}^2\\right) + 1\\right], \\quad \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{t=1}^n y_t^2,\n$$\nand under $\\mathcal{M}_1$ for a fixed change-point $\\tau$,\n$$\n\\log \\hat{L}_1(\\tau) = -\\frac{n_1}{2}\\left[\\log\\left(2\\pi \\hat{\\sigma}_1^2\\right) + 1\\right] - \\frac{n_2}{2}\\left[\\log\\left(2\\pi \\hat{\\sigma}_2^2\\right) + 1\\right],\n$$\nwhere $n_1 = \\tau$, $n_2 = n - \\tau$, $\\hat{\\sigma}_1^2 = \\frac{1}{n_1} \\sum_{t=1}^{\\tau} y_t^2$, and $\\hat{\\sigma}_2^2 = \\frac{1}{n_2} \\sum_{t=\\tau+1}^{n} y_t^2$. The MLE log-likelihood for $\\mathcal{M}_1$ is $\\log \\hat{L}_1 = \\max_{\\tau_{\\min} \\le \\tau \\le n - \\tau_{\\min}} \\log \\hat{L}_1(\\tau)$, where $\\tau_{\\min}$ is a minimum segment length constraint to avoid degenerate estimates.\n\nYour task is to:\n\n- Construct synthetic sequences $\\{y_t\\}_{t=1}^n$ with a known change in observation noise variance (or no change), by assigning deterministic values that yield specified sums of squares per segment. Specifically, set $y_t = (-1)^t \\sqrt{v}$ in each segment with target variance $v$, so that the sum of squares in a segment of length $m$ equals $m v$.\n- Compute $\\mathrm{BIC}_0$ and $\\mathrm{BIC}_1$ using the formulas above and decide which model is preferred (the one with smaller BIC).\n- Use the following test suite of three cases, with minimum segment length $\\tau_{\\min}$ set to $5$ in all cases:\n    1. Case A (happy path): $n = 100$, a single change-point at $\\tau^\\star = 60$, with variances $v_1 = 1$ for $t \\le \\tau^\\star$ and $v_2 = 9$ for $t  \\tau^\\star$.\n    2. Case B (no change): $n = 100$, no change-point, with variance $v = 4$ for all $t$.\n    3. Case C (edge change-point): $n = 80$, a single change-point at $\\tau^\\star = 5$, with variances $v_1 = 1$ for $t \\le \\tau^\\star$ and $v_2 = 4$ for $t  \\tau^\\star$.\n\nFor each case, produce an integer output: $1$ if $\\mathcal{M}_1$ (one change-point) is preferred, and $0$ if $\\mathcal{M}_0$ (zero change-point) is preferred.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\").", "solution": "The task is to perform model selection between a null model $\\mathcal{M}_0$ with constant variance and an alternative model $\\mathcal{M}_1$ with a single change-point in variance. The selection criterion is the Bayesian Information Criterion (BIC), which penalizes model complexity. The model with the lower BIC is preferred.\n\nThe BIC is given by $\\mathrm{BIC} = -2\\log\\hat{L} + k\\log n$, where $\\log\\hat{L}$ is the maximized log-likelihood of the model, $k$ is the number of parameters, and $n$ is the sample size.\n\nFor a segment of data of length $m$ with sum of squares $S = \\sum y_t^2$, the MLE of the variance is $\\hat{\\sigma}^2 = S/m$. The corresponding maximized log-likelihood is $\\log\\hat{L} = -\\frac{m}{2}[\\log(2\\pi\\hat{\\sigma}^2) + 1]$.\n\nWe will implement a procedure for each test case:\n1.  Synthesize the data by creating an array of squared observations, $\\{y_t^2\\}_{t=1}^n$, according to the specified variances for each segment.\n2.  Compute $\\mathrm{BIC}_0$ for model $\\mathcal{M}_0$.\n3.  Compute $\\mathrm{BIC}_1$ for model $\\mathcal{M}_1$. This requires finding the optimal change-point $\\hat{\\tau}$ that maximizes the log-likelihood $\\log \\hat{L}_1(\\tau)$.\n4.  Compare the BIC values and determine the preferred model.\n\n**Model $\\mathcal{M}_0$: Constant Variance**\n- Number of parameters: $k_0 = 1$ (for $\\sigma^2$).\n- The total sum of squares is $S_{total} = \\sum_{t=1}^n y_t^2$.\n- The MLE variance is $\\hat{\\sigma}^2 = S_{total} / n$.\n- The maximized log-likelihood is $\\log \\hat{L}_0 = -\\frac{n}{2}[\\log(2\\pi\\hat{\\sigma}^2) + 1]$.\n- The BIC is $\\mathrm{BIC}_0 = -2\\log\\hat{L}_0 + k_0\\log n$.\n\n**Model $\\mathcal{M}_1$: One Change-Point**\n- Number of parameters: $k_1 = 3$ (for $\\sigma_1^2$, $\\sigma_2^2$, $\\tau$).\n- We must find the optimal change-point $\\hat{\\tau}$ by maximizing $\\log \\hat{L}_1(\\tau)$ over the allowed range $\\tau \\in [\\tau_{\\min}, n-\\tau_{\\min}]$.\n- For each candidate $\\tau$:\n    - The data is split into two segments: $\\{y_t\\}_{t=1}^{\\tau}$ and $\\{y_t\\}_{t=\\tau+1}^{n}$.\n    - Let $n_1 = \\tau$ and $n_2 = n-\\tau$.\n    - Let $S_1(\\tau) = \\sum_{t=1}^{\\tau} y_t^2$ and $S_2(\\tau) = \\sum_{t=\\tau+1}^{n} y_t^2$.\n    - The MLE variances for the segments are $\\hat{\\sigma}_1^2(\\tau) = S_1(\\tau)/n_1$ and $\\hat{\\sigma}_2^2(\\tau) = S_2(\\tau)/n_2$.\n    - The log-likelihood for this $\\tau$ is the sum of the log-likelihoods of the two segments:\n    $$ \\log \\hat{L}_1(\\tau) = \\left(-\\frac{n_1}{2}[\\log(2\\pi\\hat{\\sigma}_1^2) + 1]\\right) + \\left(-\\frac{n_2}{2}[\\log(2\\pi\\hat{\\sigma}_2^2) + 1]\\right) $$\n- The overall maximized log-likelihood for $\\mathcal{M}_1$ is $\\log \\hat{L}_1 = \\max_{\\tau} \\log \\hat{L}_1(\\tau)$.\n- The BIC is $\\mathrm{BIC}_1 = -2\\log\\hat{L}_1 + k_1\\log n$.\n\nTo efficiently calculate the segment sums of squares $S_1(\\tau)$ and $S_2(\\tau)$ for all $\\tau$, we first compute the cumulative sum of squares, $C_t = \\sum_{i=1}^t y_i^2$. Then, $S_1(\\tau) = C_{\\tau}$ and $S_2(\\tau) = C_n - C_{\\tau}$.\n\n**Decision**\n- If $\\mathrm{BIC}_1  \\mathrm{BIC}_0$, model $\\mathcal{M}_1$ is preferred. The output is $1$.\n- Otherwise, model $\\mathcal{M}_0$ is preferred. The output is $0$.\n\nThe following program implements this logic.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for the three specified test cases.\n    \"\"\"\n\n    def calculate_log_likelihood(sum_sq, m):\n        \"\"\"\n        Calculates the maximized log-likelihood for a segment.\n        \n        Args:\n            sum_sq (float): The sum of squared observations in the segment.\n            m (int): The number of observations in the segment.\n            \n        Returns:\n            float: The maximized log-likelihood value.\n        \"\"\"\n        # Handle edge cases: zero-length segment or zero variance\n        if m == 0 or sum_sq = 0:\n            return -np.inf\n        \n        var_mle = sum_sq / m\n        # The formula is -m/2 * (log(2*pi*var_mle) + 1)\n        log_L = -m / 2.0 * (np.log(2.0 * np.pi * var_mle) + 1.0)\n        return log_L\n\n    def solve_case(n, y_sq, tau_min):\n        \"\"\"\n        Computes the preferred model (0 or 1) for a single test case.\n        \n        Args:\n            n (int): Total number of observations.\n            y_sq (np.array): Array of squared observation values.\n            tau_min (int): Minimum segment length.\n            \n        Returns:\n            int: 1 if Model M1 is preferred, 0 otherwise.\n        \"\"\"\n        # --- Model M0: Zero change-points ---\n        k0 = 1\n        total_sum_sq = np.sum(y_sq)\n        log_L0 = calculate_log_likelihood(total_sum_sq, n)\n        bic0 = -2.0 * log_L0 + k0 * np.log(n)\n        \n        # --- Model M1: One change-point ---\n        k1 = 3\n        # Use cumulative sum for efficient calculation of segment sums\n        cum_sum_sq = np.cumsum(y_sq)\n        \n        max_log_L1 = -np.inf\n        \n        # Search for the optimal change-point tau\n        # The range for tau is [tau_min, n - tau_min]\n        for tau in range(tau_min, n - tau_min + 1):\n            n1 = tau\n            n2 = n - tau\n            \n            # Sum of squares for segment 1 (indices 0 to tau-1)\n            s1 = cum_sum_sq[tau - 1]\n            # Sum of squares for segment 2\n            s2 = total_sum_sq - s1\n            \n            log_L1_tau = calculate_log_likelihood(s1, n1) + calculate_log_likelihood(s2, n2)\n            \n            if log_L1_tau > max_log_L1:\n                max_log_L1 = log_L1_tau\n        \n        bic1 = -2.0 * max_log_L1 + k1 * np.log(n)\n        \n        return 1 if bic1  bic0 else 0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Happy path with a clear change-point\n        {'n': 100, 'tau_star': 60, 'v1': 1.0, 'v2': 9.0, 'tau_min': 5},\n        # Case B: No change\n        {'n': 100, 'tau_star': None, 'v1': 4.0, 'v2': 4.0, 'tau_min': 5},\n        # Case C: Edge change-point\n        {'n': 80, 'tau_star': 5, 'v1': 1.0, 'v2': 4.0, 'tau_min': 5}\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case['n']\n        tau_star = case['tau_star']\n        v1 = case['v1']\n        v2 = case['v2']\n        tau_min = case['tau_min']\n\n        # Construct the array of squared observations y_t^2 = v\n        y_sq = np.zeros(n)\n        if tau_star is not None:\n            # Data with a change-point\n            y_sq[:tau_star] = v1\n            y_sq[tau_star:] = v2\n        else:\n            # Data with constant variance\n            y_sq[:] = v1\n\n        result = solve_case(n, y_sq, tau_min)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3403751"}]}