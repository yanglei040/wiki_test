{"hands_on_practices": [{"introduction": "The observation error covariance matrix, $R$, is not just an arbitrary collection of numbers; it must satisfy specific mathematical properties to be physically meaningful and numerically sound. This first exercise reinforces the fundamental requirements of symmetry and positive definiteness. By working through this foundational calculation [@problem_id:3406348], you will connect these abstract properties to concrete linear algebraic concepts like eigenvalues and condition numbers, forming the bedrock for all advanced covariance modeling.", "problem": "Consider a linear observation model in a variational data assimilation system for a single geophysical state variable, observed simultaneously by two collocated sensors with correlated errors. Let the observation error vector be modeled as a zero-mean Gaussian random vector with covariance matrix $R \\in \\mathbb{R}^{2 \\times 2}$. In scientifically realistic observation error covariance modeling, $R$ must be symmetric and positive definite to reflect physically meaningful variances and covariances and to ensure well-posedness when inverting $R$ in the assimilation cost function. Suppose the reported observation error covariance matrix is\n$$\nR = \\begin{pmatrix}\n4 & 1\\\\\n1 & 1\n\\end{pmatrix},\n$$\nwhere entries have units of the square of the observed quantity (for concreteness, take kelvin squared, $\\mathrm{K}^{2}$).\n\nStarting strictly from foundational definitions in probability and linear algebra—namely, the definition of a covariance matrix for a Gaussian random vector, properties of symmetric matrices, the characterization of positive definiteness through eigenvalues, and the definition of spectral conditioning—answer the following:\n\n1. Compute the inverse $R^{-1}$ exactly, ensuring the result is symmetric as required for an inverse of a symmetric positive definite matrix.\n2. Compute the eigenvalues $\\lambda_{1}$ and $\\lambda_{2}$ of $R$ exactly and use them to verify positive definiteness.\n3. Using the two-norm spectral condition number definition, compute the exact spectral condition number $\\kappa_{2}(R)$ to assess the numerical conditioning relevant to inversion in the assimilation cost function.\n\nExpress all results in exact symbolic form (no rounding). For unit reporting: entries of $R^{-1}$ should be understood to have units $\\mathrm{K}^{-2}$ and eigenvalues $\\lambda_{1}, \\lambda_{2}$ should be understood to have units $\\mathrm{K}^{2}$. Provide the final eigenvalues ordered such that $\\lambda_{1} \\geq \\lambda_{2} > 0$. The final answer must be a single expression comprising the four entries of $R^{-1}$, followed by $\\lambda_{1}$, $\\lambda_{2}$, and $\\kappa_{2}(R)$, arranged as one row using the $\\text{pmatrix}$ environment, in exact form.", "solution": "The problem as stated is subjected to validation.\n\n### Step 1: Extract Givens\n- **Model Context**: Linear observation model in variational data assimilation.\n- **State Variable**: A single geophysical state variable.\n- **Observation Setup**: Two collocated sensors with correlated errors.\n- **Error Model**: The observation error vector is a zero-mean Gaussian random vector.\n- **Covariance Matrix**: The observation error covariance matrix is denoted by $R \\in \\mathbb{R}^{2 \\times 2}$.\n- **Properties of $R$**: $R$ must be symmetric and positive definite.\n- **Given Matrix**: The specific covariance matrix is given as $R = \\begin{pmatrix} 4 & 1\\\\ 1 & 1 \\end{pmatrix}$.\n- **Units**: The entries of $R$ are in units of kelvin squared, $\\mathrm{K}^{2}$. Consequently, entries of $R^{-1}$ have units $\\mathrm{K}^{-2}$, and eigenvalues of $R$ have units $\\mathrm{K}^{2}$.\n- **Tasks**:\n    1. Compute the inverse matrix $R^{-1}$.\n    2. Compute the eigenvalues $\\lambda_{1}, \\lambda_{2}$ of $R$ and verify positive definiteness. Eigenvalues must be ordered such that $\\lambda_{1} \\geq \\lambda_{2}$.\n    3. Compute the exact spectral condition number $\\kappa_{2}(R)$ using the two-norm.\n- **Output Format**: A single row matrix containing the four entries of $R^{-1}$, followed by $\\lambda_{1}$, $\\lambda_{2}$, and $\\kappa_{2}(R)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is well-grounded in the established theory of data assimilation and inverse problems. The requirement for a covariance matrix to be symmetric and positive definite is fundamental. Symmetry ($R_{ij} = \\text{cov}(e_i, e_j) = \\text{cov}(e_j, e_i) = R_{ji}$) follows from the definition of covariance. Positive definiteness ensures that the variance of any linear combination of the observation errors is positive, which is a physical necessity, and it guarantees that the quadratic form in the assimilation cost function, $(\\mathbf{y} - H\\mathbf{x})^T R^{-1} (\\mathbf{y} - H\\mathbf{x})$, has a unique minimum. The given matrix is symmetric, and its diagonal elements (variances) are positive, which is a necessary, though not sufficient, condition for positive definiteness. The problem is scientifically and mathematically sound.\n- **Well-Posedness**: The tasks involve standard, well-defined computations in linear algebra (matrix inversion, eigenvalue calculation, condition number calculation) for a given non-singular matrix. A unique solution exists.\n- **Objectivity**: The problem is stated in precise, objective mathematical terms.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is self-contained, scientifically sound, and well-posed. The solution process may proceed.\n\n---\n\nThe solution is performed in three parts as requested.\n\n### Part 1: Computation of the Inverse Matrix $R^{-1}$\nLet the given matrix be $R = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{pmatrix} 4 & 1 \\\\ 1 & 1 \\end{pmatrix}$. The matrix is symmetric since $b=c$.\nThe inverse of a $2 \\times 2$ matrix is given by the formula:\n$$\nR^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}\n$$\nFirst, we compute the determinant of $R$, denoted $\\det(R)$.\n$$\n\\det(R) = (4)(1) - (1)(1) = 4 - 1 = 3\n$$\nSince $\\det(R) \\neq 0$, the inverse exists. Substituting the values into the formula:\n$$\nR^{-1} = \\frac{1}{3} \\begin{pmatrix} 1 & -1 \\\\ -1 & 4 \\end{pmatrix}\n$$\nDistributing the scalar factor $\\frac{1}{3}$ into the matrix gives the final form of the inverse:\n$$\nR^{-1} = \\begin{pmatrix} \\frac{1}{3} & -\\frac{1}{3} \\\\ -\\frac{1}{3} & \\frac{4}{3} \\end{pmatrix}\n$$\nAs required, the inverse $R^{-1}$ is also a symmetric matrix.\n\n### Part 2: Computation of Eigenvalues and Verification of Positive Definiteness\nThe eigenvalues, $\\lambda$, of the matrix $R$ are the solutions to the characteristic equation $\\det(R - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\n$$\nR - \\lambda I = \\begin{pmatrix} 4 & 1 \\\\ 1 & 1 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 4-\\lambda & 1 \\\\ 1 & 1-\\lambda \\end{pmatrix}\n$$\nThe determinant is:\n$$\n\\det(R - \\lambda I) = (4-\\lambda)(1-\\lambda) - (1)(1) = 0\n$$\nExpanding this equation gives a quadratic equation for $\\lambda$:\n$$\n4 - 4\\lambda - \\lambda + \\lambda^2 - 1 = 0\n$$\n$$\n\\lambda^2 - 5\\lambda + 3 = 0\n$$\nWe solve for $\\lambda$ using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$, with $a=1$, $b=-5$, and $c=3$.\n$$\n\\lambda = \\frac{-(-5) \\pm \\sqrt{(-5)^2 - 4(1)(3)}}{2(1)} = \\frac{5 \\pm \\sqrt{25 - 12}}{2} = \\frac{5 \\pm \\sqrt{13}}{2}\n$$\nThis gives the two eigenvalues. Following the requirement to order them such that $\\lambda_{1} \\geq \\lambda_{2}$:\n$$\n\\lambda_{1} = \\frac{5 + \\sqrt{13}}{2}\n$$\n$$\n\\lambda_{2} = \\frac{5 - \\sqrt{13}}{2}\n$$\nTo verify that $R$ is positive definite, we must show that all its eigenvalues are strictly positive.\nFor $\\lambda_{1}$, both $5$ and $\\sqrt{13}$ are positive, so $\\lambda_{1} > 0$.\nFor $\\lambda_{2}$, we need to check if $5 - \\sqrt{13} > 0$. This is equivalent to checking if $5 > \\sqrt{13}$, which is equivalent to $5^2 > 13$, or $25 > 13$. This is true.\nSince both $\\lambda_{1}$ and $\\lambda_{2}$ are strictly positive, the symmetric matrix $R$ is positive definite. This confirms the premise of the problem statement.\n\n### Part 3: Computation of the Spectral Condition Number $\\kappa_{2}(R)$\nThe spectral condition number (using the matrix $2$-norm) of a matrix $R$ is defined as $\\kappa_{2}(R) = \\|R\\|_{2} \\|R^{-1}\\|_{2}$.\nFor a symmetric positive definite matrix, the $2$-norm is equal to its largest eigenvalue ($\\lambda_{\\max}$). Therefore, $\\|R\\|_{2} = \\lambda_{\\max}(R)$.\nThe eigenvalues of the inverse matrix $R^{-1}$ are the reciprocals of the eigenvalues of $R$. Therefore, the largest eigenvalue of $R^{-1}$ is the reciprocal of the smallest eigenvalue of $R$, i.e., $\\lambda_{\\max}(R^{-1}) = 1/\\lambda_{\\min}(R)$.\nThus, for a symmetric positive definite matrix, the condition number simplifies to the ratio of the largest to the smallest eigenvalue:\n$$\n\\kappa_{2}(R) = \\frac{\\lambda_{\\max}(R)}{\\lambda_{\\min}(R)} = \\frac{\\lambda_{1}}{\\lambda_{2}}\n$$\nSubstituting the computed eigenvalues:\n$$\n\\kappa_{2}(R) = \\frac{\\frac{5 + \\sqrt{13}}{2}}{\\frac{5 - \\sqrt{13}}{2}} = \\frac{5 + \\sqrt{13}}{5 - \\sqrt{13}}\n$$\nTo express this in a simplified form, we rationalize the denominator by multiplying the numerator and denominator by the conjugate of the denominator, which is $5 + \\sqrt{13}$:\n$$\n\\kappa_{2}(R) = \\frac{5 + \\sqrt{13}}{5 - \\sqrt{13}} \\times \\frac{5 + \\sqrt{13}}{5 + \\sqrt{13}} = \\frac{(5 + \\sqrt{13})^2}{5^2 - (\\sqrt{13})^2}\n$$\nThe numerator expands to:\n$$\n(5 + \\sqrt{13})^2 = 5^2 + 2(5)(\\sqrt{13}) + (\\sqrt{13})^2 = 25 + 10\\sqrt{13} + 13 = 38 + 10\\sqrt{13}\n$$\nThe denominator is:\n$$\n5^2 - (\\sqrt{13})^2 = 25 - 13 = 12\n$$\nCombining these results:\n$$\n\\kappa_{2}(R) = \\frac{38 + 10\\sqrt{13}}{12}\n$$\nSimplifying the fraction by dividing the numerator and denominator by their greatest common divisor, which is $2$:\n$$\n\\kappa_{2}(R) = \\frac{19 + 5\\sqrt{13}}{6}\n$$\nThis is the exact spectral condition number. A high condition number suggests that the matrix is ill-conditioned, meaning its inverse is sensitive to small perturbations in the matrix entries, which can pose numerical challenges in the cost function minimization.\n\nThe final results are: the entries of $R^{-1}$, the eigenvalues $\\lambda_1, \\lambda_2$, and the condition number $\\kappa_2(R)$. These are collected into the required final format.\nThe four entries of $R^{-1}$ are $(R^{-1})_{11} = \\frac{1}{3}$, $(R^{-1})_{12} = -\\frac{1}{3}$, $(R^{-1})_{21} = -\\frac{1}{3}$, and $(R^{-1})_{22} = \\frac{4}{3}$.\n$\\lambda_{1} = \\frac{5+\\sqrt{13}}{2}$.\n$\\lambda_{2} = \\frac{5-\\sqrt{13}}{2}$.\n$\\kappa_{2}(R) = \\frac{19+5\\sqrt{13}}{6}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{3} & -\\frac{1}{3} & -\\frac{1}{3} & \\frac{4}{3} & \\frac{5 + \\sqrt{13}}{2} & \\frac{5 - \\sqrt{13}}{2} & \\frac{19 + 5\\sqrt{13}}{6} \\end{pmatrix}}$$", "id": "3406348"}, {"introduction": "Having established the essential properties of $R$, we now explore why modeling its off-diagonal structure is so critical. A common simplification is to ignore error correlations, which is equivalent to using a diagonal $R$ matrix. This practice [@problem_id:3406402] directly contrasts the optimal Generalized Least Squares (GLS) estimator, which uses the full $R$, against the suboptimal Ordinary Least Squares (OLS) estimator. Completing this exercise will allow you to quantify the loss in accuracy incurred by neglecting error correlations, providing a clear justification for more sophisticated covariance modeling.", "problem": "Consider a linear observation model in a data assimilation setting,\n$$\ny = H x_{\\text{true}} + \\varepsilon,\n$$\nwhere $x_{\\text{true}} \\in \\mathbb{R}^{2}$ is an unknown state vector, $y \\in \\mathbb{R}^{3}$ is the observed data, and $\\varepsilon \\sim \\mathcal{N}(0, R)$ represents observation errors with a known, non-diagonal covariance matrix $R$. The observation operator $H \\in \\mathbb{R}^{3 \\times 2}$ and the observation error covariance $R \\in \\mathbb{R}^{3 \\times 3}$ are given by\n$$\nH = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}, \\qquad\nR = \\begin{pmatrix}\n2 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 2\n\\end{pmatrix}.\n$$\nA single realization of the observation is\n$$\ny = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nStarting from the Gaussian likelihood for $\\varepsilon$ and assuming a flat prior on $x_{\\text{true}}$, perform the following:\n\n1) Derive, from first principles, the estimator $\\displaystyle x^{\\ast}$ that minimizes the quadratic form associated with the negative log-likelihood of the observation errors, and compute its value for the provided $H$, $R$, and $y$.\n\n2) Using linear estimator error propagation under the true observation error covariance $R$, derive the error covariance of the estimator in part 1). Then, consider the Ordinary Least Squares (OLS) estimator that ignores the off-diagonal structure of $R$ and uses identity weighting, and derive its error covariance under the true $R$. Use these results to compute the generalized variance ratio\n$$\n\\rho \\equiv \\frac{\\det\\!\\big(\\operatorname{Var}(x_{\\text{OLS}})\\big)}{\\det\\!\\big(\\operatorname{Var}(x^{\\ast})\\big)}.\n$$\n\nExpress the final answer for $\\rho$ as an exact fraction. No rounding is required. The final answer must be a single number.", "solution": "The observation model is $y = H x_{\\text{true}} + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, R)$. A flat prior on $x_{\\text{true}}$ implies that the maximum a posteriori estimate equals the maximum likelihood estimate. Under Gaussian errors, the negative log-likelihood (up to an additive constant independent of $x$) is given by the quadratic form\n$$\nJ(x) = (y - Hx)^{\\top} R^{-1} (y - Hx).\n$$\nThe estimator that minimizes $J(x)$ is the Generalized Least Squares (GLS) estimator. We derive it by setting the gradient to zero:\n$$\n\\nabla_x J(x) = -2 H^{\\top} R^{-1} (y - Hx) = 0\n\\quad \\Longrightarrow \\quad\nH^{\\top} R^{-1} H \\, x = H^{\\top} R^{-1} y.\n$$\nAssuming $H^{\\top} R^{-1} H$ is invertible, the minimizing solution is\n$$\nx^{\\ast} = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} y.\n$$\n\nWe now compute this for the provided $H$, $R$, and $y$.\n\nFirst, compute $R^{-1}$. For\n$$\nR = \\begin{pmatrix}\n2 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 2\n\\end{pmatrix},\n$$\none verifies that\n$$\nR^{-1} = \\frac{1}{4} \\begin{pmatrix}\n3 & -2 & 1 \\\\\n-2 & 4 & -2 \\\\\n1 & -2 & 3\n\\end{pmatrix}.\n$$\nNext, compute $H^{\\top} R^{-1} H$. It is convenient to compute $R^{-1} H$ first. Using\n$$\nH = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix},\n\\quad\nR^{-1} = \\frac{1}{4} \\begin{pmatrix}\n3 & -2 & 1 \\\\\n-2 & 4 & -2 \\\\\n1 & -2 & 3\n\\end{pmatrix},\n$$\nwe obtain\n$$\nR^{-1} H = \\frac{1}{4}\n\\begin{pmatrix}\n1 & -1 \\\\\n2 & 2 \\\\\n-1 & 1\n\\end{pmatrix}.\n$$\nTherefore,\n$$\nH^{\\top} R^{-1} H\n= H^{\\top} (R^{-1} H)\n= \\begin{pmatrix}\n\\frac{3}{4} & \\frac{1}{4} \\\\\n\\frac{1}{4} & \\frac{3}{4}\n\\end{pmatrix}.\n$$\nThe inverse is\n$$\n\\big(H^{\\top} R^{-1} H\\big)^{-1}\n= \\begin{pmatrix}\n\\frac{3}{2} & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & \\frac{3}{2}\n\\end{pmatrix},\n$$\nsince the determinant is $\\frac{1}{2}$ and the adjugate is\n$\\begin{pmatrix} \\frac{3}{4} & -\\frac{1}{4} \\\\ -\\frac{1}{4} & \\frac{3}{4} \\end{pmatrix}$.\n\nWe also need $H^{\\top} R^{-1} y$. With $y = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$,\n$$\nR^{-1} y\n= \\frac{1}{4}\n\\begin{pmatrix}\n3 \\\\\n-2 \\\\\n1\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{3}{4} \\\\\n-\\frac{1}{2} \\\\\n\\frac{1}{4}\n\\end{pmatrix},\n\\quad\nH^{\\top} R^{-1} y\n= \\begin{pmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{3}{4} \\\\\n-\\frac{1}{2} \\\\\n\\frac{1}{4}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{1}{4} \\\\\n-\\frac{1}{4}\n\\end{pmatrix}.\n$$\nHence,\n$$\nx^{\\ast} = \\begin{pmatrix}\n\\frac{3}{2} & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & \\frac{3}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{1}{4} \\\\\n-\\frac{1}{4}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{1}{2} \\\\\n-\\frac{1}{2}\n\\end{pmatrix}.\n$$\n\nWe now derive the error covariances under the true observation error covariance $R$. For a linear estimator $x = K y$, the estimation error is $x - x_{\\text{true}} = K(H x_{\\text{true}} + \\varepsilon) - x_{\\text{true}} = (K H - I)x_{\\text{true}} + K \\varepsilon$. For unbiasedness with respect to $x_{\\text{true}}$, we require $K H = I$, which is satisfied by both the Generalized Least Squares (GLS) and Ordinary Least Squares (OLS) choices below. Under $\\varepsilon \\sim \\mathcal{N}(0, R)$, the error covariance is\n$$\n\\operatorname{Var}(x) = K R K^{\\top}.\n$$\n\nFor GLS, the estimator is\n$$\nx^{\\ast} = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} y \\equiv K_{\\text{GLS}} y,\n$$\nwith $K_{\\text{GLS}} = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1}$. Then\n$$\n\\operatorname{Var}(x^{\\ast})\n= K_{\\text{GLS}} R K_{\\text{GLS}}^{\\top}\n= (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} R R^{-1} H (H^{\\top} R^{-1} H)^{-1}\n= (H^{\\top} R^{-1} H)^{-1}.\n$$\nWith the computed matrix, this is\n$$\n\\operatorname{Var}(x^{\\ast})\n= \\begin{pmatrix}\n\\frac{3}{2} & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & \\frac{3}{2}\n\\end{pmatrix}.\n$$\n\nFor OLS, which ignores the off-diagonal structure of $R$ and uses identity weighting, the estimator is\n$$\nx_{\\text{OLS}} = (H^{\\top} H)^{-1} H^{\\top} y \\equiv K_{\\text{OLS}} y,\n\\quad\nK_{\\text{OLS}} = (H^{\\top} H)^{-1} H^{\\top}.\n$$\nThen\n$$\n\\operatorname{Var}(x_{\\text{OLS}})\n= K_{\\text{OLS}} R K_{\\text{OLS}}^{\\top}\n= (H^{\\top} H)^{-1} H^{\\top} R H (H^{\\top} H)^{-1}.\n$$\nWe compute the needed matrices. First,\n$$\nH^{\\top} H\n= \\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix},\n\\quad\n(H^{\\top} H)^{-1}\n= \\frac{1}{3}\n\\begin{pmatrix}\n2 & -1 \\\\\n-1 & 2\n\\end{pmatrix}.\n$$\nNext,\n$$\nR H\n= \\begin{pmatrix}\n3 & 1 \\\\\n3 & 3 \\\\\n1 & 3\n\\end{pmatrix},\n\\quad\nH^{\\top} R H\n= \\begin{pmatrix}\n6 & 4 \\\\\n4 & 6\n\\end{pmatrix}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(x_{\\text{OLS}})\n= \\frac{1}{3}\n\\begin{pmatrix}\n2 & -1 \\\\\n-1 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\n6 & 4 \\\\\n4 & 6\n\\end{pmatrix}\n\\frac{1}{3}\n\\begin{pmatrix}\n2 & -1 \\\\\n-1 & 2\n\\end{pmatrix}\n= \\frac{1}{9}\n\\begin{pmatrix}\n14 & -4 \\\\\n-4 & 14\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{14}{9} & -\\frac{4}{9} \\\\\n-\\frac{4}{9} & \\frac{14}{9}\n\\end{pmatrix}.\n$$\n\nWe now compute the generalized variance ratio\n$$\n\\rho \\equiv \\frac{\\det\\!\\big(\\operatorname{Var}(x_{\\text{OLS}})\\big)}{\\det\\!\\big(\\operatorname{Var}(x^{\\ast})\\big)}.\n$$\nFor a $2 \\times 2$ symmetric matrix\n$\\begin{pmatrix} a & c \\\\ c & a \\end{pmatrix}$, the determinant is $a^{2} - c^{2}$.\n\nFor $\\operatorname{Var}(x^{\\ast})$ we have $a = \\frac{3}{2}$ and $c = -\\frac{1}{2}$, so\n$$\n\\det\\!\\big(\\operatorname{Var}(x^{\\ast})\\big)\n= \\left(\\frac{3}{2}\\right)^{2} - \\left(-\\frac{1}{2}\\right)^{2}\n= \\frac{9}{4} - \\frac{1}{4}\n= 2.\n$$\nFor $\\operatorname{Var}(x_{\\text{OLS}})$ we have $a = \\frac{14}{9}$ and $c = -\\frac{4}{9}$, so\n$$\n\\det\\!\\big(\\operatorname{Var}(x_{\\text{OLS}})\\big)\n= \\left(\\frac{14}{9}\\right)^{2} - \\left(-\\frac{4}{9}\\right)^{2}\n= \\frac{196}{81} - \\frac{16}{81}\n= \\frac{180}{81}\n= \\frac{20}{9}.\n$$\nTherefore,\n$$\n\\rho = \\frac{\\frac{20}{9}}{2} = \\frac{10}{9}.\n$$\nThis ratio exceeds $1$, indicating that properly accounting for correlated observation errors via the non-diagonal $R$ reduces the generalized variance of the estimator relative to Ordinary Least Squares (OLS).", "answer": "$$\\boxed{\\frac{10}{9}}$$", "id": "3406402"}, {"introduction": "In realistic, high-dimensional systems, constructing a full and accurate observation error covariance matrix $R$ is a formidable challenge. A common approach is to use a covariance model, but this can introduce spurious long-range correlations that degrade the analysis. This final, computational practice [@problem_id:3406395] introduces covariance localization, a state-of-the-art technique to mitigate this issue. By implementing this exercise, you will gain hands-on experience in building structured covariance matrices, applying a localization taper, and numerically evaluating how this modeling choice impacts the uncertainty of the final state estimate.", "problem": "Consider a linear Gaussian inverse problem in one spatial dimension with a finite grid. Let the unknown state vector be $x \\in \\mathbb{R}^n$, with a Gaussian prior $x \\sim \\mathcal{N}(x_b, B)$, and observations $y \\in \\mathbb{R}^m$ generated by the linear model $y = H x + \\varepsilon$, where $H \\in \\mathbb{R}^{m \\times n}$ is the observation operator, and the observation error $\\varepsilon \\sim \\mathcal{N}(0, R)$ is independent of $x$. The grid points are at positions $x_i = i$ for $i \\in \\{0,1,\\dots,n-1\\}$. The prior covariance $B \\in \\mathbb{R}^{n \\times n}$ is stationary with an exponential kernel, and the true observation error covariance $R \\in \\mathbb{R}^{m \\times m}$ is also stationary with an exponential kernel, both with finite correlation lengths.\n\nFundamental base:\n- The linear-Gaussian Bayes rule implies that the posterior (analysis) is Gaussian, and arises from the combination of the prior quadratic form and the likelihood quadratic form. From this, one can derive the analysis covariance by completing the square.\n- The Schur product theorem states that the element-wise (Hadamard) product of two positive semidefinite matrices is positive semidefinite.\n\nObservation error covariance modeling and localization:\n- The true observation error covariance has entries\n$$\nR_{ij}^{\\text{true}} = \\sigma_r^2 \\exp\\!\\left(-\\frac{|s_i - s_j|}{L_r}\\right),\n$$\nwhere $s_i$ and $s_j$ are the spatial positions of the observed components, $\\sigma_r^2$ is the variance, and $L_r$ is the correlation length.\n- A localized observation error covariance is constructed as\n$$\nR_{\\text{loc}} = R_{\\text{true}} \\odot C,\n$$\nwhere $\\odot$ denotes the Hadamard product and $C$ is a taper matrix defined by a compactly supported positive definite function. Use the Wendland $C^2$ taper\n$$\n\\phi(q) =\n\\begin{cases}\n(1 - q)^4 (1 + 4 q), & 0 \\le q \\le 1,\\\\\n0, & q > 1,\n\\end{cases}\n$$\nwith $q = \\frac{|s_i - s_j|}{L_{\\text{loc}}}$ and entries $C_{ij} = \\phi\\!\\left(\\frac{|s_i - s_j|}{L_{\\text{loc}}}\\right)$. By convention, take $L_{\\text{loc}} = \\infty$ to mean $C_{ij} = 1$ for all $i,j$ (no localization), and $L_{\\text{loc}} = 0$ to mean $C = I$ (diagonalization).\n\nTask:\n1. Using only the fundamental base above, derive the analysis covariance $A_\\star$ that uses the true $R_{\\text{true}}$, and the analysis covariance $A_{\\text{loc}}$ that uses the localized $R_{\\text{loc}}$.\n2. Quantify the bias in the analysis covariance induced by localization via the relative Frobenius norm\n$$\n\\beta = \\frac{\\|A_{\\text{loc}} - A_\\star\\|_F}{\\|A_\\star\\|_F}.\n$$\n3. Implement a program to compute $\\beta$ for each test case in the suite below. The program should construct $B$, $H$, $R_{\\text{true}}$, and $R_{\\text{loc}}$ from the given parameters, then compute $A_\\star$ and $A_{\\text{loc}}$, and finally output the list of $\\beta$ values.\n\nCovariance construction details:\n- For the prior,\n$$\nB_{ij} = \\sigma_b^2 \\exp\\!\\left(-\\frac{|x_i - x_j|}{L_b}\\right), \\quad i,j \\in \\{0,1,\\dots,n-1\\}.\n$$\n- For $R_{\\text{true}}$, compute positions of observed indices $s_i$ and apply the same exponential form as above with parameters $(\\sigma_r, L_r)$.\n- For $R_{\\text{loc}}$, apply the Wendland $C^2$ taper as described to $R_{\\text{true}}$.\n\nAngle units are not involved. No physical units are required. All outputs must be real numbers.\n\nTest suite:\n- Case $1$ (happy path, no localization):\n  - $n = 20$, observed indices $\\{0,1,2,\\dots,19\\}$.\n  - $\\sigma_b = 1$, $L_b = 2$.\n  - $\\sigma_r = 0.5$, $L_r = 3$.\n  - $L_{\\text{loc}} = \\infty$.\n- Case $2$ (moderate localization):\n  - $n = 20$, observed indices $\\{0,1,2,\\dots,19\\}$.\n  - $\\sigma_b = 1$, $L_b = 2$.\n  - $\\sigma_r = 0.5$, $L_r = 3$.\n  - $L_{\\text{loc}} = 2$.\n- Case $3$ (strong localization to diagonal):\n  - $n = 20$, observed indices $\\{0,1,2,\\dots,19\\}$.\n  - $\\sigma_b = 1$, $L_b = 2$.\n  - $\\sigma_r = 0.5$, $L_r = 3$.\n  - $L_{\\text{loc}} = 0$.\n- Case $4$ (sparser observations with localization):\n  - $n = 20$, observed indices $\\{0,2,4,6,8,10,12,14,16,18\\}$.\n  - $\\sigma_b = 1$, $L_b = 2$.\n  - $\\sigma_r = 0.5$, $L_r = 3$.\n  - $L_{\\text{loc}} = 1$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $4$, for example, $[\\beta_1,\\beta_2,\\beta_3,\\beta_4]$.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the established theory of Bayesian inverse problems, is well-posed with all necessary information provided, and is stated in objective, formal language. We may proceed with the solution.\n\nThe task is to derive the analysis covariance matrices for a linear Gaussian inverse problem under two different assumptions for the observation error covariance, and then to quantify the resulting bias in the analysis covariance.\n\n### Part 1: Derivation of the Analysis Covariance\n\nThe foundation of the solution lies in Bayes' theorem, which relates the posterior probability of the state $x$ given observations $y$, $p(x|y)$, to the likelihood $p(y|x)$ and the prior $p(x)$:\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\nThe problem specifies a Gaussian prior $x \\sim \\mathcal{N}(x_b, B)$ and a Gaussian observation error $\\varepsilon \\sim \\mathcal{N}(0, R)$ in the linear model $y = Hx + \\varepsilon$. The corresponding probability density functions are:\n- Prior: $p(x) \\propto \\exp\\left(-\\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b)\\right)$\n- Likelihood: $p(y|x) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\\right)$\n\nSince the product of two Gaussian functions (in terms of $x$) is another Gaussian function, the posterior $p(x|y)$ is also Gaussian. We can write it as $x|y \\sim \\mathcal{N}(x_a, A)$, where $x_a$ is the analysis state (posterior mean) and $A$ is the analysis covariance (posterior covariance).\n\nThe exponent of the posterior density is the sum of the exponents of the prior and the likelihood. The negative of this exponent defines the cost function $J(x)$, which is proportional to the negative log-posterior:\n$$\nJ(x) = \\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b) + \\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\n$$\nFor a Gaussian distribution $\\mathcal{N}(\\mu, \\Sigma)$, the quadratic form in the exponent is $\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1}(x-\\mu)$. The inverse of the covariance matrix, $\\Sigma^{-1}$, is the Hessian of this quadratic form with respect to $x$. Therefore, the inverse of the analysis covariance, $A^{-1}$, is the Hessian of $J(x)$ with respect to $x$.\n\nTo find the Hessian, we first expand $J(x)$:\n$$\nJ(x) = \\frac{1}{2}(x^T B^{-1} x - 2x_b^T B^{-1} x + \\text{const}) + \\frac{1}{2}(x^T H^T R^{-1} H x - 2y^T R^{-1} H x + \\text{const})\n$$\nThe gradient of $J(x)$ with respect to $x$ is:\n$$\n\\nabla_x J(x) = B^{-1}(x - x_b) - H^T R^{-1}(y - Hx) = (B^{-1} + H^T R^{-1} H)x - (B^{-1}x_b + H^T R^{-1}y)\n$$\nThe Hessian is the gradient of $\\nabla_x J(x)$ with respect to $x$:\n$$\n\\nabla_x^2 J(x) = A^{-1} = B^{-1} + H^T R^{-1} H\n$$\nInverting this expression gives the formula for the analysis covariance $A$:\n$$\nA = \\left(B^{-1} + H^T R^{-1} H\\right)^{-1}\n$$\nThis derivation uses only the fundamental principles specified in the problem statement.\n\nWe can now define the two analysis covariances required:\n1.  The true analysis covariance, $A_\\star$, is derived using the true observation error covariance, $R_{\\text{true}}$:\n    $$\n    A_\\star = \\left(B^{-1} + H^T R_{\\text{true}}^{-1} H\\right)^{-1}\n    $$\n2.  The localized analysis covariance, $A_{\\text{loc}}$, is derived using the localized observation error covariance, $R_{\\text{loc}} = R_{\\text{true}} \\odot C$:\n    $$\n    A_{\\text{loc}} = \\left(B^{-1} + H^T R_{\\text{loc}}^{-1} H\\right)^{-1}\n    $$\nThe Schur product theorem, which states that the Hadamard product of two positive semidefinite matrices is positive semidefinite, ensures that $R_{\\text{loc}}$ is a valid covariance matrix, as both $R_{\\text{true}}$ and $C$ are constructed from positive definite functions and are thus positive definite (or semidefinite for $C$).\n\n### Part 2: Bias Quantification\n\nThe bias induced by using $R_{\\text{loc}}$ instead of $R_{\\text{true}}$ is quantified by the relative Frobenius norm of the difference between the resulting analysis covariances:\n$$\n\\beta = \\frac{\\|A_{\\text{loc}} - A_\\star\\|_F}{\\|A_\\star\\|_F}\n$$\nThe Frobenius norm of an $n \\times n$ matrix $M$ is defined as $\\|M\\|_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^n |M_{ij}|^2}$.\n\n### Part 3: Algorithmic Implementation\n\nThe computation of $\\beta$ for each test case proceeds as follows:\n\n1.  **Parameter Initialization**: For each case, set the values of $n$, the observed indices, $\\sigma_b$, $L_b$, $\\sigma_r$, $L_r$, and $L_{\\text{loc}}$.\n2.  **Matrix Construction**:\n    -   Generate the vector of grid point positions $x_{pos} = [0, 1, \\dots, n-1]$.\n    -   Construct the $n \\times n$ prior covariance matrix $B$ using the formula $B_{ij} = \\sigma_b^2 \\exp(-\\frac{|x_i - x_j|}{L_b})$.\n    -   Determine the number of observations, $m$, and construct the $m \\times n$ observation operator $H$ as a selection matrix that maps state vector components to the observed components.\n    -   Generate the vector of observed positions $s_{pos}$ from $x_{pos}$ and the observed indices.\n    -   Construct the $m \\times m$ true observation error covariance $R_{\\text{true}}$ using $R_{ij}^{\\text{true}} = \\sigma_r^2 \\exp(-\\frac{|s_i - s_j|}{L_r})$.\n    -   Construct the $m \\times m$ localized observation error covariance $R_{\\text{loc}}$:\n        -   If $L_{\\text{loc}} = \\infty$, set $R_{\\text{loc}} = R_{\\text{true}}$.\n        -   If $L_{\\text{loc}} = 0$, set $R_{\\text{loc}}$ to be a diagonal matrix with diagonal entries equal to those of $R_{\\text{true}}$, which simplifies to $\\sigma_r^2 I$.\n        -   Otherwise, construct the $m \\times m$ taper matrix $C$ using the Wendland $C^2$ function, $C_{ij} = \\phi(\\frac{|s_i - s_j|}{L_{\\text{loc}}})$, and compute the Hadamard product $R_{\\text{loc}} = R_{\\text{true}} \\odot C$.\n3.  **Analysis Covariance Calculation**:\n    -   Compute the matrix inverses $B^{-1}$, $R_{\\text{true}}^{-1}$, and $R_{\\text{loc}}^{-1}$.\n    -   Calculate $A_\\star = (B^{-1} + H^T R_{\\text{true}}^{-1} H)^{-1}$.\n    -   Calculate $A_{\\text{loc}} = (B^{-1} + H^T R_{\\text{loc}}^{-1} H)^{-1}$.\n4.  **Bias Calculation**:\n    -   Compute the Frobenius norms $\\|A_{\\text{loc}} - A_\\star\\|_F$ and $\\|A_\\star\\|_F$.\n    -   Calculate their ratio $\\beta$. For the case $L_{\\text{loc}} = \\infty$, $A_{\\text{loc}} = A_\\star$, so $\\beta = 0$ exactly.\n5.  **Output**: Collect the $\\beta$ values from all test cases into a list and format the output as requested.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_beta(n, observed_indices, sigma_b, L_b, sigma_r, L_r, L_loc):\n    \"\"\"\n    Calculates the bias in analysis covariance due to localization.\n\n    Args:\n        n (int): Dimension of the state vector.\n        observed_indices (list): Indices of the observed state components.\n        sigma_b (float): Standard deviation for the prior covariance.\n        L_b (float): Correlation length for the prior covariance.\n        sigma_r (float): Standard deviation for the observation error covariance.\n        L_r (float): Correlation length for the observation error covariance.\n        L_loc (float): Correlation length for the localization taper.\n\n    Returns:\n        float: The relative Frobenius norm of the bias, beta.\n    \"\"\"\n    # 1. Define grid and observation operator H\n    x_pos = np.arange(n, dtype=float)\n    obs_indices_arr = np.array(observed_indices, dtype=int)\n    m = len(obs_indices_arr)\n    H = np.zeros((m, n), dtype=float)\n    if m > 0:\n        H[np.arange(m), obs_indices_arr] = 1.0\n\n    # 2. Construct prior covariance B\n    dist_matrix_b = np.abs(x_pos[:, None] - x_pos[None, :])\n    B = sigma_b**2 * np.exp(-dist_matrix_b / L_b)\n\n    # 3. Construct true observation error covariance R_true\n    s_pos = x_pos[obs_indices_arr]\n    dist_matrix_r = np.abs(s_pos[:, None] - s_pos[None, :])\n    R_true = sigma_r**2 * np.exp(-dist_matrix_r / L_r)\n\n    # 4. Construct localized observation error covariance R_loc\n    if np.isinf(L_loc):\n        R_loc = R_true\n    elif L_loc == 0.0:\n        R_loc = np.diag(np.diag(R_true))\n    else:\n        q = dist_matrix_r / L_loc\n        # Wendland C^2 taper function: phi(q) = (1-q)^4 * (1+4q) for 0 = q = 1\n        phi = np.zeros_like(q, dtype=float)\n        mask = (q >= 0)  (q = 1)\n        q_masked = q[mask]\n        phi[mask] = (1 - q_masked)**4 * (1 + 4 * q_masked)\n        C = phi\n        R_loc = R_true * C  # Hadamard product\n\n    # 5. Compute analysis covariances A_star and A_loc\n    # Formula: A = (B_inv + H.T @ R_inv @ H)^-1\n    try:\n        B_inv = np.linalg.inv(B)\n        H_T = H.T\n\n        R_true_inv = np.linalg.inv(R_true)\n        A_star_inv = B_inv + H_T @ R_true_inv @ H\n        A_star = np.linalg.inv(A_star_inv)\n        \n        # When L_loc=inf, A_loc is A_star, so beta=0.\n        if np.isinf(L_loc):\n            return 0.0\n\n        R_loc_inv = np.linalg.inv(R_loc)\n        A_loc_inv = B_inv + H_T @ R_loc_inv @ H\n        A_loc = np.linalg.inv(A_loc_inv)\n\n    except np.linalg.LinAlgError:\n        # Should not happen with given parameters, but robust code handles it.\n        return np.nan\n\n    # 6. Quantify bias beta\n    diff_norm = np.linalg.norm(A_loc - A_star, 'fro')\n    star_norm = np.linalg.norm(A_star, 'fro')\n    \n    if star_norm == 0:\n        return np.inf if diff_norm > 0 else 0.0\n    \n    beta = diff_norm / star_norm\n    return beta\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, no localization)\n        {'n': 20, 'observed_indices': list(range(20)), 'sigma_b': 1.0, 'L_b': 2.0, 'sigma_r': 0.5, 'L_r': 3.0, 'L_loc': np.inf},\n        # Case 2 (moderate localization)\n        {'n': 20, 'observed_indices': list(range(20)), 'sigma_b': 1.0, 'L_b': 2.0, 'sigma_r': 0.5, 'L_r': 3.0, 'L_loc': 2.0},\n        # Case 3 (strong localization to diagonal)\n        {'n': 20, 'observed_indices': list(range(20)), 'sigma_b': 1.0, 'L_b': 2.0, 'sigma_r': 0.5, 'L_r': 3.0, 'L_loc': 0.0},\n        # Case 4 (sparser observations with localization)\n        {'n': 20, 'observed_indices': list(range(0, 20, 2)), 'sigma_b': 1.0, 'L_b': 2.0, 'sigma_r': 0.5, 'L_r': 3.0, 'L_loc': 1.0}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_beta(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3406395"}]}