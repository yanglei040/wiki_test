{"hands_on_practices": [{"introduction": "The foundation of the Cramér-Rao bound lies in the Fisher information, which quantifies the amount of information that observable data carries about an unknown parameter. This first exercise moves beyond the typical Gaussian assumption to derive the Fisher information matrix from first principles for data following a Poisson distribution [@problem_id:3381487]. By working through this example, you will solidify your understanding of the link between the log-likelihood, the score function, and the information matrix, and explore its implications in practical scenarios like photon-limited imaging or event counting.", "problem": "In a photon-limited data assimilation setting, suppose $n$ independent sensor counts $y_{i}$, for $i \\in \\{1,\\dots,n\\}$, follow $y_{i} \\sim \\mathrm{Poisson}(\\lambda_{i}(\\theta))$, where $\\theta \\in \\mathbb{R}^{p}$ is an unknown parameter vector and each forward rate function $\\lambda_{i} : \\mathbb{R}^{p} \\to \\mathbb{R}_{>0}$ is continuously differentiable. Work from the first principles of likelihood-based inference, starting from the definition of the Fisher information matrix as the expectation of the outer product of the score under the data model, and derive a general expression for the Fisher information matrix $I(\\theta)$ in terms of $\\{\\lambda_{i}(\\theta)\\}_{i=1}^{n}$ and their gradients with respect to $\\theta$. Then specialize to the log-linear forward model $\\lambda_{i}(\\theta) = \\exp(x_{i}^{\\top}\\theta)$ with $x_{i} \\in \\mathbb{R}^{p}$ known.\n\nNext, consider $p=2$ and $n=3$ with\n$$\nx_{1}=\\begin{pmatrix}1 \\\\ 0.5\\end{pmatrix},\\quad x_{2}=\\begin{pmatrix}1 \\\\ 1\\end{pmatrix},\\quad x_{3}=\\begin{pmatrix}1 \\\\ 2\\end{pmatrix},\n$$\nand the true parameter $\\theta_{0}=\\begin{pmatrix}-2 \\\\ -1\\end{pmatrix}$. Using your expression for the Fisher information, compute the Cramér–Rao lower bound (CRLB) for any unbiased estimator of the linear functional $c^{\\top}\\theta$ with $c=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, evaluated at $\\theta=\\theta_{0}$. Express your final numerical answer as a single real number, rounded to $4$ significant figures.\n\nFinally, briefly discuss, without altering your numerical result, how the low-count regime (small $\\lambda_{i}(\\theta)$) influences the Fisher information and the practical tightness of the Cramér–Rao lower bound in finite-sample inverse problems and data assimilation.", "solution": "We begin from the definition of the Fisher information matrix for a parametric family with parameter $\\theta \\in \\mathbb{R}^{p}$,\n$$\nI(\\theta) \\equiv \\mathbb{E}_{\\theta}\\!\\left[\\,\\nabla_{\\theta} \\ell(\\theta; y)\\,\\nabla_{\\theta} \\ell(\\theta; y)^{\\top}\\right],\n$$\nwhere $\\ell(\\theta; y)$ is the log-likelihood, and the expectation is with respect to the data distribution at parameter $\\theta$. For independent observations $\\{y_{i}\\}_{i=1}^{n}$ with $y_{i} \\sim \\mathrm{Poisson}(\\lambda_{i}(\\theta))$, the joint likelihood is\n$$\nL(\\theta; y)=\\prod_{i=1}^{n}\\frac{\\exp(-\\lambda_{i}(\\theta))\\,\\lambda_{i}(\\theta)^{y_{i}}}{y_{i}!},\\qquad \\ell(\\theta; y)=\\sum_{i=1}^{n}\\left[-\\lambda_{i}(\\theta)+y_{i}\\ln \\lambda_{i}(\\theta)-\\ln(y_{i}!)\\right].\n$$\nDifferentiate to obtain the score:\n$$\n\\nabla_{\\theta}\\ell(\\theta; y)=\\sum_{i=1}^{n}\\left[-\\nabla_{\\theta}\\lambda_{i}(\\theta)+y_{i}\\,\\nabla_{\\theta}\\ln \\lambda_{i}(\\theta)\\right]\n=\\sum_{i=1}^{n}\\left[-\\nabla_{\\theta}\\lambda_{i}(\\theta)+\\frac{y_{i}}{\\lambda_{i}(\\theta)}\\,\\nabla_{\\theta}\\lambda_{i}(\\theta)\\right]\n=\\sum_{i=1}^{n}\\frac{y_{i}-\\lambda_{i}(\\theta)}{\\lambda_{i}(\\theta)}\\,\\nabla_{\\theta}\\lambda_{i}(\\theta).\n$$\nUsing independence, the expectation of the outer product of the sum is the sum of expectations of the outer products, because cross-terms vanish due to zero covariance across $i \\neq j$:\n$$\nI(\\theta)=\\sum_{i=1}^{n}\\mathbb{E}_{\\theta}\\!\\left[\\left(\\frac{y_{i}-\\lambda_{i}(\\theta)}{\\lambda_{i}(\\theta)}\\right)^{2}\\right]\\nabla_{\\theta}\\lambda_{i}(\\theta)\\,\\nabla_{\\theta}\\lambda_{i}(\\theta)^{\\top}.\n$$\nFor a Poisson random variable, $\\mathrm{Var}(y_{i})=\\lambda_{i}(\\theta)$ and $\\mathbb{E}\\!\\left[(y_{i}-\\lambda_{i}(\\theta))^{2}\\right]=\\lambda_{i}(\\theta)$. Therefore,\n$$\nI(\\theta)=\\sum_{i=1}^{n}\\frac{1}{\\lambda_{i}(\\theta)}\\,\\nabla_{\\theta}\\lambda_{i}(\\theta)\\,\\nabla_{\\theta}\\lambda_{i}(\\theta)^{\\top}.\n$$\n\nSpecialize to the log-linear model $\\lambda_{i}(\\theta)=\\exp(x_{i}^{\\top}\\theta)$ with $x_{i}\\in\\mathbb{R}^{p}$. Then\n$$\n\\nabla_{\\theta}\\lambda_{i}(\\theta)=\\exp(x_{i}^{\\top}\\theta)\\,x_{i}=\\lambda_{i}(\\theta)\\,x_{i},\n$$\nand hence\n$$\nI(\\theta)=\\sum_{i=1}^{n}\\lambda_{i}(\\theta)\\,x_{i}x_{i}^{\\top}.\n$$\n\nNow take $p=2$, $n=3$ with\n$$\nx_{1}=\\begin{pmatrix}1\\\\ 0.5\\end{pmatrix},\\quad x_{2}=\\begin{pmatrix}1\\\\ 1\\end{pmatrix},\\quad x_{3}=\\begin{pmatrix}1\\\\ 2\\end{pmatrix},\\quad \\theta_{0}=\\begin{pmatrix}-2\\\\ -1\\end{pmatrix}.\n$$\nEvaluate $\\lambda_{i}(\\theta_{0})=\\exp(x_{i}^{\\top}\\theta_{0})$:\n$$\nx_{1}^{\\top}\\theta_{0}=-2-0.5=-2.5,\\quad \\lambda_{1}=\\exp(-2.5),\n$$\n$$\nx_{2}^{\\top}\\theta_{0}=-2-1=-3,\\quad \\lambda_{2}=\\exp(-3),\n$$\n$$\nx_{3}^{\\top}\\theta_{0}=-2-2=-4,\\quad \\lambda_{3}=\\exp(-4).\n$$\nThus,\n$$\nI(\\theta_{0})=\\lambda_{1}\\begin{pmatrix}1 & 0.5 \\\\ 0.5 & 0.25\\end{pmatrix}+\\lambda_{2}\\begin{pmatrix}1 & 1 \\\\ 1 & 1\\end{pmatrix}+\\lambda_{3}\\begin{pmatrix}1 & 2 \\\\ 2 & 4\\end{pmatrix}.\n$$\nNumerically,\n$$\n\\lambda_{1}=\\exp(-2.5)\\approx 0.08208499862,\\quad \\lambda_{2}=\\exp(-3)\\approx 0.04978706837,\\quad \\lambda_{3}=\\exp(-4)\\approx 0.01831563889,\n$$\nso\n$$\nI(\\theta_{0})\\approx\n\\begin{pmatrix}\n0.15018770588 & 0.12746084546\\\\\n0.12746084546 & 0.14357087359\n\\end{pmatrix}.\n$$\n\nWe seek the Cramér–Rao lower bound (CRLB) for any unbiased estimator of $c^{\\top}\\theta$ with $c=\\begin{pmatrix}0\\\\ 1\\end{pmatrix}$. For a differentiable regular model, the CRLB for a linear functional is\n$$\n\\mathrm{Var}(\\widehat{c^{\\top}\\theta})\\;\\ge\\; c^{\\top}I(\\theta)^{-1}c.\n$$\nHere this equals the $(2,2)$ element of $I(\\theta_{0})^{-1}$. For a symmetric $2\\times 2$ matrix $I=\\begin{pmatrix}a & b\\\\ b & d\\end{pmatrix}$, we have\n$$\nI^{-1}=\\frac{1}{ad-b^{2}}\\begin{pmatrix}d & -b\\\\ -b & a\\end{pmatrix},\\quad \\Rightarrow\\quad \\left[I^{-1}\\right]_{22}=\\frac{a}{ad-b^{2}}.\n$$\nUsing $a=0.15018770588$, $b=0.12746084546$, $d=0.14357087359$, compute\n$$\nad\\approx 0.02156258014,\\quad b^{2}\\approx 0.01624626712,\\quad ad-b^{2}\\approx 0.00531631302,\n$$\nso\n$$\nc^{\\top}I(\\theta_{0})^{-1}c=\\frac{a}{ad-b^{2}}\\approx \\frac{0.15018770588}{0.00531631302}\\approx 28.25035.\n$$\nRounding to $4$ significant figures gives $28.25$.\n\nDiscussion of low-count regime: When $\\lambda_{i}(\\theta)$ are small, $I(\\theta)=\\sum_{i}\\lambda_{i}(\\theta)\\,x_{i}x_{i}^{\\top}$ becomes small in the positive semidefinite order, causing its inverse to inflate and the Cramér–Rao lower bound $c^{\\top}I(\\theta)^{-1}c$ to grow. Thus, low photon counts or sparse events degrade local identifiability and inflate variance lower bounds. Moreover, the asymptotic normal approximations underlying the Cramér–Rao lower bound may be less accurate in finite samples when many $y_{i}$ equal $0$, leading to skewed likelihoods and potentially poor tightness of the bound. In such regimes, exact or nonasymptotic bounds (for example, the Barankin bound or the Chapman–Robbins bound) and regularization-informed priors in Bayesian data assimilation can be more informative, while reparameterizations ensuring positivity, such as the log link used here, maintain differentiability but do not eliminate the fundamental information scarcity induced by small $\\lambda_{i}(\\theta)$.", "answer": "$$\\boxed{28.25}$$", "id": "3381487"}, {"introduction": "In many real-world data assimilation problems, we fuse measurements from multiple sensors whose errors are not independent. This exercise explores the critical impact of error correlation on the precision of a parameter estimate [@problem_id:3381472]. By comparing the true Cramér-Rao bound with a naive bound that wrongfully assumes independent errors, you will gain a quantitative appreciation for why accurate error covariance modeling is essential for realistic uncertainty quantification.", "problem": "Consider a scalar geophysical state parameter $\\theta$ to be estimated from two satellite radiometer observations collected over the same footprint. The data assimilation operator is linear, so the observation model is $y = H \\theta + \\varepsilon$, where $y \\in \\mathbb{R}^{2}$, $H = \\begin{pmatrix} h_{1} \\\\ h_{2} \\end{pmatrix}$, and the observation error $\\varepsilon \\in \\mathbb{R}^{2}$ is zero-mean Gaussian with covariance matrix $\\Sigma$. The variances of the two sensors are $\\sigma_{1}^{2}$ and $\\sigma_{2}^{2}$, and their error correlation is described by a correlation coefficient $\\rho \\in (-1,1)$, so\n$$\n\\Sigma = \\begin{pmatrix}\n\\sigma_{1}^{2} & \\rho\\,\\sigma_{1}\\sigma_{2} \\\\\n\\rho\\,\\sigma_{1}\\sigma_{2} & \\sigma_{2}^{2}\n\\end{pmatrix}.\n$$\nIn some operational settings, off-diagonal correlations are ignored and the covariance is misspecified as diagonal,\n$$\n\\Sigma_{\\text{miss}} = \\begin{pmatrix}\n\\sigma_{1}^{2} & 0 \\\\\n0 & \\sigma_{2}^{2}\n\\end{pmatrix}.\n$$\nStarting from the definition of the log-likelihood for a multivariate normal model and the definition of Fisher information (FI) and the Cramér–Rao bound (CRB), derive the scalar FI for $\\theta$ under the correctly specified model and under the misspecified model that ignores off-diagonal correlations. Then, for the specific case\n$$\nh_{1} = 1.1,\\quad h_{2} = 0.9,\\quad \\sigma_{1} = 0.4,\\quad \\sigma_{2} = 0.5,\\quad \\rho = 0.6,\n$$\ncompute the ratio of the true CRB to the misspecified CRB,\n$$\nR \\equiv \\frac{\\text{CRB}_{\\text{true}}}{\\text{CRB}_{\\text{miss}}}.\n$$\nReport $R$ as a single real number rounded to four significant figures. No units are required for the ratio.", "solution": "The problem requires the derivation of the Fisher information (FI) and the Cramér–Rao bound (CRB) for a scalar parameter $\\theta$ under two different assumptions for the error covariance matrix, and then to compute the ratio of these bounds for a specific set of parameters.\n\nThe observation model is given by the linear equation $y = H \\theta + \\varepsilon$, where $y \\in \\mathbb{R}^{2}$ is the observation vector, $H \\in \\mathbb{R}^{2 \\times 1}$ is the observation operator, $\\theta \\in \\mathbb{R}$ is the scalar state parameter to be estimated, and $\\varepsilon \\in \\mathbb{R}^{2}$ is the observation error. The error $\\varepsilon$ is assumed to be a zero-mean Gaussian random variable, so its probability density function is $p(\\varepsilon) = \\frac{1}{\\sqrt{(2\\pi)^2 \\det(\\Sigma_{gen})}} \\exp\\left(-\\frac{1}{2} \\varepsilon^T \\Sigma_{gen}^{-1} \\varepsilon\\right)$, where $\\Sigma_{gen}$ is a generic covariance matrix.\n\nFrom the observation model, the vector $y$ is also a Gaussian random variable with mean $E[y] = H\\theta$ and covariance $\\Sigma_{gen}$. The likelihood function for $\\theta$ given an observation $y$ is the probability density function of $y$ evaluated at the observed value:\n$$\nL(\\theta; y) = p(y|\\theta) = \\frac{1}{\\sqrt{(2\\pi)^2 \\det(\\Sigma_{gen})}} \\exp\\left(-\\frac{1}{2} (y - H\\theta)^T \\Sigma_{gen}^{-1} (y - H\\theta)\\right)\n$$\nThe log-likelihood function, $\\ln L(\\theta; y)$, is:\n$$\n\\ln L(\\theta; y) = -\\ln(2\\pi) - \\frac{1}{2}\\ln(\\det(\\Sigma_{gen})) - \\frac{1}{2}(y-H\\theta)^T \\Sigma_{gen}^{-1} (y-H\\theta)\n$$\nThe Fisher information $I(\\theta)$ for a scalar parameter $\\theta$ is defined as the negative of the expected value of the second derivative of the log-likelihood function with respect to $\\theta$:\n$$\nI(\\theta) = -E\\left[\\frac{\\partial^2 \\ln L(\\theta; y)}{\\partial \\theta^2}\\right]\n$$\nFirst, we compute the derivatives of the log-likelihood function. The first derivative is:\n$$\n\\frac{\\partial \\ln L}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left(-\\frac{1}{2}(y-H\\theta)^T \\Sigma_{gen}^{-1} (y-H\\theta)\\right) = -\\frac{1}{2} \\left( 2(-H^T) \\Sigma_{gen}^{-1} (y-H\\theta) \\right) = H^T \\Sigma_{gen}^{-1} (y-H\\theta)\n$$\nThe second derivative is:\n$$\n\\frac{\\partial^2 \\ln L}{\\partial \\theta^2} = \\frac{\\partial}{\\partial \\theta} \\left(H^T \\Sigma_{gen}^{-1} (y-H\\theta)\\right) = H^T \\Sigma_{gen}^{-1} (-H) = -H^T \\Sigma_{gen}^{-1} H\n$$\nSince the second derivative is not a function of the observation $y$, its expectation is simply itself. Therefore, the Fisher information is:\n$$\nI(\\theta) = -(-H^T \\Sigma_{gen}^{-1} H) = H^T \\Sigma_{gen}^{-1} H\n$$\nThe Cramér–Rao bound (CRB) gives a lower bound on the variance of any unbiased estimator of $\\theta$. For a scalar parameter, the CRB is the inverse of the Fisher information:\n$$\n\\text{CRB}(\\theta) = \\text{Var}(\\hat{\\theta}) \\geq [I(\\theta)]^{-1}\n$$\n\n**1. Fisher Information for the Correctly Specified Model ($I_{\\text{true}}$)**\n\nThe true covariance matrix is $\\Sigma = \\begin{pmatrix} \\sigma_{1}^{2} & \\rho\\,\\sigma_{1}\\sigma_{2} \\\\ \\rho\\,\\sigma_{1}\\sigma_{2} & \\sigma_{2}^{2} \\end{pmatrix}$.\nThe determinant is $\\det(\\Sigma) = \\sigma_{1}^{2}\\sigma_{2}^{2} - (\\rho\\sigma_{1}\\sigma_{2})^2 = \\sigma_{1}^{2}\\sigma_{2}^{2}(1-\\rho^2)$.\nThe inverse is $\\Sigma^{-1} = \\frac{1}{\\sigma_{1}^{2}\\sigma_{2}^{2}(1-\\rho^2)} \\begin{pmatrix} \\sigma_{2}^{2} & -\\rho\\,\\sigma_{1}\\sigma_{2} \\\\ -\\rho\\,\\sigma_{1}\\sigma_{2} & \\sigma_{1}^{2} \\end{pmatrix}$.\nThe observation matrix is $H = \\begin{pmatrix} h_{1} \\\\ h_{2} \\end{pmatrix}$.\nThe true Fisher information $I_{\\text{true}}(\\theta)$ is:\n$$\nI_{\\text{true}}(\\theta) = H^T \\Sigma^{-1} H = \\begin{pmatrix} h_{1} & h_{2} \\end{pmatrix} \\frac{1}{\\sigma_{1}^{2}\\sigma_{2}^{2}(1-\\rho^2)} \\begin{pmatrix} \\sigma_{2}^{2} & -\\rho\\,\\sigma_{1}\\sigma_{2} \\\\ -\\rho\\,\\sigma_{1}\\sigma_{2} & \\sigma_{1}^{2} \\end{pmatrix} \\begin{pmatrix} h_{1} \\\\ h_{2} \\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\nI_{\\text{true}}(\\theta) = \\frac{1}{\\sigma_{1}^{2}\\sigma_{2}^{2}(1-\\rho^2)} \\left( h_1(h_1\\sigma_2^2 - h_2\\rho\\sigma_1\\sigma_2) + h_2(-h_1\\rho\\sigma_1\\sigma_2 + h_2\\sigma_1^2) \\right)\n$$\n$$\nI_{\\text{true}}(\\theta) = \\frac{h_1^2\\sigma_2^2 - 2h_1h_2\\rho\\sigma_1\\sigma_2 + h_2^2\\sigma_1^2}{\\sigma_{1}^{2}\\sigma_{2}^{2}(1-\\rho^2)}\n$$\n\n**2. Fisher Information for the Misspecified Model ($I_{\\text{miss}}$)**\n\nThe misspecified covariance matrix is diagonal: $\\Sigma_{\\text{miss}} = \\begin{pmatrix} \\sigma_{1}^{2} & 0 \\\\ 0 & \\sigma_{2}^{2} \\end{pmatrix}$.\nIts inverse is $\\Sigma_{\\text{miss}}^{-1} = \\begin{pmatrix} 1/\\sigma_{1}^{2} & 0 \\\\ 0 & 1/\\sigma_{2}^{2} \\end{pmatrix}$.\nThe misspecified Fisher information $I_{\\text{miss}}(\\theta)$ is calculated using this inverse:\n$$\nI_{\\text{miss}}(\\theta) = H^T \\Sigma_{\\text{miss}}^{-1} H = \\begin{pmatrix} h_{1} & h_{2} \\end{pmatrix} \\begin{pmatrix} 1/\\sigma_{1}^{2} & 0 \\\\ 0 & 1/\\sigma_{2}^{2} \\end{pmatrix} \\begin{pmatrix} h_{1} \\\\ h_{2} \\end{pmatrix} = \\frac{h_1^2}{\\sigma_1^2} + \\frac{h_2^2}{\\sigma_2^2}\n$$\n\n**3. Ratio of Cramér–Rao Bounds**\n\nThe true and misspecified CRBs are the inverses of their respective Fisher information values:\n$$\n\\text{CRB}_{\\text{true}} = [I_{\\text{true}}(\\theta)]^{-1} = \\frac{\\sigma_{1}^{2}\\sigma_{2}^{2}(1-\\rho^2)}{h_1^2\\sigma_2^2 - 2h_1h_2\\rho\\sigma_1\\sigma_2 + h_2^2\\sigma_1^2}\n$$\n$$\n\\text{CRB}_{\\text{miss}} = [I_{\\text{miss}}(\\theta)]^{-1} = \\left(\\frac{h_1^2}{\\sigma_1^2} + \\frac{h_2^2}{\\sigma_2^2}\\right)^{-1} = \\frac{\\sigma_1^2\\sigma_2^2}{h_1^2\\sigma_2^2 + h_2^2\\sigma_1^2}\n$$\nThe required ratio $R$ is:\n$$\nR = \\frac{\\text{CRB}_{\\text{true}}}{\\text{CRB}_{\\text{miss}}} = \\frac{[I_{\\text{true}}(\\theta)]^{-1}}{[I_{\\text{miss}}(\\theta)]^{-1}} = \\frac{I_{\\text{miss}}(\\theta)}{I_{\\text{true}}(\\theta)}\n$$\nSubstituting the derived expressions for the Fisher information:\n$$\nR = \\frac{\\frac{h_1^2}{\\sigma_1^2} + \\frac{h_2^2}{\\sigma_2^2}}{\\frac{h_1^2\\sigma_2^2 - 2h_1h_2\\rho\\sigma_1\\sigma_2 + h_2^2\\sigma_1^2}{\\sigma_{1}^{2}\\sigma_{2}^{2}(1-\\rho^2)}} = \\left(\\frac{h_1^2\\sigma_2^2 + h_2^2\\sigma_1^2}{\\sigma_1^2\\sigma_2^2}\\right) \\left(\\frac{\\sigma_{1}^{2}\\sigma_{2}^{2}(1-\\rho^2)}{h_1^2\\sigma_2^2 - 2h_1h_2\\rho\\sigma_1\\sigma_2 + h_2^2\\sigma_1^2}\\right)\n$$\n$$\nR = \\frac{(h_1^2\\sigma_2^2 + h_2^2\\sigma_1^2)(1-\\rho^2)}{h_1^2\\sigma_2^2 - 2h_1h_2\\rho\\sigma_1\\sigma_2 + h_2^2\\sigma_1^2}\n$$\n\n**4. Numerical Calculation**\n\nWe are given the values: $h_{1} = 1.1$, $h_{2} = 0.9$, $\\sigma_{1} = 0.4$, $\\sigma_{2} = 0.5$, and $\\rho = 0.6$.\nFirst, we compute the squared and product terms:\n$h_1^2 = (1.1)^2 = 1.21$\n$h_2^2 = (0.9)^2 = 0.81$\n$\\sigma_1^2 = (0.4)^2 = 0.16$\n$\\sigma_2^2 = (0.5)^2 = 0.25$\n$\\rho^2 = (0.6)^2 = 0.36$\n$1 - \\rho^2 = 1 - 0.36 = 0.64$\n\nNow we compute the terms in the expression for $R$:\n$h_1^2\\sigma_2^2 = 1.21 \\times 0.25 = 0.3025$\n$h_2^2\\sigma_1^2 = 0.81 \\times 0.16 = 0.1296$\n$2h_1h_2\\rho\\sigma_1\\sigma_2 = 2 \\times (1.1) \\times (0.9) \\times (0.6) \\times (0.4) \\times (0.5) = 2 \\times 0.99 \\times 0.6 \\times 0.2 = 0.2376$\n\nNow, substitute these into the expression for $R$:\nNumerator of $R$: $(h_1^2\\sigma_2^2 + h_2^2\\sigma_1^2)(1-\\rho^2) = (0.3025 + 0.1296) \\times 0.64 = 0.4321 \\times 0.64 = 0.276544$\nDenominator of $R$: $h_1^2\\sigma_2^2 - 2h_1h_2\\rho\\sigma_1\\sigma_2 + h_2^2\\sigma_1^2 = 0.3025 - 0.2376 + 0.1296 = 0.1945$\n\nThe ratio is:\n$$\nR = \\frac{0.276544}{0.1945} \\approx 1.42182005...\n$$\nRounding to four significant figures, we get $R = 1.422$. This result means that the true minimum variance ($\\text{CRB}_{\\text{true}}$) is approximately $42.2\\%$ larger than the variance naively calculated by ignoring the positive error correlation ($\\text{CRB}_{\\text{miss}}$). The analyst who ignores the correlation is thus overly optimistic about the attainable precision.", "answer": "$$\\boxed{1.422}$$", "id": "3381472"}, {"introduction": "Our statistical models are rarely perfect representations of reality, a challenge known as model misspecification. This final practice delves into a powerful method for obtaining reliable uncertainty estimates even when the assumed data distribution is incorrect, such as using a Gaussian model for heavy-tailed noise [@problem_id:3381484]. You will construct the Godambe (or \"sandwich\") information matrix, a cornerstone of robust statistics, and see firsthand how it provides a more honest assessment of estimator variance compared to the naive Fisher information.", "problem": "Consider a linear inverse observation model used in data assimilation in which a scalar calibration parameter $\\,\\theta\\,$ multiplies a known template $\\,g_{i}\\,$ across $\\,n\\,$ independent observations. The forward model is $\\,y_{i} = \\theta\\, g_{i} + \\varepsilon_{i}\\,$ for $\\,i = 1, \\dots, n\\,$, where the regressors $\\,g_{i}\\,$ are known real numbers. The true observation errors $\\,\\varepsilon_{i}\\,$ are independently and identically distributed according to a Student-$t$ distribution with degrees of freedom $\\,\\nu > 2\\,$, mean $\\,0\\,$, and scale parameter $\\,\\sigma\\,$, so that $\\,\\operatorname{Var}(\\varepsilon_{i})\\,$ exists and is finite.\n\nA quasi-likelihood estimator is constructed by maximizing a Gaussian pseudo-likelihood that assumes $\\,\\varepsilon_{i}\\sim \\mathcal{N}(0,\\sigma^{2})\\,$, using the same known $\\,\\sigma\\,$ as the true scale but ignoring heavy tails. Work from first principles, using only core definitions of the score function, information, and asymptotic covariance of estimators, to:\n\n- define the quasi-score for $\\,\\theta\\,$ under the Gaussian pseudo-model and identify its sensitivity and variability under the true Student-$t$ data-generating mechanism,\n- construct the Godambe (sandwich) information for $\\,\\theta\\,$ under this misspecification,\n- deduce the asymptotic Cramér-Rao lower bound (CRLB) for unbiased estimators based on the Godambe information, and\n- compute the naive Fisher information and its associated CRLB under the Gaussian misspecification.\n\nFinally, provide a single closed-form analytic expression for the ratio of the Godambe-based CRLB to the naive Fisher-based CRLB, simplified entirely in terms of $\\,\\nu\\,$. No rounding is required. The final answer must be an analytic expression without units.", "solution": "The problem requires the derivation of the ratio between the Cramér-Rao lower bound (CRLB) based on the Godambe (sandwich) information and the naive CRLB based on a misspecified Gaussian likelihood. The true data-generating process follows a Student-$t$ distribution, while the estimation is based on a Gaussian pseudo-likelihood. We proceed from first principles.\n\nThe forward model for the $n$ independent observations is given by:\n$$y_{i} = \\theta g_{i} + \\varepsilon_{i}, \\quad i = 1, \\dots, n$$\nwhere $\\theta$ is the scalar parameter of interest, $g_{i}$ are known constants, and $\\varepsilon_{i}$ are the observation errors.\n\nThe true distribution of the errors is independent and identically distributed (i.i.d.) according to a Student-$t$ distribution with $\\nu > 2$ degrees of freedom, mean $0$, and scale parameter $\\sigma$. We denote this true data-generating process by $P$. The condition $\\nu > 2$ ensures that the variance is finite. The variance of $\\varepsilon_{i}$ under $P$ is given by:\n$$\\operatorname{Var}_{P}(\\varepsilon_{i}) = \\sigma^{2} \\frac{\\nu}{\\nu - 2}$$\n\nThe quasi-likelihood estimator for $\\theta$ is constructed by maximizing a pseudo-likelihood that incorrectly assumes the errors are Gaussian, i.e., $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$. Let this pseudo-model be denoted by $\\psi$. Under this assumption, $y_{i} \\sim \\mathcal{N}(\\theta g_{i}, \\sigma^{2})$.\n\nThe pseudo-log-likelihood for a single observation $y_{i}$ is:\n$$l_{i}(\\theta) = \\ln \\psi(y_{i}; \\theta) = -\\frac{1}{2} \\ln(2\\pi\\sigma^{2}) - \\frac{(y_{i} - \\theta g_{i})^{2}}{2\\sigma^{2}}$$\nThe total pseudo-log-likelihood is $L(\\theta) = \\sum_{i=1}^{n} l_{i}(\\theta)$.\n\nThe quasi-score function is the derivative of $L(\\theta)$ with respect to $\\theta$:\n$$S(\\theta) = \\frac{\\partial L(\\theta)}{\\partial \\theta} = \\sum_{i=1}^{n} \\frac{\\partial l_{i}(\\theta)}{\\partial \\theta}$$\nThe contribution from the $i$-th observation is:\n$$s_{i}(\\theta) = \\frac{\\partial l_{i}(\\theta)}{\\partial \\theta} = -\\frac{1}{2\\sigma^{2}} \\cdot 2(y_{i} - \\theta g_{i})(-g_{i}) = \\frac{(y_{i} - \\theta g_{i})g_{i}}{\\sigma^{2}}$$\nSo, the total quasi-score is $S(\\theta) = \\sum_{i=1}^{n} \\frac{(y_{i} - \\theta g_{i})g_{i}}{\\sigma^{2}}$.\n\nTo analyze the estimator, we evaluate the score at the true parameter value, $\\theta_{0}$. From the true model, $y_i - \\theta_0 g_i = \\varepsilon_i$.\n$$S(\\theta_{0}) = \\sum_{i=1}^{n} \\frac{\\varepsilon_{i} g_{i}}{\\sigma^{2}}$$\n\nThe Godambe information, $J_{G}(\\theta)$, is constructed from two components: the sensitivity, which we denote $A(\\theta)$, and the variability, which we denote $B(\\theta)$.\n\nFirst, we define and compute the sensitivity, $A(\\theta_0)$. This is the expectation of the negative Hessian of the pseudo-log-likelihood, evaluated under the pseudo-model $\\psi$.\n$$A(\\theta) = -\\mathbb{E}_{\\psi}\\left[\\frac{\\partial^{2} L(\\theta)}{\\partial \\theta^{2}}\\right]$$\nThe second derivative is:\n$$\\frac{\\partial^{2} L(\\theta)}{\\partial \\theta^{2}} = \\sum_{i=1}^{n} \\frac{\\partial s_{i}(\\theta)}{\\partial \\theta} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\theta} \\left(\\frac{(y_{i} - \\theta g_{i})g_{i}}{\\sigma^{2}}\\right) = \\sum_{i=1}^{n} \\frac{-g_{i}^{2}}{\\sigma^{2}}$$\nSince this expression does not depend on the data $y_{i}$, the expectation is the expression itself.\n$$A(\\theta_{0}) = -\\left(\\sum_{i=1}^{n} \\frac{-g_{i}^{2}}{\\sigma^{2}}\\right) = \\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}$$\nThis quantity, $A(\\theta_{0})$, is also the naive Fisher information, $I_{F, \\text{naive}}(\\theta_{0})$, as it is the Fisher information calculated under the misspecified Gaussian model.\n\nSecond, we define and compute the variability, $B(\\theta_0)$. This is the variance of the quasi-score function, evaluated under the true data-generating process $P$.\n$$B(\\theta_{0}) = \\operatorname{Var}_{P}(S(\\theta_{0})) = \\mathbb{E}_{P}[S(\\theta_{0})^{2}] - (\\mathbb{E}_{P}[S(\\theta_{0})])^{2}$$\nThe expectation of the score under the true model is:\n$$\\mathbb{E}_{P}[S(\\theta_{0})] = \\mathbb{E}_{P}\\left[\\sum_{i=1}^{n} \\frac{\\varepsilon_{i} g_{i}}{\\sigma^{2}}\\right] = \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n} g_{i} \\mathbb{E}_{P}[\\varepsilon_{i}]$$\nSince the true distribution of $\\varepsilon_{i}$ has mean $0$, $\\mathbb{E}_{P}[S(\\theta_{0})] = 0$. Therefore, the variability is the expectation of the squared score:\n$$B(\\theta_{0}) = \\mathbb{E}_{P}[S(\\theta_{0})^{2}] = \\mathbb{E}_{P}\\left[\\left(\\sum_{i=1}^{n} \\frac{\\varepsilon_{i} g_{i}}{\\sigma^{2}}\\right)^{2}\\right] = \\frac{1}{\\sigma^{4}} \\mathbb{E}_{P}\\left[\\left(\\sum_{i=1}^{n} \\varepsilon_{i} g_{i}\\right)^{2}\\right]$$\nBecause the errors $\\varepsilon_{i}$ are independent, $\\mathbb{E}_{P}[\\varepsilon_{i}\\varepsilon_{j}] = \\mathbb{E}_{P}[\\varepsilon_{i}]\\mathbb{E}_{P}[\\varepsilon_{j}] = 0$ for $i \\neq j$. The expression simplifies to:\n$$B(\\theta_{0}) = \\frac{1}{\\sigma^{4}} \\sum_{i=1}^{n} g_{i}^{2} \\mathbb{E}_{P}[\\varepsilon_{i}^{2}]$$\nSince $\\mathbb{E}_{P}[\\varepsilon_{i}] = 0$, we have $\\mathbb{E}_{P}[\\varepsilon_{i}^{2}] = \\operatorname{Var}_{P}(\\varepsilon_{i}) = \\sigma^{2}\\frac{\\nu}{\\nu-2}$. Substituting this in:\n$$B(\\theta_{0}) = \\frac{1}{\\sigma^{4}} \\sum_{i=1}^{n} g_{i}^{2} \\left(\\sigma^{2}\\frac{\\nu}{\\nu-2}\\right) = \\left(\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right) \\frac{\\nu}{\\nu-2}$$\n\nThe Godambe (sandwich) information for a scalar parameter is defined as $J_{G}(\\theta) = A(\\theta)^{2} / B(\\theta)$.\n$$J_{G}(\\theta_{0}) = \\frac{\\left(\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right)^{2}}{\\left(\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right) \\frac{\\nu}{\\nu-2}} = \\left(\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right) \\frac{\\nu-2}{\\nu}$$\n\nThe asymptotic CRLB for unbiased estimators under misspecification is the inverse of the Godambe information.\n$$CRLB_{G} = [J_{G}(\\theta_{0})]^{-1} = \\left[\\left(\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right) \\frac{\\nu-2}{\\nu}\\right]^{-1} = \\left(\\frac{\\sigma^{2}}{\\sum_{i=1}^{n} g_{i}^{2}}\\right) \\frac{\\nu}{\\nu-2}$$\nThis is equivalent to the asymptotic variance of the quasi-likelihood estimator, given by $B(\\theta_{0}) / A(\\theta_{0})^{2}$.\n\nThe naive Fisher information was already identified as $I_{F, \\text{naive}}(\\theta_{0}) = A(\\theta_{0}) = \\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}$.\nThe associated naive CRLB is its inverse:\n$$CRLB_{\\text{naive}} = [I_{F, \\text{naive}}(\\theta_{0})]^{-1} = \\left[\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right]^{-1} = \\frac{\\sigma^{2}}{\\sum_{i=1}^{n} g_{i}^{2}}$$\nThis is the lower bound one would incorrectly compute by assuming the Gaussian model is true.\n\nFinally, we compute the ratio of the Godambe-based CRLB to the naive CRLB:\n$$\\frac{CRLB_{G}}{CRLB_{\\text{naive}}} = \\frac{\\left(\\frac{\\sigma^{2}}{\\sum_{i=1}^{n} g_{i}^{2}}\\right) \\frac{\\nu}{\\nu-2}}{\\frac{\\sigma^{2}}{\\sum_{i=1}^{n} g_{i}^{2}}}$$\nThe common term $\\frac{\\sigma^{2}}{\\sum_{i=1}^{n} g_{i}^{2}}$ cancels out, leaving:\n$$\\frac{CRLB_{G}}{CRLB_{\\text{naive}}} = \\frac{\\nu}{\\nu-2}$$\nThis ratio quantifies the loss of efficiency of the estimator due to using a Gaussian likelihood when the true errors have heavier tails described by a Student-$t$ distribution. As $\\nu \\to \\infty$, the Student-$t$ converges to a Gaussian, and the ratio approaches $1$. As $\\nu \\to 2^{+}$, the variance of the true error distribution approaches infinity, and the ratio approaches infinity, indicating a severe loss of efficiency.", "answer": "$$\\boxed{\\frac{\\nu}{\\nu-2}}$$", "id": "3381484"}]}