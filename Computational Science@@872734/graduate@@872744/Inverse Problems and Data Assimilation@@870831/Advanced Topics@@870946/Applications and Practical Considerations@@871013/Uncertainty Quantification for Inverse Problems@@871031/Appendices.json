{"hands_on_practices": [{"introduction": "The linear-Gaussian model is the cornerstone of Bayesian inference, providing a tractable and insightful starting point for uncertainty quantification. This first exercise [@problem_id:3429478] offers a concrete application of Bayes' rule, guiding you through the numerical computation of the posterior distribution for a simple two-dimensional problem. By calculating the posterior mean and covariance and comparing them to the prior, you will gain hands-on intuition for how observational data updates our beliefs and systematically reduces uncertainty.", "problem": "Consider the linear inverse problem with an unknown parameter vector $x \\in \\mathbb{R}^{2}$ and observations $y \\in \\mathbb{R}^{2}$ governed by the forward model $y = A x + \\eta$, where $\\eta$ is zero-mean noise independent of $x$. Assume a Gaussian prior $x \\sim \\mathcal{N}(m_{0}, C_{0})$ and Gaussian observational noise $\\eta \\sim \\mathcal{N}(0, \\Gamma)$. Take the specific numerical values\n$$\nA = \\begin{pmatrix}\n2  1 \\\\\n1  3\n\\end{pmatrix}, \\quad\nC_{0} = \\mathrm{diag}(1, 4), \\quad\n\\Gamma = 0.25\\, I_{2}, \\quad\nm_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad\ny = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix},\n$$\nwhere $I_{2}$ denotes the $2 \\times 2$ identity matrix. \n\nStarting from Bayes' rule and the definition of the multivariate Gaussian density, derive the posterior distribution of $x$ given $y$ and thereby obtain the posterior mean vector $m^{y}$ and the posterior covariance matrix $C^{y}$ for this linear-Gaussian inverse problem. Compute $m^{y}$ and $C^{y}$ numerically for the given data. Then, interpret the directions in which uncertainty is most reduced by comparing the prior and posterior covariances, making clear how the measurement operator $A$ and the noise statistics shape these directions.\n\nExpress your final numerical values for $m^{y}$ and $C^{y}$ exactly (no rounding). The final answer must be a single row matrix containing, in order, the two entries of $m^{y}$ followed by the four entries of $C^{y}$ listed row-by-row.", "solution": "For a linear-Gaussian model, the posterior distribution is also Gaussian, $x|y \\sim \\mathcal{N}(m^y, C^y)$. The posterior covariance $C^y$ and mean $m^y$ are given by the formulas:\n$$ (C^y)^{-1} = C_0^{-1} + A^\\top \\Gamma^{-1} A $$\n$$ m^y = C^y (C_0^{-1} m_0 + A^\\top \\Gamma^{-1} y) $$\nWe first compute the necessary components. The inverse of the prior covariance is:\n$$ C_0^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix}^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  1/4 \\end{pmatrix} $$\nThe inverse of the noise covariance is:\n$$ \\Gamma^{-1} = (0.25 I_2)^{-1} = 4 I_2 = \\begin{pmatrix} 4  0 \\\\ 0  4 \\end{pmatrix} $$\nThe data precision term is:\n$$ A^\\top \\Gamma^{-1} A = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 4  0 \\\\ 0  4 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} = \\begin{pmatrix} 8  4 \\\\ 4  12 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} = \\begin{pmatrix} 20  20 \\\\ 20  40 \\end{pmatrix} $$\nNow we compute the posterior precision (inverse covariance):\n$$ (C^y)^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  1/4 \\end{pmatrix} + \\begin{pmatrix} 20  20 \\\\ 20  40 \\end{pmatrix} = \\begin{pmatrix} 21  20 \\\\ 20  40.25 \\end{pmatrix} = \\begin{pmatrix} 21  20 \\\\ 20  161/4 \\end{pmatrix} $$\nThe posterior covariance $C^y$ is the inverse of this matrix:\n$$ C^y = \\begin{pmatrix} 21  20 \\\\ 20  161/4 \\end{pmatrix}^{-1} = \\frac{1}{21 \\cdot \\frac{161}{4} - 20 \\cdot 20} \\begin{pmatrix} 161/4  -20 \\\\ -20  21 \\end{pmatrix} = \\frac{4}{1781} \\begin{pmatrix} 161/4  -20 \\\\ -20  21 \\end{pmatrix} = \\frac{1}{1781} \\begin{pmatrix} 161  -80 \\\\ -80  84 \\end{pmatrix} $$\nNext, we compute the posterior mean. Since the prior mean $m_0$ is zero, the formula simplifies to $m^y = C^y (A^\\top \\Gamma^{-1} y)$.\n$$ A^\\top \\Gamma^{-1} y = \\begin{pmatrix} 8  4 \\\\ 4  12 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ -8 \\end{pmatrix} $$\n$$ m^y = \\frac{1}{1781} \\begin{pmatrix} 161  -80 \\\\ -80  84 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -8 \\end{pmatrix} = \\frac{1}{1781} \\begin{pmatrix} 644 + 640 \\\\ -320 - 672 \\end{pmatrix} = \\frac{1}{1781} \\begin{pmatrix} 1284 \\\\ -992 \\end{pmatrix} $$\nThe prior variances were $\\mathrm{Var}(x_1)=1$ and $\\mathrm{Var}(x_2)=4$. The posterior variances are $\\mathrm{Var}(x_1|y) = 161/1781 \\approx 0.09$ and $\\mathrm{Var}(x_2|y) = 84/1781 \\approx 0.047$. Uncertainty is reduced in both components, but more significantly for $x_2$. This is because the second column of $A$, which corresponds to $x_2$, has a larger norm than the first, indicating the measurements are more sensitive to $x_2$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\frac{1284}{1781}  -\\frac{992}{1781}  \\frac{161}{1781}  -\\frac{80}{1781}  -\\frac{80}{1781}  \\frac{84}{1781} \\end{pmatrix}\n}\n$$", "id": "3429478"}, {"introduction": "While the linear-Gaussian model is foundational, many real-world inverse problems are fundamentally nonlinear and may be ill-posed. This practice [@problem_id:3382702] explores the critical concept of non-identifiability, where distinct parameter values can yield identical observations. Using a simple yet revealing quadratic forward model, you will analytically derive how non-identifiability can lead to a bimodal posterior distribution, demonstrating a failure of uniqueness and stability, which are key symptoms of an ill-posed problem.", "problem": "Consider a scalar Bayesian inverse problem within the forward and inverse uncertainty quantification paradigms for inverse problems and data assimilation. Let the forward map be $G(\\theta) = \\theta^{2}$ with parameter $\\theta \\in \\mathbb{R}$. A single noisy observation $y \\in \\mathbb{R}$ is acquired via the additive Gaussian noise model $y = G(\\theta) + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ and $\\sigma  0$ is known. The prior on the parameter is Gaussian $\\theta \\sim \\mathcal{N}(0, \\tau^{2})$ with $\\tau  0$ known. All quantities are scalars. Throughout, use Bayes’ rule and principled asymptotic approximation methods grounded in first principles; do not assume identifiability where it fails.\n\nThis setting exhibits non-identifiability because $G(\\theta) = G(-\\theta)$, which can lead to ill-posedness manifested as posterior multimodality and large posterior variance. Starting from first principles (Bayes’ rule, properties of Gaussian distributions, and the definition of well-posedness in the sense of Jacques Hadamard), derive an explicit leading-order analytic approximation of the posterior variance $\\mathrm{Var}[\\theta \\mid y]$ in the regime where the posterior is bimodal. In deriving your expression, justify the condition under which bimodality occurs in terms of $y$, $\\sigma$, and $\\tau$, and discuss the stability implications of this non-identifiability (existence, uniqueness, and continuous dependence on data).\n\nYour final answer must be a single closed-form analytic expression for the leading-order approximation of $\\mathrm{Var}[\\theta \\mid y]$ as a function of $y$, $\\sigma$, and $\\tau$. No rounding is required, and no units are involved.", "solution": "We are tasked with analyzing the posterior distribution of a parameter $\\theta$ given an observation $y$. The relationship is defined by the forward model $G(\\theta) = \\theta^{2}$ and the observation model $y = \\theta^{2} + \\varepsilon$, where the noise is Gaussian, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$. The prior belief about the parameter is also Gaussian, $\\theta \\sim \\mathcal{N}(0, \\tau^{2})$.\n\nFirst, we construct the posterior probability density function (PDF) $p(\\theta \\mid y)$ using Bayes' rule:\n$$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)\n$$\nThe likelihood function $p(y \\mid \\theta)$ is derived from the noise model. Given a value of $\\theta$, $y$ is a random variable distributed as $\\mathcal{N}(\\theta^{2}, \\sigma^{2})$. Thus, the likelihood is:\n$$\np(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left( -\\frac{(y - \\theta^{2})^{2}}{2\\sigma^{2}} \\right)\n$$\nThe prior PDF is given as:\n$$\np(\\theta) = \\frac{1}{\\sqrt{2\\pi\\tau^{2}}} \\exp\\left( -\\frac{\\theta^{2}}{2\\tau^{2}} \\right)\n$$\nCombining these, the unnormalized posterior PDF is:\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\frac{(y - \\theta^{2})^{2}}{2\\sigma^{2}} \\right) \\exp\\left( -\\frac{\\theta^{2}}{2\\tau^{2}} \\right) = \\exp\\left( -\\left[ \\frac{(y - \\theta^{2})^{2}}{2\\sigma^{2}} + \\frac{\\theta^{2}}{2\\tau^{2}} \\right] \\right)\n$$\nTo find the modes of the posterior distribution, we can find the minima of the negative log-posterior, $\\Phi(\\theta)$, where $p(\\theta \\mid y) \\propto \\exp(-\\Phi(\\theta))$:\n$$\n\\Phi(\\theta) = \\frac{(y - \\theta^{2})^{2}}{2\\sigma^{2}} + \\frac{\\theta^{2}}{2\\tau^{2}}\n$$\nThe modes are stationary points, found by setting the first derivative with respect to $\\theta$ to zero:\n$$\n\\frac{d\\Phi}{d\\theta} = \\frac{2(y - \\theta^{2})(-2\\theta)}{2\\sigma^{2}} + \\frac{2\\theta}{2\\tau^{2}} = -\\frac{2\\theta(y - \\theta^{2})}{\\sigma^{2}} + \\frac{\\theta}{\\tau^{2}} = \\theta \\left( \\frac{2\\theta^{2} - 2y}{\\sigma^{2}} + \\frac{1}{\\tau^{2}} \\right) = 0\n$$\nThis equation has solutions at $\\theta = 0$ and where the term in the parentheses is zero:\n$$\n\\frac{2\\theta^{2}}{\\sigma^{2}} = \\frac{2y}{\\sigma^{2}} - \\frac{1}{\\tau^{2}} \\implies \\theta^{2} = y - \\frac{\\sigma^{2}}{2\\tau^{2}}\n$$\nSo, the stationary points are $\\theta_{0} = 0$ and, provided $y  \\frac{\\sigma^{2}}{2\\tau^{2}}$, $\\theta_{\\pm} = \\pm \\sqrt{y - \\frac{\\sigma^{2}}{2\\tau^{2}}}$.\n\nTo determine if these are modes (local maxima of the PDF, minima of $\\Phi$) or antimodes (local minima of the PDF, maxima of $\\Phi$), we examine the second derivative of $\\Phi(\\theta)$:\n$$\n\\frac{d^{2}\\Phi}{d\\theta^{2}} = \\frac{d}{d\\theta} \\left( \\frac{2\\theta^{3}}{\\sigma^{2}} - \\frac{2y\\theta}{\\sigma^{2}} + \\frac{\\theta}{\\tau^{2}} \\right) = \\frac{6\\theta^{2}}{\\sigma^{2}} - \\frac{2y}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}\n$$\nWhen $y > \\frac{\\sigma^{2}}{2\\tau^{2}}$, the posterior distribution is bimodal, with two modes at $\\theta_{\\pm} = \\pm \\sqrt{y - \\frac{\\sigma^{2}}{2\\tau^{2}}}$ and an antimode at $\\theta = 0$.\n\nWe derive the leading-order approximation for the posterior variance $\\mathrm{Var}[\\theta \\mid y]$ in this bimodal regime by approximating the posterior as a symmetric mixture of two Gaussians:\n$$\np(\\theta \\mid y) \\approx \\frac{1}{2} \\mathcal{N}(\\theta \\mid \\mu_{+}, \\sigma_{L}^{2}) + \\frac{1}{2} \\mathcal{N}(\\theta \\mid \\mu_{-}, \\sigma_{L}^{2})\n$$\nwhere the means $\\mu_{\\pm}$ are the mode locations, $\\mu_{\\pm} = \\theta_{\\pm}$, and the local variance $\\sigma_{L}^{2}$ is the inverse of the Hessian of $\\Phi(\\theta)$ evaluated at the modes:\n$$\n\\sigma_{L}^{2} = \\left( \\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta_{\\pm}} \\right)^{-1} = \\left( 2\\left(\\frac{2y}{\\sigma^{2}} - \\frac{1}{\\tau^{2}}\\right) \\right)^{-1} = \\frac{\\sigma^{2}\\tau^{2}}{4y\\tau^{2} - 2\\sigma^{2}}\n$$\nThe total variance of this Gaussian mixture model is given by the law of total variance:\n$$\n\\mathrm{Var}[\\theta \\mid y] = \\mathbb{E}[\\mathrm{Var}[\\theta \\mid y, Z]] + \\mathrm{Var}[\\mathbb{E}[\\theta \\mid y, Z]]\n$$\nwhere $Z$ is a latent variable indicating the component of the mixture. This simplifies to $\\mathrm{Var}[\\theta \\mid y] \\approx \\mu_{+}^{2} + \\sigma_{L}^{2}$. Substituting the expressions for $\\mu_{+}^{2}$ and $\\sigma_{L}^{2}$:\n$$\n\\mathrm{Var}[\\theta \\mid y] \\approx \\left(y - \\frac{\\sigma^{2}}{2\\tau^{2}}\\right) + \\frac{\\sigma^{2}\\tau^{2}}{4y\\tau^{2} - 2\\sigma^{2}}\n$$\nThis expression is the leading-order approximation for the posterior variance in the bimodal regime ($y  \\frac{\\sigma^{2}}{2\\tau^{2}}$). This non-identifiability leads to a failure of uniqueness (two modes) and stability (the variance diverges as $y$ approaches the bifurcation point from above), which are classic signs of an ill-posed problem.", "answer": "$$\n\\boxed{y - \\frac{\\sigma^{2}}{2\\tau^{2}} + \\frac{\\sigma^{2}\\tau^{2}}{4y\\tau^{2} - 2\\sigma^{2}}}\n$$", "id": "3382702"}, {"introduction": "To effectively quantify uncertainty, we need robust tools to diagnose its structure, especially in high-dimensional or ill-conditioned problems. This computational exercise [@problem_id:3429460] introduces the singular value decomposition (SVD) of a prior-preconditioned operator as a primary diagnostic technique. You will implement an algorithm to identify parameter combinations that are poorly constrained by the data, revealing high-uncertainty 'ridges' in the posterior landscape and providing a direct link between the operator's properties and the structure of posterior knowledge.", "problem": "Consider a linearized Bayesian inverse problem with parameter vector $x \\in \\mathbb{R}^n$, linear forward map $G \\in \\mathbb{R}^{m \\times n}$, additive measurement noise $\\eta \\in \\mathbb{R}^m$ modeled as zero-mean Gaussian with covariance $C_n \\in \\mathbb{R}^{m \\times m}$, and a Gaussian prior on $x$ with zero mean and covariance $C_0 \\in \\mathbb{R}^{n \\times n}$. The observed data are $y \\in \\mathbb{R}^m$ satisfying $y = G x + \\eta$. Assume $C_0$ and $C_n$ are symmetric positive definite. The goal is to characterize posterior uncertainty ridges by analyzing identifiability along the nullspace using singular functions of the linearized forward map.\n\nStarting from Bayes' rule for Gaussian measures and the linearization at a reference point, the posterior distribution of $x$ is Gaussian with covariance determined by the interplay between the prior precision $C_0^{-1}$ and the data misfit curvature $G^\\top C_n^{-1} G$. Define the prior-preconditioned forward operator $A$ by\n$$\nA = C_n^{-1/2} \\, G \\, C_0^{1/2},\n$$\nwhere $C_0^{1/2}$ and $C_n^{-1/2}$ denote the unique symmetric positive definite square root of $C_0$ and the inverse of the square root of $C_n$, respectively. Let the singular value decomposition of $A$ be expressed through its right singular vectors as the eigen-decomposition of the symmetric matrix\n$$\nH = A^\\top A = C_0^{1/2} \\, G^\\top \\, C_n^{-1} \\, G \\, C_0^{1/2}.\n$$\nIf $\\lambda_i$ are the eigenvalues of $H$ with corresponding orthonormal eigenvectors $v_i \\in \\mathbb{R}^n$ (the right singular vectors of $A$), then the singular values are $s_i = \\sqrt{\\lambda_i}$. In the prior-coordinate system given by $z = C_0^{-1/2} x$, the posterior covariance has eigenvalues along $v_i$ equal to\n$$\n\\gamma_i = \\frac{1}{1 + s_i^2}.\n$$\nDirections with small $s_i$ correspond to weak data information and large posterior variance factors $\\gamma_i$, forming posterior ridges. Exact nullspace directions have $s_i = 0$ and yield $\\gamma_i = 1$, indicating no reduction from the prior.\n\nYour task is to implement a program that, for a given test suite of matrices $(G, C_0, C_n)$, computes:\n- the full set of singular values $s_i$ of $A$ via the eigenvalues of $H = A^\\top A$,\n- the posterior variance factors $\\gamma_i = 1/(1 + s_i^2)$ along the right singular vectors $v_i$,\n- a binary ridge classification for each direction defined as $\\text{ridge}_i = \\text{True}$ if $s_i \\le s_{\\text{thresh}}$ and $\\text{ridge}_i = \\text{False}$ otherwise,\n- the dimension of the numerical nullspace defined as the count of $s_i \\le s_{\\text{zero}}$.\n\nUse the following numerical thresholds:\n- $s_{\\text{thresh}} = 10^{-3}$ for ridge classification,\n- $s_{\\text{zero}} = 10^{-12}$ for nullspace dimension.\n\nFor each test case, output a list containing:\n- the integer nullspace dimension,\n- the list of singular values $s_i$ sorted in descending order, each rounded to $8$ decimal places,\n- the list of posterior variance factors $\\gamma_i$ in the same order, each rounded to $8$ decimal places,\n- the list of boolean ridge flags in the same order.\n\nConstruct and use the following test suite:\n- Test case $1$ (rank-deficient, pronounced nullspace): $m = 2$, $n = 3$, $G = \\begin{bmatrix}1  0  0 \\\\ 0  0  0\\end{bmatrix}$, $C_0 = \\mathrm{diag}(1, 4, 9)$, $C_n = \\mathrm{diag}(0.01, 0.01)$.\n- Test case $2$ (boundary case at threshold): $m = 2$, $n = 3$, $G = \\begin{bmatrix}10^{-3}  0  0 \\\\ 0  0  10^{-3}\\end{bmatrix}$, $C_0 = \\mathrm{diag}(1, 1, 1)$, $C_n = \\mathrm{diag}(1, 1)$.\n- Test case $3$ (full rank, strongly identifiable): $m = 3$, $n = 3$, $G = \\mathrm{diag}(5, 2, 1)$, $C_0 = I_{3 \\times 3}$, $C_n = \\mathrm{diag}(0.04, 0.04, 0.04)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[ \\text{result1}, \\text{result2}, \\text{result3} ]$), where each $\\text{result}$ is the list for a test case as specified above. No physical units or angle units are involved. Numeric answers must be floats or integers as stated, and lists must be composed of these types or booleans.", "solution": "The problem requires an analysis of the posterior uncertainty for a linear Bayesian inverse problem. The solution involves computing several quantities derived from the singular values of a prior-preconditioned forward operator.\n\n**Theoretical Framework**\n\nThe foundation of the problem lies in the posterior covariance matrix $C_{\\text{post}}$ for a linear-Gaussian problem:\n$$\nC_{\\text{post}} = (C_0^{-1} + G^\\top C_n^{-1} G)^{-1}\n$$\nTo analyze the structure of the posterior uncertainty, we work in a coordinate system where the prior is whitened by defining $z = C_0^{-1/2} x$. In these coordinates, the prior on $z$ is a standard normal distribution, $z \\sim \\mathcal{N}(0, I)$. The posterior precision for $z$ is $(I + A^\\top A)$, where $A = C_n^{-1/2} G C_0^{1/2}$ is the prior-preconditioned forward operator. The posterior covariance for $z$ is $C_{\\text{post},z} = (I + A^\\top A)^{-1}$.\n\n**Singular Value Decomposition Analysis**\n\nWe analyze this posterior covariance by considering the Singular Value Decomposition (SVD) of $A$. Let the eigenvectors of the symmetric matrix $H = A^\\top A$ be $v_i$, with corresponding eigenvalues $\\lambda_i = s_i^2$, where $s_i$ are the singular values of $A$. In the basis of these eigenvectors, the posterior covariance for $z$ is diagonal with entries $\\gamma_i = \\frac{1}{1+s_i^2}$. These factors quantify the reduction of variance from the prior to the posterior along each principal direction.\n- If $s_i \\gg 1$, then $\\gamma_i \\ll 1$. The data strongly constrain this direction.\n- If $s_i \\ll 1$, then $\\gamma_i \\approx 1$. The data provide little information, and the posterior variance is nearly equal to the prior variance. These directions form \"ridges\" of high posterior uncertainty.\n\n**Computational Algorithm**\n\nFor each test case $(G, C_0, C_n)$:\n1.  Compute the symmetric positive definite square root of the prior covariance, $C_0^{1/2}$, and the inverse of the noise covariance, $C_n^{-1}$.\n2.  Construct the preconditioned Hessian matrix $H = C_0^{1/2} G^\\top C_n^{-1} G C_0^{1/2}$.\n3.  Compute the eigenvalues $\\lambda_i$ of $H$ and sort them in descending order.\n4.  Calculate the singular values of $A$ as $s_i = \\sqrt{\\lambda_i}$.\n5.  Calculate the posterior variance factors: $\\gamma_i = 1 / (1 + s_i^2)$.\n6.  Classify each direction as a ridge: $\\text{ridge}_i = \\text{True}$ if $s_i \\le s_{\\text{thresh}}$.\n7.  Compute the dimension of the numerical nullspace by counting the number of singular values $s_i \\le s_{\\text{zero}}$.\n8.  Assemble the results into a list with the specified format and rounding.", "answer": "[[2, [10.0, 0.0, 0.0], [0.00990099, 1.0, 1.0], [False, True, True]],[1, [0.001, 0.001, 0.0], [0.999999, 0.999999, 1.0], [True, True, True]],[0, [25.0, 10.0, 5.0], [0.00159744, 0.00990099, 0.03846154], [False, False, False]]]", "id": "3429460"}]}