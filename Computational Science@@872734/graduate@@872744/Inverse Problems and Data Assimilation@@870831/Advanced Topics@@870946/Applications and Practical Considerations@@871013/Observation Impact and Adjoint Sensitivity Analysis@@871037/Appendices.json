{"hands_on_practices": [{"introduction": "The cornerstone of variational data assimilation and large-scale inverse problems is the efficient computation of the gradient of a cost function with respect to a high-dimensional control variable, such as the initial state of a system. The adjoint method offers a powerful solution, enabling the gradient to be calculated at a computational cost that is independent of the dimension of the control space. This exercise guides you through the fundamental process of deriving and implementing the adjoint model for a linear system, a critical first step for any practitioner. To build confidence in your implementation, you will verify your analytically derived gradient against a numerical approximation using the finite-difference method, a technique commonly known as a \"gradient check\" [@problem_id:3406525].", "problem": "Consider the linear time-invariant discrete model defined by the state transition $x_{k+1} = A x_k$ with initial condition $x_0 \\in \\mathbb{R}^n$ and a linear observation operator $h_k(x) = H x$ for all time indices $k \\in \\mathbb{N}$. Let $H \\in \\mathbb{R}^{p \\times n}$ and $A \\in \\mathbb{R}^{n \\times n}$ be fixed matrices. Given a background state $x_b \\in \\mathbb{R}^n$, a symmetric positive definite (SPD) background covariance matrix $B \\in \\mathbb{R}^{n \\times n}$, a sequence of observations $\\{y_k\\}_{k=0}^{M-1}$ with $y_k \\in \\mathbb{R}^p$, and an SPD observation error covariance matrix $R \\in \\mathbb{R}^{p \\times p}$, define the Four-Dimensional Variational (4D-Var) objective function\n$$\nJ(x_0) = \\frac{1}{2} (x_0 - x_b)^\\top B^{-1} (x_0 - x_b) + \\frac{1}{2} \\sum_{k=0}^{M-1} \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right),\n$$\nwhere $x_k = A^k x_0$ for each $k \\in \\{0,1,\\dots,M-1\\}$.\n\nTask: Derive and implement the exact gradient $\\nabla J(x_0)$ using the adjoint method starting from first principles. Then verify its correctness by comparing directional derivatives computed from $\\nabla J(x_0)$ against central finite differences of $J(x_0)$ in several random directions. Use central differences with a small step size $\\varepsilon$ and compare, for each direction $v$, the inner product $v^\\top \\nabla J(x_0)$ to the central difference\n$$\n\\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2 \\varepsilon}.\n$$\nReport the maximum relative discrepancy across a set of random directions for each test case. Use a fixed random seed for reproducibility.\n\nYou must implement a single program that carries out the following for each test case:\n- Constructs the specified matrices and vectors.\n- Computes the exact gradient $\\nabla J(x_0)$ via the adjoint method.\n- Computes central-difference directional derivatives along a fixed set of random unit directions.\n- Returns, for each test case, the maximum relative error defined as\n$$\n\\max_{v \\in \\mathcal{V}} \\frac{\\left|v^\\top \\nabla J(x_0) - \\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2 \\varepsilon}\\right|}{\\max\\left(1, \\left|v^\\top \\nabla J(x_0)\\right|, \\left|\\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2 \\varepsilon}\\right|\\right)},\n$$\nwhere $\\mathcal{V}$ is the set of random directions. Use $\\varepsilon = 10^{-6}$.\n\nTest suite:\n- Case A (identity dynamics and full observations):\n  - Dimension: $n = 3$, $p = 3$, number of observation times $M = 3$ (times $k = 0,1,2$).\n  - Matrices:\n    $$\n    A = I_3,\\quad H = I_3,\\quad B = I_3,\\quad R = I_3.\n    $$\n  - Vectors:\n    $$\n    x_0 = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 0.5 \\end{bmatrix},\\quad x_b = \\begin{bmatrix} 0.5 \\\\ -2.5 \\\\ 1.5 \\end{bmatrix},\n    $$\n    $$\n    y_0 = \\begin{bmatrix} 1.2 \\\\ -1.8 \\\\ 0.7 \\end{bmatrix},\\quad\n    y_1 = \\begin{bmatrix} 0.9 \\\\ -2.3 \\\\ 0.4 \\end{bmatrix},\\quad\n    y_2 = \\begin{bmatrix} 1.1 \\\\ -2.1 \\\\ 0.6 \\end{bmatrix}.\n    $$\n- Case B (non-normal stable dynamics and partial observations):\n  - Dimension: $n = 4$, $p = 2$, $M = 6$ (times $k = 0,1,2,3,4,5$).\n  - Matrices:\n    $$\n    A = \\begin{bmatrix}\n    0.9 & 0.2 & 0.0 & 0.0\\\\\n    0.0 & 0.9 & 0.2 & 0.0\\\\\n    0.0 & 0.0 & 0.9 & 0.2\\\\\n    0.0 & 0.0 & 0.0 & 0.9\n    \\end{bmatrix},\\quad\n    H = \\begin{bmatrix}\n    1.0 & 0.0 & 0.5 & 0.0\\\\\n    0.0 & 1.0 & 0.0 & 0.5\n    \\end{bmatrix}.\n    $$\n    Construct an SPD background covariance via\n    $$\n    M_{\\text{mat}} = \\begin{bmatrix}\n    2.0 & -0.3 & 0.0 & 0.0\\\\\n    -0.3 & 1.5 & 0.0 & 0.0\\\\\n    0.0 & 0.0 & 1.2 & 0.2\\\\\n    0.0 & 0.0 & 0.2 & 1.1\n    \\end{bmatrix},\\quad\n    B = M_{\\text{mat}} M_{\\text{mat}}^\\top + 0.1\\, I_4.\n    $$\n    Use\n    $$\n    R = \\begin{bmatrix}\n    1.0 & 0.2\\\\\n    0.2 & 0.5\n    \\end{bmatrix}.\n    $$\n  - Vectors:\n    $$\n    x_0 = \\begin{bmatrix} 0.1 \\\\ -0.9 \\\\ 0.0 \\\\ 0.5 \\end{bmatrix},\\quad\n    x_b = \\begin{bmatrix} 0.0 \\\\ -0.8 \\\\ 0.2 \\\\ 0.3 \\end{bmatrix}.\n    $$\n    Generate observations from a fixed truth and a deterministic bias:\n    $$\n    x_0^{\\text{true}} = \\begin{bmatrix} 0.5 \\\\ -1.0 \\\\ 0.3 \\\\ 0.7 \\end{bmatrix},\\quad\n    y_k = H A^k x_0^{\\text{true}} + \\begin{bmatrix} 0.01 k \\\\ -0.005 k \\end{bmatrix},\\quad k \\in \\{0,1,2,3,4,5\\}.\n    $$\n- Case C (mildly unstable non-normal dynamics and full observations, ill-conditioned background):\n  - Dimension: $n = 3$, $p = 3$, $M = 5$ (times $k = 0,1,2,3,4$).\n  - Matrices:\n    $$\n    A = \\begin{bmatrix}\n    1.2 & 0.1 & 0.0\\\\\n    0.0 & 0.95 & 0.05\\\\\n    0.0 & 0.0 & 1.05\n    \\end{bmatrix},\\quad\n    H = \\begin{bmatrix}\n    1.0 & 0.0 & 0.0\\\\\n    0.0 & 1.0 & 0.0\\\\\n    0.2 & 0.0 & 1.0\n    \\end{bmatrix}.\n    $$\n    Let\n    $$\n    Q = \\begin{bmatrix}\n    1.0 & 0.9 & 0.0\\\\\n    0.9 & 1.0 & 0.0\\\\\n    0.0 & 0.0 & 0.01\n    \\end{bmatrix},\\quad\n    B = Q + 0.1\\, I_3,\\quad\n    R = 0.05\\, I_3.\n    $$\n  - Vectors:\n    $$\n    x_0 = \\begin{bmatrix} -0.3 \\\\ 0.2 \\\\ 0.1 \\end{bmatrix},\\quad\n    x_b = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\\quad\n    x_0^{\\text{true}} = \\begin{bmatrix} -0.25 \\\\ 0.15 \\\\ 0.2 \\end{bmatrix},\\quad\n    y_k = H A^k x_0^{\\text{true}},\\quad k \\in \\{0,1,2,3,4\\}.\n    $$\n- Case D (no observations: background term only):\n  - Dimension: $n = 3$, $p = 2$, $M = 0$.\n  - Matrices:\n    $$\n    A = \\begin{bmatrix}\n    0.7 & 0.1 & 0.0\\\\\n    0.0 & 0.8 & 0.2\\\\\n    0.0 & 0.0 & 0.9\n    \\end{bmatrix},\\quad\n    H = \\begin{bmatrix}\n    1.0 & 0.0 & 0.0\\\\\n    0.0 & 1.0 & 0.0\n    \\end{bmatrix},\\quad\n    B = \\operatorname{diag}(10.0, 1.0, 0.1),\\quad\n    R = I_2.\n    $$\n  - Vectors:\n    $$\n    x_0 = \\begin{bmatrix} 1.0 \\\\ -1.0 \\\\ 0.5 \\end{bmatrix},\\quad\n    x_b = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.\n    $$\n    There are no observations, so the sum in $J(x_0)$ is empty.\n\nDirectional derivative verification details:\n- Use a fixed seed to generate the set $\\mathcal{V}$ of random directions for each case.\n- Normalize each direction $v$ to unit Euclidean norm.\n- Use the same set cardinality $|\\mathcal{V}| = 8$ for all cases.\n- Use $\\varepsilon = 10^{-6}$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the maximum relative errors for Cases A–D, as a comma-separated list enclosed in square brackets, for example\n$$\n[\\text{errA},\\text{errB},\\text{errC},\\text{errD}].\n$$\nEach entry must be a floating-point number. No additional text should be printed.", "solution": "The problem requires the derivation and implementation of the gradient of the Four-Dimensional Variational (4D-Var) cost function for a linear time-invariant system. The gradient is to be computed using the adjoint method and verified against a finite-difference approximation.\n\nThe 4D-Var objective function is given by:\n$$\nJ(x_0) = \\frac{1}{2} (x_0 - x_b)^\\top B^{-1} (x_0 - x_b) + \\frac{1}{2} \\sum_{k=0}^{M-1} \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right)\n$$\nwhere the state vector $x_k$ evolves according to the linear model $x_{k+1} = A x_k$, with $x_k = A^k x_0$. The variable $x_0$ is the control variable, representing the initial state of the system.\n\nWe seek the gradient of $J$ with respect to $x_0$, denoted $\\nabla J(x_0)$.\n\n### Derivation of the Adjoint Model\nThe gradient can be derived efficiently using the method of Lagrange multipliers, which leads to the adjoint model. We consider the states $x_1, \\dots, x_{M-1}$ as independent variables subject to the constraints imposed by the model dynamics. The problem is to minimize $J$ subject to $x_k - A x_{k-1} = 0$ for $k=1, \\dots, M-1$.\n\nThe Lagrangian $\\mathcal{L}$ is constructed by augmenting the cost function with the model constraints, weighted by Lagrange multipliers $\\eta_k$ (the adjoint variables):\n$$\n\\mathcal{L}(x_0, \\dots, x_{M-1}, \\eta_1, \\dots, \\eta_{M-1}) = J(x_0, \\dots, x_{M-1}) + \\sum_{k=1}^{M-1} \\eta_k^\\top (A x_{k-1} - x_k)\n$$\nThe gradient of the original unconstrained cost function $J(x_0)$ is equal to the total derivative of the Lagrangian with respect to $x_0$, i.e., $\\nabla J(x_0) = (d\\mathcal{L}/dx_0)^\\top$. A first-order perturbation $\\delta x_0$ in the initial state induces perturbations $\\delta x_k$ in the trajectory. The corresponding change in $\\mathcal{L}$ is:\n$$\n\\delta \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial x_0} \\delta x_0 + \\sum_{k=1}^{M-1} \\frac{\\partial \\mathcal{L}}{\\partial x_k} \\delta x_k\n$$\nThe partial derivatives are:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_0} = \\frac{\\partial J}{\\partial x_0} + \\eta_1^\\top A\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_k} = \\frac{\\partial J}{\\partial x_k} - \\eta_k^\\top + \\eta_{k+1}^\\top A, \\quad \\text{for } k=1, \\dots, M-2\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{M-1}} = \\frac{\\partial J}{\\partial x_{M-1}} - \\eta_{M-1}^\\top\n$$\nThe partial derivatives of $J$ with respect to the states $x_k$ are:\n$$\n\\frac{\\partial J}{\\partial x_0} = B^{-1}(x_0 - x_b) + H^\\top R^{-1}(H x_0 - y_0)\n$$\n$$\n\\frac{\\partial J}{\\partial x_k} = H^\\top R^{-1}(H x_k - y_k), \\quad \\text{for } k=1, \\dots, M-1\n$$\nThe adjoint method defines the multipliers $\\eta_k$ by requiring that the partial derivatives of $\\mathcal{L}$ with respect to the intermediate states $x_k$ (for $k \\ge 1$) vanish. This choice elegantly eliminates the need to compute $\\delta x_k$ for $k > 0$. Setting $\\partial\\mathcal{L}/\\partial x_k = 0$ for $k=1, \\dots, M-1$ gives a set of equations for $\\eta_k$ that must be solved backwards in time:\n1. Terminal condition at $k=M-1$: $\\eta_{M-1} = \\left(\\frac{\\partial J}{\\partial x_{M-1}}\\right)^\\top = H^\\top R^{-1}(H x_{M-1} - y_{M-1})$\n2. Backward recursion for $k=M-2, \\dots, 1$: $\\eta_k = A^\\top \\eta_{k+1} + \\left(\\frac{\\partial J}{\\partial x_k}\\right)^\\top = A^\\top \\eta_{k+1} + H^\\top R^{-1}(H x_k - y_k)$\n\nWith this choice, $\\delta\\mathcal{L} = (\\partial\\mathcal{L}/\\partial x_0)\\delta x_0$. The gradient is therefore:\n$$\n\\nabla J(x_0) = \\left(\\frac{\\partial \\mathcal{L}}{\\partial x_0}\\right)^\\top = \\left(\\frac{\\partial J}{\\partial x_0}\\right)^\\top + A^\\top \\eta_1\n$$\nSubstituting the expressions for $(\\partial J/\\partial x_0)^\\top$ and $\\eta_1$:\n$$\n\\nabla J(x_0) = B^{-1}(x_0 - x_b) + H^\\top R^{-1}(H x_0 - y_0) + A^\\top \\eta_1\n$$\nWe can unify the adjoint recursion by defining $\\eta_M = 0$. The recursion $\\eta_k = A^\\top\\eta_{k+1} + H^\\top R^{-1}(H x_k - y_k)$ then also holds for $k=M-1$. Let us define $\\eta_0 = A^\\top \\eta_1 + H^\\top R^{-1}(H x_0 - y_0)$. The recursion then extends down to $k=0$. The full gradient is then compactly written as:\n$$\n\\nabla J(x_0) = B^{-1}(x_0 - x_b) + \\eta_0\n$$\nwhere $\\eta_0$ is the result of the full backward integration.\n\n### Algorithm\nThe algorithm to compute the gradient $\\nabla J(x_0)$ is a two-stage process:\n\n1.  **Forward Integration**: Starting from the initial condition $x_0$, run the model forward in time to compute the state trajectory $x_k = A x_{k-1}$ for $k=1, \\dots, M-1$. The trajectory $\\{x_k\\}_{k=0}^{M-1}$ is stored.\n\n2.  **Backward (Adjoint) Integration**:\n    a. Initialize the adjoint variable at the final time plus one step: $\\eta_M = 0$.\n    b. Iterate backwards in time from $k=M-1$ down to $0$. In each step, compute the current adjoint state $\\eta_k$ using the future adjoint state $\\eta_{k+1}$ and the model state $x_k$:\n       $$\n       \\eta_k = A^\\top \\eta_{k+1} + H^\\top R^{-1}(H x_k - y_k)\n       $$\n    The final variable computed, $\\eta_0$, represents the gradient contribution from the observation term.\n\n3.  **Gradient Assembly**: Combine the gradient from the background term with the result from the adjoint integration:\n    $$\n    \\nabla J(x_0) = B^{-1}(x_0 - x_b) + \\eta_0\n    $$\n\n### Gradient Verification\nTo verify the correctness of the analytical gradient, we compare its projection along a direction $v$ with a finite difference approximation of the directional derivative. The directional derivative of $J$ at $x_0$ in the direction $v$ is given by the inner product $\\langle \\nabla J(x_0), v \\rangle = v^\\top \\nabla J(x_0)$.\nA second-order accurate numerical approximation is given by the central finite difference formula:\n$$\nD_v J(x_0) \\approx \\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2\\varepsilon}\n$$\nfor a small step size $\\varepsilon$. We compute the maximum relative discrepancy over a set of random unit vectors $\\mathcal{V}$:\n$$\n\\max_{v \\in \\mathcal{V}} \\frac{\\left|v^\\top \\nabla J(x_0) - \\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2 \\varepsilon}\\right|}{\\max\\left(1, \\left|v^\\top \\nabla J(x_0)\\right|, \\left|\\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2 \\varepsilon}\\right|\\right)}\n$$\nA small value for this error provides high confidence in the correctness of the derived and implemented gradient. The denominator is a robust scaling factor that prevents division by zero and provides a meaningful error measure even when the directional derivative is close to zero.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that sets up and runs all test cases for the 4D-Var gradient check.\n    \"\"\"\n\n    def run_case(n, p, M, A, H, B, R, x0, xb, observations_y, case_name):\n        \"\"\"\n        Runs a single test case: computes analytical gradient, compares with finite\n        differences, and returns the maximum relative error.\n        \n        Args:\n            n (int): State dimension.\n            p (int): Observation dimension.\n            M (int): Number of observation times.\n            A (np.ndarray): State transition matrix.\n            H (np.ndarray): Observation matrix.\n            B (np.ndarray): Background error covariance matrix.\n            R (np.ndarray): Observation error covariance matrix.\n            x0 (np.ndarray): Initial state (control variable).\n            xb (np.ndarray): Background state.\n            observations_y (list): List of observation vectors.\n            case_name (str): Name of the test case for logging.\n            \n        Returns:\n            float: Maximum relative error found across all random directions.\n        \"\"\"\n        \n        B_inv = np.linalg.inv(B)\n        R_inv = np.linalg.inv(R)\n        eps = 1e-6\n        num_directions = 8\n\n        def cost_J(current_x0):\n            \"\"\"Computes the 4D-Var cost function J(x0).\"\"\"\n            \n            # Background term\n            dev_b = current_x0 - xb\n            J_b = 0.5 * dev_b.T @ B_inv @ dev_b\n\n            # Observation term\n            J_o = 0.0\n            if M > 0:\n                xk = np.copy(current_x0)\n                for k in range(M):\n                    yk = observations_y[k]\n                    dev_o = H @ xk - yk\n                    J_o += 0.5 * dev_o.T @ R_inv @ dev_o\n                    if k  M - 1:\n                        xk = A @ xk\n            \n            return J_b + J_o\n\n        def grad_J_adjoint(current_x0):\n            \"\"\"Computes the gradient of J using the adjoint method.\"\"\"\n            \n            # Gradient of the background term\n            grad_b = B_inv @ (current_x0 - xb)\n\n            # Gradient of the observation term\n            grad_o = np.zeros(n)\n            if M > 0:\n                # Forward pass: compute and store the state trajectory\n                trajectory = []\n                xk = np.copy(current_x0)\n                for _ in range(M):\n                    trajectory.append(xk)\n                    if _  M - 1:\n                        xk = A @ xk\n\n                # Backward (adjoint) pass\n                eta_k_plus_1 = np.zeros(n)\n                for k in range(M - 1, -1, -1):\n                    xk = trajectory[k]\n                    yk = observations_y[k]\n                    dev_o = H @ xk - yk\n                    forcing = H.T @ (R_inv @ dev_o)\n                    eta_k = A.T @ eta_k_plus_1 + forcing\n                    eta_k_plus_1 = eta_k\n                \n                grad_o = eta_k_plus_1\n            \n            return grad_b + grad_o\n\n        # --- Verification ---\n        grad_analytic = grad_J_adjoint(x0)\n        \n        rng = np.random.default_rng(seed=12345)\n        random_directions = rng.standard_normal(size=(num_directions, n))\n        \n        errors = []\n        for v in random_directions:\n            v_norm = v / np.linalg.norm(v)\n\n            # Analytical directional derivative\n            dd_analytic = v_norm.T @ grad_analytic\n\n            # Numerical directional derivative (central difference)\n            J_plus = cost_J(x0 + eps * v_norm)\n            J_minus = cost_J(x0 - eps * v_norm)\n            dd_numeric = (J_plus - J_minus) / (2 * eps)\n\n            # Robust relative error\n            numerator = np.abs(dd_analytic - dd_numeric)\n            denominator = np.max([1.0, np.abs(dd_analytic), np.abs(dd_numeric)])\n            relative_error = numerator / denominator\n            errors.append(relative_error)\n            \n        return np.max(errors)\n\n    # --- Test Case Definitions ---\n    test_suite = []\n\n    # Case A\n    n_a, p_a, M_a = 3, 3, 3\n    A_a = np.identity(n_a)\n    H_a = np.identity(p_a)\n    B_a = np.identity(n_a)\n    R_a = np.identity(p_a)\n    x0_a = np.array([1.0, -2.0, 0.5])\n    xb_a = np.array([0.5, -2.5, 1.5])\n    y_a = [np.array([1.2, -1.8, 0.7]), np.array([0.9, -2.3, 0.4]), np.array([1.1, -2.1, 0.6])]\n    test_suite.append({'n': n_a, 'p': p_a, 'M': M_a, 'A': A_a, 'H': H_a, 'B': B_a, 'R': R_a, 'x0': x0_a, 'xb': xb_a, 'observations_y': y_a, 'case_name': 'A'})\n\n    # Case B\n    n_b, p_b, M_b = 4, 2, 6\n    A_b = np.array([[0.9, 0.2, 0.0, 0.0], [0.0, 0.9, 0.2, 0.0], [0.0, 0.0, 0.9, 0.2], [0.0, 0.0, 0.0, 0.9]])\n    H_b = np.array([[1.0, 0.0, 0.5, 0.0], [0.0, 1.0, 0.0, 0.5]])\n    M_mat_b = np.array([[2.0, -0.3, 0.0, 0.0], [-0.3, 1.5, 0.0, 0.0], [0.0, 0.0, 1.2, 0.2], [0.0, 0.0, 0.2, 1.1]])\n    B_b = M_mat_b @ M_mat_b.T + 0.1 * np.identity(n_b)\n    R_b = np.array([[1.0, 0.2], [0.2, 0.5]])\n    x0_b = np.array([0.1, -0.9, 0.0, 0.5])\n    xb_b = np.array([0.0, -0.8, 0.2, 0.3])\n    x0_true_b = np.array([0.5, -1.0, 0.3, 0.7])\n    y_b = []\n    xk_true = np.copy(x0_true_b)\n    for k in range(M_b):\n        bias = np.array([0.01 * k, -0.005 * k])\n        y_b.append(H_b @ xk_true + bias)\n        xk_true = A_b @ xk_true\n    test_suite.append({'n': n_b, 'p': p_b, 'M': M_b, 'A': A_b, 'H': H_b, 'B': B_b, 'R': R_b, 'x0': x0_b, 'xb': xb_b, 'observations_y': y_b, 'case_name': 'B'})\n\n    # Case C\n    n_c, p_c, M_c = 3, 3, 5\n    A_c = np.array([[1.2, 0.1, 0.0], [0.0, 0.95, 0.05], [0.0, 0.0, 1.05]])\n    H_c = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.2, 0.0, 1.0]])\n    Q_c = np.array([[1.0, 0.9, 0.0], [0.9, 1.0, 0.0], [0.0, 0.0, 0.01]])\n    B_c = Q_c + 0.1 * np.identity(n_c)\n    R_c = 0.05 * np.identity(p_c)\n    x0_c = np.array([-0.3, 0.2, 0.1])\n    xb_c = np.zeros(n_c)\n    x0_true_c = np.array([-0.25, 0.15, 0.2])\n    y_c = []\n    xk_true = np.copy(x0_true_c)\n    for _ in range(M_c):\n        y_c.append(H_c @ xk_true)\n        xk_true = A_c @ xk_true\n    test_suite.append({'n': n_c, 'p': p_c, 'M': M_c, 'A': A_c, 'H': H_c, 'B': B_c, 'R': R_c, 'x0': x0_c, 'xb': xb_c, 'observations_y': y_c, 'case_name': 'C'})\n\n    # Case D\n    n_d, p_d, M_d = 3, 2, 0\n    A_d = np.array([[0.7, 0.1, 0.0], [0.0, 0.8, 0.2], [0.0, 0.0, 0.9]])\n    H_d = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n    B_d = np.diag([10.0, 1.0, 0.1])\n    R_d = np.identity(p_d)\n    x0_d = np.array([1.0, -1.0, 0.5])\n    xb_d = np.zeros(n_d)\n    y_d = []\n    test_suite.append({'n': n_d, 'p': p_d, 'M': M_d, 'A': A_d, 'H': H_d, 'B': B_d, 'R': R_d, 'x0': x0_d, 'xb': xb_d, 'observations_y': y_d, 'case_name': 'D'})\n    \n    # --- Execute and collect results ---\n    results = []\n    for params in test_suite:\n        max_error = run_case(**params)\n        results.append(max_error)\n\n    # --- Print final output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3406525"}, {"introduction": "Adjoint sensitivity analysis is built upon the tangent-linear hypothesis, which assumes that the response of a nonlinear system or observation operator to a small perturbation can be accurately approximated by a linear operator—the Jacobian. While this linearization is powerful, its validity is not guaranteed and depends on the degree of nonlinearity and the size of the perturbation. Before trusting the results of an adjoint-based analysis, it is crucial to understand the domain where this approximation holds. This practice provides a hands-on diagnostic to test the tangent-linear hypothesis, allowing you to quantify the error of the linear approximation and verify its expected convergence properties, thereby developing a deeper, more critical understanding of the foundations of adjoint methods [@problem_id:3406508].", "problem": "Consider an observation operator $h:\\mathbb{R}^n\\to\\mathbb{R}^m$ that maps model state vectors to observation space. In variational data assimilation and inverse problems, adjoint sensitivity analysis relies on the tangent-linear model (TLM), which assumes that for sufficiently small perturbations $\\delta x$ around a base state $x$, the increment $h(x+\\delta x)-h(x)$ is well-approximated by the Jacobian $H$ of $h$ at $x$ applied to $\\delta x$. The tangent-linear hypothesis is grounded in first-order differentiability and the first-order Taylor theorem, which state that if $h$ is differentiable at $x$, then\n$$\nh(x+\\delta x) = h(x) + H\\,\\delta x + r(\\delta x),\n$$\nwhere $H$ is the Jacobian matrix of $h$ at $x$, and the remainder $r(\\delta x)$ satisfies $\\lVert r(\\delta x)\\rVert_2 / \\lVert \\delta x\\rVert_2 \\to 0$ as $\\lVert \\delta x\\rVert_2 \\to 0$, with $\\lVert\\cdot\\rVert_2$ denoting the Euclidean norm. For twice continuously differentiable $h$, the leading behavior of $r(\\delta x)$ is second order in $\\delta x$.\n\nDesign and implement a diagnostic to test the validity of the tangent-linear hypothesis by comparing the nonlinear increment $h(x+\\alpha\\,\\delta x)-h(x)$ to its linear approximation $H\\,\\alpha\\,\\delta x$ across a range of step sizes $\\alpha$. The diagnostic must quantify:\n- The empirical order $p$ of the residual $\\lVert r(\\alpha)\\rVert_2$ as a function of $\\alpha$, obtained by a least-squares fit of $\\log\\big(\\lVert r(\\alpha)\\rVert_2\\big)$ versus $\\log(\\alpha)$ over a prescribed set of $\\alpha$ values. Ideally, $p\\approx 2$ for smooth nonlinear $h$ and residuals dominated by second-order terms.\n- A monotonicity indicator that evaluates whether the relative error $e(\\alpha) = \\lVert r(\\alpha)\\rVert_2 / \\lVert H\\,\\alpha\\,\\delta x\\rVert_2$ is nonincreasing as $\\alpha$ decreases, which is consistent with first-order validity. Use a small regularization parameter to avoid division by zero when $\\lVert H\\,\\alpha\\,\\delta x\\rVert_2=0$.\n\nBase your derivation on fundamental definitions of differentiability, the Jacobian, and Taylor expansion. Do not assume any special structure beyond what is specified in the test suite. All quantities are dimensionless; no physical units are involved. Angles, where present, must be in radians.\n\nImplement the program to compute the diagnostic for the following test suite. For each case, $x\\in\\mathbb{R}^n$, $\\delta x\\in\\mathbb{R}^n$, and the set of step sizes is\n$$\n\\alpha \\in \\left\\{10^{-1},\\,5\\cdot 10^{-2},\\,2.5\\cdot 10^{-2},\\,1.25\\cdot 10^{-2},\\,6.25\\cdot 10^{-3},\\,3.125\\cdot 10^{-3},\\,10^{-3}\\right\\}.\n$$\n\nTest Case A (nonlinear logistic observation, from $\\mathbb{R}^2$ to $\\mathbb{R}^1$):\n- Define $h(x)=\\sigma(a^\\top x)$, where $\\sigma(z)=\\dfrac{1}{1+e^{-z}}$ and $a\\in\\mathbb{R}^2$ is fixed.\n- The Jacobian at $x$ is $H(x) = \\sigma(a^\\top x)\\left(1-\\sigma(a^\\top x)\\right)\\,a^\\top \\in \\mathbb{R}^{1\\times 2}$.\n- Use $a=\\begin{bmatrix}2\\\\-1\\end{bmatrix}$, $x=\\begin{bmatrix}0.3\\\\-0.2\\end{bmatrix}$, $\\delta x=\\begin{bmatrix}0.4\\\\0.1\\end{bmatrix}$.\n\nTest Case B (linear observation, from $\\mathbb{R}^2$ to $\\mathbb{R}^2$):\n- Define $h(x)=M\\,x$, where $M\\in\\mathbb{R}^{2\\times 2}$ is fixed, and $H(x)=M$.\n- Use $M=\\begin{bmatrix}12\\\\-34\\end{bmatrix}$, $x=\\begin{bmatrix}0.5\\\\-0.4\\end{bmatrix}$, $\\delta x=\\begin{bmatrix}0.2\\\\-0.1\\end{bmatrix}$.\n\nTest Case C (smooth nonlinear mixed observation, from $\\mathbb{R}^3$ to $\\mathbb{R}^2$):\n- Define $h(x)=\\begin{bmatrix}\\sin(b^\\top x)\\\\x_1^2 + e^{x_2} - \\tanh(x_3)\\end{bmatrix}$, where $b\\in\\mathbb{R}^3$ is fixed, $\\sin(\\cdot)$ is the sine function in radians, $e^{(\\cdot)}$ is the exponential function, and $\\tanh(\\cdot)$ is the hyperbolic tangent. The hyperbolic secant is defined by $\\operatorname{sech}(z)=\\dfrac{1}{\\cosh(z)}$.\n- The Jacobian at $x$ is\n$$\nH(x)=\\begin{bmatrix}\n\\cos(b^\\top x)\\,b^\\top\\\\\n2x_1  e^{x_2}  -\\operatorname{sech}^2(x_3)\n\\end{bmatrix}\\in\\mathbb{R}^{2\\times 3}.\n$$\n- Use $b=\\begin{bmatrix}1.0\\\\-2.0\\\\0.5\\end{bmatrix}$, $x=\\begin{bmatrix}0.1\\\\-0.3\\\\0.2\\end{bmatrix}$, $\\delta x=\\begin{bmatrix}-0.05\\\\0.02\\\\0.04\\end{bmatrix}$.\n\nFor each test case and each $\\alpha$, compute the nonlinear increment $d_h(\\alpha)=h(x+\\alpha\\,\\delta x)-h(x)$, the tangent-linear increment $t(\\alpha)=H(x)\\,\\alpha\\,\\delta x$, and the residual $r(\\alpha)=d_h(\\alpha)-t(\\alpha)$. Use the Euclidean norm to form $\\lVert r(\\alpha)\\rVert_2$ and $\\lVert t(\\alpha)\\rVert_2$. Estimate the empirical order $p$ by fitting a straight line to the points $\\big(\\log(\\alpha),\\log(\\lVert r(\\alpha)\\rVert_2+\\varepsilon)\\big)$ via least squares, where $\\varepsilon$ is a small regularization such as $\\varepsilon=10^{-12}$ to avoid taking the logarithm of zero. Determine the monotonicity indicator as a boolean that is true if and only if $e(\\alpha)$ is nonincreasing as $\\alpha$ decreases within a tolerance of $\\tau=10^{-12}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The output should consist of $6$ items: for each test case, the empirical order $p$ as a float followed by the monotonicity indicator as a boolean, in the order Test Case A, Test Case B, Test Case C. For example, an output line has the format $[p_A,\\mathrm{mono}_A,p_B,\\mathrm{mono}_B,p_C,\\mathrm{mono}_C]$ with no spaces. All floats are dimensionless and must be printed in the program’s default floating-point format, and all booleans must be printed as either $\\mathrm{True}$ or $\\mathrm{False}$.", "solution": "The problem requires the design and implementation of a diagnostic test to validate the tangent-linear hypothesis, a cornerstone of variational data assimilation and adjoint sensitivity analysis. This hypothesis posits that for a differentiable observation operator $h: \\mathbb{R}^n \\to \\mathbb{R}^m$, the perturbation in the observation space, $h(x+\\delta x) - h(x)$, can be accurately approximated by the action of the Jacobian matrix $H$ on the state perturbation $\\delta x$, i.e., $H\\delta x$.\n\nThe validation is based on the first-order Taylor expansion of $h$ around a state $x$:\n$$\nh(x + \\delta x) = h(x) + H \\delta x + r(\\delta x)\n$$\nwhere $H$ is the Jacobian of $h$ evaluated at $x$, and $r(\\delta x)$ is the remainder term, which satisfies $\\lim_{\\lVert \\delta x \\rVert_2 \\to 0} \\frac{\\lVert r(\\delta x) \\rVert_2}{\\lVert \\delta x \\rVert_2} = 0$. For a twice continuously differentiable function $h$, the remainder term is of second order in the perturbation, i.e., $\\lVert r(\\delta x) \\rVert_2 = O(\\lVert \\delta x \\rVert_2^2)$.\n\nTo test this behavior, we introduce a scaling parameter $\\alpha$ for the perturbation, such that $\\delta x \\to \\alpha \\delta x$. The quantities of interest are:\n1.  The nonlinear increment: $d_h(\\alpha) = h(x + \\alpha \\delta x) - h(x)$.\n2.  The tangent-linear increment: $t(\\alpha) = H(x) (\\alpha \\delta x) = \\alpha (H(x) \\delta x)$.\n3.  The residual vector: $r(\\alpha) = d_h(\\alpha) - t(\\alpha)$.\n\nThe diagnostic consists of two parts, evaluated over a prescribed decreasing sequence of $\\alpha$ values.\n\n**1. Empirical Order of Convergence ($p$)**\nThe theoretical second-order behavior of the residual implies that for small $\\alpha$, the Euclidean norm of the residual should scale quadratically:\n$$\n\\lVert r(\\alpha) \\rVert_2 \\approx C \\alpha^2\n$$\nfor some constant $C$ that depends on the second derivatives of $h$, $x$, and $\\delta x$. Taking the natural logarithm of both sides yields a linear relationship:\n$$\n\\log(\\lVert r(\\alpha) \\rVert_2) \\approx \\log(C) + 2 \\log(\\alpha)\n$$\nThis equation is of the form $y = m x + b$, where $y = \\log(\\lVert r(\\alpha) \\rVert_2)$, $x = \\log(\\alpha)$, and the slope $m$ is the order of convergence, which we expect to be $p=2$.\n\nTo estimate $p$ empirically, we compute the set of points $(\\log(\\alpha_i), \\log(\\lVert r(\\alpha_i) \\rVert_2 + \\varepsilon))$ for each given step size $\\alpha_i$. A small regularization constant $\\varepsilon  0$ is added to the norm to prevent numerical issues with $\\log(0)$, which can occur if the residual is exactly zero (e.g., for a linear operator $h$). The empirical order $p$ is then determined as the slope of the best-fit line through these points, obtained via linear least-squares regression. For a linear operator $h$, the residual $r(\\alpha)$ is identically zero. Thus, $\\log(\\lVert r(\\alpha_i) \\rVert_2 + \\varepsilon)$ is constant, and the resulting slope $p$ will be approximately $0$.\n\n**2. Monotonicity of Relative Error**\nThe validity of the tangent-linear model as a first-order approximation implies that the relative error should decrease as the perturbation size decreases. We define the relative error as:\n$$\ne(\\alpha) = \\frac{\\lVert r(\\alpha) \\rVert_2}{\\lVert t(\\alpha) \\rVert_2} = \\frac{\\lVert h(x + \\alpha \\delta x) - h(x) - \\alpha H \\delta x \\rVert_2}{\\lVert \\alpha H \\delta x \\rVert_2}\n$$\nThe numerator is $O(\\alpha^2)$ while the denominator is $O(\\alpha)$, assuming $H\\delta x \\neq 0$. Therefore, $e(\\alpha) = O(\\alpha)$, which means $e(\\alpha)$ should approach $0$ as $\\alpha \\to 0$. A key diagnostic is to check if the function $e(\\alpha)$ is nonincreasing as $\\alpha$ decreases.\n\nTo implement this check, we compute $e(\\alpha_i)$ for the given sequence of decreasing step sizes $\\{\\alpha_1, \\alpha_2, \\dots, \\alpha_k\\}$. The monotonicity indicator is a boolean value which is `True` if and only if $e(\\alpha_{i+1}) \\le e(\\alpha_i) + \\tau$ for all $i \\in \\{1, \\dots, k-1\\}$, where $\\tau$ is a small numerical tolerance to account for floating-point inaccuracies. A small regularization parameter is also added to the denominator of $e(\\alpha)$ to prevent division by zero.\n\nThe overall algorithm proceeds by iterating through each test case. For each case, the Jacobian $H(x)$ is evaluated at the base state $x$. Then, for each $\\alpha$ in the provided set, the norms of the residual, $\\lVert r(\\alpha) \\rVert_2$, and the tangent-linear increment, $\\lVert t(\\alpha) \\rVert_2$, are computed and stored. Finally, these stored values are used to calculate the empirical order $p$ and the monotonicity indicator.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Computes diagnostic tests for the tangent-linear hypothesis for three test cases.\n    \"\"\"\n    \n    # Define global parameters from the problem statement.\n    alphas = np.array([1e-1, 5e-2, 2.5e-2, 1.25e-2, 6.25e-3, 3.125e-3, 1e-3])\n    epsilon = 1e-12\n    tau = 1e-12\n\n    # --- Test Case A: Nonlinear Logistic Observation ---\n    a_A = np.array([2.0, -1.0])\n    x_A = np.array([0.3, -0.2])\n    dx_A = np.array([0.4, 0.1])\n    \n    def sigma(z):\n        return 1.0 / (1.0 + np.exp(-z))\n        \n    def h_A(x_vec):\n        return np.array([sigma(a_A @ x_vec)])\n        \n    def H_A(x_vec):\n        s_ax = sigma(a_A @ x_vec)\n        return s_ax * (1.0 - s_ax) * a_A.reshape(1, 2)\n\n    # --- Test Case B: Linear Observation ---\n    M_B = np.array([[1.0, 2.0], [-3.0, 4.0]])\n    x_B = np.array([0.5, -0.4])\n    dx_B = np.array([0.2, -0.1])\n\n    def h_B(x_vec):\n        return M_B @ x_vec\n\n    def H_B(x_vec):\n        # Jacobian of a linear map is the matrix itself, independent of x_vec.\n        return M_B\n\n    # --- Test Case C: Smooth Nonlinear Mixed Observation ---\n    b_C = np.array([1.0, -2.0, 0.5])\n    x_C = np.array([0.1, -0.3, 0.2])\n    dx_C = np.array([-0.05, 0.02, 0.04])\n\n    def h_C(x_vec):\n        return np.array([\n            np.sin(b_C @ x_vec),\n            x_vec[0]**2 + np.exp(x_vec[1]) - np.tanh(x_vec[2])\n        ])\n\n    def H_C(x_vec):\n        # sech^2(z) = (1/cosh(z))^2\n        sech_sq_x3 = (1.0 / np.cosh(x_vec[2]))**2\n        return np.array([\n            np.cos(b_C @ x_vec) * b_C,\n            [2.0 * x_vec[0], np.exp(x_vec[1]), -sech_sq_x3]\n        ])\n\n    test_cases = [\n        {'h': h_A, 'H_func': H_A, 'x': x_A, 'dx': dx_A},\n        {'h': h_B, 'H_func': H_B, 'x': x_B, 'dx': dx_B},\n        {'h': h_C, 'H_func': H_C, 'x': x_C, 'dx': dx_C},\n    ]\n\n    results = []\n    for case in test_cases:\n        h, H_func, x, dx = case['h'], case['H_func'], case['x'], case['dx']\n        \n        # Pre-compute constant terms\n        h_x = h(x)\n        H_x = H_func(x)\n        \n        r_norms = []\n        t_norms = []\n        \n        for alpha in alphas:\n            # Nonlinear increment\n            d_h = h(x + alpha * dx) - h_x\n            \n            # Tangent-linear increment\n            t_alpha = H_x @ (alpha * dx)\n            \n            # Residual\n            r_alpha = d_h - t_alpha\n            \n            # Store norms for analysis\n            r_norms.append(np.linalg.norm(r_alpha))\n            t_norms.append(np.linalg.norm(t_alpha))\n            \n        r_norms = np.array(r_norms)\n        t_norms = np.array(t_norms)\n\n        # 1. Compute empirical order p\n        log_alphas = np.log(alphas)\n        log_r_norms = np.log(r_norms + epsilon)\n        # linear regression of log_r_norms on log_alphas\n        lin_reg_result = stats.linregress(log_alphas, log_r_norms)\n        p = lin_reg_result.slope\n        \n        # 2. Compute monotonicity indicator for relative error\n        # Add epsilon to denominator to handle cases where t_norms is zero,\n        # as specified in the problem.\n        relative_errors = r_norms / (t_norms + epsilon)\n        \n        is_monotonic = True\n        # The list of alphas is already sorted in descending order.\n        # We check if the corresponding relative_errors are non-increasing.\n        for i in range(len(relative_errors) - 1):\n            if relative_errors[i+1] > relative_errors[i] + tau:\n                is_monotonic = False\n                break\n        \n        results.append(str(p))\n        results.append(str(is_monotonic))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3406508"}, {"introduction": "A primary application of adjoint methods in fields like meteorology and oceanography is to estimate the impact of individual observations on a key forecast metric. This allows scientists to assess the value of different components of the observing system without incurring the prohibitive cost of re-running the entire assimilation and forecast cycle for each observation's removal. The adjoint-based calculation is an efficient approximation based on a linearization around the final analysis state. This culminating practice pits the efficient tangent-linear adjoint approximation against the true, \"brute-force\" impact calculated by explicitly removing an observation and re-optimizing. By exploring regimes of varying nonlinearity, you will gain practical insight into the accuracy of the adjoint method and its limitations when quantifying observation impact [@problem_id:3406515].", "problem": "Consider a static Three-Dimensional Variational (3D-Var) data assimilation problem with a nonlinear, saturating observation operator. The state vector is $\\mathbf{x} \\in \\mathbb{R}^n$ and the observation vector is $\\mathbf{y} \\in \\mathbb{R}^m$. The cost function to be minimized is\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x}-\\mathbf{x}_b)^\\top \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\frac{1}{2}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right)^\\top \\mathbf{R}^{-1}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right),\n$$\nwhere $\\mathbf{x}_b \\in \\mathbb{R}^n$ is the background state, $\\mathbf{B} \\in \\mathbb{R}^{n \\times n}$ is the symmetric positive-definite background error covariance, $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$ is the symmetric positive-definite observation error covariance, and the observation operator is defined elementwise by\n$$\n\\mathbf{h}(\\mathbf{x}) = \\tanh(\\mathbf{C}\\mathbf{x}),\n$$\nwith $\\mathbf{C} \\in \\mathbb{R}^{m \\times n}$ and $\\tanh(\\cdot)$ applied elementwise.\n\nDefine a scalar forecast metric $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$ for a fixed weight vector $\\mathbf{w} \\in \\mathbb{R}^n$. The goal is to quantify and compare observation impacts using two approaches:\n- A tangent-linear adjoint sensitivity approximation derived from the optimality condition and Gauss-Newton linearization.\n- A finite-difference “brute-force” removal impact by re-optimizing the cost function after removing an observation.\n\nUse the following foundational elements as the starting point of your derivations:\n- The optimality condition for the minimizer $\\mathbf{x}_a$ is given by the vanishing gradient $\\nabla J(\\mathbf{x}_a) = \\mathbf{0}$.\n- The Jacobian of $\\mathbf{h}(\\mathbf{x})$ is $\\mathbf{H}(\\mathbf{x}) = \\operatorname{diag}\\left(\\operatorname{sech}^2(\\mathbf{C}\\mathbf{x})\\right)\\mathbf{C}$, where $\\operatorname{sech}(z) = 1/\\cosh(z)$ and $\\operatorname{sech}^2(z) = 1 - \\tanh^2(z)$.\n- The Hessian of $J(\\mathbf{x})$ consists of the background term and the observation term, which includes both the Gauss-Newton approximation and second derivative contributions from $\\mathbf{h}(\\mathbf{x})$.\n\nFor each test case specified below, perform the following steps in pure mathematical and algorithmic terms:\n1. Compute the analysis state $\\mathbf{x}_a$ by minimizing $J(\\mathbf{x})$ exactly (to numerical tolerance) using a second-order method with the full Hessian (including second derivatives of $\\mathbf{h}$).\n2. For each observation index $i \\in \\{1,\\dots,m\\}$, define the removal experiment by setting the weight of the $i$-th observation to zero. Formally, introduce a diagonal scaling vector $\\boldsymbol{\\alpha} \\in \\mathbb{R}^m$ applied to the observation term so that the modified cost is\n$$\nJ_{\\boldsymbol{\\alpha}}(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x}-\\mathbf{x}_b)^\\top \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\frac{1}{2}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right)^\\top \\operatorname{diag}(\\boldsymbol{\\alpha})\\mathbf{R}^{-1}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right).\n$$\nThe baseline is $\\boldsymbol{\\alpha} = \\mathbf{1}$; removal of observation $i$ sets $\\alpha_i = 0$ while keeping $\\alpha_j = 1$ for $j \\neq i$. Compute the re-optimized analysis $\\mathbf{x}_a^{(-i)}$ and the brute-force removal impact on the forecast metric\n$$\n\\Delta f_i^{\\text{bf}} = f(\\mathbf{x}_a^{(-i)}) - f(\\mathbf{x}_a).\n$$\n3. Derive, from first principles via the optimality condition and the Gauss-Newton approximation (neglecting second derivative terms of $\\mathbf{h}$ in the Hessian), the tangent-linear adjoint sensitivity of $f(\\mathbf{x})$ to the observation scaling parameter $\\alpha_i$. Use this to obtain a first-order approximation to the removal impact\n$$\n\\Delta f_i^{\\text{tl}} \\approx \\text{(appropriate linearization using the Gauss-Newton inverse, the Jacobian, and the analyzed innovation)}.\n$$\nImplement this approximation at the analyzed state $\\mathbf{x}_a$ and compute $\\Delta f_i^{\\text{tl}}$ for each $i$.\n4. Compare $\\Delta f_i^{\\text{bf}}$ and $\\Delta f_i^{\\text{tl}}$ across all $i$ by computing:\n   - The maximum absolute discrepancy $E_{\\max} = \\max_i \\left|\\Delta f_i^{\\text{bf}} - \\Delta f_i^{\\text{tl}}\\right|$.\n   - The count of underestimates, defined as the number of indices $i$ for which $\\left|\\Delta f_i^{\\text{tl}}\\right|  \\left|\\Delta f_i^{\\text{bf}}\\right|\\cdot(1 - \\varepsilon)$.\n   - The count of overestimates, defined as the number of indices $i$ for which $\\left|\\Delta f_i^{\\text{tl}}\\right|  \\left|\\Delta f_i^{\\text{bf}}\\right|\\cdot(1 + \\varepsilon)$.\nUse a relative tolerance of $\\varepsilon = 0.05$ (a decimal fraction) for classification. Any remaining indices are considered within tolerance and are not counted in either category.\n5. Report the results for each test case as a triple $[E_{\\max}, \\text{count\\_under}, \\text{count\\_over}]$.\n\nThere are no physical units involved in this problem; all quantities are dimensionless.\n\nTest Suite:\nAll tests use $n = 3$, $m = 3$, the same $\\mathbf{B}$, $\\mathbf{R}$, and $\\mathbf{w}$, but different $\\mathbf{C}$, $\\mathbf{x}_b$, $\\mathbf{x}_t$, and observation noise. For each test, the observation vector is generated by $\\mathbf{y} = \\tanh(\\mathbf{C}\\mathbf{x}_t) + \\boldsymbol{\\eta}$.\n\nCommon parameters:\n- $\\mathbf{B} = \\operatorname{diag}\\left([0.4^2,\\,0.6^2,\\,0.8^2]\\right)$.\n- $\\mathbf{R} = \\operatorname{diag}\\left([0.1^2,\\,0.2^2,\\,0.3^2]\\right)$.\n- $\\mathbf{w} = [1.0,\\,-0.5,\\,0.25]^\\top$.\n\nTest 1 (near-linear regime):\n- $\\mathbf{C} = \\begin{bmatrix} 0.5  0.0  0.1 \\\\ 0.0  0.5  -0.2 \\\\ 0.1  -0.1  0.4 \\end{bmatrix}$.\n- $\\mathbf{x}_b = [0.2,\\,-0.1,\\,0.05]^\\top$.\n- $\\mathbf{x}_t = [0.25,\\,-0.15,\\,0.10]^\\top$.\n- $\\boldsymbol{\\eta} = [0.01,\\,-0.005,\\,0.02]^\\top$.\n\nTest 2 (moderate saturation regime):\n- $\\mathbf{C} = \\begin{bmatrix} 2.0  0.0  1.0 \\\\ 0.0  2.5  -1.5 \\\\ 1.0  -1.0  2.0 \\end{bmatrix}$.\n- $\\mathbf{x}_b = [1.0,\\,-1.5,\\,0.8]^\\top$.\n- $\\mathbf{x}_t = [1.2,\\,-1.3,\\,1.0]^\\top$.\n- $\\boldsymbol{\\eta} = [0.05,\\,0.02,\\,-0.04]^\\top$.\n\nTest 3 (strong saturation regime):\n- $\\mathbf{C} = \\begin{bmatrix} 5.0  -3.0  2.0 \\\\ -4.0  6.0  -3.0 \\\\ 3.5  -2.5  5.5 \\end{bmatrix}$.\n- $\\mathbf{x}_b = [2.0,\\,-2.0,\\,2.0]^\\top$.\n- $\\mathbf{x}_t = [2.2,\\,-2.2,\\,2.1]^\\top$.\n- $\\boldsymbol{\\eta} = [0.01,\\,-0.02,\\,0.03]^\\top$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the three tests as a comma-separated list enclosed in square brackets, where each test case contributes its own bracketed triple. For example, the output should look like\n$$\n[\\,[E_{\\max}^{(1)},\\,\\text{count\\_under}^{(1)},\\,\\text{count\\_over}^{(1)}],\\,[E_{\\max}^{(2)},\\,\\text{count\\_under}^{(2)},\\,\\text{count\\_over}^{(2)}],\\,[E_{\\max}^{(3)},\\,\\text{count\\_under}^{(3)},\\,\\text{count\\_over}^{(3)}]\\,].\n$$\nAll numbers must be plain decimals or integers without additional text. The computation must be fully self-contained and reproducible with the specified parameters and procedures.", "solution": "The objective is to analyze the impact of individual observations on a forecast metric in a three-dimensional variational (3D-Var) data assimilation context. The observation operator is nonlinear and exhibits saturation. We will compare two methods for quantifying this impact: a \"brute-force\" approach involving the re-optimization of the cost function after removing an observation, and a computationally cheaper tangent-linear adjoint sensitivity approximation.\n\nThe state vector is $\\mathbf{x} \\in \\mathbb{R}^n$, the background state is $\\mathbf{x}_b \\in \\mathbb{R}^n$, and the observation vector is $\\mathbf{y} \\in \\mathbb{R}^m$. The cost function to be minimized is:\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x}-\\mathbf{x}_b)^\\top \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\frac{1}{2}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right)^\\top \\mathbf{R}^{-1}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right)\n$$\nwhere $\\mathbf{B}$ and $\\mathbf{R}$ are the background and observation error covariance matrices, respectively. The observation operator is $\\mathbf{h}(\\mathbf{x}) = \\tanh(\\mathbf{C}\\mathbf{x})$, applied element-wise. The forecast metric is a linear function of the state, $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$.\n\n### Part 1: Full Optimization and Brute-Force Impact\n\nTo find the analysis state $\\mathbf{x}_a$ that minimizes $J(\\mathbf{x})$, we must solve the optimality condition $\\nabla J(\\mathbf{x}_a) = \\mathbf{0}$. This nonlinear system is solved using a second-order numerical optimization method, specifically the Newton-Raphson method, which requires the gradient and the Hessian of the cost function.\n\nThe gradient of $J(\\mathbf{x})$ is:\n$$\n\\nabla J(\\mathbf{x}) = \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\mathbf{H}(\\mathbf{x})^\\top \\mathbf{R}^{-1}(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y})\n$$\nwhere $\\mathbf{H}(\\mathbf{x}) = \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{x}}$ is the Jacobian of the observation operator. Given $\\mathbf{h}(\\mathbf{x}) = \\tanh(\\mathbf{C}\\mathbf{x})$, its Jacobian is $\\mathbf{H}(\\mathbf{x}) = \\operatorname{diag}(\\operatorname{sech}^2(\\mathbf{C}\\mathbf{x}))\\mathbf{C}$.\n\nThe Hessian of $J(\\mathbf{x})$, required for Newton's method, is the derivative of the gradient:\n$$\n\\nabla^2 J(\\mathbf{x}) = \\mathbf{B}^{-1} + \\mathbf{H}(\\mathbf{x})^\\top \\mathbf{R}^{-1} \\mathbf{H}(\\mathbf{x}) + \\sum_{k=1}^m \\left[\\mathbf{R}^{-1}(\\mathbf{h}(\\mathbf{x})-\\mathbf{y})\\right]_k \\nabla^2 h_k(\\mathbf{x})\n$$\nThe first two terms represent the Gauss-Newton approximation of the Hessian. The third term involves the second derivatives of the observation operator. For $h_k(\\mathbf{x}) = \\tanh((\\mathbf{C}\\mathbf{x})_k)$, its Hessian is $\\nabla^2 h_k(\\mathbf{x}) = -2\\tanh((\\mathbf{C}\\mathbf{x})_k)\\operatorname{sech}^2((\\mathbf{C}\\mathbf{x})_k) \\mathbf{C}_{k,:}^\\top \\mathbf{C}_{k,:}$, where $\\mathbf{C}_{k,:}$ is the $k$-th row of $\\mathbf{C}$.\n\nThe Newton-Raphson iteration to find $\\mathbf{x}_a$ is:\n$$\n\\mathbf{x}_{j+1} = \\mathbf{x}_j - [\\nabla^2 J(\\mathbf{x}_j)]^{-1} \\nabla J(\\mathbf{x}_j)\n$$\nstarting from an initial guess, typically $\\mathbf{x}_0 = \\mathbf{x}_b$.\n\nThe brute-force impact of removing the $i$-th observation, $\\Delta f_i^{\\text{bf}}$, is calculated by first finding a new analysis state, $\\mathbf{x}_a^{(-i)}$, that minimizes a modified cost function $J^{(-i)}(\\mathbf{x})$ where the $i$-th observation term is omitted. This is equivalent to setting the $i$-th diagonal element of $\\mathbf{R}^{-1}$ to zero. We re-run the full Newton-Raphson minimization to find $\\mathbf{x}_a^{(-i)}$. The brute-force impact is then:\n$$\n\\Delta f_i^{\\text{bf}} = f(\\mathbf{x}_a^{(-i)}) - f(\\mathbf{x}_a) = \\mathbf{w}^\\top (\\mathbf{x}_a^{(-i)} - \\mathbf{x}_a)\n$$\nThis process is repeated for each observation $i \\in \\{1, \\dots, m\\}$.\n\n### Part 2: Tangent-Linear Adjoint Sensitivity Impact\n\nThis approach provides a first-order approximation of the observation impact without re-running the optimization. We analyze the sensitivity of the analysis state $\\mathbf{x}_a$ to a scaling parameter $\\alpha_i$ for the $i$-th observation. The modified cost function is:\n$$\nJ_{\\boldsymbol{\\alpha}}(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x}-\\mathbf{x}_b)^\\top \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\frac{1}{2}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right)^\\top \\operatorname{diag}(\\boldsymbol{\\alpha})\\mathbf{R}^{-1}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right)\n$$\nThe baseline analysis uses $\\boldsymbol{\\alpha} = \\mathbf{1}$. Removing observation $i$ corresponds to changing $\\alpha_i$ from $1$ to $0$, i.e., $\\Delta \\alpha_i = -1$.\n\nThe analysis state $\\mathbf{x}_a(\\boldsymbol{\\alpha})$ is defined by the optimality condition $\\nabla_x J_{\\boldsymbol{\\alpha}}(\\mathbf{x}_a(\\boldsymbol{\\alpha})) = \\mathbf{0}$. We differentiate this identity with respect to $\\alpha_i$ and evaluate at the baseline $\\boldsymbol{\\alpha}=\\mathbf{1}$ and $\\mathbf{x}=\\mathbf{x}_a$:\n$$\n\\frac{d}{d\\alpha_i} \\left[ \\nabla_x J_{\\boldsymbol{\\alpha}}(\\mathbf{x}_a(\\boldsymbol{\\alpha})) \\right] \\bigg|_{\\boldsymbol{\\alpha}=\\mathbf{1}} = \\left( \\nabla_x^2 J_{\\boldsymbol{\\alpha}} \\right) \\frac{d\\mathbf{x}_a}{d\\alpha_i} + \\frac{\\partial}{\\partial \\alpha_i}(\\nabla_x J_{\\boldsymbol{\\alpha}}) = \\mathbf{0}\n$$\nThe problem specifies using the Gauss-Newton approximation for the Hessian, $\\mathcal{H}_a = \\nabla_x^2 J_{\\boldsymbol{\\alpha}} \\approx \\mathbf{B}^{-1} + \\mathbf{H}(\\mathbf{x}_a)^\\top\\mathbf{R}^{-1}\\mathbf{H}(\\mathbf{x}_a)$. The partial derivative term (the forcing) is:\n$$\n\\frac{\\partial}{\\partial \\alpha_i}(\\nabla_x J_{\\boldsymbol{\\alpha}}) = \\mathbf{H}(\\mathbf{x}_a)^\\top \\mathbf{e}_i \\mathbf{e}_i^\\top \\mathbf{R}^{-1}(\\mathbf{h}(\\mathbf{x}_a) - \\mathbf{y}) = \\mathbf{H}_{i,:}(\\mathbf{x}_a)^\\top \\frac{h_i(\\mathbf{x}_a) - y_i}{R_{ii}}\n$$\nwhere $\\mathbf{H}_{i,:}$ is the $i$-th row of $\\mathbf{H}$ and $R_{ii}$ is the $i$-th diagonal element of $\\mathbf{R}$ (since $\\mathbf{R}$ is given as diagonal).\n\nSolving for the sensitivity of the analysis state:\n$$\n\\frac{d\\mathbf{x}_a}{d\\alpha_i} = - \\mathcal{H}_a^{-1} \\mathbf{H}_{i,:}(\\mathbf{x}_a)^\\top \\frac{h_i(\\mathbf{x}_a) - y_i}{R_{ii}}\n$$\nThe sensitivity of the forecast metric $f$ is then $\\frac{df}{d\\alpha_i} = \\mathbf{w}^\\top \\frac{d\\mathbf{x}_a}{d\\alpha_i}$. The tangent-linear approximation of the impact is $\\Delta f_i^{\\text{tl}} \\approx \\frac{df}{d\\alpha_i} \\Delta\\alpha_i$. With $\\Delta\\alpha_i = -1$, we get:\n$$\n\\Delta f_i^{\\text{tl}} = \\left( -\\mathbf{w}^\\top \\mathcal{H}_a^{-1} \\mathbf{H}_{i,:}(\\mathbf{x}_a)^\\top \\frac{h_i(\\mathbf{x}_a) - y_i}{R_{ii}} \\right) \\times (-1) = \\mathbf{w}^\\top \\mathcal{H}_a^{-1} \\mathbf{H}_{i,:}(\\mathbf{x}_a)^\\top \\frac{h_i(\\mathbf{x}_a) - y_i}{R_{ii}}\n$$\nThis calculation is performed for each observation $i$. It reuses the analysis state $\\mathbf{x}_a$ and its associated quantities, requiring only one Hessian inversion ($\\mathcal{H}_a^{-1}$) to compute the impacts for all observations, making it far more efficient than the brute-force method.\n\n### Part 3: Comparison\n\nThe two sets of impacts, $\\{\\Delta f_i^{\\text{bf}}\\}$ and $\\{\\Delta f_i^{\\text{tl}}\\}$, are compared to assess the accuracy of the tangent-linear approximation. The discrepancy arises from two sources: the use of the Gauss-Newton Hessian (neglecting second-derivative terms) and the first-order (linear) approximation of a finite change ($\\Delta\\alpha_i=-1$). The discrepancy is expected to increase as the observation operator becomes more nonlinear (i.e., in saturation regimes).\n\nThe comparison metrics are:\n1.  Maximum Absolute Discrepancy: $E_{\\max} = \\max_i \\left|\\Delta f_i^{\\text{bf}} - \\Delta f_i^{\\text{tl}}\\right|$.\n2.  Count of Underestimates: Number of indices $i$ where the magnitude of the tangent-linear impact is significantly smaller than the brute-force impact: $\\left|\\Delta f_i^{\\text{tl}}\\right|  \\left|\\Delta f_i^{\\text{bf}}\\right|\\cdot(1 - \\varepsilon)$, with tolerance $\\varepsilon = 0.05$.\n3.  Count of Overestimates: Number of indices $i$ where the magnitude of the tangent-linear impact is significantly larger: $\\left|\\Delta f_i^{\\text{tl}}\\right|  \\left|\\Delta f_i^{\\text{bf}}\\right|\\cdot(1 + \\varepsilon)$.\n\nThe following algorithm implements this procedure for each test case.", "answer": "```python\nimport numpy as np\n\ndef h_op(x, C):\n    \"\"\"Computes the nonlinear observation operator h(x) = tanh(Cx).\"\"\"\n    return np.tanh(C @ x)\n\ndef jacobian_h(x, C):\n    \"\"\"Computes the Jacobian of h(x).\"\"\"\n    z = C @ x\n    sech2_z = 1.0 - np.tanh(z)**2\n    # sech2_z is a vector, so diag(sech2_z) is a diagonal matrix\n    H = np.diag(sech2_z) @ C\n    return H\n\ndef gradient(x, xb, y, C, B_inv, R_inv_diag, alpha_diag):\n    \"\"\"Computes the gradient of the cost function J_alpha(x).\"\"\"\n    dx = x - xb\n    grad_b = B_inv @ dx\n    \n    H = jacobian_h(x, C)\n    d_obs = h_op(x, C) - y\n    \n    # Efficient calculation for diagonal R and alpha\n    weighted_innov = alpha_diag * R_inv_diag * d_obs\n    grad_o = H.T @ weighted_innov\n    \n    return grad_b + grad_o\n\ndef hessian(x, xb, y, C, B_inv, R_inv_diag, alpha_diag):\n    \"\"\"Computes the full Hessian of the cost function J_alpha(x).\"\"\"\n    m, n = C.shape\n    H = jacobian_h(x, C)\n    hess_b = B_inv\n    \n    # Gauss-Newton part of the Hessian\n    hess_gn_o = H.T @ np.diag(alpha_diag * R_inv_diag) @ H\n    \n    # Second derivative term\n    hess_s_o = np.zeros((n, n))\n    z = C @ x\n    tanh_z = np.tanh(z)\n    sech2_z = 1.0 - tanh_z**2\n    \n    weighted_innov = alpha_diag * R_inv_diag * (h_op(x, C) - y)\n\n    for k in range(m):\n        C_k_T = C[k, :].reshape(-1, 1)  # k-th row of C as a column vector\n        term = weighted_innov[k] * (-2.0 * tanh_z[k] * sech2_z[k])\n        hess_s_o_k = term * (C_k_T @ C_k_T.T) # Outer product\n        hess_s_o += hess_s_o_k\n        \n    return hess_b + hess_gn_o + hess_s_o\n\ndef minimize_cost(xb, y, C, B_inv, R_inv_diag, alpha_diag):\n    \"\"\"Minimizes the cost function using Newton's method.\"\"\"\n    x_k = xb.copy()\n    max_iter = 50\n    tol = 1e-10\n\n    for _ in range(max_iter):\n        grad = gradient(x_k, xb, y, C, B_inv, R_inv_diag, alpha_diag)\n        if np.linalg.norm(grad)  tol:\n            break\n        \n        hess = hessian(x_k, xb, y, C, B_inv, R_inv_diag, alpha_diag)\n        \n        try:\n            step = np.linalg.solve(hess, grad)\n        except np.linalg.LinAlgError:\n            step = np.linalg.pinv(hess) @ grad\n        \n        x_k = x_k - step\n    \n    return x_k\n\ndef solve():\n    \"\"\"Main solver function to run all test cases.\"\"\"\n    \n    # Common parameters\n    B = np.diag([0.4**2, 0.6**2, 0.8**2])\n    R = np.diag([0.1**2, 0.2**2, 0.3**2])\n    w = np.array([1.0, -0.5, 0.25])\n    \n    m, n = R.shape[0], B.shape[0]\n    B_inv = np.linalg.inv(B)\n    R_inv_diag = 1.0 / np.diag(R)\n    eps = 0.05\n\n    test_cases = [\n        # Test 1 (near-linear)\n        {\n            \"C\": np.array([[0.5, 0.0, 0.1], [0.0, 0.5, -0.2], [0.1, -0.1, 0.4]]),\n            \"xb\": np.array([0.2, -0.1, 0.05]),\n            \"xt\": np.array([0.25, -0.15, 0.10]),\n            \"eta\": np.array([0.01, -0.005, 0.02]),\n        },\n        # Test 2 (moderate saturation)\n        {\n            \"C\": np.array([[2.0, 0.0, 1.0], [0.0, 2.5, -1.5], [1.0, -1.0, 2.0]]),\n            \"xb\": np.array([1.0, -1.5, 0.8]),\n            \"xt\": np.array([1.2, -1.3, 1.0]),\n            \"eta\": np.array([0.05, 0.02, -0.04]),\n        },\n        # Test 3 (strong saturation)\n        {\n            \"C\": np.array([[5.0, -3.0, 2.0], [-4.0, 6.0, -3.0], [3.5, -2.5, 5.5]]),\n            \"xb\": np.array([2.0, -2.0, 2.0]),\n            \"xt\": np.array([2.2, -2.2, 2.1]),\n            \"eta\": np.array([0.01, -0.02, 0.03]),\n        },\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        C, xb, xt, eta = case[\"C\"], case[\"xb\"], case[\"xt\"], case[\"eta\"]\n        y = h_op(xt, C) + eta\n        \n        # 1. Compute baseline analysis xa\n        alpha_full = np.ones(m)\n        xa = minimize_cost(xb, y, C, B_inv, R_inv_diag, alpha_full)\n        \n        # 2. Compute brute-force impacts delta_f_bf\n        delta_f_bf = np.zeros(m)\n        for i in range(m):\n            alpha_removed = np.ones(m)\n            alpha_removed[i] = 0.0\n            xa_minus_i = minimize_cost(xb, y, C, B_inv, R_inv_diag, alpha_removed)\n            delta_f_bf[i] = w.T @ (xa_minus_i - xa)\n\n        # 3. Compute tangent-linear impacts delta_f_tl\n        Ha = jacobian_h(xa, C)\n        Hess_GN = B_inv + Ha.T @ np.diag(R_inv_diag) @ Ha\n        \n        # Adjoint method for efficiency\n        # Solve H_GN * adj = w for the adjoint vector\n        adj = np.linalg.solve(Hess_GN, w)\n        \n        # Project adjoint onto observation space\n        v_adj = Ha @ adj\n        \n        innov_a = h_op(xa, C) - y\n        \n        # delta_f_tl = (H * adj) * innov_a / R_ii\n        delta_f_tl = v_adj * innov_a * R_inv_diag\n\n        # 4. Compare impacts and compute metrics\n        E_max = np.max(np.abs(delta_f_bf - delta_f_tl))\n        \n        abs_bf = np.abs(delta_f_bf)\n        abs_tl = np.abs(delta_f_tl)\n        \n        # Avoid division by zero if an observation has no impact\n        # We only count underestimates/overestimates for non-zero impacts\n        non_zero_impact_mask = abs_bf > 1e-15\n        \n        count_under = np.sum(abs_tl[non_zero_impact_mask]  abs_bf[non_zero_impact_mask] * (1.0 - eps))\n        count_over = np.sum(abs_tl[non_zero_impact_mask] > abs_bf[non_zero_impact_mask] * (1.0 + eps))\n        \n        all_results.append([E_max, int(count_under), int(count_over)])\n\n    results_str_list = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in all_results]\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```", "id": "3406515"}]}