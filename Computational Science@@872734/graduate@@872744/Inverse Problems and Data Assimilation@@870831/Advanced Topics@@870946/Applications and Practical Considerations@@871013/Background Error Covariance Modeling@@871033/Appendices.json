{"hands_on_practices": [{"introduction": "One powerful method for defining a background error covariance matrix $B$ is to model its inverse, the precision matrix $B^{-1}$, as a differential operator. This approach elegantly enforces spatial correlations and smoothness properties based on physical intuition. This practice guides you through the process of discretizing a second-order elliptic operator to construct a sparse precision matrix, a fundamental skill linking continuous physical models to discrete computational frameworks. [@problem_id:3366804]", "problem": "Consider a two-dimensional discrete rectangular grid with $N_x$ nodes in the $x$-direction and $N_y$ nodes in the $y$-direction, and uniform grid spacings $h_x$ and $h_y$. In Bayesian inverse problems and data assimilation, a common way to model background error covariance is to posit a Gaussian prior whose precision (the inverse covariance) is generated by a second-order elliptic operator. Let the operator be defined in continuum form by\n$$\n\\mathcal{L} u = - \\nabla \\cdot (A \\nabla u) + \\mu u,\n$$\nwhere $A = \\mathrm{diag}(a_x, a_y)$ is a constant, symmetric, positive definite diffusion tensor with $a_x > 0$ and $a_y > 0$, and $\\mu \\ge 0$ is a constant zero-order term. On the boundary, impose homogeneous Neumann boundary conditions, i.e., $A \\nabla u \\cdot \\boldsymbol{n} = 0$ where $\\boldsymbol{n}$ is the outward unit normal. You must construct the discrete sparse precision matrix $B^{-1}$ associated with $\\mathcal{L}$ on this grid, using a second-order accurate finite difference scheme derived from the divergence form $-\\nabla \\cdot (A \\nabla u)$ so that the Neumann boundary conditions correspond to zero-flux across the boundary.\n\nBegin from the following fundamental base:\n- The Gaussian prior with precision $B^{-1}$ can be characterized by a quadratic energy functional in the continuum, \n$$\nJ(u) = \\frac{1}{2}\\int_{\\Omega} \\left( a_x \\left|\\frac{\\partial u}{\\partial x}\\right|^2 + a_y \\left|\\frac{\\partial u}{\\partial y}\\right|^2 + \\mu |u|^2 \\right)\\, \\mathrm{d}x \\mathrm{d}y,\n$$\nwhose Euler–Lagrange equation under homogeneous Neumann boundary conditions yields the operator $\\mathcal{L}$.\n- On a uniform grid, a second-order accurate discretization of $-\\nabla \\cdot (A \\nabla u)$ in divergence form uses flux differences across cell faces. For interior grid points, this results in a five-point stencil. At boundaries, zero-flux removes the missing face contribution, altering the stencil weights at edges and corners but preserving symmetry.\n\nYour tasks:\n1. Derive an algorithm to assemble the sparse matrix $B^{-1}$ in Compressed Sparse Row (CSR) format corresponding to the operator $\\mathcal{L}$ with homogeneous Neumann boundary conditions. Use a node-based discretization where the linear index $k \\in \\{0,1,\\dots,N_xN_y-1\\}$ maps the two-dimensional indices $(i,j)$ via $k = i + j N_x$. For interior points, the discretization should recover the usual five-point stencil of the elliptic operator. For boundary points, enforce zero normal flux by removing the missing face contribution, which modifies the diagonal and the single available neighbor in that direction.\n2. For each assembled matrix, compute the following diagnostics:\n   - The total number of nonzero entries, denoted $\\mathrm{nnz}$.\n   - The smallest algebraic eigenvalue, denoted $\\lambda_{\\min}$, computed from the dense representation of the matrix using standard symmetric eigenvalue routines.\n   - A boolean determining whether the matrix is Symmetric Positive Definite (SPD), defined here as the condition $\\lambda_{\\min} > 10^{-12}$.\n   - The sorted list of distinct row nonzero counts across the matrix, which characterizes the sparsity pattern differences between interior, edge, and corner nodes.\n\nDesign a program that implements this algorithm and computes the diagnostics for the following test suite of parameter values $(N_x,N_y,h_x,h_y,a_x,a_y,\\mu)$:\n- Test case A (happy path, isotropic, singular): $(3,3,1.0,1.0,1.0,1.0,0.0)$.\n- Test case B (happy path, isotropic, SPD via zero-order term): $(3,3,1.0,1.0,1.0,1.0,0.001)$.\n- Test case C (anisotropic and rectangular grid): $(4,3,1.0,2.0,1.0,10.0,0.1)$.\n- Test case D (degenerate one-dimensional limit in $x$): $(1,5,1.0,1.0,1.0,1.0,0.0)$.\n\nFor each test case, your program must produce a result encoded as the list $[\\mathrm{nnz}, \\mathrm{round}(\\lambda_{\\min}, 8), \\mathrm{SPD}, \\mathrm{row\\_counts}]$, where $\\mathrm{round}(\\lambda_{\\min}, 8)$ is the smallest algebraic eigenvalue rounded to eight decimal places, $\\mathrm{SPD}$ is a boolean, and $\\mathrm{row\\_counts}$ is the sorted list of distinct row nonzero counts. Your program should produce a single line of output containing the results aggregated over all test cases as a comma-separated list of these lists enclosed in square brackets, with no spaces, for example, $[[\\dots],[\\dots],[\\dots],[\\dots]]$. No physical units are involved; all quantities are dimensionless.", "solution": "The user wants to construct a sparse precision matrix $B^{-1}$ for a Gaussian prior, where the matrix corresponds to a discretized second-order elliptic operator $\\mathcal{L} u = - \\nabla \\cdot (A \\nabla u) + \\mu u$ on a two-dimensional rectangular grid. The discretization must be second-order accurate and handle homogeneous Neumann boundary conditions by enforcing zero flux.\n\n### Step 1: Discretization of the Operator\n\nThe operator is given by $\\mathcal L u = - \\frac{\\partial}{\\partial x}\\left(a_x \\frac{\\partial u}{\\partial x}\\right) - \\frac{\\partial}{\\partial y}\\left(a_y \\frac{\\partial u}{\\partial y}\\right) + \\mu u$. We discretize this operator on a grid with points $(x_i, y_j) = (i h_x, j h_y)$ for $i \\in \\{0, \\dots, N_x-1\\}$ and $j \\in \\{0, \\dots, N_y-1\\}$. The value of the field at a grid point is denoted $u_{i,j}$.\n\nWe use a finite difference/finite volume approach based on flux conservation. The discretized operator at a point $(i,j)$, denoted $(\\mathcal{L}_h u)_{i,j}$, is an approximation of $\\mathcal{L}u$ at that point.\n$$\n(\\mathcal{L}_h u)_{i,j} \\approx -\\frac{F_{x,i+1/2,j} - F_{x,i-1/2,j}}{h_x} - \\frac{F_{y,i,j+1/2} - F_{y,i,j-1/2}}{h_y} + \\mu u_{i,j}\n$$\nwhere $F_x = -a_x \\frac{\\partial u}{\\partial x}$ and $F_y = -a_y \\frac{\\partial u}{\\partial y}$ are the fluxes. We approximate the fluxes at cell faces using centered differences:\n$F_{x,i+1/2,j} \\approx -a_x \\frac{u_{i+1,j} - u_{i,j}}{h_x}$ and $F_{y,i,j+1/2} \\approx -a_y \\frac{u_{i,j+1} - u_{i,j}}{h_y}$.\n\n#### Interior Points\nFor an interior point $(i,j)$ where $0 < i < N_x-1$ and $0 < j < N_y-1$, all four neighboring fluxes are present. Substituting the flux approximations gives:\n$$\n(\\mathcal{L}_h u)_{i,j} = -\\frac{1}{h_x}\\left(-a_x \\frac{u_{i+1,j} - u_{i,j}}{h_x} - \\left(-a_x \\frac{u_{i,j} - u_{i-1,j}}{h_x}\\right)\\right) - \\frac{1}{h_y}\\left(-a_y \\frac{u_{i,j+1} - u_{i,j}}{h_y} - \\left(-a_y \\frac{u_{i,j} - u_{i,j-1}}{h_y}\\right)\\right) + \\mu u_{i,j}\n$$\nLet $c_x = a_x/h_x^2$ and $c_y = a_y/h_y^2$. Simplifying the expression, we get the classic five-point stencil:\n$$\n(\\mathcal{L}_h u)_{i,j} = (2c_x + 2c_y + \\mu)u_{i,j} - c_x u_{i-1,j} - c_x u_{i+1,j} - c_y u_{i,j-1} - c_y u_{i,j+1}\n$$\n\n#### Boundary Points (Homogeneous Neumann conditions)\nThe condition $A \\nabla u \\cdot \\boldsymbol{n} = 0$ means the normal flux is zero at the boundary. The problem specifies that this is handled by \"removing the missing face contribution.\"\n\nFor a point on the left boundary ($i=0$), the flux $F_{x,-1/2,j}$ across the boundary is zero. The discretization becomes:\n$$\n(\\mathcal{L}_h u)_{0,j} \\approx -\\frac{F_{x,1/2,j} - 0}{h_x} - \\dots = - \\frac{1}{h_x}\\left(-a_x \\frac{u_{1,j} - u_{0,j}}{h_x}\\right) - \\dots = c_x(u_{1,j} - u_{0,j}) - \\dots\n$$\nThe full stencil for a point on the left edge ($i=0, 0<j<N_y-1$) is:\n$$\n(\\mathcal{L}_h u)_{0,j} = (c_x + 2c_y + \\mu)u_{0,j} - c_x u_{1,j} - c_y u_{0,j-1} - c_y u_{0,j+1}\n$$\nCompared to the interior stencil, the term $c_x u_{-1,j}$ is absent, and the coefficient of $u_{0,j}$ is reduced by $c_x$. This method guarantees that the resulting matrix $B^{-1}$ is symmetric.\n\nThis logic extends to all boundaries:\n-   **Interior Node** ($0<i<N_x-1, 0<j<N_y-1$): 5-point stencil. One central + 4 neighbors.\n-   **Edge Node** (on one boundary, not a corner): 4-point stencil. One central + 3 neighbors.\n-   **Corner Node** (on two boundaries): 3-point stencil. One central + 2 neighbors.\n\nSpecial cases like $N_x=1$ or $N_y=1$ are handled naturally by this stencil logic. For instance, if $N_x=1$, any point $(0,j)$ has no west or east neighbors, reducing the stencil to the 1D equivalent.\n\n### Step 2: Matrix Assembly\n\nThe discrete system is a set of $N = N_x N_y$ linear equations, one for each grid point, which can be written as $B^{-1}\\boldsymbol{u} = \\boldsymbol{f}$. The vector $\\boldsymbol{u}$ contains all $u_{i,j}$ values, and the matrix $B^{-1}$ contains the coefficients from the stencils. We use the specified row-major mapping $k = i + j N_x$ to map the 2D grid indices $(i,j)$ to a 1D vector index $k$.\n\nThe algorithm to construct $B^{-1}$ is as follows:\n1.  Initialize an $N \\times N$ sparse matrix, e.g., in List of Lists (LIL) format for efficient element-wise insertion.\n2.  Iterate through each grid node $k=0, \\dots, N-1$.\n3.  For each node $k$, convert to $(i,j)$ indices: $i=k \\pmod{N_x}$, $j=\\lfloor k/N_x \\rfloor$.\n4.  The matrix row $B^{-1}_{k, \\cdot}$ is built based on the stencil at $(i,j)$.\n    -   The diagonal element $B^{-1}_{k,k}$ is the sum of coefficients of $u_k$.\n    -   The off-diagonal elements $B^{-1}_{k,m}$ are the coefficients of neighboring $u_m$.\n5.  Let $c_x=a_x/h_x^2$ and $c_y=a_y/h_y^2$. For each row $k$:\n    a. Initialize the diagonal entry $B^{-1}_{k,k}$ to $\\mu$.\n    b. **West Neighbor ($i>0$):** Add $c_x$ to $B^{-1}_{k,k}$ and set $B^{-1}_{k, k-1} = -c_x$.\n    c. **East Neighbor ($i<N_x-1$):** Add $c_x$ to $B^{-1}_{k,k}$ and set $B^{-1}_{k, k+1} = -c_x$.\n    d. **South Neighbor ($j>0$):** Add $c_y$ to $B^{-1}_{k,k}$ and set $B^{-1}_{k, k-N_x} = -c_y$.\n    e. **North Neighbor ($j<N_y-1$):** Add $c_y$ to $B^{-1}_{k,k}$ and set $B^{-1}_{k, k+N_x} = -c_y$.\n6. After populating, the LIL matrix is converted to Compressed Sparse Row (CSR) format for efficient calculations.\n\n### Step 3: Diagnostic Computations\n\nOnce the matrix $B^{-1}$ is assembled in CSR format, we compute the required diagnostics:\n1.  **`nnz`**: The total number of nonzero entries is obtained from the `nnz` attribute of the CSR matrix.\n2.  **`lambda_min`**: To find the smallest eigenvalue, we first convert the sparse matrix to a dense NumPy array. Since the matrix is symmetric by construction, we use `numpy.linalg.eigh`, which is optimized for symmetric/Hermitian matrices and returns eigenvalues in ascending order. The smallest eigenvalue, $\\lambda_{\\min}$, is the first element of the returned array of eigenvalues.\n3.  **`SPD`**: The matrix is considered Symmetric Positive Definite (SPD) if $\\lambda_{\\min} > 10^{-12}$. This strict inequality handles floating-point inaccuracies for singular matrices where $\\lambda_{\\min}$ should be exactly zero.\n4.  **`row_counts`**: The number of non-zero entries per row is computed using the `getnnz(axis=1)` method of the CSR matrix. The sorted list of a unique counts is found using `numpy.unique` and then converted to a standard Python list.\n\nThis procedure is applied to each test case to generate the final results.", "answer": "```python\nimport numpy as np\nfrom scipy import sparse\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    It iterates through test cases, constructs the precision matrix for each,\n    computes diagnostics, and prints the aggregated results.\n    \"\"\"\n    test_cases = [\n        # (Nx, Ny, hx, hy, ax, ay, mu)\n        (3, 3, 1.0, 1.0, 1.0, 1.0, 0.0),      # Test case A\n        (3, 3, 1.0, 1.0, 1.0, 1.0, 0.001),   # Test case B\n        (4, 3, 1.0, 2.0, 1.0, 10.0, 0.1),    # Test case C\n        (1, 5, 1.0, 1.0, 1.0, 1.0, 0.0),      # Test case D\n    ]\n\n    results = []\n    for params in test_cases:\n        result = build_and_diagnose(params)\n        results.append(result)\n\n    # The final print statement must follow the exact format.\n    # str(list) in Python includes spaces, which seems to be the intended\n    # format based on the structure of the provided f-string.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef build_and_diagnose(params):\n    \"\"\"\n    Constructs the sparse precision matrix and computes its diagnostics.\n\n    Args:\n        params (tuple): A tuple containing the model parameters\n                        (Nx, Ny, hx, hy, ax, ay, mu).\n\n    Returns:\n        list: A list containing the diagnostics [nnz, lambda_min, SPD, row_counts].\n    \"\"\"\n    Nx, Ny, hx, hy, ax, ay, mu = params\n    N = Nx * Ny\n\n    if N == 0:\n        return [0, 0.0, False, []]\n\n    # Use lil_matrix for efficient sparse matrix construction.\n    B_inv = sparse.lil_matrix((N, N), dtype=np.float64)\n\n    cx = ax / (hx**2)\n    cy = ay / (hy**2)\n\n    # Iterate through each node in the grid.\n    for k in range(N):\n        # Convert linear index k to 2D grid indices (i, j).\n        i = k % Nx\n        j = k // Nx\n\n        diag_val = mu\n\n        # West neighbor\n        if i > 0:\n            kW = k - 1\n            diag_val += cx\n            B_inv[k, kW] = -cx\n\n        # East neighbor\n        if i < Nx - 1:\n            kE = k + 1\n            diag_val += cx\n            B_inv[k, kE] = -cx\n\n        # South neighbor\n        if j > 0:\n            kS = k - Nx\n            diag_val += cy\n            B_inv[k, kS] = -cy\n\n        # North neighbor\n        if j < Ny - 1:\n            kN = k + Nx\n            diag_val += cy\n            B_inv[k, kN] = -cy\n\n        B_inv[k, k] = diag_val\n\n    # Convert to CSR format for efficient row operations and nnz counting.\n    B_inv_csr = B_inv.tocsr()\n\n    # 1. Total number of nonzero entries.\n    nnz = B_inv_csr.nnz\n\n    # 2. Smallest algebraic eigenvalue.\n    # The matrix is symmetric by construction.\n    B_inv_dense = B_inv_csr.toarray()\n    # np.linalg.eigh is for symmetric matrices and returns sorted eigenvalues.\n    eigenvalues = np.linalg.eigh(B_inv_dense)[0]\n    lambda_min = eigenvalues[0]\n\n    # 3. Symmetric Positive Definite (SPD) check.\n    is_spd = bool(lambda_min > 1e-12)\n\n    # 4. Sorted list of distinct row nonzero counts.\n    row_nz_counts = B_inv_csr.getnnz(axis=1)\n    distinct_row_counts = np.unique(row_nz_counts).tolist()\n\n    return [nnz, round(lambda_min, 8), is_spd, distinct_row_counts]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3366804"}, {"introduction": "When background error covariances are estimated from an ensemble of model states, sampling errors can introduce spurious, non-physical correlations between distant points. Covariance localization is an essential technique to mitigate this issue by tapering distant correlations to zero. This exercise provides a concrete calculation applying the widely used Gaspari-Cohn localization function, demonstrating how a sample covariance is modified through a Schur product to produce a more physically realistic localized covariance. [@problem_id:3366753]", "problem": "Consider a background error covariance modeling task in data assimilation. Let the state vector have two scalar components located at positions $\\boldsymbol{x}_{i} = (0\\,\\text{km},\\,0\\,\\text{km})$ and $\\boldsymbol{x}_{j} = (90\\,\\text{km},\\,120\\,\\text{km})$ on a locally Euclidean plane. The sample (unlocalized) background covariance entry between these two components is $\\hat{B}_{ij} = 12\\,\\text{K}^{2}$. To mitigate spurious long-range correlations, covariance localization is applied via the element-wise (Schur) product between the sample covariance matrix and a correlation taper matrix constructed from a compactly supported isotropic correlation function $\\rho$ of the scaled distance. The isotropic Gaspari–Cohn taper (order $5$) is a widely used choice; it is compactly supported on $[0,2]$, satisfies $\\rho(0)=1$, and for $0 \\le r \\le 1$ is given by\n$$\n\\rho(r) \\;=\\; 1 \\;-\\; \\frac{5}{3}\\,r^{2} \\;+\\; \\frac{5}{8}\\,r^{3} \\;+\\; \\frac{1}{2}\\,r^{4} \\;-\\; \\frac{1}{4}\\,r^{5},\n$$\nwith $\\rho(r)=0$ for $r \\ge 2$. Define the localization scale as $L=150\\,\\text{km}$ and the nondimensional scaled separation as $r = d_{ij}/L$, where $d_{ij} = \\|\\boldsymbol{x}_{i}-\\boldsymbol{x}_{j}\\|_{2}$ is the Euclidean distance. The localized covariance entry is obtained by the Schur product definition of localization.\n\nCompute the localized covariance entry $\\tilde{B}_{ij}$ and express your final value in $\\text{K}^{2}$. If a numerical approximation is necessary, round your answer to four significant figures.", "solution": "The localized covariance entry $\\tilde{B}_{ij}$ is computed by multiplying the sample covariance entry $\\hat{B}_{ij}$ with the value of the localization taper function $\\rho(r)$ evaluated at the scaled distance $r$. The formula is $\\tilde{B}_{ij} = \\hat{B}_{ij} \\cdot \\rho(r)$.\n\n**Step 1: Calculate the Euclidean distance**\nFirst, we compute the Euclidean distance $d_{ij}$ between the two locations $\\boldsymbol{x}_{i} = (0\\,\\text{km}, 0\\,\\text{km})$ and $\\boldsymbol{x}_{j} = (90\\,\\text{km}, 120\\,\\text{km})$.\n$$\nd_{ij} = \\sqrt{(90 - 0)^2 + (120 - 0)^2} = \\sqrt{90^2 + 120^2} = \\sqrt{8100 + 14400} = \\sqrt{22500} = 150\\,\\text{km}.\n$$\n\n**Step 2: Calculate the scaled separation**\nNext, we compute the nondimensional scaled separation $r$ using the given localization scale $L = 150\\,\\text{km}$.\n$$\nr = \\frac{d_{ij}}{L} = \\frac{150\\,\\text{km}}{150\\,\\text{km}} = 1.\n$$\n\n**Step 3: Evaluate the Gaspari-Cohn taper function**\nSince the scaled distance is $r=1$, we use the formula for the Gaspari-Cohn function in the range $0 \\le r \\le 1$:\n$$\n\\rho(r) = 1 - \\frac{5}{3}r^2 + \\frac{5}{8}r^3 + \\frac{1}{2}r^4 - \\frac{1}{4}r^5.\n$$\nSubstituting $r=1$:\n$$\n\\rho(1) = 1 - \\frac{5}{3}(1)^2 + \\frac{5}{8}(1)^3 + \\frac{1}{2}(1)^4 - \\frac{1}{4}(1)^5 = 1 - \\frac{5}{3} + \\frac{5}{8} + \\frac{1}{2} - \\frac{1}{4}.\n$$\nTo sum these fractions, we find a common denominator, which is 24:\n$$\n\\rho(1) = \\frac{24}{24} - \\frac{40}{24} + \\frac{15}{24} + \\frac{12}{24} - \\frac{6}{24} = \\frac{24 - 40 + 15 + 12 - 6}{24} = \\frac{5}{24}.\n$$\n\n**Step 4: Compute the localized covariance**\nFinally, we multiply the sample covariance $\\hat{B}_{ij} = 12\\,\\text{K}^2$ by the taper value $\\rho(1)$:\n$$\n\\tilde{B}_{ij} = \\hat{B}_{ij} \\cdot \\rho(1) = 12\\,\\text{K}^2 \\cdot \\frac{5}{24} = \\frac{60}{24}\\,\\text{K}^2 = \\frac{5}{2}\\,\\text{K}^2 = 2.5\\,\\text{K}^2.\n$$\nThe localized covariance entry is $2.5\\,\\text{K}^2$.", "answer": "$$\\boxed{2.5}$$", "id": "3366753"}, {"introduction": "The performance of any data assimilation system critically depends on the correct specification of the background ($B$) and observation ($R$) error covariance matrices. Desroziers diagnostics offer a powerful, data-driven method for tuning and validating these matrices by examining the statistical consistency of system outputs like innovations and analysis residuals. This practice provides a hands-on example of how to use these diagnostics to set up and solve a least-squares problem to estimate optimal scaling parameters for given covariance models. [@problem_id:3366745]", "problem": "Consider a linear, Gaussian data assimilation setting with a linear observation operator $H \\in \\mathbb{R}^{p \\times n}$ and background and observation error covariance matrices $B \\in \\mathbb{R}^{n \\times n}$ and $R \\in \\mathbb{R}^{p \\times p}$, respectively. The innovation is defined as $v = y - H x^{b}$ and the analysis residual is defined as $w = y - H x^{a}$. In the framework of Desroziers diagnostics, the following well-tested relations are used to link second-order statistics of innovations and analysis residuals to the error covariances: the second-moment of innovations approximates $H B H^{\\top} + R$, and the cross-second-moment of analysis residuals with innovations approximates $R$. Suppose the actual error covariances are modeled as scalar multiples of given templates, namely $B = \\alpha B_{0}$ and $R = \\beta R_{0}$ for unknown scalars $\\alpha, \\beta \\in \\mathbb{R}$ and known symmetric positive-definite templates $B_{0}$ and $R_{0}$.\n\nLet the observation dimension be $p = 2$ and the observation operator be $H = I_{2}$, the $2 \\times 2$ identity matrix. Let the covariance templates be\n$$\nB_{0} = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}, \\qquad R_{0} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}.\n$$\nYou are given $m = 4$ paired samples of innovations and analysis residuals, $\\{(v_{i}, w_{i})\\}_{i=1}^{4}$, with\n$$\nv_{1} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad v_{2} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}, \\quad v_{3} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}, \\quad v_{4} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\n$$\n$$\nw_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad w_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad w_{3} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad w_{4} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nForm the sample second-moment matrices\n$$\nS_{vv} = \\frac{1}{m} \\sum_{i=1}^{m} v_{i} v_{i}^{\\top}, \\qquad S_{wv} = \\frac{1}{m} \\sum_{i=1}^{m} w_{i} v_{i}^{\\top}.\n$$\nUsing the Desroziers relations as the modeling base, determine the scalars $\\alpha$ and $\\beta$ that best satisfy both relations in a least-squares sense, by minimizing the sum of squared Frobenius-norm residuals\n$$\nJ(\\alpha, \\beta) = \\left\\| S_{vv} - H (\\alpha B_{0}) H^{\\top} - \\beta R_{0} \\right\\|_{F}^{2} + \\left\\| S_{wv} - \\beta R_{0} \\right\\|_{F}^{2},\n$$\nwhere $\\| X \\|_{F}^{2} = \\operatorname{tr}(X^{\\top} X)$. Compute the exact values of $\\alpha$ and $\\beta$ that minimize $J(\\alpha, \\beta)$, and present your final answer as a $1 \\times 2$ row matrix containing $\\alpha$ and $\\beta$, respectively. No rounding is required, and no units are involved.", "solution": "We begin from the linear, Gaussian data assimilation setting and the Desroziers diagnostics, which provide well-tested relations between second-order statistics of innovations and analysis residuals and the error covariances. Specifically, under correct specification and linear assumptions, the second-moment of innovations satisfies $\\mathbb{E}[v v^{\\top}] \\approx H B H^{\\top} + R$, and the cross-second-moment between analysis residuals and innovations satisfies $\\mathbb{E}[w v^{\\top}] \\approx R$. When modeling $B = \\alpha B_{0}$ and $R = \\beta R_{0}$ with scalar multipliers applied to known templates $B_{0}$ and $R_{0}$, these relations suggest fitting\n$$\nS_{vv} \\approx H (\\alpha B_{0}) H^{\\top} + \\beta R_{0}, \\qquad S_{wv} \\approx \\beta R_{0},\n$$\nin a least-squares sense with the Frobenius norm, leading to the objective\n$$\nJ(\\alpha, \\beta) = \\left\\| S_{vv} - \\alpha A - \\beta R_{0} \\right\\|_{F}^{2} + \\left\\| S_{wv} - \\beta R_{0} \\right\\|_{F}^{2},\n$$\nwhere $A = H B_{0} H^{\\top}$. In this problem, $H = I_{2}$, so $A = B_{0}$.\n\nStep $1$: Compute the sample second-moment matrices $S_{vv}$ and $S_{wv}$ from the provided data.\n\nFor $S_{vv}$, compute each $v_{i} v_{i}^{\\top}$:\n$$\nv_{1} v_{1}^{\\top} = \\begin{pmatrix} 4 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\quad\nv_{2} v_{2}^{\\top} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 4 \\end{pmatrix}, \\quad\nv_{3} v_{3}^{\\top} = \\begin{pmatrix} 4 & 4 \\\\ 4 & 4 \\end{pmatrix}, \\quad\nv_{4} v_{4}^{\\top} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nSumming and dividing by $m = 4$ gives\n$$\nS_{vv} = \\frac{1}{4} \\left( \\begin{pmatrix} 4 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 4 \\end{pmatrix} + \\begin{pmatrix} 4 & 4 \\\\ 4 & 4 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} \\right)\n= \\frac{1}{4} \\begin{pmatrix} 8 & 4 \\\\ 4 & 8 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}.\n$$\nFor $S_{wv}$, compute each $w_{i} v_{i}^{\\top}$:\n$$\nw_{1} v_{1}^{\\top} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\quad\nw_{2} v_{2}^{\\top} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 2 \\end{pmatrix}, \\quad\nw_{3} v_{3}^{\\top} = \\begin{pmatrix} 2 & 2 \\\\ 2 & 2 \\end{pmatrix}, \\quad\nw_{4} v_{4}^{\\top} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nSumming and dividing by $m = 4$ gives\n$$\nS_{wv} = \\frac{1}{4} \\left( \\begin{pmatrix} 2 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 2 \\end{pmatrix} + \\begin{pmatrix} 2 & 2 \\\\ 2 & 2 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} \\right)\n= \\frac{1}{4} \\begin{pmatrix} 4 & 2 \\\\ 2 & 4 \\end{pmatrix} = \\begin{pmatrix} 1 & \\frac{1}{2} \\\\ \\frac{1}{2} & 1 \\end{pmatrix}.\n$$\n\nStep $2$: Set $A = H B_{0} H^{\\top} = B_{0} = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}$ and $R_{0} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}$. Introduce the Frobenius inner product $\\langle X, Y \\rangle = \\operatorname{tr}(X^{\\top} Y)$, so that $\\| X \\|_{F}^{2} = \\langle X, X \\rangle$. The objective can be expanded as\n$$\nJ(\\alpha, \\beta) = \\left\\langle S_{vv} - \\alpha A - \\beta R_{0},\\, S_{vv} - \\alpha A - \\beta R_{0} \\right\\rangle + \\left\\langle S_{wv} - \\beta R_{0},\\, S_{wv} - \\beta R_{0} \\right\\rangle.\n$$\nDifferentiating with respect to $\\alpha$ and $\\beta$ and setting the gradients to zero gives the normal equations for the least-squares minimizer:\n$$\n\\frac{\\partial J}{\\partial \\alpha} = -2 \\langle S_{vv}, A \\rangle + 2 \\alpha \\langle A, A \\rangle + 2 \\beta \\langle A, R_{0} \\rangle = 0,\n$$\n$$\n\\frac{\\partial J}{\\partial \\beta} = -2 \\langle S_{vv}, R_{0} \\rangle + 2 \\alpha \\langle A, R_{0} \\rangle + 2 \\beta \\langle R_{0}, R_{0} \\rangle - 2 \\langle S_{wv}, R_{0} \\rangle + 2 \\beta \\langle R_{0}, R_{0} \\rangle = 0.\n$$\nDividing both equations by $2$ yields the $2 \\times 2$ linear system\n$$\n\\alpha \\langle A, A \\rangle + \\beta \\langle A, R_{0} \\rangle = \\langle S_{vv}, A \\rangle,\n$$\n$$\n\\alpha \\langle A, R_{0} \\rangle + 2 \\beta \\langle R_{0}, R_{0} \\rangle = \\langle S_{vv}, R_{0} \\rangle + \\langle S_{wv}, R_{0} \\rangle.\n$$\n\nStep $3$: Compute the required inner products.\nFirst, compute $\\langle A, A \\rangle$:\n$$\n\\langle A, A \\rangle = 3^{2} + 1^{2} + 1^{2} + 2^{2} = 9 + 1 + 1 + 4 = 15.\n$$\nNext, compute $\\langle A, R_{0} \\rangle$:\n$$\n\\langle A, R_{0} \\rangle = 3 \\cdot 1 + 1 \\cdot 0 + 1 \\cdot 0 + 2 \\cdot 2 = 3 + 0 + 0 + 4 = 7.\n$$\nCompute $\\langle S_{vv}, A \\rangle$:\n$$\n\\langle S_{vv}, A \\rangle = 2 \\cdot 3 + 1 \\cdot 1 + 1 \\cdot 1 + 2 \\cdot 2 = 6 + 1 + 1 + 4 = 12.\n$$\nCompute $\\langle R_{0}, R_{0} \\rangle$:\n$$\n\\langle R_{0}, R_{0} \\rangle = 1^{2} + 0^{2} + 0^{2} + 2^{2} = 1 + 0 + 0 + 4 = 5.\n$$\nCompute $\\langle S_{vv}, R_{0} \\rangle$:\n$$\n\\langle S_{vv}, R_{0} \\rangle = 2 \\cdot 1 + 1 \\cdot 0 + 1 \\cdot 0 + 2 \\cdot 2 = 2 + 0 + 0 + 4 = 6.\n$$\nCompute $\\langle S_{wv}, R_{0} \\rangle$:\n$$\n\\langle S_{wv}, R_{0} \\rangle = 1 \\cdot 1 + \\frac{1}{2} \\cdot 0 + \\frac{1}{2} \\cdot 0 + 1 \\cdot 2 = 1 + 0 + 0 + 2 = 3.\n$$\n\nStep $4$: Solve the linear system\n$$\n15 \\alpha + 7 \\beta = 12, \\qquad 7 \\alpha + 10 \\beta = 9.\n$$\nSolve for $\\alpha$ and $\\beta$. Multiply the first equation by $10$ and the second equation by $7$:\n$$\n150 \\alpha + 70 \\beta = 120, \\qquad 49 \\alpha + 70 \\beta = 63.\n$$\nSubtract the second from the first:\n$$\n(150 - 49) \\alpha = 120 - 63 \\quad \\Rightarrow \\quad 101 \\alpha = 57 \\quad \\Rightarrow \\quad \\alpha = \\frac{57}{101}.\n$$\nSubstitute into $7 \\alpha + 10 \\beta = 9$:\n$$\n10 \\beta = 9 - 7 \\alpha = 9 - 7 \\cdot \\frac{57}{101} = \\frac{909 - 399}{101} = \\frac{510}{101},\n$$\nthus\n$$\n\\beta = \\frac{51}{101}.\n$$\n\nThese exact rational values minimize the specified least-squares objective and provide the scalar multipliers for $B$ and $R$ consistent with the Desroziers relations in the given setting.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{57}{101} & \\frac{51}{101}\\end{pmatrix}}$$", "id": "3366745"}]}