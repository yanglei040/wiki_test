{"hands_on_practices": [{"introduction": "This first exercise lays the groundwork for understanding the role of whitening in uncertainty quantification. You will derive and implement methods to compute the posterior covariance matrix, confirming that transforming the problem into a whitened space yields the same result as the standard formulation. By analyzing several test cases with varying observation error structures, you will gain a hands-on feel for how heteroscedasticity and correlations in your data redistribute uncertainty across the parameters you aim to estimate [@problem_id:3388459].", "problem": "You are given a linear inverse problem with Gaussian prior and Gaussian observation errors. The forward model is the linear map from a parameter vector to an observation vector. Let the parameter dimension be $n=3$ and the observation dimension be $m=5$. The problem is to quantify the posterior uncertainty and to study how observation whitening and heteroscedasticity (spatially varying observation error covariance) affect the posterior covariance.\n\nFundamental base to use:\n- The observation model is $\\boldsymbol{y} = J \\boldsymbol{x} + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{x} \\in \\mathbb{R}^n$ is the parameter, $\\boldsymbol{y} \\in \\mathbb{R}^m$ is the observation, and $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0, R)$ with symmetric positive definite observation covariance $R \\in \\mathbb{R}^{m \\times m}$.\n- The prior is $\\boldsymbol{x} \\sim \\mathcal{N}(\\boldsymbol{x}_b, \\Gamma)$ with symmetric positive definite prior covariance $\\Gamma \\in \\mathbb{R}^{n \\times n}$.\n- Use the laws of Gaussian conditioning and linear algebra identities to derive the posterior covariance of $\\boldsymbol{x}$ given $\\boldsymbol{y}$ in terms of $J$, $R$, and $\\Gamma$.\n- For whitening, use a factorization based on the Cholesky decomposition: if $R = L L^\\top$ with lower-triangular $L$, define the whitened quantities $\\tilde{\\boldsymbol{y}} = L^{-1} \\boldsymbol{y}$ and $\\tilde{J} = L^{-1} J$, and derive the posterior covariance in the whitened space. Show that the posterior covariance in the original and whitened formulations are equal.\n\nYour program must:\n1. Implement a numerically stable method to compute the posterior covariance using the original (unwhitened) formulation, without forming any unnecessary matrix inverses. You may use triangular solves and linear system solves.\n2. Implement the whitening transformation using the inverse of a Cholesky factor and compute the posterior covariance in the whitened space.\n3. For each test case specified below, compute:\n   - The Frobenius norm of the difference between the posterior covariance computed from the unwhitened and whitened formulations (this should be numerically close to zero if implemented correctly).\n   - The vector of posterior marginal variances, given by the diagonal of the posterior covariance matrix.\n   - The trace of the posterior covariance matrix.\n4. Analyze the effect of heteroscedastic observation covariance by comparing the posterior variances across the test cases and explain how the spatially varying $R$ redistributes uncertainty across the parameters via the observation operator $J$.\n5. Output for each test case a list of floats in the format $[d, v_1, v_2, v_3, t]$, where $d$ is the Frobenius norm difference, $v_i$ are the posterior variances (the diagonal entries of the posterior covariance) for $i \\in \\{1,2,3\\}$, and $t$ is the trace of the posterior covariance. Round each float to $10$ decimal places for deterministic output.\n\nUse the following fixed matrices for all test cases:\n- Observation operator $J \\in \\mathbb{R}^{5 \\times 3}$:\n$$\nJ =\n\\begin{bmatrix}\n1.0 & 0.2 & 0.0 \\\\\n0.0 & 1.0 & 0.3 \\\\\n0.5 & 0.1 & 1.0 \\\\\n1.0 & -0.5 & 0.2 \\\\\n0.0 & 0.7 & -0.3\n\\end{bmatrix}.\n$$\n- Prior covariance $\\Gamma \\in \\mathbb{R}^{3 \\times 3}$ (diagonal):\n$$\n\\Gamma = \\mathrm{diag}\\left(1.0, 4.0, 0.25\\right).\n$$\n\nTest suite (four cases for $R$):\n- Case A (homoscedastic, diagonal):\n$$\nR_A = 0.5 \\, I_5,\n$$\nwhere $I_5$ is the $5 \\times 5$ identity matrix.\n- Case B (heteroscedastic, diagonal):\n$$\nR_B = \\mathrm{diag}\\left(0.1, 2.0, 0.5, 1.5, 0.3\\right).\n$$\n- Case C (strongly heteroscedastic, diagonal):\n$$\nR_C = \\mathrm{diag}\\left(0.01, 10.0, 0.2, 5.0, 0.05\\right).\n$$\n- Case D (heteroscedastic and correlated):\nConstruct $R_D$ as $R_D = D^{1/2} \\, C \\, D^{1/2}$, where\n$$\nD = \\mathrm{diag}\\left(0.5, 1.0, 0.2, 2.0, 0.8\\right),\n$$\nand $C \\in \\mathbb{R}^{5 \\times 5}$ has entries $C_{ij} = \\rho^{|i-j|}$ with $\\rho = 0.3$.\n\nAngle units are not involved. There are no physical units in this problem.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list in the order specified above. For example, the final line should look like\n\"[[d_A,v1_A,v2_A,v3_A,t_A],[d_B,v1_B,v2_B,v3_B,t_B],[d_C,v1_C,v2_C,v3_C,t_C],[d_D,v1_D,v2_D,v3_D,t_D]]\"\nwith each value rounded to $10$ decimal places.", "solution": "The user-provided problem is critically validated and determined to be **valid**. It is a well-posed problem in the field of Bayesian inverse theory and data assimilation, with a complete and consistent setup. All provided data and definitions are scientifically sound and mathematically formalizable.\n\n### Theoretical Derivations\n\nThe problem is to determine the posterior covariance of a parameter vector $\\boldsymbol{x} \\in \\mathbb{R}^n$ given an observation vector $\\boldsymbol{y} \\in \\mathbb{R}^m$. The relationship is defined by a linear forward model with additive Gaussian noise and a Gaussian prior on the parameters.\n\nThe observation model is $\\boldsymbol{y} = J \\boldsymbol{x} + \\boldsymbol{\\varepsilon}$, where the observation error $\\boldsymbol{\\varepsilon}$ follows a Gaussian distribution $\\mathcal{N}(0, R)$ with a symmetric positive definite (SPD) covariance matrix $R \\in \\mathbb{R}^{m \\times m}$. The likelihood function is thus $p(\\boldsymbol{y}|\\boldsymbol{x}) \\propto \\exp\\left(-\\frac{1}{2}(\\boldsymbol{y} - J\\boldsymbol{x})^\\top R^{-1} (\\boldsymbol{y} - J\\boldsymbol{x})\\right)$.\n\nThe prior distribution for the parameter is $\\boldsymbol{x} \\sim \\mathcal{N}(\\boldsymbol{x}_b, \\Gamma)$, with mean $\\boldsymbol{x}_b$ and SPD covariance matrix $\\Gamma \\in \\mathbb{R}^{n \\times n}$. The prior probability density function is $p(\\boldsymbol{x}) \\propto \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{x}_b)^\\top \\Gamma^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b)\\right)$.\n\nAccording to Bayes' theorem, the posterior distribution $p(\\boldsymbol{x}|\\boldsymbol{y})$ is proportional to the product of the likelihood and the prior, $p(\\boldsymbol{x}|\\boldsymbol{y}) \\propto p(\\boldsymbol{y}|\\boldsymbol{x})p(\\boldsymbol{x})$. Since both distributions are Gaussian, the posterior is also Gaussian, denoted as $\\boldsymbol{x}|\\boldsymbol{y} \\sim \\mathcal{N}(\\boldsymbol{x}_a, P_a)$. The posterior density is proportional to $\\exp(-\\mathcal{J}(\\boldsymbol{x}))$, where $\\mathcal{J}(\\boldsymbol{x})$ is the cost function:\n$$\n\\mathcal{J}(\\boldsymbol{x}) = \\frac{1}{2} (\\boldsymbol{x} - \\boldsymbol{x}_b)^\\top \\Gamma^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) + \\frac{1}{2} (\\boldsymbol{y} - J\\boldsymbol{x})^\\top R^{-1} (\\boldsymbol{y} - J\\boldsymbol{x})\n$$\nThe posterior covariance $P_a$ is the inverse of the Hessian of $\\mathcal{J}(\\boldsymbol{x})$ with respect to $\\boldsymbol{x}$. The Hessian is found by taking the second derivative:\n$$\n\\nabla_{\\boldsymbol{x}}^2 \\mathcal{J}(\\boldsymbol{x}) = \\Gamma^{-1} + J^\\top R^{-1} J\n$$\nThus, the posterior covariance $P_a$ is given by:\n$$\nP_a = \\left( \\Gamma^{-1} + J^\\top R^{-1} J \\right)^{-1}\n$$\n\n### Numerically Stable Computation (Unwhitened)\n\nDirectly implementing the above formula can be numerically unstable and inefficient, especially due to matrix inversions. A more stable procedure is as follows:\n1.  Compute $\\Gamma^{-1}$. As $\\Gamma$ is diagonal, this is trivial: $(\\Gamma^{-1})_{ii} = 1/\\Gamma_{ii}$.\n2.  To compute the term $J^\\top R^{-1} J$, we avoid forming $R^{-1}$. We first find the Cholesky decomposition of $R = LL^\\top$, where $L$ is a lower-triangular matrix.\n3.  We solve the linear system for matrix $Z = R^{-1}J$. This is done in two steps using the Cholesky factor:\n    a. Solve $LK = J$ for $K$ via forward substitution ($K=L^{-1}J$).\n    b. Solve $L^\\top Z = K$ for $Z$ via backward substitution ($Z=(L^\\top)^{-1}K = (LL^\\top)^{-1}J = R^{-1}J$).\n4.  Compute the matrix product $J^\\top Z$ which equals $J^\\top R^{-1} J$.\n5.  Form the Hessian matrix $H = \\Gamma^{-1} + J^\\top R^{-1} J$.\n6.  Compute the posterior covariance by inverting the small $n \\times n$ Hessian: $P_a = H^{-1}$. For $n=3$, this is numerically safe.\n\n### Whitening Transformation\n\nThe whitening transformation simplifies the observation error structure. Using the Cholesky factor $L$ of $R$, we define whitened quantities by pre-multiplying the observation equation by $L^{-1}$:\n$$\nL^{-1}\\boldsymbol{y} = L^{-1}J\\boldsymbol{x} + L^{-1}\\boldsymbol{\\varepsilon} \\implies \\tilde{\\boldsymbol{y}} = \\tilde{J}\\boldsymbol{x} + \\tilde{\\boldsymbol{\\varepsilon}}\n$$\nHere, $\\tilde{\\boldsymbol{y}} = L^{-1}\\boldsymbol{y}$ is the whitened observation, $\\tilde{J} = L^{-1}J$ is the whitened observation operator, and $\\tilde{\\boldsymbol{\\varepsilon}} = L^{-1}\\boldsymbol{\\varepsilon}$ is the whitened error. The covariance of the whitened error is:\n$$\n\\tilde{R} = \\mathbb{E}[\\tilde{\\boldsymbol{\\varepsilon}}\\tilde{\\boldsymbol{\\varepsilon}}^\\top] = L^{-1}\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^\\top](L^{-1})^\\top = L^{-1}R(L^\\top)^{-1} = L^{-1}(LL^\\top)(L^\\top)^{-1} = I_m\n$$\nThe whitened problem has an observation error covariance equal to the identity matrix. Applying the posterior covariance formula to the whitened system yields:\n$$\nP_{a, \\text{white}} = \\left( \\Gamma^{-1} + \\tilde{J}^\\top \\tilde{R}^{-1} \\tilde{J} \\right)^{-1} = \\left( \\Gamma^{-1} + \\tilde{J}^\\top I_m^{-1} \\tilde{J} \\right)^{-1} = \\left( \\Gamma^{-1} + \\tilde{J}^\\top \\tilde{J} \\right)^{-1}\n$$\nThe equivalence of the two formulations is shown by substituting $\\tilde{J} = L^{-1}J$:\n$$\n\\tilde{J}^\\top \\tilde{J} = (L^{-1}J)^\\top(L^{-1}J) = J^\\top(L^{-1})^\\top L^{-1}J = J^\\top(L^\\top)^{-1}L^{-1}J = J^\\top(LL^\\top)^{-1}J = J^\\top R^{-1} J\n$$\nThis demonstrates that the Hessian matrices are identical, and therefore $P_a = P_{a, \\text{white}}$. The numerical difference between the two computed results, measured by the Frobenius norm, serves as a check on the implementation.\n\n### Implementation and Analysis\n\nThe program implements both methods. For each test case defined by a different observation covariance matrix $R$, it computes the posterior covariance $P_a$. The diagonal elements of $P_a$ are the posterior variances of the parameters ($v_1, v_2, v_3$), quantifying the uncertainty in each parameter estimate. The trace of $P_a$ is the total posterior variance.\n- **Case A (Homoscedastic)** provides a baseline with uniform observation error.\n- **Cases B and C (Heteroscedastic)** have non-uniform diagonal $R$. Observations with smaller variance (higher precision) contribute more information. The matrix $J$ directs this information to constrain specific parameters. For instance, in Case C, very precise observations ($R_{11}=0.01$, $R_{55}=0.05$) corresponding to rows $1$ and $5$ of $J$ will lead to a significant reduction in the posterior variances of the parameters they are sensitive to.\n- **Case D (Correlated)** introduces off-diagonal terms in $R$, modeling correlated observation errors. The whitening transformation is particularly insightful, as $L^{-1}$ mixes the rows of $J$ to form a new operator $\\tilde{J}$ corresponding to uncorrelated \"pseudo-observations\".\nComparing the posterior variances and traces across these cases reveals how observation quality and structure (heteroscedasticity and correlation) impact parameter uncertainty.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular, toeplitz\n\ndef solve():\n    \"\"\"\n    Computes and analyzes posterior covariance for a linear inverse problem\n    under different observation error covariance structures.\n    \"\"\"\n\n    # Define the fixed matrices for the problem.\n    J = np.array([\n        [1.0, 0.2, 0.0],\n        [0.0, 1.0, 0.3],\n        [0.5, 0.1, 1.0],\n        [1.0, -0.5, 0.2],\n        [0.0, 0.7, -0.3]\n    ], dtype=np.float64)\n\n    Gamma = np.diag([1.0, 4.0, 0.25]).astype(np.float64)\n\n    # Define the test cases for the observation covariance matrix R.\n    # Case A: Homoscedastic, diagonal\n    R_A = 0.5 * np.identity(5, dtype=np.float64)\n\n    # Case B: Heteroscedastic, diagonal\n    R_B = np.diag([0.1, 2.0, 0.5, 1.5, 0.3]).astype(np.float64)\n\n    # Case C: Strongly heteroscedastic, diagonal\n    R_C = np.diag([0.01, 10.0, 0.2, 5.0, 0.05]).astype(np.float64)\n\n    # Case D: Heteroscedastic and correlated\n    D = np.diag([0.5, 1.0, 0.2, 2.0, 0.8]).astype(np.float64)\n    rho = 0.3\n    # First column of the Toeplitz correlation matrix C\n    c_col = np.array([rho**i for i in range(5)], dtype=np.float64)\n    C = toeplitz(c_col)\n    D_sqrt = np.sqrt(D)\n    R_D = D_sqrt @ C @ D_sqrt\n\n    test_cases = [R_A, R_B, R_C, R_D]\n    \n    all_results = []\n\n    for R in test_cases:\n        # --- Unwhitened Formulation ---\n        # P_a = (Gamma_inv + J.T @ R_inv @ J)^-1\n        Gamma_inv = np.diag(1.0 / np.diag(Gamma))\n        \n        # Stably compute J.T @ R_inv @ J\n        L_unwhitened = cholesky(R, lower=True)\n        # Solve R @ Z = J for Z = R_inv @ J using two triangular solves\n        K = solve_triangular(L_unwhitened, J, lower=True)\n        Z = solve_triangular(L_unwhitened.T, K, lower=False)\n        \n        Hessian_unwhitened = Gamma_inv + J.T @ Z\n        Pa_unwhitened = np.linalg.inv(Hessian_unwhitened)\n\n        # --- Whitened Formulation ---\n        # P_a = (Gamma_inv + J_tilde.T @ J_tilde)^-1 where J_tilde = L_inv @ J\n        L_whitened = cholesky(R, lower=True) # Same as cholesky(R, lower=True)\n        # Compute J_tilde by solving L @ J_tilde = J\n        J_tilde = solve_triangular(L_whitened, J, lower=True)\n        \n        Hessian_whitened = Gamma_inv + J_tilde.T @ J_tilde\n        Pa_whitened = np.linalg.inv(Hessian_whitened)\n\n        # --- Compute required output values ---\n        # 1. Frobenius norm of the difference between formulations\n        d = np.linalg.norm(Pa_unwhitened - Pa_whitened, 'fro')\n\n        # 2. Posterior marginal variances (diagonal of posterior covariance)\n        variances = np.diag(Pa_unwhitened)\n        v1, v2, v3 = variances[0], variances[1], variances[2]\n        \n        # 3. Trace of the posterior covariance\n        t = np.trace(Pa_unwhitened)\n        \n        # Collect results for this case. Rounding is done here.\n        case_result = [\n            round(d, 10),\n            round(v1, 10),\n            round(v2, 10),\n            round(v3, 10),\n            round(t, 10)\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string to match the required format exactly.\n    # The string representation of a list includes spaces, which we must remove\n    # by building the string manually.\n    output_parts = []\n    for res_list in all_results:\n        # Creates a string like \"[v1,v2,v3,v4,v5]\"\n        case_str = f\"[{','.join(map(str, res_list))}]\"\n        output_parts.append(case_str)\n    \n    # Joins the case strings into the final format \"[[...],[...],...]\"\n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3388459"}, {"introduction": "Building upon the concept of whitening, this practice shifts the focus from the posterior uncertainty to computing the optimal state estimate itself within a variational framework. You will implement and compare two equivalent approaches—one direct and one using a whitening transformation—to find the Maximum A Posteriori (MAP) solution that minimizes a quadratic cost function. This exercise will solidify your understanding of the practical implementation of whitening and verify that both formulations are numerically identical, leading to the same state estimate and cost value [@problem_id:3388472].", "problem": "Consider a linear Bayesian inverse problem with Gaussian prior and Gaussian observational noise in a variational data assimilation setting. Let the state vector be $\\boldsymbol{x} \\in \\mathbb{R}^{n}$, the observation operator be $H \\in \\mathbb{R}^{m \\times n}$, the observation vector be $\\boldsymbol{y} \\in \\mathbb{R}^{m}$, the prior mean be $\\boldsymbol{x}_{b} \\in \\mathbb{R}^{n}$, and the prior covariance be $B \\in \\mathbb{R}^{n \\times n}$. Let the observational error covariance be $R \\in \\mathbb{R}^{m \\times m}$, symmetric positive definite but heteroscedastic (i.e., possibly non-uniform variances and correlations across observations). The Maximum A Posteriori (MAP) estimator minimizes the negative log-posterior, which combines the prior term and the data-misfit term weighted by $R^{-1}$. Whitening refers to constructing a transformation that maps the data space to one in which observational errors are independent with unit variance.\n\nYour tasks are to derive, implement, and validate a whitening-based variational solution, starting strictly from the following fundamental base: (i) the definition of a multivariate Gaussian density with mean and covariance; (ii) the equivalence between the MAP estimator and the minimizer of a sum of quadratic forms arising from Gaussian prior and likelihood; and (iii) the definition of Cholesky factorization for symmetric positive definite matrices. You must not invoke any special-case formula without logical derivation from these bases. The program you produce must compute and compare the MAP solution obtained directly (unwhitened) and via whitening, verify the equivalence of costs and solutions, and quantify whitening accuracy.\n\nYou are given a fixed linear model and four test cases for the observational noise covariance. The state dimension is $n = 3$ and the observation dimension is $m = 5$. The common ingredients for all test cases are:\n- Observation operator $H \\in \\mathbb{R}^{5 \\times 3}$:\n  $$\n  H \\;=\\; \\begin{bmatrix}\n  1.0 & 0.0 & 0.5 \\\\\n  0.0 & 1.0 & -0.2 \\\\\n  0.5 & -0.3 & 1.0 \\\\\n  1.0 & 1.0 & 0.0 \\\\\n  0.0 & 0.5 & 0.5\n  \\end{bmatrix}.\n  $$\n- Prior mean $\\boldsymbol{x}_{b} \\in \\mathbb{R}^{3}$:\n  $$\n  \\boldsymbol{x}_{b} \\;=\\; \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\end{bmatrix}.\n  $$\n- Observations $\\boldsymbol{y} \\in \\mathbb{R}^{5}$:\n  $$\n  \\boldsymbol{y} \\;=\\; \\begin{bmatrix} 1.2 \\\\ -0.7 \\\\ 0.9 \\\\ 0.5 \\\\ -0.2 \\end{bmatrix}.\n  $$\n- Prior covariance $B \\in \\mathbb{R}^{3 \\times 3}$ specified via its lower-triangular Cholesky factor $S_{B} \\in \\mathbb{R}^{3 \\times 3}$ with strictly positive diagonal, such that $B \\;=\\; S_{B}\\,S_{B}^{\\top}$. Use\n  $$\n  S_{B} \\;=\\; \\begin{bmatrix}\n  1.0 & 0.0 & 0.0 \\\\\n  0.3 & 1.1 & 0.0 \\\\\n  -0.2 & 0.1 & 0.9\n  \\end{bmatrix}.\n  $$\n\nFor each test case $k \\in \\{1,2,3,4\\}$, the observational covariance $R^{(k)}$ is specified by a lower-triangular Cholesky factor $L_{R}^{(k)} \\in \\mathbb{R}^{5 \\times 5}$ with strictly positive diagonal, such that $R^{(k)} \\;=\\; L_{R}^{(k)} \\,\\left(L_{R}^{(k)}\\right)^{\\top}$. The four cases are:\n- Case $1$ (heteroscedastic diagonal):\n  $$\n  L_{R}^{(1)} \\;=\\; \\mathrm{diag}\\!\\left(0.5,\\,1.0,\\,0.8,\\,1.2,\\,0.6\\right).\n  $$\n- Case $2$ (heteroscedastic correlated):\n  $$\n  L_{R}^{(2)} \\;=\\; \\begin{bmatrix}\n  0.9 & 0.0 & 0.0 & 0.0 & 0.0 \\\\\n  0.2 & 0.7 & 0.0 & 0.0 & 0.0 \\\\\n  0.1 & -0.15 & 0.8 & 0.0 & 0.0 \\\\\n  0.05 & 0.1 & -0.05 & 1.1 & 0.0 \\\\\n  -0.1 & 0.05 & 0.2 & -0.1 & 0.6\n  \\end{bmatrix}.\n  $$\n- Case $3$ (ill-conditioned diagonal, yet positive definite):\n  $$\n  L_{R}^{(3)} \\;=\\; \\mathrm{diag}\\!\\left(0.1,\\,0.9,\\,1.2,\\,0.7,\\,0.5\\right).\n  $$\n- Case $4$ (heteroscedastic correlated, different scales):\n  $$\n  L_{R}^{(4)} \\;=\\; \\begin{bmatrix}\n  0.4 & 0.0 & 0.0 & 0.0 & 0.0 \\\\\n  0.1 & 0.5 & 0.0 & 0.0 & 0.0 \\\\\n  0.0 & 0.2 & 0.6 & 0.0 & 0.0 \\\\\n  0.0 & -0.1 & 0.1 & 0.9 & 0.0 \\\\\n  0.2 & 0.0 & -0.05 & 0.1 & 0.7\n  \\end{bmatrix}.\n  $$\n\nUsing only the bases listed earlier, derive and implement the following for each case $k$:\n- Construct $R^{(k)}$ from $L_{R}^{(k)}$ by $R^{(k)} \\;=\\; L_{R}^{(k)} \\,\\left(L_{R}^{(k)}\\right)^{\\top}$. Define a whitening operator $W^{(k)}$ by solving $L_{R}^{(k)}\\,W^{(k)} \\;=\\; I_{m}$, so that $W^{(k)} \\, R^{(k)} \\, \\left(W^{(k)}\\right)^{\\top} \\;=\\; I_{m}$.\n- Form the whitened operators $\\widetilde{H}^{(k)} \\;=\\; W^{(k)} \\, H$ and $\\widetilde{\\boldsymbol{d}}^{(k)} \\;=\\; W^{(k)} \\, \\boldsymbol{y}$.\n- Let $B \\;=\\; S_{B}\\,S_{B}^{\\top}$ and define $B^{-1}$ via $S_{B}$. Implement two MAP solvers:\n  1. Unwhitened: minimize the quadratic cost $J(\\boldsymbol{x}) \\;=\\; \\tfrac{1}{2}\\,\\lVert S_{B}^{-1}\\,(\\boldsymbol{x} - \\boldsymbol{x}_{b}) \\rVert_{2}^{2} \\;+\\; \\tfrac{1}{2}\\,\\lVert \\left(L_{R}^{(k)}\\right)^{-1}\\,\\left(H \\boldsymbol{x} - \\boldsymbol{y}\\right) \\rVert_{2}^{2}$.\n  2. Whitened: minimize the quadratic cost $\\widetilde{J}(\\boldsymbol{x}) \\;=\\; \\tfrac{1}{2}\\,\\lVert S_{B}^{-1}\\,(\\boldsymbol{x} - \\boldsymbol{x}_{b}) \\rVert_{2}^{2} \\;+\\; \\tfrac{1}{2}\\,\\lVert \\widetilde{H}^{(k)} \\boldsymbol{x} - \\widetilde{\\boldsymbol{d}}^{(k)} \\rVert_{2}^{2}$.\n  Both solvers must be implemented using numerically stable linear algebra based on Cholesky factorizations and triangular solves, not explicit matrix inversion.\n- Compute the following diagnostics for each case $k$:\n  1. The Euclidean norm $\\lVert \\boldsymbol{x}_{\\mathrm{unw}}^{(k)} - \\boldsymbol{x}_{\\mathrm{w}}^{(k)} \\rVert_{2}$ between the unwhitened solution and the whitened solution.\n  2. The absolute difference between the two costs evaluated at the whitened solution, i.e., $\\left| J\\!\\left(\\boldsymbol{x}_{\\mathrm{w}}^{(k)}\\right) - \\widetilde{J}\\!\\left(\\boldsymbol{x}_{\\mathrm{w}}^{(k)}\\right) \\right|$.\n  3. The maximum absolute entry of $W^{(k)} \\, R^{(k)} \\, \\left(W^{(k)}\\right)^{\\top} - I_{m}$.\n\nTest Suite:\n- Use the four cases $k \\in \\{1,2,3,4\\}$ defined above. This suite covers a diagonal heteroscedastic case, a correlated heteroscedastic case, an ill-conditioned diagonal case, and a correlated case with different scales.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the $12$ floating-point results as a comma-separated list enclosed in square brackets, ordered by test case and, within each case, by the three diagnostics in the order listed above. Concretely, the output must be\n  $$\n  \\big[\\;\\lVert \\boldsymbol{x}_{\\mathrm{unw}}^{(1)} - \\boldsymbol{x}_{\\mathrm{w}}^{(1)} \\rVert_{2},\\;\\left| J\\!\\left(\\boldsymbol{x}_{\\mathrm{w}}^{(1)}\\right) - \\widetilde{J}\\!\\left(\\boldsymbol{x}_{\\mathrm{w}}^{(1)}\\right) \\right|,\\;\\max\\!\\left|W^{(1)} R^{(1)} \\left(W^{(1)}\\right)^{\\top} - I\\right|,\\;\\ldots,\\;\\lVert \\boldsymbol{x}_{\\mathrm{unw}}^{(4)} - \\boldsymbol{x}_{\\mathrm{w}}^{(4)} \\rVert_{2},\\;\\left| J\\!\\left(\\boldsymbol{x}_{\\mathrm{w}}^{(4)}\\right) - \\widetilde{J}\\!\\left(\\boldsymbol{x}_{\\mathrm{w}}^{(4)}\\right) \\right|,\\;\\max\\!\\left|W^{(4)} R^{(4)} \\left(W^{(4)}\\right)^{\\top} - I\\right|\\;\\big].\n  $$\n- No units are involved. All angles, if any appear, must be interpreted in radians; however, there are no angles in this problem.\n\nConstraints:\n- The program must be a complete, runnable program that takes no input, uses no external files or network access, and strictly prints the final output line in the specified format.\n- Use only numerically stable operations based on Cholesky factorizations and triangular solves to realize $B^{-1}$, $R^{-1}$, and whitening. Avoid forming explicit matrix inverses.", "solution": "The problem requires the derivation and implementation of a Maximum A Posteriori (MAP) estimator for a linear Bayesian inverse problem, with a comparison between a direct (unwhitened) and a whitening-based approach. The entire derivation must be founded on three principles: the definition of a multivariate Gaussian density, the equivalence of the MAP estimator to a minimizer of a quadratic cost function, and the properties of Cholesky factorization.\n\n1.  **Derivation of the MAP Cost Function**\n\nThe problem is framed within a Bayesian context. We are given a prior belief about the state vector $\\boldsymbol{x} \\in \\mathbb{R}^{n}$ and a model connecting the state to observations $\\boldsymbol{y} \\in \\mathbb{R}^{m}$.\n\nThe prior distribution of $\\boldsymbol{x}$ is given as a multivariate Gaussian with mean $\\boldsymbol{x}_b$ and covariance $B$. The probability density function (PDF) is:\n$$p(\\boldsymbol{x}) \\propto \\exp\\left( -\\frac{1}{2} (\\boldsymbol{x} - \\boldsymbol{x}_b)^\\top B^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) \\right)$$\nwhere $B \\in \\mathbb{R}^{n \\times n}$ is the symmetric positive definite prior covariance matrix.\n\nThe observations $\\boldsymbol{y}$ are related to the state $\\boldsymbol{x}$ via the linear model $\\boldsymbol{y} = H\\boldsymbol{x} + \\boldsymbol{\\epsilon}$, where $H \\in \\mathbb{R}^{m \\times n}$ is the observation operator and $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^{m}$ is the observational error. The error is assumed to be a zero-mean Gaussian random variable, $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, R)$, with a symmetric positive definite covariance matrix $R \\in \\mathbb{R}^{m \\times m}$. The likelihood, or the conditional PDF of $\\boldsymbol{y}$ given $\\boldsymbol{x}$, is therefore:\n$$p(\\boldsymbol{y}|\\boldsymbol{x}) \\propto \\exp\\left( -\\frac{1}{2} (\\boldsymbol{y} - H\\boldsymbol{x})^\\top R^{-1} (\\boldsymbol{y} - H\\boldsymbol{x}) \\right)$$\n\nAccording to Bayes' theorem, the posterior PDF of the state $\\boldsymbol{x}$ given the observations $\\boldsymbol{y}$ is proportional to the product of the likelihood and the prior: $p(\\boldsymbol{x}|\\boldsymbol{y}) \\propto p(\\boldsymbol{y}|\\boldsymbol{x}) p(\\boldsymbol{x})$.\nSubstituting the Gaussian forms gives:\n$$p(\\boldsymbol{x}|\\boldsymbol{y}) \\propto \\exp\\left( -\\frac{1}{2} \\left[ (\\boldsymbol{x} - \\boldsymbol{x}_b)^\\top B^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) + (H\\boldsymbol{x} - \\boldsymbol{y})^\\top R^{-1} (H\\boldsymbol{x} - \\boldsymbol{y}) \\right] \\right)$$\n\nThe MAP estimate of $\\boldsymbol{x}$ is the value that maximizes the posterior probability $p(\\boldsymbol{x}|\\boldsymbol{y})$. Maximizing $p(\\boldsymbol{x}|\\boldsymbol{y})$ is equivalent to minimizing its negative logarithm. This leads to the quadratic cost function $J(\\boldsymbol{x})$:\n$$J(\\boldsymbol{x}) = \\frac{1}{2} (\\boldsymbol{x} - \\boldsymbol{x}_b)^\\top B^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) + \\frac{1}{2} (H\\boldsymbol{x} - \\boldsymbol{y})^\\top R^{-1} (H\\boldsymbol{x} - \\boldsymbol{y})$$\nThe MAP estimate $\\boldsymbol{x}_a$ is the unique minimizer of $J(\\boldsymbol{x})$, i.e., $\\boldsymbol{x}_a = \\arg\\min_{\\boldsymbol{x}} J(\\boldsymbol{x})$.\n\n2.  **Cost Function using Cholesky Factors**\n\nThe problem provides the covariance matrices $B$ and $R$ via their lower-triangular Cholesky factors, $S_B$ and $L_R$, such that $B = S_B S_B^\\top$ and $R = L_R L_R^\\top$. Since $S_B$ and $L_R$ are stated to have strictly positive diagonals, they are invertible, which guarantees $B$ and $R$ are positive definite. The inverse of a covariance matrix can be expressed using its Cholesky factor's inverse: $B^{-1} = (S_B S_B^\\top)^{-1} = (S_B^\\top)^{-1} S_B^{-1}$.\n\nA quadratic form $\\boldsymbol{z}^\\top C^{-1} \\boldsymbol{z}$ where $C=LL^\\top$ can be written as a squared Euclidean norm:\n$$\\boldsymbol{z}^\\top C^{-1} \\boldsymbol{z} = \\boldsymbol{z}^\\top (L^\\top)^{-1} L^{-1} \\boldsymbol{z} = (L^{-1} \\boldsymbol{z})^\\top (L^{-1} \\boldsymbol{z}) = \\lVert L^{-1} \\boldsymbol{z} \\rVert_2^2$$\nApplying this to the two terms in $J(\\boldsymbol{x})$, we get:\n$$J(\\boldsymbol{x}) = \\frac{1}{2} \\lVert S_B^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) \\rVert_2^2 + \\frac{1}{2} \\lVert L_R^{-1} (H\\boldsymbol{x} - \\boldsymbol{y}) \\rVert_2^2$$\nThis is the \"unwhitened\" cost function specified in the problem statement for a given case $k$ with covariance factor $L_R^{(k)}$. This form is superior for numerical computation as it avoids the explicit formation of ill-conditioned inverse matrices, relying instead on solving triangular systems.\n\n3.  **The Whitening Transformation**\n\nWhitening is a change of variables that transforms a correlated error distribution into one with uncorrelated, unit-variance components (i.e., its covariance is the identity matrix). Let the original error vector be $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, R)$. We seek a linear transformation $W$ such that the whitened error $\\tilde{\\boldsymbol{\\epsilon}} = W\\boldsymbol{\\epsilon}$ has covariance $I$. The covariance of $\\tilde{\\boldsymbol{\\epsilon}}$ is:\n$$\\mathrm{Cov}(\\tilde{\\boldsymbol{\\epsilon}}) = E[\\tilde{\\boldsymbol{\\epsilon}} \\tilde{\\boldsymbol{\\epsilon}}^\\top] = E[(W\\boldsymbol{\\epsilon})(W\\boldsymbol{\\epsilon})^\\top] = W E[\\boldsymbol{\\epsilon} \\boldsymbol{\\epsilon}^\\top] W^\\top = W R W^\\top$$\nTo achieve $\\mathrm{Cov}(\\tilde{\\boldsymbol{\\epsilon}}) = I$, we must have $W R W^\\top = I$. Using the Cholesky factorization $R = L_R L_R^\\top$, we can choose the whitening operator $W = L_R^{-1}$. This gives:\n$$W R W^\\top = L_R^{-1} (L_R L_R^\\top) (L_R^{-1})^\\top = (L_R^{-1} L_R) (L_R^\\top (L_R^\\top)^{-1}) = I \\cdot I = I$$\nThe problem specifies defining $W$ by solving $L_R W = I$, which is precisely the definition of $W = L_R^{-1}$.\n\nWe apply this transformation to the entire observation equation:\n$$\\boldsymbol{y} = H\\boldsymbol{x} + \\boldsymbol{\\epsilon} \\implies W \\boldsymbol{y} = W(H\\boldsymbol{x}) + W\\boldsymbol{\\epsilon}$$\nDefining the whitened observations $\\tilde{\\boldsymbol{d}} = W\\boldsymbol{y}$ and the whitened observation operator $\\widetilde{H} = WH$, the model becomes:\n$$\\tilde{\\boldsymbol{d}} = \\widetilde{H}\\boldsymbol{x} + \\tilde{\\boldsymbol{\\epsilon}}, \\quad \\text{where } \\tilde{\\boldsymbol{\\epsilon}} \\sim \\mathcal{N}(0, I)$$\n\n4.  **The Whitened Cost Function**\n\nThe MAP cost function can be formulated in the whitened space. The data-misfit term corresponds to the negative log-likelihood of the whitened model. With $\\tilde{\\boldsymbol{\\epsilon}}$ having an identity covariance matrix, this term simplifies to a simple sum of squares:\n$$\\frac{1}{2} (\\tilde{\\boldsymbol{d}} - \\widetilde{H}\\boldsymbol{x})^\\top I^{-1} (\\tilde{\\boldsymbol{d}} - \\widetilde{H}\\boldsymbol{x}) = \\frac{1}{2} \\lVert \\widetilde{H}\\boldsymbol{x} - \\tilde{\\boldsymbol{d}} \\rVert_2^2$$\nCombining this with the unchanged prior term, we obtain the \"whitened\" cost function $\\widetilde{J}(\\boldsymbol{x})$:\n$$\\widetilde{J}(\\boldsymbol{x}) = \\frac{1}{2} \\lVert S_B^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) \\rVert_2^2 + \\frac{1}{2} \\lVert \\widetilde{H}\\boldsymbol{x} - \\tilde{\\boldsymbol{d}} \\rVert_2^2$$\nBy substituting the definitions $\\widetilde{H} = L_R^{-1}H$ and $\\tilde{\\boldsymbol{d}} = L_R^{-1}\\boldsymbol{y}$, we can see that this is identical to the unwhitened cost function:\n$$\\widetilde{J}(\\boldsymbol{x}) = \\frac{1}{2} \\lVert S_B^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) \\rVert_2^2 + \\frac{1}{2} \\lVert (L_R^{-1}H)\\boldsymbol{x} - L_R^{-1}\\boldsymbol{y} \\rVert_2^2 = \\frac{1}{2} \\lVert S_B^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) \\rVert_2^2 + \\frac{1}{2} \\lVert L_R^{-1}(H\\boldsymbol{x} - \\boldsymbol{y}) \\rVert_2^2 = J(\\boldsymbol{x})$$\nThe two cost functions are analytically identical. Consequently, their minima must be the same point, and their values at any given point $\\boldsymbol{x}$ must be equal.\n\n5.  **Solution via Linear Least Squares**\n\nMinimizing $J(\\boldsymbol{x})$ (or $\\widetilde{J}(\\boldsymbol{x})$) is a linear least-squares problem. We seek to find $\\boldsymbol{x}$ that minimizes the sum of two squared norms. This can be expressed as minimizing the norm of a single, larger residual vector:\n$$\\min_{\\boldsymbol{x}} \\left\\lVert \\begin{pmatrix} S_B^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) \\\\ L_R^{-1} (H\\boldsymbol{x} - \\boldsymbol{y}) \\end{pmatrix} \\right\\rVert_2^2 = \\min_{\\boldsymbol{x}} \\left\\lVert \\begin{pmatrix} S_B^{-1}\\boldsymbol{x} - S_B^{-1}\\boldsymbol{x}_b \\\\ (L_R^{-1}H)\\boldsymbol{x} - L_R^{-1}\\boldsymbol{y} \\end{pmatrix} \\right\\rVert_2^2$$\nThis is a standard least-squares problem of the form $\\min_{\\boldsymbol{x}} \\lVert G\\boldsymbol{x} - \\boldsymbol{f} \\rVert_2^2$, with the augmented matrix $G$ and vector $\\boldsymbol{f}$ defined as:\n$$G = \\begin{pmatrix} S_B^{-1} \\\\ L_R^{-1}H \\end{pmatrix} \\in \\mathbb{R}^{(n+m) \\times n}, \\quad \\boldsymbol{f} = \\begin{pmatrix} S_B^{-1}\\boldsymbol{x}_b \\\\ L_R^{-1}\\boldsymbol{y} \\end{pmatrix} \\in \\mathbb{R}^{(n+m)}$$\nThe solution $\\boldsymbol{x}_a$ is found by solving the normal equations $(G^\\top G) \\boldsymbol{x}_a = G^\\top \\boldsymbol{f}$, though numerically superior methods like QR factorization are preferred.\n\n6.  **Numerical Implementation Strategy**\n\nWe will implement two solvers as prescribed, corresponding to the \"unwhitened\" and \"whitened\" viewpoints, to verify their numerical equivalence.\n\nFor both solvers, we first compute the matrix inverses implicitly. The matrix $S_B^{-1}$ is found by solving the triangular system $S_B X = I_n$. Similarly, for each test case $k$, the whitening operator $W^{(k)} = (L_R^{(k)})^{-1}$ is found by solving $L_R^{(k)} X = I_m$. These operations are performed using `scipy.linalg.solve_triangular`, which is numerically stable.\n\n-   **Unwhitened Solver**: We construct the augmented system directly using the inverse operators. The matrix $G_{unw}$ and vector $\\boldsymbol{f}_{unw}$ are formed as:\n    $$G_{unw} = \\begin{pmatrix} S_B^{-1} \\\\ W^{(k)}H \\end{pmatrix}, \\quad \\boldsymbol{f}_{unw} = \\begin{pmatrix} S_B^{-1}\\boldsymbol{x}_b \\\\ W^{(k)}\\boldsymbol{y} \\end{pmatrix}$$\n    The solution $\\boldsymbol{x}_{unw}^{(k)}$ is obtained by solving the least-squares problem $\\min_{\\boldsymbol{x}} \\lVert G_{unw}\\boldsymbol{x} - \\boldsymbol{f}_{unw} \\rVert_2^2$.\n\n-   **Whitened Solver**: We first explicitly compute the whitened quantities $\\widetilde{H}^{(k)} = W^{(k)}H$ and $\\tilde{\\boldsymbol{d}}^{(k)} = W^{(k)}\\boldsymbol{y}$. Then, we assemble the augmented system:\n    $$G_w = \\begin{pmatrix} S_B^{-1} \\\\ \\widetilde{H}^{(k)} \\end{pmatrix}, \\quad \\boldsymbol{f}_w = \\begin{pmatrix} S_B^{-1}\\boldsymbol{x}_b \\\\ \\tilde{\\boldsymbol{d}}^{(k)} \\end{pmatrix}$$\n    The solution $\\boldsymbol{x}_{w}^{(k)}$ is obtained by solving $\\min_{\\boldsymbol{x}} \\lVert G_w \\boldsymbol{x} - \\boldsymbol{f}_w \\rVert_2^2$.\n\nNumerically, these two procedures are identical and should produce solutions $\\boldsymbol{x}_{unw}^{(k)}$ and $\\boldsymbol{x}_{w}^{(k)}$ that are equal up to machine precision.\n\n7.  **Diagnostic Metrics**\n\nThe three diagnostics serve to validate the theoretical equivalences and numerical accuracy.\n\n1.  $\\lVert \\boldsymbol{x}_{\\mathrm{unw}}^{(k)} - \\boldsymbol{x}_{\\mathrm{w}}^{(k)} \\rVert_{2}$: This measures the Euclidean distance between the solutions from the two solvers. Theory predicts this should be zero. In practice, it will be a small number on the order of machine precision, confirming that both implementation paths lead to the same result.\n\n2.  $\\left| J(\\boldsymbol{x}_{\\mathrm{w}}^{(k)}) - \\widetilde{J}(\\boldsymbol{x}_{\\mathrm{w}}^{(k)}) \\right|$: This measures the absolute difference between the values of the unwhitened and whitened cost functions, evaluated at the computed solution $\\boldsymbol{x}_{\\mathrm{w}}^{(k)}$. As shown analytically, $J(\\boldsymbol{x}) = \\widetilde{J}(\\boldsymbol{x})$ for any $\\boldsymbol{x}$. This diagnostic verifies their numerical equality, which should also hold to machine precision. The cost functions are implemented using stable triangular solves on the Cholesky factors, as derived earlier.\n\n3.  $\\max| W^{(k)} R^{(k)} (W^{(k)})^\\top - I_m |$: This quantifies the numerical accuracy of the whitening transformation itself. We compute $W^{(k)} = (L_R^{(k)})^{-1}$ and $R^{(k)} = L_R^{(k)} (L_R^{(k)})^\\top$, then form the product $W^{(k)} R^{(k)} (W^{(k)})^\\top$. The maximum absolute element of the difference between this product and the identity matrix $I_m$ should be a small number, reflecting the precision of the floating-point arithmetic and the stability of the triangular solve used to find $W^{(k)}$.\n\nBy computing these quantities for each test case, we validate the entire framework from theory to implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_triangular\n\ndef solve():\n    \"\"\"\n    Solves a linear Bayesian inverse problem using unwhitened and whitened\n    formulations, and computes diagnostics to verify their equivalence.\n    \"\"\"\n    # Common ingredients for all test cases\n    H = np.array([\n        [1.0, 0.0, 0.5],\n        [0.0, 1.0, -0.2],\n        [0.5, -0.3, 1.0],\n        [1.0, 1.0, 0.0],\n        [0.0, 0.5, 0.5]\n    ])\n    xb = np.array([0.2, -0.1, 0.3])\n    y = np.array([1.2, -0.7, 0.9, 0.5, -0.2])\n    SB = np.array([\n        [1.0, 0.0, 0.0],\n        [0.3, 1.1, 0.0],\n        [-0.2, 0.1, 0.9]\n    ])\n\n    n = H.shape[1]\n    m = H.shape[0]\n\n    # Test cases for observational error covariance Cholesky factor LR\n    test_cases = [\n        # Case 1: heteroscedastic diagonal\n        np.diag([0.5, 1.0, 0.8, 1.2, 0.6]),\n        \n        # Case 2: heteroscedastic correlated\n        np.array([\n            [0.9, 0.0, 0.0, 0.0, 0.0],\n            [0.2, 0.7, 0.0, 0.0, 0.0],\n            [0.1, -0.15, 0.8, 0.0, 0.0],\n            [0.05, 0.1, -0.05, 1.1, 0.0],\n            [-0.1, 0.05, 0.2, -0.1, 0.6]\n        ]),\n        \n        # Case 3: ill-conditioned diagonal, yet positive definite\n        np.diag([0.1, 0.9, 1.2, 0.7, 0.5]),\n        \n        # Case 4: heteroscedastic correlated, different scales\n        np.array([\n            [0.4, 0.0, 0.0, 0.0, 0.0],\n            [0.1, 0.5, 0.0, 0.0, 0.0],\n            [0.0, 0.2, 0.6, 0.0, 0.0],\n            [0.0, -0.1, 0.1, 0.9, 0.0],\n            [0.2, 0.0, -0.05, 0.1, 0.7]\n        ])\n    ]\n\n    results = []\n    \n    # Pre-compute SB^-1 as it's common to all cases and solvers\n    I_n = np.eye(n)\n    SB_inv = solve_triangular(SB, I_n, lower=True)\n\n    for LRk in test_cases:\n        I_m = np.eye(m)\n        \n        # Pre-compute Wk = LRk^-1, also common to both solvers within a case\n        Wk = solve_triangular(LRk, I_m, lower=True)\n\n        # 1. Unwhitened Solver\n        # Cost J(x) = 0.5 * ||SB_inv(x-xb)||^2 + 0.5 * ||LRk_inv(Hx-y)||^2\n        # is minimized by the least-squares problem ||G_unw * x - f_unw||^2\n        G_unw_top = SB_inv\n        G_unw_bot = Wk @ H\n        G_unw = np.vstack((G_unw_top, G_unw_bot))\n        \n        f_unw_top = SB_inv @ xb\n        f_unw_bot = Wk @ y\n        f_unw = np.concatenate((f_unw_top, f_unw_bot))\n        \n        x_unw, _, _, _ = np.linalg.lstsq(G_unw, f_unw, rcond=None)\n\n        # 2. Whitened Solver\n        # Cost J_tilde(x) = 0.5 * ||SB_inv(x-xb)||^2 + 0.5 * ||H_tilde*x - d_tilde||^2\n        # where H_tilde = Wk*H and d_tilde=Wk*y.\n        # This is minimized by the least-squares problem ||G_w * x - f_w||^2\n        H_tilde = Wk @ H\n        d_tilde = Wk @ y\n\n        G_w_top = SB_inv\n        G_w_bot = H_tilde\n        G_w = np.vstack((G_w_top, G_w_bot))\n\n        f_w_top = SB_inv @ xb\n        f_w_bot = d_tilde\n        f_w = np.concatenate((f_w_top, f_w_bot))\n\n        x_w, _, _, _ = np.linalg.lstsq(G_w, f_w, rcond=None)\n\n        # --- DIAGNOSTICS ---\n        \n        # Diagnostic 1: Norm of difference between solutions\n        diag1 = np.linalg.norm(x_unw - x_w)\n\n        # Diagnostic 2: Absolute difference between cost function values at x_w\n        # Unwhitened cost J(x_w)\n        res_b = x_w - xb\n        norm_res_b = solve_triangular(SB, res_b, lower=True)\n        cost_b_unw = 0.5 * np.dot(norm_res_b, norm_res_b)\n        \n        res_o = H @ x_w - y\n        norm_res_o = solve_triangular(LRk, res_o, lower=True)\n        cost_o_unw = 0.5 * np.dot(norm_res_o, norm_res_o)\n        J_at_xw = cost_b_unw + cost_o_unw\n\n        # Whitened cost J_tilde(x_w)\n        # Background term is the same\n        cost_b_w = cost_b_unw\n        \n        res_o_tilde = H_tilde @ x_w - d_tilde\n        cost_o_w = 0.5 * np.dot(res_o_tilde, res_o_tilde)\n        J_tilde_at_xw = cost_b_w + cost_o_w\n        \n        diag2 = np.abs(J_at_xw - J_tilde_at_xw)\n\n        # Diagnostic 3: Accuracy of whitening\n        Rk = LRk @ LRk.T\n        whitening_check_matrix = Wk @ Rk @ Wk.T\n        diag3 = np.max(np.abs(whitening_check_matrix - I_m))\n        \n        results.extend([diag1, diag2, diag3])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "3388472"}, {"introduction": "This advanced practice delves into a more subtle but critical aspect of data assimilation: how observation weighting influences the identifiability and correlation between specific subsets of parameters. Using the powerful tool of the Schur complement, you will analyze the marginal posterior covariance to quantify how down-weighting certain observations can strengthen or weaken the statistical relationships between coupled model parameters. This exercise provides deep insight into how the structure of the observation error covariance, $R$, shapes not just individual parameter uncertainties but their interdependencies as well [@problem_id:3388480].", "problem": "Consider a linear inverse problem with Gaussian prior and heteroscedastic observation errors. A state vector $\\boldsymbol{x} \\in \\mathbb{R}^n$ is observed through a linear map, and the observations have mutually uncorrelated but unequal variances. Whitening of observations and proper weighting must be handled to quantify how downweighting certain observation components (via large variances) affects identifiability and posterior correlations of coupled parameters. Your task is to derive, implement, and evaluate a Schur complement based analysis to quantify changes in the posterior correlation between selected parameters under specified perturbations of the observation error covariance.\n\nStarting from the fundamental definitions of a linear Gaussian Bayesian inverse problem and the construction of the quadratic cost from the prior and the likelihood, do the following tasks without assuming any target formulas in advance:\n\n- Derive the posterior information matrix in terms of the prior precision, the observation operator, and the observation error covariance, using only the definitions of the Gaussian prior and the Gaussian likelihood. Justify why whitening with the inverse square-root of the observation error covariance does not change the posterior but changes the normal equations, and express the posterior information in whitened coordinates.\n- Partition the posterior information matrix into two blocks corresponding to a target subset of parameters and its complement. Using the Schur complement of a symmetric positive definite block matrix, derive an explicit expression for the marginal posterior covariance of the target subset. Explain why this marginal posterior covariance is invariant to whether one computes in whitened or unwhitened coordinates.\n- Define the posterior correlation coefficient between two target parameters in terms of the entries of their marginal posterior covariance, and state how this correlation changes when the observation error covariance is perturbed.\n\nThen, apply your derivations to the following concrete instance. The state dimension is $n = 3$, and the observation dimension is $m = 3$. The prior mean is arbitrary and irrelevant for covariance calculations. The prior covariance matrix $B \\in \\mathbb{R}^{3 \\times 3}$ and the observation operator $H \\in \\mathbb{R}^{3 \\times 3}$ are fixed as:\n- $B = \\begin{bmatrix} 1.0 & 0.6 & 0.2 \\\\ 0.6 & 1.5 & 0.4 \\\\ 0.2 & 0.4 & 1.2 \\end{bmatrix}$,\n- $H = \\begin{bmatrix} 1.0 & 1.0 & 0.0 \\\\ 0.0 & 1.0 & 1.0 \\\\ 1.0 & 0.0 & 1.0 \\end{bmatrix}$.\n\nThe observation error covariance $R \\in \\mathbb{R}^{3 \\times 3}$ is diagonal and heteroscedastic, i.e., $R = \\operatorname{diag}(\\sigma_1^2,\\sigma_2^2,\\sigma_3^2)$. Denote by $W = R^{-1/2}$ the whitening matrix. The target subset of parameters is the first two components of $\\boldsymbol{x}$, denoted $\\boldsymbol{x}_a = [x_1,x_2]^\\top$, with the complement $\\boldsymbol{x}_b = [x_3]$.\n\nDefine the posterior information matrix $J \\in \\mathbb{R}^{3 \\times 3}$, and partition it as\n$J = \\begin{bmatrix} J_{aa} & J_{ab} \\\\ J_{ba} & J_{bb} \\end{bmatrix}$ with $J_{aa} \\in \\mathbb{R}^{2 \\times 2}$, $J_{bb} \\in \\mathbb{R}^{1 \\times 1}$, and conformal off-diagonal blocks. Use the Schur complement of $J_{bb}$ in $J$ to obtain the marginal posterior covariance $S_{aa} \\in \\mathbb{R}^{2 \\times 2}$ of $\\boldsymbol{x}_a$, and compute the posterior correlation coefficient $\\rho_{12}$ between $x_1$ and $x_2$ as a function of $R$.\n\nImplement a program that computes, for each of the following test cases, the posterior correlation coefficient $\\rho_{12}$ before and after a specified perturbation of $R$, and the change $\\Delta \\rho_{12}$ defined as the difference between the perturbed and baseline correlations. All computations must be performed using the Schur complement based expression of $S_{aa}$ derived above.\n\nTest suite:\n- Case A (happy path): Baseline $R_0 = \\operatorname{diag}(\\sigma_1^2,\\sigma_2^2,\\sigma_3^2)$ with $(\\sigma_1,\\sigma_2,\\sigma_3) = (0.2, 0.5, 1.0)$. Perturbation: increase the third variance by $\\Delta = 4.0$, i.e., $R_{\\text{pert}} = \\operatorname{diag}(0.2^2, 0.5^2, 1.0^2 + 4.0)$.\n- Case B (boundary downweighting): Baseline $R_0$ as in Case A. Perturbation: set the third variance to a very large value to emulate near-complete downweighting, i.e., $R_{\\text{pert}} = \\operatorname{diag}(0.2^2, 0.5^2, 10^6)$.\n- Case C (edge informative component): Baseline $R_0 = \\operatorname{diag}(0.5^2, 0.5^2, 0.5^2)$. Perturbation: decrease the second variance to a highly informative level, i.e., $R_{\\text{pert}} = \\operatorname{diag}(0.5^2, 0.1^2, 0.5^2)$.\n\nFor each case, define $\\rho_{12}^{\\text{base}}$ from $R_0$, $\\rho_{12}^{\\text{pert}}$ from $R_{\\text{pert}}$, and $\\Delta \\rho_{12} = \\rho_{12}^{\\text{pert}} - \\rho_{12}^{\\text{base}}$. No physical units are involved. Angles are not involved.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the nine floating-point numbers in this order:\n  - Case A: $\\rho_{12}^{\\text{base}}$, $\\rho_{12}^{\\text{pert}}$, $\\Delta \\rho_{12}$,\n  - Case B: $\\rho_{12}^{\\text{base}}$, $\\rho_{12}^{\\text{pert}}$, $\\Delta \\rho_{12}$,\n  - Case C: $\\rho_{12}^{\\text{base}}$, $\\rho_{12}^{\\text{pert}}$, $\\Delta \\rho_{12}$.\n- Round all floating-point outputs to six decimal places.\n\nYour implementation must be a complete, runnable program that performs the derivations computationally via linear algebra and Schur complements and prints the final list exactly as specified. No user input is required. The analysis must be universally applicable to any modern programming language; for this task, you must implement it in Python and ensure numerical stability in matrix inversions by using appropriate linear algebra routines.", "solution": "The problem is assessed as valid. It is scientifically grounded in Bayesian inverse theory, well-posed with a complete and consistent set of definitions and data, and objectively formulated. It poses a non-trivial challenge that requires both theoretical derivation and numerical implementation, directly addressing the core concepts of heteroscedastic weighting and posterior uncertainty quantification.\n\n### Theoretical Derivations\n\n#### 1. Posterior Information Matrix and Whitening\n\nIn a linear Bayesian inverse problem, the state vector $\\boldsymbol{x} \\in \\mathbb{R}^n$ is related to observations $\\boldsymbol{d} \\in \\mathbb{R}^m$ via the model $\\boldsymbol{d} = H\\boldsymbol{x} + \\boldsymbol{\\epsilon}$, where $H$ is the linear observation operator and $\\boldsymbol{\\epsilon}$ is the observation error.\n\nThe Bayesian framework combines prior knowledge about $\\boldsymbol{x}$ with information from the observations $\\boldsymbol{d}$ to yield a posterior probability distribution for $\\boldsymbol{x}$. According to Bayes' theorem, the posterior probability density function (PDF) $p(\\boldsymbol{x}|\\boldsymbol{d})$ is proportional to the product of the likelihood $p(\\boldsymbol{d}|\\boldsymbol{x})$ and the prior $p(\\boldsymbol{x})$:\n$$p(\\boldsymbol{x}|\\boldsymbol{d}) \\propto p(\\boldsymbol{d}|\\boldsymbol{x}) p(\\boldsymbol{x})$$\n\nThe problem states a Gaussian prior for $\\boldsymbol{x}$ and Gaussian observation errors.\nThe prior distribution is $\\boldsymbol{x} \\sim \\mathcal{N}(\\boldsymbol{x}_{prior}, B)$, where $\\boldsymbol{x}_{prior}$ is the prior mean and $B$ is the prior covariance matrix. Its PDF is:\n$$p(\\boldsymbol{x}) \\propto \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{x}_{prior})^\\top B^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_{prior})\\right)$$\nThe matrix $B^{-1}$ is the prior precision matrix.\n\nThe observation error is $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, R)$, where $R$ is the observation error covariance matrix. For a given state $\\boldsymbol{x}$, the observations $\\boldsymbol{d}$ are thus distributed as $\\boldsymbol{d} \\sim \\mathcal{N}(H\\boldsymbol{x}, R)$. The likelihood function is:\n$$p(\\boldsymbol{d}|\\boldsymbol{x}) \\propto \\exp\\left(-\\frac{1}{2}(\\boldsymbol{d} - H\\boldsymbol{x})^\\top R^{-1} (\\boldsymbol{d} - H\\boldsymbol{x})\\right)$$\nThe matrix $R^{-1}$ is the observation error precision matrix.\n\nCombining the prior and the likelihood, the posterior PDF is:\n$$p(\\boldsymbol{x}|\\boldsymbol{d}) \\propto \\exp\\left(-\\frac{1}{2}\\left[ (\\boldsymbol{x} - \\boldsymbol{x}_{prior})^\\top B^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_{prior}) + (\\boldsymbol{d} - H\\boldsymbol{x})^\\top R^{-1} (\\boldsymbol{d} - H\\boldsymbol{x}) \\right]\\right)$$\nThe expression in the exponent is a quadratic function of $\\boldsymbol{x}$. This implies that the posterior distribution is also Gaussian, $\\boldsymbol{x}|\\boldsymbol{d} \\sim \\mathcal{N}(\\boldsymbol{x}_{post}, P)$, where $\\boldsymbol{x}_{post}$ is the posterior mean and $P$ is the posterior covariance. The posterior PDF can be written as:\n$$p(\\boldsymbol{x}|\\boldsymbol{d}) \\propto \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{x}_{post})^\\top P^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_{post})\\right)$$\nThe matrix $J = P^{-1}$ is the posterior information matrix, also known as the posterior precision matrix. It is the Hessian of the negative log-posterior, also called the cost function $\\mathcal{J}(\\boldsymbol{x}) = \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{x}_{prior})^\\top B^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_{prior}) + \\frac{1}{2}(\\boldsymbol{d} - H\\boldsymbol{x})^\\top R^{-1} (\\boldsymbol{d} - H\\boldsymbol{x})$, with respect to $\\boldsymbol{x}$.\n\nTo find the Hessian, we compute the second derivative of $\\mathcal{J}(\\boldsymbol{x})$ with respect to $\\boldsymbol{x}$.\n$$\\nabla_{\\boldsymbol{x}} \\mathcal{J}(\\boldsymbol{x}) = B^{-1}(\\boldsymbol{x} - \\boldsymbol{x}_{prior}) - H^\\top R^{-1} (\\boldsymbol{d} - H\\boldsymbol{x})$$\n$$\\nabla_{\\boldsymbol{x}}^2 \\mathcal{J}(\\boldsymbol{x}) = B^{-1} + H^\\top R^{-1} H$$\nThus, the posterior information matrix $J$ is:\n$$J = B^{-1} + H^\\top R^{-1} H$$\n\nNow, consider the effect of whitening. Whitening transforms the observations and the observation operator using the matrix $W = R^{-1/2}$, which is the inverse matrix square root of the (typically diagonal) observation error covariance $R$. The transformed observations are $\\boldsymbol{d}' = W\\boldsymbol{d}$ and the transformed operator is $H' = WH$. The transformed observation error $\\boldsymbol{\\epsilon}' = W\\boldsymbol{\\epsilon}$ has covariance $E[\\boldsymbol{\\epsilon}'(\\boldsymbol{\\epsilon}')^\\top] = W E[\\boldsymbol{\\epsilon} \\boldsymbol{\\epsilon}^\\top] W^\\top = W R W^\\top = R^{-1/2} R R^{-1/2} = I$, the identity matrix.\n\nThe likelihood term in the cost function, expressed in whitened coordinates, is $\\frac{1}{2}(\\boldsymbol{d}' - H'\\boldsymbol{x})^\\top (\\boldsymbol{d}' - H'\\boldsymbol{x})$. Substituting the definitions of $\\boldsymbol{d}'$ and $H'$:\n$$\\frac{1}{2}(W\\boldsymbol{d} - WH\\boldsymbol{x})^\\top (W\\boldsymbol{d} - WH\\boldsymbol{x}) = \\frac{1}{2}(W(\\boldsymbol{d} - H\\boldsymbol{x}))^\\top (W(\\boldsymbol{d} - H\\boldsymbol{x})) = \\frac{1}{2}(\\boldsymbol{d} - H\\boldsymbol{x})^\\top W^\\top W (\\boldsymbol{d} - H\\boldsymbol{x})$$\nSince $R$ is symmetric, $W=R^{-1/2}$ is also symmetric. Therefore, $W^\\top = W$, and $W^\\top W = W^2 = (R^{-1/2})^2 = R^{-1}$. The likelihood term is thus $\\frac{1}{2}(\\boldsymbol{d}-H\\boldsymbol{x})^\\top R^{-1} (\\boldsymbol{d}-H\\boldsymbol{x})$, which is identical to the unwhitened form. Because the cost function $\\mathcal{J}(\\boldsymbol{x})$ remains unchanged, the posterior distribution $p(\\boldsymbol{x}|\\boldsymbol{d})$ is also unchanged.\n\nHowever, the normal equations, which are derived by setting $\\nabla_{\\boldsymbol{x}} \\mathcal{J}(\\boldsymbol{x}) = 0$ to find the posterior mean, change their form.\nIn unwhitened coordinates:\n$$(B^{-1} + H^\\top R^{-1} H)\\boldsymbol{x}_{post} = B^{-1}\\boldsymbol{x}_{prior} + H^\\top R^{-1} \\boldsymbol{d}$$\nIn whitened coordinates, the cost function is written as $\\mathcal{J}(\\boldsymbol{x}) = \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{x}_{prior})^\\top B^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_{prior}) + \\frac{1}{2}(\\boldsymbol{d}' - H'\\boldsymbol{x})^\\top (\\boldsymbol{d}' - H'\\boldsymbol{x})$. The gradient is $\\nabla_{\\boldsymbol{x}} \\mathcal{J}(\\boldsymbol{x}) = B^{-1}(\\boldsymbol{x} - \\boldsymbol{x}_{prior}) - (H')^\\top(\\boldsymbol{d}' - H'\\boldsymbol{x})$. Setting this to zero gives:\n$$(B^{-1} + (H')^\\top H')\\boldsymbol{x}_{post} = B^{-1}\\boldsymbol{x}_{prior} + (H')^\\top \\boldsymbol{d}'$$\nAlthough the final solution $\\boldsymbol{x}_{post}$ is the same, the matrix and vector components of the linear system are constructed from different quantities ($H, R, \\boldsymbol{d}$ vs. $H', \\boldsymbol{d}'$).\n\n#### 2. Marginal Posterior Covariance via Schur Complement\n\nLet the state vector $\\boldsymbol{x}$ be partitioned into a target subset $\\boldsymbol{x}_a$ and its complement $\\boldsymbol{x}_b$, such that $\\boldsymbol{x} = \\begin{bmatrix} \\boldsymbol{x}_a \\\\ \\boldsymbol{x}_b \\end{bmatrix}$. The posterior information matrix $J$ is conformally partitioned:\n$$J = \\begin{bmatrix} J_{aa} & J_{ab} \\\\ J_{ba} & J_{bb} \\end{bmatrix}$$\nThe posterior covariance matrix $P = J^{-1}$ is also partitioned:\n$$P = \\begin{bmatrix} P_{aa} & P_{ab} \\\\ P_{ba} & P_{bb} \\end{bmatrix}$$\nThe block $P_{aa}$ represents the marginal posterior covariance of the target subset $\\boldsymbol{x}_a$. Using the formula for the inverse of a symmetric block matrix, $P_{aa}$ can be expressed in terms of the blocks of $J$:\n$$P_{aa} = (J_{aa} - J_{ab} J_{bb}^{-1} J_{ba})^{-1}$$\nThe matrix $S = J_{aa} - J_{ab} J_{bb}^{-1} J_{ba}$ is the Schur complement of the block $J_{bb}$ in the matrix $J$. Thus, the marginal posterior covariance of the target parameters $\\boldsymbol{x}_a$, which the problem denotes as $S_{aa}$, is the inverse of this Schur complement.\n$$S_{aa} \\equiv P_{aa} = (J_{aa} - J_{ab} J_{bb}^{-1} J_{ba})^{-1}$$\nThis marginal covariance is invariant to whether one computes in whitened or unwhitened coordinates. As established in the previous section, the posterior information matrix $J$ itself is invariant to whitening. Since $J$ is identical in both coordinate systems, its sub-blocks ($J_{aa}, J_{ab}, J_{ba}, J_{bb}$) are also identical. Consequently, any quantity derived from these blocks, including the Schur complement and its inverse ($S_{aa}$), must also be invariant.\n\n#### 3. Posterior Correlation Coefficient\n\nGiven the marginal posterior covariance matrix $S_{aa} \\in \\mathbb{R}^{2 \\times 2}$ for the target parameters $\\boldsymbol{x}_a = [x_1, x_2]^\\top$:\n$$S_{aa} = \\begin{bmatrix} (S_{aa})_{11} & (S_{aa})_{12} \\\\ (S_{aa})_{21} & (S_{aa})_{22} \\end{bmatrix}$$\nThe diagonal elements $(S_{aa})_{11}$ and $(S_{aa})_{22}$ are the marginal posterior variances of $x_1$ and $x_2$ respectively. The off-diagonal element $(S_{aa})_{12}$ (equal to $(S_{aa})_{21}$ since $S_{aa}$ is symmetric) is the marginal posterior covariance between $x_1$ and $x_2$.\n\nThe posterior correlation coefficient $\\rho_{12}$ between $x_1$ and $x_2$ is defined as the normalized covariance:\n$$\\rho_{12} = \\frac{(S_{aa})_{12}}{\\sqrt{(S_{aa})_{11} (S_{aa})_{22}}}$$\nWhen the observation error covariance $R$ is perturbed, say from $R_0$ to $R_{\\text{pert}}$, the posterior information matrix $J$ changes according to the formula $J = B^{-1} + H^\\top R^{-1} H$. This change propagates through the partitioned blocks of $J$, altering the Schur complement, the marginal posterior covariance $S_{aa}$, and ultimately the value of the posterior correlation coefficient $\\rho_{12}$. The change in correlation is computed by evaluating the expression for $\\rho_{12}$ using $R_0$ and $R_{\\text{pert}}$ and taking their difference.\n\n### Application to the Concrete Instance\n\nThe problem provides the following:\n$n=3$, $m=3$. Target subset $\\boldsymbol{x}_a = [x_1, x_2]^\\top$, complement $\\boldsymbol{x}_b = [x_3]$.\nPrior covariance $B = \\begin{bmatrix} 1.0 & 0.6 & 0.2 \\\\ 0.6 & 1.5 & 0.4 \\\\ 0.2 & 0.4 & 1.2 \\end{bmatrix}$.\nObservation operator $H = \\begin{bmatrix} 1.0 & 1.0 & 0.0 \\\\ 0.0 & 1.0 & 1.0 \\\\ 1.0 & 0.0 & 1.0 \\end{bmatrix}$.\nObservation error covariance $R = \\operatorname{diag}(\\sigma_1^2, \\sigma_2^2, \\sigma_3^2)$.\n\nThe computational procedure is as follows:\n1.  Given $R$, compute its inverse $R^{-1} = \\operatorname{diag}(1/\\sigma_1^2, 1/\\sigma_2^2, 1/\\sigma_3^2)$.\n2.  Compute the prior precision $B^{-1}$.\n3.  Compute the posterior information matrix $J = B^{-1} + H^\\top R^{-1} H$.\n4.  Partition $J$ into blocks corresponding to $\\boldsymbol{x}_a$ and $\\boldsymbol{x}_b$: $J_{aa} \\in \\mathbb{R}^{2 \\times 2}$, $J_{ab} \\in \\mathbb{R}^{2 \\times 1}$, $J_{ba} \\in \\mathbb{R}^{1 \\times 2}$, and $J_{bb} \\in \\mathbb{R}^{1 \\times 1}$.\n5.  Compute the Schur complement of $J_{bb}$: $S = J_{aa} - J_{ab} J_{bb}^{-1} J_{ba}$. Since $J_{bb}$ is a scalar, its inverse is simply its reciprocal.\n6.  Compute the marginal posterior covariance for $\\boldsymbol{x}_a$: $S_{aa} = S^{-1}$.\n7.  Compute the posterior correlation coefficient $\\rho_{12}$ from the elements of $S_{aa}$ using the formula $\\rho_{12} = (S_{aa})_{12} / \\sqrt{(S_{aa})_{11}(S_{aa})_{22}}$.\n\nThis procedure is implemented for each test case to find the baseline and perturbed correlation coefficients and their difference.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian inverse problem to calculate posterior correlations.\n    \"\"\"\n\n    # --- Given Fixed Parameters ---\n    # Prior covariance matrix B\n    B = np.array([\n        [1.0, 0.6, 0.2],\n        [0.6, 1.5, 0.4],\n        [0.2, 0.4, 1.2]\n    ])\n\n    # Observation operator H\n    H = np.array([\n        [1.0, 1.0, 0.0],\n        [0.0, 1.0, 1.0],\n        [1.0, 0.0, 1.0]\n    ])\n\n    # Pre-compute the inverse of the prior covariance matrix (prior precision)\n    try:\n        B_inv = np.linalg.inv(B)\n    except np.linalg.LinAlgError:\n        # This case should not be reached with the given B\n        print(\"Error: Prior covariance matrix B is singular.\")\n        return\n\n    # --- Test Case Definitions ---\n    test_cases = [\n        # Case A: Happy path\n        {\n            \"name\": \"Case A\",\n            \"R0_variances\": np.array([0.2**2, 0.5**2, 1.0**2]),\n            \"Rpert_variances\": np.array([0.2**2, 0.5**2, 1.0**2 + 4.0])\n        },\n        # Case B: Boundary downweighting\n        {\n            \"name\": \"Case B\",\n            \"R0_variances\": np.array([0.2**2, 0.5**2, 1.0**2]),\n            \"Rpert_variances\": np.array([0.2**2, 0.5**2, 10.0**6])\n        },\n        # Case C: Edge informative component\n        {\n            \"name\": \"Case C\",\n            \"R0_variances\": np.array([0.5**2, 0.5**2, 0.5**2]),\n            \"Rpert_variances\": np.array([0.5**2, 0.1**2, 0.5**2])\n        }\n    ]\n\n    def compute_posterior_correlation(R_variances, B_inv_matrix, H_matrix):\n        \"\"\"\n        Computes the posterior correlation rho_12 for a given R.\n\n        Args:\n            R_variances (np.ndarray): Diagonal elements (variances) of R.\n            B_inv_matrix (np.ndarray): The inverse of the prior covariance matrix B.\n            H_matrix (np.ndarray): The observation operator H.\n\n        Returns:\n            float: The posterior correlation coefficient rho_12.\n        \"\"\"\n        # 1. Observation error precision R_inv\n        # R is diagonal, so its inverse is a diagonal matrix of reciprocal variances.\n        R_inv_diag = 1.0 / R_variances\n        R_inv = np.diag(R_inv_diag)\n\n        # 2. Posterior information matrix J\n        # J = B_inv + H^T * R_inv * H\n        J = B_inv_matrix + H_matrix.T @ R_inv @ H_matrix\n\n        # 3. Partition J\n        # The state is x = [x1, x2, x3]^T.\n        # Target subset xa = [x1, x2]^T. Complement xb = [x3].\n        # Partition is after the 2nd row/col.\n        J_aa = J[0:2, 0:2]  # 2x2\n        J_ab = J[0:2, 2:3]  # 2x1\n        J_ba = J[2:3, 0:2]  # 1x2\n        J_bb = J[2:3, 2:3]  # 1x1\n\n        # 4. Compute Schur complement of J_bb in J\n        # S = J_aa - J_ab * J_bb^-1 * J_ba\n        # J_bb is 1x1, so its inverse is just the reciprocal of its element.\n        J_bb_inv_scalar = 1.0 / J_bb[0, 0]\n        schur_complement = J_aa - (J_ab * J_bb_inv_scalar) @ J_ba\n\n        # 5. Compute marginal posterior covariance S_aa\n        # S_aa is the inverse of the Schur complement.\n        try:\n            S_aa = np.linalg.inv(schur_complement)\n        except np.linalg.LinAlgError:\n            # Should not happen with well-posed problems\n            return np.nan\n\n        # 6. Compute posterior correlation coefficient rho_12\n        # rho_12 = S_aa(1,2) / sqrt(S_aa(1,1) * S_aa(2,2))\n        S_11 = S_aa[0, 0]\n        S_22 = S_aa[1, 1]\n        S_12 = S_aa[0, 1]\n        \n        if S_11 = 0 or S_22 = 0: # Variances must be positive\n             return np.nan\n             \n        rho_12 = S_12 / np.sqrt(S_11 * S_22)\n        return rho_12\n\n    results = []\n    for case in test_cases:\n        # Compute baseline correlation\n        rho_base = compute_posterior_correlation(case[\"R0_variances\"], B_inv, H)\n        \n        # Compute perturbed correlation\n        rho_pert = compute_posterior_correlation(case[\"Rpert_variances\"], B_inv, H)\n        \n        # Compute the change\n        delta_rho = rho_pert - rho_base\n        \n        # Append formatted results\n        results.extend([f\"{rho_base:.6f}\", f\"{rho_pert:.6f}\", f\"{delta_rho:.6f}\"])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3388480"}]}