{"hands_on_practices": [{"introduction": "In the world of experimental design, what does 'optimal' truly mean? This exercise explores the crucial difference between two common criteria: A-optimality, which seeks to minimize the average posterior variance of parameters, and maximizing Expected Information Gain (EIG), which aims to reduce the overall volume of posterior uncertainty. By working through a simple but powerful counterexample, you will see firsthand why these intuitive goals do not always lead to the same 'best' experiment, a fundamental insight in designing effective studies. [@problem_id:3380335]", "problem": "Consider a linear-Gaussian inverse problem in two dimensions. Let the unknown state be $x \\in \\mathbb{R}^{2}$ with a Gaussian prior $x \\sim \\mathcal{N}(0, C_{\\text{prior}})$ where $C_{\\text{prior}} = I_{2}$ is the $2 \\times 2$ identity matrix. Observations are given by the linear forward model $y = H x + \\varepsilon$ with $H = I_{2}$ and Gaussian observation noise $\\varepsilon \\sim \\mathcal{N}(0, R)$, where $R$ is a positive-definite $2 \\times 2$ covariance matrix. Consider two alternative experimental designs:\n- Design $\\mathcal{A}$: $R_{\\mathcal{A}} = \\operatorname{diag}(1, 1)$.\n- Design $\\mathcal{B}$: $R_{\\mathcal{B}} = \\operatorname{diag}(0.01, 1000)$.\n\nYou will use the Bayesian linear-Gaussian update and the definition of Kullback-Leibler divergence (KLD) to compute the posterior covariance and the Expected Information Gain (EIG). The Expected Information Gain (EIG) is defined as the mutual information between $x$ and $y$ under the design, equivalently the expected KLD from the prior to the posterior over the marginal distribution of $y$ induced by the design.\n\nTasks:\n1. Derive the posterior covariance $C_{\\text{post}}$ for each design $\\mathcal{A}$ and $\\mathcal{B}$ from first principles of linear-Gaussian Bayesian inference.\n2. Compute $\\operatorname{tr}(C_{\\text{post}})$ for each design and determine which design minimizes $\\operatorname{tr}(C_{\\text{post}})$, that is, which design is A-optimal.\n3. Derive the EIG for each design from the definition of mutual information for jointly Gaussian variables, expressed in terms of the relevant model matrices and covariances.\n4. Use these results to show explicitly that the design which minimizes $\\operatorname{tr}(C_{\\text{post}})$ does not maximize EIG in this setting.\n5. Compute the quantity $\\Delta = \\mathrm{EIG}_{\\mathcal{B}} - \\mathrm{EIG}_{\\mathcal{A}}$ and express your final answer as a single closed-form analytic expression in natural logarithms. Do not round. The EIG is dimensionless (in nats), so no units are required in the final numerical expression.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard problem in Bayesian optimal experimental design that is free of flaws and can be solved using established principles. We may proceed with the solution.\n\nThe problem asks for an analysis of two experimental designs, $\\mathcal{A}$ and $\\mathcal{B}$, for a linear-Gaussian inverse problem. The state is $x \\in \\mathbb{R}^{2}$, with prior $x \\sim \\mathcal{N}(0, C_{\\text{prior}})$ where $C_{\\text{prior}} = I_2$. The observation model is $y = Hx + \\varepsilon$, with $H=I_2$ and noise $\\varepsilon \\sim \\mathcal{N}(0, R)$. The two designs correspond to different noise covariance matrices: $R_{\\mathcal{A}} = \\operatorname{diag}(1, 1)$ and $R_{\\mathcal{B}} = \\operatorname{diag}(0.01, 1000)$.\n\n**1. Derivation of Posterior Covariance**\n\nIn a linear-Gaussian Bayesian framework, the posterior distribution $p(x|y)$ is also Gaussian. Its covariance matrix, $C_{\\text{post}}$, is given by the inverse of the sum of the prior and data precision matrices. The precision matrix is the inverse of the covariance matrix.\nThe posterior precision matrix $C_{\\text{post}}^{-1}$ is:\n$$C_{\\text{post}}^{-1} = C_{\\text{prior}}^{-1} + H^T R^{-1} H$$\nThe posterior covariance is therefore:\n$$C_{\\text{post}} = (C_{\\text{prior}}^{-1} + H^T R^{-1} H)^{-1}$$\nGiven the problem specifics, $C_{\\text{prior}} = I_2$ and $H = I_2$, the expression simplifies:\n$$C_{\\text{post}} = (I_2^{-1} + I_2^T R^{-1} I_2)^{-1} = (I_2 + R^{-1})^{-1}$$\nWe now apply this to each design.\n\nFor Design $\\mathcal{A}$:\n$R_{\\mathcal{A}} = \\operatorname{diag}(1, 1) = I_2$. The inverse is $R_{\\mathcal{A}}^{-1} = I_2^{-1} = I_2$.\n$$C_{\\text{post}, \\mathcal{A}} = (I_2 + I_2)^{-1} = (2I_2)^{-1} = \\frac{1}{2}I_2 = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix}$$\n\nFor Design $\\mathcal{B}$:\n$R_{\\mathcal{B}} = \\operatorname{diag}(0.01, 1000) = \\operatorname{diag}(10^{-2}, 10^3)$. The inverse is $R_{\\mathcal{B}}^{-1} = \\operatorname{diag}((10^{-2})^{-1}, (10^3)^{-1}) = \\operatorname{diag}(100, 10^{-3})$.\n$$C_{\\text{post}, \\mathcal{B}}^{-1} = I_2 + R_{\\mathcal{B}}^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 100 & 0 \\\\ 0 & 0.001 \\end{pmatrix} = \\begin{pmatrix} 101 & 0 \\\\ 0 & 1.001 \\end{pmatrix}$$\nInverting this diagonal matrix gives the posterior covariance:\n$$C_{\\text{post}, \\mathcal{B}} = \\begin{pmatrix} \\frac{1}{101} & 0 \\\\ 0 & \\frac{1}{1.001} \\end{pmatrix}$$\n\n**2. A-Optimality Analysis**\n\nA-optimality aims to minimize the trace of the posterior covariance matrix, $\\operatorname{tr}(C_{\\text{post}})$, which represents the sum of the posterior variances of the state components.\n\nFor Design $\\mathcal{A}$:\n$$\\operatorname{tr}(C_{\\text{post}, \\mathcal{A}}) = \\operatorname{tr}\\left(\\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix}\\right) = \\frac{1}{2} + \\frac{1}{2} = 1$$\n\nFor Design $\\mathcal{B}$:\n$$\\operatorname{tr}(C_{\\text{post}, \\mathcal{B}}) = \\operatorname{tr}\\left(\\begin{pmatrix} \\frac{1}{101} & 0 \\\\ 0 & \\frac{1}{1.001} \\end{pmatrix}\\right) = \\frac{1}{101} + \\frac{1}{1.001}$$\nTo compare this with $1$, we compute the sum:\n$$\\frac{1}{101} + \\frac{1}{1.001} = \\frac{1}{101} + \\frac{1}{1001/1000} = \\frac{1}{101} + \\frac{1000}{1001} = \\frac{1001 + 101 \\times 1000}{101 \\times 1001} = \\frac{1001 + 101000}{101101} = \\frac{102001}{101101}$$\nSince $102001 > 101101$, we have $\\frac{102001}{101101} > 1$.\nTherefore, $\\operatorname{tr}(C_{\\text{post}, \\mathcal{B}}) > \\operatorname{tr}(C_{\\text{post}, \\mathcal{A}})$. Design $\\mathcal{A}$ minimizes the trace and is thus A-optimal.\n\n**3. Derivation of Expected Information Gain (EIG)**\n\nThe Expected Information Gain (EIG) is the mutual information $I(x; y)$. For jointly Gaussian variables, it is the reduction in entropy from the prior to the posterior:\n$$\\text{EIG} = H(x) - H(x|y)$$\nThe differential entropy of a multivariate Gaussian distribution $\\mathcal{N}(\\mu, \\Sigma)$ is $H = \\frac{1}{2}\\ln \\det(2\\pi e \\Sigma)$.\nSo,\n$$\\text{EIG} = \\frac{1}{2}\\ln \\det(2\\pi e C_{\\text{prior}}) - \\frac{1}{2}\\ln \\det(2\\pi e C_{\\text{post}})$$\n$$\\text{EIG} = \\frac{1}{2}\\left( \\ln \\det(C_{\\text{prior}}) - \\ln \\det(C_{\\text{post}}) \\right) = \\frac{1}{2}\\ln\\left(\\frac{\\det(C_{\\text{prior}})}{\\det(C_{\\text{post}})}\\right)$$\nUsing $\\det(C_{\\text{post}}^{-1}) = \\det(C_{\\text{prior}}^{-1} + H^T R^{-1} H)$, we can write:\n$$\\text{EIG} = \\frac{1}{2}\\ln\\left(\\det(C_{\\text{prior}}) \\det(C_{\\text{post}}^{-1})\\right) = \\frac{1}{2}\\ln\\left(\\det(C_{\\text{prior}}(C_{\\text{prior}}^{-1} + H^T R^{-1} H))\\right)$$\n$$\\text{EIG} = \\frac{1}{2}\\ln \\det(I + C_{\\text{prior}} H^T R^{-1} H)$$\nWith $C_{\\text{prior}} = I_2$ and $H = I_2$, this simplifies to:\n$$\\text{EIG} = \\frac{1}{2}\\ln \\det(I_2 + R^{-1})$$\n\nFor Design $\\mathcal{A}$:\n$I_2 + R_{\\mathcal{A}}^{-1} = I_2 + I_2 = 2I_2$.\n$$\\text{EIG}_{\\mathcal{A}} = \\frac{1}{2}\\ln \\det(2I_2) = \\frac{1}{2}\\ln(4) = \\ln(2)$$\n\nFor Design $\\mathcal{B}$:\n$I_2 + R_{\\mathcal{B}}^{-1} = \\begin{pmatrix} 101 & 0 \\\\ 0 & 1.001 \\end{pmatrix}$.\n$$\\det(I_2 + R_{\\mathcal{B}}^{-1}) = 101 \\times 1.001 = 101.101$$\n$$\\text{EIG}_{\\mathcal{B}} = \\frac{1}{2}\\ln(101.101)$$\n\n**4. Conflict Between A-optimality and EIG Maximization**\n\nWe have established that Design $\\mathcal{A}$ is A-optimal, as $\\operatorname{tr}(C_{\\text{post}, \\mathcal{A}}) < \\operatorname{tr}(C_{\\text{post}, \\mathcal{B}})$. To see if this design also maximizes EIG, we compare $\\text{EIG}_{\\mathcal{A}}$ and $\\text{EIG}_{\\mathcal{B}}$.\nWe need to compare $\\ln(2)$ with $\\frac{1}{2}\\ln(101.101)$.\nThis is equivalent to comparing $2\\ln(2) = \\ln(2^2) = \\ln(4)$ with $\\ln(101.101)$.\nSince $\\ln(x)$ is a strictly increasing function for $x > 0$, and $101.101 > 4$, it follows that:\n$$\\ln(101.101) > \\ln(4)$$\n$$\\frac{1}{2}\\ln(101.101) > \\ln(2)$$\n$$\\text{EIG}_{\\mathcal{B}} > \\text{EIG}_{\\mathcal{A}}$$\nThis shows explicitly that Design $\\mathcal{A}$, which minimizes $\\operatorname{tr}(C_{\\text{post}})$, does not maximize the EIG. Design $\\mathcal{B}$ yields a greater information gain. This is a classic result demonstrating that different optimality criteria can lead to conflicting design choices. A-optimality focuses on the average variance, while EIG (related to D-optimality, which minimizes $\\det(C_{\\text{post}})$) focuses on the overall volume of the posterior uncertainty ellipsoid.\n\n**5. Calculation of $\\Delta = \\mathrm{EIG}_{\\mathcal{B}} - \\mathrm{EIG}_{\\mathcal{A}}$**\n\nWe are asked to compute the difference in EIG between the two designs.\n$$\\Delta = \\mathrm{EIG}_{\\mathcal{B}} - \\mathrm{EIG}_{\\mathcal{A}} = \\frac{1}{2}\\ln(101.101) - \\ln(2)$$\nTo express this as a single closed-form expression, we use the properties of logarithms:\n$$\\Delta = \\frac{1}{2}\\ln(101.101) - \\frac{1}{2}\\ln(4) = \\frac{1}{2}\\left(\\ln(101.101) - \\ln(4)\\right) = \\frac{1}{2}\\ln\\left(\\frac{101.101}{4}\\right)$$\nFor an exact analytical expression, we write the decimal as a fraction. $101.101 = \\frac{101101}{1000}$.\n$$\\Delta = \\frac{1}{2}\\ln\\left(\\frac{101101/1000}{4}\\right) = \\frac{1}{2}\\ln\\left(\\frac{101101}{4000}\\right)$$\nThis is the final analytical expression for the difference in expected information gain.", "answer": "$$\\boxed{\\frac{1}{2}\\ln\\left(\\frac{101101}{4000}\\right)}$$", "id": "3380335"}, {"introduction": "An experiment can be designed for general discovery or for answering a very specific question. This practice delves into this distinction by contrasting a design that maximizes information about an entire parameter vector versus one tailored to a specific Quantity of Interest (QoI). You will discover that the optimal measurement strategy changes depending on your inferential goal, revealing the trade-off between a general-purpose design and a targeted, high-precision one. [@problem_id:3380354]", "problem": "Consider a linear-Gaussian Bayesian inverse problem and design-of-experiments setting in which the unknown parameter is the vector $\\theta \\in \\mathbb{R}^{2}$ with Gaussian prior $\\theta \\sim \\mathcal{N}(0, \\Sigma_{\\theta})$, where\n$$\n\\Sigma_{\\theta} \\;=\\;\n\\begin{pmatrix}\n4 & 3 \\\\\n3 & 4\n\\end{pmatrix}.\n$$\nA single scalar observation is taken using a design parameter $d \\in [0, 2\\pi)$ via the measurement model\n$$\nY \\,\\big|\\, \\theta, d \\;\\sim\\; \\mathcal{N}\\!\\big(h(d)^{\\top}\\theta,\\, \\sigma^{2}\\big), \\quad \\text{with} \\quad h(d) \\;=\\; \\begin{pmatrix}\\cos d \\\\ \\sin d\\end{pmatrix}, \\quad \\sigma^{2} \\;=\\; 1.\n$$\nThe quantity of interest is the linear functional $\\psi = a^{\\top}\\theta$ with $a = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$.\n\nUsing only fundamental definitions from information theory and Gaussian probability (in particular, the definition of mutual information $I(X;Y\\,|\\,d) = H(X) - H(X\\,|\\,Y,d)$ and the entropy of a multivariate Gaussian), do the following:\n\n1. Derive, as functions of $d$, the expected information gains $I(\\theta; Y \\mid d)$ and $I(\\psi; Y \\mid d)$.\n2. For each information objective, determine the optimal design $d_{\\theta}^{\\star}$ that maximizes $I(\\theta; Y \\mid d)$ and the optimal design $d_{\\psi}^{\\star}$ that maximizes $I(\\psi; Y \\mid d)$, under the unit-norm constraint implied by $h(d)$.\n3. Prove that $d_{\\theta}^{\\star} \\neq d_{\\psi}^{\\star}$ for the given $\\Sigma_{\\theta}$ and $\\sigma^{2}$.\n4. Explain the structural reason, grounded in the definitions and the geometry of Gaussian inference, why these two optimal designs differ in general when $\\psi$ is a non-invertible function of $\\theta$.\n\nFinally, compute the acute angle (in radians) between the two unit-norm optimal measurement directions $h(d_{\\theta}^{\\star})$ and $h(d_{\\psi}^{\\star})$, and give your answer as a single exact analytical expression. Express the angle in radians. Do not round your answer.", "solution": "The problem is well-posed and scientifically grounded within the framework of Bayesian inverse problems and optimal experimental design. All necessary parameters and definitions are provided. We can proceed with the solution.\n\nThe solution is structured according to the four tasks specified in the problem statement, culminating in the calculation of the angle between the two optimal measurement directions.\n\n### 1. Derivation of the Expected Information Gains\n\nThe mutual information between two random variables $X$ and $Y$ is given by $I(X; Y) = H(X) - H(X|Y)$, where $H(\\cdot)$ denotes the differential entropy. For Gaussian distributions, an equivalent and often more convenient expression is $I(X; Y) = H(Y) - H(Y|X)$. We use a design parameter $d$, so all quantities are conditioned on $d$. The entropy of a $k$-dimensional Gaussian random variable $Z \\sim \\mathcal{N}(\\mu, \\Sigma)$ is $H(Z) = \\frac{1}{2}\\ln(\\det(2\\pi e \\Sigma))$.\n\n**Derivation of $I(\\theta; Y \\mid d)$**\n\nWe compute $I(\\theta; Y \\mid d) = H(Y \\mid d) - H(Y \\mid \\theta, d)$.\n\nFirst, we consider the entropy of the observation $Y$ conditioned on the parameter $\\theta$ and design $d$. The problem states $Y \\mid \\theta, d \\sim \\mathcal{N}(h(d)^\\top \\theta, \\sigma^2)$. Since this is a scalar Gaussian with variance $\\sigma^2 = 1$, its entropy is:\n$$\nH(Y \\mid \\theta, d) = \\frac{1}{2}\\ln(2\\pi e \\sigma^2) = \\frac{1}{2}\\ln(2\\pi e)\n$$\n\nNext, we find the marginal distribution of $Y$ conditioned only on $d$. Since $\\theta \\sim \\mathcal{N}(0, \\Sigma_\\theta)$ and the observation model is linear in $\\theta$, $Y \\mid d$ is also Gaussian.\nIts mean is $\\mathbb{E}[Y \\mid d] = \\mathbb{E}[h(d)^\\top \\theta] = h(d)^\\top \\mathbb{E}[\\theta] = 0$.\nIts variance is $\\text{Var}(Y \\mid d) = \\text{Var}(h(d)^\\top \\theta + \\epsilon) = \\text{Var}(h(d)^\\top \\theta) + \\text{Var}(\\epsilon)$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ is the observation noise.\n$$\n\\text{Var}(Y \\mid d) = h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2\n$$\nSo, $Y \\mid d \\sim \\mathcal{N}(0, h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2)$. The entropy is:\n$$\nH(Y \\mid d) = \\frac{1}{2}\\ln(2\\pi e (\\text{Var}(Y \\mid d))) = \\frac{1}{2}\\ln(2\\pi e (h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2))\n$$\nThe expected information gain is the difference between these entropies:\n$$\nI(\\theta; Y \\mid d) = H(Y \\mid d) - H(Y \\mid \\theta, d) = \\frac{1}{2}\\ln\\left(\\frac{2\\pi e (h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2)}{2\\pi e \\sigma^2}\\right) = \\frac{1}{2}\\ln\\left(1 + \\frac{h(d)^\\top \\Sigma_\\theta h(d)}{\\sigma^2}\\right)\n$$\nWith $\\sigma^2=1$, this simplifies to $I(\\theta; Y \\mid d) = \\frac{1}{2}\\ln(1 + h(d)^\\top \\Sigma_\\theta h(d))$.\n\n**Derivation of $I(\\psi; Y \\mid d)$**\n\nWe compute $I(\\psi; Y \\mid d) = H(\\psi) - H(\\psi \\mid Y, d)$, where $\\psi = a^\\top \\theta$. Maximizing this is equivalent to minimizing the posterior entropy $H(\\psi \\mid Y, d)$.\nThe prior variance of $\\psi$ is $\\text{Var}(\\psi) = \\text{Var}(a^\\top\\theta) = a^\\top \\Sigma_\\theta a$.\nThe posterior covariance matrix for $\\theta$ after observing $Y$ is given by the Bayesian update formula for linear-Gaussian models:\n$$\n\\Sigma_{\\theta|Y,d} = \\left(\\Sigma_\\theta^{-1} + \\frac{h(d)h(d)^\\top}{\\sigma^2}\\right)^{-1}\n$$\nUsing the Sherman-Morrison-Woodbury formula, we get:\n$$\n\\Sigma_{\\theta|Y,d} = \\Sigma_\\theta - \\frac{\\Sigma_\\theta h(d) h(d)^\\top \\Sigma_\\theta}{h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2}\n$$\nThe posterior distribution of $\\psi = a^\\top \\theta$ is Gaussian, $\\psi \\mid Y,d \\sim \\mathcal{N}(a^\\top \\mu_{\\theta|Y,d}, a^\\top \\Sigma_{\\theta|Y,d} a)$, where $\\mu_{\\theta|Y,d}$ is the posterior mean. The posterior variance of $\\psi$, $\\text{Var}(\\psi \\mid Y, d) = a^\\top \\Sigma_{\\theta|Y,d} a$, does not depend on the measurement value $Y$.\n$$\n\\text{Var}(\\psi \\mid Y, d) = a^\\top \\left(\\Sigma_\\theta - \\frac{\\Sigma_\\theta h(d) h(d)^\\top \\Sigma_\\theta}{h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2}\\right) a = a^\\top\\Sigma_\\theta a - \\frac{(a^\\top \\Sigma_\\theta h(d))^2}{h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2}\n$$\nThe information gain is the reduction in entropy from prior to posterior for $\\psi$:\n$$\nI(\\psi; Y \\mid d) = H(\\psi) - H(\\psi \\mid Y, d) = \\frac{1}{2}\\ln(2\\pi e \\text{Var}(\\psi)) - \\frac{1}{2}\\ln(2\\pi e \\text{Var}(\\psi \\mid Y, d))\n$$\n$$\nI(\\psi; Y \\mid d) = \\frac{1}{2}\\ln\\left(\\frac{\\text{Var}(\\psi)}{\\text{Var}(\\psi \\mid Y, d)}\\right) = \\frac{1}{2}\\ln\\left(\\frac{a^\\top\\Sigma_\\theta a}{a^\\top\\Sigma_\\theta a - \\frac{(a^\\top \\Sigma_\\theta h(d))^2}{h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2}}\\right)\n$$\nThis can be rewritten as:\n$$\nI(\\psi; Y \\mid d) = -\\frac{1}{2}\\ln\\left(1 - \\frac{(a^\\top \\Sigma_\\theta h(d))^2}{(a^\\top \\Sigma_\\theta a) (h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2)}\\right)\n$$\n\n### 2. Determination of Optimal Designs\n\n**Optimal Design $d_{\\theta}^{\\star}$**\n\nTo maximize $I(\\theta; Y \\mid d) = \\frac{1}{2}\\ln(1 + h(d)^\\top \\Sigma_\\theta h(d))$, we must maximize the quadratic form $h(d)^\\top \\Sigma_\\theta h(d)$ subject to the constraint that $h(d)$ is a unit vector, i.e., $h(d)^\\top h(d) = 1$. This is a Rayleigh quotient maximization problem. The maximum value is the largest eigenvalue of $\\Sigma_\\theta$, and the optimal vector $h(d)$ is the corresponding eigenvector.\n\nThe prior covariance is $\\Sigma_\\theta = \\begin{pmatrix} 4 & 3 \\\\ 3 & 4 \\end{pmatrix}$. Its characteristic equation is $\\det(\\Sigma_\\theta - \\lambda I) = (4-\\lambda)^2 - 9 = 0$, which yields eigenvalues $\\lambda_1 = 7$ and $\\lambda_2 = 1$. The largest eigenvalue is $\\lambda_{\\max} = 7$.\nThe eigenvector for $\\lambda_{\\max}=7$ is found by solving $(\\Sigma_\\theta - 7I)v = 0$:\n$$\n\\begin{pmatrix} -3 & 3 \\\\ 3 & -3 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies v_1 = v_2\n$$\nThe normalized eigenvector is $\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. We set $h(d_{\\theta}^{\\star}) = \\begin{pmatrix} \\cos d_{\\theta}^{\\star} \\\\ \\sin d_{\\theta}^{\\star} \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. This gives $\\cos d_{\\theta}^{\\star} = \\sin d_{\\theta}^{\\star} = 1/\\sqrt{2}$, so an optimal design is $d_{\\theta}^{\\star} = \\pi/4$.\n\n**Optimal Design $d_{\\psi}^{\\star}$**\n\nTo maximize $I(\\psi; Y \\mid d)$, we must maximize the argument of the logarithm, which is equivalent to minimizing the posterior variance $\\text{Var}(\\psi \\mid Y, d)$. This in turn is equivalent to maximizing the term that is subtracted in the expression for posterior variance: $\\frac{(a^\\top \\Sigma_\\theta h(d))^2}{h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2}$. Let $v = \\Sigma_\\theta a$. The objective is to maximize $\\frac{(v^\\top h(d))^2}{h(d)^\\top(\\Sigma_\\theta + \\sigma^2 I)h(d)}$ over unit vectors $h(d)$. This is a generalized eigenvalue problem, and the optimal direction $h(d)$ is proportional to $(\\Sigma_\\theta + \\sigma^2 I)^{-1}v = (\\Sigma_\\theta + I)^{-1}(\\Sigma_\\theta a)$.\n\nWith $a = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $v = \\Sigma_\\theta a = \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix}$.\nLet $M = \\Sigma_\\theta + I = \\begin{pmatrix} 5 & 3 \\\\ 3 & 5 \\end{pmatrix}$. The determinant is $\\det(M) = 25 - 9 = 16$.\nThe inverse is $M^{-1} = \\frac{1}{16}\\begin{pmatrix} 5 & -3 \\\\ -3 & 5 \\end{pmatrix}$.\nThe optimal direction is proportional to:\n$$\nM^{-1} v = \\frac{1}{16}\\begin{pmatrix} 5 & -3 \\\\ -3 & 5 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix} = \\frac{1}{16}\\begin{pmatrix} 20 - 9 \\\\ -12 + 15 \\end{pmatrix} = \\frac{1}{16}\\begin{pmatrix} 11 \\\\ 3 \\end{pmatrix}\n$$\nNormalizing this vector gives the optimal measurement direction:\n$$\nh(d_{\\psi}^{\\star}) = \\frac{1}{\\sqrt{11^2 + 3^2}}\\begin{pmatrix} 11 \\\\ 3 \\end{pmatrix} = \\frac{1}{\\sqrt{130}}\\begin{pmatrix} 11 \\\\ 3 \\end{pmatrix}\n$$\nThis corresponds to an angle $d_{\\psi}^{\\star} = \\arctan(3/11)$.\n\n### 3. Proof of Unequal Optimal Designs\n\nThe optimal design for the total information is $d_{\\theta}^{\\star} = \\pi/4$.\nThe optimal design for the QoI-specific information is $d_{\\psi}^{\\star} = \\arctan(3/11)$.\nSince $\\tan(d_{\\theta}^{\\star}) = \\tan(\\pi/4) = 1$ and $\\tan(d_{\\psi}^{\\star}) = 3/11$, and $1 \\neq 3/11$, it is proven that $d_{\\theta}^{\\star} \\neq d_{\\psi}^{\\star}$.\n\n### 4. Structural Reason for the Difference\n\nThe two objectives, maximizing $I(\\theta; Y \\mid d)$ and maximizing $I(\\psi; Y \\mid d)$, are fundamentally different goals.\n- Maximizing $I(\\theta; Y \\mid d)$ aims to reduce the overall uncertainty of the parameter vector $\\theta$. This is achieved by making a measurement in the direction where the prior uncertainty is largest. The geometry of the prior uncertainty is described by the covariance matrix $\\Sigma_\\theta$. The direction of maximum variance is the principal eigenvector of $\\Sigma_\\theta$. Thus, $h(d_\\theta^\\star)$ aligns with this eigenvector. This is an untargeted, or \"parameter-focused,\" design criterion.\n- Maximizing $I(\\psi; Y \\mid d)$ aims to reduce the uncertainty of a specific quantity of interest $\\psi=a^\\top\\theta$, which is a non-invertible linear projection of $\\theta$. The optimal design must balance two competing factors. Maximizing the information gain for $\\psi$ is equivalent to maximizing the term $\\frac{(\\text{Cov}(\\psi, Y))^2}{\\text{Var}(Y)}$, where both covariance and variance are functions of the design $d$.\n  1. The numerator, $(\\text{Cov}(\\psi, Y))^2 = (a^\\top \\Sigma_\\theta h(d))^2$, encourages aligning the measurement direction $h(d)$ with the vector $\\Sigma_\\theta a$. This vector represents the covariance between the QoI and the components of $\\theta$, effectively mapping how a change in $\\theta$ impacts $\\psi$. A measurement correlated with this direction is most informative about $\\psi$.\n  2. The denominator, $\\text{Var}(Y) = h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2$, is the prior predictive variance of the observation. This term penalizes making measurements in directions of high prior variance (large $h(d)^\\top \\Sigma_\\theta h(d)$), as such measurements are inherently more \"noisy\" due to the uncertainty in $\\theta$ itself.\nThe optimal design $h(d_\\psi^\\star)$ thus represents a compromise: it is pulled away from the direction of maximal prior variance (chosen by $d_\\theta^\\star$) towards a direction that offers a better signal-to-noise ratio specifically for estimating $\\psi$. These two directions coincide only in special cases, for instance, if the QoI vector $a$ happens to be an eigenvector of $\\Sigma_\\theta$. In this problem, $a=(1,0)^\\top$ is not an eigenvector, so the optimal designs differ.\n\n### Final Calculation: Acute Angle Between Optimal Directions\n\nWe need to find the acute angle $\\phi$ between the two optimal unit-norm measurement vectors:\n$$\nh(d_{\\theta}^{\\star}) = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\quad \\text{and} \\quad h(d_{\\psi}^{\\star}) = \\frac{1}{\\sqrt{130}}\\begin{pmatrix} 11 \\\\ 3 \\end{pmatrix}\n$$\nThe cosine of the angle between two unit vectors is their dot product:\n$$\n\\cos \\phi = h(d_{\\theta}^{\\star}) \\cdot h(d_{\\psi}^{\\star}) = \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\right) \\cdot \\left(\\frac{1}{\\sqrt{130}}\\begin{pmatrix} 11 \\\\ 3 \\end{pmatrix}\\right)\n$$\n$$\n\\cos \\phi = \\frac{1}{\\sqrt{260}} (1 \\cdot 11 + 1 \\cdot 3) = \\frac{14}{\\sqrt{260}} = \\frac{14}{\\sqrt{4 \\cdot 65}} = \\frac{14}{2\\sqrt{65}} = \\frac{7}{\\sqrt{65}}\n$$\nSince $\\cos \\phi > 0$, the angle is acute. The angle is:\n$$\n\\phi = \\arccos\\left(\\frac{7}{\\sqrt{65}}\\right)\n$$\nThis is the required exact analytical expression.", "answer": "$$\n\\boxed{\\arccos\\left(\\frac{7}{\\sqrt{65}}\\right)}\n$$", "id": "3380354"}, {"introduction": "Often, our uncertainty lies not just in the model parameters, but in the statistical assumptions we make, such as the noise level of our instruments. This exercise shifts our focus to these hyperparameters by calculating the EIG for the observation noise variance itself within a classic conjugate Bayesian model. This practice demonstrates the power of EIG in designing calibration experiments, a vital step for robustly quantifying uncertainty and ensuring the reliability of subsequent inferences. [@problem_id:3380369]", "problem": "Consider a calibration experiment in a data assimilation workflow where the only objective is to learn the unknown observation noise variance (hyperparameter) $\\sigma^{2}$. You collect $r$ independent calibration residuals $\\{y_{i}\\}_{i=1}^{r}$ with a known zero-mean signal, modeled as $y_{i} \\mid \\sigma^{2} \\sim \\mathcal{N}(0, \\sigma^{2})$ for $i=1,\\dots,r$. Assume a conjugate prior for the variance, $\\sigma^{2} \\sim \\mathrm{Inverse\\text{-}Gamma}(a, b)$ with shape $a>0$ and scale $b>0$, where the probability density function is $p(\\sigma^{2}) = \\frac{b^{a}}{\\Gamma(a)}(\\sigma^{2})^{-(a+1)} \\exp\\!\\big(-\\frac{b}{\\sigma^{2}}\\big)$.\n\nAdopt the definition of Expected Information Gain (EIG) as the mutual information between the data and the parameter of interest, which is equivalently the expected Kullback–Leibler divergence (KLD) from the posterior to the prior. That is, for the data vector $Y = (y_{1},\\dots,y_{r})$, $I(Y;\\sigma^{2}) = \\mathbb{E}_{Y}\\!\\left[ \\mathrm{KLD}\\big(p(\\sigma^{2}\\mid Y)\\,\\|\\,p(\\sigma^{2})\\big) \\right]$, where $\\mathrm{KLD}$ denotes the Kullback–Leibler divergence.\n\nStarting from fundamental definitions and conjugacy, derive a closed-form analytic expression for the EIG $I(Y;\\sigma^{2})$ as a function of $a$ and $r$. The final answer must be a single analytic expression. Then, justify—based on first principles of inverse problems and data assimilation—under what conditions it is beneficial to design experiments to learn $\\sigma^{2}$ rather than the forward model parameter $\\theta$ (assume $\\theta$ appears only in non-calibration experiments and is endowed with a proper prior). No numerical approximation is required for the EIG; your final answer must be exact. If you introduce any acronyms, define them at first use.", "solution": "The problem statement is first validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Objective**: Learn the unknown observation noise variance, $\\sigma^{2}$.\n- **Data**: $r$ independent calibration residuals, $\\{y_{i}\\}_{i=1}^{r}$.\n- **Data Model (Likelihood)**: $y_{i} \\mid \\sigma^{2} \\sim \\mathcal{N}(0, \\sigma^{2})$ for $i=1,\\dots,r$. The data vector is $Y = (y_{1},\\dots,y_{r})$.\n- **Prior Distribution**: $\\sigma^{2} \\sim \\mathrm{Inverse\\text{-}Gamma}(a, b)$ with shape $a>0$ and scale $b>0$.\n- **Prior Probability Density Function (PDF)**: $p(\\sigma^{2}) = \\frac{b^{a}}{\\Gamma(a)}(\\sigma^{2})^{-(a+1)} \\exp(-\\frac{b}{\\sigma^{2}})$.\n- **Definition of Expected Information Gain (EIG)**: $I(Y;\\sigma^{2}) = \\mathbb{E}_{Y}\\!\\left[ \\mathrm{KLD}\\big(p(\\sigma^{2}\\mid Y)\\,\\|\\,p(\\sigma^{2})\\big) \\right]$, where KLD is the Kullback–Leibler divergence.\n- **Task 1**: Derive a closed-form analytic expression for $I(Y;\\sigma^{2})$ as a function of $a$ and $r$.\n- **Task 2**: Justify when it is beneficial to design experiments to learn $\\sigma^{2}$ rather than a forward model parameter $\\theta$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem is based on standard, well-established principles of Bayesian inference. The use of a Normal likelihood for residuals and a conjugate Inverse-Gamma prior for the variance is a textbook example in statistics and machine learning. The definition of EIG as the mutual information between data and parameters is also a fundamental concept in information theory and Bayesian experimental design.\n- **Well-Posed**: The problem is clearly stated and provides all necessary information to derive the required expression. The use of a conjugate prior ensures that the posterior distribution is analytically tractable, which allows for a closed-form derivation of the EIG. The question is unambiguous and a unique solution exists.\n- **Objective**: The problem is stated in precise, objective mathematical language, free from subjective or biased terminology.\n\nThe problem does not exhibit any of the flaws listed in the instructions (e.g., scientific unsoundness, incompleteness, ambiguity). It is a standard, non-trivial problem in Bayesian statistics.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe solution is derived in two parts as requested.\n\n**Part 1: Derivation of the Expected Information Gain (EIG)**\n\nThe Expected Information Gain (EIG) is the mutual information between the data $Y$ and the parameter $\\sigma^2$, which can be expressed in two equivalent ways:\n$$I(Y; \\sigma^{2}) = H(Y) - \\mathbb{E}_{\\sigma^{2}}[H(Y \\mid \\sigma^{2})]$$\n$$I(Y; \\sigma^{2}) = H(\\sigma^{2}) - \\mathbb{E}_{Y}[H(\\sigma^{2} \\mid Y)]$$\nwhere $H(\\cdot)$ denotes the differential entropy. The first form, involving the entropy of the data, is more direct for this problem. We will compute each term separately.\n\n1.  **Conditional Entropy of Data**: $H(Y \\mid \\sigma^{2})$\n    Given $\\sigma^{2}$, the data $Y = (y_1, \\dots, y_r)$ consists of $r$ independent and identically distributed random variables, with each $y_i \\sim \\mathcal{N}(0, \\sigma^2)$. Therefore, the vector $Y$ follows a multivariate normal distribution, $Y \\mid \\sigma^{2} \\sim \\mathcal{N}(0, \\sigma^{2}I_{r})$, where $I_{r}$ is the $r \\times r$ identity matrix.\n    The differential entropy of a $k$-dimensional multivariate normal distribution $\\mathcal{N}(\\mu, \\Sigma)$ is $\\frac{1}{2}\\ln((2\\pi e)^{k}|\\det(\\Sigma)|)$.\n    For $Y \\mid \\sigma^{2}$, we have dimension $k=r$ and covariance matrix $\\Sigma = \\sigma^2 I_r$, so $\\det(\\Sigma) = (\\sigma^2)^r$.\n    The conditional entropy is:\n    $$H(Y \\mid \\sigma^{2}) = \\frac{1}{2}\\ln\\left((2\\pi e)^{r}(\\sigma^{2})^{r}\\right) = \\frac{r}{2}\\ln(2\\pi e) + \\frac{r}{2}\\ln(\\sigma^{2})$$\n\n2.  **Expected Conditional Entropy**: $\\mathbb{E}_{\\sigma^{2}}[H(Y \\mid \\sigma^{2})]$\n    We take the expectation of $H(Y \\mid \\sigma^{2})$ with respect to the prior distribution of $\\sigma^2$, which is $\\sigma^{2} \\sim \\mathrm{Inverse\\text{-}Gamma}(a, b)$.\n    $$\\mathbb{E}_{\\sigma^{2}}[H(Y \\mid \\sigma^{2})] = \\mathbb{E}_{\\sigma^{2}}\\left[\\frac{r}{2}\\ln(2\\pi e) + \\frac{r}{2}\\ln(\\sigma^{2})\\right] = \\frac{r}{2}\\ln(2\\pi e) + \\frac{r}{2}\\mathbb{E}_{\\sigma^{2}}[\\ln(\\sigma^{2})]$$\n    For a random variable $X \\sim \\mathrm{Inverse\\text{-}Gamma}(\\alpha, \\beta)$, the expectation of its logarithm is $\\mathbb{E}[\\ln(X)] = \\ln(\\beta) - \\psi(\\alpha)$, where $\\psi(\\alpha) = \\frac{d}{d\\alpha}\\ln\\Gamma(\\alpha)$ is the digamma function.\n    For our prior with $\\alpha=a$ and $\\beta=b$, we have $\\mathbb{E}_{\\sigma^{2}}[\\ln(\\sigma^{2})] = \\ln(b) - \\psi(a)$.\n    Substituting this into the expression for the expected conditional entropy gives:\n    $$\\mathbb{E}_{\\sigma^{2}}[H(Y \\mid \\sigma^{2})] = \\frac{r}{2}\\ln(2\\pi e) + \\frac{r}{2}(\\ln(b) - \\psi(a)) = \\frac{r}{2}(\\ln(2\\pi) + 1) + \\frac{r}{2}\\ln(b) - \\frac{r}{2}\\psi(a)$$\n\n3.  **Marginal Entropy of Data**: $H(Y)$\n    To find $H(Y)$, we first need the marginal distribution of $Y$, obtained by integrating out $\\sigma^2$: $p(Y) = \\int_{0}^{\\infty} p(Y \\mid \\sigma^{2}) p(\\sigma^{2}) d\\sigma^{2}$.\n    This is a standard hierarchical model. A normal likelihood with an Inverse-Gamma prior on the variance results in a multivariate Student's t-distribution for the marginal.\n    Specifically, if $Y \\mid \\sigma^2 \\sim \\mathcal{N}(0, \\sigma^2 I_r)$ and $\\sigma^2 \\sim \\mathrm{IG}(a, b)$, then the marginal distribution of $Y$ is a multivariate Student's t-distribution, $Y \\sim t_r(\\mu, \\Sigma, \\nu)$, with parameters:\n    - Degrees of freedom: $\\nu = 2a$\n    - Location: $\\mu = 0$\n    - Scale matrix: $\\Sigma = \\frac{b}{a}I_r$\n    The differential entropy of a multivariate t-distribution with dimension $k$, degrees of freedom $\\nu$, and scale matrix $\\Sigma$ is:\n    $$H = \\ln\\left(\\frac{\\Gamma(\\nu/2)}{\\Gamma((\\nu+k)/2)}\\right) + \\frac{k}{2}\\ln(\\nu\\pi) + \\frac{1}{2}\\ln|\\det(\\Sigma)| + \\frac{\\nu+k}{2}\\left[\\psi\\left(\\frac{\\nu+k}{2}\\right) - \\psi\\left(\\frac{\\nu}{2}\\right)\\right]$$\n    Substituting our parameters $k=r$, $\\nu=2a$, and $\\det(\\Sigma) = \\det(\\frac{b}{a}I_r) = (\\frac{b}{a})^r$:\n    $$H(Y) = \\ln\\left(\\frac{\\Gamma(a)}{\\Gamma(a+r/2)}\\right) + \\frac{r}{2}\\ln(2a\\pi) + \\frac{1}{2}\\ln\\left(\\left(\\frac{b}{a}\\right)^{r}\\right) + \\frac{2a+r}{2}\\left[\\psi\\left(a+\\frac{r}{2}\\right) - \\psi(a)\\right]$$\n    Simplifying the logarithmic terms:\n    $$H(Y) = \\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right) + \\frac{r}{2}(\\ln(2\\pi)+\\ln a) + \\frac{r}{2}(\\ln b - \\ln a) + \\left(a+\\frac{r}{2}\\right)\\left[\\psi\\left(a+\\frac{r}{2}\\right) - \\psi(a)\\right]$$\n    $$H(Y) = \\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right) + \\frac{r}{2}\\ln(2\\pi) + \\frac{r}{2}\\ln b + \\left(a+\\frac{r}{2}\\right)\\psi\\left(a+\\frac{r}{2}\\right) - \\left(a+\\frac{r}{2}\\right)\\psi(a)$$\n\n4.  **Calculate EIG**: $I(Y; \\sigma^{2}) = H(Y) - \\mathbb{E}_{\\sigma^{2}}[H(Y \\mid \\sigma^{2})]$\n    We now subtract the expression for the expected conditional entropy from the marginal entropy:\n    $$I(Y; \\sigma^{2}) = \\left[ \\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right) + \\frac{r}{2}\\ln(2\\pi) + \\frac{r}{2}\\ln b + \\left(a+\\frac{r}{2}\\right)\\psi\\left(a+\\frac{r}{2}\\right) - \\left(a+\\frac{r}{2}\\right)\\psi(a) \\right] - \\left[ \\frac{r}{2}(\\ln(2\\pi) + 1) + \\frac{r}{2}\\ln(b) - \\frac{r}{2}\\psi(a) \\right]$$\n    Let's group terms for cancellation:\n    $$I(Y; \\sigma^{2}) = (\\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right)) + \\left(\\frac{r}{2}\\ln(2\\pi) - \\frac{r}{2}\\ln(2\\pi)\\right) + \\left(\\frac{r}{2}\\ln b - \\frac{r}{2}\\ln b\\right) + \\left(a+\\frac{r}{2}\\right)\\psi\\left(a+\\frac{r}{2}\\right) - \\left(a+\\frac{r}{2}\\right)\\psi(a) + \\frac{r}{2}\\psi(a) - \\frac{r}{2}$$\n    The $\\ln(2\\pi)$ and $\\ln b$ terms cancel. Combining the $\\psi(a)$ terms:\n    $$-(a+r/2)\\psi(a) + (r/2)\\psi(a) = -a\\psi(a) - (r/2)\\psi(a) + (r/2)\\psi(a) = -a\\psi(a)$$\n    Thus, the final expression for the EIG is:\n    $$I(Y; \\sigma^{2}) = \\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right) + \\left(a+\\frac{r}{2}\\right)\\psi\\left(a+\\frac{r}{2}\\right) - a\\psi(a) - \\frac{r}{2}$$\n    This expression depends only on the prior shape parameter $a$ and the number of data points $r$, not the prior scale parameter $b$, as expected from dimensional analysis considerations.\n\n**Part 2: Justification for Learning $\\sigma^2$**\n\nIn the context of inverse problems and data assimilation, the primary goal is typically to infer a set of forward model parameters, denoted by $\\theta$, from observational data $d$. The relationship is often modeled as $d = G(\\theta) + \\epsilon$, where $G$ is the forward model and $\\epsilon$ is observational noise, commonly assumed to be Gaussian with zero mean and variance $\\sigma^2$. In this setting, $\\sigma^2$ is a hyperparameter that governs how much trust is placed in the data during the inversion for $\\theta$. The posterior distribution of $\\theta$ is given by Bayes' theorem as $p(\\theta \\mid d) \\propto p(d \\mid \\theta) p(\\theta)$, where the likelihood term $p(d \\mid \\theta)$ depends critically on $\\sigma^2$.\n\nIt is beneficial to dedicate experimental effort to learning $\\sigma^2$ rather than (or prior to) learning $\\theta$ under the following conditions, based on first principles:\n\n1.  **When Prior Knowledge of Observation Error is Poor:** If the initial prior distribution for $\\sigma^2$ is vague or highly uncertain (e.g., the shape parameter $a$ in the Inverse-Gamma prior is small), this uncertainty propagates directly into the inference for $\\theta$. An uncertain $\\sigma^2$ means the relative weighting between the prior belief in $\\theta$ and the information from the data $d$ is ill-defined. This leads to a posterior for $\\theta$ that may be inappropriately wide (if $\\sigma^2$ is overestimated) or narrow and potentially biased (if $\\sigma^2$ is underestimated). A dedicated calibration experiment, as described in the problem, serves to reduce the uncertainty in $\\sigma^2$, yielding a more informative posterior for it. This posterior then acts as a much tighter, more reliable prior for $\\sigma^2$ in the main experiment designed to infer $\\theta$, ensuring that the data is weighted correctly and the inference on $\\theta$ is more robust and efficient.\n\n2.  **To Correctly Quantify Uncertainty and Avoid Model-Data Mismatch:** The residual term $\\|d - G(\\theta)\\|^2$ mixes the effects of observation error (random noise) and model inadequacy (the forward model $G$ being an imperfect representation of reality). Without a reliable estimate of the observation noise variance $\\sigma^2$, it is impossible to distinguish between these two sources of error. A calibration experiment, where the \"signal\" $G(\\theta)$ is known (e.g., is zero, as in the problem statement), isolates and quantifies $\\sigma^2$ exclusively. This is a critical first step in many data assimilation systems, which must explicitly specify the observation error covariance matrix (often denoted $R$). An accurate $R$ is a prerequisite for correctly inferring model error statistics (often denoted $Q$) and for obtaining a reliable posterior uncertainty quantification for the parameters $\\theta$.\n\n3.  **For Experimental Design and Cost-Effectiveness:** Experiments designed to be maximally informative for a complex model parameter $\\theta$ can be resource-intensive and expensive. Such experiments may also be suboptimal for constraining the noise variance $\\sigma^2$. It is often far more cost-effective to first conduct a simpler, cheaper calibration experiment focused solely on characterizing the measurement instrument's noise properties ($\\sigma^2$). By \"tying down\" this nuisance parameter beforehand, the subsequent, more expensive experiments targeting $\\theta$ can be designed and analyzed with much greater efficiency, maximizing the scientific return on investment. The information from the expensive data is then used to constrain the parameter of primary interest, $\\theta$, rather than being partially consumed to also learn about the noise level.\n\nIn summary, learning $\\sigma^2$ is not just a secondary task; it is a foundational step that enables robust and efficient inference for the primary model parameters $\\theta$. It is most beneficial when initial uncertainty about the noise level is high and when disentangling different sources of error is crucial for the scientific objective.", "answer": "$$\n\\boxed{\\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right) + \\left(a+\\frac{r}{2}\\right)\\psi\\left(a+\\frac{r}{2}\\right) - a\\psi(a) - \\frac{r}{2}}\n$$", "id": "3380369"}]}