{"hands_on_practices": [{"introduction": "Before we can solve an inverse problem, we must first have a robust forward model that accurately represents the underlying physics. This practice [@problem_id:3365808] challenges you to connect physical processes at the Earth's surface and the top of the atmosphere—such as emissions, deposition, and entrainment—to their correct mathematical formulations as boundary conditions on a transport-chemistry partial differential equation. Mastering this step is fundamental, as the accuracy of any inversion result depends critically on the fidelity of the forward model.", "problem": "Consider a passive-reactive atmospheric tracer with concentration field $c(\\boldsymbol{x},t)$ evolving over a three-dimensional domain $\\Omega \\subset \\mathbb{R}^{3}$ that represents the troposphere between the Earth’s surface $\\Gamma_{s}$ and a model top-of-atmosphere $\\Gamma_{t}$, with lateral boundary $\\Gamma_{\\ell}$ so that $\\partial \\Omega = \\Gamma_{s} \\cup \\Gamma_{t} \\cup \\Gamma_{\\ell}$. The tracer is transported by a known wind field $\\boldsymbol{u}(\\boldsymbol{x},t)$ and mixed by turbulent diffusion modeled by a symmetric positive-definite diffusivity tensor $\\mathbf{K}(\\boldsymbol{x},t)$. The concentration satisfies the advection–diffusion–reaction Partial Differential Equation (PDE)\n$$\n\\frac{\\partial c}{\\partial t} + \\nabla \\cdot \\left( \\boldsymbol{u}\\, c \\right) \\;=\\; \\nabla \\cdot \\left( \\mathbf{K} \\nabla c \\right) \\;-\\; \\lambda(\\boldsymbol{x},t)\\, c \\;+\\; q_{v}(\\boldsymbol{x},t)\\,, \\quad \\text{in } \\Omega,\n$$\nwhere $\\lambda(\\boldsymbol{x},t) \\ge 0$ is a first-order loss rate and $q_{v}(\\boldsymbol{x},t)$ is a known volumetric source. Let $\\boldsymbol{n}$ denote the outward unit normal on $\\partial \\Omega$. At the surface $\\Gamma_{s}$, tracer interacts with the ground via:\n- a prescribed surface emission flux $E(\\boldsymbol{x},t)$ with units of mass per area per time,\n- dry deposition parameterized by a deposition velocity $v_{d}(\\boldsymbol{x},t) \\ge 0$ with units of length per time.\nAt the top $\\Gamma_{t}$, the troposphere exchanges with an external reservoir of known concentration $c_{a}(\\boldsymbol{x},t)$ via turbulent entrainment characterized by a transfer velocity $k_{e}(\\boldsymbol{x},t) \\ge 0$ with units of length per time. Advective boundary conditions are to be specified on inflow subsets where $\\boldsymbol{n}\\cdot \\boldsymbol{u} < 0$. Assume the surface is impermeable to resolved advection so that $\\boldsymbol{n}\\cdot \\boldsymbol{u} = 0$ on $\\Gamma_{s}$. Consider the global mass tendency\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\int_{\\Omega} c\\, \\mathrm{d}\\Omega\\,,\n$$\nwhich must follow from conservation of mass and the divergence theorem, including explicit boundary flux terms across $\\partial \\Omega$.\n\nWhich option gives a physically appropriate and self-consistent set of boundary conditions at $\\Gamma_{s}$ and $\\Gamma_{t}$, justifies their physical meaning in terms of emission, deposition, and exchange with the overlying reservoir, and yields a correct global mass balance for $\\displaystyle \\frac{\\mathrm{d}}{\\mathrm{d}t} \\int_{\\Omega} c\\, \\mathrm{d}\\Omega$ with proper signs and inflow–outflow treatment?\n\nA. Surface: $\\boldsymbol{n}\\cdot \\boldsymbol{u} = 0$ and the Robin boundary condition $-\\boldsymbol{n}\\cdot \\mathbf{K}\\nabla c = v_{d}\\, c - E$ on $\\Gamma_{s}$. Top: apply a Robin diffusive exchange $-\\boldsymbol{n}\\cdot \\mathbf{K}\\nabla c = k_{e}\\, (\\,c - c_{a}\\,)$ on $\\Gamma_{t}$, and impose the Dirichlet inflow condition $c=c_{a}$ only on the inflow subset $\\Gamma_{t}^{-}=\\{\\boldsymbol{x}\\in \\Gamma_{t}: \\boldsymbol{n}\\cdot \\boldsymbol{u} < 0\\}$; no advective prescription on outflow $\\Gamma_{t}^{+}=\\{\\boldsymbol{x}\\in \\Gamma_{t}: \\boldsymbol{n}\\cdot \\boldsymbol{u} > 0\\}$. With zero diffusive flux on $\\Gamma_{\\ell}$, the global mass balance is\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\int_{\\Omega} c\\, \\mathrm{d}\\Omega\n= - \\int_{\\Gamma_{\\ell} \\cup \\Gamma_{t}} \\boldsymbol{n}\\cdot (\\boldsymbol{u}\\, c)\\, \\mathrm{d}S\n\\;-\\; \\int_{\\Gamma_{s}} \\big( v_{d}\\, c - E \\big)\\, \\mathrm{d}S\n\\;-\\; \\int_{\\Gamma_{t}} k_{e}\\, (\\,c - c_{a}\\,)\\, \\mathrm{d}S\n\\;-\\; \\int_{\\Omega} \\lambda\\, c\\, \\mathrm{d}\\Omega\n\\;+\\; \\int_{\\Omega} q_{v}\\, \\mathrm{d}\\Omega.\n$$\nThis implies deposition $v_{d}\\,c$ and entrainment $k_{e}(c-c_{a})$ remove mass when $c > c_{a}$, while emission $E$ adds mass; advection exports on outflow and imports on inflow with $c$ set by the imposed inflow values.\n\nB. Surface: $\\boldsymbol{n}\\cdot \\boldsymbol{u} = 0$ and $-\\boldsymbol{n}\\cdot \\mathbf{K}\\nabla c = E + v_{d}\\, c$ on $\\Gamma_{s}$, interpreting both emission and deposition as outward fluxes. Top: same as in A. Global mass balance:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\int_{\\Omega} c\\, \\mathrm{d}\\Omega\n= - \\int_{\\Gamma_{\\ell} \\cup \\Gamma_{t}} \\boldsymbol{n}\\cdot (\\boldsymbol{u}\\, c)\\, \\mathrm{d}S\n\\;-\\; \\int_{\\Gamma_{s}} \\big( E + v_{d}\\, c \\big)\\, \\mathrm{d}S\n\\;-\\; \\int_{\\Gamma_{t}} k_{e}\\, (\\,c - c_{a}\\,)\\, \\mathrm{d}S\n\\;-\\; \\int_{\\Omega} \\lambda\\, c\\, \\mathrm{d}\\Omega\n\\;+\\; \\int_{\\Omega} q_{v}\\, \\mathrm{d}\\Omega.\n$$\nThus both emission and deposition decrease atmospheric mass.\n\nC. Surface: as in A. Top: a sealed lid with $-\\boldsymbol{n}\\cdot \\mathbf{K}\\nabla c = 0$ and $\\boldsymbol{n}\\cdot \\boldsymbol{u} = 0$ on $\\Gamma_{t}$. Global mass balance:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\int_{\\Omega} c\\, \\mathrm{d}\\Omega\n= - \\int_{\\Gamma_{\\ell}} \\boldsymbol{n}\\cdot (\\boldsymbol{u}\\, c)\\, \\mathrm{d}S\n\\;-\\; \\int_{\\Gamma_{s}} \\big( v_{d}\\, c - E \\big)\\, \\mathrm{d}S\n\\;-\\; \\int_{\\Omega} \\lambda\\, c\\, \\mathrm{d}\\Omega\n\\;+\\; \\int_{\\Omega} q_{v}\\, \\mathrm{d}\\Omega.\n$$\nThis models no net exchange with the overlying atmosphere.\n\nD. Surface: as in A. Top: reverse-sign Robin $-\\boldsymbol{n}\\cdot \\mathbf{K}\\nabla c = k_{e}\\, (\\,c_{a} - c\\,)$ on $\\Gamma_{t}$ and impose $c=c_{a}$ on the outflow subset $\\Gamma_{t}^{+}$ while leaving inflow unconstrained. Global mass balance:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\int_{\\Omega} c\\, \\mathrm{d}\\Omega\n= - \\int_{\\Gamma_{\\ell} \\cup \\Gamma_{t}} \\boldsymbol{n}\\cdot (\\boldsymbol{u}\\, c)\\, \\mathrm{d}S\n\\;-\\; \\int_{\\Gamma_{s}} \\big( v_{d}\\, c - E \\big)\\, \\mathrm{d}S\n\\;-\\; \\int_{\\Gamma_{t}} k_{e}\\, (\\,c_{a} - c\\,)\\, \\mathrm{d}S\n\\;-\\; \\int_{\\Omega} \\lambda\\, c\\, \\mathrm{d}\\Omega\n\\;+\\; \\int_{\\Omega} q_{v}\\, \\mathrm{d}\\Omega.\n$$\nThis implies mass enters from aloft when $c > c_{a}$ and constrains outflow values.\n\nE. Surface: model deposition by Dirichlet $c=0$ on $\\Gamma_{s}$ so that $c$ vanishes at the ground; no explicit flux condition. Top: impose $c=c_{a}$ on all of $\\Gamma_{t}$ for both inflow and outflow. Global mass balance omits boundary fluxes since $c$ is prescribed:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\int_{\\Omega} c\\, \\mathrm{d}\\Omega\n= - \\int_{\\Omega} \\lambda\\, c\\, \\mathrm{d}\\Omega\n\\;+\\; \\int_{\\Omega} q_{v}\\, \\mathrm{d}\\Omega.\n$$", "solution": "The analysis proceeds in two stages. First, the global mass balance equation is derived from first principles using the provided Partial Differential Equation (PDE) and the divergence theorem. Second, each option is evaluated against this derivation and fundamental physical principles.\n\n**1. Derivation of the Global Mass Balance Equation**\n\nThe governing advection–diffusion–reaction equation for the tracer concentration $c(\\boldsymbol{x},t)$ is given as:\n$$\n\\frac{\\partial c}{\\partial t} + \\nabla \\cdot \\left( \\boldsymbol{u}\\, c \\right) \\;=\\; \\nabla \\cdot \\left( \\mathbf{K} \\nabla c \\right) \\;-\\; \\lambda(\\boldsymbol{x},t)\\, c \\;+\\; q_{v}(\\boldsymbol{x},t)\n$$\nTo find the time rate of change of the total mass of the tracer in the domain $\\Omega$, we integrate the PDE over $\\Omega$:\n$$\n\\int_{\\Omega} \\frac{\\partial c}{\\partial t} \\, \\mathrm{d}\\Omega + \\int_{\\Omega} \\nabla \\cdot \\left( \\boldsymbol{u}\\, c \\right) \\, \\mathrm{d}\\Omega = \\int_{\\Omega} \\nabla \\cdot \\left( \\mathbf{K} \\nabla c \\right) \\, \\mathrm{d}\\Omega - \\int_{\\Omega} \\lambda c \\, \\mathrm{d}\\Omega + \\int_{\\Omega} q_{v} \\, \\mathrm{d}\\Omega\n$$\nAssuming the domain $\\Omega$ is fixed in time, the integral of the time derivative can be interchanged with the differentiation operator:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\int_{\\Omega} c \\, \\mathrm{d}\\Omega + \\int_{\\Omega} \\nabla \\cdot \\left( \\boldsymbol{u}\\, c \\right) \\, \\mathrm{d}\\Omega = \\int_{\\Omega} \\nabla \\cdot \\left( \\mathbf{K} \\nabla c \\right) \\, \\mathrm{d}\\Omega - \\int_{\\Omega} \\lambda c \\, \\mathrm{d}\\Omega + \\int_{\\Omega} q_{v} \\, \\mathrm{d}\\Omega\n$$\nWe apply the divergence theorem (Gauss's theorem) to the two divergence terms, which converts volume integrals of a divergence into surface integrals of the flux across the boundary $\\partial \\Omega$:\n$$\n\\int_{\\Omega} \\nabla \\cdot \\mathbf{F} \\, \\mathrm{d}\\Omega = \\int_{\\partial \\Omega} \\boldsymbol{n} \\cdot \\mathbf{F} \\, \\mathrm{d}S\n$$\nwhere $\\boldsymbol{n}$ is the outward unit normal vector. Applying this yields:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\int_{\\Omega} c \\, \\mathrm{d}\\Omega + \\int_{\\partial \\Omega} \\boldsymbol{n} \\cdot (\\boldsymbol{u}c) \\, \\mathrm{d}S = \\int_{\\partial \\Omega} \\boldsymbol{n} \\cdot (\\mathbf{K} \\nabla c) \\, \\mathrm{d}S - \\int_{\\Omega} \\lambda c \\, \\mathrm{d}\\Omega + \\int_{\\Omega} q_{v} \\, \\mathrm{d}\\Omega\n$$\nRearranging to isolate the mass tendency gives the general form of the global mass balance:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\int_{\\Omega} c \\, \\mathrm{d}\\Omega = - \\int_{\\partial \\Omega} \\boldsymbol{n} \\cdot (\\boldsymbol{u}c) \\, \\mathrm{d}S + \\int_{\\partial \\Omega} \\boldsymbol{n} \\cdot (\\mathbf{K} \\nabla c) \\, \\mathrm{d}S - \\int_{\\Omega} \\lambda c \\, \\mathrm{d}\\Omega + \\int_{\\Omega} q_{v} \\, \\mathrm{d}\\Omega\n$$\nThe terms on the right-hand side represent the net rate of mass change due to: advective flux across the boundary, diffusive flux across the boundary, internal chemical loss, and internal volumetric sources, respectively.\n\nNow, we must formulate the boundary conditions based on the physical processes described.\n\n**Surface Boundary $\\Gamma_{s}$:**\nThe total non-advective flux at the surface is the sum of emission and deposition.\n- Emission $E(\\boldsymbol{x},t)$ is a source of mass for the atmosphere. It represents a flux *into* the domain $\\Omega$. With $\\boldsymbol{n}$ being the outward normal, an inward flux corresponds to a negative value of $\\boldsymbol{n} \\cdot \\boldsymbol{F}_{\\text{flux}}$.\n- Dry deposition, parameterized by $v_{d}(\\boldsymbol{x},t)c$, is a sink of mass. It represents a flux *out of* the domain $\\Omega$. This corresponds to a positive value of $\\boldsymbol{n} \\cdot \\boldsymbol{F}_{\\text{flux}}$.\n\nThe total non-advective (modeled as diffusive) flux out of the domain is the sum of the deposition flux (outward) and the emission flux (inward, so negative outward). The diffusive flux out of the domain is given by $-\\boldsymbol{n} \\cdot (\\mathbf{K} \\nabla c)$. Therefore, the boundary condition must be:\n$$\n-\\boldsymbol{n} \\cdot (\\mathbf{K} \\nabla c) = v_d c - E\n$$\nThis is a Robin-type boundary condition.\n\n**Top Boundary $\\Gamma_{t}$:**\nExchange with an external reservoir of concentration $c_a$ is parameterized by a transfer velocity $k_e$.\n- If the domain concentration $c$ is greater than the external concentration $c_a$, there should be a net flux *out of* the domain. The magnitude of this outward flux is $k_e(c-c_a)$.\n- If $c < c_a$, there should be a net flux *into* the domain.\nThis is captured by defining the outward flux as $k_e(c-c_a)$. Equating this physical flux to the diffusive flux out of the domain gives the boundary condition:\n$$\n-\\boldsymbol{n} \\cdot (\\mathbf{K} \\nabla c) = k_e(c-c_a)\n$$\nFor the advective part at the top boundary, in an advection-dominated system, boundary conditions must be specified on the inflow portion, where external information propagates into the domain. The inflow boundary is where $\\boldsymbol{n} \\cdot \\boldsymbol{u} < 0$. It is physically sound to set the concentration of the incoming fluid to that of the external reservoir, i.e., $c=c_a$ on $\\Gamma_t^- = \\{\\boldsymbol{x}\\in \\Gamma_{t}: \\boldsymbol{n}\\cdot \\boldsymbol{u} < 0\\}$. No condition should be specified on the outflow part ($\\boldsymbol{n} \\cdot \\boldsymbol{u} > 0$), as the concentration there is determined by the solution within $\\Omega$.\n\nNow we substitute these boundary conditions into the general mass balance equation. The boundary integral $\\int_{\\partial \\Omega}$ is split into integrals over $\\Gamma_{s}$, $\\Gamma_{t}$, and $\\Gamma_{\\ell}$.\nThe term $\\int_{\\partial \\Omega} \\boldsymbol{n} \\cdot (\\mathbf{K} \\nabla c) \\, \\mathrm{d}S$ becomes:\n$$\n\\int_{\\Gamma_s} \\boldsymbol{n} \\cdot (\\mathbf{K} \\nabla c) \\, \\mathrm{d}S + \\int_{\\Gamma_t} \\boldsymbol{n} \\cdot (\\mathbf{K} \\nabla c) \\, \\mathrm{d}S + \\int_{\\Gamma_\\ell} \\boldsymbol{n} \\cdot (\\mathbf{K} \\nabla c) \\, \\mathrm{d}S\n$$\nUsing the boundary conditions:\n- On $\\Gamma_s$: $\\boldsymbol{n} \\cdot (\\mathbf{K} \\nabla c) = -(v_d c - E)$\n- On $\\Gamma_t$: $\\boldsymbol{n} \\cdot (\\mathbf{K} \\nabla c) = -k_e(c-c_a)$\n- On $\\Gamma_\\ell$: A common and reasonable assumption for lateral boundaries far away is zero diffusive flux, i.e., $\\boldsymbol{n} \\cdot (\\mathbf{K} \\nabla c) = 0$.\n\nThe advective flux term $-\\int_{\\partial \\Omega} \\boldsymbol{n} \\cdot (\\boldsymbol{u}c) \\, \\mathrm{d}S$ is:\n- On $\\Gamma_s$: The problem states $\\boldsymbol{n} \\cdot \\boldsymbol{u} = 0$, so the integral is zero.\n- This leaves the integral over $\\Gamma_t \\cup \\Gamma_\\ell$.\n\nCombining everything, the final mass balance equation is:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\int_{\\Omega} c\\, \\mathrm{d}\\Omega = - \\int_{\\Gamma_{\\ell} \\cup \\Gamma_{t}} \\boldsymbol{n}\\cdot (\\boldsymbol{u}\\, c)\\, \\mathrm{d}S - \\int_{\\Gamma_{s}} \\big( v_{d}\\, c - E \\big)\\, \\mathrm{d}S - \\int_{\\Gamma_{t}} k_{e}\\, (\\,c - c_{a}\\,)\\, \\mathrm{d}S - \\int_{\\Omega} \\lambda\\, c\\, \\mathrm{d}\\Omega + \\int_{\\Omega} q_{v}\\, \\mathrm{d}\\Omega\n$$\n\n**2. Evaluation of Options**\n\n**A. Surface: $\\boldsymbol{n}\\cdot \\boldsymbol{u} = 0$ and the Robin boundary condition $-\\boldsymbol{n}\\cdot \\mathbf{K}\\nabla c = v_{d}\\, c - E$ on $\\Gamma_{s}$. Top: apply a Robin diffusive exchange $-\\boldsymbol{n}\\cdot \\mathbf{K}\\nabla c = k_{e}\\, (\\,c - c_{a}\\,)$ on $\\Gamma_{t}$, and impose the Dirichlet inflow condition $c=c_{a}$ only on the inflow subset $\\Gamma_{t}^{-}=\\{\\boldsymbol{x}\\in \\Gamma_{t}: \\boldsymbol{n}\\cdot \\boldsymbol{u} < 0\\}$; no advective prescription on outflow $\\Gamma_{t}^{+}=\\{\\boldsymbol{x}\\in \\Gamma_{t}: \\boldsymbol{n}\\cdot \\boldsymbol{u} > 0\\}$. ...**\nThe boundary conditions proposed at both $\\Gamma_s$ and $\\Gamma_t$ for both diffusive and advective transport match our derivation exactly. They are physically sound and mathematically well-posed. The resulting global mass balance equation also exactly matches the one derived from first principles using these boundary conditions. The physical interpretation provided is also correct: deposition $v_{d}c$ is a loss term (negative contribution to the integral); emission $E$ is a gain term (positive contribution via $-(-E)$); entrainment $k_e(c-c_a)$ is a loss when $c > c_a$.\n**Verdict: Correct.**\n\n**B. Surface: $\\boldsymbol{n}\\cdot \\boldsymbol{u} = 0$ and $-\\boldsymbol{n}\\cdot \\mathbf{K}\\nabla c = E + v_{d}\\, c$ on $\\Gamma_{s}$, interpreting both emission and deposition as outward fluxes. ...**\nThe surface boundary condition $-\\boldsymbol{n}\\cdot \\mathbf{K}\\nabla c = E + v_{d}\\, c$ is physically incorrect. It treats emission $E$ as a flux in the same direction as deposition $v_d c$, i.e., an outward flux that removes mass from the atmosphere. Emission is a source to the atmosphere and must be an inward flux.\n**Verdict: Incorrect.**\n\n**C. Surface: as in A. Top: a sealed lid with $-\\boldsymbol{n}\\cdot \\mathbf{K}\\nabla c = 0$ and $\\boldsymbol{n}\\cdot \\boldsymbol{u} = 0$ on $\\Gamma_{t}$. ...**\nWhile a sealed lid is a valid physical model for certain scenarios, it contradicts the problem statement which explicitly describes exchange with an overlying reservoir: \"the troposphere exchanges with an external reservoir of known concentration $c_{a}(\\boldsymbol{x},t)$ via turbulent entrainment characterized by a transfer velocity $k_{e}(\\boldsymbol{x},t) \\ge 0$\". By setting the fluxes to zero, this option fails to model the specified physical processes. Although the derived mass balance is self-consistent with the proposed (but inappropriate) boundary conditions, it does not correctly represent the problem as stated.\n**Verdict: Incorrect.**\n\n**D. Surface: as in A. Top: reverse-sign Robin $-\\boldsymbol{n}\\cdot \\mathbf{K}\\nabla c = k_{e}\\, (\\,c_{a} - c\\,)$ on $\\Gamma_{t}$ and impose $c=c_{a}$ on the outflow subset $\\Gamma_{t}^{+}$ while leaving inflow unconstrained. ...**\nThis option contains at least two critical errors. First, the Robin condition $-\\boldsymbol{n}\\cdot \\mathbf{K}\\nabla c = k_{e}\\,(c_{a} - c)$ implies that for $c > c_a$, the flux is inward, and for $c < c_a$, the flux is outward. This is a positive feedback that would lead to unphysical amplification of concentration differences, rather than relaxation towards equilibrium. Second, prescribing a Dirichlet condition on the outflow boundary ($\\Gamma_t^+$) over-constrains the problem and is physically and mathematically ill-posed. Information flows out of the domain at this boundary; its state should be determined by the solution inside the domain, not externally forced.\n**Verdict: Incorrect.**\n\n**E. Surface: model deposition by Dirichlet $c=0$ on $\\Gamma_{s}$ so that $c$ vanishes at the ground; no explicit flux condition. Top: impose $c=c_{a}$ on all of $\\Gamma_{t}$ for both inflow and outflow. ...**\nThis option is flawed on multiple counts. The surface condition $c=0$ represents a perfect sink but entirely ignores the prescribed emission flux $E(\\boldsymbol{x},t)$, contradicting the problem statement. The top condition $c=c_a$ on the entire boundary $\\Gamma_t$ (including outflow) is, as explained for option D, an ill-posed over-constraint. Finally, the proposed global mass balance that omits all boundary fluxes is fundamentally wrong. A prescribed concentration (Dirichlet condition) does not imply zero flux; on the contrary, the flux must adjust to maintain that concentration, and it must be accounted for in the budget.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3365808"}, {"introduction": "The engine of four-dimensional variational (4D-Var) data assimilation is the efficient computation of the cost function's gradient. This exercise [@problem_id:3365830] guides you through the derivation of the tangent linear and adjoint models, which are the essential tools for this task. By working through the linearization of a discrete transport-chemistry model, you will gain a deep understanding of how information about model-observation misfits is propagated back in time to correct the initial state.", "problem": "Consider a three-dimensional chemical species governed by a time-varying transport–chemistry model discretized in space by a conservative finite-volume scheme and in time by a first-order forward Euler method with uniform time step $\\Delta t > 0$. Let $x_{k} \\in \\mathbb{R}^{n}$ denote the vector of cell-averaged species concentrations at discrete time $t_{k} = k \\Delta t$ for $k = 0, 1, \\dots, K$. The discrete prognostic update is\n$$\nx_{k+1} \\;=\\; T_{k} \\, x_{k} \\;+\\; \\Delta t \\; c(x_{k}) \\;+\\; s_{k},\n$$\nwhere $T_{k} \\in \\mathbb{R}^{n \\times n}$ represents the linear transport–mixing operator determined by known meteorology at time $t_{k}$, $c: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ is a nonlinear local chemical tendency assumed to be continuously differentiable with a Lipschitz continuous Jacobian, and $s_{k} \\in \\mathbb{R}^{n}$ is a known source term (e.g., emissions and boundary fluxes). Observations are collected at a subset of times $\\{t_{\\ell_{i}}\\}_{i=1}^{m}$, with $\\ell_{i} \\in \\{1,\\dots,K\\}$, by a linear observation operator $H_{i} \\in \\mathbb{R}^{p_{i} \\times n}$, yielding $y_{i} = H_{i} x_{\\ell_{i}} + \\varepsilon_{i}$, where $\\varepsilon_{i}$ are independent, zero-mean Gaussian errors with known covariances $R_{i} \\in \\mathbb{R}^{p_{i} \\times p_{i}}$.\n\nYou are tasked with an initial-state inversion. Let the control variable be the initial condition $x_{0}$, with a Gaussian background (prior) $x_{b}$ and covariance $B \\in \\mathbb{R}^{n \\times n}$, symmetric positive definite. Define the nonlinear observation mapping\n$$\n\\mathcal{G}(x_{0}) \\;=\\; \\begin{pmatrix} H_{1} \\, x_{\\ell_{1}}(x_{0}) \\\\ \\vdots \\\\ H_{m} \\, x_{\\ell_{m}}(x_{0}) \\end{pmatrix} \\in \\mathbb{R}^{p}, \\quad p \\;=\\; \\sum_{i=1}^{m} p_{i},\n$$\nwhere the state trajectory $\\{x_{k}(x_{0})\\}_{k=0}^{K}$ is generated by the discrete model above with initial condition $x_{0}$. The cost function for weak-constraint four-dimensional variational data assimilation (with model error fixed to the known $s_{k}$) is\n$$\nJ(x_{0}) \\;=\\; \\tfrac{1}{2} \\, (x_{0} - x_{b})^{\\top} B^{-1} (x_{0} - x_{b}) \\;+\\; \\tfrac{1}{2} \\, \\big(\\mathcal{G}(x_{0}) - y\\big)^{\\top} R^{-1} \\big(\\mathcal{G}(x_{0}) - y\\big),\n$$\nwith $y = (y_{1}^{\\top}, \\dots, y_{m}^{\\top})^{\\top}$ and block-diagonal $R = \\mathrm{diag}(R_{1}, \\dots, R_{m})$.\n\nStarting from first principles of linearization (first-order Taylor expansion) and the chain rule applied to the discrete dynamical system, perform the following:\n\n1) Derive the tangent linear model, i.e., the linearized dynamics for perturbations $\\delta x_{k}$ about a given reference trajectory $\\{x_{k}\\}_{k=0}^{K}$ generated by a current iterate $x_{0}$.\n\n2) Using the tangent linear model, derive the Jacobian–vector product $\\mathrm{J}(x_{0}) \\, v$ for an arbitrary direction $v \\in \\mathbb{R}^{n}$, where $\\mathrm{J}(x_{0})$ is the Jacobian of $\\mathcal{G}$ with respect to $x_{0}$ evaluated along the reference trajectory. Your result should be a closed-form analytic expression in terms of $\\{T_{k}\\}$, $\\{\\nabla c(x_{k})\\}$, $\\{H_{i}\\}$, and matrix products.\n\n3) Explain, without forming the full Jacobian matrix explicitly, how to compute $\\mathrm{J}(x_{0}) \\, v$ efficiently in an iterative Gauss–Newton method, and briefly describe the corresponding efficient computation of the transpose–vector product $\\mathrm{J}(x_{0})^{\\top} w$ needed by matrix-free Krylov solvers.\n\nProvide the final answer as the single analytic expression for $\\mathrm{J}(x_{0}) \\, v$. No numerical evaluation is required. Express your final answer symbolically and do not include any units. The final answer must be a single closed-form expression.", "solution": "The problem statement is critically evaluated for validity prior to attempting a solution.\n\n### Problem Validation\n\n**Step 1: Extracted Givens**\n- **Discrete prognostic model:** $x_{k+1} = T_{k} x_{k} + \\Delta t c(x_{k}) + s_{k}$ for $k = 0, 1, \\dots, K$.\n- **State vector:** $x_{k} \\in \\mathbb{R}^{n}$ at time $t_{k} = k \\Delta t$.\n- **Time step:** $\\Delta t > 0$.\n- **Transport operator:** $T_{k} \\in \\mathbb{R}^{n \\times n}$, linear.\n- **Chemistry tendency:** $c: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$, nonlinear, continuously differentiable with a Lipschitz continuous Jacobian.\n- **Source term:** $s_{k} \\in \\mathbb{R}^{n}$, known.\n- **Observations:** $y_{i} = H_{i} x_{\\ell_{i}} + \\varepsilon_{i}$ at times $\\{t_{\\ell_{i}}\\}_{i=1}^{m}$, where $\\ell_{i} \\in \\{1,\\dots,K\\}$.\n- **Observation operator:** $H_{i} \\in \\mathbb{R}^{p_{i} \\times n}$, linear.\n- **Observation errors:** $\\varepsilon_{i}$ are independent, zero-mean Gaussian with covariances $R_{i} \\in \\mathbb{R}^{p_{i} \\times p_{i}}$.\n- **Control variable:** Initial condition $x_{0} \\in \\mathbb{R}^{n}$.\n- **Background information:** Prior estimate $x_{b}$ with covariance $B \\in \\mathbb{R}^{n \\times n}$, symmetric positive definite.\n- **Nonlinear observation mapping:** $\\mathcal{G}(x_{0}) = (H_{1} x_{\\ell_{1}}(x_{0})^{\\top}, \\dots, H_{m} x_{\\ell_{m}}(x_{0})^{\\top})^{\\top}$.\n- **Cost function:** $J(x_{0}) = \\frac{1}{2} (x_{0} - x_{b})^{\\top} B^{-1} (x_{0} - x_{b}) + \\frac{1}{2} (\\mathcal{G}(x_{0}) - y)^{\\top} R^{-1} (\\mathcal{G}(x_{0}) - y)$.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded:** The problem describes a standard framework for four-dimensional variational data assimilation (4D-Var) applied to an atmospheric chemistry transport model. This is a well-established and fundamental topic in geophysical sciences, inverse problems, and numerical optimization. All components—the discrete model, observation process, and Bayesian cost function—are standard and scientifically sound.\n- **Well-Posed:** The problem asks for specific mathematical derivations (tangent linear model, Jacobian-vector product) and an algorithmic explanation. The assumptions (e.g., continuous differentiability of $c(x)$) ensure that the required linearization is well-defined. The tasks are precise and lead to unique analytical expressions and algorithmic descriptions.\n- **Objective:** The problem is formulated in objective, mathematical language, free from ambiguity or subjective assertions.\n- **Completeness and Consistency:** The problem provides a self-contained and complete description necessary to perform the requested derivations. While the problem title mentions \"weak-constraint\" 4D-Var, the cost function provided corresponds to a \"strong-constraint\" formulation, where the model dynamics are assumed to be perfect. This is a minor terminological inconsistency but does not affect the mathematical validity, as the task is to analyze the explicitly given equations. The formulation is unambiguous.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. It is a rigorous, scientifically sound, and well-posed problem in the field of data assimilation. The solution process will now proceed.\n\n### Solution\n\nThe solution is presented in three parts as requested. It begins with the derivation of the tangent linear model, followed by the Jacobian-vector product, and concludes with an explanation of the efficient computational methods.\n\n**1) Derivation of the Tangent Linear Model**\n\nThe tangent linear model (TLM) describes the evolution of a small perturbation to the state vector. Let $\\{x_{k}\\}_{k=0}^{K}$ be a reference state trajectory generated by the nonlinear model starting from an initial condition $x_{0}$:\n$$\nx_{k+1} = T_{k} x_{k} + \\Delta t c(x_{k}) + s_{k}\n$$\nConsider a perturbed initial condition $x'_{0} = x_{0} + \\delta x_{0}$, which generates a perturbed trajectory $\\{x'_{k}\\}_{k=0}^{K}$:\n$$\nx'_{k+1} = T_{k} x'_{k} + \\Delta t c(x'_{k}) + s_{k}\n$$\nLet the perturbation at time $t_{k}$ be $\\delta x_{k} = x'_{k} - x_{k}$. Subtracting the reference model equation from the perturbed model equation gives the evolution of the exact perturbation:\n$$\nx'_{k+1} - x_{k+1} = T_{k} (x'_{k} - x_{k}) + \\Delta t (c(x'_{k}) - c(x_{k}))\n$$\n$$\n\\delta x_{k+1} = T_{k} \\delta x_{k} + \\Delta t (c(x_{k} + \\delta x_{k}) - c(x_{k}))\n$$\nFor a small perturbation $\\delta x_{k}$, we can linearize the nonlinear chemistry term $c(x_{k} + \\delta x_{k})$ using a first-order Taylor expansion around the reference state $x_{k}$. Since $c$ is continuously differentiable, its Jacobian matrix, denoted $\\nabla c(x_{k}) \\in \\mathbb{R}^{n \\times n}$, exists.\n$$\nc(x_{k} + \\delta x_{k}) \\approx c(x_{k}) + \\nabla c(x_{k}) \\delta x_{k}\n$$\nSubstituting this approximation into the perturbation evolution equation yields:\n$$\n\\delta x_{k+1} \\approx T_{k} \\delta x_{k} + \\Delta t (\\nabla c(x_{k}) \\delta x_{k})\n$$\nThis gives the tangent linear model:\n$$\n\\delta x_{k+1} = (T_{k} + \\Delta t \\nabla c(x_{k})) \\delta x_{k}\n$$\nWe define the tangent linear propagator or model matrix $M_{k} \\in \\mathbb{R}^{n \\times n}$ as:\n$$\nM_{k} = T_{k} + \\Delta t \\nabla c(x_{k})\n$$\nThe TLM is then the linear, time-varying discrete system governing the first-order evolution of perturbations:\n$$\n\\delta x_{k+1} = M_{k} \\delta x_{k}\n$$\nThis model is initialized with a perturbation $\\delta x_{0}$ at time $t_{0}$.\n\n**2) Derivation of the Jacobian–vector product $\\mathrm{J}(x_{0}) \\, v$**\n\nThe Jacobian of the nonlinear observation mapping $\\mathcal{G}(x_{0})$ with respect to the initial state $x_{0}$ is denoted by $\\mathrm{J}(x_{0}) = \\frac{\\partial \\mathcal{G}}{\\partial x_{0}}$. The Jacobian-vector product $\\mathrm{J}(x_{0}) v$ for an arbitrary direction $v \\in \\mathbb{R}^{n}$ is the directional derivative of $\\mathcal{G}$ at $x_{0}$ in the direction $v$. This corresponds to the first-order change in the model-predicted observations $\\mathcal{G}(x_{0})$ resulting from a perturbation $v$ to the initial state $x_{0}$.\n\nThe evolution of such a perturbation is governed by the tangent linear model derived in Part 1. We set the initial perturbation to be $\\delta x_{0} = v$. The perturbation at any subsequent time $t_{k}$ can be found by recursively applying the TLM propagator:\n$$\n\\delta x_{1} = M_{0} \\delta x_{0}\n$$\n$$\n\\delta x_{2} = M_{1} \\delta x_{1} = M_{1} M_{0} \\delta x_{0}\n$$\nIn general, the perturbation at time $t_{k}$ is given by:\n$$\n\\delta x_{k} = (M_{k-1} M_{k-2} \\cdots M_{0}) \\delta x_{0} = \\left( \\prod_{j=0}^{k-1} M_{j} \\right) \\delta x_{0}\n$$\nHere, the product notation $\\prod_{j=0}^{k-1} M_{j}$ denotes the matrix product $M_{k-1} M_{k-2} \\cdots M_{0}$.\n\nThe nonlinear observation mapping is $\\mathcal{G}(x_{0}) = (H_{1} x_{\\ell_{1}}(x_{0})^{\\top}, \\dots, H_{m} x_{\\ell_{m}}(x_{0})^{\\top})^{\\top}$. Since the observation operators $H_{i}$ are linear, the first-order perturbation in the output of $\\mathcal{G}$ is obtained by applying $H_{i}$ to the state perturbations $\\delta x_{\\ell_{i}}$:\n$$\n\\mathrm{J}(x_{0}) v = \\delta \\mathcal{G} = \\begin{pmatrix} H_{1} \\delta x_{\\ell_{1}} \\\\ \\vdots \\\\ H_{m} \\delta x_{\\ell_{m}} \\end{pmatrix}\n$$\nSubstituting the expression for $\\delta x_{k}$ with $\\delta x_{0} = v$, we get the perturbation at each observation time $t_{\\ell_{i}}$:\n$$\n\\delta x_{\\ell_{i}} = \\left( \\prod_{j=0}^{\\ell_{i}-1} M_{j} \\right) v\n$$\nTherefore, the Jacobian-vector product is given by:\n$$\n\\mathrm{J}(x_{0}) v = \\begin{pmatrix} H_{1} \\left( \\prod_{j=0}^{\\ell_{1}-1} M_{j} \\right) v \\\\ \\vdots \\\\ H_{m} \\left( \\prod_{j=0}^{\\ell_{m}-1} M_{j} \\right) v \\end{pmatrix}\n$$\nSubstituting the definition of $M_{j}$ gives the final analytic expression:\n$$\n\\mathrm{J}(x_{0}) v = \\begin{pmatrix} H_{1} \\left( \\prod_{j=0}^{\\ell_{1}-1} [T_{j} + \\Delta t \\nabla c(x_{j})] \\right) v \\\\ \\vdots \\\\ H_{m} \\left( \\prod_{j=0}^{\\ell_{m}-1} [T_{j} + \\Delta t \\nabla c(x_{j})] \\right) v \\end{pmatrix}\n$$\n\n**3) Efficient Computation of $\\mathrm{J}(x_{0}) v$ and $\\mathrm{J}(x_{0})^{\\top} w$**\n\nMost optimization algorithms for large-scale data assimilation, such as Gauss-Newton or quasi-Newton methods, require the computation of Jacobian-vector products and Jacobian-transpose-vector products. It is computationally infeasible to explicitly form the dense Jacobian matrix $\\mathrm{J}(x_{0})$. Instead, matrix-free methods are employed.\n\n**Efficient computation of $\\mathrm{J}(x_{0}) v$ (Tangent Linear Model Run):**\nThe expression for $\\mathrm{J}(x_{0}) v$ derived above suggests an efficient sequential algorithm. Instead of forming the large matrix products, one propagates the perturbation vector $v$ forward in time using the TLM.\n\nThe algorithm is as follows:\n1.  A reference trajectory $\\{x_{k}\\}_{k=0}^{K}$ must be computed and stored, or recomputed on the fly, from the initial state $x_{0}$. This is needed to evaluate the Jacobian of the chemistry term, $\\nabla c(x_{k})$.\n2.  Initialize the perturbation vector: $\\delta x = v$.\n3.  Initialize an empty result vector for the output.\n4.  Iterate forward in time from $k=0$ to $K-1$:\n    a. If $t_{k+1}$ is an observation time, i.e., $k+1 = \\ell_{i}$ for some $i$, compute the observed perturbation $H_{i} \\delta x_{\\ell_{i}}$ (where $\\delta x_{\\ell_{i}}$ is the current state of the perturbation) and store it. However, it's more common to propagate first, then observe. So, it's better to structure the loop from $k=0$ to $K$, and check for observations at time $k$.\n    Revised loop:\n    1. Initialize $\\delta x_0 = v$.\n    2. Loop $k=1, \\dots, K$:\n       a. $\\delta x_k = M_{k-1} \\delta x_{k-1} = (T_{k-1} + \\Delta t \\nabla c(x_{k-1})) \\delta x_{k-1}$.\n       b. If $k \\in \\{\\ell_1, \\dots, \\ell_m\\}$, find a corresponding index $i$ such that $k=\\ell_i$. Compute $z_i = H_i \\delta x_k$.\n    3. Assemble the resulting vectors $z_i$ into the final vector $\\mathrm{J}(x_{0}) v$.\n\nThis procedure, known as a TLM run, has a computational cost comparable to a single run of the full nonlinear model, assuming the cost of computing $\\nabla c(x_k)$ and performing the matrix-vector product is manageable.\n\n**Efficient computation of $\\mathrm{J}(x_{0})^{\\top} w$ (Adjoint Model Run):**\nThe gradient of the cost function $J(x_{0})$ requires the term $\\mathrm{J}(x_{0})^{\\top} R^{-1} (\\mathcal{G}(x_{0}) - y)$. This requires the computation of a Jacobian-transpose-vector product, $\\mathrm{J}(x_{0})^{\\top} w$, where $w = R^{-1}(\\mathcal{G}(x_{0}) - y)$.\n\nThe structure of $\\mathrm{J}(x_{0})^{\\top}$ involves transposes of matrix products, which reverses their order: $(\\prod_{j=0}^{k-1} M_{j})^{\\top} = M_{0}^{\\top} M_{1}^{\\top} \\cdots M_{k-1}^{\\top}$. This suggests an algorithm that integrates backward in time. This is the adjoint model.\n\nThe algorithm to compute $u = \\mathrm{J}(x_{0})^{\\top} w$, where $w = (w_{1}^{\\top}, \\dots, w_{m}^{\\top})^{\\top}$:\n1.  The reference trajectory $\\{x_{k}\\}$ is required, as in the TLM run.\n2.  Initialize an adjoint state vector $\\lambda_{K} = 0 \\in \\mathbb{R}^{n}$.\n3.  Iterate backward in time from $k = K-1$ down to $0$:\n    a. Propagate the adjoint state one step backward: $\\lambda_{k} = M_{k}^{\\top} \\lambda_{k+1} = (T_{k}^{\\top} + \\Delta t \\nabla c(x_{k})^{\\top}) \\lambda_{k+1}$.\n    b. Check if the time step $k+1$ was an observation time. If $k+1 = \\ell_{i}$ for some $i$, add the corresponding forcing from the observation residual vector $w$: $\\lambda_{k+1} \\rightarrow \\lambda_{k+1} + H_i^{\\top} w_i$. This forcing must be applied to $\\lambda_{k+1}$ before computing $M_{k}^{\\top} \\lambda_{k+1}$. A more precise formulation of the loop is:\n       1. Initialize $\\lambda_{K} = 0$.\n       2. Loop $k=K-1$ down to 0:\n          $\\lambda_{k} = M_{k}^{\\top} \\lambda_{k+1}$.\n          If $k \\in \\{\\ell_1, \\dots, \\ell_m\\}$, find the index $i$ such that $k=\\ell_i$, and update: $\\lambda_{k} \\leftarrow \\lambda_{k} + H_i^{\\top} w_i$.\n       This seems slightly off. A more rigorous derivation via the Lagrangian method or direct inspection shows the correct update:\n       1. Initialize adjoint state $\\lambda = 0 \\in \\mathbb{R}^n$.\n       2. Loop backward from $k=K$ down to 1:\n          a. If $k$ is an observation time, $k=\\ell_i$, add forcing: $\\lambda \\leftarrow \\lambda + H_i^{\\top}w_i$.\n          b. Propagate backward: $\\lambda \\leftarrow M_{k-1}^{\\top}\\lambda = (T_{k-1}^{\\top} + \\Delta t \\nabla c(x_{k-1})^{\\top})\\lambda$.\n4.  The final vector $\\lambda$ after the loop completes is the result $u=\\lambda_0 = \\mathrm{J}(x_0)^{\\top}w$.\n\nThis adjoint model run computes the transpose-vector product at a cost also comparable to a single nonlinear model run. The ability to compute the gradient of the cost function (or more generally, $\\mathrm{J}^{\\top}w$ products) efficiently is the cornerstone of practical 4D-Var.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nH_{1} \\left( \\prod_{j=0}^{\\ell_{1}-1} \\left[ T_{j} + \\Delta t \\nabla c(x_{j}) \\right] \\right) v \\\\\n\\vdots \\\\\nH_{m} \\left( \\prod_{j=0}^{\\ell_{m}-1} \\left[ T_{j} + \\Delta t \\nabla c(x_{j}) \\right] \\right) v\n\\end{pmatrix}\n}\n$$", "id": "3365830"}, {"introduction": "A mathematically optimal solution is not always a physically meaningful one, as quantities like emission fluxes must be non-negative. This practice [@problem_id:3365849] introduces you to key methods for incorporating such inequality constraints, including the log-barrier and projected gradient approaches. Understanding these techniques is crucial for ensuring that inversion results remain consistent with physical reality.", "problem": "Consider a single-grid atmospheric chemistry transport inversion for a nonnegative emission flux, where the observation operator is linear. Let $x \\in \\mathbb{R}$ denote the emission flux for the grid cell, and let $y \\in \\mathbb{R}$ denote a single observation that depends linearly on $x$ through a scalar transport sensitivity $h \\in \\mathbb{R}$. Assume additive Gaussian measurement error $\\varepsilon$ with variance $r > 0$, i.e., $y = h\\,x + \\varepsilon$, and a Gaussian prior on $x$ with mean $x_b$ and variance $b > 0$. The Maximum A Posteriori (MAP) estimate is sought under the physical inequality constraint $x \\ge 0$.\n\nStarting from Bayes’ theorem and the stated Gaussian assumptions, derive the unconstrained MAP objective as the sum of a data misfit term and a prior regularization term. Then, justify and derive two approaches to impose the inequality constraint $x \\ge 0$:\n\n1. A log-barrier method that augments the objective with a barrier term that enforces $x > 0$.\n2. A projected gradient method with Karush–Kuhn–Tucker (KKT) conditions, where gradients are projected onto the feasible set $x \\ge 0$.\n\nFor each approach, derive the modified first-order optimality condition appropriate to the constrained problem. Explain what these conditions imply about the constrained MAP solution’s stationarity and feasibility, and how they relate to the physics of nonnegative emissions in atmospheric chemistry transport inversion.\n\nFinally, for the log-barrier formulation, consider the following concrete scalar configuration:\n- Transport sensitivity $h = 0.6$,\n- Observation $y = 3.0$,\n- Measurement error variance $r = 0.25$,\n- Prior mean $x_b = 2.0$,\n- Prior variance $b = 1.0$,\n- Log-barrier parameter $\\mu = 0.1$.\n\nUsing the derived barrier first-order optimality condition, compute the unique positive stationary point $x^\\star(\\mu)$ of the barrier-augmented objective for this configuration. Express your final answer for the emission flux in $\\text{kmol s}^{-1}$ and round your result to four significant figures.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It represents a standard, albeit simplified, formulation of a constrained inverse problem in atmospheric science. All necessary information is provided for a unique solution.\n\nThe objective is to find the Maximum A Posteriori (MAP) estimate of a single-grid emission flux $x \\in \\mathbb{R}$ subject to the nonnegativity constraint $x \\ge 0$. The estimation is based on a linear observation model, Gaussian error statistics, and a Gaussian prior on the flux.\n\nFirst, we derive the unconstrained MAP objective function from Bayes' theorem. The posterior probability distribution of the state $x$ given the observation $y$ is given by Bayes' theorem:\n$$\nP(x|y) \\propto P(y|x)P(x)\n$$\nwhere $P(y|x)$ is the likelihood and $P(x)$ is the prior probability distribution.\n\nThe observation model is $y = hx + \\varepsilon$, where the measurement error $\\varepsilon$ is Gaussian with zero mean and variance $r > 0$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, r)$. The likelihood of observing $y$ given a state $x$ is therefore:\n$$\nP(y|x) = \\frac{1}{\\sqrt{2\\pi r}} \\exp\\left(-\\frac{(y - hx)^2}{2r}\\right)\n$$\nThe prior on the emission flux $x$ is Gaussian with mean $x_b$ and variance $b > 0$, i.e., $x \\sim \\mathcal{N}(x_b, b)$. The prior probability distribution is:\n$$\nP(x) = \\frac{1}{\\sqrt{2\\pi b}} \\exp\\left(-\\frac{(x - x_b)^2}{2b}\\right)\n$$\nSubstituting these into Bayes' theorem, the posterior distribution is:\n$$\nP(x|y) \\propto \\exp\\left(-\\frac{(y - hx)^2}{2r}\\right) \\exp\\left(-\\frac{(x - x_b)^2}{2b}\\right) = \\exp\\left(-\\left[\\frac{(y - hx)^2}{2r} + \\frac{(x - x_b)^2}{2b}\\right]\\right)\n$$\nThe MAP estimate of $x$ is the value that maximizes this posterior probability $P(x|y)$. This is equivalent to minimizing the negative logarithm of the posterior. We define the objective function $J(x)$ as twice the negative logarithm of the posterior, dropping constant terms:\n$$\nJ(x) = \\frac{(hx-y)^2}{r} + \\frac{(x-x_b)^2}{b}\n$$\nConventionally, a factor of $1/2$ is included, yielding:\n$$\nJ(x) = \\frac{1}{2r}(hx - y)^2 + \\frac{1}{2b}(x - x_b)^2\n$$\nThis is the unconstrained MAP objective function. The first term, $\\frac{1}{2r}(hx - y)^2$, is the data misfit term, which penalizes deviations of the model prediction $hx$ from the observation $y$, weighted by the inverse of the measurement error variance $r$. The second term, $\\frac{1}{2b}(x - x_b)^2$, is the prior (or background) regularization term, which penalizes deviations of the solution $x$ from the prior estimate $x_b$, weighted by the inverse of the prior variance $b$.\n\nNext, we impose the physical constraint $x \\ge 0$ using two different approaches.\n\n**1. Log-Barrier Method**\n\nThis method enforces the stricter constraint $x > 0$ by adding a barrier term to the objective function, which penalizes solutions that approach the boundary $x=0$. The augmented objective function $J_\\mu(x)$ includes the barrier term $-\\mu \\ln(x)$, where $\\mu > 0$ is the barrier parameter:\n$$\nJ_\\mu(x) = J(x) - \\mu \\ln(x) = \\frac{1}{2r}(hx - y)^2 + \\frac{(x-x_b)^2}{2b} - \\mu \\ln(x)\n$$\nThe stationary point of this augmented objective is found by setting its first derivative with respect to $x$ to zero. The derivative of the unconstrained objective $J(x)$ is:\n$$\n\\frac{dJ}{dx} = \\frac{1}{r}(hx-y)h + \\frac{1}{b}(x-x_b) = \\left(\\frac{h^2}{r} + \\frac{1}{b}\\right)x - \\left(\\frac{hy}{r} + \\frac{x_b}{b}\\right)\n$$\nThe first-order optimality condition for the barrier-augmented problem is $\\frac{dJ_\\mu}{dx} = 0$:\n$$\n\\frac{dJ_\\mu}{dx} = \\frac{dJ}{dx} - \\frac{\\mu}{x} = 0\n$$\nSubstituting the expression for $\\frac{dJ}{dx}$, we get:\n$$\n\\left(\\frac{h^2}{r} + \\frac{1}{b}\\right)x - \\left(\\frac{hy}{r} + \\frac{x_b}{b}\\right) - \\frac{\\mu}{x} = 0\n$$\nThis is the modified first-order optimality condition for the log-barrier method. The solution $x^\\star(\\mu)$ to this equation is guaranteed to be positive because the term $-\\mu \\ln(x)$ approaches $+\\infty$ as $x \\to 0^+$, effectively creating an infinite potential barrier at the boundary. Physically, this barrier acts as a repulsive force that prevents the emission estimate from becoming non-physical (zero or negative). A sequence of solutions for decreasing $\\mu \\to 0^+$ can be used to approach the true constrained minimum.\n\n**2. Projected Gradient Method with Karush–Kuhn–Tucker (KKT) Conditions**\n\nThis approach tackles the constrained optimization problem $\\min J(x)$ subject to $g(x) = -x \\le 0$ directly. The Karush–Kuhn–Tucker (KKT) conditions provide the necessary conditions for optimality. The Lagrangian is $\\mathcal{L}(x, \\lambda) = J(x) + \\lambda g(x) = J(x) - \\lambda x$, where $\\lambda$ is the Lagrange multiplier. The KKT conditions for a solution $x^\\star$ are:\n1.  **Stationarity**: $\\nabla_x \\mathcal{L}(x^\\star, \\lambda) = \\frac{dJ}{dx}\\bigg|_{x=x^\\star} - \\lambda = 0 \\implies \\lambda = \\frac{dJ}{dx}\\bigg|_{x=x^\\star}$\n2.  **Primal Feasibility**: $x^\\star \\ge 0$\n3.  **Dual Feasibility**: $\\lambda \\ge 0$\n4.  **Complementary Slackness**: $\\lambda x^\\star = 0$\n\nCombining these conditions leads to two distinct cases for the optimal solution $x^\\star$:\n-   Case A: If $x^\\star > 0$, complementary slackness requires $\\lambda=0$. From stationarity, this implies $\\frac{dJ}{dx}\\big|_{x=x^\\star} = 0$. This means the unconstrained minimum of $J(x)$ is already in the feasible region.\n-   Case B: If $x^\\star = 0$, complementary slackness is satisfied. Primal feasibility is met. From stationarity and dual feasibility, we must have $\\lambda = \\frac{dJ}{dx}\\big|_{x=0} \\ge 0$.\n\nLet $x_{unc}$ be the unconstrained minimizer, found by solving $\\frac{dJ}{dx} = 0$. From the expression for the derivative, $x_{unc} = \\left(\\frac{hy}{r} + \\frac{x_b}{b}\\right) / \\left(\\frac{h^2}{r} + \\frac{1}{b}\\right)$. The KKT conditions can be summarized as:\n$$\nx^\\star = \\begin{cases} x_{unc} & \\text{if } x_{unc} \\ge 0 \\\\ 0 & \\text{if } x_{unc} < 0 \\end{cases}\n$$\nThis is equivalent to projecting the unconstrained solution onto the feasible set, $x^\\star = \\max(0, x_{unc})$. Projected gradient methods use this idea iteratively. The first-order optimality condition is that the solution is a fixed point of the projection operator, which can be stated as $x^\\star = P_{[0, \\infty)}(x^\\star - \\alpha \\nabla J(x^\\star))$ for any step size $\\alpha>0$, where $P_{[0, \\infty)}(z) = \\max(0, z)$.\nPhysically, this approach has a clear interpretation. If the combination of observational evidence and prior knowledge ($x_{unc}$) points to a physically plausible positive emission, that value is accepted. If it points to an unphysical negative emission, the estimate is truncated to the most plausible physical value, which is zero, acknowledging a hard physical limit.\n\n**Numerical Calculation for the Log-Barrier Method**\n\nWe are given the following configuration:\n-   $h = 0.6$\n-   $y = 3.0$\n-   $r = 0.25$\n-   $x_b = 2.0$\n-   $b = 1.0$\n-   $\\mu = 0.1$\n\nThe numerical values are assumed to be in a consistent set of units such that the resulting flux $x$ has units of $\\text{kmol s}^{-1}$.\n\nWe use the derived first-order optimality condition for the log-barrier method:\n$$\n\\left(\\frac{h^2}{r} + \\frac{1}{b}\\right)x - \\left(\\frac{hy}{r} + \\frac{x_b}{b}\\right) - \\frac{\\mu}{x} = 0\n$$\nFirst, we compute the coefficients:\n$$\n\\frac{h^2}{r} = \\frac{0.6^2}{0.25} = \\frac{0.36}{0.25} = 1.44\n$$\n$$\n\\frac{1}{b} = \\frac{1}{1.0} = 1.0\n$$\n$$\n\\frac{hy}{r} = \\frac{0.6 \\times 3.0}{0.25} = \\frac{1.8}{0.25} = 7.2\n$$\n$$\n\\frac{x_b}{b} = \\frac{2.0}{1.0} = 2.0\n$$\nSubstituting these values, along with $\\mu=0.1$, into the optimality condition yields:\n$$\n(1.44 + 1.0)x - (7.2 + 2.0) - \\frac{0.1}{x} = 0\n$$\n$$\n2.44x - 9.2 - \\frac{0.1}{x} = 0\n$$\nTo find the positive stationary point $x^\\star(\\mu)$, we multiply by $x$ (since $x > 0$) to obtain a quadratic equation:\n$$\n2.44x^2 - 9.2x - 0.1 = 0\n$$\nWe solve this equation for $x$ using the quadratic formula, $x = \\frac{-B \\pm \\sqrt{B^2 - 4AC}}{2A}$, with $A = 2.44$, $B = -9.2$, and $C = -0.1$.\n$$\nx = \\frac{9.2 \\pm \\sqrt{(-9.2)^2 - 4(2.44)(-0.1)}}{2(2.44)}\n$$\n$$\nx = \\frac{9.2 \\pm \\sqrt{84.64 + 0.976}}{4.88}\n$$\n$$\nx = \\frac{9.2 \\pm \\sqrt{85.616}}{4.88}\n$$\n$$\nx \\approx \\frac{9.2 \\pm 9.25289}{4.88}\n$$\nWe are seeking a positive solution ($x > 0$), so we must take the positive root:\n$$\nx^\\star(\\mu) \\approx \\frac{9.2 + 9.25289}{4.88} = \\frac{18.45289}{4.88} \\approx 3.78133\n$$\nThe problem requires rounding the result to four significant figures.\n$$\nx^\\star(\\mu) \\approx 3.781\n$$", "answer": "$$\\boxed{3.781}$$", "id": "3365849"}]}