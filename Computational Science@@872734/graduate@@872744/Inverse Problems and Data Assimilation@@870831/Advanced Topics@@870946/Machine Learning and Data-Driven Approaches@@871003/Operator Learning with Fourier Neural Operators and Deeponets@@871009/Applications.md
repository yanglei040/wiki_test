## Applications and Interdisciplinary Connections

The preceding chapters have established the theoretical foundations and mechanistic principles of neural operators, particularly Fourier Neural Operators (FNOs) and Deep Operator Networks (DeepONets). We now pivot from principles to practice, exploring how these architectures are deployed to address complex, real-world problems across a spectrum of scientific and engineering disciplines. This chapter will not reteach the core concepts but will instead demonstrate their utility, versatility, and integration into sophisticated computational workflows. We will see that neural operators are not merely function approximators but are powerful tools that can be synergistically combined with classical methods in numerical analysis, [inverse problems](@entry_id:143129), [data assimilation](@entry_id:153547), and optimal control to create novel and efficient solutions.

### Surrogate Modeling for Physical Systems

A primary and transformative application of [operator learning](@entry_id:752958) is the creation of *[surrogate models](@entry_id:145436)* for complex physical systems. Many such systems are described by partial differential equations (PDEs), and their [numerical simulation](@entry_id:137087) can be computationally prohibitive, especially in many-query contexts such as [uncertainty quantification](@entry_id:138597), design optimization, or [real-time control](@entry_id:754131). Neural operators can be trained on data from high-fidelity solvers to learn the solution operator of a PDE, providing a fast and accurate approximation that can be evaluated in milliseconds.

#### Application in Fluid Dynamics

The simulation of fluid flows, governed by the Navier-Stokes equations, is a grand challenge in computational science. Before one can confidently apply a learning-based approach, it is crucial to establish that the problem is mathematically well-posed in an operator-theoretic sense. For the incompressible Navier-Stokes equations in two dimensions on a periodic domain, it is a cornerstone result of [mathematical analysis](@entry_id:139664) that the solution map, which takes an [initial velocity](@entry_id:171759) field and a [forcing term](@entry_id:165986) to the [velocity field](@entry_id:271461) at a later time, is a well-defined and [continuous operator](@entry_id:143297) between appropriate Sobolev spaces. For instance, for sufficiently smooth initial data (e.g., in the Sobolev space $H^s$ for $s \ge 1$), a unique, global-in-time "strong" solution exists and remains in that space. Similarly, for less regular initial data (e.g., in the space $L^2$), a unique, global "weak" solution is guaranteed. This well-posedness provides the rigorous mathematical justification for treating the complex, nonlinear evolution of a fluid as a learnable operator, paving the way for the application of architectures like FNOs and DeepONets to a vast range of problems in turbulence and [geophysical fluid dynamics](@entry_id:150356). [@problem_id:3407184]

#### Discretization Invariance and Generalization

One of the most celebrated properties of the Fourier Neural Operator is its *[discretization](@entry_id:145012)-invariant* nature. Because the FNO parameterizes the operator directly in the frequency domain, the same learned spectral kernel can be applied to functions discretized on different grids. This is a significant advantage in many scientific applications where data may be available at multiple resolutions. For instance, in geoscience applications such as modeling Darcy flow in porous media, we might train a [surrogate model](@entry_id:146376) on coarse-resolution simulation data but wish to apply it to a finer grid for more detailed predictions.

The architectural differences between FNOs and DeepONets lead to distinct cross-resolution generalization behaviors. An FNO trained on a coarse grid with $N_{\mathrm{tr}}$ points learns spectral multipliers for wavenumbers resolved by that grid. When evaluated on a finer grid with $N_{\mathrm{te}} > N_{\mathrm{tr}}$ points, it can apply the learned multipliers to the corresponding low-frequency modes and typically uses a residual connection ([identity mapping](@entry_id:634191)) for the new, higher-frequency modes. A DeepONet, by contrast, typically employs a trunk network that represents a fixed basis of functions (e.g., the first $K_{\mathrm{trunk}}$ Fourier modes). Its generalization is limited by the expressive capacity of this fixed basis. A principled analysis, grounded in the spectral properties of the underlying PDE, can quantify the "spectral [truncation error](@entry_id:140949)"—a lower bound on the model's prediction error—that arises from these architectural limitations. Such analysis reveals how the smoothness of the problem's solution and the chosen truncation scheme dictate the performance of the learned surrogate when moving between resolutions. [@problem_id:3407227]

#### Learning Parametric Operator Families on Variable Geometries

The power of [surrogate modeling](@entry_id:145866) is greatly enhanced if a single neural operator can learn an entire *family* of operators, parameterized by physical coefficients or even the geometry of the domain itself. Consider an elliptic PDE where the diffusion coefficient $\kappa(x)$ and the domain $\Omega$ can vary. A neural operator can be trained to learn the map from the triplet $(u, \kappa, \Omega)$ to the solution $y$, where $u$ is the source term. The DeepONet architecture is particularly well-suited for this task due to its flexible input structure. The key is to separate the inputs that define the problem instance from the inputs that define the spatial query location.

A principled extension is to feed all instance-defining parameters into the branch network, and all location-dependent parameters into the trunk network. For example, sensor measurements of the [source term](@entry_id:269111) $u$, a finite-dimensional representation of the coefficient field $\kappa$, and a global descriptor of the domain $\Omega$ would be provided as input to the branch network. The trunk network would receive the query coordinate $x$ along with local geometric features, such as the signed distance to the boundary. This design allows the branch network to compute coefficients specific to the PDE instance, while the trunk network generates a geometry-aware basis. This architecture enables a single DeepONet to generalize across a compact family of coefficients and domains, making it a powerful tool for applications involving design optimization or uncertainty quantification where these parameters are variable. [@problem_id:3407225]

Many real-world problems are posed on irregular domains, which presents a challenge for grid-based methods like FNOs. Several strategies exist to address this. One approach is to embed the irregular domain in a regular grid and use a binary mask, but the global nature of Fourier convolution can lead to information "leaking" across the boundary, causing artifacts. A second, increasingly popular approach is to discretize the domain as a graph or mesh and use [graph neural networks](@entry_id:136853), which are naturally suited to irregular structures but have a different [inductive bias](@entry_id:137419) (permutation equivariance) compared to the [translation equivariance](@entry_id:634519) of convolutions. A third, highly effective strategy, compatible with both FNOs and DeepONets, is to explicitly provide geometric information to the network. Augmenting the input with the [signed distance function](@entry_id:144900) (SDF) of the domain provides a powerful geometric prior that helps the network learn boundary conditions and local behavior, significantly improving [sample efficiency](@entry_id:637500) and accuracy. [@problem_id:3407267]

### Operator Learning for Inverse Problems and Data Assimilation

While [surrogate modeling](@entry_id:145866) of [forward problems](@entry_id:749532) is a powerful application, the true potential of [operator learning](@entry_id:752958) is often realized when it is applied to *inverse problems*. Here, the goal is not to predict the effect from the cause, but to infer the unknown cause (e.g., system parameters, [initial conditions](@entry_id:152863)) from indirect, sparse, and noisy observations of the effect. Neural operators can be integrated into [inverse problem](@entry_id:634767) workflows in several sophisticated ways.

#### Learning Regularized Inverse Operators

Many [inverse problems](@entry_id:143129) are mathematically ill-posed, meaning a small amount of noise in the observations can lead to large, unphysical errors in the solution. A canonical example is the [backward heat equation](@entry_id:164111), where one seeks to recover a past state from a current one; this problem is notoriously unstable as small, high-frequency noise components are exponentially amplified. The standard remedy is *regularization*, which incorporates prior knowledge to stabilize the inversion.

Instead of learning the (unstable) forward operator, one can train a neural operator to directly approximate the *regularized inverse operator*. Classical methods like Tikhonov regularization or Bayesian MAP estimation can be formulated as applying specific spectral filters to the data. For instance, the renowned Wiener filter, which is the Bayes-optimal linear estimator under Gaussian assumptions, acts by weighting each mode based on its [signal-to-noise ratio](@entry_id:271196). It optimally suppresses noise-dominated modes (often those with small singular values in the forward operator) while preserving signal-dominated modes. A neural operator, such as a DeepONet with a learned spectral gating function, can be trained on data to learn this optimal, regularized inverse map, effectively discovering the principles of regularization from examples. This approach directly tackles the [ill-posedness](@entry_id:635673) and provides a computationally efficient path from observation to a stable reconstruction. [@problem_id:3407263] [@problem_id:3407189]

#### Physics-Informed and Unsupervised Methodologies

A significant challenge in [scientific machine learning](@entry_id:145555) is the frequent scarcity of labeled training data (i.e., known input-solution pairs). Operator learning offers powerful paradigms to learn from incomplete information by leveraging knowledge of the underlying physics.

One such paradigm is *[amortized variational inference](@entry_id:746415)*. Here, a neural operator is trained to directly output the solution of a regularized [inverse problem](@entry_id:634767), not by learning from known solutions, but by learning to minimize the variational objective (e.g., a Tikhonov functional) itself. The training [loss function](@entry_id:136784) for the operator's parameters $\theta$ becomes the variational functional evaluated at the operator's output. This "unsupervised" approach learns a map that amortizes the cost of the optimization, enabling near-instantaneous solutions for new observations. [@problem_id:3407259]

Another powerful technique is to incorporate a *physics-informed penalty* into the training loss. Even with only sparse sensor data, we can enforce that the learned operator's output should satisfy the governing PDE. This is achieved by adding a term to the loss that penalizes the PDE residual, evaluated at a set of collocation points. A Bayesian framework provides a principled way to set the relative weight of the data-misfit term and the physics-residual term. The optimal weight is directly related to the variances of the observation noise and the assumed prior on the PDE residual, providing a clear statistical interpretation for this crucial hyperparameter. [@problem_id:3407199]

The choice of the data-misfit [loss function](@entry_id:136784) itself is a form of regularization. A standard $L^2$ loss weights all error frequencies equally. An $H^1$ loss, which includes a penalty on the gradient of the error, is equivalent to weighting the error in the Fourier domain by a factor of $(1+|k|^2)$. This more heavily penalizes high-frequency errors, promoting smoother solutions that are more robust to high-frequency noise. Using an $H^1$ loss is thus analogous to introducing a smoothness prior, a common practice in the theory of [inverse problems](@entry_id:143129). [@problem_id:3407197]

#### Applications in Large-Scale Data Assimilation

Data assimilation, the process of fusing time-evolving model forecasts with incoming observations, is a massive-scale inverse problem central to fields like weather forecasting and [oceanography](@entry_id:149256). Neural operators are emerging as transformative tools in this domain.

In [variational methods](@entry_id:163656) like 4D-Var, the goal is to find the optimal initial condition that minimizes a [cost function](@entry_id:138681) measuring the misfit to a prior estimate and to all observations over a time window. This requires repeatedly running a (typically very expensive) [forward model](@entry_id:148443) and its adjoint. A learned, differentiable surrogate operator can replace the forward model. The 4D-Var [cost function](@entry_id:138681) is then formulated with the learned operator, and its gradient is computed via backpropagation through the surrogate's architecture. The consistency of this approach relies on the surrogate being a sufficiently accurate and smooth approximation of the true dynamics, with theoretical guarantees for the convergence of the optimization resting on concepts like epi-convergence from variational analysis. [@problem_id:3407240]

The integration of [operator learning](@entry_id:752958) into data assimilation can be illustrated with a complete workflow. For instance, an FNO can be used as a fast forecast model. Given an initial state, the FNO predicts the state at the next time step. This forecast is then combined with sparse, irregular observations using a standard [data assimilation](@entry_id:153547) algorithm, such as 3D-Var or an Ensemble Kalman Filter (EnKF), to produce an improved "analysis" state. This cycle of forecast and analysis can be repeated, and the [discretization](@entry_id:145012)-invariant properties of the FNO are particularly valuable for assimilating multiresolution data sources. [@problem_id:3407194]

When dealing with [chaotic systems](@entry_id:139317), such as the Lorenz-96 model often used as a testbed for [atmospheric dynamics](@entry_id:746558), the stability of the learned surrogate is paramount. Two distinct philosophies can be compared: directly learning the nonlinear [evolution operator](@entry_id:182628) (an FNO-style approach), or attempting to find a [coordinate transformation](@entry_id:138577) in which the dynamics become linear (a Koopman operator approach, which can be implemented with a DeepONet). When these learned models are used recursively inside an EnKF, even small errors can accumulate and lead to catastrophic [filter divergence](@entry_id:749356). Assessing the stability of the forecast-[error covariance](@entry_id:194780) is thus a critical step in validating a learned operator for use in data assimilation. [@problem_id:3407212]

### Advanced Topics and Future Directions

The integration of [operator learning](@entry_id:752958) with classical computational methods opens up numerous advanced application domains and research frontiers.

#### Optimal Control and Design Optimization

In PDE-constrained optimal control, the objective is to find a control function (e.g., a heat source distribution) that steers the system state to a desired target, typically by minimizing a [cost functional](@entry_id:268062). This is an optimization problem that, like 4D-Var, requires many forward and adjoint PDE solves. By replacing the PDE solver with a learned operator, the optimization process can be dramatically accelerated. However, this introduces a mismatch between the true gradient of the [cost function](@entry_id:138681) and the gradient computed through the surrogate. The stability and convergence of the optimization process then depend on the magnitude of this gradient mismatch relative to the properties of the true optimization landscape, such as its [strong convexity](@entry_id:637898). The accuracy of the learned [adjoint operator](@entry_id:147736) is particularly critical for the success of these methods. [@problem_id:3407273]

#### Multiscale Modeling and Closure Problems

Many critical systems in science, from turbulent flows to climate models, involve interactions across a vast range of spatial and temporal scales. Direct [numerical simulation](@entry_id:137087) of all scales is often impossible. A common strategy is to solve for the large, resolved scales while modeling the average effect of the small, unresolved scales through a *closure* term. Learning this closure operator is a major challenge and a promising application for [operator learning](@entry_id:752958). For example, a DeepONet can be trained to map the state of the resolved scales to the corresponding closure term. However, the introduction of a learned, and therefore imperfect, closure can have subtle downstream consequences. For example, even a closure that provides accurate forward predictions might alter the mathematical properties of the system, potentially degrading the uniqueness or stability of an associated inverse or data assimilation problem. [@problem_id:3407190]

#### Learning of Integral Kernels

For linear PDEs, the solution can be expressed via an [integral transform](@entry_id:195422) with a kernel, known as the Green's function. A powerful strategy is to train a neural operator to learn this Green's function directly. This can be accomplished by training the network on its response to a series of approximate impulses (delta functions) located at different points in the domain. Once the Green's function is learned, the solution for any arbitrary [forcing term](@entry_id:165986) can be computed by integration. This concept finds a direct analogy in other fields. In imaging science, for example, the "forward operator" is often a convolution with a [point spread function](@entry_id:160182) (PSF). Learning the operator is equivalent to [system identification](@entry_id:201290), or learning the PSF. For space-variant imaging systems, the operator kernel depends on the spatial location, which is analogous to a PDE with spatially varying coefficients. A learned operator can capture this variance and be used within a variational framework to solve challenging deconvolution inverse problems. [@problem_id:3407256]

In conclusion, Fourier Neural Operators and Deep Operator Networks represent a paradigm shift in [scientific computing](@entry_id:143987). They move beyond the traditional role of neural networks as universal function approximators to become universal *operator* approximators. As we have seen, their applications are not limited to being simple replacements for PDE solvers. They can be deeply and rigorously integrated into the frameworks of [inverse problems](@entry_id:143129), data assimilation, and [optimal control](@entry_id:138479), leveraging physical knowledge through physics-informed losses, encoding priors through architectural choices, and enabling the rapid solution of previously intractable many-query problems. The ongoing fusion of [operator learning](@entry_id:752958) with classical [applied mathematics](@entry_id:170283) promises to continue fueling innovation across science and engineering.