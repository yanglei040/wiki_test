{"hands_on_practices": [{"introduction": "The power of Fourier Neural Operators (FNOs) lies in their ability to learn global operators efficiently. This exercise delves into the core of this efficiency by analyzing the computational complexity of an FNO layer [@problem_id:3407231]. By contrasting the Fast Fourier Transform (FFT) based implementation with a standard real-space convolution, you will derive the quasi-linear scaling that makes FNOs a powerful tool for resolution-invariant learning on fine grids.", "problem": "Consider a one-dimensional periodic domain with $N$ equispaced grid points and a Fourier Neural Operator (FNO) layer mapping $C_{\\mathrm{in}}$ input channels to $C_{\\mathrm{out}}$ output channels. The FNO layer is implemented in the frequency domain: it computes a Discrete Fourier Transform (DFT) of each input channel via the Fast Fourier Transform (FFT), multiplies a learned complex-valued spectral weight matrix over $M$ retained Fourier modes, and computes the inverse FFT to return to real space. Assume the following standard and widely validated cost models: each forward or inverse FFT on a length-$N$ sequence costs $\\alpha N \\log_{2} N$ floating-point operations (flops), independent of channel index; the spectral multiplication across retained modes costs $\\beta M C_{\\mathrm{in}} C_{\\mathrm{out}}$ flops per application and does not depend on $N$. For comparison, consider a real-space convolutional implementation of an operator layer with kernel width $k$ (a compact stencil of size $k$ per input-output channel pair), where the per-grid-point cost per output channel is proportional to the number of input channels and stencil width. In particular, assume the real-space convolution costs $\\gamma N k C_{\\mathrm{in}} C_{\\mathrm{out}}$ flops.\n\nStarting only from the linearity of the DFT and the convolution theorem, together with the above cost models, do the following:\n\n(a) Derive the leading-order flop counts for the FNO-layer implementation in the frequency domain and for the real-space convolutional implementation, expressing both as functions of $N$, $C_{\\mathrm{in}}$, $C_{\\mathrm{out}}$, $k$, and the constants $\\alpha$, $\\beta$, $\\gamma$. Your final expressions must use the natural logarithm $\\ln$ and must explicitly account for the base conversion $\\log_{2} N = \\ln N / \\ln 2$. Neglect lower-order additive terms in $N$ when presenting the leading-order dependence on $N$.\n\n(b) Under the assumption that $M$ is independent of $N$ and that the spectral multiplication term is strictly lower order than the FFT terms in the large-$N$ limit, compute the exact closed-form analytic expression for the crossover grid size $N^{\\star}$ at which the leading-order flop counts of the frequency-domain FNO implementation and the real-space convolutional implementation are equal. Express $N^{\\star}$ as a single expression in terms of $\\alpha$, $\\gamma$, $k$, $C_{\\mathrm{in}}$, $C_{\\mathrm{out}}$, and $\\ln 2$. Use the natural exponential $\\exp(\\cdot)$ and do not include any units. The final answer must be a single closed-form analytic expression.", "solution": "The problem requires the derivation and comparison of the computational costs, measured in floating-point operations (flops), for two types of neural operator layers: a Fourier Neural Operator (FNO) layer implemented in the frequency domain and a standard convolutional layer implemented in real space.\n\n### Part (a): Derivation of Leading-Order Flop Counts\n\nFirst, we derive the leading-order flop count for each implementation as a function of the given parameters.\n\n**Fourier Neural Operator (FNO) Layer Cost**\n\nThe FNO layer computation involves three main steps. The total flop count, denoted as $F_{\\mathrm{FNO}}$, is the sum of the costs of these steps.\n\n1.  **Forward Fourier Transform**: The input consists of $C_{\\mathrm{in}}$ channels, and each channel is a sequence of length $N$. A Fast Fourier Transform (FFT) is applied to each channel. The cost for a single FFT is given as $\\alpha N \\log_{2}(N)$. Due to the linearity of the Discrete Fourier Transform (DFT), we can process each of the $C_{\\mathrm{in}}$ input channels independently.\n    The total cost for this step is the number of channels multiplied by the cost per channel:\n    $$C_{\\mathrm{FFT}} = C_{\\mathrm{in}} \\alpha N \\log_{2}(N)$$\n\n2.  **Spectral Multiplication**: In the frequency domain, the $C_{\\mathrm{in}}$ transformed channels are mapped to $C_{\\mathrm{out}}$ output channels by multiplying with a learned weight matrix over $M$ retained Fourier modes. The cost for this operation is explicitly given and is independent of the grid size $N$:\n    $$C_{\\mathrm{mul}} = \\beta M C_{\\mathrm{in}} C_{\\mathrm{out}}$$\n\n3.  **Inverse Fourier Transform**: After the spectral multiplication, we have $C_{\\mathrm{out}}$ channels in the frequency domain. To return to the physical domain, an inverse FFT (IFFT) is applied to each of these channels. The cost of an IFFT is assumed to be the same as a forward FFT, $\\alpha N \\log_{2}(N)$.\n    The total cost for this step is:\n    $$C_{\\mathrm{IFFT}} = C_{\\mathrm{out}} \\alpha N \\log_{2}(N)$$\n\nThe total flop count for the FNO layer is the sum of these three costs:\n$$F_{\\mathrm{FNO}} = C_{\\mathrm{FFT}} + C_{\\mathrm{mul}} + C_{\\mathrm{IFFT}}$$\n$$F_{\\mathrm{FNO}} = C_{\\mathrm{in}} \\alpha N \\log_{2}(N) + \\beta M C_{\\mathrm{in}} C_{\\mathrm{out}} + C_{\\mathrm{out}} \\alpha N \\log_{2}(N)$$\nCombining the terms dependent on $N$:\n$$F_{\\mathrm{FNO}} = \\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}}) N \\log_{2}(N) + \\beta M C_{\\mathrm{in}} C_{\\mathrm{out}}$$\n\nThe problem asks for the leading-order flop count in $N$. The term $\\beta M C_{\\mathrm{in}} C_{\\mathrm{out}}$ is constant with respect to $N$, while the first term grows as $N \\ln(N)$. Therefore, the leading-order term is $\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}}) N \\log_{2}(N)$. We must convert the base-$2$ logarithm to the natural logarithm using the identity $\\log_{2}(N) = \\frac{\\ln(N)}{\\ln(2)}$.\n\nThe leading-order flop count for the FNO, $F_{\\mathrm{FNO, leading}}$, is:\n$$F_{\\mathrm{FNO, leading}} = \\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}}) N \\left(\\frac{\\ln(N)}{\\ln(2)}\\right) = \\frac{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})}{\\ln(2)} N \\ln(N)$$\n\n**Real-Space Convolutional Layer Cost**\n\nThe cost for the real-space convolutional layer with a kernel of width $k$ is given directly in the problem statement. This operation's cost is linear in the grid size $N$.\nThe total flop count for the convolution, $F_{\\mathrm{conv}}$, is:\n$$F_{\\mathrm{conv}} = \\gamma N k C_{\\mathrm{in}} C_{\\mathrm{out}}$$\nThis expression is the leading-order (and only) term in the cost model provided.\n\n### Part (b): Crossover Grid Size $N^{\\star}$\n\nThe crossover grid size, $N^{\\star}$, is the value of $N$ at which the leading-order flop counts of the two implementations are equal. This is found by setting $F_{\\mathrm{FNO, leading}} = F_{\\mathrm{conv}}$ and solving for $N = N^{\\star}$.\n\nThe assumption that the spectral multiplication term is strictly lower order than the FFT terms for large $N$ has already been used in determining $F_{\\mathrm{FNO, leading}}$.\n\nSetting the two leading-order cost expressions equal:\n$$F_{\\mathrm{FNO, leading}}(N^{\\star}) = F_{\\mathrm{conv}}(N^{\\star})$$\n$$\\frac{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})}{\\ln(2)} N^{\\star} \\ln(N^{\\star}) = \\gamma N^{\\star} k C_{\\mathrm{in}} C_{\\mathrm{out}}$$\n\nSince the grid size $N^{\\star}$ must be a positive integer, we can divide both sides by $N^{\\star}$ (for $N^{\\star} \\neq 0$):\n$$\\frac{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})}{\\ln(2)} \\ln(N^{\\star}) = \\gamma k C_{\\mathrm{in}} C_{\\mathrm{out}}$$\n\nNow, we solve for $\\ln(N^{\\star})$ by isolating it on one side of the equation:\n$$\\ln(N^{\\star}) = \\frac{\\gamma k C_{\\mathrm{in}} C_{\\mathrm{out}} \\ln(2)}{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})}$$\n\nTo find $N^{\\star}$, we take the exponential of both sides:\n$$N^{\\star} = \\exp\\left( \\frac{\\gamma k C_{\\mathrm{in}} C_{\\mathrm{out}} \\ln(2)}{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})} \\right)$$\n\nThis is the final closed-form analytic expression for the crossover grid size $N^{\\star}$.", "answer": "$$\n\\boxed{\\exp\\left(\\frac{\\gamma k C_{\\mathrm{in}} C_{\\mathrm{out}} \\ln(2)}{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})}\\right)}\n$$", "id": "3407231"}, {"introduction": "Deep Operator Networks (DeepONets) employ a unique architecture composed of a branch network to encode the input function and a trunk network to process query locations. Understanding how to train such a model requires a clear grasp of how gradients flow back to the parameters of each network. This practice guides you through the fundamental derivation of the backpropagation equations for a DeepONet, starting from a standard mean-squared error loss function [@problem_id:3407183]. Completing this exercise will solidify your understanding of the training dynamics within this powerful operator learning framework.", "problem": "Consider the task of learning a nonlinear operator that maps an input function $u$ to an output function $y$ in the setting of inverse problems and data assimilation. A Deep Operator Network (DeepONet) parameterizes the operator via a branch network and a trunk network as follows. The branch network encodes the discretized input $u$ into a coefficient vector $b(u; \\theta_{b}) \\in \\mathbb{R}^{p}$, and the trunk network encodes the query location $x$ into a basis vector $t(x; \\theta_{t}) \\in \\mathbb{R}^{p}$. The operator output is defined by $G(u)(x) = b(u; \\theta_{b})^{\\top} t(x; \\theta_{t})$, which is a bilinear form in the branch and trunk embeddings. You are given a training dataset of $N$ discretized input–output function pairs $\\{(u_{i}, y_{i})\\}_{i=1}^{N}$, where each $y_{i}$ is observed at $M$ query locations $\\{x_{j}\\}_{j=1}^{M}$. The training objective is the quadrature-weighted empirical mean-squared loss\n$$\nL(\\theta_{b}, \\theta_{t}) = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right)^{2},\n$$\nwhere $w_{j} > 0$ are given quadrature weights over the query locations. Assume $b(u; \\theta_{b})$ and $t(x; \\theta_{t})$ are differentiable with respect to $\\theta_{b}$ and $\\theta_{t}$, respectively, and denote the Jacobians $J_{b}(u; \\theta_{b}) = \\frac{\\partial b(u; \\theta_{b})}{\\partial \\theta_{b}} \\in \\mathbb{R}^{p \\times n_{b}}$ and $J_{t}(x; \\theta_{t}) = \\frac{\\partial t(x; \\theta_{t})}{\\partial \\theta_{t}} \\in \\mathbb{R}^{p \\times n_{t}}$, where $n_{b}$ and $n_{t}$ are the dimensions of the parameter vectors $\\theta_{b}$ and $\\theta_{t}$. Starting from first principles, namely the chain rule of differentiation and the definition of the mean-squared loss, derive the backpropagation expressions for the gradients $\\nabla_{\\theta_{b}} L$ and $\\nabla_{\\theta_{t}} L$ in closed form in terms of the Jacobians $J_{b}$ and $J_{t}$, the embeddings $b(u_{i}; \\theta_{b})$ and $t(x_{j}; \\theta_{t})$, and the residuals $G(u_{i})(x_{j}) - y_{i}(x_{j})$. Your final answer must be a single analytic expression collecting both gradients in a single row matrix using the LaTeX $\\texttt{pmatrix}$ environment, containing no units. Do not perform any numerical approximation or rounding.", "solution": "The problem requires the derivation of the gradients of the DeepONet loss function with respect to the parameters of the branch network, $\\theta_{b}$, and the trunk network, $\\theta_{t}$. The derivation will proceed from first principles, applying the chain rule of multivariable calculus to the provided loss function.\n\nThe loss function is given as the quadrature-weighted empirical mean-squared error:\n$$\nL(\\theta_{b}, \\theta_{t}) = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right)^{2}\n$$\nwhere the DeepONet operator is defined as $G(u_{i})(x_{j}) = b(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t})$. For clarity, let us denote the residual for the $i$-th input and $j$-th query location as $r_{ij} = G(u_{i})(x_{j}) - y_{i}(x_{j})$. The loss function can then be written as:\n$$\nL(\\theta_{b}, \\theta_{t}) = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} r_{ij}^{2}\n$$\nWe need to compute the gradients $\\nabla_{\\theta_{b}} L = \\frac{\\partial L}{\\partial \\theta_{b}}$ and $\\nabla_{\\theta_{t}} L = \\frac{\\partial L}{\\partial \\theta_{t}}$. These are column vectors of dimensions $n_{b} \\times 1$ and $n_{t} \\times 1$, respectively.\n\nFirst, let's derive the gradient with respect to the branch network parameters, $\\theta_{b}$.\nUsing the chain rule, the gradient of the loss function $L$ with respect to $\\theta_{b}$ is:\n$$\n\\nabla_{\\theta_{b}} L = \\frac{\\partial L}{\\partial \\theta_{b}} = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\frac{\\partial}{\\partial \\theta_{b}} (r_{ij}^{2})\n$$\nThe derivative of the squared residual is $\\frac{\\partial}{\\partial \\theta_{b}} (r_{ij}^{2}) = 2 r_{ij} \\frac{\\partial r_{ij}}{\\partial \\theta_{b}}$. Substituting this into the equation gives:\n$$\n\\nabla_{\\theta_{b}} L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} r_{ij} \\frac{\\partial r_{ij}}{\\partial \\theta_{b}}\n$$\nNow, we compute the derivative of the residual $r_{ij} = b(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t}) - y_{i}(x_{j})$ with respect to $\\theta_{b}$. The term $y_{i}(x_{j})$ does not depend on $\\theta_{b}$, so its derivative is zero.\n$$\n\\frac{\\partial r_{ij}}{\\partial \\theta_{b}} = \\frac{\\partial}{\\partial \\theta_{b}} \\left( b(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t}) \\right)\n$$\nTo evaluate this gradient, we can use the identity for the derivative of a scalar product with respect to a vector. Given vectors $\\mathbf{a}$ and $\\mathbf{c}$, where $\\mathbf{a}$ is a function of a vector $\\mathbf{x}$, the gradient of their dot product is $\\nabla_{\\mathbf{x}}(\\mathbf{a}(\\mathbf{x})^\\top \\mathbf{c}) = (\\frac{\\partial \\mathbf{a}(\\mathbf{x})}{\\partial \\mathbf{x}})^\\top \\mathbf{c}$. Here, $\\mathbf{a}$ corresponds to $b(u_{i}; \\theta_{b})$, $\\mathbf{c}$ to $t(x_{j}; \\theta_{t})$, and $\\mathbf{x}$ to $\\theta_{b}$. The term $\\frac{\\partial b(u_{i}; \\theta_{b})}{\\partial \\theta_{b}}$ is the Jacobian matrix $J_{b}(u_{i}; \\theta_{b}) \\in \\mathbb{R}^{p \\times n_{b}}$ as defined in the problem. Thus,\n$$\n\\frac{\\partial r_{ij}}{\\partial \\theta_{b}} = J_{b}(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t})\n$$\nThis result is a column vector of dimension $n_{b} \\times 1$. Substituting this back into the expression for $\\nabla_{\\theta_{b}} L$:\n$$\n\\nabla_{\\theta_{b}} L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right) J_{b}(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t})\n$$\n\nNext, we derive the gradient with respect to the trunk network parameters, $\\theta_{t}$. The procedure is symmetric to the derivation for $\\theta_{b}$.\n$$\n\\nabla_{\\theta_{t}} L = \\frac{\\partial L}{\\partial \\theta_{t}} = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} r_{ij} \\frac{\\partial r_{ij}}{\\partial \\theta_{t}}\n$$\nWe compute the derivative of the residual $r_{ij}$ with respect to $\\theta_{t}$:\n$$\n\\frac{\\partial r_{ij}}{\\partial \\theta_{t}} = \\frac{\\partial}{\\partial \\theta_{t}} \\left( b(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t}) \\right)\n$$\nIn this case, the vector $b(u_{i}; \\theta_{b})$ is constant with respect to $\\theta_{t}$. Using a similar vector calculus identity, $\\nabla_{\\mathbf{x}}(\\mathbf{c}^\\top \\mathbf{a}(\\mathbf{x})) = (\\frac{\\partial \\mathbf{a}(\\mathbf{x})}{\\partial \\mathbf{x}})^\\top \\mathbf{c}$, where now $\\mathbf{a}$ corresponds to $t(x_{j}; \\theta_{t})$, $\\mathbf{c}$ to $b(u_{i}; \\theta_{b})$, and $\\mathbf{x}$ to $\\theta_{t}$. The term $\\frac{\\partial t(x_{j}; \\theta_{t})}{\\partial \\theta_{t}}$ is the Jacobian $J_{t}(x_{j}; \\theta_{t}) \\in \\mathbb{R}^{p \\times n_{t}}$. So,\n$$\n\\frac{\\partial r_{ij}}{\\partial \\theta_{t}} = J_{t}(x_{j}; \\theta_{t})^{\\top} b(u_{i}; \\theta_{b})\n$$\nThis is a column vector of dimension $n_{t} \\times 1$. Substituting this into the expression for $\\nabla_{\\theta_{t}} L$:\n$$\n\\nabla_{\\theta_{t}} L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right) J_{t}(x_{j}; \\theta_{t})^{\\top} b(u_{i}; \\theta_{b})\n$$\nThese two expressions for $\\nabla_{\\theta_{b}} L$ and $\\nabla_{\\theta_{t}} L$ are the final closed-form backpropagation formulas for the gradients, expressed in terms of the specified quantities. The problem asks for these to be collected into a single row matrix.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right) J_{b}(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t})  \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right) J_{t}(x_{j}; \\theta_{t})^{\\top} b(u_{i}; \\theta_{b}) \\end{pmatrix}}\n$$", "id": "3407183"}, {"introduction": "Applying operator learners to solve partial differential equations (PDEs) often requires satisfying specific boundary conditions, a crucial aspect of well-posed physical problems. This exercise introduces a sophisticated and widely used method for enforcing Dirichlet boundary conditions by construction, known as the 'lifting' technique [@problem_id:3407257]. You will reparameterize the model's output to automatically satisfy the given boundary data, simplifying the training objective and leading to more accurate and physically consistent solutions.", "problem": "Consider a bounded Lipschitz domain $\\Omega \\subset \\mathbb{R}^{d}$ with boundary $\\partial \\Omega$. Let $\\mathcal{A}_{a}$ denote a linear, uniformly elliptic differential operator parametrized by a coefficient field $a \\in \\mathcal{A}$, and consider the boundary value problem with Dirichlet data\n$$\n\\mathcal{A}_{a} u = f \\quad \\text{in } \\Omega, \n\\qquad\nu = g \\quad \\text{on } \\partial \\Omega,\n$$\nfor inputs $(a,f,g)$ drawn from an application-specific distribution supported on $\\mathcal{A} \\times L^{2}(\\Omega) \\times H^{1/2}(\\partial \\Omega)$. You are given a training dataset $\\mathcal{D} = \\{(a_{i}, f_{i}, g_{i}, y_{i}, \\mathcal{H}_{i})\\}_{i=1}^{M}$ of size $M$, where $y_{i} \\in \\mathbb{R}^{p_{i}}$ are observations with observation operator $\\mathcal{H}_{i}: H^{1}(\\Omega) \\to \\mathbb{R}^{p_{i}}$ and $y_{i} = \\mathcal{H}_{i}(u_{i}) + \\eta_{i}$ with noise $\\eta_{i}$. Let $\\theta$ parametrize either a Fourier Neural Operator (FNO) or a Deep Operator Network (DeepONet) that defines an operator learner $T_{\\theta}: (a,f,g) \\mapsto u_{\\theta}(\\cdot; a,f,g)$.\n\nYou will formulate the empirical risk used for training $T_{\\theta}$ based on residual minimization and data assimilation principles. The fundamental bases you may use are: the definition of Dirichlet boundary conditions, the concept of empirical risk minimization, the use of residuals for partial differential equation satisfaction, and the incorporation of observation-model misfit via a squared norm.\n\nFor each training item $i \\in \\{1,\\dots,M\\}$, let $\\mathcal{X}_{i}^{\\mathrm{int}} \\subset \\Omega$ and $\\mathcal{X}_{i}^{\\mathrm{b}} \\subset \\partial \\Omega$ be the sets of interior and boundary collocation points, respectively, with cardinalities $N_{i}^{\\mathrm{int}}$ and $N_{i}^{\\mathrm{b}}$. Define the residuals\n$$\nr_{i}^{\\mathrm{int}}(x;\\theta) := \\mathcal{A}_{a_{i}}\\big(u_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x), \\quad x \\in \\mathcal{X}_{i}^{\\mathrm{int}},\n$$\n$$\nr_{i}^{\\mathrm{b}}(x;\\theta) := u_{\\theta}(x;a_{i},f_{i},g_{i}) - g_{i}(x), \\quad x \\in \\mathcal{X}_{i}^{\\mathrm{b}},\n$$\nand the data misfit $m_{i}(\\theta) := \\mathcal{H}_{i}\\big(u_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big) - y_{i} \\in \\mathbb{R}^{p_{i}}$. Consider the augmented empirical risk\n$$\n\\mathcal{L}_{\\mathrm{aug}}(\\theta) \n:= \\frac{1}{M} \\sum_{i=1}^{M} \\left[\n\\lambda_{\\mathrm{int}} \\cdot \\frac{1}{N_{i}^{\\mathrm{int}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{int}}} \\big| r_{i}^{\\mathrm{int}}(x;\\theta) \\big|^{2}\n\\;+\\;\n\\lambda_{\\mathrm{b}} \\cdot \\frac{1}{N_{i}^{\\mathrm{b}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{b}}} \\big| r_{i}^{\\mathrm{b}}(x;\\theta) \\big|^{2}\n\\;+\\;\n\\lambda_{\\mathrm{data}} \\cdot \\big\\| m_{i}(\\theta) \\big\\|_{2}^{2}\n\\right],\n$$\nwith tunable nonnegative weights $\\lambda_{\\mathrm{int}}, \\lambda_{\\mathrm{b}}, \\lambda_{\\mathrm{data}}$.\n\nNow, define a lifting operator $L: H^{1/2}(\\partial \\Omega) \\to H^{1}(\\Omega)$ such that $(L g)|_{\\partial \\Omega} = g$ for all admissible $g$, and a boundary-vanishing scalar function $b \\in C^{0}(\\overline{\\Omega})$ such that $b|_{\\partial \\Omega} = 0$ and $b(x^{\\star}) = 1$ at some interior reference point $x^{\\star} \\in \\Omega$. Reparametrize the model output by\n$$\nu_{\\theta}(\\cdot; a,f,g) \\;=\\; L g \\;+\\; b \\, v_{\\theta}(\\cdot; a,f,g),\n$$\nwhere $v_{\\theta}$ is the learned homogeneous component. For a Deep Operator Network, this may be realized by modifying the trunk basis by multiplication with $b$; for a Fourier Neural Operator, it may be realized by multiplying the network’s physical-space output by $b$ and adding $L g$ before computing residuals.\n\nStarting from the above definitions and without assuming any specific architectural formula beyond the stated reparametrization, derive the explicit closed-form analytic expression of the lifted empirical risk $\\mathcal{L}_{\\mathrm{lift}}(\\theta)$ that arises by substituting the reparametrized $u_{\\theta}$ into $\\mathcal{L}_{\\mathrm{aug}}(\\theta)$ and simplifying the boundary term using the Dirichlet property of $L$ and the boundary vanishing property of $b$.\n\nYour final answer must be the single simplified analytic expression for $\\mathcal{L}_{\\mathrm{lift}}(\\theta)$ in terms of $(a_{i}, f_{i}, g_{i}, y_{i}, \\mathcal{H}_{i})$, $L$, $b$, and $v_{\\theta}$, and must not include any other text. No numerical evaluation is required. Express your final answer as a closed-form expression.", "solution": "The objective is to derive the explicit analytical expression for the lifted empirical risk, denoted $\\mathcal{L}_{\\mathrm{lift}}(\\theta)$, by substituting the reparametrized model output $u_{\\theta}$ into the augmented empirical risk functional $\\mathcal{L}_{\\mathrm{aug}}(\\theta)$.\n\nThe augmented empirical risk is defined as:\n$$\n\\mathcal{L}_{\\mathrm{aug}}(\\theta) \n:= \\frac{1}{M} \\sum_{i=1}^{M} \\left[\n\\lambda_{\\mathrm{int}} \\cdot \\frac{1}{N_{i}^{\\mathrm{int}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{int}}} \\big| r_{i}^{\\mathrm{int}}(x;\\theta) \\big|^{2}\n\\;+\\;\n\\lambda_{\\mathrm{b}} \\cdot \\frac{1}{N_{i}^{\\mathrm{b}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{b}}} \\big| r_{i}^{\\mathrm{b}}(x;\\theta) \\big|^{2}\n\\;+\\;\n\\lambda_{\\mathrm{data}} \\cdot \\big\\| m_{i}(\\theta) \\big\\|_{2}^{2}\n\\right]\n$$\nThe reparametrization of the model output is given by:\n$$\nu_{\\theta}(\\cdot; a,f,g) \\;=\\; L g \\;+\\; b \\, v_{\\theta}(\\cdot; a,f,g)\n$$\nwhere $L$ is a lifting operator and $b$ is a boundary-vanishing function. The operator learner now learns the function $v_{\\theta}$.\n\nWe will analyze the effect of this substitution on each of the three components of the loss function for a single training sample $i$, suppressing the arguments of $u_{\\theta}$ and $v_{\\theta}$ for brevity where context is clear.\n\n1.  **Interior Residual Term**:\n    The interior residual is $r_{i}^{\\mathrm{int}}(x;\\theta) := \\mathcal{A}_{a_{i}}(u_{\\theta})(x) - f_{i}(x)$. Substituting the reparametrization for $u_{\\theta}$:\n    $$\n    r_{i}^{\\mathrm{int}}(x;\\theta) = \\mathcal{A}_{a_{i}}\\big(L g_{i} + b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x)\n    $$\n    The problem statement specifies that $\\mathcal{A}_{a}$ is a linear differential operator. Using this property, we can distribute $\\mathcal{A}_{a_{i}}$ over the sum:\n    $$\n    r_{i}^{\\mathrm{int}}(x;\\theta) = \\mathcal{A}_{a_{i}}(L g_{i})(x) + \\mathcal{A}_{a_{i}}\\big(b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x)\n    $$\n    The corresponding term in the loss function for sample $i$ is thus:\n    $$\n    \\lambda_{\\mathrm{int}} \\cdot \\frac{1}{N_{i}^{\\mathrm{int}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{int}}} \\left| \\mathcal{A}_{a_{i}}(L g_{i})(x) + \\mathcal{A}_{a_{i}}\\big(b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x) \\right|^{2}\n    $$\n\n2.  **Boundary Residual Term**:\n    The boundary residual is $r_{i}^{\\mathrm{b}}(x;\\theta) := u_{\\theta}(x) - g_{i}(x)$ for collocation points $x \\in \\mathcal{X}_{i}^{\\mathrm{b}} \\subset \\partial\\Omega$. Substituting the reparametrization for $u_{\\theta}$:\n    $$\n    r_{i}^{\\mathrm{b}}(x;\\theta) = \\big(L g_{i}\\big)(x) + b(x) \\, v_{\\theta}(x;a_{i},f_{i},g_{i}) - g_{i}(x)\n    $$\n    We now apply the defining properties of the lifting operator $L$ and the boundary-vanishing function $b$.\n    -   By definition of $L$, for any $x \\in \\partial\\Omega$, we have $(L g_{i})|_{\\partial\\Omega}(x) = g_{i}(x)$.\n    -   By definition of $b$, for any $x \\in \\partial\\Omega$, we have $b|_{\\partial\\Omega}(x) = 0$.\n    Since the collocation points $\\mathcal{X}_{i}^{\\mathrm{b}}$ are on the boundary $\\partial\\Omega$, for any $x \\in \\mathcal{X}_{i}^{\\mathrm{b}}$ we have:\n    $$\n    r_{i}^{\\mathrm{b}}(x;\\theta) = g_{i}(x) + 0 \\cdot v_{\\theta}(x;a_{i},f_{i},g_{i}) - g_{i}(x) = 0\n    $$\n    The boundary residual is identically zero for all boundary collocation points. Consequently, the entire boundary term in the loss function vanishes:\n    $$\n    \\lambda_{\\mathrm{b}} \\cdot \\frac{1}{N_{i}^{\\mathrm{b}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{b}}} \\big| r_{i}^{\\mathrm{b}}(x;\\theta) \\big|^{2} = \\lambda_{\\mathrm{b}} \\cdot \\frac{1}{N_{i}^{\\mathrm{b}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrmb}} |0|^{2} = 0\n    $$\n    This is the central simplification resulting from the reparametrization, as the Dirichlet boundary condition is satisfied by construction.\n\n3.  **Data Misfit Term**:\n    The data misfit is $m_{i}(\\theta) := \\mathcal{H}_{i}(u_{\\theta}) - y_{i}$. Substituting the reparametrization for $u_{\\theta}$:\n    $$\n    m_{i}(\\theta) = \\mathcal{H}_{i}\\big(L g_{i} + b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big) - y_{i}\n    $$\n    The problem does not specify that the observation operator $\\mathcal{H}_{i}$ is linear, so no further simplification of this term is possible. The corresponding term in the loss function for sample $i$ is:\n    $$\n    \\lambda_{\\mathrm{data}} \\cdot \\big\\| \\mathcal{H}_{i}\\big(L g_{i} + b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big) - y_{i} \\big\\|_{2}^{2}\n    $$\n\n**Assembling the Lifted Empirical Risk**:\nCombining the results for the three terms, we construct the lifted empirical risk $\\mathcal{L}_{\\mathrm{lift}}(\\theta)$ by summing the remaining non-zero components and averaging over the $M$ training samples. The boundary term is now absent.\n$$\n\\mathcal{L}_{\\mathrm{lift}}(\\theta) = \\frac{1}{M} \\sum_{i=1}^{M} \\left[\n\\lambda_{\\mathrm{int}} \\cdot \\frac{1}{N_{i}^{\\mathrm{int}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{int}}} \\left| \\mathcal{A}_{a_{i}}(L g_{i})(x) + \\mathcal{A}_{a_{i}}\\big(b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x) \\right|^{2}\n+\n\\lambda_{\\mathrm{data}} \\cdot \\left\\| \\mathcal{H}_{i}\\big(L g_{i} + b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big) - y_{i} \\right\\|_{2}^{2}\n\\right]\n$$\nThis expression is the final simplified analytic form of the lifted empirical risk.", "answer": "$$\n\\boxed{\n\\frac{1}{M} \\sum_{i=1}^{M} \\left[\n\\lambda_{\\mathrm{int}} \\cdot \\frac{1}{N_{i}^{\\mathrm{int}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{int}}} \\left| \\mathcal{A}_{a_{i}}(L g_{i})(x) + \\mathcal{A}_{a_{i}}\\big(b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x) \\right|^{2}\n+\n\\lambda_{\\mathrm{data}} \\cdot \\left\\| \\mathcal{H}_{i}\\big(L g_{i} + b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big) - y_{i} \\right\\|_{2}^{2}\n\\right]\n}\n$$", "id": "3407257"}]}