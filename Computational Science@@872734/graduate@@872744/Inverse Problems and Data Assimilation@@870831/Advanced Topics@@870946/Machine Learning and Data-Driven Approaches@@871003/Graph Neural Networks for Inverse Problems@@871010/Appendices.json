{"hands_on_practices": [{"introduction": "To solve an inverse problem using gradient-based methods, we must first construct a differentiable forward model that maps parameters to observations. This exercise provides fundamental practice by translating basic physical laws on a simple graph into a \"Dirichlet-to-Neumann\" map, a cornerstone of many physical inverse problems. By deriving its Jacobian, you will compute the sensitivity of the observations to the underlying physical parameters, a crucial step for integrating this model into a Graph Neural Network framework [@problem_id:3386851].", "problem": "Consider a one-dimensional ($1$D) chain graph with nodes indexed $i \\in \\{0,1,\\dots,N\\}$ and edges $(i,i+1)$ for $i \\in \\{0,1,\\dots,N-1\\}$. Each edge $(i,i+1)$ has a strictly positive conductance $w_{i,i+1} \\in \\mathbb{R}_{0}$. Define the boundary (Dirichlet) potentials by $v_0 = V_L$ and $v_N = V_R$, where $V_L, V_R \\in \\mathbb{R}$ are fixed and known. Interior node voltages $\\{v_i\\}_{i=1}^{N-1}$ are determined by Kirchhoff’s current law and Ohm’s law on the graph. Let the input parameterization be the log-conductances $x \\in \\mathbb{R}^N$ with components $x_i = \\ln w_{i,i+1}$ for $i \\in \\{0,1,\\dots,N-1\\}$, and define the output $y \\in \\mathbb{R}^2$ as the boundary current vector $y = [I_0, I_N]^{\\top}$, where $I_0$ is the net current injected at node $0$ (positive into the network) and $I_N$ is the net current injected at node $N$ (positive into the network). This defines a forward map $y = H(x)$, which is the graph-theoretic Dirichlet-to-Neumann map for this chain.\n\nYou are designing a Graph Neural Network (GNN) surrogate for $H(x)$ as part of an inverse problem in data assimilation, where gradients with respect to $x$ are required for backpropagation and for Gauss–Newton regularized inversion. Starting only from the core definitions of Ohm’s law on edges, Kirchhoff’s current law at nodes, and the positivity enforced by the log-parameterization, derive the discrete forward map $y = H(x)$ explicitly in terms of $\\{w_{i,i+1}\\}$ and $\\{V_L,V_R\\}$, and then compute the Jacobian $\\frac{\\partial y}{\\partial x} \\in \\mathbb{R}^{2 \\times N}$ in closed form at an arbitrary $x$. Express your final Jacobian using only $\\{w_{i,i+1}\\}_{i=0}^{N-1}$, $V_L$, and $V_R$.\n\nGive your final Jacobian as a single closed-form analytic expression. Do not approximate. No units are required. Angles are not involved. The final answer must be the requested expression only.", "solution": "The problem statement is first validated against the required criteria.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n-   **Graph Structure:** A one-dimensional ($1$D) chain graph with nodes indexed $i \\in \\{0,1,\\dots,N\\}$ and edges $(i,i+1)$ for $i \\in \\{0,1,\\dots,N-1\\}$.\n-   **Edge Property:** Each edge $(i,i+1)$ has a strictly positive conductance $w_{i,i+1} \\in \\mathbb{R}_{0}$.\n-   **Boundary Conditions:** Dirichlet boundary potentials are specified as $v_0 = V_L$ and $v_N = V_R$, with $V_L, V_R \\in \\mathbb{R}$ being fixed and known.\n-   **Interior Node Physics:** The interior node voltages $\\{v_i\\}_{i=1}^{N-1}$ are governed by Kirchhoff’s current law and Ohm’s law.\n-   **Input Parameterization:** The input parameters are the log-conductances $x \\in \\mathbb{R}^N$ with components $x_i = \\ln w_{i,i+1}$ for $i \\in \\{0,1,\\dots,N-1\\}$.\n-   **Output Definition:** The output is the boundary current vector $y = [I_0, I_N]^{\\top} \\in \\mathbb{R}^2$.\n-   **Current Definitions:** $I_0$ is the net current injected at node $0$ (positive into the network), and $I_N$ is the net current injected at node $N$ (positive into the network).\n-   **Forward Map:** The relationship between input and output is denoted by the map $y = H(x)$.\n-   **Objective:**\n    1.  Derive the explicit forward map $y = H(x)$ in terms of $\\{w_{i,i+1}\\}$ and $\\{V_L, V_R\\}$.\n    2.  Compute the Jacobian matrix $\\frac{\\partial y}{\\partial x} \\in \\mathbb{R}^{2 \\times N}$ in closed form.\n    3.  Express the final Jacobian using only $\\{w_{i,i+1}\\}_{i=0}^{N-1}$, $V_L$, and $V_R$.\n\n#### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is based on fundamental principles of electrical circuits, namely Ohm's law and Kirchhoff's current law. These are well-established laws of physics. The log-parameterization is a standard and valid technique in optimization and inverse problems to enforce positivity constraints. The problem is scientifically sound.\n-   **Well-Posed:** The problem describes a linear resistive network with fixed Dirichlet boundary conditions. Since all conductances $w_{i,i+1}$ are strictly positive, the resistances $1/w_{i,i+1}$ are finite and positive. This guarantees that the system of linear equations for the interior node potentials has a unique solution. Consequently, the forward map $H(x)$ is well-defined, and its Jacobian is computable. The problem is well-posed.\n-   **Objective:** The problem is stated using precise mathematical language, with all terms clearly defined. It is free from subjective or ambiguous statements.\n\nThe problem does not violate any of the invalidity criteria. It is a standard, well-posed problem in mathematical physics and is directly relevant to computational methods in data assimilation and machine learning.\n\n#### Step 3: Verdict and Action\nThe problem is valid. The solution process will proceed.\n\n### Solution Derivation\n\nThe solution is derived in two parts: first, the forward map $y=H(x)$ is determined, and second, its Jacobian $\\frac{\\partial y}{\\partial x}$ is computed.\n\n#### Part 1: Derivation of the Forward Map $y = H(x)$\n\nThe electrical network is a simple $1$D series circuit. According to Ohm's law, the current flowing from node $i$ to node $i+1$ is given by $I_{i \\to i+1} = w_{i,i+1} (v_i - v_{i+1})$, where $v_i$ is the voltage at node $i$ and $w_{i,i+1}$ is the conductance of the edge $(i, i+1)$.\n\nFor a $1$D chain with sources only at the boundaries (nodes $0$ and $N$), Kirchhoff's current law at any interior node $j \\in \\{1, \\dots, N-1\\}$ implies that the current flowing in equals the current flowing out. This means the current is constant throughout the entire chain. Let this constant current be denoted by $I_{\\text{chain}}$.\n$$ I_{\\text{chain}} = I_{0 \\to 1} = I_{1 \\to 2} = \\dots = I_{N-1 \\to N} $$\nThe total voltage drop across the chain is $v_0 - v_N = V_L - V_R$.\nThe resistance of an individual edge $(i, i+1)$ is the reciprocal of its conductance: $R_{i,i+1} = 1/w_{i,i+1}$.\nThe total resistance of the series circuit, $R_{\\text{total}}$, is the sum of the individual resistances:\n$$ R_{\\text{total}} = \\sum_{k=0}^{N-1} R_{k,k+1} = \\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}} $$\nThe constant chain current $I_{\\text{chain}}$ can now be found using Ohm's law for the entire circuit:\n$$ I_{\\text{chain}} = \\frac{V_L - V_R}{R_{\\text{total}}} = \\frac{V_L - V_R}{\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}} $$\nThe output vector $y = [I_0, I_N]^{\\top}$ consists of the boundary currents.\n-   $I_0$ is the net current injected at node $0$. This current must flow into the chain, so it is equal to the current flowing from node $0$ to node $1$: $I_0 = I_{0 \\to 1} = I_{\\text{chain}}$.\n-   $I_N$ is the net current injected at node $N$. The current flowing from the chain into node $N$ is $I_{N-1 \\to N} = I_{\\text{chain}}$. By the convention \"positive into the network\", $I_N$ must be the negative of the current exiting the network at node $N$. The current exiting the network is $I_{\\text{chain}}$, so $I_N = -I_{\\text{chain}}$.\n\nThus, the forward map is:\n$$ y = H(x) = \\begin{pmatrix} I_0 \\\\ I_N \\end{pmatrix} = \\begin{pmatrix} I_{\\text{chain}} \\\\ -I_{\\text{chain}} \\end{pmatrix} = \\frac{V_L - V_R}{\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nThis expression gives the output $y$ in terms of the conductances $\\{w_{i,i+1}\\}$.\n\n#### Part 2: Computation of the Jacobian $\\frac{\\partial y}{\\partial x}$\n\nThe Jacobian of $y$ with respect to $x$ is a $2 \\times N$ matrix whose elements are $\\frac{\\partial y_m}{\\partial x_j}$, where $y_m$ is the $m$-th component of $y$ ($m \\in \\{1,2\\}$) and $x_j$ is the $j$-th component of $x$ for $j \\in \\{0, \\dots, N-1\\}$. The components are $y_1 = I_0$ and $y_2 = I_N$.\n\nGiven that $I_N = -I_0$, the second row of the Jacobian is simply the negative of the first row: $\\frac{\\partial I_N}{\\partial x_j} = -\\frac{\\partial I_0}{\\partial x_j}$. We only need to compute the first row, which corresponds to the derivatives $\\frac{\\partial I_0}{\\partial x_j}$.\n\nWe use the chain rule to differentiate $I_0$ with respect to $x_j$:\n$$ \\frac{\\partial I_0}{\\partial x_j} = \\frac{\\partial I_0}{\\partial w_{j,j+1}} \\frac{\\partial w_{j,j+1}}{\\partial x_j} $$\nThe problem defines the parameterization $x_j = \\ln w_{j,j+1}$, which implies $w_{j,j+1} = \\exp(x_j)$. Therefore, the derivative is:\n$$ \\frac{\\partial w_{j,j+1}}{\\partial x_j} = \\exp(x_j) = w_{j,j+1} $$\nNext, we compute the derivative of $I_0$ with respect to an arbitrary conductance $w_{j,j+1}$. Let $S = \\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}$. Then $I_0 = (V_L - V_R)S^{-1}$.\n$$ \\frac{\\partial I_0}{\\partial w_{j,j+1}} = (V_L - V_R) \\frac{d(S^{-1})}{dS} \\frac{\\partial S}{\\partial w_{j,j+1}} $$\nThe derivatives are:\n$$ \\frac{d(S^{-1})}{dS} = -S^{-2} $$\n$$ \\frac{\\partial S}{\\partial w_{j,j+1}} = \\frac{\\partial}{\\partial w_{j,j+1}} \\left( \\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}} \\right) = -\\frac{1}{w_{j,j+1}^2} $$\nCombining these results:\n$$ \\frac{\\partial I_0}{\\partial w_{j,j+1}} = (V_L - V_R) (-S^{-2}) \\left( -\\frac{1}{w_{j,j+1}^2} \\right) = \\frac{V_L - V_R}{S^2 w_{j,j+1}^2} = \\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2 w_{j,j+1}^2} $$\nNow we can find $\\frac{\\partial I_0}{\\partial x_j}$:\n$$ \\frac{\\partial I_0}{\\partial x_j} = \\frac{\\partial I_0}{\\partial w_{j,j+1}} \\frac{\\partial w_{j,j+1}}{\\partial x_j} = \\left( \\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2 w_{j,j+1}^2} \\right) w_{j,j+1} = \\frac{V_L - V_R}{w_{j,j+1} \\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2} $$\nThis expression is the $(1, j+1)$-th entry of the Jacobian matrix (using $j$ from $0$ to $N-1$ to index columns). The full Jacobian matrix $\\frac{\\partial y}{\\partial x}$ is then constructed by assembling these components for $j=0, 1, \\dots, N-1$:\n$$ \\frac{\\partial y}{\\partial x} = \\begin{pmatrix} \\frac{\\partial I_0}{\\partial x_0}  \\frac{\\partial I_0}{\\partial x_1}  \\cdots  \\frac{\\partial I_0}{\\partial x_{N-1}} \\\\ \\frac{\\partial I_N}{\\partial x_0}  \\frac{\\partial I_N}{\\partial x_1}  \\cdots  \\frac{\\partial I_N}{\\partial x_{N-1}} \\end{pmatrix} $$\nSubstituting the derived expressions:\n$$ \\frac{\\partial y}{\\partial x} = \\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2} \\begin{pmatrix} \\frac{1}{w_{0,1}}  \\frac{1}{w_{1,2}}  \\cdots  \\frac{1}{w_{N-1,N}} \\\\ -\\frac{1}{w_{0,1}}  -\\frac{1}{w_{1,2}}  \\cdots  -\\frac{1}{w_{N-1,N}} \\end{pmatrix} $$\nThis is the final closed-form expression for the Jacobian, expressed in terms of the required variables.", "answer": "$$ \\boxed{\\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2} \\begin{pmatrix} \\frac{1}{w_{0,1}}  \\frac{1}{w_{1,2}}  \\cdots  \\frac{1}{w_{N-1,N}} \\\\ -\\frac{1}{w_{0,1}}  -\\frac{1}{w_{1,2}}  \\cdots  -\\frac{1}{w_{N-1,N}} \\end{pmatrix}} $$", "id": "3386851"}, {"introduction": "Many advanced GNNs for inverse problems are designed by \"unrolling\" a classical iterative optimization algorithm, where each network layer corresponds to one iteration. For such a solver to be effective, its convergence must be guaranteed, which requires the iterative operator to be a strict contraction. This practice demonstrates how to derive the stability condition for a learned gradient descent-like scheme by analyzing the spectral radius of the iteration matrix, a critical skill for designing robust and reliable physics-informed GNNs [@problem_id:3386845].", "problem": "In a learned unrolled Graph Neural Network (GNN) for a linear inverse problem on a graph, consider the data-fidelity function $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{H}\\mathbf{x} - \\mathbf{y}\\|_{2}^{2}$ with $\\mathbf{H} \\in \\mathbb{R}^{n \\times n}$ symmetric positive definite. One layer of the unrolled architecture implements the affine iteration $\\mathbf{x}_{k+1} = (\\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H})\\mathbf{x}_{k} + \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{y}$, where $\\alpha  0$ is a constant step size. Using only the spectral theorem for real symmetric matrices, the definition of the spectral radius, and basic properties of graph Laplacians, derive the necessary and sufficient condition on $\\alpha$ such that the linear operator $\\mathbf{T}(\\alpha) = \\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H}$ is a strict contraction in the Euclidean norm.\n\nSpecialize to the case $n = 3$ where $\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$ and $\\mathbf{L}$ is the combinatorial Laplacian of the path graph on $3$ nodes with unit edge weights:\n$$\n\\mathbf{L} = \\begin{pmatrix}\n1  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  1\n\\end{pmatrix}.\n$$\nCompute the largest admissible constant step size $\\alpha$ that guarantees $\\rho(\\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H})  1$. Report the final $\\alpha$ as an exact number. Do not round.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of numerical linear algebra and optimization, specifically the convergence analysis of iterative methods for solving linear systems. The problem is well-posed, objective, and contains all necessary information for a unique solution. The premises are consistent; the specified matrix $\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$ is indeed symmetric and positive definite, as the combinatorial Laplacian $\\mathbf{L}$ is symmetric and positive semidefinite, meaning its eigenvalues $\\lambda_i(\\mathbf{L}) \\ge 0$, which in turn implies that the eigenvalues of $\\mathbf{H}$, given by $1 + \\lambda_i(\\mathbf{L})$, are all greater than or equal to $1$, thus positive.\n\nThe problem asks for two main results: first, a general condition on the step size $\\alpha$ for an iterative method to converge, and second, the computation of the maximum step size for a specific case.\n\nFirst, we derive the necessary and sufficient condition for the iteration to be a strict contraction. The iterative scheme is given by\n$$\n\\mathbf{x}_{k+1} = (\\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H})\\mathbf{x}_{k} + \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{y}\n$$\nThis is an affine iteration of the form $\\mathbf{x}_{k+1} = \\mathbf{T}\\mathbf{x}_k + \\mathbf{c}$, where the iteration matrix is $\\mathbf{T}(\\alpha) = \\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H}$. The iteration converges to a unique fixed point for any starting vector $\\mathbf{x}_0$ if and only if the operator $\\mathbf{T}(\\alpha)$ is a strict contraction, which in a finite-dimensional vector space is equivalent to its spectral radius being less than $1$. That is, $\\rho(\\mathbf{T}(\\alpha))  1$.\n\nThe spectral radius of a matrix is the maximum of the absolute values of its eigenvalues. Let $\\mu_i$ be the eigenvalues of $\\mathbf{T}(\\alpha)$. The condition is $\\max_i |\\mu_i|  1$.\n\nThe eigenvalues of $\\mathbf{T}(\\alpha) = \\mathbf{I} - \\alpha\\mathbf{H}^{\\top}\\mathbf{H}$ are related to the eigenvalues of $\\mathbf{H}^{\\top}\\mathbf{H}$. Let $\\nu_i$ be the eigenvalues of $\\mathbf{H}^{\\top}\\mathbf{H}$. Then, the eigenvalues of $\\mathbf{T}(\\alpha)$ are $\\mu_i = 1 - \\alpha\\nu_i$.\n\nThe problem states that $\\mathbf{H}$ is a real symmetric matrix. Therefore, $\\mathbf{H}^{\\top} = \\mathbf{H}$, and the matrix $\\mathbf{H}^{\\top}\\mathbf{H}$ simplifies to $\\mathbf{H}^2$. Let $\\lambda_i$ be the eigenvalues of $\\mathbf{H}$. The eigenvalues of $\\mathbf{H}^2$ are then $\\nu_i = \\lambda_i^2$. Because $\\mathbf{H}$ is also positive definite, all its eigenvalues $\\lambda_i$ are strictly positive real numbers. Consequently, the eigenvalues $\\nu_i = \\lambda_i^2$ of $\\mathbf{H}^2$ are also strictly positive.\n\nThe convergence condition $\\rho(\\mathbf{T}(\\alpha))  1$ becomes $\\max_i |1 - \\alpha \\lambda_i^2|  1$. This must hold for all eigenvalues $\\lambda_i$ of $\\mathbf{H}$. This inequality is equivalent to:\n$$\n-1  1 - \\alpha \\lambda_i^2  1\n$$\nWe analyze the two inequalities separately for any given eigenvalue $\\lambda_i  0$.\n1.  $1 - \\alpha \\lambda_i^2  1 \\implies -\\alpha \\lambda_i^2  0$. Since $\\alpha  0$ is given and $\\lambda_i^2  0$, this inequality is always satisfied.\n2.  $-1  1 - \\alpha \\lambda_i^2 \\implies 2  \\alpha \\lambda_i^2 \\implies \\alpha  \\frac{2}{\\lambda_i^2}$.\n\nThis condition must be true for all eigenvalues $\\lambda_i$ of $\\mathbf{H}$. To ensure this, $\\alpha$ must be smaller than the minimum of all the upper bounds $\\frac{2}{\\lambda_i^2}$.\n$$\n\\alpha  \\min_i \\left(\\frac{2}{\\lambda_i^2}\\right) = \\frac{2}{\\max_i (\\lambda_i^2)} = \\frac{2}{(\\max_i \\lambda_i)^2}\n$$\nSince for a positive definite matrix $\\max_i \\lambda_i = \\lambda_{\\max}(\\mathbf{H})$, which is also its spectral radius $\\rho(\\mathbf{H})$, the necessary and sufficient condition on $\\alpha$ is:\n$$\n0  \\alpha  \\frac{2}{(\\lambda_{\\max}(\\mathbf{H}))^2}\n$$\n\nNext, we specialize to the case where $n=3$ and $\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$, with $\\mathbf{L}$ being the given graph Laplacian matrix:\n$$\n\\mathbf{L} = \\begin{pmatrix} 1  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  1 \\end{pmatrix}\n$$\nThe eigenvalues of $\\mathbf{H}$ are given by $\\lambda_i(\\mathbf{H}) = 1 + \\lambda_i(\\mathbf{L})$, where $\\lambda_i(\\mathbf{L})$ are the eigenvalues of $\\mathbf{L}$. We find the eigenvalues of $\\mathbf{L}$ by solving the characteristic equation $\\det(\\mathbf{L} - \\lambda \\mathbf{I}) = 0$.\n$$\n\\det \\begin{pmatrix} 1-\\lambda  -1  0 \\\\ -1  2-\\lambda  -1 \\\\ 0  -1  1-\\lambda \\end{pmatrix} = 0\n$$\nExpanding the determinant along the first row:\n$$\n(1-\\lambda) \\det \\begin{pmatrix} 2-\\lambda  -1 \\\\ -1  1-\\lambda \\end{pmatrix} - (-1) \\det \\begin{pmatrix} -1  -1 \\\\ 0  1-\\lambda \\end{pmatrix} = 0\n$$\n$$\n(1-\\lambda)[(2-\\lambda)(1-\\lambda) - 1] + [(-1)(1-\\lambda) - 0] = 0\n$$\n$$\n(1-\\lambda)[\\lambda^2 - 3\\lambda + 2 - 1] - (1-\\lambda) = 0\n$$\n$$\n(1-\\lambda)[\\lambda^2 - 3\\lambda + 1] - (1-\\lambda) = 0\n$$\nFactoring out $(1-\\lambda)$:\n$$\n(1-\\lambda)[(\\lambda^2 - 3\\lambda + 1) - 1] = 0\n$$\n$$\n(1-\\lambda)(\\lambda^2 - 3\\lambda) = 0\n$$\n$$\n\\lambda(1-\\lambda)(\\lambda-3) = 0\n$$\nThe eigenvalues of $\\mathbf{L}$ are $\\lambda(\\mathbf{L}) \\in \\{0, 1, 3\\}$.\n\nThe eigenvalues of $\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$ are therefore:\n$$\n\\lambda_1(\\mathbf{H}) = 1 + 0 = 1\n$$\n$$\n\\lambda_2(\\mathbf{H}) = 1 + 1 = 2\n$$\n$$\n\\lambda_3(\\mathbf{H}) = 1 + 3 = 4\n$$\nThe maximum eigenvalue of $\\mathbf{H}$ is $\\lambda_{\\max}(\\mathbf{H}) = 4$.\n\nFinally, we compute the largest admissible step size $\\alpha$. Using the derived condition, the interval of admissible values for $\\alpha$ is:\n$$\n0  \\alpha  \\frac{2}{(\\lambda_{\\max}(\\mathbf{H}))^2} = \\frac{2}{4^2} = \\frac{2}{16} = \\frac{1}{8}\n$$\nThe problem asks for the largest admissible constant step size $\\alpha$ that guarantees convergence. This value is the supremum of the open interval $(0, \\frac{1}{8})$. Any step size $\\alpha$ within this interval will result in a strict contraction, while at the boundary value $\\alpha = \\frac{1}{8}$, the spectral radius of the iteration matrix becomes $\\rho(\\mathbf{T}(\\frac{1}{8}))=1$, which does not guarantee convergence to the correct solution for all initial vectors. The largest value that *guarantees* the strict inequality $\\rho  1$ is the limit of the interval. Therefore, the largest admissible value is the supremum.\n\nThe largest admissible constant step size is $\\frac{1}{8}$.", "answer": "$$\\boxed{\\frac{1}{8}}$$", "id": "3386845"}, {"introduction": "Training deep, unrolled GNN solvers requires computing the gradient of a final loss function with respect to parameters in every layer, a task that can be computationally prohibitive if done naively. The adjoint method, a form of reverse-mode automatic differentiation, provides a memory-efficient solution by propagating sensitivities backward through the computational graph. This exercise guides you through the derivation of the adjoint recursion, revealing the mathematical machinery that makes training deep, physics-inspired GNNs feasible [@problem_id:3386838].", "problem": "Consider a weighted graph with $n$ nodes and a known adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$. A learnable Graph Neural Network (GNN) based solver iteratively updates a node-state vector $x_t \\in \\mathbb{R}^{n}$ according to a differentiable mapping $F:\\mathbb{R}^{n} \\times \\mathbb{R}^{p} \\times \\mathbb{R}^{n \\times n} \\to \\mathbb{R}^{n}$ via\n$$\nx_{t+1} \\;=\\; F\\!\\left(x_t,\\theta;A\\right), \\quad t=0,1,\\dots,T-1,\n$$\nwhere $\\theta \\in \\mathbb{R}^{p}$ are trainable parameters. After $T$ iterations, the state $x_T$ is mapped to predicted observations $\\hat{y} \\in \\mathbb{R}^{m}$ through a differentiable observation operator $H:\\mathbb{R}^{n}\\to\\mathbb{R}^{m}$, and the data misfit is quantified by a differentiable loss $\\ell:\\mathbb{R}^{m}\\times\\mathbb{R}^{m}\\to\\mathbb{R}$, yielding the objective\n$$\nJ(\\theta) \\;=\\; \\ell\\!\\big(y, H(x_T)\\big),\n$$\nfor a given target $y \\in \\mathbb{R}^{m}$. Assume all mappings are continuously differentiable and that the Jacobians $\\frac{\\partial F}{\\partial x}$, $\\frac{\\partial F}{\\partial \\theta}$, and $\\frac{\\partial H}{\\partial x}$ exist at the relevant points.\n\nStarting only from the chain rule of multivariate calculus and the foundational definition of reverse-mode Automatic Differentiation (AD), derive a reverse-mode adjoint recursion that avoids storing all intermediate states $\\{x_t\\}_{t=0}^{T}$ and yields a closed-form analytic expression for the gradient $\\nabla_{\\theta} J(\\theta)$ as a sum over the iterations. Your final expression must be written entirely in terms of Jacobians of $F$ and $H$, the gradient of $\\ell$ with respect to its second argument, and products of Jacobians that propagate adjoint information backward in time. Express your final answer as a single analytic expression. No numerical evaluation is required.", "solution": "### Problem Validation\n\nThe problem is scientifically sound and well-posed. It describes the standard setup for training a deep unrolled network (or a recurrent neural network) using gradient-based optimization. The request to derive the gradient using the adjoint method is a fundamental task in this domain, rooted in optimal control theory and automatic differentiation. All necessary mathematical objects (functions, parameters, states) are clearly defined, and the assumption of differentiability makes the derivation possible.\n\n### Solution Derivation\n\nThe objective is to compute the gradient $\\nabla_{\\theta} J(\\theta)$ of the objective function $J(\\theta) = \\ell(y, H(x_T))$ with respect to the parameters $\\theta \\in \\mathbb{R}^{p}$. The state vector $x_T$ is the result of a sequence of operations, forming a discrete-time dynamical system:\n$$x_{t+1} = F(x_t, \\theta; A), \\quad t=0,1,\\dots,T-1$$\nThe initial state $x_0$ is given and assumed to be independent of $\\theta$, thus $\\frac{dx_0}{d\\theta} = 0$.\n\nWe seek the total derivative of $J$ with respect to $\\theta$. We employ the chain rule of multivariate calculus. For consistency, we adopt the numerator layout for Jacobians, where for a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, its Jacobian $\\frac{\\partial f}{\\partial x}$ is an $m \\times n$ matrix. The gradient of a scalar function is taken to be a column vector.\n\nThe dependence of $J$ on $\\theta$ is mediated through the final state $x_T$. Using the chain rule:\n$$\n\\nabla_{\\theta} J(\\theta) = \\left( \\frac{d J}{d \\theta} \\right)^T = \\left( \\frac{\\partial J}{\\partial x_T} \\frac{d x_T}{d \\theta} \\right)^T = \\left( \\frac{d x_T}{d \\theta} \\right)^T \\left( \\frac{\\partial J}{\\partial x_T} \\right)^T\n$$\nwhere $\\frac{d x_T}{d \\theta}$ is the total derivative of $x_T$ with respect to $\\theta$, which is an $n \\times p$ Jacobian matrix.\n\nFirst, let's find the term $\\left( \\frac{\\partial J}{\\partial x_T} \\right)^T$. Since $J = \\ell(y, H(x_T))$, another application of the chain rule gives:\n$$\n\\frac{\\partial J}{\\partial x_T} = \\frac{\\partial \\ell(y, H(x_T))}{\\partial H} \\frac{\\partial H(x_T)}{\\partial x_T}\n$$\nHere, $\\frac{\\partial \\ell}{\\partial H}$ is a $1 \\times m$ row vector (the gradient of the scalar loss w.r.t. its second argument vector, which we denote $(\\nabla_{\\hat{y}}\\ell)^T$), and $\\frac{\\partial H}{\\partial x_T}$ is the $m \\times n$ Jacobian of the observation operator. Taking the transpose yields the column vector:\n$$\n\\left( \\frac{\\partial J}{\\partial x_T} \\right)^T = \\left( \\frac{\\partial H(x_T)}{\\partial x} \\right)^T (\\nabla_{\\hat{y}}\\ell(y, H(x_T)))\n$$\nLet us define this term as the initial adjoint state, $\\lambda_T$:\n$$\n\\lambda_T \\triangleq \\left( \\frac{\\partial H(x_T)}{\\partial x} \\right)^T \\nabla_{\\hat{y}}\\ell(y, H(x_T))\n$$\nThis vector $\\lambda_T \\in \\mathbb{R}^n$ represents the sensitivity of the final cost $J$ with respect to the final state $x_T$.\n\nNext, we must find an expression for the total derivative matrix $\\frac{d x_T}{d \\theta}$. This matrix captures how a change in $\\theta$ propagates through the $T$ iterations to affect $x_T$. We start from the recursive definition of the states:\n$$\nx_{t+1} = F(x_t, \\theta; A)\n$$\nTaking the total derivative with respect to $\\theta$ on both sides:\n$$\n\\frac{d x_{t+1}}{d \\theta} = \\frac{\\partial F}{\\partial x_t} \\frac{d x_t}{d \\theta} + \\frac{\\partial F}{\\partial \\theta}\n$$\nThe Jacobians $\\frac{\\partial F}{\\partial x_t}$ and $\\frac{\\partial F}{\\partial \\theta}$ are evaluated at $(x_t, \\theta, A)$. Let's denote them as $J_{F,x}(t)$ and $J_{F,\\theta}(t)$ for brevity. The recursion is:\n$$\n\\frac{d x_{t+1}}{d \\theta} = J_{F,x}(t) \\frac{d x_t}{d \\theta} + J_{F,\\theta}(t)\n$$\nwith the initial condition $\\frac{d x_0}{d \\theta} = 0$. Unrolling this recursion reveals the pattern for $\\frac{d x_T}{d \\theta}$:\n$$\n\\frac{d x_T}{d \\theta} = \\sum_{t=0}^{T-1} \\left( J_{F,x}(T-1) J_{F,x}(T-2) \\cdots J_{F,x}(t+1) \\right) J_{F,\\theta}(t)\n$$\nwhere the product in parentheses is the identity matrix if $t=T-1$.\n\nNow we can assemble the final expression for the gradient $\\nabla_{\\theta} J(\\theta) = \\left( \\frac{d x_T}{d \\theta} \\right)^T \\lambda_T$:\n$$\n\\nabla_{\\theta} J(\\theta) = \\left[ \\sum_{t=0}^{T-1} \\left( J_{F,x}(T-1) \\cdots J_{F,x}(t+1) \\right) J_{F,\\theta}(t) \\right]^T \\lambda_T\n$$\nUsing the linearity of the transpose and the property $(AB)^T = B^T A^T$:\n$$\n\\nabla_{\\theta} J(\\theta) = \\sum_{t=0}^{T-1} J_{F,\\theta}(t)^T \\left( J_{F,x}(T-1) \\cdots J_{F,x}(t+1) \\right)^T \\lambda_T\n$$\n$$\n\\nabla_{\\theta} J(\\theta) = \\sum_{t=0}^{T-1} J_{F,\\theta}(t)^T \\left( J_{F,x}(t+1)^T \\cdots J_{F,x}(T-1)^T \\right) \\lambda_T\n$$\nThis represents the propagation of adjoint information backward in time. We can define a sequence of adjoint states $\\lambda_t \\in \\mathbb{R}^n$ via the backward recursion:\n$$\n\\lambda_t = J_{F,x}(t)^T \\lambda_{t+1} \\quad \\text{for } t=T-1, \\dots, 0\n$$\nstarting from $\\lambda_T$ as defined above. Unrolling this backward recursion shows that the term multiplying $J_{F,\\theta}(t)^T$ in the sum is exactly the adjoint state $\\lambda_{t+1}$:\n$$ \\lambda_{t+1} = J_{F,x}(t+1)^T \\cdots J_{F,x}(T-1)^T \\lambda_T $$\nSubstituting this back into the expression for the gradient, we arrive at the elegant sum:\n$$\n\\nabla_{\\theta} J(\\theta) = \\sum_{t=0}^{T-1} J_{F,\\theta}(t)^T \\lambda_{t+1}\n$$\nThis form highlights the structure of the reverse-mode (adjoint) method. To get the final closed-form expression requested, we substitute the full expressions. Re-inserting the explicit function and evaluation point notation:\n$$\n\\nabla_{\\theta} J(\\theta) = \\sum_{t=0}^{T-1} \\left(\\frac{\\partial F}{\\partial \\theta}\\bigg|_{(x_t, \\theta)}\\right)^T \\left( \\left(\\frac{\\partial F}{\\partial x}\\bigg|_{(x_{t+1}, \\theta)}\\right)^T \\cdots \\left(\\frac{\\partial F}{\\partial x}\\bigg|_{(x_{T-1}, \\theta)}\\right)^T \\right) \\left(\\frac{\\partial H}{\\partial x}\\bigg|_{x_T}\\right)^T \\nabla_{\\hat{y}}\\ell(y, H(x_T))\n$$\nIn this expression, the product in the large parentheses is understood as a product of matrices in increasing order of the time index $k$, and is the identity matrix if $t=T-1$. The Jacobians are evaluated at the states $\\{x_k\\}$ computed during the forward pass $x_{k+1}=F(x_k, \\theta; A)$. This final expression fulfills all requirements of the problem statement.", "answer": "$$\\boxed{\\sum_{t=0}^{T-1} \\left(\\frac{\\partial F(x_t, \\theta; A)}{\\partial \\theta}\\right)^T \\left( \\prod_{k=t+1}^{T-1} \\left(\\frac{\\partial F(x_k, \\theta; A)}{\\partial x}\\right)^T \\right) \\left(\\frac{\\partial H(x_T)}{\\partial x}\\right)^T \\nabla_{\\hat{y}}\\ell(y, H(x_T))}$$", "id": "3386838"}]}