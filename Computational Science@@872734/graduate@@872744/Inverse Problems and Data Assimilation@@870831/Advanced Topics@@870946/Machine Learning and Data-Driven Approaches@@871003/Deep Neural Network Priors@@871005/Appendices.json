{"hands_on_practices": [{"introduction": "The structure of a deep neural network itself can serve as a powerful prior, a concept famously demonstrated by the Deep Image Prior (DIP). This exercise delves into the \"spectral bias\" of such implicit priors, where the optimization process naturally favors fitting low-frequency functions before high-frequency details. By analyzing the learning dynamics within the Neural Tangent Kernel (NTK) framework, you will quantitatively derive how different frequency components of the solution converge at different rates, providing a concrete understanding of this fundamental implicit regularization mechanism [@problem_id:3375205].", "problem": "Consider the Deep Image Prior (DIP) reconstruction framework, in which the unknown image is parameterized as $x = G_{\\theta}(z)$ for a fixed input $z$ and trainable parameters $\\theta$, and is estimated by minimizing the squared data misfit $\\|A G_{\\theta}(z) - y\\|^{2}$ with respect to $\\theta$ using gradient descent. Assume the generator $G_{\\theta}$ is an infinitely wide, translation-equivariant convolutional network with independent and identically distributed zero-mean filters at initialization, so that in the infinite-width and small learning-rate limit, the function-space gradient flow for $f_{t} = G_{\\theta(t)}(z)$ is well-approximated by\n$$\n\\partial_{t} f_{t} = - \\gamma \\, K \\, \\nabla_{f} \\|A f - y\\|^{2} \\big|_{f = f_{t}},\n$$\nwhere $\\gamma  0$ is the learning-rate scaling and $K$ is the Neural Tangent Kernel (NTK) operator of $G_{\\theta}$ at initialization. Let $A$ be a linear, shift-invariant forward operator on images (e.g., a blur), and denote its adjoint by $A^{\\top}$. By standard properties of least-squares gradients, $\\nabla_{f} \\|A f - y\\|^{2} = 2 A^{\\top} (A f - y)$. Assume $K$ is shift-invariant with an isotropic Gaussian Fourier multiplier and $A$ is an isotropic Gaussian blur, specifically\n$$\n\\widehat{K}(\\omega) = \\exp\\!\\big(- \\sigma^{2} \\|\\omega\\|^{2}\\big), \\quad \\widehat{A}(\\omega) = \\exp\\!\\big(- \\beta \\|\\omega\\|^{2}\\big),\n$$\nfor parameters $\\sigma^{2}  0$ and $\\beta  0$, where $\\omega \\in \\mathbb{R}^{2}$ denotes the spatial frequency. Under these assumptions, the Fourier modes of the residual $\\widehat{r}_{t}(\\omega) \\equiv \\widehat{f}_{t}(\\omega) - \\widehat{f}_{\\infty}(\\omega)$ evolve independently, where $f_{\\infty}$ is the fixed point of the gradient flow.\n\nStarting from these premises and using only properties of linear operators, adjoints, and Fourier diagonalization of shift-invariant operators, derive the expression for the exponential decay rate $\\lambda(\\omega)$ governing the evolution of $\\widehat{r}_{t}(\\omega)$, defined by the ordinary differential equation\n$$\n\\partial_{t} \\widehat{r}_{t}(\\omega) = - \\lambda(\\omega) \\, \\widehat{r}_{t}(\\omega).\n$$\nThen, for $\\sigma^{2} = 10^{-3}$, $\\beta = 5 \\times 10^{-4}$, $\\gamma = 1$, and two isotropic frequency magnitudes $\\|\\omega_{l}\\| = 20$ and $\\|\\omega_{h}\\| = 60$, compute the ratio $\\lambda(\\omega_{h}) / \\lambda(\\omega_{l})$. Round your final numerical answer to four significant figures. The final answer must be a single real number without units.", "solution": "We begin with the given gradient flow equation for the function $f_t$:\n$$\n\\partial_{t} f_{t} = - \\gamma \\, K \\, \\nabla_{f} \\|A f - y\\|^{2} \\big|_{f = f_{t}}\n$$\nSubstituting the provided expression for the gradient, $\\nabla_{f} \\|A f - y\\|^{2} = 2 A^{\\top} (A f - y)$, we get:\n$$\n\\partial_{t} f_{t} = -2 \\gamma \\, K A^{\\top} (A f_{t} - y)\n$$\nAt the fixed point $f_{\\infty}$, the time derivative is zero, which implies that $f_{\\infty}$ satisfies $-2 \\gamma \\, K A^{\\top} (A f_{\\infty} - y) = 0$. We analyze the evolution of the residual $r_{t} = f_{t} - f_{\\infty}$ by subtracting the fixed point equation from the flow equation for $f_t$:\n$$\n\\partial_{t} r_{t} = -2 \\gamma \\, K A^{\\top} [ (A f_{t} - y) - (A f_{\\infty} - y) ] = -2 \\gamma \\, K A^{\\top} A (f_t - f_\\infty) = -2 \\gamma \\, K A^{\\top} A \\, r_{t}\n$$\nThis is a linear ordinary differential equation in function space. Since the operators $K$, $A^{\\top}$, and $A$ are all shift-invariant, they are diagonalized by the Fourier transform. Transforming the equation to the Fourier domain yields:\n$$\n\\partial_{t} \\widehat{r}_{t}(\\omega) = -2 \\gamma \\, \\widehat{(K A^{\\top} A)}(\\omega) \\, \\widehat{r}_{t}(\\omega)\n$$\nThe Fourier multiplier of the composite operator is the product of the individual multipliers: $\\widehat{(K A^{\\top} A)}(\\omega) = \\widehat{K}(\\omega) \\, \\widehat{A^{\\top}}(\\omega) \\, \\widehat{A}(\\omega)$. Since the multiplier for $A$ is real, $\\widehat{A^{\\top}}(\\omega) = \\widehat{A}(\\omega)$. Thus, the composite multiplier is:\n$$\n\\widehat{(K A^{\\top} A)}(\\omega) = \\widehat{K}(\\omega) (\\widehat{A}(\\omega))^{2} = \\exp(-\\sigma^{2} \\|\\omega\\|^{2}) \\left( \\exp(-\\beta \\|\\omega\\|^{2}) \\right)^{2} = \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega\\|^{2})\n$$\nPlugging this back into the differential equation for $\\widehat{r}_{t}(\\omega)$:\n$$\n\\partial_{t} \\widehat{r}_{t}(\\omega) = -2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega\\|^{2}) \\, \\widehat{r}_{t}(\\omega)\n$$\nBy comparing this to the given form $\\partial_{t} \\widehat{r}_{t}(\\omega) = - \\lambda(\\omega) \\, \\widehat{r}_{t}(\\omega)$, we identify the decay rate $\\lambda(\\omega)$:\n$$\n\\lambda(\\omega) = 2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega\\|^{2})\n$$\nNow, we compute the ratio $\\lambda(\\omega_{h}) / \\lambda(\\omega_{l})$ for the given frequency magnitudes $\\|\\omega_{h}\\| = 60$ and $\\|\\omega_{l}\\| = 20$.\n$$\n\\frac{\\lambda(\\omega_{h})}{\\lambda(\\omega_{l})} = \\frac{2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega_{h}\\|^{2})}{2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega_{l}\\|^{2})} = \\exp(-(\\sigma^{2} + 2\\beta) (\\|\\omega_{h}\\|^{2} - \\|\\omega_{l}\\|^{2}))\n$$\nWe substitute the numerical values: $\\sigma^{2} = 10^{-3}$, $\\beta = 5 \\times 10^{-4}$, $\\|\\omega_{l}\\|^{2} = 400$, and $\\|\\omega_{h}\\|^{2} = 3600$.\nFirst, calculate the coefficient and the difference of squared frequencies:\n$$\n\\sigma^{2} + 2\\beta = 10^{-3} + 2(5 \\times 10^{-4}) = 2 \\times 10^{-3}\n$$\n$$\n\\|\\omega_{h}\\|^{2} - \\|\\omega_{l}\\|^{2} = 3600 - 400 = 3200\n$$\nThe exponent is therefore:\n$$\n-(\\sigma^{2} + 2\\beta) (\\|\\omega_{h}\\|^{2} - \\|\\omega_{l}\\|^{2}) = -(2 \\times 10^{-3}) \\times 3200 = -6.4\n$$\nFinally, the ratio is:\n$$\n\\frac{\\lambda(\\omega_{h})}{\\lambda(\\omega_{l})} = \\exp(-6.4) \\approx 0.0016615555...\n$$\nRounding to four significant figures gives $0.001662$, or $1.662 \\times 10^{-3}$.", "answer": "$$\\boxed{1.662 \\times 10^{-3}}$$", "id": "3375205"}, {"introduction": "While powerful, priors defined by deep neural networks often result in non-convex energy landscapes, posing a significant challenge for optimization algorithms that can get trapped in poor local minima. This practice explores a classic and effective strategy—simulated annealing—adapted to this modern context. You will implement an annealing schedule that gradually \"cools\" a temperature parameter in the prior, smoothly deforming the objective function from a simple, convex-like surface to the target non-convex one, thereby guiding the solution towards a more desirable minimum [@problem_id:3375150].", "problem": "Consider a deterministic linear inverse problem with an unknown state vector $x \\in \\mathbb{R}^n$ observed through a linear forward operator $A \\in \\mathbb{R}^{m \\times n}$ with additive noise, producing data $y \\in \\mathbb{R}^m$. The Maximum A Posteriori (MAP) estimate minimizes the negative log-posterior under a prior defined by a Deep Neural Network (DNN). Let the data misfit be $E_{\\text{data}}(x) = \\frac{1}{2}\\|A x - y\\|_2^2$ and the prior energy be $E_{\\phi,\\tau}(x) = \\frac{1}{2}\\|g_{\\phi,\\tau}(x)\\|_2^2$, where $g_{\\phi,\\tau}$ is a DNN mapping that depends on a temperature parameter $\\tau  0$. The total energy to minimize is\n$$\nE_{\\text{total}}(x;\\lambda,\\tau) = E_{\\text{data}}(x) + \\lambda\\,E_{\\phi,\\tau}(x),\n$$\nwhere $\\lambda  0$ weights the prior relative to the data.\n\nTo explore temperature annealing, use a DNN prior based on a Sinusoidal Representation Network (SIREN) with a scaled sinusoidal activation that controls smoothness by $\\tau$. Specifically, define $g_{\\phi,\\tau}:\\mathbb{R}^n\\to\\mathbb{R}^n$ by\n$$\ng_{\\phi,\\tau}(x) = \\tau \\,\\sin\\!\\left(\\frac{W_1 x + b_1}{\\tau}\\right),\n$$\nwith $W_1 \\in \\mathbb{R}^{n \\times n}$ and $b_1 \\in \\mathbb{R}^n$. The sine function is applied componentwise. This choice yields a prior energy surface whose nonconvexity increases as $\\tau$ decreases: for large $\\tau$, the activation is approximately linear (convex quadratic prior), whereas for small $\\tau$, the activation becomes highly oscillatory, introducing many local minima.\n\nDerive from the Bayesian formulation and the definition of MAP that the gradient of $E_{\\text{total}}$ is given by the sum of the data gradient and the prior gradient induced by the network Jacobian. Implement a gradient descent solver for MAP and compare two strategies:\n- A cold start that directly minimizes $E_{\\text{total}}(x;\\lambda,\\tau_{\\text{final}})$ at the lowest temperature $\\tau_{\\text{final}}$.\n- An annealed optimization that starts at a high temperature and progressively lowers $\\tau$ through a schedule, minimizing $E_{\\text{total}}(x;\\lambda,\\tau)$ for each stage using the previous stage’s minimizer as initialization.\n\nUse the following precise and self-consistent setup with $n = m = 2$:\n- Forward operator $A = I_2$, the $2 \\times 2$ identity matrix.\n- Ground truth state $x^\\star = [2,-2]^\\top$.\n- Observation $y = x^\\star + \\eta$ with fixed noise $\\eta = [0.05,0.05]^\\top$.\n- Prior network weights $W_1 = I_2$.\n- Final temperature $\\tau_{\\text{final}} = 0.2$.\n- Bias $b_1$ chosen so that $x^\\star$ is exactly a zero of $g_{\\phi,\\tau_{\\text{final}}}$ for a selected integer vector $k \\in \\mathbb{Z}^2$, namely\n$$\nb_1 = k\\,\\pi\\,\\tau_{\\text{final}} - W_1 x^\\star,\n$$\nwith $k = [3,-3]^\\top$.\n- Initialization $x_0 = [0,0]^\\top$ for both cold start and annealed optimization.\n- Gradient descent step size $\\alpha = 0.1$.\n- Number of iterations per temperature stage $S = 300$.\n\nFor the SIREN prior defined above, the gradient of the prior energy is\n$$\n\\nabla E_{\\phi,\\tau}(x) = J_{g_{\\phi,\\tau}}(x)^\\top\\,g_{\\phi,\\tau}(x),\n$$\nwhere the Jacobian of $g_{\\phi,\\tau}$ is\n$$\nJ_{g_{\\phi,\\tau}}(x) = \\cos\\!\\left(\\frac{W_1 x + b_1}{\\tau}\\right)\\,W_1,\n$$\nwith cosine applied componentwise. Thus, the total gradient for gradient descent is\n$$\n\\nabla E_{\\text{total}}(x;\\lambda,\\tau) = (A^\\top (A x - y)) + \\lambda\\,\\Big[\\cos\\!\\Big(\\frac{W_1 x + b_1}{\\tau}\\Big)\\,W_1\\Big]^\\top \\Big[\\tau \\,\\sin\\!\\Big(\\frac{W_1 x + b_1}{\\tau}\\Big)\\Big].\n$$\n\nConstruct the annealed optimization by iterating gradient descent over a temperature schedule $\\{\\tau_1,\\tau_2,\\dots,\\tau_T\\}$, using the final iterate at $\\tau_t$ as the initialization for $\\tau_{t+1}$. The cold start is a single stage at $\\tau_{\\text{final}}$ with the same total number of iterations as the annealed run.\n\nTest Suite:\n- Case $1$: $\\lambda = 5.0$, temperature schedule $[5.0,1.0,0.5,0.2]$.\n- Case $2$: $\\lambda = 12.0$, temperature schedule $[5.0,1.0,0.5,0.2]$.\n- Case $3$: $\\lambda = 5.0$, temperature schedule $[0.2,0.2,0.2,0.2]$ (no annealing).\n\nFor each case, compute the Euclidean distances $D_{\\text{cold}} = \\|x_{\\text{cold}} - x^\\star\\|_2$ and $D_{\\text{anneal}} = \\|x_{\\text{anneal}} - x^\\star\\|_2$, and report the improvement as the float $I = D_{\\text{cold}} - D_{\\text{anneal}}$. A positive $I$ indicates that annealing helped escape poorer local minima to get closer to the ground truth.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite cases, with each improvement value rounded to six decimal places (e.g., $[i_1,i_2,i_3]$). No physical units are involved. All angles inside trigonometric functions are in radians by construction. The program must be self-contained and require no input.", "solution": "The problem requires finding the Maximum A Posteriori (MAP) estimate of $x$, which minimizes the total energy $E_{\\text{total}}(x;\\lambda,\\tau)$. We will use gradient descent, an iterative optimization algorithm where the estimate $x_k$ is updated at each step $k$ as:\n$$\nx_{k+1} = x_k - \\alpha \\nabla E_{\\text{total}}(x_k;\\lambda,\\tau)\n$$\nwhere $\\alpha$ is the step size. The core of the method is the computation of the gradient $\\nabla E_{\\text{total}}$. The total gradient is the sum of the data misfit gradient and the prior energy gradient. For the simplified case where $A=I_2$ and $W_1=I_2$, the total gradient is:\n$$\n\\nabla E_{\\text{total}}(x;\\lambda,\\tau) = (x-y) + \\lambda \\frac{\\tau}{2} \\sin\\left(2\\frac{x + b_1}{\\tau}\\right)\n$$\nwhere the sine function is applied element-wise. This expression is derived using the vector chain rule and the identity $2\\sin(\\theta)\\cos(\\theta) = \\sin(2\\theta)$.\n\nWe will implement and compare two optimization strategies:\n\n1.  **Annealed Optimization**: This strategy addresses the non-convexity of the energy landscape by starting optimization at a high temperature $\\tau$, where the prior is nearly quadratic and the energy landscape is smoother. The temperature is then gradually decreased according to a schedule $\\{\\tau_1, \\tau_2, \\dots, \\tau_T\\}$. For each temperature $\\tau_t$ in the schedule, gradient descent is run for $S$ iterations, using the final state from the previous stage as the initial state for the current one. This procedure helps the estimate navigate the complex energy surface and avoid getting trapped in poor local minima that become prominent at low $\\tau$.\n\n2.  **Cold Start Optimization**: In contrast, this strategy directly attempts to minimize the final, highly non-convex objective function $E_{\\text{total}}(x;\\lambda, \\tau_{\\text{final}})$. It starts from the same initial state $x_0$ and runs gradient descent for a total number of iterations equal to the total in the annealed schedule (i.e., $S \\times T$). This approach is susceptible to getting trapped in local minima near the initialization.\n\nFor each test case, we implement both strategies. The pre-calculated values for the observation $y$ and bias $b_1$ are used. The annealed run iterates through the temperature schedule, running $S=300$ steps of gradient descent at each temperature. The cold start run performs $S \\times (\\text{number of stages})$ iterations at the fixed final temperature. Finally, we compute the Euclidean distance from the ground truth $x^\\star$ for both results ($D_{\\text{anneal}}$ and $D_{\\text{cold}}$) and report the improvement $I = D_{\\text{cold}} - D_{\\text{anneal}}$. A positive improvement indicates the benefit of the annealing strategy. Case 3, where the temperature is constant, serves as a sanity check, for which we expect an improvement of zero.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the MAP estimation problem using cold start and annealed gradient descent,\n    and computes the improvement offered by annealing.\n    \"\"\"\n    # --- Define problem constants and parameters ---\n    # Dimension of the state space\n    n = 2\n    # Ground truth state vector\n    x_star = np.array([2.0, -2.0])\n    # Additive noise vector\n    eta = np.array([0.05, 0.05])\n    # Observation vector y = x_star + eta\n    y = x_star + eta\n    # Prior network weights W1 (Identity matrix)\n    W1 = np.identity(n)\n    # Final temperature for the prior\n    tau_final = 0.2\n    # Integer vector for bias calculation\n    k_vec = np.array([3.0, -3.0])\n    # Bias vector b1, chosen so g(x_star) at tau_final is zero\n    b1 = k_vec * np.pi * tau_final - W1 @ x_star\n    # Initial state for gradient descent\n    x0 = np.array([0.0, 0.0])\n    # Gradient descent step size (learning rate)\n    alpha = 0.1\n    # Number of iterations per temperature stage\n    S = 300\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        (5.0, [5.0, 1.0, 0.5, 0.2]),\n        (12.0, [5.0, 1.0, 0.5, 0.2]),\n        (5.0, [0.2, 0.2, 0.2, 0.2]),\n    ]\n\n    def grad_E_total(x, tau, lam):\n        \"\"\"\n        Computes the gradient of the total energy E_total.\n        This implementation uses the simplified form derived for A=I and W1=I.\n        \"\"\"\n        # Gradient of the data misfit term: x - y\n        grad_data = x - y\n        \n        # Argument for the trigonometric function in the prior's gradient\n        # Based on the derived formula: (tau/2) * sin(2 * (x + b1) / tau)\n        arg = 2.0 * (x + b1) / tau\n        \n        # Gradient of the prior term\n        grad_prior = lam * (tau / 2.0) * np.sin(arg)\n        \n        # Total gradient is the sum of the two parts\n        return grad_data + grad_prior\n\n    results = []\n    for lam, schedule in test_cases:\n        # --- ANNEALED OPTIMIZATION ---\n        x_anneal = np.copy(x0)\n        # Iterate through the temperature schedule\n        for tau in schedule:\n            # For each temperature, run S steps of gradient descent\n            for _ in range(S):\n                grad = grad_E_total(x_anneal, tau, lam)\n                x_anneal = x_anneal - alpha * grad\n        \n        # Calculate Euclidean distance to the ground truth\n        D_anneal = np.linalg.norm(x_anneal - x_star)\n\n        # --- COLD START OPTIMIZATION ---\n        x_cold = np.copy(x0)\n        # Use the final temperature from the schedule\n        tau_cold = schedule[-1]\n        # Total number of iterations is the same as for the annealed run\n        total_iterations = S * len(schedule)\n        \n        # Run gradient descent for the total number of iterations at the fixed cold temperature\n        for _ in range(total_iterations):\n            grad = grad_E_total(x_cold, tau_cold, lam)\n            x_cold = x_cold - alpha * grad\n            \n        # Calculate Euclidean distance to the ground truth\n        D_cold = np.linalg.norm(x_cold - x_star)\n\n        # --- COMPUTE IMPROVEMENT AND STORE RESULT ---\n        improvement = D_cold - D_anneal\n        results.append(improvement)\n\n    # Format the results to six decimal places for the final output\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format: [i_1,i_2,i_3]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the solver function\nsolve()\n```", "id": "3375150"}, {"introduction": "The practical utility of any inverse problem solver depends critically on its robustness to small perturbations in the input data. This exercise provides a rigorous framework for analyzing the adversarial vulnerability of reconstructions regularized by DNN priors and comparing it to classical methods like Tikhonov regularization. By applying the Implicit Function Theorem to the MAP optimality conditions, you will derive and compute a linearized sensitivity measure that quantifies how much the solution can change in the worst-case scenario, offering crucial insights into the stability of these advanced models [@problem_id:3375211].", "problem": "Consider linear inverse problems of the form $y = A x + \\eta$ with $y \\in \\mathbb{R}^m$, $x \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{m \\times n}$, and additive noise $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I_m)$. By Bayes’ rule, the posterior density satisfies $p(x \\mid y) \\propto p(y \\mid x) \\, p(x)$, where the likelihood $p(y \\mid x)$ is Gaussian. The Maximum A Posteriori (MAP) estimator $x^\\star(y)$ is any minimizer of the negative log-posterior. We study adversarial vulnerability defined by the worst-case change in the MAP estimate under a bounded perturbation $\\delta y$ to the data, that is, $\\max_{\\lVert \\delta y \\rVert_2 \\le \\varepsilon} \\lVert x^\\star(y + \\delta y) - x^\\star(y) \\rVert_2$. Compare this sensitivity for two priors: a classical Tikhonov prior and a Deep Neural Network (DNN) prior modeled as an energy-based prior. The primary goals are to derive, from first principles, tractable expressions for these sensitivities and to compute them numerically for a test suite.\n\nStart from the following foundations: (i) Bayes’ rule $p(x \\mid y) \\propto p(y \\mid x) p(x)$, (ii) Gaussian likelihood $p(y \\mid x) \\propto \\exp\\!\\left(-\\frac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 \\right)$, (iii) the MAP estimator $x^\\star(y) \\in \\arg\\min_x \\Phi(x; y)$ where $\\Phi$ is the negative log-posterior, and (iv) the Singular Value Decomposition (SVD) characterization of operator norm for linear maps. Do not invoke any unproven shortcut formulas; derive all expressions you use from these bases together with standard first-order optimality and linearization principles.\n\nDefine the two priors as follows:\n- Tikhonov prior: $p_{\\text{Tik}}(x) \\propto \\exp\\!\\left(-\\frac{\\beta}{2} \\lVert x \\rVert_2^2\\right)$ with parameter $\\beta  0$.\n- Deep Neural Network prior: an energy-based model $p_\\phi(x) \\propto \\exp\\!\\left(- \\alpha \\, E_\\phi(x)\\right)$ with $\\alpha  0$ and energy $E_\\phi(x) = \\frac{1}{2} \\lVert x - g_\\phi(x) \\rVert_2^2$, where $g_\\phi$ is a fixed two-layer feedforward network. The function $g_\\phi$ is constructed as $g_\\phi(x) = W_2 \\, \\tanh(W_1 x + b_1) + b_2$, where $\\tanh(\\cdot)$ is applied elementwise. For any $x$, the Jacobian of $g_\\phi$ is $J_g(x) = W_2 \\, \\operatorname{diag}\\!\\left(1 - \\tanh^2(W_1 x + b_1)\\right) W_1$.\n\nTasks:\n- Using the Tikhonov prior, derive the MAP objective $\\Phi_{\\text{Tik}}(x; y)$ and the corresponding linear mapping from $y$ to $x^\\star(y)$. From the optimization and SVD facts, derive the exact worst-case perturbation $\\delta y$ (subject to $\\lVert \\delta y \\rVert_2 \\le \\varepsilon$) and the corresponding maximal shift in the MAP estimate. Your derivation should start from the first-order optimality condition and proceed to a linear $y \\mapsto x^\\star(y)$ operator whose spectral norm controls the worst-case shift.\n- Using the DNN prior, write the MAP objective $\\Phi_\\phi(x; y)$. Derive the first-order optimality condition and, by applying a local quadratic approximation (Gauss–Newton linearization) at the MAP point, derive a linearized sensitivity mapping from $y$ to $x^\\star(y)$. Use the Implicit Function Theorem (IFT) logic together with the Gauss–Newton Hessian approximation to obtain a tractable linear map whose operator norm upper-bounds the worst-case shift. You must explicitly justify the Gauss–Newton approximation by ignoring second-order derivatives of $g_\\phi$ consistent with the standard Gauss–Newton method. Introduce a small Tikhonov damping $\\gamma = 10^{-6}$ on the Hessian approximation to ensure numerical stability.\n- Implement a program that, for each test case below, computes:\n  1. The maximal shift magnitude under the Tikhonov prior, denoted $S_{\\text{Tik}}$, for the specified $\\varepsilon$.\n  2. The linearized maximal shift magnitude under the DNN prior, denoted $S_{\\text{DNN}}$, for the specified $\\varepsilon$.\n  3. The ratio $R = S_{\\text{DNN}} / S_{\\text{Tik}}$, with the convention that if $S_{\\text{Tik}} = 0$ then $R$ is defined to be $0$.\n  \nAll computations are purely numerical; no physical units are involved. Angles, where applicable within $\\tanh(\\cdot)$, are unitless. The final output must be a single line containing a comma-separated list enclosed in square brackets arranged as $[S_{\\text{Tik}}^{(1)}, S_{\\text{DNN}}^{(1)}, R^{(1)}, S_{\\text{Tik}}^{(2)}, S_{\\text{DNN}}^{(2)}, R^{(2)}, \\dots]$ across all test cases.\n\nNeural network parameters for $g_\\phi$:\n- Dimensions: $n = 5$, hidden width $h = 3$.\n- $W_1 = \\begin{bmatrix}\n0.5  -0.3  0.1  0.0  0.2 \\\\\n0.0  0.4  -0.2  0.3  0.1 \\\\\n-0.1  0.0  0.2  -0.4  0.3\n\\end{bmatrix}$, $b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$,\n- $W_2 = \\begin{bmatrix}\n0.2  -0.1  0.0 \\\\\n0.0  0.3  -0.2 \\\\\n-0.1  0.0  0.25 \\\\\n0.0  -0.2  0.1 \\\\\n0.15  0.05  -0.05\n\\end{bmatrix}$, $b_2 = \\begin{bmatrix} 0.0 \\\\ 0.05 \\\\ -0.03 \\\\ 0.02 \\\\ -0.01 \\end{bmatrix}$.\n\nTest suite:\n- Case $1$: $m = 5$, $n = 5$, \n  $A = \\begin{bmatrix}\n1.0  0.2  0.0  0.0  0.0 \\\\\n0.0  1.0  0.3  0.0  0.0 \\\\\n0.0  0.0  1.0  0.4  0.0 \\\\\n0.0  0.0  0.0  1.0  0.5 \\\\\n0.1  0.0  0.0  0.0  1.0\n\\end{bmatrix}$, $y = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.3 \\\\ 0.2 \\\\ -0.1 \\end{bmatrix}$, $\\sigma = 0.05$, $\\beta = 0.5$, $\\alpha = 0.3$, $\\varepsilon = 0.01$.\n- Case $2$: $m = 5$, $n = 5$, \n  $A = \\begin{bmatrix}\n1.0  0.99  0.98  0.0  0.0 \\\\\n0.99  0.98  0.97  0.0  0.0 \\\\\n0.98  0.97  0.96  0.0  0.0 \\\\\n0.0  0.0  0.0  1.0  0.99 \\\\\n0.0  0.0  0.0  0.99  0.98\n\\end{bmatrix}$, $y = \\begin{bmatrix} -0.2 \\\\ 0.1 \\\\ -0.1 \\\\ 0.05 \\\\ -0.05 \\end{bmatrix}$, $\\sigma = 0.05$, $\\beta = 0.5$, $\\alpha = 1.0$, $\\varepsilon = 0.01$.\n- Case $3$: $m = 5$, $n = 5$, \n  $A$ is the same as in Case $2$, $y = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.3 \\\\ 0.2 \\\\ -0.1 \\end{bmatrix}$, $\\sigma = 0.05$, $\\beta = 0.1$, $\\alpha = 0.1$, $\\varepsilon = 0.05$.\n- Case $4$: $m = 5$, $n = 5$, \n  $A$ is the same as in Case $1$, $y = \\begin{bmatrix} -0.2 \\\\ 0.1 \\\\ -0.1 \\\\ 0.05 \\\\ -0.05 \\end{bmatrix}$, $\\sigma = 0.05$, $\\beta = 2.0$, $\\alpha = 5.0$, $\\varepsilon = 0.0$.\n\nImplementation requirements:\n- For the Tikhonov prior, derive and use the exact linear mapping from $y$ to $x^\\star(y)$ stemming from the first-order optimality condition, and compute the induced operator norm to obtain the worst-case shift.\n- For the DNN prior, first compute $x^\\star(y)$ by solving the first-order optimality condition for the MAP estimate using a damped Newton method with the Gauss–Newton Hessian approximation $H(x) \\approx \\frac{1}{\\sigma^2} A^\\top A + \\alpha \\left(I - J_g(x)\\right)^\\top \\left(I - J_g(x)\\right) + \\gamma I$, where $\\gamma = 10^{-6}$. Then compute the linearized sensitivity map from $y$ to $x^\\star(y)$ using the implicit-function linearization at $x^\\star(y)$, and take its operator norm to estimate the worst-case shift; finally multiply by $\\varepsilon$ to get $S_{\\text{DNN}}$.\n- Numerical tolerances: use a stopping criterion based on the Newton step norm being less than $10^{-10}$ or a maximum of $200$ iterations.\n- The final output format must be a single line containing the results as a comma-separated list enclosed in square brackets, ordered as $[S_{\\text{Tik}}^{(1)}, S_{\\text{DNN}}^{(1)}, R^{(1)}, S_{\\text{Tik}}^{(2)}, S_{\\text{DNN}}^{(2)}, R^{(2)}, S_{\\text{Tik}}^{(3)}, S_{\\text{DNN}}^{(3)}, R^{(3)}, S_{\\text{Tik}}^{(4)}, S_{\\text{DNN}}^{(4)}, R^{(4)}]$.", "solution": "We begin by restating the fundamental Bayesian formulation. With $y = A x + \\eta$ and $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I_m)$, the likelihood is $p(y \\mid x) \\propto \\exp\\!\\left(-\\frac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 \\right)$. A prior density $p(x)$ yields a posterior $p(x \\mid y) \\propto p(y \\mid x) p(x)$, and the Maximum A Posteriori (MAP) estimator $x^\\star(y)$ solves the optimization problem $x^\\star(y) \\in \\arg\\min_x \\Phi(x; y)$, where $\\Phi(x; y) = - \\log p(y \\mid x) - \\log p(x) + \\text{const}(y)$.\n\nClassical Tikhonov prior. The Tikhonov prior is $p_{\\text{Tik}}(x) \\propto \\exp\\!\\left(-\\frac{\\beta}{2} \\lVert x \\rVert_2^2\\right)$ with $\\beta  0$. The negative log-posterior is\n$$\n\\Phi_{\\text{Tik}}(x; y) = \\frac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 + \\frac{\\beta}{2} \\lVert x \\rVert_2^2.\n$$\nThe first-order optimality condition is\n$$\n\\nabla_x \\Phi_{\\text{Tik}}(x; y) = \\frac{1}{\\sigma^2} A^\\top (A x - y) + \\beta x = 0.\n$$\nRearranging gives a linear system\n$$\n\\left( \\frac{1}{\\sigma^2} A^\\top A + \\beta I \\right) x = \\frac{1}{\\sigma^2} A^\\top y.\n$$\nThus, the MAP estimator is a linear function of the data,\n$$\nx^\\star_{\\text{Tik}}(y) = M_{\\text{Tik}} \\, y, \\quad \\text{where} \\quad M_{\\text{Tik}} = \\left( \\frac{1}{\\sigma^2} A^\\top A + \\beta I \\right)^{-1} \\frac{1}{\\sigma^2} A^\\top.\n$$\nFor any perturbation $\\delta y$ with $\\lVert \\delta y \\rVert_2 \\le \\varepsilon$, the induced change in the MAP estimate is\n$$\n\\delta x^\\star_{\\text{Tik}} = M_{\\text{Tik}} \\, \\delta y,\n$$\nso the worst-case shift magnitude is\n$$\n\\max_{\\lVert \\delta y \\rVert_2 \\le \\varepsilon} \\lVert M_{\\text{Tik}} \\, \\delta y \\rVert_2 = \\varepsilon \\, \\lVert M_{\\text{Tik}} \\rVert_2,\n$$\nwhere $\\lVert \\cdot \\rVert_2$ is the operator norm (largest singular value). The worst-case direction for $\\delta y$ is the right singular vector of $M_{\\text{Tik}}$ associated with its largest singular value. This follows from the Singular Value Decomposition (SVD) characterization of the spectral norm.\n\nDeep Neural Network prior. Consider an energy-based prior with density $p_\\phi(x) \\propto \\exp\\!\\left(- \\alpha E_\\phi(x) \\right)$, $\\alpha  0$, with energy\n$$\nE_\\phi(x) = \\frac{1}{2} \\lVert x - g_\\phi(x) \\rVert_2^2,\n$$\nwhere $g_\\phi(x) = W_2 \\, \\tanh(W_1 x + b_1) + b_2$. The negative log-posterior is\n$$\n\\Phi_\\phi(x; y) = \\frac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 + \\alpha \\, E_\\phi(x).\n$$\nThe gradient is\n$$\n\\nabla_x \\Phi_\\phi(x; y) = \\frac{1}{\\sigma^2} A^\\top (A x - y) + \\alpha \\, \\nabla_x E_\\phi(x).\n$$\nLet $r(x) = x - g_\\phi(x)$. Then $E_\\phi(x) = \\frac{1}{2} \\lVert r(x) \\rVert_2^2$, and\n$$\n\\nabla_x E_\\phi(x) = J_r(x)^\\top r(x) = \\left(I - J_g(x)\\right)^\\top \\left(x - g_\\phi(x)\\right),\n$$\nwhere $J_g(x)$ is the Jacobian of $g_\\phi$ at $x$ and $J_r(x) = I - J_g(x)$. The exact Hessian of $E_\\phi$ is\n$$\n\\nabla_x^2 E_\\phi(x) = J_r(x)^\\top J_r(x) + \\sum_{i=1}^n r_i(x) \\, \\nabla_x^2 r_i(x),\n$$\nbut the Gauss–Newton (GN) approximation neglects the second term, yielding\n$$\n\\nabla_x^2 E_\\phi(x) \\approx J_r(x)^\\top J_r(x) = \\left(I - J_g(x)\\right)^\\top \\left(I - J_g(x)\\right).\n$$\nThis approximation is standard when residuals are moderate and provides a positive semi-definite curvature model. At a MAP point $x^\\star_\\phi(y)$ satisfying $\\nabla_x \\Phi_\\phi(x^\\star_\\phi(y); y) = 0$, the Gauss–Newton Hessian approximation of the negative log-posterior is\n$$\nH(x^\\star_\\phi(y)) \\approx \\frac{1}{\\sigma^2} A^\\top A + \\alpha \\left(I - J_g(x^\\star_\\phi(y))\\right)^\\top \\left(I - J_g(x^\\star_\\phi(y))\\right).\n$$\nTo ensure numerical stability, we add Tikhonov damping $\\gamma I$ with $\\gamma = 10^{-6}$:\n$$\n\\widetilde{H}(x^\\star_\\phi(y)) = H(x^\\star_\\phi(y)) + \\gamma I.\n$$\nTo obtain the sensitivity of $x^\\star_\\phi(y)$ to $y$, differentiate the first-order optimality condition with respect to $y$ using the Implicit Function Theorem (IFT):\n$$\n\\nabla_x^2 \\Phi_\\phi(x^\\star_\\phi(y); y) \\, \\frac{\\partial x^\\star_\\phi}{\\partial y}(y) + \\frac{\\partial}{\\partial y} \\nabla_x \\Phi_\\phi(x^\\star_\\phi(y); y) = 0.\n$$\nSince $\\frac{\\partial}{\\partial y} \\left( \\frac{1}{\\sigma^2} A^\\top (A x^\\star_\\phi(y) - y) \\right) = - \\frac{1}{\\sigma^2} A^\\top$ and the prior term depends only on $x$, the linearized sensitivity mapping is\n$$\n\\frac{\\partial x^\\star_\\phi}{\\partial y}(y) \\approx \\widetilde{H}(x^\\star_\\phi(y))^{-1} \\frac{1}{\\sigma^2} A^\\top.\n$$\nTherefore, for small $\\delta y$ with $\\lVert \\delta y \\rVert_2 \\le \\varepsilon$, the worst-case linearized shift is\n$$\n\\max_{\\lVert \\delta y \\rVert_2 \\le \\varepsilon} \\left\\lVert \\frac{\\partial x^\\star_\\phi}{\\partial y}(y) \\, \\delta y \\right\\rVert_2 \\approx \\varepsilon \\left\\lVert \\widetilde{H}(x^\\star_\\phi(y))^{-1} \\frac{1}{\\sigma^2} A^\\top \\right\\rVert_2.\n$$\nThis provides a principled, tractable mechanism to assess adversarial vulnerability under a deep neural network prior using Gauss–Newton linearization and the Implicit Function Theorem.\n\nAlgorithmic realization. For the Tikhonov prior, compute\n$$\nM_{\\text{Tik}} = \\left( \\frac{1}{\\sigma^2} A^\\top A + \\beta I \\right)^{-1} \\frac{1}{\\sigma^2} A^\\top,\n$$\nand set $S_{\\text{Tik}} = \\varepsilon \\, \\lVert M_{\\text{Tik}} \\rVert_2$. For the deep neural network prior, first compute $x^\\star_\\phi(y)$ by solving $\\nabla_x \\Phi_\\phi(x; y) = 0$ using a damped Newton method with the Gauss–Newton Hessian $\\widetilde{H}(x)$. Specifically, iterate\n$$\n\\widetilde{H}(x_k) \\, \\Delta x_k = - \\nabla_x \\Phi_\\phi(x_k; y), \\quad x_{k+1} = x_k + \\Delta x_k,\n$$\nuntil $\\lVert \\Delta x_k \\rVert_2  10^{-10}$ or a maximum of $200$ iterations is reached. At the converged $x^\\star_\\phi(y)$, form\n$$\nM_{\\text{DNN}}(y) \\approx \\widetilde{H}(x^\\star_\\phi(y))^{-1} \\frac{1}{\\sigma^2} A^\\top,\n$$\nand set $S_{\\text{DNN}} = \\varepsilon \\, \\lVert M_{\\text{DNN}}(y) \\rVert_2$. Finally, define $R = S_{\\text{DNN}} / S_{\\text{Tik}}$; if $S_{\\text{Tik}} = 0$, return $R = 0$ by convention to avoid undefined values.\n\nTest suite specification. The matrices $A$, vectors $y$, scalars $\\sigma$, $\\beta$, $\\alpha$, and $\\varepsilon$ are given numerically in the problem statement. The deep neural network prior is specified by the numeric matrices $W_1$, $W_2$ and vectors $b_1$, $b_2$, with $g_\\phi(x) = W_2 \\, \\tanh(W_1 x + b_1) + b_2$ and $J_g(x) = W_2 \\, \\operatorname{diag}(1 - \\tanh^2(W_1 x + b_1)) W_1$. For each case, we compute $S_{\\text{Tik}}$, $S_{\\text{DNN}}$, and $R$, and report the results as a single line list ordered across cases as specified. Since Case $4$ has $\\varepsilon = 0$, both $S_{\\text{Tik}}$ and $S_{\\text{DNN}}$ are $0$ and thus $R = 0$ by the defined convention.\n\nThis solution integrates statistical inverse problem principles, optimization optimality, Gauss–Newton approximation, and the Implicit Function Theorem to derive sensitivity operators whose spectral norms govern worst-case adversarial vulnerability for both classical and deep neural network priors, followed by consistent numerical realization.", "answer": "```python\nimport numpy as np\n\n# Deep Neural Network prior specification: g_phi(x) = W2 * tanh(W1 x + b1) + b2\ndef g_phi(x, W1, b1, W2, b2):\n    z1 = W1 @ x + b1\n    h = np.tanh(z1)\n    return W2 @ h + b2\n\ndef J_g_phi(x, W1, b1, W2):\n    z1 = W1 @ x + b1\n    dh = 1.0 - np.tanh(z1) ** 2  # elementwise derivative of tanh\n    # J_g = W2 * diag(dh) * W1\n    return W2 @ (np.diag(dh) @ W1)\n\ndef map_tikhonov_operator(A, sigma, beta):\n    # M_Tik = (A^T A / sigma^2 + beta I)^(-1) * (A^T / sigma^2)\n    n = A.shape[1]\n    H = (A.T @ A) / (sigma**2) + beta * np.eye(n)\n    M = np.linalg.solve(H, A.T / (sigma**2))\n    return M\n\ndef newton_map_dnn_xstar(A, y, sigma, alpha, W1, b1, W2, b2, gamma=1e-6, tol=1e-10, maxit=200):\n    n = A.shape[1]\n    x = np.zeros(n)\n    AT = A.T\n    ATA_over_sigma2 = (AT @ A) / (sigma**2)\n    for _ in range(maxit):\n        # Compute g, Jg\n        g = g_phi(x, W1, b1, W2, b2)\n        Jg = J_g_phi(x, W1, b1, W2)\n        # Residual r = x - g(x)\n        r = x - g\n        # Gradient: A^T(Ax - y)/sigma^2 + alpha * (I - Jg)^T * r\n        data_grad = AT @ (A @ x - y) / (sigma**2)\n        I_minus_Jg = np.eye(n) - Jg\n        prior_grad = alpha * (I_minus_Jg.T @ r)\n        grad = data_grad + prior_grad\n        # Gauss-Newton Hessian approx: A^T A / sigma^2 + alpha * (I - Jg)^T (I - Jg) + gamma I\n        H_gn = ATA_over_sigma2 + alpha * (I_minus_Jg.T @ I_minus_Jg) + gamma * np.eye(n)\n        try:\n            dx = np.linalg.solve(H_gn, -grad)\n        except np.linalg.LinAlgError:\n            # Fallback to least-squares solve if ill-conditioned\n            dx = np.linalg.lstsq(H_gn, -grad, rcond=None)[0]\n        x = x + dx\n        if np.linalg.norm(dx)  tol:\n            break\n    return x\n\ndef dnn_sensitivity_operator(A, y, sigma, alpha, W1, b1, W2, b2, gamma=1e-6):\n    # Compute x* via Newton-GN\n    x_star = newton_map_dnn_xstar(A, y, sigma, alpha, W1, b1, W2, b2, gamma=gamma)\n    # Build GN Hessian at x*\n    n = A.shape[1]\n    AT = A.T\n    ATA_over_sigma2 = (AT @ A) / (sigma**2)\n    Jg = J_g_phi(x_star, W1, b1, W2)\n    I_minus_Jg = np.eye(n) - Jg\n    H_gn = ATA_over_sigma2 + alpha * (I_minus_Jg.T @ I_minus_Jg) + gamma * np.eye(n)\n    # Sensitivity M_DNN = H_gn^{-1} * (A^T / sigma^2)\n    try:\n        M = np.linalg.solve(H_gn, AT / (sigma**2))\n    except np.linalg.LinAlgError:\n        M = np.linalg.lstsq(H_gn, AT / (sigma**2), rcond=None)[0]\n    return M\n\ndef spectral_norm(M):\n    # Largest singular value\n    s = np.linalg.svd(M, compute_uv=False)\n    return float(s[0])\n\ndef solve():\n    # Network parameters\n    W1 = np.array([\n        [0.5, -0.3, 0.1, 0.0, 0.2],\n        [0.0,  0.4, -0.2, 0.3, 0.1],\n        [-0.1, 0.0, 0.2, -0.4, 0.3]\n    ], dtype=float)\n    b1 = np.array([0.1, -0.2, 0.05], dtype=float)\n    W2 = np.array([\n        [0.2, -0.1,  0.0],\n        [0.0,  0.3, -0.2],\n        [-0.1, 0.0,  0.25],\n        [0.0, -0.2,  0.1],\n        [0.15, 0.05, -0.05]\n    ], dtype=float)\n    b2 = np.array([0.0, 0.05, -0.03, 0.02, -0.01], dtype=float)\n\n    # Test cases\n    A1 = np.array([\n        [1.0, 0.2, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.3, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.4, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.5],\n        [0.1, 0.0, 0.0, 0.0, 1.0]\n    ], dtype=float)\n    A2 = np.array([\n        [1.0, 0.99, 0.98, 0.0, 0.0],\n        [0.99, 0.98, 0.97, 0.0, 0.0],\n        [0.98, 0.97, 0.96, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.99],\n        [0.0, 0.0, 0.0, 0.99, 0.98]\n    ], dtype=float)\n    y1 = np.array([1.0, -0.5, 0.3, 0.2, -0.1], dtype=float)\n    y2 = np.array([-0.2, 0.1, -0.1, 0.05, -0.05], dtype=float)\n    sigma = 0.05\n\n    test_cases = [\n        # (A, y, beta, alpha, epsilon)\n        (A1, y1, 0.5, 0.3, 0.01),\n        (A2, y2, 0.5, 1.0, 0.01),\n        (A2, y1, 0.1, 0.1, 0.05),\n        (A1, y2, 2.0, 5.0, 0.0),\n    ]\n\n    results = []\n    for A, y, beta, alpha, eps in test_cases:\n        # Tikhonov operator and sensitivity\n        M_tik = map_tikhonov_operator(A, sigma, beta)\n        s_tik = eps * spectral_norm(M_tik)\n\n        # DNN linearized sensitivity at MAP (GN-IFT)\n        M_dnn = dnn_sensitivity_operator(A, y, sigma, alpha, W1, b1, W2, b2, gamma=1e-6)\n        s_dnn = eps * spectral_norm(M_dnn)\n\n        # Ratio with convention: if s_tik == 0, return 0\n        R = 0.0 if np.isclose(s_tik, 0.0) else (s_dnn / s_tik)\n\n        results.extend([s_tik, s_dnn, R])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3375211"}]}