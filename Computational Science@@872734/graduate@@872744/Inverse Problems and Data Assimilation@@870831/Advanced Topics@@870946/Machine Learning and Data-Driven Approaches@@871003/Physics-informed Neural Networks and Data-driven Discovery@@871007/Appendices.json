{"hands_on_practices": [{"introduction": "Before deploying a complex numerical solver, it is essential to verify its correctness. This practice introduces the Method of Manufactured Solutions, a standard technique in scientific computing for creating a test problem with a known analytical answer to validate your implementation. By working through the exercise [@problem_id:3410533], you will derive the exact source term and derivative quantities for a Poisson problem, providing a ground truth to rigorously test the automatic differentiation engine of a Physics-Informed Neural Network.", "problem": "Consider the Poisson equation on the unit square domain $\\Omega = (0,1)\\times(0,1)$ with homogeneous Dirichlet boundary conditions on $\\partial\\Omega$. The governing law is the Poisson equation $-\\Delta u = f$ in $\\Omega$, where the Laplacian operator is defined by $\\Delta u = u_{xx} + u_{yy}$, and the Dirichlet boundary condition is $u = 0$ on $\\partial\\Omega$. In physics-informed neural networks (PINNs), the physics loss term typically involves the pointwise residual $r_{\\mathrm{PDE}}(x,y) = -\\Delta u(x,y) - f(x,y)$, and the boundary loss term enforces $r_{\\mathrm{BC}}(x,y) = u(x,y)$ on $\\partial\\Omega$. The gradients needed to validate automatic differentiation include $u_x$, $u_y$, $u_{xx}$, $u_{yy}$, and the input-gradient of the physics residual $\\nabla r_{\\mathrm{PDE}}(x,y) = \\left(\\frac{\\partial r_{\\mathrm{PDE}}}{\\partial x}(x,y), \\frac{\\partial r_{\\mathrm{PDE}}}{\\partial y}(x,y)\\right)$.\n\nYou are given the analytic solution $u(x,y) = \\sin(\\pi x)\\sin(\\pi y)$, and asked to design a canonical test for validating a PINN implementation. Starting from the fundamental definitions of the Laplacian and the Dirichlet boundary condition, derive the source term $f(x,y)$ so that $u$ satisfies $-\\Delta u = f$ in $\\Omega$, and derive the exact residuals $r_{\\mathrm{PDE}}(x,y)$ and $r_{\\mathrm{BC}}(x,y)$, together with the exact first and second derivatives $u_x$, $u_y$, $u_{xx}$, $u_{yy}$, and the gradient $\\nabla r_{\\mathrm{PDE}}(x,y)$ implied by this choice.\n\nWhich option correctly specifies $f(x,y)$, $r_{\\mathrm{PDE}}(x,y)$, $r_{\\mathrm{BC}}(x,y)$, $u_x(x,y)$, $u_y(x,y)$, $u_{xx}(x,y)$, $u_{yy}(x,y)$, and $\\nabla r_{\\mathrm{PDE}}(x,y)$ for the given $u$?\n\nA. $f(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $r_{\\mathrm{PDE}}(x,y) = 0$, $r_{\\mathrm{BC}}(x,y) = 0$ on $\\partial\\Omega$, $u_x(x,y) = \\pi \\cos(\\pi x)\\sin(\\pi y)$, $u_y(x,y) = \\pi \\sin(\\pi x)\\cos(\\pi y)$, $u_{xx}(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $u_{yy}(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, and $\\nabla r_{\\mathrm{PDE}}(x,y) = (0,0)$.\n\nB. $f(x,y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $r_{\\mathrm{PDE}}(x,y) = 4\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $r_{\\mathrm{BC}}(x,y) = 0$ on $\\partial\\Omega$, $u_x(x,y) = \\pi \\sin(\\pi x)\\sin(\\pi y)$, $u_y(x,y) = \\pi \\cos(\\pi x)\\cos(\\pi y)$, $u_{xx}(x,y) = \\pi^2 \\cos(\\pi x)\\sin(\\pi y)$, $u_{yy}(x,y) = \\pi^2 \\sin(\\pi x)\\cos(\\pi y)$, and $\\nabla r_{\\mathrm{PDE}}(x,y) = \\left(4\\pi^3 \\cos(\\pi x)\\sin(\\pi y), 4\\pi^3 \\sin(\\pi x)\\cos(\\pi y)\\right)$.\n\nC. $f(x,y) = \\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $r_{\\mathrm{PDE}}(x,y) = \\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $r_{\\mathrm{BC}}(x,y) = 0$ on $\\partial\\Omega$, $u_x(x,y) = \\pi \\cos(\\pi x)\\sin(\\pi y)$, $u_y(x,y) = \\pi \\sin(\\pi x)\\cos(\\pi y)$, $u_{xx}(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $u_{yy}(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, and $\\nabla r_{\\mathrm{PDE}}(x,y) = \\left(\\pi^3 \\cos(\\pi x)\\sin(\\pi y), \\pi^3 \\sin(\\pi x)\\cos(\\pi y)\\right)$.\n\nD. $f(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $r_{\\mathrm{PDE}}(x,y) = 0$, $r_{\\mathrm{BC}}(x,y) = \\partial_n u(x,y)$ on $\\partial\\Omega$, $u_x(x,y) = \\pi \\cos(\\pi x)\\cos(\\pi y)$, $u_y(x,y) = \\pi \\cos(\\pi x)\\cos(\\pi y)$, $u_{xx}(x,y) = \\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $u_{yy}(x,y) = \\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, and $\\nabla r_{\\mathrm{PDE}}(x,y) = (0,0)$.", "solution": "We begin from the fundamental definitions. The Laplacian is defined by $\\Delta u = u_{xx} + u_{yy}$, and the Poisson equation is $-\\Delta u = f$ in $\\Omega = (0,1)\\times(0,1)$, with homogeneous Dirichlet boundary condition $u = 0$ on $\\partial\\Omega$. For the given analytic solution $u(x,y) = \\sin(\\pi x)\\sin(\\pi y)$, we compute the required derivatives using the chain rule and standard differentiation rules for the sine and cosine functions.\n\nFirst, compute the first derivatives:\n$u_x(x,y) = \\frac{\\partial}{\\partial x}\\left[\\sin(\\pi x)\\sin(\\pi y)\\right] = \\pi \\cos(\\pi x)\\sin(\\pi y)$,\n$u_y(x,y) = \\frac{\\partial}{\\partial y}\\left[\\sin(\\pi x)\\sin(\\pi y)\\right] = \\pi \\sin(\\pi x)\\cos(\\pi y)$.\n\nSecond, compute the second derivatives:\n$u_{xx}(x,y) = \\frac{\\partial}{\\partial x}\\left[\\pi \\cos(\\pi x)\\sin(\\pi y)\\right] = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$,\n$u_{yy}(x,y) = \\frac{\\partial}{\\partial y}\\left[\\pi \\sin(\\pi x)\\cos(\\pi y)\\right] = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$.\n\nTherefore, the Laplacian is\n$\\Delta u(x,y) = u_{xx}(x,y) + u_{yy}(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y) - \\pi^2 \\sin(\\pi x)\\sin(\\pi y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$.\n\nFrom the governing equation $-\\Delta u = f$, we obtain\n$f(x,y) = -\\Delta u(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$.\n\nDefine the physics residual $r_{\\mathrm{PDE}}(x,y) = -\\Delta u(x,y) - f(x,y)$. Substituting the expressions above,\n$r_{\\mathrm{PDE}}(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y) - 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y) = 0$,\nso the residual vanishes identically when the exact $u$ is used with the consistent $f$.\n\nThe boundary residual for homogeneous Dirichlet conditions is $r_{\\mathrm{BC}}(x,y) = u(x,y)$ on $\\partial\\Omega$. Because $\\sin(\\pi x) = 0$ when $x = 0$ or $x = 1$, and $\\sin(\\pi y) = 0$ when $y = 0$ or $y = 1$, we have $u = 0$ on $\\partial\\Omega$, so $r_{\\mathrm{BC}}(x,y) = 0$ on $\\partial\\Omega$.\n\nFinally, the gradient of the physics residual with respect to the inputs is\n$\\nabla r_{\\mathrm{PDE}}(x,y) = \\left(\\frac{\\partial r_{\\mathrm{PDE}}}{\\partial x}(x,y), \\frac{\\partial r_{\\mathrm{PDE}}}{\\partial y}(x,y)\\right)$.\nSince $r_{\\mathrm{PDE}}(x,y) \\equiv 0$, we have $\\frac{\\partial r_{\\mathrm{PDE}}}{\\partial x}(x,y) = 0$ and $\\frac{\\partial r_{\\mathrm{PDE}}}{\\partial y}(x,y) = 0$, hence $\\nabla r_{\\mathrm{PDE}}(x,y) = (0,0)$.\n\nWe now evaluate each option:\n\nOption A: It states $f(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $r_{\\mathrm{PDE}}(x,y) = 0$, $r_{\\mathrm{BC}}(x,y) = 0$ on $\\partial\\Omega$, and lists $u_x$, $u_y$, $u_{xx}$, $u_{yy}$ exactly as computed above. It also correctly identifies $\\nabla r_{\\mathrm{PDE}}(x,y) = (0,0)$, since the residual is identically zero. Verdict — Correct.\n\nOption B: It assigns $f(x,y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, which contradicts $f = -\\Delta u = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$. With this incorrect $f$, it then gives $r_{\\mathrm{PDE}}(x,y) = 4\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, which is the nonzero residual that would arise from the sign error. Additionally, it lists $u_x$, $u_y$, $u_{xx}$, $u_{yy}$ incorrectly: for example, $u_x$ is given as $\\pi \\sin(\\pi x)\\sin(\\pi y)$, but the correct $u_x$ is $\\pi \\cos(\\pi x)\\sin(\\pi y)$; similarly, $u_{xx}$ is not $\\pi^2 \\cos(\\pi x)\\sin(\\pi y)$ but $-\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$. The stated $\\nabla r_{\\mathrm{PDE}}$ is also inconsistent with the correct residual. Verdict — Incorrect.\n\nOption C: It sets $f(x,y) = \\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, missing a factor of $2$. Consequently, $r_{\\mathrm{PDE}}(x,y)$ is given as $\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, which is nonzero and contradicts the exact residual. While this option lists the correct $u_x$, $u_y$, $u_{xx}$, and $u_{yy}$, it incorrectly computes $\\nabla r_{\\mathrm{PDE}}$ from its erroneous residual. The correct gradient of the exact residual is the zero vector. Verdict — Incorrect.\n\nOption D: It gives the correct $f(x,y)$ and correctly sets $r_{\\mathrm{PDE}}(x,y) = 0$, but it defines the boundary residual as $r_{\\mathrm{BC}}(x,y) = \\partial_n u(x,y)$ on $\\partial\\Omega$, which is not the Dirichlet boundary residual; for homogeneous Dirichlet conditions, $r_{\\mathrm{BC}}(x,y)$ should be $u(x,y)$, not the normal derivative. Moreover, it lists $u_x$ and $u_y$ incorrectly as $\\pi \\cos(\\pi x)\\cos(\\pi y)$ for both, which does not equal the correct first derivatives, and it incorrectly signs $u_{xx}$ and $u_{yy}$ as positive $\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$ instead of negative. Although it gives $\\nabla r_{\\mathrm{PDE}}(x,y) = (0,0)$, the other inconsistencies render the option invalid. Verdict — Incorrect.\n\nTherefore, the correct option is A.", "answer": "$$\\boxed{A}$$", "id": "3410533"}, {"introduction": "Once a model's core components are verified, the next challenge is applying it to problems with realistic geometric complexity. Enforcing boundary conditions is a critical aspect of PINN design, and this exercise [@problem_id:3410603] delves into this practical issue for an irregular domain. You will implement and compare two powerful strategies—one using an analytically exact signed level set and another using an approximate discrete distance function—to gain hands-on experience with the trade-offs in accuracy and implementation effort for different boundary enforcement methods.", "problem": "Consider the Poisson problem with Dirichlet boundary conditions on a star-shaped, irregular domain. Let the computational domain be the set of points $(x,y)$ in $\\mathbb{R}^2$ described in polar coordinates $(r,\\theta)$ by the inequality\n$$ r < r_{\\varepsilon,k}(\\theta), \\quad r_{\\varepsilon,k}(\\theta) = 1 + \\varepsilon \\cos(k \\theta), $$\nwhere $\\varepsilon \\in (0,1)$ controls the amplitude of the boundary perturbation and $k \\in \\mathbb{N}$ controls its azimuthal frequency. Angles $\\theta$ are measured in radians. The boundary is the closed curve defined by $r = r_{\\varepsilon,k}(\\theta)$.\n\nThe Poisson problem is\n$$ -\\Delta u(x,y) = f(x,y) \\quad \\text{in the interior of the domain}, \\quad u(x,y) = 0 \\quad \\text{on the boundary}. $$\nWe are interested in boundary enforcement strategies used in Physics-Informed Neural Networks (PINNs), where one often constructs a boundary-satisfying trial field by embedding a factor that vanishes on the boundary. We compare two such strategies:\n\n1. Signed level set factor: Define the signed level set function\n$$ s_{\\varepsilon,k}(x,y) = \\rho(x,y) - r_{\\varepsilon,k}(\\theta(x,y)), $$\nwhere $\\rho(x,y) = \\sqrt{x^2 + y^2}$ and $\\theta(x,y) = \\operatorname{atan2}(y,x)$. This function satisfies $s_{\\varepsilon,k}(x,y) = 0$ on the boundary by construction. The signed level set trial field is\n$$ u_s(x,y) = s_{\\varepsilon,k}(x,y) \\, p(x,y), $$\nwhere $p(x,y)$ is a fixed, smooth function representing a generic unconstrained model component, taken here as the polynomial\n$$ p(x,y) = x^2 + y^2 + x y. $$\n\n2. Euclidean distance factor from a discrete boundary: Approximate the Euclidean distance to the boundary using a discrete set of $M$ boundary samples. Let $\\{ \\theta_j \\}_{j=0}^{M-1}$ be a set of $M$ uniformly spaced angles in $[0,2\\pi)$, and define boundary sample points\n$$ b_j = \\big( r_{\\varepsilon,k}(\\theta_j) \\cos \\theta_j,\\; r_{\\varepsilon,k}(\\theta_j) \\sin \\theta_j \\big). $$\nFor any point $(x,y)$, define the approximate Euclidean distance to the boundary as\n$$ d_M(x,y) = \\min_{0 \\le j \\le M-1} \\left\\| (x,y) - b_j \\right\\|_2, $$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm. The distance-based trial field is\n$$ u_d(x,y) = d_M(x,y) \\, p(x,y). $$\n\nWe assess boundary constraint violation by evaluating both trial fields at a finer set of $N$ boundary evaluation points. Let $\\{ \\varphi_i \\}_{i=0}^{N-1}$ be $N$ uniformly spaced angles in $[0,2\\pi)$, and define\n$$ \\tilde{b}_i = \\big( r_{\\varepsilon,k}(\\varphi_i) \\cos \\varphi_i,\\; r_{\\varepsilon,k}(\\varphi_i) \\sin \\varphi_i \\big). $$\nSince the boundary condition is $u=0$ on the boundary, a natural discrete Root Mean Square Error (RMSE) metric of boundary constraint violation is\n$$ \\mathrm{RMSE}_s = \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( u_s(\\tilde{b}_i) \\right)^2 }, \\quad \\mathrm{RMSE}_d = \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( u_d(\\tilde{b}_i) \\right)^2 }. $$\nNote that $u_s(\\tilde{b}_i) = s_{\\varepsilon,k}(\\tilde{b}_i) \\, p(\\tilde{b}_i) = 0 \\cdot p(\\tilde{b}_i) = 0$ by construction, whereas $u_d(\\tilde{b}_i)$ may be nonzero if $d_M$ is computed from a coarse discretization of the boundary.\n\nYour task is to implement a program that, for each test case $(\\varepsilon,k,M,N)$, computes the pair $(\\mathrm{RMSE}_d,\\mathrm{RMSE}_s)$ and outputs, in addition, a boolean indicating whether the distance-based strategy has a strictly lower boundary constraint violation than the signed level set strategy. The boolean is defined as\n$$ \\text{better} = (\\mathrm{RMSE}_d < \\mathrm{RMSE}_s). $$\n\nAngle values must be handled in radians. No physical units are involved. All computations must be performed using double-precision floating point arithmetic.\n\nTest Suite:\n- Case 1 (baseline irregularity, moderate coarse sampling): $(\\varepsilon,k,M,N) = (0.20, 3, 24, 360)$.\n- Case 2 (increased irregularity, finer coarse sampling): $(\\varepsilon,k,M,N) = (0.35, 4, 32, 360)$.\n- Case 3 (high irregularity, sparse coarse sampling): $(\\varepsilon,k,M,N) = (0.45, 5, 18, 360)$.\n- Case 4 (coarse sampling equal to evaluation sampling, boundary edge case): $(\\varepsilon,k,M,N) = (0.30, 3, 360, 360)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of the form `[RMSE_d, RMSE_s, better]`, where `RMSE_d` and `RMSE_s` are floating-point numbers and `better` is a boolean. The overall output must be a list of these lists, for example: `[ [RMSE_d_1, RMSE_s_1, better_1], [RMSE_d_2, RMSE_s_2, better_2], [RMSE_d_3, RMSE_s_3, better_3], [RMSE_d_4, RMSE_s_4, better_4] ]`.", "solution": "The user wants to evaluate and compare two different strategies for enforcing Dirichlet boundary conditions in the context of Physics-Informed Neural Networks (PINNs). This is done by calculating the boundary constraint violation for each strategy on a star-shaped domain.\n\n### Problem Validation\nThe problem statement is first validated against the specified criteria.\n\n**1. Extraction of Givens:**\n- **Domain boundary**: Defined in polar coordinates $(r, \\theta)$ by $r = r_{\\varepsilon,k}(\\theta) = 1 + \\varepsilon \\cos(k \\theta)$, where $\\varepsilon \\in (0,1)$ and $k \\in \\mathbb{N}$.\n- **PDE**: Poisson problem $-\\Delta u(x,y) = f(x,y)$ with boundary condition $u(x,y) = 0$.\n- **Model component**: A fixed polynomial $p(x,y) = x^2 + y^2 + xy$.\n- **Strategy 1 (Signed Level Set)**: Trial field $u_s(x,y) = s_{\\varepsilon,k}(x,y) \\, p(x,y)$, where $s_{\\varepsilon,k}(x,y) = \\sqrt{x^2+y^2} - r_{\\varepsilon,k}(\\operatorname{atan2}(y,x))$.\n- **Strategy 2 (Discrete Distance)**: Trial field $u_d(x,y) = d_M(x,y) \\, p(x,y)$, where $d_M(x,y) = \\min_{j} \\| (x,y) - b_j \\|_2$ over a set of $M$ discrete boundary points $\\{b_j\\}$.\n- **Error Metric**: Root Mean Square Error (RMSE) evaluated at $N$ boundary points $\\{\\tilde{b}_i\\}$.\n  - $\\mathrm{RMSE}_s = \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} ( u_s(\\tilde{b}_i) )^2 }$\n  - $\\mathrm{RMSE}_d = \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} ( u_d(\\tilde{b}_i) )^2 }$\n- **Comparison**: A boolean $\\text{better} = (\\mathrm{RMSE}_d < \\mathrm{RMSE}_s)$.\n- **Test Cases**:\n  - Case 1: $(\\varepsilon,k,M,N) = (0.20, 3, 24, 360)$\n  - Case 2: $(\\varepsilon,k,M,N) = (0.35, 4, 32, 360)$\n  - Case 3: $(\\varepsilon,k,M,N) = (0.45, 5, 18, 360)$\n  - Case 4: $(\\varepsilon,k,M,N) = (0.30, 3, 360, 360)$\n\n**2. Validation Verdict:**\nThe problem is **valid**. It is scientifically grounded in the field of numerical methods for PDEs, specifically concerning boundary condition enforcement in PINNs. All terms and procedures are mathematically well-defined, and the problem is self-contained and objective. The setup leads to a unique, computable solution for each test case. The fact that $\\mathrm{RMSE}_s$ is analytically zero is a key feature of the problem, designed to highlight the difference between an exact and an approximate enforcement method, rather than being a flaw.\n\n### Solution Derivation\n\nThe core of the problem is to implement the computation of the two error metrics, $\\mathrm{RMSE}_s$ and $\\mathrm{RMSE}_d$, for the given test cases.\n\n**1. Analysis of the Signed Level Set Strategy ($u_s$)**\nThe trial field for this strategy is $u_s(x,y) = s_{\\varepsilon,k}(x,y) \\, p(x,y)$. The signed level set function $s_{\\varepsilon,k}(x,y)$ is defined as the difference between the radial coordinate of a point $(x,y)$, $\\rho(x,y) = \\sqrt{x^2+y^2}$, and the boundary radius at that point's angle, $r_{\\varepsilon,k}(\\theta(x,y))$.\n\nBy construction, for any point $(x,y)$ on the boundary curve, its radial coordinate $\\rho(x,y)$ is equal to $r_{\\varepsilon,k}(\\theta(x,y))$. Therefore, the signed level set function $s_{\\varepsilon,k}(x,y)$ is identically zero for all points on the boundary.\n\nThe error metric $\\mathrm{RMSE}_s$ is evaluated at a set of $N$ points, $\\{\\tilde{b}_i\\}_{i=0}^{N-1}$, which are explicitly defined to be on the boundary. At any such point $\\tilde{b}_i$, we have $s_{\\varepsilon,k}(\\tilde{b}_i) = 0$. Consequently, the trial field value is $u_s(\\tilde{b}_i) = s_{\\varepsilon,k}(\\tilde{b}_i) \\cdot p(\\tilde{b}_i) = 0 \\cdot p(\\tilde{b}_i) = 0$.\n\nSince every term in the summation for $\\mathrm{RMSE}_s$ is zero, the result is analytically determined:\n$$ \\mathrm{RMSE}_s = \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} (0)^2 } = 0 $$\nThis holds for all test cases, regardless of the parameters $\\varepsilon$, $k$, $M$, or $N$. This result illustrates the primary advantage of using an exact, analytically defined function to enforce boundary conditions: it satisfies the condition perfectly, up to machine precision.\n\n**2. Algorithm for the Discrete Distance Strategy ($u_d$)**\nThe trial field for this strategy, $u_d(x,y) = d_M(x,y) \\, p(x,y)$, relies on an approximation of the true distance to the boundary. The function $d_M(x,y)$ is the minimum Euclidean distance from a point $(x,y)$ to a finite set of $M$ pre-sampled boundary points $\\{b_j\\}$. This is an approximation because the true distance would require finding the minimum over the entire continuous boundary curve. The calculation of $\\mathrm{RMSE}_d$ proceeds as follows for each test case $(\\varepsilon, k, M, N)$.\n\n- **Step 2.1: Generate Boundary Sample Points**\n  We generate $M$ discrete points $\\{b_j\\}_{j=0}^{M-1}$ that define the approximate boundary. This is done by creating $M$ uniformly spaced angles $\\theta_j$ in $[0, 2\\pi)$ and calculating the corresponding Cartesian coordinates:\n  $$ \\theta_j = \\frac{2\\pi j}{M} \\quad \\text{for } j = 0, 1, \\dots, M-1 $$\n  $$ r_j = r_{\\varepsilon,k}(\\theta_j) = 1 + \\varepsilon \\cos(k \\theta_j) $$\n  $$ b_j = (x_j, y_j) = (r_j \\cos \\theta_j, r_j \\sin \\theta_j) $$\n  These $M$ points are stored, for instance, in a NumPy array of shape $(M, 2)$.\n\n- **Step 2.2: Generate Boundary Evaluation Points**\n  Similarly, we generate the $N$ points $\\{\\tilde{b}_i\\}_{i=0}^{N-1}$ where the boundary error will be measured.\n  $$ \\varphi_i = \\frac{2\\pi i}{N} \\quad \\text{for } i = 0, 1, \\dots, N-1 $$\n  $$ \\tilde{r}_i = r_{\\varepsilon,k}(\\varphi_i) = 1 + \\varepsilon \\cos(k \\varphi_i) $$\n  $$ \\tilde{b}_i = (\\tilde{x}_i, \\tilde{y}_i) = (\\tilde{r}_i \\cos \\varphi_i, \\tilde{r}_i \\sin \\varphi_i) $$\n  These $N$ points are stored in a NumPy array of shape $(N, 2)$.\n\n- **Step 2.3: Compute Approximate Distances**\n  For each of the $N$ evaluation points $\\tilde{b}_i$, we calculate its approximate distance $d_M(\\tilde{b}_i)$ to the boundary. This involves finding the minimum distance from $\\tilde{b}_i$ to the set of $M$ sample points $\\{b_j\\}$.\n  $$ d_M(\\tilde{b}_i) = \\min_{0 \\le j \\le M-1} \\left\\| \\tilde{b}_i - b_j \\right\\|_2 $$\n  This operation can be vectorized efficiently. We compute an $N \\times M$ matrix of pairwise Euclidean distances between all evaluation points and all sample points. The `scipy.spatial.distance.cdist` function is ideal for this. Then, for each row (each evaluation point), we find the minimum value. The result is a vector of $N$ distances.\n\n- **Step 2.4: Evaluate the Trial Field and RMSE**\n  With the vector of distances $d_M(\\tilde{b}_i)$ computed, we evaluate the polynomial term $p(\\tilde{x}_i, \\tilde{y}_i) = \\tilde{x}_i^2 + \\tilde{y}_i^2 + \\tilde{x}_i \\tilde{y}_i$ for each of the $N$ evaluation points. The trial field values are then calculated element-wise:\n  $$ u_d(\\tilde{b}_i) = d_M(\\tilde{b}_i) \\cdot p(\\tilde{b}_i) $$\n  Finally, the Root Mean Square Error is computed:\n  $$ \\mathrm{RMSE}_d = \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( u_d(\\tilde{b}_i) \\right)^2 } $$\n  Unless an evaluation point $\\tilde{b}_i$ happens to be identical to one of the sample points $b_j$, the distance $d_M(\\tilde{b}_i)$ will be non-zero, leading to $\\mathrm{RMSE}_d > 0$. This non-zero error is a direct result of the geometric error introduced by discretizing the boundary.\n\n- **Step 2.5: Special Case Analysis (Case 4)**\n  In Case 4, we have $M=N=360$. The set of angles $\\{\\theta_j\\}$ used for sampling is identical to the set of angles $\\{\\varphi_i\\}$ used for evaluation. This means the set of sample points $\\{b_j\\}$ is identical to the set of evaluation points $\\{\\tilde{b}_i\\}$. For any evaluation point $\\tilde{b}_i$, its exact location exists within the set $\\{b_j\\}$. Therefore, the minimum distance $d_M(\\tilde{b}_i)$ will be exactly $0$. As a result, $u_d(\\tilde{b}_i) = 0$ for all $i$, and $\\mathrm{RMSE}_d = 0$.\n\n**3. Final Comparison**\nThe boolean $\\text{better}$ is defined as $\\mathrm{RMSE}_d < \\mathrm{RMSE}_s$. Since we established $\\mathrm{RMSE}_s = 0$, this becomes $\\mathrm{RMSE}_d < 0$. As $\\mathrm{RMSE}_d$ is a square root of a sum of squares, it is always non-negative.\n- For Cases 1, 2, and 3, $M < N$, so the sample points are sparser than the evaluation points. In general, $d_M > 0$, leading to $\\mathrm{RMSE}_d > 0$. The comparison $\\mathrm{RMSE}_d < 0$ is `False`.\n- For Case 4, we showed $\\mathrm{RMSE}_d = 0$. The comparison becomes $0 < 0$, which is also `False`.\nTherefore, the value of `better` is expected to be `False` for all test cases, highlighting that the approximate distance method never outperforms the analytically exact level-set method in terms of boundary satisfaction at the specified evaluation points.\n\nThe implementation will use `numpy` for efficient array operations and `scipy.spatial.distance.cdist` for the pairwise distance calculations, as outlined above. All calculations are performed using double-precision floating-point arithmetic as per NumPy's default.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial import distance\n\ndef solve():\n    \"\"\"\n    Computes and compares boundary constraint violation for two PINN-inspired\n    trial fields on a star-shaped domain for a series of test cases.\n    \"\"\"\n    test_cases = [\n        # (epsilon, k, M, N)\n        (0.20, 3, 24, 360),\n        (0.35, 4, 32, 360),\n        (0.45, 5, 18, 360),\n        (0.30, 3, 360, 360),\n    ]\n\n    results = []\n\n    for eps, k, M, N in test_cases:\n        # Define the boundary radius function r(theta)\n        def r_eps_k(theta):\n            return 1.0 + eps * np.cos(k * theta)\n\n        # Define the polynomial function p(x, y)\n        def p(x, y):\n            # x^2 + y^2 + xy is equivalent to r^2 + r^2 * cos(theta) * sin(theta)\n            # which is r^2 * (1 + 0.5 * sin(2*theta)).\n            # Using the Cartesian form is more direct.\n            return x**2 + y**2 + x * y\n\n        # Strategy 1: Signed Level Set\n        # The signed level set function is zero on the boundary by definition.\n        # Thus, u_s = 0 on the boundary, and RMSE_s is always 0.\n        rmse_s = 0.0\n\n        # Strategy 2: Euclidean Distance from a Discrete Boundary\n        \n        # 1. Generate M discrete boundary sample points (b_j)\n        theta_j = np.linspace(0, 2 * np.pi, M, endpoint=False) # M angles\n        r_j = r_eps_k(theta_j)\n        # Convert polar to Cartesian for the M sample points\n        b_j = np.c_[r_j * np.cos(theta_j), r_j * np.sin(theta_j)]\n\n        # 2. Generate N boundary evaluation points (b_tilde_i)\n        phi_i = np.linspace(0, 2 * np.pi, N, endpoint=False) # N angles\n        r_i_tilde = r_eps_k(phi_i)\n        # Convert polar to Cartesian for the N evaluation points\n        b_i_tilde = np.c_[r_i_tilde * np.cos(phi_i), r_i_tilde * np.sin(phi_i)]\n\n        # 3. Compute the approximate distance d_M for each evaluation point\n        if M > 0:\n            # Calculate the NxM matrix of pairwise distances\n            # dist_matrix[i, j] = ||b_i_tilde[i] - b_j[j]||_2\n            dist_matrix = distance.cdist(b_i_tilde, b_j, 'euclidean')\n            # For each evaluation point (row), find the minimum distance to any sample point\n            d_M_values = np.min(dist_matrix, axis=1) # Shape (N,)\n        else: # Handle edge case of M=0\n            d_M_values = np.full(N, np.inf)\n\n        # 4. Evaluate the polynomial p(x,y) at each evaluation point\n        p_values = p(b_i_tilde[:, 0], b_i_tilde[:, 1]) # Shape (N,)\n        \n        # 5. Compute the trial field u_d at each evaluation point\n        u_d_values = d_M_values * p_values\n\n        # 6. Compute the RMSE for the distance-based strategy\n        sum_sq_u_d = np.sum(u_d_values**2)\n        if N > 0:\n            rmse_d = np.sqrt(sum_sq_u_d / N)\n        else: # Handle edge case of N=0\n            rmse_d = 0.0\n\n        # Define the boolean 'better'\n        # Since rmse_d is a sqrt of sum of squares, it's non-negative.\n        # rmse_s is 0.0. The comparison rmse_d < rmse_s (i.e., rmse_d < 0.0)\n        # can only be False, unless there is some floating point noise\n        # creating a tiny negative number which is not possible here.\n        better = rmse_d < rmse_s\n\n        results.append([rmse_d, rmse_s, better])\n\n    # Format the final output string to match the required format `[[...],[...]]`\n    # The template `','.join(map(str, results))` produces this format without extra spaces.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3410603"}, {"introduction": "A predictive model is most useful when it can report its own confidence. This advanced practice moves beyond point predictions to the crucial domain of uncertainty quantification (UQ). In this exercise [@problem_id:3410639], you will implement a PINN for the heat equation and use Monte Carlo dropout, a scalable Bayesian approximation technique, to generate predictive uncertainty estimates. You will then learn how to assess the quality of these estimates by computing their empirical coverage, a fundamental skill for building trustworthy scientific machine learning models.", "problem": "Implement a complete program that constructs a Physics-Informed Neural Network (PINN) with Monte Carlo dropout for the one-dimensional heat equation and calibrates predictive uncertainty by empirical coverage on held-out data. The governing partial differential equation is the heat equation $u_t = \\alpha u_{xx}$ on the unit domain $(x,t) \\in [0,1] \\times [0,1]$ with Dirichlet boundary conditions $u(0,t) = 0$, $u(1,t) = 0$, and initial condition $u(x,0) = \\sin(\\pi x)$. Here $u(x,t)$ is the temperature field and $\\alpha > 0$ is the thermal diffusivity coefficient. You must implement the following components from first principles: a single-hidden-layer fully connected neural network $u_\\theta(x,t)$ with hyperbolic tangent activation; a physics-informed loss using the residual of the heat equation and the boundary and initial conditions; and Monte Carlo dropout at inference time to obtain predictive uncertainty. Use the uniformly valid analytical solution $u^\\star(x,t) = \\exp(-\\alpha \\pi^2 t)\\sin(\\pi x)$ solely to generate held-out test targets for coverage assessment. All quantities are dimensionless, so no physical unit conversions are required.\n\nBase your design on the following principles and definitions:\n- The heat equation $u_t = \\alpha u_{xx}$ encodes conservation of heat and Fourier’s law for diffusion; a physics-informed approach penalizes the mean squared residual of this law evaluated at collocation points.\n- A Physics-Informed Neural Network (PINN) is a neural network $u_\\theta(x,t)$ trained by minimizing a loss that aggregates data mismatch and physics residuals. For this problem, use the loss\n$$\n\\mathcal{L}(\\theta) = \\lambda_{\\mathrm{pde}} \\, \\mathbb{E}_{(x,t)\\in\\mathcal{C}} \\left[ \\left( \\partial_t u_\\theta(x,t) - \\alpha \\, \\partial_{xx} u_\\theta(x,t) \\right)^2 \\right] + \\lambda_{\\mathrm{ic}} \\, \\mathbb{E}_{x\\in\\mathcal{I}} \\left[ \\left( u_\\theta(x,0) - \\sin(\\pi x) \\right)^2 \\right] + \\lambda_{\\mathrm{bc}} \\, \\left( \\mathbb{E}_{t\\in\\mathcal{B}} \\left[ u_\\theta(0,t)^2 \\right] + \\mathbb{E}_{t\\in\\mathcal{B}} \\left[ u_\\theta(1,t)^2 \\right] \\right),\n$$\nwith nonnegative weights $\\lambda_{\\mathrm{pde}}, \\lambda_{\\mathrm{ic}}, \\lambda_{\\mathrm{bc}}$ and finite sets $\\mathcal{C}, \\mathcal{I}, \\mathcal{B}$ of collocation, initial, and boundary sample points, respectively.\n- Monte Carlo dropout uses random dropout masks at inference to induce a predictive distribution. For a network with a single hidden layer and element-wise dropout mask $m$ applied to hidden activations $a = \\tanh(z)$, with inverted scaling, one forward pass returns $u_\\theta(x,t)$; repeating this $M$ times yields samples used to compute predictive mean and standard deviation at each held-out point.\n\nNeural network model and derivatives to implement:\n- Let the network be $u_\\theta(x,t) = \\mathbf{w}_2^\\top \\left( m \\odot \\tanh(\\mathbf{W}_1 [x,t]^\\top + \\mathbf{b}_1) \\right) + b_2$, where $\\mathbf{W}_1 \\in \\mathbb{R}^{H \\times 2}$, $\\mathbf{b}_1 \\in \\mathbb{R}^H$, $\\mathbf{w}_2 \\in \\mathbb{R}^H$, $b_2 \\in \\mathbb{R}$ and $H$ is the hidden width, and $m \\in \\mathbb{R}^H$ is a random mask with entries $m_i \\in \\{0, 1/(1-p)\\}$ for dropout probability $p \\in [0,1)$. For each input sample, a fresh $m$ is drawn during Monte Carlo inference.\n- Derivatives needed for the physics residual must be computed analytically by the chain rule. Denote $s = \\mathbf{W}_1 [x,t]^\\top + \\mathbf{b}_1$, $a = \\tanh(s)$, $\\operatorname{sech}^2(s) = 1 - \\tanh^2(s)$, and denote $\\mathbf{W}_1$ columns by $\\mathbf{W}_{1x}$ and $\\mathbf{W}_{1t}$ for $x$ and $t$, respectively. Then for a given dropout mask $m$ and one hidden layer, implement\n$$\n\\partial_x u_\\theta = \\mathbf{w}_2^\\top \\left( m \\odot \\operatorname{sech}^2(s) \\odot \\mathbf{W}_{1x} \\right), \\quad\n\\partial_t u_\\theta = \\mathbf{w}_2^\\top \\left( m \\odot \\operatorname{sech}^2(s) \\odot \\mathbf{W}_{1t} \\right),\n$$\n$$\n\\partial_{xx} u_\\theta = \\mathbf{w}_2^\\top \\left( m \\odot \\left( -2 \\, \\operatorname{sech}^2(s) \\odot \\tanh(s) \\right) \\odot \\mathbf{W}_{1x} \\odot \\mathbf{W}_{1x} \\right).\n$$\n\nTraining and evaluation protocol:\n- Train the PINN by minimizing $\\mathcal{L}(\\theta)$ with dropout disabled during training to ensure a deterministic objective. Use small networks and modest sample sizes so that optimization converges within a reasonable time.\n- Evaluate empirical coverage of nominal Gaussian confidence intervals on a held-out grid of test points $(x,t)$ using the analytical $u^\\star(x,t)$ for targets. For nominal levels $q \\in \\{0.68, 0.95\\}$, use $z$-scores $z_{0.68} = 1.0$ and $z_{0.95} = 1.96$. For each test point, compute the Monte Carlo predictive mean $\\mu$ and standard deviation $\\sigma$, and check whether $|u^\\star - \\mu| \\le z \\sigma$. The empirical coverage is the fraction of held-out points satisfying this inequality. If $\\sigma = 0$, treat the interval as degenerate; use a small positive $\\varepsilon$ jitter in implementation to avoid numerical issues.\n\nTest suite and required outputs:\n- Use the following three test cases. In all cases, the spatial-temporal domain is $[0,1]\\times[0,1]$, the hidden width is $H = 10$, and the loss weights are $\\lambda_{\\mathrm{pde}} = 1$, $\\lambda_{\\mathrm{ic}} = 100$, $\\lambda_{\\mathrm{bc}} = 100$. The held-out test grid should be a uniform grid of $N_x = 20$ points in $x$ and $N_t = 20$ points in $t$ (total $400$ points). For Monte Carlo, use independent masks per sample per forward pass and inverted dropout scaling. The three cases are:\n    - Case A (happy path): $\\alpha = 0.1$, dropout probability $p = 0.1$, Monte Carlo samples $M = 100$. Train with $N_{\\mathcal{C}} = 64$ collocation points, $N_{\\mathcal{I}} = 32$ initial points, $N_{\\mathcal{B}} = 32$ boundary points, and optimize for $80$ iterations.\n    - Case B (dropout sensitivity): Reuse the trained weights from Case A without further optimization. Evaluate with dropout probability $p = 0.3$, Monte Carlo samples $M = 100$.\n    - Case C (changed diffusivity and limited data): $\\alpha = 0.2$, dropout probability $p = 0.2$, Monte Carlo samples $M = 100$. Train with $N_{\\mathcal{C}} = 32$ collocation points, $N_{\\mathcal{I}} = 16$ initial points, $N_{\\mathcal{B}} = 16$ boundary points, and optimize for $60$ iterations.\n\nYour program must:\n- Implement the PINN with analytic derivatives and physics-informed loss as described, trained deterministically.\n- For each test case, compute two floats: the empirical coverage for $q=0.68$ and for $q=0.95$, in that order.\n- Produce a single line of output containing the six coverage results as a comma-separated list enclosed in square brackets in the order [A-0.68, A-0.95, B-0.68, B-0.95, C-0.68, C-0.95].\n\nNo user input is allowed; all constants and random seeds should be fixed inside the program for reproducibility. Use only the Python standard library, NumPy, and SciPy as specified in the execution environment. Clearly document in code the handling of dropout, the computation of derivatives by the chain rule, and the coverage metrics. Every mathematical symbol, function, operator, and number appearing in this problem statement is written in LaTeX for clarity and precision.", "solution": "The user-provided problem is a well-defined and substantive task in the field of scientific machine learning. It requires the implementation of a Physics-Informed Neural Network (PINN) to solve the one-dimensional heat equation, including a mechanism for uncertainty quantification using Monte Carlo (MC) dropout. The problem statement is scientifically sound, internally consistent, and complete. All necessary equations, parameters, and evaluation protocols are provided, making the problem fully formalizable and computationally solvable.\n\nThe problem is valid based on a rigorous check against the specified criteria:\n- **Scientifically Grounded:** The foundation of the problem is the heat equation, a cornerstone of physics, and PINNs, a widely recognized methodology in computational science. The provided analytical solution and derivative formulas are correct.\n- **Well-Posed:** The problem provides a clear objective (minimizing a defined loss function) and a precise evaluation procedure (empirical coverage). All necessary data and constraints for the three test cases are specified.\n- **Objective:** The problem is described using precise mathematical notation and unambiguous instructions, free of any subjective elements.\n- **Completeness and Consistency:** While the choice of optimizer is not explicitly dictated, the use of `scipy.optimize` is allowed, and a standard choice like L-BFGS-B is appropriate for this type of problem. The number of iterations serves as a clear stopping criterion. The problem is self-contained and free of contradictions. The specification of \"iterations\" is interpreted as the `maxiter` option in the selected `scipy` optimizer.\n- **Feasibility:** The scale of the network ($H=10$), dataset sizes, and number of optimization iterations are deliberately chosen to be small, ensuring the computation can be completed within a reasonable time frame on standard hardware, as expected for a self-contained script.\n\nThe problem is therefore deemed **valid**, and a complete solution will be constructed.\n\n### Algorithmic Design and Principles\n\nThe solution is structured around a central Python class, `PINN`, which encapsulates the neural network model and its associated physics-informed logic.\n\n1.  **Neural Network Architecture:**\n    The core of the PINN is a single-hidden-layer, fully-connected neural network, $u_\\theta(x,t)$, with a hyperbolic tangent ($\\tanh$) activation function. The parameters $\\theta = \\{\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{w}_2, b_2\\}$ are initialized using a standard scheme (Glorot initialization) to promote stable training. For compatibility with `scipy.optimize.minimize`, which operates on a single vector of variables, helper methods are implemented to \"pack\" the network's weight matrices and bias vectors into a flat 1D NumPy array and \"unpack\" them back.\n\n2.  **Physics-Informed Loss Function:**\n    The training objective is to minimize the composite loss function $\\mathcal{L}(\\theta)$, which is the weighted sum of three mean-squared-error terms:\n    - **PDE Residual Loss ($\\mathcal{L}_{\\mathrm{pde}}$):** This term enforces the governing physics, $u_t - \\alpha u_{xx} = 0$. It is evaluated on a set of collocation points $\\mathcal{C}$ sampled from the interior of the spatio-temporal domain $[0,1] \\times [0,1]$.\n    - **Initial Condition Loss ($\\mathcal{L}_{\\mathrm{ic}}$):** This term penalizes deviations from the known initial state, $u(x,0) = \\sin(\\pi x)$, on a set of points $\\mathcal{I}$ at time $t=0$.\n    - **Boundary Condition Loss ($\\mathcal{L}_{\\mathrm{bc}}$):** This term enforces the Dirichlet boundary conditions, $u(0,t)=0$ and $u(1,t)=0$, on points sampled along the spatial boundaries $x=0$ and $x=1$.\n\n    The derivatives $\\partial_t u_\\theta$ and $\\partial_{xx} u_\\theta$ required for the PDE residual are computed analytically using the chain rule, as specified in the problem statement. These computations are implemented in a dedicated method, ensuring correctness and efficiency. Dropout is disabled during the loss calculation for training, as required, to provide a deterministic objective function for the optimizer.\n\n3.  **Optimization:**\n    The L-BFGS-B algorithm, a quasi-Newton method available in `scipy.optimize.minimize`, is used for optimization. It is well-suited for the small number of parameters ($4H+1 = 41$) in this problem and often exhibits better convergence properties than first-order methods for PINNs. The optimizer iteratively adjusts the flattened parameter vector to minimize the loss function $\\mathcal{L}(\\theta)$. The number of iterations is controlled by the `maxiter` option, as specified for each test case.\n\n4.  **Uncertainty Quantification via Monte Carlo Dropout:**\n    After training, the model's predictive uncertainty is assessed using MC dropout. At inference time, dropout is enabled. For each point on a held-out test grid, $M$ stochastic forward passes are performed. In each pass, a different random dropout mask $m$ is applied to the hidden layer's activations. This process generates an ensemble of $M$ predictions for each test point.\n    - The **predictive mean** ($\\mu$) is the average of the $M$ predictions.\n    - The **predictive standard deviation** ($\\sigma$) quantifies the model's uncertainty, represented by the variability in the ensemble.\n\n5.  **Empirical Coverage Evaluation:**\n    The quality of the uncertainty estimates is calibrated by computing the empirical coverage. For each point on the test grid, we construct a nominal confidence interval $[\\mu - z\\sigma, \\mu + z\\sigma]$, where $z$ is the critical value from a standard normal distribution corresponding to a desired confidence level $q$ (e.g., $z=1.0$ for $q=0.68$). We then check if the true analytical solution, $u^\\star(x,t)$, falls within this interval. The empirical coverage is the fraction of test points for which this condition holds. This metric assesses whether the model's reported uncertainty is well-calibrated; for example, a $95\\%$ nominal interval should ideally contain the true value $95\\%$ of the time. A small jitter $\\varepsilon$ is added to $\\sigma$ to prevent division by zero in cases of zero predictive variance.\n\nThe implementation follows the three specified test cases, training and/or evaluating the model under different conditions for thermal diffusivity ($\\alpha$), dropout probability ($p$), and training data size, and reporting the resulting coverage metrics in the precise format required. A fixed random seed ensures the reproducibility of all stochastic operations, including weight initialization, data sampling, and dropout masks.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Implements a PINN for the 1D heat equation, trains it, and evaluates\n    predictive uncertainty calibration using Monte Carlo dropout.\n    \"\"\"\n    # Fix random seed for reproducibility of weight initialization, data sampling, and dropout masks.\n    np.random.seed(42)\n\n    class PINN:\n        \"\"\"\n        Physics-Informed Neural Network for the 1D Heat Equation.\n        Implements the network architecture, analytical derivatives, loss function,\n        and prediction with Monte Carlo dropout.\n        \"\"\"\n        def __init__(self, H=10):\n            self.H = H\n            # Glorot initialization for weights\n            self.W1 = np.random.randn(self.H, 2) * np.sqrt(2 / (2 + self.H))\n            self.b1 = np.zeros((1, self.H))\n            self.w2 = np.random.randn(self.H, 1) * np.sqrt(2 / (self.H + 1))\n            self.b2 = np.zeros((1, 1))\n\n        def pack_params(self):\n            \"\"\"Flattens all model parameters into a single 1D array for the optimizer.\"\"\"\n            return np.concatenate([self.W1.flatten(), self.b1.flatten(), self.w2.flatten(), self.b2.flatten()])\n\n        def unpack_params(self, params_vec):\n            \"\"\"Unpacks a 1D array of parameters back into the model's weight/bias attributes.\"\"\"\n            ptr = 0\n            self.W1 = params_vec[ptr:ptr + self.H * 2].reshape(self.H, 2)\n            ptr += self.H * 2\n            self.b1 = params_vec[ptr:ptr + self.H].reshape(1, self.H)\n            ptr += self.H\n            self.w2 = params_vec[ptr:ptr + self.H].reshape(self.H, 1)\n            ptr += self.H\n            self.b2 = params_vec[ptr:ptr + 1].reshape(1, 1)\n\n        def forward(self, x, t, p_drop=0.0):\n            \"\"\"\n            Performs a forward pass through the network.\n            Handles training (p_drop=0) and inference with MC dropout (p_drop>0).\n            \"\"\"\n            X = np.hstack((x, t))\n            N = X.shape[0]\n            \n            s = X @ self.W1.T + self.b1\n            a = np.tanh(s)\n            \n            # Apply dropout if in inference mode (p_drop > 0)\n            if p_drop > 0:\n                # Inverted dropout: scale by 1/(1-p) during training is standard,\n                # but here we apply it at inference time as per the problem.\n                # A fresh mask is drawn for each sample in the batch.\n                scale = 1.0 / (1.0 - p_drop)\n                mask = np.random.binomial(1, 1.0 - p_drop, size=(N, self.H)) * scale\n                a = a * mask\n            \n            # If p_drop is 0 (training), the mask is effectively all ones.\n            u = a @ self.w2 + self.b2\n            return u.flatten()\n\n        def get_derivatives(self, x, t):\n            \"\"\"\n            Computes the network output and its analytical derivatives w.r.t. x and t.\n            Dropout is disabled (mask 'm' is effectively all ones).\n            \"\"\"\n            X = np.hstack((x, t))\n            s = X @ self.W1.T + self.b1\n            a = np.tanh(s)\n            sech2_s = 1.0 - a**2\n\n            # Extract columns of W1 corresponding to x and t\n            W1x = self.W1[:, 0:1].T  # Shape (1, H)\n            W1t = self.W1[:, 1:2].T  # Shape (1, H)\n\n            # First derivatives (chain rule)\n            u_t = (sech2_s * W1t) @ self.w2\n            u_x = (sech2_s * W1x) @ self.w2\n\n            # Second derivative (chain rule)\n            term_xx = -2.0 * sech2_s * a\n            u_xx = (term_xx * W1x * W1x) @ self.w2\n\n            return u_t.flatten(), u_x.flatten(), u_xx.flatten()\n\n        def loss_function(self, params_vec, alpha, points, lambdas):\n            \"\"\"\n            The objective function for the optimizer.\n            Calculates the weighted sum of PDE, initial, and boundary condition losses.\n            \"\"\"\n            self.unpack_params(params_vec)\n            \n            # Unpack points\n            x_c, t_c = points['c']\n            x_i, t_i = points['i']\n            x_b, t_b = points['b']\n\n            # Unpack loss weights\n            lambda_pde, lambda_ic, lambda_bc = lambdas\n\n            # 1. PDE Residual Loss\n            u_t_c, _, u_xx_c = self.get_derivatives(x_c, t_c)\n            pde_residual = u_t_c - alpha * u_xx_c\n            loss_pde = lambda_pde * np.mean(pde_residual**2)\n\n            # 2. Initial Condition Loss\n            u_i = self.forward(x_i, t_i, p_drop=0.0)\n            ic_target = np.sin(np.pi * x_i.flatten())\n            loss_ic = lambda_ic * np.mean((u_i - ic_target)**2)\n            \n            # 3. Boundary Condition Loss\n            u_b = self.forward(x_b, t_b, p_drop=0.0)\n            loss_bc = lambda_bc * np.mean(u_b**2)\n            \n            return loss_pde + loss_ic + loss_bc\n\n    def train_pinn(H, alpha, N_c, N_i, N_b, iterations, lambdas):\n        \"\"\"\n        Initializes and trains a PINN model.\n        \n        Returns:\n            The trained model parameters as a 1D vector.\n        \"\"\"\n        # Initialize the model\n        model = PINN(H=H)\n        \n        # Generate training points\n        # Collocation points (domain interior)\n        x_c = np.random.rand(N_c, 1)\n        t_c = np.random.rand(N_c, 1)\n        \n        # Initial condition points (t=0)\n        x_i = np.random.rand(N_i, 1)\n        t_i = np.zeros_like(x_i)\n        \n        # Boundary condition points (x=0 and x=1)\n        t_b0 = np.random.rand(N_b // 2, 1)\n        t_b1 = np.random.rand(N_b // 2, 1)\n        x_b = np.vstack([np.zeros_like(t_b0), np.ones_like(t_b1)])\n        t_b = np.vstack([t_b0, t_b1])\n\n        points = {'c': (x_c, t_c), 'i': (x_i, t_i), 'b': (x_b, t_b)}\n        \n        # Initial parameter vector\n        p0 = model.pack_params()\n        \n        # Optimizer call\n        res = minimize(\n            fun=model.loss_function,\n            x0=p0,\n            args=(alpha, points, lambdas),\n            method='L-BFGS-B',\n            options={'maxiter': iterations}\n        )\n        return res.x\n        \n    def evaluate_coverage(params_vec, H, alpha, p_drop, M):\n        \"\"\"\n        Evaluates the empirical coverage of the model's uncertainty estimates.\n        \"\"\"\n        # Create a model with the trained parameters\n        model = PINN(H=H)\n        model.unpack_params(params_vec)\n        \n        # Create the held-out test grid\n        N_x, N_t = 20, 20\n        x_space = np.linspace(0, 1, N_x)\n        t_space = np.linspace(0, 1, N_t)\n        x_grid, t_grid = np.meshgrid(x_space, t_space)\n        x_flat, t_flat = x_grid.flatten()[:, None], t_grid.flatten()[:, None]\n        \n        # Perform Monte Carlo predictions\n        predictions = np.zeros((M, x_flat.shape[0]))\n        for i in range(M):\n            predictions[i, :] = model.forward(x_flat, t_flat, p_drop=p_drop)\n            \n        # Compute predictive mean and standard deviation\n        mu = np.mean(predictions, axis=0)\n        # Add a small epsilon for numerical stability if sigma is zero\n        sigma = np.std(predictions, axis=0) + 1e-8 \n        \n        # Get analytical solution for comparison\n        u_star = np.exp(-alpha * np.pi**2 * t_flat.flatten()) * np.sin(np.pi * x_flat.flatten())\n        \n        # Calculate empirical coverage for two confidence levels\n        z_scores = {'0.68': 1.0, '0.95': 1.96}\n        abs_error = np.abs(u_star - mu)\n        \n        coverage_68 = np.mean(abs_error <= z_scores['0.68'] * sigma)\n        coverage_95 = np.mean(abs_error <= z_scores['0.95'] * sigma)\n        \n        return coverage_68, coverage_95\n\n    # --- Test Case Execution ---\n    \n    H = 10\n    lambdas = (1.0, 100.0, 100.0) # (lambda_pde, lambda_ic, lambda_bc)\n    results = []\n\n    # Case A: Happy path\n    alpha_A = 0.1\n    p_A = 0.1\n    M_A = 100\n    trained_params_A = train_pinn(H, alpha_A, N_c=64, N_i=32, N_b=32, iterations=80, lambdas=lambdas)\n    cov_68_A, cov_95_A = evaluate_coverage(trained_params_A, H, alpha_A, p_A, M_A)\n    results.extend([cov_68_A, cov_95_A])\n    \n    # Case B: Dropout sensitivity (uses weights from A)\n    alpha_B = 0.1 # alpha is the same for the physics\n    p_B = 0.3\n    M_B = 100\n    cov_68_B, cov_95_B = evaluate_coverage(trained_params_A, H, alpha_B, p_B, M_B)\n    results.extend([cov_68_B, cov_95_B])\n\n    # Case C: Changed diffusivity and limited data\n    alpha_C = 0.2\n    p_C = 0.2\n    M_C = 100\n    trained_params_C = train_pinn(H, alpha_C, N_c=32, N_i=16, N_b=16, iterations=60, lambdas=lambdas)\n    cov_68_C, cov_95_C = evaluate_coverage(trained_params_C, H, alpha_C, p_C, M_C)\n    results.extend([cov_68_C, cov_95_C])\n    \n    # Print the final results in the required format\n    print(f\"[{','.join(f'{r:.4f}' for r in results)}]\")\n\nsolve()\n```", "id": "3410639"}]}