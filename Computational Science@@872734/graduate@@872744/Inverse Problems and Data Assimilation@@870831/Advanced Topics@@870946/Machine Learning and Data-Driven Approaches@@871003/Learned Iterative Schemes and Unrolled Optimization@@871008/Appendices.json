{"hands_on_practices": [{"introduction": "For any iterative algorithm, whether classical or learned, convergence to a stable solution is not guaranteed. This exercise provides a concrete method for analyzing the linear stability of an unrolled scheme by examining the spectral radius of its iteration operator. By comparing a classical Proximal Gradient Descent (PGD) method with a learned variant, you will gain hands-on experience with the fundamental theory governing the dynamics of these models. [@problem_id:3396250]", "problem": "Consider the linear inverse problem with additive noise, where the observed data is modeled as $y = A x + \\epsilon$ with a known system matrix $A \\in \\mathbb{R}^{n \\times n}$, an unknown state $x \\in \\mathbb{R}^{n}$, and additive noise $\\epsilon \\in \\mathbb{R}^{n}$. To recover $x$, one commonly minimizes the sum of a data misfit and a regularization term, defining an objective of the form\n$$\nF(x) = \\frac{1}{2} \\| A x - y \\|_2^2 + \\lambda R(x),\n$$\nwhere $\\lambda > 0$ is a regularization parameter and $R(x)$ is a penalty. In this problem, the penalty is quadratic and given by\n$$\nR(x) = \\frac{1}{2} \\| L x \\|_2^2,\n$$\nwith a known linear operator $L$.\n\nThe Proximal Gradient Descent (PGD) method forms iterates according to\n$$\nx^{k+1} = \\operatorname{prox}_{\\lambda R}\\!\\left( x^k - \\tau A^\\top (A x^k - y) \\right),\n$$\nwhere $\\tau > 0$ is a step size and $\\operatorname{prox}_{\\lambda R}$ denotes the proximal operator associated with $\\lambda R$. A learned unrolled scheme replaces the proximal operator with a learned operator that has been trained, for example, to approximate the action of the proximal map. In this problem, the learned proximal operator is assumed to be linear and denoted by a matrix $M \\in \\mathbb{R}^{n \\times n}$, so that the learned update is\n$$\nx^{k+1} = M \\left( x^k - \\tau A^\\top (A x^k - y) \\right).\n$$\n\nYour tasks are:\n- Starting from first principles and the core definitions above, derive the linear update operators for the classical PGD method with the quadratic penalty and for the learned unrolled scheme with a linear trained proximal operator. Express each update as an affine map $x^{k+1} = G x^k + c$ and provide the condition for the existence and stability of a fixed point in terms of the spectrum of $G$.\n- For each test case provided below, compute the eigenvalues of each update operator, their magnitudes, the spectral radius, and decide whether the fixed point is linearly stable. A fixed point is deemed linearly stable if and only if the spectral radius is strictly less than $1$.\n\nUse the following test suite of parameter values. In all cases, $n = 3$, and the norm $\\| A \\|_2$ denotes the spectral norm of $A$.\n\nCase $\\mathbf{1}$ (well-conditioned, nonexpansive learned proximal):\n- $$A_1 = \\begin{bmatrix} 1  0.2  0 \\\\ 0.2  0.8  0.1 \\\\ 0  0.1  0.5 \\end{bmatrix}, \\quad L_1 = I_3, \\quad \\lambda_1 = 0.5, \\quad M_1 = 0.9 I_3,$$\n- $$\\tau_1 = 0.9 \\cdot \\frac{2}{\\|A_1\\|_2^2}.$$\n\nCase $\\mathbf{2}$ (near-threshold step size, expansive learned proximal):\n- $$A_2 = \\begin{bmatrix} 1  0.3  0 \\\\ 0.3  0.7  0.2 \\\\ 0  0.2  0.4 \\end{bmatrix}, \\quad L_2 = I_3, \\quad \\lambda_2 = 0.4, \\quad M_2 = 1.1 I_3,$$\n- $$\\tau_2 = 0.99 \\cdot \\frac{2}{\\|A_2\\|_2^2}.$$\n\nCase $\\mathbf{3}$ (ill-conditioned system, exact classical proximal):\n- $$A_3 = \\begin{bmatrix} 1.5  -0.9  0 \\\\ -0.9  0.6  0.3 \\\\ 0  0.3  0.2 \\end{bmatrix}, \\quad L_3 = \\begin{bmatrix} -1  1  0 \\\\ 0  -1  1 \\end{bmatrix}, \\quad \\lambda_3 = 0.8,$$\n- For the learned proximal, set $$M_3 = \\left(I_3 + \\lambda_3 L_3^\\top L_3 \\right)^{-1},$$\n- $$\\tau_3 = 0.7 \\cdot \\frac{2}{\\|A_3\\|_2^2}.$$\n\nCase $\\mathbf{4}$ (no regularization, exact threshold, borderline classical stability):\n- $$A_4 = \\begin{bmatrix} 0.9  0.2  0.1 \\\\ 0.2  0.5  0.05 \\\\ 0.1  0.05  0.3 \\end{bmatrix}, \\quad L_4 = 0_{3 \\times 3}, \\quad \\lambda_4 = 0.5, \\quad M_4 = 0.95 I_3,$$\n- $$\\tau_4 = 1.0 \\cdot \\frac{2}{\\|A_4\\|_2^2}.$$\n\nFor each case, construct the classical PGD update matrix\n$$\nG_{\\text{classical}} = S \\left( I_3 - \\tau A^\\top A \\right), \\quad \\text{with} \\quad S = \\left( I_3 + \\lambda L^\\top L \\right)^{-1},\n$$\nand the learned update matrix\n$$\nG_{\\text{learned}} = M \\left( I_3 - \\tau A^\\top A \\right).\n$$\n\nFor each $G_{\\text{classical}}$ and $G_{\\text{learned}}$, compute:\n- The list of eigenvalue magnitudes, sorted in descending order.\n- The spectral radius, defined as the maximum magnitude among the eigenvalues.\n- A boolean indicating linear stability of the fixed point as per the spectral radius condition.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets. Each case’s result must be a list of the form\n$$\n[\\text{eig\\_mags\\_classical}, \\text{eig\\_mags\\_learned}, \\rho_{\\text{classical}}, \\rho_{\\text{learned}}, \\text{stable}_{\\text{classical}}, \\text{stable}_{\\text{learned}}],\n$$\nwhere $\\text{eig\\_mags\\_classical}$ and $\\text{eig\\_mags\\_learned}$ are lists of floats. For example, the entire output should look like\n$$\n[[\\ldots],[\\ldots],\\ldots].\n$$\nNo additional text should be printed. Angles are not involved, and no physical units are required. All numerical values are to be provided as floating-point numbers or booleans as appropriate.", "solution": "The problem requires the derivation and stability analysis of two iterative schemes for solving linear inverse problems: the classical Proximal Gradient Descent (PGD) and a learned unrolled variant.\n\n### Derivation of Update Operators\n\nAn iterative scheme of the form $x^{k+1} = f(x^k)$ is an affine map if it can be expressed as $x^{k+1} = G x^k + c$ for some constant matrix $G$ and vector $c$. We will derive this form for both the classical and learned methods.\n\n#### 1. Classical Proximal Gradient Descent (PGD)\n\nThe PGD update rule is given by:\n$$\nx^{k+1} = \\operatorname{prox}_{\\lambda R}\\!\\left( x^k - \\tau A^\\top (A x^k - y) \\right)\n$$\nLet $z^k = x^k - \\tau A^\\top (A x^k - y)$. The update is $x^{k+1} = \\operatorname{prox}_{\\lambda R}(z^k)$.\n\nThe proximal operator $\\operatorname{prox}_{\\gamma g}(v)$ is defined as the unique minimizer of the function $u \\mapsto g(u) + \\frac{1}{2\\gamma} \\|u - v\\|_2^2$. In our case, the function to minimize is $J(u)$ with $\\gamma=1$ and $g(u) = \\lambda R(u) = \\frac{\\lambda}{2} \\|Lu\\|_2^2$:\n$$\n\\operatorname{prox}_{\\lambda R}(z^k) = \\arg\\min_{u \\in \\mathbb{R}^n} \\left( \\frac{\\lambda}{2} \\|Lu\\|_2^2 + \\frac{1}{2} \\|u - z^k\\|_2^2 \\right)\n$$\nThis is a quadratic, strictly convex optimization problem, so a unique minimizer exists. We find it by setting the gradient with respect to $u$ to zero:\n$$\n\\nabla_u J(u) = \\nabla_u \\left( \\frac{\\lambda}{2} u^\\top L^\\top L u + \\frac{1}{2} (u - z^k)^\\top (u - z^k) \\right) = 0\n$$\n$$\n\\lambda L^\\top L u + (u - z^k) = 0\n$$\nRearranging for $u$:\n$$\n(\\lambda L^\\top L + I) u = z^k\n$$\nwhere $I$ is the identity matrix. The matrix $(I + \\lambda L^\\top L)$ is positive definite for $\\lambda > 0$ since $L^\\top L$ is positive semi-definite, and thus it is invertible. The solution is:\n$$\nu = (I + \\lambda L^\\top L)^{-1} z^k\n$$\nLet's define the matrix $S = (I + \\lambda L^\\top L)^{-1}$. The proximal operator is thus a linear transformation: $\\operatorname{prox}_{\\lambda R}(z^k) = S z^k$.\n\nNow, we substitute this back into the PGD update equation:\n$$\nx^{k+1} = S z^k = S \\left( x^k - \\tau A^\\top (A x^k - y) \\right)\n$$\nDistributing the terms to separate $x^k$ and $y$:\n$$\nx^{k+1} = S \\left( x^k - \\tau A^\\top A x^k + \\tau A^\\top y \\right)\n$$\n$$\nx^{k+1} = S (I - \\tau A^\\top A) x^k + S \\tau A^\\top y\n$$\nThis is an affine map $x^{k+1} = G_{\\text{classical}} x^k + c_{\\text{classical}}$, with the update matrix and constant vector given by:\n$$\nG_{\\text{classical}} = S (I - \\tau A^\\top A) = (I + \\lambda L^\\top L)^{-1} (I - \\tau A^\\top A)\n$$\n$$\nc_{\\text{classical}} = S \\tau A^\\top y = (I + \\lambda L^\\top L)^{-1} \\tau A^\\top y\n$$\nThe expression for $G_{\\text{classical}}$ matches the one provided in the problem statement.\n\n#### 2. Learned Unrolled Scheme\n\nThe update rule for the learned scheme is given directly:\n$$\nx^{k+1} = M \\left( x^k - \\tau A^\\top (A x^k - y) \\right)\n$$\nwhere $M$ is a learned matrix. By distributing $M$, we can immediately identify the affine form:\n$$\nx^{k+1} = M \\left( (I - \\tau A^\\top A) x^k + \\tau A^\\top y \\right)\n$$\n$$\nx^{k+1} = M (I - \\tau A^\\top A) x^k + M \\tau A^\\top y\n$$\nThis is an affine map $x^{k+1} = G_{\\text{learned}} x^k + c_{\\text{learned}}$, with:\n$$\nG_{\\text{learned}} = M (I - \\tau A^\\top A)\n$$\n$$\nc_{\\text{learned}} = M \\tau A^\\top y\n$$\nThe expression for $G_{\\text{learned}}$ also matches the one provided in the problem statement.\n\n### Fixed Point Stability Analysis\n\nAn iterative method $x^{k+1} = G x^k + c$ converges to a unique fixed point $x^*$ for any initial guess $x^0$ if and only if the iteration operator $G$ is a contraction mapping. A fixed point $x^*$ satisfies the equation $x^* = G x^* + c$.\n\nTo analyze linear stability, we consider the evolution of the error, $e^k = x^k - x^*$. Subtracting the fixed-point equation from the update equation yields:\n$$\nx^{k+1} - x^* = (G x^k + c) - (G x^* + c) = G (x^k - x^*)\n$$\n$$\ne^{k+1} = G e^k\n$$\nThis implies that $e^k = G^k e^0$. The error $e^k$ converges to zero for any initial error $e^0$ if and only if the matrix powers $G^k$ converge to the zero matrix as $k \\to \\infty$. This condition is met if and only if the spectral radius of $G$, denoted $\\rho(G)$, is strictly less than $1$. The spectral radius is defined as the maximum of the magnitudes of the eigenvalues of $G$:\n$$\n\\rho(G) = \\max_{i} |\\mu_i|\n$$\nwhere $\\mu_i$ are the eigenvalues of $G$.\n\nTherefore, a fixed point of the iterative scheme is linearly stable if and only if $\\rho(G)  1$. The numerical computations for each case will be based on constructing the respective $G$ matrices and evaluating their spectral radii.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing the stability of classical and learned proximal\n    gradient descent for four test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"A\": np.array([[1, 0.2, 0], [0.2, 0.8, 0.1], [0, 0.1, 0.5]]),\n            \"L\": np.eye(3),\n            \"lambda_val\": 0.5,\n            \"M\": 0.9 * np.eye(3),\n            \"tau_prefactor\": 0.9,\n        },\n        {\n            \"A\": np.array([[1, 0.3, 0], [0.3, 0.7, 0.2], [0, 0.2, 0.4]]),\n            \"L\": np.eye(3),\n            \"lambda_val\": 0.4,\n            \"M\": 1.1 * np.eye(3),\n            \"tau_prefactor\": 0.99,\n        },\n        {\n            \"A\": np.array([[1.5, -0.9, 0], [-0.9, 0.6, 0.3], [0, 0.3, 0.2]]),\n            \"L\": np.array([[-1, 1, 0], [0, -1, 1]]),\n            \"lambda_val\": 0.8,\n            \"M\": None,  # M is derived from L and lambda\n            \"tau_prefactor\": 0.7,\n        },\n        {\n            \"A\": np.array([[0.9, 0.2, 0.1], [0.2, 0.5, 0.05], [0.1, 0.05, 0.3]]),\n            \"L\": np.zeros((3, 3)),\n            \"lambda_val\": 0.5,\n            \"M\": 0.95 * np.eye(3),\n            \"tau_prefactor\": 1.0,\n        },\n    ]\n\n    results_for_all_cases = []\n\n    for i, case in enumerate(test_cases):\n        A = case[\"A\"]\n        L = case[\"L\"]\n        lambda_val = case[\"lambda_val\"]\n        tau_prefactor = case[\"tau_prefactor\"]\n        n = A.shape[0]\n        I_n = np.eye(n)\n\n        # Calculate step size tau\n        norm_A_sq = np.linalg.norm(A, 2)**2\n        tau = tau_prefactor * (2 / norm_A_sq)\n\n        # Common term in both G matrices\n        common_term = I_n - tau * A.T @ A\n        \n        # --- Classical PGD ---\n        # Construct S = (I + lambda * L^T * L)^-1\n        L_T_L = L.T @ L\n        S = np.linalg.inv(I_n + lambda_val * L_T_L)\n        \n        # Construct G_classical\n        G_classical = S @ common_term\n        \n        # Analyze G_classical\n        eigvals_c = np.linalg.eigvals(G_classical)\n        mags_c = np.abs(eigvals_c)\n        mags_c_sorted = -np.sort(-mags_c) # Sort descending\n        rho_c = mags_c_sorted[0]\n        stable_c = rho_c  1.0\n\n        # --- Learned Scheme ---\n        # Construct M\n        if i == 2:  # Case 3\n            M = S\n        else:\n            M = case[\"M\"]\n        \n        # Construct G_learned\n        G_learned = M @ common_term\n        \n        # Analyze G_learned\n        eigvals_l = np.linalg.eigvals(G_learned)\n        mags_l = np.abs(eigvals_l)\n        mags_l_sorted = -np.sort(-mags_l) # Sort descending\n        rho_l = mags_l_sorted[0]\n        stable_l = rho_l  1.0\n        \n        # Format the output for the current case\n        mags_c_str = f\"[{','.join([f'{m:.8f}' for m in mags_c_sorted])}]\"\n        mags_l_str = f\"[{','.join([f'{m:.8f}' for m in mags_l_sorted])}]\"\n        \n        current_case_str = (\n            f\"[{mags_c_str},\"\n            f\"{mags_l_str},\"\n            f\"{rho_c:.8f},\"\n            f\"{rho_l:.8f},\"\n            f\"{str(stable_c).lower()},\"\n            f\"{str(stable_l).lower()}]\"\n        )\n        results_for_all_cases.append(current_case_str)\n        \n    print(f\"[{','.join(results_for_all_cases)}]\")\n\nsolve()\n```", "id": "3396250"}, {"introduction": "Real-world inverse problems often involve physical or statistical constraints, such as non-negativity or bounds on solutions. This practice explores a powerful and modern technique for incorporating such inequality constraints into an unrolled optimization scheme using a learned proximal operator based on a differentiable barrier function. By implementing this method, you will learn how to design networks that can find solutions that are not only accurate but also physically feasible. [@problem_id:3396268]", "problem": "Consider the equality-constrained quadratic inverse problem with linear inequality constraints in the setting of unrolled optimization. The task is to design and analyze a learned proximal mapping that enforces the inequality constraints using a differentiable barrier within a fixed-depth unrolled scheme, and to evaluate how well feasibility is maintained across iterations.\n\nStart from the following foundational base:\n- The objective of linear least squares data fitting is represented by the function $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x} - \\mathbf{y}\\|_2^2$, where $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is a measurement operator and $\\mathbf{y} \\in \\mathbb{R}^m$ is a given data vector.\n- The constraint set is defined by $\\mathbf{C}\\mathbf{x} \\le \\mathbf{d}$, where $\\mathbf{C} \\in \\mathbb{R}^{p \\times n}$ and $\\mathbf{d} \\in \\mathbb{R}^p$.\n- A differentiable barrier is used to softly enforce inequality constraints. A smooth barrier for a scalar argument $u$ is given by $b_\\tau(u) = \\tau \\log\\big(1 + \\exp(u/\\tau)\\big)$ with temperature parameter $\\tau  0$, which is differentiable everywhere and approximates the positive part function as $\\tau \\to 0$.\n\nYou must design an unrolled iterative scheme of fixed depth $K$ that proceeds by first taking a gradient descent step on the data fidelity term and then applying a learned proximal mapping that incorporates the barrier to discourage violations of $\\mathbf{C}\\mathbf{x} \\le \\mathbf{d}$. Specifically, in each iteration $k$:\n- Compute the gradient descent update on $f(\\mathbf{x})$ with a step size $\\alpha  0$ to obtain an intermediate point $\\mathbf{z}^{(k)}$.\n- Apply a learned proximal mapping $P_{\\boldsymbol{\\theta}}$ parameterized by $\\boldsymbol{\\theta}$ to $\\mathbf{z}^{(k)}$ to obtain $\\mathbf{x}^{(k+1)} = P_{\\boldsymbol{\\theta}}(\\mathbf{z}^{(k)})$ that aims to enforce the constraints.\n\nThe learned proximal mapping $P_{\\boldsymbol{\\theta}}$ shall be constructed so that it is consistent with descending the barrier associated with the linear constraints via the chain rule. You must implement a mapping that uses only linear operators and elementwise smooth functions, and whose parameters $\\boldsymbol{\\theta}$ are treated as fixed learned values within the unrolled iterations. The initial iterate must be $\\mathbf{x}^{(0)} = \\mathbf{0}$.\n\nDefine feasibility at iteration $k$ as the condition that all components of $\\mathbf{C}\\mathbf{x}^{(k)} - \\mathbf{d}$ are less than or equal to a tolerance $\\varepsilon  0$. The feasibility maintenance across iterations for a given test case is a boolean that is true if and only if every iterate $\\mathbf{x}^{(k)}$ for $k = 1, 2, \\ldots, K$ is feasible according to this tolerance. Additionally, define the final maximum violation as the nonnegative scalar $\\max\\big\\{0, \\max_i\\big((\\mathbf{C}\\mathbf{x}^{(K)} - \\mathbf{d})_i\\big)\\big\\}$.\n\nYour program must implement the above unrolled scheme and produce, for each test case, a result consisting of the feasibility maintenance boolean and the final maximum violation. Aggregate the results for all test cases into a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is a two-element list in the form $[\\text{boolean}, \\text{float}]$.\n\nUse the following test suite, where all matrices and vectors are explicitly specified. In all cases, the dimension is $n = m = 3$, the number of constraints $p$ varies, the number of iterations is $K = 25$, and the feasibility tolerance is $\\varepsilon = 10^{-8}$.\n\n- Test case $1$ (happy path):\n  - $\\mathbf{A} = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.2  1.5  0.1 \\\\ 0.0  0.1  2.0 \\end{bmatrix}$, $\\mathbf{y} = \\begin{bmatrix} 0.4 \\\\ 0.6 \\\\ -0.1 \\end{bmatrix}$.\n  - $\\mathbf{C} = \\mathbf{I}_3$, $\\mathbf{d} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$.\n  - Step size $\\alpha = 0.15$, learned proximal parameters $\\boldsymbol{\\theta} = (\\sigma, \\beta, \\tau) = (0.35, 1.0, 0.1)$.\n\n- Test case $2$ (tighter box constraints):\n  - $\\mathbf{A} = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.2  1.5  0.1 \\\\ 0.0  0.1  2.0 \\end{bmatrix}$, $\\mathbf{y} = \\begin{bmatrix} 0.4 \\\\ 0.6 \\\\ -0.1 \\end{bmatrix}$.\n  - $\\mathbf{C} = \\mathbf{I}_3$, $\\mathbf{d} = \\begin{bmatrix} 0.2 \\\\ 0.2 \\\\ 0.05 \\end{bmatrix}$.\n  - Step size $\\alpha = 0.15$, learned proximal parameters $\\boldsymbol{\\theta} = (0.5, 2.0, 0.08)$.\n\n- Test case $3$ (mixed constraints including a sum constraint):\n  - $\\mathbf{A} = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.2  1.5  0.1 \\\\ 0.0  0.1  2.0 \\end{bmatrix}$, $\\mathbf{y} = \\begin{bmatrix} 0.4 \\\\ 0.6 \\\\ -0.1 \\end{bmatrix}$.\n  - $\\mathbf{C} = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ 1  1  1 \\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix} 0.4 \\\\ 0.4 \\\\ 0.4 \\\\ 0.6 \\end{bmatrix}$.\n  - Step size $\\alpha = 0.2$, learned proximal parameters $\\boldsymbol{\\theta} = (0.4, 1.5, 0.07)$.\n\n- Test case $4$ (aggressive data step):\n  - $\\mathbf{A} = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.2  1.5  0.1 \\\\ 0.0  0.1  2.0 \\end{bmatrix}$, $\\mathbf{y} = \\begin{bmatrix} 0.4 \\\\ 0.6 \\\\ -0.1 \\end{bmatrix}$.\n  - $\\mathbf{C} = \\mathbf{I}_3$, $\\mathbf{d} = \\begin{bmatrix} 0.3 \\\\ 0.3 \\\\ 0.3 \\end{bmatrix}$.\n  - Step size $\\alpha = 0.65$, learned proximal parameters $\\boldsymbol{\\theta} = (0.45, 2.0, 0.05)$.\n\n- Test case $5$ (ill-conditioned forward operator with a single aggregate constraint):\n  - $\\mathbf{A} = \\begin{bmatrix} 3.0  2.5  0.0 \\\\ 2.5  2.8  1.2 \\\\ 0.0  1.2  0.5 \\end{bmatrix}$, $\\mathbf{y} = \\begin{bmatrix} 2.0 \\\\ -1.0 \\\\ 0.5 \\end{bmatrix}$.\n  - $\\mathbf{C} = \\begin{bmatrix} 1  1  1 \\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix} 0.7 \\end{bmatrix}$.\n  - Step size $\\alpha = 0.3$, learned proximal parameters $\\boldsymbol{\\theta} = (0.6, 3.0, 0.1)$.\n\nThe program must:\n- Implement the unrolled scheme with $K = 25$ iterations and feasibility tolerance $\\varepsilon = 10^{-8}$.\n- For each test case, compute the sequence $\\{\\mathbf{x}^{(k)}\\}_{k=1}^{K}$ and evaluate the feasibility maintenance boolean and the final maximum violation.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each inner test-case result is itself a list $[\\text{boolean}, \\text{float}]$. For example, a valid output format is $[[\\text{True}, 0.0],[\\text{False}, 0.1234],\\ldots]$.\n\nNo physical units, angle units, or percentage units are involved in this problem. All quantities are dimensionless real numbers.", "solution": "The user has provided a problem that requires the design and implementation of a learned, unrolled optimization scheme for a constrained inverse problem. The solution must be presented according to a specific format, beginning with problem validation.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Objective Function**: $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x} - \\mathbf{y}\\|_2^2$, where $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{y} \\in \\mathbb{R}^m$.\n- **Inequality Constraints**: $\\mathbf{C}\\mathbf{x} \\le \\mathbf{d}$, where $\\mathbf{C} \\in \\mathbb{R}^{p \\times n}$ and $\\mathbf{d} \\in \\mathbb{R}^p$.\n- **Smooth Barrier Function**: For a scalar argument $u$, the barrier is $b_\\tau(u) = \\tau \\log\\big(1 + \\exp(u/\\tau)\\big)$ with a temperature parameter $\\tau > 0$.\n- **Unrolled Iterative Scheme**: A fixed-depth scheme of $K$ iterations.\n  - Initial iterate: $\\mathbf{x}^{(0)} = \\mathbf{0}$.\n  - For $k = 0, \\dots, K-1$:\n    1.  **Gradient Descent Update**: Compute an intermediate point $\\mathbf{z}^{(k)} = \\mathbf{x}^{(k)} - \\alpha \\nabla f(\\mathbf{x}^{(k)})$ with step size $\\alpha > 0$.\n    2.  **Learned Proximal Mapping**: Apply a mapping $P_{\\boldsymbol{\\theta}}$ to get $\\mathbf{x}^{(k+1)} = P_{\\boldsymbol{\\theta}}(\\mathbf{z}^{(k)})$.\n- **Proximal Mapping Specification**: $P_{\\boldsymbol{\\theta}}$ is parameterized by $\\boldsymbol{\\theta}$, is consistent with descending the barrier associated with the constraints via the chain rule, and uses only linear operators and elementwise smooth functions.\n- **Feasibility Metrics**:\n  - **Feasibility at iteration $k$**: All components of $\\mathbf{C}\\mathbf{x}^{(k)} - \\mathbf{d}$ are less than or equal to a tolerance $\\varepsilon > 0$.\n  - **Feasibility Maintenance**: A boolean value, true if and only if every iterate $\\mathbf{x}^{(k)}$ for $k = 1, 2, \\ldots, K$ is feasible.\n  - **Final Maximum Violation**: The non-negative scalar $\\max\\big\\{0, \\max_i\\big((\\mathbf{C}\\mathbf{x}^{(K)} - \\mathbf{d})_i\\big)\\big\\}$.\n- **Global Parameters**: $n = m = 3$, $K = 25$, $\\varepsilon = 10^{-8}$.\n- **Test Cases**: Five test cases are provided, each specifying matrices $\\mathbf{A}$, $\\mathbf{C}$, vectors $\\mathbf{y}$, $\\mathbf{d}$, and parameters $\\alpha$ and $\\boldsymbol{\\theta} = (\\sigma, \\beta, \\tau)$.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded**: The problem is well-founded in numerical optimization, inverse problems, and machine learning. The use of least-squares, linear constraints, barrier methods (specifically the softplus function), and unrolled gradient-based schemes are all standard and rigorous concepts.\n2.  **Well-Posed**: The problem is well-posed. It provides all necessary mathematical definitions, algorithmic steps, and numerical data to arrive at a unique computational result. The description of the learned proximal mapping $P_{\\boldsymbol{\\theta}}$ is sufficiently constrained to imply a specific, formalizable structure (a gradient descent step on the barrier penalty).\n3.  **Objective**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n4.  **Completeness and Consistency**: The problem statement is self-contained and internally consistent. All dimensions of matrices and vectors within each test case are compatible.\n5.  **Relevance and Formalizability**: The problem is directly relevant to the specified topic of *learned iterative schemes and unrolled optimization* and is entirely formalizable into a computational algorithm.\n\n**Step 3: Verdict and Action**\n\nThe problem is deemed **valid**. A complete, reasoned solution will be provided.\n\n### Principle-Based Design of the Solution\n\nThe problem requires constructing an unrolled iterative algorithm to solve a constrained optimization problem. The overall objective is to minimize the data fidelity term $f(\\mathbf{x})$ subject to the constraint $\\mathbf{C}\\mathbf{x} \\le \\mathbf{d}$. The unrolled scheme alternates between a step that minimizes the data error and a step that enforces the constraints.\n\n**1. The Unrolled Iterative Scheme**\n\nThe algorithm starts with an initial estimate $\\mathbf{x}^{(0)} = \\mathbf{0}$ and iteratively refines it for a fixed number of iterations, $K$. Each iteration $k$ (from $k=0$ to $k=K-1$) consists of two main steps:\n\n**Step A: Data Fidelity Update**\n\nThis step performs a gradient descent on the data fidelity objective function, $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x} - \\mathbf{y}\\|_2^2$. The gradient of this function with respect to $\\mathbf{x}$ is:\n$$\n\\nabla f(\\mathbf{x}) = \\mathbf{A}^T(\\mathbf{A}\\mathbf{x} - \\mathbf{y})\n$$\nThe update rule for this step, starting from the current iterate $\\mathbf{x}^{(k)}$, is:\n$$\n\\mathbf{z}^{(k)} = \\mathbf{x}^{(k)} - \\alpha \\nabla f(\\mathbf{x}^{(k)}) = \\mathbf{x}^{(k)} - \\alpha \\mathbf{A}^T(\\mathbf{A}\\mathbf{x}^{(k)} - \\mathbf{y})\n$$\nwhere $\\alpha > 0$ is the step size. The resulting vector $\\mathbf{z}^{(k)}$ is an intermediate estimate that has moved closer to minimizing the data mismatch but may violate the constraints.\n\n**Step B: Constraint Enforcement via Learned Proximal Mapping**\n\nThis step, $\\mathbf{x}^{(k+1)} = P_{\\boldsymbol{\\theta}}(\\mathbf{z}^{(k)})$, is designed to pull the intermediate estimate $\\mathbf{z}^{(k)}$ back towards the feasible region. The constraints $\\mathbf{C}\\mathbf{x} \\le \\mathbf{d}$ are equivalent to $(\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i \\le 0$ for all $i$. A violation occurs when $(\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i > 0$.\n\nWe use the provided smooth barrier function $b_\\tau(u) = \\tau \\log(1 + \\exp(u/\\tau))$ to penalize such violations. This function approximates the positive part function, $\\max(0, u)$. The total barrier penalty for a given $\\mathbf{x}$ is the sum of penalties for each constraint, scaled by a parameter $\\beta$:\n$$\nB(\\mathbf{x}) = \\sum_{i=1}^{p} \\beta \\cdot b_\\tau\\big((\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i\\big)\n$$\nThe proximal mapping $P_{\\boldsymbol{\\theta}}$ is structured as a gradient descent step on this barrier function, starting from $\\mathbf{z}^{(k)}$. First, we derive the gradient of $B(\\mathbf{x})$. The derivative of the barrier function is the sigmoid function:\n$$\nb'_\\tau(u) = \\frac{d}{du} \\left[\\tau \\log(1+e^{u/\\tau})\\right] = \\frac{1}{1+e^{-u/\\tau}} =: \\text{sigm}_\\tau(u)\n$$\nUsing the chain rule, the gradient of the total barrier penalty $B(\\mathbf{x})$ is:\n$$\n\\nabla B(\\mathbf{x}) = \\sum_{i=1}^{p} \\beta \\cdot b'_\\tau\\big((\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i\\big) \\cdot \\nabla_x\\big((\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i\\big) = \\beta \\mathbf{C}^T \\mathbf{s}(\\mathbf{x})\n$$\nwhere $\\mathbf{s}(\\mathbf{x})$ is a vector whose $i$-th component is $s_i(\\mathbf{x}) = \\text{sigm}_\\tau\\big((\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i\\big)$.\n\nThe learned proximal mapping $P_{\\boldsymbol{\\theta}}$ performs a single gradient descent step on the barrier penalty, with a step size $\\sigma$, starting from the point $\\mathbf{z}^{(k)}$:\n$$\n\\mathbf{x}^{(k+1)} = P_{\\boldsymbol{\\theta}}(\\mathbf{z}^{(k)}) = \\mathbf{z}^{(k)} - \\sigma \\nabla_z B(\\mathbf{z}^{(k)})\n$$\nSubstituting the expression for the gradient, we get the final update rule for this step:\n$$\n\\mathbf{x}^{(k+1)} = \\mathbf{z}^{(k)} - \\sigma \\beta \\mathbf{C}^T \\text{sigm}_\\tau(\\mathbf{C}\\mathbf{z}^{(k)} - \\mathbf{d})\n$$\nThe parameters $\\boldsymbol{\\theta} = (\\sigma, \\beta, \\tau)$ are treated as fixed, \"learned\" values that control the behavior of the constraint enforcement step. $\\sigma$ controls the step size, $\\beta$ scales the barrier's influence, and $\\tau$ controls the sharpness of the soft-plus barrier. This construction satisfies the problem's requirement of using only linear operators ($\\mathbf{C}$, $\\mathbf{C}^T$) and elementwise smooth functions ($\\text{sigm}_\\tau$).\n\n**2. Algorithm Summary and Evaluation**\n\nThe complete algorithm unfolds as follows:\n1.  Initialize $\\mathbf{x}^{(0)} = \\mathbf{0}$.\n2.  For $k = 0, 1, \\dots, K-1$:\n    a. Compute $\\mathbf{z}^{(k)} = \\mathbf{x}^{(k)} - \\alpha \\mathbf{A}^T(\\mathbf{A}\\mathbf{x}^{(k)} - \\mathbf{y})$.\n    b. Compute $\\mathbf{x}^{(k+1)} = \\mathbf{z}^{(k)} - \\sigma \\beta \\mathbf{C}^T \\text{sigm}_\\tau(\\mathbf{C}\\mathbf{z}^{(k)} - \\mathbf{d})$.\n    c. Check feasibility: verify if all components of $\\mathbf{C}\\mathbf{x}^{(k+1)} - \\mathbf{d}$ are $\\le \\varepsilon$.\n3.  After $K$ iterations, compute the final maximum violation from $\\mathbf{x}^{(K)}$.\n\nThe feasibility maintenance boolean is true if and only if the check in step 2.c passes for all $k=0, \\dots, K-1$. The final maximum violation is calculated as $\\max\\{0, \\max_i ((\\mathbf{C}\\mathbf{x}^{(K)} - \\mathbf{d})_i)\\}$. This provides a quantitative measure of how well the constraints are satisfied by the final iterate.", "answer": "```python\nimport numpy as np\n\ndef run_unrolled_scheme(A, y, C, d, alpha, theta, K, eps):\n    \"\"\"\n    Implements the unrolled optimization scheme for a given test case.\n\n    Args:\n        A (np.ndarray): Measurement operator.\n        y (np.ndarray): Data vector.\n        C (np.ndarray): Constraint matrix.\n        d (np.ndarray): Constraint vector.\n        alpha (float): Step size for data fidelity term.\n        theta (tuple): Parameters (sigma, beta, tau) for the proximal mapping.\n        K (int): Number of iterations.\n        eps (float): Feasibility tolerance.\n\n    Returns:\n        list: A list containing the feasibility maintenance boolean and the \n              final maximum violation float.\n    \"\"\"\n    sigma, beta, tau = theta\n    n = A.shape[1]\n    x = np.zeros(n)\n    feasibility_maintained = True\n\n    for _ in range(K):\n        # Step 1: Gradient descent on the data fidelity term f(x)\n        grad_f = A.T @ (A @ x - y)\n        z = x - alpha * grad_f\n\n        # Step 2: Apply the learned proximal mapping to enforce constraints\n        # Argument of the sigmoid function\n        u = C @ z - d\n        \n        # Elementwise sigmoid function: 1 / (1 + exp(-u/tau))\n        # Add a small constant to tau to prevent division by zero, though not strictly needed for given test cases\n        s = 1.0 / (1.0 + np.exp(-u / (tau + 1e-12)))\n        \n        # Gradient of the barrier function B(z)\n        grad_b = beta * (C.T @ s)\n        \n        # Update x for the next iteration\n        x_next = z - sigma * grad_b\n        \n        # Check feasibility for the new iterate x^(k+1)\n        violations = C @ x_next - d\n        if np.any(violations  eps):\n            feasibility_maintained = False\n        \n        x = x_next\n        \n    # After all K iterations, calculate the final maximum violation\n    final_violations = C @ x - d\n    final_max_violation = np.maximum(0.0, np.max(final_violations))\n    \n    return [feasibility_maintained, final_max_violation]\n\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the unrolled optimization scheme for each,\n    then prints the results in the specified format.\n    \"\"\"\n    \n    # Common parameters\n    K = 25\n    eps = 1e-8\n    \n    # Test Suite\n    A_common = np.array([[1.0, 0.2, 0.0], [0.2, 1.5, 0.1], [0.0, 0.1, 2.0]])\n    y_common = np.array([0.4, 0.6, -0.1])\n    \n    test_cases = [\n        # Test case 1 (happy path)\n        {\n            \"A\": A_common, \"y\": y_common,\n            \"C\": np.eye(3), \"d\": np.array([0.5, 0.5, 0.5]),\n            \"alpha\": 0.15, \"theta\": (0.35, 1.0, 0.1)\n        },\n        # Test case 2 (tighter box constraints)\n        {\n            \"A\": A_common, \"y\": y_common,\n            \"C\": np.eye(3), \"d\": np.array([0.2, 0.2, 0.05]),\n            \"alpha\": 0.15, \"theta\": (0.5, 2.0, 0.08)\n        },\n        # Test case 3 (mixed constraints)\n        {\n            \"A\": A_common, \"y\": y_common,\n            \"C\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]),\n            \"d\": np.array([0.4, 0.4, 0.4, 0.6]),\n            \"alpha\": 0.2, \"theta\": (0.4, 1.5, 0.07)\n        },\n        # Test case 4 (aggressive data step)\n        {\n            \"A\": A_common, \"y\": y_common,\n            \"C\": np.eye(3), \"d\": np.array([0.3, 0.3, 0.3]),\n            \"alpha\": 0.65, \"theta\": (0.45, 2.0, 0.05)\n        },\n        # Test case 5 (ill-conditioned operator)\n        {\n            \"A\": np.array([[3.0, 2.5, 0.0], [2.5, 2.8, 1.2], [0.0, 1.2, 0.5]]),\n            \"y\": np.array([2.0, -1.0, 0.5]),\n            \"C\": np.array([[1, 1, 1]]), \"d\": np.array([0.7]),\n            \"alpha\": 0.3, \"theta\": (0.6, 3.0, 0.1)\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_unrolled_scheme(\n            case[\"A\"], case[\"y\"], case[\"C\"], case[\"d\"],\n            case[\"alpha\"], case[\"theta\"], K, eps\n        )\n        results.append(result)\n\n    # Convert a list of lists like [[True, 0.0], [False, 0.123]]\n    # into the string \"[[True,0.0],[False,0.123]]\" to match format.\n    result_strings = [str(res).replace(', ', ',') for res in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3396268"}, {"introduction": "While mathematically elegant, unrolled optimization models can be deep and computationally demanding to train. This exercise shifts focus from the mathematical properties of the algorithm to the practical engineering trade-offs involved in its implementation, specifically the relationship between memory usage and computational time. By analyzing and comparing a standard backpropagation approach with a memory-efficient reversible design, you will develop a crucial understanding of the computational complexity that underpins the training of large-scale learned iterative schemes. [@problem_id:3396297]", "problem": "Consider training a $K$-layer unrolled Iterative Shrinkage-Thresholding Algorithm (ISTA) network for a linear inverse problem $y = A x + \\varepsilon$, where the forward operator $A \\in \\mathbb{R}^{n \\times n}$ and the latent vector $x \\in \\mathbb{R}^{n}$ is $\\ell_{1}$-regularized to promote sparsity. In the unrolled architecture, each layer performs one gradient descent step for the data-fidelity term followed by an entrywise shrinkage map. Assume the following scientifically standard modeling choices and implementation conventions, which you must use as the fundamental base for your derivation:\n\n- Each layer performs one multiplication by $A$ applied to the current sparse iterate $x^{k}$ and one multiplication by $A^{\\top}$ applied to a dense residual vector, followed by an elementwise shrinkage. That is, the per-layer forward computational work is the sum of:\n  1) accessing $s$ nonzero entries of $x^{k}$ in $A x^{k}$,\n  2) accessing $n$ entries in $A^{\\top} r^{k}$ for a dense residual $r^{k}$,\n  3) $n$ scalar shrinkage operations, and\n  4) linear-time vector additions on $n$-dimensional arrays, which you may absorb into the $n$ term.\n- The sparse iterate $x^{k}$ has exactly $s$ nonzero entries for all layers $k=1,\\dots,K$ (constant sparsity across layers), stored in a compressed sparse representation that records, per nonzero, one floating-point value and one integer index. For counting memory, treat each recorded value or index as one unit.\n- During training with backpropagation (backprop), all intermediate layer activations $\\{x^{1},\\dots,x^{K}\\}$ are retained. In addition, there are $2$ dense working buffers of length $n$ needed at peak (for example, a residual and a gradient-carrying buffer), which you must include in the peak memory count. Ignore parameter memory and batch-related overhead; focus on activation and working-buffer memory at peak.\n- In a reversible design, the $K$-layer transformation is made bijective by a standard additive coupling that preserves information, enabling backpropagation without storing intermediate activations. In that reversible regime, only the terminal activation $x^{K}$ is retained (in the same sparse format) together with the same $2$ dense working buffers of length $n$ at peak.\n- For time accounting per training sample, model the backward pass through linear operators and elementwise shrinkage as work equivalent to one additional application of the same sequence of operations as the forward pass. In the reversible design, because intermediate activations are not stored, the backward pass must first recompute them, incurring one extra forward-equivalent pass before performing the backward-equivalent pass. Thus, per layer, use the following forward-equivalent units:\n  Backprop: $2$ forward-equivalents per layer (one forward, one backward-equivalent),\n  Reversible: $3$ forward-equivalents per layer (one forward, one recomputation, one backward-equivalent).\n\nUnder these assumptions, derive symbolic expressions in terms of $n$, $s$, and $K$ for:\n1) the peak activation memory during backpropagation training, and\n2) the peak activation memory during reversible training.\nThen, using the above forward-equivalent accounting, derive the ratio of the total per-sample training time of the reversible design to that of the backpropagation design.\n\nExpress your final answer as a single row vector containing the three expressions $[M_{\\mathrm{bp}}(n,s,K),\\,M_{\\mathrm{rev}}(n,s,K),\\,R_{T}]$, where $M_{\\mathrm{bp}}$ and $M_{\\mathrm{rev}}$ denote peak activation memory under backpropagation and under the reversible design respectively, and $R_{T}$ is the reversible-to-backpropagation training time ratio. No numerical rounding is required. No units are required.", "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded within the field of computational science and deep learning, specifically concerning inverse problems and unrolled optimization. The problem is well-posed, with all necessary assumptions and definitions provided for a unique and meaningful solution. The language is objective and the setup is internally consistent and formalizable. We may therefore proceed with the derivation.\n\nThe task is to derive three quantities related to an unrolled $K$-layer ISTA network: the peak memory for standard backpropagation training ($M_{\\mathrm{bp}}$), the peak memory for reversible-design training ($M_{\\mathrm{rev}}$), and the ratio of total training times ($R_{T}$).\n\n**1. Peak Memory for Backpropagation Training ($M_{\\mathrm{bp}}$)**\n\nAccording to the problem statement, training with standard backpropagation requires retaining all intermediate layer activations $\\{x^{1}, \\dots, x^{K}\\}$.\n- Each activation vector $x^{k}$ is sparse, containing exactly $s$ non-zero entries.\n- The storage format for each sparse vector is a compressed representation where each non-zero entry requires storing one floating-point value and one integer index. This amounts to $2$ memory units per non-zero entry.\n- The memory required to store a single sparse activation vector $x^{k}$ is therefore $2 \\times s = 2s$ units.\n- Since all $K$ intermediate activations are stored, the total memory for these activations is $K \\times (2s) = 2Ks$ units.\n- In addition to the activations, the problem specifies that there are $2$ dense working buffers, each of length $n$. The memory for these buffers is $2 \\times n = 2n$ units.\n- The peak memory, $M_{\\mathrm{bp}}$, is the sum of the memory for the stored activations and the working buffers.\n\n$$M_{\\mathrm{bp}}(n,s,K) = (\\text{Activation Memory}) + (\\text{Working Buffer Memory})$$\n$$M_{\\mathrm{bp}}(n,s,K) = 2Ks + 2n$$\n\n**2. Peak Memory for Reversible Training ($M_{\\mathrm{rev}}$)**\n\nIn the reversible design, the need to store all intermediate activations is eliminated.\n- The problem states that only the terminal activation, $x^{K}$, is retained.\n- This vector is stored in the same sparse format as before. The memory cost for storing $x^{K}$ is therefore $2s$ units.\n- The problem also states that the same $2$ dense working buffers of length $n$ are required at peak. The memory cost for these buffers remains $2n$ units.\n- The peak memory for the reversible design, $M_{\\mathrm{rev}}$, is the sum of the memory for the single stored activation and the working buffers.\n\n$$M_{\\mathrm{rev}}(n,s,K) = (\\text{Terminal Activation Memory}) + (\\text{Working Buffer Memory})$$\n$$M_{\\mathrm{rev}}(n,s,K) = 2s + 2n$$\n\n**3. Ratio of Training Times ($R_{T}$)**\n\nThe problem provides a model for the computational cost of training, measured in \"forward-equivalent units\" per layer. Let $T_{\\mathrm{fwd}}$ represent the computational work of one forward pass through a single layer.\n\n- **Backpropagation Training Time ($T_{\\mathrm{bp}}$):**\n  - For each of the $K$ layers, the cost is specified as $2$ forward-equivalents. This accounts for one forward pass and one backward pass (which is assumed to have an equivalent cost).\n  - The total training time for the entire $K$-layer network is the sum of the costs for each layer.\n  $$T_{\\mathrm{bp}} = \\sum_{k=1}^{K} (2 \\cdot T_{\\mathrm{fwd}}) = K \\cdot (2 T_{\\mathrm{fwd}}) = 2K T_{\\mathrm{fwd}}$$\n\n- **Reversible Training Time ($T_{\\mathrm{rev}}$):**\n  - For the reversible design, intermediate activations are not stored and must be recomputed during the backward pass.\n  - The cost per layer is specified as $3$ forward-equivalents. This accounts for the original forward pass, a recomputation pass (another forward pass), and the backward pass.\n  - The total training time for the $K$-layer network is:\n  $$T_{\\mathrm{rev}} = \\sum_{k=1}^{K} (3 \\cdot T_{\\mathrm{fwd}}) = K \\cdot (3 T_{\\mathrm{fwd}}) = 3K T_{\\mathrm{fwd}}$$\n\n- **Ratio of Times ($R_T$):**\n  - The ratio $R_{T}$ is the total training time of the reversible design divided by that of the backpropagation design.\n  $$R_{T} = \\frac{T_{\\mathrm{rev}}}{T_{\\mathrm{bp}}} = \\frac{3K T_{\\mathrm{fwd}}}{2K T_{\\mathrm{fwd}}}$$\n  - The terms $K$ and $T_{\\mathrm{fwd}}$ cancel, yielding a constant ratio.\n  $$R_{T} = \\frac{3}{2}$$\n\nCombining these three results, we have the peak memory for backpropagation $M_{\\mathrm{bp}}(n,s,K) = 2n + 2Ks$, the peak memory for the reversible design $M_{\\mathrm{rev}}(n,s,K) = 2n + 2s$, and the training time ratio $R_{T} = \\frac{3}{2}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2n + 2Ks  2n + 2s  \\frac{3}{2}\n\\end{pmatrix}\n}\n$$", "id": "3396297"}]}