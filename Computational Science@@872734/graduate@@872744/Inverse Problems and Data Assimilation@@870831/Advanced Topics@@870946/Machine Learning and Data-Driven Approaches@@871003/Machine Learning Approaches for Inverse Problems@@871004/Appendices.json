{"hands_on_practices": [{"introduction": "A fundamental requirement for any reliable inverse problem solver is stability—the assurance that small errors in the data, such as noise, do not lead to arbitrarily large errors in the solution. This practice explores how to enforce and quantify stability for a learned inverse map represented by a neural network. By deriving the network's global Lipschitz constant via spectral normalization and relating it to the singular values of the forward operator, you will gain a hands-on understanding of how to build provably stable models that respect the intrinsic difficulty of the problem [@problem_id:3399532].", "problem": "You are given a linear forward model with observations defined by $y = A x$, where $A \\in \\mathbb{R}^{m \\times n}$ is known. Consider learning an inverse mapping $\\Phi: \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$ using a $K$-layer feedforward neural network of the form\n$$\n\\Phi(y) \\;=\\; W_{K} \\,\\rho\\!\\left(W_{K-1} \\,\\rho\\!\\left(\\cdots \\rho\\!\\left(W_{1} y\\right)\\cdots\\right)\\right),\n$$\nwhere each $W_{k} \\in \\mathbb{R}^{d_{k} \\times d_{k-1}}$ is a linear layer and $\\rho$ is a pointwise nonlinearity that is $1$-Lipschitz (for example, the Rectified Linear Unit (ReLU)). Spectral normalization is enforced so that the spectral norm (operator $2$-norm) of each layer obeys $\\|W_{k}\\|_{2} \\le s_{k}$ for known positive scalars $s_{k}$.\n\n1. Using only the definitions of Lipschitz continuity, operator norms, and their behavior under composition, derive a tight upper bound $L_{\\mathrm{sn}}$ such that for all $y_{1},y_{2} \\in \\mathbb{R}^{m}$,\n$$\n\\|\\Phi(y_{1})-\\Phi(y_{2})\\|_{2} \\;\\le\\; L_{\\mathrm{sn}} \\,\\|y_{1}-y_{2}\\|_{2}.\n$$\nExpress $L_{\\mathrm{sn}}$ in terms of the $\\{s_{k}\\}_{k=1}^{K}$.\n\n2. Let $A$ have Singular Value Decomposition (SVD) $A = U \\Sigma V^{\\top}$ with singular values $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge \\sigma_{r} > 0$ and rank $r$. Suppose that identifiability is restricted to the subspace spanned by the right singular vectors associated with singular values at least a threshold $\\tau>0$, that is, to the subspace $\\mathcal{V}_{\\mathrm{id}} = \\mathrm{span}\\{v_{i}:\\sigma_{i}\\ge\\tau\\}$. Using only properties of the SVD and the Moore-Penrose pseudoinverse (MPP), determine the smallest Lipschitz constant of any exact right-inverse on $\\mathcal{V}_{\\mathrm{id}}$, and express it as a function of the smallest singular value $\\sigma_{\\min}^{\\mathrm{id}}$ within $\\mathcal{V}_{\\mathrm{id}}$.\n\n3. In practice, to ensure conditional stability while not exceeding the amplification that is unavoidable due to the forward operator $A$, one enforces the global Lipschitz bound of $\\Phi$ at the level\n$$\nL_{\\star} \\;=\\; \\min\\!\\big\\{L_{\\mathrm{sn}},\\, L_{\\mathrm{id}}\\big\\},\n$$\nwhere $L_{\\mathrm{id}}$ is the minimal Lipschitz constant of an exact right-inverse on $\\mathcal{V}_{\\mathrm{id}}$ found in part 2. Given the following concrete values:\n- $K=3$, $s_{1}=2$, $s_{2}=1.5$, $s_{3}=1.2$,\n- $A$ has singular values $\\{10,\\, 2,\\, 0.5,\\, 0.01\\}$, and the identifiability threshold is $\\tau=0.4$,\ncompute $L_{\\star}$.\n\nExpress your final answer as a pure number (no units), rounded to four significant figures.", "solution": "The problem is validated as being self-contained, scientifically grounded in the mathematics of linear algebra and neural network theory, and well-posed. The solution process proceeds in three parts as requested.\n\n### Part 1: Derivation of the Lipschitz bound $L_{\\mathrm{sn}}$\n\nThe goal is to find a tight upper bound on the Lipschitz constant of the neural network $\\Phi(y)$. The network is a composition of functions. Let us define the function for each layer. For $k \\in \\{1, 2, \\dots, K-1\\}$, let the $k$-th layer transformation be $f_k(z) = \\rho(W_k z)$. For the final layer, let $f_K(z) = W_K z$. The network is then the composition $\\Phi(y) = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1(y)$.\n\nThe Lipschitz constant of a composition of functions is bounded by the product of their individual Lipschitz constants. That is, $L(g \\circ h) \\le L(g)L(h)$. We will apply this property recursively.\n\nFirst, let's determine the Lipschitz constant for each layer function $f_k$.\n\nFor an intermediate layer $k \\in \\{1, 2, \\dots, K-1\\}$, we have $f_k(z) = \\rho(W_k z)$. Let $z_1, z_2$ be two vectors in the domain of $f_k$. We analyze the norm of the difference:\n$$\n\\|f_k(z_1) - f_k(z_2)\\|_2 = \\|\\rho(W_k z_1) - \\rho(W_k z_2)\\|_2\n$$\nThe problem states that the nonlinearity $\\rho$ is a pointwise $1$-Lipschitz function. For any two vectors $u, v$, a pointwise $1$-Lipschitz function satisfies $\\|\\rho(u) - \\rho(v)\\|_2 \\le \\|u - v\\|_2$. Let $u = W_k z_1$ and $v = W_k z_2$. Applying this property, we get:\n$$\n\\|\\rho(W_k z_1) - \\rho(W_k z_2)\\|_2 \\le \\|W_k z_1 - W_k z_2\\|_2\n$$\nUsing the linearity of $W_k$ and the definition of the operator $2$-norm (spectral norm), we have:\n$$\n\\|W_k z_1 - W_k z_2\\|_2 = \\|W_k (z_1 - z_2)\\|_2 \\le \\|W_k\\|_2 \\|z_1 - z_2\\|_2\n$$\nThe problem specifies that spectral normalization is applied such that $\\|W_k\\|_2 \\le s_k$. Therefore, the Lipschitz constant $L_k$ of the function $f_k$ is bounded by $s_k$:\n$$\n\\|f_k(z_1) - f_k(z_2)\\|_2 \\le s_k \\|z_1 - z_2\\|_2 \\implies L_k \\le s_k\n$$\n\nFor the final layer, $k=K$, the function is $f_K(z) = W_K z$. This is a linear map. Its Lipschitz constant is precisely its operator norm, $L_K = \\|W_K\\|_2$. We are given the bound $\\|W_K\\|_2 \\le s_K$, so $L_K \\le s_K$.\n\nNow, we compose the layers to find the Lipschitz constant of the entire network $\\Phi$.\n$$\nL_{\\Phi} = L(f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1) \\le L_K \\cdot L_{K-1} \\cdots L_1\n$$\nUsing the bounds we derived for each layer:\n$$\nL_{\\Phi} \\le s_K \\cdot s_{K-1} \\cdots s_1 = \\prod_{k=1}^{K} s_k\n$$\nThis upper bound is designated as $L_{\\mathrm{sn}}$. The bound is considered tight because it is achievable for specific choices of weights $W_k$ (where $\\|W_k\\|_2=s_k$) and specific inputs (e.g., if $\\rho$ is the identity and the dominant singular vectors of the weight matrices are aligned).\n$$\nL_{\\mathrm{sn}} = \\prod_{k=1}^{K} s_k\n$$\n\n### Part 2: Derivation of the minimal Lipschitz constant $L_{\\mathrm{id}}$\n\nWe are asked for the smallest Lipschitz constant of any exact right-inverse on the subspace $\\mathcal{V}_{\\mathrm{id}} = \\mathrm{span}\\{v_{i}:\\sigma_{i}\\ge\\tau\\}$. Let such an inverse map be $\\mathcal{G}: \\mathbb{R}^m \\to \\mathbb{R}^n$. The condition of being an exact right-inverse on $\\mathcal{V}_{\\mathrm{id}}$ means that for any vector $x \\in \\mathcal{V}_{\\mathrm{id}}$, if $y = Ax$, then $\\mathcal{G}(y) = x$.\n\nLet $L_{\\mathcal{G}}$ be the Lipschitz constant of such a map $\\mathcal{G}$. Let $\\sigma_{\\min}^{\\mathrm{id}} = \\min\\{\\sigma_i : \\sigma_i \\ge \\tau\\}$, and let $v_j$ be a right singular vector corresponding to this singular value, so $Av_j = \\sigma_j u_j$ with $\\sigma_j = \\sigma_{\\min}^{\\mathrm{id}}$. Since $\\sigma_j \\ge \\tau$, $v_j \\in \\mathcal{V}_{\\mathrm{id}}$.\nLet us test the Lipschitz condition with two specific points. Let $x_1 = v_j$ and $x_2=0$. Both are in $\\mathcal{V}_{\\mathrm{id}}$. The corresponding observations are $y_1 = Ax_1 = Av_j = \\sigma_j u_j$ and $y_2 = Ax_2 = 0$.\nThe right-inverse property demands $\\mathcal{G}(y_1) = x_1 = v_j$ and $\\mathcal{G}(y_2) = x_2 = 0$.\nThe definition of the Lipschitz constant requires:\n$$\n\\|\\mathcal{G}(y_1) - \\mathcal{G}(y_2)\\|_2 \\le L_{\\mathcal{G}} \\|y_1 - y_2\\|_2\n$$\nSubstituting the vectors:\n$$\n\\|v_j - 0\\|_2 \\le L_{\\mathcal{G}} \\|\\sigma_j u_j - 0\\|_2\n$$\nThe singular vectors are orthonormal, so $\\|v_j\\|_2 = 1$ and $\\|u_j\\|_2 = 1$. The inequality becomes:\n$$\n1 \\le L_{\\mathcal{G}} |\\sigma_j| \\cdot \\|u_j\\|_2 = L_{\\mathcal{G}} \\sigma_j\n$$\nSince $\\sigma_j = \\sigma_{\\min}^{\\mathrm{id}}$, we have a lower bound on the Lipschitz constant for *any* such inverse map $\\mathcal{G}$:\n$$\nL_{\\mathcal{G}} \\ge \\frac{1}{\\sigma_{\\min}^{\\mathrm{id}}}\n$$\nThis shows that the minimal possible Lipschitz constant is at least $1/\\sigma_{\\min}^{\\mathrm{id}}$. To show that this minimum is achievable, we must construct a map that satisfies the right-inverse property and has this exact Lipschitz constant.\n\nConsider the truncated singular value decomposition (TSVD) inverse, defined as:\n$$\n\\mathcal{G}_{\\mathrm{TSVD}} = \\sum_{i : \\sigma_i \\ge \\tau} \\frac{1}{\\sigma_i} v_i u_i^{\\top}\n$$\nThis is a linear map from $\\mathbb{R}^m$ to $\\mathbb{R}^n$. Its operator norm (which is its Lipschitz constant) is given by the largest singular value of the map, which is $\\max_{i:\\sigma_i \\ge \\tau} (1/\\sigma_i) = 1/\\min_{i:\\sigma_i \\ge \\tau} (\\sigma_i) = 1/\\sigma_{\\min}^{\\mathrm{id}}$.\nLet's verify it is an exact right-inverse on $\\mathcal{V}_{\\mathrm{id}}$. Any $x \\in \\mathcal{V}_{\\mathrm{id}}$ can be written as $x = \\sum_{j : \\sigma_j \\ge \\tau} c_j v_j$. Then $y = Ax = \\sum_{j : \\sigma_j \\ge \\tau} c_j A v_j = \\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j u_j$.\nApplying $\\mathcal{G}_{\\mathrm{TSVD}}$ to $y$:\n$$\n\\mathcal{G}_{\\mathrm{TSVD}}(y) = \\mathcal{G}_{\\mathrm{TSVD}}\\left(\\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j u_j\\right) = \\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j \\mathcal{G}_{\\mathrm{TSVD}}(u_j)\n$$\nBy definition of $\\mathcal{G}_{\\mathrm{TSVD}}$ and orthogonality of $\\{u_i\\}$, $\\mathcal{G}_{\\mathrm{TSVD}}(u_j) = (1/\\sigma_j) v_j$.\n$$\n\\mathcal{G}_{\\mathrm{TSVD}}(y) = \\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j \\left(\\frac{1}{\\sigma_j} v_j\\right) = \\sum_{j : \\sigma_j \\ge \\tau} c_j v_j = x\n$$\nThe map fulfills the condition. Since we found a map that achieves the lower bound, this is the minimal possible Lipschitz constant.\n$$\nL_{\\mathrm{id}} = \\frac{1}{\\sigma_{\\min}^{\\mathrm{id}}}\n$$\n\n### Part 3: Computation of $L_{\\star}$\n\nWe are given the concrete values and need to compute $L_{\\star} = \\min\\{L_{\\mathrm{sn}}, L_{\\mathrm{id}}\\}$.\n\nFirst, compute $L_{\\mathrm{sn}}$:\n- $K=3$\n- $s_1=2$, $s_2=1.5$, $s_3=1.2$\n- Using the formula from Part 1:\n$$\nL_{\\mathrm{sn}} = \\prod_{k=1}^{3} s_k = s_1 \\cdot s_2 \\cdot s_3 = 2 \\times 1.5 \\times 1.2 = 3 \\times 1.2 = 3.6\n$$\n\nNext, compute $L_{\\mathrm{id}}$:\n- Singular values of $A$: $\\{10, 2, 0.5, 0.01\\}$\n- Identifiability threshold: $\\tau = 0.4$\n- We must find $\\sigma_{\\min}^{\\mathrm{id}} = \\min\\{\\sigma_i : \\sigma_i \\ge \\tau\\}$.\n- The singular values greater than or equal to $\\tau=0.4$ are $\\{10, 2, 0.5\\}$.\n- The minimum among these is $\\sigma_{\\min}^{\\mathrm{id}} = 0.5$.\n- Using the formula from Part 2:\n$$\nL_{\\mathrm{id}} = \\frac{1}{\\sigma_{\\min}^{\\mathrm{id}}} = \\frac{1}{0.5} = 2\n$$\n\nFinally, compute $L_{\\star}$:\n$$\nL_{\\star} = \\min\\{L_{\\mathrm{sn}}, L_{\\mathrm{id}}\\} = \\min\\{3.6, 2\\} = 2\n$$\nThe problem asks for the answer to be rounded to four significant figures.\n$$\nL_{\\star} = 2.000\n$$", "answer": "$$\\boxed{2.000}$$", "id": "3399532"}, {"introduction": "Instead of treating a neural network as a black box, we can design its architecture to explicitly mimic the steps of a classical optimization algorithm. This powerful \"unrolling\" paradigm creates more interpretable and often better-performing models. This exercise guides you through the analysis of an unrolled proximal-gradient network, demonstrating how it implicitly defines a spectral filter and how its depth acts as a regularization parameter, connecting directly to classical concepts like the discrepancy principle [@problem_id:3399544].", "problem": "Consider the linear inverse problem $y = A x^{\\star} + \\eta$ with $A \\in \\mathbb{R}^{m \\times n}$, unknown target $x^{\\star} \\in \\mathbb{R}^{n}$, and additive noise $\\eta \\in \\mathbb{R}^{m}$ satisfying a known bound $\\|\\eta\\|_{2} \\leq \\delta$. Assume a data-fidelity objective derived from the squared residual and a quadratic penalty, so that the regularized cost is $J(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\mu \\|x\\|_{2}^{2}$ with a regularization weight $\\mu > 0$. Consider an unrolled Proximal Gradient (PG) network with $L$ layers, whose $k$-th layer performs the update\n$$\nx^{k+1} = \\mathrm{prox}_{\\alpha_{k} \\mu \\|\\cdot\\|_{2}^{2}}\\!\\left(x^{k} - \\alpha_{k} A^{\\top}(A x^{k} - y)\\right), \\quad x^{0} = 0,\n$$\nwhere $\\alpha_{k} > 0$ is a trainable step size and $\\mathrm{prox}_{\\gamma R}(v)$ denotes the proximal operator of $\\gamma R$ evaluated at $v$. You may take as given the definition of the proximal operator and standard properties of the Singular Value Decomposition (SVD). The goal is to analyze the implicit spectral filtering induced by the trainable parameters and to derive a principled early-stopping rule.\n\nYour tasks:\n- Derive, from first principles of proximal calculus and linear algebra, the layer-wise update in terms of the symmetric matrix $M = A^{\\top} A$, and show that the network defines a linear mapping from $A^{\\top} y$ to $x^{L}$.\n- Using the eigen-decomposition of $M$ and the linearity of the network, derive the scalar spectral filter $g_{L}(\\lambda)$ such that for any eigenpair $(\\lambda, u)$ of $M$ with $M u = \\lambda u$, the component of $x^{L}$ along $u$ is $g_{L}(\\lambda)\\,(u^{\\top} A^{\\top} y)$. Assume for this part that the trainable parameters are chosen layer-constant, $\\alpha_{k} \\equiv \\alpha$ and $\\mu$ fixed across layers.\n- Starting from the data model and the spectral representation of the iterates, formulate a discrepancy principle for early stopping. That is, propose a rule to select the smallest $L$ such that the residual $\\|A x^{L} - y\\|_{2}$ approximately matches the noise level multiplied by a safety factor $\\tau > 1$, and explain why this guarantees stability with respect to perturbations in $y$.\n- Provide your final answer as the closed-form expression for $g_{L}(\\lambda)$ under the layer-constant choice $\\alpha_{k} \\equiv \\alpha$ and fixed $\\mu$. No numerical evaluation is required.\n\nExpress all mathematical expressions in LaTeX. The final answer must be a single closed-form analytic expression. Do not include any units.", "solution": "The problem is analyzed and solved in four parts as requested.\n\nFirst, we derive the layer-wise update in terms of the matrix $M = A^{\\top} A$ and demonstrate the linearity of the network map.\nThe $k$-th layer update is given by\n$$\nx^{k+1} = \\mathrm{prox}_{\\alpha_{k} \\mu \\|\\cdot\\|_{2}^{2}}\\!\\left(x^{k} - \\alpha_{k} A^{\\top}(A x^{k} - y)\\right)\n$$\nThe proximal operator $\\mathrm{prox}_{\\gamma R}(v)$ is defined as the solution to an optimization problem:\n$$\n\\mathrm{prox}_{\\gamma R}(v) = \\arg\\min_{z} \\left\\{ \\gamma R(z) + \\frac{1}{2}\\|z-v\\|_{2}^{2} \\right\\}\n$$\nIn our case, the regularization function is $R(x) = \\|x\\|_{2}^{2}$ and the parameter is $\\gamma = \\alpha_k \\mu$. The minimization objective is\n$$\nf(z) = \\alpha_{k} \\mu \\|z\\|_{2}^{2} + \\frac{1}{2}\\|z-v\\|_{2}^{2}\n$$\nThis is a strictly convex function. We find the minimum by setting the gradient with respect to $z$ to zero:\n$$\n\\nabla_{z} f(z) = 2 \\alpha_{k} \\mu z + (z - v) = 0\n$$\n$$\n(1 + 2 \\alpha_{k} \\mu) z = v \\implies z = \\frac{1}{1 + 2 \\alpha_{k} \\mu} v\n$$\nThus, the proximal operator is a simple scaling: $\\mathrm{prox}_{\\alpha_{k} \\mu \\|\\cdot\\|_{2}^{2}}(v) = (1 + 2 \\alpha_{k} \\mu)^{-1} v$.\nSubstituting this back into the update rule for $x^{k+1}$, we get:\n$$\nx^{k+1} = \\frac{1}{1 + 2 \\alpha_{k} \\mu} \\left(x^{k} - \\alpha_{k} A^{\\top}(A x^{k} - y)\\right)\n$$\nLet's introduce the symmetric matrix $M = A^{\\top} A$. The update becomes:\n$$\nx^{k+1} = \\frac{1}{1 + 2 \\alpha_{k} \\mu} \\left(x^{k} - \\alpha_{k} (M x^{k} - A^{\\top}y)\\right)\n$$\nRearranging the terms, we obtain the update rule expressed in terms of $M$:\n$$\nx^{k+1} = \\frac{1}{1 + 2 \\alpha_{k} \\mu} \\left( (I - \\alpha_{k} M) x^{k} + \\alpha_{k} A^{\\top}y \\right)\n$$\nThis is an affine recurrence relation. To show that the final output $x^L$ is a linear function of $A^{\\top}y$, we unroll the recursion starting from the initial condition $x^0 = 0$:\nFor $k=0$:\n$$\nx^{1} = \\frac{1}{1 + 2 \\alpha_{0} \\mu} \\left( (I - \\alpha_{0} M) x^{0} + \\alpha_{0} A^{\\top}y \\right) = \\frac{\\alpha_{0}}{1 + 2 \\alpha_{0} \\mu} A^{\\top}y\n$$\nFor $k=1$:\n$$\nx^{2} = \\frac{1}{1 + 2 \\alpha_{1} \\mu} \\left( (I - \\alpha_{1} M) x^{1} + \\alpha_{1} A^{\\top}y \\right) = \\frac{I - \\alpha_{1} M}{1 + 2 \\alpha_{1} \\mu} \\left(\\frac{\\alpha_{0}}{1 + 2 \\alpha_{0} \\mu} A^{\\top}y\\right) + \\frac{\\alpha_{1}}{1 + 2 \\alpha_{1} \\mu} A^{\\top}y\n$$\nThis can be written as $x^{2} = (Q_1 + Q_2) A^{\\top}y$, where $Q_1$ and $Q_2$ are matrices. Continuing this process up to layer $L$, $x^L$ will be a sum of terms, where each term is a product of matrices involving $M$ and $\\alpha_k$ applied to a vector proportional to $A^{\\top}y$. The final expression for $x^L$ is of the form:\n$$\nx^L = G_L(\\alpha_0, \\dots, \\alpha_{L-1}, \\mu, M) A^{\\top}y\n$$\nwhere $G_L$ is a matrix-valued function of the parameters. This demonstrates that for fixed parameters, the unrolled network defines a linear mapping from $A^{\\top}y$ to $x^L$.\n\nSecond, we derive the scalar spectral filter $g_{L}(\\lambda)$ for the case of layer-constant parameters, i.e., $\\alpha_k \\equiv \\alpha$.\nThe update rule simplifies to:\n$$\nx^{k+1} = \\frac{1}{1 + 2 \\alpha \\mu} \\left( (I - \\alpha M) x^{k} + \\alpha A^{\\top}y \\right)\n$$\nLet $C = \\frac{1}{1 + 2 \\alpha \\mu}(I - \\alpha M)$ and $d = \\frac{\\alpha}{1 + 2 \\alpha \\mu} A^{\\top}y$. The update is $x^{k+1} = C x^{k} + d$.\nStarting with $x^0=0$, we have $x^L = \\left(\\sum_{k=0}^{L-1} C^k\\right) d$.\nLet $(\\lambda, u)$ be an eigenpair of $M$, so $M u = \\lambda u$. The vector $u$ is also an eigenvector of $C$:\n$$\nC u = \\frac{1}{1 + 2 \\alpha \\mu}(I - \\alpha M)u = \\frac{1}{1 + 2 \\alpha \\mu}(u - \\alpha \\lambda u) = \\left(\\frac{1 - \\alpha \\lambda}{1 + 2 \\alpha \\mu}\\right) u\n$$\nLet's denote the eigenvalue of $C$ corresponding to $\\lambda$ as $\\gamma(\\lambda) = \\frac{1 - \\alpha \\lambda}{1 + 2 \\alpha \\mu}$.\nThe operator $\\sum_{k=0}^{L-1} C^k$ is a geometric series of matrices. Its action on the eigenvector $u$ is multiplication by the scalar $\\sum_{k=0}^{L-1} \\gamma(\\lambda)^k = \\frac{1 - \\gamma(\\lambda)^L}{1 - \\gamma(\\lambda)}$.\nLet's project the equation onto the eigenvector $u$. The component of $x^L$ along $u$ is $u^{\\top}x^L$.\n$$\nu^{\\top}x^L = \\left(\\frac{1 - \\gamma(\\lambda)^L}{1 - \\gamma(\\lambda)}\\right) u^{\\top}d\n$$\nSubstituting the expressions for $d$ and $\\gamma(\\lambda)$:\n$$\nu^{\\top}d = u^{\\top} \\left(\\frac{\\alpha}{1 + 2 \\alpha \\mu} A^{\\top}y\\right) = \\frac{\\alpha}{1 + 2 \\alpha \\mu} (u^{\\top} A^{\\top}y)\n$$\n$$\n1 - \\gamma(\\lambda) = 1 - \\frac{1 - \\alpha \\lambda}{1 + 2 \\alpha \\mu} = \\frac{(1 + 2 \\alpha \\mu) - (1 - \\alpha \\lambda)}{1 + 2 \\alpha \\mu} = \\frac{\\alpha \\lambda + 2 \\alpha \\mu}{1 + 2 \\alpha \\mu} = \\frac{\\alpha(\\lambda + 2\\mu)}{1 + 2 \\alpha \\mu}\n$$\nCombining these, the component of $x^L$ along $u$ is:\n$$\nu^{\\top}x^L = \\left(\\frac{1 - \\gamma(\\lambda)^L}{\\frac{\\alpha(\\lambda + 2\\mu)}{1 + 2 \\alpha \\mu}}\\right) \\left(\\frac{\\alpha}{1 + 2 \\alpha \\mu} (u^{\\top} A^{\\top}y)\\right) = (1 - \\gamma(\\lambda)^L) \\frac{1}{\\lambda + 2\\mu} (u^{\\top} A^{\\top}y)\n$$\nThe problem asks for the scalar filter $g_L(\\lambda)$ such that the component of $x^L$ along $u$ is $g_L(\\lambda) (u^{\\top}A^{\\top}y)$. From the derived expression, we can identify the filter as:\n$$\ng_L(\\lambda) = \\frac{1}{\\lambda + 2\\mu} \\left(1 - \\left(\\frac{1 - \\alpha\\lambda}{1 + 2\\alpha\\mu}\\right)^L\\right)\n$$\n\nThird, we formulate a discrepancy principle for early stopping and explain the stability it provides.\nThe Morozov discrepancy principle is a well-established method for choosing a regularization parameter. In the context of iterative methods, the number of iterations $L$ acts as the regularization parameter.\nThe proposed rule is: Given the data $y$, the noise bound $\\delta$, and a safety factor $\\tau > 1$, choose the number of layers (iterations) $L$ to be the smallest integer $k \\geq 1$ such that the residual norm satisfies:\n$$\n\\|A x^{k} - y\\|_{2} \\leq \\tau \\delta\n$$\nThis rule is implemented by computing the residual at each layer of the network and stopping at the first layer that meets this criterion.\n\nThis principle guarantees stability for the following reasons:\nThe inverse problem is ill-posed, meaning that small singular values of $A$ cause noise amplification in a naive inversion. The iterative process of the unrolled network acts as a regularization scheme. The number of iterations $L$ controls the degree of regularization.\nIn the initial iterations (small $L$), the method reconstructs the components of the solution $x^{\\star}$ corresponding to large singular values of $A$, which are less affected by noise. During this phase, the residual $\\|A x^{k} - y\\|_{2}$ is large and dominated by the yet-to-be-reconstructed part of the signal, $A(x^{\\star}-x^k)$.\nAs $k$ increases, $x^k$ gets closer to $x^{\\star}$, and the residual norm decreases, eventually approaching the norm of the noise, since $Ax^k - y = A(x^k - x^{\\star}) - \\eta$. The discrepancy principle stops the iteration when the residual becomes comparable to the noise level $\\delta$.\nIf the iteration were to continue past this point, the algorithm would attempt to decrease the residual further. Since the residual is already dominated by noise $\\eta$, this would mean fitting the model to the noise. Fitting noise requires the algorithm to reconstruct components corresponding to small singular values, which are heavily contaminated. This leads to a large amplification of the noise components in the solution $x^k$, destroying the reconstruction and causing instability.\nBy stopping early, the discrepancy principle prevents the algorithm from entering this noise-fitting regime. The effective spectral filter $g_L(\\lambda)$ for the chosen $L$ appropriately dampens the components corresponding to small singular values (where $\\lambda = \\sigma^2$ is small), thus preventing the amplification of noise. This ensures that the mapping from the perturbed data $y$ to the solution $x^L$ is a bounded operator, which is the definition of stability.\n\nFinally, the closed-form expression for the spectral filter $g_{L}(\\lambda)$ is provided as the final answer.", "answer": "$$\n\\boxed{\\frac{1}{\\lambda + 2\\mu}\\left(1 - \\left(\\frac{1 - \\alpha\\lambda}{1 + 2\\alpha\\mu}\\right)^L\\right)}\n$$", "id": "3399544"}, {"introduction": "One of the great promises of machine learning is the ability to learn complex, data-driven priors that outperform traditional handcrafted ones. However, since networks like CNNs operate on discrete grids, a crucial theoretical question arises: how does a learned discrete prior relate to the underlying continuous-world problem? This advanced practice tackles this issue by asking you to quantify the approximation error between a true function-space prior and a CNN-based learned prior, providing deep insight into the fidelity of learned models in the continuum limit [@problem_id:3399535].", "problem": "Consider an inverse problem on the $d$-dimensional torus $ \\mathbb{T}^{d} = [0,1]^{d} $ with periodic boundary conditions, where $ d \\in \\{1,2,3\\} $. Let $ \\{e_{k}(x)\\}_{k \\in \\mathbb{Z}^{d}} $, with $ e_{k}(x) = \\exp(2\\pi \\mathrm{i} k \\cdot x) $, denote the standard Fourier basis on $ \\mathbb{T}^{d} $. Define the periodic Laplacian $ \\Delta $ so that $ -\\Delta e_{k} = 4\\pi^{2} |k|^{2} e_{k} $. Let the unknown field $ u $ have a zero-mean Gaussian prior with Matérn covariance operator\n\n$$\n\\mathcal{C} = (\\kappa^{2} - \\Delta)^{-\\alpha},\n$$\n\nwhere $ \\kappa > 0 $ and $ \\alpha > d/2 $. Hence, in the Fourier basis, the covariance eigenvalues are\n\n$$\n\\lambda_{k} = (\\kappa^{2} + 4\\pi^{2} |k|^{2})^{-\\alpha}, \\quad k \\in \\mathbb{Z}^{d}.\n$$\n\nAssume noisy identity observations on a band-limited subspace: for a mesh size $ h \\in (0,1) $, define the band-limited space\n\n$$\nH_{h} := \\left\\{ v \\in L^{2}(\\mathbb{T}^{d}) : v(x) = \\sum_{|k|_{\\infty} \\le K(h)} v_{k} e_{k}(x), \\; K(h) := \\left\\lfloor \\frac{c}{h} \\right\\rfloor, \\; c \\in (0,1) \\right\\}.\n$$\n\nOn $ H_{h} $, the data model is\n\n$$\ny = u + \\eta, \\quad \\eta \\sim \\mathcal{N}(0, \\sigma^{2} I),\n$$\n\nwith $ \\eta $ independent of $ u $, and $ \\sigma > 0 $. Let $ \\mathcal{M} $ denote the posterior mean operator mapping $ y $ to the posterior mean in the continuum model restricted to $ H_{h} $.\n\nNow consider a discrete learned prior obtained by training a Convolutional Neural Network (CNN) on a uniform grid of mesh size $ h $, which implies a stationary Gaussian prior on the grid. Suppose its discrete covariance, when lifted to $ H_{h} $ by trigonometric interpolation, is diagonal in the Fourier basis with symbol $ \\tilde{\\lambda}_{k}(h) $ satisfying the uniform asymptotic expansion\n\n$$\n\\tilde{\\lambda}_{k}(h) = \\lambda_{k} + h^{p} a(k) + o(h^{p}) \\quad \\text{uniformly for } |k|_{\\infty} \\le K(h),\n$$\n\nfor some $p>0$, where $ a(k) $ is bounded and admits a finite supremum $ A := \\sup_{k \\in \\mathbb{Z}^{d}} |a(k)| < \\infty $. Assume further that there exists a sequence $ \\{k(h)\\} $ with $ |k(h)|_{\\infty} \\le K(h) $ such that $ |a(k(h))| \\to A $ as $ h \\to 0 $. Let $ \\tilde{\\mathcal{M}}_{h} $ denote the corresponding posterior mean operator induced by the learned prior on $ H_{h} $ under the same observation model.\n\nUsing only first principles for Gaussian linear inverse problems, the Fourier diagonalization on $ \\mathbb{T}^{d} $, and asymptotic expansion arguments that are justified by the above assumptions, derive the leading-order asymptotic expression, as $ h \\to 0 $, for the operator norm difference\n\n$$\n\\left\\| \\tilde{\\mathcal{M}}_{h} - \\mathcal{M} \\right\\|_{L^{2}(\\mathbb{T}^{d}) \\to L^{2}(\\mathbb{T}^{d}) \\text{ on } H_{h}}.\n$$\n\nYour final answer must be a single closed-form analytic expression in terms of $ A $, $ \\sigma $, and $ h $, including the exponent $ p $. Do not provide an inequality in your final answer. No numerical approximation is required.", "solution": "The task is to find the leading-order asymptotic expression for the operator norm difference $\\| \\tilde{\\mathcal{M}}_{h} - \\mathcal{M} \\|_{L^{2}(\\mathbb{T}^{d}) \\to L^{2}(\\mathbb{T}^{d}) \\text{ on } H_{h}}$ as the discretization parameter $h \\to 0$.\n\nFirst, we express the posterior mean operators $\\mathcal{M}$ and $\\tilde{\\mathcal{M}}_{h}$ in the Fourier domain. In a Gaussian linear inverse problem with data $y = Gx + \\eta$, where $x \\sim \\mathcal{N}(0, C_x)$ and $\\eta \\sim \\mathcal{N}(0, C_\\eta)$, the posterior mean is given by the operator $C_x G^* (G C_x G^* + C_\\eta)^{-1}$ acting on the data $y$.\n\nFor this problem, the setting is on the finite-dimensional space $H_h$, which is spanned by the Fourier basis $\\{e_k(x)\\}_{|k|_\\infty \\le K(h)}$. The forward operator is the identity on $H_h$, so we have $G = I_h$. The noise covariance on $H_h$ is $C_\\eta = \\sigma^2 I_h$.\n\nThe true prior is a zero-mean Gaussian process with covariance operator $\\mathcal{C}$. When restricted to $H_h$, its operator $\\mathcal{C}_h$ is diagonal in the Fourier basis, with eigenvalues $\\lambda_k = (\\kappa^2 + 4\\pi^2|k|^2)^{-\\alpha}$ for $|k|_\\infty \\le K(h)$. The corresponding true posterior mean operator $\\mathcal{M}$ is:\n$$\n\\mathcal{M} = \\mathcal{C}_h (I_h \\mathcal{C}_h I_h + \\sigma^2 I_h)^{-1} = \\mathcal{C}_h (\\mathcal{C}_h + \\sigma^2 I_h)^{-1}\n$$\nThe eigenvalues of $\\mathcal{M}$, corresponding to the Fourier mode $e_k$, are given by the scalar relation:\n$$\n\\mu_k = \\frac{\\lambda_k}{\\lambda_k + \\sigma^2}\n$$\n\nThe learned prior corresponds to a covariance operator $\\tilde{\\mathcal{C}}_h$ on $H_h$, also diagonal in the Fourier basis, with eigenvalues $\\tilde{\\lambda}_k(h)$. The associated posterior mean operator $\\tilde{\\mathcal{M}}_h$ is:\n$$\n\\tilde{\\mathcal{M}}_h = \\tilde{\\mathcal{C}}_h (\\tilde{\\mathcal{C}}_h + \\sigma^2 I_h)^{-1}\n$$\nIts eigenvalues $\\tilde{\\mu}_k(h)$ are:\n$$\n\\tilde{\\mu}_k(h) = \\frac{\\tilde{\\lambda}_k(h)}{\\tilde{\\lambda}_k(h) + \\sigma^2}\n$$\n\nThe operator whose norm we must compute is $\\tilde{\\mathcal{M}}_h - \\mathcal{M}$. Since both $\\mathcal{M}$ and $\\tilde{\\mathcal{M}}_h$ are diagonal in the Fourier basis on $H_h$, their difference is also diagonal with eigenvalues $\\tilde{\\mu}_k(h) - \\mu_k$. The $L^2 \\to L^2$ operator norm is the supremum of the absolute values of these eigenvalues:\n$$\n\\left\\| \\tilde{\\mathcal{M}}_{h} - \\mathcal{M} \\right\\| = \\sup_{|k|_\\infty \\le K(h)} |\\tilde{\\mu}_k(h) - \\mu_k|\n$$\nLet's analyze the eigenvalue difference:\n$$\n\\tilde{\\mu}_k(h) - \\mu_k = \\frac{\\tilde{\\lambda}_k(h)}{\\tilde{\\lambda}_k(h) + \\sigma^2} - \\frac{\\lambda_k}{\\lambda_k + \\sigma^2} = \\frac{\\tilde{\\lambda}_k(h)(\\lambda_k + \\sigma^2) - \\lambda_k(\\tilde{\\lambda}_k(h) + \\sigma^2)}{(\\tilde{\\lambda}_k(h) + \\sigma^2)(\\lambda_k + \\sigma^2)} = \\frac{\\sigma^2 (\\tilde{\\lambda}_k(h) - \\lambda_k)}{(\\tilde{\\lambda}_k(h) + \\sigma^2)(\\lambda_k + \\sigma^2)}\n$$\nWe are given the uniform asymptotic expansion $\\tilde{\\lambda}_k(h) = \\lambda_k + h^p a(k) + o(h^p)$ for $|k|_\\infty \\le K(h)$. Substituting this into the difference gives:\n$$\n\\tilde{\\mu}_k(h) - \\mu_k = \\frac{\\sigma^2 (h^p a(k) + o(h^p))}{(\\lambda_k + h^p a(k) + o(h^p) + \\sigma^2)(\\lambda_k + \\sigma^2)}\n$$\nFor a fixed $k$, as $h \\to 0$, the denominator approaches $(\\lambda_k + \\sigma^2)^2$. The leading-order term is:\n$$\n\\tilde{\\mu}_k(h) - \\mu_k = \\frac{\\sigma^2 h^p a(k)}{(\\lambda_k + \\sigma^2)^2} + o(h^p)\n$$\nThe uniformity of the given expansion ensures this holds uniformly for all $|k|_\\infty \\le K(h)$.\nThe operator norm is then:\n$$\n\\left\\| \\tilde{\\mathcal{M}}_{h} - \\mathcal{M} \\right\\| = \\sup_{|k|_\\infty \\le K(h)} \\left| \\frac{\\sigma^2 h^p a(k)}{(\\lambda_k + \\sigma^2)^2} + o(h^p) \\right| = h^p \\sup_{|k|_\\infty \\le K(h)} \\left( \\frac{\\sigma^2 |a(k)|}{(\\lambda_k + \\sigma^2)^2} + o(1) \\right)\n$$\nThe leading-order asymptotic expression is $C h^p$, where the constant $C$ is given by the limit:\n$$\nC = \\lim_{h\\to 0} \\sup_{|k|_\\infty \\le K(h)} \\frac{\\sigma^2 |a(k)|}{(\\lambda_k + \\sigma^2)^2}\n$$\nAs $h \\to 0$, the cutoff wavenumber $K(h) = \\lfloor c/h \\rfloor \\to \\infty$. The sets of wavenumbers over which the supremum is taken, $S_h := \\{k \\in \\mathbb{Z}^d : |k|_\\infty \\le K(h)\\}$, form an increasing sequence whose union is $\\mathbb{Z}^d$. Thus, the limit of the suprema is the supremum over the limit set:\n$$\nC = \\sup_{k \\in \\mathbb{Z}^d} \\frac{\\sigma^2 |a(k)|}{(\\lambda_k + \\sigma^2)^2}\n$$\nThe problem requires the final answer to be in terms of $A$, $\\sigma$, and $p$, and not $\\kappa$ or $\\alpha$. This implies that the supremum is determined by a universal behavior independent of the specific Matérn parameters. Let's analyze the expression for $C$. The term $|a(k)|$ is bounded by $A$. The term $\\lambda_k = (\\kappa^2+4\\pi^2|k|^2)^{-\\alpha}$ is a positive, monotonically decreasing function of $|k|$ that tends to $0$ as $|k| \\to \\infty$. Consequently, the factor $\\frac{1}{(\\lambda_k + \\sigma^2)^2}$ is a monotonically increasing function of $|k|$, approaching its supremum $\\frac{1}{\\sigma^4}$ as $|k| \\to \\infty$.\n\nThe supremum $C$ is thus achieved where the product $|a(k)| \\cdot (\\lambda_k + \\sigma^2)^{-2}$ is maximized. This occurs when $|a(k)|$ is large and simultaneously $\\lambda_k$ is small, i.e., at large wavenumbers $|k|$. The problem statement guarantees that the supremum $A = \\sup_k |a(k)|$ is \"active\" at high frequencies by assuming the existence of a sequence $\\{k(h)\\}$ with $|k(h)|_\\infty \\le K(h)$ such that $|a(k(h))| \\to A$. For the resulting expression for $C$ to be independent of $\\kappa$ and $\\alpha$, the wavenumbers $k(h)$ must be unbounded, i.e., $|k(h)| \\to \\infty$ as $h\\to 0$. In this case, $\\lambda_{k(h)} \\to 0$.\n\nWe can establish the value of $C$ by bounds. For any $k \\in \\mathbb{Z}^d$, $|a(k)| \\le A$ and $\\lambda_k > 0$, so:\n$$\n\\frac{\\sigma^2 |a(k)|}{(\\lambda_k + \\sigma^2)^2} \\le \\frac{\\sigma^2 A}{(\\lambda_k + \\sigma^2)^2} < \\frac{\\sigma^2 A}{\\sigma^4} = \\frac{A}{\\sigma^2}\n$$\nThus, $C \\le A/\\sigma^2$.\nTo find a lower bound, we consider a sequence $k_j \\to \\infty$ such that $|a(k_j)| \\to A$. Such a sequence must exist under the problem's assumption interpreted as a worst-case scenario. For this sequence, $\\lambda_{k_j} \\to 0$. We have:\n$$\nC = \\sup_{k \\in \\mathbb{Z}^d} \\frac{\\sigma^2 |a(k)|}{(\\lambda_k + \\sigma^2)^2} \\ge \\lim_{j\\to\\infty} \\frac{\\sigma^2 |a(k_j)|}{(\\lambda_{k_j} + \\sigma^2)^2} = \\frac{\\sigma^2 A}{(0+\\sigma^2)^2} = \\frac{A}{\\sigma^2}\n$$\nTherefore, $C = A/\\sigma^2$.\n\nThe leading-order asymptotic expression for the operator norm difference is $C h^p$.\n$$\n\\left\\| \\tilde{\\mathcal{M}}_{h} - \\mathcal{M} \\right\\| \\sim \\frac{A}{\\sigma^2} h^p, \\quad \\text{as } h \\to 0.\n$$", "answer": "$$\n\\boxed{\\frac{A h^{p}}{\\sigma^{2}}}\n$$", "id": "3399535"}]}