## Applications and Interdisciplinary Connections

The preceding chapters have established the foundational principles and mechanisms of multilevel and multifidelity Monte Carlo methods. We have seen that their power lies in reformulating the estimation of an expensive, high-fidelity quantity of interest into the estimation of a cheap, low-fidelity baseline plus a series of computationally inexpensive correction terms. The efficacy of this decomposition hinges on the strong correlation between models at adjacent fidelity levels, which ensures that the variance of the correction terms is small.

This chapter shifts focus from the abstract principles to their concrete realization in a wide array of scientific and engineering disciplines. We will explore how the core concepts of multilevel and multifidelity estimation are not merely theoretical constructs but are, in fact, powerful tools for tackling some of the most challenging computational problems in modern science. Our exploration will demonstrate the versatility of these methods, showing how "fidelity" can be defined in diverse ways—from mesh resolution and time steps to algorithmic tolerances and model complexity—and how the fundamental telescoping identity can be adapted to contexts ranging from Bayesian [posterior sampling](@entry_id:753636) to large-scale data assimilation.

### Core Strategies for Multifidelity Estimation

Before delving into specific application domains, it is instructive to revisit the fundamental strategies that underpin multifidelity methods. The most direct application of the multilevel idea is to express the expectation of a high-fidelity quantity of interest, $Q_h$, in terms of a low-fidelity quantity, $Q_\ell$, plus a correction. This is achieved through the simple but powerful telescoping identity:
$$
\mathbb{E}[Q_h(\theta)] = \mathbb{E}[Q_\ell(\theta)] + \mathbb{E}[Q_h(\theta) - Q_\ell(\theta)]
$$
This identity allows for the construction of an unbiased estimator by using a large number of cheap low-fidelity samples to estimate $\mathbb{E}[Q_\ell(\theta)]$ and a much smaller number of expensive, paired high- and low-fidelity samples to estimate the correction term $\mathbb{E}[Q_h(\theta) - Q_\ell(\theta)]$ [@problem_id:3405065]. The remarkable efficiency gains of Multilevel Monte Carlo (MLMC) methods arise from the analysis of the total computational cost to achieve a root-[mean-square error](@entry_id:194940) of $\varepsilon$. Under standard assumptions on the weak error, strong error, and computational cost of a hierarchy of models, the total cost of an optimized MLMC estimator is substantially lower than for a single-level method. In many cases, this cost scales as $\mathcal{O}(\varepsilon^{-2})$ or $\mathcal{O}(\varepsilon^{-2}(\log\varepsilon)^2)$, which represents a significant improvement over the higher complexity of a standard single-level approach [@problem_id:3531541].

An alternative but closely related framework is the [control variate](@entry_id:146594) estimator. Here, the low-fidelity model is used to construct a zero-mean random variable that is correlated with the high-fidelity quantity of interest. A general two-fidelity [control variate](@entry_id:146594) estimator can be written as:
$$
\hat{Q} = \frac{1}{N_h}\sum_{i=1}^{N_h} Q_h^{(i)} - \alpha\left(\frac{1}{N_\ell}\sum_{j=1}^{N_\ell} Q_\ell^{(j)} - \mu_\ell\right)
$$
where $\mu_\ell = \mathbb{E}[Q_\ell]$ is the known or estimated mean of the low-fidelity model. This estimator is unbiased for $\mathbb{E}[Q_h]$ for any choice of the coefficient $\alpha$, provided that $\mu_\ell$ is either the exact mean or itself an [unbiased estimator](@entry_id:166722). The choice of $\alpha$ and the use of paired versus [independent samples](@entry_id:177139) do not affect unbiasedness, but they are critical for variance reduction [@problem_id:3405059].

A third fundamental strategy is multifidelity importance sampling, where a cheap surrogate [posterior distribution](@entry_id:145605) is used as a proposal mechanism to generate samples for a more expensive target posterior. The samples are then re-weighted to correct for the bias introduced by sampling from the surrogate. For this scheme to be unbiased, the target distribution must be absolutely continuous with respect to the proposal distribution, a condition ensuring that the proposal has support everywhere the target does. The variance of the resulting estimator, however, depends on stricter conditions, as it can become infinite if the proposal distribution has lighter tails than the target [@problem_id:3405125].

These core strategies form a versatile toolkit that can be adapted to a multitude of computational contexts.

### Accelerating Posterior Sampling with MCMC Methods

A primary application of multifidelity methods is in accelerating Bayesian inference, where the goal is to sample from a posterior distribution $\pi(\theta | y) \propto L(y|\theta)\pi_0(\theta)$. In many high-dimensional problems, evaluating the likelihood $L(y|\theta)$ is computationally prohibitive, making standard Markov Chain Monte Carlo (MCMC) methods intractable.

One elegant solution is the **delayed-acceptance Metropolis-Hastings** algorithm. This method employs a two-stage acceptance procedure. A proposal is first tested using a cheap, approximate likelihood $L_c(y|\theta)$, which defines a surrogate posterior $\tilde{\pi}(\theta|y)$. This pre-screening step is computationally fast and rejects poor proposals without ever evaluating the expensive, high-fidelity likelihood $L_f(y|\theta)$. Only proposals that pass this initial check are subsequently evaluated with the high-fidelity likelihood in a second correction step. To ensure that the resulting Markov chain correctly samples from the true posterior $\pi(\theta|y)$, the acceptance probabilities for the two stages must be carefully constructed to satisfy the detailed balance condition. The first stage acts as a standard Metropolis-Hastings step for the surrogate posterior, while the second stage provides a correction based on the ratio of the true and surrogate likelihoods [@problem_id:3405115].

These ideas extend to function-space MCMC methods, which are essential for [inverse problems](@entry_id:143129) where the unknown parameter is a field or function. The **preconditioned Crank-Nicolson (pCN)** algorithm is a popular choice in this setting, as its proposal mechanism is constructed to be independent of the dimension of the discretized [parameter space](@entry_id:178581). This leads to an [acceptance rate](@entry_id:636682) that does not degrade to zero as the mesh is refined. Multilevel [delayed acceptance](@entry_id:748288) can be integrated with pCN to create a powerful sampler for high-dimensional posteriors. The fine-level [acceptance probability](@entry_id:138494), which depends only on the change in the [log-likelihood](@entry_id:273783), can be factorized into a product of stage-wise acceptances corresponding to different discretization levels. This allows for a sequential testing procedure, from coarsest to finest, that enables early rejection of proposals and significantly reduces the average computational cost per MCMC step [@problem_id:3405052].

### Applications in State and Parameter Estimation for Dynamical Systems

Data assimilation for dynamical systems, such as those found in weather forecasting, [oceanography](@entry_id:149256), and epidemiology, represents another fertile ground for multilevel methods. Here, the challenge is to sequentially estimate the state of a system as a stream of noisy observations becomes available.

In the context of nonlinear, non-Gaussian systems, **Particle Filters** (or Sequential Monte Carlo methods) are a standard tool. A **Multilevel Particle Filter (MLPF)** accelerates this process by running coupled [particle filters](@entry_id:181468) on a hierarchy of time discretizations. For an MLPF to be effective, coupling must be introduced in two key places. First, the propagation of particles forward in time must be coupled, typically by using [common random numbers](@entry_id:636576) (CRN) to drive the stochastic differential equations at both the coarse and fine levels. Second, the [resampling](@entry_id:142583) step, which is necessary to combat [weight degeneracy](@entry_id:756689), must also be coupled. This is often achieved by using the same stream of uniform random numbers to select ancestor particles for both levels, thereby preserving the correlation between particle paths across the [resampling](@entry_id:142583) step [@problem_id:3405089].

For systems that are approximately linear and Gaussian, the **Ensemble Kalman Filter (EnKF)** is a computationally efficient alternative. A **Multilevel Ensemble Kalman Filter (MLEnKF)** can be constructed to reduce the cost associated with high-resolution forward models. Similar to the MLPF, coupling is essential. The forecast step, where each ensemble member is propagated forward in time, is coupled using CRN. Crucially, the analysis step, where the forecast is updated with the new observation, must also be coupled. This is achieved by using a shared set of perturbed observations for the ensemble updates at both the coarse and fine levels, ensuring that the random component of the Kalman update is correlated across levels [@problem_id:3405072].

A significant challenge in these applications arises when quantities of interest or the likelihood functions depend on the entire history of the state trajectory (i.e., they are path-dependent). In such cases, a [discretization error](@entry_id:147889) at an early time can propagate and its effect can accumulate, potentially causing the variance of the MLMC estimator to grow with the length of the time horizon, $T$. To ensure that the variance bounds are uniform in $T$, which is critical for long-time simulations, certain stability conditions must be met. These typically include requirements that the latent dynamics are mixing (e.g., geometrically ergodic or contractive), that the observation model is stable (e.g., the path likelihood is Lipschitz), and that the quantity of interest has a fading memory of the past [@problem_id:3405069] [@problem_id:3405116].

### Integrating Multilevel Methods with Advanced Numerical Solvers

The abstract notion of "fidelity levels" often corresponds directly to the discretization parameters of [numerical solvers](@entry_id:634411) for partial differential equations (PDEs), which are ubiquitous in science and engineering. Multilevel methods can be deeply integrated with the structure of these solvers to achieve even greater efficiency.

A powerful example is the synergy between MLMC and **[multigrid solvers](@entry_id:752283)**. A standard multigrid V-cycle involves smoothing operations on a fine grid, restriction of the residual to a coarse grid, solving the coarse-grid problem, and prolongation of the correction back to the fine grid, followed by more smoothing. In an MLMC context where we need to solve [linear systems](@entry_id:147850) at levels $\ell$ and $\ell-1$, we can couple the solvers. For instance, the solution from a coarse-grid V-cycle can be used as a highly effective initial guess for the fine-grid solver. Furthermore, by coordinating the smoothing steps, one can induce strong correlation between the algebraic errors of the two solvers. This solver-level coupling can reduce the variance of the MLMC difference estimator beyond what is achievable with CRN on the problem data alone. Advanced techniques may even use information from the coarse-grid solver, such as the residual, to construct a [control variate](@entry_id:146594) that corrects for algebraic error on the fine grid [@problem_id:3405099].

The multilevel concept can also be extended to handle multiple sources of [discretization error](@entry_id:147889) simultaneously. **Multi-Index Monte Carlo (MIMC)** generalizes MLMC to a multi-dimensional hierarchy. For instance, in a problem involving both [spatial discretization](@entry_id:172158) and stochastic quadrature, one can define a two-index hierarchy. The expectation is then expressed as a [telescoping sum](@entry_id:262349) over a two-dimensional grid of mixed differences. This approach is particularly useful for problems where different error sources have different convergence rates. An important application is in the context of Laplace approximations to posteriors, where one might need to compute the expected value of the [log-determinant](@entry_id:751430) of a Gauss-Newton Hessian matrix. Here, MIMC can be used to balance errors from the [spatial discretization](@entry_id:172158) of the forward model and the Monte Carlo or quasi-Monte Carlo approximation of the expectation over random parameters [@problem_id:3405111].

### Fidelity Hierarchies in High-Dimensional and Likelihood-Free Inference

Many modern [inverse problems](@entry_id:143129) are characterized by high-dimensional parameter spaces and the absence of a tractable likelihood function. Multifidelity methods provide innovative solutions in these challenging settings.

In high-dimensional Bayesian inverse problems, a key challenge is to identify the directions in the parameter space that are most informed by the data. **Likelihood-Informed Subspaces (LIS)** are low-dimensional subspaces that capture these essential directions. They are typically constructed by solving a [generalized eigenvalue problem](@entry_id:151614) involving the Gauss-Newton Hessian and the prior [precision matrix](@entry_id:264481). A low-fidelity model can then be defined by restricting the parameter variations to this subspace. This reduced-dimension model can be used as an effective and inexpensive [control variate](@entry_id:146594) within a multifidelity Monte Carlo scheme to accelerate the estimation of high-fidelity posterior expectations [@problem_id:3405126].

The concept of fidelity can also be tied to physical parameters of the [forward model](@entry_id:148443). In [inverse scattering problems](@entry_id:750808), for example, the data may consist of measurements at various frequencies. A natural fidelity hierarchy can be constructed by gradually incorporating higher-frequency (higher wavenumber) information. Low-fidelity models use only low-frequency data, which is computationally cheaper to simulate and reveals only the large-scale features of the scatterer. Higher-fidelity models incorporate progressively higher frequencies to resolve finer details. In a multilevel framework, the correction term at level $\ell$ corresponds to the information gained by adding the new frequency bands between levels $\ell-1$ and $\ell$. The variance of this correction term is directly related to this incremental [information gain](@entry_id:262008), providing a deep connection between the [statistical efficiency](@entry_id:164796) of the MLMC estimator and the physics of the problem [@problem_id:3405139].

Finally, multifidelity concepts are transforming **[likelihood-free inference](@entry_id:190479)** methods such as Approximate Bayesian Computation (ABC). In ABC, the likelihood is replaced by a criterion that accepts parameter proposals if the distance between simulated and observed [summary statistics](@entry_id:196779) is within a certain tolerance, $\epsilon$. The choice of $\epsilon$ introduces a bias. A multilevel approach can be designed by creating a hierarchy of tolerances, $\{\epsilon_\ell\}$, often linked to the [discretization error](@entry_id:147889) of the forward simulator. This allows for a progressive filtering of samples, starting with a large tolerance (and a cheap, coarse simulator) and proceeding to smaller tolerances (with more expensive, finer simulators), while reusing information across levels to maintain [computational efficiency](@entry_id:270255) [@problem_id:3405116].

In summary, the applications of multilevel and multifidelity Monte Carlo methods are as diverse as the computational challenges they address. By providing a systematic framework for combining information from models of varying cost and accuracy, these methods represent a cornerstone of modern computational science, enabling scalable and efficient uncertainty quantification in some of the most complex systems studied today.