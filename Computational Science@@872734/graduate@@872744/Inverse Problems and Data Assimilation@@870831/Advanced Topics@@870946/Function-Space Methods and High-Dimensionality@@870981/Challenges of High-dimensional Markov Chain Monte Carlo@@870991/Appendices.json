{"hands_on_practices": [{"introduction": "The \"curse of dimensionality\" in MCMC is not merely an empirical observation but a consequence rooted in the geometry of high-dimensional spaces. This exercise guides you through a foundational theoretical calculation to reveal this connection explicitly [@problem_id:3370964]. By analyzing the diffusion limit of the Random-Walk Metropolis algorithm, you will derive how the statistical efficiency, measured by the Effective Sample Size ($ESS$), scales with dimension, providing a first-principles understanding of this critical challenge.", "problem": "Consider a linear-Gaussian data assimilation problem in which the posterior distribution for the discretized state is a $d$-dimensional standard normal target $\\mathcal{N}(0, I_{d})$ after appropriate whitening through prior-preconditioning and observation operator linearization. You run a Random-Walk Metropolis (RWM) Markov Chain Monte Carlo (MCMC) algorithm with Gaussian proposals $y = x + \\sigma_{d} \\,\\xi$, where $\\xi \\sim \\mathcal{N}(0, I_{d})$ is independent of the current state $x$, and the proposal variance is scaled as $\\sigma_{d}^{2} = \\ell^{2}/d$ with a fixed $\\ell > 0$. Assume the chain is started in stationarity. Let $f(x) = v^{\\top} x$ be a scalar linear functional with $\\|v\\|_{2} = 1$.\n\nUse only foundational principles and well-tested facts as the starting point, including:\n- The Metropolis-Hastings acceptance probability with symmetric proposals.\n- The law of large numbers for high dimension $d$ under the above scaling.\n- The classical diffusion limit for Random-Walk Metropolis targeting an independent and identically distributed product measure, which yields, under the time-rescaling by $d$, an Ornstein–Uhlenbeck (OU) process limit for any fixed coordinate or fixed unit-direction projection.\n- The definition of Effective Sample Size (ESS): for a stationary scalar time series with lag-$k$ autocorrelation $\\rho_{k}$, the integrated autocorrelation time is $\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_{k}$ and the effective sample size for $n$ iterations is $\\mathrm{ESS}_{n} = n/\\tau_{\\mathrm{int}}$.\n\nStarting from these bases, derive the lag-$k$ autocorrelation of $f(x)$ and then compute $\\mathrm{ESS}_{n}$ as an explicit function of $n$, $d$, and $\\ell$. Your derivation should:\n- Determine the high-dimensional acceptance rate in terms of the standard normal cumulative distribution function $\\Phi(\\cdot)$.\n- Identify the OU limit generator coefficient $g(\\ell)$ from first principles by matching the infinitesimal drift and variance in the limit.\n- Use this to obtain a closed-form expression for the lag-$k$ autocorrelation $\\rho_{k}$ and then sum the resulting geometric series for $\\tau_{\\mathrm{int}}$.\n- Express the final $\\mathrm{ESS}_{n}(d,\\ell)$ in closed form using only $n$, $d$, $\\ell$, elementary functions, and $\\Phi(\\cdot)$.\n\nAnswer form: provide a single closed-form analytic expression for $\\mathrm{ESS}_{n}(d,\\ell)$. No numerical approximation or rounding is required. Use standard mathematical notation and define any special functions you introduce. Do not include physical units. Assume angles are dimensionless.", "solution": "The problem is rigorously defined, self-contained, and scientifically grounded in the established theory of Markov chain Monte Carlo (MCMC) methods and their diffusion limits in high dimensions. All provided data and definitions are standard within this field. The problem statement is valid.\n\nThe objective is to derive the Effective Sample Size, $\\mathrm{ESS}_{n}$, for a scalar linear functional $f(x) = v^{\\top} x$ of a state evolving under a Random-Walk Metropolis (RWM) algorithm. The target distribution is a $d$-dimensional standard normal, $\\pi(x) \\propto \\exp(-\\frac{1}{2} x^{\\top}x)$.\n\nStep 1: High-dimensional acceptance rate\nThe RWM proposal is $y = x + \\sigma_{d} \\xi$, where $\\xi \\sim \\mathcal{N}(0, I_{d})$ and the proposal variance is $\\sigma_{d}^{2} = \\ell^{2}/d$. Since the proposal distribution is symmetric, $q(y|x) = q(x|y)$, the Metropolis-Hastings acceptance probability is $\\alpha(x, y) = \\min\\left(1, \\frac{\\pi(y)}{\\pi(x)}\\right)$.\n\nThe ratio of target densities is:\n$$ \\frac{\\pi(y)}{\\pi(x)} = \\frac{\\exp(-\\frac{1}{2}y^{\\top}y)}{\\exp(-\\frac{1}{2}x^{\\top}x)} = \\exp\\left(-\\frac{1}{2}(y^{\\top}y - x^{\\top}x)\\right) $$\nSubstituting $y = x + \\sigma_d \\xi$:\n$$ y^{\\top}y = (x + \\sigma_d \\xi)^{\\top}(x + \\sigma_d \\xi) = x^{\\top}x + 2\\sigma_d x^{\\top}\\xi + \\sigma_d^2 \\xi^{\\top}\\xi $$\nThus, the argument of the exponential becomes:\n$$ \\log\\left(\\frac{\\pi(y)}{\\pi(x)}\\right) = -\\sigma_d x^{\\top}\\xi - \\frac{1}{2}\\sigma_d^2 \\xi^{\\top}\\xi $$\nWe analyze the behavior of the terms in the exponent in the limit $d \\rightarrow \\infty$. The chain is assumed to be in stationarity, so $x \\sim \\mathcal{N}(0, I_{d})$.\nThe second term is $\\frac{1}{2}\\sigma_d^2 \\xi^{\\top}\\xi = \\frac{1}{2} \\frac{\\ell^2}{d} \\sum_{i=1}^{d} \\xi_i^2$. By the Law of Large Numbers, as $d \\to \\infty$, $\\frac{1}{d}\\sum_{i=1}^{d} \\xi_i^2 \\to \\mathbb{E}[\\xi_1^2] = 1$. This term converges in probability to $\\frac{1}{2}\\ell^2$.\nThe first term is $\\sigma_d x^{\\top}\\xi = \\frac{\\ell}{\\sqrt{d}} \\sum_{i=1}^{d} x_i \\xi_i$. Since $x$ and $\\xi$ are independent and standard normal, the terms $x_i \\xi_i$ are independent and identically distributed with mean $\\mathbb{E}[x_i\\xi_i]=\\mathbb{E}[x_i]\\mathbb{E}[\\xi_i]=0$ and variance $\\mathrm{Var}(x_i\\xi_i) = \\mathbb{E}[(x_i\\xi_i)^2] - (\\mathbb{E}[x_i\\xi_i])^2 = \\mathbb{E}[x_i^2]\\mathbb{E}[\\xi_i^2] = 1 \\cdot 1 = 1$. By the Central Limit Theorem, the sum $\\frac{1}{\\sqrt{d}} \\sum_{i=1}^{d} x_i \\xi_i$ converges in distribution to a standard normal variable $U \\sim \\mathcal{N}(0, 1)$. Therefore, $\\sigma_d x^{\\top}\\xi$ converges in distribution to $\\mathcal{N}(0, \\ell^2)$. Let $Z \\sim \\mathcal{N}(0, \\ell^2)$.\n\nIn the high-dimensional limit, the acceptance probability becomes independent of the specific state $x$, and the average acceptance rate $\\bar{\\alpha}(\\ell)$ is given by the expectation over the limiting distribution of the log-ratio:\n$$ \\bar{\\alpha}(\\ell) = \\mathbb{E}_{Z}\\left[\\min\\left(1, \\exp\\left(-Z - \\frac{\\ell^2}{2}\\right)\\right)\\right] $$\nwhere $Z \\sim \\mathcal{N}(0, \\ell^2)$. Let $Z = \\ell U$ where $U \\sim \\mathcal{N}(0, 1)$. Let $\\phi(u)$ be the standard normal PDF.\n$$ \\bar{\\alpha}(\\ell) = \\int_{-\\infty}^{\\infty} \\min\\left(1, \\exp\\left(-\\ell u - \\frac{\\ell^2}{2}\\right)\\right) \\phi(u) du $$\nThe term $\\exp(-\\ell u - \\ell^2/2)$ is greater than $1$ when its argument is positive, i.e., $-\\ell u - \\ell^2/2 > 0$, which implies $u  -\\ell/2$. So we split the integral:\n$$ \\bar{\\alpha}(\\ell) = \\int_{-\\infty}^{-\\ell/2} 1 \\cdot \\phi(u) du + \\int_{-\\ell/2}^{\\infty} \\exp\\left(-\\ell u - \\frac{\\ell^2}{2}\\right) \\phi(u) du $$\nThe first integral is the definition of the standard normal cumulative distribution function (CDF), $\\Phi(-\\ell/2)$.\nFor the second integral, we substitute $\\phi(u) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-u^2/2)$ and complete the square in the exponent:\n$$ -\\ell u - \\frac{\\ell^2}{2} - \\frac{u^2}{2} = -\\frac{1}{2}(u^2 + 2\\ell u + \\ell^2) = -\\frac{1}{2}(u+\\ell)^2 $$\nThe second integral becomes $\\int_{-\\ell/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2}(u+\\ell)^2) du$. Let $v=u+\\ell$, then $dv=du$. The limits of integration change from $u=-\\ell/2 \\to v=\\ell/2$ and $u \\to \\infty \\to v \\to \\infty$.\n$$ \\int_{\\ell/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{v^2}{2}\\right) dv = P(V > \\ell/2) = 1 - \\Phi(\\ell/2) $$\nwhere $V \\sim \\mathcal{N}(0, 1)$. Due to the symmetry of the standard normal distribution, $1 - \\Phi(z) = \\Phi(-z)$, so this integral is $\\Phi(-\\ell/2)$.\nCombining both parts, the asymptotic acceptance rate is:\n$$ \\bar{\\alpha}(\\ell) = \\Phi(-\\ell/2) + \\Phi(-\\ell/2) = 2\\Phi(-\\ell/2) $$\n\nStep 2: Ornstein-Uhlenbeck (OU) limit and autocorrelation\nThe problem states that under the time rescaling $t=k/d$, the process for any fixed unit-direction projection converges to an OU process. Let $F_t = f(x_{\\lfloor dt \\rfloor}) = v^{\\top}x_{\\lfloor dt \\rfloor}$. Since the target distribution $\\mathcal{N}(0, I_d)$ is isotropic and $\\|v\\|_2=1$, we can choose a coordinate system where $v = (1, 0, ..., 0)^{\\top}$, so $F_t$ is simply the first coordinate of the state vector.\n\nThe limiting SDE for a coordinate process is of the form $dF_t = -\\theta F_t dt + \\sigma dW_t$. The stationary distribution of this process is $\\mathcal{N}(0, \\sigma^2/(2\\theta))$. We require this to match the stationary distribution of $f(x) = v^{\\top}x$, which is $\\mathcal{N}(0, v^{\\top}I_d v) = \\mathcal{N}(0,1)$. Thus, we must have $\\sigma^2=2\\theta$.\n\nThe diffusion coefficient $\\sigma^2$ is found by calculating the rescaled expected squared jump distance:\n$$ \\sigma^2 = \\lim_{d \\to \\infty} d \\cdot \\mathbb{E}[ (f(x_{k+1}) - f(x_k))^2 | x_k=x ] $$\nThe change is $f(x_{k+1}) - f(x_k) = v^{\\top}(x_{k+1}-x_k)$. The state update is $x_{k+1} = x_k + \\sigma_d \\xi \\cdot \\mathbb{I}(\\text{accept})$, where $\\mathbb{I}$ is an indicator for acceptance.\n$$ \\mathbb{E}[(v^{\\top}(\\sigma_d \\xi \\cdot \\mathbb{I}(\\text{accept})))^2 | x] = \\sigma_d^2 \\mathbb{E}[(v^{\\top}\\xi)^2 \\alpha(x, x+\\sigma_d \\xi) | x] $$\nIn the limit $d \\to \\infty$, $\\alpha$ converges to the constant $\\bar{\\alpha}(\\ell)$. Thus:\n$$ \\sigma^2 \\approx d \\cdot \\sigma_d^2 \\mathbb{E}[(v^{\\top}\\xi)^2] \\bar{\\alpha}(\\ell) = d \\cdot \\frac{\\ell^2}{d} \\cdot \\mathrm{Var}(v^{\\top}\\xi) \\cdot \\bar{\\alpha}(\\ell) $$\nSince $\\xi \\sim \\mathcal{N}(0, I_d)$ and $\\|v\\|_2=1$, $\\mathrm{Var}(v^{\\top}\\xi) = v^{\\top}\\mathrm{Var}(\\xi)v = v^{\\top}I_d v = \\|v\\|_2^2=1$.\nTherefore, $\\sigma^2 = \\ell^2 \\bar{\\alpha}(\\ell) = 2\\ell^2 \\Phi(-\\ell/2)$.\nFrom the stationarity condition $\\sigma^2 = 2\\theta$, the drift coefficient is $\\theta = \\frac{1}{2}\\sigma^2 = \\ell^2 \\Phi(-\\ell/2)$.\nThis corresponds to the coefficient $g(\\ell)$ in the problem's implicit framing. The SDE is $dF_t = -(\\ell^2 \\Phi(-\\ell/2)) F_t dt + \\sqrt{2\\ell^2 \\Phi(-\\ell/2)} dW_t$.\n\nThe autocorrelation function for an OU process with drift coefficient $\\theta$ is $\\rho(\\tau) = \\exp(-\\theta \\tau)$. For the discrete MCMC chain, the time lag $\\tau$ corresponds to $k/d$ steps. The lag-$k$ autocorrelation is:\n$$ \\rho_k = \\rho(k/d) = \\exp(-\\theta k/d) = \\exp\\left(-k \\frac{\\ell^2 \\Phi(-\\ell/2)}{d}\\right) $$\n\nStep 3: Integrated Autocorrelation Time ($\\tau_{\\mathrm{int}}$) and ESS\nThe integrated autocorrelation time is defined as $\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_{k}$. The sum is a geometric series. Let $r = \\rho_1 = \\exp\\left(-\\frac{\\ell^2 \\Phi(-\\ell/2)}{d}\\right)$.\n$$ \\sum_{k=1}^{\\infty} \\rho_k = \\sum_{k=1}^{\\infty} r^k = \\frac{r}{1-r} $$\nSubstituting this into the expression for $\\tau_{\\mathrm{int}}$:\n$$ \\tau_{\\mathrm{int}} = 1 + 2\\frac{r}{1-r} = \\frac{(1-r) + 2r}{1-r} = \\frac{1+r}{1-r} $$\nSubstituting the expression for $r$:\n$$ \\tau_{\\mathrm{int}} = \\frac{1 + \\exp\\left(-\\frac{\\ell^2 \\Phi(-\\ell/2)}{d}\\right)}{1 - \\exp\\left(-\\frac{\\ell^2 \\Phi(-\\ell/2)}{d}\\right)} $$\nThis expression can be written using the hyperbolic cotangent function, $\\coth(z) = \\frac{\\exp(z)+\\exp(-z)}{\\exp(z)-\\exp(-z)} = \\frac{\\exp(2z)+1}{\\exp(2z)-1}$. Let $z = \\frac{\\ell^2 \\Phi(-\\ell/2)}{2d}$. Then $\\tau_{\\mathrm{int}} = \\coth(z)$.\n$$ \\tau_{\\mathrm{int}} = \\coth\\left(\\frac{\\ell^2 \\Phi(-\\ell/2)}{2d}\\right) $$\nThe Effective Sample Size for $n$ iterations is $\\mathrm{ESS}_n = n/\\tau_{\\mathrm{int}}$.\n$$ \\mathrm{ESS}_{n}(d,\\ell) = \\frac{n}{\\coth\\left(\\frac{\\ell^2 \\Phi(-\\ell/2)}{2d}\\right)} = n \\tanh\\left(\\frac{\\ell^2 \\Phi(-\\ell/2)}{2d}\\right) $$\nThis provides the final closed-form expression for $\\mathrm{ESS}_{n}$ as a function of $n$, $d$, and $\\ell$, using elementary functions and the standard normal CDF $\\Phi$.", "answer": "$$\n\\boxed{n \\tanh\\left(\\frac{\\ell^{2} \\Phi(-\\ell/2)}{2d}\\right)}\n$$", "id": "3370964"}, {"introduction": "Understanding that MCMC efficiency degrades in high dimensions raises a crucial practical question: how do we reliably diagnose sampler performance? Simple visual checks like trace plots can be profoundly misleading, often appearing well-mixed even when the chain is failing to explore critical regions of the state space [@problem_id:3370978]. This practice challenges you to reason about why low-dimensional projections fail and to justify the superiority of observable-specific diagnostics, such as the Integrated Autocorrelation Time (IACT), which provide a quantitative and robust measure of mixing for the specific quantities you care about.", "problem": "You are analyzing Markov chain Monte Carlo (MCMC) for a Bayesian linear inverse problem in $d$ dimensions with a Gaussian prior and a linear observation operator. After whitening, suppose the posterior in the whitened coordinates is close to a product Gaussian on $\\mathbb{R}^d$. You run a Random Walk Metropolis (RWM) algorithm with Gaussian proposal increments scaled as $l/\\sqrt{d}$ for some constant $l0$. Let $f:\\mathbb{R}^d\\to\\mathbb{R}$ denote a scalar quantity of interest (QoI), for example the data misfit, a posterior predictive at a sensor, or a linear functional of the state, and let $X_t$ denote the stationary Markov chain. The integrated autocorrelation time (IACT) of $f(X_t)$ is defined by\n$$\n\\tau_{\\text{int}}[f] \\;=\\; 1 + 2\\sum_{k=1}^{\\infty} \\rho_f(k),\n$$\nwhere $\\rho_f(k)$ is the lag-$k$ autocorrelation of the stationary time series $\\{f(X_t)\\}_{t\\ge 0}$. The effective sample size (ESS) for $N$ draws is $N/\\tau_{\\text{int}}[f]$. A wall-clock, cost-normalized efficiency metric can be defined as $N/(\\tau_{\\text{int}}[f]\\cdot T)$, where $T$ is the total computational time.\n\nUsing only fundamental definitions of IACT and ESS and well-tested facts about optimal scaling of RWM on approximately product targets (e.g., that a proposal variance scaled as $O(1/d)$ with acceptance rate of order $O(1)$ maximizes the expected squared jump distance), reason about how sensitivity to the dimension $d$ enters the mixing of $f(X_t)$. Then choose the option that proposes an observable-specific diagnostic based on $\\tau_{\\text{int}}$ that remains informative as $d$ grows, and that correctly justifies why it is superior to naive trace plots for high-dimensional chains by analyzing its sensitivity to $d$.\n\nA. For each scientifically relevant scalar observable $f$, compute a consistent spectral estimator of $\\tau_{\\text{int}}[f]$ (equivalently, the spectral density at frequency $0$ scaled by the marginal variance of $f$), and report the cost-normalized efficiency $N/(\\tau_{\\text{int}}[f]\\cdot T)$. Under RWM with proposal variance $l^2/d$ targeting an approximately product Gaussian, $\\tau_{\\text{int}}[f]$ for typical linear functionals grows on the order of $d$ because the expected squared jump in $f$ scales like $O(1/d)$, so the reported efficiency decays like $O(1/d)$. This diagnostic is superior to trace plots of coordinates because trace plots are projections that can look deceptively “noisy” regardless of the correlation length, do not quantify the correlation time, and can entirely miss slow directions invisible in the plotted coordinates as $d$ increases.\n\nB. Monitor only the overall acceptance rate. Because the optimally tuned acceptance rate of RWM is approximately $0.234$ and does not change with $d$ under product targets, it is dimension-robust and therefore superior to trace plots and any $\\tau_{\\text{int}}$-based diagnostic.\n\nC. Compute the Gelman–Rubin potential scale reduction factor (PSRF) for the first coordinate alone across a few parallel chains. Because all coordinates are exchangeable under the near-product posterior, the first coordinate reflects the global mixing properties, and thus this scalar PSRF is both dimension-robust and superior to any $\\tau_{\\text{int}}$-based diagnostic.\n\nD. Plot the trace of the squared norm $\\|X_t\\|^2$. Since $\\operatorname{Var}(\\|X_t\\|^2)$ grows with $d$ under a product Gaussian, such a trace is increasingly sensitive to $d$ and therefore superior to an observable-specific $\\tau_{\\text{int}}$-based diagnostic.\n\nE. Define a “state-wide” IACT by summing the univariate autocorrelations of all $d$ coordinates and dividing by $d$, and use this average as a dimension-invariant ESS for the full state. Because the average is normalized by $d$, it removes the $d$-dependence and is superior to observable-specific $\\tau_{\\text{int}}$ estimates.\n\nSelect the correct option(s).", "solution": "The user has provided a problem concerning the diagnosis of Markov chain Monte Carlo (MCMC) sampler performance in a high-dimensional Bayesian inverse problem. The task is to validate the problem statement and, if valid, identify the best diagnostic strategy among the given options by analyzing the scaling of sampler performance with the dimension $d$.\n\n### Problem Validation\n\nThe problem statement must first be validated for scientific soundness, clarity, and completeness.\n\n**Step 1: Extract Givens**\n\n-   **Context**: A Bayesian linear inverse problem in $d$ dimensions.\n-   **Model**: The posterior distribution in whitened coordinates is approximately a product Gaussian on $\\mathbb{R}^d$.\n-   **Algorithm**: Random Walk Metropolis (RWM) with Gaussian proposals $Y_t = X_t + \\xi_t$, where $\\xi_t \\sim \\mathcal{N}(0, (l^2/d)I_d)$ for a constant $l > 0$.\n-   **Observable**: A scalar quantity of interest (QoI), $f:\\mathbb{R}^d\\to\\mathbb{R}$.\n-   **Definitions**:\n    -   Integrated Autocorrelation Time (IACT): $\\tau_{\\text{int}}[f] = 1 + 2\\sum_{k=1}^{\\infty} \\rho_f(k)$, where $\\rho_f(k)$ is the lag-$k$ autocorrelation of the stationary time series $\\{f(X_t)\\}_{t\\ge 0}$.\n    -   Effective Sample Size (ESS): $N/\\tau_{\\text{int}}[f]$.\n    -   Cost-normalized Efficiency: $N/(\\tau_{\\text{int}}[f]\\cdot T)$, where $T$ is total computational time.\n-   **Premise**: The analysis should use the fact that proposal variance scaling as $O(1/d)$ yields an $O(1)$ acceptance rate and maximizes expected squared jump distance in the high-$d$ limit for product-like targets.\n-   **Objective**: Identify an observable-specific diagnostic based on $\\tau_{\\text{int}}$ that is informative for large $d$ and justify its superiority to naive methods by analyzing its dimensional sensitivity.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientific Grounding**: The problem setup is a canonical model for studying high-dimensional MCMC, established in seminal works (e.g., Roberts, Gelman,  Gilks, 1997; Roberts  Rosenthal, 2001). The assumptions—a near-product Gaussian posterior, RWM with $O(1/d)$ proposal variance—are standard for theoretical analysis. The definitions of IACT and ESS are correct. The stated premise about optimal scaling is a cornerstone of modern MCMC theory. The problem is scientifically and mathematically sound.\n2.  **Well-Posedness**: The question is well-posed. It asks for a conceptual analysis of diagnostic tools based on established scaling laws for MCMC in high dimensions. A unique, meaningful answer can be derived from the theory.\n3.  **Objectivity**: The language is precise and objective.\n4.  **Other Flaws**: The problem is not incomplete, contradictory, unrealistic, ill-posed, or trivial. It addresses a critical and non-trivial challenge in computational science.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. The analysis can proceed.\n\n### Derivation and Option Analysis\n\nThe core of the problem is to understand how the mixing time of an observable $f(X_t)$ behaves as the dimension $d \\to \\infty$. Let the target distribution be the standard $d$-dimensional Gaussian, $\\pi(x) \\propto \\exp(-\\|x\\|^2/2)$, which is a product of $d$ one-dimensional standard Gaussians. This is the idealized version of the \"approximately product Gaussian\" posterior.\n\n**Scaling of IACT with Dimension**\n\nThe IACT is inversely related to the rate at which the chain explores the state space, as measured by the observable $f$. A useful approximation, especially in the diffusion limit that RWM approaches for large $d$, is $\\tau_{\\text{int}}[f] \\approx \\frac{2\\operatorname{Var}_{\\pi}(f)}{\\mathbb{E}_{\\pi}[(f(X_{t+1}) - f(X_{t}))^2]}$. We must analyze the scaling of the numerator and the denominator.\n\nLet us consider a \"typical\" linear observable, $f(x) = c \\cdot x = \\sum_{i=1}^d c_i x_i$, where $c \\in \\mathbb{R}^d$ is a fixed vector, which we can normalize to $\\|c\\|=1$ without loss of generality.\n\n1.  **Numerator: Variance of the observable**: The variance of $f(X)$ under the stationary distribution $\\pi$ is $\\operatorname{Var}_{\\pi}(f) = \\operatorname{Var}_{\\pi}(c \\cdot X)$. Since $X \\sim \\mathcal{N}(0, I_d)$, the components $X_i$ are independent with variance $1$. Thus, $\\operatorname{Var}_{\\pi}(c \\cdot X) = c^T \\operatorname{Cov}(X) c = c^T I_d c = \\|c\\|^2 = 1$. The variance is $O(1)$ with respect to $d$.\n\n2.  **Denominator: Expected Squared Jump Distance (ESJD) of the observable**: The change in the chain's state is $X_{t+1} - X_t$, which is the proposed step $\\xi_t \\sim \\mathcal{N}(0, (l^2/d)I_d)$ if the move is accepted, and $0$ otherwise. The acceptance probability $\\alpha(X_t, X_t+\\xi_t)$, in the limit $d \\to \\infty$, converges to a constant $\\bar{\\alpha} > 0$ when $l$ is chosen appropriately.\n    The jump in the observable is $\\Delta f = f(X_{t+1}) - f(X_t) = c \\cdot (X_{t+1} - X_t)$.\n    The expected squared jump is $\\mathbb{E}[(\\Delta f)^2] = \\mathbb{E}[\\alpha(X, X+\\xi) (c \\cdot \\xi)^2]$. In the high-$d$ limit, this is approximately $\\bar{\\alpha} \\mathbb{E}[(c \\cdot \\xi)^2]$.\n    The random variable $c \\cdot \\xi$ is a linear combination of the components of $\\xi$. Its variance is $\\operatorname{Var}(c \\cdot \\xi) = c^T \\operatorname{Cov}(\\xi) c = c^T (l^2/d)I_d c = (l^2/d)\\|c\\|^2 = l^2/d$.\n    Since $\\mathbb{E}[c \\cdot \\xi] = 0$, we have $\\mathbb{E}[(c \\cdot \\xi)^2] = \\operatorname{Var}(c \\cdot \\xi) = l^2/d$.\n    Therefore, the ESJD for the observable $f$ scales as $\\mathbb{E}[(\\Delta f)^2] \\approx \\bar{\\alpha} \\cdot (l^2/d) = O(1/d)$.\n\n3.  **IACT Scaling**: Combining these results, we find the scaling of the IACT:\n    $$\n    \\tau_{\\text{int}}[f] \\sim \\frac{\\operatorname{Var}_{\\pi}(f)}{\\mathbb{E}[(\\Delta f)^2]} \\sim \\frac{O(1)}{O(1/d)} = O(d).\n    $$\n    For a typical linear observable, the IACT grows linearly with dimension. This means that to obtain one \"independent\" sample of $f$, one needs to run the chain for $O(d)$ steps.\n\nNow, we evaluate each option based on this understanding.\n\n**A. For each scientifically relevant scalar observable $f$, compute a consistent spectral estimator of $\\tau_{\\text{int}}[f]$ (equivalently, the spectral density at frequency $0$ scaled by the marginal variance of $f$), and report the cost-normalized efficiency $N/(\\tau_{\\text{int}}[f]\\cdot T)$. Under RWM with proposal variance $l^2/d$ targeting an approximately product Gaussian, $\\tau_{\\text{int}}[f]$ for typical linear functionals grows on the order of $d$ because the expected squared jump in $f$ scales like $O(1/d)$, so the reported efficiency decays like $O(1/d)$. This diagnostic is superior to trace plots of coordinates because trace plots are projections that can look deceptively “noisy” regardless of the correlation length, do not quantify the correlation time, and can entirely miss slow directions invisible in the plotted coordinates as $d$ increases.**\n\n-   **Proposed Diagnostic**: Compute $\\tau_{\\text{int}}[f]$ for each relevant observable $f$. This is the correct, modern approach. MCMC performance is observable-dependent.\n-   **Analysis of $\\tau_{\\text{int}}$ Scaling**: The option correctly states that $\\tau_{\\text{int}}[f]$ grows as $O(d)$ and provides the correct reason: the ESJD for $f$ scales as $O(1/d)$. This matches our derivation.\n-   **Analysis of Efficiency**: The option claims the efficiency decays as $O(1/d)$. The efficiency is $1/(\\tau_{\\text{int}}[f] \\cdot T_1)$, where $T_1$ is the cost per sample. The cost to evaluate the log-posterior ratio for RWM is at least $O(d)$ (to compute dot products like $x \\cdot \\xi$). Thus, $T_1 = O(d)$. The full computational efficiency is $1/(O(d) \\cdot O(d)) = O(d^{-2})$. The option's claim of $O(1/d)$ corresponds to statistical efficiency, $1/\\tau_{\\text{int}}[f]$, which ignores the computational cost per step. This is a common abuse of terminology in theoretical literature. Given that the rest of the statement is perfectly sound, this is likely the intended meaning.\n-   **Superiority to Trace Plots**: The justification is excellent. Trace plots of single coordinates are low-dimensional projections that fail to capture collective, slow-mixing modes in high dimensions. They do not provide a quantitative measure of mixing time and can be highly misleading.\n-   **Verdict**: This option correctly identifies the central issue, proposes the correct diagnostic tool ($\\tau_{\\text{int}}[f]$), correctly analyzes its scaling with dimension, and correctly explains its superiority over naive methods. The slight imprecision on \"cost-normalized efficiency\" is minor compared to the fundamental correctness of the proposal. **Correct**.\n\n**B. Monitor only the overall acceptance rate. Because the optimally tuned acceptance rate of RWM is approximately $0.234$ and does not change with $d$ under product targets, it is dimension-robust and therefore superior to trace plots and any $\\tau_{\\text{int}}$-based diagnostic.**\n\n-   **Analysis**: While it is true that an $O(1)$ acceptance rate is a necessary condition for an efficient RWM sampler in high dimensions, it is not a sufficient diagnostic of performance. The acceptance rate tells us that the proposal steps are reasonably sized, but it does not tell us how long it takes for the chain to decorrelate. As derived above, even with an optimal acceptance rate, the IACT still scales as $O(d)$. An acceptance rate of $0.234$ is consistent with arbitrarily slow mixing. It is a tuning parameter, not a performance measure.\n-   **Verdict**: **Incorrect**.\n\n**C. Compute the Gelman–Rubin potential scale reduction factor (PSRF) for the first coordinate alone across a few parallel chains. Because all coordinates are exchangeable under the near-product posterior, the first coordinate reflects the global mixing properties, and thus this scalar PSRF is both dimension-robust and superior to any $\\tau_{\\text{int}}$-based diagnostic.**\n\n-   **Analysis**: This suffers from the same fundamental flaw as using trace plots. Monitoring a single coordinate (or any fixed, small subset) is a low-dimensional projection. While the coordinates are exchangeable in the idealized case, a slow mixing mode may be a direction in $\\mathbb{R}^d$ (e.g., $v = (1/\\sqrt{d}, \\dots, 1/\\sqrt{d})$) that has only a small projection onto any single coordinate axis. The PSRF for $x_1$ could indicate convergence ($\\hat{R} \\approx 1$) while the chain has failed to explore the posterior along the direction $v$. An observable-specific diagnostic, such as $\\tau_{\\text{int}}[v \\cdot x]$, is required to detect such issues.\n-   **Verdict**: **Incorrect**.\n\n**D. Plot the trace of the squared norm $\\|X_t\\|^2$. Since $\\operatorname{Var}(\\|X_t\\|^2)$ grows with $d$ under a product Gaussian, such a trace is increasingly sensitive to $d$ and therefore superior to an observable-specific $\\tau_{\\text{int}}$-based diagnostic.**\n\n-   **Analysis**: Plotting the trace of $\\|X_t\\|^2$ is simply monitoring one specific observable, $f(x) = \\|x\\|^2$. It is an example of an observable-specific approach, not a superior alternative to it. The quantitative way to assess its mixing would be to compute $\\tau_{\\text{int}}[\\|X_t\\|^2]$. The claim of superiority is thus logically incoherent. While $\\|X_t\\|^2$ can be a useful diagnostic, the reasoning provided is flawed, and it is not a replacement for a general strategy of examining all relevant observables.\n-   **Verdict**: **Incorrect**.\n\n**E. Define a “state-wide” IACT by summing the univariate autocorrelations of all $d$ coordinates and dividing by $d$, and use this average as a dimension-invariant ESS for the full state. Because the average is normalized by $d$, it removes the $d$-dependence and is superior to observable-specific $\\tau_{\\text{int}}$ estimates.**\n\n-   **Analysis**: The proposed diagnostic is an average of the coordinate-wise IACTs, $\\bar{\\tau} = \\frac{1}{d}\\sum_{i=1}^d \\tau_{\\text{int}}[x_i]$. For the product Gaussian target, the coordinates are i.i.d. by symmetry, so $\\tau_{\\text{int}}[x_i]$ is the same for all $i$. Let this be $\\tau_{\\text{coord}}$. As a linear functional, its IACT scales as $\\tau_{\\text{coord}} = O(d)$.\n    The average is then $\\bar{\\tau} = \\frac{1}{d} (d \\cdot \\tau_{\\text{coord}}) = \\tau_{\\text{coord}} = O(d)$.\n    The claim that \"it removes the $d$-dependence\" is factually false. The average IACT still grows linearly with $d$. Furthermore, like other averaging or single-component methods, this can mask slow mixing in specific directions that are not aligned with the coordinate axes.\n-   **Verdict**: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "3370978"}, {"introduction": "Armed with a theoretical understanding of the challenge and robust diagnostic tools, the next step is to engineer more efficient samplers. This hands-on coding practice delves into blocked sampling, a powerful strategy for navigating the trade-offs inherent in high-dimensional exploration [@problem_id:3371022]. You will implement a blocked Random-Walk Metropolis algorithm and quantitatively determine the optimal block size by maximizing a cost-normalized measure of effective sample size, gaining direct experience in the practical art of MCMC algorithm design and optimization.", "problem": "Consider a linear Gaussian data assimilation setting in which the latent state vector $x \\in \\mathbb{R}^{d}$ is endowed with an autoregressive of order one prior modeled as a Gaussian Markov Random Field (GMRF), and noisy direct observations are available. Specifically, assume the following well-tested and widely used model components:\n\n- Prior: The state $x$ follows the autoregressive of order one (AR(1)) GMRF\n$$\nx_{1} \\sim \\mathcal{N}\\!\\left(0,\\ \\frac{\\sigma^{2}}{1-\\phi^{2}}\\right), \\quad x_{i+1}\\mid x_{i} \\sim \\mathcal{N}\\!\\left(\\phi x_{i},\\ \\sigma^{2}\\right) \\quad \\text{for } i = 1,\\dots,d-1,\n$$\nwith parameters $\\phi \\in (0,1)$ and $\\sigma  0$. The negative log-prior is a quadratic form with tridiagonal precision matrix $Q_{\\text{prior}}$ having diagonal entries and off-diagonal entries given by the standard AR(1) GMRF construction.\n\n- Likelihood: The observation operator is the identity, and noisy observations are given by\n$$\ny \\mid x \\sim \\mathcal{N}\\!\\left(x,\\ \\sigma_{y}^{2} I_{d}\\right),\n$$\nwhere $I_{d}$ denotes the $d \\times d$ identity matrix and $\\sigma_{y}  0$.\n\nBy Bayes’ rule, the posterior $p(x \\mid y)$ is Gaussian with precision $Q = Q_{\\text{prior}} + \\frac{1}{\\sigma_{y}^{2}} I_{d}$ and linear term $b = \\frac{1}{\\sigma_{y}^{2}} y$, so that its negative log-density (up to an additive constant) is\n$$\n\\mathcal{E}(x) = \\frac{1}{2} x^{\\top} Q x - b^{\\top} x.\n$$\n\nYou will implement a blocked Random-Walk Metropolis (RWM) kernel within Markov chain Monte Carlo (MCMC) to target $p(x \\mid y)$, following the principle of designing block proposals that update contiguous subsets of state dimensions conditioned on the complement. The proposal selects a contiguous block of $k$ coordinates uniformly at random and proposes\n$$\nx'_{B} = x_{B} + S_{B} z, \\quad z \\sim \\mathcal{N}(0, I_{k}),\n$$\nwhere $B$ is the chosen block index set, $S_{B}$ is a diagonal scaling matrix with entries $s_{i}$ given by a preconditioner based on the local curvature (use $s_{i} = \\frac{c}{\\sqrt{Q_{ii}}}$ where $c = \\frac{2.38}{\\sqrt{k}}$ is a dimension-aware scaling constant supported by high-dimensional diffusion limit theory for RWM), and $Q_{ii}$ denotes the $i$-th diagonal element of $Q$. Given the symmetric proposal, the Metropolis acceptance probability is\n$$\n\\alpha(x, x') = \\min\\left(1,\\ \\exp\\left(- \\left[\\mathcal{E}(x') - \\mathcal{E}(x)\\right]\\right)\\right).\n$$\nThanks to the tridiagonal structure of $Q$, the energy difference $\\mathcal{E}(x') - \\mathcal{E}(x)$ for a block update can be computed in $\\mathcal{O}(k)$ by summing only local contributions over the block and its immediate neighbors.\n\nDefine the degeneracy of the chain qualitatively as poor mixing due to strong correlations or overly large block dimension $k$. To quantify mixing, use the Effective Sample Size (ESS) computed from the autocorrelation of the scalar time series formed by the central coordinate $x_{\\lfloor d/2 \\rfloor}$ of the chain. Estimate the integrated autocorrelation time using the initial positive sequence estimator and set\n$$\n\\text{ESS} = \\frac{N}{\\tau_{\\text{int}}},\n$$\nwhere $N$ is the number of MCMC iterations. To account for computational cost, define the ESS per unit cost as\n$$\n\\text{ESS-per-cost}(k) = \\frac{\\text{ESS}(k)}{k},\n$$\nreflecting that the cost of a block update scales linearly with block size $k$ under local computations.\n\nYour tasks:\n\n1. Construct $Q_{\\text{prior}}$ for the AR(1) GMRF and $Q = Q_{\\text{prior}} + \\frac{1}{\\sigma_{y}^{2}} I_{d}$. Construct $b = \\frac{1}{\\sigma_{y}^{2}} y$.\n2. Generate synthetic observations $y$ by first simulating a latent truth $x^{\\star}$ following the AR(1) prior and then sampling $y = x^{\\star} + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{y}^{2} I_{d})$.\n3. Initialize the chain at the posterior mean $m$, obtained by solving $Q m = b$.\n4. For a set of block sizes $k \\in \\{1, 2, 4, 8, \\dots\\}$ up to $k = d$, run the blocked RWM chain for $N$ iterations per $k$. At each iteration, choose a contiguous block uniformly at random, propose and accept/reject as above, and record the time series of the central coordinate $x_{\\lfloor d/2 \\rfloor}$.\n5. For each $k$, estimate ESS from the recorded series and compute $\\text{ESS-per-cost}(k)$. Determine the optimal block size $k^{\\star}$ that maximizes $\\text{ESS-per-cost}(k)$, and report the optimal block fraction\n$$\n\\alpha = \\frac{k^{\\star}}{d}\n$$\nrounded to three decimals.\n\nTest Suite:\n\nRun your program for the following parameter sets. In each case, use the provided random seed to ensure reproducibility.\n\n- Case 1 (happy path): $(d, \\phi, \\sigma, \\sigma_{y}, N, \\text{seed}) = (64, 0.90, 1.0, 0.5, 3000, 12345)$.\n- Case 2 (high correlation edge): $(d, \\phi, \\sigma, \\sigma_{y}, N, \\text{seed}) = (64, 0.98, 1.0, 0.5, 3000, 23456)$.\n- Case 3 (strong observation, smaller dimension): $(d, \\phi, \\sigma, \\sigma_{y}, N, \\text{seed}) = (32, 0.70, 1.0, 0.3, 4000, 34567)$.\n- Case 4 (larger dimension, moderate observation noise): $(d, \\phi, \\sigma, \\sigma_{y}, N, \\text{seed}) = (128, 0.95, 1.0, 1.0, 3000, 45678)$.\n\nFinal Output Format:\n\nYour program should produce a single line of output containing the optimal block fraction $\\alpha$ for each test case, as a comma-separated list enclosed in square brackets, each entry rounded to three decimals (e.g., $[0.125,0.250,0.500,0.750]$). No other text should be printed.", "solution": "The user has provided a problem that requires the implementation and evaluation of a blocked Random-Walk Metropolis (RWM) algorithm for a specific Bayesian inference task. The problem is to identify the optimal block size for the sampler by maximizing a cost-normalized metric of sampling efficiency.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Model:** A linear Gaussian state-space model for a latent vector $x \\in \\mathbb{R}^{d}$.\n- **Prior:** An autoregressive model of order one (AR(1)) as a Gaussian Markov Random Field (GMRF). The distribution is specified by $x_{1} \\sim \\mathcal{N}\\!\\left(0,\\ \\frac{\\sigma^{2}}{1-\\phi^{2}}\\right)$ and $x_{i+1}\\mid x_{i} \\sim \\mathcal{N}\\!\\left(\\phi x_{i},\\ \\sigma^{2}\\right)$ for $i=1,\\dots,d-1$, with parameters $\\phi \\in (0,1)$ and $\\sigma > 0$. The negative log-prior corresponds to a quadratic form $\\frac{1}{2} x^{\\top} Q_{\\text{prior}} x$ with a tridiagonal precision matrix $Q_{\\text{prior}}$.\n- **Likelihood:** Direct, noisy observations $y \\mid x \\sim \\mathcal{N}\\!\\left(x,\\ \\sigma_{y}^{2} I_{d}\\right)$, where $\\sigma_y > 0$.\n- **Posterior:** The posterior $p(x \\mid y)$ is a Gaussian distribution. Its negative log-density, up to a constant, is $\\mathcal{E}(x) = \\frac{1}{2} x^{\\top} Q x - b^{\\top} x$, where the precision is $Q = Q_{\\text{prior}} + \\frac{1}{\\sigma_{y}^{2}} I_{d}$ and the linear term is $b = \\frac{1}{\\sigma_{y}^{2}} y$.\n- **MCMC Sampler:** A blocked RWM sampler.\n    - Proposal for a contiguous block $B$ of size $k$: $x'_{B} = x_{B} + S_{B} z$, where $z \\sim \\mathcal{N}(0, I_{k})$.\n    - Proposal scaling matrix $S_B$ is diagonal with entries $s_{i} = c/\\sqrt{Q_{ii}}$, where $c = 2.38/\\sqrt{k}$.\n    - Acceptance probability: $\\alpha(x, x') = \\min\\left(1,\\ \\exp\\left(- \\left[\\mathcal{E}(x') - \\mathcal{E}(x)\\right]\\right)\\right)$.\n    - The energy difference calculation is noted to be efficient ($\\mathcal{O}(k)$) due to the tridiagonal structure of $Q$.\n- **Performance Metric:**\n    - The effective sample size (ESS) of the central coordinate's time series, $x_{\\lfloor d/2 \\rfloor}$.\n    - $\\text{ESS} = N/\\tau_{\\text{int}}$, where $N$ is the number of iterations.\n    - Integrated autocorrelation time $\\tau_{\\text{int}}$ is estimated via the initial positive sequence estimator: find the first lag $T$ such that $\\hat{\\rho}(T) + \\hat{\\rho}(T+1)  0$, and then compute $\\tau_{\\text{int}} = 1 + 2 \\sum_{t=1}^{T-1} \\hat{\\rho}(t)$.\n    - The final objective function is the ESS per unit cost: $\\text{ESS-per-cost}(k) = \\text{ESS}(k)/k$.\n- **Objective:** Find the optimal block size $k^{\\star}$ that maximizes $\\text{ESS-per-cost}(k)$ and report the optimal block fraction $\\alpha = k^{\\star}/d$.\n- **Test Cases:** Four sets of parameters $(d, \\phi, \\sigma, \\sigma_{y}, N, \\text{seed})$ are provided.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientific Grounding:** The problem is set within the well-established framework of Bayesian inference for state-space models, a staple of inverse problems and data assimilation. The AR(1) prior, Gaussian likelihood, and resulting Gaussian posterior are standard textbook models. The blocked RWM sampler is a classical MCMC algorithm, and its preconditioning with local curvature information ($1/\\sqrt{Q_{ii}}$) is a theoretically motivated technique. The use of ESS as a measure of MCMC efficiency is a fundamental concept in computational statistics. The problem is scientifically and mathematically sound.\n- **Well-Posedness:** The problem specifies a clear objective function, $\\text{ESS-per-cost}(k)$, to be maximized over a defined set of candidate block sizes $k$. All necessary inputs and model definitions are provided. A unique optimal block size $k^{\\star}$ within the tested set is guaranteed to exist. The problem is well-posed.\n- **Objectivity, Completeness, and Consistency:** The problem is described using precise, objective mathematical notation. All parameters are defined and their values provided for each test case. There are no contradictions, ambiguities, or missing pieces of critical information.\n- **Feasibility:** The dimensions $d$ and number of iterations $N$ are computationally manageable. The specified algorithm and analysis are standard and can be implemented with common numerical libraries.\n\n**Step 3: Verdict and Action**\n\nThe problem is deemed **valid**. It represents a well-defined and standard exercise in computational statistics, requiring the implementation and performance analysis of an MCMC sampler. I will now proceed to develop the solution.\n\n### Algorithmic Solution Design\n\nThe solution will be implemented in Python using the `numpy` library. The core logic will be encapsulated in a function that processes a single test case, and a main routine will iterate through all provided cases.\n\n**1. Model and Data Construction**\n\n- We begin by constructing the prior precision matrix $Q_{\\text{prior}}$. For an AR(1) process, the joint probability density function $p(x) = p(x_1) \\prod_{i=1}^{d-1} p(x_{i+1}|x_i)$ leads to a negative log-probability (energy) of $\\frac{1}{2}x^\\top Q_{\\text{prior}} x$ (plus constants), where $Q_{\\text{prior}}$ is a symmetric tridiagonal matrix. Its elements are derived from the energy expression $\\frac{1}{2\\sigma^2} \\left( (1-\\phi^2)x_1^2 + \\sum_{i=1}^{d-1} (x_{i+1} - \\phi x_i)^2 \\right)$. This gives:\n    - $(Q_{\\text{prior}})_{i,i} = (1+\\phi^2)/\\sigma^2$ for $i=2, \\dots, d-1$.\n    - $(Q_{\\text{prior}})_{1,1} = (Q_{\\text{prior}})_{d,d} = 1/\\sigma^2$.\n    - $(Q_{\\text{prior}})_{i,i+1} = (Q_{\\text{prior}})_{i+1,i} = -\\phi/\\sigma^2$.\n- The posterior precision is then formed by adding the likelihood precision: $Q = Q_{\\text{prior}} + (\\sigma_y^{-2})I_d$.\n- Synthetic data $y$ are generated by first drawing a true state $x^\\star$ from the prior distribution sequentially and then adding Gaussian noise: $y = x^\\star + \\mathcal{N}(0, \\sigma_y^2 I_d)$.\n- The linear term of the posterior energy is $b = y / \\sigma_y^2$.\n\n**2. Blocked RWM Sampler**\n\n- The MCMC chain is initialized at the posterior mean $m = Q^{-1}b$, found by solving the linear system $Qm=b$.\n- For each candidate block size $k$, we run the sampler for $N$ iterations. In each iteration:\n    - A contiguous block of indices $B$ of size $k$ is chosen uniformly at random.\n    - A proposal $x'$ is generated by perturbing the current state $x$ within the block: $x'_B = x_B + \\delta_B$, where $\\delta_B$ is drawn from a Gaussian distribution with a diagonal covariance. The standard deviation for each component $i \\in B$ is $s_i = (2.38/\\sqrt{k}) / \\sqrt{Q_{ii}}$.\n    - The change in energy, $\\Delta\\mathcal{E} = \\mathcal{E}(x') - \\mathcal{E}(x)$, is calculated efficiently. Given the proposal $\\delta = x' - x$, which is non-zero only on block $B$, the energy difference is $\\Delta\\mathcal{E} = (Qx - b)^{\\top}\\delta + \\frac{1}{2}\\delta^{\\top}Q\\delta$. Due to the tridiagonality of $Q$, both terms can be calculated in $\\mathcal{O}(k)$ operations by only considering local interactions, avoiding full matrix-vector products.\n    - The proposal is accepted with probability $\\min(1, \\exp(-\\Delta\\mathcal{E}))$.\n    - The value of the central coordinate, $x_{\\lfloor d/2 \\rfloor}$, is recorded at each iteration.\n\n**3. Performance Analysis**\n\n- After each run, the ESS of the recorded trace is calculated.\n- First, the autocorrelation function (ACF), $\\hat{\\rho}(t)$, of the mean-centered trace is computed efficiently using the Wiener-Khinchin theorem via Fast Fourier Transform (FFT).\n- Then, the integrated autocorrelation time, $\\tau_{\\text{int}}$, is estimated according to the problem's rule: $\\tau_{\\text{int}} = 1 + 2 \\sum_{t=1}^{T-1} \\hat{\\rho}(t)$, where $T$ is the first lag for which $\\hat{\\rho}(T) + \\hat{\\rho}(T+1)  0$. If no such $T$ is found, we sum over all available positive lags, which indicates very high correlation.\n- The ESS is computed as $N/\\tau_{\\text{int}}$.\n- The final metric, $\\text{ESS-per-cost}(k) = \\text{ESS}(k)/k$, is calculated.\n- After evaluating all block sizes $k$, the one that maximizes this metric is identified as $k^{\\star}$. The final reported value is the fraction $\\alpha = k^{\\star}/d$. This process is repeated for all test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    test_cases = [\n        # (d, phi, sigma, sigma_y, N, seed)\n        (64, 0.90, 1.0, 0.5, 3000, 12345),\n        (64, 0.98, 1.0, 0.5, 3000, 23456),\n        (32, 0.70, 1.0, 0.3, 4000, 34567),\n        (128, 0.95, 1.0, 1.0, 3000, 45678),\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha_opt = run_simulation_for_case(*case)\n        results.append(f\"{alpha_opt:.3f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef run_simulation_for_case(d, phi, sigma, sigma_y, N, seed):\n    \"\"\"\n    Performs the full simulation for a single parameter set to find the optimal block fraction.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct model matrices Q_prior, Q, and vector b\n    # Construct Q_prior for AR(1) GMRF\n    diag_vals = np.full(d, (1.0 + phi**2) / sigma**2)\n    diag_vals[0] = 1.0 / sigma**2\n    diag_vals[-1] = 1.0 / sigma**2\n    offdiag_val = -phi / sigma**2\n    \n    Q_prior = np.diag(diag_vals) + \\\n              np.diag(np.full(d - 1, offdiag_val), k=1) + \\\n              np.diag(np.full(d - 1, offdiag_val), k=-1)\n    \n    # Construct posterior precision Q\n    Q = Q_prior + (1.0 / sigma_y**2) * np.eye(d)\n\n    # 2. Generate synthetic observations y\n    x_star = np.zeros(d)\n    x_star[0] = rng.normal(loc=0.0, scale=sigma / np.sqrt(1.0 - phi**2))\n    for i in range(d - 1):\n        x_star[i + 1] = phi * x_star[i] + rng.normal(loc=0.0, scale=sigma)\n    \n    y = x_star + rng.normal(loc=0.0, scale=sigma_y, size=d)\n    \n    b = y / sigma_y**2\n\n    # 3. Initialize chain at posterior mean\n    m_posterior = np.linalg.solve(Q, b)\n    \n    # 4. Loop over block sizes k\n    block_sizes = [2**i for i in range(int(np.ceil(np.log2(d))))]\n    if d not in block_sizes:\n        block_sizes.append(d)\n    block_sizes.sort()\n\n    ess_per_cost_results = {}\n    Q_diag = np.diag(Q)\n    Q_superdiag = np.diag(Q, k=1)\n\n    for k in block_sizes:\n        x_current = m_posterior.copy()\n        center_idx = d // 2\n        trace = np.zeros(N)\n        \n        c_k = 2.38 / np.sqrt(k)\n        proposal_sds = c_k / np.sqrt(Q_diag)\n\n        # 5. Run blocked RWM chain\n        for i in range(N):\n            start_idx = rng.integers(0, d - k + 1)\n            block_slice = slice(start_idx, start_idx + k)\n            \n            z = rng.normal(size=k)\n            delta = np.zeros(d)\n            delta[block_slice] = proposal_sds[block_slice] * z\n            \n            x_prop = x_current + delta\n\n            # Calculate energy difference in O(k)\n            delta_E = calculate_energy_difference(x_current, delta, Q, b, Q_superdiag, start_idx, k)\n            \n            acceptance_prob = min(1.0, np.exp(-delta_E))\n            \n            if rng.random()  acceptance_prob:\n                x_current = x_prop\n            \n            trace[i] = x_current[center_idx]\n            \n        # 6. Compute ESS and ESS-per-cost\n        ess = compute_ess(trace)\n        if k > 0:\n            ess_per_cost_results[k] = ess / k\n\n    # 7. Determine optimal block size k*\n    if not ess_per_cost_results:\n        best_k = d # Fallback\n    else:\n        best_k = max(ess_per_cost_results, key=ess_per_cost_results.get)\n    \n    alpha_optimal = best_k / d\n    return alpha_optimal\n\n\ndef calculate_energy_difference(x_current, delta, Q, b, Q_superdiag, start_idx, k):\n    \"\"\"\n    Calculates the energy difference E(x_prop) - E(x_current) in O(k) time.\n    delta_E = (Qx - b)^T * delta + 0.5 * delta^T * Q * delta\n    \"\"\"\n    d = len(x_current)\n    j = start_idx\n    \n    # Term 1: (Qx-b)^T * delta, computed locally\n    linear_term = 0.0\n    for i in range(j, j + k):\n        Qx_i = 0.0\n        # Left neighbor\n        if i > 0:\n            Qx_i += Q[i, i - 1] * x_current[i - 1]\n        # Diagonal\n        Qx_i += Q[i, i] * x_current[i]\n        # Right neighbor\n        if i  d - 1:\n            Qx_i += Q[i, i + 1] * x_current[i + 1]\n        \n        grad_i = Qx_i - b[i]\n        linear_term += grad_i * delta[i]\n\n    # Term 2: 0.5 * delta^T * Q * delta, computed locally for tridiagonal Q\n    delta_block = delta[j : j + k]\n    quad_term = np.sum(np.diag(Q)[j : j + k] * delta_block**2)\n    if k > 1:\n        quad_term += 2.0 * np.sum(Q_superdiag[j : j + k - 1] * delta_block[:-1] * delta_block[1:])\n    \n    quad_term *= 0.5\n\n    return linear_term + quad_term\n\n\ndef compute_ess(series):\n    \"\"\"\n    Computes the Effective Sample Size (ESS) of a time series.\n    \"\"\"\n    n = len(series)\n    if n  2:\n        return 0.0\n\n    series_demeaned = series - np.mean(series)\n    \n    # Compute Autocorrelation Function using FFT\n    fft_len = 2 * n\n    g = np.fft.fft(series_demeaned, n=fft_len)\n    autocov = np.fft.ifft(g * np.conj(g)).real\n    if autocov[0] == 0:\n        return 0.0\n    rho = autocov[:n] / autocov[0]\n\n    # Estimate integrated autocorrelation time using initial positive sequence\n    T_cutoff = -1\n    # Iterate up to a safe max lag to avoid noisy estimates\n    max_lag = min(n - 2, 1000) \n    for T in range(1, max_lag):\n        if rho[T] + rho[T + 1]  0:\n            T_cutoff = T\n            break\n    \n    if T_cutoff != -1:\n        # Sum up to T-1\n        sum_rho = np.sum(rho[1:T_cutoff])\n    else:\n        # If ACF is positive for all lags, sum what we have\n        sum_rho = np.sum(rho[1:max_lag])\n\n    tau_int = 1.0 + 2.0 * sum_rho\n    \n    if tau_int = 0: # Handle pathological cases\n        return 1.0\n\n    return n / tau_int\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3371022"}]}