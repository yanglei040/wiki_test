{"hands_on_practices": [{"introduction": "Understanding Gaussian measures on Hilbert spaces begins with the covariance operator, which encodes the entire correlation structure of the random field. This first exercise provides a concrete link between the abstract operator and its practical application. By computing the covariance of linear observations in terms of the operator's spectral decomposition [@problem_id:3385102], you will gain a hands-on understanding of how the eigenvalues and eigenvectors shape the statistical properties of the measure.", "problem": "Consider a real, separable Hilbert space $H$ with inner product $\\langle \\cdot,\\cdot \\rangle_H$ and an orthonormal basis $(e_j)_{j \\in \\mathbb{N}}$. Let $\\mu$ be a centered Gaussian measure on $H$ with covariance operator $C \\colon H \\to H$, which is self-adjoint, positive, and trace-class. Suppose that $C$ admits the spectral decomposition $C e_j = \\lambda_j e_j$ with eigenvalues $(\\lambda_j)_{j \\in \\mathbb{N}}$ satisfying $\\lambda_j \\ge 0$ and $\\sum_{j=1}^{\\infty} \\lambda_j  \\infty$. Let $X$ be an $H$-valued random variable distributed according to $\\mu$.\n\nIn the context of Bayesian data assimilation, consider linear observation functionals defined by $h,k \\in H$, and the corresponding scalar observations $\\langle X,h \\rangle_H$ and $\\langle X,k \\rangle_H$. Write $h = \\sum_{j=1}^{\\infty} h_j e_j$ and $k = \\sum_{j=1}^{\\infty} k_j e_j$, where $h_j = \\langle h, e_j \\rangle_H$ and $k_j = \\langle k, e_j \\rangle_H$.\n\nUsing only foundational properties of Gaussian measures on Hilbert spaces and the spectral decomposition above, compute the covariance $\\mathrm{cov}(\\langle X,h \\rangle_H, \\langle X,k \\rangle_H)$ explicitly as a series in terms of $(\\lambda_j)$ and the coefficients $(h_j)$ and $(k_j)$. Express your final answer as a single analytic series in $(\\lambda_j, h_j, k_j)$; do not use operator notation in the final expression. No rounding is required.", "solution": "The problem is well-posed and contains all necessary information for a complete solution. It is a standard query in the field of functional analysis and probability theory, specifically concerning Gaussian measures on infinite-dimensional spaces.\n\nLet $Y_h = \\langle X,h \\rangle_H$ and $Y_k = \\langle X,k \\rangle_H$. These are real-valued random variables, as the inner product on the real Hilbert space $H$ maps to $\\mathbb{R}$. We want to compute the covariance $\\mathrm{cov}(Y_h, Y_k)$.\n\nBy definition, the covariance of two real-valued random variables is given by:\n$$\n\\mathrm{cov}(Y_h, Y_k) = \\mathbb{E}[(Y_h - \\mathbb{E}[Y_h])(Y_k - \\mathbb{E}[Y_k])]\n$$\nFirst, we compute the expected values $\\mathbb{E}[Y_h]$ and $\\mathbb{E}[Y_k]$. The expectation operator $\\mathbb{E}$ is linear. Since the inner product $\\langle \\cdot, \\cdot \\rangle_H$ is linear in its first argument, we can write:\n$$\n\\mathbb{E}[Y_h] = \\mathbb{E}[\\langle X,h \\rangle_H] = \\langle \\mathbb{E}[X], h \\rangle_H\n$$\nThe problem states that the Gaussian measure $\\mu$ is centered. This means that the expected value (or mean) of the $H$-valued random variable $X$ is the zero vector in $H$, i.e., $\\mathbb{E}[X] = 0$. Therefore,\n$$\n\\mathbb{E}[Y_h] = \\langle 0, h \\rangle_H = 0\n$$\nSimilarly, for the random variable $Y_k$:\n$$\n\\mathbb{E}[Y_k] = \\mathbb{E}[\\langle X,k \\rangle_H] = \\langle \\mathbb{E}[X], k \\rangle_H = \\langle 0, k \\rangle_H = 0\n$$\nSubstituting these zero means back into the covariance formula, we get:\n$$\n\\mathrm{cov}(Y_h, Y_k) = \\mathbb{E}[(Y_h - 0)(Y_k - 0)] = \\mathbb{E}[Y_h Y_k] = \\mathbb{E}[\\langle X,h \\rangle_H \\langle X,k \\rangle_H]\n$$\nA fundamental property of a Gaussian measure $\\mu$ on a Hilbert space $H$ with covariance operator $C$ is that for any two elements $h, k \\in H$, the covariance of the random variables $\\langle X,h \\rangle_H$ and $\\langle X,k \\rangle_H$ is given by the inner product $\\langle Ch, k \\rangle_H$. Thus,\n$$\n\\mathrm{cov}(Y_h, Y_k) = \\langle C h, k \\rangle_H\n$$\nOur task now is to express $\\langle C h, k \\rangle_H$ in terms of the eigenvalues $(\\lambda_j)$ and the coefficients $(h_j)$ and $(k_j)$. We are given the series expansions for $h$ and $k$ in the orthonormal basis $(e_j)_{j \\in \\mathbb{N}}$:\n$$\nh = \\sum_{j=1}^{\\infty} h_j e_j \\quad \\text{and} \\quad k = \\sum_{i=1}^{\\infty} k_i e_i\n$$\nwhere $h_j = \\langle h, e_j \\rangle_H$ and $k_i = \\langle k, e_i \\rangle_H$. We have used a different index $i$ for the expansion of $k$ for clarity in the subsequent steps.\n\nThe covariance operator $C$ is a linear operator. As it is trace-class on a Hilbert space, it is a compact operator and therefore continuous. Its linearity and continuity allow us to apply it to the series expansion of $h$:\n$$\nCh = C\\left(\\sum_{j=1}^{\\infty} h_j e_j\\right) = \\sum_{j=1}^{\\infty} h_j C(e_j)\n$$\nWe are given the spectral decomposition of $C$ with respect to the basis $(e_j)$, namely $C e_j = \\lambda_j e_j$. Substituting this into the expression for $Ch$:\n$$\nCh = \\sum_{j=1}^{\\infty} h_j (\\lambda_j e_j) = \\sum_{j=1}^{\\infty} \\lambda_j h_j e_j\n$$\nNow we compute the inner product $\\langle Ch, k \\rangle_H$:\n$$\n\\langle Ch, k \\rangle_H = \\left\\langle \\sum_{j=1}^{\\infty} \\lambda_j h_j e_j, \\sum_{i=1}^{\\infty} k_i e_i \\right\\rangle_H\n$$\nSince the inner product is continuous and bilinear (on a real Hilbert space), we can interchange the sums and the inner product:\n$$\n\\langle Ch, k \\rangle_H = \\sum_{j=1}^{\\infty} \\sum_{i=1}^{\\infty} \\langle \\lambda_j h_j e_j, k_i e_i \\rangle_H = \\sum_{j=1}^{\\infty} \\sum_{i=1}^{\\infty} \\lambda_j h_j k_i \\langle e_j, e_i \\rangle_H\n$$\nThe basis $(e_j)_{j \\in \\mathbb{N}}$ is orthonormal, which means that $\\langle e_j, e_i \\rangle_H = \\delta_{ji}$, where $\\delta_{ji}$ is the Kronecker delta. The Kronecker delta is $1$ if $j=i$ and $0$ if $j \\neq i$. Substituting this into the expression:\n$$\n\\langle Ch, k \\rangle_H = \\sum_{j=1}^{\\infty} \\sum_{i=1}^{\\infty} \\lambda_j h_j k_i \\delta_{ji}\n$$\nThe presence of $\\delta_{ji}$ makes all terms in the inner sum (over $i$) equal to zero, except for the term where $i=j$. Thus, the double summation collapses into a single summation:\n$$\n\\langle Ch, k \\rangle_H = \\sum_{j=1}^{\\infty} \\lambda_j h_j k_j\n$$\nThis provides the explicit series expression for the covariance as requested. The convergence of this series is guaranteed because $h, k \\in H$ implies $\\sum h_j^2  \\infty$ and $\\sum k_j^2  \\infty$, and $C$ being trace-class implies $\\sum \\lambda_j  \\infty$. By the Cauchy-Schwarz inequality, $\\left| \\sum \\lambda_j h_j k_j \\right| \\le \\left(\\sum \\lambda_j h_j^2\\right)^{1/2} \\left(\\sum \\lambda_j k_j^2\\right)^{1/2}$, and these latter series converge.\nTherefore, the covariance is:\n$$\n\\mathrm{cov}(\\langle X,h \\rangle_H, \\langle X,k \\rangle_H) = \\sum_{j=1}^{\\infty} \\lambda_j h_j k_j\n$$", "answer": "$$\n\\boxed{\\sum_{j=1}^{\\infty} \\lambda_j h_j k_j}\n$$", "id": "3385102"}, {"introduction": "Having explored the structure of a Gaussian prior, we now apply it to a classic data assimilation problem: inferring an unknown state from noisy linear observations. When both the prior and the noise are Gaussian, the posterior distribution can be computed analytically. This exercise [@problem_id:3385131] guides you through the derivation of the posterior mean and covariance, revealing the celebrated Kalman update formulas in an infinite-dimensional setting.", "problem": "Let $H$ be a separable real Hilbert space with inner product $\\langle \\cdot,\\cdot \\rangle_H$ and norm $|\\cdot|_H$. Consider the linear inverse problem in the data assimilation setting where the forward map is the identity on $H$, so the observed data $y \\in H$ satisfy\n$$\ny = u + \\eta,\n$$\nwith $u \\in H$ the unknown state and $\\eta \\in H$ an observational noise. Assume a Gaussian prior $u \\sim \\mathcal{N}(m_0, C_0)$ on $H$, where $m_0 \\in H$ and $C_0: H \\to H$ is a self-adjoint, positive, trace-class covariance operator, and assume independent Gaussian observational noise $\\eta \\sim \\mathcal{N}(0, \\Gamma)$ with $\\Gamma: H \\to H$ bounded, self-adjoint, strictly positive (coercive). Let $\\mu_0 = \\mathcal{N}(m_0, C_0)$ and $\\nu = \\mathcal{N}(0, \\Gamma)$ denote the prior and noise measures, respectively, and let $\\mathcal{H}_{C_0} = \\operatorname{ran}(C_0^{1/2})$ and $\\mathcal{H}_{\\Gamma} = \\operatorname{ran}(\\Gamma^{1/2})$ denote the associated Cameron-Martin spaces endowed with their canonical inner products. The posterior measure $\\mu^y$ is defined via Bayes' rule by weighting $\\mu_0$ with the likelihood induced by $\\nu$ and the observation model.\n\nStarting from the fundamental definitions of Gaussian measures on Hilbert spaces and the Cameron-Martin theorem for shifts of Gaussian measures, as well as the well-tested facts that linear images of Gaussian measures are Gaussian and that joint Gaussian vectors are characterized by their first two moments, perform the following:\n\n1. Show that the joint law of $(u,y) \\in H \\times H$ is Gaussian, determine its mean and covariance operator in block form, and use this to argue that the posterior law of $u$ given $y$ coincides with the Gaussian conditional distribution of $u$ given $y$.\n\n2. Compute explicitly the posterior mean $m^y \\in H$ and posterior covariance operator $C^y: H \\to H$ of $u$ given $y$, expressing your answer only in terms of $m_0$, $C_0$, $\\Gamma$, and $y$, without introducing any extraneous quantities. You may use only well-tested operator identities and properties valid for self-adjoint, positive operators on Hilbert spaces.\n\nExpress your final answer as a single pair consisting of the posterior mean and the posterior covariance operator. No numerical rounding is required and no physical units are involved.", "solution": "The user wants me to solve a problem about Bayesian inverse problems, specifically to find the posterior mean and covariance for a linear Gaussian problem on a Hilbert space.\n\n### Step 1: Extract Givens\n\n-   **Hilbert Space**: $H$ is a separable real Hilbert space with inner product $\\langle \\cdot,\\cdot \\rangle_H$ and norm $|\\cdot|_H$.\n-   **Model**: The observation $y \\in H$ is related to the unknown state $u \\in H$ by $y = u + \\eta$, where $\\eta \\in H$ is noise.\n-   **Prior Distribution**: $u \\sim \\mathcal{N}(m_0, C_0)$, where $m_0 \\in H$ is the prior mean and $C_0: H \\to H$ is the prior covariance operator. $C_0$ is self-adjoint, positive, and trace-class. Let $\\mu_0 = \\mathcal{N}(m_0, C_0)$.\n-   **Noise Distribution**: $\\eta \\sim \\mathcal{N}(0, \\Gamma)$, where $\\Gamma: H \\to H$ is the noise covariance operator. $\\Gamma$ is bounded, self-adjoint, and strictly positive (coercive). Let $\\nu = \\mathcal{N}(0, \\Gamma)$.\n-   **Independence**: The random variables $u$ and $\\eta$ are independent.\n-   **Posterior Definition**: The posterior measure $\\mu^y$ is defined via Bayes' rule.\n-   **Task 1**: Show the joint law of $(u,y)$ is Gaussian, find its mean and covariance, and relate the posterior to the conditional distribution.\n-   **Task 2**: Compute the posterior mean $m^y$ and posterior covariance $C^y$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is a canonical example of Bayesian inference in an infinite-dimensional (function space) setting, a core topic in the field of inverse problems and data assimilation.\n\n-   **Scientific Grounding**: The problem is scientifically and mathematically sound. It is based on the theory of Gaussian measures on Hilbert spaces, operator theory, and Bayesian statistics. The assumptions on the covariance operators ($C_0$ being trace-class, $\\Gamma$ being coercive) are standard and necessary to ensure that the Gaussian measures are well-defined and the posterior is well-posed.\n-   **Well-Posedness**: The problem is well-posed. The coercivity of $\\Gamma$ ensures that the operator $C_0 + \\Gamma$ is invertible, which is essential for deriving the solution. A unique posterior distribution exists under these standard assumptions.\n-   **Objectivity**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective content.\n\nAll criteria for a valid problem are met. The problem does not exhibit any of the listed flaws. It is a standard, non-trivial derivation in its field.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. I will proceed with the solution.\n\nThe solution will be presented in two parts as requested.\n\n**Part 1: Joint Distribution and its Relation to the Posterior**\n\nA central tenet of Gaussian probability theory, which extends from finite to infinite-dimensional Hilbert spaces, is that an affine transformation of a Gaussian random variable is also a Gaussian random variable. We leverage this to determine the joint distribution of $(u, y)$.\n\nThe state $u$ and the observation $y$ are related by:\n$$ u = u $$\n$$ y = u + \\eta $$\nThe variables $u$ and $\\eta$ are independent Gaussians, with $u \\sim \\mathcal{N}(m_0, C_0)$ and $\\eta \\sim \\mathcal{N}(0, \\Gamma)$. We can express the joint variable $\\begin{pmatrix} u \\\\ y \\end{pmatrix} \\in H \\times H$ as an affine transformation of the centered, independent Gaussian variables $u' = u - m_0 \\sim \\mathcal{N}(0, C_0)$ and $\\eta \\sim \\mathcal{N}(0, \\Gamma)$.\nThe vector $\\begin{pmatrix} u' \\\\ \\eta \\end{pmatrix}$ is a zero-mean Gaussian variable on $H \\times H$. Due to the independence of $u'$ and $\\eta$, its covariance operator is the block-diagonal operator $\\begin{pmatrix} C_0  0 \\\\ 0  \\Gamma \\end{pmatrix}$.\n\nThe transformation is given by:\n$$\n\\begin{pmatrix} u \\\\ y \\end{pmatrix} = \\begin{pmatrix} I  0 \\\\ I  I \\end{pmatrix} \\begin{pmatrix} u' \\\\ \\eta \\end{pmatrix} + \\begin{pmatrix} m_0 \\\\ m_0 \\end{pmatrix}\n$$\nwhere $I$ is the identity operator on $H$.\n\nSince $\\begin{pmatrix} u \\\\ y \\end{pmatrix}$ is an affine transformation of a Gaussian variable, it is itself Gaussian. We now compute its mean and covariance operator.\n\nThe mean is found by applying the law of expectation:\n$$\n\\mathbb{E}\\left[ \\begin{pmatrix} u \\\\ y \\end{pmatrix} \\right] = \\mathbb{E}\\left[ \\begin{pmatrix} I  0 \\\\ I  I \\end{pmatrix} \\begin{pmatrix} u' \\\\ \\eta \\end{pmatrix} + \\begin{pmatrix} m_0 \\\\ m_0 \\end{pmatrix} \\right] = \\begin{pmatrix} I  0 \\\\ I  I \\end{pmatrix} \\mathbb{E}\\left[ \\begin{pmatrix} u' \\\\ \\eta \\end{pmatrix} \\right] + \\begin{pmatrix} m_0 \\\\ m_0 \\end{pmatrix}\n$$\nSince $\\mathbb{E}[u'] = 0$ and $\\mathbb{E}[\\eta] = 0$, the mean is:\n$$\n\\mathbb{E}\\left[ \\begin{pmatrix} u \\\\ y \\end{pmatrix} \\right] = \\begin{pmatrix} I  0 \\\\ I  I \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} m_0 \\\\ m_0 \\end{pmatrix} = \\begin{pmatrix} m_0 \\\\ m_0 \\end{pmatrix}\n$$\n\nThe covariance operator of the transformed variable is given by the formula $L C_{\\text{in}} L^*$, where $L = \\begin{pmatrix} I  0 \\\\ I  I \\end{pmatrix}$ and $C_{\\text{in}} = \\begin{pmatrix} C_0  0 \\\\ 0  \\Gamma \\end{pmatrix}$. The adjoint operator $L^*$ on the Hilbert space $H \\times H$ is $L^* = \\begin{pmatrix} I  I \\\\ 0  I \\end{pmatrix}$.\nThe joint covariance operator is:\n$$\n\\operatorname{Cov}\\left( \\begin{pmatrix} u \\\\ y \\end{pmatrix} \\right) = \\begin{pmatrix} I  0 \\\\ I  I \\end{pmatrix} \\begin{pmatrix} C_0  0 \\\\ 0  \\Gamma \\end{pmatrix} \\begin{pmatrix} I  I \\\\ 0  I \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} C_0  0 \\\\ C_0  \\Gamma \\end{pmatrix} \\begin{pmatrix} I  I \\\\ 0  I \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} C_0 I + 0  C_0 I + 0 \\\\ C_0 I + 0  C_0 I + \\Gamma I \\end{pmatrix} = \\begin{pmatrix} C_0  C_0 \\\\ C_0  C_0 + \\Gamma \\end{pmatrix}\n$$\nNote that $C_0$ is self-adjoint, so the cross-covariance block $C_{yu} = C_{uy}^* = C_0^* = C_0$, which is consistent.\nThus, the joint distribution of $(u, y)$ is Gaussian:\n$$\n\\begin{pmatrix} u \\\\ y \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} m_0 \\\\ m_0 \\end{pmatrix}, \\begin{pmatrix} C_0  C_0 \\\\ C_0  C_0 + \\Gamma \\end{pmatrix} \\right)\n$$\nA fundamental result of Bayesian statistics is that for a linear-Gaussian model, the posterior distribution $p(u|y)$ is identical to the conditional distribution of $u$ given $y$ derived from the joint distribution $p(u,y)$. While the posterior measure $\\mu^y$ can be formally defined using the Cameron-Martin theorem and a Radon-Nikodym derivative, this is equivalent to conditioning on the joint Gaussian measure. Therefore, the posterior law of $u$ given the observation $y$ is Gaussian, and its moments are given by the standard formulas for conditional Gaussian distributions.\n\n**Part 2: Posterior Mean and Covariance**\n\nFor a general joint Gaussian variable $\\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix} \\sim \\mathcal{N}\\left( \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{11}  \\Sigma_{12} \\\\ \\Sigma_{21}  \\Sigma_{22} \\end{pmatrix} \\right)$, the conditional distribution of $X_1$ given $X_2 = x_2$ is also Gaussian, $\\mathcal{N}(\\mu_{1|2}, \\Sigma_{11|2})$, with mean and covariance given by:\n$$ \\mu_{1|2} = \\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1} (x_2 - \\mu_2) $$\n$$ \\Sigma_{11|2} = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21} $$\nWe apply these formulas to our problem by identifying:\n$X_1 \\leftrightarrow u$, $X_2 \\leftrightarrow y$, $x_2 \\leftrightarrow y$ (the specific observation).\n$\\mu_1 \\leftrightarrow m_0$, $\\mu_2 \\leftrightarrow m_0$.\n$\\Sigma_{11} \\leftrightarrow C_0$, $\\Sigma_{12} \\leftrightarrow C_0$, $\\Sigma_{21} \\leftrightarrow C_0$, $\\Sigma_{22} \\leftrightarrow C_0 + \\Gamma$.\n\nThe inverse $\\Sigma_{22}^{-1} = (C_0 + \\Gamma)^{-1}$ exists and is a bounded operator on $H$. This is because $C_0$ is a positive operator and $\\Gamma$ is a strictly positive (coercive) self-adjoint operator, which makes their sum $C_0 + \\Gamma$ coercive and thus boundedly invertible by the Lax-Milgram theorem.\n\nThe posterior mean $m^y$ is:\n$$\nm^y = m_0 + C_0 (C_0 + \\Gamma)^{-1} (y - m_0)\n$$\nThis expression is the celebrated Kalman update formula for the mean, extended to Hilbert spaces. It adjusts the prior mean $m_0$ based on the innovation $(y-m_0)$, weighted by the \"gain\" operator $K = C_0 (C_0 + \\Gamma)^{-1}$.\n\nThe posterior covariance operator $C^y$ is:\n$$\nC^y = C_0 - C_0 (C_0 + \\Gamma)^{-1} C_0\n$$\nThis expression for the posterior covariance is also fundamental and represents the reduction in uncertainty resulting from the observation. It is a well-defined, self-adjoint, positive, and trace-class operator. The expression is rigorously defined using only the given operators and their well-defined inverses, as requested.\n\nThe pair $(m^y, C^y)$ completely characterizes the posterior Gaussian measure $\\mu^y = \\mathcal{N}(m^y, C^y)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} m_0 + C_0 (C_0 + \\Gamma)^{-1} (y - m_0)  C_0 - C_0 (C_0 + \\Gamma)^{-1} C_0 \\end{pmatrix}}\n$$", "id": "3385131"}, {"introduction": "While analytical solutions are powerful, most real-world inverse problems are nonlinear and require computational sampling methods like Markov Chain Monte Carlo (MCMC). This is where the theoretical properties of the Cameron-Martin space have profound practical implications. This advanced practice [@problem_id:3385118] asks you to investigate why a naive MCMC algorithm fails dramatically in high dimensions and how a method designed with respect to the Cameron-Martin geometry remains robust, illustrating a crucial principle for designing scalable algorithms in function spaces.", "problem": "Consider the separable Hilbert space $H$ with orthonormal basis $\\{e_k\\}_{k \\ge 1}$. Let $C$ be a self-adjoint, positive, trace-class covariance operator on $H$ defined by $C e_k = \\lambda_k e_k$ with eigenvalues $\\lambda_k = k^{-2\\alpha}$ for a fixed $\\alpha  \\tfrac{1}{2}$. Define the centered Gaussian measure $\\mu = \\mathcal{N}(0, C)$ on $H$. The Cameron-Martin space $H_C$ is the range of $C^{1/2}$, equipped with the norm $\\|h\\|_{H_C}^2 = \\langle C^{-1/2} h, C^{-1/2} h \\rangle_H$. Consider the posterior $\\pi$ set equal to $\\mu$ (i.e., the \"no data\" case), so that the target distribution is $\\mathcal{N}(0, C)$.\n\nYou will compare two Markov chain Monte Carlo (MCMC) proposal mechanisms on the finite-dimensional truncations $H_n = \\operatorname{span}\\{e_1, \\dots, e_n\\}$, with $C_n$ the restriction of $C$ to $H_n$ and diagonal in the basis $\\{e_k\\}_{k=1}^n$ with entries $\\lambda_k = k^{-2\\alpha}$.\n\n1. A naive Metropolis-Adjusted Langevin Algorithm (MALA) with identity mass matrix in $H_n$ uses the gradient step of the negative log density in $H_n$. For state $x \\in \\mathbb{R}^n$, the negative log density is $\\Phi_n(x) = \\tfrac{1}{2} \\langle x, C_n^{-1} x \\rangle$, so the gradient is $\\nabla \\Phi_n(x) = C_n^{-1} x$. For step size $\\varepsilon  0$, the proposal is\n   $$\n   x' \\sim q_{\\mathrm{MALA}}(\\cdot \\mid x) = \\mathcal{N}\\Big(x - \\tfrac{\\varepsilon}{2} C_n^{-1} x, \\, \\varepsilon I_n \\Big).\n   $$\n   The Metropolis-Hastings acceptance probability is\n   $$\n   a_{\\mathrm{MALA}}(x, x') = \\min\\left\\{1, \\frac{\\pi_n(x') \\, q_{\\mathrm{MALA}}(x \\mid x')}{\\pi_n(x) \\, q_{\\mathrm{MALA}}(x' \\mid x)} \\right\\},\n   $$\n   where $\\pi_n$ is the $\\mathcal{N}(0, C_n)$ density.\n\n2. A Cameron-Martin preconditioned, prior-preserving proposal (the preconditioned Crank–Nicolson mechanism) with parameter $\\beta \\in (0,1)$ on $H_n$ is\n   $$\n   x' = \\sqrt{1 - \\beta^2} \\, x + \\beta \\, \\eta, \\quad \\eta \\sim \\mathcal{N}(0, C_n).\n   $$\n   This proposal preserves $\\mathcal{N}(0, C_n)$ and is reversible, so for the Gaussian target $\\mathcal{N}(0, C_n)$ the acceptance probability is $a_{\\mathrm{pCN}}(x, x') \\equiv 1$.\n\nYour tasks:\n\n(a) Using only fundamental facts about Gaussian measures on Hilbert spaces, the definition of the Cameron-Martin space, and the Metropolis-Hastings acceptance rule, construct a counterexample showing that the naive MALA proposal suffers vanishing acceptance in the infinite-dimensional limit, while the Cameron-Martin preconditioned proposal remains within high-probability regions. Concretely, consider the sequence of truncations $H_n$ with $\\lambda_k = k^{-2\\alpha}$ and fixed $\\varepsilon  0$ and argue from first principles why, as $n \\to \\infty$, the naive MALA drift $\\tfrac{\\varepsilon}{2} C_n^{-1} x$ becomes dominated by directions that are not in the Cameron-Martin space of typical draws $x \\sim \\mathcal{N}(0, C)$, leading to proposals that are overwhelmingly rejected, whereas the preconditioned proposal acts along Cameron-Martin geometry and preserves the target.\n\n(b) Implement a numerical experiment that estimates empirical acceptance probabilities for both proposals on three truncations. Use the following test suite:\n- $\\alpha = 1.5$,\n- MALA step size $\\varepsilon = 10^{-2}$,\n- preconditioned proposal parameter $\\beta = 0.2$,\n- dimensions $n \\in \\{16, 64, 256\\}$,\n- number of independent trials per case $T = 2000$,\n- random seed fixed to $12345$ for reproducibility.\n\nFor the MALA cases, generate $T$ independent initial states $x \\sim \\mathcal{N}(0, C_n)$, propose $x'$ via the MALA kernel, compute the Metropolis-Hastings acceptance probability, and estimate the acceptance rate as the fraction of accepted proposals. For the preconditioned proposal, either compute proposals and the acceptance fraction or use the invariance argument to report acceptance $1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be flattened in the following order:\n$$\n[\\mathrm{acc}_{\\mathrm{MALA}}(n{=}16), \\, \\mathrm{acc}_{\\mathrm{pCN}}(n{=}16), \\, \\mathrm{acc}_{\\mathrm{MALA}}(n{=}64), \\, \\mathrm{acc}_{\\mathrm{pCN}}(n{=}64), \\, \\mathrm{acc}_{\\mathrm{MALA}}(n{=}256), \\, \\mathrm{acc}_{\\mathrm{pCN}}(n{=}256)].\n$$\nEach entry must be a floating-point number in decimal form. No physical units or angles are involved.\n\nDesign considerations:\n- The counterexample reasoning in part (a) must start from the definitions of Gaussian measures on Hilbert spaces, the Cameron-Martin space, and the Cameron-Martin theorem on translations, together with the Metropolis-Hastings acceptance rule, and should explain why naive gradient steps accumulate energy in high-frequency modes, while Cameron-Martin preconditioning avoids this pathology.\n- The numerical experiment in part (b) must adhere exactly to the test suite parameters to ensure reproducibility and coverage. The $n = 16$ case is a happy path, $n = 256$ stresses high-frequency modes, and $n = 64$ is intermediate.", "solution": "The problem as stated is mathematically and scientifically sound, well-posed, and provides a complete set of specifications for both the theoretical analysis and the numerical experiment. We may therefore proceed with a full solution.\n\nThe core of this problem lies in understanding how the geometry of an infinite-dimensional Hilbert space interacts with Markov chain Monte Carlo (MCMC) algorithms. We are comparing two proposals for sampling from a centered Gaussian measure $\\mu = \\mathcal{N}(0, C)$ on a separable Hilbert space $H$. The behavior of these algorithms in the infinite-dimensional limit ($n \\to \\infty$) reveals a critical pathology in naive gradient-based methods and highlights the necessity of preconditioning.\n\nA Gaussian measure $\\mu = \\mathcal{N}(0, C)$ on $H$ is well-defined if the covariance operator $C$ is trace-class, which means $\\operatorname{tr}(C) = \\sum_{k=1}^\\infty \\lambda_k  \\infty$. Given $\\lambda_k = k^{-2\\alpha}$, the trace is $\\sum_{k=1}^\\infty k^{-2\\alpha}$, which converges if and only if $2\\alpha > 1$, or $\\alpha > \\tfrac{1}{2}$. The problem's constraint $\\alpha > \\tfrac{1}{2}$ ensures this. A typical realization $x = \\sum_{k=1}^\\infty x_k e_k$ drawn from $\\mu$ has random, independent coefficients $x_k \\sim \\mathcal{N}(0, \\lambda_k)$. The expected squared norm is finite: $E[\\|x\\|_H^2] = E[\\sum_{k=1}^\\infty x_k^2] = \\sum_{k=1}^\\infty E[x_k^2] = \\sum_{k=1}^\\infty \\lambda_k = \\operatorname{tr}(C)  \\infty$. This implies that draws from $\\mu$ are almost surely in $H$.\n\nThe Cameron-Martin space $H_C$ associated with $\\mu$ is the subspace of $H$ consisting of vectors $h$ for which $\\|h\\|_{H_C}^2 = \\|C^{-1/2}h\\|_H^2 = \\sum_{k=1}^\\infty \\lambda_k^{-1} h_k^2  \\infty$. A fundamental result, the Feldman–Hajek theorem, states that a typical draw $x \\sim \\mu$ is almost surely *not* in the Cameron-Martin space $H_C$. This can be seen by computing the expected Cameron-Martin norm of a draw: $E[\\|x\\|_{H_C}^2] = E[\\sum_{k=1}^\\infty \\lambda_k^{-1}x_k^2] = \\sum_{k=1}^\\infty \\lambda_k^{-1}E[x_k^2] = \\sum_{k=1}^\\infty \\lambda_k^{-1}\\lambda_k = \\sum_{k=1}^\\infty 1 = \\infty$. The Cameron-Martin theorem further states that translating the measure $\\mu$ by a vector $v \\in H$ yields a new measure that is mutually absolutely continuous with $\\mu$ if and only if $v \\in H_C$. If $v \\notin H_C$, the translated measure is singular with respect to $\\mu$. This implies that moving a sample in a direction not contained in $H_C$ is a move into a region of zero probability under $\\mu$.\n\n(a) With this foundation, we can analyze the two proposals on the finite-dimensional space $H_n = \\operatorname{span}\\{e_1, \\dots, e_n\\}$, always considering the limit as $n \\to \\infty$.\n\nFirst, consider the naive Metropolis-Adjusted Langevin Algorithm (MALA). The proposal is $x' \\sim \\mathcal{N}(x - \\tfrac{\\varepsilon}{2} C_n^{-1} x, \\varepsilon I_n)$. The deterministic part of the update, the drift term, is $v_n(x) = -\\tfrac{\\varepsilon}{2} C_n^{-1} x$. For a typical draw $x \\sim \\mathcal{N}(0, C_n)$, the coefficients of this drift vector are $(v_n(x))_k = -\\tfrac{\\varepsilon}{2} \\lambda_k^{-1} x_k$. The variance of this $k$-th coefficient is $(\\tfrac{\\varepsilon}{2})^2 \\lambda_k^{-2} E[x_k^2] = (\\tfrac{\\varepsilon}{2})^2 \\lambda_k^{-1} = (\\tfrac{\\varepsilon}{2})^2 k^{2\\alpha}$. This variance grows with the mode number $k$. Consequently, the drift vector's squared $H$-norm has an expected value that diverges as $n \\to \\infty$:\n$$\nE\\left[ \\| v_n(x) \\|_H^2 \\right] = \\sum_{k=1}^n E\\left[ \\left(-\\tfrac{\\varepsilon}{2} \\lambda_k^{-1} x_k\\right)^2 \\right] = \\left(\\tfrac{\\varepsilon}{2}\\right)^2 \\sum_{k=1}^n \\lambda_k^{-1} = \\left(\\tfrac{\\varepsilon}{2}\\right)^2 \\sum_{k=1}^n k^{2\\alpha} \\to \\infty.\n$$\nThe drift term for a typical sample becomes unboundedly large, pushing the proposal $x'$ very far from the current state $x$. This itself is not the fatal flaw; the issue lies in the Metropolis-Hastings correction. The acceptance probability is $a_{\\mathrm{MALA}}(x, x') = \\min\\{1, R\\}$, where the ratio $R = \\frac{\\pi_n(x') q_{\\mathrm{MALA}}(x|x')}{\\pi_n(x) q_{\\mathrm{MALA}}(x'|x)}$. While the drift moves towards regions of higher target density $\\pi_n$ (i.e., $\\pi_n(x')/\\pi_n(x)$ is large), the ratio of proposal densities $q_{\\mathrm{MALA}}(x|x')/q_{\\mathrm{MALA}}(x'|x)$ becomes vanishingly small. The reverse proposal probability $q_{\\mathrm{MALA}}(x|x')$ is the density of $\\mathcal{N}(x' - \\tfrac{\\varepsilon}{2}C_n^{-1}x', \\varepsilon I_n)$ evaluated at $x$. The term in the exponent is proportional to $\\|x - (x' - \\tfrac{\\varepsilon}{2}C_n^{-1}x')\\|^2$. Analysis shows that the dominant term in this squared distance, for large $n$, scales with $\\|C_n^{-2}x\\|^2$. The expected value of this term diverges extremely rapidly: $E[\\|C_n^{-2}x\\|^2] = \\sum_{k=1}^n \\lambda_k^{-4}E[x_k^2] = \\sum_{k=1}^n \\lambda_k^{-3} = \\sum_{k=1}^n k^{6\\alpha} \\to \\infty$. This forces $q_{\\mathrm{MALA}}(x|x')$ to be extraordinarily small, overwhelming the gain in target density and driving the acceptance probability $a_{\\mathrm{MALA}}$ to zero as $n \\to \\infty$. The MALA proposal, by using an identity-matrix covariance for its noise term, implicitly assumes a standard Euclidean geometry, which is fundamentally mismatched with the geometry of the target Gaussian measure on the Hilbert space. The gradient term $C_n^{-1}x$ amplifies high-frequency components, pushing the proposal into \"rough\" directions that are not characteristic of typical draws from $\\mathcal{N}(0, C_n)$, and the M-H correction rightly rejects these moves.\n\nNow, consider the preconditioned Crank-Nicolson (pCN) proposal, $x' = \\sqrt{1 - \\beta^2} x + \\beta \\eta$, with $\\eta \\sim \\mathcal{N}(0, C_n)$. This algorithm is constructed to be compatible with the geometry of the target measure. If the current state $x$ is a draw from $\\mathcal{N}(0, C_n)$, then $x'$ is a linear combination of two independent, centered Gaussian variables with covariance $C_n$. Thus, $x'$ is also a centered Gaussian. Its covariance operator is $\\operatorname{Cov}(x') = (1 - \\beta^2)\\operatorname{Cov}(x) + \\beta^2\\operatorname{Cov}(\\eta) = (1 - \\beta^2)C_n + \\beta^2C_n = C_n$. The proposal mechanism maps a state from the target distribution to another state from the same distribution; it preserves the target measure $\\pi_n = \\mathcal{N}(0, C_n)$. A proposal that preserves the target measure and also satisfies the detailed balance condition, $\\pi_n(x)q(x'|x) = \\pi_n(x')q(x|x')$, will have an acceptance probability of $1$. One can show through direct algebraic verification that the pCN proposal kernel is reversible with respect to $\\pi_n$. The key insight is that the update is constructed using a \"whitened\" representation. If we let $x = C_n^{1/2}y$ where $y \\sim \\mathcal{N}(0, I_n)$, the proposal becomes $C_n^{1/2}y' = \\sqrt{1-\\beta^2}C_n^{1/2}y + \\beta C_n^{1/2}\\tilde{\\xi}$ where $\\tilde{\\xi} \\sim \\mathcal{N}(0, I_n)$. This simplifies to $y' = \\sqrt{1-\\beta^2}y + \\beta\\tilde{\\xi}$, which is a simple rotational update in the whitened space that clearly preserves the standard normal distribution $\\mathcal{N}(0, I_n)$. The pCN proposal effectively works in a coordinate system where the target is isotropic, thereby avoiding the pathologies encountered by MALA. The update step $\\beta\\eta$ is a draw from $\\mathcal{N}(0, \\beta^2 C_n)$, meaning the exploration steps have the same statistical properties as the target measure itself, ensuring they are always \"reasonable\". This makes the algorithm robust and its performance independent of the dimension $n$, a property known as being \"well-posed on the function space\".\n\n(b) The numerical experiment will empirically validate this theoretical reasoning.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a numerical experiment to compare the acceptance rates of\n    MALA and pCN proposals for a Gaussian target on truncated Hilbert spaces.\n    \"\"\"\n    # --- Problem Parameters ---\n    alpha = 1.5\n    epsilon = 1e-2  # MALA step size\n    beta = 0.2      # pCN parameter\n    dims = [16, 64, 256]\n    T = 2000        # Number of trials\n    seed = 12345\n\n    # Initialize random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n\n    results = []\n\n    for n in dims:\n        # --- Setup for dimension n ---\n        \n        # Define eigenvalues and related operators (as vectors for diagonal matrices)\n        k = np.arange(1, n + 1)\n        lambdas = k**(-2 * alpha)\n        lambdas_sqrt = k**(-alpha)\n        lambdas_inv = k**(2 * alpha)\n\n        # --- MALA Simulation ---\n        mala_accept_count = 0\n        for _ in range(T):\n            # 1. Generate an initial state x ~ N(0, C_n)\n            z = rng.normal(size=n)\n            x = lambdas_sqrt * z\n\n            # 2. Propose a new state x' via MALA kernel\n            grad_phi_x = lambdas_inv * x\n            mu_proposal = x - 0.5 * epsilon * grad_phi_x\n            xi = rng.normal(size=n)\n            xp = mu_proposal + np.sqrt(epsilon) * xi\n\n            # 3. Compute Metropolis-Hastings acceptance probability\n            # Log of target density (up to a constant)\n            log_pi_x = -0.5 * np.sum(x * x * lambdas_inv)\n            log_pi_xp = -0.5 * np.sum(xp * xp * lambdas_inv)\n            \n            # Log of proposal density (up to a constant)\n            # Forward: q(x'|x)\n            log_q_xp_given_x = -0.5 / epsilon * np.sum((xp - mu_proposal)**2)\n\n            # Reverse: q(x|x')\n            grad_phi_xp = lambdas_inv * xp\n            mu_reverse = xp - 0.5 * epsilon * grad_phi_xp\n            log_q_x_given_xp = -0.5 / epsilon * np.sum((x - mu_reverse)**2)\n\n            # Log of the full MH ratio\n            log_ratio = (log_pi_xp + log_q_x_given_xp) - (log_pi_x + log_q_xp_given_x)\n\n            # Acceptance probability\n            a_mala = min(1.0, np.exp(log_ratio))\n\n            # 4. Accept or reject\n            if rng.uniform()  a_mala:\n                mala_accept_count += 1\n        \n        mala_acceptance_rate = mala_accept_count / T\n        results.append(mala_acceptance_rate)\n        \n        # --- pCN Result ---\n        # The pCN proposal preserves the Gaussian target,\n        # and the proposal kernel is reversible with respect to the target.\n        # This guarantees an acceptance probability of 1.\n        pcn_acceptance_rate = 1.0\n        results.append(pcn_acceptance_rate)\n\n    # Print the final results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3385118"}]}