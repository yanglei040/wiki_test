{"hands_on_practices": [{"introduction": "A crucial first step in building a hierarchical model is the selection of hyperpriors, especially for scale parameters like noise variance. This exercise explores the popular Half-Cauchy distribution, providing a principled justification for its use as a weakly informative prior that allows the data to speak for itself. By deriving the implied prior on the variance, you will also practice the essential change-of-variables technique, a fundamental skill in probabilistic modeling. [@problem_id:3388823]", "problem": "Consider a linear inverse problem in data assimilation where observations $y \\in \\mathbb{R}^{m}$ are related to an unknown state $x \\in \\mathbb{R}^{n}$ by $y = A x + \\varepsilon$, with $A \\in \\mathbb{R}^{m \\times n}$ known and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$. Suppose a hierarchical Bayesian model is used in which $x \\sim \\mathcal{N}(m_{0}, C_{0})$ with known $m_{0} \\in \\mathbb{R}^{n}$ and positive-definite $C_{0} \\in \\mathbb{R}^{n \\times n}$, and the noise standard deviation $\\sigma$ is assigned a Half-Cauchy hyperprior with a scale parameter $\\tau > 0$. \n\nTask (i): Using foundational principles of hierarchical Bayesian modeling and scale-selection in inverse problems, provide a principled justification that a Half-Cauchy hyperprior on $\\sigma$ is weakly informative in this setting. Your justification should be based on first principles (Bayes' rule, basic properties of priors and likelihoods, and change-of-variables reasoning) and should clearly argue why this hyperprior avoids strong regularization while allowing the data to dominate inference on $\\sigma$ whenever the likelihood is informative.\n\nTask (ii): Derive, from first principles, the implied prior density on the noise variance $v = \\sigma^{2}$ induced by the Half-Cauchy hyperprior on $\\sigma$ with scale $\\tau$. Express your final answer as a single closed-form analytic expression in terms of $v$ and $\\tau$. No numerical approximation is required. If any intermediate transformation is used, justify it from the fundamental change-of-variables rule for probability densities. The final answer must be the analytic expression for the prior density of $v$ and should not include units.", "solution": "The hierarchical model is defined by the following components:\n1.  Likelihood: $p(y|x, \\sigma) \\propto (\\sigma^2)^{-m/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\|y - Ax\\|^2\\right)$, from the assumption $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$.\n2.  Prior on state: $p(x) \\propto \\exp\\left(-\\frac{1}{2}(x-m_0)^T C_0^{-1} (x-m_0)\\right)$, from $x \\sim \\mathcal{N}(m_{0}, C_{0})$.\n3.  Hyperprior on noise standard deviation: $\\sigma \\sim \\text{Half-Cauchy}(\\tau)$, meaning its probability density function (PDF) is $p(\\sigma|\\tau) = \\frac{2}{\\pi\\tau(1 + (\\sigma/\\tau)^2)}$ for $\\sigma > 0$ and $0$ otherwise.\n\nTask (i): Justification of the Half-Cauchy hyperprior as weakly informative.\n\nIn Bayesian inference, the posterior distribution of a parameter is proportional to the product of the likelihood and the prior distribution. For the hyperparameter $\\sigma$, after marginalizing out the state $x$, we have $p(\\sigma|y) \\propto p(y|\\sigma) p(\\sigma|\\tau)$, where $p(y|\\sigma) = \\int p(y|x,\\sigma) p(x) dx$ is the marginal likelihood.\n\nA \"weakly informative\" prior is a proper prior (it integrates to a finite value, typically $1$) that has a minimal influence on the posterior distribution, thereby allowing the evidence from the data, encapsulated in the likelihood, to dominate the inference. This is particularly important for variance or scale parameters like $\\sigma$, as an overly restrictive prior can lead to a posterior that does not accurately reflect the information in the data.\n\nThe Half-Cauchy distribution with scale $\\tau > 0$ for a non-negative parameter $\\sigma$ has the PDF:\n$$p(\\sigma|\\tau) = \\frac{2}{\\pi\\tau\\left(1 + \\frac{\\sigma^2}{\\tau^2}\\right)}, \\quad \\sigma \\ge 0$$\nWe can analyze its properties to justify its status as weakly informative:\n\n1.  **Behavior near the origin**: The PDF has its maximum (mode) at $\\sigma=0$. This encourages shrinkage of the noise parameter towards zero, a desirable form of regularization that favors simpler models (i.e., models with less noise). However, the density is relatively flat near the origin, so it does not exert an excessively strong pull towards $\\sigma=0$, a problem seen with certain other priors.\n\n2.  **Heavy tails**: The most crucial feature of the Cauchy distribution, and thus the Half-Cauchy, is its heavy polynomial tail. As $\\sigma \\to \\infty$, the density decays as:\n    $$p(\\sigma|\\tau) \\propto \\frac{1}{\\sigma^2}$$\n    This slow rate of decay is what makes the prior \"weakly informative\" for large values of $\\sigma$. It assigns a non-trivial amount of prior probability mass to large values of $\\sigma$. The implication is as follows: if the data $y$ are substantially inconsistent with the model predictions $Ax$ under the prior for $x$ (i.e., the misfit $\\|y - Ax\\|$ is large), the likelihood $p(y|\\sigma)$ will be maximized for a large value of $\\sigma$. A prior with a heavy tail, like the Half-Cauchy, will not excessively penalize this large value. The posterior distribution $p(\\sigma|y)$ will therefore be able to place its mass at this large $\\sigma$ value indicated by the data.\n\nIn contrast, a prior with light tails, such as a Half-Normal distribution where $p(\\sigma) \\propto \\exp(-\\sigma^2)$, would decay exponentially fast. Such a prior would heavily penalize large values of $\\sigma$, forcing the posterior to be a compromise between the likelihood's peak and the prior's peak at $\\sigma=0$. This can lead to underestimation of the true noise level if it is large. Similarly, the commonly used Inverse-Gamma prior on the variance $\\sigma^2$, especially with small \"non-informative\" hyperparameters, can be unintentionally informative and can concentrate its mass in a way that biases the posterior.\n\nTherefore, the Half-Cauchy hyperprior on $\\sigma$ is considered a good choice for a weakly informative prior in scale-selection for inverse problems because its heavy tail allows the data to determine the posterior scale of the noise variance, avoiding the strong regularization imposed by light-tailed priors, while still being a proper distribution that regularizes by shrinking small noise values towards zero.\n\nTask (ii): Derivation of the implied prior on the noise variance $v = \\sigma^2$.\n\nWe are given the transformation $v = \\sigma^2$ and the PDF of $\\sigma$, which is the Half-Cauchy density $p_\\Sigma(\\sigma)$. We want to find the PDF of $v$, denoted $p_V(v)$. We use the change-of-variables formula for probability densities.\n\nThe formula states that if $V = g(\\Sigma)$ is a monotonic function, then $p_V(v) = p_\\Sigma(\\sigma(v)) \\left| \\frac{d\\sigma}{dv} \\right|$.\n\n1.  **Define the transformation and its inverse**:\n    The transformation is $v = \\sigma^2$. Since $\\sigma \\ge 0$, this is a one-to-one mapping from $\\sigma \\in [0, \\infty)$ to $v \\in [0, \\infty)$.\n    The inverse transformation is $\\sigma = \\sqrt{v}$.\n\n2.  **Calculate the Jacobian determinant**:\n    We need the derivative of the inverse transformation with respect to $v$:\n    $$\\frac{d\\sigma}{dv} = \\frac{d}{dv}(v^{1/2}) = \\frac{1}{2} v^{-1/2} = \\frac{1}{2\\sqrt{v}}$$\n    Since $v \\ge 0$, the absolute value is $|\\frac{d\\sigma}{dv}| = \\frac{1}{2\\sqrt{v}}$.\n\n3.  **Substitute into the change-of-variables formula**:\n    We have $p_V(v) = p_\\Sigma(\\sqrt{v}) \\cdot \\frac{1}{2\\sqrt{v}}$.\n    The PDF for $\\sigma$ is $p_\\Sigma(\\sigma) = \\frac{2}{\\pi\\tau(1 + (\\sigma/\\tau)^2)}$.\n    Substituting $\\sigma = \\sqrt{v}$:\n    $$p_\\Sigma(\\sqrt{v}) = \\frac{2}{\\pi\\tau\\left(1 + \\frac{(\\sqrt{v})^2}{\\tau^2}\\right)} = \\frac{2}{\\pi\\tau\\left(1 + \\frac{v}{\\tau^2}\\right)}$$\n\n4.  **Combine and simplify**:\n    Now, we multiply by the Jacobian term:\n    $$p_V(v) = \\left( \\frac{2}{\\pi\\tau\\left(1 + \\frac{v}{\\tau^2}\\right)} \\right) \\cdot \\frac{1}{2\\sqrt{v}}$$\n    $$p_V(v) = \\frac{1}{\\pi\\tau\\sqrt{v}\\left(1 + \\frac{v}{\\tau^2}\\right)}$$\n    To present this in a more standard form, we can simplify the denominator:\n    $$p_V(v) = \\frac{1}{\\pi\\tau\\sqrt{v}\\left(\\frac{\\tau^2 + v}{\\tau^2}\\right)}$$\n    $$p_V(v) = \\frac{\\tau^2}{\\pi\\tau\\sqrt{v}(\\tau^2 + v)}$$\n    $$p_V(v) = \\frac{\\tau}{\\pi\\sqrt{v}(v + \\tau^2)}$$\n    This is the implied prior density for the noise variance $v=\\sigma^2$, for $v > 0$. This is a scaled F-distribution, specifically $v \\sim \\tau^2 F(1,1)$.", "answer": "$$\\boxed{\\frac{\\tau}{\\pi\\sqrt{v}(v + \\tau^2)}}$$", "id": "3388823"}, {"introduction": "While convenient, improper hyperpriors can introduce subtle pathologies into a model, and a skilled practitioner must know how to identify them. This practice guides you through an analysis of the marginal likelihood to uncover a critical scaling law that arises from using common scale-invariant hyperpriors. Understanding this property is essential, as it reveals a fundamental non-identifiability that has direct implications for Empirical Bayes procedures. [@problem_id:3388773]", "problem": "Consider a linear Gaussian inverse problem in which the observation vector $y \\in \\mathbb{R}^{m}$ is modeled as $y \\mid x, \\sigma^{2} \\sim \\mathcal{N}(A x, \\sigma^{2} I_{m})$, where $A \\in \\mathbb{R}^{m \\times n}$ is known and $I_{m}$ is the $m \\times m$ identity matrix. The unknown state $x \\in \\mathbb{R}^{n}$ is endowed with a Gaussian prior $x \\mid \\tau \\sim \\mathcal{N}(0, \\tau^{-1} L^{-1})$, where $L \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite. The hyperparameters $(\\sigma^{2}, \\tau)$ are given improper scale-invariant hyperpriors $p(\\sigma^{2}) \\propto (\\sigma^{2})^{-1}$ and $p(\\tau) \\propto \\tau^{-1}$ on $(0, \\infty)$. Let $B := A L^{-1} A^{\\top} \\in \\mathbb{R}^{m \\times m}$, which is symmetric positive semidefinite.\n\nStarting from the definitions of the multivariate Gaussian density and marginal likelihood, and assuming that all integrals are well-defined (for example, by imagining Jeffreys-type hyperpriors truncated on $(\\varepsilon, M)$ with fixed, scale-neutral bounds so that neither $c$ nor $y$ affects the truncation), carry out the following steps:\n\n1. Integrate out $x$ to express the marginal likelihood as a two-dimensional integral over $(\\sigma^{2}, \\tau)$:\n   $$\n   p(y \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} \\left(\\sigma^{2} I_{m} + \\tau^{-1} B \\right)^{-1} y \\right) \\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau},\n   $$\n   identifying clearly the dependence on $y$ and the hyperparameters.\n2. For any fixed $c > 0$, analyze the scaled data $c y$ and perform a change of variables in $(\\sigma^{2}, \\tau)$ that makes the quadratic form in the exponential independent of $c$, and determine how the Jacobian, the determinant factor, and the hyperpriors transform. Derive the scaling law\n   $$\n   p(c y \\mid A, L) \\;=\\; c^{-p} \\, p(y \\mid A, L),\n   $$\n   valid up to a factor that does not depend on $y$ nor on $c$.\n3. State the minimal conditions under which the above scaling derivation is valid, emphasizing the role of the homogeneity of $\\sigma^{2} I_{m} + \\tau^{-1} B$ in $(\\sigma^{2}, \\tau^{-1})$ and the scale-invariance of the hyperpriors. Discuss the implication of this scaling law for Empirical Bayes (EB) procedures, namely that an overall scale in the hierarchical model cannot be identified from the data when using such improper hyperpriors because the marginal likelihood is homogeneous in $y$.\n   \nReport, as your final answer, the exponent $p$ in terms of $m$ only. No numerical evaluation is required. If you provide a numerical value, it will be marked incorrect. The final answer must be given as a single analytical expression with no units.", "solution": "### Part 1: Verification of the Marginal Likelihood Expression\n\nFirst, we derive the expression for the marginal likelihood $p(y \\mid A, L)$. The hierarchical model is defined by:\n1.  Likelihood: $y \\mid x, \\sigma^{2} \\sim \\mathcal{N}(A x, \\sigma^{2} I_{m})$\n2.  Prior: $x \\mid \\tau \\sim \\mathcal{N}(0, \\tau^{-1} L^{-1})$\n3.  Hyperpriors: $p(\\sigma^{2}) \\propto (\\sigma^{2})^{-1}$ and $p(\\tau) \\propto \\tau^{-1}$\n\nTo find the marginal likelihood of the data $y$ given the model structure ($A, L$), we must integrate out the latent variable $x$ and the hyperparameters $(\\sigma^2, \\tau)$. We first integrate over $x$ to find the distribution of $y$ conditional on the hyperparameters, $p(y \\mid \\sigma^2, \\tau)$.\n\nThe model can be seen as a linear transformation of a Gaussian random variable $x$ with additive Gaussian noise. The distribution of $y$ is therefore also Gaussian. The mean is:\n$$ \\mathbb{E}[y \\mid \\sigma^2, \\tau] = \\mathbb{E}[Ax + \\epsilon \\mid \\sigma^2, \\tau] = A\\mathbb{E}[x \\mid \\tau] + \\mathbb{E}[\\epsilon \\mid \\sigma^2] = A(0) + 0 = 0 $$\nwhere $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ represents the noise. The covariance is:\n$$ \\text{Cov}(y \\mid \\sigma^2, \\tau) = \\text{Cov}(Ax + \\epsilon) = \\text{Cov}(Ax) + \\text{Cov}(\\epsilon) = A \\text{Cov}(x \\mid \\tau) A^{\\top} + \\text{Cov}(\\epsilon \\mid \\sigma^2) $$\nSubstituting the given covariances $\\text{Cov}(x \\mid \\tau) = \\tau^{-1} L^{-1}$ and $\\text{Cov}(\\epsilon \\mid \\sigma^2) = \\sigma^2 I_m$:\n$$ \\text{Cov}(y \\mid \\sigma^2, \\tau) = A(\\tau^{-1} L^{-1})A^{\\top} + \\sigma^2 I_m = \\tau^{-1} (A L^{-1} A^{\\top}) + \\sigma^2 I_m $$\nUsing the definition $B := A L^{-1} A^{\\top}$, the covariance becomes $\\Sigma_y = \\sigma^2 I_m + \\tau^{-1} B$.\nThus, the distribution of $y$ conditional on the hyperparameters is:\n$$ y \\mid \\sigma^2, \\tau \\sim \\mathcal{N}(0, \\sigma^2 I_m + \\tau^{-1} B) $$\nThe probability density function is:\n$$ p(y \\mid \\sigma^2, \\tau) = (2\\pi)^{-m/2} |\\sigma^2 I_m + \\tau^{-1} B|^{-1/2} \\exp\\left(-\\frac{1}{2} y^\\top (\\sigma^2 I_m + \\tau^{-1} B)^{-1} y\\right) $$\nTo obtain the full marginal likelihood $p(y \\mid A, L)$, we integrate over the hyperpriors for $\\sigma^2$ and $\\tau$:\n$$ p(y \\mid A, L) = \\int_0^\\infty \\int_0^\\infty p(y \\mid \\sigma^2, \\tau) p(\\sigma^2) p(\\tau) \\, d\\sigma^2 d\\tau $$\nSubstituting the densities and dropping the constant factor $(2\\pi)^{-m/2}$ into the proportionality:\n$$ p(y \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} \\left(\\sigma^{2} I_{m} + \\tau^{-1} B \\right)^{-1} y \\right) \\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau} $$\nThis matches the expression provided in the problem statement.\n\n### Part 2: Derivation of the Scaling Law\n\nWe now analyze how the marginal likelihood transforms when the data vector $y$ is scaled by a factor $c > 0$. The marginal likelihood for $cy$ is:\n$$ p(c y \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} (cy)^{\\top} (\\sigma^{2} I_{m} + \\tau^{-1} B )^{-1} (cy) \\right) \\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau} $$\nThe quadratic form in the exponent becomes:\n$$ -\\frac{1}{2} c^2 y^{\\top} (\\sigma^{2} I_{m} + \\tau^{-1} B )^{-1} y $$\nWe perform a change of variables in the integration to absorb the $c^2$ factor. Let the new variables be $(\\sigma'^2, \\tau')$. We seek a transformation such that the argument of the exponential becomes independent of $c$. This requires:\n$$ c^2 (\\sigma^{2} I_{m} + \\tau^{-1} B )^{-1} = (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} $$\nTaking the matrix inverse of both sides gives:\n$$ c^{-2} (\\sigma^{2} I_{m} + \\tau^{-1} B) = \\sigma'^{2} I_{m} + \\tau'^{-1} B $$\nBy matching the coefficients of the linearly independent matrices $I_m$ and $B$, we define the change of variables:\n$$ \\sigma'^{2} = c^{-2} \\sigma^2 \\quad \\implies \\quad \\sigma^2 = c^2 \\sigma'^2 $$\n$$ \\tau'^{-1} = c^{-2} \\tau^{-1} \\quad \\implies \\quad \\tau^{-1} = c^2 \\tau'^{-1} \\quad \\implies \\quad \\tau = c^{-2} \\tau' $$\nThe integration limits for $(\\sigma'^2, \\tau')$ remain $(0, \\infty)$. We now transform each part of the integrand:\n\n1.  **Exponential Term**: By construction, the exponential term becomes:\n    $$ \\exp\\!\\left( -\\frac{1}{2} y^{\\top} (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} y \\right) $$\n\n2.  **Determinant Factor**: We use the property $\\det(kM) = k^d \\det(M)$ for a $d \\times d$ matrix $M$. Here, the matrix is $m \\times m$.\n    $$ \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} = \\left|c^2 \\sigma'^{2} I_{m} + c^2 \\tau'^{-1} B \\right|^{-\\frac{1}{2}} = \\left|c^2 (\\sigma'^{2} I_{m} + \\tau'^{-1} B) \\right|^{-\\frac{1}{2}} $$\n    $$ = \\left( (c^2)^m \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right| \\right)^{-\\frac{1}{2}} = (c^{2m})^{-\\frac{1}{2}} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} = c^{-m} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} $$\n\n3.  **Measure**: The measure is given by the product of the hyperpriors and the differentials, $\\frac{d\\sigma^2}{\\sigma^2} \\frac{d\\tau}{\\tau}$. From the change of variables:\n    $$ \\sigma^2 = c^2 \\sigma'^2 \\implies d\\sigma^2 = c^2 d\\sigma'^2 \\implies \\frac{d\\sigma^2}{\\sigma^2} = \\frac{c^2 d\\sigma'^2}{c^2 \\sigma'^2} = \\frac{d\\sigma'^2}{\\sigma'^2} $$\n    $$ \\tau = c^{-2} \\tau' \\implies d\\tau = c^{-2} d\\tau' \\implies \\frac{d\\tau}{\\tau} = \\frac{c^{-2} d\\tau'}{c^{-2} \\tau'} = \\frac{d\\tau'}{\\tau'} $$\n    The measure is invariant under this transformation: $\\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau} = \\frac{d\\sigma'^{2}}{\\sigma'^{2}} \\frac{d\\tau'}{\\tau'}$.\n\nSubstituting these transformed parts back into the integral for $p(cy \\mid A, L)$:\n$$ p(cy \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} c^{-m} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} y \\right) \\frac{d\\sigma'^{2}}{\\sigma'^{2}} \\frac{d\\tau'}{\\tau'} $$\nThe factor $c^{-m}$ is constant with respect to the integration variables and can be pulled out:\n$$ p(cy \\mid A, L) \\propto c^{-m} \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} y \\right) \\frac{d\\sigma'^{2}}{\\sigma'^{2}} \\frac{d\\tau'}{\\tau'} $$\nThe remaining integral is identical in form to the integral for $p(y \\mid A, L)$, with $(\\sigma'^2, \\tau')$ being dummy variables of integration. Therefore, the integral is proportional to $p(y \\mid A, L)$. This establishes the scaling law:\n$$ p(c y \\mid A, L) \\;=\\; c^{-m} \\, p(y \\mid A, L) $$\nComparing this to the form $p(c y \\mid A, L) = c^{-p} \\, p(y \\mid A, L)$, we identify the exponent as $p=m$.\n\n### Part 3: Conditions and Implications\n\n**Minimal Conditions for Validity:**\nThe derivation of the scaling law relies on two crucial properties of the model setup:\n1.  **Homogeneity of the Data Covariance:** The marginal covariance of the data, $\\Sigma_y(\\sigma^2, \\tau^{-1}) = \\sigma^2 I_m + \\tau^{-1} B$, must be a homogeneous function of the hyperparameters being scaled. In this case, it is homogeneous of degree $1$ in the variables $(\\sigma^2, \\tau^{-1})$. This property allows the scaling factor $c$ in the data to be mapped onto a scaling factor for the hyperparameters, $c^{-2}$, which can then be factored out of the covariance matrix.\n2.  **Scale-Invariance of the Hyperpriors:** The integration measure over the hyperparameter space, here $p(\\sigma^2)p(\\tau)d\\sigma^2 d\\tau \\propto \\frac{d\\sigma^2}{\\sigma^2} \\frac{d\\tau}{\\tau}$, must be invariant under the specific scaling transformation induced by scaling the data. As shown in Part 2, the transformation $(\\sigma^2, \\tau) \\to (c^2 \\sigma'^2, c^{-2} \\tau')$ leaves the measure $\\frac{d\\sigma^2}{\\sigma^2}\\frac{d\\tau}{\\tau}$ unchanged. This is a general property of the Jeffreys prior for scale parameters.\n\n**Implication for Empirical Bayes (EB):**\nEmpirical Bayes procedures often involve estimating hyperparameters by maximizing the marginal likelihood $p(y \\mid \\theta)$ with respect to $\\theta = (\\sigma^2, \\tau)$. The derived scaling law, $p(cy \\mid A, L) = c^{-m} p(y \\mid A, L)$, is a symptom of a fundamental non-identifiability issue when using this model structure with improper scale-invariant priors.\n\nThe issue is that there is an inherent ambiguity between the scale of the data $y$ and the overall scale of the model's variance parameters $(\\sigma^2, \\tau)$. The model defined by `{data $y$, parameters $(\\sigma^2, \\tau)$}` is observationally indistinguishable from a model with `{data $cy$, parameters $(c^2 \\sigma^2, c^{-2} \\tau)$}`. This implies that if $(\\hat{\\sigma}^2, \\hat{\\tau})$ is the maximum a posteriori (or maximum likelihood) estimate for data $y$, then $(c^2\\hat{\\sigma}^2, c^{-2}\\hat{\\tau})$ will be the corresponding estimate for data $cy$.\n\nConsequently, the estimated scale of the hyperparameters is entirely determined by the arbitrary scale (or units) of the measurement vector $y$. The data itself provides no information to fix an absolute scale for $\\sigma^2$ and $\\tau$. This is what is meant by \"an overall scale in the hierarchical model cannot be identified\". The estimation of the *ratio* of noise variance to prior variance (e.g., $\\sigma^2 \\tau$) might be well-determined, but their absolute individual values are not identifiable from the data alone without introducing further information, for example through proper, informative hyperpriors that break the scaling symmetry.\n\nThe final answer required is the exponent $p$.\nBased on the derivation in Part 2, $p=m$.", "answer": "$$\\boxed{m}$$", "id": "3388773"}, {"introduction": "The true power of hierarchical models is revealed when they are applied to complex, high-dimensional inverse problems like image reconstruction. This hands-on problem demonstrates how to build a model that automatically adapts the strength of its regularization, a concept known as adaptive regularization, by placing a hyperprior on the Total Variation (TV) weight. You will derive and implement a complete algorithm to see firsthand how the model learns to apply different levels of smoothing based on the image content. [@problem_id:3388760]", "problem": "Consider a two-dimensional linear inverse problem with periodic boundary conditions. Let the unknown image be denoted by $x \\in \\mathbb{R}^{N_x \\times N_y}$, the observed data by $y \\in \\mathbb{R}^{N_x \\times N_y}$, and the forward operator by $A: \\mathbb{R}^{N_x \\times N_y} \\rightarrow \\mathbb{R}^{N_x \\times N_y}$ defined as periodic convolution with a known blur kernel. Assume additive independent and identically distributed Gaussian noise with zero mean and variance $\\sigma^2$, so the likelihood is $p(y \\mid x) \\propto \\exp\\left(-\\tfrac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 \\right)$.\n\nAdopt an anisotropic total variation prior on $x$ expressed through the discrete forward differences with periodic wrap:\n- For each pixel $(i,j)$, define forward differences $D_x x[i,j] = x[i, (j+1) \\bmod N_y] - x[i,j]$ and $D_y x[i,j] = x[(i+1) \\bmod N_x, j] - x[i,j]$.\n- The anisotropic total variation is $\\mathrm{TV}(x) = \\sum_{i,j} \\left( \\lvert D_x x[i,j] \\rvert + \\lvert D_y x[i,j] \\rvert \\right)$.\n\nModel a hierarchical prior by placing a Laplace prior on each gradient component with a shared scale parameter (the total variation weight) $\\lambda > 0$, and assign a Gamma hyperprior to $\\lambda$:\n- $p(\\nabla x \\mid \\lambda) \\propto \\left(\\tfrac{\\lambda}{2}\\right)^M \\exp\\left( - \\lambda \\, \\mathrm{TV}(x) \\right)$, where $M = 2 N_x N_y$ is the total number of gradient components.\n- $p(\\lambda) = \\mathrm{Gamma}(\\lambda \\mid a, b)$ with shape $a > 0$ and rate $b > 0$, so $\\log p(\\lambda) = (a-1) \\log \\lambda - b \\lambda + \\mathrm{const}$.\n\nYour task is to:\n- Start from Bayes’ rule and the definitions above as the fundamental base. Derive the joint posterior $p(x,\\lambda \\mid y)$ and the conditional distributions needed for a joint maximum a posteriori (MAP) estimator. Do not assume any normalization constants whose dependence on $\\lambda$ is unknown; instead, use the factorization induced by the Laplace prior on gradient components and the Gamma hyperprior.\n- Show that a coordinate-ascent algorithm alternating between optimizing over $x$ and updating $\\lambda$ emerges naturally. For the $x$-update with fixed $\\lambda$, formulate the subproblem as a variational regularization problem minimizing the sum of a data-fidelity term and a scaled anisotropic total variation. For the $\\lambda$-update with fixed $x$, derive the closed-form maximizer of the conditional posterior over $\\lambda$.\n- Design an algorithmic solution that leverages periodic convolution diagonalization in the two-dimensional Discrete Fourier Transform (DFT) domain for the proximal step of the quadratic data-fidelity term, and a primal-dual splitting method for the total variation term. All steps must be implementable using only array operations and fast Fourier transforms.\n- Implement the algorithm as a complete, runnable program that constructs the forward operator $A$ from a discrete Gaussian blur kernel of size $5 \\times 5$ with standard deviation $1.0$, normalized to unit sum, embedded into the periodic convolution on a grid of size $N_x = N_y = 32$. Use the following numerical choices:\n  - Periodic boundary conditions for all operators.\n  - Primal-dual step sizes $\\tau = 0.25$ and $\\sigma = 0.25$ with extrapolation parameter $\\theta = 1$.\n  - Outer iterations $T = 12$ for the coordinate-ascent loop.\n  - Inner primal-dual iterations $K = 60$ per outer iteration.\n  - Hyperprior parameters $a = 1.1$ and $b = 10^{-3}$.\n  - Random seed fixed to $0$ for reproducibility of synthetic noise.\n- Construct three synthetic ground-truth images on the $32 \\times 32$ grid:\n  1. An “edge-dominated” image equal to zero everywhere except for a centered square of side length $16$ with intensity $1$.\n  2. A “smooth-ramp” image defined by $x[i,j] = \\tfrac{i}{N_x-1}$ for all pixel indices $(i,j)$.\n  3. A “constant” image equal to a constant value $0.5$ everywhere.\n- For each ground-truth image, generate data $y = A x_\\mathrm{true} + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ for a specified noise standard deviation $\\sigma$ (express noise levels as decimals).\n- Evaluate the posterior’s ability to adapt to edges versus smooth regions using the following test suite. For each test, run the hierarchical MAP algorithm to obtain $(\\hat{x}, \\hat{\\lambda})$ and compute the stated boolean outcomes:\n  - Test $1$ (happy path): Use noise standard deviation $\\sigma = 0.01$. Reconstruct from the edge-dominated and smooth-ramp images. Let $\\hat{\\lambda}_\\mathrm{edge}$ and $\\hat{\\lambda}_\\mathrm{smooth}$ be the estimated weights, and let $\\mathrm{TV}(\\hat{x}_\\mathrm{edge})$ and $\\mathrm{TV}(\\hat{x}_\\mathrm{smooth})$ be the reconstructed anisotropic total variations. Return the list $[\\hat{\\lambda}_\\mathrm{edge} < \\hat{\\lambda}_\\mathrm{smooth}, \\ \\mathrm{TV}(\\hat{x}_\\mathrm{edge}) > \\mathrm{TV}(\\hat{x}_\\mathrm{smooth})]$.\n  - Test $2$ (boundary condition): Use noise standard deviation $\\sigma = 0.01$. Reconstruct from the constant image and from the edge-dominated image. Return the boolean $\\hat{\\lambda}_\\mathrm{constant} > \\hat{\\lambda}_\\mathrm{edge}$.\n  - Test $3$ (significant edge case versus non-adaptive baseline): Use the edge-dominated image with noise standard deviation $\\sigma = 0.01$. Compare the hierarchical MAP reconstruction to a non-hierarchical baseline that fixes the total variation weight to the hyperprior mean $\\lambda_\\mathrm{fixed} = \\tfrac{a}{b}$. Compute the mean squared error $\\mathrm{MSE}(\\hat{x}) = \\tfrac{1}{N_x N_y} \\lVert \\hat{x} - x_\\mathrm{true} \\rVert_2^2$. Return the boolean $\\mathrm{MSE}_\\mathrm{hierarchical} < \\mathrm{MSE}_\\mathrm{fixed}$.\n- The final output format must be a single line containing the results aggregated over the three tests as a comma-separated Python-style list, where:\n  - Test $1$ contributes a list of two booleans.\n  - Tests $2$ and $3$ contribute single booleans.\n  The final printed line must look like $[\\,[b_{11}, b_{12}], b_2, b_3\\,]$ with each $b$ being either $\\mathrm{True}$ or $\\mathrm{False}$.\n\nNo physical units are involved, and no angle units are needed. All numerical answers must be produced as Python booleans as specified. The program must not require any input and must run to completion using only the stated numerical libraries and random seed.", "solution": "### Hierarchical Bayesian Model Formulation\n\nThe objective is to find a joint maximum a posteriori (MAP) estimate for the unknown image $x$ and the regularization hyperparameter $\\lambda$. The posterior distribution is given by Bayes' rule:\n$$p(x, \\lambda \\mid y) \\propto p(y \\mid x) p(x \\mid \\lambda) p(\\lambda)$$\nThe components of this model are defined as:\n1.  **Likelihood**: The data $y$ is related to the image $x$ via the forward operator $A$ and additive Gaussian noise with variance $\\sigma^2$. The log-likelihood is:\n    $$\\log p(y \\mid x) = -\\frac{1}{2\\sigma^2} \\lVert Ax - y \\rVert_2^2 + C_1$$\n2.  **Prior**: The prior on the image $x$ is specified as a Laplace distribution over its gradient components, weighted by $\\lambda$. This corresponds to an anisotropic Total Variation (TV) prior. With $M = 2N_xN_y$ gradient components, the log-prior is:\n    $$\\log p(x \\mid \\lambda) = M \\log\\lambda - \\lambda \\, \\mathrm{TV}(x) + C_2$$\n    where $\\mathrm{TV}(x) = \\sum_{i,j} (\\lvert (D_x x)_{i,j} \\rvert + \\lvert (D_y x)_{i,j} \\rvert)$.\n3.  **Hyperprior**: The hyperparameter $\\lambda$ is itself given a Gamma distribution, $\\lambda \\sim \\mathrm{Gamma}(a,b)$, with shape $a$ and rate $b$. The log-hyperprior is:\n    $$\\log p(\\lambda) = (a-1)\\log\\lambda - b\\lambda + C_3$$\n\nCombining these terms, the negative log of the joint posterior $J(x, \\lambda) = -\\log p(x, \\lambda \\mid y)$ that we aim to minimize is:\n$$J(x, \\lambda) = \\frac{1}{2\\sigma^2} \\lVert Ax - y \\rVert_2^2 + \\lambda \\, \\mathrm{TV}(x) + b\\lambda - (M+a-1)\\log\\lambda + C_4$$\n\n### Coordinate Ascent for Joint MAP Estimation\n\nA natural approach to minimize $J(x, \\lambda)$ is through a coordinate ascent algorithm, which alternates between optimizing for $x$ while holding $\\lambda$ fixed, and vice-versa.\n\n**1. $x$-subproblem (fixed $\\lambda$):**\nFor a fixed value of the hyperparameter, $\\lambda_k$, the optimization problem for $x$ becomes:\n$$x_{k+1} = \\arg\\min_x \\left\\{ \\frac{1}{2\\sigma^2} \\lVert Ax - y \\rVert_2^2 + \\lambda_k \\mathrm{TV}(x) \\right\\}$$\nThis is a standard convex problem for TV-regularized image reconstruction. It consists of a quadratic data fidelity term and the non-smooth but convex TV regularizer.\n\n**2. $\\lambda$-subproblem (fixed $x$):**\nFor a fixed image estimate, $x_{k+1}$, the optimization problem for $\\lambda$ involves only the terms in $J(x, \\lambda)$ that depend on $\\lambda$:\n$$\\lambda_{k+1} = \\arg\\min_{\\lambda > 0} \\left\\{ \\lambda \\, \\mathrm{TV}(x_{k+1}) + b\\lambda - (M+a-1)\\log\\lambda \\right\\}$$\nThis is a convex function of $\\lambda$. We can find the minimizer by setting its derivative with respect to $\\lambda$ to zero:\n$$\\frac{\\partial}{\\partial\\lambda} \\left[ (\\mathrm{TV}(x_{k+1}) + b)\\lambda - (M+a-1)\\log\\lambda \\right] = \\mathrm{TV}(x_{k+1}) + b - \\frac{M+a-1}{\\lambda} = 0$$\nSolving for $\\lambda$ yields the closed-form update rule:\n$$\\lambda_{k+1} = \\frac{M+a-1}{\\mathrm{TV}(x_{k+1}) + b}$$\nThis update automatically adapts the regularization strength: a high TV value in the current estimate (implying strong edges or noise) leads to a smaller $\\lambda$, relaxing the TV penalty in the next iteration. Conversely, a smooth estimate (low TV) leads to a larger $\\lambda$, enforcing more smoothing.\n\n### Algorithmic Design for the $x$-subproblem\n\nThe $x$-subproblem is of the form $\\min_x G(x) + F(Kx)$, which is amenable to primal-dual splitting methods like the Chambolle-Pock algorithm. We define:\n- $G(x) = \\frac{1}{2\\sigma^2} \\lVert Ax-y \\rVert_2^2$ (smooth, quadratic)\n- $Kx = \\nabla x = (D_y x, D_x x)$ (discrete gradient operator)\n- $F(u) = \\lambda_k \\|u\\|_1$ (non-smooth, separable)\n\nThe iterative updates for the primal variable $x$ and a dual variable $p$ are:\n1.  **Dual Update**: $p^{j+1} = \\mathrm{prox}_{\\sigma_{\\text{pd}} F^*} (p^j + \\sigma_{\\text{pd}} K \\bar{x}^j)$\n2.  **Primal Update**: $x^{j+1} = \\mathrm{prox}_{\\tau G} (x^j - \\tau K^* p^{j+1})$\n3.  **Extrapolation**: $\\bar{x}^{j+1} = x^{j+1} + \\theta(x^{j+1} - x^j)$\n\nThe proximal operators are key:\n- $\\mathrm{prox}_{\\sigma_{\\text{pd}} F^*}$: The convex conjugate of $F(u)=\\lambda_k\\|u\\|_1$ is the indicator function of the $\\ell_\\infty$-ball of radius $\\lambda_k$. The proximal operator is thus a projection onto this ball, which simplifies to an element-wise clipping operation on the dual variable $p$:\n  $$p \\mapsto \\mathrm{clip}(p, -\\lambda_k, \\lambda_k)$$\n- $\\mathrm{prox}_{\\tau G}$: The proximal operator of the quadratic term $G(x)$ is given by:\n  $$\\mathrm{prox}_{\\tau G}(z) = \\arg\\min_x \\left\\{ \\frac{1}{2\\sigma^2}\\lVert Ax-y \\rVert_2^2 + \\frac{1}{2\\tau}\\lVert x-z \\rVert_2^2 \\right\\}$$\n  Due to the periodic boundary conditions, the convolution operator $A$ is diagonalized by the 2D Discrete Fourier Transform (DFT), $\\mathcal{F}$. Let $\\hat{h} = \\mathcal{F}(h)$ be the DFT of the blur kernel $h$. The solution can be computed efficiently in the Fourier domain:\n  $$x = \\mathcal{F}^{-1} \\left( \\frac{\\frac{1}{\\sigma^2}\\overline{\\hat{h}} \\odot \\mathcal{F}(y) + \\frac{1}{\\tau}\\mathcal{F}(z)}{\\frac{1}{\\sigma^2}|\\hat{h}|^2 + \\frac{1}{\\tau}} \\right)$$\n  where $\\odot$ and the fraction bar denote element-wise multiplication and division, respectively, and $\\overline{\\hat{h}}$ is the complex conjugate of $\\hat{h}$. This allows the primal update to be performed with a few FFTs.\n\nThe final algorithm alternates between $T$ outer loops of updating $x$ (via $K$ inner primal-dual iterations) and then updating $\\lambda$ using the closed-form solution.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a hierarchical Bayesian method for 2D deblurring.\n    \"\"\"\n    # ---- 1. Simulation Setup ----\n    N = 32\n    T = 12  # Outer iterations for hierarchical model\n    K = 60  # Inner primal-dual iterations per outer loop\n    a = 1.1\n    b = 1e-3\n    tau = 0.25 # Primal-dual step size\n    sigma_pd = 0.25 # Primal-dual step size\n    theta = 1.0 # Primal-dual extrapolation parameter\n    noise_sigma = 0.01\n\n    rng = np.random.default_rng(0)\n\n    # ---- 2. Helper Functions and Operators ----\n    def create_gaussian_kernel(size, std_dev):\n        \"\"\"Creates a 2D Gaussian kernel.\"\"\"\n        ax = np.arange(-size // 2 + 1., size // 2 + 1.)\n        xx, yy = np.meshgrid(ax, ax)\n        kernel = np.exp(-(xx**2 + yy**2) / (2. * std_dev**2))\n        return kernel / np.sum(kernel)\n\n    def create_x_true(name, n):\n        \"\"\"Generates the ground-truth images.\"\"\"\n        x_true = np.zeros((n, n), dtype=float)\n        if name == \"edge\":\n            s, h = n // 2, n // 4\n            x_true[s-h:s+h, s-h:s+h] = 1.0\n        elif name == \"smooth\":\n            x_true = np.fromfunction(lambda i, j: i / (n - 1), (n, n), dtype=float)\n        elif name == \"constant\":\n            x_true = np.full((n, n), 0.5)\n        return x_true\n\n    def grad(u):\n        \"\"\"Computes the discrete gradient with periodic boundary conditions.\"\"\"\n        dx = np.roll(u, -1, axis=1) - u\n        dy = np.roll(u, -1, axis=0) - u\n        return np.array([dy, dx])\n\n    def div(p):\n        \"\"\"Computes the negative divergence, adjoint of the gradient.\"\"\"\n        px, py = p[1], p[0]\n        div_x = px - np.roll(px, 1, axis=1)\n        div_y = py - np.roll(py, 1, axis=0)\n        return -(div_x + div_y)\n\n    def tv(u):\n        \"\"\"Computes the anisotropic total variation.\"\"\"\n        g = grad(u)\n        return np.sum(np.abs(g[0])) + np.sum(np.abs(g[1]))\n\n    def solve_reconstruction(y_obs, h_fft, n_val, noise_sig, outer_iters, inner_iters, a_hyper, b_hyper, \n                             tau_pd, sigma_pd_val, theta_val, fixed_lambda=None):\n        \"\"\"Solves the reconstruction problem using coordinate ascent and primal-dual.\"\"\"\n        M = 2 * n_val * n_val\n        h_fft_sq = np.abs(h_fft)**2\n        y_fft = np.fft.fft2(y_obs)\n        \n        prox_g_denom = (1 / noise_sig**2) * h_fft_sq + (1 / tau_pd)\n        prox_g_num_y_part = (1 / noise_sig**2) * np.conj(h_fft) * y_fft\n\n        x = np.copy(y_obs)\n        p = np.zeros((2, n_val, n_val))\n\n        if fixed_lambda is not None:\n            lambda_reg = fixed_lambda\n            run_outer_iters = 1\n        else:\n            lambda_reg = a_hyper / b_hyper\n            run_outer_iters = outer_iters\n\n        for _ in range(run_outer_iters):\n            x_bar = np.copy(x)\n            for _ in range(inner_iters): # Inner loop for x-update\n                grad_x_bar = grad(x_bar)\n                p_new = p + sigma_pd_val * grad_x_bar\n                p = np.clip(p_new, -lambda_reg, lambda_reg)\n\n                x_old = np.copy(x)\n                z = x - tau_pd * div(p)\n                z_fft = np.fft.fft2(z)\n                \n                prox_g_num = prox_g_num_y_part + (1/tau_pd) * z_fft\n                x_fft = prox_g_num / prox_g_denom\n                x = np.real(np.fft.ifft2(x_fft))\n                \n                x_bar = x + theta_val * (x - x_old)\n            \n            if fixed_lambda is None: # Lambda-update\n                current_tv = tv(x)\n                lambda_reg = (M + a_hyper - 1) / (b_hyper + current_tv)\n\n        # Final lambda value for the hierarchical case\n        if fixed_lambda is None:\n            current_tv = tv(x)\n            lambda_reg = (M + a_hyper - 1) / (b_hyper + current_tv)\n            \n        return x, lambda_reg\n\n    # ---- 3. Setup Forward Operator ----\n    kernel_size = 5\n    kernel_std = 1.0\n    kernel = create_gaussian_kernel(kernel_size, kernel_std)\n    \n    padded_kernel = np.zeros((N, N))\n    pad_y, pad_x = (N - kernel_size) // 2, (N - kernel_size) // 2\n    padded_kernel[pad_y:pad_y + kernel_size, pad_x:pad_x + kernel_size] = kernel\n    padded_kernel = np.fft.ifftshift(padded_kernel)\n    h_fft = np.fft.fft2(padded_kernel)\n\n    def A(x_in):\n        return np.real(np.fft.ifft2(h_fft * np.fft.fft2(x_in)))\n\n    # ---- 4. Run Test Cases ----\n    \n    # Test 1: Edge-dominated vs. Smooth-ramp\n    x_true_edge = create_x_true(\"edge\", N)\n    y_edge = A(x_true_edge) + rng.normal(0, noise_sigma, (N, N))\n    x_hat_edge, lambda_hat_edge = solve_reconstruction(\n        y_edge, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta)\n    tv_hat_edge = tv(x_hat_edge)\n\n    x_true_smooth = create_x_true(\"smooth\", N)\n    y_smooth = A(x_true_smooth) + rng.normal(0, noise_sigma, (N, N))\n    x_hat_smooth, lambda_hat_smooth = solve_reconstruction(\n        y_smooth, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta)\n    tv_hat_smooth = tv(x_hat_smooth)\n    \n    test1_results = [lambda_hat_edge  lambda_hat_smooth, tv_hat_edge > tv_hat_smooth]\n    \n    # Test 2: Constant vs. Edge-dominated\n    x_true_const = create_x_true(\"constant\", N)\n    y_const = A(x_true_const) + rng.normal(0, noise_sigma, (N, N))\n    _, lambda_hat_const = solve_reconstruction(\n        y_const, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta)\n    \n    test2_result = lambda_hat_const > lambda_hat_edge\n    \n    # Test 3: Hierarchical vs. Fixed Lambda\n    x_hat_hierarchical = x_hat_edge # Re-use from Test 1\n    mse_hierarchical = np.mean((x_hat_hierarchical - x_true_edge)**2)\n\n    lambda_fixed = a / b\n    x_hat_fixed, _ = solve_reconstruction(\n        y_edge, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta, fixed_lambda=lambda_fixed)\n    mse_fixed = np.mean((x_hat_fixed - x_true_edge)**2)\n\n    test3_result = mse_hierarchical  mse_fixed\n    \n    # ---- 5. Final Output ----\n    b11, b12 = test1_results\n    b2 = test2_result\n    b3 = test3_result\n    print(f\"[[{b11}, {b12}], {b2}, {b3}]\")\n\nsolve()\n```", "id": "3388760"}]}