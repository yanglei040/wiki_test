{"hands_on_practices": [{"introduction": "Many high-dimensional problems, particularly those defined on regular grids, feature separable structures that allow operators to be expressed as Kronecker products. This exercise guides you through the process of explicitly constructing a Tensor Train (TT) matrix representation for a general Kronecker product operator, $A = A_1 \\otimes \\cdots \\otimes A_d$.\n\nBy proving that such an operator has TT-ranks of at most one, you will gain a fundamental insight into how tensor networks can elegantly capture this structure and overcome the curse of dimensionality for an important class of problems [@problem_id:3424557].", "problem": "Consider a linear forward operator in a separable, high-dimensional inverse problem, written as the Kronecker product $A = A_1 \\otimes \\cdots \\otimes A_d$, where each $A_k \\in \\mathbb{R}^{m_k \\times n_k}$. Such operators arise in Gaussian Bayesian data assimilation with separable priors and likelihoods and in partial differential equation discretizations on tensor-product grids. You will show that this operator admits a representation in Tensor Train (TT) matrix format and quantify its TT ranks using only core definitions.\n\nStart from the following fundamental bases:\n- The definition of the Kronecker product: for $B \\in \\mathbb{R}^{p \\times q}$ and $C \\in \\mathbb{R}^{r \\times s}$, the Kronecker product $B \\otimes C \\in \\mathbb{R}^{pr \\times qs}$ satisfies $(B \\otimes C)_{(i-1)r+\\alpha,(j-1)s+\\beta} = B_{ij} C_{\\alpha \\beta}$.\n- The definition of a Tensor Train matrix (TT-matrix): a matrix $A \\in \\mathbb{R}^{(m_1 \\cdots m_d) \\times (n_1 \\cdots n_d)}$ is in TT-matrix format if there exist four-way cores $G^{(k)} \\in \\mathbb{R}^{r_{k-1} \\times m_k \\times n_k \\times r_k}$ with $r_0 = r_d = 1$ such that, for multi-indices $i = (i_1,\\dots,i_d)$ and $j = (j_1,\\dots,j_d)$,\n$$\nA_{i,j} \\;=\\; \\sum_{\\alpha_0,\\dots,\\alpha_d} G^{(1)}_{\\alpha_0,i_1,j_1,\\alpha_1} \\cdots G^{(d)}_{\\alpha_{d-1},i_d,j_d,\\alpha_d},\n$$\nwhere the sum is over $\\alpha_0 = 1$ and $\\alpha_d = 1$, and $r_k$ are the TT ranks.\n\nTasks:\n1. Using only these definitions and standard properties of matrix rank and Kronecker products, construct a TT-matrix representation for $A = A_1 \\otimes \\cdots \\otimes A_d$, and prove an a priori bound on each TT rank $r_s$ in terms of the classical matrix ranks of the factors $A_k$. Your argument must not assume any pre-existing TT representations of the factors and must proceed by identifying $A$ with a $2d$-way tensor and analyzing appropriate matricizations whose ranks equal the TT ranks.\n2. Specialize to the case where each $A_k$ is diagonal, i.e., $(A_k)_{i_k j_k} = a^{(k)}_{i_k} \\,\\delta_{i_k j_k}$ with diagonal entries $a^{(k)}_{1},\\dots,a^{(k)}_{n_k}$ and the Kronecker delta $\\delta_{i_k j_k}$. Compute the TT-matrix cores explicitly, in the sense of giving a closed-form expression for the nonzero core entries $G^{(k)}_{1,i_k,j_k,1}$ that reproduces $A$ exactly under the TT contraction.\n\nAnswer specification:\n- Provide your final answer as the single analytic expression for the TT core entries $G^{(k)}_{1,i_k,j_k,1}$ in the diagonal case, expressed in terms of $a^{(k)}_{i_k}$ and $\\delta_{i_k j_k}$.\n- No numerical rounding is required.\n- Do not include units.", "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and internally consistent. It poses a standard, verifiable problem in numerical linear algebra and tensor methods.\n\nThe problem is addressed in two parts as requested.\n\n**Part 1: General TT-Matrix Representation and Rank Bound**\n\nLet $A = A_1 \\otimes \\cdots \\otimes A_d$ be the forward operator, where each $A_k \\in \\mathbb{R}^{m_k \\times n_k}$. The matrix $A$ has dimensions $(\\prod_{k=1}^d m_k) \\times (\\prod_{k=1}^d n_k)$. An element of $A$ is indexed by a row multi-index $i = (i_1, \\dots, i_d)$ and a column multi-index $j = (j_1, \\dots, j_d)$, where $1 \\le i_k \\le m_k$ and $1 \\le j_k \\le n_k$. The definition of the Kronecker product implies that the element $A_{i,j}$ is given by the product of the corresponding elements of the factor matrices:\n$$\nA_{i,j} = \\prod_{k=1}^d (A_k)_{i_k, j_k}\n$$\nThe problem requires constructing a Tensor Train (TT) matrix representation for $A$ and finding a bound on its TT ranks. A matrix $A$ is in TT-matrix format if its elements can be expressed as\n$$\nA_{i,j} = G^{(1)}(i_1, j_1) G^{(2)}(i_2, j_2) \\cdots G^{(d)}(i_d, j_d)\n$$\nwhere $G^{(k)}(i_k, j_k)$ is a matrix of size $r_{k-1} \\times r_k$ with entries $(G^{(k)}(i_k, j_k))_{\\alpha_{k-1}, \\alpha_k} = G^{(k)}_{\\alpha_{k-1}, i_k, j_k, \\alpha_k}$. The scalars $r_k$ for $k=1, \\dots, d-1$ are the TT ranks, and the boundary ranks are fixed to $r_0=r_d=1$.\n\nThe $s$-th TT rank, $r_s$, of the matrix $A$ is defined as the rank of its $s$-th matricization, denoted $\\mathbf{A}_{(s)}$. This matricization is obtained by identifying the matrix $A$ with a $d$-th order tensor $\\mathcal{T}$ of size $(m_1 n_1) \\times \\dots \\times (m_d n_d)$ whose elements are $\\mathcal{T}_{(i_1,j_1), \\dots, (i_d,j_d)} = A_{i,j}$. The matrix $\\mathbf{A}_{(s)}$ is the unfolding of $\\mathcal{T}$ that groups the first $s$ modes against the remaining $d-s$ modes.\n\nThe matrix $\\mathbf{A}_{(s)}$ has dimensions $(\\prod_{k=1}^s m_k n_k) \\times (\\prod_{k=s+1}^d m_k n_k)$. Its rows are indexed by the multi-index $((i_1,j_1), \\dots, (i_s,j_s))$ and its columns by $((i_{s+1},j_{s+1}), \\dots, (i_d,j_d))$. An element of this matrix is given by:\n$$\n(\\mathbf{A}_{(s)})_{\\left((i_1,j_1), \\dots, (i_s,j_s)\\right), \\left((i_{s+1},j_{s+1}), \\dots, (i_d,j_d)\\right)} = A_{i,j} = \\left(\\prod_{k=1}^s (A_k)_{i_k, j_k}\\right) \\left(\\prod_{k=s+1}^d (A_k)_{i_k, j_k}\\right)\n$$\nThis structure shows that the matrix $\\mathbf{A}_{(s)}$ is the outer product of two vectors, $u \\in \\mathbb{R}^{\\prod_{k=1}^s m_k n_k}$ and $v \\in \\mathbb{R}^{\\prod_{k=s+1}^d m_k n_k}$. The components of these vectors are:\n$$\nu_{(i_1,j_1), \\dots, (i_s,j_s)} = \\prod_{k=1}^s (A_k)_{i_k, j_k}\n$$\n$$\nv_{(i_{s+1},j_{s+1}), \\dots, (i_d,j_d)} = \\prod_{k=s+1}^d (A_k)_{i_k, j_k}\n$$\nThese vectors can be identified with the vectorized forms of the corresponding partial Kronecker products, i.e., $u = \\mathrm{vec}(A_1 \\otimes \\cdots \\otimes A_s)$ and $v = \\mathrm{vec}(A_{s+1} \\otimes \\cdots \\otimes A_d)$.\nThe matrix $\\mathbf{A}_{(s)}$ is thus given by $\\mathbf{A}_{(s)} = u v^T$. The rank of an outer product of two vectors is at most $1$. If neither $u$ nor $v$ is the zero vector (i.e., if none of the matrices $A_k$ are zero matrices), the rank is exactly $1$. Therefore, the $s$-th TT rank is:\n$$\nr_s = \\mathrm{rank}(\\mathbf{A}_{(s)}) \\le 1\n$$\nThis provides the a priori bound $r_s \\le 1$ for all $s=1, \\dots, d-1$. This bound is the tightest possible and, being a constant, does not depend on the classical matrix ranks of the factors $A_k$.\n\nWith all TT ranks being $1$ (i.e., $r_0=r_1=\\dots=r_d=1$), the TT cores $G^{(k)}$ are tensors of size $1 \\times m_k \\times n_k \\times 1$. The TT-matrix product simplifies to a scalar product:\n$$\nA_{i,j} = G^{(1)}_{1,i_1,j_1,1} \\, G^{(2)}_{1,i_2,j_2,1} \\cdots G^{(d)}_{1,i_d,j_d,1}\n$$\nTo construct the representation, we can choose the core entries to be the elements of the factor matrices themselves:\n$$\nG^{(k)}_{1,i_k,j_k,1} = (A_k)_{i_k, j_k}\n$$\nWith this choice, the product becomes $\\prod_{k=1}^d (A_k)_{i_k, j_k}$, which is precisely the expression for $A_{i,j}$. This provides a constructive proof for the TT representation and confirms the rank bound.\n\n**Part 2: Specialization to Diagonal Factors**\n\nWe are given the special case where each factor matrix $A_k$ is diagonal:\n$$\n(A_k)_{i_k, j_k} = a^{(k)}_{i_k} \\delta_{i_k, j_k}\n$$\nwhere $a^{(k)}_{i_k}$ are the diagonal entries and $\\delta_{i_k,j_k}$ is the Kronecker delta.\n\nTo find the explicit form of the TT-matrix cores for this case, we use the general construction derived in Part 1. The ranks are all $1$, and the core entries are given by the elements of the factor matrices. Substituting the specific form of $(A_k)_{i_k, j_k}$ into our general core definition yields:\n$$\nG^{(k)}_{1,i_k,j_k,1} = a^{(k)}_{i_k} \\delta_{i_k,j_k}\n$$\nThis expression provides the entries for the TT-cores of size $1 \\times m_k \\times n_k \\times 1$. The core $G^{(k)}$ is a diagonal \"matrix of matrices,\" where for each $(i_k, j_k)$ with $i_k \\ne j_k$, the entry is $0$, and for $i_k = j_k$, the entry is $a^{(k)}_{i_k}$. This is the required closed-form expression.", "answer": "$$\n\\boxed{a^{(k)}_{i_k} \\delta_{i_k j_k}}\n$$", "id": "3424557"}, {"introduction": "The true power of tensor formats is realized when performing computations, where the goal is to work with high-dimensional objects without ever forming them explicitly. This exercise explores the computational cost of applying a Kronecker sum operator, $\\mathcal{L} = \\sum_{k=1}^{d} I \\otimes \\cdots \\otimes L_k \\otimes \\cdots \\otimes I$, to a vector represented in the TT format.\n\nDeriving the precise floating-point operation (flop) count reveals that the complexity scales favorably with the problem's dimensions and ranks. This analysis demonstrates the dramatic computational advantage of tensor-based algorithms over methods that rely on manipulating exponentially large matrices and vectors [@problem_id:3424565].", "problem": "Consider a high-dimensional Bayesian inverse problem in which the negative log-prior precision operator on a tensorized parameter field is a Kronecker sum\n$$\n\\mathcal{L} \\;=\\; \\sum_{k=1}^{d} I \\otimes \\cdots \\otimes L_k \\otimes \\cdots \\otimes I,\n$$\nwhere each $L_k \\in \\mathbb{R}^{n_k \\times n_k}$ is a dense matrix acting along mode $k$, and $I$ denotes the identity matrix of appropriate size. The parameter vector $x$ is represented in Tensor Train (TT) format with cores $\\mathcal{X}^{(k)} \\in \\mathbb{R}^{r_{k-1} \\times n_k \\times r_k}$ for $k \\in \\{1,\\ldots,d\\}$, with boundary ranks $r_0 = r_d = 1$. You may assume the standard floating point operation (flop) model in which each scalar addition or multiplication counts as one flop.\n\nStarting from the definitions of the Kronecker sum, the Tensor Train format, and the basic dense matrix-vector multiplication cost, derive a closed-form expression for the total flop count required to compute the action $y = \\mathcal{L} x$ by contracting each $L_k$ with the mode-$k$ fibers of $x$. Ignore the cost of adding the $d$ contributions and any tensor-train rounding or recompression, and ignore any costs associated with applying identities. Express your answer solely in terms of $\\{n_k\\}_{k=1}^{d}$ and $\\{r_k\\}_{k=0}^{d}$, and give the final expression as a single analytic formula. No numerical evaluation is required, and no units are involved. The final answer must be a single analytic expression.", "solution": "The problem asks for the total floating-point operation (flop) count to compute the action of a Kronecker sum operator $\\mathcal{L}$ on a vector $x$ represented in the Tensor Train (TT) format. We are given the structure of the operator and the representation of the vector, and we must derive the cost based on fundamental computational steps.\n\nFirst, we analyze the structure of the computation $y = \\mathcal{L}x$. The operator $\\mathcal{L}$ is a sum of $d$ operators:\n$$\n\\mathcal{L} \\;=\\; \\sum_{k=1}^{d} \\mathcal{L}_k, \\quad \\text{where} \\quad \\mathcal{L}_k \\;=\\; I \\otimes \\cdots \\otimes I \\otimes L_k \\otimes I \\otimes \\cdots \\otimes I\n$$\nIn the expression for $\\mathcal{L}_k$, the matrix $L_k \\in \\mathbb{R}^{n_k \\times n_k}$ is at the $k$-th position in the Kronecker product, and all other matrices are identities $I$ of appropriate sizes.\n\nThe total computation is thus $y = \\sum_{k=1}^{d} \\mathcal{L}_k x$. The problem statement instructs us to ignore the cost of adding the $d$ resulting terms. Therefore, the total flop count is the sum of the costs of computing each term $y_k = \\mathcal{L}_k x$ for $k = 1, \\ldots, d$. Let $C_k$ be the cost of computing $y_k$. The total cost $C_{\\text{total}}$ is then:\n$$\nC_{\\text{total}} = \\sum_{k=1}^{d} C_k\n$$\n\nNext, we determine the cost $C_k$ for a single term $y_k = \\mathcal{L}_k x$. The vector $x$ is given in TT format with cores $\\mathcal{X}^{(j)} \\in \\mathbb{R}^{r_{j-1} \\times n_j \\times r_j}$ for $j=1, \\ldots, d$. The operator $\\mathcal{L}_k$ acts as the identity on all modes except for the $k$-th mode, where it applies the dense matrix $L_k$. A key property of the TT format is that such a mode-$k$ operation can be performed by only modifying the $k$-th TT core, $\\mathcal{X}^{(k)}$. The resulting tensor $y_k$ will have TT cores $\\mathcal{Y}^{(j)}$ where $\\mathcal{Y}^{(j)} = \\mathcal{X}^{(j)}$ for all $j \\neq k$. The problem states to ignore costs associated with applying identities, which formalizes this observation.\n\nThe only computational work required for $y_k$ is to compute the new $k$-th core, which we denote $\\mathcal{Y}^{(k)}$. This is achieved by contracting the core $\\mathcal{X}^{(k)}$ with the matrix $L_k$ along the physical mode (the second mode of the core, of size $n_k$). The core $\\mathcal{X}^{(k)}$ is a three-dimensional tensor with elements $(\\mathcal{X}^{(k)})_{i,j,p}$, where the indices range as $i \\in \\{1, \\ldots, r_{k-1}\\}$, $j \\in \\{1, \\ldots, n_k\\}$, and $p \\in \\{1, \\ldots, r_k\\}$. The matrix $L_k$ has elements $(L_k)_{m,j}$. The elements of the resulting core $\\mathcal{Y}^{(k)}$ are given by the contraction:\n$$\n(\\mathcal{Y}^{(k)})_{i,m,p} = \\sum_{j=1}^{n_k} (L_k)_{m,j} (\\mathcal{X}^{(k)})_{i,j,p}\n$$\nThis operation must be performed for all combinations of the indices $i$, $m$, and $p$.\n\nLet's analyze the flop count for this contraction. For each fixed pair of indices $(i, p)$, the operation is equivalent to a matrix-vector multiplication. The vector is the mode-$2$ fiber $(\\mathcal{X}^{(k)})_{i,:,p}$ of length $n_k$, and the matrix is $L_k \\in \\mathbb{R}^{n_k \\times n_k}$. The cost of multiplying a dense $n_k \\times n_k$ matrix by a vector of length $n_k$ is $n_k^2$ multiplications and $n_k(n_k-1)$ additions. Following the problem's flop model (where each addition or multiplication is one flop), this gives a cost of $n_k^2 + (n_k^2 - n_k) = 2n_k^2 - n_k$ flops for each such matrix-vector product.\n\nAlternatively, we can count the flops for computing each element $(\\mathcal{Y}^{(k)})_{i,m,p}$. The sum $\\sum_{j=1}^{n_k} (L_k)_{m,j} (\\mathcal{X}^{(k)})_{i,j,p}$ involves $n_k$ multiplications and $n_k-1$ additions. This requires a total of $n_k + (n_k-1) = 2n_k - 1$ flops per element of the output core. The total number of elements in the output core $\\mathcal{Y}^{(k)}$ (which has the same dimensions $r_{k-1} \\times n_k \\times r_k$ as the input core) is $r_{k-1} n_k r_k$. Therefore, the total cost $C_k$ is:\n$$\nC_k = (r_{k-1} n_k r_k) \\times (2n_k - 1) = (2n_k^2 - n_k) r_{k-1} r_k\n$$\nThis confirms the cost derived from the matrix-vector product perspective, as there are $r_{k-1}r_k$ such products to compute.\n\nFinally, to find the total flop count $C_{\\text{total}}$, we sum the costs $C_k$ over all modes from $k=1$ to $d$:\n$$\nC_{\\text{total}} = \\sum_{k=1}^{d} C_k = \\sum_{k=1}^{d} (2n_k^2 - n_k) r_{k-1} r_k\n$$\nThe boundary ranks are given as $r_0=1$ and $r_d=1$. The summation correctly incorporates these values for the terms $k=1$ and $k=d$. This formula represents the complete flop count under the specified conditions. It can also be written as $\\sum_{k=1}^{d} n_k(2n_k-1)r_{k-1}r_k$.", "answer": "$$\n\\boxed{\\sum_{k=1}^{d} (2n_k^2 - n_k) r_{k-1} r_k}\n$$", "id": "3424565"}, {"introduction": "Beyond computational efficiency, tensor decompositions provide a framework for analyzing the intrinsic complexity of a high-dimensional model. This practice explores the Tucker decomposition, its multilinear ranks $(r_1, \\dots, r_d)$, and its connection to the fundamental inverse problem concept of identifiability.\n\nThe core of the exercise is a parameter-counting argument to determine the manifold dimension of tensors with a fixed Tucker rank structure. This result establishes the theoretical minimum number of measurements needed for unique recovery, offering crucial guidance for designing experiments and understanding the fundamental limits of inference [@problem_id:3424546].", "problem": "Consider a high-dimensional inverse problem in data assimilation where an unknown order-$d$ tensor $\\,\\mathcal{X}\\in\\mathbb{R}^{n_{1}\\times n_{2}\\times\\cdots\\times n_{d}}\\,$ represents a discretized state field over a $d$-dimensional grid. Assume that $\\,\\mathcal{X}\\,$ admits a Tucker representation with multilinear ranks $\\,\\mathbf{r}=(r_{1},r_{2},\\ldots,r_{d})\\,$, that is, there exist factor matrices $\\,U^{(k)}\\in\\mathbb{R}^{n_{k}\\times r_{k}}\\,$ with full column rank and a core tensor $\\,\\mathcal{G}\\in\\mathbb{R}^{r_{1}\\times r_{2}\\times\\cdots\\times r_{d}}\\,$ such that\n$$\n\\mathcal{X}=\\mathcal{G}\\times_{1}U^{(1)}\\times_{2}U^{(2)}\\cdots\\times_{d}U^{(d)}.\n$$\nHere $\\,\\times_{k}\\,$ denotes the mode-$k$ tensor-matrix product. Let $\\,\\mathcal{X}_{(k)}\\,$ denote the mode-$k$ matricization of $\\,\\mathcal{X}\\,$. \n\nPart A. Using only the definitions of the Tucker model, mode-$k$ product, and mode-$k$ matricization, derive how the column space of $\\,\\mathcal{X}_{(k)}\\,$ depends on $\\,U^{(k)}\\,$ and explain why, for a generic core $\\,\\mathcal{G}\\,$ with full multilinear rank, the Tucker rank $\\,r_{k}\\,$ equals $\\,\\mathrm{rank}(\\mathcal{X}_{(k)})\\,$.\n\nPart B. Now consider linear observations $\\,y=A\\,\\mathrm{vec}(\\mathcal{X})+e\\,$ where $\\,A\\in\\mathbb{R}^{m\\times N}\\,$ with $\\,N=\\prod_{i=1}^{d}n_{i}\\,$, $\\,\\mathrm{vec}(\\cdot)\\,$ is the vectorization operation, and $\\,e\\,$ is a small perturbation modeling measurement error. Suppose $\\,A\\,$ is drawn at random with independent and identically distributed (i.i.d.) Gaussian entries, and focus on noiseless local identifiability in the limit $\\,e\\to 0\\,$: the goal is to ensure that, in a neighborhood of a generic $\\,\\mathcal{X}\\,$ with fixed multilinear ranks $\\,\\mathbf{r}\\,$, the map $\\,\\mathcal{X}\\mapsto A\\,\\mathrm{vec}(\\mathcal{X})\\,$ is injective when restricted to the manifold of tensors of fixed multilinear rank $\\,\\mathbf{r}\\,$. Using first-principles parameter counting based on the Tucker model and accounting for non-uniqueness under the action of the General Linear group (GL), determine the minimal number of measurements $\\,m_{\\min}\\,$ required for generic local identifiability, expressed in closed form as a function of $\\,\\{n_{k}\\}\\,$ and $\\,\\{r_{k}\\}\\,$.\n\nProvide your final answer as a single closed-form analytic expression for $\\,m_{\\min}\\,$. No rounding is required. No units are required.", "solution": "The problem consists of two parts. Part A involves deriving a fundamental property of the Tucker decomposition related to its matricization. Part B requires calculating the minimum number of measurements for generic local identifiability of a tensor with a fixed Tucker rank structure, based on first-principles parameter counting.\n\n### Part A: Column Space of Matricized Tensors and Tucker Rank\n\nLet $\\mathcal{X} \\in \\mathbb{R}^{n_{1}\\times n_{2}\\times\\cdots\\times n_{d}}$ be an order-$d$ tensor with a Tucker decomposition given by\n$$ \\mathcal{X}=\\mathcal{G}\\times_{1}U^{(1)}\\times_{2}U^{(2)}\\cdots\\times_{d}U^{(d)} $$\nwhere $\\mathcal{G} \\in \\mathbb{R}^{r_{1}\\times r_{2}\\times\\cdots\\times r_{d}}$ is the core tensor and $U^{(k)} \\in \\mathbb{R}^{n_{k}\\times r_{k}}$ are the factor matrices. The mode-$k$ matricization (or unfolding) of $\\mathcal{X}$, denoted by $\\mathcal{X}_{(k)}$, is a matrix of size $n_{k} \\times \\left(\\prod_{j \\neq k} n_{j}\\right)$. A key property of the mode-$k$ product and matricization is that they interact according to the following formula:\n$$ \\mathcal{X}_{(k)} = U^{(k)} \\mathcal{G}_{(k)} \\left(U^{(d)} \\otimes U^{(d-1)} \\otimes \\cdots \\otimes U^{(k+1)} \\otimes U^{(k-1)} \\otimes \\cdots \\otimes U^{(1)}\\right)^T $$\nwhere $\\otimes$ denotes the Kronecker product of matrices.\n\nFrom this expression, we can analyze the column space of $\\mathcal{X}_{(k)}$. The formula shows that $\\mathcal{X}_{(k)}$ is a product of the matrix $U^{(k)}$ and another matrix, let's call it $M$:\n$$ M = \\mathcal{G}_{(k)} \\left(U^{(d)} \\otimes \\cdots \\otimes U^{(1)}\\right)^T $$\nBy the definition of matrix multiplication, every column of $\\mathcal{X}_{(k)}$ is a linear combination of the columns of $U^{(k)}$. Therefore, the column space of $\\mathcal{X}_{(k)}$ must be a subspace of the column space of $U^{(k)}$:\n$$ \\mathrm{col}(\\mathcal{X}_{(k)}) \\subseteq \\mathrm{col}(U^{(k)}) $$\n\nTo establish equality between these two spaces, we must show that they have the same dimension. The dimension of $\\mathrm{col}(U^{(k)})$ is $\\mathrm{rank}(U^{(k)})$. By assumption, $U^{(k)}$ has full column rank, so $\\mathrm{rank}(U^{(k)}) = r_{k}$. We need to show that for a generic core tensor $\\mathcal{G}$, $\\mathrm{rank}(\\mathcal{X}_{(k)}) = r_{k}$.\n\nThe rank of $\\mathcal{X}_{(k)}$ is given by $\\mathrm{rank}(U^{(k)}M)$. Since $U^{(k)}$ is a matrix of size $n_{k} \\times r_{k}$ with full column rank, it acts as an injective linear map on vectors in $\\mathbb{R}^{r_k}$. Thus, $\\mathrm{rank}(U^{(k)}M) = \\mathrm{rank}(M)$. We now need to determine the rank of $M$.\n\nThe matrix $M$ is a product of $\\mathcal{G}_{(k)}$ and the transpose of a large Kronecker product matrix.\n1.  The core tensor $\\mathcal{G}$ is assumed to be generic with full multilinear rank. This implies that each of its matricizations, $\\mathcal{G}_{(k)} \\in \\mathbb{R}^{r_{k}\\times\\prod_{j\\neq k}r_{j}}$, has full rank. This means $\\mathrm{rank}(\\mathcal{G}_{(k)}) = \\min(r_{k}, \\prod_{j\\neq k} r_{j})$. For a minimal Tucker decomposition, we generally have $r_{k} \\le \\prod_{j\\neq k} r_{j}$, so $\\mathrm{rank}(\\mathcal{G}_{(k)}) = r_{k}$.\n2.  The factor matrices $U^{(j)}$ for $j \\neq k$ are also assumed to be generic and have full column rank $r_{j}$. The Kronecker product of full-rank matrices is a full-rank matrix. Let $K = U^{(d)} \\otimes \\cdots \\otimes U^{(1)}$ (excluding $U^{(k)}$). The matrix $K$ has size $(\\prod_{j \\neq k} n_{j}) \\times (\\prod_{j \\neq k} r_{j})$ and full column rank $\\prod_{j \\neq k} r_{j}$. Consequently, its transpose $K^T$ has full row rank, i.e., $\\mathrm{rank}(K^T) = \\prod_{j \\neq k} r_{j}$.\n\nThe matrix $M = \\mathcal{G}_{(k)} K^T$ is a product of a matrix of rank $r_{k}$ and a matrix of rank $\\prod_{j \\neq k} r_{j}$. For generic matrices, the rank of their product is the maximum possible value, which is limited by the smallest dimension in the chain of multiplications. In this case, $\\mathrm{rank}(M) = \\mathrm{rank}(\\mathcal{G}_{(k)})$ for generic $K^T$. Since $\\mathrm{rank}(\\mathcal{G}_{(k)}) = r_k$, we have $\\mathrm{rank}(M) = r_k$.\n\nTherefore, $\\mathrm{rank}(\\mathcal{X}_{(k)}) = \\mathrm{rank}(M) = r_{k}$. This dimension matches the dimension of $\\mathrm{col}(U^{(k)})$. Since $\\mathrm{col}(\\mathcal{X}_{(k)})$ is a subspace of $\\mathrm{col}(U^{(k)})$ and both have the same finite dimension $r_{k}$, the subspaces must be equal. The quantity $r_k = \\mathrm{rank}(\\mathcal{X}_{(k)})$ is the definition of the mode-$k$ rank, or Tucker rank, of the tensor $\\mathcal{X}$.\n\n### Part B: Minimal Number of Measurements for Identifiability\n\nThe problem asks for the minimal number of measurements $m_{\\min}$ required for generic local identifiability of a tensor $\\mathcal{X}$ with fixed multilinear ranks $\\mathbf{r}=(r_1, \\ldots, r_d)$. The observation model is linear, $y=A\\,\\mathrm{vec}(\\mathcal{X})$, where $A$ is a random matrix with i.i.d. Gaussian entries. In this context, \"generic local identifiability\" means that the linear map $\\mathcal{X} \\mapsto A\\,\\mathrm{vec}(\\mathcal{X})$, when restricted to the manifold of tensors of rank $\\mathbf{r}$, is injective. For a random projection matrix $A$, this is true with probability $1$ if and only if the number of measurements $m$ is at least the dimension of the manifold. Thus, our task reduces to calculating the dimension of the manifold of tensors with fixed Tucker rank $\\mathbf{r}$, which we denote by $\\mathcal{M}_{\\mathbf{r}}$.\n\nWe use first-principles parameter counting. A tensor $\\mathcal{X} \\in \\mathcal{M}_{\\mathbf{r}}$ is parameterized by its Tucker factors: the core tensor $\\mathcal{G} \\in \\mathbb{R}^{r_1 \\times \\cdots \\times r_d}$ and the factor matrices $U^{(k)} \\in \\mathbb{R}^{n_k \\times r_k}$ for $k=1, \\ldots, d$.\n\n1.  **Count the total number of parameters:**\n    The number of parameters in the core tensor $\\mathcal{G}$ is $\\prod_{k=1}^d r_k$.\n    The number of parameters in the factor matrix $U^{(k)}$ is $n_k r_k$.\n    The total number of parameters in the naive parameterization $(\\mathcal{G}, U^{(1)}, \\ldots, U^{(d)})$ is:\n    $$ P = \\left(\\prod_{k=1}^d r_k\\right) + \\sum_{k=1}^d n_k r_k $$\n\n2.  **Account for non-uniqueness (redundancy):**\n    The Tucker decomposition is not unique. For any set of invertible matrices $M_k \\in \\mathrm{GL}(r_k, \\mathbb{R})$, $k=1, \\ldots, d$, the following transformation yields the same tensor $\\mathcal{X}$:\n    $$ \\tilde{U}^{(k)} = U^{(k)} M_k $$\n    $$ \\tilde{\\mathcal{G}} = \\mathcal{G} \\times_1 M_1^{-1} \\times_2 M_2^{-1} \\cdots \\times_d M_d^{-1} $$\n    This defines an equivalence relation on the space of parameters. The manifold $\\mathcal{M}_{\\mathbf{r}}$ is the quotient space of the parameter space under this equivalence. The redundancy is due to the action of the group $G = \\mathrm{GL}(r_1) \\times \\mathrm{GL}(r_2) \\times \\cdots \\times \\mathrm{GL}(r_d)$.\n    The dimension of the redundancy is the dimension of this group. The dimension of the general linear group $\\mathrm{GL}(r, \\mathbb{R})$ is $r^2$. Therefore, the total dimension of the group $G$ is:\n    $$ \\dim(G) = \\sum_{k=1}^d \\dim(\\mathrm{GL}(r_k)) = \\sum_{k=1}^d r_k^2 $$\n\n3.  **Calculate the dimension of the manifold:**\n    The dimension of the manifold $\\mathcal{M}_{\\mathbf{r}}$ is the dimension of the total parameter space minus the dimension of the redundancy, assuming the group action is free for generic points (i.e., the stabilizer of a generic point is trivial). A point in the parameter space is a tuple $(\\mathcal{G}, U^{(1)}, \\ldots, U^{(d)})$. The stabilizer of this point consists of all $(M_1, \\ldots, M_d) \\in G$ that leave it unchanged. The condition $\\tilde{U}^{(k)} = U^{(k)}$ becomes $U^{(k)}M_k = U^{(k)}$. Since $U^{(k)}$ is assumed to have full column rank, we can left-multiply by its pseudoinverse to get $M_k = I_{r_k}$, the identity matrix. If all $M_k$ are identity matrices, the transformation on $\\mathcal{G}$ is also trivial. Thus, the stabilizer is the trivial group $\\{(I, \\ldots, I)\\}$, which has dimension $0$.\n    The dimension of the manifold is then:\n    $$ \\dim(\\mathcal{M}_{\\mathbf{r}}) = P - \\dim(G) = \\left(\\left(\\prod_{k=1}^d r_k\\right) + \\sum_{k=1}^d n_k r_k\\right) - \\sum_{k=1}^d r_k^2 $$\n    Rearranging the terms, we get:\n    $$ \\dim(\\mathcal{M}_{\\mathbf{r}}) = \\sum_{k=1}^d (n_k r_k - r_k^2) + \\prod_{k=1}^d r_k $$\n\nThe minimal number of measurements $m_{\\min}$ required for generic local identifiability is equal to the dimension of this manifold.\n$$ m_{\\min} = \\sum_{k=1}^d (n_k r_k - r_k^2) + \\prod_{k=1}^d r_k $$\nThis expression provides the number of degrees of freedom in a tensor of multilinear rank $\\mathbf{r}$.", "answer": "$$\\boxed{\\sum_{k=1}^{d} \\left(n_{k} r_{k} - r_{k}^{2}\\right) + \\prod_{k=1}^{d} r_{k}}$$", "id": "3424546"}]}