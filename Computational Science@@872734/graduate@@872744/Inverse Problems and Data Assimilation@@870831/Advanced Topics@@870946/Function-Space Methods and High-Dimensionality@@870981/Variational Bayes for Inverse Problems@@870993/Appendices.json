{"hands_on_practices": [{"introduction": "The most effective way to grasp the implications of an approximation is to apply it to a problem where the exact solution is known. This first exercise provides that opportunity within a simple linear-Gaussian model [@problem_id:3430192]. By calculating both the exact, correlated posterior and the factorized mean-field variational approximation, you will derive a closed-form expression for the Kullback-Leibler divergence between them, offering a tangible measure of the information lost due to the simplifying assumption of posterior independence.", "problem": "Consider the linear inverse problem defined by the observation model $y = A x + \\varepsilon$ with $x \\in \\mathbb{R}^2$, where $A \\in \\mathbb{R}^{2 \\times 2}$, $x \\sim \\mathcal{N}(m_0, C_0)$, and $\\varepsilon \\sim \\mathcal{N}(0, R)$. Let the quantities be specified as follows: \n$A = I_2$, \n$$m_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},$$ \n$$C_0 = \\begin{pmatrix} 2 & \\frac{6}{5} \\\\ \\frac{6}{5} & 1 \\end{pmatrix},$$\n$$R = \\frac{1}{2} I_2,$$ \nand the observed data \n$$y = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.$$\n\nUsing Bayes’ rule and the definition of the Gaussian density as the fundamental base, perform the following:\n\n$1.$ Derive the exact posterior distribution $p(x \\mid y)$ and compute its mean and covariance in closed form.\n\n$2.$ Consider a variational family of fully factorized Gaussians $q(x) = \\mathcal{N}(m, \\operatorname{diag}(s_1^2, s_2^2))$. By minimizing the Kullback–Leibler (KL) divergence $\\mathrm{KL}(q \\,\\|\\, p(x \\mid y))$, determine the optimal $m$, $s_1^2$, and $s_2^2$ in closed form.\n\n$3.$ Define the “approximation gap” as the Kullback–Leibler divergence $\\mathrm{KL}(q^\\star \\,\\|\\, p(x \\mid y))$, where $q^\\star$ is the optimal mean-field Gaussian found in part $2$. Provide a single closed-form analytic expression for this value.\n\nReport only the final approximation gap as your final answer. Do not round; provide an exact analytic expression. No units are required.", "solution": "The problem requires a three-part analysis of a linear Bayesian inverse problem: first, finding the exact posterior distribution; second, determining the optimal mean-field variational approximation; and third, calculating the Kullback–Leibler (KL) divergence between the approximation and the true posterior.\n\nThe observation model is given by $y = A x + \\varepsilon$, where $x \\in \\mathbb{R}^2$.\nThe prior on $x$ is a Gaussian distribution: $p(x) = \\mathcal{N}(x \\mid m_0, C_0)$.\nThe noise $\\varepsilon$ is also Gaussian: $\\varepsilon \\sim \\mathcal{N}(0, R)$.\nThis implies the likelihood function is $p(y \\mid x) = \\mathcal{N}(y \\mid Ax, R)$.\n\nThe provided values are:\n$$A = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$$\n$$m_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n$$C_0 = \\begin{pmatrix} 2 & \\frac{6}{5} \\\\ \\frac{6}{5} & 1 \\end{pmatrix}$$\n$$R = \\frac{1}{2} I_2 = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix}$$\n$$y = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$$\n\nThe problem is valid as it is a standard, well-posed problem in Bayesian statistics and machine learning, with all necessary information provided and no internal contradictions or scientific flaws.\n\n**Part 1: Exact Posterior Distribution**\n\nFor a linear-Gaussian model, the posterior distribution $p(x \\mid y)$ is also a Gaussian, which we denote as $\\mathcal{N}(x \\mid m_p, C_p)$. The posterior covariance $C_p$ and mean $m_p$ are given by the standard Bayesian update formulas:\n$$C_p^{-1} = C_0^{-1} + A^T R^{-1} A$$\n$$m_p = C_p (C_0^{-1}m_0 + A^T R^{-1} y)$$\n\nFirst, we compute the inverse matrices $C_0^{-1}$ and $R^{-1}$.\nThe determinant of $C_0$ is $\\det(C_0) = (2)(1) - (\\frac{6}{5})^2 = 2 - \\frac{36}{25} = \\frac{50-36}{25} = \\frac{14}{25}$.\nThe inverse of $C_0$ is:\n$$C_0^{-1} = \\frac{1}{14/25} \\begin{pmatrix} 1 & -\\frac{6}{5} \\\\ -\\frac{6}{5} & 2 \\end{pmatrix} = \\frac{25}{14} \\begin{pmatrix} 1 & -\\frac{6}{5} \\\\ -\\frac{6}{5} & 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{25}{14} & -\\frac{15}{7} \\\\ -\\frac{15}{7} & \\frac{25}{7} \\end{pmatrix}$$\nThe inverse of $R$ is:\n$$R^{-1} = (\\frac{1}{2} I_2)^{-1} = 2 I_2 = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix}$$\n\nNow we can compute the posterior precision matrix $C_p^{-1}$:\n$$C_p^{-1} = C_0^{-1} + A^T R^{-1} A = C_0^{-1} + I_2 (2I_2) I_2 = C_0^{-1} + 2I_2$$\n$$C_p^{-1} = \\begin{pmatrix} \\frac{25}{14} & -\\frac{15}{7} \\\\ -\\frac{15}{7} & \\frac{25}{7} \\end{pmatrix} + \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{25}{14} + \\frac{28}{14} & -\\frac{15}{7} \\\\ -\\frac{15}{7} & \\frac{25}{7} + \\frac{14}{7} \\end{pmatrix} = \\begin{pmatrix} \\frac{53}{14} & -\\frac{15}{7} \\\\ -\\frac{15}{7} & \\frac{39}{7} \\end{pmatrix}$$\n\nNext, we calculate the posterior mean $m_p$. Since $m_0 = 0$, the formula simplifies to $m_p = C_p (A^T R^{-1} y)$.\nLet's first compute the term $A^T R^{-1} y$:\n$$A^T R^{-1} y = I_2 (2I_2) \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}$$\nSo, $m_p = C_p \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}$, which is equivalent to solving the system $C_p^{-1} m_p = \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}$.\nTo find $m_p$, we first need $C_p = (C_p^{-1})^{-1}$. The determinant of $C_p^{-1}$ is:\n$$\\det(C_p^{-1}) = \\left(\\frac{53}{14}\\right)\\left(\\frac{39}{7}\\right) - \\left(-\\frac{15}{7}\\right)^2 = \\frac{2067}{98} - \\frac{225}{49} = \\frac{2067 - 450}{98} = \\frac{1617}{98} = \\frac{33}{2}$$\nThe posterior covariance matrix $C_p$ is:\n$$C_p = \\frac{1}{33/2} \\begin{pmatrix} \\frac{39}{7} & \\frac{15}{7} \\\\ \\frac{15}{7} & \\frac{53}{14} \\end{pmatrix} = \\frac{2}{33} \\begin{pmatrix} \\frac{39}{7} & \\frac{15}{7} \\\\ \\frac{15}{7} & \\frac{53}{14} \\end{pmatrix} = \\frac{1}{231} \\begin{pmatrix} 78 & 30 \\\\ 30 & 53 \\end{pmatrix}$$\nNow we compute the posterior mean $m_p$:\n$$m_p = \\frac{1}{231} \\begin{pmatrix} 78 & 30 \\\\ 30 & 53 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix} = \\frac{1}{231} \\begin{pmatrix} 156 - 60 \\\\ 60 - 106 \\end{pmatrix} = \\frac{1}{231} \\begin{pmatrix} 96 \\\\ -46 \\end{pmatrix} = \\begin{pmatrix} \\frac{32}{77} \\\\ -\\frac{46}{231} \\end{pmatrix}$$\nSo the exact posterior is $p(x \\mid y) = \\mathcal{N}(x \\mid m_p, C_p)$ with $m_p$ and $C_p$ as derived.\n\n**Part 2: Optimal Variational Approximation**\n\nWe consider a variational family of fully factorized Gaussians $q(x) = q_1(x_1)q_2(x_2) = \\mathcal{N}(x \\mid m, \\operatorname{diag}(s_1^2, s_2^2))$. We minimize $\\mathrm{KL}(q \\,\\|\\, p(x \\mid y))$. For a Gaussian posterior $p(x \\mid y) = \\mathcal{N}(x \\mid m_p, C_p)$, the optimal parameters for $q$ that minimize this KL divergence are known to be:\n$1.$ The mean of the variational distribution is equal to the mean of the true posterior: $m^\\star = m_p$.\n$2.$ The variances of the factors are the reciprocals of the diagonal elements of the posterior precision matrix $C_p^{-1}$.\nSo, $s_1^2 = 1 / (C_p^{-1})_{11}$ and $s_2^2 = 1 / (C_p^{-1})_{22}$.\n\nUsing the posterior precision matrix $C_p^{-1}$ from Part 1:\n$$(C_p^{-1})_{11} = \\frac{53}{14} \\quad \\text{and} \\quad (C_p^{-1})_{22} = \\frac{39}{7}$$\nThe optimal variances are:\n$$s_1^2 = \\frac{1}{53/14} = \\frac{14}{53}$$\n$$s_2^2 = \\frac{1}{39/7} = \\frac{7}{39}$$\nThe optimal variational distribution is $q^\\star(x) = \\mathcal{N}(x \\mid m^\\star, C_q^\\star)$, where $m^\\star = m_p$ and $C_q^\\star = \\operatorname{diag}(\\frac{14}{53}, \\frac{7}{39})$.\n\n**Part 3: Approximation Gap**\n\nThe approximation gap is defined as $\\mathrm{KL}(q^\\star \\,\\|\\, p(x \\mid y))$. The general formula for the KL divergence between two multivariate Gaussian distributions, $q = \\mathcal{N}(m_q, C_q)$ and $p = \\mathcal{N}(m_p, C_p)$ of dimension $d$, is:\n$$\\mathrm{KL}(q \\,\\|\\, p) = \\frac{1}{2} \\left[ \\ln\\left(\\frac{\\det C_p}{\\det C_q}\\right) - d + \\operatorname{tr}(C_p^{-1} C_q) + (m_p - m_q)^T C_p^{-1} (m_p - m_q) \\right]$$\nIn our case, $d=2$, $q=q^\\star$, and $p=p(x \\mid y)$. From Part 2, we know $m_q = m_p$, so the term $(m_p - m_q)^T C_p^{-1} (m_p - m_q)$ is zero. The formula simplifies to:\n$$\\mathrm{KL}(q^\\star \\,\\|\\, p) = \\frac{1}{2} \\left[ \\ln\\left(\\frac{\\det C_p}{\\det C_q^\\star}\\right) - 2 + \\operatorname{tr}(C_p^{-1} C_q^\\star) \\right]$$\nLet's compute the trace term. Let $P = C_p^{-1}$. Then $C_q^\\star = \\operatorname{diag}(1/P_{11}, 1/P_{22})$.\n$$\\operatorname{tr}(C_p^{-1} C_q^\\star) = \\operatorname{tr}\\left( \\begin{pmatrix} P_{11} & P_{12} \\\\ P_{21} & P_{22} \\end{pmatrix} \\begin{pmatrix} 1/P_{11} & 0 \\\\ 0 & 1/P_{22} \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & P_{12}/P_{22} \\\\ P_{21}/P_{11} & 1 \\end{pmatrix} \\right) = 1+1=2$$\nThe trace term is exactly equal to the dimension $d$. The KL divergence expression becomes even simpler:\n$$\\mathrm{KL}(q^\\star \\,\\|\\, p) = \\frac{1}{2} \\left[ \\ln\\left(\\frac{\\det C_p}{\\det C_q^\\star}\\right) - 2 + 2 \\right] = \\frac{1}{2} \\ln\\left(\\frac{\\det C_p}{\\det C_q^\\star}\\right)$$\nWe have $\\det C_p = 1/\\det(C_p^{-1})$. The determinant of $C_q^\\star$ is $(\\det C_q^\\star) = s_1^2 s_2^2 = (1/P_{11})(1/P_{22})$.\nSo the ratio of determinants is:\n$$\\frac{\\det C_p}{\\det C_q^\\star} = \\frac{1/\\det(P)}{1/(P_{11}P_{22})} = \\frac{P_{11}P_{22}}{\\det(P)} = \\frac{P_{11}P_{22}}{P_{11}P_{22} - P_{12}^2}$$\nLet's substitute the values for $P_{ij} = (C_p^{-1})_{ij}$:\n$P_{11} = 53/14$, $P_{22} = 39/7$, and $P_{12} = -15/7$.\n$$P_{11}P_{22} = \\left(\\frac{53}{14}\\right)\\left(\\frac{39}{7}\\right) = \\frac{2067}{98}$$\n$$P_{12}^2 = \\left(-\\frac{15}{7}\\right)^2 = \\frac{225}{49}$$\nThe ratio is:\n$$\\frac{P_{11}P_{22}}{P_{11}P_{22} - P_{12}^2} = \\frac{2067/98}{2067/98 - 225/49} = \\frac{2067/98}{(2067-450)/98} = \\frac{2067}{1617}$$\nThis fraction can be simplified by dividing the numerator and denominator by their greatest common divisor, which is $3$.\n$$\\frac{2067 \\div 3}{1617 \\div 3} = \\frac{689}{539}$$\nThe final approximation gap is:\n$$\\mathrm{KL}(q^\\star \\,\\|\\, p(x \\mid y)) = \\frac{1}{2} \\ln\\left(\\frac{689}{539}\\right)$$\nThis can also be expressed as $-\\frac{1}{2} \\ln(1 - \\rho_P^2)$, where $\\rho_P = P_{12}/\\sqrt{P_{11}P_{22}}$ is the correlation coefficient associated with the precision matrix. The non-zero value reflects the information lost by ignoring the posterior correlation between $x_1$ and $x_2$.", "answer": "$$\\boxed{\\frac{1}{2} \\ln\\left(\\frac{689}{539}\\right)}$$", "id": "3430192"}, {"introduction": "Having quantified the mean-field approximation error in a linear setting, we now explore the complexities introduced by nonlinearity. This practice compares the Gaussian Variational Bayes approximation with another cornerstone method, the Laplace approximation, for a simple nonlinear inverse problem [@problem_id:3430120]. By deriving the posterior variance estimates from both methods, you will gain insight into how their differing local assumptions about the posterior landscape can lead to distinct characterizations of uncertainty.", "problem": "Consider a Bayesian inverse problem with scalar unknown $x \\in \\mathbb{R}$, Gaussian prior $x \\sim \\mathcal{N}(0,\\sigma_{0}^{2})$, and observation model $y = G(x) + \\eta$ with nonlinear forward map $G(x) = x^{2}$ and additive Gaussian noise $\\eta \\sim \\mathcal{N}(0,\\sigma^{2})$. Let the negative log-posterior (up to an additive constant) be denoted by $\\Phi(x)$. The maximum a posteriori (MAP) estimator $x_{\\mathrm{MAP}}$ is defined as the minimizer of $\\Phi(x)$. You may assume $y > 0$ and $0 < 2 y < \\sigma^{2}/\\sigma_{0}^{2}$ so that $x_{\\mathrm{MAP}} = 0$ is a local minimizer of $\\Phi(x)$.\n\nStarting from the definitions of the Gaussian prior and Gaussian likelihood, and using only first principles of differentiation and the chain rule, perform the following:\n\n1. Derive $\\Phi(x)$ explicitly and obtain the stationarity condition $\\nabla \\Phi(x) = 0$ to verify that $x = 0$ is a stationary point. Under the stated assumption on $y$, argue that $x_{\\mathrm{MAP}} = 0$ is a local minimizer.\n2. Derive the Hessian $\\nabla^{2}\\Phi(x)$ at $x_{\\mathrm{MAP}}$ and write the Laplace approximation to the posterior as a Gaussian with mean $x_{\\mathrm{MAP}}$ and covariance $\\Sigma_{\\mathrm{Lap}} = [\\nabla^{2}\\Phi(x_{\\mathrm{MAP}})]^{-1}$.\n3. Consider a Gaussian Variational Bayes (VB) approximation $q(x) = \\mathcal{N}(\\mu,v)$ obtained by minimizing the Kullback-Leibler divergence (KL) from $q(x)$ to the exact posterior, under the standard Gaussian VB practice of linearizing $G(x)$ around the variational mean $\\mu$ as $G(x) \\approx G(\\mu) + G'(\\mu)(x-\\mu)$. Using this linearization, derive the implied quadratic surrogate for the likelihood and the corresponding Gaussian posterior approximation, and obtain the VB covariance at $\\mu = x_{\\mathrm{MAP}}$, denoted $\\Sigma_{\\mathrm{VB}}$.\n4. Compute the ratio $\\rho = \\Sigma_{\\mathrm{VB}}/\\Sigma_{\\mathrm{Lap}}$ and express it as a single closed-form function of $\\sigma_{0}^{2}$, $\\sigma^{2}$, and $y$.\n\nYour final answer must be the single analytical expression for $\\rho$. No rounding is required and no units are involved.", "solution": "The problem requires a four-part derivation comparing the Laplace and Variational Bayes (VB) approximations for a specific nonlinear Bayesian inverse problem. We shall proceed step-by-step as requested.\n\nThe prior probability density function (PDF) for $x$ is given by $x \\sim \\mathcal{N}(0, \\sigma_{0}^{2})$, which is $p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}} \\exp\\left(-\\frac{x^2}{2\\sigma_{0}^{2}}\\right)$.\nThe observation model is $y = x^2 + \\eta$ with noise $\\eta \\sim \\mathcal{N}(0, \\sigma^2)$. This defines the likelihood PDF as $p(y|x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - x^2)^2}{2\\sigma^2}\\right)$.\n\nAccording to Bayes' theorem, the posterior PDF is proportional to the product of the likelihood and the prior: $p(x|y) \\propto p(y|x)p(x)$. The negative log-posterior, denoted $\\Phi(x)$, can be written by taking the negative logarithm and dropping any terms that do not depend on $x$:\n$$\n\\Phi(x) = -\\ln(p(y|x)) - \\ln(p(x)) + \\text{const}\n$$\n$$\n\\Phi(x) = \\frac{(y - x^2)^2}{2\\sigma^2} + \\frac{x^2}{2\\sigma_0^2}\n$$\n\n**Part 1: Analysis of the MAP estimate**\n\nTo find the stationary points of $\\Phi(x)$, we compute its first derivative with respect to $x$, which we denote as $\\nabla \\Phi(x)$:\n$$\n\\nabla \\Phi(x) = \\frac{d\\Phi}{dx} = \\frac{1}{2\\sigma^2} \\cdot 2(y - x^2) \\cdot (-2x) + \\frac{2x}{2\\sigma_0^2}\n$$\n$$\n\\nabla \\Phi(x) = -\\frac{2x(y - x^2)}{\\sigma^2} + \\frac{x}{\\sigma_0^2} = x \\left( \\frac{2x^2 - 2y}{\\sigma^2} + \\frac{1}{\\sigma_0^2} \\right)\n$$\nThe stationarity condition is $\\nabla \\Phi(x) = 0$. By inspection, one solution is $x=0$. Thus, $x=0$ is a stationary point.\n\nTo determine the nature of this stationary point, we compute the second derivative of $\\Phi(x)$, the Hessian $\\nabla^2 \\Phi(x)$:\n$$\n\\nabla^2 \\Phi(x) = \\frac{d^2\\Phi}{dx^2} = \\frac{d}{dx} \\left( \\frac{2x^3 - 2xy}{\\sigma^2} + \\frac{x}{\\sigma_0^2} \\right)\n$$\n$$\n\\nabla^2 \\Phi(x) = \\frac{6x^2 - 2y}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\n$$\nWe evaluate the Hessian at the stationary point $x=0$:\n$$\n\\nabla^2 \\Phi(0) = \\frac{6(0)^2 - 2y}{\\sigma^2} + \\frac{1}{\\sigma_0^2} = \\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2}\n$$\nFor $x=0$ to be a local minimizer, the second derivative test requires $\\nabla^2 \\Phi(0) > 0$. This implies:\n$$\n\\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2} > 0 \\implies \\frac{1}{\\sigma_0^2} > \\frac{2y}{\\sigma^2} \\implies 2y < \\frac{\\sigma^2}{\\sigma_0^2}\n$$\nThis is precisely the condition provided in the problem statement. Thus, under the given assumption, $x=0$ is a local minimizer of $\\Phi(x)$, and we identify it as the local Maximum A Posteriori estimate, $x_{\\mathrm{MAP}} = 0$.\n\n**Part 2: Laplace Approximation Covariance $\\Sigma_{\\mathrm{Lap}}$**\n\nThe Laplace approximation models the posterior distribution as a Gaussian centered at the MAP estimate, with a precision (inverse covariance) given by the Hessian of the negative log-posterior evaluated at the MAP estimate.\nFor our scalar case, the precision is $\\Sigma_{\\mathrm{Lap}}^{-1} = \\nabla^2 \\Phi(x_{\\mathrm{MAP}})$. Using our result from Part 1 with $x_{\\mathrm{MAP}} = 0$:\n$$\n\\Sigma_{\\mathrm{Lap}}^{-1} = \\nabla^2 \\Phi(0) = \\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2}\n$$\nThe covariance of the Laplace approximation is the inverse of this precision:\n$$\n\\Sigma_{\\mathrm{Lap}} = \\left( \\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2} \\right)^{-1} = \\left( \\frac{\\sigma^2 - 2y\\sigma_0^2}{\\sigma_0^2\\sigma^2} \\right)^{-1} = \\frac{\\sigma_0^2\\sigma^2}{\\sigma^2 - 2y\\sigma_0^2}\n$$\n\n**Part 3: Variational Bayes Covariance $\\Sigma_{\\mathrm{VB}}$**\n\nThe Gaussian Variational Bayes approximation considers a family of Gaussian distributions $q(x) = \\mathcal{N}(\\mu, v)$ and seeks to minimize the KL divergence $\\mathrm{KL}(q(x) || p(x \\mid y))$. The problem specifies using a linearization of the forward map $G(x) = x^2$ around the variational mean $\\mu$:\n$$\nG(x) \\approx G(\\mu) + G'(\\mu)(x-\\mu)\n$$\nSince $G'(\\mu) = 2\\mu$, the linearization is $G(x) \\approx \\mu^2 + 2\\mu(x-\\mu)$.\nWe substitute this into the likelihood term. The approximate negative log-posterior, which we denote $\\tilde{\\Phi}(x; \\mu)$, then becomes:\n$$\n\\tilde{\\Phi}(x; \\mu) = \\frac{(y - (\\mu^2 + 2\\mu(x-\\mu)))^2}{2\\sigma^2} + \\frac{x^2}{2\\sigma_0^2}\n$$\nThis expression is quadratic in $x$. The precision of the corresponding Gaussian posterior approximation is given by the coefficient of $\\frac{1}{2}x^2$. Expanding the term from the likelihood:\n$$\n\\frac{(y - \\mu^2 + 2\\mu^2 - 2\\mu x)^2}{2\\sigma^2} = \\frac{((y+\\mu^2) - 2\\mu x)^2}{2\\sigma^2} = \\frac{1}{2\\sigma^2}((y+\\mu^2)^2 - 4\\mu(y+\\mu^2)x + 4\\mu^2 x^2)\n$$\nThe term containing $x^2$ is $\\frac{4\\mu^2 x^2}{2\\sigma^2} = \\frac{1}{2}x^2\\left(\\frac{4\\mu^2}{\\sigma^2}\\right)$.\nSumming the quadratic terms from the approximate likelihood and the prior, the total quadratic term in $\\tilde{\\Phi}(x; \\mu)$ is $\\frac{1}{2}x^2\\left(\\frac{4\\mu^2}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)$. The precision of this VB posterior approximation, as a function of the linearization point $\\mu$, is therefore:\n$$\nv^{-1}(\\mu) = \\frac{4\\mu^2}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\n$$\nThe problem asks for the VB covariance evaluated at $\\mu = x_{\\mathrm{MAP}} = 0$. Substituting $\\mu=0$:\n$$\n\\Sigma_{\\mathrm{VB}}^{-1} = v^{-1}(0) = \\frac{4(0)^2}{\\sigma^2} + \\frac{1}{\\sigma_0^2} = \\frac{1}{\\sigma_0^2}\n$$\nThe VB covariance $\\Sigma_{\\mathrm{VB}}$ is the inverse of this precision:\n$$\n\\Sigma_{\\mathrm{VB}} = \\left(\\frac{1}{\\sigma_0^2}\\right)^{-1} = \\sigma_0^2\n$$\nThis result shows that when this specific VB approximation is centered at $\\mu=0$, its covariance reverts to the prior covariance. This is because the derivative of the forward map $G'(0)=0$, so the linearized model shows no dependence on $x$, and the data provide no information about the precision of $x$.\n\n**Part 4: Ratio $\\rho = \\Sigma_{\\mathrm{VB}}/\\Sigma_{\\mathrm{Lap}}$**\n\nWe now compute the ratio of the two derived covariances.\n$$\n\\rho = \\frac{\\Sigma_{\\mathrm{VB}}}{\\Sigma_{\\mathrm{Lap}}} = \\Sigma_{\\mathrm{VB}} \\cdot \\Sigma_{\\mathrm{Lap}}^{-1}\n$$\nSubstituting the expressions we found:\n$$\n\\rho = (\\sigma_0^2) \\cdot \\left(\\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2}\\right)\n$$\nDistributing $\\sigma_0^2$:\n$$\n\\rho = \\sigma_0^2 \\cdot \\frac{1}{\\sigma_0^2} - \\sigma_0^2 \\cdot \\frac{2y}{\\sigma^2}\n$$\n$$\n\\rho = 1 - \\frac{2y\\sigma_0^2}{\\sigma^2}\n$$\nThis is the final closed-form expression for the ratio $\\rho$ as a function of $\\sigma_0^2$, $\\sigma^2$, and $y$. Note that the given condition $0 < 2y < \\sigma^2/\\sigma_0^2$ implies that $0 < \\frac{2y\\sigma_0^2}{\\sigma^2} < 1$, which ensures that $0 < \\rho < 1$. This indicates that for this problem, the Laplace approximation correctly estimates a posterior variance larger than the prior variance (due to the evidence for non-zero $x$), while the VB approximation incorrectly estimates a variance equal to the prior variance.", "answer": "$$\\boxed{1 - \\frac{2y\\sigma_{0}^{2}}{\\sigma^{2}}}$$", "id": "3430120"}, {"introduction": "Our final practice transitions from theoretical analysis to full-scale implementation, addressing a common class of PDE-constrained inverse problems. You will implement a Variational Bayes solution for inferring a distributed parameter field in the Poisson equation, using a Finite Element (FE) discretization [@problem_id:3430113]. This exercise requires you to build the core components of the Bayesian model—from FE matrices to the forward operator—and use them to construct the variational approximation and compute the Evidence Lower Bound (ELBO), solidifying the link between abstract theory and practical computation.", "problem": "Consider an inverse problem for a distributed parameter field in a one-dimensional partial differential equation on the unit interval. Let the spatial domain be $[0,1]$, discretized into $N$ uniform elements with mesh width $h = 1/N$ and nodes $s_j = j h$ for $j \\in \\{0,1,\\dots,N\\}$. The forward model is the weak form of the Poisson equation with homogeneous Dirichlet boundary conditions: find the state $u \\in H_0^1([0,1])$ such that, for all test functions $v \\in H_0^1([0,1])$, one has\n$$\n\\int_0^1 u'(s) v'(s) \\, ds = \\int_0^1 x(s) v(s) \\, ds,\n$$\nwhere $x$ is a distributed parameter field to be inferred from data. Observations are interior nodal values of $u$, collected into a vector $y \\in \\mathbb{R}^m$ with $m = N-1$, with additive independent Gaussian noise of variance $\\sigma^2$, so the likelihood is $p(y \\mid c) = \\mathcal{N}(y \\mid H c, R)$, where $R = \\sigma^2 I_m$ and $c \\in \\mathbb{R}^{n}$ are coefficients of the parameter $x$ in a finite element basis of dimension $n = N+1$.\n\nRepresent the parameter $x$ in a continuous, piecewise linear Finite Element (FE) basis $\\{\\psi_j\\}_{j=0}^N$ as $x(s) \\approx \\sum_{j=0}^N c_j \\psi_j(s)$, and discretize the state $u$ in the same mesh with FE basis $\\{\\phi_i\\}_{i=1}^{N-1}$ that respects homogeneous Dirichlet boundary conditions at $s=0$ and $s=1$. The Galerkin discretization yields a linear system for the interior degrees of freedom of $u$,\n$$\nK_u \\, u = M_{ux} \\, c,\n$$\nwhere $K_u \\in \\mathbb{R}^{(N-1)\\times(N-1)}$ is the stiffness matrix for $u$ and $M_{ux} \\in \\mathbb{R}^{(N-1)\\times(N+1)}$ is the rectangular mass coupling matrix from $x$ to $u$ defined by\n$$\n(K_u)_{ij} = \\int_0^1 \\phi_i'(s)\\phi_j'(s)\\,ds, \\quad (M_{ux})_{ij} = \\int_0^1 \\phi_i(s)\\psi_j(s)\\,ds.\n$$\nLet the observation operator be the identity on the interior nodal values of $u$, so that the observation matrix $P \\in \\mathbb{R}^{m \\times (N-1)}$ is the identity, and the forward map from $c$ to the mean of the observations is\n$$\nH = P \\, K_u^{-1} \\, M_{ux} \\in \\mathbb{R}^{m \\times n}.\n$$\n\nImpose a Gaussian prior on $c$ with zero mean and precision $A_{\\mathrm{prior}} = \\tau K_x + \\delta M_x$, where $K_x \\in \\mathbb{R}^{n \\times n}$ and $M_x \\in \\mathbb{R}^{n \\times n}$ are the standard FE stiffness and mass matrices on the full mesh for the basis $\\{\\psi_j\\}_{j=0}^N$, and $\\tau > 0$, $\\delta > 0$ are fixed hyperparameters. That is, $p(c) = \\mathcal{N}(0, A_{\\mathrm{prior}}^{-1})$. This prior corresponds to a Gaussian Markov Random Field (GMRF) with smoothing controlled by $\\tau$ and local variance control by $\\delta$.\n\nUse Variational Bayes (VB) to approximate the posterior distribution $p(c \\mid y)$ within the family of full-covariance Gaussian densities $q(c) = \\mathcal{N}(m, C)$, where $m \\in \\mathbb{R}^n$ and $C \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite. Start from the foundational definitions:\n- Bayes’ rule for the posterior $p(c \\mid y) \\propto p(y \\mid c) p(c)$.\n- The Kullback–Leibler divergence $\\mathrm{KL}(q \\,\\|\\, p) = \\mathbb{E}_q[\\log q(c)] - \\mathbb{E}_q[\\log p(c \\mid y)]$.\n- The Evidence Lower Bound (ELBO), $\\mathcal{L}(q) = \\mathbb{E}_q[\\log p(y \\mid c)] + \\mathbb{E}_q[\\log p(c)] - \\mathbb{E}_q[\\log q(c)]$, which satisfies $\\log p(y) \\ge \\mathcal{L}(q)$ for any density $q$.\n\nYour tasks:\n- Derive, from first principles and the above definitions, the Gaussian variational approximation $q(c) = \\mathcal{N}(m, C)$ for this linear-Gaussian inverse problem, expressed entirely in terms of the FE system matrices $K_u$, $M_{ux}$, $K_x$, $M_x$, the hyperparameters $\\tau$, $\\delta$, and the noise variance $\\sigma^2$. You must show how to represent $q(c)$ in the FE basis and how to compute all necessary inner products via the assembled FE mass and stiffness matrices, with no use of any pre-derived closed-form posterior identities beyond the definitions given above.\n- Construct the ELBO $\\mathcal{L}(q)$ explicitly in terms of $m$, $C$, the prior precision $A_{\\mathrm{prior}}$, the forward operator $H$, the observation covariance $R$, and the observed data $y$. Your derivation must use only linear algebra, definitions of multivariate Gaussian densities, and standard properties of the FE inner products.\n- Implement a program that assembles the FE matrices for a uniform mesh on $[0,1]$ with continuous piecewise linear basis functions, computes the forward operator $H$, constructs the Gaussian variational approximation $q(c)$, and evaluates the ELBO.\n\nData generation protocol for all test cases:\n- Use the manufactured truth $x_{\\mathrm{true}}(s) = \\sin(2\\pi s)$ for all $s \\in [0,1]$.\n- Represent $x_{\\mathrm{true}}$ in the FE basis by nodal interpolation, i.e., set $c_{\\mathrm{true}, j} = x_{\\mathrm{true}}(s_j)$ for all $j \\in \\{0,\\dots,N\\}$.\n- Generate synthetic observations with zero noise realization, i.e., set $y = H c_{\\mathrm{true}}$ exactly.\n- Use the identity observation operator on the interior nodes, so the observation dimension is $m = N-1$.\n- All angles must be in radians.\n- There are no physical units in this problem; all quantities are non-dimensional.\n\nTest suite:\n- Case A (happy path): $N = 8$, $\\tau = 1.0$, $\\delta = 10^{-2}$, $\\sigma = 0.1$.\n- Case B (small mesh boundary case): $N = 3$, $\\tau = 0.5$, $\\delta = 0.05$, $\\sigma = 0.2$.\n- Case C (high smoothing and high noise): $N = 12$, $\\tau = 5.0$, $\\delta = 10^{-3}$, $\\sigma = 0.5$.\n\nFor each case, compute the ELBO value $\\mathcal{L}(q)$ for the Gaussian variational approximation $q(c)$ constructed as above. Your program must:\n- Assemble $K_u$, $M_{ux}$, $K_x$, $M_x$ from FE inner products.\n- Build $H = K_u^{-1} M_{ux}$, $A_{\\mathrm{prior}} = \\tau K_x + \\delta M_x$, and $R = \\sigma^2 I$.\n- Construct $q(c) = \\mathcal{N}(m, C)$ by minimizing the Kullback–Leibler divergence within the Gaussian family.\n- Evaluate $\\mathcal{L}(q)$ exactly from its defining expectation terms.\n\nFinal output specification:\n- Your program should produce a single line of output containing the three ELBO values for the test cases Case A, Case B, and Case C, in that order, as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places. For example, an output with placeholder numbers would look like $[1.234000,-0.567890,2.000000]$.", "solution": "The problem requires the derivation and implementation of a Variational Bayes (VB) approximation for a linear-Gaussian inverse problem formulated in a Finite Element (FE) framework. The goal is to approximate the posterior distribution $p(c \\mid y)$ with a tractable Gaussian density $q(c) = \\mathcal{N}(c \\mid m, C)$ and to compute the Evidence Lower Bound (ELBO) for this approximation.\n\nFirst, we derive the optimal parameters $m$ and $C$ for the variational distribution $q(c)$ by maximizing the ELBO, $\\mathcal{L}(q)$. The ELBO is defined as:\n$$\n\\mathcal{L}(q) = \\mathbb{E}_q[\\log p(y \\mid c)] + \\mathbb{E}_q[\\log p(c)] - \\mathbb{E}_q[\\log q(c)]\n$$\nThe problem specifies a Gaussian likelihood, $p(y \\mid c) = \\mathcal{N}(y \\mid Hc, R)$, a zero-mean Gaussian prior, $p(c) = \\mathcal{N}(c \\mid 0, A_{\\mathrm{prior}}^{-1})$, and a Gaussian variational family, $q(c) = \\mathcal{N}(c \\mid m, C)$. The log-PDFs, up to additive constants, are:\n$$\n\\log p(y \\mid c) \\propto -\\frac{1}{2}(y - Hc)^T R^{-1} (y - Hc)\n$$\n$$\n\\log p(c) \\propto -\\frac{1}{2}c^T A_{\\mathrm{prior}} c\n$$\n$$\n\\log q(c) \\propto -\\frac{1}{2}(c - m)^T C^{-1} (c - m)\n$$\nTo maximize the ELBO with respect to the variational parameters $m$ and $C$, we can analyze the terms of $\\mathcal{L}(q)$ that depend on them. Combining the log-likelihood and log-prior terms within the expectation gives $\\mathbb{E}_q[\\log p(y,c)] = \\mathbb{E}_q[\\log p(y \\mid c) + \\log p(c)]$. The argument of the expectation is:\n$$\n\\log p(y \\mid c) + \\log p(c) \\propto -\\frac{1}{2} \\left[ (y - Hc)^T R^{-1} (y - Hc) + c^T A_{\\mathrm{prior}} c \\right]\n$$\n$$\n= -\\frac{1}{2} \\left[ y^T R^{-1} y - 2y^T R^{-1} Hc + c^T H^T R^{-1} Hc + c^T A_{\\mathrm{prior}} c \\right]\n$$\n$$\n= -\\frac{1}{2} \\left[ c^T (H^T R^{-1} H + A_{\\mathrm{prior}}) c - 2y^T R^{-1} Hc \\right] + \\text{const.}\n$$\nMaximizing the ELBO is equivalent to minimizing the Kullback-Leibler divergence $\\mathrm{KL}(q \\,\\|\\, p)$, which involves fitting $q(c)$ to the true posterior $p(c \\mid y) \\propto p(y \\mid c) p(c)$. The posterior log-density is quadratic in $c$, implying the posterior is Gaussian. The optimal $q(c)$ will match this posterior exactly. The quadratic and linear terms in $c$ determine the parameters of this Gaussian. The precision (inverse covariance) matrix of the posterior is the coefficient of the quadratic term $c^T(\\cdot)c$, and its mean is related to the linear term.\nFrom the expression above, the precision matrix of the posterior is clearly $C^{-1} = H^T R^{-1} H + A_{\\mathrm{prior}}$.\n\nTo find the mean $m$, we can complete the square for the terms involving $c$. The expression inside the square brackets is of the form $c^T C^{-1} c - 2b^T c$, where $b = H^T R^{-1} y$. Completing the square yields $(c-m)^T C^{-1} (c-m) - m^T C^{-1} m$, where $C^{-1}m = b$.\nTherefore, the optimal mean $m$ satisfies:\n$$\n(H^T R^{-1} H + A_{\\mathrm{prior}}) m = H^T R^{-1} y\n$$\n$$\nm = (H^T R^{-1} H + A_{\\mathrm{prior}})^{-1} H^T R^{-1} y\n$$\nThis defines the optimal variational distribution $q(c) = \\mathcal{N}(m, C)$ where:\n$$\nC = (H^T R^{-1} H + A_{\\mathrm{prior}})^{-1}\n$$\n$$\nm = C (H^T R^{-1} y)\n$$\nThese are the parameters for the Gaussian approximation $q(c)$. Since the true posterior is Gaussian, this variational approximation is exact, i.e., $q(c) = p(c \\mid y)$.\n\nNext, we derive the explicit formula for the ELBO, $\\mathcal{L}(q)$, using the determined optimal parameters. We must evaluate each expectation term in the ELBO definition fully, including all normalization constants.\n\nThe log-PDFs are:\n$$\n\\log p(y \\mid c) = -\\frac{m_{\\mathrm{dim}}}{2} \\log(2\\pi) - \\frac{1}{2} \\log \\det(R) - \\frac{1}{2}(y - Hc)^T R^{-1} (y - Hc)\n$$\n$$\n\\log p(c) = -\\frac{n}{2} \\log(2\\pi) + \\frac{1}{2} \\log \\det(A_{\\mathrm{prior}}) - \\frac{1}{2}c^T A_{\\mathrm{prior}} c\n$$\n$$\n\\log q(c) = -\\frac{n}{2} \\log(2\\pi) - \\frac{1}{2} \\log \\det(C) - \\frac{1}{2}(c - m)^T C^{-1} (c - m)\n$$\nwhere $n=N+1$ is the dimension of the parameter vector $c$ and $m_{\\mathrm{dim}}=N-1$ is the dimension of the observation vector $y$.\n\nWe compute the expectation of each term with respect to $q(c) = \\mathcal{N}(c \\mid m, C)$.\n1. Expectation of the log-likelihood:\nUsing the property $\\mathbb{E}_q[(z-Ac)^T B (z-Ac)] = (z-Am)^T B (z-Am) + \\mathrm{Tr}(A^T B A C)$, we get:\n$$\n\\mathbb{E}_q[\\log p(y \\mid c)] = -\\frac{m_{\\mathrm{dim}}}{2}\\log(2\\pi) - \\frac{1}{2}\\log\\det(R) - \\frac{1}{2} \\left[ (y - Hm)^T R^{-1} (y - Hm) + \\mathrm{Tr}(H^T R^{-1} H C) \\right]\n$$\n2. Expectation of the log-prior:\nUsing the property $\\mathbb{E}_q[c^T A c] = m^T A m + \\mathrm{Tr}(AC)$, we get:\n$$\n\\mathbb{E}_q[\\log p(c)] = -\\frac{n}{2}\\log(2\\pi) + \\frac{1}{2}\\log\\det(A_{\\mathrm{prior}}) - \\frac{1}{2} \\left[ m^T A_{\\mathrm{prior}} m + \\mathrm{Tr}(A_{\\mathrm{prior}} C) \\right]\n$$\n3. Expectation of the negative log-variational density (Entropy of $q$):\n$$\n-\\mathbb{E}_q[\\log q(c)] = \\frac{n}{2}\\log(2\\pi) + \\frac{1}{2}\\log\\det(C) + \\frac{1}{2}\\mathbb{E}_q[(c - m)^T C^{-1} (c - m)]\n$$\nThe expectation term evaluates to $\\mathbb{E}_q[\\mathrm{Tr}(C^{-1}(c-m)(c-m)^T)] = \\mathrm{Tr}(C^{-1}\\mathbb{E}_q[(c-m)(c-m)^T]) = \\mathrm{Tr}(C^{-1}C) = \\mathrm{Tr}(I_n) = n$.\nSo, $-\\mathbb{E}_q[\\log q(c)] = \\frac{n}{2}\\log(2\\pi) + \\frac{1}{2}\\log\\det(C) + \\frac{n}{2}$.\n\nSumming these three components to obtain the ELBO $\\mathcal{L}(q)$:\n$$\n\\mathcal{L}(q) = \\left(-\\frac{m_{\\mathrm{dim}}}{2}\\log(2\\pi) - \\frac{1}{2}\\log\\det R - \\frac{1}{2}(y - Hm)^T R^{-1} (y - Hm) - \\frac{1}{2}\\mathrm{Tr}(H^T R^{-1} H C)\\right)\n$$\n$$\n+ \\left(-\\frac{n}{2}\\log(2\\pi) + \\frac{1}{2}\\log\\det A_{\\mathrm{prior}} - \\frac{1}{2}m^T A_{\\mathrm{prior}} m - \\frac{1}{2}\\mathrm{Tr}(A_{\\mathrm{prior}} C)\\right)\n$$\n$$\n+ \\left(\\frac{n}{2}\\log(2\\pi) + \\frac{1}{2}\\log\\det C + \\frac{n}{2}\\right)\n$$\nCombining terms, the $\\pm \\frac{n}{2}\\log(2\\pi)$ terms cancel. We can also group the trace terms:\n$$\n-\\frac{1}{2}\\mathrm{Tr}(H^T R^{-1} H C) - \\frac{1}{2}\\mathrm{Tr}(A_{\\mathrm{prior}} C) = -\\frac{1}{2}\\mathrm{Tr}((H^T R^{-1} H + A_{\\mathrm{prior}})C) = -\\frac{1}{2}\\mathrm{Tr}(C^{-1}C) = -\\frac{n}{2}\n$$\nThe ELBO simplifies to:\n$$\n\\mathcal{L}(q) = -\\frac{m_{\\mathrm{dim}}}{2}\\log(2\\pi) - \\frac{1}{2}\\log\\det R - \\frac{1}{2}(y - Hm)^T R^{-1} (y - Hm) - \\frac{1}{2}m^T A_{\\mathrm{prior}} m + \\frac{1}{2}\\log\\det A_{\\mathrm{prior}} + \\frac{1}{2}\\log\\det C\n$$\nThis is the final expression for the ELBO that will be implemented. Since $R = \\sigma^2 I_{m_{\\mathrm{dim}}}$, we have $R^{-1} = (1/\\sigma^2)I_{m_{\\mathrm{dim}}}$ and $\\log\\det R = m_{\\mathrm{dim}}\\log(\\sigma^2) = 2m_{\\mathrm{dim}}\\log\\sigma$.\n\nThe FE matrices are assembled for a 1D uniform mesh with piecewise linear (\"hat\") basis functions. For element size $h=1/N$, the local stiffness and mass matrices on an element are $K^e = \\frac{1}{h}\\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}$ and $M^e = \\frac{h}{6}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$. These are assembled into global matrices.\n- $K_x$ and $M_x$: Standard stiffness and mass matrices for basis functions on all $n=N+1$ nodes.\n- $K_u$: Stiffness matrix for the state space, corresponding to interior nodes. This is the submatrix of $K_x$ corresponding to nodes $1, \\dots, N-1$.\n- $M_{ux}$: Mass matrix coupling the parameter and state spaces. This corresponds to the rows of $M_x$ for interior nodes and columns for all nodes.\nWith these matrices, we construct $A_{\\mathrm{prior}} = \\tau K_x + \\delta M_x$ and $H = K_u^{-1} M_{ux}$, and then proceed to compute the variational parameters and the ELBO value.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef assemble_fem_matrices(N):\n    \"\"\"\n    Assembles the 1D Finite Element matrices for a uniform mesh.\n    \n    Args:\n        N (int): The number of uniform elements on the interval [0, 1].\n\n    Returns:\n        tuple: A tuple containing (Kx, Mx, Ku, Mux).\n            Kx (ndarray): Full stiffness matrix for parameters (n x n).\n            Mx (ndarray): Full mass matrix for parameters (n x n).\n            Ku (ndarray): Stiffness matrix for state with Dirichlet BCs (m x m).\n            Mux (ndarray): Coupling mass matrix (m x n).\n    \"\"\"\n    h = 1.0 / N\n    n = N + 1\n    m_dim = N - 1\n\n    # Assemble full stiffness matrix Kx\n    Kx = np.zeros((n, n), dtype=float)\n    diag_main = np.full(n, 2.0 / h)\n    diag_off = np.full(n - 1, -1.0 / h)\n    Kx += np.diag(diag_main)\n    Kx += np.diag(diag_off, k=1)\n    Kx += np.diag(diag_off, k=-1)\n    # Boundary nodes\n    Kx[0, 0] = 1.0 / h\n    Kx[n - 1, n - 1] = 1.0 / h\n    \n    # Assemble full mass matrix Mx\n    Mx = np.zeros((n, n), dtype=float)\n    diag_main = np.full(n, 4.0 * h / 6.0)\n    diag_off = np.full(n - 1, 1.0 * h / 6.0)\n    Mx += np.diag(diag_main)\n    Mx += np.diag(diag_off, k=1)\n    Mx += np.diag(diag_off, k=-1)\n    # Boundary nodes\n    Mx[0, 0] = 2.0 * h / 6.0\n    Mx[n - 1, n - 1] = 2.0 * h / 6.0\n\n    # Extract submatrices for the state space\n    # Interior nodes are indexed 1 to N-1\n    if m_dim > 0:\n        Ku = Kx[1:N, 1:N].copy()\n        Mux = Mx[1:N, :].copy()\n    else: # Case N=1\n        Ku = np.array([[]])\n        Mux = np.array([[]])\n\n    return Kx, Mx, Ku, Mux\n\n\ndef compute_elbo(N, tau, delta, sigma):\n    \"\"\"\n    Computes the ELBO for the variational approximation.\n    \n    Args:\n        N (int): Number of elements.\n        tau (float): Prior hyperparameter for stiffness.\n        delta (float): Prior hyperparameter for mass.\n        sigma (float): Standard deviation of observation noise.\n\n    Returns:\n        float: The computed ELBO value.\n    \"\"\"\n    n = N + 1\n    m_dim = N - 1\n\n    # 1. Assemble FE matrices\n    Kx, Mx, Ku, Mux = assemble_fem_matrices(N)\n\n    # Handle N=1, where m_dim=0, so y is empty\n    if m_dim = 0:\n        # Trivial case without observations. The posterior is the prior.\n        # This setup is not tested by the provided cases, but good to handle.\n        A_prior = tau * Kx + delta * Mx\n        C = np.linalg.inv(A_prior)\n        m_var = np.zeros(n)\n        \n        # ELBO = E_q[log p(c)] - E_q[log q(c)] = log evidence of prior-only model\n        # which simplifies to -KL(q || p) as likelihood is const. Since q=p, KL=0.\n        # Let's use the full formula to be safe.\n        sign_Ap, logdet_Ap = np.linalg.slogdet(A_prior)\n        sign_C, logdet_C = np.linalg.slogdet(C)\n        \n        elbo = (-n / 2.0 * np.log(2*np.pi)) + (0.5 * logdet_Ap) - (0.5 * np.trace(A_prior @ C)) \\\n               - ((-n / 2.0 * np.log(2*np.pi)) - (0.5 * logdet_C) - 0.5 * n)\n        # Since A_prior @ C = I, trace is n.\n        # This simplifies to 0 as expected, since log p(y) would be 0.\n        # Let's stick to the derived formula for consistency.\n        # The N=3 case has m_dim=2 > 0, so this path is not taken by tests.\n        return 0.0\n\n\n    # 2. Construct operators for the Bayesian model\n    A_prior = tau * Kx + delta * Mx\n    # R is sigma^2 * I_m, R_inv is (1/sigma^2) * I_m\n    \n    # Forward operator H = K_u^{-1} M_ux\n    Ku_inv = np.linalg.inv(Ku)\n    H = Ku_inv @ Mux\n\n    # 3. Generate synthetic data\n    s_nodes = np.linspace(0, 1, n)\n    c_true = np.sin(2 * np.pi * s_nodes)\n    y = H @ c_true\n\n    # 4. Compute optimal variational parameters m and C\n    # C_inv = H^T R^{-1} H + A_prior\n    # R_inv is a scalar matrix (1/sigma^2) * I\n    H_T_Rinv_H = (1 / sigma**2) * (H.T @ H)\n    C_inv = H_T_Rinv_H + A_prior\n    C = np.linalg.inv(C_inv)\n    \n    # m = C H^T R^{-1} y\n    H_T_Rinv_y = (1 / sigma**2) * (H.T @ y)\n    m_var = C @ H_T_Rinv_y\n\n    # 5. Evaluate the ELBO\n    # ELBO = -m/2*log(2pi) - 1/2*logdet(R) - 1/2*(y-Hm)^T R^-1 (y-Hm)\n    #        - 1/2*m^T A_prior m + 1/2*logdet(A_prior) + 1/2*logdet(C)\n    \n    # Term 1: -m_dim/2 * log(2*pi)\n    term1 = -m_dim / 2.0 * np.log(2 * np.pi)\n    \n    # Term 2: -1/2 * logdet(R) = -1/2 * m_dim * log(sigma^2)\n    log_det_R = m_dim * np.log(sigma**2)\n    term2 = -0.5 * log_det_R\n    \n    # Term 3: -1/2 * (y - Hm)^T R^-1 (y-Hm)\n    residual = y - H @ m_var\n    term3 = -0.5 * (1 / sigma**2) * (residual.T @ residual)\n    \n    # Term 4: -1/2 * m^T A_prior m\n    term4 = -0.5 * (m_var.T @ A_prior @ m_var)\n    \n    # Term 5: 1/2 * logdet(A_prior)\n    sign_Ap, logdet_Ap = np.linalg.slogdet(A_prior)\n    if sign_Ap = 0:\n        raise ValueError(\"A_prior is not positive definite.\")\n    term5 = 0.5 * logdet_Ap\n    \n    # Term 6: 1/2 * logdet(C)\n    sign_C, logdet_C = np.linalg.slogdet(C)\n    if sign_C = 0:\n        raise ValueError(\"C is not positive definite.\")\n    term6 = 0.5 * logdet_C\n\n    elbo = term1 + term2 + term3 + term4 + term5 + term6\n    \n    return elbo\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'N': 8, 'tau': 1.0, 'delta': 10**-2, 'sigma': 0.1},      # Case A\n        {'N': 3, 'tau': 0.5, 'delta': 0.05, 'sigma': 0.2},       # Case B\n        {'N': 12, 'tau': 5.0, 'delta': 10**-3, 'sigma': 0.5},    # Case C\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_elbo(\n            N=case['N'],\n            tau=case['tau'],\n            delta=case['delta'],\n            sigma=case['sigma']\n        )\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3430113"}]}