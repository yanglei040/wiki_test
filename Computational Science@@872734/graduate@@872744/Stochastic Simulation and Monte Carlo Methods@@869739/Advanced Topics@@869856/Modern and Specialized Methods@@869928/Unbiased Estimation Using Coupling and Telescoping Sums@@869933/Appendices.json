{"hands_on_practices": [{"introduction": "The efficiency of unbiased estimators built upon telescoping sums hinges on the rapid decay of the variance of level differences, $\\mathbb{E}[\\Delta_{\\ell}^2]$. This exercise provides a foundational, hands-on analysis of this crucial property. By working through the classic example of an Ornstein–Uhlenbeck process [@problem_id:3358889], you will derive from first principles how the variance of the coupled difference between two consecutive discretization levels scales with the step size, providing a concrete understanding of the theoretical underpinnings of Multilevel Monte Carlo methods.", "problem": "Consider the Ornstein–Uhlenbeck process defined by the stochastic differential equation $dX_{t} = -\\lambda X_{t}\\, dt + \\sigma\\, dW_{t}$ with deterministic initial condition $X_{0} = x_{0}$, where $\\lambda  0$, $\\sigma  0$, $t \\in [0,T]$, and $W_{t}$ is a standard Wiener process. Let $f(x) = x$ and define the Euler–Maruyama time discretization for a time step $h_{\\ell} = T/N_{\\ell}$ by the recursion\n$$\nX^{(\\ell)}_{n+1} = X^{(\\ell)}_{n} - \\lambda X^{(\\ell)}_{n}\\, h_{\\ell} + \\sigma\\, \\Delta W_{n}^{(\\ell)},\n$$\nfor $n = 0,1,\\dots,N_{\\ell}-1$, with $X^{(\\ell)}_{0} = x_{0}$ and independent normal increments $\\Delta W_{n}^{(\\ell)} \\sim \\mathcal{N}(0,h_{\\ell})$. In a Multilevel Monte Carlo (MLMC) telescoping sum construction, the level differences are formed between the fine discretization at level $\\ell$ with step $h_{\\ell}$ and the coarse discretization at level $\\ell-1$ with step $h_{\\ell-1} = 2 h_{\\ell}$. Assume the standard Brownian-bridge coupling in which each coarse increment is the sum of two consecutive fine increments, that is, for $m = 0,1,\\dots,N_{\\ell}/2 - 1$,\n$$\n\\Delta W_{m}^{(\\ell-1)} = \\Delta W_{2m}^{(\\ell)} + \\Delta W_{2m+1}^{(\\ell)}.\n$$\nDefine the coupled level difference for the terminal functional by\n$$\n\\Delta_{\\ell} := X_{T}^{(\\ell)} - X_{T}^{(\\ell-1)}.\n$$\nIn the small-step limit $h_{\\ell} \\to 0$, it is known that the second moment of the level difference admits a quadratic asymptotics of the form\n$$\n\\mathbb{E}\\!\\left[ \\Delta_{\\ell}^{2} \\right] \\approx C\\, h_{\\ell}^{2}.\n$$\nDerive the leading-order constant $C$ explicitly in terms of the model parameters $\\lambda$, $\\sigma$, $T$, and $x_{0}$ under the above coupling and telescoping construction. Your final answer must be a single closed-form analytic expression for $C$ with no unspecified constants. No numerical approximation is required.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. We proceed with the derivation.\n\nLet $h \\equiv h_{\\ell}$ be the time step for the fine discretization (level $\\ell$) and $2h \\equiv h_{\\ell-1}$ for the coarse discretization (level $\\ell-1$). The number of time steps are $N_{\\ell} = T/h$ and $N_{\\ell-1} = T/(2h) = N_{\\ell}/2$. Let $X_{n}^{(\\ell)}$ and $X_{m}^{(\\ell-1)}$ denote the fine and coarse numerical paths, respectively. The quantity of interest is the second moment of the terminal difference $\\Delta_{\\ell} = X_{T}^{(\\ell)} - X_{T}^{(\\ell-1)}$, which is $\\mathbb{E}[\\Delta_{\\ell}^2]$. We seek the constant $C$ such that $\\mathbb{E}[\\Delta_{\\ell}^2] \\approx C h^2$ for small $h$.\n\nThe Ornstein-Uhlenbeck process is a linear SDE. We can decompose the solution $X_t$ into its deterministic part (the solution to the mean ODE) and its stochastic part. Similarly, we can decompose the numerical solutions.\nLet $X_{n}^{(\\ell)} = \\hat{X}_{n}^{(\\ell)} + S_{n}^{(\\ell)}$, where $\\hat{X}_{n}^{(\\ell)}$ is the numerical solution for the deterministic part ($\\sigma=0$) and $S_{n}^{(\\ell)}$ is the numerical solution for the stochastic part (with $x_0=0$).\nThe recursions are:\n- Deterministic part: $\\hat{X}_{n+1}^{(\\ell)} = (1-\\lambda h)\\hat{X}_{n}^{(\\ell)}$ with $\\hat{X}_0^{(\\ell)} = x_0$.\n- Stochastic part: $S_{n+1}^{(\\ell)} = (1-\\lambda h)S_{n}^{(\\ell)} + \\sigma \\Delta W_n^{(\\ell)}$ with $S_0^{(\\ell)} = 0$.\n\nThe terminal difference is $\\Delta_{\\ell} = (\\hat{X}_{T}^{(\\ell)} - \\hat{X}_{T}^{(\\ell-1)}) + (S_{T}^{(\\ell)} - S_{T}^{(\\ell-1)})$. Let's denote these differences as $\\hat{\\Delta}_{\\ell}$ and $\\tilde{\\Delta}_{\\ell}$ respectively.\n$\\Delta_{\\ell} = \\hat{\\Delta}_{\\ell} + \\tilde{\\Delta}_{\\ell}$.\nThe stochastic part $\\tilde{\\Delta}_{\\ell}$ has zero mean because $S_0=0$ and the Wiener increments have zero mean. The deterministic part $\\hat{\\Delta}_{\\ell}$ is non-random. Therefore, the mean square error is:\n$$\n\\mathbb{E}[\\Delta_{\\ell}^2] = \\mathbb{E}[(\\hat{\\Delta}_{\\ell} + \\tilde{\\Delta}_{\\ell})^2] = \\hat{\\Delta}_{\\ell}^2 + 2\\hat{\\Delta}_{\\ell}\\mathbb{E}[\\tilde{\\Delta}_{\\ell}] + \\mathbb{E}[\\tilde{\\Delta}_{\\ell}^2] = \\hat{\\Delta}_{\\ell}^2 + \\mathbb{E}[\\tilde{\\Delta}_{\\ell}^2]\n$$\nWe will calculate the two terms separately.\n\n1.  **Deterministic part: $\\hat{\\Delta}_{\\ell}^2$**\n\nThe terminal values for the deterministic part are:\n$$\n\\hat{X}_{T}^{(\\ell)} = (1 - \\lambda h)^{N_{\\ell}} x_0 = (1 - \\lambda h)^{T/h} x_0\n$$\n$$\n\\hat{X}_{T}^{(\\ell-1)} = (1 - 2\\lambda h)^{N_{\\ell-1}} x_0 = (1 - 2\\lambda h)^{T/(2h)} x_0\n$$\nFor small $h$, we use the expansion $\\ln(1-z) = -z - \\frac{z^2}{2} - O(z^3)$.\n$$\n\\hat{X}_{T}^{(\\ell)} = x_0 \\exp\\left(\\frac{T}{h} \\ln(1-\\lambda h)\\right) = x_0 \\exp\\left(\\frac{T}{h} \\left(-\\lambda h - \\frac{\\lambda^2 h^2}{2} - O(h^3)\\right)\\right) = x_0 \\exp\\left(- \\lambda T - \\frac{\\lambda^2 T h}{2} - O(h^2)\\right)\n$$\nUsing $\\exp(-z) \\approx 1-z$ for small $z$, we get:\n$$\n\\hat{X}_{T}^{(\\ell)} \\approx x_0 e^{-\\lambda T} \\left(1 - \\frac{\\lambda^2 T h}{2}\\right)\n$$\nSimilarly for the coarse path:\n$$\n\\hat{X}_{T}^{(\\ell-1)} = x_0 \\exp\\left(\\frac{T}{2h} \\ln(1-2\\lambda h)\\right) = x_0 \\exp\\left(\\frac{T}{2h} \\left(-2\\lambda h - \\frac{(2\\lambda h)^2}{2} - O(h^3)\\right)\\right) = x_0 \\exp\\left(- \\lambda T - \\lambda^2 T h - O(h^2)\\right)\n$$\n$$\n\\hat{X}_{T}^{(\\ell-1)} \\approx x_0 e^{-\\lambda T} (1 - \\lambda^2 T h)\n$$\nThe difference is:\n$$\n\\hat{\\Delta}_{\\ell} = \\hat{X}_{T}^{(\\ell)} - \\hat{X}_{T}^{(\\ell-1)} \\approx x_0 e^{-\\lambda T} \\left( \\left(1 - \\frac{\\lambda^2 T h}{2}\\right) - (1 - \\lambda^2 T h) \\right) = \\frac{1}{2}\\lambda^2 T h x_0 e^{-\\lambda T}\n$$\nThe squared difference is, to leading order in $h$:\n$$\n\\hat{\\Delta}_{\\ell}^2 \\approx \\frac{1}{4} \\lambda^4 T^2 x_0^2 e^{-2\\lambda T} h^2\n$$\n\n2.  **Stochastic part: $\\mathbb{E}[\\tilde{\\Delta}_{\\ell}^2]$**\n\nLet $s_m = S_{2m}^{(\\ell)} - S_m^{(\\ell-1)}$ be the difference in the stochastic parts at coarse time points $t_m = m(2h)$.\n$S_{2m+2}^{(\\ell)} = (1-\\lambda h)^2 S_{2m}^{(\\ell)} + \\sigma(1-\\lambda h)\\Delta W_{2m}^{(\\ell)} + \\sigma \\Delta W_{2m+1}^{(\\ell)}$.\n$S_{m+1}^{(\\ell-1)} = (1-2\\lambda h)S_{m}^{(\\ell-1)} + \\sigma \\Delta W_{m}^{(\\ell-1)} = (1-2\\lambda h)S_{m}^{(\\ell-1)} + \\sigma(\\Delta W_{2m}^{(\\ell)} + \\Delta W_{2m+1}^{(\\ell)})$.\nThe difference $s_{m+1} = S_{2m+2}^{(\\ell)} - S_{m+1}^{(\\ell-1)}$ is:\n$$\ns_{m+1} = (1-\\lambda h)^2 S_{2m}^{(\\ell)} - (1-2\\lambda h)S_{m}^{(\\ell-1)} - \\sigma\\lambda h \\Delta W_{2m}^{(\\ell)}\n$$\nSubstitute $S_{2m}^{(\\ell)} = s_m + S_m^{(\\ell-1)}$:\n$$\ns_{m+1} = (1-\\lambda h)^2 (s_m + S_m^{(\\ell-1)}) - (1-2\\lambda h)S_{m}^{(\\ell-1)} - \\sigma\\lambda h \\Delta W_{2m}^{(\\ell)}\n$$\n$$\ns_{m+1} = \\left((1-\\lambda h)^2 - (1-2\\lambda h)\\right)S_{m}^{(\\ell-1)} + (1-\\lambda h)^2 s_m - \\sigma\\lambda h \\Delta W_{2m}^{(\\ell)}\n$$\n$$\ns_{m+1} = \\lambda^2 h^2 S_{m}^{(\\ell-1)} + (1-2\\lambda h+\\lambda^2 h^2) s_m - \\sigma\\lambda h \\Delta W_{2m}^{(\\ell)}\n$$\nLet $\\rho_h = 1-2\\lambda h+\\lambda^2 h^2$. The solution to this recurrence with $s_0=0$ is:\n$$\ns_{N_{\\ell-1}} = \\sum_{m=0}^{N_{\\ell-1}-1} \\rho_h^{N_{\\ell-1}-1-m} \\left( \\lambda^2 h^2 S_{m}^{(\\ell-1)} - \\sigma \\lambda h \\Delta W_{2m}^{(\\ell)} \\right)\n$$\nWe need to find the variance $\\mathbb{E}[s_{N_{\\ell-1}}^2] = \\mathbb{E}[\\tilde{\\Delta}_{\\ell}^2]$ as $\\mathbb{E}[s_{N_{\\ell-1}}]=0$.\nThe variance calculation involves squaring the sum. The increments $\\Delta W_{2m}^{(\\ell)}$ are independent for different $m$. Also, $S_m^{(\\ell-1)}$ is independent of $\\Delta W_{2k}^{(\\ell)}$ for $k \\ge m$. Cross-terms in the expectation will vanish or be of higher order. The dominant contribution comes from the terms involving $\\Delta W_{2m}^{(\\ell)}$, which are of order $h^{3/2}$, while the $S_m^{(\\ell-1)}$ terms are of order $h^2$.\nTo leading order:\n$$\n\\tilde{\\Delta}_{\\ell} = s_{N_{\\ell-1}} \\approx \\sum_{m=0}^{N_{\\ell-1}-1} \\rho_h^{N_{\\ell-1}-1-m} (-\\sigma \\lambda h \\Delta W_{2m}^{(\\ell)})\n$$\nThe variance is:\n$$\n\\mathbb{E}[\\tilde{\\Delta}_{\\ell}^2] \\approx \\mathbb{E}\\left[ \\left( \\sum_{m=0}^{N_{\\ell-1}-1} \\rho_h^{N_{\\ell-1}-1-m} (-\\sigma \\lambda h \\Delta W_{2m}^{(\\ell)}) \\right)^2 \\right]\n$$\nDue to independence of the Wiener increments, $\\mathbb{E}[\\Delta W_{2m}^{(\\ell)}\\Delta W_{2k}^{(\\ell)}] = h \\delta_{mk}$.\n$$\n\\mathbb{E}[\\tilde{\\Delta}_{\\ell}^2] \\approx \\sum_{m=0}^{N_{\\ell-1}-1} (\\rho_h^{N_{\\ell-1}-1-m})^2 (-\\sigma \\lambda h)^2 \\mathbb{E}[(\\Delta W_{2m}^{(\\ell)})^2] = \\sigma^2 \\lambda^2 h^3 \\sum_{m=0}^{N_{\\ell-1}-1} \\rho_h^{2(N_{\\ell-1}-1-m)}\n$$\nLet $k = N_{\\ell-1}-1-m$. The sum is a geometric series $\\sum_{k=0}^{N_{\\ell-1}-1} (\\rho_h^2)^k = \\frac{1-(\\rho_h^2)^{N_{\\ell-1}}}{1-\\rho_h^2}$.\nFor small $h$:\n$1-\\rho_h^2 = 1-(1-2\\lambda h+\\lambda^2 h^2)^2 = 1-(1-4\\lambda h+O(h^2)) \\approx 4\\lambda h$.\n$(\\rho_h^2)^{N_{\\ell-1}} = ((1-2\\lambda h+\\lambda^2 h^2)^2)^{T/(2h)} \\approx (1-4\\lambda h)^{T/(2h)} \\to \\exp(-2\\lambda T)$ as $h \\to 0$.\nSo the sum is $\\approx \\frac{1-e^{-2\\lambda T}}{4\\lambda h}$.\nSubstituting this back into the variance expression:\n$$\n\\mathbb{E}[\\tilde{\\Delta}_{\\ell}^2] \\approx \\sigma^2 \\lambda^2 h^3 \\left( \\frac{1-e^{-2\\lambda T}}{4\\lambda h} \\right) = \\frac{\\sigma^2 \\lambda}{4}(1-e^{-2\\lambda T}) h^2\n$$\n\n3.  **Total Mean-Square Error**\nCombining the two parts gives the total mean-square error:\n$$\n\\mathbb{E}[\\Delta_{\\ell}^2] = \\hat{\\Delta}_{\\ell}^2 + \\mathbb{E}[\\tilde{\\Delta}_{\\ell}^2] \\approx \\frac{1}{4} \\lambda^4 T^2 x_0^2 e^{-2\\lambda T} h^2 + \\frac{\\sigma^2 \\lambda}{4}(1-e^{-2\\lambda T}) h^2\n$$\n$$\n\\mathbb{E}[\\Delta_{\\ell}^2] \\approx \\left( \\frac{1}{4} \\lambda^4 T^2 x_0^2 e^{-2\\lambda T} + \\frac{\\sigma^2 \\lambda}{4}(1-e^{-2\\lambda T}) \\right) h^2\n$$\nBy comparing this with the given asymptotic form $\\mathbb{E}[\\Delta_{\\ell}^2] \\approx C h^2$, we can identify the constant $C$.\n\n$$\nC = \\frac{1}{4} \\lambda^4 T^2 x_0^2 e^{-2\\lambda T} + \\frac{\\sigma^2 \\lambda}{4}(1-e^{-2\\lambda T})\n$$\nThis expression can be rearranged to:\n$$\nC = \\frac{\\lambda}{4} \\left( \\lambda^3 T^2 x_0^2 e^{-2\\lambda T} + \\sigma^2 (1-e^{-2\\lambda T}) \\right)\n$$", "answer": "$$\\boxed{\\frac{1}{4} \\lambda^4 T^2 x_{0}^{2} \\exp(-2\\lambda T) + \\frac{\\sigma^2 \\lambda}{4} (1-\\exp(-2\\lambda T))}$$", "id": "3358889"}, {"introduction": "Moving from theoretical analysis to practical implementation requires a robust set of diagnostic tools. Before committing significant computational resources, it is essential to verify that the assumptions of the unbiased estimation scheme hold and that the implementation is correct. This exercise [@problem_id:3358899] challenges you to think critically about the validation process, asking you to identify which empirical checks are principled and necessary for diagnosing the behavior of level increments and validating the choice of the random truncation distribution.", "problem": "You are designing an unbiased estimator using randomization over a multilevel telescoping sum. Let $\\{Y_\\ell\\}_{\\ell \\ge 0}$ denote a sequence of biased approximations to a target quantity $\\mu = \\mathbb{E}[f(X)]$, where level $\\ell$ has computational cost $c_\\ell$ and is constructed so that coupled simulators allow you to form increments $\\Delta_\\ell := G_\\ell - G_{\\ell-1}$ for $\\ell \\ge 1$ and $\\Delta_0 := G_0$, where $G_\\ell$ is the output of the level $\\ell$ simulator. Let $L$ be a nonnegative integer-valued random truncation level with distribution $W$ and tail probabilities $p_\\ell := \\mathbb{P}(L \\ge \\ell)$. Consider the unbiased sum estimator\n$$\nZ := \\sum_{\\ell=0}^{L} \\frac{\\Delta_\\ell}{p_\\ell},\n$$\nwith the requirement that $L$ is independent of the randomness used to construct $\\{\\Delta_\\ell\\}_{\\ell \\ge 0}$. Before running large-scale Monte Carlo (MC) simulations, you wish to empirically diagnose whether the second moments $\\mathbb{E}[\\Delta_\\ell^2]$ decay sufficiently with $\\ell$ and whether the chosen truncation distribution $W$ is appropriate.\n\nUnder the multilevel coupling paradigm, a key sufficient condition for finite second moment of $Z$ is that the series $\\sum_{\\ell \\ge 0} \\mathbb{E}[\\Delta_\\ell^2]/p_\\ell$ converges, and that the cross-level dependence induced by coupling does not produce explosive cross-terms. In many practical contexts, one expects $\\mathbb{E}[\\Delta_\\ell^2]$ to decay approximately geometrically in $\\ell$ when the coupling is strong.\n\nWhich of the following empirical procedures are principled diagnostics to assess the decay of $\\mathbb{E}[\\Delta_\\ell^2]$ and validate the choice of the truncation distribution $W$ before large-scale unbiased simulations? Select all that apply.\n\nA. Run a short coupled pilot across levels $\\ell \\in \\{0,1,\\dots,\\ell_{\\max}\\}$, collect independent replicates of $\\Delta_\\ell$, compute the sample second moment $\\hat{v}_\\ell := \\frac{1}{n_\\ell}\\sum_{i=1}^{n_\\ell} \\Delta_{\\ell,i}^2$, and fit a linear model of $\\log \\hat{v}_\\ell$ versus $\\ell$ to estimate a geometric decay rate. Use this estimated rate to choose $p_\\ell$ with geometric tail $p_\\ell \\propto r^{-\\ell}$ that is not decaying faster than $\\hat{v}_\\ell$, and verify numerically that $\\sum_{\\ell=0}^{\\ell_{\\max}} \\hat{v}_\\ell/p_\\ell$ appears stable as $\\ell_{\\max}$ grows.\n\nB. Generate independent samples of $L \\sim W$ and verify that the empirical frequencies $\\hat{p}_\\ell$ of the event $\\{L \\ge \\ell\\}$ approximate the designed $p_\\ell$ over the range of $\\ell$ used in pilots. In addition, compute the empirical Coefficient of Variation (CV) of the weights $w_\\ell := 1/p_\\ell$ over the support of $L$ and ensure it is not excessively large, indicating that rare large $\\ell$ values do not contribute explosive weights.\n\nC. Apply a Kolmogorov–Smirnov (KS) normality test to each $\\Delta_\\ell$ in the pilot and accept $W$ only if the $\\Delta_\\ell$ appear approximately Gaussian, on the grounds that Gaussianity ensures that the sample variance reliably estimates $\\mathbb{E}[\\Delta_\\ell^2]$.\n\nD. Compute the empirical correlation $\\rho_\\ell$ between $G_\\ell$ and $G_{\\ell-1}$ under the coupling used to define $\\Delta_\\ell$. Require that $\\rho_\\ell$ is close to $1$ for moderate $\\ell$, as a proxy for strong coupling quality, and use this to anticipate faster decay of $\\mathbb{E}[\\Delta_\\ell^2]$.\n\nE. Choose $W$ so that $p_\\ell \\propto c_\\ell^{-1}$ to equalize expected computational effort per level, ignoring the behavior of $\\mathbb{E}[\\Delta_\\ell^2]$, since cost is the primary constraint in practice.\n\nF. Construct pilot unbiased estimates $Z$ under at least two distinct truncation distributions $W$ and $W'$, with correct weights $\\{1/p_\\ell\\}$ and $\\{1/p'_\\ell\\}$. Check that the empirical means of $Z$ agree within MC error across $W$ choices. Systematic differences suggest dependence between $L$ and $\\{\\Delta_\\ell\\}$ or incorrect weight implementation, invalidating the scheme.\n\nAnswer choices may include multiple correct selections. Define any acronyms on first use: Monte Carlo (MC), Coefficient of Variation (CV), Kolmogorov–Smirnov (KS).", "solution": "Begin from the core unbiased randomization principle for multilevel telescoping sums. Let $\\{\\Delta_\\ell\\}_{\\ell \\ge 0}$ be the coupled increments, and let $L$ be independent of $\\{\\Delta_\\ell\\}_{\\ell \\ge 0}$ with tail probabilities $p_\\ell := \\mathbb{P}(L \\ge \\ell)$. The estimator\n$$\nZ := \\sum_{\\ell=0}^{L} \\frac{\\Delta_\\ell}{p_\\ell}\n$$\nis unbiased for $\\sum_{\\ell=0}^{\\infty} \\mathbb{E}[\\Delta_\\ell]$ under dominated convergence and independence, because\n$$\n\\mathbb{E}[Z] = \\sum_{\\ell=0}^{\\infty} \\mathbb{E}\\left[ \\frac{\\Delta_\\ell \\, \\mathbf{1}\\{L \\ge \\ell\\}}{p_\\ell} \\right]\n= \\sum_{\\ell=0}^{\\infty} \\mathbb{E}[\\Delta_\\ell] \\, \\frac{\\mathbb{P}(L \\ge \\ell)}{p_\\ell}\n= \\sum_{\\ell=0}^{\\infty} \\mathbb{E}[\\Delta_\\ell].\n$$\nUnder appropriate telescoping construction, $\\sum_{\\ell=0}^{\\infty} \\mathbb{E}[\\Delta_\\ell] = \\mu$. For a finite second moment (and hence finite variance), a sufficient empirical condition is that\n$$\n\\sum_{\\ell=0}^{\\infty} \\frac{\\mathbb{E}[\\Delta_\\ell^2]}{p_\\ell}  \\infty,\n$$\nand that cross-level covariance terms induced by coupling do not produce divergence. More precisely, by expanding $Z^2$,\n$$\n\\mathbb{E}[Z^2] = \\sum_{\\ell=0}^{\\infty} \\frac{\\mathbb{E}\\left[\\Delta_\\ell^2 \\, \\mathbf{1}\\{L \\ge \\ell\\}\\right]}{p_\\ell^2}\n+ 2 \\sum_{0 \\le j  k  \\infty} \\frac{\\mathbb{E}\\left[\\Delta_j \\Delta_k \\, \\mathbf{1}\\{L \\ge k\\}\\right]}{p_j p_k}.\n$$\nUsing independence between $L$ and $\\{\\Delta_\\ell\\}$, $\\mathbb{E}\\left[\\Delta_\\ell^2 \\, \\mathbf{1}\\{L \\ge \\ell\\}\\right] = p_\\ell \\, \\mathbb{E}[\\Delta_\\ell^2]$, so the first sum becomes $\\sum_{\\ell=0}^{\\infty} \\mathbb{E}[\\Delta_\\ell^2]/p_\\ell$. The cross-term sum can be bounded in terms of the same tail structure under typical decay and coupling assumptions. For example, with geometric decay $\\mathbb{E}[\\Delta_\\ell^2] \\approx C r^{-\\ell}$ for some $C  0$ and $r  1$, choosing $p_\\ell$ with geometric tail $p_\\ell \\propto s^{-\\ell}$ and $s  r$ ensures that the diagonal contribution $\\sum_{\\ell=0}^{\\infty} \\mathbb{E}[\\Delta_\\ell^2]/p_\\ell \\approx \\sum_{\\ell=0}^{\\infty} (r/s)^{-\\ell}$ converges. Strong coupling often makes cross-terms negligible or at least summable, but this must be checked empirically.\n\nDiagnostics should therefore: estimate the decay rate of $\\mathbb{E}[\\Delta_\\ell^2]$; verify the implemented $W$ tail behavior; assess coupling quality; and sanity-check unbiasedness and weight implementation. We now analyze each option.\n\nOption A: The proposed pilot computes $\\hat{v}_\\ell := \\frac{1}{n_\\ell} \\sum_{i=1}^{n_\\ell} \\Delta_{\\ell,i}^2$, and fits $\\log \\hat{v}_\\ell$ versus $\\ell$. If $\\hat{v}_\\ell$ decays approximately linearly on the log-scale, that indicates a geometric decay $\\hat{v}_\\ell \\approx \\hat{C} \\hat{r}^{-\\ell}$, and one can select $p_\\ell \\propto \\hat{s}^{-\\ell}$ with $\\hat{s}  \\hat{r}$ to keep $\\sum \\hat{v}_\\ell/p_\\ell$ finite. Computing partial sums $\\sum_{\\ell=0}^{\\ell_{\\max}} \\hat{v}_\\ell/p_\\ell$ and verifying stability as $\\ell_{\\max}$ increases is a direct empirical proxy for the sufficient condition $\\sum \\mathbb{E}[\\Delta_\\ell^2]/p_\\ell  \\infty$. This procedure targets the correct quantity (the second moment), uses independence-preserving pilots, and connects decay to a principled tail choice. Verdict: Correct.\n\nOption B: Independently sampling $L \\sim W$ and verifying that empirical tail probabilities $\\hat{p}_\\ell$ match $p_\\ell$ is a direct validation that the implemented truncation distribution is correct over the range of interest. Computing the Coefficient of Variation (CV) of $w_\\ell := 1/p_\\ell$ over the realized support helps detect heavy tails that produce excessively large weights for rare large $\\ell$; such explosive weights can magnify variance through both diagonal and cross-term contributions. While CV alone does not certify finite variance, combined with the decay assessment of $\\mathbb{E}[\\Delta_\\ell^2]$ it is a reasonable stress test to avoid pathological tails. Verdict: Correct.\n\nOption C: Normality of $\\Delta_\\ell$ is neither required nor particularly informative for the unbiased scheme. The sample second moment $\\hat{v}_\\ell$ is a consistent estimator of $\\mathbb{E}[\\Delta_\\ell^2]$ under mild conditions (e.g., finite fourth moment), regardless of the shape of the distribution. A Kolmogorov–Smirnov (KS) normality test may reject or accept due to shape peculiarities but does not address whether $\\hat{v}_\\ell$ decays with $\\ell$ or whether $\\sum \\hat{v}_\\ell/p_\\ell$ is finite. Conditioning acceptance of $W$ on Gaussianity is a misdirected diagnostic. Verdict: Incorrect.\n\nOption D: The increment $\\Delta_\\ell = G_\\ell - G_{\\ell-1}$ benefits from strong coupling between $G_\\ell$ and $G_{\\ell-1}$. High empirical correlation $\\rho_\\ell$ close to $1$ indicates that fluctuations in $G_\\ell$ and $G_{\\ell-1}$ are aligned, making $\\Delta_\\ell$ small and thereby promoting faster decay of $\\mathbb{E}[\\Delta_\\ell^2]$. Although correlation is a proxy and does not directly estimate $\\mathbb{E}[\\Delta_\\ell^2]$, it is a principled qualitative diagnostic of coupling quality, especially when combined with quantitative variance decay estimates. Verdict: Correct.\n\nOption E: Choosing $W$ based solely on computational cost $c_\\ell$, so that $p_\\ell \\propto c_\\ell^{-1}$, ignores the behavior of $\\mathbb{E}[\\Delta_\\ell^2]$. If $p_\\ell$ decays too fast relative to the decay of $\\mathbb{E}[\\Delta_\\ell^2]$, the series $\\sum \\mathbb{E}[\\Delta_\\ell^2]/p_\\ell$ can diverge, resulting in infinite variance. Even if finite, this choice can be far from optimal in variance per unit cost. Thus, cost-only selection is not a valid diagnostic for variance control and truncation validation. Verdict: Incorrect.\n\nOption F: Constructing pilot unbiased estimates $Z$ under two distinct truncation distributions $W$ and $W'$ with correctly implemented weights $\\{1/p_\\ell\\}$ and $\\{1/p'_\\ell\\}$ should yield the same expectation $\\mu$. If empirical pilot means under $W$ and $W'$ differ beyond MC error, that indicates a violation of the independence requirement between $L$ and $\\{\\Delta_\\ell\\}$ or an implementation error in weights or truncation logic. This is a direct empirical check of unbiasedness and correct $W$ implementation, and it indirectly validates the independence assumption critical to the derivation. Verdict: Correct.\n\nSummary: Options A, B, D, and F are principled diagnostics; Options C and E are not aligned with the necessary variance decay assessment and truncation validation.", "answer": "$$\\boxed{ABDF}$$", "id": "3358899"}, {"introduction": "Once the behavior of the level increments is understood and validated, the final step is to design an optimal estimator. The choice of the random truncation level is a critical design parameter that balances the bias reduction from adding more levels against the rapidly increasing computational cost. This problem [@problem_id:3358913] guides you through the process of optimizing this trade-off by minimizing the work-variance product, a key measure of simulation efficiency, thereby connecting the theoretical decay rates of variance and cost directly to optimal algorithmic design.", "problem": "Consider an unbiased debiasing construction for a sequence of level differences arising from a telescoping sum. Let $\\{\\Delta_{\\ell}\\}_{\\ell \\ge 0}$ denote unbiased level increments with independent Monte Carlo realizations $\\{\\widehat{\\Delta}_{\\ell}\\}_{\\ell \\ge 0}$ satisfying, for all $\\ell \\ge 0$, the variance model $\\mathrm{Var}(\\widehat{\\Delta}_{\\ell}) = c \\, 2^{-\\kappa \\ell}$ with $c \\in (0,\\infty)$ and $\\kappa \\in (0,\\infty)$. Assume the computational cost to generate one realization of $\\widehat{\\Delta}_{\\ell}$ is $a \\, 2^{\\gamma \\ell}$ with $a \\in (0,\\infty)$ and $\\gamma \\in (0,\\infty)$. Let $L$ be a nonnegative integer-valued random truncation level, independent of the $\\widehat{\\Delta}_{\\ell}$, that defines the coupled-sum unbiased estimator\n$$\nY \\;=\\; \\sum_{\\ell = 0}^{L} \\frac{\\widehat{\\Delta}_{\\ell}}{q_{\\ell}},\n$$\nwhere $q_{\\ell} \\equiv \\mathbb{P}(L \\ge \\ell)$ are the tail probabilities. Suppose the truncation is chosen to have a geometric heavy tail, $q_{\\ell} = 2^{- r \\ell}$ for some parameter $r \\in (0,\\infty)$ with $q_{0} = 1$. Assume $\\{\\widehat{\\Delta}_{\\ell}\\}_{\\ell \\ge 0}$ are independent across $\\ell$ and independent of $L$, and that $\\mathrm{Var}(Y) = \\sum_{\\ell \\ge 0} \\mathrm{Var}(\\widehat{\\Delta}_{\\ell}) / q_{\\ell}$ and $\\mathbb{E}[\\mathrm{Work}] = \\sum_{\\ell \\ge 0} q_{\\ell} \\, a \\, 2^{\\gamma \\ell}$. Define the work–variance product\n$$\n\\Pi(r) \\;\\equiv\\; \\mathrm{Var}(Y) \\times \\mathbb{E}[\\mathrm{Work}] \\, .\n$$\nAssume $\\kappa  \\gamma$ so that finite-variance and finite-work regimes can be achieved. Determine the value $r^{\\star}$ that minimizes $\\Pi(r)$ over $r \\in (\\gamma,\\kappa)$ and derive the corresponding minimized product $\\Pi^{\\star} \\equiv \\Pi(r^{\\star})$ in closed form as a function of $a$, $c$, $\\kappa$, and $\\gamma$. Express your final answer as a row vector containing $r^{\\star}$ and $\\Pi^{\\star}$, in that order, as a single analytic expression. No rounding is required.", "solution": "The user wants me to find the optimal parameter $r^{\\star}$ that minimizes the work-variance product $\\Pi(r)$ and to determine the corresponding minimum value $\\Pi^{\\star}$.\n\nThe work-variance product is defined as $\\Pi(r) = \\mathrm{Var}(Y) \\times \\mathbb{E}[\\mathrm{Work}]$. First, we must express the two factors, the variance $\\mathrm{Var}(Y)$ and the expected work $\\mathbb{E}[\\mathrm{Work}]$, as functions of the parameter $r$.\n\nThe variance of the estimator $Y$ is given by the formula $\\mathrm{Var}(Y) = \\sum_{\\ell = 0}^{\\infty} \\frac{\\mathrm{Var}(\\widehat{\\Delta}_{\\ell})}{q_{\\ell}}$. We are given the variance model $\\mathrm{Var}(\\widehat{\\Delta}_{\\ell}) = c \\, 2^{-\\kappa \\ell}$ and the tail probabilities $q_{\\ell} = 2^{-r \\ell}$. Substituting these into the formula for $\\mathrm{Var}(Y)$ yields:\n$$\n\\mathrm{Var}(Y) = \\sum_{\\ell = 0}^{\\infty} \\frac{c \\, 2^{-\\kappa \\ell}}{2^{-r \\ell}} = c \\sum_{\\ell = 0}^{\\infty} 2^{(r - \\kappa)\\ell} = c \\sum_{\\ell = 0}^{\\infty} (2^{r - \\kappa})^{\\ell}\n$$\nThis is a geometric series with ratio $2^{r-\\kappa}$. For the series to converge, its ratio must have a magnitude less than $1$. Since $2^{r-\\kappa}  0$, this requires $2^{r-\\kappa}  1$, which is equivalent to $r - \\kappa  0$, or $r  \\kappa$. The problem specifies that we are optimizing over $r \\in (\\gamma, \\kappa)$, so this condition is satisfied. The sum of the series is $\\frac{1}{1 - \\text{ratio}}$. Therefore, the variance is:\n$$\n\\mathrm{Var}(Y) = c \\left( \\frac{1}{1 - 2^{r - \\kappa}} \\right)\n$$\n\nThe expected computational work is given by $\\mathbb{E}[\\mathrm{Work}] = \\sum_{\\ell = 0}^{\\infty} q_{\\ell} \\, a \\, 2^{\\gamma \\ell}$. We substitute $q_{\\ell} = 2^{-r \\ell}$ and use the given cost per level, $a \\, 2^{\\gamma \\ell}$:\n$$\n\\mathbb{E}[\\mathrm{Work}] = \\sum_{\\ell = 0}^{\\infty} (2^{-r \\ell}) (a \\, 2^{\\gamma \\ell}) = a \\sum_{\\ell = 0}^{\\infty} 2^{(\\gamma - r)\\ell} = a \\sum_{\\ell = 0}^{\\infty} (2^{\\gamma - r})^{\\ell}\n$$\nThis is another geometric series, with ratio $2^{\\gamma-r}$. For convergence, we need $2^{\\gamma-r}  1$, which implies $\\gamma - r  0$, or $r  \\gamma$. This condition is also met for $r \\in (\\gamma, \\kappa)$. The sum of this series is:\n$$\n\\mathbb{E}[\\mathrm{Work}] = a \\left( \\frac{1}{1 - 2^{\\gamma - r}} \\right)\n$$\n\nNow, we can write the work-variance product $\\Pi(r)$ by multiplying the expressions for the variance and the expected work:\n$$\n\\Pi(r) = \\left( c \\frac{1}{1 - 2^{r - \\kappa}} \\right) \\left( a \\frac{1}{1 - 2^{\\gamma - r}} \\right) = \\frac{ac}{(1 - 2^{r - \\kappa})(1 - 2^{\\gamma - r})}\n$$\nTo find the value $r^{\\star}$ that minimizes $\\Pi(r)$, we compute its derivative with respect to $r$ and set it to $0$. To simplify the differentiation, we can work with the natural logarithm of $\\Pi(r)$, as $\\ln(x)$ is a monotonically increasing function and will have its minimum at the same point as $x$.\n$$\n\\ln(\\Pi(r)) = \\ln(ac) - \\ln(1 - 2^{r - \\kappa}) - \\ln(1 - 2^{\\gamma - r})\n$$\nDifferentiating with respect to $r$:\n$$\n\\frac{d}{dr}\\ln(\\Pi(r)) = 0 - \\frac{-2^{r - \\kappa} \\ln(2)}{1 - 2^{r - \\kappa}} - \\frac{-(-2^{\\gamma - r} \\ln(2))}{1 - 2^{\\gamma - r}} = \\frac{2^{r - \\kappa} \\ln(2)}{1 - 2^{r - \\kappa}} - \\frac{2^{\\gamma - r} \\ln(2)}{1 - 2^{\\gamma - r}}\n$$\nSetting the derivative to $0$ to find the critical point $r^{\\star}$:\n$$\n\\frac{2^{r^{\\star} - \\kappa} \\ln(2)}{1 - 2^{r^{\\star} - \\kappa}} = \\frac{2^{\\gamma - r^{\\star}} \\ln(2)}{1 - 2^{\\gamma - r^{\\star}}}\n$$\nSince $\\ln(2) \\neq 0$, we can divide it out:\n$$\n\\frac{2^{r^{\\star} - \\kappa}}{1 - 2^{r^{\\star} - \\kappa}} = \\frac{2^{\\gamma - r^{\\star}}}{1 - 2^{\\gamma - r^{\\star}}}\n$$\nThe function $f(x) = \\frac{x}{1-x}$ is strictly increasing on its domain. For $r \\in (\\gamma, \\kappa)$, both arguments $2^{r-\\kappa}$ and $2^{\\gamma-r}$ are in the interval $(0,1)$. Thus, for $f(x_1) = f(x_2)$, we must have $x_1=x_2$. Applying this to our equation:\n$$\n2^{r^{\\star} - \\kappa} = 2^{\\gamma - r^{\\star}}\n$$\nSince the base is $2$, we can equate the exponents:\n$$\nr^{\\star} - \\kappa = \\gamma - r^{\\star} \\implies 2r^{\\star} = \\gamma + \\kappa \\implies r^{\\star} = \\frac{\\gamma + \\kappa}{2}\n$$\nThis critical point $r^{\\star}$ is indeed in the specified domain $(\\gamma, \\kappa)$ because $\\gamma  \\kappa$ implies $\\gamma = \\frac{\\gamma+\\gamma}{2}  \\frac{\\gamma+\\kappa}{2}  \\frac{\\kappa+\\kappa}{2} = \\kappa$. The second derivative of $\\ln(\\Pi(r))$ is positive on the domain, confirming that $\\ln(\\Pi(r))$ is a convex function and $r^{\\star}$ is a unique minimizer.\n\nFinally, we compute the minimized product $\\Pi^{\\star} = \\Pi(r^{\\star})$ by substituting $r^{\\star}$ back into the expression for $\\Pi(r)$:\n$$\n\\Pi^{\\star} = \\Pi\\left(\\frac{\\gamma + \\kappa}{2}\\right) = \\frac{ac}{\\left(1 - 2^{\\frac{\\gamma + \\kappa}{2} - \\kappa}\\right)\\left(1 - 2^{\\gamma - \\frac{\\gamma + \\kappa}{2}}\\right)}\n$$\nThe exponents simplify as:\n$$\n\\frac{\\gamma + \\kappa}{2} - \\kappa = \\frac{\\gamma - \\kappa}{2}\n$$\n$$\n\\gamma - \\frac{\\gamma + \\kappa}{2} = \\frac{\\gamma - \\kappa}{2}\n$$\nThe two terms in the denominator are identical. Hence,\n$$\n\\Pi^{\\star} = \\frac{ac}{\\left(1 - 2^{\\frac{\\gamma - \\kappa}{2}}\\right)\\left(1 - 2^{\\frac{\\gamma - \\kappa}{2}}\\right)} = \\frac{ac}{\\left(1 - 2^{\\frac{\\gamma - \\kappa}{2}}\\right)^2}\n$$\nThe optimal parameter is $r^{\\star} = \\frac{\\gamma + \\kappa}{2}$ and the minimum work-variance product is $\\Pi^{\\star} = \\frac{ac}{(1 - 2^{(\\gamma - \\kappa)/2})^2}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\gamma + \\kappa}{2}  \\frac{ac}{\\left(1 - 2^{\\frac{\\gamma - \\kappa}{2}}\\right)^2}\n\\end{pmatrix}\n}\n$$", "id": "3358913"}]}