{"hands_on_practices": [{"introduction": "The convergence behavior of stochastic approximation algorithms is critically dependent on the choice of the step-size sequence $a_n$. This exercise delves into the theoretical underpinnings of this relationship by comparing the canonical step-size $a_n = c/n$ with the more general form $a_n = c/n^{\\alpha}$. By analyzing the mean squared error and the conditions for a Central Limit Theorem, you will gain a deep understanding of how step-sizes control the trade-off between bias and variance, ultimately determining the optimal rate of convergence [@problem_id:3348661].", "problem": "Consider the scalar Robbinsâ€“Monro stochastic approximation recursion\n$$\nX_{n+1} \\;=\\; X_n \\;-\\; a_n\\,Y_{n+1},\n$$\nwhere the goal is to find the unique root $\\theta^\\star$ of an unknown function $f$ satisfying $f(\\theta^\\star)=0$. Assume the following foundational conditions:\n- The observation noise is given by $Y_{n+1} \\;=\\; f(X_n) \\;+\\; \\xi_{n+1}$, where $\\{\\xi_n\\}$ is a martingale difference sequence with respect to the natural filtration, with $\\mathbb{E}[\\xi_{n+1}\\mid \\mathcal{F}_n]=0$ and $\\mathbb{E}[\\xi_{n+1}^2\\mid \\mathcal{F}_n]\\to \\sigma^2\\in(0,\\infty)$.\n- The function $f$ is continuously differentiable in a neighborhood of $\\theta^\\star$, with $f'(\\theta^\\star)=\\gamma>0$, and $f$ is locally monotone so that the linearization around $\\theta^\\star$ is valid.\n- The step-size sequence is either $a_n=c/n$ with $c>0$, or $a_n=c/n^\\alpha$ with $c>0$ and $\\alpha\\in(0,1]$.\n\nUsing only these well-tested facts and definitions, compare the two step-size regimes in terms of convergence speed (as captured by the decay of $\\mathbb{E}[(X_n-\\theta^\\star)^2]$) and asymptotic variance, and determine the regimes where a Central Limit Theorem (CLT) holds for the appropriately normalized error. Select all statements that are correct.\n\nA. If $a_n=c/n$ and $2\\gamma c>1$, then a Central Limit Theorem holds with normalization $\\sqrt{n}$, and\n$$\n\\sqrt{n}\\,(X_n-\\theta^\\star)\\;\\Rightarrow\\;\\mathcal{N}\\!\\left(0,\\;\\frac{\\sigma^2 c^2}{2\\gamma c - 1}\\right),\n$$\nwith the choice $c=1/\\gamma$ minimizing the asymptotic variance among all $c>1/(2\\gamma)$.\n\nB. If $a_n=c/n$, then $\\mathbb{E}[(X_n-\\theta^\\star)^2]\\sim K/n^2$ for some constant $K>0$, so the normalization $\\sqrt{n}$ yields a degenerate limit; therefore, no Central Limit Theorem is possible in this regime.\n\nC. If $a_n=c/n^\\alpha$ with $\\alpha\\in(1/2,1)$, then $\\mathbb{E}[(X_n-\\theta^\\star)^2]\\sim \\left(\\frac{c\\sigma^2}{2\\gamma}\\right)\\,n^{-\\alpha}$ and a Central Limit Theorem holds with normalization $n^{\\alpha/2}$:\n$$\nn^{\\alpha/2}\\,(X_n-\\theta^\\star)\\;\\Rightarrow\\;\\mathcal{N}\\!\\left(0,\\;\\frac{c\\sigma^2}{2\\gamma}\\right).\n$$\n\nD. If $a_n=c/n^\\alpha$ with $\\alpha\\in(0,1/2]$, then almost sure convergence still holds by virtue of $\\sum_n a_n^2<\\infty$, but the Central Limit Theorem fails because the noise does not average sufficiently.\n\nE. For $\\alpha\\in(1/2,1)$, taking smaller $\\alpha$ always yields faster convergence than $a_n=c/n$ in the sense that the standard deviation of $X_n-\\theta^\\star$ decays strictly faster in $n$; moreover, $a_n=c/n^\\alpha$ dominates $a_n=c/n$ in asymptotic variance for all $c>0$.", "solution": "The problem asks for an analysis of the Robbins-Monro stochastic approximation algorithm under two different step-size regimes. The core of the analysis relies on the behavior of the error, $\\tilde{X}_n = X_n - \\theta^\\star$.\n\nThe recursion for the iterate is given by $X_{n+1} = X_n - a_n Y_{n+1}$. Substituting the observation model $Y_{n+1} = f(X_n) + \\xi_{n+1}$ and using $\\tilde{X}_n = X_n - \\theta^\\star$, we obtain the recursion for the error:\n$$\n\\tilde{X}_{n+1} = X_{n+1} - \\theta^\\star = (X_n - \\theta^\\star) - a_n(f(X_n) + \\xi_{n+1}) = \\tilde{X}_n - a_n(f(\\theta^\\star + \\tilde{X}_n) + \\xi_{n+1}).\n$$\nGiven that $f$ is continuously differentiable near $\\theta^\\star$ with $f'(\\theta^\\star) = \\gamma > 0$, and $f(\\theta^\\star)=0$, a first-order Taylor expansion gives $f(\\theta^\\star + \\tilde{X}_n) = f(\\theta^\\star) + f'(\\theta^\\star)\\tilde{X}_n + o(\\tilde{X}_n) = \\gamma \\tilde{X}_n + o(\\tilde{X}_n)$. Assuming convergence, so that $\\tilde{X}_n$ is small for large $n$, we can use the linearization:\n$$\n\\tilde{X}_{n+1} \\approx (1 - a_n \\gamma) \\tilde{X}_n - a_n \\xi_{n+1}.\n$$\nTo analyze the convergence speed, we examine the mean squared error (MSE), $V_n = \\mathbb{E}[\\tilde{X}_n^2]$. Squaring the linearized error recursion and taking the expectation yields:\n$$\nV_{n+1} = \\mathbb{E}[\\tilde{X}_{n+1}^2] \\approx \\mathbb{E}[((1 - a_n \\gamma) \\tilde{X}_n - a_n \\xi_{n+1})^2] = \\mathbb{E}[(1 - a_n \\gamma)^2 \\tilde{X}_n^2 - 2a_n(1 - a_n \\gamma)\\tilde{X}_n\\xi_{n+1} + a_n^2 \\xi_{n+1}^2].\n$$\nSince $\\tilde{X}_n$ is $\\mathcal{F}_n$-measurable and $\\mathbb{E}[\\xi_{n+1} \\mid \\mathcal{F}_n] = 0$, the cross-term vanishes: $\\mathbb{E}[\\tilde{X}_n\\xi_{n+1}] = \\mathbb{E}[\\tilde{X}_n \\mathbb{E}[\\xi_{n+1} \\mid \\mathcal{F}_n]] = 0$. Given $\\mathbb{E}[\\xi_{n+1}^2 \\mid \\mathcal{F}_n] \\to \\sigma^2$, we have $\\mathbb{E}[\\xi_{n+1}^2] \\to \\sigma^2$. For large $n$, we can approximate $\\mathbb{E}[a_n^2 \\xi_{n+1}^2] \\approx a_n^2 \\sigma^2$. The recursion for the MSE is approximately:\n$$\nV_{n+1} \\approx (1 - a_n \\gamma)^2 V_n + a_n^2 \\sigma^2 \\approx (1 - 2 a_n \\gamma) V_n + a_n^2 \\sigma^2,\n$$\nwhere we used $(1-x)^2 \\approx 1-2x$ for small $x=a_n\\gamma$. This recursion is the foundation for analyzing the options.\n\n### Option-by-Option Analysis\n\n**A. If $a_n=c/n$ and $2\\gamma c>1$, then a Central Limit Theorem holds with normalization $\\sqrt{n}$, and $\\sqrt{n}\\,(X_n-\\theta^\\star)\\;\\Rightarrow\\;\\mathcal{N}\\!\\left(0,\\;\\frac{\\sigma^2 c^2}{2\\gamma c - 1}\\right)$, with the choice $c=1/\\gamma$ minimizing the asymptotic variance among all $c>1/(2\\gamma)$.**\n\nWith $a_n = c/n$, the MSE recursion is $V_{n+1} \\approx (1 - 2\\gamma c/n) V_n + c^2\\sigma^2/n^2$. Under the condition $2\\gamma c > 1$, the algorithm achieves the optimal convergence rate for this class of recursions. Let's assume a solution of the form $V_n \\sim K/n$. Substituting this into the recursion:\n$$\n\\frac{K}{n+1} \\approx \\left(1 - \\frac{2\\gamma c}{n}\\right) \\frac{K}{n} + \\frac{c^2\\sigma^2}{n^2}.\n$$\nUsing the Taylor expansion $1/(n+1) \\approx 1/n - 1/n^2$:\n$$\n\\frac{K}{n} - \\frac{K}{n^2} \\approx \\frac{K}{n} - \\frac{2\\gamma c K}{n^2} + \\frac{c^2\\sigma^2}{n^2}.\n$$\nEquating the coefficients of the $1/n^2$ terms gives $-K \\approx -2\\gamma c K + c^2\\sigma^2$, which implies $K(2\\gamma c - 1) = c^2\\sigma^2$. For this to be well-defined with $K>0$, we need $2\\gamma c - 1 > 0$, or $2\\gamma c > 1$. This yields:\n$$\nK = \\frac{c^2\\sigma^2}{2\\gamma c - 1}.\n$$\nSince $\\mathbb{E}[(X_n-\\theta^\\star)^2] \\sim K/n$, the standard deviation decays as $1/\\sqrt{n}$. A Central Limit Theorem (CLT) for the error normalized by $\\sqrt{n}$ is expected. The asymptotic variance is given by $\\lim_{n\\to\\infty} n\\mathbb{E}[(X_n-\\theta^\\star)^2] = K$. Thus, $\\sqrt{n}(X_n-\\theta^\\star) \\Rightarrow \\mathcal{N}(0, \\frac{c^2\\sigma^2}{2\\gamma c - 1})$.\n\nTo find the value of $c > 1/(2\\gamma)$ that minimizes this variance, let $V(c) = \\frac{c^2\\sigma^2}{2\\gamma c - 1}$. We compute the derivative with respect to $c$:\n$$\n\\frac{dV}{dc} = \\sigma^2 \\frac{2c(2\\gamma c - 1) - c^2(2\\gamma)}{(2\\gamma c - 1)^2} = \\sigma^2 \\frac{4\\gamma c^2 - 2c - 2\\gamma c^2}{(2\\gamma c - 1)^2} = \\frac{2c\\sigma^2(\\gamma c - 1)}{(2\\gamma c - 1)^2}.\n$$\nSetting $dV/dc=0$ for $c>0$ gives $\\gamma c - 1 = 0$, so $c=1/\\gamma$. This value satisfies the condition $c > 1/(2\\gamma)$ since $\\gamma > 0$. Therefore, $c=1/\\gamma$ minimizes the asymptotic variance.\nThe statement is fully consistent with established theory.\nVerdict: **Correct**.\n\n**B. If $a_n=c/n$, then $\\mathbb{E}[(X_n-\\theta^\\star)^2]\\sim K/n^2$ for some constant $K>0$, so the normalization $\\sqrt{n}$ yields a degenerate limit; therefore, no Central Limit Theorem is possible in this regime.**\n\nAs shown in the analysis for option A, for the optimal case $2\\gamma c > 1$, the MSE behaves as $\\mathbb{E}[(X_n-\\theta^\\star)^2] \\sim K/n$. The statement's premise that the MSE decays as $K/n^2$ is incorrect. In fact, an MSE of order $O(1/n)$ is the characteristic behavior that leads to a non-degenerate CLT with $\\sqrt{n}$ normalization. If the MSE were $O(1/n^2)$, the correct normalization would be $n$, not $\\sqrt{n}$. The core claim about the MSE rate is false.\nVerdict: **Incorrect**.\n\n**C. If $a_n=c/n^\\alpha$ with $\\alpha\\in(1/2,1)$, then $\\mathbb{E}[(X_n-\\theta^\\star)^2]\\sim \\left(\\frac{c\\sigma^2}{2\\gamma}\\right)\\,n^{-\\alpha}$ and a Central Limit Theorem holds with normalization $n^{\\alpha/2}$: $n^{\\alpha/2}\\,(X_n-\\theta^\\star)\\;\\Rightarrow\\;\\mathcal{N}\\!\\left(0,\\;\\frac{c\\sigma^2}{2\\gamma}\\right)$.**\n\nFor this regime, the MSE recursion is $V_{n+1} \\approx (1 - 2\\gamma c n^{-\\alpha}) V_n + c^2 \\sigma^2 n^{-2\\alpha}$. The asymptotic behavior of this type of recursion can be found by approximating it with an ordinary differential equation (ODE). This analysis shows that the bias and variance terms balance in a way that leads to $V_n \\sim K n^{-\\alpha}$. More formally, standard results in stochastic approximation theory (e.g., from Fabian, 1968) establish that:\n$$\n\\lim_{n\\to\\infty} n^\\alpha \\mathbb{E}[(X_n-\\theta^\\star)^2] = \\frac{c^2 \\sigma^2}{2 c \\gamma} = \\frac{c \\sigma^2}{2\\gamma}.\n$$\nThis confirms the stated MSE rate: $\\mathbb{E}[(X_n-\\theta^\\star)^2] \\sim \\frac{c\\sigma^2}{2\\gamma} n^{-\\alpha}$.\nThe convergence rate of the standard deviation is thus $O(\\sqrt{n^{-\\alpha}}) = O(n^{-\\alpha/2})$. A CLT would therefore require normalization by $n^{\\alpha/2}$. The variance of the limiting normal distribution is given by the limit of the variance of the normalized sequence:\n$$\n\\lim_{n\\to\\infty} \\mathbb{E}[(n^{\\alpha/2}(X_n-\\theta^\\star))^2] = \\lim_{n\\to\\infty} n^\\alpha \\mathbb{E}[(X_n-\\theta^\\star)^2] = \\frac{c\\sigma^2}{2\\gamma}.\n$$\nSo, $n^{\\alpha/2}(X_n-\\theta^\\star) \\Rightarrow \\mathcal{N}(0, \\frac{c\\sigma^2}{2\\gamma})$. The statement accurately presents these established results.\nVerdict: **Correct**.\n\n**D. If $a_n=c/n^\\alpha$ with $\\alpha\\in(0,1/2]$, then almost sure convergence still holds by virtue of $\\sum_n a_n^2<\\infty$, but the Central Limit Theorem fails because the noise does not average sufficiently.**\n\nThis statement contains a critical flaw. The standard conditions for almost sure convergence of the Robbins-Monro algorithm (the Dvoretzky conditions) are $\\sum_n a_n = \\infty$ and $\\sum_n a_n^2 < \\infty$. For $a_n = c/n^\\alpha$:\n- $\\sum_n a_n = c \\sum_n n^{-\\alpha}$ diverges for $\\alpha \\le 1$, which is satisfied here.\n- $\\sum_n a_n^2 = c^2 \\sum_n n^{-2\\alpha}$ converges only if $2\\alpha > 1$, i.e., $\\alpha > 1/2$.\nFor the regime $\\alpha \\in (0, 1/2]$, the condition $\\sum_n a_n^2 < \\infty$ is violated. The statement's claim that almost sure convergence holds \"by virtue of $\\sum_n a_n^2 < \\infty$\" is false on two grounds: the premise ($\\sum a_n^2 < \\infty$) is false, and this condition is necessary for convergence, not just a nice-to-have property. When $\\sum a_n^2$ diverges, the accumulated noise variance is infinite, and the iterates typically fail to converge. The MSE does not go to zero, so convergence fails, and a CLT for the error (which should converge to $0$) is nonsensical.\nVerdict: **Incorrect**.\n\n**E. For $\\alpha\\in(1/2,1)$, taking smaller $\\alpha$ always yields faster convergence than $a_n=c/n$ in the sense that the standard deviation of $X_n-\\theta^\\star$ decays strictly faster in $n$; moreover, $a_n=c/n^\\alpha$ dominates $a_n=c/n$ in asymptotic variance for all $c>0$.**\n\nThis statement makes a comparison of convergence rates.\n- For $a_n = c/n$ (with $2\\gamma c > 1$), the standard deviation of the error decays as $O(n^{-1/2})$.\n- For $a_n = c/n^\\alpha$ with $\\alpha \\in (1/2, 1)$, the standard deviation decays as $O(n^{-\\alpha/2})$.\nSince $\\alpha \\in (1/2, 1)$, we have $\\alpha < 1$, which implies $\\alpha/2 < 1/2$. Therefore, $-1/2 < -\\alpha/2$. This means that $n^{-1/2}$ decays faster to zero than $n^{-\\alpha/2}$. The $a_n=c/n$ step-size rule yields a faster convergence rate than any $a_n=c/n^\\alpha$ with $\\alpha \\in (1/2,1)$.\nThe statement claims the opposite, that the $n^{-\\alpha/2}$ decay is faster; this is false. It also incorrectly claims smaller $\\alpha$ is better within the $(1/2,1)$ range; in fact, larger $\\alpha$ (closer to $1$) is better.\nThe second part of the statement compares asymptotic variances. The asymptotic variance for $a_n=c/n$ is for the $\\sqrt{n}$-normalized error, while for $a_n=c/n^\\alpha$ it is for the $n^{\\alpha/2}$-normalized error. These are fundamentally different quantities, and comparing their numerical values is like comparing apples and oranges; it is not a meaningful way to assess performance. The primary metric is the rate of convergence, which is superior for the $a_n=c/n$ case. The statement is incorrect in its comparison of rates and its comparison of variances.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{AC}$$", "id": "3348661"}, {"introduction": "While the Robbins-Monro algorithm is designed for root-finding, the Kiefer-Wolfowitz algorithm adapts the framework for stochastic optimization by estimating gradients from noisy function evaluations. This practice explores the core of that estimation process: the finite-difference approximation. You will compare the statistical properties of one-sided versus two-sided (central) difference estimators, revealing a fundamental trade-off between bias, variance, and computational cost that is central to designing effective stochastic optimization procedures [@problem_id:3348722].", "problem": "Consider the Kiefer-Wolfowitz stochastic approximation scheme for minimizing a sufficiently smooth objective function $f:\\mathbb{R}^d\\to\\mathbb{R}$ that can only be observed with noise. For any query point $x\\in\\mathbb{R}^d$, we observe $Y(x)=f(x)+\\varepsilon(x)$, where $\\mathbb{E}[\\varepsilon(x)]=0$ and $\\mathrm{Var}(\\varepsilon(x))=\\sigma^2$, and where $\\varepsilon(x)$ and $\\varepsilon(x')$ are independent for distinct query points $x\\neq x'$. Let $e_i$ denote the $i$th canonical basis vector in $\\mathbb{R}^d$. The stochastic gradient estimators along coordinate $i$ built from finite differences are defined by the one-sided (forward) difference\n$$\n\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)=\\frac{Y(x+\\delta e_i)-Y(x)}{\\delta},\n$$\nand the two-sided (central) difference\n$$\n\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)=\\frac{Y(x+\\delta e_i)-Y(x-\\delta e_i)}{2\\delta},\n$$\nfor a small perturbation size $\\delta>0$. Assume that $f$ has continuous third partial derivatives in a neighborhood of $x$. Using only the definitions of these estimators, Taylor expansions for $f$, and the basic properties of expectation and variance for independent noise, compare the bias orders and variance scaling of $\\widehat{g}_i^{\\mathrm{one}}$ and $\\widehat{g}_i^{\\mathrm{two}}$ as $\\delta\\to 0$, and determine when the two-sided scheme is preferable in Kiefer-Wolfowitz stochastic approximation, taking into account both statistical accuracy and the number of function evaluations required per iteration in dimension $d$.\n\nWhich of the following statements is correct?\n\nA. Under the stated smoothness and independence assumptions, $\\widehat{g}_i^{\\mathrm{one}}$ has bias of order $O(\\delta)$ and variance scaling $2\\sigma^2/\\delta^2$, while $\\widehat{g}_i^{\\mathrm{two}}$ has bias of order $O(\\delta^2)$ and variance scaling $\\sigma^2/(2\\delta^2)$. Two-sided differences are preferable when $f$ is sufficiently smooth, noise across the two perturbations is independent or can be made positively correlated to further reduce variance, and the dimension $d$ is modest so that the extra evaluations per coordinate are acceptable.\n\nB. Both one-sided and two-sided differences have bias of order $O(\\delta^2)$; however, the two-sided difference has variance scaling $2\\sigma^2/\\delta^2$ whereas the one-sided has variance scaling $\\sigma^2/(2\\delta^2)$. Two-sided differences are preferable only when the dimension $d$ is very large.\n\nC. The one-sided difference has bias of order $O(\\delta)$ and variance scaling $2\\sigma^2/\\delta^2$, whereas the two-sided difference has bias of order $O(\\delta^2)$ but variance scaling $4\\sigma^2/\\delta^2$, so the one-sided scheme is preferable whenever the noise is independent.\n\nD. In the Kiefer-Wolfowitz setting, two-sided differences are always preferable because symmetry eliminates the noise, yielding variance that does not depend on $\\delta$; moreover, the bias is of order $O(\\delta)$ for one-sided but $O(\\delta)$ for two-sided as well, so the variance advantage dominates regardless of dimension $d$.", "solution": "The problem statement is a valid exercise in the analysis of stochastic approximation methods. It is scientifically grounded, well-posed, and objective, providing all necessary information to proceed with a rigorous derivation.\n\nWe are asked to compare the one-sided and two-sided finite difference estimators for the gradient in the context of the Kiefer-Wolfowitz stochastic approximation algorithm. The comparison will be based on the bias and variance of these estimators, along with the computational cost per iteration. The true gradient component is denoted by $\\partial_i f(x) = \\frac{\\partial f}{\\partial x_i}(x)$.\n\nThe objective function $f$ is assumed to have continuous third partial derivatives. This allows us to use Taylor's theorem. For a small scalar $\\delta > 0$ and the canonical basis vector $e_i$, we have the following expansions around a point $x \\in \\mathbb{R}^d$:\n$$\nf(x+\\delta e_i) = f(x) + \\delta \\partial_i f(x) + \\frac{\\delta^2}{2!} \\partial_{ii}^2 f(x) + \\frac{\\delta^3}{3!} \\partial_{iii}^3 f(x) + O(\\delta^4)\n$$\n$$\nf(x-\\delta e_i) = f(x) - \\delta \\partial_i f(x) + \\frac{\\delta^2}{2!} \\partial_{ii}^2 f(x) - \\frac{\\delta^3}{3!} \\partial_{iii}^3 f(x) + O(\\delta^4)\n$$\nwhere $\\partial_{ii}^2 f(x)$ and $\\partial_{iii}^3 f(x)$ are the second and third partial derivatives of $f$ with respect to the $i$-th coordinate, evaluated at $x$. The noise $\\varepsilon(z)$ at any point $z$ satisfies $\\mathbb{E}[\\varepsilon(z)]=0$ and $\\mathrm{Var}(\\varepsilon(z))=\\sigma^2$. For distinct points $z_1 \\neq z_2$, the noise terms $\\varepsilon(z_1)$ and $\\varepsilon(z_2)$ are independent.\n\n**1. Analysis of the One-Sided Estimator**\n\nThe one-sided estimator is defined as:\n$$\n\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)=\\frac{Y(x+\\delta e_i)-Y(x)}{\\delta} = \\frac{f(x+\\delta e_i)+\\varepsilon(x+\\delta e_i)-f(x)-\\varepsilon(x)}{\\delta}\n$$\n\n**Bias of $\\widehat{g}_i^{\\mathrm{one}}$:**\nThe expectation of the estimator is:\n$$\n\\mathbb{E}[\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)] = \\frac{\\mathbb{E}[f(x+\\delta e_i)+\\varepsilon(x+\\delta e_i)] - \\mathbb{E}[f(x)+\\varepsilon(x)]}{\\delta}\n$$\nSince the function values $f(\\cdot)$ are deterministic and $\\mathbb{E}[\\varepsilon(\\cdot)] = 0$, this simplifies to:\n$$\n\\mathbb{E}[\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)] = \\frac{f(x+\\delta e_i) - f(x)}{\\delta}\n$$\nSubstituting the Taylor expansion for $f(x+\\delta e_i)$:\n$$\n\\mathbb{E}[\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)] = \\frac{(f(x) + \\delta \\partial_i f(x) + \\frac{\\delta^2}{2} \\partial_{ii}^2 f(x) + O(\\delta^3)) - f(x)}{\\delta} = \\partial_i f(x) + \\frac{\\delta}{2} \\partial_{ii}^2 f(x) + O(\\delta^2)\n$$\nThe bias is the difference between the expectation and the true value:\n$$\n\\mathrm{Bias}(\\widehat{g}_i^{\\mathrm{one}}) = \\mathbb{E}[\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)] - \\partial_i f(x) = \\frac{\\delta}{2} \\partial_{ii}^2 f(x) + O(\\delta^2)\n$$\nThus, the bias of the one-sided estimator is of order $O(\\delta)$.\n\n**Variance of $\\widehat{g}_i^{\\mathrm{one}}$:**\n$$\n\\mathrm{Var}(\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)) = \\mathrm{Var}\\left(\\frac{\\varepsilon(x+\\delta e_i) - \\varepsilon(x)}{\\delta}\\right) = \\frac{1}{\\delta^2} \\mathrm{Var}(\\varepsilon(x+\\delta e_i) - \\varepsilon(x))\n$$\nSince the query points $x+\\delta e_i$ and $x$ are distinct, the noise terms are independent. For independent random variables $A$ and $B$, $\\mathrm{Var}(A-B) = \\mathrm{Var}(A) + \\mathrm{Var}(B)$.\n$$\n\\mathrm{Var}(\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)) = \\frac{1}{\\delta^2} (\\mathrm{Var}(\\varepsilon(x+\\delta e_i)) + \\mathrm{Var}(\\varepsilon(x))) = \\frac{1}{\\delta^2}(\\sigma^2 + \\sigma^2) = \\frac{2\\sigma^2}{\\delta^2}\n$$\nThe variance scales as $2\\sigma^2/\\delta^2$.\n\n**2. Analysis of the Two-Sided Estimator**\n\nThe two-sided, or central, estimator is defined as:\n$$\n\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)=\\frac{Y(x+\\delta e_i)-Y(x-\\delta e_i)}{2\\delta} = \\frac{f(x+\\delta e_i)+\\varepsilon(x+\\delta e_i)- (f(x-\\delta e_i)+\\varepsilon(x-\\delta e_i))}{2\\delta}\n$$\n\n**Bias of $\\widehat{g}_i^{\\mathrm{two}}$:**\n$$\n\\mathbb{E}[\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)] = \\frac{f(x+\\delta e_i) - f(x-\\delta e_i)}{2\\delta}\n$$\nSubstituting the Taylor expansions for $f(x+\\delta e_i)$ and $f(x-\\delta e_i)$:\n$$\nf(x+\\delta e_i) - f(x-\\delta e_i) = \\left(f(x) + \\delta \\partial_i f(x) + \\frac{\\delta^2}{2} \\partial_{ii}^2 f(x) + \\frac{\\delta^3}{6} \\partial_{iii}^3 f(x)\\right) - \\left(f(x) - \\delta \\partial_i f(x) + \\frac{\\delta^2}{2} \\partial_{ii}^2 f(x) - \\frac{\\delta^3}{6} \\partial_{iii}^3 f(x)\\right) + O(\\delta^5)\n$$\nThe even-powered terms in $\\delta$ cancel out:\n$$\nf(x+\\delta e_i) - f(x-\\delta e_i) = 2\\delta \\partial_i f(x) + \\frac{2\\delta^3}{6} \\partial_{iii}^3 f(x) + O(\\delta^5) = 2\\delta \\partial_i f(x) + \\frac{\\delta^3}{3} \\partial_{iii}^3 f(x) + O(\\delta^5)\n$$\nDividing by $2\\delta$:\n$$\n\\mathbb{E}[\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)] = \\partial_i f(x) + \\frac{\\delta^2}{6} \\partial_{iii}^3 f(x) + O(\\delta^4)\n$$\nThe bias is:\n$$\n\\mathrm{Bias}(\\widehat{g}_i^{\\mathrm{two}}) = \\mathbb{E}[\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)] - \\partial_i f(x) = \\frac{\\delta^2}{6} \\partial_{iii}^3 f(x) + O(\\delta^4)\n$$\nThus, the bias of the two-sided estimator is of order $O(\\delta^2)$. This is a significant improvement over the one-sided estimator.\n\n**Variance of $\\widehat{g}_i^{\\mathrm{two}}$:**\n$$\n\\mathrm{Var}(\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)) = \\mathrm{Var}\\left(\\frac{\\varepsilon(x+\\delta e_i) - \\varepsilon(x-\\delta e_i)}{2\\delta}\\right) = \\frac{1}{4\\delta^2} \\mathrm{Var}(\\varepsilon(x+\\delta e_i) - \\varepsilon(x-\\delta e_i))\n$$\nThe query points $x+\\delta e_i$ and $x-\\delta e_i$ are distinct, so the noise terms are independent.\n$$\n\\mathrm{Var}(\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)) = \\frac{1}{4\\delta^2} (\\mathrm{Var}(\\varepsilon(x+\\delta e_i)) + \\mathrm{Var}(\\varepsilon(x-\\delta e_i))) = \\frac{1}{4\\delta^2}(\\sigma^2 + \\sigma^2) = \\frac{2\\sigma^2}{4\\delta^2} = \\frac{\\sigma^2}{2\\delta^2}\n$$\nThe variance scales as $\\sigma^2/(2\\delta^2)$. This is a factor of $4$ smaller than the variance of the one-sided estimator.\n\n**3. Comparison and Conclusion**\n\n| Estimator                 | Bias Order      | Variance Scaling                | Function Evals per Iteration ($d$-dim) |\n|---------------------------|-----------------|---------------------------------|---------------------------------|\n| $\\widehat{g}^{\\mathrm{one}}$  | $O(\\delta)$     | $\\frac{2\\sigma^2}{\\delta^2}$   | $d+1$                             |\n| $\\widehat{g}^{\\mathrm{two}}$  | $O(\\delta^2)$   | $\\frac{\\sigma^2}{2\\delta^2}$    | $2d$                              |\n\nThe two-sided estimator is superior in both bias (order $O(\\delta^2)$ vs. $O(\\delta)$) and variance (factor of $4$ lower). This superior statistical accuracy generally leads to faster convergence of the Kiefer-Wolfowitz algorithm. The trade-off is the computational cost: the two-sided scheme requires $2d$ function evaluations per iteration, whereas the one-sided scheme requires $d+1$. For large $d$, this is approximately a factor of $2$ increase in cost. The two-sided scheme is therefore preferable when its superior statistical properties outweigh its higher computational cost. This is typically the case when function evaluations are not prohibitively expensive and the dimension $d$ is not so large as to make the $2d$ cost unacceptable (i.e., $d$ is modest).\n\n**Evaluation of Options:**\n\n*   **A**: This option correctly states: $\\widehat{g}_i^{\\mathrm{one}}$ has bias $O(\\delta)$ and variance $2\\sigma^2/\\delta^2$; $\\widehat{g}_i^{\\mathrm{two}}$ has bias $O(\\delta^2)$ and variance $\\sigma^2/(2\\delta^2)$. It correctly identifies the conditions for preference: sufficient smoothness of $f$ (which enables the $O(\\delta^2)$ bias), modest dimension $d$ (balancing statistical gain against computational cost). It also correctly notes that using common random numbers (inducing positive correlation) can further reduce the variance of the two-sided estimator, making it even more attractive. This statement is entirely accurate.\n*   **B**: This option incorrectly claims both estimators have bias of order $O(\\delta^2)$ and incorrectly swaps the variance expressions. It is factually wrong.\n*   **C**: This option correctly states the properties of the one-sided estimator and the bias of the two-sided one. However, it incorrectly states the variance of the two-sided estimator is $4\\sigma^2/\\delta^2$, which is off by a factor of $8$.\n*   **D**: This option is fundamentally flawed. It falsely claims the variance of the two-sided estimator is independent of $\\delta$, and incorrectly states its bias is $O(\\delta)$.\n\nBased on the rigorous derivation, Option A provides a complete and correct summary of the properties and practical considerations for both estimators.\n\n**Verdict:**\n- A: **Correct**. The bias orders, variance scalings, and the discussion of the trade-offs are all accurate.\n- B: **Incorrect**. Incorrect bias for the one-sided estimator and swapped variance terms.\n- C: **Incorrect**. Incorrect variance for the two-sided estimator.\n- D: **Incorrect**. Incorrect bias and variance properties for the two-sided estimator.", "answer": "$$\\boxed{A}$$", "id": "3348722"}, {"introduction": "The efficiency of stochastic approximation is often limited by the variance of the noisy observations, which directly impacts the convergence speed. This hands-on problem demonstrates a powerful technique for mitigating this issue: the use of Common Random Numbers (CRN) within the Kiefer-Wolfowitz method. By explicitly deriving the variance of the gradient estimator, you will quantify the precise benefit of inducing positive correlation between function evaluations, a key strategy for accelerating convergence in simulation-based optimization [@problem_id:3348680].", "problem": "Consider a stochastic objective with mean response function $f(\\theta) = \\mathbb{E}[Y(\\theta, \\xi)]$ that is differentiable in a neighborhood of a given scalar parameter $\\theta \\in \\mathbb{R}$. In the Kiefer-Wolfowitz (KW) finite-difference method, the gradient is estimated via a symmetric difference using two noisy observations at perturbations $\\theta \\pm \\delta$, where $\\delta > 0$ is small. Define the one-step KW gradient estimator\n$$\n\\widehat{g}(\\theta; \\delta) = \\frac{Y(\\theta + \\delta) - Y(\\theta - \\delta)}{2 \\delta}.\n$$\nAssume the following noise structure for the observations: for any fixed $\\theta$ and $\\delta$, write\n$$\nY(\\theta \\pm \\delta) = f(\\theta \\pm \\delta) + \\varepsilon_{\\pm},\n$$\nwhere $\\mathbb{E}[\\varepsilon_{\\pm}] = 0$, $\\operatorname{Var}(\\varepsilon_{\\pm}) = \\sigma^{2}$ for some $\\sigma^{2} \\in (0, \\infty)$, and, when using Common Random Numbers (CRN), the pair $(\\varepsilon_{+}, \\varepsilon_{-})$ has correlation $\\rho \\in [-1, 1]$. Without CRN, the pair is independent, corresponding to $\\rho = 0$. Throughout, $f(\\cdot)$ is deterministic and the only source of randomness is $(\\varepsilon_{+}, \\varepsilon_{-})$.\n\nUsing only linearity of expectation and the definitions of variance and covariance, perform the following:\n\n- Derive $\\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta))$ under CRN in terms of $\\sigma^{2}$ and $\\rho$.\n\n- Derive $\\operatorname{Var}(\\widehat{g}(\\theta; \\delta))$ under CRN and, separately, under independence.\n\n- Define the variance impact ratio $R(\\rho)$ as\n$$\nR(\\rho) \\equiv \\frac{\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under CRN})}{\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under independence})}.\n$$\nCompute a closed-form analytic expression for $R(\\rho)$ as a function of $\\rho$.\n\nProvide, as your final answer, the expression for $R(\\rho)$. No numerical rounding is required.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   Objective function mean: $f(\\theta) = \\mathbb{E}[Y(\\theta, \\xi)]$\n-   Parameter: $\\theta \\in \\mathbb{R}$\n-   Kiefer-Wolfowitz gradient estimator: $\\widehat{g}(\\theta; \\delta) = \\frac{Y(\\theta + \\delta) - Y(\\theta - \\delta)}{2 \\delta}$, with $\\delta > 0$.\n-   Observation noise model: $Y(\\theta \\pm \\delta) = f(\\theta \\pm \\delta) + \\varepsilon_{\\pm}$.\n-   Noise properties: $\\mathbb{E}[\\varepsilon_{\\pm}] = 0$, $\\operatorname{Var}(\\varepsilon_{\\pm}) = \\sigma^{2}$ for $\\sigma^{2} \\in (0, \\infty)$.\n-   Correlation under Common Random Numbers (CRN): $\\rho = \\operatorname{Corr}(\\varepsilon_{+}, \\varepsilon_{-}) \\in [-1, 1]$.\n-   Independence case: The pair $(\\varepsilon_{+}, \\varepsilon_{-})$ is independent, corresponding to $\\rho = 0$.\n-   Function $f(\\cdot)$ is deterministic.\n-   The only source of randomness is the noise pair $(\\varepsilon_{+}, \\varepsilon_{-})$.\n-   Variance impact ratio definition: $R(\\rho) \\equiv \\frac{\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under CRN})}{\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under independence})}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard theoretical exercise in stochastic simulation and optimization, specifically concerning the variance of the Kiefer-Wolfowitz finite-difference gradient estimator.\n-   **Scientifically Grounded**: The problem is based on established principles of probability theory (expectation, variance, covariance, correlation) and their application to the analysis of stochastic algorithms. The Kiefer-Wolfowitz method is a cornerstone of stochastic approximation.\n-   **Well-Posed**: The problem is clearly stated, providing all necessary definitions and assumptions to derive the required quantities. The questions are specific and lead to a unique analytical solution.\n-   **Objective**: The language is precise and mathematical, containing no subjective or ambiguous statements.\n\nThe problem is deemed valid as it does not violate any of the specified criteria for invalidity. We may proceed with the solution.\n\n---\n\nThe solution is derived in three parts as requested by the problem statement.\n\n### Part 1: Derivation of $\\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta))$\nWe are asked to derive the covariance between the two noisy observations, $Y(\\theta + \\delta)$ and $Y(\\theta - \\delta)$, under the CRN assumption. The definition of covariance between two random variables $X$ and $Z$ is $\\operatorname{Cov}(X, Z) = \\mathbb{E}[(X - \\mathbb{E}[X])(Z - \\mathbb{E}[Z])]$.\n\nLet $X = Y(\\theta + \\delta)$ and $Z = Y(\\theta - \\delta)$. First, we compute their expectations. Using the linearity of expectation and the given properties:\n$$\n\\mathbb{E}[Y(\\theta + \\delta)] = \\mathbb{E}[f(\\theta + \\delta) + \\varepsilon_{+}] = \\mathbb{E}[f(\\theta + \\delta)] + \\mathbb{E}[\\varepsilon_{+}]\n$$\nSince $f(\\cdot)$ is a deterministic function, $\\mathbb{E}[f(\\theta + \\delta)] = f(\\theta + \\delta)$. We are given $\\mathbb{E}[\\varepsilon_{+}] = 0$. Thus,\n$$\n\\mathbb{E}[Y(\\theta + \\delta)] = f(\\theta + \\delta)\n$$\nSimilarly, for $Y(\\theta - \\delta)$:\n$$\n\\mathbb{E}[Y(\\theta - \\delta)] = \\mathbb{E}[f(\\theta - \\delta) + \\varepsilon_{-}] = f(\\theta - \\delta) + \\mathbb{E}[\\varepsilon_{-}] = f(\\theta - \\delta)\n$$\nNow we can apply the definition of covariance:\n$$\n\\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta)) = \\mathbb{E}[(Y(\\theta + \\delta) - \\mathbb{E}[Y(\\theta + \\delta)])(Y(\\theta - \\delta) - \\mathbb{E}[Y(\\theta - \\delta)])]\n$$\nSubstituting the expressions for the observations and their means:\n$$\n= \\mathbb{E}[((f(\\theta + \\delta) + \\varepsilon_{+}) - f(\\theta + \\delta))((f(\\theta - \\delta) + \\varepsilon_{-}) - f(\\theta - \\delta))]\n$$\n$$\n= \\mathbb{E}[\\varepsilon_{+} \\varepsilon_{-}]\n$$\nBy definition, the covariance of the noise terms is $\\operatorname{Cov}(\\varepsilon_{+}, \\varepsilon_{-}) = \\mathbb{E}[\\varepsilon_{+} \\varepsilon_{-}] - \\mathbb{E}[\\varepsilon_{+}]\\mathbb{E}[\\varepsilon_{-}]$. Since $\\mathbb{E}[\\varepsilon_{+}] = 0$ and $\\mathbb{E}[\\varepsilon_{-}] = 0$, this simplifies to $\\operatorname{Cov}(\\varepsilon_{+}, \\varepsilon_{-}) = \\mathbb{E}[\\varepsilon_{+} \\varepsilon_{-}]$.\n\nThe correlation $\\rho$ is defined as $\\rho = \\operatorname{Corr}(\\varepsilon_{+}, \\varepsilon_{-}) = \\frac{\\operatorname{Cov}(\\varepsilon_{+}, \\varepsilon_{-})}{\\sqrt{\\operatorname{Var}(\\varepsilon_{+})\\operatorname{Var}(\\varepsilon_{-})}}$.\nWe are given $\\operatorname{Var}(\\varepsilon_{+}) = \\sigma^2$ and $\\operatorname{Var}(\\varepsilon_{-}) = \\sigma^2$. Substituting these into the correlation formula:\n$$\n\\rho = \\frac{\\operatorname{Cov}(\\varepsilon_{+}, \\varepsilon_{-})}{\\sqrt{\\sigma^2 \\cdot \\sigma^2}} = \\frac{\\operatorname{Cov}(\\varepsilon_{+}, \\varepsilon_{-})}{\\sigma^2}\n$$\nFrom this, we find $\\operatorname{Cov}(\\varepsilon_{+}, \\varepsilon_{-}) = \\rho \\sigma^2$. Therefore,\n$$\n\\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta)) = \\rho \\sigma^2\n$$\n\n### Part 2: Derivation of $\\operatorname{Var}(\\widehat{g}(\\theta; \\delta))$\nWe need to derive the variance of the gradient estimator $\\widehat{g}(\\theta; \\delta)$ under CRN and, separately, under independence. We use the property of variance for a linear combination of random variables: $\\operatorname{Var}(aX + bZ) = a^2\\operatorname{Var}(X) + b^2\\operatorname{Var}(Z) + 2ab\\operatorname{Cov}(X, Z)$.\n\nThe estimator is $\\widehat{g}(\\theta; \\delta) = \\frac{1}{2\\delta} Y(\\theta + \\delta) - \\frac{1}{2\\delta} Y(\\theta - \\delta)$.\nHere, $a = \\frac{1}{2\\delta}$, $b = -\\frac{1}{2\\delta}$, $X = Y(\\theta + \\delta)$, and $Z = Y(\\theta - \\delta)$. The variance is:\n$$\n\\operatorname{Var}(\\widehat{g}(\\theta; \\delta)) = \\left(\\frac{1}{2\\delta}\\right)^2 \\operatorname{Var}(Y(\\theta + \\delta)) + \\left(-\\frac{1}{2\\delta}\\right)^2 \\operatorname{Var}(Y(\\theta - \\delta)) + 2\\left(\\frac{1}{2\\delta}\\right)\\left(-\\frac{1}{2\\delta}\\right) \\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta))\n$$\n$$\n= \\frac{1}{4\\delta^2} \\left[ \\operatorname{Var}(Y(\\theta + \\delta)) + \\operatorname{Var}(Y(\\theta - \\delta)) - 2\\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta)) \\right]\n$$\nFirst, we find the variances of the individual observations:\n$$\n\\operatorname{Var}(Y(\\theta + \\delta)) = \\operatorname{Var}(f(\\theta + \\delta) + \\varepsilon_{+}) = \\operatorname{Var}(\\varepsilon_{+}) = \\sigma^2\n$$\nbecause $f(\\theta + \\delta)$ is a deterministic constant. Similarly, $\\operatorname{Var}(Y(\\theta - \\delta)) = \\sigma^2$.\n\n**Case 1: Under CRN**\nWe substitute the variances and the covariance derived in Part 1 ($\\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta)) = \\rho \\sigma^2$) into the expression for $\\operatorname{Var}(\\widehat{g}(\\theta; \\delta))$:\n$$\n\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under CRN}) = \\frac{1}{4\\delta^2} [\\sigma^2 + \\sigma^2 - 2(\\rho \\sigma^2)] = \\frac{2\\sigma^2 - 2\\rho\\sigma^2}{4\\delta^2} = \\frac{2\\sigma^2(1 - \\rho)}{4\\delta^2}\n$$\n$$\n\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under CRN}) = \\frac{\\sigma^2(1-\\rho)}{2\\delta^2}\n$$\n\n**Case 2: Under Independence**\nThe independence of $\\varepsilon_{+}$ and $\\varepsilon_{-}$ is equivalent to setting their correlation $\\rho = 0$. In this case, their covariance is zero, and thus $\\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta)) = 0$. We can obtain the result by setting $\\rho=0$ in the CRN formula:\n$$\n\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under independence}) = \\frac{\\sigma^2(1-0)}{2\\delta^2} = \\frac{\\sigma^2}{2\\delta^2}\n$$\nAlternatively, substituting $\\operatorname{Cov}(\\cdot)=0$ directly into the general variance formula:\n$$\n\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under independence}) = \\frac{1}{4\\delta^2} [\\sigma^2 + \\sigma^2 - 0] = \\frac{2\\sigma^2}{4\\delta^2} = \\frac{\\sigma^2}{2\\delta^2}\n$$\n\n### Part 3: Computation of the Variance Impact Ratio $R(\\rho)$\nThe variance impact ratio $R(\\rho)$ is defined as the ratio of the variance of the estimator under CRN to its variance under independence.\n$$\nR(\\rho) = \\frac{\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under CRN})}{\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under independence})}\n$$\nUsing the expressions derived in Part 2:\n$$\nR(\\rho) = \\frac{\\frac{\\sigma^2(1-\\rho)}{2\\delta^2}}{\\frac{\\sigma^2}{2\\delta^2}}\n$$\nThe terms $\\frac{\\sigma^2}{2\\delta^2}$ in the numerator and denominator cancel out, as long as $\\sigma^2 > 0$ and $\\delta > 0$, which are given. This leaves:\n$$\nR(\\rho) = 1 - \\rho\n$$\nThis expression quantifies the effect of using CRN. A positive correlation ($\\rho > 0$) reduces the variance of the gradient estimate, which is the primary motivation for using CRN in this context. A negative correlation ($\\rho < 0$) inflates the variance.", "answer": "$$\n\\boxed{1 - \\rho}\n$$", "id": "3348680"}]}