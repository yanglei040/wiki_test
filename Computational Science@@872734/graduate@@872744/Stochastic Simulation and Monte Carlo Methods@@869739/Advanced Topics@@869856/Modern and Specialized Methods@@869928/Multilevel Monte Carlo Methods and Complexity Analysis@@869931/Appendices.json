{"hands_on_practices": [{"introduction": "The remarkable efficiency of the Multilevel Monte Carlo method hinges on a carefully engineered statistical structure. This exercise challenges you to explore the two most critical implementation details: inducing strong positive correlation of estimators *within* each level correction and maintaining statistical independence *between* different levels. By dissecting the estimator's bias and variance, you will confirm why these design choices are fundamental to the method's validity and its celebrated complexity results [@problem_id:3322274].", "problem": "Consider a quantity of interest $P$ defined on a probability space through a stochastic model (for example, the terminal functional of a stochastic differential equation). Let $\\{P_l\\}_{l=0}^L$ be a hierarchy of discretized approximations with increasing resolution indexed by the level $l$, so that $P_L$ is the finest approximation used. Define the Multilevel Monte Carlo (MLMC) estimator (Multilevel Monte Carlo (MLMC)) as\n$$\n\\widehat{P}_L \\;=\\; \\frac{1}{N_0}\\sum_{i=1}^{N_0} P_0^{(i)} \\;+\\; \\sum_{l=1}^{L} \\frac{1}{N_l}\\sum_{i=1}^{N_l} \\Big(P_l^{(i)} - P_{l-1}^{(i)}\\Big),\n$$\nwhere, for each level $l\\ge 1$, the pairs $\\big(P_l^{(i)},P_{l-1}^{(i)}\\big)$ are generated using a coupling that shares randomness within the pair to reduce the variance of the level-difference. Across different levels $l\\neq k$, the canonical MLMC design employs independent random number streams so that the level averages are mutually independent. In practice, two implementation choices are critical:\n\n- Independent random streams per level average: the random inputs used to form the averages at different levels are independent.\n- Reproducible shared randomness within coupled pairs $\\big(P_l,P_{l-1}\\big)$: the construction uses a consistent mapping of the fine-level random inputs to the coarse-level inputs within each pair (for example, summing Brownian increments on the fine grid to obtain coarse increments), so that the coupling is stable and replicable.\n\nStarting from the basic principles of linearity of expectation, variance additivity under independence, and the covariance decomposition for the variance of a sum, explain why these two implementation choices are needed for standard MLMC complexity claims. Then analyze the effect of accidental correlation across levels (for example, due to reusing a global random number generator state across levels without resetting per-level seeds), specifically on the bias and variance of $\\widehat{P}_L$. You may assume that for each level $l$, the $N_l$ samples $\\{(P_l^{(i)},P_{l-1}^{(i)})\\}_{i=1}^{N_l}$ are identically distributed and that their marginal distributions are correct for the discretizations at that level.\n\nWhich statement best captures the requirements and consequences described above?\n\nA. Independence across level averages is unnecessary because accidental correlation across levels does not change the variance of $\\widehat{P}_L$, while reproducible shared randomness within pairs introduces bias in $\\mathbb{E}[\\widehat{P}_L]$ by breaking the telescoping identity.\n\nB. Independence across level averages is necessary for the standard MLMC complexity analysis because it yields $\\operatorname{Var}[\\widehat{P}_L] = \\sum_{l=0}^L \\operatorname{Var}(Y_l)$ with $Y_0=\\frac{1}{N_0}\\sum_{i=1}^{N_0}P_0^{(i)}$ and $Y_l=\\frac{1}{N_l}\\sum_{i=1}^{N_l}(P_l^{(i)}-P_{l-1}^{(i)})$, and reproducible shared randomness within each coupled pair minimizes $\\operatorname{Var}(P_l - P_{l-1})$ via positive $\\operatorname{Cov}(P_l,P_{l-1})$. Accidental correlation across levels leaves $\\mathbb{E}[\\widehat{P}_L]$ unchanged but introduces covariance terms $2\\sum_{kl}\\operatorname{Cov}(Y_k,Y_l)$ that distort variance and thereby the work-versus-accuracy predictions.\n\nC. Independence across level averages is required to avoid bias in $\\mathbb{E}[\\widehat{P}_L]$, because without it $\\mathbb{E}[\\widehat{P}_L]\\neq \\mathbb{E}[P_L]$. Reproducible shared randomness within pairs increases $\\operatorname{Var}(P_l - P_{l-1})$ due to positive correlation between $P_l$ and $P_{l-1}$.\n\nD. Accidental correlation across levels strictly decreases the variance of $\\widehat{P}_L$ because covariance terms are always negative in MLMC, thereby improving complexity bounds; independence is therefore overly conservative.\n\nE. Using independent random streams per level and shared randomness inside each pair eliminates both variance and bias completely, making $\\widehat{P}_L=P$ almost surely for finite $N_l$ and finite $L$.", "solution": "To analyze the effects of the implementation choices, we examine the bias and variance of the MLMC estimator $\\widehat{P}_L$.\n\n**1. Bias Analysis (Expectation of the Estimator)**\nThe expectation of $\\widehat{P}_L$ is found using the linearity of expectation. This property holds regardless of any statistical dependence between the random variables.\n$$\n\\mathbb{E}[\\widehat{P}_L] = \\mathbb{E}\\left[\\frac{1}{N_0}\\sum_{i=1}^{N_0} P_0^{(i)}\\right] + \\sum_{l=1}^{L} \\mathbb{E}\\left[\\frac{1}{N_l}\\sum_{i=1}^{N_l} \\Big(P_l^{(i)} - P_{l-1}^{(i)}\\Big)\\right]\n$$\nSince samples are identically distributed at each level:\n$$\n\\mathbb{E}[\\widehat{P}_L] = \\mathbb{E}[P_0] + \\sum_{l=1}^{L} \\mathbb{E}[P_l - P_{l-1}] = \\mathbb{E}[P_0] + \\sum_{l=1}^{L} (\\mathbb{E}[P_l] - \\mathbb{E}[P_{l-1}])\n$$\nThis is a telescoping sum that simplifies to $\\mathbb{E}[\\widehat{P}_L] = \\mathbb{E}[P_L]$. The bias is therefore $\\mathbb{E}[P_L] - \\mathbb{E}[P]$, which is the discretization error of the finest level.\n*Conclusion on bias*: Neither the coupling within pairs nor accidental correlation across levels affects the expectation of the estimator. $\\mathbb{E}[\\widehat{P}_L]$ remains equal to $\\mathbb{E}[P_L]$.\n\n**2. Variance Analysis**\nLet $Y_l$ be the sample average for level $l$ (with $Y_0$ for the base level and $Y_l$ for the corrections $l \\geq 1$). The estimator is $\\widehat{P}_L = \\sum_{l=0}^L Y_l$. The variance is:\n$$\n\\operatorname{Var}[\\widehat{P}_L] = \\operatorname{Var}\\left[\\sum_{l=0}^L Y_l\\right] = \\sum_{l=0}^L \\operatorname{Var}[Y_l] + 2\\sum_{0 \\le k  l \\le L} \\operatorname{Cov}(Y_k, Y_l)\n$$\n*   **Role of Independence Across Levels**: The standard MLMC analysis requires independence between the estimations at different levels. This makes $\\operatorname{Cov}(Y_k, Y_l) = 0$ for $k \\neq l$. The variance then simplifies to the tractable sum $\\operatorname{Var}[\\widehat{P}_L] = \\sum_{l=0}^L \\operatorname{Var}[Y_l]$. This additive structure is essential for the optimization procedure that yields the MLMC complexity theorems. If there is accidental correlation, the covariance terms are non-zero, distorting the variance and invalidating the standard complexity analysis and optimal sample allocation.\n*   **Role of Shared Randomness (Coupling) Within Pairs**: For each level $l \\ge 1$, we want to minimize $\\operatorname{Var}[P_l - P_{l-1}]$. This variance is given by $\\operatorname{Var}[P_l] + \\operatorname{Var}[P_{l-1}] - 2\\operatorname{Cov}(P_l, P_{l-1})$. By using shared randomness, we induce a strong positive correlation between $P_l$ and $P_{l-1}$, making $\\operatorname{Cov}(P_l, P_{l-1})$ large and positive. This reduces the variance of the difference, which is the primary goal of the coupling.\n\n**Evaluating the Options:**\n-   **A**: Incorrect. Correlation changes the variance, and coupling does not introduce bias.\n-   **B**: Correct. It accurately states that independence across levels is needed for the variance sum formula, that coupling reduces the variance of the difference via positive covariance, that accidental cross-level correlation does not change the bias, but introduces covariance terms that invalidate the standard variance analysis.\n-   **C**: Incorrect. Independence across levels is not for bias, and coupling *reduces* not increases the variance of the difference.\n-   **D**: Incorrect. The covariance from accidental correlation is not guaranteed to be negative.\n-   **E**: Incorrect. MLMC for finite $N_l$ and $L$ has non-zero variance and bias.\n\nTherefore, statement B provides the most accurate and complete description.", "answer": "$$\n\\boxed{B}\n$$", "id": "3322274"}, {"introduction": "Building on the foundational variance structure, we now address the central theorem of MLMC complexity. This classic theoretical exercise asks: for a desired accuracy tolerance $\\varepsilon$, what is the minimum computational work required? This practice will guide you through using the method of Lagrange multipliers to derive the optimal allocation of samples across levels, revealing how the overall complexity depends critically on the relationship between variance decay and computational cost per level [@problem_id:3322273].", "problem": "Consider a Multilevel Monte Carlo (MLMC) estimator for a scalar quantity of interest $Q$ in which discretizations $Q(h_{\\ell})$ at mesh sizes $h_{\\ell}$ are organized on levels $\\ell = 0,1,\\dots,L$. Assume a geometric refinement $h_{\\ell} = h_{0} m^{-\\ell}$ for some $m  1$, and define the level differences $\\Delta_{\\ell} = Q(h_{\\ell}) - Q(h_{\\ell-1})$ with the convention $Q(h_{-1}) \\equiv 0$. The MLMC estimator is constructed as the telescoping sum of sample averages of the $\\Delta_{\\ell}$. Suppose the following widely used scaling assumptions hold:\n- The bias (discretization error) satisfies $\\left|\\mathbb{E}[Q(h_{L})] - \\mathbb{E}[Q]\\right| \\leq c_{b} h_{L}^{\\alpha}$ for some constants $c_{b}  0$ and $\\alpha  0$.\n- The variance of level differences satisfies $\\mathrm{Var}(\\Delta_{\\ell}) \\asymp c_{v} h_{\\ell}^{\\beta}$ for some constants $c_{v}  0$ and $\\beta  0$.\n- The computational cost per independent sample of $\\Delta_{\\ell}$ satisfies $C_{\\ell} \\asymp c_{c} h_{\\ell}^{-\\gamma}$ for some constants $c_{c}  0$ and $\\gamma  0$.\n\nLet $N_{\\ell}$ denote the number of independent samples used on level $\\ell$, so that the MLMC estimator uses a total computational work $W = \\sum_{\\ell=0}^{L} N_{\\ell} C_{\\ell}$, and has mean-squared error $\\mathrm{MSE} = \\left(\\text{bias}\\right)^{2} + \\sum_{\\ell=0}^{L} \\mathrm{Var}(\\Delta_{\\ell})/N_{\\ell}$. The goal is to choose the level $L(\\varepsilon)$ and the allocation $\\{N_{\\ell}(\\varepsilon)\\}$ so that $\\mathrm{MSE} \\leq \\varepsilon^{2}$ while minimizing the work $W$ as $\\varepsilon \\to 0$.\n\nWhich of the following statements about the optimal choice of $L(\\varepsilon)$, the optimal variance allocation across levels, and the resulting asymptotic work complexity (including any logarithmic factors) is correct?\n\nA. Choose $L(\\varepsilon)$ as the minimal integer such that $c_{b} h_{L}^{\\alpha} \\leq \\varepsilon/\\sqrt{2}$; choose the sample sizes by the optimal variance allocation $N_{\\ell} \\propto \\sqrt{\\mathrm{Var}(\\Delta_{\\ell})/C_{\\ell}}$. Then the minimal work scales as $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2})$ if $\\beta  \\gamma$, $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2} \\left(\\log \\varepsilon^{-1}\\right)^{2}\\right)$ if $\\beta = \\gamma$, and $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2 - (\\gamma - \\beta)/\\alpha}\\right)$ if $\\beta  \\gamma$.\n\nB. Choose $L(\\varepsilon)$ as the minimal integer such that $c_{b} h_{L}^{\\alpha} \\leq \\varepsilon^{2}$; allocate samples as $N_{\\ell} \\propto \\mathrm{Var}(\\Delta_{\\ell})$. Then the minimal work scales as $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2})$ for all relations between $\\beta$ and $\\gamma$, with no logarithmic factor.\n\nC. Choose $L(\\varepsilon)$ so that $h_{L} \\asymp \\varepsilon^{1/\\alpha}$; allocate samples as $N_{\\ell} \\propto \\sqrt{\\mathrm{Var}(\\Delta_{\\ell})\\,C_{\\ell}}$. Then the minimal work scales as $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2} \\log \\varepsilon^{-1}\\right)$ when $\\beta = \\gamma$, and $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2 - (\\gamma - \\beta)/(2\\alpha)}\\right)$ when $\\beta  \\gamma$.\n\nD. Choose $L(\\varepsilon)$ so that $h_{L} \\asymp \\varepsilon^{1/\\alpha}$; allocate samples to equalize the per-level variance contributions, i.e., $N_{\\ell} \\propto \\mathrm{Var}(\\Delta_{\\ell})$. Then $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2})$ if $\\beta  \\gamma$, and $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-3})$ if $\\beta  \\gamma$.\n\nE. In the borderline case $\\beta = \\gamma$, if one uses the optimal allocation $N_{\\ell} \\propto \\sqrt{\\mathrm{Var}(\\Delta_{\\ell})/C_{\\ell}}$, the minimal work scales as $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2} \\log \\varepsilon^{-1}\\right)$, where the logarithmic factor arises solely because $L(\\varepsilon)$ grows like $\\log \\varepsilon^{-1}$.", "solution": "The problem statement describes a standard setup for the complexity analysis of the Multilevel Monte Carlo (MLMC) method. All provided definitions, assumptions, and goals are consistent with the established literature on this topic (e.g., Giles, M. B. \"Multilevel Monte Carlo path simulation.\" Operations Research 56.3 (2008): 607-617). The problem is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution regarding the asymptotic complexity. Therefore, the problem statement is valid.\n\nWe proceed to derive the optimal work complexity for the MLMC estimator.\nThe goal is to minimize the total computational work, $W$, subject to the constraint that the mean-squared error, $\\mathrm{MSE}$, is less than or equal to a prescribed tolerance $\\varepsilon^2$.\n$W = \\sum_{\\ell=0}^{L} N_{\\ell} C_{\\ell}$\n$\\mathrm{MSE} = \\left(\\mathbb{E}[Q(h_{L})] - \\mathbb{E}[Q]\\right)^{2} + \\sum_{\\ell=0}^{L} \\frac{\\mathrm{Var}(\\Delta_{\\ell})}{N_{\\ell}} \\leq \\varepsilon^{2}$\n\nThe scaling assumptions are:\n1.  Bias: $|\\mathbb{E}[Q(h_{L})] - \\mathbb{E}[Q]| \\leq c_{b} h_{L}^{\\alpha}$\n2.  Level Variance: $\\mathrm{Var}(\\Delta_{\\ell}) \\asymp c_{v} h_{\\ell}^{\\beta}$ which we treat as $\\mathrm{Var}(\\Delta_{\\ell}) \\approx K_v h_{\\ell}^{\\beta}$ for some constant $K_v  0$.\n3.  Level Cost: $C_{\\ell} \\asymp c_{c} h_{\\ell}^{-\\gamma}$ which we treat as $C_{\\ell} \\approx K_c h_{\\ell}^{-\\gamma}$ for some constant $K_c  0$.\n\nAn optimal strategy to satisfy the MSE constraint is to balance the contributions from the squared bias and the total variance. A standard choice is to require each to be bounded by $\\varepsilon^2/2$:\na) $(\\text{bias})^2 \\leq \\varepsilon^2/2 \\implies |\\mathbb{E}[Q(h_{L})] - \\mathbb{E}[Q]| \\leq \\varepsilon/\\sqrt{2}$.\nb) $\\sum_{\\ell=0}^{L} \\mathrm{Var}(\\Delta_{\\ell})/N_{\\ell} \\leq \\varepsilon^2/2$.\n\nFrom constraint (a) and the bias assumption, we must choose the finest level $L$ such that:\n$c_{b} h_{L}^{\\alpha} \\leq \\varepsilon/\\sqrt{2}$\nSince $h_{L} = h_{0} m^{-L}$, this inequality determines the minimum required value for $L$. Asymptotically, for $\\varepsilon \\to 0$, this implies $h_{L} \\asymp \\varepsilon^{1/\\alpha}$. This in turn gives $h_0 m^{-L} \\asymp \\varepsilon^{1/\\alpha}$, which leads to $-L \\log m \\asymp (1/\\alpha) \\log \\varepsilon$, so $L \\asymp \\frac{1}{\\alpha \\log m} \\log(\\varepsilon^{-1})$. Thus, $L$ grows logarithmically with $\\varepsilon^{-1}$.\n\nNext, we minimize the work $W$ subject to the variance constraint (b). For minimal work, the inequality becomes an equality: $\\sum_{\\ell=0}^{L} \\mathrm{Var}(\\Delta_{\\ell})/N_{\\ell} = \\varepsilon^2/2$. Let $V_{\\ell} = \\mathrm{Var}(\\Delta_{\\ell})$. We minimize $W = \\sum_{\\ell=0}^{L} N_{\\ell} C_{\\ell}$ subject to $\\sum_{\\ell=0}^{L} V_{\\ell}/N_{\\ell} = \\varepsilon^2/2$. We use the method of Lagrange multipliers.\nThe Lagrangian is $\\mathcal{L}(\\{N_{\\ell}\\}, \\lambda) = \\sum_{\\ell=0}^{L} N_{\\ell} C_{\\ell} + \\lambda \\left(\\sum_{\\ell=0}^{L} \\frac{V_{\\ell}}{N_{\\ell}} - \\frac{\\varepsilon^2}{2}\\right)$.\nSetting the partial derivatives with respect to $N_k$ to zero:\n$\\frac{\\partial \\mathcal{L}}{\\partial N_k} = C_k - \\lambda \\frac{V_k}{N_k^2} = 0 \\implies N_k^2 = \\lambda \\frac{V_k}{C_k} \\implies N_k = \\sqrt{\\lambda} \\sqrt{V_k/C_k}$.\nThis shows that the optimal number of samples on level $\\ell$ is proportional to $\\sqrt{V_{\\ell}/C_{\\ell}}$.\n\nTo find the proportionality constant $\\sqrt{\\lambda}$, we substitute $N_{\\ell}$ back into the variance constraint:\n$\\sum_{\\ell=0}^{L} \\frac{V_{\\ell}}{\\sqrt{\\lambda} \\sqrt{V_{\\ell}/C_{\\ell}}} = \\frac{1}{\\sqrt{\\lambda}} \\sum_{\\ell=0}^{L} \\sqrt{V_{\\ell}C_{\\ell}} = \\frac{\\varepsilon^2}{2}$.\nThus, $\\sqrt{\\lambda} = \\frac{2}{\\varepsilon^2} \\sum_{k=0}^{L} \\sqrt{V_k C_k}$.\nThe optimal sample sizes are $N_{\\ell} = \\frac{2}{\\varepsilon^2} \\left(\\sum_{k=0}^{L} \\sqrt{V_k C_k}\\right) \\sqrt{\\frac{V_{\\ell}}{C_{\\ell}}}$.\n\nThe minimal total work is then:\n$W = \\sum_{\\ell=0}^{L} N_{\\ell} C_{\\ell} = \\sum_{\\ell=0}^{L} \\left[ \\frac{2}{\\varepsilon^2} \\left(\\sum_{k=0}^{L} \\sqrt{V_k C_k}\\right) \\sqrt{\\frac{V_{\\ell}}{C_{\\ell}}} \\right] C_{\\ell} = \\frac{2}{\\varepsilon^2} \\left(\\sum_{k=0}^{L} \\sqrt{V_k C_k}\\right) \\left(\\sum_{\\ell=0}^{L} \\sqrt{V_{\\ell}C_{\\ell}}\\right)$.\n$W = \\frac{2}{\\varepsilon^2} \\left(\\sum_{\\ell=0}^{L} \\sqrt{V_{\\ell}C_{\\ell}}\\right)^2$.\n\nNow we analyze the asymptotic complexity by examining the sum $\\sum_{\\ell=0}^{L} \\sqrt{V_{\\ell}C_{\\ell}}$.\nUsing the scaling assumptions, $\\sqrt{V_{\\ell}C_{\\ell}} \\asymp \\sqrt{h_{\\ell}^{\\beta} h_{\\ell}^{-\\gamma}} = h_{\\ell}^{(\\beta-\\gamma)/2}$.\nWith $h_{\\ell} = h_{0} m^{-\\ell}$, we have $h_{\\ell}^{(\\beta-\\gamma)/2} = (h_0 m^{-\\ell})^{(\\beta-\\gamma)/2} = h_0^{(\\beta-\\gamma)/2} (m^{-(\\beta-\\gamma)/2})^{\\ell}$.\nThe sum is a geometric series: $\\sum_{\\ell=0}^{L} \\sqrt{V_{\\ell}C_{\\ell}} \\asymp \\sum_{\\ell=0}^{L} (m^{-(\\beta-\\gamma)/2})^{\\ell}$.\nLet the ratio be $r = m^{-(\\beta-\\gamma)/2}$. The behavior depends on $r$.\n\nCase 1: $\\beta  \\gamma$.\nThen $\\beta - \\gamma  0$, so the exponent $-(\\beta-\\gamma)/2$ is negative. Since $m1$, the ratio $r = m^{-(\\beta-\\gamma)/2}$ satisfies $0  r  1$. As $L \\to \\infty$, the geometric series converges to a constant: $\\sum_{\\ell=0}^{\\infty} r^{\\ell} = \\frac{1}{1-r}$.\nThe sum is $\\mathcal{O}(1)$.\nThe work is $W \\asymp \\varepsilon^{-2} (\\mathcal{O}(1))^2 = \\mathcal{O}(\\varepsilon^{-2})$.\n\nCase 2: $\\beta = \\gamma$.\nThen $\\beta - \\gamma = 0$, so the ratio $r = m^0 = 1$. The sum becomes:\n$\\sum_{\\ell=0}^{L} 1 = L+1$.\nSince $L \\asymp \\log(\\varepsilon^{-1})$, the sum is $\\mathcal{O}(\\log \\varepsilon^{-1})$.\nThe work is $W \\asymp \\varepsilon^{-2} (\\mathcal{O}(\\log \\varepsilon^{-1}))^2 = \\mathcal{O}(\\varepsilon^{-2}(\\log \\varepsilon^{-1})^2)$.\n\nCase 3: $\\beta  \\gamma$.\nThen $\\beta - \\gamma  0$, so the exponent $-(\\beta-\\gamma)/2$ is positive. The ratio $r = m^{(\\gamma-\\beta)/2}$ is greater than $1$. The geometric series is dominated by its last term:\n$\\sum_{\\ell=0}^{L} r^{\\ell} = \\frac{r^{L+1}-1}{r-1} \\asymp r^L$.\nThe sum is $\\asymp (m^{(\\gamma-\\beta)/2})^L = (m^L)^{(\\gamma-\\beta)/2}$.\nFrom $h_L \\asymp \\varepsilon^{1/\\alpha}$, we have $h_0 m^{-L} \\asymp \\varepsilon^{1/\\alpha}$, so $m^L \\asymp \\varepsilon^{-1/\\alpha}$.\nThe sum is $\\asymp (\\varepsilon^{-1/\\alpha})^{(\\gamma-\\beta)/2} = \\varepsilon^{-(\\gamma-\\beta)/(2\\alpha)}$.\nThe work is $W \\asymp \\varepsilon^{-2} \\left( \\sum_{\\ell=0}^{L} \\sqrt{V_{\\ell}C_{\\ell}} \\right)^2 \\asymp \\varepsilon^{-2} \\left( \\varepsilon^{-(\\gamma-\\beta)/(2\\alpha)} \\right)^2 = \\varepsilon^{-2} \\varepsilon^{-(\\gamma-\\beta)/\\alpha} = \\varepsilon^{-2 - (\\gamma-\\beta)/\\alpha}$.\n\nSummary of derived complexity:\n- If $\\beta  \\gamma$: $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2})$.\n- If $\\beta = \\gamma$: $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2} (\\log \\varepsilon^{-1})^2)$.\n- If $\\beta  \\gamma$: $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2 - (\\gamma - \\beta)/\\alpha})$.\n\nNow we evaluate the given options.\n\nA. Choose $L(\\varepsilon)$ as the minimal integer such that $c_{b} h_{L}^{\\alpha} \\leq \\varepsilon/\\sqrt{2}$; choose the sample sizes by the optimal variance allocation $N_{\\ell} \\propto \\sqrt{\\mathrm{Var}(\\Delta_{\\ell})/C_{\\ell}}$. Then the minimal work scales as $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2})$ if $\\beta  \\gamma$, $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2} \\left(\\log \\varepsilon^{-1}\\right)^{2}\\right)$ if $\\beta = \\gamma$, and $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2 - (\\gamma - \\beta)/\\alpha}\\right)$ if $\\beta  \\gamma$.\nThis statement accurately describes the optimal choice of $L$ (by correctly balancing bias and variance error), the optimal allocation of samples $N_{\\ell}$ (derived from Lagrange multipliers), and the resulting work complexities for all three cases. These results match our derivation exactly.\nVerdict: **Correct**.\n\nB. Choose $L(\\varepsilon)$ as the minimal integer such that $c_{b} h_{L}^{\\alpha} \\leq \\varepsilon^{2}$; allocate samples as $N_{\\ell} \\propto \\mathrm{Var}(\\Delta_{\\ell})$. Then the minimal work scales as $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2})$ for all relations between $\\beta$ and $\\gamma$, with no logarithmic factor.\nThe choice for $L$ corresponds to a squared bias of $\\mathcal{O}(\\varepsilon^4)$, which is an inefficient error partition. The allocation $N_{\\ell} \\propto \\mathrm{Var}(\\Delta_{\\ell})$ is not optimal. The claimed complexity is incorrect, as it ignores the dependency on $\\beta$ and $\\gamma$.\nVerdict: **Incorrect**.\n\nC. Choose $L(\\varepsilon)$ so that $h_{L} \\asymp \\varepsilon^{1/\\alpha}$; allocate samples as $N_{\\ell} \\propto \\sqrt{\\mathrm{Var}(\\Delta_{\\ell})\\,C_{\\ell}}$. Then the minimal work scales as $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2} \\log \\varepsilon^{-1}\\right)$ when $\\beta = \\gamma$, and $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2 - (\\gamma - \\beta)/(2\\alpha)}\\right)$ when $\\beta  \\gamma$.\nThe choice for $L$ is correct. However, the sample allocation $N_{\\ell} \\propto \\sqrt{V_{\\ell}C_{\\ell}}$ is incorrect; the optimal is $N_{\\ell} \\propto \\sqrt{V_{\\ell}/C_{\\ell}}$. The asserted complexities are also incorrect: for $\\beta=\\gamma$, the logarithmic factor is squared, not to the power of $1$; for $\\beta\\gamma$, the exponent of $\\varepsilon$ has a denominator of $2\\alpha$, which is incorrect (it should be $\\alpha$).\nVerdict: **Incorrect**.\n\nD. Choose $L(\\varepsilon)$ so that $h_{L} \\asymp \\varepsilon^{1/\\alpha}$; allocate samples to equalize the per-level variance contributions, i.e., $N_{\\ell} \\propto \\mathrm{Var}(\\Delta_{\\ell})$. Then $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2})$ if $\\beta  \\gamma$, and $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-3})$ if $\\beta  \\gamma$.\nThe choice for $L$ is correct. The allocation rule \"equalize the per-level variance contributions\" means $V_{\\ell}/N_{\\ell} = \\text{constant}$, which implies $N_{\\ell} \\propto V_{\\ell}$. This is not the cost-optimal allocation. The resulting work complexities are also incorrectly stated. For $\\beta  \\gamma$, this suboptimal allocation leads to $W \\asymp \\varepsilon^{-2} \\log \\varepsilon^{-1}$, not $\\varepsilon^{-2}$. The claim for $\\beta  \\gamma$ is also not generally true.\nVerdict: **Incorrect**.\n\nE. In the borderline case $\\beta = \\gamma$, if one uses the optimal allocation $N_{\\ell} \\propto \\sqrt{\\mathrm{Var}(\\Delta_{\\ell})/C_{\\ell}}$, the minimal work scales as $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2} \\log \\varepsilon^{-1}\\right)$, where the logarithmic factor arises solely because $L(\\varepsilon)$ grows like $\\log \\varepsilon^{-1}$.\nThis option addresses the specific case $\\beta=\\gamma$ and correctly identifies the optimal allocation. However, it states the complexity as $\\mathcal{O}(\\varepsilon^{-2} \\log \\varepsilon^{-1})$, which is incorrect. As derived, the complexity is $\\mathcal{O}(\\varepsilon^{-2} (\\log \\varepsilon^{-1})^2)$ because the work is proportional to the square of a sum that grows like $L+1$.\nVerdict: **Incorrect**.\n\nThe only statement that is entirely correct is A.", "answer": "$$\\boxed{A}$$", "id": "3322273"}, {"introduction": "While theoretical analysis is essential, the ultimate goal of a simulation is to produce a credible numerical result with a quantifiable measure of uncertainty. This final practice bridges the gap from theory to application. Using a set of hypothetical simulation outputs, you will construct a confidence interval for the true expected value, a task that requires carefully combining the statistical sampling error with the systematic discretization bias to deliver a complete error estimate [@problem_id:3322294].", "problem": "Consider a Multilevel Monte Carlo (MLMC) estimator for the expectation of a real-valued functional $P$ of a stochastic process, constructed via a telescoping sum of level corrections $Y_{l} = P_{l} - P_{l-1}$, with $P_{-1} \\equiv 0$. The MLMC estimator for the truncated expectation $\\mathbb{E}[P_{L}]$ at level $L$ has the form $\\widehat{P}_{\\mathrm{ML}} = \\sum_{l=0}^{L} \\widehat{Y}_{l}$, where $\\widehat{Y}_{l}$ is the sample mean of $N_{l}$ independent and identically distributed samples of $Y_{l}$. Assume independence across levels, and that at each level $l$ the sample variance $s_{l}^{2}$ is computed from the $N_{l}$ samples.\n\nSuppose the following are given:\n- The level count $L = 3$.\n- Sample sizes $(N_{0}, N_{1}, N_{2}, N_{3}) = (1000, 800, 400, 200)$.\n- Sample variances $(s_{0}^{2}, s_{1}^{2}, s_{2}^{2}, s_{3}^{2}) = (0.80, 0.40, 0.10, 0.025)$.\n- The MLMC point estimate $\\widehat{P}_{\\mathrm{ML}} = 1.234$.\n- A weak error (bias) bound based on weak order $\\alpha = 1$ with constant $C_{b} = 0.12$ and dyadic refinement factor $2$, so that the truncation bias satisfies $|\\mathbb{E}[P] - \\mathbb{E}[P_{L}]| \\leq C_{b} \\, 2^{-\\alpha L}$.\n- A confidence level parameter $\\delta = 0.05$.\n\nUse the Central Limit Theorem (CLT) for the MLMC estimator and the variance estimator\n$$\\widehat{\\sigma}^{2} = \\sum_{l=0}^{L} \\frac{s_{l}^{2}}{N_{l}}$$\nto construct a conservative two-sided $(1-\\delta)$ confidence interval for $\\mathbb{E}[P]$ that accounts for both the sampling variance and the truncation bias. Your construction must rely on first principles: the telescoping representation, the CLT for sums of independent mean-zero terms, and the bias bound. Then, compute the numerical endpoints of the interval using the given data. Round your final interval endpoints to four significant figures. Express your final answer as the ordered pair of endpoints $\\bigl[\\text{lower}, \\text{upper}\\bigr]$.", "solution": "The problem requires the construction of a two-sided $(1-\\delta)$ confidence interval for the true expectation $\\mathbb{E}[P]$ of a quantity of interest, using a Multilevel Monte Carlo (MLMC) estimator. The total error of the MLMC point estimate $\\widehat{P}_{\\mathrm{ML}}$ with respect to the true expectation $\\mathbb{E}[P]$ is decomposed into two components: a statistical error and a truncation error (bias).\n\nThe total error is given by:\n$$\n\\mathbb{E}[P] - \\widehat{P}_{\\mathrm{ML}} = \\underbrace{(\\mathbb{E}[P] - \\mathbb{E}[P_{L}])}_{\\text{Bias}} + \\underbrace{(\\mathbb{E}[P_{L}] - \\widehat{P}_{\\mathrm{ML}})}_{\\text{Statistical Error}}\n$$\nHere, $\\mathbb{E}[P_{L}]$ is the expectation of the quantity of interest approximated at the finest level $L$.\n\nThe first term, the bias, is unknown, but we are provided with a bound on its magnitude:\n$$\n|\\mathbb{E}[P] - \\mathbb{E}[P_{L}]| \\leq B_{L}\n$$\nwhere $B_{L} = C_{b} \\, 2^{-\\alpha L}$. The parameters given are the weak order of convergence $\\alpha=1$, the constant $C_b=0.12$, and the number of levels $L=3$. We can compute this bound:\n$$\nB_{3} = C_{b} \\, 2^{-\\alpha L} = 0.12 \\times 2^{-1 \\times 3} = 0.12 \\times 2^{-3} = 0.12 \\times \\frac{1}{8} = 0.015\n$$\n\nThe second term, the statistical error, is a random variable. The MLMC estimator is defined as $\\widehat{P}_{\\mathrm{ML}} = \\sum_{l=0}^{L} \\widehat{Y}_{l}$, where $\\widehat{Y}_{l}$ is the sample mean of $N_l$ realizations of the level correction $Y_l$. The expectation of the MLMC estimator is $\\mathbb{E}[\\widehat{P}_{\\mathrm{ML}}] = \\mathbb{E}[P_L]$, meaning it is an unbiased estimator for the truncated expectation. The variance of the MLMC estimator, assuming independence across levels, is the sum of the variances of the level estimators:\n$$\n\\mathbb{V}[\\widehat{P}_{\\mathrm{ML}}] = \\sum_{l=0}^{L} \\mathbb{V}[\\widehat{Y}_{l}] = \\sum_{l=0}^{L} \\frac{\\mathbb{V}[Y_{l}]}{N_{l}} = \\sum_{l=0}^{L} \\frac{\\sigma_{l}^{2}}{N_{l}}\n$$\nWe are given an estimator for this variance based on the sample variances $s_{l}^{2}$:\n$$\n\\widehat{\\sigma}^{2} = \\sum_{l=0}^{L} \\frac{s_{l}^{2}}{N_{l}}\n$$\nUsing the provided data for $L=3$:\n- Sample sizes $(N_{0}, N_{1}, N_{2}, N_{3}) = (1000, 800, 400, 200)$\n- Sample variances $(s_{0}^{2}, s_{1}^{2}, s_{2}^{2}, s_{3}^{2}) = (0.80, 0.40, 0.10, 0.025)$\nwe compute the estimated variance:\n$$\n\\widehat{\\sigma}^{2} = \\frac{0.80}{1000} + \\frac{0.40}{800} + \\frac{0.10}{400} + \\frac{0.025}{200}\n$$\n$$\n\\widehat{\\sigma}^{2} = 0.0008 + 0.0005 + 0.00025 + 0.000125 = 0.001675\n$$\nThe estimated standard deviation is $\\widehat{\\sigma} = \\sqrt{0.001675}$.\n\nAccording to the Central Limit Theorem, the distribution of the statistical error, normalized by its standard deviation, is approximately standard normal. This allows us to construct a confidence interval for the truncated expectation $\\mathbb{E}[P_{L}]$:\n$$\n\\mathbb{P}\\left(|\\widehat{P}_{\\mathrm{ML}} - \\mathbb{E}[P_{L}]| \\leq z_{1-\\delta/2} \\widehat{\\sigma}\\right) \\approx 1-\\delta\n$$\nwhere $z_{1-\\delta/2}$ is the $(1-\\delta/2)$-quantile of the standard normal distribution. For the given confidence parameter $\\delta = 0.05$, we require a $(1-0.05) = 0.95$ confidence level, which corresponds to $z_{1-0.05/2} = z_{0.975} \\approx 1.96$.\n\nTo construct a confidence interval for the true expectation $\\mathbb{E}[P]$, we must account for both the statistical uncertainty and the bias. A conservative approach is to add the bias bound to the statistical margin of error. The total error is bounded by:\n$$\n|\\mathbb{E}[P] - \\widehat{P}_{\\mathrm{ML}}| \\leq |\\mathbb{E}[P] - \\mathbb{E}[P_L]| + |\\mathbb{E}[P_L] - \\widehat{P}_{\\mathrm{ML}}|\n$$\nThis leads to a confidence interval for $\\mathbb{E}[P]$ of the form $\\widehat{P}_{\\mathrm{ML}} \\pm E$, where the total margin of error $E$ is the sum of the statistical half-width and the bias bound:\n$$\nE = z_{1-\\delta/2} \\widehat{\\sigma} + B_{3}\n$$\nWe proceed to calculate $E$:\n$$\nE = 1.96 \\times \\sqrt{0.001675} + 0.015\n$$\n$$\nE \\approx 1.96 \\times 0.04092676 + 0.015 \\approx 0.08021645 + 0.015 = 0.09521645\n$$\nThe confidence interval for $\\mathbb{E}[P]$ is given by $[\\text{lower}, \\text{upper}]$, where:\n- Lower endpoint: $\\widehat{P}_{\\mathrm{ML}} - E = 1.234 - 0.09521645 = 1.13878355$\n- Upper endpoint: $\\widehat{P}_{\\mathrm{ML}} + E = 1.234 + 0.09521645 = 1.32921645$\n\nThe problem requires rounding the final interval endpoints to four significant figures.\n- Lower endpoint: $1.13878355 \\approx 1.139$\n- Upper endpoint: $1.32921645 \\approx 1.329$\n\nThe resulting confidence interval is $[1.139, 1.329]$.", "answer": "$$\n\\boxed{[1.139, 1.329]}\n$$", "id": "3322294"}]}