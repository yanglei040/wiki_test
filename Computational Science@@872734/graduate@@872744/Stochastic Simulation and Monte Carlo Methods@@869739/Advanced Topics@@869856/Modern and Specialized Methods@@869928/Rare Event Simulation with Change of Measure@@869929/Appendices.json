{"hands_on_practices": [{"introduction": "This first exercise serves as a foundational walkthrough of the change of measure technique using exponential tilting. We consider the canonical problem of estimating the probability that the sum of independent and identically distributed Gaussian random variables exceeds a high threshold. By working through this case, where calculations can be performed in clean, closed form, you will derive the three key ingredients of an efficient importance sampling estimator: the optimal tilting parameter guided by large deviation theory, the resulting tilted distribution, and the corresponding likelihood ratio. [@problem_id:3335110]", "problem": "Consider independent and identically distributed random variables $X_1,\\dots,X_n$ with common distribution $\\mathcal{N}(m,\\sigma^2)$, where $m \\in \\mathbb{R}$ and $\\sigma^2 \\in (0,\\infty)$. Let $S_n \\doteq \\sum_{i=1}^{n} X_i$ and fix a constant $a \\in \\mathbb{R}$ with $am$. The rare-event probability of interest is $p_n \\doteq \\mathbb{P}(S_n \\ge a n)$. For importance sampling of $p_n$, consider exponential tilting (Esscher transform) based on the cumulant generating function $K(t) \\doteq \\ln \\mathbb{E}[\\exp(t X_1)]$, which induces, for each $t$ in the effective domain of $K$, a tilted law $\\mathbb{P}_t$ under which the one-step density is modified by $f_t(x) \\propto \\exp(t x) f(x)$, and the pathwise likelihood ratio (Radon–Nikodym derivative) for a trajectory $(X_1,\\dots,X_n)$ is $L_n(t) \\doteq \\frac{\\mathrm{d}\\mathbb{P}}{\\mathrm{d}\\mathbb{P}_t}(X_1,\\dots,X_n)$. The large-deviation motivated choice $t=\\theta^\\star$ is the unique solution to $K'(\\theta^\\star)=a$, which enforces that the tilted mean equals the rare-event threshold level.\n\nStarting from these definitions and without assuming any Gaussian-specific shortcut beyond standard properties of the moment generating function, explicitly compute:\n- the optimal tilting parameter $\\theta^\\star$,\n- the tilted distribution of $X_i$ under $\\mathbb{P}_{\\theta^\\star}$,\n- and the corresponding likelihood ratio $L_n^\\star \\doteq L_n(\\theta^\\star)$ to be used with the indicator $\\mathbf{1}\\{S_n \\ge a n\\}$.\n\nExpress all results symbolically in terms of $m$, $\\sigma^2$, $a$, $n$, and $S_n$. Provide your final answer as a row matrix containing, in order, $\\theta^\\star$, the tilted one-step law, and $L_n^\\star$. No numerical approximation is required.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It is a standard problem in the field of rare event simulation and large deviations theory. We proceed to solve it by computing the requested quantities in three steps.\n\nThe random variables $X_i$ are independent and identically distributed (i.i.d.) following a normal distribution $X_i \\sim \\mathcal{N}(m, \\sigma^2)$. The probability density function (PDF) of $X_i$ is given by\n$$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-m)^2}{2\\sigma^2}\\right)$$\nThe problem requires us to find the optimal tilting parameter $\\theta^\\star$, the tilted distribution under this parameter, and the corresponding likelihood ratio $L_n^\\star$.\n\n### 1. Computation of the Optimal Tilting Parameter $\\theta^\\star$\n\nThe optimal tilting parameter $\\theta^\\star$ is defined as the unique solution to the equation $K'(\\theta^\\star) = a$, where $K(t)$ is the cumulant generating function (CGF) of $X_1$.\n\nThe CGF is the natural logarithm of the moment generating function (MGF), $K(t) = \\ln \\mathbb{E}[\\exp(t X_1)]$. First, we compute the MGF of $X_1 \\sim \\mathcal{N}(m, \\sigma^2)$.\n$$\n\\mathbb{E}[\\exp(t X_1)] = \\int_{-\\infty}^{\\infty} \\exp(tx) \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-m)^2}{2\\sigma^2}\\right) \\mathrm{d}x\n$$\nThe argument of the exponent is\n$$\ntx - \\frac{(x-m)^2}{2\\sigma^2} = tx - \\frac{x^2 - 2mx + m^2}{2\\sigma^2} = -\\frac{1}{2\\sigma^2} [x^2 - 2mx - 2\\sigma^2tx + m^2]\n$$\nWe complete the square for the terms involving $x$:\n$$\nx^2 - 2(m + \\sigma^2 t)x + m^2 = [x - (m + \\sigma^2 t)]^2 - (m + \\sigma^2 t)^2 + m^2\n$$\n$$\n= [x - (m + \\sigma^2 t)]^2 - (m^2 + 2m\\sigma^2 t + \\sigma^4 t^2) + m^2 = [x - (m + \\sigma^2 t)]^2 - 2m\\sigma^2 t - \\sigma^4 t^2\n$$\nSubstituting this back into the exponent argument:\n$$\n-\\frac{1}{2\\sigma^2} \\left( [x - (m + \\sigma^2 t)]^2 - 2m\\sigma^2 t - \\sigma^4 t^2 \\right) = -\\frac{[x - (m + \\sigma^2 t)]^2}{2\\sigma^2} + mt + \\frac{1}{2}\\sigma^2 t^2\n$$\nThe MGF becomes:\n$$\n\\mathbb{E}[\\exp(t X_1)] = \\exp\\left(mt + \\frac{1}{2}\\sigma^2 t^2\\right) \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{[x - (m + \\sigma^2 t)]^2}{2\\sigma^2}\\right) \\mathrm{d}x\n$$\nThe integral is over the PDF of a normal distribution with mean $m + \\sigma^2 t$ and variance $\\sigma^2$, so it integrates to $1$. Thus, the MGF is:\n$$\nM_{X_1}(t) = \\exp\\left(mt + \\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nThe CGF is the natural logarithm of the MGF:\n$$\nK(t) = \\ln(M_{X_1}(t)) = mt + \\frac{1}{2}\\sigma^2 t^2\n$$\nThe effective domain for $t$ is $\\mathbb{R}$. Now, we find its derivative with respect to $t$:\n$$\nK'(t) = \\frac{\\mathrm{d}}{\\mathrm{d}t} \\left(mt + \\frac{1}{2}\\sigma^2 t^2\\right) = m + \\sigma^2 t\n$$\nThe optimal parameter $\\theta^\\star$ solves $K'(\\theta^\\star) = a$:\n$$\nm + \\sigma^2 \\theta^\\star = a\n$$\nSolving for $\\theta^\\star$, we obtain:\n$$\n\\theta^\\star = \\frac{a - m}{\\sigma^2}\n$$\nGiven that $a  m$ and $\\sigma^2  0$, we have $\\theta^\\star  0$.\n\n### 2. Computation of the Tilted Distribution\n\nThe tilted density $f_t(x)$ is defined by $f_t(x) \\propto \\exp(tx)f(x)$. The normalized density is:\n$$\nf_t(x) = \\frac{\\exp(tx)f(x)}{\\mathbb{E}[\\exp(tX_1)]} = \\exp(tx - K(t)) f(x)\n$$\nWe substitute the expressions for $f(x)$ and $K(t)$ at $t=\\theta^\\star$:\n$$\nf_{\\theta^\\star}(x) = \\exp\\left(\\theta^\\star x - K(\\theta^\\star)\\right) \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-m)^2}{2\\sigma^2}\\right)\n$$\n$$\nf_{\\theta^\\star}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(\\theta^\\star x - K(\\theta^\\star) - \\frac{(x-m)^2}{2\\sigma^2}\\right)\n$$\nLet's analyze the argument of the exponent. We use the expressions for $\\theta^\\star$ and $K(\\theta^\\star)$ found previously.\nThe exponent's argument is:\n$$\n\\theta^\\star x - (m\\theta^\\star + \\frac{1}{2}\\sigma^2(\\theta^\\star)^2) - \\frac{x^2 - 2mx + m^2}{2\\sigma^2}\n$$\n$$\n= -\\frac{x^2}{2\\sigma^2} + \\left(\\frac{m}{\\sigma^2} + \\theta^\\star\\right)x - \\left(\\frac{m^2}{2\\sigma^2} + m\\theta^\\star + \\frac{1}{2}\\sigma^2(\\theta^\\star)^2\\right)\n$$\nWe identify the new mean $m_{\\theta^\\star}$ from the term linear in $x$. For a normal density with variance $\\sigma^2$, the exponent is of the form $-\\frac{(x-m_{\\theta^\\star})^2}{2\\sigma^2} = -\\frac{x^2}{2\\sigma^2} + \\frac{m_{\\theta^\\star}}{\\sigma^2}x - \\frac{m_{\\theta^\\star}^2}{2\\sigma^2}$.\nComparing coefficients of $x$, we have:\n$$\n\\frac{m_{\\theta^\\star}}{\\sigma^2} = \\frac{m}{\\sigma^2} + \\theta^\\star \\implies m_{\\theta^\\star} = m + \\sigma^2 \\theta^\\star\n$$\nSubstituting $\\theta^\\star = \\frac{a-m}{\\sigma^2}$:\n$$\nm_{\\theta^\\star} = m + \\sigma^2 \\left(\\frac{a-m}{\\sigma^2}\\right) = m + a - m = a\n$$\nThis is consistent with the property that the mean of the tilted distribution is $K'(\\theta^\\star)=a$. The variance term, determined by the coefficient of $x^2$, remains $\\sigma^2$. The resulting tilted density is:\n$$\nf_{\\theta^\\star}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-a)^2}{2\\sigma^2}\\right)\n$$\nThis is the PDF of a normal distribution with mean $a$ and variance $\\sigma^2$. Therefore, the tilted one-step law is $\\mathcal{N}(a, \\sigma^2)$.\n\n### 3. Computation of the Likelihood Ratio $L_n^\\star$\n\nThe pathwise likelihood ratio is the Radon-Nikodym derivative $L_n(t) = \\frac{\\mathrm{d}\\mathbb{P}}{\\mathrm{d}\\mathbb{P}_t}$. Since the variables $X_1, \\dots, X_n$ are i.i.d. under both measures, the ratio of joint densities is the product of the marginal ratios:\n$$\nL_n(t) = \\frac{\\prod_{i=1}^n f(X_i)}{\\prod_{i=1}^n f_t(X_i)} = \\prod_{i=1}^n \\frac{f(X_i)}{f_t(X_i)}\n$$\nFrom the definition of $f_t(x)$, we have $\\frac{f(x)}{f_t(x)} = \\exp(-tx + K(t))$. Therefore:\n$$\nL_n(t) = \\prod_{i=1}^n \\exp(-tX_i + K(t)) = \\exp\\left(\\sum_{i=1}^n (-tX_i + K(t))\\right)\n$$\n$$\nL_n(t) = \\exp\\left(-t \\sum_{i=1}^n X_i + n K(t)\\right) = \\exp(-t S_n + nK(t))\n$$\nWe need to evaluate this at $t=\\theta^\\star$. Let $L_n^\\star = L_n(\\theta^\\star)$:\n$$\nL_n^\\star = \\exp(-\\theta^\\star S_n + nK(\\theta^\\star))\n$$\nWe substitute $\\theta^\\star = \\frac{a-m}{\\sigma^2}$ and compute $K(\\theta^\\star)$:\n$$\nK(\\theta^\\star) = m\\theta^\\star + \\frac{1}{2}\\sigma^2 (\\theta^\\star)^2 = m\\left(\\frac{a-m}{\\sigma^2}\\right) + \\frac{1}{2}\\sigma^2 \\left(\\frac{a-m}{\\sigma^2}\\right)^2\n$$\n$$\nK(\\theta^\\star) = \\frac{m(a-m)}{\\sigma^2} + \\frac{(a-m)^2}{2\\sigma^2} = \\frac{2m(a-m) + (a-m)^2}{2\\sigma^2} = \\frac{(a-m)(2m + a - m)}{2\\sigma^2}\n$$\n$$\nK(\\theta^\\star) = \\frac{(a-m)(a+m)}{2\\sigma^2} = \\frac{a^2 - m^2}{2\\sigma^2}\n$$\nFinally, we substitute $\\theta^\\star$ and $K(\\theta^\\star)$ into the expression for $L_n^\\star$:\n$$\nL_n^\\star = \\exp\\left( - \\left(\\frac{a-m}{\\sigma^2}\\right) S_n + n \\left(\\frac{a^2-m^2}{2\\sigma^2}\\right) \\right)\n$$\nThis expression depends on the specified variables $m, \\sigma^2, a, n$, and the sample sum $S_n$.\n\nThe three requested quantities are:\n1.  $\\theta^\\star = \\frac{a-m}{\\sigma^2}$\n2.  The tilted one-step law is $\\mathcal{N}(a, \\sigma^2)$\n3.  $L_n^\\star = \\exp\\left( - \\frac{a-m}{\\sigma^2} S_n + n\\frac{a^2-m^2}{2\\sigma^2} \\right)$\nThese results are collected into a single row matrix for the final answer.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{a-m}{\\sigma^2}  \\mathcal{N}(a, \\sigma^2)  \\exp\\left( - \\frac{a-m}{\\sigma^2} S_n + n\\frac{a^2-m^2}{2\\sigma^2} \\right) \\end{pmatrix}}\n$$", "id": "3335110"}, {"introduction": "Real-world systems often involve multiple random components, demanding a generalization of our methods to higher dimensions. This practice tackles the rare event probability for a quadratic form of a multivariate standard normal vector, a problem relevant to fields from signal processing to structural reliability. You will use large deviation theory to find the \"most likely\" way the rare event happens, identifying a dominating point that guides the design of two powerful importance sampling schemes: exponential twisting and a direct mean-shift. [@problem_id:3335057]", "problem": "Let $X \\in \\mathbb{R}^{d}$ be a random vector distributed as a $d$-dimensional standard multivariate normal, $X \\sim \\mathcal{N}(0, I_{d})$, and let $A \\in \\mathbb{R}^{d \\times d}$ be a symmetric positive semidefinite matrix. Denote by $\\lambda_{1} \\ge \\lambda_{2} \\ge \\cdots \\ge \\lambda_{d} \\ge 0$ the eigenvalues of $A$ and by $v_{1}, \\dots, v_{d}$ an associated orthonormal basis of eigenvectors, with $(\\lambda_{1}, v_{1})$ the top eigenpair. Consider the rare-event probability\n$$\np \\;=\\; \\mathbb{P}\\!\\left( X^{\\top} A X \\,\\ge\\, c \\right),\n$$\nfor a threshold $c$ satisfying $c  \\operatorname{tr}(A)$ so that the event is indeed rare under the nominal measure. Starting from first principles, namely the definitions of the standard multivariate normal density, change of measure via the Esscher transform on a statistic, and the calculus of variations, perform the following derivations:\n\n1. Using a constrained optimization argument consistent with the Large Deviations Principle (LDP) for Gaussian measures, determine the dominating point $x^{\\star}$ that minimizes the Gaussian rate function $\\frac{1}{2}\\|x\\|^{2}$ subject to the constraint $x^{\\top} A x \\ge c$. Express $x^{\\star}$ in terms of $c$ and the top eigenpair $(\\lambda_{1}, v_{1})$ of $A$.\n\n2. Construct an exponential twisting scheme (Esscher transform) for the quadratic form $S = X^{\\top} A X$ with a positive tilting parameter $\\theta$ such that the twisted sampling distribution is absolutely continuous with respect to the nominal one. Derive the twisted sampling distribution in closed form, and write down the corresponding likelihood ratio (weight) $w_{\\theta}(x)$ that yields an unbiased Importance Sampling (IS) estimator for $p$ under the twisted measure. Define all quantities explicitly.\n\n3. Determine the tilting parameter $\\theta^{\\star}$ by imposing the condition that the expectation of $S$ under the twisted distribution equals the threshold $c$. Provide the resulting analytical characterization of $\\theta^{\\star}$ in terms of the spectrum $\\{\\lambda_{i}\\}_{i=1}^{d}$ of $A$, and justify existence and uniqueness of $\\theta^{\\star}$ within the domain where the twisted distribution is well-defined.\n\n4. As an alternative scheme guided by the dominating point, design a mean-shift Importance Sampling strategy by sampling from $\\mathcal{N}(\\mu, I_{d})$ with a deterministic mean vector $\\mu$ aligned with $v_{1}$. Derive the corresponding likelihood ratio $w_{\\mu}(x)$, and specify the choice of $\\mu$ that is consistent with the dominating point in part 1.\n\nYour final answer must be a single closed-form analytic expression or a single row matrix of closed-form analytic expressions containing: the dominating point $x^{\\star}$, the characterization of the optimal tilting parameter $\\theta^{\\star}$, the exponential-twist weight $w_{\\theta}(x)$, and the mean-shift weight $w_{\\mu}(x)$ with the choice of $\\mu$ aligned with $v_{1}$. No numerical evaluation is required.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of probability theory, large deviations, and stochastic simulation. It is well-posed, objective, and contains sufficient information for a unique solution to be derived for each part. We proceed with the solution.\n\nThe analysis hinges on the spectral decomposition of the symmetric matrix $A$, which can be written as $A = V \\Lambda V^{\\top}$, where $V$ is an orthogonal matrix whose columns are the eigenvectors $v_1, \\dots, v_d$, and $\\Lambda = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_d)$ is the diagonal matrix of corresponding eigenvalues.\n\n**1. Derivation of the Dominating Point $x^{\\star}$**\n\nAccording to the Large Deviations Principle for Gaussian measures, the probability of a rare event is dominated by the 'most likely' point(s) in the event set. For a standard Gaussian vector $X \\in \\mathbb{R}^d$, the likelihood is determined by the rate function $I(x) = \\frac{1}{2}\\|x\\|^2$. The dominating point $x^{\\star}$ is the point that minimizes this rate function subject to the constraint that defines the rare event.\n\nThe optimization problem is:\n$$\n\\text{minimize} \\quad \\frac{1}{2} x^{\\top}x \\quad \\text{subject to} \\quad x^{\\top}Ax \\ge c.\n$$\nSince the objective function $\\frac{1}{2}x^{\\top}x$ is strictly increasing with the norm of $x$, and the feasible set is pushed away from the origin (as $c  \\operatorname{tr}(A) \\ge 0$), the minimum will occur on the boundary of the feasible set, i.e., where $x^{\\top}Ax = c$.\n\nWe formulate the Lagrangian for this constrained optimization problem:\n$$\n\\mathcal{L}(x, \\mu) = \\frac{1}{2}x^{\\top}x - \\mu \\left(x^{\\top}Ax - c\\right)\n$$\nwhere $\\mu \\ge 0$ is the Lagrange multiplier. Taking the gradient with respect to $x$ and setting it to zero gives the first-order condition:\n$$\n\\nabla_x \\mathcal{L}(x, \\mu) = x - 2\\mu A x = 0 \\implies (I - 2\\mu A)x = 0.\n$$\nThis equation implies that for a non-zero solution $x$, it must be an eigenvector of the matrix $A$. Let $x$ be an eigenvector of $A$ with corresponding eigenvalue $\\lambda_i$. Then:\n$$\nAx = \\lambda_i x\n$$\nSubstituting this into the condition gives $x - 2\\mu \\lambda_i x = 0$, which for $x \\neq 0$ implies $1 - 2\\mu \\lambda_i = 0$. This requires $\\lambda_i  0$ (if $\\lambda_i=0$, then $1=0$, a contradiction) and gives $\\mu = \\frac{1}{2\\lambda_i}$. Since $\\mu0$, this is a valid Lagrange multiplier.\n\nThe solution $x$ must be an eigenvector of $A$. Let $x = k v_i$ for some scalar $k$ and unit eigenvector $v_i$. From the boundary constraint $x^{\\top}Ax=c$:\n$$\n(k v_i)^{\\top} A (k v_i) = k^2 v_i^{\\top} A v_i = k^2 \\lambda_i = c \\implies k^2 = \\frac{c}{\\lambda_i}.\n$$\nThe value of the objective function is $\\frac{1}{2}\\|x\\|^2 = \\frac{1}{2} \\|k v_i\\|^2 = \\frac{1}{2} k^2 \\|v_i\\|^2 = \\frac{1}{2} k^2 = \\frac{c}{2\\lambda_i}$.\nTo minimize this value, we must maximize the eigenvalue $\\lambda_i$. The largest eigenvalue of $A$ is $\\lambda_1$. This implies that the minimizing eigenvector must correspond to $\\lambda_1$. We note that for this to be well-defined, we must have $\\lambda_1  0$. If $\\lambda_1=0$, then $A=0$, $\\operatorname{tr}(A)=0$. The condition $c\\operatorname{tr}(A)$ implies $c0$. The constraint $x^\\top A x \\ge c$ becomes $0 \\ge c$, a contradiction. Thus, the feasible set is empty, and we must assume $\\lambda_10$.\n\nThe squared norm of the dominating point is $\\|x^{\\star}\\|^2 = c/\\lambda_1$. A possible solution is $x^{\\star} = \\sqrt{\\frac{c}{\\lambda_1}} v_1$. There is a sign ambiguity, $x^{\\star} = \\pm\\sqrt{\\frac{c}{\\lambda_1}} v_1$, but both points give the same minimum value for the rate function. Conventionally, we select one, for instance with the positive sign.\n$$\nx^{\\star} = \\sqrt{\\frac{c}{\\lambda_1}} v_1.\n$$\n\n**2. Exponential Twisting and Likelihood Ratio**\n\nThe nominal density of $X \\sim \\mathcal{N}(0, I_d)$ is $f(x) = (2\\pi)^{-d/2} \\exp\\left(-\\frac{1}{2}x^{\\top}x\\right)$. The Esscher transform for the statistic $S = X^{\\top}AX$ with parameter $\\theta0$ defines a new probability density $f_\\theta(x)$ as:\n$$\nf_{\\theta}(x) = \\frac{f(x)\\exp(\\theta S(x))}{\\mathbb{E}[\\exp(\\theta S(x))]} = \\frac{f(x)\\exp(\\theta x^{\\top}Ax)}{M_S(\\theta)},\n$$\nwhere $M_S(\\theta)$ is the moment-generating function (MGF) of $S$. The numerator is proportional to:\n$$\n\\exp\\left(-\\frac{1}{2}x^{\\top}x + \\theta x^{\\top}Ax\\right) = \\exp\\left(-\\frac{1}{2}x^{\\top}(I - 2\\theta A)x\\right).\n$$\nThis expression corresponds to the kernel of a Gaussian density provided the matrix $I - 2\\theta A$ is symmetric positive definite. Since $A$ is symmetric, $I-2\\theta A$ is symmetric. For positive definiteness, all its eigenvalues must be positive. The eigenvalues of $I-2\\theta A$ are $1-2\\theta\\lambda_i$. We require $1-2\\theta\\lambda_i  0$ for all $i=1, \\dots, d$. Given $\\theta0$, this is equivalent to $\\theta  \\frac{1}{2\\lambda_i}$ for all $\\lambda_i  0$. This must hold for the largest eigenvalue $\\lambda_1$, so the domain for $\\theta$ is $0  \\theta  \\frac{1}{2\\lambda_1}$.\n\nThe MGF $M_S(\\theta)$ is the integral of the unnormalized density:\n$$\nM_S(\\theta) = \\int_{\\mathbb{R}^d} (2\\pi)^{-d/2} \\exp\\left(-\\frac{1}{2}x^{\\top}(I - 2\\theta A)x\\right) dx.\n$$\nThis integral is recognized as the normalization constant of a $\\mathcal{N}(0, (I-2\\theta A)^{-1})$ distribution, which is $\\det(I-2\\theta A)^{-1/2}$.\nThe determinant is the product of the eigenvalues:\n$$\n\\det(I - 2\\theta A) = \\prod_{i=1}^d (1 - 2\\theta \\lambda_i).\n$$\nSo, $M_S(\\theta) = \\left(\\prod_{i=1}^d (1 - 2\\theta\\lambda_i)\\right)^{-1/2}$.\n\nThe twisted sampling distribution is therefore a zero-mean Gaussian with covariance matrix $\\Sigma_\\theta = (I - 2\\theta A)^{-1}$. Its density is:\n$$\nf_\\theta(x) = (2\\pi)^{-d/2} \\det(I-2\\theta A)^{1/2} \\exp\\left(-\\frac{1}{2}x^{\\top}(I-2\\theta A)x\\right).\n$$\nThe Importance Sampling weight (likelihood ratio) is $w_{\\theta}(x) = f(x)/f_{\\theta}(x)$.\n$$\nw_{\\theta}(x) = \\frac{f(x)}{f(x)\\exp(\\theta x^{\\top}Ax) / M_S(\\theta)} = M_S(\\theta)\\exp(-\\theta x^{\\top}Ax).\n$$\nSubstituting the expression for $M_S(\\theta)$, we get:\n$$\nw_{\\theta}(x) = \\exp(-\\theta x^{\\top}A x) \\left(\\prod_{i=1}^{d} (1 - 2\\theta\\lambda_i)\\right)^{-\\frac{1}{2}}.\n$$\n\n**3. Optimal Tilting Parameter $\\theta^{\\star}$**\n\nThe optimal tilting parameter $\\theta^{\\star}$ in this context is chosen to satisfy the condition $\\mathbb{E}_\\theta[S] = c$, which means the twisted distribution is centered, in a sense, on the rare event boundary. The expectation $\\mathbb{E}_\\theta[S]$ can be computed from the cumulant generating function (CGF), $K_S(\\theta) = \\ln M_S(\\theta)$.\n$$\nK_S(\\theta) = \\ln\\left[ \\left(\\prod_{i=1}^d (1 - 2\\theta\\lambda_i)\\right)^{-1/2} \\right] = -\\frac{1}{2} \\sum_{i=1}^d \\ln(1-2\\theta\\lambda_i).\n$$\nThe expectation is given by the derivative of the CGF:\n$$\n\\mathbb{E}_{\\theta}[S] = \\frac{d}{d\\theta} K_S(\\theta) = -\\frac{1}{2} \\sum_{i=1}^d \\frac{-2\\lambda_i}{1 - 2\\theta\\lambda_i} = \\sum_{i=1}^d \\frac{\\lambda_i}{1 - 2\\theta\\lambda_i}.\n$$\nWe need to find $\\theta^{\\star}  0$ that solves the equation:\n$$\n\\sum_{i=1}^d \\frac{\\lambda_i}{1-2\\theta^{\\star}\\lambda_i} = c.\n$$\nLet $g(\\theta) = \\sum_{i=1}^d \\frac{\\lambda_i}{1-2\\theta\\lambda_i}$. The domain for $\\theta$ is $(0, 1/(2\\lambda_1))$.\nAt $\\theta=0$, $g(0) = \\sum_{i=1}^d \\lambda_i = \\operatorname{tr}(A)$.\nThe derivative is $g'(\\theta) = \\sum_{i=1}^d \\frac{2\\lambda_i^2}{(1-2\\theta\\lambda_i)^2} \\ge 0$. Since we assumed $\\lambda_1  0$, $g'(\\theta)0$, so $g(\\theta)$ is strictly increasing on its domain.\nAs $\\theta \\to (1/(2\\lambda_1))^-$, the term with $\\lambda_1$ dominates and $g(\\theta) \\to \\infty$.\nSince the problem states $c  \\operatorname{tr}(A)$, we have $g(0)  c$ and $\\lim_{\\theta \\to (1/(2\\lambda_1))^-} g(\\theta) = \\infty$. By the Intermediate Value Theorem, there exists a unique solution $\\theta^{\\star} \\in (0, 1/(2\\lambda_1))$ to the equation $g(\\theta) = c$. An analytical closed-form expression for $\\theta^{\\star}$ is generally not possible. The equation itself is the analytical characterization of $\\theta^{\\star}$.\n\n**4. Mean-Shift Importance Sampling**\n\nIn this scheme, we sample from a distribution $X_\\mu \\sim \\mathcal{N}(\\mu, I_d)$, where the covariance is unchanged but the mean is shifted. The density is $f_{\\mu}(x) = (2\\pi)^{-d/2}\\exp\\left(-\\frac{1}{2}(x-\\mu)^{\\top}(x-\\mu)\\right)$.\nThe likelihood ratio is $w_{\\mu}(x) = f(x) / f_{\\mu}(x)$:\n$$\nw_{\\mu}(x) = \\frac{(2\\pi)^{-d/2}\\exp\\left(-\\frac{1}{2}x^{\\top}x\\right)}{(2\\pi)^{-d/2}\\exp\\left(-\\frac{1}{2}(x-\\mu)^{\\top}(x-\\mu)\\right)} = \\exp\\left(-\\frac{1}{2}x^{\\top}x + \\frac{1}{2}(x^{\\top}x - 2\\mu^{\\top}x + \\mu^{\\top}\\mu)\\right)\n$$\n$$\nw_{\\mu}(x) = \\exp\\left(-\\mu^{\\top}x + \\frac{1}{2}\\mu^{\\top}\\mu\\right).\n$$\nThe shift vector $\\mu$ is chosen to be aligned with $v_1$, so $\\mu = \\alpha v_1$. A natural choice consistent with the dominating point from Part 1 is to shift the mean of the sampling distribution to that point. Thus, we set $\\mu = x^{\\star}$.\n$$\n\\mu = x^{\\star} = \\sqrt{\\frac{c}{\\lambda_1}} v_1.\n$$\nWith this choice, we compute the terms in the weight expression:\n$$\n\\mu^{\\top}\\mu = \\left(\\sqrt{\\frac{c}{\\lambda_1}} v_1\\right)^{\\top} \\left(\\sqrt{\\frac{c}{\\lambda_1}} v_1\\right) = \\frac{c}{\\lambda_1} v_1^{\\top}v_1 = \\frac{c}{\\lambda_1},\n$$\nsince $v_1$ is a unit vector. The term $\\mu^{\\top}x$ is:\n$$\n\\mu^{\\top}x = \\left(\\sqrt{\\frac{c}{\\lambda_1}} v_1\\right)^{\\top} x = \\sqrt{\\frac{c}{\\lambda_1}} v_1^{\\top}x.\n$$\nSubstituting these into the expression for $w_{\\mu}(x)$ gives:\n$$\nw_{\\mu}(x) = \\exp\\left(-\\sqrt{\\frac{c}{\\lambda_1}} v_1^{\\top}x + \\frac{c}{2\\lambda_1}\\right).\n$$\nThis completes the required derivations.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{\\frac{c}{\\lambda_1}} v_1  \\sum_{i=1}^{d} \\frac{\\lambda_i}{1-2\\theta^{\\star}\\lambda_i} = c  \\exp(-\\theta x^{\\top}A x) \\left(\\prod_{i=1}^{d} (1 - 2\\theta\\lambda_i)\\right)^{-\\frac{1}{2}}  \\exp\\left(-\\sqrt{\\frac{c}{\\lambda_1}} v_1^{\\top}x + \\frac{c}{2\\lambda_1}\\right) \\end{pmatrix}}\n$$", "id": "3335057"}, {"introduction": "After mastering the design of a change of measure, a practical question arises: how do we choose the best sampling distribution when several candidates exist? A tempting approach is to use pilot runs to select the best one and then reuse those samples for the final estimate. This exercise exposes a subtle but critical pitfall in such adaptive procedures, demonstrating how reusing data for both parameter selection and estimation can introduce a systematic bias. By calculating this bias explicitly in a simple Bernoulli setting, you will gain a deeper appreciation for the rigorous design of valid adaptive simulation algorithms. [@problem_id:3335116]", "problem": "Consider estimating a rare-event probability under a change of measure. Let $X$ be a Bernoulli random variable under the nominal measure $\\mathbb{P}_{p}$ with $\\mathbb{P}_{p}(X=1)=p$, where $p \\in (0,1)$ is small. For importance sampling (IS), consider proposal measures $\\mathbb{Q}_{q}$ under which $X$ is Bernoulli with parameter $q \\in (0,1)$. The Radon–Nikodym derivative is $L_{q}(X)=\\frac{\\mathrm{d}\\mathbb{P}_{p}}{\\mathrm{d}\\mathbb{Q}_{q}}(X)$, so that $L_{q}(1)=\\frac{p}{q}$ and $L_{q}(0)=\\frac{1-p}{1-q}$. The ordinary IS estimator of $p$ using $n$ independent and identically distributed samples $X_{1},\\dots,X_{n}$ from $\\mathbb{Q}_{q}$ is\n$$\n\\widehat{p}(q)=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{X_{i}=1\\}\\,\\frac{p}{q},\n$$\nwhich is unbiased for $p$ when $q$ is fixed and independent of the data used in the estimator.\n\nNow construct the following $2$-stage adaptive scheme with two candidate proposals $q_{1},q_{2}\\in (0,1)$ and an integer threshold $\\tau\\in\\{1,2,\\dots,n\\}$.\n\nStage $1$ (pilot, change-of-measure selection):\n- Generate $n$ independent and identically distributed samples $X^{(1)}_{1},\\dots,X^{(1)}_{n}$ from $\\mathbb{Q}_{q_{1}}$ and independently generate $n$ independent and identically distributed samples $X^{(2)}_{1},\\dots,X^{(2)}_{n}$ from $\\mathbb{Q}_{q_{2}}$.\n- Let $S=\\sum_{i=1}^{n}X^{(1)}_{i}$. If $S\\ge \\tau$, select proposal $q=q_{1}$; otherwise, select proposal $q=q_{2}$.\n\nStage $2$ (estimation with data reuse):\n- Produce the adaptive estimator\n$$\n\\widehat{p}_{\\mathrm{adapt}}=\\mathbf{1}\\{S\\ge \\tau\\}\\,\\widehat{p}(q_{1})+\\mathbf{1}\\{S\\tau\\}\\,\\widehat{p}(q_{2}),\n$$\nwhere $\\widehat{p}(q_{1})$ is computed from $X^{(1)}_{1},\\dots,X^{(1)}_{n}$ and $\\widehat{p}(q_{2})$ is computed from $X^{(2)}_{1},\\dots,X^{(2)}_{n}$, each using the ordinary importance sampling formula above.\n\nBecause the selection event $\\{S\\ge \\tau\\}$ depends on the same data used to compute $\\widehat{p}(q_{1})$, the estimator $\\widehat{p}_{\\mathrm{adapt}}$ is biased in general.\n\nDerive a closed-form expression for the bias $\\mathbb{E}[\\widehat{p}_{\\mathrm{adapt}}]-p$ in terms of $p$, $q_{1}$, $n$, and $\\tau$, expressing your answer in a single analytical expression. Your final answer must be a single closed-form expression. Do not report an inequality or an equation. No rounding is required.", "solution": "The problem asks for the bias of the adaptive importance sampling estimator $\\widehat{p}_{\\mathrm{adapt}}$, which is defined as $\\mathbb{E}[\\widehat{p}_{\\mathrm{adapt}}] - p$. The expectation is taken with respect to the sampling distributions. The samples $X^{(1)}_{1},\\dots,X^{(1)}_{n}$ are drawn independently from a Bernoulli distribution with parameter $q_{1}$ (denoted by the measure $\\mathbb{Q}_{q_{1}}$), and the samples $X^{(2)}_{1},\\dots,X^{(2)}_{n}$ are drawn independently from a Bernoulli distribution with parameter $q_{2}$ (denoted by $\\mathbb{Q}_{q_{2}}$). Let $\\mathbb{Q}$ be the joint product measure.\n\nThe adaptive estimator is given by\n$$\n\\widehat{p}_{\\mathrm{adapt}}=\\mathbf{1}\\{S\\ge \\tau\\}\\,\\widehat{p}(q_{1})+\\mathbf{1}\\{S\\tau\\}\\,\\widehat{p}(q_{2})\n$$\nwhere $S=\\sum_{i=1}^{n}X^{(1)}_{i}$. The expectation of $\\widehat{p}_{\\mathrm{adapt}}$ under the sampling measure is\n$$\n\\mathbb{E}_{\\mathbb{Q}}[\\widehat{p}_{\\mathrm{adapt}}] = \\mathbb{E}_{\\mathbb{Q}}[\\mathbf{1}\\{S\\ge \\tau\\}\\,\\widehat{p}(q_{1})] + \\mathbb{E}_{\\mathbb{Q}}[\\mathbf{1}\\{S\\tau\\}\\,\\widehat{p}(q_{2})]\n$$\nWe analyze each term separately.\n\nFor the second term, the event $\\{S\\tau\\}$ depends only on the samples $X^{(1)}_{1},\\dots,X^{(1)}_{n}$, while the estimator $\\widehat{p}(q_{2})$ depends only on the independent samples $X^{(2)}_{1},\\dots,X^{(2)}_{n}$. Therefore, we can separate the expectation:\n$$\n\\mathbb{E}_{\\mathbb{Q}}[\\mathbf{1}\\{S\\tau\\}\\,\\widehat{p}(q_{2})] = \\mathbb{E}_{\\mathbb{Q}_{q_1}}[\\mathbf{1}\\{S\\tau\\}] \\cdot \\mathbb{E}_{\\mathbb{Q}_{q_2}}[\\widehat{p}(q_{2})]\n$$\nThe problem states that $\\widehat{p}(q)$ is an unbiased estimator for $p$. This means its expectation under the sampling measure is $p$. Thus, $\\mathbb{E}_{\\mathbb{Q}_{q_2}}[\\widehat{p}(q_{2})] = p$. Let $\\mathbb{Q}_{q_1}(S\\tau)$ denote the probability of the event $\\{S\\tau\\}$ under the measure $\\mathbb{Q}_{q_1}$. The second term simplifies to $p \\cdot \\mathbb{Q}_{q_1}(S\\tau)$.\n\nFor the first term, both the indicator $\\mathbf{1}\\{S\\ge \\tau\\}$ and the estimator $\\widehat{p}(q_{1})$ depend on the same set of samples $X^{(1)}_{1},\\dots,X^{(1)}_{n}$. The estimator $\\widehat{p}(q_{1})$ is defined as\n$$\n\\widehat{p}(q_{1}) = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{X^{(1)}_{i}=1\\}\\,\\frac{p}{q_{1}}\n$$\nSince $\\mathbf{1}\\{X^{(1)}_{i}=1\\}$ is simply $X^{(1)}_{i}$ for a Bernoulli variable, and $S = \\sum_{i=1}^{n} X^{(1)}_{i}$, we can write $\\widehat{p}(q_{1}) = \\frac{1}{n} S \\frac{p}{q_{1}}$.\nThe first term is then\n$$\n\\mathbb{E}_{\\mathbb{Q}}[\\mathbf{1}\\{S\\ge \\tau\\}\\,\\widehat{p}(q_{1})] = \\mathbb{E}_{\\mathbb{Q}_{q_1}}\\left[\\mathbf{1}\\{S\\ge \\tau\\} \\frac{S}{n} \\frac{p}{q_{1}}\\right] = \\frac{p}{n q_{1}} \\mathbb{E}_{\\mathbb{Q}_{q_1}}[S \\cdot \\mathbf{1}\\{S\\ge \\tau\\}]\n$$\nCombining both terms, the expected value of the adaptive estimator is\n$$\n\\mathbb{E}_{\\mathbb{Q}}[\\widehat{p}_{\\mathrm{adapt}}] = \\frac{p}{n q_{1}} \\mathbb{E}_{\\mathbb{Q}_{q_1}}[S \\cdot \\mathbf{1}\\{S\\ge \\tau\\}] + p \\cdot \\mathbb{Q}_{q_1}(S\\tau)\n$$\nThe bias is $\\mathbb{E}_{\\mathbb{Q}}[\\widehat{p}_{\\mathrm{adapt}}] - p$. We can write $p$ as $p=p \\cdot (\\mathbb{Q}_{q_1}(S\\ge \\tau) + \\mathbb{Q}_{q_1}(S\\tau))$, since the probabilities sum to $1$.\n$$\n\\text{Bias} = \\left( \\frac{p}{n q_{1}} \\mathbb{E}_{\\mathbb{Q}_{q_1}}[S \\cdot \\mathbf{1}\\{S\\ge \\tau\\}] + p \\cdot \\mathbb{Q}_{q_1}(S\\tau) \\right) - p \\cdot (\\mathbb{Q}_{q_1}(S\\ge \\tau) + \\mathbb{Q}_{q_1}(S\\tau))\n$$\n$$\n\\text{Bias} = \\frac{p}{n q_{1}} \\mathbb{E}_{\\mathbb{Q}_{q_1}}[S \\cdot \\mathbf{1}\\{S\\ge \\tau\\}] - p \\cdot \\mathbb{Q}_{q_1}(S\\ge \\tau)\n$$\nUnder the measure $\\mathbb{Q}_{q_1}$, the variables $X^{(1)}_{i}$ are i.i.d. Bernoulli with parameter $q_{1}$. Their sum $S$ follows a binomial distribution, $S \\sim \\text{Binomial}(n, q_{1})$. The probability mass function (PMF) is $\\mathbb{Q}_{q_1}(S=k) = \\binom{n}{k} q_{1}^k (1-q_{1})^{n-k}$.\n\nThe partial expectation $\\mathbb{E}_{\\mathbb{Q}_{q_1}}[S \\cdot \\mathbf{1}\\{S\\ge \\tau\\}]$ is\n$$\n\\mathbb{E}_{\\mathbb{Q}_{q_1}}[S \\cdot \\mathbf{1}\\{S\\ge \\tau\\}] = \\sum_{k=\\tau}^{n} k \\cdot \\mathbb{Q}_{q_1}(S=k) = \\sum_{k=\\tau}^{n} k \\binom{n}{k} q_{1}^k (1-q_{1})^{n-k}\n$$\nUsing the identity $k \\binom{n}{k} = n \\binom{n-1}{k-1}$ for $k\\geq 1$ (and noting $\\tau \\geq 1$):\n$$\n\\sum_{k=\\tau}^{n} n \\binom{n-1}{k-1} q_{1}^k (1-q_{1})^{n-k} = n q_{1} \\sum_{k=\\tau}^{n} \\binom{n-1}{k-1} q_{1}^{k-1} (1-q_{1})^{n-k}\n$$\nLet $j=k-1$. The sum becomes\n$$\nn q_{1} \\sum_{j=\\tau-1}^{n-1} \\binom{n-1}{j} q_{1}^{j} (1-q_{1})^{(n-1)-j}\n$$\nThis sum is the tail probability $\\mathbb{Q}_{q_1}(Y \\ge \\tau-1)$, where $Y$ is a random variable with distribution $Y \\sim \\text{Binomial}(n-1, q_{1})$.\nSo, $\\mathbb{E}_{\\mathbb{Q}_{q_1}}[S \\cdot \\mathbf{1}\\{S\\ge \\tau\\}] = n q_{1} \\mathbb{Q}_{q_1}(Y \\ge \\tau-1)$.\n\nSubstituting this back into the expression for the bias:\n$$\n\\text{Bias} = \\frac{p}{n q_{1}} (n q_{1} \\mathbb{Q}_{q_1}(Y \\ge \\tau-1)) - p \\cdot \\mathbb{Q}_{q_1}(S\\ge \\tau) = p (\\mathbb{Q}_{q_1}(Y \\ge \\tau-1) - \\mathbb{Q}_{q_1}(S\\ge \\tau))\n$$\nWe can find a simpler form for this difference of probabilities using the relationship between the distributions of $S$ and $Y$. Let $S = Y' + X'$, where $Y' \\sim \\text{Binomial}(n-1, q_{1})$ and $X' \\sim \\text{Bernoulli}(q_{1})$ are independent.\nThe law of total probability gives:\n$$\n\\mathbb{Q}_{q_1}(S \\ge \\tau) = \\mathbb{Q}_{q_1}(Y'+X' \\ge \\tau) = \\mathbb{Q}_{q_1}(Y'+1 \\ge \\tau)\\mathbb{Q}_{q_1}(X'=1) + \\mathbb{Q}_{q_1}(Y' \\ge \\tau)\\mathbb{Q}_{q_1}(X'=0)\n$$\n$$\n\\mathbb{Q}_{q_1}(S \\ge \\tau) = \\mathbb{Q}_{q_1}(Y' \\ge \\tau-1) \\cdot q_{1} + \\mathbb{Q}_{q_1}(Y' \\ge \\tau) \\cdot (1-q_{1})\n$$\nSince $Y'$ and $Y$ have the same distribution, we can substitute this into the bias expression:\n$$\n\\mathbb{Q}_{q_1}(Y \\ge \\tau-1) - \\mathbb{Q}_{q_1}(S \\ge \\tau) = \\mathbb{Q}_{q_1}(Y \\ge \\tau-1) - [q_{1}\\mathbb{Q}_{q_1}(Y \\ge \\tau-1) + (1-q_{1})\\mathbb{Q}_{q_1}(Y \\ge \\tau)]\n$$\n$$\n= (1-q_{1})\\mathbb{Q}_{q_1}(Y \\ge \\tau-1) - (1-q_{1})\\mathbb{Q}_{q_1}(Y \\ge \\tau)\n$$\n$$\n= (1-q_{1})[\\mathbb{Q}_{q_1}(Y \\ge \\tau-1) - \\mathbb{Q}_{q_1}(Y \\ge \\tau)]\n$$\nThe difference in the brackets is precisely the probability that $Y = \\tau-1$.\n$$\n= (1-q_{1})\\mathbb{Q}_{q_1}(Y = \\tau-1)\n$$\nThe PMF for $Y \\sim \\text{Binomial}(n-1, q_{1})$ at $\\tau-1$ is:\n$$\n\\mathbb{Q}_{q_1}(Y=\\tau-1) = \\binom{n-1}{\\tau-1} q_{1}^{\\tau-1} (1-q_{1})^{(n-1)-(\\tau-1)} = \\binom{n-1}{\\tau-1} q_{1}^{\\tau-1} (1-q_{1})^{n-\\tau}\n$$\nThe bias is therefore:\n$$\n\\text{Bias} = p \\cdot (1-q_{1}) \\cdot \\left( \\binom{n-1}{\\tau-1} q_{1}^{\\tau-1} (1-q_{1})^{n-\\tau} \\right)\n$$\n$$\n\\text{Bias} = p \\binom{n-1}{\\tau-1} q_{1}^{\\tau-1} (1-q_{1})^{n-\\tau+1}\n$$\nThis is the final closed-form expression for the bias, in terms of $p$, $n$, $q_{1}$, and $\\tau$. As expected, the parameter $q_{2}$ does not appear due to the independence of the second-stage sample set when the second proposal is chosen.", "answer": "$$\\boxed{p \\binom{n-1}{\\tau-1} q_{1}^{\\tau-1} (1-q_{1})^{n-\\tau+1}}$$", "id": "3335116"}]}