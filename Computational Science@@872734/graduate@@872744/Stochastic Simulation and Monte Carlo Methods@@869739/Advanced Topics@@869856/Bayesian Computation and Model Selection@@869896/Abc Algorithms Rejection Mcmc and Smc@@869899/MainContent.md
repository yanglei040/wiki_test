## Introduction
Bayesian inference provides a powerful framework for quantifying uncertainty and updating beliefs about model parameters in light of data. However, its application is often hindered by a major practical obstacle: the intractability of the [likelihood function](@entry_id:141927). For many complex, simulator-based models across science and engineering, evaluating the probability of observed data for a given parameter set is computationally prohibitive or even analytically impossible. This knowledge gap has spurred the development of "likelihood-free" methods, with Approximate Bayesian Computation (ABC) emerging as a leading and versatile approach. This article offers a graduate-level exploration of the ABC algorithm family, designed to equip you with the theoretical knowledge and practical insights needed to apply these powerful techniques.

This comprehensive guide is structured to build your understanding progressively. The journey begins in the **Principles and Mechanisms** chapter, which demystifies the core idea of ABC. We will start with the fundamental [rejection sampling algorithm](@entry_id:260966), then delve into the theoretical implications of [summary statistics](@entry_id:196779) and tolerance levels. Building on this foundation, we will explore the more sophisticated and efficient Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC) variants that are essential for tackling challenging, high-dimensional problems. Next, the **Applications and Interdisciplinary Connections** chapter demonstrates the real-world utility of ABC. We will examine its use in complex system modeling across fields like climate science and physics, discuss powerful methodological extensions that improve accuracy and speed, and explore its role in advanced tasks such as [model selection](@entry_id:155601) and privacy-preserving inference. Finally, the **Hands-On Practices** section provides opportunities to apply these concepts, guiding you through theoretical derivations and implementation challenges that solidify the connection between theory and practice.

## Principles and Mechanisms

Approximate Bayesian Computation (ABC) encompasses a family of algorithms designed for Bayesian inference in models where the [likelihood function](@entry_id:141927), $p(y|\theta)$, is intractable or computationally prohibitive to evaluate, but from which data can be efficiently simulated. These "likelihood-free" methods circumvent the direct evaluation of the likelihood by comparing simulated data to observed data. This chapter elucidates the foundational principles of ABC, beginning with the elementary [rejection sampling algorithm](@entry_id:260966) and progressing to more sophisticated and efficient Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC) variants.

### The Core Principle: Rejection Sampling on Summary Statistics

The fundamental challenge in Bayesian inference is to characterize the [posterior distribution](@entry_id:145605), which, according to Bayes' theorem, is proportional to the product of the prior and the likelihood: $p(\theta|y) \propto \pi(\theta)p(y|\theta)$. When $p(y|\theta)$ is intractable, this direct approach fails. The most basic ABC method reimagines this problem through simulation.

Imagine we could draw a parameter $\theta^*$ from the prior $\pi(\theta)$ and then simulate a dataset $x$ from the model's generative process, $p(x|\theta^*)$. If, by chance, the simulated data $x$ were identical to our observed data $y$, then this $\theta^*$ would be a legitimate sample from the true [posterior distribution](@entry_id:145605) $p(\theta|y)$. However, for continuous or [high-dimensional data](@entry_id:138874), the probability of the event $x=y$ is zero. This necessitates two crucial approximations.

First, instead of comparing the full datasets, we compare lower-dimensional **[summary statistics](@entry_id:196779)**, $S(y)$. A summary statistic is a function of the data that captures relevant information for inferring the parameter $\theta$. Second, instead of requiring an exact match between [summary statistics](@entry_id:196779), $S(x) = S(y)$, we introduce a **tolerance**, $\epsilon > 0$, and a **distance metric**, $\rho(\cdot, \cdot)$, accepting the proposal $\theta^*$ if the simulated summary is "close enough" to the observed summary.

This leads to the foundational **Rejection ABC algorithm**:

1.  Draw a parameter proposal $\theta^*$ from the prior distribution, $\theta^* \sim \pi(\theta)$.
2.  Simulate a synthetic dataset $x^*$ from the likelihood, $x^* \sim p(x|\theta^*)$.
3.  Compute the summary statistic of the synthetic data, $s^* = S(x^*)$.
4.  Accept $\theta^*$ if $\rho(s^*, s_{\text{obs}}) \leq \epsilon$, where $s_{\text{obs}} = S(y)$ is the summary of the observed data. Otherwise, reject $\theta^*$.
5.  Repeat steps 1-4 until a desired number of accepted samples is collected.

The set of accepted parameters forms an empirical sample from an approximation to the true posterior, known as the **ABC posterior**, denoted $p_\epsilon(\theta|s_{\text{obs}})$. This distribution is precisely the [posterior distribution](@entry_id:145605) of $\theta$ conditional on the event that the simulated summary falls within the $\epsilon$-ball around the observed summary.

A critical performance metric for this algorithm is its **[acceptance rate](@entry_id:636682)**. This rate is determined by the probability that a random draw satisfies the acceptance criterion, averaged over both the prior and the simulator. This probability is governed by the **prior-predictive distribution** of the summary statistic, which is the distribution of $s^*$ obtained by first drawing $\theta^* \sim \pi(\theta)$ and then $s^* \sim p(s|\theta^*)$.

Consider a simple univariate normal location model where $y_i|\theta \sim \mathcal{N}(\theta, \sigma^2)$ for $i=1,\dots,n$, and the prior is $\theta \sim \mathcal{N}(\mu_0, \tau_0^2)$. If we use the [sample mean](@entry_id:169249) $\bar{y}$ as our summary statistic, we know that its [sampling distribution](@entry_id:276447) is $\bar{y}|\theta \sim \mathcal{N}(\theta, \sigma^2/n)$. The prior-predictive distribution for a simulated summary $\bar{y}'$ is then the convolution of two normal distributions, resulting in $\bar{y}' \sim \mathcal{N}(\mu_0, \tau_0^2 + \sigma^2/n)$. The expected [acceptance rate](@entry_id:636682) for an ABC algorithm with tolerance $\epsilon$ and observed summary $s_{\text{obs}}$ is the probability mass of this prior-predictive distribution within the interval $[s_{\text{obs}} - \epsilon, s_{\text{obs}} + \epsilon]$. As demonstrated in the context of [@problem_id:3286901], one can leverage this relationship to determine the tolerance $\epsilon$ required to achieve a target [acceptance rate](@entry_id:636682), for instance, $\alpha=0.2$. In that specific scenario, with a prior centered on the observed data ($\mu_0 = s_{\text{obs}}$), the [acceptance probability](@entry_id:138494) simplifies to $A(\epsilon) = 2\Phi(\epsilon/d) - 1$, where $d^2$ is the prior-predictive variance and $\Phi$ is the standard normal CDF. This equation can be inverted to find the unique $\epsilon$ for a given $\alpha$.

This framework extends naturally to multivariate settings. For a multivariate parameter $\theta \in \mathbb{R}^p$ and summary statistic $s \in \mathbb{R}^p$, a Euclidean distance may be inadequate as it ignores the covariance structure of the summary statistic. A more principled choice is the **Mahalanobis distance**, $d(s^*, s_{\text{obs}}) = \sqrt{(s^* - s_{\text{obs}})^\top \Sigma_s^{-1} (s^* - s_{\text{obs}})}$, where $\Sigma_s$ is the covariance matrix of the summary statistic. A natural choice for $\Sigma_s$ is the prior-predictive covariance, which correctly scales the discrepancy by the expected variability. For a multivariate normal location model with a normal prior, the prior-predictive distribution of the summary statistic remains normal. A key result from [multivariate statistics](@entry_id:172773) states that if a vector $Z \sim \mathcal{N}(0, S)$, then the [quadratic form](@entry_id:153497) $Z^\top S^{-1} Z$ follows a **chi-squared distribution** with $p$ degrees of freedom, $\chi^2_p$. Consequently, the acceptance probability for an ABC algorithm using a Mahalanobis distance scaled by the prior-predictive covariance is simply the CDF of the $\chi^2_p$ distribution evaluated at $\epsilon^2$ [@problem_id:3286907]. This establishes a direct and computable link between the tolerance and the [acceptance rate](@entry_id:636682), even in complex multivariate problems.

### Theoretical Foundations and the Role of Sufficiency

The validity and behavior of ABC hinge on the interplay between the summary statistic $S(y)$ and the tolerance $\epsilon$. A more formal definition of the unnormalized ABC posterior reveals its theoretical properties:
$$
p_{\epsilon}(\theta | y) \propto \pi(\theta)\int K_{\epsilon}\!\big(\rho(S(x),S(y))\big)\,p(x | \theta)\,dx
$$
Here, $K_\epsilon$ is a kernel function (e.g., an indicator function for $\rho \le \epsilon$) that assigns weight based on the discrepancy. This expression can be interpreted as the prior $\pi(\theta)$ multiplied by a smoothed or "approximate" likelihood, which is the expected value of the kernel under the [generative model](@entry_id:167295) $p(x|\theta)$.

The crucial theoretical question is: what does this ABC posterior converge to as the tolerance $\epsilon$ approaches zero? As $\epsilon \to 0$, the kernel $K_\epsilon$ behaves increasingly like a **Dirac delta function**, which concentrates all its mass at a single point. This has a profound consequence: the integral collapses, and the ABC posterior converges pointwise to the true posterior *conditional on the summary statistic*, not necessarily the full data [@problem_id:3286950].
$$
\lim_{\epsilon \to 0} p_{\epsilon}(\theta | y) \propto \pi(\theta) p(S(y) | \theta) = p(\theta | S(y))
$$
This limiting behavior leads to a fundamental dichotomy:

1.  **Sufficient Statistic**: If $S(y)$ is a **sufficient statistic** for the parameter $\theta$, then by definition, it captures all the information about $\theta$ contained in the data $y$. The Fisher-Neyman [factorization theorem](@entry_id:749213) shows that the likelihood can be factored as $p(y|\theta) = g(S(y), \theta)h(y)$, meaning the likelihood's dependence on $\theta$ is exclusively through $S(y)$. In this case, conditioning on the summary is equivalent to conditioning on the full data: $p(\theta|S(y)) = p(\theta|y)$. Therefore, if one can find a [sufficient statistic](@entry_id:173645), ABC is **asymptotically exact** as $\epsilon \to 0$. For many models in the [exponential family](@entry_id:173146), such as the normal distribution with unknown mean, a low-dimensional [sufficient statistic](@entry_id:173645) (like the sample mean) exists.

2.  **Insufficient Statistic**: If $S(y)$ is not sufficient, it discards some information about $\theta$ that is present in the full data $y$. In this scenario, $p(\theta|S(y)) \neq p(\theta|y)$. The ABC posterior converges to a target distribution that is based on incomplete information, and thus it will be systematically different from the true posterior, no matter how small $\epsilon$ becomes. A stark example [@problem_id:3286950] is using a binary summary, such as $S(y) = \mathbb{I}\{\bar{y} \ge 0\}$. This summary discards all information about the magnitude of the [sample mean](@entry_id:169249). The resulting limiting ABC posterior depends only on the probability that $\bar{y} \ge 0$ given $\theta$, a far cry from the true posterior which depends on the precise value of $\bar{y}$.

The choice of [summary statistics](@entry_id:196779) is therefore a central challenge in ABC. While sufficiency is a powerful guiding principle, [sufficient statistics](@entry_id:164717) of low dimension are rare. This has motivated the development of methods that work with the full data by defining a discrepancy metric over entire datasets. One such principled metric is the **Maximum Mean Discrepancy (MMD)**, which operates in a Reproducing Kernel Hilbert Space (RKHS). For a **characteristic kernel** (like the Gaussian kernel), the MMD between two probability distributions is zero if and only if the distributions are identical. Using MMD as the discrepancy $\rho$ allows ABC to compare entire [empirical distributions](@entry_id:274074) without the need for manual selection of [summary statistics](@entry_id:196779). Theoretical analysis [@problem_id:3286895] shows that the expected squared MMD between an empirical sample of size $n$ and its true generating distribution decays at a rate dependent on $n$ and the kernel's bandwidth. This rate informs how the tolerance $\epsilon$ must be decreased as the sample size $n$ grows to ensure [statistical consistency](@entry_id:162814).

### Enhancing Efficiency: Markov Chain Monte Carlo Approaches

The primary drawback of the Rejection ABC algorithm is its inefficiency. If the prior is diffuse or mismatched with the region of high likelihood, the acceptance rate can be astronomically low, requiring an unfeasible number of simulations. MCMC methods provide a powerful remedy by constructing a Markov chain that explores the ABC posterior more intelligently.

#### ABC-MCMC

Instead of drawing independent proposals from the prior, **ABC-MCMC** uses a proposal mechanism that depends on the current state of the chain. A standard approach is to define a Markov chain on an augmented space of states $(\theta, x)$. The target distribution on this space is:
$$
\tilde{\pi}_{\epsilon}(\theta,x | y) \propto \pi(\theta) p(x | \theta) K_{\epsilon}(\rho(S(x), S(y)))
$$
The [marginal distribution](@entry_id:264862) of this target with respect to $x$ is precisely the desired ABC posterior, $p_{\epsilon}(\theta|y)$.

A Metropolis-Hastings algorithm can be constructed to sample from this augmented target. At a given state $(\theta, x)$, a new state $(\theta', x')$ is proposed by first drawing $\theta' \sim q(\theta'|\theta)$ from a proposal kernel and then simulating $x' \sim p(x'|\theta')$. The acceptance probability for this move simplifies remarkably. As shown through a detailed balance analysis [@problem_id:3286942], the forward and reverse simulation densities, $p(x'|\theta')$ and $p(x|\theta)$, cancel out, leading to an acceptance ratio that does not involve the intractable data-generating density itself:
$$
\alpha = \min \left( 1, \frac{\pi(\theta') K_{\epsilon}(\rho(S(x'), S(y))) q(\theta|\theta')}{\pi(\theta) K_{\epsilon}(\rho(S(x), S(y))) q(\theta'|\theta)} \right)
$$
This algorithm guarantees that the marginal chain on $\theta$ has $p_\epsilon(\theta|y)$ as its [stationary distribution](@entry_id:142542). The efficiency of ABC-MCMC depends critically on the choice of the proposal kernel $q(\theta'|\theta)$. General principles of MCMC tuning apply here. For instance, in high-dimensional settings where the target is approximately Gaussian, theory suggests that a random-walk proposal tuned to the local geometry (e.g., using an approximation of the Hessian of the log-posterior) can be highly effective. The asymptotically [optimal acceptance rate](@entry_id:752970) for such a scheme is approximately 0.234 [@problem_id:3286942].

#### The Pseudo-Marginal Perspective

A highly influential and powerful view of ABC-MCMC reframes it as a **pseudo-marginal algorithm**. In this perspective, we consider the exact ABC posterior target $\pi(\theta) \Phi_{\epsilon}(\theta)$, where $\Phi_{\epsilon}(\theta) = \mathbb{E}_{x \sim p(\cdot|\theta)}[K_{\epsilon}(\rho(S(x),S(y)))]$ is the "ABC likelihood"â€”the probability of acceptance for a given $\theta$. This function is intractable, as it is defined by an expectation.

However, we can obtain an **[unbiased estimator](@entry_id:166722)** of $\Phi_{\epsilon}(\theta)$ by simulating a single synthetic dataset $x$ and computing $K_{\epsilon}(\rho(S(x),S(y)))$. More generally, we can average over $M$ simulations to get a more stable estimator $\widehat{\Phi}_{M}(\theta)$. A remarkable result in MCMC theory states that if we substitute an unbiased, non-negative estimator of the likelihood into the Metropolis-Hastings acceptance ratio, the resulting Markov chain *still* has the correct [stationary distribution](@entry_id:142542).

This pseudo-marginal approach is mathematically equivalent to the ABC-MCMC algorithm described above (with $M=1$). However, this perspective provides a crucial insight: the variance of the estimator $\widehat{\Phi}_{M}(\theta)$ directly impacts the efficiency of the MCMC sampler. As detailed in the analysis of [@problem_id:3286947], the noise introduced by the estimation of the likelihood propagates into the acceptance ratio. A high variance in the [log-likelihood](@entry_id:273783) estimator, $\log(\widehat{\Phi}_{M}(\theta))$, leads to a lower average [acceptance probability](@entry_id:138494) and a "stickier" chain that mixes poorly. The derived "slowdown factor" quantifies this loss of efficiency. This explains why an ABC-MCMC chain might perform poorly even if the underlying MCMC proposal for $\theta$ is well-tuned: the Monte Carlo variability in the likelihood approximation itself can cripple the sampler. The practical implication is that the number of simulations per step, $M$, acts as a tuning parameter that controls the variance of the pseudo-likelihood estimate, trading computational cost per step for overall MCMC efficiency.

### Sequential Monte Carlo for ABC (ABC-SMC)

Sequential Monte Carlo methods offer a compelling alternative to MCMC, particularly well-suited to the ABC context. The core idea of **ABC-SMC** is to iteratively approach the target posterior by traversing a sequence of intermediate distributions corresponding to a decreasing sequence of tolerances, $\epsilon_0 > \epsilon_1 > \dots > \epsilon_T$. The algorithm begins with a population of "particles" (parameter samples) approximating the distribution at tolerance $\epsilon_0$ (which can be so large that it is effectively the prior) and sequentially propagates them to target the distribution at the next, smaller tolerance.

#### The ABC-SMC Algorithm and Importance Weighting

A generic ABC-SMC algorithm maintains a weighted set of particles $\{(\theta_i, w_i)\}_{i=1}^N$ at each stage $t$. To move from stage $t-1$ to $t$, particles are typically perturbed via a Markov kernel, and then an ABC simulation is performed. Only those particles that meet the new, stricter tolerance $\epsilon_t$ survive. Their [importance weights](@entry_id:182719) are then adjusted to account for the proposal mechanism and the new target.

In one common variant, often called a Population Monte Carlo (PMC) style algorithm, the proposal for a new particle at stage $t$ is drawn from a [mixture distribution](@entry_id:172890) based on the previous population, $q_t(\theta) = \sum_j w_{t-1}^{(j)} K_t(\theta | \theta_{t-1}^{(j)})$, where $K_t$ is a perturbation kernel. A new $\theta^*$ is drawn from $q_t$, a dataset is simulated, and if it is accepted under tolerance $\epsilon_t$, it is added to the new population. Based on fundamental [importance sampling](@entry_id:145704) principles, the correct unnormalized importance weight for this new particle is the ratio of the target density to the sampling density. A key insight [@problem_id:3286910] is that for this scheme, the acceptance probability terms in the numerator (from the target) and denominator (from the effective [sampling distribution](@entry_id:276447)) cancel out, leaving a simple expression for the unnormalized weight:
$$
w_t(\theta^*) \propto \frac{\pi_t(\theta^*)}{\text{effective proposal}} \propto \frac{\pi(\theta^*)}{q_t(\theta^*)}
$$
This provides a straightforward mechanism for propagating the particle population through the sequence of decreasing tolerances.

#### Resampling and Particle Degeneracy

A persistent challenge in any SMC method is **[weight degeneracy](@entry_id:756689)**, where after a few iterations, one particle acquires a weight close to 1 while all others have weights near 0. To combat this, a **[resampling](@entry_id:142583)** step is employed. When the weights become too uneven, the current particle set is replaced by a new set drawn with replacement from the old set, with probabilities given by their weights. This eliminates low-weight particles and duplicates high-weight ones.

A standard metric to trigger [resampling](@entry_id:142583) is the **Effective Sample Size (ESS)**. Derived from first principles by comparing the variance of a weighted estimator to an ideal unweighted one, the ESS is given by $N_{\text{eff}} = (\sum_{i=1}^N w_i^2)^{-1}$ [@problem_id:3286939]. When $N_{\text{eff}}$ drops below a threshold (e.g., $N/2$), resampling is performed.

While essential, resampling introduces its own problem: **particle impoverishment** or loss of diversity. Since particles are duplicated, the number of unique ancestors in the particle set decreases over time. The expected number of distinct particles after one round of [multinomial resampling](@entry_id:752299) can be calculated as $\sum_{i=1}^N (1 - (1-w_i)^N)$, which is always less than or equal to $N$ [@problem_id:3286939]. A more profound analysis using the Feynman-Kac formalism views ABC-SMC as a form of tempering on the discrepancy [@problem_id:3286913]. This perspective allows for the calculation of the asymptotic rate, $\lambda$, at which ancestral lineages coalesce. This rate of **path degeneracy** is shown to depend directly on the population size $N$ and the rate of tolerance reduction $r = \epsilon_t / \epsilon_{t-1}$. Specifically, $\lambda = -\ln(1 - 1/(N r^d))$, where $d$ is the dimension of the summary statistic. This powerful result provides theoretical guidance for setting algorithm parameters: to maintain genealogical diversity, one must use a larger population $N$ if the tolerances are being reduced aggressively (small $r$).

#### Adaptive ABC-SMC

A practical difficulty in ABC-SMC is choosing the tolerance schedule $\{\epsilon_t\}$ a priori. If the tolerances decrease too slowly, the algorithm is inefficient. If they decrease too quickly, [weight degeneracy](@entry_id:756689) becomes severe, and the ESS plummets. This motivates **adaptive ABC-SMC**, where the next tolerance $\epsilon_{t+1}$ is chosen automatically based on the performance at stage $t$.

A common strategy is to choose $\epsilon_{t+1}$ such that the ESS of the subsequent population is maintained at a target level, $ESS^*$. This can be framed as a control theory problem [@problem_id:3286903]. By linearizing the relationship between ESS and $\epsilon$ around an equilibrium point, we can design a [proportional feedback](@entry_id:273461) controller to update the tolerance:
$$
\epsilon_{t+1} = \epsilon_t + \kappa (ESS^* - ESS_t)
$$
Here, $\kappa$ is a feedback gain parameter. A discrete-time stability analysis of this system reveals its behavior. To ensure the adaptive scheme is stable (i.e., $\epsilon_t$ converges to the desired equilibrium) and non-oscillatory (converges monotonically), the gain $\kappa$ must be chosen carefully. The analysis shows there is a maximum allowable gain, $\kappa_{\text{max}}$, which depends on the local sensitivity of the ESS to the tolerance. This application of control theory provides a principled method for automating one of the most difficult tuning aspects of ABC-SMC, making the algorithm more robust and user-friendly.