## Introduction
In the realm of Bayesian statistics, inference hinges on the [posterior distribution](@entry_id:145605), which combines prior knowledge with the likelihood of observed data. However, for many complex systems in science and engineering—from evolutionary biology to cosmology—the generative models are so intricate that their likelihood functions are computationally intractable or impossible to express analytically. This creates a significant barrier to applying standard Bayesian methods. Approximate Bayesian Computation (ABC) emerges as a powerful and principled framework to overcome this challenge, enabling statistical inference in these so-called "likelihood-free" scenarios. This article provides a comprehensive exploration of ABC. We will first delve into the foundational **Principles and Mechanisms**, explaining how ABC works and the nature of its approximations. Next, we will survey its broad utility through **Applications and Interdisciplinary Connections** in various scientific domains. Finally, we will bridge theory and practice with a look at several **Hands-On Practices** designed to solidify understanding.

## Principles and Mechanisms

In the landscape of Bayesian statistics, the [posterior distribution](@entry_id:145605) is the cornerstone of inference, synthesizing prior knowledge with information from observed data. This synthesis is governed by Bayes' theorem, which states that the posterior density is proportional to the product of the prior density and the likelihood function: $\pi(\theta \mid x_{\text{obs}}) \propto \pi(\theta) p(x_{\text{obs}} \mid \theta)$. While elegant in theory, this prescription hinges on the ability to evaluate the [likelihood function](@entry_id:141927), $p(x_{\text{obs}} \mid \theta)$. In many complex systems across science and engineering—from cosmology and systems biology to econometrics—the underlying generative processes are described by intricate computer simulations for which the likelihood function is computationally intractable or lacks a [closed-form expression](@entry_id:267458). Approximate Bayesian Computation (ABC) provides a powerful framework for conducting Bayesian inference in these "likelihood-free" scenarios. This chapter elucidates the foundational principles and mechanisms of ABC, from its theoretical underpinnings to its practical implementation and statistical properties.

### The Foundation: Inference with Implicit Generative Models

At its core, [likelihood-free inference](@entry_id:190479) addresses models defined not by an explicit likelihood function but by a stochastic generative procedure, often called an **implicit model**. Such a model can be formalized as a simulator, represented by a function $g$, that maps a parameter vector $\theta$ from the [parameter space](@entry_id:178581) $\Theta$ and a set of stochastic inputs (or [latent variables](@entry_id:143771)) $u$ from a space $\mathcal{U}$ to an observable outcome $x$ in the data space $\mathcal{X}$. That is, $x = g(\theta, u)$, where the [stochasticity](@entry_id:202258) arises from drawing $u$ from a fixed, known distribution, such as a uniform or standard normal distribution [@problem_id:3489611].

For a fixed parameter $\theta$, this generative process induces a probability distribution over the data space $\mathcal{X}$. Formally, if the map $u \mapsto g(\theta, u)$ is a measurable function, it defines a **pushforward probability measure**, $P_{\theta}$, on the data space. The probability of observing data within a set $B \subset \mathcal{X}$ is given by the measure of the preimage of $B$ in the [latent space](@entry_id:171820) $\mathcal{U}$.

The existence of a likelihood *density* function, $p(x \mid \theta)$, requires a further condition: the [pushforward measure](@entry_id:201640) $P_{\theta}$ must be absolutely continuous with respect to a reference measure on $\mathcal{X}$ (typically the Lebesgue measure for continuous data). If this holds, the Radon-Nikodym theorem guarantees the existence of the likelihood density. However, for many simulators, this condition is not met. For instance, if the simulator maps the latent space $\mathcal{U}$ onto a lower-dimensional manifold within the data space $\mathcal{X}$, the resulting measure $P_{\theta}$ is singular. In such cases, the probability of observing any single point $x_{\text{obs}}$ is zero, and a likelihood density does not exist. Standard Bayesian inference, which relies on evaluating $p(x_{\text{obs}} \mid \theta)$, becomes ill-posed [@problem_id:3489611]. ABC is designed to circumvent this very problem.

### The Core Idea of ABC: Replacing Rejection with Approximation

The simplest conceptual algorithm for sampling from a posterior, known as [rejection sampling](@entry_id:142084), would involve sampling a parameter $\theta^{*}$ from the prior $\pi(\theta)$, simulating a dataset $x^{*}$ from the model $p(\cdot \mid \theta^{*})$, and accepting $\theta^{*}$ if and only if the simulated data exactly matches the observed data, $x^{*} = x_{\text{obs}}$. For any continuous or high-dimensional discrete data, the probability of an exact match is zero, rendering the algorithm useless.

ABC overcomes this by replacing the impossible-to-satisfy condition of an exact match with an approximate one. Instead of comparing the full datasets, we first reduce their dimensionality using a set of **[summary statistics](@entry_id:196779)**, $s(\cdot)$. Then, we accept a proposed parameter if the summary of the simulated data, $s(x^{*})$, is "close" to the summary of the observed data, $s_{\text{obs}} = s(x_{\text{obs}})$. Closeness is measured by a **distance metric**, $\rho(\cdot, \cdot)$, and a **tolerance threshold**, $\epsilon$. The basic ABC rejection algorithm is thus:

1.  Sample a parameter candidate $\theta^{*} \sim \pi(\theta)$.
2.  Simulate a synthetic dataset $x^{*} \sim p(\cdot \mid \theta^{*})$.
3.  Compute [summary statistics](@entry_id:196779) $s(x^{*})$.
4.  If $\rho(s(x^{*}), s_{\text{obs}}) \le \epsilon$, accept $\theta^{*}$; otherwise, reject it.

This procedure generates samples from an approximation to the true posterior. To formalize this approximation, we can introduce a **kernel function**, $K_{\epsilon}$, that assigns a weight based on the distance. The simplest case, corresponding to the algorithm above, uses a uniform or boxcar kernel, $K_{\epsilon}(u) \propto \mathbb{I}\{u \le \epsilon\}$, where $\mathbb{I}\{\cdot\}$ is the indicator function [@problem_id:3288743]. More generally, we can define an **ABC likelihood** as the simulator's expectation of the kernel-weighted distance:

$L_{\epsilon}(\theta \mid s_{\text{obs}}) = \mathbb{E}_{x \sim p(\cdot \mid \theta)} \left[ K_{\epsilon}(\rho(s(x), s_{\text{obs}})) \right] = \int_{\mathcal{X}} K_{\epsilon}(\rho(s(x), s_{\text{obs}})) \, p(x \mid \theta) \, dx$

The resulting **ABC posterior distribution** is then defined by substituting this approximate likelihood into Bayes' theorem:

$\pi_{\epsilon}(\theta \mid s_{\text{obs}}) \propto \pi(\theta) L_{\epsilon}(\theta \mid s_{\text{obs}})$

This formal definition reveals that ABC targets a well-defined [posterior distribution](@entry_id:145605) that is derived from smoothing the true likelihood. The quality of this approximation depends critically on the choice of [summary statistics](@entry_id:196779) $s(\cdot)$ and the tolerance $\epsilon$.

### Sources of Approximation and Their Interpretation

The deviation of the ABC posterior $\pi_{\epsilon}(\theta \mid s_{\text{obs}})$ from the true posterior $\pi(\theta \mid x_{\text{obs}})$ stems from two distinct sources: the non-zero tolerance $\epsilon > 0$ and the potential information loss from using [summary statistics](@entry_id:196779) $s(\cdot)$ instead of the full data $x$.

#### The Role of Tolerance $\epsilon$: A Convolution Interpretation

The use of a tolerance $\epsilon > 0$ can be understood as a form of smoothing. The ABC likelihood can be expressed as an integral over the summary statistic space:

$L_{\epsilon}(\theta \mid s_{\text{obs}}) = \int p_s(s \mid \theta) \, K_{\epsilon}(\rho(s, s_{\text{obs}})) \, ds$

where $p_s(s \mid \theta)$ is the likelihood of the [summary statistics](@entry_id:196779). This expression has the mathematical structure of a **convolution**. It shows that the ABC likelihood is not the true likelihood of the summary statistic, $p_s(s_{\text{obs}} \mid \theta)$, but rather a smoothed version of it, where the [smoothing kernel](@entry_id:195877) is determined by $K_{\epsilon}$ and $\rho$ [@problem_id:3288759].

This leads to a powerful interpretation: ABC can be viewed as performing *exact* Bayesian inference for a perturbed statistical model. Instead of assuming we observe $s_{\text{obs}}$ from the model $s \sim p_s(\cdot \mid \theta)$, ABC is equivalent to exact inference for a hierarchical model where $s \sim p_s(\cdot \mid \theta)$ and our observation is a noisy version of $s$, for instance, $s_{\text{obs}} = s + \eta$, where the noise distribution of $\eta$ is determined by the kernel $K_{\epsilon}$ [@problem_id:3288759].

A concrete example makes this clear. Consider data $y_i \sim \mathcal{N}(\theta, \sigma^2)$ for $i=1, \dots, n$, where $\sigma^2$ is known. The [sample mean](@entry_id:169249) $\bar{y}$ is a [sufficient statistic](@entry_id:173645), and its [sampling distribution](@entry_id:276447) is $\bar{y} \sim \mathcal{N}(\theta, \sigma^2/n)$. If we perform ABC using a Gaussian kernel $K_{\epsilon}(u) \propto \exp(-u^2 / 2\epsilon^2)$, the resulting ABC likelihood can be calculated analytically. It is the convolution of two Gaussian densities, which results in another Gaussian density. Specifically, the ABC likelihood is $L_{\epsilon}(\bar{y}_{\text{obs}} \mid \theta) = \mathcal{N}(\bar{y}_{\text{obs}}; \theta, \frac{\sigma^2}{n} + \epsilon^2)$. This shows that using a Gaussian kernel is equivalent to adding independent Gaussian noise with variance $\epsilon^2$ to the summary statistic. The tolerance parameter $\epsilon$ directly controls the variance of this artificial noise [@problem_id:3288811].

#### The Role of Summary Statistics: Sufficiency and Information Loss

The second source of approximation is the use of [summary statistics](@entry_id:196779), which may not capture all the information about $\theta$ contained in the full dataset $x_{\text{obs}}$. The key concept here is that of **sufficiency**. A statistic $s(x)$ is sufficient for a parameter $\theta$ if the [conditional distribution](@entry_id:138367) of the data $x$ given $s(x)$ does not depend on $\theta$. Equivalently, by the Neyman-Fisher [factorization theorem](@entry_id:749213), $s(x)$ is sufficient if the likelihood can be factored as $p(x \mid \theta) = g(s(x), \theta) h(x)$ [@problem_id:3288740].

The choice of [summary statistics](@entry_id:196779) has a profound impact on the limiting behavior of ABC. As the tolerance $\epsilon \to 0$, the ABC posterior $\pi_{\epsilon}(\theta \mid s_{\text{obs}})$ converges to the posterior conditional on the [summary statistics](@entry_id:196779), $\pi(\theta \mid s_{\text{obs}}) \propto \pi(\theta) p(s_{\text{obs}} \mid \theta)$. This [limiting distribution](@entry_id:174797) is identical to the true posterior $\pi(\theta \mid x_{\text{obs}})$ if and only if $s(x)$ is a sufficient statistic [@problem_id:3288743, @problem_id:3288740].

If the [summary statistics](@entry_id:196779) are not sufficient, information is lost, and even with an infinitesimal tolerance ($\epsilon \to 0$), the ABC posterior will not converge to the true posterior. The resulting posterior based on the insufficient summary will typically be more dispersed (i.e., less certain) than the true posterior based on the full data [@problem_id:3288743].

For example, when estimating the mean $\theta$ of a normal distribution, the sample mean $\bar{x}$ is a [sufficient statistic](@entry_id:173645). Using it in ABC will yield the correct posterior in the limit $\epsilon \to 0$. In contrast, the [sample median](@entry_id:267994) is not sufficient for $n>2$. Using the [sample median](@entry_id:267994) will lead to a limiting posterior that is different from, and wider than, the true posterior, reflecting the information lost by discarding aspects of the data not captured by the median [@problem_id:3288740].

The concept of "information loss" can be made precise using **Fisher information**. For a single observation $X$ from an exponential distribution with rate $\theta$, $f(x|\theta) = \theta \exp(-\theta x)$, the Fisher information in the full data is $I_{\theta}(X) = 1/\theta^2$. If we instead use a simple summary statistic, such as the indicator $s(X) = \mathbf{1}\{X \le c\}$, which only tells us if the observation is above or below a threshold $c$, the Fisher information in this summary is $I_{\theta}(s(X)) = c^2 \exp(-\theta c) / (1 - \exp(-\theta c))$. The ratio $I_{\theta}(s(X))/I_{\theta}(X) = \theta^2 c^2 / (\exp(\theta c) - 1)$ quantifies the fraction of information retained. This ratio is always less than 1, providing a concrete measure of the information lost by summarization [@problem_id:3288754].

### Advanced Analysis and Algorithmic Improvements

The principles of ABC have direct consequences for its performance and have motivated the development of more sophisticated algorithms.

#### The Bias-Variance Trade-off

The ABC estimator for a posterior expectation, $\mathbb{E}[\varphi(\theta) \mid s_{\text{obs}}]$, can be written as a kernel-weighted average, which is a form of Nadaraya-Watson estimator. Its statistical properties are subject to a classic **bias-variance trade-off**. For a fixed number of simulations $M$, the Mean Squared Error (MSE) of the ABC estimate can be decomposed into two leading-order terms [@problem_id:3288819]:

$\text{MSE} \approx \underbrace{C_1 \epsilon^{4}}_{\text{Squared Bias}} + \underbrace{\frac{C_2}{M \epsilon^{d_s}}}_{\text{Variance}}$

where $d_s$ is the dimension of the [summary statistics](@entry_id:196779), and $C_1, C_2$ are constants related to the properties of the kernel and the underlying distributions. This decomposition reveals the fundamental dilemma in ABC:
- Decreasing the tolerance $\epsilon$ reduces the approximation bias (making the ABC posterior closer to the target posterior based on summaries), but it also inflates the variance of the estimator, because fewer simulated points fall within the acceptance region.
- Increasing the number of simulations $M$ reduces the variance, but at a direct computational cost.

This trade-off is crucial for practical applications. For a given total number of simulations $n$, one can seek an optimal tolerance schedule $\epsilon_n$ that balances these competing error sources. Theoretical analysis shows that to minimize the total error, which can be measured by distances like the Wasserstein distance, the optimal tolerance should decrease as a power of the number of simulations. Balancing the approximation bias (which scales as $\epsilon^{1/2}$ in Wasserstein distance) and the Monte Carlo error (which scales as $N^{-1/2}$, where $N \approx n \epsilon^{d_s}$ is the number of accepted samples) yields an optimal schedule of the form $\epsilon_n \propto n^{-1/(d_s+1)}$ [@problem_id:3288750]. This result demonstrates how a theoretical understanding of the error components can guide practical [algorithm design](@entry_id:634229).

#### Efficient ABC Algorithms

The basic rejection algorithm is notoriously inefficient, especially for small $\epsilon$. The acceptance probability is proportional to the volume of the acceptance region, which for $d_s$-dimensional summaries, typically scales as $\epsilon^{d_s}$. This "curse of dimensionality" means that for small $\epsilon$ or high-dimensional summaries, the number of simulations required to obtain a single accepted particle can be astronomical. This has motivated the development of more advanced, adaptive algorithms.

**Markov Chain Monte Carlo ABC (ABC-MCMC)**: Instead of sampling proposals from the prior, ABC-MCMC methods use a Markov chain to explore the [parameter space](@entry_id:178581), proposing new parameters based on the current state. This allows the sampler to focus on regions of high posterior probability, dramatically improving efficiency. A particularly important variant is the **pseudo-marginal ABC-MCMC** algorithm [@problem_id:3288820]. This method uses a Monte Carlo estimate $\widehat{L}_{\epsilon,R}(\theta)$ of the ABC likelihood, based on $R$ simulations, within the Metropolis-Hastings acceptance ratio. A key result is that as long as the likelihood estimator $\widehat{L}_{\epsilon,R}(\theta)$ is *unbiased* for the true ABC likelihood $L_{\epsilon}(\theta)$, the resulting MCMC algorithm will have the exact ABC posterior $\pi_{\epsilon}(\theta \mid s_{\text{obs}})$ as its stationary distribution. The efficiency of this algorithm, however, is highly sensitive to the variance of the likelihood estimator. A large variance in the estimator can cause the chain to become "stuck," leading to poor mixing. For random-walk proposals, it has been shown that the optimal efficiency is achieved when the number of simulations $R$ is chosen such that the variance of the [log-likelihood](@entry_id:273783) estimator, $\text{Var}[\ln \widehat{L}_{\epsilon,R}(\theta)]$, is approximately 1.

**Sequential Monte Carlo ABC (SMC-ABC)**: Another powerful approach is SMC-ABC, which propagates a weighted population of parameter samples, or "particles," through a sequence of decreasing tolerances, $\epsilon_1 > \epsilon_2 > \dots > \epsilon_T$ [@problem_id:3536601]. At each stage $t$, particles from the previous stage are resampled, perturbed (e.g., via a small random walk), and then subjected to the new, stricter tolerance $\epsilon_t$. Particles that meet the new criterion survive and are re-weighted to correct for the proposal mechanism. This sequential process gradually adapts the population of particles from the prior towards the final, high-precision ABC posterior. Its efficiency stems from the fact that proposals for a given stage are generated from a population that already satisfies the previous, looser tolerance, concentrating computational effort in regions of the [parameter space](@entry_id:178581) that are already known to be promising. This allows SMC-ABC to attain high acceptance rates even for very small final tolerances, making it vastly more efficient than simple rejection ABC. The importance weight for a particle $\theta_i^{(t)}$ proposed from the previous population via a perturbation kernel $K_t$ is given by importance sampling theory as $w_i^{(t)} \propto \pi(\theta_i^{(t)}) / q_t(\theta_i^{(t)})$, where $q_t$ is the mixture proposal distribution induced by [resampling](@entry_id:142583) and perturbation [@problem_id:3536601].

In summary, Approximate Bayesian Computation is not merely an ad-hoc simulation trick, but a principled framework for inference when likelihoods are intractable. Its properties are well-understood through its interpretation as exact inference on a perturbed model, and its performance is governed by a fundamental [bias-variance trade-off](@entry_id:141977). This understanding has spurred the development of advanced MCMC and SMC algorithms that make ABC a practical and powerful tool for modern scientific inquiry.