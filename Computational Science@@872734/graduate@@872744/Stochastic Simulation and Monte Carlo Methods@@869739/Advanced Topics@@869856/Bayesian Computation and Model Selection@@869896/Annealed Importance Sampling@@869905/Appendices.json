{"hands_on_practices": [{"introduction": "To build a solid foundation in Annealed Importance Sampling (AIS), we begin with an idealized, analytically tractable scenario. This practice explores the core mechanics of AIS by bridging two Gaussian distributions, assuming perfect mixing at each stage. Under this pedagogical assumption, the expected incremental weight is simply the ratio of normalization constants, allowing us to focus purely on the properties of the annealing path itself. By deriving this ratio from first principles and implementing it in code, you will gain a fundamental, hands-on understanding of how AIS weights are structured and computed. [@problem_id:3288040]", "problem": "Consider the following setting for Annealed Importance Sampling (AIS), formally introduced here as Annealed Importance Sampling (AIS) with a geometric path. Let the target energy be $E(x)=\\lambda x^2$ for $x\\in\\mathbb{R}$ and a fixed constant $\\lambda0$. Define the base distribution $b(x)$ to be a Gaussian with zero mean and variance $s^2$, that is $b(x)=\\frac{1}{\\sqrt{2\\pi}\\,s}\\exp\\left(-\\frac{x^2}{2s^2}\\right)$, and the target distribution $t(x)$ to be the normalized Gibbs distribution associated with the energy $E(x)$, given by $t(x)=\\sqrt{\\frac{\\lambda}{\\pi}}\\exp(-\\lambda x^2)$. Consider the geometric annealing path of intermediate unnormalized densities $u_\\beta(x)=b(x)^{1-\\beta}t(x)^{\\beta}$ parameterized by an inverse-temperature schedule $\\{\\beta_k\\}_{k=0}^K$ with $0=\\beta_0\\beta_1\\cdots\\beta_K=1$.\n\nStart from the foundational principle of importance sampling and normalization constants: for any unnormalized density $u(x)$ with normalization constant $Z=\\int_\\mathbb{R}u(x)\\,\\mathrm{d}x$, the normalized density is $p(x)=\\frac{u(x)}{Z}$. In AIS with a geometric path and perfect mixing at each stage, the expected incremental weight from stage $k$ to stage $k+1$ is equal to the ratio of normalization constants $Z_{\\beta_{k+1}}/Z_{\\beta_k}$, where $Z_\\beta=\\int_\\mathbb{R}u_\\beta(x)\\,\\mathrm{d}x$. Your task is to derive, from first principles, the normalization constant $Z_\\beta$ for the given $b(x)$ and $t(x)$, and then implement a program that, given $(\\lambda,s,\\{\\beta_k\\}_{k=0}^K)$, computes the expected incremental weight at each stage, defined as $\\frac{Z_{\\beta_{k+1}}}{Z_{\\beta_k}}$ for $k=0,1,\\dots,K-1$.\n\nThe derivation must begin from the definitions of $b(x)$, $t(x)$, the geometric path $u_\\beta(x)$, and the normalization constant $Z_\\beta$, and use only algebraic manipulation and well-tested facts about Gaussian integrals, avoiding any shortcut formulas not derived in your solution.\n\nYour program must process the following test suite of parameter sets, each provided as $(\\lambda,s,\\text{schedule})$:\n- Test case $1$: $(\\lambda,s,\\{\\beta_k\\})=\\left(0.7,1.5,\\{0.0,0.2,0.5,0.9,1.0\\}\\right)$.\n- Test case $2$ (base equals target boundary): $(\\lambda,s,\\{\\beta_k\\})=\\left(0.5,1.0,\\{0.0,0.3,0.6,0.9,1.0\\}\\right)$.\n- Test case $3$ (small energy scale and wide base): $(\\lambda,s,\\{\\beta_k\\})=\\left(10^{-6},100.0,\\{0.0,0.01,0.1,0.5,1.0\\}\\right)$.\n- Test case $4$ (sharp target and narrow base): $(\\lambda,s,\\{\\beta_k\\})=\\left(10.0,0.1,\\{0.0,0.4,0.8,1.0\\}\\right)$.\n\nFor each test case, compute the list $\\left[\\frac{Z_{\\beta_1}}{Z_{\\beta_0}},\\frac{Z_{\\beta_2}}{Z_{\\beta_1}},\\dots,\\frac{Z_{\\beta_K}}{Z_{\\beta_{K-1}}}\\right]$ of expected incremental weights. There are no physical units in this problem. Your program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets, with no spaces, where each inner list corresponds to one test case in the order listed above, for example $[[w_{1,1},w_{1,2}],[w_{2,1},\\dots]]$. Each $w$ must be a floating-point number in decimal representation. Your implementation must be a complete, runnable program.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard theoretical exercise in the field of computational statistics, specifically concerning Annealed Importance Sampling (AIS). All provided definitions and parameters are standard and sufficient for deriving a unique solution.\n\nThe task is to derive the normalization constant $Z_\\beta$ for an intermediate distribution in an AIS scheme and then use it to compute the expected incremental weights. The derivation proceeds from first principles as stipulated.\n\nLet the base distribution be a zero-mean Gaussian with variance $s^2$, with its probability density function (PDF) given by:\n$$\nb(x) = \\frac{1}{\\sqrt{2\\pi}s} \\exp\\left(-\\frac{x^2}{2s^2}\\right)\n$$\nThe target distribution is the normalized Gibbs distribution for the energy $E(x) = \\lambda x^2$ with $\\lambda  0$. The unnormalized Gibbs distribution is $\\exp(-E(x)) = \\exp(-\\lambda x^2)$. To normalize this, we compute the integral $\\int_{-\\infty}^{\\infty} \\exp(-\\lambda x^2) \\mathrm{d}x$. This is a standard Gaussian integral of the form $\\int_{-\\infty}^{\\infty} \\exp(-ax^2) \\mathrm{d}x = \\sqrt{\\pi/a}$, which for $a=\\lambda$ yields $\\sqrt{\\pi/\\lambda}$. Thus, the normalized target PDF is:\n$$\nt(x) = \\frac{\\exp(-\\lambda x^2)}{\\sqrt{\\pi/\\lambda}} = \\sqrt{\\frac{\\lambda}{\\pi}} \\exp(-\\lambda x^2)\n$$\nThis matches the provided expression for $t(x)$.\n\nThe geometric annealing path defines a sequence of unnormalized intermediate densities $u_\\beta(x)$ for a schedule $0 = \\beta_0  \\beta_1  \\dots  \\beta_K = 1$:\n$$\nu_\\beta(x) = b(x)^{1-\\beta} t(x)^\\beta\n$$\nOur first step is to substitute the expressions for $b(x)$ and $t(x)$ into this definition:\n$$\nu_\\beta(x) = \\left[ \\frac{1}{\\sqrt{2\\pi}s} \\exp\\left(-\\frac{x^2}{2s^2}\\right) \\right]^{1-\\beta} \\left[ \\sqrt{\\frac{\\lambda}{\\pi}} \\exp(-\\lambda x^2) \\right]^{\\beta}\n$$\nWe can separate the terms that are constant with respect to $x$ from the exponential terms involving $x$:\n$$\nu_\\beta(x) = \\left( (\\sqrt{2\\pi}s)^{-1} \\right)^{1-\\beta} \\left( (\\pi/\\lambda)^{-1/2} \\right)^{\\beta} \\times \\exp\\left(-\\frac{x^2}{2s^2}\\right)^{1-\\beta} \\exp(-\\lambda x^2)^{\\beta}\n$$\nLet's simplify the constant pre-factor and the exponent separately.\nThe constant part is:\n$$\nC_\\beta = (2\\pi s^2)^{-(1-\\beta)/2} (\\pi/\\lambda)^{-\\beta/2}\n$$\nThe exponential part is:\n$$\n\\exp\\left( -(1-\\beta)\\frac{x^2}{2s^2} - \\beta\\lambda x^2 \\right) = \\exp\\left( -x^2 \\left[ \\frac{1-\\beta}{2s^2} + \\beta\\lambda \\right] \\right)\n$$\nSo, the unnormalized density $u_\\beta(x)$ is an unnormalized Gaussian distribution of the form $u_\\beta(x) = C_\\beta \\exp(-A_\\beta x^2)$, where the coefficient $A_\\beta$ in the exponent is:\n$$\nA_\\beta = \\frac{1-\\beta}{2s^2} + \\beta\\lambda\n$$\nThe normalization constant $Z_\\beta$ is defined as the integral of $u_\\beta(x)$ over its domain $\\mathbb{R}$:\n$$\nZ_\\beta = \\int_{-\\infty}^{\\infty} u_\\beta(x) \\, \\mathrm{d}x = \\int_{-\\infty}^{\\infty} C_\\beta \\exp(-A_\\beta x^2) \\, \\mathrm{d}x = C_\\beta \\int_{-\\infty}^{\\infty} \\exp(-A_\\beta x^2) \\, \\mathrm{d}x\n$$\nSince $\\lambda  0$, $s^2  0$, and $\\beta \\in [0, 1]$, the coefficient $A_\\beta$ is always positive. This allows us to use the aforementioned Gaussian integral formula $\\int_{-\\infty}^{\\infty} e^{-ax^2} \\mathrm{d}x = \\sqrt{\\pi/a}$ with $a = A_\\beta$:\n$$\nZ_\\beta = C_\\beta \\sqrt{\\frac{\\pi}{A_\\beta}}\n$$\nSubstituting the expression for $C_\\beta$:\n$$\nZ_\\beta = (2\\pi s^2)^{-(1-\\beta)/2} (\\pi/\\lambda)^{-\\beta/2} \\pi^{1/2} A_\\beta^{-1/2}\n$$\nLet's simplify the powers of the constants:\n$$\nZ_\\beta = 2^{-(1-\\beta)/2} \\pi^{-(1-\\beta)/2} s^{-(1-\\beta)} \\pi^{-\\beta/2} \\lambda^{\\beta/2} \\pi^{1/2} A_\\beta^{-1/2}\n$$\n$$\nZ_\\beta = 2^{-(1-\\beta)/2} s^{-(1-\\beta)} \\lambda^{\\beta/2} \\pi^{(-1+\\beta-\\beta+1)/2} A_\\beta^{-1/2}\n$$\nThe exponent of $\\pi$ simplifies to $0$, so $\\pi^0=1$:\n$$\nZ_\\beta = 2^{-(1-\\beta)/2} s^{-(1-\\beta)} \\lambda^{\\beta/2} A_\\beta^{-1/2}\n$$\nNow, substituting the expression for $A_\\beta = \\frac{1-\\beta}{2s^2} + \\beta\\lambda = \\frac{1-\\beta + 2s^2\\beta\\lambda}{2s^2}$:\n$$\nZ_\\beta = 2^{-(1-\\beta)/2} s^{-(1-\\beta)} \\lambda^{\\beta/2} \\left( \\frac{1-\\beta + 2s^2\\beta\\lambda}{2s^2} \\right)^{-1/2}\n$$\n$$\nZ_\\beta = 2^{-(1-\\beta)/2} s^{-(1-\\beta)} \\lambda^{\\beta/2} \\left( \\frac{2s^2}{1-\\beta + 2s^2\\beta\\lambda} \\right)^{1/2}\n$$\n$$\nZ_\\beta = 2^{-1/2+\\beta/2} s^{-1+\\beta} \\lambda^{\\beta/2} \\cdot 2^{1/2} s^1 \\cdot (1-\\beta + 2s^2\\beta\\lambda)^{-1/2}\n$$\nCombining terms with the same base:\n$$\nZ_\\beta = 2^{\\beta/2} s^\\beta \\lambda^{\\beta/2} (1-\\beta + 2s^2\\beta\\lambda)^{-1/2}\n$$\n$$\nZ_\\beta = (2s^2\\lambda)^{\\beta/2} (1 - \\beta + (2s^2\\lambda)\\beta)^{-1/2}\n$$\nLet's define a constant $\\alpha = 2s^2\\lambda$. The expression for $Z_\\beta$ becomes markedly simpler:\n$$\nZ_\\beta = \\alpha^{\\beta/2} (1 - \\beta + \\alpha\\beta)^{-1/2} = \\sqrt{\\frac{\\alpha^\\beta}{1 + (\\alpha-1)\\beta}}\n$$\nWe verify this formula for the boundary cases. For $\\beta=0$, $u_0(x) = b(x)$, which is normalized, so $Z_0=1$. Our formula gives $Z_0 = \\sqrt{\\frac{\\alpha^0}{1+(\\alpha-1)0}} = \\sqrt{\\frac{1}{1}} = 1$. For $\\beta=1$, $u_1(x) = t(x)$, which is also normalized, so $Z_1=1$. Our formula gives $Z_1 = \\sqrt{\\frac{\\alpha^1}{1+(\\alpha-1)1}} = \\sqrt{\\frac{\\alpha}{1+\\alpha-1}} = \\sqrt{\\frac{\\alpha}{\\alpha}} = 1$. The formula is correct.\n\nThe expected incremental weight from stage $k$ to stage $k+1$ is the ratio of normalization constants $W_k = \\frac{Z_{\\beta_{k+1}}}{Z_{\\beta_k}}$. Using our derived expression for $Z_\\beta$:\n$$\nW_k = \\frac{\\sqrt{\\frac{\\alpha^{\\beta_{k+1}}}{1 + (\\alpha-1)\\beta_{k+1}}}}{\\sqrt{\\frac{\\alpha^{\\beta_k}}{1 + (\\alpha-1)\\beta_k}}} = \\sqrt{\\frac{\\alpha^{\\beta_{k+1}}}{1 + (\\alpha-1)\\beta_{k+1}} \\cdot \\frac{1 + (\\alpha-1)\\beta_k}{\\alpha^{\\beta_k}}}\n$$\n$$\nW_k = \\sqrt{\\alpha^{\\beta_{k+1}-\\beta_k} \\frac{1 + (\\alpha-1)\\beta_k}{1 + (\\alpha-1)\\beta_{k+1}}}\n$$\nThis is the final expression to be implemented. The term $\\alpha = 2s^2\\lambda$ is computed once per test case. For each step in the annealing schedule from $\\beta_k$ to $\\beta_{k+1}$, the weight $W_k$ is calculated using this formula.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the expected incremental weights for Annealed Importance Sampling (AIS)\n    with a geometric path between two Gaussian distributions.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lambda, s, schedule)\n        (0.7, 1.5, [0.0, 0.2, 0.5, 0.9, 1.0]),\n        (0.5, 1.0, [0.0, 0.3, 0.6, 0.9, 1.0]),\n        (1e-6, 100.0, [0.0, 0.01, 0.1, 0.5, 1.0]),\n        (10.0, 0.1, [0.0, 0.4, 0.8, 1.0]),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        lam, s, schedule = case\n        \n        # The derived formula depends on the constant alpha = 2 * s^2 * lambda.\n        alpha = 2.0 * s**2 * lam\n        \n        case_weights = []\n        # Iterate through the annealing schedule to compute each incremental weight.\n        for i in range(len(schedule) - 1):\n            beta_k = schedule[i]\n            beta_k_plus_1 = schedule[i+1]\n            \n            # The incremental weight from beta_k to beta_k_plus_1 is Z_{k+1}/Z_k.\n            # Handle the special case where alpha is 1, which means the base and\n            # target distributions are identical, so all weights are 1.\n            if np.isclose(alpha, 1.0):\n                weight = 1.0\n            else:\n                # Derived formula:\n                # W_k = sqrt(alpha^(beta_{k+1}-beta_k) * (1+(alpha-1)beta_k) / (1+(alpha-1)beta_{k+1}))\n                beta_diff = beta_k_plus_1 - beta_k\n                term1 = alpha**beta_diff\n                \n                numerator = 1.0 + (alpha - 1.0) * beta_k\n                denominator = 1.0 + (alpha - 1.0) * beta_k_plus_1\n                \n                # As derived, with alpha  0 and beta in [0, 1], the denominator is always positive.\n                term2 = numerator / denominator\n                \n                weight = np.sqrt(term1 * term2)\n            \n            case_weights.append(weight)\n        \n        all_results.append(case_weights)\n\n    # Final print statement in the exact required format:\n    # A single line, comma-separated list of lists, with no spaces.\n    # Example: [[w1,w2],[w3,w4,w5]]\n    inner_lists_str = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output = f\"[{','.join(inner_lists_str)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3288040"}, {"introduction": "Real-world applications of AIS often involve distributions that are more complex than simple Gaussians. This exercise confronts a common and critical challenge: sampling from a target distribution with \"heavy tails\" (polynomial decay) starting from a base with \"light tails\" (exponential decay). You will investigate how this mismatch in tail behavior can lead to catastrophic weight degeneracy, a failure mode where the variance of the importance weights becomes infinite. By deriving the conditions for this failure and exploring a more robust \"tail-aware\" annealing path, this practice will equip you with essential skills in diagnosing algorithmic instability and designing more effective sampling strategies. [@problem_id:3288124]", "problem": "You are to investigate the failure modes of Annealed Importance Sampling (AIS) when bridging from an exponentially light-tailed base distribution to a heavy-tailed target, and to design a tail-aware alternative. All mathematical definitions below use the standard setup of AIS with unnormalized densities.\n\nLet $d \\in \\mathbb{N}$ denote the dimension. Consider a base density $p_0(x) \\propto f_0(x)$ on $\\mathbb{R}^d$ and a target $p_1(x) \\propto f_1(x)$. AIS constructs a sequence of intermediate unnormalized densities $\\{f_k\\}_{k=0}^K$ where $f_0=f_0$ and $f_K=f_1$, with corresponding normalized densities $p_k(x) \\propto f_k(x)$. For a schedule $\\{\\beta_k\\}_{k=0}^K$ with $\\beta_0=0$, $\\beta_K=1$, $\\beta_k$ increasing, the standard geometric annealing is $f_k(x) = f_0(x)^{1-\\beta_k} f_1(x)^{\\beta_k}$. The AIS incremental weight from $k$ to $k+1$ is $r_{k \\to k+1}(x_k) = f_{k+1}(x_k)/f_k(x_k)$. Catastrophic weight degeneracy occurs when the second moment $\\mathbb{E}_{p_k}[r_{k \\to k+1}(X)^2]$ is infinite for some step $k$, causing the variance of importance weights to be infinite and rendering effective sample size arbitrarily close to zero in the limit.\n\nIn this problem, you will study the case of a heavy-tailed target\n$$\np_1(x) \\propto f_1(x) \\propto \\left(1 + \\lVert x \\rVert^2 \\right)^{-\\nu},\n$$\nwith parameter $\\nu > d/2$ to ensure normalizability. You will compare two AIS paths:\n\n- Standard geometric annealing from a Gaussian base:\n  $$\n  f_0(x) \\propto \\exp\\!\\left(-\\frac{\\lVert x \\rVert^2}{2}\\right), \\quad f_k(x) = f_0(x)^{1-\\beta_k} f_1(x)^{\\beta_k}, \\quad \\beta_k = \\frac{k}{K}.\n  $$\n- A tail-aware polynomial path that replaces the base with a heavy-tailed Student-like density:\n  $$\n  q_\\kappa(x) \\propto \\left(1 + \\lVert x \\rVert^2 \\right)^{-\\kappa}, \\quad \\kappa > d/2, \\quad \\tilde f_k(x) = q_\\kappa(x)^{1-\\beta_k} f_1(x)^{\\beta_k}, \\quad \\beta_k = \\frac{k}{K}.\n  $$\n\nYour tasks are purely mathematical and algorithmic:\n- Starting from the fundamental definitions of importance sampling moments, derive tail conditions for the finiteness of the second moment of the incremental weight at the final step $k=K-1$ for each path. For standard geometric annealing, show how the exponential factor causes divergence. For the tail-aware polynomial path, derive a condition involving $d$, $\\nu$, $\\kappa$, and $\\beta_{K-1}$ for finiteness.\n- Use these conditions to write a program that, for each specified test case, computes:\n  1. A boolean indicating whether the second moment of the final incremental weight under the standard geometric path is infinite.\n  2. A boolean indicating whether the second moment of the final incremental weight under the tail-aware polynomial path is infinite.\n  3. A nonnegative real-valued \"exponential divergence coefficient\" for the standard geometric path at the final step, defined as\n     $$\n     \\theta_{\\mathrm{geo}} := \\frac{a(\\beta_{K-1})}{2} - a(\\beta_{K}), \\quad a(\\beta) := 1 - \\beta,\n     $$\n     where $a(\\beta)$ parameterizes the Gaussian factor $\\exp(-a(\\beta)\\lVert x \\rVert^2/2)$ in the geometric path. Positive $\\theta_{\\mathrm{geo}}$ implies exponential explosion of the integrand and infinite second moment.\n  4. A real-valued \"polynomial integrability margin\" for the tail-aware path at the final step, defined as\n     $$\n     \\rho_{\\mathrm{poly}} := \\bigl(2 m(\\beta_K) - m(\\beta_{K-1})\\bigr) - \\frac{d}{2}, \\quad m(\\beta) := \\nu \\beta + \\kappa (1 - \\beta).\n     $$\n     A positive margin implies the integral of the squared incremental weight is finite; a nonpositive value signals divergence or borderline divergence.\n\nImplementation details:\n- Use the linear schedule $\\beta_k = k/K$ for a given integer $K \\ge 2$.\n- Use the following test suite, each specified as a tuple $(d,\\nu,\\kappa,K)$:\n  - Test case $1$: $(2, 1.1, 1.5, 10)$.\n  - Test case $2$: $(10, 6.0, 6.0, 20)$.\n  - Test case $3$: $(50, 26.0, 26.0, 30)$.\n- For each test case, compute and return a list with four entries in the order described above: $[\\text{geo\\_inf}, \\text{poly\\_inf}, \\theta_{\\mathrm{geo}}, \\rho_{\\mathrm{poly}}]$, where $\\text{geo\\_inf}$ and $\\text{poly\\_inf}$ are booleans and $\\theta_{\\mathrm{geo}}$, $\\rho_{\\mathrm{poly}}$ are floats.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the three per-test-case lists, enclosed in square brackets, for example\n$$\n[[\\text{geo\\_inf}_1, \\text{poly\\_inf}_1, \\theta_{\\mathrm{geo},1}, \\rho_{\\mathrm{poly},1}],[\\text{geo\\_inf}_2, \\text{poly\\_inf}_2, \\theta_{\\mathrm{geo},2}, \\rho_{\\mathrm{poly},2}],[\\text{geo\\_inf}_3, \\text{poly\\_inf}_3, \\theta_{\\mathrm{geo},3}, \\rho_{\\mathrm{poly},3}]].\n$$\nNo additional text should be printed. There are no physical units involved. All angles, if any, should be in radians. All numbers must be printed in standard decimal notation inherent to Python's default string conversion for floats and booleans.", "solution": "The problem requires an analysis of the failure modes of Annealed Importance Sampling (AIS) by examining the second moment of the final incremental importance weight. The finiteness of this moment is a crucial diagnostic for the stability and efficiency of the AIS estimator. We will analyze two annealing paths: the standard geometric path from a light-tailed Gaussian to a heavy-tailed target, and a tail-aware path from one heavy-tailed distribution to another.\n\nThe central quantity of interest is the second moment of the incremental weight update from an intermediate distribution $p_k$ to $p_{k+1}$, given by $\\mathbb{E}_{p_k}[r_{k \\to k+1}(X)^2]$. The incremental weight is defined as $r_{k \\to k+1}(x) = f_{k+1}(x)/f_k(x)$, where $p_j(x) \\propto f_j(x)$ for any $j$. The expectation is taken with respect to the normalized density $p_k(x) = f_k(x) / Z_k$, where $Z_k = \\int f_k(x) dx$.\n\nThe expression for the second moment is:\n$$\n\\mathbb{E}_{p_k}[r_{k \\to k+1}(X)^2] = \\int p_k(x) \\left(\\frac{f_{k+1}(x)}{f_k(x)}\\right)^2 dx = \\int \\frac{f_k(x)}{Z_k} \\frac{f_{k+1}(x)^2}{f_k(x)^2} dx = \\frac{1}{Z_k} \\int \\frac{f_{k+1}(x)^2}{f_k(x)} dx\n$$\nThe second moment is finite if and only if the integral $\\int \\frac{f_{k+1}(x)^2}{f_k(x)} dx$ is finite, as $Z_k$ is a finite normalization constant (assuming $p_k$ is a proper distribution). We are specifically interested in the final step of the annealing schedule, from $k=K-1$ to $k=K$. The integral to analyze is thus $\\int \\frac{f_K(x)^2}{f_{K-1}(x)} dx$.\n\nThe annealing schedule is given by $\\{\\beta_k\\}_{k=0}^K$ with $\\beta_0=0$ and $\\beta_K=1$. The intermediate densities are constructed as $f_k \\propto f_{\\text{base}}^{1-\\beta_k} f_{\\text{target}}^{\\beta_k}$. For the final step, $f_K(x) \\propto f_{\\text{base}}^{1-\\beta_K} f_{\\text{target}}^{\\beta_K} = f_{\\text{base}}^{0} f_{\\text{target}}^{1} = f_1(x)$. Therefore, the integral controlling the second moment simplifies to:\n$$\nI = \\int \\frac{f_1(x)^2}{f_{K-1}(x)} dx\n$$\n\nWe will now analyze this integral for the two specified paths.\n\n### Standard Geometric Annealing Path\nFor this path, the base and target unnormalized densities are:\n- Base: $f_0(x) \\propto \\exp(-\\frac{\\lVert x \\rVert^2}{2})$\n- Target: $f_1(x) \\propto (1 + \\lVert x \\rVert^2)^{-\\nu}$\n\nThe intermediate density at step $k=K-1$ is $f_{K-1}(x) = f_0(x)^{1-\\beta_{K-1}} f_1(x)^{\\beta_{K-1}}$.\nSubstituting this into the integral $I$:\n$$\nI_{\\text{geo}} = \\int \\frac{f_1(x)^2}{f_0(x)^{1-\\beta_{K-1}} f_1(x)^{\\beta_{K-1}}} dx = \\int f_0(x)^{-(1-\\beta_{K-1})} f_1(x)^{2-\\beta_{K-1}} dx\n$$\nNow, substituting the functional forms for $f_0$ and $f_1$:\n$$\nI_{\\text{geo}} \\propto \\int \\left( \\exp\\left(-\\frac{\\lVert x \\rVert^2}{2}\\right) \\right)^{-(1-\\beta_{K-1})} \\left( (1 + \\lVert x \\rVert^2)^{-\\nu} \\right)^{2-\\beta_{K-1}} dx\n$$\n$$\nI_{\\text{geo}} \\propto \\int \\exp\\left(\\frac{1-\\beta_{K-1}}{2} \\lVert x \\rVert^2\\right) (1 + \\lVert x \\rVert^2)^{-\\nu(2-\\beta_{K-1})} dx\n$$\nFor the integral to converge, the integrand must decay to zero sufficiently fast as $\\lVert x \\rVert \\to \\infty$. However, since $\\beta_k$ is an increasing schedule with $\\beta_K=1$, we have $\\beta_{K-1}  1$. This implies that the coefficient $1-\\beta_{K-1}$ in the exponential term is strictly positive. The term $\\exp(\\frac{1-\\beta_{K-1}}{2} \\lVert x \\rVert^2)$ grows exponentially as $\\lVert x \\rVert \\to \\infty$. This exponential growth will always dominate the polynomial decay of $(1 + \\lVert x \\rVert^2)^{-\\nu(2-\\beta_{K-1})}$. The integrand diverges, and thus the integral $I_{\\text{geo}}$ is infinite.\nThis demonstrates a catastrophic failure of standard AIS when bridging from a light-tailed (Gaussian) base to a heavy-tailed (Student-like) target. The second moment of the final incremental weight is always infinite. Thus, the boolean `geo_inf` is always `True`.\n\nThe \"exponential divergence coefficient\" is defined as $\\theta_{\\mathrm{geo}} := \\frac{a(\\beta_{K-1})}{2} - a(\\beta_{K})$. With $a(\\beta) = 1-\\beta$, we have $a(\\beta_{K-1})=1-\\beta_{K-1}$ and $a(\\beta_K)=a(1)=0$. So, $\\theta_{\\mathrm{geo}} = \\frac{1-\\beta_{K-1}}{2}$. This is precisely the positive coefficient of $\\lVert x \\rVert^2$ in the diverging exponential term, confirming its role in causing the integral to explode. For a linear schedule $\\beta_k = k/K$, we have $\\beta_{K-1}=(K-1)/K$, so $\\theta_{\\mathrm{geo}} = \\frac{1-(K-1)/K}{2} = \\frac{1/K}{2} = \\frac{1}{2K}$.\n\n### Tail-Aware Polynomial Path\nFor this path, the base and target unnormalized densities are:\n- Base: $q_\\kappa(x) \\propto (1 + \\lVert x \\rVert^2)^{-\\kappa}$\n- Target: $f_1(x) \\propto (1 + \\lVert x \\rVert^2)^{-\\nu}$\n\nThe intermediate density is $\\tilde f_{K-1}(x) = q_\\kappa(x)^{1-\\beta_{K-1}} f_1(x)^{\\beta_{K-1}}$.\nThe integral to analyze is:\n$$\nI_{\\text{poly}} = \\int \\frac{f_1(x)^2}{\\tilde f_{K-1}(x)} dx = \\int q_\\kappa(x)^{-(1-\\beta_{K-1})} f_1(x)^{2-\\beta_{K-1}} dx\n$$\nSubstituting the functional forms:\n$$\nI_{\\text{poly}} \\propto \\int \\left( (1 + \\lVert x \\rVert^2)^{-\\kappa} \\right)^{-(1-\\beta_{K-1})} \\left( (1 + \\lVert x \\rVert^2)^{-\\nu} \\right)^{2-\\beta_{K-1}} dx\n$$\n$$\nI_{\\text{poly}} \\propto \\int (1 + \\lVert x \\rVert^2)^{\\kappa(1-\\beta_{K-1})} (1 + \\lVert x \\rVert^2)^{-\\nu(2-\\beta_{K-1})} dx\n$$\n$$\nI_{\\text{poly}} \\propto \\int (1 + \\lVert x \\rVert^2)^{-\\left[\\nu(2-\\beta_{K-1}) - \\kappa(1-\\beta_{K-1})\\right]} dx\n$$\nThis is an integral of the form $\\int (1+r^2)^{-p} d^d x$ over $\\mathbb{R}^d$, where $r = \\lVert x \\rVert$. Using hyperspherical coordinates, the integral is finite if and only if the integrand decays sufficiently fast. The asymptotic behavior for large $r$ is $r^{-2p}$. The integral over $\\mathbb{R}^d$ behaves as $\\int^\\infty r^{-2p} r^{d-1} dr = \\int^\\infty r^{d-1-2p} dr$. This integral converges if and only if the exponent is less than $-1$, i.e., $d-1-2p  -1$, which simplifies to $2p  d$.\n\nThe exponent in our case is $p = \\nu(2-\\beta_{K-1}) - \\kappa(1-\\beta_{K-1})$. So, the condition for a finite second moment is:\n$$\n2\\left[\\nu(2-\\beta_{K-1}) - \\kappa(1-\\beta_{K-1})\\right]  d\n$$\nLet's relate this to the \"polynomial integrability margin\" $\\rho_{\\mathrm{poly}} := (2 m(\\beta_K) - m(\\beta_{K-1})) - d/2$, where $m(\\beta) := \\nu \\beta + \\kappa (1 - \\beta)$.\nLet's expand the term $2 m(\\beta_K) - m(\\beta_{K-1})$:\n- $m(\\beta_K) = m(1) = \\nu(1) + \\kappa(1-1) = \\nu$.\n- $m(\\beta_{K-1}) = \\nu \\beta_{K-1} + \\kappa (1 - \\beta_{K-1})$.\n- $2 m(\\beta_K) - m(\\beta_{K-1}) = 2\\nu - (\\nu \\beta_{K-1} + \\kappa(1-\\beta_{K-1})) = 2\\nu - \\nu\\beta_{K-1} - \\kappa + \\kappa\\beta_{K-1} = \\nu(2-\\beta_{K-1}) - \\kappa(1-\\beta_{K-1})$.\nThis is exactly the exponent $p$ from our derivation.\n\nThe convergence condition $2p  d$ can be rewritten as $p  d/2$. Substituting $p = 2 m(\\beta_K) - m(\\beta_{K-1})$, the condition becomes:\n$$\n(2 m(\\beta_K) - m(\\beta_{K-1}))  \\frac{d}{2} \\iff (2 m(\\beta_K) - m(\\beta_{K-1})) - \\frac{d}{2}  0\n$$\nThis is precisely the condition $\\rho_{\\mathrm{poly}}  0$. Therefore, the second moment of the incremental weight is finite if and only if the polynomial integrability margin is positive. Divergence occurs if $\\rho_{\\mathrm{poly}} \\le 0$. The boolean `poly_inf` is `True` if $\\rho_{\\mathrm{poly}} \\le 0$ and `False` otherwise.\n\nWith these derivations, we can now implement a program to compute the required values for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes AIS failure diagnostics for given test cases.\n\n    For each test case (d, nu, kappa, K), this function calculates four values:\n    1. geo_inf: Boolean, True if the second moment of the final incremental weight\n                under the standard geometric path is infinite.\n    2. poly_inf: Boolean, True if the second moment of the final incremental weight\n                 under the tail-aware polynomial path is infinite.\n    3. theta_geo: The exponential divergence coefficient for the geometric path.\n    4. rho_poly: The polynomial integrability margin for the polynomial path.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (d, nu, kappa, K).\n    test_cases = [\n        (2, 1.1, 1.5, 10),\n        (10, 6.0, 6.0, 20),\n        (50, 26.0, 26.0, 30),\n    ]\n\n    results = []\n    for case in test_cases:\n        d, nu, kappa, K = case\n\n        # --- Standard Geometric Path Analysis ---\n        \n        # As derived, bridging a light-tailed exponential base to a heavy-tailed\n        # polynomial target always results in an infinite second moment for the\n        # final importance weight. The integrand contains a term exp(c*||x||^2)\n        # where c  0, which causes divergence.\n        geo_inf = True\n\n        # Calculate the exponential divergence coefficient theta_geo.\n        # beta_k = k / K. beta_{K-1} = (K-1)/K.\n        # theta_geo = (1 - beta_{K-1}) / 2 = (1 - (K-1)/K) / 2 = (1/K) / 2 = 1 / (2*K).\n        theta_geo = 1.0 / (2.0 * K)\n\n        # --- Tail-Aware Polynomial Path Analysis ---\n\n        # Calculate the polynomial integrability margin rho_poly.\n        # beta_K = 1\n        # beta_{K-1} = (K-1)/K\n        # m(beta) = nu*beta + kappa*(1-beta)\n        # m(beta_K) = nu\n        # m(beta_{K-1}) = nu * (K-1)/K + kappa * (1 - (K-1)/K) = nu * (K-1)/K + kappa/K\n        # rho_poly = (2*m(beta_K) - m(beta_{K-1})) - d/2\n        #          = (2*nu - (nu*(K-1)/K + kappa/K)) - d/2\n        #          = nu*(2 - (K-1)/K) - kappa/K - d/2\n        #          = nu*( (2K - K + 1)/K ) - kappa/K - d/2\n        #          = nu*(K+1)/K - kappa/K - d/2\n        \n        rho_poly = nu * (K + 1.0) / K - kappa / K - d / 2.0\n        \n        # The second moment is infinite if and only if rho_poly is non-positive.\n        poly_inf = rho_poly = 0\n\n        # Append the results for the current test case.\n        results.append([geo_inf, poly_inf, theta_geo, rho_poly])\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to its string representation and join them.\n    # Python's default string conversion for bools and floats is used as required.\n    formatted_results = [f\"[{res[0]},{res[1]},{res[2]},{res[3]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3288124"}, {"introduction": "Once you have a robust annealing path, the next practical question is how to use it efficiently. This hands-on theoretical exercise guides you through an analysis of a crucial trade-off: allocating a fixed computational budget between taking more intermediate steps ($T$) to reduce the variance of each path, or running more independent paths ($N$) to reduce the overall sampling error. By modeling the Mean Squared Error (MSE) as a function of these parameters, you will determine the optimal strategy for setting the number of annealing steps. This practice moves beyond implementation to the vital skill of algorithmic analysis, providing insight into how to tune complex Monte Carlo methods for maximum efficiency. [@problem_id:3288053]", "problem": "Consider Annealed Importance Sampling (AIS) as a method to estimate the ratio of normalizing constants between a tractable base density and an intractable target density. Let $\\tilde{p}_{\\lambda}(x) = \\exp(-U_{\\lambda}(x))$ denote a smooth family of unnormalized densities indexed by an annealing parameter $\\lambda \\in [0,1]$, with corresponding normalized densities $p_{\\lambda}(x) = \\tilde{p}_{\\lambda}(x) / Z_{\\lambda}$, where $Z_{\\lambda}$ is the normalizing constant at $\\lambda$. AIS constructs a sequence of intermediate distributions by choosing a schedule $\\lambda_{t} = t/T$ for $t = 0,1,\\dots,T$, and applies one step of a Markov chain Monte Carlo (MCMC) transition kernel $K_{\\lambda_{t}}$ that leaves $p_{\\lambda_{t}}$ invariant at each intermediate distribution. The AIS weight $W$ for a single trajectory is the product of incremental ratios of unnormalized densities across the schedule, and the AIS estimator for the ratio $R = Z_{1} / Z_{0}$ is the sample mean $\\hat{R} = \\frac{1}{N} \\sum_{i=1}^{N} W^{(i)}$ over $N$ independent AIS trajectories.\n\nAssume the following modeling assumptions for an advanced asymptotic analysis appropriate to small annealing step size and one MCMC transition per intermediate distribution:\n- The logarithm of the AIS weight, denoted $\\ell = \\ln W$, satisfies a Central Limit Theorem (CLT) across intermediate steps with asymptotic variance $v(T)$ obeying $v(T) = \\Theta / T$, where the constant $\\Theta  0$ is given by\n$$\n\\Theta \\equiv \\int_{0}^{1} \\operatorname{Var}_{p_{\\lambda}}\\!\\left( \\partial_{\\lambda} \\ln \\tilde{p}_{\\lambda}(X) \\right) \\, \\tau_{\\mathrm{int}}(\\lambda) \\, d\\lambda,\n$$\nand $\\tau_{\\mathrm{int}}(\\lambda)$ is the integrated autocorrelation time of the single-step MCMC kernel $K_{\\lambda}$ as it propagates the relevant score-like observable $\\partial_{\\lambda} \\ln \\tilde{p}_{\\lambda}(X)$.\n- The distribution of $\\ell$ is well-approximated by a normal distribution with mean chosen so that $\\mathbb{E}[W] = R$, ensuring the unbiasedness of $\\hat{R}$.\n- The total computational budget is fixed at $B  0$ units, and the cost per AIS trajectory is $c_{0} + c_{1} T$, where $c_{0}  0$ captures trajectory-independent overhead (for example, initialization or base density sampling) and $c_{1}  0$ is the per-intermediate-distribution cost (for example, a single MCMC transition and density evaluation). Under this budget model, the number of trajectories is $N = B / (c_{0} + c_{1} T)$, ignoring integer rounding.\n\nStarting from these assumptions and using only fundamental probabilistic definitions and the log-normal approximation implied by the Central Limit Theorem, do the following:\n1. Derive an explicit expression for the mean squared error (MSE) $\\operatorname{MSE}(T) = \\operatorname{Var}(\\hat{R})$ of the AIS estimator as a function of $T$, $R$, $\\Theta$, $B$, $c_{0}$, and $c_{1}$.\n2. Differentiate your expression with respect to $T$ and determine a condition on the parameters under which increasing the number of intermediate distributions $T$ reduces the MSE. Your condition must be derived from first principles within this model and must identify the mathematical structure responsible for the monotonic improvement or lack thereof when $T$ increases.\n3. Under the same assumptions, compute the limiting value of $\\operatorname{MSE}(T)$ as $T \\to \\infty$, and express this limit in closed form. This limiting value represents the minimal achievable MSE under arbitrarily fine annealing schedules for the given fixed budget model.\n\nYour final answer must be a single closed-form analytic expression for the limiting minimal mean squared error derived in part 3. No rounding is required and no physical units are involved.", "solution": "The problem is first validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Densities**: $\\tilde{p}_{\\lambda}(x) = \\exp(-U_{\\lambda}(x))$, $p_{\\lambda}(x) = \\tilde{p}_{\\lambda}(x) / Z_{\\lambda}$, for $\\lambda \\in [0,1]$.\n- **Target Ratio**: $R = Z_{1} / Z_{0}$.\n- **Annealing Schedule**: $\\lambda_{t} = t/T$ for $t = 0,1,\\dots,T$.\n- **MCMC Kernel**: $K_{\\lambda_{t}}$ leaves $p_{\\lambda_{t}}$ invariant; one step is applied per intermediate distribution.\n- **AIS Estimator**: $\\hat{R} = \\frac{1}{N} \\sum_{i=1}^{N} W^{(i)}$, where $W^{(i)}$ are i.i.d. AIS weights.\n- **Asymptotic Assumptions**:\n    1. The log-weight $\\ell = \\ln W$ follows a Central Limit Theorem.\n    2. The variance of the log-weight is $v(T) = \\operatorname{Var}(\\ell) = \\Theta / T$, where $\\Theta \\equiv \\int_{0}^{1} \\operatorname{Var}_{p_{\\lambda}}\\!\\left( \\partial_{\\lambda} \\ln \\tilde{p}_{\\lambda}(X) \\right) \\, \\tau_{\\mathrm{int}}(\\lambda) \\, d\\lambda  0$.\n    3. The distribution of $\\ell$ is approximated by a normal distribution, $\\ell \\sim \\mathcal{N}(\\mu, \\sigma^2)$, with $\\sigma^2 = \\Theta/T$.\n    4. The estimator is unbiased: $\\mathbb{E}[W] = R$.\n- **Cost Model**:\n    1. Total budget is a fixed constant $B  0$.\n    2. Cost per trajectory is $c_{0} + c_{1} T$, with $c_{0}  0$ and $c_{1}  0$.\n    3. Number of trajectories is $N = B / (c_{0} + c_{1} T)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is well-grounded in the theory of advanced Monte Carlo methods. The assumptions about the CLT for the log-weight and the form of the asymptotic variance $\\Theta/T$ are standard results in the literature on the analysis of AIS and similar path-sampling algorithms.\n- **Well-Posed**: The problem is well-posed, providing all necessary definitions and relationships between variables to derive the quantities requested. It asks for a derivation, a condition, and a limit, all of which are standard mathematical tasks under the given model.\n- **Objective**: The problem is stated in precise, objective mathematical language.\n- **Consistency and Completeness**: The problem is self-contained and its constraints are consistent. The positivity of all constants ($B, \\Theta, c_0, c_1$) is physically meaningful in the context of budgets, costs, and statistical variance. The model is a standard simplified framework for analyzing algorithm efficiency.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution Derivations\n\nThis solution will proceed in three parts as requested by the problem statement.\n\n**Part 1: Derivation of the Mean Squared Error (MSE)**\n\nThe estimator for the ratio $R$ is given by $\\hat{R} = \\frac{1}{N} \\sum_{i=1}^{N} W^{(i)}$, where the weights $W^{(i)}$ are independent and identically distributed (i.i.d.) random variables. The problem states that the estimator is unbiased, meaning $\\mathbb{E}[\\hat{R}] = R$. This is consistent with the provided assumption $\\mathbb{E}[W] = R$.\n\nFor an unbiased estimator, the Mean Squared Error ($\\operatorname{MSE}$) is equal to its variance:\n$$\n\\operatorname{MSE}(T) = \\mathbb{E}[(\\hat{R} - R)^2] = \\operatorname{Var}(\\hat{R})\n$$\nSince the $W^{(i)}$ are i.i.d., the variance of their sample mean is:\n$$\n\\operatorname{Var}(\\hat{R}) = \\operatorname{Var}\\left(\\frac{1}{N} \\sum_{i=1}^{N} W^{(i)}\\right) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\operatorname{Var}(W^{(i)}) = \\frac{N \\operatorname{Var}(W)}{N^2} = \\frac{\\operatorname{Var}(W)}{N}\n$$\nThe next step is to find an expression for $\\operatorname{Var}(W)$. We are given that the logarithm of the weight, $\\ell = \\ln W$, is approximately normally distributed: $\\ell \\sim \\mathcal{N}(\\mu, \\sigma^2)$. This implies that $W = \\exp(\\ell)$ follows a log-normal distribution. The variance of this distribution is $\\sigma^2 = \\Theta/T$.\n\nThe mean and variance of a log-normal random variable $W = \\exp(\\ell)$ are given by:\n$$\n\\mathbb{E}[W] = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\n$$\n$$\n\\operatorname{Var}(W) = \\left(\\exp(\\sigma^2) - 1\\right) \\exp(2\\mu + \\sigma^2) = \\left(\\exp(\\sigma^2) - 1\\right) \\left(\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\right)^2\n$$\nUsing the given condition $\\mathbb{E}[W] = R$, we can write:\n$$\n\\operatorname{Var}(W) = (\\exp(\\sigma^2) - 1) (\\mathbb{E}[W])^2 = R^2 (\\exp(\\sigma^2) - 1)\n$$\nSubstituting $\\sigma^2 = \\Theta/T$, we get the variance of a single AIS weight:\n$$\n\\operatorname{Var}(W) = R^2 \\left(\\exp\\left(\\frac{\\Theta}{T}\\right) - 1\\right)\n$$\nNow, we can assemble the expression for $\\operatorname{MSE}(T)$. We substitute $\\operatorname{Var}(W)$ and the given expression for the number of trajectories, $N = B / (c_{0} + c_{1} T)$, into the formula $\\operatorname{MSE}(T) = \\operatorname{Var}(W)/N$:\n$$\n\\operatorname{MSE}(T) = \\frac{R^2 \\left(\\exp\\left(\\frac{\\Theta}{T}\\right) - 1\\right)}{B / (c_{0} + c_{1} T)} = \\frac{R^2 (c_{0} + c_{1} T)}{B} \\left(\\exp\\left(\\frac{\\Theta}{T}\\right) - 1\\right)\n$$\nThis is the explicit expression for the MSE as a function of the model parameters.\n\n**Part 2: Condition for MSE Reduction with Increasing T**\n\nTo determine the condition under which increasing $T$ reduces the MSE, we must analyze the sign of the derivative $\\frac{d}{dT} \\operatorname{MSE}(T)$. The factor $R^2/B$ is a positive constant, so we only need to analyze the sign of the derivative of the function $f(T) = (c_{0} + c_{1} T) \\left(\\exp\\left(\\frac{\\Theta}{T}\\right) - 1\\right)$.\n\nUsing the product rule for differentiation, $(uv)' = u'v + uv'$:\n$$\n\\frac{d f}{d T} = c_{1} \\left(\\exp\\left(\\frac{\\Theta}{T}\\right) - 1\\right) + (c_{0} + c_{1} T) \\left(\\exp\\left(\\frac{\\Theta}{T}\\right) \\cdot \\left(-\\frac{\\Theta}{T^2}\\right)\\right)\n$$\n$$\n\\frac{d f}{d T} = c_{1} \\exp\\left(\\frac{\\Theta}{T}\\right) - c_{1} - \\left(\\frac{c_{0}\\Theta}{T^2} + \\frac{c_{1}\\Theta}{T}\\right) \\exp\\left(\\frac{\\Theta}{T}\\right)\n$$\n$$\n\\frac{d f}{d T} = \\exp\\left(\\frac{\\Theta}{T}\\right) \\left(c_{1} - \\frac{c_{1}\\Theta}{T} - \\frac{c_{0}\\Theta}{T^2}\\right) - c_{1}\n$$\nWe want to find when $\\frac{d f}{d T}  0$. Let $y = \\Theta/T$. Since $T0$ and $\\Theta0$, we have $y  0$. The derivative can be written as a function of $y$:\n$$\ng(y) = \\exp(y) \\left(c_{1} - c_{1}y - \\frac{c_{0}}{\\Theta}y^2\\right) - c_{1}\n$$\nThe MSE decreases with $T$ if and only if $g(y)  0$ for $y0$.\nLet's analyze the function $g(y)$. First, evaluate it at the boundary of its domain, $y=0$:\n$$\ng(0) = \\exp(0) (c_{1} - 0 - 0) - c_{1} = c_{1} - c_{1} = 0\n$$\nNext, let's examine its derivative, $g'(y)$:\n$$\ng'(y) = \\frac{d}{dy} \\left[\\exp(y) \\left(c_{1} - c_{1}y - \\frac{c_{0}}{\\Theta}y^2\\right) - c_{1}\\right]\n$$\n$$\ng'(y) = \\exp(y)\\left(c_{1} - c_{1}y - \\frac{c_{0}}{\\Theta}y^2\\right) + \\exp(y)\\left(-c_{1} - \\frac{2c_{0}}{\\Theta}y\\right)\n$$\n$$\ng'(y) = \\exp(y) \\left[ \\left(c_{1} - c_{1}y - \\frac{c_{0}}{\\Theta}y^2\\right) + \\left(-c_{1} - \\frac{2c_{0}}{\\Theta}y\\right) \\right]\n$$\n$$\ng'(y) = \\exp(y) \\left[ -c_{1}y - \\frac{3c_{0}}{\\Theta}y - \\frac{c_{0}}{\\Theta}y^2 \\right] = -y\\exp(y) \\left[ c_{1} + \\frac{3c_{0}}{\\Theta} + \\frac{c_{0}}{\\Theta}y \\right]\n$$\nAs per the problem statement, the parameters $c_{0}$, $c_{1}$, and $\\Theta$ are all positive. For $y  0$, the term $y\\exp(y)$ is positive, and the term in the square brackets, $\\left[ c_{1} + \\frac{3c_{0}}{\\Theta} + \\frac{c_{0}}{\\Theta}y \\right]$, is also a sum of positive numbers, hence positive. Therefore, $g'(y)$ is the product of a leading negative sign and three positive terms, which means $g'(y)  0$ for all $y  0$.\n\nSince $g(0)=0$ and $g(y)$ is a strictly decreasing function for $y0$, it follows that $g(y)  0$ for all $y0$. This implies that $\\frac{d}{dT} \\operatorname{MSE}(T)  0$ for all $T0$.\nThus, increasing the number of intermediate distributions $T$ always reduces the MSE under this model. The only condition required is that the parameters $c_0, c_1, \\Theta$ are positive, as specified in the problem statement. The mathematical structure responsible for this monotonic improvement is that the function $g(y)$, which governs the sign of the derivative, is zero at the origin ($T \\to \\infty$) and strictly decreases for all positive $y$ (finite $T$). This arises from the specific interplay between the linear growth of cost with $T$ and the exponential-like decay of variance, where the variance reduction effect always outweighs the increased cost per trajectory.\n\n**Part 3: Limiting Value of the MSE as T approaches infinity**\n\nWe need to compute the limit of the MSE as $T \\to \\infty$.\n$$\n\\lim_{T\\to\\infty} \\operatorname{MSE}(T) = \\lim_{T\\to\\infty} \\frac{R^2 (c_{0} + c_{1} T)}{B} \\left(\\exp\\left(\\frac{\\Theta}{T}\\right) - 1\\right)\n$$\nThis limit is of the indeterminate form $\\infty \\cdot 0$. We can resolve it by rearranging the expression and using the standard limit definition of a derivative or a Taylor expansion.\nLet's use the Taylor expansion for $\\exp(x)$ around $x=0$. For small $x$, $\\exp(x) \\approx 1 + x$. As $T \\to \\infty$, $\\Theta/T \\to 0$, so we can write:\n$$\n\\exp\\left(\\frac{\\Theta}{T}\\right) - 1 = \\left(1 + \\frac{\\Theta}{T} + O\\left(\\frac{1}{T^2}\\right)\\right) - 1 = \\frac{\\Theta}{T} + O\\left(\\frac{1}{T^2}\\right)\n$$\nSubstituting this into the expression for MSE:\n$$\n\\operatorname{MSE}(T) \\approx \\frac{R^2 (c_{0} + c_{1} T)}{B} \\left(\\frac{\\Theta}{T}\\right) = \\frac{R^2 \\Theta}{B} \\left(\\frac{c_{0}}{T} + c_{1}\\right)\n$$\nTaking the limit as $T \\to \\infty$:\n$$\n\\lim_{T\\to\\infty} \\operatorname{MSE}(T) = \\lim_{T\\to\\infty} \\frac{R^2 \\Theta}{B} \\left(\\frac{c_{0}}{T} + c_{1}\\right)\n$$\nAs $T \\to \\infty$, the term $c_{0}/T$ goes to $0$. The limit is therefore:\n$$\n\\lim_{T\\to\\infty} \\operatorname{MSE}(T) = \\frac{R^2 \\Theta c_{1}}{B}\n$$\nThis is the minimal achievable MSE under an arbitrarily fine annealing schedule for the given fixed budget model. The overhead cost $c_0$ does not appear in the limit, as it becomes negligible compared to the $c_1 T$ term for large $T$.", "answer": "$$\\boxed{\\frac{R^{2} c_{1} \\Theta}{B}}$$", "id": "3288053"}]}