{"hands_on_practices": [{"introduction": "Understanding marginal likelihood begins with a firm grasp of its analytical definition. This exercise guides you through a first-principles derivation in a conjugate Normal-Normal model, a setting where the integral is tractable. By calculating the Bayes factor directly, you will gain a concrete intuition for how the evidence weighs data-prior agreement against model complexity, a core concept in Bayesian model selection [@problem_id:3319160].", "problem": "Consider a single-parameter normal likelihood with known variance. You observe $n$ independent and identically distributed (i.i.d.) samples $y_{1},\\dots,y_{n}$ from the sampling model $y_{i} \\mid \\mu \\sim \\mathcal{N}(\\mu,\\sigma^{2})$ with known $\\sigma^{2}  0$. Two Bayesian models are posited for the unknown mean $\\mu$:\n- Model $\\mathcal{M}_{1}$: $\\mu \\sim \\mathcal{N}(m_{0},\\tau_{1}^{2})$ with $\\tau_{1}^{2}  0$,\n- Model $\\mathcal{M}_{2}$: $\\mu \\sim \\mathcal{N}(m_{0},\\tau_{2}^{2})$ with $\\tau_{2}^{2}  0$,\n\nwhere $m_{0} \\in \\mathbb{R}$ is a common prior mean and $\\tau_{1}^{2} \\neq \\tau_{2}^{2}$ are distinct prior variances. Let $\\bar{y} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} y_{i}$ denote the sample mean and let $R \\equiv \\sum_{i=1}^{n} (y_{i}-\\bar{y})^{2}$ denote the within-sample sum of squares. The Bayes factor (BF) in favor of $\\mathcal{M}_{1}$ over $\\mathcal{M}_{2}$ is defined as the ratio of marginal likelihoods,\n$$\nBF_{12} \\equiv \\frac{p(y_{1},\\dots,y_{n} \\mid \\mathcal{M}_{1})}{p(y_{1},\\dots,y_{n} \\mid \\mathcal{M}_{2})},\n$$\nwhere, for each model $\\mathcal{M}_{j}$ with $j \\in \\{1,2\\}$, the marginal likelihood $p(y_{1},\\dots,y_{n} \\mid \\mathcal{M}_{j})$ is given by the integral of the likelihood with respect to the prior.\n\nStarting only from the definition of marginal likelihood as an integral and properties of Gaussian densities, do the following:\n- Derive a closed-form expression for $p(y_{1},\\dots,y_{n} \\mid \\mathcal{M}_{j})$ by analytically integrating out $\\mu$ under each model, expressing your result in terms of $n$, $\\sigma^{2}$, $\\tau_{j}^{2}$, $m_{0}$, $\\bar{y}$, and $R$.\n- Using your result, compute $BF_{12}$ in closed form and simplify it as much as possible.\n\nThen, assess how $BF_{12}$ changes as the prior variances $\\tau_{1}^{2}$ and $\\tau_{2}^{2}$ vary by analyzing the sign of the partial derivatives of $\\ln BF_{12}$ with respect to $\\tau_{1}^{2}$ and $\\tau_{2}^{2}$. Explain briefly the roles of the sample size $n$, the known variance $\\sigma^{2}$, and the discrepancy $|\\bar{y}-m_{0}|$ in this sensitivity.\n\nAnswer form requirement: Report only the final simplified analytic expression for $BF_{12}$. Do not approximate. Your final boxed answer must be a single, closed-form analytic expression with no units and no additional commentary.", "solution": "The problem requires the derivation of the Bayes factor, $BF_{12}$, for comparing two models, $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$, which differ only in their prior variance for a parameter $\\mu$. This involves first finding a closed-form expression for the marginal likelihood $p(y_{1},\\dots,y_{n} \\mid \\mathcal{M}_{j})$ for a generic model $\\mathcal{M}_{j}$.\n\n### Step 1: Derivation of the Marginal Likelihood $p(y_{1},\\dots,y_{n} \\mid \\mathcal{M}_{j})$\n\nThe marginal likelihood for model $\\mathcal{M}_{j}$ is defined by the integral of the likelihood function multiplied by the prior probability density of the parameter $\\mu$:\n$$ p(y_{1},\\dots,y_{n} \\mid \\mathcal{M}_{j}) = \\int_{-\\infty}^{\\infty} p(y_{1},\\dots,y_{n} \\mid \\mu, \\sigma^{2}) p(\\mu \\mid \\mathcal{M}_{j}) d\\mu $$\nLet the data be denoted by the vector $\\mathbf{y} = (y_{1},\\dots,y_{n})$.\n\nThe likelihood function $p(\\mathbf{y} \\mid \\mu, \\sigma^{2})$ is given by the product of $n$ independent Gaussian densities, as the samples are i.i.d. from $\\mathcal{N}(\\mu, \\sigma^{2})$:\n$$ p(\\mathbf{y} \\mid \\mu, \\sigma^{2}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(y_{i} - \\mu)^{2}}{2\\sigma^{2}}\\right) = (2\\pi\\sigma^{2})^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2}\\right) $$\nThe sum of squares in the exponent can be decomposed using the sample mean $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_{i}$ and the within-sample sum of squares $R = \\sum_{i=1}^{n} (y_{i}-\\bar{y})^{2}$:\n$$ \\sum_{i=1}^{n} (y_{i} - \\mu)^{2} = \\sum_{i=1}^{n} ((y_{i} - \\bar{y}) + (\\bar{y} - \\mu))^{2} = \\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2} + n(\\bar{y} - \\mu)^{2} = R + n(\\bar{y} - \\mu)^{2} $$\nSubstituting this back into the likelihood function gives:\n$$ p(\\mathbf{y} \\mid \\mu, \\sigma^{2}) = (2\\pi\\sigma^{2})^{-n/2} \\exp\\left(-\\frac{R}{2\\sigma^{2}}\\right) \\exp\\left(-\\frac{n(\\mu - \\bar{y})^{2}}{2\\sigma^{2}}\\right) $$\nThe prior on $\\mu$ for model $\\mathcal{M}_{j}$ is $p(\\mu \\mid \\mathcal{M}_{j}) \\sim \\mathcal{N}(m_{0}, \\tau_{j}^{2})$:\n$$ p(\\mu \\mid \\mathcal{M}_{j}) = \\frac{1}{\\sqrt{2\\pi\\tau_{j}^{2}}} \\exp\\left(-\\frac{(\\mu - m_{0})^{2}}{2\\tau_{j}^{2}}\\right) $$\nNow, we can write the integral for the marginal likelihood. We can factor out terms that do not depend on $\\mu$:\n$$ p(\\mathbf{y} \\mid \\mathcal{M}_{j}) = (2\\pi\\sigma^{2})^{-n/2} (2\\pi\\tau_{j}^{2})^{-1/2} \\exp\\left(-\\frac{R}{2\\sigma^{2}}\\right) \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{n(\\mu - \\bar{y})^{2}}{2\\sigma^{2}} - \\frac{(\\mu - m_{0})^{2}}{2\\tau_{j}^{2}}\\right) d\\mu $$\nTo solve the integral, we complete the square in $\\mu$ in the exponent. Let the argument of the exponent be $-\\frac{1}{2}Q(\\mu)$:\n$$ Q(\\mu) = \\frac{n(\\mu - \\bar{y})^{2}}{\\sigma^{2}} + \\frac{(\\mu - m_{0})^{2}}{\\tau_{j}^{2}} $$\nExpanding the squares:\n$$ Q(\\mu) = \\mu^{2}\\left(\\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau_{j}^{2}}\\right) - 2\\mu\\left(\\frac{n\\bar{y}}{\\sigma^{2}} + \\frac{m_{0}}{\\tau_{j}^{2}}\\right) + \\left(\\frac{n\\bar{y}^{2}}{\\sigma^{2}} + \\frac{m_{0}^{2}}{\\tau_{j}^{2}}\\right) $$\nThis is a quadratic in $\\mu$ of the form $A\\mu^{2} - 2B\\mu + C$. We can write $A(\\mu - B/A)^{2} - B^{2}/A + C$.\nThe precision of the posterior of $\\mu$ is $A = \\frac{1}{\\tau_{jn}^{2}} = \\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau_{j}^{2}} = \\frac{n\\tau_{j}^{2} + \\sigma^{2}}{\\sigma^{2}\\tau_{j}^{2}}$.\nThe posterior mean is $m_{jn} = B/A = \\left(\\frac{n\\bar{y}}{\\sigma^{2}} + \\frac{m_{0}}{\\tau_{j}^{2}}\\right) \\tau_{jn}^{2} = \\frac{n\\tau_{j}^{2}\\bar{y} + \\sigma^{2}m_{0}}{n\\tau_{j}^{2} + \\sigma^{2}}$.\nThe integral is of a Gaussian kernel:\n$$ \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{(\\mu - m_{jn})^{2}}{2\\tau_{jn}^{2}}\\right) \\exp\\left(-\\frac{1}{2}(C - B^{2}/A)\\right) d\\mu $$\nThe term $C - B^{2}/A$ simplifies to $\\frac{n(\\bar{y}-m_{0})^{2}}{n\\tau_{j}^{2} + \\sigma^{2}}$.\nThe integral evaluates to $\\sqrt{2\\pi\\tau_{jn}^{2}}$.\nSo the full integral is:\n$$ I = \\sqrt{2\\pi\\tau_{jn}^{2}} \\exp\\left(-\\frac{n(\\bar{y}-m_{0})^{2}}{2(n\\tau_{j}^{2} + \\sigma^{2})}\\right) = \\sqrt{2\\pi \\frac{\\sigma^{2}\\tau_{j}^{2}}{n\\tau_{j}^{2} + \\sigma^{2}}} \\exp\\left(-\\frac{n(\\bar{y}-m_{0})^{2}}{2(n\\tau_{j}^{2} + \\sigma^{2})}\\right) $$\nSubstituting this back into the expression for $p(\\mathbf{y} \\mid \\mathcal{M}_{j})$:\n$$ p(\\mathbf{y} \\mid \\mathcal{M}_{j}) = (2\\pi\\sigma^{2})^{-n/2} (2\\pi\\tau_{j}^{2})^{-1/2} \\exp\\left(-\\frac{R}{2\\sigma^{2}}\\right) \\sqrt{2\\pi \\frac{\\sigma^{2}\\tau_{j}^{2}}{n\\tau_{j}^{2} + \\sigma^{2}}} \\exp\\left(-\\frac{n(\\bar{y}-m_{0})^{2}}{2(n\\tau_{j}^{2} + \\sigma^{2})}\\right) $$\nSimplifying the constant pre-factors:\n$$ (2\\pi)^{-n/2}(\\sigma^{2})^{-n/2}(2\\pi)^{-1/2}(\\tau_{j}^{2})^{-1/2}(2\\pi)^{1/2}\\frac{\\sigma\\tau_{j}}{\\sqrt{n\\tau_{j}^{2} + \\sigma^{2}}} = (2\\pi)^{-n/2}(\\sigma^{2})^{-(n-1)/2}(n\\tau_{j}^{2} + \\sigma^{2})^{-1/2} $$\nThus, the final expression for the marginal likelihood is:\n$$ p(\\mathbf{y} \\mid \\mathcal{M}_{j}) = (2\\pi)^{-n/2} (\\sigma^{2})^{-(n-1)/2} (n\\tau_{j}^{2} + \\sigma^{2})^{-1/2} \\exp\\left(-\\frac{R}{2\\sigma^{2}}\\right) \\exp\\left(-\\frac{n(\\bar{y}-m_{0})^{2}}{2(n\\tau_{j}^{2} + \\sigma^{2})}\\right) $$\n\n### Step 2: Computation of the Bayes Factor $BF_{12}$\n\nThe Bayes factor $BF_{12}$ is the ratio of the marginal likelihoods for $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$:\n$$ BF_{12} = \\frac{p(\\mathbf{y} \\mid \\mathcal{M}_{1})}{p(\\mathbf{y} \\mid \\mathcal{M}_{2})} $$\nUsing the expression derived above, we form the ratio. All terms that do not depend on the model index $j$ will cancel out. These terms are $(2\\pi)^{-n/2}$, $(\\sigma^{2})^{-(n-1)/2}$, and $\\exp\\left(-R/(2\\sigma^{2})\\right)$.\n$$ BF_{12} = \\frac{(n\\tau_{1}^{2} + \\sigma^{2})^{-1/2} \\exp\\left(-\\frac{n(\\bar{y}-m_{0})^{2}}{2(n\\tau_{1}^{2} + \\sigma^{2})}\\right)}{(n\\tau_{2}^{2} + \\sigma^{2})^{-1/2} \\exp\\left(-\\frac{n(\\bar{y}-m_{0})^{2}}{2(n\\tau_{2}^{2} + \\sigma^{2})}\\right)} $$\nThis simplifies to:\n$$ BF_{12} = \\left(\\frac{n\\tau_{2}^{2} + \\sigma^{2}}{n\\tau_{1}^{2} + \\sigma^{2}}\\right)^{1/2} \\exp\\left( \\frac{n(\\bar{y}-m_{0})^{2}}{2} \\left[ \\frac{1}{n\\tau_{2}^{2} + \\sigma^{2}} - \\frac{1}{n\\tau_{1}^{2} + \\sigma^{2}} \\right] \\right) $$\nFinding a common denominator for the term in the exponent's brackets:\n$$ \\frac{1}{n\\tau_{2}^{2} + \\sigma^{2}} - \\frac{1}{n\\tau_{1}^{2} + \\sigma^{2}} = \\frac{(n\\tau_{1}^{2} + \\sigma^{2}) - (n\\tau_{2}^{2} + \\sigma^{2})}{(n\\tau_{1}^{2} + \\sigma^{2})(n\\tau_{2}^{2} + \\sigma^{2})} = \\frac{n(\\tau_{1}^{2} - \\tau_{2}^{2})}{(n\\tau_{1}^{2} + \\sigma^{2})(n\\tau_{2}^{2} + \\sigma^{2})} $$\nSubstituting this back gives the final simplified expression for the Bayes factor:\n$$ BF_{12} = \\left(\\frac{n\\tau_{2}^{2} + \\sigma^{2}}{n\\tau_{1}^{2} + \\sigma^{2}}\\right)^{1/2} \\exp\\left( \\frac{n^{2}(\\bar{y}-m_{0})^{2}(\\tau_{1}^{2} - \\tau_{2}^{2})}{2(n\\tau_{1}^{2} + \\sigma^{2})(n\\tau_{2}^{2} + \\sigma^{2})} \\right) $$\n\n### Step 3: Sensitivity Analysis\n\nTo analyze how $BF_{12}$ changes with the prior variances, we examine the partial derivatives of its natural logarithm, $\\ln(BF_{12})$.\n$$ \\ln(BF_{12}) = \\frac{1}{2}\\ln(n\\tau_{2}^{2}+\\sigma^{2}) - \\frac{1}{2}\\ln(n\\tau_{1}^{2}+\\sigma^{2}) + \\frac{n(\\bar{y}-m_{0})^{2}}{2}\\left(\\frac{1}{n\\tau_{2}^{2}+\\sigma^{2}} - \\frac{1}{n\\tau_{1}^{2}+\\sigma^{2}}\\right) $$\nThe partial derivative with respect to $\\tau_{1}^{2}$ is:\n$$ \\frac{\\partial \\ln(BF_{12})}{\\partial \\tau_{1}^{2}} = -\\frac{n}{2(n\\tau_{1}^{2}+\\sigma^{2})} + \\frac{n(\\bar{y}-m_{0})^{2}}{2} \\left(\\frac{n}{(n\\tau_{1}^{2}+\\sigma^{2})^{2}}\\right) = \\frac{n}{2(n\\tau_{1}^{2}+\\sigma^{2})^{2}} \\left[ n(\\bar{y}-m_{0})^{2} - (n\\tau_{1}^{2}+\\sigma^{2}) \\right] $$\nThe sign of this derivative is determined by the term in brackets. $\\frac{\\partial \\ln(BF_{12})}{\\partial \\tau_{1}^{2}}  0$ if and only if $n(\\bar{y}-m_{0})^{2}  n\\tau_{1}^{2}+\\sigma^{2}$, or $(\\bar{y}-m_{0})^{2}  \\tau_{1}^{2} + \\frac{\\sigma^{2}}{n}$. This means increasing the prior variance $\\tau_{1}^{2}$ increases the evidence for $\\mathcal{M}_{1}$ when the squared data-prior discrepancy $|\\bar{y}-m_{0}|^{2}$ is larger than the predictive variance of $\\bar{y}$ under $\\mathcal{M}_{1}$.\n\nThe partial derivative with respect to $\\tau_{2}^{2}$ is:\n$$ \\frac{\\partial \\ln(BF_{12})}{\\partial \\tau_{2}^{2}} = \\frac{n}{2(n\\tau_{2}^{2}+\\sigma^{2})} - \\frac{n(\\bar{y}-m_{0})^{2}}{2} \\left(\\frac{n}{(n\\tau_{2}^{2}+\\sigma^{2})^{2}}\\right) = \\frac{n}{2(n\\tau_{2}^{2}+\\sigma^{2})^{2}} \\left[ (n\\tau_{2}^{2}+\\sigma^{2}) - n(\\bar{y}-m_{0})^{2} \\right] $$\nThe sign is positive if and only if $n\\tau_{2}^{2}+\\sigma^{2}  n(\\bar{y}-m_{0})^{2}$, or $\\tau_{2}^{2} + \\frac{\\sigma^{2}}{n}  (\\bar{y}-m_{0})^{2}$. This means increasing the prior variance $\\tau_{2}^{2}$ increases the evidence for $\\mathcal{M}_{1}$ (by decreasing the evidence for $\\mathcal{M}_{2}$) when the predictive variance of $\\bar{y}$ under $\\mathcal{M}_{2}$ is larger than the squared data-prior discrepancy.\n\n**Role of parameters:** The sensitivity of the Bayes factor to prior variance is mediated by the term $\\tau_{j}^{2} + \\sigma^{2}/n$, the predictive variance of $\\bar{y}$ under model $\\mathcal{M}_{j}$.\n- **$|\\bar{y}-m_{0}|$**: A large discrepancy between data and prior mean favors models with larger prior variance, as they assign higher likelihood to more extreme observations.\n- **$n$**: As sample size $n$ increases, the term $\\sigma^{2}/n$ diminishes. The data becomes more informative, and the predictive variance approaches the prior variance $\\tau_{j}^{2}$. The Bayes factor becomes more sensitive to the choice of $\\tau_{j}^{2}$.\n- **$\\sigma^{2}$**: A large data variance $\\sigma^{2}$ increases the predictive variance for both models, making them harder to distinguish and reducing the sensitivity of the Bayes factor to the specific choice of $\\tau_{j}^{2}$.", "answer": "$$\n\\boxed{ \\left(\\frac{n\\tau_{2}^{2} + \\sigma^{2}}{n\\tau_{1}^{2} + \\sigma^{2}}\\right)^{1/2} \\exp\\left( \\frac{n^{2}(\\bar{y}-m_{0})^{2}(\\tau_{1}^{2} - \\tau_{2}^{2})}{2(n\\tau_{1}^{2} + \\sigma^{2})(n\\tau_{2}^{2} + \\sigma^{2})} \\right) }\n$$", "id": "3319160"}, {"introduction": "While analytical solutions are invaluable for building intuition, most real-world models require numerical estimation. This practice introduces importance sampling, a cornerstone of Monte Carlo methods, for approximating the marginal likelihood. By using the Delta method to analyze the variance of the log-estimator, you will uncover the critical role of the weight distribution and learn to diagnose the stability of the estimator, a crucial skill for any practitioner [@problem_id:3319185].", "problem": "Consider a Bayesian model with observed data $y$ and latent variable $x$ with prior density $p(x)$ and likelihood $p(y \\mid x)$. The marginal likelihood is the normalizing constant\n$$\nZ \\equiv \\int p(y \\mid x)\\,p(x)\\,dx.\n$$\nLet $q(x)$ be a proposal density with support covering that of $p(y \\mid x)\\,p(x)$. Draw $X_{1},\\dots,X_{n} \\stackrel{\\text{i.i.d.}}{\\sim} q$ and define the importance sampling (IS) weights $w(X_{i}) \\equiv \\frac{p(y \\mid X_{i})\\,p(X_{i})}{q(X_{i})}$ and the IS estimator of $Z$ by\n$$\n\\widehat{Z}_{n} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} w(X_{i}).\n$$\nAssume that $Z \\in (0,\\infty)$, that the second moment $m_{2} \\equiv \\mathbb{E}_{q}\\!\\left[w(X)^{2}\\right]$ is finite, and that the variance $\\sigma^{2} \\equiv \\mathrm{Var}_{q}\\!\\left(w(X)\\right)= m_{2} - Z^{2}$ is positive and finite. Consider the log-IS estimator $\\ell_{n} \\equiv \\ln\\!\\left(\\widehat{Z}_{n}\\right)$.\n\nUsing only foundational results such as the Law of Large Numbers, the Central Limit Theorem, and the first-order Delta method, derive the large-sample variance of $\\ell_{n}$ in closed form as an expression in $n$, $Z$, and $m_{2}$. State clearly the regularity conditions you invoke for your derivation.\n\nThen, based on your derivation and without appealing to unproven heuristics, provide a principled recommendation about when aggregating estimates across independent runs on the logarithmic scale (that is, averaging $\\ell_{n}$ and exponentiating) is preferable to aggregating on the linear scale (that is, averaging $\\widehat{Z}_{n}$).\n\nYour final answer must be the single closed-form expression for the asymptotic variance of $\\ell_{n}$ in terms of $n$, $Z$, and $m_{2}$. No rounding is required.", "solution": "The objective is to derive the large-sample variance of the log-IS estimator, $\\ell_{n} = \\ln(\\widehat{Z}_{n})$.\nThe IS estimator, $\\widehat{Z}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} w(X_{i})$, is a sample mean of $n$ independent and identically distributed (i.i.d.) random variables $w(X_i)$. The distribution of these variables is induced by drawing $X_i \\sim q(x)$.\n\nFirst, we establish the properties of the random variable $w(X)$. Its expectation under the proposal distribution $q(x)$ is:\n$$\n\\mathbb{E}_{q}[w(X)] = \\int w(x) q(x) dx = \\int \\frac{p(y \\mid x) p(x)}{q(x)} q(x) dx = \\int p(y \\mid x) p(x) dx = Z.\n$$\nThis demonstrates that the IS estimator $\\widehat{Z}_n$ is an unbiased estimator of $Z$, as $\\mathbb{E}_{q}[\\widehat{Z}_n] = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{q}[w(X_i)] = Z$.\n\nThe variance of $w(X)$ is given in the problem statement as $\\sigma^2 = \\mathrm{Var}_{q}(w(X))$, where $\\sigma^2 = m_2 - Z^2$. Since the weights $w(X_i)$ are i.i.d., the variance of the IS estimator is:\n$$\n\\mathrm{Var}_{q}(\\widehat{Z}_{n}) = \\mathrm{Var}_{q}\\left(\\frac{1}{n}\\sum_{i=1}^{n} w(X_i)\\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\mathrm{Var}_q(w(X_i)) = \\frac{n \\sigma^2}{n^2} = \\frac{\\sigma^2}{n} = \\frac{m_{2} - Z^{2}}{n}.\n$$\nThe problem states that $Z$ is finite and $\\sigma^2$ is finite, which are the sufficient conditions for the Central Limit Theorem (CLT) to apply to the sample mean $\\widehat{Z}_n$. The CLT states that:\n$$\n\\sqrt{n}(\\widehat{Z}_n - Z) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2),\n$$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution and $\\mathcal{N}(0, \\sigma^2)$ is a normal distribution with mean $0$ and variance $\\sigma^2 = m_2 - Z^2$.\n\nWe are interested in the asymptotic distribution of $\\ell_n = \\ln(\\widehat{Z}_n)$. This can be found using the first-order Delta method. Let $g(z) = \\ln(z)$. The Delta method states that for a sequence of random variables $T_n$ such that $\\sqrt{n}(T_n - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\tau^2)$, and for a function $g$ that is differentiable at $\\theta$ with $g'(\\theta) \\neq 0$, the following holds:\n$$\n\\sqrt{n}(g(T_n) - g(\\theta)) \\xrightarrow{d} \\mathcal{N}(0, [g'(\\theta)]^2 \\tau^2).\n$$\nIn our case, $T_n = \\widehat{Z}_n$, $\\theta = Z$, and $\\tau^2 = \\sigma^2 = m_2 - Z^2$. The function is $g(z) = \\ln(z)$, whose derivative is $g'(z) = 1/z$.\nThe regularity conditions for the Delta method are satisfied:\n1.  The problem states $Z \\in (0, \\infty)$, so $g(z) = \\ln(z)$ is well-defined and continuously differentiable at $z=Z$.\n2.  The derivative at $Z$ is $g'(Z) = 1/Z$, which is non-zero since $Z \\neq \\infty$.\n\nApplying the Delta method:\n$$\n\\sqrt{n}(\\ln(\\widehat{Z}_n) - \\ln(Z)) \\xrightarrow{d} \\mathcal{N}\\left(0, [g'(Z)]^2 \\sigma^2\\right).\n$$\nSubstituting the expressions for $g'(Z)$ and $\\sigma^2$:\n$$\n\\sqrt{n}(\\ell_n - \\ln(Z)) \\xrightarrow{d} \\mathcal{N}\\left(0, \\left(\\frac{1}{Z}\\right)^2 (m_2 - Z^2)\\right).\n$$\nThe asymptotic variance of $\\sqrt{n}(\\ell_n - \\ln(Z))$ is $\\frac{m_2 - Z^2}{Z^2}$. This implies that for large $n$, the distribution of $\\ell_n$ can be approximated by a normal distribution:\n$$\n\\ell_n \\approx \\mathcal{N}\\left(\\ln(Z), \\frac{1}{n}\\frac{m_2 - Z^2}{Z^2}\\right).\n$$\nThus, the large-sample variance of $\\ell_n$ is:\n$$\n\\mathrm{Var}(\\ell_n) \\approx \\frac{m_2 - Z^2}{nZ^2}.\n$$\n\nFor the second part of the problem, we must provide a recommendation about aggregating estimates. Aggregating $K$ independent estimates on the linear scale means computing the arithmetic mean $\\frac{1}{K}\\sum_{k=1}^K \\widehat{Z}_{n,k}$. Aggregating on the logarithmic scale and then transforming back means computing $\\exp\\left(\\frac{1}{K}\\sum_{k=1}^K \\ell_{n,k}\\right) = \\left(\\prod_{k=1}^K \\widehat{Z}_{n,k}\\right)^{1/K}$, which is the geometric mean of the individual estimates.\n\nThe derived large-sample variance of $\\ell_n$ can be rewritten as:\n$$\n\\mathrm{Var}(\\ell_n) \\approx \\frac{1}{n} \\left(\\frac{m_2}{Z^2} - 1\\right) = \\frac{1}{n} \\left(\\frac{\\mathbb{E}_q[w(X)^2]}{(\\mathbb{E}_q[w(X)])^2} - 1\\right) = \\frac{1}{n} \\frac{\\mathrm{Var}_q(w(X))}{(\\mathbb{E}_q[w(X)])^2} = \\frac{1}{n} [\\mathrm{CV}_q(w(X))]^2,\n$$\nwhere $\\mathrm{CV}_q(w(X))$ is the coefficient of variation (CV) of the importance weights.\n\nA principled recommendation is based on the properties of the estimators. Importance sampling in practice is often characterized by a weight distribution that is highly right-skewed, meaning a few weights are orders of magnitude larger than the rest. This occurs when the proposal density $q(x)$ is a poor match for the target density $p(y|x)p(x)$ in regions of high probability. A high degree of skewness corresponds to a large coefficient of variation.\n\nWhen the CV of the weights is large, the distribution of the estimator $\\widehat{Z}_n$ is also skewed for finite $n$, and its convergence to normality as per the CLT is slow. An arithmetic mean of such estimates is highly sensitive to any single estimate that is exceptionally large due to an outlier weight.\n\nThe logarithm, $g(z)=\\ln(z)$, is a concave function that compresses large values. Applying the logarithm to $\\widehat{Z}_n$ produces the statistic $\\ell_n$ whose distribution is typically more symmetric and closer to normal than that of $\\widehat{Z}_n$, especially when the underlying weights have high variance. The CLT on the log scale (as established by the Delta method) often provides a better approximation for finite $n$ than the CLT on the original scale.\n\nAveraging on the log scale (i.e., averaging $\\ell_n$) is more robust because one is averaging quantities that are better-behaved and whose distribution is closer to symmetric. This corresponds to taking a geometric mean, which is known to be less sensitive to extreme outliers than the arithmetic mean.\n\nTherefore, the recommendation is as follows: Aggregating on the logarithmic scale is preferable when the variance of the importance weights is large relative to their mean. This condition is quantified by a large value of the squared coefficient of variation, $\\frac{m_2 - Z^2}{Z^2}$, which is precisely the term (scaled by $1/n$) that governs the variance of the log-estimator $\\ell_n$ as derived above. In such high-variance scenarios, the log-transformation provides a stabilizing and symmetrizing effect, leading to more robust and reliable aggregated estimates.", "answer": "$$\n\\boxed{\\frac{m_2 - Z^2}{n Z^2}}\n$$", "id": "3319185"}, {"introduction": "Building on the foundations of Monte Carlo estimation, this advanced practice explores how to design more efficient algorithms. You will work with path sampling, a powerful method for computing log marginal likelihoods, but go beyond the standard textbook implementation. By deriving and solving a differential equation for an optimal integration schedule that minimizes variance, you will engage with the principles of adaptive Monte Carlo and learn how to tailor your methods to the problem at hand for maximum efficiency [@problem_id:3319187].", "problem": "Consider a Bayesian model with data $y = (y_{1}, \\dots, y_{n})$ assumed conditionally independent and identically distributed given a parameter $\\theta \\in \\mathbb{R}$ with likelihood $p(y \\mid \\theta)$ and prior $p(\\theta)$. Define a family of unnormalized densities $q_{t}(\\theta) = p(\\theta)\\, p(y \\mid \\theta)^{\\phi(t)}$ indexed by $t \\in [0,1]$ for a continuously differentiable schedule $\\phi : [0,1] \\to [0,1]$ with boundary conditions $\\phi(0) = 0$ and $\\phi(1) = 1$. Let $Z(t) = \\int q_{t}(\\theta)\\, d\\theta$ and $p_{t}(\\theta) = q_{t}(\\theta)/Z(t)$ denote the normalized distribution. The marginal likelihood (also called the evidence) is $p(y) = Z(1)$.\n\nTask:\n- Starting from the definitions of $Z(t)$ and $p_{t}(\\theta)$, derive an identity expressing $\\ln p(y)$ as an integral of an expectation under $p_{t}(\\theta)$ that depends on $\\phi'(t)$ and $\\ln p(y \\mid \\theta)$.\n- Suppose we estimate this integral by Monte Carlo sampling at each $t$ from $p_{t}(\\theta)$ (for example by Markov Chain Monte Carlo (MCMC)), and we wish to choose $\\phi$ so that the pointwise Monte Carlo variance of the integrand is constant in $t$. Formalize this by imposing that $\\phi'(t)^{2}\\, \\mathrm{Var}_{p_{t}}[\\ln p(y \\mid \\theta)]$ is constant in $t$, and express the resulting ordinary differential equation for $\\phi$.\n- Now specialize to the conjugate normal model with known observation variance: for $i \\in \\{1,\\dots,n\\}$, assume $y_{i} \\mid \\theta \\sim \\mathcal{N}(\\theta, \\sigma^{2})$ with known $\\sigma^{2}  0$, and prior $\\theta \\sim \\mathcal{N}(\\mu_{0}, \\tau_{0}^{2})$ with known $\\tau_{0}^{2}  0$. Let $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$ and assume $\\mu_{0} = \\bar{y}$. Under the powered posterior $p_{\\beta}(\\theta) \\propto p(\\theta)\\, p(y \\mid \\theta)^{\\beta}$ with $\\beta \\in [0,1]$, compute $\\mathrm{Var}_{p_{\\beta}}[\\ln p(y \\mid \\theta)]$ as a function of $\\beta$, $n$, $\\sigma^{2}$, and $\\tau_{0}^{2}$.\n- Using the variance expression and the constant-variance condition, solve the resulting differential equation for $\\phi(t)$ in closed form, enforcing $\\phi(0) = 0$ and $\\phi(1) = 1$.\n\nYour final answer must be a single explicit closed-form expression for $\\phi(t)$ in terms of $t$, $n$, $\\sigma^{2}$, and $\\tau_{0}^{2}$, with no undefined constants. Do not include any units. No numerical evaluation is required.", "solution": "The problem has been validated and is deemed a well-posed, scientifically grounded problem in Bayesian statistics. We will proceed with a full derivation.\n\n### Part 1: Derivation of the Thermodynamic Integral Identity\n\nThe marginal likelihood is given by $p(y) = Z(1)$, where $Z(t) = \\int q_{t}(\\theta)\\, d\\theta = \\int p(\\theta)\\, p(y \\mid \\theta)^{\\phi(t)}\\, d\\theta$. The schedule $\\phi(t)$ satisfies $\\phi(0) = 0$ and $\\phi(1) = 1$.\n\nAt $t=0$, we have $\\phi(0)=0$, so $q_{0}(\\theta) = p(\\theta) p(y \\mid \\theta)^{0} = p(\\theta)$. The normalization constant is $Z(0) = \\int p(\\theta) \\, d\\theta = 1$, as $p(\\theta)$ is a valid probability density.\n\nWe can express $\\ln p(y)$ using the fundamental theorem of calculus:\n$$ \\ln p(y) = \\ln Z(1) = \\ln Z(1) - \\ln Z(0) = \\int_{0}^{1} \\frac{d}{dt} \\ln Z(t) \\, dt $$\nThe derivative of $\\ln Z(t)$ with respect to $t$ is:\n$$ \\frac{d}{dt} \\ln Z(t) = \\frac{1}{Z(t)} \\frac{dZ(t)}{dt} $$\nWe compute the derivative of $Z(t)$ by differentiating under the integral sign (Leibniz integral rule), which is permissible given that $\\phi(t)$ is continuously differentiable:\n$$ \\frac{dZ(t)}{dt} = \\frac{d}{dt} \\int p(\\theta)\\, p(y \\mid \\theta)^{\\phi(t)}\\, d\\theta = \\int p(\\theta)\\, \\frac{\\partial}{\\partial t} \\exp\\left( \\phi(t) \\ln p(y \\mid \\theta) \\right) \\, d\\theta $$\n$$ = \\int p(\\theta)\\, \\exp\\left( \\phi(t) \\ln p(y \\mid \\theta) \\right) \\cdot \\left( \\phi'(t) \\ln p(y \\mid \\theta) \\right) \\, d\\theta $$\n$$ = \\phi'(t) \\int p(\\theta)\\, p(y \\mid \\theta)^{\\phi(t)} \\ln p(y \\mid \\theta) \\, d\\theta = \\phi'(t) \\int q_{t}(\\theta) \\ln p(y \\mid \\theta) \\, d\\theta $$\nSubstituting this back into the expression for $\\frac{d}{dt} \\ln Z(t)$:\n$$ \\frac{d}{dt} \\ln Z(t) = \\frac{1}{Z(t)} \\phi'(t) \\int q_{t}(\\theta) \\ln p(y \\mid \\theta) \\, d\\theta = \\phi'(t) \\int \\frac{q_{t}(\\theta)}{Z(t)} \\ln p(y \\mid \\theta) \\, d\\theta $$\nBy definition, $p_{t}(\\theta) = q_{t}(\\theta)/Z(t)$, so the integral is the expectation of $\\ln p(y \\mid \\theta)$ with respect to $p_{t}(\\theta)$:\n$$ \\frac{d}{dt} \\ln Z(t) = \\phi'(t) \\, \\mathbb{E}_{p_{t}}[\\ln p(y \\mid \\theta)] $$\nFinally, integrating this from $t=0$ to $t=1$ yields the desired identity, known as the thermodynamic integral or path sampling identity:\n$$ \\ln p(y) = \\int_{0}^{1} \\phi'(t) \\, \\mathbb{E}_{p_{t}}[\\ln p(y \\mid \\theta)] \\, dt $$\n\n### Part 2: Constant-Variance Ordinary Differential Equation\n\nThe integrand of the identity derived above is a function of $t$ whose value at each $t$ is estimated by Monte Carlo. The quantity being estimated at a fixed $t$ is $\\phi'(t) \\mathbb{E}_{p_{t}}[\\ln p(y \\mid \\theta)]$. A Monte Carlo estimate is based on samples $\\theta_i \\sim p_t(\\theta)$, forming an average of $\\phi'(t) \\ln p(y \\mid \\theta_i)$. The variance of a single sample of the integrand $\\phi'(t) \\ln p(y \\mid \\theta)$ under the distribution $p_t(\\theta)$ is:\n$$ \\mathrm{Var}_{p_{t}} [ \\phi'(t) \\ln p(y \\mid \\theta) ] = (\\phi'(t))^{2} \\mathrm{Var}_{p_{t}} [ \\ln p(y \\mid \\theta) ] $$\nThe problem requires this pointwise variance to be constant with respect to $t$. Let this constant be $C$.\n$$ (\\phi'(t))^{2} \\mathrm{Var}_{p_{t}} [ \\ln p(y \\mid \\theta) ] = C $$\nThis is the ordinary differential equation (ODE) for $\\phi(t)$. Note that the variance term depends on $t$ through the distribution $p_t(\\theta)$, which in turn depends on $\\phi(t)$. Let us denote $V(\\beta) = \\mathrm{Var}_{p_{\\beta}}[\\ln p(y \\mid \\theta)]$, where $p_{\\beta}(\\theta) \\propto p(\\theta) p(y \\mid \\theta)^{\\beta}$. Then for $p_t(\\theta)$, we have $\\beta=\\phi(t)$, so the ODE is:\n$$ (\\phi'(t))^{2} V(\\phi(t)) = C $$\n\n### Part 3: Variance Calculation for the Conjugate Normal Model\n\nWe are given $y_{i} \\mid \\theta \\sim \\mathcal{N}(\\theta, \\sigma^{2})$ for $i=1,\\dots,n$, and $\\theta \\sim \\mathcal{N}(\\mu_{0}, \\tau_{0}^{2})$. The log-likelihood is:\n$$ \\ln p(y \\mid \\theta) = \\sum_{i=1}^{n} \\ln p(y_i \\mid \\theta) = \\sum_{i=1}^{n} \\left( -\\frac{1}{2} \\ln(2\\pi\\sigma^{2}) - \\frac{(y_i - \\theta)^{2}}{2\\sigma^{2}} \\right) $$\n$$ = -\\frac{n}{2} \\ln(2\\pi\\sigma^{2}) - \\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{n} (y_i^{2} - 2y_i\\theta + \\theta^{2}) $$\n$$ = -\\frac{n}{2\\sigma^{2}}\\theta^{2} + \\frac{n\\bar{y}}{\\sigma^{2}}\\theta - \\left( \\frac{n}{2} \\ln(2\\pi\\sigma^{2}) + \\frac{1}{2\\sigma^{2}}\\sum y_i^2 \\right) $$\nwhere $\\bar{y} = \\frac{1}{n} \\sum y_i$. This is a quadratic function of $\\theta$, which we write as $U(\\theta) = A\\theta^{2} + B\\theta + D$, with $A = -\\frac{n}{2\\sigma^{2}}$, $B = \\frac{n\\bar{y}}{\\sigma^{2}}$, and $D$ being the constant term. Note that $B = -2A\\bar{y}$.\n\nNext, we characterize the distribution $p_{\\beta}(\\theta) \\propto p(\\theta) p(y \\mid \\theta)^{\\beta}$. This is a posterior distribution from a Normal-Normal model.\n$$ p_{\\beta}(\\theta) \\propto \\exp\\left(-\\frac{(\\theta-\\mu_{0})^{2}}{2\\tau_{0}^{2}}\\right) \\exp\\left(\\beta \\ln p(y \\mid \\theta)\\right) $$\nThe exponent is a quadratic in $\\theta$. Its coefficient of $\\theta^2$ is $-\\frac{1}{2}\\left(\\frac{1}{\\tau_0^2} + \\frac{\\beta n}{\\sigma^2}\\right)$, and its coefficient of $\\theta$ is $\\left(\\frac{\\mu_0}{\\tau_0^2} + \\frac{\\beta n \\bar{y}}{\\sigma^2}\\right)$. Thus, $p_{\\beta}(\\theta)$ is a normal distribution, $\\theta \\sim \\mathcal{N}(\\mu_{\\beta}, \\tau_{\\beta}^{2})$, where the precision is $\\frac{1}{\\tau_{\\beta}^{2}} = \\frac{1}{\\tau_{0}^{2}} + \\frac{\\beta n}{\\sigma^{2}}$, so the variance is:\n$$ \\tau_{\\beta}^{2} = \\left(\\frac{1}{\\tau_{0}^{2}} + \\frac{\\beta n}{\\sigma^{2}}\\right)^{-1} = \\frac{\\sigma^{2}\\tau_{0}^{2}}{\\sigma^{2} + \\beta n \\tau_{0}^{2}} $$\nThe mean $\\mu_{\\beta}$ is given by $\\mu_{\\beta} = \\tau_{\\beta}^{2} \\left(\\frac{\\mu_0}{\\tau_0^2} + \\frac{\\beta n \\bar{y}}{\\sigma^2}\\right)$. Using the crucial assumption $\\mu_{0}=\\bar{y}$:\n$$ \\mu_{\\beta} = \\tau_{\\beta}^{2} \\left(\\frac{\\bar{y}}{\\tau_0^2} + \\frac{\\beta n \\bar{y}}{\\sigma^2}\\right) = \\tau_{\\beta}^{2} \\bar{y} \\left(\\frac{1}{\\tau_0^2} + \\frac{\\beta n}{\\sigma^2}\\right) = \\tau_{\\beta}^{2} \\bar{y} \\frac{1}{\\tau_{\\beta}^{2}} = \\bar{y} $$\nThe mean is $\\mu_{\\beta} = \\bar{y}$ for all $\\beta \\in [0,1]$.\n\nWe now compute $V(\\beta) = \\mathrm{Var}_{p_{\\beta}}[\\ln p(y \\mid \\theta)] = \\mathrm{Var}_{p_{\\beta}}[A\\theta^{2} + B\\theta + D] = \\mathrm{Var}_{p_{\\beta}}[A\\theta^{2} + B\\theta]$.\nFor a random variable $X \\sim \\mathcal{N}(\\mu, \\tau^{2})$, and a quadratic $f(X) = aX^2+bX+c$, the variance is $\\mathrm{Var}[f(X)] = \\mathrm{Var}[aX^2+bX] = 2a^2\\tau^4 + (2a\\mu+b)^2\\tau^2$.\nIn our case, $\\theta \\sim \\mathcal{N}(\\mu_{\\beta}, \\tau_{\\beta}^{2})$, $a=A$, $b=B$, $\\mu = \\mu_\\beta = \\bar{y}$, and $\\tau^2 = \\tau_\\beta^2$. Let's evaluate the term $2A\\mu_{\\beta}+B$:\n$$ 2A\\mu_{\\beta}+B = 2 \\left(-\\frac{n}{2\\sigma^{2}}\\right) \\bar{y} + \\frac{n\\bar{y}}{\\sigma^{2}} = -\\frac{n\\bar{y}}{\\sigma^{2}} + \\frac{n\\bar{y}}{\\sigma^{2}} = 0 $$\nThe variance calculation simplifies dramatically:\n$$ V(\\beta) = 2A^{2}(\\tau_{\\beta}^{2})^{2} $$\nSubstituting the expressions for $A$ and $\\tau_{\\beta}^{2}$:\n$$ V(\\beta) = 2 \\left(-\\frac{n}{2\\sigma^{2}}\\right)^{2} \\left(\\frac{\\sigma^{2}\\tau_{0}^{2}}{\\sigma^{2} + \\beta n \\tau_{0}^{2}}\\right)^{2} = 2 \\frac{n^{2}}{4\\sigma^{4}} \\frac{\\sigma^{4}\\tau_{0}^{4}}{(\\sigma^{2} + \\beta n \\tau_{0}^{2})^{2}} $$\n$$ V(\\beta) = \\frac{n^{2}\\tau_{0}^{4}}{2(\\sigma^{2} + \\beta n \\tau_{0}^{2})^{2}} $$\n\n### Part 4: Solving the Differential Equation\n\nWe insert the variance expression into the ODE from Part 2, setting $\\beta = \\phi(t)$:\n$$ (\\phi'(t))^{2} \\frac{n^{2}\\tau_{0}^{4}}{2(\\sigma^{2} + \\phi(t) n \\tau_{0}^{2})^{2}} = C $$\nTaking the square root (we assume $\\phi'(t) \\ge 0$ as $\\phi$ increases from $0$ to $1$):\n$$ \\phi'(t) \\frac{n\\tau_{0}^{2}}{\\sqrt{2}(\\sigma^{2} + \\phi(t) n \\tau_{0}^{2})} = \\sqrt{C} $$\nThis is a separable ODE. Let $\\phi'(t) = d\\phi/dt$. We can write it as:\n$$ \\frac{d\\phi}{\\sigma^{2} + \\phi n \\tau_{0}^{2}} = K \\, dt $$\nwhere $K = \\frac{\\sqrt{2C}}{n\\tau_{0}^{2}}$ is a constant. We integrate both sides. Using definite integrals is most direct. We integrate from $t'=0$ to $t$, where the function value goes from $\\phi(0)=0$ to $\\phi(t)$:\n$$ \\int_{0}^{\\phi(t)} \\frac{d\\beta}{\\sigma^{2} + \\beta n \\tau_{0}^{2}} = \\int_{0}^{t} K \\, dt' $$\n$$ \\left[ \\frac{1}{n\\tau_{0}^{2}} \\ln(\\sigma^{2} + \\beta n \\tau_{0}^{2}) \\right]_{0}^{\\phi(t)} = K t $$\n$$ \\frac{1}{n\\tau_{0}^{2}} \\left( \\ln(\\sigma^{2} + \\phi(t) n \\tau_{0}^{2}) - \\ln(\\sigma^{2}) \\right) = K t $$\n$$ \\frac{1}{n\\tau_{0}^{2}} \\ln\\left( \\frac{\\sigma^{2} + \\phi(t) n \\tau_{0}^{2}}{\\sigma^{2}} \\right) = K t $$\nTo find the constant $K$, we use the other boundary condition, $\\phi(1)=1$:\n$$ K = \\frac{1}{n\\tau_{0}^{2}} \\ln\\left( \\frac{\\sigma^{2} + n \\tau_{0}^{2}}{\\sigma^{2}} \\right) $$\nSubstituting $K$ back into the equation for $\\phi(t)$:\n$$ \\frac{1}{n\\tau_{0}^{2}} \\ln\\left( \\frac{\\sigma^{2} + \\phi(t) n \\tau_{0}^{2}}{\\sigma^{2}} \\right) = \\frac{t}{n\\tau_{0}^{2}} \\ln\\left( \\frac{\\sigma^{2} + n \\tau_{0}^{2}}{\\sigma^{2}} \\right) $$\nMultiplying by $n\\tau_{0}^{2}$ and using logarithm properties:\n$$ \\ln\\left( 1 + \\phi(t) \\frac{n \\tau_{0}^{2}}{\\sigma^{2}} \\right) = t \\ln\\left( 1 + \\frac{n \\tau_{0}^{2}}{\\sigma^{2}} \\right) = \\ln\\left( \\left(1 + \\frac{n \\tau_{0}^{2}}{\\sigma^{2}}\\right)^{t} \\right) $$\nExponentiating both sides gives:\n$$ 1 + \\phi(t) \\frac{n \\tau_{0}^{2}}{\\sigma^{2}} = \\left(1 + \\frac{n \\tau_{0}^{2}}{\\sigma^{2}}\\right)^{t} $$\nFinally, solving for $\\phi(t)$:\n$$ \\phi(t) \\frac{n \\tau_{0}^{2}}{\\sigma^{2}} = \\left(1 + \\frac{n \\tau_{0}^{2}}{\\sigma^{2}}\\right)^{t} - 1 $$\n$$ \\phi(t) = \\frac{\\sigma^{2}}{n \\tau_{0}^{2}} \\left[ \\left(1 + \\frac{n \\tau_{0}^{2}}{\\sigma^{2}}\\right)^{t} - 1 \\right] $$\nThis is the closed-form expression for the optimal schedule $\\phi(t)$ under the constant-variance criterion.", "answer": "$$\n\\boxed{\\frac{\\sigma^{2}}{n \\tau_{0}^{2}} \\left[ \\left(1 + \\frac{n \\tau_{0}^{2}}{\\sigma^{2}}\\right)^{t} - 1 \\right]}\n$$", "id": "3319187"}]}