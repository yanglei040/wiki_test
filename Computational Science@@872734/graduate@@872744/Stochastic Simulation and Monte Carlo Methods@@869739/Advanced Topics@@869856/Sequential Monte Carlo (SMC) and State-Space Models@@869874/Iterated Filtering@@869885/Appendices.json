{"hands_on_practices": [{"introduction": "To appreciate why sophisticated algorithms like iterated filtering are necessary, we must first understand the fundamental challenge in parameter estimation for general state-space models. This exercise guides you through a foundational derivation of the complete-data log-likelihood's gradient, forcing you to identify precisely which components are computationally inaccessible under typical 'plug-and-play' assumptions and thereby motivating the need for simulation-based solutions [@problem_id:3315185].", "problem": "Consider a partially observed Markov process (POMP), where the latent state process $\\{x_t\\}_{t=0}^{T}$ is a time-homogeneous Markov chain with parameter vector $\\theta \\in \\mathbb{R}^{d}$ and the observations $\\{y_t\\}_{t=1}^{T}$ are conditionally independent given the states. Let the initial state have density $p_{\\theta}(x_0)$, the transition density of the latent process be $f_{\\theta}(x_t \\mid x_{t-1})$, and the observation density be $g_{\\theta}(y_t \\mid x_t)$. Assume throughout that the Markov property and conditional independence structure hold in their standard forms, and that all densities exist and are differentiable with respect to $\\theta$.\n\nStarting from the fundamental laws and core definitions of a Markov chain and conditional independence (namely, that the joint density of the latent states and observations factorizes according to the product of the initial density, transition densities, and observation densities, consistent with the Markov property and the observation model), derive an expression for the gradient with respect to $\\theta$ of the log joint density $\\log p_{\\theta}(x_{0:T}, y_{1:T})$ as a sum of gradients of the log initial density, log transition densities, and log observation densities.\n\nThen, under a plug-and-play setup commonly used with iterated filtering, where one can simulate from $p_{\\theta}(x_0)$ and $f_{\\theta}(x_t \\mid x_{t-1})$ but cannot evaluate them pointwise, while one can evaluate $g_{\\theta}(y_t \\mid x_t)$ pointwise, explain in words which components of the gradient decomposition are directly accessible and which are not. Your explanation should be qualitative and should not introduce any shortcut formulas beyond the fundamental factorization implied by the model structure.\n\nProvide the final gradient decomposition formula only as your final answer. The final answer must be a single closed-form analytic expression. No rounding is required.", "solution": "The problem statement will be validated before a solution is attempted.\n\n### Step 1: Extract Givens\n- A partially observed Markov process (POMP).\n- Latent state process: $\\{x_t\\}_{t=0}^{T}$.\n- Latent process is a time-homogeneous Markov chain.\n- Parameter vector: $\\theta \\in \\mathbb{R}^{d}$.\n- Observations: $\\{y_t\\}_{t=1}^{T}$.\n- Observations are conditionally independent given the states.\n- Initial state density: $p_{\\theta}(x_0)$.\n- Transition density of the latent process: $f_{\\theta}(x_t \\mid x_{t-1})$.\n- Observation density: $g_{\\theta}(y_t \\mid x_t)$.\n- The Markov property and conditional independence structure hold in their standard forms.\n- All densities exist and are differentiable with respect to $\\theta$.\n- Task 1: Derive $\\nabla_{\\theta} \\log p_{\\theta}(x_{0:T}, y_{1:T})$ as a sum of gradients of log-densities, starting from fundamental laws.\n- Task 2: Under a \"plug-and-play\" setup where one can simulate from $p_{\\theta}(x_0)$ and $f_{\\theta}(x_t \\mid x_{t-1})$ but not evaluate them pointwise, while $g_{\\theta}(y_t \\mid x_t)$ can be evaluated pointwise, explain which components of the gradient decomposition are accessible.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded (Critical)**: The problem is situated within the standard mathematical framework of state-space models, also known as partially observed Markov processes (POMPs) or hidden Markov models. This is a well-established and fundamental topic in statistics, econometrics, and signal processing. All concepts—Markov chains, conditional independence, and likelihood factorization—are standard. The problem is scientifically sound.\n- **Well-Posed**: The problem is well-posed. It asks for the derivation of a standard identity (the score function for the complete data) and a qualitative analysis based on a clearly defined set of constraints. The assumptions of differentiability and the existence of all densities ensure that the required mathematical operations are valid, leading to a unique and meaningful result.\n- **Objective (Critical)**: The problem is stated using precise, unambiguous mathematical language. There are no subjective or opinion-based statements.\n- **Completeness and Consistency**: The problem provides all necessary definitions ($p_{\\theta}$, $f_{\\theta}$, $g_{\\theta}$) and properties (Markov, conditional independence, differentiability) to perform the derivation. The constraints for the second part of the question are also stated clearly. There are no contradictions.\n\nThe problem does not exhibit any of the flaws listed in the validation checklist (e.g., scientific unsoundness, non-formalizability, incompleteness, etc.).\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete, reasoned solution will be provided.\n\n***\n\nWe are tasked with deriving the gradient of the log joint density of the states and observations for a partially observed Markov process. Let the complete history of states be denoted by $x_{0:T} = (x_0, x_1, \\dots, x_T)$ and the sequence of observations be $y_{1:T} = (y_1, y_2, \\dots, y_T)$. The joint probability density is $p_{\\theta}(x_{0:T}, y_{1:T})$.\n\nThe fundamental definition of a POMP allows us to factorize this joint density. Using the chain rule of probability, we first separate the observations from the states:\n$$\np_{\\theta}(x_{0:T}, y_{1:T}) = p_{\\theta}(y_{1:T} \\mid x_{0:T}) \\, p_{\\theta}(x_{0:T})\n$$\nThe problem states that the observations $\\{y_t\\}$ are conditionally independent given the states $\\{x_t\\}$. More specifically, each observation $y_t$ depends only on the corresponding state $x_t$. This allows us to factor the conditional density of the observations:\n$$\np_{\\theta}(y_{1:T} \\mid x_{0:T}) = \\prod_{t=1}^{T} g_{\\theta}(y_t \\mid x_t)\n$$\nThe problem also states that the latent process $\\{x_t\\}$ is a time-homogeneous Markov chain. This property means that the density of the entire state trajectory can be factored into the product of the initial state density and the sequence of one-step transition densities:\n$$\np_{\\theta}(x_{0:T}) = p_{\\theta}(x_0, x_1, \\dots, x_T) = p_{\\theta}(x_0) \\prod_{t=1}^{T} f_{\\theta}(x_t \\mid x_{t-1})\n$$\nSubstituting these two factorizations into the expression for the joint density, we obtain the complete factorization:\n$$\np_{\\theta}(x_{0:T}, y_{1:T}) = p_{\\theta}(x_0) \\left( \\prod_{t=1}^{T} f_{\\theta}(x_t \\mid x_{t-1}) \\right) \\left( \\prod_{t=1}^{T} g_{\\theta}(y_t \\mid x_t) \\right)\n$$\nThe problem requires the gradient of the logarithm of this density. Taking the natural logarithm of both sides, the products are converted into sums:\n$$\n\\log p_{\\theta}(x_{0:T}, y_{1:T}) = \\log p_{\\theta}(x_0) + \\sum_{t=1}^{T} \\log f_{\\theta}(x_t \\mid x_{t-1}) + \\sum_{t=1}^{T} \\log g_{\\theta}(y_t \\mid x_t)\n$$\nFinally, we compute the gradient with respect to the parameter vector $\\theta$. As the gradient operator $\\nabla_{\\theta}$ is a linear operator, the gradient of the sum is the sum of the gradients:\n$$\n\\nabla_{\\theta} \\log p_{\\theta}(x_{0:T}, y_{1:T}) = \\nabla_{\\theta} \\log p_{\\theta}(x_0) + \\nabla_{\\theta} \\left( \\sum_{t=1}^{T} \\log f_{\\theta}(x_t \\mid x_{t-1}) \\right) + \\nabla_{\\theta} \\left( \\sum_{t=1}^{T} \\log g_{\\theta}(y_t \\mid x_t) \\right)\n$$\nSince the parameters $\\theta$ are assumed not to depend on time $t$, we can bring the gradient operator inside the summations:\n$$\n\\nabla_{\\theta} \\log p_{\\theta}(x_{0:T}, y_{1:T}) = \\nabla_{\\theta} \\log p_{\\theta}(x_0) + \\sum_{t=1}^{T} \\nabla_{\\theta} \\log f_{\\theta}(x_t \\mid x_{t-1}) + \\sum_{t=1}^{T} \\nabla_{\\theta} \\log g_{\\theta}(y_t \\mid x_t)\n$$\nThis is the desired decomposition of the score function for the complete data $(x_{0:T}, y_{1:T})$.\n\nNext, we analyze the accessibility of each component under the specified \"plug-and-play\" setup. The decomposition consists of three parts:\n1. The initial state component: $\\nabla_{\\theta} \\log p_{\\theta}(x_0)$\n2. The state transition component: $\\sum_{t=1}^{T} \\nabla_{\\theta} \\log f_{\\theta}(x_t \\mid x_{t-1})$\n3. The observation component: $\\sum_{t=1}^{T} \\nabla_{\\theta} \\log g_{\\theta}(y_t \\mid x_t)$\n\nUsing the identity for the gradient of a logarithm, $\\nabla_{\\theta} \\log h_{\\theta}(z) = \\frac{\\nabla_{\\theta} h_{\\theta}(z)}{h_{\\theta}(z)}$, evaluating each component requires pointwise evaluation of both the respective density and its gradient.\n\n- **Initial State and Transition Components**: The first two components involve the terms $\\nabla_{\\theta} \\log p_{\\theta}(x_0)$ and $\\nabla_{\\theta} \\log f_{\\theta}(x_t \\mid x_{t-1})$. To calculate these, one would need to evaluate the densities $p_{\\theta}(x_0)$ and $f_{\\theta}(x_t \\mid x_{t-1})$ themselves (as the denominators), as well as their gradients. The problem explicitly states that in the plug-and-play setting, one *cannot* evaluate these densities pointwise. Therefore, the initial state component and the state transition component of the gradient are **not directly accessible**. Their evaluation is precluded by the problem's constraints.\n\n- **Observation Component**: The third component, $\\sum_{t=1}^{T} \\nabla_{\\theta} \\log g_{\\theta}(y_t \\mid x_t)$, requires the evaluation of the observation density $g_{\\theta}(y_t \\mid x_t)$ and its gradient $\\nabla_{\\theta} g_{\\theta}(y_t \\mid x_t)$. The problem states that one *can* evaluate $g_{\\theta}(y_t \\mid x_t)$ pointwise. Since the functional form of this density is known, its gradient with respect to $\\theta$ can typically be derived analytically and also evaluated. Thus, for any given state $x_t$ and observation $y_t$, the term $\\nabla_{\\theta} \\log g_{\\theta}(y_t \\mid x_t)$ can be computed. Consequently, the observation component of the gradient decomposition **is directly accessible**.\n\nIn summary, under the specified plug-and-play conditions, only the portion of the score function related to the observation model can be directly computed. The parts related to the latent process dynamics (initialization and transition) cannot be evaluated, which is a principal difficulty addressed by algorithms like iterated filtering that use simulation-based (Monte Carlo) methods to handle these inaccessible terms.", "answer": "$$\n\\boxed{\\nabla_{\\theta} \\log p_{\\theta}(x_{0:T}, y_{1:T}) = \\nabla_{\\theta} \\log p_{\\theta}(x_0) + \\sum_{t=1}^{T} \\nabla_{\\theta} \\log f_{\\theta}(x_t \\mid x_{t-1}) + \\sum_{t=1}^{T} \\nabla_{\\theta} \\log g_{\\theta}(y_t \\mid x_t)}\n$$", "id": "3315185"}, {"introduction": "While iterated filtering is a powerful tool for general non-linear, non-Gaussian models, it is crucial to recognize the cases where it is not needed. This practice focuses on the linear Gaussian state-space model, a cornerstone of time series analysis, where the celebrated Kalman filter provides an exact, analytical solution for the likelihood. By working through this tractable case, you will gain a sharper understanding of the specific problem domain that necessitates Monte Carlo methods like IF [@problem_id:3315204].", "problem": "Consider the following linear Gaussian state-space model. Let $(x_t)_{t=0,\\dots,T}$ be a latent Markov process in $\\mathbb{R}^{d_x}$ and $(y_t)_{t=1,\\dots,T}$ be observed data in $\\mathbb{R}^{d_y}$. The parameter vector is $\\theta = \\{A, C, Q, R, m_0, P_0\\}$, with $A \\in \\mathbb{R}^{d_x \\times d_x}$, $C \\in \\mathbb{R}^{d_y \\times d_x}$, $Q \\in \\mathbb{S}_{+}^{d_x}$, $R \\in \\mathbb{S}_{++}^{d_y}$, $m_0 \\in \\mathbb{R}^{d_x}$, and $P_0 \\in \\mathbb{S}_{+}^{d_x}$, where $\\mathbb{S}_{+}^{k}$ denotes the set of symmetric positive semidefinite matrices of dimension $k$ and $\\mathbb{S}_{++}^{k}$ denotes the set of symmetric positive definite matrices of dimension $k$. The model is specified by\n$$\nx_0 \\sim \\mathcal{N}(m_0, P_0), \\quad x_t = A x_{t-1} + w_t, \\quad w_t \\overset{\\text{iid}}{\\sim} \\mathcal{N}(0, Q), \\quad t = 1,\\dots,T,\n$$\n$$\ny_t = C x_t + v_t, \\quad v_t \\overset{\\text{iid}}{\\sim} \\mathcal{N}(0, R), \\quad t = 1,\\dots,T,\n$$\nwith $(w_t)$, $(v_t)$, and $x_0$ mutually independent. Starting from fundamental definitions of the Gaussian distribution, the Markov property, and the chain rule of probability, derive the explicit closed-form expression for the marginal likelihood $p_{\\theta}(y_{1:T})$. Your derivation must proceed by integrating out the latent states using only well-tested facts about multivariate normal distributions and the properties of linear transformations of Gaussian random variables. Clearly define any intermediate quantities you use.\n\nThen, using first principles of likelihood-based inference and the role of Sequential Monte Carlo (SMC) approximations in Iterated Filtering (IF), explain why iterated filtering is unnecessary for this special case.\n\nProvide your final answer as a single closed-form analytic expression for $p_{\\theta}(y_{1:T})$. No numerical approximation is required. Do not include any physical units. There is no rounding requirement.", "solution": "The problem statement is valid. It presents a well-defined mathematical problem in the field of time-series analysis and stochastic processes, specifically concerning a linear Gaussian state-space model. All definitions are standard, and the problem is scientifically grounded, objective, and self-contained.\n\nThe objective is to derive the marginal likelihood of the observations, $p_{\\theta}(y_{1:T})$, where $y_{1:T} = (y_1, \\dots, y_T)$. This is accomplished by integrating out the latent state variables $x_{0:T}$. A direct approach is to factorize the joint probability of the observations using the chain rule:\n$$\np_{\\theta}(y_{1:T}) = p_{\\theta}(y_1) p_{\\theta}(y_2 | y_1) \\cdots p_{\\theta}(y_T | y_{1:T-1}) = \\prod_{t=1}^{T} p_{\\theta}(y_t | y_{1:t-1})\n$$\nwhere the conditioning set $y_{1:0}$ for the $t=1$ term is empty. The problem thus reduces to finding the one-step-ahead predictive distributions $p_{\\theta}(y_t | y_{1:t-1})$ for $t=1, \\dots, T$.\n\nIn a linear Gaussian state-space model, all conditional and marginal distributions of interest are Gaussian. This property allows for the exact computation of the required predictive distributions using a recursive algorithm known as the Kalman filter.\n\nLet the filtering distribution at time $t-1$ be $p(x_{t-1} | y_{1:t-1})$, which we assume to be Gaussian with mean $m_{t-1|t-1}$ and covariance $P_{t-1|t-1}$:\n$$\np(x_{t-1} | y_{1:t-1}) = \\mathcal{N}(x_{t-1} | m_{t-1|t-1}, P_{t-1|t-1})\n$$\nThe recursion is initialized at $t=0$ (before any observations) with the prior distribution of the initial state, so $m_{0|0} = m_0$ and $P_{0|0} = P_0$.\n\nThe derivation proceeds in two stages for each time step $t$: a prediction stage and an update stage. The predictive distributions for the likelihood are computed in the prediction stage.\n\n**Prediction Stage (for time $t$):**\n1.  **State Prediction:** We first compute the predictive distribution for the state $x_t$ given observations up to $t-1$, $p(x_t | y_{1:t-1})$. This involves the state transition equation $x_t = A x_{t-1} + w_t$. Since $x_{t-1} | y_{1:t-1}$ is Gaussian and $w_t$ is an independent Gaussian variable, their linear combination $x_t | y_{1:t-1}$ is also Gaussian, which we denote as $\\mathcal{N}(x_t | m_{t|t-1}, P_{t|t-1})$.\n    The mean is found by taking the expectation:\n    $$\n    m_{t|t-1} = E[x_t | y_{1:t-1}] = E[A x_{t-1} + w_t | y_{1:t-1}] = A E[x_{t-1} | y_{1:t-1}] + E[w_t] = A m_{t-1|t-1}\n    $$\n    The covariance is found by:\n    $$\n    P_{t|t-1} = \\text{Cov}(x_t | y_{1:t-1}) = \\text{Cov}(A x_{t-1} + w_t | y_{1:t-1}) = A \\text{Cov}(x_{t-1} | y_{1:t-1}) A^T + \\text{Cov}(w_t) = A P_{t-1|t-1} A^T + Q\n    $$\n\n2.  **Observation Prediction:** Next, we compute the predictive distribution for the observation $y_t$ given observations up to $t-1$, which is $p(y_t | y_{1:t-1})$. This involves the observation equation $y_t = C x_t + v_t$. As $x_t | y_{1:t-1}$ is Gaussian and $v_t$ is an independent Gaussian variable, the distribution of $y_t | y_{1:t-1}$ is also Gaussian, denoted $\\mathcal{N}(y_t | \\hat{y}_{t|t-1}, S_t)$.\n    The mean is the predicted observation:\n    $$\n    \\hat{y}_{t|t-1} = E[y_t | y_{1:t-1}] = E[C x_t + v_t | y_{1:t-1}] = C E[x_t | y_{1:t-1}] + E[v_t] = C m_{t|t-1}\n    $$\n    The covariance is the innovation covariance:\n    $$\n    S_t = \\text{Cov}(y_t | y_{1:t-1}) = \\text{Cov}(C x_t + v_t | y_{1:t-1}) = C \\text{Cov}(x_t | y_{1:t-1}) C^T + \\text{Cov}(v_t) = C P_{t|t-1} C^T + R\n    $$\n    This Gaussian distribution $p(y_t | y_{1:t-1})$ is precisely the term needed for the marginal likelihood product. Its probability density function is:\n    $$\n    p_{\\theta}(y_t | y_{1:t-1}) = (2\\pi)^{-d_y/2} |S_t|^{-1/2} \\exp\\left(-\\frac{1}{2} (y_t - \\hat{y}_{t|t-1})^T S_t^{-1} (y_t - \\hat{y}_{t|t-1})\\right)\n    $$\n\n**Update Stage (for time $t$):**\nTo continue the recursion, we must compute the updated (filtering) distribution $p(x_t | y_{1:t})$ by incorporating the new observation $y_t$. Using Bayes' rule, $p(x_t | y_{1:t}) \\propto p(y_t | x_t) p(x_t | y_{1:t-1})$. This involves multiplying the likelihood $\\mathcal{N}(y_t | C x_t, R)$ and the prior $\\mathcal{N}(x_t | m_{t|t-1}, P_{t|t-1})$. The resulting posterior is Gaussian, $\\mathcal{N}(x_t | m_{t|t}, P_{t|t})$, with parameters:\n$$\nK_t = P_{t|t-1} C^T S_t^{-1} \\quad (\\text{Kalman Gain})\n$$\n$$\nm_{t|t} = m_{t|t-1} + K_t (y_t - \\hat{y}_{t|t-1})\n$$\n$$\nP_{t|t} = (I - K_t C) P_{t|t-1}\n$$\nThese updated parameters $m_{t|t}$ and $P_{t|t}$ are then used as $m_{t-1|t-1}$ and $P_{t-1|t-1}$ in the next time step, $t+1$.\n\n**Marginal Likelihood Expression:**\nBy iterating the prediction and update steps from $t=1$ to $T$, we compute all the required predictive densities. The marginal likelihood is their product:\n$$\np_{\\theta}(y_{1:T}) = \\prod_{t=1}^{T} (2\\pi)^{-d_y/2} |S_t|^{-1/2} \\exp\\left(-\\frac{1}{2} (y_t - \\hat{y}_{t|t-1})^T S_t^{-1} (y_t - \\hat{y}_{t|t-1})\\right)\n$$\nThis can be written more compactly as:\n$$\np_{\\theta}(y_{1:T}) = (2\\pi)^{-Td_y/2} \\left( \\prod_{t=1}^{T} |S_t| \\right)^{-1/2} \\exp\\left(-\\frac{1}{2} \\sum_{t=1}^{T} (y_t - \\hat{y}_{t|t-1})^T S_t^{-1} (y_t - \\hat{y}_{t|t-1})\\right)\n$$\nThis is a closed-form analytical expression, where the terms $S_t$ and $\\hat{y}_{t|t-1}$ are generated by the Kalman filter recursion.\n\n**On the Necessity of Iterated Filtering (IF):**\nIterated Filtering is a Sequential Monte Carlo (SMC) algorithm for maximum likelihood parameter estimation in general Partially Observed Markov Processes (POMPs). Its use is motivated by the intractability of such models when they are non-linear or involve non-Gaussian noise. In these general cases, the filtering distributions and the marginal likelihood cannot be computed in closed form, necessitating Monte Carlo approximation methods like particle filters. IF uses particle filters to stochastically approximate the likelihood and its gradient for optimization.\n\nFor the special case of a linear Gaussian state-space model, this machinery is entirely unnecessary. As demonstrated in the derivation above:\n1.  All filtering and predictive distributions are Gaussian and can be characterized **exactly** by their means and covariances, which are computed deterministically via the Kalman filter recursions.\n2.  The marginal likelihood $p_{\\theta}(y_{1:T})$ has an **exact, closed-form analytical expression**, known as the prediction error decomposition.\n\nSince the likelihood function is available in closed form, parameter estimation can be performed by directly maximizing this function (or its logarithm) using standard numerical optimization techniques like quasi-Newton methods (e.g., L-BFGS) or the Expectation-Maximization (EM) algorithm. Using an approximation method like IF for a problem with a known, exact, and efficient solution would be superfluous and computationally suboptimal. The LG-SSM represents a case where the general, complex machinery of SMC collapses to a simple, elegant, analytical solution.", "answer": "$$\\boxed{ (2\\pi)^{-\\frac{Td_y}{2}} \\left( \\prod_{t=1}^{T} |S_t| \\right)^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2} \\sum_{t=1}^{T} (y_t - \\hat{y}_{t|t-1})^T S_t^{-1} (y_t - \\hat{y}_{t|t-1})\\right) }$$", "id": "3315204"}, {"introduction": "Having established the 'why' and 'when' of iterated filtering, we now turn to the 'how' by analyzing its performance. This practice combines a theoretical derivation with a hands-on coding challenge to explore the behavior of the IF score estimator. You will derive its variance scaling properties and then numerically validate your theoretical findings, gaining practical insight into how computational effort translates into statistical precision [@problem_id:3315217].", "problem": "Consider a scalar autoregressive hidden Markov model with latent state and observation given by the following state space formulation: $$x_t = \\phi x_{t-1} + \\varepsilon_t,\\quad \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma_x^2),$$ $$y_t \\mid x_t \\sim \\mathcal{N}(x_t,\\tau^2),$$ where $x_0 \\sim \\mathcal{N}(0,1)$, the autoregressive coefficient $\\phi$ is the parameter of interest, and $\\sigma_x^2$ and $\\tau^2$ are known positive constants. Iterated Filtering (IF) uses parameter perturbations within a Sequential Monte Carlo algorithm (particle filtering) to construct an estimator of the score function (the gradient of the log-likelihood with respect to the parameter). Define the particle filter with $N$ particles in which, at each time step $t$, the parameter is perturbed independently across particles by drawing $\\delta_i^{(t)} \\sim \\mathcal{N}(0,\\sigma_\\theta^2)$ and using $\\phi_i^{(t)} = \\phi + \\delta_i^{(t)}$ to propagate particle $i$ via $$x_{t,i} = \\phi_i^{(t)} x_{t-1,i} + \\eta_{t,i},\\quad \\eta_{t,i} \\sim \\mathcal{N}(0,\\sigma_x^2),$$ and to compute its incremental weight $$w_{t,i} \\propto \\exp\\left(-\\frac{(y_t - x_{t,i})^2}{2\\tau^2}\\right).$$ Let the normalized weights at time $t$ be $\\tilde{w}_{t,i} = w_{t,i} / \\sum_{j=1}^N w_{t,j}$ and denote the log-weights by $\\ell_{t,i} = \\log w_{t,i}$. A simple IF-based estimator of the score contribution at time $t$ is defined by $$g_t = \\frac{1}{\\sigma_\\theta^2}\\sum_{i=1}^N \\tilde{w}_{t,i}\\,\\delta_i^{(t)}\\left(\\ell_{t,i} - \\sum_{j=1}^N \\tilde{w}_{t,j}\\,\\ell_{t,j}\\right),$$ and the overall score estimator is $$\\hat{S} = \\sum_{t=1}^T g_t.$$ Throughout, take $\\phi$ equal to the true data-generating value so the expected score is near zero, and focus on the variance of the estimator $\\mathrm{Var}(\\hat{S})$.\n\nStarting from the central limit theorem for weighted sums and the independence of parameter perturbations from observation noise conditional on the latent state, derive a heuristic scaling law for the variance $\\mathrm{Var}(\\hat{S})$ as a function of the number of particles $N$ and sequence length $T$ under reasonable mixing conditions. Your derivation must start from well-tested principles: independence approximations for large $N$, the central limit theorem for averages of independent random variables, and the additive structure of the score contributions across time. Explicitly state any approximations you introduce along the way, and obtain a scaling of the form $$\\mathrm{Var}(\\hat{S}) \\approx \\frac{C\\,T}{N},$$ identifying the dependence of the constant $C$ on model quantities such as $\\sigma_\\theta^2$ and the variability of $\\ell_{t,i}$.\n\nThen, numerically validate the scaling law on a single fixed dataset generated from the model with the following parameter values: $$\\phi = 0.7,\\quad \\sigma_x^2 = 1.0,\\quad \\tau^2 = 1.0,\\quad \\sigma_\\theta^2 = 0.05^2.$$ Generate a single dataset $\\{y_t\\}_{t=1}^{T_{\\max}}$ with $T_{\\max} = 400$ from the model at these parameter values and $x_0 \\sim \\mathcal{N}(0,1)$. For each $(N,T)$ pair in the test suite below, compute the empirical variance of $\\hat{S}$ across $R$ independent replicates of the particle filter with parameter perturbations, where each replicate reuses the same fixed data prefix $\\{y_t\\}_{t=1}^{T}$ but draws new particle initializations, parameter perturbations, transition noises, and resampling randomness. Use $$R = 150.$$\n\nTest suite of $(N,T)$ pairs:\n- $(N,T) = (50,50)$,\n- $(N,T) = (50,200)$,\n- $(N,T) = (200,50)$,\n- $(N,T) = (200,200)$,\n- $(N,T) = (400,100)$,\n- $(N,T) = (100,400)$.\n\nFor each test case, compute the normalized variance $$V_{\\mathrm{norm}}(N,T) = \\mathrm{Var}(\\hat{S}) \\cdot \\frac{N}{T}.$$ Using the case $(N,T) = (200,200)$ as the baseline, compute the ratio $$\\rho(N,T) = \\frac{V_{\\mathrm{norm}}(N,T)}{V_{\\mathrm{norm}}(200,200)}.$$ Your program should produce a single line of output containing the values of $\\rho(N,T)$ for the test suite, in the order listed above, as a comma-separated list enclosed in square brackets (for example, $$[r_1,r_2,r_3,r_4,r_5,r_6]$$). All numerical answers are unitless real numbers and must be printed as machine-readable decimal floats in a single line as specified. The numerical experiment must be self-contained and reproducible without external input or files. The final program must implement the particle filter with parameter perturbations, the IF score estimator defined above, systematic resampling, and the empirical variance computation across $R$ replicates for each test case.", "solution": "The problem is well-posed, scientifically grounded, and provides a complete specification for both a theoretical derivation and a numerical experiment. All parameters and algorithmic components are clearly defined.\n\nThe first task is to derive a heuristic scaling law for the variance of the iterated filtering score estimator, $\\mathrm{Var}(\\hat{S})$, as a function of the number of particles $N$ and the length of the time series $T$. The estimator is given by $\\hat{S} = \\sum_{t=1}^T g_t$, where\n$$g_t = \\frac{1}{\\sigma_\\theta^2}\\sum_{i=1}^N \\tilde{w}_{t,i}\\,\\delta_i^{(t)}\\left(\\ell_{t,i} - \\sum_{j=1}^N \\tilde{w}_{t,j}\\,\\ell_{t,j}\\right).$$\nOur derivation proceeds in three main steps: decomposing the variance over time, analyzing the variance of a single time-step contribution, and combining the results.\n\n**Step 1: Decomposition of Variance over Time**\nThe total score estimator $\\hat{S}$ is a sum of contributions $g_t$ from each time step. The variance of this sum is given by\n$$\\mathrm{Var}(\\hat{S}) = \\mathrm{Var}\\left(\\sum_{t=1}^T g_t\\right) = \\sum_{t=1}^T \\mathrm{Var}(g_t) + 2\\sum_{1 \\le t  s \\le T} \\mathrm{Cov}(g_t, g_s).$$\nWe introduce our first key approximation:\n**Approximation 1:** For a well-mixing system, the particle filter's state at time $s$ becomes progressively independent of its state at time $t$ as the gap $s-t$ increases. Consequently, the score contributions $g_t$ and $g_s$ become approximately uncorrelated. We assume these covariances are negligible, which is a common assumption for ergodic systems.\nThis reduces the variance to a sum of variances:\n$$\\mathrm{Var}(\\hat{S}) \\approx \\sum_{t=1}^T \\mathrm{Var}(g_t).$$\nNext, we introduce a second approximation related to stationarity:\n**Approximation 2:** For a sufficiently long time series (large $t$), the filtering process is assumed to reach a stochastic steady state. This implies that the statistical properties of the filter, and thus the variance of the score contribution $\\mathrm{Var}(g_t)$, become approximately constant over time. Let us denote this stationary variance as $\\mathrm{Var}(g)$.\nUnder this assumption, the sum simplifies to:\n$$\\mathrm{Var}(\\hat{S}) \\approx T \\cdot \\mathrm{Var}(g).$$\nThis establishes the linear dependence on the time series length $T$.\n\n**Step 2: Analysis of Single-Step Variance $\\mathrm{Var}(g)$**\nNow we analyze the variance of the single-step score contribution, $g_t$. Let us drop the time index $t$ for notational simplicity. The estimator $g$ is a complex function of the particle states, weights, and parameter perturbations. However, its structure is that of a function of empirical means of quantities associated with the particles.\nConditional on the filter's history up to time $t-1$, which we denote by the sigma-algebra $\\mathcal{F}_{t-1}$, the set of random variables driving the filter at step $t$ for each particle $i$, namely the resampled ancestor $x_{t-1,i}^\\star$, the parameter perturbation $\\delta_i^{(t)}$, and the process noise $\\eta_{t,i}$, are independent and identically distributed across the particles $i=1, \\dots, N$. The term $g_t$ is a function of the empirical averages of quantities derived from these i.i.d. variables.\nAccording to the Central Limit Theorem for particle filters (and more generally for functions of means of i.i.d. random variables, via the Delta method), any such estimator converges to its true value, and its variance scales inversely with the number of particles $N$.\n**Conclusion from CLT:** Based on established theory for Monte Carlo methods, the variance of $g_t$ must scale with $N$ as:\n$$\\mathrm{Var}(g) \\approx \\frac{C}{N},$$\nfor some constant $C$ that does not depend on $N$, but depends on the model parameters and the data-generating process.\n\nTo gain more insight into the constant $C$, we can make a further approximation:\n**Approximation 3:** We assume the perturbation variance $\\sigma_\\theta^2$ is small. This allows us to treat the weights $\\tilde{w}_{i}$ and log-weights $\\ell_{i}$ as being primarily determined by the state propagation noise $\\eta_i$ and the ancestor particle $x_{t-1,i}$, and only weakly dependent on the perturbation $\\delta_i$.\nLet us analyze the variance of $g_t$ conditional on the state of the filter just before the parameter perturbations are applied. In this context, the quantities $\\tilde{w}_{t,i}$ and $\\ell_{t,i}$ are considered fixed coefficients for the random variables $\\delta_i^{(t)}$.\nLet $c_i = \\frac{1}{\\sigma_\\theta^2}\\tilde{w}_{t,i}(\\ell_{t,i} - \\bar{\\ell}_t)$. Then $g_t = \\sum_{i=1}^N c_i \\delta_i^{(t)}$. Since the $\\delta_i^{(t)} \\sim \\mathcal{N}(0, \\sigma_\\theta^2)$ are i.i.d., the conditional variance is:\n$$\\mathrm{Var}(g_t \\mid \\mathcal{F}_{t-1}, \\{\\eta_{t,i}\\}) = \\mathrm{Var}\\left(\\sum_{i=1}^N c_i \\delta_i^{(t)}\\right) = \\sum_{i=1}^N c_i^2 \\mathrm{Var}(\\delta_i^{(t)}) = \\sigma_\\theta^2 \\sum_{i=1}^N c_i^2.$$\nSubstituting the expression for $c_i$:\n$$\\mathrm{Var}(g_t \\mid \\dots) = \\sigma_\\theta^2 \\sum_{i=1}^N \\frac{1}{(\\sigma_\\theta^2)^2} \\tilde{w}_{t,i}^2 (\\ell_{t,i} - \\bar{\\ell}_t)^2 = \\frac{1}{\\sigma_\\theta^2} \\sum_{i=1}^N \\tilde{w}_{t,i}^2 (\\ell_{t,i} - \\bar{\\ell}_t)^2.$$\nThe term $\\sum_{i=1}^N \\tilde{w}_{t,i}^2$ is the inverse of the effective sample size, $N_{\\text{eff}}$. For a healthy particle filter, $N_{\\text{eff}}$ is proportional to $N$, so $\\sum_{i=1}^N \\tilde{w}_{t,i}^2 \\propto 1/N$. The sum can be approximated as $\\frac{1}{N} \\mathrm{Var}_{\\tilde{w}}(\\ell)$, where $\\mathrm{Var}_{\\tilde{w}}(\\ell) = \\sum_i \\tilde{w}_i (\\ell_i - \\bar{\\ell})^2$ is the weighted variance of the log-weights. This approximation is more accurate if the weights are not heavily concentrated.\nThus, the conditional variance scales as:\n$$\\mathrm{Var}(g_t \\mid \\dots) \\approx \\frac{1}{N} \\cdot \\frac{\\mathrm{Var}_{\\tilde{w}}(\\ell)}{\\sigma_\\theta^2}.$$\nTaking the expectation over thefilter randomness (the noise $\\eta$ and resampling) gives the unconditional variance $\\mathrm{Var}(g_t)$, assuming the mean term in the law of total variance is negligible (which it is, since $g_t$ is an approximately unbiased estimator of a quantity whose expectation is zero at the true parameter value).\n$$\\mathrm{Var}(g) \\approx \\frac{1}{N} \\cdot \\frac{1}{\\sigma_\\theta^2} \\mathbb{E}[\\mathrm{Var}_{\\tilde{w}}(\\ell)].$$\nThis gives us the explicit form of the constant $C = \\frac{1}{\\sigma_\\theta^2} \\mathbb{E}[\\mathrm{Var}_{\\tilde{w}}(\\ell)]$, which depends on the perturbation variance and the expected variability of the particle log-weights.\n\n**Step 3: Final Scaling Law**\nCombining the results from Step 1 and Step 2, we arrive at the final heuristic scaling law for the variance of the score estimator:\n$$\\mathrm{Var}(\\hat{S}) \\approx T \\cdot \\mathrm{Var}(g) \\approx T \\left( \\frac{C}{N} \\right) = \\frac{C\\,T}{N}.$$\nThis relationship implies that the quantity $\\mathrm{Var}(\\hat{S}) \\cdot \\frac{N}{T}$ should be approximately constant across different choices of $N$ and $T$, provided these are large enough for the approximations to hold. The numerical experiment is designed to verify this prediction.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to perform the numerical validation of the IF estimator variance scaling.\n    \"\"\"\n    # Set random seed for complete reproducibility of the experiment.\n    np.random.seed(12345)\n\n    # --- Problem Parameters ---\n    phi = 0.7\n    sigma_x_sq = 1.0\n    tau_sq = 1.0\n    sigma_theta_sq = 0.05**2\n    x0_var = 1.0\n    T_max = 400\n    R = 150\n\n    # --- Test Suite ---\n    test_cases = [\n        (50, 50),\n        (50, 200),\n        (200, 50),\n        (200, 200),\n        (400, 100),\n        (100, 400),\n    ]\n\n    # --- Helper Functions ---\n\n    def generate_data(T, phi_val, sigma_x_val, tau_val, x0_stdev):\n        \"\"\"Generates a single time series from the state-space model.\"\"\"\n        x = np.zeros(T + 1)\n        y = np.zeros(T)\n        x[0] = np.random.normal(0, x0_stdev)\n        for t in range(T):\n            x[t+1] = phi_val * x[t] + np.random.normal(0, sigma_x_val)\n            y[t] = x[t+1] + np.random.normal(0, tau_val)\n        return y\n\n    def systematic_resample(weights):\n        \"\"\"Performs systematic resampling.\"\"\"\n        N = len(weights)\n        # Generate N ordered random numbers from U[0, 1/N), U[1/N, 2/N), ...\n        positions = (np.arange(N) + np.random.uniform()) / N\n        cum_weights = np.cumsum(weights)\n        # Find indices of particles to keep\n        indices = np.searchsorted(cum_weights, positions)\n        return indices\n\n    def run_if_replicate(y_data_prefix, N, T, phi_val, sigma_x_val, tau_sq_val, sigma_theta_sq_val, x0_stdev):\n        \"\"\"Runs one replicate of the iterated filtering algorithm.\"\"\"\n        # Initial particles drawn from the prior p(x_0)\n        x_particles = np.random.normal(0, x0_stdev, N)\n        s_hat = 0.0\n        \n        y_obs = y_data_prefix[:T]\n        sigma_theta_val = np.sqrt(sigma_theta_sq_val)\n\n        for t in range(T):\n            # Propagate particles with perturbed parameters\n            deltas = np.random.normal(0, sigma_theta_val, N)\n            phis_perturbed = phi_val + deltas\n            etas = np.random.normal(0, sigma_x_val, N)\n            \n            x_particles = phis_perturbed * x_particles + etas\n\n            # Compute log-weights based on observation y_t\n            log_weights = -0.5 * ((y_obs[t] - x_particles)**2) / tau_sq_val\n\n            # Normalize weights for stability\n            max_log_w = np.max(log_weights)\n            weights = np.exp(log_weights - max_log_w)\n            sum_weights = np.sum(weights)\n            if sum_weights == 0: # Handle potential underflow, though unlikely with stabilization\n                # This case indicates filter collapse, assign uniform weights\n                norm_weights = np.full(N, 1.0/N)\n            else:\n                norm_weights = weights / sum_weights\n\n            # Calculate score contribution g_t\n            log_weight_mean = np.sum(norm_weights * log_weights)\n            g_t = (1.0 / sigma_theta_sq_val) * np.sum(norm_weights * deltas * (log_weights - log_weight_mean))\n            s_hat += g_t\n\n            # Resample particles for the next iteration\n            indices = systematic_resample(norm_weights)\n            x_particles = x_particles[indices]\n            \n        return s_hat\n\n    # --- Main Execution Logic ---\n\n    # 1. Generate a single, fixed dataset for all experiments\n    y_fixed_data = generate_data(T_max, phi, np.sqrt(sigma_x_sq), np.sqrt(tau_sq), np.sqrt(x0_var))\n\n    v_norm_results = []\n    # 2. Iterate through test cases\n    for N, T in test_cases:\n        s_hat_replicates = np.zeros(R)\n        for r in range(R):\n            s_hat = run_if_replicate(\n                y_fixed_data, N, T, phi, \n                np.sqrt(sigma_x_sq), tau_sq, sigma_theta_sq, np.sqrt(x0_var)\n            )\n            s_hat_replicates[r] = s_hat\n        \n        # 3. Compute empirical variance and normalized variance\n        var_s = np.var(s_hat_replicates, ddof=1)\n        v_norm = var_s * N / T\n        v_norm_results.append(v_norm)\n\n    # 4. Calculate ratios relative to the baseline case (200, 200)\n    baseline_v_norm = v_norm_results[3]  # (200, 200) is the 4th case (index 3)\n    rho_results = [v / baseline_v_norm for v in v_norm_results]\n\n    # 5. Print the final result in the specified format\n    print(f\"[{','.join(f'{r:.6f}' for r in rho_results)}]\")\n\nsolve()\n\n```", "id": "3315217"}]}