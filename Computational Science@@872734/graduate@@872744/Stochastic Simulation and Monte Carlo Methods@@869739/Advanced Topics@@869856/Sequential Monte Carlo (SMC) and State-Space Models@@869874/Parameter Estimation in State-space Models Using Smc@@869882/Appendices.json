{"hands_on_practices": [{"introduction": "Our first hands-on exercise focuses on a cornerstone of parameter estimation in state-space models: using a particle filter to approximate the marginal likelihood. By evaluating this likelihood across a grid of candidate parameters, you will perform maximum likelihood estimation for a static parameter in a linear Gaussian model. This practice [@problem_id:3326835] is crucial for understanding the practical mechanics of a bootstrap particle filter and for exploring the impact of different resampling schemes on estimation accuracy.", "problem": "Consider a linear Gaussian state-space model for latent states $x_{t}$ and observations $y_{t}$ with known autoregressive coefficient $\\phi$ and process noise standard deviation $q$, and an unknown observation noise standard deviation $r$. The model is given by the following two equations:\n- State transition: $x_{t} = \\phi x_{t-1} + q \\,\\eta_{t}$ with $\\eta_{t} \\sim \\mathcal{N}(0,1)$.\n- Observation: $y_{t} = x_{t} + r \\,\\epsilon_{t}$ with $\\epsilon_{t} \\sim \\mathcal{N}(0,1)$.\n\nYour task is to implement a parameter estimation procedure for $r$ using Sequential Monte Carlo (SMC) with a bootstrap particle filter and to compare different resampling schemes and their properties in terms of parameter estimation accuracy.\n\nYou must base your work on the following fundamental principles and definitions:\n- The Markov property: $p(x_{t} \\mid x_{1:t-1}) = p(x_{t} \\mid x_{t-1})$ and conditional independence of observations: $p(y_{t} \\mid x_{1:t}) = p(y_{t} \\mid x_{t})$.\n- Bayes’ theorem for sequential inference: the filtering distribution $p(x_{t} \\mid y_{1:t})$ can be updated from $p(x_{t-1} \\mid y_{1:t-1})$ using the likelihood $p(y_{t} \\mid x_{t})$ and the transition $p(x_{t} \\mid x_{t-1})$.\n- The bootstrap particle filter draws particles from the transition $p(x_{t} \\mid x_{t-1})$ and updates weights proportionally to the likelihood $p(y_{t} \\mid x_{t})$. The marginal likelihood factorization $p(y_{1:T}) = \\prod_{t=1}^{T} p(y_{t} \\mid y_{1:t-1})$ follows from the chain rule of probability.\n- The effective sample size (ESS) is defined as $\\mathrm{ESS} = \\left(\\sum_{i=1}^{N} (w_{t}^{(i)})^2\\right)^{-1}$ where $w_{t}^{(i)}$ are normalized weights, and the resampling trigger is determined by $\\mathrm{ESS}/N \\le \\tau$ for a threshold $\\tau \\in [0,1]$.\n\nImplementation requirements:\n- Simulate a single dataset of length $T$ using a fixed random seed and the specified model parameters. Use the stationary prior $x_{0} \\sim \\mathcal{N}\\!\\left(0, \\frac{q^{2}}{1-\\phi^{2}}\\right)$ and simulate $x_{t}$ and $y_{t}$ according to the model.\n- Implement a bootstrap particle filter with $N$ particles that estimates the log marginal likelihood $\\log p(y_{1:T} \\mid r)$ for any supplied candidate value of $r$.\n- Implement four resampling schemes: multinomial, stratified, systematic, and residual resampling. When resampling occurs, replace particles by resampled ancestors and reset weights to be uniform.\n- Implement the resampling trigger using the ESS-based rule $\\mathrm{ESS}/N \\le \\tau$.\n- For parameter estimation, evaluate a grid of candidate observation noise standard deviations and select the maximizer of the estimated log marginal likelihood as the point estimate $\\hat{r}$.\n\nDataset generation protocol:\n- Use $\\phi = 0.9$, $q = 1.0$, $r_{\\text{true}} = 0.5$, $T = 200$.\n- Use a fixed dataset-generation seed $s_{\\text{data}} = 20240517$.\n- Generate $x_{0} \\sim \\mathcal{N}\\!\\left(0, \\frac{q^{2}}{1-\\phi^{2}}\\right)$, then simulate $\\{x_{t}\\}_{t=1}^{T}$ and $\\{y_{t}\\}_{t=1}^{T}$ accordingly.\n\nParticle filtering and resampling details:\n- Use $N = 1000$ particles.\n- At time $t$, propagate each particle via the transition density and compute weights proportional to the observation likelihood at $y_{t}$.\n- Compute the incremental normalizing constant at each time step as the average of unnormalized weights, and accumulate its logarithm to form the log marginal likelihood estimate.\n- Compute the ESS using the normalized weights and apply the resampling trigger $\\mathrm{ESS}/N \\le \\tau$.\n- Implement the following resampling schemes:\n  - Multinomial resampling.\n  - Stratified resampling.\n  - Systematic resampling.\n  - Residual resampling.\n\nParameter grid and reproducibility:\n- Use the candidate grid $\\mathcal{R} = \\{0.25, 0.5, 0.75, 1.0\\}$.\n- For reproducibility across test cases and candidate parameters, use a base seed $s_{\\text{base}} = 987654321$ and set the particle filter’s random seed for the candidate with index $k \\in \\{0,1,2,3\\}$ in test case $j \\in \\{0,1,2,3,4,5\\}$ to $s_{\\text{base}} + 1000\\,j + k$. This seed controls all internal randomness of the particle filter for that evaluation, including initial particles and all resampling random numbers.\n\nTest suite:\n- There are $6$ test cases, each a pair $(\\text{scheme}, \\tau)$:\n  1. $(\\text{multinomial}, 0.5)$\n  2. $(\\text{stratified}, 0.5)$\n  3. $(\\text{systematic}, 0.5)$\n  4. $(\\text{residual}, 0.5)$\n  5. $(\\text{multinomial}, 1.0)$\n  6. $(\\text{stratified}, 0.0)$\n\nFor each test case, compute $\\hat{r}$ by maximizing the estimated $\\log p(y_{1:T} \\mid r)$ over $r \\in \\mathcal{R}$ and report the absolute error $|\\hat{r} - r_{\\text{true}}|$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the $j$-th entry is the absolute error for the $j$-th test case, in the order listed above. For example, a valid output has the form $[\\text{err}_{1},\\text{err}_{2},\\ldots,\\text{err}_{6}]$.", "solution": "The problem presented is a valid and well-posed exercise in computational statistics, specifically focusing on parameter estimation within a linear Gaussian state-space model using Sequential Monte Carlo (SMC) methods. All necessary parameters, models, and procedural details are provided, and the problem is scientifically grounded in the principles of Bayesian inference and stochastic simulation. We will proceed with a full solution.\n\nThe core of the problem is to estimate a static parameter, the observation noise standard deviation $r$, of a state-space model. The model is defined by a state transition equation and an observation equation.\n\nThe state transition equation describes the evolution of a latent (unobserved) state $x_t$ over time $t$:\n$$x_{t} = \\phi x_{t-1} + q \\,\\eta_{t}, \\quad \\eta_{t} \\sim \\mathcal{N}(0,1)$$\nThis is an autoregressive process of order $1$ (AR($1$)) with a known coefficient $\\phi$ and process noise of standard deviation $q$.\n\nThe observation equation links the latent state $x_t$ to the observed data $y_t$:\n$$y_{t} = x_{t} + r \\,\\epsilon_{t}, \\quad \\epsilon_{t} \\sim \\mathcal{N}(0,1)$$\nThe observations are a noisy version of the state, where the observation noise has an unknown standard deviation $r$.\n\nOur goal is to find an estimate $\\hat{r}$ for $r$ given a sequence of observations $y_{1:T} = \\{y_1, y_2, \\dots, y_T\\}$. A standard and powerful method for this is maximum likelihood estimation. We aim to find the value of $r$ that maximizes the marginal likelihood of the observations, $p(y_{1:T} \\mid r)$.\n$$\\hat{r} = \\underset{r}{\\arg\\max} \\, p(y_{1:T} \\mid r) = \\underset{r}{\\arg\\max} \\, \\log p(y_{1:T} \\mid r)$$\nFor state-space models, the marginal likelihood is generally intractable to compute directly, as it requires integrating out the latent states:\n$$p(y_{1:T} \\mid r) = \\int p(y_{1:T}, x_{0:T} \\mid r) \\,dx_{0:T}$$\nSMC methods, also known as particle filters, provide a way to approximate this quantity. The key idea of a particle filter is to represent the filtering distribution $p(x_t \\mid y_{1:t})$ with a set of $N$ weighted samples or \"particles\" $\\{x_t^{(i)}, W_t^{(i)}\\}_{i=1}^N$. The distribution is then approximated by an empirical measure:\n$$p(x_t \\mid y_{1:t}) \\approx \\sum_{i=1}^N W_t^{(i)} \\delta(x_t - x_t^{(i)})$$\nwhere $\\delta(\\cdot)$ is the Dirac delta function.\n\nThe bootstrap particle filter is a specific type of SMC algorithm that is employed here. It operates sequentially through time, updating the particle set as each new observation $y_t$ arrives. It leverages the factorization of the marginal likelihood:\n$$\\log p(y_{1:T} \\mid r) = \\sum_{t=1}^T \\log p(y_t \\mid y_{1:t-1}, r)$$\nThe particle filter provides a Monte Carlo estimate for each term $p(y_t \\mid y_{1:t-1}, r)$.\n\nThe algorithm proceeds as follows:\n1.  **Initialization ($t=0$):**\n    We start by sampling $N$ particles $\\{x_0^{(i)}\\}_{i=1}^N$ from the prior distribution $p(x_0)$. The problem specifies using the stationary distribution of the AR($1$) process, which is $x_0 \\sim \\mathcal{N}(0, q^2/(1-\\phi^2))$. The initial weights are uniform, $W_0^{(i)} = 1/N$ for all $i \\in \\{1, \\dots, N\\}$.\n\n2.  **Sequential Update (for $t=1, \\dots, T$):**\n    For each time step, the following operations are performed:\n    a. **Propagation:** Each particle is evolved according to the state transition model. Since the bootstrap filter uses the transition dynamics as its proposal, we sample:\n    $$x_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(i)}) = \\mathcal{N}(x_t; \\phi x_{t-1}^{(i)}, q^2)$$\n    b. **Weighting:** The importance weight for each new particle $x_t^{(i)}$ is calculated based on how well it explains the new observation $y_t$. The unnormalized weight $\\tilde{w}_t^{(i)}$ is given by the likelihood of the observation given the particle's state:\n    $$\\tilde{w}_t^{(i)} = p(y_t \\mid x_t^{(i)}, r) = \\frac{1}{\\sqrt{2\\pi}r} \\exp\\left(-\\frac{(y_t - x_t^{(i)})^2}{2r^2}\\right)$$\n    c. **Likelihood Estimation:** The predictive likelihood $p(y_t \\mid y_{1:t-1}, r)$ is approximated by the average of the unnormalized weights:\n    $$\\hat{p}(y_t \\mid y_{1:t-1}, r) \\approx \\frac{1}{N} \\sum_{i=1}^N \\tilde{w}_t^{(i)}$$\n    The logarithm of this value is added to the total log marginal likelihood estimate.\n    d. **Normalization:** The weights are normalized to sum to $1$:\n    $$W_t^{(i)} = \\frac{\\tilde{w}_t^{(i)}}{\\sum_{j=1}^N \\tilde{w}_t^{(j)}}$$\n    e. **Resampling:** A common issue in particle filtering is weight degeneracy, where after a few steps, one particle has a weight close to $1$ and all others have weights close to $0$. This is monitored using the Effective Sample Size (ESS), estimated as:\n    $$\\mathrm{ESS} = \\left(\\sum_{i=1}^{N} (W_{t}^{(i)})^2\\right)^{-1}$$\n    If the ESS falls below a certain fraction $\\tau$ of the total number of particles, i.e., $\\mathrm{ESS}/N \\le \\tau$, a resampling step is performed. Resampling involves drawing $N$ new particles with replacement from the current set $\\{x_t^{(i)}\\}_{i=1}^N$, where the probability of drawing particle $i$ is its weight $W_t^{(i)}$. This eliminates low-weight particles and multiplies high-weight ones. After resampling, the weights are reset to uniform, $W_t^{(i)} = 1/N$.\n\nThe problem requires comparing four resampling schemes:\n*   **Multinomial Resampling:** This is the most straightforward method, equivalent to drawing $N$ samples from a categorical distribution defined by the weights.\n*   **Systematic Resampling:** This scheme reduces the Monte Carlo variance of the resampling step. It generates a single random number $u \\in [0, 1/N)$ and then creates a sequence of $N$ ordered pointers $u, u+1/N, \\dots, u+(N-1)/N$. These pointers are used to select particles from the cumulative distribution of weights.\n*   **Stratified Resampling:** This is a related variance-reduction technique. It divides the interval $[0, 1)$ into $N$ strata and draws one random sample from each stratum. These $N$ samples then act as pointers, as in the systematic case.\n*   **Residual Resampling:** This is a hybrid method. It first deterministically duplicates particles with weights $W_t^{(i)} > 1/N$. Specifically, each particle $i$ gets $\\lfloor N W_t^{(i)} \\rfloor$ copies. The remaining number of particles are then drawn via multinomial resampling using the residual parts of the weights.\n\nTo implement the solution, a single dataset of length $T=200$ is first generated using the true parameters $\\phi=0.9$, $q=1.0$, and $r_{\\text{true}}=0.5$, with a specified random seed for reproducibility. Then, for each test case defined by a pair of (resampling scheme, threshold $\\tau$), the log marginal likelihood $\\log \\hat{p}(y_{1:T} \\mid r)$ is estimated for each candidate $r$ in the grid $\\mathcal{R}=\\{0.25, 0.5, 0.75, 1.0\\}$. The candidate $\\hat{r}$ that yields the highest log-likelihood is chosen as the estimate. The absolute error $|\\hat{r} - r_{\\text{true}}|$ is then computed and reported. The seeding protocol ensures that each evaluation of the particle filter is deterministic. The test cases explore the behavior of different resampling schemes and trigger thresholds: $\\tau=0.5$ is a common choice, $\\tau=1.0$ forces resampling at every step, and $\\tau=0.0$ prevents resampling entirely.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\n#\n# --- Problem Constants and Definitions ---\n#\n\n# Dataset parameters\nPHI = 0.9\nQ = 1.0\nR_TRUE = 0.5\nT = 200\nDATA_SEED = 20240517\n\n# Particle filter parameters\nN_PARTICLES = 1000\nR_GRID = [0.25, 0.5, 0.75, 1.0]\n\n# Reproducibility parameters\nBASE_SEED = 987654321\n\n# Test cases: (resampling_scheme_name, resampling_threshold_tau)\nTEST_CASES = [\n    ('multinomial', 0.5),\n    ('stratified', 0.5),\n    ('systematic', 0.5),\n    ('residual', 0.5),\n    ('multinomial', 1.0),\n    ('stratified', 0.0),\n]\n\n\n#\n# --- Data Generation ---\n#\n\ndef generate_data(phi, q, r_true, T, seed):\n    \"\"\"\n    Generates a single time series dataset from the linear Gaussian state-space model.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Initial state from stationary distribution\n    std_x0 = q / np.sqrt(1 - phi**2)\n    x = np.zeros(T + 1)\n    y = np.zeros(T)\n    x[0] = rng.normal(0, std_x0)\n    \n    # Simulate states and observations for t = 1 to T\n    for t in range(1, T + 1):\n        x[t] = phi * x[t-1] + q * rng.normal()\n        y[t-1] = x[t] + r_true * rng.normal()\n        \n    return y\n\n\n#\n# --- Resampling Schemes ---\n#\n\ndef multinomial_resample(weights, num_samples, rng):\n    \"\"\"Performs multinomial resampling.\"\"\"\n    return rng.choice(len(weights), size=num_samples, p=weights, replace=True)\n\ndef systematic_resample(weights, num_samples, rng):\n    \"\"\"Performs systematic resampling.\"\"\"\n    positions = (rng.random() + np.arange(num_samples)) / num_samples\n    cumulative_weights = np.cumsum(weights)\n    return np.searchsorted(cumulative_weights, positions)\n\ndef stratified_resample(weights, num_samples, rng):\n    \"\"\"Performs stratified resampling.\"\"\"\n    positions = (rng.random(size=num_samples) + np.arange(num_samples)) / num_samples\n    cumulative_weights = np.cumsum(weights)\n    return np.searchsorted(cumulative_weights, positions)\n\ndef residual_resample(weights, num_samples, rng):\n    \"\"\"Performs residual resampling.\"\"\"\n    num_copies = np.floor(num_samples * weights).astype(int)\n    num_residual = num_samples - np.sum(num_copies)\n    \n    indices = np.repeat(np.arange(num_samples), num_copies)\n    \n    if num_residual > 0:\n        residual_weights = (num_samples * weights) - num_copies\n        # Normalize residual weights to form a probability distribution\n        residual_sum = np.sum(residual_weights)\n        if residual_sum > 1e-12:\n            residual_weights /= residual_sum\n            residual_indices = rng.choice(num_samples, size=num_residual, p=residual_weights, replace=True)\n            indices = np.concatenate([indices, residual_indices])\n\n    return indices\n\nRESAMPLING_SCHEMES = {\n    'multinomial': multinomial_resample,\n    'stratified': stratified_resample,\n    'systematic': systematic_resample,\n    'residual': residual_resample,\n}\n\n\n#\n# --- Particle Filter Implementation ---\n#\n\ndef run_particle_filter(obs, phi, q, r_candidate, N, scheme, tau, seed):\n    \"\"\"\n    Runs a bootstrap particle filter to estimate the log marginal likelihood.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    T_obs = len(obs)\n    \n    # 1. Initialization\n    std_x0 = q / np.sqrt(1 - phi**2)\n    particles = rng.normal(0, std_x0, N)\n    log_likelihood = 0.0\n    \n    # 2. Sequential Importance Sampling and Resampling\n    for t in range(T_obs):\n        # a. Propagate particles\n        particles = phi * particles + q * rng.normal(size=N)\n\n        # b. Compute unnormalized weights\n        unnormalized_weights = norm.pdf(obs[t], loc=particles, scale=r_candidate)\n\n        # c. Update log-likelihood estimate\n        mean_weight = np.mean(unnormalized_weights)\n        if mean_weight > 1e-100: # Avoid log(0)\n            log_likelihood += np.log(mean_weight)\n        else:\n            return -np.inf # Log-likelihood is effectively -infinity\n\n        # d. Normalize weights\n        sum_weights = np.sum(unnormalized_weights)\n        if sum_weights  1e-100: # All particles have zero weight\n            # This can happen if r_candidate is very wrong. Re-initialize to avoid crash.\n            particles = rng.normal(0, std_x0, N)\n            normalized_weights = np.full(N, 1.0 / N)\n        else:\n            normalized_weights = unnormalized_weights / sum_weights\n\n        # e. Resampling\n        ess = 1.0 / np.sum(normalized_weights**2)\n        if (ess / N) = tau:\n            resampling_func = RESAMPLING_SCHEMES[scheme]\n            indices = resampling_func(normalized_weights, N, rng)\n            particles = particles[indices]\n            # Weights are reset to uniform implicitly for the next propagation step\n            # because the next unnormalized weights will be calculated from a\n            # uniformly weighted particle set. We only need to store the normalized\n            # weights for the ESS check for the current step.\n\n    return log_likelihood\n\n\n#\n# --- Main Solver ---\n#\n\ndef solve():\n    \"\"\"\n    Main function to execute the problem specification.\n    \"\"\"\n    y_obs = generate_data(PHI, Q, R_TRUE, T, DATA_SEED)\n    \n    results = []\n\n    for j, (scheme, tau) in enumerate(TEST_CASES):\n        log_likelihoods = []\n        for k, r_candidate in enumerate(R_GRID):\n            \n            # Set seed for this specific run for reproducibility\n            run_seed = BASE_SEED + 1000 * j + k\n            \n            log_lik = run_particle_filter(\n                obs=y_obs,\n                phi=PHI,\n                q=Q,\n                r_candidate=r_candidate,\n                N=N_PARTICLES,\n                scheme=scheme,\n                tau=tau,\n                seed=run_seed\n            )\n            log_likelihoods.append(log_lik)\n        \n        # Find the r that maximizes the log-likelihood\n        best_r_index = np.argmax(log_likelihoods)\n        r_hat = R_GRID[best_r_index]\n        \n        # Calculate absolute error\n        error = abs(r_hat - R_TRUE)\n        results.append(error)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solver\nsolve()\n\n```", "id": "3326835"}, {"introduction": "Having learned to estimate parameters, we now turn to the crucial question of model validation. An estimate is only as good as the model it comes from, so we must have tools to diagnose misspecification. This practice [@problem_id:3326831] guides you through implementing a powerful posterior predictive check, where you will use particle filter outputs to generate predictive residuals and assess their statistical properties, thereby learning to detect when a model's underlying assumptions are inconsistent with the observed data.", "problem": "Consider a univariate linear Gaussian state-space model with latent states and observations defined for time index $t \\in \\{1,2,\\dots,T\\}$ by the relations\n$$\nx_t = a \\, x_{t-1} + \\sqrt{q} \\, \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0,1),\n$$\n$$\ny_t = c \\, x_t + \\sqrt{r} \\, \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0,1),\n$$\nwhere $(a,q,r,c)$ are unknown parameters collected into $\\theta = (a,q,r,c)$, $x_t$ are latent states and $y_t$ are observations. Let $y_{1:t} = (y_1,\\dots,y_t)$ denote the observed sequence up to time $t$, and let $\\mathcal{N}(0,1)$ denote the standard normal distribution.\n\nSequential Monte Carlo (SMC) methods, and in particular the Particle Filter (PF), approximate the filtering distribution $p(x_t \\mid y_{1:t}, \\theta)$ by a weighted ensemble of particles propagated through the latent dynamics and reweighted by the likelihood of observations. Under correct model specification, the one-step-ahead predictive distribution $p(y_t \\mid y_{1:t-1}, \\theta)$ is consistent with the realized $y_t$. A principled way to diagnose misspecification is to examine predictive residuals constructed from PF-based predictive moments and perform posterior predictive checks.\n\nYour task is to implement an SMC-based posterior predictive check that compares PF residuals across samples of $\\theta$ and flags systematic discrepancies in latent dynamics. Starting from the following foundational bases:\n\n- The state-space model definitions above and the conditional independence structure they imply.\n- The law of total expectation and variance applied to predictive moments, and the definition of the Probability Integral Transform (PIT): if $Y$ has cumulative distribution function $F_Y$, then $U = F_Y(Y)$ is uniform on $(0,1)$ under correct specification.\n- The principle that standardized innovations under a correctly specified linear Gaussian model are approximately independent and identically distributed standard normal, and hence the PIT values are approximately independent and identically distributed uniform on $(0,1)$.\n\nProceed as follows for each candidate parameter vector $\\theta$:\n\n- Use a bootstrap Particle Filter (PF) with multinomial or systematic resampling to approximate $p(x_{t-1} \\mid y_{1:t-1}, \\theta)$ with $N$ particles. From this empirical predictive set, form the one-step-ahead predictive moments for $y_t$ given $y_{1:t-1}$,\n$$\n\\hat{\\mu}_t(\\theta) = c \\, \\mathbb{E}_{\\text{PF}}[x_t \\mid y_{1:t-1}, \\theta], \\quad \\hat{\\sigma}^2_t(\\theta) = c^2 \\, \\operatorname{Var}_{\\text{PF}}[x_t \\mid y_{1:t-1}, \\theta] + r,\n$$\nwhere the expectation and variance are computed from the PF-predicted particles for $x_t$ prior to seeing $y_t$.\n- Define the standardized one-step-ahead predictive residual\n$$\nz_t(\\theta) = \\frac{y_t - \\hat{\\mu}_t(\\theta)}{\\hat{\\sigma}_t(\\theta)},\n$$\nand the Probability Integral Transform\n$$\nu_t(\\theta) = \\Phi\\!\\left(z_t(\\theta)\\right),\n$$\nwhere $\\Phi$ is the cumulative distribution function of $\\mathcal{N}(0,1)$. Under correct specification, the sequence $\\{u_t(\\theta)\\}_{t=1}^T$ should be approximately independent and identically distributed uniform on $(0,1)$.\n- Construct a discrepancy statistic that jointly measures marginal uniformity and temporal independence. Let $u_{(1)} \\le \\dots \\le u_{(T)}$ be the ordered PIT values. Define the two-sided Kolmogorov–Smirnov distance\n$$\nD_{\\mathrm{KS}}(\\theta) = \\max\\!\\left\\{\\max_{1 \\le k \\le T} \\left(\\frac{k}{T} - u_{(k)}\\right), \\; \\max_{1 \\le k \\le T} \\left(u_{(k)} - \\frac{k-1}{T}\\right)\\right\\},\n$$\nand the lag-$1$ autocorrelation of the centered PIT sequence\n$$\n\\rho_1(\\theta) = \\frac{\\sum_{t=2}^T \\left(u_t(\\theta) - \\bar{u}(\\theta)\\right)\\left(u_{t-1}(\\theta) - \\bar{u}(\\theta)\\right)}{\\sum_{t=1}^T \\left(u_t(\\theta) - \\bar{u}(\\theta)\\right)^2},\n$$\nwhere $\\bar{u}(\\theta) = \\frac{1}{T}\\sum_{t=1}^T u_t(\\theta)$. Finally, define the combined discrepancy\n$$\nD(\\theta) = D_{\\mathrm{KS}}(\\theta) + \\left|\\rho_1(\\theta)\\right|.\n$$\n- Flag $\\theta$ as misspecified if $D(\\theta)  \\tau$ for a pre-specified threshold $\\tau$.\n\nImplement the above check for each of the test cases described below. In each case, generate synthetic data by simulating the latent and observed sequences under a specified true parameter $\\theta^\\star$ and a fixed random seed. Then evaluate the discrepancy $D(\\theta)$ across a set of candidate $\\theta$ values using the PF with the specified number of particles $N$ and the specified threshold $\\tau$. Use an initial distribution $x_0 \\sim \\mathcal{N}\\!\\left(0, \\sigma_0^2(\\theta)\\right)$ with $\\sigma_0^2(\\theta) = \\frac{q}{1-a^2}$ when $a^2  1$; if $a^2 \\ge 1$, set $\\sigma_0^2(\\theta) = 10\\,q$.\n\nTest suite:\n\n- Case $1$ (general case, correctly specified dynamics included):\n  - True parameters $\\theta^\\star = (a^\\star,q^\\star,r^\\star,c^\\star) = (0.9, 0.16, 0.04, 1.0)$.\n  - Time length $T = 300$.\n  - Particle count $N = 1500$.\n  - Threshold $\\tau = 0.12$.\n  - Random seed $s = 12345$.\n  - Candidate parameters list:\n    - $\\theta_1 = (0.9, 0.16, 0.04, 1.0)$,\n    - $\\theta_2 = (0.88, 0.16, 0.04, 1.0)$,\n    - $\\theta_3 = (0.5, 0.16, 0.04, 1.0)$.\n- Case $2$ (strongly misspecified dynamics among candidates):\n  - True parameters $\\theta^\\star = (0.2, 0.25, 0.09, 1.0)$.\n  - Time length $T = 300$.\n  - Particle count $N = 1500$.\n  - Threshold $\\tau = 0.12$.\n  - Random seed $s = 54321$.\n  - Candidate parameters list:\n    - $\\theta_1 = (0.2, 0.25, 0.09, 1.0)$,\n    - $\\theta_2 = (0.9, 0.25, 0.09, 1.0)$,\n    - $\\theta_3 = (0.2, 0.05, 0.09, 1.0)$.\n- Case $3$ (boundary dynamics near unit root):\n  - True parameters $\\theta^\\star = (0.99, 0.01, 0.01, 1.0)$.\n  - Time length $T = 300$.\n  - Particle count $N = 2000$.\n  - Threshold $\\tau = 0.12$.\n  - Random seed $s = 999$.\n  - Candidate parameters list:\n    - $\\theta_1 = (0.99, 0.01, 0.01, 1.0)$,\n    - $\\theta_2 = (0.9, 0.01, 0.01, 1.0)$,\n    - $\\theta_3 = (0.99, 0.25, 0.01, 1.0)$.\n\nProgram requirements:\n\n- Your program must generate the synthetic data for each case using $\\theta^\\star$, then, for each candidate $\\theta$ in that case, run the PF to compute $\\{u_t(\\theta)\\}_{t=1}^T$, evaluate $D(\\theta)$, and flag the candidate if $D(\\theta)  \\tau$.\n- For each case, compute the integer count of flagged candidates.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, namely the counts of flagged candidates for cases $1$, $2$, and $3$ in order (e.g., $[n_1,n_2,n_3]$).\n- Angles are not present, and physical units are not applicable in this purely mathematical formulation; no unit conversion is required.", "solution": "The user-provided problem is valid. It presents a well-defined task in computational statistics, specifically concerning model validation for state-space models using Sequential Monte Carlo (SMC) methods. The problem is scientifically grounded in the theory of particle filters and posterior predictive checking, is self-contained with all necessary parameters and specifications, and is expressed in objective, formal language. All criteria for a valid problem are met.\n\nThe solution involves implementing a procedure to calculate a discrepancy statistic, $D(\\theta)$, for a set of candidate parameters $\\theta$ against a time series of observations $y_{1:T}$. This procedure is built upon a bootstrap particle filter. For each of the three test cases provided, we first generate a synthetic observation sequence using the specified true parameters $\\theta^\\star$. Then, for each candidate parameter vector $\\theta$ in the test case's list, we evaluate its suitability by computing $D(\\theta)$ and comparing it to a threshold $\\tau$.\n\n### 1. Synthetic Data Generation\nFor each test case, a synthetic observation sequence $y_{1:T} = (y_1, \\dots, y_T)$ of length $T$ is generated from the true model specified by parameters $\\theta^\\star = (a^\\star, q^\\star, r^\\star, c^\\star)$. The process begins by drawing an initial state $x_0$ from its stationary distribution. For a stable process where $(a^\\star)^2  1$, this is $x_0 \\sim \\mathcal{N}(0, \\sigma_0^2(\\theta^\\star))$ with variance $\\sigma_0^2(\\theta^\\star) = q^\\star / (1 - (a^\\star)^2)$. If the process is non-stationary ($(a^\\star)^2 \\ge 1$), the initial variance is set to $\\sigma_0^2(\\theta^\\star) = 10q^\\star$. Subsequently, the latent states $x_t$ and observations $y_t$ for $t \\in \\{1, \\dots, T\\}$ are simulated according to the model equations:\n$$\nx_t = a^\\star x_{t-1} + \\sqrt{q^\\star} \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0,1)\n$$\n$$\ny_t = c^\\star x_t + \\sqrt{r^\\star} \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0,1)\n$$\nA fixed random seed is used for each case to ensure the entire process, including data generation and subsequent particle filtering, is reproducible.\n\n### 2. Discrepancy Calculation via Particle Filter\nFor a given candidate parameter set $\\theta = (a, q, r, c)$ and the observation sequence $y_{1:T}$, the discrepancy is computed using a bootstrap particle filter with $N$ particles. At each time step $t \\in \\{1, \\dots, T\\}$, the following operations are performed:\n\n**a. Predictive Moment Estimation**: At the beginning of step $t$, we have a set of particles $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ that provides a Monte Carlo approximation of the filtering distribution $p(x_{t-1} \\mid y_{1:t-1}, \\theta)$. Since systematic resampling is performed at the end of every step, these particles are equally weighted. The empirical mean $\\mathbb{E}_{\\text{PF}}[x_{t-1}]$ and variance $\\operatorname{Var}_{\\text{PF}}[x_{t-1}]$ are calculated from this particle set. Using the law of total expectation and variance, these moments are propagated forward to obtain the predictive moments for $x_t$:\n$$\n\\mathbb{E}_{\\text{PF}}[x_t \\mid y_{1:t-1}, \\theta] = a \\, \\mathbb{E}_{\\text{PF}}[x_{t-1}]\n$$\n$$\n\\operatorname{Var}_{\\text{PF}}[x_t \\mid y_{1:t-1}, \\theta] = a^2 \\operatorname{Var}_{\\text{PF}}[x_{t-1}] + q\n$$\nFrom these, we derive the one-step-ahead predictive moments for the observation $y_t$:\n$$\n\\hat{\\mu}_t(\\theta) = c \\, \\mathbb{E}_{\\text{PF}}[x_t \\mid y_{1:t-1}, \\theta]\n$$\n$$\n\\hat{\\sigma}^2_t(\\theta) = c^2 \\operatorname{Var}_{\\text{PF}}[x_t \\mid y_{1:t-1}, \\theta] + r\n$$\n\n**b. Residual and PIT Calculation**: Using the predictive moments, the standardized one-step-ahead predictive residual $z_t(\\theta)$ is computed:\n$$\nz_t(\\theta) = \\frac{y_t - \\hat{\\mu}_t(\\theta)}{\\sqrt{\\hat{\\sigma}^2_t(\\theta)}}\n$$\nThis residual is then transformed into a Probability Integral Transform (PIT) value using the standard normal cumulative distribution function $\\Phi$:\n$$\nu_t(\\theta) = \\Phi(z_t(\\theta))\n$$\nIf the model is correctly specified, the sequence of PIT values $\\{u_t(\\theta)\\}_{t=1}^T$ should be approximately independent and identically distributed according to a uniform distribution on $(0,1)$.\n\n**c. Particle Filter Update**: The particle set is updated to approximate $p(x_t \\mid y_{1:t}, \\theta)$. This involves three steps:\n1.  **Propagation**: Each particle $x_{t-1}^{(i)}$ is propagated forward using the state equation to obtain a predicted particle $\\tilde{x}_t^{(i)} = a x_{t-1}^{(i)} + \\sqrt{q} \\epsilon_t^{(i)}$.\n2.  **Weighting**: The importance weight of each particle $\\tilde{x}_t^{(i)}$ is calculated based on the likelihood of observing $y_t$, i.e., $w_t^{(i)} \\propto p(y_t \\mid \\tilde{x}_t^{(i)}, \\theta) = \\mathcal{N}(y_t; c\\tilde{x}_t^{(i)}, r)$. The weights are then normalized.\n3.  **Resampling**: A new set of $N$ particles $\\{x_t^{(i)}\\}_{i=1}^N$ is drawn with replacement from the propagated set $\\{\\tilde{x}_t^{(j)}\\}_{j=1}^N$ according to the normalized weights $\\{w_t^{(j)}\\}_{j=1}^N$. Systematic resampling is used to reduce the variance of the Monte Carlo estimate. The new particles are equally weighted and serve as the basis for the next time step.\n\n### 3. Discrepancy Statistic and Flagging\nAfter iterating through all $T$ observations and collecting the PIT sequence $\\{u_t(\\theta)\\}_{t=1}^T$, the combined discrepancy statistic $D(\\theta)$ is constructed. This statistic measures deviations from the ideal i.i.d. uniform property of the PIT values.\n- The **Kolmogorov-Smirnov distance** $D_{\\mathrm{KS}}(\\theta)$ measures the deviation from marginal uniformity by comparing the empirical distribution function of the PITs to the uniform CDF:\n$$\nD_{\\mathrm{KS}}(\\theta) = \\max\\!\\left\\{\\max_{1 \\le k \\le T} \\left(\\frac{k}{T} - u_{(k)}\\right), \\; \\max_{1 \\le k \\le T} \\left(u_{(k)} - \\frac{k-1}{T}\\right)\\right\\}\n$$\nwhere $u_{(k)}$ are the sorted PIT values.\n- The **lag-$1$ autocorrelation** $\\rho_1(\\theta)$ measures temporal dependence:\n$$\n\\rho_1(\\theta) = \\frac{\\sum_{t=2}^T \\left(u_t(\\theta) - \\bar{u}(\\theta)\\right)\\left(u_{t-1}(\\theta) - \\bar{u}(\\theta)\\right)}{\\sum_{t=1}^T \\left(u_t(\\theta) - \\bar{u}(\\theta)\\right)^2}\n$$\nThe final discrepancy is the sum $D(\\theta) = D_{\\mathrm{KS}}(\\theta) + |\\rho_1(\\theta)|$. A candidate parameter vector $\\theta$ is flagged as misspecified if its discrepancy exceeds a given threshold, i.e., if $D(\\theta)  \\tau$.\n\nThe full procedure is applied to each candidate $\\theta$ in each test case, and the number of flagged candidates per case is counted to produce the final result.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the SMC-based posterior predictive check for all test cases.\n    \"\"\"\n\n    def get_initial_variance(a, q):\n        \"\"\"Calculates the variance for the initial state distribution.\"\"\"\n        if a**2  1:\n            return q / (1 - a**2)\n        else:\n            return 10 * q\n\n    def systematic_resample(particles, weights, rng):\n        \"\"\"Performs systematic resampling.\"\"\"\n        N = len(particles)\n        positions = (np.arange(N) + rng.random()) / N\n        cumulative_sum = np.cumsum(weights)\n        indexes = np.searchsorted(cumulative_sum, positions)\n        return particles[indexes]\n\n    def generate_data(theta_star, T, rng):\n        \"\"\"Generates synthetic data from the state-space model.\"\"\"\n        a_star, q_star, r_star, c_star = theta_star\n        \n        x = np.zeros(T)\n        y = np.zeros(T)\n        \n        # Initial state x0 from its stationary distribution\n        sigma0_sq_star = get_initial_variance(a_star, q_star)\n        x0 = rng.normal(0, np.sqrt(sigma0_sq_star))\n        \n        # Generate state and observation sequences\n        x_prev = x0\n        for t in range(T):\n            x[t] = a_star * x_prev + np.sqrt(q_star) * rng.normal()\n            y[t] = c_star * x[t] + np.sqrt(r_star) * rng.normal()\n            x_prev = x[t]\n            \n        return y\n\n    def calculate_discrepancy(theta, y_obs, N, rng):\n        \"\"\"\n        Calculates the discrepancy statistic D(theta) for a candidate parameter set.\n        \"\"\"\n        a, q, r, c = theta\n        T = len(y_obs)\n        \n        u_values = np.zeros(T)\n\n        # Initialize particles for x_0 from its stationary distribution under theta\n        sigma0_sq = get_initial_variance(a, q)\n        particles_prev = rng.normal(0, np.sqrt(sigma0_sq), N)\n\n        for t in range(T):\n            y_t = y_obs[t]\n            \n            # 1. Compute one-step-ahead predictive moments for y_t\n            mean_x_prev = np.mean(particles_prev)\n            var_x_prev = np.var(particles_prev)\n            \n            pred_mean_x_t = a * mean_x_prev\n            pred_var_x_t = a**2 * var_x_prev + q\n            \n            mu_hat_t = c * pred_mean_x_t\n            sigma2_hat_t = c**2 * pred_var_x_t + r\n            if sigma2_hat_t = 0: sigma2_hat_t = 1e-9 # Avoid sqrt of non-positive\n            sigma_hat_t = np.sqrt(sigma2_hat_t)\n            \n            # 2. Define standardized residual and Probability Integral Transform (PIT)\n            z_t = (y_t - mu_hat_t) / sigma_hat_t\n            u_t = norm.cdf(z_t)\n            u_values[t] = u_t\n            \n            # 3. Particle filter update (propagate, weight, resample)\n            particles_propagated = a * particles_prev + np.sqrt(q) * rng.normal(size=N)\n            \n            log_weights = -0.5 * ((y_t - c * particles_propagated)**2 / r)\n            log_weights -= np.max(log_weights)\n            weights = np.exp(log_weights)\n            weights_sum = np.sum(weights)\n            \n            if weights_sum == 0:\n                weights = np.ones(N) / N\n            else:\n                weights /= weights_sum\n            \n            particles_curr = systematic_resample(particles_propagated, weights, rng)\n            particles_prev = particles_curr\n\n        # 4. Construct discrepancy statistic D(theta)\n        # Kolmogorov-Smirnov distance\n        u_sorted = np.sort(u_values)\n        k = np.arange(1, T + 1)\n        d_ks = np.max([\n            np.max(k / T - u_sorted),\n            np.max(u_sorted - (k - 1) / T)\n        ])\n        \n        # Lag-1 autocorrelation\n        u_bar = np.mean(u_values)\n        centered_u = u_values - u_bar\n        denominator = np.sum(centered_u**2)\n        if denominator == 0:\n            rho_1 = 0\n        else:\n            numerator = np.sum(centered_u[1:] * centered_u[:-1])\n            rho_1 = numerator / denominator\n        \n        # Combined discrepancy\n        D = d_ks + np.abs(rho_1)\n        \n        return D\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"theta_star\": (0.9, 0.16, 0.04, 1.0), \"T\": 300, \"N\": 1500, \"tau\": 0.12, \"seed\": 12345,\n            \"candidates\": [(0.9, 0.16, 0.04, 1.0), (0.88, 0.16, 0.04, 1.0), (0.5, 0.16, 0.04, 1.0)]\n        },\n        {\n            \"theta_star\": (0.2, 0.25, 0.09, 1.0), \"T\": 300, \"N\": 1500, \"tau\": 0.12, \"seed\": 54321,\n            \"candidates\": [(0.2, 0.25, 0.09, 1.0), (0.9, 0.25, 0.09, 1.0), (0.2, 0.05, 0.09, 1.0)]\n        },\n        {\n            \"theta_star\": (0.99, 0.01, 0.01, 1.0), \"T\": 300, \"N\": 2000, \"tau\": 0.12, \"seed\": 999,\n            \"candidates\": [(0.99, 0.01, 0.01, 1.0), (0.9, 0.01, 0.01, 1.0), (0.99, 0.25, 0.01, 1.0)]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        theta_star, T, N, tau, seed, candidates = case.values()\n        \n        # A single random number generator for the entire case ensures reproducibility\n        rng = np.random.default_rng(seed)\n        \n        y_obs = generate_data(theta_star, T, rng)\n        \n        flagged_count = 0\n        for theta_cand in candidates:\n            # The same rng is passed to maintain a consistent random number stream\n            D_val = calculate_discrepancy(theta_cand, y_obs, N, rng)\n            if D_val > tau:\n                flagged_count += 1\n        \n        results.append(flagged_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3326831"}, {"introduction": "This advanced exercise brings together all the concepts by having you implement the sophisticated Sequential Monte Carlo Squared (SMC$^2$) algorithm. By building this nested filter, which performs sequential inference on the static parameters themselves, and comparing its output to a Kalman filter baseline, you will gain a deep, practical understanding of a state-of-the-art method for online Bayesian parameter estimation [@problem_id:3326834]. This practice serves as a capstone, transitioning from simple likelihood evaluation to full posterior approximation in a sequential setting.", "problem": "Implement a complete, runnable program that estimates a static parameter in a linear Gaussian state-space model using Sequential Monte Carlo (SMC) methods for static parameters and Sequential Monte Carlo Squared (SMC$^2$). The goal is to infer the autoregressive coefficient from noisy observations and to validate the SMC-based estimators against numerically stable baselines derived from the Kalman filter and numerical integration.\n\nConsider the following state-space model. The latent state sequence $\\{x_t\\}_{t=0}^T$ and observations $\\{y_t\\}_{t=1}^T$ satisfy\n$$\nx_0 \\sim \\mathcal{N}(0, s_0^2), \\quad x_t = \\phi x_{t-1} + \\sigma_w \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0,1), \\\\\ny_t = x_t + \\sigma_v \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0,1),\n$$\nwhere $\\phi \\in (-1,1)$ is an unknown static parameter to be estimated, and $\\sigma_w  0$, $\\sigma_v  0$, $s_0  0$ are known constants. Assume a uniform prior for $\\phi$ on $(-a,a)$ with $a \\in (0,1)$, i.e., $p(\\phi) = \\frac{1}{2a} \\mathbf{1}_{\\{|\\phi|a\\}}$.\n\nYour program must implement the following components from first principles:\n\n1. A bootstrap particle filter to estimate the likelihood $p_\\phi(y_{1:T})$ using importance sampling and resampling. At each time $t$, propagate particles according to the state transition, weight them by the observation likelihood, compute the incremental likelihood estimate as the arithmetic mean of the unnormalized weights, and resample. For a given $\\phi$ and a fixed number of state particles $N_x$, the estimator $\\widehat{p_\\phi(y_{1:T})}$ must be the product of these incremental mean-weight estimates across $t=1,\\dots,T$.\n\n2. An SMC$^2$-style sequential algorithm over the static parameter $\\phi$. Maintain $N_\\theta$ particles $\\{\\phi^{(j)}\\}_{j=1}^{N_\\theta}$ drawn from the prior, each with its own internal bootstrap particle filter (with $N_x$ state particles) that is updated sequentially as new data arrive. At time $t$, update the parameter weights using the incremental likelihood estimates produced by the internal filters. Monitor the effective sample size (ESS) of the parameter weights, and resample the parameter particles and their associated internal state particle systems when the ESS falls below a fraction threshold $\\alpha \\in (0,1)$ of $N_\\theta$. It is acceptable to omit a rejuvenation move (e.g., no Markov Chain Monte Carlo step) for this assignment. At the end of the sequence, form the approximate posterior over $\\phi$ given $y_{1:T}$ using the final normalized parameter weights and compute the posterior mean estimate $\\widehat{E[\\phi \\mid y_{1:T}]}$.\n\n3. A pure SMC estimator for the marginal likelihood (evidence) $p(y_{1:T}) = \\int p_\\phi(y_{1:T}) p(\\phi)\\, d\\phi$ using importance sampling with the prior as the proposal. Draw $\\{\\phi^{(m)}\\}_{m=1}^M$ independently from $p(\\phi)$, compute unbiased particle filter likelihood estimates $\\widehat{p_{\\phi^{(m)}}(y_{1:T})}$ for each, and estimate the evidence by the Monte Carlo average $\\widehat{p(y_{1:T})} = \\frac{1}{M} \\sum_{m=1}^M \\widehat{p_{\\phi^{(m)}}(y_{1:T})}$. For numerical stability, compute and return the natural logarithm $\\log \\widehat{p(y_{1:T})}$ using a log-sum-exp computation.\n\n4. A numerically stable baseline calculation of the log-likelihood $\\log p_\\phi(y_{1:T})$ using the Kalman filter for the specified linear Gaussian model. Using this exact likelihood, construct a high-accuracy numerical approximation to the posterior density of $\\phi$ on $(-a,a)$ by evaluating $\\log p_\\phi(y_{1:T})$ on a dense grid $\\{\\phi_k\\}_{k=1}^G \\subset (-a,a)$ with $G \\in \\mathbb{N}$, and compute:\n   - The posterior mean $E[\\phi \\mid y_{1:T}] \\approx \\frac{\\int_{-a}^a \\phi\\, p_\\phi(y_{1:T}) p(\\phi)\\, d\\phi}{\\int_{-a}^a p_\\phi(y_{1:T}) p(\\phi)\\, d\\phi}$ via a numerical quadrature rule over the grid.\n   - The log-evidence $\\log p(y_{1:T}) = \\log \\int_{-a}^a p_\\phi(y_{1:T}) p(\\phi)\\, d\\phi$ via the same numerical quadrature.\n\nUse the following fundamental bases and definitions to derive and justify your algorithms:\n- Bayes’ rule and the definition of posterior density $p(\\phi \\mid y_{1:T}) \\propto p_\\phi(y_{1:T}) p(\\phi)$.\n- The definition of a state-space model and the Markov property of the latent process.\n- Importance sampling with resampling for particle filters, including the unbiasedness of the bootstrap particle filter likelihood estimator when using the product of incremental mean weights.\n- The definition of effective sample size $\\mathrm{ESS} = \\frac{1}{\\sum_{j=1}^{N_\\theta} W_j^2}$ for normalized weights $\\{W_j\\}$ in the parameter space.\n\nTest Suite. Your program must generate synthetic observations for three test cases using the specified parameters and random seeds for reproducibility. For each test case, compute two quantities: \n- A boolean asserting whether the SMC$^2$ posterior mean of $\\phi$ is within a specified absolute tolerance of the numerical baseline posterior mean.\n- A boolean asserting whether the log-evidence estimated by importance sampling is within a specified absolute tolerance of the baseline log-evidence from numerical quadrature.\n\nThe three test cases are:\n\n- Case 1 (happy path): $T=25$, true $\\phi=0.7$, $s_0=1.0$, $\\sigma_w=1.0$, $\\sigma_v=0.5$, prior bound $a=0.99$, data-generation seed $=20211$, SMC$^2$ hyperparameters $N_\\theta=150$, $N_x=100$, $\\alpha=0.5$, evidence estimator hyperparameters $M=200$, $N_x^{\\mathrm{IS}}=150$, grid size $G=1501$, tolerances $\\tau_{\\mathrm{mean}}=0.10$, $\\tau_{\\log \\mathrm{ev}}=0.25$.\n\n- Case 2 (near-boundary parameter): $T=10$, true $\\phi=0.95$, $s_0=1.0$, $\\sigma_w=1.0$, $\\sigma_v=0.5$, prior bound $a=0.99$, data-generation seed $=20212$, SMC$^2$ hyperparameters $N_\\theta=150$, $N_x=100$, $\\alpha=0.5$, evidence estimator hyperparameters $M=200$, $N_x^{\\mathrm{IS}}=150$, grid size $G=1501$, tolerances $\\tau_{\\mathrm{mean}}=0.12$, $\\tau_{\\log \\mathrm{ev}}=0.35$.\n\n- Case 3 (negative dependence, short series): $T=5$, true $\\phi=-0.5$, $s_0=1.0$, $\\sigma_w=1.0$, $\\sigma_v=0.5$, prior bound $a=0.99$, data-generation seed $=20213$, SMC$^2$ hyperparameters $N_\\theta=150$, $N_x=100$, $\\alpha=0.5$, evidence estimator hyperparameters $M=200$, $N_x^{\\mathrm{IS}}=150$, grid size $G=1501$, tolerances $\\tau_{\\mathrm{mean}}=0.15$, $\\tau_{\\log \\mathrm{ev}}=0.45$.\n\nOutput specification. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain exactly six elements corresponding to the three test cases in order, with each case contributing two booleans in the order: posterior-mean-accuracy boolean, log-evidence-accuracy boolean. For example, an output could be of the form\n\"[True,False,True,True,False,True]\".\nNo additional text should be printed.\n\nAngles are not involved. No physical units are involved. All numerical answers are booleans. Ensure your code uses the exact seeds above for data generation and fixed randomness in the algorithms to guarantee reproducibility.", "solution": "The problem requires the implementation and validation of Sequential Monte Carlo (SMC) methods for static parameter estimation in a linear Gaussian state-space model. Specifically, we must implement an SMC$^2$ algorithm to estimate an autoregressive parameter $\\phi$ and an SMC importance sampler to estimate the model evidence. These Monte Carlo estimators are to be validated against high-accuracy baselines derived from the Kalman filter and numerical quadrature.\n\nThe state-space model is defined by the following equations:\nState transition:\n$$x_t = \\phi x_{t-1} + \\sigma_w \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0,1)$$\nInitial state:\n$$x_0 \\sim \\mathcal{N}(0, s_0^2)$$\nObservation:\n$$y_t = x_t + \\sigma_v \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0,1)$$\nHere, $\\{x_t\\}$ is the latent state sequence and $\\{y_t\\}$ is the observed data. The parameter of interest is the static autoregressive coefficient $\\phi$, for which we assume a uniform prior on the interval $(-a, a)$, i.e., $p(\\phi) = \\mathcal{U}(-a, a)$. The parameters $\\sigma_w$, $\\sigma_v$, $s_0$, and $a$ are known constants.\n\nThe goal is to approximate the posterior distribution $p(\\phi \\mid y_{1:T}) \\propto p_\\phi(y_{1:T}) p(\\phi)$, where $p_\\phi(y_{1:T})$ is the likelihood of the observations given $\\phi$. We will compute the posterior mean $E[\\phi \\mid y_{1:T}]$ and the marginal likelihood (evidence) $p(y_{1:T}) = \\int p_\\phi(y_{1:T}) p(\\phi) d\\phi$.\n\n### 1. Baseline Calculation via Kalman Filter and Numerical Quadrature\n\nFor a linear Gaussian state-space model, the likelihood $p_\\phi(y_{1:T})$ can be calculated exactly using the Kalman filter. The filter recursively computes the mean and variance of the filtering distribution $p(x_t | y_{1:t})$.\n\nLet $m_{t|t-1} = E[x_t \\mid y_{1:t-1}]$ and $P_{t|t-1} = \\mathrm{Var}(x_t \\mid y_{1:t-1})$ be the predicted state mean and variance.\nLet $m_{t|t} = E[x_t \\mid y_{1:t}]$ and $P_{t|t} = \\mathrm{Var}(x_t \\mid y_{1:t})$ be the updated state mean and variance.\n\nThe algorithm proceeds as follows:\n- **Initialization:** The prior on the initial state $x_0$ gives $m_{0|0} = 0$ and $P_{0|0} = s_0^2$.\n- **Recursion for $t=1, \\dots, T$:**\n    1.  **Prediction Step:** Based on the state at $t-1$, predict the state at time $t$.\n        $$m_{t|t-1} = \\phi m_{t-1|t-1}$$\n        $$P_{t|t-1} = \\phi^2 P_{t-1|t-1} + \\sigma_w^2$$\n    2.  **Update Step:** Use the observation $y_t$ to correct the prediction. The distribution of the observation given past data is $y_t \\mid y_{1:t-1} \\sim \\mathcal{N}(m_{t|t-1}, P_{t|t-1} + \\sigma_v^2)$.\n        -   Forecast error (innovation): $v_t = y_t - m_{t|t-1}$\n        -   Innovation variance: $S_t = P_{t|t-1} + \\sigma_v^2$\n        -   Kalman gain: $K_t = P_{t|t-1} / S_t$\n        -   Updated mean: $m_{t|t} = m_{t|t-1} + K_t v_t$\n        -   Updated variance: $P_{t|t} = (1 - K_t) P_{t|t-1}$\n    3.  **Log-Likelihood Increment:** The log-likelihood of observation $y_t$ is given by the log-pdf of the predictive distribution:\n        $$\\log p_\\phi(y_t \\mid y_{1:t-1}) = -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log(S_t) - \\frac{v_t^2}{2S_t}$$\nThe total log-likelihood is the sum of these increments: $\\log p_\\phi(y_{1:T}) = \\sum_{t=1}^T \\log p_\\phi(y_t \\mid y_{1:t-1})$.\n\nWith the exact log-likelihood function, we can compute the baseline posterior mean and log-evidence using numerical quadrature. We create a dense grid of $G$ points $\\{\\phi_k\\}_{k=1}^G$ over the prior support $(-a, a)$.\n- **Posterior Mean:** The posterior mean is $E[\\phi \\mid y_{1:T}] = \\frac{\\int_{-a}^a \\phi p_\\phi(y_{1:T}) p(\\phi) d\\phi}{\\int_{-a}^a p_\\phi(y_{1:T}) p(\\phi) d\\phi}$. This can be approximated using a quadrature rule (e.g., trapezoidal rule) over the grid. Let $q(\\phi_k) = p_{\\phi_k}(y_{1:T}) p(\\phi_k)$.\n$$E[\\phi \\mid y_{1:T}] \\approx \\frac{\\sum_k \\phi_k q(\\phi_k)}{\\sum_k q(\\phi_k)} = \\frac{\\sum_k \\phi_k \\exp(\\log q(\\phi_k))}{\\sum_k \\exp(\\log q(\\phi_k))}$$\nTo ensure numerical stability, we use a normalization trick: let $L_k = \\log q(\\phi_k)$ and $L_{\\max} = \\max_k(L_k)$. Then the mean is approximated by $\\frac{\\sum_k \\phi_k \\exp(L_k - L_{\\max})}{\\sum_k \\exp(L_k - L_{\\max})}$.\n- **Log-Evidence:** The log-evidence is $\\log p(y_{1:T}) = \\log \\int_{-a}^a q(\\phi_k) d\\phi$. This is approximated by\n$$\\log p(y_{1:T}) \\approx \\log \\left( \\sum_k q(\\phi_k) \\Delta\\phi \\right) = \\log(\\Delta\\phi) + \\log\\left(\\sum_k \\exp(L_k)\\right)$$\nwhere $\\Delta\\phi$ is the grid spacing. The sum is computed using the log-sum-exp trick: $\\log(\\sum_k \\exp(L_k)) = L_{\\max} + \\log(\\sum_k \\exp(L_k - L_{\\max}))$.\n\n### 2. Bootstrap Particle Filter for Likelihood Estimation\n\nThe bootstrap particle filter (BPF) provides an unbiased estimator for the likelihood $p_\\phi(y_{1:T})$. It approximates the filtering distribution with a set of $N_x$ weighted samples or \"particles\".\n- **Initialization ($t=0$):** Sample $N_x$ particles from the initial state distribution: $x_0^{(i)} \\sim \\mathcal{N}(0, s_0^2)$ for $i=1, \\dots, N_x$. The log-likelihood estimate is initialized to $\\mathcal{L} = 0$.\n- **Recursion for $t=1, \\dots, T$:**\n    1.  **Propagation:** For each particle $i$, propagate it through the state transition model to get a new set of particles at time $t$:\n        $$x_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(i)}, \\phi) = \\mathcal{N}(\\phi x_{t-1}^{(i)}, \\sigma_w^2)$$\n    2.  **Weighting:** Compute the importance weight for each particle based on how well it explains the observation $y_t$. The unnormalized weight is the observation likelihood:\n        $$w_t^{(i)} = p(y_t \\mid x_t^{(i)}) \\quad (\\text{the PDF of } \\mathcal{N}(y_t \\mid x_t^{(i)}, \\sigma_v^2))$$\n    3.  **Likelihood Increment:** An unbiased estimate of the incremental likelihood $p_\\phi(y_t \\mid y_{1:t-1})$ is the mean of the unnormalized weights:\n        $$\\widehat{p_\\phi(y_t \\mid y_{1:t-1})} = \\frac{1}{N_x} \\sum_{i=1}^{N_x} w_t^{(i)}$$\n        The total log-likelihood estimate is updated: $\\mathcal{L} \\leftarrow \\mathcal{L} + \\log(\\widehat{p_\\phi(y_t \\mid y_{1:t-1})})$.\n    4.  **Resampling:** To combat particle degeneracy, we resample the particles. First, normalize the weights: $W_t^{(i)} = w_t^{(i)} / \\sum_j w_t^{(j)}$. Then, draw $N_x$ new particles from the current set $\\{x_t^{(i)}\\}$ with replacement, using probabilities $\\{W_t^{(i)}\\}$. We use systematic resampling for efficiency. The resampled particles $\\{x_t'^{(i)}\\}$ become the set $\\{x_t^{(i)}\\}$ for the next time step.\n\nThe final estimate is $\\widehat{\\log p_\\phi(y_{1:T})} = \\mathcal{L}$.\n\n### 3. SMC for Marginal Likelihood (Evidence)\n\nA simple Monte Carlo estimator for the evidence $p(y_{1:T})$ can be constructed using importance sampling with the prior as the proposal distribution.\n1.  Draw $M$ samples of the parameter from the prior: $\\phi^{(m)} \\sim p(\\phi) = \\mathcal{U}(-a, a)$ for $m=1, \\dots, M$.\n2.  For each sample $\\phi^{(m)}$, compute an unbiased estimate of the likelihood, $\\widehat{p_{\\phi^{(m)}}(y_{1:T})}$, using the bootstrap particle filter described above (with $N_x^{\\mathrm{IS}}$ state particles).\n3.  The evidence is estimated by the Monte Carlo average:\n    $$\\widehat{p(y_{1:T})} = \\frac{1}{M} \\sum_{m=1}^M \\widehat{p_{\\phi^{(m)}}(y_{1:T})}$$\nTo compute this stably, we work in the log domain. Let $\\mathcal{L}_m = \\widehat{\\log p_{\\phi^{(m)}}(y_{1:T})}$.\n$$\\log \\widehat{p(y_{1:T})} = \\log\\left(\\frac{1}{M} \\sum_{m=1}^M \\exp(\\mathcal{L}_m)\\right) = -\\log(M) + \\mathrm{logsumexp}(\\{\\mathcal{L}_m\\}_{m=1}^M)$$\n\n### 4. Sequential Monte Carlo Squared (SMC$^2$)\n\nSMC$^2$ is designed to estimate static parameters by treating them as part of the state in a nested SMC framework.\n- **Outer level:** An SMC algorithm targets the posterior distribution of the static parameter $\\phi$. It maintains $N_\\theta$ parameter particles $\\{\\phi^{(j)}\\}_{j=1}^{N_\\theta}$.\n- **Inner level:** Each parameter particle $\\phi^{(j)}$ has its own bootstrap particle filter (with $N_x$ state particles $\\{x_t^{(j,i)}\\}$) that tracks the latent state $x_t$. This inner filter's purpose is to provide an estimate of the incremental likelihood required for the outer filter's weight update.\n\nThe algorithm proceeds sequentially through the data:\n- **Initialization ($t=0$):**\n    1.  Sample $N_\\theta$ parameter particles from the prior: $\\phi^{(j)} \\sim \\mathcal{U}(-a, a)$.\n    2.  Initialize parameter weights to be uniform: $W_0^{(j)} = 1/N_\\theta$.\n    3.  For each $\\phi^{(j)}$, initialize its inner BPF by sampling $N_x$ state particles: $x_0^{(j,i)} \\sim \\mathcal{N}(0, s_0^2)$.\n- **Recursion for $t=1, \\dots, T$:**\n    1.  **Inner Filter Update:** For each parameter particle $j=1, \\dots, N_\\theta$:\n        a. Propagate and weight the inner state particles $\\{x_{t-1}^{(j,i)}\\}$ using $\\phi^{(j)}$ to get $\\{x_t^{(j,i)}\\}$ and weights $\\{w_t^{(j,i)}\\}$.\n        b. Compute the incremental likelihood estimate: $\\hat{l}_t^{(j)} = \\frac{1}{N_x} \\sum_{i=1}^{N_x} w_t^{(j,i)}$.\n        c. Resample the state particles $\\{x_t^{(j,i)}\\}$ based on their normalized weights to get the particles for the next step.\n    2.  **Outer Filter Weight Update:** Update the parameter particle weights using the incremental likelihoods:\n        $$W_t^{(j)} \\propto W_{t-1}^{(j)} \\times \\hat{l}_t^{(j)}$$\n        Then, normalize the weights: $W_t^{(j)} \\leftarrow W_t^{(j)} / \\sum_k W_t^{(k)}$.\n    3.  **Outer Filter Resampling:** To prevent weight degeneracy, monitor the effective sample size (ESS) of the parameter weights: $\\mathrm{ESS}_t = 1 / \\sum_{j=1}^{N_\\theta} (W_t^{(j)})^2$.\n        - If $\\mathrm{ESS}_t  \\alpha N_\\theta$ for a threshold $\\alpha \\in (0,1)$, resample the parameter particles.\n        - A new set of indices $\\{k_j\\}_{j=1}^{N_\\theta}$ is drawn from $\\{1, \\dots, N_\\theta\\}$ with probabilities $\\{W_t^{(j)}\\}$.\n        - The new parameter set is $\\{\\phi'^{(j)} = \\phi^{(k_j)}\\}$.\n        - Crucially, the associated inner particle filter systems are also copied: the state particles for the new $j$-th particle become a copy of the state particles of the old $k_j$-th particle.\n        - After resampling, the parameter weights are reset to uniform, $W_t^{(j)} = 1/N_\\theta$.\n\n- **Final Estimation:** After processing all data up to $T$, the posterior distribution $p(\\phi \\mid y_{1:T})$ is approximated by the weighted sample set $\\{\\phi^{(j)}, W_T^{(j)}\\}$. The posterior mean is estimated as:\n$$\\widehat{E[\\phi \\mid y_{1:T}]} = \\sum_{j=1}^{N_\\theta} W_T^{(j)} \\phi^{(j)}$$", "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef _log_norm_pdf(x, mu, sigma):\n    \"\"\"Computes the log of the normal PDF, log N(x | mu, sigma^2).\"\"\"\n    var = sigma**2\n    return -0.5 * np.log(2 * np.pi * var) - (x - mu)**2 / (2 * var)\n\ndef generate_data(T, phi, s0, sigma_w, sigma_v, rng):\n    \"\"\"Generates synthetic data from the linear Gaussian SSM.\"\"\"\n    x = np.zeros(T + 1)\n    y = np.zeros(T)\n    x[0] = rng.normal(0, s0)\n    for t in range(1, T + 1):\n        x[t] = phi * x[t - 1] + rng.normal(0, sigma_w)\n    y = x[1:] + rng.normal(0, sigma_v, size=T)\n    return y\n\ndef kalman_log_likelihood(y, phi, s0, sigma_w, sigma_v):\n    \"\"\"Computes the exact log-likelihood using the Kalman filter.\"\"\"\n    T = len(y)\n    m_filt = 0.0\n    P_filt = s0**2\n    total_log_lik = 0.0\n\n    for t in range(T):\n        # Prediction\n        m_pred = phi * m_filt\n        P_pred = phi**2 * P_filt + sigma_w**2\n\n        # Update\n        v = y[t] - m_pred\n        S = P_pred + sigma_v**2\n        \n        # Log-likelihood increment\n        total_log_lik += -0.5 * np.log(2 * np.pi) - 0.5 * np.log(S) - v**2 / (2 * S)\n\n        # Filtered state\n        K = P_pred / S\n        m_filt = m_pred + K * v\n        P_filt = (1 - K) * P_pred\n        \n    return total_log_lik\n\ndef baseline_estimators(y, a, s0, sigma_w, sigma_v, G):\n    \"\"\"Computes baseline posterior mean and log-evidence via numerical quadrature.\"\"\"\n    phi_grid = np.linspace(-a, a, G)\n    log_prior = -np.log(2 * a)\n    \n    log_lik_vals = np.array([kalman_log_likelihood(y, p, s0, sigma_w, sigma_v) for p in phi_grid])\n    log_q_vals = log_lik_vals + log_prior\n    \n    log_q_max = np.max(log_q_vals)\n    q_vals_norm = np.exp(log_q_vals - log_q_max)\n\n    # Posterior Mean\n    numerator = np.trapz(phi_grid * q_vals_norm, phi_grid)\n    denominator = np.trapz(q_vals_norm, phi_grid)\n    baseline_mean = numerator / denominator\n\n    # Log-Evidence\n    baseline_log_evidence = np.log(denominator) + log_q_max\n\n    return baseline_mean, baseline_log_evidence\n\ndef systematic_resample(weights, rng):\n    \"\"\"Performs systematic resampling.\"\"\"\n    N = len(weights)\n    u = (np.arange(N) + rng.random()) / N\n    cum_weights = np.cumsum(weights)\n    indices = np.searchsorted(cum_weights, u)\n    return indices\n\ndef bpf_log_likelihood(y, phi, s0, sigma_w, sigma_v, Nx, rng):\n    \"\"\"Estimates log-likelihood using a bootstrap particle filter.\"\"\"\n    T = len(y)\n    particles = rng.normal(0.0, s0, size=Nx)\n    total_log_lik = 0.0\n\n    for t in range(T):\n        # Propagate\n        particles = phi * particles + rng.normal(0.0, sigma_w, size=Nx)\n        \n        # Weight\n        log_weights = _log_norm_pdf(y[t], particles, sigma_v)\n        max_log_weight = np.max(log_weights)\n        weights = np.exp(log_weights - max_log_weight)\n        \n        # Incremental likelihood and update total\n        mean_weight = np.mean(weights)\n        if mean_weight > 0:\n            total_log_lik += np.log(mean_weight) + max_log_weight\n        else:\n            return -np.inf # Particle collapse\n        \n        # Resample\n        norm_weights = weights / np.sum(weights)\n        indices = systematic_resample(norm_weights, rng)\n        particles = particles[indices]\n\n    return total_log_lik\n\ndef smc_log_evidence(y, a, s0, sigma_w, sigma_v, M, Nx_is, rng):\n    \"\"\"Estimates log-evidence using SMC with prior as proposal.\"\"\"\n    phi_samples = rng.uniform(-a, a, size=M)\n    log_liks = np.array([bpf_log_likelihood(y, p, s0, sigma_w, sigma_v, Nx_is, rng) for p in phi_samples])\n    \n    # Check for -inf to avoid NaN in logsumexp\n    valid_log_liks = log_liks[np.isfinite(log_liks)]\n    if len(valid_log_liks) == 0:\n        return -np.inf\n\n    return -np.log(M) + logsumexp(valid_log_liks)\n\ndef smc_squared(y, a, s0, sigma_w, sigma_v, N_theta, Nx, alpha_threshold, rng):\n    \"\"\"Estimates posterior mean of phi using SMC^2.\"\"\"\n    T = len(y)\n    \n    # Initialization\n    phi_particles = rng.uniform(-a, a, size=N_theta)\n    log_weights = np.full(N_theta, -np.log(N_theta))\n    \n    state_particles = rng.normal(0.0, s0, size=(N_theta, Nx))\n\n    for t in range(T):\n        log_lik_increments = np.zeros(N_theta)\n        \n        # Inner BPF step for each theta particle\n        for j in range(N_theta):\n            # Propagate\n            current_states = state_particles[j, :]\n            propagated_states = phi_particles[j] * current_states + rng.normal(0.0, sigma_w, size=Nx)\n            \n            # Weight\n            log_w_inner = _log_norm_pdf(y[t], propagated_states, sigma_v)\n            max_log_w_inner = np.max(log_w_inner)\n            w_inner = np.exp(log_w_inner - max_log_w_inner)\n            \n            # Incremental likelihood\n            mean_weight = np.mean(w_inner)\n            if mean_weight > 0:\n                log_lik_increments[j] = np.log(mean_weight) + max_log_w_inner\n            else:\n                log_lik_increments[j] = -np.inf\n            \n            # Resample inner particles\n            norm_w_inner = w_inner / np.sum(w_inner)\n            indices = systematic_resample(norm_w_inner, rng)\n            state_particles[j, :] = propagated_states[indices]\n\n        # Update outer weights\n        log_weights += log_lik_increments\n        log_weights -= logsumexp(log_weights)\n        \n        # Resample outer particles if ESS is low\n        weights = np.exp(log_weights)\n        ess = 1.0 / np.sum(weights**2)\n        if ess  alpha_threshold * N_theta:\n            indices = systematic_resample(weights, rng)\n            phi_particles = phi_particles[indices]\n            state_particles = state_particles[indices, :]\n            log_weights = np.full(N_theta, -np.log(N_theta))\n\n    # Final posterior mean estimate\n    final_weights = np.exp(log_weights - logsumexp(log_weights))\n    posterior_mean = np.sum(final_weights * phi_particles)\n    \n    return posterior_mean\n\ndef solve():\n    \"\"\"Main function to run test cases and print results.\"\"\"\n    test_cases = [\n        {'T': 25, 'true_phi': 0.7, 's0': 1.0, 'sigma_w': 1.0, 'sigma_v': 0.5, 'a': 0.99, 'seed': 20211,\n         'N_theta': 150, 'Nx': 100, 'alpha': 0.5, 'M': 200, 'Nx_is': 150, 'G': 1501, \n         'tau_mean': 0.10, 'tau_log_ev': 0.25},\n        {'T': 10, 'true_phi': 0.95, 's0': 1.0, 'sigma_w': 1.0, 'sigma_v': 0.5, 'a': 0.99, 'seed': 20212,\n         'N_theta': 150, 'Nx': 100, 'alpha': 0.5, 'M': 200, 'Nx_is': 150, 'G': 1501, \n         'tau_mean': 0.12, 'tau_log_ev': 0.35},\n        {'T': 5, 'true_phi': -0.5, 's0': 1.0, 'sigma_w': 1.0, 'sigma_v': 0.5, 'a': 0.99, 'seed': 20213,\n         'N_theta': 150, 'Nx': 100, 'alpha': 0.5, 'M': 200, 'Nx_is': 150, 'G': 1501, \n         'tau_mean': 0.15, 'tau_log_ev': 0.45},\n    ]\n\n    results = []\n    for case in test_cases:\n        rng = np.random.default_rng(case['seed'])\n        \n        y = generate_data(case['T'], case['true_phi'], case['s0'], case['sigma_w'], case['sigma_v'], rng)\n\n        # SMC^2 estimate\n        smc2_mean = smc_squared(y, case['a'], case['s0'], case['sigma_w'], case['sigma_v'],\n                                case['N_theta'], case['Nx'], case['alpha'], rng)\n        \n        # SMC evidence estimate\n        smc_log_ev = smc_log_evidence(y, case['a'], case['s0'], case['sigma_w'], case['sigma_v'],\n                                      case['M'], case['Nx_is'], rng)\n\n        # Baseline calculation\n        base_mean, base_log_ev = baseline_estimators(y, case['a'], case['s0'], case['sigma_w'], case['sigma_v'], case['G'])\n        \n        # Comparisons\n        mean_accurate = np.abs(smc2_mean - base_mean)  case['tau_mean']\n        log_ev_accurate = np.abs(smc_log_ev - base_log_ev)  case['tau_log_ev']\n        \n        results.extend([mean_accurate, log_ev_accurate])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3326834"}]}