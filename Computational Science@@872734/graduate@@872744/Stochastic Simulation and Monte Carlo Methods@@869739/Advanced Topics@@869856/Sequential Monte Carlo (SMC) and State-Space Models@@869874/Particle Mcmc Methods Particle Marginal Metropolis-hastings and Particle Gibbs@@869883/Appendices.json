{"hands_on_practices": [{"introduction": "The performance of any particle MCMC algorithm depends critically on the quality of its underlying particle filter. A key determinant of this quality is the variance of the importance weights, as high variance leads to the problem of particle degeneracy. This exercise provides a foundational look at how the choice of proposal distribution within the particle filter impacts this variance. By deriving the weight variance for both the simple bootstrap proposal and the theoretically optimal proposal, you will gain a concrete, first-principles understanding of why sophisticated proposal mechanisms are essential for building efficient and reliable particle filters [@problem_id:3327380].", "problem": "Consider a scalar Hidden Markov Model (HMM) with latent state $x_t \\in \\mathbb{R}$ and observation $y_t \\in \\mathbb{R}$, specified by the transition density $p_{\\theta}(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t ; \\phi x_{t-1}, \\sigma_x^2)$ and the emission density $p_{\\theta}(y_t \\mid x_t) = \\mathcal{N}(y_t ; x_t, \\sigma_y^2)$, where $\\phi \\in \\mathbb{R}$, $\\sigma_x^2  0$, and $\\sigma_y^2  0$ are fixed parameters. In the context of Particle Markov chain Monte Carlo (PMCMC), and specifically for a single time step within a particle filter used inside Particle Marginal Metropolis–Hastings (PMMH) or Particle Gibbs, consider the incremental importance weight at time $t$, defined for a general proposal $q(x_t \\mid x_{t-1}, y_t)$ by\n$$\nw_t(x_t; x_{t-1}, y_t) \\propto \\frac{p_{\\theta}(y_t \\mid x_t)\\, p_{\\theta}(x_t \\mid x_{t-1})}{q(x_t \\mid x_{t-1}, y_t)}.\n$$\nFocus on a single time step $t$ with $x_{t-1} = a \\in \\mathbb{R}$ and observed $y_t = y \\in \\mathbb{R}$. Compare the variance (with respect to the proposal draw of $x_t$) of the incremental importance weights under:\n- the bootstrap proposal $q_{\\text{boot}}(x_t \\mid x_{t-1}, y_t) = p_{\\theta}(x_t \\mid x_{t-1})$, and\n- the optimal proposal $q_{\\text{opt}}(x_t \\mid x_{t-1}, y_t) \\propto p_{\\theta}(y_t \\mid x_t)\\, p_{\\theta}(x_t \\mid x_{t-1})$.\n\nAssume the variance is computed conditional on the fixed values $x_{t-1} = a$ and $y_t = y$, and use the unnormalized incremental weights. Derive from first principles the exact variance under each proposal. Then, report as your final answer the closed-form analytic expression for the variance of the incremental importance weight under the bootstrap proposal as a function of $\\phi$, $a$, $y$, $\\sigma_x^2$, and $\\sigma_y^2$.\n\nAnswer in a single closed-form analytic expression. Do not include any units.", "solution": "The problem asks for the variance of the incremental importance weight for a particle filter at a single time step, under two different proposal distributions: the bootstrap proposal and the optimal proposal. The model is a linear Gaussian state-space model.\n\nFirst, we establish the notation for a single time step $t$, dropping the subscript for clarity.\nThe prior state is fixed at $x_{t-1} = a$.\nThe current observation is fixed at $y_t = y$.\nThe state transition model is $p(x|a) = p_{\\theta}(x_t=x | x_{t-1}=a) = \\mathcal{N}(x; \\phi a, \\sigma_x^2)$.\nThe observation model is $p(y|x) = p_{\\theta}(y_t=y | x_t=x) = \\mathcal{N}(y; x, \\sigma_y^2)$.\nThe unnormalized incremental importance weight is given by\n$$\nw(x) = \\frac{p(y|x) p(x|a)}{q(x|a, y)}\n$$\nThe variance is to be computed with respect to the random variable $x \\sim q(x|a, y)$, conditional on the fixed values $a$ and $y$.\n\nFirst, we analyze the optimal proposal, as requested for comparison.\nThe optimal proposal, in the sense that it minimizes the variance of the weights, is the posterior distribution of the state given the observation and the previous state:\n$$\nq_{\\text{opt}}(x|a, y) = p(x|a, y) = \\frac{p(y|x) p(x|a)}{p(y|a)}\n$$\nwhere $p(y|a) = \\int p(y|x) p(x|a) dx$ is the marginal likelihood of the observation.\nFor this proposal, the incremental weight is:\n$$\nw_{\\text{opt}}(x) = \\frac{p(y|x) p(x|a)}{q_{\\text{opt}}(x|a, y)} = \\frac{p(y|x) p(x|a)}{ \\frac{p(y|x) p(x|a)}{p(y|a)} } = p(y|a)\n$$\nThe weight $w_{\\text{opt}}(x)$ is equal to the marginal likelihood $p(y|a)$, which is a constant with respect to $x$. The variance of a constant is zero.\n$$\n\\text{Var}_{x \\sim q_{\\text{opt}}}[w_{\\text{opt}}(x)] = \\text{Var}[p(y|a)] = 0\n$$\nThis demonstrates the \"optimality\" of this proposal in terms of weight variance.\n\nNext, we analyze the bootstrap proposal, which is the main focus of the problem.\nThe bootstrap proposal uses the state transition model as the proposal distribution:\n$$\nq_{\\text{boot}}(x|a, y) = p(x|a) = \\mathcal{N}(x; \\phi a, \\sigma_x^2)\n$$\nFor this proposal, the incremental weight simplifies to the observation likelihood:\n$$\nw_{\\text{boot}}(x) = \\frac{p(y|x) p(x|a)}{q_{\\text{boot}}(x|a, y)} = \\frac{p(y|x) p(x|a)}{p(x|a)} = p(y|x)\n$$\nSpecifically, $w_{\\text{boot}}(x) = \\mathcal{N}(y; x, \\sigma_y^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left(-\\frac{(y-x)^2}{2\\sigma_y^2}\\right)$.\nWe need to find the variance of this weight, $\\text{Var}_{x \\sim q_{\\text{boot}}}[w_{\\text{boot}}(x)]$, where the randomness is from $x \\sim q_{\\text{boot}}(x|a,y) = \\mathcal{N}(x; \\phi a, \\sigma_x^2)$.\nThe variance is given by $\\text{Var}(w) = E[w^2] - (E[w])^2$.\n\nFirst, we compute the expected weight, $E[w_{\\text{boot}}(x)]$.\n$$\nE[w_{\\text{boot}}(x)] = \\int_{-\\infty}^{\\infty} w_{\\text{boot}}(x) q_{\\text{boot}}(x) dx = \\int_{-\\infty}^{\\infty} p(y|x) p(x|a) dx\n$$\nThis integral is the marginal likelihood $p(y|a)$. For this linear Gaussian system, $p(y|a)$ is also a Gaussian distribution. Since $x|a \\sim \\mathcal{N}(\\phi a, \\sigma_x^2)$ and $y|x \\sim \\mathcal{N}(x, \\sigma_y^2)$, we can view $y$ as $y=x+\\epsilon_y$ with $\\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2)$. Given $a$, $x$ is a random variable, so $y$ is the sum of two independent Gaussian random variables (one being $x$ and the other $\\epsilon_y$), and thus is also Gaussian. Its mean is $E[y|a] = E[x|a] = \\phi a$ and its variance is $\\text{Var}(y|a) = \\text{Var}(x|a) + \\text{Var}(\\epsilon_y) = \\sigma_x^2 + \\sigma_y^2$.\nTherefore, $p(y|a) = \\mathcal{N}(y; \\phi a, \\sigma_x^2+\\sigma_y^2)$.\nThe expected weight is the value of this PDF at $y$:\n$$\nE[w_{\\text{boot}}(x)] = \\frac{1}{\\sqrt{2\\pi(\\sigma_x^2+\\sigma_y^2)}} \\exp\\left(-\\frac{(y-\\phi a)^2}{2(\\sigma_x^2+\\sigma_y^2)}\\right)\n$$\nThe square of the expected weight is:\n$$\n(E[w_{\\text{boot}}(x)])^2 = \\frac{1}{2\\pi(\\sigma_x^2+\\sigma_y^2)} \\exp\\left(-\\frac{(y-\\phi a)^2}{\\sigma_x^2+\\sigma_y^2}\\right)\n$$\n\nNext, we compute the second moment, $E[w_{\\text{boot}}(x)^2]$.\n$$\nE[w_{\\text{boot}}(x)^2] = \\int_{-\\infty}^{\\infty} [w_{\\text{boot}}(x)]^2 q_{\\text{boot}}(x) dx = \\int_{-\\infty}^{\\infty} [p(y|x)]^2 p(x|a) dx\n$$\nWe have $[p(y|x)]^2 = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left(-\\frac{(y-x)^2}{2\\sigma_y^2}\\right)\\right)^2 = \\frac{1}{2\\pi\\sigma_y^2} \\exp\\left(-\\frac{(y-x)^2}{\\sigma_y^2}\\right)$.\nThe proposal is $p(x|a) = \\frac{1}{\\sqrt{2\\pi\\sigma_x^2}} \\exp\\left(-\\frac{(x-\\phi a)^2}{2\\sigma_x^2}\\right)$.\nThe integral for the second moment is:\n$$\nE[w_{\\text{boot}}(x)^2] = \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi\\sigma_y^2} \\exp\\left(-\\frac{(x-y)^2}{\\sigma_y^2}\\right) \\frac{1}{\\sqrt{2\\pi\\sigma_x^2}} \\exp\\left(-\\frac{(x-\\phi a)^2}{2\\sigma_x^2}\\right) dx\n$$\nThe integrand is a product of two Gaussian-like functions of $x$. The exponent is a quadratic in $x$:\n$$\n-\\left( \\frac{(x-y)^2}{\\sigma_y^2} + \\frac{(x-\\phi a)^2}{2\\sigma_x^2} \\right) = - \\left( x^2\\left(\\frac{1}{\\sigma_y^2}+\\frac{1}{2\\sigma_x^2}\\right) - 2x\\left(\\frac{y}{\\sigma_y^2}+\\frac{\\phi a}{2\\sigma_x^2}\\right) + \\left(\\frac{y^2}{\\sigma_y^2}+\\frac{(\\phi a)^2}{2\\sigma_x^2}\\right) \\right)\n$$\nThis is an unnormalized Gaussian form. Completing the square for $x$ in the exponent, we find that the integral of $\\exp(-\\text{quadratic})$ is a standard result. The calculation yields:\n$$\n\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{(x-y)^2}{\\sigma_y^2}\\right) \\exp\\left(-\\frac{(x-\\phi a)^2}{2\\sigma_x^2}\\right) dx = \\sqrt{\\frac{2\\pi\\sigma_x^2\\sigma_y^2}{2\\sigma_x^2+\\sigma_y^2}} \\exp\\left(-\\frac{(y-\\phi a)^2}{2\\sigma_x^2+\\sigma_y^2}\\right)\n$$\nSubstituting this back into the expression for $E[w_{\\text{boot}}(x)^2]$:\n$$\nE[w_{\\text{boot}}(x)^2] = \\frac{1}{2\\pi\\sigma_y^2 \\sqrt{2\\pi\\sigma_x^2}} \\left( \\sqrt{\\frac{2\\pi\\sigma_x^2\\sigma_y^2}{2\\sigma_x^2+\\sigma_y^2}} \\exp\\left(-\\frac{(y-\\phi a)^2}{2\\sigma_x^2+\\sigma_y^2}\\right) \\right)\n$$\nSimplifying the pre-factors:\n$$\n\\frac{1}{2\\pi\\sigma_y^2 \\sqrt{2\\pi}\\sigma_x} \\left( \\sqrt{2\\pi} \\frac{\\sigma_x\\sigma_y}{\\sqrt{2\\sigma_x^2+\\sigma_y^2}} \\right) = \\frac{\\sigma_x\\sigma_y}{2\\pi\\sigma_y^2\\sigma_x\\sqrt{2\\sigma_x^2+\\sigma_y^2}} = \\frac{1}{2\\pi\\sigma_y\\sqrt{2\\sigma_x^2+\\sigma_y^2}}\n$$\nSo, the second moment is:\n$$\nE[w_{\\text{boot}}(x)^2] = \\frac{1}{2\\pi\\sigma_y\\sqrt{2\\sigma_x^2+\\sigma_y^2}} \\exp\\left(-\\frac{(y-\\phi a)^2}{2\\sigma_x^2+\\sigma_y^2}\\right)\n$$\nFinally, the variance is found by subtracting the squared mean from the second moment:\n$$\n\\text{Var}(w_{\\text{boot}}) = E[w_{\\text{boot}}^2] - (E[w_{\\text{boot}}])^2\n$$\n$$\n\\text{Var}(w_{\\text{boot}}) = \\frac{1}{2\\pi\\sigma_y\\sqrt{2\\sigma_x^2+\\sigma_y^2}} \\exp\\left(-\\frac{(y-\\phi a)^2}{2\\sigma_x^2+\\sigma_y^2}\\right) - \\frac{1}{2\\pi(\\sigma_x^2+\\sigma_y^2)} \\exp\\left(-\\frac{(y-\\phi a)^2}{\\sigma_x^2+\\sigma_y^2}\\right)\n$$\nThis is the closed-form analytic expression for the variance of the incremental importance weight under the bootstrap proposal.", "answer": "$$ \\boxed{ \\frac{1}{2\\pi\\sigma_y\\sqrt{2\\sigma_x^2+\\sigma_y^2}} \\exp\\left(-\\frac{(y-\\phi a)^2}{2\\sigma_x^2+\\sigma_y^2}\\right) - \\frac{1}{2\\pi(\\sigma_x^2+\\sigma_y^2)} \\exp\\left(-\\frac{(y-\\phi a)^2}{\\sigma_x^2+\\sigma_y^2}\\right) } $$", "id": "3327380"}, {"introduction": "Having explored the variance of weights within a particle filter, we now scale up to see how this property affects the behavior of the entire Particle Marginal Metropolis-Hastings (PMMH) algorithm. The introduction of an estimated likelihood adds a layer of randomness that directly influences the MCMC sampler's acceptance probability. This practice guides you through the derivation of a classic and elegant result that quantifies this relationship. By deriving the closed-form expression for the average acceptance rate as a function of the log-likelihood estimator's variance, $\\sigma^2$, you will uncover the theoretical basis for the crucial task of tuning the number of particles in a PMMH algorithm [@problem_id:3327363].", "problem": "Consider a Particle Markov chain Monte Carlo (PMCMC) algorithm using the particle marginal Metropolis–Hastings (PMMH) method to sample a parameter $\\theta$ from a posterior distribution with unnormalized density $\\pi(\\theta) p_{\\theta}(y)$, where $p_{\\theta}(y)$ is an intractable likelihood estimated unbiasedly by a particle filter. Let $\\hat{p}(y \\mid \\theta, U)$ denote the unbiased likelihood estimator obtained from the particle filter using random seeds $U$. Define the log-likelihood noise by $Z(\\theta, U) = \\log \\hat{p}(y \\mid \\theta, U) - \\log p_{\\theta}(y)$. Assume the following:\n- The estimator is unbiased on the likelihood scale, that is, $\\mathbb{E}[\\hat{p}(y \\mid \\theta, U) \\mid \\theta] = p_{\\theta}(y)$, which implies $\\mathbb{E}[\\exp(Z(\\theta, U)) \\mid \\theta] = 1$.\n- For each fixed $\\theta$, the distribution of $Z(\\theta, U)$ is approximately Gaussian with variance $\\sigma^{2}$, and (by the unbiasedness constraint) mean $-\\sigma^{2}/2$.\n- The proposal for $\\theta$ in the Metropolis–Hastings step is such that, conditionally on $\\theta$ and the proposed $\\theta'$, the exact Metropolis–Hastings ratio based on $\\pi(\\theta) p_{\\theta}(y)$ and the proposal kernel equals $1$. This isolates the effect of likelihood estimation noise on acceptance.\n- The particle filter is run with independent randomness at the current and proposed states, so that the corresponding log-noises are independent given $\\theta$ and $\\theta'$.\n- The Markov chain is at stationarity on the extended PMMH target over $(\\theta, U)$.\n\nUnder these conditions, the PMMH acceptance probability for a proposed move simplifies to\n$$\n\\alpha = \\min\\left\\{ 1, \\exp\\left(Z' - Z \\right) \\right\\},\n$$\nwhere $Z' = Z(\\theta', U')$ is the proposed log-noise and $Z = Z(\\theta, U)$ is the current log-noise.\n\nStarting from the above fundamental definitions, derive a closed-form analytic expression for the average acceptance probability $\\mathbb{E}[\\alpha]$ as a function of the noise variance $\\sigma^{2}$, where the expectation is taken jointly over the stationary distribution of the current log-noise and the proposal distribution of the new log-noise. You may express your answer in terms of the standard normal cumulative distribution function $\\Phi(\\cdot)$. Your final answer must be a single closed-form analytic expression depending only on $\\sigma^{2}$. No numerical approximation is required, and no units are involved. Express the final answer in exact form (no rounding). Also briefly indicate, in words or symbols, how the average acceptance depends on $\\sigma^{2}$.", "solution": "The objective is to derive a closed-form analytic expression for the average acceptance probability, $\\mathbb{E}[\\alpha]$, of a particle marginal Metropolis-Hastings (PMMH) move under a specific set of simplifying assumptions. The acceptance probability is given by $\\alpha = \\min\\left\\{ 1, \\exp\\left(Z' - Z \\right) \\right\\}$, where $Z$ and $Z'$ are the log-likelihood noises at the current and proposed states, respectively.\n\nThe core of the derivation lies in correctly identifying the probability distributions for the current noise $Z$ and the proposed noise $Z'$. The problem states that for any given parameter $\\theta$, the log-likelihood noise $Z(\\theta, U) = \\log \\hat{p}(y \\mid \\theta, U) - \\log p_{\\theta}(y)$ is approximately Gaussian. The condition that the likelihood estimator is unbiased, $\\mathbb{E}[\\exp(Z(\\theta, U)) \\mid \\theta] = 1$, implies that if $Z$ follows a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, its mean must be $\\mu = -\\sigma^2/2$. Thus, the distribution of log-noise generated by the particle filter for a given $\\theta$ is $q(z) = \\mathcal{N}(-\\frac{\\sigma^2}{2}, \\sigma^2)$.\n\nThe proposed log-noise, $Z' = Z(\\theta', U')$, is generated by running the particle filter with a new set of random numbers $U'$ for the proposed parameter $\\theta'$. Therefore, the distribution of $Z'$ is the one specified above:\n$$\nZ' \\sim \\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)\n$$\n\nThe current log-noise, $Z$, is a component of the PMMH chain which is assumed to be at stationarity. The stationary distribution of the PMMH algorithm, on the extended space of parameters and auxiliary random variables $(\\theta, U)$, has a density proportional to the target $\\pi(\\theta) p_{\\theta}(y)$ times the proposal for the auxiliary variables. In our case, this corresponds to $\\pi(\\theta) \\hat{p}(y|\\theta, U)$. We can express this in terms of the log-noise $Z$:\n$$\n\\pi_{\\text{stationary}}(\\theta, Z) \\propto \\pi(\\theta) p_{\\theta}(y) \\exp(Z) q(Z \\mid \\theta)\n$$\nwhere $q(Z \\mid \\theta)$ is the density of $\\mathcal{N}(-\\sigma^2/2, \\sigma^2)$, which is assumed to be independent of $\\theta$. The marginal stationary distribution for $Z$, let's call its density $\\pi(z)$, is obtained by integrating over $\\theta$. Crucially, it is proportional to the original noise distribution $q(z)$ \"tilted\" by the factor $\\exp(z)$:\n$$\n\\pi(z) \\propto q(z) \\exp(z)\n$$\nLet's find the form of this distribution. Substituting the density for $q(z)$:\n$$\n\\pi(z) \\propto \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z - (-\\sigma^2/2))^2}{2\\sigma^2}\\right) \\exp(z)\n$$\n$$\n\\pi(z) \\propto \\exp\\left(-\\frac{(z + \\sigma^2/2)^2}{2\\sigma^2} + z\\right)\n$$\nWe complete the square for the term in the exponent:\n$$\n-\\frac{z^2 + z\\sigma^2 + \\sigma^4/4}{2\\sigma^2} + z = \\frac{-z^2 - z\\sigma^2 - \\sigma^4/4 + 2z\\sigma^2}{2\\sigma^2} = \\frac{-z^2 + z\\sigma^2 - \\sigma^4/4}{2\\sigma^2}\n$$\n$$\n= -\\frac{(z^2 - z\\sigma^2 + \\sigma^4/4)}{2\\sigma^2} = -\\frac{(z - \\sigma^2/2)^2}{2\\sigma^2}\n$$\nThis is the kernel of a Gaussian distribution with mean $\\sigma^2/2$ and variance $\\sigma^2$. Therefore, the stationary distribution of the current log-noise $Z$ is:\n$$\nZ \\sim \\mathcal{N}\\left(\\frac{\\sigma^2}{2}, \\sigma^2\\right)\n$$\n\nWe now define the difference $\\Delta Z = Z' - Z$. Since $Z'$ and $Z$ are independent, $\\Delta Z$ is also normally distributed. Its mean and variance are:\n$$\n\\mathbb{E}[\\Delta Z] = \\mathbb{E}[Z'] - \\mathbb{E}[Z] = \\left(-\\frac{\\sigma^2}{2}\\right) - \\left(\\frac{\\sigma^2}{2}\\right) = -\\sigma^2\n$$\n$$\n\\text{Var}(\\Delta Z) = \\text{Var}(Z') + \\text{Var}(Z) = \\sigma^2 + \\sigma^2 = 2\\sigma^2\n$$\nSo, $\\Delta Z \\sim \\mathcal{N}(-\\sigma^2, 2\\sigma^2)$.\n\nThe average acceptance probability is the expectation of $\\alpha = \\min\\{1, \\exp(\\Delta Z)\\}$. Let $f_{\\Delta Z}(x)$ be the probability density function of $\\Delta Z$. The expectation is:\n$$\n\\mathbb{E}[\\alpha] = \\int_{-\\infty}^{\\infty} \\min\\{1, \\exp(x)\\} f_{\\Delta Z}(x) \\, dx\n$$\nWe split the integral into two parts based on the value of $x$:\n$$\n\\mathbb{E}[\\alpha] = \\int_{-\\infty}^{0} \\exp(x) f_{\\Delta Z}(x) \\, dx + \\int_{0}^{\\infty} 1 \\cdot f_{\\Delta Z}(x) \\, dx\n$$\nThe second integral is simply the probability $P(\\Delta Z  0)$. Let $S$ be a standard normal random variable, $S \\sim \\mathcal{N}(0, 1)$.\n$$\nP(\\Delta Z  0) = P\\left(\\frac{\\Delta Z - (-\\sigma^2)}{\\sqrt{2\\sigma^2}}  \\frac{0 - (-\\sigma^2)}{\\sqrt{2\\sigma^2}}\\right) = P\\left(S  \\frac{\\sigma^2}{\\sqrt{2\\sigma^2}}\\right) = P\\left(S  \\frac{\\sigma}{\\sqrt{2}}\\right)\n$$\nUsing the standard normal cumulative distribution function $\\Phi(\\cdot)$, this probability is $1 - \\Phi(\\sigma/\\sqrt{2}) = \\Phi(-\\sigma/\\sqrt{2})$.\n\nThe first integral is $\\int_{-\\infty}^{0} \\exp(x) f_{\\Delta Z}(x) \\, dx$.\nLet $X = \\Delta Z \\sim \\mathcal{N}(\\mu, v)$ with $\\mu = -\\sigma^2$ and $v = 2\\sigma^2$. The integral is $\\mathbb{E}[\\exp(X) I(X \\le 0)]$.\nWe know that for a normally distributed variable $X$, $\\exp(x) f_X(x) = \\exp(\\mu + v/2) f_Y(x)$, where $Y \\sim \\mathcal{N}(\\mu+v, v)$.\nLet's compute the pre-factor $\\exp(\\mu + v/2)$:\n$$\n\\exp\\left(-\\sigma^2 + \\frac{2\\sigma^2}{2}\\right) = \\exp(-\\sigma^2 + \\sigma^2) = \\exp(0) = 1\n$$\nThe new mean is $\\mu+v = -\\sigma^2 + 2\\sigma^2 = \\sigma^2$. So $Y \\sim \\mathcal{N}(\\sigma^2, 2\\sigma^2)$.\nThe first integral becomes:\n$$\n\\int_{-\\infty}^{0} 1 \\cdot f_Y(x) \\, dx = P(Y \\le 0) = P\\left(\\frac{Y - \\sigma^2}{\\sqrt{2\\sigma^2}} \\le \\frac{0 - \\sigma^2}{\\sqrt{2\\sigma^2}}\\right)\n$$\n$$\n= P\\left(S \\le -\\frac{\\sigma^2}{\\sqrt{2\\sigma^2}}\\right) = P\\left(S \\le -\\frac{\\sigma}{\\sqrt{2}}\\right) = \\Phi\\left(-\\frac{\\sigma}{\\sqrt{2}}\\right)\n$$\n\nCombining the two results:\n$$\n\\mathbb{E}[\\alpha] = \\Phi\\left(-\\frac{\\sigma}{\\sqrt{2}}\\right) + \\Phi\\left(-\\frac{\\sigma}{\\sqrt{2}}\\right) = 2\\Phi\\left(-\\frac{\\sigma}{\\sqrt{2}}\\right)\n$$\nTo express this as a function of $\\sigma^2$ as requested:\n$$\n\\mathbb{E}[\\alpha] = 2\\Phi\\left(-\\sqrt{\\frac{\\sigma^2}{2}}\\right)\n$$\n\nThe average acceptance probability $\\mathbb{E}[\\alpha]$ is a monotonically decreasing function of the noise variance $\\sigma^2$. As $\\sigma^2 \\to 0$, the argument of $\\Phi$ goes to $0$, so $\\mathbb{E}[\\alpha] \\to 2\\Phi(0) = 2(1/2) = 1$, which corresponds to the case with no estimation noise. As $\\sigma^2 \\to \\infty$, the argument goes to $-\\infty$, so $\\mathbb{E}[\\alpha] \\to 2\\Phi(-\\infty) = 0$, meaning the acceptance rate collapses for very noisy likelihood estimates.", "answer": "$$\n\\boxed{2\\Phi\\left(-\\sqrt{\\frac{\\sigma^2}{2}}\\right)}\n$$", "id": "3327363"}, {"introduction": "Our final practice integrates the concepts of estimator variance and acceptance rates into a holistic view of algorithmic efficiency. In any real-world application of PMMH, there is a fundamental trade-off: using more particles reduces the variance of the likelihood estimate, which can improve MCMC mixing, but at the cost of more computation per iteration. This exercise challenges you to formalize this trade-off using a stylized but powerful model of computational cost and sampler efficiency. By deriving an optimality condition and implementing an algorithm to find the ideal number of particles, you will tackle the practical challenge of balancing statistical performance and computational resources, a central theme in modern computational statistics [@problem_id:3327314].", "problem": "You are given a stylized, mathematically tractable model for the Particle Marginal Metropolis–Hastings (PMMH) algorithm within the pseudo-marginal Markov chain Monte Carlo framework. The aim is to analyze the trade-off between the number of particles $N$ in a Sequential Monte Carlo estimator and the random-walk proposal step size $\\sigma$ in PMMH by maximizing an efficiency proxy equal to effective sample size per unit time. You must derive a necessary optimality condition for the optimal particle number as a function of step size, and implement a program that computes the optimal integer particle counts across several test cases.\n\nAssumptions and model:\n\n- The target posterior for a single parameter $\\theta \\in \\mathbb{R}$ is locally Gaussian with curvature (Fisher information) $I  0$, so that a second-order expansion around the mode implies that, for a Gaussian random-walk Metropolis proposal $\\theta' = \\theta + \\sigma Z$ with $Z \\sim \\mathcal{N}(0,1)$, the difference in log target density can be modeled as a Gaussian random variable $\\Delta \\mid \\sigma \\sim \\mathcal{N}(m, v)$ with $m = - \\frac{1}{2} I \\sigma^2$ and $v = I \\sigma^2$. This reflects a standard quadratic approximation of the log posterior and is a well-tested approximation in diffusion limits of random-walk Metropolis.\n\n- In PMMH, the log-likelihood is estimated using a Sequential Monte Carlo estimator with $N$ particles. The estimator is unbiased on the likelihood scale and admits an additive noise model on the log scale. Specifically, the log-likelihood estimator error for a single likelihood evaluation is $\\xi \\sim \\mathcal{N}(-\\frac{1}{2}\\tau^2, \\tau^2)$ where the variance obeys $\\tau^2 = c / N$ for some constant $c  0$. For a proposal step, two independent such errors are used, so the difference in log-likelihood estimator noise is $\\varepsilon = \\xi' - \\xi \\sim \\mathcal{N}(0, 2 \\tau^2)$. This is a standard pseudo-marginal model based on unbiased log-normal estimators.\n\n- The Metropolis–Hastings acceptance probability for a proposal is $\\alpha = \\mathbb{E}\\left[ \\min \\left(1, \\exp(X) \\right) \\right]$, where $X = \\Delta + \\varepsilon$ is Gaussian with mean $m$ and variance $v + 2 \\tau^2$ under the above approximations.\n\n- The per-iteration computational cost is modeled as $t(N) = a N + b$ with $a  0$ and $b \\ge 0$. This captures linear particle cost and an iteration overhead cost.\n\n- As a proxy for mixing efficiency, use the expected squared jumping distance per unit time: $\\mathcal{J}(N, \\sigma) = \\sigma^2 \\alpha(m, v + 2 \\tau^2(N)) / t(N)$, where $\\alpha$ is the acceptance probability as above. This proxy is widely used as a surrogate for effective sample size in random-walk Metropolis in the Gaussian approximation regime.\n\nYour tasks:\n\n- Derivation. Starting from the definitions above and only using fundamental properties of the Gaussian distribution and unbiasedness on the likelihood scale, perform the following.\n  1. Show that the acceptance probability under the Gaussian model can be written as an expectation over a Gaussian random variable $X \\sim \\mathcal{N}(m, v + 2 \\tau^2)$: $\\alpha = \\mathbb{E}[ \\min(1, e^X) ]$.\n  2. Evaluate this expectation in closed form using elementary properties of the Gaussian distribution and indicator functions.\n  3. Introduce the continuous relaxation of the particle count via $x = \\tau^2 = c/N$ with $x \\in (0, c]$, and write $\\mathcal{J}$ as a function of $x$ and $\\sigma$ with cost $t(x) = a c / x + b$.\n  4. Derive the necessary optimality condition for the maximizer $x^\\star(\\sigma)$ of $\\mathcal{J}$ by setting the derivative of $\\log \\mathcal{J}$ with respect to $x$ to zero. Express this condition explicitly in terms of $m$, $v$, $x$, $a$, $b$, and $c$ without assuming any specific numerical values.\n  5. Argue the monotonicity of the acceptance probability with respect to $x$ under this model and the existence and uniqueness (or selection of boundary optima) of $x^\\star(\\sigma)$ where appropriate.\n\n- Algorithm design. Based on the derivation:\n  1. Propose a numerically stable algorithm to compute the optimal integer particle count $N^\\star(\\sigma)$ for a given $\\sigma$ by maximizing $\\mathcal{J}(N, \\sigma)$ over integer $N \\in \\{1, 2, \\dots, N_{\\max}\\}$ for a chosen finite $N_{\\max}$, explaining how this discrete maximization relates to the continuous relaxation.\n  2. State how to break ties if multiple $N$ yield the same maximal value within floating-point tolerance.\n\n- Implementation. Implement a complete, runnable program that:\n  1. Defines a function for the acceptance probability $\\alpha(m, v_{\\mathrm{tot}})$ for $X \\sim \\mathcal{N}(m, v_{\\mathrm{tot}})$.\n  2. Defines a function for the efficiency proxy $\\mathcal{J}(N, \\sigma)$ under the model described above.\n  3. For each test case in the suite below, computes the optimal integer particle count $N^\\star(\\sigma)$ that maximizes $\\mathcal{J}(N, \\sigma)$ over $N \\in \\{1, \\dots, N_{\\max}\\}$ with $N_{\\max} = 1000$, selecting the smallest $N$ in the event of ties up to a tolerance of $10^{-12}$.\n  4. Produces exactly one line of output, which is a comma-separated list of the optimal integers for each test case, enclosed in square brackets.\n\nTest suite:\n\nUse the following parameter sets $(I, c, a, b, \\sigma)$ as the test suite. All variables and numbers below are mathematically dimensionless; there are no physical units.\n\n- Case A (small step size, moderate overhead): $I = 1.0$, $c = 2.0$, $a = 1.0$, $b = 5.0$, $\\sigma = 0.1$.\n- Case B (moderate step size, moderate overhead): $I = 1.0$, $c = 2.0$, $a = 1.0$, $b = 5.0$, $\\sigma = 0.7$.\n- Case C (large step size, moderate overhead): $I = 1.0$, $c = 2.0$, $a = 1.0$, $b = 5.0$, $\\sigma = 1.5$.\n- Case D (moderate step size, zero overhead): $I = 1.0$, $c = 2.0$, $a = 1.0$, $b = 0.0$, $\\sigma = 0.7$.\n- Case E (moderate step size, higher particle cost and higher noise at $N = 1$): $I = 1.0$, $c = 4.0$, $a = 2.0$, $b = 1.0$, $\\sigma = 0.7$.\n- Case F (moderate step size, high overhead and low noise at $N = 1$): $I = 1.0$, $c = 0.5$, $a = 1.0$, $b = 10.0$, $\\sigma = 1.0$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the optimal particle counts for the above six cases as a comma-separated list of integers enclosed in square brackets, in the order A, B, C, D, E, F; for example, an output of the form $[n_A,n_B,n_C,n_D,n_E,n_F]$.", "solution": "The problem statement has been validated and is deemed sound, self-contained, and scientifically grounded within the field of computational statistics and Monte Carlo methods. All necessary models, parameters, and objectives are clearly defined, presenting a well-posed problem. We may therefore proceed with the derivation and implementation as requested.\n\nThe problem asks for an analysis of the trade-off between the number of particles, $N$, and the proposal step size, $\\sigma$, in the context of the Particle Marginal Metropolis-Hastings (PMMH) algorithm. The goal is to maximize an efficiency proxy, the expected squared jumping distance per unit time, $\\mathcal{J}(N, \\sigma)$.\n\n**Derivation**\n\n**1. The PMMH Acceptance Probability**\n\nThe Metropolis-Hastings acceptance probability for a proposal $\\theta \\to \\theta'$ is generally given by $\\min\\left(1, \\frac{p(\\theta'|y)q(\\theta|\\theta')}{p(\\theta|y)q(\\theta'|\\theta)}\\right)$, where $p(\\cdot|y)$ is the target posterior and $q(\\cdot|\\cdot)$ is the proposal. In PMMH, the true likelihood is replaced by an unbiased estimate from a particle filter, rendering the log-posterior estimate a random variable. The acceptance ratio is therefore also a random variable.\nLet the log-target difference be approximated by $\\Delta \\sim \\mathcal{N}(m,v)$, and the difference in the log-likelihood estimator noise be $\\varepsilon \\sim \\mathcal{N}(0, 2\\tau^2)$. The total difference in the PMMH log-acceptance ratio is $X = \\Delta + \\varepsilon$. Given that the sum of independent Gaussian random variables is Gaussian, $X$ follows a Gaussian distribution with mean $m+0 = m$ and variance $v+2\\tau^2$. Thus, $X \\sim \\mathcal{N}(m, v+2\\tau^2)$.\nThe acceptance probability for a specific realization of the estimator noise is $\\min(1, \\exp(X))$. The overall acceptance probability, $\\alpha$, is the expectation of this quantity over the distribution of $X$, which is exactly what is stated in the problem:\n$$\n\\alpha = \\mathbb{E}\\left[ \\min \\left(1, e^X \\right) \\right]\n$$\nwhere the expectation is taken with respect to $X \\sim \\mathcal{N}(m, v+2\\tau^2)$.\n\n**2. Closed-Form Expression for the Acceptance Probability**\n\nTo evaluate this expectation, we integrate over the probability density function (PDF) of $X$. Let $v_{\\mathrm{tot}} = v+2\\tau^2$. Let $\\phi(x; \\mu, \\sigma^2)$ denote the PDF of a $\\mathcal{N}(\\mu, \\sigma^2)$ distribution.\n$$\n\\alpha = \\int_{-\\infty}^{\\infty} \\min(1, e^x) \\phi(x; m, v_{\\mathrm{tot}}) dx\n$$\nWe split the integral at $x=0$, since $\\min(1, e^x)$ changes its functional form there.\n$$\n\\alpha = \\int_{-\\infty}^{0} e^x \\phi(x; m, v_{\\mathrm{tot}}) dx + \\int_{0}^{\\infty} 1 \\cdot \\phi(x; m, v_{\\mathrm{tot}}) dx\n$$\nThe second integral is the probability $P(X  0)$, which is $1 - P(X \\le 0)$. Letting $\\Phi(\\cdot)$ be the standard normal cumulative distribution function (CDF), this is $1 - \\Phi\\left(\\frac{0-m}{\\sqrt{v_{\\mathrm{tot}}}}\\right) = \\Phi\\left(\\frac{m}{\\sqrt{v_{\\mathrm{tot}}}}\\right)$.\n\nFor the first integral, we combine the exponential terms:\n$$\ne^x \\phi(x; m, v_{\\mathrm{tot}}) = e^x \\frac{1}{\\sqrt{2\\pi v_{\\mathrm{tot}}}} \\exp\\left(-\\frac{(x-m)^2}{2v_{\\mathrm{tot}}}\\right) = \\frac{1}{\\sqrt{2\\pi v_{\\mathrm{tot}}}} \\exp\\left(x - \\frac{x^2 - 2mx + m^2}{2v_{\\mathrm{tot}}}\\right)\n$$\nCompleting the square for the terms involving $x$ in the exponent:\n$$\nx - \\frac{(x-m)^2}{2v_{\\mathrm{tot}}} = -\\frac{x^2 - 2(m+v_{\\mathrm{tot}})x + m^2}{2v_{\\mathrm{tot}}} = -\\frac{(x-(m+v_{\\mathrm{tot}}))^2 - (m+v_{\\mathrm{tot}})^2 + m^2}{2v_{\\mathrm{tot}}}\n$$\n$$\n= -\\frac{(x-(m+v_{\\mathrm{tot}}))^2}{2v_{\\mathrm{tot}}} + \\frac{2mv_{\\mathrm{tot}}+v_{\\mathrm{tot}}^2}{2v_{\\mathrm{tot}}} = -\\frac{(x-(m+v_{\\mathrm{tot}}))^2}{2v_{\\mathrm{tot}}} + m + \\frac{v_{\\mathrm{tot}}}{2}\n$$\nTherefore, $e^x \\phi(x; m, v_{\\mathrm{tot}}) = e^{m + v_{\\mathrm{tot}}/2} \\phi(x; m+v_{\\mathrm{tot}}, v_{\\mathrm{tot}})$.\nThe first integral becomes:\n$$\n\\int_{-\\infty}^{0} e^{m + v_{\\mathrm{tot}}/2} \\phi(x; m+v_{\\mathrm{tot}}, v_{\\mathrm{tot}}) dx = e^{m + v_{\\mathrm{tot}}/2} P(Y \\le 0)\n$$\nwhere $Y \\sim \\mathcal{N}(m+v_{\\mathrm{tot}}, v_{\\mathrm{tot}})$. This probability is $\\Phi\\left(\\frac{0-(m+v_{\\mathrm{tot}})}{\\sqrt{v_{\\mathrm{tot}}}}\\right) = \\Phi\\left(-\\frac{m+v_{\\mathrm{tot}}}{\\sqrt{v_{\\mathrm{tot}}}}\\right)$.\nCombining the two parts, the acceptance probability is:\n$$\n\\alpha(m, v_{\\mathrm{tot}}) = e^{m + v_{\\mathrm{tot}}/2} \\Phi\\left(-\\frac{m+v_{\\mathrm{tot}}}{\\sqrt{v_{\\mathrm{tot}}}}\\right) + \\Phi\\left(\\frac{m}{\\sqrt{v_{\\mathrm{tot}}}}\\right)\n$$\n\n**3. Continuous Relaxation and Efficiency Function**\n\nWe introduce the continuous variable $x = \\tau^2 = c/N$, where $x \\in (0, c]$ is a proxy for the estimator variance. The number of particles is $N = c/x$. The computational cost becomes a function of $x$:\n$$\nt(x) = aN+b = a(c/x) + b = \\frac{ac}{x} + b\n$$\nThe total variance in $X$ is $v_{\\mathrm{tot}}(x) = v + 2x$. The efficiency proxy $\\mathcal{J}$ is now a function of $x$ and $\\sigma$:\n$$\n\\mathcal{J}(x, \\sigma) = \\frac{\\sigma^2 \\alpha(m, v+2x)}{ac/x + b}\n$$\n\n**4. Necessary Optimality Condition**\n\nTo find the optimal $x^\\star(\\sigma)$ that maximizes $\\mathcal{J}(x, \\sigma)$, we maximize its logarithm, $\\log\\mathcal{J}(x, \\sigma)$, which is analytically more convenient.\n$$\n\\log \\mathcal{J}(x, \\sigma) = \\log(\\sigma^2) + \\log(\\alpha(m, v+2x)) - \\log(ac/x + b)\n$$\nThe necessary condition for an interior maximum is that the derivative with respect to $x$ is zero:\n$$\n\\frac{\\partial}{\\partial x} \\log \\mathcal{J}(x, \\sigma) = \\frac{1}{\\alpha} \\frac{\\partial \\alpha}{\\partial x} - \\frac{1}{t(x)} \\frac{\\partial t(x)}{\\partial x} = 0\n$$\nThe derivative of the cost term is $\\frac{\\partial t(x)}{\\partial x} = \\frac{\\partial}{\\partial x}(acx^{-1} + b) = -acx^{-2}$.\nFor the alpha term, we use the chain rule. Let $V = v+2x$. Then $\\frac{\\partial V}{\\partial x}=2$.\n$$\n\\frac{\\partial \\alpha}{\\partial x} = \\frac{\\partial \\alpha}{\\partial V} \\frac{\\partial V}{\\partial x} = 2 \\frac{\\partial \\alpha}{\\partial V}\n$$\nUsing the identity $e^{m+V/2}\\phi\\left(-\\frac{m+V}{\\sqrt{V}}\\right) = \\phi\\left(\\frac{m}{\\sqrt{V}}\\right)$, we have:\n$$\n\\frac{\\partial \\alpha}{\\partial V} = \\frac{\\partial}{\\partial V} \\left[ e^{m+V/2} \\Phi\\left(-\\frac{m+V}{\\sqrt{V}}\\right) \\right] + \\frac{\\partial}{\\partial V} \\left[ \\Phi\\left(\\frac{m}{\\sqrt{V}}\\right) \\right]\n$$\n$$\n= \\frac{1}{2} e^{m+V/2} \\Phi\\left(-\\frac{m+V}{\\sqrt{V}}\\right) + e^{m+V/2} \\phi\\left(-\\frac{m+V}{\\sqrt{V}}\\right) \\left(\\frac{m-V}{2V^{3/2}}\\right) + \\phi\\left(\\frac{m}{\\sqrt{V}}\\right) \\left(-\\frac{m}{2V^{3/2}}\\right)\n$$\n$$\n= \\frac{1}{2} e^{m+V/2} \\Phi\\left(-\\frac{m+V}{\\sqrt{V}}\\right) + \\phi\\left(\\frac{m}{\\sqrt{V}}\\right) \\left(\\frac{m-V}{2V^{3/2}}\\right) - \\phi\\left(\\frac{m}{\\sqrt{V}}\\right) \\left(\\frac{m}{2V^{3/2}}\\right)\n$$\n$$\n= \\frac{1}{2} e^{m+V/2} \\Phi\\left(-\\frac{m+V}{\\sqrt{V}}\\right) - \\frac{V}{2V^{3/2}}\\phi\\left(\\frac{m}{\\sqrt{V}}\\right) = \\frac{1}{2} \\left[ e^{m+V/2}\\Phi\\left(-\\frac{m+V}{\\sqrt{V}}\\right) - \\frac{1}{\\sqrt{V}}\\phi\\left(\\frac{m}{\\sqrt{V}}\\right) \\right]\n$$\nThe optimality condition $\\frac{1}{\\alpha} \\frac{\\partial \\alpha}{\\partial x} = \\frac{1}{t(x)} \\frac{\\partial t(x)}{\\partial x}$ becomes:\n$$\n\\frac{2}{\\alpha(m, V)} \\frac{\\partial \\alpha}{\\partial V} = \\frac{-acx^{-2}}{acx^{-1}+b} = \\frac{-ac}{x(ac+bx)}\n$$\nSubstituting the expression for $\\frac{\\partial \\alpha}{\\partial V}$ with $V=v+2x$:\n$$\n\\frac{1}{\\alpha(m, v+2x)}\\left[ e^{m+(v+2x)/2}\\Phi\\left(-\\frac{m+v+2x}{\\sqrt{v+2x}}\\right) - \\frac{1}{\\sqrt{v+2x}}\\phi\\left(\\frac{m}{\\sqrt{v+2x}}\\right) \\right] = \\frac{-ac}{x(ac+bx)}\n$$\nThis is the necessary optimality condition for $x^\\star(\\sigma)$, implicitly defining the optimal estimator variance as a function of the model parameters.\n\n**5. Existence and Uniqueness of the Optimum**\n\nThe acceptance probability $\\alpha$ is a monotonically decreasing function of the noise variance parameter $x=\\tau^2$, as more noise degrades the quality of the acceptance ratio estimate. This can be seen by noting that $\\frac{\\partial \\alpha}{\\partial V}  0$, since $m0$ implies that $e^{m+V/2}\\Phi(-\\frac{m+V}{\\sqrt{V}})$ is a very small positive number, while $\\frac{1}{\\sqrt{V}}\\phi(\\frac{m}{\\sqrt{V}})$ is a larger positive number.\nThe efficiency $\\mathcal{J}(N, \\sigma)$ is being optimized for integer $N$ in the range $\\{1, 2, \\ldots, N_{\\max}\\}$. This is a finite set. The function $\\mathcal{J}(N, \\sigma)$ is well-defined for all $N \\ge 1$. Therefore, a maximum value is guaranteed to exist on this set.\nWhile a full analysis of the concavity of $\\log \\mathcal{J}(x, \\sigma)$ is complex, such efficiency functions are typically unimodal. As $N \\to \\infty$ (or $x \\to 0$), the cost $t(N) \\to \\infty$, so $\\mathcal{J} \\to 0$. As $N \\to 1$ (or $x \\to c$), $\\mathcal{J}$ is some finite positive value. This behavior, combined with the expected single peak, suggests a unique optimum for $N$ is likely for most parameter settings. Should multiple values of $N$ yield the same maximal efficiency, the problem specifies a tie-breaking rule.\n\n**Algorithm Design**\n\n**1. Numerical Optimization Algorithm**\n\nThe objective is to find $N^\\star = \\arg\\max_{N \\in \\{1, 2, \\ldots, N_{\\max}\\}} \\mathcal{J}(N, \\sigma)$. Given that $N_{\\max} = 1000$ is a relatively small number, a direct search is the most robust and straightforward approach.\nThe algorithm is as follows:\n1. For a given parameter set $(I, c, a, b, \\sigma)$, define the search range for the number of particles, $N \\in \\{1, 2, \\ldots, 1000\\}$.\n2. Pre-compute the constants $m = - \\frac{1}{2} I \\sigma^2$ and $v = I \\sigma^2$.\n3. For each integer $N$ in the search range, calculate the efficiency proxy $\\mathcal{J}(N, \\sigma)$:\n    a. Compute $\\tau^2 = c/N$.\n    b. Compute total variance $v_{\\mathrm{tot}} = v + 2\\tau^2$.\n    c. Evaluate the acceptance probability $\\alpha(m, v_{\\mathrm{tot}})$ using the derived closed-form expression.\n    d. Compute the cost $t(N) = aN+b$.\n    e. Compute $\\mathcal{J}(N, \\sigma) = \\sigma^2 \\alpha / t(N)$.\n4. Store the computed $\\mathcal{J}(N, \\sigma)$ values for all $N$ in an array.\n5. Find the maximum value in this array.\n6. Identify all values of $N$ for which the efficiency is within a tolerance of $10^{-12}$ of the maximum.\n7. The optimal particle count, $N^\\star$, is the minimum $N$ from this set.\n\nThis procedure directly implements the optimization over the discrete set of allowed $N$ values, thus avoiding the need to solve the more complex continuous optimality equation for $x^\\star$ and then rounding.\n\n**2. Tie-Breaking Rule**\n\nIf multiple integer values of $N$ result in a maximal value for $\\mathcal{J}$ (within a floating-point tolerance of $10^{-12}$), the problem requires selecting the smallest such $N$. The proposed algorithm naturally accommodates this by searching through the array of results for all $N$ that are close to the maximum and picking the one with the smallest index, which corresponds to the smallest $N$.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\n# Define global constants as specified in the problem\nN_MAX = 1000\nTOLERANCE = 1e-12\n\ndef acceptance_probability(m, v_tot):\n    \"\"\"\n    Computes the PMMH acceptance probability for X ~ N(m, v_tot).\n    The derivation is based on E[min(1, exp(X))].\n\n    Args:\n        m (float or np.ndarray): Mean of the Gaussian random variable X.\n        v_tot (float or np.ndarray): Variance of the Gaussian random variable X.\n\n    Returns:\n        float or np.ndarray: The acceptance probability.\n    \"\"\"\n    # Ensure variance is positive to avoid sqrt/division issues.\n    # The model guarantees v_tot  0, but this adds numerical robustness.\n    v_tot = np.maximum(v_tot, 1e-300)\n    sd_tot = np.sqrt(v_tot)\n\n    # The closed-form expression for the acceptance probability\n    term1 = np.exp(m + 0.5 * v_tot) * norm.cdf(-(m + v_tot) / sd_tot)\n    term2 = norm.cdf(m / sd_tot)\n    \n    return term1 + term2\n\ndef efficiency_proxy(N, I, c, a, b, sigma):\n    \"\"\"\n    Computes the efficiency proxy J(N, sigma) for a given number of particles N.\n    This function is designed to be vectorized over N.\n\n    Args:\n        N (np.ndarray): Array of particle counts.\n        I (float): Fisher information (curvature).\n        c (float): Constant for estimator variance.\n        a (float): Linear cost per particle.\n        b (float): Fixed cost per iteration.\n        sigma (float): Random-walk proposal step size.\n\n    Returns:\n        np.ndarray: The efficiency proxy values corresponding to each N.\n    \"\"\"\n    # Model parameters for the log-target difference\n    m = -0.5 * I * sigma**2\n    v = I * sigma**2\n    \n    # Variance of the log-likelihood estimator error\n    tau2 = c / N\n    \n    # Total variance for the log-acceptance ratio variable X\n    v_tot = v + 2 * tau2\n    \n    # Compute acceptance probability\n    alpha = acceptance_probability(m, v_tot)\n    \n    # Per-iteration computational cost\n    cost = a * N + b\n    \n    # Efficiency proxy: expected squared jumping distance per unit time\n    # Cost is always positive for a  0, N = 1, b = 0\n    J = (sigma**2 * alpha) / cost\n    \n    return J\n\ndef find_optimal_N(params):\n    \"\"\"\n    Finds the optimal integer N in [1, N_MAX] that maximizes the efficiency proxy.\n\n    Args:\n        params (tuple): A tuple of parameters (I, c, a, b, sigma).\n\n    Returns:\n        int: The optimal number of particles N*.\n    \"\"\"\n    I, c, a, b, sigma = params\n    \n    # Create the search range for N\n    N_range = np.arange(1, N_MAX + 1)\n    \n    # Vectorized computation of J over the entire range of N\n    J_values = efficiency_proxy(N_range, I, c, a, b, sigma)\n    \n    # Find the maximum value of the efficiency proxy\n    max_J = np.max(J_values)\n    \n    # Check for non-finite values as a safeguard\n    if not np.isfinite(max_J):\n        finite_J_values = J_values[np.isfinite(J_values)]\n        if len(finite_J_values) == 0:\n            return 1 # Fallback to a default value\n        max_J = np.max(finite_J_values)\n\n    # Find all indices where J is close to the max value, respecting the tolerance\n    # np.where returns a tuple of arrays, one for each dimension.\n    tie_indices = np.where(J_values = max_J - TOLERANCE)[0]\n    \n    # The first index in the resulting array corresponds to the smallest N\n    # due to the ascending order of N_range. This handles the tie-breaking rule.\n    optimal_N_index = tie_indices[0]\n    \n    optimal_N = N_range[optimal_N_index]\n    \n    return optimal_N\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Test suite from the problem statement\n    test_cases = [\n        # (I, c, a, b, sigma)\n        (1.0, 2.0, 1.0, 5.0, 0.1),   # Case A\n        (1.0, 2.0, 1.0, 5.0, 0.7),   # Case B\n        (1.0, 2.0, 1.0, 5.0, 1.5),   # Case C\n        (1.0, 2.0, 1.0, 0.0, 0.7),   # Case D\n        (1.0, 4.0, 2.0, 1.0, 0.7),   # Case E\n        (1.0, 0.5, 1.0, 10.0, 1.0),  # Case F\n    ]\n\n    results = []\n    for case in test_cases:\n        opt_N = find_optimal_N(case)\n        results.append(opt_N)\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3327314"}]}