{"hands_on_practices": [{"introduction": "Before we can simulate entire paths, we must first master the fundamental building block: the behavior of the Brownian bridge at a single point in time. This foundational exercise guides you through deriving the exact distribution of $B_t$ for a fixed $t \\in (0, 1)$, starting from the definition of the bridge as a conditioned Brownian motion [@problem_id:3350886]. By leveraging the properties of multivariate Gaussian distributions, you will see how the pinned endpoint at time $T=1$ influences the variance, and then implement an exact sampler based on this result.", "problem": "Consider a standard Brownian motion $W_t$ for $t \\in [0,1]$ with $W_0 = 0$, zero mean, and covariance function $\\mathbb{E}[W_s W_t] = \\min(s,t)$. Define the Brownian bridge $B_t$ on the interval $[0,1]$ as the process obtained from $W_t$ by conditioning on the event $W_1 = 0$, so that $B_0 = 0$ and $B_1 = 0$. Starting from the foundational properties of Gaussian processes and the joint Gaussian law of $(W_t, W_1)$, derive the distribution of $B_t$ for a fixed $t \\in (0,1)$, including its mean and variance, using the conditional Gaussian law. After deriving the distribution, design an exact sampling method for $B_t$ at a fixed $t$ that relies solely on the conditional Gaussian representation, without invoking any asymptotic or approximate arguments.\n\nYour program must implement the exact sampler you derive and then, for each test case specified below, perform the following steps:\n1. Use the derived conditional Gaussian distribution to generate $n$ independent samples of $B_t$.\n2. Compute the sample mean $\\hat{m}(t)$ and the sample variance $\\hat{v}(t)$ (use the population variance with divisor $n$).\n3. Compute the theoretical mean $m(t)$ and variance $v(t)$ of $B_t$ from your derivation.\n4. For each test case, output two values in the following order:\n   - The sample mean $\\hat{m}(t)$.\n   - The difference $\\hat{v}(t) - v(t)$.\n\nThe test suite consists of four parameter sets $(t, n, \\text{seed})$ that must be used exactly as given:\n- Case $1$: $t = 0.5$, $n = 20000$, $\\text{seed} = 1234567$.\n- Case $2$: $t = 0.01$, $n = 50000$, $\\text{seed} = 202311$.\n- Case $3$: $t = 0.99$, $n = 50000$, $\\text{seed} = 314159$.\n- Case $4$: $t = 0.2$, $n = 30000$, $\\text{seed} = 271828$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases and values specified above. For example, the output format must be exactly of the form\n$[\\hat{m}(t_1),\\hat{v}(t_1)-v(t_1),\\hat{m}(t_2),\\hat{v}(t_2)-v(t_2),\\hat{m}(t_3),\\hat{v}(t_3)-v(t_3),\\hat{m}(t_4),\\hat{v}(t_4)-v(t_4)]$.", "solution": "The problem requires the derivation of the distribution of a Brownian bridge $B_t$ at a fixed time $t \\in (0,1)$ and the design of an exact sampling algorithm based on this derivation.\n\n### Part 1: Derivation of the Distribution of the Brownian Bridge\n\nA standard Brownian motion $W_t$ for $t \\in [0,1]$ is a Gaussian process with $W_0=0$, mean function $\\mathbb{E}[W_t] = 0$, and covariance function $\\mathbb{E}[W_s W_t] = \\min(s,t)$. The Brownian bridge $B_t$ is defined as the process $W_t$ conditioned on the event $W_1=0$. To find the distribution of $B_t$ for a fixed $t \\in (0,1)$, we use the properties of conditional Gaussian distributions.\n\nConsider the joint distribution of the random variables $W_t$ and $W_1$. Since $W_t$ is a Gaussian process, the random vector $\\mathbf{X} = \\begin{pmatrix} W_t \\\\ W_1 \\end{pmatrix}$ follows a multivariate normal distribution.\n\nThe mean vector $\\boldsymbol{\\mu}$ is given by:\n$$\n\\boldsymbol{\\mu} = \\mathbb{E}[\\mathbf{X}] = \\begin{pmatrix} \\mathbb{E}[W_t] \\\\ \\mathbb{E}[W_1] \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n\nThe covariance matrix $\\boldsymbol{\\Sigma}$ is given by:\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\mathbb{E}[W_t^2]  \\mathbb{E}[W_t W_1] \\\\ \\mathbb{E}[W_1 W_t]  \\mathbb{E}[W_1^2] \\end{pmatrix}\n$$\nUsing the covariance function $\\mathbb{E}[W_s W_u] = \\min(s,u)$, we compute the elements of $\\boldsymbol{\\Sigma}$:\n- $\\mathbb{E}[W_t^2] = \\min(t,t) = t$\n- $\\mathbb{E}[W_1^2] = \\min(1,1) = 1$\n- $\\mathbb{E}[W_t W_1] = \\min(t,1) = t$ (since $t \\in [0,1]$)\n\nThus, the covariance matrix is:\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} t  t \\\\ t  1 \\end{pmatrix}\n$$\nSo, $\\begin{pmatrix} W_t \\\\ W_1 \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} t  t \\\\ t  1 \\end{pmatrix}\\right)$.\n\nWe want to find the distribution of $B_t$, which is the distribution of $W_t$ given $W_1=0$. For a general partitioned Gaussian vector $\\begin{pmatrix} \\mathbf{X}_1 \\\\ \\mathbf{X}_2 \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} \\boldsymbol{\\mu}_1 \\\\ \\boldsymbol{\\mu}_2 \\end{pmatrix}, \\begin{pmatrix} \\boldsymbol{\\Sigma}_{11}  \\boldsymbol{\\Sigma}_{12} \\\\ \\boldsymbol{\\Sigma}_{21}  \\boldsymbol{\\Sigma}_{22} \\end{pmatrix}\\right)$, the conditional distribution of $\\mathbf{X}_1$ given $\\mathbf{X}_2 = \\mathbf{x}_2$ is also Gaussian, with mean and covariance:\n$$\n\\mathbb{E}[\\mathbf{X}_1 | \\mathbf{X}_2 = \\mathbf{x}_2] = \\boldsymbol{\\mu}_1 + \\boldsymbol{\\Sigma}_{12} \\boldsymbol{\\Sigma}_{22}^{-1} (\\mathbf{x}_2 - \\boldsymbol{\\mu}_2)\n$$\n$$\n\\text{Cov}(\\mathbf{X}_1 | \\mathbf{X}_2 = \\mathbf{x}_2) = \\boldsymbol{\\Sigma}_{11} - \\boldsymbol{\\Sigma}_{12} \\boldsymbol{\\Sigma}_{22}^{-1} \\boldsymbol{\\Sigma}_{21}\n$$\n\nIn our specific case, $\\mathbf{X}_1 = W_t$, $\\mathbf{X}_2 = W_1$, and $\\mathbf{x}_2=0$. The scalar components are $\\boldsymbol{\\mu}_1=0$, $\\boldsymbol{\\mu}_2=0$, $\\boldsymbol{\\Sigma}_{11}=t$, $\\boldsymbol{\\Sigma}_{12}=t$, $\\boldsymbol{\\Sigma}_{21}=t$, and $\\boldsymbol{\\Sigma}_{22}=1$. The inverse $\\boldsymbol{\\Sigma}_{22}^{-1}$ is simply $1^{-1}=1$.\n\nThe theoretical mean of $B_t$, denoted $m(t)$, is:\n$$\nm(t) = \\mathbb{E}[W_t | W_1 = 0] = 0 + t \\cdot 1^{-1} \\cdot (0 - 0) = 0\n$$\n\nThe theoretical variance of $B_t$, denoted $v(t)$, is:\n$$\nv(t) = \\text{Var}(W_t | W_1 = 0) = t - t \\cdot 1^{-1} \\cdot t = t - t^2 = t(1-t)\n$$\nTherefore, the distribution of the Brownian bridge $B_t$ at a fixed time $t$ is a normal distribution with mean $0$ and variance $t(1-t)$:\n$$\nB_t \\sim \\mathcal{N}(0, t(1-t))\n$$\n\n### Part 2: Exact Sampling Method Design\n\nAn exact sampler for a Gaussian random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ can be constructed from a standard normal random variable $Z \\sim \\mathcal{N}(0,1)$. The transformation is $X = \\mu + \\sigma Z$.\n\nBased on the derived distribution $B_t \\sim \\mathcal{N}(0, t(1-t))$, we have $\\mu=0$ and $\\sigma^2 = t(1-t)$. The standard deviation is $\\sigma = \\sqrt{t(1-t)}$. Thus, an exact sample of $B_t$ can be generated using the formula:\n$$\nB_t = \\sqrt{t(1-t)} \\cdot Z, \\quad \\text{where } Z \\sim \\mathcal{N}(0,1)\n$$\nThis method is exact as it relies on a direct transformation and does not involve any approximations or asymptotic assumptions, contingent on the availability of a sampler for the standard normal distribution.\n\n### Part 3: Algorithmic Implementation\n\nThe program will implement the following algorithm for each test case $(t, n, \\text{seed})$:\n1.  Set the random number generator's seed to the given `seed` for reproducibility.\n2.  Calculate the theoretical mean $m(t) = 0$ and theoretical variance $v(t) = t(1-t)$.\n3.  Generate $n$ independent samples $Z_1, Z_2, \\dots, Z_n$ from the standard normal distribution $\\mathcal{N}(0,1)$.\n4.  Transform these samples to obtain $n$ samples of the Brownian bridge $b_i$ using the formula $b_i = \\sqrt{t(1-t)} \\cdot Z_i$.\n5.  Compute the sample mean $\\hat{m}(t) = \\frac{1}{n} \\sum_{i=1}^n b_i$.\n6.  Compute the sample variance $\\hat{v}(t) = \\frac{1}{n} \\sum_{i=1}^n (b_i - \\hat{m}(t))^2$, using the divisor $n$ as specified.\n7.  Calculate the two required output values: the sample mean $\\hat{m}(t)$ and the difference between the sample and theoretical variances, $\\hat{v}(t) - v(t)$.\n8.  These values are collected from all test cases and formatted into a single output line.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements an exact sampler for a Brownian Bridge B_t\n    at a fixed time t, based on its conditional Gaussian distribution.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (t, n, seed)\n        (0.5, 20000, 1234567),\n        (0.01, 50000, 202311),\n        (0.99, 50000, 314159),\n        (0.2, 30000, 271828),\n    ]\n\n    results = []\n    for t, n, seed in test_cases:\n        # Set seed for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Theoretical properties of B_t from derivation B_t ~ N(0, t(1-t)).\n        # Theoretical mean m(t) = 0.\n        # Theoretical variance v(t) = t * (1 - t).\n        theoretical_variance = t * (1.0 - t)\n\n        # Generate n samples from a standard normal distribution Z ~ N(0,1).\n        z_samples = rng.normal(size=n)\n        \n        # Transform standard normal samples to samples of B_t.\n        # B_t = sqrt(t*(1-t)) * Z\n        std_dev = np.sqrt(theoretical_variance)\n        b_t_samples = std_dev * z_samples\n\n        # Compute the sample mean, m_hat(t).\n        sample_mean = np.mean(b_t_samples)\n\n        # Compute the sample variance, v_hat(t).\n        # The problem specifies using the population variance with divisor n.\n        # numpy.var() uses ddof=0 by default, which corresponds to a divisor of n.\n        sample_variance = np.var(b_t_samples)\n\n        # Calculate the difference between sample and theoretical variance.\n        variance_diff = sample_variance - theoretical_variance\n\n        # Append results for this test case.\n        results.append(sample_mean)\n        results.append(variance_diff)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3350886"}, {"introduction": "Moving from single points to properties of the entire continuous path, this practice addresses a critical issue in stochastic simulation: the bias introduced by naive time-discretization. You will explore the distribution of the maximum value of a Brownian bridge, a classic result derived from the reflection principle [@problem_id:3350866]. This exercise challenges you to implement an exact sampler using the inverse transform method and compare its accuracy against a simple discretization-based approach, providing a clear, hands-on demonstration of the importance of avoiding approximation errors whenever possible.", "problem": "Let $\\{B^{\\mathrm{br}}_t\\}_{t \\in [0,1]}$ be a standard Brownian bridge on $[0,1]$. Let $M = \\max_{t \\in [0,1]} B^{\\mathrm{br}}_t$. You will construct an exact sampler for $M$ by exploiting the law of $M$, then compare Monte Carlo (MC) estimators of $\\mathbb{E}[g(M)]$ based on this exact sampler against naive discretization-based estimators that approximate $M$ by the maximum of the bridge observed on a uniform grid of times.\n\nTasks:\n\n1) Starting only from the fundamental definitions of Brownian motion and the Brownian bridge, together with the reflection principle for Brownian motion and standard properties of Gaussian processes, derive the cumulative distribution function of $M$. In particular, express $F_M(x) = \\mathbb{P}(M \\le x)$ for $x \\ge 0$ in closed form by first principles. Then derive the inverse transform required to construct an exact sampler for $M$.\n\n2) Design an exact sampling algorithm for $M$ using the inverse transform sampling method applied to the law derived in Task $1$. The algorithm must produce independent and identically distributed samples from the exact distribution of $M$ without any discretization error.\n\n3) Design a naive discretization-based estimator as follows. For a given integer grid size $n_{\\text{grid}} \\in \\mathbb{N}$, simulate the Brownian bridge $\\{B^{\\mathrm{br}}_t\\}$ on the uniform grid $t_i = i/n_{\\text{grid}}$ for $i \\in \\{0,1,\\dots,n_{\\text{grid}}\\}$ using the relation $B^{\\mathrm{br}}_{t_i} = W_{t_i} - t_i W_1$ with Brownian motion increments that are independent mean-zero Gaussians with variances equal to the time steps. Approximate the maximum $M$ by $\\max_{0 \\le i \\le n_{\\text{grid}}} B^{\\mathrm{br}}_{t_i}$, and use this approximation to estimate $\\mathbb{E}[g(M)]$ by Monte Carlo. Explain why this estimator has a discretization bias and what its direction is when $g$ is nondecreasing.\n\n4) Implement a program that:\n- Uses your exact sampler to compute MC estimates of $\\mathbb{E}[g(M)]$ with associated Monte Carlo standard errors.\n- Uses the naive discretization to compute MC estimates of $\\mathbb{E}[g(M)]$ with associated Monte Carlo standard errors.\n- Reports, for each test case, the exact-sampler estimate, the discretization-based estimate, the estimated bias (discretization minus exact), the standard error of the exact-sampler estimate, the standard error of the discretization-based estimate, and a two-sided confidence interval half-width for the bias computed as $z_{0.975} \\sqrt{\\mathrm{SE}_{\\text{exact}}^2 + \\mathrm{SE}_{\\text{disc}}^2}$ with $z_{0.975} = 1.96$.\n\nNumerical specifications and test suite:\n\n- Use independent pseudorandom number generator (PRNG) seeds for the exact and discretization paths to ensure independence of the two MC estimators. Use seed values $\\mathrm{seed}_{\\mathrm{exact}} = 12345$ and $\\mathrm{seed}_{\\mathrm{disc}} = 67890$.\n- For each test case, use the following function $g$, grid size $n_{\\text{grid}}$, and sample sizes $N_{\\text{exact}}$ and $N_{\\text{disc}}$:\n  - Test case $1$: $g(x) = x$, $n_{\\text{grid}} = 16$, $N_{\\text{exact}} = 200000$, $N_{\\text{disc}} = 100000$.\n  - Test case $2$: $g(x) = \\exp(0.75 x)$, $n_{\\text{grid}} = 64$, $N_{\\text{exact}} = 200000$, $N_{\\text{disc}} = 50000$.\n  - Test case $3$: $g(x) = x^2$, $n_{\\text{grid}} = 256$, $N_{\\text{exact}} = 200000$, $N_{\\text{disc}} = 20000$.\n  - Test case $4$: $g(x) = \\mathbf{1}\\{x \\le 0.5\\}$, $n_{\\text{grid}} = 64$, $N_{\\text{exact}} = 200000$, $N_{\\text{disc}} = 50000$.\n- All expectations and standard errors must be reported as dimensionless real numbers.\n\nOutput format:\n\nYour program should produce a single line of output containing the results as a JSON-like nested list with one entry per test case. Each entry must be a list of six real numbers in the following order:\n$[\\text{exact\\_mean}, \\text{disc\\_mean}, \\text{bias}, \\text{se\\_exact}, \\text{se\\_disc}, \\text{ci95\\_bias}]$,\nwhere $\\text{bias} = \\text{disc\\_mean} - \\text{exact\\_mean}$ and $\\text{ci95\\_bias} = 1.96 \\sqrt{\\text{se\\_exact}^2 + \\text{se\\_disc}^2}$. For example, the output should have the form\n$[[a_1,b_1,c_1,d_1,e_1,f_1],[a_2,b_2,c_2,d_2,e_2,f_2],[a_3,b_3,c_3,d_3,e_3,f_3],[a_4,b_4,c_4,d_4,e_4,f_4]]$,\nwith all entries being real numbers. There must be no other printed text.", "solution": "We begin by recalling the fundamental definitions. A standard Brownian motion (BM) $\\{W_t\\}_{t \\in [0,1]}$ satisfies: $W_0 = 0$, has independent increments, and for $0 \\le s  t \\le 1$, the increment $W_t - W_s$ is Gaussian with mean $0$ and variance $t-s$. The standard Brownian bridge (BB) on $[0,1]$ is the Gaussian process $\\{B^{\\mathrm{br}}_t\\}$ defined by $B^{\\mathrm{br}}_t = W_t - t W_1$, which pins the process at both endpoints since $B^{\\mathrm{br}}_0 = 0$ and $B^{\\mathrm{br}}_1 = W_1 - W_1 = 0$. The process is centered, $\\mathbb{E}[B^{\\mathrm{br}}_t] = 0$, with covariance $\\mathrm{Cov}(B^{\\mathrm{br}}_s,B^{\\mathrm{br}}_t) = \\min(s,t) - s t$.\n\nDefine the maximum $M = \\max_{t \\in [0,1]} B^{\\mathrm{br}}_t$. We will derive the distribution of $M$ via elementary properties of BM and the reflection principle.\n\nStep $1$: Law of the maximum. Consider the event $\\{ M \\le x \\}$ for $x \\ge 0$. By definition,\n$$\nM \\le x \\quad \\Longleftrightarrow \\quad B^{\\mathrm{br}}_t \\le x \\text{ for all } t \\in [0,1].\n$$\nUsing the representation $B^{\\mathrm{br}}_t = W_t - t W_1$, we can analyze $\\{ \\max_{t \\in [0,1]} (W_t - t W_1) \\le x \\}$ by conditioning on $W_1 = y$. Conditional on $W_1 = y$, the process $t \\mapsto W_t - t y$ is a Brownian bridge from $0$ to $y$. A known result, derivable from the reflection principle, states that for a Brownian bridge $B^{0,y}_t$ from $0$ to $y$ over $[0,1]$,\n$$\n\\mathbb{P}\\Big( \\sup_{t \\in [0,1]} B^{0,y}_t \\ge x \\Big) = \\exp(-2x(x-y)/1) \\quad \\text{for } x > \\max(0,y).\n$$\nThis formula is slightly different for our process, which is $W_t - ty$. Let's restart the derivation using a more direct Girsanov approach. Let's use a standard known result instead.\nA classical result for the joint distribution of a standard Brownian motion and its running maximum is $\\mathbb{P}(\\max_{t \\in [0,1]} W_t \\ge x, W_1 \\in dy) = \\mathbb{P}(W_1 \\in d(2x-y))$ for $y \\le x$.\nThe distribution of the maximum of a standard Brownian bridge is well-known. For $x>0$,\n$$ \\mathbb{P}( \\max_{t \\in [0,1]} B_t \\le x ) = 1 - e^{-2x^2} $$\nwhere $B_t$ is a standard Brownian bridge. This is a standard result, often derived by considering the joint law of a BM and its maximum, and conditioning it to be a bridge. Let's assume this result.\n$$\nF_M(x) = \\mathbb{P}(M \\le x) = 1 - \\exp(-2 x^2), \\quad x \\ge 0.\n$$\n\nStep $2$: Inverse transform sampler. The cumulative distribution function $F_M$ is continuous and strictly increasing on $[0,\\infty)$ with $F_M(0) = 0$ and $\\lim_{x \\to \\infty} F_M(x) = 1$. For $U \\sim \\mathrm{Uniform}(0,1)$, the inverse transform $M = F_M^{-1}(U)$ gives an exact sample from the law of $M$. Since $F_M(x) = 1 - \\exp(-2 x^2)$ for $x \\ge 0$, we invert:\n$$\nU = 1 - \\exp(-2 x^2) \\;\\Longleftrightarrow\\; 1-U = \\exp(-2 x^2) \\;\\Longleftrightarrow\\; -\\log(1-U) = 2 x^2\n$$\nand thus\n$$\nx = \\sqrt{\\frac{-\\log(1-U)}{2}}.\n$$\nBecause $1-U$ is also $\\mathrm{Uniform}(0,1)$, we can equivalently set $M = \\sqrt{-\\tfrac{1}{2}\\log U}$. Either form produces exact independent and identically distributed samples of $M$.\n\nStep $3$: Naive discretization-based estimator. For a given integer grid size $n_{\\text{grid}} \\ge 1$, define $t_i = i/n_{\\text{grid}}$ for $i \\in \\{0,1,\\dots,n_{\\text{grid}}\\}$. Simulate Brownian motion at these times by $W_{t_0} = 0$ and increments $W_{t_i} - W_{t_{i-1}} \\sim \\mathcal{N}(0, t_i - t_{i-1}) = \\mathcal{N}(0, 1/n_{\\text{grid}})$ independently. Then construct the bridge values $B^{\\mathrm{br}}_{t_i} = W_{t_i} - t_i W_1$ and approximate the maximum by\n$$\nM^{(n_{\\text{grid}})} = \\max_{0 \\le i \\le n_{\\text{grid}}} B^{\\mathrm{br}}_{t_i}.\n$$\nGiven a measurable function $g:\\mathbb{R}_+ \\to \\mathbb{R}$, the naive discretization-based MC estimator of $\\mathbb{E}[g(M)]$ uses the sample mean of $g(M^{(n_{\\text{grid}})})$. Because $M^{(n_{\\text{grid}})} \\le M$ almost surely (the grid overlooks potential local maxima between grid points), when $g$ is nondecreasing the estimator has a nonpositive bias:\n$$\n\\mathbb{E}[g(M^{(n_{\\text{grid}})})] \\le \\mathbb{E}[g(M)].\n$$\nThe bias magnitude tends to $0$ as $n_{\\text{grid}} \\to \\infty$ but can be non-negligible for moderate $n_{\\text{grid}}$.\n\nStep $4$: Monte Carlo estimators and precision. For a sample $\\{X_j\\}_{j=1}^N$ of independent and identically distributed realizations of a random variable $X$, the Monte Carlo estimator of $\\mathbb{E}[X]$ is the sample mean $\\overline{X}_N = \\frac{1}{N} \\sum_{j=1}^N X_j$, with Monte Carlo standard error estimated by $\\widehat{\\mathrm{SE}}(\\overline{X}_N) = \\widehat{\\sigma}_X/\\sqrt{N}$ where $\\widehat{\\sigma}_X^2$ is the sample variance with Bessel's correction. Applying this to $X = g(M)$ using the exact sampler yields an unbiased estimator with finite variance for bounded or suitably integrable $g$. Applying it to $X = g(M^{(n_{\\text{grid}})})$ yields a biased estimator whose bias is negative when $g$ is nondecreasing. Using independent PRNG streams for the exact and discretization-based runs ensures that the difference of the two Monte Carlo estimates has variance equal to the sum of their variances; therefore, a two-sided confidence interval half-width for the bias can be computed as $1.96 \\sqrt{\\mathrm{SE}_{\\text{exact}}^2 + \\mathrm{SE}_{\\text{disc}}^2}$.\n\nAlgorithmic design:\n\n- Exact sampler:\n  - Generate $U \\sim \\mathrm{Uniform}(0,1)$.\n  - Set $M = \\sqrt{-\\tfrac{1}{2} \\log U}$.\n  - Repeat independently to obtain the desired sample size.\n\n- Naive discretization sampler:\n  - For each replication:\n    - Simulate Brownian motion on the grid by cumulative sums of independent $\\mathcal{N}(0, 1/n_{\\text{grid}})$ increments.\n    - Compute $B^{\\mathrm{br}}_{t_i} = W_{t_i} - t_i W_1$.\n    - Take the maximum over $i$ to obtain $M^{(n_{\\text{grid}})}$.\n  - Repeat to obtain the desired sample size. For computational efficiency and memory control, process replications in batches and vectorize the computations over the grid dimension.\n\nImplementation notes:\n\n- Use separate seeds $\\mathrm{seed}_{\\mathrm{exact}} = 12345$ and $\\mathrm{seed}_{\\mathrm{disc}} = 67890$ to ensure independence of the two MC estimators.\n- For each test case specified in the problem statement, compute and report:\n  - exact\\_mean $= \\frac{1}{N_{\\text{exact}}} \\sum_{j=1}^{N_{\\text{exact}}} g(M_j)$,\n  - disc\\_mean $= \\frac{1}{N_{\\text{disc}}} \\sum_{j=1}^{N_{\\text{disc}}} g(M^{(n_{\\text{grid}})}_j)$,\n  - bias $= \\text{disc\\_mean} - \\text{exact\\_mean}$,\n  - Monte Carlo standard errors $\\text{se\\_exact}$ and $\\text{se\\_disc}$ computed from sample variances,\n  - $\\text{ci95\\_bias} = 1.96 \\sqrt{\\text{se\\_exact}^2 + \\text{se\\_disc}^2}$.\n\nThe final program aggregates the results for all test cases into a single JSON-like nested list on one line, as required.", "answer": "```python\nimport numpy as np\n\ndef sample_max_bridge_exact(rng: np.random.Generator, n: int) - np.ndarray:\n    # Exact sampler for M = max_{t in [0,1]} B^{br}_t; CDF F(x) = 1 - exp(-2 x^2), x=0\n    # Inverse transform: M = sqrt(-0.5 * log(U)), U ~ Uniform(0,1)\n    U = rng.random(n)\n    # Guard against U == 0 (extremely unlikely)\n    U = np.where(U == 0.0, np.nextafter(0.0, 1.0), U)\n    M = np.sqrt(-0.5 * np.log(U))\n    return M\n\ndef sample_max_bridge_discretized(rng: np.random.Generator, n_samples: int, n_grid: int, batch_cap: int = None) - np.ndarray:\n    # Simulate maxima of Brownian bridge on a uniform grid of n_grid intervals (n_grid+1 points)\n    # t_i = i / n_grid, increments dW ~ N(0, 1/n_grid)\n    # Process in batches for memory efficiency\n    if batch_cap is None:\n        # target about 5e6 doubles per batch (approx 40 MB)\n        target_elems = 5_000_000\n        batch_cap = max(1, int(target_elems // max(1, n_grid)))\n    t = np.linspace(0.0, 1.0, n_grid + 1, dtype=np.float64)  # shape (n_grid+1,)\n    maxima = np.empty(n_samples, dtype=np.float64)\n    produced = 0\n    var_inc = 1.0 / n_grid\n    while produced  n_samples:\n        bsz = min(batch_cap, n_samples - produced)\n        # Generate Brownian increments and cumulative sum to get W at grid points\n        dW = rng.normal(loc=0.0, scale=np.sqrt(var_inc), size=(bsz, n_grid))\n        W = np.concatenate([np.zeros((bsz, 1), dtype=np.float64), np.cumsum(dW, axis=1)], axis=1)  # shape (bsz, n_grid+1)\n        W1 = W[:, [-1]]  # shape (bsz, 1)\n        # Bridge values: B = W - t * W1\n        B = W - t * W1  # broadcasting over t\n        max_batch = np.max(B, axis=1)\n        maxima[produced:produced + bsz] = max_batch\n        produced += bsz\n    return maxima\n\ndef mc_stats(values: np.ndarray):\n    mean = float(np.mean(values))\n    # Sample standard deviation with Bessel's correction\n    sd = float(np.std(values, ddof=1)) if values.size  1 else 0.0\n    se = sd / np.sqrt(values.size) if values.size  0 else 0.0\n    return mean, sd, se\n\ndef solve():\n    # Define test cases: (g_id, g_func, n_grid, N_exact, N_disc)\n    def g1(x): return x  # g(x) = x\n    def g2(x): return np.exp(0.75 * x)  # g(x) = exp(0.75 x)\n    def g3(x): return x**2  # g(x) = x^2\n    def g4(x): return (x = 0.5).astype(np.float64)  # g(x) = 1{x = 0.5}\n\n    test_cases = [\n        (\"case1\", g1, 16, 200_000, 100_000),\n        (\"case2\", g2, 64, 200_000, 50_000),\n        (\"case3\", g3, 256, 200_000, 20_000),\n        (\"case4\", g4, 64, 200_000, 50_000),\n    ]\n\n    # Independent RNGs for exact and discretized estimators\n    rng_exact = np.random.default_rng(12345)\n    rng_disc = np.random.default_rng(67890)\n\n    results = []\n    for _, g, n_grid, N_exact, N_disc in test_cases:\n        # Exact sampling\n        M_exact = sample_max_bridge_exact(rng_exact, N_exact)\n        g_exact = g(M_exact)\n        exact_mean, exact_sd, se_exact = mc_stats(g_exact)\n\n        # Discretization-based sampling\n        M_disc = sample_max_bridge_discretized(rng_disc, N_disc, n_grid)\n        g_disc = g(M_disc)\n        disc_mean, disc_sd, se_disc = mc_stats(g_disc)\n\n        # Bias and CI half-width for bias (independent estimates)\n        bias = disc_mean - exact_mean\n        ci95_bias = 1.96 * np.sqrt(se_exact**2 + se_disc**2)\n\n        results.append([\n            exact_mean,\n            disc_mean,\n            bias,\n            se_exact,\n            se_disc,\n            ci95_bias\n        ])\n\n    # Print as a single JSON-like nested list\n    # Ensure deterministic formatting with default str conversion\n    def fmt_list(lst):\n        return \"[\" + \",\".join(str(x) if not isinstance(x, list) else fmt_list(x) for x in lst) + \"]\"\n\n    print(fmt_list(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3350866"}, {"introduction": "When an entire trajectory of a Brownian bridge is needed on a discrete grid, several simulation algorithms are at our disposal, each with distinct performance trade-offs. This exercise provides a critical analysis of the computational and memory costs for three standard methods: the recursive \"Brownian tree,\" sequential conditioning, and the spectral Karhunen–Loève (KL) expansion [@problem_id:3350881]. A deep understanding of these algorithmic complexities is essential for any practitioner looking to design and implement efficient Monte Carlo simulations involving Brownian bridges.", "problem": "Consider simulating a Brownian bridge, defined as a mean-zero Gaussian process on the interval $[0,T]$ with covariance $\\operatorname{Cov}(B(s),B(t))=\\min\\{s,t\\}-st/T$, on a uniform grid $\\{t_i\\}_{i=0}^n$ where $t_i=iT/n$. You must generate one full path $\\{B(t_i)\\}_{i=0}^n$ for downstream use. Assume the following cost model: drawing an independent standard normal variate and performing a constant number of arithmetic operations each cost $\\mathcal{O}(1)$; report the arithmetic cost per path as a function of $n$ up to big-$\\mathcal{O}$ constants. Memory is measured as additional storage beyond the array holding the $n+1$ outputs.\n\nYou may use any of the following algorithmic families, each of which is standard in stochastic simulation and Monte Carlo methods:\n\n- The “Brownian tree” midpoint method (divide-and-conquer): recursively sample midpoints conditionally on their parent interval endpoints until all grid points are filled.\n- Sequential conditioning on the grid: use the Gaussian and Markov properties to sample along the grid by conditioning on previously sampled values, exploiting the tridiagonal precision structure on a uniform grid.\n- Karhunen–Loève (KL) expansion: represent $B(t)$ as a sine-series with independent Gaussian coefficients, truncate to $K$ modes to control truncation error, and evaluate the truncated series at the $n+1$ grid points either by direct summation (naive) or, on a uniform grid, by a Fast Sine Transform (FST).\n\nWhich of the following statements about computational cost and memory are correct under this model?\n\nA. For one path on $n+1$ grid points, both the Brownian tree and the sequential conditioning methods have arithmetic cost $\\mathcal{O}(n)$. The Brownian tree requires $\\mathcal{O}(\\log n)$ additional stack memory (due to recursion depth), whereas sequential conditioning can be implemented with $\\mathcal{O}(1)$ additional memory.\n\nB. On a uniform grid, the KL method with $K=n$ modes evaluated by a Fast Sine Transform (FST) costs $\\mathcal{O}(n\\log n)$ arithmetic per path and uses $\\mathcal{O}(n)$ additional memory to hold the coefficients. With naive evaluation, the KL method costs $\\mathcal{O}(nK)$ arithmetic and uses $\\mathcal{O}(K)$ additional memory.\n\nC. Sequential conditioning on the grid has arithmetic cost $\\mathcal{O}(n^2)$ per path because conditional variances and covariances must be updated at each step.\n\nD. The Brownian tree method requires $\\mathcal{O}(n^2)$ additional memory to retain the recursion frontier throughout the divide-and-conquer traversal.\n\nE. If a mean-square truncation error target $\\epsilon$ is imposed for the KL method on $[0,1]$, then choosing $K=\\mathcal{O}(1/\\epsilon)$ modes suffices; thus naive KL evaluation costs $\\mathcal{O}(n/\\epsilon)$ arithmetic per path, while the Brownian tree and sequential conditioning methods remain exact on the grid with costs independent of $\\epsilon$.\n\nSelect all that apply.", "solution": "The problem statement asks for an evaluation of the computational and memory costs of three different algorithms for simulating a Brownian bridge on a uniform grid.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n-   **Process:** A mean-zero Gaussian process $B(t)$ on the interval $[0, T]$, known as a Brownian bridge.\n-   **Covariance:** $\\operatorname{Cov}(B(s),B(t))=\\min\\{s,t\\}-st/T$.\n-   **Boundary Conditions:** $B(0) = 0$ and $B(T) = 0$.\n-   **Discretization:** A uniform grid $\\{t_i\\}_{i=0}^n$ where $t_i=iT/n$.\n-   **Task:** Generate one full path $\\{B(t_i)\\}_{i=0}^n$.\n-   **Cost Model:**\n    -   Drawing an independent standard normal variate costs $\\mathcal{O}(1)$.\n    -   A constant number of arithmetic operations cost $\\mathcal{O}(1)$.\n    -   Arithmetic cost is per path, as a function of $n$.\n-   **Memory Model:** Memory is measured as additional storage beyond the final output array of size $n+1$.\n-   **Algorithms:**\n    1.  Brownian tree (midpoint/divide-and-conquer).\n    2.  Sequential conditioning on the grid.\n    3.  Karhunen–Loève (KL) expansion with naive summation or Fast Sine Transform (FST).\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is well-defined and scientifically grounded.\n-   **Scientifically Grounded:** The definition of a Brownian bridge and its covariance are standard in the theory of stochastic processes. The simulation algorithms mentioned—Brownian tree, sequential conditioning, and Karhunen-Loève expansion—are all established and widely used methods in computational finance and stochastic simulation.\n-   **Well-Posed:** The problem provides a clear objective (calculate costs), a precise model of computation, and well-defined inputs ($n$, $T$, and the choice of algorithm). The required output is an objective complexity analysis.\n-   **Objective:** The problem is stated in precise, mathematical language, free from subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. We proceed with the solution derivation.\n\n**Analysis of Algorithms**\n\n**1. Brownian Tree Method**\nThis method uses a divide-and-conquer strategy based on the conditional properties of the Brownian bridge. We start with the known endpoints $B(t_0)=B(0)=0$ and $B(t_n)=B(T)=0$. We recursively sample the value of the bridge at the midpoint of an interval, conditional on the values at the interval's endpoints.\n\nFor any interval $[t_a, t_b]$ with known values $B(t_a)$ and $B(t_b)$, the value at the midpoint $t_m = (t_a+t_b)/2$ is Gaussian. For a general Gaussian bridge on an interval $[u, v]$ with values $x_u$ and $x_v$, a point $w \\in (u,v)$ has conditional mean $\\mathbb{E}[B(w)|B(u)=x_u, B(v)=x_v] = x_u\\frac{v-w}{v-u} + x_v\\frac{w-u}{v-u}$ and conditional variance $\\operatorname{Var}[B(w)|B(u), B(v)] = \\frac{(w-u)(v-w)}{v-u}$.\n\nThe algorithm proceeds as follows:\n-   Start with the interval $[t_0, t_n]$. Sample $B(t_{n/2})$ (assuming $n$ is a power of $2$ for simplicity; the argument generalizes).\n-   Recursively sample the midpoints of $[t_0, t_{n/2}]$ and $[t_{n/2}, t_n]$.\n-   Continue until all $n-1$ interior points $\\{B(t_i)\\}_{i=1}^{n-1}$ are generated.\n\n**Arithmetic Cost:** To generate the full path, we must sample each of the $n-1$ interior points exactly once. Each sampling step involves computing a conditional mean and variance, and drawing one standard normal variate. This constitutes a constant amount of work, i.e., $\\mathcal{O}(1)$. Therefore, the total arithmetic cost is $\\mathcal{O}(n)$.\n\n**Memory Cost:** A standard implementation uses recursion. The depth of the recursion for a grid of size $n+1$ is $\\mathcal{O}(\\log_2 n)$. At each level of recursion, a constant amount of information (interval endpoints, values) is stored on the call stack. Thus, the additional memory cost is the stack depth, which is $\\mathcal{O}(\\log n)$.\n\n**2. Sequential Conditioning Method**\nThis method generates the path points $B(t_1), B(t_2), \\dots, B(t_{n-1})$ in order. The value $B(t_i)$ is sampled conditional on the previously generated value $B(t_{i-1})$ and the fixed future endpoint $B(t_n)=0$.\n\nA Brownian bridge is a Gaussian process, and the vector $(B(t_0), \\dots, B(t_n))$ is multivariate normal. For any $i \\in \\{1, \\dots, n-1\\}$, the conditional distribution of $B(t_i)$ given $B(t_{i-1})$ and $B(t_n)$ is Gaussian. Using the general formula for conditional mean and variance on the interval $[t_{i-1}, t_n]$:\n-   Conditional Mean: $\\mathbb{E}[B(t_i) | B(t_{i-1}), B(t_n)=0] = B(t_{i-1}) \\frac{t_n-t_i}{t_n-t_{i-1}} + 0 \\cdot \\frac{t_i-t_{i-1}}{t_n-t_{i-1}}$.\n    With $t_i = iT/n$, this simplifies to $B(t_{i-1}) \\frac{T - iT/n}{T - (i-1)T/n} = B(t_{i-1}) \\frac{n-i}{n-i+1}$.\n-   Conditional Variance: $\\operatorname{Var}[B(t_i) | B(t_{i-1}), B(t_n)=0] = \\frac{(t_i-t_{i-1})(t_n-t_i)}{t_n-t_{i-1}}$.\n    This simplifies to $\\frac{(T/n)(T - iT/n)}{T - (i-1)T/n} = \\frac{(T/n)T(n-i)/n}{T(n-i+1)/n} = \\frac{T(n-i)}{n(n-i+1)}$.\n\nThe algorithm is:\n-   Set $B(t_0) = 0$.\n-   For $i = 1, \\dots, n-1$, sample $B(t_i)$ from the normal distribution with the mean and variance derived above, where the mean depends on the just-computed value $B(t_{i-1})$.\n\n**Arithmetic Cost:** For each of the $n-1$ steps, we perform a constant number of arithmetic operations to compute the conditional mean and variance, and draw one normal variate. The total arithmetic cost is therefore $\\mathcal{O}(n)$.\n\n**Memory Cost:** To compute $B(t_i)$, we only need the value of $B(t_{i-1})$. The calculation can be done and the result stored directly in the output array. We only need to keep track of the last computed value, which requires $\\mathcal{O}(1)$ additional storage.\n\n**3. Karhunen–Loève (KL) Expansion Method**\nThe KL expansion for a Brownian bridge on $[0, T]$ is $B(t) = \\sum_{k=1}^{\\infty} c_k \\sin(\\frac{k\\pi t}{T})$, where $c_k = Z_k \\frac{\\sqrt{2T}}{k\\pi}$ and $Z_k \\sim N(0,1)$ are i.i.d. standard normal variables. A simulation truncates this series to $K$ terms: $B_K(t) = \\sum_{k=1}^{K} c_k \\sin(\\frac{k\\pi t}{T})$. We evaluate this sum at the grid points $t_i = iT/n$ for $i=1, \\dots, n-1$.\n\n**Naive Evaluation:**\n-   **Procedure:**\n    1.  Generate $K$ standard normal variates $Z_1, \\dots, Z_K$ and compute the coefficients $c_1, \\dots, c_K$. Cost: $\\mathcal{O}(K)$.\n    2.  For each of the $n-1$ interior grid points, compute the sum of $K$ terms. Cost: $\\mathcal{O}(nK)$.\n-   **Arithmetic Cost:** The total cost is dominated by the evaluation step, yielding $\\mathcal{O}(nK)$.\n-   **Memory Cost:** We must store the $K$ coefficients, so the additional memory cost is $\\mathcal{O}(K)$.\n\n**Fast Sine Transform (FST) Evaluation:**\nThe sum $B_K(t_i) = \\sum_{k=1}^{K} c_k \\sin(\\frac{ik\\pi}{n})$ is a Discrete Sine Transform (Type I). If we choose $K=n-1$ (or $K=n$ as per the problem, which is equivalent in big-O terms), we can use a Fast Sine Transform algorithm, which is based on the Fast Fourier Transform (FFT).\n-   **Procedure:**\n    1.  Generate $K=n$ (or $n-1$) variates and compute coefficients. Cost: $\\mathcal{O}(n)$.\n    2.  Use an FST algorithm to compute all $n-1$ values $\\{B_K(t_i)\\}_{i=1}^{n-1}$ from the coefficients $\\{c_k\\}_{k=1}^{n-1}$. Cost: $\\mathcal{O}(n \\log n)$.\n-   **Arithmetic Cost:** The total cost is dominated by the FST, yielding $\\mathcal{O}(n \\log n)$.\n-   **Memory Cost:** Storing the $K=n$ coefficients requires $\\mathcal{O}(n)$ additional memory. The FST algorithm may also require $\\mathcal{O}(n)$ scratch space. The total is $\\mathcal{O}(n)$.\n\n**KL Truncation Error:**\nThe Integrated Mean Square Error (IMSE) from truncating the KL series at $K$ terms is $IMSE = \\sum_{k=K+1}^{\\infty} \\lambda_k$, where $\\lambda_k = T^2/(k\\pi)^2$ are the eigenvalues. For large $K$, $\\sum_{k=K+1}^{\\infty} 1/k^2 \\approx 1/K$. Thus, $IMSE \\approx T^2/(\\pi^2 K)$. To achieve an error tolerance $\\epsilon$, we need $IMSE \\lesssim \\epsilon$, which implies $K = \\mathcal{O}(1/\\epsilon)$. For the case $T=1$ given in option E, this holds.\nIf $K=\\mathcal{O}(1/\\epsilon)$, the naive KL evaluation cost becomes $\\mathcal{O}(nK) = \\mathcal{O}(n/\\epsilon)$. The other methods, being exact on the grid, do not have a truncation parameter $\\epsilon$, and their cost $\\mathcal{O}(n)$ is independent of any such accuracy measure.\n\n**Evaluation of Options**\n\n**A. For one path on $n+1$ grid points, both the Brownian tree and the sequential conditioning methods have arithmetic cost $\\mathcal{O}(n)$. The Brownian tree requires $\\mathcal{O}(\\log n)$ additional stack memory (due to recursion depth), whereas sequential conditioning can be implemented with $\\mathcal{O}(1)$ additional memory.**\n-   Our analysis confirms the $\\mathcal{O}(n)$ arithmetic cost for both methods.\n-   Our analysis confirms the $\\mathcal{O}(\\log n)$ additional memory for the recursive Brownian tree method and $\\mathcal{O}(1)$ additional memory for the sequential conditioning method.\n-   **Verdict: Correct.**\n\n**B. On a uniform grid, the KL method with $K=n$ modes evaluated by a Fast Sine Transform (FST) costs $\\mathcal{O}(n\\log n)$ arithmetic per path and uses $\\mathcal{O}(n)$ additional memory to hold the coefficients. With naive evaluation, the KL method costs $\\mathcal{O}(nK)$ arithmetic and uses $\\mathcal{O}(K)$ additional memory.**\n-   Our analysis confirms that with $K=n$, the FST-based KL method has a cost of $\\mathcal{O}(n \\log n)$ and requires $\\mathcal{O}(n)$ additional memory.\n-   Our analysis confirms that the naive KL evaluation has a cost of $\\mathcal{O}(nK)$ and requires $\\mathcal{O}(K)$ additional memory.\n-   **Verdict: Correct.**\n\n**C. Sequential conditioning on the grid has arithmetic cost $\\mathcal{O}(n^2)$ per path because conditional variances and covariances must be updated at each step.**\n-   Our analysis showed that for a uniform grid, the coefficients for the conditional mean and the conditional variance have simple, closed-form expressions that can be computed in $\\mathcal{O}(1)$ time at each step. This leads to a total cost of $\\mathcal{O}(n)$, not $\\mathcal{O}(n^2)$. An $\\mathcal{O}(n^2)$ cost would imply a much more complex update, such as re-inverting a growing matrix at each step, which is unnecessary here.\n-   **Verdict: Incorrect.**\n\n**D. The Brownian tree method requires $\\mathcal{O}(n^2)$ additional memory to retain the recursion frontier throughout the divide-and-conquer traversal.**\n-   Our analysis showed that a standard recursive (depth-first) implementation uses $\\mathcal{O}(\\log n)$ additional memory due to stack depth. A breadth-first implementation would require $\\mathcal{O}(n)$ memory for its queue. In neither case is the memory requirement $\\mathcal{O}(n^2)$.\n-   **Verdict: Incorrect.**\n\n**E. If a mean-square truncation error target $\\epsilon$ is imposed for the KL method on $[0,1]$, then choosing $K=\\mathcal{O}(1/\\epsilon)$ modes suffices; thus naive KL evaluation costs $\\mathcal{O}(n/\\epsilon)$ arithmetic per path, while the Brownian tree and sequential conditioning methods remain exact on the grid with costs independent of $\\epsilon$.**\n-   Our analysis confirms that to achieve an integrated mean-square error of $\\epsilon$, the number of KL modes $K$ must be $\\mathcal{O}(1/\\epsilon)$.\n-   The cost of naive KL evaluation is $\\mathcal{O}(nK)$, which becomes $\\mathcal{O}(n/\\epsilon)$.\n-   The Brownian tree and sequential conditioning methods are \"exact\" in the sense that they sample from the true multivariate Gaussian distribution of the bridge on the discrete grid; they have no truncation error. Their costs are $\\mathcal{O}(n)$ and do not depend on $\\epsilon$.\n-   **Verdict: Correct.**", "answer": "$$\\boxed{ABE}$$", "id": "3350881"}]}