{"hands_on_practices": [{"introduction": "Before simulating any stochastic process, we must quantify the rates at which different events can occur. In the context of chemical kinetics, these fundamental rates are the reaction propensities. This exercise will ground your understanding of propensities by having you calculate them from the first principles of combinatorial chemistry, a crucial step that precedes any simulation. Mastering the correct calculation of propensities, especially by distinguishing between reaction orders and reactant types, is the foundation for building an accurate stochastic simulation via either the Direct or First Reaction Method [@problem_id:3302951].", "problem": "Consider a well-mixed, isothermal system modeled by the Chemical Master Equation, to be simulated via the Stochastic Simulation Algorithm (SSA). There are two species, $A$ and $B$, with the state vector at time $t_{0}$ given by $x(t_{0}) = (X_{A}, X_{B}) = (7, 4)$. The reaction network is:\n- $R_{1}: A + A \\rightarrow B$, with stoichiometric change vector $\\nu_{1} = (-2, +1)$ and stochastic rate constant $c_{1} = 0.5\\,\\text{s}^{-1}$.\n- $R_{2}: A + B \\rightarrow 2A$, with stoichiometric change vector $\\nu_{2} = (+1, -1)$ and stochastic rate constant $c_{2} = 0.2\\,\\text{s}^{-1}$.\n- $R_{3}: B \\rightarrow A$, with stoichiometric change vector $\\nu_{3} = (+1, -1)$ and stochastic rate constant $c_{3} = 3\\,\\text{s}^{-1}$.\n- $R_{4}: A \\rightarrow \\varnothing$, with stoichiometric change vector $\\nu_{4} = (-1, 0)$ and stochastic rate constant $c_{4} = 1\\,\\text{s}^{-1}$.\n\nWorking from first principles consistent with discrete mass-action kinetics in the Chemical Master Equation framework, determine the propensities $a_{\\mu}(x(t_{0}))$ for all reactions by enumerating the number of distinct reactant combinations present in the state $x(t_{0})$, and then compute the total propensity $a_{0}(x(t_{0})) = \\sum_{\\mu} a_{\\mu}(x(t_{0}))$. Provide the final answer as the single number $a_{0}(x(t_{0}))$ expressed in $\\text{s}^{-1}$. Give the exact value; do not round.", "solution": "The problem asks for the calculation of the total propensity, $a_{0}(x(t_{0}))$, for a given chemical reaction network at a specific state $x(t_{0})$. The calculation is to be performed from first principles consistent with the discrete and stochastic nature of the system, as described by the Chemical Master Equation (CME).\n\nThe propensity function, $a_{\\mu}(x)$, for a reaction $R_{\\mu}$ is defined such that $a_{\\mu}(x) dt$ represents the probability that one instance of reaction $R_{\\mu}$ will occur within the infinitesimal time interval $[t, t+dt)$, given the system is in state $x$ at time $t$. For mass-action kinetics in a well-mixed, isothermal system, the propensity function is the product of the stochastic rate constant, $c_{\\mu}$, and the number of distinct combinations of reactant molecules, $h_{\\mu}(x)$, that are available in the current state $x$. The general form is:\n$$a_{\\mu}(x) = c_{\\mu} h_{\\mu}(x)$$\nThe state of the system at the initial time $t_{0}$ is given by the vector of molecule counts $x(t_{0}) = (X_{A}, X_{B}) = (7, 4)$, where $X_{A}$ is the number of molecules of species $A$ and $X_{B}$ is the number of molecules of species $B$. We will compute the propensity for each of the four specified reactions at this state.\n\n**Reaction $R_{1}: A + A \\rightarrow B$**\nThis reaction is a bimolecular reaction involving two identical molecules of species $A$. The number of distinct combinations of reactants, $h_{1}(x)$, is found by counting the number of ways to choose $2$ molecules of $A$ from the total population of $X_{A}$ molecules. This is given by the binomial coefficient:\n$$h_{1}(x) = \\binom{X_{A}}{2} = \\frac{X_{A}(X_{A}-1)}{2!}$$\nThe propensity for reaction $R_{1}$ is therefore:\n$$a_{1}(x) = c_{1} h_{1}(x) = c_{1} \\frac{X_{A}(X_{A}-1)}{2}$$\nWith the given state $X_{A} = 7$ and rate constant $c_{1} = 0.5\\,\\text{s}^{-1}$, the propensity is:\n$$a_{1}(x(t_{0})) = (0.5) \\cdot \\frac{7(7-1)}{2} = (0.5) \\cdot \\frac{7 \\cdot 6}{2} = (0.5) \\cdot 21 = 10.5\\,\\text{s}^{-1}$$\n\n**Reaction $R_{2}: A + B \\rightarrow 2A$**\nThis reaction is a bimolecular reaction involving two distinct reactant species, $A$ and $B$. The number of distinct combinations of reactants, $h_{2}(x)$, is the number of ways to choose one molecule of $A$ from the $X_{A}$ available and one molecule of $B$ from the $X_{B}$ available. This is simply the product of their counts:\n$$h_{2}(x) = X_{A} \\cdot X_{B}$$\nThe propensity for reaction $R_{2}$ is:\n$$a_{2}(x) = c_{2} h_{2}(x) = c_{2} X_{A} X_{B}$$\nWith $X_{A} = 7$, $X_{B} = 4$, and $c_{2} = 0.2\\,\\text{s}^{-1}$, the propensity is:\n$$a_{2}(x(t_{0})) = (0.2) \\cdot (7 \\cdot 4) = (0.2) \\cdot 28 = 5.6\\,\\text{s}^{-1}$$\n\n**Reaction $R_{3}: B \\rightarrow A$**\nThis reaction is a unimolecular reaction, where a single molecule of species $B$ is the reactant. The number of reactant \"combinations\", $h_{3}(x)$, is simply the total number of molecules of species $B$.\n$$h_{3}(x) = X_{B}$$\nThe propensity for reaction $R_{3}$ is:\n$$a_{3}(x) = c_{3} h_{3}(x) = c_{3} X_{B}$$\nWith $X_{B} = 4$ and $c_{3} = 3\\,\\text{s}^{-1}$, the propensity is:\n$$a_{3}(x(t_{0})) = (3) \\cdot 4 = 12\\,\\text{s}^{-1}$$\n\n**Reaction $R_{4}: A \\rightarrow \\varnothing$**\nThis reaction is a unimolecular decay of species $A$. The number of reactant molecules is the number of combinations, $h_{4}(x)$.\n$$h_{4}(x) = X_{A}$$\nThe propensity for reaction $R_{4}$ is:\n$$a_{4}(x) = c_{4} h_{4}(x) = c_{4} X_{A}$$\nWith $X_{A} = 7$ and $c_{4} = 1\\,\\text{s}^{-1}$, the propensity is:\n$$a_{4}(x(t_{0})) = (1) \\cdot 7 = 7\\,\\text{s}^{-1}$$\n\n**Total Propensity**\nThe total propensity, $a_{0}(x(t_{0}))$, is the sum of all individual reaction propensities. It represents the rate parameter of the exponential distribution from which the time to the next reaction event is drawn in the Gillespie algorithm.\n$$a_{0}(x(t_{0})) = \\sum_{\\mu=1}^{4} a_{\\mu}(x(t_{0})) = a_{1}(x(t_{0})) + a_{2}(x(t_{0})) + a_{3}(x(t_{0})) + a_{4}(x(t_{0}))$$\nSubstituting the calculated values:\n$$a_{0}(x(t_{0})) = 10.5 + 5.6 + 12 + 7$$\n$$a_{0}(x(t_{0})) = 16.1 + 12 + 7$$\n$$a_{0}(x(t_{0})) = 28.1 + 7$$\n$$a_{0}(x(t_{0})) = 35.1\\,\\text{s}^{-1}$$\nThe exact value for the total propensity at the given state is $35.1\\,\\text{s}^{-1}$.", "answer": "$$\\boxed{35.1}$$", "id": "3302951"}, {"introduction": "A core task for both the Direct Method (DM) and the First Reaction Method (FRM) is to answer the question: \"When will the next reaction occur?\" While their internal mechanics differ—the DM samples one time from the total propensity, while the FRM orchestrates a \"race\" between individual reaction clocks—they must be mathematically equivalent. This practice challenges you to implement both algorithms and apply the powerful Kolmogorov-Smirnov statistical test to verify that they both generate waiting times, $\\tau$, from the correct theoretical exponential distribution. This hands-on validation provides concrete proof of the methods' equivalence and builds essential skills in the statistical verification of simulation software [@problem_id:3302917].", "problem": "Consider a fixed state of a continuous-time Markov jump process representing stochastic chemical kinetics with reaction channels indexed by $k \\in \\{1,\\dots,K\\}$. Each reaction channel $k$ has a propensity function $a_k \\ge 0$ that is constant at the fixed state, and the total propensity is $a_0 = \\sum_{k=1}^{K} a_k$. At a fixed state, the waiting time to the next reaction event is a random variable $\\tau \\ge 0$. You will implement and compare the Direct Method (DM) and the First Reaction Method (FRM) for sampling $\\tau$, and then design a goodness-of-fit test using the Kolmogorov-Smirnov (KS) statistic to verify that empirical waiting times from DM or FRM follow the cumulative distribution function $F(t) = 1 - e^{-a_0 t}$ for $t \\ge 0$.\n\nStarting from first principles suitable for continuous-time Markov processes and stochastic simulation, use well-tested facts about propensities and event-time distributions, but do not assume any shortcut formulas beyond those needed to define the null hypothesis for the KS test. The Kolmogorov-Smirnov statistic should be applied to empirical samples of $\\tau$ obtained by simulating waiting times using DM or FRM at the fixed state with specified propensities.\n\nYour program must:\n- Implement sampling of $\\tau$ using the Direct Method (DM) at a fixed state with total propensity $a_0$, and the First Reaction Method (FRM) with propensities $(a_1,\\dots,a_K)$.\n- For each test case, generate an independent sample of waiting times $\\{\\tau_i\\}_{i=1}^{n}$ using DM or FRM, compute the Kolmogorov-Smirnov statistic against the theoretical cumulative distribution function $F(t) = 1 - e^{-a_0^{\\mathrm{test}} t}$, where $a_0^{\\mathrm{test}}$ is either the true $a_0$ or a specified alternative value. Compute the corresponding $p$-value for the KS test and make a decision at the given significance level $\\alpha$.\n- Return a boolean for each test case: return $\\mathrm{True}$ if the KS test fails to reject the null hypothesis at the specified $\\alpha$ (that is, the $p$-value is greater than or equal to $\\alpha$), and return $\\mathrm{False}$ otherwise.\n- Use time units in seconds for all waiting times, but the final output should be booleans and therefore unitless.\n- Ensure reproducibility by using fixed random seeds for each test case.\n\nTest suite:\n- Case $1$ (happy path, DM): propensities $(0.5, 0.7, 2.3)$, so $a_0 = 3.5$; sample size $n = 300$; significance level $\\alpha = 0.001$; use $a_0^{\\mathrm{test}} = a_0$; random seed $12345$.\n- Case $2$ (happy path, FRM): propensities $(0.5, 0.7, 2.3)$, so $a_0 = 3.5$; sample size $n = 300$; significance level $\\alpha = 0.001$; use $a_0^{\\mathrm{test}} = a_0$; random seed $54321$.\n- Case $3$ (misspecification edge case, FRM): propensities $(0.5, 0.7, 2.3)$, so $a_0 = 3.5$; sample size $n = 300$; significance level $\\alpha = 0.05$; use $a_0^{\\mathrm{test}} = 0.5 a_0$; random seed $999$.\n- Case $4$ (boundary condition with small propensities, DM): propensities $(0.001, 0.002)$, so $a_0 = 0.003$; sample size $n = 2000$; significance level $\\alpha = 0.001$; use $a_0^{\\mathrm{test}} = a_0$; random seed $2023$.\n- Case $5$ (edge case with a zero propensity, FRM): propensities $(0.0, 1.5)$, so $a_0 = 1.5$; sample size $n = 300$; significance level $\\alpha = 0.001$; use $a_0^{\\mathrm{test}} = a_0$; random seed $777$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\mathrm{result}_1,\\mathrm{result}_2,\\mathrm{result}_3,\\mathrm{result}_4,\\mathrm{result}_5]$, where each $\\mathrm{result}_i$ is either $\\mathrm{True}$ or $\\mathrm{False}$ corresponding to the decision for test case $i$.", "solution": "The problem requires the implementation and statistical validation of two methods for sampling the waiting time, $\\tau$, in a continuous-time Markov jump process model of stochastic chemical kinetics: the Direct Method (DM) and the First Reaction Method (FRM). The validation is to be performed using the Kolmogorov-Smirnov (KS) goodness-of-fit test.\n\nThe theoretical foundation of this problem lies in the stochastic formulation of chemical kinetics, where the state of a system is described by the number of molecules of each chemical species. Reactions occur as discrete, random events. For a system in a fixed state, each of the $K$ possible reaction channels, indexed by $k \\in \\{1, \\dots, K\\}$, is treated as an independent Poisson process. The rate of the $k$-th Poisson process is given by its propensity function, $a_k$. Since the system state is considered fixed for the purpose of sampling the next event time, each $a_k$ is a non-negative constant, $a_k \\ge 0$.\n\nThe total propensity, $a_0$, is the sum of the individual propensities:\n$$\na_0 = \\sum_{k=1}^{K} a_k\n$$\nThis quantity represents the total rate at which *any* reaction event occurs. A fundamental result of this model is that the waiting time $\\tau$ until the next reaction event (regardless of which one) is a continuous random variable that follows an exponential distribution with a rate parameter equal to the total propensity $a_0$. The probability density function (PDF) of $\\tau$ is given by:\n$$\np(t; a_0) = a_0 e^{-a_0 t} \\quad \\text{for } t \\ge 0\n$$\nThe corresponding cumulative distribution function (CDF), which gives the probability that the waiting time is less than or equal to some time $t$, is:\n$$\nF(t; a_0) = P(\\tau \\le t) = \\int_0^t a_0 e^{-a_0 s} ds = 1 - e^{-a_0 t} \\quad \\text{for } t \\ge 0\n$$\nThis theoretical CDF forms the basis for the null hypothesis in our statistical test.\n\nWe will now describe the two simulation methods for sampling $\\tau$.\n\n**Direct Method (DM)**\n\nThe Direct Method, a core component of Gillespie's Stochastic Simulation Algorithm (SSA), samples the waiting time $\\tau$ directly from its exponential distribution. This is efficiently achieved using the inverse transform sampling method. We start with the CDF $F(\\tau) = 1 - e^{-a_0 \\tau}$. Let $r$ be a random number drawn from a uniform distribution on the interval $(0, 1)$, and set $r = F(\\tau)$. We then solve for $\\tau$:\n$$\nr = 1 - e^{-a_0 \\tau}\n$$\n$$\n1 - r = e^{-a_0 \\tau}\n$$\n$$\n\\ln(1 - r) = -a_0 \\tau\n$$\n$$\n\\tau = -\\frac{1}{a_0} \\ln(1 - r)\n$$\nSince if $r$ is uniformly distributed on $(0, 1)$, then $1-r$ is also uniformly distributed on $(0, 1)$. We can therefore simplify the expression by replacing $1-r$ with another uniform random number, which we also denote by $r$ for simplicity. This leads to the well-known formula for sampling $\\tau$:\n$$\n\\tau = \\frac{1}{a_0} \\ln\\left(\\frac{1}{r}\\right)\n$$\nThis method requires generating one random number to determine the waiting time. This assumes $a_0 > 0$. If $a_0 = 0$, no reaction can occur, and $\\tau$ is effectively infinite.\n\n**First Reaction Method (FRM)**\n\nThe First Reaction Method provides an alternative but mathematically equivalent way to sample $\\tau$. Instead of considering the total propensity, we consider each reaction channel independently. For each channel $k$ with propensity $a_k > 0$, we propose a putative waiting time $\\tau_k$ until that specific reaction would occur, assuming it were the only possible reaction. Each $\\tau_k$ is sampled from an exponential distribution with its own rate $a_k$:\n$$\n\\tau_k = \\frac{1}{a_k} \\ln\\left(\\frac{1}{r_k}\\right)\n$$\nwhere each $r_k$ is an independent random number from a uniform distribution on $(0, 1)$. For any channel with $a_k=0$, the waiting time $\\tau_k$ is infinite. The waiting time for the *next* reaction to occur in the full system is then the minimum of all these proposed times, as the first reaction to \"fire\" determines the event time for the system as a whole:\n$$\n\\tau = \\min_{k \\in \\{1,\\dots,K\\}} \\{\\tau_k\\}\n$$\nA key property of the exponential distribution is that the minimum of a set of independent exponentially distributed random variables is also exponentially distributed. If $\\tau_k \\sim \\text{Exponential}(a_k)$, then $\\min\\{\\tau_k\\}$ follows an exponential distribution with a rate parameter equal to the sum of the individual rates, $\\sum_k a_k = a_0$. Thus, FRM is guaranteed to generate samples from the same distribution as DM. This method requires generating $K$ random numbers (or one for each channel with $a_k > 0$) to determine the waiting time.\n\n**Kolmogorov-Smirnov (KS) Goodness-of-Fit Test**\n\nTo validate that the samples $\\{\\tau_i\\}_{i=1}^{n}$ generated by DM or FRM conform to the theoretical distribution, we employ the KS test. This test compares the empirical distribution function (EDF) of the sample to the CDF of the reference distribution. The EDF, denoted $F_n(t)$, is defined as the proportion of sample points less than or equal to $t$:\n$$\nF_n(t) = \\frac{1}{n} \\sum_{i=1}^n I(\\tau_i \\le t)\n$$\nwhere $I(\\cdot)$ is the indicator function. The null hypothesis, $H_0$, is that the samples are drawn from the theoretical distribution with CDF $F(t) = 1 - e^{-a_0^{\\mathrm{test}} t}$. The KS statistic, $D_n$, is the maximum absolute difference between the EDF and the theoretical CDF over all possible values of $t$:\n$$\nD_n = \\sup_{t} |F_n(t) - F(t)|\n$$\nThe test calculates a $p$-value, which is the probability of observing a $D_n$ statistic as large as or larger than the one computed from the sample, assuming $H_0$ is true. The decision rule is based on a pre-defined significance level $\\alpha$:\n- If $p$-value $\\ge \\alpha$, we fail to reject the null hypothesis $H_0$. This suggests the data are consistent with the theoretical distribution. The program should return $\\mathrm{True}$.\n- If $p$-value $< \\alpha$, we reject the null hypothesis $H_0$. This indicates a statistically significant discrepancy between the data and the theoretical distribution. The program should return $\\mathrm{False}$.\n\nFor each test case, the procedure is as follows:\n1.  Set the specified random seed for reproducibility.\n2.  Based on the specified method (DM or FRM), generate a sample of $n$ waiting times using the given propensities $(a_1, \\dots, a_K)$.\n3.  Calculate the total propensity $a_0 = \\sum_k a_k$ and the test propensity $a_0^{\\mathrm{test}}$ as specified.\n4.  Perform a two-sided KS test on the generated sample $\\{\\tau_i\\}$ against the theoretical exponential distribution with rate $a_0^{\\mathrm{test}}$ (which corresponds to a scale parameter of $1/a_0^{\\mathrm{test}}$ in `scipy`).\n5.  Compare the resulting $p$-value with the significance level $\\alpha$ to decide whether to reject $H_0$ and determine the boolean output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest\n\ndef solve():\n    \"\"\"\n    Implements and validates waiting time sampling for stochastic simulation\n    using the Direct Method (DM) and First Reaction Method (FRM).\n    Uses the Kolmogorov-Smirnov test to check if simulated waiting times\n    follow the theoretical exponential distribution.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"method\": \"DM\",\n            \"propensities\": (0.5, 0.7, 2.3),\n            \"n\": 300,\n            \"alpha\": 0.001,\n            \"a0_test_factor\": 1.0,\n            \"seed\": 12345\n        },\n        {\n            \"method\": \"FRM\",\n            \"propensities\": (0.5, 0.7, 2.3),\n            \"n\": 300,\n            \"alpha\": 0.001,\n            \"a0_test_factor\": 1.0,\n            \"seed\": 54321\n        },\n        {\n            \"method\": \"FRM\",\n            \"propensities\": (0.5, 0.7, 2.3),\n            \"n\": 300,\n            \"alpha\": 0.05,\n            \"a0_test_factor\": 0.5,\n            \"seed\": 999\n        },\n        {\n            \"method\": \"DM\",\n            \"propensities\": (0.001, 0.002),\n            \"n\": 2000,\n            \"alpha\": 0.001,\n            \"a0_test_factor\": 1.0,\n            \"seed\": 2023\n        },\n        {\n            \"method\": \"FRM\",\n            \"propensities\": (0.0, 1.5),\n            \"n\": 300,\n            \"alpha\": 0.001,\n            \"a0_test_factor\": 1.0,\n            \"seed\": 777\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Set seed for reproducibility\n        np.random.seed(case[\"seed\"])\n\n        propensities = np.array(case[\"propensities\"])\n        n = case[\"n\"]\n        alpha = case[\"alpha\"]\n        method = case[\"method\"]\n        a0_test_factor = case[\"a0_test_factor\"]\n\n        # Calculate true total propensity\n        a0_true = np.sum(propensities)\n\n        samples = []\n        if method == \"DM\":\n            if a0_true <= 0:\n                # Waiting time is infinite, simulation should stop.\n                # KS test is not meaningful on a sample of infinities.\n                # This case isn't in the test suite, but is a necessary check.\n                # For this problem, we can assume a0_true > 0 based on test cases.\n                pass\n            \n            # Generate n random numbers for inverse transform sampling\n            rand_nums = np.random.uniform(low=0.0, high=1.0, size=n)\n            # Vectorized calculation of waiting times\n            samples = (1.0 / a0_true) * np.log(1.0 / rand_nums)\n\n        elif method == \"FRM\":\n            # Generate n samples one by one\n            for _ in range(n):\n                putative_times = []\n                for a_k in propensities:\n                    if a_k > 0:\n                        rand_num = np.random.uniform(low=0.0, high=1.0)\n                        tau_k = (1.0 / a_k) * np.log(1.0 / rand_num)\n                        putative_times.append(tau_k)\n                \n                if not putative_times:\n                    # All propensities are zero, waiting time is infinite.\n                    # As with DM, this case is not in the test suite.\n                    tau = np.inf\n                else:\n                    tau = min(putative_times)\n                samples.append(tau)\n        \n        # Define the null hypothesis for the KS test\n        a0_test = a0_true * a0_test_factor\n        \n        # The theoretical CDF is F(t) = 1 - exp(-a0_test * t).\n        # scipy.stats.expon uses a scale parameter, where scale = 1/rate.\n        # The rate parameter lambda is a0_test.\n        scale_param = 1.0 / a0_test\n        \n        # Perform the two-sided Kolmogorov-Smirnov test\n        # H0: samples are drawn from an exponential distribution with the specified scale.\n        ks_statistic, p_value = kstest(samples, 'expon', args=(0, scale_param))\n        \n        # Decision: Fail to reject H0 if p-value is greater than or equal to alpha\n        decision = p_value >= alpha\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    # The boolean values True/False must be capitalized as such in the output string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3302917"}, {"introduction": "After determining *when* a reaction occurs, the Gillespie algorithm must decide *which* reaction it is. This selection is not a uniform guess but a weighted choice, where the probability of selecting a particular reaction is proportional to its propensity, $p_{\\mu} = a_{\\mu}/a_{0}$. This exercise guides you in using the Pearson $\\chi^2$ goodness-of-fit test to confirm that a simulator's reaction choices precisely match this theoretical categorical distribution. By validating this selection mechanism, you complete the verification of your simulation engine, ensuring both the timing and the identity of events are correct, and you will also explore the concept of statistical power—a vital skill for designing meaningful computational experiments [@problem_id:3302887].", "problem": "Consider a well-mixed stochastic chemical reaction network with $M$ reaction channels indexed by $\\mu \\in \\{1,\\dots,M\\}$, each endowed with a propensity (intensity) $a_\\mu(\\mathbf{x})$ evaluated at a fixed state $\\mathbf{x}$. Let $a_0(\\mathbf{x}) = \\sum_{\\mu=1}^M a_\\mu(\\mathbf{x})$. Under both the Direct Method and the First Reaction Method of the Stochastic Simulation Algorithm (often referred to as the Gillespie Algorithm (GA)), the next reaction index is selected according to a categorical distribution with probabilities proportional to the propensities. Define the normalized target probabilities $p_\\mu$ by $p_\\mu = a_\\mu(\\mathbf{x}) / a_0(\\mathbf{x})$, for $\\mu = 1,\\dots,M$, and assume $\\sum_{\\mu=1}^M p_\\mu = 1$ and $p_\\mu > 0$.\n\nSuppose an implementation of either method produces a sequence of $N$ reaction selections at the fixed state $\\mathbf{x}$, with observed counts $C_\\mu$ for each channel $\\mu$, such that $\\sum_{\\mu=1}^M C_\\mu = N$. The scientific task is to assess whether the observed selection frequencies agree with the theoretical probabilities $\\{p_\\mu\\}$ and, furthermore, to determine the sample size required to detect a specified deviation with a given statistical power.\n\nStarting from well-tested facts and core definitions appropriate to stochastic simulation and Monte Carlo methods, namely:\n- Exponential waiting times for reaction events conditioned on the current state,\n- Independence across reaction-clock samples at the instant of decision,\n- The multinomial distribution governing categorical counts,\nformulate a principled hypothesis test and power analysis for the selection mechanism.\n\nYour program must implement the following, for each test case:\n1. Model the count vector $(C_1,\\dots,C_M)$ under the null hypothesis $H_0$ as multinomial with probabilities $(p_1,\\dots,p_M)$ and total count $N$. Construct the Pearson $\\chi^2$ statistic, specify the degrees of freedom, and compute the $p$-value for the null hypothesis $H_0: \\text{selection probabilities} = \\{p_\\mu\\}$. Use significance level $\\alpha$ supplied in the test case. The rejection rule must be defined by the $\\chi^2$ distribution under $H_0$.\n2. For a specified alternative probability vector $q = (q_1,\\dots,q_M)$ with $q_\\mu > 0$ and $\\sum_{\\mu=1}^M q_\\mu = 1$, compute the minimum integer sample size $N^\\star$ required to achieve a target power $1-\\beta$ for the Pearson $\\chi^2$ test at significance level $\\alpha$. Use the large-sample approximation in which the Pearson $\\chi^2$ statistic under the alternative is distributed as a noncentral $\\chi^2$ with the same degrees of freedom and a noncentrality parameter that depends on $N$, $p$, and $q$. Additionally, enforce the standard applicability condition for the Pearson $\\chi^2$ test that all expected counts $N p_\\mu$ be at least $5$; if this condition is tighter than the power requirement, report the larger $N$.\n3. For interpretability consistent with the scientific context, treat all probabilities as dimensionless quantities. The only physical unit potentially involved is time in the underlying process, but it is not required in the outputs here. Counts are integers. No angles are involved. Report the final outputs numerically without units.\n\nTest suite:\n- Case A (general three-channel scenario, \"happy path\"):\n  - $M = 3$\n  - Propensities $a = (1.0, 2.0, 3.0)$ so $p = \\left(\\frac{1.0}{6.0}, \\frac{2.0}{6.0}, \\frac{3.0}{6.0}\\right)$\n  - Observed counts $C = (16, 33, 51)$ with $N = 100$\n  - Significance level $\\alpha = 0.05$\n  - Alternative $q = (0.15, 0.30, 0.55)$\n  - Target power $1-\\beta = 0.80$\n- Case B (two-channel boundary to binomial):\n  - $M = 2$\n  - Propensities $a = (0.4, 0.6)$ so $p = (0.4, 0.6)$\n  - Observed counts $C = (36, 64)$ with $N = 100$\n  - Significance level $\\alpha = 0.05$\n  - Alternative $q = (0.45, 0.55)$\n  - Target power $1-\\beta = 0.80$\n- Case C (four-channel with low expected count boundary):\n  - $M = 4$\n  - Propensities $a = (0.5, 0.5, 2.0, 7.0)$ so $p = (0.05, 0.05, 0.20, 0.70)$\n  - Observed counts $C = (6, 3, 24, 67)$ with $N = 100$\n  - Significance level $\\alpha = 0.05$\n  - Alternative $q = (0.10, 0.05, 0.20, 0.65)$\n  - Target power $1-\\beta = 0.80$\n\nFinal output specification:\n- For each case, your program must compute:\n  - The $p$-value of the Pearson $\\chi^2$ test under $H_0$ at level $\\alpha$,\n  - A boolean indicating whether $H_0$ is rejected at level $\\alpha$,\n  - The minimal integer $N^\\star$ satisfying both the target power requirement against $q$ and the expected-count condition $N p_\\mu \\ge 5$ for all $\\mu$.\n- Your program should produce a single line of output containing the results as a comma-separated Python-style list of per-case lists, in the order Case A, Case B, Case C. For example, it should print a top-level list of the form $[\\,[p\\_A,\\text{reject}\\_A,N^\\star\\_A],\\,[p\\_B,\\text{reject}\\_B,N^\\star\\_B],\\,[p\\_C,\\text{reject}\\_C,N^\\star\\_C]\\,]$, where $p\\_A$, $p\\_B$, $p\\_C$ are floats, $\\text{reject}\\_A$, $\\text{reject}\\_B$, $\\text{reject}\\_C$ are booleans, and $N^\\star\\_A$, $N^\\star\\_B$, $N^\\star\\_C$ are integers.", "solution": "The problem is valid as it presents a well-posed, scientifically grounded, and computationally tractable question in applied statistics, specifically concerning the validation of stochastic simulation algorithms. The premises are factually correct, the terminology is precise, and all necessary data are provided. We proceed with a principled solution.\n\nThe core of the problem lies in two standard statistical tasks: a goodness-of-fit test and a prospective power analysis. Both are based on the Pearson $\\chi^2$ statistic applied to multinomial count data. The context is the selection of a reaction channel in the Gillespie Stochastic Simulation Algorithm (SSA), which, at any given state $\\mathbf{x}$, is a discrete random event.\n\nLet the state of the system be described by the population vector $\\mathbf{x}$. The system has $M$ reaction channels, indexed by $\\mu \\in \\{1,\\dots,M\\}$. The propensity for reaction $\\mu$ to occur is $a_\\mu(\\mathbf{x})$. The total propensity is $a_0(\\mathbf{x}) = \\sum_{\\mu=1}^M a_\\mu(\\mathbf{x})$. According to the principles of the SSA, the probability of the next reaction being of type $\\mu$ is given by $p_\\mu = a_\\mu(\\mathbf{x}) / a_0(\\mathbf{x})$. A sequence of $N$ such selections, made at a fixed state $\\mathbf{x}$, results in an observed count vector $\\mathbf{C} = (C_1, C_2, \\dots, C_M)$, where $\\sum_{\\mu=1}^M C_\\mu = N$.\n\n**Part 1: The Pearson $\\chi^2$ Goodness-of-Fit Test**\n\nThe first task is to test whether the observed counts $\\mathbf{C}$ are statistically consistent with the theoretical probabilities $\\mathbf{p} = (p_1, \\dots, p_M)$.\n\n**Null Hypothesis ($H_0$)**: The observed counts $\\mathbf{C}$ are drawn from a multinomial distribution with $N$ trials and success probabilities $\\mathbf{p}$. Formally, $\\mathbf{C} \\sim \\text{Multinomial}(N, \\mathbf{p})$.\n\nUnder $H_0$, the expected count for reaction channel $\\mu$ is $E_\\mu = N p_\\mu$. The Pearson $\\chi^2$ test statistic measures the squared deviation between observed counts ($O_\\mu = C_\\mu$) and expected counts, normalized by the expected counts:\n$$\n\\chi^2 = \\sum_{\\mu=1}^M \\frac{(O_\\mu - E_\\mu)^2}{E_\\mu} = \\sum_{\\mu=1}^M \\frac{(C_\\mu - N p_\\mu)^2}{N p_\\mu}\n$$\nUnder the assumption that $H_0$ is true, and for a sufficiently large sample size $N$, this statistic, $\\chi^2$, is approximately distributed as a central chi-squared random variable with $k = M-1$ degrees of freedom, denoted $\\chi^2_k$. The single constraint is that the counts must sum to $N$, hence $M-1$ degrees of freedom.\n\nThe $p$-value for the test is the probability of observing a $\\chi^2$ statistic at least as large as the value computed from the data, $\\chi^2_{obs}$, assuming $H_0$ is true.\n$$\np\\text{-value} = P(\\chi^2_k \\ge \\chi^2_{obs})\n$$\nThis is calculated using the survival function (complementary cumulative distribution function) of the $\\chi^2_k$ distribution. The null hypothesis is rejected at a significance level $\\alpha$ if the $p$-value is less than or equal to $\\alpha$.\n\n**Part 2: Power Analysis for Sample Size Determination**\n\nThe second task is to determine the minimum sample size $N^\\star$ required to detect a specific deviation from $H_0$ with a desired statistical power.\n\n**Alternative Hypothesis ($H_1$)**: The observed counts $\\mathbf{C}$ are drawn from a multinomial distribution with $N$ trials and a different set of probabilities $\\mathbf{q} = (q_1, \\dots, q_M)$, where $\\mathbf{q} \\ne \\mathbf{p}$.\n\nUnder $H_1$, the same Pearson statistic (which is still computed using $E_\\mu = Np_\\mu$ from $H_0$) follows, for large $N$, a noncentral chi-squared distribution, $\\chi^2_k(\\lambda)$, with the same degrees of freedom $k=M-1$ but with a noncentrality parameter $\\lambda$. The noncentrality parameter is given by:\n$$\n\\lambda = N \\sum_{\\mu=1}^M \\frac{(q_\\mu - p_\\mu)^2}{p_\\mu}\n$$\nThe power of the test, $1-\\beta$, is the probability of correctly rejecting $H_0$ when $H_1$ is in fact true. Rejection occurs when the observed statistic $\\chi^2_{obs}$ exceeds a critical value, $\\chi^2_{crit}$. This critical value is determined by the significance level $\\alpha$ and is the upper $1-\\alpha$ quantile of the central $\\chi^2_k$ distribution (the distribution under $H_0$):\n$$\n\\chi^2_{crit} = F^{-1}_{\\chi^2_k}(1-\\alpha)\n$$\nwhere $F^{-1}$ is the quantile function (inverse CDF).\n\nThe power is thus the probability that a random variable from the noncentral chi-squared distribution $\\chi^2_k(\\lambda)$ exceeds $\\chi^2_{crit}$:\n$$\n\\text{Power} = P(\\chi^2_k(\\lambda) > \\chi^2_{crit}) = 1 - F_{\\chi^2_k(\\lambda)}(\\chi^2_{crit})\n$$\nWe seek the smallest integer $N$ that satisfies Power $\\ge 1-\\beta$. Since power is a monotonically increasing function of the noncentrality parameter $\\lambda$, and $\\lambda$ is a linear function of $N$, we can numerically solve for the value of $N$, let's call it $N_{power}$, that yields exactly the target power $1-\\beta$.\n\nAdditionally, the validity of the $\\chi^2$ approximation requires that the expected counts under the null hypothesis are not too small. A standard rule of thumb is $E_\\mu = N p_\\mu \\ge 5$ for all $\\mu=1, \\dots, M$. This imposes a separate constraint on $N$:\n$$\nN \\ge \\max_{\\mu} \\left( \\frac{5}{p_\\mu} \\right)\n$$\nLet $N_{counts} = \\lceil \\max_{\\mu}(5/p_\\mu) \\rceil$ be the minimum integer sample size satisfying this condition.\n\nThe final required sample size, $N^\\star$, must satisfy both the power requirement and the expected count condition. Therefore, it is the maximum of the two minimums:\n$$\nN^\\star = \\max \\left( \\lceil N_{power} \\rceil, N_{counts} \\right)\n$$\nThis ensures the test is both powerful enough to detect the specified alternative and robust in its distributional assumptions. We now apply this framework to the provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2, ncx2\nfrom scipy.optimize import brentq\nimport math\n\n# Per the problem description, use clean, well-structured code.\n# The `solve` function encapsulates the entire logic.\n\ndef solve():\n    \"\"\"\n    Processes all test cases for the Gillespie reaction selection analysis.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"M\": 3,\n            \"a\": np.array([1.0, 2.0, 3.0]),\n            \"C\": np.array([16, 33, 51]),\n            \"N\": 100,\n            \"alpha\": 0.05,\n            \"q\": np.array([0.15, 0.30, 0.55]),\n            \"power\": 0.80,\n        },\n        {\n            \"name\": \"Case B\",\n            \"M\": 2,\n            \"a\": np.array([0.4, 0.6]),\n            \"C\": np.array([36, 64]),\n            \"N\": 100,\n            \"alpha\": 0.05,\n            \"q\": np.array([0.45, 0.55]),\n            \"power\": 0.80,\n        },\n        {\n            \"name\": \"Case C\",\n            \"M\": 4,\n            \"a\": np.array([0.5, 0.5, 2.0, 7.0]),\n            \"C\": np.array([6, 3, 24, 67]),\n            \"N\": 100,\n            \"alpha\": 0.05,\n            \"q\": np.array([0.10, 0.05, 0.20, 0.65]),\n            \"power\": 0.80,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # --- Unpack Test Case ---\n        a = case[\"a\"]\n        C = case[\"C\"]\n        N = case[\"N\"]\n        M = case[\"M\"]\n        alpha = case[\"alpha\"]\n        q = case[\"q\"]\n        target_power = case[\"power\"]\n        \n        # --- Part 1: Pearson Chi-squared Goodness-of-Fit Test ---\n        \n        # Calculate null hypothesis probabilities p\n        a0 = np.sum(a)\n        p = a / a0\n        \n        # Calculate expected counts under H0\n        E = N * p\n        \n        # Calculate the Pearson chi-squared statistic\n        chi2_obs = np.sum((C - E)**2 / E)\n        \n        # Degrees of freedom\n        df = M - 1\n        \n        # Calculate the p-value\n        p_value = chi2.sf(chi2_obs, df)\n        \n        # Determine rejection of H0\n        reject_h0 = p_value <= alpha\n        \n        # --- Part 2: Power Analysis for Sample Size N* ---\n        \n        # Critical value for the test at significance level alpha\n        chi2_crit = chi2.ppf(1 - alpha, df)\n        \n        # Effect size for noncentrality parameter calculation\n        # This is the sum part of the lambda formula, without N\n        delta = np.sum((q - p)**2 / p)\n        \n        # Define a function to find the noncentrality parameter (lambda)\n        # that yields the target power. We want to find the root of this function.\n        # Power = ncx2.sf(chi2_crit, df, lambda). We want Power = target_power.\n        def power_eq(lmbda):\n            return ncx2.sf(chi2_crit, df, lmbda) - target_power\n\n        # Solve for the noncentrality parameter lambda_star\n        # The search interval [1e-9, 10000] is sufficiently large and robust.\n        try:\n            lambda_star = brentq(power_eq, 1e-9, 10000)\n        except ValueError:\n            # This would happen if the target power is not achievable within the bracket,\n            # which is not expected for these problem parameters.\n            lambda_star = np.nan\n            \n        # Calculate required sample size N_power from lambda_star = N * delta\n        # If delta is zero (p=q), N_power would be infinite.\n        if delta > 0:\n            n_power = lambda_star / delta\n        else:\n            n_power = float('inf')\n\n        # Calculate minimum N to satisfy the E_mu >= 5 rule of thumb\n        n_counts_min_float = np.max(5.0 / p)\n        n_counts_min = math.ceil(n_counts_min_float)\n        \n        # The final required sample size N_star is the maximum of the two requirements\n        n_star = max(math.ceil(n_power), n_counts_min)\n        \n        results.append([p_value, reject_h0, n_star])\n\n    # Final print statement in the exact required format.\n    # The default string representation of the list of lists is correct.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3302887"}]}