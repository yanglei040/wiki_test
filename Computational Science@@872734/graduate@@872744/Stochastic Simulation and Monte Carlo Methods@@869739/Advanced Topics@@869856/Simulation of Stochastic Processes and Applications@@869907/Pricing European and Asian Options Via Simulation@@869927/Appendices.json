{"hands_on_practices": [{"introduction": "Pricing path-dependent options like the arithmetic Asian option presents a challenge, as the continuous-time average in the payoff is often intractable. This practice guides you through implementing a Monte Carlo simulation to tackle this problem, where you will compare two different numerical schemes for approximating the path average. By employing the Common Random Numbers (CRN) variance reduction technique, you will design a rigorous statistical test to determine if the difference between the two schemes is statistically significant, providing a practical foundation in both simulation and hypothesis testing [@problem_id:3331249].", "problem": "Consider a risk-neutral market model in which the underlying asset price process $\\{S_t: t \\in [0,T]\\}$ follows Geometric Brownian Motion under the risk-neutral measure, with stochastic differential equation $dS_t = r S_t \\, dt + \\sigma S_t \\, dW_t$, where $r$ is the continuously compounded risk-free rate, $\\sigma$ is the volatility, and $\\{W_t\\}$ is a standard Brownian motion. The continuously monitored arithmetic average of the asset over the interval $[0,T]$ is defined as $A = \\frac{1}{T} \\int_0^T S_t \\, dt$. Consider the Asian call option payoff $(A - K)^+ = \\max\\{A - K, 0\\}$ with strike $K$, and the corresponding arbitrage-free price $V = \\mathbb{E}\\left[e^{-r T} (A - K)^+ \\right]$.\n\nWe wish to investigate the effect of time discretization of the average $A$ on Monte Carlo price estimates. Let the uniform time grid be $0 = t_0 < t_1 < \\cdots < t_N = T$ with $t_k = k \\Delta t$ and $\\Delta t = T/N$. Define two time-discretization schemes for approximating $A$:\n- Left Riemann scheme: $A_N^{(\\mathrm{L})} = \\frac{1}{N} \\sum_{k=0}^{N-1} S_{t_k}$,\n- Trapezoidal scheme: $A_N^{(\\mathrm{T})} = \\frac{1}{2N} \\left(S_{t_0} + 2 \\sum_{k=1}^{N-1} S_{t_k} + S_{t_N}\\right)$.\n\nUse exact simulation of the Geometric Brownian Motion along the grid, $S_{t_{k+1}} = S_{t_k} \\exp\\left((r - \\tfrac{1}{2}\\sigma^2)\\Delta t + \\sigma \\sqrt{\\Delta t} \\, Z_{k+1}\\right)$, where $(Z_1,\\dots,Z_N)$ are independent standard normal variates. For each simulated path, form the pair of discounted payoffs\n$P_i^{(\\mathrm{L})} = e^{-r T} \\max\\left\\{A_N^{(\\mathrm{L})} - K, 0\\right\\}$ and $P_i^{(\\mathrm{T})} = e^{-r T} \\max\\left\\{A_N^{(\\mathrm{T})} - K, 0\\right\\}$.\nEmploy Common Random Numbers (CRN), i.e., for each path use the same sequence $(Z_1,\\dots,Z_N)$ for both schemes so that differences across schemes are due only to the discretization of $A$.\n\nDesign and implement a paired-samples significance test for the mean difference $\\mu_D = \\mathbb{E}\\left[P^{(\\mathrm{T})} - P^{(\\mathrm{L})}\\right]$, using $M$ independent paired paths that produce differences $D_i = P_i^{(\\mathrm{T})} - P_i^{(\\mathrm{L})}$, $i = 1,\\dots,M$. Your test must:\n- Compute the sample mean $\\bar{D}_M = \\frac{1}{M} \\sum_{i=1}^M D_i$ and the unbiased sample variance $S_D^2 = \\frac{1}{M-1} \\sum_{i=1}^M (D_i - \\bar{D}_M)^2$,\n- Form the Student’s $t$ statistic $T_{\\text{stat}} = \\bar{D}_M / (S_D / \\sqrt{M})$,\n- Compute the two-sided $p$-value under the null hypothesis $H_0: \\mu_D = 0$ using the $t$ distribution with $M-1$ degrees of freedom,\n- Decide whether to reject $H_0$ at a given significance level $\\alpha$.\n\nStart from the fundamental base that under risk-neutral pricing, option values are expectations of discounted payoffs, and that the Geometric Brownian Motion admits exact sampling at discrete times. Use Common Random Numbers to ensure that for each simulated path, both discretization schemes share identical randomness, so the paired differences isolate discretization effects. Justify the paired-samples test using properties of independent and identically distributed differences and finite-variance Monte Carlo estimators.\n\nImplement your solution as a complete, runnable program that performs the above paired test for the following test suite of parameter values (each case specifies $(S_0,K,r,\\sigma,T,N,M,\\alpha)$):\n- Case $1$ (general case): $(S_0,K,r,\\sigma,T,N,M,\\alpha) = (\\,100,\\,100,\\,0.05,\\,0.20,\\,1,\\,16,\\,50000,\\,0.05\\,)$,\n- Case $2$ (boundary coarse grid): $(\\,100,\\,100,\\,0.05,\\,0.20,\\,1,\\,1,\\,50000,\\,0.01\\,)$,\n- Case $3$ (high strike, likely zero payoffs): $(\\,100,\\,200,\\,0.05,\\,0.20,\\,1,\\,16,\\,50000,\\,0.05\\,)$,\n- Case $4$ (deterministic path, zero variance difference): $(\\,100,\\,90,\\,0,\\,0,\\,1,\\,16,\\,10000,\\,0.05\\,)$.\n\nYour program must:\n- For each case, simulate $M$ paired paths using exact Geometric Brownian Motion sampling on the given grid,\n- Use Common Random Numbers for the two discretizations within each case,\n- Compute the paired $t$ statistic and the two-sided $p$-value,\n- Output a single line containing a list of booleans, one per case, indicating whether $H_0$ is rejected at the given $\\alpha$ for that case. The output format must be a single line with the list in order of the cases, e.g., $[b_1,b_2,b_3,b_4]$, where each $b_j$ is either $\\text{True}$ or $\\text{False}$.\n\nNo user input is permitted; all parameters are as specified above. All probabilities must be expressed as decimals. Angles do not appear. Physical units do not apply. The program must be self-contained and must adhere to scientific realism, using exact Geometric Brownian Motion sampling and well-defined statistical testing.", "solution": "The user-provided problem statement is assessed to be valid. It is scientifically grounded in the principles of financial engineering and statistics, is well-posed with a clear objective and sufficient data, and is expressed in objective, formal language. The problem asks for the implementation of a paired-samples $t$-test to compare two numerical approximation schemes for the price of an Asian option. The framework is based on standard risk-neutral pricing theory and Monte Carlo simulation of Geometric Brownian Motion.\n\nThe solution proceeds by first principles, as outlined below, leading to a complete computational implementation.\n\n**1. Risk-Neutral Pricing and Monte Carlo Simulation**\n\nThe arbitrage-free price $V$ of a European-style derivative with payoff $\\text{Payoff}(S)$ at maturity $T$ is given by the expectation of its discounted payoff under the risk-neutral measure $\\mathbb{Q}$:\n$$V = \\mathbb{E}_{\\mathbb{Q}}\\left[e^{-r T} \\text{Payoff}(S)\\right]$$\nFor the Asian call option, the payoff depends on the arithmetic average of the asset price, $A = \\frac{1}{T} \\int_0^T S_t \\, dt$, so the price is $V = \\mathbb{E}_{\\mathbb{Q}}\\left[e^{-r T} (A - K)^+\\right]$. This expectation is generally intractable analytically. The Monte Carlo method approximates this expectation by averaging the discounted payoff over a large number $M$ of simulated asset price paths:\n$$\\hat{V} \\approx \\frac{1}{M} \\sum_{i=1}^M e^{-r T} \\left(A^{(i)} - K\\right)^+$$\nwhere $A^{(i)}$ is the average price for the $i$-th simulated path.\n\n**2. Asset Path Simulation using Exact GBM Sampling**\n\nThe asset price $S_t$ is modeled by Geometric Brownian Motion (GBM), with the stochastic differential equation $dS_t = r S_t \\, dt + \\sigma S_t \\, dW_t$. The exact solution to this SDE provides a formula for simulating the asset price at discrete time steps $t_{k+1} = t_k + \\Delta t$ without introducing any time-stepping error into the path simulation itself:\n$$S_{t_{k+1}} = S_{t_k} \\exp\\left(\\left(r - \\frac{1}{2}\\sigma^2\\right)\\Delta t + \\sigma \\sqrt{\\Delta t} \\, Z_{k+1}\\right)$$\nHere, $\\{Z_k\\}_{k=1}^N$ are independent and identically distributed (i.i.d.) standard normal random variables. This formula is used to generate $M$ independent paths of the asset price, each path consisting of $N+1$ points $\\{S_{t_0}, S_{t_1}, \\ldots, S_{t_N}\\}$.\n\n**3. Numerical Approximation of the Path Average**\n\nSince the continuous integral for the average $A$ cannot be computed, it is approximated numerically from the discrete path values. The problem specifies two schemes:\n- **Left Riemann Scheme**: This approximates the integral using the value at the left endpoint of each subinterval. The corresponding average is:\n  $$A_N^{(\\mathrm{L})} = \\frac{1}{T} \\sum_{k=0}^{N-1} S_{t_k} \\Delta t = \\frac{1}{N} \\sum_{k=0}^{N-1} S_{t_k}$$\n- **Trapezoidal Scheme**: This approximates the integral by averaging the left and right endpoint values on each subinterval. The resulting average is more accurate for smooth functions:\n  $$A_N^{(\\mathrm{T})} = \\frac{1}{T} \\left( \\sum_{k=0}^{N-1} \\frac{S_{t_k} + S_{t_{k+1}}}{2} \\Delta t \\right) = \\frac{1}{2N} \\left(S_{t_0} + 2 \\sum_{k=1}^{N-1} S_{t_k} + S_{t_N}\\right)$$\n\nFor each simulated path $i$, we compute two estimated payoffs: $P_i^{(\\mathrm{L})}$ using $A_N^{(\\mathrm{L}, i)}$ and $P_i^{(\\mathrm{T})}$ using $A_N^{(\\mathrm{T}, i)}$.\n\n**4. Paired-Samples $t$-Test with Common Random Numbers**\n\nTo compare the two approximation schemes, we test if the mean difference in their estimated option prices is zero. The null hypothesis is $H_0: \\mu_D = 0$, where $\\mu_D = \\mathbb{E}[P^{(\\mathrm{T})} - P^{(\\mathrm{L})}]$.\n\nA crucial element is the use of **Common Random Numbers (CRN)**. For each of the $M$ trials, the same sequence of random variates $\\{Z_k\\}$ is used to generate the asset path. This path is then used to compute both $A_N^{(\\mathrm{L})}$ and $A_N^{(\\mathrm{T})}$, and subsequently $P_i^{(\\mathrm{L})}$ and $P_i^{(\\mathrm{T})}$. This technique creates a strong positive correlation between the paired estimators. The variance of the difference $D_i = P_i^{(\\mathrm{T})} - P_i^{(\\mathrm{L})}$ is given by:\n$$\\text{Var}(D_i) = \\text{Var}(P_i^{(\\mathrm{T})}) + \\text{Var}(P_i^{(\\mathrm{L})}) - 2\\text{Cov}(P_i^{(\\mathrm{T})}, P_i^{(\\mathrm{L})})$$\nDue to the CRN-induced positive covariance, $\\text{Var}(D_i)$ is much smaller than it would be if independent random numbers were used. This increases the statistical power of the test, allowing us to detect smaller true differences.\n\nThe differences $D_1, D_2, \\ldots, D_M$ form a sample of i.i.d. random variables. By the Central Limit Theorem, their sample mean $\\bar{D}_M$ is approximately normally distributed. The paired-samples $t$-test is therefore an appropriate statistical tool. We compute the following statistics:\n- **Sample Mean Difference**: $\\bar{D}_M = \\frac{1}{M} \\sum_{i=1}^M D_i$\n- **Unbiased Sample Variance of Differences**: $S_D^2 = \\frac{1}{M-1} \\sum_{i=1}^M (D_i - \\bar{D}_M)^2$\n- **Student's $t$-statistic**: $T_{\\text{stat}} = \\frac{\\bar{D}_M}{S_D / \\sqrt{M}}$\n\nUnder the null hypothesis $H_0$, this statistic follows a Student's $t$ distribution with $M-1$ degrees of freedom.\n\n**5. Hypothesis Decision**\n\nThe two-sided $p$-value is calculated, which is the probability of observing a $t$-statistic as extreme as or more extreme than $T_{\\text{stat}}$ if $H_0$ were true: $p = \\text{Pr}(|t_{M-1}| \\geq |T_{\\text{stat}}|)$. A special case arises if $S_D = 0$ (as in Case $4$ with $\\sigma=0$), meaning all differences $D_i$ are identical. If they are all zero, $\\bar{D}_M = 0$ and the data perfectly supports $H_0$, so the $p$-value is $1$. Finally, we reject $H_0$ at a significance level $\\alpha$ if the computed $p$-value is less than $\\alpha$.\n\nThe implementation will perform this entire procedure for each test case, leveraging vectorized operations in `numpy` for efficiency.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef run_paired_test(S0, K, r, sigma, T, N, M, alpha):\n    \"\"\"\n    Performs a paired-samples t-test for the difference in Asian option prices\n    estimated using Left Riemann and Trapezoidal averaging schemes.\n\n    Args:\n        S0 (float): Initial asset price.\n        K (float): Strike price.\n        r (float): Risk-free rate.\n        sigma (float): Volatility.\n        T (float): Time to maturity.\n        N (int): Number of time steps.\n        M (int): Number of Monte Carlo paths.\n        alpha (float): Significance level for the test.\n\n    Returns:\n        bool: True if the null hypothesis is rejected, False otherwise.\n    \"\"\"\n    dt = T / N\n\n    # Generate M x N matrix of standard normal randoms for Common Random Numbers\n    # This ensures both schemes use the same randomness for each path.\n    Z = np.random.randn(M, N)\n\n    # Simulate M paths, each with N+1 time points (t_0, ..., t_N)\n    # S is a M x (N+1) matrix\n    S = np.zeros((M, N + 1))\n    S[:, 0] = S0\n\n    # GBM simulation using the exact solution, vectorized over M paths\n    drift = (r - 0.5 * sigma**2) * dt\n    vol = sigma * np.sqrt(dt)\n    for k in range(N):\n        S[:, k + 1] = S[:, k] * np.exp(drift + vol * Z[:, k])\n\n    # Calculate averages for each path (vectorized)\n    # Left Riemann scheme: A_L = (1/N) * sum_{k=0}^{N-1} S_{t_k}\n    A_L = np.mean(S[:, :-1], axis=1)\n\n    # Trapezoidal scheme: A_T = (1/2N) * (S_0 + 2*sum_{k=1}^{N-1} S_k + S_N)\n    # which is equivalent to (1/N) * (0.5*S_0 + sum_{k=1}^{N-1} S_k + 0.5*S_N)\n    # This implementation is robust and works for N=1, where the sum term is empty.\n    sum_middle_terms = np.sum(S[:, 1:-1], axis=1) if N > 1 else 0\n    A_T = (0.5 * S[:, 0] + sum_middle_terms + 0.5 * S[:, -1]) / N\n\n    # Calculate discounted payoffs for both schemes\n    discount_factor = np.exp(-r * T)\n    P_L = discount_factor * np.maximum(0, A_L - K)\n    P_T = discount_factor * np.maximum(0, A_T - K)\n\n    # Compute the paired differences\n    D = P_T - P_L\n\n    # Perform the paired-samples t-test\n    D_bar = np.mean(D)\n    S_D = np.std(D, ddof=1)  # Unbiased sample standard deviation\n\n    # Handle the special case of zero variance\n    # This occurs if all differences D_i are identical (e.g., in the deterministic case)\n    if np.isclose(S_D, 0.0):\n        # If the mean difference is also zero, all D_i are zero.\n        # The data perfectly supports the null hypothesis. p-value is 1.\n        if np.isclose(D_bar, 0.0):\n            p_value = 1.0\n        # If the mean difference is non-zero, it means a constant, non-zero difference.\n        # This is infinite evidence against the null hypothesis. p-value is 0.\n        else:\n            p_value = 0.0\n    else:\n        # Standard t-test calculation\n        t_stat = D_bar / (S_D / np.sqrt(M))\n        df = M - 1\n        # Two-sided p-value from the survival function (1 - cdf)\n        p_value = 2 * t.sf(np.abs(t_stat), df)\n\n    # Decision rule: reject H0 if p-value is less than the significance level\n    return p_value  alpha\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement:\n    # (S0, K, r, sigma, T, N, M, alpha)\n    test_cases = [\n        (100.0, 100.0, 0.05, 0.20, 1.0, 16, 50000, 0.05),\n        (100.0, 100.0, 0.05, 0.20, 1.0, 1, 50000, 0.01),\n        (100.0, 200.0, 0.05, 0.20, 1.0, 16, 50000, 0.05),\n        (100.0, 90.0, 0.0, 0.0, 1.0, 16, 10000, 0.05),\n    ]\n\n    # Set a random seed for reproducibility of the results\n    np.random.seed(42)\n\n    results = []\n    for case in test_cases:\n        reject_h0 = run_paired_test(*case)\n        results.append(reject_h0)\n\n    # Final print statement in the exact required format: [True,False,...]\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solution\nsolve()\n\n```", "id": "3331249"}, {"introduction": "Beyond simply applying variance reduction techniques, a quantitative analyst must be able to prove their effectiveness. This exercise shifts the focus from implementation to theoretical validation, specifically for the Common Random Numbers (CRN) method when comparing two scenarios, a core task in sensitivity analysis. Starting from the multivariate Central Limit Theorem, you will derive the confidence interval for the difference between two option prices and quantify the efficiency gain from using CRN versus independent sampling, thereby solidifying your understanding of the statistical underpinnings of this essential tool [@problem_id:3331290].", "problem": "Consider a frictionless market where an underlying asset price process under the risk-neutral measure follows geometric Brownian motion with dynamics $dS_{t} = r S_{t}\\,dt + \\sigma S_{t}\\,dW_{t}$, with constant risk-free rate $r$, volatility $\\sigma$, and standard Brownian motion $W_{t}$. Let $S_{0} = 100$, maturity $T = 1$ (in years), and strike $K = 100$. Consider the discretely monitored arithmetic-average Asian call with monitoring at $m = 12$ equally spaced times $t_{j} = j T/m$, $j = 1,\\dots,m$, and discounted payoff $X(\\sigma) = \\exp(-rT) \\max\\!\\big(\\frac{1}{m}\\sum_{j=1}^{m} S_{t_{j}} - K, 0\\big)$.\n\nYou wish to estimate the difference in prices $\\Delta = P(\\sigma_{2}) - P(\\sigma_{1})$, where $P(\\sigma) = \\mathbb{E}[X(\\sigma)]$ and the two scenarios are $\\sigma_{1} = 0.20$ and $\\sigma_{2} = 0.22$. You simulate $n$ independent and identically distributed pairs $\\{(X_{i}(\\sigma_{1}), X_{i}(\\sigma_{2}))\\}_{i=1}^{n}$ by driving both scenarios with the same standard normal vectors per path (common random numbers), i.e., using the same Brownian increments for each pair so that the two components of each pair are dependent.\n\n- Using only foundational principles from risk-neutral pricing, the multivariate Central Limit Theorem, and properties of covariance under linear transformations, perform the following:\n\n1) Starting from the multivariate Central Limit Theorem applied to the bivariate vector of discounted payoffs $(X(\\sigma_{1}), X(\\sigma_{2}))$, derive the large-sample $(1-\\alpha)$ confidence interval for $\\Delta$ under the paired-differences common-random-numbers estimator $\\widehat{\\Delta}_{n} = \\overline{X}_{n}(\\sigma_{2}) - \\overline{X}_{n}(\\sigma_{1})$, expressed explicitly in terms of $n$, $\\operatorname{Var}[X(\\sigma_{1})]$, $\\operatorname{Var}[X(\\sigma_{2})]$, and $\\operatorname{Cov}(X(\\sigma_{1}), X(\\sigma_{2}))$.\n\n2) Suppose instead that you organize the $n$ paired replications into $B$ blocks of equal size $b$ (so $n = B b$). In each block, use common random numbers across the two volatility scenarios, and form block means for each scenario and their paired differences. Derive the corresponding large-sample $(1-\\alpha)$ confidence interval for $\\Delta$ based on the $B$ blockwise paired differences, and show its relation to your expression in part $1)$.\n\n3) A pilot run with $n = 10000$ paired paths produces unbiased estimates $\\widehat{\\operatorname{Var}}[X(\\sigma_{1})] = 225$, $\\widehat{\\operatorname{Var}}[X(\\sigma_{2})] = 400$, and $\\widehat{\\operatorname{Cov}}(X(\\sigma_{1}), X(\\sigma_{2})) = 280$. Treat these as the true second moments for the purpose of this calculation. Compute the asymptotic ratio $R$ of the half-width of the two-sided confidence interval at confidence level $0.95$ using the paired-differences common-random-numbers method versus the half-width you would obtain if you simulated the two scenarios independently with the same total number of paths and without common random numbers. Round your answer to four significant figures.\n\nYour final answer should be the single real value of $R$.", "solution": "This problem requires the derivation of confidence intervals for the difference of two expectations estimated via Monte Carlo simulation and a comparison of the efficiency of the common random numbers (CRN) technique versus independent sampling.\n\n1) Derivation of the confidence interval for the paired-differences estimator.\n\nLet the bivariate random vector $\\mathbf{Y}_{i}$ represent the $i$-th simulated pair of discounted payoffs:\n$$\n\\mathbf{Y}_{i} = \\begin{pmatrix} X_{i}(\\sigma_{1}) \\\\ X_{i}(\\sigma_{2}) \\end{pmatrix}\n$$\nThese vectors $\\{\\mathbf{Y}_{i}\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.). The expectation of each vector is the true pair of prices:\n$$\n\\boldsymbol{\\mu} = \\mathbb{E}[\\mathbf{Y}_{i}] = \\begin{pmatrix} \\mathbb{E}[X(\\sigma_{1})] \\\\ \\mathbb{E}[X(\\sigma_{2})] \\end{pmatrix} = \\begin{pmatrix} P(\\sigma_{1}) \\\\ P(\\sigma_{2}) \\end{pmatrix}\n$$\nThe covariance matrix of $\\mathbf{Y}_{i}$ is given by:\n$$\n\\mathbf{\\Sigma} = \\begin{pmatrix} \\operatorname{Var}[X(\\sigma_{1})]  \\operatorname{Cov}(X(\\sigma_{1}), X(\\sigma_{2})) \\\\ \\operatorname{Cov}(X(\\sigma_{1}), X(\\sigma_{2}))  \\operatorname{Var}[X(\\sigma_{2})] \\end{pmatrix}\n$$\nThe sample mean vector is $\\overline{\\mathbf{Y}}_{n} = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{Y}_{i} = (\\overline{X}_{n}(\\sigma_{1}), \\overline{X}_{n}(\\sigma_{2}))^T$. According to the multivariate Central Limit Theorem (MCLT), for large $n$, the distribution of the sample mean vector is approximately multivariate normal:\n$$\n\\sqrt{n}(\\overline{\\mathbf{Y}}_{n} - \\boldsymbol{\\mu}) \\xrightarrow{D} \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma})\n$$\nwhere $\\xrightarrow{D}$ denotes convergence in distribution.\n\nWe are interested in the difference $\\Delta = P(\\sigma_{2}) - P(\\sigma_{1})$. This can be expressed as a linear transformation of the mean vector $\\boldsymbol{\\mu}$ using the vector $\\mathbf{c} = (-1, 1)^T$, such that $\\Delta = \\mathbf{c}^T \\boldsymbol{\\mu}$. The estimator for $\\Delta$ is $\\widehat{\\Delta}_{n} = \\overline{X}_{n}(\\sigma_{2}) - \\overline{X}_{n}(\\sigma_{1}) = \\mathbf{c}^T \\overline{\\mathbf{Y}}_{n}$.\n\nApplying the same linear transformation to the MCLT result, we find the asymptotic distribution of our estimator:\n$$\n\\sqrt{n}(\\widehat{\\Delta}_{n} - \\Delta) = \\mathbf{c}^T \\left( \\sqrt{n}(\\overline{\\mathbf{Y}}_{n} - \\boldsymbol{\\mu}) \\right) \\xrightarrow{D} \\mathcal{N}(0, \\mathbf{c}^T \\mathbf{\\Sigma} \\mathbf{c})\n$$\nThe variance of the limiting distribution, $\\sigma_{\\Delta}^2 = \\mathbf{c}^T \\mathbf{\\Sigma} \\mathbf{c}$, is calculated as:\n$$\n\\sigma_{\\Delta}^2 = \\begin{pmatrix} -1  1 \\end{pmatrix} \\begin{pmatrix} \\operatorname{Var}[X(\\sigma_{1})]  \\operatorname{Cov}(X(\\sigma_{1}), X(\\sigma_{2})) \\\\ \\operatorname{Cov}(X(\\sigma_{1}), X(\\sigma_{2}))  \\operatorname{Var}[X(\\sigma_{2})] \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}\n$$\n$$\n\\sigma_{\\Delta}^2 = \\operatorname{Var}[X(\\sigma_{1})] + \\operatorname{Var}[X(\\sigma_{2})] - 2\\operatorname{Cov}(X(\\sigma_{1}), X(\\sigma_{2}))\n$$\nThis is the variance of the difference of the two random variables, $\\operatorname{Var}[X(\\sigma_{2}) - X(\\sigma_{1})]$.\nFor large $n$, $\\widehat{\\Delta}_{n}$ is approximately normally distributed with mean $\\Delta$ and variance $\\frac{\\sigma_{\\Delta}^2}{n}$. Therefore, the standardized variable\n$$\n\\frac{\\widehat{\\Delta}_{n} - \\Delta}{\\sqrt{\\sigma_{\\Delta}^2/n}} \\approx \\mathcal{N}(0, 1)\n$$\nA large-sample $(1-\\alpha)$ confidence interval for $\\Delta$ is constructed as:\n$$\n\\widehat{\\Delta}_{n} \\pm z_{1-\\alpha/2} \\sqrt{\\frac{\\sigma_{\\Delta}^2}{n}}\n$$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution. Substituting the expression for $\\sigma_{\\Delta}^2$, the confidence interval is:\n$$\n\\widehat{\\Delta}_{n} \\pm z_{1-\\alpha/2} \\sqrt{\\frac{\\operatorname{Var}[X(\\sigma_{1})] + \\operatorname{Var}[X(\\sigma_{2})] - 2\\operatorname{Cov}(X(\\sigma_{1}), X(\\sigma_{2}))}{n}}\n$$\n\n2) Derivation for the blocking method.\n\nThe $n$ replications are grouped into $B$ blocks of size $b$, where $n = B b$. For each block $k \\in \\{1, \\dots, B\\}$, we compute the paired difference of the block means:\n$$\nD_{k} = \\overline{X}_{b,k}(\\sigma_{2}) - \\overline{X}_{b,k}(\\sigma_{1}) = \\frac{1}{b}\\sum_{i=(k-1)b+1}^{kb} X_{i}(\\sigma_{2}) - \\frac{1}{b}\\sum_{i=(k-1)b+1}^{kb} X_{i}(\\sigma_{1})\n$$\nThe variables $D_{1}, \\dots, D_{B}$ are i.i.d. because they are computed from non-overlapping blocks of i.i.d. simulations.\nThe expectation of each $D_k$ is $\\mathbb{E}[D_k] = \\mathbb{E}[\\overline{X}_{b,k}(\\sigma_{2})] - \\mathbb{E}[\\overline{X}_{b,k}(\\sigma_{1})] = P(\\sigma_{2}) - P(\\sigma_{1}) = \\Delta$.\nThe variance of each $D_k$ is $\\operatorname{Var}(D_k) = \\operatorname{Var}(\\overline{X}_{b,k}(\\sigma_{2}) - \\overline{X}_{b,k}(\\sigma_{1}))$. Using the property that the variance of a sample mean of $b$ i.i.d variables is $\\frac{1}{b}$ times the variance of a single variable, we have:\n$$\n\\operatorname{Var}(D_k) = \\operatorname{Var}(\\overline{X}_{b,k}(\\sigma_{2})) + \\operatorname{Var}(\\overline{X}_{b,k}(\\sigma_{1})) - 2\\operatorname{Cov}(\\overline{X}_{b,k}(\\sigma_{1}), \\overline{X}_{b,k}(\\sigma_{2}))\n$$\n$$\n\\operatorname{Var}(D_k) = \\frac{\\operatorname{Var}[X(\\sigma_{2})]}{b} + \\frac{\\operatorname{Var}[X(\\sigma_{1})]}{b} - 2 \\frac{\\operatorname{Cov}(X(\\sigma_{1}), X(\\sigma_{2}))}{b}\n$$\n$$\n\\operatorname{Var}(D_k) = \\frac{1}{b} \\left( \\operatorname{Var}[X(\\sigma_{1})] + \\operatorname{Var}[X(\\sigma_{2})] - 2\\operatorname{Cov}(X(\\sigma_{1}), X(\\sigma_{2})) \\right)\n$$\nThe overall estimator is the average of these block differences, $\\widehat{\\Delta}_{B} = \\frac{1}{B}\\sum_{k=1}^B D_k$. Note that this is identical to the original estimator: $\\widehat{\\Delta}_{B} = \\frac{1}{B}\\sum_{k=1}^B \\frac{1}{b}\\sum_{i \\in \\text{block } k} (X_i(\\sigma_2) - X_i(\\sigma_1)) = \\frac{1}{Bb}\\sum_{i=1}^n(X_i(\\sigma_2) - X_i(\\sigma_1)) = \\widehat{\\Delta}_{n}$.\nBy the standard CLT applied to the i.i.d. variables $D_k$, for large $B$, the estimator $\\widehat{\\Delta}_{B}$ is approximately normal with mean $\\Delta$ and variance $\\frac{\\operatorname{Var}(D_k)}{B}$. The confidence interval is:\n$$\n\\widehat{\\Delta}_{B} \\pm z_{1-\\alpha/2} \\sqrt{\\frac{\\operatorname{Var}(D_k)}{B}}\n$$\nSubstituting $\\operatorname{Var}(D_k)$:\n$$\n\\widehat{\\Delta}_{B} \\pm z_{1-\\alpha/2} \\sqrt{\\frac{1}{Bb} \\left( \\operatorname{Var}[X(\\sigma_{1})] + \\operatorname{Var}[X(\\sigma_{2})] - 2\\operatorname{Cov}(X(\\sigma_{1}), X(\\sigma_{2})) \\right)}\n$$\nSince $n=Bb$ and $\\widehat{\\Delta}_{B} = \\widehat{\\Delta}_{n}$, this simplifies to:\n$$\n\\widehat{\\Delta}_{n} \\pm z_{1-\\alpha/2} \\sqrt{\\frac{\\operatorname{Var}[X(\\sigma_{1})] + \\operatorname{Var}[X(\\sigma_{2})] - 2\\operatorname{Cov}(X(\\sigma_{1}), X(\\sigma_{2}))}{n}}\n$$\nThis expression is identical to the one derived in part 1). The relationship is that both approaches are asymptotically equivalent. The blocking method offers a practical way to estimate the variance of the overall estimator by computing the sample variance of the block differences $D_k$, without needing to estimate individual variances and the covariance term explicitly.\n\n3) Calculation of the ratio of half-widths.\n\nThe half-width (HW) of a two-sided confidence interval is given by $HW = z_{1-\\alpha/2} \\times SE$, where $SE$ is the standard error of the estimator. We need to compute the ratio $R = \\frac{HW_{\\text{CRN}}}{HW_{\\text{Ind}}}$. The $z_{1-\\alpha/2}$ term will cancel out, so $R = \\frac{SE_{\\text{CRN}}}{SE_{\\text{Ind}}}$.\n\nFor the paired-differences Common Random Numbers (CRN) method, the estimator is $\\widehat{\\Delta}_n = \\overline{X}_n(\\sigma_2) - \\overline{X}_n(\\sigma_1)$. The variance of this estimator is:\n$$\n\\operatorname{Var}[\\widehat{\\Delta}_n]_{\\text{CRN}} = \\frac{1}{n} \\operatorname{Var}[X(\\sigma_{2}) - X(\\sigma_{1})] = \\frac{1}{n} \\left( \\operatorname{Var}[X(\\sigma_{1})] + \\operatorname{Var}[X(\\sigma_{2})] - 2\\operatorname{Cov}(X(\\sigma_{1}), X(\\sigma_{2})) \\right)\n$$\nUsing the provided values $\\operatorname{Var}[X(\\sigma_{1})] = 225$, $\\operatorname{Var}[X(\\sigma_{2})] = 400$, $\\operatorname{Cov}(X(\\sigma_{1}), X(\\sigma_{2})) = 280$, and $n=10000$:\n$$\n\\operatorname{Var}[\\widehat{\\Delta}_n]_{\\text{CRN}} = \\frac{1}{10000} (225 + 400 - 2 \\times 280) = \\frac{625 - 560}{10000} = \\frac{65}{10000}\n$$\nThe standard error is $SE_{\\text{CRN}} = \\sqrt{\\frac{65}{10000}} = \\frac{\\sqrt{65}}{100}$.\n\nFor the case of independent simulations, we estimate $P(\\sigma_1)$ with $\\overline{X}_n(\\sigma_1)$ from $n$ paths and $P(\\sigma_2)$ with $\\overline{X}_n(\\sigma_2)$ from another $n$ paths, where the two sets of simulations are independent. The covariance between $\\overline{X}_n(\\sigma_1)$ and $\\overline{X}_n(\\sigma_2)$ is zero. The variance of the difference estimator $\\widehat{\\Delta}_n$ is:\n$$\n\\operatorname{Var}[\\widehat{\\Delta}_n]_{\\text{Ind}} = \\operatorname{Var}[\\overline{X}_n(\\sigma_{2}) - \\overline{X}_n(\\sigma_{1})] = \\operatorname{Var}[\\overline{X}_n(\\sigma_{2})] + \\operatorname{Var}[\\overline{X}_n(\\sigma_{1})]\n$$\n$$\n\\operatorname{Var}[\\widehat{\\Delta}_n]_{\\text{Ind}} = \\frac{\\operatorname{Var}[X(\\sigma_{2})]}{n} + \\frac{\\operatorname{Var}[X(\\sigma_{1})]}{n} = \\frac{1}{n} \\left( \\operatorname{Var}[X(\\sigma_{1})] + \\operatorname{Var}[X(\\sigma_{2})] \\right)\n$$\nUsing the provided values:\n$$\n\\operatorname{Var}[\\widehat{\\Delta}_n]_{\\text{Ind}} = \\frac{1}{10000} (225 + 400) = \\frac{625}{10000}\n$$\nThe standard error is $SE_{\\text{Ind}} = \\sqrt{\\frac{625}{10000}} = \\frac{25}{100}$.\n\nThe ratio $R$ of the half-widths is the ratio of the standard errors:\n$$\nR = \\frac{SE_{\\text{CRN}}}{SE_{\\text{Ind}}} = \\frac{\\sqrt{V_{\\text{CRN}}}}{\\sqrt{V_{\\text{Ind}}}} = \\sqrt{\\frac{V_{\\text{CRN}}}{V_{\\text{Ind}}}}\n$$\n$$\nR = \\sqrt{\\frac{\\frac{1}{n} \\left( \\operatorname{Var}[X(\\sigma_{1})] + \\operatorname{Var}[X(\\sigma_{2})] - 2\\operatorname{Cov}(X(\\sigma_{1}), X(\\sigma_{2})) \\right)}{\\frac{1}{n} \\left( \\operatorname{Var}[X(\\sigma_{1})] + \\operatorname{Var}[X(\\sigma_{2})] \\right)}}\n$$\n$$\nR = \\sqrt{\\frac{225 + 400 - 2(280)}{225 + 400}} = \\sqrt{\\frac{625 - 560}{625}} = \\sqrt{\\frac{65}{625}} = \\frac{\\sqrt{65}}{25}\n$$\nNow, we compute the numerical value:\n$$\nR = \\frac{\\sqrt{65}}{25} \\approx \\frac{8.0622577}{25} \\approx 0.3224903\n$$\nRounding to four significant figures, we get $0.3225$.", "answer": "$$\\boxed{0.3225}$$", "id": "3331290"}, {"introduction": "The control variate method is another powerful variance reduction technique that leverages a correlated instrument with a known price to improve estimation efficiency. However, the ideal control coefficient $\\beta$ is rarely known in practice and must be estimated from the data. This exercise [@problem_id:3331327] challenges you to derive the exact finite-sample variance of the control variate estimator, revealing the statistical \"cost\" of estimating $\\beta$. Under the simplifying—though often hypothetical—assumption of joint normality, you will gain deep insight into the mechanics of the estimator and the factors that influence its precision in a real-world application.", "problem": "Consider the problem of pricing a path-dependent option via Monte Carlo (MC) simulation using the control variate method. Let $Y$ denote the discounted payoff of the target option (for example, an arithmetic Asian call under the Black–Scholes model) and let $X$ denote the discounted payoff of a correlated control variate with known expectation $E[X]=\\nu$ (for example, a geometric Asian call whose value is known in closed form). Assume that $(Y,X)$ is jointly Gaussian with mean vector $(\\mu_{Y},\\mu_{X})$ and covariance matrix\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n\\sigma_{X}^{2}  \\sigma_{XY} \\\\\n\\sigma_{XY}  \\sigma_{Y}^{2}\n\\end{pmatrix},\n$$\nand define the correlation coefficient $\\rho$ by $\\rho \\;=\\; \\sigma_{XY}/(\\sigma_{X}\\sigma_{Y})$. Suppose that $n$ independent and identically distributed samples $\\{(Y_{i},X_{i})\\}_{i=1}^{n}$ are drawn. The control variate estimator of $E[Y]$ using an estimated control coefficient is\n$$\n\\widehat{\\mu}_{n}^{\\mathrm{cv}} \\;=\\; \\overline{Y}_{n} \\;-\\; \\widehat{\\beta}_{n}\\,\\big(\\overline{X}_{n} \\;-\\; \\nu\\big),\n$$\nwhere $\\overline{Y}_{n} \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} Y_{i}$, $\\overline{X}_{n} \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$, and $\\widehat{\\beta}_{n}$ is the sample control coefficient defined by\n$$\n\\widehat{\\beta}_{n} \\;=\\; \\frac{\\sum_{i=1}^{n} (X_{i}-\\overline{X}_{n})(Y_{i}-\\overline{Y}_{n})}{\\sum_{i=1}^{n} (X_{i}-\\overline{X}_{n})^{2}} \\;=\\; \\frac{S_{XY}}{S_{XX}}.\n$$\nStarting from foundational probability facts (joint normality, properties of the multivariate normal distribution, and the Wishart distribution of the sample covariance under normality), derive the sampling distribution of $\\widehat{\\beta}_{n}$ and use it to quantify how estimating $\\beta$ contributes to the uncertainty of $\\widehat{\\mu}_{n}^{\\mathrm{cv}}$. Express the exact finite-sample variance $\\mathrm{Var}(\\widehat{\\mu}_{n}^{\\mathrm{cv}})$ in closed form as a function of $n$, $\\sigma_{Y}$, $\\sigma_{X}$, and $\\rho$.\n\nProvide as your final answer the single closed-form analytic expression for $\\mathrm{Var}(\\widehat{\\mu}_{n}^{\\mathrm{cv}})$ under the stated assumptions. No numerical evaluation is required.", "solution": "The problem requires the derivation of the exact finite-sample variance of the control variate estimator $\\widehat{\\mu}_{n}^{\\mathrm{cv}}$ under the assumption that the target payoff $Y$ and the control payoff $X$ are jointly normally distributed.\n\nThe given estimator is $\\widehat{\\mu}_{n}^{\\mathrm{cv}} = \\overline{Y}_{n} - \\widehat{\\beta}_{n}(\\overline{X}_{n} - \\nu)$, where $\\nu = E[X] = \\mu_{X}$. The quantity to be computed is $\\mathrm{Var}(\\widehat{\\mu}_{n}^{\\mathrm{cv}})=E\\left[\\left(\\widehat{\\mu}_{n}^{\\mathrm{cv}} - E[\\widehat{\\mu}_{n}^{\\mathrm{cv}}]\\right)^2\\right]$. First, we analyze the structure of the estimator.\n\nThe joint normality of $(Y, X)$ implies a linear conditional expectation. The distribution of $Y$ conditional on $X$ is normal with mean $E[Y|X] = \\mu_{Y} + \\frac{\\sigma_{XY}}{\\sigma_{X}^{2}}(X - \\mu_{X})$ and variance $\\mathrm{Var}(Y|X) = \\sigma_{Y}^{2}(1-\\rho^{2})$.\nThis allows us to write a linear regression model for each observation $Y_i$:\n$$Y_{i} = \\mu_{Y} + \\beta(X_{i} - \\mu_{X}) + \\epsilon_{i}$$\nwhere $\\beta = \\frac{\\sigma_{XY}}{\\sigma_{X}^{2}}$ is the theoretical regression coefficient. The error terms $\\epsilon_{i}$ are independent and identically distributed as $N(0, \\sigma_{\\epsilon}^{2})$, where $\\sigma_{\\epsilon}^{2} = \\sigma_{Y}^{2}(1-\\rho^{2})$. Furthermore, the error terms $\\epsilon_i$ are independent of the predictor variables $X_j$ for all $i,j$.\nAveraging over the $n$ samples gives:\n$$\\overline{Y}_{n} = \\mu_{Y} + \\beta(\\overline{X}_{n} - \\mu_{X}) + \\overline{\\epsilon}_{n}$$\nwhere $\\overline{\\epsilon}_{n} = \\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}$.\n\nThe estimator for the mean of $Y$ is $\\widehat{\\mu}_{n}^{\\mathrm{cv}} = \\overline{Y}_{n} - \\widehat{\\beta}_{n}(\\overline{X}_{n} - \\mu_{X})$, where $\\widehat{\\beta}_{n}$ is the Ordinary Least Squares (OLS) estimator of $\\beta$. The deviation of the estimator from the true mean $\\mu_Y$ is:\n$$\\widehat{\\mu}_{n}^{\\mathrm{cv}} - \\mu_{Y} = \\left(\\overline{Y}_{n} - \\mu_{Y}\\right) - \\widehat{\\beta}_{n}(\\overline{X}_{n} - \\mu_{X})$$\nSubstitute the expression for $\\overline{Y}_{n}$:\n$$\\widehat{\\mu}_{n}^{\\mathrm{cv}} - \\mu_{Y} = \\left(\\mu_{Y} + \\beta(\\overline{X}_{n} - \\mu_{X}) + \\overline{\\epsilon}_{n} - \\mu_{Y}\\right) - \\widehat{\\beta}_{n}(\\overline{X}_{n} - \\mu_{X})$$\n$$\\widehat{\\mu}_{n}^{\\mathrm{cv}} - \\mu_{Y} = \\beta(\\overline{X}_{n} - \\mu_{X}) + \\overline{\\epsilon}_{n} - \\widehat{\\beta}_{n}(\\overline{X}_{n} - \\mu_{X})$$\n$$\\widehat{\\mu}_{n}^{\\mathrm{cv}} - \\mu_{Y} = \\overline{\\epsilon}_{n} - (\\widehat{\\beta}_{n} - \\beta)(\\overline{X}_{n} - \\mu_{X})$$\nNow, we can compute the variance. The variance is the expectation of the square of this deviation, since we can show the estimator is unbiased ($E[\\widehat{\\mu}_{n}^{\\mathrm{cv}}] = \\mu_{Y}$) under normality.\n$$\\mathrm{Var}(\\widehat{\\mu}_{n}^{\\mathrm{cv}}) = E\\left[ \\left( \\overline{\\epsilon}_{n} - (\\widehat{\\beta}_{n} - \\beta)(\\overline{X}_{n} - \\mu_{X}) \\right)^{2} \\right]$$\nExpanding the square gives:\n$$\\mathrm{Var}(\\widehat{\\mu}_{n}^{\\mathrm{cv}}) = E[\\overline{\\epsilon}_{n}^{2}] - 2E[\\overline{\\epsilon}_{n}(\\widehat{\\beta}_{n} - \\beta)(\\overline{X}_{n} - \\mu_{X})] + E[(\\widehat{\\beta}_{n} - \\beta)^{2}(\\overline{X}_{n} - \\mu_{X})^{2}]$$\nWe evaluate each term separately.\n\n1.  **First term: $E[\\overline{\\epsilon}_{n}^{2}]$**\n    Since $E[\\overline{\\epsilon}_{n}]=0$, this term is the variance of $\\overline{\\epsilon}_{n}$. The $\\epsilon_{i}$ are i.i.d. with variance $\\sigma_{\\epsilon}^{2} = \\sigma_{Y}^{2}(1-\\rho^{2})$.\n    $$E[\\overline{\\epsilon}_{n}^{2}] = \\mathrm{Var}(\\overline{\\epsilon}_{n}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\right) = \\frac{1}{n^{2}}\\sum_{i=1}^{n}\\mathrm{Var}(\\epsilon_{i}) = \\frac{n\\sigma_{\\epsilon}^{2}}{n^{2}} = \\frac{\\sigma_{\\epsilon}^{2}}{n} = \\frac{\\sigma_{Y}^{2}(1-\\rho^{2})}{n}$$\n\n2.  **Second term (cross-product): $-2E[\\overline{\\epsilon}_{n}(\\widehat{\\beta}_{n} - \\beta)(\\overline{X}_{n} - \\mu_{X})]$**\n    First, we express $(\\widehat{\\beta}_{n} - \\beta)$ in terms of the error terms $\\epsilon_i$.\n    The OLS estimator is $\\widehat{\\beta}_{n} = \\frac{\\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n})(Y_{i}-\\overline{Y}_{n})}{\\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n})^{2}}$.\n    From $Y_i = \\mu_Y + \\beta(X_i-\\mu_X) + \\epsilon_i$, we get $Y_i - \\overline{Y}_n = \\beta(X_i - \\overline{X}_n) + (\\epsilon_i - \\overline{\\epsilon}_n)$.\n    Substituting this into the numerator for $\\widehat{\\beta}_{n}$:\n    $$\\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n})(Y_{i}-\\overline{Y}_{n}) = \\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n})[\\beta(X_{i}-\\overline{X}_{n}) + (\\epsilon_i - \\overline{\\epsilon}_n)]$$\n    $$= \\beta \\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n})^{2} + \\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n})\\epsilon_{i} - \\overline{\\epsilon}_{n}\\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n})$$\n    Since $\\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n}) = 0$, the last term vanishes. Let $S_{XX} = \\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n})^{2}$.\n    $$\\widehat{\\beta}_{n} = \\frac{\\beta S_{XX} + \\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n})\\epsilon_{i}}{S_{XX}} = \\beta + \\frac{\\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n})\\epsilon_{i}}{S_{XX}}$$\n    Thus, $\\widehat{\\beta}_{n} - \\beta = \\frac{\\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n})\\epsilon_{i}}{S_{XX}}$.\n    The expectation of the cross-term is:\n    $$E\\left[\\left(\\frac{1}{n}\\sum_{j=1}^{n}\\epsilon_{j}\\right) \\left(\\frac{\\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n})\\epsilon_{i}}{S_{XX}}\\right) (\\overline{X}_{n}-\\mu_{X})\\right]$$\n    We use the Law of Total Expectation by conditioning on the sequence $\\mathbf{X} = \\{X_1, ..., X_n\\}$. The error terms $\\epsilon_i$ are independent of $\\mathbf{X}$.\n    $$E_{\\mathbf{X}}\\left[\\frac{\\overline{X}_{n}-\\mu_{X}}{n S_{XX}} E_{\\epsilon|\\mathbf{X}}\\left[ \\left(\\sum_{j=1}^{n}\\epsilon_{j}\\right) \\left(\\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n})\\epsilon_{i}\\right) \\right]\\right]$$\n    The inner conditional expectation is:\n    $$E\\left[ \\sum_{i,j} (X_{i}-\\overline{X}_{n})\\epsilon_{i}\\epsilon_{j} \\right] = \\sum_{i,j} (X_{i}-\\overline{X}_{n})E[\\epsilon_{i}\\epsilon_{j}]$$\n    Since $E[\\epsilon_{i}\\epsilon_{j}] = \\sigma_{\\epsilon}^{2}\\delta_{ij}$ (where $\\delta_{ij}$ is the Kronecker delta), this simplifies to:\n    $$\\sum_{i=1}^{n} (X_{i}-\\overline{X}_{n})\\sigma_{\\epsilon}^{2} = \\sigma_{\\epsilon}^{2}\\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n}) = 0$$\n    Therefore, the entire cross-product term is zero.\n\n3.  **Third term: $E[(\\widehat{\\beta}_{n} - \\beta)^{2}(\\overline{X}_{n} - \\mu_{X})^{2}]$**\n    Using the expression for $\\widehat{\\beta}_n - \\beta$, this term is:\n    $$E\\left[ \\left(\\frac{\\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n})\\epsilon_{i}}{S_{XX}}\\right)^{2} (\\overline{X}_{n}-\\mu_{X})^{2} \\right]$$\n    Again, we condition on $\\mathbf{X}$:\n    $$E_{\\mathbf{X}}\\left[ (\\overline{X}_{n}-\\mu_{X})^{2} E_{\\epsilon|\\mathbf{X}}\\left[ \\left(\\frac{\\sum_{i=1}^{n}(X_{i}-\\overline{X}_{n})\\epsilon_{i}}{S_{XX}}\\right)^{2} \\right] \\right]$$\n    The inner conditional expectation is:\n    $$E\\left[ \\frac{1}{S_{XX}^{2}} \\sum_{i,j} (X_i-\\overline{X}_n)(X_j-\\overline{X}_n)\\epsilon_i\\epsilon_j \\right] = \\frac{1}{S_{XX}^{2}}\\sum_{i,j} (X_i-\\overline{X}_n)(X_j-\\overline{X}_n)E[\\epsilon_i\\epsilon_j]$$\n    $$= \\frac{\\sigma_{\\epsilon}^{2}}{S_{XX}^{2}} \\sum_{i=1}^{n} (X_{i}-\\overline{X}_{n})^{2} = \\frac{\\sigma_{\\epsilon}^{2}S_{XX}}{S_{XX}^{2}} = \\frac{\\sigma_{\\epsilon}^{2}}{S_{XX}}$$\n    Substituting this back, the third term becomes:\n    $$E_{\\mathbf{X}}\\left[ (\\overline{X}_{n}-\\mu_{X})^{2} \\frac{\\sigma_{\\epsilon}^{2}}{S_{XX}} \\right] = \\sigma_{\\epsilon}^{2} E\\left[ (\\overline{X}_{n}-\\mu_{X})^{2} \\frac{1}{S_{XX}} \\right]$$\n    For i.i.d. normal samples, by Cochran's theorem, the sample mean $\\overline{X}_n$ is independent of the sum of squared deviations $S_{XX}$. Therefore, the expectation of the product is the product of the expectations:\n    $$\\sigma_{\\epsilon}^{2} E\\left[ (\\overline{X}_{n}-\\mu_{X})^{2} \\right] E\\left[ \\frac{1}{S_{XX}} \\right]$$\n    We evaluate these two expectations:\n    a) $E\\left[ (\\overline{X}_{n}-\\mu_{X})^{2} \\right] = \\mathrm{Var}(\\overline{X}_{n}) = \\frac{\\sigma_{X}^{2}}{n}$.\n    b) To find $E[1/S_{XX}]$, we use the fact that $W = S_{XX}/\\sigma_{X}^{2}$ follows a chi-squared distribution with $k=n-1$ degrees of freedom, $W \\sim \\chi_{n-1}^{2}$. We require the expectation of the reciprocal of this variable. The distribution of $1/W$ is an inverse-chi-squared distribution. Its mean is $1/(k-2)$. This expectation is finite only if $k-2>0$, which means $k>2$. Since $k=n-1$, we require $n-1>2$, or $n>3$.\n    So, for $n>3$, $E[1/W] = 1/(n-3)$, which implies $E[1/S_{XX}] = \\frac{1}{\\sigma_{X}^{2}(n-3)}$.\n    Combining these parts, the third term is:\n    $$\\sigma_{\\epsilon}^{2} \\left(\\frac{\\sigma_{X}^{2}}{n}\\right) \\left(\\frac{1}{\\sigma_{X}^{2}(n-3)}\\right) = \\frac{\\sigma_{\\epsilon}^{2}}{n(n-3)} = \\frac{\\sigma_{Y}^{2}(1-\\rho^{2})}{n(n-3)}$$\n\n4.  **Final Assembly**\n    Combining the terms for $\\mathrm{Var}(\\widehat{\\mu}_{n}^{\\mathrm{cv}})$ for $n>3$:\n    $$\\mathrm{Var}(\\widehat{\\mu}_{n}^{\\mathrm{cv}}) = \\frac{\\sigma_{Y}^{2}(1-\\rho^{2})}{n} + 0 + \\frac{\\sigma_{Y}^{2}(1-\\rho^{2})}{n(n-3)}$$\n    $$\\mathrm{Var}(\\widehat{\\mu}_{n}^{\\mathrm{cv}}) = \\frac{\\sigma_{Y}^{2}(1-\\rho^{2})}{n} \\left(1 + \\frac{1}{n-3}\\right)$$\n    $$\\mathrm{Var}(\\widehat{\\mu}_{n}^{\\mathrm{cv}}) = \\frac{\\sigma_{Y}^{2}(1-\\rho^{2})}{n} \\left(\\frac{n-3+1}{n-3}\\right)$$\n    $$\\mathrm{Var}(\\widehat{\\mu}_{n}^{\\mathrm{cv}}) = \\frac{\\sigma_{Y}^{2}(1-\\rho^{2})(n-2)}{n(n-3)}$$\nThis is the exact finite-sample variance for $n>3$. The factor $\\frac{n-2}{n-3}$ represents the penalty incurred for estimating the optimal control coefficient $\\beta$ from the sample, compared to the ideal variance $\\frac{\\sigma_Y^2(1-\\rho^2)}{n}$ that would be achieved if $\\beta$ were known. For $n \\le 3$, the variance is infinite.", "answer": "$$\\boxed{\\frac{\\sigma_{Y}^{2}(1-\\rho^{2})(n-2)}{n(n-3)}}$$", "id": "3331327"}]}