{"hands_on_practices": [{"introduction": "Understanding a new mathematical measure often begins with a hands-on calculation in a simple, controlled setting. This first practice problem [@problem_id:3303337] provides exactly that opportunity by asking you to compute the star discrepancy $D_N^*$ for one of the most basic one-dimensional point sets: $N$ points equispaced on the unit interval. By working through this derivation, you will gain a concrete feel for how discrepancy quantifies deviations from uniformity and explore the fundamental concept of optimality in low-discrepancy sequences.", "problem": "Let $\\{x_{n}\\}_{n=1}^{N} \\subset [0,1]$ be a point set used in Quasi-Monte Carlo (QMC) methods, and recall the star discrepancy in $s$ dimensions is defined by\n$$\nD_{N}^{*}(\\{x_{n}\\}_{n=1}^{N}) \\;=\\; \\sup_{t \\in [0,1]^{s}} \\left| \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}\\{x_{n} \\in [0,t)\\} \\;-\\; \\prod_{j=1}^{s} t_{j} \\right|,\n$$\nwhere $[0,t) = \\prod_{j=1}^{s} [0,t_{j})$ and $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function. Specialize to the one-dimensional case $s=1$, so that\n$$\nD_{N}^{*}(\\{x_{n}\\}_{n=1}^{N}) \\;=\\; \\sup_{t \\in [0,1]} \\left| \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}\\{x_{n}  t\\} \\;-\\; t \\right|.\n$$\nConsider the equispaced point set $x_{n} = \\frac{n}{N}$ for $n=1,2,\\dots,N$. Starting from the above definition, derive an exact closed-form expression for $D_{N}^{*}$ as a function of $N$. Then, interpret your result in terms of one-dimensional optimality: compare the value you obtained to the minimal possible star discrepancy achievable by any $N$-point set on $[0,1]$, and state whether the equispaced endpoints design is optimal in one dimension.\n\nExpress your final answer for $D_{N}^{*}$ as a closed-form function of $N$. No rounding is required.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. It presents a standard exercise in the theory of uniform distribution and Quasi-Monte Carlo methods.\n\nWe are asked to derive a closed-form expression for the star discrepancy $D_N^*$ of the one-dimensional point set $\\{x_n\\}_{n=1}^N$ where $x_n = \\frac{n}{N}$ for $n=1, 2, \\dots, N$. The one-dimensional star discrepancy is defined as:\n$$\nD_{N}^{*}(\\{x_{n}\\}_{n=1}^{N}) = \\sup_{t \\in [0,1]} \\left| \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}\\{x_{n}  t\\} - t \\right|\n$$\nLet us define the function whose supremum we seek:\n$$\nf(t) = E_N(t) - t = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}\\{x_{n}  t\\} - t\n$$\nHere, $E_N(t)$ is the empirical distribution function of the point set. The points are $x_1 = \\frac{1}{N}, x_2 = \\frac{2}{N}, \\dots, x_N = \\frac{N}{N} = 1$. These points partition the interval $[0,1]$ into subintervals. We will analyze the function $f(t)$ on these subintervals.\n\nThe domain $[0,1]$ can be written as the union of the point $\\{0\\}$ and the intervals $(\\frac{k}{N}, \\frac{k+1}{N}]$ for $k=0, 1, \\dots, N-1$.\n\nCase 1: $t=0$.\nAt $t=0$, no points $x_n = \\frac{n}{N}$ (which are all positive) are strictly less than $0$. Thus, the sum is $0$.\n$$\nf(0) = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}\\{x_{n}  0\\} - 0 = \\frac{0}{N} - 0 = 0\n$$\n\nCase 2: $t \\in (0, \\frac{1}{N}]$.\nFor any $t$ in this interval, there are no points $x_n = \\frac{n}{N}$ that are strictly less than $t$. The smallest point is $x_1 = \\frac{1}{N}$, and for any $t \\in (0, \\frac{1}{N}]$, we have $x_n \\ge \\frac{1}{N} \\ge t$. Thus, $\\mathbf{1}\\{x_n  t\\} = 0$ for all $n$.\nThe empirical distribution function is $E_N(t) = 0$.\nThe function $f(t)$ becomes:\n$$\nf(t) = 0 - t = -t\n$$\nWe are interested in $|f(t)| = t$. On the interval $(0, \\frac{1}{N}]$, this function is maximized at $t=\\frac{1}{N}$, where $|f(\\frac{1}{N})| = \\frac{1}{N}$.\n\nCase 3: $t \\in (\\frac{k}{N}, \\frac{k+1}{N}]$ for $k=1, 2, \\dots, N-1$.\nFor any $t$ in such an interval, we need to count how many points $x_n = \\frac{n}{N}$ are strictly less than $t$.\nThe condition $x_n  t$ is $\\frac{n}{N}  t$.\nSince $t > \\frac{k}{N}$, the points $x_1=\\frac{1}{N}, x_2=\\frac{2}{N}, \\dots, x_k=\\frac{k}{N}$ are all strictly less than $t$. This accounts for $k$ points.\nSince $t \\le \\frac{k+1}{N}$, for any $n \\ge k+1$, we have $x_n = \\frac{n}{N} \\ge \\frac{k+1}{N} \\ge t$. So these points are not strictly less than $t$.\nTherefore, for any $t \\in (\\frac{k}{N}, \\frac{k+1}{N}]$, there are exactly $k$ points satisfying $x_n  t$.\nThe empirical distribution function is $E_N(t) = \\frac{k}{N}$.\nThe function $f(t)$ on this interval is:\n$$\nf(t) = \\frac{k}{N} - t\n$$\nThis is a linear function of $t$ with a slope of $-1$. To find the supremum of its absolute value on the interval $(\\frac{k}{N}, \\frac{k+1}{N}]$, we examine the endpoints.\nAs $t$ approaches the left endpoint from the right, $t \\to (\\frac{k}{N})^{+}$:\n$$\n\\lim_{t \\to (\\frac{k}{N})^{+}} f(t) = \\frac{k}{N} - \\frac{k}{N} = 0\n$$\nAt the right endpoint, $t = \\frac{k+1}{N}$:\n$$\nf\\left(\\frac{k+1}{N}\\right) = \\frac{k}{N} - \\frac{k+1}{N} = -\\frac{1}{N}\n$$\nOn the interval $(\\frac{k}{N}, \\frac{k+1}{N}]$, $f(t)$ ranges from (but not including) $0$ down to $-\\frac{1}{N}$. Thus, $|f(t)|$ ranges from $0$ up to $\\frac{1}{N}$. The supremum of $|f(t)|$ on this interval is $\\frac{1}{N}$.\n\nCombining all cases, for any $t \\in (0,1]$, there is a $k \\in \\{0, \\dots, N-1\\}$ such that $t \\in (\\frac{k}{N}, \\frac{k+1}{N}]$. On each of these intervals, the supremum of $|f(t)|$ is $\\frac{1}{N}$. At $t=0$, $|f(0)|=0$.\nThe overall supremum over the entire interval $[0,1]$ is therefore the maximum of these values.\n$$\nD_N^* = \\sup_{t \\in [0,1]} |f(t)| = \\max\\left( |f(0)|, \\sup_{t \\in (0,1]} |f(t)| \\right) = \\max\\left(0, \\frac{1}{N}\\right) = \\frac{1}{N}\n$$\nThe closed-form expression for the star discrepancy of this point set is $D_N^* = \\frac{1}{N}$.\n\nFor the second part of the problem, we must interpret this result. In one dimension, it is a classical result that for any set of $N$ points $\\{y_n\\}_{n=1}^N \\subset [0,1]$, the star discrepancy is bounded from below:\n$$\nD_N^*(\\{y_n\\}_{n=1}^N) \\ge \\frac{1}{2N}\n$$\nThis lower bound is sharp. It is achieved by the point set $y_n = \\frac{2n-1}{2N}$ for $n=1, \\dots, N$. This set consists of the midpoints of the intervals $[\\frac{n-1}{N}, \\frac{n}{N}]$. The star discrepancy for this set is exactly $\\frac{1}{2N}$. A point set that achieves the minimal possible discrepancy is called optimal.\nThe point set given in the problem, $x_n = \\frac{n}{N}$, yields a discrepancy of $D_N^* = \\frac{1}{N}$.\nComparing our result to the optimal value, we have $\\frac{1}{N}$ versus $\\frac{1}{2N}$. For any $N > 1$, we see that $\\frac{1}{N} > \\frac{1}{2N}$.\nTherefore, the equispaced endpoint design $x_n = \\frac{n}{N}$ is not optimal in one dimension, as its star discrepancy is twice the minimum possible value.", "answer": "$$\n\\boxed{\\frac{1}{N}}\n$$", "id": "3303337"}, {"introduction": "While star discrepancy is often introduced as a geometric measure of uniformity, its significance extends deep into the realm of probability theory. This exercise [@problem_id:3303289] guides you through establishing a crucial identity: for random points drawn from a uniform distribution, the one-dimensional star discrepancy is almost surely equal to the celebrated Kolmogorov-Smirnov statistic. This powerful connection allows us to leverage tools from statistical learning theory, such as the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality, to obtain non-asymptotic probability bounds on the discrepancy.", "problem": "Consider independent and identically distributed random variables $X_{1},\\dots,X_{N}$ with common distribution $\\mathrm{Uniform}(0,1)$. Let the empirical distribution function be $F_{N}(t) = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\{X_{i} \\le t\\}$ for $t \\in \\mathbb{R}$, and let $F(t)=t$ for $t \\in [0,1]$ denote the true cumulative distribution function. Define the one-dimensional star-discrepancy of the point set $\\{X_{i}\\}_{i=1}^{N}$ by\n$$\nD_{N}^{*} \\;=\\; \\sup_{t \\in [0,1]}\\left|\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\{X_{i} \\in [0,t)\\} \\;-\\; t\\right|.\n$$\nUsing only the definitions of star-discrepancy, empirical distribution functions, and distribution functions, first relate $D_{N}^{*}$ to the Kolmogorov–Smirnov statistic (Kolmogorov–Smirnov (KS)) under the $\\mathrm{Uniform}(0,1)$ model by establishing an almost sure identity between $D_{N}^{*}$ and a supremum of $|F_{N}(t)-F(t)|$. Then, using the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality for empirical distribution functions, derive an explicit closed-form upper bound for $\\mathbb{P}\\!\\left(D_{N}^{*}  \\varepsilon\\right)$ as a function of $N$ and $\\varepsilon0$, with fully specified constants and no unspecified asymptotic notation. Provide your final answer as a single closed-form analytic expression for this probability bound. No numerical rounding is required.", "solution": "The problem asks for an explicit closed-form upper bound for the probability $\\mathbb{P}\\!\\left(D_{N}^{*}  \\varepsilon\\right)$. The solution requires two main steps: first, relating the star-discrepancy $D_{N}^{*}$ to the Kolmogorov-Smirnov (KS) statistic, and second, applying the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality.\n\nLet's begin by precisely defining the quantities involved.\nWe are given $N$ independent and identically distributed (i.i.d.) random variables $X_{1}, \\dots, X_{N}$ from the $\\mathrm{Uniform}(0,1)$ distribution. The cumulative distribution function (CDF) of this distribution is $F(t) = t$ for $t \\in [0,1]$, $F(t)=0$ for $t0$, and $F(t)=1$ for $t1$.\n\nThe empirical distribution function (EDF) is given by\n$$\nF_{N}(t) = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\{X_{i} \\le t\\}\n$$\nfor $t \\in \\mathbb{R}$. For a given sample $\\{X_i\\}_{i=1}^N$, $F_N(t)$ is a right-continuous step function.\n\nThe one-dimensional star-discrepancy is defined as\n$$\nD_{N}^{*} = \\sup_{t \\in [0,1]}\\left|\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\{X_{i} \\in [0,t)\\} - t\\right|.\n$$\nSince the $X_i$ are drawn from a continuous distribution, the probability of any $X_i$ being exactly $0$ is zero, so $\\mathbf{1}\\{X_{i} \\in [0,t)\\}$ is almost surely equivalent to $\\mathbf{1}\\{X_{i}  t\\}$. Let's define a left-continuous version of the EDF as\n$$\nG_{N}(t) = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\{X_{i}  t\\}.\n$$\nWith this definition, the star-discrepancy is almost surely given by $D_{N}^{*} = \\sup_{t \\in [0,1]}|G_{N}(t) - t|$.\n\nThe two-sided Kolmogorov-Smirnov (KS) statistic is defined as\n$$\nD_{N} = \\sup_{t \\in \\mathbb{R}} |F_{N}(t) - F(t)|.\n$$\nFor our uniform case, since $F_N(t)$ and $F(t)$ are both $0$ for $t0$ and both $1$ for $t1$, the supremum is effectively taken over the interval $[0,1]$:\n$$\nD_{N} = \\sup_{t \\in [0,1]} |F_{N}(t) - F(t)| = \\sup_{t \\in [0,1]} |F_{N}(t) - t|.\n$$\n\nNow, we establish the almost sure identity between $D_{N}^{*}$ and $D_{N}$. The functions $F_N(t)$ and $G_N(t)$ are step functions that change values only at the sample points $X_i$. Let $X_{(1)}  X_{(2)}  \\dots  X_{(N)}$ be the order statistics of the sample. Since the underlying distribution is continuous, the probability of any two $X_i$ being equal is $0$, so we can assume the strict inequalities almost surely.\n\nThe functions $f(t) = F_N(t) - t$ and $g(t) = G_N(t) - t$ are piecewise linear with a slope of $-1$ on the intervals $(X_{(j)}, X_{(j+1)})$. The supremum of $|f(t)|$ and $|g(t)|$ over $t \\in [0,1]$ must therefore be attained at the endpoints of these intervals, considering also the limits at the jump points.\nThe points where the supremum of $|F_N(t)-t|$ are achieved are among the values $\\{X_{(j)}\\}_{j=1}^N$ and the points approached from the left, $\\{X_{(j)}^{-}\\}_{j=1}^N$.\nFor any $j \\in \\{1, \\dots, N\\}$:\nAt a jump point $t = X_{(j)}$, we have $F_N(X_{(j)}) = \\frac{j}{N}$.\nThe value of the function is $|F_N(X_{(j)}) - X_{(j)}| = \\left|\\frac{j}{N} - X_{(j)}\\right|$.\nAt the limit from the left, $t \\to X_{(j)}^{-}$, we have $\\lim_{t \\to X_{(j)}^{-}} F_N(t) = F_N(X_{(j)}^{-}) = \\frac{j-1}{N}$.\nThe value of the function is $|F_N(X_{(j)}^{-}) - X_{(j)}| = \\left|\\frac{j-1}{N} - X_{(j)}\\right|$.\nThus, the KS statistic is given by the maximum of these values over all $j$:\n$$\nD_{N} = \\sup_{t \\in [0,1]} |F_{N}(t) - t| = \\max_{j \\in \\{1,\\dots,N\\}} \\max\\left\\{ \\left|\\frac{j}{N} - X_{(j)}\\right|, \\left|\\frac{j-1}{N} - X_{(j)}\\right| \\right\\}.\n$$\n\nNow we analyze $D_N^* = \\sup_{t \\in [0,1]} |G_N(t) - t|$. The function $G_N(t)$ is left-continuous. At a jump point $t=X_{(j)}$, we have $G_N(X_{(j)}) = \\frac{j-1}{N}$. The value of the function is $|G_N(X_{(j)}) - X_{(j)}| = \\left|\\frac{j-1}{N} - X_{(j)}\\right|$.\nAt the limit from the right, $t \\to X_{(j)}^{+}$, we have $\\lim_{t \\to X_{(j)}^{+}} G_N(t) = G_N(X_{(j)}^{+}) = \\frac{j}{N}$. The value approached by the function is $|G_N(X_{(j)}^{+}) - X_{(j)}| = \\left|\\frac{j}{N} - X_{(j)}\\right|$.\nThus, the star-discrepancy is given by the maximum of these values over all $j$:\n$$\nD_{N}^{*} = \\sup_{t \\in [0,1]} |G_{N}(t) - t| = \\max_{j \\in \\{1,\\dots,N\\}} \\max\\left\\{ \\left|\\frac{j}{N} - X_{(j)}\\right|, \\left|\\frac{j-1}{N} - X_{(j)}\\right| \\right\\}.\n$$\nComparing the expressions for $D_N$ and $D_N^*$, we see that they are identical. This identity holds almost surely because it relies on the sample points being distinct.\n$$\nD_{N}^{*} = D_{N} = \\sup_{t \\in [0,1]} |F_{N}(t) - t| \\quad (\\text{a.s.})\n$$\nThis establishes the required relationship between $D_N^*$ and the supremum of $|F_N(t)-F(t)|$.\n\nThe second step is to use the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality. The DKW inequality provides a non-asymptotic bound on the probability that the KS statistic exceeds a certain value $\\varepsilon > 0$. It states that for any i.i.d. random variables,\n$$\n\\mathbb{P}\\left(\\sup_{t \\in \\mathbb{R}} |F_N(t) - F(t)|  \\varepsilon\\right) \\le 2\\exp(-2N\\varepsilon^2).\n$$\nThis inequality holds for any sample size $N$ and any underlying distribution $F$.\n\nWe have already shown that the KS statistic $D_N$ is equal to $\\sup_{t \\in \\mathbb{R}} |F_N(t)-F(t)|$ in our case. Using the almost sure identity $D_N^* = D_N$, we can write:\n$$\n\\mathbb{P}(D_N^*  \\varepsilon) = \\mathbb{P}(D_N  \\varepsilon) = \\mathbb{P}\\left(\\sup_{t \\in \\mathbb{R}} |F_N(t) - F(t)|  \\varepsilon\\right).\n$$\nApplying the DKW inequality directly yields the desired upper bound:\n$$\n\\mathbb{P}(D_N^*  \\varepsilon) \\le 2\\exp(-2N\\varepsilon^2).\n$$\nThis expression is a closed-form upper bound for $\\mathbb{P}(D_N^*  \\varepsilon)$ as a function of $N$ and $\\varepsilon$. It has fully specified constants and contains no unspecified asymptotic notation, as required by the problem statement.", "answer": "$$\n\\boxed{2 \\exp(-2 N \\varepsilon^2)}\n$$", "id": "3303289"}, {"introduction": "The theoretical link between discrepancy and probabilistic bounds finds a powerful application in assessing the error of Monte Carlo integration. This final hands-on problem [@problem_id:3303333] challenges you to put theory into practice by developing and implementing a method for constructing rigorous error bars for integrals of discontinuous functions. By leveraging the connection to the empirical distribution function and the DKW inequality, you will create a tool that remains valid even when classical QMC error bounds based on Hardy-Krause variation become infinite, demonstrating the practical power of discrepancy measures in modern computational science.", "problem": "You are given the task of constructing discrepancy-aware error bars for Monte Carlo estimates of integrals of discontinuous functions in a way that remains valid even when the Hardy–Krause variation is infinite. Consider the integration of a bounded measurable function $f:[0,1]^d \\to \\mathbb{R}$ under the uniform probability measure on $[0,1]^d$, so that the target integral is the expectation $\\mathbb{E}[f(X)]$ with $X \\sim \\text{Uniform}([0,1]^d)$. The classical Koksma–Hlawka inequality uses Hardy–Krause variation to bound integration error for quasi-Monte Carlo methods, but it becomes vacuous when $V_{\\mathrm{HK}}(f)=\\infty$ for discontinuous $f$ in dimensions $d \\geq 2$. Your goal is to derive and implement an alternative approach that uses discrepancy of the pushforward sample values $Y_i = f(X_i)$ and yields finite, rigorous error bars without relying on Hardy–Krause variation.\n\nFrom first principles, proceed as follows. Start from the definition of the empirical cumulative distribution function (ECDF) of the pushforward random variable $Y = f(X)$, denoted $F_n(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{Y_i \\le t\\}$ with $Y_i = f(X_i)$ and $X_i$ independent and identically distributed draws from the uniform distribution on $[0,1]^d$. Use well-tested facts about almost sure uniform convergence of the ECDF to the true cumulative distribution function $F_Y(t)$, and rigorously relate the integration error $\\left|\\frac{1}{n}\\sum_{i=1}^n f(X_i) - \\mathbb{E}[f(X)]\\right|$ to a bound expressed using the Kolmogorov–Smirnov discrepancy for the ECDF of $Y_i$. Derive high-confidence error bars for $\\mathbb{E}[f(X)]$ solely in terms of:\n- the sample values $Y_1,\\dots,Y_n$,\n- the fact that $f$ is bounded with known bounds $a \\le f(x) \\le b$ for all $x$ in $[0,1]^d$,\n- and standard, distribution-free inequalities for the ECDF.\n\nYou must implement the resulting error bars in a program and demonstrate their behavior on discontinuous functions $f$ for which $V_{\\mathrm{HK}}(f)=\\infty$ in dimension $d \\ge 2$. In all test cases below, $f$ is an indicator function of a geometric region, so $Y = f(X) \\in \\{0,1\\}$ and $(a,b) = (0,1)$. Use independent and identically distributed sampling for $X_i$ with a fixed random seed, and compute:\n- the Monte Carlo estimate $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n Y_i$,\n- a rigorous two-sided confidence interval $[\\mathrm{L}, \\mathrm{U}]$ for $\\mathbb{E}[f(X)]$ at confidence level $1 - \\alpha$ based on the discrepancy of the ECDF of $Y_i$,\n- the true integral $\\mu^\\star$ (known analytically for the chosen regions),\n- and a boolean indicating whether $\\mu^\\star \\in [\\mathrm{L},\\mathrm{U}]$.\n\nFundamental base to be used in your derivation:\n- The definition of the empirical cumulative distribution function $F_n$ and the true cumulative distribution function $F_Y$ for a real-valued random variable $Y$.\n- The identity expressing the expectation of a bounded random variable in terms of its distribution function via tail integrals.\n- The fact that the empirical cumulative distribution function converges uniformly to the true cumulative distribution function and admits nonasymptotic distribution-free exponential tail bounds.\n\nYou are not allowed to rely on any bound derived from Hardy–Krause variation, and you must ensure the resulting error bars are valid when $V_{\\mathrm{HK}}(f) = \\infty$. Your program must compute these error bars using only the discrepancy of the ECDF of $Y_i$ and produce the final results as specified below.\n\nTest suite:\nFor each test case, generate $n$ independent and identically distributed points $X_i$ uniformly on $[0,1]^d$ using a fixed seed, evaluate $Y_i = f(X_i)$, and compute the requested outputs.\n\n- Test case $1$ (happy path, moderate sample size):\n  - Dimension: $d = 2$.\n  - Function: $f(x) = \\mathbf{1}\\{\\|x - c\\|_2 \\le r\\}$ where $c = (0.5, 0.5)$ and $r = 0.3$.\n  - This $f$ is discontinuous on a curved boundary in $d \\ge 2$, so $V_{\\mathrm{HK}}(f) = \\infty$.\n  - Analytical integral: the area of a disk of radius $r$ centered in the unit square, equal to $\\mu^\\star = \\pi r^2$ since the disk lies entirely inside $[0,1]^2$ for $r = 0.3$.\n  - Sample size: $n = 4096$.\n  - Confidence parameter: $\\alpha = 0.05$.\n  - Seed: $12345$.\n\n- Test case $2$ (larger dimension, more samples):\n  - Dimension: $d = 3$.\n  - Function: $f(x) = \\mathbf{1}\\{\\|x - c\\|_2 \\le r\\}$ where $c = (0.5, 0.5, 0.5)$ and $r = 0.3$.\n  - Discontinuity on a curved boundary implies $V_{\\mathrm{HK}}(f) = \\infty$.\n  - Analytical integral: the volume of a ball of radius $r$ centered in the unit cube, equal to $\\mu^\\star = \\frac{4}{3}\\pi r^3$ since the ball lies entirely inside $[0,1]^3$ for $r = 0.3$.\n  - Sample size: $n = 8192$.\n  - Confidence parameter: $\\alpha = 0.01$.\n  - Seed: $67890$.\n\n- Test case $3$ (edge case, small sample size and a different discontinuous geometry):\n  - Dimension: $d = 2$.\n  - Function: $f(x) = \\mathbf{1}\\{r_1 \\le \\|x - c\\|_2 \\le r_2\\}$ (an annulus), where $c = (0.5, 0.5)$, $r_1 = 0.2$, and $r_2 = 0.4$.\n  - Discontinuity on two curved boundaries implies $V_{\\mathrm{HK}}(f) = \\infty$.\n  - Analytical integral: the area of the annulus, equal to $\\mu^\\star = \\pi(r_2^2 - r_1^2)$ since the annulus lies entirely inside $[0,1]^2$ for $r_2 = 0.4$ and $c = (0.5, 0.5)$.\n  - Sample size: $n = 64$.\n  - Confidence parameter: $\\alpha = 0.2$.\n  - Seed: $24680$.\n\nOutput specification:\nYour program should produce a single line of output containing a list of results, one per test case, with each result structured as a list $[\\mathrm{L},\\mathrm{U},\\mu^\\star,\\mathrm{inside}]$, where:\n- $\\mathrm{L}$ and $\\mathrm{U}$ are floats representing the lower and upper ends of the discrepancy-aware confidence interval for $\\mathbb{E}[f(X)]$,\n- $\\mu^\\star$ is a float representing the analytical true integral,\n- $\\mathrm{inside}$ is a boolean indicating whether $\\mu^\\star$ lies within $[\\mathrm{L},\\mathrm{U}]$.\n\nFor example, the final output should look like $[[\\mathrm{L}_1,\\mathrm{U}_1,\\mu^\\star_1,\\mathrm{inside}_1],[\\mathrm{L}_2,\\mathrm{U}_2,\\mu^\\star_2,\\mathrm{inside}_2],[\\mathrm{L}_3,\\mathrm{U}_3,\\mu^\\star_3,\\mathrm{inside}_3]]$ with the actual numerical values computed by your program.", "solution": "The problem requires the derivation and implementation of a method to construct rigorous confidence intervals for Monte Carlo estimates of integrals for discontinuous, bounded functions. This is to be achieved without relying on the Hardy–Krause variation, which is infinite for the functions under consideration, but instead by leveraging the properties of the empirical cumulative distribution function (ECDF) of the pushforward samples.\n\nLet the integral of interest be $I = \\int_{[0,1]^d} f(x) dx$, where $f: [0,1]^d \\to \\mathbb{R}$ is a bounded, measurable function. Let $X$ be a random variable uniformly distributed on $[0,1]^d$. The integral can be expressed as the expectation $I = \\mu^\\star = \\mathbb{E}[f(X)]$. The Monte Carlo method estimates this integral using the sample mean of $n$ independent and identically distributed (i.i.d.) evaluations of $f$. Let $X_1, \\dots, X_n$ be i.i.d. draws from $\\text{Uniform}([0,1]^d)$, and let $Y_i = f(X_i)$ be the corresponding pushforward samples. The Monte Carlo estimate is $\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n Y_i$.\n\nThe function $f$ is bounded, so there exist constants $a$ and $b$ such that $a \\le f(x) \\le b$ for all $x \\in [0,1]^d$. Consequently, the random variable $Y = f(X)$ has its support contained within the interval $[a, b]$.\n\nThe derivation proceeds from the fundamental relationship between the expectation of a random variable and its cumulative distribution function (CDF). For any random variable $Y$ with support in $[a,b]$, its expectation can be written as:\n$$ \\mu^\\star = \\mathbb{E}[Y] = a + \\int_a^b (1 - F_Y(t)) dt $$\nwhere $F_Y(t) = P(Y \\le t)$ is the true CDF of $Y$. This identity is derived from integration by parts of $\\mathbb{E}[Y] = \\int_a^b t dF_Y(t)$ and holds for any random variable with bounded support, regardless of whether its CDF is continuous.\n\nThe Monte Carlo estimate $\\hat{\\mu}$ is the expectation with respect to the empirical measure of the sample $\\{Y_1, \\dots, Y_n\\}$. The empirical cumulative distribution function (ECDF) is defined as $F_n(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{Y_i \\le t\\}$. The same identity relating expectation to the CDF can be applied to the empirical measure, yielding an exact expression for the sample mean:\n$$ \\hat{\\mu} = \\mathbb{E}_{F_n}[Y] = a + \\int_a^b (1 - F_n(t)) dt $$\n\nThe error of the Monte Carlo estimate is the difference between $\\hat{\\mu}$ and $\\mu^\\star$. Subtracting the two expectation identities gives:\n$$ \\hat{\\mu} - \\mu^\\star = \\left(a + \\int_a^b (1 - F_n(t)) dt\\right) - \\left(a + \\int_a^b (1 - F_Y(t)) dt\\right) = \\int_a^b (F_Y(t) - F_n(t)) dt $$\nTo obtain a bound on the error, we take the absolute value:\n$$ |\\hat{\\mu} - \\mu^\\star| = \\left| \\int_a^b (F_Y(t) - F_n(t)) dt \\right| \\le \\int_a^b |F_Y(t) - F_n(t)| dt $$\nThe term $|F_Y(t) - F_n(t)|$ is the pointwise difference between the true CDF and the ECDF. This difference is uniformly bounded by the Kolmogorov–Smirnov statistic, $D_n = \\sup_{t \\in \\mathbb{R}} |F_Y(t) - F_n(t)|$.\nApplying this uniform bound to the integral, we get:\n$$ |\\hat{\\mu} - \\mu^\\star| \\le \\int_a^b D_n dt = D_n \\int_a^b dt = D_n (b-a) $$\nThis inequality connects the integration error directly to the discrepancy of the one-dimensional pushforward samples, avoiding any dependence on the multi-dimensional structure of $f$ or its Hardy–Krause variation.\n\nTo make this bound useful, we require a high-probability bound on the random variable $D_n$. The Dvoretzky–Kiefer–Wolfowitz (DKW) inequality provides a non-asymptotic, distribution-free bound on the probability that $D_n$ exceeds some value $\\epsilon$. For any $\\epsilon > 0$, the inequality states:\n$$ P(D_n > \\epsilon) \\le 2e^{-2n\\epsilon^2} $$\nWe want to construct a confidence interval for $\\mu^\\star$ with a confidence level of $1-\\alpha$. We can set the upper bound on the probability of a large deviation to $\\alpha$:\n$$ \\alpha = 2e^{-2n\\epsilon^2} $$\nSolving for $\\epsilon$ gives the critical value, which we denote $\\epsilon_{n, \\alpha}$:\n$$ \\ln\\left(\\frac{\\alpha}{2}\\right) = -2n\\epsilon^2 \\implies \\epsilon_{n, \\alpha} = \\sqrt{\\frac{1}{2n}\\ln\\left(\\frac{2}{\\alpha}\\right)} $$\nThe DKW inequality implies that with probability at least $1-\\alpha$, the event $D_n \\le \\epsilon_{n, \\alpha}$ occurs.\n\nCombining these results, we can state that with probability at least $1-\\alpha$:\n$$ |\\mu^\\star - \\hat{\\mu}| \\le (b-a) D_n \\le (b-a) \\epsilon_{n, \\alpha} $$\nThis inequality defines a symmetric confidence interval for $\\mu^\\star$ around the estimate $\\hat{\\mu}$. The lower bound $\\mathrm{L}$ and upper bound $\\mathrm{U}$ of this confidence interval are:\n$$ \\mathrm{L} = \\hat{\\mu} - (b-a)\\sqrt{\\frac{1}{2n}\\ln\\left(\\frac{2}{\\alpha}\\right)} $$\n$$ \\mathrm{U} = \\hat{\\mu} + (b-a)\\sqrt{\\frac{1}{2n}\\ln\\left(\\frac{2}{\\alpha}\\right)} $$\nThis confidence interval is valid for any bounded function $f$, including discontinuous ones for which the Hardy–Krause variation $V_{\\mathrm{HK}}(f)$ is infinite. For the test cases provided, $f$ is an indicator function, so its output $Y$ is a Bernoulli random variable with values in $\\{0, 1\\}$. Thus, the bounds are $a=0$ and $b=1$, and the term $(b-a)$ simplifies to $1$. The sample mean $\\hat{\\mu}$ is the proportion of samples for which $f(X_i)=1$. This rigorous, distribution-free method provides a practical way to assess Monte Carlo integration error in challenging scenarios.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes discrepancy-aware confidence intervals for Monte Carlo estimates\n    of integrals for several discontinuous functions.\n    \"\"\"\n    test_cases = [\n        {\n            \"d\": 2,\n            \"f_params\": {\"c\": np.array([0.5, 0.5]), \"r\": 0.3},\n            \"f_type\": \"disk\",\n            \"mu_star\": np.pi * 0.3**2,\n            \"n\": 4096,\n            \"alpha\": 0.05,\n            \"seed\": 12345,\n        },\n        {\n            \"d\": 3,\n            \"f_params\": {\"c\": np.array([0.5, 0.5, 0.5]), \"r\": 0.3},\n            \"f_type\": \"ball\",\n            \"mu_star\": (4/3) * np.pi * 0.3**3,\n            \"n\": 8192,\n            \"alpha\": 0.01,\n            \"seed\": 67890,\n        },\n        {\n            \"d\": 2,\n            \"f_params\": {\"c\": np.array([0.5, 0.5]), \"r1\": 0.2, \"r2\": 0.4},\n            \"f_type\": \"annulus\",\n            \"mu_star\": np.pi * (0.4**2 - 0.2**2),\n            \"n\": 64,\n            \"alpha\": 0.2,\n            \"seed\": 24680,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        d = case[\"d\"]\n        n = case[\"n\"]\n        alpha = case[\"alpha\"]\n        seed = case[\"seed\"]\n        f_params = case[\"f_params\"]\n        f_type = case[\"f_type\"]\n        mu_star = case[\"mu_star\"]\n\n        # Set the random seed for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate n i.i.d. points in the d-dimensional unit cube.\n        X = rng.random((n, d))\n        \n        # Evaluate the function f(x) on the sample points to get Y_i.\n        # Since f is an indicator function, its bounds are a=0, b=1.\n        if f_type == \"disk\" or f_type == \"ball\":\n            c = f_params[\"c\"]\n            r = f_params[\"r\"]\n            # Compute squared Euclidean distance from the center c.\n            dist_sq = np.sum((X - c)**2, axis=1)\n            Y = (dist_sq = r**2).astype(float)\n        elif f_type == \"annulus\":\n            c = f_params[\"c\"]\n            r1 = f_params[\"r1\"]\n            r2 = f_params[\"r2\"]\n            dist_sq = np.sum((X - c)**2, axis=1)\n            Y = ((dist_sq = r1**2)  (dist_sq = r2**2)).astype(float)\n\n        # Calculate the Monte Carlo estimate (sample mean).\n        mu_hat = np.mean(Y)\n\n        # The function is bounded with a=0 and b=1.\n        a, b = 0.0, 1.0\n\n        # Calculate the error term for the confidence interval based on the DKW inequality.\n        # delta = (b-a) * sqrt( (1/(2n)) * ln(2/alpha) )\n        delta = (b - a) * np.sqrt((1.0 / (2.0 * n)) * np.log(2.0 / alpha))\n\n        # Compute the lower and upper bounds of the confidence interval.\n        L = mu_hat - delta\n        U = mu_hat + delta\n\n        # Check if the true integral value lies within the computed interval.\n        inside = (L = mu_star = U)\n\n        # Append the results for this test case.\n        results.append([L, U, mu_star, inside])\n    \n    # Custom string conversion to match required output format.\n    def format_results(data):\n        outer_list = []\n        for sublist in data:\n            inner_list = []\n            for item in sublist:\n                if isinstance(item, bool):\n                    inner_list.append(str(item).lower())\n                else:\n                    inner_list.append(f\"{item}\") \n            outer_list.append(f\"[{','.join(inner_list)}]\")\n        return f\"[{','.join(outer_list)}]\"\n    \n    # Using the native `str()` representation of the list is closer to the expected format.\n    print(results)\n\nsolve()\n```", "id": "3303333"}]}