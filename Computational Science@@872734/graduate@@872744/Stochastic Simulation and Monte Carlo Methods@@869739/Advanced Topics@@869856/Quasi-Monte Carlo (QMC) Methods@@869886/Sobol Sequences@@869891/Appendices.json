{"hands_on_practices": [{"introduction": "To truly grasp the power and elegance of Sobol' sequences, it is essential to understand their construction from first principles. This exercise demystifies the process by guiding you through the creation of a two-dimensional Sobol' sequence, starting from the foundational elements of primitive polynomials and direction numbers. By performing the explicit bitwise manipulations involving Gray codes and exclusive-or (XOR) operations, you will build a tangible connection between the abstract algebraic framework and the geometric distribution of the resulting points in the unit square. This foundational practice is key to appreciating how these sequences achieve their remarkable uniformity.", "problem": "Consider the construction of a two-dimensional Sobol' sequence in base $2$ from first principles. The construction uses the following ingredients.\n\n- Direction numbers for each coordinate (dimension) are generated from a primitive polynomial over the finite field $\\mathbb{F}_2$ and a set of initial odd integers. For a dimension with a degree-$m$ primitive polynomial $p(x) = x^m + a_1 x^{m-1} + \\cdots + a_{m-1} x + 1$ with $a_k \\in \\{0,1\\}$, one specifies odd integers $m_1,\\dots,m_m$ with $1 \\leq m_j  2^j$, and defines the direction numbers $v_j = m_j / 2^j$ for $j \\leq m$. For $j  m$, the direction numbers are defined recursively by bitwise exclusive-or (XOR), using the recurrence on dyadic rationals\n$$\nv_j \\;=\\; \\bigoplus_{k=1}^{m-1} a_k \\, 2^{-k} v_{j-k} \\;\\oplus\\; 2^{-m} v_{j-m},\n$$\nwhere $\\oplus$ denotes exclusive-or (XOR) applied bit-by-bit to the binary expansions of dyadic rationals.\n\n- Given direction numbers $\\{v_j^{(d)}\\}_{j \\geq 1}$ for dimension $d \\in \\{1,2\\}$, the $n$-th coordinate in dimension $d$ is obtained from the Gray code of the index $n$, namely $G(n) = n \\oplus (n \\gg 1)$, where $\\gg$ denotes a right bit-shift and $\\oplus$ denotes bitwise XOR on nonnegative integers. If $G(n)$ has binary expansion $G(n) = \\sum_{j \\geq 1} g_j(n) 2^{j-1}$ with bits $g_j(n) \\in \\{0,1\\}$, then\n$$\nx_n^{(d)} \\;=\\; \\bigoplus_{j \\geq 1} g_j(n) \\, v_j^{(d)}.\n$$\n\nUse the above fundamental definitions to explicitly construct the first $8$ two-dimensional Sobol' points $(x_n^{(1)}, x_n^{(2)})$ for $n = 1,2,\\dots,8$, under the following concrete and fully specified choices.\n\n- For dimension $1$, use the degree-$1$ primitive polynomial $p_1(x) = x + 1$, and the canonical initial odd integers, which imply $v_j^{(1)} = 2^{-j}$ for all $j \\geq 1$.\n\n- For dimension $2$, use the degree-$3$ primitive polynomial $p_2(x) = x^3 + x + 1$, so that $m = 3$ and $(a_1,a_2) = (0,1)$. Take initial odd integers $(m_1,m_2,m_3) = (1,3,5)$, so that $v_1^{(2)} = \\frac{1}{2}$, $v_2^{(2)} = \\frac{3}{4}$, and $v_3^{(2)} = \\frac{5}{8}$. Generate $v_4^{(2)}$ as needed by the above XOR recurrence.\n\nCarry out all steps by showing the explicit bitwise manipulations in base $2$, including:\n- the computation of the Gray code bits $g_j(n)$ for each $n$,\n- the evaluation of the XOR combinations of the direction numbers in binary expansions to obtain each coordinate.\n\nProvide exact rational values for all coordinates in lowest terms. No rounding is required. For the final answer, list the $8$ two-dimensional points in order $n = 1,2,\\dots,8$, flattened as a single row matrix\n$$\n\\bigl[x_1^{(1)} \\; \\; x_1^{(2)} \\; \\; x_2^{(1)} \\; \\; x_2^{(2)} \\; \\; \\dots \\; \\; x_8^{(1)} \\; \\; x_8^{(2)}\\bigr].\n$$", "solution": "The problem statement is validated as self-contained, logically consistent, and mathematically well-posed. All the necessary definitions, constants, and initial conditions required for the construction of the specified two-dimensional Sobol' sequence are provided. The problem is thus solvable as stated. I will proceed with a detailed derivation of the first $8$ points.\n\nThe core of the task is to compute the coordinates $x_n^{(d)}$ for dimension $d \\in \\{1,2\\}$ and index $n \\in \\{1, 2, \\dots, 8\\}$. The formula for this is\n$$x_n^{(d)} = \\bigoplus_{j \\geq 1} g_j(n) v_j^{(d)}$$\nwhere $g_j(n)$ are the bits in the binary expansion of the Gray code of $n$, $G(n) = \\sum_{j \\geq 1} g_j(n) 2^{j-1}$, and $\\{v_j^{(d)}\\}$ are the direction numbers for dimension $d$. The operator $\\oplus$ denotes the bitwise exclusive-or (XOR) on the binary representations of the dyadic rationals.\n\nFirst, we establish the necessary direction numbers for each dimension. Since we need to compute points up to $n=8$, we must find the bits for $G(8)$. The index $n=8$ is $(1000)_2$. Its Gray code is $G(8) = 8 \\oplus (8 \\gg 1) = 8 \\oplus 4 = 12 = (1100)_2$. Since $12 = 1 \\cdot 2^3 + 1 \\cdot 2^2$, the highest index for a non-zero bit $g_j(n)$ is $j=4$ (for $g_4(8)=1$). Therefore, we need direction numbers up to $v_4^{(d)}$ for both dimensions.\n\n**Dimension 1:**\nThe problem specifies that for dimension $1$, the direction numbers are $v_j^{(1)} = 2^{-j}$ for all $j \\geq 1$. We list the first four:\n- $v_1^{(1)} = \\frac{1}{2} = (0.1)_2$\n- $v_2^{(1)} = \\frac{1}{4} = (0.01)_2$\n- $v_3^{(1)} = \\frac{1}{8} = (0.001)_2$\n- $v_4^{(1)} = \\frac{1}{16} = (0.0001)_2$\n\n**Dimension 2:**\nFor dimension $2$, the primitive polynomial is $p_2(x) = x^3 + x + 1$. This corresponds to degree $m=3$ and coefficients $(a_1, a_2) = (0,1)$ in the form $p(x) = x^m + a_1 x^{m-1} + \\dots + a_{m-1}x + 1$.\nThe initial direction numbers are given by $(m_1, m_2, m_3) = (1, 3, 5)$:\n- $v_1^{(2)} = \\frac{m_1}{2^1} = \\frac{1}{2} = (0.1)_2$\n- $v_2^{(2)} = \\frac{m_2}{2^2} = \\frac{3}{4} = (0.11)_2$\n- $v_3^{(2)} = \\frac{m_3}{2^3} = \\frac{5}{8} = (0.101)_2$\n\nWe must compute $v_4^{(2)}$ using the provided recurrence for $jm=3$:\n$$v_j^{(2)} = \\bigoplus_{k=1}^{2} a_k \\, 2^{-k} v_{j-k}^{(2)} \\oplus 2^{-3} v_{j-3}^{(2)} = a_1 (v_{j-1}^{(2)} \\gg 1) \\oplus a_2 (v_{j-2}^{(2)} \\gg 2) \\oplus (v_{j-3}^{(2)} \\gg 3)$$\nWith $a_1=0$ and $a_2=1$, the recurrence simplifies to $v_j^{(2)} = (v_{j-2}^{(2)} \\gg 2) \\oplus (v_{j-3}^{(2)} \\gg 3)$.\nFor $j=4$:\n$$v_4^{(2)} = (v_2^{(2)} \\gg 2) \\oplus (v_1^{(2)} \\gg 3)$$\nThe operation $v \\gg k$ is a right bit-shift by $k$ places, equivalent to multiplying by $2^{-k}$.\n$v_2^{(2)} \\gg 2 = \\frac{3}{4} \\times \\frac{1}{4} = \\frac{3}{16}$\n$v_1^{(2)} \\gg 3 = \\frac{1}{2} \\times \\frac{1}{8} = \\frac{1}{16}$\nThe XOR operation is performed on the binary representations:\n$v_4^{(2)} = \\frac{3}{16} \\oplus \\frac{1}{16} = (0.0011)_2 \\oplus (0.0001)_2 = (0.0010)_2 = \\frac{2}{16} = \\frac{1}{8}$.\nSo, the direction number is $v_4^{(2)} = \\frac{1}{8} = (0.001)_2$.\n\nNow we compute the points $(x_n^{(1)}, x_n^{(2)})$ for $n=1, \\dots, 8$.\n\n**n=1**: $1=(1)_2$. $G(1)=1 \\oplus 0 = 1=(1)_2$. So $g_1(1)=1$, other $g_j(1)=0$.\n$x_1^{(1)} = v_1^{(1)} = \\frac{1}{2}$.\n$x_1^{(2)} = v_1^{(2)} = \\frac{1}{2}$.\nPoint 1: $(\\frac{1}{2}, \\frac{1}{2})$.\n\n**n=2**: $2=(10)_2$. $G(2)=2 \\oplus 1 = 3=(11)_2$. So $g_1(2)=1, g_2(2)=1$.\n$x_2^{(1)} = v_1^{(1)} \\oplus v_2^{(1)} = \\frac{1}{2} \\oplus \\frac{1}{4} = (0.1)_2 \\oplus (0.01)_2 = (0.11)_2 = \\frac{3}{4}$.\n$x_2^{(2)} = v_1^{(2)} \\oplus v_2^{(2)} = \\frac{1}{2} \\oplus \\frac{3}{4} = (0.10)_2 \\oplus (0.11)_2 = (0.01)_2 = \\frac{1}{4}$.\nPoint 2: $(\\frac{3}{4}, \\frac{1}{4})$.\n\n**n=3**: $3=(11)_2$. $G(3)=3 \\oplus 1 = 2=(10)_2$. So $g_2(3)=1$.\n$x_3^{(1)} = v_2^{(1)} = \\frac{1}{4}$.\n$x_3^{(2)} = v_2^{(2)} = \\frac{3}{4}$.\nPoint 3: $(\\frac{1}{4}, \\frac{3}{4})$.\n\n**n=4**: $4=(100)_2$. $G(4)=4 \\oplus 2 = 6=(110)_2$. So $g_2(4)=1, g_3(4)=1$.\n$x_4^{(1)} = v_2^{(1)} \\oplus v_3^{(1)} = \\frac{1}{4} \\oplus \\frac{1}{8} = (0.010)_2 \\oplus (0.001)_2 = (0.011)_2 = \\frac{3}{8}$.\n$x_4^{(2)} = v_2^{(2)} \\oplus v_3^{(2)} = \\frac{3}{4} \\oplus \\frac{5}{8} = (0.110)_2 \\oplus (0.101)_2 = (0.011)_2 = \\frac{3}{8}$.\nPoint 4: $(\\frac{3}{8}, \\frac{3}{8})$.\n\n**n=5**: $5=(101)_2$. $G(5)=5 \\oplus 2 = 7=(111)_2$. So $g_1(5)=1, g_2(5)=1, g_3(5)=1$.\n$x_5^{(1)} = v_1^{(1)} \\oplus v_2^{(1)} \\oplus v_3^{(1)} = (v_1^{(1)} \\oplus v_2^{(1)}) \\oplus v_3^{(1)} = \\frac{3}{4} \\oplus \\frac{1}{8} = (0.110)_2 \\oplus (0.001)_2 = (0.111)_2 = \\frac{7}{8}$.\n$x_5^{(2)} = v_1^{(2)} \\oplus v_2^{(2)} \\oplus v_3^{(2)} = (v_1^{(2)} \\oplus v_2^{(2)}) \\oplus v_3^{(2)} = \\frac{1}{4} \\oplus \\frac{5}{8} = (0.010)_2 \\oplus (0.101)_2 = (0.111)_2 = \\frac{7}{8}$.\nPoint 5: $(\\frac{7}{8}, \\frac{7}{8})$.\n\n**n=6**: $6=(110)_2$. $G(6)=6 \\oplus 3 = 5=(101)_2$. So $g_1(6)=1, g_3(6)=1$.\n$x_6^{(1)} = v_1^{(1)} \\oplus v_3^{(1)} = \\frac{1}{2} \\oplus \\frac{1}{8} = (0.100)_2 \\oplus (0.001)_2 = (0.101)_2 = \\frac{5}{8}$.\n$x_6^{(2)} = v_1^{(2)} \\oplus v_3^{(2)} = \\frac{1}{2} \\oplus \\frac{5}{8} = (0.100)_2 \\oplus (0.101)_2 = (0.001)_2 = \\frac{1}{8}$.\nPoint 6: $(\\frac{5}{8}, \\frac{1}{8})$.\n\n**n=7**: $7=(111)_2$. $G(7)=7 \\oplus 3 = 4=(100)_2$. So $g_3(7)=1$.\n$x_7^{(1)} = v_3^{(1)} = \\frac{1}{8}$.\n$x_7^{(2)} = v_3^{(2)} = \\frac{5}{8}$.\nPoint 7: $(\\frac{1}{8}, \\frac{5}{8})$.\n\n**n=8**: $8=(1000)_2$. $G(8)=8 \\oplus 4 = 12=(1100)_2$. So $g_3(8)=1, g_4(8)=1$.\n$x_8^{(1)} = v_3^{(1)} \\oplus v_4^{(1)} = \\frac{1}{8} \\oplus \\frac{1}{16} = (0.0010)_2 \\oplus (0.0001)_2 = (0.0011)_2 = \\frac{3}{16}$.\n$x_8^{(2)} = v_3^{(2)} \\oplus v_4^{(2)} = \\frac{5}{8} \\oplus \\frac{1}{8} = (0.101)_2 \\oplus (0.001)_2 = (0.100)_2 = \\frac{4}{8} = \\frac{1}{2}$.\nPoint 8: $(\\frac{3}{16}, \\frac{1}{2})$.\n\nThe computed coordinates, in exact fractional form and lowest terms, are:\n- $n=1: (\\frac{1}{2}, \\frac{1}{2})$\n- $n=2: (\\frac{3}{4}, \\frac{1}{4})$\n- $n=3: (\\frac{1}{4}, \\frac{3}{4})$\n- $n=4: (\\frac{3}{8}, \\frac{3}{8})$\n- $n=5: (\\frac{7}{8}, \\frac{7}{8})$\n- $n=6: (\\frac{5}{8}, \\frac{1}{8})$\n- $n=7: (\\frac{1}{8}, \\frac{5}{8})$\n- $n=8: (\\frac{3}{16}, \\frac{1}{2})$\n\nThese are collected into a single flattened row matrix for the final answer.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2}  \\frac{1}{2}  \\frac{3}{4}  \\frac{1}{4}  \\frac{1}{4}  \\frac{3}{4}  \\frac{3}{8}  \\frac{3}{8}  \\frac{7}{8}  \\frac{7}{8}  \\frac{5}{8}  \\frac{1}{8}  \\frac{1}{8}  \\frac{5}{8}  \\frac{3}{16}  \\frac{1}{2}\n\\end{pmatrix}\n}\n$$", "id": "3345407"}, {"introduction": "Now that you have seen how a Sobol' sequence is constructed, the next logical step is to assess its quality. This practice introduces the rigorous concept of a digital $(t,m,s)$-net, where the parameter $t$ serves as a crucial quality metric—the lower, the better. You will derive and apply a method to calculate $t$ directly from the sequence's generating matrices, revealing the deep connection between the uniformity of the point set and the linear algebraic properties of these matrices over the finite field $\\mathbb{F}_2$ [@problem_id:3345448]. This exercise highlights how specific choices in the construction algorithm can impact the overall quality of the sequence.", "problem": "Consider base two ($b=2$) digital nets generated by $m \\times m$ binary generating matrices $\\{C_{j}\\}_{j=1}^{s}$, where $m \\in \\mathbb{N}$ and $s \\in \\mathbb{N}$. In a Sobol' construction, the choice of direction numbers determines the entries of the generating matrices. A point set of size $N=2^{m}$ in dimension $s$ is obtained by mapping each index vector $n \\in \\{0,1\\}^{m}$ to coordinates whose $m$ binary digits are linear images of $n$ under the corresponding generating matrices. An elementary interval in $[0,1)^{s}$ is specified by fixing the first $d_{j}$ binary digits in coordinate $j$ for $j=1,\\dots,s$, where $d_{j} \\in \\{0,1,\\dots,m\\}$.\n\nStart from the following fundamental base:\n- In a base two digital net generated by $C_{1},\\dots,C_{s}$, fixing the first $d_{j}$ binary digits in coordinate $j$ imposes linear constraints $R n = r$ over the finite field of two elements, where $R$ is the matrix formed by stacking the first $d_{j}$ rows of each $C_{j}$, and $r$ is the right-hand side determined by the chosen digits.\n- The number of solutions $n \\in \\{0,1\\}^{m}$ to $R n = r$ equals $2^{m - \\operatorname{rank}(R)}$ and depends only on the rank of $R$ over the finite field of two elements.\n- A point set is a digital $(t,m,s)$-net if and only if for all choices of nonnegative integers $d_{1},\\dots,d_{s}$ with $\\sum_{j=1}^{s} d_{j} = m - t$, every such elementary interval contains exactly $2^{t}$ points.\n\nUsing only these base facts, derive a method to compute the $t$-value from the generating matrices in dimension $s=2$, and apply it to the following two scenarios with $m=4$ and $C_{1}$ fixed. In both scenarios, let\n$$\nC_{1} \\;=\\;\n\\begin{pmatrix}\n1  0  0  0 \\\\\n0  1  0  0 \\\\\n0  0  1  0 \\\\\n0  0  0  1\n\\end{pmatrix}.\n$$\nScenario A uses direction numbers that produce\n$$\nC_{2}^{(A)} \\;=\\;\n\\begin{pmatrix}\n0  0  0  1 \\\\\n0  0  1  1 \\\\\n0  1  0  1 \\\\\n1  0  0  1\n\\end{pmatrix},\n$$\nwhile Scenario B uses different direction numbers that produce\n$$\nC_{2}^{(B)} \\;=\\;\n\\begin{pmatrix}\n0  0  0  1 \\\\\n0  0  1  1 \\\\\n0  1  0  1 \\\\\n0  1  1  1\n\\end{pmatrix}.\n$$\n\nCompute the $t$-values $t_{A}$ and $t_{B}$ corresponding to Scenario A and Scenario B, respectively, by deriving and applying the rank-based method implied by the base facts above. Express your final answer as a row matrix whose entries are $t_{A}$ and $t_{B}$. No rounding is required. Provide your reasoning that shows how the change in direction numbers from Scenario A to Scenario B modifies the computed $t$-value.", "solution": "The problem requires the derivation of a method to compute the quality parameter $t$ of a digital net, based on a specific set of axiomatic definitions, and the application of this method to two scenarios. All operations are considered over the finite field of two elements, $\\mathbb{F}_2$.\n\nFirst, we derive the method for computing the $t$-value. The problem provides three fundamental facts:\n1. An elementary interval is specified by fixing the first $d_j$ binary digits for each coordinate $j=1, \\dots, s$. The constraints on the input index vector $n \\in \\{0,1\\}^m$ form a linear system $R n = r$, where $R$ is a matrix formed by stacking the first $d_j$ rows of each generating matrix $C_j$.\n2. The number of points from the net that fall into such an elementary interval is given by $2^{m - \\operatorname{rank}(R)}$, where the rank is computed over $\\mathbb{F}_2$.\n3. A point set constitutes a digital $(t,m,s)$-net if and only if for all choices of non-negative integers $d_1, \\dots, d_s$ satisfying $\\sum_{j=1}^{s} d_j = m - t$, every corresponding elementary interval contains exactly $2^t$ points.\n\nTo derive the method, we combine these facts. For a set to be a $(t,m,s)$-net, the number of points in the specified intervals must be $2^t$. According to fact 2, this number is also $2^{m - \\operatorname{rank}(R)}$. Equating these two expressions gives:\n$$2^t = 2^{m - \\operatorname{rank}(R)}$$\nThis implies the following condition on the rank of the matrix $R$:\n$$t = m - \\operatorname{rank}(R) \\implies \\operatorname{rank}(R) = m - t$$\nThis rank condition must hold for all matrices $R$ constructed from choices of $d_1, \\dots, d_s$ where $\\sum_{j=1}^{s} d_j = m - t$. The matrix $R$ is formed by stacking a total of $k = \\sum d_j = m-t$ rows. For its rank to be equal to $m-t$, the set of its rows must be linearly independent.\n\nThe quality parameter $t$ for a given set of generating matrices is defined as the smallest non-negative integer for which the set satisfies the properties of a $(t,m,s)$-net. Therefore, to compute the $t$-value, we must find the minimum non-negative integer $t$ such that for all choices of non-negative integers $d_1, \\dots, d_s$ with $\\sum_{j=1}^{s} d_j = m - t$, the corresponding set of $m-t$ rows taken from the generating matrices $\\{C_j\\}$ is linearly independent.\n\nThe derived method is as follows:\nWe test integer values for $t$ starting from $t=0$. For each $t$, we check if for all partitions $d_1 + \\dots + d_s = m-t$, the matrix $R$ formed by the first $d_j$ rows of each $C_j$ has full rank, i.e., $\\operatorname{rank}(R) = m-t$. The smallest $t$ for which this condition holds is the $t$-value of the net.\n\nNow, we apply this method to the two scenarios provided, where $m=4$ and $s=2$.\n\n**Scenario A: Computation of $t_A$**\nThe generating matrices are $C_1 = I_4$ and $C_2 = C_2^{(A)}$.\n$$ C_{1} = \\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  1  0 \\\\ 0  0  0  1 \\end{pmatrix}, \\quad C_{2}^{(A)} = \\begin{pmatrix} 0  0  0  1 \\\\ 0  0  1  1 \\\\ 0  1  0  1 \\\\ 1  0  0  1 \\end{pmatrix} $$\nWe start by testing $t=0$. For this, we must verify that for all non-negative integers $d_1, d_2$ with $d_1+d_2=m-t = 4-0 = 4$, the corresponding matrix $R(d_1, d_2)$ has rank $4$. The possible pairs for $(d_1, d_2)$ are $(0,4), (1,3), (2,2), (3,1), (4,0)$.\n- For $(d_1, d_2) = (4,0)$: $R(4,0) = C_1 = I_4$. The rank is clearly $4$.\n- For $(d_1, d_2) = (0,4)$: $R(0,4) = C_2^{(A)}$. The determinant is $\\det(C_2^{(A)}) = 1 \\pmod 2$. Thus, the rank is $4$.\n- For $(d_1, d_2) = (3,1)$: $R(3,1)$ consists of the first $3$ rows of $C_1$ and the first row of $C_2^{(A)}$. The matrix is $\\begin{pmatrix} 1000 \\\\ 0100 \\\\ 0010 \\\\ 0001 \\end{pmatrix} = I_4$. The rank is $4$.\n- For $(d_1, d_2) = (1,3)$: $R(1,3)$ consists of the first row of $C_1$ and the first $3$ rows of $C_2^{(A)}$. The matrix is $M = \\begin{pmatrix} 1000 \\\\ 0001 \\\\ 0011 \\\\ 0101 \\end{pmatrix}$. We find $\\det(M) = 1 \\pmod 2$. The rank is $4$.\n- For $(d_1, d_2) = (2,2)$: $R(2,2)$ consists of the first $2$ rows of $C_1$ and the first $2$ rows of $C_2^{(A)}$. The matrix is $M = \\begin{pmatrix} 1000 \\\\ 0100 \\\\ 0001 \\\\ 0011 \\end{pmatrix}$. We find $\\det(M) = 1 \\pmod 2$. The rank is $4$.\n\nSince the condition holds for all partitions of $4$, the net is a $(0,4,2)$-net. As $t$ must be the smallest such non-negative integer, we conclude that $t_A=0$.\n\n**Scenario B: Computation of $t_B$**\nThe generating matrices are $C_1 = I_4$ and $C_2 = C_2^{(B)}$.\n$$ C_{1} = \\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  1  0 \\\\ 0  0  0  1 \\end{pmatrix}, \\quad C_{2}^{(B)} = \\begin{pmatrix} 0  0  0  1 \\\\ 0  0  1  1 \\\\ 0  1  0  1 \\\\ 0  1  1  1 \\end{pmatrix} $$\nLet the rows of $C_2^{(B)}$ be denoted $c_{2,i}^{(B)}$. We find a linear dependency among the rows of $C_2^{(B)}$:\n$c_{2,1}^{(B)} + c_{2,2}^{(B)} + c_{2,3}^{(B)} = (0,0,0,1)+(0,0,1,1)+(0,1,0,1) = (0,1,1,1) = c_{2,4}^{(B)}$.\nThis means the four rows of $C_2^{(B)}$ are linearly dependent.\n\nWe test $t=0$. We check partitions of $d_1+d_2=4$.\n- For $(d_1, d_2) = (0,4)$: $R(0,4) = C_2^{(B)}$. As shown, its rows are linearly dependent, so its rank is less than $4$. (The rank is $3$, as the first three rows are linearly independent). The condition for $t=0$ fails. Thus, $t_B > 0$.\n\nNext, we test $t=1$. For this, we must verify that for all non-negative integers $d_1, d_2$ with $d_1+d_2=m-t = 4-1 = 3$, the corresponding matrix $R(d_1, d_2)$ has rank $3$. The possible pairs are $(0,3), (1,2), (2,1), (3,0)$.\n- For $(d_1, d_2) = (3,0)$: $R(3,0)$ consists of the first $3$ rows of $C_1$, which are standard basis vectors and thus linearly independent. The rank is $3$.\n- For $(d_1, d_2) = (0,3)$: $R(0,3)$ consists of the first $3$ rows of $C_2^{(B)}$. The matrix is $\\begin{pmatrix} 0001 \\\\ 0011 \\\\ 0101 \\end{pmatrix}$. These rows are linearly independent. The rank is $3$.\n- For $(d_1, d_2) = (2,1)$: $R(2,1)$ consists of the first $2$ rows of $C_1$ and the first row of $C_2^{(B)}$. The rows are $\\{(1,0,0,0), (0,1,0,0), (0,0,0,1)\\}$, which are linearly independent. The rank is $3$.\n- For $(d_1, d_2) = (1,2)$: $R(1,2)$ consists of the first row of $C_1$ and the first $2$ rows of $C_2^{(B)}$. The rows are $\\{(1,0,0,0), (0,0,0,1), (0,0,1,1)\\}$, which are linearly independent. The rank is $3$.\n\nSince the condition holds for all partitions of $3$, the net is a $(1,4,2)$-net. Because the condition failed for $t=0$, the smallest integer for which it holds is $t=1$. Therefore, $t_B=1$.\n\nThe change in the $t$-value from $t_A=0$ to $t_B=1$ is a direct consequence of the modification of the generating matrix $C_2$. In Scenario A, all combinations of $4$ rows partitioned between $C_1$ and $C_2^{(A)}$ are linearly independent. In Scenario B, the fourth row of $C_2^{(B)}$ was chosen such that it created a linear dependency among the rows of $C_2^{(B)}$ itself. This specific dependency, $c_{2,4}^{(B)} = c_{2,1}^{(B)} + c_{2,2}^{(B)} + c_{2,3}^{(B)}$, caused the rank of $C_2^{(B)}$ to be $3$ instead of $4$, violating the condition required for a $(0,4,2)$-net. This failure forced the $t$-value to be at least $1$. Subsequent analysis showed that the conditions for a $(1,4,2)$-net are met, establishing $t_B=1$.", "answer": "$$\\boxed{\\begin{pmatrix} 0  1 \\end{pmatrix}}$$", "id": "3345448"}, {"introduction": "A high-quality point set does not guarantee superior performance for every possible problem; the nature of the function being integrated is just as important. This exercise explores the crucial interplay between the properties of a Sobol' sequence and the characteristics of the integrand, framed by the Koksma-Hlawka inequality and the concept of Hardy–Krause variation. By analyzing a set of carefully chosen functions, you will develop an intuition for why certain features—such as non-axis-aligned discontinuities or complex high-dimensional dependencies—can diminish or even nullify the advantage of Quasi-Monte Carlo methods over standard Monte Carlo [@problem_id:3345460]. This critical perspective is vital for the effective application of Sobol' sequences in practice.", "problem": "Consider numerical integration of a function $f:[0,1]^d \\to \\mathbb{R}$ using points $\\{\\boldsymbol{x}_i\\}_{i=1}^N \\subset [0,1]^d$. Independent and identically distributed Monte Carlo (MC) uses independent $\\boldsymbol{x}_i \\sim \\mathrm{Uniform}([0,1]^d)$, while Quasi-Monte Carlo (QMC) uses low-discrepancy deterministic points such as a Sobol' sequence. Two widely accepted base facts are the following: (i) for functions of bounded Hardy–Krause variation $V_{\\mathrm{HK}}(f)$, QMC worst-case error is controlled by the product of $V_{\\mathrm{HK}}(f)$ and the star discrepancy of the point set, and (ii) Sobol' sequences achieve small star discrepancy that decays with $N$ up to logarithmic factors. Your task is to reason from these bases and from the definition-level behavior of $V_{\\mathrm{HK}}(f)$ (in particular, how axis-aligned versus oblique discontinuities and separable products affect it) to identify concrete examples where Sobol' sequences may not significantly outperform MC for that integrand.\n\nSelect all options that correctly provide a function $f$ together with a correct explanation for why a Sobol' sequence may not significantly outperform MC for that integrand.\n\nA. Let $d=2$ and $f(\\boldsymbol{x})=\\mathbf{1}\\{x_1+x_2 \\ge \\tfrac{3}{2}\\}$. Because the discontinuity is across a non-axis-aligned line within $[0,1]^2$, the Hardy–Krause variation $V_{\\mathrm{HK}}(f)$ is infinite, rendering the standard QMC bound vacuous; hence, Sobol' sequences are not guaranteed to beat the $\\mathcal{O}(N^{-1/2})$ behavior of MC and may not significantly outperform it.\n\nB. Let $d=3$ and $f(\\boldsymbol{x})=\\prod_{j=1}^3 \\left(1+0.1\\,\\sin(2\\pi x_j)\\right)$. Because $f$ is a product over coordinates, $V_{\\mathrm{HK}}(f)$ is necessarily huge in dimension $3$, so Sobol' sequences cannot gain much over MC.\n\nC. Let $d=50$ and $f(\\boldsymbol{x})=\\prod_{j=1}^{50} \\left(1+x_j\\right)$. Here $V_{\\mathrm{HK}}(f)$ grows exponentially in $d$ (indeed, it can be computed exactly and is astronomically large), so the QMC error bound carries a massive prefactor; consequently, without additional structure such as low effective dimension, a Sobol' sequence may not significantly outperform MC at practical $N$.\n\nD. Let $d=1$ and $f(x)=\\mathbf{1}\\{x\\le 1/2\\}$. Since $f$ has a jump, $V_{\\mathrm{HK}}(f)$ is infinite, so Sobol' sequences offer no improvement over MC for this integrand.\n\nChoose all that apply.", "solution": "The problem asks us to identify scenarios where Quasi-Monte Carlo (QMC) integration using Sobol' sequences may not offer a significant performance improvement over standard Monte Carlo (MC). The theoretical basis for QMC's superior performance is the Koksma-Hlawka inequality, which bounds the integration error:\n$$ \\left| \\frac{1}{N} \\sum_{i=1}^N f(\\boldsymbol{x}_i) - \\int_{[0,1]^d} f(\\boldsymbol{x}) d\\boldsymbol{x} \\right| \\le V_{\\mathrm{HK}}(f) D_N^* $$\nHere, $V_{\\mathrm{HK}}(f)$ is the Hardy–Krause variation of the function $f$, and $D_N^*$ is the star discrepancy of the point set $\\{\\boldsymbol{x}_i\\}_{i=1}^N$. For Sobol' sequences, the star discrepancy has a favorable decay rate of $D_N^* = \\mathcal{O}(N^{-1}(\\log N)^d)$. This leads to a QMC error bound of $\\mathcal{O}(N^{-1}(\\log N)^d V_{\\mathrm{HK}}(f))$. In contrast, standard MC has a probabilistic root-mean-square error that decays at a rate of $\\mathcal{O}(N^{-1/2})$, independent of the dimension $d$ (though the constant depends on the variance of $f$).\n\nQMC's asymptotic advantage holds if $V_{\\mathrm{HK}}(f)$ is finite. However, the practical performance of QMC can be non-superior to MC in two primary situations:\n1.  The Hardy–Krause variation $V_{\\mathrm{HK}}(f)$ is infinite. In this case, the Koksma-Hlawka inequality is vacuous and provides no guarantee of convergence, let alone a fast rate. QMC performance can degrade significantly.\n2.  The Hardy–Krause variation $V_{\\mathrm{HK}}(f)$ is finite but extremely large. The QMC error bound is a product of $V_{\\mathrm{HK}}(f)$ and $D_N^*$. If the prefactor $V_{\\mathrm{HK}}(f)$ is sufficiently large, the overall error $V_{\\mathrm{HK}}(f) D_N^*$ can exceed the MC error for practical, or even astronomical, values of $N$.\n\nWe will now analyze each option based on these principles.\n\n**Analysis of Option A:**\nThe function is $f(\\boldsymbol{x})=\\mathbf{1}\\{x_1+x_2 \\ge \\tfrac{3}{2}\\}$ in dimension $d=2$. This is an indicator function for the region defined by $x_1+x_2 \\ge \\tfrac{3}{2}$ within the unit square $[0,1]^2$. The boundary of this region is the line segment $x_1+x_2 = 3/2$ that connects the points $(\\tfrac{1}{2}, 1)$ and $(1, \\tfrac{1}{2})$. This boundary is a diagonal, or *oblique*, line; it is not aligned with the coordinate axes.\nA fundamental result in the theory of functions of bounded variation states that the Hardy–Krause variation of an indicator function is finite if and only if the boundary of its support set is composed of a finite number of surfaces parallel to the coordinate planes. Since the discontinuity of $f$ lies on an oblique line, its Hardy–Krause variation is infinite, i.e., $V_{\\mathrm{HK}}(f) = \\infty$.\nConsequently, the Koksma-Hlawka inequality becomes trivial ($|\\text{error}| \\le \\infty \\cdot D_N^*$), offering no guarantee on the convergence rate of the QMC method. While Sobol' sequences may still perform better than random points in some cases due to their uniformity properties, their guaranteed superior rate of convergence is lost. Empirical and theoretical studies show that for such integrands, QMC's convergence can slow down considerably, potentially approaching the $\\mathcal{O}(N^{-1/2})$ rate of MC. Thus, it is plausible that a Sobol' sequence would not *significantly* outperform MC. The reasoning provided in this option is sound.\n\n**Verdict: Correct.**\n\n**Analysis of Option B:**\nThe function is $f(\\boldsymbol{x})=\\prod_{j=1}^3 \\left(1+0.1\\,\\sin(2\\pi x_j)\\right)$ in dimension $d=3$. This function is a product of one-dimensional functions, $f(\\boldsymbol{x}) = \\prod_{j=1}^3 g(x_j)$ where $g(x) = 1+0.1\\,\\sin(2\\pi x)$. The function $g(x)$ is infinitely differentiable and thus has finite variation. Its one-dimensional variation (in the sense of Koksma/Jordan) is given by $V_K(g) = \\int_0^1 |g'(x)| dx$.\n$g'(x) = 0.1 \\cdot 2\\pi \\cos(2\\pi x) = 0.2\\pi \\cos(2\\pi x)$.\n$V_K(g) = \\int_0^1 |0.2\\pi \\cos(2\\pi x)| dx = 0.2\\pi \\int_0^1 |\\cos(2\\pi x)| dx = 0.2\\pi \\left( 4 \\int_0^{1/4} \\cos(2\\pi x) dx \\right) = 0.2\\pi \\left( 4 \\left[\\frac{\\sin(2\\pi x)}{2\\pi}\\right]_0^{1/4} \\right) = 0.2\\pi \\left( \\frac{2}{\\pi} \\right) = 0.4$.\nFor a product function, the Hardy–Krause variation is given by $V_{\\mathrm{HK}}(f) = \\prod_{j=1}^d (1 + V_K(g_j)) - 1$.\nIn this case, $V_{\\mathrm{HK}}(f) = (1 + 0.4)^3 - 1 = 1.4^3 - 1 = 2.744 - 1 = 1.744$.\nThe Hardy–Krause variation is finite and very small. The function is smooth and the dimension is low. This is a classic example of a function well-suited for QMC methods. The claim that \"$V_{\\mathrm{HK}}(f)$ is necessarily huge\" is demonstrably false. QMC is expected to significantly outperform MC for this integrand.\n\n**Verdict: Incorrect.**\n\n**Analysis of Option C:**\nThe function is $f(\\boldsymbol{x})=\\prod_{j=1}^{50} \\left(1+x_j\\right)$ in dimension $d=50$. This is again a product function, $f(\\boldsymbol{x}) = \\prod_{j=1}^{50} g(x_j)$ with $g(x_j) = 1+x_j$.\nThe one-dimensional variation of $g(x)=1+x$ on $[0,1]$ is $V_K(g) = g(1) - g(0) = (1+1) - (1+0) = 1$.\nUsing the product formula for $V_{\\mathrm{HK}}(f)$ with $d=50$:\n$V_{\\mathrm{HK}}(f) = \\prod_{j=1}^{50} (1 + V_K(g(x_j))) - 1 = \\prod_{j=1}^{50} (1+1) - 1 = 2^{50} - 1$.\nThis value is astronomically large: $2^{50}-1 \\approx 1.125 \\times 10^{15}$. The QMC error bound is proportional to this massive number: error $\\le (2^{50}-1) D_N^*$. Even though $D_N^*$ for a Sobol' sequence decays almost as $N^{-1}$, the huge prefactor $V_{\\mathrm{HK}}(f)$ means that for any practical number of sample points $N$, the QMC error bound will be very large. The asymptotic advantage of QMC may only manifest for values of $N$ that are computationally infeasible. This phenomenon, where the constant in the error bound grows exponentially with dimension, is a manifestation of the \"curse of dimensionality\" for QMC applied to functions without special structure (like low effective dimension). It is entirely possible, and even likely, that MC's $\\mathcal{O}(N^{-1/2})$ convergence would yield a smaller error for practical $N$ than a Sobol' sequence. The reasoning in the option is correct.\n\n**Verdict: Correct.**\n\n**Analysis of Option D:**\nThe function is $f(x)=\\mathbf{1}\\{x\\le 1/2\\}$ in dimension $d=1$. This function has a single discontinuity, a jump at $x=1/2$.\nThe option claims that because $f$ has a jump, $V_{\\mathrm{HK}}(f)$ is infinite. This is incorrect. In one dimension, the Hardy–Krause variation is simply the total variation in the sense of Jordan. A function of one variable has finite total variation if it is a sum of a monotonic function and a function with a finite number of jumps (or more generally, if it can be expressed as the difference of two non-decreasing functions). For $f(x) = \\mathbf{1}\\{x \\le 1/2\\}$, the total variation is the sum of the absolute values of the jumps. There is one jump of magnitude $|0-1|=1$ at $x=1/2$. More formally, $V_K(f) = \\sup \\sum_i |f(t_i) - f(t_{i-1})|$ over all partitions of $[0,1]$. This supremum is exactly $1$.\nSo, $V_{\\mathrm{HK}}(f) = V_K(f) = 1$. Since $V_{\\mathrm{HK}}(f)$ is finite and small, the Koksma-Hlawka inequality applies and guarantees a fast convergence rate for QMC. For $d=1$, the error is bounded by $\\approx \\mathcal{O}(N^{-1} \\log N)$, which is substantially better than MC's $\\mathcal{O}(N^{-1/2})$. The premise of the option's reasoning is factually wrong.\n\n**Verdict: Incorrect.**", "answer": "$$\\boxed{AC}$$", "id": "3345460"}]}