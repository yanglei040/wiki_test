{"hands_on_practices": [{"introduction": "To truly grasp the mechanics of quasi-Monte Carlo methods, it's essential to move beyond abstract definitions and engage with concrete examples. This first practice problem provides a foundational, hands-on experience with the building blocks of many advanced QMC techniques: $(t,m,s)$-nets. By explicitly constructing a small, two-dimensional point set and verifying its properties [@problem_id:3334624], you will develop a tangible intuition for how these low-discrepancy sets achieve their remarkable uniformity across the integration domain.", "problem": "Let $b$ denote the base for digital constructions, and let an elementary interval in base $b$ be any Cartesian product of the form $\\prod_{j=1}^{s} [a_{j} b^{-d_{j}}, (a_{j}+1) b^{-d_{j}})$, where $s$ is the dimension, $d_{j} \\in \\mathbb{Z}_{\\ge 0}$ and $a_{j} \\in \\{0,1,\\dots,b^{d_{j}}-1\\}$ for each $j$. A finite point set of size $N=b^{m}$ in $[0,1)^{s}$ is called a $(t,m,s)$-net in base $b$ if for every choice of nonnegative integers $d_{1},\\dots,d_{s}$ with $\\sum_{j=1}^{s} d_{j} = m - t$, each elementary interval of the above form contains exactly $b^{t}$ points of the set.\n\nThe radical inverse function in base $b$, denoted $\\phi_{b}$, is defined by the following well-tested rule: if $n \\in \\mathbb{Z}_{\\ge 0}$ has base-$b$ expansion $n = \\sum_{k=0}^{K} d_{k} b^{k}$ with digits $d_{k} \\in \\{0,1,\\dots,b-1\\}$, then $\\phi_{b}(n) = \\sum_{k=0}^{K} d_{k} b^{-(k+1)}$.\n\nConsider the two-dimensional point set constructed at base $b=2$, dimension $s=2$, and $m=3$, with $N=2^{3} = 8$ points given by\n$$\nP = \\left\\{ \\left(\\frac{i}{2^{3}}, \\phi_{2}(i) \\right) : i = 0,1,\\dots,7 \\right\\} \\subset [0,1)^{2}.\n$$\n\nTasks:\n- Construct explicitly all $8$ points in $P$ by writing each $i$ in binary and evaluating $\\phi_{2}(i)$.\n- Using only the core definition of a $(t,m,s)$-net and the structure of dyadic elementary intervals in base $2$, verify the occupancy property for all pairs $(d_{1},d_{2})$ with $d_{1}+d_{2}=3$.\n- Determine the minimal value of $t$ for which $P$ is a $(t,3,2)$-net in base $2$.\n- Briefly justify why a randomized digital shift in base $2$ (that is, applying a fixed bitwise exclusive-or to the first $m$ fractional bits of each coordinate) preserves the verified occupancy property, thereby yielding a randomized quasi-Monte Carlo point set with the same $(t,m,s)$ parameters.\n\nProvide the minimal $t$ as your final answer. Express your answer as an exact integer with no rounding and no units.", "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and all necessary information is provided. The problem statement defines a $(t,m,s)$-net, the radical inverse function $\\phi_b(n)$, and a specific two-dimensional point set $P$. All definitions are standard in the theory of quasi-Monte Carlo methods. The parameters $b=2$, $m=3$, $s=2$, and $N=8$ are clearly stated. The tasks are well-defined and lead to a unique, verifiable answer. The problem is therefore valid.\n\nWe proceed with the solution, addressing each task in sequence. The point set is given by\n$$P = \\left\\{ \\left(\\frac{i}{2^{3}}, \\phi_{2}(i) \\right) : i = 0,1,\\dots,7 \\right\\}$$\nwhere $b=2$, $m=3$, $s=2$.\n\nFirst, we construct the $N=2^3=8$ points of $P$. For each $i \\in \\{0, 1, \\dots, 7\\}$, we find its base-$2$ expansion $i = d_{2}2^2 + d_{1}2^1 + d_{0}2^0$, which we denote as $(d_2 d_1 d_0)_2$. The radical inverse function $\\phi_2(i)$ is then $\\phi_{2}(i) = d_{0}2^{-1} + d_{1}2^{-2} + d_{2}2^{-3}$. The first coordinate of each point is $x_1 = i/8$. The $8$ points are:\n- For $i=0=(000)_2$: $\\phi_2(0) = 0\\cdot 2^{-1} + 0\\cdot 2^{-2} + 0\\cdot 2^{-3} = 0$. The point is $(0/8, 0/8)$.\n- For $i=1=(001)_2$: $\\phi_2(1) = 1\\cdot 2^{-1} + 0\\cdot 2^{-2} + 0\\cdot 2^{-3} = 1/2 = 4/8$. The point is $(1/8, 4/8)$.\n- For $i=2=(010)_2$: $\\phi_2(2) = 0\\cdot 2^{-1} + 1\\cdot 2^{-2} + 0\\cdot 2^{-3} = 1/4 = 2/8$. The point is $(2/8, 2/8)$.\n- For $i=3=(011)_2$: $\\phi_2(3) = 1\\cdot 2^{-1} + 1\\cdot 2^{-2} + 0\\cdot 2^{-3} = 3/4 = 6/8$. The point is $(3/8, 6/8)$.\n- For $i=4=(100)_2$: $\\phi_2(4) = 0\\cdot 2^{-1} + 0\\cdot 2^{-2} + 1\\cdot 2^{-3} = 1/8$. The point is $(4/8, 1/8)$.\n- For $i=5=(101)_2$: $\\phi_2(5) = 1\\cdot 2^{-1} + 0\\cdot 2^{-2} + 1\\cdot 2^{-3} = 5/8$. The point is $(5/8, 5/8)$.\n- For $i=6=(110)_2$: $\\phi_2(6) = 0\\cdot 2^{-1} + 1\\cdot 2^{-2} + 1\\cdot 2^{-3} = 3/8$. The point is $(6/8, 3/8)$.\n- For $i=7=(111)_2$: $\\phi_2(7) = 1\\cdot 2^{-1} + 1\\cdot 2^{-2} + 1\\cdot 2^{-3} = 7/8$. The point is $(7/8, 7/8)$.\n\nNext, we determine the minimal non-negative integer $t$ for which $P$ is a $(t,m,s)$-net. This requires that for any choice of non-negative integers $d_1, d_2$ with $d_1+d_2=m-t=3-t$, every elementary interval $\\prod_{j=1}^{2} [a_{j} 2^{-d_{j}}, (a_{j}+1) 2^{-d_{j}})$ contains exactly $2^t$ points.\n\nWe test for $t=0$. This is the smallest possible non-negative value for $t$. If $P$ is a $(0,3,2)$-net, then $t=0$ is minimal. The condition for $t=0$ is that for all $d_1, d_2 \\ge 0$ with $d_1+d_2=3-0=3$, every elementary interval must contain exactly $2^0=1$ point. The possible pairs $(d_1, d_2)$ are $(3,0)$, $(2,1)$, $(1,2)$, and $(0,3)$.\n\nTo analyze this systematically, let's express the coordinates of the points in binary. For $i = (d_2 d_1 d_0)_2$, the $i$-th point $p_i=(x_{i,1}, x_{i,2})$ has coordinates:\n$x_{i,1} = i/8 = (d_2 2^2 + d_1 2^1 + d_0 2^0)/8 = d_2 2^{-1} + d_1 2^{-2} + d_0 2^{-3} = (0.d_2 d_1 d_0)_2$.\n$x_{i,2} = \\phi_2(i) = d_0 2^{-1} + d_1 2^{-2} + d_2 2^{-3} = (0.d_0 d_1 d_2)_2$.\n\nAn elementary interval $[a_1 2^{-d_1}, (a_1+1) 2^{-d_1}) \\times [a_2 2^{-d_2}, (a_2+1) 2^{-d_2})$ is the set of all points $(y_1, y_2) \\in [0,1)^2$ whose first $d_1$ binary digits of $y_1$ and first $d_2$ binary digits of $y_2$ are fixed. The values of $a_1 \\in \\{0,\\dots,2^{d_1}-1\\}$ and $a_2 \\in \\{0,\\dots,2^{d_2}-1\\}$ determine these fixed digits.\n\nLet's verify the occupancy property for $d_1+d_2=3$. There are $2^{d_1} \\times 2^{d_2} = 2^{d_1+d_2} = 2^3=8$ such intervals, and we must show each contains exactly one point.\n\nCase 1: $(d_1, d_2) = (3,0)$. Intervals are $[a_1/8, (a_1+1)/8) \\times [0,1)$ for $a_1 \\in \\{0,\\dots,7\\}$. An interval is defined by fixing the first $3$ bits of the first coordinate. This means $(d_2, d_1, d_0)$ are fixed. This uniquely specifies $i$, so exactly one point falls into each such interval. For instance, for $a_1=5=(101)_2$, the interval is $[5/8, 6/8) \\times [0,1)$. A point $p_i$ is in this interval if its first coordinate $x_{i,1}$ has binary expansion $(0.101)_2$. This requires $(d_2,d_1,d_0)=(1,0,1)$, which means $i=5$. Only a single point $p_5$ satisfies this. This holds for all $8$ intervals.\n\nCase 2: $(d_1, d_2) = (0,3)$. Intervals are $[0,1) \\times [a_2/8, (a_2+1)/8)$ for $a_2 \\in \\{0,\\dots,7\\}$. An interval is defined by fixing the first $3$ bits of the second coordinate. For a point $p_i$, this fixes $(d_0, d_1, d_2)$. This again uniquely determines $i = d_2 2^2 + d_1 2^1 + d_0$. Thus, each of these $8$ intervals contains exactly one point. For instance, for $a_2=5=(101)_2$, we need $x_{i,2}$ to have binary expansion $(0.101)_2$. This fixes $(d_0, d_1, d_2)=(1,0,1)$, which corresponds to $i = 1\\cdot 2^2 + 0\\cdot 2^1 + 1\\cdot 2^0=5$. Only $p_5$ is in this interval.\n\nCase 3: $(d_1, d_2) = (2,1)$. Intervals are $[a_1/4, (a_1+1)/4) \\times [a_2/2, (a_2+1)/2)$. An interval is defined by fixing the first $d_1=2$ bits of $x_1$ and the first $d_2=1$ bit of $x_2$. This means we fix $(d_2,d_1)$ and $d_0$. The triple $(d_2,d_1,d_0)$ is uniquely determined by the choice of $a_1$ and $a_2$. For instance, take $a_1=2=(10)_2$ and $a_2=1=(1)_2$. The interval is $[2/4, 3/4) \\times [1/2, 1)$. A point $p_i$ is in it if $x_{i,1}$ starts with $(0.10)_2$ and $x_{i,2}$ starts with $(0.1)_2$. This means $(d_2,d_1)=(1,0)$ and $d_0=1$. So we must have $i = (101)_2 = 5$. Only point $p_5=(5/8, 5/8)$ is in this interval. Since for each of the $4 \\times 2 = 8$ choices of $(a_1, a_2)$, the bits $(d_2, d_1, d_0)$ are uniquely specified, each interval contains exactly one point.\n\nCase 4: $(d_1, d_2) = (1,2)$. Intervals are $[a_1/2, (a_1+1)/2) \\times [a_2/4, (a_2+1)/4)$. An interval is defined by fixing the first $d_1=1$ bit of $x_1$ and the first $d_2=2$ bits of $x_2$. This means we fix $d_2$ and $(d_0, d_1)$. Again, the triple $(d_2, d_1, d_0)$ is uniquely determined. For example, $a_1=0=(0)_2$ and $a_2=3=(11)_2$. The interval is $[0,1/2) \\times [3/4,1)$. A point $p_i$ is in it if $d_2=0$ and $(d_0,d_1)=(1,1)$. This fixes $i=(011)_2=3$. Only point $p_3=(3/8, 6/8)$ is in this interval. This holds for all $2 \\times 4 = 8$ such intervals.\n\nSince in all cases for $d_1+d_2=3$, every elementary interval contains exactly $1=2^0$ point, the point set $P$ is a $(0,3,2)$-net in base $2$. As $t$ must be a non-negative integer, the minimal value for $t$ is $0$.\n\nFinally, we briefly justify why a randomized digital shift preserves this property. A digital shift in base $2$ is performed by taking a single random vector $u=(u_1, u_2) \\in [0,1)^2$ and applying it to all points in $P$ to get a new set $P'=\\{p'_i\\}$, where $p'_i = p_i \\oplus u$. The operation $\\oplus$ is coordinate-wise addition modulo $1$, which for binary expansions is a bitwise exclusive-or (XOR) operation.\nLet $E$ be an elementary interval defined by fixing the first $d_j$ bits of coordinate $j$ for $j=1,2$. The number of points of $P'$ in $E$ is the size of the set $\\{i \\mid p_i \\oplus u \\in E\\}$. A point $p_i \\oplus u$ is in $E$ if and only if $p_i$ is in the set $E' = \\{y \\in [0,1)^2 \\mid y \\oplus u \\in E\\}$. The set $E'$ is itself an elementary interval of the same type (i.e., with the same $d_1, d_2$). This is because fixing the first $d_j$ bits of $y_j \\oplus u_j$ is equivalent to fixing the first $d_j$ bits of $y_j$ to a new set of values, determined by the bits of $u_j$.\nSo, the number of points of $P'$ in $E$ is equal to the number of points of $P$ in $E'$. Since $P$ is a $(t,m,s)$-net, it contains exactly $b^t$ points in any elementary interval $E'$ with $\\sum d_j = m-t$. Therefore, $P'$ also has $b^t$ points in any interval $E$ of this type. The randomization preserves the $(t,m,s)$-net property. For our specific case, the shifted set is also a $(0,3,2)$-net.\n\nThe minimal value of $t$ for which $P$ is a $(t,3,2)$-net in base $2$ has been determined to be $0$.", "answer": "$$ \\boxed{0} $$", "id": "3334624"}, {"introduction": "Having constructed a low-discrepancy net, we now explore the profound implications of its structure for numerical integration. This exercise demonstrates the ideal performance scenario for a randomized quasi-Monte Carlo (RQMC) estimator, illustrating the concept of perfect stratification. When an integrand is perfectly aligned with the net's inherent partitioning of the space, the integration error can be eliminated entirely. By computing the variance for the indicator function of an elementary interval [@problem_id:3334644], you will see firsthand how RQMC can achieve zero variance, a dramatic improvement that is impossible with standard Monte Carlo methods.", "problem": "Let $b \\geq 2$ be an integer base, and let $m \\in \\mathbb{N}$, $s \\in \\mathbb{N}$, and $t \\in \\{0,1,\\dots,m\\}$. Consider a $(t,m,s)$-net in base $b$ comprising $N = b^{m}$ points in $[0,1)^{s}$. Apply Owen’s nested uniform scrambling (a standard randomization in randomized quasi-Monte Carlo (RQMC)) to obtain randomized points $\\tilde{X}_{1},\\dots,\\tilde{X}_{N}$ that are marginally independent and identically distributed as uniform on $[0,1)^{s}$ and such that, almost surely, the scrambled point set remains a $(t,m,s)$-net in base $b$. Define a base-$b$ elementary interval\n$$\nJ \\;=\\; \\prod_{i=1}^{s} \\Big[ a_{i} b^{-d_{i}},\\, (a_{i}+1) b^{-d_{i}} \\Big),\n$$\nwhere for each $i \\in \\{1,\\dots,s\\}$, $d_{i} \\in \\mathbb{N}_{0}$ and $a_{i} \\in \\{0,1,\\dots,b^{d_{i}}-1\\}$, and suppose that $\\sum_{i=1}^{s} d_{i} \\leq m - t$. Let $f(x) = \\mathbf{1}_{J}(x)$ and $I = \\int_{[0,1)^{s}} f(x)\\,\\mathrm{d}x$ denote the integral of $f$. Consider the RQMC estimator\n$$\n\\widehat{I}_{N} \\;=\\; \\frac{1}{N} \\sum_{n=1}^{N} f(\\tilde{X}_{n}).\n$$\n\nUsing only the defining properties of $(t,m,s)$-nets in base $b$, the structure of base-$b$ elementary intervals, and the fact that Owen’s scrambling preserves the $(t,m,s)$-net property almost surely, compute the variance of $\\widehat{I}_{N}$ with respect to the scrambling randomization (conditioning on the underlying deterministic net). For scientific contrast, also derive in your working the variance of the ordinary Monte Carlo estimator with $N$ independent uniform samples on $[0,1)^{s}$ for the same $f$, but report only the variance of $\\widehat{I}_{N}$ as your final answer. The final answer must be a single exact value with no rounding.", "solution": "The problem is scientifically grounded, well-posed, and objective. It is based on standard, verifiable concepts in the mathematical theory of randomized quasi-Monte Carlo methods, specifically $(t,m,s)$-nets and Owen's scrambling. The problem statement is complete, consistent, and provides all necessary information to derive a unique solution. Therefore, the problem is valid. We proceed with the solution.\n\nThe integral to be estimated is $I = \\int_{[0,1)^{s}} f(x)\\,\\mathrm{d}x$, where the integrand is the indicator function $f(x) = \\mathbf{1}_{J}(x)$ of an elementary interval $J$. The value of the integral is the volume of the interval, $I = \\text{Vol}(J)$. The elementary interval is given by\n$$\nJ \\;=\\; \\prod_{i=1}^{s} \\Big[ a_{i} b^{-d_{i}},\\, (a_{i}+1) b^{-d_{i}} \\Big)\n$$\nwhere $d_{i} \\in \\mathbb{N}_{0}$ and $a_{i} \\in \\{0,1,\\dots,b^{d_{i}}-1\\}$ for each $i \\in \\{1,\\dots,s\\}$. The volume of this interval is $\\text{Vol}(J) = \\prod_{i=1}^{s} b^{-d_{i}} = b^{-\\sum_{i=1}^{s} d_{i}}$.\n\nThe randomized quasi-Monte Carlo (RQMC) estimator for $I$ is\n$$\n\\widehat{I}_{N} \\;=\\; \\frac{1}{N} \\sum_{n=1}^{N} f(\\tilde{X}_{n}) \\;=\\; \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}_{J}(\\tilde{X}_{n})\n$$\nwhere $N=b^m$ and $\\tilde{X}_{1},\\dots,\\tilde{X}_{N}$ are points from a scrambled $(t,m,s)$-net. We want to compute the variance of this estimator with respect to the scrambling randomization, which is given by $\\mathrm{Var}(\\widehat{I}_{N}) = E[\\widehat{I}_{N}^2] - (E[\\widehat{I}_{N}])^2$.\n\nFirst, we compute the expectation of the estimator. Since the scrambled points $\\tilde{X}_{n}$ are marginally distributed as uniform on $[0,1)^{s}$, the expectation of $f(\\tilde{X}_{n})$ for any $n$ is\n$$\nE[f(\\tilde{X}_{n})] = \\int_{[0,1)^{s}} f(x)\\,\\mathrm{d}x = I = \\text{Vol}(J).\n$$\nBy linearity of expectation, the expectation of the estimator is\n$$\nE[\\widehat{I}_{N}] = E\\left[\\frac{1}{N} \\sum_{n=1}^{N} f(\\tilde{X}_{n})\\right] = \\frac{1}{N} \\sum_{n=1}^{N} E[f(\\tilde{X}_{n})] = \\frac{1}{N} \\sum_{n=1}^{N} I = \\frac{N \\cdot I}{N} = I.\n$$\nThe RQMC estimator is unbiased.\n\nNow we analyze the core property of the estimator. The quantity $\\sum_{n=1}^{N} \\mathbf{1}_{J}(\\tilde{X}_{n})$ is the number of scrambled points that fall inside the interval $J$, which we denote by $\\tilde{n}(J)$.\nThe problem provides two critical pieces of information:\n$1$. The set of scrambled points $\\{\\tilde{X}_{1},\\dots,\\tilde{X}_{N}\\}$ almost surely constitutes a $(t,m,s)$-net in base $b$.\n$2$. The elementary interval $J$ satisfies the condition $\\sum_{i=1}^{s} d_{i} \\leq m - t$.\n\nLet's use the definition of a $(t,m,s)$-net. A set of $N=b^m$ points is a $(t,m,s)$-net in base $b$ if any elementary interval $E$ with volume $\\text{Vol}(E) = b^{t-m}$ contains exactly $b^t$ points of the set.\nThe given interval $J$ has volume $\\text{Vol}(J) = b^{-k}$, where $k = \\sum_{i=1}^{s} d_{i}$. The condition is $k \\leq m-t$.\nThis interval $J$ can be expressed as a disjoint union of smaller elementary intervals, each having volume $b^{t-m}$. Specifically, $J$ can be partitioned into a number of such intervals equal to the ratio of their volumes:\n$$\n\\frac{\\text{Vol}(J)}{\\text{Vol}(E)} = \\frac{b^{-k}}{b^{-(m-t)}} = b^{m-t-k}.\n$$\nSince $\\{\\tilde{X}_{1},\\dots,\\tilde{X}_{N}\\}$ forms a $(t,m,s)$-net (almost surely), each of these $b^{m-t-k}$ sub-intervals contains exactly $b^t$ points. Therefore, the total number of scrambled points in $J$ is, almost surely,\n$$\n\\tilde{n}(J) = (\\text{number of sub-intervals}) \\times (\\text{points per sub-interval}) = (b^{m-t-k}) \\times (b^t) = b^{m-k}.\n$$\nLet's relate this to the integral $I$. We have $N = b^m$ and $I = \\text{Vol}(J) = b^{-k}$. Thus, almost surely,\n$$\n\\tilde{n}(J) = b^{m-k} = b^m \\cdot b^{-k} = N \\cdot I.\n$$\nThis means that for any realization of the scrambling, the number of points falling into $J$ is a fixed, non-random quantity (with probability $1$). The RQMC estimator is therefore almost surely constant:\n$$\n\\widehat{I}_{N} = \\frac{1}{N} \\tilde{n}(J) = \\frac{1}{N} (N \\cdot I) = I.\n$$\nA random variable that is almost surely equal to a constant has a variance of zero. Thus,\n$$\n\\mathrm{Var}(\\widehat{I}_{N}) = E\\big[(\\widehat{I}_{N} - E[\\widehat{I}_{N}])^2\\big] = E\\big[(I - I)^2\\big] = E[0] = 0.\n$$\n\nFor scientific contrast, we derive the variance of the ordinary Monte Carlo (MC) estimator, $\\widehat{I}_{\\text{MC}}$, based on $N$ independent and identically distributed samples $X_{n} \\sim U([0,1)^{s})$. The MC estimator is $\\widehat{I}_{\\text{MC}} = \\frac{1}{N} \\sum_{n=1}^{N} f(X_{n})$. Since the samples are independent, the variance is\n$$\n\\mathrm{Var}(\\widehat{I}_{\\text{MC}}) = \\frac{1}{N^2} \\sum_{n=1}^{N} \\mathrm{Var}(f(X_{n})) = \\frac{N}{N^2} \\mathrm{Var}(f(X_1)) = \\frac{\\mathrm{Var}(f(X_1))}{N}.\n$$\nThe random variable $f(X_1) = \\mathbf{1}_{J}(X_1)$ is a Bernoulli trial with success probability $p = P(X_1 \\in J) = \\text{Vol}(J) = I$. The variance of a Bernoulli$(p)$ variable is $p(1-p)$. Therefore,\n$$\n\\mathrm{Var}(f(X_1)) = I(1-I).\n$$\nThe variance of the standard MC estimator is\n$$\n\\mathrm{Var}(\\widehat{I}_{\\text{MC}}) = \\frac{I(1-I)}{N}.\n$$\nThis result highlights the remarkable property of RQMC: for certain well-structured integrands, such as indicator functions of elementary intervals that align with the net's structure, the integration error variance can be reduced to zero, whereas ordinary MC variance is always positive (for $0  I  1$). The variance of the RQMC estimator $\\widehat{I}_{N}$ for the specified problem is exactly $0$.", "answer": "$$\\boxed{0}$$", "id": "3334644"}, {"introduction": "While the previous exercise showed an ideal case, the power of randomization truly shines when dealing with complex, oscillatory functions. This problem confronts a limitation of classical, deterministic QMC theory: the Koksma-Hlawka inequality, which can provide overly pessimistic error bounds. You will analyze a specially constructed function [@problem_id:3334588] that has a very large Hardy-Krause variation but a small amplitude, causing the deterministic error bound to be extremely loose. This practice powerfully illustrates why randomized QMC is so crucial; its probabilistic error estimates, based on variance rather than variation, often give a much more accurate picture of the true integration error.", "problem": "Consider dimension $s \\ge 2$ and the Koksma–Hlawka inequality linking the integration error of a function over $[0,1]^s$ by a quasi–Monte Carlo rule to the star discrepancy of the point set and the variation of the integrand in the sense of Hardy–Krause. You are asked to identify a concrete Lipschitz construction in $[0,1]^s$ with finite but large higher–order interaction contribution to the variation, and to explain the implications for the tightness of the Koksma–Hlawka bound when using low–discrepancy point sets in dimension $s$, as well as how a standard randomization of digital nets changes the picture.\n\nSelect the single option that correctly constructs such a function and accurately characterizes the resulting bound’s tightness and the effect of randomization.\n\nA. Let $h(t) = \\epsilon \\sin(2\\pi m t)(1-t)$ with parameters $\\epsilon \\in (0,1)$ and integer $m \\ge 1$, and define $f(x) = \\prod_{j=1}^s h(x_j)$ on $[0,1]^s$. Then $f$ is Lipschitz and has finite anchored Hardy–Krause variation $V_{\\mathrm{HK}}^{(1)}(f)$ dominated exclusively by the $s$–way mixed interaction, with $V_{\\mathrm{HK}}^{(1)}(f) = \\left(\\int_0^1 |h'(t)|\\,\\mathrm{d}t\\right)^s$, which can be made very large by increasing $m$ even if $\\epsilon$ is small. Consequently, the Koksma–Hlawka bound with a low–discrepancy point set in dimension $s$ can be extremely loose for this $f$ because it scales like $\\left(\\int_0^1 |h'(t)|\\,\\mathrm{d}t\\right)^s$ times the star discrepancy. By contrast, for a randomized digital net (e.g., a scrambled $(t,m,s)$–net), the root–mean–square error depends on square–integrable projections of $f$ rather than $V_{\\mathrm{HK}}^{(1)}(f)$, and for this construction it can be much smaller due to $\\|f\\|_{\\infty} \\le \\epsilon^s$.\n\nB. Let $h(t) = \\epsilon \\sin(2\\pi m t)(1-t)$ and define $f(x) = \\sum_{j=1}^s h(x_j)$. Then $f$ is Lipschitz and its variation is dominated by first–order effects so the Koksma–Hlawka bound tightens as $s$ grows because low–discrepancy point sets have decreasing star discrepancy with dimension.\n\nC. Let $h(t) = \\epsilon \\sin(2\\pi m t)(1-t)$ and define $f(x) = \\prod_{j=1}^s h(x_j)$. Then $f$ is Lipschitz and $V_{\\mathrm{HK}}^{(1)}(f) = s \\int_0^1 |h'(t)|\\,\\mathrm{d}t$, so randomization of a digital net tightens the Koksma–Hlawka bound to order $(\\log N)^s/N$ times $s$.\n\nD. Let $h(t) = \\epsilon \\sin(2\\pi m t)(1-t)$ and define $f(x) = \\prod_{j=1}^s h(x_j)$. Because $h(1)=0$, all lower–order analysis of variance components vanish and the Koksma–Hlawka inequality becomes exact up to a universal constant for any $(t,m,s)$–net, hence the bound is tight in high dimension.", "solution": "## PROBLEM VALIDATION\n\n### Step 1: Extract Givens\nThe problem statement provides the following information:\n1.  **Context**: Integration of a function $f$ over the $s$-dimensional unit cube, $[0,1]^s$, with dimension $s \\ge 2$.\n2.  **Method**: Quasi–Monte Carlo (QMC) integration.\n3.  **Inequality**: The Koksma–Hlawka inequality, which bounds the integration error by the product of the star discrepancy of the point set, $D_N^*$, and the variation of the integrand in the sense of Hardy–Krause, $V_{\\mathrm{HK}}(f)$.\n4.  **Task**:\n    a. Identify a concrete Lipschitz function $f: [0,1]^s \\to \\mathbb{R}$ that has a finite but large contribution to its Hardy–Krause variation from higher-order interactions.\n    b. Explain the implications for the tightness of the Koksma–Hlawka bound when using low-discrepancy point sets.\n    c. Explain how randomizing a digital net (a type of low-discrepancy point set) affects the integration error for this function.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria.\n\n*   **Scientifically Grounded**: The problem is based on well-established, fundamental concepts in the mathematical theory of numerical integration, specifically the field of Quasi-Monte Carlo methods. The Koksma–Hlawka inequality, Hardy–Krause variation, star discrepancy, digital nets, and randomization techniques are all standard topics in this area. The problem is scientifically and mathematically sound.\n*   **Well-Posed**: The question is well-posed. It asks for the identification and analysis of a specific type of function to illustrate a known theoretical phenomenon in QMC. The task is clearly defined, and a unique, reasoned answer can be derived from the theory.\n*   **Objective**: The problem is stated in precise, objective, and technical language, free from ambiguity, subjectivity, or opinion.\n\nThe problem does not exhibit any of the flaws listed in the instructions:\n1.  **Scientific or Factual Unsoundness**: None. The underlying principles are correct.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly formalizable and central to the topic of randomized quasi-Monte Carlo methods.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained and provides sufficient context to proceed. The term \"anchored Hardy–Krause variation $V_{\\mathrm{HK}}^{(1)}(f)$\" used in option A is slightly non-standard but its meaning is clear from the context and the provided formula.\n4.  **Unrealistic or Infeasible**: Not applicable, as this is a problem in pure mathematics.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-structured.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is non-trivial, requiring a nuanced understanding of different error estimation regimes in QMC.\n7.  **Outside Scientific Verifiability**: The claims are mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A full solution will be derived.\n\n## DERIVATION AND OPTION ANALYSIS\n\nThe core of the problem is to find a function for which the deterministic QMC error bound (Koksma–Hlawka) is very pessimistic, while the error for randomized QMC (RQMC) is small. This typically occurs for functions with high-frequency oscillations but small amplitude.\n\nLet's analyze the proposed function construction. Let $h(t) = \\epsilon \\sin(2\\pi m t)(1-t)$ for $\\epsilon \\in (0,1)$ and integer $m \\ge 1$. The function to be integrated is $f(x) = \\prod_{j=1}^s h(x_j)$ for $x = (x_1, \\dots, x_s) \\in [0,1]^s$.\n\n**1. Properties of the constructed function $f$:**\n\n*   **Lipschitz continuity**: The derivative of $h$ is $h'(t) = \\epsilon [ - \\sin(2\\pi m t) + 2\\pi m (1-t) \\cos(2\\pi m t) ]$. Since $t \\in [0,1]$, $|h'(t)|$ is bounded by a constant, say $L_h$. Thus, $h$ is Lipschitz. The partial derivative of $f$ with respect to $x_k$ is $\\frac{\\partial f}{\\partial x_k} = h'(x_k) \\prod_{j \\neq k} h(x_j)$. Its magnitude is bounded: $|\\frac{\\partial f}{\\partial x_k}| \\le L_h \\cdot (\\sup_{t \\in [0,1]} |h(t)|)^{s-1}$. Since $h$ is bounded, all first-order partial derivatives of $f$ are bounded on $[0,1]^s$, which implies $f$ is Lipschitz.\n\n*   **Hardy–Krause Variation ($V_{\\mathrm{HK}}(f)$)**: The variation in the sense of Hardy and Krause is given by the sum of integrals of the absolute values of mixed partial derivatives, evaluated with the non-differentiated variables set to $1$.\n    $$ V_{\\mathrm{HK}}(f) = \\sum_{\\emptyset \\neq u \\subseteq \\{1, \\dots, s\\}} \\int_{[0,1]^{|u|}} \\left| \\frac{\\partial^{|u|} f}{\\prod_{j \\in u} \\partial x_j} (x_u; 1) \\right| d x_u $$\n    where $(x_u; 1)$ denotes the point where coordinates in $u$ are variables and coordinates not in $u$ are set to $1$.\n    The mixed partial derivative is $\\frac{\\partial^{|u|} f}{\\prod_{j \\in u} \\partial x_j} = \\left(\\prod_{j \\in u} h'(x_j)\\right) \\left(\\prod_{k \\notin u} h(x_k)\\right)$.\n    A key property of our chosen $h(t)$ is that $h(1) = \\epsilon \\sin(2\\pi m)(1-1) = 0$.\n    Consequently, for any $u \\neq \\{1, \\dots, s\\}$, the set of indices not in $u$ is non-empty. The term $\\prod_{k \\notin u} h(x_k)$ evaluated with $x_k=1$ will contain a factor $h(1)=0$, making the entire term zero.\n    The only non-vanishing term in the sum for $V_{\\mathrm{HK}}(f)$ is the one corresponding to $u = \\{1, \\dots, s\\}$.\n    $$ V_{\\mathrm{HK}}(f) = \\int_{[0,1]^s} \\left| \\prod_{j=1}^s h'(x_j) \\right| dx_1 \\cdots dx_s = \\prod_{j=1}^s \\int_0^1 |h'(x_j)| dx_j = \\left( \\int_0^1 |h'(t)| dt \\right)^s $$\n    This shows the variation is determined exclusively by the highest-order ($s$-way) interaction.\n\n*   **Magnitude of Variation**: Let's estimate the integral of $|h'(t)|$.\n    $|h'(t)| = |\\epsilon [ - \\sin(2\\pi m t) + 2\\pi m (1-t) \\cos(2\\pi m t) ]|$.\n    For large $m$, the term $2\\pi m (1-t) \\cos(2\\pi m t)$ dominates.\n    $\\int_0^1 |h'(t)| dt \\approx \\int_0^1 |2\\pi m \\epsilon (1-t) \\cos(2\\pi m t)| dt = 2\\pi m \\epsilon \\int_0^1 (1-t) |\\cos(2\\pi m t)| dt$.\n    The integral $\\int_0^1 (1-t) |\\cos(2\\pi m t)| dt$ is approximately the average value of $(1-t)$ (which is $1/2$) times the average value of $|\\cos(\\cdot)|$ over many periods (which is $2/\\pi$), so the integral is roughly $(1/2)(2/\\pi) = 1/\\pi$.\n    Thus, $\\int_0^1 |h'(t)| dt \\approx 2\\pi m \\epsilon (1/\\pi) = 2m\\epsilon$. This grows linearly with $m$.\n    Therefore, $V_{\\mathrm{HK}}(f) \\approx (2m\\epsilon)^s$. By choosing a large integer $m$, the variation can be made arbitrarily large.\n\n*   **Magnitude of the function**: $|h(t)| = |\\epsilon \\sin(2\\pi m t)(1-t)| \\le \\epsilon$.\n    Therefore, $\\|f\\|_{\\infty} = \\sup_{x \\in [0,1]^s} |f(x)| \\le \\epsilon^s$. For $\\epsilon  1$, this can be very small.\n\n**2. Implications for QMC Bounds:**\n\n*   **Koksma–Hlawka Bound (deterministic QMC)**: The error is bounded by $|E_N(f)| \\le V_{\\mathrm{HK}}(f) D_N^*$. Using a low-discrepancy sequence (e.g., a $(t,m,s)$-net), $D_N^* = O(N^{-1} (\\log N)^{s-1})$.\n    The bound is approximately $(2m\\epsilon)^s \\cdot O(N^{-1} (\\log N)^{s-1})$.\n    The actual error is trivially bounded by $2\\|f\\|_{\\infty} \\le 2\\epsilon^s$.\n    If we choose $m$ to be very large, the Koksma–Hlawka bound can be huge, e.g., much larger than $1$, while the true error is guaranteed to be tiny. This demonstrates that for this class of functions, the Koksma-Hlawka bound is extremely loose and uninformative.\n\n*   **Randomized QMC (RQMC) Error**: For a randomized QMC method, such as a scrambled digital net, the error is a random variable. We analyze its root-mean-square error (RMSE), which is the square root of the variance for an unbiased estimator. The variance of the RQMC estimator is determined not by the Hardy-Krause variation, but by the sum of variances of the ANOVA (Analysis of Variance) components of the function, $\\sigma_u^2(f)$.\n    The total variance of the function itself is $\\sigma^2(f) = \\int_{[0,1]^s} f(x)^2 dx - (\\int_{[0,1]^s} f(x) dx)^2$.\n    The RMSE is bounded by the function's standard deviation $\\sigma(f)$.\n    $\\sigma(f)^2 \\le \\int f(x)^2 dx = \\int_0^1 \\cdots \\int_0^1 \\prod_{j=1}^s h(x_j)^2 dx_1 \\cdots dx_s = \\left( \\int_0^1 h(t)^2 dt \\right)^s$.\n    Also, $\\int_0^1 h(t)^2 dt \\le \\int_0^1 \\epsilon^2 dt = \\epsilon^2$. Thus, $\\sigma(f)^2 \\le (\\epsilon^2)^s = \\epsilon^{2s}$.\n    The RMSE is bounded by a quantity of order $\\epsilon^s$. This is small.\n    The RQMC error rate for smooth functions is typically $O(N^{-1})$ or even faster, and the constant factor depends on $L_2$-norms of $f$ or its derivatives, not the $L_1$-based Hardy-Krause variation. For this function, the RQMC error estimate will be small, reflecting the small magnitude of the function, and will provide a much tighter bound on the error.\n\n### Option-by-Option Analysis\n\n**A. Let $h(t) = \\epsilon \\sin(2\\pi m t)(1-t)$ with parameters $\\epsilon \\in (0,1)$ and integer $m \\ge 1$, and define $f(x) = \\prod_{j=1}^s h(x_j)$ on $[0,1]^s$. Then $f$ is Lipschitz and has finite anchored Hardy–Krause variation $V_{\\mathrm{HK}}^{(1)}(f)$ dominated exclusively by the $s$–way mixed interaction, with $V_{\\mathrm{HK}}^{(1)}(f) = \\left(\\int_0^1 |h'(t)|\\,\\mathrm{d}t\\right)^s$, which can be made very large by increasing $m$ even if $\\epsilon$ is small. Consequently, the Koksma–Hlawka bound with a low–discrepancy point set in dimension $s$ can be extremely loose for this $f$ because it scales like $\\left(\\int_0^1 |h'(t)|\\,\\mathrm{d}t\\right)^s$ times the star discrepancy. By contrast, for a randomized digital net (e.g., a scrambled $(t,m,s)$–net), the root–mean–square error depends on square–integrable projections of $f$ rather than $V_{\\mathrm{HK}}^{(1)}(f)$, and for this construction it can be much smaller due to $\\|f\\|_{\\infty} \\le \\epsilon^s$.**\nThis option correctly describes the construction. It correctly calculates the structure of the Hardy–Krause variation and its magnitude. It correctly deduces that the Koksma–Hlawka bound is loose. It correctly contrasts this with the RQMC error, which depends on different properties of $f$ (related to its $L_2$-norm or ANOVA decomposition, i.e., \"square-integrable projections\") and is consequently much smaller. Every claim in this option is consistent with our derivation.\n**Verdict: Correct.**\n\n**B. Let $h(t) = \\epsilon \\sin(2\\pi m t)(1-t)$ and define $f(x) = \\sum_{j=1}^s h(x_j)$. Then $f$ is Lipschitz and its variation is dominated by first–order effects so the Koksma–Hlawka bound tightens as $s$ grows because low–discrepancy point sets have decreasing star discrepancy with dimension.**\nThis option proposes an additive function $f(x) = \\sum_{j=1}^s h(x_j)$. This function is dominated by first-order effects, contradicting the problem's requirement to have large *higher-order* interaction contributions. Furthermore, the claim that star-discrepancy $D_N^*$ decreases with dimension $s$ is false; the curse of dimensionality implies that for fixed $N$, $D_N^*$ deteriorates (grows) rapidly with $s$.\n**Verdict: Incorrect.**\n\n**C. Let $h(t) = \\epsilon \\sin(2\\pi m t)(1-t)$ and define $f(x) = \\prod_{j=1}^s h(x_j)$. Then $f$ is Lipschitz and $V_{\\mathrm{HK}}^{(1)}(f) = s \\int_0^1 |h'(t)|\\,\\mathrm{d}t$, so randomization of a digital net tightens the Koksma–Hlawka bound to order $(\\log N)^s/N$ times $s$.**\nThis option uses the correct functional form but gives an incorrect formula for the variation. The formula $V_{\\mathrm{HK}}(f) = s \\int_0^1 |h'(t)|\\,\\mathrm{d}t$ is for an additive function, not a product function. The correct formula is a product of integrals, $\\left(\\int_0^1 |h'(t)|\\,dt\\right)^s$. Additionally, the statement about randomization is confused. Randomization leads to a different type of error bound (probabilistic, based on variance), it does not \"tighten the Koksma-Hlawka bound\". The angegeben error rate is also strangely formulated.\n**Verdict: Incorrect.**\n\n**D. Let $h(t) = \\epsilon \\sin(2\\pi m t)(1-t)$ and define $f(x) = \\prod_{j=1}^s h(x_j)$. Because $h(1)=0$, all lower–order analysis of variance components vanish and the Koksma–Hlawka inequality becomes exact up to a universal constant for any $(t,m,s)$–net, hence the bound is tight in high dimension.**\nThe premise \"Because $h(1)=0$, all lower–order analysis of variance components vanish\" is false. The condition $h(1)=0$ makes lower-order terms in the *Hardy-Krause variation formula* vanish, not the ANOVA components. For instance, the mean $f_\\emptyset = \\int f(x) dx = (\\int h(t) dt)^s \\neq 0$. The conclusion that the K-H inequality becomes exact or tight is also false; as demonstrated, for this function, the bound is extremely loose.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3334588"}]}