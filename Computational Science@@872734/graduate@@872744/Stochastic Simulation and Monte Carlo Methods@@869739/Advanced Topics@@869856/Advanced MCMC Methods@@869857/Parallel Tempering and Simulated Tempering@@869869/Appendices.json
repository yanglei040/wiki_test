{"hands_on_practices": [{"introduction": "The efficiency of Parallel Tempering hinges on the successful exchange of configurations between different temperatures, a process governed by the swap acceptance probability. This foundational exercise guides you through a hands-on derivation to approximate this critical rate in a high-dimensional Gaussian model. By working through this problem, you will gain a deep, quantitative understanding of how factors like system dimensionality and temperature spacing dictate the performance of a Parallel Tempering simulation, providing the theoretical basis for designing effective temperature ladders [@problem_id:3326637].", "problem": "Consider Parallel Tempering within the framework of Markov Chain Monte Carlo (MCMC), where the target at inverse temperature $\\beta$ is the $d$-dimensional Gaussian product with energy $U(x) = \\frac{1}{2}\\|x\\|^{2}$ and density $\\pi_{\\beta}(x) \\propto \\exp(-\\beta U(x))$. Let two adjacent inverse temperatures be $\\beta_{1}$ and $\\beta_{2}$ with $\\beta_{2} - \\beta_{1} = \\Delta \\beta  0$ and mean inverse temperature $\\bar{\\beta} = (\\beta_{1} + \\beta_{2})/2$. In a swap move, independent states $x \\sim \\pi_{\\beta_{1}}$ and $y \\sim \\pi_{\\beta_{2}}$ are proposed to be exchanged, with Metropolis acceptance probability\n$$\n\\alpha = \\min\\left\\{1, \\exp\\big((\\beta_{1} - \\beta_{2})(U(x) - U(y))\\big)\\right\\}.\n$$\nStarting from the definitions of the Gaussian product and the properties of $\\chi^{2}$ distributions, approximate the difference of energies $U(x) - U(y)$ by a normal distribution for large $d$ and derive the resulting leading-order approximation (for small $\\Delta \\beta$) to the expected swap acceptance $\\mathbb{E}[\\alpha]$ between these adjacent inverse temperatures. Your derivation must clearly show how the approximation depends on the dimension $d$ and the mean inverse temperature $\\bar{\\beta}$. Express your final answer as a single closed-form analytic expression involving $d$, $\\bar{\\beta}$, and $\\Delta \\beta$ using the standard normal cumulative distribution function $\\Phi$. No rounding is required, and no units are involved.", "solution": "The user wants to derive an approximation for the expected swap acceptance probability in a Parallel Tempering simulation.\n\n### Step 1: Problem Validation\n\n**1.1. Extraction of Givens:**\n- Framework: Parallel Tempering, Markov Chain Monte Carlo (MCMC).\n- Target density at inverse temperature $\\beta$: $\\pi_{\\beta}(x) \\propto \\exp(-\\beta U(x))$ for $x \\in \\mathbb{R}^d$.\n- Energy function: $U(x) = \\frac{1}{2}\\|x\\|^{2}$, which is $\\frac{1}{2}\\sum_{i=1}^{d} x_i^2$.\n- Inverse temperatures: Two adjacent levels, $\\beta_{1}$ and $\\beta_{2}$.\n- Temperature difference: $\\beta_{2} - \\beta_{1} = \\Delta \\beta  0$.\n- Mean inverse temperature: $\\bar{\\beta} = (\\beta_{1} + \\beta_{2})/2$.\n- Swap proposal: Exchange independent states $x \\sim \\pi_{\\beta_{1}}$ and $y \\sim \\pi_{\\beta_{2}}$.\n- Metropolis acceptance probability: $\\alpha = \\min\\left\\{1, \\exp\\big((\\beta_{1} - \\beta_{2})(U(x) - U(y))\\big)\\right\\}$.\n- Task: Derive the leading-order approximation for the expected swap acceptance, $\\mathbb{E}[\\alpha]$, for large dimension $d$ and small $\\Delta \\beta$.\n- Method: Approximate the energy difference $U(x) - U(y)$ by a normal distribution, using properties of $\\chi^{2}$ distributions.\n- Required final form: A closed-form expression depending on $d$, $\\bar{\\beta}$, $\\Delta \\beta$, and the standard normal cumulative distribution function $\\Phi$.\n\n**1.2. Validation using Givens:**\n- **Scientific Grounding:** The problem is firmly rooted in the theory of MCMC methods, specifically Parallel Tempering. The choice of a multidimensional Gaussian as the target distribution is a standard and analytically tractable model. The swap acceptance probability is the correct Metropolis-Hastings probability for this exchange. The use of the Central Limit Theorem (via $\\chi^2$ distributions) for high-dimensional systems is a standard and valid technique in statistical physics and computational statistics. All concepts are scientifically sound.\n- **Well-Posedness:** The problem statement is clear, self-contained, and provides a well-defined objective: to derive a specific approximation. The constraints (large $d$, small $\\Delta \\beta$) guide the derivation process. A unique analytical solution is expected.\n- **Objectivity:** The problem is formulated in precise mathematical language, free from ambiguity or subjective elements.\n\n**1.3. Verdict and Action:**\nThe problem is valid. It is scientifically sound, well-posed, objective, and contains no discernible flaws. I will proceed with the derivation.\n\n### Step 2: Solution Derivation\n\nThe target probability density at inverse temperature $\\beta$ is given by\n$$ \\pi_{\\beta}(x) = C_{\\beta} \\exp(-\\beta U(x)) = C_{\\beta} \\exp\\left(-\\frac{\\beta}{2} \\sum_{i=1}^d x_i^2\\right) = \\prod_{i=1}^d \\left(\\sqrt{\\frac{\\beta}{2\\pi}} \\exp\\left(-\\frac{\\beta}{2} x_i^2\\right)\\right) $$\nwhere $C_{\\beta}$ is the normalization constant. This shows that for a state $x \\sim \\pi_{\\beta}(x)$, each component $x_i$ is an independent random variable drawn from a zero-mean Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$ with variance $\\sigma^2 = 1/\\beta$.\n\nLet's characterize the distribution of the energy $U(x)$. If $x_i \\sim \\mathcal{N}(0, 1/\\beta)$, then $\\sqrt{\\beta}x_i \\sim \\mathcal{N}(0, 1)$. The square of a standard normal variable follows a chi-squared distribution with one degree of freedom, so $(\\sqrt{\\beta}x_i)^2 = \\beta x_i^2 \\sim \\chi^2_1$.\n\nThe energy is $U(x) = \\frac{1}{2}\\sum_{i=1}^d x_i^2$. Let's consider the scaled energy $2\\beta U(x)$:\n$$ 2\\beta U(x) = \\beta \\sum_{i=1}^d x_i^2 = \\sum_{i=1}^d (\\beta x_i^2) $$\nSince each term $\\beta x_i^2$ is an independent $\\chi^2_1$ variable, their sum is a chi-squared variable with $d$ degrees of freedom: $2\\beta U(x) \\sim \\chi^2_d$.\nThis implies that the energy $U(x)$ for a state drawn from $\\pi_{\\beta}(x)$ follows the distribution $U(x) \\sim \\frac{1}{2\\beta}\\chi^2_d$. This is a Gamma distribution with shape $k=d/2$ and scale $\\theta=1/\\beta$.\n\nThe mean and variance of a $\\chi^2_d$ distribution are $d$ and $2d$, respectively. We can find the mean and variance of the energy $U(x)$:\n$$ \\mathbb{E}[U(x)] = \\mathbb{E}\\left[\\frac{1}{2\\beta}\\chi^2_d\\right] = \\frac{1}{2\\beta}\\mathbb{E}[\\chi^2_d] = \\frac{d}{2\\beta} $$\n$$ \\text{Var}(U(x)) = \\text{Var}\\left(\\frac{1}{2\\beta}\\chi^2_d\\right) = \\left(\\frac{1}{2\\beta}\\right)^2\\text{Var}(\\chi^2_d) = \\frac{2d}{4\\beta^2} = \\frac{d}{2\\beta^2} $$\nFor large $d$, the Central Limit Theorem states that the $\\chi^2_d$ distribution can be approximated by a normal distribution. Thus, for large $d$, the distribution of $U(x)$ is approximately normal:\n$$ U(x) \\approx \\mathcal{N}\\left(\\frac{d}{2\\beta}, \\frac{d}{2\\beta^2}\\right) $$\nWe are interested in the difference of energies $\\Delta U = U(x) - U(y)$, where $x \\sim \\pi_{\\beta_1}$ and $y \\sim \\pi_{\\beta_2}$ are independent. Using the normal approximation for large $d$:\n$$ U(x) \\approx \\mathcal{N}\\left(\\frac{d}{2\\beta_1}, \\frac{d}{2\\beta_1^2}\\right) \\quad \\text{and} \\quad U(y) \\approx \\mathcal{N}\\left(\\frac{d}{2\\beta_2}, \\frac{d}{2\\beta_2^2}\\right) $$\nSince $U(x)$ and $U(y)$ are independent, their difference $\\Delta U$ is also approximately normal, $\\Delta U \\approx \\mathcal{N}(\\mu_{\\Delta U}, \\sigma^2_{\\Delta U})$, with mean and variance:\n$$ \\mu_{\\Delta U} = \\mathbb{E}[U(x)] - \\mathbb{E}[U(y)] = \\frac{d}{2\\beta_1} - \\frac{d}{2\\beta_2} = \\frac{d(\\beta_2 - \\beta_1)}{2\\beta_1\\beta_2} = \\frac{d \\Delta\\beta}{2\\beta_1\\beta_2} $$\n$$ \\sigma^2_{\\Delta U} = \\text{Var}(U(x)) + \\text{Var}(U(y)) = \\frac{d}{2\\beta_1^2} + \\frac{d}{2\\beta_2^2} = \\frac{d}{2}\\left(\\frac{1}{\\beta_1^2} + \\frac{1}{\\beta_2^2}\\right) $$\nThe problem requires a leading-order approximation for small $\\Delta\\beta$. We use $\\beta_1 = \\bar{\\beta} - \\Delta\\beta/2$ and $\\beta_2 = \\bar{\\beta} + \\Delta\\beta/2$. For small $\\Delta\\beta$:\n$$ \\beta_1\\beta_2 = \\bar{\\beta}^2 - (\\Delta\\beta/2)^2 \\approx \\bar{\\beta}^2 $$\n$$ \\frac{1}{\\beta_1^2} + \\frac{1}{\\beta_2^2} \\approx \\frac{1}{\\bar{\\beta}^2} + \\frac{1}{\\bar{\\beta}^2} = \\frac{2}{\\bar{\\beta}^2} $$\nSubstituting these into the expressions for the mean and variance gives the leading-order approximations:\n$$ \\mu_{\\Delta U} \\approx \\frac{d \\Delta\\beta}{2\\bar{\\beta}^2} $$\n$$ \\sigma^2_{\\Delta U} \\approx \\frac{d}{2}\\left(\\frac{2}{\\bar{\\beta}^2}\\right) = \\frac{d}{\\bar{\\beta}^2} $$\nThe swap acceptance probability is $\\alpha = \\min\\{1, \\exp((\\beta_1 - \\beta_2)\\Delta U)\\}$. Since $\\beta_1 - \\beta_2 = -\\Delta\\beta$, we have:\n$$ \\alpha = \\min\\{1, \\exp(-\\Delta\\beta \\cdot \\Delta U)\\} $$\nLet's define a new random variable $K = \\Delta\\beta \\cdot \\Delta U$. Since $\\Delta U$ is approximately normal, $K$ is also approximately normal, $K \\approx \\mathcal{N}(\\mu_K, \\sigma_K^2)$, where:\n$$ \\mu_K = \\Delta\\beta \\cdot \\mu_{\\Delta U} \\approx \\Delta\\beta \\cdot \\frac{d \\Delta\\beta}{2\\bar{\\beta}^2} = \\frac{d(\\Delta\\beta)^2}{2\\bar{\\beta}^2} $$\n$$ \\sigma_K^2 = (\\Delta\\beta)^2 \\cdot \\sigma^2_{\\Delta U} \\approx (\\Delta\\beta)^2 \\cdot \\frac{d}{\\bar{\\beta}^2} = \\frac{d(\\Delta\\beta)^2}{\\bar{\\beta}^2} $$\nA crucial relationship emerges from these approximations: $\\mu_K \\approx \\frac{1}{2}\\sigma_K^2$. We will use this exact relationship for our approximated variables from now on.\n\nWe need to calculate $\\mathbb{E}[\\alpha] = \\mathbb{E}[\\min\\{1, \\exp(-K)\\}]$. Let $f_K(k)$ be the probability density function of $K \\approx \\mathcal{N}(\\mu_K, \\sigma_K^2)$. The expectation is given by:\n$$ \\mathbb{E}[\\alpha] = \\int_{-\\infty}^{\\infty} \\min\\{1, e^{-k}\\} f_K(k) \\,dk $$\nWe split the integral based on the value of $k$:\n$$ \\mathbb{E}[\\alpha] = \\int_{-\\infty}^{0} 1 \\cdot f_K(k) \\,dk + \\int_{0}^{\\infty} e^{-k} f_K(k) \\,dk $$\nThe first integral is the cumulative probability $P(K \\le 0)$:\n$$ \\int_{-\\infty}^{0} f_K(k) \\,dk = P(K \\le 0) = \\Phi\\left(\\frac{0 - \\mu_K}{\\sigma_K}\\right) = \\Phi\\left(-\\frac{\\mu_K}{\\sigma_K}\\right) $$\nFor the second integral, we have:\n$$ \\int_{0}^{\\infty} e^{-k} f_K(k) \\,dk = \\int_{0}^{\\infty} e^{-k} \\frac{1}{\\sqrt{2\\pi\\sigma_K^2}} \\exp\\left(-\\frac{(k - \\mu_K)^2}{2\\sigma_K^2}\\right) \\,dk $$\nWe complete the square in the exponent of the integrand. The term inside the exponential is:\n$$ -k - \\frac{(k - \\mu_K)^2}{2\\sigma_K^2} = -\\frac{2k\\sigma_K^2 + k^2 - 2k\\mu_K + \\mu_K^2}{2\\sigma_K^2} = -\\frac{k^2 - 2k(\\mu_K - \\sigma_K^2) + \\mu_K^2}{2\\sigma_K^2} $$\n$$ = -\\frac{(k - (\\mu_K - \\sigma_K^2))^2 - (\\mu_K - \\sigma_K^2)^2 + \\mu_K^2}{2\\sigma_K^2} = -\\frac{(k - (\\mu_K - \\sigma_K^2))^2}{2\\sigma_K^2} - \\mu_K + \\frac{\\sigma_K^2}{2} $$\nSo the integrand becomes:\n$$ \\exp\\left(-\\mu_K + \\frac{\\sigma_K^2}{2}\\right) \\frac{1}{\\sqrt{2\\pi\\sigma_K^2}} \\exp\\left(-\\frac{(k - (\\mu_K - \\sigma_K^2))^2}{2\\sigma_K^2}\\right) $$\nThe second part is the PDF of a normal distribution $\\mathcal{N}(\\mu_K - \\sigma_K^2, \\sigma_K^2)$. Let's call a random variable with this distribution $K'$. The integral becomes:\n$$ \\exp\\left(-\\mu_K + \\frac{\\sigma_K^2}{2}\\right) \\int_{0}^{\\infty} f_{K'}(k) \\,dk = \\exp\\left(-\\mu_K + \\frac{\\sigma_K^2}{2}\\right) P(K'  0) $$\n$$ P(K'  0) = 1 - \\Phi\\left(\\frac{0 - (\\mu_K - \\sigma_K^2)}{\\sigma_K}\\right) = 1 - \\Phi\\left(\\frac{-\\mu_K + \\sigma_K^2}{\\sigma_K}\\right) = \\Phi\\left(\\frac{\\mu_K - \\sigma_K^2}{\\sigma_K}\\right) $$\nNow, we use the key relation $\\mu_K = \\frac{1}{2}\\sigma_K^2$. The prefactor becomes $\\exp(-\\frac{1}{2}\\sigma_K^2 + \\frac{1}{2}\\sigma_K^2) = \\exp(0) = 1$.\nThe argument of $\\Phi$ in the first integral part becomes:\n$$ -\\frac{\\mu_K}{\\sigma_K} = -\\frac{\\frac{1}{2}\\sigma_K^2}{\\sigma_K} = -\\frac{\\sigma_K}{2} $$\nThe argument of $\\Phi$ in the second integral part becomes:\n$$ \\frac{\\mu_K - \\sigma_K^2}{\\sigma_K} = \\frac{\\frac{1}{2}\\sigma_K^2 - \\sigma_K^2}{\\sigma_K} = \\frac{-\\frac{1}{2}\\sigma_K^2}{\\sigma_K} = -\\frac{\\sigma_K}{2} $$\nCombining the results, the expected acceptance probability is:\n$$ \\mathbb{E}[\\alpha] \\approx \\Phi\\left(-\\frac{\\sigma_K}{2}\\right) + 1 \\cdot \\Phi\\left(-\\frac{\\sigma_K}{2}\\right) = 2\\Phi\\left(-\\frac{\\sigma_K}{2}\\right) $$\nFinally, we substitute the expression for $\\sigma_K$. Since $\\Delta\\beta  0$ and $\\bar{\\beta}  0$, the standard deviation is:\n$$ \\sigma_K = \\sqrt{\\frac{d(\\Delta\\beta)^2}{\\bar{\\beta}^2}} = \\frac{\\sqrt{d}\\Delta\\beta}{\\bar{\\beta}} $$\nThe resulting approximation for the expected acceptance probability is:\n$$ \\mathbb{E}[\\alpha] \\approx 2\\Phi\\left(-\\frac{\\sqrt{d}\\Delta\\beta}{2\\bar{\\beta}}\\right) $$\nThis expression relates the swap acceptance rate to the dimension $d$ and the temperature schedule parameters $\\bar{\\beta}$ and $\\Delta\\beta$.", "answer": "$$\\boxed{2\\Phi\\left(-\\frac{\\sqrt{d}\\Delta\\beta}{2\\bar{\\beta}}\\right)}$$", "id": "3326637"}, {"introduction": "While both Parallel and Simulated Tempering leverage temperature to explore complex energy landscapes, their practical implementation details differ significantly. This coding exercise illuminates a key distinction by focusing on the log-weights, or free energy estimates, required by Simulated Tempering. You will implement a simple three-state model to see firsthand how inaccurate weights can skew the simulation's time at the target temperature, degrading sampling efficiency compared to the more structurally robust Parallel Tempering method [@problem_id:3326582].", "problem": "Consider a discrete energy landscape with three microstates, denoted by $L$, $C$, and $R$, with energy function $U$ specified by $U(L) = 0$, $U(C) = h$, and $U(R) = 0$, where $h  0$ represents a barrier height. A macro-step of the local Metropolisâ€“Hastings update at inverse temperature $\\beta$ is defined as two successive proposals that attempt to move from a well to the barrier and then from the barrier to the opposite well. Under this macro-step definition, and assuming standard Metropolis accept/reject with proposals that deterministically attempt to move to $C$ from either well and deterministically attempt to move to the opposite well from the barrier, the probability to flip from $L$ to $R$ (and symmetrically from $R$ to $L$) in one macro-step at inverse temperature $\\beta$ is $e^{-\\beta h}$.\n\nSimulated Tempering (ST) targets the joint distribution over state $x \\in \\{L,C,R\\}$ and inverse temperature index $k \\in \\{1,\\dots,K\\}$ with the density proportional to $\\exp(-\\beta_k U(x) + g_k)$, where $\\beta_k$ is the inverse temperature ladder and $g_k$ are scalar log-weights. The stationary marginal over temperatures in ST is proportional to the partition function $Z(\\beta_k)$ times the exponential of the log-weights, that is\n$$\n\\pi_k^{\\mathrm{ST}} \\propto Z(\\beta_k)\\, e^{g_k}, \\quad \\text{where} \\quad Z(\\beta) = \\sum_{x \\in \\{L,C,R\\}} e^{-\\beta U(x)} = 2 + e^{-\\beta h}.\n$$\nParallel Tempering (PT) uses $K$ replicas, one at each inverse temperature $\\beta_k$, and attempts neighbor swaps; in the present construction, for states restricted to $\\{L,R\\}$, swaps are always accepted because $U(L) = U(R)$, so the replica identity performing updates at $\\beta=1$ is uniformly distributed over the ladder in stationarity. Thus, the per macro-step probability that the well state at $\\beta=1$ changes under PT equals the average transition probability across the ladder,\n$$\nM_{\\mathrm{PT}} = \\frac{1}{K} \\sum_{k=1}^K e^{-\\beta_k h}.\n$$\nIn contrast, the per macro-step probability that the well state changes at $\\beta=1$ under ST equals the product of the stationary occupancy at $\\beta=1$ and the local flip probability there,\n$$\nM_{\\mathrm{ST}} = \\pi_1^{\\mathrm{ST}}\\, e^{-\\beta_1 h}, \\quad \\text{where} \\quad \\pi_1^{\\mathrm{ST}} = \\frac{Z(\\beta_1)\\, e^{g_1}}{\\sum_{j=1}^K Z(\\beta_j) e^{g_j}}.\n$$\nThis construction isolates the effect of inaccurate $g_k$ in ST, which induces skewed occupancy across temperatures via the stationary distribution and thereby changes $M_{\\mathrm{ST}}$ at $\\beta=1$, while $M_{\\mathrm{PT}}$ depends only on the ladder.\n\nYour task is to implement a program that, for a given barrier height $h$, inverse temperature ladder $(\\beta_1,\\dots,\\beta_K)$, and log-weights $(g_1,\\dots,g_K)$, computes the degradation ratio\n$$\nR = \\frac{M_{\\mathrm{ST}}}{M_{\\mathrm{PT}}}.\n$$\nA smaller value of $R$ indicates worse mixing at $\\beta=1$ under ST relative to PT with the same ladder. Derive this ratio from first principles based on the specified joint ST target, the partition function, and the macro-step flip probability $e^{-\\beta h}$, without using any additional formulas that are not implied by these definitions.\n\nUse the following test suite of parameter values:\n\n- Test case 1 (general case with inaccurate weights favoring hot temperatures by default): $h = 6.0$, $(\\beta_1,\\beta_2,\\beta_3) = (1.0,0.5,0.2)$, $(g_1,g_2,g_3) = (0.0,0.0,0.0)$.\n- Test case 2 (more severely inaccurate $g_k$ skewing towards hot temperatures): $h = 6.0$, $(\\beta_1,\\beta_2,\\beta_3) = (1.0,0.5,0.2)$, $(g_1,g_2,g_3) = (0.0,1.5,2.0)$.\n- Test case 3 (lower barrier with the same inaccurate weights as test case 1): $h = 3.0$, $(\\beta_1,\\beta_2,\\beta_3) = (1.0,0.5,0.2)$, $(g_1,g_2,g_3) = (0.0,0.0,0.0)$.\n- Test case 4 (control case with perfect weights in ST): $h = 6.0$, $(\\beta_1,\\beta_2,\\beta_3) = (1.0,0.5,0.2)$, and $g_k = -\\log Z(\\beta_k)$ for $k=1,2,3$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, where each element is the floating-point value of $R$ for the corresponding test case. For example, the output format must be exactly of the form $[r_1,r_2,r_3,r_4]$ with no additional text.\n\nNo physical units are involved in this computation; all quantities are dimensionless. Angles do not appear in this problem. Express all numbers as decimal floating-point values.", "solution": "The problem requires the computation of the degradation ratio $R$, defined as the ratio of the mixing probabilities for Simulated Tempering ($M_{\\mathrm{ST}}$) and Parallel Tempering ($M_{\\mathrm{PT}}$), $R = M_{\\mathrm{ST}}/M_{\\mathrm{PT}}$. This ratio quantifies the performance of Simulated Tempering at the target inverse temperature $\\beta_1=1$ relative to an idealized Parallel Tempering setup on the same temperature ladder. The derivation and computation are based entirely on the principles and definitions provided.\n\nFirst, we recall the definitions for the per macro-step flip probabilities for the two methods. For Parallel Tempering (PT), the problem specifies a construction where the replica at the target inverse temperature $\\beta_1$ samples uniformly from the entire ladder of $K$ inverse temperatures $\\{\\beta_k\\}_{k=1}^K$. Consequently, its effective flip probability is the average of the local flip probabilities, $e^{-\\beta_k h}$, across the ladder:\n$$\nM_{\\mathrm{PT}} = \\frac{1}{K} \\sum_{k=1}^K e^{-\\beta_k h}\n$$\n\nFor Simulated Tempering (ST), the system evolves in a joint space of state $x$ and temperature index $k$. The probability of a state flip at the target inverse temperature $\\beta_1$ is the product of the stationary probability of the system being at that temperature, $\\pi_1^{\\mathrm{ST}}$, and the local flip probability at that temperature, $e^{-\\beta_1 h}$.\n$$\nM_{\\mathrm{ST}} = \\pi_1^{\\mathrm{ST}}\\, e^{-\\beta_1 h}\n$$\nThe stationary probability $\\pi_1^{\\mathrm{ST}}$ depends on the log-weights $g_k$ and the partition functions $Z(\\beta_k)$ at each temperature. The problem provides the expression for the stationary marginal probability over temperatures:\n$$\n\\pi_k^{\\mathrm{ST}} \\propto Z(\\beta_k)\\, e^{g_k}\n$$\nNormalizing this gives the probability for $k=1$:\n$$\n\\pi_1^{\\mathrm{ST}} = \\frac{Z(\\beta_1)\\, e^{g_1}}{\\sum_{j=1}^K Z(\\beta_j) e^{g_j}}\n$$\nWe are also given the partition function $Z(\\beta)$ for the three-state system with energy barrier $h$:\n$$\nZ(\\beta) = \\sum_{x \\in \\{L,C,R\\}} e^{-\\beta U(x)} = e^{-\\beta \\cdot 0} + e^{-\\beta h} + e^{-\\beta \\cdot 0} = 2 + e^{-\\beta h}\n$$\n\nTo derive the degradation ratio $R$, we substitute the expressions for $M_{\\mathrm{ST}}$ and $M_{\\mathrm{PT}}$ into its definition:\n$$\nR = \\frac{M_{\\mathrm{ST}}}{M_{\\mathrm{PT}}} = \\frac{\\pi_1^{\\mathrm{ST}}\\, e^{-\\beta_1 h}}{\\frac{1}{K} \\sum_{k=1}^K e^{-\\beta_k h}}\n$$\nNext, we substitute the full expression for $\\pi_1^{\\mathrm{ST}}$:\n$$\nR = \\frac{\\left( \\frac{Z(\\beta_1)\\, e^{g_1}}{\\sum_{j=1}^K Z(\\beta_j) e^{g_j}} \\right) e^{-\\beta_1 h}}{\\frac{1}{K} \\sum_{k=1}^K e^{-\\beta_k h}}\n$$\nRearranging this expression yields the final formula for the degradation ratio, which can be directly implemented programmatically:\n$$\nR = \\frac{K \\cdot Z(\\beta_1) \\cdot e^{g_1} \\cdot e^{-\\beta_1 h}}{\\left(\\sum_{j=1}^K Z(\\beta_j) e^{g_j}\\right) \\left(\\sum_{k=1}^K e^{-\\beta_k h}\\right)}\n$$\nThis formula depends only on the given parameters: the barrier height $h$, the inverse temperature ladder $\\boldsymbol{\\beta} = (\\beta_1, \\dots, \\beta_K)$, and the log-weights $\\mathbf{g} = (g_1, \\dots, g_K)$.\n\nThe algorithmic implementation will follow these steps for each test case:\n1.  Given $h$, $\\boldsymbol{\\beta}$, and $\\mathbf{g}$, determine the number of temperatures $K$. For Test Case 4, first compute the partition functions $Z(\\beta_k)$ to determine the values for $g_k = -\\log(Z(\\beta_k))$.\n2.  Compute an array of partition function values, $Z(\\beta_k) = 2 + e^{-\\beta_k h}$, for all $k=1, \\dots, K$.\n3.  Compute the terms in the denominator of the expression for $R$:\n    - The ST normalization term: $\\sum_{j=1}^K Z(\\beta_j) e^{g_j}$.\n    - The PT mixing term: $\\sum_{k=1}^K e^{-\\beta_k h}$.\n4.  Compute the term in the numerator of the expression for $R$: $K \\cdot Z(\\beta_1) \\cdot e^{g_1} \\cdot e^{-\\beta_1 h}$.\n5.  Calculate the final ratio $R$.\n\nThis procedure is applied to each of the four specified test cases to generate the final list of results.", "answer": "```python\nimport numpy as np\n\ndef calculate_ratio(h, betas, gs):\n    \"\"\"\n    Computes the degradation ratio R for a given set of parameters.\n\n    Args:\n        h (float): The energy barrier height.\n        betas (list or np.ndarray): The inverse temperature ladder.\n        gs (list or np.ndarray): The log-weights for ST.\n\n    Returns:\n        float: The degradation ratio R.\n    \"\"\"\n    betas = np.array(betas, dtype=float)\n    gs = np.array(gs, dtype=float)\n    K = len(betas)\n\n    # --- Calculation for M_PT ---\n    # The term e^(-beta_k * h) appears in both PT and ST calculations.\n    exp_beta_h = np.exp(-betas * h)\n    \n    # PT mixing probability is the average transition probability over the ladder.\n    m_pt_sum_term = np.sum(exp_beta_h)\n    m_pt = (1.0 / K) * m_pt_sum_term\n\n    # --- Calculation for M_ST ---\n    # The partition function Z(beta) = 2 + e^(-beta * h).\n    z_vals = 2.0 + exp_beta_h\n    \n    # Stationary probability pi_1^ST at the target temperature beta_1.\n    st_normalization_term = np.sum(z_vals * np.exp(gs))\n    \n    # Handle the case where the normalization term might be zero, although\n    # it is positive definite for finite z_vals and gs.\n    if st_normalization_term == 0.0:\n        pi_1_st = 0.0\n    else:\n        pi_1_st_numerator = z_vals[0] * np.exp(gs[0])\n        pi_1_st = pi_1_st_numerator / st_normalization_term\n\n    # ST mixing probability at the target temperature.\n    m_st = pi_1_st * exp_beta_h[0]\n\n    # --- Degradation Ratio R ---\n    # The ratio R = M_ST / M_PT.\n    # Handle the division-by-zero case, though unlikely with given problem constraints.\n    # If m_pt is 0, exp_beta_h are all 0, so m_st is also 0. R -> 0/0.\n    # In this limit, R can be shown to be K * pi_1_st, but here we can just return 0.\n    if m_pt == 0.0:\n        return 0.0\n    \n    return m_st / m_pt\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    # Test case 1 (general case with inaccurate weights favoring hot temperatures by default): \n    # h = 6.0, (beta_1,beta_2,beta_3) = (1.0,0.5,0.2), (g_1,g_2,g_3) = (0.0,0.0,0.0).\n    tc1 = (6.0, [1.0, 0.5, 0.2], [0.0, 0.0, 0.0])\n\n    # Test case 2 (more severely inaccurate g_k skewing towards hot temperatures): \n    # h = 6.0, (beta_1,beta_2,beta_3) = (1.0,0.5,0.2), (g_1,g_2,g_3) = (0.0,1.5,2.0).\n    tc2 = (6.0, [1.0, 0.5, 0.2], [0.0, 1.5, 2.0])\n\n    # Test case 3 (lower barrier with the same inaccurate weights as test case 1): \n    # h = 3.0, (beta_1,beta_2,beta_3) = (1.0,0.5,0.2), (g_1,g_2,g_3) = (0.0,0.0,0.0).\n    tc3 = (3.0, [1.0, 0.5, 0.2], [0.0, 0.0, 0.0])\n\n    # Test case 4 (control case with perfect weights in ST): \n    # h = 6.0, (beta_1,beta_2,beta_3) = (1.0,0.5,0.2), and g_k = -log Z(beta_k) for k=1,2,3.\n    h4 = 6.0\n    betas4 = np.array([1.0, 0.5, 0.2])\n    z_vals4 = 2.0 + np.exp(-betas4 * h4)\n    gs4 = -np.log(z_vals4)\n    tc4 = (h4, betas4.tolist(), gs4.tolist())\n    \n    test_cases = [tc1, tc2, tc3, tc4]\n    \n    results = []\n    for h_val, betas_val, gs_val in test_cases:\n        ratio = calculate_ratio(h_val, betas_val, gs_val)\n        results.append(ratio)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3326582"}, {"introduction": "Effective scientific computing is often a matter of optimal resource allocation. In Parallel Tempering, computational effort is divided between exploring the energy surface at each temperature (within-replica updates) and exchanging information across temperatures (swaps). This advanced practice models this fundamental trade-off, challenging you to derive and implement the optimal frequency of swap attempts to minimize the overall computational cost for a given level of sampling accuracy. Mastering this balance is a crucial step towards designing and running high-performance tempering simulations in practice [@problem_id:3326598].", "problem": "You are tasked with deriving and implementing an optimization for the swap attempt frequency in Parallel Tempering (PT) within the field of Markov Chain Monte Carlo (MCMC). Consider a PT algorithm with a temperature ladder that includes an inverse temperature $\\beta = 1$, where an observable of interest is measured. The dynamics at $\\beta = 1$ are modeled as follows.\n\nAssumptions and modeling primitives:\n- The within-replica update at $\\beta = 1$ is a reversible Markov kernel with stationary distribution corresponding to $\\beta = 1$ and an observable-specific spectral gap $g_{\\mathrm{w}} \\in (0,1]$.\n- A swap attempt between adjacent replicas is made with acceptance probability $a \\in (0,1]$. When a swap involving the $\\beta = 1$ replica is accepted, it produces an additional decorrelation for the observable, modeled by an effective factor $\\kappa \\in [0,1]$ that summarizes the impact of the adjacent ladder connectivity on that observable. The swap step is taken to be reversible with respect to the joint PT target.\n- At each discrete step, with frequency $f \\in [0,1]$ (the decision variable), a swap attempt is made; otherwise a within-replica update at $\\beta = 1$ is made. This produces a mixture kernel $P_f = (1-f) K_{\\mathrm{w}} + f K_{\\mathrm{s}}$, where $K_{\\mathrm{w}}$ is the within-replica kernel and $K_{\\mathrm{s}}$ is the swap kernel.\n- For the observable at $\\beta = 1$, approximate the lag-$1$ correlation as autoregressive of order $1$ (AR($1$)) with coefficient $r(f) = 1 - g_{\\mathrm{eff}}(f)$, where\n$$\ng_{\\mathrm{eff}}(f) = (1-f)\\, g_{\\mathrm{w}} + f \\, a \\, \\kappa.\n$$\nThis corresponds to using the standard convexity of Dirichlet forms for reversible mixtures and a one-mode AR($1$) reduction of the slowest mode affecting the observable.\n- Under the AR($1$) model, the integrated autocorrelation time (IACT) of the observable is\n$$\n\\tau_{\\mathrm{int}}(f) = \\frac{1 + r(f)}{1 - r(f)} = \\frac{2 - g_{\\mathrm{eff}}(f)}{g_{\\mathrm{eff}}(f)} = \\frac{2}{g_{\\mathrm{eff}}(f)} - 1.\n$$\n- The computational cost per discrete step is linear in the decision $f$, with cost\n$$\nc(f) = (1-f)\\, c_{\\mathrm{w}} + f \\, c_{\\mathrm{s}},\n$$\nwhere $c_{\\mathrm{w}}  0$ is the cost of a within-replica update at $\\beta = 1$, and $c_{\\mathrm{s}}  0$ is the cost of a swap attempt.\n\nObjective:\n- For a given parameter tuple $(g_{\\mathrm{w}}, a, \\kappa, c_{\\mathrm{w}}, c_{\\mathrm{s}})$, choose $f \\in [0,1]$ to minimize the cost-normalized integrated autocorrelation time\n$$\nJ(f) = \\tau_{\\mathrm{int}}(f) \\, c(f) = \\left(\\frac{2}{g_{\\mathrm{eff}}(f)} - 1\\right)\\left((1-f)\\, c_{\\mathrm{w}} + f \\, c_{\\mathrm{s}}\\right).\n$$\n\nTasks:\n- Starting only from the above modeling assumptions and the fundamental definitions of reversible Markov chains, spectral gap, and AR($1$) IACT, derive a principled algorithm to compute the minimizing frequency $f^{\\star} \\in [0,1]$. Your derivation must reason about the structure of the objective $J(f)$ over the closed interval $[0,1]$, including boundary behavior and any admissibility constraints (e.g., ensure $g_{\\mathrm{eff}}(f)  0$ when evaluating $J(f)$).\n- Your program must implement this algorithm robustly. Specifically:\n  - Compute all stationary points of $J(f)$ in $(0,1)$, if any exist, by solving the first-order optimality condition derived from $dJ/df = 0$.\n  - Include the interval endpoints $f=0$ and $f=1$ among the candidates.\n  - Evaluate $J(f)$ on all admissible candidates (those with $g_{\\mathrm{eff}}(f)  0$). Choose the $f^{\\star}$ that minimizes $J(f)$. In the event of ties within numerical tolerance, choose the smaller $f$.\n  - Use a small positive threshold when testing $g_{\\mathrm{eff}}(f)  0$ to avoid division by zero; specify and use a threshold of $\\varepsilon = 10^{-12}$.\n\nTest suite:\nRun your program on the following parameter tuples, each given as a $5$-tuple $(g_{\\mathrm{w}}, a, \\kappa, c_{\\mathrm{w}}, c_{\\mathrm{s}})$, in this exact order:\n- Case A (strong improvement, modest swap cost): $(0.1, 0.4, 0.8, 1.0, 1.5)$.\n- Case B (swap harms mixing, high swap cost): $(0.3, 0.5, 0.4, 1.0, 5.0)$.\n- Case C (equal costs, swap harms mixing): $(0.2, 0.3, 0.5, 2.0, 2.0)$.\n- Case D (equal mixing effectiveness, unequal costs): $(0.25, 0.5, 0.5, 1.5, 4.0)$.\n- Case E (very strong improvement, extremely high swap cost): $(0.05, 0.9, 0.8, 1.0, 200.0)$.\n- Case F (swap beneficial and cheaper): $(0.3, 0.6, 0.6, 2.0, 0.5)$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the optimal frequencies for the test cases, in order, as a comma-separated list enclosed in square brackets. Each optimal frequency must be printed as a decimal with exactly six digits after the decimal point. For example, the format must be like\n$$\n[0.123456,0.000000,1.000000]\n$$\nwith no spaces and no other text.\n\nAngle or physical units are not applicable. All numerical outputs must be unitless floating-point values as specified.", "solution": "We begin from the basic definitions of reversible Markov chains and spectral gap. Let $K_{\\mathrm{w}}$ and $K_{\\mathrm{s}}$ be reversible Markov kernels on the joint state space of Parallel Tempering (PT), both reversible with respect to the PT target. For the single-observable dynamics at inverse temperature $\\beta = 1$, suppose a scalar reduction of the slowest mode dominates the relaxation of that observable. Then, for a mixture $P_f = (1-f) K_{\\mathrm{w}} + f K_{\\mathrm{s}}$ applied as a random-scan composite step with frequency $f \\in [0,1]$, the Dirichlet form is an affine combination of the component Dirichlet forms. In particular, for reversible kernels and any fixed observable subspace, one obtains an effective spectral gap lower bound that is a convex combination of the individual gaps for the dominant mode. Under a single-mode approximation, we write\n$$\ng_{\\mathrm{eff}}(f) = (1-f)\\, g_{\\mathrm{w}} + f \\, g_{\\mathrm{s}},\n$$\nwhere $g_{\\mathrm{w}}$ is the observable-specific spectral gap of the within-replica update at $\\beta = 1$, and $g_{\\mathrm{s}}$ is the effective spectral gap contribution of a swap step for that observable. Since a swap attempt is accepted with probability $a \\in (0,1]$ and, when accepted, produces an additional decorrelation summarized by a factor $\\kappa \\in [0,1]$, we set\n$$\ng_{\\mathrm{s}} = a \\, \\kappa,\n$$\nwhich captures the expected gap contribution of a swap step to the specific observable. This yields\n$$\ng_{\\mathrm{eff}}(f) = (1-f)\\, g_{\\mathrm{w}} + f \\, a \\, \\kappa = g_{\\mathrm{w}} + f \\, (a \\kappa - g_{\\mathrm{w}}).\n$$\nDefine\n$$\nA := a \\kappa - g_{\\mathrm{w}}.\n$$\n\nFor a reversible Markov chain whose slow dynamics for a given observable are well captured by an autoregressive of order $1$ (AR($1$)) approximation, the lag-$1$ correlation can be related to the spectral gap via $r(f) = 1 - g_{\\mathrm{eff}}(f)$. The integrated autocorrelation time (IACT) is then\n$$\n\\tau_{\\mathrm{int}}(f) = \\frac{1 + r(f)}{1 - r(f)} = \\frac{2 - g_{\\mathrm{eff}}(f)}{g_{\\mathrm{eff}}(f)} = \\frac{2}{g_{\\mathrm{eff}}(f)} - 1.\n$$\nWe assume admissibility only if $g_{\\mathrm{eff}}(f)  0$, otherwise the IACT is infinite and such $f$ is suboptimal.\n\nThe computational cost per discrete step is linear in $f$:\n$$\nc(f) = (1-f)\\, c_{\\mathrm{w}} + f \\, c_{\\mathrm{s}} = c_{\\mathrm{w}} + f \\, (c_{\\mathrm{s}} - c_{\\mathrm{w}}).\n$$\nDefine\n$$\nB := c_{\\mathrm{s}} - c_{\\mathrm{w}}.\n$$\n\nThe objective to minimize is the cost-normalized IACT:\n$$\nJ(f) = \\left(\\frac{2}{g_{\\mathrm{eff}}(f)} - 1\\right) c(f) = \\left(\\frac{2}{g_{\\mathrm{w}} + A f} - 1\\right) \\left(c_{\\mathrm{w}} + B f\\right),\n$$\nover the compact interval $f \\in [0,1]$, subject to $g_{\\mathrm{eff}}(f)  0$ for admissibility.\n\nTo analyze $J(f)$, compute the derivative for $g_{\\mathrm{eff}}(f)  0$:\n$$\n\\frac{dJ}{df} = \\frac{d}{df}\\left(\\frac{2 C}{G} - C\\right) = 2\\left(\\frac{C'}{G} - \\frac{C G'}{G^2}\\right) - C',\n$$\nwhere $G := g_{\\mathrm{w}} + A f$, $C := c_{\\mathrm{w}} + B f$, $G' = A$, and $C' = B$. Consequently,\n$$\n\\frac{dJ}{df} = \\frac{2(B G - C A)}{G^2} - B.\n$$\nEquivalently, with a common denominator $G^2$,\n$$\n\\frac{dJ}{df} = \\frac{N(f)}{G^2}, \\quad \\text{where}\n$$\n$$\nN(f) = 2(B G - C A) - B G^2.\n$$\nSubstituting $G = g_{\\mathrm{w}} + A f$ and $C = c_{\\mathrm{w}} + B f$, we obtain an explicit quadratic polynomial for the numerator:\n$$\nN(f) = - B A^2 f^2 - 2 B g_{\\mathrm{w}} A f + \\left(2 B g_{\\mathrm{w}} - 2 A c_{\\mathrm{w}} - B g_{\\mathrm{w}}^2\\right).\n$$\nTherefore, the stationary points in $(0,1)$ are the real roots of $N(f) = 0$ lying in $(0,1)$ and satisfying $G  0$ (admissibility). Special degenerate cases are:\n- If $A = 0$, then $g_{\\mathrm{eff}}(f) \\equiv g_{\\mathrm{w}}$, and $J(f)$ is linear in $f$ because $C(f)$ is linear; the optimum is at a boundary: $f^{\\star} = 0$ if $B \\ge 0$ and $f^{\\star} = 1$ if $B  0$.\n- If $B = 0$, then $c(f) \\equiv c_{\\mathrm{w}}$, and $J(f)$ is monotone in $f$ with derivative\n$$\n\\frac{dJ}{df} = - \\frac{2 c_{\\mathrm{w}} A}{G^2},\n$$\nwhich has constant sign; the optimum is at a boundary: $f^{\\star} = 1$ if $A  0$ and $f^{\\star} = 0$ if $A \\le 0$.\n- Otherwise, solve $N(f) = 0$ (a quadratic or, if the quadratic coefficient vanishes, a linear equation) to obtain interior candidates, filter admissible ones with $G  0$, and evaluate $J(f)$ at all candidates and at $f = 0$ and $f = 1$. Choose the $f$ with the smallest $J(f)$; in ties choose the smaller $f$.\n\nQualitative shape analysis:\n- For $B  0$, the leading coefficient of $N(f)$ is $- B A^2 \\le 0$. Hence $N(f)$ is a concave quadratic and $\\frac{dJ}{df}$, being $N(f)$ divided by the positive $G^2$, changes sign at most twice, with positive derivative between roots and negative outside. Thus any interior stationary point is a local maximum. Therefore, minima occur at the boundaries $f = 0$ or $f = 1$.\n- For $B  0$, the leading coefficient of $N(f)$ is $- B A^2 \\ge 0$. In this case, an interior stationary point can be a local minimum; nevertheless, the global minimum over the compact interval is attained among the boundaries and any admissible interior stationary points.\n\nAlgorithmic prescription:\n- For each tuple $(g_{\\mathrm{w}}, a, \\kappa, c_{\\mathrm{w}}, c_{\\mathrm{s}})$, compute $A = a \\kappa - g_{\\mathrm{w}}$ and $B = c_{\\mathrm{s}} - c_{\\mathrm{w}}$.\n- Define $G(f) = g_{\\mathrm{w}} + A f$ and $J(f) = \\left(\\frac{2}{G(f)} - 1\\right)\\left(c_{\\mathrm{w}} + B f\\right)$.\n- Initialize the candidate set with boundaries $\\{0, 1\\}$.\n- If $|A|$ and $|B|$ are not both below a small threshold, form the coefficients of $N(f)$:\n$$\nn_2 = - B A^2, \\quad n_1 = - 2 B g_{\\mathrm{w}} A, \\quad n_0 = 2 B g_{\\mathrm{w}} - 2 A c_{\\mathrm{w}} - B g_{\\mathrm{w}}^2.\n$$\nSolve $n_2 f^2 + n_1 f + n_0 = 0$:\n  - If $|n_2|  \\varepsilon$ and $|n_1| \\ge \\varepsilon$, solve the linear root $f = - n_0 / n_1$.\n  - If $|n_2| \\ge \\varepsilon$, compute the discriminant $D = n_1^2 - 4 n_2 n_0$; if $D \\ge 0$, add the real roots $f = \\frac{- n_1 \\pm \\sqrt{D}}{2 n_2}$.\nFilter candidates to those in $[0,1]$ with $G(f)  \\varepsilon$.\n- Evaluate $J(f)$ for all admissible candidates and choose $f^{\\star}$ minimizing $J(f)$, tie-breaking by the smaller $f$.\n\nApplying this to the specified test suite yields the following qualitative outcomes:\n- Case A: strong improvement and modest swap cost $\\Rightarrow f^{\\star} = 1$.\n- Case B: swap harms mixing and is expensive $\\Rightarrow f^{\\star} = 0$.\n- Case C: equal costs but swap harms mixing $\\Rightarrow f^{\\star} = 0$.\n- Case D: equal mixing effectiveness, unequal costs $\\Rightarrow f^{\\star} = 0$ (choose cheaper step).\n- Case E: very strong improvement but extremely high swap cost $\\Rightarrow $ boundary comparison favors $f^{\\star} = 0$.\n- Case F: swap beneficial and cheaper $\\Rightarrow f^{\\star} = 1$.\n\nThe program implements the above algorithm, handles degeneracies robustly using a threshold $\\varepsilon = 10^{-12}$, and prints the list $[f^{\\star}_{A}, f^{\\star}_{B}, f^{\\star}_{C}, f^{\\star}_{D}, f^{\\star}_{E}, f^{\\star}_{F}]$ with each entry formatted to six digits after the decimal point, as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef objective_J(f, gw, a, kappa, cw, cs, eps=1e-12):\n    \"\"\"\n    Compute J(f) = tau_int(f) * cost(f) with tau_int(f) = 2/g_eff(f) - 1,\n    g_eff(f) = (1 - f) * gw + f * a * kappa, and cost(f) = (1 - f) * cw + f * cs.\n    Returns np.inf if g_eff(f) = eps (inadmissible).\n    \"\"\"\n    g_eff = gw + (a * kappa - gw) * f\n    if g_eff = eps:\n        return np.inf\n    tau = 2.0 / g_eff - 1.0\n    cost = (1.0 - f) * cw + f * cs\n    return tau * cost\n\ndef stationary_points(gw, a, kappa, cw, cs, eps=1e-12):\n    \"\"\"\n    Solve for interior stationary points of J(f) in (0,1) by setting dJ/df = 0.\n    Based on the numerator quadratic N(f) = 0, where:\n        N(f) = -B A^2 f^2 - 2 B gw A f + (2 B gw - 2 A cw - B gw^2)\n    Returns a list of candidate roots (not filtered for domain or admissibility).\n    \"\"\"\n    A = a * kappa - gw\n    B = cs - cw\n\n    # If both A and B are near zero, derivative is approximately zero everywhere: no interior candidates.\n    if abs(A)  eps and abs(B)  eps:\n        return []\n\n    # Coefficients of N(f) = n2 f^2 + n1 f + n0 = 0\n    n2 = -B * (A ** 2)\n    n1 = -2.0 * B * gw * A\n    n0 = 2.0 * B * gw - 2.0 * A * cw - B * (gw ** 2)\n\n    candidates = []\n    if abs(n2)  eps:\n        # Linear case: n1 f + n0 = 0\n        if abs(n1) = eps:\n            f_lin = -n0 / n1\n            candidates.append(f_lin)\n        # If both n2 and n1 ~ 0, then N(f) ~ n0, so derivative sign is constant: no interior stationary point.\n    else:\n        # Quadratic case\n        D = n1 * n1 - 4.0 * n2 * n0\n        # Numerical safety: clamp tiny negative due to rounding\n        if D = -1e-18:\n            D = max(D, 0.0)\n            sqrtD = np.sqrt(D)\n            denom = 2.0 * n2\n            f1 = (-n1 + sqrtD) / denom\n            f2 = (-n1 - sqrtD) / denom\n            candidates.extend([f1, f2])\n\n    return candidates\n\ndef optimal_frequency(gw, a, kappa, cw, cs, eps=1e-12):\n    \"\"\"\n    Compute the optimal swap frequency f* in [0,1] minimizing J(f).\n    Consider boundaries and any admissible interior stationary points.\n    Tie-break by choosing the smaller f.\n    \"\"\"\n    A = a * kappa - gw\n\n    # Collect candidate points: boundaries\n    cands = [0.0, 1.0]\n\n    # Add stationary points\n    for f in stationary_points(gw, a, kappa, cw, cs, eps=eps):\n        if np.isfinite(f):\n            cands.append(f)\n\n    # Filter candidates to [0,1] and admissible g_eff  eps\n    def admissible(f):\n        if f  0.0 - 1e-12 or f  1.0 + 1e-12:\n            return False\n        # Project tiny out-of-range numerical noise\n        f_proj = min(max(f, 0.0), 1.0)\n        g_eff = gw + (a * kappa - gw) * f_proj\n        return g_eff  eps\n\n    filtered = []\n    for f in cands:\n        # Project minor numerical drift into [0,1]\n        f_proj = min(max(f, 0.0), 1.0)\n        if admissible(f_proj):\n            filtered.append(f_proj)\n\n    # Deduplicate with tolerance\n    unique = []\n    for f in filtered:\n        if not any(abs(f - u)  1e-10 for u in unique):\n            unique.append(f)\n\n    # Evaluate objective and pick minimum; tie-break by smaller f\n    best_f = None\n    best_J = np.inf\n    for f in unique:\n        J = objective_J(f, gw, a, kappa, cw, cs, eps=eps)\n        if (J  best_J - 1e-14) or (abs(J - best_J) = 1e-14 and (best_f is None or f  best_f)):\n            best_J = J\n            best_f = f\n\n    # Safety fallback (should not trigger): if no admissible candidate, choose f=0.0\n    if best_f is None:\n        best_f = 0.0\n\n    # Clip to [0,1]\n    best_f = min(max(best_f, 0.0), 1.0)\n    return best_f\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each is a tuple: (g_w, a, kappa, c_w, c_s)\n    test_cases = [\n        (0.1, 0.4, 0.8, 1.0, 1.5),    # Case A\n        (0.3, 0.5, 0.4, 1.0, 5.0),    # Case B\n        (0.2, 0.3, 0.5, 2.0, 2.0),    # Case C\n        (0.25, 0.5, 0.5, 1.5, 4.0),   # Case D\n        (0.05, 0.9, 0.8, 1.0, 200.0), # Case E\n        (0.3, 0.6, 0.6, 2.0, 0.5),    # Case F\n    ]\n\n    results = []\n    for gw, a, kappa, cw, cs in test_cases:\n        f_star = optimal_frequency(gw, a, kappa, cw, cs, eps=1e-12)\n        results.append(f_star)\n\n    # Final print statement in the exact required format, six decimals, no spaces.\n    print(\"[\" + \",\".join(f\"{r:.6f}\" for r in results) + \"]\")\n\nsolve()\n```", "id": "3326598"}]}