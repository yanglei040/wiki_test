{"hands_on_practices": [{"introduction": "Before we can optimize an algorithm, we must first understand its fundamental mechanics. The acceptance probability is the heart of the Metropolis-Hastings method, and its expected value is a key measure of sampler performance. This first practice [@problem_id:3353623] guides you through a foundational, exact calculation of the expected acceptance rate for a Gaussian target, providing a concrete understanding of how it depends on the proposal scale and dimension before we move to high-dimensional approximations.", "problem": "Consider the Haario-Saksman-Tamminen Adaptive Metropolis (AM) algorithm at a fixed iteration where the proposal is isotropic with scale $s>0$. Let the target distribution be the $d$-dimensional standard normal $\\pi = \\mathcal{N}(0, I_{d})$, and suppose the current state $x$ is distributed according to stationarity $x \\sim \\pi$. The proposal is given by $y = x + s Z$, where $Z \\sim \\mathcal{N}(0, I_{d})$ is independent of $x$. For the Metropolis-Hastings (MH) acceptance probability $\\alpha(x,y) = \\min\\{1, \\pi(y)/\\pi(x)\\}$ for symmetric proposals, derive an exact expression for the expected acceptance probability $\\mathbb{E}[\\alpha(x,y)]$ first as an integral over $Z \\in \\mathbb{R}^{d}$, and then reduce it to a single one-dimensional integral over the proposal radius $R = \\|Z\\|$. You must base your derivation on the core definitions of the Metropolis-Hastings algorithm, properties of multivariate normal distributions, and rotational invariance. Express your final result as a single analytic one-dimensional integral in the variable $r \\ge 0$, involving only standard functions. Your final answer must be a single closed-form expression. Do not provide a numerical approximation.", "solution": "The objective is to derive an exact expression for the expected acceptance probability $\\mathbb{E}[\\alpha(x,y)]$ for a Metropolis-Hastings step.\n\nThe problem specifies the following:\n- Target distribution: $\\pi(x) = \\mathcal{N}(0, I_d)$, a $d$-dimensional standard normal distribution. Its probability density function (PDF) is $\\pi(x) = (2\\pi)^{-d/2} \\exp(-\\frac{1}{2}\\|x\\|^2)$.\n- Current state: $x$ is drawn from the stationary distribution, so $x \\sim \\pi$.\n- Proposal: $y = x + sZ$, where $s > 0$ is a fixed scale and $Z$ is a random variable.\n- Proposal noise: $Z \\sim \\mathcal{N}(0, I_d)$, independent of $x$. The PDF of $Z$ is $p(z) = (2\\pi)^{-d/2} \\exp(-\\frac{1}{2}\\|z\\|^2)$.\n- Acceptance probability: For a symmetric proposal, $\\alpha(x,y) = \\min\\{1, \\frac{\\pi(y)}{\\pi(x)}\\}$. The proposal $y \\sim \\mathcal{N}(x, s^2 I_d)$ is indeed symmetric.\n\nThe expected acceptance probability is the expectation of $\\alpha(x,y)$ over the joint distribution of $x$ and $Z$.\n$$\n\\mathbb{E}[\\alpha(x,y)] = \\mathbb{E}_{x,Z}[\\min\\{1, \\frac{\\pi(x+sZ)}{\\pi(x)}\\}]\n$$\nThe expectation is taken with respect to $x \\sim \\mathcal{N}(0, I_d)$ and $Z \\sim \\mathcal{N}(0, I_d)$.\n\nFirst, we simplify the ratio of the target densities:\n$$\n\\frac{\\pi(y)}{\\pi(x)} = \\frac{(2\\pi)^{-d/2} \\exp(-\\frac{1}{2}\\|y\\|^2)}{(2\\pi)^{-d/2} \\exp(-\\frac{1}{2}\\|x\\|^2)} = \\exp\\left(-\\frac{1}{2}(\\|y\\|^2 - \\|x\\|^2)\\right)\n$$\nSubstitute $y = x + sZ$:\n$$\n\\|y\\|^2 = \\|x + sZ\\|^2 = (x + sZ)^T(x + sZ) = x^T x + 2s(x^T Z) + s^2(Z^T Z) = \\|x\\|^2 + 2s(x^T Z) + s^2 \\|Z\\|^2\n$$\nThe difference in squared norms is:\n$$\n\\|y\\|^2 - \\|x\\|^2 = 2s(x^T Z) + s^2 \\|Z\\|^2\n$$\nThus, the ratio becomes:\n$$\n\\frac{\\pi(y)}{\\pi(x)} = \\exp\\left(-\\frac{1}{2}(2s(x^T Z) + s^2 \\|Z\\|^2)\\right) = \\exp\\left(-s(x^T Z) - \\frac{s^2}{2}\\|Z\\|^2\\right)\n$$\nThe expectation is therefore:\n$$\n\\mathbb{E}[\\alpha(x,y)] = \\mathbb{E}_{x,Z}\\left[\\min\\left\\{1, \\exp\\left(-s(x^T Z) - \\frac{s^2}{2}\\|Z\\|^2\\right)\\right\\}\\right]\n$$\nWe can compute this expectation by iterated expectation, first conditioning on $Z$. Let $z$ be a realization of $Z$.\n$$\n\\mathbb{E}[\\alpha(x,y)] = \\mathbb{E}_Z\\left[ \\mathbb{E}_x\\left[ \\min\\left\\{1, \\exp\\left(-s(x^T z) - \\frac{s^2}{2}\\|z\\|^2\\right)\\right\\} \\bigg| Z=z \\right] \\right]\n$$\nLet's analyze the inner expectation. For a fixed vector $z \\in \\mathbb{R}^d$, the term $W = x^T z$ is a linear combination of the components of $x$, which are independent standard normal random variables. Thus, $W$ is a normal random variable. Its mean and variance are:\n$$\n\\mathbb{E}[W] = \\mathbb{E}[x^T z] = \\mathbb{E}[x]^T z = 0^T z = 0\n$$\n$$\n\\text{Var}(W) = \\text{Var}(x^T z) = z^T \\text{Cov}(x) z = z^T I_d z = \\|z\\|^2\n$$\nSo, conditional on $Z=z$, the scalar random variable $W = x^T z$ is distributed as $\\mathcal{N}(0, \\|z\\|^2)$. Let $R = \\|z\\|$. The conditional distribution of $W$ is $\\mathcal{N}(0, R^2)$, with PDF $p_W(w) = \\frac{1}{\\sqrt{2\\pi R^2}} \\exp(-\\frac{w^2}{2R^2})$.\n\nThe inner expectation, which we denote $I(R)$, is a function of $R=\\|z\\|$:\n$$\nI(R) = \\int_{-\\infty}^{\\infty} \\min\\left\\{1, \\exp\\left(-sw - \\frac{s^2 R^2}{2}\\right)\\right\\} p_W(w) dw\n$$\nThe argument of the exponential is non-positive, i.e., $\\exp(\\dots) \\le 1$, when $-sw - \\frac{s^2 R^2}{2} \\le 0$, which is equivalent to $w \\ge -\\frac{sR^2}{2}$. Let $w_0 = -\\frac{sR^2}{2}$. The integral splits at $w_0$:\n$$\nI(R) = \\int_{-\\infty}^{w_0} 1 \\cdot p_W(w) dw + \\int_{w_0}^{\\infty} \\exp\\left(-sw - \\frac{s^2 R^2}{2}\\right) p_W(w) dw\n$$\nThe first term is the cumulative distribution function (CDF) of $W \\sim \\mathcal{N}(0, R^2)$ evaluated at $w_0$:\n$$\n\\int_{-\\infty}^{w_0} p_W(w) dw = P(W \\le w_0) = P\\left(\\frac{W}{R} \\le \\frac{w_0}{R}\\right) = \\Phi\\left(\\frac{-sR^2/2}{R}\\right) = \\Phi\\left(-\\frac{sR}{2}\\right)\n$$\nwhere $\\Phi(\\cdot)$ is the CDF of the standard normal distribution $\\mathcal{N}(0,1)$.\n\nFor the second term, we combine the exponents in the integrand:\n$$\n\\exp\\left(-sw - \\frac{s^2 R^2}{2}\\right) \\exp\\left(-\\frac{w^2}{2R^2}\\right) = \\exp\\left(-\\frac{w^2 + 2sR^2 w + s^2 R^4}{2R^2}\\right) = \\exp\\left(-\\frac{(w + sR^2)^2}{2R^2}\\right)\n$$\nThe second integral is:\n$$\n\\int_{w_0}^{\\infty} \\frac{1}{\\sqrt{2\\pi R^2}} \\exp\\left(-\\frac{(w + sR^2)^2}{2R^2}\\right) dw\n$$\nThis is the tail probability $P(U \\ge w_0)$ for a random variable $U \\sim \\mathcal{N}(-sR^2, R^2)$. Standardizing $U$:\n$$\nP(U \\ge w_0) = P\\left(\\frac{U - (-sR^2)}{R} \\ge \\frac{w_0 - (-sR^2)}{R}\\right) = P\\left(\\mathcal{N}(0,1) \\ge \\frac{-sR^2/2 + sR^2}{R}\\right)\n$$\n$$\n= P\\left(\\mathcal{N}(0,1) \\ge \\frac{sR^2/2}{R}\\right) = P\\left(\\mathcal{N}(0,1) \\ge \\frac{sR}{2}\\right) = 1 - \\Phi\\left(\\frac{sR}{2}\\right)\n$$\nUsing the symmetry property $\\Phi(-u) = 1 - \\Phi(u)$, this term is equal to $\\Phi(-sR/2)$.\nCombining the two parts, the conditional expectation is:\n$$\nI(R) = \\Phi\\left(-\\frac{sR}{2}\\right) + \\Phi\\left(-\\frac{sR}{2}\\right) = 2\\Phi\\left(-\\frac{sR}{2}\\right)\n$$\nNow we can write the total expectation by unconditioning on $Z$. The result $I(R)$ depends only on the magnitude $\\|z\\|$.\n$$\n\\mathbb{E}[\\alpha(x,y)] = \\mathbb{E}_Z\\left[2\\Phi\\left(-\\frac{s\\|Z\\|}{2}\\right)\\right] = \\int_{\\mathbb{R}^d} 2\\Phi\\left(-\\frac{s\\|z\\|}{2}\\right) p(z) dz\n$$\nThis is the expression for the expected acceptance probability as an integral over $Z \\in \\mathbb{R}^d$:\n$$\n\\mathbb{E}[\\alpha(x,y)] = \\int_{\\mathbb{R}^d} 2\\Phi\\left(-\\frac{s\\|z\\|}{2}\\right) (2\\pi)^{-d/2} \\exp\\left(-\\frac{\\|z\\|^2}{2}\\right) dz\n$$\nTo reduce this to a one-dimensional integral over the proposal radius $R = \\|Z\\|$, we need the distribution of $R$. Since $Z \\sim \\mathcal{N}(0, I_d)$, its squared norm $R^2 = \\|Z\\|^2 = \\sum_{i=1}^d Z_i^2$ follows a chi-squared distribution with $d$ degrees of freedom, $R^2 \\sim \\chi^2_d$. The variable $R = \\|Z\\|$ follows a chi distribution with $d$ degrees of freedom. The PDF of $R$, for $r > 0$, is:\n$$\np_R(r) = \\frac{2 r^{d-1} \\exp(-r^2/2)}{2^{d/2} \\Gamma(d/2)}\n$$\nwhere $\\Gamma(\\cdot)$ is the gamma function.\n\nThe expectation is now an integral over this one-dimensional distribution:\n$$\n\\mathbb{E}[\\alpha(x,y)] = \\mathbb{E}_R\\left[2\\Phi\\left(-\\frac{sR}{2}\\right)\\right] = \\int_0^{\\infty} 2\\Phi\\left(-\\frac{sr}{2}\\right) p_R(r) dr\n$$\nSubstituting the PDF $p_R(r)$:\n$$\n\\mathbb{E}[\\alpha(x,y)] = \\int_0^{\\infty} 2\\Phi\\left(-\\frac{sr}{2}\\right) \\frac{2 r^{d-1} \\exp(-r^2/2)}{2^{d/2} \\Gamma(d/2)} dr\n$$\nSimplifying the constant terms:\n$$\n\\mathbb{E}[\\alpha(x,y)] = \\frac{4}{2^{d/2} \\Gamma(d/2)} \\int_0^{\\infty} \\Phi\\left(-\\frac{sr}{2}\\right) r^{d-1} \\exp\\left(-\\frac{r^2}{2}\\right) dr\n$$\n$$\n= \\frac{2^2 \\cdot 2^{-d/2}}{\\Gamma(d/2)} \\int_0^{\\infty} \\Phi\\left(-\\frac{sr}{2}\\right) r^{d-1} \\exp\\left(-\\frac{r^2}{2}\\right) dr\n$$\n$$\n= \\frac{2^{2-d/2}}{\\Gamma(d/2)} \\int_0^{\\infty} \\Phi\\left(-\\frac{sr}{2}\\right) r^{d-1} \\exp\\left(-\\frac{r^2}{2}\\right) dr\n$$\nThis is the final expression as a single one-dimensional integral over $r \\ge 0$. The function $\\Phi(u)$ itself represents an integral, $\\Phi(u) = (2\\pi)^{-1/2}\\int_{-\\infty}^u \\exp(-t^2/2) dt$, but is considered a standard function.", "answer": "$$\n\\boxed{\\frac{2^{2-d/2}}{\\Gamma(d/2)} \\int_0^{\\infty} \\Phi\\left(-\\frac{sr}{2}\\right) r^{d-1} \\exp\\left(-\\frac{r^2}{2}\\right) dr}\n$$", "id": "3353623"}, {"introduction": "With a foundational understanding of the acceptance probability, we can now address the central question of optimal tuning. This exercise [@problem_id:3353620] tackles the two critical aspects of proposal design: its shape and its size. By combining an elegant affine-invariance argument with the principle of maximizing sampler efficiency in the high-dimensional limit, you will derive the celebrated result that motivates the entire adaptive Metropolis framework: the proposal covariance should mimic the target's, with a step size that scales as $s_d \\approx 2.38/\\sqrt{d}$.", "problem": "Consider the Haario-Saksman-Tamminen Adaptive Metropolis (AM) algorithm targeting a $d$-dimensional zero-mean Gaussian distribution with covariance matrix $\\Sigma_{\\star}$, that is, a target density $\\pi(x)$ proportional to $\\exp\\!\\big(-\\tfrac{1}{2} x^{\\top}\\Sigma_{\\star}^{-1}x\\big)$ for $x\\in\\mathbb{R}^{d}$. At each iteration, the AM algorithm uses a symmetric Gaussian random-walk proposal of the form $q(x,\\cdot)=\\mathcal{N}\\!\\big(x,\\;S\\big)$, where $S$ is a positive-definite proposal covariance matrix that is adapted over time.\n\nStarting from the foundational properties of the Metropolis–Hastings algorithm and the affine invariance of Gaussian targets under linear transformations, argue—without invoking any specific optimal-scaling formula—that the asymptotically optimal structure of $S$ is a scalar multiple of $\\Sigma_{\\star}$, namely $S=s_{d}^{2}\\Sigma_{\\star}$ for some scalar $s_{d}>0$ depending on the dimension $d$. Then, using the diffusion-limit heuristic for high-dimensional Random Walk Metropolis (RWM), derive the limiting form of the acceptance probability as a function of the rescaled step size parameter and, by maximizing the expected squared jump distance, determine the optimal scalar $s_{d}$ as an explicit function of $d$.\n\nYour final answer must be a single closed-form analytic expression for $s_{d}$ in terms of $d$. Round the numerical prefactor to three significant figures. No physical units are involved in this problem.", "solution": "The problem asks for two main results regarding the Adaptive Metropolis (AM) algorithm for a Gaussian target. First, to deduce the optimal structure of the proposal covariance matrix $S$ using an affine invariance argument. Second, to derive the optimal scaling factor for this covariance matrix in the high-dimensional limit.\n\n### Part 1: Optimal Structure of the Proposal Covariance\n\nThe target density is $\\pi(x) \\propto \\exp(-\\frac{1}{2} x^{\\top}\\Sigma_{\\star}^{-1}x)$ for $x \\in \\mathbb{R}^d$. The proposal is a Gaussian random walk, $x' = x + \\xi$, where $\\xi \\sim \\mathcal{N}(0, S)$. The Metropolis-Hastings acceptance probability is $\\alpha(x, x') = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x)}\\right)$.\n\nWe invoke the principle of affine invariance. The efficiency of the sampling algorithm should not depend on the choice of basis for the state space $\\mathbb{R}^d$. Consider an arbitrary invertible linear transformation $y = Ax$, where $A$ is a $d \\times d$ matrix.\nIf $x \\sim \\mathcal{N}(0, \\Sigma_{\\star})$, then the transformed variable $y$ follows a Gaussian distribution $y \\sim \\mathcal{N}(0, A\\Sigma_{\\star}A^{\\top})$. The target density for $y$ is $\\pi_y(y) \\propto \\exp(-\\frac{1}{2} y^{\\top}(A\\Sigma_{\\star}A^{\\top})^{-1}y)$.\n\nThe proposal mechanism in the $y$-space corresponding to the one in the $x$-space is $y' = Ax' = A(x+\\xi) = y + A\\xi$. The proposal increment in the $y$-space is $\\xi_y = A\\xi$. Since $\\xi \\sim \\mathcal{N}(0, S)$, the transformed increment follows $\\xi_y \\sim \\mathcal{N}(0, ASA^{\\top})$. So, the proposal covariance in the $y$-space is $S_y = ASA^{\\top}$.\n\nThe acceptance probability for the original chain is a function of the log-ratio of target densities:\n$$ \\ln\\left(\\frac{\\pi(x')}{\\pi(x)}\\right) = -\\frac{1}{2}\\left( (x+\\xi)^{\\top}\\Sigma_{\\star}^{-1}(x+\\xi) - x^{\\top}\\Sigma_{\\star}^{-1}x \\right) = -x^{\\top}\\Sigma_{\\star}^{-1}\\xi - \\frac{1}{2}\\xi^{\\top}\\Sigma_{\\star}^{-1}\\xi $$\nThe acceptance probability for the transformed chain depends on the analogous quantity:\n$$ \\ln\\left(\\frac{\\pi_y(y')}{\\pi_y(y)}\\right) = -y^{\\top}(A\\Sigma_{\\star}A^{\\top})^{-1}\\xi_y - \\frac{1}{2}\\xi_y^{\\top}(A\\Sigma_{\\star}A^{\\top})^{-1}\\xi_y $$\nSubstituting $y = Ax$, $\\xi_y = A\\xi$, and $(A\\Sigma_{\\star}A^{\\top})^{-1} = (A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}$:\n$$ \\ln\\left(\\frac{\\pi_y(y')}{\\pi_y(y)}\\right) = -(Ax)^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}(A\\xi) - \\frac{1}{2}(A\\xi)^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}(A\\xi) $$\n$$ = -x^{\\top}A^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}A\\xi - \\frac{1}{2}\\xi^{\\top}A^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}A\\xi $$\n$$ = -x^{\\top}\\Sigma_{\\star}^{-1}\\xi - \\frac{1}{2}\\xi^{\\top}\\Sigma_{\\star}^{-1}\\xi = \\ln\\left(\\frac{\\pi(x')}{\\pi(x)}\\right) $$\nThe acceptance probabilities are identical. This means that algorithm performance metrics (like acceptance rate and autocorrelation time) are invariant under affine transformations, provided the proposal covariance is transformed accordingly as $S \\to ASA^{\\top}$.\n\nFor an adaptive algorithm that learns the proposal covariance $S$, it is natural to require that the optimal learned covariance respects this invariance. The problem becomes simplest in the coordinate system where the target distribution is uncorrelated and has unit variance in all directions, i.e., it is isotropic. We can achieve this by choosing a transformation matrix $A$ such that $A\\Sigma_{\\star}A^{\\top} = I_d$, the $d \\times d$ identity matrix. For instance, we can use a whitening transformation $A = \\Sigma_{\\star}^{-1/2}$. In this whitened space, the target is $\\mathcal{N}(0, I_d)$.\n\nFor an isotropic target, there is no reason to prefer one direction over another. Any efficient exploration of the state space should also be isotropic. Therefore, the optimal proposal covariance in the whitened space must be proportional to the identity matrix: $S_y = c I_d$ for some scalar $c > 0$.\n\nTransforming this optimal proposal back to the original $x$-space gives the structure of the optimal $S$:\n$S_y = ASA^{\\top} \\implies S = A^{-1} S_y (A^{\\top})^{-1}$\nUsing $A = \\Sigma_{\\star}^{-1/2}$ (so $A^{-1} = \\Sigma_{\\star}^{1/2}$) and $S_y = c I_d$:\n$$ S = \\Sigma_{\\star}^{1/2} (c I_d) (\\Sigma_{\\star}^{-1/2})^{\\top} = c \\Sigma_{\\star}^{1/2} (\\Sigma_{\\star}^{1/2})^{\\top} = c \\Sigma_{\\star} $$\nThus, the asymptotically optimal proposal covariance matrix $S$ must be a scalar multiple of the target covariance matrix $\\Sigma_{\\star}$. We denote the positive scalar constant of proportionality by $s_d^2$, so $S = s_d^2 \\Sigma_{\\star}$.\n\n### Part 2: Optimal Scaling Factor $s_d$\n\nBased on the argument above, we can analyze the algorithm's performance in the standardized space where the target is $\\pi(y) \\propto \\exp(-\\frac{1}{2}y^{\\top}y)$ and the proposal is $\\mathcal{N}(y, s_d^2 I_d)$, and the results will apply to the general case. We use $x$ instead of $y$ for simplicity in this section. The proposal is $x' = x + \\xi$ with $\\xi \\sim \\mathcal{N}(0, s_d^2 I_d)$.\n\nWe use the high-dimensional ($d \\to \\infty$) diffusion-limit heuristic. For the acceptance rate to remain non-zero, the step size must vanish. We use the scaling ansatz $s_d = \\lambda / \\sqrt{d}$ for some constant $\\lambda > 0$. The proposal is thus $\\xi \\sim \\mathcal{N}(0, (\\lambda^2/d)I_d)$.\n\nThe change in the log-target is $\\Delta E(x, \\xi) = \\frac{1}{2}\\|x\\|^2 - \\frac{1}{2}\\|x'\\|^2 = -x^{\\top}\\xi - \\frac{1}{2}\\|\\xi\\|^2$. The acceptance probability is $\\alpha(x, x') = \\min(1, \\exp(\\Delta E))$. Let's analyze the two terms in $\\Delta E$:\n1.  By the Law of Large Numbers, as $d \\to \\infty$, $\\|\\xi\\|^2 = \\sum_{i=1}^d \\xi_i^2$ converges to its expectation:\n    $$ E[\\|\\xi\\|^2] = E\\left[\\sum_{i=1}^d \\xi_i^2\\right] = d \\cdot E[\\xi_1^2] = d \\cdot \\text{Var}(\\xi_1) = d \\cdot \\frac{\\lambda^2}{d} = \\lambda^2 $$\n    So, $\\|\\xi\\|^2 \\to \\lambda^2$.\n2.  The term $x^{\\top}\\xi = \\sum_{i=1}^d x_i \\xi_i$ is a sum of i.i.d. terms, where $x_i \\sim \\mathcal{N}(0, 1)$ (since $x$ is from the stationary distribution) and $\\xi_i \\sim \\mathcal{N}(0, \\lambda^2/d)$. Each term has mean $E[x_i\\xi_i] = E[x_i]E[\\xi_i] = 0$ and variance $\\text{Var}(x_i\\xi_i) = E[x_i^2]E[\\xi_i^2] = 1 \\cdot (\\lambda^2/d) = \\lambda^2/d$.\n    The variance of the sum is $\\text{Var}(x^{\\top}\\xi) = d \\cdot (\\lambda^2/d) = \\lambda^2$. By the Central Limit Theorem, $x^{\\top}\\xi$ converges in distribution to a normal random variable $Z \\sim \\mathcal{N}(0, \\lambda^2)$.\n\nIn the limit $d\\to\\infty$, the acceptance probability becomes a deterministic function of $\\lambda$, obtained by averaging over the limiting distributions of the random components:\n$$ \\alpha(\\lambda) = E_Z[\\min(1, \\exp(-Z - \\frac{1}{2}\\lambda^2))] $$\nLet $Y = -Z - \\frac{1}{2}\\lambda^2$. Since $Z \\sim \\mathcal{N}(0, \\lambda^2)$, $Y \\sim \\mathcal{N}(-\\frac{1}{2}\\lambda^2, \\lambda^2)$. The expectation is\n$$ \\alpha(\\lambda) = \\int_{-\\infty}^{\\infty} \\min(1, e^y) p(y) dy = \\int_{-\\infty}^{0} e^y p(y) dy + \\int_{0}^{\\infty} p(y) dy $$\nwhere $p(y)$ is the density of $Y$. Let $\\Phi$ be the standard normal CDF. The second term is $P(Y > 0) = P\\left(\\mathcal{N}(-\\frac{1}{2}\\lambda^2, \\lambda^2) > 0\\right) = P\\left(\\mathcal{N}(0, 1) > \\frac{\\lambda}{2}\\right) = 1 - \\Phi(\\lambda/2) = \\Phi(-\\lambda/2)$.\nIt can be shown that the first integral also evaluates to $\\Phi(-\\lambda/2)$. Thus, the limiting acceptance rate is\n$$ \\alpha(\\lambda) = 2\\Phi(-\\lambda/2) $$\n\nTo find the optimal $\\lambda$, we maximize the sampler's efficiency, which is proportional to the Expected Squared Jump Distance (ESJD) per iteration. The jump is $x_{n+1}-x_n$, which is $\\xi$ on acceptance and $0$ on rejection.\n$$ \\text{ESJD} = E[\\|x_{n+1}-x_n\\|^2] = E[\\alpha(x_n, x_n') \\|\\xi\\|^2] $$\nIn the high-dimensional limit, the acceptance probability and the norm of the proposal step become asymptotically independent.\n$$ \\text{ESJD}(\\lambda) \\approx E[\\alpha(x_n, x_n')] E[\\|\\xi\\|^2] = \\alpha(\\lambda) E[\\|\\xi\\|^2] $$\nAs shown earlier, $E[\\|\\xi\\|^2] = \\lambda^2$. So we want to maximize the function $f(\\lambda) = \\lambda^2 \\alpha(\\lambda) = 2\\lambda^2 \\Phi(-\\lambda/2)$. We find the maximum by setting the derivative with respect to $\\lambda$ to zero:\n$$ \\frac{df}{d\\lambda} = \\frac{d}{d\\lambda} \\left[2\\lambda^2 \\Phi(-\\lambda/2)\\right] = 4\\lambda\\Phi(-\\lambda/2) + 2\\lambda^2 \\frac{d}{d\\lambda}\\Phi(-\\lambda/2) = 0 $$\nUsing the chain rule, and letting $\\phi$ be the standard normal PDF, $\\frac{d}{d\\lambda}\\Phi(-\\lambda/2) = \\phi(-\\lambda/2) \\cdot (-\\frac{1}{2}) = -\\frac{1}{2}\\phi(\\lambda/2)$ since $\\phi$ is an even function.\n$$ 4\\lambda\\Phi(-\\lambda/2) - \\lambda^2\\phi(\\lambda/2) = 0 $$\nFor $\\lambda > 0$, we can divide by $\\lambda$:\n$$ 4\\Phi(-\\lambda/2) = \\lambda\\phi(\\lambda/2) $$\nThis is a transcendental equation for the optimal $\\lambda$, which we label $\\lambda_{opt}$. The equation cannot be solved in terms of elementary functions. Numerical solution yields $\\lambda_{opt} \\approx 2.379$. The question requires rounding to three significant figures, so $\\lambda_{opt} \\approx 2.38$.\n\nThe optimal scalar $s_d$ is then given by our scaling ansatz:\n$$ s_d = \\frac{\\lambda_{opt}}{\\sqrt{d}} \\approx \\frac{2.38}{\\sqrt{d}} $$\nThis provides the explicit functional dependence of $s_d$ on the dimension $d$.", "answer": "$$ \\boxed{\\frac{2.38}{\\sqrt{d}}} $$", "id": "3353620"}, {"introduction": "Theory provides the \"what,\" but practice is concerned with the \"how.\" The Adaptive Metropolis algorithm, while statistically efficient, can be computationally demanding in high dimensions due to its reliance on matrix operations at each iteration. This final practice [@problem_id:3353639] moves from statistical theory to computational reality, asking you to perform a detailed complexity analysis of the algorithm's linear algebra workload. By deriving the per-iteration cost under practical cost-saving strategies, you will gain crucial insight into the trade-offs required to make adaptive MCMC a feasible tool for large-scale problems.", "problem": "Consider the Adaptive Metropolis (AM) algorithm of Haario, Saksman, and Tamminen for a Markov Chain Monte Carlo (MCMC) method targeting a distribution on $\\mathbb{R}^{d}$ with state $X_{n} \\in \\mathbb{R}^{d}$ at iteration $n$. The AM proposal has the form $Y_{n} = X_{n} + s L_{n} Z_{n}$, where $s > 0$ is a scalar, $Z_{n} \\sim \\mathcal{N}(0, I_{d})$, and $L_{n}$ is a Cholesky factor so that $C_{n} = L_{n} L_{n}^{\\top}$ approximates the target covariance. The covariance update is based on the empirical covariance of the chain and a Robbins-Monro step-size $\\eta_{n} \\in (0,1)$ with a small regularization on the diagonal. Let the mean update be $ \\mu_{n+1} = \\mu_{n} + \\eta_{n} (X_{n} - \\mu_{n})$ and the covariance update be\n$$\nC_{n+1} = (1 - \\eta_{n}) C_{n} + \\eta_{n} \\left( (X_{n} - \\mu_{n})(X_{n} - \\mu_{n})^{\\top} \\right) + \\eta_{n} \\epsilon I_{d},\n$$\nwith $\\epsilon > 0$ a small constant. Assume a standard floating-point cost model in which a matrix-vector multiplication by a dense $d \\times d$ matrix costs $2 d^{2}$ flops, a multiplication by a lower-triangular $d \\times d$ matrix costs exactly $d^{2}$ flops, a dense outer product of two $d$-vectors costs $d^{2}$ flops, addition or scaling of a $d \\times d$ matrix costs $d^{2}$ flops, addition or scaling of a $d$-vector costs $d$ flops, and a Cholesky factorization of a dense symmetric positive definite $d \\times d$ matrix costs $\\frac{1}{3} d^{3}$ flops. Ignore random number generation costs and target density evaluation costs; focus only on the linear-algebraic work of proposal generation and adaptation.\n\nTwo cost-reduction strategies are proposed:\n\n- Periodic covariance factorization: Instead of recomputing $L_{n}$ at every iteration, recompute the full Cholesky factorization of $C_{n}$ once every $m \\in \\mathbb{N}$ iterations and reuse the most recent $L_{n}$ in between. Treat the Cholesky cost amortized over $m$ iterations as $\\frac{1}{3} d^{3} / m$ flops per iteration.\n\n- Low-rank covariance approximation: Maintain an approximation $C_{n} \\approx D_{n} + U_{n} U_{n}^{\\top}$ where $D_{n} \\in \\mathbb{R}^{d \\times d}$ is diagonal and $U_{n} \\in \\mathbb{R}^{d \\times r}$ has rank $r$ with $1 \\le r \\ll d$. For proposal generation, draw $Z_{r} \\sim \\mathcal{N}(0, I_{r})$ and $Z_{d} \\sim \\mathcal{N}(0, I_{d})$, then compute $Y_{n} = X_{n} + s \\left( U_{n} Z_{r} + \\sqrt{D_{n}} \\odot Z_{d} \\right)$, where $\\odot$ denotes elementwise multiplication and $\\sqrt{D_{n}}$ denotes elementwise square roots of the diagonal entries. Use the following operation counts for this low-rank proposal: computing $U_{n} Z_{r}$ costs $2 d r$ flops, computing $\\sqrt{D_{n}} \\odot Z_{d}$ costs $d$ flops, vector additions and scalings add $3 d$ flops, so the total proposal generation cost is $2 d r + 4 d$ flops. For the low-rank covariance update, assume the following steps and costs per iteration: compute $v_{n} = X_{n} - \\mu_{n}$ in $d$ flops, update $\\mu_{n+1} = \\mu_{n} + \\eta_{n} v_{n}$ in $2 d$ flops, update $D_{n+1}$ from the diagonal of the rank-one term in $2 d$ flops, and update $U_{n+1}$ via a rank-$r$ incremental projection and augmentation costing $2 d r + r^{2}$ flops. The total low-rank adaptation cost per iteration is therefore $2 d r + r^{2} + 5 d$ flops. Continue to perform full Cholesky factorizations of $C_{n}$ periodically every $m$ iterations to prevent long-term drift, so the amortized periodic cost $\\frac{1}{3} d^{3} / m$ applies as above.\n\nStarting from these fundamental definitions and cost models, derive, in closed form, the amortized per-iteration flop count, denoted $C_{\\mathrm{lr}}(d, m, r)$, when both strategies are used jointly: low-rank proposal and covariance updates every iteration, and full Cholesky recomputation every $m$ iterations. Express your final answer as a single analytic expression in terms of $d$, $m$, and $r$. Do not simplify to big-$\\mathcal{O}$ notation; give the exact expression under the stated flop model. No numerical rounding is required, and no units are to be reported in the answer.", "solution": "The Adaptive Metropolis algorithm constructs proposals by drawing from a Gaussian distribution with covariance proportional to the current estimate $C_{n}$. The computational burden per iteration comes from three sources: proposal generation using a factor of the covariance, mean and covariance adaptation updates, and occasional refactorization of the covariance to obtain a new Cholesky factor. To derive a closed-form expression for the amortized per-iteration flop count using both periodic factorization and low-rank covariance approximation, we sum the contributions from each component.\n\nThe fundamental AM proposal in a full-rank setting is $Y_{n} = X_{n} + s L_{n} Z_{n}$ with $C_{n} = L_{n} L_{n}^{\\top}$ and $Z_{n} \\sim \\mathcal{N}(0, I_{d})$. The cost of multiplying a lower-triangular $d \\times d$ matrix by a $d$-vector is exactly $d^{2}$ flops (there are $d(d+1)/2$ multiplications and $d(d-1)/2$ additions), and scaling and adding $d$-vectors add $2 d$ flops, so the full-rank proposal sampling cost is $d^{2} + 2 d$ flops. The mean update $\\mu_{n+1} = \\mu_{n} + \\eta_{n} (X_{n} - \\mu_{n})$ is a vector operation requiring computation of $v_{n} = X_{n} - \\mu_{n}$ in $d$ flops and scaling plus addition in $2 d$ flops, totaling $3 d$ flops. The covariance update\n$$\nC_{n+1} = (1 - \\eta_{n}) C_{n} + \\eta_{n} \\left( v_{n} v_{n}^{\\top} \\right) + \\eta_{n} \\epsilon I_{d}\n$$\nrequires the following operations: compute the outer product $v_{n} v_{n}^{\\top}$ in $d^{2}$ flops, scale $C_{n}$ by $(1 - \\eta_{n})$ in $d^{2}$ flops, add the two $d \\times d$ matrices in $d^{2}$ flops, and add the diagonal regularization in $d$ flops. Together with the $3 d$ flops for the mean update, the total adaptation cost in the full-rank case is $3 d^{2} + 3 d$ flops per iteration. If one recomputes the Cholesky factorization $L_{n}$ every iteration, the cost per iteration would also include $\\frac{1}{3} d^{3}$ flops for the Cholesky factorization; however, with periodic factorization every $m$ iterations, the amortized Cholesky cost per iteration is $\\frac{1}{3} d^{3} / m$ flops.\n\nWe now analyze the low-rank strategy. The covariance is approximated by $C_{n} \\approx D_{n} + U_{n} U_{n}^{\\top}$ where $D_{n}$ is diagonal and $U_{n}$ has $r$ columns. The proposal uses two independent standard normals, $Z_{r} \\sim \\mathcal{N}(0, I_{r})$ and $Z_{d} \\sim \\mathcal{N}(0, I_{d})$, and computes\n$$\nY_{n} = X_{n} + s \\left( U_{n} Z_{r} + \\sqrt{D_{n}} \\odot Z_{d} \\right).\n$$\nThe cost for $U_{n} Z_{r}$, a dense $d \\times r$ matrix times an $r$-vector, is $2 d r$ flops; $\\sqrt{D_{n}} \\odot Z_{d}$ is a diagonal scaling costing $d$ flops; the three $d$-vector operations (addition of the two components, scaling by $s$, and addition to $X_{n}$) cost $3 d$ flops. Therefore, proposal generation under the low-rank approximation costs $2 d r + 4 d$ flops per iteration.\n\nFor the low-rank covariance update, we retain the mean update cost $3 d$ flops as before, with $v_{n} = X_{n} - \\mu_{n}$ computed in $d$ flops and scaling plus addition in $2 d$ flops. The diagonal $D_{n}$ captures the variance not represented in the low-rank part; updating $D_{n}$ with the diagonal of the rank-one term $\\eta_{n} v_{n} v_{n}^{\\top}$ requires elementwise operations on $d$ entries, modeled as $2 d$ flops (one for computing $v_{n} \\odot v_{n}$ and one for scaling and addition). Updating the factor $U_{n}$ to reflect the off-diagonal structure of the covariance can be done with an incremental rank-$r$ procedure (for example, a projection of $v_{n}$ onto the current span of $U_{n}$ followed by a correction and truncation), whose dominant costs are a $d \\times r$ by $r$-vector multiplication and associated small $r \\times r$ operations. We model this as $2 d r + r^{2}$ flops per iteration. Summing these pieces, the low-rank adaptation cost per iteration is\n$$\n\\text{adapt}_{\\mathrm{lr}}(d, r) = d + 2 d + 2 d + (2 d r + r^{2}) = 2 d r + r^{2} + 5 d.\n$$\n\nTo prevent drift from accumulating in the low-rank approximation, we still perform periodic full Cholesky factorizations of the current $C_{n}$ every $m$ iterations. The amortized cost of this periodic factorization is $\\frac{1}{3} d^{3} / m$ flops per iteration.\n\nCombining the three components for the joint strategy—low-rank proposal generation, low-rank adaptation, and amortized periodic full Cholesky factorization—the total amortized per-iteration flop count is\n$$\nC_{\\mathrm{lr}}(d, m, r) = \\underbrace{(2 d r + 4 d)}_{\\text{proposal}} + \\underbrace{(2 d r + r^{2} + 5 d)}_{\\text{adaptation}} + \\underbrace{\\frac{1}{3} \\frac{d^{3}}{m}}_{\\text{amortized Cholesky}}.\n$$\nSimplifying,\n$$\nC_{\\mathrm{lr}}(d, m, r) = 4 d r + r^{2} + 9 d + \\frac{1}{3} \\frac{d^{3}}{m}.\n$$\n\nThis is the closed-form amortized per-iteration flop count under the stated flop model when using a rank-$r$ low-rank proposal and covariance update, together with periodic full Cholesky recomputation every $m$ iterations. It makes explicit the trade-offs: lowering $r$ reduces the $4 d r + r^{2}$ term, and increasing $m$ reduces the $\\frac{1}{3} \\frac{d^{3}}{m}$ term, while the linear term $9 d$ reflects unavoidable $d$-vector operations per iteration.", "answer": "$$\\boxed{4 d r + r^{2} + 9 d + \\frac{d^{3}}{3 m}}$$", "id": "3353639"}]}