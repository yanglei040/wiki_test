{"hands_on_practices": [{"introduction": "Before diving into complex error reduction techniques, it's crucial to build a solid intuition for the fundamental behavior of the standard Monte Carlo estimator. This first practice invites you to empirically verify the celebrated $n^{-1/2}$ convergence rate of the Root Mean Squared Error (RMSE). By implementing a simulation across various integrands and distributions, you will see firsthand how the Central Limit Theorem governs the precision of Monte Carlo integration, a cornerstone of error analysis. [@problem_id:3306236]", "problem": "Consider Monte Carlo integration of a function $f$ under a probability distribution of a real-valued random variable $X$. Let $\\mu = \\mathbb{E}[f(X)]$ and for each integer $n \\ge 1$, let $\\hat{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n f(X_i)$ where $\\{X_i\\}_{i=1}^n$ are independent and identically distributed copies of $X$. Define the root mean squared error $\\operatorname{RMSE}(n)$ of the estimator $\\hat{\\mu}_n$ as $\\operatorname{RMSE}(n) = \\sqrt{\\mathbb{E}\\big[(\\hat{\\mu}_n - \\mu)^2\\big]}$. Assume throughout that $\\mathbb{E}[f(X)^2]$ is finite.\n\nYour task is to design a program that estimates the empirical decay rate of $\\operatorname{RMSE}(n)$ as a function of $n$ by simulation. Use the following fundamental base:\n- The linearity of expectation for independent and identically distributed random variables.\n- The additivity of variance for independent random variables.\n- The definition of the sample mean and mean squared error.\n\nAlgorithmic specification:\n- For each test case described below, fix a list of sample sizes $\\mathcal{N}$ and a replication count $R$. For each $n \\in \\mathcal{N}$, generate $R$ independent Monte Carlo estimators $\\hat{\\mu}_n$ by drawing $n$ independent and identically distributed samples of $X$ per replication, evaluating $f$ on those samples, and averaging. Use a single pool of $R \\times n_{\\max}$ samples to efficiently obtain all $\\hat{\\mu}_n$ across $n \\in \\mathcal{N}$, where $n_{\\max} = \\max \\mathcal{N}$, by taking the first $n$ samples per replication for each $n$. Compute the empirical root mean squared error as $\\widehat{\\operatorname{RMSE}}(n) = \\sqrt{\\frac{1}{R}\\sum_{r=1}^R (\\hat{\\mu}_n^{(r)} - \\mu)^2}$, where $\\mu$ is known analytically for each test case below.\n- Estimate the scaling exponent $\\alpha$ for the relation $\\widehat{\\operatorname{RMSE}}(n) \\approx C n^{\\alpha}$ by performing an ordinary least squares fit of $\\log \\widehat{\\operatorname{RMSE}}(n)$ versus $\\log n$ across all $n \\in \\mathcal{N}$. Report the slope $\\alpha$.\n\nAngle convention: any appearance of $\\sin(\\cdot)$ must interpret its argument in radians.\n\nTest suite (three test cases):\n1. Case A (bounded integrand under a Gaussian law):\n   - Distribution: $X \\sim \\mathcal{N}(0,1)$.\n   - Integrand: $f(x) = \\exp(-x^2)$.\n   - Analytical mean: $\\mu = \\mathbb{E}[\\exp(-X^2)] = \\frac{1}{\\sqrt{1 + 2 \\cdot 1}} = \\frac{1}{\\sqrt{3}}$.\n   - Replications: $R = 4096$.\n   - Sample sizes: $\\mathcal{N} = \\{100, 400, 1600\\}$.\n   - Random seed: $s = 13579$.\n\n2. Case B (heavy-tailed sampling with finite integrand variance):\n   - Distribution: $X \\sim t_{\\nu}$ with degrees of freedom $\\nu = 6$.\n   - Integrand: $f(x) = x^2$.\n   - Analytical mean: $\\mu = \\mathbb{E}[X^2] = \\frac{\\nu}{\\nu - 2} = \\frac{6}{4} = 1.5$.\n   - Replications: $R = 4096$.\n   - Sample sizes: $\\mathcal{N} = \\{100, 400, 1600\\}$.\n   - Random seed: $s = 24680$.\n\n3. Case C (skewed law with mixed oscillatory-linear integrand):\n   - Distribution: $X \\sim \\operatorname{Exponential}(\\lambda)$ with rate $\\lambda = 1$ (mean $1$).\n   - Integrand: $f(x) = \\sin(x) + x$ with $x$ in radians.\n   - Analytical mean: since $\\mathbb{E}[e^{iX}] = \\frac{1}{1 - i}$ for $X \\sim \\operatorname{Exponential}(1)$, we have $\\mathbb{E}[\\sin(X)] = \\operatorname{Im}\\big(\\frac{1}{1 - i}\\big) = \\frac{1}{2}$ and $\\mathbb{E}[X] = 1$, hence $\\mu = \\frac{1}{2} + 1 = 1.5$.\n   - Replications: $R = 4096$.\n   - Sample sizes: $\\mathcal{N} = \\{100, 400, 1600\\}$.\n   - Random seed: $s = 98765$.\n\nOutput specification:\n- For each test case, compute the slope $\\alpha$ of the least squares fit of $\\log \\widehat{\\operatorname{RMSE}}(n)$ versus $\\log n$ over $\\mathcal{N}$.\n- Your program should produce a single line of output containing the three slopes for Cases A, B, and C, respectively, as a comma-separated list enclosed in square brackets, for example, $[\\alpha_A,\\alpha_B,\\alpha_C]$.\n- The outputs must be real numbers (floats). No other text or units should be printed.\n\nAll random number generation must be deterministic given the specified seeds. No user input is required. The program must be self-contained and must not access external resources.", "solution": "The task is to perform an empirical error analysis for Monte Carlo integration and estimate the convergence rate of the root mean squared error. The theoretical foundation for this analysis rests upon fundamental principles of probability theory, namely the Law of Large Numbers and the Central Limit Theorem.\n\nLet $X$ be a real-valued random variable with a given probability distribution, and let $f$ be a real-valued function. We are interested in computing the expectation $\\mu = \\mathbb{E}[f(X)]$. The Monte Carlo method approximates this value by drawing $n$ independent and identically distributed (i.i.d.) samples $\\{X_1, X_2, \\dots, X_n\\}$ from the distribution of $X$, and then computing the sample mean of the function evaluated at these points:\n$$\n\\hat{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n f(X_i)\n$$\nThis estimator $\\hat{\\mu}_n$ is itself a random variable, as its value depends on the random samples drawn. To assess its quality, we analyze its statistical properties.\n\nFirst, we determine the bias of the estimator. By the linearity of expectation, and since each $X_i$ has the same distribution as $X$:\n$$\n\\mathbb{E}[\\hat{\\mu}_n] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^n f(X_i)\\right] = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}[f(X_i)] = \\frac{1}{n}\\sum_{i=1}^n \\mu = \\frac{n\\mu}{n} = \\mu\n$$\nSince $\\mathbb{E}[\\hat{\\mu}_n] = \\mu$, the estimator is unbiased.\n\nNext, we analyze the variance of the estimator. Let $\\sigma_f^2 = \\operatorname{Var}(f(X)) = \\mathbb{E}[f(X)^2] - (\\mathbb{E}[f(X)])^2 = \\mathbb{E}[f(X)^2] - \\mu^2$. The problem states that $\\mathbb{E}[f(X)^2]$ is finite, which ensures $\\sigma_f^2$ is also finite. Since the samples $\\{X_i\\}$ are independent, the function values $\\{f(X_i)\\}$ are also independent random variables. For independent variables, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}(\\hat{\\mu}_n) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n f(X_i)\\right) = \\frac{1}{n^2}\\operatorname{Var}\\left(\\sum_{i=1}^n f(X_i)\\right) = \\frac{1}{n^2}\\sum_{i=1}^n \\operatorname{Var}(f(X_i)) = \\frac{1}{n^2} \\sum_{i=1}^n \\sigma_f^2 = \\frac{n\\sigma_f^2}{n^2} = \\frac{\\sigma_f^2}{n}\n$$\nThe problem defines the estimator's error using the Root Mean Squared Error (RMSE), $\\operatorname{RMSE}(n) = \\sqrt{\\mathbb{E}[(\\hat{\\mu}_n - \\mu)^2]}$. The quantity inside the square root is the Mean Squared Error (MSE), which can be decomposed into variance and squared bias:\n$$\n\\operatorname{MSE}(n) = \\mathbb{E}[(\\hat{\\mu}_n - \\mu)^2] = \\operatorname{Var}(\\hat{\\mu}_n) + (\\mathbb{E}[\\hat{\\mu}_n] - \\mu)^2\n$$\nSince the estimator is unbiased, its bias is $0$. Therefore, the MSE is equal to the variance:\n$$\n\\operatorname{MSE}(n) = \\frac{\\sigma_f^2}{n}\n$$\nTaking the square root gives the theoretical RMSE:\n$$\n\\operatorname{RMSE}(n) = \\sqrt{\\frac{\\sigma_f^2}{n}} = \\frac{\\sigma_f}{\\sqrt{n}} = \\sigma_f n^{-1/2}\n$$\nThis result shows that the RMSE of the Monte Carlo estimator decays with the sample size $n$ as $n^{-1/2}$. This corresponds to the general form $\\operatorname{RMSE}(n) = C n^{\\alpha}$ with a constant $C = \\sigma_f$ and a decay exponent $\\alpha = -1/2$.\n\nThe task requires estimating this exponent $\\alpha$ empirically. This is done by simulating the estimation process multiple times. For each sample size $n$ from a given set $\\mathcal{N}$, we generate $R$ independent Monte Carlo estimates, denoted $\\{\\hat{\\mu}_n^{(1)}, \\hat{\\mu}_n^{(2)}, \\dots, \\hat{\\mu}_n^{(R)}\\}$. We then compute an empirical estimate of the RMSE, denoted $\\widehat{\\operatorname{RMSE}}(n)$, which approximates the true $\\operatorname{RMSE}(n)$:\n$$\n\\widehat{\\operatorname{RMSE}}(n) = \\sqrt{\\frac{1}{R}\\sum_{r=1}^R (\\hat{\\mu}_n^{(r)} - \\mu)^2}\n$$\nHere, $\\mu$ is the known analytical mean of $f(X)$, which is provided for each test case.\n\nTo estimate $\\alpha$, we assume the relationship $\\widehat{\\operatorname{RMSE}}(n) \\approx C n^{\\alpha}$ holds for our empirical data. By taking the natural logarithm of both sides, we obtain a linear relationship:\n$$\n\\log(\\widehat{\\operatorname{RMSE}}(n)) \\approx \\log(C) + \\alpha \\log(n)\n$$\nThis corresponds to a linear equation $y = m x + c$, where $y = \\log(\\widehat{\\operatorname{RMSE}}(n))$, the slope $m = \\alpha$, the independent variable $x = \\log(n)$, and the intercept $c = \\log(C)$. We can therefore estimate $\\alpha$ by performing an ordinary least squares (OLS) linear regression on the set of data points $\\{(\\log(n), \\log(\\widehat{\\operatorname{RMSE}}(n)))\\}$ for all $n \\in \\mathcal{N}$. The slope of the resulting regression line is our empirical estimate of $\\alpha$.\n\nThe algorithmic procedure is as follows:\n1. For each test case, initialize a random number generator with the specified seed for reproducibility.\n2. Generate a single large block of random samples of size $R \\times n_{\\max}$, where $R$ is the number of replications and $n_{\\max}$ is the maximum sample size in $\\mathcal{N}$. Let this matrix of samples be $S$.\n3. Apply the integrand function $f$ element-wise to $S$ to obtain a matrix of function values, $F$.\n4. For each sample size $n \\in \\mathcal{N}$:\n   a. Extract the first $n$ columns of $F$, corresponding to $R$ sets of $n$ function evaluations.\n   b. For each of the $R$ rows, compute the mean to obtain the vector of $R$ estimates $\\{\\hat{\\mu}_n^{(r)}\\}_{r=1}^R$.\n   c. Compute the empirical RMSE, $\\widehat{\\operatorname{RMSE}}(n)$, using the provided formula and the known true mean $\\mu$.\n5. After computing $\\widehat{\\operatorname{RMSE}}(n)$ for all $n \\in \\mathcal{N}$, create two vectors: one with the logarithms of the sample sizes, $\\log(n)$, and another with the logarithms of the empirical RMSEs, $\\log(\\widehat{\\operatorname{RMSE}}(n))$.\n6. Perform a linear regression on these two vectors to find the slope, which is the desired estimate for $\\alpha$.\n7. Repeat this process for all test cases and report the resulting slopes.\nThe theoretically expected value for $\\alpha$ in all cases is $-0.5$, provided the variance $\\sigma_f^2$ is finite and non-zero. The empirical results should be close to this value.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress, t as t_dist\n\ndef run_simulation_and_regression(params):\n    \"\"\"\n    Runs a Monte Carlo simulation for a single test case and computes the RMSE decay rate.\n\n    Args:\n        params (dict): A dictionary containing all parameters for the test case.\n\n    Returns:\n        float: The estimated decay exponent alpha.\n    \"\"\"\n    R = params['R']\n    N_list = params['N_list']\n    mu = params['mu']\n    seed = params['seed']\n    integrand = params['integrand']\n    sampler = params['sampler']\n    \n    # Set the random seed for reproducibility using a Generator object\n    rng = np.random.default_rng(seed)\n    \n    n_max = max(N_list)\n    \n    # Generate the base pool of random samples\n    # For scipy.stats distributions, the `random_state` argument accepts a Generator.\n    samples = sampler(rng, (R, n_max))\n    \n    # Evaluate the integrand on all samples\n    f_samples = integrand(samples)\n    \n    rmses = []\n    for n in N_list:\n        # Take the first n samples for each replication\n        f_samples_n = f_samples[:, :n]\n        \n        # Compute the R Monte Carlo estimators\n        mu_hat_n_r = np.mean(f_samples_n, axis=1)\n        \n        # Compute the empirical RMSE\n        # RMSE_hat(n) = sqrt(1/R * sum_{r=1 to R} (mu_hat_n^(r) - mu)^2)\n        rmse_n = np.sqrt(np.mean((mu_hat_n_r - mu)**2))\n        rmses.append(rmse_n)\n        \n    # Perform ordinary least squares fit on the log-log data\n    # log(RMSE_hat(n)) vs log(n)\n    log_N = np.log(N_list)\n    log_rmse = np.log(rmses)\n    \n    # linregress returns (slope, intercept, r_value, p_value, stderr)\n    regression_result = linregress(log_N, log_rmse)\n    \n    # The slope of the log-log plot is the scaling exponent alpha\n    alpha = regression_result.slope\n    return alpha\n\ndef solve():\n    \"\"\"\n    Defines and runs all test cases, then prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        {\n            # Case A: Bounded integrand, Gaussian law\n            'sampler': lambda rng, size: rng.normal(loc=0.0, scale=1.0, size=size),\n            'integrand': lambda x: np.exp(-x**2),\n            'mu': 1.0 / np.sqrt(3.0),\n            'R': 4096,\n            'N_list': [100, 400, 1600],\n            'seed': 13579\n        },\n        {\n            # Case B: Heavy-tailed sampling, finite integrand variance\n            'sampler': lambda rng, size: t_dist.rvs(df=6, size=size, random_state=rng),\n            'integrand': lambda x: x**2,\n            'mu': 1.5,\n            'R': 4096,\n            'N_list': [100, 400, 1600],\n            'seed': 24680\n        },\n        {\n            # Case C: Skewed law, mixed integrand\n            'sampler': lambda rng, size: rng.exponential(scale=1.0, size=size),\n            'integrand': lambda x: np.sin(x) + x,\n            'mu': 1.5,\n            'R': 4096,\n            'N_list': [100, 400, 1600],\n            'seed': 98765\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha = run_simulation_and_regression(case)\n        results.append(alpha)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3306236"}, {"introduction": "While standard Monte Carlo is robust, its efficiency can be dramatically improved using variance reduction techniques like importance sampling. This exercise focuses on the widely used self-normalized importance sampler, which introduces a subtle but critical feature: finite-sample bias. You will write a simulation to empirically decompose the Mean Squared Error (MSE) into its variance and squared bias components, providing a concrete illustration of the bias-variance tradeoff inherent in many advanced statistical estimators. [@problem_id:3306258]", "problem": "Consider estimating the expectation of a function under an unnormalized target density using self-normalized importance sampling. Let the target density on the real line be defined up to a normalizing constant by the unnormalized density $\\tilde{\\pi}(x) = \\exp\\!\\left(-\\tfrac{x^{4}}{2}\\right)$ for all real $x$. The normalized target density is $\\pi(x) = \\tilde{\\pi}(x)/Z$, where the normalizing constant $Z$ is $Z = \\int_{-\\infty}^{\\infty} \\tilde{\\pi}(x)\\,dx$. The quantity of interest is the expectation $\\mu = \\mathbb{E}_{\\pi}[f(X)]$ with $f(x) = x^{2}$, that is,\n$$\n\\mu = \\frac{\\int_{-\\infty}^{\\infty} x^{2}\\,\\tilde{\\pi}(x)\\,dx}{\\int_{-\\infty}^{\\infty} \\tilde{\\pi}(x)\\,dx}.\n$$\nIn importance sampling with proposal density $q(x)$, the self-normalized importance sampling estimator with $N$ samples is\n$$\n\\hat{\\mu}_{\\mathrm{SNIS}} = \\frac{\\sum_{i=1}^{N} w_{i} f(x_{i})}{\\sum_{i=1}^{N} w_{i}},\n$$\nwhere $x_{i} \\sim q$ independently and the unnormalized importance weights are $w_{i} = \\tilde{\\pi}(x_{i})/q(x_{i})$. This estimator is biased for finite $N$ because it is a ratio of random sums.\n\nStarting from the fundamental definitions of expectation, variance, mean squared error, and importance sampling, write a complete, runnable program that:\n- Computes a high-accuracy numerical approximation of the ground-truth value $\\mu$ using numerical quadrature over the real line for both the numerator and denominator integrals.\n- For each test case in the test suite below, generates $R$ independent replicates of $\\hat{\\mu}_{\\mathrm{SNIS}}$ using the specified proposal $q$ and sample size $N$, with a fixed pseudorandom number generator seed for reproducibility. For a given replicate $r$ in $\\{1,\\dots,R\\}$, draw $N$ independent samples $x_{i}^{(r)} \\sim q$, compute the corresponding weights $w_{i}^{(r)} = \\tilde{\\pi}(x_{i}^{(r)})/q(x_{i}^{(r)})$, and form the self-normalized estimator $\\hat{\\mu}_{r} = \\left(\\sum_{i=1}^{N} w_{i}^{(r)} f(x_{i}^{(r)})\\right)\\Big/\\left(\\sum_{i=1}^{N} w_{i}^{(r)}\\right)$.\n- Computes the empirical mean squared error, empirical variance, and squared empirical bias across the $R$ replicates:\n  - The empirical mean squared error is\n    $$\n    \\widehat{\\mathrm{MSE}} = \\frac{1}{R} \\sum_{r=1}^{R} \\left(\\hat{\\mu}_{r} - \\mu\\right)^{2}.\n    $$\n  - The empirical variance uses the population normalization,\n    $$\n    \\widehat{\\mathrm{Var}} = \\frac{1}{R} \\sum_{r=1}^{R} \\left(\\hat{\\mu}_{r} - \\bar{\\mu}\\right)^{2}, \\quad \\text{where} \\quad \\bar{\\mu} = \\frac{1}{R}\\sum_{r=1}^{R} \\hat{\\mu}_{r}.\n    $$\n  - The squared empirical bias is\n    $$\n    \\widehat{\\mathrm{Bias}}^{2} = \\left(\\bar{\\mu} - \\mu\\right)^{2}.\n    $$\n- Verifies the decomposition by reporting $\\widehat{\\mathrm{MSE}}$, $\\widehat{\\mathrm{Var}}$, and $\\widehat{\\mathrm{Bias}}^{2}$ (the sum $\\widehat{\\mathrm{Var}} + \\widehat{\\mathrm{Bias}}^{2}$ should numerically match $\\widehat{\\mathrm{MSE}}$ up to Monte Carlo error).\n\nUse the following scientifically sound and diverse test suite of cases, specified by $(N, R, \\text{proposal}, \\text{degrees-of-freedom}, \\text{seed})$:\n- Case $1$: $(N = 1, R = 50000, \\text{proposal} = \\text{Normal}, \\text{degrees-of-freedom} = \\text{None}, \\text{seed} = 12345)$. This is a boundary case illustrating maximal finite-sample bias since $\\hat{\\mu}_{\\mathrm{SNIS}}$ reduces to $f(x)$ when $N = 1$.\n- Case $2$: $(N = 20, R = 10000, \\text{proposal} = \\text{Normal}, \\text{degrees-of-freedom} = \\text{None}, \\text{seed} = 23456)$. This is a moderate-sample case with a light-tailed proposal.\n- Case $3$: $(N = 50, R = 5000, \\text{proposal} = \\text{Student-t}, \\text{degrees-of-freedom} = 3, \\text{seed} = 34567)$. This case uses a heavier-tailed proposal to examine variance reduction in the tails.\n- Case $4$: $(N = 200, R = 2000, \\text{proposal} = \\text{Normal}, \\text{degrees-of-freedom} = \\text{None}, \\text{seed} = 45678)$. This is a larger-sample case for the light-tailed proposal.\n\nFor the proposal densities $q(x)$, use:\n- The standard Normal proposal with density $q(x) = (2\\pi)^{-1/2}\\exp\\!\\left(-x^{2}/2\\right)$.\n- The Student-$t$ proposal with $\\nu$ degrees of freedom, with density\n$$\nq(x) = \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu\\pi}\\,\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)}\\left(1 + \\frac{x^{2}}{\\nu}\\right)^{-\\frac{\\nu+1}{2}},\n$$\nfor real $x$.\n\nYour program should produce a single line of output containing the results for all cases as a list of triples in the following format:\n$$\n\\big[\\,[\\widehat{\\mathrm{MSE}}_{1},\\,\\widehat{\\mathrm{Var}}_{1},\\,\\widehat{\\mathrm{Bias}}^{2}_{1}],\\,[\\widehat{\\mathrm{MSE}}_{2},\\,\\widehat{\\mathrm{Var}}_{2},\\,\\widehat{\\mathrm{Bias}}^{2}_{2}],\\,[\\widehat{\\mathrm{MSE}}_{3},\\,\\widehat{\\mathrm{Var}}_{3},\\,\\widehat{\\mathrm{Bias}}^{2}_{3}],\\,[\\widehat{\\mathrm{MSE}}_{4},\\,\\widehat{\\mathrm{Var}}_{4},\\,\\widehat{\\mathrm{Bias}}^{2}_{4}]\\,\\big].\n$$\nAll numerical results must be reported as floating-point numbers in the order of the test suite specified above, with no additional text printed.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of Monte Carlo integration and error analysis, is well-posed with a clear and complete set of instructions, and is expressed in objective, formal language. There are no contradictions, ambiguities, or factual errors.\n\nThe solution is implemented by following a structured, principle-based approach.\n\n### Step 1: Ground Truth Calculation\nThe true value of the quantity of interest, $\\mu = \\mathbb{E}_{\\pi}[f(X)]$, is required as a baseline for computing the bias and mean squared error of the estimator. The function is $f(x) = x^2$ and the unnormalized target density is $\\tilde{\\pi}(x) = \\exp(-x^4/2)$. By definition, the expectation is the ratio of two integrals:\n$$\n\\mu = \\frac{\\int_{-\\infty}^{\\infty} x^2 \\tilde{\\pi}(x)\\,dx}{\\int_{-\\infty}^{\\infty} \\tilde{\\pi}(x)\\,dx} = \\frac{\\int_{-\\infty}^{\\infty} x^2 \\exp(-x^4/2)\\,dx}{\\int_{-\\infty}^{\\infty} \\exp(-x^4/2)\\,dx}\n$$\nThese integrals do not have simple closed-form anti-derivatives in terms of elementary functions, but they are related to the Gamma function, $\\Gamma(z) = \\int_0^\\infty t^{z-1} e^{-t} dt$. The denominator integral is $Z = 2^{-3/4}\\Gamma(1/4)$ and the numerator integral is $2^{-1/4}\\Gamma(3/4)$. Thus, $\\mu = \\sqrt{2}\\Gamma(3/4)/\\Gamma(1/4)$.\nFor implementation, a more direct and high-precision approach is to use numerical quadrature. The `scipy.integrate.quad` function is employed to compute both definite integrals over the domain $(-\\infty, \\infty)$. The ratio of the resulting numerical values provides a high-accuracy approximation of $\\mu$.\n\n### Step 2: Monte Carlo Simulation Design\nFor each test case, specified by the sample size $N$, number of replicates $R$, proposal distribution $q(x)$, and a random seed, we perform a Monte Carlo simulation to analyze the statistical properties of the self-normalized importance sampling (SNIS) estimator. The overall process consists of $R$ independent repetitions, or replicates, of the same estimation procedure. Using a fixed seed for the pseudo-random number generator ensures that the results are reproducible.\n\n### Step 3: Generation of a Single SNIS Estimate\nWithin each of the $R$ replicates, indexed by $r \\in \\{1, \\dots, R\\}$, a single estimate $\\hat{\\mu}_r$ is generated as follows:\n1.  **Sampling**: $N$ independent and identically distributed samples, $\\{x_i^{(r)}\\}_{i=1}^N$, are drawn from the specified proposal density $q(x)$. The problem uses two types of proposals: the standard Normal distribution, $\\mathcal{N}(0, 1)$, and the Student-$t$ distribution with a specified number of degrees of freedom, $\\nu$.\n2.  **Weight Calculation**: For each sample $x_i^{(r)}$, an unnormalized importance weight $w_i^{(r)}$ is calculated. The weight is the ratio of the unnormalized target density to the proposal density, evaluated at the sample point:\n    $$\n    w_i^{(r)} = \\frac{\\tilde{\\pi}(x_i^{(r)})}{q(x_i^{(r)})} = \\frac{\\exp\\left(-\\frac{(x_i^{(r)})^4}{2}\\right)}{q(x_i^{(r)})}\n    $$\n3.  **Estimator Construction**: The SNIS estimator $\\hat{\\mu}_r$ is computed as the weighted average of the function values $f(x_i^{(r)}) = (x_i^{(r)})^2$, using the calculated weights:\n    $$\n    \\hat{\\mu}_{r} = \\frac{\\sum_{i=1}^{N} w_{i}^{(r)} f(x_{i}^{(r)})}{\\sum_{i=1}^{N} w_{i}^{(r)}}\n    $$\nThis process is repeated $R$ times to generate a collection of estimates $\\{\\hat{\\mu}_1, \\hat{\\mu}_2, \\dots, \\hat{\\mu}_R\\}$.\n\n### Step 4: Empirical Error Analysis\nWith the set of $R$ estimates, we quantify the performance of the estimator. The key statistical metrics are computed empirically from this sample of estimates.\n1.  **Empirical Mean**: The average of the $R$ estimates is calculated:\n    $$\n    \\bar{\\mu} = \\frac{1}{R} \\sum_{r=1}^{R} \\hat{\\mu}_r\n    $$\n2.  **Squared Empirical Bias**: The squared difference between the empirical mean $\\bar{\\mu}$ and the true value $\\mu$ provides an estimate of the squared bias of the estimator:\n    $$\n    \\widehat{\\mathrm{Bias}}^2 = (\\bar{\\mu} - \\mu)^2\n    $$\n3.  **Empirical Variance**: The sample variance of the estimates is computed (using the population form with $1/R$ normalization as specified):\n    $$\n    \\widehat{\\mathrm{Var}} = \\frac{1}{R} \\sum_{r=1}^{R} (\\hat{\\mu}_r - \\bar{\\mu})^2\n    $$\n4.  **Empirical Mean Squared Error (MSE)**: The MSE is estimated by averaging the squared differences between each estimate and the true value $\\mu$:\n    $$\n    \\widehat{\\mathrm{MSE}} = \\frac{1}{R} \\sum_{r=1}^{R} (\\hat{\\mu}_r - \\mu)^2\n    $$\nThese three quantities are connected by the fundamental algebraic identity $\\widehat{\\mathrm{MSE}} = \\widehat{\\mathrm{Var}} + \\widehat{\\mathrm{Bias}}^2$. Their numerical computation serves to verify this decomposition and provides insight into how the total error of the SNIS estimator is split between its variance and its finite-sample bias for different choices of $N$ and $q(x)$.\n\n### Step 5: Implementation Details\nThe algorithm is implemented in Python using the `numpy` and `scipy` libraries.\n-   The ground truth $\\mu$ is computed once at the beginning.\n-   A main loop iterates through the specified test cases. \n-   Inside a simulation function for each case, `numpy.random.default_rng` is used for reproducible random number generation. The inner loop over the $N$ samples for each replicate is vectorized using `numpy` arrays for computational efficiency. The `scipy.stats` module provides the necessary probability density functions (`pdf`) and random variate samplers (`rvs`, or in this case, direct `rng` methods) for both the Normal and Student-$t$ proposal distributions.\n-   The final results for each case, consisting of the triple $[\\widehat{\\mathrm{MSE}}, \\widehat{\\mathrm{Var}}, \\widehat{\\mathrm{Bias}}^2]$, are collected and formatted into a single string as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import integrate\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the self-normalized importance sampling problem.\n    1. Computes the ground-truth value mu.\n    2. Iterates through test cases to run Monte Carlo simulations.\n    3. For each case, computes and stores the empirical MSE, variance, and squared bias.\n    4. Prints the final results in the specified format.\n    \"\"\"\n\n    # Define the function of interest f(x) = x^2 and the unnormalized target density pi_tilde(x).\n    f_x = lambda x: x**2\n    pi_tilde = lambda x: np.exp(-x**4 / 2)\n\n    def calculate_ground_truth():\n        \"\"\"\n        Computes the ground truth value mu = E_pi[f(X)] using numerical quadrature.\n        \"\"\"\n        numerator_integrand = lambda x: f_x(x) * pi_tilde(x)\n        denominator_integrand = pi_tilde\n\n        numerator_val, _ = integrate.quad(numerator_integrand, -np.inf, np.inf)\n        denominator_val, _ = integrate.quad(denominator_integrand, -np.inf, np.inf)\n\n        if denominator_val == 0:\n            raise ValueError(\"Normalizing constant Z is zero.\")\n            \n        return numerator_val / denominator_val\n\n    def run_simulation(N, R, proposal_type, dof, seed, mu_true):\n        \"\"\"\n        Runs the SNIS simulation for a single test case.\n\n        Args:\n            N (int): Number of samples per replicate.\n            R (int): Number of replicates.\n            proposal_type (str): Type of proposal distribution ('Normal' or 'Student-t').\n            dof (int or None): Degrees of freedom for Student-t proposal.\n            seed (int): Seed for the random number generator.\n            mu_true (float): The ground-truth value of the expectation.\n\n        Returns:\n            list: A list containing [MSE, Var, Bias^2].\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        mu_hat_replicates = np.zeros(R)\n\n        for r in range(R):\n            if proposal_type == 'Normal':\n                x_samples = rng.normal(size=N)\n                q_pdf_vals = stats.norm.pdf(x_samples)\n            elif proposal_type == 'Student-t':\n                x_samples = rng.standard_t(df=dof, size=N)\n                q_pdf_vals = stats.t.pdf(x_samples, df=dof)\n            else:\n                raise ValueError(\"Unknown proposal type specified.\")\n\n            pi_tilde_vals = pi_tilde(x_samples)\n            f_vals = f_x(x_samples)\n            \n            # Unnormalized importance weights\n            weights = pi_tilde_vals / q_pdf_vals\n\n            numerator = np.sum(weights * f_vals)\n            denominator = np.sum(weights)\n            \n            if denominator > 0:\n                mu_hat_replicates[r] = numerator / denominator\n            else:\n                # This case is highly unlikely for the given proposals as weights are non-negative.\n                # It would only occur if all weights underflow to zero. We assign NaN, which\n                # might indicate a problem, but will be ignored in stats calculations.\n                mu_hat_replicates[r] = np.nan\n\n        # Filter out any potential NaN values before computing statistics.\n        valid_replicates = mu_hat_replicates[~np.isnan(mu_hat_replicates)]\n        \n        # Empirical mean of the estimates\n        mu_bar = np.mean(valid_replicates)\n        \n        # Empirical Mean Squared Error (MSE)\n        # Note: MSE is computed over all R trials as defined, NaNs become large numbers\n        mse_hat = np.mean((mu_hat_replicates - mu_true)**2)\n\n        # Empirical Variance (with 1/R normalization as specified)\n        var_hat = np.var(valid_replicates)\n        \n        # Squared Empirical Bias\n        bias_sq_hat = (mu_bar - mu_true)**2\n        \n        return [mse_hat, var_hat, bias_sq_hat]\n\n    # Calculate the ground truth value for mu\n    mu_true = calculate_ground_truth()\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1, 50000, 'Normal', None, 12345),\n        (20, 10000, 'Normal', None, 23456),\n        (50, 5000, 'Student-t', 3, 34567),\n        (200, 2000, 'Normal', None, 45678)\n    ]\n\n    results = []\n    for case in test_cases:\n        N, R, prop_type, dof, seed = case\n        result_triple = run_simulation(N, R, prop_type, dof, seed, mu_true)\n        results.append(result_triple)\n\n    # Final print statement in the exact required format.\n    result_strs = [f\"[{res[0]},{res[1]},{res[2]}]\" for res in results]\n    print(f\"[{','.join(result_strs)}]\")\n\nsolve()\n```", "id": "3306258"}, {"introduction": "The robustness of Monte Carlo methods is put to the test when dealing with integrands that contain singularities, a common occurrence in fields like physics and finance. In such cases, a naive uniform sampling approach may lead to infinite variance or even a divergent estimate. This practice challenges you to analyze such a scenario, first by theoretically deriving the conditions for convergence, and then by designing a targeted importance sampling scheme to control the error, showcasing how analytical insight is key to developing effective simulation strategies. [@problem_id:3306284]", "problem": "Consider the integration of a spherically symmetric integrand with a singularity at the origin over a hypercube. Let $d \\in \\mathbb{N}$ with $d \\geq 1$, and let $\\alpha \\in \\mathbb{R}$ with $\\alpha \\geq 0$. Define the function $f:[0,1]^d \\to [0,\\infty)$ by $f(x) = \\|x\\|^{-\\alpha}$, where $\\|x\\|$ is the Euclidean norm in $\\mathbb{R}^d$. We are interested in Monte Carlo integration of $\\int_{[0,1]^d} f(x)\\,dx$ and the error properties of estimators when the sampling distribution is either uniform or designed via importance sampling.\n\nStarting from fundamental definitions:\n- The Monte Carlo estimator under the uniform distribution on $[0,1]^d$ for an integrable function $f$ is the sample mean of independent evaluations of $f$, and its variance (if finite) equals the variance of $f(X)$ when $X$ is uniformly distributed on $[0,1]^d$.\n- The second moment of $f$ under the uniform distribution is $\\int_{[0,1]^d} f(x)^2\\,dx$.\n- The error of the Monte Carlo estimator, under finite variance, decays on the order of $N^{-1/2}$ where $N$ is the number of independent samples, and the mean squared error scales as the variance divided by $N$.\n\nYou must analyze the singularity at $x=0$ using the following base principles only:\n- Spherical coordinates in $\\mathbb{R}^d$ with volume element $r^{d-1}\\,dr\\,d\\Omega$, where $r \\geq 0$ and $d\\Omega$ is the surface area element on the unit sphere $S^{d-1}$.\n- The surface area of $S^{d-1}$ is a known constant denoted by $S_{d-1}$.\n- The integral $\\int_0^\\varepsilon r^\\beta\\,dr$ converges if and only if $\\beta  -1$ for any fixed $\\varepsilon \\in (0,1)$.\n\nYour tasks:\n1. Derive the necessary and sufficient thresholds on $\\alpha$ (in terms of $d$) for:\n   - The integrability of $f$ over $[0,1]^d$ under the uniform measure (i.e., the existence of the mean of $f(X)$ for $X$ uniform on $[0,1]^d$).\n   - The finiteness of the variance of the uniform Monte Carlo estimator (i.e., the finiteness of the second moment $\\int_{[0,1]^d} f(x)^2\\,dx$).\n   Your derivation must reason from the behavior near $x=0$ using spherical coordinates and the above base principles, without quoting the final threshold formulas directly.\n\n2. Propose an importance sampling transformation that specifically targets the singularity near $x=0$ by splitting the domain into a small ball around the origin and its complement. For the inner region of radius $\\varepsilon \\in (0,1)$, restricted to the positive orthant $[0,1]^d$, define a radial importance sampling scheme that samples the radius $r \\in [0,\\varepsilon]$ with a probability density proportional to $r^q$ for some exponent $q$, and samples directions uniformly over the positive orthant of the unit sphere. Provide an error analysis for the inner-region estimator based on this scheme:\n   - Express the importance weight in terms of $r$, $q$, $d$, and $\\alpha$.\n   - Derive the threshold condition on $q$ and $\\alpha$ (in terms of $d$) ensuring the finiteness of the variance of the inner-region estimator, again using only the base principles listed above.\n   - Identify the value of $q$ that would make the inner-region estimator have zero variance if it were admissible, and state the condition on $\\alpha$ for which this is admissible.\n\n3. For a fixed choice $\\varepsilon = 1/2$, express the exact inner-region contribution to the mean and the inner-region contribution to the second moment of $f$ under uniform sampling, both restricted to the positive orthant intersection with the ball of radius $\\varepsilon$. These exact expressions must be written in terms of $S_{d-1}$, $d$, $\\alpha$, and $\\varepsilon$, and should account for the fact that within $[0,1]^d$ the directions are restricted to the positive orthant. If a quantity diverges, indicate it as $+\\infty$.\n\nYour program must implement the following test suite and compute for each case:\n- A boolean indicating whether $f$ is integrable under uniform sampling on $[0,1]^d$.\n- A boolean indicating whether the variance of the uniform Monte Carlo estimator is finite.\n- A boolean indicating, for the inner-region radial importance sampling with exponent choice $q = d - 1 - \\alpha + \\delta$ and $\\delta = (d - \\alpha)/2$ (when $\\alpha  d$; otherwise take this importance sampling boolean to be false), whether the theoretical inner-region variance is finite.\n- A float equal to the exact inner-region mean contribution on $\\{x \\in [0,1]^d: \\|x\\| \\le \\varepsilon\\}$ with $\\varepsilon = 1/2$; if it diverges, return positive infinity.\n- A float equal to the exact inner-region second moment contribution under uniform sampling on $\\{x \\in [0,1]^d: \\|x\\| \\le \\varepsilon\\}$ with $\\varepsilon = 1/2$; if it diverges, return positive infinity.\n\nUse the test suite:\n- Case $1$: $(d,\\alpha) = (2,0.6)$.\n- Case $2$: $(d,\\alpha) = (2,1.2)$.\n- Case $3$: $(d,\\alpha) = (3,1.4)$.\n- Case $4$: $(d,\\alpha) = (3,1.5)$.\n- Case $5$: $(d,\\alpha) = (2,2.0)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the five test cases as a comma-separated list of lists, in the order of the cases above. Each inner list must be of the form $[b_1,b_2,b_3,m,s]$, where $b_1$, $b_2$, and $b_3$ are booleans, $m$ is the inner-region mean float, and $s$ is the inner-region second moment float. Use a plain list-of-lists notation and represent divergence by positive infinity. For example, the printed output must look like\n  \"[[True,False,True,0.123,inf],[...],...]\"\nwith the five inner lists in order.", "solution": "The problem requires an analysis of the convergence properties of Monte Carlo estimators for the integral of $f(x) = \\|x\\|^{-\\alpha}$ over the hypercube $[0,1]^d$. The analysis must be derived from first principles involving spherical coordinates and properties of radial integrals.\n\n### Part 1: Integrability and Variance Thresholds\n\nThe function to be integrated is $f(x) = \\|x\\|^{-\\alpha}$ over the domain $D = [0,1]^d$. The singularity of this function is located at $x=0$. The convergence of the integral $\\int_D f(x) dx$ depends solely on the behavior of the integrand in an arbitrarily small neighborhood of this singularity. Let's consider a small region around the origin, $B_\\varepsilon^+ = \\{x \\in [0,1]^d : \\|x\\| \\le \\varepsilon\\}$ for some small $\\varepsilon \\in (0,1)$. For such an $\\varepsilon$ (e.g., $\\varepsilon \\le 1$), this region is the portion of a $d$-dimensional ball of radius $\\varepsilon$ that lies in the positive orthant. The integral over the remainder of the domain, $D \\setminus B_\\varepsilon^+$, is finite because $f(x)$ is continuous and bounded there. Therefore, the convergence of the integral over $D$ is equivalent to the convergence of the integral over $B_\\varepsilon^+$.\n\nWe use spherical coordinates in $\\mathbb{R}^d$. A point $x$ is represented by its radial distance $r = \\|x\\|$ and a set of angles $\\Omega$ on the unit $(d-1)$-sphere, $S^{d-1}$. The volume element is $dx = r^{d-1} dr d\\Omega$. The function $f(x)$ becomes $f(r) = r^{-\\alpha}$. The region $B_\\varepsilon^+$ corresponds to $r \\in [0, \\varepsilon]$ and angles $\\Omega$ spanning the positive orthant. The surface area of the unit sphere in the positive orthant is $\\int_{S^{d-1}^+} d\\Omega = S_{d-1} / 2^d$, where $S_{d-1}$ is the total surface area of $S^{d-1}$.\n\n**1. Condition for Integrability of $f(x)$**\nThe integral of $f(x)$ is given by $I = \\int_{[0,1]^d} \\|x\\|^{-\\alpha} dx$. Its convergence depends on the integral near the origin:\n$$\n\\int_{B_\\varepsilon^+} f(x) dx = \\int_0^\\varepsilon \\int_{S_{d-1}^+} (r^{-\\alpha}) (r^{d-1} dr d\\Omega) = \\left(\\int_{S_{d-1}^+} d\\Omega\\right) \\left(\\int_0^\\varepsilon r^{-\\alpha} r^{d-1} dr\\right)\n$$\n$$\n= \\frac{S_{d-1}}{2^d} \\int_0^\\varepsilon r^{d-1-\\alpha} dr\n$$\nAccording to the provided base principle, the integral $\\int_0^\\varepsilon r^\\beta dr$ converges if and only if the exponent $\\beta  -1$. In our case, $\\beta = d-1-\\alpha$. Thus, for the integral to converge, we require:\n$$\nd-1-\\alpha  -1 \\implies d-\\alpha  0 \\implies \\alpha  d\n$$\nThis is the necessary and sufficient condition for the integrability of $f(x)$ over $[0,1]^d$.\n\n**2. Condition for Finiteness of Variance**\nThe variance of the uniform Monte Carlo estimator is finite if and only if the second moment of the function, $E[f(X)^2]$, is finite. For a uniform distribution on $[0,1]^d$ (which has volume $1$), this is equivalent to the convergence of the integral $\\int_{[0,1]^d} f(x)^2 dx$. The integrand is $f(x)^2 = (\\|x\\|^{-\\alpha})^2 = \\|x\\|^{-2\\alpha}$.\nAgain, we analyze the behavior near the origin:\n$$\n\\int_{B_\\varepsilon^+} f(x)^2 dx = \\int_0^\\varepsilon \\int_{S_{d-1}^+} (r^{-2\\alpha}) (r^{d-1} dr d\\Omega) = \\frac{S_{d-1}}{2^d} \\int_0^\\varepsilon r^{d-1-2\\alpha} dr\n$$\nUsing the same convergence principle, the exponent $\\beta = d-1-2\\alpha$ must satisfy $\\beta  -1$:\n$$\nd-1-2\\alpha  -1 \\implies d-2\\alpha  0 \\implies \\alpha  \\frac{d}{2}\n$$\nThis is the necessary and sufficient condition for the finiteness of the variance of the uniform Monte Carlo estimator.\n\n### Part 2: Importance Sampling Analysis\n\nWe are considering an importance sampling scheme on the inner region $B_\\varepsilon^+$. The radius $r$ is sampled from a probability density function $p_r(r) \\propto r^q$ for $r \\in [0,\\varepsilon]$, and the direction $\\Omega$ is sampled uniformly from the positive orthant of the unit sphere.\n\nThe normalized radial PDF is $p_r(r) = C r^q$. We find $C$ from $\\int_0^\\varepsilon C r^q dr = 1$, which gives $C[\\frac{r^{q+1}}{q+1}]_0^\\varepsilon = 1$. This requires $q  -1$ for convergence at $r=0$. Then $C = \\frac{q+1}{\\varepsilon^{q+1}}$, so $p_r(r) = \\frac{q+1}{\\varepsilon^{q+1}} r^q$.\n\nThe integral of interest is $I_{inner} = \\int_{B_\\varepsilon^+} f(x) dx = \\frac{S_{d-1}}{2^d} \\int_0^\\varepsilon r^{-\\alpha} r^{d-1} dr = \\frac{S_{d-1}}{2^d} \\int_0^\\varepsilon r^{d-1-\\alpha} dr$.\nThe Monte Carlo estimator for this integral based on sampling $r$ from $p_r(r)$ is based on the expectation $E_{p_r}[\\frac{g(r)}{p_r(r)}]$, where $g(r)$ is the function being integrated with respect to $r$. Here, $g(r) = \\frac{S_{d-1}}{2^d} r^{d-1-\\alpha}$. The importance weight is the ratio evaluated at a sample. Let's analyze the variance. The variance of the estimator is finite if $\\int_0^\\varepsilon \\frac{g(r)^2}{p_r(r)} dr$ is finite.\n\n**1. Importance Weight**\nThe question can be interpreted as asking for the term that is averaged in the estimator, which is a weight multiplied by the original value. The term under the expectation is $\\frac{g(r)}{p_r(r)} = \\frac{(\\frac{S_{d-1}}{2^d}) r^{d-1-\\alpha}}{(\\frac{q+1}{\\varepsilon^{q+1}}) r^q} = \\frac{S_{d-1}}{2^d} \\frac{\\varepsilon^{q+1}}{q+1} r^{d-1-\\alpha-q}$. This expression represents the value that each sample $(r_i, \\Omega_i)$ contributes to the sum.\n\n**2. Variance Finiteness for Importance Sampling**\nThe variance is finite if the integral of the squared term is finite:\n$$\n\\int_0^\\varepsilon \\frac{g(r)^2}{p_r(r)} dr = \\int_0^\\varepsilon \\frac{\\left( \\frac{S_{d-1}}{2^d} r^{d-1-\\alpha} \\right)^2}{\\frac{q+1}{\\varepsilon^{q+1}} r^q} dr = \\left( \\frac{S_{d-1}}{2^d} \\right)^2 \\frac{\\varepsilon^{q+1}}{q+1} \\int_0^\\varepsilon \\frac{r^{2(d-1-\\alpha)}}{r^q} dr\n$$\n$$\n\\propto \\int_0^\\varepsilon r^{2(d-1-\\alpha)-q} dr = \\int_0^\\varepsilon r^{2d-2-2\\alpha-q} dr\n$$\nFor this integral to converge, the exponent $\\beta = 2d-2-2\\alpha-q$ must be greater than $-1$:\n$$\n2d-2-2\\alpha-q  -1 \\implies 2(d-1-\\alpha)  q-1\n$$\n\n**3. Optimal Choice of $q$**\nA zero-variance estimator is achieved if the quantity being averaged, $\\frac{g(r)}{p_r(r)}$, is a constant. This means $p_r(r)$ must be proportional to $g(r)$.\n$$\np_r(r) \\propto g(r) \\propto r^{d-1-\\alpha}\n$$\nComparing this with the sampling form $p_r(r) \\propto r^q$, the optimal choice is $q = d-1-\\alpha$.\nFor this choice of $q$ to define a valid probability distribution, the normalization integral $\\int_0^\\varepsilon r^q dr$ must converge, which requires $q  -1$.\nTherefore, the condition for this optimal choice to be admissible is:\n$$\nd-1-\\alpha  -1 \\implies \\alpha  d\n$$\nThis is the same as the condition for the original integral to be finite. If the integral is infinite, no importance sampling scheme can yield a finite estimate with finite variance.\n\n### Part 3: Exact Inner-Region Expressions\n\nWe are given $\\varepsilon = 1/2$. The inner region is $B_{1/2}^+ = \\{x \\in [0,1]^d : \\|x\\| \\le 1/2\\}$.\n\n**1. Inner-Region Mean Contribution**\nThe mean contribution is $m = \\int_{B_{1/2}^+} f(x) dx$. From Part 1, this is:\n$$\nm = \\frac{S_{d-1}}{2^d} \\int_0^{1/2} r^{d-1-\\alpha} dr\n$$\nIf $\\alpha  d$ (i.e., $d-1-\\alpha  -1$), the integral evaluates to:\n$$\n\\int_0^{1/2} r^{d-1-\\alpha} dr = \\left[ \\frac{r^{d-\\alpha}}{d-\\alpha} \\right]_0^{1/2} = \\frac{(1/2)^{d-\\alpha}}{d-\\alpha}\n$$\nSo, for $\\alpha  d$, $m = \\frac{S_{d-1}}{2^d} \\frac{(1/2)^{d-\\alpha}}{d-\\alpha}$.\nIf $\\alpha \\ge d$, the integral diverges, and $m = +\\infty$.\n\n**2. Inner-Region Second Moment Contribution**\nThe second moment contribution is $s = \\int_{B_{1/2}^+} f(x)^2 dx$. From Part 1, this is:\n$$\ns = \\frac{S_{d-1}}{2^d} \\int_0^{1/2} r^{d-1-2\\alpha} dr\n$$\nIf $\\alpha  d/2$ (i.e., $d-1-2\\alpha  -1$), the integral evaluates to:\n$$\n\\int_0^{1/2} r^{d-1-2\\alpha} dr = \\left[ \\frac{r^{d-2\\alpha}}{d-2\\alpha} \\right]_0^{1/2} = \\frac{(1/2)^{d-2\\alpha}}{d-2\\alpha}\n$$\nSo, for $\\alpha  d/2$, $s = \\frac{S_{d-1}}{2^d} \\frac{(1/2)^{d-2\\alpha}}{d-2\\alpha}$.\nIf $\\alpha \\ge d/2$, the integral diverges, and $s = +\\infty$.\n\n### Summary for Implementation\n\nFor each test case $(d, \\alpha)$:\n- **Integrable ($b_1$):** True if $\\alpha  d$, else False.\n- **Finite Variance ($b_2$):** True if $\\alpha  d/2$, else False.\n- **IS Finite Variance ($b_3$):** The importance sampling scheme is defined only if $\\alpha  d$. For this scheme, we are given $q = d-1-\\alpha+\\delta$ with $\\delta = (d-\\alpha)/2$. The condition for finite variance was derived as $2(d-1-\\alpha)  q-1$. Substituting $q$ gives $2(d-1-\\alpha)  (d-1-\\alpha+\\frac{d-\\alpha}{2}) - 1$, which simplifies to $2(d-\\alpha)-2  \\frac{3}{2}(d-\\alpha)-2$, or $\\frac{1}{2}(d-\\alpha)  0$. This is equivalent to $\\alpha  d$. Therefore, $b_3$ is True if $\\alpha  d$, and False otherwise.\n- **Inner Mean ($m$):** If $\\alpha \\ge d$, $m = +\\infty$. Otherwise, $m = \\frac{S_{d-1}}{2^d} \\frac{(0.5)^{d-\\alpha}}{d-\\alpha}$, where $S_{d-1} = \\frac{2\\pi^{d/2}}{\\Gamma(d/2)}$.\n- **Inner Second Moment ($s$):** If $\\alpha \\ge d/2$, $s = +\\infty$. Otherwise, $s = \\frac{S_{d-1}}{2^d} \\frac{(0.5)^{d-2\\alpha}}{d-2\\alpha}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gamma\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases based on the derived theoretical thresholds\n    and formulas for Monte Carlo integration of a singular function.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (2, 0.6),  # Case 1\n        (2, 1.2),  # Case 2\n        (3, 1.4),  # Case 3\n        (3, 1.5),  # Case 4\n        (2, 2.0),  # Case 5\n    ]\n\n    results = []\n    \n    for d, alpha in test_cases:\n        # Task 1: Check integrability and finite variance for uniform sampling.\n        # Condition for integrability: alpha  d\n        b1 = alpha  d\n        \n        # Condition for finite variance: alpha  d / 2\n        b2 = alpha  d / 2\n\n        # Task 2: Check finite variance for the specified importance sampling scheme.\n        # The scheme is defined for alpha  d.\n        # The condition for finite variance of this scheme was derived to be alpha  d.\n        # Therefore, b3 is true if and only if alpha  d.\n        b3 = alpha  d\n\n        # Task 3: Calculate inner-region mean and second moment contributions.\n        epsilon = 0.5\n        \n        # Calculate surface area of (d-1)-sphere, S_{d-1} = 2 * pi^(d/2) / Gamma(d/2)\n        try:\n            sd_minus_1 = 2 * (np.pi**(d/2)) / gamma(d/2)\n        except (ValueError, TypeError):\n            # This case shouldn't be hit with integer d = 1\n            sd_minus_1 = np.nan\n\n        # Calculate inner-region mean contribution 'm'\n        # Formula: (S_{d-1} / 2^d) * (epsilon^(d-alpha) / (d-alpha))\n        # This is finite if and only if alpha  d.\n        if alpha  d:\n            m = (sd_minus_1 / (2**d)) * (epsilon**(d - alpha)) / (d - alpha)\n        else:\n            m = np.inf\n\n        # Calculate inner-region second moment contribution 's'\n        # Formula: (S_{d-1} / 2^d) * (epsilon^(d-2*alpha) / (d-2*alpha))\n        # This is finite if and only if alpha  d / 2.\n        if alpha  d / 2:\n            s = (sd_minus_1 / (2**d)) * (epsilon**(d - 2 * alpha)) / (d - 2 * alpha)\n        else:\n            s = np.inf\n            \n        # Append the list of results for the current test case.\n        results.append([b1, b2, b3, m, s])\n\n    # Custom string formatting to match the required output format \"[[True,False,True,0.123,inf],...]\"\n    def format_list(lst):\n        items = []\n        for item in lst:\n            if isinstance(item, bool):\n                items.append(str(item))\n            elif isinstance(item, float):\n                if np.isinf(item):\n                    items.append('inf')\n                else:\n                    items.append(str(item))\n            else: # Should not happen based on logic\n                items.append(str(item))\n        return f\"[{','.join(items)}]\"\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(format_list, results))}]\")\n\nsolve()\n```", "id": "3306284"}]}