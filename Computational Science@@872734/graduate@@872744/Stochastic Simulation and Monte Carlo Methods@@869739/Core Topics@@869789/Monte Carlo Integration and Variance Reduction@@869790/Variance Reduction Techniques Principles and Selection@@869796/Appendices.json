{"hands_on_practices": [{"introduction": "This first practice provides a foundational look at antithetic variates, a technique that exploits symmetry to reduce variance. By working with the monotonic function $f(u) = \\exp(u)$, you will derive the variance reduction from first principles, building a concrete understanding of how inducing negative correlation between samples leads to a more precise estimator. This exercise is key to grasping the core mechanism behind one of the most elegant variance reduction methods. [@problem_id:3360544]", "problem": "Consider estimating the integral $I=\\mathbb{E}[f(U)]$ for $f(u)=\\exp(u)$ when $U\\sim \\text{Uniform}(0,1)$. Work from first principles: definitions of expectation as an integral, variance as $\\operatorname{Var}(X)=\\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2}$, and linearity of expectation. Two estimators using a fixed computational budget of $N$ function evaluations (with $N$ even) are considered.\n\n- Plain Monte Carlo (MC): Draw independent and identically distributed (i.i.d.) $U_{1},\\ldots,U_{N}\\sim \\text{Uniform}(0,1)$ and compute $\\widehat{I}_{N}=\\frac{1}{N}\\sum_{i=1}^{N} f(U_{i})$.\n\n- Antithetic variates: Draw i.i.d. $U_{1},\\ldots,U_{N/2}\\sim \\text{Uniform}(0,1)$, form antithetic pairs $(U_{i},1-U_{i})$, and compute $\\widehat{I}^{\\mathrm{anti}}_{N}=\\frac{2}{N}\\sum_{i=1}^{N/2}\\frac{f(U_{i})+f(1-U_{i})}{2}$.\n\nUsing only the integral definitions and properties of expectation and variance stated above, do the following:\n\n1. Derive the exact expression for $\\operatorname{Var}(f(U))$.\n2. Derive the exact expression for $\\operatorname{Var}\\!\\left(\\frac{f(U)+f(1-U)}{2}\\right)$ for a single antithetic pair.\n3. Hence, for equal computational cost $N$, derive the exact variance of $\\widehat{I}_{N}$ and of $\\widehat{I}^{\\mathrm{anti}}_{N}$.\n4. Define the variance reduction factor $R$ for equal cost as the ratio $R=\\operatorname{Var}(\\widehat{I}^{\\mathrm{anti}}_{N})/\\operatorname{Var}(\\widehat{I}_{N})$, and simplify it to a closed-form expression that does not depend on $N$.\n\nProvide $R$ as your final answer in exact symbolic form. Do not approximate and do not include units. If any intermediate numerical value is used for verification, do not use it in the final result.", "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. We proceed with the derivation.\n\nLet $U$ be a random variable following a Uniform($0,1$) distribution. Its probability density function is $p(u) = 1$ for $u \\in [0, 1]$ and $0$ otherwise. The function of interest is $f(u) = \\exp(u)$. The integral to be estimated is $I = \\mathbb{E}[f(U)]$.\n\nFirst, we compute the true value of the integral $I$:\n$$I = \\mathbb{E}[f(U)] = \\int_{0}^{1} f(u) p(u) \\, du = \\int_{0}^{1} \\exp(u) \\cdot 1 \\, du = [\\exp(u)]_{0}^{1} = \\exp(1) - \\exp(0) = e-1$$\n\nThis confirms the expectation of our estimators should be $e-1$.\n\n**1. Derivation of $\\operatorname{Var}(f(U))$**\n\nTo compute the variance, we use the formula $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. We first need the second moment of $f(U)$.\n$$\\mathbb{E}[(f(U))^2] = \\mathbb{E}[\\exp(2U)] = \\int_{0}^{1} \\exp(2u) \\, du = \\left[\\frac{1}{2}\\exp(2u)\\right]_{0}^{1} = \\frac{1}{2}(\\exp(2) - \\exp(0)) = \\frac{e^2 - 1}{2}$$\nNow, we can compute the variance of $f(U)$:\n$$\\operatorname{Var}(f(U)) = \\mathbb{E}[(f(U))^2] - (\\mathbb{E}[f(U)])^2 = \\frac{e^2 - 1}{2} - (e-1)^2$$\nExpanding the squared term:\n$$(e-1)^2 = e^2 - 2e + 1$$\nSubstituting this back into the variance expression:\n$$\\operatorname{Var}(f(U)) = \\frac{e^2 - 1}{2} - (e^2 - 2e + 1) = \\frac{e^2 - 1 - 2(e^2 - 2e + 1)}{2} = \\frac{e^2 - 1 - 2e^2 + 4e - 2}{2}$$\n$$\\operatorname{Var}(f(U)) = \\frac{-e^2 + 4e - 3}{2}$$\n\n**2. Derivation of $\\operatorname{Var}\\!\\left(\\frac{f(U)+f(1-U)}{2}\\right)$**\n\nLet $Y = \\frac{f(U)+f(1-U)}{2} = \\frac{\\exp(U)+\\exp(1-U)}{2}$. We need to compute $\\operatorname{Var}(Y)$.\nFirst, we find the expectation of $Y$:\n$$\\mathbb{E}[Y] = \\mathbb{E}\\left[\\frac{\\exp(U)+\\exp(1-U)}{2}\\right] = \\frac{1}{2}(\\mathbb{E}[\\exp(U)] + \\mathbb{E}[\\exp(1-U)])$$\nIf $U \\sim \\text{Uniform}(0,1)$, then the random variable $V = 1-U$ is also distributed as $\\text{Uniform}(0,1)$. Therefore, $\\mathbb{E}[\\exp(1-U)] = \\mathbb{E}[\\exp(V)] = \\mathbb{E}[\\exp(U)] = e-1$.\n$$\\mathbb{E}[Y] = \\frac{1}{2}((e-1) + (e-1)) = e-1$$\nThis shows that the antithetic variate sampling provides an unbiased estimate.\n\nNext, we find the second moment of $Y$:\n$$\\mathbb{E}[Y^2] = \\mathbb{E}\\left[\\left(\\frac{\\exp(U)+\\exp(1-U)}{2}\\right)^2\\right] = \\frac{1}{4}\\mathbb{E}[\\exp(2U) + 2\\exp(U)\\exp(1-U) + \\exp(2(1-U))]$$\nThe middle term simplifies: $2\\exp(U)\\exp(1-U) = 2\\exp(U+1-U) = 2e$.\n$$\\mathbb{E}[Y^2] = \\frac{1}{4}\\mathbb{E}[\\exp(2U) + 2e + \\exp(2-2U)]$$\nUsing linearity of expectation:\n$$\\mathbb{E}[Y^2] = \\frac{1}{4}(\\mathbb{E}[\\exp(2U)] + \\mathbb{E}[2e] + \\mathbb{E}[\\exp(2-2U)])$$\nWe already know $\\mathbb{E}[\\exp(2U)] = \\frac{e^2-1}{2}$. For the last term, we compute the integral:\n$$\\mathbb{E}[\\exp(2-2U)] = \\int_{0}^{1} \\exp(2-2u) \\, du = \\left[-\\frac{1}{2}\\exp(2-2u)\\right]_{0}^{1} = -\\frac{1}{2}(\\exp(0) - \\exp(2)) = \\frac{e^2-1}{2}$$\nSubstituting these results back:\n$$\\mathbb{E}[Y^2] = \\frac{1}{4}\\left(\\frac{e^2-1}{2} + 2e + \\frac{e^2-1}{2}\\right) = \\frac{1}{4}(e^2-1+2e) = \\frac{e^2+2e-1}{4}$$\nNow we compute the variance of $Y$:\n$$\\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = \\frac{e^2+2e-1}{4} - (e-1)^2 = \\frac{e^2+2e-1 - 4(e^2-2e+1)}{4}$$\n$$\\operatorname{Var}(Y) = \\frac{e^2+2e-1 - 4e^2+8e-4}{4} = \\frac{-3e^2+10e-5}{4}$$\n\n**3. Derivation of the Variances of the Estimators**\n\nThe plain Monte Carlo estimator is $\\widehat{I}_{N} = \\frac{1}{N}\\sum_{i=1}^{N} f(U_{i})$, where $U_i$ are i.i.d. The variance of the mean of $N$ i.i.d. random variables is the variance of a single variable divided by $N$.\n$$\\operatorname{Var}(\\widehat{I}_{N}) = \\frac{1}{N}\\operatorname{Var}(f(U)) = \\frac{1}{N} \\left(\\frac{-e^2 + 4e - 3}{2}\\right) = \\frac{-e^2 + 4e - 3}{2N}$$\nThe antithetic variates estimator is $\\widehat{I}^{\\mathrm{anti}}_{N} = \\frac{1}{N/2}\\sum_{i=1}^{N/2} Y_i$, where $Y_i = \\frac{f(U_i)+f(1-U_i)}{2}$. The random variables $Y_i$ are i.i.d. because the $U_i$ are i.i.d. for $i=1, \\dots, N/2$. The estimator is the mean of $N/2$ i.i.d. samples.\n$$\\operatorname{Var}(\\widehat{I}^{\\mathrm{anti}}_{N}) = \\frac{1}{N/2}\\operatorname{Var}(Y) = \\frac{2}{N}\\left(\\frac{-3e^2+10e-5}{4}\\right) = \\frac{-3e^2+10e-5}{2N}$$\n\n**4. Derivation of the Variance Reduction Factor $R$**\n\nThe variance reduction factor $R$ for equal computational cost $N$ is the ratio of the two variances.\n$$R = \\frac{\\operatorname{Var}(\\widehat{I}^{\\mathrm{anti}}_{N})}{\\operatorname{Var}(\\widehat{I}_{N})} = \\frac{\\frac{-3e^2+10e-5}{2N}}{\\frac{-e^2+4e-3}{2N}}$$\nThe term $2N$ in the denominators cancels out, yielding an expression for $R$ that is independent of $N$:\n$$R = \\frac{-3e^2+10e-5}{-e^2+4e-3}$$\nThis can be rewritten by multiplying the numerator and denominator by $-1$ to have positive leading coefficients:\n$$R = \\frac{3e^2-10e+5}{e^2-4e+3}$$\nBoth forms are equivalent. We present the form derived directly from the positive variance terms.", "answer": "$$\\boxed{\\frac{-3e^2+10e-5}{-e^2+4e-3}}$$", "id": "3360544"}, {"introduction": "We now turn to control variates, a powerful and widely applicable technique that uses information from a correlated auxiliary variable to reduce estimation error. The effectiveness of this method hinges on finding the optimal way to adjust our primary estimator using the 'control'. This practice guides you through the fundamental derivation of the optimal control coefficient and its application in a practical scenario, highlighting how leveraging problem-specific knowledge can yield significant efficiency gains. [@problem_id:3360574]", "problem": "Let $X$ be a real-valued random variable with distribution $X \\sim \\mathcal{N}(0,1)$. Consider estimating the expectation $\\mu = \\mathbb{E}[f(X)]$ using a control variate based on a function $g(X)$ whose expectation is known. Let $f(X) = X^{4}$ and $g(X) = X^{2}$. For a scalar coefficient $\\beta \\in \\mathbb{R}$, define the control variate estimator $Y_{\\beta} = f(X) - \\beta\\big(g(X) - \\mathbb{E}[g(X)]\\big)$. Using only the definitions of variance, covariance, and the properties of linearity of expectation, derive from first principles the coefficient $\\beta^{*}$ that minimizes $\\operatorname{Var}(Y_{\\beta})$ over $\\beta$. Then, specializing to the given $f$ and $g$, compute:\n- the optimal coefficient $\\beta^{*}$,\n- the resulting variance $\\operatorname{Var}\\big(X^{4} - \\beta^{*} X^{2}\\big)$,\n- the reduction factor defined as $\\operatorname{Var}\\big(X^{4} - \\beta^{*} X^{2}\\big)\\big/\\operatorname{Var}(X^{4})$.\n\nProvide exact values; no rounding is required. For clarity, note that subtracting a constant does not change variance, so $\\operatorname{Var}\\big(X^{4} - \\beta^{*} X^{2}\\big) = \\operatorname{Var}\\big(X^{4} - \\beta^{*}(X^{2} - \\mathbb{E}[X^{2}])\\big)$ for any $\\beta \\in \\mathbb{R}$. Report your final results in the order listed above.", "solution": "The problem is valid as it is scientifically grounded in probability theory, well-posed, objective, and self-contained. We shall proceed with the solution.\n\nThe problem asks for three items: the derivation of the optimal control variate coefficient $\\beta^{*}$, its specific value for the given functions, the resulting variance, and the variance reduction factor.\n\nFirst, we derive the optimal coefficient $\\beta^{*}$ that minimizes the variance of the control variate estimator $Y_{\\beta}$. The estimator is defined as:\n$$\nY_{\\beta} = f(X) - \\beta\\big(g(X) - \\mathbb{E}[g(X)]\\big)\n$$\nwhere $f(X)$ and $g(X)$ are functions of a random variable $X$, and $\\mathbb{E}[g(X)]$ is the known expectation of $g(X)$.\n\nWe seek to minimize $\\operatorname{Var}(Y_{\\beta})$ with respect to the coefficient $\\beta \\in \\mathbb{R}$. We begin by expressing the variance of $Y_{\\beta}$ using the properties of variance. Let $C = \\beta \\mathbb{E}[g(X)]$ be a constant. The variance of a random variable is not affected by adding or subtracting a constant, so:\n$$\n\\operatorname{Var}(Y_{\\beta}) = \\operatorname{Var}\\big(f(X) - \\beta g(X) + \\beta \\mathbb{E}[g(X)]\\big) = \\operatorname{Var}\\big(f(X) - \\beta g(X)\\big)\n$$\nUsing the formula for the variance of a linear combination of random variables, $\\operatorname{Var}(A - B) = \\operatorname{Var}(A) + \\operatorname{Var}(B) - 2\\operatorname{Cov}(A, B)$, we have:\n$$\n\\operatorname{Var}(Y_{\\beta}) = \\operatorname{Var}(f(X)) + \\operatorname{Var}(\\beta g(X)) - 2\\operatorname{Cov}(f(X), \\beta g(X))\n$$\nUsing the properties $\\operatorname{Var}(cZ) = c^2\\operatorname{Var}(Z)$ and $\\operatorname{Cov}(Z_1, cZ_2) = c\\operatorname{Cov}(Z_1, Z_2)$ for a constant $c$, we obtain:\n$$\n\\operatorname{Var}(Y_{\\beta}) = \\operatorname{Var}(f(X)) + \\beta^2 \\operatorname{Var}(g(X)) - 2\\beta \\operatorname{Cov}(f(X), g(X))\n$$\nTo find the value of $\\beta$ that minimizes this variance, we treat $\\operatorname{Var}(Y_{\\beta})$ as a function of $\\beta$ and find its minimum by taking the derivative with respect to $\\beta$ and setting it to zero.\n$$\n\\frac{d}{d\\beta} \\operatorname{Var}(Y_{\\beta}) = \\frac{d}{d\\beta} \\left( \\operatorname{Var}(f(X)) + \\beta^2 \\operatorname{Var}(g(X)) - 2\\beta \\operatorname{Cov}(f(X), g(X)) \\right)\n$$\n$$\n\\frac{d}{d\\beta} \\operatorname{Var}(Y_{\\beta}) = 2\\beta \\operatorname{Var}(g(X)) - 2\\operatorname{Cov}(f(X), g(X))\n$$\nSetting the derivative to zero to find the optimal coefficient $\\beta^{*}$:\n$$\n2\\beta^{*} \\operatorname{Var}(g(X)) - 2\\operatorname{Cov}(f(X), g(X)) = 0\n$$\n$$\n\\beta^{*} \\operatorname{Var}(g(X)) = \\operatorname{Cov}(f(X), g(X))\n$$\nAssuming $\\operatorname{Var}(g(X)) > 0$ (i.e., $g(X)$ is not a constant), we can solve for $\\beta^{*}$:\n$$\n\\beta^{*} = \\frac{\\operatorname{Cov}(f(X), g(X))}{\\operatorname{Var}(g(X))}\n$$\nThe second derivative, $\\frac{d^2}{d\\beta^2}\\operatorname{Var}(Y_{\\beta}) = 2\\operatorname{Var}(g(X))$, is positive, confirming that this value of $\\beta$ corresponds to a minimum.\n\nNext, we specialize to the given problem where $X \\sim \\mathcal{N}(0,1)$, $f(X) = X^4$, and $g(X) = X^2$. To compute $\\beta^*$, we must find the moments of the standard normal distribution. The $n$-th moment of a standard normal random variable $X$ is $\\mathbb{E}[X^n]$. For $n$ odd, $\\mathbb{E}[X^n] = 0$. For $n$ even, say $n=2k$, the moments are given by $\\mathbb{E}[X^{2k}] = (2k-1)!! = (2k-1)(2k-3)\\cdots 1$.\nWe need the following expectations:\n- $\\mathbb{E}[X^2] = (2-1)!! = 1$\n- $\\mathbb{E}[X^4] = (4-1)!! = 3 \\times 1 = 3$\n- $\\mathbb{E}[X^6] = (6-1)!! = 5 \\times 3 \\times 1 = 15$\n- $\\mathbb{E}[X^8] = (8-1)!! = 7 \\times 5 \\times 3 \\times 1 = 105$\n\nNow we compute the terms in the expression for $\\beta^{*}$:\nThe covariance term is:\n$$\n\\operatorname{Cov}(f(X), g(X)) = \\operatorname{Cov}(X^4, X^2) = \\mathbb{E}[X^4 \\cdot X^2] - \\mathbb{E}[X^4]\\mathbb{E}[X^2] = \\mathbb{E}[X^6] - \\mathbb{E}[X^4]\\mathbb{E}[X^2]\n$$\n$$\n\\operatorname{Cov}(X^4, X^2) = 15 - (3)(1) = 12\n$$\nThe variance term is:\n$$\n\\operatorname{Var}(g(X)) = \\operatorname{Var}(X^2) = \\mathbb{E}[(X^2)^2] - (\\mathbb{E}[X^2])^2 = \\mathbb{E}[X^4] - (\\mathbb{E}[X^2])^2\n$$\n$$\n\\operatorname{Var}(X^2) = 3 - (1)^2 = 2\n$$\nThe optimal coefficient is therefore:\n$$\n\\beta^{*} = \\frac{12}{2} = 6\n$$\nNow, we compute the resulting variance $\\operatorname{Var}\\big(X^4 - \\beta^{*} X^2\\big) = \\operatorname{Var}\\big(X^4 - 6X^2\\big)$.\nLet $W = X^4 - 6X^2$.\n$$\n\\operatorname{Var}(W) = \\mathbb{E}[W^2] - (\\mathbb{E}[W])^2\n$$\nFirst, the expectation of $W$:\n$$\n\\mathbb{E}[W] = \\mathbb{E}[X^4 - 6X^2] = \\mathbb{E}[X^4] - 6\\mathbb{E}[X^2] = 3 - 6(1) = -3\n$$\nNext, the expectation of $W^2$:\n$$\n\\mathbb{E}[W^2] = \\mathbb{E}[(X^4 - 6X^2)^2] = \\mathbb{E}[X^8 - 12X^6 + 36X^4]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[W^2] = \\mathbb{E}[X^8] - 12\\mathbb{E}[X^6] + 36\\mathbb{E}[X^4] = 105 - 12(15) + 36(3) = 105 - 180 + 108 = 33\n$$\nThus, the variance is:\n$$\n\\operatorname{Var}(X^4 - 6X^2) = 33 - (-3)^2 = 33 - 9 = 24\n$$\nFinally, we compute the reduction factor, which is the ratio of the controlled variance to the original variance. The original variance is $\\operatorname{Var}(f(X)) = \\operatorname{Var}(X^4)$.\n$$\n\\operatorname{Var}(X^4) = \\mathbb{E}[(X^4)^2] - (\\mathbb{E}[X^4])^2 = \\mathbb{E}[X^8] - (\\mathbb{E}[X^4])^2 = 105 - (3)^2 = 105 - 9 = 96\n$$\nThe reduction factor is:\n$$\n\\frac{\\operatorname{Var}(X^4 - \\beta^{*} X^2)}{\\operatorname{Var}(X^4)} = \\frac{24}{96} = \\frac{1}{4}\n$$\nThe three required values are the optimal coefficient $\\beta^{*} = 6$, the resulting variance $\\operatorname{Var}(X^4 - 6X^2) = 24$, and the reduction factor $1/4$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 6  24  \\frac{1}{4} \\end{pmatrix}}\n$$", "id": "3360574"}, {"introduction": "A crucial aspect of mastering variance reduction is not just knowing how to apply techniques, but knowing *when* to apply them. This advanced practice explores a scenario where the common random numbers (CRN) technique, typically used to sharpen comparisons between systems, unexpectedly increases variance. By analyzing a model with sign-changing response functions, you will uncover the underlying principles that determine the success or failure of CRN, learning to develop diagnostics to make informed methodological selections. [@problem_id:3360579]", "problem": "Consider two competing stochastic systems with outputs modeled as $Y_{1} = f_{1}(U) + \\epsilon_{1}$ and $Y_{2} = f_{2}(U) + \\epsilon_{2}$, where $U \\sim \\mathcal{N}(0,1)$ is a scalar common input driver, and $\\epsilon_{1} \\sim \\mathcal{N}(0,\\sigma_{1}^{2})$, $\\epsilon_{2} \\sim \\mathcal{N}(0,\\sigma_{2}^{2})$ are independent of each other and of $U$. Define the response functions to exhibit regime-dependent regression slopes that change sign across $U=0$:\n$$\nf_{1}(U) = \\alpha_{1} + \\begin{cases}\na\\,U,  U  0, \\\\\n- a\\,U,  U \\ge 0,\n\\end{cases}\n\\qquad\nf_{2}(U) = \\alpha_{2} + \\begin{cases}\n- b\\,U,  U  0, \\\\\nb\\,U,  U \\ge 0,\n\\end{cases}\n$$\nwith fixed constants $\\alpha_{1}, \\alpha_{2} \\in \\mathbb{R}$ and $a > 0$, $b > 0$. Consider estimating the mean performance difference $\\mu_{\\Delta} = \\mathbb{E}[Y_{1} - Y_{2}]$ by a one-replication Monte Carlo difference estimator using either Common Random Numbers (CRN), where the same realization of $U$ is used for both systems, or Independent Random Numbers (IRN), where independent realizations $U_{1} \\sim \\mathcal{N}(0,1)$ and $U_{2} \\sim \\mathcal{N}(0,1)$ are used for the two systems.\n\nStarting only from fundamental probabilistic definitions and properties, carry out the following:\n- Derive the variance of the one-replication difference estimator under CRN and under IRN and obtain the variance inflation ratio $R$, defined as the CRN variance divided by the IRN variance, expressed in closed form in terms of $a$, $b$, $\\sigma_{1}^{2}$, $\\sigma_{2}^{2}$, and known constants.\n- Using local linearity, derive a diagnostic quantity $D$ based on the product of local regression slopes that indicates whether CRN is suitable for variance reduction for the difference estimator in this setting. Express $D$ in closed form in terms of $a$ and $b$.\n\nProvide your final answer as a row matrix containing the variance inflation ratio $R$ and the diagnostic $D$. No numerical rounding is required.", "solution": "The problem is deemed valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique mathematical solution within the domain of stochastic simulation. We may proceed with the derivation.\n\nThe objective is to estimate the mean performance difference $\\mu_{\\Delta} = \\mathbb{E}[Y_{1} - Y_{2}]$. The two systems are given by $Y_{1} = f_{1}(U) + \\epsilon_{1}$ and $Y_{2} = f_{2}(U) + \\epsilon_{2}$. The noise terms $\\epsilon_{1} \\sim \\mathcal{N}(0,\\sigma_{1}^{2})$ and $\\epsilon_{2} \\sim \\mathcal{N}(0,\\sigma_{2}^{2})$ are independent of each other and of the common input driver $U \\sim \\mathcal{N}(0,1)$. The constants $a, b$ are positive.\n\nThe response functions are:\n$$\nf_{1}(U) = \\alpha_{1} + \\begin{cases}\na\\,U,  U  0, \\\\\n- a\\,U,  U \\ge 0,\n\\end{cases}\n\\qquad\nf_{2}(U) = \\alpha_{2} + \\begin{cases}\n- b\\,U,  U  0, \\\\\nb\\,U,  U \\ge 0,\n\\end{cases}\n$$\nThese can be written more compactly using the absolute value function. For $U  0$, we have $|U| = -U$, so $aU = -a|U|$ and $-bU = b|U|$. For $U \\ge 0$, we have $|U| = U$, so $-aU = -a|U|$ and $bU = b|U|$. Thus, the functions are equivalent to:\n$$\nf_{1}(U) = \\alpha_{1} - a|U|\n$$\n$$\nf_{2}(U) = \\alpha_{2} + b|U|\n$$\nThis compact representation simplifies the subsequent calculations.\n\nFirst, we compute the moments of $|U|$ for $U \\sim \\mathcal{N}(0,1)$. The probability density function of $U$ is $\\phi(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2}\\right)$.\nThe first moment, or expectation, is:\n$$\n\\mathbb{E}[|U|] = \\int_{-\\infty}^{\\infty} |u| \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2}\\right) du = 2 \\int_{0}^{\\infty} u \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2}\\right) du\n$$\nUsing the substitution $v = u^2/2$, where $dv = u\\,du$:\n$$\n\\mathbb{E}[|U|] = \\frac{2}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} \\exp(-v) dv = \\frac{2}{\\sqrt{2\\pi}} [-\\exp(-v)]_{0}^{\\infty} = \\frac{2}{\\sqrt{2\\pi}} (0 - (-1)) = \\sqrt{\\frac{2}{\\pi}}\n$$\nThe second moment of $|U|$ is the same as the second moment of $U$:\n$$\n\\mathbb{E}[|U|^2] = \\mathbb{E}[U^2] = \\operatorname{Var}(U) + (\\mathbb{E}[U])^2 = 1 + 0^2 = 1\n$$\nThe variance of $|U|$ is therefore:\n$$\n\\operatorname{Var}(|U|) = \\mathbb{E}[|U|^2] - (\\mathbb{E}[|U|])^2 = 1 - \\left(\\sqrt{\\frac{2}{\\pi}}\\right)^2 = 1 - \\frac{2}{\\pi}\n$$\nThe expectations of the system outputs are:\n$\\mathbb{E}[Y_{1}] = \\mathbb{E}[\\alpha_{1} - a|U| + \\epsilon_{1}] = \\alpha_{1} - a\\mathbb{E}[|U|] + \\mathbb{E}[\\epsilon_{1}] = \\alpha_{1} - a\\sqrt{\\frac{2}{\\pi}}$\n$\\mathbb{E}[Y_{2}] = \\mathbb{E}[\\alpha_{2} + b|U| + \\epsilon_{2}] = \\alpha_{2} + b\\mathbb{E}[|U|] + \\mathbb{E}[\\epsilon_{2}] = \\alpha_{2} + b\\sqrt{\\frac{2}{\\pi}}$\nThe true mean difference is $\\mu_{\\Delta} = \\mathbb{E}[Y_1-Y_2] = (\\alpha_1-\\alpha_2) - (a+b)\\sqrt{\\frac{2}{\\pi}}$.\n\n**Variance under Independent Random Numbers (IRN)**\nThe one-replication difference estimator is $\\hat{\\mu}_{\\Delta, \\text{IRN}} = Y_{1} - Y_{2} = (f_{1}(U_{1}) + \\epsilon_{1}) - (f_{2}(U_{2}) + \\epsilon_{2})$, where $U_{1}$, $U_{2}$, $\\epsilon_{1}$, and $\\epsilon_{2}$ are mutually independent. The variance is the sum of the variances of the independent terms:\n$$\n\\operatorname{Var}_{\\text{IRN}} = \\operatorname{Var}(\\hat{\\mu}_{\\Delta, \\text{IRN}}) = \\operatorname{Var}(f_{1}(U_{1})) + \\operatorname{Var}(\\epsilon_{1}) + \\operatorname{Var}(f_{2}(U_{2})) + \\operatorname{Var}(\\epsilon_{2})\n$$\nWe calculate the variances of the function outputs:\n$$\n\\operatorname{Var}(f_{1}(U_{1})) = \\operatorname{Var}(\\alpha_{1} - a|U_{1}|) = (-a)^2 \\operatorname{Var}(|U_{1}|) = a^2 \\left(1 - \\frac{2}{\\pi}\\right)\n$$\n$$\n\\operatorname{Var}(f_{2}(U_{2})) = \\operatorname{Var}(\\alpha_{2} + b|U_{2}|) = b^2 \\operatorname{Var}(|U_{2}|) = b^2 \\left(1 - \\frac{2}{\\pi}\\right)\n$$\nThe noise variances are given as $\\operatorname{Var}(\\epsilon_{1}) = \\sigma_{1}^2$ and $\\operatorname{Var}(\\epsilon_{2}) = \\sigma_{2}^2$.\nSubstituting these into the expression for $\\operatorname{Var}_{\\text{IRN}}$:\n$$\n\\operatorname{Var}_{\\text{IRN}} = (a^2+b^2)\\left(1 - \\frac{2}{\\pi}\\right) + \\sigma_{1}^2 + \\sigma_{2}^2\n$$\n\n**Variance under Common Random Numbers (CRN)**\nThe one-replication difference estimator is $\\hat{\\mu}_{\\Delta, \\text{CRN}} = Y_{1} - Y_{2} = (f_{1}(U) + \\epsilon_{1}) - (f_{2}(U) + \\epsilon_{2})$, where the same random number $U$ drives both systems. The variance is:\n$$\n\\operatorname{Var}_{\\text{CRN}} = \\operatorname{Var}(\\hat{\\mu}_{\\Delta, \\text{CRN}}) = \\operatorname{Var}( (f_{1}(U) - f_{2}(U)) + (\\epsilon_{1} - \\epsilon_{2}) )\n$$\nBecause $U$, $\\epsilon_{1}$, and $\\epsilon_{2}$ are mutually independent, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}_{\\text{CRN}} = \\operatorname{Var}(f_{1}(U) - f_{2}(U)) + \\operatorname{Var}(\\epsilon_{1} - \\epsilon_{2})\n$$\nThe difference in the functions is $f_{1}(U) - f_{2}(U) = (\\alpha_{1} - a|U|) - (\\alpha_{2} + b|U|) = (\\alpha_{1} - \\alpha_{2}) - (a+b)|U|$.\nThe variance of this difference is:\n$$\n\\operatorname{Var}(f_{1}(U) - f_{2}(U)) = \\operatorname{Var}((\\alpha_{1} - \\alpha_{2}) - (a+b)|U|) = (-(a+b))^2 \\operatorname{Var}(|U|) = (a+b)^2 \\left(1 - \\frac{2}{\\pi}\\right)\n$$\nThe variance of the noise difference is $\\operatorname{Var}(\\epsilon_{1} - \\epsilon_{2}) = \\operatorname{Var}(\\epsilon_{1}) + \\operatorname{Var}(- \\epsilon_{2}) = \\operatorname{Var}(\\epsilon_{1}) + \\operatorname{Var}(\\epsilon_{2}) = \\sigma_{1}^2 + \\sigma_{2}^2$.\nSubstituting these into the expression for $\\operatorname{Var}_{\\text{CRN}}$:\n$$\n\\operatorname{Var}_{\\text{CRN}} = (a+b)^2 \\left(1 - \\frac{2}{\\pi}\\right) + \\sigma_{1}^2 + \\sigma_{2}^2\n$$\n\n**Variance Inflation Ratio (R)**\nThe ratio $R$ is defined as the CRN variance divided by the IRN variance:\n$$\nR = \\frac{\\operatorname{Var}_{\\text{CRN}}}{\\operatorname{Var}_{\\text{IRN}}} = \\frac{(a+b)^2 \\left(1 - \\frac{2}{\\pi}\\right) + \\sigma_{1}^2 + \\sigma_{2}^2}{(a^2+b^2)\\left(1 - \\frac{2}{\\pi}\\right) + \\sigma_{1}^2 + \\sigma_{2}^2}\n$$\n\n**Diagnostic Quantity (D)**\nCRN provides variance reduction if $\\operatorname{Cov}(Y_{1}, Y_{2}) > 0$. From the property $\\operatorname{Var}(Y_1 - Y_2) = \\operatorname{Var}(Y_1) + \\operatorname{Var}(Y_2) - 2\\operatorname{Cov}(Y_1, Y_2)$, positive covariance reduces the variance of the difference. Here,\n$$\n\\operatorname{Cov}(Y_{1}, Y_{2}) = \\operatorname{Cov}(f_{1}(U) + \\epsilon_1, f_{2}(U) + \\epsilon_2) = \\operatorname{Cov}(f_{1}(U), f_{2}(U))\n$$\ndue to the independence of the noise terms.\nA heuristic based on local linearity states that the sign of this covariance is often indicated by the sign of the product of the response functions' derivatives. The local regression slopes are the derivatives $f_{1}'(U)$ and $f_{2}'(U)$:\n$$\nf_{1}'(U) = \\frac{d}{dU}(\\alpha_{1} - a|U|) = -a \\cdot \\operatorname{sgn}(U) = \\begin{cases} a,  U  0, \\\\ -a,  U  0, \\end{cases}\n$$\n$$\nf_{2}'(U) = \\frac{d}{dU}(\\alpha_{2} + b|U|) = b \\cdot \\operatorname{sgn}(U) = \\begin{cases} -b,  U  0, \\\\ b,  U  0, \\end{cases}\n$$\nThese are defined for $U \\ne 0$. The diagnostic is based on the product of these slopes, $f_{1}'(U)f_{2}'(U)$.\nFor $U  0$, the product is $(a)(-b) = -ab$.\nFor $U  0$, the product is $(-a)(b) = -ab$.\nThe product is a constant, $-ab$, for almost all $U$ (i.e., for all $U \\in \\mathbb{R} \\setminus \\{0\\}$). The diagnostic quantity $D$ is this product, which represents the consistent relationship between the local slopes across the domain of $U$.\n$$\nD = -ab\n$$\nThe sign of $D$ is negative, since $a > 0$ and $b > 0$. This correctly predicts that $\\operatorname{Cov}(f_{1}(U), f_{2}(U))$ is negative, and thus CRN is not suitable for variance reduction in this specific problem. We can verify this with the exact covariance:\n$\\operatorname{Cov}(f_{1}(U), f_{2}(U)) = \\operatorname{Cov}(\\alpha_{1} - a|U|, \\alpha_{2} + b|U|) = -ab \\cdot \\operatorname{Var}(|U|) = -ab\\left(1 - \\frac{2}{\\pi}\\right)$. Since $1 > 2/\\pi$, this covariance is indeed negative.\n\nThe final results are the expressions for $R$ and $D$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{(a+b)^{2} \\left(1 - \\frac{2}{\\pi}\\right) + \\sigma_{1}^{2} + \\sigma_{2}^{2}}{(a^{2}+b^{2})\\left(1 - \\frac{2}{\\pi}\\right) + \\sigma_{1}^{2} + \\sigma_{2}^{2}}  -ab \\end{pmatrix}}\n$$", "id": "3360579"}]}