{"hands_on_practices": [{"introduction": "The efficiency of the hit-or-miss method is not absolute; it critically depends on the choice of the bounding domain $D$ and the height $H$. This first practice challenges you to quantify this relationship by deriving the variance of the estimator from first principles [@problem_id:3312364]. By comparing two estimators that differ only in the volume of their bounding boxes, you will uncover a fundamental principle: a tighter fit around the function to be integrated directly translates to a more precise estimate for the same number of samples.", "problem": "Consider a nonnegative integrable function $g:\\mathbb{R}^{d}\\to[0,\\infty)$ with compact support $S\\subset\\mathbb{R}^{d}$, and define the integral $I=\\int_{\\mathbb{R}^{d}} g(x)\\,\\mathrm{d}x=\\int_{S} g(x)\\,\\mathrm{d}x$. Let $D_{1}$ and $D_{2}$ be two axis-aligned bounding boxes (hyperrectangles) in $\\mathbb{R}^{d}$ such that $S\\subset D_{1}$ and $S\\subset D_{2}$, with volumes $|D_{1}||D_{2}|$, where $|\\cdot|$ denotes the Lebesgue measure. Assume there is a known finite constant $H$ satisfying $H\\ge \\sup_{x\\in\\mathbb{R}^{d}} g(x)$, and $H$ is the same for both $D_{1}$ and $D_{2}$.\n\nFor each $k\\in\\{1,2\\}$, construct a hit-or-miss Monte Carlo (MC) estimator of $I$ by drawing $M$ independent and identically distributed (i.i.d.) samples $(X_{i}^{(k)},U_{i}^{(k)})$, $i=1,\\dots,M$, where $X_{i}^{(k)}$ is uniformly distributed on $D_{k}$ and $U_{i}^{(k)}$ is uniformly distributed on $[0,H]$, independent of $X_{i}^{(k)}$. Define $Y_{i}^{(k)}=\\mathbf{1}\\{U_{i}^{(k)}\\le g(X_{i}^{(k)})\\}$ and the estimator\n$$\n\\widehat{I}_{k} = |D_{k}|\\,H\\,\\frac{1}{M}\\sum_{i=1}^{M} Y_{i}^{(k)}.\n$$\n\nStarting from first principles of probability (expectation of indicator functions and variance of sample means of independent Bernoulli random variables), derive a closed-form expression for the ratio of the variances $\\mathrm{Var}(\\widehat{I}_{1})/\\mathrm{Var}(\\widehat{I}_{2})$ in terms of $|D_{1}|$, $|D_{2}|$, $H$, and $I$. Express your final answer as a single analytic expression. No rounding is required.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- A nonnegative integrable function $g:\\mathbb{R}^{d}\\to[0,\\infty)$ with compact support $S\\subset\\mathbb{R}^{d}$.\n- The integral $I=\\int_{\\mathbb{R}^{d}} g(x)\\,\\mathrm{d}x=\\int_{S} g(x)\\,\\mathrm{d}x$.\n- Two axis-aligned bounding boxes $D_{1}$ and $D_{2}$ in $\\mathbb{R}^{d}$ such that $S\\subset D_{1}$ and $S\\subset D_{2}$.\n- The volumes satisfy $|D_{1}||D_{2}|$, where $|\\cdot|$ denotes the Lebesgue measure.\n- A known finite constant $H$ satisfying $H\\ge \\sup_{x\\in\\mathbb{R}^{d}} g(x)$.\n- For each $k\\in\\{1,2\\}$, $M$ independent and identically distributed (i.i.d.) samples $(X_{i}^{(k)},U_{i}^{(k)})$ are drawn for $i=1,\\dots,M$.\n- $X_{i}^{(k)}$ is uniformly distributed on $D_{k}$.\n- $U_{i}^{(k)}$ is uniformly distributed on $[0,H]$ and is independent of $X_{i}^{(k)}$.\n- The random variable $Y_{i}^{(k)}=\\mathbf{1}\\{U_{i}^{(k)}\\le g(X_{i}^{(k)})\\}$.\n- The estimator for $I$ is $\\widehat{I}_{k} = |D_{k}|\\,H\\,\\frac{1}{M}\\sum_{i=1}^{M} Y_{i}^{(k)}$.\n- The task is to derive a closed-form expression for the ratio of the variances $\\mathrm{Var}(\\widehat{I}_{1})/\\mathrm{Var}(\\widehat{I}_{2})$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement describes a standard hit-or-miss Monte Carlo integration scenario.\n- **Scientifically Grounded**: The problem is based on the established mathematical theory of Monte Carlo methods, a core topic in numerical analysis and computational science. All principles are sound.\n- **Well-Posed**: The problem provides all necessary definitions and conditions to derive the variances of the estimators and their ratio. The quantities are well-defined and a unique solution is expected.\n- **Objective**: The problem is stated using precise mathematical language, free from ambiguity or subjective content.\n\nThe problem does not exhibit any of the invalidity flaws. It is a valid, well-defined problem in the field of stochastic simulation.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation\nOur goal is to compute the ratio $\\mathrm{Var}(\\widehat{I}_{1})/\\mathrm{Var}(\\widehat{I}_{2})$. We begin by deriving a general expression for $\\mathrm{Var}(\\widehat{I}_{k})$ for $k \\in \\{1, 2\\}$.\n\nThe estimator is given by\n$$\n\\widehat{I}_{k} = |D_{k}|\\,H\\,\\frac{1}{M}\\sum_{i=1}^{M} Y_{i}^{(k)}\n$$\nThe variance of this estimator can be found by using the properties of variance. Since the $Y_{i}^{(k)}$ for $i=1,\\dots,M$ are i.i.d.,\n$$\n\\mathrm{Var}(\\widehat{I}_{k}) = \\mathrm{Var}\\left( |D_{k}|\\,H\\,\\frac{1}{M}\\sum_{i=1}^{M} Y_{i}^{(k)} \\right) = \\left( \\frac{|D_{k}|\\,H}{M} \\right)^2 \\mathrm{Var}\\left( \\sum_{i=1}^{M} Y_{i}^{(k)} \\right) = \\frac{(|D_{k}|\\,H)^2}{M^2} \\sum_{i=1}^{M} \\mathrm{Var}(Y_{i}^{(k)})\n$$\nAs the variables are identically distributed, $\\mathrm{Var}(Y_{i}^{(k)}) = \\mathrm{Var}(Y_{1}^{(k)})$ for all $i$. For simplicity, let's denote $Y_1^{(k)}$ as $Y^{(k)}$.\n$$\n\\mathrm{Var}(\\widehat{I}_{k}) = \\frac{(|D_{k}|\\,H)^2}{M^2} \\cdot M \\cdot \\mathrm{Var}(Y^{(k)}) = \\frac{(|D_{k}|\\,H)^2}{M} \\mathrm{Var}(Y^{(k)})\n$$\nThe random variable $Y^{(k)} = \\mathbf{1}\\{U^{(k)} \\le g(X^{(k)})\\}$ is an indicator variable. Therefore, it follows a Bernoulli distribution with parameter $p_k = P(Y^{(k)}=1)$. The variance of a Bernoulli random variable is given by $\\mathrm{Var}(Y^{(k)}) = p_k(1-p_k)$.\n\nWe must find $p_k$. The parameter $p_k$ is the probability of a \"hit,\" i.e., $p_k = P(U^{(k)} \\le g(X^{(k)}))$. We can compute this probability by integrating the joint probability density function (PDF) of $(X^{(k)}, U^{(k)})$ over the region where $u \\le g(x)$.\n\nThe random variable $X^{(k)}$ is uniform on $D_k$, so its PDF is $f_{X^{(k)}}(x) = \\frac{1}{|D_k|}$ for $x \\in D_k$ and $0$ otherwise.\nThe random variable $U^{(k)}$ is uniform on $[0,H]$, so its PDF is $f_{U^{(k)}}(u) = \\frac{1}{H}$ for $u \\in [0,H]$ and $0$ otherwise.\nSince $X^{(k)}$ and $U^{(k)}$ are independent, their joint PDF is $f_{X^{(k)},U^{(k)}}(x,u) = f_{X^{(k)}}(x)f_{U^{(k)}}(u) = \\frac{1}{|D_k|H}$ for $(x,u) \\in D_k \\times [0,H]$, and $0$ otherwise.\n\nNow we compute $p_k$:\n$$\np_k = P(U^{(k)} \\le g(X^{(k)})) = \\int_{\\mathbb{R}^d} \\int_{\\mathbb{R}} \\mathbf{1}\\{u \\le g(x)\\} f_{X^{(k)},U^{(k)}}(x,u) \\,\\mathrm{d}u \\,\\mathrm{d}x\n$$\nThe integral is non-zero only over the domain $D_k \\times [0,H]$:\n$$\np_k = \\int_{D_k} \\int_{0}^{H} \\mathbf{1}\\{u \\le g(x)\\} \\frac{1}{|D_k|H} \\,\\mathrm{d}u \\,\\mathrm{d}x = \\frac{1}{|D_k|H} \\int_{D_k} \\left( \\int_{0}^{H} \\mathbf{1}\\{u \\le g(x)\\} \\,\\mathrm{d}u \\right) \\,\\mathrm{d}x\n$$\nThe inner integral evaluates to $\\int_{0}^{g(x)} 1 \\,\\mathrm{d}u = g(x)$. This is valid because we are given that $g(x) \\ge 0$ and $g(x) \\le \\sup g \\le H$.\n$$\np_k = \\frac{1}{|D_k|H} \\int_{D_k} g(x) \\,\\mathrm{d}x\n$$\nWe are given that the support of $g$ is $S$, and $S \\subset D_k$. This means $g(x)=0$ for any $x \\in D_k \\setminus S$. Therefore, the integral of $g(x)$ over $D_k$ is equal to its integral over all of $\\mathbb{R}^d$:\n$$\n\\int_{D_k} g(x) \\,\\mathrm{d}x = \\int_{S} g(x) \\,\\mathrm{d}x = I\n$$\nSubstituting this into the expression for $p_k$, we get:\n$$\np_k = \\frac{I}{|D_k|H}\n$$\nNow we can express the variance of $Y^{(k)}$ in terms of the problem parameters:\n$$\n\\mathrm{Var}(Y^{(k)}) = p_k(1-p_k) = \\frac{I}{|D_k|H} \\left( 1 - \\frac{I}{|D_k|H} \\right)\n$$\nSubstituting this back into the formula for the variance of the estimator $\\widehat{I}_k$:\n$$\n\\mathrm{Var}(\\widehat{I}_{k}) = \\frac{(|D_k|\\,H)^2}{M} \\mathrm{Var}(Y^{(k)}) = \\frac{(|D_k|\\,H)^2}{M} \\left[ \\frac{I}{|D_k|H} \\left( 1 - \\frac{I}{|D_k|H} \\right) \\right]\n$$\nSimplifying the expression:\n$$\n\\mathrm{Var}(\\widehat{I}_{k}) = \\frac{|D_k|\\,H \\cdot I}{M} \\left( 1 - \\frac{I}{|D_k|H} \\right) = \\frac{|D_k|\\,H \\cdot I}{M} \\left( \\frac{|D_k|\\,H - I}{|D_k|\\,H} \\right)\n$$\n$$\n\\mathrm{Var}(\\widehat{I}_{k}) = \\frac{I}{M} (|D_k|\\,H - I)\n$$\nThis is the variance of the estimator for a given bounding box $D_k$. We can now form the ratio of the variances for $k=1$ and $k=2$.\n$$\n\\frac{\\mathrm{Var}(\\widehat{I}_{1})}{\\mathrm{Var}(\\widehat{I}_{2})} = \\frac{\\frac{I}{M} (|D_{1}|\\,H - I)}{\\frac{I}{M} (|D_{2}|\\,H - I)}\n$$\nAssuming $I  0$, the term $\\frac{I}{M}$ cancels. If $I=0$, $g(x)$ is zero almost everywhere, the variances are both zero, and the ratio is indeterminate. For a non-trivial problem, we assume $I0$.\n$$\n\\frac{\\mathrm{Var}(\\widehat{I}_{1})}{\\mathrm{Var}(\\widehat{I}_{2})} = \\frac{|D_{1}|\\,H - I}{|D_{2}|\\,H - I}\n$$\nThis expression is the final result, expressed solely in terms of $|D_{1}|$, $|D_{2}|$, $H$, and $I$, as required.", "answer": "$$\n\\boxed{\\frac{|D_{1}|\\,H - I}{|D_{2}|\\,H - I}}\n$$", "id": "3312364"}, {"introduction": "Hit-or-miss Monte Carlo is one of several tools for numerical integration, and knowing when to use it is as important as knowing how. This practice provides a direct comparative analysis between the hit-or-miss method and the standard simple Monte Carlo estimator [@problem_id:3312391]. By constructing two carefully chosen integrands, you will quantitatively explore the scenarios where hit-or-miss is perfectly efficient and where its performance degrades, thereby building intuition for algorithmic selection.", "problem": "Let $D=[0,1]$ be the unit interval equipped with the Lebesgue measure. Consider estimating the integral $I=\\int_{0}^{1} g(x)\\,dx$ using two methods:\n- Hit-or-miss Monte Carlo (HOM): draw independent and identically distributed pairs $(X_i,U_i)$ with $X_i \\sim \\text{Uniform}[0,1]$ and $U_i \\sim \\text{Uniform}[0,1]$, independent of $X_i$, and define $H_i=\\mathbf{1}\\{U_i \\le g(X_i)\\}$. Use the estimator $\\hat{I}_{\\text{hom}}=\\frac{1}{N}\\sum_{i=1}^{N} H_i$.\n- Simple Monte Carlo (SMC): draw $X_i \\sim \\text{Uniform}[0,1]$ independent and identically distributed, and use $\\hat{I}_{\\text{smc}}=\\frac{1}{N}\\sum_{i=1}^{N} g(X_i)$.\n\nStart from the core definitions of expectation and variance for independent and identically distributed averages and the construction of indicator variables. For a fixed envelope bound consistent with HOM, assume $g(x)\\in[0,1]$ for all $x\\in[0,1]$, so that the HOM construction is valid without scaling.\n\nConstruct two concrete functions $g$ and analyze the efficiency of HOM relative to SMC by computing the exact ratio of their asymptotic variances (i.e., the variance of the estimators scaled by $N$), defined as\n$$\nR(g)\\equiv \\frac{\\operatorname{Var}(\\hat{I}_{\\text{hom}})}{\\operatorname{Var}(\\hat{I}_{\\text{smc}})}.\n$$\nSpecifically, use the following two functions:\n- Near-optimal case: $g_1(x)=\\mathbf{1}\\{x\\le p\\}$ for an arbitrary but fixed parameter $p\\in(0,1)$.\n- Dramatically suboptimal case: $g_2(x)=\\begin{cases}\n1, x\\in[0,q],\\\\\n\\epsilon, x\\in(q,1],\n\\end{cases}$ for fixed parameters $q\\in(0,1)$ and $\\epsilon\\in(0,1)$.\n\nCompute $R(g_1)$ and $R(g_2)$ exactly as closed-form analytic expressions in terms of $p$, $q$, and $\\epsilon$, and provide the final result as a two-entry row matrix $\\begin{pmatrix} R(g_1)  R(g_2)\\end{pmatrix}$. No numerical rounding is required.", "solution": "The problem is valid as it is a well-posed, scientifically grounded question within the domain of stochastic simulation and Monte Carlo methods. All terms are formally defined, the requested calculations are feasible, and the premise rests on established principles of probability and statistics.\n\nLet $I = \\int_{0}^{1} g(x)\\,dx$. The two estimators are given by:\n1.  Hit-or-miss Monte Carlo (HOM): $\\hat{I}_{\\text{hom}}=\\frac{1}{N}\\sum_{i=1}^{N} H_i$, where $H_i=\\mathbf{1}\\{U_i \\le g(X_i)\\}$.\n2.  Simple Monte Carlo (SMC): $\\hat{I}_{\\text{smc}}=\\frac{1}{N}\\sum_{i=1}^{N} g(X_i)$.\n\nThe samples $\\{H_i\\}_{i=1}^N$ are independent and identically distributed (i.i.d.) random variables. Similarly, $\\{g(X_i)\\}_{i=1}^N$ are i.i.d. The variance of an average of $N$ i.i.d. random variables $Y_i$ is $\\frac{1}{N}\\operatorname{Var}(Y_1)$. Thus, the asymptotic variances (scaled by $N$) are $\\operatorname{Var}(H_1)$ and $\\operatorname{Var}(g(X_1))$, respectively. The ratio of these variances is:\n$$\nR(g) = \\frac{N \\operatorname{Var}(\\hat{I}_{\\text{hom}})}{N \\operatorname{Var}(\\hat{I}_{\\text{smc}})} = \\frac{\\operatorname{Var}(H_1)}{\\operatorname{Var}(g(X_1))}\n$$\n\nWe first derive general expressions for these two variances.\n\nFor the HOM estimator, the variable $H_1 = \\mathbf{1}\\{U_1 \\le g(X_1)\\}$ is a Bernoulli random variable. Its expectation, $\\pi$, is the probability of a \"hit\":\n$$\n\\pi = P(H_1=1) = E[H_1] = E[\\mathbf{1}\\{U_1 \\le g(X_1)\\}]\n$$\nBy the law of total expectation, we can condition on $X_1=x$:\n$$\n\\pi = E[ P(U_1 \\le g(X_1) | X_1=x) ] = E[g(x)]\n$$\nSince $X_1 \\sim \\text{Uniform}[0,1]$ with probability density function $f_X(x)=1$ for $x \\in [0,1]$, we have:\n$$\n\\pi = \\int_{0}^{1} g(x)f_X(x)\\,dx = \\int_{0}^{1} g(x)\\,dx = I\n$$\nThe variance of a Bernoulli variable with parameter $\\pi$ is $\\pi(1-\\pi)$. Therefore:\n$$\n\\operatorname{Var}(H_1) = I(1-I)\n$$\n\nFor the SMC estimator, the variance of the random variable $g(X_1)$ is given by the standard formula:\n$$\n\\operatorname{Var}(g(X_1)) = E[g(X_1)^2] - (E[g(X_1)])^2\n$$\nWe have already shown that $E[g(X_1)] = I$. The expectation of the square is:\n$$\nE[g(X_1)^2] = \\int_{0}^{1} g(x)^2 f_X(x)\\,dx = \\int_{0}^{1} g(x)^2 \\,dx\n$$\nSo, the variance for the SMC method is:\n$$\n\\operatorname{Var}(g(X_1)) = \\int_{0}^{1} g(x)^2 \\,dx - I^2\n$$\n\nCombining these results, the general expression for the relative efficiency is:\n$$\nR(g) = \\frac{I(1-I)}{\\int_{0}^{1} g(x)^2 \\,dx - I^2}\n$$\n\nNow we apply this formula to the two specified functions.\n\nCase 1: $g_1(x) = \\mathbf{1}\\{x \\le p\\}$ for $p \\in (0,1)$.\nFirst, we compute the integral $I_1$:\n$$\nI_1 = \\int_{0}^{1} g_1(x)\\,dx = \\int_{0}^{1} \\mathbf{1}\\{x \\le p\\}\\,dx = \\int_{0}^{p} 1 \\,dx = p\n$$\nNext, we compute the integral of the squared function. Since $g_1(x)$ only takes values $0$ and $1$, $g_1(x)^2 = g_1(x)$.\n$$\n\\int_{0}^{1} g_1(x)^2 \\,dx = \\int_{0}^{1} g_1(x)\\,dx = p\n$$\nNow, we substitute these into the expression for $R(g_1)$:\n$$\nR(g_1) = \\frac{I_1(1-I_1)}{\\int_{0}^{1} g_1(x)^2 \\,dx - I_1^2} = \\frac{p(1-p)}{p - p^2} = \\frac{p(1-p)}{p(1-p)} = 1\n$$\nThis result is expected, as for an indicator function, the SMC random variable $g_1(X_1)$ is itself a Bernoulli variable with parameter $p$, identical in distribution to the HOM variable $H_1$.\n\nCase 2: $g_2(x)=\\begin{cases} 1, x\\in[0,q] \\\\ \\epsilon, x\\in(q,1] \\end{cases}$ for $q \\in (0,1)$ and $\\epsilon \\in (0,1)$.\nFirst, we compute the integral $I_2$:\n$$\nI_2 = \\int_{0}^{1} g_2(x)\\,dx = \\int_{0}^{q} 1\\,dx + \\int_{q}^{1} \\epsilon\\,dx = q + \\epsilon(1-q)\n$$\nNext, we compute the integral of the squared function:\n$$\n\\int_{0}^{1} g_2(x)^2 \\,dx = \\int_{0}^{q} 1^2\\,dx + \\int_{q}^{1} \\epsilon^2\\,dx = q + \\epsilon^2(1-q)\n$$\nNow we compute the numerator and denominator for $R(g_2)$.\nThe numerator is:\n$$\nI_2(1-I_2) = (q + \\epsilon(1-q))(1 - (q + \\epsilon(1-q))) = (q + \\epsilon(1-q))(1-q - \\epsilon(1-q))\n$$\nFactoring out common terms:\n$$\nI_2(1-I_2) = (q(1-\\epsilon)+\\epsilon) \\cdot (1-q)(1-\\epsilon)\n$$\nThe denominator is:\n$$\n\\int_{0}^{1} g_2(x)^2 \\,dx - I_2^2 = (q + \\epsilon^2(1-q)) - (q + \\epsilon(1-q))^2\n$$\nLet's expand and simplify:\n$$\n(q + \\epsilon^2(1-q)) - (q^2 + 2q\\epsilon(1-q) + \\epsilon^2(1-q)^2)\n$$\n$$\n= q - q^2 - 2q\\epsilon(1-q) + \\epsilon^2(1-q) - \\epsilon^2(1-q)^2\n$$\nFactor out $(1-q)$:\n$$\n= (1-q)[q - 2q\\epsilon + \\epsilon^2 - \\epsilon^2(1-q)]\n$$\n$$\n= (1-q)[q - 2q\\epsilon + \\epsilon^2 - \\epsilon^2 + q\\epsilon^2]\n$$\n$$\n= (1-q)[q - 2q\\epsilon + q\\epsilon^2]\n$$\nFactor out $q$:\n$$\n= q(1-q)[1 - 2\\epsilon + \\epsilon^2] = q(1-q)(1-\\epsilon)^2\n$$\nNow, we form the ratio $R(g_2)$:\n$$\nR(g_2) = \\frac{(q(1-\\epsilon)+\\epsilon)(1-q)(1-\\epsilon)}{q(1-q)(1-\\epsilon)^2}\n$$\nSince $q \\in (0,1)$ and $\\epsilon \\in (0,1)$, we have $1-q \\ne 0$ and $1-\\epsilon \\ne 0$, so we can cancel these terms:\n$$\nR(g_2) = \\frac{q(1-\\epsilon)+\\epsilon}{q(1-\\epsilon)} = \\frac{q(1-\\epsilon)}{q(1-\\epsilon)} + \\frac{\\epsilon}{q(1-\\epsilon)} = 1 + \\frac{\\epsilon}{q(1-\\epsilon)}\n$$\nThis expression demonstrates that for $g_2$, the HOM method is strictly less efficient (i.e., has a larger variance, since $R(g_2)1$) than the SMC method. The inefficiency becomes arbitrarily large as $q \\to 0^+$.\n\nThe two results are $R(g_1)=1$ and $R(g_2)=1 + \\frac{\\epsilon}{q(1-\\epsilon)}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  1 + \\frac{\\epsilon}{q(1-\\epsilon)} \\end{pmatrix}}\n$$", "id": "3312391"}, {"introduction": "A significant challenge in stochastic simulation is the estimation of rare-event probabilities, a domain where naive Monte Carlo methods often fail spectacularly. This final practice explores why the standard hit-or-miss estimator becomes impractical when the target integral $p$ is very small, and challenges you to evaluate more sophisticated solutions [@problem_id:3312383]. You will analyze how the estimator's relative error behaves as $p \\to 0$ and assess proposed variance reduction techniques, such as importance sampling and stratification, to determine which ones can provide reliable estimates with a feasible computational budget.", "problem": "Consider estimating the rare-event integral (a volume) $$p \\;=\\; \\int_{[0,1]^d} \\mathbf{1}_A(x)\\,dx,$$ where $A \\subset [0,1]^d$ is the hyperrectangle $A \\;=\\; [0,\\varepsilon]^d$ with $\\varepsilon \\in (0,1)$ and dimension $d \\in \\mathbb{N}$. The standard hit-or-miss Monte Carlo estimator with a fixed number $n \\in \\mathbb{N}$ of independent and identically distributed samples $X_1,\\dots,X_n \\sim \\text{Uniform}([0,1]^d)$ is $$\\widehat{p}_{\\text{HM}} \\;=\\; \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}_A(X_i).$$ Using only fundamental definitions of expectation, variance, Bernoulli random variables, and the law of large numbers, reason about why the fixed-$n$ hit-or-miss strategy becomes impractical as $\\varepsilon \\to 0$ (so that $p = \\varepsilon^d \\to 0$). Then, assess each proposed modification below with respect to: (i) unbiasedness for $p$, and (ii) whether it mitigates the rare-event blow-up in relative error as $p \\to 0$ under fixed $n$, by achieving a relative error that does not scale like a positive power of $p^{-1}$.\n\nSelect all options that both correctly explain the impracticality of fixed-$n$ hit-or-miss in the rare-event regime and propose a valid unbiased modification that achieves an $O(n^{-1/2})$ relative error independent of $p$ or strictly smaller (in order) than the baseline $O((n p)^{-1/2})$.\n\nA. Importance sampling via a two-component mixture. Let $u_D$ denote the uniform density on $[0,1]^d$ (density equal to $1$ on $[0,1]^d$) and $u_A$ the uniform density on $A$ (density equal to $1/p$ on $A$ and $0$ elsewhere). For a fixed $\\alpha \\in (0,1)$, define the proposal density $$q(x) \\;=\\; \\alpha\\,u_A(x) \\,+\\, (1-\\alpha)\\,u_D(x).$$ Draw $X_1,\\dots,X_n \\overset{\\text{i.i.d.}}{\\sim} q$ and use the importance-weighted estimator $$\\widehat{p}_{\\text{IS}} \\;=\\; \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}_A(X_i)\\,\\frac{1}{q(X_i)}.$$ This estimator is unbiased for $p$ and, as $p \\to 0$, has relative error of order $O(n^{-1/2})$ independent of $p$.\n\nB. Two-strata stratified sampling using knowledge of a cover of the rare event. Suppose one can sample uniformly within a measurable set $S \\subset [0,1]^d$ of known volume $s \\in (0,1)$ such that $A \\subseteq S$ (for example, $S = [0,\\varepsilon']^d$ with $\\varepsilon' \\in (\\varepsilon,1)$ so $s = \\varepsilon'^d$). Form two strata $S$ and $S^c$, allocate $n_S \\in \\{1,\\dots,n\\}$ samples to $S$ and $n-n_S$ to $S^c$, and estimate $$\\widehat{p}_{\\text{STR}} \\;=\\; s \\cdot \\frac{1}{n_S}\\sum_{i=1}^{n_S} \\mathbf{1}_A(X_i^{(S)}) \\;+\\; (1-s)\\cdot \\frac{1}{n-n_S}\\sum_{j=1}^{n-n_S} \\mathbf{1}_A(X_j^{(S^c)}),$$ where $X_i^{(S)} \\sim \\text{Uniform}(S)$ and $X_j^{(S^c)} \\sim \\text{Uniform}(S^c)$. This estimator is unbiased; when $A \\subseteq S$ one has $\\mathbf{1}_A \\equiv 0$ on $S^c$, so the second term vanishes in expectation. If $S = A$ (so $s=p$), the estimator has zero variance; more generally, if $s \\in (p,1)$ and all samples are allocated to $S$ (i.e., $n_S = n$), then the relative error scales as $O\\!\\left(\\sqrt{s/(n p)}\\right)$, which strictly improves on the baseline $O((n p)^{-1/2})$ by the factor $\\sqrt{s}$ and becomes $O(n^{-1/2})$ when $s \\asymp p$.\n\nC. Control variates with a constant reference function. Take $g(x) \\equiv 1$ on $[0,1]^d$ (so $\\int g = 1$ is known) and form the control-variate estimator $\\widehat{p}_{\\text{CV}} = \\widehat{p}_{\\text{HM}} - \\beta(\\widehat{G} - 1)$ with $\\widehat{G} = (1/n)\\sum_{i=1}^n g(X_i)$ and some $\\beta \\in \\mathbb{R}$. This reduces the variance sufficiently to remove the rare-event difficulty and yields $O(n^{-1/2})$ relative error independent of $p$.\n\nD. Unweighted oversampling of the rare set. Draw $m \\in \\{1,\\dots,n-1\\}$ samples uniformly from $A$ and $n-m$ samples uniformly from $[0,1]^d$, and estimate $$\\widehat{p}_{\\text{UNW}} \\;=\\; \\frac{1}{n}\\left(\\sum_{i=1}^m \\mathbf{1}_A(X_i^{(A)}) \\;+\\; \\sum_{j=1}^{n-m} \\mathbf{1}_A(X_j^{(D)})\\right).$$ This is unbiased for $p$ and enjoys $O(n^{-1/2})$ relative error independent of $p$ as $p \\to 0$ even though no importance weights are used.\n\nE. Negative-binomial stopping with naive ratio. Instead of fixing $n$, keep sampling i.i.d. $X_i \\sim \\text{Uniform}([0,1]^d)$ until exactly $K \\in \\mathbb{N}$ hits in $A$ are observed (a negative-binomial stopping rule), stop at the random time $N$, and report $\\widehat{p}_{\\text{NB}} = K/N$. This is unbiased and attains $O(n^{-1/2})$ relative error independent of $p$ when $K$ is fixed and $p \\to 0$.", "solution": "The problem asks for an analysis of the standard hit-or-miss Monte Carlo method for estimating a rare-event probability and an evaluation of several proposed modifications.\n\nFirst, let us validate the problem statement. The problem is a well-defined question in the field of stochastic simulation and Monte Carlo methods. It concerns the estimation of an integral $$p \\;=\\; \\int_{[0,1]^d} \\mathbf{1}_A(x)\\,dx,$$ where $A = [0,\\varepsilon]^d$ is a hyperrectangle within the unit hypercube $[0,1]^d$. The volume of this set is clearly $p = \\varepsilon^d$. The standard hit-or-miss estimator is given, and the task is to analyze its behavior as $p \\to 0$ and then to evaluate alternative methods on the criteria of unbiasedness and mitigation of relative error blow-up. All terms are standard and the problem is scientifically and mathematically sound, well-posed, and objective. We may therefore proceed with the solution.\n\n**Analysis of the Standard Hit-or-Miss Estimator ($\\widehat{p}_{\\text{HM}}$)**\n\nThe estimator is $\\widehat{p}_{\\text{HM}} = \\frac{1}{n}\\sum_{i=1}^n Y_i$, where $Y_i = \\mathbf{1}_A(X_i)$ and $X_i \\sim \\text{Uniform}([0,1]^d)$. Each $Y_i$ is an independent and identically distributed Bernoulli random variable. The probability of success (i.e., $X_i \\in A$) is\n$$ \\mathbb{P}(Y_i=1) = \\mathbb{P}(X_i \\in A) = \\int_{[0,1]^d} \\mathbf{1}_A(x) \\,dx = p. $$\nThe expectation of the estimator is\n$$ \\mathbb{E}[\\widehat{p}_{\\text{HM}}] = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}[Y_i] = \\frac{1}{n} \\sum_{i=1}^n p = p. $$\nThe estimator is unbiased. The variance of a single Bernoulli trial is $\\text{Var}(Y_i) = p(1-p)$. Since the samples are i.i.d., the variance of the estimator is\n$$ \\text{Var}(\\widehat{p}_{\\text{HM}}) = \\frac{1}{n^2}\\sum_{i=1}^n \\text{Var}(Y_i) = \\frac{n p (1-p)}{n^2} = \\frac{p(1-p)}{n}. $$\nA key metric for the efficiency of an estimator is its relative error, defined as the ratio of its standard deviation to its mean:\n$$ \\text{RE}(\\widehat{p}_{\\text{HM}}) = \\frac{\\sqrt{\\text{Var}(\\widehat{p}_{\\text{HM}})}}{\\mathbb{E}[\\widehat{p}_{\\text{HM}}]} = \\frac{\\sqrt{p(1-p)/n}}{p} = \\frac{\\sqrt{1-p}}{\\sqrt{np}}. $$\nIn the rare-event regime, as $\\varepsilon \\to 0$, we have $p = \\varepsilon^d \\to 0$. The limiting behavior of the relative error is\n$$ \\lim_{p \\to 0} \\text{RE}(\\widehat{p}_{\\text{HM}}) = \\lim_{p \\to 0} \\frac{\\sqrt{1-p}}{\\sqrt{np}} = \\frac{1}{\\sqrt{np}}. $$\nThe relative error scales as $O((np)^{-1/2})$. For a fixed number of samples $n$, the relative error blows up as $p \\to 0$. To achieve a constant relative error $\\delta$, one would require $\\frac{1}{\\sqrt{np}} \\approx \\delta$, which implies $n \\approx \\frac{1}{p\\delta^2}$. The number of samples required grows as $p^{-1}$, which becomes computationally prohibitive for very small $p$. This is the fundamental impracticality of the fixed-$n$ hit-or-miss strategy for rare events.\n\nNow, we evaluate each proposed modification.\n\n**Option A: Importance Sampling**\n\nThe proposal density is $q(x) = \\alpha\\,u_A(x) + (1-\\alpha)\\,u_D(x)$. For $x \\in A$, $u_A(x) = 1/p$ and $u_D(x) = 1$, so $q(x) = \\alpha/p + 1-\\alpha$. For $x \\in [0,1]^d \\setminus A$, $u_A(x) = 0$ and $u_D(x)=1$, so $q(x) = 1-\\alpha$. The integral of $q(x)$ over $[0,1]^d$ is $\\int_A (\\frac{\\alpha}{p} + 1-\\alpha)dx + \\int_{[0,1]^d\\setminus A} (1-\\alpha)dx = p(\\frac{\\alpha}{p} + 1-\\alpha) + (1-p)(1-\\alpha) = \\alpha + p(1-\\alpha) + (1-p)(1-\\alpha) = \\alpha + (1-\\alpha) = 1$. The density is valid.\n\nThe estimator is $\\widehat{p}_{\\text{IS}} = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}_A(X_i)\\,\\frac{1}{q(X_i)}$, where $X_i \\sim q$.\n- **Unbiasedness**: The expectation is $\\mathbb{E}_q[\\widehat{p}_{\\text{IS}}] = \\mathbb{E}_q\\left[\\mathbf{1}_A(X)\\frac{1}{q(X)}\\right] = \\int_{[0,1]^d} \\mathbf{1}_A(x)\\frac{1}{q(x)} q(x) dx = \\int_A 1\\,dx = p$. The estimator is unbiased.\n- **Relative Error**: The variance is $\\frac{1}{n}\\text{Var}_q\\left(\\mathbf{1}_A(X)\\frac{1}{q(X)}\\right)$. The second moment is $M_2 = \\mathbb{E}_q\\left[\\left(\\mathbf{1}_A(X)\\frac{1}{q(X)}\\right)^2\\right] = \\int_{[0,1]^d} \\left(\\mathbf{1}_A(x)\\frac{1}{q(x)}\\right)^2 q(x) dx = \\int_A \\frac{1}{q(x)} dx$. For $x \\in A$, $q(x) = \\frac{\\alpha}{p} + 1-\\alpha$.\n$$ M_2 = \\int_A \\frac{1}{\\frac{\\alpha}{p} + 1-\\alpha} dx = p \\frac{p}{\\alpha + p(1-\\alpha)} = \\frac{p^2}{\\alpha + p(1-\\alpha)}. $$\nThe variance of a single term is $M_2 - (\\mathbb{E}[\\cdot])^2 = M_2 - p^2 = p^2\\left(\\frac{1}{\\alpha + p(1-\\alpha)} - 1\\right) = p^2 \\frac{1-\\alpha-p(1-\\alpha)}{\\alpha+p(1-\\alpha)}$.\nThe variance of the estimator is $\\text{Var}(\\widehat{p}_{\\text{IS}}) = \\frac{p^2}{n} \\frac{1-\\alpha-p(1-\\alpha)}{\\alpha+p(1-\\alpha)}$.\nThe relative error is $\\text{RE}(\\widehat{p}_{\\text{IS}}) = \\frac{\\sqrt{\\text{Var}(\\widehat{p}_{\\text{IS}})}}{p} = \\frac{1}{\\sqrt{n}}\\sqrt{\\frac{1-\\alpha-p(1-\\alpha)}{\\alpha+p(1-\\alpha)}}$.\nAs $p \\to 0$, this converges to $\\frac{1}{\\sqrt{n}}\\sqrt{\\frac{1-\\alpha}{\\alpha}}$. This is of order $O(n^{-1/2})$ and is independent of $p$. The method successfully mitigates the relative error blow-up.\nThe option's claims are entirely correct.\n\nVerdict for A: **Correct**.\n\n**Option B: Stratified Sampling**\n\nThe estimator is $\\widehat{p}_{\\text{STR}} = s \\widehat{p}_S + (1-s) \\widehat{p}_{S^c}$, where $\\widehat{p}_S$ and $\\widehat{p}_{S^c}$ are estimators for the conditional probabilities $p_S = \\mathbb{P}(X \\in A | X \\in S)$ and $p_{S^c} = \\mathbb{P}(X \\in A | X \\in S^c)$, respectively.\n- **Unbiasedness**: Since $A \\subseteq S$, any sample $X_j^{(S^c)}$ from $S^c$ cannot be in $A$. Thus, $\\mathbf{1}_A(X_j^{(S^c)}) = 0$ for all $j$, making $\\widehat{p}_{S^c}=0$. The estimator simplifies to $\\widehat{p}_{\\text{STR}} = s \\cdot \\frac{1}{n_S}\\sum_{i=1}^{n_S} \\mathbf{1}_A(X_i^{(S)})$. The expectation is $\\mathbb{E}[\\widehat{p}_{\\text{STR}}] = s \\cdot \\mathbb{E}\\left[\\frac{1}{n_S}\\sum \\mathbf{1}_A(X_i^{(S)})\\right] = s \\cdot \\mathbb{P}(X^{(S)} \\in A) = s \\cdot \\frac{\\text{vol}(A)}{\\text{vol}(S)} = s \\cdot \\frac{p}{s} = p$. The estimator is unbiased.\n- **Relative Error**: The variance is $\\text{Var}(\\widehat{p}_{\\text{STR}}) = s^2 \\text{Var}\\left(\\frac{1}{n_S}\\sum \\mathbf{1}_A(X_i^{(S)})\\right)$. The term $\\mathbf{1}_A(X_i^{(S)})$ is a Bernoulli random variable with parameter $p/s$. Its variance is $\\frac{p}{s}(1-\\frac{p}{s})$.\n$$ \\text{Var}(\\widehat{p}_{\\text{STR}}) = s^2 \\frac{\\frac{p}{s}(1-\\frac{p}{s})}{n_S} = \\frac{s^2}{n_S}\\frac{p(s-p)}{s^2} = \\frac{p(s-p)}{n_S}. $$\nThe relative error is $\\text{RE}(\\widehat{p}_{\\text{STR}}) = \\frac{\\sqrt{p(s-p)/n_S}}{p} = \\sqrt{\\frac{s-p}{p n_S}} = \\sqrt{\\frac{s/p - 1}{n_S}}$.\nThe option considers allocating all samples with $n_S=n$. The RE is $\\sqrt{\\frac{s/p - 1}{n}}$. Compared to the baseline RE of $\\sqrt{\\frac{1-p}{np}} \\approx \\sqrt{\\frac{1}{np}}$, this is an improvement by a factor of $\\sqrt{s(1-p/s)/(1-p)} \\approx \\sqrt{s}$. Since $s1$, this is a strict improvement as claimed. If one can choose $S$ such that $s \\asymp p$ (i.e., $s=Cp$ for some constant $C1$), the RE becomes $\\sqrt{\\frac{C-1}{n}}$, which is $O(n^{-1/2})$ and independent of $p$. The option's claims are correct.\n\nVerdict for B: **Correct**.\n\n**Option C: Control Variates**\n\nThe control variate is $g(x) \\equiv 1$. The estimator for its mean is $\\widehat{G} = \\frac{1}{n}\\sum_{i=1}^n g(X_i) = \\frac{1}{n}\\sum_{i=1}^n 1 = 1$. The known mean of $g(x)$ is $\\int_{[0,1]^d} 1\\,dx = 1$.\nThe control-variate estimator is $\\widehat{p}_{\\text{CV}} = \\widehat{p}_{\\text{HM}} - \\beta(\\widehat{G} - 1)$. Since $\\widehat{G}=1$, the correction term is $\\beta(1-1) = 0$.\nThus, $\\widehat{p}_{\\text{CV}} = \\widehat{p}_{\\text{HM}}$. The estimator is unchanged from the standard hit-or-miss estimator. Consequently, its variance is also unchanged. A control variate must be correlated with the integrand to achieve variance reduction. The covariance between $\\mathbf{1}_A(X)$ and the constant $g(X)=1$ is $\\text{Cov}(\\mathbf{1}_A(X), 1) = \\mathbb{E}[\\mathbf{1}_A(X) \\cdot 1] - \\mathbb{E}[\\mathbf{1}_A(X)]\\mathbb{E}[1] = p - p \\cdot 1 = 0$. The optimal coefficient $\\beta^*$ is proportional to this covariance and is therefore $0$. With any $\\beta$, the variance is not reduced because $\\text{Var}(g(X))=0$. The claim that this method reduces variance and removes the rare-event difficulty is false.\n\nVerdict for C: **Incorrect**.\n\n**Option D: Unweighted Oversampling**\n\nThe estimator is $\\widehat{p}_{\\text{UNW}} = \\frac{1}{n}\\left(\\sum_{i=1}^m \\mathbf{1}_A(X_i^{(A)}) + \\sum_{j=1}^{n-m} \\mathbf{1}_A(X_j^{(D)})\\right)$.\n$X_i^{(A)} \\sim \\text{Uniform}(A)$, so $\\mathbf{1}_A(X_i^{(A)})$ is identically $1$. The first sum is just $m$.\n$X_j^{(D)} \\sim \\text{Uniform}([0,1]^d)$, so $\\mathbb{E}[\\mathbf{1}_A(X_j^{(D)})] = p$.\n- **Unbiasedness**: Let's compute the expectation.\n$$ \\mathbb{E}[\\widehat{p}_{\\text{UNW}}] = \\frac{1}{n}\\left( \\mathbb{E}\\left[\\sum_{i=1}^m 1\\right] + \\mathbb{E}\\left[\\sum_{j=1}^{n-m} \\mathbf{1}_A(X_j^{(D)})\\right] \\right) = \\frac{1}{n}(m + (n-m)p) = \\frac{m}{n} + p - \\frac{m}{n}p = p + \\frac{m}{n}(1-p). $$\nSince $m \\in \\{1,\\dots,n-1\\}$ and we are in the regime $p \\in (0,1)$, we have $\\frac{m}{n}(1-p)  0$. Therefore, $\\mathbb{E}[\\widehat{p}_{\\text{UNW}}] \\neq p$. The estimator is biased. This method is a form of mixture sampling but fails to use the necessary importance weights to correct for the biased sampling scheme, making the resulting estimator biased. The option's claim of unbiasedness is false.\n\nVerdict for D: **Incorrect**.\n\n**Option E: Negative-Binomial Stopping**\n\nThe sampling stops when $K$ hits are observed at sample $N$. The estimator is $\\widehat{p}_{\\text{NB}} = K/N$.\n- **Unbiasedness**: The number of trials $N$ is a random variable following a negative binomial distribution. Its support is $\\{K, K+1, \\dots\\}$. The function $f(x)=K/x$ is strictly convex for $x0$. By Jensen's inequality, for any non-constant random variable $N$:\n$$ \\mathbb{E}[\\widehat{p}_{\\text{NB}}] = \\mathbb{E}[K/N]  K/\\mathbb{E}[N]. $$\nThe expectation of a negative binomial random variable $N$ with parameters $K$ and $p$ is $\\mathbb{E}[N] = K/p$.\nThus, $\\mathbb{E}[\\widehat{p}_{\\text{NB}}]  K/(K/p) = p$. The estimator is biased (it overestimates $p$). The claim that it is unbiased is false. (Note: An unbiased estimator for this sampling scheme is $\\widehat{p} = (K-1)/(N-1)$ for $K \\ge 2$.)\n\nVerdict for E: **Incorrect**.\n\n**Conclusion**\n\nOnly options A and B correctly describe a valid, unbiased modification to the standard Monte Carlo method that successfully mitigates the blow-up of relative error in the rare-event regime. Both importance sampling (A) and stratified sampling (B), when properly implemented, can achieve a relative error of $O(n^{-1/2})$ that is independent of $p$, thus solving the rare event problem.", "answer": "$$\\boxed{AB}$$", "id": "3312383"}]}