{"hands_on_practices": [{"introduction": "Understanding the variance of an estimator is the first step toward quantifying its uncertainty. This exercise provides a foundational look into the precision of the post-stratified mean estimator. You will first derive the exact conditional variance formula from first principles in a finite population context and then use Monte Carlo simulation to assess the performance of a common 'plug-in' variance estimator, revealing its properties in both typical and edge-case scenarios [@problem_id:3330424].", "problem": "Consider a finite population partitioned into $H$ post-strata indexed by $h \\in \\{1,\\dots,H\\}$. Stratum $h$ contains $N_h$ units with values $\\{y_{h,i}\\}_{i=1}^{N_h}$. A single-stage Simple Random Sample Without Replacement (SRSWOR) of size $n$ is drawn from the full population of size $N=\\sum_{h=1}^H N_h$. After sampling, the observed stratum counts are $\\{n_h\\}_{h=1}^H$ with $\\sum_{h=1}^H n_h = n$. Define the population post-stratification weights $W_h = N_h/N$. The post-stratified estimator of the overall mean is $\\hat{\\mu}_{PS} = \\sum_{h=1}^H W_h \\,\\bar{Y}_{h}$ where $\\bar{Y}_{h}$ is the sample mean of the observed units from stratum $h$. Work conditionally on the realized $\\{n_h\\}$.\n\nTask A (derivation): Starting from fundamental definitions of SRSWOR in a finite population and the law of total variance, derive the exact conditional variance $\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\})$ in terms of the stratum sizes $\\{N_h\\}$, the realized stratum sample sizes $\\{n_h\\}$, the finite population post-stratum weights $\\{W_h\\}$, and the finite-population within-stratum variances. You must begin from the well-tested fact that under SRSWOR within a single finite stratum of size $N_h$, for a sample mean computed from $n_h$ units, the variance equals the product of the finite population correction and the within-stratum variance scaled by $1/n_h$. Do not assume or state the final expression in advance; derive it from first principles.\n\nTask B (assessment via stochastic simulation): When the within-stratum finite-population variances are unknown in practice, a natural plug-in estimator replaces each unknown within-stratum variance with the corresponding unbiased sample variance computed from the observed units in that stratum. Assess the accuracy of this plug-in estimator by Monte Carlo (stochastic) simulation conditional on the realized $\\{n_h\\}$ by comparing its average value to the exact conditional variance derived in Task A.\n\nPrecisely implement the following for each test case in the test suite below.\n\n1. Deterministic population construction. For each stratum $h \\in \\{1,\\dots,H\\}$ and each unit index $i \\in \\{1,\\dots,N_h\\}$, generate the finite population values $y_{h,i}$ by the case-specific deterministic formula given in the test suite. This ensures that the population is fixed and known.\n\n2. Exact conditional variance. Let $S_h^2$ denote the finite-population variance within stratum $h$ computed with denominator $N_h-1$, i.e., $S_h^2 = \\frac{1}{N_h-1}\\sum_{i=1}^{N_h}\\left(y_{h,i}-\\mu_h\\right)^2$ where $\\mu_h = \\frac{1}{N_h}\\sum_{i=1}^{N_h} y_{h,i}$. Using your derivation from Task A, compute the exact conditional variance $V_{\\text{true}}$ of $\\hat{\\mu}_{PS}$ given the realized $\\{n_h\\}$.\n\n3. Monte Carlo approximation of the sampling distribution. Conditional on the realized $\\{n_h\\}$, simulate $R$ independent replicates of the following experiment: for each stratum $h$, draw an SRSWOR sample of size $n_h$ from the $N_h$ units in that stratum; compute the stratum sample mean $\\bar{Y}_h$ and the unbiased stratum sample variance $s_h^2$ with denominator $n_h-1$ when $n_h \\ge 2$. When $n_h = 1$, set $s_h^2 = 0$ by convention. For each replicate $r \\in \\{1,\\dots,R\\}$, compute:\n   - the post-stratified estimator $\\hat{\\mu}_{PS}^{(r)} = \\sum_{h=1}^H W_h \\bar{Y}_h^{(r)}$,\n   - the plug-in variance estimator $\\hat{V}^{(r)} = \\sum_{h=1}^H W_h^2 \\left(1 - \\frac{n_h}{N_h}\\right)\\frac{s_h^{2\\,(r)}}{n_h}$.\n   At the end, compute the Monte Carlo variance $V_{\\text{mc}}$ as the empirical variance (with denominator $R-1$) of $\\{\\hat{\\mu}_{PS}^{(r)}\\}_{r=1}^R$, and compute the Monte Carlo average plug-in variance $\\overline{V}_{\\text{plug}} = \\frac{1}{R}\\sum_{r=1}^R \\hat{V}^{(r)}$.\n\n4. Accuracy metrics. Report, for each test case, the following floats:\n   - $V_{\\text{true}}$;\n   - the absolute relative error of the Monte Carlo variance, $|V_{\\text{mc}} - V_{\\text{true}}|/V_{\\text{true}}$;\n   - the relative bias of the plug-in estimator, $(\\overline{V}_{\\text{plug}} - V_{\\text{true}})/V_{\\text{true}}$.\n   All ratios must be reported as decimals (not percentages).\n\nImplementation details:\n- Randomness must be reproducible. Use a fixed pseudo-random seed as specified per test case.\n- All sampling within strata must be without replacement and uniformly over combinations.\n- All computations are dimensionless; no physical units are involved.\n\nTest Suite:\nProvide results for the following three test cases. For each case, use the stated replication count $R$ and seed.\n\n- Case $1$:\n  - $H = 3$,\n  - $(N_1,N_2,N_3) = (30,40,50)$,\n  - $(n_1,n_2,n_3) = (5,8,10)$,\n  - population generator: for each $h \\in \\{1,2,3\\}$ and $i \\in \\{1,\\dots,N_h\\}$,\n    $y_{h,i} = \\alpha_h + \\beta_h \\, i + \\gamma_h \\, (-1)^i$ with\n    $(\\alpha_1,\\alpha_2,\\alpha_3) = (10,20,30)$,\n    $(\\beta_1,\\beta_2,\\beta_3) = (0.5,1.0,1.5)$,\n    $(\\gamma_1,\\gamma_2,\\gamma_3) = (3,2,1)$.\n  - Replications $R = 20000$,\n  - Seed $= 202401$.\n\n- Case $2$ (edge case with a stratum of size $n_h=1$):\n  - $H = 3$,\n  - $(N_1,N_2,N_3) = (20,25,35)$,\n  - $(n_1,n_2,n_3) = (1,4,7)$,\n  - population generator: for each $h \\in \\{1,2,3\\}$ and $i \\in \\{1,\\dots,N_h\\}$,\n    $y_{h,i} = \\alpha_h + \\beta_h \\, \\sqrt{i} + \\gamma_h \\, \\left((i \\bmod 3) - 1\\right)$ with\n    $(\\alpha_1,\\alpha_2,\\alpha_3) = (5,8,12)$,\n    $(\\beta_1,\\beta_2,\\beta_3) = (2.0,1.0,0.5)$,\n    $(\\gamma_1,\\gamma_2,\\gamma_3) = (1,3,2)$.\n  - Replications $R = 20000$,\n  - Seed $= 202402$.\n\n- Case $3$ (boundary with full enumeration in one stratum to stress the finite population correction):\n  - $H = 2$,\n  - $(N_1,N_2) = (12,18)$,\n  - $(n_1,n_2) = (12,9)$,\n  - population generator: for each $h \\in \\{1,2\\}$ and $i \\in \\{1,\\dots,N_h\\}$,\n    $y_{h,i} = \\alpha_h + \\beta_h \\, i + \\gamma_h \\, \\frac{i^2}{N_h}$ with\n    $(\\alpha_1,\\alpha_2) = (0,4)$,\n    $(\\beta_1,\\beta_2) = (1.0,0.5)$,\n    $(\\gamma_1,\\gamma_2) = (0.2,0.1)$.\n  - Replications $R = 20000$,\n  - Seed $= 202403$.\n\nFinal output format:\nYour program should produce a single line of output containing a JSON-like representation of a list with three elements (one per test case), where each element is a list of the three floats in the order specified in item $4$ above, i.e.,\n$[ [V_{\\text{true}}, |V_{\\text{mc}}-V_{\\text{true}}|/V_{\\text{true}}, (\\overline{V}_{\\text{plug}}-V_{\\text{true}})/V_{\\text{true}}], \\dots ]$.\nFor example, a valid shape is $[[x_{11},x_{12},x_{13}],[x_{21},x_{22},x_{23}],[x_{31},x_{32},x_{33}]]$ with no additional text.", "solution": "The user-provided problem is assessed as valid. It is scientifically grounded in the principles of survey sampling and Monte Carlo simulation, well-posed with a clear objective and sufficient data, and free from any of the invalidating flaws listed in the problem validation protocol. We may therefore proceed with a full solution.\n\nThe problem is divided into two parts: Task A, a theoretical derivation of the conditional variance of the post-stratified estimator, and Task B, a numerical assessment of a plug-in variance estimator via stochastic simulation.\n\n### Task A: Derivation of the Conditional Variance\n\nWe are asked to derive the exact conditional variance $\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\})$. The post-stratified estimator for the population mean is given by:\n$$\n\\hat{\\mu}_{PS} = \\sum_{h=1}^H W_h \\bar{Y}_{h}\n$$\nwhere $W_h = N_h/N$ are the known population post-stratum weights and $\\bar{Y}_h$ is the sample mean of the $n_h$ units observed in stratum $h$.\n\nThe derivation proceeds conditionally on the realized stratum sample sizes $\\{n_h\\}_{h=1}^H$. This means we treat the $n_h$ values as fixed constants throughout the derivation. The randomness comes from the selection of specific units within each stratum, given the sample size for that stratum.\n\nThe variance of a sum of random variables involves covariance terms. Applying the variance operator to $\\hat{\\mu}_{PS}$:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\}) = \\operatorname{Var}\\left(\\sum_{h=1}^H W_h \\bar{Y}_h \\mid \\{n_h\\}\\right)\n$$\nUsing the general formula for the variance of a weighted sum, $\\operatorname{Var}(\\sum_{i} a_i X_i) = \\sum_{i} a_i^2 \\operatorname{Var}(X_i) + \\sum_{i \\neq j} a_i a_j \\operatorname{Cov}(X_i, X_j)$, we get:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\}) = \\sum_{h=1}^H W_h^2 \\operatorname{Var}(\\bar{Y}_h \\mid n_h) + \\sum_{h \\neq k} W_h W_k \\operatorname{Cov}(\\bar{Y}_h, \\bar{Y}_k \\mid \\{n_h\\})\n$$\nThe problem states that a single Simple Random Sample Without Replacement (SRSWOR) of size $n$ is drawn from the total population. However, for the purpose of analyzing the post-stratified estimator, the standard theoretical approach, and the one implied by conditioning on $\\{n_h\\}$, is to consider the sampling process as being conceptually equivalent to performing an independent SRSWOR of size $n_h$ from each stratum $h$. Under this conditional framework, the selection of units in one stratum is independent of the selection in another. Consequently, the sample means $\\bar{Y}_h$ and $\\bar{Y}_k$ for $h \\ne k$ are conditionally independent. This implies their conditional covariance is zero:\n$$\n\\operatorname{Cov}(\\bar{Y}_h, \\bar{Y}_k \\mid \\{n_h\\}) = 0 \\quad \\text{for } h \\neq k\n$$\nThe variance expression thus simplifies, as the sum of covariance terms vanishes:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\}) = \\sum_{h=1}^H W_h^2 \\operatorname{Var}(\\bar{Y}_h \\mid n_h)\n$$\nNow, we must find the expression for $\\operatorname{Var}(\\bar{Y}_h \\mid n_h)$. The problem instructs us to use the well-tested fact for the variance of a sample mean from an SRSWOR of size $n_h$ within a single finite population stratum of size $N_h$. This variance is given by:\n$$\n\\operatorname{Var}(\\bar{Y}_h \\mid n_h) = \\left(1 - \\frac{n_h}{N_h}\\right) \\frac{S_h^2}{n_h}\n$$\nwhere:\n-   $\\left(1 - \\frac{n_h}{N_h}\\right)$ is the Finite Population Correction (FPC), which accounts for the fact that sampling is without replacement from a finite population. As $n_h$ approaches $N_h$, the uncertainty about the mean decreases, vanishing completely when $n_h = N_h$.\n-   $S_h^2$ is the finite-population variance for the values $\\{y_{h,i}\\}_{i=1}^{N_h}$ in stratum $h$, defined with an $N_h-1$ denominator to be consistent with its role in sampling theory:\n    $$\n    S_h^2 = \\frac{1}{N_h-1}\\sum_{i=1}^{N_h}\\left(y_{h,i}-\\mu_h\\right)^2\n    $$\n    where $\\mu_h$ is the true mean of stratum $h$.\n\nSubstituting the expression for $\\operatorname{Var}(\\bar{Y}_h \\mid n_h)$ back into our summation, we arrive at the final derived expression for the exact conditional variance of the post-stratified estimator:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{PS}\\mid \\{n_h\\}) = \\sum_{h=1}^H W_h^2 \\left(1 - \\frac{n_h}{N_h}\\right) \\frac{S_h^2}{n_h}\n$$\nThis is the expression we will denote as $V_{\\text{true}}$. It is a function of the population stratum sizes $\\{N_h\\}$, stratum weights $\\{W_h\\}$, realized sample sizes $\\{n_h\\}$, and the (usually unknown) true within-stratum population variances $\\{S_h^2\\}$.\n\n### Task B: Simulation Design\n\nThe simulation assesses the properties of a \"plug-in\" variance estimator, $\\hat{V}$, which is formed by substituting an estimate for the unknown population variance $S_h^2$ into the formula for $V_{\\text{true}}$. The standard unbiased estimator for $S_h^2$ based on a sample of size $n_h$ is the sample variance $s_h^2$:\n$$\ns_h^2 = \\frac{1}{n_h-1}\\sum_{j=1}^{n_h}(y_{h,j}^{\\text{sample}} - \\bar{Y}_h)^2 \\quad \\text{for } n_h \\ge 2\n$$\nThe problem specifies that by convention, $s_h^2=0$ if $n_h=1$. The plug-in estimator for the variance is therefore:\n$$\n\\hat{V} = \\sum_{h=1}^H W_h^2 \\left(1 - \\frac{n_h}{N_h}\\right)\\frac{s_h^2}{n_h}\n$$\nThe simulation proceeds as follows for each test case:\n\n1.  **Population Construction**: An immutable, finite population is constructed for each stratum $h$ according to the deterministic function $y_{h,i} = f(h,i)$ provided in the test case. This allows for the exact computation of true population parameters.\n\n2.  **True Variance Calculation ($V_{\\text{true}}$)**: For each stratum $h$, the true finite-population variance $S_h^2$ is computed from the generated population values. These, along with the given $N_h$ and $n_h$, are used in the derived formula from Task A to calculate the exact conditional variance $V_{\\text{true}}$.\n\n3.  **Monte Carlo Simulation**: A large number of replicates, $R$, are generated to approximate the sampling distribution. For each replicate $r \\in \\{1,\\dots,R\\}$:\n    a.  A new sample of size $n_h$ is drawn via SRSWOR from the population of stratum $h$, for each $h \\in \\{1,\\dots,H\\}$.\n    b.  The sample mean $\\bar{Y}_h^{(r)}$ and sample variance $s_h^{2,(r)}$ are computed for each stratum from the drawn samples. Crucially, the rule $s_h^2=0$ is applied for any stratum where $n_h=1$.\n    c.  The post-stratified estimate $\\hat{\\mu}_{PS}^{(r)} = \\sum_{h} W_h \\bar{Y}_h^{(r)}$ and its corresponding plug-in variance estimate $\\hat{V}^{(r)} = \\sum_{h} W_h^2 (1 - n_h/N_h) s_h^{2,(r)}/n_h$ are computed and stored.\n\n4.  **Metric Computation**: After all $R$ replicates are completed:\n    a.  The Monte Carlo variance, $V_{\\text{mc}}$, is calculated as the sample variance (with denominator $R-1$) of the $R$ stored values of $\\hat{\\mu}_{PS}^{(r)}$. This serves as a simulation-based estimate of $V_{\\text{true}}$.\n    b.  The average plug-in variance, $\\overline{V}_{\\text{plug}}$, is calculated as the arithmetic mean of the $R$ stored values of $\\hat{V}^{(r)}$. This approximates the expected value of the plug-in estimator, $E[\\hat{V} \\mid \\{n_h\\}]$.\n    c.  The specified accuracy metrics are computed:\n        -   $V_{\\text{true}}$\n        -   $|V_{\\text{mc}} - V_{\\text{true}}|/V_{\\text{true}}$: This measures how well the Monte Carlo simulation itself converged to the true variance. With large $R$, this should be small.\n        -   $(\\overline{V}_{\\text{plug}} - V_{\\text{true}})/V_{\\text{true}}$: This measures the relative bias of the plug-in variance estimator. Theory predicts this estimator is unbiased ($E[\\hat{V}] = V_{\\text{true}}$) if and only if all $n_h \\ge 2$. If any $n_h=1$, the estimator is expected to be biased, as $E[s_h^2 | n_h=1] = 0 \\neq S_h^2$.\n\nThe implementation will use a fixed random seed for reproducibility and will execute these steps for each of the three test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the post-stratification problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"H\": 3,\n            \"N_h\": (30, 40, 50),\n            \"n_h\": (5, 8, 10),\n            \"pop_gen_params\": {\n                \"alpha\": (10, 20, 30),\n                \"beta\": (0.5, 1.0, 1.5),\n                \"gamma\": (3, 2, 1)\n            },\n            \"pop_gen_func\": lambda h, i, p: p[\"alpha\"][h-1] + p[\"beta\"][h-1] * i + p[\"gamma\"][h-1] * (-1)**i,\n            \"R\": 20000,\n            \"seed\": 202401\n        },\n        {\n            \"H\": 3,\n            \"N_h\": (20, 25, 35),\n            \"n_h\": (1, 4, 7),\n            \"pop_gen_params\": {\n                \"alpha\": (5, 8, 12),\n                \"beta\": (2.0, 1.0, 0.5),\n                \"gamma\": (1, 3, 2)\n            },\n            \"pop_gen_func\": lambda h, i, p: p[\"alpha\"][h-1] + p[\"beta\"][h-1] * np.sqrt(i) + p[\"gamma\"][h-1] * ((i % 3) - 1),\n            \"R\": 20000,\n            \"seed\": 202402\n        },\n        {\n            \"H\": 2,\n            \"N_h\": (12, 18),\n            \"n_h\": (12, 9),\n            \"pop_gen_params\": {\n                \"alpha\": (0, 4),\n                \"beta\": (1.0, 0.5),\n                \"gamma\": (0.2, 0.1)\n            },\n            \"pop_gen_func\": lambda h, i, p, N_h: p[\"alpha\"][h-1] + p[\"beta\"][h-1] * i + p[\"gamma\"][h-1] * i**2 / N_h,\n            \"R\": 20000,\n            \"seed\": 202403\n        }\n    ]\n\n    results_all_cases = []\n\n    for case_idx, case in enumerate(test_cases):\n        H = case[\"H\"]\n        N_h_tuple = case[\"N_h\"]\n        n_h_tuple = case[\"n_h\"]\n        pop_gen_params = case[\"pop_gen_params\"]\n        pop_gen_func = case[\"pop_gen_func\"]\n        R = case[\"R\"]\n        seed = case[\"seed\"]\n\n        rng = np.random.default_rng(seed)\n\n        # 1. Deterministic population construction\n        populations = []\n        for h_idx in range(H):\n            h = h_idx + 1\n            N_h_val = N_h_tuple[h_idx]\n            indices = np.arange(1, N_h_val + 1)\n            if case_idx == 2: # Case 3 has a different generator signature\n                y_values = pop_gen_func(h, indices, pop_gen_params, N_h_val)\n            else:\n                y_values = pop_gen_func(h, indices, pop_gen_params)\n            populations.append(y_values)\n        \n        # 2. Exact conditional variance\n        N_h_arr = np.array(N_h_tuple, dtype=float)\n        n_h_arr = np.array(n_h_tuple, dtype=float)\n        \n        N_total = np.sum(N_h_arr)\n        W_h_arr = N_h_arr / N_total\n        \n        # Calculate true population variances S_h^2\n        S_h_sq_arr = np.array([np.var(pop, ddof=1) if len(pop) > 1 else 0 for pop in populations])\n        \n        # Finite Population Correction\n        fpc_arr = 1.0 - n_h_arr / N_h_arr\n        \n        # Calculate V_true\n        # Add a small epsilon to denominator to avoid division by zero warning if n_h=0, though not in test cases\n        \n        # If n_h_arr[h] is zero, the term is undefined. Here n_h >= 1.\n        # If S_h_sq_arr[h] is zero, the term is zero.\n        # If fpc_arr[h] is zero (n_h=N_h), the term is zero.\n        # The logic below handles these cases correctly.\n        V_true_terms = np.zeros_like(W_h_arr, dtype=float)\n        valid_n_h_mask = n_h_arr > 0\n        V_true_terms[valid_n_h_mask] = (W_h_arr[valid_n_h_mask]**2 * fpc_arr[valid_n_h_mask] * S_h_sq_arr[valid_n_h_mask]) / n_h_arr[valid_n_h_mask]\n        V_true = np.sum(V_true_terms)\n\n        # 3. Monte Carlo approximation\n        mu_ps_reps = np.zeros(R)\n        V_hat_reps = np.zeros(R)\n\n        for r in range(R):\n            mu_ps_r = 0.0\n            V_hat_r = 0.0\n            for h_idx in range(H):\n                # Draw SRSWOR sample for stratum h\n                sample = rng.choice(populations[h_idx], size=int(n_h_arr[h_idx]), replace=False)\n                \n                # Compute sample mean\n                y_bar_h = np.mean(sample)\n                \n                # Compute sample variance s_h^2, with rule for n_h = 1\n                s_h_sq_r = 0.0\n                if n_h_arr[h_idx] > 1:\n                    s_h_sq_r = np.var(sample, ddof=1)\n                \n                # Accumulate for mu_ps and V_hat for this replicate\n                mu_ps_r += W_h_arr[h_idx] * y_bar_h\n                \n                # The contribution to variance is zero if n_h = 0. Problem has n_h >= 1.\n                if n_h_arr[h_idx] > 0:\n                    V_hat_r += (W_h_arr[h_idx]**2 * fpc_arr[h_idx] * s_h_sq_r) / n_h_arr[h_idx]\n            \n            mu_ps_reps[r] = mu_ps_r\n            V_hat_reps[r] = V_hat_r\n\n        # Calculate Monte Carlo variance and average plug-in variance\n        V_mc = np.var(mu_ps_reps, ddof=1)\n        V_plug_bar = np.mean(V_hat_reps)\n\n        # 4. Accuracy metrics\n        # Avoid division by zero if V_true is zero\n        if V_true == 0:\n            rel_err_V_mc = 0.0 if V_mc == 0.0 else np.inf\n            rel_bias_V_plug = 0.0 if V_plug_bar == 0.0 else np.inf\n        else:\n            rel_err_V_mc = np.abs(V_mc - V_true) / V_true\n            rel_bias_V_plug = (V_plug_bar - V_true) / V_true\n\n        results_all_cases.append([V_true, rel_err_V_mc, rel_bias_V_plug])\n    \n    # Final print statement in the exact required format.\n    print(f\"{results_all_cases}\")\n\nsolve()\n```", "id": "3330424"}, {"introduction": "While analytical formulas for variance are insightful, they can become intractable for estimators derived from complex survey designs. This practice introduces replication methods, such as the jackknife and bootstrap, as a powerful and widely used alternative for variance estimation. You will gain hands-on experience implementing these techniques, focusing on the critical step of adjusting replicate weights to preserve the post-stratification calibration constraints, a vital skill for practical survey analysis [@problem_id:3330435].", "problem": "Consider a finite population partitioned into $K$ post-strata indexed by $k \\in \\{1,\\dots,K\\}$, with known post-stratum population sizes $N_k$ and a sample $s = \\bigcup_{k=1}^K s_k$ consisting of $n_k$ sampled units in each post-stratum $k$. Denote the sampled units in post-stratum $k$ by indices $i \\in s_k$. Let the initial sampling weights $w_i$ be nonnegative and satisfy the post-stratification calibration constraints $\\sum_{i \\in s_k} w_i = N_k$ for each post-stratum $k$. Let $y_i$ be a scalar survey variable of interest for unit $i$. Define the post-stratified weighted mean estimator as\n$$\n\\hat{\\mu} \\equiv \\frac{\\sum_{k=1}^K \\sum_{i \\in s_k} w_i \\, y_i}{\\sum_{k=1}^K N_k}.\n$$\nYour task is to construct replicate weights for two resampling schemes—delete-one jackknife and within-post-stratum bootstrap—that preserve the post-stratification constraints for every replicate, and then use these replicates to estimate the variance of the post-stratified weighted mean.\n\nFundamental base and definitions to use:\n- The Horvitz–Thompson total estimator for a sample is $ \\hat{T} = \\sum_{i \\in s} w_i y_i $, and the post-stratified weighted mean is $ \\hat{\\mu} = \\hat{T} / N $, where $ N = \\sum_{k=1}^K N_k $ is known.\n- Post-stratification calibration requires, for each post-stratum $k$, the constraint $ \\sum_{i \\in s_k} w_i = N_k $.\n- In delete-one jackknife, a replicate is formed by dropping one unit and adjusting the remaining units so that the constraints remain satisfied.\n- In the bootstrap, replicates are formed by resampling with replacement within each post-stratum and then calibrating the temporary replicate weights to satisfy the constraints.\n\nYou must implement the following replicate constructions:\n\n1. Delete-one jackknife replicate weights:\n   - For a unit $d \\in s_{k^\\ast}$ in post-stratum $k^\\ast$, define a replicate by setting $w_d^{(r)} = 0$ and scaling the remaining weights in the same post-stratum by a factor $ \\alpha_{k^\\ast}^{(r)} $ so that $ \\sum_{i \\in s_{k^\\ast}} w_i^{(r)} = N_{k^\\ast} $. For all other post-strata $k \\neq k^\\ast$, set $w_i^{(r)} = w_i$.\n   - You must handle the boundary case when $n_{k^\\ast} = 1$. In this case, deleting the only unit makes calibration undefined; therefore, skip creating jackknife replicates for such post-strata.\n   - For each jackknife replicate $r$, compute the post-stratified weighted mean $ \\hat{\\mu}^{(r)} $.\n   - Estimate the jackknife variance using the delete-one jackknife formula for a smooth statistic:\n     $$\n     \\widehat{\\mathrm{Var}}_{\\mathrm{JK}}(\\hat{\\mu}) = \\frac{m-1}{m} \\sum_{r=1}^{m} \\left(\\hat{\\mu}^{(r)} - \\hat{\\mu}\\right)^2,\n     $$\n     where $m$ is the total number of jackknife replicates actually constructed (i.e., summing only over post-strata with $n_k \\geq 2$).\n\n2. Bootstrap replicate weights within post-strata:\n   - For each replicate $b \\in \\{1,\\dots,B\\}$ and each post-stratum $k$, draw counts $c_i^{(b)}$ for $i \\in s_k$ from a multinomial distribution with $n_k$ trials and equal probabilities $1/n_k$ for each sampled unit in $s_k$. Use the same independent multinomial construction within each post-stratum.\n   - Form temporary weights $\\tilde{w}_i^{(b)} = w_i \\, c_i^{(b)}$, and then for each post-stratum compute a calibration factor $ \\alpha_k^{(b)} $ so that $ \\sum_{i \\in s_k} \\alpha_k^{(b)} \\tilde{w}_i^{(b)} = N_k $, and set $ w_i^{(b)} = \\alpha_k^{(b)} \\tilde{w}_i^{(b)} $.\n   - For each bootstrap replicate $b$, compute $ \\hat{\\mu}^{(b)} $.\n   - Estimate the bootstrap variance as the unbiased sample variance of the replicate estimates:\n     $$\n     \\widehat{\\mathrm{Var}}_{\\mathrm{BS}}(\\hat{\\mu}) = \\frac{1}{B-1} \\sum_{b=1}^{B} \\left(\\hat{\\mu}^{(b)} - \\bar{\\mu}_{\\mathrm{BS}}\\right)^2,\n     $$\n     where $ \\bar{\\mu}_{\\mathrm{BS}} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\mu}^{(b)} $.\n\nVerification of constraints:\n- For every jackknife replicate constructed and for every post-stratum $k$, verify that $ \\sum_{i \\in s_k} w_i^{(r)} = N_k $.\n- For every bootstrap replicate $b$ and for every post-stratum $k$, verify that $ \\sum_{i \\in s_k} w_i^{(b)} = N_k $.\n\nImplement a complete program that constructs replicate weights, computes replicate estimates, verifies the post-stratification constraints, and outputs the jackknife and bootstrap variance estimates of the weighted mean together with boolean flags indicating whether the constraints were satisfied in all replicates, for the following test suite. All arrays are specified per post-stratum, in order $k = 1, 2, \\dots, K$.\n\nTest suite:\n- Case $1$ (general case, equal weights within post-strata):\n  - $K = 3$, $N_k = \\{\\,1200, 800, 1000\\,\\}$.\n  - Sample sizes: $n_k = \\{\\,4, 3, 5\\,\\}$.\n  - Weights per post-stratum: $w_i$ in post-stratum $1$ are $\\{\\,300, 300, 300, 300\\,\\}$; in post-stratum $2$ are $\\{\\,266.\\overline{6}, 266.\\overline{6}, 266.\\overline{6}\\,\\}$; in post-stratum $3$ are $\\{\\,200, 200, 200, 200, 200\\,\\}$.\n  - Values $y_i$: post-stratum $1$ has $\\{\\,10.0, 12.5, 9.3, 11.1\\,\\}$; post-stratum $2$ has $\\{\\,7.0, 8.2, 6.5\\,\\}$; post-stratum $3$ has $\\{\\,15.0, 14.2, 16.8, 13.9, 15.5\\,\\}$.\n- Case $2$ (boundary case including a post-stratum with $n_k = 1$ and unequal weights):\n  - $K = 3$, $N_k = \\{\\,500, 300, 200\\,\\}$.\n  - Sample sizes: $n_k = \\{\\,1, 3, 2\\,\\}$.\n  - Weights per post-stratum: post-stratum $1$ has $\\{\\,500\\,\\}$; post-stratum $2$ has $\\{\\,120, 90, 90\\,\\}$; post-stratum $3$ has $\\{\\,120, 80\\,\\}$.\n  - Values $y_i$: post-stratum $1$ has $\\{\\,20.0\\,\\}$; post-stratum $2$ has $\\{\\,5.5, 4.0, 6.5\\,\\}$; post-stratum $3$ has $\\{\\,8.0, 7.5\\,\\}$.\n- Case $3$ (edge case with highly unequal weights and heavy-tailed $y_i$):\n  - $K = 2$, $N_k = \\{\\,10000, 5000\\,\\}$.\n  - Sample sizes: $n_k = \\{\\,5, 4\\,\\}$.\n  - Weights per post-stratum: post-stratum $1$ has $\\{\\,7000, 500, 500, 1000, 1000\\,\\}$; post-stratum $2$ has $\\{\\,1000, 1200, 1800, 1000\\,\\}$.\n  - Values $y_i$: post-stratum $1$ has $\\{\\,1000, 50, 60, 55, 65\\,\\}$; post-stratum $2$ has $\\{\\,70, 80, 75, 85\\,\\}$.\n\nUse $B = 500$ bootstrap replicates and a fixed pseudo-random number generator seed of $271828$ for reproducibility. All computations must be in real numbers. The program must verify the calibration constraints for every replicate with a numerical tolerance of $10^{-12}$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of lists, one per case, where each inner list is\n  $[\\,\\widehat{\\mathrm{Var}}_{\\mathrm{JK}}(\\hat{\\mu}), \\widehat{\\mathrm{Var}}_{\\mathrm{BS}}(\\hat{\\mu}), \\text{constraints\\_ok\\_JK}, \\text{constraints\\_ok\\_BS}\\,]$,\n  with the variance entries as floating-point numbers and the constraint indicators as booleans. The entire output must be a single line in the exact format, for example, $[\\,[0.0,0.0,\\text{True},\\text{True}]\\,]$, but populated with the results for the three specified cases.", "solution": "The problem requires the implementation and comparison of two resampling-based variance estimation methods for a post-stratified weighted mean estimator: the delete-one jackknife and the within-post-stratum bootstrap. A critical component of both methods is the preservation of the post-stratification calibration constraints for each replicate.\n\nFirst, we establish the initial setup. The population is partitioned into $K$ post-strata, with known population counts $N_k$ for each post-stratum $k \\in \\{1, \\dots, K\\}$. The total population size is $N = \\sum_{k=1}^K N_k$. We are given a sample $s$ comprised of $n_k$ units from each post-stratum $k$, denoted $s_k$. For each sampled unit $i \\in s$, we have a survey variable $y_i$ and an initial non-negative weight $w_i$. These initial weights are calibrated such that for each post-stratum $k$, the constraint $\\sum_{i \\in s_k} w_i = N_k$ is satisfied.\n\nThe post-stratified weighted mean estimator, $\\hat{\\mu}$, is defined as:\n$$\n\\hat{\\mu} = \\frac{\\sum_{k=1}^K \\sum_{i \\in s_k} w_i y_i}{N}\n$$\nOur objective is to estimate the variance of this estimator, $\\widehat{\\mathrm{Var}}(\\hat{\\mu})$, using the two specified resampling schemes.\n\n**1. Delete-One Jackknife Variance Estimation**\n\nThe delete-one jackknife method involves creating a series of \"replicates\" by systematically removing one sample unit at a time and adjusting the weights of the remaining units to maintain the calibration constraints.\n\nLet a replicate be indexed by $r$, corresponding to the deletion of unit $d$ from its post-stratum $k^\\ast$. The total number of valid replicates, $m$, is the sum of sample sizes over all post-strata with at least two units: $m = \\sum_{k: n_k \\ge 2} n_k$. The case where $n_k = 1$ is explicitly excluded, as deleting the sole unit would make the re-calibration of that stratum impossible.\n\nFor a replicate $r$ formed by deleting unit $d \\in s_{k^\\ast}$ (where $n_{k^\\ast} \\ge 2$), the replicate weights $w_i^{(r)}$ are defined as follows:\n- For the deleted unit $d$, its weight is set to zero: $w_d^{(r)} = 0$.\n- For any other unit $i$ in a different post-stratum ($k \\neq k^\\ast$), the weight is unchanged: $w_i^{(r)} = w_i$ for $i \\in s_k$.\n- For the remaining units $i$ within the same post-stratum $k^\\ast$ ($i \\in s_{k^\\ast}, i \\neq d$), their original weights are uniformly scaled by a factor $\\alpha_{k^\\ast}^{(r)}$: $w_i^{(r)} = \\alpha_{k^\\ast}^{(r)} w_i$.\n\nThe scaling factor $\\alpha_{k^\\ast}^{(r)}$ is derived by enforcing the calibration constraint for post-stratum $k^\\ast$:\n$$\n\\sum_{i \\in s_{k^\\ast}} w_i^{(r)} = N_{k^\\ast}\n$$\nSubstituting the definitions for $w_i^{(r)}$:\n$$\nw_d^{(r)} + \\sum_{i \\in s_{k^\\ast}, i \\neq d} w_i^{(r)} = 0 + \\sum_{i \\in s_{k^\\ast}, i \\neq d} \\alpha_{k^\\ast}^{(r)} w_i = \\alpha_{k^\\ast}^{(r)} \\sum_{i \\in s_{k^\\ast}, i \\neq d} w_i = N_{k^\\ast}\n$$\nThe sum of the original weights of the remaining units is $(\\sum_{i \\in s_{k^\\ast}} w_i) - w_d$. Since the original weights are calibrated, this sum is $N_{k^\\ast} - w_d$. Therefore:\n$$\n\\alpha_{k^\\ast}^{(r)} (N_{k^\\ast} - w_d) = N_{k^\\ast} \\implies \\alpha_{k^\\ast}^{(r)} = \\frac{N_{k^\\ast}}{N_{k^\\ast} - w_d}\n$$\nThis factor is well-defined provided $N_{k^\\ast} - w_d \\neq 0$, which is guaranteed if $n_{k^\\ast} \\ge 2$ and the weights are not pathological (e.g., one unit having a weight of $N_{k^\\ast}$ while others have zero weight, which would contradict $n_{k^\\ast} \\ge 2$ if we assume weights are positive).\n\nFor each of the $m$ replicates, we compute the replicate mean estimate:\n$$\n\\hat{\\mu}^{(r)} = \\frac{\\sum_{k=1}^K \\sum_{i \\in s_k} w_i^{(r)} y_i}{N}\n$$\nThe jackknife variance of $\\hat{\\mu}$ is then estimated as:\n$$\n\\widehat{\\mathrm{Var}}_{\\mathrm{JK}}(\\hat{\\mu}) = \\frac{m-1}{m} \\sum_{r=1}^{m} \\left(\\hat{\\mu}^{(r)} - \\hat{\\mu}\\right)^2\n$$\n\n**2. Within-Post-Stratum Bootstrap Variance Estimation**\n\nThe bootstrap method involves generating replicates by resampling units with replacement from the original sample. For post-stratified data, this resampling is performed independently within each post-stratum.\n\nFor each of $B$ bootstrap replicates, indexed by $b \\in \\{1, \\dots, B\\}$, and for each post-stratum $k$, we perform the following steps:\n1.  Draw a set of integer counts $\\{c_i^{(b)}\\}_{i \\in s_k}$ from a multinomial distribution with $n_k$ trials and $n_k$ categories, each with equal probability $1/n_k$. The count $c_i^{(b)}$ represents how many times unit $i$ is selected in the bootstrap resample for stratum $k$. Note that $\\sum_{i \\in s_k} c_i^{(b)} = n_k$.\n2.  Form temporary replicate weights by multiplying the original weights by these counts: $\\tilde{w}_i^{(b)} = w_i c_i^{(b)}$ for each $i \\in s_k$.\n3.  Calibrate these temporary weights to meet the post-stratification constraint. For each post-stratum $k$, a scaling factor $\\alpha_k^{(b)}$ is computed:\n    $$\n    \\alpha_k^{(b)} = \\frac{N_k}{\\sum_{i \\in s_k} \\tilde{w}_i^{(b)}} = \\frac{N_k}{\\sum_{i \\in s_k} w_i c_i^{(b)}}\n    $$\n    This is well-defined as long as the denominator is non-zero. This condition holds if at least one unit $i$ with $w_i > 0$ is selected in the bootstrap resample (i.e., $c_i^{(b)} > 0$). Given the sample sizes and positive weights in the test cases, the probability of the denominator being zero is negligible.\n4.  The final bootstrap replicate weights are $w_i^{(b)} = \\alpha_k^{(b)} \\tilde{w}_i^{(b)}$ for all units $i \\in s_k$. This process is repeated for all post-strata, yielding a full set of replicate weights $\\{w_i^{(b)}\\}_{i \\in s}$.\n\nFor each of the $B$ replicates, we compute the replicate mean estimate:\n$$\n\\hat{\\mu}^{(b)} = \\frac{\\sum_{k=1}^K \\sum_{i \\in s_k} w_i^{(b)} y_i}{N}\n$$\nThe bootstrap variance is the sample variance of these replicate estimates. First, we find the mean of the replicate estimates, $\\bar{\\mu}_{\\mathrm{BS}} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\mu}^{(b)}$. The variance is then:\n$$\n\\widehat{\\mathrm{Var}}_{\\mathrm{BS}}(\\hat{\\mu}) = \\frac{1}{B-1} \\sum_{b=1}^{B} \\left(\\hat{\\mu}^{(b)} - \\bar{\\mu}_{\\mathrm{BS}}\\right)^2\n$$\n\n**3. Implementation and Verification**\n\nThe implementation will process each test case by first calculating the initial estimate $\\hat{\\mu}$. It will then execute the jackknife procedure, generating each replicate, verifying that the calibration constraints $\\sum_{i \\in s_k} w_i^{(r)} = N_k$ hold for all $k$ within a tolerance of $10^{-12}$, and storing the replicate estimates $\\hat{\\mu}^{(r)}$. Subsequently, it will compute $\\widehat{\\mathrm{Var}}_{\\mathrm{JK}}(\\hat{\\mu})$.\n\nNext, it will execute the bootstrap procedure for $B=500$ replicates, using a fixed random number generator seed of $271828$ for reproducibility. For each replicate, it will generate the weights $w_i^{(b)}$ and verify the calibration constraints $\\sum_{i \\in s_k} w_i^{(b)} = N_k$ for all $k$. It will store the replicate estimates $\\hat{\\mu}^{(b)}$ and then compute $\\widehat{\\mathrm{Var}}_{\\mathrm{BS}}(\\hat{\\mu})$. Boolean flags will track whether all constraints were met for each method. The final output will be a list containing the results for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    \n    # Global parameters from the problem statement.\n    B = 500\n    SEED = 271828\n    TOL = 1e-12\n\n    def process_case(Nk_list, nk_list, weights_per_stratum_list, y_per_stratum_list):\n        \"\"\"\n        Processes a single test case to compute jackknife and bootstrap variances.\n        \"\"\"\n        # --- Data Preprocessing ---\n        Nk = np.array(Nk_list, dtype=float)\n        nk = np.array(nk_list, dtype=int)\n        \n        K = len(Nk)\n        N_total = np.sum(Nk)\n        \n        # Flatten data into single arrays and create an index mapping each unit to its stratum.\n        w0 = np.concatenate([np.array(w, dtype=float) for w in weights_per_stratum_list])\n        y = np.concatenate([np.array(val, dtype=float) for val in y_per_stratum_list])\n        \n        stratum_indices = np.repeat(np.arange(K), nk)\n\n        # --- Initial Estimate Calculation ---\n        T_hat = np.sum(w0 * y)\n        mu_hat = T_hat / N_total\n\n        # --- Method 1: Delete-One Jackknife ---\n        constraints_ok_JK = True\n        mu_hat_jk_reps = []\n        \n        num_total_units = len(w0)\n        replicate_count_jk = 0\n\n        for i in range(num_total_units):\n            k_star = stratum_indices[i]  # Stratum of the unit to be dropped.\n            \n            if nk[k_star] < 2:\n                continue  # Skip strata with n_k < 2 as per instruction.\n            \n            replicate_count_jk += 1\n            \n            # Create replicate weights.\n            w_rep = w0.copy()\n            \n            # Calculate the adjustment factor for the stratum of the dropped unit.\n            w_d = w0[i]\n            sum_w_minus_d = Nk[k_star] - w_d\n            \n            # This should always be safe due to the nk[k_star] >= 2 check.\n            alpha = Nk[k_star] / sum_w_minus_d if sum_w_minus_d != 0 else 0\n            \n            # Apply the adjustment.\n            stratum_mask = (stratum_indices == k_star)\n            w_rep[stratum_mask] *= alpha\n            w_rep[i] = 0.0  # Set weight of dropped unit to zero.\n            \n            # Verification of constraints for the current replicate.\n            for k in range(K):\n                sum_w_k = np.sum(w_rep[stratum_indices == k])\n                if not np.isclose(sum_w_k, Nk[k], atol=TOL, rtol=0):\n                    constraints_ok_JK = False\n            \n            # Calculate replicate estimate.\n            mu_hat_rep = np.sum(w_rep * y) / N_total\n            mu_hat_jk_reps.append(mu_hat_rep)\n        \n        # Calculate Jackknife variance.\n        var_jk = 0.0\n        if replicate_count_jk > 1:\n            m = replicate_count_jk\n            mu_hat_jk_reps = np.array(mu_hat_jk_reps)\n            var_jk = ((m - 1.0) / m) * np.sum((mu_hat_jk_reps - mu_hat)**2)\n        \n        # --- Method 2: Within-Post-Stratum Bootstrap ---\n        constraints_ok_BS = True\n        mu_hat_bs_reps = []\n        rng = np.random.default_rng(SEED)\n        \n        for _ in range(B):\n            w_rep = np.zeros_like(w0)\n            \n            for k in range(K):\n                stratum_mask = (stratum_indices == k)\n                if nk[k] == 0:\n                    continue\n\n                w0_k = w0[stratum_mask]\n                \n                # Draw multinomial counts for resampling with replacement.\n                probs = np.full(nk[k], 1.0 / nk[k])\n                counts = rng.multinomial(nk[k], probs, size=1)[0]\n                \n                # Calculate temporary weights and their sum.\n                w_tilde_k = w0_k * counts\n                sum_w_tilde_k = np.sum(w_tilde_k)\n                \n                # Calculate calibration factor.\n                alpha_k = Nk[k] / sum_w_tilde_k if sum_w_tilde_k > 0 else 0.0\n                \n                # Final replicate weights for the stratum.\n                w_rep_k = w_tilde_k * alpha_k\n                w_rep[stratum_mask] = w_rep_k\n                \n                # Verification for this stratum in this replicate.\n                if not np.isclose(np.sum(w_rep_k), Nk[k], atol=TOL, rtol=0):\n                    constraints_ok_BS = False\n            \n            # Calculate total replicate estimate.\n            mu_hat_rep = np.sum(w_rep * y) / N_total\n            mu_hat_bs_reps.append(mu_hat_rep)\n        \n        # Calculate Bootstrap variance (using sample variance, ddof=1).\n        mu_hat_bs_reps = np.array(mu_hat_bs_reps)\n        var_bs = np.var(mu_hat_bs_reps, ddof=1)\n        \n        return [var_jk, var_bs, constraints_ok_JK, constraints_ok_BS]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"Nk\": [1200, 800, 1000], \"nk\": [4, 3, 5],\n            \"weights\": [[300.0] * 4, [800.0 / 3.0] * 3, [200.0] * 5],\n            \"y\": [[10.0, 12.5, 9.3, 11.1], [7.0, 8.2, 6.5], [15.0, 14.2, 16.8, 13.9, 15.5]]\n        },\n        {\n            \"Nk\": [500, 300, 200], \"nk\": [1, 3, 2],\n            \"weights\": [[500.0], [120.0, 90.0, 90.0], [120.0, 80.0]],\n            \"y\": [[20.0], [5.5, 4.0, 6.5], [8.0, 7.5]]\n        },\n        {\n            \"Nk\": [10000, 5000], \"nk\": [5, 4],\n            \"weights\": [[7000.0, 500.0, 500.0, 1000.0, 1000.0], [1000.0, 1200.0, 1800.0, 1000.0]],\n            \"y\": [[1000.0, 50.0, 60.0, 55.0, 65.0], [70.0, 80.0, 75.0, 85.0]]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        res = process_case(case[\"Nk\"], case[\"nk\"], case[\"weights\"], case[\"y\"])\n        results.append(res)\n\n    # Format the final output string exactly as required.\n    # Python's str(True) -> 'True', which matches the desired format.\n    result_strings = [f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3330435"}, {"introduction": "A point estimate and its variance do not tell the whole story; a responsible analyst must also diagnose the estimator's stability. This exercise equips you with essential diagnostic tools to assess the 'health' of a post-stratified estimator, guarding against misleading precision claims. By deriving and calculating measures of influence and leverage, you will learn to identify when an estimate might be unstable due to sparse cells or extreme weights, a crucial step toward producing robust and trustworthy results [@problem_id:3330477].", "problem": "Consider a finite population partitioned into $H$ post-strata (cells). Let the true population composition across cells be given by the vector of cell proportions $\\{p_h\\}_{h=1}^H$ satisfying $\\sum_{h=1}^H p_h = 1$ and $p_h \\in (0,1)$. From a sample of size $n$, suppose that the realized sample counts per cell are $\\{n_h\\}_{h=1}^H$, with $\\sum_{h=1}^H n_h = n$ and $n_h \\in \\mathbb{N}$, $n_h \\geq 1$. Assume that within each cell $h$, the sample provides a cell mean $\\bar{y}_h$ and a within-cell sample variance estimate $\\hat{\\sigma}_h^2$ for the outcome of interest. Consider the standard post-stratified estimator of the population mean that aggregates cell means weighted by the population composition.\n\nYou will construct diagnostics that quantify how many observational units and how many cells effectively contribute to the estimator, and that flag extreme weights using leverage or influence measures indicative of potential variance inflation. These diagnostics must be derived from first principles starting from the following fundamental bases:\n\n- The Horvitz–Thompson characterization of weighted mean estimation: the estimator of the mean is expressed as a ratio of weighted sums, with weights calibrated so that the weighted sample reproduces the known population composition.\n- First-order Taylor linearization of the variance of a smooth estimator: the asymptotic variance of the post-stratified mean can be expressed as a sum of cell-specific contributions depending on $p_h$, $n_h$, and $\\hat{\\sigma}_h^2$ under independence across cells and within-cell sampling assumptions.\n\nDefine the following quantities solely from the given inputs, without introducing any external parameters:\n\n1. The observation-level weights implied by post-stratification and the associated normalized influence for each observation as the sensitivity of the estimator to a perturbation in that observation’s value, scaled to sum to $1$ across all observations. Use this to quantify the effective number of observational units contributing to the estimator.\n\n2. The cell-level variance contributions from a first-order linearization of the post-stratified mean’s variance and the associated normalized variance-leverage for each cell. Use this to quantify the effective number of variance-contributing cells.\n\n3. An influence-based diagnostic to flag extreme observations: classify an observation as extreme if its normalized influence exceeds $2/n$.\n\n4. A leverage-based diagnostic to flag extreme cells with respect to variance: classify a cell as extreme if its normalized variance-leverage exceeds $2/H$.\n\nIf the total linearized variance aggregation is zero, define the variance-leverage vector as all zeros and the effective number of variance-contributing cells as $H$ by convention.\n\nYour task is to implement a program that, for each test case, computes a list containing the following six quantities in this exact order:\n\n- The effective number of observational units, expressed as a real number.\n- The effective number of variance-contributing cells, expressed as a real number.\n- The count of extreme observations (an integer) under the observation influence rule described above.\n- The count of extreme cells (an integer) under the variance-leverage rule described above.\n- The maximum observation-level normalized influence value across all observations, expressed as a real number.\n- The maximum cell-level normalized variance-leverage value across all cells, expressed as a real number.\n\nYour program must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case’s six-tuple appears as a bracketed, comma-separated list with no spaces. For example, the output format must be exactly of the form $[ [a_1,a_2,a_3,a_4,a_5,a_6],[b_1,b_2,b_3,b_4,b_5,b_6],\\ldots ]$ but without any spaces anywhere.\n\nUse the following test suite of parameter values, which covers a general case, a case with a highly sparse cell that should induce extreme observation weights, and a boundary case with a zero-variance cell:\n\n- Test Case 1:\n  - $H = 5$\n  - $p = [0.15, 0.25, 0.10, 0.30, 0.20]$\n  - $n_h = [60, 80, 40, 70, 50]$\n  - $n = 300$\n  - $\\hat{\\sigma}^2 = [1.0, 0.8, 1.2, 0.9, 1.1]$\n\n- Test Case 2:\n  - $H = 4$\n  - $p = [0.40, 0.30, 0.20, 0.10]$\n  - $n_h = [10, 90, 80, 70]$\n  - $n = 250$\n  - $\\hat{\\sigma}^2 = [1.0, 0.8, 0.7, 0.6]$\n\n- Test Case 3:\n  - $H = 3$\n  - $p = [0.50, 0.30, 0.20]$\n  - $n_h = [150, 1, 49]$\n  - $n = 200$\n  - $\\hat{\\sigma}^2 = [1.0, 0.0, 1.5]$\n\nYour program must not read any input; it must embed the above test suite and print the single-line output in the exact required format. All answers are unitless real numbers or integers. The implementation must be consistent with the derivations from the fundamental bases described above and must rely exclusively on the given parameters $H$, $p$, $n_h$, $n$, and $\\hat{\\sigma}_h^2$.", "solution": "The problem asks for the derivation and computation of several diagnostic quantities for a post-stratified estimator of a population mean. The derivations will be based on first principles as specified. Let the number of post-strata be $H$, the population cell proportions be $\\{p_h\\}_{h=1}^H$, the realized sample cell counts be $\\{n_h\\}_{h=1}^H$, the total sample size be $n = \\sum_{h=1}^H n_h$, and the within-cell sample variance estimates be $\\{\\hat{\\sigma}_h^2\\}_{h=1}^H$.\n\nThe post-stratified estimator of the population mean, $\\mu$, is given by:\n$$ \\hat{\\mu}_{ps} = \\sum_{h=1}^H p_h \\bar{y}_h $$\nwhere $\\bar{y}_h$ is the sample mean of the outcome variable in cell $h$. Let the individual observations in the sample be indexed by $i=1, \\dots, n$, and let $c(i)$ denote the cell to which observation $i$ belongs. The cell mean is $\\bar{y}_h = \\frac{1}{n_h} \\sum_{i: c(i)=h} y_i$. Substituting this into the estimator gives:\n$$ \\hat{\\mu}_{ps} = \\sum_{h=1}^H p_h \\left( \\frac{1}{n_h} \\sum_{i: c(i)=h} y_i \\right) = \\sum_{h=1}^H \\sum_{i: c(i)=h} \\frac{p_h}{n_h} y_i $$\nThis reveals that the estimator is a weighted sum of the individual observations, $\\hat{\\mu}_{ps} = \\sum_{i=1}^n w_i y_i$, where the weight $w_i$ for an observation $i$ in cell $c(i)$ is given by:\n$$ w_i = w_{c(i)} = \\frac{p_{c(i)}}{n_{c(i)}} $$\nIt is straightforward to verify that these weights are calibrated to sum to $1$:\n$$ \\sum_{i=1}^n w_i = \\sum_{h=1}^H \\sum_{i: c(i)=h} w_i = \\sum_{h=1}^H n_h \\left( \\frac{p_h}{n_h} \\right) = \\sum_{h=1}^H p_h = 1 $$\n\nWith these weights established, we can derive the required diagnostic quantities.\n\n**1. Observation-Level Diagnostics and Effective Number of Units**\n\nThe problem defines the normalized influence of an observation as the sensitivity of the estimator to that observation's value, scaled to sum to $1$. The sensitivity of $\\hat{\\mu}_{ps}$ to a change in $y_i$ is its partial derivative:\n$$ \\frac{\\partial \\hat{\\mu}_{ps}}{\\partial y_i} = \\frac{\\partial}{\\partial y_i} \\sum_{j=1}^n w_j y_j = w_i $$\nSince the weights $w_i$ already sum to $1$, they are themselves the normalized influence values, which we denote as $\\text{infl}_i = w_i$.\n\nThe effective number of observational units, $n_{eff, obs}$, can be quantified using Kish's effective sample size formula for a set of weights that sum to $1$:\n$$ n_{eff, obs} = \\frac{\\left(\\sum_{i=1}^n w_i\\right)^2}{\\sum_{i=1}^n w_i^2} = \\frac{1}{\\sum_{i=1}^n w_i^2} $$\nThe sum of squared weights can be calculated by grouping observations by cell:\n$$ \\sum_{i=1}^n w_i^2 = \\sum_{h=1}^H \\sum_{i: c(i)=h} \\left(\\frac{p_h}{n_h}\\right)^2 = \\sum_{h=1}^H n_h \\frac{p_h^2}{n_h^2} = \\sum_{h=1}^H \\frac{p_h^2}{n_h} $$\nTherefore, the effective number of observational units is:\n$$ n_{eff, obs} = \\frac{1}{\\sum_{h=1}^H p_h^2 / n_h} $$\nAn observation is flagged as extreme if its influence exceeds $2/n$. For an observation in cell $h$, this condition is $\\text{infl}_i = p_h/n_h > 2/n$. If this holds, all $n_h$ observations in that cell are classified as extreme. The total count of extreme observations is the sum of $n_h$ over all cells $h$ that satisfy this criterion. The maximum observation-level influence is simply $\\max_{h} (p_h/n_h)$.\n\n**2. Cell-Level Diagnostics and Effective Number of Cells**\n\nThe variance of the post-stratified estimator, conditional on the realized sample sizes $n_h$ and assuming independence of sampling across cells, is:\n$$ \\text{Var}(\\hat{\\mu}_{ps}) = \\text{Var}\\left(\\sum_{h=1}^H p_h \\bar{y}_h\\right) = \\sum_{h=1}^H p_h^2 \\text{Var}(\\bar{y}_h) $$\nUsing the provided sample variance estimates $\\hat{\\sigma}_h^2$ for the population variance $\\sigma_h^2$ within each cell, and the formula $\\text{Var}(\\bar{y}_h) \\approx \\hat{\\sigma}_h^2/n_h$, the estimated variance of $\\hat{\\mu}_{ps}$ is:\n$$ \\hat{V}(\\hat{\\mu}_{ps}) = \\sum_{h=1}^H \\frac{p_h^2 \\hat{\\sigma}_h^2}{n_h} $$\nThis expression represents the total variance as a sum of cell-specific contributions, $V_h = p_h^2 \\hat{\\sigma}_h^2 / n_h$. The \"normalized variance-leverage\" for cell $h$, denoted $\\lambda_h$, is its fractional contribution to the total variance:\n$$ \\lambda_h = \\frac{V_h}{\\sum_{j=1}^H V_j} = \\frac{p_h^2 \\hat{\\sigma}_h^2 / n_h}{\\sum_{j=1}^H p_j^2 \\hat{\\sigma}_j^2 / n_j} $$\nThese leverages sum to $1$ (provided the total variance is non-zero). The effective number of variance-contributing cells, $n_{eff, cell}$, is calculated using the same Kish formula applied to the cell-level leverages:\n$$ n_{eff, cell} = \\frac{1}{\\sum_{h=1}^H \\lambda_h^2} $$\nAs per the problem statement, if the total variance $\\sum_j V_j=0$, we define $\\lambda_h=0$ for all $h$, and $n_{eff, cell}=H$.\nA cell is flagged as extreme if its leverage exceeds $2/H$, i.e., $\\lambda_h > 2/H$. The total count is the number of cells satisfying this. The maximum cell-level leverage is $\\max_h \\lambda_h$.\n\n**Summary of Formulas for Computation:**\n1.  **Effective number of observational units**: $n_{eff, obs} = 1 / (\\sum_{h=1}^H p_h^2 / n_h)$\n2.  **Effective number of variance-contributing cells**:\n    - Let $V_h = p_h^2 \\hat{\\sigma}_h^2 / n_h$ and $V_{tot} = \\sum_h V_h$.\n    - If $V_{tot} = 0$, $n_{eff, cell} = H$.\n    - Otherwise, $\\lambda_h = V_h / V_{tot}$ and $n_{eff, cell} = 1 / (\\sum_{h=1}^H \\lambda_h^2)$.\n3.  **Count of extreme observations**: $\\sum_{h=1}^H n_h \\cdot \\mathbf{1}(p_h/n_h > 2/n)$, where $\\mathbf{1}(\\cdot)$ is the indicator function.\n4.  **Count of extreme cells**: $\\sum_{h=1}^H \\mathbf{1}(\\lambda_h > 2/H)$, where $\\lambda_h$ is defined as $0$ if $V_{tot}=0$.\n5.  **Maximum observation-level normalized influence**: $\\max_{h} (p_h / n_h)$.\n6.  **Maximum cell-level normalized variance-leverage**: $\\max_h \\lambda_h$, defined as $0$ if $V_{tot}=0$.\nThese formulas are implemented below for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the post-stratification diagnostics problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        (5, [0.15, 0.25, 0.10, 0.30, 0.20], [60, 80, 40, 70, 50], 300, [1.0, 0.8, 1.2, 0.9, 1.1]),\n        # Test Case 2\n        (4, [0.40, 0.30, 0.20, 0.10], [10, 90, 80, 70], 250, [1.0, 0.8, 0.7, 0.6]),\n        # Test Case 3\n        (3, [0.50, 0.30, 0.20], [150, 1, 49], 200, [1.0, 0.0, 1.5]),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        H, p_list, n_h_list, n, sigma2_list = case\n        \n        p = np.array(p_list, dtype=float)\n        n_h = np.array(n_h_list, dtype=float)\n        sigma2 = np.array(sigma2_list, dtype=float)\n\n        # 1. Effective number of observational units\n        sum_sq_weights = np.sum(p**2 / n_h)\n        n_eff_obs = 1.0 / sum_sq_weights\n\n        # 3. & 5. Observation-level diagnostics\n        obs_influences = p / n_h\n        # 5. Maximum observation-level normalized influence\n        max_obs_influence = np.max(obs_influences)\n        \n        # 3. Count of extreme observations\n        threshold_obs = 2.0 / n\n        extreme_obs_mask = obs_influences > threshold_obs\n        count_extreme_obs = int(np.sum(n_h[extreme_obs_mask]))\n\n        # 2., 4., & 6. Cell-level diagnostics\n        V_h = (p**2 * sigma2) / n_h\n        V_tot = np.sum(V_h)\n\n        if V_tot == 0:\n            # Special case as defined in the problem\n            # 2. Effective number of variance-contributing cells\n            n_eff_cell = float(H)\n            # 6. Maximum cell-level normalized variance-leverage\n            max_cell_leverage = 0.0\n            # 4. Count of extreme cells\n            count_extreme_cells = 0\n        else:\n            cell_leverages = V_h / V_tot\n            # 2. Effective number of variance-contributing cells\n            n_eff_cell = 1.0 / np.sum(cell_leverages**2)\n            # 6. Maximum cell-level normalized variance-leverage\n            max_cell_leverage = np.max(cell_leverages)\n            \n            # 4. Count of extreme cells\n            threshold_cell = 2.0 / H\n            count_extreme_cells = int(np.sum(cell_leverages > threshold_cell))\n            \n        result_tuple = [\n            n_eff_obs,\n            n_eff_cell,\n            count_extreme_obs,\n            count_extreme_cells,\n            max_obs_influence,\n            max_cell_leverage\n        ]\n        all_results.append(result_tuple)\n\n    # Final print statement in the exact required format.\n    # Format each list of results into \"[v1,v2,...]\" and join them.\n    formatted_results = []\n    for res_list in all_results:\n        # Using a general a number format that avoids scientific notation for\n        # some values and keeps precision, and converting ints to int strings.\n        s_list = []\n        for v in res_list:\n            if isinstance(v, int):\n                s_list.append(str(v))\n            else:\n                s_list.append(format(v, '.17g').rstrip('0').rstrip('.'))\n        \n        inner_str = \"[\" + \",\".join(s_list) + \"]\"\n        formatted_results.append(inner_str)\n    \n    final_output_string = \"[\" + \",\".join(formatted_results) + \"]\"\n    print(final_output_string.replace(\" \", \"\"))\n\nsolve()\n```", "id": "3330477"}]}