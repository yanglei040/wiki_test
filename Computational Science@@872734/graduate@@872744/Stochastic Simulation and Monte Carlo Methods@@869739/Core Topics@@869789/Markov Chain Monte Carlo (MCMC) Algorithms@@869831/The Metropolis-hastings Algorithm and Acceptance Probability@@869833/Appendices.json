{"hands_on_practices": [{"introduction": "The heart of the Metropolis-Hastings algorithm lies in its acceptance probability, which ensures the resulting Markov chain converges to the desired target distribution. However, in real-world applications involving large datasets, log-probabilities can become extremely large in magnitude, making naive computation of the acceptance ratio susceptible to numerical underflow. This practice will guide you through calculating the acceptance probability in a numerically stable way by working entirely in the log domain, a critical skill for any robust MCMC implementation [@problem_id:3355587].", "problem": "Consider a Metropolis-Hastings (MH) step targeting an unnormalized probability density on $\\mathbb{R}^{2}$ of the form $\\pi(\\theta) \\propto \\exp(\\ell(\\theta))$, where $\\ell(\\theta)$ is the log-posterior. The current state is $x = (2.0,\\, 0.5)$ and the proposed state is $y = (2.1,\\, 0.55)$. The log-posterior values (which aggregate a very large number of data contributions) are\n$$\n\\ell(x) \\;=\\; -1.357913942 \\times 10^{6}, \n\\qquad\n\\ell(y) \\;=\\; -1.357919942 \\times 10^{6}.\n$$\nThe proposal is an independent-component multiplicative log-normal random walk with fixed scale parameters $\\sigma_{1}  0$ and $\\sigma_{2}  0$:\n$$\nq(y \\mid x) \\;=\\; \\prod_{i=1}^{2} \\left[ \\frac{1}{y_{i}\\,\\sigma_{i}\\,\\sqrt{2\\pi}} \\; \\exp\\!\\left( -\\frac{\\left(\\ln y_{i} - \\ln x_{i}\\right)^{2}}{2\\sigma_{i}^{2}} \\right) \\right].\n$$\nUsing only fundamental Markov chain Monte Carlo principles and definitions, compute the Metropolis-Hastings acceptance probability for moving from $x$ to $y$ in a numerically stable manner. Your computation must not separately exponentiate $\\ell(x)$ or $\\ell(y)$. Round your final numerical answer to six significant figures.", "solution": "The fundamental principle governing the Metropolis-Hastings (MH) algorithm is the detailed balance condition, which ensures that the resulting Markov chain has the target distribution $\\pi(\\theta)$ as its stationary distribution. The acceptance probability $\\alpha(y \\mid x)$ for a proposed move from a current state $x$ to a new state $y$ is defined to satisfy this condition. The formula for the acceptance probability is:\n$$\n\\alpha(y \\mid x) = \\min\\left(1, A(y \\mid x)\\right)\n$$\nwhere $A(y \\mid x)$ is the Metropolis-Hastings acceptance ratio, given by:\n$$\nA(y \\mid x) = \\frac{\\pi(y) \\, q(x \\mid y)}{\\pi(x) \\, q(y \\mid x)}\n$$\nHere, $\\pi(\\cdot)$ is the target probability density and $q(\\cdot \\mid \\cdot)$ is the proposal (or transition) density.\n\nThe problem states that the target density is an unnormalized posterior of the form $\\pi(\\theta) \\propto \\exp(\\ell(\\theta))$, where $\\ell(\\theta)$ is the log-posterior. The ratio of the target densities at states $y$ and $x$ is therefore:\n$$\n\\frac{\\pi(y)}{\\pi(x)} = \\frac{C \\exp(\\ell(y))}{C \\exp(\\ell(x))} = \\exp(\\ell(y) - \\ell(x))\n$$\nwhere $C$ is the unknown normalization constant, which cancels out.\n\nNext, we must determine the ratio of the proposal densities, $\\frac{q(x \\mid y)}{q(y \\mid x)}$, which is known as the Hastings correction factor. The proposal density is given as:\n$$\nq(y \\mid x) = \\prod_{i=1}^{2} \\left[ \\frac{1}{y_{i}\\,\\sigma_{i}\\,\\sqrt{2\\pi}} \\; \\exp\\!\\left( -\\frac{\\left(\\ln y_{i} - \\ln x_{i}\\right)^{2}}{2\\sigma_{i}^{2}} \\right) \\right]\n$$\nThis describes a process where a new state $y=(y_1, y_2)$ is generated from $x=(x_1, x_2)$ by drawing $\\ln y_i$ from a normal distribution with mean $\\ln x_i$ and standard deviation $\\sigma_i$.\n\nTo find $q(x \\mid y)$, we simply exchange the roles of $x$ and $y$ in the expression for $q(y \\mid x)$:\n$$\nq(x \\mid y) = \\prod_{i=1}^{2} \\left[ \\frac{1}{x_{i}\\,\\sigma_{i}\\,\\sqrt{2\\pi}} \\; \\exp\\!\\left( -\\frac{\\left(\\ln x_{i} - \\ln y_{i}\\right)^{2}}{2\\sigma_{i}^{2}} \\right) \\right]\n$$\nNow, we can compute the ratio. Let's observe that the squared term in the exponent is symmetric: $(\\ln x_i - \\ln y_i)^2 = (-( \\ln y_i - \\ln x_i))^2 = (\\ln y_i - \\ln x_i)^2$. Therefore, the exponential parts of the densities are identical and cancel out in the ratio.\n\\begin{align*}\n\\frac{q(x \\mid y)}{q(y \\mid x)} = \\frac{\\displaystyle\\prod_{i=1}^{2} \\left[ \\frac{1}{x_{i}\\,\\sigma_{i}\\,\\sqrt{2\\pi}} \\; \\exp\\!\\left( -\\frac{\\left(\\ln x_{i} - \\ln y_{i}\\right)^{2}}{2\\sigma_{i}^{2}} \\right) \\right]}{\\displaystyle\\prod_{i=1}^{2} \\left[ \\frac{1}{y_{i}\\,\\sigma_{i}\\,\\sqrt{2\\pi}} \\; \\exp\\!\\left( -\\frac{\\left(\\ln y_{i} - \\ln x_{i}\\right)^{2}}{2\\sigma_{i}^{2}} \\right) \\right]} \\\\\n= \\frac{\\displaystyle\\prod_{i=1}^{2} \\frac{1}{x_{i}\\,\\sigma_{i}\\,\\sqrt{2\\pi}}}{\\displaystyle\\prod_{i=1}^{2} \\frac{1}{y_{i}\\,\\sigma_{i}\\,\\sqrt{2\\pi}}} \\\\\n= \\frac{\\frac{1}{(x_1 \\sigma_1 \\sqrt{2\\pi})(x_2 \\sigma_2 \\sqrt{2\\pi})}}{\\frac{1}{(y_1 \\sigma_1 \\sqrt{2\\pi})(y_2 \\sigma_2 \\sqrt{2\\pi})}} \\\\\n= \\frac{y_1 y_2 \\sigma_1 \\sigma_2 (2\\pi)}{x_1 x_2 \\sigma_1 \\sigma_2 (2\\pi)} \\\\\n= \\frac{y_1 y_2}{x_1 x_2}\n\\end{align*}\nNote that the unknown scale parameters $\\sigma_1$ and $\\sigma_2$ have cancelled out, so their values are not needed for the calculation.\n\nNow we can assemble the full acceptance ratio $A(y \\mid x)$:\n$$\nA(y \\mid x) = \\frac{\\pi(y)}{\\pi(x)} \\cdot \\frac{q(x \\mid y)}{q(y \\mid x)} = \\exp(\\ell(y) - \\ell(x)) \\cdot \\frac{y_1 y_2}{x_1 x_2}\n$$\nThe problem requires a numerically stable computation, specifically forbidding the separate exponentiation of $\\ell(x)$ and $\\ell(y)$. The large negative values of these log-posteriors would cause $\\exp(\\ell(x))$ and $\\exp(\\ell(y))$ to underflow to zero in any standard floating-point arithmetic. Our expression for $A(y \\mid x)$ correctly avoids this by first taking the difference $\\ell(y) - \\ell(x)$.\n\nThe most stable way to compute $\\alpha$ is to first compute the logarithm of the acceptance ratio:\n$$\n\\ln A(y \\mid x) = \\ln\\left( \\exp(\\ell(y) - \\ell(x)) \\cdot \\frac{y_1 y_2}{x_1 x_2} \\right) = (\\ell(y) - \\ell(x)) + \\ln\\left(\\frac{y_1 y_2}{x_1 x_2}\\right)\n$$\nThen, the acceptance probability is $\\alpha(y \\mid x) = \\min(1, \\exp(\\ln A(y \\mid x)))$.\n\nLet's substitute the given numerical values:\nCurrent state: $x = (x_1, x_2) = (2.0, 0.5)$.\nProposed state: $y = (y_1, y_2) = (2.1, 0.55)$.\nLog-posterior values:\n$\\ell(x) = -1.357913942 \\times 10^{6}$\n$\\ell(y) = -1.357919942 \\times 10^{6}$\n\nFirst, we compute the difference in the log-posteriors:\n$$\n\\ell(y) - \\ell(x) = (-1.357919942 \\times 10^{6}) - (-1.357913942 \\times 10^{6}) = -0.000006000 \\times 10^{6} = -6.0\n$$\nNext, we compute the logarithm of the Hastings correction factor:\n$$\n\\ln\\left(\\frac{y_1 y_2}{x_1 x_2}\\right) = \\ln\\left(\\frac{2.1 \\times 0.55}{2.0 \\times 0.5}\\right) = \\ln\\left(\\frac{1.155}{1.0}\\right) = \\ln(1.155)\n$$\nUsing a standard calculator, $\\ln(1.155) \\approx 0.1441003433$.\n\nNow we compute $\\ln A(y \\mid x)$:\n$$\n\\ln A(y \\mid x) = -6.0 + \\ln(1.155) \\approx -6.0 + 0.1441003433 = -5.8558996567\n$$\nSince $\\ln A(y \\mid x)$ is negative, its exponential $A(y \\mid x)$ will be less than $1$. Therefore, the acceptance probability is:\n$$\n\\alpha(y \\mid x) = A(y \\mid x) = \\exp(\\ln A(y \\mid x))\n$$\n$$\n\\alpha(y \\mid x) \\approx \\exp(-5.8558996567) \\approx 0.0028631168\n$$\nThe problem requires the answer to be rounded to six significant figures. The first six significant digits are $2, 8, 6, 3, 1, 1$. The seventh digit is $6$, so we round up the sixth digit.\n$$\n\\alpha(y \\mid x) \\approx 0.00286312\n$$\nThis can also be expressed as $2.86312 \\times 10^{-3}$.", "answer": "$$\n\\boxed{0.00286312}\n$$", "id": "3355587"}, {"introduction": "Many statistical models feature parameters that are subject to constraints, such as residing within a specific interval like $[0, 1]$. This exercise demonstrates how to correctly adapt the Metropolis-Hastings algorithm for such a scenario, where the state space may even be a mix of continuous regions and discrete points at the boundaries. You will see how defining the proposal and target with respect to a shared, mixed dominating measure is the key to satisfying the detailed balance condition across the entire state space [@problem_id:3355557].", "problem": "Consider a target probability measure $\\pi$ supported on the convex set $\\mathcal{C} = [0,1]$. The target is a mixture of a continuous component on $(0,1)$ and atoms at the boundaries: with weights $w_{0} = \\frac{1}{10}$ at $0$, $w_{1} = \\frac{1}{5}$ at $1$, and continuous weight $w_{c} = 1 - w_{0} - w_{1} = \\frac{7}{10}$ distributed on $(0,1)$ according to the probability density function (pdf) $f(x) = 6 x (1 - x)$ for $x \\in (0,1)$. Thus $\\pi$ is a probability measure on $[0,1]$ whose Radon–Nikodym derivative with respect to the mixed dominating measure $\\mu = \\lambda|_{(0,1)} + \\nu_{\\{0,1\\}}$ (Lebesgue measure $\\lambda$ restricted to $(0,1)$ plus counting measure $\\nu$ on $\\{0,1\\}$) is given by $\\pi(x) = w_{c} f(x)$ for $x \\in (0,1)$, $\\pi(0) = w_{0}$, and $\\pi(1) = w_{1}$.\n\nA Metropolis–Hastings (MH) proposal is constructed by first taking a Gaussian step $\\eta \\sim \\mathcal{N}(0, \\sigma^{2})$ and then projecting onto the constraint set using the Euclidean projection $\\Pi_{\\mathcal{C}}$, so that in one dimension $y = \\Pi_{\\mathcal{C}}(x + \\eta) = \\min\\{\\max\\{x + \\eta, 0\\}, 1\\}$. The resulting proposal distribution $q(\\cdot \\mid x)$ is a mixed measure on $[0,1]$ with a continuous density on $(0,1)$ and nonzero point masses at $\\{0,1\\}$ induced by the projection of out-of-bounds proposals. Let $\\varphi(z) = \\frac{1}{\\sqrt{2 \\pi}} \\exp\\!\\left(-\\frac{z^{2}}{2}\\right)$ denote the standard normal pdf and let $\\Phi(z)$ denote its cumulative distribution function (cdf). For $y \\in (0,1)$ one has $q(y \\mid x) = \\frac{1}{\\sigma} \\varphi\\!\\left(\\frac{y - x}{\\sigma}\\right)$, while $q(0 \\mid x) = \\Phi\\!\\left(\\frac{-x}{\\sigma}\\right)$ and $q(1 \\mid x) = 1 - \\Phi\\!\\left(\\frac{1 - x}{\\sigma}\\right)$.\n\nStarting from the principle of detailed balance for Markov chains, derive the corrected Metropolis–Hastings acceptance probability $\\alpha(x, y)$ for this projected proposal, making explicit the roles of the continuous density on $(0,1)$ and the atomic masses at the boundaries $\\{0,1\\}$. Explain how the choice of dominating measure shapes the form of the acceptance ratio and elucidate the effect of boundary mass induced by projection on reversibility.\n\nThen, for a concrete calculation, take $\\sigma = 0.2$, current state $x = \\frac{1}{2}$ and a proposed state $y = \\frac{1}{10}$ (note that both lie in $(0,1)$). Compute the Metropolis–Hastings acceptance probability $\\alpha\\!\\left(\\frac{1}{2}, \\frac{1}{10}\\right)$ for this interior move. Round your final numerical answer to four significant figures and report it without units.", "solution": "The core principle underpinning the Metropolis-Hastings (MH) algorithm is the detailed balance condition, which ensures that the target probability measure $\\pi$ is the stationary distribution of the constructed Markov chain. For a chain with transition kernel $P(x, \\cdot)$, the detailed balance condition with respect to $\\pi$ states that for any two measurable sets $A$ and $B$, $\\int_A \\pi(dx) P(x, B) = \\int_B \\pi(dy) P(y, A)$.\n\nTo work with densities, we must introduce a dominating measure $\\mu$. The problem correctly identifies the necessary dominating measure for the mixed target distribution as $\\mu = \\lambda|_{(0,1)} + \\nu_{\\{0,1\\}}$, which is the sum of the Lebesgue measure on the open interval $(0,1)$ and the counting measure on the boundary points $\\{0,1\\}$. The Radon-Nikodym derivative of $\\pi$ with respect to $\\mu$, which we denote simply as $\\pi(x)$ following the problem's convention, is:\n$$\n\\pi(x) =\n\\begin{cases}\nw_0 = \\frac{1}{10}  \\text{if } x=0 \\\\\nw_1 = \\frac{1}{5}  \\text{if } x=1 \\\\\nw_c f(x) = \\frac{7}{10} \\cdot 6x(1-x) = \\frac{21}{5}x(1-x)  \\text{if } x \\in (0,1)\n\\end{cases}\n$$\nThe MH transition kernel $P(x, dy)$ is composed of a proposal step and an acceptance/rejection step. For $x \\neq y$, the density of the transition kernel with respect to $\\mu$ is $p(x, y) = q(y|x) \\alpha(x,y)$, where $q(y|x)$ is the density of the proposal distribution with respect to $\\mu$ and $\\alpha(x,y)$ is the acceptance probability. The detailed balance condition can then be written in terms of these densities as:\n$$\n\\pi(x) q(y|x) \\alpha(x,y) = \\pi(y) q(x|y) \\alpha(y,x)\n$$\nThe standard solution for the acceptance probability that satisfies this equation is:\n$$\n\\alpha(x,y) = \\min\\left\\{1, \\frac{\\pi(y) q(x|y)}{\\pi(x) q(y|x)}\\right\\}\n$$\nThis general formula is valid for any state space (continuous, discrete, or mixed) provided that the densities $\\pi(\\cdot)$ and $q(\\cdot|\\cdot)$ are taken with respect to the same dominating measure $\\mu$. The proposal density $q(y|x)$ with respect to $\\mu$ is given as:\n$$\nq(y|x) =\n\\begin{cases}\nq(0|x) = \\Phi\\left(\\frac{-x}{\\sigma}\\right)  \\text{if } y=0 \\\\\nq(1|x) = 1 - \\Phi\\left(\\frac{1-x}{\\sigma}\\right)  \\text{if } y=1 \\\\\n\\frac{1}{\\sigma}\\varphi\\left(\\frac{y-x}{\\sigma}\\right)  \\text{if } y \\in (0,1)\n\\end{cases}\n$$\nThe role of the dominating measure $\\mu$ is to provide a unified framework to handle the continuous and discrete parts of the state space. By defining densities relative to $\\mu$, the MH acceptance ratio retains its standard form, correctly balancing probability flows between continuous and discrete states. The boundary mass induced by projection is explicitly captured by the terms $q(0|x)$ and $q(1|x)$, which represent the probability of a proposed Gaussian step from $x$ landing outside the interval $[0,1]$ and being projected to the respective boundary point. These terms are crucial for maintaining reversibility. For instance, for a move from $x \\in (0,1)$ to the boundary point $y=0$, the acceptance ratio correctly incorporates the discrete mass $\\pi(0)$ and the integrated proposal probability $q(0|x)$.\n\nWe are asked to compute the acceptance probability for a specific move from $x = \\frac{1}{2}$ to $y = \\frac{1}{10}$. Since both $x$ and $y$ are in the interior of the interval, $(0,1)$, we are in the case of a continuous-to-continuous transition.\nFor $x, y \\in (0,1)$, the relevant densities are:\n$\\pi(x) = w_c f(x)$\n$\\pi(y) = w_c f(y)$\n$q(y|x) = \\frac{1}{\\sigma}\\varphi\\left(\\frac{y-x}{\\sigma}\\right)$\n$q(x|y) = \\frac{1}{\\sigma}\\varphi\\left(\\frac{x-y}{\\sigma}\\right)$\n\nThe acceptance ratio $R(x,y)$ is:\n$$\nR(x,y) = \\frac{\\pi(y) q(x|y)}{\\pi(x) q(y|x)} = \\frac{w_c f(y) \\cdot \\frac{1}{\\sigma}\\varphi\\left(\\frac{x-y}{\\sigma}\\right)}{w_c f(x) \\cdot \\frac{1}{\\sigma}\\varphi\\left(\\frac{y-x}{\\sigma}\\right)}\n$$\nThe standard normal pdf $\\varphi(z)$ is an even function, meaning $\\varphi(z) = \\varphi(-z)$. Therefore, $\\varphi\\left(\\frac{x-y}{\\sigma}\\right) = \\varphi\\left(-\\frac{y-x}{\\sigma}\\right) = \\varphi\\left(\\frac{y-x}{\\sigma}\\right)$.\nThe proposal ratio simplifies to $1$:\n$$\n\\frac{q(x|y)}{q(y|x)} = 1\n$$\nThis is a characteristic of a symmetric proposal, which reduces the Metropolis-Hastings algorithm to the simpler Metropolis algorithm for such moves. The acceptance ratio becomes:\n$$\nR(x,y) = \\frac{f(y)}{f(x)}\n$$\nAnd the acceptance probability is:\n$$\n\\alpha(x,y) = \\min\\left\\{1, \\frac{f(y)}{f(x)}\\right\\}\n$$\nNow we perform the concrete calculation with the given values:\nCurrent state: $x = \\frac{1}{2} = 0.5$\nProposed state: $y = \\frac{1}{10} = 0.1$\nThe density function is $f(z) = 6z(1-z)$.\n\nFirst, we evaluate $f(x)$:\n$$\nf\\left(\\frac{1}{2}\\right) = 6 \\cdot \\frac{1}{2} \\cdot \\left(1 - \\frac{1}{2}\\right) = 6 \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{6}{4} = \\frac{3}{2} = 1.5\n$$\nNext, we evaluate $f(y)$:\n$$\nf\\left(\\frac{1}{10}\\right) = 6 \\cdot \\frac{1}{10} \\cdot \\left(1 - \\frac{1}{10}\\right) = 6 \\cdot \\frac{1}{10} \\cdot \\frac{9}{10} = \\frac{54}{100} = 0.54\n$$\nThe ratio is:\n$$\n\\frac{f(y)}{f(x)} = \\frac{0.54}{1.5} = \\frac{54/100}{3/2} = \\frac{54}{100} \\cdot \\frac{2}{3} = \\frac{18}{50} = \\frac{9}{25} = 0.36\n$$\nFinally, the acceptance probability is:\n$$\n\\alpha\\left(\\frac{1}{2}, \\frac{1}{10}\\right) = \\min\\left\\{1, 0.36\\right\\} = 0.36\n$$\nThe problem requires the answer to be rounded to four significant figures, which is $0.3600$.", "answer": "$$\n\\boxed{0.3600}\n$$", "id": "3355557"}, {"introduction": "A frontier in modern computational statistics is dealing with models where the likelihood function is intractable and can only be approximated. This problem introduces the powerful pseudo-marginal Metropolis-Hastings algorithm, which cleverly uses an unbiased estimator of the likelihood within the acceptance ratio. You will derive the expected acceptance rate and analyze how strategically inducing correlation in the noise of the likelihood estimator can substantially reduce the variance of the log-ratio and boost sampler efficiency, all while preserving the correctness of the algorithm [@problem_id:3355594].", "problem": "Consider a Metropolis–Hastings algorithm targeting a posterior density $\\pi(x)$ on $\\mathbb{R}^{d}$ known up to a normalizing constant, where $\\pi(x) \\propto \\pi_{0}(x) L(x)$ with prior $\\pi_{0}(x)$ and likelihood $L(x)$. To avoid evaluating $L(x)$ exactly, use a pseudo-marginal construction with an auxiliary random variable $U \\sim g(u)$ and an unbiased nonnegative likelihood estimator $\\widehat{L}(x, U)$ satisfying $\\mathbb{E}_{U \\sim g}[\\widehat{L}(x, U)] = L(x)$ for all $x$. Assume the multiplicative-noise model\n$$\n\\widehat{L}(x, U) \\;=\\; L(x)\\,\\exp\\!\\big(\\epsilon\\big), \\quad \\epsilon \\sim \\mathcal{N}\\!\\big(-\\sigma^{2}/2,\\,\\sigma^{2}\\big),\n$$\nwith $\\sigma^{2} \\in (0,\\infty)$ not depending on $x$. At state $(x, U)$, propose a new parameter $y \\sim q(x, \\cdot)$ with a symmetric random-walk proposal so that $q(x, y) = q(y, x)$, and propose a new auxiliary variable $U'$ from a Markov transition kernel $K_{\\rho}(U, \\cdot)$ that leaves $g$ invariant and induces correlation $\\rho \\in [0,1)$ between the current and proposed log-noises:\n$$\n\\epsilon \\,=\\, \\ln\\!\\big(\\widehat{L}(x, U)/L(x)\\big), \\quad \\epsilon' \\,=\\, \\ln\\!\\big(\\widehat{L}(y, U')/L(y)\\big), \\quad \\text{with } \\begin{pmatrix}\\epsilon \\\\ \\epsilon'\\end{pmatrix} \\sim \\mathcal{N}\\!\\left(\\begin{pmatrix}-\\sigma^{2}/2 \\\\ -\\sigma^{2}/2\\end{pmatrix}, \\begin{pmatrix}\\sigma^{2}  \\rho\\,\\sigma^{2} \\\\ \\rho\\,\\sigma^{2}  \\sigma^{2}\\end{pmatrix}\\right).\n$$\nThe Metropolis–Hastings acceptance probability on the extended space is\n$$\n\\alpha\\big((x,U),(y,U')\\big) \\;=\\; \\min\\!\\left\\{1,\\; \\frac{\\pi_{0}(y)\\,\\widehat{L}(y, U')\\,q(y,x)}{\\pi_{0}(x)\\,\\widehat{L}(x, U)\\,q(x,y)} \\right\\}.\n$$\nDefine the exact log Metropolis ratio\n$$\n\\Delta \\;=\\; \\log\\!\\big(\\pi(y)\\,q(y,x)\\big) \\,-\\, \\log\\!\\big(\\pi(x)\\,q(x,y)\\big),\n$$\nand note that by symmetry of $q$ this reduces to $\\Delta = \\log \\pi(y) - \\log \\pi(x)$. Let $\\Phi(\\cdot)$ denote the cumulative distribution function of the standard normal distribution.\n\nStarting from the fundamental definitions of the Metropolis–Hastings acceptance probability, unbiased estimation, and properties of the multivariate normal distribution, derive a closed-form expression for the conditional expected acceptance probability\n$$\n\\mathbb{E}\\!\\left[\\, \\alpha\\big((x,U),(y,U')\\big) \\,\\middle|\\, x,y \\,\\right]\n$$\nas a function of $\\Delta$, $\\sigma^{2}$, and $\\rho$, expressed using $\\Phi(\\cdot)$ only. Your derivation should proceed by identifying the distribution of the log-noise difference and evaluating the resulting expectation exactly. In your explanation, analyze how increasing $\\rho$ changes the variance of the log-ratio noise and consequently affects the expected acceptance probability, and justify why using $K_{\\rho}$ that leaves $g$ invariant preserves the correctness of the marginal stationary distribution for $x$.\n\nProvide as your final answer a single closed-form analytic expression for $\\mathbb{E}\\!\\left[\\, \\alpha\\big((x,U),(y,U')\\big) \\,\\middle|\\, x,y \\,\\right]$ in terms of $\\Delta$, $\\sigma^{2}$, $\\rho$, and $\\Phi(\\cdot)$. Do not include any inequalities or limits in your final answer. No numerical evaluation is required.", "solution": "### Derivation of the Expected Acceptance Probability\n\nThe acceptance probability is given by\n$$\n\\alpha\\big((x,U),(y,U')\\big) = \\min\\!\\left\\{1, R \\right\\}, \\quad \\text{where} \\quad R = \\frac{\\pi_{0}(y)\\,\\widehat{L}(y, U')\\,q(y,x)}{\\pi_{0}(x)\\,\\widehat{L}(x, U)\\,q(x,y)}.\n$$\nWe substitute the given definitions into the ratio $R$. Using the symmetry of the proposal, $q(x,y) = q(y,x)$, these terms cancel. We also substitute the multiplicative noise model for the likelihood estimators, $\\widehat{L}(x,U) = L(x) \\exp(\\epsilon)$ and $\\widehat{L}(y,U') = L(y) \\exp(\\epsilon')$.\n$$\nR = \\frac{\\pi_{0}(y)\\,L(y)\\,\\exp(\\epsilon')}{\\pi_{0}(x)\\,L(x)\\,\\exp(\\epsilon)}.\n$$\nSince the target posterior is $\\pi(x) \\propto \\pi_0(x) L(x)$, we have $\\frac{\\pi_0(y)L(y)}{\\pi_0(x)L(x)} = \\frac{\\pi(y)}{\\pi(x)}$. The ratio becomes\n$$\nR = \\frac{\\pi(y)}{\\pi(x)}\\exp(\\epsilon' - \\epsilon) = \\exp(\\log\\pi(y) - \\log\\pi(x) + \\epsilon' - \\epsilon).\n$$\nUsing the definition of the exact log Metropolis ratio, $\\Delta = \\log\\pi(y) - \\log\\pi(x)$, we can write\n$$\nR = \\exp(\\Delta + \\epsilon' - \\epsilon).\n$$\nThe acceptance probability is therefore a function of $\\Delta$ and the difference of the log-noises $Z = \\epsilon' - \\epsilon$:\n$$\n\\alpha = \\min\\{1, \\exp(\\Delta + Z)\\}.\n$$\nTo find the conditional expectation $\\mathbb{E}[\\alpha | x, y]$, we must average over the distribution of the random variable $Z$. Since $x$ and $y$ are fixed, $\\Delta$ is a constant. The expectation is taken with respect to the joint distribution of $(U, U')$, which induces the specified joint distribution of $(\\epsilon, \\epsilon')$.\n\nThe variable $Z$ is the difference of two jointly normal random variables, so it is also normally distributed. We find its mean and variance:\n$$\n\\mathbb{E}[Z] = \\mathbb{E}[\\epsilon' - \\epsilon] = \\mathbb{E}[\\epsilon'] - \\mathbb{E}[\\epsilon] = \\left(-\\frac{\\sigma^2}{2}\\right) - \\left(-\\frac{\\sigma^2}{2}\\right) = 0.\n$$\n$$\n\\text{Var}(Z) = \\text{Var}(\\epsilon' - \\epsilon) = \\text{Var}(\\epsilon') + \\text{Var}(\\epsilon) - 2\\text{Cov}(\\epsilon, \\epsilon') = \\sigma^2 + \\sigma^2 - 2(\\rho\\sigma^2) = 2\\sigma^2(1-\\rho).\n$$\nLet $\\sigma_Z^2 = 2\\sigma^2(1-\\rho)$. Thus, $Z \\sim \\mathcal{N}(0, \\sigma_Z^2)$. Let $p(z)$ be the probability density function of $Z$. The desired expectation is\n$$\n\\mathbb{E}[\\alpha | x, y] = \\int_{-\\infty}^{\\infty} \\min\\{1, \\exp(\\Delta + z)\\} p(z) dz.\n$$\nThe function $\\min\\{1, \\exp(\\Delta + z)\\}$ equals $\\exp(\\Delta+z)$ if $\\Delta+z \\le 0$ (i.e., $z \\le -\\Delta$), and it equals $1$ if $\\Delta+z  0$ (i.e., $z  -\\Delta$). We split the integral at $z = -\\Delta$:\n$$\n\\mathbb{E}[\\alpha | x, y] = \\int_{-\\infty}^{-\\Delta} \\exp(\\Delta + z) p(z) dz + \\int_{-\\Delta}^{\\infty} 1 \\cdot p(z) dz.\n$$\nThe second integral is the probability $P(Z  -\\Delta)$. Since $Z \\sim \\mathcal{N}(0, \\sigma_Z^2)$, the standardized variable $S = Z/\\sigma_Z$ is $\\mathcal{N}(0,1)$.\n$$\n\\int_{-\\Delta}^{\\infty} p(z) dz = P(Z  -\\Delta) = P\\left(\\frac{Z}{\\sigma_Z}  -\\frac{\\Delta}{\\sigma_Z}\\right) = 1 - \\Phi\\left(-\\frac{\\Delta}{\\sigma_Z}\\right) = \\Phi\\left(\\frac{\\Delta}{\\sigma_Z}\\right).\n$$\nThe first integral is\n$$\nI_1 = \\int_{-\\infty}^{-\\Delta} \\exp(\\Delta + z) \\frac{1}{\\sqrt{2\\pi\\sigma_Z^2}} \\exp\\left(-\\frac{z^2}{2\\sigma_Z^2}\\right) dz.\n$$\nWe combine the exponential terms and complete the square in the exponent:\n$$\n\\Delta + z - \\frac{z^2}{2\\sigma_Z^2} = \\Delta - \\frac{1}{2\\sigma_Z^2}(z^2 - 2\\sigma_Z^2 z) = \\Delta - \\frac{1}{2\\sigma_Z^2}((z - \\sigma_Z^2)^2 - (\\sigma_Z^2)^2) = \\Delta + \\frac{\\sigma_Z^2}{2} - \\frac{(z-\\sigma_Z^2)^2}{2\\sigma_Z^2}.\n$$\nSubstituting this back into the integral $I_1$:\n$$\nI_1 = \\int_{-\\infty}^{-\\Delta} \\exp\\left(\\Delta + \\frac{\\sigma_Z^2}{2}\\right) \\frac{1}{\\sqrt{2\\pi\\sigma_Z^2}} \\exp\\left(-\\frac{(z-\\sigma_Z^2)^2}{2\\sigma_Z^2}\\right) dz.\n$$\n$$\nI_1 = \\exp\\left(\\Delta + \\frac{\\sigma_Z^2}{2}\\right) \\int_{-\\infty}^{-\\Delta} \\frac{1}{\\sqrt{2\\pi\\sigma_Z^2}} \\exp\\left(-\\frac{(z-\\sigma_Z^2)^2}{2\\sigma_Z^2}\\right) dz.\n$$\nThe integral is the CDF of a normal distribution with mean $\\sigma_Z^2$ and variance $\\sigma_Z^2$, evaluated at $-\\Delta$. Let $W \\sim \\mathcal{N}(\\sigma_Z^2, \\sigma_Z^2)$. The integral is $P(W \\le -\\Delta)$.\n$$\nP(W \\le -\\Delta) = P\\left(\\frac{W-\\sigma_Z^2}{\\sigma_Z} \\le \\frac{-\\Delta-\\sigma_Z^2}{\\sigma_Z}\\right) = \\Phi\\left(-\\frac{\\Delta+\\sigma_Z^2}{\\sigma_Z}\\right).\n$$\nThus, $I_1 = \\exp\\left(\\Delta + \\frac{\\sigma_Z^2}{2}\\right) \\Phi\\left(-\\frac{\\Delta+\\sigma_Z^2}{\\sigma_Z}\\right)$.\nCombining the two parts gives the expected acceptance probability:\n$$\n\\mathbb{E}[\\alpha | x, y] = \\exp\\left(\\Delta + \\frac{\\sigma_Z^2}{2}\\right) \\Phi\\left(-\\frac{\\Delta+\\sigma_Z^2}{\\sigma_Z}\\right) + \\Phi\\left(\\frac{\\Delta}{\\sigma_Z}\\right).\n$$\nSubstituting $\\sigma_Z^2 = 2\\sigma^2(1-\\rho)$ and $\\sigma_Z = \\sqrt{2\\sigma^2(1-\\rho)}$, we obtain the final expression:\n$$\n\\mathbb{E}[\\alpha | x, y] = \\exp\\left(\\Delta + \\sigma^2(1-\\rho)\\right) \\Phi\\left(-\\frac{\\Delta + 2\\sigma^2(1-\\rho)}{\\sqrt{2\\sigma^2(1-\\rho)}}\\right) + \\Phi\\left(\\frac{\\Delta}{\\sqrt{2\\sigma^2(1-\\rho)}}\\right).\n$$\n\n### Analysis of Parameters\n- **Effect of $\\rho$ on noise variance:** The variance of the log-ratio noise, $\\text{Var}(Z) = 2\\sigma^2(1-\\rho)$, is a decreasing function of the correlation $\\rho$. As $\\rho$ increases from $0$ to $1$, the variance decreases from $2\\sigma^2$ to $0$. A higher correlation $\\rho$ means that the noise terms $\\epsilon$ and $\\epsilon'$ are more similar, making their difference $Z$ less variable.\n\n- **Effect of $\\rho$ on expected acceptance probability:** The noise term $Z$ introduces additional randomness into the acceptance ratio, which generally degrades the performance of the MCMC sampler by reducing the average acceptance probability compared to an algorithm with access to the exact likelihoods (where $Z=0$). By increasing $\\rho$, we reduce the variance of $Z$, thereby making the pseudo-marginal algorithm behave more like the exact algorithm. In the limit as $\\rho \\to 1$, $\\sigma_Z^2 \\to 0$, and $Z$ converges in distribution to $0$. The expected acceptance probability converges to $\\mathbb{E}[\\min\\{1, \\exp(\\Delta)\\}] = \\min\\{1, \\exp(\\Delta)\\}$, which is the acceptance probability of the exact Metropolis-Hastings algorithm. Thus, increasing $\\rho$ increases the expected acceptance probability, which typically improves the statistical efficiency of the sampler.\n\n- **Invariance of $g$ under $K_\\rho$:** The correctness of the algorithm relies on the fact that the joint density $\\pi_{ext}(x, u) \\propto \\pi_0(x) \\widehat{L}(x, u) g(u)$ is the stationary distribution of the Markov chain on the extended space $(X, U)$. The Metropolis-Hastings algorithm guarantees this if the detailed balance condition is met. The full acceptance ratio requires comparing the forward proposal density $Q((x,U), (y,U')) = q(x,y)K_\\rho(U,U')$ with the reverse proposal density $Q((y,U'), (x,U)) = q(y,x)K_\\rho(U',U)$. The ratio in the M-H rule for the extended target $\\pi_{ext}(x,u)$ contains the term $\\frac{g(U)K_\\rho(U,U')}{g(U')K_\\rho(U',U)}$. The condition that $K_\\rho$ leaves $g$ invariant is precisely the detailed balance condition $g(U)K_\\rho(U,U') = g(U')K_\\rho(U',U)$. This causes the ratio of proposal densities for the auxiliary variables to be unity, simplifying the acceptance probability to the form given in the problem statement. This guarantees that $\\pi_{ext}$ is the stationary distribution. Consequently, the marginal distribution for $x$, given by $\\int \\pi_{ext}(x, u) du = \\int \\pi_0(x) \\widehat{L}(x,u) g(u) du = \\pi_0(x) \\mathbb{E}[\\widehat{L}(x,U)] = \\pi_0(x) L(x) \\propto \\pi(x)$, is the correct target posterior.", "answer": "$$\n\\boxed{\\exp\\left(\\Delta + \\sigma^{2}(1-\\rho)\\right) \\Phi\\left(-\\frac{\\Delta + 2\\sigma^{2}(1-\\rho)}{\\sqrt{2\\sigma^{2}(1-\\rho)}}\\right) + \\Phi\\left(\\frac{\\Delta}{\\sqrt{2\\sigma^{2}(1-\\rho)}}\\right)}\n$$", "id": "3355594"}]}