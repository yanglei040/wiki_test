{"hands_on_practices": [{"introduction": "The efficiency of a Gibbs sampler is determined by the dependence between successive samples. To quantify this, we can analyze the autocorrelation function (ACF) of the generated chain. This exercise provides a foundational look at this analysis by deriving the ACF for the canonical two-variable Gaussian case, directly linking the sampler's mixing speed to the correlation in the target distribution [@problem_id:3358507].", "problem": "Consider a two-variable Gibbs sampler targeting a centered bivariate normal distribution. Let $(X,Y) \\sim \\mathcal{N}(\\boldsymbol{0}, \\Sigma)$ with covariance matrix\n$$\n\\Sigma \\;=\\; \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix},\n$$\nwhere $|\\rho| < 1$. The Gibbs sampler performs a deterministic scan at each iteration $t$: first draw $X$ from its conditional distribution given the current $Y$, then draw $Y$ from its conditional distribution given the newly updated $X$. Let $x_t$ denote the value of $X$ after completion of the full two-variable Gibbs sweep at iteration $t$, so that the sequence $\\{x_t\\}_{t \\geq 0}$ forms a univariate Markov chain embedded in the full Gibbs chain.\n\nUsing only fundamental properties of the multivariate normal distribution (in particular, the form of conditional distributions) and basic definitions from Markov chain Monte Carlo (MCMC), derive the autocorrelation function (ACF) of $\\{x_t\\}$ at lag $k$ under the stationary regime. Express the ACF, denoted $\\mathrm{acf}(k) = \\mathrm{Corr}(x_t, x_{t+k})$, as a closed-form analytic expression in terms of $\\rho$ and $k$. Your final answer must be a single analytic expression. No numerical approximation is required.", "solution": "The target distribution is a bivariate normal distribution for the random vector $(X, Y)$, given by $(X,Y) \\sim \\mathcal{N}(\\boldsymbol{0}, \\Sigma)$, where the mean vector is $\\boldsymbol{\\mu} = \\boldsymbol{0}$ and the covariance matrix is\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_X^2 & \\rho \\sigma_X \\sigma_Y \\\\ \\rho \\sigma_X \\sigma_Y & \\sigma_Y^2 \\end{pmatrix} = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\n$$\nThis implies that the marginal distributions are standard normal, $X \\sim \\mathcal{N}(0,1)$ and $Y \\sim \\mathcal{N}(0,1)$, with $\\sigma_X^2 = \\sigma_Y^2 = 1$. The parameter $\\rho$ is the correlation coefficient between $X$ and $Y$, with the constraint $|\\rho| < 1$ ensuring the covariance matrix is positive definite.\n\nThe primary step is to determine the conditional distributions required for the Gibbs sampler. For a general bivariate normal distribution as specified above, the conditional distribution of $X$ given $Y=y$ is also normal:\n$$\nX | Y=y \\sim \\mathcal{N}\\left(\\mu_X + \\frac{\\rho \\sigma_X}{\\sigma_Y}(y - \\mu_Y), \\sigma_X^2(1-\\rho^2)\\right)\n$$\nIn our specific case, with $\\mu_X = \\mu_Y = 0$ and $\\sigma_X = \\sigma_Y = 1$, the conditional distributions simplify significantly.\nThe conditional distribution of $X$ given $Y=y$ is:\n$$\np(x|y) \\sim \\mathcal{N}(\\rho y, 1-\\rho^2)\n$$\nBy symmetry, the conditional distribution of $Y$ given $X=x$ is:\n$$\np(y|x) \\sim \\mathcal{N}(\\rho x, 1-\\rho^2)\n$$\n\nThe Gibbs sampling algorithm proceeds in iterations. Let $(X_t, Y_t)$ be the state of the chain after iteration $t$. The problem statement defines a deterministic scan where, to get from state $t$ to $t+1$, we first update $X$ and then $Y$. Let the state at the beginning of iteration $t+1$ be $(X_t, Y_t)$. The updates are:\n$1$. Draw an intermediate value $X'_{t+1}$ from the conditional distribution of $X$ given $Y_t$: $X'_{t+1} \\sim p(X | Y=Y_t)$.\n$2$. Draw the new value $Y_{t+1}$ from the conditional distribution of $Y$ given this new $X'_{t+1}$: $Y_{t+1} \\sim p(Y | X=X'_{t+1})$.\nThe state after the full sweep is $(X'_{t+1}, Y_{t+1})$. The problem specifies that the sequence of interest is $\\{x_t\\}_{t \\geq 0}$, where $x_t$ is the value of the $X$ component after the sweep at iteration $t$. Adopting this notation, we set $X_t = X'_t$. So, the transition from iteration $t$ to $t+1$ involves the sequence of states $(X_t, Y_t) \\rightarrow (X_{t+1}, Y_{t+1})$, generated as follows:\n$$\nX_{t+1} \\sim p(X | Y=Y_t)\n$$\n$$\nY_{t+1} \\sim p(Y | X=X_{t+1})\n$$\nWe seek the autocorrelation function $\\mathrm{acf}(k) = \\mathrm{Corr}(X_t, X_{t+k})$ for the stationary sequence $\\{X_t\\}$. In the stationary regime, the moments of the distribution are time-invariant. In particular, from the target distribution, we have $E[X_t] = 0$ and $\\mathrm{Var}(X_t) = E[X_t^2] = 1$ for all $t$.\n\nThe autocorrelation at lag $k$ is defined as:\n$$\n\\mathrm{acf}(k) = \\mathrm{Corr}(X_t, X_{t+k}) = \\frac{\\mathrm{Cov}(X_t, X_{t+k})}{\\sqrt{\\mathrm{Var}(X_t) \\mathrm{Var}(X_{t+k})}}\n$$\nSince $\\mathrm{Var}(X_t) = \\mathrm{Var}(X_{t+k}) = 1$ in stationarity, the autocorrelation is equal to the autocovariance:\n$$\n\\mathrm{acf}(k) = \\mathrm{Cov}(X_t, X_{t+k}) = E[X_t X_{t+k}] - E[X_t]E[X_{t+k}] = E[X_t X_{t+k}]\n$$\nTo find $E[X_t X_{t+k}]$, we can use the law of iterated expectations. First, let us establish the relationship between $X_{t+1}$ and $X_t$. The update can be described by a generative model based on the conditional distributions:\n$$\nX_{t+1} = \\rho Y_t + \\varepsilon_{t+1}, \\quad \\text{where } \\varepsilon_{t+1} \\sim \\mathcal{N}(0, 1-\\rho^2)\n$$\nAnd from the previous step (or from the stationary distribution of $(X_t, Y_t)$), we know that $Y_t$ and $X_t$ are related via:\n$$\nY_t = \\rho X_t + \\delta_t, \\quad \\text{where } \\delta_t \\sim \\mathcal{N}(0, 1-\\rho^2)\n$$\nThe noise term $\\delta_t$ is independent of $X_t$, and $\\varepsilon_{t+1}$ is independent of $Y_t$ (and thus also of $X_t$ and $\\delta_t$).\nSubstituting the expression for $Y_t$ into the one for $X_{t+1}$, we get:\n$$\nX_{t+1} = \\rho(\\rho X_t + \\delta_t) + \\varepsilon_{t+1} = \\rho^2 X_t + \\rho \\delta_t + \\varepsilon_{t+1}\n$$\nThis reveals that the sequence $\\{X_t\\}$ follows a first-order autoregressive process, AR($1$). Let's use this structure to find the conditional expectation $E[X_{t+1} | X_t]$.\n$$\nE[X_{t+1} | X_t=x_t] = E[\\rho^2 x_t + \\rho \\delta_t + \\varepsilon_{t+1} | X_t=x_t] = \\rho^2 x_t + \\rho E[\\delta_t] + E[\\varepsilon_{t+1}] = \\rho^2 x_t\n$$\nSo, $E[X_{t+1} | X_t] = \\rho^2 X_t$.\nNow, we can find the autocovariance for lag $k=1$:\n$$\nE[X_t X_{t+1}] = E[E[X_t X_{t+1} | X_t]] = E[X_t E[X_{t+1} | X_t]] = E[X_t (\\rho^2 X_t)] = \\rho^2 E[X_t^2] = \\rho^2 \\mathrm{Var}(X_t) = \\rho^2\n$$\nThus, $\\mathrm{acf}(1) = \\rho^2$.\n\nTo find the ACF for an arbitrary lag $k$, we find the conditional expectation $E[X_{t+k} | X_t]$ by iterating the conditional expectation one step at a time:\n$$\nE[X_{t+k} | X_t] = E[E[X_{t+k} | X_{t+k-1}] | X_t] = E[\\rho^2 X_{t+k-1} | X_t] = \\rho^2 E[X_{t+k-1} | X_t]\n$$\nThis gives a recursive relation. Unfolding this recursion, we get:\n$$\nE[X_{t+k} | X_t] = \\rho^2 E[X_{t+k-1} | X_t] = (\\rho^2)^2 E[X_{t+k-2} | X_t] = \\dots = (\\rho^2)^k E[X_t | X_t] = (\\rho^2)^k X_t\n$$\nNow we can compute the autocovariance for lag $k$:\n$$\n\\mathrm{Cov}(X_t, X_{t+k}) = E[X_t X_{t+k}] = E[E[X_t X_{t+k} | X_t]] = E[X_t E[X_{t+k} | X_t]]\n$$\nSubstituting the result for the conditional expectation:\n$$\n\\mathrm{Cov}(X_t, X_{t+k}) = E[X_t ((\\rho^2)^k X_t)] = (\\rho^2)^k E[X_t^2] = (\\rho^2)^k \\mathrm{Var}(X_t) = (\\rho^2)^k \\cdot 1 = \\rho^{2k}\n$$\nSince the autocorrelation equals the autocovariance in this case, we have our final result. The autocorrelation function of the embedded Markov chain $\\{x_t\\}$ at lag $k$ is:\n$$\n\\mathrm{acf}(k) = \\rho^{2k}\n$$\nThis result shows that the correlation between samples decays geometrically. The rate of decay is determined by $\\rho^2$. A value of $\\rho$ close to $\\pm 1$ implies high correlation in the target distribution, which leads to slow decay ($\\rho^2$ close to $1$) and thus slow mixing of the Gibbs sampler for the $X$ component. Conversely, when $\\rho$ is close to $0$, the variables are nearly independent, and the samples generated by the Gibbs sampler are nearly independent ($\\rho^2$ close to $0$).", "answer": "$$\n\\boxed{\\rho^{2k}}\n$$", "id": "3358507"}, {"introduction": "When sampling from a distribution with more than two variables, we must choose an update strategy, or \"scan.\" This exercise contrasts two popular methods, systematic-scan and random-scan Gibbs sampling, within a three-variable Gaussian model. By analyzing the underlying linear operators that govern the sampler's evolution, you will see how different scanning schemes can lead to vastly different performance characteristics [@problem_id:3358547].", "problem": "Consider the target distribution given by a centered Gaussian on $\\mathbb{R}^{3}$ with precision matrix\n$$\nQ \\;=\\; \\begin{pmatrix}\n1 & -\\rho & 0\\\\\n-\\rho & 1 & -\\rho\\\\\n0 & -\\rho & 1\n\\end{pmatrix},\n$$\nwhere $\\rho \\in \\mathbb{R}$ satisfies $|\\rho| < 1/\\sqrt{2}$ so that $Q$ is positive definite. Let $X = (X_{1},X_{2},X_{3})^{\\top} \\sim \\mathcal{N}(0, Q^{-1})$ denote a random vector with this distribution, and let $f(x) = x_{2}$. You run two Markov chain Monte Carlo (MCMC) schemes targeting this Gaussian:\n\n- Systematic-scan Gibbs sampling: sequentially update $X_{1} \\mid (X_{2},X_{3})$, then $X_{2} \\mid (X_{1},X_{3})$, then $X_{3} \\mid (X_{1},X_{2})$ in that fixed order.\n- Random-scan Gibbs sampling: at each “mini-step,” select a single coordinate uniformly at random from $\\{1,2,3\\}$ and update only that coordinate conditional on the other two.\n\nWork in the stationary regime and consider the time series $\\{X^{(t)}\\}_{t\\ge 0}$ of states. Treat one full systematic sweep as one time step for the systematic-scan chain; for the random-scan chain, a “mini-step” is one time step. Starting only from the core definitions of Gibbs sampling for multivariate Gaussians, the notion of Markov transition operators for linear Gaussian updates, and the Markov chain Central Limit Theorem (CLT) for additive functionals, proceed as follows:\n\n1) For the systematic-scan Gibbs sampler, derive the deterministic linear operator $B_{\\mathrm{sys}} \\in \\mathbb{R}^{3\\times 3}$ such that the conditional mean of one full sweep satisfies\n$$\n\\mathbb{E}\\!\\left[\\,X^{(t+1)} \\mid X^{(t)} = x\\,\\right] \\;=\\; B_{\\mathrm{sys}}\\, x.\n$$\nUsing only linear-algebraic reasoning from first principles, determine the spectrum of $B_{\\mathrm{sys}}$ and its spectral radius as a function of $\\rho$.\n\n2) Using an eigen-analysis argument for linear Gaussian Markov chains of the form $X^{(t+1)} = B_{\\mathrm{sys}} X^{(t)} + \\xi^{(t+1)}$ with independent Gaussian innovations $\\xi^{(t+1)}$, show that the stationary autocovariance at lag $k$ for $f(X^{(t)}) = X^{(t)}_{2}$ has the form\n$$\n\\mathrm{Cov}\\!\\left(f(X^{(t+k)}), f(X^{(t)})\\right)\n\\;=\\; \\alpha(\\rho)\\,\\lambda(\\rho)^{k},\n$$\nfor some coefficient $\\alpha(\\rho)$ and base $\\lambda(\\rho)$ that you must identify explicitly in terms of $\\rho$. Justify each step from the linear-operator viewpoint, without assuming reversibility.\n\n3) Let $\\overline{f}_{T} = T^{-1}\\sum_{t=1}^{T} f(X^{(t)})$. Using the Markov chain CLT and your result in part $2$, derive the exact closed-form expression (as a function of $\\rho$) for the asymptotic variance of $\\overline{f}_{T}$ under the systematic-scan chain, defined by\n$$\n\\sigma_{\\mathrm{sys}}^{2}(\\rho) \\;=\\; \\lim_{T\\to\\infty} T\\cdot \\mathrm{Var}\\!\\left(\\overline{f}_{T}\\right).\n$$\n\n4) Now consider the random-scan chain at stationarity. Compute the lag-one autocorrelation of $f(X^{(t)})$ across a single mini-step. Based on this and your part $1$ analysis, discuss—purely qualitatively and from first principles—which scheme is expected to yield a smaller asymptotic variance per full sweep when $|\\rho|$ is small, without attempting to compute an exact closed-form for the random-scan asymptotic variance.\n\nProvide your final answer as the closed-form analytic expression for $\\sigma_{\\mathrm{sys}}^{2}(\\rho)$ from part $3$ (no units). Do not provide any numerical approximation.", "solution": "The target distribution is a multivariate normal distribution $X \\sim \\mathcal{N}(0, \\Sigma)$, where the precision matrix $Q = \\Sigma^{-1}$ is given by\n$$\nQ = \\begin{pmatrix} 1 & -\\rho & 0 \\\\ -\\rho & 1 & -\\rho \\\\ 0 & -\\rho & 1 \\end{pmatrix}\n$$\nThe condition $|\\rho| < 1/\\sqrt{2}$ ensures that $Q$ is positive definite. For a general multivariate normal distribution with precision $Q$ and mean $\\mu$, the conditional distribution of $X_i$ given all other components $X_{-i}$ is\n$$\nX_i \\mid X_{-i} \\sim \\mathcal{N}\\left(\\mu_i - \\frac{1}{Q_{ii}} \\sum_{j \\neq i} Q_{ij} (X_j - \\mu_j), \\frac{1}{Q_{ii}}\\right)\n$$\nIn our case, the mean is $\\mu=0$ and all diagonal elements of $Q$ are $Q_{ii}=1$. The conditional expectations are:\n$$ \\mathbb{E}[X_1 \\mid X_2, X_3] = - (Q_{12}X_2 + Q_{13}X_3) = \\rho X_2 $$\n$$ \\mathbb{E}[X_2 \\mid X_1, X_3] = - (Q_{21}X_1 + Q_{23}X_3) = \\rho X_1 + \\rho X_3 $$\n$$ \\mathbb{E}[X_3 \\mid X_1, X_2] = - (Q_{31}X_1 + Q_{32}X_2) = \\rho X_2 $$\n\n**1) Systematic-scan Gibbs sampler: Transition operator and spectrum**\n\nLet $X^{(t)} = (X_1^{(t)}, X_2^{(t)}, X_3^{(t)})^\\top$ be the state of the chain at time $t$. A full sweep of the systematic-scan Gibbs sampler updates the components in the order $1, 2, 3$. Let the new state be $X'=(X'_1, X'_2, X'_3)^\\top = X^{(t+1)}$.\nThe updates are performed sequentially:\n1. $X'_1$ is sampled from $p(X_1 \\mid X_2=X_2^{(t)}, X_3=X_3^{(t)})$.\n2. $X'_2$ is sampled from $p(X_2 \\mid X_1=X'_1, X_3=X_3^{(t)})$.\n3. $X'_3$ is sampled from $p(X_3 \\mid X_1=X'_1, X_2=X'_2)$.\n\nThe operator $B_{\\mathrm{sys}}$ is defined by the conditional expectation of the next state: $\\mathbb{E}[X^{(t+1)} \\mid X^{(t)} = x] = B_{\\mathrm{sys}} x$. We compute this using the law of total expectation:\n$$ \\mathbb{E}[X'_1 \\mid X^{(t)}=x] = \\mathbb{E}[ \\mathbb{E}[X'_1 \\mid X_2^{(t)}=x_2, X_3^{(t)}=x_3] \\mid X^{(t)}=x] = \\rho x_2 $$\n$$ \\mathbb{E}[X'_2 \\mid X^{(t)}=x] = \\mathbb{E}[ \\mathbb{E}[X'_2 \\mid X'_1, X_3^{(t)}=x_3] \\mid X^{(t)}=x] = \\mathbb{E}[\\rho X'_1 + \\rho x_3 \\mid X^{(t)}=x] = \\rho \\mathbb{E}[X'_1 \\mid X^{(t)}=x] + \\rho x_3 = \\rho(\\rho x_2) + \\rho x_3 = \\rho^2 x_2 + \\rho x_3 $$\n$$ \\mathbb{E}[X'_3 \\mid X^{(t)}=x] = \\mathbb{E}[ \\mathbb{E}[X'_3 \\mid X'_1, X'_2] \\mid X^{(t)}=x] = \\mathbb{E}[\\rho X'_2 \\mid X^{(t)}=x] = \\rho \\mathbb{E}[X'_2 \\mid X^{(t)}=x] = \\rho(\\rho^2 x_2 + \\rho x_3) = \\rho^3 x_2 + \\rho^2 x_3 $$\nWriting this in matrix form:\n$$\n\\mathbb{E}[X^{(t+1)} \\mid X^{(t)}=x] = \\begin{pmatrix} \\rho x_2 \\\\ \\rho^2 x_2 + \\rho x_3 \\\\ \\rho^3 x_2 + \\rho^2 x_3 \\end{pmatrix} = \\begin{pmatrix} 0 & \\rho & 0 \\\\ 0 & \\rho^2 & \\rho \\\\ 0 & \\rho^3 & \\rho^2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}\n$$\nThus, the operator is\n$$\nB_{\\mathrm{sys}} = \\begin{pmatrix} 0 & \\rho & 0 \\\\ 0 & \\rho^2 & \\rho \\\\ 0 & \\rho^3 & \\rho^2 \\end{pmatrix}\n$$\nTo find its spectrum, we solve the characteristic equation $\\det(B_{\\mathrm{sys}} - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix} -\\lambda & \\rho & 0 \\\\ 0 & \\rho^2-\\lambda & \\rho \\\\ 0 & \\rho^3 & \\rho^2-\\lambda \\end{pmatrix} = -\\lambda \\left[ (\\rho^2-\\lambda)^2 - \\rho^4 \\right] = -\\lambda[\\lambda^2 - 2\\rho^2\\lambda + \\rho^4 - \\rho^4] = -\\lambda^2(\\lambda - 2\\rho^2) = 0\n$$\nThe eigenvalues are $\\lambda_1 = 2\\rho^2$, $\\lambda_2 = 0$, and $\\lambda_3 = 0$. The spectrum of $B_{\\mathrm{sys}}$ is $\\{2\\rho^2, 0, 0\\}$.\nThe spectral radius is $\\rho(B_{\\mathrm{sys}}) = \\max_i |\\lambda_i| = |2\\rho^2| = 2\\rho^2$. The condition for convergence, $\\rho(B_{\\mathrm{sys}}) < 1$, gives $2\\rho^2 < 1$, or $|\\rho| < 1/\\sqrt{2}$, which matches the problem statement.\n\n**2) Autocovariance of $f(X^{(t)}) = X_2^{(t)}$**\n\nThe linear Gaussian Markov chain can be written as $X^{(t+1)} = B_{\\mathrm{sys}} X^{(t)} + \\xi^{(t+1)}$, where $\\xi^{(t+1)}$ is a zero-mean Gaussian innovation independent of $X^{(t)}$. In the stationary regime, $\\mathbb{E}[X^{(t)}] = 0$.\nThe autocovariance of the state vector at lag $k$ is given by:\n$$\n\\mathrm{Cov}(X^{(t+k)}, X^{(t)}) = \\mathbb{E}[X^{(t+k)} (X^{(t)})^\\top] = \\mathbb{E}[(B_{\\mathrm{sys}}X^{(t+k-1)} + \\xi^{(t+k)})(X^{(t)})^\\top] = B_{\\mathrm{sys}} \\mathbb{E}[X^{(t+k-1)} (X^{(t)})^\\top]\n$$\nBy recursion, this leads to $\\mathrm{Cov}(X^{(t+k)}, X^{(t)}) = (B_{\\mathrm{sys}})^k \\mathbb{E}[X^{(t)} (X^{(t)})^\\top] = (B_{\\mathrm{sys}})^k \\Sigma$, where $\\Sigma$ is the stationary covariance matrix $\\Sigma=Q^{-1}$.\nThe function of interest is $f(X^{(t)}) = X_2^{(t)} = e_2^\\top X^{(t)}$, where $e_2 = (0, 1, 0)^\\top$. Its autocovariance is:\n$$\n\\mathrm{Cov}(f(X^{(t+k)}), f(X^{(t)})) = \\mathrm{Cov}(X_2^{(t+k)}, X_2^{(t)}) = e_2^\\top \\mathrm{Cov}(X^{(t+k)}, X^{(t)}) e_2 = e_2^\\top (B_{\\mathrm{sys}})^k \\Sigma e_2\n$$\nThe matrix $B_{\\mathrm{sys}}$ is not diagonalizable, as the geometric multiplicity of the eigenvalue $0$ is $1$, while its algebraic multiplicity is $2$. To analyze $(B_{\\mathrm{sys}})^k$, we use a Jordan decomposition. However, it's simpler to analyze the action of $(B_{\\mathrm{sys}})^k$ on the specific vector $\\Sigma e_2$. First, we compute $\\Sigma = Q^{-1}$:\n$$\n\\Sigma = \\frac{1}{1-2\\rho^2} \\begin{pmatrix} 1-\\rho^2 & \\rho & \\rho^2 \\\\ \\rho & 1 & \\rho \\\\ \\rho^2 & \\rho & 1-\\rho^2 \\end{pmatrix}\n$$\nThe vector $\\Sigma e_2$ is the second column of $\\Sigma$:\n$$\n\\Sigma e_2 = \\frac{1}{1-2\\rho^2} \\begin{pmatrix} \\rho \\\\ 1 \\\\ \\rho \\end{pmatrix}\n$$\nLet's find the eigenvectors of $B_{\\mathrm{sys}}$. For the eigenvalue $\\lambda_1 = 2\\rho^2$, the eigenvector equation $(B_{\\mathrm{sys}} - 2\\rho^2 I)v = 0$ gives $v_1 \\propto (1, 2\\rho, 2\\rho^2)^\\top$. The eigenspace for $\\lambda=0$ is spanned by $v_2 = (1, 0, 0)^\\top$.\nThe vector $\\Sigma e_2$ is not an eigenvector. However, its projection onto the relevant eigenspaces determines the long-term behavior. Let's analyze the action of $B_{\\mathrm{sys}}$ on $\\Sigma e_2$:\n$$\nB_{\\mathrm{sys}} (\\Sigma e_2) = \\frac{1}{1-2\\rho^2} \\begin{pmatrix} 0 & \\rho & 0 \\\\ 0 & \\rho^2 & \\rho \\\\ 0 & \\rho^3 & \\rho^2 \\end{pmatrix} \\begin{pmatrix} \\rho \\\\ 1 \\\\ \\rho \\end{pmatrix} = \\frac{1}{1-2\\rho^2} \\begin{pmatrix} \\rho \\\\ \\rho^2+\\rho^2 \\\\ \\rho^3+\\rho^2 \\end{pmatrix} = \\frac{1}{1-2\\rho^2} \\begin{pmatrix} \\rho \\\\ 2\\rho^2 \\\\ \\rho^2(\\rho+1)\\end{pmatrix}\n$$\nThis direct computation is cumbersome. The form suggests a simpler structure. Let's check if $B_{\\mathrm{sys}}^\\top$ has a more convenient structure. A key property of Gibbs samplers for Gaussian targets is that $B=I-\\Sigma_\\xi \\Sigma^{-1}$ where $\\Sigma_\\xi$ is the covariance of the innovation. Here, for systematic scan, $B_{sys} = B_3 B_2 B_1$. Stationarity implies $\\Sigma = B_{sys} \\Sigma B_{sys}^\\top + \\Sigma_\\xi$. The autocovariance is also $\\Sigma (B_{sys}^\\top)^k$. Thus we need to analyze $e_2^\\top \\Sigma (B_{\\mathrm{sys}}^\\top)^k e_2$. The left eigenvectors of $B_{sys}$ are the right eigenvectors of $B_{sys}^\\top$. The left eigenvector for $\\lambda_1=2\\rho^2$ is $w_1 = (0, \\rho, 1)^\\top$.\nLet's directly compute $e_2^\\top(B_{\\mathrm{sys}})^k$. Note $e_2^\\top B_{\\mathrm{sys}} = (0, \\rho^2, \\rho)$.\n$e_2^\\top (B_{\\mathrm{sys}})^2 = (0, \\rho^2, \\rho) B_{\\mathrm{sys}} = (0, \\rho^4+\\rho^4, \\rho^3+\\rho^3) = (0, 2\\rho^4, 2\\rho^3) = 2\\rho^2 (0, \\rho^2, \\rho) = (2\\rho^2) e_2^\\top B_{\\mathrm{sys}}$.\nBy induction, for $k \\ge 1$: $e_2^\\top (B_{\\mathrm{sys}})^k = (2\\rho^2)^{k-1} e_2^\\top B_{\\mathrm{sys}}$.\nSo, for $k \\ge 1$:\n$$ \\mathrm{Cov}(X_2^{(t+k)}, X_2^{(t)}) = (e_2^\\top (B_{\\mathrm{sys}})^k) (\\Sigma e_2) = (2\\rho^2)^{k-1} (e_2^\\top B_{\\mathrm{sys}}) (\\Sigma e_2) $$\n$$ e_2^\\top B_{\\mathrm{sys}} = (0, \\rho^2, \\rho) $$\n$$ (\\Sigma e_2) = \\frac{1}{1-2\\rho^2} (\\rho, 1, \\rho)^\\top $$\n$$ (e_2^\\top B_{\\mathrm{sys}}) (\\Sigma e_2) = \\frac{1}{1-2\\rho^2} [ \\rho^2(1) + \\rho(\\rho) ] = \\frac{2\\rho^2}{1-2\\rho^2} $$\nTherefore, for $k \\ge 1$:\n$$ \\mathrm{Cov}(X_2^{(t+k)}, X_2^{(t)}) = (2\\rho^2)^{k-1} \\frac{2\\rho^2}{1-2\\rho^2} = \\frac{1}{1-2\\rho^2} (2\\rho^2)^k $$\nFor $k=0$, $\\mathrm{Cov}(X_2^{(t)}, X_2^{(t)}) = \\mathrm{Var}(X_2) = \\Sigma_{22} = \\frac{1}{1-2\\rho^2}$. The formula $\\frac{1}{1-2\\rho^2} (2\\rho^2)^0$ also gives this result.\nSo, the autocovariance is $\\alpha(\\rho) \\lambda(\\rho)^k$ for all $k \\ge 0$, with\n$$ \\alpha(\\rho) = \\frac{1}{1-2\\rho^2}, \\quad \\lambda(\\rho) = 2\\rho^2 $$\n\n**3) Asymptotic variance $\\sigma_{\\mathrm{sys}}^2(\\rho)$**\n\nThe Markov chain Central Limit Theorem states that the asymptotic variance of the sample mean $\\overline{f}_{T} = T^{-1}\\sum_{t=1}^{T} f(X^{(t)})$ is given by:\n$$\n\\sigma^2(\\rho) = \\lim_{T\\to\\infty} T \\cdot \\mathrm{Var}(\\overline{f}_{T}) = \\mathrm{Var}_\\pi(f(X)) + 2 \\sum_{k=1}^{\\infty} \\mathrm{Cov}_\\pi(f(X^{(k)}), f(X^{(0)}))\n$$\nIn our case, $f(X) = X_2$. We have:\n$$ \\mathrm{Var}_\\pi(X_2) = \\Sigma_{22} = \\frac{1}{1-2\\rho^2} $$\nThe sum of autocovariances is a geometric series:\n$$ \\sum_{k=1}^{\\infty} \\mathrm{Cov}(X_2^{(t+k)}, X_2^{(t)}) = \\sum_{k=1}^{\\infty} \\frac{1}{1-2\\rho^2} (2\\rho^2)^k $$\nThe ratio of the series is $2\\rho^2$, and since $|\\rho| < 1/\\sqrt{2}$, we have $|2\\rho^2| < 1$. The sum is:\n$$ \\frac{1}{1-2\\rho^2} \\sum_{k=1}^{\\infty} (2\\rho^2)^k = \\frac{1}{1-2\\rho^2} \\left(\\frac{2\\rho^2}{1-2\\rho^2}\\right) = \\frac{2\\rho^2}{(1-2\\rho^2)^2} $$\nCombining the terms, the asymptotic variance is:\n$$\n\\sigma_{\\mathrm{sys}}^{2}(\\rho) = \\frac{1}{1-2\\rho^2} + 2 \\cdot \\frac{2\\rho^2}{(1-2\\rho^2)^2} = \\frac{1-2\\rho^2}{(1-2\\rho^2)^2} + \\frac{4\\rho^2}{(1-2\\rho^2)^2} = \\frac{1-2\\rho^2+4\\rho^2}{(1-2\\rho^2)^2} = \\frac{1+2\\rho^2}{(1-2\\rho^2)^2}\n$$\n\n**4) Qualitative comparison with random-scan Gibbs sampler**\n\nFor the random-scan sampler, at each mini-step, we choose an index $i \\in \\{1,2,3\\}$ with probability $1/3$ and update only $X_i$. We first compute the lag-one autocorrelation of $f(X^{(t)})=X_2^{(t)}$ over one mini-step.\n$$ \\mathrm{Corr}(X_2^{(t+1)}, X_2^{(t)}) = \\frac{\\mathrm{Cov}(X_2^{(t+1)}, X_2^{(t)})}{\\mathrm{Var}(X_2)} = \\frac{\\mathbb{E}[X_2^{(t+1)}X_2^{(t)}]}{\\mathrm{Var}(X_2)} $$\nUsing the law of total expectation, conditioning on the chosen index $I_{t+1}$:\n$$ \\mathbb{E}[X_2^{(t+1)} \\mid X^{(t)}] = \\frac{1}{3}\\mathbb{E}[X_2^{(t+1)} \\mid X^{(t)}, I=1] + \\frac{1}{3}\\mathbb{E}[X_2^{(t+1)} \\mid X^{(t)}, I=2] + \\frac{1}{3}\\mathbb{E}[X_2^{(t+1)} \\mid X^{(t)}, I=3] $$\nIf $I=1$ or $I=3$, $X_2$ is not updated, so $X_2^{(t+1)} = X_2^{(t)}$. If $I=2$, $X_2$ is updated and its conditional mean is $\\rho X_1^{(t)} + \\rho X_3^{(t)}$. Thus:\n$$ \\mathbb{E}[X_2^{(t+1)} \\mid X^{(t)}] = \\frac{1}{3}X_2^{(t)} + \\frac{1}{3}(\\rho X_1^{(t)} + \\rho X_3^{(t)}) + \\frac{1}{3}X_2^{(t)} = \\frac{2}{3}X_2^{(t)} + \\frac{\\rho}{3}(X_1^{(t)} + X_3^{(t)}) $$\nNow we compute the covariance:\n$$ \\mathbb{E}[X_2^{(t+1)}X_2^{(t)}] = \\mathbb{E}\\left[X_2^{(t)} \\left(\\frac{2}{3}X_2^{(t)} + \\frac{\\rho}{3}(X_1^{(t)} + X_3^{(t)})\\right)\\right] = \\frac{2}{3}\\mathbb{E}[(X_2^{(t)})^2] + \\frac{\\rho}{3}\\mathbb{E}[X_1^{(t)}X_2^{(t)}] + \\frac{\\rho}{3}\\mathbb{E}[X_3^{(t)}X_2^{(t)}] $$\nUsing the stationary covariances from $\\Sigma = Q^{-1}$: $\\mathbb{E}[(X_2)^2] = \\Sigma_{22} = \\frac{1}{1-2\\rho^2}$, $\\mathbb{E}[X_1X_2] = \\Sigma_{12} = \\frac{\\rho}{1-2\\rho^2}$, $\\mathbb{E}[X_2X_3] = \\Sigma_{23} = \\frac{\\rho}{1-2\\rho^2}$.\n$$ \\mathbb{E}[X_2^{(t+1)}X_2^{(t)}] = \\frac{2}{3}\\frac{1}{1-2\\rho^2} + \\frac{\\rho}{3}\\frac{\\rho}{1-2\\rho^2} + \\frac{\\rho}{3}\\frac{\\rho}{1-2\\rho^2} = \\frac{2+2\\rho^2}{3(1-2\\rho^2)} $$\nThe lag-one autocorrelation per mini-step is:\n$$ \\rho_{\\mathrm{rand}}(1) = \\frac{\\mathbb{E}[X_2^{(t+1)}X_2^{(t)}]}{\\mathrm{Var}(X_2)} = \\frac{(2+2\\rho^2)/(3(1-2\\rho^2))}{1/(1-2\\rho^2)} = \\frac{2(1+\\rho^2)}{3} $$\nA full sweep corresponds to $3$ mini-steps. To compare the two schemes, we compare their effective autocorrelation over one full sweep. For the systematic-scan chain, the lag-one autocorrelation for $f(X_2)$ is precisely $\\lambda(\\rho)=2\\rho^2$. For the random-scan chain, if we approximate its behavior over 3 steps as an AR(1) process, the effective autocorrelation would be $(\\rho_{\\mathrm{rand}}(1))^3 = (\\frac{2(1+\\rho^2)}{3})^3$.\nFor small $|\\rho|$, let $\\varepsilon = |\\rho|$:\n- Systematic scan autocorrelation: $2\\varepsilon^2$. This is of order $O(\\varepsilon^2)$.\n- Random scan effective autocorrelation: $(\\frac{2(1+\\varepsilon^2)}{3})^3 \\approx (\\frac{2}{3})^3 = \\frac{8}{27}$. This is a constant.\n\nThe asymptotic variance is an increasing function of the process's autocorrelation. Since for small $|\\rho|$, the autocorrelation of systematic scan ($2\\rho^2$) is much smaller than that of random scan ($\\approx 8/27$), the systematic-scan Gibbs sampler is expected to mix much faster and thus yield a significantly smaller asymptotic variance per full sweep. The random scan spends $2/3$ of its time on updates that do not change $X_2$, leading to high persistence, whereas the systematic scan updates all variables in every sweep, and the specific ordered interaction leads to faster convergence in this case.", "answer": "$$\\boxed{\\frac{1+2\\rho^2}{(1-2\\rho^2)^2}}$$", "id": "3358547"}, {"introduction": "Collapsed Gibbs sampling, which analytically integrates out some variables to improve efficiency, is a powerful technique. However, its power comes with pitfalls, as the validity of the resulting sampler can depend on the order of operations. This practice demonstrates a critical subtlety by analyzing a hierarchical model where one update order produces a valid sampler while another breaks stationarity, failing to converge to the correct target distribution [@problem_id:3358515].", "problem": "Consider the following linear-Gaussian hierarchical model for three real-valued variables $x$, $y$, and $z$:\n- The prior for $z$ is $z \\sim \\mathcal{N}(0,\\tau^{2})$.\n- The conditional distribution for $y$ given $z$ is $y \\mid z \\sim \\mathcal{N}(\\alpha z,\\sigma_{y}^{2})$.\n- The conditional distribution for $x$ given $y$ and $z$ is $x \\mid y,z \\sim \\mathcal{N}(\\beta y + \\gamma z,\\sigma_{x}^{2})$.\n\nThe joint distribution $\\pi(x,y,z)$ is thus well-defined and strictly positive for all $(x,y,z) \\in \\mathbb{R}^{3}$.\n\nA partially collapsed Gibbs sampler is constructed by replacing one full-conditional update with a collapsed update that integrates out a nuisance variable. Specifically, consider the collapsed update for $z$ based on $p(z \\mid x)$, obtained by marginalizing over $y$ from the joint $\\pi(x,y,z)$.\n\nTwo different composition orders of the partially collapsed Gibbs sampler are proposed:\n\n- Scheme A (update $y$ first): starting from $(x^{(t)},y^{(t)},z^{(t)})$, perform\n  1. Sample $y^{(t+1)} \\sim p(y \\mid z^{(t)})$,\n  2. Sample $z^{(t+1)} \\sim p(z \\mid x^{(t)})$,\n  3. Sample $x^{(t+1)} \\sim p(x \\mid y^{(t+1)}, z^{(t+1)})$.\n\n- Scheme B (update $z$ first): starting from $(x^{(t)},y^{(t)},z^{(t)})$, perform\n  1. Sample $z^{(t+1)} \\sim p(z \\mid x^{(t)})$,\n  2. Sample $y^{(t+1)} \\sim p(y \\mid z^{(t+1)})$,\n  3. Sample $x^{(t+1)} \\sim p(x \\mid y^{(t+1)}, z^{(t+1)})$.\n\nUsing only the foundational definitions of Gibbs sampling, Markov chain stationarity, and Gaussian conditioning, do the following:\n\n1. Derive the collapsed conditional $p(z \\mid x)$ in closed form by marginalizing out $y$ from the joint $\\pi(x,y,z)$. Express its mean and variance explicitly in terms of $(\\alpha,\\beta,\\gamma,\\tau^{2},\\sigma_{y}^{2},\\sigma_{x}^{2})$.\n\n2. Analyze the stationarity of Scheme A and Scheme B by explicitly evaluating the invariance condition for the composed Markov kernel acting on $\\pi(x,y,z)$. Explain precisely why one of the orders fails to preserve $\\pi(x,y,z)$ as the stationary distribution while the other maintains stationarity.\n\n3. For the correctly ordered scheme that maintains stationarity, compute the marginal variance of $x$ under the stationary distribution $\\pi(x,y,z)$ for the parameter values\n$$\n\\alpha = 0.7,\\quad \\beta = 1.2,\\quad \\gamma = -0.5,\\quad \\tau^{2} = 2.5,\\quad \\sigma_{y}^{2} = 1.3,\\quad \\sigma_{x}^{2} = 0.9.\n$$\nRound your final numeric answer for the marginal variance of $x$ to four significant figures. No physical units are required.", "solution": "### Solution\n\n**1. Derivation of the Collapsed Conditional $p(z \\mid x)$**\n\nTo derive $p(z \\mid x)$, we first need the joint distribution of $(x, z)$, which is obtained by marginalizing $y$ out of the full joint distribution $\\pi(x, y, z)$. The model can be written as a system of linear equations with Gaussian noise:\n$z = \\epsilon_z$, where $\\epsilon_z \\sim \\mathcal{N}(0, \\tau^2)$.\n$y = \\alpha z + \\epsilon_y$, where $\\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2)$.\n$x = \\beta y + \\gamma z + \\epsilon_x$, where $\\epsilon_x \\sim \\mathcal{N}(0, \\sigma_x^2)$.\nThe noise terms $\\epsilon_z$, $\\epsilon_y$, $\\epsilon_x$ are mutually independent.\n\nBy substituting the expression for $y$ into the expression for $x$, we can express $x$ in terms of $z$ and the independent noise terms:\n$$ x = \\beta(\\alpha z + \\epsilon_y) + \\gamma z + \\epsilon_x = (\\alpha\\beta + \\gamma)z + \\beta\\epsilon_y + \\epsilon_x $$\nSince $x$ and $z$ are linear combinations of Gaussian random variables, they are jointly Gaussian. We can now find the parameters of this joint distribution, $\\pi(x,z)$. The means are $E[z]=0$ and $E[x] = E[(\\alpha\\beta + \\gamma)z + \\beta\\epsilon_y + \\epsilon_x] = 0$.\n\nThe variances and covariance are:\n$Var(z) = E[z^2] = \\tau^2$.\n$Var(x) = Var((\\alpha\\beta + \\gamma)z + \\beta\\epsilon_y + \\epsilon_x)$. Due to the independence of $z$ (which is $\\epsilon_z$), $\\epsilon_y$, and $\\epsilon_x$:\n$$ Var(x) = (\\alpha\\beta + \\gamma)^2 Var(z) + \\beta^2 Var(\\epsilon_y) + Var(\\epsilon_x) = (\\alpha\\beta + \\gamma)^2 \\tau^2 + \\beta^2 \\sigma_y^2 + \\sigma_x^2 $$\n$Cov(x, z) = E[xz] = E[ ((\\alpha\\beta + \\gamma)z + \\beta\\epsilon_y + \\epsilon_x) z ] = E[(\\alpha\\beta + \\gamma)z^2 + \\beta\\epsilon_y z + \\epsilon_x z]$.\nUsing linearity of expectation and independence ($E[\\epsilon_y z] = E[\\epsilon_y]E[z] = 0$, $E[\\epsilon_x z] = 0$):\n$$ Cov(x, z) = (\\alpha\\beta + \\gamma)E[z^2] = (\\alpha\\beta + \\gamma)\\tau^2 $$\nSo, the joint distribution of $(z, x)$ is:\n$$ \\begin{pmatrix} z \\\\ x \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\tau^2 & (\\alpha\\beta + \\gamma)\\tau^2 \\\\ (\\alpha\\beta + \\gamma)\\tau^2 & (\\alpha\\beta + \\gamma)^2 \\tau^2 + \\beta^2 \\sigma_y^2 + \\sigma_x^2 \\end{pmatrix} \\right) $$\nUsing the standard formula for conditional Gaussian distributions, the distribution of $z$ given $x$, denoted $p(z \\mid x)$, is a Gaussian $\\mathcal{N}(\\mu_{z|x}, \\sigma_{z|x}^2)$ with:\n$$ \\mu_{z|x} = E[z \\mid x] = E[z] + \\frac{Cov(z,x)}{Var(x)}(x - E[x]) = \\frac{(\\alpha\\beta + \\gamma)\\tau^2}{(\\alpha\\beta + \\gamma)^2 \\tau^2 + \\beta^2 \\sigma_y^2 + \\sigma_x^2} x $$\n$$ \\sigma_{z|x}^2 = Var(z \\mid x) = Var(z) - \\frac{Cov(z,x)^2}{Var(x)} = \\tau^2 - \\frac{\\left((\\alpha\\beta + \\gamma)\\tau^2\\right)^2}{(\\alpha\\beta + \\gamma)^2 \\tau^2 + \\beta^2 \\sigma_y^2 + \\sigma_x^2} $$\n\n**2. Analysis of Stationarity**\n\nA Markov chain with transition kernel $K(s' \\mid s)$ has a stationary distribution $\\pi(s)$ if the invariance condition holds: $\\int K(s' \\mid s) \\pi(s) ds = \\pi(s')$. For Gibbs sampling, the overall kernel is a composition of individual update kernels.\n\n**Scheme B Analysis:** The transition from state $s^{(t)}=(x^{(t)},y^{(t)},z^{(t)})$ to $s^{(t+1)}=(x^{(t+1)},y^{(t+1)},z^{(t+1)})$ is defined by a sequence of three draws. The combined transition probability (the kernel) is:\n$$ K_B(x',y',z' \\mid x,y,z) = p(x' \\mid y',z') p(y' \\mid z') p(z' \\mid x) $$\nwhere we have relabeled the next state variables with primes for clarity. The old state is $(x,y,z)$ and the new is $(x',y',z')$.\n\nTo check for invariance, we evaluate the integral:\n$$ \\int \\pi(x,y,z) K_B(x',y',z' \\mid x,y,z) \\,dx\\,dy\\,dz $$\nSubstituting the kernel and the joint distribution $\\pi(x,y,z) = p(x|y,z)p(y|z)p(z)$:\n$$ \\int p(x|y,z)p(y|z)p(z) \\cdot p(x' \\mid y',z') p(y' \\mid z') p(z' \\mid x) \\,dx\\,dy\\,dz $$\nThe terms $p(x' \\mid y',z')$ and $p(y' \\mid z')$ can be moved outside the integral:\n$$ p(x' \\mid y',z') p(y' \\mid z') \\int p(z' \\mid x) \\, [p(x|y,z)p(y|z)p(z)] \\,dx\\,dy\\,dz $$\nThe term in the square brackets is $\\pi(x,y,z)$. The integral becomes:\n$$ \\int p(z' \\mid x) \\pi(x,y,z) \\,dx\\,dy\\,dz = \\int_x p(z' \\mid x) \\left( \\int_y \\int_z \\pi(x,y,z) \\,dz\\,dy \\right) \\,dx = \\int_x p(z' \\mid x) \\pi(x) \\,dx $$\nwhere $\\pi(x)$ is the marginal distribution of $x$. By the definition of conditional probability, $p(z' \\mid x) = \\frac{\\pi(x,z')}{\\pi(x)}$.\nThe integral is therefore $\\int_x \\frac{\\pi(x,z')}{\\pi(x)} \\pi(x) \\,dx = \\int_x \\pi(x,z') \\,dx = \\pi(z')$, the marginal distribution of $z'$.\nSo, the full expression for the next-state distribution is:\n$$ p(x' \\mid y',z') p(y' \\mid z') \\pi(z') $$\nFrom the model definition, $\\pi(z')$ is identical to the prior $p(z')$. Therefore, this expression is exactly the definition of the joint distribution $\\pi(x',y',z') = p(x'|y',z')p(y'|z')p(z')$.\nThus, Scheme B preserves $\\pi(x,y,z)$ as its stationary distribution. It is a valid sampler. This structure is valid because it represents an ancestral sampling scheme for the joint $(x',y',z')$ where the root node $z'$ is drawn from a collapsed conditional distribution $p(z'|x)$ that itself preserves the marginal $\\pi(x,z)$.\n\n**Scheme A Analysis:** The transition kernel for this scheme is:\n$$ K_A(x',y',z' \\mid x,y,z) = p(x' \\mid y',z') p(z' \\mid x) p(y' \\mid z) $$\nThe critical difference is that the new draw $y'$ depends on the old state variable $z$, not the new one $z'$.\nLet's check the invariance condition:\n$$ \\int \\pi(x,y,z) K_A(x',y',z' \\mid x,y,z) \\,dx\\,dy\\,dz $$\n$$ = p(x' \\mid y',z') \\int p(z' \\mid x) p(y' \\mid z) \\pi(x,y,z) \\,dx\\,dy\\,dz $$\nThe integral term does not simplify to $\\pi(y', z')$ as needed. Let's analyze it:\n$$ \\int_x \\int_y \\int_z p(z' \\mid x) p(y' \\mid z) p(x|y,z) p(y|z) p(z) \\,dz\\,dy\\,dx $$\nRearranging the integration gives:\n$$ \\int_x p(z'|x) \\left( \\int_z p(y'|z) p(z) \\left( \\int_y p(x|y,z)p(y|z) \\,dy \\right) \\,dz \\right) \\,dx $$\nThe innermost integral is $\\int_y p(x,y|z) \\,dy = p(x|z)$, where $p(x|z) = \\int p(x|y,z)p(y|z) \\text{d}y$. So the expression becomes:\n$$ \\int_x p(z'|x) \\left( \\int_z p(y'|z) p(x|z) p(z) \\,dz \\right) \\,dx $$\nThe term $\\int_z p(y'|z) p(x,z) \\,dz$ does not simplify to a convenient form like $\\pi(x,y')$. It represents an expectation with respect to $\\pi(z)$ and depends on both $x$ and $y'$. Because this integral does not evaluate to $\\pi(y',z')$, the overall expression does not reduce to $\\pi(x',y',z')$.\nThe structural flaw in Scheme A is the broken chain of conditioning. The draw of $y'$ uses \"stale\" information ($z=z^{(t)}$) while the subsequent draw of $x'$ uses \"updated\" information ($z'=z^{(t+1)}$). This failure to propagate the most recent information at each step prevents the composed kernel from preserving the target stationary distribution. Therefore, Scheme A is not a valid Gibbs sampler for the target $\\pi(x,y,z)$.\n\n**3. Marginal Variance of $x$**\n\nThe correctly ordered scheme that maintains stationarity is Scheme B. The problem asks for the marginal variance of $x$ under the stationary distribution $\\pi(x,y,z)$. This quantity, $Var(x)$, is an attribute of the target distribution itself, not the sampler dynamics. We have already derived the expression for $Var(x)$ in Part 1.\n$$ Var(x) = (\\alpha\\beta + \\gamma)^2 \\tau^2 + \\beta^2 \\sigma_y^2 + \\sigma_x^2 $$\nWe substitute the given parameter values:\n$\\alpha = 0.7$\n$\\beta = 1.2$\n$\\gamma = -0.5$\n$\\tau^{2} = 2.5$\n$\\sigma_{y}^{2} = 1.3$\n$\\sigma_{x}^{2} = 0.9$\n\nFirst, calculate the term $\\alpha\\beta + \\gamma$:\n$$ \\alpha\\beta + \\gamma = (0.7)(1.2) + (-0.5) = 0.84 - 0.5 = 0.34 $$\nNow, substitute this and the other values into the variance formula:\n$$ Var(x) = (0.34)^2 (2.5) + (1.2)^2 (1.3) + 0.9 $$\n$$ Var(x) = (0.1156)(2.5) + (1.44)(1.3) + 0.9 $$\n$$ Var(x) = 0.289 + 1.872 + 0.9 $$\n$$ Var(x) = 3.061 $$\nThe question asks to round the result to four significant figures. The calculated value $3.061$ already has four significant figures.", "answer": "$$\\boxed{3.061}$$", "id": "3358515"}]}