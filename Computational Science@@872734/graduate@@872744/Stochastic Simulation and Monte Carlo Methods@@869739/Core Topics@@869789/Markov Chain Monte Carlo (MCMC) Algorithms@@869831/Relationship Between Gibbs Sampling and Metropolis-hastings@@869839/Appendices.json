{"hands_on_practices": [{"introduction": "To understand the deep connection between different MCMC methods, we must first master their most general form: the Metropolis-Hastings (MH) algorithm. This exercise guides you through deriving the famous MH acceptance probability from the fundamental principle of detailed balance, which ensures that our sampler will eventually converge to the correct target distribution. By working through a concrete example with an asymmetric proposal, you will gain a practical understanding of how the proposal mechanism directly influences the acceptance rule, a crucial insight for designing efficient samplers [@problem_id:3336061].", "problem": "Consider a Markov chain Monte Carlo transition of the Metropolis–Hastings (MH) type designed to leave a target distribution $\\pi(x)$ invariant on the real line. The proposal mechanism is characterized by a conditional density $q(y \\mid x)$ that may be asymmetric. The acceptance function $a(x,y)$, which satisfies $0 \\leq a(x,y) \\leq 1$, is to be chosen so that the chain is reversible with respect to $\\pi$ and hence has $\\pi$ as its stationary distribution. Starting from the principle of detailed balance for reversible Markov kernels, derive the explicit Hastings acceptance ratio $r(x,y)$ in terms of $\\pi$ and $q$, and thereby obtain the functional form of the acceptance probability $a(x,y)$.\n\nThen, instantiate your derivation for the following scientifically realistic setting:\n- The unnormalized target density is given by $\\tilde{\\pi}(x) = \\exp\\left(-\\frac{1}{4}x^{4} + \\frac{1}{2}x^{2}\\right)$, which is log-concave in the tails and integrable on $\\mathbb{R}$. The normalized $\\pi(x)$ is proportional to $\\tilde{\\pi}(x)$.\n- The proposal is an additive drifted Gaussian random walk, $q(y \\mid x) = \\mathcal{N}(y; x+\\delta, \\sigma^{2})$ with fixed drift $\\delta \\in \\mathbb{R}$ and scale $\\sigma > 0$. Here $\\mathcal{N}(y; m, s^{2})$ denotes the normal density in $y$ with mean $m$ and variance $s^{2}$.\n\nProvide the closed-form expression for the Hastings ratio $r(x,y)$ for this specific $\\tilde{\\pi}$ and $q$, expressed only in terms of $x$, $y$, $\\delta$, and $\\sigma$. Finally, explain from first principles why symmetry of $q$ (for example, $\\delta=0$ in the above proposal) simplifies the MH acceptance rule, and briefly relate this simplification to the special case of Gibbs sampling, where proposals are drawn from full conditional distributions.\n\nYour final answer must be a single closed-form analytic expression for $r(x,y)$. No rounding is required, and no units are involved.", "solution": "The core principle underpinning the Metropolis–Hastings (MH) algorithm is the concept of detailed balance, which is a sufficient condition for a Markov chain's transition kernel to leave a target distribution $\\pi(x)$ invariant. For a discrete-time Markov process on a continuous state space $\\mathcal{X}$, with transition kernel density $K(y \\mid x)$, the detailed balance condition with respect to a stationary distribution $\\pi(x)$ is given by:\n$$\n\\pi(x) K(y \\mid x) = \\pi(y) K(x \\mid y) \\quad \\forall x,y \\in \\mathcal{X}\n$$\nThe Metropolis–Hastings algorithm constructs the transition kernel $K(y \\mid x)$ from two components: a proposal distribution a priori designated by the conditional density $q(y \\mid x)$, and an acceptance probability $a(x,y)$. For $x \\neq y$, the density of making a transition from $x$ to $y$ is the product of the density of proposing $y$ and the probability of accepting it:\n$$\nK(y \\mid x) = q(y \\mid x) a(x, y)\n$$\nSubstituting this form into the detailed balance equation yields:\n$$\n\\pi(x) q(y \\mid x) a(x, y) = \\pi(y) q(x \\mid y) a(y, x)\n$$\nwhere $a(y,x)$ is the probability of accepting a move from $y$ to $x$. This equation can be rearranged to isolate the ratio of the acceptance probabilities:\n$$\n\\frac{a(x, y)}{a(y, x)} = \\frac{\\pi(y) q(x \\mid y)}{\\pi(x) q(y \\mid x)}\n$$\nWe define the quantity on the right-hand side as the Hastings ratio, $r(x,y)$:\n$$\nr(x, y) \\equiv \\frac{\\pi(y) q(x \\mid y)}{\\pi(x) q(y \\mid x)}\n$$\nTo satisfy the detailed balance condition while maximizing the acceptance rate, we select the functional form for $a(x,y)$ as proposed by Hastings:\n$$\na(x, y) = \\min(1, r(x, y))\n$$\nThis choice satisfies the constraint $0 \\leq a(x, y) \\leq 1$ and fulfills the ratio condition. This completes the general derivation of the acceptance probability.\n\nWe now instantiate this for the specific problem setting. The unnormalized target density is $\\tilde{\\pi}(x) = \\exp(-\\frac{1}{4}x^4 + \\frac{1}{2}x^2)$, and the proposal density is a drifted Gaussian, $q(y \\mid x) = \\mathcal{N}(y; x+\\delta, \\sigma^2)$. The Hastings ratio $r(x,y)$ can be computed using the unnormalized density $\\tilde{\\pi}(x)$, because any normalization constant will cancel in the ratio $\\pi(y)/\\pi(x)$.\n$$\nr(x, y) = \\frac{\\tilde{\\pi}(y)}{\\tilde{\\pi}(x)} \\frac{q(x \\mid y)}{q(y \\mid x)}\n$$\nFirst, the ratio of the target densities is:\n$$\n\\frac{\\tilde{\\pi}(y)}{\\tilde{\\pi}(x)} = \\frac{\\exp(-\\frac{1}{4}y^4 + \\frac{1}{2}y^2)}{\\exp(-\\frac{1}{4}x^4 + \\frac{1}{2}x^2)} = \\exp\\left(-\\frac{1}{4}(y^4 - x^4) + \\frac{1}{2}(y^2 - x^2)\\right)\n$$\nNext, the ratio of the proposal densities $q(y \\mid x) \\propto \\exp\\left(-\\frac{(y - x - \\delta)^2}{2\\sigma^2}\\right)$ and $q(x \\mid y) \\propto \\exp\\left(-\\frac{(x - y - \\delta)^2}{2\\sigma^2}\\right)$ is:\n$$\n\\frac{q(x \\mid y)}{q(y \\mid x)} = \\exp\\left(\\frac{1}{2\\sigma^2} \\left[ (y - x - \\delta)^2 - (x - y - \\delta)^2 \\right] \\right)\n$$\nThe term in the exponent simplifies to:\n$$\n(y - x - \\delta)^2 - (x - y - \\delta)^2 = ((y-x)-\\delta)^2 - (-(y-x)-\\delta)^2 = ((y-x)-\\delta)^2 - ((y-x)+\\delta)^2 = -4\\delta(y-x) = 4\\delta(x-y)\n$$\nSo, the proposal density ratio is:\n$$\n\\frac{q(x \\mid y)}{q(y \\mid x)} = \\exp\\left(\\frac{2\\delta(x-y)}{\\sigma^2}\\right)\n$$\nCombining the target and proposal ratios gives the full Hastings ratio $r(x,y)$:\n$$\nr(x, y) = \\exp\\left(-\\frac{1}{4}(y^4 - x^4) + \\frac{1}{2}(y^2 - x^2) + \\frac{2\\delta(x-y)}{\\sigma^2}\\right)\n$$\nIf the proposal distribution $q$ is symmetric, i.e., $q(y \\mid x) = q(x \\mid y)$ (which corresponds to $\\delta = 0$), the ratio $q(x \\mid y) / q(y \\mid x)$ becomes 1. The Hastings ratio simplifies to $r(x,y) = \\pi(y)/\\pi(x)$, which is the acceptance rule of the original Metropolis algorithm.\n\nGibbs sampling for a multivariate distribution $\\pi(\\mathbf{x})$ updates a component $x_i$ by drawing from its full conditional distribution, $\\pi(x_i \\mid \\mathbf{x}_{-i})$. If we frame this as an MH step, the proposal is $q(\\mathbf{x}' \\mid \\mathbf{x}) = \\pi(x_i' \\mid \\mathbf{x}_{-i})$. The reverse proposal is $q(\\mathbf{x} \\mid \\mathbf{x}') = \\pi(x_i \\mid \\mathbf{x}_{-i}')$. Since only $x_i$ changes, $\\mathbf{x}_{-i}' = \\mathbf{x}_{-i}$. The Hastings ratio is:\n$$\nr(\\mathbf{x}, \\mathbf{x}') = \\frac{\\pi(\\mathbf{x}')}{\\pi(\\mathbf{x})} \\frac{q(\\mathbf{x} \\mid \\mathbf{x}')}{q(\\mathbf{x}' \\mid \\mathbf{x})} = \\frac{\\pi(x_i' \\mid \\mathbf{x}_{-i})\\pi(\\mathbf{x}_{-i})}{\\pi(x_i \\mid \\mathbf{x}_{-i})\\pi(\\mathbf{x}_{-i})} \\frac{\\pi(x_i \\mid \\mathbf{x}_{-i})}{\\pi(x_i' \\mid \\mathbf{x}_{-i})} = 1\n$$\nThe acceptance probability is $\\min(1, 1) = 1$. Gibbs sampling is therefore an MH algorithm with an acceptance probability of 1.", "answer": "$$\n\\boxed{\\exp\\left(-\\frac{1}{4}(y^{4} - x^{4}) + \\frac{1}{2}(y^{2} - x^{2}) + \\frac{2\\delta(x-y)}{\\sigma^{2}}\\right)}\n$$", "id": "3336061"}, {"introduction": "With the general Metropolis-Hastings framework established, we can now formally prove that Gibbs sampling is one of its most elegant special cases. This practice uses the classic example of a bivariate normal distribution to demonstrate a profound principle: when a proposal is drawn from a variable's true full conditional distribution, the MH acceptance probability becomes exactly one. This exercise makes the theoretical link between Gibbs and Metropolis-Hastings explicit, showing that the \"perfect\" proposal of Gibbs sampling leads to an algorithm with no rejections [@problem_id:3336066].", "problem": "Let $\\pi(x_{1}, x_{2})$ be a target probability density on $\\mathbb{R}^{2}$ given by a bivariate normal distribution with mean vector $0$ and covariance matrix $\\Sigma$, where\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_{11} & \\sigma_{12} \\\\ \\sigma_{12} & \\sigma_{22} \\end{pmatrix},\n$$\nwith $\\sigma_{11} > 0$, $\\sigma_{22} > 0$, and $\\sigma_{11}\\sigma_{22} - \\sigma_{12}^{2} > 0$. Consider the corresponding Markov chain Monte Carlo procedure that alternates coordinate-wise updates using the full conditionals (Gibbs sampling). Starting from the fundamental definition of a multivariate normal density and the properties of quadratic forms, derive the full conditional distributions $\\pi(x_{1} \\mid x_{2})$ and $\\pi(x_{2} \\mid x_{1})$ by explicitly completing the square, and identify their Gaussian means and variances as functions of $\\sigma_{11}$, $\\sigma_{22}$, $\\sigma_{12}$, $x_{1}$, and $x_{2}$.\n\nThen, interpret a single-coordinate Gibbs update as a Metropolis-Hastings proposal within the Metropolis-Hastings (MH) algorithm framework, where the proposal for $x_{1}$ given $x_{2}$ is exactly $\\pi(\\cdot \\mid x_{2})$, and similarly for $x_{2}$ given $x_{1}$. Using only the definitions of the MH acceptance probability and conditional densities, derive the exact acceptance probability for such a proposal in terms of $\\pi$ and the proposal density, and simplify it to a numerical value that holds for all states.\n\nReport your answer as a single row matrix containing, in this order:\n- the mean of $\\pi(x_{1} \\mid x_{2})$,\n- the variance of $\\pi(x_{1} \\mid x_{2})$,\n- the mean of $\\pi(x_{2} \\mid x_{1})$,\n- the variance of $\\pi(x_{2} \\mid x_{1})$,\n- the MH acceptance probability for either coordinate update.\n\nNo rounding is required. The final answer must be a single closed-form analytic expression.", "solution": "The probability density function (PDF) for a bivariate normal distribution with mean zero is proportional to the exponential of a quadratic form:\n$$ \\pi(x_1, x_2) \\propto \\exp\\left(-\\frac{1}{2}\\begin{pmatrix} x_1 & x_2 \\end{pmatrix} \\Sigma^{-1} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\\right) $$\nThe inverse of the covariance matrix is $\\Sigma^{-1} = \\frac{1}{\\det(\\Sigma)} \\begin{pmatrix} \\sigma_{22} & -\\sigma_{12} \\\\ -\\sigma_{12} & \\sigma_{11} \\end{pmatrix}$. The quadratic form is:\n$$ \\mathbf{x}^T \\Sigma^{-1} \\mathbf{x} = \\frac{1}{\\det(\\Sigma)} (\\sigma_{22}x_1^2 - 2\\sigma_{12}x_1x_2 + \\sigma_{11}x_2^2) $$\nTo find the full conditional distribution $\\pi(x_1 | x_2)$, we use $\\pi(x_1 | x_2) \\propto \\pi(x_1, x_2)$ and treat all terms involving only $x_2$ as constants. The exponent, viewed as a function of $x_1$, contains the terms $\\sigma_{22}x_1^2 - 2\\sigma_{12}x_2 x_1$. Completing the square for $x_1$:\n$$ \\sigma_{22}x_1^2 - 2\\sigma_{12}x_2 x_1 = \\sigma_{22}\\left(x_1^2 - 2\\frac{\\sigma_{12}}{\\sigma_{22}}x_2 x_1\\right) = \\sigma_{22}\\left(x_1 - \\frac{\\sigma_{12}}{\\sigma_{22}}x_2\\right)^2 - \\frac{\\sigma_{12}^2}{\\sigma_{22}}x_2^2 $$\nThe last term depends only on $x_2$ and is absorbed into the proportionality constant. Thus, the conditional density is:\n$$ \\pi(x_1 | x_2) \\propto \\exp\\left( -\\frac{\\sigma_{22}}{2\\det(\\Sigma)} \\left(x_1 - \\frac{\\sigma_{12}}{\\sigma_{22}}x_2\\right)^2 \\right) $$\nThis is the kernel of a normal distribution for $x_1$. By comparing with the generic form $\\exp\\left(-\\frac{(z-\\mu_z)^2}{2\\sigma_z^2}\\right)$, we identify the mean and variance:\n- $E[x_1 | x_2] = \\frac{\\sigma_{12}}{\\sigma_{22}}x_2$\n- $\\text{Var}(x_1 | x_2) = \\frac{\\det(\\Sigma)}{\\sigma_{22}} = \\frac{\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2}{\\sigma_{22}} = \\sigma_{11} - \\frac{\\sigma_{12}^2}{\\sigma_{22}}$\n\nBy symmetry, swapping indices $1 \\leftrightarrow 2$, we get the parameters for $\\pi(x_2 | x_1)$:\n- $E[x_2 | x_1] = \\frac{\\sigma_{12}}{\\sigma_{11}}x_1$\n- $\\text{Var}(x_2 | x_1) = \\sigma_{22} - \\frac{\\sigma_{12}^2}{\\sigma_{11}}$\n\nNow, we interpret a Gibbs update on $x_1$ as a Metropolis-Hastings (MH) step. The state transitions from $\\mathbf{x}=(x_1, x_2)$ to $\\mathbf{x}'=(x_1', x_2)$.\nThe proposal density is the full conditional: $q(\\mathbf{x}' | \\mathbf{x}) = \\pi(x_1' | x_2)$.\nThe reverse proposal density is: $q(\\mathbf{x} | \\mathbf{x}') = \\pi(x_1 | x_2')$. Since $x_2'=x_2$, this is $q(\\mathbf{x} | \\mathbf{x}') = \\pi(x_1 | x_2)$.\nThe MH acceptance probability is $\\alpha = \\min\\left(1, R\\right)$, where the ratio $R$ is:\n$$ R = \\frac{\\pi(\\mathbf{x}') q(\\mathbf{x}|\\mathbf{x}')}{\\pi(\\mathbf{x}) q(\\mathbf{x}'|\\mathbf{x})} = \\frac{\\pi(x_1', x_2) \\pi(x_1 | x_2)}{\\pi(x_1, x_2) \\pi(x_1' | x_2)} $$\nUsing the identity $\\pi(A,B) = \\pi(A|B)\\pi(B)$, we have $\\pi(x_1, x_2) = \\pi(x_1 | x_2) \\pi(x_2)$ and $\\pi(x_1', x_2) = \\pi(x_1' | x_2) \\pi(x_2)$. Substituting these into the ratio:\n$$ R = \\frac{\\left(\\pi(x_1' | x_2) \\pi(x_2)\\right) \\cdot \\pi(x_1 | x_2)}{\\left(\\pi(x_1 | x_2) \\pi(x_2)\\right) \\cdot \\pi(x_1' | x_2)} = 1 $$\nAll terms cancel. Therefore, the acceptance probability is $\\alpha = \\min(1, 1) = 1$. The same result holds for updating $x_2$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sigma_{12}}{\\sigma_{22}} x_2 & \\sigma_{11} - \\frac{\\sigma_{12}^{2}}{\\sigma_{22}} & \\frac{\\sigma_{12}}{\\sigma_{11}} x_1 & \\sigma_{22} - \\frac{\\sigma_{12}^{2}}{\\sigma_{11}} & 1\n\\end{pmatrix}\n}\n$$", "id": "3336066"}, {"introduction": "In many realistic Bayesian models, some parameters have intractable full conditional distributions, making a pure Gibbs sampler impossible. This is where understanding Gibbs as a special case of Metropolis-Hastings becomes a powerful tool for practical problem-solving, allowing us to build hybrid samplers. This exercise tackles a Bayesian logistic regression model, a workhorse of modern statistics, where you will design a \"Metropolis-within-Gibbs\" algorithm by combining a direct Gibbs step for the tractable variance parameter with an MH step for the intractable regression coefficients [@problem_id:3336126].", "problem": "Consider a binary response model with observations $\\{(y_{i}, x_{i})\\}_{i=1}^{n}$, where $y_{i} \\in \\{0,1\\}$ and $x_{i} \\in \\mathbb{R}^{p}$. The likelihood is given by a logistic regression:\n- $y_{i} \\mid \\beta \\sim \\mathrm{Bernoulli}(\\sigma(x_{i}^{\\top}\\beta))$ independently for $i=1,\\dots,n$, where $\\sigma(t) = \\frac{1}{1+\\exp(-t)}$.\n\nAssume a Gaussian prior with a global variance hyperparameter:\n- $\\beta \\mid \\tau^{2} \\sim \\mathcal{N}\\!\\left(0, \\tau^{2}\\Sigma_{0}\\right)$ with known positive definite matrix $\\Sigma_{0} \\in \\mathbb{R}^{p \\times p}$ and unknown $\\tau^{2} > 0$.\n- $\\tau^{2} \\sim \\mathrm{Inverse\\text{-}Gamma}(a_{0}, b_{0})$ with density $p(\\tau^{2}) = \\dfrac{b_{0}^{a_{0}}}{\\Gamma(a_{0})} (\\tau^{2})^{-(a_{0}+1)} \\exp\\!\\left(-\\dfrac{b_{0}}{\\tau^{2}}\\right)$ for $a_{0} > 0$ and $b_{0} > 0$.\n\nTasks:\n- Identify which full conditional distributions in this model are available in closed form and which are not.\n- Design a Metropolis-within-Gibbs sampler that alternates between updates for $\\tau^{2}$ and $\\beta$.\n- For the $\\beta$-update, suppose you employ a Gaussian random-walk proposal $\\beta^{\\star} \\sim \\mathcal{N}(\\beta, \\Sigma_{\\mathrm{prop}})$ with a fixed symmetric positive definite matrix $\\Sigma_{\\mathrm{prop}} \\in \\mathbb{R}^{p \\times p}$. Derive a closed-form expression for the Metropolis-Hastings acceptance probability $\\alpha(\\beta \\to \\beta^{\\star} \\mid \\tau^{2}, X, y)$ as a function of $(X, y, \\beta, \\beta^{\\star}, \\tau^{2}, \\Sigma_{0})$, where $X$ is the $n \\times p$ design matrix with rows $x_{i}^{\\top}$.\n\nProvide your final answer as a single closed-form analytical expression for the acceptance probability. Do not approximate or round your answer.", "solution": "The joint posterior distribution is $p(\\beta, \\tau^2 \\mid y, X) \\propto p(y \\mid \\beta, X) p(\\beta \\mid \\tau^2) p(\\tau^2)$.\n\n**1. Full Conditional Distributions**\n\n- **Full conditional for $\\beta$**: $p(\\beta \\mid \\tau^2, y, X) \\propto p(y \\mid \\beta, X) p(\\beta \\mid \\tau^2)$.\n  $$p(\\beta \\mid \\tau^2, y, X) \\propto \\left( \\prod_{i=1}^{n} \\sigma(x_{i}^{\\top}\\beta)^{y_{i}} (1-\\sigma(x_{i}^{\\top}\\beta))^{1-y_{i}} \\right) \\exp\\left(-\\frac{1}{2\\tau^2}\\beta^\\top\\Sigma_0^{-1}\\beta\\right)$$\n  The product of the logistic likelihood and the Gaussian prior does not result in a standard distribution. Thus, the full conditional for $\\beta$ is **not available in closed form**.\n\n- **Full conditional for $\\tau^2$**: $p(\\tau^2 \\mid \\beta, y, X) \\propto p(\\beta \\mid \\tau^2) p(\\tau^2)$, as the likelihood is independent of $\\tau^2$.\n  $$p(\\beta \\mid \\tau^2) \\propto (\\tau^2)^{-p/2} \\exp\\left(-\\frac{\\beta^\\top\\Sigma_0^{-1}\\beta}{2\\tau^2}\\right)$$\n  $$p(\\tau^2) \\propto (\\tau^2)^{-(a_0+1)} \\exp\\left(-\\frac{b_0}{\\tau^2}\\right)$$\n  Multiplying these gives the kernel of the full conditional for $\\tau^2$:\n  $$p(\\tau^2 \\mid \\beta) \\propto (\\tau^2)^{-(a_0+p/2+1)} \\exp\\left(-\\frac{1}{\\tau^2}\\left(b_0 + \\frac{1}{2}\\beta^\\top\\Sigma_0^{-1}\\beta\\right)\\right)$$\n  This is the kernel of an Inverse-Gamma distribution. The full conditional for $\\tau^2$ **is available in closed form**:\n  $$\\tau^2 \\mid \\beta, y, X \\sim \\mathrm{Inverse\\text{-}Gamma}\\left(a_0 + \\frac{p}{2},\\ b_0 + \\frac{1}{2}\\beta^\\top\\Sigma_0^{-1}\\beta\\right)$$\n\n**2. Metropolis-within-Gibbs Sampler Design**\n\nA hybrid sampler alternates between a Gibbs step for $\\tau^2$ and a Metropolis-Hastings step for $\\beta$.\n1.  **Gibbs Step**: Sample $(\\tau^2)^{(k+1)} \\sim \\mathrm{Inverse\\text{-}Gamma}\\left(a_0 + p/2, b_0 + \\frac{1}{2}(\\beta^{(k)})^\\top\\Sigma_0^{-1}\\beta^{(k)}\\right)$.\n2.  **MH Step**:\n    a. Propose $\\beta^\\star \\sim \\mathcal{N}\\left(\\beta^{(k)}, \\Sigma_{\\mathrm{prop}}\\right)$.\n    b. Compute the acceptance probability $\\alpha = \\alpha(\\beta^{(k)} \\to \\beta^\\star)$.\n    c. Accept $\\beta^{(k+1)} = \\beta^\\star$ with probability $\\alpha$; otherwise, set $\\beta^{(k+1)} = \\beta^{(k)}$.\n\n**3. Derivation of the Acceptance Probability**\n\nThe acceptance probability is $\\alpha = \\min\\left(1, R\\right)$, where $R$ is the Hastings ratio. The proposal distribution is a symmetric Gaussian random walk, so the proposal density ratio $q(\\beta \\mid \\beta^\\star) / q(\\beta^\\star \\mid \\beta)$ is 1. The ratio $R$ simplifies to the ratio of the target conditional densities:\n$$R = \\frac{p(\\beta^\\star \\mid \\tau^2, y, X)}{p(\\beta \\mid \\tau^2, y, X)} = \\frac{p(y \\mid \\beta^\\star, X) p(\\beta^\\star \\mid \\tau^2)}{p(y \\mid \\beta, X) p(\\beta \\mid \\tau^2)}$$\nThe likelihood ratio is:\n$$\\frac{p(y \\mid \\beta^\\star, X)}{p(y \\mid \\beta, X)} = \\prod_{i=1}^{n} \\frac{\\sigma(x_i^\\top\\beta^\\star)^{y_i}(1-\\sigma(x_i^\\top\\beta^\\star))^{1-y_i}}{\\sigma(x_i^\\top\\beta)^{y_i}(1-\\sigma(x_i^\\top\\beta))^{1-y_i}} = \\exp\\left(y^{\\top}X(\\beta^{\\star} - \\beta)\\right) \\prod_{i=1}^{n}\\frac{1+\\exp(x_{i}^{\\top}\\beta)}{1+\\exp(x_{i}^{\\top}\\beta^{\\star})}$$\nThe prior ratio is:\n$$\\frac{p(\\beta^\\star \\mid \\tau^2)}{p(\\beta \\mid \\tau^2)} = \\frac{\\exp\\left(-\\frac{1}{2\\tau^2}(\\beta^\\star)^\\top\\Sigma_0^{-1}\\beta^\\star\\right)}{\\exp\\left(-\\frac{1}{2\\tau^2}\\beta^\\top\\Sigma_0^{-1}\\beta\\right)} = \\exp\\left(\\frac{1}{2\\tau^2}\\left(\\beta^\\top\\Sigma_0^{-1}\\beta - (\\beta^\\star)^\\top\\Sigma_0^{-1}\\beta^\\star\\right)\\right)$$\nCombining these gives the full ratio $R$:\n$$R = \\exp\\left(y^{\\top}X(\\beta^{\\star} - \\beta) + \\frac{1}{2\\tau^2}\\left(\\beta^{\\top}\\Sigma_{0}^{-1}\\beta - (\\beta^{\\star})^{\\top}\\Sigma_{0}^{-1}\\beta^{\\star}\\right)\\right) \\prod_{i=1}^{n}\\frac{1+\\exp(x_{i}^{\\top}\\beta)}{1+\\exp(x_{i}^{\\top}\\beta^{\\star})}$$\nThe acceptance probability is $\\alpha = \\min(1, R)$.", "answer": "$$\\boxed{\\min\\left\\{1, \\exp\\left(y^{\\top}X(\\beta^{\\star} - \\beta) + \\frac{1}{2\\tau^2}\\left(\\beta^{\\top}\\Sigma_{0}^{-1}\\beta - (\\beta^{\\star})^{\\top}\\Sigma_{0}^{-1}\\beta^{\\star}\\right)\\right) \\prod_{i=1}^{n}\\frac{1+\\exp(x_{i}^{\\top}\\beta)}{1+\\exp(x_{i}^{\\top}\\beta^{\\star})}\\right\\}}$$", "id": "3336126"}]}