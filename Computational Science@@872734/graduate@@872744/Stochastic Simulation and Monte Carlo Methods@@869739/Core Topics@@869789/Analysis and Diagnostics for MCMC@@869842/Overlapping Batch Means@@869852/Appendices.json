{"hands_on_practices": [{"introduction": "Understanding the statistical properties of an estimator is the first step toward its effective application. This exercise delves into the finite-sample behavior of the Overlapping Batch Means (OBM) estimator under the idealized case of independent and identically distributed (IID) data. By deriving the exact expectation of the OBM variance estimator, you will uncover a subtle but important source of bias and discover the analytical correction factor required to make it unbiased for this foundational scenario [@problem_id:3326195].", "problem": "Consider a stationary and independent and identically distributed (IID) sequence $\\{X_{t}\\}_{t=1}^{n}$ with $\\mathbb{E}[X_{t}]=\\mu$ and $\\operatorname{Var}(X_{t})=\\sigma^{2}\\in(0,\\infty)$. Let the sample mean be $\\bar{X}_{n}=(1/n)\\sum_{t=1}^{n}X_{t}$. For a chosen batch length $b$ satisfying $1\\leq b\\leq n$, define $m=n-b+1$ overlapping batches and their batch means by\n$$\nY_{j}=\\frac{1}{b}\\sum_{t=j}^{j+b-1}X_{t},\\quad j=1,2,\\dots,m.\n$$\nThe overlapping batch means (OBM) estimator of the time-average (long-run) variance (which equals $\\sigma^{2}$ under IID sampling) is defined as\n$$\n\\hat{\\sigma}^{2}_{\\text{OBM}}=\\frac{b}{m}\\sum_{j=1}^{m}\\left(Y_{j}-\\bar{X}_{n}\\right)^{2}.\n$$\nStarting from fundamental properties of IID random variables, derive the exact finite-sample expectation $\\mathbb{E}[\\hat{\\sigma}^{2}_{\\text{OBM}}]$ as a function of $n$, $b$, and $\\sigma^{2}$, without invoking asymptotic approximations. Then determine a multiplicative correction factor $c(n,b)$ such that $c(n,b)\\,\\hat{\\sigma}^{2}_{\\text{OBM}}$ is unbiased for $\\sigma^{2}$. Your final answer should be the closed-form analytic expression for $c(n,b)$ only. No rounding is required.", "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n-   A stationary and independent and identically distributed (IID) sequence $\\{X_{t}\\}_{t=1}^{n}$.\n-   $\\mathbb{E}[X_{t}]=\\mu$.\n-   $\\operatorname{Var}(X_{t})=\\sigma^{2}\\in(0,\\infty)$.\n-   Sample mean: $\\bar{X}_{n}=(1/n)\\sum_{t=1}^{n}X_{t}$.\n-   Batch length: $b$, with $1\\leq b\\leq n$.\n-   Number of overlapping batches: $m=n-b+1$.\n-   Batch means: $Y_{j}=\\frac{1}{b}\\sum_{t=j}^{j+b-1}X_{t}$, for $j=1,2,\\dots,m$.\n-   Overlapping Batch Means (OBM) estimator: $\\hat{\\sigma}^{2}_{\\text{OBM}}=\\frac{b}{m}\\sum_{j=1}^{m}\\left(Y_{j}-\\bar{X}_{n}\\right)^{2}$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is a standard exercise in mathematical statistics, specifically within the context of variance estimation in Monte Carlo simulation. All terms and concepts (IID sequences, batch means, variance estimators) are well-established in the field.\n-   **Well-Posed**: The problem asks for the derivation of an expectation and a resulting correction factor. The setup is clear and provides a path to a unique analytical solution.\n-   **Objective**: The problem is stated in precise mathematical language, free from any subjective or ambiguous terminology.\n-   **Completeness and Consistency**: The problem is self-contained. All necessary definitions and assumptions (IID property, finite non-zero variance) are provided. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Solution Derivation\nOur objective is to find the exact finite-sample expectation of the OBM estimator, $\\mathbb{E}[\\hat{\\sigma}^{2}_{\\text{OBM}}]$, and then determine the correction factor $c(n,b)$ that makes the estimator unbiased.\n\nThe OBM variance estimator is given by:\n$$\n\\hat{\\sigma}^{2}_{\\text{OBM}}=\\frac{b}{m}\\sum_{j=1}^{m}\\left(Y_{j}-\\bar{X}_{n}\\right)^{2}\n$$\nBy linearity of expectation, we have:\n$$\n\\mathbb{E}[\\hat{\\sigma}^{2}_{\\text{OBM}}] = \\frac{b}{m}\\sum_{j=1}^{m}\\mathbb{E}\\left[\\left(Y_{j}-\\bar{X}_{n}\\right)^{2}\\right]\n$$\nLet's analyze the expectation term inside the summation. First, we compute the expected values of the batch means $Y_j$ and the overall sample mean $\\bar{X}_n$.\nFor any batch mean $Y_j$:\n$$\n\\mathbb{E}[Y_{j}] = \\mathbb{E}\\left[\\frac{1}{b}\\sum_{t=j}^{j+b-1}X_{t}\\right] = \\frac{1}{b}\\sum_{t=j}^{j+b-1}\\mathbb{E}[X_{t}] = \\frac{1}{b}\\sum_{t=j}^{j+b-1}\\mu = \\frac{b\\mu}{b} = \\mu\n$$\nFor the overall sample mean $\\bar{X}_n$:\n$$\n\\mathbb{E}[\\bar{X}_{n}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^{n}X_{t}\\right] = \\frac{1}{n}\\sum_{t=1}^{n}\\mathbb{E}[X_{t}] = \\frac{1}{n}\\sum_{t=1}^{n}\\mu = \\frac{n\\mu}{n} = \\mu\n$$\nTherefore, the expectation of the difference is zero:\n$$\n\\mathbb{E}[Y_{j}-\\bar{X}_{n}] = \\mathbb{E}[Y_{j}]-\\mathbb{E}[\\bar{X}_{n}] = \\mu - \\mu = 0\n$$\nThis simplifies the term in the summation, as its expectation is now its variance:\n$$\n\\mathbb{E}\\left[\\left(Y_{j}-\\bar{X}_{n}\\right)^{2}\\right] = \\operatorname{Var}(Y_{j}-\\bar{X}_{n}) + (\\mathbb{E}[Y_j-\\bar{X}_n])^2 = \\operatorname{Var}(Y_{j}-\\bar{X}_{n})\n$$\nUsing the property of variance of a difference, we have:\n$$\n\\operatorname{Var}(Y_{j}-\\bar{X}_{n}) = \\operatorname{Var}(Y_{j}) + \\operatorname{Var}(\\bar{X}_{n}) - 2\\operatorname{Cov}(Y_{j}, \\bar{X}_{n})\n$$\nWe now calculate each of these three terms. Since $\\{X_t\\}$ is an IID sequence with $\\operatorname{Var}(X_t) = \\sigma^2$:\n1.  The variance of a batch mean $Y_j$:\n    $$\n    \\operatorname{Var}(Y_{j}) = \\operatorname{Var}\\left(\\frac{1}{b}\\sum_{t=j}^{j+b-1}X_{t}\\right) = \\frac{1}{b^2}\\sum_{t=j}^{j+b-1}\\operatorname{Var}(X_{t}) = \\frac{1}{b^2}(b\\sigma^2) = \\frac{\\sigma^2}{b}\n    $$\n2.  The variance of the overall sample mean $\\bar{X}_n$:\n    $$\n    \\operatorname{Var}(\\bar{X}_{n}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{t=1}^{n}X_{t}\\right) = \\frac{1}{n^2}\\sum_{t=1}^{n}\\operatorname{Var}(X_{t}) = \\frac{1}{n^2}(n\\sigma^2) = \\frac{\\sigma^2}{n}\n    $$\n3.  The covariance between a batch mean $Y_j$ and the overall sample mean $\\bar{X}_n$. We use the property that for IID variables, $\\operatorname{Cov}(X_t, X_k) = \\sigma^2\\delta_{tk}$, where $\\delta_{tk}$ is the Kronecker delta.\n    $$\n    \\operatorname{Cov}(Y_{j}, \\bar{X}_{n}) = \\operatorname{Cov}\\left(\\frac{1}{b}\\sum_{t=j}^{j+b-1}X_{t}, \\frac{1}{n}\\sum_{k=1}^{n}X_{k}\\right) = \\frac{1}{bn}\\sum_{t=j}^{j+b-1}\\sum_{k=1}^{n}\\operatorname{Cov}(X_{t}, X_{k})\n    $$\n    The covariance term is non-zero only when $k=t$. The indices for $t$ are $\\{j, j+1, \\dots, j+b-1\\}$, which is a subset of the indices for $k$, $\\{1, 2, \\dots, n\\}$. Thus, we sum over the $b$ instances where $k=t$:\n    $$\n    \\operatorname{Cov}(Y_{j}, \\bar{X}_{n}) = \\frac{1}{bn}\\sum_{t=j}^{j+b-1}\\operatorname{Var}(X_{t}) = \\frac{1}{bn}(b\\sigma^2) = \\frac{\\sigma^2}{n}\n    $$\nNotice that $\\operatorname{Var}(Y_j)$, $\\operatorname{Var}(\\bar{X}_n)$, and $\\operatorname{Cov}(Y_j, \\bar{X}_n)$ are all independent of the batch index $j$.\n\nNow, we substitute these back into the expression for $\\operatorname{Var}(Y_{j}-\\bar{X}_{n})$:\n$$\n\\operatorname{Var}(Y_{j}-\\bar{X}_{n}) = \\frac{\\sigma^2}{b} + \\frac{\\sigma^2}{n} - 2\\frac{\\sigma^2}{n} = \\frac{\\sigma^2}{b} - \\frac{\\sigma^2}{n} = \\sigma^2\\left(\\frac{1}{b} - \\frac{1}{n}\\right)\n$$\nSince this term is constant for all $j=1, \\dots, m$, the summation becomes straightforward:\n$$\n\\sum_{j=1}^{m}\\mathbb{E}\\left[\\left(Y_{j}-\\bar{X}_{n}\\right)^{2}\\right] = \\sum_{j=1}^{m}\\sigma^2\\left(\\frac{1}{b} - \\frac{1}{n}\\right) = m\\sigma^2\\left(\\frac{1}{b} - \\frac{1}{n}\\right)\n$$\nFinally, we substitute this into the expression for $\\mathbb{E}[\\hat{\\sigma}^{2}_{\\text{OBM}}]$:\n$$\n\\mathbb{E}[\\hat{\\sigma}^{2}_{\\text{OBM}}] = \\frac{b}{m}\\left[m\\sigma^2\\left(\\frac{1}{b} - \\frac{1}{n}\\right)\\right] = b\\sigma^2\\left(\\frac{n-b}{bn}\\right) = \\sigma^2\\left(\\frac{n-b}{n}\\right) = \\sigma^2\\left(1 - \\frac{b}{n}\\right)\n$$\nThis is the exact finite-sample expectation of the OBM estimator for an IID sequence.\n\nThe second part of the problem asks for a multiplicative correction factor $c(n,b)$ such that $c(n,b)\\hat{\\sigma}^{2}_{\\text{OBM}}$ is an unbiased estimator for $\\sigma^2$. This means we require:\n$$\n\\mathbb{E}\\left[c(n,b)\\hat{\\sigma}^{2}_{\\text{OBM}}\\right] = \\sigma^2\n$$\nSince $c(n,b)$ is a constant with respect to the random variables, we have:\n$$\nc(n,b)\\mathbb{E}\\left[\\hat{\\sigma}^{2}_{\\text{OBM}}\\right] = \\sigma^2\n$$\nSubstituting the expectation we derived:\n$$\nc(n,b)\\sigma^2\\left(1 - \\frac{b}{n}\\right) = \\sigma^2\n$$\nGiven that $\\sigma^2 \\in (0, \\infty)$, we can divide by $\\sigma^2$. We also assume $b<n$, so that $1-b/n \\ne 0$ and the estimator is not degenerate.\n$$\nc(n,b)\\left(1 - \\frac{b}{n}\\right) = 1\n$$\nSolving for $c(n,b)$:\n$$\nc(n,b) = \\frac{1}{1 - \\frac{b}{n}} = \\frac{1}{\\frac{n-b}{n}} = \\frac{n}{n-b}\n$$\nThis is the required correction factor.", "answer": "$$\\boxed{\\frac{n}{n-b}}$$", "id": "3326195"}, {"introduction": "As we move from theory to practice, the computational efficiency of an estimator becomes a critical concern, especially when analyzing large-scale simulation outputs. A naive calculation of the OBM estimator can be prohibitively slow. This practice guides you to design and implement a highly efficient algorithm that computes the OBM variance estimate in linear time, $\\mathcal{O}(n)$, by leveraging the power of cumulative sums [@problem_id:3359876].", "problem": "You are given a finite time series $\\{X_t\\}_{t=1}^n$ produced by a weakly dependent, stationary, and ergodic stochastic process. By the Central Limit Theorem for weakly dependent sequences, the sample mean $\\bar{X}_n = \\frac{1}{n}\\sum_{t=1}^n X_t$ satisfies $\\sqrt{n}(\\bar{X}_n - \\mu) \\Rightarrow \\mathcal{N}(0,\\sigma^2)$ for a well-defined long-run variance $\\sigma^2$. A widely used consistent estimator of $\\sigma^2$ is the Overlapping Batch Means (OBM) estimator. For a chosen batch size $b$ with $1 \\le b \\le n$, define the overlapping batch means $\\bar{X}_i^{(b)} = \\frac{1}{b}\\sum_{t=i}^{i+b-1} X_t$ for $i = 1,2,\\ldots,n-b+1$. The OBM estimator is defined by\n$$\n\\hat{\\sigma}^2_{\\text{OBM}}(b) = \\frac{b}{n-b+1} \\sum_{i=1}^{n-b+1} \\left(\\bar{X}_i^{(b)} - \\bar{X}_n\\right)^2.\n$$\nYour task is to design and implement an algorithm that computes all overlapping batch means $\\bar{X}_i^{(b)}$ and the OBM estimator $\\hat{\\sigma}^2_{\\text{OBM}}(b)$ in time $\\mathcal{O}(n)$ using cumulative sums. The algorithm must be derived from first principles and must avoid redundant recomputation.\n\nThe program must execute with no external input and must compute $\\hat{\\sigma}^2_{\\text{OBM}}(b)$ for the following test suite. All random sequences must be generated using the NumPy pseudorandom generator $\\texttt{np.random.default_rng}(\\text{seed})$ to ensure reproducibility.\n\nTest Suite:\n- Test case $1$ (general correlated case): $\\text{AR}(1)$ sequence with parameter $\\phi = 0.7$, innovation standard deviation $\\sigma_{\\varepsilon} = 1$, initial $X_0 = 0$, length $n = 1000$, seed $2025$, batch size $b = 50$.\n- Test case $2$ (boundary $b=1$): deterministic linear sequence $X_t = t$ for $t = 1,\\ldots,n$, length $n = 10$, batch size $b = 1$.\n- Test case $3$ (boundary $b=n$): $\\text{AR}(1)$ sequence with parameter $\\phi = 0.9$, innovation standard deviation $\\sigma_{\\varepsilon} = 0.5$, initial $X_0 = 0$, length $n = 128$, seed $7$, batch size $b = n$.\n- Test case $4$ (large independent case): independent and identically distributed (IID) standard normal sequence $X_t \\sim \\mathcal{N}(0,1)$, length $n = 5000$, seed $123456$, batch size $b = 100$.\n- Test case $5$ (boundary $b=n-1$): IID uniform sequence $X_t \\sim \\text{Uniform}(-1,1)$, length $n = 50$, seed $99$, batch size $b = n-1 = 49$.\n\nAlgorithmic Requirements:\n- Use a single pass cumulative sum array $S_k = \\sum_{t=1}^k X_t$ with $S_0 = 0$ to compute all overlapping window sums $\\sum_{t=i}^{i+b-1} X_t = S_{i+b-1} - S_{i-1}$, and hence $\\bar{X}_i^{(b)}$, in $\\mathcal{O}(n)$ time.\n- Compute $\\bar{X}_n$ and then the sum of squared deviations $\\sum_{i=1}^{n-b+1}(\\bar{X}_i^{(b)} - \\bar{X}_n)^2$ in a single pass over the batch means.\n- Ensure the overall time complexity is $\\mathcal{O}(n)$ and memory usage is scientifically reasonable.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results for the five test cases as a comma-separated list of floating-point numbers enclosed in square brackets, in the order of the test suite described above (e.g., $[\\text{result}_1,\\text{result}_2,\\ldots,\\text{result}_5]$).\n- There are no physical units or angle units in this problem. All outputs are real numbers.", "solution": "The problem is valid as it is scientifically grounded in the theory of stochastic processes and simulation output analysis, is well-posed with all necessary parameters and definitions provided, and is objective and computationally verifiable.\n\nThe task is to implement an algorithm for computing the Overlapping Batch Means (OBM) variance estimator $\\hat{\\sigma}^2_{\\text{OBM}}(b)$ in $\\mathcal{O}(n)$ time for a time series $\\{X_t\\}_{t=1}^n$ of length $n$ and a given batch size $b$. The OBM estimator is defined as:\n$$\n\\hat{\\sigma}^2_{\\text{OBM}}(b) = \\frac{b}{n-b+1} \\sum_{i=1}^{n-b+1} \\left(\\bar{X}_i^{(b)} - \\bar{X}_n\\right)^2\n$$\nwhere $\\bar{X}_n = \\frac{1}{n}\\sum_{t=1}^n X_t$ is the grand mean and $\\bar{X}_i^{(b)} = \\frac{1}{b}\\sum_{t=i}^{i+b-1} X_t$ are the overlapping batch means.\n\nA naive implementation would compute each of the $n-b+1$ batch means by iterating through $b$ data points, leading to a total time complexity of $\\mathcal{O}((n-b+1)b)$, which is $\\mathcal{O}(nb)$ in general. This is inefficient. To achieve the required $\\mathcal{O}(n)$ complexity, we must avoid this redundant recomputation of sums. The fundamental principle is to use cumulative sums.\n\nThe algorithm proceeds in the following steps, derived from first principles.\n\n**Step 1: Precomputation of Cumulative Sums**\nFirst, we construct a cumulative sum array, $S$, for the time series $X = \\{X_1, X_2, \\ldots, X_n\\}$. Let $S_0 = 0$ and for $k \\in \\{1, \\ldots, n\\}$, let $S_k$ be the sum of the first $k$ terms of the series:\n$$\nS_k = \\sum_{t=1}^k X_t\n$$\nThis array can be computed in a single pass over the data, requiring $\\mathcal{O}(n)$ time. For instance, $S_k = S_{k-1} + X_k$.\n\n**Step 2: Computation of the Grand Mean $\\bar{X}_n$**\nUsing the precomputed cumulative sum array, the grand mean $\\bar{X}_n$ is calculated efficiently. The total sum of the series is simply $S_n$.\n$$\n\\bar{X}_n = \\frac{1}{n} \\sum_{t=1}^n X_t = \\frac{S_n}{n}\n$$\nThis calculation is an $\\mathcal{O}(1)$ operation once $S$ is available.\n\n**Step 3: Computation of All Overlapping Batch Means $\\bar{X}_i^{(b)}$**\nThe sum of values within any batch, from index $i$ to $i+b-1$, can be expressed as the difference between two cumulative sums:\n$$\n\\sum_{t=i}^{i+b-1} X_t = \\left(\\sum_{t=1}^{i+b-1} X_t\\right) - \\left(\\sum_{t=1}^{i-1} X_t\\right) = S_{i+b-1} - S_{i-1}\n$$\nTherefore, each batch mean $\\bar{X}_i^{(b)}$ can be computed in $\\mathcal{O}(1)$ time:\n$$\n\\bar{X}_i^{(b)} = \\frac{S_{i+b-1} - S_{i-1}}{b}\n$$\nWe need to compute these means for all $i = 1, 2, \\ldots, n-b+1$. This can be done by iterating through these $n-b+1$ indices and storing the results in an array. Since each computation is $\\mathcal{O}(1)$, the total time for this step is $\\mathcal{O}(n-b+1)$, which is bounded by $\\mathcal{O}(n)$. In a vectorized environment like NumPy, this can be performed as a single operation on slices of the cumulative sum array, further improving practical efficiency.\n\n**Step 4: Computation of the Sum of Squared Deviations**\nThe next component is the sum of the squared differences between each batch mean and the grand mean:\n$$\n\\text{SSD} = \\sum_{i=1}^{n-b+1} \\left(\\bar{X}_i^{(b)} - \\bar{X}_n\\right)^2\n$$\nHaving computed $\\bar{X}_n$ in Step 2 and all $\\bar{X}_i^{(b)}$ in Step 3, we can compute this sum in a single pass over the array of batch means. This involves $n-b+1$ subtractions, squarings, and additions, resulting in a time complexity of $\\mathcal{O}(n-b+1)$, which is $\\mathcal{O}(n)$.\n\n**Step 5: Final Estimator Calculation**\nFinally, the OBM estimator is calculated by multiplying the sum of squared deviations by the normalization factor:\n$$\n\\hat{\\sigma}^2_{\\text{OBM}}(b) = \\frac{b}{n-b+1} \\times \\text{SSD}\n$$\nThis is a single multiplication and division, an $\\mathcal{O}(1)$ operation.\n\n**Overall Complexity**\nThe total time complexity is the sum of a series of $\\mathcal{O}(n)$ and $\\mathcal{O}(1)$ operations, which is $\\mathcal{O}(n)$. The memory required is $\\mathcal{O}(n)$ to store the input series, the cumulative sums, and the batch means. This design fulfills all algorithmic requirements of the problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_obm_variance(X: np.ndarray, b: int) -> float:\n    \"\"\"\n    Computes the Overlapping Batch Means (OBM) variance estimator in O(n) time.\n\n    Args:\n        X (np.ndarray): The time series data of length n.\n        b (int): The batch size.\n\n    Returns:\n        float: The OBM variance estimate.\n    \"\"\"\n    n = len(X)\n    if not (1 = b = n):\n        raise ValueError(\"Batch size b must be between 1 and n.\")\n\n    # Let k be the number of overlapping batches.\n    k = n - b + 1\n\n    # Step 1: Compute the cumulative sum array S.\n    # S[j] = sum(X[0]...X[j-1]), with S[0] = 0.\n    S = np.concatenate(([0.0], np.cumsum(X)))\n\n    # Step 2: Compute the grand mean X_bar_n.\n    # S[n] contains the sum of all elements in X.\n    X_bar_n = S[n] / n\n\n    # Step 3: Compute all overlapping batch means using vectorized operations.\n    # The sums of the b-sized batches are S[b:] - S[:-b].\n    # This creates an array of k batch sums.\n    batch_sums = S[b:] - S[:k]\n    batch_means = batch_sums / b\n    \n    # Step 4: Compute the sum of squared deviations.\n    sum_sq_dev = np.sum((batch_means - X_bar_n)**2)\n\n    # Step 5: Final OBM estimator calculation.\n    # The pre-factor is b / k.\n    obm_variance = (b / k) * sum_sq_dev\n    \n    return obm_variance\n\ndef generate_ar1_series(phi: float, sigma_eps: float, n: int, seed: int, X0: float = 0.0) -> np.ndarray:\n    \"\"\"\n    Generates an AR(1) time series: X_t = phi * X_{t-1} + eps_t.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    eps = rng.normal(loc=0.0, scale=sigma_eps, size=n)\n    X = np.zeros(n)\n    if n > 0:\n        X[0] = phi * X0 + eps[0]\n        for t in range(1, n):\n            X[t] = phi * X[t-1] + eps[t]\n    return X\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: General correlated case (AR(1))\n        {'type': 'ar1', 'phi': 0.7, 'sigma_eps': 1.0, 'n': 1000, 'seed': 2025, 'b': 50, 'X0': 0.0},\n        \n        # Test case 2: Boundary b=1 (deterministic linear)\n        {'type': 'linear', 'n': 10, 'b': 1},\n        \n        # Test case 3: Boundary b=n (AR(1))\n        {'type': 'ar1', 'phi': 0.9, 'sigma_eps': 0.5, 'n': 128, 'seed': 7, 'b': 128, 'X0': 0.0},\n        \n        # Test case 4: Large independent case (IID Normal)\n        {'type': 'normal', 'n': 5000, 'seed': 123456, 'b': 100},\n        \n        # Test case 5: Boundary b=n-1 (IID Uniform)\n        {'type': 'uniform', 'n': 50, 'seed': 99, 'b': 49}\n    ]\n\n    results = []\n    for case in test_cases:\n        X = None\n        if case['type'] == 'ar1':\n            X = generate_ar1_series(case['phi'], case['sigma_eps'], case['n'], case['seed'], case['X0'])\n        elif case['type'] == 'linear':\n            X = np.arange(1, case['n'] + 1, dtype=float)\n        elif case['type'] == 'normal':\n            rng = np.random.default_rng(case['seed'])\n            X = rng.normal(loc=0.0, scale=1.0, size=case['n'])\n        elif case['type'] == 'uniform':\n            rng = np.random.default_rng(case['seed'])\n            X = rng.uniform(low=-1.0, high=1.0, size=case['n'])\n        \n        b = case['b']\n        \n        result = compute_obm_variance(X, b)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3359876"}, {"introduction": "The primary motivation for using OBM is its superior statistical efficiency compared to the simpler Non-Overlapping Batch Means (NBM) method. This capstone exercise provides an opportunity to verify and quantify this advantage through a hands-on Monte Carlo study. You will simulate a correlated time series, implement both OBM and NBM estimators, and empirically analyze their mean squared error to find the optimal batch size, gaining practical intuition for the crucial bias-variance tradeoff in variance estimation [@problem_id:3359912].", "problem": "Consider a strictly stationary and ergodic autoregressive process of order one, denoted by $AR(1)$, defined by the recursion $X_t = \\phi X_{t-1} + \\varepsilon_t$, where $\\{ \\varepsilon_t \\}$ is an independent and identically distributed sequence with zero mean and finite variance, and $|\\phi|  1$. The spectral density $f(\\omega)$ is the Fourier transform of the autocovariance function, and the value at zero frequency, $f(0)$, determines the asymptotic variance of the sample mean via the Central Limit Theorem (CLT) and the Functional Central Limit Theorem (FCLT) as follows: $\\sqrt{n}(\\bar{X}_n - \\mu) \\Rightarrow \\mathcal{N}(0, 2\\pi f(0))$, where $\\mu = \\mathbb{E}[X_t]$ and $\\bar{X}_n = n^{-1}\\sum_{t=1}^n X_t$. The long-run variance is $2\\pi f(0)$.\n\nYour task is to empirically compare nonoverlapping batch means and overlapping batch means for estimating $f(0)$ in the following controlled study. You must start from the above foundational definitions and the CLT/FCLT statement and derive valid estimators of $f(0)$ based on batch means. You must then implement the derived estimators and study their mean squared error (MSE) as a function of the batch size $b$, finally identifying the empirically optimal batch size within a prescribed grid.\n\nSimulation model and requirements:\n- Fix the autoregressive parameter at $\\phi = 0.95$.\n- Choose the innovation variance to make the marginal variance of $X_t$ equal to $1$.\n- Initialize $X_0$ to be drawn from the stationary marginal distribution of $X_t$, to avoid start-up bias.\n- For each simulation path, generate a length-$n$ time series $\\{X_t\\}_{t=1}^n$.\n- For each candidate batch size $b$, compute two estimators of $f(0)$:\n  - One based on nonoverlapping batch means using batches of size $b$.\n  - One based on overlapping batch means using all consecutive windows of length $b$.\n- Using $R$ independent Monte Carlo replications, approximate the MSE for each estimator and each $b$ as the empirical mean of squared errors relative to the true $f(0)$ of the $AR(1)$ model. Ties in the empirically optimal $b$ must be broken by selecting the smallest $b$ attaining the minimum MSE.\n\nTest suite:\n- Use the three parameter sets below. For each case, evaluate both estimators over the prescribed batch-size grid.\n  - Case A: $n = 8192$, $R = 256$, candidate $b \\in \\{8, 16, 32, 64, 128, 256, 512, 1024, 2048, 3072\\}$.\n  - Case B: $n = 512$, $R = 512$, candidate $b \\in \\{4, 8, 16, 32, 64, 96, 128, 192, 256\\}$.\n  - Case C: $n = 20000$, $R = 128$, candidate $b \\in \\{20, 40, 80, 160, 320, 640, 1280, 2560, 4000, 8000\\}$.\n- For nonoverlapping batch means, only apply candidate $b$ values that produce at least two full batches in length $n$. For overlapping batch means, only apply candidate $b$ values that produce at least two overlapping windows in length $n$.\n- For reproducibility, use a fixed pseudorandom number generator seed equal to $20240519$.\n\nDeliverables and output format:\n- Your program must:\n  - Derive from first principles valid nonoverlapping and overlapping batch means estimators of $f(0)$, implement both, and compute their empirical MSEs across the specified batch-size grids.\n  - For each case, report the empirically minimizing batch size $b$ for the nonoverlapping method and for the overlapping method, with ties broken in favor of the smallest $b$.\n- Final output format: a single line that contains a list of three lists, in the order Case A, Case B, Case C, where each inner list has the form $[b_{\\text{nonoverlap}}, b_{\\text{overlap}}]$. For example, the output must have the exact form $[[b_{A,\\text{non}},b_{A,\\text{over}}],[b_{B,\\text{non}},b_{B,\\text{over}}],[b_{C,\\text{non}},b_{C,\\text{over}}]]$ with no spaces after commas inside numbers.\n\nThere are no physical units involved in this problem. All angles, if any were to appear, would be in radians. The required outputs are integers. Your code must be self-contained, require no user input, and produce exactly one line of output in the specified format.", "solution": "The problem requires an empirical comparison of nonoverlapping and overlapping batch means estimators for the spectral density at zero frequency, $f(0)$, of a specific AR(1) process. The comparison is based on the Mean Squared Error (MSE) of the estimators for various batch sizes.\n\nFirst, we establish the theoretical foundation and derive the estimators. The problem states that for a stationary time series with sample mean $\\bar{X}_n$, the Central Limit Theorem (CLT) holds in the form $\\sqrt{n}(\\bar{X}_n - \\mu) \\Rightarrow \\mathcal{N}(0, \\sigma^2_{LR})$, where $\\sigma^2_{LR} = 2\\pi f(0)$ is the long-run variance. This implies that for large $n$, the variance of the sample mean is $\\text{Var}(\\bar{X}_n) \\approx \\frac{\\sigma^2_{LR}}{n}$. The core task is to estimate $\\sigma^2_{LR}$ (and thus $f(0) = \\sigma^2_{LR}/(2\\pi)$) from a single time series realization $\\{X_t\\}_{t=1}^n$.\n\nThe specified simulation model is the AR(1) process $X_t = \\phi X_{t-1} + \\varepsilon_t$ with $\\phi=0.95$. The innovations $\\{\\varepsilon_t\\}$ are i.i.d. with $\\mathbb{E}[\\varepsilon_t] = 0$. For this process, the mean is $\\mu = \\mathbb{E}[X_t] = 0$. The marginal variance is $\\sigma_X^2 = \\text{Var}(X_t) = \\frac{\\text{Var}(\\varepsilon_t)}{1-\\phi^2}$. The problem requires $\\sigma_X^2=1$, so we must set the innovation variance to $\\text{Var}(\\varepsilon_t) = \\sigma_\\varepsilon^2 = 1-\\phi^2$. The initial state $X_0$ is drawn from the stationary distribution, which, assuming Gaussian innovations, is $\\mathcal{N}(0, \\sigma_X^2) = \\mathcal{N}(0, 1)$.\n\nThe true value of the long-run variance $\\sigma^2_{LR}$ for an AR(1) process is given by the sum of its autocovariances:\n$$\n\\sigma^2_{LR} = \\sum_{h=-\\infty}^{\\infty} \\gamma(h) = \\sum_{h=-\\infty}^{\\infty} \\sigma_X^2 \\phi^{|h|} = \\sigma_X^2 \\frac{1+\\phi}{1-\\phi}\n$$\nWith $\\sigma_X^2 = 1$ and $\\phi = 0.95$, the true value is $\\sigma^2_{LR, \\text{true}} = \\frac{1+0.95}{1-0.95} = \\frac{1.95}{0.05} = 39$.\nThe corresponding true value for $f(0)$ is $f(0)_{\\text{true}} = \\frac{\\sigma^2_{LR, \\text{true}}}{2\\pi} = \\frac{39}{2\\pi}$. Our estimators will be evaluated against this value.\n\n**Derivation of Estimators**\n\n**1. Nonoverlapping Batch Means (NBM) Estimator**\nThe time series $X_1, \\dots, X_n$ is partitioned into $k = \\lfloor n/b \\rfloor$ contiguous, non-overlapping batches of size $b$. The data from $X_{kb+1}, \\dots, X_n$ are discarded. The mean of the $i$-th batch is:\n$$\nY_i = \\frac{1}{b} \\sum_{j=1}^{b} X_{(i-1)b+j} \\quad \\text{for } i=1, \\dots, k\n$$\nIf the batch size $b$ is sufficiently large, the batch means $\\{Y_i\\}_{i=1}^k$ are approximately uncorrelated and identically distributed. From the CLT, each $Y_i$ is a sample mean over a series of length $b$, so $\\text{Var}(Y_i) \\approx \\sigma^2_{LR}/b$. We can estimate this variance using the sample variance of the batch means. The standard unbiased estimator for the variance of the population of $Y_i$'s is:\n$$\n\\hat{\\text{Var}}(Y_i) = \\frac{1}{k-1} \\sum_{i=1}^k (Y_i - \\bar{Y}_k)^2, \\quad \\text{where } \\bar{Y}_k = \\frac{1}{k} \\sum_{i=1}^k Y_i\n$$\nSince this quantity estimates $\\sigma^2_{LR}/b$, an estimator for $\\sigma^2_{LR}$ is obtained by multiplying by $b$:\n$$\n\\hat{\\sigma}^2_{NBM} = \\frac{b}{k-1} \\sum_{i=1}^k (Y_i - \\bar{Y}_k)^2\n$$\nThe NBM estimator for $f(0)$ is therefore $\\hat{f}(0)_{NBM} = \\frac{\\hat{\\sigma}^2_{NBM}}{2\\pi}$. A minimum of $k=2$ batches is required to compute a variance.\n\n**2. Overlapping Batch Means (OBM) Estimator**\nInstead of disjoint batches, the OBM method uses all $m = n-b+1$ possible contiguous subsequences of length $b$. The $i$-th overlapping batch mean is:\n$$\nZ_i = \\frac{1}{b} \\sum_{j=0}^{b-1} X_{i+j} \\quad \\text{for } i=1, \\dots, m\n$$\nThe sequence $\\{Z_i\\}_{i=1}^m$ is highly correlated, so a simple application of the sample variance formula is not appropriate without careful consideration of scaling factors. A rigorous derivation, often connected to the Bartlett spectral density estimator, shows that a consistent and often-used estimator for $\\sigma^2_{LR}$ is:\n$$\n\\hat{\\sigma}^2_{OBM} = \\frac{nb}{(n-b+1)(n-b)} \\sum_{i=1}^{n-b+1} (Z_i - \\bar{X}_n)^2\n$$\nwhere $\\bar{X}_n = \\frac{1}{n} \\sum_{t=1}^n X_t$ is the grand mean of the entire series. This form includes a factor of $\\frac{n}{n-b}$ which serves as a bias correction relative to some simpler forms. The OBM estimator for $f(0)$ is $\\hat{f}(0)_{OBM} = \\frac{\\hat{\\sigma}^2_{OBM}}{2\\pi}$. Similar to NBM, this requires at least two batch means ($m \\ge 2$, or $b \\le n-1$) to be well-defined.\n\n**Simulation and Evaluation**\nFor each test case (A, B, C), we perform $R$ independent Monte Carlo replications. In each replication, a time series of length $n$ is generated. For each candidate batch size $b$, we compute $\\hat{f}(0)_{NBM}$ and $\\hat{f}(0)_{OBM}$. The performance is measured by the Mean Squared Error (MSE), approximated by the empirical mean over the $R$ replications:\n$$\n\\text{MSE}(\\hat{f}(0); b) = \\frac{1}{R} \\sum_{r=1}^R \\left(\\hat{f}(0)^{(r)}(b) - f(0)_{\\text{true}}\\right)^2\n$$\nwhere $\\hat{f}(0)^{(r)}(b)$ is the estimate from replication $r$ using batch size $b$. For each estimator type, we identify the batch size $b$ from the given grid that minimizes this empirical MSE, breaking ties by selecting the smallest $b$. The implementation will follow these derived formulas and procedures.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the batch means comparison problem by running a Monte Carlo simulation.\n    \"\"\"\n    \n    # --- Simulation Constants ---\n    PHI = 0.95\n    SEED = 20240519\n\n    # --- Test Cases ---\n    test_cases = [\n        # Case A\n        {'n': 8192, 'R': 256, 'b_grid': [8, 16, 32, 64, 128, 256, 512, 1024, 2048, 3072]},\n        # Case B\n        {'n': 512, 'R': 512, 'b_grid': [4, 8, 16, 32, 64, 96, 128, 192, 256]},\n        # Case C\n        {'n': 20000, 'R': 128, 'b_grid': [20, 40, 80, 160, 320, 640, 1280, 2560, 4000, 8000]},\n    ]\n\n    # --- Helper Functions ---\n    \n    def generate_ar1_series(n, phi, rng):\n        \"\"\"Generates a stationary AR(1) series of length n with Var(X)=1.\"\"\"\n        sigma_eps = np.sqrt(1 - phi**2)\n        innovations = rng.normal(loc=0, scale=sigma_eps, size=n)\n        x = np.zeros(n)\n        x0 = rng.normal(loc=0, scale=1.0)\n        \n        x[0] = phi * x0 + innovations[0]\n        for t in range(1, n):\n            x[t] = phi * x[t-1] + innovations[t]\n        return x\n\n    def estimate_f0_nbm(x, b):\n        \"\"\"Estimates f(0) using non-overlapping batch means.\"\"\"\n        n = len(x)\n        k = n // b\n        if k  2:\n            return np.nan\n        \n        x_trunc = x[:k * b]\n        # Reshape and take mean along axis 1 to get batch means\n        batch_means = x_trunc.reshape((k, b)).mean(axis=1)\n        \n        # Unbiased sample variance of batch means (ddof=1)\n        var_batch_means = batch_means.var(ddof=1)\n        sigma_sq_lr_hat = b * var_batch_means\n        \n        f0_hat = sigma_sq_lr_hat / (2 * np.pi)\n        return f0_hat\n\n    def estimate_f0_obm(x, b):\n        \"\"\"Estimates f(0) using overlapping batch means.\"\"\"\n        n = len(x)\n        m = n - b + 1\n        if m  2:\n            return np.nan\n\n        # Use convolution for efficient calculation of moving averages\n        batch_sums = np.convolve(x, np.ones(b), mode='valid')\n        batch_means = batch_sums / b\n        \n        x_bar = x.mean()\n        \n        sum_sq_dev = np.sum((batch_means - x_bar)**2)\n        \n        # Use the normalization from the solution derivation\n        # Note: some sources use m in the denominator instead of (m-1), but the prompt's derivation points to this form.\n        sigma_sq_lr_hat = (n * b) / (m * (m-1)) * sum_sq_dev\n        \n        f0_hat = sigma_sq_lr_hat / (2 * np.pi)\n        return f0_hat\n\n    # --- Main Logic ---\n    \n    final_results = []\n    rng = np.random.default_rng(SEED)\n    f0_true = (1 / (2 * np.pi)) * (1 + PHI) / (1 - PHI)\n\n    for case in test_cases:\n        n, R, b_grid = case['n'], case['R'], case['b_grid']\n        \n        sse_nbm = {b: 0.0 for b in b_grid}\n        sse_obm = {b: 0.0 for b in b_grid}\n        \n        for _ in range(R):\n            x_series = generate_ar1_series(n, PHI, rng)\n            \n            for b in b_grid:\n                # NBM estimator\n                if n // b >= 2:\n                    f0_hat_nbm = estimate_f0_nbm(x_series, b)\n                    if not np.isnan(f0_hat_nbm):\n                       sse_nbm[b] += (f0_hat_nbm - f0_true)**2\n                \n                # OBM estimator\n                if n - b + 1 >= 2:\n                    f0_hat_obm = estimate_f0_obm(x_series, b)\n                    if not np.isnan(f0_hat_obm):\n                        sse_obm[b] += (f0_hat_obm - f0_true)**2\n        \n        # Find optimal batch size for NBM\n        min_mse_nbm = float('inf')\n        opt_b_nbm = -1\n        for b in b_grid:\n            if n // b >= 2:\n                mse = sse_nbm[b] / R\n                if mse  min_mse_nbm:\n                    min_mse_nbm = mse\n                    opt_b_nbm = b\n        \n        # Find optimal batch size for OBM\n        min_mse_obm = float('inf')\n        opt_b_obm = -1\n        for b in b_grid:\n            if n - b + 1 >= 2:\n                mse = sse_obm[b] / R\n                if mse  min_mse_obm:\n                    min_mse_obm = mse\n                    opt_b_obm = b\n                    \n        final_results.append([opt_b_nbm, opt_b_obm])\n\n    # Format and print the final output as specified\n    inner_lists_str = [f\"[{r[0]},{r[1]}]\" for r in final_results]\n    print(f\"[{','.join(inner_lists_str)}]\")\n\nsolve()\n```", "id": "3359912"}]}