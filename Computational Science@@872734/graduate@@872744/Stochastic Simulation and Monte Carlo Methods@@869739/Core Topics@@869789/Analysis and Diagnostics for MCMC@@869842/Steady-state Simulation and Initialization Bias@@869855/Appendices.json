{"hands_on_practices": [{"introduction": "To effectively manage bias, it is instructive to first understand its mathematical origins. This exercise provides a rare opportunity to derive the exact leading term of the initialization bias for a simple, yet illustrative, system [@problem_id:3347939]. By solving the Poisson equation for a two-state Markov chain, you will see how the system's dynamics—its transition probabilities—directly determine the magnitude of the estimation error, providing a concrete foundation for why bias exists and how it behaves.", "problem": "Consider a discrete-time, time-homogeneous, irreducible and aperiodic Markov chain on the finite state space $\\{0,1\\}$ with transition matrix\n$$\nP \\;=\\; \\begin{pmatrix} 1-p  p \\\\ q  1-q \\end{pmatrix},\n$$\nwhere $p \\in (0,1)$ and $q \\in (0,1)$. Let $X_0$ denote the initial state and let $\\{X_k\\}_{k \\geq 0}$ denote the trajectory of the chain. Define the performance function $f:\\{0,1\\}\\to\\mathbb{R}$ by $f(0)=0$ and $f(1)=1$, and define the time average\n$$\n\\bar{f}_n \\;=\\; \\frac{1}{n} \\sum_{k=0}^{n-1} f(X_k).\n$$\nLet $\\pi$ denote the unique stationary distribution of the Markov chain and write $\\pi(f) = \\sum_{x \\in \\{0,1\\}} \\pi(x) f(x)$. The initialization bias in steady-state simulation is the deviation $\\mathbb{E}[\\bar{f}_n] - \\pi(f)$ induced by the initial condition $X_0=0$.\n\nStarting from core definitions of stationary distributions and the Markov property, and using the characterization that a solution $h:\\{0,1\\}\\to\\mathbb{R}$ to the Poisson equation satisfies $h - P h = f - \\pi(f)$ together with the normalization $\\sum_{x \\in \\{0,1\\}} \\pi(x) h(x) = 0$, carry out the following steps:\n\n- Derive the stationary distribution $\\pi$ and compute $\\pi(f)$.\n- Solve the Poisson equation for $h$ with the stated normalization.\n- From first principles, derive an explicit expression for the leading constant $C(p,q)$ in the asymptotic expansion of the initialization bias for $X_0=0$,\n$$\n\\mathbb{E}[\\bar{f}_n] - \\pi(f) \\;=\\; \\frac{C(p,q)}{n} + o\\!\\left(\\frac{1}{n}\\right) \\quad \\text{as } n \\to \\infty.\n$$\n\nProvide your final answer as a single closed-form analytic expression for $C(p,q)$. No rounding is required and no physical units apply.", "solution": "The problem asks for the leading constant $C(p,q)$ in the asymptotic expansion of the initialization bias for a two-state Markov chain. The analysis will proceed in three steps as outlined in the problem: first, we derive the stationary distribution $\\pi$ and the steady-state performance $\\pi(f)$; second, we solve the Poisson equation for the potential function $h$; and third, we use this solution to find the asymptotic bias.\n\n**Step 1: Stationary Distribution and Steady-State Performance**\n\nLet the stationary distribution be denoted by the row vector $\\pi = (\\pi_0, \\pi_1)$, where $\\pi_x = \\pi(x)$ is the stationary probability of being in state $x \\in \\{0, 1\\}$. By definition, the stationary distribution satisfies the equation $\\pi P = \\pi$, along with the normalization condition $\\sum_{x \\in \\{0,1\\}} \\pi_x = 1$.\n\nThe equation $\\pi P = \\pi$ is:\n$$\n(\\pi_0, \\pi_1) \\begin{pmatrix} 1-p  p \\\\ q  1-q \\end{pmatrix} = (\\pi_0, \\pi_1)\n$$\nThis gives a system of two linear equations:\n$$\n\\pi_0 (1-p) + \\pi_1 q = \\pi_0 \\\\\n\\pi_0 p + \\pi_1 (1-q) = \\pi_1\n$$\nFrom the first equation, we get $-\\pi_0 p + \\pi_1 q = 0$, which implies $\\pi_0 p = \\pi_1 q$. The second equation gives the same relationship. Since $p,q \\in (0,1)$, this implies a non-trivial relationship between $\\pi_0$ and $\\pi_1$.\n\nUsing the normalization condition $\\pi_0 + \\pi_1 = 1$, we can substitute $\\pi_1 = 1 - \\pi_0$ into the relation $\\pi_0 p = \\pi_1 q$:\n$$\n\\pi_0 p = (1-\\pi_0) q = q - \\pi_0 q\n$$\n$$\n\\pi_0 p + \\pi_0 q = q\n$$\n$$\n\\pi_0 (p+q) = q\n$$\nSince $p,q \\in (0,1)$, $p+q  0$, so we can solve for $\\pi_0$:\n$$\n\\pi_0 = \\frac{q}{p+q}\n$$\nAnd for $\\pi_1$:\n$$\n\\pi_1 = 1 - \\pi_0 = 1 - \\frac{q}{p+q} = \\frac{p+q-q}{p+q} = \\frac{p}{p+q}\n$$\nSo the stationary distribution is $\\pi = \\left(\\frac{q}{p+q}, \\frac{p}{p+q}\\right)$.\n\nThe steady-state performance $\\pi(f)$ is the expected value of the function $f$ under the stationary distribution:\n$$\n\\pi(f) = \\sum_{x \\in \\{0,1\\}} \\pi(x) f(x) = \\pi_0 f(0) + \\pi_1 f(1)\n$$\nUsing the given values $f(0)=0$ and $f(1)=1$:\n$$\n\\pi(f) = \\left(\\frac{q}{p+q}\\right) \\cdot 0 + \\left(\\frac{p}{p+q}\\right) \\cdot 1 = \\frac{p}{p+q}\n$$\n\n**Step 2: Solving the Poisson Equation**\n\nThe Poisson equation is given as $h - P h = f - \\pi(f)$, where $h$ and $f$ are understood as column vectors indexed by the state space $\\{0,1\\}$. We write this as $(I-P)h = f - \\pi(f)\\mathbf{1}$, where $I$ is the identity matrix and $\\mathbf{1}$ is a column vector of ones.\n\nThe vectors are:\n$h = \\begin{pmatrix} h(0) \\\\ h(1) \\end{pmatrix}$, $f = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThe right-hand side is:\n$$\nf - \\pi(f)\\mathbf{1} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} - \\frac{p}{p+q} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -\\frac{p}{p+q} \\\\ 1 - \\frac{p}{p+q} \\end{pmatrix} = \\begin{pmatrix} -\\frac{p}{p+q} \\\\ \\frac{q}{p+q} \\end{pmatrix}\n$$\nThe matrix $(I-P)$ is:\n$$\nI-P = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\begin{pmatrix} 1-p  p \\\\ q  1-q \\end{pmatrix} = \\begin{pmatrix} p  -p \\\\ -q  q \\end{pmatrix}\n$$\nThe Poisson equation becomes:\n$$\n\\begin{pmatrix} p  -p \\\\ -q  q \\end{pmatrix} \\begin{pmatrix} h(0) \\\\ h(1) \\end{pmatrix} = \\begin{pmatrix} -\\frac{p}{p+q} \\\\ \\frac{q}{p+q} \\end{pmatrix}\n$$\nThis matrix equation yields two dependent linear equations:\n1. $p \\cdot h(0) - p \\cdot h(1) = -\\frac{p}{p+q}$\n2. $-q \\cdot h(0) + q \\cdot h(1) = \\frac{q}{p+q}$\n\nSince $p, q \\in (0,1)$, we can divide the first equation by $p$ and the second by $q$, both yielding the same relation:\n$$\nh(0) - h(1) = -\\frac{1}{p+q}\n$$\nThis system is underdetermined, as expected. We use the specified normalization condition $\\sum_{x \\in \\{0,1\\}} \\pi(x) h(x) = 0$:\n$$\n\\pi_0 h(0) + \\pi_1 h(1) = 0\n$$\n$$\n\\frac{q}{p+q} h(0) + \\frac{p}{p+q} h(1) = 0\n$$\n$$\nq h(0) + p h(1) = 0\n$$\nWe now solve the system of two independent linear equations for $h(0)$ and $h(1)$:\n(a) $h(0) - h(1) = -\\frac{1}{p+q}$\n(b) $q h(0) + p h(1) = 0$\n\nFrom (b), we express $h(0)$ in terms of $h(1)$: $h(0) = -\\frac{p}{q}h(1)$. Substituting this into (a):\n$$\n-\\frac{p}{q}h(1) - h(1) = -\\frac{1}{p+q}\n$$\n$$\n-h(1)\\left(\\frac{p}{q} + 1\\right) = -\\frac{1}{p+q}\n$$\n$$\n-h(1)\\left(\\frac{p+q}{q}\\right) = -\\frac{1}{p+q}\n$$\n$$\nh(1) = \\frac{q}{(p+q)^2}\n$$\nNow we find $h(0)$:\n$$\nh(0) = -\\frac{p}{q} h(1) = -\\frac{p}{q} \\left(\\frac{q}{(p+q)^2}\\right) = -\\frac{p}{(p+q)^2}\n$$\nThus, the solution to the Poisson equation with the given normalization is $h(0) = -\\frac{p}{(p+q)^2}$ and $h(1) = \\frac{q}{(p+q)^2}$.\n\n**Step 3: Derivation of the Initialization Bias Constant**\n\nLet $\\tilde{f}(x) = f(x) - \\pi(f)$. The Poisson equation is $h(x) - \\mathbb{E}[h(X_{k+1})|X_k=x] = \\tilde{f}(x)$.\nWe sum this relation over $k$ from $0$ to $n-1$:\n$$\n\\sum_{k=0}^{n-1} \\tilde{f}(X_k) = \\sum_{k=0}^{n-1} \\left( h(X_k) - \\mathbb{E}[h(X_{k+1})|X_k] \\right)\n$$\nTaking the expectation conditional on the initial state $X_0=x_0$:\n$$\n\\mathbb{E}_{x_0}\\left[\\sum_{k=0}^{n-1} \\tilde{f}(X_k)\\right] = \\sum_{k=0}^{n-1} \\left(\\mathbb{E}_{x_0}[h(X_k)] - \\mathbb{E}_{x_0}[\\mathbb{E}[h(X_{k+1})|X_k]]\\right)\n$$\nBy the law of total expectation, $\\mathbb{E}_{x_0}[\\mathbb{E}[h(X_{k+1})|X_k]] = \\mathbb{E}_{x_0}[h(X_{k+1})]$. The sum becomes a telescoping series:\n$$\n\\sum_{k=0}^{n-1} (\\mathbb{E}_{x_0}[h(X_k)] - \\mathbb{E}_{x_0}[h(X_{k+1})]) = \\mathbb{E}_{x_0}[h(X_0)] - \\mathbb{E}_{x_0}[h(X_n)]\n$$\nSince $X_0=x_0$ is fixed, $\\mathbb{E}_{x_0}[h(X_0)] = h(x_0)$. The left side of the equation is $\\mathbb{E}_{x_0}\\left[n \\bar{f}_n - n\\pi(f)\\right] = n(\\mathbb{E}_{x_0}[\\bar{f}_n] - \\pi(f))$. Therefore,\n$$\nn\\left(\\mathbb{E}_{x_0}[\\bar{f}_n] - \\pi(f)\\right) = h(x_0) - \\mathbb{E}_{x_0}[h(X_n)]\n$$\nDividing by $n$:\n$$\n\\mathbb{E}_{x_0}[\\bar{f}_n] - \\pi(f) = \\frac{h(x_0)}{n} - \\frac{\\mathbb{E}_{x_0}[h(X_n)]}{n}\n$$\nFor an irreducible and aperiodic Markov chain on a finite state space, the distribution of $X_n$ converges exponentially fast to the stationary distribution $\\pi$. Thus,\n$$\n\\mathbb{E}_{x_0}[h(X_n)] = \\sum_{y \\in \\{0,1\\}} \\mathbb{P}(X_n = y | X_0=x_0)h(y) \\to \\sum_{y \\in \\{0,1\\}} \\pi(y)h(y) \\quad \\text{as } n \\to \\infty\n$$\nBy our normalization of $h$, this limit is $\\sum_y \\pi(y)h(y) = 0$. The convergence is exponential, meaning $\\mathbb{E}_{x_0}[h(X_n)] = O(\\lambda^n)$ for some $|\\lambda|1$. Therefore, the term $\\frac{1}{n}\\mathbb{E}_{x_0}[h(X_n)]$ decays faster than $\\frac{1}{n}$ and is of order $o(1/n)$.\nThe asymptotic expansion is:\n$$\n\\mathbb{E}_{x_0}[\\bar{f}_n] - \\pi(f) = \\frac{h(x_0)}{n} + o\\left(\\frac{1}{n}\\right)\n$$\nThe problem specifies the initial condition $X_0=0$. The constant $C(p,q)$ is therefore $h(0)$.\nUsing our result from Step 2:\n$$\nC(p,q) = h(0) = -\\frac{p}{(p+q)^2}\n$$\nThis is the final expression for the leading constant in the initialization bias.", "answer": "$$\\boxed{-\\frac{p}{(p+q)^{2}}}$$", "id": "3347939"}, {"introduction": "The most common method for mitigating initialization bias is deleting an initial warm-up period, but this action is not a free lunch. This practice exercise challenges you to analyze the consequences of data deletion, revealing the fundamental bias-variance tradeoff at the heart of simulation output analysis [@problem_id:3347933]. You will quantitatively assess whether the reduction in bias achieved by discarding data is worth the cost of increased variance, a critical skill for any serious practitioner.", "problem": "A steady-state simulation produces a time series $\\{X_t\\}_{t=1}^{n}$ intended to estimate the steady-state mean $\\mu$ of a stationary, ergodic process with variance $\\sigma^{2}$. The series has autocorrelation function $\\rho_k = \\beta^{k}$ for $k \\geq 1$ with $0  \\beta  1$. The Effective Sample Size (ESS) is defined as the size of an independent and identically distributed sample that would produce the same variance for the sample mean as the dependent sample. An initial transient deletion (ITD) strategy removes the first $m$ observations, and the mean of the remaining $n-m$ observations estimates $\\mu$.\n\nAssume that:\n- The series length is $n = 10^{5}$.\n- The autocorrelation parameter is $\\beta = 0.99$.\n- The deletion size is $m = \\lfloor n^{1/2} \\rfloor$.\n- The simulation is started from a deterministic initial condition whose mean is offset from $\\mu$ by $\\delta$, so that the transient mean offset decays geometrically as $\\delta \\beta^{t-1}$.\n\nTasks:\n1. Starting from the definition of the variance of the sample mean for a stationary time series and the above ESS definition, derive expressions for the ESS of the full run of length $n$ and of the deleted run of length $n-m$, both as explicit functions of $n$, $m$, and $\\beta$.\n2. Evaluate these expressions numerically for $n = 10^{5}$, $\\beta = 0.99$, and $m = \\lfloor n^{1/2} \\rfloor$, and compute the ratio $R$ of the ESS after deletion to the ESS before deletion.\n3. Using first principles for initialization bias under geometric mean reversion, analyze whether deleting $m = \\lfloor n^{1/2} \\rfloor$ is justified relative to the bias reduction it achieves. Base your argument on how the squared bias and the variance of the time-average estimator scale with $n$, $m$, and $\\beta$, without assuming any value for $\\delta$.\n\nReport the ratio $R$ as your final answer. Round your final answer to four significant figures.", "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the principles of stochastic simulation output analysis, well-posed with a complete and consistent set of givens, and expressed in objective, formal language. We may therefore proceed with a full solution.\n\nThe problem asks for three tasks: first, to derive expressions for the Effective Sample Size (ESS) before and after an initial transient deletion; second, to numerically evaluate these and their ratio; and third, to analyze the justification of the deletion strategy based on bias-variance trade-offs.\n\n### Task 1: Derivation of Effective Sample Size (ESS) Expressions\n\nLet $\\{Y_t\\}_{t=1}^{N}$ be a weakly stationary time series with mean $\\mu$, variance $\\sigma^2$, and autocorrelation function $\\rho_k = \\text{Corr}(Y_t, Y_{t+k})$. The variance of the sample mean, $\\bar{Y}_N = \\frac{1}{N}\\sum_{t=1}^{N} Y_t$, is given by:\n$$\n\\text{Var}(\\bar{Y}_N) = \\text{Var}\\left(\\frac{1}{N}\\sum_{t=1}^{N} Y_t\\right) = \\frac{1}{N^2} \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\text{Cov}(Y_i, Y_j)\n$$\nSince the process is stationary, $\\text{Cov}(Y_i, Y_j) = \\sigma^2 \\rho_{|i-j|}$. The double summation can be rewritten by grouping terms with the same lag $k = |i-j|$:\n$$\n\\text{Var}(\\bar{Y}_N) = \\frac{\\sigma^2}{N^2} \\left[ N\\rho_0 + 2\\sum_{k=1}^{N-1}(N-k)\\rho_k \\right]\n$$\nWith $\\rho_0 = 1$, this simplifies to:\n$$\n\\text{Var}(\\bar{Y}_N) = \\frac{\\sigma^2}{N} \\left[ 1 + 2\\sum_{k=1}^{N-1}\\left(1-\\frac{k}{N}\\right)\\rho_k \\right]\n$$\nThe problem specifies the autocorrelation function $\\rho_k = \\beta^k$ for $k \\geq 1$. Substituting this into the variance formula:\n$$\n\\text{Var}(\\bar{Y}_N) = \\frac{\\sigma^2}{N} \\left[ 1 + 2\\sum_{k=1}^{N-1}\\left(1-\\frac{k}{N}\\right)\\beta^k \\right]\n$$\nThe Effective Sample Size, $\\text{ESS}_N$, is defined as the size of an independent and identically distributed (i.i.d.) sample that yields the same variance for the sample mean. For an i.i.d. sample of size $\\text{ESS}_N$, the variance of the mean is $\\frac{\\sigma^2}{\\text{ESS}_N}$. Equating the two variance expressions:\n$$\n\\frac{\\sigma^2}{\\text{ESS}_N} = \\frac{\\sigma^2}{N} \\left[ 1 + 2\\sum_{k=1}^{N-1}\\left(1-\\frac{k}{N}\\right)\\beta^k \\right]\n$$\nSolving for $\\text{ESS}_N$:\n$$\n\\text{ESS}_N = \\frac{N}{1 + 2\\sum_{k=1}^{N-1}\\left(1-\\frac{k}{N}\\right)\\beta^k} = \\frac{N^2}{N + 2\\sum_{k=1}^{N-1}(N-k)\\beta^k}\n$$\nTo obtain an explicit function, we must evaluate the sum $S = \\sum_{k=1}^{N-1}(N-k)\\beta^k$:\n$$\nS = N\\sum_{k=1}^{N-1}\\beta^k - \\sum_{k=1}^{N-1}k\\beta^k\n$$\nThe first term is a geometric series: $\\sum_{k=1}^{N-1}\\beta^k = \\frac{\\beta(1-\\beta^{N-1})}{1-\\beta}$.\nThe second term is an arithmetico-geometric series, for which the sum is $\\sum_{k=1}^{M} kx^k = \\frac{x(1-(M+1)x^M + Mx^{M+1})}{(1-x)^2}$. With $M=N-1$ and $x=\\beta$:\n$$\n\\sum_{k=1}^{N-1}k\\beta^k = \\frac{\\beta(1-N\\beta^{N-1} + (N-1)\\beta^N)}{(1-\\beta)^2}\n$$\nCombining these terms:\n$$\nS = N\\frac{\\beta(1-\\beta^{N-1})}{1-\\beta} - \\frac{\\beta(1-N\\beta^{N-1} + (N-1)\\beta^N)}{(1-\\beta)^2}\n$$\n$$\nS = \\frac{N\\beta(1-\\beta^{N-1})(1-\\beta) - \\beta(1-N\\beta^{N-1} + (N-1)\\beta^N)}{(1-\\beta)^2}\n$$\nThe numerator simplifies to:\n$$\n(N\\beta - N\\beta^2 - N\\beta^N + N\\beta^{N+1}) - (\\beta - N\\beta^N + N\\beta^{N+1} - \\beta^{N+1}) = N\\beta - N\\beta^2 - \\beta + \\beta^{N+1} = N\\beta(1-\\beta) - \\beta + \\beta^{N+1}\n$$\nSo, the sum is $S = \\frac{N\\beta(1-\\beta) - \\beta + \\beta^{N+1}}{(1-\\beta)^2}$.\nSubstituting this back into the expression for $\\text{ESS}_N$:\n$$\n\\text{ESS}_N = \\frac{N^2}{N + \\frac{2(N\\beta(1-\\beta) - \\beta + \\beta^{N+1})}{(1-\\beta)^2}}\n$$\nThis can be rewritten as:\n$$\n\\text{ESS}_N = \\frac{N^2}{N + \\frac{2N\\beta}{1-\\beta} - \\frac{2\\beta}{(1-\\beta)^2} + \\frac{2\\beta^{N+1}}{(1-\\beta)^2}}\n$$\nThis is the general expression for the ESS of a sample of size $N$ with the given autocorrelation structure.\n\n1.  **ESS of the full run**: The full run has length $n$. Thus, we set $N=n$.\n    $$\n    \\text{ESS}_{\\text{full}}(n, \\beta) = \\frac{n^2}{n + \\frac{2n\\beta}{1-\\beta} - \\frac{2\\beta}{(1-\\beta)^2} + \\frac{2\\beta^{n+1}}{(1-\\beta)^2}}\n    $$\n\n2.  **ESS of the deleted run**: The deleted run removes the first $m$ observations, leaving a series of length $n-m$. Since the original process is stationary, the remaining series is statistically equivalent to a stationary series of length $N = n-m$.\n    $$\n    \\text{ESS}_{\\text{deleted}}(n, m, \\beta) = \\frac{(n-m)^2}{(n-m) + \\frac{2(n-m)\\beta}{1-\\beta} - \\frac{2\\beta}{(1-\\beta)^2} + \\frac{2\\beta^{(n-m)+1}}{(1-\\beta)^2}}\n    $$\n\n### Task 2: Numerical Evaluation and Ratio\n\nWe are given $n = 10^5$, $\\beta = 0.99$, and $m = \\lfloor n^{1/2} \\rfloor$.\nFirst, calculate $m$:\n$$\nm = \\lfloor(10^5)^{1/2}\\rfloor = \\lfloor 100\\sqrt{10} \\rfloor \\approx \\lfloor 316.2277 \\rfloor = 316\n$$\nThe length of the deleted run is $n-m = 10^5 - 316 = 99684$.\nThe value of $1-\\beta = 1 - 0.99 = 0.01$.\n\nNow, we evaluate the terms in the ESS formulas.\nThe term $\\beta^{N+1}$ for $N=n$ or $N=n-m$ is extremely small. For instance, $(0.99)^{100001} \\approx \\exp(-1000.01) \\approx 0$, so this term can be safely neglected in the numerical calculation.\n\nFor the full run ($N=n=10^5$):\nThe denominator is $D_{\\text{full}} = n + \\frac{2n\\beta}{1-\\beta} - \\frac{2\\beta}{(1-\\beta)^2}$.\n$$\nD_{\\text{full}} = 10^5 + \\frac{2(10^5)(0.99)}{0.01} - \\frac{2(0.99)}{(0.01)^2} = 100000 + 19800000 - 19800 = 19880200\n$$\n$$\n\\text{ESS}_{\\text{full}} = \\frac{(10^5)^2}{19880200} = \\frac{10^{10}}{19880200} \\approx 503.0130\n$$\n\nFor the deleted run ($N=n-m=99684$):\nThe denominator is $D_{\\text{deleted}} = (n-m) + \\frac{2(n-m)\\beta}{1-\\beta} - \\frac{2\\beta}{(1-\\beta)^2}$.\n$$\nD_{\\text{deleted}} = 99684 + \\frac{2(99684)(0.99)}{0.01} - 19800 = 99684 + 19737432 - 19800 = 19817316\n$$\n$$\n\\text{ESS}_{\\text{deleted}} = \\frac{(99684)^2}{19817316} = \\frac{9936899856}{19817316} \\approx 501.4285\n$$\n\nThe ratio $R$ is:\n$$\nR = \\frac{\\text{ESS}_{\\text{deleted}}}{\\text{ESS}_{\\text{full}}} = \\frac{501.4285}{503.0130} \\approx 0.99684999\n$$\nRounding to four significant figures, $R = 0.9968$.\n\n### Task 3: Justification of Deletion Strategy\n\nThe justification for deleting initial data rests on the trade-off between reducing initialization bias and increasing the variance of the estimator. The Mean Squared Error (MSE) of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is $\\text{MSE}(\\hat{\\theta}) = \\text{Var}(\\hat{\\theta}) + [\\text{Bias}(\\hat{\\theta})]^2$.\n\nLet $\\bar{X}_{m,n} = \\frac{1}{n-m} \\sum_{t=m+1}^n X_t$ be the estimator for $\\mu$ after deleting $m$ points.\nThe problem states that the transient mean is $E[X_t] = \\mu + \\delta\\beta^{t-1}$.\nThe bias of the estimator is:\n$$\n\\text{Bias}(\\bar{X}_{m,n}) = E[\\bar{X}_{m,n}] - \\mu = \\frac{1}{n-m}\\sum_{t=m+1}^{n} (\\mu + \\delta\\beta^{t-1}) - \\mu = \\frac{\\delta}{n-m}\\sum_{t=m+1}^{n}\\beta^{t-1}\n$$\nThe sum is a geometric series: $\\sum_{j=m}^{n-1}\\beta^j = \\frac{\\beta^m(1-\\beta^{n-m})}{1-\\beta}$. For large $n$, $1-\\beta^{n-m} \\approx 1$.\n$$\n\\text{Bias}(\\bar{X}_{m,n}) \\approx \\frac{\\delta \\beta^m}{(n-m)(1-\\beta)}\n$$\nThe squared bias therefore scales as $[\\text{Bias}(\\bar{X}_{m,n})]^2 \\propto (n-m)^{-2}\\beta^{2m}$.\n\nThe variance of the estimator, assuming the transient part does not affect the covariance structure, is the variance of a sample mean of a stationary series of length $N=n-m$. For large $N$ and $\\beta$ close to $1$, $\\text{ESS}_N \\approx N \\frac{1-\\beta}{1+\\beta}$.\n$$\n\\text{Var}(\\bar{X}_{m,n}) = \\frac{\\sigma^2}{\\text{ESS}_{n-m}} \\approx \\frac{\\sigma^2(1+\\beta)}{(n-m)(1-\\beta)}\n$$\nThe variance scales as $\\text{Var}(\\bar{X}_{m,n}) \\propto (n-m)^{-1}$.\n\nNow, we compare the MSE for $m=0$ (no deletion) and $m=\\lfloor n^{1/2} \\rfloor \\approx n^{1/2}$.\n\nCase $m=0$:\n- $\\text{Var} \\propto n^{-1}$.\n- $[\\text{Bias}]^2 \\propto n^{-2}$.\nFor large $n$, the variance term $O(n^{-1})$ dominates the squared bias term $O(n^{-2})$. The bias is already a higher-order error term.\n\nCase $m \\approx n^{1/2}$:\n- $\\text{Var} \\propto (n-n^{1/2})^{-1} \\approx n^{-1}(1+n^{-1/2})$. The variance term is increased relative to the $m=0$ case.\n- $[\\text{Bias}]^2 \\propto (n-n^{1/2})^{-2} \\beta^{2n^{1/2}} \\approx n^{-2} \\beta^{2\\sqrt{n}}$. The bias is reduced by an exponential factor $\\beta^{2\\sqrt{n}}$, which goes to zero extremely quickly.\n\nLet's analyze the change in MSE.\nThe increase in variance is:\n$$\n\\Delta\\text{Var} = \\text{Var}_{m=\\sqrt{n}} - \\text{Var}_{m=0} \\approx \\frac{C_V}{n-\\sqrt{n}} - \\frac{C_V}{n} = \\frac{C_V\\sqrt{n}}{n(n-\\sqrt{n})} \\approx C_V n^{-3/2}\n$$\nwhere $C_V = \\frac{\\sigma^2(1+\\beta)}{1-\\beta}$. The increase in variance is of order $O(n^{-3/2})$.\n\nThe reduction in squared bias is:\n$$\n\\Delta[\\text{Bias}]^2 = [\\text{Bias}]_{m=0}^2 - [\\text{Bias}]_{m=\\sqrt{n}}^2 \\approx \\frac{C_B}{n^2} - \\frac{C_B \\beta^{2\\sqrt{n}}}{n^2} \\approx \\frac{C_B}{n^2}\n$$\nwhere $C_B = (\\frac{\\delta}{1-\\beta})^2$. The reduction in squared bias is of order $O(n^{-2})$.\n\nFor large $n$, the term $n^{-3/2}$ is asymptotically larger than $n^{-2}$. This means the increase in variance from deleting $\\sqrt{n}$ samples is asymptotically greater than the corresponding reduction in squared bias. Therefore, the total MSE is increased by this deletion strategy.\n\nConclusion: The deletion strategy $m=\\lfloor n^{1/2} \\rfloor$ is **not justified** from an MSE perspective for large run lengths $n$. It aggressively reduces an already higher-order bias term at the cost of a more significant increase in the dominant variance term. A much smaller deletion size, such as $m=O(1)$ or $m=O(\\ln n)$, would be more appropriate for minimizing the overall MSE.", "answer": "$$\n\\boxed{0.9968}\n$$", "id": "3347933"}, {"introduction": "The ultimate goal of many steady-state simulations is to produce a reliable confidence interval for a performance measure. This final exercise integrates the concepts of bias mitigation and variance estimation into a complete, practical procedure for output analysis [@problem_id:3347878]. It tests your ability to correctly combine warm-up deletion to handle bias with the batch means method to handle autocorrelation, ensuring the final statistical estimate is both valid and credible.", "problem": "Consider a Harris ergodic and aperiodic Markov chain $\\{X_t\\}_{t \\ge 0}$ on a general state space with invariant distribution $\\pi$. Let $f$ be a measurable function such that $E_{\\pi}[|f(X)|]  \\infty$ and define the steady-state mean $\\pi f = \\int f(x)\\,\\pi(dx)$. Suppose the chain is initialized out of stationarity at $X_0 \\sim \\mu \\neq \\pi$ and is simulated for $n$ steps, producing the time series $Y_t = f(X_t)$ for $t = 1, \\dots, n$. To mitigate initialization bias, you discard a warm-up of $w$ observations and retain $n' = n - w$ observations $\\{Y_{w+1}, \\dots, Y_{n}\\}$. Denote the post-warm-up average by $\\bar{f}_{n'} = \\frac{1}{n'} \\sum_{t=w+1}^{n} Y_t$. Under standard strong-mixing conditions, the Central Limit Theorem (CLT) for stationary sequences asserts that $\\sqrt{n'}(\\bar{f}_{n'} - \\pi f) \\Rightarrow \\mathcal{N}(0, \\tau^2)$, where the time-average variance constant $\\tau^2$ is finite and equals the spectral density at frequency zero or, equivalently, the sum of covariances $\\tau^2 = \\sum_{h=-\\infty}^{\\infty} \\operatorname{Cov}_{\\pi}(Y_0, Y_h)$ when the series is absolutely summable.\n\nTo estimate $\\tau^2$, you apply the non-overlapping batch means method: choose a positive integer batch size $k$ such that $n' = b k$ for an integer $b \\ge 2$, form batch means $M_j = \\frac{1}{k} \\sum_{t=(j-1)k + w + 1}^{j k + w} Y_t$ for $j = 1, \\dots, b$, and compute their empirical average $\\bar{M} = \\frac{1}{b} \\sum_{j=1}^{b} M_j$ and sample variance $s_b^2 = \\frac{1}{b-1} \\sum_{j=1}^{b} (M_j - \\bar{M})^2$. Under standard conditions ensuring weak dependence and sufficiently large $k$, the batch-means estimator $\\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2$ is consistent for $\\tau^2$, the batch means $M_j$ are approximately independent and approximately normal, and $\\bar{M} = \\bar{f}_{n'}$ by construction.\n\nWhich option below correctly constructs a $(1-\\alpha)$ confidence interval for $\\pi f$ based on $\\bar{f}_{n'}$ and the batch-means estimator of $\\tau^2$, and provides the most principled justification for using either Student’s $t$-distribution or standard normal quantiles in finite samples?\n\nA. Use the full series without warm-up deletion and invoke the CLT directly: the interval is $\\bar{f}_{n} \\pm z_{1-\\alpha/2}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n}}$, where $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$ standard normal quantile and $\\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2$ is computed from $b$ batches over all $n$ observations. Justification: the CLT provides asymptotic normality, so normal quantiles are appropriate even in finite samples.\n\nB. Delete a warm-up of $w$ observations and partition the remaining $n' = b k$ observations into $b$ non-overlapping batches of size $k$. Form $\\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2$ from the $b$ batch means and use the interval $\\bar{f}_{n'} \\pm t_{1-\\alpha/2,\\, b-1}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n'}}$, where $t_{1-\\alpha/2,\\, b-1}$ is the $(1-\\alpha/2)$ quantile of Student’s $t$-distribution with $b-1$ degrees of freedom. Justification: after warm-up deletion, the batches are approximately independent and approximately normal; since $\\tau^2$ is estimated from $b$ batch means, using Studentization with $b-1$ degrees of freedom yields a finite-sample correction relative to the normal limit, becoming asymptotically equivalent to normal quantiles as $b \\to \\infty$.\n\nC. After warm-up deletion, treat the raw post-warm-up observations as independent and use the usual $t$-interval with $n'-1$ degrees of freedom: the interval is $\\bar{f}_{n'} \\pm t_{1-\\alpha/2,\\, n'-1}\\,\\frac{s_{n'}}{\\sqrt{n'}}$, where $s_{n'}^2 = \\frac{1}{n'-1} \\sum_{t=w+1}^{n} (Y_t - \\bar{f}_{n'})^2$. Justification: estimating the variance from all $n'$ dependent observations induces a Student’s $t$ pivot with $n'-1$ degrees of freedom.\n\nD. Delete warm-up and use the asymptotic CLT with the consistent batch-means estimator, but retain normal quantiles: the interval is $\\bar{f}_{n'} \\pm z_{1-\\alpha/2}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n'}}$ with $\\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2$. Justification: the CLT implies asymptotic normality regardless of variance estimation, so normal quantiles remain valid in finite samples; using Student’s $t$ is unnecessary without exact normality and independence.\n\nE. Delete warm-up and use Student’s $t$ with $b-1$ degrees of freedom, but estimate $\\tau^2$ by $\\hat{\\tau}_{\\mathrm{BM}}^2 = \\frac{s_b^2}{k}$. Justification: the batch means are approximately independent, so dividing by the batch size $k$ corrects for inflation in $s_b^2$, and Student’s $t$ accounts for finite-sample uncertainty.", "solution": "We begin from the fundamental definitions and well-tested results. The steady-state quantity of interest is $\\pi f = E_{\\pi}[f(X)]$. For steady-state simulation of a dependent sequence $\\{Y_t\\}$ derived from a Harris ergodic Markov chain, standard strong-mixing conditions and the Central Limit Theorem (CLT) for stationary sequences yield\n$$\n\\sqrt{n'}\\left(\\bar{f}_{n'} - \\pi f\\right) \\Rightarrow \\mathcal{N}(0, \\tau^2),\n$$\nwhere $\\bar{f}_{n'} = \\frac{1}{n'} \\sum_{t=w+1}^{n} Y_t$, $n' = n - w$ reflects warm-up deletion, and $\\tau^2$ equals the spectral density at zero (equivalently $\\tau^2 = \\sum_{h=-\\infty}^{\\infty}\\operatorname{Cov}_{\\pi}(Y_0,Y_h)$ when the auto-covariances are absolutely summable). The CLT is the fundamental base: it governs the asymptotic behavior of the sample mean and motivates confidence interval construction.\n\nInitialization bias arises because $X_0 \\sim \\mu \\neq \\pi$. Under geometric ergodicity, $E_{\\mu}[f(X_t)] \\to E_{\\pi}[f(X)]$ at a geometric rate, and the bias of $\\bar{f}_{n}$ decays on the order of $O(1/n)$; however, in finite samples, especially moderate $n$, this bias can be non-negligible. Therefore, warm-up deletion of $w$ observations is a principled approach to ensure the retained sequence is approximately stationary, thereby aligning the finite-sample distribution of $\\bar{f}_{n'}$ more closely with the CLT’s stationary premise.\n\nTo estimate the time-average variance constant $\\tau^2$, the non-overlapping batch means method partitions the post-warm-up sequence into $b$ contiguous batches of size $k$ with $n' = b k$. For batch $j$, define\n$$\nM_j = \\frac{1}{k} \\sum_{t=(j-1)k+w+1}^{jk+w} Y_t, \\quad j = 1, \\dots, b.\n$$\nLet $\\bar{M} = \\frac{1}{b} \\sum_{j=1}^{b} M_j$ and\n$$\ns_b^2 = \\frac{1}{b-1}\\sum_{j=1}^{b} \\left(M_j - \\bar{M}\\right)^2.\n$$\nUnder mixing and for sufficiently large $k$, each $M_j$ is approximately normal with\n$$\n\\operatorname{Var}(M_j) \\approx \\frac{\\tau^2}{k}.\n$$\nConsequently, $s_b^2$ estimates $\\operatorname{Var}(M_j)$, and the batch-means estimator\n$$\n\\hat{\\tau}_{\\mathrm{BM}}^2 = k\\, s_b^2\n$$\nis consistent for $\\tau^2$. Moreover, $\\bar{M} = \\bar{f}_{n'}$ by construction.\n\nA confidence interval for $\\pi f$ follows from the CLT and a consistent estimator of $\\tau^2$. As $n' \\to \\infty$ (equivalently $b \\to \\infty$ with $k \\to \\infty$), the standardized statistic\n$$\nZ_{n'} = \\frac{\\bar{f}_{n'} - \\pi f}{\\hat{\\tau}_{\\mathrm{BM}}/\\sqrt{n'}}\n$$\nconverges in distribution to $\\mathcal{N}(0,1)$, yielding the asymptotically valid interval\n$$\n\\bar{f}_{n'} \\pm z_{1-\\alpha/2}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n'}}.\n$$\nHowever, in finite samples, there is additional uncertainty introduced by estimating $\\tau^2$ from a finite number $b$ of batch means. When the $M_j$ are approximately independent and approximately normal (a premise strengthened by larger $k$ and adequate mixing), Studentization with the sample variance $s_b^2$ across $b$ nearly independent normals leads to an approximate Student’s $t$ distribution with $b-1$ degrees of freedom:\n$$\nT_{b} = \\frac{\\bar{M} - \\pi f}{\\sqrt{s_b^2/b}} \\approx t_{b-1}.\n$$\nEquivalently,\n$$\n\\frac{\\bar{f}_{n'} - \\pi f}{\\hat{\\tau}_{\\mathrm{BM}}/\\sqrt{n'}} = \\frac{\\bar{M} - \\pi f}{\\sqrt{s_b^2/b}} \\approx t_{b-1}.\n$$\nThis finite-sample correction inflates the quantiles relative to $z_{1-\\alpha/2}$ when $b$ is not large, providing more reliable coverage by accounting for the variability in the variance estimator. As $b \\to \\infty$, $t_{b-1} \\to z$, recovering the asymptotic normal interval.\n\nWe now analyze each option:\n\nA. The proposed interval is\n$$\n\\bar{f}_{n} \\pm z_{1-\\alpha/2}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n}}.\n$$\nThis ignores initialization bias by using all $n$ observations without warm-up deletion, violating the stationary premise needed for the CLT to be applicable in finite samples. Furthermore, it relies on standard normal quantiles without addressing the finite-sample uncertainty arising from estimating $\\tau^2$ from a finite number of batches. In small to moderate $b$, this can under-cover. Verdict: Incorrect.\n\nB. The proposed interval is\n$$\n\\bar{f}_{n'} \\pm t_{1-\\alpha/2,\\, b-1}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n'}}, \\quad \\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2,\n$$\nwith $n' = b k$ post-warm-up observations. This construction accounts for initialization bias via warm-up deletion, uses a consistent estimator $\\hat{\\tau}_{\\mathrm{BM}}^2$ derived from the variance of batch means, and employs Student’s $t$ quantiles with $b-1$ degrees of freedom to address finite-sample uncertainty due to estimating the variance from $b$ approximately independent and approximately normal batch means. As $b$ grows, the $t$ quantiles approach normal quantiles, ensuring asymptotic validity. Verdict: Correct.\n\nC. The proposed interval is\n$$\n\\bar{f}_{n'} \\pm t_{1-\\alpha/2,\\, n'-1}\\,\\frac{s_{n'}}{\\sqrt{n'}}, \\quad s_{n'}^2 = \\frac{1}{n'-1}\\sum_{t=w+1}^{n} \\left(Y_t - \\bar{f}_{n'}\\right)^2.\n$$\nThis treats the autocorrelated sequence $\\{Y_t\\}$ as independent and bases variance estimation on the sample variance of the raw dependent data, which estimates $\\operatorname{Var}(Y_t)$, not the time-average variance constant $\\tau^2$ governing the CLT for $\\bar{f}_{n'}$. The exact $t$-pivot with $n'-1$ degrees of freedom requires independent and normally distributed observations, which does not hold here. Verdict: Incorrect.\n\nD. The proposed interval is\n$$\n\\bar{f}_{n'} \\pm z_{1-\\alpha/2}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n'}}, \\quad \\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2.\n$$\nThis deletes warm-up and uses the correct batch-means estimator, yielding an asymptotically valid interval by the CLT. However, the justification explicitly claims normal quantiles are valid in finite samples without accounting for the additional uncertainty in estimating $\\tau^2$ from $b$ batch means. In practice, with moderate $b$, using $t_{b-1}$ is more principled and yields better finite-sample coverage. The prompt specifically asks to justify the choice between Student’s $t$ and normal quantiles in finite samples; this option fails to provide the finite-sample correction. Verdict: Incorrect for the stated objective.\n\nE. The proposed interval uses $t$ quantiles but sets\n$$\n\\hat{\\tau}_{\\mathrm{BM}}^2 = \\frac{s_b^2}{k},\n$$\nso the interval is\n$$\n\\bar{f}_{n'} \\pm t_{1-\\alpha/2,\\, b-1}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n'}} = \\bar{f}_{n'} \\pm t_{1-\\alpha/2,\\, b-1}\\,\\frac{\\sqrt{s_b^2/k}}{\\sqrt{n'}}.\n$$\nThis inverts the correct scaling: since $\\operatorname{Var}(M_j) \\approx \\tau^2/k$, we have $s_b^2 \\approx \\tau^2/k$, so the consistent estimator is $\\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2$, not $s_b^2/k$. The proposed scaling underestimates the variance and leads to undercoverage. Verdict: Incorrect.\n\nIn summary, the principled finite-sample construction is to delete an appropriate warm-up, estimate $\\tau^2$ via batch means with $\\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2$, and use Student’s $t$ quantiles with $b-1$ degrees of freedom:\n$$\n\\text{CI}_{1-\\alpha}: \\quad \\bar{f}_{n'} \\pm t_{1-\\alpha/2,\\, b-1}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n'}}.\n$$\nThis yields conservative coverage for moderate $b$ and recovers the asymptotic normal interval as $b \\to \\infty$.", "answer": "$$\\boxed{B}$$", "id": "3347878"}]}