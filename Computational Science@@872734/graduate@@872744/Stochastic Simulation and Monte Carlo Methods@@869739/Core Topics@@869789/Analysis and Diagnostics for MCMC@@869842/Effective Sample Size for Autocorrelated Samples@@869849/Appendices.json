{"hands_on_practices": [{"introduction": "Mastering the concept of Effective Sample Size ($n_{\\text{eff}}$) begins with a firm grasp of its mathematical foundations. This exercise challenges you to derive the $n_{\\text{eff}}$ from first principles for a stationary autoregressive moving-average (ARMA(1,1)) process, a versatile model for time series data. By working through the derivation, you will solidify your understanding of how a process's underlying parameters directly influence its autocorrelation structure and, consequently, the information content of the samples it generates [@problem_id:3304666].", "problem": "Consider a strictly stationary autoregressive moving-average process of order (1,1), denoted $X_t$, defined by the recursion $X_t = \\phi X_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1}$, where $|\\phi|  1$, $\\{\\epsilon_t\\}$ is a zero-mean independent and identically distributed white noise sequence with variance $\\sigma_{\\epsilon}^{2}$, and $\\phi,\\theta \\in \\mathbb{R}$. Let the mean of $X_t$ be zero. For a stationary process, the autocovariance function is $\\gamma_k = \\operatorname{Cov}(X_t,X_{t+k})$ and the autocorrelation function is $\\rho_k = \\gamma_k/\\gamma_0$. The integrated autocorrelation time (IAT) is defined by $\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k$. For the Monte Carlo estimator of the mean $\\bar{X}_n = \\frac{1}{n}\\sum_{t=1}^{n} X_t$, assume its variance can be written as $\\operatorname{Var}(\\bar{X}_n) = \\operatorname{Var}(X_t)\\,\\tau_{\\mathrm{int}}/n = \\operatorname{Var}(X_t)/n_{\\mathrm{eff}}$, thereby defining the effective sample size $n_{\\mathrm{eff}}$.\n\nStarting only from these definitions and the given data-generating equation, and without using any pre-tabulated formulas for autoregressive moving-average processes, derive the closed-form expression of the autocorrelation function $\\rho_k$ for all integers $k \\geq 1$. Then, use it to obtain closed-form expressions for $\\tau_{\\mathrm{int}}$ and $n_{\\mathrm{eff}}$ as functions of $\\phi$, $\\theta$, and $n$. Your final answer must be a closed-form analytic expression for both $\\tau_{\\mathrm{int}}$ and $n_{\\mathrm{eff}}$; do not substitute numerical values and do not include physical units. If any simplification is possible, present the simplified form. The final boxed answer should contain only the expressions for $\\tau_{\\mathrm{int}}$ and $n_{\\mathrm{eff}}$ (in that order), formatted as a row matrix. No rounding is required.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. It presents a standard problem in time series analysis without any factual unsoundness, ambiguity, or contradiction. We can therefore proceed with the derivation.\n\nThe goal is to derive the autocorrelation function ($\\rho_k$), the integrated autocorrelation time ($\\tau_{\\mathrm{int}}$), and the effective sample size ($n_{\\mathrm{eff}}$) for the given strictly stationary ARMA(1,1) process:\n$$\nX_t = \\phi X_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1}\n$$\nwhere $\\{\\epsilon_t\\}$ is a zero-mean white noise process with variance $\\sigma_{\\epsilon}^{2}$, and $|\\phi|  1$. The mean of the process is given as $\\operatorname{E}[X_t] = 0$.\n\nFirst, we derive the autocovariance function $\\gamma_k = \\operatorname{Cov}(X_t, X_{t+k}) = \\operatorname{E}[X_t X_{t+k}]$.\n\n**Step 1: Calculate the variance, $\\gamma_0$.**\nThe variance is $\\gamma_0 = \\operatorname{Var}(X_t) = \\operatorname{E}[X_t^2]$. Using the process definition:\n$$\n\\gamma_0 = \\operatorname{E}[(\\phi X_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1})^2]\n$$\nExpanding the square and taking the expectation of each term:\n$$\n\\gamma_0 = \\operatorname{E}[\\phi^2 X_{t-1}^2 + \\epsilon_t^2 + \\theta^2 \\epsilon_{t-1}^2 + 2\\phi X_{t-1}\\epsilon_t + 2\\phi\\theta X_{t-1}\\epsilon_{t-1} + 2\\theta \\epsilon_t\\epsilon_{t-1}]\n$$\nWe evaluate each term using the properties of the process:\n- By stationarity, $\\operatorname{E}[X_{t-1}^2] = \\gamma_0$.\n- By definition of white noise, $\\operatorname{E}[\\epsilon_t^2] = \\sigma_{\\epsilon}^2$ and $\\operatorname{E}[\\epsilon_{t-1}^2] = \\sigma_{\\epsilon}^2$.\n- $\\epsilon_t$ is independent of past values of the process, so $\\operatorname{E}[X_{t-1}\\epsilon_t] = \\operatorname{E}[X_{t-1}]\\operatorname{E}[\\epsilon_t] = 0$.\n- $\\epsilon_t$ and $\\epsilon_{t-1}$ are uncorrelated, so $\\operatorname{E}[\\epsilon_t\\epsilon_{t-1}] = 0$.\n- To evaluate $\\operatorname{E}[X_{t-1}\\epsilon_{t-1}]$, we substitute the definition of $X_{t-1}$:\n$$\n\\operatorname{E}[X_{t-1}\\epsilon_{t-1}] = \\operatorname{E}[(\\phi X_{t-2} + \\epsilon_{t-1} + \\theta \\epsilon_{t-2})\\epsilon_{t-1}] = \\phi\\operatorname{E}[X_{t-2}\\epsilon_{t-1}] + \\operatorname{E}[\\epsilon_{t-1}^2] + \\theta\\operatorname{E}[\\epsilon_{t-2}\\epsilon_{t-1}] = 0 + \\sigma_{\\epsilon}^2 + 0 = \\sigma_{\\epsilon}^2\n$$\nSubstituting these back into the expression for $\\gamma_0$:\n$$\n\\gamma_0 = \\phi^2\\gamma_0 + \\sigma_{\\epsilon}^2 + \\theta^2\\sigma_{\\epsilon}^2 + 0 + 2\\phi\\theta\\sigma_{\\epsilon}^2 + 0\n$$\nSolving for $\\gamma_0$:\n$$\n\\gamma_0(1-\\phi^2) = \\sigma_{\\epsilon}^2(1 + 2\\phi\\theta + \\theta^2)\n$$\n$$\n\\gamma_0 = \\sigma_{\\epsilon}^2 \\frac{1 + 2\\phi\\theta + \\theta^2}{1 - \\phi^2}\n$$\n\n**Step 2: Calculate the autocovariance for lag $k=1$, $\\gamma_1$.**\n$$\n\\gamma_1 = \\operatorname{E}[X_t X_{t+1}] = \\operatorname{E}[X_t (\\phi X_t + \\epsilon_{t+1} + \\theta \\epsilon_t)]\n$$\n$$\n\\gamma_1 = \\phi\\operatorname{E}[X_t^2] + \\operatorname{E}[X_t\\epsilon_{t+1}] + \\theta\\operatorname{E}[X_t\\epsilon_t]\n$$\n- $\\operatorname{E}[X_t^2] = \\gamma_0$.\n- $\\epsilon_{t+1}$ is independent of $X_t$, so $\\operatorname{E}[X_t\\epsilon_{t+1}] = 0$.\n- Using the same logic as for $\\operatorname{E}[X_{t-1}\\epsilon_{t-1}]$, we find $\\operatorname{E}[X_t\\epsilon_t] = \\sigma_{\\epsilon}^2$.\nTherefore:\n$$\n\\gamma_1 = \\phi\\gamma_0 + \\theta\\sigma_{\\epsilon}^2\n$$\n\n**Step 3: Calculate the autocovariance for lags $k \\geq 2$, $\\gamma_k$.**\n$$\n\\gamma_k = \\operatorname{E}[X_t X_{t+k}] = \\operatorname{E}[X_t (\\phi X_{t+k-1} + \\epsilon_{t+k} + \\theta \\epsilon_{t+k-1})]\n$$\n$$\n\\gamma_k = \\phi\\operatorname{E}[X_t X_{t+k-1}] + \\operatorname{E}[X_t\\epsilon_{t+k}] + \\theta\\operatorname{E}[X_t\\epsilon_{t+k-1}]\n$$\n- $\\operatorname{E}[X_t X_{t+k-1}] = \\gamma_{k-1}$.\n- For $k \\geq 2$, both $t+k$ and $t+k-1$ are greater than $t$. Thus, $\\epsilon_{t+k}$ and $\\epsilon_{t+k-1}$ are independent of $X_t$. This implies $\\operatorname{E}[X_t\\epsilon_{t+k}] = 0$ and $\\operatorname{E}[X_t\\epsilon_{t+k-1}] = 0$.\nWe obtain the recurrence relation for $k \\geq 2$:\n$$\n\\gamma_k = \\phi\\gamma_{k-1}\n$$\n\n**Step 4: Derive the autocorrelation function, $\\rho_k$.**\nThe autocorrelation function is $\\rho_k = \\gamma_k/\\gamma_0$.\nFor $k \\geq 1$, we have $\\rho_k = \\phi^{k-1}\\rho_1$.\nNow we find $\\rho_1$:\n$$\n\\rho_1 = \\frac{\\gamma_1}{\\gamma_0} = \\frac{\\phi\\gamma_0 + \\theta\\sigma_{\\epsilon}^2}{\\gamma_0} = \\phi + \\frac{\\theta\\sigma_{\\epsilon}^2}{\\gamma_0}\n$$\nSubstitute the expression for $\\gamma_0$:\n$$\n\\rho_1 = \\phi + \\frac{\\theta\\sigma_{\\epsilon}^2}{\\sigma_{\\epsilon}^2 \\frac{1 + 2\\phi\\theta + \\theta^2}{1 - \\phi^2}} = \\phi + \\frac{\\theta(1 - \\phi^2)}{1 + 2\\phi\\theta + \\theta^2}\n$$\nBringing to a common denominator:\n$$\n\\rho_1 = \\frac{\\phi(1 + 2\\phi\\theta + \\theta^2) + \\theta(1 - \\phi^2)}{1 + 2\\phi\\theta + \\theta^2} = \\frac{\\phi + 2\\phi^2\\theta + \\phi\\theta^2 + \\theta - \\theta\\phi^2}{1 + 2\\phi\\theta + \\theta^2}\n$$\n$$\n\\rho_1 = \\frac{\\phi + \\theta + \\phi^2\\theta + \\phi\\theta^2}{1 + 2\\phi\\theta + \\theta^2} = \\frac{(\\phi + \\theta)(1 + \\phi\\theta)}{1 + 2\\phi\\theta + \\theta^2}\n$$\nSo, the autocorrelation function for $k \\geq 1$ is:\n$$\n\\rho_k = \\phi^{k-1} \\left( \\frac{(\\phi + \\theta)(1 + \\phi\\theta)}{1 + 2\\phi\\theta + \\theta^2} \\right)\n$$\n\n**Step 5: Derive the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$.**\nThe definition is $\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k$.\nThe sum is a geometric series:\n$$\n\\sum_{k=1}^{\\infty} \\rho_k = \\sum_{k=1}^{\\infty} \\phi^{k-1}\\rho_1 = \\rho_1 \\sum_{j=0}^{\\infty} \\phi^j\n$$\nSince $|\\phi|1$, the series converges to $1/(1-\\phi)$.\n$$\n\\sum_{k=1}^{\\infty} \\rho_k = \\frac{\\rho_1}{1-\\phi}\n$$\nSubstituting this into the definition of $\\tau_{\\mathrm{int}}$:\n$$\n\\tau_{\\mathrm{int}} = 1 + \\frac{2\\rho_1}{1-\\phi} = 1 + \\frac{2}{1-\\phi} \\frac{(\\phi + \\theta)(1 + \\phi\\theta)}{1 + 2\\phi\\theta + \\theta^2}\n$$\nTo simplify, we combine the terms over a common denominator:\n$$\n\\tau_{\\mathrm{int}} = \\frac{(1-\\phi)(1 + 2\\phi\\theta + \\theta^2) + 2(\\phi + \\theta)(1 + \\phi\\theta)}{(1-\\phi)(1 + 2\\phi\\theta + \\theta^2)}\n$$\nLet's expand the numerator:\nNumerator = $(1 + 2\\phi\\theta + \\theta^2 - \\phi - 2\\phi^2\\theta - \\phi\\theta^2) + (2\\phi + 2\\phi^2\\theta + 2\\theta + 2\\phi\\theta^2)$\nCollecting terms, the numerator simplifies to: $1 + \\phi + 2\\theta + 2\\phi\\theta + \\theta^2 + \\phi\\theta^2$.\nThis expression can be factored by grouping terms:\n$(1 + 2\\theta + \\theta^2) + \\phi(1 + 2\\theta + \\theta^2) = (1+\\theta)^2 + \\phi(1+\\theta)^2 = (1+\\phi)(1+\\theta)^2$\nSo, the simplified expression for $\\tau_{\\mathrm{int}}$ is:\n$$\n\\tau_{\\mathrm{int}} = \\frac{(1+\\phi)(1+\\theta)^2}{(1-\\phi)(1 + 2\\phi\\theta + \\theta^2)}\n$$\n\n**Step 6: Derive the effective sample size, $n_{\\mathrm{eff}}$.**\nThe problem defines $n_{\\mathrm{eff}} = n / \\tau_{\\mathrm{int}}$. Substituting the expression for $\\tau_{\\mathrm{int}}$:\n$$\nn_{\\mathrm{eff}} = n \\frac{1}{\\tau_{\\mathrm{int}}} = n \\frac{(1-\\phi)(1 + 2\\phi\\theta + \\theta^2)}{(1+\\phi)(1+\\theta)^2}\n$$\nThese are the required closed-form expressions.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{(1+\\phi)(1+\\theta)^2}{(1-\\phi)(1 + 2\\phi\\theta + \\theta^2)}  n \\frac{(1-\\phi)(1+2\\phi\\theta+\\theta^2)}{(1+\\phi)(1+\\theta)^2} \\end{pmatrix}}\n$$", "id": "3304666"}, {"introduction": "Once we can quantify statistical inefficiency using the Effective Sample Size ($n_{\\text{eff}}$), the natural next step is to find ways to improve it. This practice demonstrates how a clever application of control variates, a powerful variance reduction technique, can dramatically reduce autocorrelation and boost efficiency. You will analyze how a simple transformation of an AR(1) process can yield an uncorrelated sequence, effectively maximizing the ESS and showcasing a key principle in the design of advanced Monte Carlo estimators [@problem_id:3304651].", "problem": "Consider a stationary Gaussian autoregressive process of order one (AR(1)), $\\{X_t\\}_{t \\geq 0}$, with mean zero, marginal variance $\\operatorname{Var}(X_t)=\\sigma^{2}$, and autocorrelation function $\\rho_{X}(k)=\\rho^{k}$ for lag $k \\geq 0$, where $|\\rho|1$. A Monte Carlo estimator at iteration $t$ is the scalar $X_t$. You now apply a control variate that uses the lagged value $X_{t-1}$ to construct a new estimator sequence\n$$\nY_t \\;=\\; s\\,\\big(X_t - \\lambda X_{t-1}\\big),\n$$\nwhere $s \\in \\mathbb{R}$ is chosen so that $\\operatorname{Var}(Y_t)=\\operatorname{Var}(X_t)=\\sigma^{2}$ for all choices of $\\lambda \\in \\mathbb{R}$. This control variate does not alter the marginal variance but changes the dependence structure in time by introducing correlation between successive estimates through $\\lambda$.\n\nStarting from the definitions of the autocovariance function and the integrated autocorrelation time (IAT), and using only stationarity and the AR(1) dependence structure, derive the closed-form factor by which the effective sample size (ESS) improves when $\\lambda$ is chosen equal to the AR(1) coefficient $\\rho$. Express your final answer as a single analytic function of $\\rho$, denoted $\\mathcal{F}(\\rho)=n_{\\text{eff,new}}/n_{\\text{eff,old}}$, where $n_{\\text{eff,old}}$ is the ESS of the original sequence $\\{X_t\\}$ and $n_{\\text{eff,new}}$ is the ESS of the transformed sequence $\\{Y_t\\}$ with $\\lambda=\\rho$.", "solution": "The problem requires the derivation of the improvement factor in the effective sample size (ESS) when a specific control variate is applied to a stationary AR(1) process. The improvement factor is defined as the ratio $\\mathcal{F}(\\rho) = n_{\\text{eff,new}}/n_{\\text{eff,old}}$.\n\nThe effective sample size, $n_{\\text{eff}}$, for a sequence of $n$ correlated samples is given by $n_{\\text{eff}} = n / \\tau$, where $\\tau$ is the integrated autocorrelation time (IAT). The IAT is defined in terms of the autocorrelation function (ACF), $\\rho(k)$, as:\n$$\n\\tau = 1 + 2 \\sum_{k=1}^{\\infty} \\rho(k)\n$$\nThe improvement factor can therefore be expressed as the ratio of the IATs of the original and transformed sequences:\n$$\n\\mathcal{F}(\\rho) = \\frac{n_{\\text{eff,new}}}{n_{\\text{eff,old}}} = \\frac{n / \\tau_{\\text{new}}}{n / \\tau_{\\text{old}}} = \\frac{\\tau_{\\text{old}}}{\\tau_{\\text{new}}}\n$$\nOur task reduces to calculating $\\tau_{\\text{old}}$ for the sequence $\\{X_t\\}$ and $\\tau_{\\text{new}}$ for the sequence $\\{Y_t\\}$.\n\nFirst, we calculate the IAT for the original AR(1) sequence, $\\{X_t\\}$. The problem states that its ACF is $\\rho_X(k) = \\rho^k$ for $k \\geq 0$. Using the definition of IAT, we have:\n$$\n\\tau_{\\text{old}} = \\tau_X = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_X(k) = 1 + 2 \\sum_{k=1}^{\\infty} \\rho^k\n$$\nThe sum is a geometric series. Since $|\\rho|  1$, the series converges:\n$$\n\\sum_{k=1}^{\\infty} \\rho^k = \\frac{\\rho}{1-\\rho}\n$$\nSubstituting this back into the expression for $\\tau_{\\text{old}}$ gives:\n$$\n\\tau_{\\text{old}} = 1 + 2 \\frac{\\rho}{1-\\rho} = \\frac{1-\\rho + 2\\rho}{1-\\rho} = \\frac{1+\\rho}{1-\\rho}\n$$\nNext, we analyze the transformed sequence $\\{Y_t\\}$ to find its IAT, $\\tau_{\\text{new}}$. The sequence is defined as $Y_t = s(X_t - \\lambda X_{t-1})$, with the specific choice $\\lambda = \\rho$.\n$$\nY_t = s(X_t - \\rho X_{t-1})\n$$\nA stationary AR(1) process $\\{X_t\\}$ with mean zero and ACF $\\rho_X(k)=\\rho^k$ is defined by the stochastic difference equation:\n$$\nX_t = \\rho X_{t-1} + \\epsilon_t\n$$\nwhere $\\{\\epsilon_t\\}$ is a sequence of uncorrelated (white noise) random variables with mean zero, $\\mathbb{E}[\\epsilon_t]=0$, and constant variance $\\operatorname{Var}(\\epsilon_t) = \\sigma_\\epsilon^2$. The variables $\\epsilon_t$ are also uncorrelated with past values of the process, i.e., $\\operatorname{Cov}(\\epsilon_t, X_{t-j})=0$ for $j>0$.\n\nThe variance of $X_t$ is $\\sigma^2$. From the defining equation, we have:\n$$\n\\operatorname{Var}(X_t) = \\operatorname{Var}(\\rho X_{t-1} + \\epsilon_t) = \\rho^2 \\operatorname{Var}(X_{t-1}) + \\operatorname{Var}(\\epsilon_t)\n$$\nSince the process is stationary, $\\operatorname{Var}(X_t) = \\operatorname{Var}(X_{t-1}) = \\sigma^2$. This yields:\n$$\n\\sigma^2 = \\rho^2 \\sigma^2 + \\sigma_\\epsilon^2 \\implies \\sigma_\\epsilon^2 = \\sigma^2(1-\\rho^2)\n$$\nNow, we substitute the definition of the AR(1) process into the expression for $Y_t$:\n$$\nY_t = s(X_t - \\rho X_{t-1}) = s((\\rho X_{t-1} + \\epsilon_t) - \\rho X_{t-1}) = s \\epsilon_t\n$$\nThis reveals that the transformed sequence $\\{Y_t\\}$ is simply a scaled version of the white noise innovation process $\\{\\epsilon_t\\}$. The problem statement requires that $\\operatorname{Var}(Y_t) = \\sigma^2$. Let's verify the scaling factor $s$:\n$$\n\\operatorname{Var}(Y_t) = \\operatorname{Var}(s \\epsilon_t) = s^2 \\operatorname{Var}(\\epsilon_t) = s^2 \\sigma^2 (1-\\rho^2)\n$$\nSetting $\\operatorname{Var}(Y_t) = \\sigma^2$, we get $s^2 \\sigma^2 (1-\\rho^2) = \\sigma^2$, which implies $s^2 = \\frac{1}{1-\\rho^2}$. This confirms the existence of such a scaling factor $s$.\n\nTo compute $\\tau_{\\text{new}}$, we need the ACF of the $\\{Y_t\\}$ sequence, $\\rho_Y(k)$. The ACF is defined as $\\rho_Y(k) = \\frac{\\operatorname{Cov}(Y_t, Y_{t-k})}{\\operatorname{Var}(Y_t)}$. We know a priori that $\\operatorname{Var}(Y_t)=\\sigma^2$. The autocovariance for lag $k \\ge 1$ is:\n$$\n\\operatorname{Cov}(Y_t, Y_{t-k}) = \\operatorname{Cov}(s \\epsilon_t, s \\epsilon_{t-k}) = s^2 \\operatorname{Cov}(\\epsilon_t, \\epsilon_{t-k})\n$$\nSince $\\{\\epsilon_t\\}$ is a white noise process, its elements are uncorrelated in time. Thus, for any $k \\ge 1$:\n$$\n\\operatorname{Cov}(\\epsilon_t, \\epsilon_{t-k}) = 0\n$$\nThis implies that for all $k \\ge 1$, $\\operatorname{Cov}(Y_t, Y_{t-k})=0$. The ACF of the $\\{Y_t\\}$ sequence is therefore:\n$$\n\\rho_Y(k) = \\frac{0}{\\sigma^2} = 0 \\quad \\text{for all } k \\ge 1\n$$\nThe $\\{Y_t\\}$ sequence is uncorrelated. Now we can compute its IAT, $\\tau_{\\text{new}}$:\n$$\n\\tau_{\\text{new}} = \\tau_Y = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_Y(k) = 1 + 2 \\sum_{k=1}^{\\infty} 0 = 1\n$$\nThe IAT of the new sequence is $1$, which is the theoretical minimum, corresponding to a sequence of independent samples.\n\nFinally, we compute the improvement factor $\\mathcal{F}(\\rho)$:\n$$\n\\mathcal{F}(\\rho) = \\frac{\\tau_{\\text{old}}}{\\tau_{\\text{new}}} = \\frac{\\frac{1+\\rho}{1-\\rho}}{1} = \\frac{1+\\rho}{1-\\rho}\n$$\nThis is the closed-form factor by which the effective sample size improves. By transforming the correlated sequence $\\{X_t\\}$ into the uncorrelated sequence $\\{Y_t\\}$, the control variate technique with $\\lambda=\\rho$ has maximized the effective sample size for a fixed number of samples.", "answer": "$$\n\\boxed{\\frac{1+\\rho}{1-\\rho}}\n$$", "id": "3304651"}, {"introduction": "In real-world scientific computing, statistical efficiency is only valuable when considered alongside computational cost. This final practice confronts this crucial trade-off by tasking you with optimizing an MCMC simulation to achieve the maximum Effective Sample Size (ESS) per second of wall-clock time. By building and analyzing a model that incorporates both algorithmic parameters and hardware performance characteristics, you will engage in a holistic optimization problem that lies at the heart of modern computational statistics [@problem_id:3304672].", "problem": "Consider a class of Markov Chain Monte Carlo (MCMC) algorithms that produce autocorrelated samples. For a given algorithm configuration, define a parameter vector $\\theta$ consisting of an algorithmic step size $\\varepsilon$ and the number of parallel chains $p$ executed concurrently on computing hardware. Assume that each chain is stationary with mean $0$ and variance $\\sigma^2$, and that its lag-$k$ autocorrelation is denoted by $r(k)$. Further suppose that the per-chain dynamics can be well approximated by a first-order autoregressive process, so that there exists a correlation parameter $\\rho(\\varepsilon)$ satisfying $r(k) = \\rho(\\varepsilon)^k$ for all integer $k \\ge 0$, with $0  \\rho(\\varepsilon)  1$.\n\nDefine the total number of iterations per chain as $n$, and consider running $p$ independent chains in parallel so that the total number of samples produced per global iteration (one step per chain) is $p$. The hardware executes these $p$ chain updates concurrently with imperfect scaling. Model the per-chain computation time as an increasing function of step size\n$$\nt_{\\text{chain}}(\\varepsilon) = t_0 + t_1 \\varepsilon^2,\n$$\nwhere $t_0$ and $t_1$ are positive constants measured in seconds. Model the parallel speedup via an Amdahl-type expression\n$$\nS(p) = \\frac{p}{1 + \\kappa (p - 1)},\n$$\nwhere $\\kappa > 0$ quantifies the fraction of non-parallelizable work. Model additional overhead as\n$$\n\\text{overhead}(p) = \\eta (p - 1)^2,\n$$\nwith $\\eta > 0$ measured in seconds per global iteration. The total wall-time cost per global iteration (one step taken on every chain in parallel) is then\n$$\nc(\\varepsilon,p) = \\frac{t_{\\text{chain}}(\\varepsilon)\\, p}{S(p)} + \\text{overhead}(p),\n$$\nexpressed in seconds.\n\nStarting from first principles and core definitions of autocovariance and autocorrelation in time series, derive the effective sample size (ESS) achieved by one chain after $n$ iterations as a function of $\\rho(\\varepsilon)$, and extend it to $p$ independent chains. The derivation must begin from the definition of the variance of the sample mean in terms of the autocovariance function and the autocorrelation function, and proceed logically to identify the integrated autocorrelation time and the resulting effective sample size. Use that derivation to construct a program that, for each test case below, evaluates the objective\n$$\n\\max_{\\varepsilon \\in \\mathcal{E},\\, p \\in \\mathcal{P}} \\frac{\\text{ESS}(\\varepsilon,p)}{c(\\varepsilon,p)},\n$$\nwhere $\\mathcal{E}$ is a finite set of candidate step sizes and $\\mathcal{P}$ is a finite set of candidate thread counts. The parameterization of $\\rho(\\varepsilon)$ is given by\n$$\n\\rho(\\varepsilon) = \\exp(-\\alpha \\varepsilon),\n$$\nwith $\\alpha > 0$ provided per test case.\n\nYour program must:\n- Implement the derivation-based formula for $\\text{ESS}(\\varepsilon,p)$ using the autocorrelation structure $r(k) = \\rho(\\varepsilon)^k$.\n- Evaluate $\\text{ESS}(\\varepsilon,p) / c(\\varepsilon,p)$ for every pair $(\\varepsilon,p)$ in the provided test suite.\n- Select the maximizer $(\\varepsilon^\\star, p^\\star)$ that attains the largest ratio. If multiple pairs attain the same maximum value (to within numerical tolerance of $10^{-12}$), select the one with the smallest $p$; if still tied, select the one with the smallest $\\varepsilon$.\n- Produce as output, for each test case, a list containing $[\\varepsilon^\\star, p^\\star, \\text{ESS-per-second}^\\star]$, where $\\text{ESS-per-second}^\\star = \\text{ESS}(\\varepsilon^\\star,p^\\star) / c(\\varepsilon^\\star,p^\\star)$ is expressed in samples per second. Report $\\varepsilon^\\star$ rounded to $3$ decimal places and $\\text{ESS-per-second}^\\star$ rounded to $6$ decimal places. The thread count $p^\\star$ is an integer.\n\nPhysical and numerical units:\n- Time quantities $t_0$, $t_1$, and $\\eta$ are in seconds.\n- The objective ratio $\\text{ESS}(\\varepsilon,p) / c(\\varepsilon,p)$ must be reported in samples per second (a float).\n\nAngle units do not apply. No percentages appear in the final answer.\n\nTest suite:\n- Case $1$ (happy path):\n    - $n = 10000$, $\\alpha = 3.0$, $t_0 = 1\\times 10^{-4}$, $t_1 = 5\\times 10^{-4}$, $\\kappa = 0.15$, $\\eta = 2\\times 10^{-5}$,\n    - $\\mathcal{E} = \\{0.02, 0.05, 0.10, 0.20, 0.40\\}$,\n    - $\\mathcal{P} = \\{1, 2, 4, 8, 16\\}$.\n- Case $2$ (limited scaling and high overhead):\n    - $n = 10000$, $\\alpha = 1.0$, $t_0 = 5\\times 10^{-4}$, $t_1 = 1\\times 10^{-4}$, $\\kappa = 0.5$, $\\eta = 1\\times 10^{-4}$,\n    - $\\mathcal{E} = \\{0.01, 0.03, 0.05, 0.10, 0.20, 0.30\\}$,\n    - $\\mathcal{P} = \\{1, 2, 3, 4, 6, 8\\}$.\n- Case $3$ (near-ideal scaling and fast decorrelation):\n    - $n = 20000$, $\\alpha = 5.0$, $t_0 = 1\\times 10^{-4}$, $t_1 = 1\\times 10^{-4}$, $\\kappa = 0.05$, $\\eta = 5\\times 10^{-6}$,\n    - $\\mathcal{E} = \\{0.05, 0.10, 0.20, 0.40, 0.80\\}$,\n    - $\\mathcal{P} = \\{1, 2, 4, 8, 16, 32\\}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no extra spaces. For example, the output should look like\n$$\n[\\,[\\varepsilon_1^\\star,p_1^\\star,\\text{ESSperSec}_1^\\star],\\,[\\varepsilon_2^\\star,p_2^\\star,\\text{ESSperSec}_2^\\star],\\,[\\varepsilon_3^\\star,p_3^\\star,\\text{ESSperSec}_3^\\star]\\,],\n$$\nwhere every inner list corresponds to one test case, ordered as Case $1$, Case $2$, Case $3$.", "solution": "**Problem Validation**\n\nThe problem statement has been evaluated and is determined to be **valid**. It is scientifically grounded in the principles of Markov Chain Monte Carlo (MCMC) methods, statistical time series analysis, and computational performance modeling. The problem is well-posed, with a clearly defined objective function to be maximized over a finite set of parameters, and includes a specific tie-breaking rule to ensure a unique solution. All necessary parameters and models are explicitly provided, and the problem is free from ambiguity, contradiction, or subjective content.\n\n**Derivation and Solution Methodology**\n\nThe objective is to maximize the ratio of effective sample size (ESS) to computational cost. We first derive the formula for ESS from first principles as required.\n\nLet $\\{X_i\\}_{i=1}^n$ be a sequence of $n$ samples from a single stationary Markov chain with mean $\\mathbb{E}[X_i] = 0$ and variance $\\text{Var}(X_i) = \\sigma^2$. The sample mean is $\\hat{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$.\n\nThe variance of the sample mean is given by:\n$$ \\text{Var}(\\hat{\\mu}_n) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\text{Cov}(X_i, X_j) $$\nFor a stationary process, the autocovariance $\\gamma(k) = \\text{Cov}(X_i, X_{i+k})$ depends only on the lag $k$. Also, $\\gamma(k) = \\gamma(-k)$ and $\\gamma(0) = \\sigma^2$. The double summation can be re-indexed by $k=j-i$:\n$$ \\text{Var}(\\hat{\\mu}_n) = \\frac{1}{n^2} \\sum_{k=-(n-1)}^{n-1} (n-|k|) \\gamma(k) $$\nUsing the symmetry of $\\gamma(k)$ and expressing it in terms of the autocorrelation function $r(k) = \\gamma(k)/\\gamma(0)$, we have:\n$$ \\text{Var}(\\hat{\\mu}_n) = \\frac{\\sigma^2}{n} \\left[ 1 + 2 \\sum_{k=1}^{n-1} \\left(1-\\frac{k}{n}\\right) r(k) \\right] $$\nFor a number of samples $n$ much larger than the correlation length of the chain, this expression is well approximated by replacing the finite sum with an infinite one and dropping the $(1-k/n)$ term:\n$$ \\text{Var}(\\hat{\\mu}_n) \\approx \\frac{\\sigma^2}{n} \\left[ 1 + 2 \\sum_{k=1}^{\\infty} r(k) \\right] $$\nThe effective sample size, $n_{\\text{eff}}$, is defined as the size of an i.i.d. sample that would yield the same variance in its sample mean, i.e., $\\text{Var}(\\hat{\\mu}_{n_{\\text{eff}}}^{\\text{iid}}) = \\sigma^2/n_{\\text{eff}}$. Equating this with the variance for the autocorrelated chain gives:\n$$ \\frac{\\sigma^2}{n_{\\text{eff}}} = \\frac{\\sigma^2}{n} \\left[ 1 + 2 \\sum_{k=1}^{\\infty} r(k) \\right] $$\nThis gives the definition of ESS for a single chain:\n$$ n_{\\text{eff}} = \\frac{n}{1 + 2 \\sum_{k=1}^{\\infty} r(k)} = \\frac{n}{\\tau} $$\nwhere $\\tau = 1 + 2 \\sum_{k=1}^{\\infty} r(k)$ is the integrated autocorrelation time (IACT).\n\nThe problem specifies an AR(1) process for the chain's autocorrelation structure, $r(k) = \\rho(\\varepsilon)^k$ for $k \\ge 0$, where $\\rho(\\varepsilon)$ is the lag-$1$ autocorrelation which depends on the step size $\\varepsilon$. Given $0  \\rho(\\varepsilon)  1$, the infinite sum is a convergent geometric series:\n$$ \\sum_{k=1}^{\\infty} \\rho(\\varepsilon)^k = \\frac{\\rho(\\varepsilon)}{1-\\rho(\\varepsilon)} $$\nSubstituting this into the expression for $\\tau$:\n$$ \\tau(\\varepsilon) = 1 + 2 \\frac{\\rho(\\varepsilon)}{1-\\rho(\\varepsilon)} = \\frac{1-\\rho(\\varepsilon) + 2\\rho(\\varepsilon)}{1-\\rho(\\varepsilon)} = \\frac{1+\\rho(\\varepsilon)}{1-\\rho(\\varepsilon)} $$\nThe ESS for a single chain of length $n$ is therefore:\n$$ \\text{ESS}_{\\text{single}}(n, \\varepsilon) = \\frac{n}{\\tau(\\varepsilon)} = n \\frac{1-\\rho(\\varepsilon)}{1+\\rho(\\varepsilon)} $$\nSince we are running $p$ independent chains, the total ESS is the sum of the ESS from each chain. As all chains are assumed to have identical statistical properties, the total ESS is:\n$$ \\text{ESS}(\\varepsilon, p) = p \\times \\text{ESS}_{\\text{single}}(n, \\varepsilon) = p n \\frac{1-\\rho(\\varepsilon)}{1+\\rho(\\varepsilon)} $$\n\nThe objective is to maximize the ratio of total ESS to the computational cost per global iteration, denoted $c(\\varepsilon,p)$. The problem states that maximizing $\\text{ESS}(\\varepsilon,p) / c(\\varepsilon,p)$ is the goal. Although $\\text{ESS}$ is the total over $n$ iterations and $c$ is the cost per iteration, maximizing this quantity is equivalent to maximizing the rate of ESS generation, as the factor $n$ is constant for a given test case.\n\nThe full objective function to be maximized is:\n$$ f(\\varepsilon, p) = \\frac{\\text{ESS}(\\varepsilon,p)}{c(\\varepsilon,p)} $$\nwhere the components are given by:\n1.  **Correlation parameter**: $\\rho(\\varepsilon) = \\exp(-\\alpha \\varepsilon)$\n2.  **Effective Sample Size**: $\\text{ESS}(\\varepsilon, p) = p n \\frac{1 - \\exp(-\\alpha \\varepsilon)}{1 + \\exp(-\\alpha \\varepsilon)}$\n3.  **Cost per global iteration**: $c(\\varepsilon,p) = \\frac{t_{\\text{chain}}(\\varepsilon)\\, p}{S(p)} + \\text{overhead}(p)$, which simplifies to:\n    $$ c(\\varepsilon, p) = (t_0 + t_1 \\varepsilon^2) (1 + \\kappa(p-1)) + \\eta(p-1)^2 $$\n\nThe program must find the pair $(\\varepsilon^\\star, p^\\star)$ that maximizes $f(\\varepsilon, p)$ for $\\varepsilon \\in \\mathcal{E}$ and $p \\in \\mathcal{P}$, where $\\mathcal{E}$ and $\\mathcal{P}$ are the finite sets of step sizes and thread counts provided for each test case.\n\nThe optimization is performed by a grid search over all pairs $(\\varepsilon, p)$ in the Cartesian product $\\mathcal{E} \\times \\mathcal{P}$. For each pair, we compute the objective function $f(\\varepsilon, p)$. The candidate sets $\\mathcal{E}$ and $\\mathcal{P}$ are sorted in ascending order before commencing the search. We maintain the current best-found parameters $(\\varepsilon^\\star, p^\\star)$ and the corresponding maximum value $f^\\star$. A new pair $(\\varepsilon,p)$ replaces the current best if its objective value is strictly greater than $f^\\star$ (by more than a numerical tolerance of $10^{-12}$). Due to the sorted iteration order, the first time a maximum value is found, it will be with the smallest possible $p$, and for that $p$, the smallest possible $\\varepsilon$. Any subsequent pair that achieves a tied value will have a larger $p$ or $\\varepsilon$ and will thus be correctly ignored, satisfying the tie-breaking rule.\n\nThe final output for each test case is a list containing the optimal step size $\\varepsilon^\\star$ (rounded to $3$ decimal places), the optimal number of chains $p^\\star$ (as an integer), and the maximal objective value $f(\\varepsilon^\\star, p^\\star)$ (rounded to $6$ decimal places).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the MCMC optimization problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"n\": 10000, \"alpha\": 3.0, \"t0\": 1e-4, \"t1\": 5e-4, \"kappa\": 0.15, \"eta\": 2e-5,\n            \"E_set\": [0.02, 0.05, 0.10, 0.20, 0.40],\n            \"P_set\": [1, 2, 4, 8, 16]\n        },\n        # Case 2 (limited scaling and high overhead)\n        {\n            \"n\": 10000, \"alpha\": 1.0, \"t0\": 5e-4, \"t1\": 1e-4, \"kappa\": 0.5, \"eta\": 1e-4,\n            \"E_set\": [0.01, 0.03, 0.05, 0.10, 0.20, 0.30],\n            \"P_set\": [1, 2, 3, 4, 6, 8]\n        },\n        # Case 3 (near-ideal scaling and fast decorrelation)\n        {\n            \"n\": 20000, \"alpha\": 5.0, \"t0\": 1e-4, \"t1\": 1e-4, \"kappa\": 0.05, \"eta\": 5e-6,\n            \"E_set\": [0.05, 0.10, 0.20, 0.40, 0.80],\n            \"P_set\": [1, 2, 4, 8, 16, 32]\n        }\n    ]\n\n    results = []\n    \n    TOL = 1e-12\n\n    for case in test_cases:\n        n_iters = case[\"n\"]\n        alpha = case[\"alpha\"]\n        t0 = case[\"t0\"]\n        t1 = case[\"t1\"]\n        kappa = case[\"kappa\"]\n        eta = case[\"eta\"]\n        E_set = sorted(case[\"E_set\"])\n        P_set = sorted(case[\"P_set\"])\n\n        best_eps_star = -1.0\n        best_p_star = -1\n        max_ratio = -1.0\n\n        for p in P_set:\n            for eps in E_set:\n                # Calculate the correlation parameter rho\n                rho = np.exp(-alpha * eps)\n                \n                # Calculate the total Effective Sample Size (ESS)\n                # ESS is calculated for p independent chains, each of length n\n                if abs(1.0 + rho)  1e-15: # Avoid division by zero\n                    ess_factor = (1.0 - rho) / (1.0 + rho)\n                    ess = p * n_iters * ess_factor\n                else: \n                    ess = 0.0\n\n                # Calculate the total wall-time cost per global iteration, c(eps, p)\n                cost_per_iter = (t0 + t1 * eps**2) * (1.0 + kappa * (p - 1.0)) + eta * (p - 1.0)**2\n                \n                # Calculate the objective function: ESS-per-cost-unit\n                if cost_per_iter  1e-15: # Avoid division by zero\n                    current_ratio = ess / cost_per_iter\n                else:\n                    current_ratio = 0.0\n\n                # Apply maximization and tie-breaking rules.\n                # By iterating through p and eps in sorted order, the first time we\n                # find a new maximum, it will be at the combination with the smallest\n                # p, and for that p, the smallest eps. Subsequent ties will be ignored.\n                if current_ratio  max_ratio + TOL:\n                    max_ratio = current_ratio\n                    best_eps_star = eps\n                    best_p_star = p\n        \n        # Format results as specified for the final output\n        # Round eps_star to 3 decimal places\n        # Round ess_per_second_star to 6 decimal places\n        # p_star is an integer\n        formatted_eps = round(best_eps_star, 3)\n        formatted_ratio = round(max_ratio, 6)\n        \n        results.append([formatted_eps, best_p_star, formatted_ratio])\n\n    # Construct the final output string exactly as specified.\n    inner_lists = [f\"[{e},{p},{r}]\" for e, p, r in results]\n    print(f\"[{','.join(inner_lists)}]\")\n\nsolve()\n```", "id": "3304672"}]}