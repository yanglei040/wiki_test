{"hands_on_practices": [{"introduction": "The batch means method comes in two primary forms: nonoverlapping (NBM) and overlapping (OBM). While NBM is conceptually simpler, partitioning the data into disjoint blocks, OBM utilizes all possible contiguous data windows, which can lead to a more efficient and less variable estimate of the long-run variance. This practice guides you through a direct, empirical comparison of their performance, allowing you to simulate a standard time series process and quantify the mean squared error of each estimator to understand the practical advantages of the overlapping approach [@problem_id:3359912].", "problem": "Consider a strictly stationary and ergodic autoregressive process of order one, denoted by $AR(1)$, defined by the recursion $X_t = \\phi X_{t-1} + \\varepsilon_t$, where $\\{ \\varepsilon_t \\}$ is an independent and identically distributed sequence with zero mean and finite variance, and $|\\phi|  1$. The spectral density $f(\\omega)$ is the Fourier transform of the autocovariance function, and the value at zero frequency, $f(0)$, determines the asymptotic variance of the sample mean via the Central Limit Theorem (CLT) and the Functional Central Limit Theorem (FCLT) as follows: $\\sqrt{n}(\\bar{X}_n - \\mu) \\Rightarrow \\mathcal{N}(0, 2\\pi f(0))$, where $\\mu = \\mathbb{E}[X_t]$ and $\\bar{X}_n = n^{-1}\\sum_{t=1}^n X_t$. The long-run variance is $2\\pi f(0)$.\n\nYour task is to empirically compare nonoverlapping batch means and overlapping batch means for estimating $f(0)$ in the following controlled study. You must start from the above foundational definitions and the CLT/FCLT statement and derive valid estimators of $f(0)$ based on batch means. You must then implement the derived estimators and study their mean squared error (MSE) as a function of the batch size $b$, finally identifying the empirically optimal batch size within a prescribed grid.\n\nSimulation model and requirements:\n- Fix the autoregressive parameter at $\\phi = 0.95$.\n- Choose the innovation variance to make the marginal variance of $X_t$ equal to $1$.\n- Initialize $X_0$ to be drawn from the stationary marginal distribution of $X_t$, to avoid start-up bias.\n- For each simulation path, generate a length-$n$ time series $\\{X_t\\}_{t=1}^n$.\n- For each candidate batch size $b$, compute two estimators of $f(0)$:\n  - One based on nonoverlapping batch means using batches of size $b$.\n  - One based on overlapping batch means using all consecutive windows of length $b$.\n- Using $R$ independent Monte Carlo replications, approximate the MSE for each estimator and each $b$ as the empirical mean of squared errors relative to the true $f(0)$ of the $AR(1)$ model. Ties in the empirically optimal $b$ must be broken by selecting the smallest $b$ attaining the minimum MSE.\n\nTest suite:\n- Use the three parameter sets below. For each case, evaluate both estimators over the prescribed batch-size grid.\n  - Case A: $n = 8192$, $R = 256$, candidate $b \\in \\{8, 16, 32, 64, 128, 256, 512, 1024, 2048, 3072\\}$.\n  - Case B: $n = 512$, $R = 512$, candidate $b \\in \\{4, 8, 16, 32, 64, 96, 128, 192, 256\\}$.\n  - Case C: $n = 20000$, $R = 128$, candidate $b \\in \\{20, 40, 80, 160, 320, 640, 1280, 2560, 4000, 8000\\}$.\n- For nonoverlapping batch means, only apply candidate $b$ values that produce at least two full batches in length $n$. For overlapping batch means, only apply candidate $b$ values that produce at least two overlapping windows in length $n$.\n- For reproducibility, use a fixed pseudorandom number generator seed equal to $20240519$.\n\nDeliverables and output format:\n- Your program must:\n  - Derive from first principles valid nonoverlapping and overlapping batch means estimators of $f(0)$, implement both, and compute their empirical MSEs across the specified batch-size grids.\n  - For each case, report the empirically minimizing batch size $b$ for the nonoverlapping method and for the overlapping method, with ties broken in favor of the smallest $b$.\n- Final output format: a single line that contains a list of three lists, in the order Case A, Case B, Case C, where each inner list has the form $[b_{\\text{nonoverlap}}, b_{\\text{overlap}}]$. For example, the output must have the exact form $[[b_{A,\\text{non}},b_{A,\\text{over}}],[b_{B,\\text{non}},b_{B,\\text{over}}],[b_{C,\\text{non}},b_{C,\\text{over}}]]$ with no spaces after commas inside numbers.\n\nThere are no physical units involved in this problem. All angles, if any were to appear, would be in radians. The required outputs are integers. Your code must be self-contained, require no user input, and produce exactly one line of output in the specified format.", "solution": "The problem requires an empirical comparison of nonoverlapping and overlapping batch means estimators for the spectral density at zero frequency, $f(0)$, of a specific AR(1) process. The comparison is based on the Mean Squared Error (MSE) of the estimators for various batch sizes.\n\nFirst, we establish the theoretical foundation and derive the estimators. The problem states that for a stationary time series with sample mean $\\bar{X}_n$, the Central Limit Theorem (CLT) holds in the form $\\sqrt{n}(\\bar{X}_n - \\mu) \\Rightarrow \\mathcal{N}(0, \\sigma^2_{LR})$, where $\\sigma^2_{LR} = 2\\pi f(0)$ is the long-run variance. This implies that for large $n$, the variance of the sample mean is $\\text{Var}(\\bar{X}_n) \\approx \\frac{\\sigma^2_{LR}}{n}$. The core task is to estimate $\\sigma^2_{LR}$ (and thus $f(0) = \\sigma^2_{LR}/(2\\pi)$) from a single time series realization $\\{X_t\\}_{t=1}^n$.\n\nThe specified simulation model is the AR(1) process $X_t = \\phi X_{t-1} + \\varepsilon_t$ with $\\phi=0.95$. The innovations $\\{\\varepsilon_t\\}$ are i.i.d. with $\\mathbb{E}[\\varepsilon_t] = 0$. For this process, the mean is $\\mu = \\mathbb{E}[X_t] = 0$. The marginal variance is $\\sigma_X^2 = \\text{Var}(X_t) = \\frac{\\text{Var}(\\varepsilon_t)}{1-\\phi^2}$. The problem requires $\\sigma_X^2=1$, so we must set the innovation variance to $\\text{Var}(\\varepsilon_t) = \\sigma_\\varepsilon^2 = 1-\\phi^2$. The initial state $X_0$ is drawn from the stationary distribution, which, assuming Gaussian innovations, is $\\mathcal{N}(0, \\sigma_X^2) = \\mathcal{N}(0, 1)$.\n\nThe true value of the long-run variance $\\sigma^2_{LR}$ for an AR(1) process is given by the sum of its autocovariances:\n$$\n\\sigma^2_{LR} = \\sum_{h=-\\infty}^{\\infty} \\gamma(h) = \\sum_{h=-\\infty}^{\\infty} \\sigma_X^2 \\phi^{|h|} = \\sigma_X^2 \\frac{1+\\phi}{1-\\phi}\n$$\nWith $\\sigma_X^2 = 1$ and $\\phi = 0.95$, the true value is $\\sigma^2_{LR, \\text{true}} = \\frac{1+0.95}{1-0.95} = \\frac{1.95}{0.05} = 39$.\nThe corresponding true value for $f(0)$ is $f(0)_{\\text{true}} = \\frac{\\sigma^2_{LR, \\text{true}}}{2\\pi} = \\frac{39}{2\\pi}$. Our estimators will be evaluated against this value.\n\n**Derivation of Estimators**\n\n**1. Nonoverlapping Batch Means (NBM) Estimator**\nThe time series $X_1, \\dots, X_n$ is partitioned into $a = \\lfloor n/b \\rfloor$ contiguous, non-overlapping batches of size $b$. The data from $X_{ab+1}, \\dots, X_n$ are discarded. The mean of the $i$-th batch is:\n$$\nY_i = \\frac{1}{b} \\sum_{j=1}^{b} X_{(i-1)b+j} \\quad \\text{for } i=1, \\dots, a\n$$\nIf the batch size $b$ is sufficiently large, the batch means $\\{Y_i\\}_{i=1}^a$ are approximately uncorrelated and identically distributed. From the CLT, each $Y_i$ is a sample mean over a series of length $b$, so $\\text{Var}(Y_i) \\approx \\sigma^2_{LR}/b$. We can estimate this variance using the sample variance of the batch means. The standard unbiased estimator for the variance of the population of $Y_i$'s is:\n$$\n\\hat{\\text{Var}}(Y_i) = \\frac{1}{a-1} \\sum_{i=1}^a (Y_i - \\bar{Y}_a)^2, \\quad \\text{where } \\bar{Y}_a = \\frac{1}{a} \\sum_{i=1}^a Y_i\n$$\nSince this quantity estimates $\\sigma^2_{LR}/b$, an estimator for $\\sigma^2_{LR}$ is obtained by multiplying by $b$:\n$$\n\\hat{\\sigma}^2_{NBM} = \\frac{b}{a-1} \\sum_{i=1}^a (Y_i - \\bar{Y}_a)^2\n$$\nThe NBM estimator for $f(0)$ is therefore $\\hat{f}(0)_{NBM} = \\frac{\\hat{\\sigma}^2_{NBM}}{2\\pi}$. A minimum of $a=2$ batches is required to compute a variance.\n\n**2. Overlapping Batch Means (OBM) Estimator**\nInstead of disjoint batches, the OBM method uses all $m = n-b+1$ possible contiguous subsequences of length $b$. The $i$-th overlapping batch mean is:\n$$\nZ_i = \\frac{1}{b} \\sum_{j=0}^{b-1} X_{i+j} \\quad \\text{for } i=1, \\dots, m\n$$\nThe sequence $\\{Z_i\\}_{i=1}^m$ is highly correlated, so a simple application of the sample variance formula is not appropriate without careful consideration of scaling factors. A rigorous derivation, often connected to the Bartlett spectral density estimator, shows that a consistent and often-used estimator for $\\sigma^2_{LR}$ is:\n$$\n\\hat{\\sigma}^2_{OBM} = \\frac{nb}{(n-b+1)(n-b)} \\sum_{i=1}^{n-b+1} (Z_i - \\bar{X}_n)^2\n$$\nwhere $\\bar{X}_n = \\frac{1}{n} \\sum_{t=1}^n X_t$ is the grand mean of the entire series. This form includes a factor of $\\frac{n}{n-b}$ which serves as a bias correction. The OBM estimator for $f(0)$ is $\\hat{f}(0)_{OBM} = \\frac{\\hat{\\sigma}^2_{OBM}}{2\\pi}$. Similar to NBM, this requires at least two batch means ($m \\ge 2$, or $b \\le n-1$) to be well-defined.\n\n**Simulation and Evaluation**\nFor each test case (A, B, C), we perform $R$ independent Monte Carlo replications. In each replication, a time series of length $n$ is generated. For each candidate batch size $b$, we compute $\\hat{f}(0)_{NBM}$ and $\\hat{f}(0)_{OBM}$. The performance is measured by the Mean Squared Error (MSE), approximated by the empirical mean over the $R$ replications:\n$$\n\\text{MSE}(\\hat{f}(0); b) = \\frac{1}{R} \\sum_{r=1}^R \\left(\\hat{f}(0)^{(r)}(b) - f(0)_{\\text{true}}\\right)^2\n$$\nwhere $\\hat{f}(0)^{(r)}(b)$ is the estimate from replication $r$ using batch size $b$. For each estimator type, we identify the batch size $b$ from the given grid that minimizes this empirical MSE, breaking ties by selecting the smallest $b$. The implementation will follow these derived formulas and procedures.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the batch means comparison problem by running a Monte Carlo simulation.\n    \"\"\"\n    \n    # --- Simulation Constants ---\n    PHI = 0.95\n    SEED = 20240519\n\n    # --- Test Cases ---\n    test_cases = [\n        # Case A\n        {'n': 8192, 'R': 256, 'b_grid': [8, 16, 32, 64, 128, 256, 512, 1024, 2048, 3072]},\n        # Case B\n        {'n': 512, 'R': 512, 'b_grid': [4, 8, 16, 32, 64, 96, 128, 192, 256]},\n        # Case C\n        {'n': 20000, 'R': 128, 'b_grid': [20, 40, 80, 160, 320, 640, 1280, 2560, 4000, 8000]},\n    ]\n\n    # --- Helper Functions ---\n    \n    def generate_ar1_series(n, phi, rng):\n        \"\"\"Generates a stationary AR(1) series of length n with Var(X)=1.\"\"\"\n        sigma_eps = np.sqrt(1 - phi**2)\n        innovations = rng.normal(loc=0, scale=sigma_eps, size=n)\n        x = np.zeros(n)\n        x0 = rng.normal(loc=0, scale=1.0)\n        \n        x[0] = phi * x0 + innovations[0]\n        for t in range(1, n):\n            x[t] = phi * x[t-1] + innovations[t]\n        return x\n\n    def estimate_f0_nbm(x, b):\n        \"\"\"Estimates f(0) using non-overlapping batch means.\"\"\"\n        n = len(x)\n        k = n // b\n        if k  2:\n            return np.nan\n        \n        x_trunc = x[:k * b]\n        batch_means = x_trunc.reshape((k, b)).mean(axis=1)\n        \n        var_batch_means = batch_means.var(ddof=1)\n        sigma_sq_lr_hat = b * var_batch_means\n        \n        f0_hat = sigma_sq_lr_hat / (2 * np.pi)\n        return f0_hat\n\n    def estimate_f0_obm(x, b):\n        \"\"\"Estimates f(0) using overlapping batch means.\"\"\"\n        n = len(x)\n        m = n - b + 1\n        if m  2:\n            return np.nan\n\n        batch_sums = np.convolve(x, np.ones(b), mode='valid')\n        batch_means = batch_sums / b\n        \n        x_bar = x.mean()\n        \n        sum_sq_dev = np.sum((batch_means - x_bar)**2)\n        \n        sigma_sq_lr_hat = (n * b) / (m * (m - 1)) * sum_sq_dev\n        \n        f0_hat = sigma_sq_lr_hat / (2 * np.pi)\n        return f0_hat\n\n    # --- Main Logic ---\n    \n    final_results = []\n    rng = np.random.default_rng(SEED)\n    f0_true = (1 / (2 * np.pi)) * (1 + PHI) / (1 - PHI)\n\n    for case in test_cases:\n        n, R, b_grid = case['n'], case['R'], case['b_grid']\n        \n        sse_nbm = {b: 0.0 for b in b_grid}\n        sse_obm = {b: 0.0 for b in b_grid}\n        \n        for _ in range(R):\n            x_series = generate_ar1_series(n, PHI, rng)\n            \n            for b in b_grid:\n                # NBM estimator\n                if n // b >= 2:\n                    f0_hat_nbm = estimate_f0_nbm(x_series, b)\n                    if not np.isnan(f0_hat_nbm):\n                       sse_nbm[b] += (f0_hat_nbm - f0_true)**2\n                \n                # OBM estimator\n                if n - b + 1 >= 2:\n                    f0_hat_obm = estimate_f0_obm(x_series, b)\n                    if not np.isnan(f0_hat_obm):\n                        sse_obm[b] += (f0_hat_obm - f0_true)**2\n        \n        # Find optimal batch size for NBM\n        min_mse_nbm = float('inf')\n        opt_b_nbm = -1\n        for b in b_grid:\n            if n // b >= 2:\n                mse = sse_nbm[b] / R\n                if mse  min_mse_nbm:\n                    min_mse_nbm = mse\n                    opt_b_nbm = b\n        \n        # Find optimal batch size for OBM\n        min_mse_obm = float('inf')\n        opt_b_obm = -1\n        for b in b_grid:\n            if n - b + 1 >= 2:\n                mse = sse_obm[b] / R\n                if mse  min_mse_obm:\n                    min_mse_obm = mse\n                    opt_b_obm = b\n                    \n        final_results.append([opt_b_nbm, opt_b_obm])\n\n    # Format and print the final output as specified\n    inner_lists_str = [f\"[{r[0]},{r[1]}]\" for r in final_results]\n    print(f\"[{','.join(inner_lists_str)}]\")\n\nsolve()\n```", "id": "3359912"}, {"introduction": "After understanding how the batch means method works, the most critical practical question is how to select the batch size. This choice is not arbitrary and embodies a fundamental trade-off: small batches are highly correlated, introducing bias, while large batches reduce the sample size for the variance calculation, increasing the estimator's variance. This hands-on exercise [@problem_id:3359849] explores this bias-variance dilemma by parameterizing the number of batches as $a = \\lfloor n^\\alpha \\rfloor$ and tasking you with finding the empirically optimal scaling exponent $\\alpha$, providing crucial intuition for tuning this parameter in real-world simulation studies.", "problem": "You are asked to design and execute an empirical study of variance estimation via batch means for stationary autoregressive order one processes. Begin from the following foundational base: If $\\{X_t\\}_{t \\ge 1}$ is a stationary and geometrically ergodic Markov chain with finite second moments, then a Markov Chain Central Limit Theorem (CLT) holds for the sample average $\\bar{X}_n = \\frac{1}{n} \\sum_{t=1}^n X_t$, namely\n$$\n\\sqrt{n}\\left(\\bar{X}_n - \\mathbb{E}[X_1]\\right) \\Rightarrow \\mathcal{N}\\left(0, \\sigma_{\\mathrm{LRV}}^2\\right),\n$$\nwhere the long-run variance is\n$$\n\\sigma_{\\mathrm{LRV}}^2 = \\gamma_0 + 2 \\sum_{k=1}^\\infty \\gamma_k, \\quad \\gamma_k = \\mathrm{Cov}(X_1, X_{1+k}).\n$$\nThe variance of the sample mean satisfies $\\mathrm{Var}(\\bar{X}_n) \\approx \\sigma_{\\mathrm{LRV}}^2 / n$ for large $n$. The batch means method estimates $\\mathrm{Var}(\\bar{X}_n)$ by partitioning the data into equal-length batches, computing batch averages, and using their variability to infer $\\sigma_{\\mathrm{LRV}}^2$.\n\nConsider the autoregressive process of order one defined by\n$$\nX_{t+1} = \\lambda X_t + \\varepsilon_t, \\quad \\varepsilon_t \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, 1 - \\lambda^2),\n$$\nstarted at stationarity with $X_1 \\sim \\mathcal{N}(0,1)$ so that $\\mathrm{Var}(X_t) = 1$ for all $t \\ge 1$. For this process, $\\gamma_k = \\lambda^k$ and the long-run variance equals\n$$\n\\sigma_{\\mathrm{LRV}}^2 = \\frac{1 + \\lambda}{1 - \\lambda}.\n$$\n\nYour task is to implement a complete program that performs the following empirical experiment using the batch means method:\n\n- For a fixed total length $n$, and a grid of exponents $\\alpha \\in (0,1)$, let the number of batches be $a = \\lfloor n^\\alpha \\rfloor$. Let the batch size be $b = \\lfloor n / a \\rfloor$; discard any remainder observations so that the effective sample size is $n_{\\mathrm{eff}} = a \\cdot b$. Form $a$ non-overlapping contiguous batches of size $b$ from the first $n_{\\mathrm{eff}}$ observations. Use these batch averages to produce a batch-means-based estimator of $\\mathrm{Var}(\\bar{X}_{n_{\\mathrm{eff}}})$. If $a  2$ or $b  2$, regard the estimator as undefined for that $\\alpha$ and exclude it from consideration.\n\n- For each $\\alpha$ and each mixing rate $\\lambda \\in \\{0.5, 0.9, 0.99\\}$, estimate the mean squared error (MSE) of the batch means estimator of $\\mathrm{Var}(\\bar{X}_{n_{\\mathrm{eff}}})$ by Monte Carlo with $R$ independent replicates, and compare it to the theoretical truth $\\sigma_{\\mathrm{LRV}}^2 / n_{\\mathrm{eff}}$ with $\\sigma_{\\mathrm{LRV}}^2 = (1+\\lambda)/(1-\\lambda)$. Use a single fixed pseudorandom seed for reproducibility and reuse the same simulated paths across different $\\alpha$ values for fair comparisons.\n\n- For each $(n,\\lambda)$, scan the grid of $\\alpha$ and identify the MSE-optimal exponent $\\alpha^\\star$ that minimizes the empirical MSE. If multiple $\\alpha$ achieve the same minimum up to floating-point equality, choose the smallest such $\\alpha$.\n\nImplementation details to enforce scientific realism and comparability:\n\n- Use the autoregressive order one dynamics above with initial state drawn from the stationary distribution so that stationarity holds from the start. Use independent normal noise with variance $1-\\lambda^2$ so that $\\mathrm{Var}(X_t)=1$.\n\n- The Monte Carlo study must use a common grid $\\mathcal{A}$ of exponents $\\alpha$ and the same number of replications $R$ for all combinations. Use the grid\n$$\n\\mathcal{A} = \\{0.10, 0.15, 0.20, \\dots, 0.90\\}.\n$$\n\n- Use $R = 200$ replications and the fixed seed $12345$ for the pseudorandom number generator.\n\n- For the batch means estimator, construct non-overlapping batches from the beginning of the series and discard any remainder to enforce equal batch sizes across all batches used.\n\n- For each $\\alpha$ kept, estimate the MSE of the batch means estimator relative to the theoretical target $\\sigma_{\\mathrm{LRV}}^2 / n_{\\mathrm{eff}}$.\n\nTest suite and required outputs:\n\nRun your program for the following three test cases, each consisting of a total length $n$ and the three mixing rates $\\lambda \\in \\{0.5, 0.9, 0.99\\}$:\n\n- Test case $1$: $n = 2000$.\n- Test case $2$: $n = 4096$.\n- Test case $3$: $n = 8192$.\n\nFor each test case and each $\\lambda$, report the MSE-optimal $\\alpha^\\star$ from the grid $\\mathcal{A}$. The final output must be a single line containing a list of $9$ numbers corresponding to the selected $\\alpha^\\star$ in the following order: test case $1$ with $\\lambda = 0.5$, test case $1$ with $\\lambda = 0.9$, test case $1$ with $\\lambda = 0.99$, then test case $2$ with $\\lambda = 0.5$, test case $2$ with $\\lambda = 0.9$, test case $2$ with $\\lambda = 0.99$, and finally test case $3$ with $\\lambda = 0.5$, test case $3$ with $\\lambda = 0.9$, test case $3$ with $\\lambda = 0.99$. Express each $\\alpha^\\star$ rounded to two decimal places as a decimal number.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[0.35,0.50,0.65,0.30,0.45,0.60,0.25,0.40,0.55]\").", "solution": "The problem requires an empirical study to determine the optimal batching strategy for the batch means variance estimator when applied to a stationary autoregressive process of order one, AR(1). The optimality criterion is the minimization of the Mean Squared Error (MSE) of the variance estimator. The core of the problem lies in the inherent trade-off in the choice of the number of batches, $a$, and the batch size, $b$.\n\n### 1. Theoretical Foundation\n\nWe begin with the specified AR(1) process:\n$$\nX_{t+1} = \\lambda X_t + \\varepsilon_t\n$$\nwhere the innovations $\\varepsilon_t$ are independent and identically distributed (i.i.d.) as $\\mathcal{N}(0, 1 - \\lambda^2)$. The process is initiated in its stationary distribution, $X_1 \\sim \\mathcal{N}(0, 1)$, ensuring that the time series $\\{X_t\\}$ is stationary with $\\mathbb{E}[X_t] = 0$ and $\\mathrm{Var}(X_t) = 1$ for all $t \\ge 1$.\n\nFor this process, a Central Limit Theorem (CLT) for dependent sequences holds. For the sample mean $\\bar{X}_n = \\frac{1}{n}\\sum_{t=1}^n X_t$, we have:\n$$\n\\sqrt{n} \\bar{X}_n \\Rightarrow \\mathcal{N}(0, \\sigma_{\\mathrm{LRV}}^2)\n$$\nwhere $\\Rightarrow$ denotes convergence in distribution. The long-run variance, $\\sigma_{\\mathrm{LRV}}^2$, is given by the sum of all autocovariances:\n$$\n\\sigma_{\\mathrm{LRV}}^2 = \\sum_{k=-\\infty}^{\\infty} \\gamma_k = \\gamma_0 + 2 \\sum_{k=1}^\\infty \\gamma_k\n$$\nwhere $\\gamma_k = \\mathrm{Cov}(X_t, X_{t+k})$. For the given AR(1) process, $\\gamma_k = \\lambda^{|k|}$, which yields the analytical expression:\n$$\n\\sigma_{\\mathrm{LRV}}^2 = 1 + 2 \\sum_{k=1}^\\infty \\lambda^k = 1 + 2 \\frac{\\lambda}{1-\\lambda} = \\frac{1-\\lambda+2\\lambda}{1-\\lambda} = \\frac{1+\\lambda}{1-\\lambda}\n$$\nThe variance of the sample mean for a large sample of size $n$ is approximated by $\\mathrm{Var}(\\bar{X}_n) \\approx \\sigma_{\\mathrm{LRV}}^2/n$. Our objective is to estimate this quantity.\n\n### 2. The Batch Means Estimator\n\nThe batch means method provides an estimate for $\\mathrm{Var}(\\bar{X}_n)$. The procedure is as follows:\n1. A time series of total length $n$ is partitioned. The number of batches, $a$, is determined by an exponent $\\alpha \\in (0,1)$ as $a = \\lfloor n^\\alpha \\rfloor$.\n2. The batch size is $b = \\lfloor n/a \\rfloor$. Any remainder data points are discarded, leading to an effective sample size of $n_{\\mathrm{eff}} = a \\cdot b$.\n3. The data $X_1, \\dots, X_{n_{\\mathrm{eff}}}$ are divided into $a$ non-overlapping batches of size $b$. The mean of the $j$-th batch is computed as:\n   $$\n   Y_j = \\frac{1}{b} \\sum_{i=1}^{b} X_{(j-1)b+i}, \\quad j = 1, \\dots, a\n   $$\n4. If the batch size $b$ is sufficiently large, the batch means $\\{Y_j\\}_{j=1}^a$ are approximately uncorrelated, and each $Y_j$ has a variance $\\mathrm{Var}(Y_j) \\approx \\sigma_{\\mathrm{LRV}}^2/b$.\n5. Treating the batch means as approximately i.i.d. observations, we can estimate their variance using the sample variance:\n   $$\n   S_Y^2 = \\frac{1}{a-1} \\sum_{j=1}^a (Y_j - \\bar{Y})^2, \\quad \\text{where} \\quad \\bar{Y} = \\frac{1}{a}\\sum_{j=1}^a Y_j = \\bar{X}_{n_{\\mathrm{eff}}}\n   $$\n   $S_Y^2$ is an estimator for $\\sigma_{\\mathrm{LRV}}^2 / b$.\n6. An estimator for the long-run variance is thus $\\hat{\\sigma}_{\\mathrm{LRV}}^2 = b \\cdot S_Y^2$.\n7. Finally, the batch means estimator for the variance of the sample mean, $\\mathrm{Var}(\\bar{X}_{n_{\\mathrm{eff}}})$, is:\n   $$\n   \\widehat{\\mathrm{Var}}(\\bar{X}_{n_{\\mathrm{eff}}}) = \\frac{\\hat{\\sigma}_{\\mathrm{LRV}}^2}{n_{\\mathrm{eff}}} = \\frac{b \\cdot S_Y^2}{a \\cdot b} = \\frac{S_Y^2}{a}\n   $$\n\n### 3. Experimental Design and MSE Estimation\n\nThe choice of $\\alpha$ governs a critical bias-variance trade-off:\n- **Large $\\alpha$**: Leads to a large number of batches $a$ and a small batch size $b$. The assumption that batch means are uncorrelated breaks down, as correlation between adjacent batches becomes significant. This introduces a negative bias in the estimator, i.e., $\\mathbb{E}[S_Y^2]  \\sigma_{\\mathrm{LRV}}^2/b$.\n- **Small $\\alpha$**: Leads to a small number of batches $a$ and a large batch size $b$. The batch means are more nearly uncorrelated, reducing bias. However, estimating a variance from a small sample (of size $a$) results in a high-variance estimator $S_Y^2$.\n\nThe optimal $\\alpha^\\star$ is the one that minimizes the Mean Squared Error (MSE):\n$$\n\\mathrm{MSE}(\\alpha) = \\mathbb{E}\\left[ \\left( \\widehat{\\mathrm{Var}}(\\bar{X}_{n_{\\mathrm{eff}}}) - \\frac{\\sigma_{\\mathrm{LRV}}^2}{n_{\\mathrm{eff}}} \\right)^2 \\right]\n$$\nThis expectation is intractable to compute analytically, so we estimate it via a Monte Carlo simulation. For a given $(n, \\lambda)$ pair:\n1. We generate $R$ independent replications (paths) of the AR(1) process, each of length $n$.\n2. For each path and for each candidate $\\alpha$ from the grid $\\mathcal{A}$, we compute the batch means estimate $\\widehat{\\mathrm{Var}}_r(\\bar{X}_{n_{\\mathrm{eff}}})$, where $r \\in \\{1, \\dots, R\\}$.\n3. The MSE for a given $\\alpha$ is then estimated as the sample average of the squared errors over the $R$ replicates:\n   $$\n   \\widehat{\\mathrm{MSE}}(\\alpha) = \\frac{1}{R} \\sum_{r=1}^R \\left( \\widehat{\\mathrm{Var}}_r(\\bar{X}_{n_{\\mathrm{eff}}}) - \\frac{\\sigma_{\\mathrm{LRV}}^2}{n_{\\mathrm{eff}}} \\right)^2\n   $$\n   where $\\sigma_{\\mathrm{LRV}}^2/n_{\\mathrm{eff}}$ is the known theoretical target value.\n4. The optimal exponent, $\\alpha^\\star$, is found by a grid search over $\\mathcal{A}$ to find the $\\alpha$ that minimizes $\\widehat{\\mathrm{MSE}}(\\alpha)$.\n\n### 4. Implementation Strategy\n\nThe simulation is implemented in Python using the `numpy` library for efficient vectorized computation.\n- A single pseudorandom number generator is initialized with a fixed seed to ensure the reproducibility of the entire experiment.\n- For each $(n, \\lambda)$ test case, $R = 200$ paths are generated. To ensure fair comparisons, this same set of paths is used to evaluate all $\\alpha$ values in the grid $\\mathcal{A} = \\{0.10, 0.15, \\dots, 0.90\\}$.\n- The simulation of $R$ paths is vectorized: at each time step $t$, the values $X_{t+1}$ for all $R$ paths are computed simultaneously.\n- Similarly, for a given $\\alpha$, the calculation of the batch means estimator is vectorized across the $R$ replicates. The $R$ paths are sliced to length $n_{\\mathrm{eff}}$, reshaped into a 3D array of shape $(R, a, b)$, and the batch means and their sample variances are computed along the appropriate axes. This yields $R$ estimates of $\\mathrm{Var}(\\bar{X}_{n_{\\mathrm{eff}}})$.\n- The estimated MSE is then a simple mean of the squared differences between these $R$ estimates and the theoretical target.\n- The program iterates through the specified test cases, finds the $\\alpha^\\star$ for each, and formats the results as required. The tie-breaking rule (choosing the smallest $\\alpha$ in case of equal MSEs) is naturally handled by iterating through $\\alpha$ in increasing order and only updating the optimum on strict improvement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs an empirical study of the batch means variance estimator for an AR(1) process\n    to find the MSE-optimal batching exponent alpha.\n    \"\"\"\n    \n    # Define the experiment and test case parameters from the problem statement.\n    ns = [2000, 4096, 8192]\n    lambdas = [0.5, 0.9, 0.99]\n    alphas = np.linspace(0.10, 0.90, 17) # Grid A = {0.10, 0.15, ..., 0.90}\n    R = 200\n    seed = 12345\n    \n    # Initialize a single random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n    \n    # The problem specifies the order of test cases.\n    test_cases = [(n, lam) for n in ns for lam in lambdas]\n    \n    results = []\n    \n    # Iterate through each test case (n, lambda).\n    for case in test_cases:\n        n, lam = case\n        \n        # --- Core logic for one (n, lam) pair ---\n        \n        # 1. Generate R independent paths of the stationary AR(1) process.\n        # This is done once per (n, lam) and reused for all alpha values.\n        paths = np.zeros((R, n))\n        noise_std = np.sqrt(1 - lam**2)\n        \n        # Vectorized generation of initial states and subsequent steps.\n        paths[:, 0] = rng.normal(loc=0.0, scale=1.0, size=R)\n        for t in range(n - 1):\n            noise = rng.normal(loc=0.0, scale=noise_std, size=R)\n            paths[:, t + 1] = lam * paths[:, t] + noise\n\n        # 2. Calculate the theoretical long-run variance.\n        sigma2_lrv = (1 + lam) / (1 - lam)\n\n        min_mse = float('inf')\n        best_alpha = -1.0\n        \n        # 3. Grid search over alpha to find the one that minimizes MSE.\n        for alpha in alphas:\n            # Determine batching parameters.\n            # a = number of batches, b = batch size\n            a = int(np.floor(n**alpha))\n            \n            # Per problem spec, estimator must have at least 2 batches and batch size of at least 2.\n            if a  2:\n                continue\n            \n            b = int(np.floor(n / a))\n            if b  2:\n                continue\n            \n            n_eff = a * b\n\n            # Vectorized computation of the batch means estimator across all R replicates.\n            paths_eff = paths[:, :n_eff]\n            batches = paths_eff.reshape((R, a, b))\n            batch_means = np.mean(batches, axis=2)\n            \n            # Sample variance of batch means for each replicate (ddof=1).\n            var_of_batch_means = np.var(batch_means, axis=1, ddof=1)\n            \n            # The batch means estimator for Var(X_bar_n_eff) is S_Y^2 / a.\n            var_x_bar_hat = var_of_batch_means / a\n\n            # 4. Estimate the MSE of the estimator.\n            target_variance = sigma2_lrv / n_eff\n            errors_sq = (var_x_bar_hat - target_variance)**2\n            mse = np.mean(errors_sq)\n\n            # 5. Update the best alpha.\n            # Tie-breaking (choose smallest alpha) is handled by using ''\n            # and iterating through alphas in increasing order.\n            if mse  min_mse:\n                min_mse = mse\n                best_alpha = alpha\n        \n        # Append the formatted result for the current case.\n        results.append(f\"{best_alpha:.2f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3359849"}, {"introduction": "All statistical methods are built on assumptions, and the batch means method is no exception, relying on the process having \"short memory\" so that batch means become approximately uncorrelated for large-enough batches. This advanced practice [@problem_id:3359838] confronts this limitation by exploring processes with long-range dependence, characterized by a Hurst parameter $H  0.5$. You will simulate such a process, demonstrate the systematic failure of the standard batch means estimator, and then implement a corrected, rescaled estimator to see how adapting the method can restore consistency in the face of these challenging dependencies.", "problem": "You are tasked with investigating the behavior of variance estimation via the Batch Means (BM) method when the output process exhibits long-range dependence characterized by a Hurst parameter. Consider a stationary Gaussian time series with increments that follow fractional Gaussian noise (fGn) with Hurst parameter $H \\in (0,1)$. Let $\\{X_t\\}_{t=1}^n$ be the simulated fGn increments of length $n$, with mean $0$ and variance $1$. The objective is to estimate the variability of the sample mean $\\bar{X}_n = \\frac{1}{n}\\sum_{t=1}^n X_t$ using BM under different memory regimes, and to assess the failure of standard BM for long-memory outputs when $H  0.5$, along with a remedy based on rescaling.\n\nFundamental definitions and facts:\n- The Hurst parameter $H$ controls the memory of the process. For $H = 0.5$, the increments reduce to independent and identically distributed (i.i.d.) Gaussian white noise; for $H  0.5$, the series exhibits long-range dependence with autocovariances that decay slowly and a nonstandard central limit behavior.\n- For a batch size $b \\in \\mathbb{N}$ and $a = \\lfloor n/b \\rfloor$ non-overlapping batches, define batch means $Y_j = \\frac{1}{b}\\sum_{t=(j-1)b+1}^{jb} X_t$ for $j \\in \\{1,\\dots,a\\}$. Let $s_b^2$ be the unbiased sample variance of $\\{Y_j\\}_{j=1}^a$.\n- The standard Batch Means estimator of the long-run variance (the variance scaling constant entering the central limit theorem for short-memory processes) is $\\hat{\\Gamma}_{\\mathrm{BM}}(b) = b \\, s_b^2$.\n- For short-memory processes (e.g., $H = 0.5$), $\\hat{\\Gamma}_{\\mathrm{BM}}(b)$ stabilizes as $b$ increases, targeting a finite constant. For long-memory fGn with $H  0.5$, it is known that $\\mathrm{Var}(Y_j) \\asymp C_H \\, b^{2H-2}$ for some constant $C_H  0$, implying $\\hat{\\Gamma}_{\\mathrm{BM}}(b) \\asymp C_H \\, b^{2H-1}$, which diverges as $b \\to \\infty$. A natural rescaling is therefore\n$$\n\\hat{C}_H(b) \\equiv \\frac{\\hat{\\Gamma}_{\\mathrm{BM}}(b)}{b^{2H-1}},\n$$\nwhich should stabilize near $C_H$ for $H  0.5$.\n\nYour tasks are:\n1. Simulation. For given $n$ and $H$, simulate a single realization of length $n$ of fGn with mean $0$ and variance $1$ using a method that produces the exact Gaussian covariance structure (e.g., a circulant embedding approach with the Davies–Harte construction). Use the autocovariance function\n$$\n\\gamma(k) = \\frac{1}{2}\\left(|k+1|^{2H} - 2|k|^{2H} + |k-1|^{2H}\\right), \\quad k \\in \\mathbb{Z},\n$$\nto define the Toeplitz covariance of the fGn increments.\n2. Batch Means and diagnostics across batch sizes. For a fixed grid of batch sizes $\\mathcal{B} = \\{16, 32, 64, 128, 256, 512, 1024, 2048\\}$, compute $\\hat{\\Gamma}_{\\mathrm{BM}}(b)$ for each $b \\in \\mathcal{B}$ using the first $a b$ observations, where $a = \\lfloor n/b \\rfloor$. Construct the following diagnostics:\n   - Divergence diagnostic for standard BM: Fit a least-squares line to the points $\\{(\\log b, \\log \\hat{\\Gamma}_{\\mathrm{BM}}(b)) : b \\in \\mathcal{B}\\}$ and extract the slope $\\hat{s}$. Declare “BM diverges” if $\\hat{s}  \\tau_1$.\n   - Stability diagnostic for rescaled BM: Compute $\\hat{C}_H(b) = \\hat{\\Gamma}_{\\mathrm{BM}}(b)/b^{2H-1}$ for $b \\in \\mathcal{B}$ and evaluate its coefficient of variation $\\mathrm{cv} = \\mathrm{sd}(\\{\\hat{C}_H(b)\\}_{b \\in \\mathcal{B}})/\\mathrm{mean}(\\{\\hat{C}_H(b)\\}_{b \\in \\mathcal{B}})$. Declare “RBM stable” if $\\mathrm{cv}  \\tau_2$.\n3. Thresholds and reproducibility. Use thresholds $\\tau_1 = 0.2$ and $\\tau_2 = 0.4$. For reproducibility, initialize a pseudorandom number generator with the fixed seed $12345$.\n4. Test suite. Run the procedure for the following test cases:\n   - Case A (happy path, long-memory): $(H, n) = (0.7, 65536)$.\n   - Case B (strong long-memory): $(H, n) = (0.9, 65536)$.\n   - Case C (boundary, short-memory): $(H, n) = (0.5, 65536)$.\n5. Output specification. For each case, output a pair of booleans $[\\text{BM-diverges}, \\text{RBM-stable}]$ computed as above. Aggregate the results for the three cases into a single list in the same order as specified. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[[\\mathrm{True},\\mathrm{False}],[\\mathrm{True},\\mathrm{True}],[\\mathrm{False},\\mathrm{True}]]$). No other text should be printed.\n\nAll angles, physical units, and percentages are not applicable in this problem. All numerical thresholds and parameters have been fully specified above to ensure unambiguous evaluation.", "solution": "Here, we present a step-by-step procedure to address the problem.\n\n**Step 1: Simulation of Fractional Gaussian Noise (fGn)**\n\nWe begin by generating a realization of a fractional Gaussian noise time series $\\{X_t\\}_{t=1}^n$ of length $n$ with a specified Hurst parameter $H$. This is a stationary Gaussian process with mean $0$, variance $1$, and an autocovariance function (ACVF) given by:\n$$ \\gamma(k) = \\frac{1}{2}\\left(|k+1|^{2H} - 2|k|^{2H} + |k-1|^{2H}\\right) $$\nTo generate an exact sample, we employ the Davies-Harte algorithm, a circulant embedding method.\n1.  Given the desired series length $n$, we choose a length $m$ for the circulant matrix construction such that $m \\ge 2(n-1)$ and $m$ is a power of $2$ for FFT efficiency. For $n=65536$, the minimum such $m$ is $131072 = 2^{17}$.\n2.  We construct the first row $\\mathbf{s}$ of the $m \\times m$ circulant matrix $C$. This row is defined based on the ACVF $\\gamma(k)$ to correctly embed the $n \\times n$ Toeplitz covariance matrix of the fGn series. The elements of $\\mathbf{s}=(s_0, s_1, \\dots, s_{m-1})$ are given by:\n    $$\n    s_j = \\begin{cases}\n        \\gamma(j)  \\text{if } j \\in \\{0, \\dots, n-1\\} \\\\\n        \\gamma(m-j)  \\text{if } j \\in \\{m-n+1, \\dots, m-1\\} \\\\\n        0  \\text{otherwise}\n    \\end{cases}\n    $$\n3.  The eigenvalues of $C$ are the discrete Fourier transform of $\\mathbf{s}$. We compute this using `numpy.fft.rfft`, which is efficient for real-valued input. Let the eigenvalues be $\\lambda_k$. We ensure all $\\lambda_k \\ge 0$ by clipping any small negative values resulting from floating-point inaccuracies.\n4.  We generate a sequence of complex Gaussian random variables $W_k$ in the frequency domain. These must have a specific conjugate symmetry for the resulting time series to be real. This is achieved by creating a vector of complex numbers from two independent standard normal vectors and ensuring the DC ($k=0$) and Nyquist ($k=m/2$) components are real.\n5.  We \"color\" this white noise by multiplying it with the square root of the eigenvalues: $Z_k = W_k \\sqrt{\\lambda_k}$.\n6.  An inverse real FFT (`numpy.fft.irfft`) is applied to the sequence $Z_k$ to obtain a time series of length $m$.\n7.  The first $n$ points of this series, $\\{X_t\\}_{t=1}^n$, form the required fGn sample.\nA fixed seed for the pseudorandom number generator ensures the reproducibility of this simulation.\n\n**Step 2: Batch Means Estimation**\n\nFor the simulated series $X_t$, we compute the Batch Means (BM) estimator of variance for a grid of batch sizes $\\mathcal{B} = \\{16, 32, \\dots, 2048\\}$.\n1.  For each batch size $b \\in \\mathcal{B}$, we form $a = \\lfloor n/b \\rfloor$ non-overlapping batches of data, using the first $a \\times b$ observations of the series.\n2.  We compute the mean for each batch, $Y_j = \\frac{1}{b}\\sum_{t=(j-1)b+1}^{jb} X_t$ for $j=1, \\dots, a$.\n3.  We calculate the unbiased sample variance of these batch means: $s_b^2 = \\frac{1}{a-1}\\sum_{j=1}^a (Y_j - \\bar{Y})^2$, where $\\bar{Y}$ is the grand mean of the batch means.\n4.  The standard BM estimator is then $\\hat{\\Gamma}_{\\mathrm{BM}}(b) = b \\cdot s_b^2$. We store these values for each $b$.\n\n**Step 3: Diagnostic Analysis**\n\nWe perform two diagnostics based on the behavior of the estimators across the range of batch sizes.\n1.  **Divergence Diagnostic**: The theory of long-range dependent processes suggests that for $H  0.5$, $\\hat{\\Gamma}_{\\mathrm{BM}}(b)$ diverges approximately as $b^{2H-1}$. In a log-log plot, this corresponds to a line with slope $2H-1$. We test this by performing a simple linear regression of $\\log(\\hat{\\Gamma}_{\\mathrm{BM}}(b))$ against $\\log(b)$ for all $b \\in \\mathcal{B}$. The `scipy.stats.linregress` function provides the estimated slope, $\\hat{s}$. We declare that \"BM diverges\" if $\\hat{s}  \\tau_1 = 0.2$.\n2.  **Stability Diagnostic**: A remedy for the divergence of the standard BM estimator is to rescale it. The rescaled estimator is defined as $\\hat{C}_H(b) = \\hat{\\Gamma}_{\\mathrm{BM}}(b)/b^{2H-1}$. For $H0.5$, this estimator should stabilize to a constant. For the short-memory case $H=0.5$, the exponent is $2H-1=0$, so $\\hat{C}_{0.5}(b) = \\hat{\\Gamma}_{\\mathrm{BM}}(b)$, which should also stabilize. We evaluate this stability by calculating the coefficient of variation (CV) of the set $\\{\\hat{C}_H(b) | b \\in \\mathcal{B}\\}$, defined as the sample standard deviation divided by the sample mean. If this $\\mathrm{cv}$ is less than the threshold $\\tau_2 = 0.4$, we declare that \"RBM stable\".\n\n**Step 4: Application to Test Cases**\n\nThe entire process is executed for each of the three test cases provided:\n-   Case A: $(H, n) = (0.7, 65536)$ (long memory)\n-   Case B: $(H, n) = (0.9, 65536)$ (strong long memory)\n-   Case C: $(H, n) = (0.5, 65536)$ (short memory / white noise)\n\nFor each case, we obtain a pair of boolean values $[\\text{BM-diverges}, \\text{RBM-stable}]$. These results are then aggregated into a final list for output.", "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print results.\n    \"\"\"\n    \n    def simulate_fgn(H, n, rng):\n        \"\"\"\n        Simulates fractional Gaussian noise using the Davies-Harte exact method.\n        \n        Args:\n            H (float): Hurst parameter, in (0, 1).\n            n (int): Length of the time series to generate.\n            rng (numpy.random.Generator): A NumPy random number generator.\n            \n        Returns:\n            numpy.ndarray: A 1-D array of length n representing the fGn series.\n        \"\"\"\n        m = 1\n        while m  2 * (n - 1):\n            m *= 2\n\n        def gamma_fgn(k, H):\n            k = np.asarray(k, dtype=float)\n            return 0.5 * (np.abs(k - 1)**(2 * H) - 2 * np.abs(k)**(2 * H) + np.abs(k + 1)**(2 * H))\n\n        r_acvf = gamma_fgn(np.arange(n), H)\n        \n        s = np.zeros(m)\n        s[:n] = r_acvf\n        s[m - n + 1:] = r_acvf[1:n][::-1]\n\n        lambda_ = np.fft.rfft(s)\n        lambda_[lambda_  0] = 0.0\n\n        num_freq = m // 2 + 1\n        z1 = rng.standard_normal(num_freq)\n        z2 = rng.standard_normal(num_freq)\n        \n        W = (z1 + 1j * z2) / np.sqrt(2.0)\n        W[0] = z1[0] \n        if m % 2 == 0:\n            W[-1] = z1[-1]\n\n        f_sim = W * np.sqrt(lambda_)\n        sim_series = np.fft.irfft(f_sim, n=m)\n        \n        return sim_series[:n]\n\n    def run_analysis(H, n, B, tau1, tau2, rng):\n        \"\"\"\n        Runs the full BM analysis and diagnostics for a single case.\n        \n        Args:\n            H (float): Hurst parameter.\n            n (int): Series length.\n            B (list): List of batch sizes.\n            tau1 (float): Threshold for the divergence diagnostic.\n            tau2 (float): Threshold for the stability diagnostic.\n            rng (numpy.random.Generator): A NumPy random number generator.\n\n        Returns:\n            list: A list of two booleans [BM-diverges, RBM-stable].\n        \"\"\"\n        X = simulate_fgn(H, n, rng)\n        \n        gamma_hat_list = []\n        c_hat_list = []\n        log_b_list = []\n\n        B_numeric = np.array(B, dtype=float)\n\n        for b in B_numeric:\n            b_int = int(b)\n            a = n // b_int\n            if a  2: continue # Need at least 2 batches for variance calculation\n\n            X_trunc = X[:a * b_int]\n            Y = X_trunc.reshape(a, b_int).mean(axis=1)\n            \n            s_b_sq = np.var(Y, ddof=1)\n            gamma_hat = b * s_b_sq\n            \n            gamma_hat_list.append(gamma_hat)\n            log_b_list.append(np.log(b))\n\n        gamma_hat_list = np.array(gamma_hat_list)\n        \n        # Divergence diagnostic\n        log_gamma_hat_list = np.log(gamma_hat_list)\n        regression_result = stats.linregress(log_b_list, log_gamma_hat_list)\n        s_hat = regression_result.slope\n        bm_diverges = s_hat > tau1\n\n        # Stability diagnostic\n        rescaling_exponent = 2 * H - 1\n        c_hat_list = gamma_hat_list / (B_numeric[:len(gamma_hat_list)] ** rescaling_exponent)\n        \n        mean_c_hat = np.mean(c_hat_list)\n        std_c_hat = np.std(c_hat_list, ddof=1)\n        \n        cv = std_c_hat / mean_c_hat if mean_c_hat != 0 else np.inf\n        rbm_stable = cv  tau2\n\n        return [bm_diverges, rbm_stable]\n\n    # Problem parameters\n    seed = 12345\n    rng = np.random.default_rng(seed)\n    n = 65536\n    batch_sizes = [16, 32, 64, 128, 256, 512, 1024, 2048]\n    tau1 = 0.2\n    tau2 = 0.4\n\n    # Test cases\n    test_cases = [\n        (0.7, n),  # Case A\n        (0.9, n),  # Case B\n        (0.5, n)   # Case C\n    ]\n    \n    results = []\n    for H_val, n_val in test_cases:\n        result = run_analysis(H_val, n_val, batch_sizes, tau1, tau2, rng)\n        results.append(result)\n\n    # Format output string to match [[True,False],[True,True],...]\n    result_strings = [str(r).replace(' ', '') for r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3359838"}]}