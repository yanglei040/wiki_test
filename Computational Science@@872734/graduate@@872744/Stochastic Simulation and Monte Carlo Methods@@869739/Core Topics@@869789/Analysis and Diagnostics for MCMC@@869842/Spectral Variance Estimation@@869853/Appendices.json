{"hands_on_practices": [{"introduction": "The autoregressive process of order one, or AR(1), is a cornerstone model for time series analysis, capturing the common scenario where an observation is a scaled version of the previous one plus random noise. This exercise provides a foundational calculation, guiding you to derive the spectral density at zero frequency, $f(0)$, directly from the model's parameters. By completing this practice [@problem_id:3346157], you will establish the crucial theoretical link between the process dynamics ($\\phi$, $\\sigma_{\\epsilon}^2$), its autocovariance structure, and the resulting long-run variance, which is essential for assessing the precision of Monte Carlo estimators.", "problem": "Consider a strictly stationary, zero-mean, Gaussian autoregressive of order one (AR(1)) process, defined by $X_{t}=\\phi X_{t-1}+\\epsilon_{t}$ with $|\\phi|1$, where the innovations $\\epsilon_{t}$ are independent and identically distributed with $\\epsilon_{t}\\sim \\mathcal{N}(0,\\sigma_{\\epsilon}^{2})$. In the context of spectral variance estimation for Monte Carlo time-average estimators, the spectral density $f(\\omega)$ of $\\{X_{t}\\}$ is defined as the Fourier transform of the autocovariance function $\\gamma_{k}=\\operatorname{Cov}(X_{t},X_{t+k})$,\n$$\nf(\\omega)=\\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty}\\gamma_{k}\\exp(-\\mathrm{i}k\\omega),\n$$\nand the long-run variance $\\sigma^{2}$ (also called the spectral variance) is defined either as the absolutely convergent series $\\sigma^{2}=\\gamma_{0}+2\\sum_{k=1}^{\\infty}\\gamma_{k}$ or equivalently as $\\sigma^{2}=2\\pi f(0)$.\n\nStarting from these definitions and the given stochastic difference equation, derive $f(0)$ explicitly in terms of $\\phi$ and $\\sigma_{\\epsilon}^{2}$, and hence obtain a closed-form expression for $\\sigma^{2}$. Provide your final answer as exact analytic expressions in terms of $\\phi$ and $\\sigma_{\\epsilon}^{2}$, listing $f(0)$ and $\\sigma^{2}$ in that order. No numerical approximation or rounding is required.", "solution": "The problem asks for the derivation of the spectral density at frequency zero, $f(0)$, and the long-run variance, $\\sigma^2$, for a strictly stationary, zero-mean, Gaussian AR(1) process defined by $X_{t}=\\phi X_{t-1}+\\epsilon_{t}$. The innovations $\\epsilon_{t}$ are i.i.d. with distribution $\\mathcal{N}(0,\\sigma_{\\epsilon}^{2})$, and the stationarity condition $|\\phi|1$ holds.\n\nFirst, we determine the autocovariance function $\\gamma_{k}=\\operatorname{Cov}(X_{t},X_{t+k})$. Since the process is zero-mean, $\\gamma_{k} = \\operatorname{E}[X_{t}X_{t+k}]$.\nThe variance of the process, $\\gamma_{0}$, can be found by taking the variance of the defining equation:\n$$\n\\operatorname{Var}(X_{t}) = \\operatorname{Var}(\\phi X_{t-1} + \\epsilon_{t})\n$$\nDue to stationarity, $\\operatorname{Var}(X_{t}) = \\operatorname{Var}(X_{t-1}) = \\gamma_{0}$. The term $X_{t-1}$ is a function of past innovations $\\{\\epsilon_{t-1}, \\epsilon_{t-2}, \\dots\\}$ and is therefore uncorrelated with the current innovation $\\epsilon_{t}$. Thus, $\\operatorname{Cov}(X_{t-1}, \\epsilon_{t}) = 0$.\nThe variance of the sum of uncorrelated random variables is the sum of their variances:\n$$\n\\gamma_{0} = \\operatorname{Var}(\\phi X_{t-1}) + \\operatorname{Var}(\\epsilon_{t}) = \\phi^{2}\\operatorname{Var}(X_{t-1}) + \\sigma_{\\epsilon}^{2}\n$$\n$$\n\\gamma_{0} = \\phi^{2}\\gamma_{0} + \\sigma_{\\epsilon}^{2}\n$$\nSolving for $\\gamma_{0}$ yields:\n$$\n\\gamma_{0}(1-\\phi^{2}) = \\sigma_{\\epsilon}^{2} \\implies \\gamma_{0} = \\frac{\\sigma_{\\epsilon}^{2}}{1-\\phi^{2}}\n$$\nThe condition $|\\phi|1$ ensures that the variance $\\gamma_{0}$ is finite and positive.\n\nNext, we find the autocovariance $\\gamma_{k}$ for lag $k > 0$:\n$$\n\\gamma_{k} = \\operatorname{E}[X_{t}X_{t+k}] = \\operatorname{E}[X_{t}(\\phi X_{t+k-1} + \\epsilon_{t+k})]\n$$\nBy linearity of expectation:\n$$\n\\gamma_{k} = \\phi \\operatorname{E}[X_{t}X_{t+k-1}] + \\operatorname{E}[X_{t}\\epsilon_{t+k}]\n$$\nFor $k0$, the innovation $\\epsilon_{t+k}$ is uncorrelated with $X_{t}$. Since both have zero mean, their covariance equals their expectation product: $\\operatorname{E}[X_{t}\\epsilon_{t+k}]=\\operatorname{E}[X_t]\\operatorname{E}[\\epsilon_{t+k}]=0 \\cdot 0 = 0$.\nThe expression simplifies to a recurrence relation:\n$$\n\\gamma_{k} = \\phi \\operatorname{E}[X_{t}X_{t+k-1}] = \\phi \\gamma_{k-1}\n$$\nfor $k \\ge 1$. This implies $\\gamma_{k} = \\phi^{k}\\gamma_{0}$ for $k \\ge 0$. The autocovariance function is symmetric, $\\gamma_{-k} = \\gamma_{k}$, so we can write the general form as $\\gamma_{k} = \\phi^{|k|}\\gamma_{0}$ for any integer $k$.\n\nThe spectral density is defined as $f(\\omega)=\\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty}\\gamma_{k}\\exp(-\\mathrm{i}k\\omega)$. We are asked to find $f(0)$, which corresponds to setting $\\omega=0$:\n$$\nf(0) = \\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty}\\gamma_{k}\\exp(0) = \\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty}\\gamma_{k}\n$$\nSubstituting the expression for $\\gamma_k$:\n$$\nf(0) = \\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty}\\phi^{|k|}\\gamma_{0} = \\frac{\\gamma_{0}}{2\\pi}\\sum_{k=-\\infty}^{\\infty}\\phi^{|k|}\n$$\nThe summation can be split into two parts:\n$$\n\\sum_{k=-\\infty}^{\\infty}\\phi^{|k|} = \\sum_{k=0}^{\\infty}\\phi^{k} + \\sum_{k=-\\infty}^{-1}\\phi^{-k}\n$$\nThe first part is a standard geometric series which, for $|\\phi|1$, converges to $\\frac{1}{1-\\phi}$. For the second part, let $j=-k$. As $k$ goes from $-1$ to $-\\infty$, $j$ goes from $1$ to $\\infty$:\n$$\n\\sum_{k=-\\infty}^{-1}\\phi^{-k} = \\sum_{j=1}^{\\infty}\\phi^{j} = \\frac{\\phi}{1-\\phi}\n$$\nCombining the two parts:\n$$\n\\sum_{k=-\\infty}^{\\infty}\\phi^{|k|} = \\frac{1}{1-\\phi} + \\frac{\\phi}{1-\\phi} = \\frac{1+\\phi}{1-\\phi}\n$$\nNow, substitute this sum back into the expression for $f(0)$:\n$$\nf(0) = \\frac{\\gamma_{0}}{2\\pi} \\left( \\frac{1+\\phi}{1-\\phi} \\right)\n$$\nFinally, substituting the expression for $\\gamma_{0} = \\frac{\\sigma_{\\epsilon}^{2}}{1-\\phi^{2}} = \\frac{\\sigma_{\\epsilon}^{2}}{(1-\\phi)(1+\\phi)}$:\n$$\nf(0) = \\frac{1}{2\\pi} \\left( \\frac{\\sigma_{\\epsilon}^{2}}{(1-\\phi)(1+\\phi)} \\right) \\left( \\frac{1+\\phi}{1-\\phi} \\right) = \\frac{\\sigma_{\\epsilon}^{2}}{2\\pi(1-\\phi)^{2}}\n$$\nThis is the closed-form expression for $f(0)$.\n\nThe long-run variance $\\sigma^2$ is given by the relation $\\sigma^2 = 2\\pi f(0)$. Using our result for $f(0)$:\n$$\n\\sigma^2 = 2\\pi \\left( \\frac{\\sigma_{\\epsilon}^{2}}{2\\pi(1-\\phi)^{2}} \\right) = \\frac{\\sigma_{\\epsilon}^{2}}{(1-\\phi)^{2}}\n$$\nThis is the closed-form expression for the long-run variance $\\sigma^2$.\nThe requested quantities are $f(0)$ and $\\sigma^2$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\sigma_{\\epsilon}^{2}}{2\\pi(1-\\phi)^{2}}  \\frac{\\sigma_{\\epsilon}^{2}}{(1- \\phi)^{2}} \\end{pmatrix}}\n$$", "id": "3346157"}, {"introduction": "While autoregressive models have infinite memory, many processes are better described by a finite dependence on past shocks. The moving-average process of order one, MA(1), is the simplest such model, where the current value depends only on the current and immediately preceding noise terms. This exercise [@problem_id:3346167] offers a complementary perspective to the AR(1) model by asking you to compute the long-run variance for a process whose autocovariances are zero beyond a finite lag. This provides a concrete and tractable application of the definition $\\sigma^2 = \\sum_{k=-\\infty}^{\\infty} \\gamma_k$, reinforcing your understanding of how different correlation structures map to the spectral variance.", "problem": "Consider a moving-average process of order one, defined by $X_{t} = \\epsilon_{t} + \\theta \\epsilon_{t-1}$, where $\\{\\epsilon_{t}\\}_{t \\in \\mathbb{Z}}$ is a zero-mean white noise sequence with variance $\\sigma_{\\epsilon}^{2}$ and is uncorrelated across time. Let the autocovariance function be defined by $\\gamma_{k} = \\operatorname{Cov}(X_{t}, X_{t+k})$ for all integers $k$, and define the long-run variance (also called the time-average variance constant) by $\\sigma^{2} := \\gamma_{0} + 2 \\sum_{k=1}^{\\infty} \\gamma_{k}$. \n\nStarting only from these definitions and the properties of white noise, first derive the entire sequence $\\{\\gamma_{k}\\}_{k \\in \\mathbb{Z}}$ for this process. Then evaluate the long-run variance $\\sigma^{2}$ in closed form as a function of $\\theta$ and $\\sigma_{\\epsilon}^{2}$. \n\nReport only the closed-form expression for $\\sigma^{2}$ as your final answer. No numerical approximation is required, and no units are necessary.", "solution": "The problem defines a moving-average process of order one, MA($1$), as $X_{t} = \\epsilon_{t} + \\theta \\epsilon_{t-1}$, where $\\{\\epsilon_{t}\\}_{t \\in \\mathbb{Z}}$ is a zero-mean white noise sequence with variance $\\sigma_{\\epsilon}^{2}$. The properties of a white noise process imply that $\\operatorname{E}[\\epsilon_{t}] = 0$ for all integers $t$, and the autocovariance of the noise is $\\operatorname{E}[\\epsilon_{t} \\epsilon_{s}] = \\sigma_{\\epsilon}^{2} \\delta_{ts}$, where $\\delta_{ts}$ is the Kronecker delta, which is $1$ if $t=s$ and $0$ otherwise.\n\nFirst, we determine the mean of the process $X_{t}$. By linearity of expectation:\n$$\n\\operatorname{E}[X_{t}] = \\operatorname{E}[\\epsilon_{t} + \\theta \\epsilon_{t-1}] = \\operatorname{E}[\\epsilon_{t}] + \\theta \\operatorname{E}[\\epsilon_{t-1}]\n$$\nSince $\\{\\epsilon_{t}\\}$ is a zero-mean process, $\\operatorname{E}[\\epsilon_{t}] = 0$ for any $t$. Thus:\n$$\n\\operatorname{E}[X_{t}] = 0 + \\theta \\cdot 0 = 0\n$$\nThe process $X_{t}$ is therefore a zero-mean process.\n\nThe autocovariance function is defined as $\\gamma_{k} = \\operatorname{Cov}(X_{t}, X_{t+k})$. Because the process has a mean of zero, this simplifies to $\\gamma_{k} = \\operatorname{E}[X_{t} X_{t+k}]$. Substituting the definition of $X_{t}$:\n$$\n\\gamma_{k} = \\operatorname{E}[(\\epsilon_{t} + \\theta \\epsilon_{t-1})(\\epsilon_{t+k} + \\theta \\epsilon_{t+k-1})]\n$$\nExpanding the product inside the expectation yields:\n$$\n\\gamma_{k} = \\operatorname{E}[\\epsilon_{t}\\epsilon_{t+k} + \\theta \\epsilon_{t}\\epsilon_{t+k-1} + \\theta \\epsilon_{t-1}\\epsilon_{t+k} + \\theta^2 \\epsilon_{t-1}\\epsilon_{t+k-1}]\n$$\nUsing the linearity of the expectation operator, we can write:\n$$\n\\gamma_{k} = \\operatorname{E}[\\epsilon_{t}\\epsilon_{t+k}] + \\theta \\operatorname{E}[\\epsilon_{t}\\epsilon_{t+k-1}] + \\theta \\operatorname{E}[\\epsilon_{t-1}\\epsilon_{t+k}] + \\theta^2 \\operatorname{E}[\\epsilon_{t-1}\\epsilon_{t+k-1}]\n$$\nWe now evaluate this expression for different values of lag $k$, using the fact that $\\operatorname{E}[\\epsilon_{i}\\epsilon_{j}]$ is $\\sigma_{\\epsilon}^{2}$ if $i=j$ and $0$ if $i \\neq j$.\n\nCase $k=0$: This gives the variance of the process, $\\gamma_{0} = \\operatorname{Var}(X_{t})$.\n$$\n\\gamma_{0} = \\operatorname{E}[\\epsilon_{t}\\epsilon_{t}] + \\theta \\operatorname{E}[\\epsilon_{t}\\epsilon_{t-1}] + \\theta \\operatorname{E}[\\epsilon_{t-1}\\epsilon_{t}] + \\theta^2 \\operatorname{E}[\\epsilon_{t-1}\\epsilon_{t-1}]\n$$\nThe terms $\\operatorname{E}[\\epsilon_{t}\\epsilon_{t-1}]$ and $\\operatorname{E}[\\epsilon_{t-1}\\epsilon_{t}]$ are zero because the time indices are different. The terms $\\operatorname{E}[\\epsilon_{t}\\epsilon_{t}]$ and $\\operatorname{E}[\\epsilon_{t-1}\\epsilon_{t-1}]$ are both equal to the variance of the noise process, $\\sigma_{\\epsilon}^{2}$.\n$$\n\\gamma_{0} = \\sigma_{\\epsilon}^{2} + \\theta(0) + \\theta(0) + \\theta^2 \\sigma_{\\epsilon}^{2} = (1 + \\theta^2) \\sigma_{\\epsilon}^{2}\n$$\n\nCase $k=1$: This gives the lag-$1$ autocovariance.\n$$\n\\gamma_{1} = \\operatorname{E}[\\epsilon_{t}\\epsilon_{t+1}] + \\theta \\operatorname{E}[\\epsilon_{t}\\epsilon_{t}] + \\theta \\operatorname{E}[\\epsilon_{t-1}\\epsilon_{t+1}] + \\theta^2 \\operatorname{E}[\\epsilon_{t-1}\\epsilon_{t}]\n$$\nThe only term with matching indices is $\\operatorname{E}[\\epsilon_{t}\\epsilon_{t}] = \\sigma_{\\epsilon}^{2}$. All other expectation terms are zero.\n$$\n\\gamma_{1} = 0 + \\theta \\sigma_{\\epsilon}^{2} + 0 + 0 = \\theta \\sigma_{\\epsilon}^{2}\n$$\n\nCase $|k| \\ge 2$: Let's consider $k \\ge 2$. The autocovariance function is even, so $\\gamma_{-k} = \\gamma_{k}$. The four expectation terms are $\\operatorname{E}[\\epsilon_{t}\\epsilon_{t+k}]$, $\\operatorname{E}[\\epsilon_{t}\\epsilon_{t+k-1}]$, $\\operatorname{E}[\\epsilon_{t-1}\\epsilon_{t+k}]$, and $\\operatorname{E}[\\epsilon_{t-1}\\epsilon_{t+k-1}]$. For $k \\ge 2$, the differences between the time indices in each pair are $k$, $k-1$, $k+1$, and $k$, respectively. Since $k \\ge 2$, all these differences are non-zero. Therefore, all four expectation terms evaluate to zero.\n$$\n\\gamma_{k} = 0 + \\theta(0) + \\theta(0) + \\theta^2(0) = 0 \\quad \\text{for } |k| \\ge 2\n$$\nThe autocovariance function for the MA($1$) process is thus:\n$$\n\\gamma_{k} =\n\\begin{cases}\n(1 + \\theta^2) \\sigma_{\\epsilon}^{2}  \\text{if } k = 0 \\\\\n\\theta \\sigma_{\\epsilon}^{2}  \\text{if } |k| = 1 \\\\\n0  \\text{if } |k| \\ge 2\n\\end{cases}\n$$\nThis completes the first part of the derivation.\n\nNext, we evaluate the long-run variance $\\sigma^{2}$, which is defined as:\n$$\n\\sigma^{2} \\coloneqq \\gamma_{0} + 2 \\sum_{k=1}^{\\infty} \\gamma_{k}\n$$\nWe compute the summation term first. Using our derived values for $\\gamma_k$:\n$$\n\\sum_{k=1}^{\\infty} \\gamma_{k} = \\gamma_{1} + \\gamma_{2} + \\gamma_{3} + \\dots\n$$\nSince $\\gamma_{k}=0$ for all $k \\ge 2$, the infinite series truncates to a single term:\n$$\n\\sum_{k=1}^{\\infty} \\gamma_{k} = \\gamma_{1} + 0 + 0 + \\dots = \\gamma_{1} = \\theta \\sigma_{\\epsilon}^{2}\n$$\nNow, we substitute the expressions for $\\gamma_{0}$ and the sum back into the definition of $\\sigma^{2}$:\n$$\n\\sigma^{2} = \\gamma_{0} + 2 \\left( \\sum_{k=1}^{\\infty} \\gamma_{k} \\right) = (1 + \\theta^2) \\sigma_{\\epsilon}^{2} + 2 (\\theta \\sigma_{\\epsilon}^{2})\n$$\nFactoring out the common term $\\sigma_{\\epsilon}^{2}$:\n$$\n\\sigma^{2} = (1 + \\theta^2 + 2\\theta) \\sigma_{\\epsilon}^{2}\n$$\nThe expression in the parentheses, $1 + 2\\theta + \\theta^2$, is a perfect square trinomial, equal to $(1 + \\theta)^2$.\nTherefore, the closed-form expression for the long-run variance is:\n$$\n\\sigma^{2} = (1 + \\theta)^2 \\sigma_{\\epsilon}^{2}\n$$", "answer": "$$\n\\boxed{(1 + \\theta)^{2} \\sigma_{\\epsilon}^{2}}\n$$", "id": "3346167"}, {"introduction": "Moving from theoretical models to practical implementation, this exercise [@problem_id:3346100] demonstrates the diagnostic power of spectral variance estimation in a realistic MCMC setting. The quality of a Monte Carlo simulation depends critically on the quality of the underlying random number generator (RNG), and subtle flaws can introduce artifacts that invalidate results. In this hands-on problem, you will implement several estimators of the spectral density at zero and use them to build a diagnostic test capable of detecting distortions caused by a flawed RNG, revealing how theoretical concepts translate into essential tools for ensuring simulation fidelity.", "problem": "Consider a stationary scalar time series generated by a reversible Markov Chain Monte Carlo (MCMC) sampler targeting a standard normal distribution. Let the time series be denoted by $\\{Y_t\\}_{t=1}^n$, where $Y_t = g(X_t)$ and $\\{X_t\\}$ is a Markov chain with invariant density proportional to $\\exp(-x^2/2)$. Assume $g(x) = x$ so that $Y_t = X_t$. Define the spectral density $S(\\omega)$ of $\\{Y_t\\}$ as the Fourier transform of its autocovariance function $\\gamma_k = \\mathrm{Cov}(Y_t,Y_{t+k})$, that is, $S(\\omega) = \\sum_{k=-\\infty}^{\\infty} \\gamma_k e^{-i \\omega k}$ for $\\omega \\in [-\\pi,\\pi]$. The zero-frequency spectral density $S(0)$ governs the asymptotic variance of the sample mean $\\bar{Y}_n$, in the sense that $\\mathrm{Var}(\\bar{Y}_n) \\approx S(0)/n$ under appropriate mixing conditions.\n\nYou will investigate the sensitivity of an estimator $\\hat{S}(0)$ to flaws in the Random Number Generator (RNG) by injecting lattice artifacts into MCMC transitions and quantifying deviations in the periodogram $I(\\omega)$ near $\\omega = 0$. Specifically:\n\n- Construct a Random-Walk Metropolis sampler with proposal $Y = X + \\sigma Z$, where $Z$ is standard normal. Use a symmetric proposal so that the Metropolis acceptance probability is $\\alpha(X,Y) = \\min\\{1, \\exp(-(Y^2 - X^2)/2)\\}$.\n- To inject lattice artifacts, quantize proposals to a grid of spacing $\\delta  0$ by replacing $Y$ with $Y_q = \\delta \\,\\mathrm{round}(Y/\\delta)$ before applying the acceptance rule. This quantization is intended to mimic lattice structure introduced by flawed RNGs or deterministic artifacts.\n- Consider two types of RNGs for driving the proposal noise: a high-quality generator and a flawed linear congruential generator with a small modulus that exhibits short-period lattice behavior. Implement the flawed generator explicitly and use it to produce uniform variates transformed to normal variates via a method of your choice.\n\nYour tasks are:\n\n1. Generate the Markov chain for a burn-in period followed by $n$ iterations, and extract the time series $\\{Y_t\\}_{t=1}^n$ after centering by subtracting its sample mean.\n2. Estimate the zero-frequency spectral density using a lag-window estimator with a Bartlett kernel and a finite bandwidth. The estimator must be constructed from first principles, starting from autocovariance estimates of the centered series and an explicit taper function supported on a finite set of lags. Do not assume any shortcut formulas; use the core definitions of autocovariance and kernel weighting to build the estimator.\n3. Compute the periodogram $I(\\omega_j)$ at the discrete Fourier frequencies $\\omega_j = 2\\pi j/n$ for $j=0,\\dots,\\lfloor n/2 \\rfloor$ using the Fast Fourier Transform (FFT). Quantify deviations near zero frequency by forming the ratio $R = \\bar{I}_{\\text{low}}/\\bar{I}_{\\text{mid}}$, where $\\bar{I}_{\\text{low}}$ is the average of $I(\\omega_j)$ for the $J$ smallest positive frequencies and $\\bar{I}_{\\text{mid}}$ is the average over $J$ frequencies centered at $j \\approx n/4$.\n4. Form an independent reference estimator of $S(0)$ using the non-overlapping batch means method with a batch size that grows with $n$. Starting from the definition of batch means and their variance, derive and implement a consistent estimator for $S(0)$.\n5. Design a decision rule that flags under-dispersed spectral variance by comparing the Bartlett lag-window estimate of $S(0)$ to the batch-means reference and the near-zero periodogram ratio. Your rule should produce a boolean flag that is true if and only if both the Bartlett estimate is substantially smaller than the batch-means estimate and the near-zero periodogram ratio indicates suppressed low-frequency power.\n\nFor reproducibility and coverage, use the following test suite of parameter sets $(n,\\sigma,\\delta,\\text{rng},J)$:\n\n- Test 1 (happy path): $(8192, 1.25, 0.0, \\text{good}, 16)$\n- Test 2 (lattice artifact, moderate): $(8192, 1.25, 0.5, \\text{lcg}, 16)$\n- Test 3 (lattice artifact, strong): $(8192, 1.25, 1.0, \\text{lcg}, 16)$\n- Test 4 (boundary case, shorter chain): $(2048, 1.25, 0.0, \\text{good}, 8)$\n\nAll angles must be in radians; no physical units are involved. For each test case, your program must compute the boolean flag described in item 5. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,true,false]\"). The exact output must be a list of Python booleans \"True\" or \"False\" in order corresponding to the tests above.", "solution": "The problem requires an investigation into the sensitivity of spectral variance estimators to artifacts in a Random Number Generator (RNG) used within a Markov Chain Monte Carlo (MCMC) simulation. This will be accomplished by implementing a Metropolis sampler, injecting specific artifacts, and evaluating the resulting time series using several spectral analysis techniques. The final output is a decision rule that flags potential issues based on these analyses.\n\n### 1. MCMC Simulation Framework\n\nA Random-Walk Metropolis (RWM) sampler is constructed to generate a time series $\\{X_t\\}$ from a target distribution with probability density function $p(x) \\propto \\exp(-x^2/2)$, which is the standard normal distribution $\\mathcal{N}(0,1)$. The observable is $g(x)=x$, so the time series is $Y_t = X_t$.\n\nThe RWM algorithm proceeds as follows:\n1.  Given the current state $X_t$, a proposal $Y$ is generated from a proposal distribution $q(Y|X_t)$. We use a normal random walk: $Y = X_t + \\sigma Z$, where $Z \\sim \\mathcal{N}(0,1)$ and $\\sigma$ is the proposal step size.\n2.  An optional artifact is introduced by quantizing the proposal: $Y_q = \\delta \\cdot \\mathrm{round}(Y/\\delta)$. If $\\delta = 0$, no quantization is performed, and $Y_q = Y$.\n3.  The proposal $Y_q$ is accepted with probability $\\alpha(X_t, Y_q) = \\min\\left(1, \\frac{p(Y_q)}{p(X_t)}\\right)$. Since the proposal is symmetric, the Hastings ratio is $1$. The acceptance probability simplifies to $\\alpha(X_t, Y_q) = \\min\\left(1, \\exp\\left(-\\frac{1}{2}(Y_q^2 - X_t^2)\\right)\\right)$.\n4.  If the proposal is accepted, $X_{t+1} = Y_q$. Otherwise, the chain remains at its current state, $X_{t+1} = X_t$.\n\nA burn-in period of $2000$ iterations is used to allow the chain to converge to its stationary distribution before collecting $n$ samples for analysis. The initial state of the chain is $X_0 = 0$.\n\n### 2. Random Number Generators and Artifacts\n\nTwo types of RNGs are used to generate the standard normal variates $Z$ for the proposals.\n\n-   **High-Quality RNG**: The `numpy.random.default_rng()` generator, which is based on the PCG64 algorithm, serves as the high-quality, \"good\" RNG.\n-   **Flawed RNG**: A Linear Congruential Generator (LCG) with a small modulus is implemented to simulate a flawed RNG. The LCG is defined by the recurrence relation $S_{i+1} = (a S_i + c) \\pmod m$. We choose parameters known to produce poor results: $a=1229$, $c=1$, and $m=2^{16}$. Uniform variates $U_i \\in [0, 1)$ are produced via $U_i = S_i / m$. Pairs of these uniform variates are then transformed into standard normal variates using the Box-Muller transform:\n    $$Z_1 = \\sqrt{-2 \\ln U_1} \\cos(2\\pi U_2)$$\n    $$Z_2 = \\sqrt{-2 \\ln U_1} \\sin(2\\pi U_2)$$\n    This combination of a short-period LCG and the Box-Muller transform is known to produce structured artifacts in the resulting normal deviates.\n\n### 3. Spectral Analysis and Estimators of $S(0)$\n\nThe primary goal is to estimate the spectral density at zero frequency, $S(0) = \\sum_{k=-\\infty}^{\\infty} \\gamma_k$, where $\\gamma_k$ is the autocovariance of the stationary process at lag $k$. For the analysis, the raw time series $\\{Y_t\\}_{t=1}^n$ is first centered by subtracting its sample mean $\\bar{Y}_n = \\frac{1}{n}\\sum_{t=1}^n Y_t$. Let the centered series be $\\{y_t\\}_{t=1}^n$ where $y_t = Y_t - \\bar{Y}_n$.\n\n#### 3.1. Bartlett Lag-Window Estimator\n\nThis estimator is based on a weighted sum of sample autocovariances.\nThe sample autocovariance at lag $k \\ge 0$ is estimated as:\n$$ \\hat{\\gamma}_k = \\frac{1}{n} \\sum_{t=1}^{n-k} y_t y_{t+k} $$\nThe Bartlett lag-window estimator of $S(0)$ is given by:\n$$ \\hat{S}_B(0) = \\sum_{k=-(M-1)}^{M-1} \\lambda_B(k/M) \\hat{\\gamma}_k $$\nwhere $M$ is the truncation lag (bandwidth parameter) and $\\lambda_B(u) = 1 - |u|$ for $|u| \\le 1$ ($0$ otherwise) is the Bartlett kernel. Due to the symmetry $\\hat{\\gamma}_k = \\hat{\\gamma}_{-k}$, the estimator simplifies to:\n$$ \\hat{S}_B(0) = \\hat{\\gamma}_0 + 2 \\sum_{k=1}^{M-1} \\left(1 - \\frac{k}{M}\\right) \\hat{\\gamma}_k $$\nFor consistency, the truncation lag $M$ must grow with $n$ such that $M \\to \\infty$ and $M/n \\to 0$. A standard choice is $M = \\lfloor n^{1/3} \\rfloor$.\n\n#### 3.2. Non-Overlapping Batch Means (NOBM) Estimator\n\nThe NOBM method provides an alternative, consistent estimator for $S(0)$ under weak mixing conditions. It serves as a more robust reference estimator.\n1.  The time series $\\{Y_t\\}_{t=1}^n$ is divided into $a_n$ non-overlapping batches, each of size $m_n$. For simplicity, we assume $n = a_n m_n$.\n2.  The mean of each batch is calculated:\n    $$ B_j = \\frac{1}{m_n} \\sum_{i=1}^{m_n} Y_{(j-1)m_n + i} \\quad \\text{for} \\quad j = 1, \\dots, a_n $$\n3.  Under stationarity and mixing, for large $m_n$, the batch means $B_j$ are approximately independent and identically distributed normal random variables with $\\mathrm{Var}(B_j) \\approx S(0)/m_n$.\n4.  The variance of the grand sample mean $\\bar{Y}_n$ is related to the variance of the batch means: $\\mathrm{Var}(\\bar{Y}_n) \\approx \\frac{1}{a_n} \\mathrm{Var}(B_j)$.\n5.  Estimating $\\mathrm{Var}(B_j)$ by the sample variance of the batch means, we get an estimator for $S(0)$:\n    $$ \\hat{S}_{NOBM}(0) = m_n \\cdot \\frac{1}{a_n-1} \\sum_{j=1}^{a_n} (B_j - \\bar{Y}_n)^2 $$\n    where $\\bar{Y}_n$ is the overall sample mean.\nFor consistency, the batch size $m_n$ must grow with $n$. A standard choice is $m_n = \\lfloor n^{1/2} \\rfloor$, which implies $a_n = \\lfloor n/m_n \\rfloor \\approx \\lfloor n^{1/2} \\rfloor$.\n\n#### 3.3. Periodogram Analysis\n\nThe periodogram is an estimator of the spectral density at the discrete Fourier frequencies $\\omega_j = 2\\pi j/n$. It is calculated from the centered series $\\{y_t\\}$:\n$$ I(\\omega_j) = \\frac{1}{n} \\left| \\sum_{t=1}^n y_t e^{-i \\omega_j t} \\right|^2 $$\nThis is efficiently computed using the Fast Fourier Transform (FFT).\nTo quantify the suppression of power near zero frequency, we compute the ratio $R$:\n$$ R = \\frac{\\bar{I}_{\\text{low}}}{\\bar{I}_{\\text{mid}}} $$\nwhere:\n-   $\\bar{I}_{\\text{low}} = \\frac{1}{J} \\sum_{j=1}^{J} I(\\omega_j)$ is the average periodogram value over the $J$ lowest positive frequencies.\n-   $\\bar{I}_{\\text{mid}} = \\frac{1}{J} \\sum_{k} I(\\omega_k)$ is the average periodogram value over $J$ frequencies centered at the index $j_{\\text{mid}} = \\lfloor n/4 \\rfloor$.\n\nFor a process with positive autocorrelation, we expect $S(0)  S(\\omega)$ for $\\omega  0$, which would typically yield $R  1$. A value of $R  1$ indicates an unusual depression of low-frequency power relative to mid-range frequencies, a potential sign of artifacts.\n\n### 4. Decision Rule for Artifact Detection\n\nA decision rule is formulated to flag a time series as potentially flawed. The rule is designed to detect under-dispersion in the spectral variance, a characteristic symptom of lattice artifacts. An alert is triggered (flag is `True`) if and only if both of the following conditions are met:\n\n1.  **Discrepancy between Estimators**: The Bartlett estimator is substantially smaller than the more robust NOBM estimator. The Bartlett estimator, being based on short-lag correlations, is sensitive to the local chain behavior altered by quantization, while the NOBM estimator captures longer-term variance. We formalize this as:\n    $$ \\hat{S}_B(0)  0.5 \\cdot \\hat{S}_{NOBM}(0) $$\n2.  **Suppressed Low-Frequency Power**: The periodogram ratio $R$ indicates a deficit of power near zero frequency. This is quantified by the condition:\n    $$ R  0.7 $$\n\nThis two-pronged rule combines evidence from both the time domain (via autocovariances and batching) and the frequency domain (via the periodogram) to make a robust decision.", "answer": "[False,True,True,False]", "id": "3346100"}]}