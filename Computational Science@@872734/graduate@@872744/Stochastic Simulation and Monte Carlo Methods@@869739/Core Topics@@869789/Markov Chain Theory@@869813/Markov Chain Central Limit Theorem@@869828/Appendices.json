{"hands_on_practices": [{"introduction": "The first-order autoregressive (AR(1)) process is a cornerstone of time-series analysis and provides a canonical example for understanding the Markov chain central limit theorem (MCCLT) in a continuous state space. This exercise guides you through deriving the chain's fundamental correlation structure from first principles. By calculating the asymptotic variance and integrated autocorrelation time, you will build a concrete understanding of how temporal dependence in a Markov chain dictates the uncertainty of its long-run averages.", "problem": "Consider the first-order autoregressive Markov chain (AR(1)) on the real line defined by $X_{n+1}=\\phi X_n+\\epsilon_{n+1}$, where $|\\phi|1$ and $\\{\\epsilon_n\\}_{n\\geq 1}$ are independent and identically distributed Gaussian random variables with $\\epsilon_n\\sim\\mathcal{N}(0,\\sigma_{\\epsilon}^{2})$. The chain is initiated in its stationary distribution. Let $f:\\mathbb{R}\\to\\mathbb{R}$ be the identity function $f(x)=x$. Denote by $\\pi$ the stationary distribution of $\\{X_n\\}$, by $\\bar{f}_n=n^{-1}\\sum_{k=1}^{n}f(X_k)$ the empirical average, and by $\\rho_k$ the lag-$k$ normalized autocorrelation of $\\{f(X_n)\\}$ under stationarity. Define the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ by\n$$\n\\tau_{\\mathrm{int}} \\equiv 1 + 2\\sum_{k=1}^{\\infty} \\rho_k,\n$$\nwhenever the series converges. Using only fundamental definitions of stationarity, covariance, geometric ergodicity, and the Markov chain Central Limit Theorem (CLT), do the following:\n(i) Derive the stationary variance of $\\pi$, the autocovariance function of $\\{X_n\\}$, and the normalized autocorrelation $\\rho_k$ for $k\\geq 0$.\n(ii) Prove that the Markov chain CLT holds for $\\sqrt{n}\\,(\\bar{f}_n-\\pi(f))$ by verifying the appropriate drift and irreducibility conditions, and express the asymptotic variance $\\sigma^2$ in terms of the covariance series.\n(iii) Evaluate the series to obtain closed-form expressions for $\\sigma^2$ and $\\tau_{\\mathrm{int}}$ as functions of $\\phi$ and $\\sigma_{\\epsilon}^{2}$.\n\nProvide your final answer for $\\sigma^2$ and $\\tau_{\\mathrm{int}}$ as exact symbolic expressions. No rounding is required, and there are no physical units to report. Express the final answer as a single row matrix containing the two expressions.", "solution": "The problem is well-posed and scientifically sound. It describes a standard first-order autoregressive process, AR(1), and asks for a derivation of its key statistical properties and the parameters of the associated Markov chain Central Limit Theorem (CLT). We proceed with the solution in three parts as requested.\n\n(i) Stationary Variance, Autocovariance, and Autocorrelation\n\nThe AR(1) process is defined by the recurrence relation $X_{n+1} = \\phi X_n + \\epsilon_{n+1}$, where $\\{\\epsilon_n\\}$ are i.i.d. $\\mathcal{N}(0, \\sigma_{\\epsilon}^2)$ and $|\\phi|1$. The chain is assumed to be in its stationary distribution $\\pi$. Let $X_n \\sim \\pi$. By stationarity, the statistical properties of $X_n$ do not depend on the time index $n$.\n\nFirst, we determine the mean of the stationary distribution, $\\mu = E[X_n]$. Taking the expectation of the defining relation, we have:\n$$E[X_{n+1}] = E[\\phi X_n + \\epsilon_{n+1}]$$\nBy linearity of expectation and stationarity ($E[X_{n+1}]=E[X_n]=\\mu$):\n$$\\mu = \\phi E[X_n] + E[\\epsilon_{n+1}]$$\nSince $\\epsilon_{n+1}$ has mean $0$, this simplifies to $\\mu = \\phi \\mu$. This can be written as $(1-\\phi)\\mu = 0$. Given the constraint $|\\phi|1$, the term $1-\\phi$ is non-zero, which implies that the stationary mean must be $\\mu=0$.\n\nNext, we derive the variance of the stationary distribution, $\\sigma_X^2 = \\mathrm{Var}(X_n)$. By stationarity, $\\mathrm{Var}(X_{n+1}) = \\mathrm{Var}(X_n) = \\sigma_X^2$.\n$$\\mathrm{Var}(X_{n+1}) = \\mathrm{Var}(\\phi X_n + \\epsilon_{n+1})$$\nThe term $X_n$ is a function of the noise terms $\\{\\epsilon_k\\}_{k \\le n}$. Since the noise terms are independent, $X_n$ is independent of $\\epsilon_{n+1}$. The variance of a sum of independent random variables is the sum of their variances:\n$$\\mathrm{Var}(X_{n+1}) = \\mathrm{Var}(\\phi X_n) + \\mathrm{Var}(\\epsilon_{n+1})$$\nUsing the property $\\mathrm{Var}(aY) = a^2 \\mathrm{Var}(Y)$:\n$$\\sigma_X^2 = \\phi^2 \\mathrm{Var}(X_n) + \\sigma_{\\epsilon}^2$$\n$$\\sigma_X^2 = \\phi^2 \\sigma_X^2 + \\sigma_{\\epsilon}^2$$\nSolving for $\\sigma_X^2$:\n$$\\sigma_X^2 (1-\\phi^2) = \\sigma_{\\epsilon}^2$$\n$$\\sigma_X^2 = \\frac{\\sigma_{\\epsilon}^2}{1-\\phi^2}$$\nThis is the variance of the stationary distribution $\\pi$. Since $|\\phi|1$, $1-\\phi^20$, so the variance is positive and finite.\n\nNow we compute the autocovariance function, $\\gamma_k = \\mathrm{Cov}(X_n, X_{n+k})$ for $k \\geq 0$. Since the mean is $0$, this is $\\gamma_k = E[X_n X_{n+k}]$. The test function is $f(x)=x$, so the autocovariance of $\\{f(X_n)\\}$ is identical to that of $\\{X_n\\}$.\nFor lag $k=0$, the autocovariance is the variance:\n$$\\gamma_0 = \\mathrm{Var}(X_n) = \\sigma_X^2 = \\frac{\\sigma_{\\epsilon}^2}{1-\\phi^2}$$\nFor lag $k0$, we use the process definition recursively:\n$$X_{n+k} = \\phi X_{n+k-1} + \\epsilon_{n+k}$$\n$$X_{n+k} = \\phi(\\phi X_{n+k-2} + \\epsilon_{n+k-1}) + \\epsilon_{n+k} = \\phi^2 X_{n+k-2} + \\phi\\epsilon_{n+k-1} + \\epsilon_{n+k}$$\nBy iterating this process $k$ times, we can express $X_{n+k}$ in terms of $X_n$ and subsequent noise terms:\n$$X_{n+k} = \\phi^k X_n + \\sum_{j=0}^{k-1} \\phi^j \\epsilon_{n+k-j}$$\nNow we compute $\\gamma_k = E[X_n X_{n+k}]$:\n$$\\gamma_k = E\\left[X_n \\left(\\phi^k X_n + \\sum_{j=0}^{k-1} \\phi^j \\epsilon_{n+k-j}\\right)\\right]$$\n$$\\gamma_k = E[\\phi^k X_n^2] + E\\left[X_n \\sum_{j=0}^{k-1} \\phi^j \\epsilon_{n+k-j}\\right]$$\n$$\\gamma_k = \\phi^k E[X_n^2] + \\sum_{j=0}^{k-1} \\phi^j E[X_n \\epsilon_{n+k-j}]$$\nThe term $E[X_n^2]$ is the stationary variance $\\gamma_0$. The term $X_n$ depends only on noise terms $\\{\\epsilon_i\\}_{i \\le n}$. For any $j \\in \\{0, \\dots, k-1\\}$, the index of the noise term is $n+k-j$, which is greater than $n$ since $kj$. Thus, $X_n$ and $\\epsilon_{n+k-j}$ are independent. This implies $E[X_n \\epsilon_{n+k-j}] = E[X_n]E[\\epsilon_{n+k-j}] = 0 \\cdot 0 = 0$.\nTherefore, the sum vanishes, and we are left with:\n$$\\gamma_k = \\phi^k E[X_n^2] = \\phi^k \\gamma_0$$\nSubstituting the expression for $\\gamma_0$:\n$$\\gamma_k = \\phi^k \\frac{\\sigma_{\\epsilon}^2}{1-\\phi^2} \\quad \\text{for } k \\geq 0$$\nFinally, the normalized autocorrelation $\\rho_k$ is defined as $\\rho_k = \\gamma_k / \\gamma_0$. For $k \\ge 0$:\n$$\\rho_k = \\frac{\\phi^k \\gamma_0}{\\gamma_0} = \\phi^k$$\n\n(ii) Verification of the Markov Chain CLT and Asymptotic Variance\n\nThe Markov chain Central Limit Theorem states that for a suitably well-behaved Markov chain and test function $f$, $\\sqrt{n}(\\bar{f}_n - \\pi(f)) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2)$, where $\\pi(f) = E_{\\pi}[f(X)]$. A sufficient condition for this theorem to hold is that the Markov chain is geometrically ergodic and that $f$ has a finite second moment under the stationary distribution, i.e., $E_{\\pi}[f(X)^2]  \\infty$.\n\nFor the AR(1) process $X_{n+1} = \\phi X_n + \\epsilon_{n+1}$ with $|\\phi|1$ and Gaussian noise, geometric ergodicity is a standard result. It can be established by verifying a drift condition and an irreducibility property.\n- **Irreducibility:** The transition kernel from $x$ to a set $A$ is $P(x, A) = \\int_A \\frac{1}{\\sqrt{2\\pi}\\sigma_\\epsilon} \\exp(-\\frac{(y-\\phi x)^2}{2\\sigma_\\epsilon^2}) dy$. The transition density is positive everywhere, so from any state $x$, there is a positive probability of reaching any open neighborhood in a single step. This implies the chain is $\\psi$-irreducible (with $\\psi$ being the Lebesgue measure) and aperiodic.\n- **Drift Condition:** We can use the quadratic Lyapunov function $V(x) = x^2+1$.\n$$E[V(X_{n+1}) | X_n=x] = E[(\\phi x + \\epsilon_{n+1})^2 + 1] = E[\\phi^2 x^2 + 2\\phi x \\epsilon_{n+1} + \\epsilon_{n+1}^2 + 1]$$\n$$E[V(X_{n+1}) | X_n=x] = \\phi^2 x^2 + \\sigma_{\\epsilon}^2 + 1 = \\phi^2 (x^2+1) + 1 - \\phi^2 + \\sigma_\\epsilon^2 = \\phi^2 V(x) + (1-\\phi^2+\\sigma_\\epsilon^2)$$\nThis is of the form $E[V(X_1)|X_0=x] \\le \\lambda V(x) + L$ with $\\lambda = \\phi^2  1$ and $L=1-\\phi^2+\\sigma_\\epsilon^2  \\infty$. This drift condition, along with irreducibility, guarantees geometric ergodicity.\n\nThe test function is $f(x)=x$. We check the finite second moment condition:\n$$E_{\\pi}[f(X)^2] = E_{\\pi}[X^2] = \\mathrm{Var}_{\\pi}(X) + (E_{\\pi}[X])^2 = \\gamma_0 + 0^2 = \\frac{\\sigma_{\\epsilon}^2}{1-\\phi^2}$$\nSince $|\\phi|1$, this is finite. The conditions for the CLT are satisfied.\n\nThe asymptotic variance $\\sigma^2$ is given by the sum of autocovariances:\n$$\\sigma^2 = \\sum_{k=-\\infty}^{\\infty} \\mathrm{Cov}_{\\pi}(f(X_0), f(X_k)) = \\sum_{k=-\\infty}^{\\infty} \\gamma_k$$\nBy stationarity, $\\gamma_k = \\gamma_{-k}$, so we can write this as:\n$$\\sigma^2 = \\gamma_0 + 2\\sum_{k=1}^{\\infty} \\gamma_k$$\n\n(iii) Closed-Form Expressions for $\\sigma^2$ and $\\tau_{\\mathrm{int}}$\n\nWe now evaluate the series for $\\sigma^2$ using the results from part (i).\n$$\\sigma^2 = \\gamma_0 + 2\\sum_{k=1}^{\\infty} (\\phi^k \\gamma_0) = \\gamma_0 \\left(1 + 2\\sum_{k=1}^{\\infty} \\phi^k\\right)$$\nThe sum is a geometric series. Since $|\\phi|1$, it converges: $\\sum_{k=1}^{\\infty} \\phi^k = \\frac{\\phi}{1-\\phi}$.\nSubstituting this into the expression for $\\sigma^2$:\n$$\\sigma^2 = \\gamma_0 \\left(1 + 2 \\frac{\\phi}{1-\\phi}\\right) = \\gamma_0 \\left(\\frac{1-\\phi+2\\phi}{1-\\phi}\\right) = \\gamma_0 \\frac{1+\\phi}{1-\\phi}$$\nNow, we substitute $\\gamma_0 = \\frac{\\sigma_{\\epsilon}^2}{1-\\phi^2} = \\frac{\\sigma_{\\epsilon}^2}{(1-\\phi)(1+\\phi)}$:\n$$\\sigma^2 = \\left(\\frac{\\sigma_{\\epsilon}^2}{(1-\\phi)(1+\\phi)}\\right) \\left(\\frac{1+\\phi}{1-\\phi}\\right) = \\frac{\\sigma_{\\epsilon}^2}{(1-\\phi)^2}$$\n\nThe integrated autocorrelation time $\\tau_{\\mathrm{int}}$ is defined as:\n$$\\tau_{\\mathrm{int}} = 1 + 2\\sum_{k=1}^{\\infty} \\rho_k$$\nUsing the result from part (i), $\\rho_k = \\phi^k$ for $k \\ge 1$:\n$$\\tau_{\\mathrm{int}} = 1 + 2\\sum_{k=1}^{\\infty} \\phi^k$$\nAgain, using the sum of the geometric series:\n$$\\tau_{\\mathrm{int}} = 1 + 2\\left(\\frac{\\phi}{1-\\phi}\\right) = \\frac{1-\\phi+2\\phi}{1-\\phi} = \\frac{1+\\phi}{1-\\phi}$$\nThe asymptotic variance can be expressed as $\\sigma^2 = \\gamma_0 \\tau_{\\mathrm{int}}$, which is consistent with our findings.\nThe final expressions are $\\sigma^2 = \\frac{\\sigma_{\\epsilon}^2}{(1-\\phi)^2}$ and $\\tau_{\\mathrm{int}} = \\frac{1+\\phi}{1-\\phi}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sigma_{\\epsilon}^2}{(1-\\phi)^2}  \\frac{1+\\phi}{1-\\phi}\n\\end{pmatrix}\n}\n$$", "id": "3319470"}, {"introduction": "A key requirement for the standard MCCLT is aperiodicity, a condition that simple Markov chains can sometimes violate. This hands-on problem [@problem_id:3319528] tackles this issue directly by showing how to transform a periodic two-state chain into an aperiodic one by introducing \"laziness\". Your task is to prove that the CLT is restored and to derive the asymptotic variance as a function of the laziness parameter, revealing how this simple modification impacts estimator uncertainty.", "problem": "Consider the two-state Markov chain with state space $\\{0,1\\}$ and transition kernel $P$ given by $P(0,1)=1$, $P(1,0)=1$, and $P(0,0)=P(1,1)=0$. This chain is irreducible and has period $2$. Define its $\\alpha$-lazy version by $P_{\\alpha}=\\alpha I+(1-\\alpha)P$ for a parameter $\\alpha \\in (0,1)$, where $I$ is the identity kernel. Let $\\pi$ denote the stationary distribution of $P_{\\alpha}$ and consider the function $h:\\{0,1\\}\\to\\mathbb{R}$ defined by $h(x)=\\mathbf{1}\\{x=1\\}$, and its centered version $g=h-\\pi(h)$.\n\nUsing only fundamental definitions of irreducibility, periodicity, aperiodicity, reversibility, and the standard form of the Markov chain Central Limit Theorem (MCCLT), proceed as follows:\n\n- Establish that for any $\\alpha \\in (0,1)$, the chain with kernel $P_{\\alpha}$ is irreducible and aperiodic.\n- Show that $P_{\\alpha}$ is reversible with respect to $\\pi$ and that $\\pi$ is the uniform distribution on $\\{0,1\\}$.\n- Argue carefully, from first principles appropriate to Markov chains used in stochastic simulation and Monte Carlo methods, that the Markov chain Central Limit Theorem (MCCLT) holds for the stationary, centered process $\\{g(X_n)\\}$ under $P_{\\alpha}$.\n- Derive a closed-form expression for the asymptotic variance $\\sigma^2(\\alpha)$ in the MCCLT for the empirical mean $\\frac{1}{n}\\sum_{k=1}^{n} h(X_k)$, that is, the variance parameter appearing in\n$$\n\\sqrt{n}\\left(\\frac{1}{n}\\sum_{k=1}^{n} h(X_k) - \\pi(h)\\right)\\ \\xrightarrow{d}\\ \\mathcal{N}(0,\\ \\sigma^2(\\alpha)).\n$$\n\nYour final answer must be the single, closed-form analytic expression for $\\sigma^2(\\alpha)$ as a function of $\\alpha$, simplified as far as possible. No numerical approximation is required and no rounding should be performed. Provide the expression without units.", "solution": "The problem statement is evaluated to be valid as it is scientifically grounded in the theory of Markov chains, well-posed, objective, and self-contained. It contains no contradictions or ambiguities. We may proceed with the solution.\n\nThe problem asks for a multi-part derivation concerning a two-state Markov chain. We will address each part in sequence, leading to the final expression for the asymptotic variance $\\sigma^2(\\alpha)$.\n\nThe state space is $S=\\{0,1\\}$. The initial transition kernel is given by $P(0,1)=1$, $P(1,0)=1$, and $P(0,0)=P(1,1)=0$. In matrix form, this is $P = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}$. The $\\alpha$-lazy version of the chain has the transition kernel $P_{\\alpha} = \\alpha I + (1-\\alpha) P$ for $\\alpha \\in (0,1)$, where $I$ is the identity matrix. The matrix representation of $P_{\\alpha}$ is:\n$$\nP_{\\alpha} = \\alpha \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + (1-\\alpha) \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix} = \\begin{pmatrix} \\alpha  1-\\alpha \\\\ 1-\\alpha  \\alpha \\end{pmatrix}\n$$\n\n**1. Irreducibility and Aperiodicity of $P_{\\alpha}$**\n\n*   **Irreducibility:** A Markov chain is irreducible if it is possible to go from any state to any other state. For our chain on $S=\\{0,1\\}$:\n    *   The probability of transitioning from state $0$ to state $1$ is $P_{\\alpha}(0,1) = 1-\\alpha$. Since $\\alpha \\in (0,1)$, we have $1-\\alpha  0$.\n    *   The probability of transitioning from state $1$ to state $0$ is $P_{\\alpha}(1,0) = 1-\\alpha  0$.\n    Since any state can be reached from any other state (possibly in one step), the chain is irreducible.\n\n*   **Aperiodicity:** For an irreducible chain, the period is the same for all states. The period of a state $i$ is defined as $d(i) = \\text{gcd}\\{n \\geq 1 : P_{\\alpha}^n(i,i)  0\\}$. A chain is aperiodic if this period is $1$.\n    *   Let's consider state $0$. The probability of returning to state $0$ in one step is $P_{\\alpha}(0,0) = \\alpha$.\n    *   Since $\\alpha \\in (0,1)$, we have $\\alpha0$. Thus, $P_{\\alpha}^1(0,0)  0$.\n    *   The set of possible return times $n$ for state $0$ includes $n=1$. The greatest common divisor (gcd) of any set of integers containing $1$ must be $1$.\n    *   Therefore, the period of state $0$ is $d(0)=1$. Since the chain is irreducible, all states have period $1$, and the chain is aperiodic.\n\n**2. Reversibility and the Stationary Distribution $\\pi$**\n\n*   **Stationary Distribution:** A probability distribution $\\pi$ on $S$ is stationary for $P_{\\alpha}$ if $\\pi P_{\\alpha} = \\pi$. Let $\\pi = (\\pi(0), \\pi(1))$. The condition requires:\n    $$\n    (\\pi(0), \\pi(1)) \\begin{pmatrix} \\alpha  1-\\alpha \\\\ 1-\\alpha  \\alpha \\end{pmatrix} = (\\pi(0), \\pi(1))\n    $$\n    This yields the system of equations:\n    $\\alpha \\pi(0) + (1-\\alpha) \\pi(1) = \\pi(0)$\n    $(1-\\alpha) \\pi(0) + \\alpha \\pi(1) = \\pi(1)$\n    From the first equation, we get $(1-\\alpha) \\pi(1) = (1-\\alpha) \\pi(0)$. Since $\\alpha \\in (0,1)$, $1-\\alpha \\neq 0$, which implies $\\pi(1) = \\pi(0)$.\n    Using the normalization condition $\\pi(0)+\\pi(1)=1$, we have $2\\pi(0)=1$, so $\\pi(0)=1/2$. Consequently, $\\pi(1)=1/2$.\n    The unique stationary distribution is the uniform distribution on $\\{0,1\\}$, i.e., $\\pi(x) = 1/2$ for $x \\in \\{0,1\\}$.\n\n*   **Reversibility:** A chain with kernel $P$ is reversible with respect to a distribution $\\pi$ if it satisfies the detailed balance condition: $\\pi(i) P(i,j) = \\pi(j) P(j,i)$ for all $i,j \\in S$.\n    *   We test this for $P_{\\alpha}$ and the uniform distribution $\\pi$. For $i=0, j=1$:\n        $\\pi(0) P_{\\alpha}(0,1) = (1/2)(1-\\alpha)$\n        $\\pi(1) P_{\\alpha}(1,0) = (1/2)(1-\\alpha)$\n        The condition holds. The other cases ($i=1,j=0$ and $i=j$) are trivially satisfied due to symmetry.\n    *   Thus, the chain with kernel $P_{\\alpha}$ is reversible with respect to the uniform distribution $\\pi$.\n\n**3. Validity of the Markov Chain Central Limit Theorem (MCCLT)**\n\nThe MCCLT states that for a suitably \"well-behaved\" Markov chain $(X_n)$ started in its stationary distribution $\\pi$, and for a function $g$ with $\\pi(g)=0$, the normalized sum of observations converges in distribution to a normal distribution. For a finite-state Markov chain, the conditions for the MCCLT to hold for any function $g$ with finite variance are irreducibility and aperiodicity. We have established both for the chain with kernel $P_{\\alpha}$ for any $\\alpha \\in (0,1)$. Reversibility is a stronger condition which also guarantees the theorem holds and simplifies the structure of the asymptotic variance. The process $\\{g(X_n)\\}$ is a stationary, centered process. All necessary conditions are met, so the MCCLT applies.\n\n**4. Derivation of the Asymptotic Variance $\\sigma^2(\\alpha)$**\n\nThe problem concerns the empirical mean of the function $h(x) = \\mathbf{1}\\{x=1\\}$. The asymptotic variance $\\sigma^2(\\alpha)$ in the MCCLT for $\\frac{1}{n}\\sum h(X_k)$ is identical to that for the centered function $g = h - \\pi(h)$.\nFirst, we compute $\\pi(h)$:\n$$\n\\pi(h) = \\sum_{x \\in \\{0,1\\}} \\pi(x) h(x) = \\pi(0)h(0) + \\pi(1)h(1) = (1/2)(0) + (1/2)(1) = 1/2.\n$$\nThe centered function is $g(x) = h(x) - 1/2$. The values of $g$ are:\n$g(0) = 0 - 1/2 = -1/2$\n$g(1) = 1 - 1/2 = 1/2$\n\nThe asymptotic variance is given by the formula:\n$$\n\\sigma^2(\\alpha) = \\text{Var}_{\\pi}(g(X_0)) + 2 \\sum_{k=1}^{\\infty} \\text{Cov}_{\\pi}(g(X_0), g(X_k))\n$$\nAssuming the chain starts in stationarity ($X_0 \\sim \\pi$), we have $\\text{Var}_{\\pi}(g(X_0)) = \\mathbb{E}_{\\pi}[g(X_0)^2] - (\\mathbb{E}_{\\pi}[g(X_0)])^2$. Since $g$ is centered, $\\mathbb{E}_{\\pi}[g(X_0)] = \\pi(g) = 0$. So, $\\text{Var}_{\\pi}(g(X_0)) = \\pi(g^2)$.\n$$\n\\text{Var}_{\\pi}(g(X_0)) = \\pi(g^2) = \\sum_{x \\in \\{0,1\\}} \\pi(x) g(x)^2 = \\frac{1}{2}\\left(-\\frac{1}{2}\\right)^2 + \\frac{1}{2}\\left(\\frac{1}{2}\\right)^2 = \\frac{1}{2} \\cdot \\frac{1}{4} + \\frac{1}{2} \\cdot \\frac{1}{4} = \\frac{1}{4}.\n$$\nThe covariance term is $\\text{Cov}_{\\pi}(g(X_0), g(X_k)) = \\mathbb{E}_{\\pi}[g(X_0)g(X_k)]$. We can compute this by considering the action of the transition operator $P_{\\alpha}$ on the function $g$. Let $(P_{\\alpha}g)(i) = \\sum_{j \\in S} P_{\\alpha}(i,j) g(j)$.\nFor $i=0$:\n$$\n(P_{\\alpha}g)(0) = P_{\\alpha}(0,0)g(0) + P_{\\alpha}(0,1)g(1) = \\alpha(-1/2) + (1-\\alpha)(1/2) = \\frac{1-\\alpha-\\alpha}{2} = \\frac{1-2\\alpha}{2} = (2\\alpha-1)(-\\frac{1}{2}) = (2\\alpha-1)g(0).\n$$\nFor $i=1$:\n$$\n(P_{\\alpha}g)(1) = P_{\\alpha}(1,0)g(0) + P_{\\alpha}(1,1)g(1) = (1-\\alpha)(-1/2) + \\alpha(1/2) = \\frac{\\alpha-(1-\\alpha)}{2} = \\frac{2\\alpha-1}{2} = (2\\alpha-1)(\\frac{1}{2}) = (2\\alpha-1)g(1).\n$$\nThis shows that $g$ is an eigenfunction of the operator $P_{\\alpha}$ with eigenvalue $\\lambda = 2\\alpha-1$.\nBy induction, $(P_{\\alpha}^k g)(x) = (2\\alpha-1)^k g(x)$.\n\nThe covariance can now be calculated elegantly. Using inner product notation $\\langle f,g \\rangle_{\\pi} = \\sum_x \\pi(x)f(x)g(x)$:\n$$\n\\text{Cov}_{\\pi}(g(X_0), g(X_k)) = \\mathbb{E}_{\\pi}[g(X_0)g(X_k)] = \\langle g, P_{\\alpha}^k g \\rangle_{\\pi}.\n$$\nSince $P_{\\alpha}^k g = (2\\alpha-1)^k g$:\n$$\n\\text{Cov}_{\\pi}(g(X_0), g(X_k)) = \\langle g, (2\\alpha-1)^k g \\rangle_{\\pi} = (2\\alpha-1)^k \\langle g, g \\rangle_{\\pi} = (2\\alpha-1)^k \\pi(g^2).\n$$\nWe already found $\\pi(g^2) = 1/4$. So,\n$$\n\\text{Cov}_{\\pi}(g(X_0), g(X_k)) = \\frac{1}{4}(2\\alpha-1)^k.\n$$\nNow we can compute $\\sigma^2(\\alpha)$:\n$$\n\\sigma^2(\\alpha) = \\frac{1}{4} + 2 \\sum_{k=1}^{\\infty} \\frac{1}{4}(2\\alpha-1)^k = \\frac{1}{4} \\left( 1 + 2 \\sum_{k=1}^{\\infty} (2\\alpha-1)^k \\right).\n$$\nThe sum is a geometric series with ratio $r = 2\\alpha-1$. Since $\\alpha \\in(0,1)$, we have $r \\in (-1,1)$, so the series converges to $\\frac{r}{1-r}$.\n$$\n\\sum_{k=1}^{\\infty} (2\\alpha-1)^k = \\frac{2\\alpha-1}{1 - (2\\alpha-1)} = \\frac{2\\alpha-1}{2 - 2\\alpha}.\n$$\nSubstituting this into the expression for $\\sigma^2(\\alpha)$:\n$$\n\\sigma^2(\\alpha) = \\frac{1}{4} \\left( 1 + 2 \\cdot \\frac{2\\alpha-1}{2(1-\\alpha)} \\right) = \\frac{1}{4} \\left( 1 + \\frac{2\\alpha-1}{1-\\alpha} \\right).\n$$\nSimplifying the term in the parenthesis:\n$$\n\\sigma^2(\\alpha) = \\frac{1}{4} \\left( \\frac{1-\\alpha + 2\\alpha-1}{1-\\alpha} \\right) = \\frac{1}{4} \\left( \\frac{\\alpha}{1-\\alpha} \\right).\n$$\nThis gives the final closed-form expression for the asymptotic variance.\n$$\n\\sigma^2(\\alpha) = \\frac{\\alpha}{4(1-\\alpha)}.\n$$", "answer": "$$\\boxed{\\frac{\\alpha}{4(1-\\alpha)}}$$", "id": "3319528"}, {"introduction": "Spectral methods provide a powerful lens for analyzing the convergence of Markov chains, connecting the chain's asymptotic variance directly to the eigenvalues of its transition operator. This exercise focuses on a general two-state reversible Markov chain, a fundamental building block in the study of stochastic processes. By applying spectral decomposition techniques, you will derive a closed-form expression for the asymptotic variance, illustrating the deep relationship between the chain's spectral gap and its statistical efficiency.", "problem": "Consider an irreducible, aperiodic, two-state, time-homogeneous, reversible Markov chain with state space $\\{1,2\\}$ and transition matrix\n$$\nQ \\;=\\; \\begin{pmatrix}\n1-\\alpha  \\alpha\\\\\n\\beta  1-\\beta\n\\end{pmatrix},\n$$\nwhere $\\alpha\\in(0,1)$ and $\\beta\\in(0,1)$. Let $\\pi$ be its unique stationary distribution, and let $f:\\{1,2\\}\\to\\mathbb{R}$ be defined by $f(1)=a$ and $f(2)=b$, with the zero-mean constraint $\\pi(f)=0$. The Markov chain Central Limit Theorem (CLT) states that, under standard regularity conditions, the normalized ergodic average of $f$ converges in distribution to a normal law with variance parameter $\\sigma^{2}$, known as the asymptotic variance.\n\nUsing only fundamental definitions and the spectral decomposition of the Markov transition operator on $L^{2}(\\pi)$, derive the exact closed-form expression for the asymptotic variance $\\sigma^{2}$ in terms of $\\alpha$, $\\beta$, $a$, and $b$.\n\nYour final answer must be a single closed-form analytic expression. Do not provide an inequality or an equation to be solved. No rounding is required.", "solution": "The problem statement is critically validated and found to be self-contained, scientifically grounded, and well-posed. The setup describes a standard two-state, time-homogeneous Markov chain. The conditions of irreducibility and aperiodicity are met by an appropriate choice of parameters $\\alpha, \\beta \\in (0,1)$. The reversibility is a key property that is consistent with the provided structure. The request to derive the asymptotic variance using spectral methods is a standard, non-trivial problem in the theory of Markov chains. Thus, the problem is valid, and we may proceed with the solution.\n\nThe asymptotic variance $\\sigma^2$ of a function $f$ for a Markov chain Central Limit Theorem is given by\n$$\n\\sigma^2 = \\lim_{n\\to\\infty} \\frac{1}{n} \\text{Var}_{\\pi}\\left(\\sum_{k=0}^{n-1} f(X_k)\\right) = \\sum_{k=-\\infty}^{\\infty} \\text{Cov}_{\\pi}(f(X_0), f(X_k))\n$$\nwhere the process $\\{X_k\\}$ is in its stationary distribution $\\pi$. For a reversible Markov chain, the covariance terms are symmetric, $\\text{Cov}_{\\pi}(f(X_0), f(X_k)) = \\text{Cov}_{\\pi}(f(X_0), f(X_{-k}))$, and since $\\pi(f) = 0$, we have\n$$\n\\sigma^2 = \\text{Var}_{\\pi}(f) + 2 \\sum_{k=1}^{\\infty} \\text{Cov}_{\\pi}(f(X_0), f(X_k))\n$$\nThe derivation will proceed in several steps:\n1. Determine the unique stationary distribution $\\pi$.\n2. Analyze the spectral properties of the transition operator $Q$ on the Hilbert space $L^2(\\pi)$.\n3. Use the spectral decomposition to compute $\\sigma^2$.\n\nFirst, we find the stationary distribution $\\pi = (\\pi(1), \\pi(2))$. It is the unique probability vector satisfying $\\pi Q = \\pi$, which translates to the system of equations:\n$$\n\\begin{cases}\n\\pi(1)(1-\\alpha) + \\pi(2)\\beta = \\pi(1) \\\\\n\\pi(1)\\alpha + \\pi(2)(1-\\beta) = \\pi(2) \\\\\n\\pi(1) + \\pi(2) = 1\n\\end{cases}\n$$\nThe first two equations both simplify to $\\pi(1)\\alpha = \\pi(2)\\beta$. This is precisely the detailed balance condition, $\\pi(i)Q(i,j) = \\pi(j)Q(j,i)$ for $i=1, j=2$, which confirms the chain is reversible with respect to $\\pi$. Substituting $\\pi(2) = \\pi(1) \\frac{\\alpha}{\\beta}$ into the normalization condition $\\pi(1)+\\pi(2)=1$ yields:\n$$\n\\pi(1)\\left(1 + \\frac{\\alpha}{\\beta}\\right) = 1 \\implies \\pi(1)\\frac{\\alpha+\\beta}{\\beta} = 1 \\implies \\pi(1) = \\frac{\\beta}{\\alpha+\\beta}\n$$\nConsequently, $\\pi(2) = 1 - \\pi(1) = 1 - \\frac{\\beta}{\\alpha+\\beta} = \\frac{\\alpha}{\\alpha+\\beta}$. So, the stationary distribution is $\\pi = \\left(\\frac{\\beta}{\\alpha+\\beta}, \\frac{\\alpha}{\\alpha+\\beta}\\right)$.\n\nNext, we use the zero-mean constraint $\\pi(f)=0$. This is an expectation with respect to the stationary distribution:\n$$\n\\pi(f) = \\sum_{i=1}^{2} \\pi(i) f(i) = \\pi(1)f(1) + \\pi(2)f(2) = 0\n$$\nSubstituting the expressions for $\\pi(i)$ and $f(i)$:\n$$\n\\frac{\\beta}{\\alpha+\\beta} a + \\frac{\\alpha}{\\alpha+\\beta} b = 0 \\implies \\beta a + \\alpha b = 0\n$$\nThis equation establishes a necessary relationship between $a$ and $b$.\n\nThe problem requires using the spectral decomposition of the Markov transition operator $Q$ on the Hilbert space $L^2(\\pi)$ of functions on $\\{1,2\\}$, endowed with the inner product $\\langle g, h \\rangle_{\\pi} = \\sum_{i=1}^{2} \\pi(i) g(i) h(i)$. For a reversible chain, $Q$ is self-adjoint in $L^2(\\pi)$. The asymptotic variance for a zero-mean function $f$ can be expressed in terms of the operator $Q$ as:\n$$\n\\sigma^2 = \\langle f, (I + Q)(I-Q)^{-1}f \\rangle_{\\pi}\n$$\nwhere $(I-Q)^{-1}$ is the potential operator, well-defined on the subspace of zero-mean functions.\n\nTo apply this formula, we find the eigenvalues and eigenfunctions of $Q$. The eigenvalues $\\lambda$ of the matrix $Q$ are solutions to $\\det(Q-\\lambda I)=0$:\n$$\n\\det \\begin{pmatrix} 1-\\alpha-\\lambda  \\alpha \\\\ \\beta  1-\\beta-\\lambda \\end{pmatrix} = (1-\\alpha-\\lambda)(1-\\beta-\\lambda) - \\alpha\\beta = 0\n$$\n$$\n\\lambda^2 - (2-\\alpha-\\beta)\\lambda + (1-\\alpha-\\beta) = 0\n$$\nOne eigenvalue is always $\\lambda_1=1$, which corresponds to the stationary distribution. The sum of eigenvalues is the trace of $Q$, so $\\lambda_1 + \\lambda_2 = 1+\\lambda_2 = \\text{Tr}(Q) = 2-\\alpha-\\beta$, which gives the second eigenvalue $\\lambda_2 = 1-\\alpha-\\beta$.\n\nThe eigenfunction $\\psi_1$ corresponding to $\\lambda_1=1$ is a constant function; we can take $\\psi_1(i)=1$ for $i=1,2. $This eigenfunction spans the space of constant functions.\nThe eigenfunction $\\psi_2$ corresponding to $\\lambda_2 = 1-\\alpha-\\beta$ is a vector in the kernel of $Q - \\lambda_2 I$:\n$$\n(Q-(1-\\alpha-\\beta)I)v = \\begin{pmatrix} \\beta  \\alpha \\\\ \\beta  \\alpha \\end{pmatrix} v = 0\n$$\nThis gives $\\beta v_1 + \\alpha v_2 = 0$. We can choose the eigenfunction $\\psi_2$ to be represented by the vector $(\\alpha, -\\beta)^T$, so $\\psi_2(1)=\\alpha$ and $\\psi_2(2)=-\\beta$. The two eigenfunctions $\\psi_1$ and $\\psi_2$ form an orthogonal basis of $L^2(\\pi)$:\n$$\n\\langle \\psi_1, \\psi_2 \\rangle_{\\pi} = \\pi(1)\\psi_1(1)\\psi_2(1) + \\pi(2)\\psi_1(2)\\psi_2(2) = \\frac{\\beta}{\\alpha+\\beta}(1)(\\alpha) + \\frac{\\alpha}{\\alpha+\\beta}(1)(-\\beta) = \\frac{\\alpha\\beta-\\alpha\\beta}{\\alpha+\\beta} = 0\n$$\nAny function $f$ in $L^2(\\pi)$ can be written as $f=c_1\\psi_1+c_2\\psi_2$. The mean of $f$ is $\\pi(f) = \\langle f, \\psi_1 \\rangle_{\\pi}$. The condition $\\pi(f)=0$ implies that $f$ is orthogonal to the constant function $\\psi_1$, so $c_1=0$. Thus, $f$ must be a multiple of $\\psi_2$, i.e., $f=c_2\\psi_2$. This means $f$ is itself an eigenfunction of $Q$ corresponding to the eigenvalue $\\lambda_2$.\n$$f(1) = a = c_2\\psi_2(1) = c_2\\alpha$$\n$$f(2) = b = c_2\\psi_2(2) = -c_2\\beta$$\nThis is consistent with the constraint $\\beta a + \\alpha b = 0$.\n\nSince $f$ is an eigenfunction of $Q$ with eigenvalue $\\lambda_2$, we have $Qf = \\lambda_2 f$. Applying the operator $(I+Q)(I-Q)^{-1}$ to $f$ yields:\n$$\n(I+Q)(I-Q)^{-1}f = (I+Q)\\left(\\frac{1}{1-\\lambda_2}f\\right) = \\frac{1}{1-\\lambda_2}(f+Qf) = \\frac{1}{1-\\lambda_2}(f+\\lambda_2 f) = \\frac{1+\\lambda_2}{1-\\lambda_2}f\n$$\nNow, we substitute this into the formula for $\\sigma^2$:\n$$\n\\sigma^2 = \\left\\langle f, \\frac{1+\\lambda_2}{1-\\lambda_2}f \\right\\rangle_{\\pi} = \\frac{1+\\lambda_2}{1-\\lambda_2} \\langle f, f \\rangle_{\\pi}\n$$\nThe term $\\langle f, f \\rangle_{\\pi}$ is the variance of $f$, $\\text{Var}_{\\pi}(f)$, since its mean is zero.\n$$\n\\text{Var}_{\\pi}(f) = \\langle f, f \\rangle_{\\pi} = \\pi(f^2) = \\pi(1)f(1)^2 + \\pi(2)f(2)^2 = \\frac{\\beta}{\\alpha+\\beta}a^2 + \\frac{\\alpha}{\\alpha+\\beta}b^2\n$$\nWe can express this in a more symmetric form using the constraint $\\beta a + \\alpha b = 0$. From this, $a = \\frac{\\alpha}{\\alpha+\\beta}(a-b)$ and $b = -\\frac{\\beta}{\\alpha+\\beta}(a-b)$. Substituting these into the variance expression:\n$$\n\\text{Var}_{\\pi}(f) = \\frac{\\beta}{\\alpha+\\beta}\\left(\\frac{\\alpha(a-b)}{\\alpha+\\beta}\\right)^2 + \\frac{\\alpha}{\\alpha+\\beta}\\left(\\frac{-\\beta(a-b)}{\\alpha+\\beta}\\right)^2\n$$\n$$\n= \\frac{\\beta\\alpha^2(a-b)^2}{(\\alpha+\\beta)^3} + \\frac{\\alpha\\beta^2(a-b)^2}{(\\alpha+\\beta)^3} = \\frac{\\alpha\\beta(a-b)^2(\\alpha+\\beta)}{(\\alpha+\\beta)^3} = \\frac{\\alpha\\beta(a-b)^2}{(\\alpha+\\beta)^2}\n$$\nThe other factor in the expression for $\\sigma^2$ is:\n$$\n\\frac{1+\\lambda_2}{1-\\lambda_2} = \\frac{1+(1-\\alpha-\\beta)}{1-(1-\\alpha-\\beta)} = \\frac{2-\\alpha-\\beta}{\\alpha+\\beta}\n$$\nFinally, multiplying the two factors gives the asymptotic variance:\n$$\n\\sigma^2 = \\left(\\frac{2-\\alpha-\\beta}{\\alpha+\\beta}\\right) \\left(\\frac{\\alpha\\beta(a-b)^2}{(\\alpha+\\beta)^2}\\right) = \\frac{\\alpha\\beta(a-b)^2(2-\\alpha-\\beta)}{(\\alpha+\\beta)^3}\n$$\nThis is the closed-form expression for the asymptotic variance in terms of the given parameters.", "answer": "$$\\boxed{\\frac{\\alpha\\beta(a-b)^2(2-\\alpha-\\beta)}{(\\alpha+\\beta)^3}}$$", "id": "3319504"}]}