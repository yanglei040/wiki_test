{"hands_on_practices": [{"introduction": "This practice provides a direct, computational experience with Markov chain convergence by implementing the celebrated PageRank algorithm. You will model a web graph as a Markov chain and use power iteration to find its stationary distribution, which represents the importance of each page. By simulating this process [@problem_id:3300074], you will gain practical insight into how the convergence rate is concretely affected by the algorithm's parameters and the underlying graph structure.", "problem": "Consider the Google PageRank formulation as the stationary distribution of a Markov chain on a finite directed graph with teleportation. Let $n \\in \\mathbb{N}$ denote the number of nodes, let $W \\in \\{0,1\\}^{n \\times n}$ be the adjacency matrix defined by columns as follows: $W_{ij} = 1$ if there is a directed edge from node $j$ to node $i$, and $W_{ij} = 0$ otherwise. Define the teleportation distribution $v \\in \\mathbb{R}^n$ to be the uniform probability vector, $v_i = 1/n$ for all $i \\in \\{1,\\dots,n\\}$. Define the column-stochastic matrix $S \\in \\mathbb{R}^{n \\times n}$ by normalizing each column of $W$; if a column $j$ has zero sum (a dangling node), set $S_{\\cdot j} = v$. For a teleportation probability $\\alpha \\in [0,1)$, consider the PageRank iteration\n$$\nx_{k+1} \\;=\\; \\alpha S x_k \\;+\\; (1 - \\alpha)\\, v,\\quad k = 0,1,2,\\dots,\n$$\nwith initial distribution $x_0 = v$. The stationary distribution $x^\\star \\in \\mathbb{R}^n$ is the unique solution to\n$$\nx^\\star \\;=\\; \\alpha S x^\\star \\;+\\; (1-\\alpha)\\, v, \\qquad x^\\star \\ge 0, \\;\\; \\sum_{i=1}^n x^\\star_i = 1,\n$$\nequivalently,\n$$\n(I - \\alpha S)\\, x^\\star \\;=\\; (1-\\alpha)\\, v.\n$$\nUse the total variation distance $d_{\\mathrm{TV}}(p,q) = \\tfrac{1}{2}\\,\\lVert p - q\\rVert_1$ to quantify convergence. The goal is to simulate the convergence to $x^\\star$ by power iteration and to quantify how the convergence speed depends on $\\alpha$ and the presence of dangling nodes.\n\nStarting from the fundamental facts that (i) $S$ is column-stochastic by construction, (ii) $x^\\star$ exists and is unique for any $\\alpha \\in [0,1)$, and (iii) the iteration is linear, design and implement a program that, for each test case below, performs the following tasks:\n1. Construct $S$ from $W$ and $v$ as defined above.\n2. Compute the exact stationary distribution $x^\\star$ by solving $(I - \\alpha S)\\,x^\\star = (1-\\alpha) v$ and normalizing to a probability vector.\n3. Run the power iteration $x_{k+1} = \\alpha S x_k + (1 - \\alpha) v$ from $x_0 = v$ until the stopping criterion $d_{\\mathrm{TV}}(x_k, x^\\star) \\le \\varepsilon$ is met, or until a maximum of $K_{\\max} = 100000$ iterations is reached. Use $\\varepsilon = 10^{-10}$ for all cases.\n4. Record $K$, the number of iterations performed when the stopping criterion is first met (if the initial $x_0$ already satisfies the criterion, then $K=0$). If the stopping criterion is not met within $K_{\\max}$ iterations, set $K=K_{\\max}$.\n5. Estimate an empirical linear convergence rate $r_{\\mathrm{hat}}$ from the sequence of $\\ell_1$-errors $e_k = \\lVert x_k - x^\\star \\rVert_1$ by computing the ratios $e_k/e_{k-1}$ for $k \\ge 1$ over the run and reporting the median of the last $\\min\\{50, \\text{number of ratios}\\}$ ratios. If fewer than two iterates are available, set $r_{\\mathrm{hat}}=0.0$. This $r_{\\mathrm{hat}}$ approximates the asymptotic contraction factor on the zero-sum subspace and reflects sensitivity to $\\alpha$ and dangling nodes.\n\nYou must implement the above for the following test suite of graphs and parameters. Each graph is specified by its adjacency matrix $W$ with the column-based convention described above, and each test case uses $v$ uniform and $\\varepsilon = 10^{-10}$:\n\n- Test case 1 (no dangling nodes, moderate teleportation): $n=4$, $\\alpha=0.85$, \n$$\nW = \\begin{bmatrix}\n0  0  0  1 \\\\\n1  0  0  0 \\\\\n0  1  0  0 \\\\\n0  0  1  0\n\\end{bmatrix}.\n$$\n- Test case 2 (no dangling nodes, slow mixing): $n=4$, $\\alpha=0.99$, same $W$ as in test case 1.\n- Test case 3 (one dangling node, moderate teleportation): $n=3$, $\\alpha=0.85$,\n$$\nW = \\begin{bmatrix}\n0  1  0 \\\\\n1  0  0 \\\\\n0  0  0\n\\end{bmatrix}.\n$$\n- Test case 4 (one dangling node, slow mixing): $n=3$, $\\alpha=0.99$, same $W$ as in test case 3.\n- Test case 5 (reducible graph without dangling nodes, moderate teleportation): $n=4$, $\\alpha=0.90$,\n$$\nW = \\begin{bmatrix}\n0  1  0  0 \\\\\n1  0  0  0 \\\\\n0  0  0  1 \\\\\n0  0  1  0\n\\end{bmatrix}.\n$$\n- Test case 6 (teleportation dominates): $n=4$, $\\alpha=0.00$, same $W$ as in test case 1.\n\nYour program must produce a single line of output containing the results for the six test cases as a comma-separated list enclosed in square brackets. Each test case result must be a two-element list $[K, r_{\\mathrm{hat}}]$ in that order. Thus, the output format must be exactly\n\"[ [K1,rhat1],[K2,rhat2],[K3,rhat3],[K4,rhat4],[K5,rhat5],[K6,rhat6] ]\"\nwith no spaces required beyond those shown and with floating-point numbers expressed in decimal or scientific notation. No physical units or angles are involved, and no percentages appear in the output. All computations are purely numerical and dimensionless.", "solution": "The problem statement has been critically validated and is deemed sound. It is scientifically grounded in the theory of Markov chains and numerical linear algebra, specifically the PageRank algorithm. The problem is well-posed, with all necessary data, definitions, and constraints provided to ensure a unique, stable, and meaningful solution. The objectives are clear and expressed in precise mathematical language. We may therefore proceed with the solution.\n\nThe problem requires the implementation and analysis of the PageRank algorithm for a set of specified graphs and parameters. The solution involves five main tasks for each test case:\n1.  Constructing the Google matrix component $S$.\n2.  Computing the exact stationary PageRank vector $x^\\star$.\n3.  Simulating the convergence to $x^\\star$ using power iteration.\n4.  Determining the number of iterations $K$ required to reach a specified tolerance.\n5.  Estimating the empirical convergence rate $r_{\\mathrm{hat}}$.\n\nWe will now detail the methodology for each of these tasks.\n\n**1. Construction of the Stochastic Matrix $S$**\n\nThe matrix $S \\in \\mathbb{R}^{n \\times n}$ is derived from the graph's adjacency matrix $W \\in \\{0,1\\}^{n \\times n}$. The problem specifies a column-based convention for $W$, where $W_{ij} = 1$ indicates a directed edge from node $j$ to node $i$. $S$ is a column-stochastic matrix constructed as follows:\n\nFor each column $j \\in \\{1, \\dots, n\\}$ of $W$:\n- Let $c_j = \\sum_{i=1}^n W_{ij}$ be the sum of the elements in column $j$, which corresponds to the out-degree of node $j$.\n- If $c_j  0$, the node $j$ has outgoing links. The corresponding column $S_{\\cdot j}$ of $S$ is obtained by normalizing the column $W_{\\cdot j}$:\n$$S_{ij} = \\frac{W_{ij}}{c_j} \\quad \\text{for } i=1, \\dots, n.$$\n- If $c_j = 0$, node $j$ is a \"dangling node\" with no outgoing links. To ensure that the resulting matrix is stochastic (i.e., its columns sum to $1$), the column $S_{\\cdot j}$ is set to the uniform teleportation vector $v \\in \\mathbb{R}^n$, where $v_i = 1/n$ for all $i$.\n$$S_{\\cdot j} = v = \\begin{bmatrix} 1/n  1/n  \\dots  1/n \\end{bmatrix}^T.$$\nBy this construction, every column of $S$ is non-negative and sums to $1$, making $S$ a column-stochastic matrix.\n\n**2. Computation of the Exact Stationary Distribution $x^\\star$**\n\nThe stationary distribution, or PageRank vector, $x^\\star$, is the unique probability vector that is a fixed point of the PageRank iteration. It satisfies the equation:\n$$x^\\star = \\alpha S x^\\star + (1-\\alpha)v$$\nwhere $\\alpha \\in [0,1)$ is the teleportation probability. This equation can be rearranged into a standard linear system:\n$$I x^\\star - \\alpha S x^\\star = (1-\\alpha)v$$\n$$(I - \\alpha S) x^\\star = (1-\\alpha)v$$\nwhere $I$ is the $n \\times n$ identity matrix.\n\nThe existence and uniqueness of the solution $x^\\star$ are guaranteed for $\\alpha \\in [0,1)$. The matrix $S$ is stochastic, so its spectral radius (the maximum modulus of its eigenvalues) is $\\rho(S)=1$. The spectral radius of $\\alpha S$ is $\\rho(\\alpha S) = \\alpha \\rho(S) = \\alpha$. Since $\\alpha  1$, all eigenvalues of $\\alpha S$ are strictly inside the unit circle in the complex plane. This ensures that $1$ is not an eigenvalue of $\\alpha S$, and thus the matrix $(I - \\alpha S)$ is invertible.\n\nThe unique solution $x^\\star$ can be computed by solving this linear system. While the problem statement suggests normalizing the result, the solution to this system is guaranteed to be a probability vector (its components sum to $1$), so normalization is redundant but harmless.\n\n**3. Power Iteration Simulation**\n\nThe PageRank vector $x^\\star$ is computed iteratively using the power method. The iteration is defined as:\n$$x_{k+1} = \\alpha S x_k + (1-\\alpha)v$$\nstarting with an initial distribution $x_0 = v$. The process continues until the iterate $x_k$ is sufficiently close to the true stationary distribution $x^\\star$. Convergence is measured using the total variation distance, $d_{\\mathrm{TV}}(p,q) = \\frac{1}{2} \\lVert p - q \\rVert_1$. The iteration stops when the condition $d_{\\mathrm{TV}}(x_k, x^\\star) \\le \\varepsilon$ is met, for a given tolerance $\\varepsilon = 10^{-10}$. The number of iterations, $K$, required to meet this criterion is recorded. If the criterion is not met within a maximum of $K_{\\max} = 100000$ iterations, $K$ is set to $K_{\\max}$. If the initial vector $x_0$ already satisfies the criterion, $K=0$.\n\n**4. Estimation of Empirical Convergence Rate $r_{\\mathrm{hat}}$**\n\nThe convergence of the power iteration is linear. The error vector at step $k$ is $e_k = x_k - x^\\star$. The error propagates as:\n$$e_{k+1} = x_{k+1} - x^\\star = (\\alpha S x_k + (1-\\alpha)v) - (\\alpha S x^\\star + (1-\\alpha)v) = \\alpha S (x_k - x^\\star) = \\alpha S e_k.$$\nTaking the $\\ell_1$-norm, we get $\\lVert e_{k+1} \\rVert_1 = \\alpha \\lVert S e_k \\rVert_1$. The asymptotic rate of convergence of the $\\ell_1$-error, $\\lVert e_k \\rVert_1$, is determined by the action of the operator $\\alpha S$ on the subspace of zero-sum vectors (since $x_k$ and $x^\\star$ are probability vectors, their difference $e_k$ sums to zero). The theoretical rate is related to $\\alpha$ and the spectral properties of $S$.\n\nThe problem requires an *empirical* estimate of this rate, $r_{\\mathrm{hat}}$. This is calculated from the sequence of $\\ell_1$-errors, $e_k = \\lVert x_k - x^\\star \\rVert_1$, generated during the power iteration. The ratio $e_k/e_{k-1}$ for $k \\ge 1$ approximates the per-step reduction in error. To obtain a stable estimate of the asymptotic rate, we compute the median of the last $\\min\\{50, K\\}$ of these ratios, where $K$ is the total number of iterations performed. If fewer than two iterates are generated (i.e., $K=0$), implying less than one ratio can be computed, $r_{\\mathrm{hat}}$ is set to $0.0$.\n\nThe following program implements this entire procedure for the six specified test cases, adhering strictly to the methodology described.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the PageRank convergence problem for all test cases.\n    \"\"\"\n\n    def solve_one_case(n, alpha, W, epsilon, k_max):\n        \"\"\"\n        Solves a single test case of the PageRank problem.\n\n        Args:\n            n (int): Number of nodes.\n            alpha (float): Teleportation probability.\n            W (np.ndarray): Adjacency matrix (column-based).\n            epsilon (float): Convergence tolerance for total variation distance.\n            k_max (int): Maximum number of iterations.\n\n        Returns:\n            list: A list containing [K, r_hat], the number of iterations and the\n                  empirical convergence rate.\n        \"\"\"\n        # 1. Construct the stochastic matrix S\n        v = np.ones((n, 1)) / n\n        S = np.zeros((n, n), dtype=np.float64)\n        col_sums = np.sum(W, axis=0)\n        for j in range(n):\n            if col_sums[j] == 0:\n                S[:, j] = v.flatten()\n            else:\n                S[:, j] = W[:, j] / col_sums[j]\n\n        # 2. Compute the exact stationary distribution x_star\n        I = np.identity(n, dtype=np.float64)\n        A = I - alpha * S\n        b = (1 - alpha) * v\n        x_star = np.linalg.solve(A, b)\n        # Normalization is generally redundant but specified as a safeguard\n        x_star /= np.sum(x_star)\n\n        # 3.  4. Run power iteration and find K\n        x_k = v.copy()\n        errors_l1 = []\n        K = 0\n\n        # Loop from k=0 to k_max\n        for k_iter in range(k_max + 1):\n            e_k = np.linalg.norm(x_k - x_star, ord=1)\n            errors_l1.append(e_k)\n            d_tv = 0.5 * e_k\n\n            if d_tv = epsilon:\n                K = k_iter\n                break\n            \n            if k_iter == k_max:\n                K = k_max\n                # The loop will naturally break here, K is set correctly.\n                # No need for special handling if limit is reached.\n                break\n\n            # Update for next iteration\n            x_k = alpha * S @ x_k + (1 - alpha) * v\n        else:\n            # This part of loop is only executed if the loop completes without a break.\n            # In our case, range(k_max + 1) will always terminate.\n            # We set K=k_max if the loop finishes without meeting the criterion.\n            # The break statement at k_iter == k_max handles this.\n            K = k_max\n\n        # 5. Estimate empirical convergence rate r_hat\n        if len(errors_l1)  2:  # K=0 case, not enough data for a ratio\n            r_hat = 0.0\n        else:\n            # Calculate ratios e_k / e_{k-1}\n            ratios = [errors_l1[i] / errors_l1[i-1] \n                      for i in range(1, len(errors_l1)) if errors_l1[i-1]  0]\n\n            if not ratios:\n                r_hat = 0.0\n            else:\n                num_ratios_to_consider = min(50, len(ratios))\n                last_ratios = ratios[-num_ratios_to_consider:]\n                r_hat = np.median(last_ratios)\n\n        return [K, r_hat]\n\n    # --- Test Case Definitions ---\n    # Epsilon and K_max are common for all cases\n    epsilon = 1e-10\n    k_max = 100000\n\n    test_cases = [\n        # Case 1: n=4, alpha=0.85, W for a 4-cycle\n        (4, 0.85, np.array([[0, 0, 0, 1], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]], dtype=np.float64)),\n        # Case 2: n=4, alpha=0.99, W for a 4-cycle (slower mixing)\n        (4, 0.99, np.array([[0, 0, 0, 1], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]], dtype=np.float64)),\n        # Case 3: n=3, alpha=0.85, W with one dangling node\n        (3, 0.85, np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]], dtype=np.float64)),\n        # Case 4: n=3, alpha=0.99, W with one dangling node (slower mixing)\n        (3, 0.99, np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]], dtype=np.float64)),\n        # Case 5: n=4, alpha=0.90, reducible graph (two 2-cycles)\n        (4, 0.90, np.array([[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]], dtype=np.float64)),\n        # Case 6: n=4, alpha=0.00, teleportation only\n        (4, 0.00, np.array([[0, 0, 0, 1], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]], dtype=np.float64)),\n    ]\n\n    results = []\n    for n, alpha, W in test_cases:\n        result = solve_one_case(n, alpha, W, epsilon, k_max)\n        results.append(result)\n\n    # Format the final output string exactly as required.\n    # The default string representation of a list of lists matches the format needs.\n    # E.g., str([[1, 2.0], [3, 4.0]]) - '[[1, 2.0], [3, 4.0]]'\n    # The request is for \"[ [K1,rhat1],[K2,rhat2]... ]\" which str() does not produce.\n    # The template `print(f\"[{','.join(map(str, results))}]\")` will produce a string like\n    # '[[71, 0.85],[498, 0.99]]' which is a standard compact representation.\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "3300074"}, {"introduction": "Building on the concept of a stationary distribution, we now explore its stability. This theoretical exercise [@problem_id:3300024] challenges you to quantify how much the stationary distribution $\\pi$ can change when the transition kernel $P$ is slightly perturbed. By applying the Dobrushin coefficient, you will derive a fundamental bound that is essential for understanding the robustness of simulation results against model inaccuracies.", "problem": "Consider a time-homogeneous Markov chain on a finite state space $\\mathcal{X}$ with transition kernel $P$. Let $\\pi$ denote the unique stationary distribution of $P$. Define the Total Variation (TV) norm of a signed measure $\\mu$ on $\\mathcal{X}$ by $\\|\\mu\\|_{\\mathrm{TV}} = \\frac{1}{2} \\sum_{x \\in \\mathcal{X}} |\\mu(x)|$, and for a kernel $K$, define its perturbation seminorm by\n$$\n\\|K\\|_{\\mathrm{ker}} = \\sup_{x \\in \\mathcal{X}} \\|K(x,\\cdot)\\|_{\\mathrm{TV}}.\n$$\nThe Dobrushin coefficient $\\delta(P)$ of the kernel $P$ is defined as\n$$\n\\delta(P) = \\sup_{\\mu \\neq \\nu} \\frac{\\|\\mu P - \\nu P\\|_{\\mathrm{TV}}}{\\|\\mu - \\nu\\|_{\\mathrm{TV}}},\n$$\nwhere $\\mu$ and $\\nu$ are probability measures on $\\mathcal{X}$ and $\\mu P$ denotes the push-forward of $\\mu$ by $P$. Assume $\\delta(P)  1$ so that $P$ is uniquely ergodic and geometrically contracting in the TV norm.\n\nNow consider a perturbed kernel $\\tilde{P}$ with stationary distribution $\\tilde{\\pi}$, and suppose the kernel perturbation is uniformly small in the sense that\n$$\n\\|P - \\tilde{P}\\|_{\\mathrm{ker}} \\le \\epsilon,\n$$\nfor some $\\epsilon  0$. Using only the core definitions above and their immediate consequences, derive an explicit upper bound on $\\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}}$ in terms of $\\delta(P)$ and $\\epsilon$.\n\nFinally, evaluate your bound for $\\delta(P) = 0.87$ and $\\epsilon = 3.0 \\times 10^{-2}$, and report the numerical value of the bound. Round your final numerical answer to four significant figures.", "solution": "The user wants to solve a problem: \"Consider a time-homogeneous Markov chain on a finite state space $\\mathcal{X}$ with transition kernel $P$. Let $\\pi$ denote the unique stationary distribution of $P$. Define the Total Variation (TV) norm of a signed measure $\\mu$ on $\\mathcal{X}$ by $\\|\\mu\\|_{\\mathrm{TV}} = \\frac{1}{2} \\sum_{x \\in \\mathcal{X}} |\\mu(x)|$, and for a kernel $K$, define its perturbation seminorm by\n$$\n\\|K\\|_{\\mathrm{ker}} = \\sup_{x \\in \\mathcal{X}} \\|K(x,\\cdot)\\|_{\\mathrm{TV}}.\n$$\nThe Dobrushin coefficient $\\delta(P)$ of the kernel $P$ is defined as\n$$\n\\delta(P) = \\sup_{\\mu \\neq \\nu} \\frac{\\|\\mu P - \\nu P\\|_{\\mathrm{TV}}}{\\|\\mu - \\nu\\|_{\\mathrm{TV}}},\n$$\nwhere $\\mu$ and $\\nu$ are probability measures on $\\mathcal{X}$ and $\\mu P$ denotes the push-forward of $\\mu$ by $P$. Assume $\\delta(P)  1$ so that $P$ is uniquely ergodic and geometrically contracting in the TV norm.\n\nNow consider a perturbed kernel $\\tilde{P}$ with stationary distribution $\\tilde{\\pi}$, and suppose the kernel perturbation is uniformly small in the sense that\n$$\n\\|P - \\tilde{P}\\|_{\\mathrm{ker}} \\le \\epsilon,\n$$\nfor some $\\epsilon  0$. Using only the core definitions above and their immediate consequences, derive an explicit upper bound on $\\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}}$ in terms of $\\delta(P)$ and $\\epsilon$.\n\nFinally, evaluate your bound for $\\delta(P) = 0.87$ and $\\epsilon = 3.0 \\times 10^{-2}$, and report the numerical value of the bound. Round your final numerical answer to four significant figures.\"\n\n### Step 1: Extract Givens\n- State space $\\mathcal{X}$ is finite.\n- Markov chain is time-homogeneous with transition kernel $P$.\n- $\\pi$ is the unique stationary distribution of $P$, so $\\pi P = \\pi$.\n- Total Variation (TV) norm of a signed measure $\\mu$: $\\|\\mu\\|_{\\mathrm{TV}} = \\frac{1}{2} \\sum_{x \\in \\mathcal{X}} |\\mu(x)|$.\n- Kernel perturbation seminorm of a kernel $K$: $\\|K\\|_{\\mathrm{ker}} = \\sup_{x \\in \\mathcal{X}} \\|K(x,\\cdot)\\|_{\\mathrm{TV}}$.\n- Dobrushin coefficient of kernel $P$: $\\delta(P) = \\sup_{\\mu \\neq \\nu} \\frac{\\|\\mu P - \\nu P\\|_{\\mathrm{TV}}}{\\|\\mu - \\nu\\|_{\\mathrm{TV}}}$ for probability measures $\\mu, \\nu$.\n- Assumption: $\\delta(P)  1$.\n- Perturbed kernel is $\\tilde{P}$ with stationary distribution $\\tilde{\\pi}$, so $\\tilde{\\pi} \\tilde{P} = \\tilde{\\pi}$.\n- Perturbation bound: $\\|P - \\tilde{P}\\|_{\\mathrm{ker}} \\le \\epsilon$ for some $\\epsilon  0$.\n- Task 1: Derive an upper bound on $\\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}}$ in terms of $\\delta(P)$ and $\\epsilon$.\n- Task 2: Evaluate the bound for $\\delta(P) = 0.87$ and $\\epsilon = 3.0 \\times 10^{-2}$. Report the numerical value rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is formulated using standard, well-established concepts from the theory of Markov chains, including the Total Variation norm, Dobrushin coefficient, and perturbation bounds. These are fundamental tools for analyzing the stability and convergence of stochastic processes. The premises and definitions are mathematically sound.\n- **Well-Posed**: The problem is well-posed. The condition $\\delta(P)  1$ guarantees the existence and uniqueness of the stationary distribution $\\pi$ and the geometric contractivity of the kernel $P$ in the TV norm, which are essential for the analysis. The question asks for an upper bound, which is a typical and meaningful question in this context. The provided information is sufficient to derive the result.\n- **Objective**: The problem is stated in precise, formal mathematical language, free from any subjectivity or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a standard result in the perturbation theory of Markov chains, requiring a rigorous derivation from first principles as outlined. I will proceed with the solution.\n\nThe objective is to find an upper bound for the total variation distance $\\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}}$ between the stationary distributions $\\pi$ and $\\tilde{\\pi}$ of the kernels $P$ and $\\tilde{P}$, respectively.\n\nBy definition, $\\pi$ and $\\tilde{\\pi}$ are stationary distributions, meaning they are probability measures on $\\mathcal{X}$ satisfying:\n$$\n\\pi P = \\pi\n$$\n$$\n\\tilde{\\pi} \\tilde{P} = \\tilde{\\pi}\n$$\nWe wish to bound $\\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}}$. We can use the stationarity properties to rewrite the difference $\\pi - \\tilde{\\pi}$ as:\n$$\n\\pi - \\tilde{\\pi} = \\pi P - \\tilde{\\pi} \\tilde{P}\n$$\nTo connect this to the given quantities, we add and subtract a cross-term, $\\tilde{\\pi} P$:\n$$\n\\pi - \\tilde{\\pi} = (\\pi P - \\tilde{\\pi} P) + (\\tilde{\\pi} P - \\tilde{\\pi} \\tilde{P})\n$$\nThe second term can be factored as $\\tilde{\\pi}(P - \\tilde{P})$. Applying the triangle inequality for the TV norm gives:\n$$\n\\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}} = \\|(\\pi P - \\tilde{\\pi} P) + \\tilde{\\pi}(P - \\tilde{P})\\|_{\\mathrm{TV}} \\le \\|\\pi P - \\tilde{\\pi} P\\|_{\\mathrm{TV}} + \\|\\tilde{\\pi}(P - \\tilde{P})\\|_{\\mathrm{TV}}\n$$\nWe now analyze each of the two terms on the right-hand side separately.\n\nFor the first term, $\\|\\pi P - \\tilde{\\pi} P\\|_{\\mathrm{TV}}$, we use the definition of the Dobrushin coefficient $\\delta(P)$. Since $\\pi$ and $\\tilde{\\pi}$ are probability measures, we have:\n$$\n\\frac{\\|\\pi P - \\tilde{\\pi} P\\|_{\\mathrm{TV}}}{\\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}}} \\le \\sup_{\\mu \\neq \\nu} \\frac{\\|\\mu P - \\nu P\\|_{\\mathrm{TV}}}{\\|\\mu - \\nu\\|_{\\mathrm{TV}}} = \\delta(P)\n$$\nThis directly implies:\n$$\n\\|\\pi P - \\tilde{\\pi} P\\|_{\\mathrm{TV}} \\le \\delta(P) \\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}}\n$$\n\nFor the second term, $\\|\\tilde{\\pi}(P - \\tilde{P})\\|_{\\mathrm{TV}}$, let $K = P - \\tilde{P}$. The push-forward measure $\\tilde{\\pi} K$ evaluated at a state $y \\in \\mathcal{X}$ is $(\\tilde{\\pi} K)(y) = \\sum_{x \\in \\mathcal{X}} \\tilde{\\pi}(x) K(x,y)$. By the definition of the TV norm:\n$$\n\\|\\tilde{\\pi} K\\|_{\\mathrm{TV}} = \\frac{1}{2} \\sum_{y \\in \\mathcal{X}} \\left| \\sum_{x \\in \\mathcal{X}} \\tilde{\\pi}(x) K(x,y) \\right|\n$$\nUsing the triangle inequality for sums of real numbers, and noting that $\\tilde{\\pi}(x) \\ge 0$:\n$$\n\\|\\tilde{\\pi} K\\|_{\\mathrm{TV}} \\le \\frac{1}{2} \\sum_{y \\in \\mathcal{X}} \\sum_{x \\in \\mathcal{X}} |\\tilde{\\pi}(x) K(x,y)| = \\frac{1}{2} \\sum_{y \\in \\mathcal{X}} \\sum_{x \\in \\mathcal{X}} \\tilde{\\pi}(x) |K(x,y)|\n$$\nSince the sums are over a finite space, we can swap the order of summation:\n$$\n\\|\\tilde{\\pi} K\\|_{\\mathrm{TV}} \\le \\sum_{x \\in \\mathcal{X}} \\tilde{\\pi}(x) \\left( \\frac{1}{2} \\sum_{y \\in \\mathcal{X}} |K(x,y)| \\right)\n$$\nThe term in the parenthesis is the TV norm of the signed measure $K(x, \\cdot)$:\n$$\n\\frac{1}{2} \\sum_{y \\in \\mathcal{X}} |K(x,y)| = \\|K(x, \\cdot)\\|_{\\mathrm{TV}}\n$$\nSo, the inequality becomes:\n$$\n\\|\\tilde{\\pi} K\\|_{\\mathrm{TV}} \\le \\sum_{x \\in \\mathcal{X}} \\tilde{\\pi}(x) \\|K(x, \\cdot)\\|_{\\mathrm{TV}}\n$$\nWe now use the definition of the kernel seminorm, $\\|K\\|_{\\mathrm{ker}} = \\sup_{z \\in \\mathcal{X}} \\|K(z, \\cdot)\\|_{\\mathrm{TV}}$. This implies that $\\|K(x, \\cdot)\\|_{\\mathrm{TV}} \\le \\|K\\|_{\\mathrm{ker}}$ for all $x \\in \\mathcal{X}$.\n$$\n\\|\\tilde{\\pi} K\\|_{\\mathrm{TV}} \\le \\sum_{x \\in \\mathcal{X}} \\tilde{\\pi}(x) \\|K\\|_{\\mathrm{ker}} = \\|K\\|_{\\mathrm{ker}} \\sum_{x \\in \\mathcal{X}} \\tilde{\\pi}(x)\n$$\nSince $\\tilde{\\pi}$ is a probability measure, $\\sum_{x \\in \\mathcal{X}} \\tilde{\\pi}(x) = 1$. Thus,\n$$\n\\|\\tilde{\\pi}(P - \\tilde{P})\\|_{\\mathrm{TV}} \\le \\|P - \\tilde{P}\\|_{\\mathrm{ker}}\n$$\nThe problem states that $\\|P - \\tilde{P}\\|_{\\mathrm{ker}} \\le \\epsilon$. Therefore, we have:\n$$\n\\|\\tilde{\\pi}(P - \\tilde{P})\\|_{\\mathrm{TV}} \\le \\epsilon\n$$\nSubstituting the bounds for the two terms back into the main inequality:\n$$\n\\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}} \\le \\delta(P) \\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}} + \\epsilon\n$$\nLet $d = \\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}}$. The inequality is $d \\le \\delta(P) d + \\epsilon$. Rearranging the terms to solve for $d$:\n$$\nd - \\delta(P) d \\le \\epsilon\n$$\n$$\n(1 - \\delta(P)) d \\le \\epsilon\n$$\nSince we are given $\\delta(P)  1$, the term $(1 - \\delta(P))$ is positive. We can divide by it without changing the direction of the inequality:\n$$\n\\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}} \\le \\frac{\\epsilon}{1 - \\delta(P)}\n$$\nThis is the desired explicit upper bound.\n\nFinally, we evaluate this bound for the given numerical values: $\\delta(P) = 0.87$ and $\\epsilon = 3.0 \\times 10^{-2}$.\n$$\n\\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}} \\le \\frac{3.0 \\times 10^{-2}}{1 - 0.87} = \\frac{0.03}{0.13} = \\frac{3}{13}\n$$\nThe numerical value is:\n$$\n\\frac{3}{13} \\approx 0.23076923...\n$$\nRounding to four significant figures, we get $0.2308$.", "answer": "$$\n\\boxed{0.2308}\n$$", "id": "3300024"}, {"introduction": "A well-designed Markov chain is crucial for reliable simulation, but subtle flaws can lead to incorrect results. This exercise [@problem_id:3300040] presents a case study in diagnosing MCMC failure, where a seemingly correct Metropolis-Hastings sampler violates the condition of irreducibility. Analyzing this chain will reveal why it fails to converge to the intended target distribution, underscoring the importance of rigorously verifying the ergodic properties of any MCMC scheme before its use.", "problem": "Consider a Markov chain Monte Carlo (MCMC) scheme designed to sample from a target distribution $\\pi$ on the integer lattice $\\mathbb{Z}$, where $\\pi(k) \\propto \\exp(-|k|)$ for $k \\in \\mathbb{Z}$. The algorithm is a Metropolis-Hastings (MH) scheme with a deterministic constraint in its proposal mechanism: starting from a current state $x \\in \\mathbb{Z}$, it proposes $y \\in \\{x-2, x, x+2\\}$ with $q(y \\mid x)$ given by $q(x-2 \\mid x) = p$, $q(x \\mid x) = 1 - 2p$, $q(x+2 \\mid x) = p$ for some fixed $p \\in (0, 1/2)$, and uses the standard MH acceptance rule. By construction, the proposal preserves the parity of the state; that is, if $x$ is even (respectively odd), then any proposed $y$ is also even (respectively odd). Assume $\\pi(k)  0$ for all $k \\in \\mathbb{Z}$.\n\nFoundational definitions for reference:\n- A Markov chain with transition kernel $P$ on a countable state space is irreducible if for all states $x, y$ there exists $n \\in \\mathbb{N}$ such that $P^n(x, y)  0$.\n- The period $d(x)$ of a state $x$ is defined as $d(x) = \\gcd\\{n \\geq 1 : P^n(x, x)  0\\}$. A chain is aperiodic if $d(x) = 1$ for all $x$ in the communicating class of interest.\n- A probability measure $\\mu$ is stationary (invariant) for $P$ if $\\mu P = \\mu$.\n\nIn this setting, identify the correct statements about ergodicity, invariant distributions, and convergence behavior of the described chain.\n\nA. The chain is irreducible on $\\mathbb{Z}$ and converges to the unique stationary distribution $\\pi$, regardless of initialization.\n\nB. The chain is aperiodic but reducible, with two disjoint closed communicating classes given by the even integers and the odd integers; the chain never transitions between these classes.\n\nC. There are infinitely many invariant distributions of the form $\\mu_\\alpha = \\alpha \\,\\pi_{\\mathrm{even}} + (1-\\alpha)\\,\\pi_{\\mathrm{odd}}$ for $\\alpha \\in [0,1]$, where $\\pi_{\\mathrm{even}}$ and $\\pi_{\\mathrm{odd}}$ are the normalizations of $\\pi$ restricted to the even and odd integers, respectively. If initialized at an even state, the chain converges to $\\pi_{\\mathrm{even}}$.\n\nD. Because the MH acceptance rule enforces detailed balance with respect to $\\pi$, irreducibility is not necessary; the chain converges to $\\pi$ from any initial state.\n\nE. For any integrable function $f$, empirical averages $\\frac{1}{T} \\sum_{t=1}^T f(X_t)$ converge almost surely to $\\mathbb{E}_\\pi[f(X)]$, independent of initialization, because the chain is aperiodic.", "solution": "The user wants me to act as a meticulous and exacting professor in the STEM fields to validate and solve a problem related to a Metropolis-Hastings MCMC scheme.\n\n### Problem Validation\n\nFirst, I will validate the problem statement according to the specified criteria.\n\n**Step 1: Extract Givens**\n\n*   Target distribution: $\\pi$ on the integer lattice $\\mathbb{Z}$, where $\\pi(k) \\propto \\exp(-|k|)$ for $k \\in \\mathbb{Z}$.\n*   Algorithm: Metropolis-Hastings (MH) scheme.\n*   State space: $\\mathcal{S} = \\mathbb{Z}$.\n*   Current state: $x \\in \\mathbb{Z}$.\n*   Proposal distribution $q(y \\mid x)$:\n    *   $q(x-2 \\mid x) = p$.\n    *   $q(x \\mid x) = 1 - 2p$.\n    *   $q(x+2 \\mid x) = p$.\n*   Parameter: $p \\in (0, 1/2)$.\n*   Proposal property: Preserves the parity of the state.\n*   Assumption: $\\pi(k)  0$ for all $k \\in \\mathbb{Z}$.\n*   Reference Definitions: Irreducibility, period, stationary distribution.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem statement describes a standard Metropolis-Hastings algorithm on a countable state space with a well-defined target distribution (a discrete version of the Laplace distribution) and a specific proposal mechanism.\n\n*   **Scientifically Grounded:** The problem is firmly rooted in the theory of Markov chain Monte Carlo methods, a standard and well-established area of statistics and applied mathematics. All concepts—Metropolis-Hastings, target distribution, stationary distribution, irreducibility, aperiodicity—are standard. The setup is mathematically sound.\n*   **Well-Posed:** The question asks for an analysis of the properties of the described Markov chain. The setup is complete and unambiguous, allowing for a rigorous and unique determination of these properties.\n*   **Objective:** The problem is stated in precise, objective mathematical language, free from any subjective or non-formalizable content.\n\nAll elements of the problem statement are consistent. The proposal probabilities sum to $p + (1-2p) + p = 1$. The condition $p \\in (0, 1/2)$ ensures that $p  0$ and $1-2p  0$. The assumption $\\pi(k)  0$ is consistent with $\\pi(k) \\propto \\exp(-|k|)$. The problem is free of any scientific, logical, or structural flaws.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. I will proceed with the detailed derivation and analysis.\n\n### Derivation and Solution\n\nThe problem concerns a Metropolis-Hastings (MH) algorithm designed to sample from the target distribution $\\pi(k) \\propto \\exp(-|k|)$ on the state space $\\mathcal{S} = \\mathbb{Z}$. The transition kernel $P(x,y)$ of the resulting Markov chain determines its properties.\n\n**1. Analysis of the Proposal Distribution $q(y \\mid x)$**\n\nThe proposal distribution is given by $q(x-2 \\mid x) = p$, $q(x+2 \\mid x) = p$, and $q(x \\mid x) = 1-2p$. We note that this proposal distribution is symmetric: $q(y \\mid x) = q(x \\mid y)$. For instance, if $y = x+2$, then $x=y-2$. The proposal probability for this is $q(x+2 \\mid x) = p$, and $q(x \\mid y) = q(y-2 \\mid y) = p$. Symmetry holds. For $y=x$, it is trivial. For other pairs $(x,y)$, $q(y \\mid x) = q(x \\mid y) = 0$.\n\n**2. MH Acceptance Probability $\\alpha(x,y)$**\n\nThe standard MH acceptance probability is $\\alpha(x, y) = \\min\\left(1, \\frac{\\pi(y)q(x \\mid y)}{\\pi(x)q(y \\mid x)}\\right)$. Due to the symmetry of the proposal distribution ($q(x \\mid y) = q(y \\mid x)$), the acceptance probability simplifies to that of the Metropolis algorithm:\n$$ \\alpha(x, y) = \\min\\left(1, \\frac{\\pi(y)}{\\pi(x)}\\right) $$\nSubstituting the form of $\\pi(k)$, we get:\n$$ \\alpha(x, y) = \\min\\left(1, \\frac{C \\exp(-|y|)}{C \\exp(-|x|)}\\right) = \\min\\left(1, \\exp(|x| - |y|)\\right) $$\nSince $\\pi(k)  0$ for all $k \\in \\mathbb{Z}$, the acceptance probability $\\alpha(x, y)$ is strictly positive for any pair $(x, y)$ for which a proposal is possible ($y \\in \\{x-2, x+2\\}$).\n\n**3. Analysis of Irreducibility**\n\nA Markov chain is irreducible on $\\mathbb{Z}$ if for any $x, y \\in \\mathbb{Z}$, there exists an integer $n \\geq 1$ such that $P^n(x, y)  0$. The transition probability from $x$ to $y$ (where $y \\neq x$) is $P(x,y) = q(y \\mid x) \\alpha(x,y)$. A transition is possible only if $q(y \\mid x)  0$.\n\nThe proposal mechanism only allows jumps between states of the same parity. If $x$ is an even integer, $x=2k$ for some $k \\in \\mathbb{Z}$. A proposed state $y$ can be $2k-2 = 2(k-1)$, $2k$, or $2k+2 = 2(k+1)$. All proposed states are even. If $x$ is an odd integer, $x=2k+1$, a proposed state $y$ can be $(2k+1)-2 = 2(k-1)+1$, $2k+1$, or $(2k+1)+2 = 2(k+1)+1$. All proposed states are odd.\n\nConsequently, the chain can never transition from an even state to an odd state, or vice-versa. The state space $\\mathbb{Z}$ is partitioned into two disjoint, closed sets of communicating states:\n*   $\\mathcal{S}_E = \\{ k \\in \\mathbb{Z} : k \\text{ is even} \\}$\n*   $\\mathcal{S}_O = \\{ k \\in \\mathbb{Z} : k \\text{ is odd} \\}$\n\nSince it is impossible to go from a state in $\\mathcal{S}_E$ to a state in $\\mathcal{S}_O$ (e.g., from $0$ to $1$), the chain is **reducible** on $\\mathbb{Z}$.\n\nWithin each subclass, the chain is irreducible. For any $x, y \\in \\mathcal{S}_E$, one can get from $x$ to $y$ through a sequence of steps of size $+2$ or $-2$. Each such step has a positive probability $P(z, z\\pm2) = q(z\\pm2 \\mid z)\\alpha(z, z\\pm2) = p \\cdot \\alpha(z, z\\pm2)  0$. The same logic applies to $\\mathcal{S}_O$.\n\n**4. Analysis of Aperiodicity**\n\nThe period of a state $x$ is $d(x) = \\gcd\\{n \\geq 1 : P^n(x, x)  0\\}$. A chain is aperiodic if $d(x)=1$ for all states in a communicating class. The probability of staying at state $x$ is:\n$$ P(x,x) = q(x \\mid x) + \\sum_{y \\neq x} q(y \\mid x)(1-\\alpha(x,y)) $$\nSince $q(x \\mid x) = 1-2p$ and we are given $p \\in (0, 1/2)$, it follows that $1-2p  0$. Therefore, $P(x,x) \\ge q(x \\mid x)  0$ for all $x \\in \\mathbb{Z}$.\nSince $P^1(x,x)  0$, the set $\\{n \\geq 1 : P^n(x, x)  0\\}$ contains $1$. The greatest common divisor (gcd) of any set of integers containing $1$ is $1$. Thus, $d(x) = 1$ for all $x \\in \\mathbb{Z}$. The chain is **aperiodic**.\n\n**5. Analysis of Invariant Distributions and Convergence**\n\nBy construction, the MH algorithm satisfies the detailed balance condition with respect to $\\pi$: $\\pi(x)P(x,y) = \\pi(y)P(y,x)$. This implies that $\\pi$ is a stationary distribution of the chain. However, because the chain is reducible, the stationary distribution is not unique.\n\nThe set of all stationary distributions is the convex hull of the unique stationary distributions of the irreducible components.\n*   Let $\\pi_{\\mathrm{even}}$ be the normalization of $\\pi$ restricted to $\\mathcal{S}_E$: $\\pi_{\\mathrm{even}}(k) = \\frac{\\pi(k)}{\\sum_{j \\in \\mathcal{S}_E} \\pi(j)}$ for $k \\in \\mathcal{S}_E$ and $0$ otherwise. The chain restricted to $\\mathcal{S}_E$ is irreducible and aperiodic, so it has $\\pi_{\\mathrm{even}}$ as its unique stationary distribution.\n*   Similarly, let $\\pi_{\\mathrm{odd}}$ be the normalization of $\\pi$ restricted to $\\mathcal{S}_O$. The chain restricted to $\\mathcal{S}_O$ converges to $\\pi_{\\mathrm{odd}}$.\n\nAny distribution of the form $\\mu_\\alpha = \\alpha \\, \\pi_{\\mathrm{even}} + (1-\\alpha) \\, \\pi_{\\mathrm{odd}}$ for $\\alpha \\in [0, 1]$ is a valid stationary distribution for the full chain on $\\mathbb{Z}$. This constitutes an infinite family of invariant distributions.\n\nRegarding convergence:\n*   If the chain is initialized at a state $x_0 \\in \\mathcal{S}_E$, it will forever remain in $\\mathcal{S}_E$. As the sub-chain on $\\mathcal{S}_E$ is irreducible and aperiodic, the distribution of $X_t$ converges to $\\pi_{\\mathrm{even}}$ as $t \\to \\infty$.\n*   If initialized at $x_0 \\in \\mathcal{S}_O$, the distribution converges to $\\pi_{\\mathrm{odd}}$.\n\nThe chain does not converge to a single distribution $\\pi$ irrespective of initialization.\n\n### Option-by-Option Analysis\n\n**A. The chain is irreducible on $\\mathbb{Z}$ and converges to the unique stationary distribution $\\pi$, regardless of initialization.**\nThis statement is incorrect. As demonstrated, the chain is **reducible** because it cannot transition between even and odd integers. Consequently, it does not have a unique stationary distribution, and its convergence behavior depends on the initial state's parity.\n\n**B. The chain is aperiodic but reducible, with two disjoint closed communicating classes given by the even integers and the odd integers; the chain never transitions between these classes.**\nThis statement is correct. Our analysis confirms that the chain is aperiodic (since $P(x,x)  0$) and reducible. The two disjoint closed communicating classes are precisely the set of even integers ($\\mathcal{S}_E$) and the set of odd integers ($\\mathcal{S}_O$).\n\n**C. There are infinitely many invariant distributions of the form $\\mu_\\alpha = \\alpha \\,\\pi_{\\mathrm{even}} + (1-\\alpha)\\,\\pi_{\\mathrm{odd}}$ for $\\alpha \\in [0,1]$, where $\\pi_{\\mathrm{even}}$ and $\\pi_{\\mathrm{odd}}$ are the normalizations of $\\pi$ restricted to the even and odd integers, respectively. If initialized at an even state, the chain converges to $\\pi_{\\mathrm{even}}$.**\nThis statement is correct. Our analysis shows that due to reducibility, any convex combination of the stationary distributions of the irreducible components ($\\pi_{\\mathrm{even}}$ and $\\pi_{\\mathrm{odd}}$) is also a stationary distribution. This gives an infinite family parameterized by $\\alpha \\in [0,1]$. Furthermore, since the chain restricted to even integers is irreducible and aperiodic, it converges to its unique stationary distribution, $\\pi_{\\mathrm{even}}$, when started in an even state.\n\n**D. Because the MH acceptance rule enforces detailed balance with respect to $\\pi$, irreducibility is not necessary; the chain converges to $\\pi$ from any initial state.**\nThis statement is incorrect. While detailed balance ensures that $\\pi$ is a stationary distribution, it does not guarantee convergence to $\\pi$ from any initial state. Ergodicity, which requires both irreducibility and aperiodicity on the space of interest, is necessary for such global convergence. This problem is a canonical example where the lack of irreducibility prevents convergence to a single target distribution across the entire state space.\n\n**E. For any integrable function $f$, empirical averages $\\frac{1}{T} \\sum_{t=1}^T f(X_t)$ converge almost surely to $\\mathbb{E}_\\pi[f(X)]$, independent of initialization, because the chain is aperiodic.**\nThis statement is incorrect. The conclusion describes the strong law of large numbers for ergodic Markov chains. This chain is not ergodic on $\\mathbb{Z}$ because it is not irreducible. The limit of the empirical average depends on the initial state. If started at an even state $x_0 \\in \\mathcal{S}_E$, the average converges to $\\mathbb{E}_{\\pi_{\\mathrm{even}}}[f(X)]$. If started at an odd state $x_0 \\in \\mathcal{S}_O$, it converges to $\\mathbb{E}_{\\pi_{\\mathrm{odd}}}[f(X)]$. These two values are generally not equal to each other or to $\\mathbb{E}_\\pi[f(X)]$. Aperiodicity alone is insufficient to guarantee the stated convergence.\n\nBoth options B and C are correct descriptions of the chain's properties and behavior.", "answer": "$$\\boxed{BC}$$", "id": "3300040"}]}