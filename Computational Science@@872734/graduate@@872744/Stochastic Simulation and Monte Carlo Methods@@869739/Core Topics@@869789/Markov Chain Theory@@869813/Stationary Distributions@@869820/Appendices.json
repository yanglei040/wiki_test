{"hands_on_practices": [{"introduction": "This first exercise takes us back to the foundational definition of a stationary distribution. By working with a small, finite-state Markov chain, you will compute the invariant distribution $\\pi$ by directly solving the system of linear equations given by the invariance condition $\\pi = \\pi P$, combined with the normalization constraint. This fundamental practice reinforces the algebraic underpinnings of stationarity before moving on to more complex algorithmic applications.", "problem": "Consider a discrete-time, finite-state Markov chain with state space $\\{1,2,3,4\\}$ and a row-stochastic transition matrix $P$ given by\n$$\nP \\;=\\;\n\\begin{pmatrix}\n\\frac{1}{2}  \\frac{1}{2}  0  0 \\\\\n\\frac{1}{4}  \\frac{1}{2}  \\frac{1}{4}  0 \\\\\n0  \\frac{1}{3}  \\frac{1}{3}  \\frac{1}{3} \\\\\n\\frac{1}{6}  0  \\frac{1}{2}  \\frac{1}{3}\n\\end{pmatrix}.\n$$\nUsing only foundational definitions of a Markov chain transition matrix and a stationary (invariant) distribution, derive and compute exactly the stationary distribution vector $\\pi$ that satisfies $\\pi = \\pi P$ with $\\sum_{i=1}^{4} \\pi_i = 1$ and $\\pi_i \\ge 0$ for all $i$. Present $\\pi$ in simplest rational form, and verify explicitly that the computed $\\pi$ satisfies invariance and normalization. The final answer must be the stationary distribution vector written as a single row matrix. No rounding is required.", "solution": "The problem as stated is subjected to validation.\n\n**Step 1: Extracted Givens**\n- **System Type**: Discrete-time, finite-state Markov chain.\n- **State Space**: $S = \\{1, 2, 3, 4\\}$.\n- **Transition Matrix**:\n$$\nP \\;=\\;\n\\begin{pmatrix}\n\\frac{1}{2}  \\frac{1}{2}  0  0 \\\\\n\\frac{1}{4}  \\frac{1}{2}  \\frac{1}{4}  0 \\\\\n0  \\frac{1}{3}  \\frac{1}{3}  \\frac{1}{3} \\\\\n\\frac{1}{6}  0  \\frac{1}{2}  \\frac{1}{3}\n\\end{pmatrix}\n$$\n- **Stationary Distribution Definition**: A vector $\\pi = \\begin{pmatrix} \\pi_1  \\pi_2  \\pi_3  \\pi_4 \\end{pmatrix}$ such that:\n    1. Invariance: $\\pi = \\pi P$.\n    2. Normalization: $\\sum_{i=1}^{4} \\pi_i = 1$.\n    3. Non-negativity: $\\pi_i \\ge 0$ for $i \\in \\{1, 2, 3, 4\\}$.\n- **Task**: Compute the exact stationary distribution $\\pi$ and explicitly verify the result.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is scientifically grounded, being a standard problem in the theory of stochastic processes. The provided matrix $P$ is a valid row-stochastic matrix, as all entries are non-negative and the sum of each row is $1$. For a unique stationary distribution to exist, the Markov chain must be irreducible and aperiodic.\n\n- **Irreducibility**: The graph of the chain shows that every state can be reached from every other state (e.g., $1 \\to 2 \\to 3 \\to 4 \\to 1$). The chain is strongly connected, hence irreducible.\n- **Aperiodicity**: The presence of self-loops (e.g., $P_{11}  0$, $P_{22}  0$, etc.) ensures that the period of each state is $1$. The chain is therefore aperiodic.\n\nSince the chain is finite, irreducible, and aperiodic, a unique stationary distribution exists. The problem is well-posed, unambiguous, and contains all necessary information. It does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be derived.\n\n**Derivation of the Stationary Distribution**\n\nA stationary distribution $\\pi = \\begin{pmatrix} \\pi_1  \\pi_2  \\pi_3  \\pi_4 \\end{pmatrix}$ must satisfy the matrix equation $\\pi P = \\pi$. This corresponds to the following system of linear equations:\n$$\n\\begin{cases}\n\\pi_1 = \\frac{1}{2}\\pi_1 + \\frac{1}{4}\\pi_2 + (0)\\pi_3 + \\frac{1}{6}\\pi_4 \\\\\n\\pi_2 = \\frac{1}{2}\\pi_1 + \\frac{1}{2}\\pi_2 + \\frac{1}{3}\\pi_3 + (0)\\pi_4 \\\\\n\\pi_3 = (0)\\pi_1 + \\frac{1}{4}\\pi_2 + \\frac{1}{3}\\pi_3 + \\frac{1}{2}\\pi_4 \\\\\n\\pi_4 = (0)\\pi_1 + (0)\\pi_2 + \\frac{1}{3}\\pi_3 + \\frac{1}{3}\\pi_4\n\\end{cases}\n$$\nThis system can be rewritten as $\\pi(P - I) = 0$, where $I$ is the identity matrix.\n$$\n\\begin{cases}\n-\\frac{1}{2}\\pi_1 + \\frac{1}{4}\\pi_2 + \\frac{1}{6}\\pi_4 = 0  (1) \\\\\n\\frac{1}{2}\\pi_1 - \\frac{1}{2}\\pi_2 + \\frac{1}{3}\\pi_3 = 0  (2) \\\\\n\\frac{1}{4}\\pi_2 - \\frac{2}{3}\\pi_3 + \\frac{1}{2}\\pi_4 = 0  (3) \\\\\n\\frac{1}{3}\\pi_3 - \\frac{2}{3}\\pi_4 = 0  (4)\n\\end{cases}\n$$\nThis system is linearly dependent. We will solve for three variables in terms of a fourth, and then use the normalization condition $\\pi_1 + \\pi_2 + \\pi_3 + \\pi_4 = 1$.\n\nFrom equation ($4$):\n$$\n\\frac{1}{3}\\pi_3 = \\frac{2}{3}\\pi_4 \\implies \\pi_3 = 2\\pi_4\n$$\nSubstitute $\\pi_3 = 2\\pi_4$ into equation ($3$):\n$$\n\\frac{1}{4}\\pi_2 - \\frac{2}{3}(2\\pi_4) + \\frac{1}{2}\\pi_4 = 0 \\\\\n\\frac{1}{4}\\pi_2 - \\frac{4}{3}\\pi_4 + \\frac{1}{2}\\pi_4 = 0 \\\\\n\\frac{1}{4}\\pi_2 = (\\frac{4}{3} - \\frac{1}{2})\\pi_4 = (\\frac{8-3}{6})\\pi_4 = \\frac{5}{6}\\pi_4 \\\\\n\\pi_2 = 4 \\cdot \\frac{5}{6}\\pi_4 = \\frac{20}{6}\\pi_4 = \\frac{10}{3}\\pi_4\n$$\nSubstitute $\\pi_2 = \\frac{10}{3}\\pi_4$ and $\\pi_3 = 2\\pi_4$ into equation ($2$):\n$$\n\\frac{1}{2}\\pi_1 - \\frac{1}{2}(\\frac{10}{3}\\pi_4) + \\frac{1}{3}(2\\pi_4) = 0 \\\\\n\\frac{1}{2}\\pi_1 - \\frac{5}{3}\\pi_4 + \\frac{2}{3}\\pi_4 = 0 \\\\\n\\frac{1}{2}\\pi_1 - \\frac{3}{3}\\pi_4 = 0 \\\\\n\\frac{1}{2}\\pi_1 = \\pi_4 \\implies \\pi_1 = 2\\pi_4\n$$\nNow we express all components in terms of $\\pi_4$ and apply the normalization condition $\\pi_1 + \\pi_2 + \\pi_3 + \\pi_4 = 1$:\n$$\n2\\pi_4 + \\frac{10}{3}\\pi_4 + 2\\pi_4 + \\pi_4 = 1 \\\\\n(2 + \\frac{10}{3} + 2 + 1)\\pi_4 = 1 \\\\\n(5 + \\frac{10}{3})\\pi_4 = 1 \\\\\n(\\frac{15+10}{3})\\pi_4 = 1 \\\\\n\\frac{25}{3}\\pi_4 = 1 \\implies \\pi_4 = \\frac{3}{25}\n$$\nNow we can compute the other components:\n$$\n\\pi_1 = 2\\pi_4 = 2 \\left( \\frac{3}{25} \\right) = \\frac{6}{25} \\\\\n\\pi_2 = \\frac{10}{3}\\pi_4 = \\frac{10}{3} \\left( \\frac{3}{25} \\right) = \\frac{10}{25} = \\frac{2}{5} \\\\\n\\pi_3 = 2\\pi_4 = 2 \\left( \\frac{3}{25} \\right) = \\frac{6}{25}\n$$\nThus, the stationary distribution is $\\pi = \\begin{pmatrix} \\frac{6}{25}  \\frac{2}{5}  \\frac{6}{25}  \\frac{3}{25} \\end{pmatrix}$.\n\n**Explicit Verification**\n\n1.  **Normalization Check**:\n    $$\n    \\sum_{i=1}^{4} \\pi_i = \\frac{6}{25} + \\frac{2}{5} + \\frac{6}{25} + \\frac{3}{25} = \\frac{6}{25} + \\frac{10}{25} + \\frac{6}{25} + \\frac{3}{25} = \\frac{6+10+6+3}{25} = \\frac{25}{25} = 1\n    $$\n    The normalization condition is satisfied.\n\n2.  **Invariance Check** ($\\pi = \\pi P$):\n    $$\n    \\pi P = \\begin{pmatrix} \\frac{6}{25}  \\frac{10}{25}  \\frac{6}{25}  \\frac{3}{25} \\end{pmatrix} \\begin{pmatrix}\n    \\frac{1}{2}  \\frac{1}{2}  0  0 \\\\\n    \\frac{1}{4}  \\frac{1}{2}  \\frac{1}{4}  0 \\\\\n    0  \\frac{1}{3}  \\frac{1}{3}  \\frac{1}{3} \\\\\n    \\frac{1}{6}  0  \\frac{1}{2}  \\frac{1}{3}\n    \\end{pmatrix}\n    $$\n    Let's compute each element of the resulting vector:\n    - $(\\pi P)_1 = (\\frac{6}{25})(\\frac{1}{2}) + (\\frac{10}{25})(\\frac{1}{4}) + (\\frac{6}{25})(0) + (\\frac{3}{25})(\\frac{1}{6}) = \\frac{3}{25} + \\frac{5}{50} + 0 + \\frac{1}{50} = \\frac{6}{50} + \\frac{5}{50} + \\frac{1}{50} = \\frac{12}{50} = \\frac{6}{25} = \\pi_1$.\n    - $(\\pi P)_2 = (\\frac{6}{25})(\\frac{1}{2}) + (\\frac{10}{25})(\\frac{1}{2}) + (\\frac{6}{25})(\\frac{1}{3}) + (\\frac{3}{25})(0) = \\frac{3}{25} + \\frac{5}{25} + \\frac{2}{25} + 0 = \\frac{10}{25} = \\frac{2}{5} = \\pi_2$.\n    - $(\\pi P)_3 = (\\frac{6}{25})(0) + (\\frac{10}{25})(\\frac{1}{4}) + (\\frac{6}{25})(\\frac{1}{3}) + (\\frac{3}{25})(\\frac{1}{2}) = 0 + \\frac{5}{50} + \\frac{2}{25} + \\frac{3}{50} = \\frac{5}{50} + \\frac{4}{50} + \\frac{3}{50} = \\frac{12}{50} = \\frac{6}{25} = \\pi_3$.\n    - $(\\pi P)_4 = (\\frac{6}{25})(0) + (\\frac{10}{25})(0) + (\\frac{6}{25})(\\frac{1}{3}) + (\\frac{3}{25})(\\frac{1}{3}) = 0 + 0 + \\frac{2}{25} + \\frac{1}{25} = \\frac{3}{25} = \\pi_4$.\n\nThe computed vector $\\pi P$ is equal to $\\pi$. The invariance condition is satisfied. The derived distribution is correct.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{6}{25}  \\frac{2}{5}  \\frac{6}{25}  \\frac{3}{25} \\end{pmatrix}}\n$$", "id": "3347139"}, {"introduction": "While satisfying the detailed balance condition ensures a Markov chain has the correct stationary distribution, it does not guarantee convergence. A useful MCMC algorithm must also be ergodic, which requires it to be irreducible—capable of reaching all parts of the state space. This exercise [@problem_id:3347152] presents a critical thought experiment where a seemingly valid independence sampler fails because its proposal mechanism cannot access the full support of the target density, trapping the chain and making it non-ergodic.", "problem": "Consider a state space $\\mathsf{X}=\\mathbb{R}$ with its Borel $\\sigma$-algebra. Let the target probability density $\\pi$ be a mixture supported on two disjoint compact intervals, defined by\n$$\n\\pi(x)\\;=\\;w\\cdot \\frac{1}{20}\\,\\mathbf{1}_{[-10,\\,10]}(x)\\;+\\;(1-w)\\cdot \\mathbf{1}_{[100,\\,101]}(x),\n$$\nwhere $w\\in(0,1)$ is fixed (e.g., $w=0.97$), and $\\mathbf{1}_A$ denotes the indicator of a set $A$. Consider the independence Metropolis–Hastings algorithm targeting $\\pi$ with proposal density\n$$\nq(y)\\;=\\;\\frac{1}{20}\\,\\mathbf{1}_{[-10,\\,10]}(y).\n$$\nLet $\\mathcal{A}=[-10,\\,10]$ and $\\mathcal{B}=[100,\\,101]$, and let $P$ denote the resulting Markov transition kernel on $\\mathsf{X}$.\n\nBased only on core definitions of stationarity (invariance), irreducibility, and ergodicity for Markov chains, and the construction of the independence Metropolis–Hastings algorithm, determine which of the following statements are true. Select all that apply.\n\nA. The chain has $\\pi$ as an invariant distribution, but it is not $\\pi$-irreducible; moreover, there exist uncountably many distinct invariant distributions for $P$.\n\nB. The chain is irreducible and ergodic because an independence proposal does not depend on the current state, hence it eventually explores the entire state space with probability $1$.\n\nC. If initialized at any $x\\in \\mathcal{B}$, the chain remains at that exact state forever with probability $1$.\n\nD. If the proposal is replaced by $q^\\star(y)=\\frac{1}{2}e^{-|y|}$ for all $y\\in\\mathbb{R}$ (so $q^\\star0$ everywhere), then the resulting independence Metropolis–Hastings chain is $\\pi$-irreducible and aperiodic with unique invariant distribution $\\pi$; furthermore, if $\\sup_{x\\in \\mathsf{X}}\\pi(x)/q^\\star(x)\\infty$, it is uniformly ergodic.\n\nE. The detailed balance equation with respect to $\\pi$ fails because $q(y)=0$ on a subset where $\\pi(y)0$, so $\\pi$ cannot be stationary for $P$.", "solution": "The problem statement is a valid exercise in the theory of Markov Chain Monte Carlo methods. All provided definitions and data are mathematically consistent and well-posed. We may proceed with the solution.\n\nThe problem describes an independence Metropolis–Hastings (IMH) algorithm. For an IMH sampler, a candidate state $Y$ is proposed from a fixed distribution $q(y)$, independently of the current state $X_n = x$. The proposed state is accepted with probability\n$$\n\\alpha(x, y) = \\min\\left(1, \\frac{\\pi(y)q(x)}{\\pi(x)q(y)}\\right)\n$$\nIf the proposal is accepted, $X_{n+1} = Y$. If rejected, $X_{n+1} = X_n = x$.\n\nThe target density is $\\pi(x)\\;=\\;w\\cdot \\frac{1}{20}\\,\\mathbf{1}_{\\mathcal{A}}(x)\\;+\\;(1-w)\\cdot \\mathbf{1}_{\\mathcal{B}}(x)$, where $\\mathcal{A}=[-10,\\,10]$ and $\\mathcal{B}=[100,\\,101]$. The support of $\\pi$ is $\\text{supp}(\\pi) = \\mathcal{A} \\cup \\mathcal{B}$.\nThe proposal density is $q(y)\\;=\\;\\frac{1}{20}\\,\\mathbf{1}_{\\mathcal{A}}(y)$. The support of $q$ is $\\text{supp}(q) = \\mathcal{A} = [-10, 10]$.\n\nA crucial observation is that the support of the proposal density $q$ does not cover the entire support of the target density $\\pi$. Specifically, for any point $x \\in \\mathcal{B}$, we have $\\pi(x) = 1-w  0$, but since $x \\notin \\mathcal{A}$, we have $q(x) = 0$.\n\nLet us analyze the behavior of the chain when the current state $X_n = x$ is in the set $\\mathcal{B}$. A new state $Y$ is proposed from $q$, which means $Y$ is always in $\\mathcal{A}$. The acceptance probability is:\n$$\n\\alpha(x, Y) = \\min\\left(1, \\frac{\\pi(Y)q(x)}{\\pi(x)q(Y)}\\right)\n$$\nSince $x \\in \\mathcal{B}$, we have $q(x) = 0$. The proposal $Y$ is in $\\mathcal{A}$, so $Y \\in \\text{supp}(\\pi)$ and $Y \\in \\text{supp}(q)$, meaning $\\pi(Y)0$ and $q(Y)0$. Also, for $x \\in \\mathcal{B}$, $\\pi(x)  0$. The ratio inside the minimum becomes:\n$$\n\\frac{\\pi(Y) \\cdot 0}{\\pi(x)q(Y)} = 0\n$$\nTherefore, the acceptance probability is $\\alpha(x, Y) = \\min(1, 0) = 0$.\nThis means that if the chain is currently in any state $x \\in \\mathcal{B}$, any proposed move to a state $Y \\in \\mathcal{A}$ will be rejected with probability $1$. The chain will remain at state $x$. Thus, every state $x \\in \\mathcal{B}$ is an absorbing state.\n\nWith this central finding, we can evaluate each statement.\n\n**A. The chain has $\\pi$ as an invariant distribution, but it is not $\\pi$-irreducible; moreover, there exist uncountably many distinct invariant distributions for $P$.**\n\n1.  **Stationarity of $\\pi$**: The Metropolis-Hastings algorithm is constructed to satisfy the detailed balance condition $\\pi(x)p(x,y) = \\pi(y)p(y,x)$, which implies that $\\pi$ is a stationary (invariant) distribution for the transition kernel $P$. This property holds even if the supports do not match, as long as the terms are handled carefully. For any $x,y$, $\\pi(x)q(y)\\alpha(x,y) = \\min(\\pi(x)q(y), \\pi(y)q(x)) = \\pi(y)q(x)\\alpha(y,x)$. Integrating this shows $\\pi$ is stationary. So, this part is true.\n2.  **$\\pi$-irreducibility**: A chain is $\\pi$-irreducible if it can move from any state in $\\text{supp}(\\pi)$ to any set $A \\subseteq \\text{supp}(\\pi)$ with $\\pi(A)  0$ in a finite number of steps. We have shown that if the chain starts at $x \\in \\mathcal{B}$, it can never transition to any state in $\\mathcal{A}$. Let $x \\in \\mathcal{B}$ and $A = \\mathcal{A}$. We have $\\pi(A)=w  0$. However, $P^n(x, \\mathcal{A}) = 0$ for all $n \\ge 1$. Therefore, the chain is not $\\pi$-irreducible.\n3.  **Uncountably many invariant distributions**: Since every state $x_0 \\in \\mathcal{B}$ is an absorbing state, the transition kernel satisfies $P(x_0, \\{x_0\\}) = 1$. This implies that the Dirac delta measure $\\delta_{x_0}$ is an invariant distribution for each $x_0 \\in \\mathcal{B}$. Since $\\mathcal{B} = [100, 101]$ is an uncountable set, there are uncountably many such distinct invariant distributions.\n\nAll three clauses of the statement are correct.\n**Verdict: Correct**\n\n**B. The chain is irreducible and ergodic because an independence proposal does not depend on the current state, hence it eventually explores the entire state space with probability $1$.**\n\nThis statement claims the chain is irreducible, which we have just proven false. A chain that is not irreducible cannot be ergodic. The reasoning provided is flawed because it neglects the acceptance-rejection step of the algorithm. While the proposal mechanism is independent of the current state, the acceptance probability $\\alpha(x, y)$ is not, and in this case, it prevents the chain from exploring the full state space.\n**Verdict: Incorrect**\n\n**C. If initialized at any $x\\in \\mathcal{B}$, the chain remains at that exact state forever with probability $1$.**\n\nAs derived above, if the chain is at a state $x \\in \\mathcal{B}$, the acceptance probability for any proposal is $0$. This means the chain will always reject the proposal and remain at $x$. Thus, $P(x, \\{x\\}) = 1$ for all $x \\in \\mathcal{B}$. This statement is a direct consequence of our initial analysis.\n**Verdict: Correct**\n\n**D. If the proposal is replaced by $q^\\star(y)=\\frac{1}{2}e^{-|y|}$ for all $y\\in\\mathbb{R}$ (so $q^\\star0$ everywhere), then the resulting independence Metropolis–Hastings chain is $\\pi$-irreducible and aperiodic with unique invariant distribution $\\pi$; furthermore, if $\\sup_{x\\in \\mathsf{X}}\\pi(x)/q^\\star(x)\\infty$, it is uniformly ergodic.**\n\nThis statement concerns a hypothetical modification. We must verify the claims for this new chain.\n1.  **$\\pi$-irreducibility**: The new proposal $q^\\star$ has support over all of $\\mathbb{R}$. The support of the target $\\pi$ is $\\mathcal{A} \\cup \\mathcal{B}$. Thus, $\\text{supp}(\\pi) \\subset \\text{supp}(q^\\star)$. For an IMH algorithm, this condition is sufficient for $\\pi$-irreducibility. The chain can transition from any $x \\in \\text{supp}(\\pi)$ to any set $A$ with $\\pi(A)0$ because a proposal into $A$ is possible ($ \\int_A q^\\star(y)dy  0 $) and the acceptance probability $\\alpha(x,y)=\\min(1, \\frac{\\pi(y)q^\\star(x)}{\\pi(x)q^\\star(y)})$ will be positive for $y \\in A$.\n2.  **Aperiodicity**: The chain is aperiodic because the probability of staying at state $x$, $P(x,\\{x\\})$, is positive (the rejection probability is generally greater than $0$).\n3.  **Unique invariant distribution**: Since the chain is $\\pi$-irreducible and has $\\pi$ as a stationary distribution (by construction), $\\pi$ must be the unique stationary distribution.\n4.  **Uniform ergodicity**: A standard theorem states that an IMH chain is uniformly ergodic if and only if $\\sup_{x \\in \\mathsf{X}} \\pi(x)/q(x)  \\infty$. Let's check this for $\\pi$ and $q^\\star$. The ratio is $f(x) = \\frac{\\pi(x)}{q^\\star(x)} = \\frac{\nw\\cdot \\frac{1}{20}\\,\\mathbf{1}_{\\mathcal{A}}(x)\\;+\\;(1-w)\\cdot \\mathbf{1}_{\\mathcal{B}}(x)}{\\frac{1}{2}e^{-|x|}}$.\nThis function is non-zero only on the compact set $\\mathcal{A} \\cup \\mathcal{B}$. On this compact set, the numerator $\\pi(x)$ is bounded above, and the denominator $q^\\star(x)$ is strictly positive and bounded below by a positive constant. Thus, the ratio is bounded above. The supremum is finite. The provided condition for uniform ergodicity is both correct as a theorem and satisfied by the given functions.\n\nThe entire \"if-then\" statement and all its components are consistent with established MCMC theory.\n**Verdict: Correct**\n\n**E. The detailed balance equation with respect to $\\pi$ fails because $q(y)=0$ on a subset where $\\pi(y)0$, so $\\pi$ cannot be stationary for $P$.**\n\nThe premise that detailed balance fails is incorrect. As shown in the analysis for option A, the detailed balance equation $\\pi(x)q(y)\\alpha(x,y) = \\pi(y)q(x)\\alpha(y,x)$ holds for all $x,y$. If one side is zero due to a $q$ term being zero, the other side will also be zero. For example, if $x \\in \\mathcal{A}$ and $y \\in \\mathcal{B}$, $q(y)=0$, so the LHS is $0$. For the RHS, $\\alpha(y,x)=0$ since its ratio contains $q(y)=0$ in the numerator, so the RHS is also $0$. The conclusion that $\\pi$ cannot be stationary is also false; we explicitly showed it is stationary.\n**Verdict: Incorrect**\n\nSummary of verdicts:\nA: Correct\nB: Incorrect\nC: Correct\nD: Correct\nE: Incorrect\n\nThe correct options are A, C, and D.", "answer": "$$\n\\boxed{ACD}\n$$", "id": "3347152"}, {"introduction": "The detailed balance condition, which implies reversibility, is a powerful tool for designing MCMC algorithms, but is it always the optimal choice for rapid convergence? This hands-on computational exercise [@problem_id:3347133] challenges you to implement and compare two Markov chains on a ring: one reversible and one non-reversible, both having the same uniform stationary distribution. By numerically computing their spectral gaps, you can directly investigate the powerful idea that non-reversible dynamics can lead to faster mixing and more efficient sampling.", "problem": "Consider a discrete-time Markov chain on a ring (cycle) with $N$ states labeled $\\{0,1,\\dots,N-1\\}$. Two transition kernels are defined on the same state space for a fixed laziness parameter $\\ell \\in (0,1)$:\n\n- A reversible, lazy, symmetric nearest-neighbor kernel $P_{\\mathrm{rev}}$:\n  - From state $i$, the chain stays put with probability $\\ell$, moves to $(i+1) \\bmod N$ with probability $(1-\\ell)/2$, and moves to $(i-1) \\bmod N$ with probability $(1-\\ell)/2$.\n\n- A nonreversible, lazy, forward-oriented kernel $P_{\\mathrm{nonrev}}$:\n  - From state $i$, the chain stays put with probability $\\ell$, and moves to $(i+1) \\bmod N$ with probability $1-\\ell$.\n\nWork in purely mathematical terms. You may assume standard definitions from the theory of Markov chains and linear algebra. Use the following foundational base:\n\n- The definition of a stationary (invariant) distribution $\\pi$ as a probability vector satisfying $\\pi P = \\pi$.\n- The definition of reversibility (detailed balance) with respect to $\\pi$ for a kernel $P$.\n- The spectrum of a finite stochastic matrix $P$ consists of eigenvalues $\\{\\lambda_k\\}$ with $\\max_k |\\lambda_k| \\le 1$ and at least one eigenvalue equal to $1$ for an irreducible chain.\n- The modulus spectral gap of a kernel $P$ is defined as $\\delta(P) \\equiv 1 - \\rho(P)$ where $\\rho(P) \\equiv \\max\\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } P, \\lambda \\ne 1\\}$.\n- A spectral-radius-based proxy for the mixing time in total variation, for a tolerance $\\varepsilon \\in (0,1)$, is defined by\n  $$ t_{\\varepsilon}(P) \\equiv \\min\\{ t \\in \\mathbb{N} : \\rho(P)^t \\le \\varepsilon\\} = \\left\\lceil \\frac{\\log(\\varepsilon)}{\\log(\\rho(P))} \\right\\rceil, $$\n  with the convention that $\\log$ denotes the natural logarithm and $0  \\rho(P)  1$ for the chains considered here.\n\nTasks:\n\n1. For each of the two kernels, determine the stationary distribution $\\pi$ and argue whether the kernel is reversible with respect to $\\pi$.\n2. For each kernel, justify that all eigenvalues lie in the closed unit disk in the complex plane and that $\\lambda = 1$ is an eigenvalue. Explain why the modulus spectral gap $\\delta(P)$ as defined above is well-defined for the cases considered (i.e., $\\rho(P)  1$ when $\\ell \\in (0,1)$).\n3. Design an algorithm that, given $(N,\\ell)$, constructs the transition matrices $P_{\\mathrm{rev}}$ and $P_{\\mathrm{nonrev}}$, computes all eigenvalues numerically, identifies and removes the eigenvalue closest to $1$ (to handle numerical rounding), and then computes:\n   - The modulus spectral gap $\\delta(P)$ for each kernel.\n   - The proxy mixing time $t_{\\varepsilon}(P)$ for each kernel for a given tolerance $\\varepsilon$.\n4. Implement the algorithm as a complete program that outputs, for each test case, a list containing $[\\delta(P_{\\mathrm{rev}}), \\delta(P_{\\mathrm{nonrev}}), t_{\\varepsilon}(P_{\\mathrm{rev}}), t_{\\varepsilon}(P_{\\mathrm{nonrev}}), \\mathbf{1}\\{t_{\\varepsilon}(P_{\\mathrm{nonrev}})  t_{\\varepsilon}(P_{\\mathrm{rev}})\\}]$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator taking value $1$ for true and $0$ for false. Round all floating-point outputs to six decimal places.\n\nUse the following test suite of parameter values $(N,\\ell,\\varepsilon)$:\n\n- Case A (general): $(N,\\ell,\\varepsilon) = (10, 0.2, 10^{-4})$.\n- Case B (small ring, edge case): $(N,\\ell,\\varepsilon) = (3, 0.3, 10^{-4})$.\n- Case C (larger ring, low laziness): $(N,\\ell,\\varepsilon) = (50, 0.05, 10^{-4})$.\n- Case D (moderate ring, high laziness): $(N,\\ell,\\varepsilon) = (15, 0.9, 10^{-4})$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list of per-case lists, with no spaces, enclosed in square brackets. That is, print a list of lists, one per test case, in order A, B, C, D. For example, an output with two hypothetical cases would look like\n  $$ [[0.123456,0.234567,100,80,1],[0.111111,0.222222,90,70,1]]. $$", "solution": "The problem statement is a well-defined exercise in the theory of finite Markov chains and numerical linear algebra. It is scientifically sound, self-contained, and objective. There are no contradictions or ambiguities. Therefore, the problem is deemed valid and I shall proceed with a complete solution.\n\nThe solution is structured to first address the theoretical aspects of the problem (Tasks 1 and 2) and then to detail the algorithmic design for the numerical computation (Task 3), which forms the basis for the final implementation.\n\n### Task 1: Stationary Distribution and Reversibility\n\nLet the state space be $S = \\{0, 1, \\dots, N-1\\}$. A probability distribution $\\pi = (\\pi_0, \\pi_1, \\dots, \\pi_{N-1})$ is stationary for a transition matrix $P$ if it is a left eigenvector of $P$ with eigenvalue $1$, i.e., $\\pi P = \\pi$.\n\nFor a system with cyclic symmetry, such as the one described, the uniform distribution $\\pi_{\\text{unif}}$ where $\\pi_i = 1/N$ for all $i \\in S$ is a natural candidate for the stationary distribution. We verify this for both kernels.\n\nA sufficient condition for the uniform distribution to be stationary is that the transition matrix $P$ is doubly stochastic, meaning that both its rows and columns sum to $1$. If $P$ is doubly stochastic, and $\\mathbf{1}$ is a column vector of all ones, then $\\mathbf{1}^T P = \\mathbf{1}^T$ (since the columns of $P$ sum to $1$). Taking $\\pi = (1/N)\\mathbf{1}^T$, we have $\\pi P = ((1/N)\\mathbf{1}^T)P = (1/N)(\\mathbf{1}^T P) = (1/N)\\mathbf{1}^T = \\pi$.\n\n1.  **Kernel $P_{\\mathrm{rev}}$**: The transition probabilities from state $i$ are $(P_{\\mathrm{rev}})_{i,i} = \\ell$, $(P_{\\mathrm{rev}})_{i,(i+1)\\bmod N} = (1-\\ell)/2$, and $(P_{\\mathrm{rev}})_{i,(i-1)\\bmod N} = (1-\\ell)/2$. The matrix is a symmetric circulant matrix. The sum of each row is $\\ell + (1-\\ell)/2 + (1-\\ell)/2 = 1$. Since the matrix is symmetric, the sum of each column is also $1$. Thus, $P_{\\mathrm{rev}}$ is doubly stochastic, and the uniform distribution $\\pi = (1/N, \\dots, 1/N)$ is its unique stationary distribution.\n\n    A kernel $P$ is reversible with respect to a distribution $\\pi$ if it satisfies the detailed balance condition: $\\pi_i P_{ij} = \\pi_j P_{ji}$ for all states $i, j$. With the uniform distribution $\\pi_i=\\pi_j=1/N$, this condition simplifies to $P_{ij} = P_{ji}$. The matrix $P_{\\mathrm{rev}}$ is symmetric by construction: $(P_{\\mathrm{rev}})_{i,j} = (P_{\\mathrm{rev}})_{j,i}$. Therefore, **$P_{\\mathrm{rev}}$ is reversible with respect to the uniform distribution**.\n\n2.  **Kernel $P_{\\mathrm{nonrev}}$**: The transition probabilities from state $i$ are $(P_{\\mathrm{nonrev}})_{i,i} = \\ell$ and $(P_{\\mathrm{nonrev}})_{i,(i+1)\\bmod N} = 1-\\ell$. This is also a circulant matrix. Each row sums to $\\ell + (1-\\ell) = 1$. Because it is a circulant matrix, each column is a cyclic permutation of any other column, and thus must have the same sum. The sum of the first column is $(P_{\\mathrm{nonrev}})_{0,0} + (P_{\\mathrm{nonrev}})_{N-1,0} = \\ell + (1-\\ell) = 1$. Thus, $P_{\\mathrm{nonrev}}$ is also doubly stochastic, and the uniform distribution $\\pi = (1/N, \\dots, 1/N)$ is its unique stationary distribution.\n\n    To check reversibility, we test if $P_{\\mathrm{nonrev}}$ is symmetric. Consider the transition from $0$ to $1$: $(P_{\\mathrm{nonrev}})_{0,1} = 1-\\ell$. The reverse transition is from $1$ to $0$: $(P_{\\mathrm{nonrev}})_{1,0} = 0$, assuming $N  2$. Since $\\ell \\in (0,1)$, we have $1-\\ell \\neq 0$. Thus, $(P_{\\mathrm{nonrev}})_{0,1} \\ne (P_{\\mathrm{nonrev}})_{1,0}$. The matrix is not symmetric. Therefore, **$P_{\\mathrm{nonrev}}$ is not reversible with respect to the uniform distribution**.\n\n### Task 2: Eigenvalue Properties\n\n1.  **Eigenvalues in the Unit Disk**: For any stochastic matrix $P$, all its eigenvalues $\\lambda$ must satisfy $|\\lambda| \\le 1$. Let $v$ be an eigenvector corresponding to $\\lambda$. Let $k$ be an index such that $|v_k| = \\max_i |v_i|$. From the eigenvector equation $\\lambda v = Pv$, we have $\\lambda v_k = \\sum_{j=0}^{N-1} P_{kj} v_j$. By the triangle inequality, $|\\lambda||v_k| = |\\sum_j P_{kj} v_j| \\le \\sum_j |P_{kj} v_j| = \\sum_j P_{kj} |v_j|$. Since $|v_j| \\le |v_k|$ for all $j$, we have $\\sum_j P_{kj} |v_j| \\le \\sum_j P_{kj} |v_k| = |v_k| \\sum_j P_{kj}$. As $P$ is a stochastic matrix, its rows sum to $1$, so $\\sum_j P_{kj} = 1$. Combining these, we get $|\\lambda||v_k| \\le |v_k|$. Since $v$ is an eigenvector, $v \\ne 0$, so $|v_k|  0$, and we can divide by $|v_k|$ to obtain $|\\lambda| \\le 1$.\n\n2.  **Existence of Eigenvalue $\\lambda=1$**: For any stochastic matrix $P$, the vector of all ones, $\\mathbf{1} = (1, 1, \\dots, 1)^T$, is a right eigenvector with eigenvalue $1$. This is because $(P\\mathbf{1})_i = \\sum_j P_{ij} \\cdot 1 = 1$, which is the $i$-th component of $1 \\cdot \\mathbf{1}$.\n\n3.  **Well-Posedness of Spectral Gap ($\\rho(P)  1$)**: A finite, irreducible Markov chain is aperiodic if and only if its eigenvalue $\\lambda=1$ is simple and all other eigenvalues $\\lambda'$ satisfy $|\\lambda'|  1$. A chain is aperiodic if for any state $i$, the greatest common divisor of the set of times $\\{ t0 : (P^t)_{ii}  0 \\}$ is $1$. For both $P_{\\mathrm{rev}}$ and $P_{\\mathrm{nonrev}}$, the laziness parameter $\\ell \\in (0,1)$ ensures that $(P)_{ii} = \\ell  0$. This means a transition of length $t=1$ from state $i$ to itself is possible. Thus, the gcd is $1$, and both chains are aperiodic. Both chains are also clearly irreducible (it's possible to get from any state to any other state). By the Perron-Frobenius theorem for stochastic matrices, an irreducible and aperiodic chain has a unique stationary distribution, $\\lambda=1$ is a simple eigenvalue, and all other eigenvalues have a modulus strictly less than $1$. Consequently, $\\rho(P) = \\max\\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } P, \\lambda \\ne 1\\}  1$. This confirms that the modulus spectral gap $\\delta(P) = 1 - \\rho(P)$ is well-defined and positive, and the logarithm in the definition of $t_{\\varepsilon}(P)$ is well-defined and negative.\n\n### Task 3: Algorithmic Design\n\nThe algorithm will construct the specified transition matrices and then compute their spectral properties numerically.\n\n1.  **Inputs**: An integer $N  1$, a laziness parameter $\\ell \\in (0,1)$, and a tolerance $\\varepsilon \\in (0,1)$.\n\n2.  **Matrix Construction**:\n    *   **For $P_{\\mathrm{rev}}$**: Create an $N \\times N$ matrix initialized to zeros. For each row $i \\in \\{0, \\dots, N-1\\}$, set the entries:\n        $$\n        (P_{\\mathrm{rev}})_{i,j} = \\begin{cases} \\ell  \\text{if } j=i \\\\ (1-\\ell)/2  \\text{if } j=(i+1)\\bmod N \\text{ or } j=(i-1)\\bmod N \\\\ 0  \\text{otherwise} \\end{cases}\n        $$\n        Note that the modulo for the \"minus\" case should be implemented as `(i - 1 + N) % N` to avoid negative indices.\n    *   **For $P_{\\mathrm{nonrev}}$**: Create an $N \\times N$ matrix initialized to zeros. For each row $i \\in \\{0, \\dots, N-1\\}$, set the entries:\n        $$\n        (P_{\\mathrm{nonrev}})_{i,j} = \\begin{cases} \\ell  \\text{if } j=i \\\\ 1-\\ell  \\text{if } j=(i+1)\\bmod N \\\\ 0  \\text{otherwise} \\end{cases}\n        $$\n\n3.  **Spectral Computation** (for each matrix $P \\in \\{P_{\\mathrm{rev}}, P_{\\mathrm{nonrev}}\\}$):\n    *   Use a standard numerical library (e.g., `numpy.linalg.eigvals`) to compute the full spectrum of eigenvalues, $\\{\\lambda_k\\}_{k=0}^{N-1}$.\n    *   Identify the trivial eigenvalue $\\lambda=1$. Due to numerical precision, this is done by finding the eigenvalue $\\lambda_k$ that is closest to $1$, i.e., finding $k^*$ that minimizes $|\\lambda_k - 1|$.\n    *   Remove this eigenvalue from the set to form a new set of $N-1$ eigenvalues.\n    *   Calculate $\\rho(P)$ by finding the maximum absolute value (modulus) of the eigenvalues in this new set:\n        $$ \\rho(P) = \\max_{k \\ne k^*} \\{|\\lambda_k|\\} $$\n    *   Compute the modulus spectral gap:\n        $$ \\delta(P) = 1 - \\rho(P) $$\n    *   Compute the proxy mixing time. This value must be an integer, so the ceiling function is applied:\n        $$ t_{\\varepsilon}(P) = \\left\\lceil \\frac{\\log(\\varepsilon)}{\\log(\\rho(P))} \\right\\rceil $$\n\n4.  **Final Comparison and Output**:\n    *   After computing $t_{\\varepsilon}(P_{\\mathrm{rev}})$ and $t_{\\varepsilon}(P_{\\mathrm{nonrev}})$, evaluate the indicator function $\\mathbf{1}\\{t_{\\varepsilon}(P_{\\mathrm{nonrev}})  t_{\\varepsilon}(P_{\\mathrm{rev}})\\}$, which is $1$ if the condition is true and $0$ otherwise.\n    *   Assemble the five final values for a given test case into a list: $[\\delta(P_{\\mathrm{rev}}), \\delta(P_{\\mathrm{nonrev}}), t_{\\varepsilon}(P_{\\mathrm{rev}}), t_{\\varepsilon}(P_{\\mathrm{nonrev}}), \\mathbf{1}\\{\\cdot\\}]$.\n    *   The floating-point values are to be rounded to six decimal places. The mixing times and indicator function are integers.\n\nThis algorithm provides a robust numerical path to the solution, directly implementing the definitions provided in the problem statement.\n\nFor completeness, we can state the analytical eigenvalues, which can be derived because the matrices are circulant. Let $\\omega_N = e^{2\\pi i / N}$. The eigenvalues for a circulant matrix with first row $(c_0, c_1, \\dots, c_{N-1})$ are $\\lambda_j = \\sum_{k=0}^{N-1} c_k \\omega_N^{jk}$ for $j=0, \\dots, N-1$.\n\n*   For $P_{\\mathrm{rev}}$, the eigenvalues are real: $\\lambda_j = \\ell + (1-\\ell)\\cos(2\\pi j/N)$.\n*   For $P_{\\mathrm{nonrev}}$, the eigenvalues are complex: $\\lambda_j = \\ell + (1-\\ell)e^{i 2\\pi j/N}$.\n\nIn both cases, $j=0$ gives the eigenvalue $\\lambda_0=1$. The second largest modulus $\\rho(P)$ is obtained at $j=1$ and $j=N-1$.\n*   $\\rho(P_{\\mathrm{rev}}) = \\max_{j=1,\\dots,N-1} |\\ell + (1-\\ell)\\cos(2\\pi j/N)|$.\n*   $\\rho(P_{\\mathrm{nonrev}}) = |\\ell + (1-\\ell)e^{i 2\\pi /N}| = \\sqrt{(\\ell+(1-\\ell)\\cos(2\\pi/N))^2 + ((1-\\ell)\\sin(2\\pi/N))^2}$.\n\nThese analytical forms are consistent with the numerical approach and provide a good theoretical foundation for the expected results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Markov chain analysis problem for a suite of test cases.\n    For each case, it constructs reversible and non-reversible transition\n    matrices, computes their spectral gaps and proxy mixing times, and\n    compares the mixing times.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, ell, epsilon)\n        (10, 0.2, 1e-4),  # Case A\n        (3, 0.3, 1e-4),   # Case B\n        (50, 0.05, 1e-4), # Case C\n        (15, 0.9, 1e-4)   # Case D\n    ]\n\n    all_results = []\n    for N, ell, epsilon in test_cases:\n        # 1. Construct the transition matrices\n\n        # Reversible kernel P_rev\n        P_rev = np.zeros((N, N))\n        p_move = (1 - ell) / 2\n        for i in range(N):\n            P_rev[i, i] = ell\n            P_rev[i, (i + 1) % N] = p_move\n            P_rev[i, (i - 1 + N) % N] = p_move\n\n        # Non-reversible kernel P_nonrev\n        P_nonrev = np.zeros((N, N))\n        p_forward = 1 - ell\n        for i in range(N):\n            P_nonrev[i, i] = ell\n            P_nonrev[i, (i + 1) % N] = p_forward\n\n        # 2. Compute eigenvalues and spectral properties for P_rev\n        eigs_rev = np.linalg.eigvals(P_rev)\n        # Identify and remove the eigenvalue closest to 1\n        idx_1_rev = np.argmin(np.abs(eigs_rev - 1.0))\n        other_eigs_rev = np.delete(eigs_rev, idx_1_rev)\n        # Compute rho(P) = max(|lambda|) for lambda != 1\n        rho_rev = np.max(np.abs(other_eigs_rev))\n        # Compute spectral gap delta(P) = 1 - rho(P)\n        delta_rev = 1 - rho_rev\n        # Compute mixing time t_epsilon\n        # Using int(np.ceil(...)) to ensure integer output\n        t_eps_rev = int(np.ceil(np.log(epsilon) / np.log(rho_rev)))\n\n        # 3. Compute eigenvalues and spectral properties for P_nonrev\n        eigs_nonrev = np.linalg.eigvals(P_nonrev)\n        # Identify and remove the eigenvalue closest to 1\n        idx_1_nonrev = np.argmin(np.abs(eigs_nonrev - 1.0))\n        other_eigs_nonrev = np.delete(eigs_nonrev, idx_1_nonrev)\n        # Compute rho(P) = max(|lambda|) for lambda != 1\n        rho_nonrev = np.max(np.abs(other_eigs_nonrev))\n        # Compute spectral gap delta(P) = 1 - rho(P)\n        delta_nonrev = 1 - rho_nonrev\n        # Compute mixing time t_epsilon\n        t_eps_nonrev = int(np.ceil(np.log(epsilon) / np.log(rho_nonrev)))\n        \n        # 4. Compare mixing times and create the indicator\n        indicator = 1 if t_eps_nonrev  t_eps_rev else 0\n\n        # 5. Assemble and format the results for the current case\n        case_result = [\n            delta_rev,\n            delta_nonrev,\n            t_eps_rev,\n            t_eps_nonrev,\n            indicator\n        ]\n        all_results.append(case_result)\n\n    # 6. Final print statement in the exact required format.\n    # The output is a list of lists, with floats rounded to 6 decimal places.\n    output_str = []\n    for res in all_results:\n        formatted_res = []\n        # res[0]: delta_rev (float)\n        # res[1]: delta_nonrev (float)\n        # res[2]: t_eps_rev (int)\n        # res[3]: t_eps_nonrev (int)\n        # res[4]: indicator (int)\n        formatted_res.append(f\"{res[0]:.6f}\")\n        formatted_res.append(f\"{res[1]:.6f}\")\n        formatted_res.append(str(res[2]))\n        formatted_res.append(str(res[3]))\n        formatted_res.append(str(res[4]))\n        output_str.append(f\"[{','.join(formatted_res)}]\")\n        \n    print(f\"[{','.join(output_str)}]\")\n\nsolve()\n\n```", "id": "3347133"}]}