{"hands_on_practices": [{"introduction": "The cornerstone of proving geometric ergodicity is often the Foster-Lyapunov drift condition, which mathematically describes how a Markov chain is systematically drawn towards a central part of its state space. This exercise provides direct practice in this fundamental technique by asking you to construct a suitable quadratic Lyapunov function for a mixture kernel and explicitly verify the drift inequality [@problem_id:3347148]. Mastering this skill is essential for analyzing the stability and convergence of a wide range of stochastic processes.", "problem": "Consider a time-homogeneous Markov chain on the real line with transition kernel defined as follows. Given the current state $x \\in \\mathbb{R}$, the next state $X'$ is generated by a mixture mechanism:\n- with probability $\\delta \\in (0,1)$, $X'$ is drawn independently from a Gaussian distribution with mean $0$ and variance $\\sigma_{0}^{2} > 0$, i.e., $X' \\sim \\mathcal{N}(0,\\sigma_{0}^{2})$;\n- with probability $1-\\delta$, $X'$ is generated by the autoregressive update $X' = \\rho x + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ with $\\sigma^{2} > 0$, and $\\rho \\in \\mathbb{R}$ satisfies $|\\rho|  1$.\n\nThis kernel commonly arises in Stochastic Simulation and Markov Chain Monte Carlo (MCMC), for example when periodically refreshing from a tractable reference distribution while otherwise following a stable linear Gaussian dynamic.\n\nTasks:\n1. Starting from the foundational definitions of Markov chains and invariant distributions, state the Foster–Lyapunov drift condition that ensures the existence of a stationary (invariant) distribution and geometric ergodicity for a $\\psi$-irreducible, aperiodic Markov chain.\n2. For the given kernel, construct a Lyapunov function $V:\\mathbb{R}\\to[1,\\infty)$ and verify the drift inequality by explicitly computing $\\mathbb{E}[V(X')\\mid X=x]$ and showing it can be bounded in the Foster–Lyapunov form with appropriate constants and a suitable small set. You should justify the choice of $V$ from first principles by analyzing the one-step expectation under the transition mechanism and demonstrating contraction away from a petite set.\n\nYour final answer must be a single closed-form analytic expression for a Lyapunov function $V(x)$ that satisfies the drift condition for the specified kernel. No numerical rounding is required. Express your final answer solely as a function of $x$.", "solution": "Part 1: The Foster-Lyapunov Drift Condition\n\nA time-homogeneous Markov chain is a sequence of random variables $\\{X_n\\}_{n \\ge 0}$ taking values in a state space $\\mathcal{X}$, whose evolution is governed by a fixed transition kernel $P(x, A) = \\mathbb{P}(X_{n+1} \\in A \\mid X_n = x)$ for any measurable set $A \\subseteq \\mathcal{X}$. An invariant (or stationary) distribution is a probability measure $\\pi$ on $\\mathcal{X}$ such that if $X_n \\sim \\pi$, then $X_{n+1} \\sim \\pi$. This is equivalent to the condition $\\pi(A) = \\int_{\\mathcal{X}} P(x, A) \\pi(dx)$ for all measurable sets $A$.\n\nFor a Markov chain on a general state space, geometric ergodicity guarantees the existence of a unique stationary distribution $\\pi$ and that the distribution of $X_n$ converges to $\\pi$ at a geometric rate in total variation norm, irrespective of the initial state $X_0=x$. A sufficient condition for geometric ergodicity is provided by the Foster-Lyapunov drift criterion.\n\nFirst, some preliminary definitions are required. A Markov chain is $\\psi$-irreducible if for some measure $\\psi$ on $\\mathcal{X}$, for every $x \\in \\mathcal{X}$ and any set $A$ with $\\psi(A) > 0$, there exists an integer $n \\ge 1$ such that $P^n(x, A) > 0$. An irreducible chain is aperiodic if it does not get trapped in cycles. A measurable set $C \\subseteq \\mathcal{X}$ is called a small set (or $m$-small set) if there exists an integer $m \\ge 1$, a constant $\\epsilon > 0$, and a probability measure $\\nu$ such that for all $x \\in C$, the $m$-step transition kernel satisfies $P^m(x, \\cdot) \\ge \\epsilon \\nu(\\cdot)$. If $m=1$, the set is called one-step small. A set $C$ is petite if for some probability distribution $a$ on the non-negative integers, the kernel $K_a(x, \\cdot) = \\sum_{n=0}^{\\infty} a(n) P^n(x, \\cdot)$ satisfies $K_a(x, \\cdot) \\ge \\epsilon \\nu(\\cdot)$ for all $x \\in C$.\n\nThe Foster-Lyapunov drift condition for geometric ergodicity states that a $\\psi$-irreducible and aperiodic Markov chain is geometrically ergodic if there exists a test function (or Lyapunov function) $V: \\mathcal{X} \\to [1, \\infty)$, a petite set $C \\subset \\mathcal{X}$, and constants $\\lambda \\in [0, 1)$ and $L  \\infty$ such that the following drift inequality holds:\n$$ \\mathbb{E}[V(X_{n+1}) \\mid X_n = x] \\le \\lambda V(x) + L \\mathbb{I}_C(x) \\quad \\text{for all } x \\in \\mathcal{X} $$\nwhere $\\mathbb{I}_C(x)$ is the indicator function, which is $1$ if $x \\in C$ and $0$ otherwise. The expectation is taken with respect to the transition kernel $P(x, \\cdot)$. An equivalent, and often more practical, formulation is that for some $\\lambda' \\in [0, 1)$ and $L'  \\infty$, the condition holds for all $x$ outside the petite set $C$:\n$$ \\mathbb{E}[V(X_{n+1}) \\mid X_n = x] \\le \\lambda' V(x) + L' \\quad \\text{for all } x \\in \\mathcal{X} \\setminus C $$\n\nPart 2: Construction and Verification of a Lyapunov Function\n\nWe are given a Markov chain on $\\mathcal{X} = \\mathbb{R}$. The transition mechanism for generating the next state $X'$ from the current state $x$ is a mixture: with probability $\\delta$, $X' \\sim \\mathcal{N}(0, \\sigma_0^2)$, and with probability $1-\\delta$, $X' = \\rho x + \\varepsilon$ where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. The parameters satisfy $\\delta \\in (0,1)$, $|\\rho|  1$, $\\sigma_0^2 > 0$, and $\\sigma^2 > 0$.\n\nFirst, we establish the irreducibility and aperiodicity of the chain. The transition kernel $P(x, dy)$ has a component $\\delta \\phi(y; 0, \\sigma_0^2) dy$, where $\\phi(y; \\mu, s^2)$ is the Gaussian probability density function. Since $\\delta > 0$ and $\\phi(y; 0, \\sigma_0^2) > 0$ for all $y \\in \\mathbb{R}$, the transition probability from any $x$ to any open set is positive. This implies the chain is Lebesgue-irreducible (i.e., $\\psi$-irreducible with $\\psi$ being the Lebesgue measure) and aperiodic. Furthermore, this implies that any compact set in $\\mathbb{R}$ is a one-step small set, and therefore also petite.\n\nNext, we construct a Lyapunov function $V(x)$. The dynamics involve a linear autoregressive component, $X' = \\rho x + \\varepsilon$. The squaring operation inherent in calculating the second moment suggests that a quadratic function would be a natural candidate for a Lyapunov function, as the expectation of a quadratic function of the next state might be a quadratic function of the current state. We seek a function $V: \\mathbb{R} \\to [1, \\infty)$. The simplest quadratic function satisfying this condition is $V(x) = x^2 + 1$. We will use this as our candidate Lyapunov function.\n\nWe now compute the one-step expected value of $V(X')$ conditioned on $X=x$:\n$$ \\mathbb{E}[V(X') \\mid X=x] = \\mathbb{E}[(X')^2 + 1 \\mid X=x] = \\mathbb{E}[(X')^2 \\mid X=x] + 1 $$\nUsing the law of total expectation over the mixture components:\n$$ \\mathbb{E}[(X')^2 \\mid X=x] = \\delta \\cdot \\mathbb{E}[Y_0^2] + (1-\\delta) \\cdot \\mathbb{E}[Y_1^2 \\mid X=x] $$\nwhere $Y_0 \\sim \\mathcal{N}(0, \\sigma_0^2)$ and $Y_1 \\sim \\mathcal{N}(\\rho x, \\sigma^2)$.\nFor a random variable $Y \\sim \\mathcal{N}(\\mu, s^2)$, its second moment is $\\mathbb{E}[Y^2] = \\text{Var}(Y) + (\\mathbb{E}[Y])^2 = s^2 + \\mu^2$.\nApplying this, we get:\n- For the first component, $\\mathbb{E}[Y_0^2] = \\sigma_0^2 + 0^2 = \\sigma_0^2$.\n- For the second component, $\\mathbb{E}[Y_1^2 \\mid X=x] = \\sigma^2 + (\\rho x)^2 = \\sigma^2 + \\rho^2 x^2$.\n\nSubstituting these back into the expression for the conditional expectation of $(X')^2$:\n$$ \\mathbb{E}[(X')^2 \\mid X=x] = \\delta \\sigma_0^2 + (1-\\delta)(\\sigma^2 + \\rho^2 x^2) = (1-\\delta)\\rho^2 x^2 + \\delta\\sigma_0^2 + (1-\\delta)\\sigma^2 $$\nTherefore, the expected value of our Lyapunov function is:\n$$ \\mathbb{E}[V(X') \\mid X=x] = (1-\\delta)\\rho^2 x^2 + \\delta\\sigma_0^2 + (1-\\delta)\\sigma^2 + 1 $$\nTo verify the drift condition, we express the right-hand side in terms of $V(x) = x^2+1$. We have $x^2 = V(x) - 1$.\n$$ \\mathbb{E}[V(X') \\mid X=x] = (1-\\delta)\\rho^2 (V(x) - 1) + \\delta\\sigma_0^2 + (1-\\delta)\\sigma^2 + 1 $$\n$$ \\mathbb{E}[V(X') \\mid X=x] = (1-\\delta)\\rho^2 V(x) + [1 - (1-\\delta)\\rho^2 + \\delta\\sigma_0^2 + (1-\\delta)\\sigma^2] $$\nThis equation is of the form $\\mathbb{E}[V(X') \\mid X=x] = \\lambda_V V(x) + b_V$, where the drift coefficient is $\\lambda_V = (1-\\delta)\\rho^2$ and the additive constant is $b_V = 1 - (1-\\delta)\\rho^2 + \\delta\\sigma_0^2 + (1-\\delta)\\sigma^2$.\n\nThe conditions on the parameters are $\\delta \\in (0,1)$ and $|\\rho|1$. This implies $0  1-\\delta  1$ and $0 \\le \\rho^2  1$. Thus, the drift coefficient satisfies $0 \\le \\lambda_V  1$. The constant $b_V$ is finite, as all parameters are finite.\nThe derived relationship\n$$ \\mathbb{E}[V(X') \\mid X=x] = \\lambda_V V(x) + b_V $$\nholds for all $x \\in \\mathbb{R}$. This is a global drift condition, which is stronger than what is required by the Foster-Lyapunov theorem. It directly satisfies the drift inequality $\\mathbb{E}[V(X') \\mid X=x] \\le \\lambda V(x) + L$ for any choice of petite set $C$, with $\\lambda = \\lambda_V$ and $L=b_V$. The contraction towards the center of the state space is guaranteed by $\\lambda_V  1$. This confirms that $V(x) = x^2+1$ is a valid Lyapunov function for the given Markov chain, ensuring its geometric ergodicity and the existence of a unique stationary distribution.", "answer": "$$\n\\boxed{x^2 + 1}\n$$", "id": "3347148"}, {"introduction": "While a drift condition ensures a chain frequently returns to a central set $C$, geometric ergodicity also requires the chain to \"reset\" itself from within that set. This is formalized by the minorization condition, which guarantees that from any point in a \"small set\" $C$, the chain can transition to a common region with a uniform minimum probability. This practice challenges you to work directly from the definition of a small set to construct the minorizing measure and find the optimal minorization constant, solidifying your understanding of this crucial concept [@problem_id:3310306].", "problem": "Consider a time-homogeneous Markov transition kernel $P$ on $\\mathbb{R}$ with density $p(x,y)$ with respect to the Lebesgue measure, defined for a fixed $\\lambda \\in (0,1)$ by\n$$\np(x,y) \\;=\\; \\lambda \\cdot \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y) \\;+\\; (1-\\lambda)\\cdot \\frac{1}{2}\\,\\exp\\!\\big(-|y-x|\\big),\n$$\nwhere $\\mathbf{1}_{[-1,1]}$ is the indicator function of the interval $[-1,1]$. Let $C = [-2,2]$. The notions of Markov transition kernel and small set are understood in the standard way: a measurable set $C$ is called $1$-small if there exist a constant $\\epsilon \\in (0,1]$ and a probability measure $\\nu$ on $(\\mathbb{R},\\mathcal{B}(\\mathbb{R}))$ such that, for all $x \\in C$ and all measurable $A \\subset \\mathbb{R}$, one has $P(x,A) \\ge \\epsilon\\,\\nu(A)$.\n\nStarting from first principles and using only the definitions of a Markov transition kernel, the indicator function, and the small set (minorization) condition, construct an explicit probability measure $\\nu$ and a constant $\\epsilon$ that verify the small set condition on $C$. Among all such pairs $(\\epsilon,\\nu)$ with $\\nu$ a probability measure, determine the largest possible value of $\\epsilon$ as a closed-form analytic expression in terms of $\\lambda$.\n\nYour final answer must be this maximal $\\epsilon$ expressed in simplest exact form. No rounding is required. Do not include units.", "solution": "We begin with the definition of a Markov transition kernel $P$ on $\\mathbb{R}$ with density $p(x,y)$ with respect to the Lebesgue measure, so that for each $x \\in \\mathbb{R}$ and measurable $A \\subset \\mathbb{R}$,\n$$\nP(x,A) \\;=\\; \\int_{A} p(x,y)\\,\\mathrm{d}y.\n$$\nA set $C \\subset \\mathbb{R}$ is called $1$-small if there exist $\\epsilon \\in (0,1]$ and a probability measure $\\nu$ on $\\mathbb{R}$ such that, for all $x \\in C$ and all measurable $A \\subset \\mathbb{R}$,\n$$\nP(x,A) \\;\\ge\\; \\epsilon\\,\\nu(A).\n$$\nEquivalently, writing $\\nu$ in terms of its density $v(y)$ with respect to the Lebesgue measure (when it exists), the minorization condition is verified if one can find a nonnegative function $g(y)$ and a constant $\\epsilon$ such that, for all $x \\in C$ and almost every $y \\in \\mathbb{R}$,\n$$\np(x,y) \\;\\ge\\; g(y), \\qquad \\text{and} \\qquad \\epsilon \\;=\\; \\int_{\\mathbb{R}} g(y)\\,\\mathrm{d}y, \\qquad v(y) \\;=\\; \\frac{g(y)}{\\epsilon}.\n$$\nIndeed, if $p(x,y) \\ge g(y)$ for all $x \\in C$, then for all measurable $A$,\n$$\nP(x,A) \\;=\\; \\int_{A} p(x,y)\\,\\mathrm{d}y \\;\\ge\\; \\int_{A} g(y)\\,\\mathrm{d}y \\;=\\; \\epsilon \\int_{A} \\frac{g(y)}{\\epsilon}\\,\\mathrm{d}y \\;=\\; \\epsilon\\,\\nu(A),\n$$\nwith $\\nu(\\mathrm{d}y) = v(y)\\,\\mathrm{d}y = \\frac{g(y)}{\\epsilon}\\,\\mathrm{d}y$. By construction, $\\nu$ is a probability measure because\n$$\n\\int_{\\mathbb{R}} \\nu(\\mathrm{d}y) \\;=\\; \\int_{\\mathbb{R}} \\frac{g(y)}{\\epsilon}\\,\\mathrm{d}y \\;=\\; \\frac{1}{\\epsilon} \\int_{\\mathbb{R}} g(y)\\,\\mathrm{d}y \\;=\\; 1.\n$$\n\nWe now apply this principle to the given density\n$$\np(x,y) \\;=\\; \\lambda \\cdot \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y) \\;+\\; (1-\\lambda)\\cdot \\frac{1}{2}\\,\\exp\\!\\big(-|y-x|\\big),\n$$\nwith $C = [-2,2]$. For fixed $y \\in \\mathbb{R}$, we compute the pointwise lower envelope over $x \\in C$:\n$$\ng(y) \\;:=\\; \\inf_{x \\in C} p(x,y).\n$$\nWe handle the two mixture components separately. The first component does not depend on $x$ and contributes\n$$\n\\lambda \\cdot \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y)\n$$\nto $g(y)$ directly. For the second component, we minimize $\\frac{1}{2}\\exp(-|y-x|)$ over $x \\in [-2,2]$. Since $x \\mapsto \\exp(-|y-x|)$ is strictly decreasing in $|y-x|$, the infimum over $x \\in [-2,2]$ is attained at the point in $[-2,2]$ that maximizes $|y-x|$. The maximum distance from $y$ to the interval $[-2,2]$ is\n$$\n\\max_{x \\in [-2,2]} |y-x| \\;=\\; 2 + |y|.\n$$\nTherefore,\n$$\n\\inf_{x \\in C} \\frac{1}{2}\\exp\\!\\big(-|y-x|\\big) \\;=\\; \\frac{1}{2}\\,\\exp\\!\\big(-(2+|y|)\\big).\n$$\nCombining the two components, we obtain the common lower bound\n$$\ng(y) \\;=\\; \\lambda \\cdot \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y) \\;+\\; (1-\\lambda)\\cdot \\frac{1}{2}\\,\\exp\\!\\big(-(2+|y|)\\big),\n$$\nwhich satisfies $p(x,y) \\ge g(y)$ for all $x \\in C$ and all $y \\in \\mathbb{R}$.\n\nWe now compute\n$$\n\\epsilon \\;=\\; \\int_{\\mathbb{R}} g(y)\\,\\mathrm{d}y \\;=\\; \\lambda \\int_{\\mathbb{R}} \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y)\\,\\mathrm{d}y \\;+\\; (1-\\lambda) \\int_{\\mathbb{R}} \\frac{1}{2}\\,\\exp\\!\\big(-(2+|y|)\\big)\\,\\mathrm{d}y.\n$$\nThe first integral equals\n$$\n\\int_{\\mathbb{R}} \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y)\\,\\mathrm{d}y \\;=\\; \\frac{1}{2} \\cdot 2 \\;=\\; 1.\n$$\nFor the second integral, factor out $\\exp(-2)$ and use the symmetry of the Laplace density:\n$$\n\\int_{\\mathbb{R}} \\frac{1}{2}\\,\\exp\\!\\big(-(2+|y|)\\big)\\,\\mathrm{d}y \\;=\\; \\exp(-2)\\int_{\\mathbb{R}} \\frac{1}{2}\\,\\exp(-|y|)\\,\\mathrm{d}y \\;=\\; \\exp(-2)\\cdot 1 \\;=\\; \\exp(-2).\n$$\nTherefore,\n$$\n\\epsilon \\;=\\; \\lambda \\cdot 1 \\;+\\; (1-\\lambda)\\cdot \\exp(-2) \\;=\\; \\lambda \\;+\\; (1-\\lambda)\\,\\exp(-2).\n$$\n\nDefine the probability measure $\\nu$ by its density\n$$\nv(y) \\;=\\; \\frac{g(y)}{\\epsilon} \\;=\\; \\frac{\\lambda \\cdot \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y) \\;+\\; (1-\\lambda)\\cdot \\frac{1}{2}\\,\\exp\\!\\big(-(2+|y|)\\big)}{\\lambda \\;+\\; (1-\\lambda)\\,\\exp(-2)}.\n$$\nThen, for all $x \\in C$ and all measurable $A \\subset \\mathbb{R}$,\n$$\nP(x,A) \\;=\\; \\int_{A} p(x,y)\\,\\mathrm{d}y \\;\\ge\\; \\int_{A} g(y)\\,\\mathrm{d}y \\;=\\; \\epsilon \\int_{A} v(y)\\,\\mathrm{d}y \\;=\\; \\epsilon\\,\\nu(A),\n$$\nverifying that $C$ is $1$-small with this explicit $\\nu$ and $\\epsilon$.\n\nIt remains to show that this $\\epsilon$ is maximal over all probability measures $\\nu$. Suppose there exists a probability measure $\\tilde{\\nu}$ and a constant $\\tilde{\\epsilon}$ such that, for all $x \\in C$ and all measurable $A$,\n$$\nP(x,A) \\;\\ge\\; \\tilde{\\epsilon}\\,\\tilde{\\nu}(A).\n$$\nIf $\\tilde{\\nu}$ has density $\\tilde{v}(y)$ with respect to the Lebesgue measure, then $p(x,y) \\ge \\tilde{\\epsilon}\\,\\tilde{v}(y)$ for all $x \\in C$ and almost every $y$. Taking the infimum over $x \\in C$ yields $g(y) \\ge \\tilde{\\epsilon}\\,\\tilde{v}(y)$ almost everywhere. Integrating both sides over $\\mathbb{R}$ gives\n$$\n\\epsilon \\;=\\; \\int_{\\mathbb{R}} g(y)\\,\\mathrm{d}y \\;\\ge\\; \\tilde{\\epsilon} \\int_{\\mathbb{R}} \\tilde{v}(y)\\,\\mathrm{d}y \\;=\\; \\tilde{\\epsilon},\n$$\nbecause $\\tilde{\\nu}$ is a probability measure. Hence no $\\tilde{\\epsilon}$ larger than $\\epsilon$ is possible, and the value\n$$\n\\epsilon \\;=\\; \\lambda \\;+\\; (1-\\lambda)\\,\\exp(-2)\n$$\nis the largest constant for which the small set minorization on $C$ holds.\n\nThus an explicit verifying pair is\n$$\n\\nu(\\mathrm{d}y) \\;=\\; \\frac{\\lambda \\cdot \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y) \\;+\\; (1-\\lambda)\\cdot \\frac{1}{2}\\,\\exp\\!\\big(-(2+|y|)\\big)}{\\lambda \\;+\\; (1-\\lambda)\\,\\exp(-2)}\\,\\mathrm{d}y,\n$$\nand\n$$\n\\epsilon \\;=\\; \\lambda \\;+\\; (1-\\lambda)\\,\\exp(-2),\n$$\nwith this $\\epsilon$ being maximal.", "answer": "$$\\boxed{\\lambda+(1-\\lambda)\\exp(-2)}$$", "id": "3310306"}, {"introduction": "The theoretical constants from drift and minorization conditions have profound practical implications, directly influencing the convergence speed of algorithms like MCMC. This exercise bridges theory and practice by analyzing the Random-Walk Metropolis algorithm, a workhorse of computational statistics. You will derive how the minorization constant depends on the algorithm's proposal scaling and then use this relationship to optimize its performance, demonstrating how theoretical analysis can guide practical algorithm design [@problem_id:3310303].", "problem": "Consider a one-dimensional Random-Walk Metropolis (RWM) algorithm, a special case of Markov Chain Monte Carlo (MCMC), targeting the standard normal distribution with density $\\pi(x) \\propto \\exp(-x^{2}/2)$. The proposal is $Y = X + Z$ where $Z \\sim \\mathcal{N}(0, s^{2})$ with proposal variance $s^{2}  0$, and the Metropolis acceptance probability is $\\alpha(x,y) = \\min\\{1, \\exp(-(y^{2} - x^{2})/2)\\}$.\n\nLet the drift function be $V(x) = 1 + x^{2}$ and take the small set $C = [-r, r]$ with fixed constants $r  0$ and $\\delta \\in (0, r)$. The geometric ergodicity analysis proceeds via a drift condition $P V(x) \\leq \\lambda V(x) + b \\mathbf{1}_{C}(x)$ with $\\lambda \\in (0,1)$ and a small-set minorization condition $P(x, A) \\geq \\epsilon \\nu(A)$ for all measurable sets $A$ and all $x \\in C$, where $P$ is the one-step transition kernel, $\\epsilon \\in (0,1)$ is the minorization constant, and $\\nu$ is a fixed probability measure supported on a subset of $\\mathbb{R}$.\n\nWork under the following construction for the small-set minorization: for $x \\in C$, minorize $P(x, \\cdot)$ against the uniform probability measure $\\nu$ on the interval $[-\\delta, \\delta]$. Starting from the core definitions above, derive an explicit lower bound for the minorization constant $\\epsilon$ as a function of $s$, $r$, and $\\delta$. Then, consider the geometric rate bound in terms of the contraction factor governed by the combination of drift and minorization, and, in a regime where the drift contraction is not the dominant bottleneck, optimize the proposal scale $s$ to maximize the derived minorization lower bound.\n\nYour tasks are:\n\n1. Derive, from first principles, a rigorous lower bound expression for the minorization constant $\\epsilon(s)$ obtained by restricting the minorization to the interval $[-\\delta, \\delta]$ with uniform $\\nu$.\n2. Based on the derived $\\epsilon(s)$, determine the analytic expression for the value $s^{\\star}$ of the proposal scale $s$ that maximizes the lower bound $\\epsilon(s)$, thereby improving the geometric rate bound dominated by the small-set step.\n3. Express your final answer for $s^{\\star}$ as a single closed-form analytic expression. No rounding is required.\n\nEnsure all steps in your derivation are scientifically sound and fully justified from the accepted definitions of the RWM kernel, drift conditions, and small-set minorization. Do not invoke any shortcut formulas for geometric ergodicity; instead, build the argument from the definitions of $\\alpha(x,y)$, $P(x, A)$, and the construction of the lower bound for $\\epsilon$ under the chosen $\\nu$ and $C$.", "solution": "The problem requires the derivation of a lower bound for the minorization constant $\\epsilon$ for a Random-Walk Metropolis (RWM) algorithm and the optimization of this bound with respect to the proposal scale $s$.\n\nThe one-step transition kernel of the Markov chain is given by\n$$ P(x, A) = \\int_A \\alpha(x, y) q(y|x) dy + \\left(1 - \\int_{\\mathbb{R}} \\alpha(x, y) q(y|x) dy\\right) \\delta_x(A) $$\nfor any measurable set $A \\subseteq \\mathbb{R}$. The term $k(x, y) = \\alpha(x, y) q(y|x)$ is the density of the accepted proposal component of the kernel. The second term corresponds to the rejection event, where the chain remains at $x$.\n\nThe minorization condition is stated as $P(x, A) \\geq \\epsilon \\nu(A)$ for all $x \\in C = [-r, r]$ and all measurable sets $A$. The measure $\\nu$ is the uniform probability measure on the interval $[-\\delta, \\delta]$, with density $\\nu(y) = \\frac{1}{2\\delta} \\mathbf{1}_{[-\\delta, \\delta]}(y)$.\n\nTo establish this condition, we seek a lower bound on the density part of the kernel for all $x \\in C$ and for all points $y$ in the support of $\\nu$. Let's consider a set $A \\subseteq [-\\delta, \\delta]$. The rejection part of the kernel $P(x,A)$ contributes only if $x \\in A$. Since we need a bound for all $x \\in C = [-r, r]$ and it is given that $\\delta  r$, it's possible that $x \\notin [-\\delta, \\delta]$. Therefore, we must establish the minorization condition based solely on the continuous part of the kernel.\n$$ P(x, A) \\geq \\int_A k(x, y) dy $$\nWe need to find an $\\epsilon  0$ such that for all $x \\in C$ and $A \\subseteq [-\\delta, \\delta]$,\n$$ \\int_A k(x, y) dy \\geq \\epsilon \\nu(A) = \\epsilon \\frac{\\text{Leb}(A)}{2\\delta} $$\nwhere $\\text{Leb}(A)$ is the Lebesgue measure of $A$.\nThis inequality is satisfied if we can find a constant $c  0$ such that $k(x, y) \\geq c$ for all $x \\in C = [-r, r]$ and for all $y \\in [-\\delta, \\delta]$. If such a $c$ exists, then\n$$ \\int_A k(x, y) dy \\geq \\int_A c dy = c \\cdot \\text{Leb}(A) $$\nComparing with the requirement, we can set $c \\cdot \\text{Leb}(A) = \\epsilon \\frac{\\text{Leb}(A)}{2\\delta}$, which yields $\\epsilon = 2\\delta c$.\nOur task is to find a lower bound $c$ for $k(x, y)$ on the domain $x \\in [-r, r]$ and $y \\in [-\\delta, \\delta]$.\n$$ c = \\inf_{x \\in [-r, r], y \\in [-\\delta, \\delta]} k(x, y) = \\inf_{x \\in [-r, r], y \\in [-\\delta, \\delta]} \\alpha(x, y) q(y|x) $$\nTo derive a tractable lower bound, we use the property that for non-negative functions $f$ and $g$, $\\inf(f \\cdot g) \\geq (\\inf f) \\cdot (\\inf g)$. Both $\\alpha(x,y)$ and $q(y|x)$ are non-negative. This provides a valid, though not necessarily the tightest, lower bound.\n\n**Task 1: Derive a lower bound expression for $\\epsilon(s)$**\n\nFirst, let's find the infimum of the proposal density $q(y|x)$ over the specified domain.\n$$ q(y|x) = \\frac{1}{\\sqrt{2\\pi}s} \\exp\\left(-\\frac{(y-x)^2}{2s^2}\\right) $$\nTo find the infimum of $q(y|x)$, we must find the maximum of the term $(y-x)^2$ for $x \\in [-r, r]$ and $y \\in [-\\delta, \\delta]$. The distance $|y-x|$ is maximized when $x$ and $y$ are at the opposite endpoints of their respective intervals. Since $r  \\delta$, the options are $x=r, y=-\\delta$ or $x=-r, y=\\delta$. In both cases, the squared distance is $(r+\\delta)^2$.\n$$ \\sup_{x \\in [-r, r], y \\in [-\\delta, \\delta]} (y-x)^2 = (r+\\delta)^2 $$\nTherefore, the infimum of the proposal density on this domain is:\n$$ \\inf_{x,y} q(y|x) = \\frac{1}{\\sqrt{2\\pi}s} \\exp\\left(-\\frac{(r+\\delta)^2}{2s^2}\\right) $$\nNext, we find the infimum of the acceptance probability $\\alpha(x, y)$ over the same domain.\n$$ \\alpha(x, y) = \\min\\left\\{1, \\exp\\left(-\\frac{y^2 - x^2}{2}\\right)\\right\\} $$\nTo find the infimum of $\\alpha(x, y)$, we must maximize the term $y^2 - x^2$, as this will minimize the exponential term $\\exp(-(y^2-x^2)/2)$. To maximize $y^2 - x^2$ for $x \\in [-r, r]$ and $y \\in [-\\delta, \\delta]$, we maximize $y^2$ and minimize $x^2$.\nThe maximum value of $y^2$ on $[-\\delta, \\delta]$ is $\\delta^2$. The minimum value of $x^2$ on $[-r, r]$ is $0$. Thus,\n$$ \\sup_{x,y} (y^2 - x^2) = \\delta^2 - 0 = \\delta^2 $$\nSince $\\delta  0$, we have $\\exp(-\\delta^2/2)  1$. The infimum of the acceptance probability is:\n$$ \\inf_{x,y} \\alpha(x, y) = \\exp\\left(-\\frac{\\delta^2}{2}\\right) $$\nCombining these results, we establish a lower bound for $c$:\n$$ c \\geq \\left(\\inf_{x,y} q(y|x)\\right) \\left(\\inf_{x,y} \\alpha(x,y)\\right) = \\frac{1}{\\sqrt{2\\pi}s} \\exp\\left(-\\frac{(r+\\delta)^2}{2s^2}\\right) \\exp\\left(-\\frac{\\delta^2}{2}\\right) $$\nUsing $\\epsilon = 2\\delta c$, we obtain a lower bound for the minorization constant, which we denote $\\epsilon(s)$:\n$$ \\epsilon(s) = 2\\delta \\left( \\frac{1}{\\sqrt{2\\pi}s} \\exp\\left(-\\frac{(r+\\delta)^2}{2s^2}\\right) \\exp\\left(-\\frac{\\delta^2}{2}\\right) \\right) $$\nRe-arranging the terms gives the explicit lower bound:\n$$ \\epsilon(s) = \\frac{2\\delta}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{\\delta^2}{2}\\right) \\frac{1}{s} \\exp\\left(-\\frac{(r+\\delta)^2}{2s^2}\\right) $$\n\n**Task 2: Determine the optimal proposal scale $s^{\\star}$**\n\nTo improve the geometric rate bound, we want to maximize the derived lower bound $\\epsilon(s)$ with respect to the proposal scale $s  0$. Let's define a function $f(s)$ which is proportional to $\\epsilon(s)$:\n$$ f(s) = \\frac{1}{s} \\exp\\left(-\\frac{A}{s^2}\\right) $$\nwhere $A = \\frac{(r+\\delta)^2}{2}$ is a positive constant. The value of $s$ that maximizes $f(s)$ will also maximize $\\epsilon(s)$, as the other terms are positive constants with respect to $s$.\nTo find the maximum, we can optimize the natural logarithm of $f(s)$, which is a monotonic transformation.\n$$ L(s) = \\ln(f(s)) = \\ln\\left(\\frac{1}{s}\\right) + \\ln\\left(\\exp\\left(-\\frac{A}{s^2}\\right)\\right) = -\\ln(s) - \\frac{A}{s^2} $$\nWe take the derivative of $L(s)$ with respect to $s$ and set it to zero:\n$$ \\frac{dL}{ds} = -\\frac{1}{s} - A(-2s^{-3}) = -\\frac{1}{s} + \\frac{2A}{s^3} $$\nSetting the derivative to zero for $s  0$:\n$$ -\\frac{1}{s} + \\frac{2A}{s^3} = 0 \\implies \\frac{2A}{s^3} = \\frac{1}{s} $$\nMultiplying by $s^3$ (valid since $s \\neq 0$), we get:\n$$ s^2 = 2A $$\nTo confirm this is a maximum, we check the second derivative:\n$$ \\frac{d^2L}{ds^2} = \\frac{d}{ds}\\left(-s^{-1} + 2As^{-3}\\right) = s^{-2} - 6As^{-4} = \\frac{1}{s^2} - \\frac{6A}{s^4} $$\nSubstituting $s^2 = 2A$:\n$$ \\left.\\frac{d^2L}{ds^2}\\right|_{s^2=2A} = \\frac{1}{2A} - \\frac{6A}{(2A)^2} = \\frac{1}{2A} - \\frac{6A}{4A^2} = \\frac{1}{2A} - \\frac{3}{2A} = -\\frac{1}{A} $$\nSince $A = \\frac{(r+\\delta)^2}{2}  0$, the second derivative is negative, confirming that $s^2=2A$ corresponds to a local maximum. As $f(s) \\to 0$ for $s \\to 0^+$ and $s \\to \\infty$, this single critical point must be the global maximum.\n\n**Task 3: Express the final answer for $s^{\\star}$**\n\nThe optimal value for the proposal scale $s$, denoted $s^{\\star}$, is obtained by solving for $s$:\n$$ (s^{\\star})^2 = 2A = 2 \\left(\\frac{(r+\\delta)^2}{2}\\right) = (r+\\delta)^2 $$\nSince $s  0$ and $r, \\delta  0$, we take the positive square root:\n$$ s^{\\star} = r + \\delta $$\nThis is the analytic expression for the value of the proposal scale $s$ that maximizes the derived lower bound on the minorization constant $\\epsilon$.", "answer": "$$\\boxed{r+\\delta}$$", "id": "3310303"}]}