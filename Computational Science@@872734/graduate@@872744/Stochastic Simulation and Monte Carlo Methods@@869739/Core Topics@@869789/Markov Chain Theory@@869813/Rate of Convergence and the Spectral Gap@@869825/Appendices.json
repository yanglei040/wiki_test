{"hands_on_practices": [{"introduction": "Understanding the spectral gap begins with mastering the fundamental definitions. This first practice provides a concrete setting on a finite state space where you can apply these definitions directly, from verifying a stationary distribution to computing the eigenvalues of a transition matrix. By working through this foundational example [@problem_id:3335415], you will build a solid intuition for what the spectral gap represents before moving to more complex, continuous settings.", "problem": "Consider a finite-state, time-homogeneous Markov chain on the state space $\\{1,2,3,4\\}$ with transition matrix $P \\in \\mathbb{R}^{4 \\times 4}$ given by\n$$\nP \\;=\\; \\begin{pmatrix}\n\\frac{7}{10}  \\frac{1}{10}  \\frac{1}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{7}{10}  \\frac{1}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{1}{10}  \\frac{7}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{1}{10}  \\frac{1}{10}  \\frac{7}{10}\n\\end{pmatrix}.\n$$\nLet the probability vector $\\pi \\in \\mathbb{R}^{4}$ be given by $\\pi_i = \\frac{1}{4}$ for each $i \\in \\{1,2,3,4\\}$. In the sense of the $L^2(\\pi)$ operator induced by $P$, the spectral gap is defined as the difference between the largest eigenvalue and the next-largest eigenvalue of $P$ (for a reversible chain this equals $1 - \\lambda_2$ when the eigenvalues are ordered $1 = \\lambda_1 \\ge \\lambda_2 \\ge \\cdots$).\n\nStarting only from the core definitions of a stationary distribution, reversibility, and the spectral decomposition of a linear operator, do the following:\n1. Verify that $\\pi$ is a stationary distribution of $P$.\n2. Justify that the chain is reversible with respect to $\\pi$.\n3. Compute all eigenvalues of $P$ and order them in nonincreasing order to identify $\\lambda_2$.\n4. Using the definition above, compute the spectral gap $\\gamma$ of $P$.\n\nExpress the final answer for the spectral gap $\\gamma$ as an exact rational number.", "solution": "The problem requires a four-part analysis of a given Markov chain: verification of the stationary distribution, justification of reversibility, computation of the eigenvalues of the transition matrix, and calculation of the spectral gap. We will address each part in sequence, starting from the fundamental definitions as specified.\n\nThe state space is $S = \\{1, 2, 3, 4\\}$. The transition matrix is given by:\n$$\nP \\;=\\; \\begin{pmatrix}\n\\frac{7}{10}  \\frac{1}{10}  \\frac{1}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{7}{10}  \\frac{1}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{1}{10}  \\frac{7}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{1}{10}  \\frac{1}{10}  \\frac{7}{10}\n\\end{pmatrix}\n$$\nThe proposed stationary distribution is the uniform distribution $\\pi = \\begin{pmatrix} \\frac{1}{4}  \\frac{1}{4}  \\frac{1}{4}  \\frac{1}{4} \\end{pmatrix}$.\n\n**1. Verification of the Stationary Distribution**\n\nA probability distribution $\\pi$ is a stationary distribution of a Markov chain with transition matrix $P$ if it is a left eigenvector of $P$ with eigenvalue $1$. The core definition is the condition $\\pi P = \\pi$. We compute the product of the row vector $\\pi$ and the matrix $P$:\n$$\n\\pi P = \\begin{pmatrix} \\frac{1}{4}  \\frac{1}{4}  \\frac{1}{4}  \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix}\n\\frac{7}{10}  \\frac{1}{10}  \\frac{1}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{7}{10}  \\frac{1}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{1}{10}  \\frac{7}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{1}{10}  \\frac{1}{10}  \\frac{7}{10}\n\\end{pmatrix}\n$$\nThe $j$-th component of the resulting vector is given by $(\\pi P)_j = \\sum_{i=1}^{4} \\pi_i P_{ij}$. Since $\\pi_i = \\frac{1}{4}$ for all $i \\in \\{1, 2, 3, 4\\}$, this simplifies to:\n$$\n(\\pi P)_j = \\frac{1}{4} \\sum_{i=1}^{4} P_{ij}\n$$\nThis is $\\frac{1}{4}$ times the sum of the elements in the $j$-th column of $P$.\nFor $j=1$: $\\sum_{i=1}^{4} P_{i1} = \\frac{7}{10} + \\frac{1}{10} + \\frac{1}{10} + \\frac{1}{10} = \\frac{10}{10} = 1$. So, $(\\pi P)_1 = \\frac{1}{4} \\cdot 1 = \\frac{1}{4}$.\nDue to the symmetry of the matrix $P$, all column sums are identical. For any $j \\in \\{1, 2, 3, 4\\}$:\n$$\n\\sum_{i=1}^{4} P_{ij} = \\frac{7}{10} + 3 \\times \\frac{1}{10} = 1\n$$\nThus, for all $j$, we have $(\\pi P)_j = \\frac{1}{4} \\cdot 1 = \\frac{1}{4} = \\pi_j$.\nThis confirms that $\\pi P = \\pi$, and therefore $\\pi$ is a stationary distribution of the Markov chain.\n\n**2. Justification of Reversibility**\n\nA Markov chain is reversible with respect to a stationary distribution $\\pi$ if the detailed balance condition is satisfied. The definition of detailed balance is:\n$$\n\\pi_i P_{ij} = \\pi_j P_{ji} \\quad \\text{for all } i, j \\in S\n$$\nIn this problem, the stationary distribution is uniform, meaning $\\pi_i = \\frac{1}{4}$ for all $i$. Substituting this into the detailed balance equation gives:\n$$\n\\frac{1}{4} P_{ij} = \\frac{1}{4} P_{ji}\n$$\nThis equation simplifies to $P_{ij} = P_{ji}$. This condition requires the transition matrix $P$ to be symmetric. Observing the given matrix $P$, we see that it is indeed symmetric ($P = P^T$). For any pair of indices $(i, j)$, the entry $P_{ij}$ is equal to the entry $P_{ji}$. Therefore, the detailed balance condition holds, and the Markov chain is reversible with respect to $\\pi$.\n\n**3. Computation of Eigenvalues**\n\nThe eigenvalues $\\lambda$ of the matrix $P$ are the roots of the characteristic equation $\\det(P - \\lambda I) = 0$, where $I$ is the $4 \\times 4$ identity matrix. The matrix $P$ has a specific structure which simplifies this calculation. It can be expressed as a linear combination of the identity matrix $I$ and the matrix of all ones, $J$.\n$$\nP = \\frac{6}{10} I + \\frac{1}{10} J = \\frac{3}{5} I + \\frac{1}{10} J\n$$\nwhere $I$ is the identity matrix and $J$ is the $4 \\times 4$ matrix with all entries equal to $1$. The eigenvalues of an $n \\times n$ matrix of the form $aI + bJ$ are known. The matrix $J$ has two distinct eigenvalues: $n$ with multiplicity $1$ (eigenvector is a vector of all ones) and $0$ with multiplicity $n-1$ (the eigenspace is the set of vectors whose components sum to zero).\nFor an eigenvector $v$ of $J$ with eigenvalue $\\lambda_J$, we have:\n$$\n(aI + bJ)v = aIv + bJv = av + b\\lambda_J v = (a + b\\lambda_J)v\n$$\nSo, the eigenvalues of $aI + bJ$ are $a + b\\lambda_J$.\nIn our case, $n=4$, $a = \\frac{6}{10}$, and $b = \\frac{1}{10}$. The eigenvalues of $J$ are $4$ (multiplicity $1$) and $0$ (multiplicity $3$).\nThe eigenvalues of $P$ are therefore:\n- One eigenvalue corresponding to $\\lambda_J=4$:\n$$\n\\lambda_1 = a + b \\cdot 4 = \\frac{6}{10} + \\frac{1}{10} \\cdot 4 = \\frac{6+4}{10} = \\frac{10}{10} = 1\n$$\n- Three eigenvalues corresponding to $\\lambda_J=0$:\n$$\n\\lambda_{2,3,4} = a + b \\cdot 0 = \\frac{6}{10} + 0 = \\frac{6}{10}\n$$\nThe eigenvalues of $P$ are $\\{1, \\frac{6}{10}, \\frac{6}{10}, \\frac{6}{10}\\}$.\nOrdering them in nonincreasing order as $\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_3 \\ge \\lambda_4$:\n$\\lambda_1 = 1$\n$\\lambda_2 = \\frac{6}{10} = \\frac{3}{5}$\n$\\lambda_3 = \\frac{6}{10} = \\frac{3}{5}$\n$\\lambda_4 = \\frac{6}{10} = \\frac{3}{5}$\nThe second-largest eigenvalue is $\\lambda_2 = \\frac{3}{5}$.\n\n**4. Computation of the Spectral Gap**\n\nThe problem defines the spectral gap $\\gamma$ as the difference between the largest and the next-largest eigenvalue of $P$. This corresponds to $\\gamma = \\lambda_1 - \\lambda_2$.\nUsing the eigenvalues computed above:\n$\\lambda_1 = 1$\n$\\lambda_2 = \\frac{3}{5}$\nThe spectral gap $\\gamma$ is:\n$$\n\\gamma = 1 - \\frac{3}{5} = \\frac{5}{5} - \\frac{3}{5} = \\frac{2}{5}\n$$\nThe spectral gap is an exact rational number, as required.", "answer": "$$\n\\boxed{\\frac{2}{5}}\n$$", "id": "3335415"}, {"introduction": "We now transition from finite state spaces to the more common setting of continuous distributions. This exercise [@problem_id:3335418] guides you through the analysis of a workhorse algorithm, the Gibbs sampler, for a bivariate normal target. You will see how the spectral gap $\\gamma$ of the sampler's Markov operator is directly determined by the correlation $\\rho$ between the variables, elegantly demonstrating how the geometry of the target distribution dictates the algorithm's rate of convergence.", "problem": "Consider a target distribution that is a centered bivariate normal with covariance matrix $\\Sigma$,\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n\\sigma_{x}^{2}  \\rho\\,\\sigma_{x}\\sigma_{y} \\\\\n\\rho\\,\\sigma_{x}\\sigma_{y}  \\sigma_{y}^{2}\n\\end{pmatrix},\n$$\nwhere $\\sigma_{x}  0$, $\\sigma_{y}  0$, and $|\\rho|  1$. Let $(X,Y) \\sim \\mathcal{N}\\!\\left((0,0)^{\\top}, \\Sigma\\right)$ be the target law. Consider the two-coordinate systematic Gibbs sampler for Markov Chain Monte Carlo (MCMC) that, at each iteration, first updates $X$ given $Y$ from its full conditional distribution and then updates $Y$ given the newly sampled $X$ from its full conditional distribution. Let $K$ denote the one-step Markov operator acting on square-integrable functions with respect to the target, defined by\n$$\n(K f)(x,y) \\;=\\; \\mathbb{E}\\!\\left[\\, f\\!\\left(X',Y'\\right) \\,\\middle|\\, (X,Y)=(x,y) \\right],\n$$\nwhere $(X',Y')$ is the next state produced by one full Gibbs sweep. Let $L_{2}(\\pi)$ denote the Hilbert space of square-integrable functions with respect to the target density $\\pi$, and let $L_{2}^{0}(\\pi)$ be the subspace of mean-zero functions. The $L_{2}$ spectral gap is defined as\n$$\n\\gamma \\;:=\\; 1 - \\|K\\|_{L_{2}^{0}(\\pi)} \\;=\\; 1 \\;-\\; \\sup_{f \\in L_{2}^{0}(\\pi),\\, f \\neq 0} \\frac{\\|K f\\|_{2}}{\\|f\\|_{2}}.\n$$\n\nStarting from first principles, namely the full conditional distributions of the multivariate normal and the definition of the Markov operator and $L_{2}$ spectral gap, derive the induced autoregressive representation for the $Y$-marginal under one full Gibbs sweep, use it to characterize the action of $K$ on $L_{2}^{0}(\\pi)$, and compute the spectral gap $\\gamma$ in closed form as a function of $\\rho$. Express your final answer as a closed-form analytic expression in $\\rho$. No rounding is required.", "solution": "The solution requires finding the spectral gap of the systematic Gibbs sampler by analyzing its Markov operator $K$. The strategy is to determine the action of the operator on simple functions and identify the largest eigenvalue modulus on the space of mean-zero functions, $L_{2}^{0}(\\pi)$.\n\n**1. Full Conditional Distributions**\nFor a bivariate normal distribution $(X,Y) \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$, the full conditional distributions are also normal.\nThe conditional distribution of $X$ given $Y=y$ is $\\mathcal{N}(\\mu_{X|Y}, \\sigma_{X|Y}^2)$, where:\n-   Conditional mean: $\\mu_{X|Y} = \\mathbb{E}[X] + \\Sigma_{XY}\\Sigma_{YY}^{-1}(y - \\mathbb{E}[Y]) = 0 + (\\rho\\sigma_x\\sigma_y)(\\sigma_y^2)^{-1}(y) = \\rho\\frac{\\sigma_x}{\\sigma_y} y$.\n-   Conditional variance: $\\sigma_{X|Y}^2 = \\Sigma_{XX} - \\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX} = \\sigma_x^2 - (\\rho\\sigma_x\\sigma_y)(\\sigma_y^2)^{-1}(\\rho\\sigma_x\\sigma_y) = \\sigma_x^2(1-\\rho^2)$.\nSo, the sampling step for $X$ is: $X' \\sim \\mathcal{N}(\\rho\\frac{\\sigma_x}{\\sigma_y} y, \\sigma_x^2(1-\\rho^2))$.\nBy symmetry, the sampling step for $Y$ given $X=x'$ is: $Y' \\sim \\mathcal{N}(\\rho\\frac{\\sigma_y}{\\sigma_x} x', \\sigma_y^2(1-\\rho^2))$.\n\n**2. Action of the Markov Operator $K$**\nThe operator $K$ describes one full sweep of the Gibbs sampler. We can write it as a composition of two conditional expectation operators, $P_{X|Y}$ and $P_{Y|X}$.\n$(Kf)(x,y) = \\mathbb{E}[f(X', Y') | (X,Y)=(x,y)] = \\mathbb{E}_{X' \\sim p(\\cdot|y)}[\\mathbb{E}_{Y' \\sim p(\\cdot|X')}[f(X', Y')]]$.\nTo find the spectrum, we analyze the action of $K$ on a simple mean-zero function, such as $f(x,y)=y$. Since the target distribution is centered, $\\mathbb{E}_\\pi[y]=0$, so $y \\in L_{2}^{0}(\\pi)$.\n$$ (Ky)(x,y) = \\mathbb{E}[Y' | (X,Y)=(x,y)] $$\nWe compute this using iterated expectation:\n$$ (Ky)(x,y) = \\mathbb{E}_{X' \\sim p(\\cdot|y)} [ \\mathbb{E}[Y'|X'] ] $$\nFrom the conditional distribution for $Y'$, we know the inner expectation is its mean: $\\mathbb{E}[Y'|X'] = \\rho\\frac{\\sigma_y}{\\sigma_x}X'$.\nSubstituting this back, we get:\n$$ (Ky)(x,y) = \\mathbb{E}_{X' \\sim p(\\cdot|y)} \\left[ \\rho\\frac{\\sigma_y}{\\sigma_x}X' \\right] = \\rho\\frac{\\sigma_y}{\\sigma_x} \\mathbb{E}[X'|y] $$\nThe expectation $\\mathbb{E}[X'|y]$ is the mean of the conditional distribution for $X'$, which is $\\rho\\frac{\\sigma_x}{\\sigma_y}y$.\n$$ (Ky)(x,y) = \\rho\\frac{\\sigma_y}{\\sigma_x} \\left( \\rho\\frac{\\sigma_x}{\\sigma_y}y \\right) = \\rho^2 y $$\nThis shows that the function $f(x,y)=y$ is an eigenfunction of the operator $K$ with eigenvalue $\\rho^2$. Similarly, one can show that $f(x,y)=x$ is also an eigenfunction with eigenvalue $\\rho^2$.\n\n**3. Computing the Spectral Gap $\\gamma$**\nThe norm of the operator $K$ restricted to the mean-zero subspace, $\\|K\\|_{L_{2}^{0}(\\pi)}$, is the supremum of the absolute values of its eigenvalues on that subspace. We have found an eigenvalue $\\rho^2$. For the Gibbs sampler on a bivariate normal distribution, it can be shown that the eigenvalue with the largest magnitude (other than 1) is precisely $\\rho^2$.\nTherefore, $\\|K\\|_{L_{2}^{0}(\\pi)} = \\rho^2$.\nThe $L_2$ spectral gap is defined as $\\gamma = 1 - \\|K\\|_{L_{2}^{0}(\\pi)}$.\nSubstituting our result, we get:\n$$ \\gamma = 1 - \\rho^2 $$\nThis demonstrates that the convergence rate of the sampler slows down as the correlation $|\\rho|$ approaches 1, a key insight in MCMC diagnostics.", "answer": "$$\n\\boxed{1 - \\rho^2}\n$$", "id": "3335418"}, {"introduction": "This final practice delves deeper into the spectral theory of reversible processes and its connection to practical MCMC diagnostics. By analyzing the simple but powerful Autoregressive (AR(1)) model [@problem_id:3335482], you will learn how to use an orthonormal basis of functions—in this case, Hermite polynomials—to determine the operator's entire spectrum. This analysis culminates in explicitly calculating both the spectral gap $\\gamma$ and the integrated autocorrelation time $\\tau_{\\mathrm{int}}$, allowing you to verify a fundamental inequality that links these two crucial measures of sampler performance.", "problem": "Consider the Autoregressive of order one (AR(1)) Markov chain defined by the stochastic recursion $X_{n+1}=a X_{n}+\\xi_{n}$ for $n\\geq 0$, where $a\\in\\mathbb{R}$ satisfies $|a|1$ and $(\\xi_{n})_{n\\geq 0}$ is an independent and identically distributed sequence with $\\xi_{n}\\sim \\mathcal{N}(0,\\sigma_{\\xi}^{2})$. Assume the chain is initialized in its stationary distribution, so $X_{n}\\sim \\mathcal{N}(0,\\sigma^{2})$ for all $n$, with $\\sigma^{2}=\\sigma_{\\xi}^{2}/(1-a^{2})$. Let $\\pi$ denote this stationary Gaussian distribution. Define the Markov operator $P$ associated with one step of the chain by $(Pf)(x)=\\mathbb{E}[f(a x+\\xi)]$, where the expectation is taken with respect to $\\xi\\sim \\mathcal{N}(0,\\sigma_{\\xi}^{2})$ independently of $x$. The chain is reversible with respect to $\\pi$.\n\nStarting from the core definitions of reversibility, the Markov operator, and the spectral gap on $L^{2}(\\pi)$, derive the spectrum of $P$ using the orthonormal polynomial basis appropriate for $\\pi$, and compute the spectral gap $\\gamma$ (the difference between the largest eigenvalue, which is $1$, and the second largest eigenvalue in absolute value on the mean-zero subspace). Next, for the observable $f(x)=x$, define the autocorrelation function $\\rho_{k}=\\mathrm{Cov}_{\\pi}(f(X_{0}),f(X_{k}))/\\mathrm{Var}_{\\pi}(f)$ and the integrated autocorrelation time $\\tau_{\\mathrm{int}}(f)=1+2\\sum_{k=1}^{\\infty}\\rho_{k}$, and compute $\\tau_{\\mathrm{int}}(f)$ explicitly.\n\nFinally, using these quantities, verify a nonasymptotic bound that relates the spectral gap to the integrated autocorrelation time in this setting, namely that for all $a$ with $|a|1$, the integrated autocorrelation time of $f(x)=x$ satisfies $\\tau_{\\mathrm{int}}(f)\\leq \\dfrac{2}{\\gamma}-1$. Provide the final answers for $\\gamma$ and $\\tau_{\\mathrm{int}}(f)$ as closed-form analytic expressions in $a$. No numerical approximation or rounding is required.", "solution": "The solution involves three main steps: (1) finding the spectrum of the Markov operator $P$ to compute the spectral gap $\\gamma$, (2) calculating the autocorrelation function for the observable $f(x)=x$ to find the integrated autocorrelation time $\\tau_{\\mathrm{int}}(f)$, and (3) verifying the given inequality.\n\n**1. Spectrum and Spectral Gap $\\gamma$**\nThe stationary distribution $\\pi$ is a Gaussian measure $\\mathcal{N}(0, \\sigma^2)$. The standard orthonormal basis for the space $L^2(\\pi)$ is formed by the probabilists' Hermite polynomials, $p_k(x) = He_k(x/\\sigma)$. To find the spectrum of the operator $P$, we analyze its action on these basis functions.\n$(P p_k)(x) = \\mathbb{E}[p_k(ax+\\xi)] = \\mathbb{E}[He_k((ax+\\xi)/\\sigma)]$, where $\\xi \\sim \\mathcal{N}(0, \\sigma_\\xi^2)$.\n\nThe argument of the polynomial, $Z = (ax+\\xi)/\\sigma$, is a Gaussian random variable. Its mean is $\\mathbb{E}[Z] = ax/\\sigma$. Its variance is $\\mathrm{Var}(Z) = \\mathrm{Var}(\\xi/\\sigma) = \\sigma_\\xi^2/\\sigma^2$. Using the stationary variance condition $\\sigma^2 = \\sigma_\\xi^2 / (1-a^2)$, we get $\\mathrm{Var}(Z) = 1-a^2$.\n\nA key property of Hermite polynomials is their behavior under affine transformations followed by Gaussian convolution. Specifically, for $z \\in \\mathbb{R}$ and $W \\sim \\mathcal{N}(0,1)$, the following identity holds:\n$$ \\mathbb{E}_W[He_k(cz + \\sqrt{1-c^2}W)] = c^k He_k(z) $$\nIn our case, setting $z=x/\\sigma$ and $c=a$, we have $(ax+\\xi)/\\sigma = az + \\xi/\\sigma$. Since $\\xi/\\sigma \\sim \\mathcal{N}(0, 1-a^2)$, we can write $\\xi/\\sigma = \\sqrt{1-a^2}W$ for $W \\sim \\mathcal{N}(0,1)$. The identity applies directly:\n$$ (P p_k)(x) = \\mathbb{E}_W[He_k(az + \\sqrt{1-a^2}W)] = a^k He_k(z) = a^k p_k(x) $$\nThis shows that the Hermite polynomials $p_k(x)$ are eigenfunctions of $P$ with corresponding eigenvalues $\\lambda_k = a^k$ for $k=0, 1, 2, \\dots$.\nThe spectrum of $P$ is thus $\\{a^k\\}_{k=0}^\\infty$. The largest eigenvalue is $\\lambda_0 = a^0 = 1$. The eigenvalues on the mean-zero subspace $L_0^2(\\pi)$ are $\\{a^k\\}_{k=1}^\\infty$.\nThe spectral gap $\\gamma$ is $1$ minus the largest absolute eigenvalue on this subspace:\n$$ \\gamma = 1 - \\sup_{k \\ge 1} |a^k| = 1 - |a| $$\nsince $|a|  1$.\n\n**2. Integrated Autocorrelation Time $\\tau_{\\mathrm{int}}(f)$**\nFor the observable $f(x)=x$, we first need the autocorrelation function $\\rho_k = \\mathrm{Cov}(X_0, X_k) / \\mathrm{Var}(X_0)$. By iterating the AR(1) recursion, we get $X_k = a^k X_0 + \\sum_{j=0}^{k-1} a^j \\xi_{k-1-j}$.\nSince the noise terms $\\xi_i$ are independent of $X_0$ and have zero mean, the covariance is:\n$$ \\mathrm{Cov}(X_0, X_k) = \\mathbb{E}[X_0 (a^k X_0 + \\dots)] = a^k \\mathbb{E}[X_0^2] = a^k \\mathrm{Var}(X_0) $$\nThe autocorrelation is therefore $\\rho_k = \\frac{a^k \\mathrm{Var}(X_0)}{\\mathrm{Var}(X_0)} = a^k$.\nThe integrated autocorrelation time is calculated as a geometric series:\n$$ \\tau_{\\mathrm{int}}(f) = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k = 1 + 2 \\sum_{k=1}^{\\infty} a^k = 1 + 2\\frac{a}{1-a} = \\frac{1-a+2a}{1-a} = \\frac{1+a}{1-a} $$\n\n**3. Verification of the Inequality**\nWe need to verify $\\tau_{\\mathrm{int}}(f) \\leq \\frac{2}{\\gamma}-1$. Substituting our expressions:\n$$ \\frac{1+a}{1-a} \\leq \\frac{2}{1-|a|} - 1 $$\n- If $a \\in [0, 1)$, then $|a|=a$. The inequality becomes $\\frac{1+a}{1-a} \\leq \\frac{2}{1-a} - 1 = \\frac{1+a}{1-a}$, which holds with equality.\n- If $a \\in (-1, 0)$, then $|a|=-a$. The inequality becomes $\\frac{1+a}{1-a} \\leq \\frac{2}{1+a} - 1 = \\frac{1-a}{1+a}$. Since both denominators are positive, we can cross-multiply: $(1+a)^2 \\leq (1-a)^2$. This simplifies to $1+2a+a^2 \\leq 1-2a+a^2$, which gives $4a \\leq 0$, or $a \\leq 0$. This is true for this case.\nThe inequality is verified for all $|a|1$.\n\nThe required expressions are $\\gamma = 1-|a|$ and $\\tau_{\\mathrm{int}}(f) = \\frac{1+a}{1-a}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1-|a|  \\frac{1+a}{1-a}\n\\end{pmatrix}\n}\n$$", "id": "3335482"}]}