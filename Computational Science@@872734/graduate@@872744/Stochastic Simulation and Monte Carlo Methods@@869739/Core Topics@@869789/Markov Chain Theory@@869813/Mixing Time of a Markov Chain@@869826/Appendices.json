{"hands_on_practices": [{"introduction": "We begin with a foundational exercise in spectral analysis. This problem ([@problem_id:787957]) asks us to construct the transition matrix for a Markov chain derived from two independent random walks on a cycle and then to calculate a key eigenvalue. This practice reinforces the connection between the transition probabilities of a chain and its spectral properties, which are fundamental to understanding its long-term convergence rate.", "problem": "Consider two independent random walks, $X_t$ and $Y_t$, on the vertices of a cycle graph $\\mathbb{Z}_N = \\{0, 1, \\dots, N-1\\}$, where $N$ is an integer multiple of 4.\nThe first walker, $X_t$, moves according to the following rule: from its current position $i$, it jumps to $(i+1) \\pmod N$ with probability $p$, and stays at $i$ with probability $1-p$.\nThe second walker, $Y_t$, moves as follows: from its current position $j$, it jumps to $(j-1) \\pmod N$ with probability $q$, and stays at $j$ with probability $1-q$. Here $p, q \\in (0, 1)$ are constant probabilities.\n\nA new stochastic process, $Z_t$, is defined as the difference between the positions of the two walkers, taken modulo $N$:\n$$Z_t = (X_t - Y_t) \\pmod N$$\nThis process $Z_t$ forms a Markov chain on the state space $\\mathbb{Z}_N$. The rate of convergence of this Markov chain to its stationary distribution is governed by the magnitudes of the eigenvalues of its transition matrix.\n\nLet $\\lambda_k$ for $k \\in \\{0, 1, \\dots, N-1\\}$ be the eigenvalues of the transition matrix for the Markov chain $Z_t$. Find the exact value of the modulus of the eigenvalue $\\lambda_{N/4}$.", "solution": "1. Transition probabilities for $Z_t$:\n   The change in $Z_t$ in one step, $\\Delta Z_t = Z_{t+1} - Z_t \\pmod N$, depends on the independent moves of $X_t$ and $Y_t$. The possible changes and their probabilities are:\n   - $P(\\Delta Z_t=0) = P(\\Delta X_t=0, \\Delta Y_t=0) = (1-p)(1-q)$\n   - $P(\\Delta Z_t=1) = P(\\Delta X_t=1, \\Delta Y_t=0) + P(\\Delta X_t=0, \\Delta Y_t=-1) = p(1-q)+(1-p)q$\n   - $P(\\Delta Z_t=2) = P(\\Delta X_t=1, \\Delta Y_t=-1) = pq$\n   Since the transition probabilities only depend on the displacement, the transition matrix is circulant.\n2. Eigenvalue formula on $\\Bbb Z_N$:\n   The eigenvalues of a circulant matrix defined by the transitions $z \\to z+j$ with probability $P_j$ are given by $\\lambda_k = \\sum_j P_j e^{2\\pi i kj/N}$. For our chain:\n   $$\\lambda_k = (1-p)(1-q) + \\bigl[p(1-q)+(1-p)q\\bigr]e^{2\\pi i k/N} + pq\\,e^{4\\pi i k/N}.$$\n3. For $k=N/4$, set $\\theta=2\\pi k/N=\\pi/2$, so $e^{i\\theta}=i$, $e^{2i\\theta}=-1$. Then\n   $$\\lambda_{N/4} = (1-p)(1-q) + \\bigl[p(1-q)+(1-p)q\\bigr]\\,i + pq\\,(-1).$$\n4. Write real and imaginary parts:\n   $$\\Re\\lambda_{N/4} = (1-p)(1-q)-pq = 1 - p - q,\\quad\n     \\Im\\lambda_{N/4} = p(1-q)+(1-p)q = p+q-2pq.$$\n5. Modulus:\n   $$|\\lambda_{N/4}| = \\sqrt{\\bigl(1-p-q\\bigr)^2 + \\bigl(p+q-2pq\\bigr)^2}.$$", "answer": "$$\\boxed{\\sqrt{(1-p-q)^2 + (p+q-2pq)^2}}$$", "id": "787957"}, {"introduction": "How does the structure of a Markov chain affect its mixing speed? This exercise ([@problem_id:2409097]) explores the crucial and sometimes counter-intuitive role of \"laziness\"—the probability of staying in the same state. By analyzing how the mixing time of a random walk on a ring depends on the laziness parameter $p$, we develop deeper intuition for the factors that govern convergence speed in Monte Carlo simulations.", "problem": "An analyst in computational economics models a discretized stock price as a finite-state Markov chain on a ring of $K$ price levels to enable ergodic sampling in Monte Carlo valuation. The state space is $\\{0,1,\\dots,K-1\\}$, with transitions defined as follows: from any state $i$, with probability $p \\in (0,1)$ the chain stays in $i$, with probability $\\frac{1-p}{2}$ it moves to $i+1 \\pmod K$, and with probability $\\frac{1-p}{2}$ it moves to $i-1 \\pmod K$. Denote the transition matrix by $P_p$. Let $\\pi$ be the unique stationary distribution and, for a fixed tolerance $\\varepsilon \\in (0,\\tfrac{1}{4})$, define the total variation (TV) $\\varepsilon$-mixing time\n$$\nt_{\\mathrm{mix}}^{(p)}(\\varepsilon) \\equiv \\inf\\Big\\{t \\in \\mathbb{N}: \\max_{x \\in \\{0,\\dots,K-1\\}} \\left\\| P_p^t(x,\\cdot) - \\pi \\right\\|_{\\mathrm{TV}} \\le \\varepsilon \\Big\\}.\n$$\nFor fixed $K \\ge 5$ and fixed $\\varepsilon \\in (0,\\tfrac{1}{4})$, which statement best characterizes how the laziness parameter $p$ affects the mixing time as $p$ varies in $(0,1)$?\n\nA. $t_{\\mathrm{mix}}^{(p)}(\\varepsilon)$ decreases linearly in $p$ and approaches $0$ as $p \\to 1$.\n\nB. $t_{\\mathrm{mix}}^{(p)}(\\varepsilon)$ is essentially independent of $p$ for fixed $K$ and $\\varepsilon$.\n\nC. $t_{\\mathrm{mix}}^{(p)}(\\varepsilon)$ strictly decreases in $p$ for all $p \\in (0,1)$ because laziness breaks periodicity.\n\nD. $t_{\\mathrm{mix}}^{(p)}(\\varepsilon)$ increases approximately by a factor of $(1-p)^{-1}$ relative to the non-lazy walk, and diverges as $p \\to 1$.", "solution": "The problem statement must first be validated for scientific and logical consistency.\n\n### Step 1: Extract Givens\n-   **System**: A finite-state Markov chain.\n-   **State Space**: $S = \\{0, 1, \\dots, K-1\\}$, representing price levels on a ring. $K$ is a fixed integer with $K \\ge 5$.\n-   **Transition Probabilities**: From any state $i$, the chain transitions as follows:\n    -   To state $i$ (stays) with probability $p$.\n    -   To state $i+1 \\pmod K$ with probability $\\frac{1-p}{2}$.\n    -   To state $i-1 \\pmod K$ with probability $\\frac{1-p}{2}$.\n-   **Parameter**: The laziness parameter $p$ is in the open interval $(0, 1)$.\n-   **Notation**: The transition matrix is denoted by $P_p$.\n-   **Stationary Distribution**: $\\pi$ is the unique stationary distribution.\n-   **Mixing Time**: The total variation (TV) $\\varepsilon$-mixing time is defined as\n    $$\n    t_{\\mathrm{mix}}^{(p)}(\\varepsilon) \\equiv \\inf\\Big\\{t \\in \\mathbb{N}: \\max_{x \\in \\{0,\\dots,K-1\\}} \\left\\| P_p^t(x,\\cdot) - \\pi \\right\\|_{\\mathrm{TV}} \\le \\varepsilon \\Big\\}\n    $$\n    for a fixed tolerance $\\varepsilon \\in (0, \\tfrac{1}{4})$.\n-   **Question**: Characterize the behavior of $t_{\\mathrm{mix}}^{(p)}(\\varepsilon)$ as a function of $p \\in (0,1)$ for fixed $K$ and $\\varepsilon$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Grounding**: The problem describes a lazy random walk on a cycle graph $C_K$. This is a canonical model in the theory of Markov chains. The definitions of the transition matrix, stationary distribution, and total variation mixing time are all standard and mathematically rigorous. The context of computational economics is a valid application domain for such models. The problem is scientifically sound.\n\n2.  **Well-Posedness**:\n    -   The transition matrix $P_p$ is a valid stochastic matrix because for any row $i$, the sum of probabilities is $p + \\frac{1-p}{2} + \\frac{1-p}{2} = 1$, and all probabilities are non-negative since $p \\in (0,1)$.\n    -   The chain is irreducible as any state can be reached from any other state.\n    -   Since $p>0$, there is a positive probability of staying in the same state (a self-loop), which makes the chain aperiodic.\n    -   An irreducible and aperiodic finite-state Markov chain has a unique stationary distribution $\\pi$. For this chain, the transition matrix is doubly stochastic (all columns also sum to $1$), so the unique stationary distribution is the uniform distribution: $\\pi_j = \\frac{1}{K}$ for all $j \\in S$.\n    -   The definition of mixing time is standard. The question of how this time depends on the parameter $p$ is a well-posed mathematical problem.\n\n3.  **Objectivity and Completeness**: The problem is stated in precise, objective mathematical language. All necessary information ($K \\ge 5$, $p \\in (0,1)$, $\\varepsilon \\in (0, 1/4)$) is provided. The constraints are consistent.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a well-posed, scientifically grounded problem from the field of probability theory with no apparent flaws. I will proceed with the solution.\n\n### Derivation\nThe mixing time of a Markov chain is fundamentally related to the eigenvalues of its transition matrix $P_p$. The rate of convergence to the stationary distribution is governed by the second largest eigenvalue modulus (SLEM) of $P_p$.\n\nThe transition matrix $P_p$ is a circulant matrix. Its eigenvalues, $\\lambda_j(p)$, for $j \\in \\{0, 1, \\dots, K-1\\}$, can be calculated explicitly. They are given by:\n$$\n\\lambda_j(p) = p + \\left(\\frac{1-p}{2}\\right)e^{i \\frac{2\\pi j}{K}} + \\left(\\frac{1-p}{2}\\right)e^{-i \\frac{2\\pi j}{K}}\n$$\nUsing the identity $\\cos\\theta = \\frac{e^{i\\theta} + e^{-i\\theta}}{2}$, this simplifies to:\n$$\n\\lambda_j(p) = p + (1-p) \\cos\\left(\\frac{2\\pi j}{K}\\right)\n$$\nFor $j=0$, $\\lambda_0(p) = p + (1-p)\\cos(0) = 1$, which corresponds to the stationary distribution. Since the chain is aperiodic and reversible, all other eigenvalues are real and less than $1$. The convergence rate is determined by $\\lambda_* = \\max_{j \\in \\{1,\\dots,K-1\\}} |\\lambda_j(p)|$. The mixing time is, to first order, inversely proportional to the spectral gap, which for this chain is determined by $1 - \\lambda_*$.\n$$\nt_{\\mathrm{mix}}^{(p)}(\\varepsilon) \\approx \\frac{1}{1-\\lambda_*} \\log\\left(\\frac{1}{\\varepsilon}\\right)\n$$\nWe must analyze how $1-\\lambda_*$ depends on $p$.\n\nA simple and powerful way to understand the effect of the laziness parameter $p$ is through a heuristic argument. The process can be viewed as two stages: deciding whether to move, and then deciding where to move.\n1.  A move occurs with probability $1-p$. A self-loop occurs with probability $p$. The number of time steps until the chain makes its first move is a geometric random variable with mean $\\frac{1}{1-p}$.\n2.  Let $T_{\\text{moves}}(\\varepsilon)$ be the number of *moves* (i.e., transitions to a different state) required for the system to mix to tolerance $\\varepsilon$. This quantity depends on the structure of the graph ($K$) but not on the laziness $p$.\n3.  The total time $t_{\\mathrm{mix}}^{(p)}(\\varepsilon)$ is the number of moves needed, multiplied by the average number of time steps per move.\n$$\nt_{\\mathrm{mix}}^{(p)}(\\varepsilon) \\approx T_{\\text{moves}}(\\varepsilon) \\times (\\text{average time per move}) = T_{\\text{moves}}(\\varepsilon) \\times \\frac{1}{1-p}\n$$\nThis relationship shows that the mixing time is approximately proportional to $(1-p)^{-1}$. As $p$ approaches $1$, the chain becomes exceedingly lazy, and the time between moves diverges. Consequently, the total mixing time must also diverge.\n$$\n\\lim_{p \\to 1^-} t_{\\mathrm{mix}}^{(p)}(\\varepsilon) = \\infty\n$$\nThis heuristic is confirmed by a more formal spectral analysis. For $p \\ge \\frac{1}{2}$, all eigenvalues $\\lambda_j(p)$ are non-negative. The SLEM is $\\lambda_* = \\lambda_1(p) = p + (1-p)\\cos(\\frac{2\\pi}{K})$. The spectral gap is $\\gamma_p = 1 - \\lambda_1(p) = (1-p)(1 - \\cos(\\frac{2\\pi}{K}))$. The mixing time satisfies:\n$$\nt_{\\mathrm{mix}}^{(p)}(\\varepsilon) \\asymp \\frac{1}{\\gamma_p} = \\frac{1}{(1-p)\\left(1-\\cos\\left(\\frac{2\\pi}{K}\\right)\\right)}\n$$\nSince $K$ is fixed, the term $1-\\cos(\\frac{2\\pi}{K})$ is a positive constant. Therefore, $t_{\\mathrm{mix}}^{(p)}(\\varepsilon)$ is proportional to $(1-p)^{-1}$. This scaling by $(1-p)^{-1}$ is the dominant behavior for the mixing time as a function of the laziness $p$. The phrase \"relative to the non-lazy walk\" refers to the fact that the constant of proportionality, $T_{\\text{moves}}(\\varepsilon)$, is determined by the properties of the underlying walk structure without laziness.\n\n### Evaluation of Options\n\n**A. $t_{\\mathrm{mix}}^{(p)}(\\varepsilon)$ decreases linearly in $p$ and approaches $0$ as $p \\to 1$.**\nThis is incorrect. Our analysis shows that as $p \\to 1$, the mixing time diverges to infinity, it does not approach $0$. The relationship is not a linear decrease.\n\n**B. $t_{\\mathrm{mix}}^{(p)}(\\varepsilon)$ is essentially independent of $p$ for fixed $K$ and $\\varepsilon$.**\nThis is incorrect. The mixing time exhibits a very strong dependence on $p$, scaling approximately as $(1-p)^{-1}$. This dependence is significant and cannot be ignored.\n\n**C. $t_{\\mathrm{mix}}^{(p)}(\\varepsilon)$ strictly decreases in $p$ for all $p \\in (0,1)$ because laziness breaks periodicity.**\nThis is incorrect. While adding a small amount of laziness ($p>0$) to a periodic chain (e.g., the case $p=0$ for even $K$) does remove periodicity and drastically reduces mixing time from infinite to finite, increasing laziness further is not always beneficial. The mixing time reaches a minimum for some optimal $p_{\\text{opt}} \\in (0, \\frac{1}{2})$ and then *increases* for $p > p_{\\text{opt}}$. Thus, it does not strictly decrease over the entire interval $(0,1)$.\n\n**D. $t_{\\mathrm{mix}}^{(p)}(\\varepsilon)$ increases approximately by a factor of $(1-p)^{-1}$ relative to the non-lazy walk, and diverges as $p \\to 1$.**\nThis is correct. As derived above, the governing relationship is $t_{\\mathrm{mix}}^{(p)}(\\varepsilon) \\propto (1-p)^{-1}$. This captures two key features: (1) the approximate scaling factor that describes how mixing time changes with $p$, and (2) the divergence of mixing time as the chain becomes almost completely lazy ($p \\to 1$). This statement provides the best characterization of the behavior among the given options.", "answer": "$$\\boxed{D}$$", "id": "2409097"}, {"introduction": "While analytical solutions are invaluable, most real-world Markov chains are too complex to be solved by hand. This final practice ([@problem_id:3283319]) bridges theory and computation by guiding you to implement the power method to numerically estimate a transition matrix's spectral gap. You will then use this estimate to derive a practical upper bound on the mixing time, a core skill for any practitioner of stochastic simulation.", "problem": "You are given a collection of finite-state Markov chains, each represented by a transition matrix $P \\in \\mathbb{R}^{n \\times n}$ with nonnegative entries and unit column sums. In all provided test cases, $P$ is symmetric and doubly stochastic (both row and column sums are $1$), so the stationary distribution is uniform. Your task is to design and implement a program that estimates the spectral gap using the power method with deflation, and then uses this estimate to compute an upper bound on the mixing time in total variation distance.\n\nBegin from fundamental facts:\n- A real symmetric doubly stochastic matrix $P$ has eigenvalues in $[-1,1]$, with the largest eigenvalue equal to $1$ and the corresponding eigenvector proportional to the all-ones vector. By the Perron–Frobenius theorem, the dominant eigenvalue $\\lambda_1$ of a nonnegative, irreducible column-stochastic matrix is $1$.\n- The spectral gap is defined as $\\gamma = 1 - \\lambda_\\ast$, where $\\lambda_\\ast = \\max\\{|\\lambda| : \\lambda \\in \\sigma(P), \\lambda \\ne 1\\}$ is the magnitude of the second-largest eigenvalue in absolute value.\n- The power method iterates $x_{k+1} = P x_k / \\|P x_k\\|_2$ from an initial $x_0 \\ne 0$, and converges to a dominant eigenvector. To estimate a subdominant eigenvalue (here $\\lambda_\\ast$), you must deflate by projecting iterates onto the subspace orthogonal to the dominant eigenvector so that they converge to an eigenvector associated with $\\lambda_\\ast$.\n- For a reversible, symmetric, doubly stochastic, and lazy chain, the spectral decomposition yields an $L^2$-contraction governed by $\\lambda_\\ast$, and norms satisfy $\\|\\cdot\\|_1 \\le \\sqrt{n}\\,\\|\\cdot\\|_2$. Total variation distance is $\\|\\cdot\\|_{\\mathrm{TV}} = \\tfrac{1}{2}\\|\\cdot\\|_1$. Use these facts to derive an upper bound on the mixing time to achieve total variation distance at most a given tolerance $\\varepsilon$, starting from the worst-case initial state.\n\nImplement the following steps for each test case:\n1. Use the power method to approximate the dominant eigenvector $v_1$ of $P$ and its eigenvalue. Normalize $v_1$ to have unit Euclidean norm.\n2. Use deflated power iteration: start from a random nonzero vector $x_0$, orthogonalize it against $v_1$ using the Euclidean inner product, and iterate $x_{k+1} = P x_k$, orthogonalizing after each multiplication against $v_1$, followed by normalization. Estimate the subdominant eigenvalue $\\lambda_\\ast$ via the Rayleigh quotient $r(x) = \\frac{x^\\top P x}{x^\\top x}$ when $x$ has converged.\n3. Compute the spectral gap $\\gamma = 1 - \\lambda_\\ast$.\n4. Using the inequality chain from the spectral contraction in $L^2$ and $\\|\\cdot\\|_1 \\le \\sqrt{n}\\,\\|\\cdot\\|_2$, derive a worst-case upper bound on the total variation mixing time $t_{\\mathrm{mix}}(\\varepsilon)$ for initial distribution equal to a point mass (one-hot vector), that is, a bound on the smallest integer $t$ such that $\\|\\mu_t - \\pi\\|_{\\mathrm{TV}} \\le \\varepsilon$. Express your final bound solely in terms of $n$, $\\varepsilon$, and the estimated $\\lambda_\\ast$.\n\nUse the following test suite of matrices and tolerances, written in LaTeX:\n- Case A: Two-state, strongly lazy and symmetric,\n$$\nP_A = \\begin{bmatrix}\n0.95 & 0.05\\\\\n0.05 & 0.95\n\\end{bmatrix},\\quad \\varepsilon_A = 0.01.\n$$\n- Case B: Five-state cycle with lazy random walk, where $S_B$ is the simple random walk on the cycle graph with probability $\\tfrac{1}{2}$ to each neighbor and $0$ otherwise, and $P_B = \\tfrac{1}{2}I + \\tfrac{1}{2}S_B$. Explicitly,\n$$\nS_B = \\begin{bmatrix}\n0 & \\tfrac{1}{2} & 0 & 0 & \\tfrac{1}{2}\\\\\n\\tfrac{1}{2} & 0 & \\tfrac{1}{2} & 0 & 0\\\\\n0 & \\tfrac{1}{2} & 0 & \\tfrac{1}{2} & 0\\\\\n0 & 0 & \\tfrac{1}{2} & 0 & \\tfrac{1}{2}\\\\\n\\tfrac{1}{2} & 0 & 0 & \\tfrac{1}{2} & 0\n\\end{bmatrix},\\quad\nP_B = \\tfrac{1}{2}I + \\tfrac{1}{2}S_B,\\quad \\varepsilon_B = 0.01.\n$$\n- Case C: Three-state symmetric, doubly stochastic tridiagonal,\n$$\nP_C = \\begin{bmatrix}\n0.9 & 0.1 & 0.0\\\\\n0.1 & 0.8 & 0.1\\\\\n0.0 & 0.1 & 0.9\n\\end{bmatrix},\\quad \\varepsilon_C = 0.01.\n$$\n- Case D: Four-state complete graph with lazy random walk,\n$$\nJ_4 = \\begin{bmatrix}\n1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 1\n\\end{bmatrix},\\quad\nP_D = \\tfrac{1}{2}I + \\tfrac{1}{2}\\tfrac{1}{4}J_4,\\quad \\varepsilon_D = 0.01.\n$$\n\nYour program must:\n- Implement the power method and its deflation as described, using a stopping tolerance for the Rayleigh quotient difference of $10^{-12}$ or a maximum of $10{,}000$ iterations per phase.\n- For each case, compute the spectral gap $\\gamma$ and an upper bound on the mixing time $t_{\\mathrm{mix}}(\\varepsilon)$ derived from the above principles, assuming worst-case initialization as a point mass.\n- Aggregate the results for all cases into a single line. The final output format must be a comma-separated Python-style list of lists. Each inner list corresponds to one case and contains two entries $[\\gamma, t]$, where $\\gamma$ is rounded to six decimal places and $t$ is an integer bound. For example, the output should look like\n$[\\,[\\gamma_A, t_A],[\\gamma_B, t_B],[\\gamma_C, t_C],[\\gamma_D, t_D]\\,]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[[gap1,time1],[gap2,time2],[gap3,time3],[gap4,time4]]\"). No external input should be read. No physical units or angles are involved. All answers must be numerical floats or integers as specified.", "solution": "We start from the core definitions and properties for the provided test matrices. A transition matrix $P$ for a finite-state Markov chain with nonnegative entries and unit column sums acts on column probability vectors by $\\mu_{t+1} = P\\,\\mu_t$. In the symmetric, doubly stochastic setting, $P = P^\\top$ and both row and column sums are $1$, which implies that the stationary distribution is uniform: $\\pi = \\frac{1}{n}\\mathbf{1}$. The spectrum of $P$ lies in $[-1,1]$, with the dominant eigenvalue $\\lambda_1 = 1$ and eigenvector proportional to $\\mathbf{1}$.\n\nThe spectral gap is defined by\n$$\n\\gamma = 1 - \\lambda_\\ast,\\quad \\text{where}\\quad \\lambda_\\ast = \\max\\{|\\lambda|:\\lambda\\in\\sigma(P), \\lambda\\ne 1\\}.\n$$\nIn the symmetric setting, the eigenvalues are real, and for a lazy chain (for example, $P = \\tfrac{1}{2}I + \\tfrac{1}{2}S$ with $S$ column-stochastic), all eigenvalues lie in $[0,1]$, so $\\lambda_\\ast$ is the second-largest eigenvalue in $[0,1]$.\n\nTo approximate $\\lambda_1$ and its eigenvector $v_1$, we apply the power method. Let $x_0\\in\\mathbb{R}^n$ be a nonzero vector. Define the iteration\n$$\nx_{k+1} = \\frac{P x_k}{\\|P x_k\\|_2}.\n$$\nUnder the assumptions that $P$ is nonnegative and irreducible with dominant eigenvalue $\\lambda_1=1$ strictly greater in magnitude than other eigenvalues, the sequence $x_k$ converges to an eigenvector $v_1$ associated with $\\lambda_1$. A numerically stable estimate of the eigenvalue is given by the Rayleigh quotient,\n$$\n\\rho(x) = \\frac{x^\\top P x}{x^\\top x},\n$$\nwhich converges to $\\lambda_1$ when $x$ converges to $v_1$.\n\nTo compute the subdominant eigenvalue $\\lambda_\\ast$ via deflation, we restrict the iteration to the subspace orthogonal to $v_1$. Specifically, choose an initial vector $y_0$, project it onto the orthogonal complement of $v_1$ using the Euclidean inner product, and iterate:\n$$\n\\tilde{y}_k = P y_k,\\quad\n\\tilde{y}_k^\\perp = \\tilde{y}_k - (v_1^\\top \\tilde{y}_k) v_1,\\quad\ny_{k+1} = \\frac{\\tilde{y}_k^\\perp}{\\|\\tilde{y}_k^\\perp\\|_2}.\n$$\nIn the symmetric case, this orthogonal iteration converges to an eigenvector $v_2$ associated with the largest eigenvalue in the orthogonal complement to $v_1$, namely $\\lambda_\\ast$. Again, the Rayleigh quotient\n$$\n\\rho(y) = \\frac{y^\\top P y}{y^\\top y}\n$$\nevaluated at the converged $y$ gives an accurate estimate of $\\lambda_\\ast$.\n\nWe now derive an upper bound on the mixing time in total variation distance. For reversible chains (which include symmetric doubly stochastic matrices), the spectral decomposition implies $L^2$ contraction relative to the stationary distribution. Let $\\mu_t = P^t \\mu_0$ be the distribution after $t$ steps from initial distribution $\\mu_0$. Writing the deviation from stationarity as $d_t = \\mu_t - \\pi$, and letting $\\lambda_\\ast$ be the largest eigenvalue magnitude below $1$, we have the $L^2$ bound\n$$\n\\|d_t\\|_2 \\le \\lambda_\\ast^t \\|d_0\\|_2.\n$$\nUsing norm inequalities, we relate the total variation distance to the $L^2$ norm. Since $\\|\\cdot\\|_1 \\le \\sqrt{n}\\,\\|\\cdot\\|_2$ and total variation is defined by $\\|\\cdot\\|_{\\mathrm{TV}} = \\tfrac{1}{2}\\|\\cdot\\|_1$, we obtain\n$$\n\\|\\mu_t - \\pi\\|_{\\mathrm{TV}} = \\tfrac{1}{2}\\|d_t\\|_1 \\le \\tfrac{1}{2} \\sqrt{n}\\,\\|d_t\\|_2 \\le \\tfrac{1}{2} \\sqrt{n}\\,\\lambda_\\ast^t \\|d_0\\|_2.\n$$\nFor worst-case initialization as a point mass, say $\\mu_0 = e_i$ (the $i$-th standard basis vector), and uniform stationary distribution $\\pi = \\tfrac{1}{n}\\mathbf{1}$, the initial deviation has\n$$\n\\|d_0\\|_2 = \\left\\|e_i - \\tfrac{1}{n}\\mathbf{1}\\right\\|_2 = \\sqrt{1 - \\tfrac{1}{n}}.\n$$\nThus,\n$$\n\\|\\mu_t - \\pi\\|_{\\mathrm{TV}} \\le \\frac{1}{2} \\sqrt{n}\\,\\sqrt{1 - \\frac{1}{n}}\\;\\lambda_\\ast^t.\n$$\nTo achieve a tolerance $\\varepsilon$, it suffices to choose $t$ such that\n$$\n\\lambda_\\ast^t \\le \\frac{2\\varepsilon}{\\sqrt{n}\\,\\sqrt{1 - \\frac{1}{n}}}.\n$$\nSolving for $t$ yields\n$$\nt \\ge \\frac{\\ln\\left(\\frac{2\\varepsilon}{\\sqrt{n}\\,\\sqrt{1 - \\frac{1}{n}}}\\right)}{\\ln(\\lambda_\\ast)}.\n$$\nSince $\\lambda_\\ast \\in (0,1)$ for lazy chains, $\\ln(\\lambda_\\ast) < 0$ and the right-hand side is positive. We take the ceiling to obtain an integer upper bound:\n$$\nt_{\\mathrm{mix}}(\\varepsilon) = \\left\\lceil \\frac{\\ln\\left(\\frac{2\\varepsilon}{\\sqrt{n}\\,\\sqrt{1 - \\frac{1}{n}}}\\right)}{\\ln(\\lambda_\\ast)} \\right\\rceil.\n$$\n\nAlgorithmic design:\n- Implement a power method to approximate $v_1$ and $\\lambda_1$ with stopping criterion based on the change in the Rayleigh quotient below $10^{-12}$ or reaching $10{,}000$ iterations.\n- Implement deflated power iteration: orthogonalize iterates against $v_1$ at each step, normalize, and monitor the Rayleigh quotient for convergence to estimate $\\lambda_\\ast$.\n- Compute the spectral gap $\\gamma = 1 - \\lambda_\\ast$.\n- Compute $t_{\\mathrm{mix}}(\\varepsilon)$ using the derived formula with $n$ equal to the dimension of $P$ and the estimated $\\lambda_\\ast$. In the numerically degenerate case when $\\lambda_\\ast$ is extremely small (e.g., due to machine precision), one can safely bound $t_{\\mathrm{mix}}(\\varepsilon)$ by $1$ since the contraction is immediate; however, the provided test suite avoids this edge case.\n\nTest suite matrices and $\\varepsilon$ are:\n- Case A:\n$$\nP_A = \\begin{bmatrix}\n0.95 & 0.05\\\\\n0.05 & 0.95\n\\end{bmatrix},\\quad \\varepsilon_A = 0.01.\n$$\n- Case B:\n$$\nS_B = \\begin{bmatrix}\n0 & \\tfrac{1}{2} & 0 & 0 & \\tfrac{1}{2}\\\\\n\\tfrac{1}{2} & 0 & \\tfrac{1}{2} & 0 & 0\\\\\n0 & \\tfrac{1}{2} & 0 & \\tfrac{1}{2} & 0\\\\\n0 & 0 & \\tfrac{1}{2} & 0 & \\tfrac{1}{2}\\\\\n\\tfrac{1}{2} & 0 & 0 & \\tfrac{1}{2} & 0\n\\end{bmatrix},\\quad\nP_B = \\tfrac{1}{2}I + \\tfrac{1}{2}S_B,\\quad \\varepsilon_B = 0.01.\n$$\n- Case C:\n$$\nP_C = \\begin{bmatrix}\n0.9 & 0.1 & 0.0\\\\\n0.1 & 0.8 & 0.1\\\\\n0.0 & 0.1 & 0.9\n\\end{bmatrix},\\quad \\varepsilon_C = 0.01.\n$$\n- Case D:\n$$\nJ_4 = \\begin{bmatrix}\n1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 1\n\\end{bmatrix},\\quad\nP_D = \\tfrac{1}{2}I + \\tfrac{1}{2}\\tfrac{1}{4}J_4,\\quad \\varepsilon_D = 0.01.\n$$\n\nThe program computes $[\\gamma, t_{\\mathrm{mix}}(\\varepsilon)]$ for each case and prints a single line in the format $[\\,[\\gamma_A, t_A],[\\gamma_B, t_B],[\\gamma_C, t_C],[\\gamma_D, t_D]\\,]$, with $\\gamma$ rounded to six decimal places and $t$ an integer obtained by ceiling.", "answer": "[[0.100000, 38],[0.095492, 46],[0.100000, 41],[0.500000, 7]]", "id": "3283319"}]}