{"hands_on_practices": [{"introduction": "A central challenge in importance sampling is selecting a proposal distribution $q$ that adequately covers the target $\\pi$. This practice explores a critical failure mode where the proposal has lighter tails than the target, which can cause the variance of the estimator to become infinite. By working through this concrete example [@problem_id:3241960], you will gain a crucial intuition for why the condition $\\operatorname{Var}_q(w(X)f(X)) \\lt \\infty$ is not automatically guaranteed and how to diagnose this fundamental problem.", "problem": "Consider the task of estimating the target expectation $I = \\mathbb{E}_{\\pi}[f(X)]$ using importance sampling (IS), where the weight function is $w(x) = \\frac{\\pi(x)}{q(x)}$ and samples are drawn from a proposal distribution $q(x)$. Start from the fundamental definitions of expectation and variance under a probability density function (PDF): for any measurable function $g$, $\\mathbb{E}_{p}[g(X)] = \\int_{\\mathbb{R}} g(x)\\,p(x)\\,dx$, and $\\operatorname{Var}_{p}(Y) = \\mathbb{E}_{p}[Y^{2}] - \\left(\\mathbb{E}_{p}[Y]\\right)^{2}$. In importance sampling, the estimator of $I$ based on samples from $q$ is the average of $w(X)f(X)$, and its variance under $q$ is $\\operatorname{Var}_{q}\\!\\left[w(X)f(X)\\right]$.\n\nConstruct a one-dimensional example in which the proposal $q$ under-covers the tails of the target $\\pi$, causing the variance $\\operatorname{Var}_{q}\\!\\left[w(X)f(X)\\right]$ to be infinite even though the target expectation $I$ is finite. Specifically, take the target PDF to be the standard Cauchy distribution\n$$\n\\pi(x) = \\frac{1}{\\pi\\,(1+x^{2})}, \\quad x \\in \\mathbb{R},\n$$\nthe proposal PDF to be the standard normal distribution\n$$\nq(x) = \\frac{1}{\\sqrt{2\\pi}}\\,\\exp\\!\\left(-\\frac{x^{2}}{2}\\right), \\quad x \\in \\mathbb{R},\n$$\nand the integrand to be the bounded function\n$$\nf(x) = 1, \\quad x \\in \\mathbb{R}.\n$$\nUsing only the core definitions above, perform the following:\n\n1. Compute the target expectation $I = \\int_{\\mathbb{R}} f(x)\\,\\pi(x)\\,dx$ exactly.\n2. Derive an explicit integral expression for the second moment $\\mathbb{E}_{q}\\!\\left[(w(X)f(X))^{2}\\right]$ and show, by a tail comparison argument grounded in the given PDFs, that this integral diverges to $+\\infty$. Conclude that $\\operatorname{Var}_{q}\\!\\left[w(X)f(X)\\right]$ is infinite while $I$ is finite.\n\nState your final numeric answer for $I$ exactly. No rounding is required and no units apply.", "solution": "The problem presents a well-posed question in the domain of numerical methods, specifically importance sampling. All provided functions—the target distribution $\\pi(x)$, the proposal distribution $q(x)$, and the integrand $f(x)$—are standard, well-defined mathematical objects. The problem is scientifically grounded and self-contained, allowing for a rigorous analysis. We proceed with the two required tasks.\n\nThe problem asks us to investigate an importance sampling scenario where the target expectation is finite but the variance of the estimator is infinite. The components are defined as follows:\nTarget probability density function (PDF): Standard Cauchy distribution, $\\pi(x) = \\frac{1}{\\pi(1+x^2)}$.\nProposal PDF: Standard normal distribution, $q(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{x^2}{2})$.\nIntegrand function: $f(x) = 1$.\nThe goal is to estimate the target expectation $I = \\mathbb{E}_{\\pi}[f(X)]$. The importance sampling weight is $w(x) = \\frac{\\pi(x)}{q(x)}$.\n\n**1. Computation of the Target Expectation $I$**\n\nThe target expectation is defined as $I = \\int_{\\mathbb{R}} f(x)\\pi(x)dx$. Substituting the given expressions for $f(x)$ and $\\pi(x)$:\n$$\nI = \\int_{-\\infty}^{\\infty} 1 \\cdot \\frac{1}{\\pi(1+x^2)} dx = \\frac{1}{\\pi} \\int_{-\\infty}^{\\infty} \\frac{1}{1+x^2} dx\n$$\nThe integral of $\\pi(x)$ over its support $\\mathbb{R}$ must equal $1$ by the definition of a probability density function. We can verify this directly. The antiderivative of $\\frac{1}{1+x^2}$ is $\\arctan(x)$. Therefore, we can evaluate the definite integral:\n$$\nI = \\frac{1}{\\pi} \\left[ \\arctan(x) \\right]_{-\\infty}^{\\infty} = \\frac{1}{\\pi} \\left( \\lim_{b \\to \\infty} \\arctan(b) - \\lim_{a \\to -\\infty} \\arctan(a) \\right)\n$$\nUsing the known limits of the arctangent function, $\\lim_{b \\to \\infty} \\arctan(b) = \\frac{\\pi}{2}$ and $\\lim_{a \\to -\\infty} \\arctan(a) = -\\frac{\\pi}{2}$, we obtain:\n$$\nI = \\frac{1}{\\pi} \\left( \\frac{\\pi}{2} - \\left(-\\frac{\\pi}{2}\\right) \\right) = \\frac{1}{\\pi} \\left( \\frac{\\pi}{2} + \\frac{\\pi}{2} \\right) = \\frac{1}{\\pi}(\\pi) = 1\n$$\nThe target expectation $I$ is finite and equal to $1$.\n\n**2. Analysis of the Importance Sampling Estimator Variance**\n\nThe variance of the importance sampling estimator for $I$ is given by $\\operatorname{Var}_{q}[w(X)f(X)]$. Using the definition of variance, $\\operatorname{Var}_{p}(Y) = \\mathbb{E}_{p}[Y^2] - (\\mathbb{E}_{p}[Y])^2$, we have:\n$$\n\\operatorname{Var}_{q}[w(X)f(X)] = \\mathbb{E}_{q}\\left[(w(X)f(X))^2\\right] - \\left( \\mathbb{E}_{q}[w(X)f(X)] \\right)^2\n$$\nFirst, let's compute the expectation of the estimator, $\\mathbb{E}_{q}[w(X)f(X)]$:\n$$\n\\mathbb{E}_{q}[w(X)f(X)] = \\int_{-\\infty}^{\\infty} w(x)f(x)q(x)dx = \\int_{-\\infty}^{\\infty} \\frac{\\pi(x)}{q(x)} \\cdot 1 \\cdot q(x)dx = \\int_{-\\infty}^{\\infty} \\pi(x)dx = I = 1\n$$\nThis confirms that the importance sampling estimator is unbiased for $I$, and so $(\\mathbb{E}_{q}[w(X)f(X)])^2 = 1^2 = 1$. The variance is thus infinite if and only if the second moment, $\\mathbb{E}_{q}\\left[(w(X)f(X))^2\\right]$, is infinite.\n\nNext, we derive the explicit integral for the second moment:\n$$\n\\mathbb{E}_{q}\\left[(w(X)f(X))^2\\right] = \\int_{-\\infty}^{\\infty} (w(x)f(x))^2 q(x) dx\n$$\nSubstituting $f(x)=1$ and $w(x) = \\frac{\\pi(x)}{q(x)}$:\n$$\n\\mathbb{E}_{q}\\left[(w(X)f(X))^2\\right] = \\int_{-\\infty}^{\\infty} \\left(\\frac{\\pi(x)}{q(x)}\\right)^2 q(x) dx = \\int_{-\\infty}^{\\infty} \\frac{\\pi(x)^2}{q(x)} dx\n$$\nNow, we substitute the expressions for the PDFs $\\pi(x)$ and $q(x)$:\n$$\n\\frac{\\pi(x)^2}{q(x)} = \\frac{\\left(\\frac{1}{\\pi(1+x^2)}\\right)^2}{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)} = \\frac{\\frac{1}{\\pi^2(1+x^2)^2}}{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)} = \\frac{\\sqrt{2\\pi}}{\\pi^2} \\frac{\\exp\\left(\\frac{x^2}{2}\\right)}{(1+x^2)^2}\n$$\nThe second moment is the integral of this expression:\n$$\n\\mathbb{E}_{q}\\left[(w(X)f(X))^2\\right] = \\frac{\\sqrt{2\\pi}}{\\pi^2} \\int_{-\\infty}^{\\infty} \\frac{\\exp\\left(\\frac{x^2}{2}\\right)}{(1+x^2)^2} dx\n$$\nTo show that this integral diverges, we perform a tail comparison argument. The integrand is a positive function. A necessary condition for the convergence of an improper integral $\\int_{-\\infty}^{\\infty} g(x) dx$ is that $\\lim_{|x| \\to \\infty} g(x) = 0$. Let's examine this limit for our integrand:\n$$\n\\lim_{|x| \\to \\infty} \\frac{\\exp\\left(\\frac{x^2}{2}\\right)}{(1+x^2)^2}\n$$\nThe numerator, $\\exp\\left(\\frac{x^2}{2}\\right)$, exhibits exponential growth. The denominator, $(1+x^2)^2$, exhibits polynomial growth (asymptotic to $x^4$). It is a fundamental result of calculus that exponential growth dominates any polynomial growth. Therefore:\n$$\n\\lim_{|x| \\to \\infty} \\frac{\\exp\\left(\\frac{x^2}{2}\\right)}{(1+x^2)^2} = +\\infty\n$$\nSince the limit of the integrand as $|x| \\to \\infty$ is not $0$, the necessary condition for convergence is not met. Because the integrand is strictly positive, the integral must diverge to $+\\infty$.\n$$\n\\mathbb{E}_{q}\\left[(w(X)f(X))^2\\right] = +\\infty\n$$\nThis occurs because the proposal distribution $q(x)$ (standard normal) has tails that decay to zero much faster ($\\exp(-x^2/2)$) than the tails of the target distribution $\\pi(x)$ (Cauchy, which decays as $1/x^2$). This is a classic example of under-covering the tails. The weight function $w(x)$ becomes extremely large in the tails, causing the variance to explode.\n\nFinally, we conclude for the variance:\n$$\n\\operatorname{Var}_{q}[w(X)f(X)] = \\mathbb{E}_{q}\\left[(w(X)f(X))^2\\right] - \\left( \\mathbb{E}_{q}[w(X)f(X)] \\right)^2 = +\\infty - 1 = +\\infty\n$$\nWe have successfully demonstrated an example where the target expectation $I$ is finite ($I=1$), yet the variance of the importance sampling estimator is infinite.", "answer": "$$\\boxed{1}$$", "id": "3241960"}, {"introduction": "Having seen how variance can become infinite, we now focus on a scenario where it is finite and can be explicitly minimized. This exercise [@problem_id:3360244] will guide you through the process of deriving a closed-form expression for the variance of an importance sampling estimator. You will then use this expression to find the optimal proposal distribution within a given parametric family, a core skill in tuning Monte Carlo simulations for efficiency.", "problem": "Let $p(x)$ be the probability density function of the standard normal distribution $\\mathcal{N}(0,1)$ on $\\mathbb{R}$. Consider the task of estimating the target expectation $I = \\mathbb{E}_{p}[f(X)]$ for $f(x)=x$ using importance sampling (IS) based on a Gaussian proposal family $\\{q_{\\theta}: \\theta \\in \\mathbb{R}\\}$, where $q_{\\theta}(x)$ is the density of $\\mathcal{N}(\\theta,1)$. The importance weight is defined by $w_{\\theta}(x) = p(x)/q_{\\theta}(x)$, and the single-draw IS summand is $Y_{\\theta} = w_{\\theta}(X) f(X)$ with $X \\sim q_{\\theta}$. The standard Monte Carlo (MC) IS estimator with $n$ independent and identically distributed draws is $\\widehat{I}_{n}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} Y_{\\theta,i}$.\n\nStarting from the definitions of importance sampling and variance, and using only well-tested formulas for the Gaussian exponential family (e.g., the moment generating function of a normal random variable), derive the closed-form expression for the variance\n$$\nV(\\theta) \\equiv \\operatorname{Var}_{q_{\\theta}}\\!\\left(Y_{\\theta}\\right),\n$$\nas an explicit function of the parameter $\\theta$. Then, determine all stationary points of the function $\\theta \\mapsto V(\\theta)$ and classify them.\n\nYour final answer must be given as a two-entry row vector using the $\\LaTeX$ $\\pmatrix$ environment, where the first entry is the closed-form expression for $V(\\theta)$ and the second entry is the collection of stationary points. No numerical approximation is required for any part of the answer, and no units are involved. If there are multiple stationary points, list them all in the second entry separated by commas inside the same row vector.", "solution": "The objective is to derive the closed-form expression for the variance $V(\\theta) = \\operatorname{Var}_{q_{\\theta}}(Y_{\\theta})$ and to find and classify the stationary points of $V(\\theta)$. The variance is defined as $V(\\theta) = \\mathbb{E}_{q_{\\theta}}[Y_{\\theta}^2] - (\\mathbb{E}_{q_{\\theta}}[Y_{\\theta}])^2$.\n\nFirst, we calculate the expected value of the importance sampling summand, $\\mathbb{E}_{q_{\\theta}}[Y_{\\theta}]$.\nThe summand is $Y_{\\theta} = w_{\\theta}(X) f(X)$, where $X \\sim q_{\\theta}$. The importance weight is $w_{\\theta}(x) = p(x)/q_{\\theta}(x)$.\nThe expectation is:\n$$\n\\mathbb{E}_{q_{\\theta}}[Y_{\\theta}] = \\mathbb{E}_{q_{\\theta}}\\left[\\frac{p(X)}{q_{\\theta}(X)} f(X)\\right] = \\int_{-\\infty}^{\\infty} \\frac{p(x)}{q_{\\theta}(x)} f(x) q_{\\theta}(x) \\,dx = \\int_{-\\infty}^{\\infty} f(x) p(x) \\,dx = \\mathbb{E}_{p}[f(X)]\n$$\nThis is the target quantity $I$. In this problem, $p(x)$ is the probability density function (PDF) of the standard normal distribution $\\mathcal{N}(0,1)$, and $f(x) = x$.\nTherefore, the expectation is the mean of the standard normal distribution:\n$$\n\\mathbb{E}_{q_{\\theta}}[Y_{\\theta}] = I = \\mathbb{E}_{p}[X] = 0\n$$\nSince the mean of $Y_{\\theta}$ is $0$, the variance simplifies to the second moment:\n$$\nV(\\theta) = \\mathbb{E}_{q_{\\theta}}[Y_{\\theta}^2] - (0)^2 = \\mathbb{E}_{q_{\\theta}}[Y_{\\theta}^2]\n$$\n\nNext, we derive the expression for this second moment. The squared summand is $Y_{\\theta}^2 = (w_{\\theta}(X) f(X))^2$.\nWe first find the explicit form of the weight $w_{\\theta}(x)$. The PDF for $p(x)$ is $\\frac{1}{\\sqrt{2\\pi}}\\exp(-x^2/2)$ and for $q_{\\theta}(x)$ is $\\frac{1}{\\sqrt{2\\pi}}\\exp(-(x-\\theta)^2/2)$.\n$$\nw_{\\theta}(x) = \\frac{p(x)}{q_{\\theta}(x)} = \\frac{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)}{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x-\\theta)^2}{2}\\right)} = \\exp\\left(-\\frac{x^2}{2} + \\frac{(x-\\theta)^2}{2}\\right)\n$$\nThe exponent simplifies to:\n$$\n-\\frac{x^2}{2} + \\frac{x^2 - 2\\theta x + \\theta^2}{2} = \\frac{-x^2 + x^2 - 2\\theta x + \\theta^2}{2} = -\\theta x + \\frac{\\theta^2}{2}\n$$\nSo, the weight is $w_{\\theta}(x) = \\exp(-\\theta x + \\theta^2/2)$.\nThe summand is $Y_{\\theta} = f(X) w_{\\theta}(X) = X \\exp(-\\theta X + \\theta^2/2)$, where $X \\sim q_{\\theta}$, i.e., $X \\sim \\mathcal{N}(\\theta, 1)$.\nThe squared summand is $Y_{\\theta}^2 = X^2 \\exp(-2\\theta X + \\theta^2)$.\nNow we compute its expectation under $q_{\\theta}$:\n$$\nV(\\theta) = \\mathbb{E}_{q_{\\theta}}[Y_{\\theta}^2] = \\mathbb{E}_{q_{\\theta}}\\left[X^2 \\exp(-2\\theta X + \\theta^2)\\right] = \\exp(\\theta^2) \\mathbb{E}_{q_{\\theta}}\\left[X^2 \\exp(-2\\theta X)\\right]\n$$\nTo evaluate the expectation, we use the moment generating function (MGF) of $X \\sim \\mathcal{N}(\\theta, 1)$. The MGF of a general normal random variable $Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is $M_Z(t) = \\exp(\\mu t + \\sigma^2 t^2/2)$. For $X \\sim \\mathcal{N}(\\theta, 1)$, we have $\\mu = \\theta$ and $\\sigma^2 = 1$, so its MGF is:\n$$\nM_X(t) = \\mathbb{E}_{q_{\\theta}}[\\exp(tX)] = \\exp(\\theta t + t^2/2)\n$$\nWe use the property that $\\mathbb{E}_{q_{\\theta}}[X^2 \\exp(tX)] = \\frac{d^2}{dt^2} M_X(t)$.\nFirst derivative:\n$$\n\\frac{d}{dt} M_X(t) = \\frac{d}{dt} \\exp(\\theta t + t^2/2) = (\\theta + t) \\exp(\\theta t + t^2/2)\n$$\nSecond derivative (using the product rule):\n$$\n\\frac{d^2}{dt^2} M_X(t) = \\frac{d}{dt} \\left[(\\theta + t) \\exp(\\theta t + t^2/2)\\right] = (1)\\exp(\\theta t + t^2/2) + (\\theta + t)(\\theta + t)\\exp(\\theta t + t^2/2) = \\left[1 + (\\theta + t)^2\\right] \\exp(\\theta t + t^2/2)\n$$\nWe need to evaluate $\\mathbb{E}_{q_{\\theta}}[X^2 \\exp(-2\\theta X)]$, which corresponds to setting $t = -2\\theta$ in the expression for the second derivative of the MGF:\n\\begin{align*}\n\\mathbb{E}_{q_{\\theta}}\\left[X^2 \\exp(-2\\theta X)\\right] = \\left[1 + (\\theta + (-2\\theta))^2\\right] \\exp\\left(\\theta(-2\\theta) + \\frac{(-2\\theta)^2}{2}\\right) \\\\\n= \\left[1 + (-\\theta)^2\\right] \\exp\\left(-2\\theta^2 + \\frac{4\\theta^2}{2}\\right) \\\\\n= (1 + \\theta^2) \\exp(-2\\theta^2 + 2\\theta^2) \\\\\n= (1 + \\theta^2) \\exp(0) = 1 + \\theta^2\n\\end{align*}\nSubstituting this back into the expression for $V(\\theta)$:\n$$\nV(\\theta) = \\exp(\\theta^2) \\cdot (1 + \\theta^2)\n$$\nSo, the closed-form expression for the variance is $V(\\theta) = (1 + \\theta^2)\\exp(\\theta^2)$.\n\nNext, we find and classify the stationary points of $V(\\theta)$. Stationary points occur where the first derivative, $V'(\\theta) = \\frac{dV}{d\\theta}$, is zero. We use the product rule to differentiate $V(\\theta)$:\n$$\nV'(\\theta) = \\frac{d}{d\\theta}\\left[(1 + \\theta^2)\\exp(\\theta^2)\\right] = \\left(\\frac{d}{d\\theta}(1+\\theta^2)\\right)\\exp(\\theta^2) + (1+\\theta^2)\\left(\\frac{d}{d\\theta}\\exp(\\theta^2)\\right)\n$$\n$$\nV'(\\theta) = (2\\theta)\\exp(\\theta^2) + (1+\\theta^2)(2\\theta\\exp(\\theta^2)) = 2\\theta\\exp(\\theta^2) [1 + (1+\\theta^2)] = 2\\theta(2+\\theta^2)\\exp(\\theta^2)\n$$\nTo find the stationary points, we set $V'(\\theta) = 0$:\n$$\n2\\theta(2+\\theta^2)\\exp(\\theta^2) = 0\n$$\nFor any real $\\theta$, $\\exp(\\theta^2)  0$ and $2+\\theta^2 \\ge 2  0$. Therefore, the only way for the product to be zero is if $2\\theta = 0$, which implies $\\theta = 0$.\nThere is a single stationary point at $\\theta = 0$.\n\nTo classify this stationary point, we use the second derivative test. We compute $V''(\\theta) = \\frac{d^2V}{d\\theta^2}$.\nLet's differentiate $V'(\\theta) = (4\\theta+2\\theta^3)\\exp(\\theta^2)$:\n$$\nV''(\\theta) = \\frac{d}{d\\theta}\\left[(4\\theta+2\\theta^3)\\exp(\\theta^2)\\right] = (4+6\\theta^2)\\exp(\\theta^2) + (4\\theta+2\\theta^3)(2\\theta\\exp(\\theta^2))\n$$\n$$\nV''(\\theta) = \\exp(\\theta^2) \\left[ (4+6\\theta^2) + 2\\theta(4\\theta+2\\theta^3) \\right] = \\exp(\\theta^2) \\left[ 4+6\\theta^2+8\\theta^2+4\\theta^4 \\right]\n$$\n$$\nV''(\\theta) = (4\\theta^4 + 14\\theta^2 + 4)\\exp(\\theta^2)\n$$\nNow, we evaluate the second derivative at the stationary point $\\theta = 0$:\n$$\nV''(0) = (4(0)^4 + 14(0)^2 + 4)\\exp(0) = (4)(1) = 4\n$$\nSince $V''(0) = 4  0$, the stationary point $\\theta=0$ corresponds to a local minimum.\nFurthermore, for any real $\\theta$, $V(\\theta) = (1+\\theta^2)\\exp(\\theta^2) \\ge (1)\\exp(0) = 1$, and $V(0) = 1$. Thus, $\\theta=0$ is a global minimum.", "answer": "$$\n\\boxed{\\begin{pmatrix} (1+\\theta^2)\\exp(\\theta^2)  0 \\end{pmatrix}}\n$$", "id": "3360244"}, {"introduction": "Estimating the probability of rare events is a classic challenge where standard simulation methods are notoriously inefficient. Importance sampling offers a powerful solution, provided the proposal distribution is chosen to minimize variance. This advanced practice [@problem_id:3360206] applies the principles of variance analysis to this very problem, demonstrating how to use asymptotic methods to find an optimal proposal for estimating a small tail probability of a Gaussian distribution.", "problem": "Let $p(x)$ be the standard normal density $p(x)=\\varphi(x)=(2\\pi)^{-1/2}\\exp(-x^{2}/2)$ on $\\mathbb{R}$, and let $f(x)=\\mathbf{1}\\{xa\\}$ with a fixed threshold $a0$. We wish to estimate the rare-event probability $\\theta=\\mathbb{P}_{p}(Xa)$ using importance sampling (IS), where the proposal is taken from the canonical exponential tilting family $q_{\\mu}(x)=\\varphi(x-\\mu)$, that is, $q_{\\mu}$ is the normal density with mean $\\mu\\in\\mathbb{R}$ and variance $1$. The importance weight is $W(x)=p(x)/q_{\\mu}(x)$. The IS estimator of $\\theta$ based on a single draw from $q_{\\mu}$ is $f(X)W(X)$, and its variance under $q_{\\mu}$ is $\\operatorname{Var}_{q_{\\mu}}(fW)$.\n\nStarting from the definitions of expectation and variance, express $\\operatorname{Var}_{q_{\\mu}}(fW)$ exactly in terms of the Gaussian survival function $\\bar{\\Phi}(x)=\\int_{x}^{\\infty}\\varphi(t)\\,\\mathrm{d}t$. Then, using the asymptotic form of Mills’ ratio for the Gaussian tail, namely $\\bar{\\Phi}(x)\\sim \\varphi(x)/x$ as $x\\to\\infty$, derive the leading-order asymptotic of $\\operatorname{Var}_{q_{\\mu}}(fW)$ as $a\\to\\infty$ for fixed $\\mu$. Finally, within the canonical exponential tilting family $\\{q_{\\mu}:\\mu\\in\\mathbb{R}\\}$, determine the choice of $\\mu$ that minimizes the leading-order asymptotic and provide the resulting minimal leading-order asymptotic expression for $\\operatorname{Var}_{q_{\\mu}}(fW)$ as $a\\to\\infty$.\n\nState your final answer as a single closed-form analytic expression in terms of $a$ and universal constants. No numerical rounding is required. Expand any acronym at its first appearance, for example, importance sampling (IS) and Monte Carlo (MC).", "solution": "The problem requires the analysis of the variance of an importance sampling (IS) estimator for a rare-event probability. We must first derive an exact expression for this variance, then find its leading-order asymptotic behavior, and finally, determine the optimal parameter choice within the proposal family that minimizes this asymptotic variance, providing the resulting minimal expression.\n\nLet the target probability distribution be the standard normal density $p(x)=\\varphi(x)=(2\\pi)^{-1/2}\\exp(-x^{2}/2)$ and the proposal distribution be $q_{\\mu}(x)=\\varphi(x-\\mu)$, a normal density with mean $\\mu$ and variance $1$. We want to estimate $\\theta=\\mathbb{P}_{p}(Xa)=\\int_{a}^{\\infty}p(x)\\,\\mathrm{d}x = \\bar{\\Phi}(a)$, where $f(x)=\\mathbf{1}\\{xa\\}$ is the indicator function for the event and $\\bar{\\Phi}(x)$ is the Gaussian survival function.\n\nThe IS estimator based on a single sample $X \\sim q_{\\mu}$ is $Z = f(X)W(X)$, where the importance weight is $W(x) = p(x)/q_{\\mu}(x)$. The variance of this estimator under the proposal distribution $q_{\\mu}$ is given by:\n$$\n\\operatorname{Var}_{q_{\\mu}}(fW) = \\mathbb{E}_{q_{\\mu}}[(fW)^{2}] - (\\mathbb{E}_{q_{\\mu}}[fW])^{2}\n$$\n\nFirst, we compute the expectation of the estimator. This is a fundamental property of IS and should yield the quantity we seek to estimate, $\\theta$.\n$$\n\\mathbb{E}_{q_{\\mu}}[f(X)W(X)] = \\int_{-\\infty}^{\\infty} f(x) \\frac{p(x)}{q_{\\mu}(x)} q_{\\mu}(x) \\,\\mathrm{d}x = \\int_{-\\infty}^{\\infty} f(x) p(x) \\,\\mathrm{d}x = \\mathbb{E}_{p}[f(X)] = \\theta\n$$\nSince $f(x)=\\mathbf{1}\\{xa\\}$ and $p(x) = \\varphi(x)$, we have $\\theta = \\int_{a}^{\\infty} \\varphi(x)\\,\\mathrm{d}x = \\bar{\\Phi}(a)$. Thus, the second term in the variance expression is $(\\mathbb{E}_{q_{\\mu}}[fW])^{2} = (\\bar{\\Phi}(a))^{2}$.\n\nNext, we calculate the second moment, $\\mathbb{E}_{q_{\\mu}}[(fW)^{2}]$.\n$$\n\\mathbb{E}_{q_{\\mu}}[(fW)^{2}] = \\int_{-\\infty}^{\\infty} (f(x)W(x))^{2} q_{\\mu}(x) \\,\\mathrm{d}x\n$$\nSince $f(x) = \\mathbf{1}\\{xa\\}$, its square is $f(x)^{2}=f(x)$. The integral is non-zero only for $xa$.\n$$\n\\mathbb{E}_{q_{\\mu}}[(fW)^{2}] = \\int_{a}^{\\infty} \\left(\\frac{p(x)}{q_{\\mu}(x)}\\right)^{2} q_{\\mu}(x) \\,\\mathrm{d}x = \\int_{a}^{\\infty} \\frac{p(x)^{2}}{q_{\\mu}(x)} \\,\\mathrm{d}x\n$$\nLet's analyze the integrand. The densities are $p(x) = (2\\pi)^{-1/2}\\exp(-x^{2}/2)$ and $q_{\\mu}(x) = (2\\pi)^{-1/2}\\exp(-(x-\\mu)^{2}/2)$.\n\\begin{align*}\n\\frac{p(x)^{2}}{q_{\\mu}(x)} = \\frac{ (2\\pi)^{-1}\\exp(-x^{2}) }{ (2\\pi)^{-1/2}\\exp(-(x-\\mu)^{2}/2) } \\\\\n= (2\\pi)^{-1/2} \\exp\\left(-x^{2} + \\frac{(x-\\mu)^{2}}{2}\\right) \\\\\n= (2\\pi)^{-1/2} \\exp\\left(\\frac{-2x^{2} + x^{2}-2\\mu x+\\mu^{2}}{2}\\right) \\\\\n= (2\\pi)^{-1/2} \\exp\\left(\\frac{-x^{2}-2\\mu x+\\mu^{2}}{2}\\right)\n\\end{align*}\nTo simplify the exponent, we complete the square for the terms involving $x$: $-x^{2}-2\\mu x = -(x^{2}+2\\mu x) = -((x+\\mu)^{2}-\\mu^{2}) = -(x+\\mu)^{2}+\\mu^{2}$.\nThe expression in the exponential becomes $\\frac{-(x+\\mu)^{2}+\\mu^{2}+\\mu^{2}}{2} = -\\frac{(x+\\mu)^{2}}{2} + \\mu^{2}$.\nSo, the integrand is:\n$$\n\\frac{p(x)^{2}}{q_{\\mu}(x)} = (2\\pi)^{-1/2} \\exp\\left(-\\frac{(x+\\mu)^{2}}{2} + \\mu^{2}\\right) = \\exp(\\mu^{2}) \\varphi(x+\\mu)\n$$\nThe second moment is therefore:\n$$\n\\mathbb{E}_{q_{\\mu}}[(fW)^{2}] = \\int_{a}^{\\infty} \\exp(\\mu^{2}) \\varphi(x+\\mu) \\,\\mathrm{d}x = \\exp(\\mu^{2}) \\int_{a}^{\\infty} \\varphi(x+\\mu) \\,\\mathrm{d}x\n$$\nWith a change of variable $t=x+\\mu$, $\\mathrm{d}t=\\mathrm{d}x$, the integral becomes $\\int_{a+\\mu}^{\\infty} \\varphi(t)\\,\\mathrm{d}t = \\bar{\\Phi}(a+\\mu)$.\nThus, $\\mathbb{E}_{q_{\\mu}}[(fW)^{2}] = \\exp(\\mu^{2}) \\bar{\\Phi}(a+\\mu)$.\n\nCombining the terms, the exact expression for the variance is:\n$$\n\\operatorname{Var}_{q_{\\mu}}(fW) = \\exp(\\mu^{2}) \\bar{\\Phi}(a+\\mu) - (\\bar{\\Phi}(a))^{2}\n$$\n\nNow, we derive the leading-order asymptotic of this variance as $a\\to\\infty$. We use the given Mills' ratio asymptotic: $\\bar{\\Phi}(x) \\sim \\varphi(x)/x$ as $x\\to\\infty$.\nThe first term behaves as:\n$$\n\\exp(\\mu^{2}) \\bar{\\Phi}(a+\\mu) \\sim \\exp(\\mu^{2}) \\frac{\\varphi(a+\\mu)}{a+\\mu} = \\exp(\\mu^{2}) \\frac{(2\\pi)^{-1/2}\\exp(-(a+\\mu)^{2}/2)}{a+\\mu}\n$$\nThe second term behaves as:\n$$\n(\\bar{\\Phi}(a))^{2} \\sim \\left(\\frac{\\varphi(a)}{a}\\right)^{2} = \\frac{(2\\pi)^{-1}\\exp(-a^{2})}{a^{2}}\n$$\nThe exponential part of the first term is $\\exp(\\mu^{2})\\exp(-(a+\\mu)^{2}/2) = \\exp(\\mu^{2} - (a^{2}+2a\\mu+\\mu^{2})/2) = \\exp(-a^{2}/2 - a\\mu + \\mu^{2}/2)$. The exponential part of the second term is $\\exp(-a^{2})$. As $a \\to \\infty$, $\\exp(-a^{2})$ decays much faster than $\\exp(-a^{2}/2)$. Therefore, the second term is of a smaller order, and the leading-order asymptotic of the variance is determined by the first term:\n$$\n\\operatorname{Var}_{q_{\\mu}}(fW) \\sim \\exp(\\mu^{2}) \\bar{\\Phi}(a+\\mu)\n$$\nUsing the Mills' ratio approximation again gives:\n\\begin{align*}\n\\operatorname{Var}_{q_{\\mu}}(fW) \\sim \\exp(\\mu^{2}) \\frac{\\varphi(a+\\mu)}{a+\\mu} \\\\\n\\sim \\frac{\\exp(\\mu^{2})}{\\sqrt{2\\pi}(a+\\mu)} \\exp\\left(-\\frac{(a+\\mu)^{2}}{2}\\right) \\\\\n= \\frac{1}{\\sqrt{2\\pi}(a+\\mu)} \\exp\\left(\\mu^{2} - \\frac{a^{2}+2a\\mu+\\mu^{2}}{2}\\right) \\\\\n= \\frac{1}{\\sqrt{2\\pi}(a+\\mu)} \\exp\\left(-\\frac{a^{2}}{2} - a\\mu + \\frac{\\mu^{2}}{2}\\right)\n\\end{align*}\nLet's denote this leading-order asymptotic expression by $V_{\\text{asym}}(\\mu)$.\n\nFinally, we find the choice of $\\mu$ that minimizes $V_{\\text{asym}}(\\mu)$. Minimizing $V_{\\text{asym}}(\\mu)$ is equivalent to minimizing its natural logarithm with respect to $\\mu$:\n$$\nL(\\mu) = \\ln(V_{\\text{asym}}(\\mu)) = -\\ln(\\sqrt{2\\pi}) - \\ln(a+\\mu) - \\frac{a^{2}}{2} - a\\mu + \\frac{\\mu^{2}}{2}\n$$\nTaking the derivative with respect to $\\mu$ and setting it to zero to find the minimum:\n$$\n\\frac{\\mathrm{d}L}{\\mathrm{d}\\mu} = -\\frac{1}{a+\\mu} - a + \\mu = 0\n$$\nThis implies $\\mu - a = \\frac{1}{a+\\mu}$, which leads to $(\\mu-a)(a+\\mu)=1$, or $\\mu^{2}-a^{2}=1$. Since we are estimating a right-tail probability for $a0$, the proposal mean $\\mu$ should be positive. Thus, the optimal $\\mu$ is $\\mu_{\\text{opt}} = \\sqrt{a^{2}+1}$.\n\nNow we determine the resulting minimal leading-order asymptotic expression. We substitute $\\mu_{\\text{opt}}$ into $V_{\\text{asym}}(\\mu)$ and find its asymptotic behavior as $a\\to\\infty$.\nAs $a\\to\\infty$, we have $\\mu_{\\text{opt}} = \\sqrt{a^{2}+1} = a\\sqrt{1+1/a^{2}} \\sim a(1+\\frac{1}{2a^{2}}) = a+\\frac{1}{2a}$.\nSo, $\\mu_{\\text{opt}} \\to a$ as $a\\to\\infty$.\nLet's analyze the terms in $V_{\\text{asym}}(\\mu_{\\text{opt}})$ as $a\\to\\infty$:\nThe pre-factor denominator: $a+\\mu_{\\text{opt}} = a+\\sqrt{a^{2}+1} \\sim a+a = 2a$.\nThe exponent: $-\\frac{a^{2}}{2} - a\\mu_{\\text{opt}} + \\frac{\\mu_{\\text{opt}}^{2}}{2} = -\\frac{a^{2}}{2} - a\\sqrt{a^{2}+1} + \\frac{a^{2}+1}{2} = - a\\sqrt{a^{2}+1} + \\frac{1}{2}$.\nUsing the expansion $\\sqrt{a^{2}+1} \\sim a+\\frac{1}{2a}$:\n$$\n-a\\left(a+\\frac{1}{2a}\\right) + \\frac{1}{2} = -a^{2}-\\frac{1}{2}+\\frac{1}{2} = -a^{2}\n$$\nMore formally, the exponent is $-a\\sqrt{a^2+1} + 1/2 = -a^2\\sqrt{1+1/a^2} + 1/2 = -a^2(1+1/(2a^2)-O(a^{-4})) + 1/2 = -a^2-1/2+O(a^{-2}) + 1/2 = -a^2 + O(a^{-2})$.\nAs $a\\to\\infty$, the exponent tends to $-a^{2}$.\n\nCombining these results, the minimal leading-order asymptotic variance is:\n$$\nV_{\\text{asym}}(\\mu_{\\text{opt}}) \\sim \\frac{1}{\\sqrt{2\\pi}(2a)} \\exp(-a^{2}) = \\frac{\\exp(-a^{2})}{2a\\sqrt{2\\pi}}\n$$\nThis is the minimal leading-order asymptotic expression for the variance.", "answer": "$$\\boxed{\\frac{\\exp(-a^{2})}{2a\\sqrt{2\\pi}}}$$", "id": "3360206"}]}