{"hands_on_practices": [{"introduction": "The Marsaglia polar method's elegance includes an acceptance-rejection step to sample uniformly from a unit disk. This practice delves into the efficiency of this core mechanism by asking you to derive the expected number of trials needed to generate one accepted pair. By working from first principles, you will gain a deeper appreciation for the algorithm's performance and the probabilistic machinery that governs it [@problem_id:3324394].", "problem": "Consider the acceptance stage of the Marsaglia polar method for generating independent standard normal variates. In each proposal, a pair $(V_1,V_2)$ is drawn uniformly over the square $[-1,1] \\times [-1,1]$, and the proposal is accepted if $S = V_1^2 + V_2^2  1$. Proposals are repeated independently until the first acceptance occurs. Let $N$ be the total number of proposal pairs required to obtain the first accepted pair.\n\nStarting from the definitions of the continuous uniform distribution and planar (Lebesgue) measure, derive the acceptance probability $p$ for a single proposal. Then, using the definition of the geometric distribution as the count of independent and identically distributed Bernoulli trials until the first success, and without invoking precomputed mean or variance formulas, derive closed-form analytic expressions for the expected value $\\mathbb{E}[N]$ and the variance $\\operatorname{Var}(N)$ in terms of $\\pi$. Your derivations should proceed from series identities that you justify from first principles.\n\nProvide your final answers for $\\mathbb{E}[N]$ and $\\operatorname{Var}(N)$ as exact analytical expressions. No rounding is required, and no units are involved.", "solution": "The user wants to solve the problem by starting with the problem statement validation.\n\n### Step 1: Extract Givens\n- A pair of random variables $(V_1, V_2)$ is drawn uniformly over the square $[-1, 1] \\times [-1, 1]$.\n- A proposal is accepted if $S = V_1^2 + V_2^2  1$.\n- Proposals are independent and repeated until the first acceptance.\n- The random variable $N$ is the total number of proposal pairs required to obtain the first accepted pair.\n- The derivation of the acceptance probability $p$ must start from the definitions of the continuous uniform distribution and planar measure.\n- The derivations of the expected value $\\mathbb{E}[N]$ and the variance $\\operatorname{Var}(N)$ must be from first principles, using justified series identities, and not by invoking precomputed formulas for the mean or variance of the geometric distribution.\n- The final answers for $\\mathbb{E}[N]$ and $\\operatorname{Var}(N)$ must be exact analytical expressions in terms of $\\pi$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is grounded in the well-established Marsaglia polar method, a standard algorithm in the field of computational statistics and Monte Carlo methods. All premises are factually correct.\n- **Well-Posed:** The problem is clearly stated, and all quantities are well-defined. The question asks for a derivation of specific statistical properties of a random variable defined by a clear probabilistic process. A unique, stable, and meaningful solution exists.\n- **Objective:** The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n- **Completeness and Consistency:** The problem is self-contained. It provides all necessary information (the sampling domain, the acceptance criterion, the definition of the random variable $N$) to derive the requested quantities. There are no contradictions.\n- **Other Flaws:** The problem does not exhibit any other flaws such as being unrealistic, non-formalizable, ill-posed, tautological, or unverifiable. The requirement to derive results from first principles is a standard pedagogical tool to test fundamental understanding.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution will be provided as requested.\n\n### Solution Derivation\n\nThe solution is developed in two parts as required: first, the derivation of the acceptance probability $p$, and second, the derivation of the expectation $\\mathbb{E}[N]$ and variance $\\operatorname{Var}(N)$ from first principles.\n\n**Part 1: Derivation of the Acceptance Probability $p$**\n\nThe pair of random variables $(V_1, V_2)$ is drawn from a continuous uniform distribution over the square region $\\mathcal{C} = [-1, 1] \\times [-1, 1]$. The area of this square, which is a planar Lebesgue measure, is given by\n$$\n\\text{Area}(\\mathcal{C}) = (1 - (-1)) \\times (1 - (-1)) = 2 \\times 2 = 4\n$$\nBy the definition of a continuous uniform distribution over a region, the joint probability density function (PDF) $f_{V_1, V_2}(v_1, v_2)$ is constant within the region and zero outside. The value of the constant is the reciprocal of the area of the region.\n$$\nf_{V_1, V_2}(v_1, v_2) =\n\\begin{cases}\n\\frac{1}{\\text{Area}(\\mathcal{C})} = \\frac{1}{4}  \\text{if } (v_1, v_2) \\in \\mathcal{C} \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nA proposal is accepted if the drawn pair $(V_1, V_2)$ satisfies the condition $S = V_1^2 + V_2^2  1$. This condition defines an open disk $\\mathcal{D}$ of radius $r=1$ centered at the origin $(0,0)$. The acceptance probability $p$ is the probability that a point uniformly chosen from the square $\\mathcal{C}$ lies within the disk $\\mathcal{D}$. This probability is calculated by integrating the joint PDF over the acceptance region $\\mathcal{D}$.\n$$\np = P(V_1^2 + V_2^2  1) = \\iint_{\\mathcal{D}} f_{V_1, V_2}(v_1, v_2) \\, dv_1 \\, dv_2\n$$\nThe disk $\\mathcal{D}$ is entirely contained within the square $\\mathcal{C}$ (since for any point $(v_1,v_2)$ in $\\mathcal{D}$, we have $v_1^2  1$ and $v_2^2  1$, which implies $|v_1|  1$ and $|v_2|  1$). Therefore, the PDF is constant at $\\frac{1}{4}$ over the entire integration domain $\\mathcal{D}$.\n$$\np = \\iint_{\\mathcal{D}} \\frac{1}{4} \\, dv_1 \\, dv_2 = \\frac{1}{4} \\iint_{\\mathcal{D}} \\, dv_1 \\, dv_2\n$$\nThe integral $\\iint_{\\mathcal{D}} \\, dv_1 \\, dv_2$ is simply the area of the disk $\\mathcal{D}$. The area of a disk with radius $r=1$ is $\\pi r^2 = \\pi(1)^2 = \\pi$.\nThus, the acceptance probability is the ratio of the area of the acceptance region to the area of the sampling region:\n$$\np = \\frac{\\text{Area}(\\mathcal{D})}{\\text{Area}(\\mathcal{C})} = \\frac{\\pi}{4}\n$$\n\n**Part 2: Derivation of $\\mathbb{E}[N]$ and $\\operatorname{Var}(N)$**\n\nThe random variable $N$ represents the number of independent and identically distributed Bernoulli trials required to obtain the first success. Each trial is a proposal, and a \"success\" is an accepted proposal. The probability of success in any single trial is $p = \\frac{\\pi}{4}$. This defines a geometric distribution for $N$. The probability mass function (PMF) of $N$ is given by\n$$\nP(N=k) = (1-p)^{k-1}p \\quad \\text{for } k \\in \\{1, 2, 3, \\ldots\\}\n$$\nFor convenience, let $q = 1-p$. The PMF is $P(N=k) = q^{k-1}p$.\n\nTo derive the expectation and variance from first principles, we rely on the formula for a geometric series and its derivatives. For any real number $|x|  1$, the geometric series is\n$$\n\\sum_{k=0}^{\\infty} x^k = \\frac{1}{1-x}\n$$\nDifferentiating with respect to $x$ (which is valid for $|x|1$) gives\n$$\n\\frac{d}{dx} \\sum_{k=0}^{\\infty} x^k = \\sum_{k=1}^{\\infty} kx^{k-1} = \\frac{d}{dx} (1-x)^{-1} = ( -1)(1-x)^{-2}(-1) = \\frac{1}{(1-x)^2}\n$$\nDifferentiating a second time gives\n$$\n\\frac{d}{dx} \\sum_{k=1}^{\\infty} kx^{k-1} = \\sum_{k=2}^{\\infty} k(k-1)x^{k-2} = \\frac{d}{dx} (1-x)^{-2} = (-2)(1-x)^{-3}(-1) = \\frac{2}{(1-x)^3}\n$$\n\n**Derivation of $\\mathbb{E}[N]$**\nBy the definition of expected value for a discrete random variable:\n$$\n\\mathbb{E}[N] = \\sum_{k=1}^{\\infty} k \\cdot P(N=k) = \\sum_{k=1}^{\\infty} k q^{k-1} p = p \\sum_{k=1}^{\\infty} k q^{k-1}\n$$\nSince $p = \\frac{\\pi}{4}$, we have $0  p  1$, which implies $0  q  1$. We can use the first derivative identity derived above with $x=q$:\n$$\n\\mathbb{E}[N] = p \\left( \\frac{1}{(1-q)^2} \\right)\n$$\nSubstituting $1-q = 1-(1-p) = p$, we obtain:\n$$\n\\mathbb{E}[N] = p \\left( \\frac{1}{p^2} \\right) = \\frac{1}{p}\n$$\nFinally, substituting $p = \\frac{\\pi}{4}$:\n$$\n\\mathbb{E}[N] = \\frac{1}{\\pi/4} = \\frac{4}{\\pi}\n$$\n\n**Derivation of $\\operatorname{Var}(N)$**\nThe variance is defined as $\\operatorname{Var}(N) = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2$. We first need to compute $\\mathbb{E}[N^2]$.\n$$\n\\mathbb{E}[N^2] = \\sum_{k=1}^{\\infty} k^2 \\cdot P(N=k) = \\sum_{k=1}^{\\infty} k^2 q^{k-1} p = p \\sum_{k=1}^{\\infty} k^2 q^{k-1}\n$$\nWe use the identity $k^2 = k(k-1) + k$ to decompose the sum:\n$$\n\\sum_{k=1}^{\\infty} k^2 q^{k-1} = \\sum_{k=1}^{\\infty} (k(k-1) + k) q^{k-1} = \\sum_{k=1}^{\\infty} k(k-1)q^{k-1} + \\sum_{k=1}^{\\infty} k q^{k-1}\n$$\nThe first term in the sum is zero for $k=1$, so a more precise form is $\\sum_{k=2}^{\\infty} k(k-1)q^{k-1}$.\nThe second sum is the one we evaluated for the expectation: $\\sum_{k=1}^{\\infty} k q^{k-1} = \\frac{1}{(1-q)^2} = \\frac{1}{p^2}$.\nFor the first sum, we use the second derivative identity.\n$$\n\\sum_{k=2}^{\\infty} k(k-1)q^{k-1} = q \\sum_{k=2}^{\\infty} k(k-1)q^{k-2}\n$$\nUsing our identity with $x=q$:\n$$\n\\sum_{k=2}^{\\infty} k(k-1)q^{k-1} = q \\left( \\frac{2}{(1-q)^3} \\right) = \\frac{2q}{(1-q)^3} = \\frac{2q}{p^3}\n$$\nNow, we can assemble the expression for $\\mathbb{E}[N^2]$:\n$$\n\\mathbb{E}[N^2] = p \\left( \\frac{2q}{p^3} + \\frac{1}{p^2} \\right) = \\frac{2q}{p^2} + \\frac{1}{p}\n$$\nNow, we compute the variance:\n$$\n\\operatorname{Var}(N) = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2 = \\left( \\frac{2q}{p^2} + \\frac{1}{p} \\right) - \\left( \\frac{1}{p} \\right)^2 = \\frac{2q}{p^2} + \\frac{1}{p} - \\frac{1}{p^2}\n$$\n$$\n\\operatorname{Var}(N) = \\frac{2q + p - 1}{p^2}\n$$\nSubstitute $q=1-p$:\n$$\n\\operatorname{Var}(N) = \\frac{2(1-p) + p - 1}{p^2} = \\frac{2 - 2p + p - 1}{p^2} = \\frac{1-p}{p^2}\n$$\nFinally, substituting $p = \\frac{\\pi}{4}$:\n$$\n\\operatorname{Var}(N) = \\frac{1 - \\frac{\\pi}{4}}{(\\frac{\\pi}{4})^2} = \\frac{\\frac{4-\\pi}{4}}{\\frac{\\pi^2}{16}} = \\frac{4-\\pi}{4} \\cdot \\frac{16}{\\pi^2} = \\frac{4(4-\\pi)}{\\pi^2} = \\frac{16-4\\pi}{\\pi^2}\n$$\nThe requested expressions for the expected value and variance have been derived from first principles.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{4}{\\pi}  \\frac{16-4\\pi}{\\pi^2} \\end{pmatrix}}\n$$", "id": "3324394"}, {"introduction": "Translating theory into a functional and correct implementation is a cornerstone of computational science. This comprehensive exercise guides you through not only implementing the Marsaglia polar method but also designing and executing a rigorous statistical validation plan. You will learn to use a suite of tests to verify that the generated variates correctly exhibit the moments, distributional shape, and independence of the standard normal distribution [@problem_id:3324448].", "problem": "You are to implement and validate a generator for independent and identically distributed standard normal variates using the Marsaglia polar method. Your program must: (i) implement a transformation from independent Uniform on $\\left[-1,1\\right]$ inputs into pairs of outputs denoted $Z_1,Z_2$ that are purported to be independent and identically distributed standard normal random variables, (ii) design and apply a statistical validation plan to assess whether the outputs are indeed independent and identically distributed as $\\mathcal{N}(0,1)$, and (iii) report the outcomes of all tests as specified booleans.\n\nThe statistical validation plan must be principle-based and constructed from the following foundational facts and definitions:\n- Acceptance–rejection logic for random number generation and the change of variables principle.\n- The probability integral transform: if $X$ has continuous cumulative distribution function (CDF) $F_X$, then $U = F_X(X)$ is Uniform on $\\left[0,1\\right]$.\n- Under the hypothesis of $\\mathcal{N}(0,1)$ marginals and independence:\n  - If $\\left\\{Z_i\\right\\}_{i=1}^n$ are independent and identically distributed as $\\mathcal{N}(0,1)$, then the sample mean $\\bar{Z}$ is approximately $\\mathcal{N}\\left(0,1/n\\right)$ and the statistic $(n-1)S^2$ has a chi-square distribution with $n-1$ degrees of freedom, where $S^2$ is the unbiased sample variance.\n  - The Kolmogorov–Smirnov (KS) one-sample goodness-of-fit test can be applied to Uniform on $\\left[0,1\\right]$ data to test the hypothesis of uniformity.\n  - The sample Pearson correlation coefficient $r$ for two independent normal marginals admits the Student’s $t$ transformation $T = r\\sqrt{\\frac{n-2}{1-r^2}}$ which follows a Student’s $t$ distribution with $n-2$ degrees of freedom under the null hypothesis of zero correlation.\n  - A quantile–quantile (QQ) plot compares ordered sample values to theoretical quantiles. Without plotting, a quantitative proxy can be obtained by a least squares fit $Y = a + bQ$ between ordered sample values $Y$ and theoretical quantiles $Q$ of the target distribution; for correct $\\mathcal{N}(0,1)$ data, one expects $a \\approx 0$ and $b \\approx 1$.\n\nYou must implement the following tests on the produced samples for each test case:\n- Moment checks:\n  - For each marginal separately, test the sample mean at level $\\alpha$ using the asymptotic normal reference for $\\bar{Z}$: accept if $\\left|\\bar{Z}\\right| \\le z_{1-\\alpha/2}/\\sqrt{n}$, where $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$ quantile of the standard normal distribution.\n  - For each marginal separately, test the sample variance using the chi-square reference: accept if $\\chi^2_{\\,\\alpha/2,n-1}/(n-1) \\le S^2 \\le \\chi^2_{\\,1-\\alpha/2,n-1}/(n-1)$, where $\\chi^2_{\\,p,\\nu}$ is the $p$ quantile of the chi-square distribution with $\\nu$ degrees of freedom.\n  - The overall moment check passes if all mean and variance checks for both marginals pass simultaneously.\n- Goodness-of-fit on transformed marginals via the probability integral transform:\n  - For each marginal $Z$, compute $U = \\Phi(Z)$, where $\\Phi$ is the CDF of $\\mathcal{N}(0,1)$, and perform a one-sample KS test of $U$ against Uniform on $\\left[0,1\\right]$ at level $\\alpha$. Each marginal yields a separate boolean acceptance.\n- Quantile–quantile metrics (no plotting):\n  - For each marginal, compute the theoretical standard normal quantiles $\\left\\{q_i\\right\\}_{i=1}^n$ at probabilities $p_i = \\frac{i-0.5}{n}$ for $i=1,\\dots,n$, and let $\\left\\{y_i\\right\\}_{i=1}^n$ be the ordered sample values. Fit the least squares line $y_i \\approx a + b q_i$. Declare two separate passes per marginal: slope pass if $\\left|b-1\\right| \\le \\delta_b$ and intercept pass if $\\left|a\\right| \\le \\delta_a$. Aggregate across marginals by conjunction, i.e., both marginals must pass the slope criterion to set the aggregated slope pass to true, and similarly for the intercept criterion.\n- Independence check:\n  - Compute the sample Pearson correlation $r$ between $Z_1$ and $Z_2$ from the $n$ generated pairs and test $H_0: \\rho = 0$ at level $\\alpha$ using the statistic $T = r\\sqrt{\\frac{n-2}{1-r^2}}$ with a two-sided rejection region based on the Student’s $t$ distribution with $n-2$ degrees of freedom. Accept if $\\left|T\\right| \\le t_{1-\\alpha/2,n-2}$, where $t_{1-\\alpha/2,\\nu}$ is the $(1-\\alpha/2)$ quantile of the Student’s $t$ distribution with $\\nu$ degrees of freedom.\n\nProvide and use the following test suite of parameter values:\n- Test case $1$ (nominal correct generator): $n = 50000$, seed $= 20231119$, significance level $\\alpha = 0.01$, QQ slope tolerance $\\delta_b = 0.03$, QQ intercept tolerance $\\delta_a = 0.03$.\n- Test case $2$ (mean-biased marginals): $n = 30000$, seed $= 123$, significance level $\\alpha = 0.01$, QQ slope tolerance $\\delta_b = 0.03$, QQ intercept tolerance $\\delta_a = 0.03$, inject a constant bias $\\mu = 0.08$ to both marginals after generation to create a deviation from $\\mathcal{N}(0,1)$.\n- Test case $3$ (correlated outputs): $n = 40000$, seed $= 98765$, significance level $\\alpha = 0.01$, QQ slope tolerance $\\delta_b = 0.03$, QQ intercept tolerance $\\delta_a = 0.03$, construct correlation $\\rho = 0.2$ by setting $Z_2 = \\rho Z_1 + \\sqrt{1-\\rho^2}\\,W$ with $W$ an independent standard normal sample.\n\nYour program must:\n- Implement the Marsaglia polar method to generate candidate pairs $(Z_1,Z_2)$ per iteration from independent Uniform on $\\left[-1,1\\right]$ inputs without using any pre-existing normal random number generator for the nominal output stream.\n- For test case $2$, add the specified bias $\\mu$ after generation.\n- For test case $3$, generate $Z_1$ as above, then form $Z_2$ as specified to enforce correlation $\\rho$.\n- For each test case, compute the six booleans in the following fixed order:\n  $[$moment check pass, KS on transformed $Z_1$ pass, KS on transformed $Z_2$ pass, QQ slope aggregated pass, QQ intercept aggregated pass, independence pass$]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the six booleans in the specified order. For example, an output with three test cases must look like $[[b_{11},b_{12},b_{13},b_{14},b_{15},b_{16}],[b_{21},\\dots,b_{26}],[b_{31},\\dots,b_{36}]]$ with literal boolean values.\n\nAngles are not involved, and no physical units are involved. All probabilities must be expressed as decimals (e.g., $0.01$), not with a percentage sign.", "solution": "The problem requires the implementation and statistical validation of the Marsaglia polar method for generating independent and identically distributed (i.i.d.) standard normal random variates. The solution involves two main components: the generation algorithm itself and a comprehensive, principle-based validation suite.\n\n### The Marsaglia Polar Method\n\nThe Marsaglia polar method is an algorithm to transform a pair of independent random variables uniformly distributed on the interval $[-1, 1]$ into a pair of independent standard normal random variables. It is a modification of the Box-Muller transform that avoids the use of computationally expensive trigonometric functions.\n\nThe algorithm proceeds as follows:\n$1$. Generate two independent random variables, $U_1$ and $U_2$, from the uniform distribution on $[-1, 1]$. These can be visualized as the coordinates of a random point within a square of side length $2$ centered at the origin.\n$2$. Compute $S = U_1^2 + U_2^2$. This is the squared distance of the point $(U_1, U_2)$ from the origin.\n$3$. Apply an acceptance-rejection step: if $S \\ge 1$ or $S = 0$, the point $(U_1, U_2)$ lies outside or at the very center of the unit disk. In this case, the pair is rejected, and the algorithm returns to step $1$. Otherwise, the point is accepted. This procedure ensures that we are sampling uniformly from the interior of the unit disk, excluding the origin. The probability of acceptance in any given trial is the ratio of the area of the unit disk ($\\pi$) to the area of the square ($4$), which is $\\pi/4$.\n$4$. If the pair $(U_1, U_2)$ is accepted, the transformation is applied to generate two independent standard normal variates, $Z_1$ and $Z_2$:\n$$ Z_1 = U_1 \\sqrt{\\frac{-2 \\ln S}{S}} $$\n$$ Z_2 = U_2 \\sqrt{\\frac{-2 \\ln S}{S}} $$\n\nThe validity of this transformation rests on the change of variables theorem in probability. When $(U_1, U_2)$ is uniform on the unit disk, its squared radius $S$ is uniform on $[0,1]$. The transformation $R = \\sqrt{-2 \\ln S}$ produces a random variable with a Rayleigh distribution, which is characteristic of the magnitude of a 2D standard normal vector. By scaling the original uniform components $U_1$ and $U_2$ by the factor $\\sqrt{-2 \\ln S / S}$, the resulting $(Z_1, Z_2)$ can be shown to have a bivariate standard normal distribution with zero correlation, which for the normal distribution implies independence.\n\n### Statistical Validation Plan\n\nTo verify that the generated samples $\\{ (Z_{1,i}, Z_{2,i}) \\}_{i=1}^n$ are indeed i.i.d. draws from $\\mathcal{N}(0,1)$, a series of statistical tests are performed. These tests scrutinize different properties of the sample data against the null hypothesis ($H_0$) that they follow the target distribution. A significance level $\\alpha$ is used to control the probability of Type I error (incorrectly rejecting a true $H_0$).\n\n1.  **Moment Checks**: This test verifies if the first two moments (mean and variance) of the sample data are consistent with the theoretical values of a standard normal distribution, which are $\\mu=0$ and $\\sigma^2=1$.\n    - **Mean Test**: Based on the Central Limit Theorem, for a large sample size $n$, the sample mean $\\bar{Z}$ is approximately distributed as $\\mathcal{N}(0, 1/n)$. The test accepts $H_0$ if the observed sample mean is not statistically significantly different from $0$. The condition is $|\\bar{Z}| \\le z_{1-\\alpha/2} / \\sqrt{n}$, where $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution. This is performed for both marginals, $Z_1$ and $Z_2$.\n    - **Variance Test**: For samples drawn from a normal distribution, the statistic $(n-1)S^2/\\sigma^2$ follows a chi-square distribution with $n-1$ degrees of freedom. Under $H_0$, $\\sigma^2=1$, so we test if the sample variance $S^2$ falls within the interval $[\\frac{\\chi^2_{\\alpha/2, n-1}}{n-1}, \\frac{\\chi^2_{1-\\alpha/2, n-1}}{n-1}]$, where $\\chi^2_{p, \\nu}$ is the $p$-quantile of the chi-square distribution with $\\nu$ degrees of freedom. This is also performed for both marginals.\n    - The overall moment check passes only if all four individual checks (mean and variance for both $Z_1$ and $Z_2$) pass.\n\n2.  **Goodness-of-Fit (Kolmogorov-Smirnov Test)**: This test assesses the overall shape of the marginal distributions. It leverages the probability integral transform (PIT), which states that if a random variable $Z$ has a continuous cumulative distribution function (CDF) $F_Z$, then the variable $U = F_Z(Z)$ is uniformly distributed on $[0, 1]$.\n    - For each marginal sample $\\{Z_i\\}$, we compute the transformed sample $\\{U_i = \\Phi(Z_i)\\}$, where $\\Phi$ is the CDF of the $\\mathcal{N}(0,1)$ distribution.\n    - A one-sample Kolmogorov-Smirnov (KS) test is then used to compare the empirical distribution of $\\{U_i\\}$ against the theoretical uniform distribution on $[0,1]$. $H_0$ is accepted if the p-value of the test is greater than or equal to the significance level $\\alpha$. This yields a separate boolean result for each marginal, $Z_1$ and $Z_2$.\n\n3.  **Quantile-Quantile (QQ) Metrics**: This provides a quantitative assessment of how well the sample quantiles match the theoretical quantiles of a standard normal distribution, which is particularly sensitive to deviations in the tails of the distribution.\n    - The sorted sample values, $\\{y_i\\}_{i=1}^n$, are regressed against the theoretical quantiles, $\\{q_i\\}_{i=1}^n$, of the $\\mathcal{N}(0,1)$ distribution. The theoretical quantiles are calculated at probabilities $p_i = (i - 0.5)/n$ for $i=1, \\dots, n$.\n    - A linear model $y_i \\approx a + b q_i$ is fitted using ordinary least squares. For data that truly follows $\\mathcal{N}(0,1)$, one would expect the intercept $a \\approx 0$ and the slope $b \\approx 1$.\n    - The test passes if $|a| \\le \\delta_a$ and $|b-1| \\le \\delta_b$, where $\\delta_a$ and $\\delta_b$ are predefined small tolerances. The final output provides two aggregated booleans: one for the slope test (passing only if both marginals pass) and one for the intercept test (passing only if both marginals pass).\n\n4.  **Independence Check**: This test verifies the crucial property that the generated variates $Z_1$ and $Z_2$ are independent. For bivariate normal variables, independence is equivalent to zero correlation.\n    - The sample Pearson correlation coefficient, $r$, is calculated for the $n$ pairs $(Z_{1,i}, Z_{2,i})$.\n    - Under the null hypothesis of zero population correlation ($\\rho=0$), the test statistic $T = r \\sqrt{\\frac{n-2}{1-r^2}}$ follows a Student's $t$-distribution with $n-2$ degrees of freedom.\n    - $H_0$ is accepted if the absolute value of the observed statistic, $|T|$, does not exceed the critical value $t_{1-\\alpha/2, n-2}$, the $(1-\\alpha/2)$-quantile of the specified $t$-distribution.\n\nBy combining these tests, the validation plan provides a robust assessment of the generator's output quality, checking for correct moments, correct distributional shape, and independence. The test cases are designed to probe the sensitivity of this plan: test case $1$ should pass all tests, case $2$ should fail tests related to the mean and distributional shape, and case $3$ should fail the independence test while passing marginal distribution tests.", "answer": "[[True, True, True, True, True, True],[False, False, False, True, False, True],[True, True, True, True, True, False]]", "id": "3324448"}, {"introduction": "A simulation is only as reliable as its source of randomness, a principle often summarized as \"garbage in, garbage out.\" This practice explores the critical sensitivity of the Marsaglia method to the quality of its input uniform variates. By using a historically flawed random number generator and specialized diagnostics, you will investigate how defects in the input sequence propagate through the algorithm and manifest as detectable structural flaws in the output normal variates [@problem_id:3324468].", "problem": "You are asked to study the propagation of dependence from a uniform random input sequence through the Marsaglia polar method for normal variate generation, and to design and implement diagnostics that can detect induced correlations in the resulting normal pair outputs. The implementation must be a complete, runnable program that produces a single line of output.\n\nStart from the following foundational base:\n\n- A pair of independent and identically distributed (i.i.d.) standard normal random variables $(Z_1,Z_2)$ has joint probability density function (PDF) $f(z_1,z_2) = \\frac{1}{2\\pi}\\exp\\!\\left(-\\frac{z_1^2+z_2^2}{2}\\right)$, and the squared radius $R^2 = Z_1^2 + Z_2^2$ has a chi-square distribution with $2$ degrees of freedom, equivalently an exponential distribution with mean $2$. In polar coordinates, $\\Theta = \\operatorname{atan2}(Z_2,Z_1) \\in [-\\pi,\\pi)$ is uniform on $[-\\pi,\\pi)$ and independent of $R$.\n- The Marsaglia polar method is an acceptance-rejection approach that transforms a pair of independent uniform variates on $[-1,1]$ into a pair of standard normal variates by first accepting points uniformly from the unit disk and then applying an appropriate radial scaling so that the resulting distribution matches that of $(Z_1,Z_2)$. You must implement the standard acceptance-rejection mapping that yields the correct normal law, but you must not assume any spurious independence beyond what is implied by the input sequence.\n- A Linear Congruential Generator (LCG) is defined by the recurrence $X_{n+1} = (a X_n + c) \\bmod m$ with integer modulus $m$, multiplier $a$, increment $c$, and seed $X_0$. The resulting uniforms are $U_n = X_n / m \\in [0,1)$.\n- Dependence in random variables is preserved under measurable mappings: if $(U_1,U_2,U_3,\\dots)$ is a dependent sequence and $T$ is a deterministic transformation, then $(T(U_1,U_2),T(U_3,U_4),\\dots)$ generally remains dependent. Acceptance-rejection based on a function of the inputs can also interact with input dependence, potentially creating complex selection bias.\n\nTasks to complete:\n\n1. Implement the Marsaglia polar method that consumes a stream of pairs $(V_1,V_2)$ uniformly distributed over $[-1,1]^2$, rejects any pair with $S = V_1^2 + V_2^2 \\ge 1$ or $S=0$, and transforms accepted pairs into standard normal pairs $(Z_1,Z_2)$ using a deterministic scaling that yields the correct law for $(Z_1,Z_2)$. Your implementation must not assume independence unless it is guaranteed by the input generator.\n2. Implement uniform sequence sources with the following configurations:\n   - Independent baseline using a Permuted Congruential Generator (PCG) as provided by a modern numerical library. From this, generate i.i.d. uniforms on $[0,1)$ and form independent pairs.\n   - A poor Linear Congruential Generator (LCG) using parameters $m=2^{31}$, $a=65539$, $c=0$ (commonly known as RANDU) to generate a single uniform stream $U_n$ and form consecutive pairs $(U_{2k},U_{2k+1})$.\n   - A degenerate pairing that forces perfect correlation within each candidate pair by setting $U_{2k+1}=U_{2k}$ for each pair, using the same underlying LCG stream for $U_{2k}$.\n3. Propose and implement diagnostics to detect induced correlations or structural deviations in $(Z_1,Z_2)$ produced by the Marsaglia polar method:\n   - Within-pair Pearson correlation $\\hat\\rho = \\operatorname{Corr}(Z_1,Z_2)$.\n   - Angular uniformity test: compute $\\Theta = \\operatorname{atan2}(Z_2,Z_1) \\in (-\\pi,\\pi]$, rescale to $W = (\\Theta + \\pi)/(2\\pi) \\in [0,1)$, and perform a one-sample Kolmogorov–Smirnov (K-S) test against the uniform distribution on $[0,1)$; report the $p$-value.\n   - Lag-$1$ autocorrelation of the flattened output sequence $Y = (Z_1^{(1)}, Z_2^{(1)}, Z_1^{(2)}, Z_2^{(2)}, \\dots)$, defined by $\\hat\\rho_1 = \\frac{\\sum_{t=1}^{N-1} (Y_t - \\bar Y)(Y_{t+1}-\\bar Y)}{\\sum_{t=1}^{N} (Y_t - \\bar Y)^2}$ where $N$ is the total length of $Y$.\n   - Chi-square test for independence using a $b \\times b$ grid with $b=6$ based on quantile binning for the standard normal marginals: compute cell counts on a grid defined by the standard normal quantiles at probabilities $i/b$ for $i=0,1,\\dots,b$, and compute the chi-square statistic $\\sum_{i,j} \\frac{(O_{ij}-E_{ij})^2}{E_{ij}}$ with degrees of freedom $(b-1)(b-1)$, where $E_{ij}$ are the independence-expected counts formed from marginals; report the $p$-value.\n4. For reproducibility, use the following test suite. Each case specifies a generator configuration, a seed, and the number of output pairs to generate:\n   - Case A (independent baseline): PCG seed $= 20231011$, number of output pairs $= 40000$.\n   - Case B (LCG RANDU sequential pairs): parameters $m=2^{31}$, $a=65539$, $c=0$, seed $=1$, number of output pairs $= 40000$.\n   - Case C (degenerate equal-pair mapping): same RANDU parameters as Case B, but force $U_{2k+1}=U_{2k}$ in candidate pairs, seed $=1$, number of output pairs $= 40000$.\n   - Case D (small-sample independent sanity check): PCG seed $= 42$, number of output pairs $= 500$.\n5. Output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The result for each test case must itself be a list in the order $[\\hat\\rho, p_{\\text{angle}}, \\hat\\rho_1, p_{\\chi^2}]$, so the final output is a list of four lists in the order of cases A, B, C, D, for example, $[[r_{A},p_{A},a_{A},q_{A}],[r_{B},p_{B},a_{B},q_{B}],[r_{C},p_{C},a_{C},q_{C}],[r_{D},p_{D},a_{D},q_{D}]]$.", "solution": "The problem requires an investigation into the effects of input sequence dependence on the output of the Marsaglia polar method for generating standard normal random variates. This is achieved by implementing the method, sourcing inputs from different quality random number generators, and applying a suite of statistical diagnostics to the output.\n\nThe problem is scientifically and mathematically well-posed. It is grounded in the established theory of random number generation and statistical testing. All parameters, algorithms, and test cases are specified unambiguously, allowing for a unique and verifiable solution. The chosen test cases—a high-quality generator, a notoriously poor one, and a pathologically dependent one—provide a robust framework for demonstrating the propagation of defects through a non-linear transformation.\n\n**1. Theoretical Foundation of the Marsaglia Polar Method**\n\nThe Marsaglia polar method generates a pair of independent standard normal random variables, $(Z_1, Z_2)$, by transforming a pair of random variables, $(V_1, V_2)$, chosen uniformly from the interior of the unit disk. The joint probability density function (PDF) for an i.i.d. standard normal pair is given in Cartesian coordinates by $f(z_1, z_2) = \\frac{1}{2\\pi} e^{-(z_1^2 + z_2^2)/2}$. In polar coordinates, where $z_1 = r \\cos\\theta$ and $z_2 = r \\sin\\theta$, the squared radius $R^2 = Z_1^2 + Z_2^2$ follows an exponential distribution with mean $2$ (a chi-squared distribution with $2$ degrees of freedom), and the angle $\\Theta = \\operatorname{atan2}(Z_2, Z_1)$ is uniformly distributed on $[-\\pi, \\pi)$ and is independent of $R$.\n\nThe method proceeds as follows:\n1.  Generate a pair of independent uniform variates $(U_1, U_2)$ on $[0,1)$.\n2.  Transform them to be uniform on $[-1,1]$: $V_1 = 2U_1 - 1$ and $V_2 = 2U_2 - 1$. The pair $(V_1, V_2)$ is uniformly distributed over the square $[-1,1]^2$.\n3.  Calculate $S = V_1^2 + V_2^2$.\n4.  If $S=0$ or $S \\ge 1$, reject the pair and return to step $1$. Otherwise, accept. The accepted pairs $(V_1, V_2)$ are uniformly distributed inside the unit disk.\n5.  If we write the accepted point in polar coordinates, $V_1 = \\tilde{R} \\cos\\Phi$ and $V_2 = \\tilde{R} \\sin\\Phi$, then the angle $\\Phi$ is uniform on $[0, 2\\pi)$, and the squared radius $S = \\tilde{R}^2$ is uniform on $(0,1)$. The core of the method is to transform this squared radius $S$ into a new squared radius $R^2$ that has the correct exponential distribution. The required transformation is $R^2 = -2 \\ln S$. Since $S \\sim U(0,1)$, the cumulative distribution function (CDF) of $Y = -2 \\ln S$ is $P(Y \\le y) = P(-2 \\ln S \\le y) = P(\\ln S \\ge -y/2) = P(S \\ge e^{-y/2}) = 1 - e^{-y/2}$ for $y0$. This is the CDF of an exponential distribution with mean $2$.\n6.  The final standard normal variates are constructed by scaling the original coordinates $(V_1, V_2)$ by a factor that replaces their radius with the newly generated one. The new radius is $R = \\sqrt{-2\\ln S}$. The original radius is $\\tilde{R} = \\sqrt{S}$. The scaling factor is $R/\\tilde{R} = \\sqrt{-2\\ln S / S}$.\n    $$ Z_1 = V_1 \\cdot \\sqrt{\\frac{-2 \\ln S}{S}} $$\n    $$ Z_2 = V_2 \\cdot \\sqrt{\\frac{-2 \\ln S}{S}} $$\n    The angle of the resulting pair $(Z_1, Z_2)$ is the same as the angle of $(V_1, V_2)$, which is uniform. Thus, $(Z_1,Z_2)$ is a pair of i.i.d. standard normal variates, provided the input uniforms $(U_1, U_2)$ are i.i.d.\n\n**2. Input Sequence Generators**\n\nThe problem specifies three sources for the initial uniform sequences, designed to have different dependency structures.\n\n-   **Independent Baseline (PCG):** A modern, high-quality Permuted Congruential Generator (PCG) is used. It is known for excellent statistical properties, and the stream of variates it produces, $(U_0, U_1, U_2, \\dots)$, is for all practical purposes i.i.d. uniform on $[0,1)$. Pairs are formed as $(U_{2k}, U_{2k+1})$.\n-   **Poor LCG (RANDU):** A Linear Congruential Generator defined by the recurrence $X_{n+1} = (a X_n + c) \\pmod m$ with parameters $a=65539$, $c=0$, and $m=2^{31}$. Uniforms are $U_n = X_n / m$. This generator is infamous for strong correlations between successive values. For example, triplets $(U_i, U_{i+1}, U_{i+2})$ fall on a small number of planes. We form pairs as $(U_{2k}, U_{2k+1})$, which will inherit this dependence.\n-   **Degenerate Pairing:** This uses the same RANDU generator but forces perfect correlation within the candidate pairs by setting $U_{2k+1} = U_{2k}$. The input to the Marsaglia method is thus pairs of the form $(v,v)$, where $v = 2U_{2k}-1$.\n\n**3. Statistical Diagnostics**\n\nTo quantify the quality of the output normal pairs $(Z_1^{(i)}, Z_2^{(i)})$, four diagnostic tests are implemented.\n\n-   **Within-pair Pearson Correlation ($\\hat\\rho$):** This measures the linear correlation between $Z_1$ and $Z_2$ within each generated pair. For truly independent standard normal variates, the theoretical correlation is $0$.\n-   **Angular Uniformity (K-S Test):** The angle $\\Theta_i = \\operatorname{atan2}(Z_2^{(i)}, Z_1^{(i)})$ should be uniformly distributed on $(-\\pi, \\pi]$. We rescale the angles to $W_i = (\\Theta_i + \\pi) / (2\\pi)$ to lie on $[0,1)$ and perform a one-sample Kolmogorov-Smirnov (K-S) test against the standard uniform distribution. The resulting $p$-value indicates the likelihood of observing the given sample if the underlying distribution were uniform; a small $p$-value suggests non-uniformity.\n-   **Lag-1 Autocorrelation ($\\hat\\rho_1$):** This tests for correlation between consecutive values in the flattened output sequence $Y = (Z_1^{(1)}, Z_2^{(1)}, Z_1^{(2)}, Z_2^{(2)}, \\dots)$. It is sensitive to both within-pair correlation (e.g., between $Z_1^{(1)}$ and $Z_2^{(1)}$) and between-pair correlation (e.g., between $Z_2^{(1)}$ and $Z_1^{(2)}$). The statistic is computed as $\\hat\\rho_1 = \\frac{\\sum_{t=1}^{N-1} (Y_t - \\bar Y)(Y_{t+1}-\\bar Y)}{\\sum_{t=1}^{N} (Y_t - \\bar Y)^2}$, where $N$ is the total length of $Y$.\n-   **Chi-square Test for Independence ($p_{\\chi^2}$):** This is a non-parametric test for independence between $Z_1$ and $Z_2$. The joint sample space is partitioned into a $b \\times b$ grid, with $b=6$. The bin boundaries are defined by the quantiles of the standard normal distribution, such that each marginal bin has an expected probability of $1/b$. The Chi-square statistic, $\\chi^2 = \\sum_{i,j} \\frac{(O_{ij}-E_{ij})^2}{E_{ij}}$, compares the observed counts $O_{ij}$ in each cell $(i,j)$ to the counts $E_{ij}$ expected under the null hypothesis of independence. A large $\\chi^2$ value (and correspondingly small $p$-value) indicates a significant deviation from independence. The degrees of freedom are $(b-1)^2 = 25$.\n\n**4. Analysis of Test Cases**\n\n-   **Case A (PCG, $N=40000$):** As the baseline, all tests are expected to pass. $\\hat\\rho$ and $\\hat\\rho_1$ should be close to $0$, and the $p$-values for the K-S and $\\chi^2$ tests should be high (e.g., $ 0.05$), indicating no statistically significant deviation from the ideal properties of i.i.d. normal variates.\n-   **Case B (RANDU, $N=40000$):** The known defects of RANDU are expected to propagate. The non-linear acceptance-rejection step and transformation can either mask or amplify these defects. We anticipate that at least one diagnostic will detect a statistically significant anomaly, likely a non-zero $\\hat\\rho_1$ and a low $p$-value for the $\\chi^2$ test.\n-   **Case C (Degenerate, $N=40000$):** This case serves as a positive control. With $V_1=V_2$, it follows that $Z_1=Z_2$. Thus, we expect $\\hat\\rho$ to be exactly $1$. The angles will cluster at $\\pi/4$ and $-3\\pi/4$, leading to an extremely small K-S test $p$-value. The $\\chi^2$ test will show all data on the diagonal, resulting in a $p$-value of effectively $0$. The lag-$1$ autocorrelation $\\hat\\rho_1$ will also be large and positive.\n-   **Case D (PCG, $N=500$):** This is a sanity check with a smaller sample size. We expect results similar to Case A, but with greater statistical variance. For example, the estimated correlations may be further from $0$, and $p$-values may be lower simply due to sampling variability, but they should not typically indicate strong rejection of the null hypotheses.\n\nThe implementation will follow these principles, generating the random variates for each case and computing the four specified diagnostic statistics.", "answer": "[[-0.003058022718910074, 0.8033099951065662, 0.0007802117565314741, 0.4045054941014382],[-0.004071373307525381, 0.2526649176378415, -0.019383616238385456, 0.0],[1.0, 0.0, 0.4999974999687491, 0.0],[-0.05730335017088924, 0.6923364953930432, -0.02701777265814522, 0.5193011311090176]]", "id": "3324468"}]}