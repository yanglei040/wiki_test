{"hands_on_practices": [{"introduction": "The first essential skill in stochastic simulation is the ability to construct reliable random variate generators from first principles. This exercise challenges you to build a comprehensive gamma distribution sampler, a cornerstone of many statistical models. You will implement two state-of-the-art methods: the highly efficient Marsaglia-Tsang algorithm for shape parameters $k>1$ and an augmentation technique for the more complex case of $0 \\lt k \\lt 1$, ensuring your generator is robust across the full range of shape parameters. By validating your implementation against fundamental distributional relationships, you will practice the critical skill of software verification in a scientific computing context. [@problem_id:3056410]", "problem": "Implement a statistically exact random variate generator for the gamma distribution using acceptance–rejection sampling and augmentation, and verify its correctness through a set of tests grounded in fundamental distributional relationships. Specifically, construct a program that simulates independent and identically distributed samples from the gamma distribution with shape parameter $k$ and scale parameter $\\theta$, denoted by $\\mathrm{Gamma}(k,\\theta)$, relying on the following foundational base only: (i) the definition of the gamma distribution as a family of distributions with probability density function on $(0,\\infty)$, (ii) the law of large numbers for verifying sample moments, (iii) the acceptance–rejection method with a valid proposal distribution, (iv) the definition of the beta distribution as the distribution of a ratio of independent gamma random variables with the same scale, and (v) the definition of the chi-squared distribution as a special case of the gamma distribution. Do not use any pre-built gamma, beta, or chi-squared samplers.\n\nYour implementation must satisfy the following specification.\n- For $k>1$, use an acceptance–rejection sampler based on a transformation of a standard normal proposal and a “squeeze” acceptance region. The method must be fully coded from first principles using independent uniform and standard normal variates.\n- For $0<k<1$, use augmentation: construct a $\\mathrm{Gamma}(k,\\theta)$ variate from a $\\mathrm{Gamma}(k+1,\\theta)$ variate and an independent $\\operatorname{Uniform}(0,1)$ variate via an appropriate transformation. You must handle the boundary case $k=1$ correctly.\n- The generator must accept arbitrary positive real $k$ and $\\theta$, and a requested sample size $n \\in \\mathbb{N}$, and return $n$ independent samples.\n\nUse the following well-tested facts as the fundamental base for your derivations and checks:\n- If $X \\sim \\mathrm{Gamma}(k,\\theta)$, then $\\mathbb{E}[X]=k\\theta$ and $\\operatorname{Var}(X)=k\\theta^{2}$.\n- If $X \\sim \\mathrm{Gamma}(a,1)$ and $Y \\sim \\mathrm{Gamma}(b,1)$ are independent, then $U=\\frac{X}{X+Y}$ follows the beta distribution with parameters $(a,b)$, with mean $\\frac{a}{a+b}$.\n- If $Z \\sim \\chi^{2}_{\\nu}$, then $Z \\sim \\mathrm{Gamma}(\\frac{\\nu}{2},2)$, with $\\mathbb{E}[Z]=\\nu$ and $\\operatorname{Var}(Z)=2\\nu$.\n- The acceptance–rejection method: if $(X,U)$ are generated from a proposal distribution and a uniform variable with an acceptance region designed so that accepted $X$ have the target density, then the distribution of accepted $X$ equals the target.\n\nTest suite and required outputs:\n- Use a fixed seed for the pseudorandom number generator to ensure reproducibility. Let the seed be the integer $123456789$.\n- For each test below, generate $n=200000$ independent samples and compute the indicated statistics. Each test returns one or two boolean values. Your program must aggregate these booleans in the order listed and print them in a single line as a comma-separated list enclosed in square brackets. No other output is permitted.\n\nTests:\n1. Gamma, shape greater than one: Let $k=3.5$, $\\theta=2.0$. Compute the sample mean $\\hat{m}$ and sample variance $\\hat{v}$ and compare to the targets $m^{\\star}=k\\theta$ and $v^{\\star}=k\\theta^{2}$. Output two booleans: $|\\hat{m}-m^{\\star}|<0.03$ and $|\\hat{v}-v^{\\star}|<0.30$.\n2. Gamma, shape less than one (augmentation): Let $k=0.7$, $\\theta=1.5$. Output two booleans: $|\\hat{m}-m^{\\star}|<0.03$ and $|\\hat{v}-v^{\\star}|<0.10$ with $m^{\\star}=k\\theta$ and $v^{\\star}=k\\theta^{2}$.\n3. Beta via ratio of independent gammas: Let $a=2.1$, $b=3.3$. Generate independent $X \\sim \\mathrm{Gamma}(a,1)$ and $Y \\sim \\mathrm{Gamma}(b,1)$, set $U=\\frac{X}{X+Y}$. Output one boolean: $|\\hat{m}_{U}-\\frac{a}{a+b}|<0.01$, where $\\hat{m}_{U}$ is the sample mean of $U$.\n4. Chi-squared as a gamma: Let $\\nu=10$, which corresponds to $\\mathrm{Gamma}(\\frac{\\nu}{2},2)$ with $k=\\frac{\\nu}{2}=5$ and $\\theta=2$. Output two booleans: $|\\hat{m}-\\nu|<0.03$ and $|\\hat{v}-2\\nu|<0.20$.\n5. Boundary case $k=1$ (exponential): Let $k=1.0$, $\\theta=0.8$. Output two booleans: $|\\hat{m}-k\\theta|<0.02$ and $|\\hat{v}-k\\theta^{2}|<0.05$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., `[true,true,...]`), where each entry is the boolean result of the corresponding check in the exact order: test $1$ (mean, variance), test $2$ (mean, variance), test $3$, test $4$ (mean, variance), test $5$ (mean, variance).\n- No angles or physical units are involved in this problem; all outputs are unitless booleans.", "solution": "The user-provided problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and internally consistent. It poses a standard, albeit non-trivial, problem in computational statistics: implementing a random variate generator for the gamma distribution from first principles and verifying its properties. The problem statement provides all necessary definitions, algorithms, and test conditions for a unique and verifiable solution.\n\nThe solution is implemented by first constructing a generator for the standard gamma distribution, $\\mathrm{Gamma}(k,1)$, which is then extended to the general $\\mathrm{Gamma}(k,\\theta)$ distribution by exploiting the scaling property: if $X \\sim \\mathrm{Gamma}(k,1)$, then $\\theta X \\sim \\mathrm{Gamma}(k,\\theta)$. The generation of standard gamma variates is handled by three distinct methods, selected based on the value of the shape parameter $k$.\n\nThe required standard normal variates, $Z \\sim N(0,1)$, are synthesized from pairs of independent uniform variates, $U_1, U_2 \\sim \\operatorname{Uniform}(0,1)$, using the Box-Muller transform: $Z_1 = \\sqrt{-2 \\ln(U_1)} \\cos(2\\pi U_2)$ and $Z_2 = \\sqrt{-2 \\ln(U_1)} \\sin(2\\pi U_2)$. This transform produces two independent $N(0,1)$ samples for each pair of uniform samples.\n\nThe three cases for the shape parameter $k$ are as follows:\n\n1.  **For $k > 1$**: The Marsaglia and Tsang (2000) acceptance-rejection method is used. This is a highly efficient algorithm.\n    Let the target distribution be the standard gamma distribution $\\mathrm{Gamma}(k,1)$. The method uses a clever transformation of a standard normal variate $Z \\sim N(0,1)$ to propose a candidate value $X$. The algorithm is as follows:\n    a. Pre-calculate constants $d = k - 1/3$ and $c = 1/\\sqrt{9d}$.\n    b. Generate a standard normal variate $Z$ and a uniform variate $U \\sim \\operatorname{Uniform}(0,1)$.\n    c. Calculate $V = (1+cZ)^3$. If $V \\le 0$, reject and return to step (b).\n    d. The proposed gamma-distributed value is $X = dV$.\n    e. Accept $X$ if the following condition holds: $\\ln(U) < \\frac{Z^2}{2} + d - X + d \\ln(V)$. Otherwise, reject $X$ and return to step (b).\n    This process is repeated until the desired number of samples is generated.\n\n2.  **For $0 < k < 1$**: An augmentation method, as described by Ahrens and Dieter (1974), is employed. This method leverages the generator for shape parameters greater than $1$. It is based on the property that if $Y \\sim \\mathrm{Gamma}(k+1,1)$ and $U \\sim \\operatorname{Uniform}(0,1)$ are independent, then the random variable $X = Y \\cdot U^{1/k}$ is distributed as $\\mathrm{Gamma}(k,1)$.\n    Since $0 < k < 1$, the shape parameter $k+1$ of the required variate $Y$ will be in the range $(1,2)$. Therefore, we can generate $Y$ using the acceptance-rejection method for $k > 1$ described above. This is a direct method (no rejection), so one $Y$ and one $U$ produce one $X$.\n\n3.  **For $k = 1$**: This is a boundary case. The $\\mathrm{Gamma}(1,1)$ distribution is the standard exponential distribution, $\\operatorname{Exp}(1)$. This can be sampled efficiently and exactly using the inverse transform method. If $U \\sim \\operatorname{Uniform}(0,1)$, then $X = -\\ln(U)$ is distributed as $\\operatorname{Exp}(1)$.\n\nThe final implementation consolidates these methods into a single generator function. This generator is then used to perform the five specified tests. For each test, a large number of samples ($n=200000$) is generated, and their sample moments (mean and variance) are compared against the theoretical values derived from the properties of the gamma, beta, and chi-squared distributions. The results of these comparisons, which are boolean values, are collected and printed in the specified format. The use of a fixed seed for the random number generator ensures that the results are reproducible.", "answer": "```python\nimport numpy as np\n\ndef _my_normal_sampler(size: int, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"Generates standard normal variates using the Box-Muller transform.\"\"\"\n    if size == 0:\n        return np.array([])\n    \n    num_pairs = (size + 1) // 2\n    u1 = rng.uniform(size=num_pairs)\n    # Avoid u1=0 for log\n    u1[u1 == 0] = 1e-100 \n    u2 = rng.uniform(size=num_pairs)\n    \n    r = np.sqrt(-2.0 * np.log(u1))\n    theta = 2.0 * np.pi * u2\n    \n    z1 = r * np.cos(theta)\n    z2 = r * np.sin(theta)\n    \n    all_z = np.empty(2 * num_pairs)\n    all_z[0::2] = z1\n    all_z[1::2] = z2\n    \n    return all_z[:size]\n\ndef _gamma_sampler_gt1(k: float, n: int, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"\n    Generates n samples from a standard Gamma(k, 1) distribution for k > 1.\n    Uses the Marsaglia and Tsang (2000) acceptance-rejection method.\n    \"\"\"\n    d = k - 1.0/3.0\n    c = 1.0 / np.sqrt(9.0 * d)\n    \n    samples = np.empty(n)\n    count = 0\n    \n    while count < n:\n        needed = n - count\n        \n        # Generate candidate Zs and Us\n        z = _my_normal_sampler(needed, rng)\n        u = rng.uniform(size=needed)\n        \n        v = (1.0 + c * z)**3\n        \n        # Filter for valid V > 0\n        mask_valid_v = v > 0\n        z_valid = z[mask_valid_v]\n        u_valid = u[mask_valid_v]\n        v_valid = v[mask_valid_v]\n        \n        if len(v_valid) == 0:\n            continue\n            \n        x_cand = d * v_valid\n        \n        # Squeeze test and main acceptance condition\n        log_u_valid = np.log(u_valid)\n        accept_mask = log_u_valid < (0.5 * z_valid**2 + d - x_cand + d * np.log(v_valid))\n        \n        accepted_samples = x_cand[accept_mask]\n        num_accepted = len(accepted_samples)\n        \n        if num_accepted > 0:\n            fill_count = min(num_accepted, needed)\n            samples[count:count + fill_count] = accepted_samples[:fill_count]\n            count += fill_count\n            \n    return samples\n\ndef _gamma_sampler_eq1(n: int, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"\n    Generates n samples from a standard Gamma(1, 1) distribution (Exponential).\n    Uses the inverse transform method.\n    \"\"\"\n    u = rng.uniform(size=n)\n    # Avoid u=0 for log\n    u[u == 0] = 1e-100\n    return -np.log(u)\n\ndef _gamma_sampler_lt1(k: float, n: int, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"\n    Generates n samples from a standard Gamma(k, 1) distribution for 0 < k < 1.\n    Uses augmentation method: Y * U^(1/k) where Y ~ Gamma(k+1, 1).\n    \"\"\"\n    # Generate Y ~ Gamma(k+1, 1)\n    # Since k+1 is in (1, 2), we use the k>1 sampler.\n    y_samples = _gamma_sampler_gt1(k + 1.0, n, rng)\n    \n    # Generate U ~ Uniform(0, 1)\n    u_samples = rng.uniform(size=n)\n    \n    return y_samples * (u_samples ** (1.0/k))\n\ndef gamma_sampler(k: float, theta: float, n: int, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"\n    Generates n samples from a Gamma(k, theta) distribution.\n    \"\"\"\n    if k <= 0 or theta <= 0 or not isinstance(n, int) or n < 0:\n        raise ValueError(\"k and theta must be positive, n must be a non-negative integer.\")\n\n    if k > 1.0:\n        std_samples = _gamma_sampler_gt1(k, n, rng)\n    elif k == 1.0:\n        std_samples = _gamma_sampler_eq1(n, rng)\n    else:  # 0 < k < 1\n        std_samples = _gamma_sampler_lt1(k, n, rng)\n        \n    return std_samples * theta\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    seed = 123456789\n    n = 200000\n    rng = np.random.default_rng(seed)\n    \n    results = []\n\n    # Test 1: Gamma, shape greater than one\n    k, theta = 3.5, 2.0\n    samples_1 = gamma_sampler(k, theta, n, rng)\n    m_star_1, v_star_1 = k * theta, k * theta**2\n    m_hat_1, v_hat_1 = np.mean(samples_1), np.var(samples_1, ddof=1)\n    results.append(np.abs(m_hat_1 - m_star_1) < 0.03)\n    results.append(np.abs(v_hat_1 - v_star_1) < 0.30)\n    \n    # Test 2: Gamma, shape less than one (augmentation)\n    k, theta = 0.7, 1.5\n    samples_2 = gamma_sampler(k, theta, n, rng)\n    m_star_2, v_star_2 = k * theta, k * theta**2\n    m_hat_2, v_hat_2 = np.mean(samples_2), np.var(samples_2, ddof=1)\n    results.append(np.abs(m_hat_2 - m_star_2) < 0.03)\n    results.append(np.abs(v_hat_2 - v_star_2) < 0.10)\n\n    # Test 3: Beta via ratio of independent gammas\n    a, b = 2.1, 3.3\n    x_samples_3 = gamma_sampler(a, 1.0, n, rng)\n    y_samples_3 = gamma_sampler(b, 1.0, n, rng)\n    u_samples_3 = x_samples_3 / (x_samples_3 + y_samples_3)\n    u_mean_star_3 = a / (a + b)\n    u_mean_hat_3 = np.mean(u_samples_3)\n    results.append(np.abs(u_mean_hat_3 - u_mean_star_3) < 0.01)\n\n    # Test 4: Chi-squared as a gamma\n    nu = 10\n    k, theta = nu / 2.0, 2.0\n    samples_4 = gamma_sampler(k, theta, n, rng)\n    m_star_4, v_star_4 = nu, 2.0 * nu\n    m_hat_4, v_hat_4 = np.mean(samples_4), np.var(samples_4, ddof=1)\n    results.append(np.abs(m_hat_4 - m_star_4) < 0.03)\n    results.append(np.abs(v_hat_4 - v_star_4) < 0.20)\n\n    # Test 5: Boundary case k=1 (exponential)\n    k, theta = 1.0, 0.8\n    samples_5 = gamma_sampler(k, theta, n, rng)\n    m_star_5, v_star_5 = k * theta, k * theta**2\n    m_hat_5, v_hat_5 = np.mean(samples_5), np.var(samples_5, ddof=1)\n    results.append(np.abs(m_hat_5 - m_star_5) < 0.02)\n    results.append(np.abs(v_hat_5 - v_star_5) < 0.05)\n\n    print(f\"[{','.join(map(str, results))}]\".lower())\n\nsolve()\n```", "id": "3056410"}, {"introduction": "Beyond mere implementation, designing an efficient sampler requires a deep understanding of the underlying theory. This practice moves from construction to optimization, focusing on the acceptance-rejection method for shape parameters $k < 1$. You will theoretically derive the optimal \"tilt\" for an exponentially tilted gamma proposal, a choice that maximizes the sampler's efficiency by minimizing the variance of the acceptance indicator. This exercise will sharpen your skills in applying analytical tools, such as convex duality and moment-matching, to optimize the performance of a Monte Carlo algorithm. [@problem_id:3309202]", "problem": "Consider a target gamma distribution with shape parameter $k \\in (0,1)$ and rate parameter $r>0$, whose probability density function is\n$$\nf(x) \\;=\\; \\frac{r^{k}}{\\Gamma(k)} \\, x^{k-1} \\exp(-r x), \\quad x>0.\n$$\nYou are tasked with designing an acceptance-rejection scheme to generate variates from $f$ using an exponentially tilted proposal that preserves the power-law behavior near the origin. Specifically, consider the proposal family\n$$\nq_{t}(x) \\;=\\; \\frac{t^{k}}{\\Gamma(k)} \\, x^{k-1} \\exp(-t x), \\quad x>0,\n$$\nwith tilt parameter $t>0$, so that $q_{t}$ is a gamma distribution with the same shape parameter $k$ and rate parameter $t$. For acceptance-rejection, you must choose a finite constant $M(t)$ such that $f(x) \\leq M(t)\\, q_{t}(x)$ for all $x>0$, and you accept a draw $X \\sim q_{t}$ with probability $f(X)/(M(t) q_{t}(X))$. Let $I_{t}$ denote the acceptance indicator, which is a Bernoulli random variable with success probability $p(t) = 1/M(t)$.\n\nStarting from the foundational definitions of acceptance-rejection and the gamma family, and without invoking any shortcut formulas, perform the following:\n\n- Derive an explicit expression for $M(t)$ as a function of $t$, and characterize the values of $t$ for which $M(t)$ is finite.\n\n- Express $p(t)$ and $\\operatorname{Var}(I_{t})$ in terms of $t$, $r$, and $k$.\n\n- Using convex duality by analyzing the supremum of the log-likelihood ratio and by interpreting the solution via moment-matching, determine the optimal tilt $t^{\\ast}$ that minimizes $\\operatorname{Var}(I_{t})$ over all admissible $t$.\n\nYour final answer must be the optimal tilt $t^{\\ast}$ expressed in terms of $r$. No numerical approximation is required; provide the exact symbolic expression. If you find multiple candidates, select the one that achieves the global minimum of $\\operatorname{Var}(I_{t})$ subject to the admissibility constraint you derived. The final answer should be a single closed-form expression.", "solution": "The user wants me to solve and validate a problem about an acceptance-rejection scheme for a gamma distribution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Target distribution:** Gamma distribution with shape parameter $k \\in (0,1)$ and rate parameter $r>0$.\n*   **Target PDF:** $f(x) = \\frac{r^{k}}{\\Gamma(k)} x^{k-1} \\exp(-r x)$ for $x>0$.\n*   **Proposal distribution family:** Gamma distribution with shape parameter $k$ and rate parameter $t>0$.\n*   **Proposal PDF:** $q_{t}(x) = \\frac{t^{k}}{\\Gamma(k)} x^{k-1} \\exp(-t x)$ for $x>0$.\n*   **Acceptance-Rejection condition:** $f(x) \\leq M(t) q_{t}(x)$ for all $x>0$.\n*   **Acceptance indicator:** $I_t$, a Bernoulli random variable.\n*   **Success probability:** $p(t) = 1/M(t)$.\n\n**Tasks:**\n1.  Derive an expression for $M(t)$ and find the values of $t$ for which it is finite.\n2.  Express $p(t)$ and $\\operatorname{Var}(I_t)$ in terms of $t$, $r$, and $k$.\n3.  Determine the optimal tilt $t^{\\ast}$ that minimizes $\\operatorname{Var}(I_t)$ by using convex duality and a moment-matching interpretation.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded, well-posed, and objective. It is a standard problem in computational statistics and Monte Carlo methods, specifically concerning the optimization of proposal distributions for acceptance-rejection sampling.\n\n1.  **Scientific or Factual Unsoundness:** None. The problem is based on established principles of probability theory (gamma distribution, Bernoulli trials) and simulation (acceptance-rejection).\n2.  **Non-Formalizable or Irrelevant:** None. The problem is stated in precise mathematical terms and is directly relevant to the specified topic.\n3.  **Incomplete or Contradictory Setup:** None. All necessary parameters and definitions ($k, r, t, f(x), q_t(x)$) are provided.\n4.  **Unrealistic or Infeasible:** None. The problem is purely mathematical.\n5.  **Ill-Posed or Poorly Structured:** None. The objective function $\\operatorname{Var}(I_t)$ is well-defined, and the optimization is over a specified parameter $t$. The structure is logical.\n6.  **Pseudo-Profound, Trivial, or Tautological:** None. Although the final answer might appear simple, the required justification involving convex duality and moment-matching necessitates a deep understanding of the underlying theory of sampling algorithms. The problem is not trivial.\n7.  **Outside Scientific Verifiability:** None. The derivation can be rigorously verified through mathematical proof.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A full solution will be provided.\n\n### Solution\n\nThe acceptance-rejection algorithm requires a constant $M(t)$ such that $f(x) \\leq M(t) q_t(x)$ for all $x > 0$. The most efficient choice for $M(t)$ is the smallest such constant, which is given by the supremum of the ratio of the densities.\n$$\nM(t) = \\sup_{x>0} \\frac{f(x)}{q_t(x)}\n$$\nLet's compute this ratio:\n$$\n\\frac{f(x)}{q_t(x)} = \\frac{\\frac{r^{k}}{\\Gamma(k)} x^{k-1} \\exp(-r x)}{\\frac{t^{k}}{\\Gamma(k)} x^{k-1} \\exp(-t x)} = \\left(\\frac{r}{t}\\right)^{k} \\exp(-r x + t x) = \\left(\\frac{r}{t}\\right)^{k} \\exp((t-r)x)\n$$\nTo find the supremum of this expression over $x>0$, we analyze the behavior of the exponential term $\\exp((t-r)x)$, which depends on the sign of the coefficient $t-r$.\n\n1.  If $t > r$, then $t-r > 0$. The function $\\exp((t-r)x)$ is strictly increasing in $x$. As $x \\to \\infty$, the function grows without bound. Thus, $M(t) = \\infty$.\n2.  If $t = r$, then $t-r = 0$. The ratio is simply $(r/r)^k \\exp(0) = 1$ for all $x>0$. Thus, $M(r) = 1$.\n3.  If $0 < t < r$, then $t-r < 0$. The function $\\exp((t-r)x)$ is strictly decreasing in $x$. The supremum is achieved as $x \\to 0^{+}$.\n    $$\n    \\sup_{x>0} \\left(\\frac{r}{t}\\right)^{k} \\exp((t-r)x) = \\lim_{x \\to 0^{+}} \\left(\\frac{r}{t}\\right)^{k} \\exp((t-r)x) = \\left(\\frac{r}{t}\\right)^{k}\n    $$\n\nCombining these cases, we find that a finite constant $M(t)$ exists only for $t \\in (0, r]$. For this admissible range of $t$, the optimal constant is given by:\n$$\nM(t) = \\left(\\frac{r}{t}\\right)^{k}, \\quad t \\in (0, r]\n$$\n\nThe acceptance indicator $I_t$ is a Bernoulli random variable. The probability of accepting a proposed value $X \\sim q_t(x)$ is $f(X)/(M(t)q_t(X))$. The overall acceptance probability, $p(t)$, is the expectation of this quantity.\n$$\np(t) = \\mathbb{E}_{X \\sim q_t}\\left[ \\frac{f(X)}{M(t) q_t(X)} \\right] = \\frac{1}{M(t)} \\int_0^{\\infty} \\frac{f(x)}{q_t(x)} q_t(x) \\,dx = \\frac{1}{M(t)} \\int_0^{\\infty} f(x) \\,dx = \\frac{1}{M(t)}\n$$\nsince $f(x)$ is a probability density function. For $t \\in (0, r]$, this gives:\n$$\np(t) = \\frac{1}{(r/t)^k} = \\left(\\frac{t}{r}\\right)^{k}\n$$\nThe variance of the Bernoulli indicator $I_t$ is $\\operatorname{Var}(I_t) = p(t)(1-p(t))$.\n$$\n\\operatorname{Var}(I_t) = \\left(\\frac{t}{r}\\right)^{k} \\left(1 - \\left(\\frac{t}{r}\\right)^{k}\\right)\n$$\nOur goal is to find the value of $t$ in the admissible range $(0, r]$ that minimizes this variance. Let $P = p(t) = (t/r)^k$. As $t$ varies in $(0, r]$, $P$ varies in $(0, 1]$. We wish to minimize the function $g(P) = P(1-P)$ for $P \\in (0, 1]$. This function is a downward-opening parabola with a maximum at $P=1/2$ and minima at the boundaries of its domain, $P=0$ and $P=1$. On the interval $(0, 1]$, the minimum value of $g(P)$ is $0$, which is achieved at $P=1$.\nSetting $p(t)=1$, we get:\n$$\n\\left(\\frac{t}{r}\\right)^{k} = 1 \\implies \\frac{t}{r} = 1 \\implies t = r\n$$\nSince $t=r$ is in the admissible set $(0, r]$, it is the optimal value. At $t=r$, the variance is $\\operatorname{Var}(I_r) = 1(1-1) = 0$.\n\nWe now provide the justification for this result using the concepts requested in the problem statement. Minimizing $\\operatorname{Var}(I_t)$ is equivalent to making $p(t)$ as close to $0$ or $1$ as possible. For sampling efficiency, we seek to maximize the acceptance probability $p(t)$, which corresponds to the case $p(t)=1$. Maximizing $p(t)$ is equivalent to minimizing its reciprocal, $M(t)$. The problem is thus equivalent to solving:\n$$\n\\min_{t \\in (0, r]} M(t) \\quad \\text{which is} \\quad \\min_{t \\in (0, r]} \\sup_{x>0} \\frac{f(x)}{q_t(x)}\n$$\nThis is a minimax problem. By taking the logarithm, which is a monotonic function, we can equivalently solve:\n$$\n\\min_{t \\in (0, r]} \\sup_{x>0} \\log\\left(\\frac{f(x)}{q_t(x)}\\right)\n$$\nLet $L(x,t) = \\log(f(x)/q_t(x)) = k\\log(r/t) + (t-r)x$. The function $L(x,t)$ is convex in $t$ and linear (hence concave) in $x$. Since the domains for $t$ and $x$ are convex sets, we can apply a minimax theorem and swap the order of minimization and maximization (an application of convex duality).\n$$\n\\min_{t \\in (0, r]} \\sup_{x>0} L(x,t) = \\sup_{x>0} \\min_{t \\in (0, r]} L(x,t)\n$$\nLet's analyze the inner minimization problem: $\\min_{t \\in (0, r]} L(x,t)$ for a fixed $x>0$. We differentiate $L(x,t)$ with respect to $t$:\n$$\n\\frac{\\partial L}{\\partial t} = -\\frac{k}{t} + x\n$$\nSetting the derivative to zero gives $t = k/x$. This critical point corresponds to a minimum since the second derivative $\\frac{\\partial^2 L}{\\partial t^2} = k/t^2 > 0$.\nThe condition $t=k/x$ provides the moment-matching interpretation. The mean of the proposal distribution $q_t(x)$, which is a Gamma($k,t$) distribution, is $\\mathbb{E}_{q_t}[X] = k/t$. The condition $t = k/x$ is equivalent to $\\mathbb{E}_{q_t}[X] = x$. This means that for a given point $x$, the locally optimal proposal is the one whose expected value is $x$.\n\nThe solution $t=k/x$ is only valid if it lies in the admissible range $(0,r]$.\n- If $k/x \\in (0,r]$, i.e., $x \\ge k/r$, the minimum is attained at $t=k/x$.\n- If $k/x > r$, i.e., $x < k/r$, the derivative $\\partial L / \\partial t = x - k/t$ is always negative for $t \\in (0,r]$ since $t \\le r < k/x$. Thus, $L(x,t)$ is decreasing in $t$, and the minimum is at the boundary $t=r$.\n\nNow, substituting this result back into the outer problem gives the maximin value. However, the direct solution to the primal problem $\\min_{t \\in (0, r]} \\sup_{x>0} L(x,t)$ is more straightforward. As shown earlier, $\\sup_{x>0} L(x,t) = k\\log(r/t)$ for $t \\in (0,r]$. The function $k\\log(r/t)$ is a decreasing function of $t$. Its minimum over $(0,r]$ is achieved at the largest possible value of $t$, which is $t=r$.\nThus, the optimal tilt is $t^*=r$.\n\nThe moment-matching interpretation gives the final insight. The globally optimal proposal is found by matching the mean of the proposal distribution to the mean of the target distribution.\nThe mean of the target distribution $f(x) \\sim \\text{Gamma}(k,r)$ is $\\mathbb{E}_f[X] = k/r$.\nThe mean of the proposal distribution $q_t(x) \\sim \\text{Gamma}(k,t)$ is $\\mathbb{E}_{q_t}[X] = k/t$.\nEquating the two means to find the optimal $t$:\n$$\n\\mathbb{E}_{q_t}[X] = \\mathbb{E}_{f}[X] \\implies \\frac{k}{t} = \\frac{k}{r} \\implies t = r\n$$\nThis confirms that $t^*=r$ is the optimal choice under the requested framework. At $t=r$, the proposal and target distributions are identical ($q_r = f$), leading to $M(r)=1$, $p(r)=1$, and $\\operatorname{Var}(I_r)=0$, the most efficient outcome possible.", "answer": "$$\n\\boxed{r}\n$$", "id": "3309202"}, {"introduction": "The tools of variate generation are not ends in themselves, but powerful components in solving larger problems. This final practice showcases the application of gamma-related distributions within the framework of importance sampling, a crucial technique for estimating rare event probabilities. You will design an importance sampling estimator for a gamma distribution's tail probability by constructing a new proposal distribution through exponential tilting, which itself turns out to be a gamma distribution. The core task is to derive the asymptotically optimal tilt parameter that minimizes the estimator's variance, providing a hands-on introduction to the principles of variance reduction in rare-event simulation. [@problem_id:3309211]", "problem": "Consider a random variable $X$ distributed as a Gamma distribution with shape $k>0$ and scale $\\theta>0$, denoted $X\\sim\\Gamma(k,\\theta)$. Let $\\beta\\equiv 1/\\theta$ denote the rate. The objective is to estimate the right-tail probability $p(t)\\equiv \\mathbb{P}(X>t)$ for a fixed threshold $t>0$ by importance sampling. You are asked to design and analyze an importance sampler based on exponential tilting of the target density. Specifically, consider the proposal family $\\{g_{\\lambda}:\\lambda\\in[0,\\beta)\\}$ defined by\n$$\ng_{\\lambda}(x)\\propto \\exp(\\lambda x) f(x),\n$$\nwhere $f(x)$ is the probability density function of $X$, and the proportionality constant normalizes $g_{\\lambda}$ to a density. Using this proposal, the standard importance sampling estimator of $p(t)$ is\n$$\n\\widehat{p}_{n}(t;\\lambda)\\equiv \\frac{1}{n}\\sum_{i=1}^{n} w_{\\lambda}(X_{i})\\,\\mathbf{1}\\{X_{i}>t\\},\\quad X_{i}\\stackrel{\\text{i.i.d.}}{\\sim} g_{\\lambda},\n$$\nwith importance weight $w_{\\lambda}(x)\\equiv f(x)/g_{\\lambda}(x)$.\n\nTasks:\n1. Starting from first principles in importance sampling, derive the exact form of $g_{\\lambda}(x)$ (including its normalizing constant) and the exact weight function $w_{\\lambda}(x)$. Then, derive an analytic expression for the per-sample second moment $\\mathbb{E}_{g_{\\lambda}}[w_{\\lambda}(X)^{2}\\,\\mathbf{1}\\{X>t\\}]$ in terms of $k$, $\\beta$, $t$, and $\\lambda$ only, and use it to obtain the objective whose minimizer in $\\lambda\\in[0,\\beta)$ gives the variance-minimizing choice for the exponential tilting.\n2. Compute the first-order optimality condition for the variance-minimizing $\\lambda$, express it in terms of standard special functions, and use asymptotic analysis in the rare-event regime $t\\to\\infty$ to obtain a leading-order asymptotic expansion for the optimal tilting parameter $\\lambda^{\\star}$.\n3. Define the Effective Sample Size ($\\mathrm{ESS}$) for the weighted estimator as $\\mathrm{ESS} \\equiv n\\cdot \\left(\\mathbb{E}_{g_{\\lambda}}[w_{\\lambda}(X)\\,\\mathbf{1}\\{X>t\\}]\\right)^{2}\\big/\\mathbb{E}_{g_{\\lambda}}[w_{\\lambda}(X)^{2}\\,\\mathbf{1}\\{X>t\\}]$, and derive a closed-form expression for the ESS fraction $\\mathrm{ESS}/n$ in terms of $k$, $\\beta$, $t$, and $\\lambda$. Then, evaluate this expression at the asymptotically optimal $\\lambda^{\\star}$ obtained in Task 2 and simplify to its leading asymptotic order for large $t$.\n\nYour final answer must consist of two items:\n- The leading-order asymptotic expression for $\\lambda^{\\star}$ as $t\\to\\infty$.\n- The leading-order asymptotic expression for $\\mathrm{ESS}/n$ at $\\lambda^{\\star}$ as $t\\to\\infty$.\n\nExpress the final answer as a single closed-form analytic expression using a row matrix with two entries. No numerical rounding is required.", "solution": "The problem asks for an analysis of an importance sampling estimator for the right-tail probability of a Gamma-distributed random variable. The strategy involves exponential tilting. We will solve the problem in three sequential tasks as requested.\n\nThe target random variable $X$ follows a Gamma distribution with shape $k>0$ and scale $\\theta>0$, denoted $X\\sim\\Gamma(k,\\theta)$. The rate parameter is $\\beta = 1/\\theta$. The probability density function (PDF) of $X$ is given by\n$$\nf(x) = \\frac{\\beta^k}{\\Gamma(k)} x^{k-1} \\exp(-\\beta x), \\quad \\text{for } x>0.\n$$\nWe want to estimate $p(t) = \\mathbb{P}(X>t) = \\int_t^\\infty f(x) dx$.\n\n### Task 1: Derivation of the Proposal Density, Weights, and the Second Moment Objective\n\nThe proposal density $g_{\\lambda}(x)$ is defined by the exponential tilting relation $g_{\\lambda}(x) \\propto \\exp(\\lambda x) f(x)$ for $\\lambda \\in [0, \\beta)$. Let's find the normalized density.\n$$\ng_{\\lambda}(x) = C_{\\lambda} \\exp(\\lambda x) f(x) = C_{\\lambda} \\exp(\\lambda x) \\frac{\\beta^k}{\\Gamma(k)} x^{k-1} \\exp(-\\beta x) = C_{\\lambda} \\frac{\\beta^k}{\\Gamma(k)} x^{k-1} \\exp(-(\\beta-\\lambda)x).\n$$\nThe normalizing constant $C_{\\lambda}$ is the reciprocal of the integral of the unnormalized density over its support $(0, \\infty)$:\n$$\n\\frac{1}{C_{\\lambda}} = \\int_0^\\infty \\exp(\\lambda x) f(x) dx = \\mathbb{E}[\\exp(\\lambda X)].\n$$\nThis is the moment generating function (MGF) of $X$, denoted $M_X(\\lambda)$. For a $\\Gamma(k, \\beta)$ distribution, the MGF is\n$$\nM_X(\\lambda) = \\int_0^\\infty \\frac{\\beta^k}{\\Gamma(k)} x^{k-1} \\exp(-(\\beta-\\lambda)x) dx = \\frac{\\beta^k}{\\Gamma(k)} \\frac{\\Gamma(k)}{(\\beta-\\lambda)^k} = \\left(\\frac{\\beta}{\\beta-\\lambda}\\right)^k.\n$$\nThis is finite for $\\lambda < \\beta$, which justifies the given domain for $\\lambda$.\nThe normalizing constant is $C_{\\lambda} = 1/M_X(\\lambda) = (\\frac{\\beta-\\lambda}{\\beta})^k$.\nSubstituting $C_{\\lambda}$ back, the proposal density is\n$$\ng_{\\lambda}(x) = \\left(\\frac{\\beta-\\lambda}{\\beta}\\right)^k \\exp(\\lambda x) \\left(\\frac{\\beta^k}{\\Gamma(k)} x^{k-1} \\exp(-\\beta x)\\right) = \\frac{(\\beta-\\lambda)^k}{\\Gamma(k)} x^{k-1} \\exp(-(\\beta-\\lambda)x).\n$$\nThis is the PDF of a Gamma distribution with shape $k$ and rate $\\beta' = \\beta-\\lambda$, i.e., $X_i \\sim \\Gamma(k, (\\beta-\\lambda)^{-1})$.\n\nThe importance weight function $w_{\\lambda}(x)$ is defined as $w_{\\lambda}(x) = f(x)/g_{\\lambda}(x)$. Using the relation $g_{\\lambda}(x) = \\exp(\\lambda x) f(x)/M_X(\\lambda)$, we get\n$$\nw_{\\lambda}(x) = M_X(\\lambda) \\exp(-\\lambda x) = \\left(\\frac{\\beta}{\\beta-\\lambda}\\right)^k \\exp(-\\lambda x).\n$$\nThe objective is to find $\\lambda$ that minimizes the variance of the estimator $\\widehat{p}_{n}(t;\\lambda)$. The variance is given by $\\frac{1}{n}\\mathrm{Var}_{g_{\\lambda}}(w_{\\lambda}(X)\\mathbf{1}\\{X>t\\})$. The mean of the estimator is\n$$\n\\mathbb{E}_{g_{\\lambda}}[w_{\\lambda}(X)\\mathbf{1}\\{X>t\\}] = \\int_t^\\infty w_{\\lambda}(x) g_{\\lambda}(x) dx = \\int_t^\\infty f(x) dx = p(t).\n$$\nSince the mean is independent of $\\lambda$, minimizing the variance is equivalent to minimizing the second moment, which we denote by $V(\\lambda)$:\n$$\nV(\\lambda) \\equiv \\mathbb{E}_{g_{\\lambda}}[w_{\\lambda}(X)^2 \\mathbf{1}\\{X>t\\}] = \\int_t^\\infty w_{\\lambda}(x)^2 g_{\\lambda}(x) dx.\n$$\nLet's evaluate this integral:\n$$\nw_{\\lambda}(x)^2 g_{\\lambda}(x) = \\left(M_X(\\lambda) \\exp(-\\lambda x)\\right)^2 g_{\\lambda}(x) = M_X(\\lambda)^2 \\exp(-2\\lambda x) \\frac{(\\beta-\\lambda)^k}{\\Gamma(k)} x^{k-1} \\exp(-(\\beta-\\lambda)x).\n$$\nIntegrating this from $t$ to $\\infty$:\n$$\nV(\\lambda) = M_X(\\lambda)^2 \\frac{(\\beta-\\lambda)^k}{\\Gamma(k)} \\int_t^\\infty x^{k-1} \\exp(-(2\\lambda + \\beta - \\lambda)x) dx = M_X(\\lambda)^2 \\frac{(\\beta-\\lambda)^k}{\\Gamma(k)} \\int_t^\\infty x^{k-1} \\exp(-(\\beta+\\lambda)x) dx.\n$$\nThe integral is related to the upper incomplete gamma function, $\\Gamma(s,z) = \\int_z^\\infty u^{s-1}\\exp(-u)du$. By a change of variable $u=(\\beta+\\lambda)x$, the integral becomes $\\frac{\\Gamma(k, (\\beta+\\lambda)t)}{(\\beta+\\lambda)^k}$.\nSubstituting this and the expression for $M_X(\\lambda)$:\n$$\nV(\\lambda) = \\left(\\frac{\\beta}{\\beta-\\lambda}\\right)^{2k} \\frac{(\\beta-\\lambda)^k}{\\Gamma(k)} \\frac{\\Gamma(k, (\\beta+\\lambda)t)}{(\\beta+\\lambda)^k} = \\frac{\\beta^{2k}}{(\\beta-\\lambda)^k (\\beta+\\lambda)^k \\Gamma(k)} \\Gamma(k, (\\beta+\\lambda)t).\n$$\nThis can be written as\n$$\nV(\\lambda) = \\frac{\\beta^{2k}}{(\\beta^2-\\lambda^2)^k \\Gamma(k)} \\Gamma(k, (\\beta+\\lambda)t).\n$$\nThis is the objective function whose minimizer gives the variance-minimizing choice of $\\lambda$.\n\n### Task 2: Optimality Condition and Asymptotic Analysis\n\nTo find the optimal $\\lambda$, we set the derivative of $V(\\lambda)$ with respect to $\\lambda$ to zero. It is simpler to differentiate $\\ln V(\\lambda)$:\n$$\n\\ln V(\\lambda) = 2k\\ln\\beta - k\\ln(\\beta^2-\\lambda^2) + \\ln\\Gamma(k, (\\beta+\\lambda)t) - \\ln\\Gamma(k).\n$$\n$$\n\\frac{d}{d\\lambda}\\ln V(\\lambda) = -k \\frac{-2\\lambda}{\\beta^2-\\lambda^2} + \\frac{1}{\\Gamma(k, (\\beta+\\lambda)t)} \\frac{d}{d\\lambda}\\Gamma(k, (\\beta+\\lambda)t) = 0.\n$$\nUsing the Leibniz integral rule, $\\frac{d}{dz}\\Gamma(s,z) = -z^{s-1}\\exp(-z)$. With $z(\\lambda) = (\\beta+\\lambda)t$, the chain rule gives\n$$\n\\frac{d}{d\\lambda}\\Gamma(k, (\\beta+\\lambda)t) = - ((\\beta+\\lambda)t)^{k-1} \\exp(-(\\beta+\\lambda)t) \\cdot t = -t^k (\\beta+\\lambda)^{k-1} \\exp(-(\\beta+\\lambda)t).\n$$\nSubstituting this into the derivative of $\\ln V(\\lambda)$:\n$$\n\\frac{2k\\lambda}{\\beta^2-\\lambda^2} - \\frac{t^k (\\beta+\\lambda)^{k-1} \\exp(-(\\beta+\\lambda)t)}{\\Gamma(k, (\\beta+\\lambda)t)} = 0.\n$$\nThis is the first-order optimality condition for the variance-minimizing $\\lambda$.\n\nFor the asymptotic analysis as $t\\to\\infty$, we use the leading-order asymptotic expansion for the incomplete gamma function: $\\Gamma(s,z) \\sim z^{s-1}\\exp(-z)$ for large $z$. Let $z = (\\beta+\\lambda)t$. As $t\\to\\infty$, $z\\to\\infty$. The second term in the optimality condition becomes\n$$\n\\frac{t^k (\\beta+\\lambda)^{k-1} \\exp(-(\\beta+\\lambda)t)}{\\Gamma(k, (\\beta+\\lambda)t)} \\sim \\frac{t^k (\\beta+\\lambda)^{k-1} \\exp(-(\\beta+\\lambda)t)}{((\\beta+\\lambda)t)^{k-1} \\exp(-(\\beta+\\lambda)t)} = \\frac{t^k (\\beta+\\lambda)^{k-1}}{t^{k-1}(\\beta+\\lambda)^{k-1}} = t.\n$$\nThus, the asymptotic optimality condition is\n$$\n\\frac{2k\\lambda}{\\beta^2-\\lambda^2} \\approx t.\n$$\nThis simplifies to a quadratic equation for $\\lambda$: $t\\lambda^2 + 2k\\lambda - t\\beta^2 = 0$. The solutions are\n$$\n\\lambda = \\frac{-2k \\pm \\sqrt{4k^2 - 4(t)(-t\\beta^2)}}{2t} = \\frac{-k \\pm \\sqrt{k^2+t^2\\beta^2}}{t}.\n$$\nSince $\\lambda$ must be in $[0, \\beta)$, we must take the positive root. For large $t$, $\\lambda$ must be positive.\n$$\n\\lambda^{\\star} \\approx \\frac{-k + \\sqrt{t^2\\beta^2(1+k^2/(t^2\\beta^2))}}{t} = \\frac{-k + t\\beta\\sqrt{1+k^2/(t^2\\beta^2)}}{t}.\n$$\nUsing the Taylor expansion $\\sqrt{1+x} \\approx 1 + x/2$ for small $x$:\n$$\n\\lambda^{\\star} \\approx \\frac{-k + t\\beta(1 + \\frac{k^2}{2t^2\\beta^2})}{t} = \\frac{-k + t\\beta + \\frac{k^2}{2t\\beta}}{t} = \\beta - \\frac{k}{t} + O(t^{-2}).\n$$\nThe leading-order asymptotic expression for the optimal tilting parameter is $\\lambda^{\\star} \\approx \\beta - \\frac{k}{t}$.\n\n### Task 3: Effective Sample Size (ESS) Analysis\n\nThe ESS fraction is defined as\n$$\n\\frac{\\mathrm{ESS}}{n} = \\frac{\\left(\\mathbb{E}_{g_{\\lambda}}[w_{\\lambda}(X)\\,\\mathbf{1}\\{X>t\\}]\\right)^{2}}{\\mathbb{E}_{g_{\\lambda}}[w_{\\lambda}(X)^{2}\\,\\mathbf{1}\\{X>t\\}]} = \\frac{p(t)^2}{V(\\lambda)}.\n$$\nWe have $p(t) = \\mathbb{P}(X>t) = \\frac{\\Gamma(k, \\beta t)}{\\Gamma(k)}$. Substituting the expressions for $p(t)$ and $V(\\lambda)$:\n$$\n\\frac{\\mathrm{ESS}}{n} = \\frac{(\\Gamma(k, \\beta t)/\\Gamma(k))^2}{ \\frac{\\beta^{2k}}{(\\beta^2-\\lambda^2)^k \\Gamma(k)} \\Gamma(k, (\\beta+\\lambda)t) } = \\frac{\\Gamma(k, \\beta t)^2 (\\beta^2-\\lambda^2)^k}{\\Gamma(k) \\beta^{2k} \\Gamma(k, (\\beta+\\lambda)t)}.\n$$\nWe evaluate this at the asymptotically optimal $\\lambda^{\\star} \\approx \\beta - k/t$.\nFor large $t$:\n- $\\beta^2 - (\\lambda^{\\star})^2 \\approx \\beta^2 - (\\beta-k/t)^2 = 2k\\beta/t - k^2/t^2 \\sim \\frac{2k\\beta}{t}$.\n- $\\beta + \\lambda^{\\star} \\approx \\beta + (\\beta - k/t) = 2\\beta - k/t$.\nThe argument of the second incomplete gamma function is $(\\beta+\\lambda^\\star)t \\approx 2\\beta t - k$.\n\nWe substitute these into the ESS/n expression, using the asymptotic form $\\Gamma(s,z) \\sim z^{s-1}\\exp(-z)$:\n- $\\Gamma(k, \\beta t)^2 \\sim ((\\beta t)^{k-1}\\exp(-\\beta t))^2 = (\\beta t)^{2k-2}\\exp(-2\\beta t)$.\n- $\\Gamma(k, 2\\beta t-k) \\sim (2\\beta t-k)^{k-1}\\exp(-(2\\beta t-k)) \\sim (2\\beta t)^{k-1}\\exp(-2\\beta t)\\exp(k)$.\nThe expression for the ESS fraction becomes:\n$$\n\\frac{\\mathrm{ESS}}{n}(\\lambda^\\star) \\approx \\frac{[(\\beta t)^{k-1}\\exp(-\\beta t)]^2 (\\frac{2k\\beta}{t})^k}{\\Gamma(k) \\beta^{2k} [(2\\beta t)^{k-1}\\exp(-2\\beta t)\\exp(k)]}.\n$$\nSimplifying the expression by cancelling $\\exp(-2\\beta t)$:\n$$\n\\frac{\\mathrm{ESS}}{n} \\approx \\frac{(\\beta t)^{2k-2} (2k\\beta)^k t^{-k}}{\\Gamma(k) \\beta^{2k} (2\\beta t)^{k-1} \\exp(k)} = \\frac{\\beta^{2k-2} t^{2k-2} \\cdot 2^k k^k \\beta^k t^{-k}}{\\Gamma(k) \\beta^{2k} \\cdot 2^{k-1} \\beta^{k-1} t^{k-1} \\exp(k)}.\n$$\nNow, we collect powers of each term:\n- Power of $2$: $k - (k-1) = 1$.\n- Power of $k$: $k$.\n- Power of $\\beta$: $(2k-2)+k - (2k) - (k-1) = 3k-2-3k+1 = -1$.\n- Power of $t$: $(2k-2)-k - (k-1) = k-2-k+1 = -1$.\n- Remaining terms: $1/\\Gamma(k)$ and $1/\\exp(k)$.\n\nCombining these gives the leading-order asymptotic expression for the ESS fraction:\n$$\n\\frac{\\mathrm{ESS}}{n} \\approx \\frac{2 k^k}{\\Gamma(k) \\exp(k) \\beta t}.\n$$\n\nThe final answers are the asymptotic expressions for $\\lambda^{\\star}$ and $\\mathrm{ESS}/n$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\beta - \\frac{k}{t} & \\frac{2 k^k \\exp(-k)}{\\Gamma(k) \\beta t} \\end{pmatrix}}\n$$", "id": "3309211"}]}