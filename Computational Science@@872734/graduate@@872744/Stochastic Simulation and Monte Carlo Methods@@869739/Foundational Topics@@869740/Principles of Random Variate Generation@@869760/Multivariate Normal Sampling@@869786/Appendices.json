{"hands_on_practices": [{"introduction": "A cornerstone of efficient Monte Carlo simulation is variance reduction. This exercise explores one of the most elegant and intuitive techniques: antithetic variates. By generating negatively correlated pairs of samples, we can often dramatically reduce the variance of our estimators, especially for functions with certain symmetries. This practice will guide you through the theoretical underpinnings of this method, proving that it preserves the target distribution while quantifying the precise variance reduction achieved [@problem_id:3322640].", "problem": "Consider a dimension $d \\in \\mathbb{N}$ and a random vector $Z \\in \\mathbb{R}^{d}$ distributed as a $d$-dimensional multivariate normal $\\mathcal{N}_{d}(0, I_{d})$, where $I_{d}$ is the $d \\times d$ identity matrix. Let $\\mu \\in \\mathbb{R}^{d}$ be fixed and let $A \\in \\mathbb{R}^{d \\times d}$ be a fixed full-rank matrix, so that the covariance matrix $\\Sigma := A A^{\\top}$ is positive definite. Define the linear transform $X := \\mu + A Z$, which is a standard way to sample from a multivariate normal distribution $\\mathcal{N}_{d}(\\mu, \\Sigma)$ in stochastic simulation.\n\nIn the antithetic variates (AV) technique in Monte Carlo (MC), one forms pairs by using $Z$ and its antithetic $-Z$, and correspondingly $X := \\mu + A Z$ and $X^{-} := \\mu + A(-Z) = \\mu - A Z$. Assume we wish to estimate $\\theta := \\mathbb{E}[f(X)]$ for a measurable function $f : \\mathbb{R}^{d} \\to \\mathbb{R}$ such that $\\operatorname{Var}(f(X))  \\infty$. Consider two estimators that use two function evaluations per replication:\n(i) the independent-sample average $\\hat{\\theta}_{\\mathrm{ind}} := \\frac{1}{2}\\big(f(X^{(1)}) + f(X^{(2)})\\big)$ with $X^{(1)}$ and $X^{(2)}$ independent and identically distributed as $\\mathcal{N}_{d}(\\mu, \\Sigma)$,\nand (ii) the antithetic-pair average $\\hat{\\theta}_{\\mathrm{ant}} := \\frac{1}{2}\\big(f(X) + f(X^{-})\\big)$ built from a single $Z \\sim \\mathcal{N}_{d}(0, I_{d})$ and its antithetic $-Z$.\n\nStarting only from the defining properties of multivariate normal distributions and the linear transformation of Gaussian vectors, as well as the definitions of variance, covariance, and correlation, do the following:\n1. Prove that the marginal sampling distribution of each element of an antithetic pair, $X$ and $X^{-}$, is unchanged and remains $\\mathcal{N}_{d}(\\mu, \\Sigma)$.\n2. Derive the exact expression for $\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}})$ in terms of $\\operatorname{Var}(f(X))$ and $\\operatorname{Cov}(f(X), f(X^{-}))$.\n3. Derive the variance ratio $R := \\frac{\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}})}{\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ind}})}$ and express it in terms of the correlation $\\rho := \\operatorname{Corr}(f(X), f(X^{-}))$.\n4. Suppose $f$ is odd about $\\mu$, meaning $f(\\mu + y) = - f(\\mu - y)$ for all $y \\in \\mathbb{R}^{d}$. Use the symmetry of the multivariate normal distribution and the linearity of $A$ to compute $\\rho$ in this case and deduce the value of $R$.\n\nProvide your final answer as a single closed-form analytic expression for $R$ in terms of $\\rho$, together with its value for odd $f$ as described above. No rounding is required, and no physical units are involved.", "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. We proceed with a step-by-step derivation.\n\n1. Proof of the marginal distribution of $X$ and $X^{-}$.\n\nWe are given that $Z \\in \\mathbb{R}^{d}$ is a random vector with a standard multivariate normal distribution, $Z \\sim \\mathcal{N}_{d}(0, I_{d})$, where $I_d$ is the $d \\times d$ identity matrix. The probability density function (PDF) of $Z$ is symmetric about the origin, meaning $p_{Z}(z) = p_{Z}(-z)$ for all $z \\in \\mathbb{R}^{d}$. This implies that the random vector $-Z$ has the same distribution as $Z$.\nTo show this formally, let's find the mean and covariance of $-Z$.\nThe mean is $\\mathbb{E}[-Z] = -\\mathbb{E}[Z] = -0 = 0$.\nThe covariance matrix is $\\operatorname{Cov}(-Z) = \\mathbb{E}[(-Z)(-Z)^{\\top}] - \\mathbb{E}[-Z]\\mathbb{E}[-Z]^{\\top} = \\mathbb{E}[ZZ^{\\top}] - 0 = \\operatorname{Cov}(Z) = I_{d}$.\nSince $-Z$ is a linear transformation of the multivariate normal vector $Z$, it is also multivariate normal. As it has the same mean ($0$) and covariance matrix ($I_d$) as $Z$, we conclude that $-Z \\sim \\mathcal{N}_{d}(0, I_{d})$.\n\nThe random vector $X$ is defined by the affine transformation $X := \\mu + A Z$. Since $Z$ is multivariate normal, $X$ is also multivariate normal. Its mean is:\n$$ \\mathbb{E}[X] = \\mathbb{E}[\\mu + A Z] = \\mu + A \\mathbb{E}[Z] = \\mu + A \\cdot 0 = \\mu $$\nIts covariance matrix is:\n$$ \\operatorname{Cov}(X) = \\operatorname{Cov}(\\mu + A Z) = \\operatorname{Cov}(A Z) = A \\operatorname{Cov}(Z) A^{\\top} = A I_{d} A^{\\top} = A A^{\\top} = \\Sigma $$\nThus, as stated in the problem, $X \\sim \\mathcal{N}_{d}(\\mu, \\Sigma)$.\n\nThe antithetic vector $X^{-}$ is defined as $X^{-} := \\mu - A Z = \\mu + A(-Z)$. Since we have established that $-Z$ has the same distribution as $Z$, namely $\\mathcal{N}_{d}(0, I_{d})$, the random vector $X^{-}$ must have the same distribution as $X$. Following the same derivation:\n$$ \\mathbb{E}[X^{-}] = \\mathbb{E}[\\mu + A(-Z)] = \\mu + A \\mathbb{E}[-Z] = \\mu + A \\cdot 0 = \\mu $$\n$$ \\operatorname{Cov}(X^{-}) = \\operatorname{Cov}(\\mu + A(-Z)) = \\operatorname{Cov}(A(-Z)) = A \\operatorname{Cov}(-Z) A^{\\top} = A I_{d} A^{\\top} = A A^{\\top} = \\Sigma $$\nTherefore, the marginal distribution of $X^{-}$ is also $\\mathcal{N}_{d}(\\mu, \\Sigma)$, identical to the distribution of $X$. This completes the proof for the first part.\n\n2. Derivation of $\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}})$.\n\nThe antithetic-pair estimator is $\\hat{\\theta}_{\\mathrm{ant}} = \\frac{1}{2}\\big(f(X) + f(X^{-})\\big)$. We use the general formula for the variance of a sum of two random variables, $\\operatorname{Var}(aY_1 + bY_2) = a^2\\operatorname{Var}(Y_1) + b^2\\operatorname{Var}(Y_2) + 2ab\\operatorname{Cov}(Y_1, Y_2)$.\nHere, $Y_1 = f(X)$, $Y_2 = f(X^{-})$, and $a = b = \\frac{1}{2}$.\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}}) = \\operatorname{Var}\\left(\\frac{1}{2}f(X) + \\frac{1}{2}f(X^{-})\\right) = \\left(\\frac{1}{2}\\right)^2 \\operatorname{Var}(f(X)) + \\left(\\frac{1}{2}\\right)^2 \\operatorname{Var}(f(X^{-})) + 2\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right)\\operatorname{Cov}(f(X), f(X^{-})) $$\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}}) = \\frac{1}{4}\\operatorname{Var}(f(X)) + \\frac{1}{4}\\operatorname{Var}(f(X^{-})) + \\frac{1}{2}\\operatorname{Cov}(f(X), f(X^{-})) $$\nFrom part $1$, we know that $X$ and $X^{-}$ are identically distributed. This implies that the random variables $f(X)$ and $f(X^{-})$ are also identically distributed, and thus have the same variance: $\\operatorname{Var}(f(X)) = \\operatorname{Var}(f(X^{-}))$.\nSubstituting this into the equation, we get:\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}}) = \\frac{1}{4}\\operatorname{Var}(f(X)) + \\frac{1}{4}\\operatorname{Var}(f(X)) + \\frac{1}{2}\\operatorname{Cov}(f(X), f(X^{-})) $$\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}}) = \\frac{1}{2}\\operatorname{Var}(f(X)) + \\frac{1}{2}\\operatorname{Cov}(f(X), f(X^{-})) $$\nThis is the desired expression.\n\n3. Derivation of the variance ratio $R$.\n\nFirst, we compute the variance of the independent-sample estimator, $\\hat{\\theta}_{\\mathrm{ind}} = \\frac{1}{2}\\big(f(X^{(1)}) + f(X^{(2)})\\big)$. The vectors $X^{(1)}$ and $X^{(2)}$ are independent and identically distributed (i.i.d.) as $\\mathcal{N}_{d}(\\mu, \\Sigma)$. Consequently, the random variables $f(X^{(1)})$ and $f(X^{(2)})$ are i.i.d. Their independence means their covariance is zero: $\\operatorname{Cov}(f(X^{(1)}), f(X^{(2)})) = 0$.\nThe variance is:\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ind}}) = \\operatorname{Var}\\left(\\frac{1}{2}f(X^{(1)}) + \\frac{1}{2}f(X^{(2)})\\right) = \\left(\\frac{1}{2}\\right)^2\\operatorname{Var}(f(X^{(1)})) + \\left(\\frac{1}{2}\\right)^2\\operatorname{Var}(f(X^{(2)})) + 0 $$\nSince they are identically distributed, $\\operatorname{Var}(f(X^{(1)})) = \\operatorname{Var}(f(X^{(2)})) = \\operatorname{Var}(f(X))$.\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ind}}) = \\frac{1}{4}\\operatorname{Var}(f(X)) + \\frac{1}{4}\\operatorname{Var}(f(X)) = \\frac{1}{2}\\operatorname{Var(f(X))} $$\nThe variance ratio $R$ is defined as $R := \\frac{\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}})}{\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ind}})}$. Substituting the expressions we derived:\n$$ R = \\frac{\\frac{1}{2}\\operatorname{Var}(f(X)) + \\frac{1}{2}\\operatorname{Cov}(f(X), f(X^{-}))}{\\frac{1}{2}\\operatorname{Var}(f(X))} = 1 + \\frac{\\operatorname{Cov}(f(X), f(X^{-}))}{\\operatorname{Var}(f(X))} $$\nThe correlation coefficient $\\rho$ is defined as $\\rho := \\operatorname{Corr}(f(X), f(X^{-})) = \\frac{\\operatorname{Cov}(f(X), f(X^{-}))}{\\sqrt{\\operatorname{Var}(f(X)) \\operatorname{Var}(f(X^{-}))}}$.\nSince $\\operatorname{Var}(f(X)) = \\operatorname{Var}(f(X^{-}))$, this simplifies to $\\rho = \\frac{\\operatorname{Cov}(f(X), f(X^{-}))}{\\operatorname{Var}(f(X))}$.\nSubstituting this into our expression for $R$, we obtain:\n$$ R = 1 + \\rho $$\n\n4. Calculation of $\\rho$ and $R$ for an odd function $f$.\n\nWe are given that $f$ is odd about $\\mu$, meaning $f(\\mu + y) = -f(\\mu - y)$ for all $y \\in \\mathbb{R}^{d}$.\nLet us express $f(X)$ and $f(X^{-})$ using this property. We have $X = \\mu + AZ$ and $X^{-} = \\mu - AZ$. Let $y = AZ$. Then, we can write:\n$f(X) = f(\\mu + AZ)$\n$f(X^{-}) = f(\\mu - AZ)$\nUsing the odd-function property with $y = AZ$, we have $f(\\mu + AZ) = -f(\\mu - AZ)$.\nThis implies a deterministic relationship between the random variables $f(X)$ and $f(X^{-})$:\n$$ f(X) = -f(X^{-}) $$\nThis relation holds for every realization of $Z$. Now we compute the correlation $\\rho = \\operatorname{Corr}(f(X), f(X^{-}))$.\nUsing the relation $f(X^{-}) = -f(X)$, we have:\n$$ \\rho = \\operatorname{Corr}(f(X), -f(X)) $$\nUsing the properties of covariance and variance, $\\operatorname{Cov}(U, -V) = -\\operatorname{Cov}(U, V)$ and $\\operatorname{Var}(-U) = \\operatorname{Var}(U)$:\n$$ \\rho = \\frac{\\operatorname{Cov}(f(X), -f(X))}{\\sqrt{\\operatorname{Var}(f(X))\\operatorname{Var}(-f(X))}} = \\frac{-\\operatorname{Cov}(f(X), f(X))}{\\sqrt{\\operatorname{Var}(f(X))\\operatorname{Var}(f(X))}} = \\frac{-\\operatorname{Var}(f(X))}{\\operatorname{Var}(f(X))} $$\nAssuming $\\operatorname{Var}(f(X)) > 0$ (if not, $f(X)$ is a constant, and the problem is trivial), we get:\n$$ \\rho = -1 $$\nA perfect negative correlation is achieved.\nFinally, we can deduce the value of the variance ratio $R$:\n$$ R = 1 + \\rho = 1 + (-1) = 0 $$\nThis result signifies that for a function odd about $\\mu$, the antithetic variates technique yields a zero-variance estimator, which is the maximum possible variance reduction. The estimator $\\hat{\\theta}_{\\mathrm{ant}} = \\frac{1}{2}(f(X)+f(X^-)) = \\frac{1}{2}(f(X)-f(X)) = 0$, so its variance is indeed $0$.\n\nThe final answer combines the expression for $R$ in terms of $\\rho$ and the value of $R$ for the case of an odd function $f$.", "answer": "$$ \\boxed{R = 1 + \\rho; \\text{ for an odd function } f, R=0} $$", "id": "3322640"}, {"introduction": "Building on the theme of variance reduction, we now turn to control variates, a powerful and widely applicable technique. This method leverages our knowledge of the distribution's moments—quantities for which we know the expectation, like the mean—to correct for sampling error in our estimate of an unknown expectation. This problem challenges you to derive the optimal control variate coefficients for a multivariate normal sample under a linear approximation, revealing a fundamental connection between the estimator's variance and the gradient of the target function [@problem_id:3322639].", "problem": "Let $X \\in \\mathbb{R}^{3}$ be a random vector with multivariate normal distribution $X \\sim \\mathcal{N}(\\mu,\\Sigma)$, where the mean and covariance are known and given by\n$$\n\\mu \\;=\\; \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}, \n\\qquad\n\\Sigma \\;=\\; \\begin{pmatrix} 2  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{pmatrix}.\n$$\nYou wish to estimate $\\mathbb{E}[g(X)]$ by Monte Carlo (MC) simulation using a control variate (CV) based on the known first two moments of $X$. Consider the scalar function\n$$\ng(x) \\;=\\; \\ln\\!\\big(1 + x_{1}^{2}\\big) \\;+\\; \\exp\\!\\big(\\tfrac{x_{2}}{2}\\big) \\;-\\; \\cos(x_{3}) \\;+\\; 2 x_{1} \\;-\\; 3 x_{2} \\;+\\; 4 x_{3}^{2}, \\quad x = (x_{1},x_{2},x_{3})^{\\top}.\n$$\nYou will use the $3$-dimensional control variate $C(X) = X - \\mu$, whose expectation is $\\mathbb{E}[C(X)] = 0$. The controlled estimator of $\\mathbb{E}[g(X)]$ based on a single draw is\n$$\nY_{\\beta} \\;=\\; g(X) \\;-\\; \\beta^{\\top} (X - \\mu),\n$$\nwhere $\\beta \\in \\mathbb{R}^{3}$ is a coefficient vector to be chosen to reduce variance.\n\nStarting only from the definitions of variance and covariance and the fact that $X$ is multivariate normal with known $\\mu$ and $\\Sigma$, derive the optimal $\\beta$ that minimizes $\\operatorname{Var}(Y_{\\beta})$ under the approximation that $g$ is locally linear around $\\mu$. Then, for the specific $g$ and $\\mu$ given above, compute the resulting optimal coefficient vector. Express your final answer as a single row vector in exact analytic form. Do not round.", "solution": "The problem asks for the optimal coefficient vector $\\beta \\in \\mathbb{R}^{3}$ that minimizes the variance of the controlled estimator $Y_{\\beta} = g(X) - \\beta^{\\top} (X - \\mu)$, under a local linearity approximation for the function $g(x)$.\n\nFirst, we derive the general expression for the optimal $\\beta$. The objective is to minimize the variance of $Y_{\\beta}$:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\operatorname{Var}(Y_{\\beta}) = \\min_{\\beta \\in \\mathbb{R}^{3}} \\operatorname{Var}(g(X) - \\beta^{\\top}(X - \\mu))\n$$\nUsing the properties of variance, we can expand this expression. For a scalar random variable $A$ and a vector random variable $B$, $\\operatorname{Var}(A - \\beta^{\\top}B) = \\operatorname{Var}(A) - 2\\operatorname{Cov}(A, \\beta^\\top B) + \\operatorname{Var}(\\beta^\\top B)$.\nLet $A = g(X)$ and $B = X - \\mu$. The variance of the controlled estimator is:\n$$\n\\operatorname{Var}(Y_{\\beta}) = \\operatorname{Var}(g(X)) - 2\\operatorname{Cov}(g(X), \\beta^{\\top}(X-\\mu)) + \\operatorname{Var}(\\beta^{\\top}(X-\\mu))\n$$\nLet's analyze the covariance and variance terms involving $\\beta$.\nThe covariance term is:\n$$\n\\operatorname{Cov}(g(X), \\beta^{\\top}(X-\\mu)) = \\operatorname{Cov}\\left(g(X), \\sum_{i=1}^{3} \\beta_i (X_i - \\mu_i)\\right) = \\sum_{i=1}^{3} \\beta_i \\operatorname{Cov}(g(X), X_i - \\mu_i)\n$$\nSince $\\mu_i$ is a constant, $\\operatorname{Cov}(g(X), X_i - \\mu_i) = \\operatorname{Cov}(g(X), X_i)$. Let's define the column vector of covariances $\\mathbf{c}_{gX}$ as:\n$$\n\\mathbf{c}_{gX} = \\begin{pmatrix} \\operatorname{Cov}(g(X), X_1) \\\\ \\operatorname{Cov}(g(X), X_2) \\\\ \\operatorname{Cov}(g(X), X_3) \\end{pmatrix}\n$$\nThen, the covariance term can be written in vector notation as $\\beta^{\\top}\\mathbf{c}_{gX}$.\n\nThe variance term involving $\\beta$ is:\n$$\n\\operatorname{Var}(\\beta^{\\top}(X-\\mu)) = \\beta^{\\top}\\operatorname{Cov}(X-\\mu, X-\\mu)\\beta = \\beta^{\\top}\\operatorname{Cov}(X, X)\\beta = \\beta^{\\top}\\Sigma\\beta\n$$\nwhere $\\Sigma = \\operatorname{Cov}(X,X)$ is the covariance matrix of $X$.\n\nSubstituting these back into the expression for $\\operatorname{Var}(Y_{\\beta})$, we get a quadratic function of $\\beta$:\n$$\nL(\\beta) = \\operatorname{Var}(Y_{\\beta}) = \\operatorname{Var}(g(X)) - 2\\beta^{\\top}\\mathbf{c}_{gX} + \\beta^{\\top}\\Sigma\\beta\n$$\nTo find the optimal $\\beta$ that minimizes this variance, we take the gradient of $L(\\beta)$ with respect to $\\beta$ and set it to zero. Using standard matrix calculus identities ($\\nabla_x(a^{\\top}x) = a$ and $\\nabla_x(x^{\\top}Ax) = 2Ax$ for a symmetric matrix $A$), we have:\n$$\n\\nabla_{\\beta}L(\\beta) = -2\\mathbf{c}_{gX} + 2\\Sigma\\beta\n$$\nSetting the gradient to zero to find the optimal $\\beta$, denoted $\\beta_{\\text{opt}}$:\n$$\n-2\\mathbf{c}_{gX} + 2\\Sigma\\beta_{\\text{opt}} = 0 \\implies \\Sigma\\beta_{\\text{opt}} = \\mathbf{c}_{gX}\n$$\nThe problem states that the covariance matrix $\\Sigma$ is known. We can verify that it is invertible (and positive definite), thus we can solve for $\\beta_{\\text{opt}}$:\n$$\n\\beta_{\\text{opt}} = \\Sigma^{-1}\\mathbf{c}_{gX} = \\Sigma^{-1} \\operatorname{Cov}(g(X), X)\n$$\nThe problem specifies that we must proceed under the approximation that $g$ is locally linear around $\\mu$. This means we can use the first-order Taylor expansion of $g(X)$ around $X=\\mu$:\n$$\ng(X) \\approx g(\\mu) + \\nabla g(\\mu)^{\\top}(X-\\mu)\n$$\nwhere $\\nabla g(\\mu)$ is the gradient of $g$ evaluated at $\\mu$. Under this approximation, we can compute the covariance vector $\\mathbf{c}_{gX}$. The $i$-th component is:\n$$\n(\\mathbf{c}_{gX})_i = \\operatorname{Cov}(g(X), X_i) \\approx \\operatorname{Cov}(g(\\mu) + \\nabla g(\\mu)^{\\top}(X-\\mu), X_i)\n$$\nSince $g(\\mu)$ is a constant, its covariance with any variable is zero. Let $c = \\nabla g(\\mu)$, which is a constant vector.\n$$\n(\\mathbf{c}_{gX})_i \\approx \\operatorname{Cov}(c^{\\top}(X-\\mu), X_i) = \\operatorname{Cov}\\left(\\sum_{j=1}^{3} c_j(X_j-\\mu_j), X_i\\right)\n$$\n$$\n= \\sum_{j=1}^{3} c_j \\operatorname{Cov}(X_j-\\mu_j, X_i) = \\sum_{j=1}^{3} c_j \\operatorname{Cov}(X_j, X_i) = \\sum_{j=1}^{3} \\Sigma_{ij} c_j\n$$\nThe last expression is the $i$-th component of the matrix-vector product $\\Sigma c$. Therefore, the covariance vector is approximated as:\n$$\n\\mathbf{c}_{gX} \\approx \\Sigma \\nabla g(\\mu)\n$$\nSubstituting this approximation into our expression for $\\beta_{\\text{opt}}$:\n$$\n\\beta_{\\text{opt}} \\approx \\Sigma^{-1}(\\Sigma \\nabla g(\\mu)) = \\nabla g(\\mu)\n$$\nSo, the optimal coefficient vector under the linear approximation is simply the gradient of the function $g$ evaluated at the mean $\\mu$.\n\nNow, we compute this for the specific function $g(x)$ and mean $\\mu$ provided.\nThe function is:\n$$\ng(x) = \\ln(1 + x_{1}^{2}) + \\exp(\\tfrac{x_{2}}{2}) - \\cos(x_{3}) + 2 x_{1} - 3 x_{2} + 4 x_{3}^{2}\n$$\nThe gradient vector $\\nabla g(x)$ is:\n$$\n\\nabla g(x) = \\begin{pmatrix} \\frac{\\partial g}{\\partial x_1} \\\\ \\frac{\\partial g}{\\partial x_2} \\\\ \\frac{\\partial g}{\\partial x_3} \\end{pmatrix} = \\begin{pmatrix} \\frac{2x_1}{1+x_1^2} + 2 \\\\ \\frac{1}{2}\\exp(\\frac{x_2}{2}) - 3 \\\\ \\sin(x_3) + 8x_3 \\end{pmatrix}\n$$\nThe mean vector is given as $\\mu = (1, -2, 0)^{\\top}$. We evaluate the gradient at $\\mu$:\n$$\n\\beta_{\\text{opt}} = \\nabla g(\\mu) = \\nabla g(1, -2, 0)\n$$\nThe components are:\n1.  $\\frac{\\partial g}{\\partial x_1}\\bigg|_{\\mu} = \\frac{2(1)}{1+(1)^2} + 2 = \\frac{2}{2} + 2 = 1 + 2 = 3$\n2.  $\\frac{\\partial g}{\\partial x_2}\\bigg|_{\\mu} = \\frac{1}{2}\\exp\\left(\\frac{-2}{2}\\right) - 3 = \\frac{1}{2}\\exp(-1) - 3 = \\frac{1}{2e} - 3$\n3.  $\\frac{\\partial g}{\\partial x_3}\\bigg|_{\\mu} = \\sin(0) + 8(0) = 0 + 0 = 0$\n\nThus, the optimal coefficient vector is:\n$$\n\\beta_{\\text{opt}} = \\begin{pmatrix} 3 \\\\ \\frac{1}{2e} - 3 \\\\ 0 \\end{pmatrix}\n$$\nThe problem asks for the answer as a single row vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} 3  \\frac{1}{2e} - 3  0 \\end{pmatrix}}\n$$", "id": "3322639"}, {"introduction": "Theory comes to life through implementation. This comprehensive coding exercise situates the concepts of efficient sampling within a practical and important application: Bayesian linear regression. You will implement and compare the performance of standard IID sampling, the antithetic method, and the advanced quasi-Monte Carlo (QMC) technique for estimating predictive quantities. This practice will not only solidify your understanding of these methods but also provide a tangible demonstration of their convergence properties and practical benefits in a high-dimensional setting [@problem_id:3322642].", "problem": "Construct a complete program that, for multiple high-dimensional Bayesian linear regression instances with Gaussian priors, compares three Monte Carlo strategies for approximating predictive moments under a multivariate normal posterior: independent and identically distributed sampling, antithetic pairing, and quasi-Monte Carlo sampling via Sobol sequences transformed to a multivariate normal. The goal is to demonstrate improved convergence of predictive moments under quasi-multivariate normal sampling and to quantify the variance reduction properties of antithetic pairing for linear functionals.\n\nYou must work from the following fundamental base:\n- The linear-Gaussian model is given by $y = X w + \\epsilon$, with $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ and prior $w \\sim \\mathcal{N}(0, \\tau^2 I_d)$.\n- For Gaussian likelihood and Gaussian prior, the posterior $p(w \\mid y, X)$ is Gaussian. Its precision matrix is the sum of prior precision and data precision. Avoid any explicit matrix inversion; rely only on linear algebra identities and factorizations (for example, Cholesky factorization of a positive definite precision matrix) and triangular solves.\n- If $A$ is a symmetric positive definite matrix and $A = R^\\top R$ is its Cholesky factorization with $R$ upper triangular, then sampling $z \\sim \\mathcal{N}(0, I_d)$ and setting $u = R^{-1} z$ yields $\\operatorname{Cov}(u) = A^{-1}$. For multivariate normal sampling with covariance $A^{-1}$ and mean $\\mu$, one can use $w = \\mu + u$.\n- Antithetic pairing uses paired reference draws $z$ and $-z$ for symmetric distributions to reduce estimator variance of odd integrands.\n- Quasi-Monte Carlo sampling maps a low-discrepancy sequence $u \\in (0,1)^d$ to a standard normal vector $z$ via the inverse cumulative distribution function, then to the target multivariate normal via a linear transform.\n\nProgram requirements:\n1) For each test case below, generate data as follows, using the specified random number generator seeds (all seeds are nonnegative integers, to be used with any language’s pseudo-random number generator). All vectors and matrices are to be real-valued.\n   - Draw design matrix $X \\in \\mathbb{R}^{n \\times d}$ with rows independently distributed as $\\mathcal{N}(0, \\Sigma_X)$, where $(\\Sigma_X)_{ij} = \\rho^{|i-j|}$ for all $i, j \\in \\{1,\\dots,d\\}$ and $|\\rho|  1$. Use a Cholesky factorization of $\\Sigma_X$ to generate these rows without relying on a generic multivariate normal routine. For the generation of $X$, use the seed $s_X$.\n   - Draw a true coefficient vector $w_{true} \\sim \\mathcal{N}(0, \\tau^2 I_d)$ using seed $s_w$, and draw noise $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ using seed $s_{epsilon}$. Then set $y = X w_{true} + \\epsilon$.\n   - Draw a prediction covariate $x_* \\sim \\mathcal{N}(0, \\Sigma_X)$ using the same $\\Sigma_X$, with seed $s_{star}$.\n2) Compute the posterior precision matrix $A$ and posterior mean $\\mu$ using linear algebraic identities for the Gaussian model. Factor $A$ via a Cholesky factorization $A = R^\\top R$ with $R$ upper triangular. Do not invert any matrix explicitly.\n3) Compute the exact predictive mean $\\mu_* = \\mathbb{E}[x_*^\\top w \\mid y, X]$ and exact predictive variance $v_* = \\operatorname{Var}(x_*^\\top w \\mid y, X) + \\sigma^2$ by expressing them in terms of $A$, $\\mu$, and $x_*$, again using linear solves without explicit matrix inversion.\n4) Implement three sampling estimators to approximate the predictive mean and variance of $y_*$:\n   - Independent and identically distributed (IID) sampler: draw $N$ independent $z \\sim \\mathcal{N}(0, I_d)$ and map to $w = \\mu + R^{-1} z$. For the IID sampler, use seed $s_{\\text{iid}}$.\n   - Antithetic sampler: draw $N/2$ independent $z \\sim \\mathcal{N}(0, I_d)$ and include both $z$ and $-z$, mapping each to $w = \\mu + R^{-1} z$. Use seed $s_{\\text{anti}}$. Assume $N$ is even for all sample sizes considered.\n   - Quasi-Monte Carlo (QMC) sampler: create a Sobol sequence in $[0,1)^d$ with Owen scrambling using seed $s_{\\text{qmc}}$, transform componentwise via the inverse standard normal cumulative distribution function to obtain $z \\in \\mathbb{R}^d$, and then map to $w = \\mu + R^{-1} z$. Use power-of-two sample sizes and the first $N$ Sobol points for each $N$.\n5) For each sampler and each $N$ in the sample size set, approximate:\n   - The predictive mean by the sample mean of $x_*^\\top w$.\n   - The predictive variance by the sample variance of $x_*^\\top w$ (using population normalization) plus $\\sigma^2$.\n6) For each sampler, accumulate the integrated absolute errors across the sample sizes $N \\in \\{128, 512, 2048, 8192\\}$:\n   - For predictive mean: $E_{\\text{mean}} = \\sum_{N} \\left| \\hat{\\mu}_*(N) - \\mu_* \\right|$.\n   - For predictive variance: $E_{\\text{var}} = \\sum_{N} \\left| \\hat{v}_*(N) - v_* \\right|$.\n7) For each test case, report three boolean outcomes:\n   - Whether antithetic pairing improves the predictive mean convergence relative to IID: $E_{\\text{mean}}^{\\text{anti}}  E_{\\text{mean}}^{\\text{iid}}$.\n   - Whether quasi-Monte Carlo improves the predictive mean convergence relative to IID: $E_{\\text{mean}}^{\\text{qmc}}  E_{\\text{mean}}^{\\text{iid}}$.\n   - Whether quasi-Monte Carlo improves the predictive variance convergence relative to IID: $E_{\\text{var}}^{\\text{qmc}}  E_{\\text{var}}^{\\text{iid}}$.\n\nTest suite:\n- Case A (high-dimensional, moderately ill-conditioned):\n  - $n = 60$, $d = 50$, $\\tau = 1.0$, $\\sigma = 0.5$, $\\rho = 0.7$,\n  - $s_X = 1729$, $s_w = 2718$, $s_{epsilon} = 31415$, $s_{star} = 4242$,\n  - $s_{\\text{iid}} = 7771$, $s_{\\text{anti}} = 7772$, $s_{\\text{qmc}} = 12345$.\n- Case B (underdetermined, $d \\gg n$):\n  - $n = 30$, $d = 100$, $\\tau = 1.5$, $\\sigma = 1.0$, $\\rho = 0.5$,\n  - $s_X = 2021$, $s_w = 1618$, $s_{epsilon} = 1414$, $s_{star} = 1732$,\n  - $s_{\\text{iid}} = 8881$, $s_{\\text{anti}} = 8882$, $s_{\\text{qmc}} = 23456$.\n- Case C (near-collinearity):\n  - $n = 80$, $d = 80$, $\\tau = 0.8$, $\\sigma = 0.3$, $\\rho = 0.95$,\n  - $s_X = 271828$, $s_w = 161803$, $s_{epsilon} = 141421$, $s_{star} = 173205$,\n  - $s_{\\text{iid}} = 9991$, $s_{\\text{anti}} = 9992$, $s_{\\text{qmc}} = 34567$.\n\nImplementation notes:\n- All linear algebra must avoid explicit matrix inversion. Use only factorizations and triangular solves.\n- The Sobol-based quasi-Monte Carlo must use a scrambled Sobol generator with the provided seed and power-of-two sample sizes $N \\in \\{128, 512, 2048, 8192\\}$; use the first $N$ points for each $N$. Map to standard normal via the inverse cumulative distribution function componentwise.\n- Clip any uniform values used in the inverse cumulative distribution function mapping to a closed interval $[10^{-12}, 1-10^{-12}]$ to avoid infinities.\n- Angles do not appear; physical units do not apply.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list of bracketed triplets with no spaces, e.g., \"[[True,True,True],[True,True,True],[True,True,True]]\". Each inner triplet corresponds to a case and is ordered as $[\\text{antithetic improves mean}, \\text{quasi improves mean}, \\text{quasi improves variance}]$.", "solution": "The problem requires a comparative analysis of three Monte Carlo methods for approximating predictive moments in a high-dimensional Bayesian linear regression model. The methods are independent and identically distributed (IID) sampling, antithetic sampling, and quasi-Monte Carlo (QMC) sampling. The solution involves deriving the posterior distribution, calculating the exact predictive moments, implementing the three samplers, and evaluating their performance based on integrated absolute error.\n\n### 1. Bayesian Linear Regression Model and Posterior Distribution\n\nThe specified model is a linear relationship between a design matrix $X \\in \\mathbb{R}^{n \\times d}$ and a response vector $y \\in \\mathbb{R}^n$, with Gaussian noise.\nThe likelihood of the data is given by $p(y \\mid w, \\sigma^2) = \\mathcal{N}(y \\mid Xw, \\sigma^2 I_n)$, where $w \\in \\mathbb{R}^d$ is the vector of regression coefficients and $\\sigma^2$ is the noise variance. The likelihood function is proportional to:\n$$\np(y \\mid w) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} (y - Xw)^\\top (y - Xw)\\right)\n$$\nA Gaussian prior is placed on the weights, $p(w \\mid \\tau^2) = \\mathcal{N}(w \\mid 0, \\tau^2 I_d)$, where $\\tau^2$ is the prior variance. The prior is proportional to:\n$$\np(w) \\propto \\exp\\left(-\\frac{1}{2\\tau^2} w^\\top w\\right)\n$$\nAccording to Bayes' theorem, the posterior distribution $p(w \\mid y, X)$ is proportional to the product of the likelihood and the prior, $p(w \\mid y, X) \\propto p(y \\mid w) p(w)$. The exponent of the posterior is the sum of the exponents of the likelihood and the prior:\n$$\n-\\frac{1}{2\\sigma^2} (y^\\top y - 2y^\\top Xw + w^\\top X^\\top X w) - \\frac{1}{2\\tau^2} w^\\top w\n$$\nGrouping terms with respect to $w$, we get:\n$$\n-\\frac{1}{2} \\left( w^\\top \\left(\\frac{1}{\\sigma^2} X^\\top X + \\frac{1}{\\tau^2} I_d\\right) w - 2 \\left(\\frac{1}{\\sigma^2} y^\\top X\\right) w \\right) + \\text{const}\n$$\nThis is the exponent of a multivariate Gaussian distribution, confirming that the posterior is Gaussian, $p(w \\mid y, X) = \\mathcal{N}(w \\mid \\mu, A^{-1})$. By \"completing the square\" or comparing with the general Gaussian form $\\exp(-\\frac{1}{2}(w-\\mu)^\\top A(w-\\mu))$, we identify the posterior precision matrix $A$ and the posterior mean $\\mu$:\nThe posterior precision matrix is:\n$$\nA = \\frac{1}{\\sigma^2} X^\\top X + \\frac{1}{\\tau^2} I_d\n$$\nThe posterior mean $\\mu$ is the solution to the linear system:\n$$\nA\\mu = \\frac{1}{\\sigma^2} X^\\top y\n$$\nTo compute $\\mu$ without explicit matrix inversion, we first compute the Cholesky factorization of the symmetric positive definite matrix $A$, such that $A = R^\\top R$ where $R$ is upper triangular. Then, we solve $R^\\top R \\mu = \\frac{1}{\\sigma^2} X^\\top y$ using two triangular solves: first solving $R^\\top v = \\frac{1}{\\sigma^2} X^\\top y$ for $v$ (forward substitution), and then solving $R\\mu = v$ for $\\mu$ (backward substitution).\n\n### 2. Exact Predictive Moments\n\nFor a new covariate vector $x_* \\in \\mathbb{R}^d$, the predicted response is $y_* = x_*^\\top w + \\epsilon_*$, where $w$ is drawn from the posterior. The predictive quantity of interest is $x_*^\\top w$. Since $w$ is Gaussian, $x_*^\\top w$ is a univariate Gaussian.\n\nThe exact predictive mean of $x_*^\\top w$ is:\n$$\n\\mu_* = \\mathbb{E}[x_*^\\top w \\mid y, X] = x_*^\\top \\mathbb{E}[w \\mid y, X] = x_*^\\top \\mu\n$$\nThe exact predictive variance of $y_*$ includes both the uncertainty in $w$ and the observational noise $\\sigma^2$. The variance of the term $x_*^\\top w$ is given by:\n$$\n\\operatorname{Var}(x_*^\\top w \\mid y, X) = x_*^\\top \\operatorname{Cov}(w) x_* = x_*^\\top A^{-1} x_*\n$$\nThe total predictive variance $v_*$ is:\n$$\nv_* = \\operatorname{Var}(x_*^\\top w \\mid y, X) + \\sigma^2 = x_*^\\top A^{-1} x_* + \\sigma^2\n$$\nTo compute $x_*^\\top A^{-1} x_*$ without inverting $A$, we solve the linear system $Av = x_*$ for $v$. Then, the variance term is $x_*^\\top v$. This linear system is solved using the Cholesky factor $R$ as before: solve $R^\\top u = x_*$ for $u$ and then $Rv = u$ for $v$.\n\n### 3. Monte Carlo Estimation Strategies\n\nThe core of the problem is to approximate $\\mu_*$ and $v_*$ using samples from the posterior $p(w \\mid y, X) = \\mathcal{N}(\\mu, A^{-1})$. To draw a sample $w$ from this distribution, we first draw a standard normal vector $z \\sim \\mathcal{N}(0, I_d)$ and then apply an affine transformation:\n$$\nw = \\mu + R^{-1} z\n$$\nThis is valid because if $z \\sim \\mathcal{N}(0, I_d)$, then $u = R^{-1} z$ has covariance $\\operatorname{Cov}(u) = R^{-1} \\operatorname{Cov}(z) (R^{-1})^\\top = R^{-1} (R^\\top)^{-1} = (R^\\top R)^{-1} = A^{-1}$. The term $R^{-1}z$ is computed by solving $R u = z$ for $u$ via backward substitution. The three sampling methods differ in how they generate the sequence of $z$ vectors.\n\n**Independent and Identically Distributed (IID) Sampling:** This is the baseline method. A set of $N$ vectors $\\{z_i\\}_{i=1}^N$ is drawn independently from $\\mathcal{N}(0, I_d)$, leading to $N$ posterior samples $\\{w_i\\}_{i=1}^N$. The approximation error for Monte Carlo integration typically decreases at a rate of $O(N^{-1/2})$.\n\n**Antithetic Sampling:** This variance reduction technique exploits symmetries in the integrand. For a symmetric distribution like $\\mathcal{N}(0, I_d)$, if we draw a sample $z$, we also include its antithetic pair $-z$. For an integrand $f(z)$ that is an odd function, the average of the pair $[f(z) + f(-z)]/2 = 0$, perfectly canceling the variation. The target for the predictive mean is $x_*^\\top w = x_*^\\top \\mu + x_*^\\top R^{-1} z$. The term $x_*^\\top R^{-1} z$ is an odd function of $z$. Each antithetic pair average for this term is $0$, so the estimator for $\\mathbb{E}[x_*^\\top R^{-1} z]$ is exactly $0$, and the estimator for $\\mathbb{E}[x_*^\\top w]$ becomes exactly $x_*^\\top \\mu$. This provides a massive variance reduction for the mean estimate. For an even integrand, like the one for variance, antithetic sampling provides no benefit.\n\n**Quasi-Monte Carlo (QMC) Sampling:** QMC methods replace random samples with deterministic, low-discrepancy sequences, such as the Sobol sequence. These points fill the sample space more uniformly than pseudo-random points. A Sobol sequence generates points $u_i$ in the unit hypercube $[0,1)^d$. To obtain standard normal samples $z_i$, each component of $u_i$ is transformed using the inverse of the standard normal cumulative distribution function (CDF), also known as the probit function. For well-behaved integrands, QMC methods can achieve a faster convergence rate, approaching $O(N^{-1})$, significantly outperforming IID sampling. Scrambling the Sobol sequence further improves its properties by breaking up deterministic patterns while preserving low discrepancy.\n\n### 4. Algorithm and Implementation\n\nFor each test case, the program will execute the following steps:\n1.  **Data Generation:**\n    - Construct the $d \\times d$ covariance matrix $\\Sigma_X$ with elements $(\\Sigma_X)_{ij} = \\rho^{|i-j|}$.\n    - Compute the lower Cholesky factor $L_X$ of $\\Sigma_X$ such that $\\Sigma_X = L_X L_X^\\top$.\n    - Using the specified seed $s_X$, generate $n$ vectors $z_{X,i} \\sim \\mathcal{N}(0, I_d)$ and form the rows of $X$ as $x_i^\\top = (L_X z_{X,i})^\\top$.\n    - Generate $w_{true}$, $\\epsilon$, and $x_*$ similarly using their respective seeds and parameters.\n    - Compute the observed data $y = X w_{true} + \\epsilon$.\n\n2.  **Posterior and Exact Moments Calculation:**\n    - Calculate the posterior precision $A = (1/\\sigma^2) X^\\top X + (1/\\tau^2) I_d$.\n    - Compute the upper Cholesky factor $R$ of $A$ such that $A = R^\\top R$.\n    - Calculate the right-hand side $b = (1/\\sigma^2) X^\\top y$.\n    - Solve for the posterior mean $\\mu$ from $R^\\top R \\mu = b$ using two triangular solves.\n    - Calculate the exact predictive mean $\\mu_* = x_*^\\top \\mu$.\n    - Solve for $v$ from $R^\\top R v = x_*$ using two triangular solves.\n    - Calculate the exact predictive variance $v_* = x_*^\\top v + \\sigma^2$.\n\n3.  **Monte Carlo Estimation and Error Accumulation:**\n    - For each sampler (IID, Antithetic, QMC) and each sample size $N \\in \\{128, 512, 2048, 8192\\}$:\n        - Generate $N$ standard normal vectors $\\{z_i\\}$ according to the sampler's logic, using its specified seed. For QMC, this involves generating a scrambled Sobol sequence and applying the inverse normal CDF.\n        - For each $z_i$, solve $R u_i = z_i$ for $u_i$ and form the posterior sample $w_i = \\mu + u_i$.\n        - Compute the sample predictions $p_i = x_*^\\top w_i$.\n        - Estimate the mean $\\hat{\\mu}_*(N)$ as the sample mean of $\\{p_i\\}$.\n        - Estimate the variance $\\hat{v}_*(N)$ as the sample variance of $\\{p_i\\}$ (with $1/N$ normalization) plus $\\sigma^2$.\n        - Calculate the absolute errors $|\\hat{\\mu}_*(N) - \\mu_*|$ and $|\\hat{v}_*(N) - v_*|$.\n    - Sum these absolute errors over all $N$ to get the integrated errors $E_{\\text{mean}}$ and $E_{\\text{var}}$ for each sampler.\n\n4.  **Final Comparison:**\n    - For each test case, perform the three boolean comparisons specified in the problem statement and store the resulting triplet.\n\n5.  **Output:**\n    - Collect the triplets from all test cases and print them in the required format: `[[bool,bool,bool],[bool,bool,bool],[...]]`.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\nfrom scipy.stats import norm\nfrom scipy.stats.qmc import Sobol\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        # Case A: high-dimensional, moderately ill-conditioned\n        {'n': 60, 'd': 50, 'tau': 1.0, 'sigma': 0.5, 'rho': 0.7,\n         's_X': 1729, 's_w': 2718, 's_epsilon': 31415, 's_star': 4242,\n         's_iid': 7771, 's_anti': 7772, 's_qmc': 12345},\n        # Case B: underdetermined, d > n\n        {'n': 30, 'd': 100, 'tau': 1.5, 'sigma': 1.0, 'rho': 0.5,\n         's_X': 2021, 's_w': 1618, 's_epsilon': 1414, 's_star': 1732,\n         's_iid': 8881, 's_anti': 8882, 's_qmc': 23456},\n        # Case C: near-collinearity\n        {'n': 80, 'd': 80, 'tau': 0.8, 'sigma': 0.3, 'rho': 0.95,\n         's_X': 271828, 's_w': 161803, 's_epsilon': 141421, 's_star': 173205,\n         's_iid': 9991, 's_anti': 9992, 's_qmc': 34567},\n    ]\n\n    sample_sizes = [128, 512, 2048, 8192]\n    all_results = []\n    for case_params in test_cases:\n        results = run_case(case_params, sample_sizes)\n        all_results.append(results)\n\n    print(f\"[{','.join(str(r).replace(' ', '') for r in all_results)}]\")\n\ndef run_case(params, sample_sizes):\n    \"\"\"\n    Executes the entire simulation for a single test case.\n    \"\"\"\n    n, d, tau, sigma, rho = params['n'], params['d'], params['tau'], params['sigma'], params['rho']\n    s_X, s_w, s_epsilon, s_star = params['s_X'], params['s_w'], params['s_epsilon'], params['s_star']\n    s_iid, s_anti, s_qmc = params['s_iid'], params['s_anti'], params['s_qmc']\n\n    # 1. Data Generation\n    # Generate covariance matrix Sigma_X and its Cholesky factor\n    indices = np.arange(d)\n    Sigma_X = rho ** np.abs(indices[:, np.newaxis] - indices)\n    L_X = np.linalg.cholesky(Sigma_X)\n\n    # Generate X\n    rng_X = np.random.default_rng(s_X)\n    Z_X = rng_X.standard_normal((n, d))\n    X = Z_X @ L_X.T\n\n    # Generate w_true\n    rng_w = np.random.default_rng(s_w)\n    w_true = tau * rng_w.standard_normal(d)\n\n    # Generate epsilon\n    rng_eps = np.random.default_rng(s_epsilon)\n    epsilon = sigma * rng_eps.standard_normal(n)\n\n    # Generate y\n    y = X @ w_true + epsilon\n\n    # Generate x_star\n    rng_star = np.random.default_rng(s_star)\n    x_star = L_X @ rng_star.standard_normal(d)\n\n    # 2. Posterior and Exact Moments Calculation\n    # Posterior precision matrix A\n    A = (1 / sigma**2) * (X.T @ X) + (1 / tau**2) * np.identity(d)\n    \n    # Cholesky factorization of A (upper triangular)\n    R = cholesky(A, lower=False)\n\n    # Posterior mean mu\n    b = (1 / sigma**2) * (X.T @ y)\n    v = solve_triangular(R, b, trans='T', lower=False)\n    mu = solve_triangular(R, v, trans='N', lower=False)\n\n    # Exact predictive mean mu_star\n    mu_star = x_star @ mu\n\n    # Exact predictive variance v_star\n    v_solve = solve_triangular(R, x_star, trans='T', lower=False)\n    u_solve = solve_triangular(R, v_solve, trans='N', lower=False)\n    var_w_term = x_star @ u_solve\n    v_star = var_w_term + sigma**2\n\n    # 3. Monte Carlo Estimation\n    errors = {}\n    samplers = {\n        'iid': lambda N, seed: np.random.default_rng(seed).standard_normal((N, d)),\n        'anti': lambda N, seed: _generate_antithetic(N, d, seed),\n        'qmc': lambda N, seed: _generate_qmc(N, d, seed)\n    }\n    sampler_seeds = {'iid': s_iid, 'anti': s_anti, 'qmc': s_qmc}\n\n    for name, sampler_func in samplers.items():\n        E_mean, E_var = 0.0, 0.0\n        for N in sample_sizes:\n            z_samples = sampler_func(N, sampler_seeds[name])\n\n            # Transform standard normal samples to posterior samples\n            # u = R^-1 * z\n            u_samples = solve_triangular(R, z_samples.T, lower=False).T\n            w_samples = mu + u_samples\n            \n            # Predictions\n            p_samples = w_samples @ x_star\n\n            # Estimated moments\n            mu_hat = np.mean(p_samples)\n            v_hat = np.var(p_samples, ddof=0) + sigma**2\n\n            # Accumulate errors\n            E_mean += np.abs(mu_hat - mu_star)\n            E_var += np.abs(v_hat - v_star)\n        \n        errors[name] = {'mean': E_mean, 'var': E_var}\n\n    # 4. Final Comparison\n    anti_improves_mean = errors['anti']['mean']  errors['iid']['mean']\n    qmc_improves_mean = errors['qmc']['mean']  errors['iid']['mean']\n    qmc_improves_var = errors['qmc']['var']  errors['iid']['var']\n\n    return [anti_improves_mean, qmc_improves_mean, qmc_improves_var]\n\ndef _generate_antithetic(N, d, seed):\n    \"\"\"Generates N antithetic standard normal samples.\"\"\"\n    rng = np.random.default_rng(seed)\n    half_N = N // 2\n    z_half = rng.standard_normal((half_N, d))\n    return np.vstack((z_half, -z_half))\n\ndef _generate_qmc(N, d, seed):\n    \"\"\"Generates N QMC standard normal samples.\"\"\"\n    sobol_engine = Sobol(d=d, scramble=True, seed=seed)\n    u_samples = sobol_engine.random(n=N)\n    # Clip to avoid infinity from ppf\n    u_samples = np.clip(u_samples, 1e-12, 1 - 1e-12)\n    return norm.ppf(u_samples)\n\nif __name__ == '__main__':\n    # A simple modification to the print statement to match the exact output format\n    # The original solve() function is called to get the results, then formatted.\n    test_cases = [\n        {'n': 60, 'd': 50, 'tau': 1.0, 'sigma': 0.5, 'rho': 0.7, 's_X': 1729, 's_w': 2718, 's_epsilon': 31415, 's_star': 4242, 's_iid': 7771, 's_anti': 7772, 's_qmc': 12345},\n        {'n': 30, 'd': 100, 'tau': 1.5, 'sigma': 1.0, 'rho': 0.5, 's_X': 2021, 's_w': 1618, 's_epsilon': 1414, 's_star': 1732, 's_iid': 8881, 's_anti': 8882, 's_qmc': 23456},\n        {'n': 80, 'd': 80, 'tau': 0.8, 'sigma': 0.3, 'rho': 0.95, 's_X': 271828, 's_w': 161803, 's_epsilon': 141421, 's_star': 173205, 's_iid': 9991, 's_anti': 9992, 's_qmc': 34567},\n    ]\n    sample_sizes = [128, 512, 2048, 8192]\n    all_results = []\n    for case_params in test_cases:\n        results = run_case(case_params, sample_sizes)\n        all_results.append(str(results).replace(' ', ''))\n    print(f\"[{','.join(all_results)}]\")\n```", "id": "3322642"}]}