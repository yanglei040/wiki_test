{"hands_on_practices": [{"introduction": "We begin with a foundational exercise that applies the ratio-of-uniforms method to its most classic use case: the standard normal distribution. This practice will guide you through the complete analytical derivation, from defining the acceptance region $A$ to calculating the method's exact efficiency [@problem_id:3356654]. Mastering these steps provides the core intuition needed for understanding and implementing the algorithm in more complex scenarios.", "problem": "Consider the ratio-of-uniforms construction for sampling from an unnormalized target probability density function (PDF) on the real line given by $f(x)\\propto \\exp(-x^{2}/2)$. In the ratio-of-uniforms method, one samples uniformly over a set $A$ in the $(u,v)$-plane and returns $x=v/u$ for $u0$. Starting only from the defining change of variables $(x,u)\\mapsto (u,v)=(u,ux)$ with $u0$ and the admissibility condition that ensures the target density is proportional to the marginal density of $x$, do the following:\n\n1. Derive the exact analytic inequality describing the admissible set $A$ in the $(u,v)$-plane for this target.\n2. Determine the tight axis-aligned bounding rectangle of $A$ by computing $\\max u$ and $\\max|v|$.\n3. Compute the exact acceptance probability of the algorithm that proposes uniformly over this bounding rectangle and accepts if and only if the proposed point lies in $A$.\n4. In classical rejection sampling using a Gaussian proposal $q_{\\sigma}(x)=(2\\pi\\sigma^{2})^{-1/2}\\exp(-x^{2}/(2\\sigma^{2}))$ with variance parameter $\\sigma^{2}0$, find the choice of $\\sigma$ and the minimal majorizing constant $M$ that minimize the rejection bound for the same unnormalized target $f(x)\\propto \\exp(-x^{2}/2)$, and compute the corresponding acceptance probability.\n\nProvide your final answer as a single row matrix (using parentheses notation) containing, in order: the analytic description of $A$, $\\max u$, $\\max|v|$, the acceptance probability from the ratio-of-uniforms bounding-rectangle scheme, and the optimal Gaussian-proposal rejection-sampling acceptance probability. Give exact expressions; do not approximate numerically. No rounding is required, and no physical units are involved. Express angles, if any, in radians.", "solution": "We will address each of the four parts in sequence. The unnormalized target probability density function is given as $f(x) \\propto \\exp(-x^2/2)$. For definiteness and without loss of generality for the ratio-of-uniforms method, we choose the specific representative $h(x) = \\exp(-x^2/2)$.\n\nFirst, we derive the analytic description of the admissible set $A$. The ratio-of-uniforms method samples points $(u,v)$ uniformly from the set $A$ defined in the $(u,v)$-plane by\n$$ A = \\left\\{ (u, v) \\in \\mathbb{R}^2 \\mid 0  u \\le \\sqrt{h(v/u)} \\right\\} $$\nSubstituting our choice for $h(x)$, we get:\n$$ 0  u \\le \\sqrt{\\exp\\left(-\\frac{(v/u)^2}{2}\\right)} $$\nThis simplifies to:\n$$ 0  u \\le \\exp\\left(-\\frac{v^2}{4u^2}\\right) $$\nSince $u  0$, we can take the natural logarithm of both sides of the inequality $u \\le \\exp(-v^2/(4u^2))$:\n$$ \\ln(u) \\le -\\frac{v^2}{4u^2} $$\nMultiplying by $4u^2$ (which is positive) and rearranging to isolate $v^2$:\n$$ 4u^2 \\ln(u) \\le -v^2 $$\n$$ v^2 \\le -4u^2 \\ln(u) $$\nFor this inequality to have a solution for $v \\in \\mathbb{R}$, the right-hand side must be non-negative. Since $u^2  0$, we require $-4\\ln(u) \\ge 0$, which implies $\\ln(u) \\le 0$. This holds for $u \\in (0, 1]$. Therefore, the set $A$ is described by the conditions:\n$$ 0  u \\le 1 \\quad \\text{and} \\quad v^2 \\le -4u^2 \\ln(u) $$\n\nSecond, we determine the tight axis-aligned bounding rectangle of $A$. This requires finding the maximum values of $u$ and $|v|$ over the set $A$.\nFrom the derivation above, the range of $u$ is $(0, 1]$, so the maximum value of $u$ is:\n$$ \\max u = 1 $$\nTo find the maximum value of $|v|$, we need to maximize the expression $\\sqrt{-4u^2 \\ln(u)}$ over the interval $u \\in (0, 1]$. This is equivalent to maximizing its square, the function $g(u) = -4u^2 \\ln(u)$. We find the maximum by taking the derivative with respect to $u$ and setting it to zero:\n$$ g'(u) = \\frac{d}{du}\\left(-4u^2 \\ln(u)\\right) = -4\\left(2u\\ln(u) + u^2\\frac{1}{u}\\right) = -4u(2\\ln(u) + 1) $$\nSetting $g'(u) = 0$ for $u \\in (0, 1]$ implies $2\\ln(u) + 1 = 0$, which gives $\\ln(u) = -1/2$, so $u = \\exp(-1/2)$.\nTo confirm this is a maximum, we can check the second derivative or note that $g'(u)  0$ for $u  \\exp(-1/2)$ and $g'(u)  0$ for $u  \\exp(-1/2)$. The value of $g(u)$ at the endpoints $u \\to 0^+$ and $u=1$ is $0$. Thus, the global maximum occurs at $u = \\exp(-1/2)$.\nThe maximum value of $g(u)$ is:\n$$ g(\\exp(-1/2)) = -4(\\exp(-1/2))^2 \\ln(\\exp(-1/2)) = -4\\exp(-1)\\left(-\\frac{1}{2}\\right) = 2\\exp(-1) $$\nThis is the maximum value of $v^2$. Therefore, the maximum value of $|v|$ is:\n$$ \\max|v| = \\sqrt{2\\exp(-1)} $$\nThe bounding rectangle is $R = [0, 1] \\times [-\\sqrt{2\\exp(-1)}, \\sqrt{2\\exp(-1)}]$.\n\nThird, we compute the acceptance probability of the algorithm. This is the ratio of the area of the set $A$ to the area of the bounding rectangle $R$.\nThe area of the bounding rectangle is:\n$$ \\text{Area}(R) = (\\max u) \\times (2 \\max|v|) = 1 \\cdot 2\\sqrt{2\\exp(-1)} = 2\\sqrt{2\\exp(-1)} $$\nThe area of the set $A$ is given by the integral:\n$$ \\text{Area}(A) = \\iint_A du\\,dv = \\int_0^1 \\left( \\int_{-\\sqrt{-4u^2\\ln(u)}}^{\\sqrt{-4u^2\\ln(u)}} dv \\right) du = \\int_0^1 2\\sqrt{-4u^2\\ln(u)} \\,du = \\int_0^1 4u\\sqrt{-\\ln(u)} \\,du $$\nTo evaluate this integral, we use the substitution $t = -\\ln(u)$, which implies $u = \\exp(-t)$ and $du = -\\exp(-t)dt$. The limits of integration change from $u \\in (0, 1]$ to $t \\in [\\infty, 0)$.\n$$ \\text{Area}(A) = \\int_\\infty^0 4\\exp(-t)\\sqrt{t} (-\\exp(-t)dt) = \\int_0^\\infty 4\\sqrt{t} \\exp(-2t) dt $$\nWe perform another substitution, $s=2t$, so $t=s/2$ and $dt=ds/2$.\n$$ \\text{Area}(A) = \\int_0^\\infty 4\\sqrt{s/2} \\exp(-s) \\frac{ds}{2} = \\frac{4}{\\sqrt{2} \\cdot 2} \\int_0^\\infty s^{1/2} \\exp(-s) ds = \\sqrt{2} \\int_0^\\infty s^{3/2 - 1} \\exp(-s) ds $$\nThe integral is the Gamma function $\\Gamma(3/2)$. Using $\\Gamma(z+1)=z\\Gamma(z)$ and $\\Gamma(1/2)=\\sqrt{\\pi}$, we have $\\Gamma(3/2) = \\frac{1}{2}\\Gamma(1/2) = \\frac{\\sqrt{\\pi}}{2}$.\n$$ \\text{Area}(A) = \\sqrt{2} \\cdot \\frac{\\sqrt{\\pi}}{2} = \\frac{\\sqrt{2\\pi}}{2} $$\nThe acceptance probability $P_{\\text{acc, RoU}}$ is:\n$$ P_{\\text{acc, RoU}} = \\frac{\\text{Area}(A)}{\\text{Area}(R)} = \\frac{\\sqrt{2\\pi}/2}{2\\sqrt{2\\exp(-1)}} = \\frac{\\sqrt{2\\pi}}{4\\sqrt{2}\\sqrt{\\exp(-1)}} = \\frac{\\sqrt{\\pi}}{4}\\sqrt{\\exp(1)} = \\frac{\\sqrt{\\pi \\exp(1)}}{4} $$\n\nFourth, we analyze the rejection sampling scheme. The unnormalized target is $p^*(x) = \\exp(-x^2/2)$. The proposal is the normalized Gaussian density $q_{\\sigma}(x) = (2\\pi\\sigma^2)^{-1/2}\\exp(-x^2/(2\\sigma^2))$ with $\\sigma^20$. Rejection sampling requires finding a constant $M$ such that $p^*(x) \\le Mq_{\\sigma}(x)$ for all $x$. To maximize the acceptance rate, we must find the minimal such $M$.\n$$ M = \\sup_{x \\in \\mathbb{R}} \\frac{p^*(x)}{q_{\\sigma}(x)} = \\sup_{x \\in \\mathbb{R}} \\frac{\\exp(-x^2/2)}{(2\\pi\\sigma^2)^{-1/2}\\exp(-x^2/(2\\sigma^2))} = \\sup_{x \\in \\mathbb{R}} \\sqrt{2\\pi\\sigma^2} \\exp\\left(x^2\\left(\\frac{1}{2\\sigma^2} - \\frac{1}{2}\\right)\\right) $$\nFor $M$ to be finite, the term in the exponent must be non-positive, i.e., $\\frac{1}{2\\sigma^2} - \\frac{1}{2} \\le 0$, which simplifies to $\\sigma^2 \\ge 1$.\nIf $\\sigma^2  1$, the exponent is negative, and the supremum of the expression is achieved at $x=0$, giving $M(\\sigma^2) = \\sqrt{2\\pi\\sigma^2}$.\nIf $\\sigma^2 = 1$, the exponent is zero, and the expression is constant, $M(1) = \\sqrt{2\\pi}$.\nWe want to find the choice of $\\sigma$ that minimizes $M$. The function $M(\\sigma^2) = \\sqrt{2\\pi\\sigma^2}$ for $\\sigma^2 \\ge 1$ is an increasing function of $\\sigma^2$. Therefore, its minimum occurs at the lowest possible value, $\\sigma^2=1$.\nThe optimal choice is $\\sigma=1$, which yields the minimal majorizing constant $M = \\sqrt{2\\pi}$.\nThe acceptance probability for rejection sampling is given by the formula $P_{\\text{acc, RS}} = \\frac{\\int_{-\\infty}^{\\infty} p^*(x) dx}{M}$.\nThe integral of the unnormalized target is a standard Gaussian integral:\n$$ \\int_{-\\infty}^{\\infty} p^*(x) dx = \\int_{-\\infty}^{\\infty} \\exp(-x^2/2) dx = \\sqrt{2\\pi} $$\nThus, the optimal acceptance probability is:\n$$ P_{\\text{acc, RS}} = \\frac{\\sqrt{2\\pi}}{M} = \\frac{\\sqrt{2\\pi}}{\\sqrt{2\\pi}} = 1 $$\nThis result is expected, as choosing $\\sigma=1$ means the proposal distribution is identical to the normalized target distribution, turning rejection sampling into direct sampling.\n\nThe five requested quantities are collected for the final answer.\n1. Analytic description of $A$: $0  u \\le 1 \\land v^2 \\le -4u^2\\ln(u)$\n2. $\\max u$: $1$\n3. $\\max|v|$: $\\sqrt{2\\exp(-1)}$\n4. Acceptance probability (ratio-of-uniforms): $\\frac{\\sqrt{\\pi \\exp(1)}}{4}$\n5. Acceptance probability (rejection sampling): $1$", "answer": "$$ \\boxed{ \\pmatrix{ 0  u \\le 1 \\land v^2 \\le -4u^2\\ln(u)  1  \\sqrt{2\\exp(-1)}  \\frac{\\sqrt{\\pi \\exp(1)}}{4}  1 } } $$", "id": "3356654"}, {"introduction": "While analytical derivations are insightful, many real-world densities are too complex for closed-form solutions for the bounding box. This practice tackles this crucial issue by having you develop a robust numerical scheme to find the required bounds using only oracle access to the log-density and its gradient [@problem_id:3356655]. This scenario is common in modern Bayesian statistics, and learning to create algorithms with numerical guarantees is an essential skill.", "problem": "Consider a one-dimensional, strictly positive, integrable target density on the real line described only through an oracle that returns its unnormalized log-density and its gradient. Formally, let $g(x) \\propto f(x)$ denote an unnormalized version of the density, and write $\\ell(x) = \\log g(x)$. You are given oracle access to both $\\ell(x)$ and $\\ell'(x)$, and a constant $L \\in (0,\\infty)$ such that $\\ell'(x)$ is $L$-Lipschitz, i.e., for all $x,y \\in \\mathbb{R}$, $\\lvert \\ell'(x) - \\ell'(y) \\rvert \\le L \\lvert x-y \\rvert$. Assume throughout that $\\ell$ is concave (log-concave target), twice continuously differentiable on $\\mathbb{R}$, and that $g$ is integrable on $\\mathbb{R}$, so the log-density has a unique maximizer.\n\nThe ratio-of-uniforms construction for one-dimensional simulation requires bounding the set of admissible pairs by a simple rectangle. This rectangle can be chosen using the quantities\n- $S_0 = \\sup_{x \\in \\mathbb{R}} \\sqrt{g(x)}$,\n- $S_1 = \\sup_{x \\in \\mathbb{R}} \\lvert x \\rvert \\sqrt{g(x)}$.\n\nYour task is to derive, justify, and implement a robust numerical scheme that computes $S_0$ and $S_1$ to high accuracy using only $\\ell$ and $\\ell'$, together with the Lipschitz constant $L$. Your derivation must start from the concavity of $\\ell$ and the Lipschitz continuity of $\\ell'$ and must justify existence and uniqueness of the maximizers, as well as provide non-asymptotic error guarantees for the numerical scheme.\n\nSpecifically, do the following:\n1. Derive a principled global bracketing-and-bisection scheme to locate:\n   - the unique maximizer $x_\\star$ of $\\ell(x)$, and\n   - the unique maximizers on each side of the origin of the function $r(x) = \\ell(x) + 2 \\log \\lvert x \\rvert$ on the intervals $(0,\\infty)$ and $(-\\infty,0)$.\n   Your derivation must justify that $x_\\star$ is well-defined and unique and that the maximizers of $r$ on $(0,\\infty)$ and $(-\\infty,0)$ are well-defined and unique under the given assumptions.\n2. From the bracketing-and-bisection scheme, derive explicit error bounds for the objective values:\n   - Show that if the bracket for $x_\\star$ has width $\\delta_x$, then the error in the objective $\\ell$ at the bisection point is bounded by a quantity that depends on $L$ and $\\delta_x$.\n   - Show that if the bracket for a maximizer of $r$ does not cross the origin and has width $\\delta_x$ and minimum absolute distance from the origin $a = \\min_{x \\text{ in bracket}} \\lvert x \\rvert$, then the error in the objective $r$ at the bisection point is bounded by a quantity that depends on $L$, $a$, and $\\delta_x$.\n   Using these bounds, provide guarantees on the multiplicative error of $S_0 = \\exp(\\tfrac{1}{2}\\sup_x \\ell(x))$ and $S_1 = \\exp(\\tfrac{1}{2}\\sup_x r(x))$.\n3. Implement your scheme in a single program that uses only the oracles $\\ell$ and $\\ell'$, together with $L$, to compute numerical approximations of $S_0$ and $S_1$.\n\nTest Suite. Your program must compute $S_0$ and $S_1$ for the following three oracle-defined, log-concave targets (each case specifies $\\ell$, $\\ell'$, and $L$):\n\n- Case A (standard normal, unnormalized):\n  - $\\ell(x) = -\\tfrac{1}{2} x^2$,\n  - $\\ell'(x) = -x$,\n  - $L = 1$.\n- Case B (logistic, normalized):\n  - $\\ell(x) = -x - 2 \\log\\!\\bigl(1 + e^{-x}\\bigr)$,\n  - $\\ell'(x) = -1 + \\dfrac{2}{1 + e^{x}}$,\n  - $L = \\tfrac{1}{2}$.\n- Case C (normal with mean $\\mu = 1.5$ and standard deviation $\\sigma = 0.5$, unnormalized):\n  - $\\ell(x) = -\\dfrac{(x - \\mu)^2}{2 \\sigma^2} = -\\dfrac{(x - 1.5)^2}{0.5}$,\n  - $\\ell'(x) = -\\dfrac{x - \\mu}{\\sigma^2} = -\\dfrac{x - 1.5}{0.25} = -4(x-1.5)$,\n  - $L = \\dfrac{1}{\\sigma^2} = 4$.\n\nNumerical requirements and guarantees:\n- Your scheme must employ globally valid bracketing-and-bisection on the first derivatives to locate the unique critical points, exploiting concavity to guarantee uniqueness and Lipschitz continuity to obtain objective error bounds.\n- Use a bisection tolerance in $x$ of $\\delta_x = 10^{-12}$ for root-finding. Use the curvature-based objective error bounds you derived to certify the objective errors for $\\ell$ and $r$ at the computed points, and propagate these to multiplicative error bounds for $S_0$ and $S_1$. You do not need to print the error bounds, but your code must adhere to the stopping rule that ensures these objective error guarantees.\n\nFinal output format:\n- For each of the three cases A, B, and C, compute the pair $(S_0, S_1)$ using your scheme.\n- Your program should produce a single line of output containing the six floating-point results in the following order: $[S_{0,\\mathrm{A}}, S_{1,\\mathrm{A}}, S_{0,\\mathrm{B}}, S_{1,\\mathrm{B}}, S_{0,\\mathrm{C}}, S_{1,\\mathrm{C}}]$. The line must be a comma-separated list enclosed in square brackets, with no additional text. There are no physical units. Angles are not involved. Do not use a percentage sign; all numbers must be printed as decimal floating-point values.", "solution": "This problem directs us to devise and implement a numerical scheme for computing the bounding constants $S_0$ and $S_1$ required for the one-dimensional ratio-of-uniforms sampling method. These constants are defined as $S_0 = \\sup_{x \\in \\mathbb{R}} \\sqrt{g(x)}$ and $S_1 = \\sup_{x \\in \\mathbb{R}} \\lvert x \\rvert \\sqrt{g(x)}$, where $g(x)$ is an unnormalized, strictly positive, integrable target density. We are provided oracle access to the log-density $\\ell(x) = \\log g(x)$ and its derivative $\\ell'(x)$. Key assumptions are that $\\ell(x)$ is concave and twice continuously differentiable, and its derivative $\\ell'(x)$ is $L$-Lipschitz continuous for a known constant $L  0$.\n\nOur derivation and implementation will proceed in three stages: first, we analyze the structure of the optimization problems for $S_0$ and $S_1$ and justify the existence and uniqueness of the relevant maximizers. Second, we develop a robust numerical algorithm based on these properties. Third, we derive non-asymptotic error bounds for this algorithm.\n\n**1. Analysis of the Maximization Problems**\n\nThe computation of $S_0$ and $S_1$ is equivalent to finding the suprema of certain functions derived from $\\ell(x)$.\n\n**1.1. Maximizer for $S_0$**\nThe constant $S_0$ is defined as $S_0 = \\sup_{x \\in \\mathbb{R}} \\sqrt{g(x)}$. This is equivalent to finding the maximum of $g(x)$, or, more conveniently, the maximum of $\\ell(x) = \\log g(x)$, since the logarithm is a strictly increasing function. Let $x_\\star$ be the maximizer of $\\ell(x)$. Then $S_0 = \\sqrt{g(x_\\star)} = \\exp(\\frac{1}{2}\\ell(x_\\star))$.\n\nTo find $x_\\star$, we seek a critical point by solving $\\ell'(x_\\star)=0$.\n*   **Existence and Uniqueness**: The function $\\ell(x)$ is concave, which implies its second derivative satisfies $\\ell''(x) \\le 0$. The additional assumption that $g(x) = \\exp(\\ell(x))$ is integrable on $\\mathbb{R}$ implies that $\\ell(x) \\to -\\infty$ as $\\lvert x \\rvert \\to \\infty$. A continuous function on $\\mathbb{R}$ that tends to $-\\infty$ at its extremities must attain a global maximum. Since $\\ell(x)$ is differentiable, this maximum must occur at a critical point where $\\ell'(x)=0$.\n    To establish uniqueness, consider the derivative $\\ell'(x)$. Its derivative is $\\ell''(x) \\le 0$. Since $g(x)$ must be a proper density (up to normalization) and not a constant function over an infinite interval, $\\ell''(x)$ cannot be zero everywhere. Thus, $\\ell'(x)$ is a strictly decreasing function. A strictly decreasing continuous function can cross the zero-axis at most once. Therefore, the solution $x_\\star$ to $\\ell'(x_\\star)=0$ is unique.\n\n**1.2. Maximizers for $S_1$**\nThe constant $S_1$ is defined as $S_1 = \\sup_{x \\in \\mathbb{R}} \\lvert x \\rvert \\sqrt{g(x)}$. This is equivalent to maximizing the function $h(x) = (\\lvert x \\rvert \\sqrt{g(x)})^2 = x^2 g(x)$. Maximizing $h(x)$ is equivalent to maximizing its logarithm, $r(x) = \\log(x^2 g(x)) = \\log(x^2) + \\log(g(x)) = 2\\log\\lvert x \\rvert + \\ell(x)$. The function $r(x)$ is defined on $\\mathbb{R} \\setminus \\{0\\}$. We analyze it on the intervals $(-\\infty, 0)$ and $(0, \\infty)$ separately.\n\nThe critical points of $r(x)$ are found by solving $r'(x)=0$. The derivative is $r'(x) = \\frac{2}{x} + \\ell'(x)$.\n*   **Existence and Uniqueness of Maximizers**: Let us analyze the function $q(x) = r'(x) = \\ell'(x) + \\frac{2}{x}$. We need to find its roots. The derivative of $q(x)$ is $q'(x) = \\ell''(x) - \\frac{2}{x^2}$. Since $\\ell''(x) \\le 0$, it is immediate that $q'(x)  0$ for all $x \\neq 0$. Thus, $q(x)$ is strictly decreasing on $(-\\infty, 0)$ and on $(0, \\infty)$. This guarantees that $q(x)$ can have at most one root in each of these two intervals.\n\n    Let's verify the existence of these roots.\n    *   On $(0, \\infty)$: As $x \\to 0^+$, $\\frac{2}{x} \\to \\infty$. Since $\\ell'(x)$ is continuous, $\\ell'(x)$ is bounded near $x=0$, so $q(x) \\to \\infty$. As $x \\to \\infty$, we have $\\frac{2}{x} \\to 0$. Since $\\exp(\\ell(x))$ is integrable, $x^2 \\exp(\\ell(x))$ must tend to $0$ as $x \\to \\infty$ for its integral (the variance) to be finite, which is a reasonable physical assumption for a density. This means $r(x) = \\ell(x) + 2\\log x \\to -\\infty$. A continuous function on $(0, \\infty)$ that goes to $-\\infty$ as $x \\to \\infty$ and has $r(x) \\to -\\infty$ as $x \\to 0^{+}$ must attain a maximum. At this maximum $x_+$, we must have $r'(x_+)=0$. Thus, a unique root of $q(x)$ exists in $(0, \\infty)$.\n    *   On $(-\\infty, 0)$: A symmetric argument holds. As $x \\to 0^-$, $\\frac{2}{x} \\to -\\infty$, so $q(x) \\to -\\infty$. As $x \\to -\\infty$, $r(x) = \\ell(x) + 2\\log(-x) \\to -\\infty$. By similar reasoning, a unique maximizer $x_-$ must exist, at which $r'(x_-)=0$.\n\nThe value of $\\sup_x r(x)$ is then $\\max(r(x_+), r(x_-))$, and $S_1 = \\exp(\\frac{1}{2} \\max(r(x_+), r(x_-)))$.\n\n**2. Numerical Scheme: Bracketing and Bisection**\n\nThe uniqueness of the roots of $\\ell'(x)=0$ and $r'(x)=0$ on the respective domains makes them ideal candidates for root-finding via a bracketing method like bisection.\n\n*   **Bracketing**: For a function $f(x)$ with a unique root, we need to find an interval $[a, b]$ such that $f(a)$ and $f(b)$ have opposite signs.\n    *   For $\\ell'(x)$: We check $\\ell'(0)$. If $\\ell'(0)  0$, the root $x_\\star$ is positive. We set $a=0$ and find $b0$ with $\\ell'(b)0$. If $\\ell'(0)  0$, $x_\\star$ is negative. We set $b=0$ and find $a0$ with $\\ell'(a)0$. An expanding search (e.g., trying points $1, 2, 4, ...$ or $-1, -2, -4, ...$) is guaranteed to find such a point because $\\ell(x) \\to -\\infty$ as $\\lvert x \\rvert \\to \\infty$.\n    *   For $r'(x)$ on $(0, \\infty)$: We know $r'(x) \\to \\infty$ as $x \\to 0^+$ and $r'(x)$ eventually becomes negative. We can start with a small $a  0$ (e.g., $10^{-8}$) to ensure $r'(a)  0$, and use an expanding search for $b$ where $r'(b)  0$.\n    *   For $r'(x)$ on $(-\\infty, 0)$: We know $r'(x) \\to -\\infty$ as $x \\to 0^-$ and $r'(x)$ is eventually positive. We start with a small negative $b  0$ (e.g., $-10^{-8}$) to ensure $r'(b)  0$, and use an expanding search for $a$ where $r'(a)  0$.\n\n*   **Bisection**: Once a bracket $[a,b]$ is established for a root of a function $f$, the bisection algorithm iteratively halves the interval while keeping the root bracketed. After $k$ iterations, the width of the bracket is $(b-a)/2^k$. We continue until the bracket width is less than a prescribed tolerance $\\delta_x$. The midpoint of the final bracket is taken as the estimate for the root.\n\n**3. Error Analysis**\n\nWe now derive bounds on the function values based on the tolerance $\\delta_x$ of our bisection root-finder.\n\n*   **Error in $\\ell(x)$**: Let $x_\\star$ be the true maximizer and $\\hat{x}$ be our numerical estimate from bisection on $\\ell'(x)=0$. The final bracket of width $\\delta_x$ contains $x_\\star$. If we take $\\hat{x}$ as the midpoint, then $\\lvert \\hat{x} - x_\\star \\rvert \\le \\frac{\\delta_x}{2}$. By Taylor's theorem, for some $\\xi$ between $\\hat{x}$ and $x_\\star$:\n    $\\ell(\\hat{x}) = \\ell(x_\\star) + \\ell'(x_\\star)(\\hat{x} - x_\\star) + \\frac{1}{2}\\ell''(\\xi)(\\hat{x} - x_\\star)^2$.\n    Since $\\ell'(x_\\star)=0$, the error in the objective function is $\\ell(x_\\star) - \\ell(\\hat{x}) = -\\frac{1}{2}\\ell''(\\xi)(\\hat{x} - x_\\star)^2$.\n    The problem states $\\ell'(x)$ is $L$-Lipschitz, and $\\ell(x)$ is concave. This implies $-L \\le \\ell''(x) \\le 0$ for all $x$. Thus, $0 \\le -\\ell''(\\xi) \\le L$. The error is bounded by:\n    $$0 \\le \\ell(x_\\star) - \\ell(\\hat{x}) \\le \\frac{L}{2}(\\hat{x} - x_\\star)^2 \\le \\frac{L}{2} \\left(\\frac{\\delta_x}{2}\\right)^2 = \\frac{L \\delta_x^2}{8}$$\n    The multiplicative error for the estimate $\\hat{S}_0 = \\exp(\\frac{1}{2}\\ell(\\hat{x}))$ is $\\frac{S_0}{\\hat{S}_0} = \\exp(\\frac{1}{2}(\\ell(x_\\star) - \\ell(\\hat{x})))$, which is bounded by:\n    $$1 \\le \\frac{S_0}{\\hat{S}_0} \\le \\exp\\left(\\frac{L \\delta_x^2}{16}\\right)$$\n\n*   **Error in $r(x)$**: Let $x_r$ be a true maximizer of $r(x)$ (i.e., $x_+$ or $x_-$) and $\\hat{x}_r$ be its numerical estimate. The error is $r(x_r) - r(\\hat{x}_r) = -\\frac{1}{2}r''(\\xi)(\\hat{x}_r - x_r)^2$.\n    The second derivative is $r''(x) = \\ell''(x) - \\frac{2}{x^2}$. Since $-L \\le \\ell''(x) \\le 0$, we have $-L - \\frac{2}{x^2} \\le r''(x) \\le -\\frac{2}{x^2}  0$.\n    The magnitude is $\\lvert r''(x) \\rvert = -\\ell''(x) + \\frac{2}{x^2} \\le L + \\frac{2}{x^2}$.\n    Let the final bisection bracket for $x_r$ be $[u, v]$, which does not contain $0$. Let $a = \\min_{x \\in [u,v]} \\lvert x \\rvert  0$. For any $\\xi$ in this bracket, we have $\\lvert r''(\\xi) \\rvert \\le L + \\frac{2}{a^2}$. The error is thus bounded by:\n    $$0 \\le r(x_r) - r(\\hat{x}_r) \\le \\frac{1}{2}\\left(L + \\frac{2}{a^2}\\right)(\\hat{x}_r - x_r)^2 \\le \\frac{(L + 2/a^2)\\delta_x^2}{8}$$\n    The multiplicative error for $\\hat{S}_1 = \\exp(\\frac{1}{2}\\sup r(\\hat{x}_r))$ is bounded by a similar expression, using the maximum of the error bounds for the positive and negative branches. For a tolerance of $\\delta_x = 10^{-12}$, these errors are negligible for practical purposes, confirming the high accuracy of the scheme.\n\nThis completes the derivation and justification of the numerical method. The following implementation codifies this principled approach.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to compute S0 and S1 for the given test cases.\n    \"\"\"\n\n    def find_bracket(f, domain):\n        \"\"\"\n        Finds a bracketing interval [a, b] for a root of f.\n        \n        Args:\n            f: The function for which to find a root.\n            domain: A string, either 'R' (for ell'), 'pos' (for r' on (0, inf)),\n                    or 'neg' (for r' on (-inf, 0)).\n        \n        Returns:\n            A tuple (a, b) such that f(a) * f(b) = 0.\n        \"\"\"\n        if domain == 'R':\n            # For ell_prime'(x)\n            f0 = f(0.0)\n            if np.isclose(f0, 0.0):\n                return 0.0, 0.0\n            if f0  0:\n                # Root is positive\n                a, b = 0.0, 1.0\n                while f(b)  0:\n                    b *= 2.0\n                return a, b\n            else:\n                # Root is negative\n                a, b = -1.0, 0.0\n                while f(a)  0:\n                    a *= 2.0\n                return a, b\n        elif domain == 'pos':\n            # For r_prime(x) on (0, inf), f(x) - +inf as x - 0+\n            a, b = 1e-8, 1.0\n            if f(a)  0: # a is already on the other side of the root\n                return 0, a\n            while f(b)  0:\n                b *= 2.0\n            return a, b\n        elif domain == 'neg':\n            # For r_prime(x) on (-inf, 0), f(x) - -inf as x - 0-\n            a, b = -1.0, -1e-8\n            if f(b)  0: # b is already on the other side of the root\n                return b, 0\n            while f(a)  0:\n                a *= 2.0\n            return a, b\n\n    def bisection(f, a, b, tol=1e-12):\n        \"\"\"\n        Simple bisection algorithm to find a root of f in [a, b].\n        \"\"\"\n        if a  b:\n            a, b = b, a\n        fa, fb = f(a), f(b)\n        if fa * fb  0:\n            raise ValueError(\"Root not bracketed or multiple roots exist.\")\n        if np.isclose(fa, 0.0):\n            return a\n        if np.isclose(fb, 0.0):\n            return b\n\n        while (b - a)  tol:\n            c = a + (b - a) / 2.0\n            fc = f(c)\n            if np.isclose(fc, 0.0):\n                return c\n            if fa * fc  0:\n                b = c\n            else:\n                a = c\n                fa = fc\n        return a + (b - a) / 2.0\n\n    def compute_bounds(ell, ell_prime, L):\n        \"\"\"\n        Computes S0 and S1 for a given log-density oracle.\n        \"\"\"\n        tol = 1e-12\n\n        # --- Compute S0 ---\n        # Find maximizer of ell(x) by finding root of ell_prime(x)\n        f0 = ell_prime\n        bracket_x_star = find_bracket(f0, 'R')\n        x_star = bisection(f0, bracket_x_star[0], bracket_x_star[1], tol)\n        ell_max = ell(x_star)\n        s0 = np.exp(0.5 * ell_max)\n\n        # --- Compute S1 ---\n        # Find maximizers of r(x) = ell(x) + 2*log|x|\n        # This is done by finding roots of r'(x) = ell_prime(x) + 2/x\n        f1 = lambda x: ell_prime(x) + 2.0 / x\n\n        # Positive maximizer x_+\n        bracket_x_plus = find_bracket(f1, 'pos')\n        x_plus = bisection(f1, bracket_x_plus[0], bracket_x_plus[1], tol)\n        r_plus = ell(x_plus) + 2.0 * np.log(x_plus)\n        \n        # Negative maximizer x_-\n        bracket_x_minus = find_bracket(f1, 'neg')\n        x_minus = bisection(f1, bracket_x_minus[0], bracket_x_minus[1], tol)\n        r_minus = ell(x_minus) + 2.0 * np.log(np.abs(x_minus))\n\n        r_max = max(r_plus, r_minus)\n        s1 = np.exp(0.5 * r_max)\n\n        return s0, s1\n\n    # Test cases\n    # Case A: Standard Normal\n    ell_A = lambda x: -0.5 * x**2\n    ell_prime_A = lambda x: -x\n    L_A = 1.0\n\n    # Case B: Logistic\n    ell_B = lambda x: -x - 2.0 * np.log(1.0 + np.exp(-x))\n    ell_prime_B = lambda x: -1.0 + 2.0 / (1.0 + np.exp(x))\n    L_B = 0.5\n    \n    # Case C: Normal(mu=1.5, sigma=0.5)\n    mu_C, sigma_C = 1.5, 0.5\n    ell_C = lambda x: -((x - mu_C)**2) / (2.0 * sigma_C**2)\n    ell_prime_C = lambda x: -(x - mu_C) / (sigma_C**2)\n    L_C = 1.0 / (sigma_C**2)\n    \n    test_cases = [\n        (ell_A, ell_prime_A, L_A),\n        (ell_B, ell_prime_B, L_B),\n        (ell_C, ell_prime_C, L_C),\n    ]\n\n    results = []\n    for ell, ell_prime, L in test_cases:\n        s0, s1 = compute_bounds(ell, ell_prime, L)\n        results.extend([s0, s1])\n    \n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3356655"}, {"introduction": "Sampling methods are often most powerful when combined into hybrid algorithms that leverage the strengths of each component. In this final practice, you will construct a composite sampler that uses the efficient inverse-CDF method for the center of a distribution and the robust ratio-of-uniforms method for its tails [@problem_id:3356643]. The core challenge involves optimizing the trade-off between these two approaches to create a sampler with the lowest overall computational cost, a key task in practical algorithm design.", "problem": "Consider the target distribution with probability density function $f(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)$ on $\\mathbb{R}$ (the standard normal distribution). The ratio-of-uniforms method constructs a set $A$ in the plane defined by\n$$\nA = \\left\\{(u,v)\\in\\mathbb{R}^2: u gt; 0,\\; u \\le \\sqrt{f\\!\\left(\\frac{v}{u}\\right)}\\right\\}.\n$$\nA draw $(u,v)$ is sampled uniformly from a bounding rectangle that contains $A$, and a value $x$ is accepted if $(u,v)\\in A$, with the output $x=v/u$. Consider a composite sampler that uses inverse-cumulative distribution function (inverse-CDF) sampling in the central region $\\{|x|\\le r\\}$ and ratio-of-uniforms sampling in the tails $\\{|x|gt;r\\}$. The inverse-CDF sampling of the central region uses the conditional distribution truncated to $\\{-r\\le x\\le r\\}$, and the ratio-of-uniforms sampling is restricted to $\\{|x|gt;r\\}$.\n\nAssume the following cost model:\n- Each inverse-CDF sample from the truncated center has a constant cost $c_{\\mathrm{ppf}} gt; 0$.\n- Each proposal in the ratio-of-uniforms scheme has cost $c_{\\mathrm{prop}} gt; 0$, and the expected number of proposals needed for one accepted tail sample equals the reciprocal of the acceptance rate for the tail region.\n\nLet $A_r$ denote the ratio-of-uniforms acceptance set restricted to the tails,\n$$\nA_r = \\left\\{(u,v)\\in A: \\left|\\frac{v}{u}\\right| gt; r\\right\\}.\n$$\nLet $a(r)$ denote the acceptance rate for the tail region, equal to the proportion of the bounding rectangleâ€™s area that lies in $A_r$. Let $p_{\\mathrm{center}}(r)=\\mathbb{P}(|X|\\le r)$ and $p_{\\mathrm{tail}}(r)=1-p_{\\mathrm{center}}(r)$, where $X\\sim \\mathcal{N}(0,1)$ is the target. The expected total cost per composite sample is\n$$\nC(r) = p_{\\mathrm{center}}(r)\\,c_{\\mathrm{ppf}} \\;+\\; p_{\\mathrm{tail}}(r)\\,\\frac{c_{\\mathrm{prop}}}{a(r)}.\n$$\n\nTasks:\n1. Derive the bounding rectangle for $A$ by expressing its $u$-extent and $v$-extent in terms of $f$. Then implement a numerical procedure that approximates $a(r)$ by computing the area fraction of $A_r$ within this rectangle via a uniform grid over $(u,v)$.\n2. Implement the composite sampler:\n   - Center sampling: draw from the truncated central distribution $\\{-r\\le x\\le r\\}$ using inverse-CDF (quantile) transformation of the standard normal cumulative distribution function.\n   - Tail sampling: draw $(u,v)$ uniformly from the bounding rectangle and accept if $(u,v)\\in A$ and $\\left|\\frac{v}{u}\\right|gt;r$, returning $x=v/u$.\n   - Mixture selection: choose center versus tail sampling with probabilities $p_{\\mathrm{center}}(r)$ and $p_{\\mathrm{tail}}(r)$, respectively, to reproduce the target distribution exactly.\n3. Optimize $r$ by minimizing $C(r)$ over a specified interval $[0,R]$ using the acceptance rates $a(r)$ computed from Task 1. Use a deterministic search procedure and report the optimizer $r^\\star$.\n\nUse only the mathematical definitions above, without any shortcut formulas. In particular, do not assume precomputed constants beyond those deducible from $f$. Your program should perform the numerical approximation of $a(r)$ using a uniform grid over the bounding rectangle for $A$ and use the standard normal cumulative distribution function to compute $p_{\\mathrm{center}}(r)$.\n\nTest Suite:\nEvaluate $r^\\star$ for the following parameter sets; for each test case, use a uniform $(u,v)$ grid of the prescribed resolution to compute $a(r)$, and search over $r\\in[0,R]$ with a deterministic grid search over $N_r$ equally spaced points.\n\n- Test case 1: $(c_{\\mathrm{prop}}, c_{\\mathrm{ppf}}, M_u, M_v, R, N_r) = (1.0,\\,1.0,\\,600,\\,1200,\\,3.5,\\,701)$.\n- Test case 2: $(c_{\\mathrm{prop}}, c_{\\mathrm{ppf}}, M_u, M_v, R, N_r) = (1.0,\\,6.0,\\,600,\\,1200,\\,3.5,\\,701)$.\n- Test case 3: $(c_{\\mathrm{prop}}, c_{\\mathrm{ppf}}, M_u, M_v, R, N_r) = (0.2,\\,3.0,\\,600,\\,1200,\\,3.5,\\,701)$.\n- Test case 4: $(c_{\\mathrm{prop}}, c_{\\mathrm{ppf}}, M_u, M_v, R, N_r) = (3.0,\\,0.5,\\,600,\\,1200,\\,3.5,\\,701)$.\n\nHere, $M_u$ is the number of grid points for the $u$-axis, $M_v$ for the $v$-axis, $R$ is the maximum radius for the search interval, and $N_r$ is the number of search points for $r$. For numerical stability in computing $\\frac{v}{u}$, avoid $u=0$ exactly by beginning the $u$-grid at a small positive value.\n\nFinal Output Format:\nYour program should produce a single line of output containing the optimal radii for all test cases as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places (e.g., \"[r1,r2,r3,r4]\") and in the same order as the test cases.", "solution": "The solution involves three main parts as outlined in the problem: deriving the bounding box for the ratio-of-uniforms method, constructing the cost function $C(r)$, and numerically optimizing $r$ through a grid search.\n\n**1. Bounding Rectangle for Set A**\n\nThe set $A$ is defined by $u  0$ and $u \\le \\sqrt{f(v/u)}$. Let $x = v/u$. The condition is $u \\le \\sqrt{f(x)}$.\n\n- **u-extent:** To find the maximum value of $u$, we must maximize its upper bound, $\\sqrt{f(x)}$. The function $f(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}$ has its maximum at $x=0$, where $f(0) = 1/\\sqrt{2\\pi}$. Therefore, the maximum value of $u$ is:\n$$u_{\\max} = \\sqrt{f(0)} = \\sqrt{\\frac{1}{\\sqrt{2\\pi}}} = (2\\pi)^{-1/4}$$\nThe range for $u$ is $(0, u_{\\max}]$.\n\n- **v-extent:** The variable $v$ is related to $u$ and $x$ by $v=ux$. At the boundary of the set $A$, we have $u = \\sqrt{f(x)}$. Substituting this into the expression for $v$ gives the profile of the boundary in the $(x,v)$ plane:\n$$v(x) = x \\sqrt{f(x)} = x \\left(\\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}\\right)^{1/2} = x (2\\pi)^{-1/4} e^{-x^2/4}$$\nTo find the extrema of $v$, we differentiate $v(x)$ with respect to $x$ and set the derivative to zero. It is simpler to find the extrema of $g(x) = x e^{-x^2/4}$.\n$$g'(x) = \\frac{d}{dx}\\left(x e^{-x^2/4}\\right) = 1 \\cdot e^{-x^2/4} + x \\cdot \\left(-\\frac{2x}{4}\\right)e^{-x^2/4} = \\left(1 - \\frac{x^2}{2}\\right)e^{-x^2/4}$$\nSetting $g'(x)=0$ gives $1 - x^2/2 = 0$, which implies $x^2=2$, or $x = \\pm\\sqrt{2}$. These are the locations of the extrema.\nThe extreme values are:\n$$v_{\\text{extrema}} = (\\pm\\sqrt{2}) (2\\pi)^{-1/4} e^{-(\\pm\\sqrt{2})^2/4} = \\pm\\sqrt{2} (2\\pi)^{-1/4} e^{-1/2} = \\pm\\sqrt{\\frac{2}{e}}(2\\pi)^{-1/4}$$\nThus, the range for $v$ is $[-v_{\\max}, v_{\\max}]$, where $v_{\\max} = \\sqrt{2/e} \\cdot u_{\\max}$.\nThe minimal bounding rectangle for the set $A$ is $[0, u_{\\max}] \\times [-v_{\\max}, v_{\\max}]$.\n\n**2. Numerical Approximation of Acceptance Rate a(r)**\n\nThe acceptance rate $a(r)$ for the tail region is the ratio of the area of $A_r$ to the area of the bounding rectangle. We approximate this by discretizing the bounding rectangle into a uniform grid of $M_u \\times M_v$ points and counting the fraction of points that fall into $A_r$.\nA grid point $(u_i,v_j)$ is in $A_r$ if it satisfies two conditions:\n1. $u_i \\le \\sqrt{f(v_j/u_i)}$, which is numerically more stable to check as $u_i^2 \\le f(v_j/u_i)$.\n2. $|v_j/u_i|  r$.\n\nLet $N_{\\text{total}} = M_u M_v$ be the total number of grid points and $N_{A_r}$ be the number of grid points satisfying both conditions. The acceptance rate is then approximated as:\n$$a(r) \\approx \\frac{N_{A_r}}{N_{\\text{total}}}$$\n\n**3. Cost Function and Optimization**\n\nThe total expected cost per sample is given by:\n$$C(r) = p_{\\mathrm{center}}(r)\\,c_{\\mathrm{ppf}} \\;+\\; p_{\\mathrm{tail}}(r)\\,\\frac{c_{\\mathrm{prop}}}{a(r)}$$\nThe probabilities are calculated using the standard normal cumulative distribution function (CDF), $\\Phi(\\cdot)$:\n- $p_{\\mathrm{center}}(r) = \\mathbb{P}(|X|\\le r) = \\Phi(r) - \\Phi(-r) = 2\\Phi(r) - 1$.\n- $p_{\\mathrm{tail}}(r) = 1 - p_{\\mathrm{center}}(r) = 2(1 - \\Phi(r))$.\n\nTo find the optimal radius $r^\\star$, we perform a deterministic grid search. We evaluate $C(r)$ for $N_r$ equally spaced values of $r$ in the interval $[0, R]$. The value of $r$ that yields the minimum cost is our optimal radius, $r^\\star$. For computational efficiency, we can first generate the full $(u, v)$ grid, compute the corresponding $x=v/u$ values, and create a boolean mask indicating which points are in the base set $A$. Then, for each $r$ in our search, we can efficiently find $N_{A_r}$ by combining this mask with the tail condition $|x|r$.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef find_optimal_r(c_prop, c_ppf, M_u, M_v, R, N_r):\n    \"\"\"\n    Finds the optimal radius r for the composite sampler by minimizing the cost function.\n\n    Args:\n        c_prop (float): Cost of one ratio-of-uniforms proposal.\n        c_ppf (float): Cost of one inverse-CDF sample from the center.\n        M_u (int): Number of grid points for the u-axis.\n        M_v (int): Number of grid points for the v-axis.\n        R (float): Maximum radius for the r search interval.\n        N_r (int): Number of search points for r.\n\n    Returns:\n        float: The optimal radius r that minimizes the cost.\n    \"\"\"\n    \n    # 1. Define the bounding rectangle for the ratio-of-uniforms set A.\n    # The target PDF is f(x) = 1/sqrt(2*pi) * exp(-x^2/2).\n    # u_max = sqrt(f(0)) = (2*pi)^(-1/4)\n    # v_max = sqrt(2/e) * u_max\n    u_max = (2 * np.pi)**(-0.25)\n    v_max = u_max * np.sqrt(2 / np.e)\n\n    # 2. Create a uniform grid over the bounding rectangle.\n    # Grid points are at the center of each cell to avoid u=0.\n    u_step = u_max / M_u\n    u_vals = np.linspace(u_step / 2.0, u_max - u_step / 2.0, M_u)\n    \n    v_step = 2 * v_max / M_v\n    v_vals = np.linspace(-v_max + v_step / 2.0, v_max - v_step / 2.0, M_v)\n    \n    # Use 'ij' indexing so that u_grid.shape == (M_u, M_v)\n    u_grid, v_grid = np.meshgrid(u_vals, v_vals, indexing='ij')\n\n    # 3. Pre-compute values on the grid that are independent of r.\n    x_grid = v_grid / u_grid\n    \n    # Check the condition u^2 = f(v/u)\n    f_of_x_over_u = (1 / np.sqrt(2 * np.pi)) * np.exp(-x_grid**2 / 2.0)\n    in_A_mask = u_grid**2 = f_of_x_over_u\n    \n    N_total = M_u * M_v\n\n    # 4. Perform grid search for the optimal r.\n    r_vals = np.linspace(0, R, N_r)\n    costs = np.zeros_like(r_vals)\n\n    for i, r in enumerate(r_vals):\n        # Calculate probabilities for center and tail regions.\n        # p_center(r) = P(|X| = r) = Phi(r) - Phi(-r) = 2*Phi(r) - 1\n        p_center = 2 * norm.cdf(r) - 1\n        p_tail = 1 - p_center\n        \n        # Calculate the acceptance rate a(r) for the tail region.\n        # A point is in A_r if it's in A and |x|  r.\n        in_tail_mask = np.abs(x_grid)  r\n        N_Ar = np.sum(in_A_mask  in_tail_mask)\n        a_r = N_Ar / N_total\n        \n        # Calculate the total expected cost C(r).\n        if a_r == 0:\n            # If a_r is 0, the cost is infinite unless p_tail is also 0.\n            if p_tail  1e-15:\n                costs[i] = np.inf\n            else:\n                costs[i] = p_center * c_ppf\n        else:\n            costs[i] = p_center * c_ppf + p_tail * (c_prop / a_r)\n            \n    # Find the r that minimizes the cost.\n    optimal_r_index = np.argmin(costs)\n    optimal_r = r_vals[optimal_r_index]\n    \n    return optimal_r\n\ndef solve():\n    \"\"\"\n    Runs the optimization for all test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        # (c_prop, c_ppf, M_u, M_v, R, N_r)\n        (1.0, 1.0, 600, 1200, 3.5, 701),\n        (1.0, 6.0, 600, 1200, 3.5, 701),\n        (0.2, 3.0, 600, 1200, 3.5, 701),\n        (3.0, 0.5, 600, 1200, 3.5, 701),\n    ]\n\n    results = []\n    for params in test_cases:\n        c_prop, c_ppf, M_u, M_v, R, N_r = params\n        r_star = find_optimal_r(c_prop, c_ppf, M_u, M_v, R, N_r)\n        results.append(r_star)\n        \n    # Format the output as specified: a list of comma-separated values,\n    # rounded to six decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3356643"}]}