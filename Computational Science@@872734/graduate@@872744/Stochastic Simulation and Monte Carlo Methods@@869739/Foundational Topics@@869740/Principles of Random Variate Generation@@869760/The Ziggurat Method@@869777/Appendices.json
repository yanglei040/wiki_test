{"hands_on_practices": [{"introduction": "Before we can draw samples using the ziggurat method, we must first build the eponymous structure of stacked rectangles. This foundational exercise [@problem_id:3356972] guides you through the pre-computation phase, where the goal is to partition a probability distribution into layers of equal area. You will implement a constructive algorithm that uses numerical quadrature and root-finding to precisely determine the boundaries of each layer, thereby generating the lookup tables essential for the sampler's operation.", "problem": "Consider a nonnegative, integrable, monotonically non-increasing probability density function $f$ on the nonnegative real line $[0,\\infty)$, with total mass $$S = \\int_{0}^{\\infty} f(x)\\,dx.$$ The Ziggurat method constructs a set of stacked rectangles that support acceptance-rejection sampling for $f$ by partitioning its mass into layers of equal target area. One rigorous way to parameterize such layers is by an equal-area vertical partition: for a given integer $n \\ge 1$, define a sequence of abscissae $$0 = x_0  x_1  \\dots  x_{n-1}  x_n$$ and corresponding ordinates $$y_i = f(x_i)\\quad \\text{for}\\quad i=0,1,\\dots,n,$$ such that the equal-area condition $$\\int_{x_{i-1}}^{x_i} f(x)\\,dx = A \\quad \\text{for all}\\quad i=1,2,\\dots,n$$ holds, where $$A = \\frac{S}{n}.$$ Because $f$ is integrable and non-increasing, the cumulative integral $$F(x) = \\int_{0}^{x} f(u)\\,du$$ is continuous, strictly increasing on intervals where $f0$, and satisfies $\\lim_{x\\to\\infty}F(x)=S$. Therefore, for $i=1,2,\\dots,n-1$, the equation $$F(x_i) - F(x_{i-1}) = A$$ has a unique finite solution $x_i$. For the terminal index $i=n$, the solution is $x_n = +\\infty$ in exact mathematics; in numerical computation one must approximate $x_n$ by a finite surrogate that attains a prescribed tail tolerance.\n\nYour task is to design and implement a constructive algorithm, using numerical root-finding and numerical quadrature of $f$, to precompute the arrays $\\{x_i\\}_{i=0}^{n}$ and $\\{y_i\\}_{i=0}^{n}$ satisfying the above properties. Your algorithm must:\n\n- Compute $S$ by numerical integration of $f$ over $[0,\\infty)$.\n- Set $A = S/n$.\n- Initialize $x_0 = 0$, $y_0 = f(0)$.\n- For $i=1,\\dots,n-1$, solve $$F(x_i) - F(x_{i-1}) - A = 0$$ for $x_i$ by bracketed root-finding, where $$F(x) = \\int_{0}^{x} f(u)\\,du,$$ and then set $y_i = f(x_i)$.\n- For the terminal index $i=n$, numerically approximate $x_n$ by solving $$F(x_n) = S - \\varepsilon$$ for a specified tolerance $\\varepsilon  0$, and set $y_n = f(x_n)$.\n- Verify numerically that the equal-area condition holds to within a small numerical error by computing $$E = \\max_{1\\le i\\le n}\\left|\\int_{x_{i-1}}^{x_i} f(x)\\,dx - A\\right|.$$\n\nThe algorithm must be implemented generically for any $f$ that satisfies the above assumptions, and it must use numerical quadrature for integrals and bracketed root-finding for the inversion tasks.\n\nTest Suite:\nImplement and run your algorithm for the following test cases. In each case, the final quantity to report is the real number $E$ defined above.\n\n- Case 1 (happy path): $f(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\big(-\\frac{x^2}{2}\\big)$ for $x\\ge 0$ (the standard normal density restricted to $[0,\\infty)$), with $n=8$, and tail tolerance $\\varepsilon = 10^{-12}$.\n- Case 2 (smooth exponential): $f(x) = e^{-x}$ for $x\\ge 0$ (the rate-$1$ exponential density), with $n=64$, and tail tolerance $\\varepsilon = 10^{-12}$.\n- Case 3 (boundary condition): $f(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\big(-\\frac{x^2}{2}\\big)$ for $x\\ge 0$, with $n=1$, and tail tolerance $\\varepsilon = 10^{-12}$.\n- Case 4 (large $n$ stress): $f(x) = e^{-x}$ for $x\\ge 0$, with $n=256$, and tail tolerance $\\varepsilon = 10^{-12}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases listed above. Precisely, the printed line must be of the form\n$$[E_1,E_2,E_3,E_4],$$\nwhere $E_k$ is the real number computed for the $k$-th case. No other text should be printed.", "solution": "The problem asks for the construction of a sequence of abscissae $$0 = x_0  x_1  \\dots  x_n$$ and corresponding ordinates $$y_i = f(x_i)$$ for a given non-negative, integrable, and monotonically non-increasing function $$f(x)$$ on $$[0, \\infty)$$. The construction must satisfy an equal-area condition for $$n$$ vertical strips.\n\nThe algorithm proceeds as follows:\n\n**1. Computation of Total Mass and Target Area**\nFirst, the total mass (integral) of the function $$f$$ over its domain $$[0, \\infty)$$ is computed using numerical quadrature:\n$$\nS = \\int_{0}^{\\infty} f(x)\\,dx\n$$\nThe domain is partitioned into $$n$$ segments, each intended to contain an equal portion of the total mass. The target area for each segment is therefore:\n$$\nA = \\frac{S}{n}\n$$\n\n**2. The Cumulative Integral Function**\nThe core of the construction relies on the cumulative integral function, $$F(x)$$, defined as:\n$$\nF(x) = \\int_{0}^{x} f(u)\\,du\n$$\nSince $$f(x) \\ge 0$$, $$F(x)$$ is a monotonically non-decreasing function. Given that $$f$$ is also specified as non-increasing, $$F(x)$$ is a concave function. The strict monotonicity of $$F(x)$$ on intervals where $$f(x)  0$$ guarantees that its inverse is well-defined, which is essential for finding the abscissae $$x_i$$. In this implementation, $$F(x)$$ will be evaluated at any given $$x$$ using numerical quadrature.\n\n**3. Iterative Construction of Abscissae**\nThe abscissae $$x_i$$ are determined sequentially.\n\n- **Initialization**: The sequence starts with the boundary condition at the origin:\n  $$\n  x_0 = 0, \\quad y_0 = f(0)\n  $$\n\n- **Interior Abscissae ($$i = 1, \\dots, n-1$$)**: For each interior index $$i$$, the abscissa $$x_i$$ is defined by the equal-area condition. The cumulative area up to $$x_i$$ should ideally be $$i \\cdot A$$. This leads to the equation:\n  $$\n  F(x_i) = \\int_{0}^{x_i} f(u)\\,du = i \\cdot A\n  $$\n  This equation must be solved for $$x_i$$. We can define a function $$g_i(x) = F(x) - iA$$ and find its root. Since $$F(x)$$ is monotonic, a unique root exists. We employ a bracketed root-finding algorithm, such as Brent's method, which is robust and efficient. To find a root of $$g_i(x) = 0$$, we must supply a bracket $$[a, b]$$ such that $$g_i(a) \\cdot g_i(b)  0$$.\n    - A natural lower bound is $$a = x_{i-1}$$, since the sequence of $$x_i$$ must be strictly increasing. At this point, $$F(x_{i-1}) \\approx (i-1)A$$, which implies $$g_i(x_{i-1}) = F(x_{i-1}) - iA \\approx (i-1)A - iA = -A  0$$.\n    - An upper bound $$b  x_{i-1}$$ can be found using an expanding search until $$g_i(b)  0$$.\n  Once $$x_i$$ is numerically determined, the corresponding ordinate is simply $$y_i = f(x_i)$$.\n\n- **Terminal Abscissa ($$i=n$$)**: In exact mathematics, $$x_n = \\infty$$ would ensure the last segment contains the remaining area $$A$$. For numerical computation, $$x_n$$ is approximated by a finite value that captures almost all the remaining mass in the tail of the distribution, leaving a small, prescribed residual area $$\\varepsilon$$. The condition is:\n  $$\n  \\int_{x_n}^{\\infty} f(x)\\,dx = \\varepsilon\n  $$\n  This is equivalent to finding $$x_n$$ such that $$F(x_n) = S - \\varepsilon$$. We solve for $$x_n$$ by finding the root of the function $$h(x) = F(x) - (S - \\varepsilon)$$. The bracketing and root-finding procedure is analogous to the one used for the interior abscissae, using $$x_{n-1}$$ as the lower bound for the search. Finally, $$y_n = f(x_n)$$.\n\n**4. Numerical Verification**\nThe final step is to verify the accuracy of the construction by computing the maximum absolute error $$E$$. For each segment $$i \\in \\{1, \\dots, n\\}$$, the actual area under the curve is computed via numerical quadrature:\n$$\nA_i = \\int_{x_{i-1}}^{x_i} f(x)\\,dx\n$$\nThe error for each segment is $$|A_i - A|$$. The overall error metric $$E$$ is the maximum of these individual errors:\n$$\nE = \\max_{1 \\le i \\le n} |A_i - A|\n$$\nDue to the definition of $$x_n$$, the area of the last segment $$A_n = F(x_n) - F(x_{n-1})$$ will be approximately $$(S-\\varepsilon) - (n-1)A = (nA-\\varepsilon) - (n-1)A = A-\\varepsilon$$. Thus, its contribution to the error is $$| (A-\\varepsilon) - A | = \\varepsilon$$. The total error $$E$$ is therefore expected to be at least $$\\varepsilon$$, and will be dominated by $$\\varepsilon$$ if the numerical errors from quadrature and root-finding for the other segments are smaller. To ensure this, the tolerance settings for the numerical integration routines must be more stringent than $$\\varepsilon$$.\n\nThis procedure is implemented using the `scipy` library, specifically `scipy.integrate.quad` for all numerical integrations and `scipy.optimize.brentq` for the bracketed root-finding.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.optimize import brentq\nfrom typing import Callable\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    def compute_ziggurat_params(f: Callable[[float], float], n: int, epsilon: float) -> float:\n        \"\"\"\n        Constructs the Ziggurat partitions and computes the maximum area error.\n\n        Args:\n            f: The probability density function on [0, inf).\n            n: The number of partitions.\n            epsilon: The tail tolerance for the last partition.\n\n        Returns:\n            The maximum error E.\n        \"\"\"\n        # Define high-precision quadrature options to ensure numerical errors\n        # do not dominate the prescribed tolerance epsilon.\n        quad_opts = {'epsabs': 1e-14, 'epsrel': 1e-14}\n        \n        # Step 1: Compute the total mass S.\n        S, _ = quad(f, 0, np.inf, **quad_opts)\n\n        # Step 2: Compute the target area A.\n        A = S / n\n\n        # Initialize arrays for abscissae and ordinates.\n        x = np.zeros(n + 1)\n        y = np.zeros(n + 1)\n\n        # Step 3: Initialize x_0 and y_0.\n        x[0] = 0.0\n        y[0] = f(x[0])\n\n        memoized_F = {}\n        def F(val: float) -> float:\n            \"\"\"Cumulative integral F(x) with memoization for performance.\"\"\"\n            if val in memoized_F:\n                return memoized_F[val]\n            if val == 0:\n                return 0.0\n            res, _ = quad(f, 0, val, **quad_opts)\n            memoized_F[val] = res\n            return res\n\n        # Step 4: Compute interior abscissae x_i for i = 1, ..., n-1.\n        if n > 1:\n            for i in range(1, n):\n                target_F = i * A\n                \n                # Define the function whose root we need to find.\n                g = lambda val: F(val) - target_F\n                \n                # Bracket the root. The lower bound is the previous abscissa.\n                a = x[i-1]\n                \n                # The value at the lower bound should be negative.\n                # g(a) = F(x_{i-1}) - i*A ≈ (i-1)*A - i*A = -A\n                \n                # Perform an expanding search for the upper bound.\n                b = a + 1.0 \n                # In the rare case of starting at a root...\n                if g(a) == 0:\n                    a -= 1e-9 \n                \n                while g(b)  0:\n                    a = b\n                    b *= 1.5\n                \n                # Solve for x_i using Brent's method.\n                x[i] = brentq(g, a, b)\n                y[i] = f(x[i])\n\n        # Step 5: Compute the terminal abscissa x_n.\n        target_F_n = S - epsilon\n        \n        # Define the function for the terminal root-finding problem.\n        h = lambda val: F(val) - target_F_n\n        \n        # Bracket the root for x_n. Lower bound is x_{n-1}.\n        a_n = x[n-1]\n        \n        # Expanding search for the upper bound.\n        b_n = a_n + 1.0\n        # h(a_n) = F(x_{n-1}) - (S - epsilon) ≈ (n-1)*A - S + epsilon = -A + epsilon  0\n        if h(a_n) >= 0: # Handle edge case where n=1 and S-epsilon is very small\n            b_n = a_n\n            a_n -= 1.0\n            while h(a_n) >= 0: a_n -= 1.0\n\n        while h(b_n)  0:\n            a_n = b_n\n            b_n *= 1.5\n\n        # Solve for x_n.\n        x[n] = brentq(h, a_n, b_n)\n        y[n] = f(x[n])\n\n        # Step 6: Verify the equal-area condition numerically.\n        errors = []\n        for i in range(1, n + 1):\n            slice_area, _ = quad(f, x[i-1], x[i], **quad_opts)\n            errors.append(abs(slice_area - A))\n            \n        E = max(errors)\n        return E\n\n    # Test Suite\n    # Case 1: Standard normal density (half-normal)\n    f_normal = lambda x: (1.0 / np.sqrt(2.0 * np.pi)) * np.exp(-x**2 / 2.0)\n    \n    # Case 2: Standard exponential density\n    f_exp = lambda x: np.exp(-x)\n\n    test_cases = [\n        {'f': f_normal, 'n': 8, 'epsilon': 1e-12},\n        {'f': f_exp, 'n': 64, 'epsilon': 1e-12},\n        {'f': f_normal, 'n': 1, 'epsilon': 1e-12},\n        {'f': f_exp, 'n': 256, 'epsilon': 1e-12},\n    ]\n\n    results = []\n    for case in test_cases:\n        E = compute_ziggurat_params(case['f'], case['n'], case['epsilon'])\n        results.append(E)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "3356972"}, {"introduction": "The remarkable speed of the ziggurat method stems from its ability to perform most sampling steps using only fast integer arithmetic, avoiding costly floating-point operations. This practice [@problem_id:3357000] delves into this high-performance core by challenging you to design and implement the mapping from a single uniform random integer to a layer index and intra-layer coordinates. You will use bit-slicing techniques to create a bijection and prove its uniformity, gaining insight into the low-level optimizations that make modern random number generation practical.", "problem": "You are implementing the core integer-only mapping primitive used in the Ziggurat method for transforming uniformly distributed $32$-bit integers into tuples comprising a layer index and intra-rectangle coordinates. Consider the following general requirement. You must design a mapping from a $32$-bit uniformly distributed integer $R \\in \\{0,1,\\dots,2^{32}-1\\}$ to a triple $(i,u,v)$ where $i$ is a layer index and $(u,v)$ are coordinates within the unit square to be later scaled to the width and height of the chosen layer’s rectangle. The mapping must satisfy the following constraints.\n\n1. Bit-slicing design:\n   - Choose nonnegative integers $b,m,n$ such that $b + m + n = 32$.\n   - The mapping must depend only on bit slicing, integer shifts, masking, and comparisons. Specifically, it must not use floating-point multiplications when extracting $i,u,v$ from $R$.\n   - Interpret $(u,v)$ as fixed-point fractions, that is, as rational numbers of the form $u = U / 2^m$ and $v = V / 2^n$ for integers $U \\in \\{0,1,\\dots,2^m-1\\}$ and $V \\in \\{0,1,\\dots,2^n-1\\}$.\n\n2. Layer-count support:\n   - For a number of layers $K = 2^b$, the mapping must be a bijection from $\\{0,1,\\dots,2^{32}-1\\}$ onto $\\{0,1,\\dots,K-1\\} \\times \\{0,1,\\dots,2^m-1\\} \\times \\{0,1,\\dots,2^n-1\\}$ under the fixed-point interpretation described above, and it must induce independence between $i$, $u$, and $v$.\n   - For a number of layers $K \\le 2^b$ that is not a power of two, the mapping must be extended by rejection on the layer index so that $i$ is exactly uniform on $\\{0,1,\\dots,K-1\\}$, while still using only integer operations and preserving independence between the accepted $i$ and the fixed-point coordinates $(u,v)$. The rejection must be based solely on the sliced index bits.\n\n3. Proof obligation:\n   - Starting from the foundational facts that a $32$-bit uniform integer is uniform over its state space and that distinct bit positions are independent Bernoulli with parameter $1/2$, prove that your mapping yields marginal uniformity of $i$ and of $(u,v)$ over their discrete grids, and prove the independence properties in both the $K = 2^b$ case and the rejection case $K \\le 2^b$.\n\n4. Program requirements:\n   - Implement your mapping for two scenarios:\n     (a) $K = 2^b$ with $(b,m,n) = (8,12,12)$.\n     (b) $K = 250 \\le 2^8$ with $(b,m,n) = (8,12,12)$ using rejection on the top $b$ bits only.\n   - For testing, you will use Monte Carlo simulation with a fixed seed. All randomness arises from independent draws of $32$-bit uniform integers.\n\n5. Statistical test suite and outputs:\n   Implement the following tests. Each test must produce a boolean based on a specified statistical decision rule. Use a significance level $\\alpha = 0.01$. When a chi-square test is required, use the standard chi-square goodness-of-fit test or chi-square test of independence without Yates’ continuity correction.\n\n   Test set A (power-of-two case):\n   - Parameters: $K = 2^8$, $b = 8$, $m = 12$, $n = 12$, sample size $N_A = 400000$.\n   - A1: Uniformity of layer index $i$ over $\\{0,1,\\dots,255\\}$ via chi-square goodness-of-fit with equal expected counts. Output boolean $T_{A1}$ indicating whether the null is not rejected at level $\\alpha$.\n   - A2: Uniformity of horizontal fixed-point coordinate $U \\in \\{0,1,\\dots,4095\\}$ via chi-square goodness-of-fit with equal expected counts. Output boolean $T_{A2}$.\n   - A3: Independence between $i$ and a coarsened horizontal coordinate $U_{\\mathrm{coarse}} \\in \\{0,1,\\dots,15\\}$ defined by the top $4$ bits of $U$, via chi-square test of independence on the $256 \\times 16$ contingency table. Output boolean $T_{A3}$.\n\n   Test set B (boundary determinism under bit slicing for power-of-two case):\n   - Parameters: $K = 2^8$, $b = 8$, $m = 12$, $n = 12$.\n   - B1: For $R = 0$, verify that $(i,U,V) = (0,0,0)$. Output boolean $T_{B1}$.\n   - B2: For $R = 2^{32}-1$, verify that $(i,U,V) = (255,4095,4095)$. Output boolean $T_{B2}$.\n\n   Test set C (non-power-of-two case with rejection on index bits):\n   - Parameters: $K = 250$, $b = 8$, $m = 12$, $n = 12$, sample size $N_C = 300000$.\n   - C1: Using only rejection on the index bits, test uniformity of $i$ on $\\{0,1,\\dots,249\\}$ by chi-square goodness-of-fit with equal expected counts. Output boolean $T_{C1}$.\n   - C2: Uniformity of $U \\in \\{0,1,\\dots,4095\\}$ via chi-square goodness-of-fit with equal expected counts. Output boolean $T_{C2}$.\n   - C3: Independence between $i$ and $U_{\\mathrm{coarse}} \\in \\{0,1,\\dots,15\\}$ via chi-square test of independence on the $250 \\times 16$ contingency table. Output boolean $T_{C3}$.\n\n6. Final output format:\n   Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true1,true2,...]\") in the following order:\n   - $[T_{A1}, T_{A2}, T_{A3}, T_{B1}, T_{B2}, T_{C1}, T_{C2}, T_{C3}]$.\n\nAll angles are dimensionless and no physical units are involved. All answers must be booleans printed as either the Python literal true or false. The entire program must run without input, using its own internal test suite and a fixed random seed for reproducibility. Ensure all computations relevant to the mapping use only integer bit slicing, shifts, masking, comparisons, and integer arithmetic. Floating-point operations may be used only for statistical testing and do not affect the mapping design itself.", "solution": "We begin with the foundational facts for discrete uniform random variables on binary representations. Let $R$ be uniformly distributed on $\\{0,1,\\dots,2^{32}-1\\}$. Then:\n\n1. The mapping from $R$ to its bit vector $(B_{31},B_{30},\\dots,B_{0}) \\in \\{0,1\\}^{32}$, with $R = \\sum_{j=0}^{31} B_j 2^j$, is a bijection between $\\{0,1,\\dots,2^{32}-1\\}$ and $\\{0,1\\}^{32}$. Therefore $R$ uniform implies that the bit vector is uniform over $\\{0,1\\}^{32}$.\n\n2. Independence of distinct bit positions: For any subset $S \\subset \\{0,1,\\dots,31\\}$, the bits $(B_j)_{j \\in S}$ are independent and each marginal $B_j$ is Bernoulli with parameter $1/2$. This follows from the uniformity of the bit vector over the Cartesian product $\\{0,1\\}^{32}$.\n\nUsing these facts, we design an integer-only mapping from $R$ to $(i,u,v)$ by bit slicing. Choose nonnegative integers $b,m,n$ such that $b+m+n=32$, and define integer-valued coordinates\n$$\nI(R) \\equiv \\left\\lfloor \\frac{R}{2^{32-b}} \\right\\rfloor, \\quad\nU(R) \\equiv \\left\\lfloor \\frac{R \\bmod 2^{32-b}}{2^n} \\right\\rfloor, \\quad\nV(R) \\equiv R \\bmod 2^n.\n$$\nEquivalently, using shifts and masks,\n$$\nI(R) = R \\gg (32-b), \\quad\nU(R) = (R \\gg n) \\;\\\\; (2^m-1), \\quad\nV(R) = R \\;\\\\; (2^n-1),\n$$\nwhere $\\gg$ is the arithmetic right-shift on unsigned integers and $\\$ is the bitwise AND. The fixed-point coordinates are $u(R) = U(R)/2^m$ and $v(R) = V(R)/2^n$.\n\nCase 1: Power-of-two layer count $K = 2^b$.\nWe claim that the mapping\n$$\n\\Phi: \\{0,1,\\dots,2^{32}-1\\} \\to \\{0,1,\\dots,2^b-1\\} \\times \\{0,1,\\dots,2^m-1\\} \\times \\{0,1,\\dots,2^n-1\\},\n$$\ngiven by $\\Phi(R) = \\big(I(R), U(R), V(R)\\big)$, is a bijection. This follows because bit slicing partitions the $32$ bits into three disjoint blocks of lengths $b,m,n$; every triple $(i,u,v)$ corresponds to exactly one $R$ that has those bits in those positions. Therefore:\n- Uniformity of $I$: Since $\\Phi$ is a bijection and $R$ is uniform on a set of size $2^{32}$, the marginal distribution of $I$ is uniform on $\\{0,\\dots,2^b-1\\}$, since each $i$ appears in exactly $2^{m+n} = 2^{32-b}$ preimages.\n- Uniformity of $U$ and $V$: Similarly, each $u$ and each $v$ appears in exactly $2^{b+n}$ and $2^{b+m}$ preimages, respectively, yielding uniform discrete distributions on their grids.\n- Independence: Because the bit blocks are disjoint and the bit vector is uniform over $\\{0,1\\}^{32}$, the triple $(I,U,V)$ factors as a Cartesian product of independent coordinates. More explicitly, for any $(i,u,v)$,\n$$\n\\mathbb{P}(I=i, U=u, V=v) = \\frac{1}{2^{32}} = \\frac{1}{2^b} \\cdot \\frac{1}{2^m} \\cdot \\frac{1}{2^n} = \\mathbb{P}(I=i)\\, \\mathbb{P}(U=u)\\, \\mathbb{P}(V=v).\n$$\nThus $I$, $U$, and $V$ are mutually independent, and the fixed-point fractions $u = U/2^m$ and $v = V/2^n$ inherit these properties.\n\nCase 2: Non-power-of-two layer count $K \\le 2^b$ with rejection on index bits.\nDefine $I^\\ast(R) = \\left\\lfloor R/2^{32-b} \\right\\rfloor$ as before, but now accept the draw only if $I^\\ast(R) \\in \\{0,1,\\dots,K-1\\}$; otherwise, resample a fresh $R$ independently and repeat. Upon acceptance, define $U(R)$ and $V(R)$ from the same accepted $R$ using the lower $m$ and $n$ bits as above.\n\nWe now show that:\n- The accepted index $I$ is uniform on $\\{0,1,\\dots,K-1\\}$. Indeed, for each $i \\in \\{0,\\dots,K-1\\}$, the acceptance event is $\\{I^\\ast(R) = i\\}$, which has probability $2^{32-b}/2^{32} = 2^{-b}$. Conditional on acceptance, the probability that $I=i$ is $\\frac{2^{-b}}{K \\cdot 2^{-b}} = 1/K$.\n- Independence between $(U,V)$ and $I$: The decision to accept depends only on the top $b$ bits. By independence of disjoint bit blocks, the lower $m+n$ bits are independent of the top $b$ bits. Conditioning on the acceptance event (which is a function only of the top bits) does not alter the distribution of the lower bits; hence $(U,V)$ remain independent of the acceptance indicator and of the realized $I$. In particular, $\\mathbb{P}(U=u, V=v \\mid I=i) = 2^{-(m+n)}$, so $(U,V)$ are uniform on their grids and independent of $I$.\n- Uniformity of $U$ and $V$: Follows directly from the independence argument above.\n\nThus, this rejection scheme preserves the required uniformity and independence, while using only integer shifts, masks, and comparisons. No floating-point multiplication is used in the mapping itself; only at the point of interpreting fixed-point fractions might one divide by $2^m$ or $2^n$, which is not required for the mapping correctness and can be avoided if one retains integer fixed-point coordinates.\n\nStatistical validation design.\nTo empirically validate the mapping, we perform chi-square goodness-of-fit tests for uniformity and chi-square tests of independence at significance $\\alpha = 0.01$. We use:\n- Test set A for $K=2^8$, $(b,m,n)=(8,12,12)$ and sample size $N_A = 400000$, checking uniformity of $I$, uniformity of $U$, and independence between $I$ and a coarsened $U_{\\mathrm{coarse}}$ formed by the top $4$ bits of $U$.\n- Test set B for deterministic boundaries $R=0$ and $R=2^{32}-1$, verifying exact bit-slicing outputs.\n- Test set C for $K=250 \\le 2^8$ with rejection, $(b,m,n)=(8,12,12)$ and sample size $N_C = 300000$, checking uniformity of $I$, uniformity of $U$, and independence between $I$ and $U_{\\mathrm{coarse}}$.\n\nWe fix the pseudorandom number generator seed to ensure reproducibility. The expected outcomes are booleans:\n- $T_{A1}, T_{A2}, T_{A3}$ should be true if the mapping produces distributions consistent with uniformity and independence at level $\\alpha$.\n- $T_{B1}, T_{B2}$ should both be true if boundary bit slicing is implemented correctly.\n- $T_{C1}, T_{C2}, T_{C3}$ should be true for the rejection-based mapping.\n\nAll mapping computations use only integer bit slicing. Floating-point arithmetic appears only inside the statistical testing functions and does not affect the mapping’s uniformity properties. The final program aggregates the eight booleans in the specified order and prints them on a single line as required.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chisquare, chi2_contingency\n\n# Mapping functions using integer-only bit slicing.\n\ndef slice_bits_power_of_two(R_uint32, b, m, n):\n    \"\"\"\n    Vectorized bit-slicing mapping for K = 2^b.\n    Input:\n        R_uint32: np.ndarray of dtype np.uint32\n        b, m, n: nonnegative integers with b+m+n = 32\n    Output:\n        i: np.ndarray of indices in [0, 2^b-1]\n        U: np.ndarray of ints in [0, 2^m-1] (fixed-point horizontal coordinate)\n        V: np.ndarray of ints in [0, 2^n-1] (fixed-point vertical coordinate)\n    \"\"\"\n    assert b + m + n == 32\n    R = R_uint32.astype(np.uint32)\n    if b == 0:\n        i = np.zeros_like(R, dtype=np.uint32)\n    else:\n        i = (R >> np.uint32(32 - b)).astype(np.uint32)\n    if n == 0:\n        V = np.zeros_like(R, dtype=np.uint32)\n        U = R  ((np.uint32(1)  np.uint32(m)) - np.uint32(1))\n    else:\n        V = (R  ((np.uint32(1)  np.uint32(n)) - np.uint32(1))).astype(np.uint32)\n        if m == 0:\n            U = np.zeros_like(R, dtype=np.uint32)\n        else:\n            U = ((R >> np.uint32(n))  ((np.uint32(1)  np.uint32(m)) - np.uint32(1))).astype(np.uint32)\n    return i, U, V\n\ndef slice_bits_with_rejection_K_le_2b(size, K, b, m, n, rng):\n    \"\"\"\n    Vectorized rejection scheme on the index bits only for K = 2^b.\n    Returns exactly 'size' accepted samples of (i, U, V).\n    \"\"\"\n    assert b + m + n == 32\n    assert 1 = K = (1  b)\n    i_out = np.empty(size, dtype=np.uint32)\n    U_out = np.empty(size, dtype=np.uint32)\n    V_out = np.empty(size, dtype=np.uint32)\n    filled = 0\n    # Use batches to reduce overhead; batch size chosen adaptively.\n    # Expected acceptance probability is K / 2^b.\n    acc_prob = K / (1  b)\n    # Choose a batch size that likely yields enough acceptances per batch.\n    base_batch = max(10000, int(1.5 * size / max(acc_prob, 1e-6)))\n    # Cap the batch size to keep memory reasonable\n    base_batch = min(base_batch, 1_000_000)\n\n    while filled  size:\n        batch = min(base_batch, size - filled)\n        R = rng.integers(0, 1  32, size=batch, dtype=np.uint32)\n        i, U, V = slice_bits_power_of_two(R, b, m, n)\n        mask = (i  K)\n        num_acc = int(mask.sum())\n        if num_acc > 0:\n            i_out[filled:filled + num_acc] = i[mask]\n            U_out[filled:filled + num_acc] = U[mask]\n            V_out[filled:filled + num_acc] = V[mask]\n            filled += num_acc\n    return i_out, U_out, V_out\n\n# Statistical tests.\n\ndef test_uniform_counts(values, num_categories, alpha=0.01):\n    \"\"\"\n    Chi-square goodness-of-fit test for uniformity over num_categories.\n    values: np.ndarray of nonnegative integers expected in [0, num_categories-1]\n    Returns True if p-value > alpha, else False.\n    \"\"\"\n    counts = np.bincount(values, minlength=num_categories).astype(np.int64)\n    # Ensure we restrict to exact length\n    counts = counts[:num_categories]\n    expected = np.full(num_categories, counts.sum() / num_categories)\n    stat, p = chisquare(counts, f_exp=expected)\n    return bool(p > alpha)\n\ndef test_independence(x, x_categories, y, y_categories, alpha=0.01):\n    \"\"\"\n    Chi-square test of independence on the contingency table formed by (x,y).\n    Returns True if p-value > alpha else False.\n    \"\"\"\n    # Build contingency table via flat indexing\n    flat = (x.astype(np.int64) * y_categories + y.astype(np.int64))\n    counts = np.bincount(flat, minlength=x_categories * y_categories).astype(np.int64)\n    table = counts.reshape((x_categories, y_categories))\n    chi2, p, dof, expected = chi2_contingency(table, correction=False)\n    return bool(p > alpha)\n\ndef main():\n    rng = np.random.default_rng(20250110)\n\n    # Test set A: K=2^8, (b,m,n)=(8,12,12), N_A=400000\n    b = 8\n    m = 12\n    n = 12\n    K_pow2 = 1  b\n    N_A = 400_000\n    R_A = rng.integers(0, 1  32, size=N_A, dtype=np.uint32)\n    i_A, U_A, V_A = slice_bits_power_of_two(R_A, b, m, n)\n\n    T_A1 = test_uniform_counts(i_A, K_pow2, alpha=0.01)\n    T_A2 = test_uniform_counts(U_A, 1  m, alpha=0.01)\n    # Coarse U: top 4 bits of U\n    U_A_coarse = (U_A >> np.uint32(m - 4)).astype(np.uint32)\n    T_A3 = test_independence(i_A, K_pow2, U_A_coarse, 1  4, alpha=0.01)\n\n    # Test set B: Boundary determinism for power-of-two case\n    R_low = np.uint32(0)\n    i_low, U_low, V_low = slice_bits_power_of_two(np.array([R_low], dtype=np.uint32), b, m, n)\n    T_B1 = bool((i_low[0] == 0) and (U_low[0] == 0) and (V_low[0] == 0))\n\n    R_high = np.uint32((1  32) - 1)\n    i_high, U_high, V_high = slice_bits_power_of_two(np.array([R_high], dtype=np.uint32), b, m, n)\n    T_B2 = bool((i_high[0] == (K_pow2 - 1)) and (U_high[0] == ((1  m) - 1)) and (V_high[0] == ((1  n) - 1)))\n\n    # Test set C: K=250 = 2^8, rejection on index bits only, N_C=300000\n    K_nonpow = 250\n    N_C = 300_000\n    i_C, U_C, V_C = slice_bits_with_rejection_K_le_2b(N_C, K_nonpow, b, m, n, rng)\n\n    T_C1 = test_uniform_counts(i_C, K_nonpow, alpha=0.01)\n    T_C2 = test_uniform_counts(U_C, 1  m, alpha=0.01)\n    U_C_coarse = (U_C >> np.uint32(m - 4)).astype(np.uint32)\n    T_C3 = test_independence(i_C, K_nonpow, U_C_coarse, 1  4, alpha=0.01)\n\n    results = [T_A1, T_A2, T_A3, T_B1, T_B2, T_C1, T_C2, T_C3]\n    print(f\"[{','.join(str(r).lower() for r in results)}]\")\n\nif __name__ == \"__main__\":\n    main()\n```", "id": "3357000"}, {"introduction": "An implemented sampler is useless without strong evidence of its correctness. This final practice [@problem_id:3357047] addresses this crucial validation step by asking you to design a rigorous statistical test suite to detect subtle flaws in a ziggurat-based normal sampler. You will learn to perform power analysis to calculate the necessary sample sizes to identify small biases in moments and tail probabilities, all while controlling the overall error rate, reflecting the statistical discipline required in professional Monte Carlo applications.", "problem": "A developer implements a piecewise-constant and tail-exponential accept–reject sampler based on the ziggurat method to generate independent and identically distributed samples that should follow a standard normal distribution. The sampler is suspected to have the following subtle deviations relative to the ideal standard normal target: a small mean bias of magnitude $\\delta = 5 \\times 10^{-4}$, a small variance inflation of magnitude $\\epsilon = 10^{-3}$ so that the true variance is $\\sigma^{2} = 1 + \\epsilon$, and a tail under-sampling where the probability of exceeding a fixed threshold is reduced by a factor of $1 - \\eta$ with $\\eta = 0.05$ for the event $\\{|X|  t\\}$ at $t = 4$. The samples are independent and the target distribution under the null hypothesis is $\\mathcal{N}(0,1)$. The testing objective is to empirically validate correctness of the sampler by constructing a scientifically rigorous test suite that controls the family-wise error rate at level $\\alpha = 0.01$ across all tests and achieves power at least $0.8$ for each of the three deviations described above. The suite should include tests with complementary sensitivity in the center, scale, global shape, and tails. Assume that planning may conservatively use a Bonferroni per-test level $\\alpha^{\\star} = \\alpha/m$ for $m$ tests, and that normal and chi-square approximations are acceptable for sample size calculations at large $n$.\n\nWhich option most correctly specifies such a test suite and provides order-of-magnitude sample size calculations that meet the power requirement for the given alternatives, together with scientifically justified reasoning for sensitivity and error control?\n\nA. Use only the one-sample Kolmogorov–Smirnov test at level $\\alpha = 0.01$ with $n = 10^{6}$; this single omnibus test has sufficient sensitivity to detect mean, variance, and tail deviations with power exceeding $0.8$ and controls family-wise error by testing once.\n\nB. Use a suite comprising a one-sample $z$-test for the mean at per-test level $\\alpha^{\\star}$, a chi-square test for variance at per-test level $\\alpha^{\\star}$, a binomial proportion test for tail exceedances at threshold $t = 4$ at per-test level $\\alpha^{\\star}$, plus both Kolmogorov–Smirnov and Anderson–Darling goodness-of-fit tests for global shape, with family-wise error controlled via Holm–Bonferroni at $\\alpha = 0.01$. Planning with Bonferroni $\\alpha^{\\star} = \\alpha/5$ yields the following sample sizes to achieve power at least $0.8$ under the stated alternatives: for the mean bias $\\delta = 5 \\times 10^{-4}$, $n \\approx 6.2 \\times 10^{7}$; for the variance inflation $\\epsilon = 10^{-3}$, $n \\approx 3.1 \\times 10^{7}$; for the tail under-sampling $\\eta = 0.05$ at $t = 4$, using $p_{t} = 2 \\Phi(-4) \\approx 6.334 \\times 10^{-5}$, $n \\approx 9.8 \\times 10^{7}$. Selecting $n = 10^{8}$ ensures power at least $0.8$ across all three deviations while controlling family-wise error, with Anderson–Darling providing enhanced tail sensitivity complementary to Kolmogorov–Smirnov.\n\nC. Use a Student’s $t$-test for the mean at level $\\alpha = 0.05$ and an Anderson–Darling test at level $\\alpha = 0.05$; ignore multiple-testing adjustments to preserve power. With $n = 10^{7}$, both tests will surpass $0.8$ power for the specified mean, variance, and tail deviations due to the omnibus sensitivity of Anderson–Darling.\n\nD. Replace global distribution tests with moment tests up to the fourth central moment at level $\\alpha = 0.01$, combined with a variance test; omit tail-specific exceedance tests as redundant once kurtosis is controlled. With $n = 10^{6}$, the combined tests provide $0.8$ power for the given deviations and control family-wise error by virtue of fewer tests.", "solution": "This problem requires the design of a statistical test suite to validate a random number generator. The core task is to perform a power analysis to determine the necessary sample sizes to detect specific, small deviations from a standard normal distribution, while rigorously controlling the overall Type I error rate.\n\n**1. Problem Analysis and Methodology**\n\nThe objective is to create a test suite that achieves a power of at least $0.8$ (i.e., a Type II error rate $\\beta \\le 0.2$) for detecting three alternative hypotheses, while maintaining a family-wise error rate (FWER) of $\\alpha = 0.01$. The alternatives are:\n-   A mean bias: $H_A: \\mu = \\delta = 5 \\times 10^{-4}$\n-   A variance inflation: $H_A: \\sigma^2 = 1 + \\epsilon = 1.001$\n-   A tail probability reduction: $H_A: P(|X| > 4) = p_0(1-\\eta)$, with $\\eta = 0.05$\n\nA robust test suite should employ tests that are most powerful for each specific alternative. For multiple tests, the FWER must be controlled. The problem allows for planning using a Bonferroni correction, where for $m$ tests, the per-test significance level is $\\alpha^\\star = \\alpha/m$.\n\n**2. Sample Size Calculations**\n\nLet's assume a suite of $m=5$ tests as proposed in option B (mean, variance, tail, KS, AD). The per-test significance level for planning is $\\alpha^\\star = 0.01 / 5 = 0.002$. The quantiles for a two-sided test are $z_{1-\\alpha^\\star/2} = z_{0.999} \\approx 3.0902$, and for power $0.8$, $z_{1-\\beta} = z_{0.8} \\approx 0.8416$.\n\n-   **Mean Test (z-test):** To detect a mean shift $\\delta$ against $\\mu_0=0$ with variance $\\sigma_0^2=1$:\n    $$ n_{\\text{mean}} \\approx \\left( \\frac{z_{1-\\alpha^\\star/2} + z_{1-\\beta}}{\\delta} \\right)^2 \\sigma_0^2 = \\left( \\frac{3.0902 + 0.8416}{5 \\times 10^{-4}} \\right)^2 \\approx (7864)^2 \\approx 6.18 \\times 10^7 $$\n\n-   **Variance Test ($\\chi^2$-test):** To detect a variance $\\sigma_A^2=1+\\epsilon$ against $\\sigma_0^2=1$, using a normal approximation for the $\\chi^2$ distribution, for small $\\epsilon$:\n    $$ n_{\\text{var}} \\approx 1 + \\frac{2(z_{1-\\alpha^\\star/2} + z_{1-\\beta})^2}{(\\sigma_A^2/\\sigma_0^2 - 1)^2} = 1 + \\frac{2(3.0902 + 0.8416)^2}{(10^{-3})^2} \\approx 3.09 \\times 10^7 $$\n\n-   **Tail Probability Test (Proportion z-test):** To detect a change in a small proportion from $p_0$ to $p_A$.\n    First, calculate $p_0 = P(|X|>4) = 2\\Phi(-4) \\approx 2 \\times (3.167 \\times 10^{-5}) = 6.334 \\times 10^{-5}$.\n    The alternative is $p_A = p_0(1-\\eta) = p_0(0.95) \\approx 6.017 \\times 10^{-5}$. The difference is $p_0 - p_A = p_0 \\eta \\approx 3.167 \\times 10^{-6}$.\n    The sample size is:\n    $$ n_{\\text{tail}} \\approx \\frac{\\left( z_{1-\\alpha^\\star/2}\\sqrt{p_0} + z_{1-\\beta}\\sqrt{p_A} \\right)^2}{(p_0 - p_A)^2} \\approx \\frac{\\left( 3.0902\\sqrt{6.334\\times 10^{-5}} + 0.8416\\sqrt{6.017\\times 10^{-5}} \\right)^2}{(3.167 \\times 10^{-6})^2} \\approx 9.66 \\times 10^7 $$\n\nThe largest required sample size is driven by the tail test, necessitating $n \\ge 9.7 \\times 10^7$. A choice of $n=10^8$ safely meets the power requirements for all three specific tests.\n\n**3. Evaluation of Options**\n\n-   **A:** Incorrect. The Kolmogorov-Smirnov test is an omnibus test that lacks the statistical power of specific tests for detecting small shifts in moments or tail probabilities. The proposed sample size of $n=10^6$ is orders of magnitude too small to achieve the desired power.\n\n-   **B:** Correct. This option proposes a comprehensive suite of tests: specific, powerful tests for each targeted deviation (z-test for mean, $\\chi^2$-test for variance, proportion test for tails) and general-purpose goodness-of-fit tests (KS and AD) for overall correctness. It correctly identifies a valid method for FWER control (Holm-Bonferroni). Crucially, the sample size calculations are accurate, and the final proposed sample size of $n=10^8$ is scientifically justified by the power analysis.\n\n-   **C:** Incorrect. This option makes two critical errors. It proposes ignoring multiple-testing adjustments, which violates the FWER constraint. The FWER would be close to $10\\%$, not the required $1\\%$. Secondly, it vastly underestimates the required sample size; $n=10^7$ is insufficient to power the tests for the specified effects.\n\n-   **D:** Incorrect. The sample size of $n=10^6$ is grossly inadequate, as shown by the power analysis. Additionally, the reasoning that a kurtosis test makes a specific tail probability test redundant is flawed. Kurtosis is a global property, whereas the specified alternative is a local defect in the far tail; a direct test is more sensitive and appropriate.\n\n**Conclusion**\n\nOption B is the only one that presents a scientifically rigorous, statistically sound, and quantitatively correct approach to solving the problem. It demonstrates a correct understanding of hypothesis testing, power analysis, and multiple testing corrections.", "answer": "$$\\boxed{B}$$", "id": "3357047"}]}