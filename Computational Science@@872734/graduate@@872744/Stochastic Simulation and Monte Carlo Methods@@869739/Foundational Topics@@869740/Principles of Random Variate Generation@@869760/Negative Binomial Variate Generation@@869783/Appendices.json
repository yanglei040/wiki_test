{"hands_on_practices": [{"introduction": "Before we can build or test a random variate generator, we must first understand the essential mathematical structure of the target distribution. This practice focuses on deriving the recurrence ratio for the Negative Binomial probability mass function (PMF), a relationship that connects the probability of any two adjacent outcomes. This ratio is a fundamental building block for many efficient algorithms, as it allows us to navigate the PMF without costly re-computations of complex terms like factorials or Gamma functions. By working through this derivation and using it to find the distribution's mode—its most probable outcome—you will gain a deeper insight into the analytical properties that drive the design and validation of sophisticated samplers. [@problem_id:3323089]", "problem": "In designing a recursive accept–reject generator for the Negative Binomial distribution, you need an explicit ratio of successive probabilities to traverse the mass function efficiently from its maximum outward. Consider a Negative Binomial random variable $X$ defined as the number of failures before the $r$-th success in a sequence of independent Bernoulli trials with success probability $p \\in (0,1)$. Assume $r \\in \\{1,2,3,\\dots\\}$. The probability mass function (PMF) of $X$ is\n$$\n\\mathbb{P}(X=k) \\;=\\; \\binom{k+r-1}{k}\\, p^{r}\\, (1-p)^{k},\\quad k\\in\\{0,1,2,\\dots\\}.\n$$\nStarting strictly from this PMF and basic algebraic manipulations of binomial coefficients, do the following:\n- Derive the recurrence ratio $R(k) = \\dfrac{\\mathbb{P}(X=k+1)}{\\mathbb{P}(X=k)}$ in closed form as a function of $k$, $r$, and $p$.\n- Using only the ratio $R(k)$ and elementary monotonicity reasoning, determine the exact integer mode $m$ (that is, the largest integer at which the PMF attains its global maximum) in closed form as a function of $r$ and $p$ when $r \\ge 1$.\n\nFinally, for the specific parameters $r=7$ and $p=0.37$, compute the mode $m$ as a single integer. No rounding is required.", "solution": "The starting point is the Probability Mass Function (PMF) for a Negative Binomial random variable $X$ with parameters $r \\in \\{1,2,3,\\dots\\}$ and $p \\in (0,1)$:\n$$\n\\mathbb{P}(X=k) = \\binom{k+r-1}{k} p^{r} (1-p)^{k}, \\quad k \\in \\{0, 1, 2, \\dots\\}\n$$\nThe first task is to derive the recurrence ratio $R(k) = \\dfrac{\\mathbb{P}(X=k+1)}{\\mathbb{P}(X=k)}$.\n\nWe write the expression for $\\mathbb{P}(X=k+1)$ by substituting $k+1$ for $k$ in the PMF formula:\n$$\n\\mathbb{P}(X=k+1) = \\binom{(k+1)+r-1}{k+1} p^{r} (1-p)^{k+1} = \\binom{k+r}{k+1} p^{r} (1-p)^{k+1}\n$$\nNow, we form the ratio $R(k)$:\n$$\nR(k) = \\frac{\\mathbb{P}(X=k+1)}{\\mathbb{P}(X=k)} = \\frac{\\binom{k+r}{k+1} p^{r} (1-p)^{k+1}}{\\binom{k+r-1}{k} p^{r} (1-p)^{k}}\n$$\nWe simplify this expression by canceling common terms. The term $p^r$ cancels out. The ratio of the powers of $(1-p)$ is:\n$$\n\\frac{(1-p)^{k+1}}{(1-p)^{k}} = 1-p\n$$\nNext, we simplify the ratio of the binomial coefficients. Using the factorial definition $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$, we have:\n$$\n\\binom{k+r-1}{k} = \\frac{(k+r-1)!}{k!((k+r-1)-k)!} = \\frac{(k+r-1)!}{k!(r-1)!}\n$$\nand\n$$\n\\binom{k+r}{k+1} = \\frac{(k+r)!}{(k+1)!((k+r)-(k+1))!} = \\frac{(k+r)!}{(k+1)!(r-1)!}\n$$\nTheir ratio is:\n$$\n\\frac{\\binom{k+r}{k+1}}{\\binom{k+r-1}{k}} = \\frac{(k+r)!}{(k+1)!(r-1)!} \\cdot \\frac{k!(r-1)!}{(k+r-1)!}\n$$\nWe can simplify the factorials: $\\frac{(k+r)!}{(k+r-1)!} = k+r$ and $\\frac{k!}{(k+1)!} = \\frac{1}{k+1}$. The term $(r-1)!$ cancels.\n$$\n\\frac{\\binom{k+r}{k+1}}{\\binom{k+r-1}{k}} = \\frac{k+r}{k+1}\n$$\nCombining the parts, the recurrence ratio $R(k)$ is:\n$$\nR(k) = \\frac{k+r}{k+1} (1-p)\n$$\nThis completes the first part of the problem.\n\nThe second task is to find the mode $m$ of the distribution. The mode is the value of $k$ that maximizes the PMF, $\\mathbb{P}(X=k)$. A value $m$ is a mode if $\\mathbb{P}(X=m) \\ge \\mathbb{P}(X=m-1)$ for $m \\ge 1$ and $\\mathbb{P}(X=m) \\ge \\mathbb{P}(X=m+1)$. In terms of our ratio $R(k)$, these conditions are equivalent to $R(m-1) \\ge 1$ and $R(m) \\le 1$.\n\nWe analyze the inequality $R(k) \\ge 1$, which indicates where the PMF is non-decreasing:\n$$\n\\frac{k+r}{k+1} (1-p) \\ge 1\n$$\nSince $k \\ge 0$, $k+1$ is positive, so we can multiply both sides by $k+1$ without changing the inequality direction:\n$$\n(k+r)(1-p) \\ge k+1\n$$\n$$\nk(1-p) + r(1-p) \\ge k+1\n$$\n$$\nr(1-p) - 1 \\ge k - k(1-p)\n$$\n$$\nr(1-p) - 1 \\ge k(1 - (1-p))\n$$\n$$\nr(1-p) - 1 \\ge kp\n$$\nSince $p \\in (0,1)$, $p$ is positive, so we can divide by $p$:\n$$\nk \\le \\frac{r(1-p) - 1}{p}\n$$\nLet $C = \\frac{r(1-p) - 1}{p}$. The PMF is non-decreasing for all integers $k \\le C$.\nThe mode $m$ must satisfy $m-1 \\le C$ and $m \\ge C$. This places $m$ in the interval $[C, C+1]$.\nThe length of this interval is $1$, so it contains either one or two integers.\n\nCase 1: $C$ is not an integer. There is a unique integer $m$ in the interval $[C, C+1]$, which must be $m = \\lfloor C \\rfloor + 1$. For this $m$, we have $R(m-1) > 1$ and $R(m)  1$, so it is the unique mode.\n\nCase 2: $C$ is an integer, say $C=I$. The inequality $k \\le I$ means $R(k) > 1$ for $k  I$ and $R(I) = 1$. $R(I)=1$ implies $\\mathbb{P}(X=I+1) = \\mathbb{P}(X=I)$. For $k > I$, we have $R(k)  1$. Thus, the maximum value of the PMF is attained at both $k=I$ and $k=I+1$. The problem asks for the largest integer mode, which is $m=I+1$. This is also given by $\\lfloor I \\rfloor + 1 = I+1$.\n\nTherefore, in all cases, the largest mode is given by $m = \\lfloor C \\rfloor + 1$.\n$$\nm = \\left\\lfloor \\frac{r(1-p) - 1}{p} \\right\\rfloor + 1\n$$\nUsing the property that for any integer $n$, $\\lfloor x+n \\rfloor = \\lfloor x \\rfloor + n$, we can write:\n$$\nm = \\left\\lfloor \\frac{r(1-p) - 1}{p} + 1 \\right\\rfloor = \\left\\lfloor \\frac{r(1-p) - 1 + p}{p} \\right\\rfloor\n$$\n$$\nm = \\left\\lfloor \\frac{r - rp - 1 + p}{p} \\right\\rfloor = \\left\\lfloor \\frac{(r-1) - rp + p}{p} \\right\\rfloor = \\left\\lfloor \\frac{(r-1) - p(r-1)}{p} \\right\\rfloor\n$$\n$$\nm = \\left\\lfloor \\frac{(r-1)(1-p)}{p} \\right\\rfloor\n$$\nThis expression is valid for $r \\ge 1$. If $r=1$, $m=\\lfloor 0 \\rfloor = 0$, which is correct for the geometric distribution. If $(r-1)(1-p)/p  0$, which cannot happen for the given parameter constraints, the argument must be handled with care, but since $(r-1)\\ge0$ and $(1-p)/p  0$, the expression is non-negative.\n\nFinally, we compute the mode $m$ for the specific parameters $r=7$ and $p=0.37$.\nWe have $r-1 = 6$ and $1-p = 1 - 0.37 = 0.63$.\nSubstituting these values into the formula for the mode:\n$$\nm = \\left\\lfloor \\frac{(7-1)(1-0.37)}{0.37} \\right\\rfloor = \\left\\lfloor \\frac{6 \\times 0.63}{0.37} \\right\\rfloor\n$$\n$$\nm = \\left\\lfloor \\frac{3.78}{0.37} \\right\\rfloor\n$$\nTo evaluate the floor, we perform the division:\n$$\n\\frac{3.78}{0.37} = \\frac{378}{37}\n$$\n$$\n\\frac{378}{37} = \\frac{370 + 8}{37} = \\frac{370}{37} + \\frac{8}{37} = 10 + \\frac{8}{37}\n$$\nSince $0  \\frac{8}{37}  1$, the value is approximately $10.216$.\n$$\nm = \\lfloor 10 + \\frac{8}{37} \\rfloor = 10\n$$\nThe mode for the given parameters is $10$.", "answer": "$$\\boxed{10}$$", "id": "3323089"}, {"introduction": "A critical part of deploying any simulation is validation. How can we verify that a generator—perhaps one provided by an external library—is producing samples that truly follow the desired distribution? This practice guides you through building a complete Monte Carlo unit-test harness that uses the theoretical PMF ratio from [@problem_id:3323089] as a ground truth. You will learn to compare empirical estimates from simulated data against this theoretical property using principled statistical inference. By constructing confidence intervals and checking for systematic deviations, you will master a powerful, general-purpose technique for validating the correctness of any stochastic simulator.", "problem": "You are tasked with writing a complete, runnable program that implements an automated unit-test harness for validating a core structural identity of the Negative Binomial distribution through Monte Carlo simulation. The harness must use only fundamental definitions and well-tested foundational facts as its starting point, derive the required targets from those, and apply a principled inferential procedure to detect systematic deviations. Your implementation must not assume or hard-code the identity to be tested; it must compute all necessary theoretical quantities from first principles derived from the definition of the Negative Binomial distribution.\n\nThe Negative Binomial distribution in this problem is parameterized by a positive real shape parameter $r \\in (0,\\infty)$ and a success probability $p \\in (0,1)$, and models the count $K \\in \\{0,1,2,\\dots\\}$ of failures before the $r$-th success in a sequence of independent Bernoulli trials with success probability $p$. Its Probability Mass Function (PMF) is defined for all $k \\in \\{0,1,2,\\dots\\}$ by\n$$\n\\mathbb{P}(K=k) \\;=\\; f(k) \\;=\\; \\binom{k+r-1}{k}\\,p^{\\,r}\\,(1-p)^{\\,k}\n\\;=\\; \\frac{\\Gamma(k+r)}{\\Gamma(r)\\,\\Gamma(k+1)}\\,p^{\\,r}\\,(1-p)^{\\,k},\n$$\nwhere $\\Gamma(\\cdot)$ is the gamma function and $\\binom{k+r-1}{k}$ is defined via gamma functions, which allows non-integer $r$. This identity is your only permitted starting point for all derivations of theoretical quantities about $f(k)$ requested below.\n\nYour program must implement the following components.\n\n- Simulation engine:\n  - Generate independent and identically distributed samples from the Negative Binomial distribution for general real $r0$ and $p \\in (0,1)$. Use only fundamental constructions that are valid for real $r$, such as the Poisson–Gamma mixture representation: if $\\Lambda \\sim \\mathrm{Gamma}(\\text{shape}=r,\\text{scale}=(1-p)/p)$ and, conditional on $\\Lambda$, $X \\mid \\Lambda \\sim \\mathrm{Poisson}(\\Lambda)$, then $X$ has the Negative Binomial distribution with parameters $r$ and $p$. This is a well-tested identity that you may take as a foundation for your simulator.\n  - The random number generator must be seeded deterministically so that the output is exactly reproducible.\n\n- Empirical histogram and adjacent-ratio estimator:\n  - From a simulated sample of size $n$, form the histogram counts $N_k$ for $k \\in \\{0,1,2,\\dots\\}$.\n  - For each $k$ such that $M_k := N_k + N_{k+1} \\ge m_{\\min}$, define the two-cell conditional proportion $q_k := \\frac{f(k+1)}{f(k) + f(k+1)}$ and its empirical estimator $\\hat{q}_k := \\frac{N_{k+1}}{M_k}$.\n  - Use the transformation $g(q) = \\frac{q}{1-q}$ to define the adjacent ratio of the PMF as $R_k := \\frac{f(k+1)}{f(k)} = g(q_k)$ and its empirical counterpart $\\hat{R}_k := g(\\hat{q}_k)$, provided that $\\hat{q}_k \\in [0,1)$ so that $g$ is finite.\n  - The quantities $f(k)$ and $f(k+1)$ needed for $q_k$ must be computed from the PMF definition above, without shortcut identities.\n\n- Confidence Interval (CI) construction and systematic deviation flagging:\n  - For each eligible $k$ with $M_k \\ge m_{\\min}$ and $\\hat{q}_k \\in [0,1)$, build an approximate two-sided level-$1-\\alpha$ Confidence Interval (CI) for $R_k$ based on the Binomial model for $N_{k+1} \\mid M_k \\sim \\mathrm{Binomial}(M_k, q_k)$ and the Delta method linearization of $g(q)=\\frac{q}{1-q}$. Justify the form of the standard error by applying the Delta method to the Binomial proportion.\n  - Let $z_{1-\\alpha/2}$ denote the standard Normal quantile at tail probability $\\alpha/2$. Construct the CI centered at $\\hat{R}_k$ with a half-width equal to $z_{1-\\alpha/2}$ times an appropriate standard error that follows from the Delta method, expressed in terms of $q_k$ and $M_k$. Use $q_k$ (not $\\hat{q}_k$) inside the standard error to avoid conditioning on the data in the variance term.\n  - Declare a violation for index $k$ if the theoretically derived $R_k$ does not lie inside the constructed CI for that $k$. Count the total number of violations for the test case as an integer.\n\n- Aggregation and output:\n  - For each specified test case, compute the integer number of violations across all eligible $k$. If no $k$ is eligible, return the integer $0$ for that case.\n  - Your program must produce a single line of output containing these integers for all provided test cases, in the test case order, as a comma-separated list enclosed in square brackets, for example, $[v_1,v_2,\\dots,v_T]$, where $T$ is the number of test cases and $v_t$ is the violation count for test case $t$.\n\nParameter choices and test suite to implement:\n\n- Use a fixed significance level $\\alpha = 0.01$, which corresponds to a confidence level $1-\\alpha = 0.99$. Do not use a percentage sign anywhere; express all coverage levels as decimals or fractions.\n- Use a minimum pooling threshold $m_{\\min} = 50$ to decide whether index $k$ is eligible for testing.\n- Implement the following $5$ test cases, each specified by $(r,p,n)$:\n  - Case $1$: $(r,p,n) = (5,\\,0.3,\\,200000)$.\n  - Case $2$: $(r,p,n) = (2.5,\\,0.7,\\,150000)$.\n  - Case $3$: $(r,p,n) = (0.7,\\,0.2,\\,300000)$.\n  - Case $4$: $(r,p,n) = (20,\\,0.5,\\,80000)$.\n  - Case $5$: $(r,p,n) = (1.3,\\,0.95,\\,120000)$.\n\nAdditional requirements:\n\n- All mathematical derivations needed to implement the harness must begin from the PMF definition and the Poisson–Gamma mixture identity only. Do not assume or quote any additional target identities without derivation. Provide the resulting expressions in code, not in the output.\n- Angles and physical units are not involved.\n- The final output must be a single line as described above, with no extra text.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[v_1,v_2,v_3,v_4,v_5]$.", "solution": "The problem statement has been analyzed and is determined to be valid. It is a well-posed, scientifically grounded problem in computational statistics, providing all necessary definitions, parameters, and procedures to construct a unique and verifiable solution.\n\nThe task is to build a Monte Carlo-based test harness to validate a structural property of the Negative Binomial distribution. This involves several steps: deriving the theoretical quantities, simulating data from the distribution, performing a statistical analysis on the simulated data, and comparing the empirical results to the theoretical predictions.\n\nThe core of the problem lies in testing the relationship between probabilities of adjacent counts. This relationship is captured by the ratio $R_k = f(k+1)/f(k)$. The test methodology relies on comparing a theoretical derivation of this ratio with an empirical estimate obtained from a large number of simulated samples.\n\nFirst, we derive the theoretical value of the adjacent-count ratio, $R_k$, starting from the provided Probability Mass Function (PMF) of the Negative Binomial distribution, $K \\sim \\mathrm{NB}(r, p)$:\n$$\nf(k) = \\mathbb{P}(K=k) = \\frac{\\Gamma(k+r)}{\\Gamma(r)\\,\\Gamma(k+1)}\\,p^{\\,r}\\,(1-p)^{\\,k}\n$$\nThe ratio $R_k$ is defined as:\n$$\nR_k = \\frac{f(k+1)}{f(k)} = \\frac{\\frac{\\Gamma(k+1+r)}{\\Gamma(r)\\,\\Gamma(k+2)}\\,p^{\\,r}\\,(1-p)^{\\,k+1}}{\\frac{\\Gamma(k+r)}{\\Gamma(r)\\,\\Gamma(k+1)}\\,p^{\\,r}\\,(1-p)^{\\,k}}\n$$\nSimplifying the expression by cancelling common terms ($p^r$, a portion of $(1-p)^k$, and $\\Gamma(r)$):\n$$\nR_k = \\frac{\\Gamma(k+r+1)}{\\Gamma(k+r)} \\cdot \\frac{\\Gamma(k+1)}{\\Gamma(k+2)} \\cdot \\frac{(1-p)^{k+1}}{(1-p)^k}\n$$\nUsing the fundamental property of the Gamma function, $\\Gamma(z+1)=z\\Gamma(z)$, we can simplify the ratios of Gamma functions:\n$$\n\\frac{\\Gamma(k+r+1)}{\\Gamma(k+r)} = k+r\n$$\n$$\n\\frac{\\Gamma(k+1)}{\\Gamma(k+2)} = \\frac{\\Gamma(k+1)}{(k+1)\\Gamma(k+1)} = \\frac{1}{k+1}\n$$\nSubstituting these back into the expression for $R_k$ yields the simple structural identity to be tested:\n$$\nR_k = (k+r) \\cdot \\left(\\frac{1}{k+1}\\right) \\cdot (1-p) = \\frac{k+r}{k+1}(1-p)\n$$\nThis is the theoretical target value for the ratio of adjacent probabilities.\n\nThe statistical test is constructed based on observed cell counts, $N_k$, from a large sample of size $n$. For any pair of adjacent cells $k$ and $k+1$, we consider the total count $M_k = N_k + N_{k+1}$. Conditionally, given the total $M_k$, the number of observations in cell $k+1$, $N_{k+1}$, follows a Binomial distribution:\n$$\nN_{k+1} \\mid M_k \\sim \\mathrm{Binomial}(M_k, q_k)\n$$\nwhere $q_k$ is the conditional probability of an observation falling into cell $k+1$ given that it falls into either cell $k$ or $k+1$:\n$$\nq_k = \\frac{f(k+1)}{f(k) + f(k+1)}\n$$\nBy dividing the numerator and denominator by $f(k)$, we can express $q_k$ in terms of $R_k$:\n$$\nq_k = \\frac{f(k+1)/f(k)}{1 + f(k+1)/f(k)} = \\frac{R_k}{1+R_k}\n$$\nThe natural estimator for $q_k$ is the empirical proportion $\\hat{q}_k = N_{k+1}/M_k$. The problem defines the estimator for $R_k$ via the transformation $g(q) = q/(1-q)$. It is straightforward to see that $g(q_k) = R_k$. Thus, our empirical estimator for $R_k$ is $\\hat{R}_k = g(\\hat{q}_k) = \\hat{q}_k/(1-\\hat{q}_k)$.\n\nTo construct a confidence interval for $R_k$, we apply the Delta method to find the approximate variance of $\\hat{R}_k$. For a large number of trials $M_k$, the variance of the Binomial proportion estimator $\\hat{q}_k$ is $\\mathrm{Var}(\\hat{q}_k) = q_k(1-q_k)/M_k$. The Delta method approximates the variance of the transformed estimator as $\\mathrm{Var}(\\hat{R}_k) \\approx [g'(q_k)]^2 \\mathrm{Var}(\\hat{q}_k)$. We compute the derivative of $g(q)$:\n$$\ng'(q) = \\frac{d}{dq}\\left(\\frac{q}{1-q}\\right) = \\frac{1(1-q) - q(-1)}{(1-q)^2} = \\frac{1}{(1-q)^2}\n$$\nSubstituting this into the variance approximation:\n$$\n\\mathrm{Var}(\\hat{R}_k) \\approx \\left(\\frac{1}{(1-q_k)^2}\\right)^2 \\frac{q_k(1-q_k)}{M_k} = \\frac{q_k}{M_k(1-q_k)^3}\n$$\nThe standard error (SE) is the square root of this variance. As specified, we use the theoretical value $q_k$ in this formula.\n$$\n\\mathrm{SE}(\\hat{R}_k) = \\sqrt{\\frac{q_k}{M_k(1-q_k)^3}}\n$$\nAn approximate $1-\\alpha$ confidence interval for $R_k$ is centered at the empirical estimate $\\hat{R}_k$ with a half-width determined by the standard Normal quantile, $z_{1-\\alpha/2}$:\n$$\n\\mathrm{CI}_k = \\left[ \\hat{R}_k - z_{1-\\alpha/2} \\mathrm{SE}(\\hat{R}_k), \\; \\hat{R}_k + z_{1-\\alpha/2} \\mathrm{SE}(\\hat{R}_k) \\right]\n$$\nA violation is counted for index $k$ if the theoretical ratio $R_k$ falls outside this empirically constructed confidence interval. This occurs if $|\\hat{R}_k - R_k| > z_{1-\\alpha/2} \\mathrm{SE}(\\hat{R}_k)$.\n\nThe overall algorithm proceeds as follows for each test case $(r, p, n)$:\n1. A deterministic random number generator is seeded.\n2. $n$ samples from $\\mathrm{NB}(r,p)$ are generated using the specified Poisson-Gamma mixture method: for each sample, draw $\\Lambda \\sim \\mathrm{Gamma}(r, (1-p)/p)$, then draw the sample value from $\\mathrm{Poisson}(\\Lambda)$.\n3. The samples are aggregated into a frequency histogram, producing counts $N_k$.\n4. A loop iterates through each integer $k$ from $0$ up to the maximum observed value.\n5. For each $k$, the pooled count $M_k = N_k + N_{k+1}$ is computed.\n6. If $M_k \\ge m_{\\min}$ and $\\hat{q}_k = N_{k+1}/M_k  1$, the index $k$ is considered eligible for testing.\n7. For each eligible $k$, the theoretical quantities $R_k$ and $q_k$ are calculated, along with the empirical estimate $\\hat{R}_k$.\n8. The standard error $\\mathrm{SE}(\\hat{R}_k)$ is computed using the theoretical $q_k$ and the observed $M_k$.\n9. The violation condition is checked: $|R_k - \\hat{R}_k|  z_{1-\\alpha/2} \\mathrm{SE}(\\hat{R}_k)$. If true, a violation is recorded.\n10. The total number of violations across all eligible $k$ is summed up and reported for the test case. If no $k$ is eligible, the count is $0$.\nThis procedure is repeated for all specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the Monte Carlo validation harness for all test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (r, p, n)\n        (5.0, 0.3, 200000),\n        (2.5, 0.7, 150000),\n        (0.7, 0.2, 300000),\n        (20.0, 0.5, 80000),\n        (1.3, 0.95, 120000),\n    ]\n\n    # Fixed parameters for the test harness\n    alpha = 0.01\n    m_min = 50\n    # Seed for reproducibility, as requested.\n    seed = 42\n    \n    # Pre-calculate the standard normal quantile\n    z_quantile = norm.ppf(1 - alpha / 2.0)\n    \n    # Initialize the random number generator\n    rng = np.random.default_rng(seed=seed)\n\n    results = []\n    \n    # Process each test case\n    for r, p, n in test_cases:\n        violation_count = run_test_case(r, p, n, m_min, z_quantile, rng)\n        results.append(violation_count)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_test_case(r, p, n, m_min, z_quantile, rng):\n    \"\"\"\n    Runs the simulation and analysis for a single test case.\n    \n    Args:\n        r (float): Shape parameter of the Negative Binomial distribution.\n        p (float): Success probability parameter.\n        n (int): Sample size for the simulation.\n        m_min (int): Minimum pooled count for a cell pair to be eligible.\n        z_quantile (float): The z-score for the confidence interval.\n        rng (numpy.random.Generator): The random number generator instance.\n        \n    Returns:\n        int: The total count of violations for the test case.\n    \"\"\"\n    \n    # --- Step 1: Simulation Engine ---\n    # Generate samples using the Poisson-Gamma mixture representation\n    # Lambda ~ Gamma(shape=r, scale=(1-p)/p)\n    # X | Lambda ~ Poisson(Lambda)\n    if n = 0:\n        return 0\n        \n    gamma_scale = (1.0 - p) / p\n    lambda_samples = rng.gamma(shape=r, scale=gamma_scale, size=n)\n    nb_samples = rng.poisson(lam=lambda_samples, size=n)\n\n    # --- Step 2: Empirical Histogram ---\n    # Form the histogram counts Nk for k = 0, 1, 2, ...\n    if nb_samples.size == 0:\n        max_k_obs = -1\n    else:\n        max_k_obs = np.max(nb_samples)\n    \n    # We need counts up to max_k_obs + 1 for the loop\n    hist_counts = np.bincount(nb_samples, minlength=max_k_obs + 2)\n    \n    violations = 0\n    \n    # Iterate through all possible k values where Nk and N_{k+1} can be non-zero\n    for k in range(max_k_obs + 1):\n        N_k = hist_counts[k]\n        N_k_plus_1 = hist_counts[k + 1]\n        \n        M_k = N_k + N_k_plus_1\n        \n        # --- Step 3: Check Eligibility ---\n        if M_k  m_min:\n            continue\n            \n        # --- Step 4: Empirical and Theoretical Calculations ---\n        # Empirical two-cell conditional proportion\n        hat_q_k = N_k_plus_1 / M_k\n        \n        # According to the problem, the estimator for Rk is only defined for hat_q_k  1.\n        if hat_q_k = 1.0:\n            continue\n            \n        # Empirical adjacent ratio\n        hat_R_k = hat_q_k / (1.0 - hat_q_k)\n        \n        # Theoretical adjacent ratio R_k = (k+r)/(k+1) * (1-p)\n        R_k = (k + r) / (k + 1.0) * (1.0 - p)\n        \n        # Theoretical two-cell conditional proportion q_k = R_k / (1 + R_k)\n        q_k = R_k / (1.0 + R_k)\n\n        # --- Step 5: Confidence Interval and Violation Check ---\n        # The standard error of hat_R_k uses the theoretical q_k as specified.\n        # SE(hat_R_k) = sqrt( q_k / (M_k * (1-q_k)^3) )\n        if q_k = 0.0 or q_k = 1.0:\n            # This case is theoretically unlikely but guards against numerical issues.\n            continue\n        \n        var_hat_R_k_numerator = q_k\n        var_hat_R_k_denominator = M_k * ((1.0 - q_k)**3)\n        \n        if var_hat_R_k_denominator = 0:\n            continue\n            \n        std_err_hat_R_k = np.sqrt(var_hat_R_k_numerator / var_hat_R_k_denominator)\n        \n        # CI half-width\n        half_width = z_quantile * std_err_hat_R_k\n        \n        # Check if the theoretical R_k is outside the CI\n        if np.abs(hat_R_k - R_k)  half_width:\n            violations += 1\n            \n    return violations\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3323022"}, {"introduction": "Having learned to analyze and validate a generator, we now turn to the challenge of implementation. This exercise challenges you to build a sampler that is not only correct but also numerically robust and memory-efficient, skills essential for practical scientific computing. You will implement an inverse transform sampler by leveraging the PMF recurrence from [@problem_id:3323089] in the logarithmic domain, a crucial technique for preventing numerical underflow with small probabilities. This deep dive into implementation details, including the use of stable `logsumexp` updates and the analysis of floating-point error, will equip you to translate theoretical algorithms into production-quality scientific software.", "problem": "Design and implement a constrained-memory random variate generator for the negative binomial distribution using on-the-fly probability mass function ratios in the logarithmic domain, together with a principled bound on cumulative floating-point error. Let the negative binomial distribution with parameters $r0$ and $p \\in (0,1)$ denote the distribution of a nonnegative integer-valued random variable $K \\in \\{0,1,2,\\dots\\}$ with probability mass function\n$$\n\\mathbb{P}(K=k) \\;=\\; \\binom{k+r-1}{k} (1-p)^k p^r \\;=\\; \\frac{\\Gamma(k+r)}{\\Gamma(r)\\,\\Gamma(k+1)} (1-p)^k p^r,\n$$\nwhere $\\Gamma(\\cdot)$ is the Gamma function. The cumulative distribution function satisfies\n$$\nF(k) \\;=\\; \\mathbb{P}(K \\le k) \\;=\\; \\sum_{j=0}^k \\mathbb{P}(K=j) \\;=\\; I_{p}(r, k+1),\n$$\nwhere $I_x(a,b)$ is the regularized incomplete Beta function.\n\nYour generator must:\n- Sample from $NB(r,p)$ by inverse transform sampling using cumulative sums of the probability mass function computed on-the-fly from a recurrence based only on previously computed values and the parameters $(r,p)$.\n- Maintain all running quantities in the logarithmic domain to avoid underflow/overflow, using two-argument $\\log\\text{sum}\\exp$ updates of the form $s_{k+1}=\\log\\left(\\exp(s_k)+\\exp(\\ell_{k+1})\\right)$, where $\\ell_{k}$ denotes the logarithm of the probability mass function at $k$.\n- Use only $\\mathcal{O}(1)$ additional memory per generated variate; for example, do not precompute or store an array of probabilities or cumulative probabilities.\n- Employ vectorized evaluation of the logarithm of the Gamma function (that is, $\\log \\Gamma(\\cdot)$) to validate your on-the-fly recurrence against the closed form for $\\log \\mathbb{P}(K=k)$ across a range of $k$ values.\n\nFloating-point error model and bound requirement:\n- Assume standard floating-point arithmetic with unit roundoff $\\varepsilon_{\\mathrm{mach}}$, modeled by $\\mathrm{fl}(x \\circ y) = (x \\circ y)(1+\\delta)$ with $|\\delta| \\le \\varepsilon_{\\mathrm{mach}}$ for each elementary arithmetic operation $\\circ \\in \\{+,-,\\times,\\div\\}$, and treat evaluations of $\\exp(\\cdot)$, $\\log(\\cdot)$, and $\\log \\Gamma(\\cdot)$ as incurring multiplicative relative errors of the same order. You must derive a computable, explicit bound $B(m)$ on the absolute error in the computed log-cumulative $s_m$ after $m$ two-argument $\\log\\text{sum}\\exp$ updates, of the form\n$$\n\\big| s_m^{\\mathrm{comp}} - \\log F(k_m) \\big| \\;\\le\\; B(m),\n$$\nwhere $k_m$ is the current index and $s_m^{\\mathrm{comp}}$ is the computed $\\log$-cumulative after $m$ updates. Your bound must be expressed in terms of $m$ and $\\varepsilon_{\\mathrm{mach}}$ and must not rely on unknown internal constants; you may choose a conservative universal constant multiplier justified by operation counting.\n\nProgram requirements:\n- Implement an inverse-transform generator for $NB(r,p)$ that adheres to the memory constraint and uses on-the-fly logarithmic probability mass function ratios; derive the ratio yourself from the given definition of the probability mass function without pre-storing any table.\n- Implement a numerically stable two-argument $\\log\\text{sum}\\exp$ update for the cumulative sum in the log domain.\n- Provide a function that, for a given set of parameters $(r,p)$ and a set of predetermined uniform random numbers $(u_i)$, runs the generator until the stopping index $k$ for each $u_i$ and returns, for each $u_i$, the computed log-cumulative $s$, the exact $\\log F(k)$ evaluated via the regularized incomplete Beta function, the number of $\\log\\text{sum}\\exp$ updates $m$, and whether the inequality $\\big| s - \\log F(k) \\big| \\le B(m)$ holds.\n- Provide a vectorized check of the on-the-fly recurrence for $\\log \\mathbb{P}(K=k)$ against the closed form $\\log \\mathbb{P}(K=k)$ computed from $\\log \\Gamma(\\cdot)$ over a contiguous range $k=0,1,\\dots,K^\\star$, and verify that the maximum absolute discrepancy is bounded by a conservative expression of the form $C^\\star \\cdot (K^\\star+1)\\cdot \\varepsilon_{\\mathrm{mach}}$ for an explicit constant $C^\\star$ you must specify.\n\nStatistical validation:\n- For a generated independent and identically distributed sample of size $n$, compute the sample mean and sample variance using a one-pass, numerically stable algorithm that uses $\\mathcal{O}(1)$ memory (for example, Welford's method). Compare the sample mean to the theoretical mean $\\mathbb{E}[K] = r\\frac{1-p}{p}$. Treat the standard error of the sample mean as $\\sqrt{\\mathrm{Var}(K)/n}$, where $\\mathrm{Var}(K) = r\\frac{1-p}{p^2}$. Report whether the absolute deviation of the sample mean from the theoretical mean is at most $4$ standard errors.\n\nTest suite and output specification:\n- Use the following parameter sets and seeds. All random numbers must be generated from a fixed seed to ensure reproducibility.\n    - Test $1$: $r=2.5$, $p=0.3$, $n=10000$, seed $1729$. In addition to the statistical validation, also perform the numerical validation with the uniform grid $\\{u=0.1,\\,0.5,\\,0.9\\}$, and the vectorized $\\log \\Gamma$ validation over $k=0,1,\\dots,K^\\star$ where $K^\\star$ is the largest stopping index encountered among these three $u$ values.\n    - Test $2$: $r=10.0$, $p=0.05$, $n=5000$, seed $314159$.\n    - Test $3$: $r=0.7$, $p=0.95$, $n=20000$, seed $271828$.\n    - Test $4$: numerical validation only with $(r,p)=(50.0,0.2)$ and uniform grid $\\{u=0.01,\\,0.5,\\,0.99\\}$.\n- For each test, produce the following outputs:\n    - Test $1$: a list $[\\hat{\\mu}, \\hat{\\sigma}^2, \\mathrm{mean\\_ok}, \\mathrm{cdf\\_bound\\_ok}, \\mathrm{logpmf\\_bound\\_ok}]$, where $\\hat{\\mu}$ is the sample mean (a float), $\\hat{\\sigma}^2$ is the sample variance (a float), $\\mathrm{mean\\_ok}$ is a boolean indicating whether the sample mean is within $4$ standard errors of the theoretical mean, $\\mathrm{cdf\\_bound\\_ok}$ is a boolean indicating whether all three uniform inputs satisfy the bound $\\big| s - \\log F(k) \\big| \\le B(m)$, and $\\mathrm{logpmf\\_bound\\_ok}$ is a boolean indicating whether the vectorized $\\log \\Gamma$ check satisfies its bound for $k=0,1,\\dots,K^\\star$.\n    - Test $2$: a list $[\\hat{\\mu}, \\hat{\\sigma}^2, \\mathrm{mean\\_ok}]$.\n    - Test $3$: a list $[\\hat{\\mu}, \\hat{\\sigma}^2, \\mathrm{mean\\_ok}]$.\n    - Test $4$: a list $[\\mathrm{cdf\\_bound\\_ok}, \\mathrm{logpmf\\_bound\\_ok}]$ defined analogously to Test $1$ but without statistical outputs.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is one test's list in the order Tests $1$ through $4$ (for example, $[[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$). No other text should be printed.\n\nConstraints and clarifications:\n- All angles, if any arise, must be measured in radians. No physical units are involved here.\n- You must use scalar state and $\\mathcal{O}(1)$ additional memory in the generator; vectorization is only permitted for diagnostic validation steps (such as the $\\log \\Gamma$ check).\n- Your bound $B(m)$ must be explicitly stated in your solution and implemented exactly as stated in your code, with all constants clearly specified.", "solution": "We begin from the definition of the negative binomial distribution with parameters $r0$ and $p \\in (0,1)$, which has probability mass function\n$$\nf(k) \\;=\\; \\mathbb{P}(K=k) \\;=\\; \\frac{\\Gamma(k+r)}{\\Gamma(r)\\,\\Gamma(k+1)} (1-p)^k p^r,\\quad k\\in\\{0,1,2,\\dots\\}.\n$$\nThe cumulative distribution function satisfies\n$$\nF(k) \\;=\\; \\sum_{j=0}^{k} f(j) \\;=\\; I_{p}(r,k+1),\n$$\nwhere $I_x(a,b)$ is the regularized incomplete Beta function. The theoretical mean and variance are well known and follow from standard properties of the negative binomial distribution:\n$$\n\\mathbb{E}[K] \\;=\\; r\\frac{1-p}{p}, \\qquad \\mathrm{Var}(K) \\;=\\; r\\frac{1-p}{p^2}.\n$$\n\nPrinciple for constrained-memory on-the-fly generation. We will implement inverse transform sampling. Draw $U\\sim \\mathrm{Uniform}(0,1)$ and find the smallest $k$ such that $F(k)\\ge U$. To avoid precomputing or storing an array, we compute $f(k)$ successively from $f(0)$ using a ratio derived from the probability mass function. From the definition, the ratio satisfies\n$$\n\\frac{f(k+1)}{f(k)} \\;=\\; \\frac{\\Gamma(k+1+r)}{\\Gamma(k+r)} \\cdot \\frac{\\Gamma(k+1)}{\\Gamma(k+2)} \\cdot (1-p) \\;=\\; \\frac{k+r}{k+1} \\,(1-p),\n$$\nwhere we used $\\Gamma(x+1)=x\\,\\Gamma(x)$. This yields the log-recursion\n$$\n\\ell_{k} \\;=\\; \\log f(k),\\quad \\ell_{0} \\;=\\; r\\log p,\\qquad \\ell_{k+1} \\;=\\; \\ell_{k} + \\log(1-p) + \\log(k+r) - \\log(k+1).\n$$\nTo remain in the logarithmic domain for the cumulative sum, we maintain $s_k = \\log\\left(\\sum_{j=0}^{k} \\exp(\\ell_j)\\right)$ via the two-argument $\\log\\text{sum}\\exp$ update\n$$\ns_{k+1} \\;=\\; \\log\\Big( \\exp(s_k) + \\exp(\\ell_{k+1}) \\Big) \\;=\\; m + \\log\\left(\\exp(s_k - m) + \\exp(\\ell_{k+1}-m)\\right),\n$$\nwhere $m=\\max\\{s_k,\\ell_{k+1}\\}$. We compare in the log domain by noting that $U \\le \\sum_{j=0}^{k} f(j)$ is equivalent to $\\log U \\le s_k$. This algorithm uses only a scalar for $\\ell_k$, a scalar for $s_k$, and the index $k$, thus satisfying the $\\mathcal{O}(1)$ memory constraint per generated variate.\n\nVectorized $\\log \\Gamma$ validation. For any finite range $k=0,1,\\dots,K^\\star$, we can compute the closed-form log-probabilities\n$$\n\\ell_k^{\\mathrm{ref}} \\;=\\; \\log f(k) \\;=\\; \\log\\Gamma(k+r) - \\log\\Gamma(r) - \\log\\Gamma(k+1) + k\\log(1-p) + r\\log p.\n$$\nBy evaluating $\\log\\Gamma(\\cdot)$ vectorially over $k=0,1,\\dots,K^\\star$, we can validate the recurrence-based $\\ell_k$ sequence against $\\ell_k^{\\mathrm{ref}}$ and record the maximum absolute discrepancy.\n\nFloating-point error model and cumulative bound. Adopt the standard model for floating-point arithmetic:\n$$\n\\mathrm{fl}(x \\circ y) \\;=\\; (x\\circ y)(1+\\delta), \\qquad |\\delta|\\le \\varepsilon_{\\mathrm{mach}},\n$$\nfor operations $\\circ \\in \\{+,-,\\times,\\div\\}$. We assume that the implementations of $\\exp(\\cdot)$, $\\log(\\cdot)$, and $\\log\\Gamma(\\cdot)$ incur relative errors of order $\\varepsilon_{\\mathrm{mach}}$ as well. Consider one two-argument $\\log\\text{sum}\\exp$ update:\n$$\ns' \\;=\\; \\log\\Big(\\exp(s) + \\exp(\\ell)\\Big) \\;=\\; m + \\log\\left(\\exp(s-m)+\\exp(\\ell-m)\\right),\\quad m=\\max\\{s,\\ell\\}.\n$$\nThe computed $s'_{\\mathrm{comp}}$ involves a finite sequence of elementary operations: one $\\max$, two exponentials, one addition, one logarithm, and one addition. Counting all elementary arithmetic operations and elementary functions (which we conservatively model as each incurring a relative error of at most $\\varepsilon_{\\mathrm{mach}}$), a first-order error propagation (neglecting $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}}^2)$ terms) yields that the absolute error in one update is bounded by a constant multiple $\\gamma_0 \\varepsilon_{\\mathrm{mach}}$, where the constant $\\gamma_0$ accounts for the accumulation through the chain of operations. Similarly, one update of the log-probability $\\ell$ uses three evaluations (two $\\log$ and one addition) and incurring a bounded absolute error per step of order $\\varepsilon_{\\mathrm{mach}}$ times a modest constant. Over $m$ steps, a standard forward error accumulation argument implies a bound of the form\n$$\n\\big| s_m^{\\mathrm{comp}} - \\log F(k_m) \\big| \\;\\le\\; \\gamma \\, m \\, \\varepsilon_{\\mathrm{mach}} + \\mathcal{O}(\\varepsilon_{\\mathrm{mach}}^2),\n$$\nfor some universal constant $\\gamma$ that upper-bounds the per-step contribution. To be explicit and conservative without relying on unknown implementation details, we choose\n$$\nB(m) \\;=\\; \\big(128\\, m + 1\\big)\\,\\varepsilon_{\\mathrm{mach}},\n$$\ntaking $\\gamma=128$ and adding one extra unit-roundoff to cover initialization. This $B(m)$ bounds the absolute error in the computed $\\log$-cumulative after $m$ two-argument $\\log\\text{sum}\\exp$ updates. A similar argument applied to the recurrence for $\\ell_k$ implies that the maximum absolute discrepancy between recurrence-generated $\\ell_k$ and $\\ell_k^{\\mathrm{ref}}$ across $k=0,1,\\dots,K^\\star$ is bounded by\n$$\n\\max_{0\\le k\\le K^\\star} \\big|\\ell_k^{\\mathrm{comp}} - \\ell_k^{\\mathrm{ref}}\\big| \\;\\le\\; C^\\star \\,(K^\\star+1)\\,\\varepsilon_{\\mathrm{mach}},\n$$\nand we conservatively set $C^\\star=64$.\n\nStatistical validation. For a sample of size $n$, compute the sample mean $\\hat{\\mu}$ and sample variance $\\hat{\\sigma}^2$ using a one-pass algorithm such as Welford's method to preserve numerical stability in $\\mathcal{O}(1)$ memory. Compare $\\hat{\\mu}$ to the theoretical mean $\\mu = r(1-p)/p$ using the standard error $\\sqrt{\\mathrm{Var}(K)/n}$ with $\\mathrm{Var}(K)=r(1-p)/p^2$. Report whether $|\\hat{\\mu}-\\mu|\\le 4 \\sqrt{\\mathrm{Var}(K)/n}$.\n\nTest suite and deterministic outputs. Use the specified parameter sets and seeds:\n- Test $1$: $(r,p,n,\\text{seed})=(2.5,\\,0.3,\\,10000,\\,1729)$ and uniform grid $\\{0.1,0.5,0.9\\}$.\n- Test $2$: $(r,p,n,\\text{seed})=(10.0,\\,0.05,\\,5000,\\,314159)$.\n- Test $3$: $(r,p,n,\\text{seed})=(0.7,\\,0.95,\\,20000,\\,271828)$.\n- Test $4$: $(r,p)=(50.0,\\,0.2)$ and uniform grid $\\{0.01,0.5,0.99\\}$.\n\nFor Tests $1$–$3$, generate the sample using the constrained-memory inverse-transform algorithm described above; for Tests $1$ and $4$, perform the numerical validations using $B(m)$ and $C^\\star$ as specified. The final program must print a single line formatted as a list of the four test outputs:\n$$\n\\big[\\,[\\hat{\\mu}_1,\\hat{\\sigma}^2_1,\\mathrm{mean\\_ok}_1,\\mathrm{cdf\\_bound\\_ok}_1,\\mathrm{logpmf\\_bound\\_ok}_1],\\,[\\hat{\\mu}_2,\\hat{\\sigma}^2_2,\\mathrm{mean\\_ok}_2],\\,[\\hat{\\mu}_3,\\hat{\\sigma}^2_3,\\mathrm{mean\\_ok}_3],\\,[\\mathrm{cdf\\_bound\\_ok}_4,\\mathrm{logpmf\\_bound\\_ok}_4]\\,\\big].\n$$\n\nThis design integrates the core definition of the negative binomial distribution, derives the on-the-fly log-ratio recurrence from first principles, implements numerically stable $\\log\\text{sum}\\exp$ updates, and uses a principled, explicit floating-point error accumulation bound based on the standard floating-point model.", "answer": "```python\nimport math\nfrom typing import Tuple, List\nimport numpy as np\nfrom scipy.special import gammaln, betainc\n\n# Negative Binomial NB(r, p) where K counts failures before r successes with success prob p\n# pmf: P(K=k) = C(k+r-1, k) (1-p)^k p^r\n# mean = r*(1-p)/p, var = r*(1-p)/p^2\n\ndef logsumexp2(a: float, b: float) - float:\n    \"\"\"Stable log(exp(a)+exp(b)) for two scalars.\"\"\"\n    if a == -math.inf:\n        return b\n    if b == -math.inf:\n        return a\n    m = a if a = b else b\n    return m + math.log(math.exp(a - m) + math.exp(b - m))\n\ndef sample_nb_inverse_log(r: float, p: float, rng: np.random.Generator) - Tuple[int, int]:\n    \"\"\"\n    Sample one NB(r,p) variate using inverse transform in the log domain.\n    Returns (k, m_updates) where m_updates is the number of logsumexp updates performed.\n    Memory footprint per sample is O(1).\n    \"\"\"\n    # Precompute logs\n    logp = math.log(p)\n    log1mp = math.log1p(-p)  # log(1-p)\n    # Initialize\n    l = r * logp  # log pmf at k=0\n    s = l         # log cumulative at k=0\n    m_updates = 1\n    u = rng.random()\n    logu = math.log(u)\n    if logu = s:\n        return 0, m_updates\n    # Iterate k = 1,2,...\n    k = 0\n    while True:\n        # Update k - k+1: l_{k+1} = l_k + log(1-p) + log(k+r) - log(k+1)\n        kp1 = k + 1.0\n        l += log1mp + math.log(k + r) - math.log(kp1)\n        # Update cumulative in log domain: s = logsumexp(s, l)\n        # Inline two-arg logsumexp to reduce overhead\n        m = s if s = l else l\n        s = m + math.log(math.exp(s - m) + math.exp(l - m))\n        m_updates += 1\n        k += 1\n        if logu = s:\n            return k, m_updates\n\ndef nb_logpmf_closed_form_vec(r: float, p: float, kmax: int) - np.ndarray:\n    \"\"\"Vectorized closed-form log pmf using gammaln for k=0..kmax.\"\"\"\n    ks = np.arange(kmax + 1, dtype=np.float64)\n    return gammaln(ks + r) - gammaln(r) - gammaln(ks + 1.0) + ks * math.log1p(-p) + r * math.log(p)\n\ndef nb_logpmf_recurrence_seq(r: float, p: float, kmax: int) - np.ndarray:\n    \"\"\"Generate log pmf sequence 0..kmax using recurrence in double precision.\"\"\"\n    logp = math.log(p)\n    log1mp = math.log1p(-p)\n    out = np.empty(kmax + 1, dtype=np.float64)\n    l = r * logp\n    out[0] = l\n    for k in range(kmax):\n        l = l + log1mp + math.log(k + r) - math.log(k + 1.0)\n        out[k + 1] = l\n    return out\n\ndef cdf_log_exact_via_betainc(r: float, p: float, k: int) - float:\n    \"\"\"Compute log CDF = log P(K = k) using regularized incomplete Beta.\"\"\"\n    # CDF F(k) = I_p(r, k+1)\n    F = betainc(r, k + 1.0, p)\n    # Ensure it's in (0,1]; due to numerical issues, clip safely\n    F = min(max(F, np.finfo(float).tiny), 1.0)\n    return math.log(F)\n\ndef bound_logcdf_error(m_updates: int, eps: float) - float:\n    \"\"\"Conservative absolute error bound for log-cumulative after m two-arg logsumexp updates.\"\"\"\n    # B(m) = (128*m + 1) * eps\n    return (128.0 * m_updates + 1.0) * eps\n\ndef bound_logpmf_seq_error(klen: int, eps: float) - float:\n    \"\"\"Conservative bound for max abs discrepancy in log pmf sequence of length klen.\"\"\"\n    # C* (K+1) * eps with C=64; here klen = K+1\n    return 64.0 * klen * eps\n\ndef welford_mean_variance_nb(r: float, p: float, n: int, seed: int) - Tuple[float, float]:\n    \"\"\"Generate n samples and compute mean and variance via Welford, using O(1) memory.\"\"\"\n    rng = np.random.default_rng(seed)\n    mean = 0.0\n    M2 = 0.0\n    for i in range(1, n + 1):\n        k, _ = sample_nb_inverse_log(r, p, rng)\n        x = float(k)\n        delta = x - mean\n        mean += delta / i\n        M2 += delta * (x - mean)\n    var = M2 / (n - 1) if n  1 else 0.0\n    return mean, var\n\ndef numerical_validation(r: float, p: float, u_values: List[float]) - Tuple[bool, bool]:\n    \"\"\"\n    For given (r,p) and list of uniform values, run the generator until stopping index k(u)\n    and check:\n      - |s - log F(k)| = B(m) for all u (cdf_bound_ok)\n      - max_k |l_recur - l_ref| = 64*(K*+1)*eps over k=0..K*, where K* is max stopping index (logpmf_bound_ok)\n    \"\"\"\n    eps = np.finfo(float).eps\n    # To force specific U values, wrap a RNG that returns given u's in order\n    # But we can emulate by looping the inversion logic with supplied u (without RNG)\n    cdf_bound_ok = True\n    Kmax = 0\n    # For collecting L sequences up to K per u to set Kmax\n    ks_for_check = []\n    for u in u_values:\n        # Reproduce the inversion steps deterministically for given u\n        logp = math.log(p)\n        log1mp = math.log1p(-p)\n        l = r * logp\n        s = l\n        m_updates = 1\n        logu = math.log(u)\n        k = 0\n        if not (logu = s):\n            while True:\n                l = l + log1mp + math.log(k + r) - math.log(k + 1.0)\n                # logsumexp\n                m = s if s = l else l\n                s = m + math.log(math.exp(s - m) + math.exp(l - m))\n                m_updates += 1\n                k += 1\n                if logu = s:\n                    break\n        # Compare against exact log CDF\n        logF = cdf_log_exact_via_betainc(r, p, k)\n        err = abs(s - logF)\n        if err  bound_logcdf_error(m_updates, eps):\n            cdf_bound_ok = False\n        Kmax = max(Kmax, k)\n        ks_for_check.append(k)\n    # Vectorized log-pmf validation over 0..Kmax\n    l_ref = nb_logpmf_closed_form_vec(r, p, Kmax)\n    l_rec = nb_logpmf_recurrence_seq(r, p, Kmax)\n    max_abs_diff = float(np.max(np.abs(l_ref - l_rec)))\n    logpmf_bound_ok = (max_abs_diff = bound_logpmf_seq_error(Kmax + 1, eps))\n    return cdf_bound_ok, logpmf_bound_ok\n\ndef test_case_1():\n    r, p, n, seed = 2.5, 0.3, 10000, 1729\n    mu = r * (1.0 - p) / p\n    var = r * (1.0 - p) / (p * p)\n    mean_hat, var_hat = welford_mean_variance_nb(r, p, n, seed)\n    se = math.sqrt(var / n)\n    mean_ok = abs(mean_hat - mu) = 4.0 * se\n    # numerical validation\n    cdf_ok, lpmf_ok = numerical_validation(r, p, [0.1, 0.5, 0.9])\n    return [mean_hat, var_hat, bool(mean_ok), bool(cdf_ok), bool(lpmf_ok)]\n\ndef test_case_2():\n    r, p, n, seed = 10.0, 0.05, 5000, 314159\n    mu = r * (1.0 - p) / p\n    var = r * (1.0 - p) / (p * p)\n    mean_hat, var_hat = welford_mean_variance_nb(r, p, n, seed)\n    se = math.sqrt(var / n)\n    mean_ok = abs(mean_hat - mu) = 4.0 * se\n    return [mean_hat, var_hat, bool(mean_ok)]\n\ndef test_case_3():\n    r, p, n, seed = 0.7, 0.95, 20000, 271828\n    mu = r * (1.0 - p) / p\n    var = r * (1.0 - p) / (p * p)\n    mean_hat, var_hat = welford_mean_variance_nb(r, p, n, seed)\n    se = math.sqrt(var / n)\n    mean_ok = abs(mean_hat - mu) = 4.0 * se\n    return [mean_hat, var_hat, bool(mean_ok)]\n\ndef test_case_4():\n    r, p = 50.0, 0.2\n    cdf_ok, lpmf_ok = numerical_validation(r, p, [0.01, 0.5, 0.99])\n    return [bool(cdf_ok), bool(lpmf_ok)]\n\ndef solve():\n    results = []\n    results.append(test_case_1())\n    results.append(test_case_2())\n    results.append(test_case_3())\n    results.append(test_case_4())\n    # Print as a single line with exact required format\n    # Ensure Python default string of list is acceptable\n    print(str(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3323013"}]}