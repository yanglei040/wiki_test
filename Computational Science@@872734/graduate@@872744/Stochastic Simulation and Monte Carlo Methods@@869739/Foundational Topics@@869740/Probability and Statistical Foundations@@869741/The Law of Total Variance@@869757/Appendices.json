{"hands_on_practices": [{"introduction": "To build a solid intuition for the law of total variance, we begin with a direct computational verification. This exercise uses a two-point mixture model, a structure commonly encountered in stratified sampling and other areas of stochastic simulation. By calculating the total variance in two distinct ways—first directly, and then through the decomposed components—you will gain a concrete understanding of how the total variance is partitioned into the average variance within subgroups, $\\mathbb{E}[\\operatorname{Var}(X \\mid Y)]$, and the variance between the means of those subgroups, $\\operatorname{Var}(\\mathbb{E}[X \\mid Y])$ [@problem_id:3354738].", "problem": "Consider the following two-point mixture model that is commonly used to formalize stratified sampling in Monte Carlo (MC) simulation. Let $Y \\in \\{0,1\\}$ be a Bernoulli random variable with $\\mathbb{P}(Y=1)=p$, where $p=\\frac{2}{5}$. Conditional on $Y$, let $X \\mid (Y=0) \\sim \\mathcal{N}(\\mu_{0},\\sigma_{0}^{2})$ and $X \\mid (Y=1) \\sim \\mathcal{N}(\\mu_{1},\\sigma_{1}^{2})$, with parameters $\\mu_{0}=0$, $\\sigma_{0}^{2}=1$, $\\mu_{1}=3$, and $\\sigma_{1}^{2}=4$. \n\nUsing only the definitions of expectation and variance and the tower property of conditional expectation (i.e., $\\mathbb{E}[\\mathbb{E}[Z \\mid Y]]=\\mathbb{E}[Z]$ for any integrable random variable $Z$), do the following:\n\n1. Compute $\\mathbb{E}[X]$ and $\\operatorname{Var}(X)$ directly by expanding $\\operatorname{Var}(X)=\\mathbb{E}[X^{2}] - (\\mathbb{E}[X])^{2}$ and conditioning on $Y$ where appropriate.\n2. Compute $\\mathbb{E}[\\operatorname{Var}(X \\mid Y)]$ and $\\operatorname{Var}(\\mathbb{E}[X \\mid Y])$ separately, using only conditional means and variances.\n3. Verify by explicit computation that the two routes to $\\operatorname{Var}(X)$ agree, and report the common value of $\\operatorname{Var}(X)$ as a single number. \n\nRound your final reported value to four significant figures. No units are required.", "solution": "The problem provides a two-point mixture model for a random variable $X$. The mixing variable is $Y \\sim \\text{Bernoulli}(p)$, with $\\mathbb{P}(Y=1)=p=\\frac{2}{5}$ and thus $\\mathbb{P}(Y=0)=1-p=\\frac{3}{5}$. The conditional distributions of $X$ given $Y$ are specified as $X \\mid (Y=0) \\sim \\mathcal{N}(\\mu_{0},\\sigma_{0}^{2})$ and $X \\mid (Y=1) \\sim \\mathcal{N}(\\mu_{1},\\sigma_{1}^{2})$. The parameters are given as $\\mu_{0}=0$, $\\sigma_{0}^{2}=1$, $\\mu_{1}=3$, and $\\sigma_{1}^{2}=4$. We are tasked with computing the variance of $X$, $\\operatorname{Var}(X)$, through two different methods and verifying their agreement.\n\nFirst, we establish the necessary conditional moments. For a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, the mean is $\\mu$ and the variance is $\\sigma^2$. The second moment is $\\mathbb{E}[X^2] = \\operatorname{Var}(X) + (\\mathbb{E}[X])^2 = \\sigma^2 + \\mu^2$.\nTherefore, we have the following conditional moments:\n$\\mathbb{E}[X \\mid Y=0] = \\mu_{0} = 0$.\n$\\operatorname{Var}(X \\mid Y=0) = \\sigma_{0}^{2} = 1$.\n$\\mathbb{E}[X^2 \\mid Y=0] = \\sigma_{0}^{2} + \\mu_{0}^2 = 1 + 0^2 = 1$.\n\n$\\mathbb{E}[X \\mid Y=1] = \\mu_{1} = 3$.\n$\\operatorname{Var}(X \\mid Y=1) = \\sigma_{1}^{2} = 4$.\n$\\mathbb{E}[X^2 \\mid Y=1] = \\sigma_{1}^{2} + \\mu_{1}^2 = 4 + 3^2 = 4 + 9 = 13$.\n\nWe address the three parts of the problem in sequence.\n\n1. Direct computation of $\\mathbb{E}[X]$ and $\\operatorname{Var}(X)$.\n\nTo compute the unconditional expectation $\\mathbb{E}[X]$, we use the law of total expectation (or tower property): $\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X \\mid Y]]$. The inner expectation, $\\mathbb{E}[X \\mid Y]$, is a random variable that depends on $Y$.\n$$\n\\mathbb{E}[X \\mid Y] = \\begin{cases} \\mu_0  \\text{if } Y=0 \\\\ \\mu_1  \\text{if } Y=1 \\end{cases}\n$$\nTaking the expectation over $Y$:\n$$\n\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X \\mid Y]] = \\mathbb{E}[X \\mid Y=0] \\cdot \\mathbb{P}(Y=0) + \\mathbb{E}[X \\mid Y=1] \\cdot \\mathbb{P}(Y=1)\n$$\n$$\n\\mathbb{E}[X] = \\mu_{0}(1-p) + \\mu_{1}p = (0)\\left(\\frac{3}{5}\\right) + (3)\\left(\\frac{2}{5}\\right) = 0 + \\frac{6}{5} = \\frac{6}{5}\n$$\nTo compute $\\operatorname{Var}(X)$, we use the formula $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. We first need $\\mathbb{E}[X^2]$, which we find again using the law of total expectation: $\\mathbb{E}[X^2] = \\mathbb{E}[\\mathbb{E}[X^2 \\mid Y]]$.\n$$\n\\mathbb{E}[X^2 \\mid Y] = \\begin{cases} \\sigma_0^2 + \\mu_0^2  \\text{if } Y=0 \\\\ \\sigma_1^2 + \\mu_1^2  \\text{if } Y=1 \\end{cases}\n$$\nTaking the expectation over $Y$:\n$$\n\\mathbb{E}[X^2] = \\mathbb{E}[\\mathbb{E}[X^2 \\mid Y]] = (\\sigma_{0}^{2} + \\mu_{0}^2)(1-p) + (\\sigma_{1}^{2} + \\mu_{1}^2)p\n$$\n$$\n\\mathbb{E}[X^2] = (1)\\left(\\frac{3}{5}\\right) + (13)\\left(\\frac{2}{5}\\right) = \\frac{3}{5} + \\frac{26}{5} = \\frac{29}{5}\n$$\nNow we can compute the variance:\n$$\n\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\frac{29}{5} - \\left(\\frac{6}{5}\\right)^2 = \\frac{29}{5} - \\frac{36}{25} = \\frac{145}{25} - \\frac{36}{25} = \\frac{109}{25}\n$$\n\n2. Computation of the components of the law of total variance.\n\nThe law of total variance states that $\\operatorname{Var}(X) = \\mathbb{E}[\\operatorname{Var}(X \\mid Y)] + \\operatorname{Var}(\\mathbb{E}[X \\mid Y])$. We compute each term separately.\n\nFirst, we compute the expected conditional variance, $\\mathbb{E}[\\operatorname{Var}(X \\mid Y)]$. The term $\\operatorname{Var}(X \\mid Y)$ is a random variable whose value depends on $Y$:\n$$\n\\operatorname{Var}(X \\mid Y) = \\begin{cases} \\sigma_0^2  \\text{if } Y=0 \\\\ \\sigma_1^2  \\text{if } Y=1 \\end{cases}\n$$\nTaking the expectation over $Y$:\n$$\n\\mathbb{E}[\\operatorname{Var}(X \\mid Y)] = \\operatorname{Var}(X \\mid Y=0) \\cdot \\mathbb{P}(Y=0) + \\operatorname{Var}(X \\mid Y=1) \\cdot \\mathbb{P}(Y=1)\n$$\n$$\n\\mathbb{E}[\\operatorname{Var}(X \\mid Y)] = \\sigma_{0}^{2}(1-p) + \\sigma_{1}^{2}p = (1)\\left(\\frac{3}{5}\\right) + (4)\\left(\\frac{2}{5}\\right) = \\frac{3}{5} + \\frac{8}{5} = \\frac{11}{5}\n$$\nSecond, we compute the variance of the conditional expectation, $\\operatorname{Var}(\\mathbb{E}[X \\mid Y])$. Let $Z = \\mathbb{E}[X \\mid Y]$. From Part 1, we know that $Z$ is a random variable that takes the value $\\mu_0=0$ with probability $1-p=\\frac{3}{5}$ and $\\mu_1=3$ with probability $p=\\frac{2}{5}$. We also found its expectation, $\\mathbb{E}[Z] = \\mathbb{E}[\\mathbb{E}[X \\mid Y]] = \\mathbb{E}[X] = \\frac{6}{5}$. To find $\\operatorname{Var}(Z)$, we use $\\operatorname{Var}(Z) = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2$. We must compute $\\mathbb{E}[Z^2] = \\mathbb{E}[(\\mathbb{E}[X \\mid Y])^2]$.\n$$\n\\mathbb{E}[(\\mathbb{E}[X \\mid Y])^2] = (\\mathbb{E}[X \\mid Y=0])^2 \\cdot \\mathbb{P}(Y=0) + (\\mathbb{E}[X \\mid Y=1])^2 \\cdot \\mathbb{P}(Y=1)\n$$\n$$\n\\mathbb{E}[(\\mathbb{E}[X \\mid Y])^2] = \\mu_{0}^2(1-p) + \\mu_{1}^2 p = (0)^2\\left(\\frac{3}{5}\\right) + (3)^2\\left(\\frac{2}{5}\\right) = 0 + \\frac{18}{5} = \\frac{18}{5}\n$$\nNow, we compute the variance of the conditional expectation:\n$$\n\\operatorname{Var}(\\mathbb{E}[X \\mid Y]) = \\mathbb{E}[(\\mathbb{E}[X \\mid Y])^2] - (\\mathbb{E}[\\mathbb{E}[X \\mid Y]])^2 = \\frac{18}{5} - \\left(\\frac{6}{5}\\right)^2 = \\frac{18}{5} - \\frac{36}{25} = \\frac{90}{25} - \\frac{36}{25} = \\frac{54}{25}\n$$\n\n3. Verification and final result.\n\nWe now verify that the sum of the two components calculated in Part 2 equals the direct calculation of $\\operatorname{Var}(X)$ from Part 1.\n$$\n\\mathbb{E}[\\operatorname{Var}(X \\mid Y)] + \\operatorname{Var}(\\mathbb{E}[X \\mid Y]) = \\frac{11}{5} + \\frac{54}{25} = \\frac{55}{25} + \\frac{54}{25} = \\frac{109}{25}\n$$\nThis result matches the value of $\\operatorname{Var}(X) = \\frac{109}{25}$ computed in Part 1. The agreement is verified.\n\nThe common value of the variance is $\\operatorname{Var}(X) = \\frac{109}{25} = 4.36$. The problem requires this value to be reported to four significant figures. Thus, the value is $4.360$.", "answer": "$$\n\\boxed{4.360}\n$$", "id": "3354738"}, {"introduction": "Moving from analytical verification to practical estimation, this exercise challenges you to implement a nested Monte Carlo simulation. The goal is to empirically estimate the components of the variance decomposition from generated data, a fundamental skill for analyzing real-world simulation outputs. By coding this experiment, you will not only see the law of total variance in action but also uncover an exact algebraic identity for sample variances that mirrors the theoretical law, bridging the gap between abstract concepts and computational practice [@problem_id:3354823].", "problem": "Consider random variables $Y$ and $Z$ that are independent, and define $X$ by the linear relation $X = aY + Z$, where $a$ is a real constant. Let $\\operatorname{E}[\\cdot]$ denote expectation and $\\operatorname{Var}[\\cdot]$ denote variance. The foundational definitions to be used are the conditional expectation $\\operatorname{E}[X \\mid Y]$, the conditional variance $\\operatorname{Var}[X \\mid Y]$, and the unconditional variance $\\operatorname{Var}[X] = \\operatorname{E}[X^2] - (\\operatorname{E}[X])^2$. The goal is to create a reproducible Monte Carlo (MC) experiment to estimate the decomposition of $\\operatorname{Var}[X]$ into an average of conditional variances and a variance of conditional expectations, and to quantify how adjusting $Y$ changes this decomposition.\n\nYou must implement a nested Monte Carlo estimator that proceeds as follows. For a fixed integer $N$ (outer sample size) and fixed integer $M$ (inner sample size), draw $Y_1,\\dots,Y_N$ from a specified distribution of $Y$. For each $i \\in \\{1,\\dots,N\\}$, draw $Z_{i,1},\\dots,Z_{i,M}$ from a specified distribution of $Z$, independent of $Y$. Then construct $X_{i,j} = a Y_i + Z_{i,j}$ for all $i \\in \\{1,\\dots,N\\}$ and $j \\in \\{1,\\dots,M\\}$. Using these samples, compute the following three estimators based on the foundational definitions:\n- The estimated average conditional variance $\\widehat{v}_{\\text{in}} = \\dfrac{1}{N}\\sum_{i=1}^N \\widehat{\\operatorname{Var}}_j\\!\\left(X_{i,j} \\mid Y_i\\right)$, where $\\widehat{\\operatorname{Var}}_j(\\cdot)$ is the sample variance across the $M$ inner samples for a fixed $i$ computed without degrees-of-freedom correction.\n- The estimated variance of the conditional means $\\widehat{v}_{\\text{ex}} = \\widehat{\\operatorname{Var}}_i\\!\\left(\\widehat{m}(Y_i)\\right)$, where $\\widehat{m}(Y_i) = \\dfrac{1}{M}\\sum_{j=1}^M X_{i,j}$ is the inner-sample mean at a fixed $Y_i$, and $\\widehat{\\operatorname{Var}}_i(\\cdot)$ is the sample variance across $i \\in \\{1,\\dots,N\\}$ computed without degrees-of-freedom correction.\n- The estimated unconditional variance $\\widehat{v}_X = \\widehat{\\operatorname{Var}}\\!\\left(X_{i,j}\\right)$ computed across all $N \\times M$ samples $\\{X_{i,j}\\}$, without degrees-of-freedom correction.\n\nFor each test case described below, produce two scalar quantities:\n- The absolute decomposition residual $\\Delta = \\left| \\widehat{v}_X - \\left(\\widehat{v}_{\\text{in}} + \\widehat{v}_{\\text{ex}}\\right) \\right|$.\n- The explained fraction $\\rho = \\dfrac{\\widehat{v}_{\\text{ex}}}{\\widehat{v}_X}$.\n\nUse the same values of $N$ and $M$ across all test cases, with the outer and inner sample sizes set to $N = 6000$ and $M = 40$. Ensure reproducibility by using the specified integer random seeds for each test case; use a single pseudorandom generator initialized with the given seed per test case to generate both $Y$ and $Z$.\n\nTest suite. Apply the above procedure to the following five scientifically plausible and self-consistent parameter sets, each of which specifies the distribution of $Y$, the distribution of $Z$, the coefficient $a$, and a random seed. In all cases, $Z$ is Gaussian with mean $0$ and standard deviation $\\sigma_Z$, i.e., $Z \\sim \\mathcal{N}(0,\\sigma_Z^2)$.\n\n- Case $1$ (baseline Gaussian): $Y \\sim \\mathcal{N}(0,\\sigma_Y^2)$ with $\\sigma_Y = 1$, $a = 2$, $\\sigma_Z = 1$, seed $= 123456$.\n- Case $2$ (degenerate $Y$): $Y \\equiv 0$ (deterministic constant), $a = 2$, $\\sigma_Z = 1$, seed $= 223344$.\n- Case $3$ (high-variance $Y$): $Y \\sim \\mathcal{N}(0,\\sigma_Y^2)$ with $\\sigma_Y = 5$, $a = 1.5$, $\\sigma_Z = 0.5$, seed $= 345678$.\n- Case $4$ (heavy-tailed $Y$): $Y = \\sigma_Y T$ with $T$ following the Student-$t$ distribution with $\\nu = 5$ degrees of freedom and $\\sigma_Y = 1$, $a = 1$, $\\sigma_Z = 1$, seed $= 456789$.\n- Case $5$ (rare-event binary $Y$): $Y \\sim \\text{Bernoulli}(p)$ taking values in $\\{0,1\\}$ with $p = 0.01$, $a = 3$, $\\sigma_Z = 1$, seed $= 567890$.\n\nYour program must implement the nested Monte Carlo estimation as defined and must produce a single line of output containing the results aggregated across all five test cases. The output format must be a single comma-separated list enclosed in square brackets, consisting of the pair $(\\Delta,\\rho)$ for each case in order, flattened into a single list. For example, the format is $[\\Delta_1,\\rho_1,\\Delta_2,\\rho_2,\\Delta_3,\\rho_3,\\Delta_4,\\rho_4,\\Delta_5,\\rho_5]$, where each $\\Delta_k$ and $\\rho_k$ is a floating-point number.", "solution": "The problem requires the implementation of a nested Monte Carlo simulation to empirically verify the Law of Total Variance for a random variable $X$ defined as a linear transformation of two independent random variables, $Y$ and $Z$, such that $X = aY + Z$. The validation will be demonstrated through the estimation of the components of variance and an analysis of the residual from the theoretical identity.\n\nThe theoretical foundation for this problem is the Law of Total Variance (LoTV), also known as Eve's Law, which states that for any two random variables $X$ and $Y$ (not necessarily independent), the variance of $X$ can be decomposed as:\n$$\n\\operatorname{Var}[X] = \\operatorname{E}[\\operatorname{Var}[X \\mid Y]] + \\operatorname{Var}[\\operatorname{E}[X \\mid Y]]\n$$\nThe term $\\operatorname{E}[\\operatorname{Var}[X \\mid Y]]$ represents the expected value of the conditional variance of $X$ given $Y$. It is the portion of the variance of $X$ that is not explained by the variability of $Y$. The term $\\operatorname{Var}[\\operatorname{E}[X \\mid Y]]$ is the variance of the conditional expectation of $X$ given $Y$. It represents the portion of the variance of $X$ that is explained by the variability of $Y$.\n\nFor the specific linear model $X = aY + Z$, where $Y$ and $Z$ are independent, we can determine the theoretical components of this decomposition.\nThe conditional expectation of $X$ given $Y=y$ is:\n$$\n\\operatorname{E}[X \\mid Y=y] = \\operatorname{E}[aY + Z \\mid Y=y] = \\operatorname{E}[ay + Z] = ay + \\operatorname{E}[Z]\n$$\nThus, the conditional expectation as a random variable is $\\operatorname{E}[X \\mid Y] = aY + \\operatorname{E}[Z]$. The variance of this term is:\n$$\n\\operatorname{Var}[\\operatorname{E}[X \\mid Y]] = \\operatorname{Var}[aY + \\operatorname{E}[Z]] = a^2 \\operatorname{Var}[Y]\n$$\nsince $\\operatorname{E}[Z]$ is a constant.\n\nThe conditional variance of $X$ given $Y=y$ is:\n$$\n\\operatorname{Var}[X \\mid Y=y] = \\operatorname{Var}[aY + Z \\mid Y=y] = \\operatorname{Var}[ay + Z] = \\operatorname{Var}[Z]\n$$\nsince $ay$ is a constant when conditioned on $Y=y$, and $Z$ is independent of $Y$. The conditional variance is constant and does not depend on the value of $y$. Therefore, its expectation is simply:\n$$\n\\operatorname{E}[\\operatorname{Var}[X \\mid Y]] = \\operatorname{E}[\\operatorname{Var}[Z]] = \\operatorname{Var}[Z]\n$$\nSubstituting these into the LoTV, we obtain the well-known result for the variance of a scaled sum of independent variables:\n$$\n\\operatorname{Var}[X] = \\operatorname{Var}[Z] + a^2 \\operatorname{Var}[Y]\n$$\nThe simulation aims to estimate the terms $\\operatorname{E}[\\operatorname{Var}[X \\mid Y]]$ and $\\operatorname{Var}[\\operatorname{E}[X \\mid Y]]$ from generated samples and verify their sum against an estimate of the total variance $\\operatorname{Var}[X]$.\n\nThe nested Monte Carlo procedure is as follows:\n1.  Generate $N$ outer samples $Y_1, \\dots, Y_N$ from the distribution of $Y$.\n2.  For each $Y_i$, generate $M$ inner samples $Z_{i,1}, \\dots, Z_{i,M}$ from the distribution of $Z$.\n3.  Construct the matrix of $N \\times M$ samples $X_{i,j} = a Y_i + Z_{i,j}$.\n\nUsing these samples, the following estimators are computed, as specified, without degrees-of-freedom correction (i.e., using population variance formulae):\n-   The estimated average conditional variance, $\\widehat{v}_{\\text{in}}$, which estimates $\\operatorname{E}[\\operatorname{Var}[X \\mid Y]]$.\n    $$\n    \\widehat{v}_{\\text{in}} = \\frac{1}{N}\\sum_{i=1}^N \\widehat{\\operatorname{Var}}_j(X_{i,j} \\mid Y_i) = \\frac{1}{N}\\sum_{i=1}^N \\left[ \\frac{1}{M}\\sum_{j=1}^M (X_{i,j} - \\bar{X}_{i\\cdot})^2 \\right]\n    $$\n    where $\\bar{X}_{i\\cdot} = \\frac{1}{M}\\sum_{j=1}^M X_{i,j}$ is the sample mean for the $i$-th outer sample.\n\n-   The estimated variance of conditional means, $\\widehat{v}_{\\text{ex}}$, which estimates $\\operatorname{Var}[\\operatorname{E}[X \\mid Y]]$.\n    $$\n    \\widehat{v}_{\\text{ex}} = \\widehat{\\operatorname{Var}}_i(\\widehat{m}(Y_i)) = \\frac{1}{N}\\sum_{i=1}^N \\left( \\bar{X}_{i\\cdot} - \\bar{X}_{\\cdot\\cdot} \\right)^2\n    $$\n    where $\\widehat{m}(Y_i) = \\bar{X}_{i\\cdot}$ and $\\bar{X}_{\\cdot\\cdot} = \\frac{1}{N}\\sum_{i=1}^N \\bar{X}_{i\\cdot} = \\frac{1}{NM}\\sum_{i=1}^N\\sum_{j=1}^M X_{i,j}$ is the grand mean of all samples.\n\n-   The estimated unconditional variance, $\\widehat{v}_X$, which estimates $\\operatorname{Var}[X]$.\n    $$\n    \\widehat{v}_X = \\widehat{\\operatorname{Var}}(X_{i,j}) = \\frac{1}{NM}\\sum_{i=1}^N\\sum_{j=1}^M (X_{i,j} - \\bar{X}_{\\cdot\\cdot})^2\n    $$\n\nA key insight into this problem is that these empirical estimators satisfy an exact algebraic identity, analogous to the LoTV for true variances. This identity arises from the decomposition of the total sum of squares:\n$$\n\\sum_{i=1}^N\\sum_{j=1}^M (X_{i,j} - \\bar{X}_{\\cdot\\cdot})^2 = \\sum_{i=1}^N\\sum_{j=1}^M (X_{i,j} - \\bar{X}_{i\\cdot})^2 + M\\sum_{i=1}^N (\\bar{X}_{i\\cdot} - \\bar{X}_{\\cdot\\cdot})^2\n$$\nDividing the entire equation by the total number of samples, $NM$, yields:\n$$\n\\frac{1}{NM}\\sum_{i,j} (X_{i,j} - \\bar{X}_{\\cdot\\cdot})^2 = \\frac{1}{N}\\sum_i \\left(\\frac{1}{M}\\sum_j (X_{i,j} - \\bar{X}_{i\\cdot})^2\\right) + \\frac{1}{N}\\sum_i (\\bar{X}_{i\\cdot} - \\bar{X}_{\\cdot\\cdot})^2\n$$\nThis directly translates to the identity $\\widehat{v}_X = \\widehat{v}_{\\text{in}} + \\widehat{v}_{\\text{ex}}$. Consequently, the absolute decomposition residual, $\\Delta = \\left| \\widehat{v}_X - (\\widehat{v}_{\\text{in}} + \\widehat{v}_{\\text{ex}}) \\right|$, must be zero, apart from negligible floating-point precision errors. Its calculation serves as a powerful validation of the correct implementation of the specified estimators.\n\nThe second quantity to be computed, the explained fraction $\\rho = \\dfrac{\\widehat{v}_{\\text{ex}}}{\\widehat{v}_X}$, measures the proportion of the total sample variance of $X$ that can be attributed to the sample variance of the conditional means, which in turn reflects the variability of $Y$.\n\nThe implementation will use `numpy` for efficient vectorized calculations. For each test case, a `numpy.random.default_rng` instance is initialized with the specified seed to ensure reproducibility. The random variates for $Y$ and $Z$ are generated from this single generator instance. The estimators are computed using `numpy.var` with its default parameter `ddof=0`, which aligns with the problem's requirement for no degrees-of-freedom correction. The procedure is applied to each of the five specified parameter sets.\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# scipy is not strictly needed as numpy.random.Generator provides the\n# necessary distributions, but it is listed as an available library.\n# We will use only numpy for consistency in random number generation.\n\ndef solve():\n    \"\"\"\n    Implements a nested Monte Carlo experiment to estimate the decomposition of Var[X]\n    for X = aY + Z, based on the Law of Total Variance.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (Y_type, Y_params, a, sigma_Z, seed)\n        ('gaussian', {'mu': 0.0, 'sigma': 1.0}, 2.0, 1.0, 123456),\n        ('constant', {'value': 0.0}, 2.0, 1.0, 223344),\n        ('gaussian', {'mu': 0.0, 'sigma': 5.0}, 1.5, 0.5, 345678),\n        ('student-t', {'df': 5, 'sigma': 1.0}, 1.0, 1.0, 456789),\n        ('bernoulli', {'p': 0.01}, 3.0, 1.0, 567890)\n    ]\n\n    N = 6000  # Outer sample size\n    M = 40    # Inner sample size\n    \n    results = []\n\n    for case in test_cases:\n        y_type, y_params, a, sigma_Z, seed = case\n        \n        # Initialize a single pseudorandom generator for reproducibility per case.\n        rng = np.random.default_rng(seed)\n\n        # Generate N outer samples for Y.\n        if y_type == 'gaussian':\n            Y_samples = rng.normal(loc=y_params['mu'], scale=y_params['sigma'], size=N)\n        elif y_type == 'constant':\n            Y_samples = np.full(N, y_params['value'], dtype=np.float64)\n        elif y_type == 'student-t':\n            # Y = sigma_Y * T, where T ~ Student-t(df)\n            Y_samples = y_params['sigma'] * rng.standard_t(df=y_params['df'], size=N)\n        elif y_type == 'bernoulli':\n            # Generate from binomial with n=1.\n            Y_samples = rng.binomial(n=1, p=y_params['p'], size=N).astype(np.float64)\n\n        # Generate N x M inner samples for Z, independent of Y.\n        # Z ~ N(0, sigma_Z^2)\n        Z_samples = rng.normal(loc=0.0, scale=sigma_Z, size=(N, M))\n\n        # Construct N x M samples for X using broadcasting.\n        # Y_samples[:, np.newaxis] reshapes (N,) to (N, 1) to allow addition with (N, M).\n        X_samples = a * Y_samples[:, np.newaxis] + Z_samples\n        \n        # --- Compute Estimators ---\n        # All variances are computed without degrees-of-freedom correction (ddof=0),\n        # as per the problem specification. This is the default for numpy.var.\n\n        # v_in: Estimated average conditional variance, E[Var(X|Y)].\n        # 1. Compute Var(X|Y_i) for each i across inner samples j.\n        var_j_X_given_Y = np.var(X_samples, axis=1) # Shape (N,)\n        # 2. Average these conditional variances.\n        v_in = np.mean(var_j_X_given_Y)\n\n        # v_ex: Estimated variance of conditional means, Var(E[X|Y]).\n        # 1. Compute E[X|Y_i] for each i across inner samples j.\n        m_hat_Y = np.mean(X_samples, axis=1) # Shape (N,)\n        # 2. Compute the variance of these conditional means.\n        v_ex = np.var(m_hat_Y)\n\n        # v_X: Estimated unconditional variance, Var(X).\n        # Computed across all N*M samples.\n        v_X = np.var(X_samples)\n\n        # --- Compute Final Quantities ---\n        \n        # Absolute decomposition residual. Due to an exact algebraic identity for sample\n        # variances, this should be zero up to floating-point error.\n        delta = np.abs(v_X - (v_in + v_ex))\n        \n        # Explained fraction rho.\n        # Handle potential division by zero, although v_X is highly unlikely to be zero.\n        if v_X == 0.0:\n            rho = np.nan\n        else:\n            rho = v_ex / v_X\n\n        results.extend([delta, rho])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "answer": "[0.0,0.8001633519846371,0.0,0.0,3.552713678800501e-15,0.9890257321689255,0.0,0.5606622869507914,0.0,0.08182046206122678]", "id": "3354823"}, {"introduction": "This final practice demonstrates the true power of the law of total variance as a strategic tool for optimizing Monte Carlo methods. Here, you will use the variance decomposition not just for analysis, but for design: deriving an optimal boundary placement for stratified sampling to minimize the \"within-stratum\" variance component. This exercise reveals how a deep understanding of variance sources allows us to engineer more efficient estimators, a central goal in the field of stochastic simulation [@problem_id:3354809].", "problem": "Consider a stratified simulation design for a univariate random input where $U$ is uniformly distributed on the unit interval, $U \\sim \\mathrm{Uniform}(0,1)$. Let $f:[0,1]\\to\\mathbb{R}$ be a continuously differentiable, strictly monotone function representing the scalar output of interest in a Monte Carlo (MC) computation. Define a stratification by choosing boundaries $0=b_{0}b_{1}\\dotsb_{K}=1$ and a stratum index $Y\\in\\{1,\\dots,K\\}$ via the deterministic rule $Y=k$ if and only if $U\\in[b_{k-1},b_{k})$. Let $m_{k}=\\mathbb{E}[f(U)\\mid Y=k]$ denote the conditional mean of $f(U)$ in stratum $k$ and $\\sigma_{k}^{2}=\\mathrm{Var}(f(U)\\mid Y=k)$ its conditional variance; let $w_{k}=\\mathbb{P}(Y=k)=b_{k}-b_{k-1}$ be the stratum mass.\n\nStarting from the fundamental definitions of conditional expectation and variance together with the law of total variance, derive a necessary optimality criterion for the stratification boundaries $\\{b_{k}\\}$ that minimize the objective $\\mathbb{E}\\big[\\mathrm{Var}(f(U)\\mid Y)\\big]$ over all partitions of $[0,1]$ into $K$ intervals. Your derivation must proceed from first principles and logically justify any equivalence or transformation you use; do not assume unproven results from quantization or clustering theory.\n\nThen, specialize to the case $K=2$ and $f(u)=u^{2}$. In this case there is a single interior boundary $b\\in(0,1)$, free to vary. Compute the optimal boundary $b$ implied by your necessary criterion as an exact closed-form expression. Express your final answer as a single exact algebraic expression. No rounding is required, and no physical units apply.", "solution": "The objective is to find a necessary optimality criterion for the stratification boundaries $\\{b_k\\}_{k=1}^{K-1}$ that minimize the objective function $\\mathbb{E}\\big[\\mathrm{Var}(f(U)\\mid Y)\\big]$. This objective represents the expected conditional variance, which is the \"within-stratum variance\" component in the law of total variance, $\\mathrm{Var}(f(U)) = \\mathbb{E}[\\mathrm{Var}(f(U)\\mid Y)] + \\mathrm{Var}(\\mathbb{E}[f(U)\\mid Y)]$.\n\nLet the objective function be denoted by $L$. By the definition of expectation for a discrete random variable $Y$, the objective function can be written as a sum over the $K$ strata:\n$$ L = \\mathbb{E}\\big[\\mathrm{Var}(f(U)\\mid Y)\\big] = \\sum_{k=1}^{K} \\mathrm{Var}(f(U)\\mid Y=k) \\cdot \\mathbb{P}(Y=k) $$\nUsing the notation provided in the problem statement, this is:\n$$ L(b_1, \\dots, b_{K-1}) = \\sum_{k=1}^{K} \\sigma_k^2 w_k $$\nThe problem defines $w_k = \\mathbb{P}(Y=k) = \\mathbb{P}(U \\in [b_{k-1}, b_k)) = b_k - b_{k-1}$, since $U \\sim \\mathrm{Uniform}(0,1)$. The conditional variance is $\\sigma_k^2 = \\mathbb{E}[f(U)^2 \\mid Y=k] - (\\mathbb{E}[f(U) \\mid Y=k])^2$.\n\nTo express these conditional expectations as integrals, we need the conditional probability density of $U$ given $Y=k$. This is a uniform distribution over the interval $[b_{k-1}, b_k)$:\n$$ p(u \\mid Y=k) = \\begin{cases} \\frac{1}{b_k - b_{k-1}}  \\text{if } u \\in [b_{k-1}, b_k) \\\\ 0  \\text{otherwise} \\end{cases} $$\nThe conditional moments are integrals with respect to this density. Specifically, the conditional mean $m_k$ and the conditional expectation of $f(U)^2$ are:\n$$ m_k = \\mathbb{E}[f(U) \\mid Y=k] = \\int_{b_{k-1}}^{b_k} f(u) \\frac{1}{b_k - b_{k-1}} du = \\frac{1}{w_k} \\int_{b_{k-1}}^{b_k} f(u) du $$\n$$ \\mathbb{E}[f(U)^2 \\mid Y=k] = \\int_{b_{k-1}}^{b_k} f(u)^2 \\frac{1}{b_k - b_{k-1}} du = \\frac{1}{w_k} \\int_{b_{k-1}}^{b_k} f(u)^2 du $$\nSubstituting these into the expression for $\\sigma_k^2$:\n$$ \\sigma_k^2 = \\frac{1}{w_k} \\int_{b_{k-1}}^{b_k} f(u)^2 du - m_k^2 = \\frac{1}{w_k} \\int_{b_{k-1}}^{b_k} f(u)^2 du - \\left(\\frac{1}{w_k} \\int_{b_{k-1}}^{b_k} f(u) du\\right)^2 $$\nThe objective function $L = \\sum w_k \\sigma_k^2$ becomes:\n$$ L = \\sum_{k=1}^{K} w_k \\left[ \\frac{1}{w_k} \\int_{b_{k-1}}^{b_k} f(u)^2 du - \\frac{1}{w_k^2} \\left(\\int_{b_{k-1}}^{b_k} f(u) du\\right)^2 \\right] $$\n$$ L = \\sum_{k=1}^{K} \\left[ \\int_{b_{k-1}}^{b_k} f(u)^2 du - \\frac{1}{b_k - b_{k-1}} \\left(\\int_{b_{k-1}}^{b_k} f(u) du\\right)^2 \\right] $$\nTo find the necessary optimality criterion, we differentiate $L$ with respect to an arbitrary interior boundary $b_j$ for $j \\in \\{1, \\dots, K-1\\}$ and set the derivative to zero. The boundary $b_j$ appears only in the terms for stratum $j$ (as the upper boundary) and stratum $j+1$ (as the lower boundary). Let $T_k$ denote the $k$-th term in the sum for $L$.\n$$ \\frac{\\partial L}{\\partial b_j} = \\frac{\\partial T_j}{\\partial b_j} + \\frac{\\partial T_{j+1}}{\\partial b_j} $$\nWe compute each partial derivative using the Leibniz integral rule.\nFor the term $T_j(b_{j-1}, b_j) = \\int_{b_{j-1}}^{b_j} f(u)^2 du - \\frac{1}{b_j - b_{j-1}} (\\int_{b_{j-1}}^{b_j} f(u) du)^2$:\n$$ \\frac{\\partial T_j}{\\partial b_j} = f(b_j)^2 - \\frac{2\\left(\\int_{b_{j-1}}^{b_j} f(u) du\\right)f(b_j)(b_j - b_{j-1}) - \\left(\\int_{b_{j-1}}^{b_j} f(u) du\\right)^2(1)}{(b_j - b_{j-1})^2} $$\nDividing the numerator and denominator by $(b_j - b_{j-1})^2$ and recognizing $m_j = \\frac{1}{b_j-b_{j-1}}\\int_{b_{j-1}}^{b_j} f(u) du$, we get:\n$$ \\frac{\\partial T_j}{\\partial b_j} = f(b_j)^2 - \\left[2 m_j f(b_j) - m_j^2\\right] = (f(b_j) - m_j)^2 $$\nFor the term $T_{j+1}(b_j, b_{j+1}) = \\int_{b_j}^{b_{j+1}} f(u)^2 du - \\frac{1}{b_{j+1} - b_j} (\\int_{b_j}^{b_{j+1}} f(u) du)^2$:\n$$ \\frac{\\partial T_{j+1}}{\\partial b_j} = -f(b_j)^2 - \\frac{2\\left(\\int_{b_j}^{b_{j+1}} f(u) du\\right)(-f(b_j))(b_{j+1} - b_j) - \\left(\\int_{b_j}^{b_{j+1}} f(u) du\\right)^2(-1)}{(b_{j+1} - b_j)^2} $$\nSimilarly, recognizing $m_{j+1} = \\frac{1}{b_{j+1}-b_j}\\int_{b_j}^{b_{j+1}} f(u) du$, we have:\n$$ \\frac{\\partial T_{j+1}}{\\partial b_j} = -f(b_j)^2 - \\left[-2 m_{j+1} f(b_j) + m_{j+1}^2\\right] = - (f(b_j)^2 - 2m_{j+1}f(b_j) + m_{j+1}^2) = -(f(b_j) - m_{j+1})^2 $$\nSetting the total derivative $\\frac{\\partial L}{\\partial b_j}$ to zero for an optimal configuration yields:\n$$ (f(b_j) - m_j)^2 - (f(b_j) - m_{j+1})^2 = 0 $$\nThis implies $|f(b_j) - m_j| = |f(b_j) - m_{j+1}|$, which leads to two possibilities:\n$1$. $f(b_j) - m_j = f(b_j) - m_{j+1}$, which simplifies to $m_j = m_{j+1}$.\n$2$. $f(b_j) - m_j = -(f(b_j) - m_{j+1}) = m_{j+1} - f(b_j)$.\n\nThe function $f$ is given as strictly monotone. Let's assume it is strictly increasing without loss of generality. For any $u_1 \\in [b_{j-1}, b_j)$ and $u_2 \\in [b_j, b_{j+1})$, we have $u_1  b_j \\leq u_2$, so $f(u_1)  f(u_2)$. The conditional mean $m_j$ is the average of $f(u)$ values over the first interval, and $m_{j+1}$ is the average over the second. Therefore, $m_j  m_{j+1}$. The equality $m_j = m_{j+1}$ is impossible for non-degenerate intervals, so the first case is rejected.\n\nThe necessary optimality criterion must therefore be the second case:\n$$ 2f(b_j) = m_j + m_{j+1} \\implies f(b_j) = \\frac{m_j + m_{j+1}}{2} $$\nThis criterion states that at an optimal boundary $b_j$, the function value $f(b_j)$ must be the arithmetic mean of the conditional expectations of $f(U)$ in the two adjacent strata.\n\nNow, we specialize to the case where $K=2$ and $f(u)=u^2$. There is a single interior boundary, $b \\in (0,1)$. The strata are $[0, b)$ and $[b, 1)$. The optimality criterion becomes:\n$$ f(b) = \\frac{m_1 + m_2}{2} $$\nwhere $m_1 = \\mathbb{E}[f(U) \\mid U \\in [0, b)]$ and $m_2 = \\mathbb{E}[f(U) \\mid U \\in [b, 1)]$. With $f(u)=u^2$, we have $f(b)=b^2$. We compute the conditional means:\n$$ m_1 = \\mathbb{E}[U^2 \\mid U \\in [0, b)] = \\int_0^b u^2 \\frac{1}{b} du = \\frac{1}{b} \\left[ \\frac{u^3}{3} \\right]_0^b = \\frac{b^3}{3b} = \\frac{b^2}{3} $$\n$$ m_2 = \\mathbb{E}[U^2 \\mid U \\in [b, 1)] = \\int_b^1 u^2 \\frac{1}{1-b} du = \\frac{1}{1-b} \\left[ \\frac{u^3}{3} \\right]_b^1 = \\frac{1-b^3}{3(1-b)} $$\nUsing the identity $1-b^3 = (1-b)(1+b+b^2)$, we simplify $m_2$:\n$$ m_2 = \\frac{(1-b)(1+b+b^2)}{3(1-b)} = \\frac{1+b+b^2}{3} $$\nSubstituting these expressions into the optimality criterion:\n$$ b^2 = \\frac{1}{2} \\left(\\frac{b^2}{3} + \\frac{1+b+b^2}{3}\\right) $$\n$$ b^2 = \\frac{1}{6} (b^2 + 1 + b + b^2) $$\n$$ 6b^2 = 2b^2 + b + 1 $$\n$$ 4b^2 - b - 1 = 0 $$\nThis is a quadratic equation for the optimal boundary $b$. We use the quadratic formula to solve for $b$:\n$$ b = \\frac{-(-1) \\pm \\sqrt{(-1)^2 - 4(4)(-1)}}{2(4)} = \\frac{1 \\pm \\sqrt{1 + 16}}{8} = \\frac{1 \\pm \\sqrt{17}}{8} $$\nWe have two solutions: $b_1 = \\frac{1+\\sqrt{17}}{8}$ and $b_2 = \\frac{1-\\sqrt{17}}{8}$.\nThe boundary $b$ must lie in the interval $(0,1)$. We know that $4  \\sqrt{17}  5$.\nFor the first root: $\\frac{1+4}{8}  b_1  \\frac{1+5}{8}$, so $0.625  b_1  0.75$. This root is within the valid interval $(0,1)$.\nFor the second root: $\\frac{1-5}{8}  b_2  \\frac{1-4}{8}$, so $-0.5  b_2  -0.375$. This root is negative and thus is not a valid boundary.\nThe only valid solution for the optimal boundary is $b = \\frac{1+\\sqrt{17}}{8}$.", "answer": "$$\\boxed{\\frac{1+\\sqrt{17}}{8}}$$", "id": "3354809"}]}