{"hands_on_practices": [{"introduction": "Parameter estimation is the cornerstone of applying statistical models to data. This exercise provides fundamental practice by guiding you through the derivation of Maximum Likelihood Estimators (MLEs) for the Bernoulli, Binomial, Poisson, and Geometric distributions from first principles [@problem_id:3296963]. By working through the process of maximizing the likelihood function and calculating the Fisher information, you will gain a deep understanding of how model parameters are inferred and how the precision of these estimates is quantified.", "problem": "A researcher designs a Monte Carlo study to assess estimators for four common discrete distributions. For each distribution, the researcher collects $N$ independent and identically distributed observations and assumes that the model parameters are in the interior of their domains. The distributions and their probability mass functions (pmf) are taken as fundamental definitions:\n\n1. Bernoulli with success probability $p_{\\mathrm{Ber}} \\in (0,1)$: for each observation $X_i \\in \\{0,1\\}$, the pmf is $\\Pr(X_i=x) = p_{\\mathrm{Ber}}^{x} (1 - p_{\\mathrm{Ber}})^{1-x}$ for $x \\in \\{0,1\\}$.\n\n2. Binomial with a known number of trials $m \\in \\{1,2,3,\\dots\\}$ and success probability $p_{\\mathrm{Bin}} \\in (0,1)$: for each observation $Y_i \\in \\{0,1,\\dots,m\\}$, the pmf is $\\Pr(Y_i=y) = \\binom{m}{y} p_{\\mathrm{Bin}}^{y} (1 - p_{\\mathrm{Bin}})^{m-y}$ for $y \\in \\{0,1,\\dots,m\\}$.\n\n3. Poisson with rate $\\lambda \\in (0,\\infty)$: for each observation $Z_i \\in \\{0,1,2,\\dots\\}$, the pmf is $\\Pr(Z_i=z) = \\exp(-\\lambda) \\lambda^{z} / z!$ for $z \\in \\{0,1,2,\\dots\\}$.\n\n4. Geometric with success probability $p_{\\mathrm{Geom}} \\in (0,1)$, supported on $\\{1,2,3,\\dots\\}$ (number of trials until the first success): for each observation $W_i \\in \\{1,2,3,\\dots\\}$, the pmf is $\\Pr(W_i=w) = p_{\\mathrm{Geom}} (1 - p_{\\mathrm{Geom}})^{w-1}$ for $w \\in \\{1,2,3,\\dots\\}$.\n\nUsing only these definitions and the first principles of likelihood-based inference, do the following for each family:\n\n- Derive the Maximum Likelihood Estimator (MLE) of the unknown parameter, where Maximum Likelihood Estimator (MLE) is defined as any parameter value that maximizes the likelihood function constructed from the observed sample.\n- Compute the Fisher information for one observation and hence the asymptotic variance of the MLE based on $N$ observations, using the classical large-sample normality of the MLE that states $\\sqrt{N}(\\hat{\\theta}_N - \\theta) \\xrightarrow{d} \\mathcal{N}(0, I_1(\\theta)^{-1})$, where $I_1(\\theta)$ is the Fisher information for a single observation.\n\nYou must start from the pmf for each model to form the likelihood, derive the score and observed information, and then take expectations as needed to obtain the Fisher information. Denote the samples by $\\{X_i\\}_{i=1}^{N}$, $\\{Y_i\\}_{i=1}^{N}$, $\\{Z_i\\}_{i=1}^{N}$, and $\\{W_i\\}_{i=1}^{N}$. The number of trials $m$ for the binomial model is known and fixed.\n\nExpress your final answer as a single $1 \\times 8$ row matrix in the following order of entries:\n\n$\\left(\\hat{p}_{\\mathrm{Ber}}, \\ \\mathrm{Avar}(\\hat{p}_{\\mathrm{Ber}}), \\ \\hat{p}_{\\mathrm{Bin}}, \\ \\mathrm{Avar}(\\hat{p}_{\\mathrm{Bin}}), \\ \\hat{\\lambda}, \\ \\mathrm{Avar}(\\hat{\\lambda}), \\ \\hat{p}_{\\mathrm{Geom}}, \\ \\mathrm{Avar}(\\hat{p}_{\\mathrm{Geom}})\\right)$,\n\nwhere $\\mathrm{Avar}(\\cdot)$ denotes the asymptotic variance. Your expressions may depend on $N$, $m$, the sample sums $\\sum_{i=1}^{N} X_i$, $\\sum_{i=1}^{N} Y_i$, $\\sum_{i=1}^{N} Z_i$, $\\sum_{i=1}^{N} W_i$, and on the true parameters $p_{\\mathrm{Ber}}$, $p_{\\mathrm{Bin}}$, $\\lambda$, and $p_{\\mathrm{Geom}}$. No numerical rounding is required; present closed-form symbolic expressions only.", "solution": "The problem is valid as it is a standard exercise in maximum likelihood estimation for fundamental discrete probability distributions, resting on well-established principles of mathematical statistics. All definitions and objectives are clear, scientifically sound, and well-posed. We will proceed to derive the Maximum Likelihood Estimator (MLE) and its asymptotic variance for each of the four specified distributions.\n\nThe general procedure for each distribution with parameter $\\theta$ and a sample of $N$ independent and identically distributed (i.i.d.) observations $\\{X_i\\}_{i=1}^{N}$ is as follows:\n1.  Construct the likelihood function $L(\\theta) = \\prod_{i=1}^{N} f(X_i; \\theta)$, where $f(x; \\theta)$ is the probability mass function (pmf).\n2.  Form the log-likelihood function $\\ell(\\theta) = \\ln L(\\theta) = \\sum_{i=1}^{N} \\ln f(X_i; \\theta)$.\n3.  Find the MLE $\\hat{\\theta}$ by solving the score equation $\\frac{d\\ell(\\theta)}{d\\theta} = 0$.\n4.  Calculate the Fisher information for a single observation, $I_1(\\theta) = -E\\left[\\frac{d^2}{d\\theta^2} \\ln f(X; \\theta)\\right]$.\n5.  The asymptotic variance of the MLE, denoted $\\mathrm{Avar}(\\hat{\\theta})$, is given by $\\frac{1}{N I_1(\\theta)}$.\n\nWe now apply this procedure to each of the four distributions.\n\n**1. Bernoulli Distribution**\nThe pmf for a single observation $X \\in \\{0,1\\}$ is $f(x; p_{\\mathrm{Ber}}) = p_{\\mathrm{Ber}}^{x} (1 - p_{\\mathrm{Ber}})^{1-x}$. For simplicity, let $p = p_{\\mathrm{Ber}}$.\nThe sample is $\\{X_i\\}_{i=1}^{N}$. The log-likelihood function is:\n$$ \\ell(p) = \\ln \\left( \\prod_{i=1}^{N} p^{X_i} (1-p)^{1-X_i} \\right) = \\sum_{i=1}^{N} [X_i \\ln(p) + (1-X_i) \\ln(1-p)] = \\left(\\sum_{i=1}^{N} X_i\\right) \\ln(p) + \\left(N - \\sum_{i=1}^{N} X_i\\right) \\ln(1-p) $$\nThe score function is the first derivative of $\\ell(p)$ with respect to $p$:\n$$ \\frac{d\\ell(p)}{dp} = \\frac{\\sum_{i=1}^{N} X_i}{p} - \\frac{N - \\sum_{i=1}^{N} X_i}{1-p} $$\nSetting the score to $0$ and solving for $p$ gives the MLE, $\\hat{p}_{\\mathrm{Ber}}$:\n$$ \\frac{\\sum X_i}{\\hat{p}_{\\mathrm{Ber}}} = \\frac{N - \\sum X_i}{1-\\hat{p}_{\\mathrm{Ber}}} \\implies (1-\\hat{p}_{\\mathrm{Ber}})\\sum X_i = \\hat{p}_{\\mathrm{Ber}}(N - \\sum X_i) \\implies \\sum X_i = N \\hat{p}_{\\mathrm{Ber}} $$\n$$ \\hat{p}_{\\mathrm{Ber}} = \\frac{\\sum_{i=1}^{N} X_i}{N} $$\nTo find the asymptotic variance, we first compute the Fisher information for a single observation. The log-pmf is $\\ln f(x;p) = x \\ln(p) + (1-x) \\ln(1-p)$. The second derivative is:\n$$ \\frac{d^2}{dp^2} \\ln f(x; p) = -\\frac{x}{p^2} - \\frac{1-x}{(1-p)^2} $$\nThe Fisher information $I_1(p)$ is the negative expectation of this quantity. Since $X \\sim \\mathrm{Bernoulli}(p)$, $E[X] = p$.\n$$ I_1(p) = -E\\left[ -\\frac{X}{p^2} - \\frac{1-X}{(1-p)^2} \\right] = \\frac{E[X]}{p^2} + \\frac{1-E[X]}{(1-p)^2} = \\frac{p}{p^2} + \\frac{1-p}{(1-p)^2} = \\frac{1}{p} + \\frac{1}{1-p} = \\frac{1}{p(1-p)} $$\nThe asymptotic variance of $\\hat{p}_{\\mathrm{Ber}}$ is:\n$$ \\mathrm{Avar}(\\hat{p}_{\\mathrm{Ber}}) = \\frac{1}{N I_1(p_{\\mathrm{Ber}})} = \\frac{p_{\\mathrm{Ber}}(1-p_{\\mathrm{Ber}})}{N} $$\n\n**2. Binomial Distribution**\nThe pmf for a single observation $Y \\in \\{0, \\dots, m\\}$ is $f(y; p_{\\mathrm{Bin}}) = \\binom{m}{y} p_{\\mathrm{Bin}}^{y} (1-p_{\\mathrm{Bin}})^{m-y}$. Let $p = p_{\\mathrm{Bin}}$.\nThe sample is $\\{Y_i\\}_{i=1}^{N}$. The log-likelihood function is:\n$$ \\ell(p) = \\ln \\left( \\prod_{i=1}^{N} \\binom{m}{Y_i} p^{Y_i} (1-p)^{m-Y_i} \\right) = \\sum_{i=1}^{N} \\ln\\binom{m}{Y_i} + \\left(\\sum_{i=1}^{N} Y_i\\right) \\ln(p) + \\left(Nm - \\sum_{i=1}^{N} Y_i\\right) \\ln(1-p) $$\nThe score function is:\n$$ \\frac{d\\ell(p)}{dp} = \\frac{\\sum_{i=1}^{N} Y_i}{p} - \\frac{Nm - \\sum_{i=1}^{N} Y_i}{1-p} $$\nSetting the score to $0$ to find the MLE, $\\hat{p}_{\\mathrm{Bin}}$:\n$$ \\frac{\\sum Y_i}{\\hat{p}_{\\mathrm{Bin}}} = \\frac{Nm - \\sum Y_i}{1-\\hat{p}_{\\mathrm{Bin}}} \\implies (1-\\hat{p}_{\\mathrm{Bin}})\\sum Y_i = \\hat{p}_{\\mathrm{Bin}}(Nm - \\sum Y_i) \\implies \\sum Y_i = Nm \\hat{p}_{\\mathrm{Bin}} $$\n$$ \\hat{p}_{\\mathrm{Bin}} = \\frac{\\sum_{i=1}^{N} Y_i}{Nm} $$\nFor the asymptotic variance, the log-pmf for a single observation is $\\ln f(y;p) = \\ln\\binom{m}{y} + y\\ln(p) + (m-y)\\ln(1-p)$. The second derivative is:\n$$ \\frac{d^2}{dp^2} \\ln f(y; p) = -\\frac{y}{p^2} - \\frac{m-y}{(1-p)^2} $$\nThe Fisher information $I_1(p)$ is the negative expectation, where $Y \\sim \\mathrm{Binomial}(m,p)$ so $E[Y] = mp$.\n$$ I_1(p) = -E\\left[ -\\frac{Y}{p^2} - \\frac{m-Y}{(1-p)^2} \\right] = \\frac{E[Y]}{p^2} + \\frac{m-E[Y]}{(1-p)^2} = \\frac{mp}{p^2} + \\frac{m-mp}{(1-p)^2} = \\frac{m}{p} + \\frac{m(1-p)}{(1-p)^2} = \\frac{m}{p(1-p)} $$\nThe asymptotic variance of $\\hat{p}_{\\mathrm{Bin}}$ is:\n$$ \\mathrm{Avar}(\\hat{p}_{\\mathrm{Bin}}) = \\frac{1}{N I_1(p_{\\mathrm{Bin}})} = \\frac{p_{\\mathrm{Bin}}(1-p_{\\mathrm{Bin}})}{Nm} $$\n\n**3. Poisson Distribution**\nThe pmf for a single observation $Z \\in \\{0, 1, 2, \\dots\\}$ is $f(z; \\lambda) = \\frac{\\exp(-\\lambda)\\lambda^z}{z!}$.\nThe sample is $\\{Z_i\\}_{i=1}^{N}$. The log-likelihood function is:\n$$ \\ell(\\lambda) = \\ln \\left( \\prod_{i=1}^{N} \\frac{\\exp(-\\lambda)\\lambda^{Z_i}}{Z_i!} \\right) = \\sum_{i=1}^{N} [-\\lambda + Z_i \\ln(\\lambda) - \\ln(Z_i!)] = -N\\lambda + \\left(\\sum_{i=1}^{N} Z_i\\right) \\ln(\\lambda) - \\sum_{i=1}^{N} \\ln(Z_i!) $$\nThe score function is:\n$$ \\frac{d\\ell(\\lambda)}{d\\lambda} = -N + \\frac{\\sum_{i=1}^{N} Z_i}{\\lambda} $$\nSetting the score to $0$ gives the MLE, $\\hat{\\lambda}$:\n$$ -N + \\frac{\\sum Z_i}{\\hat{\\lambda}} = 0 \\implies \\hat{\\lambda} = \\frac{\\sum_{i=1}^{N} Z_i}{N} $$\nFor the asymptotic variance, the log-pmf is $\\ln f(z; \\lambda) = -\\lambda + z\\ln(\\lambda) - \\ln(z!)$. The second derivative is:\n$$ \\frac{d^2}{d\\lambda^2} \\ln f(z; \\lambda) = -\\frac{z}{\\lambda^2} $$\nThe Fisher information $I_1(\\lambda)$ is the negative expectation, where $Z \\sim \\mathrm{Poisson}(\\lambda)$ so $E[Z] = \\lambda$.\n$$ I_1(\\lambda) = -E\\left[ -\\frac{Z}{\\lambda^2} \\right] = \\frac{E[Z]}{\\lambda^2} = \\frac{\\lambda}{\\lambda^2} = \\frac{1}{\\lambda} $$\nThe asymptotic variance of $\\hat{\\lambda}$ is:\n$$ \\mathrm{Avar}(\\hat{\\lambda}) = \\frac{1}{N I_1(\\lambda)} = \\frac{\\lambda}{N} $$\n\n**4. Geometric Distribution**\nThe pmf for a single observation $W \\in \\{1, 2, 3, \\dots\\}$ is $f(w; p_{\\mathrm{Geom}}) = p_{\\mathrm{Geom}}(1-p_{\\mathrm{Geom}})^{w-1}$. Let $p = p_{\\mathrm{Geom}}$.\nThe sample is $\\{W_i\\}_{i=1}^{N}$. The log-likelihood function is:\n$$ \\ell(p) = \\ln \\left( \\prod_{i=1}^{N} p(1-p)^{W_i-1} \\right) = \\sum_{i=1}^{N} [\\ln(p) + (W_i-1)\\ln(1-p)] = N\\ln(p) + \\left(\\sum_{i=1}^{N} W_i - N\\right)\\ln(1-p) $$\nThe score function is:\n$$ \\frac{d\\ell(p)}{dp} = \\frac{N}{p} - \\frac{\\sum W_i - N}{1-p} $$\nSetting the score to $0$ gives the MLE, $\\hat{p}_{\\mathrm{Geom}}$:\n$$ \\frac{N}{\\hat{p}_{\\mathrm{Geom}}} = \\frac{\\sum W_i - N}{1-\\hat{p}_{\\mathrm{Geom}}} \\implies N(1-\\hat{p}_{\\mathrm{Geom}}) = \\hat{p}_{\\mathrm{Geom}}(\\sum W_i - N) \\implies N = \\hat{p}_{\\mathrm{Geom}}\\sum W_i $$\n$$ \\hat{p}_{\\mathrm{Geom}} = \\frac{N}{\\sum_{i=1}^{N} W_i} $$\nFor the asymptotic variance, the log-pmf is $\\ln f(w;p) = \\ln(p) + (w-1)\\ln(1-p)$. The second derivative is:\n$$ \\frac{d^2}{dp^2} \\ln f(w; p) = -\\frac{1}{p^2} - \\frac{w-1}{(1-p)^2} $$\nThe Fisher information $I_1(p)$ is the negative expectation, where $W \\sim \\mathrm{Geometric}(p)$ with support on $\\{1,2,\\dots\\}$, so $E[W] = 1/p$.\n$$ I_1(p) = -E\\left[ -\\frac{1}{p^2} - \\frac{W-1}{(1-p)^2} \\right] = \\frac{1}{p^2} + \\frac{E[W]-1}{(1-p)^2} = \\frac{1}{p^2} + \\frac{1/p - 1}{(1-p)^2} = \\frac{1}{p^2} + \\frac{(1-p)/p}{(1-p)^2} = \\frac{1}{p^2} + \\frac{1}{p(1-p)} = \\frac{1-p+p}{p^2(1-p)} = \\frac{1}{p^2(1-p)} $$\nThe asymptotic variance of $\\hat{p}_{\\mathrm{Geom}}$ is:\n$$ \\mathrm{Avar}(\\hat{p}_{\\mathrm{Geom}}) = \\frac{1}{N I_1(p_{\\mathrm{Geom}})} = \\frac{p_{\\mathrm{Geom}}^2(1-p_{\\mathrm{Geom}})}{N} $$\n\nThe requested eight quantities are now assembled into a single $1 \\times 8$ row matrix.", "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{\\sum_{i=1}^{N} X_i}{N}  \\frac{p_{\\mathrm{Ber}}(1-p_{\\mathrm{Ber}})}{N}  \\frac{\\sum_{i=1}^{N} Y_i}{Nm}  \\frac{p_{\\mathrm{Bin}}(1-p_{\\mathrm{Bin}})}{Nm}  \\frac{\\sum_{i=1}^{N} Z_i}{N}  \\frac{\\lambda}{N}  \\frac{N}{\\sum_{i=1}^{N} W_i}  \\frac{p_{\\mathrm{Geom}}^{2}(1-p_{\\mathrm{Geom}})}{N} \\end{pmatrix}} $$", "id": "3296963"}, {"introduction": "While direct simulation is useful, its efficiency can often be dramatically improved. This practice introduces one of the most powerful variance reduction techniques, control variates, in a simple yet insightful setting [@problem_id:3296919]. You will derive the optimal way to use a correlated auxiliary variable—the underlying uniform draw—to reduce the variance when estimating the mean of a Bernoulli variable, laying the groundwork for more advanced Monte Carlo applications.", "problem": "Consider a single draw $U$ from the $\\mathrm{Uniform}(0,1)$ distribution and a fixed parameter $p \\in (0,1)$. Define the Bernoulli random variable $X=\\mathbf{1}\\{U \\le p\\}$, and the control variate $g(U)=U$, whose expectation $\\mathbb{E}[g(U)]$ is known. Define the control variate estimator\n$$\nZ_{c} \\;=\\; X \\;-\\; c\\big(g(U)-\\mathbb{E}[g(U)]\\big).\n$$\nStarting from the definitions of expectation, variance, and covariance, and without using any pre-derived control variate formulas, derive the following in closed form as functions of $p$:\n- the optimal control coefficient $c^{\\star}$ that minimizes $\\mathrm{Var}(Z_{c})$;\n- the corresponding variance reduction factor, defined as the ratio $\\mathrm{Var}(Z_{c^{\\star}})/\\mathrm{Var}(X)$.\n\nExpress your final answer as two closed-form analytic expressions in terms of $p$. No numerical rounding is required.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It is a standard problem in the field of Monte Carlo methods that can be solved from first principles as requested.\n\nThe control variate estimator is given by\n$$\nZ_{c} \\;=\\; X \\;-\\; c\\big(g(U)-\\mathbb{E}[g(U)]\\big)\n$$\nwhere $X = \\mathbf{1}\\{U \\le p\\}$ with $U \\sim \\mathrm{Uniform}(0,1)$, $p \\in (0,1)$, and the control variate is $g(U)=U$. Our objective is to find the value of the coefficient $c$, denoted $c^{\\star}$, that minimizes the variance of $Z_{c}$, and then to find the corresponding variance reduction factor.\n\nFirst, we express the variance of $Z_{c}$ using the properties of the variance operator. Let $Y = g(U) = U$. The estimator is $Z_c = X - c(Y - \\mathbb{E}[Y])$.\n$$\n\\mathrm{Var}(Z_c) = \\mathrm{Var}\\left(X - c(Y - \\mathbb{E}[Y])\\right)\n$$\nUsing the identity $\\mathrm{Var}(A - B) = \\mathrm{Var}(A) + \\mathrm{Var}(B) - 2\\mathrm{Cov}(A, B)$, we have:\n$$\n\\mathrm{Var}(Z_c) = \\mathrm{Var}(X) + \\mathrm{Var}\\left(c(Y - \\mathbb{E}[Y])\\right) - 2\\mathrm{Cov}\\left(X, c(Y - \\mathbb{E}[Y])\\right)\n$$\nUsing the properties $\\mathrm{Var}(aZ) = a^2\\mathrm{Var}(Z)$ and $\\mathrm{Cov}(Z_1, aZ_2) = a\\mathrm{Cov}(Z_1, Z_2)$, this becomes:\n$$\n\\mathrm{Var}(Z_c) = \\mathrm{Var}(X) + c^2\\mathrm{Var}(Y - \\mathbb{E}[Y]) - 2c\\mathrm{Cov}(X, Y - \\mathbb{E}[Y])\n$$\nSince $\\mathrm{Var}(Y - k) = \\mathrm{Var}(Y)$ and $\\mathrm{Cov}(X, Y - k) = \\mathrm{Cov}(X,Y)$ for any constant $k$, the expression simplifies to:\n$$\n\\mathrm{Var}(Z_c) = \\mathrm{Var}(X) + c^2\\mathrm{Var}(Y) - 2c\\mathrm{Cov}(X, Y)\n$$\nThis is a quadratic function of $c$. To find the minimum, we take the derivative with respect to $c$ and set it to $0$:\n$$\n\\frac{d}{dc}\\mathrm{Var}(Z_c) = 2c\\mathrm{Var}(Y) - 2\\mathrm{Cov}(X, Y)\n$$\nSetting the derivative to $0$ for the optimal coefficient $c^{\\star}$:\n$$\n2c^{\\star}\\mathrm{Var}(Y) - 2\\mathrm{Cov}(X, Y) = 0 \\implies c^{\\star} = \\frac{\\mathrm{Cov}(X, Y)}{\\mathrm{Var}(Y)}\n$$\nTo find the explicit value of $c^{\\star}$, we must compute $\\mathrm{Var}(Y)$ and $\\mathrm{Cov}(X, Y)$.\n\n1.  **Moments of the control variate** $Y=U$:\n    Since $U \\sim \\mathrm{Uniform}(0,1)$, its probability density function is $f_U(u) = 1$ for $u \\in [0,1]$.\n    The expectation is $\\mathbb{E}[Y] = \\mathbb{E}[U] = \\int_0^1 u \\cdot 1 \\,du = \\frac{1}{2}$.\n    The variance is $\\mathrm{Var}(Y) = \\mathrm{Var}(U) = \\mathbb{E}[U^2] - (\\mathbb{E}[U])^2$.\n    $\\mathbb{E}[U^2] = \\int_0^1 u^2 \\cdot 1 \\,du = \\frac{1}{3}$.\n    So, $\\mathrm{Var}(Y) = \\frac{1}{3} - (\\frac{1}{2})^2 = \\frac{1}{3} - \\frac{1}{4} = \\frac{1}{12}$.\n\n2.  **Moments of the estimator** $X = \\mathbf{1}\\{U \\le p\\}$:\n    $X$ is a Bernoulli random variable, since it takes the value $1$ if the event $\\{U \\le p\\}$ occurs and $0$ otherwise.\n    The probability of success is $\\mathbb{P}(X=1) = \\mathbb{P}(U \\le p) = \\int_0^p 1 \\,du = p$.\n    Thus, $X \\sim \\mathrm{Bernoulli}(p)$.\n    The expectation is $\\mathbb{E}[X] = p$.\n    The variance is $\\mathrm{Var}(X) = p(1-p)$.\n\n3.  **Covariance between** $X$ **and** $Y$:\n    $\\mathrm{Cov}(X, Y) = \\mathrm{Cov}(X, U) = \\mathbb{E}[XU] - \\mathbb{E}[X]\\mathbb{E}[U]$.\n    We need to compute $\\mathbb{E}[XU] = \\mathbb{E}[\\mathbf{1}\\{U \\le p\\} \\cdot U]$.\n    Using the law of the unconscious statistician:\n    $$\n    \\mathbb{E}[XU] = \\int_0^1 (\\mathbf{1}\\{u \\le p\\} \\cdot u) f_U(u) \\,du = \\int_0^1 \\mathbf{1}\\{u \\le p\\} \\cdot u \\,du = \\int_0^p u \\,du\n    $$\n    $$\n    \\mathbb{E}[XU] = \\left[\\frac{u^2}{2}\\right]_0^p = \\frac{p^2}{2}\n    $$\n    Now, we can compute the covariance:\n    $$\n    \\mathrm{Cov}(X, Y) = \\frac{p^2}{2} - (p)\\left(\\frac{1}{2}\\right) = \\frac{p^2 - p}{2} = -\\frac{p(1-p)}{2}\n    $$\n\n4.  **Optimal coefficient** $c^{\\star}$:\n    Substituting the calculated variance and covariance into the expression for $c^{\\star}$:\n    $$\n    c^{\\star} = \\frac{\\mathrm{Cov}(X, Y)}{\\mathrm{Var}(Y)} = \\frac{-p(1-p)/2}{1/12} = -\\frac{p(1-p)}{2} \\cdot 12 = -6p(1-p)\n    $$\n    This is the first required expression.\n\nNext, we derive the variance reduction factor, $\\mathrm{Var}(Z_{c^{\\star}})/\\mathrm{Var}(X)$.\nFirst, we find the minimal variance $\\mathrm{Var}(Z_{c^{\\star}})$ by substituting $c^{\\star}$ back into the variance formula:\n$$\n\\mathrm{Var}(Z_{c^{\\star}}) = \\mathrm{Var}(X) + (c^{\\star})^2\\mathrm{Var}(Y) - 2c^{\\star}\\mathrm{Cov}(X, Y)\n$$\n$$\n\\mathrm{Var}(Z_{c^{\\star}}) = \\mathrm{Var}(X) + \\left(\\frac{\\mathrm{Cov}(X, Y)}{\\mathrm{Var}(Y)}\\right)^2\\mathrm{Var}(Y) - 2\\frac{\\mathrm{Cov}(X, Y)}{\\mathrm{Var}(Y)}\\mathrm{Cov}(X, Y)\n$$\n$$\n\\mathrm{Var}(Z_{c^{\\star}}) = \\mathrm{Var}(X) - \\frac{\\mathrm{Cov}(X, Y)^2}{\\mathrm{Var}(Y)}\n$$\nThis result can be expressed in terms of the squared correlation coefficient, $\\rho_{XY}^2 = \\frac{\\mathrm{Cov}(X,Y)^2}{\\mathrm{Var}(X)\\mathrm{Var}(Y)}$.\n$$\n\\mathrm{Var}(Z_{c^{\\star}}) = \\mathrm{Var}(X) - \\frac{\\rho_{XY}^2 \\mathrm{Var}(X)\\mathrm{Var}(Y)}{\\mathrm{Var}(Y)} = \\mathrm{Var}(X)(1 - \\rho_{XY}^2)\n$$\nThe variance reduction factor is therefore:\n$$\n\\frac{\\mathrm{Var}(Z_{c^{\\star}})}{\\mathrm{Var}(X)} = 1 - \\rho_{XY}^2\n$$\nNow we compute $\\rho_{XY}^2$ using our previously derived quantities:\n$$\n\\rho_{XY}^2 = \\frac{\\mathrm{Cov}(X, Y)^2}{\\mathrm{Var}(X)\\mathrm{Var}(Y)} = \\frac{\\left(-\\frac{p(1-p)}{2}\\right)^2}{p(1-p) \\cdot \\frac{1}{12}} = \\frac{\\frac{p^2(1-p)^2}{4}}{\\frac{p(1-p)}{12}}\n$$\nAssuming $p \\in (0,1)$, we have $p \\neq 0$ and $1-p \\neq 0$, so we can simplify:\n$$\n\\rho_{XY}^2 = \\frac{p^2(1-p)^2}{4} \\cdot \\frac{12}{p(1-p)} = 3p(1-p)\n$$\nFinally, the variance reduction factor is:\n$$\n\\frac{\\mathrm{Var}(Z_{c^{\\star}})}{\\mathrm{Var}(X)} = 1 - 3p(1-p)\n$$\nNote that since $p \\in (0,1)$, the term $p(1-p)$ has a maximum value of $1/4$ at $p=1/2$. Therefore, $0  \\rho_{XY}^2 \\le 3/4$, which ensures that $1/4 \\le 1 - \\rho_{XY}^2  1$, confirming a strict reduction in variance.\n\nThe two closed-form expressions are $c^{\\star} = -6p(1-p)$ and the variance reduction factor $1 - 3p(1-p)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} -6p(1-p)  1 - 3p(1-p) \\end{pmatrix}}\n$$", "id": "3296919"}, {"introduction": "Estimating the probability of rare events poses a significant challenge for standard Monte Carlo methods, which may require an astronomical number of trials to observe the event even once. This advanced practice explores importance sampling, a technique designed to overcome this very problem by simulating from a \"tilted\" distribution where the rare event is more common [@problem_id:3296986]. You will apply the principle of exponential tilting to construct an efficient estimator for a binomial tail probability, a classic problem with applications ranging from communication theory to risk management.", "problem": "Consider independent and identically distributed Bernoulli random variables $X_{1},\\dots,X_{n}$ with parameter $p\\in(0,1)$, so that $\\mathbb{P}(X_{i}=1)=p$ and $\\mathbb{P}(X_{i}=0)=1-p$. Let $S_{n}=\\sum_{i=1}^{n}X_{i}$ denote the binomial count with parameters $n$ and $p$. Fix an integer threshold $a\\in\\{0,1,\\dots,n\\}$ satisfying $anp$ so that the upper tail probability $\\mathbb{P}(S_{n}\\ge a)$ is rare. The task is to construct an Importance Sampling (IS) scheme based on exponential tilting to estimate $\\mathbb{P}(S_{n}\\ge a)$ efficiently.\n\nStarting only from the following foundational elements:\n- the definition of Bernoulli and binomial distributions,\n- the definition of Monte Carlo (MC) estimation and Importance Sampling (IS) using the Radon–Nikodym derivative (likelihood ratio),\n- the moment generating function (MGF) and the cumulant generating function (CGF) for sums of independent random variables,\n\nderive an exponential tilting change of measure that replaces $p$ by a new parameter $p^{\\star}\\in(0,1)$, under which samples are drawn from an independent and identically distributed Bernoulli model with parameter $p^{\\star}$, and the IS estimator uses the corresponding likelihood ratio to remain unbiased for $\\mathbb{P}(S_{n}\\ge a)$. Explicitly:\n\n- Derive the likelihood ratio as a function of the realized count $s$ (where $s=S_{n}$ for a given sample), $n$, $p$, and $p^{\\star}$.\n- Using the cumulant generating function and the exponential tilting principle, derive the choice of $p^{\\star}$ that makes the tilted mean of $S_{n}$ equal to the threshold $a$, which asymptotically minimizes the variance growth rate for estimating $\\mathbb{P}(S_{n}\\ge a)$ in the class of independent and identically distributed Bernoulli tilts.\n\nYour final answer must be a single closed-form analytic expression. Express your final answer as the row matrix $\\begin{pmatrix}p^{\\star}  L(s)\\end{pmatrix}$, where $p^{\\star}$ is given in terms of $n$ and $a$, and $L(s)$ is the likelihood ratio expressed solely in terms of $n$, $p$, $a$, and $s$. No rounding is required, and no physical units are involved.", "solution": "The problem is a valid and standard application of Importance Sampling (IS) theory, specifically using exponential tilting to efficiently estimate a rare event probability for a sum of independent and identically distributed random variables. The objective is to derive the optimal change of measure and the corresponding likelihood ratio from first principles.\n\nLet $\\mathbb{P}_{p}$ denote the original probability measure, under which $X_i \\sim \\mathrm{Bernoulli}(p)$ are independent. The sum $S_n = \\sum_{i=1}^n X_i$ follows a $\\mathrm{Binomial}(n,p)$ distribution. We want to estimate $P_a = \\mathbb{P}_{p}(S_{n} \\ge a)$.\n\nImportance Sampling changes the measure to $\\mathbb{P}_{p^{\\star}}$, under which $X_i \\sim \\mathrm{Bernoulli}(p^{\\star})$ are independent. The IS estimator for $P_a$ is $\\mathbb{E}_{p^{\\star}}[L(S_n)\\mathbf{1}_{\\{S_n \\ge a\\}}]$, where $L(S_n)$ is the likelihood ratio.\n\n**1. Derive the Likelihood Ratio**\n\nThe probability of observing a specific sequence of outcomes $\\mathbf{x} = (x_1, \\dots, x_n)$ with sum $s = \\sum x_i$ is $f(\\mathbf{x}; p) = p^s(1-p)^{n-s}$ under $\\mathbb{P}_p$, and $f(\\mathbf{x}; p^{\\star}) = (p^{\\star})^s(1-p^{\\star})^{n-s}$ under $\\mathbb{P}_{p^{\\star}}$. The likelihood ratio $L(s)$ depends only on the sum $s$:\n$$ L(s) = \\frac{f(\\mathbf{x}; p)}{f(\\mathbf{x}; p^{\\star})} = \\frac{p^{s}(1-p)^{n-s}}{(p^{\\star})^{s}(1-p^{\\star})^{n-s}} = \\left(\\frac{p}{p^{\\star}}\\right)^{s} \\left(\\frac{1-p}{1-p^{\\star}}\\right)^{n-s} $$\nThis expression for $L(s)$ is valid for any choice of $p^{\\star}$.\n\n**2. Derive the Optimal `p^{\\star}`**\n\nThe problem directs us to use exponential tilting and choose $p^{\\star}$ such that the mean of $S_n$ under the new measure, $\\mathbb{E}_{p^{\\star}}[S_n]$, equals the threshold $a$. This choice is known to be asymptotically optimal for minimizing the variance of the estimator in the large deviations sense.\n\nThe cumulant generating function (CGF) of a single Bernoulli($p$) variable $X$ is $K_X(\\theta) = \\ln(\\mathbb{E}_p[e^{\\theta X}]) = \\ln(pe^\\theta + 1-p)$.\nFor the sum $S_n$, the CGF is $K_{S_n}(\\theta) = nK_X(\\theta) = n\\ln(pe^\\theta + 1-p)$.\n\nExponential tilting with parameter $\\theta$ creates a new probability measure under which the probability of success $p^{\\star}$ is given by:\n$$ p^{\\star} = \\frac{e^{\\theta \\cdot 1} \\mathbb{P}_p(X=1)}{\\mathbb{E}_p[e^{\\theta X}]} = \\frac{pe^\\theta}{pe^\\theta + 1-p} $$\nThe mean of $S_n$ under this new measure is given by the derivative of the CGF:\n$$ \\mathbb{E}_{p^{\\star}}[S_n] = \\frac{d}{d\\theta} K_{S_n}(\\theta) = n \\frac{pe^\\theta}{pe^\\theta + 1-p} = np^{\\star} $$\nWe set this mean to the threshold $a$:\n$$ np^{\\star} = a \\implies p^{\\star} = \\frac{a}{n} $$\nThis is the required expression for $p^{\\star}$.\n\n**3. Finalize the Likelihood Ratio**\n\nSubstituting $p^{\\star} = a/n$ into our general expression for $L(s)$:\n$$ L(s) = \\left(\\frac{p}{a/n}\\right)^{s} \\left(\\frac{1-p}{1-a/n}\\right)^{n-s} = \\left(\\frac{np}{a}\\right)^{s} \\left(\\frac{1-p}{(n-a)/n}\\right)^{n-s} = \\left(\\frac{np}{a}\\right)^{s} \\left(\\frac{n(1-p)}{n-a}\\right)^{n-s} $$\nThis is the final form of the likelihood ratio, expressed in terms of the specified variables. The final answer combines $p^{\\star}$ and $L(s)$ into the requested matrix format.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{a}{n}  \\left(\\frac{np}{a}\\right)^{s} \\left(\\frac{n(1-p)}{n-a}\\right)^{n-s}\n\\end{pmatrix}\n}\n$$", "id": "3296986"}]}