{"hands_on_practices": [{"introduction": "One of the most elegant variance reduction techniques in Monte Carlo simulation is the method of antithetic variates, which strategically introduces negative correlation to improve estimation efficiency. While this method is famously effective for monotone functions, its performance can degrade or even reverse for more complex integrands. This exercise [@problem_id:3300839] challenges you to analyze this breakdown by dissecting the covariance structure of an estimator for a non-monotone function, revealing the precise point where the desired negative correlation is overwhelmed.", "problem": "Consider Monte Carlo (MC) estimation of the integral $I = \\mathbb{E}[g(U)]$ where $U \\sim \\mathrm{Uniform}(0,1)$ and $g$ is an integrand modeled as a monotone baseline plus a non-monotone perturbation. Use the antithetic variates technique, which forms pairs $(U,1-U)$ to construct correlated estimates of $g(U)$ and $g(1-U)$ designed for variance reduction. Define the antithetic pair\n$$\nX = g(U), \\qquad X' = g(1-U),\n$$\nwith the parametric integrand\n$$\ng(u) = u + a \\cos(2\\pi u),\n$$\nwhere $a \\in \\mathbb{R}$ controls the amplitude of a symmetric, non-monotone oscillation about the monotone baseline $u$.\n\nStarting from the fundamental definitions of covariance and correlation,\n$$\n\\mathrm{Cov}(X,Y) = \\mathbb{E}\\big[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])\\big], \\qquad \\mathrm{Corr}(X,Y) = \\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)\\,\\mathrm{Var}(Y)}},\n$$\nderive the correlation $\\mathrm{Corr}(X,X')$ as a function of $a$ and determine the precise condition under which the sign of $\\mathrm{Corr}(X,X')$ flips from negative to positive due to the non-monotonicity induced by the perturbation $\\cos(2\\pi u)$. Provide the closed-form analytic expression for the critical amplitude $a_{\\star}$ at which $\\mathrm{Corr}(X,X') = 0$.\n\nExpress your final answer as an exact symbolic expression. No rounding is required. No units are required.", "solution": "We begin with the antithetic construction: for $U \\sim \\mathrm{Uniform}(0,1)$, define\n$$\nX = g(U) = U + a \\cos(2\\pi U), \\qquad X' = g(1-U) = (1-U) + a \\cos\\big(2\\pi(1-U)\\big).\n$$\nUsing the trigonometric identity $\\cos(2\\pi(1-U)) = \\cos(2\\pi U)$, we have the shared perturbation term\n$$\nX' = (1-U) + a \\cos(2\\pi U).\n$$\nThis illustrates a key feature: the baseline components $U$ and $1-U$ are perfectly negatively correlated, while the non-monotone oscillatory components coincide exactly for $X$ and $X'$, introducing positive dependence. The net correlation is thus determined by the competition between these two effects.\n\nWe will compute $\\mathrm{Cov}(X,X')$ and $\\mathrm{Var}(X)$ using fundamental definitions. Because $X$ and $X'$ are affine in $U$ and $\\cos(2\\pi U)$, it is convenient to use bilinearity of covariance and the following expectations under $U \\sim \\mathrm{Uniform}(0,1)$:\n- $\\mathbb{E}[U] = \\frac{1}{2}$ and $\\mathrm{Var}(U) = \\frac{1}{12}$.\n- $\\mathbb{E}[\\cos(2\\pi U)] = \\int_{0}^{1} \\cos(2\\pi u)\\,\\mathrm{d}u = \\frac{\\sin(2\\pi u)}{2\\pi}\\Big|_{0}^{1} = 0$.\n- $\\mathbb{E}[\\cos^{2}(2\\pi U)] = \\int_{0}^{1} \\cos^{2}(2\\pi u)\\,\\mathrm{d}u = \\frac{1}{2}$, so $\\mathrm{Var}(\\cos(2\\pi U)) = \\frac{1}{2}$.\n- $\\mathrm{Cov}(U,\\cos(2\\pi U)) = \\mathbb{E}[U \\cos(2\\pi U)] - \\mathbb{E}[U]\\mathbb{E}[\\cos(2\\pi U)] = \\int_{0}^{1} u \\cos(2\\pi u)\\,\\mathrm{d}u.$\n\nWe compute the last integral by integration by parts. Let $f(u) = u$ and $g'(u) = \\cos(2\\pi u)$, so $g(u) = \\frac{\\sin(2\\pi u)}{2\\pi}$. Then\n$$\n\\int_{0}^{1} u \\cos(2\\pi u)\\,\\mathrm{d}u = \\left[ u \\cdot \\frac{\\sin(2\\pi u)}{2\\pi} \\right]_{0}^{1} - \\int_{0}^{1} 1 \\cdot \\frac{\\sin(2\\pi u)}{2\\pi}\\,\\mathrm{d}u = 0 - \\frac{1}{2\\pi} \\int_{0}^{1} \\sin(2\\pi u)\\,\\mathrm{d}u.\n$$\nSince $\\int_{0}^{1} \\sin(2\\pi u)\\,\\mathrm{d}u = \\left[-\\frac{\\cos(2\\pi u)}{2\\pi}\\right]_{0}^{1} = -\\frac{\\cos(2\\pi) - \\cos(0)}{2\\pi} = 0$, it follows that\n$$\n\\mathrm{Cov}(U,\\cos(2\\pi U)) = 0.\n$$\n\nNext, compute $\\mathrm{Cov}(X,X')$:\n\n$$\n\\begin{aligned}\n\\mathrm{Cov}(X,X') = \\mathrm{Cov}\\big(U + a \\cos(2\\pi U),\\, 1 - U + a \\cos(2\\pi U)\\big) \\\\\n= \\mathrm{Cov}(U,1-U) + a\\,\\mathrm{Cov}(U,\\cos(2\\pi U)) + a\\,\\mathrm{Cov}(\\cos(2\\pi U),1-U) + a^{2}\\,\\mathrm{Cov}(\\cos(2\\pi U),\\cos(2\\pi U)).\n\\end{aligned}\n$$\n\nWe have $\\mathrm{Cov}(U,1-U) = \\mathrm{Cov}(U,1) - \\mathrm{Cov}(U,U) = 0 - \\mathrm{Var}(U) = -\\frac{1}{12}$, and $\\mathrm{Cov}(U,\\cos(2\\pi U)) = 0$. Also,\n$$\n\\mathrm{Cov}(\\cos(2\\pi U),1-U) = \\mathrm{Cov}(\\cos(2\\pi U),1) - \\mathrm{Cov}(\\cos(2\\pi U),U) = 0 - \\mathrm{Cov}(U,\\cos(2\\pi U)) = 0,\n$$\nand $\\mathrm{Cov}(\\cos(2\\pi U),\\cos(2\\pi U)) = \\mathrm{Var}(\\cos(2\\pi U)) = \\frac{1}{2}$. Hence\n$$\n\\mathrm{Cov}(X,X') = -\\frac{1}{12} + a^{2}\\cdot\\frac{1}{2} = \\frac{a^{2}}{2} - \\frac{1}{12}.\n$$\n\nNow compute $\\mathrm{Var}(X)$ and $\\mathrm{Var}(X')$. Since adding or subtracting a constant does not affect variance and using independence properties is not applicable here (note $U$ and $1-U$ are perfectly dependent), we use bilinearity with the previously computed covariances:\n\n$$\n\\begin{aligned}\n\\mathrm{Var}(X) = \\mathrm{Var}\\big(U + a \\cos(2\\pi U)\\big) \\\\\n= \\mathrm{Var}(U) + a^{2}\\mathrm{Var}(\\cos(2\\pi U)) + 2a\\,\\mathrm{Cov}(U,\\cos(2\\pi U)) \\\\\n= \\frac{1}{12} + a^{2}\\cdot\\frac{1}{2}.\n\\end{aligned}\n$$\n\nSimilarly,\n$$\n\\mathrm{Var}(X') = \\mathrm{Var}\\big(1 - U + a \\cos(2\\pi U)\\big) = \\mathrm{Var}(-U + a \\cos(2\\pi U)) = \\mathrm{Var}(U) + a^{2}\\mathrm{Var}(\\cos(2\\pi U)) = \\frac{1}{12} + a^{2}\\cdot\\frac{1}{2}.\n$$\n\nTherefore, the correlation is\n\n$$\n\\mathrm{Corr}(X,X') = \\frac{\\mathrm{Cov}(X,X')}{\\sqrt{\\mathrm{Var}(X)\\mathrm{Var}(X')}} = \\frac{\\frac{a^{2}}{2} - \\frac{1}{12}}{\\frac{1}{12} + \\frac{a^{2}}{2}}.\n$$\n\nThe denominator is strictly positive for all real $a$. The sign of $\\mathrm{Corr}(X,X')$ is thus determined by the numerator $\\frac{a^{2}}{2} - \\frac{1}{12}$. The correlation flips sign exactly when\n$$\n\\frac{a^{2}}{2} - \\frac{1}{12} = 0 \\quad \\Longleftrightarrow \\quad a^{2} = \\frac{1}{6}.\n$$\nThus, the critical amplitude is\n$$\na_{\\star} = \\sqrt{\\frac{1}{6}}.\n$$\nFor $|a|  a_{\\star}$, the negative correlation from the monotone baseline dominates, and $\\mathrm{Corr}(X,X')  0$, enabling variance reduction via antithetic variates. For $|a|  a_{\\star}$, the positive correlation induced by the shared non-monotone oscillation dominates, leading to $\\mathrm{Corr}(X,X')  0$ and undermining variance reduction.\n\nThis completes the derivation from the foundational definitions of covariance and correlation, incorporating the effect of non-monotonicity on antithetic dependence structure.", "answer": "$$\\boxed{\\sqrt{\\frac{1}{6}}}$$", "id": "3300839"}, {"introduction": "Beyond inducing negative correlation, we can use our understanding of covariance to actively optimize estimators by introducing a control variate—a zero-mean random variable correlated with our estimator. This practice [@problem_id:3300811] delves into this principle in the context of stochastic gradient estimation, a cornerstone of modern machine learning, by tasking you with deriving the variance-minimizing baseline for the score-function estimator. By solving for the optimal baseline, you will see how covariance provides a direct mathematical tool for designing more efficient estimators.", "problem": "Let $X$ be a real-valued random variable with density $p_{\\theta}(x)$ depending smoothly on a scalar parameter $\\theta \\in \\mathbb{R}$. For any integrable function $f_{\\theta}:\\mathbb{R} \\to \\mathbb{R}$ depending smoothly on $\\theta$, define the score function $s_{\\theta}(x) \\equiv \\nabla_{\\theta} \\ln p_{\\theta}(x)$. Assume all regularity conditions required for interchanging differentiation and integration hold.\n\nConsider two unbiased Monte Carlo gradient estimators of $\\nabla_{\\theta} \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X)]$:\n\n$1.$ The score-function estimator with a constant baseline $b_{\\theta} \\in \\mathbb{R}$ that does not depend on $X$:\n$$g_{\\mathrm{SF}}(X;b_{\\theta}) \\equiv \\big(f_{\\theta}(X) - b_{\\theta}\\big)\\, s_{\\theta}(X) + \\nabla_{\\theta} f_{\\theta}(X).$$\n\n$2.$ The pathwise (reparameterization) estimator under a differentiable transformation $X = T_{\\theta}(\\varepsilon)$ where $\\varepsilon$ has a $\\theta$-independent density $q(\\varepsilon)$:\n$$g_{\\mathrm{PW}}(\\varepsilon) \\equiv \\nabla_{\\theta} f_{\\theta}\\!\\big(T_{\\theta}(\\varepsilon)\\big).$$\n\nYou will analyze variance reduction via a learned constant baseline $b_{\\theta}$ through covariance and derive the variance-minimizing choice.\n\nWork with the concrete, scientifically consistent specialization: $X \\sim \\mathcal{N}(\\theta,\\sigma^{2})$ with known $\\sigma  0$, and\n$$f_{\\theta}(x) \\equiv x^{2} + \\theta x.$$\nFor this model, the pathwise map is $T_{\\theta}(\\varepsilon) \\equiv \\theta + \\sigma \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0,1)$.\n\nTasks:\n\n$1.$ Using only foundational identities about expectation, covariance, and the score function, prove that $g_{\\mathrm{SF}}(X;b_{\\theta})$ is unbiased for any constant $b_{\\theta}$ and derive an explicit expression for the variance-minimizing baseline $b_{\\theta}^{\\star}$ in terms of covariances and variances involving $s_{\\theta}(X)$, $f_{\\theta}(X)$, and $\\nabla_{\\theta} f_{\\theta}(X)$.\n\n$2.$ Specialize your expression to the given Gaussian model and compute the closed-form analytic expression for $b_{\\theta}^{\\star}$ as a function of $\\theta$ and $\\sigma$.\n\n$3.$ For the pathwise estimator $g_{\\mathrm{PW}}(\\varepsilon)$ in this Gaussian model, analyze whether a constant baseline $b_{\\theta}$ that is independent of $\\varepsilon$ can reduce the variance without introducing bias. Relate your conclusion to the covariance structure between $g_{\\mathrm{PW}}(\\varepsilon)$ and any zero-mean control variate. You do not need to produce a numerical value here, only a logically complete argument grounded in covariance.\n\nProvide as your final answer the closed-form analytic expression for the optimal baseline $b_{\\theta}^{\\star}$ from Task $2$. No rounding is required. The final answer must be a single symbolic expression.", "solution": "### Task 1: Analysis of the Score-Function Estimator\n\nThe score-function estimator is given by\n$$g_{\\mathrm{SF}}(X;b_{\\theta}) \\equiv \\big(f_{\\theta}(X) - b_{\\theta}\\big)\\, s_{\\theta}(X) + \\nabla_{\\theta} f_{\\theta}(X),$$\nwhere $s_{\\theta}(x) = \\nabla_{\\theta} \\ln p_{\\theta}(x)$ is the score function and $b_{\\theta}$ is a constant baseline with respect to $X$.\n\nFirst, we prove that $g_{\\mathrm{SF}}(X;b_{\\theta})$ is an unbiased estimator of $\\nabla_{\\theta} \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X)]$ for any $b_{\\theta}$.\nThe target quantity is the gradient of the expectation:\n$$ \\nabla_{\\theta} \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X)] = \\nabla_{\\theta} \\int f_{\\theta}(x) p_{\\theta}(x) \\,dx. $$\nUsing the product rule for differentiation and the assumption that we can interchange differentiation and integration:\n$$ \\nabla_{\\theta} \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X)] = \\int \\left( (\\nabla_{\\theta} f_{\\theta}(x)) p_{\\theta}(x) + f_{\\theta}(x) (\\nabla_{\\theta} p_{\\theta}(x)) \\right) \\,dx. $$\nBy rewriting $\\nabla_{\\theta} p_{\\theta}(x) = (\\frac{\\nabla_{\\theta} p_{\\theta}(x)}{p_{\\theta}(x)}) p_{\\theta}(x) = s_{\\theta}(x) p_{\\theta}(x)$, we get:\n$$ \\nabla_{\\theta} \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X)] = \\int (\\nabla_{\\theta} f_{\\theta}(x)) p_{\\theta}(x) \\,dx + \\int f_{\\theta}(x) s_{\\theta}(x) p_{\\theta}(x) \\,dx. $$\nThis is equivalent to the expectation:\n$$ \\nabla_{\\theta} \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X)] = \\mathbb{E}_{p_{\\theta}}[\\nabla_{\\theta} f_{\\theta}(X)] + \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X) s_{\\theta}(X)]. $$\nNow, let's compute the expectation of the estimator $g_{\\mathrm{SF}}(X;b_{\\theta})$:\n$$ \\mathbb{E}_{p_{\\theta}}[g_{\\mathrm{SF}}(X;b_{\\theta})] = \\mathbb{E}_{p_{\\theta}}[\\big(f_{\\theta}(X) - b_{\\theta}\\big)\\, s_{\\theta}(X) + \\nabla_{\\theta} f_{\\theta}(X)]. $$\nBy linearity of expectation:\n$$ \\mathbb{E}_{p_{\\theta}}[g_{\\mathrm{SF}}(X;b_{\\theta})] = \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X) s_{\\theta}(X)] - b_{\\theta} \\mathbb{E}_{p_{\\theta}}[s_{\\theta}(X)] + \\mathbb{E}_{p_{\\theta}}[\\nabla_{\\theta} f_{\\theta}(X)]. $$\nA fundamental property of the score function is that its expectation is zero, provided regularity conditions hold:\n$$ \\mathbb{E}_{p_{\\theta}}[s_{\\theta}(X)] = \\int s_{\\theta}(x) p_{\\theta}(x) \\,dx = \\int \\frac{\\nabla_{\\theta} p_{\\theta}(x)}{p_{\\theta}(x)} p_{\\theta}(x) \\,dx = \\int \\nabla_{\\theta} p_{\\theta}(x) \\,dx = \\nabla_{\\theta} \\int p_{\\theta}(x) \\,dx = \\nabla_{\\theta}(1) = 0. $$\nSubstituting this result, we find:\n$$ \\mathbb{E}_{p_{\\theta}}[g_{\\mathrm{SF}}(X;b_{\\theta})] = \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X) s_{\\theta}(X)] - b_{\\theta} \\cdot 0 + \\mathbb{E}_{p_{\\theta}}[\\nabla_{\\theta} f_{\\theta}(X)] = \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X) s_{\\theta}(X)] + \\mathbb{E}_{p_{\\theta}}[\\nabla_{\\theta} f_{\\theta}(X)]. $$\nThis matches the expression for the true gradient, proving that $g_{\\mathrm{SF}}(X;b_{\\theta})$ is an unbiased estimator for any constant baseline $b_{\\theta}$.\n\nNext, we derive the variance-minimizing baseline, $b_{\\theta}^{\\star}$. We want to minimize $\\mathrm{Var}(g_{\\mathrm{SF}}(X;b_{\\theta}))$. Let $G_{0}(X) = f_{\\theta}(X) s_{\\theta}(X) + \\nabla_{\\theta} f_{\\theta}(X)$. The estimator is $g_{\\mathrm{SF}}(X;b_{\\theta}) = G_{0}(X) - b_{\\theta} s_{\\theta}(X)$.\nThe problem is to find the coefficient $b_{\\theta}$ that minimizes the variance of $G_0 - b_{\\theta} C$, where $C=s_{\\theta}(X)$ is a control variate with mean $\\mathbb{E}[C]=0$. The variance is:\n$$ \\mathrm{Var}(G_0 - b_{\\theta} s_{\\theta}(X)) = \\mathrm{Var}(G_0) + b_{\\theta}^2 \\mathrm{Var}(s_{\\theta}(X)) - 2 b_{\\theta} \\mathrm{Cov}(G_0, s_{\\theta}(X)). $$\nThis is a quadratic function of $b_{\\theta}$. To find the minimum, we differentiate with respect to $b_{\\theta}$ and set the result to zero:\n$$ \\frac{d}{d b_{\\theta}} \\mathrm{Var}(g_{\\mathrm{SF}}) = 2 b_{\\theta} \\mathrm{Var}(s_{\\theta}(X)) - 2 \\mathrm{Cov(G_0, s_{\\theta}(X))} = 0. $$\nSolving for $b_{\\theta}$ gives the optimal baseline $b_{\\theta}^{\\star}$:\n$$ b_{\\theta}^{\\star} = \\frac{\\mathrm{Cov}(G_0, s_{\\theta}(X))}{\\mathrm{Var}(s_{\\theta}(X))}. $$\nSubstituting $G_0 = f_{\\theta}(X) s_{\\theta}(X) + \\nabla_{\\theta} f_{\\theta}(X)$:\n$$ b_{\\theta}^{\\star} = \\frac{\\mathrm{Cov}(f_{\\theta}(X) s_{\\theta}(X) + \\nabla_{\\theta} f_{\\theta}(X), s_{\\theta}(X))}{\\mathrm{Var}(s_{\\theta}(X))}. $$\nUsing the linearity of covariance in its first argument:\n$$ b_{\\theta}^{\\star} = \\frac{\\mathrm{Cov}(f_{\\theta}(X) s_{\\theta}(X), s_{\\theta}(X)) + \\mathrm{Cov}(\\nabla_{\\theta} f_{\\theta}(X), s_{\\theta}(X))}{\\mathrm{Var}(s_{\\theta}(X))}. $$\nThis is the general expression for the optimal baseline.\n\n### Task 2: Specialization to the Gaussian Model\n\nWe specialize to $X \\sim \\mathcal{N}(\\theta, \\sigma^2)$ and $f_{\\theta}(x) = x^2 + \\theta x$.\nFirst, we compute the necessary components. The log-density is $\\ln p_{\\theta}(x) = C - \\frac{(x-\\theta)^2}{2\\sigma^2}$, where $C$ is a constant. The score function is:\n$$ s_{\\theta}(x) = \\nabla_{\\theta} \\left( -\\frac{(x-\\theta)^2}{2\\sigma^2} \\right) = -\\frac{1}{2\\sigma^2} \\cdot 2(x-\\theta)(-1) = \\frac{x-\\theta}{\\sigma^2}. $$\nSo, $s_{\\theta}(X) = \\frac{X-\\theta}{\\sigma^2}$. The partial derivative of $f_{\\theta}$ is:\n$$ \\nabla_{\\theta} f_{\\theta}(x) = \\nabla_{\\theta}(x^2 + \\theta x) = x. $$\nSo, $\\nabla_{\\theta} f_{\\theta}(X) = X$.\n\nNow we compute the terms in the expression for $b_{\\theta}^{\\star}$:\n1.  **Denominator: $\\mathrm{Var}(s_{\\theta}(X))$**\n    $$ \\mathrm{Var}(s_{\\theta}(X)) = \\mathrm{Var}\\left(\\frac{X-\\theta}{\\sigma^2}\\right) = \\frac{1}{(\\sigma^2)^2} \\mathrm{Var}(X) = \\frac{\\sigma^2}{\\sigma^4} = \\frac{1}{\\sigma^2}. $$\n\n2.  **Numerator Term 1: $\\mathrm{Cov}(\\nabla_{\\theta} f_{\\theta}(X), s_{\\theta}(X))$**\n    Since $\\mathbb{E}[s_{\\theta}(X)]=0$, the covariance is $\\mathrm{Cov}(X, s_{\\theta}(X)) = \\mathbb{E}[X s_{\\theta}(X)]$.\n    $$ \\mathbb{E}\\left[X \\cdot \\frac{X-\\theta}{\\sigma^2}\\right] = \\frac{1}{\\sigma^2} \\mathbb{E}[X^2 - \\theta X] = \\frac{1}{\\sigma^2}(\\mathbb{E}[X^2] - \\theta \\mathbb{E}[X]). $$\n    For $X \\sim \\mathcal{N}(\\theta, \\sigma^2)$, we have $\\mathbb{E}[X]=\\theta$ and $\\mathbb{E}[X^2] = \\mathrm{Var}(X) + (\\mathbb{E}[X])^2 = \\sigma^2 + \\theta^2$.\n    $$ \\mathrm{Cov}(X, s_{\\theta}(X)) = \\frac{1}{\\sigma^2}((\\sigma^2+\\theta^2) - \\theta^2) = \\frac{\\sigma^2}{\\sigma^2} = 1. $$\n\n3.  **Numerator Term 2: $\\mathrm{Cov}(f_{\\theta}(X) s_{\\theta}(X), s_{\\theta}(X))$**\n    Again, since $\\mathbb{E}[s_{\\theta}(X)]=0$, this covariance simplifies to an expectation:\n    $$ \\mathrm{Cov}(f_{\\theta}(X) s_{\\theta}(X), s_{\\theta}(X)) = \\mathbb{E}[ (f_{\\theta}(X) s_{\\theta}(X)) \\cdot s_{\\theta}(X) ] = \\mathbb{E}[f_{\\theta}(X) s_{\\theta}(X)^2]. $$\n    Let $Y = (X-\\theta)/\\sigma$, so $Y \\sim \\mathcal{N}(0,1)$. Then $X = \\sigma Y + \\theta$.\n    We have $s_{\\theta}(X)^2 = \\frac{(X-\\theta)^2}{\\sigma^4} = \\frac{(\\sigma Y)^2}{\\sigma^4} = \\frac{Y^2}{\\sigma^2}$.\n    And $f_{\\theta}(X) = X^2 + \\theta X = (\\sigma Y + \\theta)^2 + \\theta(\\sigma Y + \\theta) = (\\sigma^2 Y^2 + 2\\sigma\\theta Y + \\theta^2) + (\\sigma\\theta Y + \\theta^2) = \\sigma^2 Y^2 + 3\\sigma\\theta Y + 2\\theta^2$.\n    The expectation is:\n    $$ \\mathbb{E}[(\\sigma^2 Y^2 + 3\\sigma\\theta Y + 2\\theta^2) \\frac{Y^2}{\\sigma^2}] = \\frac{1}{\\sigma^2} \\mathbb{E}[\\sigma^2 Y^4 + 3\\sigma\\theta Y^3 + 2\\theta^2 Y^2]. $$\n    Using the moments of a standard normal distribution, $\\mathbb{E}[Y^2]=1$, $\\mathbb{E}[Y^3]=0$, and $\\mathbb{E}[Y^4]=3$:\n    $$ \\frac{1}{\\sigma^2} [\\sigma^2 \\mathbb{E}[Y^4] + 3\\sigma\\theta \\mathbb{E}[Y^3] + 2\\theta^2 \\mathbb{E}[Y^2]] = \\frac{1}{\\sigma^2} [\\sigma^2(3) + 3\\sigma\\theta(0) + 2\\theta^2(1)] = \\frac{3\\sigma^2 + 2\\theta^2}{\\sigma^2} = 3 + \\frac{2\\theta^2}{\\sigma^2}. $$\n\nFinally, we assemble $b_{\\theta}^{\\star}$:\n$$ b_{\\theta}^{\\star} = \\frac{(\\text{Term 2}) + (\\text{Term 1})}{\\text{Denominator}} = \\frac{(3 + \\frac{2\\theta^2}{\\sigma^2}) + 1}{\\frac{1}{\\sigma^2}} = \\frac{4 + \\frac{2\\theta^2}{\\sigma^2}}{\\frac{1}{\\sigma^2}} = \\sigma^2 \\left(4 + \\frac{2\\theta^2}{\\sigma^2}\\right) = 4\\sigma^2 + 2\\theta^2. $$\n\n### Task 3: Analysis of the Pathwise Estimator\n\nThe pathwise estimator is $g_{\\mathrm{PW}}(\\varepsilon) = \\nabla_{\\theta} f_{\\theta}(T_{\\theta}(\\varepsilon))$. For the given model, $T_{\\theta}(\\varepsilon) = \\theta + \\sigma\\varepsilon$ and $f_{\\theta}(x) = x^2 + \\theta x$.\nFirst, we find the explicit form of the estimator:\n$$ f_{\\theta}(T_{\\theta}(\\varepsilon)) = (\\theta + \\sigma\\varepsilon)^2 + \\theta(\\theta + \\sigma\\varepsilon) = \\theta^2 + 2\\theta\\sigma\\varepsilon + \\sigma^2\\varepsilon^2 + \\theta^2 + \\theta\\sigma\\varepsilon = 2\\theta^2 + 3\\theta\\sigma\\varepsilon + \\sigma^2\\varepsilon^2. $$\nDifferentiating with respect to $\\theta$:\n$$ g_{\\mathrm{PW}}(\\varepsilon) = \\nabla_{\\theta} (2\\theta^2 + 3\\theta\\sigma\\varepsilon + \\sigma^2\\varepsilon^2) = 4\\theta + 3\\sigma\\varepsilon. $$\nThe true gradient is $\\nabla_{\\theta} \\mathbb{E}[f_{\\theta}(X)] = \\nabla_{\\theta}(\\sigma^2+2\\theta^2) = 4\\theta$. The expectation of the estimator is $\\mathbb{E}[g_{\\mathrm{PW}}(\\varepsilon)] = \\mathbb{E}[4\\theta + 3\\sigma\\varepsilon] = 4\\theta + 3\\sigma\\mathbb{E}[\\varepsilon] = 4\\theta$, so it is unbiased.\n\nNow we analyze the effect of a constant baseline $b_{\\theta}$ that is independent of $\\varepsilon$. The proposed estimator would be $g'_{\\mathrm{PW}}(\\varepsilon) = g_{\\mathrm{PW}}(\\varepsilon) - b_{\\theta}$.\nThe expectation of this new estimator is:\n$$ \\mathbb{E}[g'_{\\mathrm{PW}}(\\varepsilon)] = \\mathbb{E}[g_{\\mathrm{PW}}(\\varepsilon) - b_{\\theta}] = \\mathbb{E}[g_{\\mathrm{PW}}(\\varepsilon)] - b_{\\theta} = 4\\theta - b_{\\theta}. $$\nFor this estimator to be unbiased, we must have $4\\theta - b_{\\theta} = 4\\theta$, which implies $b_{\\theta}=0$. Any non-zero constant baseline $b_{\\theta}$ introduces bias.\n\nFurthermore, a constant baseline cannot reduce variance. The variance of the modified estimator is:\n$$ \\mathrm{Var}(g'_{\\mathrm{PW}}(\\varepsilon)) = \\mathrm{Var}(g_{\\mathrm{PW}}(\\varepsilon) - b_{\\theta}) = \\mathrm{Var}(g_{\\mathrm{PW}}(\\varepsilon)), $$\nsince adding or subtracting a constant does not change the variance of a random variable.\n\nThis situation contrasts with the score-function estimator. In that case, the baseline $b_{\\theta}$ multiplies the score $s_{\\theta}(X)$, which is a random variable with mean zero. The subtracted term, $b_{\\theta}s_{\\theta}(X)$, is a valid zero-mean control variate. Its covariance with the rest of the estimator is non-zero, allowing for variance reduction. For the pathwise estimator, subtracting a constant $b_{\\theta}$ is equivalent to using a \"control variate\" that is a constant. The covariance between any random variable and a constant is always zero: $\\mathrm{Cov}(g_{\\mathrm{PW}}(\\varepsilon), b_{\\theta})=0$. Thus, such a term cannot be used to reduce variance via the standard control variate method. In essence, a constant baseline is ineffective for variance reduction and detrimental due to bias introduction when applied directly to an unbiased estimator like $g_{\\mathrm{PW}}(\\varepsilon)$.", "answer": "$$\n\\boxed{2\\theta^{2} + 4\\sigma^{2}}\n$$", "id": "3300811"}, {"introduction": "The efficiency of many Monte Carlo methods depends not only on the estimator's form but also on the sampling strategy used to generate the data. In this exercise [@problem_id:3300821], you will explore this concept through the lens of Hutchinson's trace estimator, a vital tool in large-scale numerical linear algebra. You will first analyze the variance of the standard estimator built from independent random probes and then discover the dramatic consequence of replacing them with a structured, orthogonal set, revealing the profound impact of probe dependence on statistical error.", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a fixed real symmetric matrix. For each probe index $i \\in \\{1, \\dots, m\\}$, draw a random vector $z_i \\in \\{\\pm 1\\}^{n}$ whose coordinates are independent and identically distributed Rademacher random variables (i.e., for each coordinate $k$, $\\mathbb{P}(z_{i,k} = 1) = \\mathbb{P}(z_{i,k} = -1) = \\tfrac{1}{2}$), and assume the probes $\\{z_i\\}_{i=1}^{m}$ are mutually independent across $i$. Define Hutchinson’s trace estimator\n$$\n\\hat{t} \\;=\\; \\frac{1}{m} \\sum_{i=1}^{m} z_i^{\\top} A z_i.\n$$\nUsing only the core definitions of covariance, independence, and trace, and standard properties of expectations of products of independent Rademacher random variables, do the following:\n- Derive the covariance $\\mathrm{Cov}\\big(z_i^{\\top} A z_i,\\; z_j^{\\top} A z_j\\big)$ for $i \\neq j$ under the independent probe model stated above.\n- Based on your derivation, express $\\mathrm{Var}(\\hat{t})$ for the independent probe model in terms of $\\mathrm{Var}\\big(z^{\\top} A z\\big)$, where $z \\in \\{\\pm 1\\}^{n}$ is an independent Rademacher vector with the same distribution as each $z_i$.\n\nNext, propose an orthogonalization scheme to reduce $\\mathrm{Var}(\\hat{t})$: let $H \\in \\{\\pm 1\\}^{n \\times n}$ be a Hadamard matrix satisfying $H H^{\\top} = n I_n$, and take the orthogonalized probes to be the full set of rows $\\{h_1, \\dots, h_n\\}$ of $H$ (so $m=n$ and $h_i^{\\top} h_j = 0$ for $i \\neq j$). Consider the orthogonalized estimator\n$$\n\\hat{t}_{\\mathrm{orth}} \\;=\\; \\frac{1}{n} \\sum_{i=1}^{n} h_i^{\\top} A h_i.\n$$\nUsing only linearity of expectation and the defining property $H H^{\\top} = n I_n$, determine $\\mathrm{Var}\\!\\big(\\hat{t}_{\\mathrm{orth}}\\big)$.\n\nYour final answer must be a single real number or a single closed-form analytic expression. If you choose to express a numerical result, provide the exact value without rounding.", "solution": "First, we analyze the standard Hutchinson estimator with independent random probes.\nLet $X_i = z_i^{\\top} A z_i$ and $X_j = z_j^{\\top} A z_j$ for two distinct probe indices $i, j \\in \\{1, \\dots, m\\}$ where $i \\neq j$. We are asked to derive the covariance $\\mathrm{Cov}(X_i, X_j)$.\n\nBy definition, the covariance is given by:\n$$\n\\mathrm{Cov}(X_i, X_j) = \\mathbb{E}[X_i X_j] - \\mathbb{E}[X_i]\\mathbb{E}[X_j]\n$$\nThe problem states that the probe vectors $\\{z_i\\}_{i=1}^{m}$ are mutually independent. This means that for $i \\neq j$, the random vectors $z_i$ and $z_j$ are independent.\nThe random variable $X_i = z_i^{\\top} A z_i$ is a function of $z_i$, and the random variable $X_j = z_j^{\\top} A z_j$ is a function of $z_j$. For independent random vectors $z_i$ and $z_j$, any functions of these vectors, $f(z_i)$ and $g(z_j)$, are also independent. Therefore, $X_i$ and $X_j$ are independent random variables.\n\nA fundamental property of expectation for independent random variables is that the expectation of their product is the product of their expectations:\n$$\n\\mathbb{E}[X_i X_j] = \\mathbb{E}[X_i] \\mathbb{E}[X_j]\n$$\nSubstituting this into the definition of covariance, we find:\n$$\n\\mathrm{Cov}(X_i, X_j) = \\mathbb{E}[X_i]\\mathbb{E}[X_j] - \\mathbb{E}[X_i]\\mathbb{E}[X_j] = 0\n$$\nThus, the covariance between the quadratic forms evaluated at two different independent probes is zero.\n\nNext, we are asked to express the variance of the estimator $\\hat{t} = \\frac{1}{m} \\sum_{i=1}^{m} z_i^{\\top} A z_i$ in terms of $\\mathrm{Var}(z^{\\top} A z)$.\nLet's again use the notation $X_i = z_i^{\\top} A z_i$. The estimator is $\\hat{t} = \\frac{1}{m} \\sum_{i=1}^{m} X_i$.\nUsing the properties of variance, we have:\n$$\n\\mathrm{Var}(\\hat{t}) = \\mathrm{Var}\\left(\\frac{1}{m} \\sum_{i=1}^{m} X_i\\right) = \\frac{1}{m^2} \\mathrm{Var}\\left(\\sum_{i=1}^{m} X_i\\right)\n$$\nThe variance of a sum of random variables is the sum of all entries in their covariance matrix:\n$$\n\\mathrm{Var}\\left(\\sum_{i=1}^{m} X_i\\right) = \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\mathrm{Cov}(X_i, X_j)\n$$\nWe can split the double summation into terms where $i=j$ (the diagonal terms) and terms where $i \\neq j$ (the off-diagonal terms):\n$$\n\\sum_{i=1}^{m} \\sum_{j=1}^{m} \\mathrm{Cov}(X_i, X_j) = \\sum_{i=1}^{m} \\mathrm{Cov}(X_i, X_i) + \\sum_{i \\neq j} \\mathrm{Cov}(X_i, X_j)\n$$\nFrom our previous derivation, we know that for $i \\neq j$, $\\mathrm{Cov}(X_i, X_j) = 0$. Therefore, the second sum is zero.\nThe term $\\mathrm{Cov}(X_i, X_i)$ is, by definition, the variance $\\mathrm{Var}(X_i)$. So the first sum becomes:\n$$\n\\sum_{i=1}^{m} \\mathrm{Var}(X_i)\n$$\nThe problem states that the coordinates of each vector $z_i$ are independent and identically distributed (i.i.d.) Rademacher random variables, and the probes $\\{z_i\\}$ are mutually independent. This implies that the random variables $\\{X_i = z_i^{\\top} A z_i\\}_{i=1}^{m}$ are i.i.d. As they are identically distributed, they all have the same variance. Let this common variance be denoted by $\\mathrm{Var}(z^{\\top} A z)$, where $z$ is a generic random vector with the same distribution as each $z_i$.\nTherefore, the sum of variances is:\n$$\n\\sum_{i=1}^{m} \\mathrm{Var}(X_i) = \\sum_{i=1}^{m} \\mathrm{Var}(z^{\\top} A z) = m \\cdot \\mathrm{Var}(z^{\\top} A z)\n$$\nSubstituting this back into the expression for $\\mathrm{Var}(\\hat{t})$:\n$$\n\\mathrm{Var}(\\hat{t}) = \\frac{1}{m^2} \\left(m \\cdot \\mathrm{Var}(z^{\\top} A z)\\right) = \\frac{1}{m} \\mathrm{Var}(z^{\\top} A z)\n$$\nThis expression shows that the variance of the standard Hutchinson estimator decreases as the number of probes $m$ increases.\n\nFinally, we analyze the orthogonalized estimator $\\hat{t}_{\\mathrm{orth}}$.\nThe probes for this estimator are the rows $\\{h_1, \\dots, h_n\\}$ of a fixed $n \\times n$ Hadamard matrix $H$. These probes are deterministic vectors, not random variables.\nThe estimator is defined as:\n$$\n\\hat{t}_{\\mathrm{orth}} = \\frac{1}{n} \\sum_{i=1}^{n} h_i^{\\top} A h_i\n$$\nSince the matrix $A$ is fixed and the vectors $\\{h_i\\}$ are fixed, the entire expression $\\hat{t}_{\\mathrm{orth}}$ evaluates to a single, fixed scalar value. It is not a random variable. To see what this value is, we can expand the sum.\nThe quadratic form $h_i^{\\top} A h_i$ can be written in terms of components. Let $h_i$ be the $i$-th row of $H$, so its components are $(h_i)_k = H_{ik}$.\n$$\nh_i^{\\top} A h_i = \\sum_{k=1}^{n} \\sum_{l=1}^{n} (h_i)_k A_{kl} (h_i)_l = \\sum_{k=1}^{n} \\sum_{l=1}^{n} H_{ik} A_{kl} H_{il}\n$$\nNow, summing over $i$:\n$$\n\\sum_{i=1}^{n} h_i^{\\top} A h_i = \\sum_{i=1}^{n} \\sum_{k=1}^{n} \\sum_{l=1}^{n} H_{ik} A_{kl} H_{il}\n$$\nBy reordering the summations, we get:\n$$\n\\sum_{k=1}^{n} \\sum_{l=1}^{n} A_{kl} \\left( \\sum_{i=1}^{n} H_{ik} H_{il} \\right)\n$$\nThe inner sum $\\sum_{i=1}^{n} H_{ik} H_{il}$ corresponds to the $(k,l)$-th element of the matrix product $H^{\\top} H$. The problem states that $H$ is a Hadamard matrix satisfying $H H^{\\top} = n I_n$. Since $H$ is a square matrix, this implies $H$ is invertible and $H^{\\top} = n H^{-1}$, which further implies that $H^{\\top} H = n I_n$. The $(k,l)$-th element of $n I_n$ is $n \\delta_{kl}$, where $\\delta_{kl}$ is the Kronecker delta.\nSubstituting this result back:\n$$\n\\sum_{i=1}^{n} h_i^{\\top} A h_i = \\sum_{k=1}^{n} \\sum_{l=1}^{n} A_{kl} (n \\delta_{kl}) = n \\sum_{k=1}^{n} A_{kk}\n$$\nThe sum $\\sum_{k=1}^{n} A_{kk}$ is the definition of the trace of $A$, denoted $\\mathrm{Tr}(A)$.\nThus, we have:\n$$\n\\sum_{i=1}^{n} h_i^{\\top} A h_i = n \\cdot \\mathrm{Tr}(A)\n$$\nSubstituting this into the definition of the orthogonalized estimator:\n$$\n\\hat{t}_{\\mathrm{orth}} = \\frac{1}{n} (n \\cdot \\mathrm{Tr}(A)) = \\mathrm{Tr}(A)\n$$\nThis shows that $\\hat{t}_{\\mathrm{orth}}$ is not a stochastic estimator but a deterministic algorithm that computes the trace of $A$ exactly. The result is a constant value, $C = \\mathrm{Tr}(A)$.\nThe variance of any constant is zero. Formally, for a constant $C$:\n$$\n\\mathrm{Var}(C) = \\mathbb{E}[(C - \\mathbb{E}[C])^2] = \\mathbb{E}[(C-C)^2] = \\mathbb{E}[0] = 0\n$$\nTherefore, the variance of the orthogonalized estimator is:\n$$\n\\mathrm{Var}(\\hat{t}_{\\mathrm{orth}}) = 0\n$$\nThis result demonstrates the power of using a complete orthogonal basis for probes: it eliminates statistical error entirely, yielding an exact answer in a deterministic fashion. The \"estimator\" designation is a formalism; in this case, it is an exact computation.", "answer": "$$\n\\boxed{0}\n$$", "id": "3300821"}]}