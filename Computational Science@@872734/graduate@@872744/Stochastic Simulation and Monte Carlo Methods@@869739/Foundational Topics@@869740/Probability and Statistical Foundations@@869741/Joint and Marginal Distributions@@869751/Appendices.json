{"hands_on_practices": [{"introduction": "A fundamental skill in probability modeling is to understand how a distribution changes when the random variables are transformed. This first practice provides a concrete, hands-on exercise in applying the change of variables formula, a cornerstone technique for deriving the joint distribution of functions of random variables. By working through the calculation of the Jacobian and the subsequent marginalization via integration, you will solidify your understanding of the essential mechanics that connect different representations of a probabilistic system. [@problem_id:3315512]", "problem": "Let $(U,V)$ be a pair of random variables with joint probability density function $f_{U,V}(u,v)=2\\,\\mathbf{1}\\{0\\le v\\le u\\le 1\\}$. Consider the transformation to $(X,Y)$ defined by $X=U$ and $Y=U\\,V$. Using only the definitions of joint densities under smooth, bijective transformations and the definition of marginalization by integration, derive the joint density $f_{X,Y}(x,y)$, including its precise support. Then compute the marginal density $f_Y(y)$ by integrating out $x$. Provide your final answer as a single closed-form analytic expression for $f_Y(y)$, simplified. Do not provide intermediate results in the final answer. No rounding is required.", "solution": "The problem is first validated to ensure it is well-posed and scientifically sound.\n\nThe problem provides a joint probability density function (PDF) for two random variables $(U,V)$, given by $f_{U,V}(u,v) = 2\\,\\mathbf{1}\\{0\\le v\\le u\\le 1\\}$. The symbol $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function. The support of this distribution is the triangular region $\\mathcal{S}_{U,V} = \\{(u,v) \\in \\mathbb{R}^2 \\mid 0 \\le v \\le u \\le 1\\}$.\nTo be a valid PDF, the function must be non-negative and its integral over its support must equal $1$.\nThe function $f_{U,V}(u,v)$ is $2$ on its support and $0$ elsewhere, so it is non-negative.\nThe integral is:\n$$ \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f_{U,V}(u,v) \\,dv\\,du = \\int_0^1 \\int_0^u 2 \\,dv\\,du $$\n$$ = \\int_0^1 [2v]_{v=0}^{v=u} \\,du = \\int_0^1 2u \\,du = [u^2]_0^1 = 1^2 - 0^2 = 1 $$\nThe PDF is valid. The problem statement is self-contained, unambiguous, and poses a standard question in probability theory. Thus, the problem is valid, and we may proceed with the solution.\n\nThe task is to find the marginal density $f_Y(y)$ of the random variable $Y = UV$, where the transformation is from $(U,V)$ to $(X,Y)$ defined by $X=U$ and $Y=UV$. This will be accomplished in two steps: first, finding the joint PDF $f_{X,Y}(x,y)$ using the change of variables method, and second, finding the marginal PDF $f_Y(y)$ by integrating $f_{X,Y}(x,y)$ with respect to $x$.\n\n**Step 1: Derive the joint density $f_{X,Y}(x,y)$**\n\nThe transformation from $(U,V)$ to $(X,Y)$ is given by the functions:\n$x = g_1(u,v) = u$\n$y = g_2(u,v) = uv$\n\nTo use the change of variables formula, we first need to find the inverse transformation, which expresses $(u,v)$ in terms of $(x,y)$.\nFrom $x=u$, we immediately have $u=x$.\nSubstituting this into the second equation, $y = xv$, which gives $v = y/x$.\nSo, the inverse transformation is:\n$u = h_1(x,y) = x$\n$v = h_2(x,y) = y/x$\n\nNext, we compute the Jacobian of this inverse transformation. The Jacobian matrix is:\n$$ J = \\begin{pmatrix} \\frac{\\partial u}{\\partial x}  \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x}  \\frac{\\partial v}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial}{\\partial x}(x)  \\frac{\\partial}{\\partial y}(x) \\\\ \\frac{\\partial}{\\partial x}(y/x)  \\frac{\\partial}{\\partial y}(y/x) \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ -y/x^2  1/x \\end{pmatrix} $$\nThe determinant of the Jacobian is:\n$$ \\det(J) = (1)\\left(\\frac{1}{x}\\right) - (0)\\left(-\\frac{y}{x^2}\\right) = \\frac{1}{x} $$\nThe absolute value of the Jacobian determinant is $|\\det(J)| = |1/x|$.\n\nNow, we determine the support of the new variables $(X,Y)$, denoted $\\mathcal{S}_{X,Y}$. This region is the image of the support $\\mathcal{S}_{U,V}$ under the transformation. The constraints for $\\mathcal{S}_{U,V}$ are $0 \\le v \\le u \\le 1$. We substitute the inverse transformation expressions for $u$ and $v$:\n$1$. $u \\le 1 \\implies x \\le 1$\n$2$. $v \\le u \\implies y/x \\le x \\implies y \\le x^2$ (assuming $x  0$)\n$3$. $v \\ge 0 \\implies y/x \\ge 0 \\implies y \\ge 0$ (assuming $x  0$)\nThe condition $u \\ge 0$ implies $x \\ge 0$. Combining with $x \\le 1$, we have $0 \\le x \\le 1$.\nSince $x=u$ and $u \\ge v \\ge 0$, $x$ must be non-negative. On the interior of the support, $u0$, hence $x0$, so the multiplications by $x$ are valid. The case $x=0$ corresponds to $u=0$, which implies $v=0$, and thus $y=0$. This is a single point $(0,0)$ which has zero probability mass. Thus, we can safely assume $x0$ for the density calculation, making $|\\det(J)| = 1/x$.\n\nThe support for $(X,Y)$ is therefore defined by the inequalities: $0 \\le x \\le 1$ and $0 \\le y \\le x^2$. This can be written compactly as $\\mathcal{S}_{X,Y} = \\{(x,y) \\in \\mathbb{R}^2 \\mid 0 \\le y \\le x^2 \\le 1\\}$.\n\nThe change of variables formula for the joint PDF is:\n$$ f_{X,Y}(x,y) = f_{U,V}(u(x,y), v(x,y)) |\\det(J)| $$\nSubstituting the expressions for $u,v$, the value of $f_{U,V}$ (which is $2$ on its support), and $|\\det(J)|$:\n$$ f_{X,Y}(x,y) = 2 \\cdot \\frac{1}{x} = \\frac{2}{x} $$\nThis expression is valid for $(x,y) \\in \\mathcal{S}_{X,Y}$. So, the full joint PDF for $(X,Y)$ is:\n$$ f_{X,Y}(x,y) = \\frac{2}{x} \\mathbf{1}\\{0 \\le y \\le x^2 \\le 1\\} $$\n\n**Step 2: Derive the marginal density $f_Y(y)$**\n\nThe marginal PDF of $Y$ is found by integrating the joint PDF $f_{X,Y}(x,y)$ over all possible values of $x$:\n$$ f_Y(y) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\,dx $$\nTo perform this integration, we need the limits for $x$ for a fixed value of $y$. From the support $\\mathcal{S}_{X,Y}$, the inequalities are $0 \\le y \\le x^2$ and $x \\le 1$.\nThe first inequality, $y \\le x^2$, implies $x \\ge \\sqrt{y}$ (since $x \\ge 0$).\nThe second inequality is $x \\le 1$.\nSo, for a fixed $y$, $x$ ranges from $\\sqrt{y}$ to $1$.\nThe possible range for $y$ is determined from the support as well. Since $y \\ge 0$ and $y \\le x^2 \\le 1^2=1$, we have $0 \\le y \\le 1$.\n\nFor any $y \\in [0,1]$, the integral is:\n$$ f_Y(y) = \\int_{\\sqrt{y}}^{1} \\frac{2}{x} \\,dx $$\nThis is a standard integral:\n$$ f_Y(y) = [2 \\ln|x|]_{\\sqrt{y}}^{1} $$\nSince $x$ is integrated over the interval $[\\sqrt{y}, 1]$ where $y \\in [0,1]$, $x$ is always positive, so $|x|=x$.\n$$ f_Y(y) = 2 (\\ln(1) - \\ln(\\sqrt{y})) $$\nUsing the logarithmic properties $\\ln(1)=0$ and $\\ln(\\sqrt{y}) = \\ln(y^{1/2}) = \\frac{1}{2}\\ln(y)$:\n$$ f_Y(y) = 2 \\left(0 - \\frac{1}{2}\\ln(y)\\right) = -\\ln(y) $$\nThis expression is valid for $y \\in (0,1]$. For $y \\le 0$ or $y  1$, $f_Y(y)=0$. The problem asks for the analytic expression, which is $-\\ln(y)$.\nAs a check, we can verify that $\\int_0^1 f_Y(y) \\,dy = 1$:\n$$ \\int_0^1 -\\ln(y) \\,dy $$\nUsing integration by parts, with $u = -\\ln(y)$ and $dv = dy$, so $du = -1/y \\,dy$ and $v=y$:\n$$ \\int_0^1 -\\ln(y) \\,dy = [-y\\ln(y)]_0^1 - \\int_0^1 y\\left(-\\frac{1}{y}\\right) \\,dy $$\n$$ = [-y\\ln(y)]_0^1 + \\int_0^1 1 \\,dy $$\nThe limit $\\lim_{y \\to 0^+} y\\ln(y) = 0$. At $y=1$, $-1\\ln(1)=0$. So the first term evaluates to $0$. The second term is $[y]_0^1=1$. The total is $0+1=1$, which confirms the marginal density is correct.\n\nThe final simplified, closed-form analytic expression for $f_Y(y)$ is $-\\ln(y)$.", "answer": "$$\\boxed{-\\ln(y)}$$", "id": "3315512"}, {"introduction": "While many introductory examples feature joint distributions with well-behaved density functions over a 2D region, this is not always the case. This exercise explores a more subtle and profound scenario where the relationship between two variables, $Y = X^2$, is deterministic. The resulting joint distribution is 'singular' because all its probability mass is concentrated on a one-dimensional curve, which has zero area, meaning no joint density exists with respect to the standard 2D Lebesgue measure. This practice will challenge your intuition and deepen your understanding of how marginal distributions can exist even when a conventional joint density does not. [@problem_id:3315509]", "problem": "Let $X$ be a real-valued random variable with a probability density function (PDF) $f_{X}(x)$ that is the normal density with mean $1$ and variance $1$, i.e., $f_{X}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\big(-\\frac{(x-1)^{2}}{2}\\big)$. Define the measurable mapping $T:\\mathbb{R}\\to\\mathbb{R}^{2}$ by $T(x) = (x,x^{2})$, and let $(X,Y) := T(X) = (X, X^{2})$. Denote by $\\mu$ the joint law of $(X,Y)$ on $\\mathbb{R}^{2}$, i.e., $\\mu = \\mathbb{P}\\circ T^{-1}$. Let $\\lambda_{1}$ and $\\lambda_{2}$ denote one-dimensional and two-dimensional Lebesgue measure, respectively.\n\nTasks:\n- Using only foundational measure-theoretic definitions (pushforward measure, absolute continuity, and singularity), construct the joint distribution $\\mu$ of $(X,Y)$ and prove that $\\mu$ is supported on the set $\\{(x,y)\\in\\mathbb{R}^{2}: y = x^{2}\\}$, has no joint density with respect to $\\lambda_{2}$, and yet has well-defined marginal distributions with respect to $\\lambda_{1}$.\n- Starting from the definition of the cumulative distribution function (CDF) and the chain rule for differentiation, derive the PDF of $Y$ for $y0$.\n- Evaluate the resulting marginal density of $Y$ at $y=1$ as a closed-form expression. Provide your final answer as a single analytic expression. Do not approximate; no rounding is required.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information to derive a unique and meaningful solution. The tasks require a rigorous application of measure theory, probability theory, and calculus.\n\nThe problem asks for three items: a measure-theoretic analysis of the joint distribution, the derivation of a marginal probability density function (PDF), and the evaluation of this PDF at a specific point. We will address these in sequence.\n\nLet $X$ be a random variable with a normal distribution of mean $\\mu_X = 1$ and variance $\\sigma_X^2 = 1$. Its PDF is given by $f_{X}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-1)^{2}}{2}\\right)$. Let $P_X$ be the law of $X$, which is a measure on $(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))$. For any Borel set $B \\subseteq \\mathbb{R}$, $P_X(B) = \\mathbb{P}(X \\in B) = \\int_B f_X(x) \\, d\\lambda_1(x)$, where $\\lambda_1$ is the one-dimensional Lebesgue measure. This shows that $P_X$ is absolutely continuous with respect to $\\lambda_1$.\n\nThe random vector $(X,Y)$ is defined by the measurable transformation $T:\\mathbb{R} \\to \\mathbb{R}^2$, $T(x) = (x, x^2)$, such that $(X,Y) = T(X)$. The joint distribution $\\mu$ of $(X,Y)$ is the pushforward measure of $P_X$ under $T$.\n\n_Task 1: Measure-Theoretic Analysis of the Joint Distribution $\\mu$_\n\n- **Construction of $\\mu$**:\nBy the definition of a pushforward measure, for any Borel set $A \\subseteq \\mathbb{R}^2$, the measure $\\mu(A)$ is given by:\n$$ \\mu(A) = P_X(T^{-1}(A)) $$\nwhere $T^{-1}(A) = \\{x \\in \\mathbb{R} \\mid T(x) \\in A\\} = \\{x \\in \\mathbb{R} \\mid (x, x^2) \\in A\\}$.\nSince $P_X$ has a density $f_X$ with respect to $\\lambda_1$, we can write $\\mu(A)$ as an integral:\n$$ \\mu(A) = \\int_{T^{-1}(A)} f_X(x) \\, d\\lambda_1(x) $$\nThis integral expression is the construction of the joint distribution $\\mu$.\n\n- **Support of $\\mu$**:\nThe support of a measure is the smallest closed set $S$ such that $\\mu(\\mathbb{R}^2 \\setminus S) = 0$. Let $C$ be the parabola defined by $C = \\{(x,y) \\in \\mathbb{R}^2 \\mid y=x^2\\}$. The set $C$ is a closed set in $\\mathbb{R}^2$. By definition, the random vector $(X,Y)=(X,X^2)$ always lies on this parabola. Therefore, the entire probability mass of $\\mu$ is concentrated on $C$. We can show this formally:\n$$ \\mu(C) = \\mathbb{P}((X,Y) \\in C) = \\mathbb{P}((X, X^2) \\in C) = \\mathbb{P}(\\text{true}) = 1 $$\nSince $\\mu(C)=1$, the support of $\\mu$ must be a subset of $C$. To show that the support is exactly $C$, we must show that for any point $p \\in C$ and any open neighborhood $U$ of $p$, we have $\\mu(U)  0$. Let $p = (x_0, x_0^2) \\in C$ and let $U$ be an open set containing $p$. The function $T(x)=(x, x^2)$ is continuous, so the preimage $T^{-1}(U)$ is an open set in $\\mathbb{R}$. Since $(x_0, x_0^2) \\in U$, we have $x_0 \\in T^{-1}(U)$, so $T^{-1}(U)$ is non-empty. Any non-empty open set in $\\mathbb{R}$ has a positive Lebesgue measure, $\\lambda_1(T^{-1}(U))  0$. The density $f_X(x)$ is strictly positive for all $x \\in \\mathbb{R}$. Therefore,\n$$ \\mu(U) = \\int_{T^{-1}(U)} f_X(x) \\, d\\lambda_1(x)  0 $$\nThis confirms that the support of $\\mu$ is precisely the parabola $C = \\{(x,y) \\in \\mathbb{R}^2 \\mid y=x^2\\}$.\n\n- **Absence of Joint Density with respect to $\\lambda_2$**:\nA measure $\\mu$ on $\\mathbb{R}^2$ has a density with respect to the two-dimensional Lebesgue measure $\\lambda_2$ if and only if $\\mu$ is absolutely continuous with respect to $\\lambda_2$ ($\\mu \\ll \\lambda_2$). This means that for any Borel set $A \\subseteq \\mathbb{R}^2$, if $\\lambda_2(A)=0$, then $\\mu(A)=0$.\nConsider the set $C$, the support of $\\mu$. As the graph of a continuous function, $C$ is a curve in $\\mathbb{R}^2$ and has a $\\lambda_2$-measure of zero, i.e., $\\lambda_2(C)=0$. However, as shown above, $\\mu(C)=1$.\nSince we have a set $C$ for which $\\lambda_2(C)=0$ but $\\mu(C)=1 \\neq 0$, the condition for absolute continuity is violated. Therefore, by the Radon-Nikodym theorem, the measure $\\mu$ does not have a density with respect to $\\lambda_2$. The distribution of $(X,Y)$ is singular with respect to the 2D Lebesgue measure.\n\n- **Well-Defined Marginal Distributions**:\nThe marginal distribution of $X$, denoted $\\mu_X$, is given by $\\mu_X(B) = \\mu(B \\times \\mathbb{R})$ for a Borel set $B \\subseteq \\mathbb{R}$.\n$$ \\mu_X(B) = \\mathbb{P}((X,Y) \\in B \\times \\mathbb{R}) = \\mathbb{P}(X \\in B) = P_X(B) $$\nAs given, $P_X$ has a density $f_X(x)$ with respect to $\\lambda_1$. Thus, the marginal distribution of $X$ is well-defined and absolutely continuous with respect to $\\lambda_1$.\n\nThe marginal distribution of $Y$, denoted $\\mu_Y$, is given by $\\mu_Y(A) = \\mu(\\mathbb{R} \\times A)$ for a Borel set $A \\subseteq \\mathbb{R}$.\n$$ \\mu_Y(A) = \\mathbb{P}((X,Y) \\in \\mathbb{R} \\times A) = \\mathbb{P}(Y \\in A) $$\nIn the next task, we will derive the PDF $f_Y(y)$ for $Y$. The existence of this PDF demonstrates that $\\mu_Y$ is also absolutely continuous with respect to $\\lambda_1$. Therefore, both marginal distributions are well-defined and possess densities with respect to $\\lambda_1$.\n\n_Task 2: Derivation of the PDF of $Y$ for $y  0$_\n\nWe derive the PDF of $Y=X^2$ using the cumulative distribution function (CDF) method. Let $F_Y(y)$ be the CDF of $Y$.\n$$ F_Y(y) = \\mathbb{P}(Y \\le y) $$\nSince $Y=X^2$, $Y$ is non-negative, so $F_Y(y) = 0$ for $y  0$. We are asked to find the PDF for $y0$.\nFor $y  0$:\n$$ F_Y(y) = \\mathbb{P}(X^2 \\le y) = \\mathbb{P}(-\\sqrt{y} \\le X \\le \\sqrt{y}) $$\nThis probability can be expressed as an integral of the PDF of $X$:\n$$ F_Y(y) = \\int_{-\\sqrt{y}}^{\\sqrt{y}} f_X(x) \\, dx $$\nThe PDF $f_Y(y)$ is the derivative of the CDF, $f_Y(y) = \\frac{d}{dy}F_Y(y)$. We use the Leibniz integral rule, which is an application of the Fundamental Theorem of Calculus and the chain rule as requested:\n$$ f_Y(y) = \\frac{d}{dy} \\int_{-\\sqrt{y}}^{\\sqrt{y}} f_X(x) \\, dx = f_X(\\sqrt{y}) \\cdot \\frac{d}{dy}(\\sqrt{y}) - f_X(-\\sqrt{y}) \\cdot \\frac{d}{dy}(-\\sqrt{y}) $$\nThe derivatives of the limits are $\\frac{d}{dy}(\\sqrt{y}) = \\frac{1}{2\\sqrt{y}}$ and $\\frac{d}{dy}(-\\sqrt{y}) = -\\frac{1}{2\\sqrt{y}}$.\nSubstituting these into the expression for $f_Y(y)$:\n$$ f_Y(y) = f_X(\\sqrt{y}) \\left(\\frac{1}{2\\sqrt{y}}\\right) - f_X(-\\sqrt{y}) \\left(-\\frac{1}{2\\sqrt{y}}\\right) = \\frac{1}{2\\sqrt{y}} \\left[ f_X(\\sqrt{y}) + f_X(-\\sqrt{y}) \\right] $$\nNow, we substitute the specific PDF for $X$, $f_X(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-1)^2}{2}\\right)$:\n$$ f_X(\\sqrt{y}) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\sqrt{y}-1)^2}{2}\\right) $$\n$$ f_X(-\\sqrt{y}) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(-\\sqrt{y}-1)^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\sqrt{y}+1)^2}{2}\\right) $$\nCombining these gives:\n$$ f_Y(y) = \\frac{1}{2\\sqrt{y}} \\frac{1}{\\sqrt{2\\pi}} \\left[ \\exp\\left(-\\frac{y-2\\sqrt{y}+1}{2}\\right) + \\exp\\left(-\\frac{y+2\\sqrt{y}+1}{2}\\right) \\right] $$\n$$ f_Y(y) = \\frac{1}{2\\sqrt{2\\pi y}} \\exp\\left(-\\frac{y+1}{2}\\right) \\left[ \\exp\\left(\\sqrt{y}\\right) + \\exp\\left(-\\sqrt{y}\\right) \\right] $$\nUsing the identity $\\cosh(z) = \\frac{\\exp(z)+\\exp(-z)}{2}$, we have $2\\cosh(\\sqrt{y}) = \\exp(\\sqrt{y}) + \\exp(-\\sqrt{y})$.\n$$ f_Y(y) = \\frac{1}{2\\sqrt{2\\pi y}} \\exp\\left(-\\frac{y+1}{2}\\right) [2\\cosh(\\sqrt{y})] $$\n$$ f_Y(y) = \\frac{1}{\\sqrt{2\\pi y}} \\exp\\left(-\\frac{y+1}{2}\\right) \\cosh(\\sqrt{y}) $$\nThis is the PDF of $Y$ for $y0$. This corresponds to a non-central chi-squared distribution with one degree of freedom and non-centrality parameter $\\lambda=1$.\n\n_Task 3: Evaluation of the Marginal Density of $Y$ at $y=1$_\n\nWe need to evaluate $f_Y(y)$ at $y=1$.\n$$ f_Y(1) = \\frac{1}{\\sqrt{2\\pi(1)}} \\exp\\left(-\\frac{1+1}{2}\\right) \\cosh(\\sqrt{1}) $$\n$$ f_Y(1) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-1) \\cosh(1) $$\nTo obtain a closed-form expression in terms of elementary exponentials, we use the definition of the hyperbolic cosine function: $\\cosh(1) = \\frac{\\exp(1) + \\exp(-1)}{2}$.\n$$ f_Y(1) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-1) \\left( \\frac{\\exp(1) + \\exp(-1)}{2} \\right) $$\n$$ f_Y(1) = \\frac{1}{2\\sqrt{2\\pi}} \\left( \\exp(-1)\\exp(1) + \\exp(-1)\\exp(-1) \\right) $$\n$$ f_Y(1) = \\frac{1}{2\\sqrt{2\\pi}} \\left( \\exp(0) + \\exp(-2) \\right) $$\n$$ f_Y(1) = \\frac{1 + \\exp(-2)}{2\\sqrt{2\\pi}} $$\nThis is the final analytic expression.", "answer": "$$\\boxed{\\frac{1 + \\exp(-2)}{2\\sqrt{2\\pi}}}$$", "id": "3315509"}, {"introduction": "We now pivot from theoretical foundations to a powerful practical application in Monte Carlo methods. This practice demonstrates how cleverly constructing a joint distribution can lead to more efficient estimators, a core goal in stochastic simulation. You will explore the concept of Rao-Blackwellization, where we reduce the variance of an estimator by replacing a random quantity with its conditional expectation, effectively using the structure of a joint distribution to our advantage. This exercise bridges the gap between the theory of joint and marginal distributions and the practical art of designing better computational algorithms. [@problem_id:3315571]", "problem": "Consider the problem of estimating the tail probability $P(Xa)$ of a one-dimensional target distribution with density $\\pi(x)\\propto \\exp(-x^2/2)$, that is, a standard normal distribution. Define the indicator function $h(x)=\\mathbf{1}\\{x > a\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator of an event. Two Monte Carlo estimation strategies are considered:\n\n1. Direct sampling from the marginal: Draw independent and identically distributed samples $X_1,\\ldots,X_N$ from $\\pi(x)$ and use the estimator $\\hat{p}_{\\mathrm{dir}}=\\frac{1}{N}\\sum_{i=1}^N h(X_i)$.\n\n2. Sampling from a bivariate joint distribution and then marginalizing: Consider a bivariate target $(X,Y)$ with joint density\n$$\nf_{\\rho}(x,y)=\\frac{1}{2\\pi\\sqrt{1-\\rho^2}}\\exp\\!\\left(-\\frac{1}{2(1-\\rho^2)}\\left(x^2-2\\rho xy+y^2\\right)\\right),\n$$\nwhich is a centered bivariate normal distribution with unit variances and correlation coefficient $\\rho\\in(-1,1)$. The marginal of $X$ is $\\pi(x)$. The estimator is constructed by drawing independent and identically distributed samples $(X_i,Y_i)$ from $f_{\\rho}$ and then marginalizing over $X$ via the conditional expectation, using $\\hat{p}_{\\mathrm{RB}}=\\frac{1}{N}\\sum_{i=1}^N r(Y_i)$, where $r(y)=\\mathbb{P}(Xa\\,|\\,Y=y)$.\n\nStarting from the fundamental definitions of joint and marginal distributions, the law of total expectation, and the law of total variance, and using the well-established properties of the multivariate normal distribution, perform the following:\n\n- Derive and justify the unbiasedness of both $\\hat{p}_{\\mathrm{dir}}$ and $\\hat{p}_{\\mathrm{RB}}$ for any fixed threshold $a\\in\\mathbb{R}$ and any fixed correlation $\\rho\\in(-1,1)$.\n\n- Derive expressions for the variances of the two estimators as functions of $a$, $\\rho$, and $N$. Express the direct estimator variance in terms of the marginal tail probability $p(a)=\\mathbb{P}(Xa)$. For the marginalization-based estimator, express its variance in terms of the distribution of $Y$ and the conditional probability $r(Y)=\\mathbb{P}(Xa\\,|\\,Y)$, and then reduce it to an integral with respect to the known marginal density of $Y$. Show that the variance of the marginalization-based estimator is less than or equal to that of the direct estimator.\n\n- Use the conditional distribution $X\\,|\\,Y=y$ induced by $f_{\\rho}$ to write $r(y)$ in terms of the standard normal cumulative distribution function. Then, express the second moment $\\mathbb{E}[r(Y)^2]$ as a one-dimensional integral with respect to the standard normal density of $Y$, or equivalently as a bivariate normal tail probability, and use this to obtain a computable expression for the variance.\n\n- Implement a program that, for a given test suite, computes the ratio of the estimator variances $\\mathrm{Var}(\\hat{p}_{\\mathrm{RB}})/\\mathrm{Var}(\\hat{p}_{\\mathrm{dir}})$ using exact numerical integration for the integral representation of $\\mathbb{E}[r(Y)^2]$ under the distribution of $Y$.\n\nNo physical units or angles are involved. All numerical answers must be real numbers. The test suite is as follows, where each tuple $(a,\\rho,N)$ specifies the threshold $a$, correlation $\\rho$, and number of samples $N$ considered:\n\n- Test case 1 (boundary independence): $(0.0,\\,0.0,\\,10000)$.\n- Test case 2 (happy path, positive correlation): $(2.0,\\,0.8,\\,50000)$.\n- Test case 3 (rare event, strong positive correlation): $(5.0,\\,0.9,\\,1000000)$.\n- Test case 4 (happy path, negative correlation): $(2.0,\\,-0.7,\\,50000)$.\n- Test case 5 (near-degenerate correlation): $(1.0,\\,0.99,\\,50000)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite, where each entry is the single precision ratio $\\mathrm{Var}(\\hat{p}_{\\mathrm{RB}})/\\mathrm{Var}(\\hat{p}_{\\mathrm{dir}})$ for the corresponding test case (e.g., $[\\text{ratio}_1,\\text{ratio}_2,\\ldots]$).", "solution": "The starting point is the standard normal target $\\pi(x)\\propto \\exp(-x^2/2)$ and the indicator $h(x)=\\mathbf{1}\\{xa\\}$. The quantity of interest is $p(a)=\\mathbb{P}(Xa)$ under $X\\sim \\pi$. The direct estimator is\n$$\n\\hat{p}_{\\mathrm{dir}}=\\frac{1}{N}\\sum_{i=1}^N h(X_i),\n$$\nwith $X_i$ independent and identically distributed from $\\pi$. By the definition of expectation, $\\mathbb{E}[h(X_i)]=\\mathbb{P}(Xa)=p(a)$, so $\\hat{p}_{\\mathrm{dir}}$ is unbiased:\n$$\n\\mathbb{E}[\\hat{p}_{\\mathrm{dir}}]=\\frac{1}{N}\\sum_{i=1}^N \\mathbb{E}[h(X_i)]=p(a).\n$$\nBecause $h(X_i)$ are independent and identically distributed Bernoulli random variables with success probability $p(a)$, the variance is\n$$\n\\mathrm{Var}(\\hat{p}_{\\mathrm{dir}})=\\frac{1}{N}\\mathrm{Var}(h(X))=\\frac{1}{N}p(a)\\left(1-p(a)\\right).\n$$\n\nFor the marginalization-based approach, we sample $(X,Y)$ from the bivariate normal distribution with joint density\n$$\nf_{\\rho}(x,y)=\\frac{1}{2\\pi\\sqrt{1-\\rho^2}}\\exp\\!\\left(-\\frac{1}{2(1-\\rho^2)}\\left(x^2-2\\rho xy+y^2\\right)\\right),\n$$\nwhich has marginal $X\\sim \\pi$ by construction and $Y\\sim \\mathcal{N}(0,1)$, with correlation coefficient $\\rho\\in(-1,1)$. The conditional distribution is a well-known property of the centered bivariate normal:\n$$\nX\\,|\\,Y=y \\sim \\mathcal{N}\\!\\left(\\rho y,\\;1-\\rho^2\\right).\n$$\nUsing the law of total expectation, since $h(X)$ is measurable, we have\n$$\n\\mathbb{E}[h(X)]=\\mathbb{E}\\big[\\mathbb{E}[h(X)\\,|\\,Y]\\big].\n$$\nDefine $r(y)=\\mathbb{P}(Xa\\,|\\,Y=y)=\\mathbb{E}[h(X)\\,|\\,Y=y]$ under the conditional normal. The conditional cumulative distribution function for a normal variable yields\n$$\nr(y)=1-\\Phi\\!\\left(\\frac{a-\\rho y}{\\sqrt{1-\\rho^2}}\\right),\n$$\nwhere $\\Phi(\\cdot)$ is the standard normal cumulative distribution function. Then define the Rao-Blackwellized estimator\n$$\n\\hat{p}_{\\mathrm{RB}}=\\frac{1}{N}\\sum_{i=1}^N r(Y_i),\n$$\nwith $Y_i$ independent and identically distributed from the marginal $Y\\sim \\mathcal{N}(0,1)$. Unbiasedness follows from the tower property:\n$$\n\\mathbb{E}[\\hat{p}_{\\mathrm{RB}}]=\\frac{1}{N}\\sum_{i=1}^N \\mathbb{E}[r(Y_i)]=\\mathbb{E}[r(Y)]=\\mathbb{E}[\\mathbb{E}[h(X)\\,|\\,Y]]=p(a).\n$$\n\nTo compare the variances, we employ the law of total variance. For a single draw,\n$$\n\\mathrm{Var}(h(X))=\\mathbb{E}\\!\\left[\\mathrm{Var}(h(X)\\,|\\,Y)\\right]+\\mathrm{Var}\\!\\left(\\mathbb{E}[h(X)\\,|\\,Y]\\right).\n$$\nSince $\\hat{p}_{\\mathrm{dir}}$ averages independent and identically distributed $h(X_i)$ and $\\hat{p}_{\\mathrm{RB}}$ averages independent and identically distributed $r(Y_i)$, their variances scale as $1/N$:\n$$\n\\mathrm{Var}(\\hat{p}_{\\mathrm{dir}})=\\frac{1}{N}\\mathrm{Var}(h(X)),\\qquad \\mathrm{Var}(\\hat{p}_{\\mathrm{RB}})=\\frac{1}{N}\\mathrm{Var}(r(Y)).\n$$\nBy the law of total variance, $\\mathrm{Var}(r(Y))\\le \\mathrm{Var}(h(X))$, with equality only in degenerate cases where $\\mathrm{Var}(h(X)\\,|\\,Y)=0$ a.s. or $r(Y)$ does not depend on $Y$. Therefore,\n$$\n\\mathrm{Var}(\\hat{p}_{\\mathrm{RB}})\\le \\mathrm{Var}(\\hat{p}_{\\mathrm{dir}}).\n$$\n\nWe now express $\\mathrm{Var}(\\hat{p}_{\\mathrm{RB}})$ more concretely. For $Y\\sim \\mathcal{N}(0,1)$ and $r(y)$ as above,\n$$\n\\mathrm{Var}(r(Y))=\\mathbb{E}[r(Y)^2]-\\left(\\mathbb{E}[r(Y)]\\right)^2=\\mathbb{E}[r(Y)^2]-p(a)^2.\n$$\nThe first moment equals $p(a)$ by the tower property. The second moment admits a useful probabilistic representation. Introduce conditional independent replicas $U_1,U_2$ given $Y=y$, with\n$$\nU_j\\,|\\,Y=y \\sim \\mathcal{N}\\!\\left(\\rho y,\\;1-\\rho^2\\right),\\quad j\\in\\{1,2\\},\n$$\nand independent given $Y$. Then\n$$\nr(y)^2=\\mathbb{P}(U_1a\\,|\\,Y=y)\\,\\mathbb{P}(U_2a\\,|\\,Y=y)=\\mathbb{P}(U_1a,\\,U_2a\\,|\\,Y=y),\n$$\nand by taking expectation over $Y$,\n$$\n\\mathbb{E}[r(Y)^2]=\\mathbb{P}(U_1a,\\,U_2a).\n$$\nUnconditioning yields a centered bivariate normal for $(U_1,U_2)$ with variances $1$ and correlation $\\rho^2$. Indeed, write $U_j=\\rho Y+\\sqrt{1-\\rho^2}\\,\\varepsilon_j$ with $Y\\sim \\mathcal{N}(0,1)$ and $\\varepsilon_j\\sim \\mathcal{N}(0,1)$ independent of $Y$ and of each other. Then\n$$\n\\mathrm{Var}(U_j)=\\rho^2 \\mathrm{Var}(Y)+(1-\\rho^2)\\mathrm{Var}(\\varepsilon_j)=1,\n$$\nand\n$$\n\\mathrm{Cov}(U_1,U_2)=\\mathrm{Var}(\\rho Y)=\\rho^2.\n$$\nTherefore,\n$$\n\\mathbb{E}[r(Y)^2]=\\mathbb{P}(Z_1a,\\,Z_2a),\n$$\nwhere $(Z_1,Z_2)$ is a centered bivariate normal with unit variances and correlation $\\rho^2$. Equivalently, one may compute $\\mathbb{E}[r(Y)^2]$ via the one-dimensional integral\n$$\n\\mathbb{E}[r(Y)^2]=\\int_{-\\infty}^{\\infty} \\phi(y)\\left[1-\\Phi\\!\\left(\\frac{a-\\rho y}{\\sqrt{1-\\rho^2}}\\right)\\right]^2\\,\\mathrm{d}y,\n$$\nwith $\\phi(y)=\\frac{1}{\\sqrt{2\\pi}}\\exp(-y^2/2)$ the standard normal density.\n\nCollecting these, the variances are\n$$\n\\mathrm{Var}(\\hat{p}_{\\mathrm{dir}})=\\frac{1}{N}p(a)\\left(1-p(a)\\right),\n$$\n$$\n\\mathrm{Var}(\\hat{p}_{\\mathrm{RB}})=\\frac{1}{N}\\left(\\int_{-\\infty}^{\\infty} \\phi(y)\\left[1-\\Phi\\!\\left(\\frac{a-\\rho y}{\\sqrt{1-\\rho^2}}\\right)\\right]^2\\,\\mathrm{d}y - p(a)^2\\right).\n$$\nThe ratio of variances is\n$$\nR(a,\\rho)=\\frac{\\mathrm{Var}(\\hat{p}_{\\mathrm{RB}})}{\\mathrm{Var}(\\hat{p}_{\\mathrm{dir}})}=\\frac{\\int_{-\\infty}^{\\infty} \\phi(y)\\left[1-\\Phi\\!\\left(\\frac{a-\\rho y}{\\sqrt{1-\\rho^2}}\\right)\\right]^2\\,\\mathrm{d}y - p(a)^2}{p(a)\\left(1-p(a)\\right)},\n$$\nwhich does not depend on $N$. For $\\rho=0$, we have $X$ independent of $Y$, so $r(y)=p(a)$ almost surely and $R(a,0)=0$; for $|\\rho|\\to 1$, the conditional variance $1-\\rho^2\\to 0$ and $r(Y)$ approaches an indicator, so $R(a,\\rho)$ increases but remains bounded by $1$.\n\nAlgorithmic design:\n\n- Compute $p(a)=1-\\Phi(a)$ using the standard normal cumulative distribution function.\n\n- Define $r(y)=1-\\Phi\\!\\left((a-\\rho y)/\\sqrt{1-\\rho^2}\\right)$.\n\n- Compute $\\mathbb{E}[r(Y)^2]$ by numerically integrating $\\phi(y)\\,r(y)^2$ over $y\\in(-\\infty,\\infty)$ using reliable one-dimensional quadrature.\n\n- Assemble the variance ratio $R(a,\\rho)$ using the expressions above for each test case.\n\nThe program evaluates $R(a,\\rho)$ for each tuple in the given test suite and outputs a single line containing a comma-separated list enclosed in square brackets, in the specified order.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.integrate import quad\n\ndef variance_ratio(a: float, rho: float, N: int) - float:\n    \"\"\"\n    Compute the ratio Var(hat_p_RB) / Var(hat_p_dir) for the given parameters.\n    Although N appears, the ratio is independent of N; it is included to match the test case signature.\n    \"\"\"\n    # Tail probability p(a) = P(X  a) for X ~ N(0,1)\n    p = 1.0 - norm.cdf(a)\n\n    # Handle the independence boundary case explicitly for numerical stability.\n    if abs(rho)  1e-15:\n        # When rho = 0, r(y) = p almost surely, so Var(r(Y)) = 0 and ratio = 0.\n        return 0.0\n\n    # Define r(y) = P(X  a | Y = y) with X|Y=y ~ N(rho*y, 1 - rho^2)\n    sqrt_one_minus_rho2 = np.sqrt(1.0 - rho**2)\n\n    def r_of_y(y: float) - float:\n        t = (a - rho * y) / sqrt_one_minus_rho2\n        return 1.0 - norm.cdf(t)\n\n    # Integrand for E[r(Y)^2] = ∫ φ(y) r(y)^2 dy over y ∈ (-∞, ∞)\n    def integrand(y: float) - float:\n        ry = r_of_y(y)\n        return norm.pdf(y) * (ry * ry)\n\n    # Numerical integration with high accuracy over the real line\n    # quad supports infinite limits; set tighter tolerances to handle rare-event tails.\n    r2_expectation, _ = quad(integrand, -np.inf, np.inf, epsabs=1e-12, epsrel=1e-12, limit=200)\n\n    var_dir = p * (1.0 - p) / N\n    var_rb = (r2_expectation - p**2) / N\n\n    # Ratio is independent of N, but computed with N to emphasize estimator variance context\n    if var_dir == 0.0:\n        # This happens only when p is 0 or 1; for finite a, p ∈ (0,1).\n        return 0.0\n    return var_rb / var_dir\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case is a tuple (a, rho, N).\n    test_cases = [\n        (0.0, 0.0, 10000),       # Boundary independence\n        (2.0, 0.8, 50000),       # Happy path, positive correlation\n        (5.0, 0.9, 1000000),     # Rare event, strong positive correlation\n        (2.0, -0.7, 50000),      # Happy path, negative correlation\n        (1.0, 0.99, 50000),      # Near-degenerate correlation\n    ]\n\n    results = []\n    for a, rho, N in test_cases:\n        ratio = variance_ratio(a, rho, N)\n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    # Use a compact float representation for consistency.\n    print(f\"[{','.join(f'{r:.12g}' for r in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3315571"}]}