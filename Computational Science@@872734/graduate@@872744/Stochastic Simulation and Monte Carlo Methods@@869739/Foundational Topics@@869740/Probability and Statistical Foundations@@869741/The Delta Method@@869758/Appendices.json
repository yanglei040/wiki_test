{"hands_on_practices": [{"introduction": "The delta method is a cornerstone of asymptotic statistics, but its power comes from a simple and elegant combination of the Central Limit Theorem and Taylor's theorem. This first exercise takes you back to these foundations. By deriving the variance approximation and the leading-order bias for a logarithm-transformed estimator from first principles, you will solidify your understanding of why and how the delta method works, and see its direct connection to the local linearity of smooth functions [@problem_id:3352081].", "problem": "Consider a Monte Carlo setting where a positive quantity is estimated by sample averaging. Let $\\{X_{i}\\}_{i=1}^{n}$ be independent and identically distributed random variables, and let $h(X)$ be a measurable, almost surely positive integrand with finite variance. Define the positive parameter $\\theta \\equiv \\mathbb{E}[h(X)] > 0$ and its Monte Carlo estimator $\\hat{\\theta}_{n} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} h(X_{i})$. Denote $\\sigma_{h}^{2} \\equiv \\operatorname{Var}(h(X)) \\in (0,\\infty)$. Let $g(x) = \\ln x$ be the natural logarithm.\n\nStarting only from the Central Limit Theorem (CLT) and a second-order Taylor expansion, and without invoking any pre-stated delta-method formulas, do the following:\n\n- Derive a first-order approximation to $\\operatorname{Var}(g(\\hat{\\theta}_{n}))$ in closed form as $n \\to \\infty$, expressed solely in terms of $\\theta$, $\\sigma_{h}^{2}$, and $n$.\n- Explain how the curvature of the logarithm (its second derivative) determines the sign and leading order of the bias of $g(\\hat{\\theta}_{n})$ as an estimator of $g(\\theta)$.\n\nYour final answer must be a single closed-form analytic expression for the first-order approximation to $\\operatorname{Var}(g(\\hat{\\theta}_{n}))$ in terms of $\\theta$, $\\sigma_{h}^{2}$, and $n$. No numerical evaluation is required. Do not include any units.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective, situated within the standard framework of statistical inference and Monte Carlo methods. All necessary information is provided, and the problem is free of contradictions or ambiguities.\n\nThis problem requires two main tasks: first, to derive a first-order approximation for the variance of a transformed Monte Carlo estimator, and second, to explain the bias of this transformed estimator.\n\nLet the sequence of random variables $\\{h(X_{i})\\}_{i=1}^{n}$ be denoted by $\\{Y_{i}\\}_{i=1}^{n}$. By the problem statement, these are independent and identically distributed (i.i.d.) random variables with expectation $\\mathbb{E}[Y_{i}] = \\theta$ and variance $\\operatorname{Var}(Y_{i}) = \\sigma_{h}^{2}$. The Monte Carlo estimator is the sample mean $\\hat{\\theta}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{i}$.\n\nThe expectation of the estimator is $\\mathbb{E}[\\hat{\\theta}_{n}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} Y_{i}\\right] = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}[Y_{i}] = \\frac{1}{n}(n\\theta) = \\theta$, which shows that $\\hat{\\theta}_{n}$ is an unbiased estimator of $\\theta$.\nThe variance of the estimator, due to the independence of the $Y_i$, is $\\operatorname{Var}(\\hat{\\theta}_{n}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} Y_{i}\\right) = \\frac{1}{n^{2}}\\sum_{i=1}^{n} \\operatorname{Var}(Y_{i}) = \\frac{1}{n^{2}}(n\\sigma_{h}^{2}) = \\frac{\\sigma_{h}^{2}}{n}$.\n\nAccording to the Central Limit Theorem (CLT), as $n \\to \\infty$, the distribution of the normalized sample mean converges to a standard normal distribution. A common statement of the CLT is:\n$$ \\sqrt{n}(\\hat{\\theta}_{n} - \\theta) \\Rightarrow \\mathcal{N}(0, \\sigma_{h}^{2}) $$\nwhere $\\Rightarrow$ denotes convergence in distribution. This implies that for large $n$, $\\hat{\\theta}_{n}$ is approximately normally distributed, with mean $\\theta$ and variance $\\frac{\\sigma_{h}^{2}}{n}$.\n\n**Part 1: Derivation of the Variance Approximation**\n\nWe are asked to find an approximation for $\\operatorname{Var}(g(\\hat{\\theta}_{n}))$, where $g(x) = \\ln x$. The derivation must start from first principles, namely the CLT and a Taylor expansion. We perform a first-order Taylor series expansion of the function $g(\\hat{\\theta}_{n})$ around the point $\\theta = \\mathbb{E}[\\hat{\\theta}_{n}]$.\nA first-order expansion is given by:\n$$ g(\\hat{\\theta}_{n}) = g(\\theta) + g'(\\theta)(\\hat{\\theta}_{n} - \\theta) + R_{1} $$\nwhere $R_{1}$ is the remainder term, which is of a smaller order than the linear term for $\\hat{\\theta}_{n}$ close to $\\theta$. For $g(x) = \\ln x$, the first derivative is $g'(x) = \\frac{1}{x}$. Evaluating this at $x = \\theta$ gives $g'(\\theta) = \\frac{1}{\\theta}$.\nThus, the first-order approximation is:\n$$ g(\\hat{\\theta}_{n}) \\approx g(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta) $$\nWe are interested in the variance of this expression. Using the properties of variance, where $\\operatorname{Var}(aZ+b) = a^{2}\\operatorname{Var}(Z)$ for a constant $a$ and a random variable $Z$:\n$$ \\operatorname{Var}(g(\\hat{\\theta}_{n})) \\approx \\operatorname{Var}\\left(g(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta)\\right) $$\nSince $g(\\theta)=\\ln(\\theta)$ and $\\theta$ are constants, they do not contribute to the variance. The expression simplifies to:\n$$ \\operatorname{Var}(g(\\hat{\\theta}_{n})) \\approx \\operatorname{Var}\\left(\\frac{1}{\\theta}\\hat{\\theta}_{n}\\right) = \\left(\\frac{1}{\\theta}\\right)^{2} \\operatorname{Var}(\\hat{\\theta}_{n}) $$\nSubstituting the known variance $\\operatorname{Var}(\\hat{\\theta}_{n}) = \\frac{\\sigma_{h}^{2}}{n}$:\n$$ \\operatorname{Var}(g(\\hat{\\theta}_{n})) \\approx \\frac{1}{\\theta^{2}} \\left( \\frac{\\sigma_{h}^{2}}{n} \\right) = \\frac{\\sigma_{h}^{2}}{n\\theta^{2}} $$\nThis is the required first-order approximation for the variance of $g(\\hat{\\theta}_{n})$ as $n \\to \\infty$.\n\n**Part 2: Bias and the Role of Curvature**\n\nTo analyze the bias of $g(\\hat{\\theta}_{n})$ as an estimator of $g(\\theta) = \\ln \\theta$, we must approximate its expectation, $\\mathbb{E}[g(\\hat{\\theta}_{n})]$. A first-order Taylor expansion is insufficient, as $\\mathbb{E}[\\hat{\\theta}_{n} - \\theta] = 0$. We must proceed to the second-order Taylor expansion:\n$$ g(\\hat{\\theta}_{n}) = g(\\theta) + g'(\\theta)(\\hat{\\theta}_{n} - \\theta) + \\frac{1}{2}g''(\\theta)(\\hat{\\theta}_{n} - \\theta)^{2} + R_{2} $$\nFor $g(x) = \\ln x$, the second derivative is $g''(x) = -\\frac{1}{x^{2}}$. Evaluating at $x = \\theta$ gives $g''(\\theta) = -\\frac{1}{\\theta^{2}}$. Substituting the derivatives into the expansion yields:\n$$ g(\\hat{\\theta}_{n}) \\approx \\ln(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta) - \\frac{1}{2\\theta^{2}}(\\hat{\\theta}_{n} - \\theta)^{2} $$\nTaking the expectation of both sides and using the linearity of expectation:\n$$ \\mathbb{E}[g(\\hat{\\theta}_{n})] \\approx \\mathbb{E}\\left[\\ln(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta) - \\frac{1}{2\\theta^{2}}(\\hat{\\theta}_{n} - \\theta)^{2}\\right] $$\n$$ \\mathbb{E}[g(\\hat{\\theta}_{n})] \\approx \\ln(\\theta) + \\frac{1}{\\theta}\\mathbb{E}[\\hat{\\theta}_{n} - \\theta] - \\frac{1}{2\\theta^{2}}\\mathbb{E}[(\\hat{\\theta}_{n} - \\theta)^{2}] $$\nWe evaluate the expectation terms:\n- The first term is a constant: $\\mathbb{E}[\\ln(\\theta)] = \\ln(\\theta)$.\n- The second term is zero because $\\hat{\\theta}_{n}$ is an unbiased estimator of $\\theta$: $\\mathbb{E}[\\hat{\\theta}_{n} - \\theta] = \\mathbb{E}[\\hat{\\theta}_{n}] - \\theta = \\theta - \\theta = 0$.\n- The third term's expectation is the definition of the variance of $\\hat{\\theta}_{n}$: $\\mathbb{E}[(\\hat{\\theta}_{n} - \\theta)^{2}] = \\operatorname{Var}(\\hat{\\theta}_{n}) = \\frac{\\sigma_{h}^{2}}{n}$.\n\nSubstituting these results back into the approximation for the expectation:\n$$ \\mathbb{E}[g(\\hat{\\theta}_{n})] \\approx \\ln(\\theta) - \\frac{1}{2\\theta^{2}}\\left(\\frac{\\sigma_{h}^{2}}{n}\\right) $$\nThe bias of an estimator is defined as $\\operatorname{Bias}(g(\\hat{\\theta}_{n})) = \\mathbb{E}[g(\\hat{\\theta}_{n})] - g(\\theta)$. Therefore, the leading-order term for the bias is:\n$$ \\operatorname{Bias}(g(\\hat{\\theta}_{n})) \\approx -\\frac{\\sigma_{h}^{2}}{2n\\theta^{2}} $$\nThe bias is proportional to the second derivative of the function $g$, which represents its curvature. For a general function $g$, the leading-order bias term is $\\frac{1}{2}g''(\\theta)\\operatorname{Var}(\\hat{\\theta}_{n})$. The sign of the bias is thus determined by the sign of the curvature $g''(\\theta)$. For the natural logarithm function, $g''(x) = -1/x^{2}$, which is strictly negative for all $x$ in its domain $(0, \\infty)$. Since $\\theta > 0$, the curvature at $\\theta$, $g''(\\theta) = -1/\\theta^{2}$, is negative. Given that $\\sigma_{h}^{2} > 0$ and $n \\ge 1$, the entire bias term is negative.\nThis demonstrates that the concavity of the logarithm function (negative curvature) causes the estimator $\\ln(\\hat{\\theta}_{n})$ to systematically underestimate the true value $\\ln(\\theta)$. This result is a quantitative refinement of Jensen's inequality, which for a concave function $g$ states that $\\mathbb{E}[g(Z)] \\le g(\\mathbb{E}[Z])$. Here, with $Z=\\hat{\\theta}_{n}$, we get $\\mathbb{E}[\\ln(\\hat{\\theta}_{n})] \\le \\ln(\\mathbb{E}[\\hat{\\theta}_{n}]) = \\ln(\\theta)$. The Taylor expansion quantifies this inequality to leading order in $1/n$.", "answer": "$$\\boxed{\\frac{\\sigma_{h}^{2}}{n\\theta^{2}}}$$", "id": "3352081"}, {"introduction": "Real-world applications of stochastic simulation, especially in Bayesian inference, often involve estimators derived from correlated sequences, such as the output of a Markov Chain Monte Carlo (MCMC) algorithm. This practice extends the delta method to this crucial setting. You will analyze a transformed posterior mean where the samples are not independent, requiring a careful application of the Markov Chain Central Limit Theorem to determine the correct asymptotic variance before applying the delta method's logic [@problem_id:3352103]. This exercise demonstrates the method's versatility beyond the i.i.d. world.", "problem": "Consider a Bayesian model in which the posterior distribution of a positive scalar parameter $\\theta$ is $\\operatorname{Gamma}(\\alpha,\\beta)$ with shape $\\alpha>0$ and rate $\\beta>0$. A Markov Chain Monte Carlo (MCMC) procedure produces a strictly stationary, ergodic Markov chain $\\{\\theta_t\\}_{t=1}^{\\infty}$ with invariant distribution equal to the posterior, and the posterior mean is $\\theta=\\mathbb{E}_{\\pi}[\\theta_t]$. Define the Monte Carlo estimator of the posterior mean by $\\hat{\\theta}_n = \\frac{1}{n}\\sum_{t=1}^{n}\\theta_t$. Assume the Markov Chain Central Limit Theorem (CLT) holds for $\\hat{\\theta}_n$, and that the autocorrelation function of $\\{\\theta_t\\}$ under stationarity is geometric, that is, for all integers $k\\geq 0$,\n$$\n\\rho_k = \\operatorname{Corr}(\\theta_t, \\theta_{t+k}) = c^k\n$$\nfor a fixed constant $c\\in(0,1)$.\n\nYou are tasked to analyze the reparameterized estimator $g(\\hat{\\theta}_n)=\\log(\\hat{\\theta}_n)$. Using only the definition of the Markov Chain Central Limit Theorem (CLT) in terms of autocovariances and a first-order Taylor expansion around the posterior mean, derive the asymptotic variance constant for the normalized error $\\sqrt{n}\\,\\big(g(\\hat{\\theta}_n)-g(\\theta)\\big)$, expressed as a single closed-form analytic expression in terms of $\\alpha$ and $c$ only. Do not introduce any auxiliary numerical approximations. Provide your final answer as an analytic expression. No rounding is required, and no units are involved.", "solution": "The problem asks for the asymptotic variance constant for the normalized error of a log-transformed Monte Carlo estimator of a posterior mean. Let us denote the parameter of interest by the random variable $\\Theta$. Its posterior distribution is given as $\\pi(\\Theta) = \\operatorname{Gamma}(\\alpha, \\beta)$, with shape $\\alpha > 0$ and rate $\\beta > 0$.\n\nThe problem states that the posterior mean is denoted by $\\theta$. For a $\\operatorname{Gamma}(\\alpha, \\beta)$ distribution, the mean is given by:\n$$\n\\theta = \\mathbb{E}[\\Theta] = \\frac{\\alpha}{\\beta}\n$$\nThe variance of this distribution is:\n$$\n\\operatorname{Var}(\\Theta) = \\frac{\\alpha}{\\beta^2}\n$$\nWe are given a stationary and ergodic Markov chain $\\{\\theta_t\\}_{t=1}^{\\infty}$ whose invariant distribution is this posterior. The Monte Carlo estimator for the posterior mean $\\theta$ is given by $\\hat{\\theta}_n = \\frac{1}{n}\\sum_{t=1}^{n}\\theta_t$.\n\nThe problem assumes the Markov Chain Central Limit Theorem (CLT) holds for $\\hat{\\theta}_n$. This theorem states that the distribution of the normalized error converges to a normal distribution:\n$$\n\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\Rightarrow \\mathcal{N}(0, \\sigma^2_{asy})\n$$\nwhere $\\Rightarrow$ denotes convergence in distribution, and $\\sigma^2_{asy}$ is the asymptotic variance constant. The task is to find the corresponding asymptotic variance constant for the transformed estimator $g(\\hat{\\theta}_n)$, where $g(x) = \\log(x)$. We assume $\\log(x)$ denotes the natural logarithm, $\\ln(x)$, as is standard convention in theoretical statistics and calculus.\n\nFirst, we must determine $\\sigma^2_{asy}$. For a stationary Markov chain, the asymptotic variance is given by the sum of all autocovariances:\n$$\n\\sigma^2_{asy} = \\sum_{k=-\\infty}^{\\infty} \\operatorname{Cov}(\\theta_t, \\theta_{t+k})\n$$\nLet $\\gamma_k = \\operatorname{Cov}(\\theta_t, \\theta_{t+k})$ be the autocovariance at lag $k$. Due to stationarity, $\\gamma_k = \\gamma_{-k}$. Thus, the expression can be written as:\n$$\n\\sigma^2_{asy} = \\gamma_0 + 2\\sum_{k=1}^{\\infty} \\gamma_k\n$$\nwhere $\\gamma_0 = \\operatorname{Var}(\\theta_t) = \\operatorname{Var}(\\Theta) = \\frac{\\alpha}{\\beta^2}$. The autocorrelation function $\\rho_k$ is defined as $\\rho_k = \\frac{\\gamma_k}{\\gamma_0}$, so $\\gamma_k = \\rho_k \\gamma_0$. The problem provides that $\\rho_k = c^k$ for a constant $c \\in (0,1)$ and for all integers $k \\geq 0$.\n\nSubstituting these into the expression for $\\sigma^2_{asy}$:\n$$\n\\sigma^2_{asy} = \\gamma_0 + 2\\sum_{k=1}^{\\infty} (c^k \\gamma_0) = \\gamma_0 \\left(1 + 2\\sum_{k=1}^{\\infty} c^k\\right)\n$$\nThe summation term is an infinite geometric series: $\\sum_{k=1}^{\\infty} c^k = \\frac{c}{1-c}$, since $|c| < 1$.\nSubstituting this result back into the expression for $\\sigma^2_{asy}$:\n$$\n\\sigma^2_{asy} = \\gamma_0 \\left(1 + 2\\frac{c}{1-c}\\right) = \\frac{\\alpha}{\\beta^2} \\left(\\frac{1-c+2c}{1-c}\\right) = \\frac{\\alpha}{\\beta^2} \\left(\\frac{1+c}{1-c}\\right)\n$$\nNow, we analyze the transformed estimator $g(\\hat{\\theta}_n) = \\ln(\\hat{\\theta}_n)$. The problem requires using a first-order Taylor expansion of $g(\\hat{\\theta}_n)$ around the true posterior mean $\\theta$.\n$$\ng(\\hat{\\theta}_n) \\approx g(\\theta) + g'(\\theta)(\\hat{\\theta}_n - \\theta)\n$$\nwhere $g'(\\theta)$ is the derivative of $g(x)$ evaluated at $x=\\theta$.\nRearranging the terms, we have:\n$$\ng(\\hat{\\theta}_n) - g(\\theta) \\approx g'(\\theta)(\\hat{\\theta}_n - \\theta)\n$$\nMultiplying by $\\sqrt{n}$:\n$$\n\\sqrt{n}\\big(g(\\hat{\\theta}_n) - g(\\theta)\\big) \\approx g'(\\theta) \\sqrt{n}(\\hat{\\theta}_n - \\theta)\n$$\nThis approximation, known as the Delta Method, implies that the asymptotic distribution of the left-hand side is the same as the distribution of the right-hand side. Since $\\sqrt{n}(\\hat{\\theta}_n - \\theta)$ converges to a normal distribution with mean $0$ and variance $\\sigma^2_{asy}$, the right-hand side converges to a normal distribution with mean $g'(\\theta) \\times 0 = 0$ and variance $(g'(\\theta))^2 \\sigma^2_{asy}$.\n\nTherefore, the asymptotic variance constant for the normalized error $\\sqrt{n}(g(\\hat{\\theta}_n) - g(\\theta))$, let's call it $V_g$, is:\n$$\nV_g = (g'(\\theta))^2 \\sigma^2_{asy}\n$$\nThe function is $g(x) = \\ln(x)$, so its derivative is $g'(x) = \\frac{1}{x}$. We evaluate this at the posterior mean $\\theta = \\frac{\\alpha}{\\beta}$:\n$$\ng'(\\theta) = \\frac{1}{\\theta} = \\frac{1}{\\alpha/\\beta} = \\frac{\\beta}{\\alpha}\n$$\nNow we can substitute the expressions for $g'(\\theta)$ and $\\sigma^2_{asy}$ into the formula for $V_g$:\n$$\nV_g = \\left(\\frac{\\beta}{\\alpha}\\right)^2 \\left( \\frac{\\alpha}{\\beta^2} \\left(\\frac{1+c}{1-c}\\right) \\right)\n$$\n$$\nV_g = \\frac{\\beta^2}{\\alpha^2} \\cdot \\frac{\\alpha}{\\beta^2} \\cdot \\frac{1+c}{1-c}\n$$\nThe terms $\\beta^2$ in the numerator and denominator cancel out. Similarly, one factor of $\\alpha$ cancels.\n$$\nV_g = \\frac{1}{\\alpha} \\cdot \\frac{1+c}{1-c}\n$$\nThis is the final expression for the asymptotic variance constant, expressed solely in terms of $\\alpha$ and $c$, as required.", "answer": "$$\n\\boxed{\\frac{1}{\\alpha}\\frac{1+c}{1-c}}\n$$", "id": "3352103"}, {"introduction": "While many textbook problems feature functions with clean, analytical derivatives, practical statistical modeling often involves complex transformations where the Jacobian is intractable. This hands-on coding exercise bridges the gap between analytical theory and computational practice. You will implement a numerical delta method, estimating a complex Jacobian using Monte Carlo integration and finite differences, and then quantify the approximation error against an analytical solution [@problem_id:3352079]. This practice is essential for developing the skills to apply the delta method in realistic, research-level scenarios where purely analytical solutions are out of reach.", "problem": "Let $\\theta \\in \\mathbb{R}^2$ denote a parameter vector with coordinates $\\theta = (\\mu,\\sigma)$, where $\\mu \\in \\mathbb{R}$ and $\\sigma \\in (0,\\infty)$. Consider the mapping $g:\\mathbb{R}^2 \\to \\mathbb{R}^2$ defined by\n$$\ng(\\theta)\n=\n\\begin{bmatrix}\n\\mathbb{E}_{\\theta}[X^3] \\\\\n\\mathbb{E}_{\\theta}[\\sin(X)]\n\\end{bmatrix},\n\\quad\nX \\sim \\mathcal{N}(\\mu,\\sigma^2),\n$$\nwith the angle of the sine function in radians.\n\nSuppose $\\hat{\\theta}$ is an estimator of $\\theta$ such that, for large sample size, its distribution is approximately multivariate normal with mean $\\theta$ and covariance matrix $V_{\\theta}$ (a given, fixed $2 \\times 2$ positive semidefinite matrix).\n\nYou will implement a fully numerical Monte Carlo procedure to estimate the Jacobian $J_g(\\theta)$ (the $2 \\times 2$ matrix of first partial derivatives of $g$ at $\\theta$) using finite differences where the mapping $g$ is itself evaluated by Monte Carlo integration. You will then use this estimated Jacobian in a delta method variance calculation and quantify the effect of Jacobian approximation error on the resulting delta method covariance estimate.\n\nFundamental base to be used:\n- The mapping $g(\\theta)$ is defined by expectations under a normal law and admits Monte Carlo approximations as sample averages by the Law of Large Numbers.\n- The Jacobian $J_g(\\theta)$ can be approximated by symmetric finite differences with step size $h$.\n- The delta method arises from a first-order Taylor expansion of $g(\\hat{\\theta})$ around $\\theta$ and the asymptotic normality of $\\hat{\\theta}$.\n\nYour tasks are:\n1. Implement a Monte Carlo estimator of $g(\\theta)$ for any $\\theta = (\\mu,\\sigma)$ as follows. Draw $M$ independent standard normal samples $Z_1,\\dots,Z_M \\sim \\mathcal{N}(0,1)$, and define $X_i(\\mu,\\sigma) = \\mu + \\sigma Z_i$ for $i \\in \\{1,\\dots,M\\}$. Approximate $g(\\theta)$ by the sample averages\n$$\n\\widehat{g}_1(\\theta) = \\frac{1}{M}\\sum_{i=1}^M X_i(\\mu,\\sigma)^3, \n\\quad\n\\widehat{g}_2(\\theta) = \\frac{1}{M}\\sum_{i=1}^M \\sin\\bigl(X_i(\\mu,\\sigma)\\bigr).\n$$\n2. Implement a symmetric finite-difference approximation to each column of $J_g(\\theta)$ using the same underlying standard normal draws to reduce variance (common random numbers). For the $\\mu$-direction, use\n$$\n\\widehat{J}_{\\cdot,1}(\\theta;h,M)\n=\n\\frac{\\widehat{g}(\\mu+h,\\sigma) - \\widehat{g}(\\mu-h,\\sigma)}{2h},\n$$\nand for the $\\sigma$-direction, use\n$$\n\\widehat{J}_{\\cdot,2}(\\theta;h,M)\n=\n\\frac{\\widehat{g}(\\mu,\\sigma+h) - \\widehat{g}(\\mu,\\sigma-h)}{2h}.\n$$\nUse the same $Z_1,\\dots,Z_M$ to compute all four evaluations $\\widehat{g}(\\mu\\pm h,\\sigma)$ and $\\widehat{g}(\\mu,\\sigma\\pm h)$.\n3. Given $V_{\\theta}$, construct the delta method covariance estimate\n$$\n\\widehat{\\Sigma}_g(\\theta;h,M) = \\widehat{J}_g(\\theta;h,M)\\, V_{\\theta}\\, \\widehat{J}_g(\\theta;h,M)^{\\top}.\n$$\n4. For assessment, derive the analytic Jacobian $J_g(\\theta)$ and the corresponding analytic delta method covariance\n$$\n\\Sigma_g(\\theta) = J_g(\\theta)\\, V_{\\theta}\\, J_g(\\theta)^{\\top}.\n$$\n5. For each test case specified below, compute the relative Frobenius error\n$$\n\\mathrm{Err}(\\theta,h,M,V_{\\theta})\n=\n\\frac{\\left\\| \\widehat{\\Sigma}_g(\\theta;h,M) - \\Sigma_g(\\theta) \\right\\|_F}{\\left\\| \\Sigma_g(\\theta) \\right\\|_F}.\n$$\n\nAngle unit requirement: all trigonometric arguments are in radians.\n\nTest suite:\n- Case $1$: $\\mu = 0.5$, $\\sigma = 1.2$, \n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.04 & 0.01 \\\\\n0.01 & 0.09\n\\end{bmatrix},\n$$\n$h = 10^{-3}$, $M = 50000$, seed $= 12345$.\n- Case $2$: $\\mu = 0.5$, $\\sigma = 1.2$, \n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.04 & 0.01 \\\\\n0.01 & 0.09\n\\end{bmatrix},\n$$\n$h = 10^{-1}$, $M = 50000$, seed $= 67890$.\n- Case $3$: $\\mu = 0.5$, $\\sigma = 1.2$, \n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.04 & 0.01 \\\\\n0.01 & 0.09\n\\end{bmatrix},\n$$\n$h = 10^{-5}$, $M = 2000$, seed $= 13579$.\n- Case $4$: $\\mu = 1.5$, $\\sigma = 0.5$, \n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.01 & -0.003 \\\\\n-0.003 & 0.02\n\\end{bmatrix},\n$$\n$h = 5 \\times 10^{-4}$, $M = 200000$, seed $= 24680$.\n\nRequired final output format: Your program should produce a single line of output containing the four relative errors for the four cases as a comma-separated list enclosed in square brackets, for example\n$$\n[\\mathrm{Err}_1,\\mathrm{Err}_2,\\mathrm{Err}_3,\\mathrm{Err}_4].\n$$\nEach entry must be a real number.", "solution": "The problem has been validated and is deemed sound. It is a well-posed, scientifically grounded problem in computational statistics that is free of contradictions, ambiguities, or pseudo-scientific claims. All necessary data and definitions for a unique, reproducible solution are provided.\n\nThe primary objective is to quantify the error in the delta method's covariance matrix approximation when the underlying Jacobian is estimated numerically. This involves a two-layer Monte Carlo procedure: the function $g(\\theta)$ is itself an expectation estimated by Monte Carlo, and its Jacobian $J_g(\\theta)$ is then approximated using finite differences on this Monte Carlo estimator. The error analysis compares this fully numerical result against the analytical ground truth.\n\nThe solution proceeds in the following steps:\n1.  Derivation of the analytic expressions for the mapping $g(\\theta)$ and its Jacobian $J_g(\\theta)$.\n2.  Implementation of the numerical estimators for $g(\\theta)$ and $J_g(\\theta)$ as specified.\n3.  Computation of both the analytic and estimated delta method covariance matrices.\n4.  Calculation of the specified relative Frobenius error for each test case.\n\n**1. Analytic Derivations**\n\nLet $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The parameter vector is $\\theta = (\\mu, \\sigma)$. The mapping is $g(\\theta) = [\\mathbb{E}_{\\theta}[X^3], \\mathbb{E}_{\\theta}[\\sin(X)]]^{\\top}$.\n\nFirst component, $g_1(\\theta) = \\mathbb{E}[X^3]$:\nThe third central moment of a normal distribution is $0$. The relationship between raw moments $m_k = \\mathbb{E}[X^k]$ and central moments is given by the centered variable $X-\\mu$. A more direct approach is to use the moment-generating function $M_X(t) = \\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$ or to expand $X=\\mu+\\sigma Z$ where $Z \\sim \\mathcal{N}(0,1)$.\nUsing the latter, $\\mathbb{E}[X^3] = \\mathbb{E}[(\\mu + \\sigma Z)^3] = \\mathbb{E}[\\mu^3 + 3\\mu^2\\sigma Z + 3\\mu\\sigma^2 Z^2 + \\sigma^3 Z^3]$.\nBy linearity of expectation and using the moments of the standard normal distribution $\\mathbb{E}[Z]=0$, $\\mathbb{E}[Z^2]=1$, and $\\mathbb{E}[Z^3]=0$, we get:\n$g_1(\\mu, \\sigma) = \\mu^3 + 3\\mu\\sigma^2 \\mathbb{E}[Z^2] = \\mu^3 + 3\\mu\\sigma^2$.\n\nSecond component, $g_2(\\theta) = \\mathbb{E}[\\sin(X)]$:\nWe compute this by considering the characteristic function of $X$, $\\phi_X(t) = \\mathbb{E}[e^{itX}] = \\exp(i\\mu t - \\frac{1}{2}\\sigma^2 t^2)$.\nUsing Euler's formula, $\\sin(X) = \\mathrm{Im}(e^{iX})$, and the linearity of expectation:\n$\\mathbb{E}[\\sin(X)] = \\mathbb{E}[\\mathrm{Im}(e^{iX})] = \\mathrm{Im}(\\mathbb{E}[e^{iX}])$.\n$\\mathbb{E}[e^{iX}]$ is $\\phi_X(1) = \\exp(i\\mu - \\frac{1}{2}\\sigma^2) = e^{-\\sigma^2/2}e^{i\\mu} = e^{-\\sigma^2/2}(\\cos(\\mu) + i\\sin(\\mu))$.\nThe imaginary part is:\n$g_2(\\mu, \\sigma) = \\sin(\\mu) e^{-\\sigma^2/2}$.\n\nThe analytic mapping is:\n$g(\\theta) = \\begin{bmatrix} \\mu^3 + 3\\mu\\sigma^2 \\\\ \\sin(\\mu) e^{-\\sigma^2/2} \\end{bmatrix}$.\n\nThe analytic Jacobian $J_g(\\theta)$ is the matrix of first partial derivatives of $g$ with respect to $\\mu$ and $\\sigma$:\n$J_g(\\theta) = \\begin{bmatrix} \\frac{\\partial g_1}{\\partial \\mu} & \\frac{\\partial g_1}{\\partial \\sigma} \\\\ \\frac{\\partial g_2}{\\partial \\mu} & \\frac{\\partial g_2}{\\partial \\sigma} \\end{bmatrix}$.\nThe partial derivatives are:\n$\\frac{\\partial g_1}{\\partial \\mu} = 3\\mu^2 + 3\\sigma^2$\n$\\frac{\\partial g_1}{\\partial \\sigma} = 6\\mu\\sigma$\n$\\frac{\\partial g_2}{\\partial \\mu} = \\cos(\\mu) e^{-\\sigma^2/2}$\n$\\frac{\\partial g_2}{\\partial \\sigma} = \\sin(\\mu) \\cdot e^{-\\sigma^2/2} \\cdot (-\\sigma) = -\\sigma \\sin(\\mu) e^{-\\sigma^2/2}$\n\nThus, the analytic Jacobian is:\n$$\nJ_g(\\theta) = \\begin{bmatrix} 3\\mu^2 + 3\\sigma^2 & 6\\mu\\sigma \\\\ \\cos(\\mu) e^{-\\sigma^2/2} & -\\sigma \\sin(\\mu) e^{-\\sigma^2/2} \\end{bmatrix}\n$$\n\n**2. Numerical Estimation Procedure**\n\nThe implementation will follow the specified numerical methods.\n\nA function, `g_hat(mu, sigma, Z)`, estimates $g(\\theta)$ for given parameters $\\mu, \\sigma$ and a pre-generated array of $M$ standard normal samples $Z = (Z_1, \\dots, Z_M)$. It computes $X_i = \\mu + \\sigma Z_i$ and then returns the sample means of $X_i^3$ and $\\sin(X_i)$.\n\nA second function, `J_hat(mu, sigma, h, Z)`, estimates the Jacobian $J_g(\\theta)$. It uses the symmetric finite difference formulas provided. Crucially, it employs the common random numbers (CRN) technique by passing the same array of samples `Z` to each call of `g_hat`:\n$$\n\\widehat{J}_{\\cdot,1} = \\frac{\\text{g\\_hat}(\\mu+h, \\sigma, Z) - \\text{g\\_hat}(\\mu-h, \\sigma, Z)}{2h}\n$$\n$$\n\\widehat{J}_{\\cdot,2} = \\frac{\\text{g\\_hat}(\\mu, \\sigma+h, Z) - \\text{g\\_hat}(\\mu, \\sigma-h, Z)}{2h}\n$$\nThese two column vectors are then assembled into the $2 \\times 2$ estimated Jacobian matrix $\\widehat{J}_g(\\theta; h, M)$.\n\n**3. Delta Method and Error Calculation**\n\nFor each test case, we compute two covariance matrices:\nThe analytic delta method covariance: $\\Sigma_g(\\theta) = J_g(\\theta)\\, V_{\\theta}\\, J_g(\\theta)^{\\top}$, using the analytically derived $J_g(\\theta)$.\nThe estimated delta method covariance: $\\widehat{\\Sigma}_g(\\theta;h,M) = \\widehat{J}_g(\\theta;h,M)\\, V_{\\theta}\\, \\widehat{J}_g(\\theta;h,M)^{\\top}$, using the numerically estimated $\\widehat{J}_g$.\n\nThe final error is calculated as the relative Frobenius norm of the difference between these two matrices:\n$$\n\\mathrm{Err} = \\frac{\\left\\| \\widehat{\\Sigma}_g(\\theta;h,M) - \\Sigma_g(\\theta) \\right\\|_F}{\\left\\| \\Sigma_g(\\theta) \\right\\|_F}\n$$\nwhere $\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}$.\n\n**4. Execution for Test Cases**\n\nA loop iterates through each of the four test cases. In each iteration:\n1.  The parameters $(\\mu, \\sigma, V_{\\theta}, h, M, \\text{seed})$ are extracted.\n2.  The random number generator is seeded.\n3.  $M$ standard normal samples are drawn.\n4.  $\\Sigma_g(\\theta)$ and $\\widehat{\\Sigma}_g(\\theta;h,M)$ are computed as described above.\n5.  The relative error $\\mathrm{Err}$ is calculated and stored.\n\nFinally, the list of four error values is formatted and printed as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef g_hat_mc(mu, sigma, z_samples):\n    \"\"\"\n    Computes the Monte Carlo estimate of the mapping g(theta).\n    \n    Args:\n        mu (float): The mean parameter.\n        sigma (float): The standard deviation parameter.\n        z_samples (np.ndarray): An array of standard normal random samples.\n    \n    Returns:\n        np.ndarray: A 2-element array containing the estimates of E[X^3] and E[sin(X)].\n    \"\"\"\n    x_samples = mu + sigma * z_samples\n    g1_hat = np.mean(x_samples**3)\n    g2_hat = np.mean(np.sin(x_samples))  # np.sin uses radians\n    return np.array([g1_hat, g2_hat])\n\ndef J_g_hat_mc(mu, sigma, h, z_samples):\n    \"\"\"\n    Computes the finite-difference estimate of the Jacobian of g(theta)\n    using common random numbers.\n    \n    Args:\n        mu (float): The mean parameter.\n        sigma (float): The standard deviation parameter.\n        h (float): The finite difference step size.\n        z_samples (np.ndarray): An array of standard normal random samples.\n    \n    Returns:\n        np.ndarray: A 2x2 matrix representing the estimated Jacobian.\n    \"\"\"\n    # First column: partial derivatives with respect to mu\n    g_plus_mu = g_hat_mc(mu + h, sigma, z_samples)\n    g_minus_mu = g_hat_mc(mu - h, sigma, z_samples)\n    J_col1 = (g_plus_mu - g_minus_mu) / (2 * h)\n    \n    # Second column: partial derivatives with respect to sigma\n    g_plus_sigma = g_hat_mc(mu, sigma + h, z_samples)\n    g_minus_sigma = g_hat_mc(mu, sigma - h, z_samples)\n    J_col2 = (g_plus_sigma - g_minus_sigma) / (2 * h)\n    \n    # Assemble the Jacobian matrix from its columns\n    return np.stack([J_col1, J_col2], axis=1)\n\ndef J_g_analytic(mu, sigma):\n    \"\"\"\n    Computes the analytic Jacobian of g(theta).\n    \n    Args:\n        mu (float): The mean parameter.\n        sigma (float): The standard deviation parameter.\n    \n    Returns:\n        np.ndarray: A 2x2 matrix representing the analytic Jacobian.\n    \"\"\"\n    J11 = 3 * mu**2 + 3 * sigma**2\n    J12 = 6 * mu * sigma\n    \n    exp_term = np.exp(-0.5 * sigma**2)\n    J21 = np.cos(mu) * exp_term\n    J22 = -sigma * np.sin(mu) * exp_term\n    \n    return np.array([[J11, J12], [J21, J22]])\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute the errors.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Base case\n        {'mu': 0.5, 'sigma': 1.2, 'V_theta': [[0.04, 0.01], [0.01, 0.09]], 'h': 1e-3, 'M': 50000, 'seed': 12345},\n        # Case 2: Large finite-difference step h\n        {'mu': 0.5, 'sigma': 1.2, 'V_theta': [[0.04, 0.01], [0.01, 0.09]], 'h': 1e-1, 'M': 50000, 'seed': 67890},\n        # Case 3: Small Monte Carlo sample size M\n        {'mu': 0.5, 'sigma': 1.2, 'V_theta': [[0.04, 0.01], [0.01, 0.09]], 'h': 1e-5, 'M': 2000, 'seed': 13579},\n        # Case 4: Different parameter set, large M\n        {'mu': 1.5, 'sigma': 0.5, 'V_theta': [[0.01, -0.003], [-0.003, 0.02]], 'h': 5e-4, 'M': 200000, 'seed': 24680},\n    ]\n\n    results = []\n    for case in test_cases:\n        mu = case['mu']\n        sigma = case['sigma']\n        V_theta = np.array(case['V_theta'])\n        h = case['h']\n        M = case['M']\n        seed = case['seed']\n\n        # --- Analytic \"Ground Truth\" Calculation ---\n        J_analytic = J_g_analytic(mu, sigma)\n        Sigma_g_analytic = J_analytic @ V_theta @ J_analytic.T\n        \n        # --- Numerical Estimation ---\n        # 1. Set up the random number generator and draw samples\n        rng = np.random.default_rng(seed)\n        z_samples = rng.standard_normal(M)\n        \n        # 2. Estimate the Jacobian using Monte Carlo and finite differences\n        J_hat = J_g_hat_mc(mu, sigma, h, z_samples)\n        \n        # 3. Compute the estimated delta method covariance matrix\n        Sigma_g_hat = J_hat @ V_theta @ J_hat.T\n\n        # --- Error Quantification ---\n        # Compute the relative Frobenius error\n        numerator = np.linalg.norm(Sigma_g_hat - Sigma_g_analytic, 'fro')\n        denominator = np.linalg.norm(Sigma_g_analytic, 'fro')\n        \n        # Denominator should not be zero for the given test cases\n        if denominator == 0:\n            # This case is unlikely here but is good practice to handle.\n            # If numerator is also 0, error is 0. Otherwise, error is infinite.\n            relative_error = 0.0 if numerator == 0.0 else np.inf\n        else:\n            relative_error = numerator / denominator\n        \n        results.append(relative_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3352079"}]}