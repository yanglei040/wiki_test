{"hands_on_practices": [{"introduction": "While the mathematical definition of a discrete Cumulative Distribution Function (CDF) is a straightforward sum, its direct computation can be inefficient for distributions with many outcomes. This exercise challenges you to bridge the gap between probability theory and efficient algorithm design by developing a data structure that can evaluate a discrete CDF in logarithmic time [@problem_id:3304360]. Mastering this transition from a formal definition to a high-performance computational tool is a fundamental skill in building robust simulation systems.", "problem": "Consider a discrete random variable $X$ with a finite set of real-valued outcomes $\\{x_1, x_2, \\dots, x_n\\}$ and a Probability Mass Function (PMF) $p_X(x_i)$ such that $p_X(x_i) \\ge 0$ for all $i$ and $\\sum_{i=1}^n p_X(x_i) = 1$. The Cumulative Distribution Function (CDF) $F_X(x)$ is defined by the fundamental law $F_X(x) = \\mathbb{P}(X \\le x) = \\sum_{i: x_i \\le x} p_X(x_i)$. You must design a cumulative-sum data structure that evaluates $F_X(x)$ in $O(\\log n)$ time for arbitrary query values $x \\in \\mathbb{R}$, and analyze memory-query tradeoffs justified from first principles.\n\nStarting only from core definitions of PMF and CDF, propose and justify a data structure and algorithmic strategy that:\n- Constructs a cumulative-sum structure over the outcomes ordered by their real values, supporting $O(\\log n)$ queries for $F_X(x)$.\n- Optionally supports point updates of the PMF in $O(\\log n)$ time (where one outcome’s probability is increased by $\\Delta$ and another outcome’s probability is decreased by the same $\\Delta$ to preserve normalization $\\sum_i p_X(x_i) = 1$), and explains the implications for memory and query performance.\n\nYour program must implement your proposed data structure and produce numerical results for the following test suite. The distribution parameters and query values are given as explicit lists. All probabilities must be treated as decimals (no percentage sign), and there are no physical units in this problem.\n\nTest Suite:\n- Test case $1$ (general case): outcomes $[1.0,\\,2.5,\\,3.7,\\,5.0,\\,8.0]$, probabilities $[0.1,\\,0.2,\\,0.05,\\,0.25,\\,0.4]$, queries $[0.0,\\,1.0,\\,2.0,\\,2.5,\\,10.0]$.\n- Test case $2$ (boundary conditions and zeros): outcomes $[-5.0,\\,-1.0,\\,0.0,\\,7.0]$, probabilities $[0.0,\\,0.2,\\,0.3,\\,0.5]$, queries $[-10.0,\\,-5.0,\\,-1.0,\\,0.0,\\,7.0,\\,100.0]$.\n- Test case $3$ (edge case with structured distribution and a normalization-preserving update): outcomes $\\left[1^2,\\,2^2,\\,3^2,\\,\\dots,\\,16^2\\right] = [1,\\,4,\\,9,\\,16,\\,25,\\,36,\\,49,\\,64,\\,81,\\,100,\\,121,\\,144,\\,169,\\,196,\\,225,\\,256]$, probabilities $p_i = \\dfrac{i}{\\sum_{j=1}^{16} j}$ for $i \\in \\{1,2,\\dots,16\\}$, apply an update $\\Delta = 0.01$ that increases $p_1$ by $\\Delta$ and decreases $p_{16}$ by $\\Delta$ (resulting probabilities must remain nonnegative and still sum to $1$), queries $[0.0,\\,50.0,\\,100.0,\\,256.0]$.\n\nRequirements:\n- Your data structure must achieve $O(\\log n)$ time for each CDF query $F_X(x)$ by first locating the largest index $k$ such that $x_k \\le x$ via binary search over the sorted outcomes and then returning the cumulative probability mass up to $k$ using your cumulative-sum structure.\n- Your analysis must discuss, in principle, the tradeoff between memory and query/update performance when choosing between a static prefix-sum array, a Binary Indexed Tree (Fenwick tree), and a Segment Tree. Justify $O(\\log n)$ query complexity and memory bounds, starting from standard facts about cumulative sums and binary search.\n- Your program must embed the test suite and run without any external input. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the CDF values (as floating-point numbers) for all queries across all test cases, in the order they are specified above (first test case $1$ queries, then test case $2$, then test case $3$), i.e., output format: $[r_1, r_2, \\dots, r_m]$ where each $r_i$ is a floating-point number and $m$ is the total number of queries across all test cases.", "solution": "We begin from the fundamental definitions. For a discrete random variable $X$ with outcomes $\\{x_1, x_2, \\dots, x_n\\}$ and a Probability Mass Function (PMF) $p_X(x_i)$, the Cumulative Distribution Function (CDF) is defined by\n$$\nF_X(x) = \\mathbb{P}(X \\le x) = \\sum_{i: x_i \\le x} p_X(x_i).\n$$\nIf the outcomes are sorted in ascending order so that $x_1 \\le x_2 \\le \\dots \\le x_n$, then for any query value $x$, the evaluation of $F_X(x)$ requires locating the largest index $k$ such that $x_k \\le x$, and then computing the partial sum $\\sum_{i=1}^k p_X(x_i)$.\n\nA baseline approach evaluates $k$ in $O(\\log n)$ time using binary search over the sorted outcomes and then computes $\\sum_{i=1}^k p_X(x_i)$ in $O(n)$ time by scanning. This yields $O(n)$ per query, which is suboptimal. To improve to $O(\\log n)$ per query, we need a cumulative-sum data structure that supports prefix sums in $O(\\log n)$ time (or better), so that the overall complexity is dominated by the binary search to locate $k$ plus the prefix sum evaluation.\n\nThe core principle for cumulative sums is that partial sums can be stored in a hierarchical structure that decomposes the index set into ranges whose lengths are powers of two. This decomposition arises from the binary representation of integers. Two standard data structures embody this idea:\n\n1. Static prefix-sum array:\n   - Precompute the array of prefix sums $S$ where $S_k = \\sum_{i=1}^k p_X(x_i)$ for $k = 1, 2, \\dots, n$.\n   - Query: find $k$ via binary search and return $S_k$. The complexity is $O(\\log n)$ for the search and $O(1)$ for accessing the prefix sum, for an overall $O(\\log n)$ query.\n   - Update: a point update to $p_X(x_i)$ requires updating all $S_k$ for $k \\ge i$, which is $O(n)$ time in the worst case.\n   - Memory: $O(n)$ to store $S$.\n\n2. Binary Indexed Tree (Fenwick tree):\n   - Store a tree array $T$ of length $n$ where entries represent partial sums over ranges whose sizes are powers of two. The tree is arranged so that each index $i$ contributes to and is covered by a set of nodes determined by the least significant set bit of $i$.\n   - Prefix sum operation:\n     For a $1$-based index $k$, the prefix sum is obtained by iteratively accumulating $T[k]$, then setting $k \\leftarrow k - (k \\,\\\\, -k)$, and repeating until $k = 0$. Here, $(k \\,\\\\, -k)$ extracts the least significant set bit of $k$, which determines the largest power-of-two block ending at $k$ that is included in the cumulative sum. Each step reduces $k$ by removing its least significant power-of-two component, so the number of steps is bounded by the number of bits in $k$, which is $O(\\log n)$.\n     Formally, the operation is\n     $$\n     \\text{prefix\\_sum}(k) = \\sum_{\\text{iterates over }k} T[k], \\quad k \\leftarrow k - (k \\,\\\\, -k).\n     $$\n   - Point update operation:\n     To add $\\Delta$ to $p_X(x_i)$, we update $T[i] \\leftarrow T[i] + \\Delta$, then set $i \\leftarrow i + (i \\,\\\\, -i)$ and repeat until $i > n$. This walks up the tree and updates all partial sums that cover index $i$. Each step advances $i$ by the least significant power-of-two component, so the number of steps is $O(\\log n)$.\n     Formally,\n     $$\n     \\text{update}(i, \\Delta):\\quad \\text{while } i \\le n:~ T[i] \\leftarrow T[i] + \\Delta;~ i \\leftarrow i + (i \\,\\\\, -i).\n     $$\n   - Query: find $k$ via binary search on outcomes (in $O(\\log n)$) and return $\\text{prefix\\_sum}(k)$ (in $O(\\log n)$), for an overall $O(\\log n)$ query.\n   - Memory: $O(n)$ to store $T$; same asymptotic memory as a prefix-sum array, but with a different access pattern.\n   - Advantage: supports $O(\\log n)$ point updates, making it suitable when the PMF changes over time while preserving normalization by compensating updates.\n\n3. Segment tree:\n   - Stores sums over intervals in a binary tree structure. Queries for prefix sums or general range sums and point updates run in $O(\\log n)$.\n   - Memory: typically $O(2n)$ to $O(4n)$ due to tree node overhead, larger than a Binary Indexed Tree.\n   - Advantage: supports more general range queries; disadvantage: larger memory and more complex implementation overhead.\n\nTradeoff analysis:\n- If the distribution is static, a prefix-sum array plus binary search is optimal for queries: $O(\\log n)$ per query with minimal implementation complexity and $O(n)$ memory. Updates are expensive at $O(n)$ per update.\n- If the distribution needs dynamic point updates while maintaining $\\sum_i p_X(x_i) = 1$, a Binary Indexed Tree achieves both queries and updates in $O(\\log n)$ with $O(n)$ memory. This balances performance across both operations.\n- A Segment Tree supports the broadest range of queries but with higher memory overhead (typically a constant factor larger than $n$) and is more complex; it is appropriate when queries go beyond prefix sums.\n\nAlgorithmic design for this problem:\n- Sort outcomes $\\{x_i\\}$ in ascending order together with their probabilities so that indices align.\n- For a query $x$, use binary search to find $k = \\max\\{i : x_i \\le x\\}$, or $k = 0$ if $x  x_1$. This binary search is $O(\\log n)$.\n- Use either a prefix-sum array or a Binary Indexed Tree to obtain $\\sum_{i=1}^k p_X(x_i)$:\n  - With a prefix-sum array, return $S_k$ in $O(1)$ time.\n  - With a Binary Indexed Tree, return $\\text{prefix\\_sum}(k)$ in $O(\\log n)$ time.\n- For dynamic updates (as in the third test case), apply $\\Delta$ to one index $i$ and $-\\Delta$ to another index $j$ to preserve normalization. In the Binary Indexed Tree, apply both updates using $\\text{update}(i, \\Delta)$ and $\\text{update}(j, -\\Delta)$, each in $O(\\log n)$ time. Since probabilities remain nonnegative by choice of $\\Delta$, the PMF remains valid.\n\nComplexity justification from first principles:\n- Binary search over $n$ sorted outcomes requires comparing the query $x$ to outcomes and narrowing the search region by halving the interval at each step; the number of steps is $O(\\log n)$.\n- The Binary Indexed Tree prefix-sum operation reduces the index by removing its least significant power-of-two contribution at each step, which is bounded by the number of bits in the index; thus $O(\\log n)$ steps.\n- Memory for both the prefix-sum array and Binary Indexed Tree is linear in $n$ because each stores one real number per index (up to constant-factor overhead).\n\nProgram outputs:\n- The program computes the CDF values for all queries across the three test cases using a Binary Indexed Tree. For test case $3$, it applies the specified normalization-preserving update before answering queries.\n- The final output is a single line containing a flattened list of all CDF results: $[r_1, r_2, \\dots, r_m]$ where each $r_i$ is a floating-point number corresponding to $F_X(x)$ for the given queries in the specified order.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport bisect\nfrom typing import List, Tuple\n\nclass FenwickTree:\n    \"\"\"\n    Fenwick Tree (Binary Indexed Tree) for prefix sums over a 1-based array.\n    Supports:\n    - build in O(n)\n    - point updates in O(log n)\n    - prefix sums in O(log n)\n    \"\"\"\n    def __init__(self, size: int):\n        self.n = size\n        # 1-based indexing for tree\n        self.tree = [0.0] * (self.n + 1)\n\n    @staticmethod\n    def _lsb(i: int) - int:\n        return i  -i\n\n    def build(self, arr: List[float]) - None:\n        # Build Fenwick tree in O(n) time.\n        # Copy arr into tree and propagate contributions.\n        for i in range(1, self.n + 1):\n            self.tree[i] += arr[i - 1]\n            j = i + (i  -i)\n            if j = self.n:\n                self.tree[j] += self.tree[i]\n\n    def update(self, index_1based: int, delta: float) - None:\n        i = index_1based\n        while i = self.n:\n            self.tree[i] += delta\n            i += (i  -i)\n\n    def prefix_sum(self, index_1based: int) - float:\n        s = 0.0\n        i = index_1based\n        while i  0:\n            s += self.tree[i]\n            i -= (i  -i)\n        return s\n\n\nclass CDFEvaluator:\n    \"\"\"\n    Evaluates F_X(x) = sum_{i: x_i = x} p_i in O(log n) time using:\n    - binary search over sorted outcomes to find k = max{i: x_i = x}\n    - Fenwick tree prefix sums to compute sum_{i=1..k} p_i\n    Also supports point updates to probabilities with Fenwick tree propagation.\n    \"\"\"\n    def __init__(self, outcomes: List[float], probabilities: List[float]):\n        assert len(outcomes) == len(probabilities), \"Outcomes and probabilities length mismatch.\"\n        n = len(outcomes)\n        # Sort by outcomes, keep aligned probabilities\n        paired = sorted(zip(outcomes, probabilities), key=lambda t: t[0])\n        self.outcomes = [t[0] for t in paired]\n        self.probabilities = [t[1] for t in paired]\n        self.n = n\n        # Build Fenwick tree\n        self.ft = FenwickTree(n)\n        self.ft.build(self.probabilities)\n\n    def query_cdf(self, x: float) - float:\n        # Find largest index k such that outcomes[k-1] = x\n        # bisect_right returns insertion point, so k = insertion_index\n        k = bisect.bisect_right(self.outcomes, x)\n        if k = 0:\n            return 0.0\n        return self.ft.prefix_sum(k)\n\n    def point_update(self, index_zero_based: int, delta: float) - None:\n        \"\"\"\n        Update probability at a given zero-based index by delta; the caller must\n        ensure normalization across paired updates.\n        \"\"\"\n        # Apply update to internal probability array\n        self.probabilities[index_zero_based] += delta\n        # Propagate update to Fenwick tree\n        self.ft.update(index_zero_based + 1, delta)\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Test case 1\n    outcomes1 = [1.0, 2.5, 3.7, 5.0, 8.0]\n    probs1 = [0.1, 0.2, 0.05, 0.25, 0.4]\n    queries1 = [0.0, 1.0, 2.0, 2.5, 10.0]\n\n    # Test case 2\n    outcomes2 = [-5.0, -1.0, 0.0, 7.0]\n    probs2 = [0.0, 0.2, 0.3, 0.5]\n    queries2 = [-10.0, -5.0, -1.0, 0.0, 7.0, 100.0]\n\n    # Test case 3\n    outcomes3 = [i * i for i in range(1, 17)]  # 1^2 to 16^2\n    total_weight = sum(range(1, 17))\n    probs3 = [i / total_weight for i in range(1, 17)]\n    # Apply normalization-preserving update: +0.01 to p1, -0.01 from p16\n    delta = 0.01\n    queries3 = [0.0, 50.0, 100.0, 256.0]\n\n    # Initialize evaluators\n    evaluator1 = CDFEvaluator(outcomes1, probs1)\n    evaluator2 = CDFEvaluator(outcomes2, probs2)\n    evaluator3 = CDFEvaluator(outcomes3, probs3)\n\n    # Apply updates for test case 3\n    # Need to locate the indices corresponding to outcomes 1 and 256 in evaluator3's sorted outcomes\n    # Since outcomes3 is already sorted ascending, index 0 - 1, index 15 - 256\n    # Ensure nonnegativity after update\n    # p1 increases by delta, p16 decreases by delta\n    evaluator3.point_update(0, delta)\n    evaluator3.point_update(15, -delta)\n\n    results: List[float] = []\n\n    # Compute results for test case 1\n    for x in queries1:\n        results.append(evaluator1.query_cdf(x))\n\n    # Compute results for test case 2\n    for x in queries2:\n        results.append(evaluator2.query_cdf(x))\n\n    # Compute results for test case 3\n    for x in queries3:\n        results.append(evaluator3.query_cdf(x))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3304360"}, {"introduction": "Many real-world systems are best described by mixed distributions, which combine discrete probability masses at specific points with a continuous density over an interval. This practice requires you to design and validate a sampler for such a distribution, moving beyond standard cases to a more complex and realistic scenario [@problem_id:3304408]. By constructing a two-stage rejection sampler and rigorously proving its correctness, you will solidify your understanding of how to generate random variates that precisely follow a target law, a foundational capability in stochastic simulation.", "problem": "You are given a target distribution on the real line that is mixed, consisting of a discrete component with atoms and a continuous component with a density. Let the target be described by a finite set of atom locations $\\{a_i\\}_{i=1}^m$ with probabilities $\\{p_i\\}_{i=1}^m$ (so that $\\sum_{i=1}^m p_i = w_{\\mathrm{d}} \\in [0,1]$) and a continuous probability density function (PDF) $f(x)$ on $\\mathbb{R}$ weighted by $w_{\\mathrm{c}} = 1 - w_{\\mathrm{d}}$. The cumulative distribution function (CDF) is then\n$$\nF(x) \\;=\\; \\sum_{i=1}^m p_i \\,\\mathbf{1}\\{a_i \\le x\\} \\;+\\; w_{\\mathrm{c}} \\int_{-\\infty}^x f(t)\\,dt.\n$$\nHere $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function. The Probability Mass Function (PMF) of the discrete component is the map $i \\mapsto p_i$, and the PDF of the continuous component is $w_{\\mathrm{c}} f(x)$.\n\nYour tasks are as follows.\n\n- From first principles, using only the definitions of Probability Mass Function (PMF), Probability Density Function (PDF), and Cumulative Distribution Function (CDF), and the acceptance–rejection paradigm as a foundational fact, propose a two-stage rejection sampling algorithm that separates the handling of the discrete atoms from that of the continuous density. Assume you are given:\n  - constants $\\{c_i\\}_{i=1}^m$ satisfying $c_i \\ge p_i$ for all $i$,\n  - a proposal density $q(x)$ on $\\mathbb{R}$ and a finite constant $M \\ge \\sup_x \\frac{f(x)}{q(x)}$,\n  - a continuous envelope constant $c_{\\mathrm{c}} \\ge w_{\\mathrm{c}} M$,\n  and define the total envelope mass $C \\equiv \\sum_{i=1}^m c_i + c_{\\mathrm{c}}$.\n\n- Prove mathematically that the law of the accepted sample produced by your algorithm has CDF exactly $F(x)$.\n\n- Implement your algorithm as a program that, for each of the test cases below, generates $n$ accepted samples and numerically verifies that the empirical CDF is close to $F(x)$ with respect to the Kolmogorov distance\n$$\nD_n \\;\\equiv\\; \\sup_{x \\in \\mathbb{R}} \\big| \\widehat{F}_n(x) - F(x) \\big|,\n$$\nwhere $\\widehat{F}_n(x)$ is the empirical CDF of the $n$ accepted samples. Approximate the supremum by evaluating on a sufficiently fine grid on $\\mathbb{R}$ that covers the effective support of the distributions listed below.\n\nFoundational base you may rely on:\n- The definition of PMF, PDF, CDF, and mixture distributions.\n- The acceptance–rejection principle: if $t(z)$ is a target nonnegative function dominated by $M g(z)$ for a proposal $g$, proposing $Z \\sim g$ and accepting with probability $t(Z)/(M g(Z))$ yields accepted samples with density proportional to $t(z)$.\n\nProgram requirements:\n- Use a fixed random seed for each test case to ensure reproducibility.\n- For each test case, compute $D_n$ on a fixed grid and return a boolean indicating whether $D_n \\le \\tau$, where $\\tau$ is a specified tolerance.\n- Your program must produce a single line of output containing the boolean results for all test cases as a comma-separated list enclosed in square brackets, for example, $[\\mathrm{True},\\mathrm{False},\\mathrm{True}]$.\n\nTest suite:\n- Test case $1$ (mixed Laplace with atoms):\n  - Atoms: $a_1 = -1$, $a_2 = 2$ with masses $p_1 = 0.25$, $p_2 = 0.15$; hence $w_{\\mathrm{d}} = 0.40$ and $w_{\\mathrm{c}} = 0.60$.\n  - Continuous density $f$ is Laplace with location $0$ and scale $1$, that is $f(x) = \\frac{1}{2} e^{-|x|}$.\n  - Proposal $q$ equals the same Laplace$(0,1)$ density, so one may take $M = 1$.\n  - Envelope constants: $c_1 = 1.2 \\, p_1$, $c_2 = 1.2 \\, p_2$, $c_{\\mathrm{c}} = 1.5 \\, w_{\\mathrm{c}}$.\n  - Sample size $n = 50000$ accepted draws, random seed $12345$.\n  - Grid for $D_n$: equispaced on $[-8,8]$ with $2001$ points, augmented by including the atoms $\\{-1,2\\}$.\n  - Tolerance $\\tau = 0.02$.\n\n- Test case $2$ (pure continuous normal):\n  - No atoms ($m=0$), so $w_{\\mathrm{d}} = 0$ and $w_{\\mathrm{c}} = 1$.\n  - Continuous density $f$ is Normal with mean $0$ and standard deviation $1.5$, that is $f(x) = \\frac{1}{1.5 \\sqrt{2\\pi}} \\exp\\!\\big(-\\frac{x^2}{2 \\cdot 1.5^2}\\big)$.\n  - Proposal $q$ equals the same Normal$(0,1.5)$ density, so take $M = 1$ and $c_{\\mathrm{c}} = 1$.\n  - Sample size $n = 50000$ accepted draws, random seed $24680$.\n  - Grid for $D_n$: equispaced on $[-8,8]$ with $2001$ points.\n  - Tolerance $\\tau = 0.02$.\n\n- Test case $3$ (purely discrete):\n  - Atoms: $a_1 = 0$, $a_2 = 3$ with masses $p_1 = 0.7$, $p_2 = 0.3$; hence $w_{\\mathrm{d}} = 1$ and $w_{\\mathrm{c}} = 0$.\n  - No continuous component; choose any placeholder $f$ and $q$ but set $c_{\\mathrm{c}} = 0$ so that the continuous branch is never selected.\n  - Envelope constants: $c_1 = 1.1 \\, p_1$, $c_2 = 1.1 \\, p_2$.\n  - Sample size $n = 50000$ accepted draws, random seed $98765$.\n  - Grid for $D_n$: equispaced on $[-2,5]$ with $1401$ points, augmented by including the atoms $\\{0,3\\}$.\n  - Tolerance $\\tau = 0.02$.\n\nOutput specification:\n- Your program must run without user input and print one line in the exact format $[b_1,b_2,b_3]$ where $b_j$ is $\\mathrm{True}$ if the corresponding test case satisfies $D_n \\le \\tau$ and $\\mathrm{False}$ otherwise.", "solution": "The problem statement has been meticulously reviewed and is determined to be valid. It is scientifically grounded in probability theory and computational statistics, well-posed with a clear objective, and provides a complete and consistent set of parameters for both the theoretical derivation and the computational implementation.\n\n### Part 1: Derivation of the Two-Stage Rejection Sampling Algorithm\n\nThe target distribution is a mixture of a discrete component and a continuous component. Its cumulative distribution function (CDF) is given by:\n$$\nF(x) = \\sum_{i=1}^m p_i \\mathbf{1}\\{a_i \\le x\\} + w_{\\mathrm{c}} \\int_{-\\infty}^x f(t)\\,dt\n$$\nwhere $\\{a_i\\}_{i=1}^m$ are atom locations with probabilities $\\{p_i\\}_{i=1}^m$, $w_{\\mathrm{d}} = \\sum p_i$, and $w_{\\mathrm{c}} = 1 - w_{\\mathrm{d}}$. The function $f(x)$ is a probability density function (PDF).\n\nSuch a distribution does not have a PDF in the standard sense due to the discrete mass points (atoms). To apply the rejection sampling principle, we can describe the target distribution using a generalized density function involving Dirac delta functions, $\\delta(\\cdot)$:\n$$\nt(x) = \\sum_{i=1}^m p_i \\delta(x - a_i) + w_{\\mathrm{c}} f(x)\n$$\nThe function $t(x)$ is not a density but a positive function whose integral over $\\mathbb{R}$ is $\\sum p_i + w_c \\int f(t) dt = w_d + w_c = 1$. Our goal is to generate samples from the law described by $t(x)$.\n\nThe rejection sampling method requires a proposal distribution and an envelope function that dominates the target. The problem provides constants $\\{c_i\\}_{i=1}^m$ for the discrete part and $c_{\\mathrm{c}}$ for the continuous part. We use these to construct a mixture-like envelope process. This leads to a two-stage algorithm.\n\nLet us define a categorical distribution over $m+1$ indices $\\{0, 1, \\dots, m\\}$. The index $0$ will correspond to the continuous component, and indices $i \\in \\{1, \\dots, m\\}$ will correspond to the discrete atoms. We define the probabilities of choosing these categories as:\n$$\n\\pi_0 = \\frac{c_{\\mathrm{c}}}{C}, \\quad \\pi_i = \\frac{c_i}{C} \\quad \\text{for } i \\in \\{1, \\dots, m\\}\n$$\nwhere $C = \\sum_{i=1}^m c_i + c_{\\mathrm{c}}$ is the total envelope mass. Note that $\\sum_{i=0}^m \\pi_i = 1$, so this is a valid probability distribution.\n\nThe algorithm proceeds by first sampling a category, then proposing a value based on that category, and finally performing a rejection test.\n\n**The Two-Stage Rejection Sampling Algorithm:**\n\nTo generate one accepted sample, repeat the following steps until a sample is accepted:\n\n1.  **Stage 1: Select Component.**\n    Draw a category index $J$ from the set $\\{0, 1, \\dots, m\\}$ with probabilities $P(J=j) = \\pi_j$.\n\n2.  **Stage 2: Propose and Accept/Reject.**\n    - If $J = i$ for some $i \\in \\{1, \\dots, m\\}$ (discrete component selected):\n        a. Propose the deterministic value $X = a_i$.\n        b. Draw a uniform random variable $U \\sim U(0,1)$.\n        c. Accept the proposal $X=a_i$ if $U \\le \\frac{p_i}{c_i}$. The condition $c_i \\ge p_i$ ensures this probability is at most $1$. If accepted, the sample is $a_i$; terminate the loop.\n\n    - If $J = 0$ (continuous component selected):\n        a. Draw a proposal value $X$ from the proposal density $q(x)$.\n        b. Draw a uniform random variable $U \\sim U(0,1)$.\n        c. Accept the proposal $X$ if $U \\le \\frac{w_{\\mathrm{c}} f(X)}{c_{\\mathrm{c}} q(X)}$. The conditions $M \\ge \\sup_x \\frac{f(x)}{q(x)}$ and $c_{\\mathrm{c}} \\ge w_{\\mathrm{c}} M$ ensure that $w_{\\mathrm{c}} f(x) \\le w_{\\mathrm{c}} M q(x) \\le c_{\\mathrm{c}} q(x)$ for all $x$, so the acceptance probability is at most $1$. If accepted, the sample is $X$; terminate the loop.\n\n### Part 2: Mathematical Proof of Correctness\n\nLet $Y$ be a random variable representing an accepted sample generated by the algorithm. We must prove that the CDF of $Y$, $F_Y(x) = P(Y \\le x)$, is equal to the target CDF $F(x)$.\n\nFirst, let's calculate the overall probability of accepting a proposal in a single iteration of the loop, denoted $P(\\text{accept})$.\n$$\nP(\\text{accept}) = \\sum_{j=0}^m P(\\text{accept} | J=j) P(J=j)\n$$\nFor the discrete components ($j=i > 0$):\n$P(\\text{accept} | J=i) = P\\left(U \\le \\frac{p_i}{c_i}\\right) = \\frac{p_i}{c_i}$.\nFor the continuous component ($j=0$):\n$P(\\text{accept} | J=0) = \\int_{-\\infty}^{\\infty} P(\\text{accept } X | X=t, J=0) q(t) dt = \\int_{-\\infty}^{\\infty} \\frac{w_{\\mathrm{c}} f(t)}{c_{\\mathrm{c}} q(t)} q(t) dt = \\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}} \\int_{-\\infty}^{\\infty} f(t) dt = \\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}}$, since $f(x)$ is a PDF.\n\nCombining these, the total acceptance probability is:\n$$\nP(\\text{accept}) = P(\\text{accept}|J=0)P(J=0) + \\sum_{i=1}^m P(\\text{accept}|J=i)P(J=i)\n$$\n$$\nP(\\text{accept}) = \\left(\\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}}\\right) \\left(\\frac{c_{\\mathrm{c}}}{C}\\right) + \\sum_{i=1}^m \\left(\\fracp_i}{c_i}\\right) \\left(\\frac{c_i}{C}\\right) = \\frac{w_{\\mathrm{c}}}{C} + \\sum_{i=1}^m \\frac{p_i}{C} = \\frac{w_{\\mathrm{c}} + w_{\\mathrm{d}}}{C} = \\frac{1}{C}\n$$\nNow we derive the CDF of the accepted sample $Y$. By definition of conditional probability:\n$$\nF_Y(x) = P(Y \\le x) = \\frac{P(\\text{accepted proposal } X \\le x)}{P(\\text{accept})}\n$$\nThe numerator is the probability that in one iteration, a proposal is generated that is both less than or equal to $x$ *and* is accepted. We compute this using the law of total probability, conditioning on the chosen component $J$.\n$$\nP(\\text{accepted proposal } X \\le x) = \\sum_{j=0}^{m} P(\\text{accepted } X \\le x | J=j) P(J=j)\n$$\n- For $j=0$ (continuous):\nThe proposed value is $X \\sim q(x)$. The event is that $X \\le x$ and it is accepted.\n$$\nP(\\text{accepted } X \\le x | J=0) = \\int_{-\\infty}^x P(\\text{accept at } t | J=0) q(t) dt = \\int_{-\\infty}^x \\frac{w_{\\mathrm{c}} f(t)}{c_{\\mathrm{c}} q(t)} q(t) dt = \\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}} \\int_{-\\infty}^x f(t) dt\n$$\nThe total contribution from this branch is $\\left(\\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}} \\int_{-\\infty}^x f(t) dt\\right) \\cdot P(J=0) = \\left(\\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}} \\int_{-\\infty}^x f(t) dt\\right) \\cdot \\frac{c_{\\mathrm{c}}}{C} = \\frac{w_{\\mathrm{c}}}{C} \\int_{-\\infty}^x f(t) dt$.\n\n- For $j=i  0$ (discrete):\nThe proposed value is the constant $a_i$. The event is $a_i \\le x$ and it is accepted.\n$$\nP(\\text{accepted } X \\le x | J=i) = P(a_i \\le x \\text{ and } U \\le p_i/c_i) = P(a_i \\le x) \\cdot P(U \\le p_i/c_i) = \\mathbf{1}\\{a_i \\le x\\} \\cdot \\frac{p_i}{c_i}\n$$\nThe total contribution from this branch is $\\left(\\mathbf{1}\\{a_i \\le x\\} \\frac{p_i}{c_i}\\right) \\cdot P(J=i) = \\left(\\mathbf{1}\\{a_i \\le x\\} \\frac{p_i}{c_i}\\right) \\cdot \\frac{c_i}{C} = \\frac{p_i}{C} \\mathbf{1}\\{a_i \\le x\\}$.\n\nSumming all contributions to the numerator:\n$$\nP(\\text{accepted proposal } X \\le x) = \\frac{w_{\\mathrm{c}}}{C} \\int_{-\\infty}^x f(t) dt + \\sum_{i=1}^m \\frac{p_i}{C} \\mathbf{1}\\{a_i \\le x\\} = \\frac{1}{C} \\left( w_{\\mathrm{c}} \\int_{-\\infty}^x f(t) dt + \\sum_{i=1}^m p_i \\mathbf{1}\\{a_i \\le x\\} \\right)\n$$\nFinally, we compute the CDF of the accepted sample $Y$:\n$$\nF_Y(x) = \\frac{P(\\text{accepted proposal } X \\le x)}{P(\\text{accept})} = \\frac{\\frac{1}{C} \\left( w_{\\mathrm{c}} \\int_{-\\infty}^x f(t) dt + \\sum_{i=1}^m p_i \\mathbf{1}\\{a_i \\le x\\} \\right)}{1/C}\n$$\n$$\nF_Y(x) = \\sum_{i=1}^m p_i \\mathbf{1}\\{a_i \\le x\\} + w_{\\mathrm{c}} \\int_{-\\infty}^x f(t) dt\n$$\nThis is precisely the target CDF $F(x)$. The proof is complete. Q.E.D.\n\n### Part 3: Implementation and Numerical Verification\n\nThe derived algorithm is implemented for the three specified test cases. For each case, $n$ samples are generated, and the empirical CDF, $\\widehat{F}_n(x)$, is compared to the true CDF, $F(x)$, by computing the Kolmogorov distance $D_n$ on a specified grid. The result is a boolean indicating if $D_n$ is within the given tolerance $\\tau$.\n\nThe empirical CDF is calculated as $\\widehat{F}_n(x) = \\frac{1}{n} \\sum_{k=1}^n \\mathbf{1}\\{Y_k \\le x\\}$, where $\\{Y_k\\}_{k=1}^n$ are the generated samples. The supremum for $D_n$ is approximated by taking the maximum absolute difference over the points in the test grid.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import laplace, norm\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for the two-stage rejection sampler.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1: Mixed Laplace with atoms\n        {\n            \"atoms\": np.array([-1.0, 2.0]),\n            \"probs\": np.array([0.25, 0.15]),\n            \"f_dist\": laplace(loc=0, scale=1),\n            \"q_dist\": laplace(loc=0, scale=1),\n            \"M\": 1.0,\n            \"c_factors\": (1.2, 1.5),  # (discrete_factor, continuous_factor)\n            \"n_samples\": 50000,\n            \"seed\": 12345,\n            \"grid_params\": (-8.0, 8.0, 2001),\n            \"tolerance\": 0.02,\n        },\n        # Test case 2: Pure continuous normal\n        {\n            \"atoms\": np.array([]),\n            \"probs\": np.array([]),\n            \"f_dist\": norm(loc=0, scale=1.5),\n            \"q_dist\": norm(loc=0, scale=1.5),\n            \"M\": 1.0,\n            \"c_factors\": (1.0, 1.0), # Factors are not really used here, c_c=1\n            \"n_samples\": 50000,\n            \"seed\": 24680,\n            \"grid_params\": (-8.0, 8.0, 2001),\n            \"tolerance\": 0.02,\n        },\n        # Test case 3: Purely discrete\n        {\n            \"atoms\": np.array([0.0, 3.0]),\n            \"probs\": np.array([0.7, 0.3]),\n            \"f_dist\": None,  # Placeholder\n            \"q_dist\": None,  # Placeholder\n            \"M\": 1.0, # Placeholder\n            \"c_factors\": (1.1, 0.0), # No continuous part\n            \"n_samples\": 50000,\n            \"seed\": 98765,\n            \"grid_params\": (-2.0, 5.0, 1401),\n            \"tolerance\": 0.02,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(run_test_case(**case))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef run_test_case(atoms, probs, f_dist, q_dist, M, c_factors, n_samples, seed, grid_params, tolerance):\n    \"\"\"\n    Executes a single test case for the sampler and returns the verification result.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # --- 1. Setup sampler parameters ---\n    m = len(atoms)\n    w_d = np.sum(probs)\n    w_c = 1.0 - w_d\n\n    discrete_c_factor, continuous_c_factor = c_factors\n    \n    # Envelope constants\n    c_i = discrete_c_factor * probs\n    \n    if w_c  0:\n        c_c = continuous_c_factor * w_c * M\n    else:\n        # Handles pure discrete case where w_c is 0\n        c_c = 0.0\n        \n    C = np.sum(c_i) + c_c\n\n    # Category selection probabilities (pi_0 for continuous, pi_1...pi_m for discrete)\n    pi_vec = np.zeros(m + 1)\n    if C  0:\n        pi_vec[0] = c_c / C\n        pi_vec[1:] = c_i / C\n    else: # This branch is for cases like pure continuous with M=1, c_c=1 - C=1\n        if m == 0 and w_c == 1.0:\n            C = 1.0\n            c_c = 1.0\n            pi_vec[0] = 1.0\n            \n    # --- 2. Define the sampler function ---\n    def sampler():\n        while True:\n            # Stage 1: Select Component\n            j = rng.choice(m + 1, p=pi_vec)\n            \n            # Stage 2: Propose and Accept/Reject\n            if j == 0:  # Continuous component\n                if c_c == 0: continue # Should not happen if pi_vec[0]0, but for safety\n                \n                x_proposal = q_dist.rvs(random_state=rng)\n                u = rng.uniform(0, 1)\n                \n                f_val = f_dist.pdf(x_proposal)\n                q_val = q_dist.pdf(x_proposal)\n                \n                # Avoid division by zero if q(x)=0. If f(x) is also 0, prob is 0.\n                if q_val  0:\n                    acceptance_prob = (w_c * f_val) / (c_c * q_val)\n                    if u = acceptance_prob:\n                        return x_proposal\n            else:  # Discrete component j-1\n                i = j - 1\n                x_proposal = atoms[i]\n                u = rng.uniform(0, 1)\n                \n                acceptance_prob = probs[i] / c_i[i]\n                if u = acceptance_prob:\n                    return x_proposal\n\n    # --- 3. Generate samples ---\n    samples = np.array([sampler() for _ in range(n_samples)])\n\n    # --- 4. Define true CDF ---\n    def F_true(x):\n        F_val = np.zeros_like(x, dtype=float)\n        # Discrete part\n        for i in range(m):\n            F_val += probs[i] * (x = atoms[i])\n        # Continuous part\n        if w_c  0:\n            F_val += w_c * f_dist.cdf(x)\n        return F_val\n\n    # --- 5. Compute Kolmogorov distance ---\n    grid_start, grid_end, grid_points = grid_params\n    x_grid = np.linspace(grid_start, grid_end, grid_points)\n    # Augment grid with atom locations\n    if m  0:\n        x_grid = np.unique(np.concatenate((x_grid, atoms)))\n    \n    # Calculate true CDF on the grid\n    true_cdf_on_grid = F_true(x_grid)\n    \n    # Calculate empirical CDF on the grid\n    sorted_samples = np.sort(samples)\n    empirical_cdf_on_grid = np.searchsorted(sorted_samples, x_grid, side='right') / n_samples\n    \n    # Kolmogorov distance\n    D_n = np.max(np.abs(empirical_cdf_on_grid - true_cdf_on_grid))\n    \n    return D_n = tolerance\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3304408"}, {"introduction": "Estimating rare-event probabilities, such as values of a CDF $F_X(t)$ in the far tails of a distribution, is a classic challenge where naive Monte Carlo simulation fails due to its high variance. This hands-on problem introduces importance sampling (IS) as a powerful variance reduction technique to overcome this limitation [@problem_id:3304377]. You will design an unbiased IS estimator, analytically derive its variance, and numerically demonstrate its superiority over naive methods, providing a concrete example of how to intelligently guide a simulation to efficiently probe rare but important outcomes.", "problem": "Design an unbiased Monte Carlo (MC) estimator for the cumulative distribution function (CDF) $F_X(t) = \\mathbb{P}(X \\le t)$ of a heavy-tailed random variable $X$ using importance sampling (IS) focused on the neighborhood of $t$, and compare the per-sample variance against naive sampling as $t$ moves into the tail.\n\nLet $X$ be a heavy-tailed continuous random variable with probability density function (PDF) $f_X(x)$ and cumulative distribution function (CDF) $F_X(t) = \\int_{-\\infty}^t f_X(x)\\,\\mathrm{d}x$. Consider estimating $F_X(t)$ for thresholds $t$ in the left tail. You will construct an unbiased IS estimator by sampling from a proposal density $g_t(x)$ that concentrates probability mass near $t$, and compare its per-sample variance to the per-sample variance of the naive indicator estimator.\n\nFundamental base and definitions to use:\n- The cumulative distribution function (CDF) of a continuous random variable $X$ with probability density function (PDF) $f_X$ is $F_X(t) = \\mathbb{P}(X \\le t) = \\int_{-\\infty}^t f_X(x)\\,\\mathrm{d}x$.\n- The naive MC estimator for $F_X(t)$ based on independent and identically distributed samples $X_1,\\dots,X_n \\sim f_X$ is $\\widehat{F}_{\\text{naive}}(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\}$, which has per-sample variance $V_{\\text{naive}}(t) = F_X(t)\\left(1 - F_X(t)\\right)$.\n- The importance sampling identity: for any integrable function $h(x)$ and any proposal density $g(x)$ with support covering that of $f_X$, $\\mathbb{E}_f[h(X)] = \\mathbb{E}_g\\!\\left[h(X)\\,\\frac{f_X(X)}{g(X)}\\right]$.\n\nSpecification of the target distribution and proposal family:\n- Heavy-tailed target: Student’s $t$ distribution with $\\nu = 3$ degrees of freedom, denoted $X \\sim t_\\nu$ with $\\nu = 3$. Let its PDF be $f_\\nu(x)$ and CDF be $F_\\nu(t)$.\n- Proposal focused at the threshold: For each threshold $t$, use a Gaussian proposal $g_t(x) = \\mathcal{N}(t, \\sigma^2)$ with $\\sigma = 1$. This proposal has PDF $g_t(x)$ and log-PDF $\\log g_t(x)$ known in closed form.\n\nUnbiased IS estimator design:\n- For a fixed threshold $t$, define the IS estimator\n$$\n\\widehat{F}_{\\text{IS}}(t) \\;=\\; \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\}\\,w_t(X_i),\n\\quad X_i \\overset{\\text{i.i.d.}}{\\sim} g_t,\\quad w_t(x) \\;=\\; \\frac{f_\\nu(x)}{g_t(x)}.\n$$\n- Show that $\\widehat{F}_{\\text{IS}}(t)$ is unbiased by first principles and obtain its per-sample variance in terms of a second moment under $g_t$:\n$$\nV_{\\text{IS}}(t) \\;=\\; \\mathbb{E}_{g_t}\\!\\left[\\left(\\mathbf{1}\\{X \\le t\\}\\,w_t(X)\\right)^2\\right] \\;-\\; \\left(F_\\nu(t)\\right)^2.\n$$\n\nNumerical comparison task:\n- For each threshold $t$, compute the ratio of per-sample variances\n$$\nR(t) \\;=\\; \\frac{V_{\\text{naive}}(t)}{V_{\\text{IS}}(t)} \\;=\\; \\frac{F_\\nu(t)\\left(1 - F_\\nu(t)\\right)}{\\mathbb{E}_{g_t}\\!\\left[\\left(\\mathbf{1}\\{X \\le t\\}\\,w_t(X)\\right)^2\\right] - \\left(F_\\nu(t)\\right)^2}.\n$$\n- The numerator $F_\\nu(t)\\left(1 - F_\\nu(t)\\right)$ must be computed exactly from the CDF of the Student’s $t$ distribution with $\\nu = 3$ degrees of freedom. The denominator’s first term $\\mathbb{E}_{g_t}[\\cdot]$ must be approximated via MC by sampling from $g_t$ and averaging the integrand. Use a large auxiliary MC budget $M$ to estimate this second moment robustly. You must use $\\sigma = 1$ and $M = 400000$ for all thresholds.\n\nTest suite:\n- Degrees of freedom: $\\nu = 3$.\n- Proposal standard deviation: $\\sigma = 1$.\n- Monte Carlo budget for the second-moment estimation: $M = 400000$.\n- Thresholds: $t \\in \\{-0.5,\\,-2.0,\\,-3.0,\\,-4.0,\\,-5.0\\}$.\n\nRandomness:\n- Use a fixed random seed $s = 123456$ for reproducibility.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, namely the list\n$$\n\\left[ R(-0.5),\\, R(-2.0),\\, R(-3.0),\\, R(-4.0),\\, R(-5.0) \\right],\n$$\nrounded to six decimal places per entry.\n\nNotes:\n- All computations are unitless; no physical units are involved.\n- Ensure absolute continuity of $f_\\nu$ with respect to $g_t$ is satisfied, and restrict to the continuous case (no probability mass function (PMF) is required).\n- Your implementation must be a complete, runnable program that performs these computations and prints the required single line as specified.", "solution": "The problem as stated is scientifically grounded, well-posed, and contains all necessary information for a unique, verifiable solution. It represents a standard and meaningful exercise in computational statistics, specifically concerning variance reduction techniques for Monte Carlo estimation. Thus, the problem is deemed **valid**.\n\nThe primary objective is to construct an importance sampling (IS) based estimator for the cumulative distribution function (CDF) of a Student's $t$-distribution and to evaluate its efficiency relative to a naive Monte Carlo estimator. The efficiency is quantified by the ratio of their respective per-sample variances.\n\nLet $X$ be a random variable following a Student's $t$-distribution with $\\nu=3$ degrees of freedom. Its probability density function (PDF) is denoted by $f_\\nu(x)$ and its CDF by $F_\\nu(t)$. The quantity to be estimated is $F_\\nu(t) = \\mathbb{P}(X \\le t) = \\int_{-\\infty}^t f_\\nu(x) \\, \\mathrm{d}x$. This can be expressed as an expectation:\n$$\nF_\\nu(t) = \\mathbb{E}_{f_\\nu}[\\mathbf{1}\\{X \\le t\\}]\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\nThe naive Monte Carlo estimator for $F_\\nu(t)$ is based on $n$ independent and identically distributed (i.i.d.) samples $X_1, \\dots, X_n$ drawn from the target density $f_\\nu(x)$:\n$$\n\\widehat{F}_{\\text{naive}}(t) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\}\n$$\nEach term $\\mathbf{1}\\{X_i \\le t\\}$ is a Bernoulli random variable with success probability $p = \\mathbb{P}(X_i \\le t) = F_\\nu(t)$. The per-sample variance (i.e., the variance for $n=1$) is therefore:\n$$\nV_{\\text{naive}}(t) = \\text{Var}(\\mathbf{1}\\{X \\le t\\}) = p(1-p) = F_\\nu(t)(1-F_\\nu(t))\n$$\nFor rare events where $F_\\nu(t)$ is very small (i.e., $t$ is deep in the left tail), this variance becomes approximately $F_\\nu(t)$.\n\nImportance sampling aims to reduce this variance by sampling from a different distribution, the proposal distribution $g_t(x)$, which is chosen to concentrate samples in the region of interest. Here, the proposal is a Gaussian distribution centered at the threshold $t$: $g_t(x) = \\mathcal{N}(t, \\sigma^2)$ with $\\sigma=1$.\n\nThe IS estimator is formulated by rewriting the expectation with respect to the proposal density $g_t$:\n$$\nF_\\nu(t) = \\int_{-\\infty}^{\\infty} \\mathbf{1}\\{x \\le t\\} f_\\nu(x) \\, \\mathrm{d}x = \\int_{-\\infty}^{\\infty} \\mathbf{1}\\{x \\le t\\} \\frac{f_\\nu(x)}{g_t(x)} g_t(x) \\, \\mathrm{d}x = \\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\} w_t(X)]\n$$\nwhere $w_t(x) = f_\\nu(x) / g_t(x)$ is the importance weight. The support of $g_t(x)$ (all of $\\mathbb{R}$) covers the support of $f_\\nu(x)$ (all of $\\mathbb{R}$), so the weights are well-defined.\n\nThe IS estimator, based on $n$ i.i.d. samples $X_1, \\dots, X_n$ from the proposal $g_t(x)$, is:\n$$\n\\widehat{F}_{\\text{IS}}(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\}\\,w_t(X_i)\n$$\nThis estimator is unbiased. We demonstrate this by taking its expectation with respect to $g_t$:\n$$\n\\mathbb{E}_{g_t}[\\widehat{F}_{\\text{IS}}(t)] = \\mathbb{E}_{g_t}\\left[\\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\}w_t(X_i)\\right] = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_{g_t}[\\mathbf{1}\\{X_i \\le t\\}w_t(X_i)]\n$$\nBy the derivation of the IS identity, $\\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\}w_t(X)] = F_\\nu(t)$. Thus,\n$$\n\\mathbb{E}_{g_t}[\\widehat{F}_{\\text{IS}}(t)] = \\frac{1}{n} \\sum_{i=1}^n F_\\nu(t) = F_\\nu(t)\n$$\n\nThe per-sample variance of the IS estimator is given by the variance of a single weighted sample term, $Y_i = \\mathbf{1}\\{X_i \\le t\\}w_t(X_i)$:\n$$\nV_{\\text{IS}}(t) = \\text{Var}_{g_t}(\\mathbf{1}\\{X \\le t\\}w_t(X)) = \\mathbb{E}_{g_t}[(\\mathbf{1}\\{X \\le t\\}w_t(X))^2] - (\\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\}w_t(X)])^2\n$$\nSubstituting $\\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\}w_t(X)] = F_\\nu(t)$ and noting that $(\\mathbf{1}\\{X \\le t\\})^2 = \\mathbf{1}\\{X \\le t\\}$, we get:\n$$\nV_{\\text{IS}}(t) = \\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\} (w_t(X))^2] - (F_\\nu(t))^2\n$$\nLet us denote the second moment term as $M_2(t) = \\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\} (w_t(X))^2]$. The variance is $V_{\\text{IS}}(t) = M_2(t) - (F_\\nu(t))^2$.\n\nThe task is to compute the variance reduction ratio $R(t) = V_{\\text{naive}}(t) / V_{\\text{IS}}(t)$.\nTo compute this ratio, we follow the specified numerical procedure:\n1.  For each threshold $t$, the numerator $V_{\\text{naive}}(t) = F_\\nu(t)(1-F_\\nu(t))$ is calculated using the exact value of the Student's $t$-CDF $F_\\nu(t)$ with $\\nu=3$.\n2.  The second moment term $M_2(t)$ in the denominator is estimated via a separate, high-budget Monte Carlo simulation. We draw a large number of samples, $Y_j \\sim g_t$ for $j=1, \\dots, M$, and compute the sample mean:\n    $$\n    \\widehat{M}_2(t) = \\frac{1}{M} \\sum_{j=1}^{M} \\mathbf{1}\\{Y_j \\le t\\}(w_t(Y_j))^2\n    $$\n    The problem specifies a budget of $M=400000$. To maintain numerical stability, especially for tail values, the squared weights $(w_t(Y_j))^2$ are computed using log-probabilities:\n    $$\n    (w_t(Y_j))^2 = \\left(\\frac{f_\\nu(Y_j)}{g_t(Y_j)}\\right)^2 = \\exp\\left(2 \\left(\\log f_\\nu(Y_j) - \\log g_t(Y_j)\\right)\\right)\n    $$\n3.  The IS variance is then estimated as $\\widehat{V}_{\\text{IS}}(t) = \\widehat{M}_2(t) - (F_\\nu(t))^2$.\n4.  Finally, the ratio is computed as $R(t) = V_{\\text{naive}}(t) / \\widehat{V}_{\\text{IS}}(t)$.\n\nThe algorithm proceeds by iterating through the given thresholds $t \\in \\{-0.5, -2.0, -3.0, -4.0, -5.0\\}$, performing these steps with $\\nu=3$, $\\sigma=1$, $M=400000$, and a fixed random seed for reproducibility. A ratio $R(t)  1$ indicates that the importance sampling strategy is more efficient (i.e., has lower variance) than the naive sampling approach for the given threshold $t$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t as student_t\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the ratio of naive MC variance to importance sampling MC variance\n    for estimating the CDF of a Student's t-distribution.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Parameters\n    nu = 3.0  # Degrees of freedom for Student's t\n    sigma = 1.0  # Std dev for Gaussian proposal\n    M = 400000  # MC budget for second-moment estimation\n    thresholds = [-0.5, -2.0, -3.0, -4.0, -5.0]\n    seed = 123456\n\n    # Initialize a random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    results = []\n    \n    for t_val in thresholds:\n        # Step 1: Compute the exact per-sample variance of the naive estimator.\n        # F_nu(t) = P(X = t) for X ~ student_t(nu).\n        F_nu_t = student_t.cdf(t_val, df=nu)\n\n        # V_naive(t) = F_nu(t) * (1 - F_nu(t))\n        V_naive_t = F_nu_t * (1.0 - F_nu_t)\n\n        # Step 2: Estimate the per-sample variance of the IS estimator via MC.\n        # a) Generate samples from the proposal distribution g_t(x) = N(t, sigma^2).\n        samples_g = rng.normal(loc=t_val, scale=sigma, size=M)\n\n        # b) Estimate M_2(t) = E_g[ (1{X=t} * w_t(X))^2 ]\n        # The MC estimator is (1/M) * sum( 1{Y_j=t} * (w_t(Y_j))^2 ) \n        # where Y_j are samples from g_t.\n        # This simplifies to E_g[ 1{X=t} * w_t(X)^2 ].\n        \n        # We only need to compute weights for samples that contribute to the integral,\n        # i.e., where the indicator function is 1.\n        integration_samples = samples_g[samples_g = t_val]\n\n        if integration_samples.size == 0:\n            M2_hat_t = 0.0\n        else:\n            # Use log-PDFs for numerical stability when computing weights.\n            # log(w_t(x)) = log(f_nu(x)) - log(g_t(x))\n            # w_t(x)^2 = exp(2 * log(w_t(x)))\n            log_f_nu = student_t.logpdf(integration_samples, df=nu)\n            log_g_t = norm.logpdf(integration_samples, loc=t_val, scale=sigma)\n            \n            log_w_sq = 2.0 * (log_f_nu - log_g_t)\n            w_sq = np.exp(log_w_sq)\n\n            # The expectation is over all M samples, but terms for Y_j  t_val are zero.\n            # So, we sum the non-zero terms and divide by the total number of samples M.\n            M2_hat_t = np.sum(w_sq) / M\n\n        # c) Compute the estimated IS variance\n        # V_IS(t) = M_2(t) - (F_nu(t))^2\n        V_is_t = M2_hat_t - (F_nu_t**2)\n\n        # Step 3: Compute the ratio of variances.\n        # We check for V_is_t = 0 to prevent division by zero or nonsensical results,\n        # though this is unlikely with a large M if the variance is non-zero.\n        if V_is_t  0:\n            ratio = V_naive_t / V_is_t\n        else:\n            # Handle cases where the estimated variance is non-positive.\n            # A large value signifies variance reduction is extremely effective.\n            ratio = np.inf\n\n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3304377"}]}