{"hands_on_practices": [{"introduction": "The entire process of automated structure elucidation rests upon a foundation of highly accurate mass measurements. However, even the most advanced mass spectrometers are subject to drift, which can introduce systematic errors. This exercise demonstrates a fundamental data calibration technique, lock-mass correction, which is essential for ensuring the fidelity of the raw data before any further analysis is performed [@problem_id:3693918]. By working through this calculation, you will see how platforms algorithmically correct for instrumental error to achieve the sub-ppm mass accuracy required for confident molecular formula assignment.", "problem": "An automated structure elucidation platform relies on high-resolution mass measurements to prune candidate structures generated from tandem mass spectrometry. A core scoring feature is the mass accuracy expressed in parts per million. Starting from the fundamental definition of relative error and the definition of parts per million as one part per million units, first derive an expression for the mass accuracy in parts per million in terms of an observed mass and a theoretical mass. Then, consider a high-resolution mass spectrometry acquisition in which a background calibrant ion is used for lock-mass correction. Assume measurements are taken in the positive mode with charge $z=1$, so that $m/z$ equals mass numerically. Over the relevant mass range, assume the instrument’s mass error is well-approximated by an additive constant offset.\n\nYou are given:\n- Theoretical lock-mass $m_{\\text{L,theoretical}} = 371.101237$.\n- Observed lock-mass $m_{\\text{L,observed}} = 371.101737$.\n- Theoretical analyte monoisotopic mass $m_{\\text{A,theoretical}} = 300.123456$.\n- Observed analyte monoisotopic mass $m_{\\text{A,observed}} = 300.124011$.\n\nUnder the additive-offset assumption, derive the lock-mass-corrected analyte mass and then compute the analyte’s post-correction mass accuracy in parts per million using your derived expression. Round your final answer to four significant figures. Express the final answer in ppm.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\nThe data and conditions explicitly provided are:\n-   Theoretical lock-mass: $m_{\\text{L,theoretical}} = 371.101237$\n-   Observed lock-mass: $m_{\\text{L,observed}} = 371.101737$\n-   Theoretical analyte monoisotopic mass: $m_{\\text{A,theoretical}} = 300.123456$\n-   Observed analyte monoisotopic mass: $m_{\\text{A,observed}} = 300.124011$\n-   Charge state: $z=1$\n-   Mass error model: The instrument's mass error is approximated by an additive constant offset over the relevant mass range.\n-   Task 1: Derive an expression for mass accuracy in parts per million (ppm).\n-   Task 2: Derive the lock-mass-corrected analyte mass.\n-   Task 3: Compute the analyte's post-correction mass accuracy in ppm.\n-   Rounding requirement: Round the final answer to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as lock-mass correction is a standard and fundamental technique in high-resolution mass spectrometry for improving mass accuracy by correcting for instrument drift. The given mass values are realistic for small organic molecules. The assumption of an additive constant mass error is a common and reasonable first-order approximation for instrument calibration over a limited mass range. The problem is well-posed, providing all necessary data and a clear, unambiguous objective. The language is precise and objective. The problem is therefore deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe solution proceeds in four stages as requested by the problem:\n1.  Derivation of the mass accuracy formula in parts per million (ppm).\n2.  Calculation of the mass error offset using the lock-mass calibrant.\n3.  Calculation of the lock-mass-corrected analyte mass.\n4.  Calculation of the post-correction mass accuracy for the analyte.\n\n#### 1. Derivation of Mass Accuracy in ppm\n\nMass accuracy is a measure of the error in a mass measurement. The absolute error, $\\Delta m$, is the difference between the observed mass, $m_{\\text{observed}}$, and the theoretical mass, $m_{\\text{theoretical}}$.\n$$\n\\Delta m = m_{\\text{observed}} - m_{\\text{theoretical}}\n$$\nThe relative error, $E_{\\text{rel}}$, normalizes the absolute error by the theoretical mass, making it a dimensionless quantity.\n$$\nE_{\\text{rel}} = \\frac{\\Delta m}{m_{\\text{theoretical}}} = \\frac{m_{\\text{observed}} - m_{\\text{theoretical}}}{m_{\\text{theoretical}}}\n$$\nThe unit \"parts per million\" (ppm) corresponds to a factor of $10^{-6}$. Therefore, to express the relative error in ppm, we multiply it by $10^6$. Let this be denoted as $E_{\\text{ppm}}$.\n$$\nE_{\\text{ppm}} = E_{\\text{rel}} \\times 10^6 = \\left( \\frac{m_{\\text{observed}} - m_{\\text{theoretical}}}{m_{\\text{theoretical}}} \\right) \\times 10^6\n$$\nThis is the required expression for mass accuracy in ppm.\n\n#### 2. Calculation of the Mass Error Offset\n\nThe problem states that the instrument's mass error can be modeled as an additive constant offset, which we will denote as $\\delta$. This implies that for any true mass $m_{\\text{true}}$, the instrument measures an observed mass $m_{\\text{observed}}$ given by:\n$$\nm_{\\text{observed}} = m_{\\text{true}} + \\delta\n$$\nThe lock-mass calibrant provides a reference point to estimate this offset. Using the lock-mass data, the offset $\\delta$ can be calculated as the difference between the observed and theoretical lock-mass values.\n$$\n\\delta = m_{\\text{L,observed}} - m_{\\text{L,theoretical}}\n$$\nSubstituting the given values:\n$$\n\\delta = 371.101737 - 371.101237 = 0.000500\n$$\nThis constant offset of $0.000500$ Da (Daltons) is assumed to apply to all measurements in the relevant mass range, including that of the analyte.\n\n#### 3. Calculation of the Lock-Mass-Corrected Analyte Mass\n\nTo find the corrected mass of the analyte, $m_{\\text{A,corrected}}$, we subtract the calculated mass error offset $\\delta$ from the observed analyte mass, $m_{\\text{A,observed}}$.\n$$\nm_{\\text{A,corrected}} = m_{\\text{A,observed}} - \\delta\n$$\nSubstituting the given value for $m_{\\text{A,observed}}$ and the calculated value for $\\delta$:\n$$\nm_{\\text{A,corrected}} = 300.124011 - 0.000500 = 300.123511\n$$\nThis is the lock-mass-corrected analyte mass.\n\n#### 4. Calculation of Post-Correction Mass Accuracy\n\nFinally, we compute the analyte's mass accuracy in ppm after the lock-mass correction. We use the derived formula for $E_{\\text{ppm}}$ with the corrected analyte mass, $m_{\\text{A,corrected}}$, now serving as the \"observed\" mass, and compare it to the theoretical analyte mass, $m_{\\text{A,theoretical}}$.\n$$\nE_{\\text{A,ppm}} = \\left( \\frac{m_{\\text{A,corrected}} - m_{\\text{A,theoretical}}}{m_{\\text{A,theoretical}}} \\right) \\times 10^6\n$$\nFirst, we calculate the new absolute error for the analyte:\n$$\n\\Delta m_{\\text{A,corrected}} = m_{\\text{A,corrected}} - m_{\\text{A,theoretical}} = 300.123511 - 300.123456 = 0.000055\n$$\nNow, we calculate the ppm error:\n$$\nE_{\\text{A,ppm}} = \\left( \\frac{0.000055}{300.123456} \\right) \\times 10^6 \\approx 0.18325609 \\text{ ppm}\n$$\nThe problem requires rounding the final answer to four significant figures. The first significant figure is $1$, followed by $8$, $3$, and $2$. The fifth significant figure is $5$, which requires rounding up the fourth digit.\n$$\nE_{\\text{A,ppm}} \\approx 0.1833 \\text{ ppm}\n$$\nThis is the final post-correction mass accuracy for the analyte.", "answer": "$$\n\\boxed{0.1833}\n$$", "id": "3693918"}, {"introduction": "With accurately calibrated mass data, an automated platform can begin to extract crucial chemical information, such as the elemental composition. The natural abundance of isotopes like $^{13}$C creates a distinct isotopic pattern, where the relative intensity of the satellite peaks reveals the number of carbon atoms in the molecule. This practice moves beyond simple arithmetic to apply a powerful statistical framework, maximum likelihood estimation, to determine the most probable carbon count from isotopic data and to quantify the confidence in that estimate [@problem_id:3693966]. This is a prime example of how modern platforms use statistical inference to reason about chemical structures.", "problem": "An automated mass spectrometry structure elucidation platform based on Fourier Transform Ion Cyclotron Resonance (FT-ICR) acquires fine isotopic structure for a molecular ion and algorithmically assigns isotopologue counts. The platform isolates carbon-13 satellite peaks by correcting for non-carbon isotopic contributions (for example, nitrogen-15, deuterium, and oxygen-17) using internal calibration and elemental ratio priors, and then reports the number of detected ions corresponding to isotopologues with exactly one carbon-13 substitution.\n\nAssume the following scientifically standard foundations:\n- Each carbon site independently carries the heavy isotope carbon-13 with probability $q$ (the natural abundance of carbon-13 under terrestrial conditions).\n- For a molecule with $n$ carbon atoms, the total number of carbon-13 substitutions follows a Binomial distribution across the $n$ sites.\n- Ion counting by the detector is well modeled by independent sampling from the isotopologue distribution across $N$ ions assigned to the monoisotopic cluster of the molecular ion.\n\nThe platform reports $N = 200000$ total assigned ions and $x_{1} = 28280$ ions assigned to the isotopologue with exactly one carbon-13 substitution. The calibrated natural abundance is $q = 0.0110$.\n\nStarting only from the foundations stated above, use maximum likelihood to estimate the number of carbon atoms $n$ in the analyte and, using a Fisher information approximation, quantify the uncertainty as a standard deviation for the estimator. Express your final numerical answers as follows:\n- Round both the maximum likelihood estimate and its standard deviation to three significant figures.\n- Report the two numbers in a single response.\n- No units are required; treat the count of carbons as dimensionless.", "solution": "The problem is situated in spectrometric identification of organic compounds using fine isotopic structure, where an automated structure elucidation platform estimates elemental counts by exploiting isotopic patterns. The fundamental statistical base is that each carbon atom independently carries the heavy isotope carbon-13 with probability $q$, so the number of carbon-13 substitutions across $n$ carbon sites is Binomial.\n\nLet $n$ denote the number of carbon atoms. The number of carbon-13 substitutions in a given molecule is a Binomial random variable $K \\sim \\text{Binomial}(n, q)$, with probability mass function\n$$\n\\Pr(K = k) = \\binom{n}{k} q^{k} (1 - q)^{n - k}.\n$$\nThe event \"exactly one carbon-13 substitution\" has probability\n$$\np(n) = \\Pr(K = 1) = \\binom{n}{1} q (1 - q)^{n - 1} = n q (1 - q)^{n - 1}.\n$$\nAcross $N$ independent ion assignments, the count $x_{1}$ of ions with exactly one carbon-13 substitution is Binomial with parameters $N$ and $p(n)$,\n$$\nx_{1} \\sim \\text{Binomial}(N, p(n)), \\quad p(n) = n q (1 - q)^{n - 1}.\n$$\nThe likelihood for $n$ given $x_{1}$ and $N$ (up to a multiplicative constant independent of $n$) is\n$$\nL(n) \\propto \\left[p(n)\\right]^{x_{1}} \\left[1 - p(n)\\right]^{N - x_{1}},\n$$\nand the log-likelihood is\n$$\n\\ell(n) = x_{1} \\ln p(n) + (N - x_{1}) \\ln\\!\\left(1 - p(n)\\right).\n$$\nTo find the maximum likelihood estimate, differentiate with respect to $n$ and set the derivative to zero. Write $s = 1 - q$ to simplify notation. Then $p(n) = n q s^{n - 1}$ and\n$$\n\\frac{d}{dn} p(n) = q s^{n - 1} \\left(1 + n \\ln s \\right),\n$$\nwhich follows from $\\ln p(n) = \\ln q + \\ln n + (n - 1) \\ln s$ and the chain rule. The derivative of the log-likelihood is\n$$\n\\frac{d}{dn} \\ell(n) = x_{1} \\frac{p'(n)}{p(n)} - (N - x_{1}) \\frac{p'(n)}{1 - p(n)} = p'(n) \\left[ \\frac{x_{1}}{p(n)} - \\frac{N - x_{1}}{1 - p(n)} \\right].\n$$\nSetting $\\frac{d}{dn} \\ell(n) = 0$ yields the likelihood equation\n$$\n\\frac{x_{1}}{p(n)} = \\frac{N - x_{1}}{1 - p(n)} \\quad \\Rightarrow \\quad \\frac{x_{1}}{N} = p(n).\n$$\nDefine the empirical fraction $y = x_{1} / N$. The maximum likelihood estimate $\\hat{n}$ solves\n$$\ny = n q s^{n - 1}.\n$$\nThis equation can be solved in closed form using the Lambert $W$ function. Rearranging gives\n$$\n\\frac{y}{q} = n s^{n - 1} \\quad \\Rightarrow \\quad \\frac{y s}{q} = n s^{n}.\n$$\nLet $\\alpha = \\ln s$, so $s^{n} = \\exp(\\alpha n)$. Then\n$$\nn \\exp(\\alpha n) = \\frac{y s}{q}.\n$$\nSet $z = \\alpha n$. Then the equation becomes\n$$\n\\frac{z}{\\alpha} \\exp(z) = \\frac{y s}{q} \\quad \\Rightarrow \\quad z \\exp(z) = \\alpha \\frac{y s}{q}.\n$$\nBy definition of the Lambert $W$ function, $z = W\\!\\left(\\alpha \\frac{y s}{q}\\right)$, hence\n$$\n\\hat{n} = \\frac{W\\!\\left(\\alpha \\frac{y s}{q}\\right)}{\\alpha}, \\quad \\text{with} \\quad \\alpha = \\ln(1 - q), \\quad s = 1 - q, \\quad y = \\frac{x_{1}}{N}.\n$$\n\nTo quantify uncertainty, we use the Fisher information approximation. For the Binomial likelihood in the reparameterization by $n$, the Fisher information in $n$ is\n$$\nI(n) = N \\frac{\\left[p'(n)\\right]^{2}}{p(n) \\left[1 - p(n)\\right]}.\n$$\nWe already have $p(n) = n q s^{n - 1}$ and $p'(n) = q s^{n - 1} \\left(1 + n \\ln s \\right)$. Substituting yields\n$$\nI(n) = N \\cdot \\frac{\\left[q s^{n - 1} \\left(1 + n \\ln s \\right)\\right]^{2}}{n q s^{n - 1} \\left(1 - n q s^{n - 1}\\right)} = N \\cdot \\frac{q s^{n - 1} \\left(1 + n \\ln s \\right)^{2}}{n \\left(1 - n q s^{n - 1}\\right)}.\n$$\nThe asymptotic variance of the maximum likelihood estimator is approximated by the inverse Fisher information evaluated at $\\hat{n}$,\n$$\n\\operatorname{Var}(\\hat{n}) \\approx \\frac{1}{I(\\hat{n})}, \\quad \\text{so} \\quad \\operatorname{sd}(\\hat{n}) \\approx \\sqrt{\\frac{1}{I(\\hat{n})}}.\n$$\n\nNow plug in the given numerical values. The inputs are $q = 0.0110$, $N = 200000$, $x_{1} = 28280$. Compute $y = x_{1}/N$,\n$$\ny = \\frac{28280}{200000} = 0.1414.\n$$\nThen $s = 1 - q = 0.9890$ and $\\alpha = \\ln(s) = \\ln(0.9890)$. Numerically, $\\alpha \\approx -0.011061$. The argument for the Lambert $W$ function is\n$$\n\\alpha \\frac{y s}{q} = \\alpha \\cdot \\frac{0.1414 \\times 0.9890}{0.0110}.\n$$\nCompute $c = \\frac{y s}{q} \\approx \\frac{0.1414 \\times 0.9890}{0.0110} \\approx 12.713$, hence $\\alpha c \\approx -0.011061 \\times 12.713 \\approx -0.1406$. The Lambert $W$ evaluation gives $W(-0.1406) \\approx -0.166$. Therefore\n$$\n\\hat{n} = \\frac{W(\\alpha c)}{\\alpha} \\approx \\frac{-0.166}{-0.011061} \\approx 15.01.\n$$\nRounding to three significant figures, the maximum likelihood estimate is $15.0$.\n\nNext compute the Fisher information at $\\hat{n} \\approx 15.01$. Using $p(\\hat{n}) = y = 0.1414$, $s^{\\hat{n} - 1} = s^{14} = \\exp(14 \\alpha) \\approx \\exp(14 \\times -0.011061) \\approx \\exp(-0.1549) \\approx 0.8564$, and\n$$\n1 + \\hat{n} \\ln s \\approx 1 + 15.01 \\times (-0.011061) \\approx 0.834.\n$$\nThen\n$$\nI(\\hat{n}) = N \\cdot \\frac{q s^{\\hat{n} - 1} \\left(1 + \\hat{n} \\ln s \\right)^{2}}{\\hat{n} \\left(1 - \\hat{n} q s^{\\hat{n} - 1}\\right)} \\approx 200000 \\cdot \\frac{0.0110 \\times 0.8564 \\times (0.834)^{2}}{15.01 \\times \\left(1 - 0.1414\\right)}.\n$$\nNumerically, the numerator factor is $0.0110 \\times 0.8564 \\times (0.834)^{2} \\approx 0.00655$, and the denominator factor is $15.01 \\times 0.8586 \\approx 12.88$, giving\n$$\nI(\\hat{n}) \\approx 200000 \\times \\frac{0.00655}{12.88} \\approx 1.02 \\times 10^{2}.\n$$\nHence\n$$\n\\operatorname{sd}(\\hat{n}) \\approx \\sqrt{\\frac{1}{I(\\hat{n})}} \\approx \\sqrt{\\frac{1}{1.02 \\times 10^{2}}} \\approx 0.0992.\n$$\nRounded to three significant figures, the standard deviation is $0.0992$.\n\nReport the two numbers in a single response, rounded to three significant figures, with no units.", "answer": "$$\\boxed{\\begin{pmatrix}15.0 & 0.0992\\end{pmatrix}}$$", "id": "3693966"}, {"introduction": "While precursor ion data helps determine a molecular formula, tandem mass spectrometry (MS/MS) provides the fragmentation data needed to piece together the atomic connectivity. Automated platforms test candidate structures by simulating their fragmentation and scoring how well the predicted fragments explain the observed MS/MS spectrum. This hands-on programming challenge guides you through building a core scoring algorithm for fragmentation pathways [@problem_id:3693961]. You will construct a probabilistic objective function that combines spectral evidence with prior chemical knowledge, learning how to translate complex chemical reasoning into a computable score that can rank structural hypotheses.", "problem": "You are designing a simplified automated structure elucidation module that scores fragmentation trees against observed tandem mass spectrometry (MS/MS) data. The module must operate on a fragmentation graph in which nodes represent candidate substructures (including the precursor) and directed edges represent plausible cleavages yielding a neutral loss. The task is to construct the fragmentation graph from provided masses, define a principled objective function grounded in probabilistic modeling, and compute the most plausible directed arborescence (directed spanning tree rooted at the precursor) that maximizes the objective.\n\nThe fundamental base for this problem is the interpretation of MS/MS intensities as proxies for fragment formation probabilities, Gaussian mass measurement error for peak-to-fragment matching, and neutral loss plausibility priors derived from empirical chemistry knowledge. Your derivation and computation must start from:\n\n- The Gaussian probability density function for mass measurement error, which for a peak at mass-to-charge $m/z$ value $x$ matching a fragment of mass $m$ with mass accuracy $\\sigma$ is $$\\mathcal{N}(x; m, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - m)^2}{2\\sigma^2}\\right).$$\n- Empirical priors for neutral losses (e.g., water, ammonia, methyl, carbon monoxide, carbon dioxide), specified as base probabilities known from fragmentation chemistry.\n\nYou must formalize the objective function for a directed arborescence $T$ on the fragmentation graph $G=(V,E)$ rooted at the precursor node $r$, with node set $V=\\{r, v_1, \\dots, v_n\\}$ and directed edges $E$. Each node $v_i$ for $i \\geq 1$ has a candidate fragment mass $m_i$ in daltons (Da), and the precursor node $r$ has mass $M_p$ in daltons. The observed MS/MS spectrum is a finite set of peaks $S=\\{(m_j, I_j)\\}_{j=1}^N$ with $m_j$ in daltons and nonnegative intensities $I_j$ that you must normalize so that $\\sum_{j=1}^{N} I_j = 1$. Let $\\alpha > 0$ be an intensity weighting exponent, and let $\\sigma > 0$ be the mass accuracy in daltons.\n\nDefine the fragment likelihood for node $v$ with mass $m_v$ as a mixture of Gaussian likelihoods weighted by intensities,\n$$\n\\ell(v) = \\sum_{j=1}^{N} I_j^\\alpha \\, \\mathcal{N}(m_j; m_v, \\sigma).\n$$\nDefine the neutral loss prior for a directed edge $(u \\to v)$ with loss magnitude $d = m_u - m_v > 0$ using the following rule. Let the set of known losses be\n$$\n\\mathcal{L} = \\{(\\text{H}_2\\text{O}, \\, 18.0106, \\, p_{\\text{H}_2\\text{O}}), \\, (\\text{NH}_3, \\, 17.0265, \\, p_{\\text{NH}_3}), \\, (\\text{CH}_3, \\, 15.0235, \\, p_{\\text{CH}_3}), \\, (\\text{CO}, \\, 27.9949, \\, p_{\\text{CO}}), \\, (\\text{CO}_2, \\, 43.9898, \\, p_{\\text{CO}_2}) \\},\n$$\nwhere the middle entry is the loss mass in daltons and the last entry is its base prior probability. Given a tolerance $\\delta > 0$ in daltons, if there exists a known loss with mass $l$ such that $|d - l| \\leq \\delta$, set the prior to that loss’s base probability; otherwise, set an unknown-loss prior decaying with magnitude,\n$$\n\\pi(d) = p_u \\, \\exp(-\\lambda d),\n$$\nwhere $p_u \\in (0,1)$ and $\\lambda > 0$ are constants.\n\nLet $\\beta > 0$ weight the contribution of the prior and let $\\gamma \\geq 0$ penalize larger losses. Define the total objective for a directed arborescence $T$ with edge set $E_T \\subseteq E$ (every node $v \\in V \\setminus \\{r\\}$ must have exactly one incoming edge) as\n$$\nJ(T) = \\sum_{(u \\to v) \\in E_T} \\left[ \\log \\ell(v) + \\beta \\log \\pi(m_u - m_v) - \\gamma (m_u - m_v) \\right].\n$$\nYou must assume that edges are only allowed from higher mass to lower mass, i.e., $(u \\to v) \\in E$ is permitted only if $m_u > m_v$. Under this constraint, the fragmentation graph is a directed acyclic graph in mass order.\n\nYour program must:\n\n1. For each test case, construct the node set $V$ from the precursor mass $M_p$ and the list of candidate fragment masses $m_i$ (in daltons). Construct the directed edge set $E$ by including all pairs $(u \\to v)$ such that $m_u > m_v$, including edges from the precursor node $r$ to every fragment node $v$.\n2. Compute $\\ell(v)$ for every fragment node using the normalized intensities and the Gaussian mass likelihood. Use a small floor value $\\epsilon = 10^{-12}$ to avoid taking the logarithm of zero by replacing $\\ell(v)$ with $\\max(\\ell(v), \\epsilon)$.\n3. Compute $\\pi(d)$ for every edge using the known-loss tolerance rule above.\n4. Compute edge weights $w(u \\to v) = \\log \\ell(v) + \\beta \\log \\pi(m_u - m_v) - \\gamma (m_u - m_v)$.\n5. Because the graph is acyclic in mass order, find the maximum objective arborescence by choosing, for each fragment node $v \\in V \\setminus \\{r\\}$, the single incoming edge $(u \\to v)$ with the largest weight $w(u \\to v)$ among all permitted parents $u$ with $m_u > m_v$. The total objective $J(T)$ is then the sum of these selected weights over all fragment nodes.\n6. Produce the final output for the test suite as a single line containing the list of total objective values $J(T)$ for each test case, rounded to six decimal places, as a comma-separated list enclosed in square brackets. No other text may be printed.\n\nPhysical and numerical units: All masses ($m_j$, $m_v$, $M_p$, and known-loss masses) are in daltons (Da). The final answers must be floats (decimals).\n\nUse the following test suite. For each case, the inputs are provided explicitly:\n\n- Case A (happy path with multiple known neutral losses):\n    - Precursor mass $M_p = 180.0630$ Da.\n    - Candidate fragment masses (Da): $[162.0524, \\, 145.0259, \\, 130.0024, \\, 102.0075]$.\n    - Observed peaks $(m_j, I_j)$: $[(162.0520, \\, 0.8), \\, (145.0260, \\, 0.6), \\, (130.0020, \\, 0.5), \\, (102.0080, \\, 0.3), \\, (90.0000, \\, 0.1)]$.\n- Case B (boundary condition with a single dominant fragment):\n    - Precursor mass $M_p = 150.0520$ Da.\n    - Candidate fragment masses (Da): $[132.0414]$.\n    - Observed peaks $(m_j, I_j)$: $[(132.0410, \\, 0.9), \\, (120.0000, \\, 0.2), \\, (100.0000, \\, 0.1)]$.\n- Case C (edge case with ambiguity between direct and stepwise losses):\n    - Precursor mass $M_p = 200.0840$ Da.\n    - Candidate fragment masses (Da): $[182.0734, \\, 165.0469, \\, 150.0234]$.\n    - Observed peaks $(m_j, I_j)$: $[(165.0470, \\, 0.7), \\, (150.0235, \\, 0.6), \\, (182.0730, \\, 0.05), \\, (140.0000, \\, 0.1)]$.\n\nGlobal parameters (identical across all test cases):\n- Mass accuracy $\\sigma = 0.01$ Da.\n- Intensity exponent $\\alpha = 0.5$.\n- Known losses and base priors:\n  - Water $\\text{H}_2\\text{O}$: mass $18.0106$ Da, base prior $p_{\\text{H}_2\\text{O}} = 0.3$.\n  - Ammonia $\\text{NH}_3$: mass $17.0265$ Da, base prior $p_{\\text{NH}_3} = 0.2$.\n  - Methyl $\\text{CH}_3$: mass $15.0235$ Da, base prior $p_{\\text{CH}_3} = 0.1$.\n  - Carbon monoxide $\\text{CO}$: mass $27.9949$ Da, base prior $p_{\\text{CO}} = 0.05$.\n  - Carbon dioxide $\\text{CO}_2$: mass $43.9898$ Da, base prior $p_{\\text{CO}_2} = 0.05$.\n- Tolerance for known-loss matching $\\delta = 0.25$ Da.\n- Unknown-loss prior parameters: $p_u = 0.02$, $\\lambda = 0.02$ $\\text{Da}^{-1}$.\n- Loss magnitude penalty coefficient $\\gamma = 0.01$.\n- Prior weighting coefficient $\\beta = 1.0$.\n- Likelihood floor $\\epsilon = 10^{-12}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1, r_2, r_3]$), where each $r_i$ is the total objective value $J(T)$ for the corresponding test case, rounded to six decimal places.", "solution": "The problem requires the design and implementation of a module for scoring fragmentation arborescences in tandem mass spectrometry. This involves constructing a probabilistic objective function and finding the arborescence that maximizes it. The solution proceeds by first principles, starting from the probabilistic interpretation of spectral data and chemical knowledge, and culminating in an efficient optimization algorithm.\n\nThe core task is to find the most plausible directed arborescence $T$ rooted at a precursor node $r$ within a fragmentation graph $G=(V, E)$. An arborescence is a directed spanning tree where every node except the root has exactly one incoming edge. This structure represents a hypothesis about how the precursor molecule fragmented to produce the observed smaller molecules (fragments).\n\nFirst, we formalize the graph structure. The set of nodes $V$ comprises the precursor ion, denoted $r$ with mass $M_p$, and a set of candidate fragment ions $\\{v_1, \\dots, v_n\\}$ with corresponding masses $\\{m_1, \\dots, m_n\\}$. A directed edge $(u \\to v)$ exists in the edge set $E$ if and only if the mass of node $u$, $m_u$, is greater than the mass of node $v$, $m_v$. This constraint, $m_u > m_v$, ensures that fragmentation always results in a positive mass loss, $d = m_u - m_v > 0$. Consequently, the graph $G$ is a Directed Acyclic Graph (DAG), where nodes are partially ordered by mass.\n\nNext, we define a principled, probabilistic objective function $J(T)$ for an arborescence $T$. The score of an entire arborescence is defined as the sum of scores of its constituent edges. The score for a single edge $(u \\to v)$ in the arborescence, denoted $w(u \\to v)$, is composed of three terms. The total objective for an arborescence $T$ with edge set $E_T$ is:\n$$\nJ(T) = \\sum_{(u \\to v) \\in E_T} w(u \\to v)\n$$\nwhere the weight of an edge is given by:\n$$\nw(u \\to v) = \\log \\ell(v) + \\beta \\log \\pi(m_u - m_v) - \\gamma (m_u - m_v)\n$$\nEach term in the edge weight has a distinct physical and probabilistic meaning. The use of logarithms transforms a product of probabilities (a joint probability model) into a sum of log-probabilities, which is numerically more stable and analytically more tractable.\n\nThe first term, $\\log \\ell(v)$, is the log-likelihood of the fragment node $v$. It quantifies the evidence for the existence of a fragment with mass $m_v$ based on the observed MS/MS spectrum $S = \\{(m_j, I_j)\\}_{j=1}^N$. The likelihood $\\ell(v)$ is formulated as a mixture model:\n$$\n\\ell(v) = \\sum_{j=1}^{N} I_j'^\\alpha \\, \\mathcal{N}(m_j; m_v, \\sigma)\n$$\nHere, $I_j'$ represents the normalized intensity of the $j$-th peak, such that $\\sum_{j=1}^{N} I_j' = 1$. The exponent $\\alpha$ is a weighting factor that modulates the influence of peak intensities. The function $\\mathcal{N}(m_j; m_v, \\sigma)$ is the Gaussian probability density function, which models the probability of observing a peak at mass $m_j$ given that the true fragment mass is $m_v$, with a standard deviation $\\sigma$ representing the mass accuracy of the instrument. This term effectively measures how well the theoretical mass of the candidate fragment $v$ is explained by the observed spectral peaks. To prevent numerical issues with taking the logarithm of zero, we apply a floor value $\\epsilon$, replacing $\\ell(v)$ with $\\max(\\ell(v), \\epsilon)$.\n\nThe second term, $\\beta \\log \\pi(m_u - m_v)$, incorporates prior chemical knowledge. The function $\\pi(d)$ is the prior probability of a neutral loss of mass $d = m_u - m_v$. Certain neutral losses (e.g., of small, stable molecules like water or carbon monoxide) are chemically preferred and occur with high frequency. The model captures this by defining a set of known common losses $\\mathcal{L}$. If the calculated loss $d$ matches a known loss mass $l \\in \\mathcal{L}$ within a given tolerance $\\delta$ (i.e., $|d-l| \\le \\delta$), its prior probability is set to a pre-defined value $p_l$. If multiple known losses match, the one with the smallest mass difference is chosen. For all other \"unknown\" losses, we assign a prior that decays exponentially with the loss mass: $\\pi(d) = p_u \\, \\exp(-\\lambda d)$. This reflects the decreasing plausibility of losing arbitrarily large, non-specific fragments. The coefficient $\\beta$ balances the contribution of this prior information against the data-driven likelihood term.\n\nThe third term, $-\\gamma (m_u - m_v)$, is a penalty proportional to the mass of the neutral loss. It acts as a regularization term, controlled by the coefficient $\\gamma \\geq 0$. This term introduces a preference for fragmentation pathways with smaller steps, promoting more detailed and parsimonious explanations, all else being equal.\n\nThe final step is to find the arborescence $T$ that maximizes $J(T)$. Because the graph is a DAG and the edge weights are defined as they are, the problem simplifies significantly. The structure of the objective function allows for a greedy optimization. The full objective is a sum over the edges in the arborescence. Since each fragment node $v \\in V \\setminus \\{r\\}$ has exactly one incoming edge, we can rewrite the objective as a sum over the fragment nodes, where for each node we select its best parent:\n$$\nJ(T) = \\sum_{v \\in V \\setminus \\{r\\}} \\max_{(u \\to v) \\in E} w(u \\to v)\n$$\nThis is equivalent to:\n$$\nJ(T) = \\sum_{v \\in V \\setminus \\{r\\}} \\left( \\log \\ell(v) + \\max_{(u \\to v) \\in E} \\left[ \\beta \\log \\pi(m_u - m_v) - \\gamma(m_u - m_v) \\right] \\right)\n$$\nThis decomposition reveals that the optimal parent for a node $v$ can be chosen independently of the choices made for any other node. The algorithm is therefore as follows:\n1.  For each fragment node $v_i$, calculate its likelihood $\\ell(v_i)$ based on the spectral data.\n2.  For each fragment node $v_i$, consider all possible parent nodes $u$ (i.e., all nodes with $m_u > m_{v_i}$).\n3.  For each potential parent $u$, calculate the corresponding edge weight $w(u \\to v_i)$.\n4.  Identify the parent $u^*$ that yields the maximum edge weight for $v_i$. This maximum weight is the contribution of node $v_i$ to the total score.\n5.  The total objective $J(T)$ is the sum of these maximum weights over all fragment nodes.\n\nThis procedure guarantees finding the maximum-scoring arborescence in polynomial time, specifically $O(n^2)$ where $n$ is the number of nodes, as each of the $O(n)$ fragment nodes considers $O(n)$ potential parents. The implementation will precisely follow this logic.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the structure elucidation problem for all test cases.\n    \"\"\"\n    # Global parameters (identical across all test cases)\n    params = {\n        'sigma': 0.01,\n        'alpha': 0.5,\n        'known_losses': [\n            (18.0106, 0.3),   # H2O\n            (17.0265, 0.2),   # NH3\n            (15.0235, 0.1),   # CH3\n            (27.9949, 0.05),  # CO\n            (43.9898, 0.05),  # CO2\n        ],\n        'delta': 0.25,\n        'pu': 0.02,\n        'lambda_': 0.02,\n        'gamma': 0.01,\n        'beta': 1.0,\n        'epsilon': 1e-12,\n    }\n\n    # Test cases\n    test_cases = [\n        # Case A\n        (180.0630, [162.0524, 145.0259, 130.0024, 102.0075],\n         [(162.0520, 0.8), (145.0260, 0.6), (130.0020, 0.5), (102.0080, 0.3), (90.0000, 0.1)]),\n        # Case B\n        (150.0520, [132.0414],\n         [(132.0410, 0.9), (120.0000, 0.2), (100.0000, 0.1)]),\n        # Case C\n        (200.0840, [182.0734, 165.0469, 150.0234],\n         [(165.0470, 0.7), (150.0235, 0.6), (182.0730, 0.05), (140.0000, 0.1)]),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_max_objective(case, params)\n        results.append(round(result, 6))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef gaussian_pdf(x, mu, sigma):\n    \"\"\"\n    Calculates the Gaussian probability density function.\n    N(x; mu, sigma) = (1 / (sigma * sqrt(2*pi))) * exp(-(x - mu)^2 / (2*sigma^2))\n    \"\"\"\n    prefactor = 1.0 / (sigma * np.sqrt(2 * np.pi))\n    exponent = -0.5 * ((x - mu) / sigma)**2\n    return prefactor * np.exp(exponent)\n\ndef get_loss_prior(loss_mass, params):\n    \"\"\"Calculates the neutral loss prior pi(d).\"\"\"\n    min_mass_diff = np.inf\n    best_prior = None\n\n    for l_mass, l_prior in params['known_losses']:\n        diff = abs(loss_mass - l_mass)\n        if diff  min_mass_diff:\n            min_mass_diff = diff\n            best_prior = l_prior\n\n    if min_mass_diff = params['delta']:\n        return best_prior\n    else:\n        return params['pu'] * np.exp(-params['lambda_'] * loss_mass)\n\ndef calculate_likelihood(fragment_mass, peaks, params):\n    \"\"\"Calculates the fragment likelihood l(v).\"\"\"\n    total_intensity = sum(I for _, I in peaks)\n    if total_intensity == 0:\n        return params['epsilon']\n\n    likelihood = 0.0\n    for peak_mass, intensity in peaks:\n        norm_intensity = intensity / total_intensity\n        likelihood += (norm_intensity**params['alpha']) * gaussian_pdf(peak_mass, fragment_mass, params['sigma'])\n    \n    return max(likelihood, params['epsilon'])\n\ndef calculate_max_objective(case, params):\n    \"\"\"Calculates the total objective J(T) for a single test case.\"\"\"\n    precursor_mass, fragment_masses, peaks = case\n    \n    # Node masses include the precursor\n    all_node_masses = [precursor_mass] + fragment_masses\n    \n    # Pre-compute fragment likelihoods\n    fragment_likelihoods = {\n        fm: calculate_likelihood(fm, peaks, params) for fm in fragment_masses\n    }\n\n    total_objective = 0.0\n    \n    # For each fragment, find the best parent and add its edge weight to the total\n    for frag_mass in fragment_masses:\n        max_edge_weight = -np.inf\n        log_likelihood = np.log(fragment_likelihoods[frag_mass])\n\n        # Potential parents are the precursor and any heavier fragments\n        potential_parents = [m for m in all_node_masses if m > frag_mass]\n\n        for parent_mass in potential_parents:\n            loss_mass = parent_mass - frag_mass\n            \n            prior_prob = get_loss_prior(loss_mass, params)\n            \n            # Use a large negative number for log(0)\n            log_prior = np.log(prior_prob) if prior_prob > 0 else -np.inf\n            \n            # Compute edge weight\n            edge_weight = log_likelihood + params['beta'] * log_prior - params['gamma'] * loss_mass\n            \n            if edge_weight > max_edge_weight:\n                max_edge_weight = edge_weight\n        \n        # Add the max weight for this fragment to the total objective\n        total_objective += max_edge_weight\n\n    return total_objective\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3693961"}]}