{"hands_on_practices": [{"introduction": "A primary goal of lattice gauge theory is to compute physical observables, which exist in the continuum of spacetime. Since simulations are necessarily performed on a discrete grid with a finite lattice spacing $a$, a critical final step in any calculation is the extrapolation of results to the continuum limit, where $a \\to 0$. This practice [@problem_id:3560462] guides you through this essential procedure by applying a weighted least squares fit to mock data, using a fitting function motivated by the Symanzik effective theory to control for discretization artifacts. Mastering this technique is fundamental to extracting meaningful physical predictions from numerical lattice data.", "problem": "You are given measurements of a dimensionless lattice observable $O(a)$ computed at several nonzero lattice spacings $a$, together with independent standard deviations for each measurement. Motivated by the Symanzik effective theory for cutoff effects in lattice gauge theory, assume the following model for discretization errors: $$O(a) = O(0) + c_1 \\, a^2 + c_2 \\, a^4,$$ where $O(0)$ is the continuum-limit value to be estimated, and $c_1$ and $c_2$ are unknown coefficients. Suppose each measured value $y_i$ at lattice spacing $a_i$ satisfies $$y_i = O(a_i) + \\varepsilon_i,$$ where the $\\varepsilon_i$ are independent Gaussian random variables with zero mean and known variances $\\sigma_i^2$. Work in units where all quantities are dimensionless.\n\nYour task is to estimate $O(0)$ for each dataset using maximum likelihood estimation under the Gaussian noise model, which is equivalent to a weighted least squares fit of the model above.\n\nImplement a program that performs the weighted least squares estimation of $O(0)$ for each of the following three datasets. Each dataset provides a list of lattice spacings $a_i$, observed values $y_i$, and standard deviations $\\sigma_i$. The three datasets are:\n\n- Dataset A (general case, multiple spacings with moderate discretization effects):\n  - $a$: [$0.14$, $0.12$, $0.10$, $0.08$, $0.06$]\n  - $y$: [$0.52000792$, $0.51449632$, $0.50965$, $0.50627952$, $0.50399352$]\n  - $\\sigma$: [$0.0015$, $0.0012$, $0.0010$, $0.0010$, $0.0009$]\n\n- Dataset B (coverage from coarse to fine spacings with stronger $a^4$ effects):\n  - $a$: [$0.20$, $0.16$, $0.12$, $0.08$, $0.04$]\n  - $y$: [$0.7279$, $0.73662304$, $0.74187104$, $0.74562144$, $0.74914384$]\n  - $\\sigma$: [$0.0020$, $0.0016$, $0.0013$, $0.0011$, $0.0010$]\n\n- Dataset C (edge case with the minimal number of points to identify all three parameters):\n  - $a$: [$0.18$, $0.10$, $0.05$]\n  - $y$: [$1.25139952$, $1.2505$, $1.2498125$]\n  - $\\sigma$: [$0.0012$, $0.0008$, $0.0006$]\n\nMathematical and algorithmic requirements:\n\n- Treat the parameter vector as $\\boldsymbol{\\beta} = \\big(O(0), c_1, c_2\\big)$ and the design matrix with rows $\\big(1, a_i^2, a_i^4\\big)$.\n- Use weighted least squares under the assumption of independent Gaussian errors with known variances $\\sigma_i^2$.\n- The estimator must be computed by solving the normal equations in a numerically stable way using standard linear algebra routines.\n- For each dataset, output only the estimated $O(0)$, rounded to $6$ decimal places.\n\nFinal output requirement:\n\n- Your program should produce a single line of output containing the three estimated continuum values $O(0)$ for Dataset A, Dataset B, and Dataset C, in that order, as a comma-separated list enclosed in square brackets. For example, the format must be exactly like this (the numbers below are illustrative only): \n  - Example format: [$0.123456$, $0.234567$, $0.345678$]\n- The numbers in your output must be rounded to exactly $6$ decimal places.\n\nTest suite coverage rationale:\n\n- Dataset A is a general multi-point case testing robust estimation with moderate discretization effects.\n- Dataset B emphasizes stronger higher-order effects and a broader range of lattice spacings to test sensitivity to the $a^4$ term.\n- Dataset C is an edge case with exactly three points, stressing identifiability and matrix conditioning when the model is just-determined.\n\nYour solution should be universally implementable in any modern programming language and must follow the weighted least squares principle as described above. No physical units are required because all quantities are dimensionless.", "solution": "The user wants me to solve the problem. I will now proceed with generating the solution.\n### Step 1: Extract Givens\n\nThe problem provides the following information:\n- A model for the lattice observable $O(a)$ as a function of lattice spacing $a$:\n$$O(a) = O(0) + c_1 \\, a^2 + c_2 \\, a^4$$\n- The parameters to be estimated are $O(0)$, $c_1$, and $c_2$.\n- The relationship between a measurement $y_i$ at lattice spacing $a_i$ and the model is:\n$$y_i = O(a_i) + \\varepsilon_i$$\n- The errors $\\varepsilon_i$ are independent Gaussian random variables with mean $0$ and known variances $\\sigma_i^2$.\n- The parameter vector is defined as $\\boldsymbol{\\beta} = \\big(O(0), c_1, c_2\\big)$.\n- The design matrix has rows of the form $\\big(1, a_i^2, a_i^4\\big)$.\n- Three datasets are provided:\n    - **Dataset A**:\n        - $a$: [$0.14$, $0.12$, $0.10$, $0.08$, $0.06$]\n        - $y$: [$0.52000792$, $0.51449632$, $0.50965$, $0.50627952$, $0.50399352$]\n        - $\\sigma$: [$0.0015$, $0.0012$, $0.0010$, $0.0010$, $0.0009$]\n    - **Dataset B**:\n        - $a$: [$0.20$, $0.16$, $0.12$, $0.08$, $0.04$]\n        - $y$: [$0.7279$, $0.73662304$, $0.74187104$, $0.74562144$, $0.74914384$]\n        - $\\sigma$: [$0.0020$, $0.0016$, $0.0013$, $0.0011$, $0.0010$]\n    - **Dataset C**:\n        - $a$: [$0.18$, $0.10$, $0.05$]\n        - $y$: [$1.25139952$, $1.2505$, $1.2498125$]\n        - $\\sigma$: [$0.0012$, $0.0008$, $0.0006$]\n- The task is to estimate $O(0)$ for each dataset using maximum likelihood estimation, which is equivalent to weighted least squares.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The model $O(a) = O(0) + c_1 \\, a^2 + c_2 \\, a^4$ is a standard expansion used in Symanzik effective theory to describe discretization errors for non-improved Wilson-fermion or gauge-action observables in lattice gauge theory. The use of a Gaussian noise model and maximum likelihood estimation (equivalent to weighted least squares for this case) is a standard and robust statistical method for parameter estimation from experimental or numerical data. The problem is firmly grounded in computational physics principles.\n- **Well-Posed**: The problem is to solve a linear regression problem for the parameters $\\boldsymbol{\\beta} = \\big(O(0), c_1, c_2\\big)^T$. A unique solution exists if the normal equations matrix is invertible. For Datasets A and B, there are $5$ data points to determine $3$ parameters, making the system over-determined and generally well-posed. For Dataset C, there are exactly $3$ data points for $3$ parameters, making the system just-determined. The design matrix $X$ for Dataset C has rows $\\big(1, a_i^2, a_i^4\\big)$. Its determinant is proportional to $(a_2^2 - a_1^2)(a_3^2 - a_1^2)(a_3^2 - a_2^2)$. Since all given $a_i$ values are distinct and positive, the determinant is non-zero, the matrix is invertible, and a unique solution exists. The problem is well-posed for all datasets.\n- **Objective**: The problem is stated in precise, unambiguous mathematical language. The data and requirements are objective and clearly specified.\n\n### Step 3: Verdict and Action\n\n**Verdict**: The problem is valid, scientifically sound, well-posed, and objective. I will proceed with the solution.\n\n###\nThe goal is to find the continuum-limit value $O(0)$ of an observable by fitting the model $O(a) = O(0) + c_1 a^2 + c_2 a^4$ to data points $(a_i, y_i)$ with associated uncertainties $\\sigma_i$. This is a linear regression problem, as the model is linear in its parameters $\\boldsymbol{\\beta} = \\big(O(0), c_1, c_2\\big)^T$. The system of equations for $n$ data points can be expressed in matrix form as:\n$$ \\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} $$\nwhere $\\mathbf{y}$ is the $n \\times 1$ vector of observed values, $X$ is the $n \\times 3$ design matrix, $\\boldsymbol{\\beta}$ is the $3 \\times 1$ parameter vector, and $\\boldsymbol{\\varepsilon}$ is the $n \\times 1$ vector of random errors.\n\nThe $i$-th row of the design matrix $X$ corresponds to the $i$-th measurement and is given by $\\big(1, a_i^2, a_i^4\\big)$. Thus,\n$$\nX = \\begin{pmatrix}\n1 & a_1^2 & a_1^4 \\\\\n1 & a_2^2 & a_2^4 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & a_n^2 & a_n^4\n\\end{pmatrix}\n, \\quad\n\\boldsymbol{\\beta} = \\begin{pmatrix}\nO(0) \\\\ c_1 \\\\ c_2\n\\end{pmatrix}\n, \\quad\n\\mathbf{y} = \\begin{pmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n\\end{pmatrix}\n$$\nThe problem states that the errors $\\varepsilon_i$ are independent and normally distributed with mean $0$ and variance $\\sigma_i^2$. Maximizing the likelihood of observing the data $\\mathbf{y}$ given the parameters $\\boldsymbol{\\beta}$ is equivalent to minimizing the weighted sum of squared residuals, also known as the chi-squared, $\\chi^2$:\n$$ \\chi^2(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( \\frac{y_i - (O(0) + c_1 a_i^2 + c_2 a_i^4)}{\\sigma_i} \\right)^2 $$\nThis can be expressed in matrix form as:\n$$ \\chi^2(\\boldsymbol{\\beta}) = (\\mathbf{y} - X\\boldsymbol{\\beta})^T W (\\mathbf{y} - X\\boldsymbol{\\beta}) $$\nwhere $W$ is the weight matrix, which is the inverse of the error covariance matrix. Since the errors are independent, $W$ is a diagonal matrix with elements $W_{ii} = 1/\\sigma_i^2$.\n\nThe parameter vector $\\hat{\\boldsymbol{\\beta}}$ that minimizes $\\chi^2$ is found by solving the weighted least squares normal equations:\n$$ (X^T W X) \\hat{\\boldsymbol{\\beta}} = X^T W \\mathbf{y} $$\nThe solution is formally given by:\n$$ \\hat{\\boldsymbol{\\beta}} = (X^T W X)^{-1} X^T W \\mathbf{y} $$\nFor numerical stability, it is preferable to solve this system without explicitly computing the inverse. A standard method is to transform the weighted least squares (WLS) problem into an ordinary least squares (OLS) problem. We define a scaled vector $\\mathbf{y}'$ and a scaled design matrix $X'$:\n$$ \\mathbf{y}' = W^{1/2} \\mathbf{y} \\quad \\text{and} \\quad X' = W^{1/2} X $$\nwhere $W^{1/2}$ is a diagonal matrix with entries $(W^{1/2})_{ii} = 1/\\sigma_i$. This means the $i$-th row of $X'$ is the $i$-th row of $X$ divided by $\\sigma_i$, and the $i$-th element of $\\mathbf{y}'$ is $y_i/\\sigma_i$. The $\\chi^2$ expression becomes the unweighted sum of squares for the transformed variables:\n$$ \\chi^2(\\boldsymbol{\\beta}) = (\\mathbf{y}' - X'\\boldsymbol{\\beta})^T (\\mathbf{y}' - X'\\boldsymbol{\\beta}) $$\nThis is a standard OLS problem, whose solution $\\hat{\\boldsymbol{\\beta}}$ can be found using robust numerical techniques like QR decomposition. This is the procedure implemented by standard library functions such as `numpy.linalg.lstsq`.\n\nThe algorithm for each dataset is as follows:\n1. Construct the vector of observations $\\mathbf{y}$ from the provided $y_i$ values.\n2. Construct the design matrix $X$, where the $i$-th row is $(1, a_i^2, a_i^4)$.\n3. Construct the scaled matrix $X'$ by dividing each row $i$ of $X$ by the corresponding standard deviation $\\sigma_i$.\n4. Construct the scaled vector $\\mathbf{y}'$ by dividing each element $y_i$ by $\\sigma_i$.\n5. Solve the linear least squares system $X'\\boldsymbol{\\beta} \\approx \\mathbf{y}'$ for the parameter vector $\\hat{\\boldsymbol{\\beta}} = \\big(\\hat{O}(0), \\hat{c}_1, \\hat{c}_2\\big)^T$.\n6. The first element of the resulting vector, $\\hat{\\boldsymbol{\\beta}}_0$, is the estimated value of $O(0)$.\n7. Round the estimate to $6$ decimal places as required. This procedure is repeated for all three datasets.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs weighted least squares estimation for continuum limit extrapolation\n    on three datasets as specified in the problem.\n    \"\"\"\n    \n    # Define the three datasets from the problem statement.\n    test_cases = [\n        # Dataset A (general case)\n        {\n            \"a\": [0.14, 0.12, 0.10, 0.08, 0.06],\n            \"y\": [0.52000792, 0.51449632, 0.50965, 0.50627952, 0.50399352],\n            \"sigma\": [0.0015, 0.0012, 0.0010, 0.0010, 0.0009]\n        },\n        # Dataset B (stronger a^4 effects)\n        {\n            \"a\": [0.20, 0.16, 0.12, 0.08, 0.04],\n            \"y\": [0.7279, 0.73662304, 0.74187104, 0.74562144, 0.74914384],\n            \"sigma\": [0.0020, 0.0016, 0.0013, 0.0011, 0.0010]\n        },\n        # Dataset C (minimal number of points)\n        {\n            \"a\": [0.18, 0.10, 0.05],\n            \"y\": [1.25139952, 1.2505, 1.2498125],\n            \"sigma\": [0.0012, 0.0008, 0.0006]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Extract data and convert to numpy arrays for calculation\n        a = np.array(case[\"a\"])\n        y = np.array(case[\"y\"])\n        sigma = np.array(case[\"sigma\"])\n        \n        # The model is O(a) = beta_0 + beta_1 * a^2 + beta_2 * a^4\n        # where beta_0 = O(0), beta_1 = c1, beta_2 = c2.\n        # This is a linear model. We perform a weighted least squares fit.\n        \n        # Construct the design matrix X.\n        # The columns are the basis functions: 1, a^2, a^4.\n        X = np.vstack([np.ones_like(a), a**2, a**4]).T\n        \n        # A weighted least squares problem with weights w_i = 1/sigma_i^2 is\n        # equivalent to an ordinary least squares problem on variables scaled\n        # by 1/sigma_i.\n        \n        # Scale the rows of the design matrix X by 1/sigma.\n        # The inv_sigma must be a column vector to broadcast correctly.\n        inv_sigma = 1.0 / sigma\n        X_prime = X * inv_sigma[:, np.newaxis]\n        \n        # Scale the measurement vector y by 1/sigma.\n        y_prime = y * inv_sigma\n        \n        # Solve the linear least squares system X_prime * beta = y_prime\n        # for the parameter vector beta = [O(0), c1, c2].\n        # np.linalg.lstsq is a robust solver, typically using QR decomposition.\n        beta_hat, _, _, _ = np.linalg.lstsq(X_prime, y_prime, rcond=None)\n        \n        # The first element of the solution vector is the estimate for O(0).\n        O0_estimate = beta_hat[0]\n        \n        results.append(O0_estimate)\n\n    # Format the results to exactly 6 decimal places and print in the required format.\n    results_formatted = [\"{:.6f}\".format(r) for r in results]\n    print(f\"[{','.join(results_formatted)}]\")\n\nsolve()\n```", "id": "3560462"}, {"introduction": "The choice of the lattice action significantly impacts the accuracy and efficiency of a simulation. While the simple Wilson gauge action provides a valid discretization, \"improved\" actions like the Symanzik and Iwasaki actions are designed to systematically reduce discretization errors. This practice [@problem_id:3560442] offers a hands-on comparison of these actions by analyzing their cutoff effects on modern gradient-flow scales, demonstrating a key principle of lattice gauge theory: how theoretical improvements to the action manifest as a more rapid convergence to the continuum limit in practice.", "problem": "You are tasked with analyzing cut-off effects in pure-gauge lattice simulations by comparing three standard gauge actions: the Wilson action, the tree-level Symanzik-improved action, and the Iwasaki action. The comparison focuses on how the gradient-flow scales respond to changes in lattice spacing. The two gradient-flow scales of interest are defined by the conditions\n$$t^{2}\\langle E(t)\\rangle\\big|_{t=t_{0}} = c$$\nand\n$$t\\frac{d}{dt}\\big(t^{2}\\langle E(t)\\rangle\\big)\\big|_{t=w_{0}^{2}} = d,$$\nwhere $\\langle E(t)\\rangle$ is the energy density at flow time $t$, and $c$ and $d$ are scheme-setting constants taken to be $c=d=0.3$ in many applications. In the continuum limit of Quantum Chromodynamics (QCD), the gradient flow defines renormalized observables with a smoothing radius of order $\\sqrt{8t}$, and for sufficiently smooth gauge fields, the Symanzik effective theory asserts that discretization effects for these scales in pure-gauge theories enter at leading order as $\\mathcal{O}(a^{2})$ for gauge actions that respect the relevant symmetries.\n\nStarting from the Symanzik expansion and the definitions above, assume the following leading-order scaling relations for the measured scales:\n$$t_{0}(a) = t_{0}^{\\text{cont}} + S_{t_{0}}\\,a^{2} + \\mathcal{O}(a^{4}),$$\n$$w_{0}^{2}(a) = (w_{0}^{2})^{\\text{cont}} + S_{w_{0}^{2}}\\,a^{2} + \\mathcal{O}(a^{4}),$$\nwhere $t_{0}^{\\text{cont}}$ and $(w_{0}^{2})^{\\text{cont}}$ are the continuum-limit values, $S_{t_{0}}$ and $S_{w_{0}^{2}}$ are action-dependent slopes that quantify leading cut-off effects, and $a$ is the lattice spacing. All quantities must be handled with consistent physical units: $a$ in femtometers ($\\mathrm{fm}$), $t_{0}$ in square femtometers ($\\mathrm{fm}^{2}$), and $w_{0}^{2}$ in square femtometers ($\\mathrm{fm}^{2}$). Angles do not appear in this task. Percentages do not appear in this task.\n\nWrite a complete, runnable program that, given test-suite measurements of $t_{0}(a)$ and $w_{0}^{2}(a)$ at several lattice spacings $a$, performs a linear regression in $a^{2}$ to extract, for each case, the estimates of $t_{0}^{\\text{cont}}$, $S_{t_{0}}$, $(w_{0}^{2})^{\\text{cont}}$, and $S_{w_{0}^{2}}$. Use Ordinary Least Squares (OLS) regression, justified by the leading-order linear-in-$a^{2}$ behavior from the Symanzik effective theory.\n\nYour program must apply the following test suite. For scientific realism and reproducibility in this exercise, the measured values are constructed to be consistent with the stated leading-order relations without statistical noise. Use these values exactly.\n\n- Test Case $1$ (Wilson action, coarse spacings):\n  - Lattice spacings $a$ in $\\mathrm{fm}$: $[0.12, 0.09, 0.06]$.\n  - Measured $t_{0}(a)$ in $\\mathrm{fm}^{2}$: $[0.0247, 0.02155, 0.0193]$.\n  - Measured $w_{0}^{2}(a)$ in $\\mathrm{fm}^{2}$: $[0.034945, 0.033055, 0.031705]$.\n\n- Test Case $2$ (tree-level Symanzik action, coarse spacings):\n  - Lattice spacings $a$ in $\\mathrm{fm}$: $[0.12, 0.09, 0.06]$.\n  - Measured $t_{0}(a)$ in $\\mathrm{fm}^{2}$: $[0.02038, 0.01912, 0.01822]$.\n  - Measured $w_{0}^{2}(a)$ in $\\mathrm{fm}^{2}$: $[0.032353, 0.031597, 0.031057]$.\n\n- Test Case $3$ (Iwasaki action, coarse spacings):\n  - Lattice spacings $a$ in $\\mathrm{fm}$: $[0.12, 0.09, 0.06]$.\n  - Measured $t_{0}(a)$ in $\\mathrm{fm}^{2}$: $[0.018652, 0.018148, 0.017788]$.\n  - Measured $w_{0}^{2}(a)$ in $\\mathrm{fm}^{2}$: $[0.031345, 0.03103, 0.030805]$.\n\n- Test Case $4$ (Wilson action, fine spacings approaching the continuum boundary condition):\n  - Lattice spacings $a$ in $\\mathrm{fm}$: $[0.03, 0.02, 0.015]$.\n  - Measured $t_{0}(a)$ in $\\mathrm{fm}^{2}$: $[0.01795, 0.0177, 0.0176125]$.\n  - Measured $w_{0}^{2}(a)$ in $\\mathrm{fm}^{2}$: $[0.030895, 0.030745, 0.0306925]$.\n\nAlgorithmic requirements:\n- For each test case, perform two independent OLS fits of the form $y = \\beta_{0} + \\beta_{1} x$:\n  - Fit $y = t_{0}(a)$ against $x = a^{2}$ to obtain $\\beta_{0} \\equiv t_{0}^{\\text{cont}}$ and $\\beta_{1} \\equiv S_{t_{0}}$.\n  - Fit $y = w_{0}^{2}(a)$ against $x = a^{2}$ to obtain $\\beta_{0} \\equiv (w_{0}^{2})^{\\text{cont}}$ and $\\beta_{1} \\equiv S_{w_{0}^{2}}$.\n- Implement the OLS estimators from first principles:\n  $$\\bar{x} = \\frac{1}{N}\\sum_{i=1}^{N} x_{i}, \\quad \\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_{i},$$\n  $$S_{xy} = \\sum_{i=1}^{N} (x_{i}-\\bar{x})(y_{i}-\\bar{y}), \\quad S_{xx} = \\sum_{i=1}^{N} (x_{i}-\\bar{x})^{2},$$\n  $$\\hat{\\beta}_{1} = \\frac{S_{xy}}{S_{xx}}, \\quad \\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}.$$\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- The results for each test case must be appended in order, each contributing exactly four floating-point numbers: $[t_{0}^{\\text{cont}}, S_{t_{0}}, (w_{0}^{2})^{\\text{cont}}, S_{w_{0}^{2}}]$.\n- Thus the final line must contain a flattened list of $16$ floats corresponding to the four test cases, in the order $1$ through $4$.\n- Express all results in the physical units stated above: $t_{0}^{\\text{cont}}$ in $\\mathrm{fm}^{2}$, $(w_{0}^{2})^{\\text{cont}}$ in $\\mathrm{fm}^{2}$, and the slopes $S_{t_{0}}$ and $S_{w_{0}^{2}}$ as dimensionless factors multiplying $a^{2}$ in $\\mathrm{fm}^{2}$, such that the products yield $\\mathrm{fm}^{2}$-valued corrections. Print the floating-point numbers in standard decimal form.\n\nYour program must be self-contained, require no input, and use only permitted libraries. The numerical values chosen are scientifically plausible within the context of computational nuclear physics and the fundamentals of lattice gauge theory, while being simplified to emphasize the derivation from the Symanzik effective theory and the gradient-flow definitions.", "solution": "The comparison of cut-off effects across gauge actions in lattice gauge theory rests on two pillars: the Symanzik effective theory for discretization artifacts and the renormalized gradient-flow scales. In pure-gauge Quantum Chromodynamics (QCD), actions such as the Wilson action, the tree-level Symanzik-improved action, and the Iwasaki action differ by the coefficients of higher-dimensional operators in their lattice discretizations. The Symanzik effective theory states that the lattice theory at spacing $a$ is equivalent to the continuum theory supplemented by a tower of higher-dimensional operators suppressed by powers of $a$. For observables that respect the relevant discrete symmetries and are sufficiently smooth in the continuum limit, the leading discretization corrections are typically $\\mathcal{O}(a^{2})$ for gauge actions designed without $\\mathcal{O}(a)$ terms.\n\nThe gradient flow defines a continuous smoothing of the gauge field with flow time $t$, which has a smoothing radius of order $\\sqrt{8t}$. The energy density $\\langle E(t)\\rangle$ is a renormalized quantity at positive $t$, and reference scales $t_{0}$ and $w_{0}$ are defined by the conditions\n$$t^{2}\\langle E(t)\\rangle\\big|_{t=t_{0}} = c, \\quad t\\frac{d}{dt}\\big(t^{2}\\langle E(t)\\rangle\\big)\\big|_{t=w_{0}^{2}} = d,$$\nwith $c=d=0.3$ in many standard choices. These definitions yield dimensionful scales that can be used to set the lattice spacing in physical units.\n\nUnder the Symanzik framework, we expect for such scales\n$$t_{0}(a) = t_{0}^{\\text{cont}} + S_{t_{0}}\\,a^{2} + \\mathcal{O}(a^{4}), \\quad w_{0}^{2}(a) = (w_{0}^{2})^{\\text{cont}} + S_{w_{0}^{2}}\\,a^{2} + \\mathcal{O}(a^{4}),$$\nwhere the action-dependent slopes $S_{t_{0}}$ and $S_{w_{0}^{2}}$ encode leading cut-off effects. These relations follow from dimensional analysis and the absence of $\\mathcal{O}(a)$ terms in the pure-gauge sector for the actions considered, together with the smoothness of the gradient-flow observables. Because $a^{2}$ carries units of $\\mathrm{fm}^{2}$, the intercepts $t_{0}^{\\text{cont}}$ and $(w_{0}^{2})^{\\text{cont}}$ also carry units of $\\mathrm{fm}^{2}$, while the slopes $S_{t_{0}}$ and $S_{w_{0}^{2}}$ are dimensionless coefficients multiplying $a^{2}$.\n\nTo estimate the continuum limits and slopes from measured values at several lattice spacings, we employ Ordinary Least Squares (OLS) regression. For each test case, we fit a straight line $y = \\beta_{0} + \\beta_{1} x$ with $x = a^{2}$ and $y$ equal to either $t_{0}(a)$ or $w_{0}^{2}(a)$. The OLS estimators are\n$$\\bar{x} = \\frac{1}{N}\\sum_{i=1}^{N} x_{i}, \\quad \\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_{i},$$\n$$S_{xy} = \\sum_{i=1}^{N} (x_{i}-\\bar{x})(y_{i}-\\bar{y}), \\quad S_{xx} = \\sum_{i=1}^{N} (x_{i}-\\bar{x})^{2},$$\n$$\\hat{\\beta}_{1} = \\frac{S_{xy}}{S_{xx}}, \\quad \\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}.$$\nWe identify $\\hat{\\beta}_{0}$ with the continuum-limit value and $\\hat{\\beta}_{1}$ with the leading slope in $a^{2}$.\n\nThe provided measurements are constructed to follow the leading-order scaling relations exactly, with physically plausible values:\n- A common continuum baseline of $t_{0}^{\\text{cont}} = 0.0175\\,\\mathrm{fm}^{2}$ and $(w_{0}^{2})^{\\text{cont}} = 0.030625\\,\\mathrm{fm}^{2}$ is used.\n- Action-dependent slopes for the coarse sets are:\n  - Wilson: $S_{t_{0}} = 0.50$, $S_{w_{0}^{2}} = 0.30$.\n  - Tree-level Symanzik: $S_{t_{0}} = 0.20$, $S_{w_{0}^{2}} = 0.12$.\n  - Iwasaki: $S_{t_{0}} = 0.08$, $S_{w_{0}^{2}} = 0.05$.\n- For the fine Wilson set, the same slopes are used, emphasizing the boundary condition near the continuum.\n\nBecause the measurements satisfy $y = \\beta_{0} + \\beta_{1} x$ exactly, the OLS fits recover the parameters without error. We can verify the parameters analytically:\n- For Test Case $1$ (Wilson, coarse), with $x = [0.0144, 0.0081, 0.0036]$,\n  - $t_{0}(a) = 0.0175 + 0.50\\,x$ yields $\\hat{\\beta}_{0} = 0.0175$, $\\hat{\\beta}_{1} = 0.50$.\n  - $w_{0}^{2}(a) = 0.030625 + 0.30\\,x$ yields $\\hat{\\beta}_{0} = 0.030625$, $\\hat{\\beta}_{1} = 0.30$.\n- For Test Case $2$ (Symanzik, coarse),\n  - $t_{0}(a) = 0.0175 + 0.20\\,x$ yields $\\hat{\\beta}_{0} = 0.0175$, $\\hat{\\beta}_{1} = 0.20$.\n  - $w_{0}^{2}(a) = 0.030625 + 0.12\\,x$ yields $\\hat{\\beta}_{0} = 0.030625$, $\\hat{\\beta}_{1} = 0.12$.\n- For Test Case $3$ (Iwasaki, coarse),\n  - $t_{0}(a) = 0.0175 + 0.08\\,x$ yields $\\hat{\\beta}_{0} = 0.0175$, $\\hat{\\beta}_{1} = 0.08$.\n  - $w_{0}^{2}(a) = 0.030625 + 0.05\\,x$ yields $\\hat{\\beta}_{0} = 0.030625$, $\\hat{\\beta}_{1} = 0.05$.\n- For Test Case $4$ (Wilson, fine), with $x = [0.0009, 0.0004, 0.000225]$,\n  - $t_{0}(a) = 0.0175 + 0.50\\,x$ yields $\\hat{\\beta}_{0} = 0.0175$, $\\hat{\\beta}_{1} = 0.50$.\n  - $w_{0}^{2}(a) = 0.030625 + 0.30\\,x$ yields $\\hat{\\beta}_{0} = 0.030625$, $\\hat{\\beta}_{1} = 0.30$.\n\nThese results demonstrate that, as expected, the improved actions (Symanzik and Iwasaki) exhibit smaller leading slopes $S_{t_{0}}$ and $S_{w_{0}^{2}}$ than the Wilson action, indicating reduced cut-off effects. The fine Wilson set confirms that the same slopes govern the approach to the continuum boundary where $a\\to 0$, and the intercepts recover the continuum scales.\n\nThe program implements the OLS computations directly and prints the flattened list of $16$ floats in the prescribed order:\n$$[t_{0}^{\\text{cont}}, S_{t_{0}}, (w_{0}^{2})^{\\text{cont}}, S_{w_{0}^{2}}]$$\nfor Test Cases $1$ through $4$, aggregated into a single bracketed list on one line. All values are reported in $\\mathrm{fm}^{2}$ for the intercepts and dimensionless slopes consistent with the linear-in-$a^{2}$ corrections, ensuring unit consistency.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef ols_fit(x, y):\n    \"\"\"\n    Perform an Ordinary Least Squares (OLS) fit for y = b0 + b1 * x.\n    Returns (b0, b1).\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    Sxy = np.sum((x - x_mean) * (y - y_mean))\n    Sxx = np.sum((x - x_mean) ** 2)\n    # Guard against the degenerate case where all x-values are identical (Sxx=0).\n    if Sxx == 0.0:\n        # If all x are identical, the slope is undefined.\n        # The best fit is a horizontal line at the mean of y.\n        return (y_mean, 0.0)\n    b1 = Sxy / Sxx\n    b0 = y_mean - b1 * x_mean\n    return (b0, b1)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: dict with 'a_fm', 't0_fm2', 'w0sq_fm2'\n    test_cases = [\n        # Test Case 1: Wilson action, coarse spacings\n        {\n            \"a_fm\": [0.12, 0.09, 0.06],\n            \"t0_fm2\": [0.0247, 0.02155, 0.0193],\n            \"w0sq_fm2\": [0.034945, 0.033055, 0.031705],\n        },\n        # Test Case 2: Tree-level Symanzik action, coarse spacings\n        {\n            \"a_fm\": [0.12, 0.09, 0.06],\n            \"t0_fm2\": [0.02038, 0.01912, 0.01822],\n            \"w0sq_fm2\": [0.032353, 0.031597, 0.031057],\n        },\n        # Test Case 3: Iwasaki action, coarse spacings\n        {\n            \"a_fm\": [0.12, 0.09, 0.06],\n            \"t0_fm2\": [0.018652, 0.018148, 0.017788],\n            \"w0sq_fm2\": [0.031345, 0.03103, 0.030805],\n        },\n        # Test Case 4: Wilson action, fine spacings\n        {\n            \"a_fm\": [0.03, 0.02, 0.015],\n            \"t0_fm2\": [0.01795, 0.0177, 0.0176125],\n            \"w0sq_fm2\": [0.030895, 0.030745, 0.0306925],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        a = np.array(case[\"a_fm\"], dtype=float)\n        a2 = a**2\n        t0 = np.array(case[\"t0_fm2\"], dtype=float)\n        w0sq = np.array(case[\"w0sq_fm2\"], dtype=float)\n\n        # Fit t0(a) = t0_cont + S_t0 * a^2\n        t0_cont, S_t0 = ols_fit(a2, t0)\n        # Fit w0^2(a) = w0sq_cont + S_w0sq * a^2\n        w0sq_cont, S_w0sq = ols_fit(a2, w0sq)\n\n        # Append results in the required order\n        results.extend([\n            t0_cont, S_t0, w0sq_cont, S_w0sq\n        ])\n\n    # Format floats to a reasonable precision for readability\n    formatted = []\n    for val in results:\n        # Use 6 decimal places to ensure consistent output format\n        formatted.append(f\"{val:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted)}]\")\n\nsolve()\n```", "id": "3560442"}, {"introduction": "Beyond physical accuracy, the feasibility of large-scale lattice simulations hinges on computational performance. Modern high-performance computing, especially on GPUs, requires careful optimization to effectively utilize the hardware. This practice [@problem_id:3560466] introduces the roofline model, a vital tool for performance analysis, applying it to the core components of the Hybrid Monte Carlo (HMC) algorithm. By calculating the arithmetic intensity and attainable performance, you will diagnose computational bottlenecks and quantify the impact of a powerful optimization technique known as kernel fusion.", "problem": "You are tasked with implementing a complete, runnable program that performs a principled roofline analysis for a simplified, yet scientifically sound, model of a Hybrid Monte Carlo (HMC) integrator step for lattice gauge theory with special unitary group of degree three ($SU(3)$). The goal is to compute, for several parameter sets, the arithmetic intensity, attainable performance under the roofline model, and the expected performance impact of kernel fusion between a gauge-force accumulation kernel and a link-update kernel on a graphics processing unit (GPU). The program must produce results that are universally applicable in computational terms and avoid reliance on hardware-specific micro-details beyond the provided peak compute and bandwidth parameters.\n\nStart from the following fundamental base:\n\n- In lattice gauge theory for the strong interaction, the gauge field is represented by link variables $U \\in SU(3)$ stored on edges of a hypercubic lattice. A link is a $3 \\times 3$ special unitary complex matrix. The Lie algebra element (the conjugate momentum in the Hamiltonian dynamics of Hybrid Monte Carlo) lives in $\\mathfrak{su}(3)$ and can be represented by $8$ real parameters corresponding to the eight generators of $SU(3)$. \n- The Hybrid Monte Carlo (HMC) algorithm evolves $(U,P)$ using Hamiltonian dynamics integrating gauge forces computed from local staples of neighboring links. A simplified model of a single integrator substep consists of:\n  1. A gauge-force kernel that reads neighbor $SU(3)$ link matrices to accumulate a force contribution and updates the momentum $P$.\n  2. A link-update kernel that constructs an exponential representation from $P$ and multiplies it into $U$.\n- The roofline model bounds attainable floating-point performance by the minimum of compute peak and the product of memory bandwidth and arithmetic intensity. If the memory bandwidth is $B$ in gigabytes per second and the arithmetic intensity is $I$ in floating-point operations per byte, then the attainable performance is $P_{\\mathrm{att}} = \\min\\{P_{\\mathrm{peak}}, B \\times I\\}$, where $P_{\\mathrm{peak}}$ is the peak floating-point throughput in gigaflops per second.\n- For a dense complex $3 \\times 3$ matrix-matrix product, we model the floating-point cost as $F_M = 198$ real floating-point operations. For an elementwise $3 \\times 3$ complex matrix addition, we model the floating-point cost as $F_A = 18$ real floating-point operations. These costs are widely used in lattice quantum chromodynamics (QCD) performance modeling.\n- Memory sizes follow from the data layout:\n  - Each $SU(3)$ link matrix is stored as $18$ real numbers; its size is $S_U = 18 s$ bytes, where $s$ is the bytes per real number ($s = 4$ for single precision or $s = 8$ for double precision).\n  - Each $\\mathfrak{su}(3)$ momentum is stored as $8$ real numbers; its size is $S_P = 8 s$ bytes.\n- The arithmetic intensity is $I = \\dfrac{F}{M}$, where $F$ is the total floating-point operation count and $M$ is the total data movement from or to global memory in bytes.\n- Kernel fusion that computes the force and immediately applies the update can reduce data movement by eliminating intermediate global-memory traffic for temporary results, raising $I$ and hence $P_{\\mathrm{att}}$.\n\nYour task is to compute, for each test case, the following quantities:\n\n1. For the separate gauge-force kernel:\n   - Floating-point count $F_{\\mathrm{force}} = M_{\\mathrm{force}} F_M + A_{\\mathrm{force}} F_A + F_{\\mathrm{proj}}$, where $M_{\\mathrm{force}}$ is the number of $SU(3)$ matrix-matrix multiplications per link, $A_{\\mathrm{force}}$ is the number of $SU(3)$ matrix additions per link, and $F_{\\mathrm{proj}}$ models any additional flops to project the computed object into $\\mathfrak{su}(3)$.\n   - Memory traffic $M_{\\mathrm{force}}^{\\mathrm{bytes}} = c_{\\mathrm{sep}} \\left(R_{\\mathrm{force},U} S_U + R_{\\mathrm{force},P} S_P + W_{\\mathrm{force}} S_P\\right)$, where $R_{\\mathrm{force},U}$ is the number of $SU(3)$ link reads, $R_{\\mathrm{force},P}$ is the number of momentum reads, $W_{\\mathrm{force}}$ is the number of momentum writes, and $c_{\\mathrm{sep}} \\geq 1$ models coalescing and cache inefficiency for separate kernels.\n   - Arithmetic intensity $I_{\\mathrm{force}} = \\dfrac{F_{\\mathrm{force}}}{M_{\\mathrm{force}}^{\\mathrm{bytes}}}$.\n   - Attainable performance $P_{\\mathrm{force}} = \\min\\{P_{\\mathrm{peak}}, B I_{\\mathrm{force}}\\}$ in gigaflops per second (GFLOP/s).\n   - Memory-bound boolean flag $b_{\\mathrm{force}}$, which is true if $B I_{\\mathrm{force}} < P_{\\mathrm{peak}}$ and false otherwise.\n\n2. For the separate link-update kernel:\n   - Floating-point count $F_{\\mathrm{update}} = M_{\\mathrm{update}} F_M + F_{\\mathrm{exp}}$, where $M_{\\mathrm{update}}$ is the number of $SU(3)$ matrix-matrix multiplications and $F_{\\mathrm{exp}}$ models the flops to construct the exponential from the momentum.\n   - Memory traffic $M_{\\mathrm{update}}^{\\mathrm{bytes}} = c_{\\mathrm{sep}} \\left(R_{\\mathrm{update},U} S_U + R_{\\mathrm{update},P} S_P + W_{\\mathrm{update},U} S_U\\right)$, where $R_{\\mathrm{update},U}$ is the number of $SU(3)$ link reads, $R_{\\mathrm{update},P}$ is the number of momentum reads, and $W_{\\mathrm{update},U}$ is the number of $SU(3)$ link writes.\n   - Arithmetic intensity $I_{\\mathrm{update}} = \\dfrac{F_{\\mathrm{update}}}{M_{\\mathrm{update}}^{\\mathrm{bytes}}}$.\n   - Attainable performance $P_{\\mathrm{update}} = \\min\\{P_{\\mathrm{peak}}, B I_{\\mathrm{update}}\\}$ in GFLOP/s.\n   - Memory-bound boolean flag $b_{\\mathrm{update}}$, defined analogously to $b_{\\mathrm{force}}$.\n\n3. For a fused gauge-force-plus-update kernel:\n   - Floating-point count $F_{\\mathrm{fused}} = F_{\\mathrm{force}} + F_{\\mathrm{update}}$.\n   - Memory traffic $M_{\\mathrm{fused}}^{\\mathrm{bytes}} = c_{\\mathrm{fused}} \\left(R_{\\mathrm{force},U} S_U + R_{\\mathrm{force},P} S_P + W_{\\mathrm{update},U} S_U\\right)$, which removes the intermediate momentum write and its subsequent read, and also removes the separate redundant link read because the force kernel already reads the required link matrix. The factor $c_{\\mathrm{fused}} \\geq 1$ allows for improved memory locality under fusion when $c_{\\mathrm{fused}} < c_{\\mathrm{sep}}$.\n   - Arithmetic intensity $I_{\\mathrm{fused}} = \\dfrac{F_{\\mathrm{fused}}}{M_{\\mathrm{fused}}^{\\mathrm{bytes}}}$.\n   - Attainable performance $P_{\\mathrm{fused}} = \\min\\{P_{\\mathrm{peak}}, B I_{\\mathrm{fused}}\\}$ in GFLOP/s.\n   - Memory-bound boolean flag $b_{\\mathrm{fused}}$.\n\n4. The predicted performance impact of fusion on the end-to-end substep time for one link update. Using the roofline-predicted performance for each kernel, define the model time per link for each kernel as $T = \\dfrac{F}{P_{\\mathrm{att}}}$ where $F$ is in floating-point operations and $P_{\\mathrm{att}}$ is in GFLOP/s; the common gigascale factor cancels in ratios. Then the separate-kernel time is $T_{\\mathrm{sep}} = \\dfrac{F_{\\mathrm{force}}}{P_{\\mathrm{force}}} + \\dfrac{F_{\\mathrm{update}}}{P_{\\mathrm{update}}}$ and the fused-kernel time is $T_{\\mathrm{fused}} = \\dfrac{F_{\\mathrm{fused}}}{P_{\\mathrm{fused}}}$. Report the speedup factor $S = \\dfrac{T_{\\mathrm{sep}}}{T_{\\mathrm{fused}}}$ as a floating-point number, with $S > 1$ indicating that fusion is beneficial.\n\nUnits and numerical conventions:\n\n- Treat memory bandwidth $B$ as gigabytes per second (GB/s).\n- Treat peak compute throughput $P_{\\mathrm{peak}}$ as gigaflops per second (GFLOP/s).\n- Arithmetic intensity $I$ is in floating-point operations per byte.\n- The attainable performance $P_{\\mathrm{att}}$ should be reported in GFLOP/s.\n- There are no angles involved. No physical units beyond those specified are required in the output.\n- All outputs should be provided as plain numbers without unit symbols.\n\nTest suite:\n\nImplement your program to compute the above quantities for the following four parameter sets. Each parameter set is defined by the tuple $(s, B, P_{\\mathrm{peak}}, M_{\\mathrm{force}}, A_{\\mathrm{force}}, F_{\\mathrm{proj}}, M_{\\mathrm{update}}, F_{\\mathrm{exp}}, R_{\\mathrm{force},U}, R_{\\mathrm{force},P}, W_{\\mathrm{force}}, R_{\\mathrm{update},U}, R_{\\mathrm{update},P}, W_{\\mathrm{update},U}, c_{\\mathrm{sep}}, c_{\\mathrm{fused}})$ with all parameters interpreted as described above.\n\n- Case $1$ (single precision, typical high-bandwidth GPU):\n  - $s = 4$, $B = 900$, $P_{\\mathrm{peak}} = 19000$, $M_{\\mathrm{force}} = 8$, $A_{\\mathrm{force}} = 6$, $F_{\\mathrm{proj}} = 120$, $M_{\\mathrm{update}} = 1$, $F_{\\mathrm{exp}} = 300$, $R_{\\mathrm{force},U} = 12$, $R_{\\mathrm{force},P} = 1$, $W_{\\mathrm{force}} = 1$, $R_{\\mathrm{update},U} = 1$, $R_{\\mathrm{update},P} = 1$, $W_{\\mathrm{update},U} = 1$, $c_{\\mathrm{sep}} = 1.0$, $c_{\\mathrm{fused}} = 1.0$.\n\n- Case $2$ (double precision on the same GPU class):\n  - $s = 8$, $B = 900$, $P_{\\mathrm{peak}} = 9700$, $M_{\\mathrm{force}} = 8$, $A_{\\mathrm{force}} = 6$, $F_{\\mathrm{proj}} = 200$, $M_{\\mathrm{update}} = 1$, $F_{\\mathrm{exp}} = 600$, $R_{\\mathrm{force},U} = 12$, $R_{\\mathrm{force},P} = 1$, $W_{\\mathrm{force}} = 1$, $R_{\\mathrm{update},U} = 1$, $R_{\\mathrm{update},P} = 1$, $W_{\\mathrm{update},U} = 1$, $c_{\\mathrm{sep}} = 1.0$, $c_{\\mathrm{fused}} = 1.0$.\n\n- Case $3$ (bandwidth-limited environment, single precision):\n  - $s = 4$, $B = 300$, $P_{\\mathrm{peak}} = 19000$, $M_{\\mathrm{force}} = 8$, $A_{\\mathrm{force}} = 6$, $F_{\\mathrm{proj}} = 120$, $M_{\\mathrm{update}} = 1$, $F_{\\mathrm{exp}} = 300$, $R_{\\mathrm{force},U} = 12$, $R_{\\mathrm{force},P} = 1$, $W_{\\mathrm{force}} = 1$, $R_{\\mathrm{update},U} = 1$, $R_{\\mathrm{update},P} = 1$, $W_{\\mathrm{update},U} = 1$, $c_{\\mathrm{sep}} = 1.0$, $c_{\\mathrm{fused}} = 1.0$.\n\n- Case $4$ (imperfect coalescing in separate kernels, improved locality under fusion, single precision):\n  - $s = 4$, $B = 900$, $P_{\\mathrm{peak}} = 19000$, $M_{\\mathrm{force}} = 8$, $A_{\\mathrm{force}} = 6$, $F_{\\mathrm{proj}} = 120$, $M_{\\mathrm{update}} = 1$, $F_{\\mathrm{exp}} = 300$, $R_{\\mathrm{force},U} = 12$, $R_{\\mathrm{force},P} = 1$, $W_{\\mathrm{force}} = 1$, $R_{\\mathrm{update},U} = 1$, $R_{\\mathrm{update},P} = 1$, $W_{\\mathrm{update},U} = 1$, $c_{\\mathrm{sep}} = 1.3$, $c_{\\mathrm{fused}} = 1.05$.\n\nRequired final output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list of per-case result lists, with no embedded spaces. For each case, output the list\n$[I_{\\mathrm{force}}, I_{\\mathrm{update}}, I_{\\mathrm{fused}}, P_{\\mathrm{force}}, P_{\\mathrm{update}}, P_{\\mathrm{fused}}, b_{\\mathrm{force}}, b_{\\mathrm{update}}, b_{\\mathrm{fused}}, S]$\nin this exact order. The booleans $b_{\\mathrm{force}}$, $b_{\\mathrm{update}}$, and $b_{\\mathrm{fused}}$ must be printed as $True$ or $False$. For example, the overall output has the form\n$[[r_{1,1},\\dots,r_{1,10}],[r_{2,1},\\dots,r_{2,10}],[r_{3,1},\\dots,r_{3,10}],[r_{4,1},\\dots,r_{4,10}]]$\nwith no spaces anywhere.", "solution": "The user has provided a problem statement that requires a roofline analysis for a simplified Hybrid Monte Carlo (HMC) integrator step in lattice gauge theory. The analysis is to be performed for separate and fused computational kernels.\n\n### Step 1: Extract Givens\n\nThe problem statement provides the following data, definitions, and constants:\n\n**Constants and Model Parameters:**\n-   $F_M$: Floating-point cost for a dense complex $3 \\times 3$ matrix-matrix product, modeled as $198$ real floating-point operations.\n-   $F_A$: Floating-point cost for an elementwise $3 \\times 3$ complex matrix addition, modeled as $18$ real floating-point operations.\n-   $s$: Bytes per real number ($4$ for single precision, $8$ for double precision).\n-   $S_U$: Memory size of an $SU(3)$ link matrix, defined as $18s$ bytes.\n-   $S_P$: Memory size of an $\\mathfrak{su}(3)$ momentum matrix, defined as $8s$ bytes.\n-   $B$: Memory bandwidth in gigabytes per second (GB/s).\n-   $P_{\\mathrm{peak}}$: Peak floating-point throughput in gigaflops per second (GFLOP/s).\n-   $c_{\\mathrm{sep}}$: Memory access inefficiency factor for separate kernels.\n-   $c_{\\mathrm{fused}}$: Memory access inefficiency factor for the fused kernel.\n\n**Kernel-Specific Parameters (per link):**\n-   $M_{\\mathrm{force}}$: Number of $SU(3)$ matrix-matrix multiplications in the force kernel.\n-   $A_{\\mathrm{force}}$: Number of $SU(3)$ matrix additions in the force kernel.\n-   $F_{\\mathrm{proj}}$: Additional flops for projection in the force kernel.\n-   $R_{\\mathrm{force},U}$: Number of $SU(3)$ link reads in the force kernel.\n-   $R_{\\mathrm{force},P}$: Number of momentum reads in the force kernel.\n-   $W_{\\mathrm{force}}$: Number of momentum writes in the force kernel.\n-   $M_{\\mathrm{update}}$: Number of $SU(3)$ matrix-matrix multiplications in the update kernel.\n-   $F_{\\mathrm{exp}}$: Additional flops for exponential construction in the update kernel.\n-   $R_{\\mathrm{update},U}$: Number of $SU(3)$ link reads in the update kernel.\n-   $R_{\\mathrm{update},P}$: Number of momentum reads in the update kernel.\n-   $W_{\\mathrm{update},U}$: Number of $SU(3)$ link writes in the update kernel.\n\n**Formulas:**\n-   Arithmetic Intensity: $I = \\dfrac{F}{M}$, where $F$ is total flops and $M$ is total memory traffic in bytes.\n-   Attainable Performance: $P_{\\mathrm{att}} = \\min\\{P_{\\mathrm{peak}}, B \\times I\\}$.\n-   Memory-bound flag, $b$: True if $B \\times I < P_{\\mathrm{peak}}$.\n-   Force kernel flops: $F_{\\mathrm{force}} = M_{\\mathrm{force}} F_M + A_{\\mathrm{force}} F_A + F_{\\mathrm{proj}}$.\n-   Force kernel memory: $M_{\\mathrm{force}}^{\\mathrm{bytes}} = c_{\\mathrm{sep}} \\left(R_{\\mathrm{force},U} S_U + R_{\\mathrm{force},P} S_P + W_{\\mathrm{force}} S_P\\right)$.\n-   Update kernel flops: $F_{\\mathrm{update}} = M_{\\mathrm{update}} F_M + F_{\\mathrm{exp}}$.\n-   Update kernel memory: $M_{\\mathrm{update}}^{\\mathrm{bytes}} = c_{\\mathrm{sep}} \\left(R_{\\mathrm{update},U} S_U + R_{\\mathrm{update},P} S_P + W_{\\mathrm{update},U} S_U\\right)$.\n-   Fused kernel flops: $F_{\\mathrm{fused}} = F_{\\mathrm{force}} + F_{\\mathrm{update}}$.\n-   Fused kernel memory: $M_{\\mathrm{fused}}^{\\mathrm{bytes}} = c_{\\mathrm{fused}} \\left(R_{\\mathrm{force},U} S_U + R_{\\mathrm{force},P} S_P + W_{\\mathrm{update},U} S_U\\right)$.\n-   Time for a kernel: $T = \\dfrac{F}{P_{\\mathrm{att}}}$.\n-   Separate kernel time: $T_{\\mathrm{sep}} = \\dfrac{F_{\\mathrm{force}}}{P_{\\mathrm{force}}} + \\dfrac{F_{\\mathrm{update}}}{P_{\\mathrm{update}}}$.\n-   Fused kernel time: $T_{\\mathrm{fused}} = \\dfrac{F_{\\mathrm{fused}}}{P_{\\mathrm{fused}}}$.\n-   Speedup: $S = \\dfrac{T_{\\mathrm{sep}}}{T_{\\mathrm{fused}}}$.\n\n**Test Cases:**\nFour test cases are provided as tuples of the above parameters.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is set in the context of computational lattice gauge theory, a subfield of nuclear and particle physics. The concepts used—$SU(3)$ matrices for gauge links, $\\mathfrak{su}(3)$ elements for momenta, Hybrid Monte Carlo, kernel fusion, and the roofline performance model—are all standard, valid, and central to the field. The flop counts for $SU(3)$ operations are standard approximations used in performance modeling. The problem is scientifically sound.\n-   **Well-Posed**: The problem provides a complete set of deterministic formulas and a complete set of input parameters for each of the four required test cases. The objective is to compute a list of specific quantities. For each set of inputs, the formulas yield a unique, well-defined numerical result. The problem is well-posed.\n-   **Objective**: The problem is stated in precise, quantitative terms. All variables are defined, and all required computations are specified by explicit mathematical formulas. The problem is free of subjective language or opinion.\n-   **No Other Flaws**: The problem is formalizable, complete, and internally consistent. The provided hardware parameters ($B$, $P_{\\mathrm{peak}}$) are realistic for modern GPUs. The required computations are non-trivial and represent a standard performance analysis exercise.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. It is scientifically grounded, well-posed, objective, and complete. A complete, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe solution involves a systematic application of the provided formulas to each of the four test cases. We will structure the calculation by first defining the fundamental constants and then processing each case to compute the required ten output quantities: $I_{\\mathrm{force}}$, $I_{\\mathrm{update}}$, $I_{\\mathrm{fused}}$, $P_{\\mathrm{force}}$, $P_{\\mathrm{update}}$, $P_{\\mathrm{fused}}$, $b_{\\mathrm{force}}$, $b_{\\mathrm{update}}$, $b_{\\mathrm{fused}}$, and $S$.\n\nLet the input parameters for a given case be $(s, B, P_{\\mathrm{peak}}, M_{\\mathrm{force}}, \\dots, c_{\\mathrm{fused}})$. The computational procedure is as follows:\n\n**1. Preliminary Calculations:**\nFirst, we define the constant flop counts for matrix operations:\n-   $F_M = 198$\n-   $F_A = 18$\n\nNext, we calculate the byte sizes of the data structures, which depend on the precision parameter $s$:\n-   $S_U = 18s$\n-   $S_P = 8s$\n\n**2. Analysis of Separate Kernels:**\n\n**Gauge-Force Kernel:**\n-   The total number of floating-point operations is calculated as:\n    $$F_{\\mathrm{force}} = M_{\\mathrm{force}} F_M + A_{\\mathrm{force}} F_A + F_{\\mathrm{proj}}$$\n-   The total memory traffic in bytes, including the inefficiency factor, is:\n    $$M_{\\mathrm{force}}^{\\mathrm{bytes}} = c_{\\mathrm{sep}} \\left(R_{\\mathrm{force},U} S_U + R_{\\mathrm{force},P} S_P + W_{\\mathrm{force}} S_P\\right)$$\n-   The arithmetic intensity is the ratio of flops to bytes:\n    $$I_{\\mathrm{force}} = \\frac{F_{\\mathrm{force}}}{M_{\\mathrm{force}}^{\\mathrm{bytes}}}$$\n-   The attainable performance is determined by the roofline model:\n    $$P_{\\mathrm{force}} = \\min\\left(P_{\\mathrm{peak}}, B \\cdot I_{\\mathrm{force}}\\right)$$\n-   The kernel is memory-bound if the performance is limited by the memory ceiling:\n    $$b_{\\mathrm{force}} = (B \\cdot I_{\\mathrm{force}} < P_{\\mathrm{peak}})$$\n\n**Link-Update Kernel:**\n-   The total number of floating-point operations is:\n    $$F_{\\mathrm{update}} = M_{\\mathrm{update}} F_M + F_{\\mathrm{exp}}$$\n-   The total memory traffic in bytes is:\n    $$M_{\\mathrm{update}}^{\\mathrm{bytes}} = c_{\\mathrm{sep}} \\left(R_{\\mathrm{update},U} S_U + R_{\\mathrm{update},P} S_P + W_{\\mathrm{update},U} S_U\\right)$$\n-   The arithmetic intensity is:\n    $$I_{\\mathrm{update}} = \\frac{F_{\\mathrm{update}}}{M_{\\mathrm{update}}^{\\mathrm{bytes}}}$$\n-   The attainable performance is:\n    $$P_{\\mathrm{update}} = \\min\\left(P_{\\mathrm{peak}}, B \\cdot I_{\\mathrm{update}}\\right)$$\n-   The memory-bound status is:\n    $$b_{\\mathrm{update}} = (B \\cdot I_{\\mathrm{update}} < P_{\\mathrm{peak}})$$\n\n**3. Analysis of the Fused Kernel:**\n\n-   The total flop count is the sum of the individual kernel flops:\n    $$F_{\\mathrm{fused}} = F_{\\mathrm{force}} + F_{\\mathrm{update}}$$\n-   The memory traffic is reduced by eliminating the intermediate write and read of the momentum $P$ and the redundant read of the link $U$ that is being updated. The formula provided is:\n    $$M_{\\mathrm{fused}}^{\\mathrm{bytes}} = c_{\\mathrm{fused}} \\left(R_{\\mathrm{force},U} S_U + R_{\\mathrm{force},P} S_P + W_{\\mathrm{update},U} S_U\\right)$$\n-   The arithmetic intensity for the fused kernel is:\n    $$I_{\\mathrm{fused}} = \\frac{F_{\\mathrm{fused}}}{M_{\\mathrm{fused}}^{\\mathrm{bytes}}}$$\n-   The attainable performance is:\n    $$P_{\\mathrm{fused}} = \\min\\left(P_{\\mathrm{peak}}, B \\cdot I_{\\mathrm{fused}}\\right)$$\n-   The memory-bound status is:\n    $$b_{\\mathrm{fused}} = (B \\cdot I_{\\mathrm{fused}} < P_{\\mathrm{peak}})$$\n\n**4. Calculation of Speedup:**\n\nThe performance improvement from fusion is quantified by the speedup factor $S$, which is the ratio of the total time for separate kernels to the time for the fused kernel.\n\n-   The time for the separate-kernel execution is the sum of the individual kernel times:\n    $$T_{\\mathrm{sep}} = \\frac{F_{\\mathrm{force}}}{P_{\\mathrm{force}}} + \\frac{F_{\\mathrm{update}}}{P_{\\mathrm{update}}}$$\n-   The time for the fused-kernel execution is:\n    $$T_{\\mathrm{fused}} = \\frac{F_{\\mathrm{fused}}}{P_{\\mathrm{fused}}}$$\n-   The speedup is their ratio:\n    $$S = \\frac{T_{\\mathrm{sep}}}{T_{\\mathrm{fused}}}$$\nThe units of $F$ (GigaFLOP) and $P_{\\mathrm{att}}$ (GFLOP/s) result in time $T$ in seconds, but the giga-prefix cancels in the ratio, simplifying implementation.\n\nThis complete procedure will be implemented in the provided Python environment to process the four given test cases and generate the output in the specified format. The code will follow these steps precisely for each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries outside the Python standard library are permitted.\n\ndef solve():\n    \"\"\"\n    Performs a roofline analysis for a simplified HMC integrator step in lattice gauge theory.\n    Computes arithmetic intensity, attainable performance, and kernel fusion speedup\n    for several parameter sets.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each tuple contains:\n    # (s, B, P_peak, M_force, A_force, F_proj, M_update, F_exp, R_force_U,\n    # R_force_P, W_force, R_update_U, R_update_P, W_update_U, c_sep, c_fused)\n    test_cases = [\n        # Case 1: single precision, typical high-bandwidth GPU\n        (4, 900, 19000, 8, 6, 120, 1, 300, 12, 1, 1, 1, 1, 1, 1.0, 1.0),\n        # Case 2: double precision on the same GPU class\n        (8, 900, 9700, 8, 6, 200, 1, 600, 12, 1, 1, 1, 1, 1, 1.0, 1.0),\n        # Case 3: bandwidth-limited environment, single precision\n        (4, 300, 19000, 8, 6, 120, 1, 300, 12, 1, 1, 1, 1, 1, 1.0, 1.0),\n        # Case 4: imperfect coalescing, single precision\n        (4, 900, 19000, 8, 6, 120, 1, 300, 12, 1, 1, 1, 1, 1, 1.3, 1.05),\n    ]\n\n    # Fundamental constants for FLOP counts\n    F_M = 198  # FLOPs for 3x3 complex matrix-matrix product\n    F_A = 18   # FLOPs for 3x3 complex matrix addition\n\n    all_results = []\n    \n    for case in test_cases:\n        (s, B, P_peak, M_force, A_force, F_proj, M_update, F_exp,\n         R_force_U, R_force_P, W_force, R_update_U, R_update_P, W_update_U,\n         c_sep, c_fused) = case\n\n        # --- 1. Preliminary Calculations ---\n        S_U = 18 * s  # Size of SU(3) matrix in bytes\n        S_P = 8 * s   # Size of su(3) matrix in bytes\n\n        # --- 2. Separate Gauge-Force Kernel Analysis ---\n        F_force = M_force * F_M + A_force * F_A + F_proj\n        M_force_bytes = c_sep * (R_force_U * S_U + R_force_P * S_P + W_force * S_P)\n        I_force = F_force / M_force_bytes\n        P_force_mem_ceil = B * I_force\n        P_force = min(P_peak, P_force_mem_ceil)\n        b_force = P_force_mem_ceil  P_peak\n\n        # --- 3. Separate Link-Update Kernel Analysis ---\n        F_update = M_update * F_M + F_exp\n        M_update_bytes = c_sep * (R_update_U * S_U + R_update_P * S_P + W_update_U * S_U)\n        I_update = F_update / M_update_bytes\n        P_update_mem_ceil = B * I_update\n        P_update = min(P_peak, P_update_mem_ceil)\n        b_update = P_update_mem_ceil  P_peak\n\n        # --- 4. Fused Kernel Analysis ---\n        F_fused = F_force + F_update\n        M_fused_bytes = c_fused * (R_force_U * S_U + R_force_P * S_P + W_update_U * S_U)\n        I_fused = F_fused / M_fused_bytes\n        P_fused_mem_ceil = B * I_fused\n        P_fused = min(P_peak, P_fused_mem_ceil)\n        b_fused = P_fused_mem_ceil  P_peak\n\n        # --- 5. Speedup Calculation ---\n        # Time T = F / P_att. Units are consistent, so giga- prefix cancels.\n        T_sep = (F_force / P_force) + (F_update / P_update)\n        T_fused = F_fused / P_fused\n        S = T_sep / T_fused if T_fused > 0 else float('inf')\n\n        # Store results for this case\n        case_results = [\n            I_force, I_update, I_fused,\n            P_force, P_update, P_fused,\n            b_force, b_update, b_fused,\n            S\n        ]\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # Produces a string like [[r1,r2,...],[s1,s2,...]] with no spaces.\n    inner_strings = []\n    for r_list in all_results:\n        # Convert each item in the list to its string representation\n        r_str_list = [str(item) for item in r_list]\n        # Join with commas and wrap in brackets\n        inner_strings.append(f\"[{','.join(r_str_list)}]\")\n    \n    # Join all case strings with commas and wrap in brackets\n    final_output_string = f\"[{','.join(inner_strings)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "3560466"}]}