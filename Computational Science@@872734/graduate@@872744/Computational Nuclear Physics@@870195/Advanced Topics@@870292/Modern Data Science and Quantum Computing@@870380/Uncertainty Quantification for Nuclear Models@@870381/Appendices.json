{"hands_on_practices": [{"introduction": "A crucial task in nuclear theory is to understand how uncertainties in the parameters of a model, such as those in an energy density functional, propagate to its predictions. Polynomial Chaos Expansion (PCE) provides a powerful and efficient framework for this by creating a polynomial surrogate of the full computational model. This exercise provides foundational practice in implementing a non-intrusive PCE, where you will compute the expansion coefficients by projecting the model output onto an orthonormal basis of polynomials, a cornerstone technique in forward uncertainty propagation [@problem_id:3610367].", "problem": "Construct a complete, runnable program that builds a Polynomial Chaos Expansion (PCE) for a nuclear-model observable as a function of uncertain parameters and computes the expansion coefficients via projection with respect to the prior measure. The context is uncertainty quantification for computational nuclear physics. Begin from foundational definitions: a PCE represents a square-integrable random observable as an expansion in an orthonormal polynomial basis with respect to its prior probability measure, and the coefficients are obtained by projection using the inner product induced by that measure. Assume independent priors for uncertain parameters, each supported on a closed interval, and use the orthonormal polynomial family associated with the uniform probability measure for each parameter. In multiple dimensions, use the tensor-product basis and truncate by total degree. Consistently, treat parameters as random variables on their support intervals and construct a multi-dimensional product measure. Implement coefficient computation by numerically approximating the required projections using tensor-product Gaussian Quadrature (GQ) on the canonical interval. The observable definitions below are physically plausible reductions of the semi-empirical mass formula and are expressed for fixed nuclei. All outputs that represent the observable or its PCE coefficients must be expressed in megalelectronvolt (MeV).\n\nDefine the observable for a fixed nucleus by the following deterministic mapping from the uncertain parameters to the observable:\n- Case definitions use the standard semi-empirical mass model components: a volume energy coefficient, a surface energy coefficient, and a Coulomb energy coefficient, denoted by $a_v$, $a_s$, and $a_c$ respectively. Let $A$ be the mass number and $Z$ be the proton number. Consider the simplified binding energy observable $B$ in megalelectronvolt (MeV) given by $B(a_v,a_s,a_c;A,Z) = a_v A - a_s A^{2/3} - a_c Z^2 A^{-1/3}$, with reduced forms when fewer coefficients are modeled as uncertain.\n\nModeling and basis assumptions:\n- Treat each uncertain parameter as independently uniformly distributed on its specified interval, and use the orthonormal polynomial basis associated with this uniform probability measure for each parameter. Use the total-degree truncation of the multi-dimensional tensor-product basis, i.e., keep all multi-indices whose component-wise degree sum is less than or equal to the specified maximum degree.\n- Implement numerical projection integrals by tensor-product Gaussian Quadrature on the canonical interval, and apply the appropriate linear mapping from parameter supports to the canonical interval. Choose a sufficient number of quadrature nodes per dimension to integrate products of the observable and basis functions up to the required polynomial degree without aliasing. The implementation must ensure correct normalization consistent with the orthonormal basis and the prior probability measure.\n\nTest suite specification and required outputs:\nFor each test case, compute and output the full list of PCE coefficients ordered by increasing total degree and, within each total degree, by lexicographic order of the multi-index (first component varying slowest). The coefficients must be reported in MeV as floating-point numbers.\n\n- Test Case $1$: One-dimensional, fixed nucleus with mass number $A=56$ and no Coulomb term. Observable $B(a_v;A)=a_v A$. Prior for $a_v$: uniform on $[14.5,16.5]$. Maximum total degree $p=3$.\n\n- Test Case $2$: Two-dimensional, fixed nucleus with mass number $A=100$ and no Coulomb term. Observable $B(a_v,a_s;A) = a_v A - a_s A^{2/3}$. Priors: $a_v$ uniform on $[14.0,16.0]$, $a_s$ uniform on $[16.0,18.0]$. Maximum total degree $p=2$.\n\n- Test Case $3$: Three-dimensional, fixed nucleus with mass number $A=208$ and proton number $Z=82$. Observable $B(a_v,a_s,a_c;A,Z) = a_v A - a_s A^{2/3} - a_c Z^2 A^{-1/3}$. Priors: $a_v$ uniform on $[14.0,16.0]$, $a_s$ uniform on $[16.0,18.0]$, $a_c$ uniform on $[0.6,0.8]$. Maximum total degree $p=2$.\n\n- Test Case $4$: Boundary case to test constant-term projection in three dimensions. Same observable and priors as Test Case $3$, with maximum total degree $p=0$.\n\nOutput format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test caseâ€™s result is itself a comma-separated list enclosed in square brackets. For example, if there are $4$ test cases, the final output must look like $[[c_{1,1},c_{1,2},\\dots],[c_{2,1},\\dots],[c_{3,1},\\dots],[c_{4,1},\\dots]]$ without any spaces. Each $c_{i,j}$ is a floating-point number in MeV.\n\nAngle units are not applicable. All numeric outputs are unit-bearing in megalelectronvolt (MeV).", "solution": "The task is to construct a Polynomial Chaos Expansion (PCE) for a simplified nuclear binding energy observable and compute the expansion coefficients via projection. The problem is well-defined, scientifically grounded in uncertainty quantification and nuclear physics, and provides a complete specification for its solution. We proceed by first establishing the mathematical framework and then detailing the numerical implementation.\n\n### 1. Mathematical Formulation\n\nA square-integrable random observable $M(\\mathbf{x})$, which is a function of a $d$-dimensional vector of independent random parameters $\\mathbf{x} = (x_1, \\dots, x_d)$, can be represented by a Polynomial Chaos Expansion. Each parameter $x_i$ is assumed to be uniformly distributed on a known interval $[a_i, b_i]$, i.e., $x_i \\sim U(a_i, b_i)$.\n\nTo work with a standard set of basis polynomials, we transform each physical parameter $x_i$ to a canonical random variable $\\xi_i$ that is uniformly distributed on the interval $[-1, 1]$. The corresponding probability density function is $\\rho(\\xi_i) = 1/2$ for $\\xi_i \\in [-1, 1]$. The affine transformation is:\n$$x_i(\\xi_i) = \\frac{b_i-a_i}{2} \\xi_i + \\frac{a_i+b_i}{2}$$\nThe observable $M(\\mathbf{x})$ can now be expressed as a function of the canonical variables, $\\tilde{M}(\\boldsymbol{\\xi}) = M(\\mathbf{x}(\\boldsymbol{\\xi}))$.\n\nThe orthonormal polynomial basis for a random variable uniformly distributed on $[-1, 1]$ consists of scaled Legendre polynomials. The standard Legendre polynomials $P_k(\\xi)$ are orthogonal with respect to the standard weight function $w(\\xi)=1$ on $[-1, 1]$: $\\int_{-1}^1 P_k(\\xi) P_j(\\xi) d\\xi = \\frac{2}{2k+1}\\delta_{kj}$.\nThe inner product induced by our probability measure $\\rho(\\xi)d\\xi$ is $\\langle f, g \\rangle = \\int_{-1}^1 f(\\xi) g(\\xi) \\frac{1}{2} d\\xi$. The correctly-normalized basis polynomials $\\Psi_k(\\xi)$ must satisfy $\\langle \\Psi_k, \\Psi_j \\rangle = \\delta_{kj}$. Setting $\\Psi_k(\\xi) = c_k P_k(\\xi)$, we find:\n$$\\langle \\Psi_k, \\Psi_j \\rangle = c_k c_j \\int_{-1}^1 P_k(\\xi) P_j(\\xi) \\frac{1}{2} d\\xi = \\frac{c_k c_j}{2} \\frac{2}{2k+1} \\delta_{kj} = \\frac{c_k^2}{2k+1} \\delta_{kj}$$\nFor this to equal $\\delta_{kj}$, we require $c_k = \\sqrt{2k+1}$. Thus, the one-dimensional orthonormal basis functions are:\n$$\\Psi_k(\\xi) = \\sqrt{2k+1} P_k(\\xi)$$\n\nFor the $d$-dimensional case with independent parameters, the basis is formed by the tensor product of the one-dimensional bases. A basis function is indexed by a multi-index $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_d) \\in \\mathbb{N}_0^d$:\n$$\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi}) = \\prod_{i=1}^d \\Psi_{\\alpha_i}(\\xi_i) = \\prod_{i=1}^d \\sqrt{2\\alpha_i+1} P_{\\alpha_i}(\\xi_i)$$\nThe PCE is truncated using a total-degree scheme, retaining all basis functions for which the sum of the degrees of the components does not exceed a maximum total degree $p$: $|\\boldsymbol{\\alpha}| = \\sum_{i=1}^d \\alpha_i \\le p$. The truncated PCE for the observable is:\n$$\\tilde{M}(\\boldsymbol{\\xi}) \\approx \\sum_{|\\boldsymbol{\\alpha}| \\le p} c_{\\boldsymbol{\\alpha}} \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})$$\n\nThe coefficients $c_{\\boldsymbol{\\alpha}}$ are found by projecting the observable onto each basis function using the inner product induced by the joint probability measure $\\rho(\\boldsymbol{\\xi})d\\boldsymbol{\\xi} = (1/2)^d d\\boldsymbol{\\xi}$ on the hypercube $[-1, 1]^d$:\n$$c_{\\boldsymbol{\\alpha}} = \\langle \\tilde{M}, \\Psi_{\\boldsymbol{\\alpha}} \\rangle = \\int_{[-1, 1]^d} \\tilde{M}(\\boldsymbol{\\xi}) \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi}) \\rho(\\boldsymbol{\\xi}) d\\boldsymbol{\\xi} = \\left(\\frac{1}{2}\\right)^d \\int_{[-1, 1]^d} \\tilde{M}(\\boldsymbol{\\xi}) \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi}) d\\boldsymbol{\\xi}$$\n\n### 2. Numerical Implementation\n\nThe multi-dimensional integral for $c_{\\boldsymbol{\\alpha}}$ is computed numerically using a tensor-product Gaussian Quadrature (GQ) rule. Specifically, we use Gauss-Legendre quadrature. A one-dimensional $N_q$-point Gauss-Legendre rule approximates an integral as $\\int_{-1}^1 f(\\xi) d\\xi \\approx \\sum_{j=1}^{N_q} f(\\xi_j) w_j$, where $\\xi_j$ are the quadrature nodes and $w_j$ are the corresponding weights. The tensor-product rule for $d$ dimensions is:\n$$\\int_{[-1, 1]^d} g(\\boldsymbol{\\xi}) d\\boldsymbol{\\xi} \\approx \\sum_{j_1=1}^{N_q} \\dots \\sum_{j_d=1}^{N_q} g(\\xi_{j_1}, \\dots, \\xi_{j_d}) \\prod_{k=1}^d w_{j_k}$$\nApplying this to the coefficient integral, we get:\n$$c_{\\boldsymbol{\\alpha}} \\approx \\left(\\frac{1}{2}\\right)^d \\sum_{j_1, \\dots, j_d} \\tilde{M}(\\xi_{j_1}, \\dots, \\xi_{j_d}) \\Psi_{\\boldsymbol{\\alpha}}(\\xi_{j_1}, \\dots, \\xi_{j_d}) \\prod_{k=1}^d w_{j_k}$$\n\nTo ensure accuracy, the number of quadrature points $N_q$ must be chosen large enough to exactly integrate the polynomial integrand $\\tilde{M}(\\boldsymbol{\\xi}) \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})$. The observable $B$ is a linear function of its parameters $(a_v, a_s, a_c)$. Since the mapping from $\\xi_i$ to $x_i$ is linear, $\\tilde{M}(\\boldsymbol{\\xi})$ is a polynomial of total degree $1$ in $\\boldsymbol{\\xi}$. The basis function $\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})$ is a polynomial of total degree $|\\boldsymbol{\\alpha}|$. The highest degree of the integrand in any single dimension $i$ is $1 + \\alpha_i$. Since the maximum value of any $\\alpha_i$ for a total degree $p$ expansion is $p$, the maximum polynomial degree to be integrated in any one dimension is $1+p$. An $N_q$-point Gauss-Legendre rule can integrate polynomials of degree up to $2N_q - 1$ exactly. Therefore, we require $2N_q - 1 \\ge p + 1$, which simplifies to $N_q \\ge (p+2)/2$. For robustness, we select $N_q = p+1$, which satisfies this condition for all non-negative $p$.\n\nThe overall algorithm is as follows:\n1.  For each test case (dimension $d$, max degree $p$, observable $M$, and priors):\n2.  Generate the set of multi-indices $\\boldsymbol{\\alpha}$ such that $|\\boldsymbol{\\alpha}| \\le p$. Sort these indices first by total degree $|\\boldsymbol{\\alpha}|$, then lexicographically.\n3.  Set the number of quadrature points per dimension, $N_q = p + 1$.\n4.  Obtain the $N_q$ one-dimensional Gauss-Legendre nodes and weights.\n5.  Construct the $d$-dimensional grid of quadrature nodes $\\boldsymbol{\\xi}^{(j)}$ and the corresponding product weights $W^{(j)}$.\n6.  For each node $\\boldsymbol{\\xi}^{(j)}$ on the grid, transform it to the physical parameter space $\\mathbf{x}^{(j)}$.\n7.  Evaluate the model observable $M(\\mathbf{x}^{(j)})$ at each physical grid point to obtain a grid of model values.\n8.  For each multi-index $\\boldsymbol{\\alpha}$ in the sorted list:\n    a. Evaluate the basis function $\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi}^{(j)})$ at each canonical grid node.\n    b. Compute the integrand by taking the element-wise product of the model values grid and the basis function values grid.\n    c. Approximate the integral by taking the dot product of the flattened integrand grid with the flattened grid of product weights.\n    d. Calculate the coefficient $c_{\\boldsymbol{\\alpha}}$ by multiplying the integral result by $(0.5)^d$.\n9.  Collect the computed coefficients for each test case and format the output as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import legendre, roots_legendre\nimport itertools\n\ndef solve():\n    \"\"\"\n    Computes Polynomial Chaos Expansion coefficients for a simplified nuclear binding energy model.\n    \"\"\"\n\n    def generate_multi_indices(dim, max_degree):\n        \"\"\"\n        Generates multi-indices for a given dimension and maximum total degree.\n        The indices are sorted by total degree, then lexicographically.\n        \"\"\"\n        if dim == 0:\n            if max_degree >= 0:\n                return [()]\n            else:\n                return []\n        \n        if dim == 1:\n            return [(i,) for i in range(max_degree + 1)]\n\n        indices = []\n        for i in range(max_degree + 1):\n            sub_indices = generate_multi_indices(dim - 1, max_degree - i)\n            for sub_index in sub_indices:\n                indices.append((i,) + sub_index)\n        \n        # Sort by total degree, then lexicographically\n        indices.sort(key=lambda idx: (sum(idx), idx))\n        return indices\n\n    def compute_pce_coefficients(model_func, priors, dim, max_degree):\n        \"\"\"\n        Computes the PCE coefficients for a given model, priors, dimension, and max degree.\n        \"\"\"\n        # Step 1: Generate multi-indices\n        multi_indices = generate_multi_indices(dim, max_degree)\n\n        # Step 2: Determine quadrature rule\n        num_quad_points = max_degree + 1\n        nodes_1d, weights_1d = roots_legendre(num_quad_points)\n\n        # Step 3: Construct multi-dimensional quadrature grid\n        # 'ij' indexing creates grids that can be stacked correctly for our purpose.\n        xi_grids = np.meshgrid(*([nodes_1d] * dim), indexing='ij')\n        w_grids = np.meshgrid(*([weights_1d] * dim), indexing='ij')\n\n        # Create a single grid of product weights\n        product_weights_grid = np.prod(np.stack(w_grids, axis=-1), axis=-1)\n\n        # Step 4: Transform canonical nodes to physical parameter space\n        centers = np.array([(p[0] + p[1]) / 2.0 for p in priors])\n        half_widths = np.array([(p[1] - p[0]) / 2.0 for p in priors])\n        \n        physical_params_grid = []\n        for i in range(dim):\n             physical_params_grid.append(centers[i] + half_widths[i] * xi_grids[i])\n\n        # Step 5: Evaluate the model on the grid\n        model_values_grid = model_func(*physical_params_grid)\n\n        # Step 6: Compute coefficients\n        coefficients = []\n        for alpha in multi_indices:\n            # Evaluate the multi-dimensional basis polynomial on the grid\n            basis_values_grid = np.ones_like(model_values_grid)\n            for i in range(dim):\n                degree = alpha[i]\n                # Orthonormal Legendre polynomials: sqrt(2k+1) * P_k(x)\n                poly = legendre(degree)\n                norm_factor = np.sqrt(2 * degree + 1)\n                basis_values_grid *= (norm_factor * poly(xi_grids[i]))\n            \n            # Form the integrand\n            integrand_grid = model_values_grid * basis_values_grid\n            \n            # Approximate the integral using the quadrature rule\n            integral_val = np.sum(integrand_grid * product_weights_grid)\n            \n            # Final coefficient calculation\n            # c_alpha = (1/2)^d * integral\n            c_alpha = (0.5**dim) * integral_val\n            coefficients.append(c_alpha)\n        \n        return coefficients\n\n    # Define test cases\n    # (observable_lambda, priors_list, dimension, max_pce_degree)\n    test_cases = [\n        (lambda a_v: 56.0 * a_v, [(14.5, 16.5)], 1, 3),\n        (lambda a_v, a_s: 100.0 * a_v - (100.0**(2.0/3.0)) * a_s, \n         [(14.0, 16.0), (16.0, 18.0)], 2, 2),\n        (lambda a_v, a_s, a_c: 208.0 * a_v - (208.0**(2.0/3.0)) * a_s - (82.0**2 * 208.0**(-1.0/3.0)) * a_c,\n         [(14.0, 16.0), (16.0, 18.0), (0.6, 0.8)], 3, 2),\n        (lambda a_v, a_s, a_c: 208.0 * a_v - (208.0**(2.0/3.0)) * a_s - (82.0**2 * 208.0**(-1.0/3.0)) * a_c,\n         [(14.0, 16.0), (16.0, 18.0), (0.6, 0.8)], 3, 0),\n    ]\n\n    all_results = []\n    for model, priors, dim, p_max in test_cases:\n        coeffs = compute_pce_coefficients(model, priors, dim, p_max)\n        all_results.append(coeffs)\n    \n    # Format the final output string\n    result_str = \",\".join([f\"[{','.join([f'{c:.15g}' for c in case_coeffs])}]\" for case_coeffs in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3610367"}, {"introduction": "Calibrating nuclear models to experimental data is a delicate inverse problem that requires careful validation to avoid overfitting. Standard cross-validation techniques can be misleading if the dataset contains systematic, unmodeled errors or correlations, such as those that might affect an entire isotopic chain. This practice explores how to design a more robust validation scheme that accounts for such data structures, a critical skill for developing predictive models. By implementing and comparing a naive leave-one-nucleus-out scheme with a physics-informed leave-one-isotopic-chain-out scheme, you will gain practical insight into identifying and mitigating overfitting in the presence of structured model discrepancies [@problem_id:3610428].", "problem": "You are tasked with writing a complete and runnable program that compares two cross-validation schemes for calibrating a linear energy density functional (EDF)-like surrogate model used to predict nuclear binding energy residuals. Your program must estimate predictive risk and detect potential overfitting differences between leave-one-nucleus-out and leave-one-isotopic-chain-out schemes.\n\nThe model and data generation are as follows.\n\n1. Fundamental base and modeling assumptions:\n   - Assume that the observable is a binding-energy residual per nucleus, denoted by $y$ and measured in megaelectronvolts (MeV). Assume a linear surrogate model with a polynomial feature map up to second order in proton number $Z$ and neutron number $N$. For a given nucleus $(Z, N)$, define the feature vector\n     $$\\phi(Z,N) = \\begin{bmatrix} 1 & Z & N & Z^2 & N^2 & ZN \\end{bmatrix}^{\\top}.$$\n   - Assume additive independent Gaussian noise with zero mean and standard deviation $\\sigma$ measured in MeV, that is, $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$.\n   - The data are generated from a fixed but unknown linear parameter vector $\\beta^{\\star} \\in \\mathbb{R}^6$, plus an unmodeled isotopic-chain offset $g(Z)$ that depends only on $Z$, and an independent noise $\\epsilon$. That is,\n     $$ y = \\phi(Z,N)^{\\top} \\beta^{\\star} + g(Z) + \\epsilon.$$\n   - The calibration uses ridge-regularized least squares with a Gaussian prior interpretation on the coefficients, where the intercept (the first component corresponding to the constant term) is not penalized. For a given regularization strength $\\lambda \\ge 0$, the estimator $\\hat{\\beta}_{\\lambda}$ minimizes\n     $$ \\sum_{i=1}^{n} \\left(y_i - \\phi(Z_i,N_i)^{\\top}\\beta\\right)^2 + \\lambda \\sum_{j=2}^{6} \\beta_j^2.$$\n\n2. Cross-validation schemes and predictive risk:\n   - Leave-one-nucleus-out cross-validation (LONO-CV): For each data point $i$, fit $\\hat{\\beta}_{\\lambda}^{(-i)}$ on the $n-1$ other data points and compute the squared prediction error on the held-out point $i$. The LONO-CV predictive risk for $\\lambda$ is the mean of these squared prediction errors over all $n$ hold-outs.\n   - Leave-one-isotopic-chain-out cross-validation (LOICO-CV): For each distinct isotopic chain identified by proton number $Z$, fit on all nuclei not in that chain, then predict the held-out chain and accumulate the squared prediction errors for all nuclei in that chain. The LOICO-CV predictive risk for $\\lambda$ is the mean of all squared prediction errors across all held-out chains. An isotopic chain is defined here as the set of nuclei with the same proton number $Z$ and varying neutron number $N$.\n\n3. Regularization grid and selection:\n   - Consider the discrete grid of regularization strengths\n     $$ \\Lambda = \\{\\, 0,\\, 10^{-6},\\, 10^{-3},\\, 10^{-1},\\, 1,\\, 10 \\,\\}. $$\n   - For each cross-validation scheme, select the $\\lambda \\in \\Lambda$ that minimizes the corresponding predictive risk. In case of ties within an absolute tolerance of $10^{-12}$ in the risk value, choose the smallest $\\lambda$ among those tied.\n\n4. Overfitting detection criterion:\n   - Define an overfitting detection indicator as a boolean computed as\n     $$ \\text{overfit\\_detected} = \\left( \\lambda_{\\text{LONO}}^{\\star} = 0 \\right) \\wedge \\left( \\lambda_{\\text{LOICO}}^{\\star} > 0 \\right), $$\n     where $\\lambda_{\\text{LONO}}^{\\star}$ and $\\lambda_{\\text{LOICO}}^{\\star}$ are the optimal choices under LONO-CV and LOICO-CV respectively.\n\n5. Data set, true parameters, and isotopic-chain offsets:\n   - Use the following set of nuclei $(Z,N)$ grouped into isotopic chains by $Z$:\n     - $Z=20$: $N \\in \\{\\, 20,\\, 22,\\, 24,\\, 26 \\,\\}$,\n     - $Z=28$: $N \\in \\{\\, 28,\\, 30,\\, 32,\\, 34 \\,\\}$,\n     - $Z=50$: $N \\in \\{\\, 64,\\, 66,\\, 68,\\, 70 \\,\\}$,\n     - $Z=82$: $N \\in \\{\\, 120,\\, 122,\\, 124,\\, 126 \\,\\}$.\n   - Use the true parameter vector\n     $$ \\beta^{\\star} = \\begin{bmatrix} 5.0 & -0.08 & -0.10 & 5.0\\times 10^{-4} & 3.0\\times 10^{-4} & 2.0\\times 10^{-4} \\end{bmatrix}^{\\top} \\text{ MeV}, $$\n     and the isotopic-chain offsets\n     $$ g(20)=1.2 \\text{ MeV},\\quad g(28)=-1.0 \\text{ MeV},\\quad g(50)=0.5 \\text{ MeV},\\quad g(82)=-2.0 \\text{ MeV}.$$\n\n6. Test suite:\n   - You must evaluate three distinct test cases that only differ in the noise level $\\sigma$ and the random seed used to generate the Gaussian noise. For each test case $t \\in \\{1,2,3\\}$, generate the outputs $y$ by drawing independent $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ with the specified seed for reproducibility. Use:\n     - Case $1$: $\\sigma = 0.20$ MeV, random seed $123$,\n     - Case $2$: $\\sigma = 0.00$ MeV, random seed $456$,\n     - Case $3$: $\\sigma = 2.00$ MeV, random seed $789$.\n\n7. Required computations and outputs per test case:\n   - For each test case, compute:\n     - the optimal LONO-CV predictive risk (mean squared prediction error) in megaelectronvolt squared, denoted $R_{\\text{LONO}}^{\\star}$,\n     - the optimal LOICO-CV predictive risk (mean squared prediction error) in megaelectronvolt squared, denoted $R_{\\text{LOICO}}^{\\star}$,\n     - the corresponding optimal regularization strengths $\\lambda_{\\text{LONO}}^{\\star}$ and $\\lambda_{\\text{LOICO}}^{\\star}$ from the grid $\\Lambda$,\n     - the overfitting detection boolean as defined above.\n   - All risks must be expressed as real numbers in megaelectronvolt squared (MeV$^2$). All regularization strengths must be expressed as real numbers without units. The boolean must be either the literal True or False.\n\n8. Final output format:\n   - Your program should produce a single line of output containing the results of all three test cases as a comma-separated list of lists, with no spaces, enclosed in a single pair of square brackets. For each test case, output the list\n     $$ \\left[ R_{\\text{LONO}}^{\\star},\\ R_{\\text{LOICO}}^{\\star},\\ \\lambda_{\\text{LONO}}^{\\star},\\ \\lambda_{\\text{LOICO}}^{\\star},\\ \\text{overfit\\_detected} \\right]. $$\n   - The final output should therefore look like\n     $$ \\big[ [\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot] \\big], $$\n     with every numeric entry printed as a standard floating-point literal and the boolean printed as either True or False, and with no spaces anywhere in the line.\n\nYour solution must start from the fundamental assumptions above and implement the two cross-validation schemes precisely as defined. No external inputs are allowed; all constants and data are specified here. The program must be deterministic given the random seeds. The entirety of the logic must be embedded in the program, and it must produce exactly one line as specified.", "solution": "The user has provided a computational problem in the domain of nuclear physics, specifically focusing on uncertainty quantification for energy density functional (EDF) surrogate models. The task is to compare two cross-validation (CV) schemes, leave-one-nucleus-out (LONO-CV) and leave-one-isotopic-chain-out (LOICO-CV), for calibrating a linear model and detecting potential overfitting.\n\n### Step 1: Extract Givens\n- **Observable**: Binding-energy residual per nucleus, $y$, in units of megaelectronvolts (MeV).\n- **Surrogate Model**: A linear model with a polynomial feature map.\n- **Feature Vector**: For a nucleus $(Z,N)$, $\\phi(Z,N) = \\begin{bmatrix} 1 & Z & N & Z^2 & N^2 & ZN \\end{bmatrix}^{\\top}$.\n- **Data Generation Model**: $y = \\phi(Z,N)^{\\top} \\beta^{\\star} + g(Z) + \\epsilon$.\n- **Noise Model**: $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$, independent and identically distributed Gaussian noise.\n- **True Parameters**: $\\beta^{\\star} = \\begin{bmatrix} 5.0 & -0.08 & -0.10 & 5.0\\times 10^{-4} & 3.0\\times 10^{-4} & 2.0\\times 10^{-4} \\end{bmatrix}^{\\top}$ MeV.\n- **Isotopic-Chain Offsets**: $g(20)=1.2$ MeV, $g(28)=-1.0$ MeV, $g(50)=0.5$ MeV, $g(82)=-2.0$ MeV.\n- **Calibration Method**: Ridge-regularized least squares minimizing $\\sum_{i=1}^{n} (y_i - \\phi(Z_i,N_i)^{\\top}\\beta)^2 + \\lambda \\sum_{j=2}^{6} \\beta_j^2$. The intercept term $\\beta_1$ is not penalized.\n- **Cross-Validation Schemes**:\n    - **LONO-CV**: Leave-one-nucleus-out cross-validation. Risk is the mean of squared prediction errors over all $n$ hold-outs.\n    - **LOICO-CV**: Leave-one-isotopic-chain-out cross-validation. An isotopic chain is the set of nuclei with the same $Z$. Risk is the mean of squared prediction errors across all held-out chains.\n- **Regularization Grid**: $\\Lambda = \\{\\, 0,\\, 10^{-6},\\, 10^{-3},\\, 10^{-1},\\, 1,\\, 10 \\,\\}$.\n- **Optimal $\\lambda$ Selection**: Choose $\\lambda \\in \\Lambda$ that minimizes the CV risk. In case of a tie (risk values within an absolute tolerance of $10^{-12}$), choose the smallest $\\lambda$.\n- **Overfitting Indicator**: $\\text{overfit\\_detected} = (\\lambda_{\\text{LONO}}^{\\star} = 0) \\wedge (\\lambda_{\\text{LOICO}}^{\\star} > 0)$.\n- **Dataset**:\n    - $Z=20$: $N \\in \\{20, 22, 24, 26\\}$\n    - $Z=28$: $N \\in \\{28, 30, 32, 34\\}$\n    - $Z=50$: $N \\in \\{64, 66, 68, 70\\}$\n    - $Z=82$: $N \\in \\{120, 122, 124, 126\\}$\n- **Test Cases**:\n    - Case 1: $\\sigma = 0.20$ MeV, random seed $123$.\n    - Case 2: $\\sigma = 0.00$ MeV, random seed $456$.\n    - Case 3: $\\sigma = 2.00$ MeV, random seed $789$.\n- **Required Output**: For each case, a list $[R_{\\text{LONO}}^{\\star}, R_{\\text{LOICO}}^{\\star}, \\lambda_{\\text{LONO}}^{\\star}, \\lambda_{\\text{LOICO}}^{\\star}, \\text{overfit\\_detected}]$, where $R$ are risks in MeV$^2$.\n- **Final Format**: A single line `[[...],[...],[...]]` with no spaces.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined computational task in statistical modeling and machine learning, set within a nuclear physics context.\n- **Scientifically Grounded**: The problem uses concepts like energy density functionals, surrogate modeling, and cross-validation, which are standard in computational physics. The model is a specified simplification for a computational exercise, not a claim about fundamental physics. This is scientifically sound as a modeling study.\n- **Well-Posed**: All components are specified: the data-generating process, the model to be fitted, the optimization objective (ridge regression with an unpenalized intercept), the cross-validation procedures, the hyperparameter grid, and the evaluation metrics. The use of random seeds ensures reproducibility. The problem admits a unique and deterministically computable solution.\n- **Objective**: The problem is stated with precise mathematical and algorithmic definitions, free of ambiguity or subjective language.\n- **No Flaws**: The problem does not violate any of the invalidity criteria. It is self-contained, consistent, numerically feasible, and clearly structured. All parameters, constants, and data are provided.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Principle-Based Solution Design\n\nThe core of the problem is to implement and compare two cross-validation strategies for a ridge-regularized linear model. The key distinction lies in the unmodeled systematic error term, $g(Z)$, which introduces correlations among data points within the same isotopic chain (same $Z$). The two CV schemes will handle this correlation differently.\n\n**1. Data Generation and Model Setup**\nFirst, we construct the dataset of $n=16$ nuclei $(Z,N)$ and the corresponding design matrix $\\mathbf{X}$, where each row is the feature vector $\\phi(Z,N)^{\\top}$. The response vector $\\mathbf{y}$ is generated for each test case according to the model $y_i = \\phi(Z_i, N_i)^{\\top} \\beta^{\\star} + g(Z_i) + \\epsilon_i$, using the specified true parameters $\\beta^{\\star}$, isotopic offsets $g(Z)$, noise level $\\sigma$, and random seed.\n\n**2. Ridge Regression Solver**\nThe calibration requires solving a ridge regression problem where the intercept, $\\beta_1$, is not regularized. The objective function to minimize with respect to $\\beta \\in \\mathbb{R}^6$ is:\n$$ L(\\beta) = \\|\\mathbf{y} - \\mathbf{X}\\beta\\|_2^2 + \\lambda \\|\\beta_{2:6}\\|_2^2 = (\\mathbf{y} - \\mathbf{X}\\beta)^{\\top}(\\mathbf{y} - \\mathbf{X}\\beta) + \\beta^{\\top}\\mathbf{P}\\beta $$\nwhere $\\mathbf{P}$ is a diagonal penalty matrix $\\mathbf{P} = \\lambda \\cdot \\text{diag}(0, 1, 1, 1, 1, 1)$.\nTaking the gradient with respect to $\\beta$ and setting it to zero gives the normal equations:\n$$ (\\mathbf{X}^{\\top}\\mathbf{X} + \\mathbf{P})\\beta = \\mathbf{X}^{\\top}\\mathbf{y} $$\nThis linear system is solved for $\\hat{\\beta}_{\\lambda}$ for each training fold and regularization strength $\\lambda$. We use a robust linear algebra solver for this purpose.\n\n**3. Cross-Validation Schemes**\nThe goal is to estimate the model's predictive risk for different values of $\\lambda$ from the grid $\\Lambda$.\n\n- **Leave-One-Nucleus-Out (LONO-CV)**: This is the standard leave-one-out cross-validation. For each of the $n$ data points, we train the model on the other $n-1$ points and calculate the squared prediction error on the single held-out point. The LONO-CV risk for a given $\\lambda$ is the average of these $n$ squared errors.\n$$ R_{\\text{LONO}}(\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\phi(Z_i, N_i)^{\\top}\\hat{\\beta}_{\\lambda}^{(-i)})^2 $$\nwhere $\\hat{\\beta}_{\\lambda}^{(-i)}$ is the parameter vector estimated without data point $i$. Because the training set for each fold almost certainly contains other nuclei from the same isotopic chain as the held-out point, this scheme allows the model to \"learn\" the systematic offset $g(Z)$ for that chain. This can lead to an overly optimistic estimate of the true generalization error, potentially favoring an unregularized, overfitted model ($\\lambda=0$).\n\n- **Leave-One-Isotopic-Chain-Out (LOICO-CV)**: This scheme is designed to account for the data correlation structure. Instead of holding out one nucleus, we hold out all nuclei belonging to the same isotopic chain (i.e., all nuclei with the same $Z$). The model is trained on the remaining chains and tested on the entire held-out chain. The process is repeated for each isotopic chain. The LOICO-CV risk for a given $\\lambda$ is the average of the squared errors over all $n$ nuclei.\n$$ R_{\\text{LOICO}}(\\lambda) = \\frac{1}{n} \\sum_{Z_k \\in \\text{Chains}} \\sum_{i \\in \\text{Chain } Z_k} (y_i - \\phi(Z_i, N_i)^{\\top}\\hat{\\beta}_{\\lambda}^{(-Z_k)})^2 $$\nwhere $\\hat{\\beta}_{\\lambda}^{(-Z_k)}$ is estimated without any data from the chain with proton number $Z_k$. Since the model is tested on a chain for which the systematic offset $g(Z_k)$ is completely unseen during training, this provides a more realistic estimate of the model's ability to extrapolate to new physical regions (i.e., new proton numbers). This will likely reveal the model's misspecification and favor a non-zero $\\lambda$ to prevent overfitting to the specific offsets present in the training data.\n\n**4. Optimal Parameter Selection and Overfitting Detection**\nFor each CV scheme, we compute the risk for every $\\lambda \\in \\Lambda$. We then identify the minimum risk and select the corresponding $\\lambda$ as the optimal one ($\\lambda^{\\star}$). The problem specifies a tie-breaking rule: if multiple $\\lambda$ values yield a risk within $10^{-12}$ of the minimum, the smallest $\\lambda$ among them is chosen. The risk associated with this chosen $\\lambda^{\\star}$ is the optimal risk $R^{\\star}$.\n\nFinally, the overfitting indicator is computed as the logical AND of two conditions: the optimal $\\lambda$ from LONO-CV is zero, and the optimal $\\lambda$ from LOICO-CV is greater than zero. This condition formally captures the scenario where the naive CV scheme (LONO) suggests no regularization is needed, while the more robust scheme (LOICO) correctly identifies the need for regularization to improve generalization, a classic sign of overfitting due to unmodeled systematic errors.\n\nThe entire procedure is repeated for the three test cases, varying the noise level $\\sigma$ and the random seed.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that runs the three test cases and prints the final result.\n    \"\"\"\n\n    def phi(Z, N):\n        \"\"\"Constructs the feature vector for a given nucleus (Z, N).\"\"\"\n        return np.array([1, Z, N, Z**2, N**2, Z*N], dtype=float)\n\n    def solve_ridge(X_train, y_train, lambda_reg):\n        \"\"\"\n        Solves ridge regression with a non-penalized intercept.\n        The objective is: ||y - X*beta||^2 + lambda * ||beta[1:]||^2\n        \"\"\"\n        num_features = X_train.shape[1]\n        # Create the penalty matrix P = lambda * diag(0, 1, 1, ...)\n        P = np.diag([0.0] + [lambda_reg] * (num_features - 1))\n        \n        # Solve the normal equations: (X.T*X + P)*beta = X.T*y\n        A = X_train.T @ X_train + P\n        b = X_train.T @ y_train\n        \n        try:\n            beta_hat = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse if solve fails (for singular matrices. e.g. when lambda=0 and X is not full rank)\n            # This is generally not expected here but is good practice.\n            A_inv = np.linalg.pinv(A)\n            beta_hat = A_inv @ b\n            \n        return beta_hat\n\n    def select_optimal_lambda(lambdas, risks):\n        \"\"\"\n        Selects the optimal lambda according to the problem's criteria.\n        Chooses the smallest lambda in case of a tie in risk values.\n        \"\"\"\n        tolerance = 1e-12\n        min_risk_val = np.min(risks)\n        \n        # Find all lambdas whose risk is within tolerance of the minimum\n        candidate_lambdas = []\n        for lam, risk in zip(lambdas, risks):\n            if abs(risk - min_risk_val) = tolerance:\n                candidate_lambdas.append(lam)\n        \n        # From the candidates, choose the smallest lambda\n        optimal_lambda = min(candidate_lambdas)\n        \n        # Find the index of this optimal lambda to get its corresponding risk\n        optimal_lambda_index = lambdas.index(optimal_lambda)\n        optimal_risk = risks[optimal_lambda_index]\n        \n        return optimal_risk, optimal_lambda\n\n    def run_case(sigma, seed):\n        \"\"\"\n        Executes the full analysis for a single test case (sigma, seed).\n        \"\"\"\n        # 1. Define constants, data structures, and nuclei\n        nuclei_chains = {\n            20: [20, 22, 24, 26],\n            28: [28, 30, 32, 34],\n            50: [64, 66, 68, 70],\n            82: [120, 122, 124, 126]\n        }\n        nuclei = []\n        for z, ns in nuclei_chains.items():\n            for n in ns:\n                nuclei.append((z, n))\n        \n        num_nuclei = len(nuclei)\n\n        beta_star = np.array([5.0, -0.08, -0.10, 5.0e-4, 3.0e-4, 2.0e-4])\n        g_offsets = {20: 1.2, 28: -1.0, 50: 0.5, 82: -2.0}\n        lambdas = [0.0, 1e-6, 1e-3, 1e-1, 1.0, 10.0]\n\n        # 2. Generate data based on the model\n        rng = np.random.default_rng(seed)\n        X = np.array([phi(z, n) for z, n in nuclei])\n        y = np.zeros(num_nuclei)\n        for i, (z, n) in enumerate(nuclei):\n            true_model_val = X[i, :] @ beta_star\n            chain_offset = g_offsets[z]\n            noise = rng.normal(0, sigma) if sigma > 0 else 0.0\n            y[i] = true_model_val + chain_offset + noise\n\n        # 3. Leave-One-Nucleus-Out Cross-Validation (LONO-CV)\n        lono_risks = []\n        for lam in lambdas:\n            squared_errors = []\n            for i in range(num_nuclei):\n                X_train = np.delete(X, i, axis=0)\n                y_train = np.delete(y, i)\n                X_test_row = X[i, :]\n                y_test_val = y[i]\n                \n                beta_hat = solve_ridge(X_train, y_train, lam)\n                \n                y_pred = X_test_row @ beta_hat\n                squared_errors.append((y_pred - y_test_val)**2)\n            lono_risks.append(np.mean(squared_errors))\n        \n        R_lono_star, lambda_lono_star = select_optimal_lambda(lambdas, lono_risks)\n\n        # 4. Leave-One-Isotopic-Chain-Out Cross-Validation (LOICO-CV)\n        unique_Zs = sorted(nuclei_chains.keys())\n        loico_risks = []\n        for lam in lambdas:\n            all_fold_errors = []\n            for z_out in unique_Zs:\n                test_indices = [i for i, (z, n) in enumerate(nuclei) if z == z_out]\n                train_indices = [i for i, (z, n) in enumerate(nuclei) if z != z_out]\n                \n                X_train, y_train = X[train_indices], y[train_indices]\n                X_test, y_test = X[test_indices], y[test_indices]\n                \n                beta_hat = solve_ridge(X_train, y_train, lam)\n                \n                y_pred = X_test @ beta_hat\n                all_fold_errors.extend((y_pred - y_test)**2)\n            loico_risks.append(np.mean(all_fold_errors))\n            \n        R_loico_star, lambda_loico_star = select_optimal_lambda(lambdas, loico_risks)\n        \n        # 5. Overfitting Detection\n        overfit_detected = (lambda_lono_star == 0.0) and (lambda_loico_star > 0.0)\n        \n        return [R_lono_star, R_loico_star, lambda_lono_star, lambda_loico_star, overfit_detected]\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        (0.20, 123),  # Case 1\n        (0.00, 456),  # Case 2\n        (2.00, 789),  # Case 3\n    ]\n\n    results = []\n    for sigma, seed in test_cases:\n        case_result = run_case(sigma, seed)\n        results.append(case_result)\n\n    # Format the final output string as specified\n    formatted_results = []\n    for res_list in results:\n        str_list = list(map(str, res_list))\n        formatted_results.append(f\"[{','.join(str_list)}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3610428"}, {"introduction": "Uncertainty quantification not only assesses the current state of knowledge but also guides future research. Given limited experimental resources, how do we select the next set of measurements that will be most effective at reducing the uncertainties in our model parameters? This exercise introduces the concept of D-optimal experimental design, a powerful method for answering this question by maximizing the information gained from new experiments. You will implement an exhaustive search to find the set of nuclear measurements that maximizes the determinant of the Fisher Information Matrix, providing hands-on experience in using uncertainty analysis to strategically plan future experiments [@problem_id:3610361].", "problem": "Consider a linearized Energy Density Functional (EDF) model (Energy Density Functional (EDF)) for nuclear observables, where the response of an observable for nucleus $i$ to small parameter perturbations is written as $y_i(\\theta) \\approx y_i(\\theta_0) + \\mathbf{x}_i^{\\top}(\\theta - \\theta_0)$, with $\\theta \\in \\mathbb{R}^p$ denoting the model parameters and $\\mathbf{x}_i \\in \\mathbb{R}^p$ the Jacobian row evaluated at $\\theta_0$. Assume independent Gaussian observational noise $n_i \\sim \\mathcal{N}(0,\\sigma_i^2)$, so that the measured observable satisfies $y_i^{\\text{obs}} = y_i(\\theta) + n_i$. For a design (set of selected nuclei) $\\mathcal{D}$, the Fisher Information Matrix (FIM) (Fisher Information Matrix (FIM)) under this linearized Gaussian model is defined as\n$$\nI(\\theta;\\mathcal{D}) = \\sum_{i \\in \\mathcal{D}} \\frac{1}{\\sigma_i^2} \\mathbf{x}_i \\mathbf{x}_i^{\\top}.\n$$\nThe D-optimal criterion seeks to maximize the determinant of the Fisher Information Matrix, namely\n$$\n\\max_{\\mathcal{D} \\subseteq \\mathcal{S},\\, |\\mathcal{D}|=K} \\det\\left(I(\\theta;\\mathcal{D})\\right),\n$$\nwhere $\\mathcal{S}$ is the candidate set of nuclei and $K$ is the selection budget. In this problem, you will implement an exhaustive search to compute the D-optimal design for a specified candidate set and budgets.\n\nUse the following candidate set $\\mathcal{S}$ of $8$ nuclei, indexed from $0$ to $7$, with proton number $Z$, neutron number $N$, and mass number $A = Z + N$:\n- Index $0$: $^{16}\\text{O}$ with $(Z,N,A) = (8,8,16)$.\n- Index $1$: $^{40}\\text{Ca}$ with $(Z,N,A) = (20,20,40)$.\n- Index $2$: $^{48}\\text{Ca}$ with $(Z,N,A) = (20,28,48)$.\n- Index $3$: $^{56}\\text{Fe}$ with $(Z,N,A) = (26,30,56)$.\n- Index $4$: $^{90}\\text{Zr}$ with $(Z,N,A) = (40,50,90)$.\n- Index $5$: $^{120}\\text{Sn}$ with $(Z,N,A) = (50,70,120)$.\n- Index $6$: $^{132}\\text{Sn}$ with $(Z,N,A) = (50,82,132)$.\n- Index $7$: $^{208}\\text{Pb}$ with $(Z,N,A) = (82,126,208)$.\n\nLet the parameter dimension be $p=4$ and define the Jacobian row $\\mathbf{x}_i$ for nucleus $i$ as the dimensionless feature vector\n$$\n\\mathbf{x}_i = \\begin{bmatrix}\n1 \\\\\nA_i^{-1/3} \\\\\n\\delta_i \\\\\nA_i^{-1}\n\\end{bmatrix}, \\quad \\delta_i = \\frac{N_i - Z_i}{A_i}.\n$$\nAll quantities in $\\mathbf{x}_i$ are unitless by construction. The Fisher Information Matrix $I(\\theta;\\mathcal{D})$ is thus unitless, and the determinant $\\det(I)$ to be reported is also unitless. For numerical stability, compute $\\det(I)$ by taking the product of the eigenvalues of $I$, where eigenvalues smaller than a threshold $\\epsilon$ are set to $0$:\n$$\n\\det(I) = \\prod_{j=1}^{p} \\max(\\lambda_j, \\epsilon), \\quad \\epsilon = 10^{-14}.\n$$\nIf $|\\mathcal{D}|  p$, $I$ is rank-deficient and the determinant should be reported as $0$ by this rule. When multiple designs attain the same determinant within a tolerance $\\tau = 10^{-12}$, break ties by selecting the lexicographically smallest list of indices.\n\nImplement a program that, for the following test suite, computes the D-optimal design and the corresponding determinant:\n- Test case $1$: Selection budget $K=4$ and homogeneous variances $\\sigma_i^2 = 1$ for all $i \\in \\{0,1,\\dots,7\\}$.\n- Test case $2$: Selection budget $K=5$ and heteroscedastic variances $\\sigma_i^2$ defined by\n$$\n\\sigma_i^2 = \\begin{cases}\n0.25  \\text{if } i \\in \\{0,1,2,6,7\\}, \\\\\n1  \\text{otherwise,}\n\\end{cases}\n$$\nso that precision is higher (variance is lower) for doubly magic choices within this set.\n- Test case $3$: Selection budget $K=3$ and homogeneous variances $\\sigma_i^2 = 1$ for all $i \\in \\{0,1,\\dots,7\\}$.\n\nYour program must exhaustively enumerate all $\\binom{8}{K}$ combinations for each test case, compute the Fisher Information Matrix for each combination, evaluate the determinant by the eigenvalue product rule with threshold $\\epsilon = 10^{-14}$, select the combination that maximizes the determinant with tie-breaking tolerance $\\tau = 10^{-12}$ as specified, and return the selected indices and determinant for each test case.\n\nFinal output format requirement: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a two-element list where the first element is the list of selected indices (in increasing order, zero-based) and the second element is the determinant value as a float. For example, the output must have the form $[[\\text{indices}_1,\\det_1],[\\text{indices}_2,\\det_2],[\\text{indices}_3,\\det_3]]$. All outputs are unitless.", "solution": "The problem presented is a well-posed exercise in computational physics, specifically addressing D-optimal experimental design for the uncertainty quantification of a nuclear Energy Density Functional (EDF) model. The task is to identify an optimal subset of nuclei for measurement that will maximally constrain the model parameters. This will be accomplished by maximizing the determinant of the Fisher Information Matrix (FIM), a standard approach in the theory of optimal design.\n\nThe foundation of this problem is a linearized model for a nuclear observable, $y_i$, which depends on a set of model parameters $\\theta \\in \\mathbb{R}^p$. The model is given by\n$$\ny_i(\\theta) \\approx y_i(\\theta_0) + \\mathbf{x}_i^{\\top}(\\theta - \\theta_0)\n$$\nwhere $\\mathbf{x}_i$ is the Jacobian (gradient) of the observable with respect to the parameters, evaluated at a reference parameter point $\\theta_0$. We assume that experimental measurements $y_i^{\\text{obs}}$ are subject to independent, zero-mean Gaussian noise, $n_i \\sim \\mathcal{N}(0, \\sigma_i^2)$.\n\nUnder these assumptions, the Fisher Information Matrix for a given set of experiments (a design) $\\mathcal{D}$ is\n$$\nI(\\theta;\\mathcal{D}) = \\sum_{i \\in \\mathcal{D}} \\frac{1}{\\sigma_i^2} \\mathbf{x}_i \\mathbf{x}_i^{\\top}\n$$\nThe FIM is the inverse of the covariance matrix for the estimated parameters, $I^{-1} = \\text{Cov}(\\hat{\\theta})$. The D-optimality criterion aims to minimize the volume of the confidence ellipsoid for the parameters, which is equivalent to maximizing the determinant of the FIM, $\\det(I)$. The optimization problem is thus:\n$$\n\\max_{\\mathcal{D} \\subseteq \\mathcal{S}, \\, |\\mathcal{D}|=K} \\det\\left(I(\\theta;\\mathcal{D})\\right)\n$$\nwhere $\\mathcal{S}$ is a candidate set of $8$ nuclei, and $K$ is the experimental budget (the number of nuclei to select).\n\nThe problem specifies a parameter dimension of $p=4$ and defines the Jacobian row vector $\\mathbf{x}_i$ for each nucleus $i$ based on its proton number $Z_i$, neutron number $N_i$, and mass number $A_i = Z_i + N_i$:\n$$\n\\mathbf{x}_i = \\begin{bmatrix}\n1 \\\\\nA_i^{-1/3} \\\\\n\\delta_i \\\\\nA_i^{-1}\n\\end{bmatrix}, \\quad \\text{with} \\quad \\delta_i = \\frac{N_i - Z_i}{A_i}\n$$\nThe components of this vector represent physically motivated dependencies: a constant term related to bulk properties, a term proportional to $A_i^{-1/3}$ which is a proxy for surface effects, an isospin asymmetry term $\\delta_i$, and a higher-order or curvature term proportional to $A_i^{-1}$.\n\nThe algorithmic approach to solving this optimization problem is an exhaustive search. Given the small size of the candidate pool ($|\\mathcal{S}|=8$), it is computationally feasible to enumerate all $\\binom{8}{K}$ possible designs for the given budgets $K \\in \\{3, 4, 5\\}$.\n\nThe procedure for each test case is as follows:\n1.  Enumerate all possible subsets (designs) $\\mathcal{D}$ of indices from $\\{0, 1, \\dots, 7\\}$ of size $K$.\n2.  For each design $\\mathcal{D}$:\n    a.  Check the rank condition. The FIM is a $p \\times p$ matrix, which is a sum of $|\\mathcal{D}|$ rank-1 matrices. If $|\\mathcal{D}|  p$, the FIM is singular, and its determinant is exactly $0$. The problem states that the determinant should be reported as $0$ in this case. For the test case with $K=3$ and $p=4$, all designs will result in a determinant of $0$.\n    b.  If $|\\mathcal{D}| \\ge p$, construct the $4 \\times 4$ FIM, $I(\\mathcal{D})$, by initializing a zero matrix and summing the contributions from each selected nucleus:\n        $$\n        I(\\mathcal{D}) = \\sum_{i \\in \\mathcal{D}} w_i \\mathbf{x}_i \\mathbf{x}_i^{\\top}\n        $$\n        where the weights are the precisions, $w_i = 1/\\sigma_i^2$.\n    c.  Compute the determinant of the FIM. To ensure numerical stability, the problem specifies a particular rule. First, the eigenvalues $\\{\\lambda_j\\}_{j=1}^p$ of the symmetric, positive semi-definite FIM are found. The determinant is then calculated as the product of these eigenvalues, with a floor set by a small threshold $\\epsilon = 10^{-14}$:\n        $$\n        \\det(I) = \\prod_{j=1}^{p} \\max(\\lambda_j, \\epsilon)\n        $$\n        This regularization prevents the determinant from becoming an unrepresentably small number or zero due to floating-point imprecision when it should be positive.\n3.  Compare the computed determinant, $\\det(\\mathcal{D})$, with the maximum determinant found so far, $\\det_{\\max}$. A tie-breaking rule is specified. If multiple designs yield a determinant that is equal within a tolerance of $\\tau = 10^{-12}$, the design with the lexicographically smallest list of indices is to be chosen. This is handled by processing the designs in lexicographical order (which is the default for Python's `itertools.combinations`) and only updating the optimal design if a new determinant is found that is strictly greater than the current maximum plus the tolerance, i.e., $\\det(\\mathcal{D}_{\\text{new}})  \\det_{\\max} + \\tau$.\n\nThis procedure is applied to each of the three test cases, which differ in their selection budget $K$ and their noise variance assumptions $\\sigma_i^2$. The final output for each case will be the lexicographically smallest optimal design and its corresponding determinant.", "answer": "```python\nimport numpy as np\nfrom itertools import combinations\nimport sys\n\ndef solve():\n    \"\"\"\n    Solves for the D-optimal experimental design for a linearized nuclear EDF model.\n\n    The program exhaustively searches over all combinations of nuclei to find the\n    subset that maximizes the determinant of the Fisher Information Matrix (FIM).\n    \"\"\"\n\n    # --- Problem Constants and Definitions ---\n\n    # Parameter dimension\n    P_DIM = 4\n    # Determinant computation threshold\n    EPSILON = 1e-14\n    # Tie-breaking tolerance\n    TAU = 1e-12\n\n    # Candidate set of nuclei S, indexed 0-7\n    # Data: (Z, N)\n    nuclei_data = [\n        (8, 8),     # 0: 16O\n        (20, 20),   # 1: 40Ca\n        (20, 28),   # 2: 48Ca\n        (26, 30),   # 3: 56Fe\n        (40, 50),   # 4: 90Zr\n        (50, 70),   # 5: 120Sn\n        (50, 82),   # 6: 132Sn\n        (82, 126),  # 7: 208Pb\n    ]\n\n    # Pre-compute Jacobian rows (feature vectors) x_i for all nuclei\n    x_vectors = []\n    for z, n in nuclei_data:\n        a = float(z + n)\n        delta = (n - z) / a\n        x_i = np.array([\n            1.0,\n            a**(-1.0/3.0),\n            delta,\n            a**(-1.0)\n        ])\n        x_vectors.append(x_i)\n\n    # --- Test Cases ---\n\n    test_cases = [\n        {\n            \"K\": 4,\n            \"variances\": np.ones(8)\n        },\n        {\n            \"K\": 5,\n            \"variances\": np.array([0.25 if i in {0, 1, 2, 6, 7} else 1.0 for i in range(8)])\n        },\n        {\n            \"K\": 3,\n            \"variances\": np.ones(8)\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        K = case[\"K\"]\n        variances = case[\"variances\"]\n        weights = 1.0 / variances\n\n        best_design = []\n        max_det = -1.0\n\n        # Generate all combinations (designs) of size K\n        designs = combinations(range(len(nuclei_data)), K)\n\n        for design in designs:\n            current_det = 0.0\n            \n            # If K  p, the FIM is rank-deficient and det(I) is 0.\n            if K  P_DIM:\n                current_det = 0.0\n            else:\n                # Construct the Fisher Information Matrix (FIM)\n                fim = np.zeros((P_DIM, P_DIM))\n                for i in design:\n                    x_i = x_vectors[i]\n                    w_i = weights[i]\n                    # Add rank-1 update for nucleus i\n                    fim += w_i * np.outer(x_i, x_i)\n\n                # Compute eigenvalues. Use eigvalsh for symmetric matrices.\n                eigenvalues = np.linalg.eigvalsh(fim)\n                \n                # Compute determinant using the specified rule\n                # det(I) = product(max(lambda_j, epsilon))\n                stable_eigs = np.maximum(eigenvalues, EPSILON)\n                current_det = np.prod(stable_eigs)\n\n            # Update best design found so far.\n            # Update if the new determinant is larger than the current max by the tolerance TAU.\n            # This handles tie-breaking implicitly, as combinations are generated in\n            # lexicographical order. The first one to achieve a certain max_det value\n            # will be kept.\n            if current_det > max_det + TAU:\n                max_det = current_det\n                best_design = design\n\n        # Store the result for this test case\n        all_results.append([list(best_design), max_det])\n\n    # --- Format and Print Final Output ---\n    \n    # Manually construct the output string to match the exact format requirement,\n    # specifically with no spaces inside the list brackets.\n    result_strings = []\n    for res in all_results:\n        indices, det_val = res\n        # Convert the list of indices to a comma-separated string `[i1,i2,...]`\n        indices_str = f\"[{','.join(map(str, indices))}]\"\n        # Combine into the `[[indices],det]` format for one test case\n        result_strings.append(f\"[{indices_str},{det_val}]\")\n    \n    # Join all test case results and enclose in the outermost brackets\n    final_output_str = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output_str)\n\nsolve()\n\n```", "id": "3610361"}]}