{"hands_on_practices": [{"introduction": "Before tackling complex, multi-parameter nuclear models, it is essential to master the core mechanism of Bayesian inference. This exercise provides a foundational look at how information from data updates our prior beliefs in the simplest non-trivial setting: a single parameter with a Gaussian prior and likelihood [@problem_id:3544172]. By analytically deriving how the posterior distribution shifts and narrows as a function of the prior's width, you will gain a concrete intuition for the precision-weighted 'compromise' between the prior and the data, a principle that underlies all Bayesian calibration.", "problem": "Consider the Bayesian calibration of a single dimensionless low-energy constant $\\theta$ appearing in a contact interaction of a nucleon-nucleon potential within Effective Field Theory (EFT). Suppose a single emulator-predicted observable, such as a binding-energy residual, is modeled as a linear response to $\\theta$ with additive Gaussian noise:\n$$\ny \\mid \\theta \\sim \\mathcal{N}(a\\theta,\\ \\sigma^{2}),\n$$\nwhere $y$ is the observed residual, $a$ is a known sensitivity coefficient from the linearized emulator, and $\\sigma^2$ is the known variance of combined emulator and experimental uncertainties. The prior for $\\theta$ is Gaussian,\n$$\n\\theta \\sim \\mathcal{N}(\\mu_{0},\\ \\tau^{2}),\n$$\nwith known prior mean $\\mu_0$ and prior variance $\\tau^2$. All quantities $\\theta, \\mu_0, a, y$ are treated as dimensionless for this calibration.\n\nYou will compare two calibrations that differ only in the prior width: one uses prior variance $\\tau_{1}^{2}$ and the other uses $\\tau_{2}^{2}$, both sharing the same prior mean $\\mu_{0}$. Starting from Bayes’ theorem and the definitions of Gaussian likelihood and Gaussian prior, derive the posterior for each case and then compute:\n\n1. The exact shift in the posterior mean caused by changing the prior variance from $\\tau_{1}^{2}$ to $\\tau_{2}^{2}$:\n$$\n\\Delta \\mu_{\\mathrm{post}} \\equiv \\mu_{\\mathrm{post}}(\\tau_{2}^{2}) - \\mu_{\\mathrm{post}}(\\tau_{1}^{2}).\n$$\n\n2. The exact ratio of the posterior variances:\n$$\n\\rho \\equiv \\frac{\\tau_{\\mathrm{post}}^{2}(\\tau_{2}^{2})}{\\tau_{\\mathrm{post}}^{2}(\\tau_{1}^{2})}.\n$$\n\nExpress both $\\Delta \\mu_{\\mathrm{post}}$ and $\\rho$ as closed-form analytic functions of $\\mu_0, a, y, \\sigma, \\tau_1^2, \\tau_2^2$. No numerical evaluation is required. Provide your final answer as a single row vector containing $\\Delta \\mu_{\\mathrm{post}}$ and $\\rho$ in that order. Since $\\theta$ and all relevant quantities are dimensionless in this setup, no physical units are needed. No rounding is required; present exact analytic expressions.", "solution": "The problem is well-posed and scientifically grounded, resting on the standard principles of Bayesian inference with conjugate priors. The task is to derive the change in the posterior mean and the ratio of posterior variances for a parameter $\\theta$ when its Gaussian prior variance is changed.\n\nFirst, we must derive the general form of the posterior distribution for $\\theta$. According to Bayes' theorem, the posterior probability density function (PDF) $p(\\theta \\mid y)$ is proportional to the product of the likelihood function $p(y \\mid \\theta)$ and the prior PDF $p(\\theta)$:\n$$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) \\, p(\\theta)\n$$\nThe problem states that the likelihood is Gaussian:\n$$\np(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y - a\\theta)^2}{2\\sigma^2} \\right)\n$$\nAnd the prior for $\\theta$ is also Gaussian:\n$$\np(\\theta) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left( -\\frac{(\\theta - \\mu_0)^2}{2\\tau^2} \\right)\n$$\nMultiplying these two distributions, and dropping the constant normalization factors, we get the unnormalized posterior:\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\frac{(y - a\\theta)^2}{2\\sigma^2} \\right) \\exp\\left( -\\frac{(\\theta - \\mu_0)^2}{2\\tau^2} \\right)\n$$\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{(y - a\\theta)^2}{\\sigma^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau^2} \\right] \\right)\n$$\nTo identify the form of the posterior distribution, we complete the square for $\\theta$ in the exponent. Let the term in the square brackets be $Q(\\theta)$:\n$$\nQ(\\theta) = \\frac{y^2 - 2ay\\theta + a^2\\theta^2}{\\sigma^2} + \\frac{\\theta^2 - 2\\mu_0\\theta + \\mu_0^2}{\\tau^2}\n$$\nWe collect terms involving $\\theta^2$ and $\\theta$:\n$$\nQ(\\theta) = \\theta^2 \\left( \\frac{a^2}{\\sigma^2} + \\frac{1}{\\tau^2} \\right) - 2\\theta \\left( \\frac{ay}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2} \\right) + \\mathrm{const.}\n$$\nwhere \"const.\" includes terms not dependent on $\\theta$. The posterior distribution for $\\theta$ is a Gaussian of the form $\\mathcal{N}(\\mu_{\\mathrm{post}}, \\tau_{\\mathrm{post}}^2)$, which has a PDF proportional to $\\exp\\left( -\\frac{(\\theta - \\mu_{\\mathrm{post}})^2}{2\\tau_{\\mathrm{post}}^2} \\right)$. The exponent can be written as:\n$$\n-\\frac{1}{2\\tau_{\\mathrm{post}}^2} (\\theta^2 - 2\\mu_{\\mathrm{post}}\\theta + \\mu_{\\mathrm{post}}^2) = -\\frac{1}{2} \\left[ \\theta^2 \\left(\\frac{1}{\\tau_{\\mathrm{post}}^2}\\right) - 2\\theta \\left(\\frac{\\mu_{\\mathrm{post}}}{\\tau_{\\mathrm{post}}^2}\\right) + \\mathrm{const.} \\right]\n$$\nBy comparing the coefficients of the $\\theta^2$ term in $Q(\\theta)$ and the standard Gaussian form, we identify the inverse of the posterior variance, which is the posterior precision:\n$$\n\\frac{1}{\\tau_{\\mathrm{post}}^2} = \\frac{a^2}{\\sigma^2} + \\frac{1}{\\tau^2} = \\frac{a^2\\tau^2 + \\sigma^2}{\\sigma^2\\tau^2}\n$$\nFrom this, the posterior variance $\\tau_{\\mathrm{post}}^2$ is:\n$$\n\\tau_{\\mathrm{post}}^2 = \\frac{\\sigma^2\\tau^2}{a^2\\tau^2 + \\sigma^2}\n$$\nBy comparing the coefficients of the $\\theta$ term, we find the posterior mean $\\mu_{\\mathrm{post}}$:\n$$\n\\frac{\\mu_{\\mathrm{post}}}{\\tau_{\\mathrm{post}}^2} = \\frac{ay}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2}\n$$\n$$\n\\mu_{\\mathrm{post}} = \\tau_{\\mathrm{post}}^2 \\left( \\frac{ay}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2} \\right) = \\left( \\frac{\\sigma^2\\tau^2}{a^2\\tau^2 + \\sigma^2} \\right) \\left( \\frac{ay\\tau^2 + \\mu_0\\sigma^2}{\\sigma^2\\tau^2} \\right)\n$$\n$$\n\\mu_{\\mathrm{post}} = \\frac{ay\\tau^2 + \\mu_0\\sigma^2}{a^2\\tau^2 + \\sigma^2}\n$$\nNow we apply these general results to the two specified cases.\n\nCase 1: Prior variance $\\tau_1^2$.\n$$\n\\mu_{\\mathrm{post}}(\\tau_1^2) = \\frac{ay\\tau_1^2 + \\mu_0\\sigma^2}{a^2\\tau_1^2 + \\sigma^2}\n$$\n$$\n\\tau_{\\mathrm{post}}^2(\\tau_1^2) = \\frac{\\sigma^2\\tau_1^2}{a^2\\tau_1^2 + \\sigma^2}\n$$\n\nCase 2: Prior variance $\\tau_2^2$.\n$$\n\\mu_{\\mathrm{post}}(\\tau_2^2) = \\frac{ay\\tau_2^2 + \\mu_0\\sigma^2}{a^2\\tau_2^2 + \\sigma^2}\n$$\n$$\n\\tau_{\\mathrm{post}}^2(\\tau_2^2) = \\frac{\\sigma^2\\tau_2^2}{a^2\\tau_2^2 + \\sigma^2}\n$$\n\n1.  Compute the shift in the posterior mean, $\\Delta \\mu_{\\mathrm{post}} = \\mu_{\\mathrm{post}}(\\tau_2^2) - \\mu_{\\mathrm{post}}(\\tau_1^2)$:\n$$\n\\Delta \\mu_{\\mathrm{post}} = \\frac{ay\\tau_2^2 + \\mu_0\\sigma^2}{a^2\\tau_2^2 + \\sigma^2} - \\frac{ay\\tau_1^2 + \\mu_0\\sigma^2}{a^2\\tau_1^2 + \\sigma^2}\n$$\nWe find a common denominator:\n$$\n\\Delta \\mu_{\\mathrm{post}} = \\frac{(ay\\tau_2^2 + \\mu_0\\sigma^2)(a^2\\tau_1^2 + \\sigma^2) - (ay\\tau_1^2 + \\mu_0\\sigma^2)(a^2\\tau_2^2 + \\sigma^2)}{(a^2\\tau_1^2 + \\sigma^2)(a^2\\tau_2^2 + \\sigma^2)}\n$$\nExpanding the numerator:\n$$\n(a^3y\\tau_1^2\\tau_2^2 + ay\\sigma^2\\tau_2^2 + a^2\\mu_0\\sigma^2\\tau_1^2 + \\mu_0\\sigma^4) - (a^3y\\tau_1^2\\tau_2^2 + ay\\sigma^2\\tau_1^2 + a^2\\mu_0\\sigma^2\\tau_2^2 + \\mu_0\\sigma^4)\n$$\nThe terms $a^3y\\tau_1^2\\tau_2^2$ and $\\mu_0\\sigma^4$ cancel. We are left with:\n$$\nay\\sigma^2\\tau_2^2 + a^2\\mu_0\\sigma^2\\tau_1^2 - ay\\sigma^2\\tau_1^2 - a^2\\mu_0\\sigma^2\\tau_2^2 = ay\\sigma^2(\\tau_2^2 - \\tau_1^2) - a^2\\mu_0\\sigma^2(\\tau_2^2 - \\tau_1^2)\n$$\nFactoring out common terms gives:\n$$\na\\sigma^2(y - a\\mu_0)(\\tau_2^2 - \\tau_1^2)\n$$\nThus, the shift in the posterior mean is:\n$$\n\\Delta \\mu_{\\mathrm{post}} = \\frac{a\\sigma^2(y - a\\mu_0)(\\tau_2^2 - \\tau_1^2)}{(a^2\\tau_1^2 + \\sigma^2)(a^2\\tau_2^2 + \\sigma^2)}\n$$\n\n2.  Compute the ratio of posterior variances, $\\rho = \\frac{\\tau_{\\mathrm{post}}^2(\\tau_2^2)}{\\tau_{\\mathrm{post}}^2(\\tau_1^2)}$:\n$$\n\\rho = \\frac{\\frac{\\sigma^2\\tau_2^2}{a^2\\tau_2^2 + \\sigma^2}}{\\frac{\\sigma^2\\tau_1^2}{a^2\\tau_1^2 + \\sigma^2}}\n$$\nThe $\\sigma^2$ term in the numerator of each fraction cancels:\n$$\n\\rho = \\frac{\\tau_2^2}{a^2\\tau_2^2 + \\sigma^2} \\cdot \\frac{a^2\\tau_1^2 + \\sigma^2}{\\tau_1^2}\n$$\nRearranging gives the final expression for the ratio:\n$$\n\\rho = \\frac{\\tau_2^2(a^2\\tau_1^2 + \\sigma^2)}{\\tau_1^2(a^2\\tau_2^2 + \\sigma^2)}\n$$\nThe two requested quantities, $\\Delta \\mu_{\\mathrm{post}}$ and $\\rho$, are now expressed as closed-form analytic functions of the given parameters. The final answer is presented as a row vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{a\\sigma^2(y - a\\mu_0)(\\tau_2^2 - \\tau_1^2)}{(a^2\\tau_1^2 + \\sigma^2)(a^2\\tau_2^2 + \\sigma^2)} & \\frac{\\tau_2^2 (a^2\\tau_1^2 + \\sigma^2)}{\\tau_1^2 (a^2\\tau_2^2 + \\sigma^2)} \\end{pmatrix}}\n$$", "id": "3544172"}, {"introduction": "A successful parameter fit does not guarantee that the parameters are meaningful or well-determined by the data. This practice moves beyond simple posterior calculation to the crucial concept of parameter identifiability, asking: how much information does our experiment actually provide about each parameter combination? By deriving the Fisher Information matrix for a linearized model, you will uncover how the model's sensitivity (the Jacobian) and the noise characteristics (the covariance) dictate which parameter directions are well-constrained ('stiff') and which are poorly constrained ('sloppy'), a foundational concept for both model diagnostics and experimental design [@problem_id:3544124].", "problem": "In the Bayesian calibration of nuclear interaction parameters, consider a vector of model predictions $f(\\theta) \\in \\mathbb{R}^{n}$ for $n$ observables as a function of $p$ parameters $\\theta \\in \\mathbb{R}^{p}$. Suppose the data model assumes additive Gaussian noise with zero mean and known, positive-definite covariance $C \\in \\mathbb{R}^{n \\times n}$, and the likelihood is constructed accordingly. Around a nominal point $\\theta_{0}$, linearize the model by writing $f(\\theta) \\approx f(\\theta_{0}) + J(\\theta - \\theta_{0})$, where $J \\in \\mathbb{R}^{n \\times p}$ is the Jacobian with entries $J_{i\\alpha} = \\partial f_{i} / \\partial \\theta_{\\alpha}$ evaluated at $\\theta_{0}$. \n\nTask A: Starting from the Gaussian likelihood and the definition of the Fisher information matrix $F$ via the expectation of the Hessian of the negative log-likelihood, derive the Fisher information matrix for the linearized model in terms of $J$ and $C$. Your derivation must begin from the fundamental definition of the likelihood for Gaussian-distributed errors and the definition of Fisher information, and it must not assume any pre-stated closed-form for $F$.\n\nTask B: Explain, based on your derivation, how the eigenvalues of $F$ quantify local identifiability of the components of $\\theta$ near $\\theta_{0}$ in the sense of whether small perturbations in $\\theta$ produce distinguishable changes in the model predictions relative to the noise level.\n\nTask C: For a concrete calibration of $p = 2$ dimensionless interaction parameters to $n = 3$ dimensionless observables (constructed by suitable normalization so that all Jacobian entries are dimensionless and $C$ is the covariance of dimensionless errors), suppose the Jacobian at $\\theta_{0}$ and the observational error covariance are\n$$\nJ \\;=\\; \\begin{pmatrix}\n0.8 & -0.2 \\\\\n0.5 & 0.3 \\\\\n-0.1 & 0.6\n\\end{pmatrix},\n\\qquad\nC \\;=\\; \\operatorname{diag}\\!\\big(0.04,\\, 0.01,\\, 0.09\\big).\n$$\nCompute the Fisher information matrix for this linearized model and then compute the D-optimality measure, defined as $\\det(F)$. Provide the final numerical value of $\\det(F)$, rounded to four significant figures. Because the parameters and observables have been non-dimensionalized, the requested numerical answer is dimensionless. The final answer must be a single real number as specified.", "solution": "The problem is valid as it is scientifically grounded in statistical inference and computational physics, well-posed with all necessary information, and stated objectively. We proceed with the solution.\n\nThe problem is divided into three tasks. We will address them sequentially.\n\n### Task A: Derivation of the Fisher Information Matrix\n\nThe goal is to derive the Fisher information matrix $F$ for a linearized model with Gaussian noise. The definition of $F$ is the expectation of the Hessian of the negative log-likelihood.\n\nLet the data vector be $d \\in \\mathbb{R}^{n}$. The data model is given by $d = f(\\theta) + \\epsilon$, where $\\theta \\in \\mathbb{R}^{p}$ is the parameter vector and $\\epsilon$ is a random noise vector. The problem states that the noise is Gaussian with zero mean and a known, positive-definite covariance matrix $C \\in \\mathbb{R}^{n \\times n}$. Thus, $\\epsilon \\sim \\mathcal{N}(0, C)$.\n\nThe probability density of the data $d$ given the parameters $\\theta$, which is the likelihood function $\\mathcal{L}(\\theta|d)$, is given by the multivariate Gaussian distribution:\n$$\n\\mathcal{L}(\\theta|d) = p(d|\\theta) = \\frac{1}{\\sqrt{(2\\pi)^{n} \\det(C)}} \\exp\\left( -\\frac{1}{2} (d - f(\\theta))^{T} C^{-1} (d - f(\\theta)) \\right)\n$$\nThe log-likelihood, $\\ln\\mathcal{L}(\\theta|d)$, is:\n$$\n\\ln\\mathcal{L}(\\theta|d) = -\\frac{n}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(C)) - \\frac{1}{2} (d - f(\\theta))^{T} C^{-1} (d - f(\\theta))\n$$\nThe negative log-likelihood, which we denote as $\\mathcal{NLL}(\\theta)$, is obtained by negating the above expression. The terms that do not depend on $\\theta$ can be grouped into a constant.\n$$\n\\mathcal{NLL}(\\theta) = \\text{const} + \\frac{1}{2} (d - f(\\theta))^{T} C^{-1} (d - f(\\theta))\n$$\nThe problem specifies using a linearized model for $f(\\theta)$ around a nominal point $\\theta_{0}$:\n$$\nf(\\theta) \\approx f(\\theta_{0}) + J(\\theta - \\theta_{0})\n$$\nwhere $J$ is the Jacobian matrix evaluated at $\\theta_{0}$. Substituting this into the expression for $\\mathcal{NLL}(\\theta)$:\n$$\nd - f(\\theta) \\approx d - [f(\\theta_{0}) + J(\\theta - \\theta_{0})] = [d - f(\\theta_{0})] - J(\\theta - \\theta_{0})\n$$\nLet the residual at the nominal point be $r_{0} = d - f(\\theta_{0})$. Then, $d - f(\\theta) \\approx r_{0} - J(\\theta - \\theta_{0})$.\nThe negative log-likelihood for the linearized model is:\n$$\n\\mathcal{NLL}(\\theta) \\approx \\text{const} + \\frac{1}{2} (r_{0} - J(\\theta - \\theta_{0}))^{T} C^{-1} (r_{0} - J(\\theta - \\theta_{0}))\n$$\nThe Fisher information matrix $F$ is a $p \\times p$ matrix with entries $F_{\\alpha\\beta} = E_d \\left[ \\frac{\\partial^2 \\mathcal{NLL}}{\\partial \\theta_{\\alpha} \\partial \\theta_{\\beta}} \\right]$. First, we compute the Hessian matrix $H_{\\mathcal{NLL}}$ of $\\mathcal{NLL}(\\theta)$. To do this, we first find the gradient, $\\nabla_{\\theta} \\mathcal{NLL}$.\nLet $v(\\theta) = r_{0} - J(\\theta - \\theta_{0})$. The quadratic term is $\\frac{1}{2} v(\\theta)^{T} C^{-1} v(\\theta)$. The derivative of this quadratic form with respect to a component $\\theta_{\\alpha}$ is:\n$$\n\\frac{\\partial}{\\partial \\theta_{\\alpha}} \\left( \\frac{1}{2} v^{T} C^{-1} v \\right) = \\left( \\frac{\\partial v}{\\partial \\theta_{\\alpha}} \\right)^{T} C^{-1} v\n$$\nThe partial derivative of the vector $v(\\theta)$ with respect to $\\theta_{\\alpha}$ is the $\\alpha$-th column of the Jacobian of $v$ with respect to $\\theta$, which is $-J$. So, $\\frac{\\partial v_i}{\\partial \\theta_\\alpha} = -J_{i\\alpha}$. This means the derivative vector is the $\\alpha$-th column of $-J$, denoted $(-J)_{:,\\alpha}$.\n$$\n\\left( \\frac{\\partial v}{\\partial \\theta_{\\alpha}} \\right) = (-J)_{:,\\alpha}\n$$\nSo the gradient component is:\n$$\n\\frac{\\partial \\mathcal{NLL}}{\\partial \\theta_{\\alpha}} = ((-J)_{:,\\alpha})^{T} C^{-1} v = ((-J)^{T}C^{-1}v)_{\\alpha}\n$$\nThe full gradient vector is:\n$$\n\\nabla_{\\theta} \\mathcal{NLL} = (-J)^{T} C^{-1} v(\\theta) = -J^{T} C^{-1} (r_{0} - J(\\theta - \\theta_{0}))\n$$\nNow we compute the Hessian by differentiating the gradient with respect to $\\theta_{\\beta}$:\n$$\n\\frac{\\partial^2 \\mathcal{NLL}}{\\partial \\theta_{\\beta} \\partial \\theta_{\\alpha}} = \\frac{\\partial}{\\partial \\theta_{\\beta}} \\left[ -J^{T} C^{-1} r_{0} + J^{T} C^{-1} J (\\theta - \\theta_{0}) \\right]_{\\alpha}\n$$\nThe term $-J^{T} C^{-1} r_{0}$ is constant with respect to $\\theta$. The second term is a matrix $J^{T} C^{-1} J$ times a vector $(\\theta - \\theta_{0})$.\n$$\n\\frac{\\partial}{\\partial \\theta_{\\beta}} \\left[ \\sum_{\\gamma=1}^{p} (J^{T} C^{-1} J)_{\\alpha\\gamma} (\\theta_{\\gamma} - (\\theta_{0})_{\\gamma}) \\right] = (J^{T} C^{-1} J)_{\\alpha\\beta}\n$$\nThis holds for all $\\alpha, \\beta$. Therefore, the Hessian matrix is:\n$$\nH_{\\mathcal{NLL}}(\\theta) = J^{T} C^{-1} J\n$$\nA key feature of the linearized model is that its Hessian is constant, independent of both $\\theta$ and the data $d$ (which is contained in $r_0$).\nThe final step is to take the expectation over the data $d$. Since the Hessian is independent of $d$, the expectation is the matrix itself.\n$$\nF = E_d[H_{\\mathcal{NLL}}(\\theta)] = E_d[J^{T} C^{-1} J] = J^{T} C^{-1} J\n$$\nThis completes the derivation for Task A.\n\n### Task B: Interpretation of the Eigenvalues of F\n\nThe Fisher information matrix $F = J^{T} C^{-1} J$ quantifies the amount of information that the observable data provides about the unknown parameters $\\theta$. The eigenvalues of $F$ provide a specific, geometric interpretation of this information in terms of parameter identifiability.\n\nConsider a small perturbation in the parameters, $\\delta\\theta$, away from the nominal point $\\theta_{0}$. This causes a change in the model prediction, $\\delta f = f(\\theta_0 + \\delta\\theta) - f(\\theta_0) \\approx J \\delta\\theta$. The significance of this change must be evaluated relative to the observational noise, which is characterized by the covariance matrix $C$. The squared Mahalanobis distance, $\\delta f^{T} C^{-1} \\delta f$, measures the squared distance of the change $\\delta f$ from the origin in the data space, weighted by the inverse covariance. It is a dimensionless measure of how distinguishable the change $\\delta f$ is from a typical noise fluctuation.\n\nWe can relate this directly to $F$ by considering the quadratic form $\\delta\\theta^{T} F \\delta\\theta$:\n$$\n\\delta\\theta^{T} F \\delta\\theta = \\delta\\theta^{T} (J^{T} C^{-1} J) \\delta\\theta = (J \\delta\\theta)^{T} C^{-1} (J \\delta\\theta) = \\delta f^{T} C^{-1} \\delta f\n$$\nThis shows that $\\delta\\theta^{T} F \\delta\\theta$ is precisely the squared statistical \"distance\" or distinguishability of the change in model output.\n\nNow, let $F$ have an eigendecomposition $F = V \\Lambda V^{T}$, where $V$ is an orthogonal matrix whose columns $v_k$ are the eigenvectors of $F$, and $\\Lambda$ is a diagonal matrix of the corresponding eigenvalues $\\lambda_k$. The eigenvectors $\\{v_k\\}$ form an orthonormal basis for the parameter space.\n\nIf we choose a perturbation along a specific eigenvector, $\\delta\\theta = \\epsilon v_k$ for a small magnitude $\\epsilon$, the distinguishability measure becomes:\n$$\n\\delta\\theta^{T} F \\delta\\theta = (\\epsilon v_k)^{T} F (\\epsilon v_k) = \\epsilon^{2} v_k^{T} F v_k = \\epsilon^{2} v_k^{T} (\\lambda_k v_k) = \\epsilon^{2} \\lambda_k \\|v_k\\|^2 = \\epsilon^{2} \\lambda_k\n$$\nTherefore, the eigenvalue $\\lambda_k$ is the distinguishability of the model output change per unit squared perturbation of the parameters in the direction of the eigenvector $v_k$.\n- A **large eigenvalue $\\lambda_k$** implies that even a small perturbation in the parameter combination defined by $v_k$ leads to a large, statistically significant change in the model predictions. This parameter combination is said to be \"stiff\" or **well-identified** by the data. The data are sensitive to changes in this direction.\n- A **small eigenvalue $\\lambda_k$** implies that a large perturbation in the parameter combination defined by $v_k$ produces only a small, statistically insignificant change in the model predictions, which would be difficult to distinguish from noise. This parameter combination is \"sloppy\" or **poorly-identified**. The data provide little to no information to constrain this combination of parameters.\n\nIn summary, the eigenvalues of $F$ quantify the local identifiability of different linear combinations of parameters, with the eigenvectors defining the directions of these combinations. The spectrum of eigenvalues reveals a hierarchy of parameter sensitivities.\n\n### Task C: Numerical Calculation\n\nWe are given the Jacobian $J$ and the covariance matrix $C$:\n$$\nJ = \\begin{pmatrix}\n0.8 & -0.2 \\\\\n0.5 & 0.3 \\\\\n-0.1 & 0.6\n\\end{pmatrix},\n\\qquad\nC = \\begin{pmatrix}\n0.04 & 0 & 0 \\\\\n0 & 0.01 & 0 \\\\\n0 & 0 & 0.09\n\\end{pmatrix}\n$$\nFirst, we compute the inverse of the covariance matrix, $C^{-1}$. Since $C$ is diagonal, $C^{-1}$ is also diagonal with entries that are the reciprocals of the entries of $C$.\n$$\nC^{-1} = \\begin{pmatrix}\n1/0.04 & 0 & 0 \\\\\n0 & 1/0.01 & 0 \\\\\n0 & 0 & 1/0.09\n\\end{pmatrix} = \\begin{pmatrix}\n25 & 0 & 0 \\\\\n0 & 100 & 0 \\\\\n0 & 0 & 100/9\n\\end{pmatrix}\n$$\nNext, we compute the Fisher information matrix using the formula from Task A, $F = J^{T} C^{-1} J$.\nThe transpose of $J$ is:\n$$\nJ^{T} = \\begin{pmatrix}\n0.8 & 0.5 & -0.1 \\\\\n-0.2 & 0.3 & 0.6\n\\end{pmatrix}\n$$\nWe first compute the product $J^{T} C^{-1}$:\n$$\nJ^{T} C^{-1} = \\begin{pmatrix}\n0.8 & 0.5 & -0.1 \\\\\n-0.2 & 0.3 & 0.6\n\\end{pmatrix}\n\\begin{pmatrix}\n25 & 0 & 0 \\\\\n0 & 100 & 0 \\\\\n0 & 0 & 100/9\n\\end{pmatrix}\n= \\begin{pmatrix}\n(0.8)(25) & (0.5)(100) & (-0.1)(100/9) \\\\\n(-0.2)(25) & (0.3)(100) & (0.6)(100/9)\n\\end{pmatrix}\n$$\n$$\nJ^{T} C^{-1} = \\begin{pmatrix}\n20 & 50 & -10/9 \\\\\n-5 & 30 & 60/9\n\\end{pmatrix} = \\begin{pmatrix}\n20 & 50 & -10/9 \\\\\n-5 & 30 & 20/3\n\\end{pmatrix}\n$$\nNow we multiply this result by $J$ to get $F$:\n$$\nF = (J^{T} C^{-1}) J = \\begin{pmatrix}\n20 & 50 & -10/9 \\\\\n-5 & 30 & 20/3\n\\end{pmatrix}\n\\begin{pmatrix}\n0.8 & -0.2 \\\\\n0.5 & 0.3 \\\\\n-0.1 & 0.6\n\\end{pmatrix}\n$$\nThe elements of the $2 \\times 2$ matrix $F$ are:\n$$\nF_{11} = (20)(0.8) + (50)(0.5) + (-10/9)(-0.1) = 16 + 25 + 1/9 = 41 + 1/9 = \\frac{369}{9} + \\frac{1}{9} = \\frac{370}{9}\n$$\n$$\nF_{12} = (20)(-0.2) + (50)(0.3) + (-10/9)(0.6) = -4 + 15 - 6/9 = 11 - 2/3 = \\frac{33}{3} - \\frac{2}{3} = \\frac{31}{3}\n$$\n$$\nF_{21} = (-5)(0.8) + (30)(0.5) + (20/3)(-0.1) = -4 + 15 - 2/3 = 11 - 2/3 = \\frac{31}{3}\n$$\nAs expected for a Fisher matrix, $F_{12} = F_{21}$.\n$$\nF_{22} = (-5)(-0.2) + (30)(0.3) + (20/3)(0.6) = 1 + 9 + 12/3 = 10 + 4 = 14\n$$\nSo, the Fisher information matrix is:\n$$\nF = \\begin{pmatrix}\n370/9 & 31/3 \\\\\n31/3 & 14\n\\end{pmatrix}\n$$\nThe D-optimality measure is the determinant of $F$, $\\det(F)$.\n$$\n\\det(F) = F_{11} F_{22} - F_{12} F_{21} = \\left(\\frac{370}{9}\\right)(14) - \\left(\\frac{31}{3}\\right)^2\n$$\n$$\n\\det(F) = \\frac{370 \\times 14}{9} - \\frac{31^2}{9} = \\frac{5180}{9} - \\frac{961}{9} = \\frac{5180 - 961}{9} = \\frac{4219}{9}\n$$\nTo get the final numerical value, we perform the division:\n$$\n\\det(F) = \\frac{4219}{9} \\approx 468.777...\n$$\nRounding to four significant figures, we get $468.8$.", "answer": "$$\n\\boxed{468.8}\n$$", "id": "3544124"}, {"introduction": "The ultimate goal of theoretical physics is often not just to fit parameters, but to decide which theory provides a better explanation of nature. This exercise advances from parameter estimation within a single model to the powerful framework of Bayesian model selection [@problem_id:3544169]. You will implement a robust algorithm to compute the Bayes factor, a quantity that represents the evidence for one model over another, by calculating the marginal likelihood. This practice bridges fundamental Bayesian principles with the practical computational skills needed to compare different theoretical schemes, such as competing truncation orders in an Effective Field Theory.", "problem": "Consider a linearized surrogate for an observable in chiral Effective Field Theory (chiral EFT) calibration at truncation order $n$, where $m$ measurements collected in the vector $y \\in \\mathbb{R}^m$ are modeled as $y \\approx X \\theta + \\varepsilon_{\\mathrm{exp}} + \\varepsilon_{\\mathrm{tr}}$. Here, $X \\in \\mathbb{R}^{m \\times d}$ is the design matrix of sensitivities with respect to $d$ low-energy constants (LECs) $\\theta \\in \\mathbb{R}^d$. The experimental noise $\\varepsilon_{\\mathrm{exp}}$ is zero-mean Gaussian with diagonal covariance $\\Sigma_{\\mathrm{exp}} = \\mathrm{diag}(s_1^2,\\dots,s_m^2)$, and the truncation error $\\varepsilon_{\\mathrm{tr}}$ is modeled as a zero-mean Gaussian with order-dependent variance $\\sigma_n^2 I_m$, where $\\sigma_n^2 = c^2 Q^{2(n+1)}$, with $c>0$ a scheme-dependent scale and $Q \\in (0,1)$ an expansion parameter. Assume a Gaussian prior for the LECs, $\\theta \\sim \\mathcal{N}(0,\\tau^2 I_d)$ with $\\tau^2>0$. Under these assumptions, the marginal likelihood (Bayesian evidence) for a model $\\mathcal{M}$ that specifies $(n,Q,c)$ is given by integrating out $\\theta$ from the Gaussian likelihood and prior. The Bayes factor between two truncation schemes $\\mathcal{M}_A$ and $\\mathcal{M}_B$ is $B_{AB} = \\dfrac{p(y \\mid \\mathcal{M}_A)}{p(y \\mid \\mathcal{M}_B)}$. Use fundamental Bayesian principles (Bayes’ theorem, Gaussian integrals, and linear-Gaussian models) to derive an algorithm that, given $(y,X,\\{s_i^2\\}_{i=1}^m,\\tau^2)$ and the two scheme specifications $(n,Q_A,c_A)$ and $(n,Q_B,c_B)$, computes the natural logarithm of the Bayes factor, $\\ln B_{AB}$, exactly and robustly using linear algebra.\n\nYour task is to implement a program that computes $\\ln B_{AB}$ for each of the following test cases. All quantities are dimensionless, and no physical units are needed. You must use the following well-tested mathematical facts as the foundational base: (i) the distributional properties of multivariate Gaussian integrals, (ii) the rule for the sum of independent Gaussian random variables, (iii) the linear transformation of Gaussian variables, and (iv) the definition of the Bayes factor as the ratio of marginal likelihoods. Do not assume any shortcut formulas beyond those that follow from these principles.\n\nFor each test case, the model evidence must be constructed from the marginal $y \\sim \\mathcal{N}(0, C)$ with covariance $C = \\Sigma_{\\mathrm{exp}} + \\sigma_n^2 I_m + X (\\tau^2 I_d) X^\\top$, and the natural logarithm of the evidence must be evaluated using a numerically stable method based on the Cholesky factorization of $C$.\n\nInput specifications for the test suite:\n- Case $1$ (happy path): $m = 3$, $d = 2$, $y = [1.05, 0.95, 1.10]$, $X = \\begin{bmatrix}1.0 & 0.2 \\\\ 1.0 & -0.1 \\\\ 1.0 & 0.3\\end{bmatrix}$, experimental variances $\\{s_i^2\\} = [0.0025, 0.0025, 0.0025]$, $\\tau^2 = 1.0$, $n = 2$, $(Q_A,c_A) = (0.3, 1.0)$, $(Q_B,c_B) = (0.5, 1.0)$.\n- Case $2$ (low-$Q$ edge): $m = 2$, $d = 1$, $y = [0.02, -0.01]$, $X = \\begin{bmatrix}1.0 \\\\ 1.0\\end{bmatrix}$, experimental variances $\\{s_i^2\\} = [0.0001, 0.0001]$, $\\tau^2 = 0.1$, $n = 3$, $(Q_A,c_A) = (0.05, 1.0)$, $(Q_B,c_B) = (0.1, 1.0)$.\n- Case $3$ (high-uncertainty regime): $m = 4$, $d = 3$, $y = [0.5, 0.7, 0.6, 0.9]$, $X = \\begin{bmatrix}1.0 & 0.3 & -0.2 \\\\ 1.0 & -0.1 & 0.4 \\\\ 1.0 & 0.2 & 0.1 \\\\ 1.0 & -0.3 & 0.2\\end{bmatrix}$, experimental variances $\\{s_i^2\\} = [0.04, 0.04, 0.01, 0.09]$, $\\tau^2 = 0.5$, $n = 1$, $(Q_A,c_A) = (0.6, 1.0)$, $(Q_B,c_B) = (0.8, 1.0)$.\n- Case $4$ (different prefactors at fixed $Q$): $m = 3$, $d = 1$, $y = [2.0, 1.8, 2.2]$, $X = \\begin{bmatrix}1.0 \\\\ 0.9 \\\\ 1.1\\end{bmatrix}$, experimental variances $\\{s_i^2\\} = [0.25, 0.16, 0.36]$, $\\tau^2 = 2.0$, $n = 0$, $(Q_A,c_A) = (0.7, 1.0)$, $(Q_B,c_B) = (0.7, 2.0)$.\n\nProgram requirements:\n- Implement the computation of $\\sigma_n^2 = c^2 Q^{2(n+1)}$ for each scheme.\n- Construct $C = \\Sigma_{\\mathrm{exp}} + \\sigma_n^2 I_m + X (\\tau^2 I_d) X^\\top$ and evaluate $\\ln p(y \\mid \\mathcal{M})$ using a Cholesky factorization of $C$ in the numerically stable closed-form for a zero-mean Gaussian: $\\ln p(y \\mid \\mathcal{M}) = -\\tfrac{1}{2}\\left( m \\ln(2\\pi) + \\ln \\det C + y^\\top C^{-1} y \\right)$.\n- Return $\\ln B_{AB} = \\ln p(y \\mid \\mathcal{M}_A) - \\ln p(y \\mid \\mathcal{M}_B)$ for each case.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_i$ is the value of $\\ln B_{AB}$ for the $i$-th test case rounded to six decimal places.", "solution": "The problem requires the computation of the natural logarithm of the Bayes factor, $\\ln B_{AB}$, between two competing theoretical models, $\\mathcal{M}_A$ and $\\mathcal{M}_B$, for the calibration of a chiral Effective Field Theory (EFT). The solution must be derived from fundamental principles of Bayesian statistics and implemented using numerically robust linear algebra techniques.\n\nThe statistical model for the $m$ observables, collected in the vector $y \\in \\mathbb{R}^m$, is given by the linear relation:\n$$\ny \\approx X \\theta + \\varepsilon_{\\mathrm{exp}} + \\varepsilon_{\\mathrm{tr}}\n$$\nHere, $X \\in \\mathbb{R}^{m \\times d}$ is the design matrix representing the sensitivity of the observables to the $d$ low-energy constants (LECs) contained in the vector $\\theta \\in \\mathbb{R}^d$. The model components are defined by the following distributional assumptions:\n1.  The prior distribution for the LECs is a zero-mean Gaussian: $p(\\theta) = \\mathcal{N}(\\theta \\mid 0, \\tau^2 I_d)$, where $\\tau^2 > 0$ is the prior variance and $I_d$ is the $d \\times d$ identity matrix.\n2.  The experimental error, $\\varepsilon_{\\mathrm{exp}}$, is modeled as a zero-mean Gaussian with a diagonal covariance matrix: $\\varepsilon_{\\mathrm{exp}} \\sim \\mathcal{N}(0, \\Sigma_{\\mathrm{exp}})$, where $\\Sigma_{\\mathrm{exp}} = \\mathrm{diag}(s_1^2, \\dots, s_m^2)$.\n3.  The theory truncation error, $\\varepsilon_{\\mathrm{tr}}$, is modeled as a zero-mean Gaussian with a covariance matrix $\\sigma_n^2 I_m$, where $I_m$ is the $m \\times m$ identity matrix. The variance $\\sigma_n^2$ depends on the EFT truncation order $n$, an expansion parameter $Q \\in (0,1)$, and a scale parameter $c>0$, according to the formula $\\sigma_n^2 = c^2 Q^{2(n+1)}$.\n\nA specific model, denoted $\\mathcal{M}$, is defined by the triplet $(n, Q, c)$. The Bayes factor $B_{AB}$ is the ratio of the marginal likelihoods (or evidences) of two models, $\\mathcal{M}_A = (n, Q_A, c_A)$ and $\\mathcal{M}_B = (n, Q_B, c_B)$:\n$$\nB_{AB} = \\frac{p(y \\mid \\mathcal{M}_A)}{p(y \\mid \\mathcal{M}_B)}\n$$\nOur objective is to compute its natural logarithm, $\\ln B_{AB} = \\ln p(y \\mid \\mathcal{M}_A) - \\ln p(y \\mid \\mathcal{M}_B)$.\n\nThe derivation proceeds by first determining the marginal likelihood $p(y \\mid \\mathcal{M})$. This is accomplished by integrating out the parameter vector $\\theta$ from the joint distribution:\n$$\np(y \\mid \\mathcal{M}) = \\int p(y \\mid \\theta, \\mathcal{M}) p(\\theta \\mid \\mathcal{M}) \\, d\\theta\n$$\nGiven the model structure, the vector $y$ can be expressed as the sum of three independent random vectors: $X\\theta$, $\\varepsilon_{\\mathrm{exp}}$, and $\\varepsilon_{\\mathrm{tr}}$. According to the properties of Gaussian distributions, a linear transformation of a Gaussian variable is also Gaussian. Thus, the term $X\\theta$ is a zero-mean Gaussian random vector with mean $\\mathbb{E}[X\\theta] = X \\mathbb{E}[\\theta] = 0$ and covariance $\\mathrm{Cov}(X\\theta) = X \\mathrm{Cov}(\\theta) X^\\top = X(\\tau^2 I_d)X^\\top = \\tau^2 X X^\\top$.\n\nThe sum of independent Gaussian random vectors is also a Gaussian random vector. The mean of the sum is the sum of the means, and the covariance of the sum is the sum of the covariances. Therefore, the marginal distribution of $y$ is Gaussian, $y \\sim \\mathcal{N}(0, C)$, with a total covariance matrix $C$ given by:\n$$\nC = \\mathrm{Cov}(X\\theta) + \\mathrm{Cov}(\\varepsilon_{\\mathrm{exp}}) + \\mathrm{Cov}(\\varepsilon_{\\mathrm{tr}}) = \\tau^2 X X^\\top + \\Sigma_{\\mathrm{exp}} + \\sigma_n^2 I_m\n$$\nThis derivation validates the problem's stated marginal data distribution. The probability density function for this zero-mean multivariate Gaussian distribution is:\n$$\np(y \\mid \\mathcal{M}) = \\frac{1}{\\sqrt{(2\\pi)^m \\det(C)}} \\exp\\left(-\\frac{1}{2} y^\\top C^{-1} y\\right)\n$$\nTaking the natural logarithm yields the log-marginal likelihood:\n$$\n\\ln p(y \\mid \\mathcal{M}) = -\\frac{1}{2} \\left( m \\ln(2\\pi) + \\ln(\\det(C)) + y^\\top C^{-1} y \\right)\n$$\nTo compute this quantity in a numerically stable manner, we avoid direct computation of the inverse $C^{-1}$ and the determinant $\\det(C)$, which are prone to numerical underflow/overflow and inaccuracies. The covariance matrix $C$ is symmetric and positive definite, as it is a sum of positive semi-definite and positive definite matrices. It therefore admits a unique Cholesky factorization $C = L L^\\top$, where $L$ is a lower triangular matrix with positive diagonal entries.\n\nThis factorization allows for stable computation of the required terms:\n1.  The log-determinant term: $\\ln(\\det(C)) = \\ln(\\det(L L^\\top)) = \\ln(\\det(L)^2) = 2 \\ln(\\det(L))$. Since the determinant of a triangular matrix is the product of its diagonal elements, $\\det(L) = \\prod_{i=1}^m L_{ii}$, we have $\\ln(\\det(C)) = 2 \\sum_{i=1}^m \\ln(L_{ii})$. This approach prevents the numerical issues associated with computing the product of many small or large numbers.\n2.  The quadratic form term $y^\\top C^{-1} y$: We can write this as $y^\\top (L L^\\top)^{-1} y = y^\\top (L^\\top)^{-1} L^{-1} y$. Let $z = L^{-1} y$. The quadratic form becomes $z^\\top z = \\|z\\|_2^2$. The vector $z$ is found by solving the lower triangular system $L z = y$ for $z$ via forward substitution, a computationally efficient and stable operation.\n\nThe algorithm to compute the log-evidence for a single model $\\mathcal{M}$ is as follows:\n1.  Given model parameters $(n,Q,c)$, compute the truncation variance $\\sigma_n^2 = c^2 Q^{2(n+1)}$.\n2.  Construct the total covariance matrix $C = \\tau^2 X X^\\top + \\mathrm{diag}(\\{s_i^2\\}) + \\sigma_n^2 I_m$.\n3.  Compute the Cholesky factor $L$ such that $C = L L^\\top$.\n4.  Calculate the log-determinant as $2 \\sum_i \\ln(L_{ii})$.\n5.  Solve $L z = y$ for $z$ using forward substitution.\n6.  Calculate the quadratic form as $z^\\top z$.\n7.  Combine these terms to find $\\ln p(y \\mid \\mathcal{M})$.\n\nTo find the log-Bayes factor $\\ln B_{AB}$, this algorithm is executed for both model $\\mathcal{M}_A$ (with parameters $(Q_A, c_A)$) and model $\\mathcal{M}_B$ (with parameters $(Q_B, c_B)$), and the results are subtracted:\n$$\n\\ln B_{AB} = \\ln p(y \\mid \\mathcal{M}_A) - \\ln p(y \\mid \\mathcal{M}_B)\n$$\nThis procedure is implemented for each provided test case to yield the final results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases specified.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"y\": np.array([1.05, 0.95, 1.10]),\n            \"X\": np.array([[1.0, 0.2], [1.0, -0.1], [1.0, 0.3]]),\n            \"s_sq\": np.array([0.0025, 0.0025, 0.0025]),\n            \"tau_sq\": 1.0,\n            \"n\": 2,\n            \"model_A\": {\"Q\": 0.3, \"c\": 1.0},\n            \"model_B\": {\"Q\": 0.5, \"c\": 1.0},\n        },\n        # Case 2 (low-Q edge)\n        {\n            \"y\": np.array([0.02, -0.01]),\n            \"X\": np.array([[1.0], [1.0]]),\n            \"s_sq\": np.array([0.0001, 0.0001]),\n            \"tau_sq\": 0.1,\n            \"n\": 3,\n            \"model_A\": {\"Q\": 0.05, \"c\": 1.0},\n            \"model_B\": {\"Q\": 0.1, \"c\": 1.0},\n        },\n        # Case 3 (high-uncertainty regime)\n        {\n            \"y\": np.array([0.5, 0.7, 0.6, 0.9]),\n            \"X\": np.array([[1.0, 0.3, -0.2], [1.0, -0.1, 0.4], [1.0, 0.2, 0.1], [1.0, -0.3, 0.2]]),\n            \"s_sq\": np.array([0.04, 0.04, 0.01, 0.09]),\n            \"tau_sq\": 0.5,\n            \"n\": 1,\n            \"model_A\": {\"Q\": 0.6, \"c\": 1.0},\n            \"model_B\": {\"Q\": 0.8, \"c\": 1.0},\n        },\n        # Case 4 (different prefactors at fixed Q)\n        {\n            \"y\": np.array([2.0, 1.8, 2.2]),\n            \"X\": np.array([[1.0], [0.9], [1.1]]),\n            \"s_sq\": np.array([0.25, 0.16, 0.36]),\n            \"tau_sq\": 2.0,\n            \"n\": 0,\n            \"model_A\": {\"Q\": 0.7, \"c\": 1.0},\n            \"model_B\": {\"Q\": 0.7, \"c\": 2.0},\n        }\n    ]\n\n    def compute_log_evidence(y, X, s_sq, tau_sq, n, Q, c):\n        \"\"\"\n        Computes the log-marginal likelihood for a given model.\n        \n        Args:\n            y (np.ndarray): Measurement vector.\n            X (np.ndarray): Design matrix.\n            s_sq (np.ndarray): Vector of experimental variances.\n            tau_sq (float): Prior variance for LECs.\n            n (int): Truncation order.\n            Q (float): Expansion parameter.\n            c (float): Scheme-dependent scale.\n        \n        Returns:\n            float: The natural logarithm of the marginal likelihood.\n        \"\"\"\n        m = X.shape[0]\n\n        # 1. Calculate truncation error variance\n        sigma_n_sq = c**2 * Q**(2 * (n + 1))\n\n        # 2. Construct the total covariance matrix C\n        Sigma_exp = np.diag(s_sq)\n        C = tau_sq * (X @ X.T) + Sigma_exp + sigma_n_sq * np.identity(m)\n\n        # 3. Compute Cholesky factorization: C = L L^T\n        try:\n            L = linalg.cholesky(C, lower=True)\n        except linalg.LinAlgError:\n            # Fallback for numerically challenging cases, though not expected here.\n            # This would indicate the matrix is not positive definite.\n            return -np.inf\n\n        # 4. Calculate log-determinant: log(det(C)) = 2 * sum(log(diag(L)))\n        log_det_C = 2 * np.sum(np.log(np.diag(L)))\n\n        # 5. Solve L z = y for z using forward substitution\n        z = linalg.solve_triangular(L, y, lower=True)\n\n        # 6. Calculate quadratic form: y^T C^{-1} y = z^T z\n        quad_form = np.dot(z, z)\n\n        # 7. Combine terms for the log-evidence\n        log_p = -0.5 * (m * np.log(2 * np.pi) + log_det_C + quad_form)\n        \n        return log_p\n\n    results = []\n    for case in test_cases:\n        y = case[\"y\"]\n        X = case[\"X\"]\n        s_sq = case[\"s_sq\"]\n        tau_sq = case[\"tau_sq\"]\n        n = case[\"n\"]\n        \n        # Unpack model parameters\n        params_A = case[\"model_A\"]\n        params_B = case[\"model_B\"]\n\n        # Compute log-evidence for model A\n        log_p_A = compute_log_evidence(y, X, s_sq, tau_sq, n, params_A[\"Q\"], params_A[\"c\"])\n        \n        # Compute log-evidence for model B\n        log_p_B = compute_log_evidence(y, X, s_sq, tau_sq, n, params_B[\"Q\"], params_B[\"c\"])\n        \n        # Compute log Bayes factor\n        log_B_AB = log_p_A - log_p_B\n        \n        results.append(round(log_B_AB, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3544169"}]}