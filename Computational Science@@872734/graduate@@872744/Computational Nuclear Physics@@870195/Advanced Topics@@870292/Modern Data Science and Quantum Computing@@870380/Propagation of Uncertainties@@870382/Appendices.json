{"hands_on_practices": [{"introduction": "The cornerstone of analytical uncertainty propagation lies in the Taylor series expansion. This practice provides a foundational exercise where we apply this method to a simple, nonlinear function. By comparing the first and second-order approximations for the mean and variance to their exact, analytically derived values, you will gain a concrete understanding of how Taylor-based propagation works and quantify the approximation error introduced by linearization [@problem_id:3581693]. This serves as an essential baseline for appreciating the strengths and limitations of the method.", "problem": "In uncertainty quantification for computational nuclear physics, it is common to propagate uncertainties from model parameters to derived observables. Consider a simplified surrogate observable defined by $y=f(x)=x^{2}$, where the scalar parameter $x$ represents a dimensionless calibrated scaling factor in a transport response model and is modeled as a Gaussian random variable $x \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. Let $\\mu=1.2$ and $\\sigma=0.15$. \n\nStarting from the definitions of expectation and variance, and using the known moments of the Gaussian distribution, derive the exact expressions for the mean $\\mathbb{E}[y]$ and variance $\\mathrm{Var}(y)$ of $y$. Then, perform uncertainty propagation using a Taylor expansion of $f$ around $x=\\mu$:\n\n- First-order propagation: retain the linear term in the Taylor expansion about $\\mu$.\n- Second-order propagation: retain terms up to the quadratic term in the Taylor expansion about $\\mu$.\n\nFrom these two approximations, obtain the approximate mean and variance of $y$ in each case. To quantify approximation quality, define the dimensionless relative errors (for mean and for variance) as\n$$\\epsilon_{\\text{mean}}=\\frac{|\\mathbb{E}_{\\text{approx}}[y]-\\mathbb{E}[y]|}{\\mathbb{E}[y]}, \\quad \\epsilon_{\\text{var}}=\\frac{|\\mathrm{Var}_{\\text{approx}}(y)-\\mathrm{Var}(y)|}{\\mathrm{Var}(y)}.$$\n\nCompute the four relative errors corresponding to the first-order and second-order approximations for the mean and for the variance, respectively, using the given numerical values of $\\mu$ and $\\sigma$. Report your final answer as a single row vector containing these four numbers in the order\n$$\\big(\\epsilon_{\\text{mean, 1st}},\\,\\epsilon_{\\text{mean, 2nd}},\\,\\epsilon_{\\text{var, 1st}},\\,\\epsilon_{\\text{var, 2nd}}\\big),$$\nrounded to six significant figures. The final answer is dimensionless.", "solution": "The user wants me to solve a problem in uncertainty propagation. Here's my plan:\n\n1.  **Validate the Problem:**\n    *   **Extract Givens:** List all the provided information: the function `y = f(x) = x^2`, the distribution of `x` as a Gaussian `x ~ N(μ, σ^2)`, the values `μ = 1.2` and `σ = 0.15`, the definitions of the relative errors `ε_mean` and `ε_var`, and the required output format.\n    *   **Validate:** Check for scientific soundness, well-posedness, objectivity, completeness, etc.\n        *   **Scientific Grounding:** The problem deals with standard probability theory and statistics (moments of a random variable, Taylor expansion for uncertainty propagation). This is a well-established method in applied mathematics and physics. The context of \"computational nuclear physics\" is a framing device, but the core problem is a standard statistical calculation. It is scientifically sound.\n        *   **Well-Posed:** The problem is well-defined. It asks for specific calculations (exact moments, approximate moments using Taylor series) and a comparison. A unique solution exists.\n        *   **Objective:** The language is precise and mathematical. No subjective claims.\n        *   **Completeness:** All necessary information (`f(x)`, distribution of `x`, parameters `μ`, `σ`, error formulas) is provided.\n        *   **Realism:** The values `μ = 1.2` and `σ = 0.15` are reasonable for a scaling factor.\n        *   **Other Flaws:** The problem is not trivial, not tautological, and is verifiable.\n\n    *   **Verdict:** The problem is valid.\n\n2.  **Solve the Problem (if valid):**\n\n    *   **Part 1: Exact Mean and Variance**\n        *   The observable is `y = x^2`.\n        *   `x` is a Gaussian random variable: `x ~ N(μ, σ^2)`.\n        *   **Exact Mean `E[y]`:**\n            *   `E[y] = E[x^2]`.\n            *   I know the relationship between variance, mean, and the second moment: `Var(x) = E[x^2] - (E[x])^2`.\n            *   For a Gaussian `x ~ N(μ, σ^2)`, we have `E[x] = μ` and `Var(x) = σ^2`.\n            *   Therefore, `E[x^2] = Var(x) + (E[x])^2 = σ^2 + μ^2`.\n            *   So, the exact mean is `E[y] = μ^2 + σ^2`.\n\n        *   **Exact Variance `Var(y)`:**\n            *   `Var(y) = Var(x^2) = E[(x^2)^2] - (E[x^2])^2 = E[x^4] - (μ^2 + σ^2)^2`.\n            *   I need the fourth moment of a Gaussian distribution, `E[x^4]`.\n            *   The moments of a centered Gaussian `z ~ N(0, σ^2)` are: `E[z^n] = 0` for `n` odd, and `E[z^n] = (n-1)!! σ^n` for `n` even.\n            *   Let `x = μ + z`, where `z ~ N(0, σ^2)`.\n            *   `E[x^4] = E[(μ + z)^4] = E[μ^4 + 4μ^3z + 6μ^2z^2 + 4μz^3 + z^4]`.\n            *   Using linearity of expectation: `E[x^4] = μ^4 + 4μ^3E[z] + 6μ^2E[z^2] + 4μE[z^3] + E[z^4]`.\n            *   I know `E[z] = 0`, `E[z^2] = σ^2`, `E[z^3] = 0`, and `E[z^4] = 3!! σ^4 = 3σ^4`.\n            *   So, `E[x^4] = μ^4 + 6μ^2σ^2 + 3σ^4`.\n            *   Now, substitute this back into the variance formula:\n                `Var(y) = (μ^4 + 6μ^2σ^2 + 3σ^4) - (μ^2 + σ^2)^2`\n                `Var(y) = (μ^4 + 6μ^2σ^2 + 3σ^4) - (μ^4 + 2μ^2σ^2 + σ^4)`\n                `Var(y) = 4μ^2σ^2 + 2σ^4`.\n            *   So, the exact variance is `Var(y) = 2σ^2(2μ^2 + σ^2)`.\n\n    *   **Part 2: First-Order Approximation**\n        *   Taylor expand `f(x) = x^2` around `x = μ`: `f(x) ≈ f(μ) + f'(μ)(x - μ)`.\n        *   `f(μ) = μ^2`.\n        *   `f'(x) = 2x`, so `f'(μ) = 2μ`.\n        *   The first-order approximation of `y` is `y_{1} = μ^2 + 2μ(x - μ)`.\n        *   **Approximate Mean `E_approx[y]` (1st order):**\n            *   `E[y_1] = E[μ^2 + 2μ(x - μ)] = μ^2 + 2μE[x - μ]`.\n            *   Since `E[x] = μ`, then `E[x - μ] = E[x] - μ = 0`.\n            *   So, `E_{approx, 1st}[y] = μ^2`.\n        *   **Approximate Variance `Var_approx(y)` (1st order):**\n            *   `Var(y_1) = Var(μ^2 + 2μ(x - μ))`.\n            *   Using variance properties `Var(a + bX) = b^2 Var(X)`:\n            *   `Var(y_1) = Var(2μx - 2μ^2 + μ^2) = Var(2μx - μ^2) = (2μ)^2 Var(x) = 4μ^2σ^2`.\n            *   So, `Var_{approx, 1st}(y) = 4μ^2σ^2`.\n\n    *   **Part 3: Second-Order Approximation**\n        *   Taylor expand `f(x) = x^2` around `x = μ` up to the second term: `f(x) ≈ f(μ) + f'(μ)(x - μ) + (1/2)f''(μ)(x - μ)^2`.\n        *   `f''(x) = 2`, so `f''(μ) = 2`.\n        *   The second-order approximation of `y` is `y_{2} = μ^2 + 2μ(x - μ) + (1/2)(2)(x - μ)^2 = μ^2 + 2μ(x - μ) + (x - μ)^2`.\n        *   **Approximate Mean `E_approx[y]` (2nd order):**\n            *   `E[y_2] = E[μ^2 + 2μ(x - μ) + (x - μ)^2]`.\n            *   `E[y_2] = μ^2 + 2μE[x - μ] + E[(x - μ)^2]`.\n            *   We know `E[x - μ] = 0` and `E[(x - μ)^2]` is the definition of variance, `Var(x) = σ^2`.\n            *   So, `E_{approx, 2nd}[y] = μ^2 + σ^2`.\n            *   Notice this is the same as the exact mean.\n\n        *   **Approximate Variance `Var_approx(y)` (2nd order):**\n            *   Let's simplify `y_2`: `y_2 = μ^2 + 2μx - 2μ^2 + x^2 - 2μx + μ^2 = x^2`.\n            *   The second-order Taylor expansion of `f(x) = x^2` is *exactly* `f(x)`. This is because `f(x)` is a quadratic polynomial, and its Taylor series of order 2 (or higher) is the polynomial itself. All higher derivatives are zero.\n            *   Therefore, the \"approximation\" `y_2` is actually the exact function `y`.\n            *   `y_2 = x^2`.\n            *   `E_{approx, 2nd}[y] = E[x^2] = μ^2 + σ^2`. (Matches my previous calculation).\n            *   `Var_{approx, 2nd}(y) = Var(x^2) = 4μ^2σ^2 + 2σ^4`. (Matches the exact variance).\n            *   This is a special case. The Taylor expansion is exact.\n\n    *   **Part 4: Calculate Relative Errors**\n        *   **Given values:** `μ = 1.2`, `σ = 0.15`.\n        *   `μ^2 = (1.2)^2 = 1.44`.\n        *   `σ^2 = (0.15)^2 = 0.0225`.\n\n        *   **Exact Values:**\n            *   `E[y] = μ^2 + σ^2 = 1.44 + 0.0225 = 1.4625`.\n            *   `Var(y) = 4μ^2σ^2 + 2σ^4 = 4(1.44)(0.0225) + 2(0.0225)^2`.\n                *   `4 * 1.44 * 0.0225 = 0.1296`.\n                *   `2 * (0.0225)^2 = 2 * 0.00050625 = 0.0010125`.\n                *   `Var(y) = 0.1296 + 0.0010125 = 0.1306125`.\n\n        *   **Approximations (1st order):**\n            *   `E_{approx, 1st}[y] = μ^2 = 1.44`.\n            *   `Var_{approx, 1st}(y) = 4μ^2σ^2 = 4(1.44)(0.0225) = 0.1296`.\n\n        *   **Approximations (2nd order):**\n            *   `E_{approx, 2nd}[y] = μ^2 + σ^2 = 1.44 + 0.0225 = 1.4625`. (This is exact).\n            *   `Var_{approx, 2nd}(y) = 4μ^2σ^2 + 2σ^4 = 0.1306125`. (This is exact).\n\n        *   **Error Calculations:**\n            *   `ε_mean, 1st`:\n                `|E_{approx, 1st}[y] - E[y]| / E[y] = |1.44 - 1.4625| / 1.4625 = |-0.0225| / 1.4625 = 0.0225 / 1.4625`.\n                `0.0225 / 1.4625 = 0.015384615...`\n                Rounding to six significant figures: `1.53846 \\times 10^{-2}`.\n\n            *   `ε_mean, 2nd`:\n                `|E_{approx, 2nd}[y] - E[y]| / E[y] = |1.4625 - 1.4625| / 1.4625 = 0 / 1.4625 = 0`.\n                Expressed with six significant figures, this is `0.000000`.\n\n            *   `ε_var, 1st`:\n                `|Var_{approx, 1st}(y) - Var(y)| / Var(y) = |0.1296 - 0.1306125| / 0.1306125 = |-0.0010125| / 0.1306125 = 0.0010125 / 0.1306125`.\n                `0.0010125 / 0.1306125 = 0.0077519379...`\n                Rounding to six significant figures: `7.75194 \\times 10^{-3}`.\n\n            *   `ε_var, 2nd`:\n                `|Var_{approx, 2nd}(y) - Var(y)| / Var(y) = |0.1306125 - 0.1306125| / 0.1306125 = 0 / 0.1306125 = 0`.\n                Expressed with six significant figures, this is `0.000000`.\n\n    *   **Final Answer Formatting:**\n        *   A single row vector: `(ε_mean, 1st, ε_mean, 2nd, ε_var, 1st, ε_var, 2nd)`.\n        *   Order is correct.\n        *   Values rounded to six significant figures.\n        *   `ε_mean, 1st = 0.0153846`\n        *   `ε_mean, 2nd = 0.000000`\n        *   `ε_var, 1st = 0.00775194`\n        *   `ε_var, 2nd = 0.000000`\nThe entire logic is sound. I will now write the final solution.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.0153846  0.000000  0.00775194  0.000000\n\\end{pmatrix}\n}\n$$", "id": "3581693"}, {"introduction": "In real-world computational physics, models are often too complex for exact analytical solutions, and their nonlinearity is not known a priori. This exercise transitions from pure theory to practical diagnostics, addressing a critical question: when is a Taylor-based approximation good enough? You will implement a numerical diagnostic to assess the impact of model curvature on propagated uncertainties, helping you decide whether a second-order approximation is adequate or if more robust, sampling-based methods are necessary [@problem_id:3581708]. This practice is vital for developing the judgment required in applied uncertainty quantification.", "problem": "Consider an observable from computational nuclear physics defined as a Maxwellian-averaged single-resonance contribution to a reaction rate. Let the scalar observable be the integral $$R(A,E_r,\\Gamma;kT) = \\int_{0}^{\\infty} y e^{-y} \\frac{A}{\\big(kT\\,y - E_r\\big)^2 + \\big(\\Gamma/2\\big)^2}\\,dy,$$ where $A$ is a scale parameter (with units of barns), $E_r$ is the resonance energy (in megaelectronvolts), $\\Gamma$ is the total resonance width (in megaelectronvolts), and $kT$ is the thermal energy (in megaelectronvolts). The weighting function $y e^{-y}$ arises from the Maxwell-Boltzmann spectrum after the change of variables $y = E/(kT)$, which preserves dimensional consistency and ensures that $R$ is a physically meaningful rate-like observable.\n\nAssume that the parameter vector $$\\boldsymbol{\\theta} = \\begin{bmatrix} A \\\\ E_r \\\\ \\Gamma \\end{bmatrix}$$ is modeled as a multivariate normal random variable with mean $$\\boldsymbol{\\mu} = \\begin{bmatrix} \\mu_A \\\\ \\mu_{E_r} \\\\ \\mu_{\\Gamma} \\end{bmatrix}$$ and covariance matrix $$\\mathbf{C} \\in \\mathbb{R}^{3\\times 3}.$$ The task is to implement a diagnostic for propagation of uncertainties that determines, for each specified test case, whether higher-order (second-order Taylor) propagation is warranted or whether Monte Carlo (random sampling) propagation is necessary due to nonlinearity.\n\nStarting from the definitions of the Taylor expansion of a scalar function of multiple variables and the properties of a multivariate normal distribution, you must:\n- Derive and implement numerical approximations for the gradient $$\\mathbf{g} = \\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\mu};kT)$$ and the Hessian $$\\mathbf{H} = \\nabla^2_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\mu};kT)$$ using central finite differences. Use a dimension-wise step $$h_i = \\varepsilon \\max\\big(|\\mu_i|, 1\\big)$$ with $$\\varepsilon = 10^{-4}.$$\n- Using the quantities $$\\mathbf{g}$$ and $$\\mathbf{H}$$ along with the covariance $$\\mathbf{C},$$ compute:\n  1. The leading (linear) variance contribution $$V_1,$$ attributable to the gradient and covariance.\n  2. The leading curvature-induced (second-order) variance correction $$V_2,$$ attributable to the Hessian and covariance.\n  3. The curvature-induced mean shift (bias) $$b_2,$$ attributable to the Hessian and covariance.\n- Define the diagnostic metrics:\n  - The curvature fraction $$\\kappa_{\\mathrm{var}} = V_2 / V_1.$$\n  - The normalized bias $$\\kappa_{\\mathrm{bias}} = |b_2| / \\sqrt{V_1}.$$\n- Decision rule: For each test case, decide that higher-order (second-order Taylor) propagation is warranted if both $$\\kappa_{\\mathrm{var}} \\leq 0.25$$ and $$\\kappa_{\\mathrm{bias}} \\leq 0.25;$$ otherwise decide that Monte Carlo (random sampling) propagation is warranted. Report the decision as an integer $$0$$ for higher-order propagation and $$1$$ for Monte Carlo propagation.\n\nImplement the integral exactly as given, with numerical quadrature over $$y \\in [0,\\infty).$$ Angles do not appear in this problem; no angle units are required. All energies must be understood in megaelectronvolts, and the amplitude $$A$$ must be understood in barns; however, the required outputs are unitless integers, so you must report only the decisions for each test case.\n\nTest suite:\n- Case $1$ (Nominal case, mild curvature):\n  - $$kT = 0.5.$$\n  - $$\\boldsymbol{\\mu} = [5.0,\\,1.5,\\,0.4].$$\n  - $$\\mathbf{C} = \\mathrm{diag}\\big([0.2^2,\\,0.05^2,\\,0.04^2]\\big) = \\mathrm{diag}([0.04,\\,0.0025,\\,0.0016]).$$\n- Case $2$ (near-resonance, moderate curvature, weak correlation):\n  - $$kT = 0.5.$$\n  - $$\\boldsymbol{\\mu} = [5.0,\\,0.7,\\,0.2].$$\n  - Standard deviations $$[0.5,\\,0.07,\\,0.02],$$ correlation between $$E_r$$ and $$\\Gamma$$ equal to $$0.3.$$ Therefore $$\\mathbf{C}$$ has diagonal $$[0.25,\\,0.0049,\\,0.0004]$$ and off-diagonal entries $$C_{E_r,\\Gamma} = C_{\\Gamma,E_r} = 0.3 \\cdot 0.07 \\cdot 0.02 = 0.00042,$$ other off-diagonals zero.\n- Case $3$ (stronger curvature, strong correlation):\n  - $$kT = 0.4.$$\n  - $$\\boldsymbol{\\mu} = [5.0,\\,0.52,\\,0.05].$$\n  - Standard deviations $$[1.0,\\,0.08,\\,0.02],$$ correlation between $$E_r$$ and $$\\Gamma$$ equal to $$0.8.$$ Therefore $$\\mathbf{C}$$ has diagonal $$[1.0,\\,0.0064,\\,0.0004]$$ and off-diagonal entries $$C_{E_r,\\Gamma} = C_{\\Gamma,E_r} = 0.8 \\cdot 0.08 \\cdot 0.02 = 0.00128,$$ other off-diagonals zero.\n- Case $4$ (narrow width at resonance, nonlinearity edge, negative correlation):\n  - $$kT = 0.6.$$\n  - $$\\boldsymbol{\\mu} = [5.0,\\,0.5,\\,0.02].$$\n  - Standard deviations $$[0.1,\\,0.005,\\,0.004],$$ correlation between $$E_r$$ and $$\\Gamma$$ equal to $$-0.6.$$ Therefore $$\\mathbf{C}$$ has diagonal $$[0.01,\\,2.5\\times 10^{-5},\\,1.6\\times 10^{-5}]$$ and off-diagonal entries $$C_{E_r,\\Gamma} = C_{\\Gamma,E_r} = -0.6 \\cdot 0.005 \\cdot 0.004 = -1.2\\times 10^{-5},$$ other off-diagonals zero.\n\nYour program should produce a single line of output containing the decisions for the four cases as a comma-separated list enclosed in square brackets (for example, $$[0,1,0,1]$$). The output elements must be integers $$0$$ or $$1.$$", "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded in the principles of nuclear reaction theory and statistical uncertainty propagation. The problem is well-posed, with all necessary data, definitions, and constraints provided to permit a unique solution. The language is objective and mathematically precise. We may therefore proceed with the solution.\n\nThe core of the problem is to assess the degree of nonlinearity in a function $R(\\boldsymbol{\\theta})$ with respect to its parameters $\\boldsymbol{\\theta} = [A, E_r, \\Gamma]^T$ in the context of propagating uncertainties from a multivariate normal distribution, $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{C})$. The diagnostic tools are derived from a second-order Taylor series expansion of $R(\\boldsymbol{\\theta})$ around the mean parameter vector $\\boldsymbol{\\mu}$.\n\nThe Taylor expansion is given by:\n$$ R(\\boldsymbol{\\theta}) \\approx R(\\boldsymbol{\\mu}) + (\\boldsymbol{\\theta} - \\boldsymbol{\\mu})^T \\mathbf{g} + \\frac{1}{2} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu})^T \\mathbf{H} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}) $$\nwhere $\\mathbf{g} = \\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\mu})$ is the gradient vector and $\\mathbf{H} = \\nabla^2_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\mu})$ is the Hessian matrix of $R$ evaluated at the mean $\\boldsymbol{\\mu}$. The components of the gradient and Hessian are the partial derivatives $g_i = \\partial R / \\partial \\theta_i$ and $H_{ij} = \\partial^2 R / (\\partial \\theta_i \\partial \\theta_j)$, respectively.\n\nFrom this expansion, we can find the approximate moments of the distribution of $R(\\boldsymbol{\\theta})$.\nThe expected value (mean) of $R$ is found by taking the expectation of the expansion. Let $\\boldsymbol{\\delta} = \\boldsymbol{\\theta} - \\boldsymbol{\\mu}$. By definition, $\\mathbb{E}[\\boldsymbol{\\delta}] = \\mathbf{0}$ and the covariance is $\\mathbb{E}[\\boldsymbol{\\delta}\\boldsymbol{\\delta}^T] = \\mathbf{C}$.\n$$ \\mathbb{E}[R(\\boldsymbol{\\theta})] \\approx \\mathbb{E}[R(\\boldsymbol{\\mu}) + \\mathbf{g}^T\\boldsymbol{\\delta} + \\frac{1}{2}\\boldsymbol{\\delta}^T\\mathbf{H}\\boldsymbol{\\delta}] $$\n$$ \\mathbb{E}[R(\\boldsymbol{\\theta})] \\approx R(\\boldsymbol{\\mu}) + \\mathbf{g}^T\\mathbb{E}[\\boldsymbol{\\delta}] + \\frac{1}{2}\\mathbb{E}[\\boldsymbol{\\delta}^T\\mathbf{H}\\boldsymbol{\\delta}] $$\n$$ \\mathbb{E}[R(\\boldsymbol{\\theta})] \\approx R(\\boldsymbol{\\mu}) + \\frac{1}{2}\\mathbb{E}[\\text{Tr}(\\mathbf{H}\\boldsymbol{\\delta}\\boldsymbol{\\delta}^T)] = R(\\boldsymbol{\\mu}) + \\frac{1}{2}\\text{Tr}(\\mathbf{H}\\mathbb{E}[\\boldsymbol{\\delta}\\boldsymbol{\\delta}^T]) = R(\\boldsymbol{\\mu}) + \\frac{1}{2}\\text{Tr}(\\mathbf{H}\\mathbf{C}) $$\nThe term $b_2 = \\frac{1}{2}\\text{Tr}(\\mathbf{H}\\mathbf{C})$ is the second-order bias or mean-shift induced by the curvature of the function $R$.\n\nThe variance of $R$ is more complex. To first order, we consider only the linear term:\n$$ R(\\boldsymbol{\\theta}) \\approx R(\\boldsymbol{\\mu}) + \\mathbf{g}^T\\boldsymbol{\\delta} $$\nThe variance is then:\n$$ \\text{Var}(R) \\approx \\text{Var}(\\mathbf{g}^T\\boldsymbol{\\delta}) = \\mathbb{E}[(\\mathbf{g}^T\\boldsymbol{\\delta})(\\mathbf{g}^T\\boldsymbol{\\delta})^T] = \\mathbb{E}[\\mathbf{g}^T\\boldsymbol{\\delta}\\boldsymbol{\\delta}^T\\mathbf{g}] = \\mathbf{g}^T\\mathbb{E}[\\boldsymbol{\\delta}\\boldsymbol{\\delta}^T]\\mathbf{g} = \\mathbf{g}^T\\mathbf{C}\\mathbf{g} $$\nThis is the leading, linear contribution to the variance, denoted $V_1 = \\mathbf{g}^T\\mathbf{C}\\mathbf{g}$.\n\nThe next leading contribution to the variance arises from the quadratic term in the Taylor expansion. For a multivariate normal input, the variance of the quadratic form $\\frac{1}{2}\\boldsymbol{\\delta}^T\\mathbf{H}\\boldsymbol{\\delta}$ is given by $\\frac{1}{2}\\text{Tr}((\\mathbf{H}\\mathbf{C})^2)$. There is also a covariance term between the linear and quadratic parts, but for a symmetric distribution like the normal distribution, this term is zero if the third central moments are zero. Thus, the leading curvature-induced variance correction is $V_2 = \\frac{1}{2}\\text{Tr}((\\mathbf{H}\\mathbf{C})^2)$.\n\nThe solution strategy is as follows:\n1.  Numerically compute the observable $R(\\boldsymbol{\\theta}; kT)$ for any given parameter vector $\\boldsymbol{\\theta}$ and thermal energy $kT$. The integral is over a semi-infinite domain, which suggests using a specialized numerical quadrature routine, such as `scipy.integrate.quad`. The integrand is $f(y; \\boldsymbol{\\theta}, kT) = y e^{-y} A \\left[ (kT\\,y - E_r)^2 + (\\Gamma/2)^2 \\right]^{-1}$.\n2.  Numerically approximate the gradient $\\mathbf{g}$ and the Hessian matrix $\\mathbf{H}$ at $\\boldsymbol{\\theta} = \\boldsymbol{\\mu}$ using central finite differences, as required. For a function $R(\\boldsymbol{\\theta})$, the derivatives are:\n    - Gradient component: $g_i \\approx \\frac{R(\\boldsymbol{\\mu} + h_i\\mathbf{e}_i) - R(\\boldsymbol{\\mu} - h_i\\mathbf{e}_i)}{2h_i}$\n    - Hessian diagonal component: $H_{ii} \\approx \\frac{R(\\boldsymbol{\\mu} + h_i\\mathbf{e}_i) - 2R(\\boldsymbol{\\mu}) + R(\\boldsymbol{\\mu} - h_i\\mathbf{e}_i)}{h_i^2}$\n    - Hessian off-diagonal component: $H_{ij} \\approx \\frac{R(\\boldsymbol{\\mu}+h_i\\mathbf{e}_i+h_j\\mathbf{e}_j) - R(\\boldsymbol{\\mu}+h_i\\mathbf{e}_i-h_j\\mathbf{e}_j) - R(\\boldsymbol{\\mu}-h_i\\mathbf{e}_i+h_j\\mathbf{e}_j) + R(\\boldsymbol{\\mu}-h_i\\mathbf{e}_i-h_j\\mathbf{e}_j)}{4h_i h_j}$\n    where $\\mathbf{e}_i$ is the $i$-th standard basis vector and $h_i = \\varepsilon \\max(|\\mu_i|, 1)$ is the step size with $\\varepsilon = 10^{-4}$.\n3.  For each test case, assemble the mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\mathbf{C}$.\n4.  Compute the quantities $V_1 = \\mathbf{g}^T\\mathbf{C}\\mathbf{g}$, $b_2 = \\frac{1}{2}\\text{Tr}(\\mathbf{H}\\mathbf{C})$, and $V_2 = \\frac{1}{2}\\text{Tr}((\\mathbf{H}\\mathbf{C})^2)$.\n5.  Calculate the diagnostic metrics $\\kappa_{\\mathrm{var}} = V_2/V_1$ and $\\kappa_{\\mathrm{bias}} = |b_2|/\\sqrt{V_1}$.\n6.  Apply the decision rule: if $\\kappa_{\\mathrm{var}} \\leq 0.25$ and $\\kappa_{\\mathrm{bias}} \\leq 0.25$, the decision is higher-order propagation (coded as $0$). Otherwise, Monte Carlo is warranted (coded as $1$).\n\nThis procedure is implemented for each of the four test cases provided. The implementation requires careful handling of numerical precision and correct application of matrix algebra. The final output will be an integer list representing the decision for each case.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Solves the uncertainty propagation diagnostic problem for all test cases.\n    \"\"\"\n\n    def R_integrand(y, A, Er, Gamma, kT):\n        \"\"\"The integrand for the observable R.\"\"\"\n        denominator = (kT * y - Er)**2 + (Gamma / 2)**2\n        # A small constant is added to avoid potential division by zero if Gamma and (kT*y-Er) are both zero.\n        # This is physically unlikely but good for numerical stability.\n        if denominator == 0:\n            return 0.0  # The peak is infinitely narrow and contributes zero to the integral\n        return y * np.exp(-y) * A / denominator\n\n    def R_func(theta, kT):\n        \"\"\"\n        Computes the observable R for a given parameter vector theta and thermal energy kT.\n        \n        Args:\n            theta (np.ndarray): Parameter vector [A, Er, Gamma].\n            kT (float): Thermal energy.\n        \n        Returns:\n            float: The value of the integral R.\n        \"\"\"\n        A, Er, Gamma = theta\n        # All parameters must be physically meaningful, e.g., A  0, Gamma  0.\n        if A  0 or Gamma = 0:\n            return np.nan\n\n        # Use scipy.integrate.quad for the semi-infinite integral.\n        # The integral converges rapidly due to the exp(-y) term.\n        result, _ = quad(R_integrand, 0, np.inf, args=(A, Er, Gamma, kT))\n        return result\n\n    def compute_gradient(func, mu, kT, epsilon=1e-4):\n        \"\"\"\n        Computes the gradient of func using central finite differences.\n        \"\"\"\n        n = len(mu)\n        g = np.zeros(n)\n        h_vec = epsilon * np.maximum(np.abs(mu), 1.0)\n        \n        for i in range(n):\n            h_i = h_vec[i]\n            mu_plus = mu.copy()\n            mu_plus[i] += h_i\n            mu_minus = mu.copy()\n            mu_minus[i] -= h_i\n            g[i] = (func(mu_plus, kT) - func(mu_minus, kT)) / (2 * h_i)\n            \n        return g\n\n    def compute_hessian(func, mu, kT, epsilon=1e-4):\n        \"\"\"\n        Computes the Hessian of func using central finite differences.\n        \"\"\"\n        n = len(mu)\n        H = np.zeros((n, n))\n        h_vec = epsilon * np.maximum(np.abs(mu), 1.0)\n\n        f0 = func(mu, kT)\n\n        # Diagonal elements\n        for i in range(n):\n            h_i = h_vec[i]\n            mu_plus = mu.copy()\n            mu_plus[i] += h_i\n            mu_minus = mu.copy()\n            mu_minus[i] -= h_i\n            f_plus = func(mu_plus, kT)\n            f_minus = func(mu_minus, kT)\n            H[i, i] = (f_plus - 2 * f0 + f_minus) / (h_i**2)\n\n        # Off-diagonal elements\n        for i in range(n):\n            for j in range(i + 1, n):\n                h_i = h_vec[i]\n                h_j = h_vec[j]\n                \n                mu_pp = mu.copy()\n                mu_pp[i] += h_i\n                mu_pp[j] += h_j\n                f_pp = func(mu_pp, kT)\n\n                mu_pm = mu.copy()\n                mu_pm[i] += h_i\n                mu_pm[j] -= h_j\n                f_pm = func(mu_pm, kT)\n\n                mu_mp = mu.copy()\n                mu_mp[i] -= h_i\n                mu_mp[j] += h_j\n                f_mp = func(mu_mp, kT)\n\n                mu_mm = mu.copy()\n                mu_mm[i] -= h_i\n                mu_mm[j] -= h_j\n                f_mm = func(mu_mm, kT)\n                \n                H[i, j] = (f_pp - f_pm - f_mp + f_mm) / (4 * h_i * h_j)\n                H[j, i] = H[i, j]\n                \n        return H\n\n    # Define test cases\n    test_cases = [\n        # Case 1\n        {\n            \"kT\": 0.5,\n            \"mu\": np.array([5.0, 1.5, 0.4]),\n            \"C\": np.diag([0.04, 0.0025, 0.0016])\n        },\n        # Case 2\n        {\n            \"kT\": 0.5,\n            \"mu\": np.array([5.0, 0.7, 0.2]),\n            \"C\": np.array([\n                [0.25, 0, 0],\n                [0, 0.0049, 0.00042],\n                [0, 0.00042, 0.0004]\n            ])\n        },\n        # Case 3\n        {\n            \"kT\": 0.4,\n            \"mu\": np.array([5.0, 0.52, 0.05]),\n            \"C\": np.array([\n                [1.0, 0, 0],\n                [0, 0.0064, 0.00128],\n                [0, 0.00128, 0.0004]\n            ])\n        },\n        # Case 4\n        {\n            \"kT\": 0.6,\n            \"mu\": np.array([5.0, 0.5, 0.02]),\n            \"C\": np.array([\n                [0.01, 0, 0],\n                [0, 2.5e-5, -1.2e-5],\n                [0, -1.2e-5, 1.6e-5]\n            ])\n        }\n    ]\n\n    decisions = []\n    for case in test_cases:\n        kT = case[\"kT\"]\n        mu = case[\"mu\"]\n        C = case[\"C\"]\n\n        # Compute gradient and Hessian\n        g = compute_gradient(R_func, mu, kT)\n        H = compute_hessian(R_func, mu, kT)\n\n        # Compute V1, b2, V2\n        V1 = g.T @ C @ g\n        HC = H @ C\n        b2 = 0.5 * np.trace(HC)\n        V2 = 0.5 * np.trace(HC @ HC)\n\n        # Compute diagnostic metrics\n        if V1 = 0:\n            # This indicates a flat function or numerical instability.\n            # If V1 is zero, any non-zero curvature implies MC is needed.\n            kappa_var = np.inf if V2  0 else 0\n            kappa_bias = np.inf if abs(b2)  0 else 0\n        else:\n            kappa_var = V2 / V1\n            kappa_bias = abs(b2) / np.sqrt(V1)\n            \n        # Apply decision rule\n        if kappa_var = 0.25 and kappa_bias = 0.25:\n            decision = 0  # Higher-order Taylor is warranted\n        else:\n            decision = 1  # Monte Carlo is warranted\n            \n        decisions.append(decision)\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, decisions))}]\")\n\nsolve()\n```", "id": "3581708"}, {"introduction": "When analytical approximations are insufficient due to strong nonlinearity, Monte Carlo simulation becomes the method of choice. This hands-on practice guides you through implementing a full Monte Carlo uncertainty propagation for a toy model of neutron multiplication [@problem_id:3581694]. More importantly, it extends beyond simply generating samples to the rigorous statistical analysis of the results, including the estimation of credible intervals and, crucially, the uncertainty in those estimates due to the finite sample size. This skill is paramount for producing reliable and defensible results in any simulation-based scientific inquiry.", "problem": "Consider a one-parameter infinite-medium toy model for neutron multiplication in computational nuclear physics. The microscopic absorption cross section is modeled as a lognormal random variable with parameters on the logarithmic scale: if $\\sigma$ denotes the cross section in barns, then $\\ln \\sigma \\sim \\mathcal{N}(\\mu,\\tau^2)$, where $\\mu \\in \\mathbb{R}$ and $\\tau^2 \\in (0,\\infty)$ are known. The effective neutron multiplication factor is modeled as a deterministic function of the cross section, $k_{\\mathrm{eff}} = g(\\sigma)$, where \n$$\ng(\\sigma) = k_0 \\exp(-\\alpha \\sigma),\n$$\nwith $k_0 \\in (0,\\infty)$ dimensionless and $\\alpha \\in (0,\\infty)$ in inverse barns. The mapping $g$ is strictly decreasing and maps $\\sigma \\in (0,\\infty)$ to $k_{\\mathrm{eff}} \\in (0,k_0)$.\n\nYour task is to implement a Monte Carlo uncertainty propagation from $\\sigma$ to $k_{\\mathrm{eff}}$ and then estimate the equal-tailed credible interval at level $0.95$ for $k_{\\mathrm{eff}}$ using $N$ independent and identically distributed samples, together with an error bar on each endpoint that quantifies the Monte Carlo sampling uncertainty due to finite $N$.\n\nYou must start from fundamental definitions:\n- The density of a lognormal distribution arises from the change of variables from a normal distribution on $\\ln \\sigma$.\n- The distribution of $k_{\\mathrm{eff}}$ is the pushforward of the lognormal distribution through $g$.\n- The equal-tailed credible interval at level $0.95$ is the interval between the empirical quantiles at probabilities $0.025$ and $0.975$ of the propagated samples.\n- The error bar on each endpoint must be a one-standard-error estimate derived from the asymptotic behavior of empirical quantiles of independent and identically distributed samples, expressed in terms of the probability density function (PDF) at the corresponding true quantile. You must obtain the PDF of $k_{\\mathrm{eff}}$ via the change-of-variables formula applied to the known lognormal PDF of $\\sigma$, and then evaluate it at the plug-in empirical quantiles.\n\nPhysical units and numerical conventions:\n- Express $\\sigma$ in barns, $\\alpha$ in inverse barns, and $k_{\\mathrm{eff}}$ as dimensionless.\n- Angles do not appear in this problem.\n- The credible level $0.95$ must be treated as a probability in $(0,1)$, not as a percent.\n- To ensure deterministic results, use a fixed random seed equal to $12345$ for all simulations.\n\nImplementation requirements:\n- Draw $N$ independent and identically distributed samples of $\\sigma$ from the specified lognormal distribution.\n- Propagate each sample through $g$ to obtain samples of $k_{\\mathrm{eff}}$.\n- Compute the empirical quantiles of $k_{\\mathrm{eff}}$ at probabilities $0.025$ and $0.975$ to form the estimated credible interval $[\\hat{q}_{0.025}, \\hat{q}_{0.975}]$.\n- Using the analytical PDF of $\\sigma$ and the change-of-variables formula under $g$, compute the PDF of $k_{\\mathrm{eff}}$ at $y=\\hat{q}_{0.025}$ and $y=\\hat{q}_{0.975}$. Use these to form asymptotic one-standard-error estimates for the quantiles at $p=0.025$ and $p=0.975$ with $N$ samples.\n- Report the results for each test case as a list $[\\hat{q}_{0.025}, \\hat{q}_{0.975}, s_{0.025}, s_{0.975}]$, where $s_{p}$ is the one-standard-error for the empirical $p$-quantile.\n\nTest suite:\nImplement your program to run the following four test cases in order. For each case, use the provided tuple $(\\mu,\\tau,\\alpha,k_0,N)$:\n- Case $1$: $(\\ln(1.5), 0.25, 0.05, 1.0, 40000)$\n- Case $2$: $(\\ln(2.5), 0.60, 0.08, 1.0, 60000)$\n- Case $3$: $(0.0, 0.05, 0.03, 1.0, 5000)$\n- Case $4$: $(\\ln(0.5), 0.40, 0.10, 0.98, 30000)$\n\nFinal output specification:\n- Your program must produce a single line of output containing the results for the four cases aggregated into a single list of lists, with no spaces, where each inner list is $[\\hat{q}_{0.025},\\hat{q}_{0.975},s_{0.025},s_{0.975}]$ for the corresponding case.\n- Each floating-point value must be rounded to exactly $6$ digits after the decimal point.\n- For example, the syntactic form must be like $[[a,b,c,d],[e,f,g,h],[i,j,k,l],[m,n,o,p]]$ where each letter represents a floating-point number with exactly $6$ digits after the decimal point.", "solution": "The objective is to perform a Monte Carlo uncertainty propagation from a lognormally distributed microscopic absorption cross section, $\\sigma$, to the effective neutron multiplication factor, $k_{\\mathrm{eff}}$, and to estimate the $0.95$ equal-tailed credible interval for $k_{\\mathrm{eff}}$ along with the standard error of the interval's endpoints. The process relies on fundamental principles of probability theory and statistics, including the change of variables formula for probability distributions and the asymptotic properties of sample quantiles.\n\nLet's begin by establishing the required probability density functions (PDFs).\n\n**1. Probability Density Function of the Cross Section $\\sigma$**\nThe problem states that the natural logarithm of the cross section, $\\ln \\sigma$, follows a normal distribution, $\\ln \\sigma \\sim \\mathcal{N}(\\mu, \\tau^2)$. Let $X = \\ln \\sigma$. The PDF of $X$ is given by:\n$$f_X(x) = \\frac{1}{\\tau \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\tau^2}\\right)$$\nThe cross section $\\sigma$ is related to $X$ by the transformation $\\sigma = \\exp(X)$. To find the PDF of $\\sigma$, denoted $f_\\Sigma(\\sigma)$, we apply the change of variables formula:\n$$f_\\Sigma(\\sigma) = f_X(\\ln \\sigma) \\left| \\frac{d(\\ln \\sigma)}{d\\sigma} \\right| = f_X(\\ln \\sigma) \\cdot \\frac{1}{\\sigma}$$\nSubstituting the expression for $f_X(x)$, we obtain the PDF for a lognormal distribution:\n$$f_\\Sigma(\\sigma) = \\frac{1}{\\sigma\\tau\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln \\sigma - \\mu)^2}{2\\tau^2}\\right) \\quad \\text{for } \\sigma  0$$\n\n**2. Probability Density Function of the Multiplication Factor $k_{\\mathrm{eff}}$**\nThe effective multiplication factor $k_{\\mathrm{eff}}$ is a deterministic function of $\\sigma$: $k_{\\mathrm{eff}} = g(\\sigma) = k_0 \\exp(-\\alpha \\sigma)$. Let's denote the random variable for the multiplication factor as $Y$, so $Y = g(\\sigma)$. To find its PDF, $f_Y(y)$, we again use the change of variables formula. First, we need the inverse function $\\sigma = g^{-1}(y)$:\n$$y = k_0 \\exp(-\\alpha \\sigma) \\implies \\frac{y}{k_0} = \\exp(-\\alpha \\sigma) \\implies \\ln\\left(\\frac{y}{k_0}\\right) = -\\alpha \\sigma$$\n$$\\sigma = g^{-1}(y) = -\\frac{1}{\\alpha}\\ln\\left(\\frac{y}{k_0}\\right) = \\frac{1}{\\alpha}\\ln\\left(\\frac{k_0}{y}\\right)$$\nThe function $g$ maps $\\sigma \\in (0, \\infty)$ to $y \\in (0, k_0)$. The inverse function is defined over this range. Next, we compute the derivative of the inverse function with respect to $y$:\n$$\\frac{d\\sigma}{dy} = \\frac{d}{dy}\\left[\\frac{1}{\\alpha}\\left(\\ln k_0 - \\ln y\\right)\\right] = -\\frac{1}{\\alpha y}$$\nThe change of variables rule states $f_Y(y) = f_\\Sigma(g^{-1}(y)) \\left|\\frac{d(g^{-1}(y))}{dy}\\right|$. Substituting the expressions for $g^{-1}(y)$, its derivative, and $f_\\Sigma(\\sigma)$:\n$$f_Y(y) = f_\\Sigma\\left(\\frac{1}{\\alpha}\\ln\\left(\\frac{k_0}{y}\\right)\\right) \\cdot \\left|-\\frac{1}{\\alpha y}\\right|$$\nLet $\\sigma(y) = \\frac{1}{\\alpha}\\ln(\\frac{k_0}{y})$. The PDF $f_Y(y)$ is then:\n$$f_Y(y) = \\frac{1}{\\sigma(y)\\tau\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln(\\sigma(y)) - \\mu)^2}{2\\tau^2}\\right) \\cdot \\frac{1}{\\alpha y}$$\nSubstituting $\\sigma(y)$ into the expression yields the final form for $y \\in (0, k_0)$:\n$$f_Y(y) = \\frac{1}{\\left(\\frac{1}{\\alpha}\\ln\\frac{k_0}{y}\\right)\\tau\\sqrt{2\\pi}} \\exp\\left(-\\frac{\\left(\\ln\\left(\\frac{1}{\\alpha}\\ln\\frac{k_0}{y}\\right) - \\mu\\right)^2}{2\\tau^2}\\right) \\cdot \\frac{1}{\\alpha y}$$\nThis expression can be simplified to:\n$$f_Y(y) = \\frac{1}{y \\ln(k_0/y) \\tau\\sqrt{2\\pi}} \\exp\\left(-\\frac{\\left(\\ln\\left(\\frac{1}{\\alpha}\\ln\\frac{k_0}{y}\\right) - \\mu\\right)^2}{2\\tau^2}\\right)$$\n\n**3. Monte Carlo Simulation and Credible Interval Estimation**\nThe core of the task is a Monte Carlo simulation. The procedure involves these steps:\n1.  Generate $N$ independent and identically distributed (i.i.d.) samples, $\\{x_i\\}_{i=1}^N$, from the normal distribution $\\mathcal{N}(\\mu, \\tau^2)$.\n2.  Transform these samples to obtain $N$ samples of the cross section: $\\sigma_i = \\exp(x_i)$.\n3.  Propagate the cross section samples through the model to obtain $N$ samples of the multiplication factor: $y_i = k_0 \\exp(-\\alpha \\sigma_i)$.\n4.  The $0.95$ equal-tailed credible interval for $k_{\\mathrm{eff}}$ is estimated by the empirical quantiles of the sample set $\\{y_i\\}$. The interval is $[\\hat{q}_{0.025}, \\hat{q}_{0.975}]$, where $\\hat{q}_p$ is the empirical $p$-quantile. These are computed from the sorted samples of $k_{\\mathrm{eff}}$, typically using linear interpolation between adjacent samples.\n\n**4. Standard Error of the Quantile Estimates**\nThe precision of the empirical quantile $\\hat{q}_p$ as an estimator for the true quantile $q_p$ can be quantified by its standard error. For a large number of samples $N$, the distribution of $\\hat{q}_p$ is approximately normal. The standard error, $s_p$, is the standard deviation of this approximate distribution, given by the asymptotic formula:\n$$s_p = \\sqrt{\\mathrm{Var}(\\hat{q}_p)} \\approx \\frac{\\sqrt{p(1-p)}}{\\sqrt{N} f_Y(q_p)}$$\nSince the true quantile $q_p$ and thus $f_Y(q_p)$ are unknown, we use a \"plug-in\" estimator by substituting the empirical quantile $\\hat{q}_p$ for $q_p$:\n$$\\hat{s}_p = \\frac{\\sqrt{p(1-p)}}{\\sqrt{N} f_Y(\\hat{q}_p)}$$\nWe will calculate this standard error for the lower quantile ($p_1=0.025$) and the upper quantile ($p_2=0.975$). The numerator $\\sqrt{p(1-p)}$ is identical for both: $\\sqrt{0.025(1-0.025)} = \\sqrt{0.975(1-0.975)} = \\sqrt{0.024375}$.\n\nThe implementation will follow these steps for each test case: generate samples for $k_{\\mathrm{eff}}$, compute the quantiles $\\hat{q}_{0.025}$ and $\\hat{q}_{0.975}$, evaluate the derived PDF $f_Y(y)$ at these quantiles, and finally compute the standard errors $\\hat{s}_{0.025}$ and $\\hat{s}_{0.975}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    \n    # Per the problem, use a fixed random seed for deterministic results.\n    seed = 12345\n    rng = np.random.default_rng(seed)\n\n    # Define the test cases from the problem statement.\n    # Each tuple is (mu, tau, alpha, k0, N)\n    test_cases = [\n        (np.log(1.5), 0.25, 0.05, 1.0, 40000),\n        (np.log(2.5), 0.60, 0.08, 1.0, 60000),\n        (0.0, 0.05, 0.03, 1.0, 5000),\n        (np.log(0.5), 0.40, 0.10, 0.98, 30000)\n    ]\n\n    all_results = []\n    for case in test_cases:\n        mu, tau, alpha, k0, N = case\n        result = process_case(mu, tau, alpha, k0, N, rng)\n        all_results.append(result)\n\n    # Format the output as a single list of lists string with no spaces and\n    # each float rounded to 6 decimal places.\n    formatted_inner_lists = []\n    for res in all_results:\n        # res is [q_low, q_high, s_low, s_high]\n        formatted_inner_lists.append(f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f},{res[3]:.6f}]\")\n    \n    final_output_string = f\"[{','.join(formatted_inner_lists)}]\"\n    print(final_output_string)\n\n\ndef process_case(mu, tau, alpha, k0, N, rng):\n    \"\"\"\n    Performs the full calculation for a single test case.\n\n    Args:\n        mu (float): Log-scale mean of the cross section.\n        tau (float): Log-scale standard deviation of the cross section.\n        alpha (float): Parameter in the g(sigma) function.\n        k0 (float): Parameter in the g(sigma) function.\n        N (int): Number of Monte Carlo samples.\n        rng (numpy.random.Generator): Random number generator instance.\n\n    Returns:\n        list: A list containing [q_low, q_high, s_low, s_high].\n    \"\"\"\n    # 1. Generate N samples of sigma from the lognormal distribution.\n    #    This is done by generating from a normal distribution and exponentiating.\n    ln_sigma_samples = rng.normal(loc=mu, scale=tau, size=N)\n    sigma_samples = np.exp(ln_sigma_samples)\n\n    # 2. Propagate samples through g(sigma) to get k_eff samples.\n    k_eff_samples = k0 * np.exp(-alpha * sigma_samples)\n\n    # 3. Compute the empirical quantiles for the 0.95 credible interval.\n    p_low = 0.025\n    p_high = 0.975\n    q_low = np.quantile(k_eff_samples, p_low)\n    q_high = np.quantile(k_eff_samples, p_high)\n\n    # 4. Define the analytical PDF of k_eff, f_Y(y).\n    def pdf_k_eff(y):\n        # The support for y is (0, k0). Outside this, PDF is 0.\n        # Quantiles from samples will be within this range.\n        if not (0  y  k0):\n            return 0.0\n        \n        # This term corresponds to sigma = g_inv(y)\n        ln_k0_over_y = np.log(k0 / y)\n        sigma_val = (1.0 / alpha) * ln_k0_over_y\n        \n        # This term corresponds to ln(sigma)\n        ln_sigma_val = np.log(sigma_val)\n        \n        # Numerator of the exponent term in the PDF\n        exp_num = -((ln_sigma_val - mu)**2)\n        # Denominator of the exponent term\n        exp_den = 2 * tau**2\n        \n        # Overall multiplicative factor for the PDF of k_eff\n        # This factor is 1 / (y * ln(k0/y) * tau * sqrt(2*pi))\n        factor = 1.0 / (y * ln_k0_over_y * tau * np.sqrt(2 * np.pi))\n        \n        pdf = factor * np.exp(exp_num / exp_den)\n        return pdf\n\n    # 5. Evaluate the PDF at the estimated quantiles.\n    pdf_at_q_low = pdf_k_eff(q_low)\n    pdf_at_q_high = pdf_k_eff(q_high)\n\n    # 6. Compute the standard error for each quantile estimate.\n    # The numerator sqrt(p*(1-p)) is the same for p=0.025 and p=0.975.\n    numerator = np.sqrt(p_low * (1 - p_low))\n    \n    s_low = numerator / (np.sqrt(N) * pdf_at_q_low)\n    s_high = numerator / (np.sqrt(N) * pdf_at_q_high)\n\n    return [q_low, q_high, s_low, s_high]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3581694"}]}