{"hands_on_practices": [{"introduction": "A foundational skill in working with optical potentials is the ability to compute their integral properties from first principles. This exercise [@problem_id:3578612] guides you through the numerical integration of a Woods-Saxon potential to find its volume integral, a key measure of its overall strength. You will then implement a simple bisection search, demonstrating the core logic of an inverse problem: finding the model parameter that reproduces a target observable.", "problem": "You are tasked with developing a complete program that implements numerical evaluation and a simple parameter search algorithm for the central real part of an optical model potential in a spherically symmetric nucleus. Begin from the physically standard definition of the spherically symmetric central real optical potential, defined by a Woods–Saxon form, and the definition of its volume integral. The goal is to compute the volume integral and, in specified cases, infer the depth parameter by matching a target volume integral through a search procedure. All quantities must be handled in a scientifically consistent manner with clear units.\n\nFundamental base:\n- The optical model treats nucleon–nucleus interactions through an effective complex potential. For the central real part, a common form is the Woods–Saxon potential, defined as a function of the radial coordinate $r$ by\n$$\nV(r) = -\\frac{V_0}{1 + \\exp\\left(\\frac{r - R}{a}\\right)} \\, ,\n$$\nwhere $V_0$ is the positive depth parameter expressed in $\\mathrm{MeV}$, $a$ is the diffuseness expressed in $\\mathrm{fm}$, and $R$ is the nuclear radius parameter given by $R = r_0 A^{1/3}$ where $A$ is the mass number (dimensionless) and $r_0$ is expressed in $\\mathrm{fm}$.\n- The volume integral $J_V$ of the central real part is defined by\n$$\nJ_V = \\int_{\\mathbb{R}^3} V(\\mathbf{r}) \\, d^3\\mathbf{r} = 4\\pi \\int_{0}^{\\infty} V(r) \\, r^2 \\, dr \\, ,\n$$\nwhich has units of $\\mathrm{MeV \\, fm^3}$. For an attractive Woods–Saxon form with the conventional sign convention, $J_V$ is negative. This quantity is a standard measure of the integrated strength of the mean-field interaction.\n\nTasks:\n1. Implement a numerical quadrature for $J_V$ using the trapezoidal rule on a uniform radial grid. Discretize $r \\in [0, r_{\\max}]$ with a fixed step size $\\Delta r$, evaluate the integrand $V(r) r^2$, approximate the integral as\n$$\n\\int_{0}^{r_{\\max}} V(r) \\, r^2 \\, dr \\approx \\Delta r \\left[\\tfrac{1}{2} f(0) + \\sum_{k=1}^{N-1} f(k \\Delta r) + \\tfrac{1}{2} f(r_{\\max}) \\right], \\quad f(r) \\equiv V(r) r^2 \\, ,\n$$\nand then compute $J_V = 4\\pi \\times (\\text{trapezoidal result})$. Choose $r_{\\max}$ sufficiently large such that the tail contribution beyond $r_{\\max}$ is negligible compared to the specified accuracy. Express $J_V$ in $\\mathrm{MeV \\, fm^3}$.\n2. Implement a one-dimensional parameter search algorithm to estimate $V_0$ such that the computed $J_V$ matches a specified target $J_T$. Use a robust bracketing strategy and bisection on the function\n$$\nF(V_0) = J_V(V_0) - J_T \\, ,\n$$\nwhere $J_V(V_0)$ is the numerically integrated volume integral for the given $V_0$. Due to the linear dependence of $J_V$ on $V_0$ for the Woods–Saxon central term, $F(V_0)$ is monotonic in $V_0$ for fixed $A$, $r_0$, $a$, $\\Delta r$, and $r_{\\max}$. Stop when the absolute difference $\\lvert J_V(V_0) - J_T \\rvert$ is below a specified tolerance. Express $V_0$ in $\\mathrm{MeV}$.\n\nNumerical details:\n- The trapezoidal rule should be used exactly as defined above.\n- Use the uniform grid spacing specified for each test case.\n- All radii are in $\\mathrm{fm}$, all energies are in $\\mathrm{MeV}$, and the final volume integrals are in $\\mathrm{MeV \\, fm^3}$.\n- No angles are used; thus no angle unit specification is required.\n- For all tests, round the reported numerical outputs to three decimal places.\n\nTest suite:\nCompute results for the following four test cases, designed to cover a standard happy path, heavier system scaling, a coarse-grid boundary condition, and a parameter search:\n\n- Test $1$ (happy path numerical integration): Given $A = 40$, $r_0 = 1.25 \\, \\mathrm{fm}$, $a = 0.65 \\, \\mathrm{fm}$, and $V_0 = 50 \\, \\mathrm{MeV}$, compute $J_V$ numerically by the trapezoidal rule with $\\Delta r = 0.05 \\, \\mathrm{fm}$ up to $r_{\\max} = 12 \\, \\mathrm{fm}$, and report the value in $\\mathrm{MeV \\, fm^3}$ to within $1 \\, \\mathrm{MeV \\, fm^3}$.\n- Test $2$ (heavier system): Given $A = 208$, $r_0 = 1.25 \\, \\mathrm{fm}$, $a = 0.63 \\, \\mathrm{fm}$, and $V_0 = 52 \\, \\mathrm{MeV}$, compute $J_V$ numerically by the trapezoidal rule with $\\Delta r = 0.05 \\, \\mathrm{fm}$ up to $r_{\\max} = 15 \\, \\mathrm{fm}$, and report the value in $\\mathrm{MeV \\, fm^3}$ to within $1 \\, \\mathrm{MeV \\, fm^3}$.\n- Test $3$ (coarse grid boundary condition): Given $A = 40$, $r_0 = 1.25 \\, \\mathrm{fm}$, $a = 0.65 \\, \\mathrm{fm}$, and $V_0 = 50 \\, \\mathrm{MeV}$, compute $J_V$ numerically by the trapezoidal rule with $\\Delta r = 0.5 \\, \\mathrm{fm}$ up to $r_{\\max} = 12 \\, \\mathrm{fm}$, and report the value in $\\mathrm{MeV \\, fm^3}$ to within $100 \\, \\mathrm{MeV \\, fm^3}$ (the looser tolerance reflects discretization error).\n- Test $4$ (parameter search for depth): Given $A = 40$, $r_0 = 1.25 \\, \\mathrm{fm}$, $a = 0.65 \\, \\mathrm{fm}$, and target $J_T = -20000 \\, \\mathrm{MeV \\, fm^3}$, use a bisection search on $V_0 \\in [1 \\, \\mathrm{MeV}, 200 \\, \\mathrm{MeV}]$ with the trapezoidal rule using $\\Delta r = 0.05 \\, \\mathrm{fm}$ up to $r_{\\max} = 12 \\, \\mathrm{fm}$, and report the estimated $V_0$ in $\\mathrm{MeV}$ such that $\\lvert J_V(V_0) - J_T \\rvert \\le 0.1 \\, \\mathrm{MeV \\, fm^3}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain four floating-point numbers, in the order of Tests $1$ through $4$, each rounded to three decimal places. For example, the printed output should look like\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4] \\, .\n$$\nAll $J_V$ results must be in $\\mathrm{MeV \\, fm^3}$ and the last entry (the parameter search result) must be in $\\mathrm{MeV}$.", "solution": "The optical model establishes an effective mean-field description of nucleon–nucleus interactions in terms of a complex potential; in this problem we focus on the central real part, which is commonly represented by a Woods–Saxon form. The Woods–Saxon potential is defined for a spherically symmetric nucleus as\n$$\nV(r) = -\\frac{V_0}{1 + \\exp\\left(\\frac{r - R}{a}\\right)} \\, ,\n$$\nwith $R = r_0 A^{1/3}$. Here $V_0$ is the positive well depth in $\\mathrm{MeV}$ (the potential itself is negative), $a$ is the diffuseness in $\\mathrm{fm}$, and $r_0$ is the radius parameter in $\\mathrm{fm}$. The volume integral $J_V$ is defined as the spatial integral of the potential over three-dimensional space,\n$$\nJ_V = \\int_{\\mathbb{R}^3} V(\\mathbf{r}) \\, d^3\\mathbf{r} \\, ,\n$$\nand by spherical symmetry this reduces to\n$$\nJ_V = 4\\pi \\int_{0}^{\\infty} V(r) \\, r^2 \\, dr \\, .\n$$\nFor an attractive potential $V(r) < 0$, the integral $J_V$ is negative. This value summarizes the integrated strength of the central real part of the optical potential and is widely used in parameter systematics and comparisons.\n\nTo compute $J_V$ numerically, we adopt the trapezoidal rule on a uniform grid in $r$. Defining the integrand $f(r) = V(r) r^2$, for a grid $r_k = k \\Delta r$ with $k = 0, 1, \\dots, N$ and $N \\Delta r = r_{\\max}$, the trapezoidal rule gives\n$$\n\\int_{0}^{r_{\\max}} f(r) \\, dr \\approx \\Delta r \\left[\\tfrac{1}{2} f(0) + \\sum_{k=1}^{N-1} f(r_k) + \\tfrac{1}{2} f(r_{\\max}) \\right] \\, .\n$$\nThe full volume integral is then computed by multiplying the radial quadrature by $4\\pi$. The truncation radius $r_{\\max}$ is chosen so that the contribution beyond $r_{\\max}$ is negligible to the specified tolerance, which is physically justified because the Woods–Saxon form decays exponentially as $r$ increases beyond the nuclear surface. The trapezoidal rule incurs a standard discretization error that scales as $\\mathcal{O}((\\Delta r)^2)$ for sufficiently smooth integrands, and we reflect this in the tolerances for the coarse grid test.\n\nFor the parameter search in $V_0$, we use the bisection method on the function\n$$\nF(V_0) = J_V(V_0) - J_T \\, ,\n$$\nwhere $J_T$ is the target volume integral. For the Woods–Saxon form, $J_V(V_0)$ is linearly proportional to $V_0$ because\n$$\nV(r; V_0) = -V_0 \\, g(r), \\quad g(r) \\equiv \\frac{1}{1 + \\exp\\left(\\frac{r - R}{a}\\right)} \\, ,\n$$\nso\n$$\nJ_V(V_0) = 4\\pi \\int_{0}^{\\infty} \\left[-V_0 \\, g(r)\\right] r^2 \\, dr = -V_0 \\, K \\, ,\n$$\nwith\n$$\nK = 4\\pi \\int_{0}^{\\infty} g(r) r^2 \\, dr > 0 \\, .\n$$\nThus $F(V_0)$ is strictly monotonic in $V_0$ for fixed geometry, guaranteeing that bisection will converge provided the bracket is chosen so that $F(V_{\\min})$ and $F(V_{\\max})$ have opposite signs. Although one could exploit linearity to compute $V_0$ in closed form as $V_0 = -J_T / K$, implementing bisection is more generally applicable when the potential includes additional nonlinearity or energy dependence, and it aligns with the goal of exercising parameter search algorithms.\n\nAlgorithmic steps:\n1. For each test case requiring $J_V$, compute $R = r_0 A^{1/3}$, construct the grid $r_k$ for $k = 0, \\dots, N$ with $N = r_{\\max}/\\Delta r$, evaluate $V(r_k)$, form $f(r_k) = V(r_k) r_k^2$, apply the trapezoidal rule, and multiply by $4\\pi$ to obtain $J_V$ in $\\mathrm{MeV \\, fm^3}$.\n2. For the parameter search case, define a bracket $[V_{\\min}, V_{\\max}]$ with $V_{\\min} = 1 \\, \\mathrm{MeV}$ and $V_{\\max} = 200 \\, \\mathrm{MeV}$, compute $F(V_{\\min})$ and $F(V_{\\max})$, and ensure they have opposite signs (if not, expand the bracket). Apply bisection:\n   - Compute $V_{\\text{mid}} = (V_{\\min} + V_{\\max}) / 2$.\n   - Evaluate $F(V_{\\text{mid}})$ via a trapezoidal computation of $J_V$.\n   - If $\\lvert F(V_{\\text{mid}}) \\rvert$ is below the tolerance (here $0.1 \\, \\mathrm{MeV \\, fm^3}$), stop and return $V_{\\text{mid}}$.\n   - Otherwise, replace the bracket endpoint that has the same sign as $F(V_{\\text{mid}})$ by $V_{\\text{mid}}$ and repeat.\n3. Round each reported output to three decimal places.\n\nScientific realism and consistency:\n- The choices $r_{\\max} = 12 \\, \\mathrm{fm}$ and $r_{\\max} = 15 \\, \\mathrm{fm}$ are reasonable given the exponential tail of the Woods–Saxon form with typical $a$ values in the range $0.6$–$0.7 \\, \\mathrm{fm}$, ensuring negligible tail contribution at the stated tolerances.\n- Using $\\Delta r = 0.05 \\, \\mathrm{fm}$ is sufficiently fine to achieve $1 \\, \\mathrm{MeV \\, fm^3}$ accuracy for the specified nuclei, as the integrand is smooth. The coarse case with $\\Delta r = 0.5 \\, \\mathrm{fm}$ is included to highlight discretization effects, and a looser tolerance is appropriate.\n- All quantities are expressed with correct units: $\\mathrm{fm}$ for radii, $\\mathrm{MeV}$ for energies, and $\\mathrm{MeV \\, fm^3}$ for volume integrals.\n\nThe final program will implement the above logic and produce a single line of output containing four comma-separated floating-point numbers enclosed in square brackets, corresponding to Tests $1$ through $4$ in order.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef woods_saxon_potential(r, A, r0, a, V0):\n    \"\"\"\n    Woods-Saxon central real potential:\n    V(r) = -V0 / (1 + exp((r - R)/a)), with R = r0 * A^(1/3).\n    r: numpy array of radii [fm]\n    A: mass number (dimensionless)\n    r0: radius parameter [fm]\n    a: diffuseness [fm]\n    V0: depth parameter [MeV], positive (potential itself is negative)\n    Returns V(r) in MeV as numpy array.\n    \"\"\"\n    R = r0 * (A ** (1.0 / 3.0))\n    return -V0 / (1.0 + np.exp((r - R) / a))\n\ndef volume_integral_JV(A, r0, a, V0, dr, rmax):\n    \"\"\"\n    Compute J_V = 4*pi * integral_0^{rmax} V(r) * r^2 dr via trapezoidal rule.\n    Units: J_V in MeV fm^3.\n    \"\"\"\n    # Ensure inclusion of rmax endpoint\n    # Using arange may omit rmax if rmax/dr not integer; use linspace instead.\n    N = int(np.round(rmax / dr))\n    r = np.linspace(0.0, rmax, N + 1)\n    V = woods_saxon_potential(r, A, r0, a, V0)\n    integrand = V * (r ** 2)\n    radial_integral = np.trapz(integrand, r)\n    JV = 4.0 * np.pi * radial_integral\n    return JV\n\ndef find_V0_for_target_JV(A, r0, a, J_target, dr, rmax, vmin=1.0, vmax=200.0, tol=0.1, max_iter=100):\n    \"\"\"\n    Find V0 such that volume_integral_JV(A, r0, a, V0, dr, rmax) ~= J_target using bisection.\n    vmin, vmax: bracket in MeV\n    tol: tolerance on |J_V - J_target| in MeV fm^3\n    Returns estimated V0 in MeV.\n    \"\"\"\n    def F(V0):\n        return volume_integral_JV(A, r0, a, V0, dr, rmax) - J_target\n\n    # Ensure bracketing condition: F(vmin) and F(vmax) must have opposite signs\n    fmin = F(vmin)\n    fmax = F(vmax)\n    # Expand bracket if necessary (simple strategy)\n    expand_factor = 2.0\n    expansion_count = 0\n    while fmin * fmax > 0 and expansion_count  10:\n        # Expand towards larger vmax to encompass target magnitude\n        vmax *= expand_factor\n        fmax = F(vmax)\n        expansion_count += 1\n\n    # If still not bracketed, try lowering vmin\n    if fmin * fmax > 0:\n        vmin = max(1e-6, vmin / expand_factor)\n        fmin = F(vmin)\n        expansion_count = 0\n        while fmin * fmax > 0 and expansion_count  10:\n            vmin = max(1e-6, vmin / expand_factor)\n            fmin = F(vmin)\n            expansion_count += 1\n\n    # If still not bracketed, fall back to direct linear estimate using K from V0=1 MeV\n    if fmin * fmax > 0:\n        K = -volume_integral_JV(A, r0, a, 1.0, dr, rmax)  # J_V = -V0*K => K = -J_V(V0=1)\n        V0_est = -J_target / K\n        return V0_est\n\n    # Bisection iterations\n    low, high = vmin, vmax\n    for _ in range(max_iter):\n        mid = 0.5 * (low + high)\n        fmid = F(mid)\n        if abs(fmid) = tol:\n            return mid\n        # Decide which subinterval to keep\n        if fmin * fmid  0:\n            high = mid\n            fmax = fmid\n        else:\n            low = mid\n            fmin = fmid\n    # If not converged within max_iter, return midpoint\n    return 0.5 * (low + high)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case is a tuple:\n    # ('J', {A, r0, a, V0, dr, rmax}) -> compute J_V\n    # ('search', {A, r0, a, J_target, dr, rmax}) -> find V0 matching target J_V\n    test_cases = [\n        ('J', {'A': 40, 'r0': 1.25, 'a': 0.65, 'V0': 50.0, 'dr': 0.05, 'rmax': 12.0}),\n        ('J', {'A': 208, 'r0': 1.25, 'a': 0.63, 'V0': 52.0, 'dr': 0.05, 'rmax': 15.0}),\n        ('J', {'A': 40, 'r0': 1.25, 'a': 0.65, 'V0': 50.0, 'dr': 0.5, 'rmax': 12.0}),\n        ('search', {'A': 40, 'r0': 1.25, 'a': 0.65, 'J_target': -20000.0, 'dr': 0.05, 'rmax': 12.0}),\n    ]\n\n    results = []\n    for kind, params in test_cases:\n        if kind == 'J':\n            JV = volume_integral_JV(\n                A=params['A'], r0=params['r0'], a=params['a'],\n                V0=params['V0'], dr=params['dr'], rmax=params['rmax']\n            )\n            results.append(JV)\n        elif kind == 'search':\n            V0_est = find_V0_for_target_JV(\n                A=params['A'], r0=params['r0'], a=params['a'],\n                J_target=params['J_target'], dr=params['dr'], rmax=params['rmax'],\n                vmin=1.0, vmax=200.0, tol=0.1, max_iter=200\n            )\n            results.append(V0_est)\n        else:\n            # Unknown test case type; append NaN\n            results.append(float('nan'))\n\n    # Round each result to three decimal places and format as strings.\n    formatted = [f\"{x:.3f}\" for x in results]\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3578612"}, {"introduction": "Advanced parameter searches rely on gradient-based optimizers like the Gauss-Newton and Levenberg-Marquardt (LM) algorithms. This practice [@problem_id:3578676] focuses on the core mathematical step of these methods: calculating the parameter update from the local gradient and approximated Hessian information. By solving for the update step directly, you will gain a concrete understanding of how the LM algorithm, with its characteristic damping parameter $\\lambda$, regularizes the search direction to ensure robust convergence.", "problem": "In fitting a two-parameter Woods–Saxon optical model to elastic-scattering data, consider the weighted nonlinear least-squares objective function $\\chi^{2}(\\boldsymbol{p}) = \\boldsymbol{r}(\\boldsymbol{p})^{T} W \\boldsymbol{r}(\\boldsymbol{p})$, where $\\boldsymbol{p} \\in \\mathbb{R}^{2}$ denotes a scaled, dimensionless parameter vector, $\\boldsymbol{r}(\\boldsymbol{p})$ is the residual vector (data minus model prediction evaluated at $\\boldsymbol{p}$), $W$ is a symmetric positive-definite weight matrix, and $J$ is the Jacobian of $\\boldsymbol{r}(\\boldsymbol{p})$ with respect to $\\boldsymbol{p}$. At the current iterate $\\boldsymbol{p}_{k}$, the available summaries of the local sensitivity information are\n$$\nJ^{T} W J \\;=\\; \\begin{pmatrix} 400  -120 \\\\ -120  50 \\end{pmatrix}, \n\\qquad\nJ^{T} W \\boldsymbol{r} \\;=\\; \\begin{pmatrix} -80 \\\\ 30 \\end{pmatrix}.\n$$\nAssuming the standard Gauss–Newton linearization for weighted least squares and the Levenberg–Marquardt (LM) damping with parameter $\\lambda = 10$ and scaling matrix $D = \\mathrm{diag}(1,1)$, determine:\n- the Gauss–Newton step $\\Delta \\boldsymbol{p}_{\\mathrm{GN}}$ at $\\boldsymbol{p}_{k}$, and\n- the Levenberg–Marquardt step $\\Delta \\boldsymbol{p}_{\\mathrm{LM}}$ at $\\boldsymbol{p}_{k}$.\n\nTreat the parameters as dimensionless for this computation. Provide exact values (no rounding). Report your final result as a single row matrix in the order $(\\Delta p_{\\mathrm{GN},1}, \\Delta p_{\\mathrm{GN},2}, \\Delta p_{\\mathrm{LM},1}, \\Delta p_{\\mathrm{LM},2})$.", "solution": "We start from the definition of the weighted nonlinear least-squares objective $\\chi^{2}(\\boldsymbol{p}) = \\boldsymbol{r}(\\boldsymbol{p})^{T} W \\boldsymbol{r}(\\boldsymbol{p})$, with gradient given by $\\nabla \\chi^{2}(\\boldsymbol{p}) = J^{T} W \\boldsymbol{r}(\\boldsymbol{p})$. A second-order Taylor model uses the exact Hessian $\\nabla^{2} \\chi^{2}(\\boldsymbol{p})$, but for least-squares problems a standard and well-tested approximation is the Gauss–Newton approximation,\n$$\n\\nabla^{2} \\chi^{2}(\\boldsymbol{p}) \\;\\approx\\; J^{T} W J,\n$$\nwhich neglects the second-order tensor of residuals. The Gauss–Newton step $\\Delta \\boldsymbol{p}_{\\mathrm{GN}}$ is defined as the solution of the linear system obtained by minimizing the quadratic model,\n$$\n\\left(J^{T} W J\\right) \\, \\Delta \\boldsymbol{p}_{\\mathrm{GN}} \\;=\\; - \\, J^{T} W \\boldsymbol{r}.\n$$\nWith the provided summaries,\n$$\nJ^{T} W J \\;=\\; \\begin{pmatrix} 400  -120 \\\\ -120  50 \\end{pmatrix}, \n\\qquad\nJ^{T} W \\boldsymbol{r} \\;=\\; \\begin{pmatrix} -80 \\\\ 30 \\end{pmatrix},\n$$\nthe Gauss–Newton step is\n$$\n\\Delta \\boldsymbol{p}_{\\mathrm{GN}} \\;=\\; - \\left(J^{T} W J\\right)^{-1} \\left(J^{T} W \\boldsymbol{r}\\right).\n$$\nWe compute $\\left(J^{T} W J\\right)^{-1}$. For \n$$\nM \\;=\\; \\begin{pmatrix} 400  -120 \\\\ -120  50 \\end{pmatrix},\n$$\nthe determinant is\n$$\n\\det(M) \\;=\\; 400 \\cdot 50 \\;-\\; (-120)\\cdot(-120) \\;=\\; 20000 \\;-\\; 14400 \\;=\\; 5600,\n$$\nand the inverse is\n$$\nM^{-1} \\;=\\; \\frac{1}{5600}\\begin{pmatrix} 50  120 \\\\ 120  400 \\end{pmatrix}.\n$$\nThus\n$$\nM^{-1} \\left(J^{T} W \\boldsymbol{r}\\right) \\;=\\; \\frac{1}{5600} \n\\begin{pmatrix} 50  120 \\\\ 120  400 \\end{pmatrix}\n\\begin{pmatrix} -80 \\\\ 30 \\end{pmatrix}\n\\;=\\; \\frac{1}{5600} \n\\begin{pmatrix} 50(-80) + 120(30) \\\\ 120(-80) + 400(30) \\end{pmatrix}\n\\;=\\; \\frac{1}{5600} \\begin{pmatrix} -400 \\\\ 2400 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} -\\frac{1}{14} \\\\ \\frac{3}{7} \\end{pmatrix}.\n$$\nTherefore,\n$$\n\\Delta \\boldsymbol{p}_{\\mathrm{GN}} \\;=\\; - \\begin{pmatrix} -\\frac{1}{14} \\\\ \\frac{3}{7} \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} \\frac{1}{14} \\\\ -\\frac{3}{7} \\end{pmatrix}.\n$$\n\nNext, the Levenberg–Marquardt (LM) method augments the Gauss–Newton system by a positive-semidefinite damping term to improve robustness. With damping parameter $\\lambda  0$ and scaling matrix $D$, the LM step $\\Delta \\boldsymbol{p}_{\\mathrm{LM}}$ solves\n$$\n\\left(J^{T} W J + \\lambda D^{T} D\\right)\\Delta \\boldsymbol{p}_{\\mathrm{LM}} \\;=\\; - \\, J^{T} W \\boldsymbol{r}.\n$$\nGiven $\\lambda = 10$ and $D = \\mathrm{diag}(1,1)$, we have $D^{T}D = I$ and\n$$\nJ^{T} W J + \\lambda I \\;=\\; \n\\begin{pmatrix} 400  -120 \\\\ -120  50 \\end{pmatrix}\n\\;+\\; 10 \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 410  -120 \\\\ -120  60 \\end{pmatrix}.\n$$\nLet \n$$\n\\widetilde{M} \\;=\\; \\begin{pmatrix} 410  -120 \\\\ -120  60 \\end{pmatrix}.\n$$\nIts determinant is\n$$\n\\det(\\widetilde{M}) \\;=\\; 410 \\cdot 60 \\;-\\; (-120)\\cdot(-120) \\;=\\; 24600 \\;-\\; 14400 \\;=\\; 10200,\n$$\nand its inverse is\n$$\n\\widetilde{M}^{-1} \\;=\\; \\frac{1}{10200}\\begin{pmatrix} 60  120 \\\\ 120  410 \\end{pmatrix}.\n$$\nHence\n$$\n\\widetilde{M}^{-1}\\left(J^{T} W \\boldsymbol{r}\\right) \\;=\\; \\frac{1}{10200}\n\\begin{pmatrix} 60  120 \\\\ 120  410 \\end{pmatrix}\n\\begin{pmatrix} -80 \\\\ 30 \\end{pmatrix}\n\\;=\\; \\frac{1}{10200} \\begin{pmatrix} 60(-80) + 120(30) \\\\ 120(-80) + 410(30) \\end{pmatrix}\n\\;=\\; \\frac{1}{10200} \\begin{pmatrix} -1200 \\\\ 2700 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} -\\frac{2}{17} \\\\ \\frac{9}{34} \\end{pmatrix}.\n$$\nTherefore,\n$$\n\\Delta \\boldsymbol{p}_{\\mathrm{LM}} \\;=\\; - \\begin{pmatrix} -\\frac{2}{17} \\\\ \\frac{9}{34} \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} \\frac{2}{17} \\\\ -\\frac{9}{34} \\end{pmatrix}.\n$$\n\nAs a consistency check, the damping parameter $\\lambda = 10$ yields a step whose components have reduced magnitude compared to the Gauss–Newton step, which is consistent with the regularizing effect of Levenberg–Marquardt.\n\nFinally, collating the components in the requested order $(\\Delta p_{\\mathrm{GN},1}, \\Delta p_{\\mathrm{GN},2}, \\Delta p_{\\mathrm{LM},1}, \\Delta p_{\\mathrm{LM},2})$ gives\n$$\n\\left( \\frac{1}{14}, \\; -\\frac{3}{7}, \\; \\frac{2}{17}, \\; -\\frac{9}{34} \\right).\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{14}  -\\frac{3}{7}  \\frac{2}{17}  -\\frac{9}{34}\\end{pmatrix}}$$", "id": "3578676"}, {"introduction": "The performance of gradient-based search algorithms is critically dependent on the accurate calculation of derivatives, a task where traditional finite-difference methods often fail due to numerical instability. This exercise [@problem_id:3578614] introduces the complex-step method, a powerful and elegant technique for computing analytical derivatives to near machine precision. By implementing and benchmarking this method, you will learn a robust alternative for obtaining the gradients essential for sophisticated optimization codes.", "problem": "You are tasked with implementing a robust gradient evaluation for an observable derived from a standard complex optical model potential used in nuclear reaction theory. The goal is to demonstrate how the complex-step derivative with step $h = i \\epsilon$ produces near-machine-precision partial derivatives for parameter search, and to quantitatively benchmark this against real-valued central differences.\n\nConsider the Woods–Saxon optical potential geometry factor\n$$\nf(r; R, a) = \\frac{1}{1 + \\exp\\!\\left(\\frac{r - R}{a}\\right)},\n$$\nand the complex optical potential\n$$\nV(r; \\boldsymbol{\\theta}) = -\\left(V_0 + i\\, W_0\\right)\\, f(r; R, a),\n$$\nwhere the parameter vector is $\\boldsymbol{\\theta} = [V_0, W_0, R, a]$, with $V_0$ and $W_0$ in megaelectronvolts (MeV), $R$ in femtometers (fm), and $a$ in femtometers (fm). Define the geometric volume integral\n$$\nI(R, a) = 4\\pi \\int_0^{r_{\\max}} f(r; R, a)\\, r^2\\, dr,\n$$\nwith $r_{\\max} = 20\\,\\mathrm{fm}$ and numerical quadrature performed by composite Simpson’s rule on a uniform grid of $N = 20001$ points over $[0, r_{\\max}]$. Because $f(r; R, a)$ is real for real $R$ and $a$, we may write, for any $\\boldsymbol{\\theta}$,\n$$\nJ_R(\\boldsymbol{\\theta}) = -V_0\\, I(R, a), \\qquad J_I(\\boldsymbol{\\theta}) = -W_0\\, I(R, a),\n$$\nwhich correspond to the volume integrals of the real and imaginary parts of $V(r;\\boldsymbol{\\theta})$.\n\nDefine the scalar objective function\n$$\nO(\\boldsymbol{\\theta}) = \\left[J_R(\\boldsymbol{\\theta}) - J_R^\\star\\right]^2 + \\left[J_I(\\boldsymbol{\\theta}) - J_I^\\star\\right]^2,\n$$\nwhere $J_R^\\star$ and $J_I^\\star$ are fixed target values constructed from a test-suite-specific target parameter vector $\\boldsymbol{\\theta}^\\star = [V_0^\\star, W_0^\\star, R^\\star, a^\\star]$ via\n$$\nI^\\star = I(R^\\star, a^\\star), \\quad J_R^\\star = -V_0^\\star I^\\star, \\quad J_I^\\star = -W_0^\\star I^\\star.\n$$\n\nYour program must:\n1. Implement $O(\\boldsymbol{\\theta})$ exactly as above, evaluating the integral $I(R,a)$ by composite Simpson’s rule on a uniform grid of $N=20001$ points in $[0, r_{\\max}]$ with $r_{\\max} = 20\\,\\mathrm{fm}$. All internal calculations must be performed in floating-point arithmetic consistent with the programming language’s default double precision. Angles are not involved. Distances must be treated in femtometers and energies in megaelectronvolts within the model; however, the final requested error metrics are dimensionless.\n2. Compute the gradient component $\\partial O/\\partial \\theta_i$ using the complex-step method: for each component index $i$, evaluate $O(\\boldsymbol{\\theta} + i\\,\\epsilon\\, \\mathbf{e}_i)$ with $\\epsilon = 10^{-30}$ and extract\n$$\n\\left.\\frac{\\partial O}{\\partial \\theta_i}\\right|_{\\text{CS}} \\approx \\frac{\\operatorname{Im}\\left[ O(\\boldsymbol{\\theta} + i\\,\\epsilon\\, \\mathbf{e}_i) \\right]}{\\epsilon}.\n$$\nThe code implementing $O(\\cdot)$ must be free of non-analytic operations with respect to its parameters; use only algebraic operations and the exponential as shown.\n3. Compute the central-difference approximation for the same gradient component using two real step scales $s \\in \\{10^{-6}, 10^{-8}\\}$ with parameter-wise steps $h_i = s \\cdot \\max(|\\theta_i|, 1)$:\n$$\n\\left.\\frac{\\partial O}{\\partial \\theta_i}\\right|_{\\text{FD}}(s) \\approx \\frac{O(\\boldsymbol{\\theta} + h_i \\mathbf{e}_i) - O(\\boldsymbol{\\theta} - h_i \\mathbf{e}_i)}{2 h_i}.\n$$\n4. For each $s$, compute the relative error of the central-difference derivative with respect to the complex-step derivative as\n$$\n\\mathrm{rel\\_err}_i(s) = \\frac{\\left| \\left.\\frac{\\partial O}{\\partial \\theta_i}\\right|_{\\text{FD}}(s) - \\left.\\frac{\\partial O}{\\partial \\theta_i}\\right|_{\\text{CS}} \\right|}{\\max\\!\\left(\\left|\\left.\\frac{\\partial O}{\\partial \\theta_i}\\right|_{\\text{CS}}\\right|, \\delta\\right)},\n$$\nwhere $\\delta = 10^{-20}$ is used to avoid division by zero. Report, for each test case and for each $s$, the maximum relative error across all components $i$:\n$$\nE_{\\max}(s) = \\max_i \\mathrm{rel\\_err}_i(s).\n$$\nThe reported error metrics $E_{\\max}(s)$ are dimensionless.\n\nTest Suite:\nUse exactly the following three test cases. For each case, $\\boldsymbol{\\theta}$ denotes the current parameter vector and $\\boldsymbol{\\theta}^\\star$ denotes the target parameter vector that defines $J_R^\\star$ and $J_I^\\star$.\n\n- Case $1$:\n  - $\\boldsymbol{\\theta} = [50, 10, 5.0, 0.60]$ with units $[\\,\\mathrm{MeV},\\,\\mathrm{MeV},\\,\\mathrm{fm},\\,\\mathrm{fm}\\,]$.\n  - $\\boldsymbol{\\theta}^\\star = [52, 11, 4.9, 0.62]$ with the same units.\n- Case $2$:\n  - $\\boldsymbol{\\theta} = [20, 5, 6.5, 1.20]$.\n  - $\\boldsymbol{\\theta}^\\star = [18, 6, 6.0, 1.00]$.\n- Case $3$:\n  - $\\boldsymbol{\\theta} = [70, 30, 4.0, 0.50]$.\n  - $\\boldsymbol{\\theta}^\\star = [68, 28, 4.2, 0.45]$.\n\nYour program must:\n- Implement the integral with $r_{\\max} = 20\\,\\mathrm{fm}$ and $N = 20001$ equally spaced points.\n- Use $\\epsilon = 10^{-30}$ for complex-step and $s \\in \\{10^{-6}, 10^{-8}\\}$ for central differences with $h_i = s \\cdot \\max(|\\theta_i|, 1)$.\n- For each case, compute and collect $E_{\\max}(10^{-6})$ and $E_{\\max}(10^{-8})$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the six results as a comma-separated list enclosed in square brackets in the following order:\n$$\n\\left[ E_{\\max}^{(1)}(10^{-6}),\\, E_{\\max}^{(1)}(10^{-8}),\\, E_{\\max}^{(2)}(10^{-6}),\\, E_{\\max}^{(2)}(10^{-8}),\\, E_{\\max}^{(3)}(10^{-6}),\\, E_{\\max}^{(3)}(10^{-8}) \\right],\n$$\nwhere the superscript indicates the test case index. These values are dimensionless floats. Do not print any additional text. The numeric values should be produced by your code; do not round or format them beyond the default floating-point string conversion of the programming language.", "solution": "The user's request is to develop a Python program that benchmarks the accuracy of numerical gradient approximations for a specific objective function used in computational nuclear physics. The benchmark compares a central finite-difference (FD) method against the complex-step (CS) derivative method, treating the latter as a high-precision reference.\n\n### **Problem Validation**\n\nThe problem statement has been meticulously reviewed against the established criteria.\n\n1.  **Givens Extraction**: All definitions, constants, and computational procedures have been extracted verbatim. This includes the functional forms of the Woods-Saxon factor $f(r;R,a)$ and the optical potential $V(r;\\boldsymbol{\\theta})$, the definitions of the volume integral $I(R,a)$ and objective function $O(\\boldsymbol{\\theta})$, all numerical parameters ($r_{\\max}=20$, $N=20001$, $\\epsilon=10^{-30}$, $s \\in \\{10^{-6}, 10^{-8}\\}$, $\\delta=10^{-20}$), the exact specifications for numerical quadrature (composite Simpson's rule) and derivative approximations (CS and FD), and the three test cases with specific parameter vectors $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\theta}^\\star$.\n2.  **Validation Check**: The problem is **valid**.\n    *   **Scientifically Grounded**: The problem uses standard models (Woods-Saxon optical potential) and techniques (parameter fitting via an objective function, numerical integration, numerical differentiation) from computational nuclear physics. It is scientifically sound and realistic.\n    *   **Well-Posed**: All required information is provided. The functions are mathematically well-defined, and the sequence of calculations leads to a unique set of results. The use of $\\delta$ to prevent division by zero is a good practice that enhances well-posedness.\n    *   **Objective**: The problem is stated using precise, unambiguous mathematical and computational language.\n    *   **Consistency and Completeness**: The setup is internally consistent and complete. For instance, the number of quadrature points $N=20001$ is odd, which is appropriate for the composite Simpson's rule over $N-1$ even intervals. The functions involved ($e^x$ and algebraic operations) are analytic, which is a critical requirement for the complex-step method.\n\nThe problem is a valid, well-defined exercise in scientific computing. The solution process may proceed.\n\n### **Methodology and Implementation**\n\nThe core of the solution is to implement the objective function $O(\\boldsymbol{\\theta})$ in a way that supports evaluation with both real and complex-valued parameters. This allows the same function to be used for both the finite-difference and complex-step methods.\n\n**1. Numerical Integration: The Volume Integral $I(R, a)$**\n\nThe geometric volume integral is defined as:\n$$\nI(R, a) = 4\\pi \\int_0^{r_{\\max}} \\frac{r^2}{1 + \\exp\\left(\\frac{r - R}{a}\\right)} dr\n$$\nThis integral is computed using the composite Simpson's rule. We first establish a uniform radial grid $r_k$ with $k=0, 1, \\dots, N-1$ from $r=0$ to $r=r_{\\max}=20\\,\\mathrm{fm}$ with $N=20001$ points. The integrand, $g(r) = f(r;R,a) r^2$, is evaluated at each grid point. The `scipy.integrate.simpson` function is ideally suited for this task, as it efficiently implements the composite Simpson's rule given the function values and grid points. To support the complex-step method, the implementation must handle complex-valued parameters $R$ and $a$. `NumPy`'s mathematical functions, such as `np.exp`, are overloaded to work correctly with complex numbers, making this straightforward.\n\n**2. The Objective Function $O(\\boldsymbol{\\theta})$**\n\nThe objective function is:\n$$\nO(\\boldsymbol{\\theta}) = \\left[J_R(\\boldsymbol{\\theta}) - J_R^\\star\\right]^2 + \\left[J_I(\\boldsymbol{\\theta}) - J_I^\\star\\right]^2\n$$\nwhere $J_R(\\boldsymbol{\\theta}) = -V_0 I(R, a)$ and $J_I(\\boldsymbol{\\theta}) = -W_0 I(R, a)$. The target values, distinguished by a star $(\\star)$, are constants for a given test case, computed using the target parameter vector $\\boldsymbol{\\theta}^\\star = [V_0^\\star, W_0^\\star, R^\\star, a^\\star]$.\n\nThe implementation of $O(\\boldsymbol{\\theta})$ involves:\n1.  Calculating the fixed, real-valued target integrals $J_R^\\star$ and $J_I^\\star$ from $\\boldsymbol{\\theta}^\\star$.\n2.  Calculating $J_R(\\boldsymbol{\\theta})$ and $J_I(\\boldsymbol{\\theta})$ using the current parameter vector $\\boldsymbol{\\theta}$. If $\\boldsymbol{\\theta}$ is complex (as in the complex-step method), then $I(R,a)$, $J_R$, and $J_I$ will be complex numbers.\n3.  Evaluating the final expression for $O(\\boldsymbol{\\theta})$. All arithmetic operations must correctly handle complex numbers.\n\n**3. Gradient Calculation: Complex-Step (CS) Method**\n\nThe complex-step method provides a numerically stable way to compute derivatives to near machine precision, avoiding the subtractive cancellation errors that plague finite-difference methods. The formula for the partial derivative of $O$ with respect to a parameter $\\theta_i$ is:\n$$\n\\frac{\\partial O}{\\partial \\theta_i} \\approx \\frac{\\operatorname{Im}\\left[ O(\\boldsymbol{\\theta} + i\\,\\epsilon\\, \\mathbf{e}_i) \\right]}{\\epsilon}\n$$\nHere, $\\mathbf{e}_i$ is the standard basis vector for the $i$-th parameter, and $\\epsilon$ is a very small real number (here, $\\epsilon = 10^{-30}$). For each parameter $\\theta_i$, we add a small imaginary perturbation $i\\epsilon$, evaluate the (now complex-valued) objective function $O$, and extract the derivative from its imaginary part. This result serves as our high-accuracy \"ground truth\" for evaluating the finite-difference method.\n\n**4. Gradient Calculation: Central Finite-Difference (FD) Method**\n\nThe central-difference approximation is a standard numerical technique:\n$$\n\\frac{\\partial O}{\\partial \\theta_i} \\approx \\frac{O(\\boldsymbol{\\theta} + h_i \\mathbf{e}_i) - O(\\boldsymbol{\\theta} - h_i \\mathbf{e}_i)}{2 h_i}\n$$\nThe step size $h_i$ is scaled by the magnitude of the parameter, $h_i = s \\cdot \\max(|\\theta_i|, 1)$, for two different scale factors $s \\in \\{10^{-6}, 10^{-8}\\}$. This method requires two real-valued function evaluations per partial derivative. Its accuracy is limited by a trade-off between truncation error (which decreases with $h_i$) and round-off error from subtractive cancellation (which increases as $h_i$ decreases).\n\n**5. Error Analysis**\n\nThe relative error of the FD approximation with respect to the CS result is computed for each parameter $\\theta_i$ and each scale factor $s$:\n$$\n\\mathrm{rel\\_err}_i(s) = \\frac{\\left| \\left.\\frac{\\partial O}{\\partial \\theta_i}\\right|_{\\text{FD}}(s) - \\left.\\frac{\\partial O}{\\partial \\theta_i}\\right|_{\\text{CS}} \\right|}{\\max\\!\\left(\\left|\\left.\\frac{\\partial O}{\\partial \\theta_i}\\right|_{\\text{CS}}\\right|, \\delta\\right)}\n$$\nThe denominator is floored at $\\delta = 10^{-20}$ to prevent division by small numbers. For each test case and each value of $s$, the final metric is the maximum relative error over all four parameters, $E_{\\max}(s) = \\max_i \\mathrm{rel\\_err}_i(s)$. The expected outcome is that the smaller FD step size ($s=10^{-8}$) leads to larger errors due to increased subtractive cancellation, demonstrating a key limitation of the method.\n\nThe final program structure iterates through the three test cases, performs these calculations for each, collects the six required $E_{\\max}$ values, and prints them in the specified format.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import simpson\n\ndef solve():\n    \"\"\"\n    Implements the full calculation pipeline as specified in the problem statement.\n    \"\"\"\n\n    # --- Problem Constants ---\n    R_MAX = 20.0\n    N_POINTS = 20001\n    EPSILON_CS = 1e-30\n    S_VALS = [1e-6, 1e-8]\n    DELTA = 1e-20\n\n    # --- Test Suite ---\n    test_cases = [\n        # Case 1\n        {'theta': np.array([50.0, 10.0, 5.0, 0.60]), 'theta_star': np.array([52.0, 11.0, 4.9, 0.62])},\n        # Case 2\n        {'theta': np.array([20.0, 5.0, 6.5, 1.20]), 'theta_star': np.array([18.0, 6.0, 6.0, 1.00])},\n        # Case 3\n        {'theta': np.array([70.0, 30.0, 4.0, 0.50]), 'theta_star': np.array([68.0, 28.0, 4.2, 0.45])},\n    ]\n\n    # --- Pre-calculate the radial grid ---\n    r_grid = np.linspace(0, R_MAX, N_POINTS)\n\n    # --- Helper Functions ---\n\n    def f_woods_saxon(r, R, a):\n        \"\"\"\n        Calculates the Woods-Saxon form factor. Handles complex R and a.\n        \"\"\"\n        # np.exp handles complex arguments correctly.\n        return 1.0 / (1.0 + np.exp((r - R) / a))\n\n    def calculate_I(R, a, r_grid_):\n        \"\"\"\n        Calculates the geometric volume integral I(R, a) using Simpson's rule.\n        Handles complex R and a.\n        \"\"\"\n        integrand = f_woods_saxon(r_grid_, R, a) * r_grid_**2\n        integral_val = simpson(integrand, r_grid_)\n        return 4.0 * np.pi * integral_val\n\n    def calculate_O(theta, theta_star, r_grid_):\n        \"\"\"\n        Calculates the objective function O(theta). Handles complex theta.\n        \"\"\"\n        V0, W0, R, a = theta\n        V0_star, W0_star, R_star, a_star = theta_star\n\n        # Calculate target values (always real)\n        I_star = np.real(calculate_I(R_star, a_star, r_grid_))\n        J_R_star = -V0_star * I_star\n        J_I_star = -W0_star * I_star\n\n        # Calculate current values (may be complex)\n        I_val = calculate_I(R, a, r_grid_)\n        J_R = -V0 * I_val\n        J_I = -W0 * I_val\n\n        # Calculate objective function\n        obj_func_val = (J_R - J_R_star)**2 + (J_I - J_I_star)**2\n        return obj_func_val\n\n    # --- Main Calculation Loop ---\n    final_results = []\n    for case in test_cases:\n        theta = case['theta']\n        theta_star = case['theta_star']\n        num_params = len(theta)\n\n        # 1. Compute Complex-Step (CS) gradient (the \"ground truth\")\n        grad_cs = np.zeros(num_params, dtype=np.float64)\n        theta_complex = theta.astype(np.complex128)\n        for i in range(num_params):\n            pert = np.zeros(num_params, dtype=np.complex128)\n            pert[i] = 1j * EPSILON_CS\n            O_perturbed = calculate_O(theta_complex + pert, theta_star, r_grid)\n            grad_cs[i] = np.imag(O_perturbed) / EPSILON_CS\n\n        # Loop over s values for Finite-Difference (FD) method\n        for s in S_VALS:\n            grad_fd = np.zeros(num_params, dtype=np.float64)\n            rel_errs = np.zeros(num_params, dtype=np.float64)\n\n            for i in range(num_params):\n                # 2. Compute Central-Difference gradient\n                h_i = s * max(abs(theta[i]), 1.0)\n                \n                pert_vec = np.zeros(num_params)\n                pert_vec[i] = h_i\n                \n                O_fwd = calculate_O(theta + pert_vec, theta_star, r_grid)\n                O_bwd = calculate_O(theta - pert_vec, theta_star, r_grid)\n                \n                # These results should be real, so take the real part to be safe\n                grad_fd[i] = (np.real(O_fwd) - np.real(O_bwd)) / (2.0 * h_i)\n                \n                # 3. Compute relative error vs CS gradient\n                numerator = abs(grad_fd[i] - grad_cs[i])\n                denominator = max(abs(grad_cs[i]), DELTA)\n                rel_errs[i] = numerator / denominator\n\n            # 4. Find max relative error\n            E_max = np.max(rel_errs)\n            final_results.append(E_max)\n\n    # --- Final Output ---\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3578614"}]}