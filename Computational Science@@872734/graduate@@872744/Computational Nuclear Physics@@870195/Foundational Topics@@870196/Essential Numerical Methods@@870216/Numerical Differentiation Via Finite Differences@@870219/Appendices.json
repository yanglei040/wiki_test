{"hands_on_practices": [{"introduction": "The foundation of numerical differentiation lies in understanding its two primary sources of error: truncation error from the mathematical approximation and rounding error from finite-precision arithmetic. This first exercise [@problem_id:3576284] provides a crucial hands-on analysis of these competing factors by comparing the simple forward-difference formula to the more accurate central-difference scheme. By investigating how each method behaves as the stepsize $h$ becomes very small, you will gain a practical intuition for the phenomenon of subtractive cancellation and see why higher-order accuracy in the truncation error is so desirable.", "problem": "Consider the numerical approximation of the first derivative of a smooth scalar function $f$ at a point $x$ by finite differences in double-precision floating-point arithmetic. Start from the mathematical definition of the derivative as a limit and the Taylor expansion of $f$ about $x$. In your reasoning, use the following well-tested facts as the foundational base: the derivative is defined by $f^{\\prime}(x)=\\lim_{h\\to 0}\\frac{f(x+h)-f(x)}{h}$, smooth functions admit a Taylor expansion with a remainder term, and floating-point arithmetic with unit roundoff (machine epsilon) $\\varepsilon_{\\text{mach}}$ introduces relative errors of order $\\varepsilon_{\\text{mach}}$ in basic operations. Do not assume or use any pre-derived finite-difference error formulas beyond what follows from these bases.\n\nYour task is to explain, in terms of truncation error from Taylor expansion and rounding error from floating-point arithmetic, why subtractive cancellation affects the one-sided forward-difference approximation more severely than the two-sided central-difference approximation for smooth $f$. The explanation should be based on how the competing truncation and rounding errors scale with the stepsize $h$, and on the fact that a smaller $h$ exacerbates subtraction of nearly equal numbers.\n\nThen, demonstrate the effect computationally for $f(x)=\\exp(x)$ at $x=0$ using double-precision arithmetic. Implement two discrete difference quotients derived directly from the limit definition of the derivative by replacing the limit with a finite increment:\n- the forward difference using $h$: $\\frac{f(x+h)-f(x)}{h}$,\n- the central difference using $h$: $\\frac{f(x+h)-f(x-h)}{2h}$.\n\nUse the true derivative value $f^{\\prime}(0)=1$ for error assessment, which follows from the elementary property of the exponential function. There are no physical units involved.\n\nTest Suite:\nCompute the approximations and errors for the following stepsizes $h$ at $x=0$:\n- $h=10^{-2}$ (a general case where truncation dominates over rounding for both schemes),\n- $h=10^{-8}$ (the specified case to showcase subtractive cancellation effects),\n- $h=10^{-16}$ (a boundary case approaching the scale of machine epsilon).\n\nFor each $h$ in the test suite, produce the following outputs:\n- the forward-difference approximation,\n- the central-difference approximation,\n- the absolute error of the forward difference relative to $1$,\n- the absolute error of the central difference relative to $1$,\n- a boolean indicating whether the central difference has strictly smaller absolute error than the forward difference.\n\nFinal Output Format:\nYour program should produce a single line of output containing a comma-separated list of items enclosed in square brackets. Each item corresponds to one $h$ in the order listed above and is itself a bracketed, comma-separated list:\n$[h,\\ \\text{forward\\_approx},\\ \\text{central\\_approx},\\ \\text{abs\\_error\\_forward},\\ \\text{abs\\_error\\_central},\\ \\text{central\\_better}]$.\nFor example, a syntactically valid shape is $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$. Do not include any spaces in the printed line.", "solution": "The problem requires an explanation of how truncation and rounding errors affect forward- and central-difference approximations for the first derivative, followed by a computational demonstration.\n\nThe total error in a numerical approximation is the sum of the truncation error and the rounding error. The truncation error, $E_T$, arises from approximating an infinite process (the limit definition of the derivative) with a finite one (using a small stepsize $h$). The rounding error, $E_R$, arises from the finite precision of floating-point arithmetic.\n\nLet the function being differentiated be $f(x)$, which is assumed to be smooth. For our analysis, we will use Taylor's theorem to expand $f$ around a point $x$. Floating-point arithmetic is characterized by the machine epsilon, $\\varepsilon_{\\text{mach}}$, which is the smallest number such that $\\text{fl}(1 + \\varepsilon_{\\text{mach}}) > 1$, where $\\text{fl}(\\cdot)$ denotes the floating-point representation. A floating-point operation on a value $y$ introduces a relative error, such that the computed value is $\\text{fl}(y) = y(1+\\delta)$, where $|\\delta| \\leq \\varepsilon_{\\text{mach}}$.\n\n**1. Forward-Difference Approximation**\n\nThe one-sided forward-difference formula is given by:\n$$\nD_f(h) = \\frac{f(x+h) - f(x)}{h}\n$$\n\n**Truncation Error ($E_{T,f}$):**\nWe use the Taylor expansion of $f(x+h)$ around $x$:\n$$\nf(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\frac{h^3}{6}f'''(\\xi)\n$$\nfor some $\\xi \\in (x, x+h)$. Substituting this into the formula for $D_f(h)$:\n$$\nD_f(h) = \\frac{\\left(f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + O(h^3)\\right) - f(x)}{h} = f'(x) + \\frac{h}{2}f''(x) + O(h^2)\n$$\nThe truncation error is the difference between the approximation and the true value:\n$$\nE_{T,f} = D_f(h) - f'(x) = \\frac{h}{2}f''(x) + O(h^2)\n$$\nThe truncation error is of order $O(h)$. As the stepsize $h$ decreases, this error decreases linearly.\n\n**Rounding Error ($E_{R,f}$):**\nIn floating-point arithmetic, we compute $\\hat{D}_f(h) = \\frac{\\text{fl}(f(x+h)) - \\text{fl}(f(x))}{h}$. Let $\\hat{f}(y) = \\text{fl}(f(y))$. Then $\\hat{f}(x+h) = f(x+h)(1+\\delta_1)$ and $\\hat{f}(x) = f(x)(1+\\delta_2)$, where $|\\delta_1|, |\\delta_2| \\leq \\varepsilon_{\\text{mach}}$.\nThe computed numerator is $\\hat{N}_f = \\text{fl}(\\hat{f}(x+h) - \\hat{f}(x))$. For small $h$, $f(x+h) \\approx f(x)$, representing a classic case of **subtractive cancellation**. The subtraction of two nearly equal numbers results in a loss of relative precision. The absolute error in the numerator is approximately $|f(x+h)\\delta_1 - f(x)\\delta_2| \\approx |f(x)(\\delta_1 - \\delta_2)|$, which is bounded by $2|f(x)|\\varepsilon_{\\text{mach}}$.\nThe rounding error in the final result is this numerator error divided by $h$:\n$$\n|E_{R,f}| \\approx \\frac{2|f(x)|\\varepsilon_{\\text{mach}}}{h}\n$$\nThe rounding error is of order $O(\\varepsilon_{\\text{mach}}/h)$. This error component grows as $h$ decreases.\n\n**Total Error:** The total absolute error for the forward difference is approximately $|E_{\\text{total},f}| \\approx |E_{T,f}| + |E_{R,f}| \\approx C_1 h + C_2 \\frac{\\varepsilon_{\\text{mach}}}{h}$, where $C_1 = |\\frac{1}{2}f''(x)|$ and $C_2 = 2|f(x)|$.\n\n**2. Central-Difference Approximation**\n\nThe two-sided central-difference formula is given by:\n$$\nD_c(h) = \\frac{f(x+h) - f(x-h)}{2h}\n$$\n\n**Truncation Error ($E_{T,c}$):**\nWe use Taylor expansions for both $f(x+h)$ and $f(x-h)$:\n$$\nf(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\frac{h^3}{6}f'''(x) + \\frac{h^4}{24}f^{(4)}(x) + O(h^5)\n$$\n$$\nf(x-h) = f(x) - hf'(x) + \\frac{h^2}{2}f''(x) - \\frac{h^3}{6}f'''(x) + \\frac{h^4}{24}f^{(4)}(x) - O(h^5)\n$$\nSubtracting the second expansion from the first cancels the even-powered terms:\n$$\nf(x+h) - f(x-h) = 2hf'(x) + \\frac{h^3}{3}f'''(x) + O(h^5)\n$$\nSubstituting this into the formula for $D_c(h)$:\n$$\nD_c(h) = \\frac{2hf'(x) + \\frac{h^3}{3}f'''(x) + O(h^5)}{2h} = f'(x) + \\frac{h^2}{6}f'''(x) + O(h^4)\n$$\nThe truncation error is therefore:\n$$\nE_{T,c} = D_c(h) - f'(x) = \\frac{h^2}{6}f'''(x) + O(h^4)\n$$\nThe truncation error is of order $O(h^2)$, which is quadratically small in $h$ and thus superior to the $O(h)$ error of the forward difference.\n\n**Rounding Error ($E_{R,c}$):**\nThe analysis is similar to the forward-difference case. The numerator involves the subtraction $f(x+h) - f(x-h)$, which is also subject to subtractive cancellation for small $h$. The absolute error in the computed numerator is approximately bounded by $2|f(x)|\\varepsilon_{\\text{mach}}$.\nThe rounding error in the final result is this error divided by $2h$:\n$$\n|E_{R,c}| \\approx \\frac{2|f(x)|\\varepsilon_{\\text{mach}}}{2h} = \\frac{|f(x)|\\varepsilon_{\\text{mach}}}{h}\n$$\nThe rounding error is also of order $O(\\varepsilon_{\\text{mach}}/h)$, the same as for the forward difference (differing only by a factor of approximately $2$).\n\n**3. Comparison and Conclusion**\n\nThe total absolute error for the central difference is approximately $|E_{\\text{total},c}| \\approx |E_{T,c}| + |E_{R,c}| \\approx K_1 h^2 + K_2 \\frac{\\varepsilon_{\\text{mach}}}{h}$, where $K_1 = |\\frac{1}{6}f'''(x)|$ and $K_2 = |f(x)|$.\n\nThe reason subtractive cancellation appears to affect the forward-difference method more severely lies in the interplay between its $O(h)$ truncation error and the $O(\\varepsilon_{\\text{mach}}/h)$ rounding error.\n- For a moderately small $h$, the $O(h^2)$ truncation error of the central-difference formula is significantly smaller than the $O(h)$ error of the forward-difference formula. The total error is therefore dominated by this much smaller truncation error, making the central difference inherently more accurate.\n- Both methods suffer from rounding error proportional to $1/h$ due to subtractive-cancellation. However, because the central-difference method starts with a much smaller truncation error, it can tolerate a smaller value of $h$ before the growing rounding error overwhelms the result.\n- The optimal stepsize $h^*$ that minimizes the total error is found by balancing the two error terms. For forward difference, this occurs when $C_1 h \\approx C_2 \\varepsilon_{\\text{mach}}/h$, yielding $h_{opt,f} \\propto \\sqrt{\\varepsilon_{\\text{mach}}}$. The minimum achievable error is $O(\\sqrt{\\varepsilon_{\\text{mach}}})$. For central difference, $K_1 h^2 \\approx K_2 \\varepsilon_{\\text{mach}}/h$, yielding $h_{opt,c} \\propto \\sqrt[3]{\\varepsilon_{\\text{mach}}}$. The minimum achievable error is $O(\\varepsilon_{\\text{mach}}^{2/3})$.\n- Since $\\varepsilon_{\\text{mach}}^{2/3}  \\varepsilon_{\\text{mach}}^{1/2}$ for $\\varepsilon_{\\text{mach}}  1$, the central-difference method can achieve a higher ultimate accuracy.\n\nIn conclusion, while the mechanism of subtractive cancellation causes a rounding error of order $O(\\varepsilon_{\\text{mach}}/h)$ in both methods, the central-difference method is \"less severely affected\" because its far superior truncation error ($O(h^2)$ vs. $O(h)$) means its total error remains smaller across a much wider and more practical range of stepsizes $h$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes forward and central difference approximations for the derivative of exp(x) at x=0\n    and analyzes their errors for different step sizes.\n    \"\"\"\n    x = 0.0\n    true_deriv = np.exp(x)  # which is 1.0\n\n    test_cases = [1e-2, 1e-8, 1e-16]\n    \n    results = []\n\n    for h in test_cases:\n        # np.float64 is used to ensure double-precision as specified.\n        h_val = np.float64(h)\n        x_val = np.float64(x)\n        \n        # Function evaluations\n        f_x = np.exp(x_val)\n        f_x_plus_h = np.exp(x_val + h_val)\n        f_x_minus_h = np.exp(x_val - h_val)\n        \n        # Forward difference approximation\n        # (f(x+h) - f(x)) / h\n        forward_approx = (f_x_plus_h - f_x) / h_val\n        \n        # Central difference approximation\n        # (f(x+h) - f(x-h)) / (2*h)\n        central_approx = (f_x_plus_h - f_x_minus_h) / (2.0 * h_val)\n        \n        # Absolute errors relative to the true value\n        abs_error_forward = np.abs(forward_approx - true_deriv)\n        abs_error_central = np.abs(central_approx - true_deriv)\n        \n        # Boolean check: is central difference more accurate?\n        central_better = bool(abs_error_central  abs_error_forward)\n        \n        # Collect results for this case\n        case_result = [\n            h_val,\n            forward_approx,\n            central_approx,\n            abs_error_forward,\n            abs_error_central,\n            central_better\n        ]\n        results.append(case_result)\n\n    # Format the final output string exactly as specified\n    output_parts = []\n    for r in results:\n        # Format: [h,forward_approx,central_approx,abs_error_forward,abs_error_central,central_better]\n        # str() on a boolean produces 'True' or 'False'\n        part = f\"[{r[0]},{r[1]},{r[2]},{r[3]},{r[4]},{str(r[5]).lower()}]\"\n        output_parts.append(part)\n        \n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```", "id": "3576284"}, {"introduction": "Having established the importance of higher-order methods, we now explore a powerful technique to construct them: Richardson extrapolation. This practice [@problem_id:3576238] guides you through the process of combining two lower-order approximations to create a single, much more accurate higher-order one. By applying this method and empirically measuring the convergence rate, you will verify the theoretical accuracy gains and develop a key skill for validating and improving numerical algorithms.", "problem": "In computational nuclear physics, high-fidelity spatial derivatives computed on finite grids underpin the accuracy of transport, structure, and response calculations, for example when discretizing radial derivatives in the Schrödinger equation for nucleons under an effective potential or evaluating gradients of energy density functionals. A rigorous way to validate a finite-difference differentiation operator is to compare it against a known analytic derivative and to study its convergence order as the grid spacing is refined, using only fundamental definitions and series expansions.\n\nStarting from the definition of the derivative as a limit, namely $f^{\\prime}(x) = \\lim_{h \\to 0} \\dfrac{f(x+h) - f(x)}{h}$, and the Taylor series expansion of a sufficiently smooth function $f(x)$ about a point, design a finite-difference approximation to $f^{\\prime}(x)$ that is accurate to leading order $O(h^2)$ using a symmetric stencil. Then, using an extrapolation technique that eliminates the leading truncation error between two such $O(h^2)$ approximations at different step sizes, derive and implement a higher-accuracy approximation to $f^{\\prime}(x)$ whose truncation error scales as $O(h^4)$. Do not use any pre-memorized stencil coefficients or pre-assembled extrapolation formulas; derive all required expressions explicitly from the Taylor series.\n\nImplement both the baseline second-order-accurate symmetric approximation and the fourth-order-accurate extrapolated approximation in a program and apply them to the analytic test function $f(x) = \\exp(x)$ at $x = 0$, for which the exact derivative is $f^{\\prime}(0) = \\exp(0) = 1$. Work in pure mathematics without introducing physical units. All angles, if any, are to be considered in radians.\n\nYour program must compute the following test suite, covering general performance, convergence order confirmation, and numerical edge behavior:\n\n- Test case A (happy path, error reduction at a moderate grid spacing): With $h = 0.1$, compute the absolute error $E_{\\mathrm{C}}(h)$ of the symmetric second-order approximation and the absolute error $E_{\\mathrm{R}}(h)$ of the fourth-order extrapolated approximation. Report the error reduction factor defined by $R_{0.1} = E_{\\mathrm{C}}(0.1) / E_{\\mathrm{R}}(0.1)$ as a float.\n\n- Test case B (observed convergence order of the second-order method): Using $h$ values $h_{1} = 0.1$, $h_{2} = 0.05$, and $h_{3} = 0.025$, compute the absolute errors $E_{\\mathrm{C}}(h_{i})$ for $i \\in \\{1,2,3\\}$. Compute the observed orders $p_{12} = \\dfrac{\\log\\left(E_{\\mathrm{C}}(h_{1}) / E_{\\mathrm{C}}(h_{2})\\right)}{\\log\\left(h_{1}/h_{2}\\right)}$ and $p_{23} = \\dfrac{\\log\\left(E_{\\mathrm{C}}(h_{2}) / E_{\\mathrm{C}}(h_{3})\\right)}{\\log\\left(h_{2}/h_{3}\\right)}$, and return their average $\\bar{p}_{\\mathrm{C}} = \\dfrac{p_{12} + p_{23}}{2}$ as a float.\n\n- Test case C (observed convergence order of the fourth-order method): Using the same $h$ values $h_{1} = 0.1$, $h_{2} = 0.05$, and $h_{3} = 0.025$, compute the absolute errors $E_{\\mathrm{R}}(h_{i})$ for $i \\in \\{1,2,3\\}$. Compute the observed orders $q_{12} = \\dfrac{\\log\\left(E_{\\mathrm{R}}(h_{1}) / E_{\\mathrm{R}}(h_{2})\\right)}{\\log\\left(h_{1}/h_{2}\\right)}$ and $q_{23} = \\dfrac{\\log\\left(E_{\\mathrm{R}}(h_{2}) / E_{\\mathrm{R}}(h_{3})\\right)}{\\log\\left(h_{2}/h_{3}\\right)}$, and return their average $\\bar{q}_{\\mathrm{R}} = \\dfrac{q_{12} + q_{23}}{2}$ as a float. This confirms the expected $O(h^4)$ behavior.\n\n- Test case D (round-off dominated edge case): With a very small step $h = 10^{-10}$, compute $E_{\\mathrm{C}}(h)$ and $E_{\\mathrm{R}}(h)$ and return the boolean $B_{\\mathrm{small}}$ indicating whether the extrapolated method still improves over the non-extrapolated symmetric method, i.e., $B_{\\mathrm{small}}$ is true if $E_{\\mathrm{R}}(10^{-10})  E_{\\mathrm{C}}(10^{-10})$, otherwise false.\n\n- Test case E (coarse grid behavior): With $h = 1.0$, compute the error reduction factor $R_{1.0} = E_{\\mathrm{C}}(1.0) / E_{\\mathrm{R}}(1.0)$ as a float.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the exact order $[R_{0.1}, \\bar{p}_{\\mathrm{C}}, \\bar{q}_{\\mathrm{R}}, B_{\\mathrm{small}}, R_{1.0}]$ with no additional text.", "solution": "The user wants me to solve a problem in numerical differentiation.\n\nThe problem statement has been validated and found to be scientifically grounded, well-posed, and objective. It is a standard exercise in numerical analysis, requiring the derivation and implementation of finite difference formulas from first principles.\n\n**1. Derivation of the Second-Order Symmetric Finite Difference Formula**\n\nOur goal is to approximate the first derivative, $f^{\\prime}(x)$, using function evaluations at points symmetric around $x$. We begin with the Taylor series expansion of a sufficiently smooth function $f(x)$ expanded about the point $x$.\n\nThe expansions for $f(x+h)$ and $f(x-h)$ are:\n$$f(x+h) = f(x) + hf^{\\prime}(x) + \\frac{h^2}{2!}f^{\\prime\\prime}(x) + \\frac{h^3}{3!}f^{\\prime\\prime\\prime}(x) + \\frac{h^4}{4!}f^{(4)}(x) + \\frac{h^5}{5!}f^{(5)}(x) + O(h^6) \\quad (1)$$\n$$f(x-h) = f(x) - hf^{\\prime}(x) + \\frac{h^2}{2!}f^{\\prime\\prime}(x) - \\frac{h^3}{3!}f^{\\prime\\prime\\prime}(x) + \\frac{h^4}{4!}f^{(4)}(x) - \\frac{h^5}{5!}f^{(5)}(x) + O(h^6) \\quad (2)$$\n\nTo isolate the $f^{\\prime}(x)$ term, we subtract equation $(2)$ from equation $(1)$. The even-powered derivative terms ($f(x)$, $f^{\\prime\\prime}(x)$, $f^{(4)}(x)$, etc.) cancel out:\n$$f(x+h) - f(x-h) = 2hf^{\\prime}(x) + 2\\frac{h^3}{3!}f^{\\prime\\prime\\prime}(x) + 2\\frac{h^5}{5!}f^{(5)}(x) + O(h^7)$$\n\nSolving for $f^{\\prime}(x)$ yields:\n$$f^{\\prime}(x) = \\frac{f(x+h) - f(x-h)}{2h} - \\frac{h^2}{6}f^{\\prime\\prime\\prime}(x) - \\frac{h^4}{120}f^{(5)}(x) - O(h^6)$$\n\nThis gives us the symmetric, or central, finite difference approximation for the first derivative, which we will denote as $D_{\\mathrm{C}}(h)$:\n$$D_{\\mathrm{C}}(h) = \\frac{f(x+h) - f(x-h)}{2h}$$\n\nThe expression for $f^{\\prime}(x)$ can be written as an approximation plus an error term:\n$$f^{\\prime}(x) = D_{\\mathrm{C}}(h) + E_{\\text{trunc}}(h)$$\nwhere the truncation error, $E_{\\text{trunc}}(h)$, is given by the series:\n$$E_{\\text{trunc}}(h) = \\frac{h^2}{6}f^{\\prime\\prime\\prime}(x) + \\frac{h^4}{120}f^{(5)}(x) + O(h^6)$$\n\nSince the leading term of the error is proportional to $h^2$, this is a second-order accurate method, denoted as $O(h^2)$. For the purposes of extrapolation, we express the true value $f^{\\prime}(x)$ as:\n$$f^{\\prime}(x) = D_{\\mathrm{C}}(h) + c_2 h^2 + c_4 h^4 + \\dots$$\nwhere the coefficients $c_2 = \\frac{1}{6}f^{\\prime\\prime\\prime}(x)$, $c_4 = \\frac{1}{120}f^{(5)}(x)$, etc., are constants that depend on the higher derivatives of $f(x)$ at the point $x$, but not on the step size $h$.\n\n**2. Derivation of the Fourth-Order Extrapolated Formula**\n\nRichardson extrapolation is a technique to construct a higher-order approximation by combining lower-order approximations computed at different step sizes. Here, we will combine two second-order approximations to obtain a fourth-order one.\n\nWe start with the error expansion for the central difference approximation $D_{\\mathrm{C}}$ at two different step sizes, $h$ and $h/2$:\n$$f^{\\prime}(x) = D_{\\mathrm{C}}(h) + c_2 h^2 + c_4 h^4 + O(h^6) \\quad (3)$$\n$$f^{\\prime}(x) = D_{\\mathrm{C}}(h/2) + c_2 \\left(\\frac{h}{2}\\right)^2 + c_4 \\left(\\frac{h}{2}\\right)^4 + O(h^6) = D_{\\mathrm{C}}(h/2) + \\frac{1}{4}c_2 h^2 + \\frac{1}{16}c_4 h^4 + O(h^6) \\quad (4)$$\n\nOur objective is to find a linear combination of equations $(3)$ and $(4)$ that eliminates the leading error term, which is proportional to $c_2 h^2$. We multiply equation $(4)$ by $4$:\n$$4f^{\\prime}(x) = 4D_{\\mathrm{C}}(h/2) + c_2 h^2 + \\frac{4}{16}c_4 h^4 + O(h^6) \\quad (5)$$\n\nNow, we subtract equation $(3)$ from equation $(5)$:\n$$4f^{\\prime}(x) - f^{\\prime}(x) = (4D_{\\mathrm{C}}(h/2) - D_{\\mathrm{C}}(h)) + (c_2 h^2 - c_2 h^2) + \\left(\\frac{1}{4}c_4 h^4 - c_4 h^4\\right) + O(h^6)$$\n$$3f^{\\prime}(x) = 4D_{\\mathrm{C}}(h/2) - D_{\\mathrm{C}}(h) - \\frac{3}{4}c_4 h^4 + O(h^6)$$\n\nSolving for $f^{\\prime}(x)$ gives:\n$$f^{\\prime}(x) = \\frac{4D_{\\mathrm{C}}(h/2) - D_{\\mathrm{C}}(h)}{3} - \\frac{1}{4}c_4 h^4 + O(h^6)$$\n\nThis gives us a new, higher-order approximation, which we will denote as $D_{\\mathrm{R}}(h)$:\n$$D_{\\mathrm{R}}(h) = \\frac{4D_{\\mathrm{C}}(h/2) - D_{\\mathrm{C}}(h)}{3}$$\n\nThe truncation error of this extrapolated approximation is of order $O(h^4)$, making it a fourth-order accurate method.\n\n**3. Implementation and Test Case Analysis**\n\nThe problem requires applying these formulas to differentiate the function $f(x) = \\exp(x)$ at the point $x=0$. The exact derivative is $f^{\\prime}(0) = \\exp(0) = 1$. The absolute errors are defined as $E_{\\mathrm{C}}(h) = |D_{\\mathrm{C}}(h) - 1|$ and $E_{\\mathrm{R}}(h) = |D_{\\mathrm{R}}(h) - 1|$.\n\n- **Test case A ($h=0.1$):** We will compute $E_{\\mathrm{C}}(0.1)$ and $E_{\\mathrm{R}}(0.1)$. The calculation of $D_{\\mathrm{R}}(0.1)$ requires both $D_{\\mathrm{C}}(0.1)$ and $D_{\\mathrm{C}}(0.05)$. The ratio $R_{0.1} = E_{\\mathrm{C}}(0.1) / E_{\\mathrm{R}}(0.1)$ measures the improvement in accuracy gained by extrapolation.\n\n- **Test case B (Convergence of $D_{\\mathrm{C}}$):** The observed order of convergence $p$ for a method with error $E(h) \\approx C h^p$ can be computed using errors at two step sizes, $h_1$ and $h_2$, as $p = \\log(E(h_1)/E(h_2)) / \\log(h_1/h_2)$. We will calculate this for two pairs of step sizes $(h_1, h_2)=(0.1, 0.05)$ and $(h_2, h_3)=(0.05, 0.025)$ and average the results. Since $D_{\\mathrm{C}}$ is an $O(h^2)$ method, the result $\\bar{p}_{\\mathrm{C}}$ is expected to be close to $2$.\n\n- **Test case C (Convergence of $D_{\\mathrm{R}}$):** We repeat the convergence order calculation for the extrapolated method $D_{\\mathrm{R}}$. Since it is an $O(h^4)$ method, the result $\\bar{q}_{\\mathrm{R}}$ is expected to be close to $4$.\n\n- **Test case D ($h=10^{-10}$):** This case explores the regime where numerical round-off error dominates truncation error. The total error of a finite difference formula is a sum of truncation error, which decreases with $h$, and round-off error, which is amplified by small $h$ (typically as $O(\\epsilon_{\\text{machine}}/h)$). For very small $h$, the higher-order extrapolated method, which combines more terms, can suffer from greater round-off error amplification than the simpler central difference method. We will check if $E_{\\mathrm{R}}(10^{-10})  E_{\\mathrm{C}}(10^{-10})$.\n\n- **Test case E ($h=1.0$):** This case examines behavior on a coarse grid, where truncation error is large. We expect the higher-order method to provide a substantial improvement, so the error reduction factor $R_{1.0}$ should be significant.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies second- and fourth-order finite difference schemes\n    to a test function, and computes a suite of validation metrics.\n    \"\"\"\n\n    # Define the analytic test function and its properties\n    f = np.exp\n    x = 0.0\n    exact_derivative = np.exp(x)\n\n    def central_diff(fun, x_val, h_val):\n        \"\"\"Computes the second-order symmetric finite difference approximation.\"\"\"\n        return (fun(x_val + h_val) - fun(x_val - h_val)) / (2.0 * h_val)\n\n    def extrapolated_diff(fun, x_val, h_val):\n        \"\"\"Computes the fourth-order extrapolated finite difference approximation.\"\"\"\n        # This formula combines two second-order approximations at h and h/2\n        # to yield a more accurate result.\n        d_c_h = central_diff(fun, x_val, h_val)\n        d_c_h_half = central_diff(fun, x_val, h_val / 2.0)\n        return (4.0 * d_c_h_half - d_c_h) / 3.0\n\n    results = []\n\n    # Test Case A: Happy path, error reduction at a moderate grid spacing\n    h_a = 0.1\n    # Error for the Central difference (second-order) method\n    e_c_a = np.abs(central_diff(f, x, h_a) - exact_derivative)\n    # Error for the Richardson extrapolated (fourth-order) method\n    e_r_a = np.abs(extrapolated_diff(f, x, h_a) - exact_derivative)\n    r_0_1 = e_c_a / e_r_a\n    results.append(r_0_1)\n\n    # Test Case B: Observed convergence order of the second-order method\n    h_b1, h_b2, h_b3 = 0.1, 0.05, 0.025\n    e_c_b1 = np.abs(central_diff(f, x, h_b1) - exact_derivative)\n    e_c_b2 = np.abs(central_diff(f, x, h_b2) - exact_derivative)\n    e_c_b3 = np.abs(central_diff(f, x, h_b3) - exact_derivative)\n    \n    p12_c = np.log(e_c_b1 / e_c_b2) / np.log(h_b1 / h_b2)\n    p23_c = np.log(e_c_b2 / e_c_b3) / np.log(h_b2 / h_b3)\n    p_bar_c = (p12_c + p23_c) / 2.0\n    results.append(p_bar_c)\n\n    # Test Case C: Observed convergence order of the fourth-order method\n    h_c1, h_c2, h_c3 = 0.1, 0.05, 0.025\n    e_r_c1 = np.abs(extrapolated_diff(f, x, h_c1) - exact_derivative)\n    e_r_c2 = np.abs(extrapolated_diff(f, x, h_c2) - exact_derivative)\n    e_r_c3 = np.abs(extrapolated_diff(f, x, h_c3) - exact_derivative)\n    \n    q12_r = np.log(e_r_c1 / e_r_c2) / np.log(h_c1 / h_c2)\n    q23_r = np.log(e_r_c2 / e_r_c3) / np.log(h_c2 / h_c3)\n    q_bar_r = (q12_r + q23_r) / 2.0\n    results.append(q_bar_r)\n\n    # Test Case D: Round-off dominated edge case\n    h_d = 1e-10\n    e_c_d = np.abs(central_diff(f, x, h_d) - exact_derivative)\n    e_r_d = np.abs(extrapolated_diff(f, x, h_d) - exact_derivative)\n    b_small = e_r_d  e_c_d\n    results.append(b_small)\n\n    # Test Case E: Coarse grid behavior\n    h_e = 1.0\n    e_c_e = np.abs(central_diff(f, x, h_e) - exact_derivative)\n    e_r_e = np.abs(extrapolated_diff(f, x, h_e) - exact_derivative)\n    r_1_0 = e_c_e / e_r_e\n    results.append(r_1_0)\n\n    # Final print statement in the exact required format\n    # Convert boolean to lowercase 'true'/'false' as per convention\n    final_results_str = [str(r).lower() if isinstance(r, bool) else str(r) for r in results]\n    print(f\"[{','.join(final_results_str)}]\")\n\nsolve()\n```", "id": "3576238"}, {"introduction": "Real-world nuclear physics problems are rarely one-dimensional, requiring us to generalize our methods to three-dimensional space. This final practice [@problem_id:3576233] makes that leap by tasking you with constructing the seven-point finite-difference stencil for the Laplacian operator, $\\nabla^2$, a cornerstone of equations describing phenomena from neutron diffusion to nuclear structure. You will implement this discrete operator on a 3D grid and verify its second-order accuracy, a foundational step in building robust solvers for partial differential equations.", "problem": "You are to derive, implement, and verify a second-order accurate finite-difference discretization of the three-dimensional Laplacian operator using a seven-point stencil, then measure the discrete error and demonstrate second-order convergence under grid refinement. This exercise is motivated by computational treatments of partial differential equations in computational nuclear physics, such as neutron diffusion or mean-field equations, where accurate and efficient approximations to the Laplacian are critical.\n\nStart from the fundamental definition that the Laplacian of a sufficiently smooth scalar field $f(x,y,z)$ is the sum of its unmixed second partial derivatives,\n$$ \\nabla^{2} f = \\frac{\\partial^{2} f}{\\partial x^{2}} + \\frac{\\partial^{2} f}{\\partial y^{2}} + \\frac{\\partial^{2} f}{\\partial z^{2}} $$\nOn a structured Cartesian grid with uniform spacings $h_{x}$, $h_{y}$, and $h_{z}$ along the $x$, $y$, and $z$ directions, respectively, the seven-point stencil approximation uses only nearest-neighbor samples in the coordinate-aligned directions. Derive a consistent, second-order accurate, central-difference discretization for each second derivative using Taylor series expansions and then combine them to form a discrete Laplacian acting on interior grid points. For boundary treatment, impose Dirichlet boundary conditions by sampling the exact analytical function at the domain boundary.\n\nConsider the smooth test function $f(x,y,z) = \\sin x \\,\\cos y \\,\\exp z$, which is representative of separable angular and radial dependences encountered in simplified models of nuclear systems. Angles must be interpreted in radians. Derive the exact Laplacian $\\nabla^{2} f$ analytically for this $f$, and use it as the reference to assess discretization error.\n\nDefine the discrete $L^{2}$ error norm on the interior grid as follows. Let the domain be a rectangular box $[0,L_{x}]\\times[0,L_{y}]\\times[0,L_{z}]$ partitioned by $N_{x}$, $N_{y}$, $N_{z}$ grid points in each direction (including boundaries), with $h_{x} = L_{x}/(N_{x}-1)$, $h_{y} = L_{y}/(N_{y}-1)$, and $h_{z} = L_{z}/(N_{z}-1)$. Let $\\mathcal{I}$ denote the set of interior indices $(i,j,k)$ with $1 \\le i \\le N_{x}-2$, $1 \\le j \\le N_{y}-2$, $1 \\le k \\le N_{z}-2$. If $\\mathcal{L}_{h} f$ denotes your discrete Laplacian applied to the grid samples of $f$, and $(\\nabla^{2}f)_{\\text{exact}}$ denotes the exact analytical Laplacian evaluated at grid nodes, define\n$$\nE_{2}(h_{x},h_{y},h_{z}) \\equiv \\left( \\sum_{(i,j,k)\\in \\mathcal{I}} \\left[ \\mathcal{L}_{h} f - (\\nabla^{2}f)_{\\text{exact}} \\right]^{2} \\, h_{x} h_{y} h_{z} \\right)^{1/2}\n$$\nDemonstrate second-order convergence by refining the grid in each coordinate direction by a factor of two (which halves each spacing). Given a base grid with $(N_{x},N_{y},N_{z})$, define the refined grid by $(N'_{x},N'_{y},N'_{z})$ with\n$$\nN'_{x} = 2\\,(N_{x}-1) + 1,\\quad N'_{y} = 2\\,(N_{y}-1) + 1,\\quad N'_{z} = 2\\,(N_{z}-1) + 1,\n$$\nso that $h'_{x} = h_{x}/2$, $h'_{y} = h_{y}/2$, $h'_{z} = h_{z}/2$. For each test case, compute the observed order of convergence\n$$\np \\equiv \\log_{2} \\left( \\frac{E_{2}(h_{x},h_{y},h_{z})}{E_{2}(h'_{x},h'_{y},h'_{z})} \\right)\n$$\n\nImplement a complete, runnable program that:\n- Constructs the grids and evaluates $f$ and $(\\nabla^{2}f)_{\\text{exact}}$ at all nodes.\n- Enforces Dirichlet boundary conditions by setting boundary values of $f$ to the exact $f$.\n- Applies your seven-point discrete Laplacian $\\mathcal{L}_{h}$ to interior nodes only.\n- Computes $E_{2}(h_{x},h_{y},h_{z})$ and $E_{2}(h'_{x},h'_{y},h'_{z})$ for each test case.\n- Reports the observed order $p$ for each test case.\n\nAngles must be in radians. There are no physical unit conversions required beyond stating the angles in radians.\n\nUse the following test suite to exercise general and edge-case behavior:\n- Test case A (happy path, cubic-ish grid): $L_{x}=\\pi$, $L_{y}=\\pi/2$, $L_{z}=1$, with base $(N_{x},N_{y},N_{z})=(17,17,17)$ and refined $(33,33,33)$.\n- Test case B (anisotropic spacings and non-cubic counts): $L_{x}=\\pi$, $L_{y}=\\pi$, $L_{z}=1/2$, with base $(N_{x},N_{y},N_{z})=(18,14,10)$ and refined $(35,27,19)$.\n- Test case C (moderate, strongly anisotropic domain extents): $L_{x}=2$, $L_{y}=1$, $L_{z}=1/4$, with base $(N_{x},N_{y},N_{z})=(5,7,9)$ and refined $(9,13,17)$.\n\nYour program must produce a single line of output containing the observed orders $p$ for the three test cases, in the order A, B, C, as a comma-separated list enclosed in square brackets. For reproducibility and comparison, round each reported $p$ to $6$ decimal places. For example, the output format must be of the form\n[p_A,p_B,p_C]\nwhere each entry is a floating-point number with $6$ digits after the decimal point.", "solution": "The problem requires the derivation, implementation, and verification of a second-order accurate finite-difference scheme for the three-dimensional Laplacian operator, $\\nabla^2$. The verification will be performed by measuring the convergence rate of the discretization error for a known smooth function.\n\n### 1. Derivation of the Seven-Point Finite-Difference Stencil\n\nThe Laplacian of a scalar field $f(x,y,z)$ is defined as $\\nabla^2 f = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2} + \\frac{\\partial^2 f}{\\partial z^2}$. We seek a discrete approximation for this operator on a Cartesian grid with points $(x_i, y_j, z_k)$, where $x_i = i h_x$, $y_j = j h_y$, and $z_k = k h_z$ for uniform grid spacings $h_x, h_y, h_z$.\n\nThe core of the method is the central-difference approximation for the second derivative. Consider a one-dimensional function $u(x)$. Its Taylor series expansions around a point $x$ are:\n$$ u(x+h) = u(x) + h u'(x) + \\frac{h^2}{2!} u''(x) + \\frac{h^3}{3!} u'''(x) + \\frac{h^4}{4!} u^{(4)}(x) + O(h^5) $$\n$$ u(x-h) = u(x) - h u'(x) + \\frac{h^2}{2!} u''(x) - \\frac{h^3}{3!} u'''(x) + \\frac{h^4}{4!} u^{(4)}(x) - O(h^5) $$\nAdding these two expressions eliminates the odd-powered derivative terms:\n$$ u(x+h) + u(x-h) = 2u(x) + h^2 u''(x) + \\frac{h^4}{12} u^{(4)}(x) + O(h^6) $$\nSolving for $u''(x)$ yields:\n$$ u''(x) = \\frac{u(x+h) - 2u(x) + u(x-h)}{h^2} - \\frac{h^2}{12} u^{(4)}(x) + O(h^4) $$\nThe finite-difference approximation is the first term on the right-hand side. The truncation error is $-\\frac{h^2}{12}u^{(4)}(x) + O(h^4)$, which is of order $O(h^2)$. The approximation is therefore second-order accurate.\n\nTo discretize the three-dimensional Laplacian, we apply this formula to each second partial derivative. Let $f_{i,j,k} = f(x_i, y_j, z_k)$.\n$$ \\left.\\frac{\\partial^2 f}{\\partial x^2}\\right|_{i,j,k} \\approx \\frac{f_{i+1,j,k} - 2f_{i,j,k} + f_{i-1,j,k}}{h_x^2} $$\n$$ \\left.\\frac{\\partial^2 f}{\\partial y^2}\\right|_{i,j,k} \\approx \\frac{f_{i,j+1,k} - 2f_{i,j,k} + f_{i,j-1,k}}{h_y^2} $$\n$$ \\left.\\frac{\\partial^2 f}{\\partial z^2}\\right|_{i,j,k} \\approx \\frac{f_{i,j,k+1} - 2f_{i,j,k} + f_{i,j,k-1}}{h_z^2} $$\nThe discrete Laplacian, denoted $\\mathcal{L}_h f$, at grid point $(i,j,k)$ is the sum of these approximations:\n$$ (\\mathcal{L}_h f)_{i,j,k} = \\frac{f_{i+1,j,k} - 2f_{i,j,k} + f_{i-1,j,k}}{h_x^2} + \\frac{f_{i,j+1,k} - 2f_{i,j,k} + f_{i,j-1,k}}{h_y^2} + \\frac{f_{i,j,k+1} - 2f_{i,j,k} + f_{i,j,k-1}}{h_z^2} $$\nThis formula is known as the seven-point stencil because it involves the value at a central point and its six nearest neighbors along the coordinate axes. The total truncation error is of order $O(h_x^2, h_y^2, h_z^2)$, confirming the scheme is second-order accurate.\n\n### 2. Analytical Laplacian of the Test Function\n\nTo assess the accuracy of our discrete operator, we require a test function for which the exact Laplacian is known. The problem specifies the function $f(x,y,z) = \\sin x \\, \\cos y \\, e^z$. We compute its second partial derivatives analytically:\n$$ \\frac{\\partial f}{\\partial x} = \\cos x \\, \\cos y \\, e^z \\quad \\implies \\quad \\frac{\\partial^2 f}{\\partial x^2} = -\\sin x \\, \\cos y \\, e^z = -f(x,y,z) $$\n$$ \\frac{\\partial f}{\\partial y} = -\\sin x \\, \\sin y \\, e^z \\quad \\implies \\quad \\frac{\\partial^2 f}{\\partial y^2} = -\\sin x \\, \\cos y \\, e^z = -f(x,y,z) $$\n$$ \\frac{\\partial f}{\\partial z} = \\sin x \\, \\cos y \\, e^z \\quad \\implies \\quad \\frac{\\partial^2 f}{\\partial z^2} = \\sin x \\, \\cos y \\, e^z = f(x,y,z) $$\nSumming these provides the exact analytical Laplacian:\n$$ (\\nabla^2 f)_{\\text{exact}} = (-f) + (-f) + (f) = -f(x,y,z) = -\\sin x \\, \\cos y \\, e^z $$\nThis analytical result serves as the ground truth for measuring the error of our numerical approximation.\n\n### 3. Error Measurement and Convergence Order\n\nThe quality of the approximation is quantified by the discrete $L^2$ error norm, calculated over the set of interior grid points $\\mathcal{I}$. The error is defined as:\n$$ E_{2}(h_{x},h_{y},h_{z}) = \\left( \\sum_{(i,j,k)\\in \\mathcal{I}} \\left[ (\\mathcal{L}_{h} f)_{i,j,k} - (\\nabla^{2}f)_{\\text{exact},i,j,k} \\right]^{2} \\, h_{x} h_{y} h_{z} \\right)^{1/2} $$\nFor a second-order accurate scheme, the error $E_2$ is expected to scale with the square of the grid spacing. That is, $E_2(h) \\approx C h^2$ for some constant $C$ and a characteristic grid spacing $h$. If we refine the grid by halving the spacings ($h' = h/2$), the new error should be $E_2(h') \\approx C (h/2)^2 = E_2(h)/4$. The ratio of errors would therefore be $E_2(h)/E_2(h') \\approx 4$.\n\nThe observed order of convergence, $p$, is calculated from this ratio:\n$$ p = \\log_{2} \\left( \\frac{E_{2}(h)}{E_{2}(h')} \\right) $$\nFor a second-order scheme, we expect $p \\approx \\log_2(4) = 2$. The numerical experiment will compute $p$ for several grid configurations to verify this theoretical prediction.\n\n### 4. Implementation Design\n\nThe implementation will follow a systematic procedure for each test case. A single computational routine will be tasked with calculating the $L^2$ error for a given domain, grid size, and test function.\n1.  **Grid Generation**: For a domain of size $[0,L_x] \\times [0,L_y] \\times [0,L_z]$ and grid point counts $(N_x, N_y, N_z)$, the grid spacings are $h_x = L_x/(N_x-1)$, $h_y = L_y/(N_y-1)$, and $h_z = L_z/(N_z-1)$. Three-dimensional coordinate arrays `X`, `Y`, `Z` are generated using `numpy.meshgrid` with `indexing='ij'` to align with the $(i,j,k)$ index convention.\n2.  **Function Evaluation**: The analytical test function $f(x,y,z)$ and its exact Laplacian $(\\nabla^2 f)_{\\text{exact}} = -f(x,y,z)$ are evaluated at every point on the grid, yielding arrays `F` and `Lap_F_exact`. Populating the `F` array with exact values implicitly enforces the required Dirichlet boundary conditions.\n3.  **Discrete Laplacian Calculation**: The seven-point stencil is applied to the interior points of the `F` array. This is performed efficiently using vectorized `numpy` array slicing. For example, the term $(f_{i+1,j,k} - 2f_{i,j,k} + f_{i-1,j,k})/h_x^2$ is computed for all interior points simultaneously. The sum of the three directional derivative approximations gives the discrete Laplacian array `Lap_F_discrete`.\n4.  **Error Calculation**: The $L^2$ error norm is computed by taking the element-wise difference between `Lap_F_discrete` and the corresponding interior points of `Lap_F_exact`, squaring the result, summing over all interior points, multiplying by the volume element $h_x h_y h_z$, and taking the square root.\n\nThis entire process is executed first for the base grid $(N_x, N_y, N_z)$ to find $E_2(h)$, and then again for the refined grid $(N'_x, N'_y, N'_z)$ to find $E_2(h')$. Finally, the observed order of convergence $p$ is calculated from these two error values. This procedure is repeated for each of the specified test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives, implements, and verifies a second-order accurate finite-difference \n    discretization of the 3D Laplacian, measuring convergence order.\n    \"\"\"\n    \n    test_cases = [\n        # Test case A: Happy path, cubic-ish grid\n        {\n            \"name\": \"A\",\n            \"L_xyz\": (np.pi, np.pi / 2.0, 1.0),\n            \"N_base\": (17, 17, 17)\n        },\n        # Test case B: Anisotropic spacings and non-cubic counts\n        {\n            \"name\": \"B\",\n            \"L_xyz\": (np.pi, np.pi, 0.5),\n            \"N_base\": (18, 14, 10)\n        },\n        # Test case C: Moderate, strongly anisotropic domain extents\n        {\n            \"name\": \"C\",\n            \"L_xyz\": (2.0, 1.0, 0.25),\n            \"N_base\": (5, 7, 9)\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        L_xyz = case[\"L_xyz\"]\n        N_base = case[\"N_base\"]\n\n        def compute_l2_error(domain_lengths, grid_points):\n            \"\"\"\n            Computes the discrete L2 error of the 7-point Laplacian stencil.\n\n            Args:\n                domain_lengths (tuple): (Lx, Ly, Lz) of the domain.\n                grid_points (tuple): (Nx, Ny, Nz) number of points in each dimension.\n\n            Returns:\n                float: The computed L2 error norm.\n            \"\"\"\n            Lx, Ly, Lz = domain_lengths\n            Nx, Ny, Nz = grid_points\n\n            # Ensure grid points allow for interior points\n            if any(n  3 for n in grid_points):\n                raise ValueError(\"Grid must have at least 3 points in each dimension for an interior.\")\n\n            # Calculate grid spacings\n            hx = Lx / (Nx - 1)\n            hy = Ly / (Ny - 1)\n            hz = Lz / (Nz - 1)\n            \n            # Create grid coordinates\n            x_vec = np.linspace(0.0, Lx, Nx)\n            y_vec = np.linspace(0.0, Ly, Ny)\n            z_vec = np.linspace(0.0, Lz, Nz)\n            X, Y, Z = np.meshgrid(x_vec, y_vec, z_vec, indexing='ij')\n\n            # Evaluate the test function and its exact Laplacian on the grid\n            # f(x,y,z) = sin(x) * cos(y) * exp(z)\n            # ∇²f = -f(x,y,z)\n            F = np.sin(X) * np.cos(Y) * np.exp(Z)\n            lap_F_exact = -F\n\n            # Apply the 7-point stencil to compute the discrete Laplacian on interior points\n            # F[i, j, k] corresponds to (x_i, y_j, z_k)\n            F_ijk  = F[1:-1, 1:-1, 1:-1]\n            \n            term_x = (F[2:, 1:-1, 1:-1] - 2 * F_ijk + F[:-2, 1:-1, 1:-1]) / hx**2\n            term_y = (F[1:-1, 2:, 1:-1] - 2 * F_ijk + F[1:-1, :-2, 1:-1]) / hy**2\n            term_z = (F[1:-1, 1:-1, 2:] - 2 * F_ijk + F[1:-1, 1:-1, :-2]) / hz**2\n            \n            lap_F_discrete = term_x + term_y + term_z\n\n            # Isolate the exact Laplacian on the same interior grid\n            lap_F_exact_interior = lap_F_exact[1:-1, 1:-1, 1:-1]\n\n            # Calculate the L2 error norm on the interior\n            diff_sq = (lap_F_discrete - lap_F_exact_interior)**2\n            volume_element = hx * hy * hz\n            error_norm = np.sqrt(np.sum(diff_sq) * volume_element)\n            \n            return error_norm\n\n        # Compute error for the base grid\n        E_base = compute_l2_error(L_xyz, N_base)\n\n        # Define and compute error for the refined grid\n        Nx, Ny, Nz = N_base\n        N_refined = (2 * (Nx - 1) + 1, 2 * (Ny - 1) + 1, 2 * (Nz - 1) + 1)\n        E_refined = compute_l2_error(L_xyz, N_refined)\n        \n        # Calculate the observed order of convergence\n        p = np.log2(E_base / E_refined)\n        results.append(p)\n\n    # Format the final output string\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3576233"}]}