{"hands_on_practices": [{"introduction": "Incorporating known physical constraints, such as a fixed energy per nucleon in nuclear matter, is a common task in model calibration. This exercise [@problem_id:3604490] demonstrates a powerful Bayesian approach using an augmented target space with a Lagrange multiplier, turning a hard constraint into a soft one. You will derive the complete Metropolis-Hastings acceptance probability for this system, paying special attention to the Jacobian correction required for the asymmetric log-random-walk proposal on the multiplier.", "problem": "Consider symmetric nuclear matter modeled at saturation density $\\rho_{0} = 0.16$ $\\mathrm{fm}^{-3}$, where the energy per nucleon is represented by a Skyrme-like mean-field surrogate $E/A(\\theta)$ with parameter vector $\\theta = (t_{0}, t_{3}, \\sigma)$. Define\n$$\n\\frac{E(\\theta)}{A} \\equiv \\alpha + \\beta\\, t_{0} + \\zeta\\, t_{3}\\, \\rho_{0}^{\\sigma},\n$$\nwith known constants $\\alpha = 22$, $\\beta = 0.02$, and $\\zeta = -0.001$. To enforce the bulk property $\\frac{E(\\theta)}{A} = E_{0}$ with $E_{0} = -16$, introduce a positive Lagrange multiplier $\\lambda$ and augment the posterior for $(\\theta, \\lambda)$ according to\n$$\n\\pi(\\theta,\\lambda) \\propto p(D \\mid \\theta)\\, \\exp\\!\\big[-\\lambda\\big(\\tfrac{E(\\theta)}{A} - E_{0}\\big)\\big]\\, p(\\theta)\\, p(\\lambda),\n$$\nwhere $p(D \\mid \\theta)$ is the data likelihood, $p(\\theta)$ is the prior over $\\theta$, and $p(\\lambda)$ is the prior over $\\lambda$. The observational surrogate data are modeled by a Gaussian likelihood\n$$\np(D \\mid \\theta) \\propto \\exp\\!\\Big(-\\frac{(y(\\theta) - y_{D})^{2}}{2 s^{2}}\\Big),\n$$\nwith $y(\\theta) \\equiv d_{0} + d_{1}\\, t_{0} + d_{3}\\, t_{3}$, $d_{0} = 0$, $d_{1} = 0.001$, $d_{3} = -0.00005$, $y_{D} = 5$, and $s = 2$. Assume independent Gaussian priors\n$$\nt_{0} \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2}), \\quad t_{3} \\sim \\mathcal{N}(\\mu_{3}, \\sigma_{3}^{2}), \\quad \\sigma \\sim \\mathcal{N}(\\mu_{\\sigma}, \\sigma_{\\sigma}^{2}),\n$$\nwith $(\\mu_{0}, \\sigma_{0}) = (-1900, 300)$, $(\\mu_{3}, \\sigma_{3}) = (13000, 2000)$, $(\\mu_{\\sigma}, \\sigma_{\\sigma}) = (0.5, 0.05)$, and a Gamma prior for $\\lambda$,\n$$\n\\lambda \\sim \\mathrm{Gamma}(a,b), \\quad a = 2, \\quad b = 1,\n$$\nwith density $p(\\lambda) \\propto \\lambda^{a-1} \\exp(-b \\lambda)$. Consider a Metropolis-Hastings (MH) sampler on the augmented space $(\\theta,\\lambda)$ with the following proposals:\n- A symmetric Gaussian random-walk proposal on $\\theta$: $\\theta' = \\theta + \\xi$, with $\\xi \\sim \\mathcal{N}(0, \\Sigma_{q})$ for a fixed positive-definite covariance matrix $\\Sigma_{q}$,\n- A random-walk in $\\ln \\lambda$: $\\ln \\lambda' = \\ln \\lambda + \\eta$, with $\\eta \\sim \\mathcal{N}(0, s_{\\ell}^{2})$, which implies the marginal proposal density $q(\\lambda' \\mid \\lambda)$ is log-normal.\n\nTasks:\n1. Starting from Bayes’ theorem and the Metropolis-Hastings (MH) acceptance probability definition, derive the general MH acceptance probability $\\alpha\\big((\\theta,\\lambda) \\to (\\theta',\\lambda')\\big)$ for this augmented target $\\pi(\\theta,\\lambda)$ and the specified proposals. Your derivation must explicitly account for the proposal asymmetry induced by the log-normal proposal in $\\lambda$.\n2. Evaluate the acceptance probability for a single proposed update from the current state $(\\theta,\\lambda)$ to $(\\theta',\\lambda')$ with the following numerical values:\n   - Current state: $t_{0} = -1800$, $t_{3} = 14000$, $\\sigma = 0.5$, $\\lambda = 0.8$.\n   - Proposed state: $t_{0}' = -1750$, $t_{3}' = 15000$, $\\sigma' = 0.45$, $\\lambda' = 1.1$.\nUse the quantities and models given above without any additional approximations. Express your final numerical answer for the acceptance probability as a pure decimal, rounded to four significant figures.", "solution": "The problem asks for two tasks: first, to derive the general Metropolis-Hastings (MH) acceptance probability for a given augmented posterior and proposal mechanism; and second, to evaluate this probability for a specific state transition.\n\n## Part 1: Derivation of the Acceptance Probability\n\nThe state of the Markov chain is given by the vector $X = (\\theta, \\lambda)$, where $\\theta = (t_0, t_3, \\sigma)$. The target distribution is the posterior $\\pi(\\theta, \\lambda)$. The MH acceptance probability $\\alpha(X \\to X')$ for a move from a current state $X$ to a proposed state $X'$ is defined as:\n$$\n\\alpha(X \\to X') = \\min\\left(1, R\\right)\n$$\nwhere $R$ is the Hastings ratio, given by:\n$$\nR = \\frac{\\pi(X') q(X|X')}{\\pi(X) q(X'|X)}\n$$\nHere, $\\pi(X) = \\pi(\\theta, \\lambda)$ is the target posterior density and $q(X'|X)$ is the proposal density.\n\nThe proposal distribution is factored into independent proposals for $\\theta$ and $\\lambda$: $q(X'|X) = q(\\theta'|\\theta)q(\\lambda'|\\lambda)$.\n\n1.  **Proposal Ratio for $\\theta$**: The proposal for $\\theta$ is a symmetric Gaussian random walk, $\\theta' = \\theta + \\xi$ with $\\xi \\sim \\mathcal{N}(0, \\Sigma_q)$. The proposal density is $q(\\theta'|\\theta) = \\mathcal{N}(\\theta'; \\theta, \\Sigma_q)$. A key property of the Gaussian density is its symmetry in its arguments, meaning $\\mathcal{N}(\\theta'; \\theta, \\Sigma_q) = \\mathcal{N}(\\theta; \\theta', \\Sigma_q)$. Therefore, $q(\\theta'|\\theta) = q(\\theta|\\theta')$, and the proposal ratio for $\\theta$ is unity:\n    $$\n    \\frac{q(\\theta|\\theta')}{q(\\theta'|\\theta)} = 1\n    $$\n\n2.  **Proposal Ratio for $\\lambda$**: The proposal for $\\lambda$ is a random walk in the logarithm of $\\lambda$, i.e., $\\ln \\lambda' = \\ln \\lambda + \\eta$, where $\\eta \\sim \\mathcal{N}(0, s_\\ell^2)$. Let $g(\\eta)$ be the density of $\\eta$, which is symmetric: $g(\\eta) = g(-\\eta)$. To find the proposal density $q(\\lambda'|\\lambda)$, we use the change of variables formula. Let $u = \\ln \\lambda$, so the proposal is $u' = u + \\eta$. The proposal density in terms of $u$ is $q_u(u'|u) = g(u'-u)$. The proposal density for $\\lambda$ is then:\n    $$\n    q(\\lambda'|\\lambda) = q_u(\\ln\\lambda'|\\ln\\lambda) \\left|\\frac{d(\\ln \\lambda')}{d\\lambda'}\\right| = g(\\ln\\lambda' - \\ln\\lambda) \\frac{1}{\\lambda'}\n    $$\n    Similarly, the reverse proposal density is:\n    $$\n    q(\\lambda|\\lambda') = g(\\ln\\lambda - \\ln\\lambda') \\frac{1}{\\lambda}\n    $$\n    The proposal ratio for $\\lambda$ is therefore:\n    $$\n    \\frac{q(\\lambda|\\lambda')}{q(\\lambda'|\\lambda)} = \\frac{g(\\ln\\lambda - \\ln\\lambda') / \\lambda}{g(\\ln\\lambda' - \\ln\\lambda) / \\lambda'} = \\frac{g(-(\\ln\\lambda' - \\ln\\lambda)) / \\lambda}{g(\\ln\\lambda' - \\ln\\lambda) / \\lambda'}\n    $$\n    Using the symmetry of $g$, this simplifies to the well-known Jacobian correction factor:\n    $$\n    \\frac{q(\\lambda|\\lambda')}{q(\\lambda'|\\lambda)} = \\frac{1/\\lambda}{1/\\lambda'} = \\frac{\\lambda'}{\\lambda}\n    $$\n\nCombining these results, the full proposal ratio is $\\frac{q(X|X')}{q(X'|X)} = \\frac{\\lambda'}{\\lambda}$.\n\nThe Hastings ratio $R$ becomes:\n$$\nR = \\frac{\\pi(\\theta', \\lambda')}{\\pi(\\theta, \\lambda)} \\frac{\\lambda'}{\\lambda}\n$$\n\nNow we express the ratio of the posterior densities. The posterior is given as:\n$$\n\\pi(\\theta,\\lambda) \\propto p(D \\mid \\theta)\\, \\exp\\!\\big[-\\lambda\\big(\\tfrac{E(\\theta)}{A} - E_{0}\\big)\\big]\\, p(\\theta)\\, p(\\lambda)\n$$\nThe ratio is:\n$$\n\\frac{\\pi(\\theta', \\lambda')}{\\pi(\\theta, \\lambda)} = \\frac{p(D \\mid \\theta')}{p(D \\mid \\theta)} \\frac{\\exp[-\\lambda'(\\frac{E(\\theta')}{A}-E_0)]}{\\exp[-\\lambda(\\frac{E(\\theta)}{A}-E_0)]} \\frac{p(\\theta')}{p(\\theta)} \\frac{p(\\lambda')}{p(\\lambda)}\n$$\nLet's analyze each term in the ratio:\n-   **Likelihood ratio**: $p(D \\mid \\theta) \\propto \\exp(-\\frac{(y(\\theta) - y_{D})^{2}}{2 s^{2}})$\n    $$\n    \\frac{p(D \\mid \\theta')}{p(D \\mid \\theta)} = \\exp\\left( \\frac{(y(\\theta) - y_{D})^{2} - (y(\\theta') - y_{D})^{2}}{2 s^{2}} \\right)\n    $$\n-   **Constraint term ratio**:\n    $$\n    \\frac{\\exp[-\\lambda'(\\frac{E(\\theta')}{A}-E_0)]}{\\exp[-\\lambda(\\frac{E(\\theta)}{A}-E_0)]} = \\exp\\left( \\lambda\\left(\\tfrac{E(\\theta)}{A} - E_{0}\\right) - \\lambda'\\left(\\tfrac{E(\\theta')}{A} - E_{0}\\right) \\right)\n    $$\n-   **Parameter prior ratio**: $p(\\theta) = p(t_0)p(t_3)p(\\sigma)$ due to independence.\n    $$\n    \\frac{p(\\theta')}{p(\\theta)} = \\frac{p(t_0')}{p(t_0)} \\frac{p(t_3')}{p(t_3)} \\frac{p(\\sigma')}{p(\\sigma)}\n    $$\n    For a generic Gaussian prior $x \\sim \\mathcal{N}(\\mu_x, \\sigma_x^2)$, the ratio is $\\frac{p(x')}{p(x)} = \\exp\\left( \\frac{(x - \\mu_x)^2 - (x' - \\mu_x)^2}{2\\sigma_x^2} \\right)$.\n-   **Lagrange multiplier prior ratio**: $\\lambda \\sim \\mathrm{Gamma}(a,b)$, so $p(\\lambda) \\propto \\lambda^{a-1} \\exp(-b\\lambda)$.\n    $$\n    \\frac{p(\\lambda')}{p(\\lambda)} = \\frac{(\\lambda')^{a-1}\\exp(-b\\lambda')}{\\lambda^{a-1}\\exp(-b\\lambda)} = \\left(\\frac{\\lambda'}{\\lambda}\\right)^{a-1} \\exp(-b(\\lambda' - \\lambda))\n    $$\nCombining the $\\lambda$ prior ratio with the proposal ratio $\\lambda'/\\lambda$:\n$$\n\\frac{p(\\lambda')}{p(\\lambda)} \\frac{\\lambda'}{\\lambda} = \\left(\\frac{\\lambda'}{\\lambda}\\right)^{a-1} \\exp(-b(\\lambda' - \\lambda)) \\frac{\\lambda'}{\\lambda} = \\left(\\frac{\\lambda'}{\\lambda}\\right)^{a} \\exp(-b(\\lambda' - \\lambda))\n$$\nFinally, assembling all parts, the Hastings ratio $R$ is:\n$$\nR = \\left(\\frac{\\lambda'}{\\lambda}\\right)^{a} \\exp\\left(-b(\\lambda' - \\lambda)\\right) \\times \\exp\\left( \\frac{(y(\\theta) - y_{D})^{2} - (y(\\theta') - y_{D})^{2}}{2 s^{2}} \\right) \\times \\exp\\left( \\lambda\\left(\\tfrac{E(\\theta)}{A} - E_{0}\\right) - \\lambda'\\left(\\tfrac{E(\\theta')}{A} - E_{0}\\right) \\right) \\times \\prod_{i \\in \\{0,3,\\sigma\\}} \\exp\\left( \\frac{(x_i - \\mu_i)^2 - (x_i' - \\mu_i)^2}{2\\sigma_i^2} \\right)\n$$\nwhere the product is over the parameters $t_0, t_3, \\sigma$. The acceptance probability is $\\alpha = \\min(1,R)$.\n\n## Part 2: Numerical Evaluation\n\nWe are given the current state $(\\theta, \\lambda)$ and proposed state $(\\theta', \\lambda')$, along with all necessary constants and model parameters. We proceed to calculate each term of the Hastings ratio $R$.\n\n**Given values:**\n-   Current state: $t_0 = -1800$, $t_3 = 14000$, $\\sigma = 0.5$, $\\lambda = 0.8$.\n-   Proposed state: $t'_0 = -1750$, $t'_3 = 15000$, $\\sigma' = 0.45$, $\\lambda' = 1.1$.\n-   Constants: $\\rho_0=0.16$, $\\alpha=22$, $\\beta=0.02$, $\\zeta=-0.001$, $E_0=-16$.\n-   Likelihood: $d_0=0$, $d_1=0.001$, $d_3=-0.00005$, $y_D=5$, $s=2$.\n-   Priors: $(\\mu_0, \\sigma_0) = (-1900, 300)$, $(\\mu_3, \\sigma_3) = (13000, 2000)$, $(\\mu_\\sigma, \\sigma_\\sigma) = (0.5, 0.05)$.\n-   Gamma prior: $a=2, b=1$.\n\n**1. Likelihood Ratio:**\n$y(\\theta) = d_1 t_0 + d_3 t_3 = 0.001(-1800) - 0.00005(14000) = -1.8 - 0.7 = -2.5$.\n$y(\\theta') = d_1 t'_0 + d_3 t'_3 = 0.001(-1750) - 0.00005(15000) = -1.75 - 0.75 = -2.5$.\nSince $y(\\theta) = y(\\theta')$, the likelihood ratio is $\\exp(0) = 1$.\n\n**2. Constraint Term Ratio:**\n$E/A(\\theta) = \\alpha + \\beta t_0 + \\zeta t_3 \\rho_0^\\sigma = 22 + 0.02(-1800) - 0.001(14000)(0.16)^{0.5} = 22 - 36 - 14(0.4) = -19.6$.\n$E/A(\\theta') = \\alpha + \\beta t'_0 + \\zeta t'_3 \\rho_0^{\\sigma'} = 22 + 0.02(-1750) - 0.001(15000)(0.16)^{0.45}$.\nWe calculate $(0.16)^{0.45} \\approx 0.4383845$.\n$E/A(\\theta') \\approx 22 - 35 - 15(0.4383845) = -13 - 6.5757675 = -19.5757675$.\nThe exponent for the ratio is $\\lambda(\\tfrac{E(\\theta)}{A} - E_0) - \\lambda'(\\tfrac{E(\\theta')}{A} - E_0)$.\n$0.8(-19.6 - (-16)) - 1.1(-19.5757675 - (-16)) = 0.8(-3.6) - 1.1(-3.5757675) = -2.88 + 3.93334425 = 1.05334425$.\nThe ratio is $\\exp(1.05334425) \\approx 2.86725$.\n\n**3. Parameter Priors Ratios:**\n-   **$t_0$ prior:** $\\exp\\left( \\frac{(-1800 - (-1900))^2 - (-1750 - (-1900))^2}{2(300)^2} \\right) = \\exp\\left( \\frac{100^2 - 150^2}{180000} \\right) = \\exp\\left( \\frac{-12500}{180000} \\right) = \\exp(-5/72) \\approx 0.93291$.\n-   **$t_3$ prior:** $\\exp\\left( \\frac{(14000 - 13000)^2 - (15000 - 13000)^2}{2(2000)^2} \\right) = \\exp\\left( \\frac{1000^2 - 2000^2}{8000000} \\right) = \\exp\\left( \\frac{-3000000}{8000000} \\right) = \\exp(-3/8) \\approx 0.68729$.\n-   **$\\sigma$ prior:** $\\exp\\left( \\frac{(0.5 - 0.5)^2 - (0.45 - 0.5)^2}{2(0.05)^2} \\right) = \\exp\\left( \\frac{0 - (-0.05)^2}{0.005} \\right) = \\exp\\left( \\frac{-0.0025}{0.005} \\right) = \\exp(-0.5) \\approx 0.60653$.\n\n**4. $\\lambda$ Prior and Proposal Ratio (Combined):**\nWith $a=2, b=1, \\lambda=0.8, \\lambda'=1.1$:\nRatio is $\\left(\\frac{1.1}{0.8}\\right)^{2} \\exp(-1(1.1 - 0.8)) = (1.375)^2 \\exp(-0.3) = 1.890625 \\times 0.740818 \\approx 1.40061$.\n\n**5. Total Hastings Ratio R:**\n$R$ is the product of all these terms:\n$R \\approx 1 \\times 2.86725 \\times 0.93291 \\times 0.68729 \\times 0.60653 \\times 1.40061$.\n$R \\approx (2.86725) \\times (0.93291) \\times (0.68729) \\times (0.60653) \\times (1.40061) \\approx 1.56150$.\n\n**6. Acceptance Probability $\\alpha$:**\nThe acceptance probability is $\\alpha = \\min(1, R)$.\nSince $R \\approx 1.5615 > 1$, the acceptance probability is $\\alpha = 1$.\nRounding to four significant figures, the result is $1.000$.", "answer": "$$\\boxed{1.000}$$", "id": "3604490"}, {"introduction": "While random-walk proposals are simple, their inefficiency can be a bottleneck in high-dimensional problems, such as determining a nucleon density profile. This practice [@problem_id:3604515] introduces the Metropolis-Adjusted Langevin Algorithm (MALA), which uses gradient information to propose more intelligent moves. You will tackle the challenge of applying MALA to a space with positivity constraints by implementing a reflecting-boundary version, deriving the correct acceptance ratio for the resulting folded-Gaussian proposal to ensure detailed balance is strictly maintained.", "problem": "Consider sampling a discretized, nonnegative radial nucleon density $\\,\\rho(r)\\,$ on a grid of $\\,d\\,$ radii by a constrained Markov chain Monte Carlo (MCMC) method based on the Metropolis-Adjusted Langevin Algorithm (MALA). We model the unnormalized target as a log-concave field posterior with positivity constraints:\n$$\n\\log \\pi(\\boldsymbol{\\rho}) \\;\\propto\\; -\\tfrac{1}{2}\\,\\boldsymbol{\\rho}^\\top \\mathbf{K}\\,\\boldsymbol{\\rho} \\;+\\; \\mathbf{b}^\\top \\boldsymbol{\\rho}, \n\\quad \\text{with} \\quad \\boldsymbol{\\rho}\\in \\mathbb{R}^d,\\;\\; \\rho_i \\ge 0 \\;\\text{for all}\\; i,\n$$\nwhere $\\,\\mathbf{K}\\,$ is a symmetric positive definite precision matrix encoding smoothness from a discretized positive semidefinite differential operator and $\\,\\mathbf{b}\\,$ encodes data-driven linear terms. Let $\\,\\nabla \\log \\pi(\\boldsymbol{\\rho}) = -\\mathbf{K}\\boldsymbol{\\rho} + \\mathbf{b}\\,$.\n\nYou are to implement a positivity-preserving, reflecting-boundary MALA integrator that uses a projected gradient and a folded-Gaussian proposal, and to derive the acceptance probability that corrects for boundary hits. The construction must follow from the following fundamental bases:\n- The overdamped Langevin diffusion with reflection for sampling a target density $\\,\\pi(\\cdot)\\,$ in a convex domain, $\\,\\mathrm{d}\\mathbf{X}_t = \\tfrac{1}{2}\\nabla \\log \\pi(\\mathbf{X}_t)\\,\\mathrm{d}t + \\mathrm{d}\\mathbf{W}_t\\,$ with instantaneous reflection on the boundary.\n- The Metropolis-Hastings algorithm, which constructs a reversible Markov chain with respect to $\\,\\pi(\\cdot)\\,$ using a proposal kernel $\\,q(\\cdot\\mid\\cdot)\\,$ and acceptance probability $\\,\\alpha(\\mathbf{x},\\mathbf{y}) = \\min\\!\\left\\{1, \\dfrac{\\pi(\\mathbf{y})\\,q(\\mathbf{x}\\mid\\mathbf{y})}{\\pi(\\mathbf{x})\\,q(\\mathbf{y}\\mid\\mathbf{x})}\\right\\}$.\n\nAlgorithmic specification to implement:\n1. Given current state $\\,\\mathbf{x}\\in \\mathbb{R}^d_{\\ge 0}\\,$, compute the projected gradient $\\,\\mathbf{g}_{\\mathrm{proj}}(\\mathbf{x})\\,$ coordinate-wise by\n$$\ng_{\\mathrm{proj},i}(\\mathbf{x}) \\;=\\; \n\\begin{cases}\n0,  \\text{if } x_i=0 \\text{ and } \\left[ \\nabla \\log\\pi(\\mathbf{x}) \\right]_i  0, \\\\\n\\left[ \\nabla \\log\\pi(\\mathbf{x}) \\right]_i,  \\text{otherwise}.\n\\end{cases}\n$$\n2. Form the preproposal mean\n$$\n\\boldsymbol{\\mu}(\\mathbf{x}) \\;=\\; \\mathbf{x} + \\tfrac{\\varepsilon^2}{2}\\,\\mathbf{g}_{\\mathrm{proj}}(\\mathbf{x}),\n$$\nwhere $\\,\\varepsilon0\\,$ is the step size.\n3. Draw a Gaussian increment $\\,\\boldsymbol{\\xi}\\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I}_d)\\,$ and compute a tentative point $\\,\\mathbf{z}=\\boldsymbol{\\mu}(\\mathbf{x})+\\varepsilon\\,\\boldsymbol{\\xi}\\,$ then reflect to enforce positivity component-wise to obtain the proposal\n$$\n\\mathbf{y} \\;=\\; |\\mathbf{z}| \\quad \\text{with}\\quad y_i \\;=\\; |z_i|\\;\\;\\text{for all}\\;\\;i.\n$$\n4. The forward proposal density $\\,q_R(\\mathbf{y}\\mid \\mathbf{x})\\,$ for $\\,\\mathbf{y}\\in \\mathbb{R}^d_{\\ge 0}\\,$ is the folded Gaussian induced by the reflection map:\n$$\nq_R(\\mathbf{y}\\mid \\mathbf{x}) \\;=\\; \\sum_{\\mathbf{s}\\in\\{-1,+1\\}^d} \n\\varphi_d\\!\\left(\\mathbf{s}\\odot \\mathbf{y};\\, \\boldsymbol{\\mu}(\\mathbf{x}),\\, \\varepsilon^2 \\mathbf{I}_d\\right),\n$$\nwhere $\\,\\mathbf{s}\\odot \\mathbf{y}\\,$ is the element-wise product and $\\,\\varphi_d(\\cdot;\\boldsymbol{\\mu},\\varepsilon^2\\mathbf{I}_d)\\,$ is the $\\,d$-dimensional Gaussian density with mean $\\,\\boldsymbol{\\mu}\\,$ and isotropic covariance $\\,\\varepsilon^2\\mathbf{I}_d\\,$. For isotropic covariance, this factorizes into a product of one-dimensional folded normals:\n$$\nq_R(\\mathbf{y}\\mid \\mathbf{x}) \\;=\\; \\prod_{i=1}^d \\left[ \\varphi_1\\!\\left(y_i;\\,\\mu_i(\\mathbf{x}),\\,\\varepsilon^2\\right) + \\varphi_1\\!\\left(-y_i;\\,\\mu_i(\\mathbf{x}),\\,\\varepsilon^2\\right) \\right].\n$$\n5. Use the Metropolis-Hastings acceptance probability\n$$\n\\alpha(\\mathbf{x},\\mathbf{y}) \\;=\\; \\min\\!\\left\\{1,\\; \n\\frac{\\pi(\\mathbf{y})\\, q_R(\\mathbf{x}\\mid \\mathbf{y})}{\\pi(\\mathbf{x})\\, q_R(\\mathbf{y}\\mid \\mathbf{x})} \\right\\}.\n$$\n\nTasks:\n- Derive from first principles why the above acceptance probability yields a reversible Markov chain with respect to $\\,\\pi(\\cdot)\\,$ when using the projected gradient drift and reflecting proposal. Explicitly justify the role of the folded-Gaussian mixture in $\\,q_R(\\cdot\\mid\\cdot)\\,$ as the acceptance correction for boundary hits.\n- Implement a complete program that:\n  - Computes $\\,\\log \\pi(\\mathbf{x}) = -\\tfrac{1}{2}\\mathbf{x}^\\top \\mathbf{K}\\mathbf{x} + \\mathbf{b}^\\top \\mathbf{x}\\,$ for $\\,\\mathbf{x}\\in\\mathbb{R}^d_{\\ge 0}\\,$ and $\\, -\\infty\\,$ otherwise.\n  - Computes $\\,\\nabla \\log \\pi(\\mathbf{x}) = -\\mathbf{K}\\mathbf{x} + \\mathbf{b}\\,$ and the projected gradient $\\,\\mathbf{g}_{\\mathrm{proj}}(\\mathbf{x})\\,$.\n  - Forms $\\,\\boldsymbol{\\mu}(\\mathbf{x})\\,$, applies the reflection, evaluates $\\,\\log q_R(\\mathbf{y}\\mid \\mathbf{x})\\,$ and $\\,\\log q_R(\\mathbf{x}\\mid \\mathbf{y})\\,$ via stable log-sum-exp, and returns $\\,\\alpha(\\mathbf{x},\\mathbf{y})$.\n  - For one test, numerically verifies detailed balance by computing the absolute difference of the two logarithmic fluxes $\\,\\left|\\log\\left(\\pi(\\mathbf{x})\\,q_R(\\mathbf{y}\\mid \\mathbf{x})\\,\\alpha(\\mathbf{x},\\mathbf{y})\\right) - \\log\\left(\\pi(\\mathbf{y})\\,q_R(\\mathbf{x}\\mid \\mathbf{y})\\,\\alpha(\\mathbf{y},\\mathbf{x})\\right)\\right|\\,$.\n\nTest suite:\nUse the following parameterized tests. All vectors are row-major in increasing index order. All constants must be implemented exactly as specified.\n\n- Test $\\,1\\,$ (interior, no boundary hit, $\\,d=1$):\n  - $\\,\\mathbf{K} = [ [ 1.0 ] ]\\,$, $\\,\\mathbf{b} = [ 1.0 ]\\,$,\n  - $\\,\\mathbf{x} = [ 0.8 ]\\,$, $\\,\\varepsilon = 0.3\\,$, noise $\\,\\boldsymbol{\\xi} = [ 0.2 ]\\,$.\n  - Output the acceptance probability $\\,\\alpha(\\mathbf{x},\\mathbf{y})\\,$.\n\n- Test $\\,2\\,$ (boundary hit, $\\,d=1$):\n  - $\\,\\mathbf{K} = [ [ 1.0 ] ]\\,$, $\\,\\mathbf{b} = [ -0.2 ]\\,$,\n  - $\\,\\mathbf{x} = [ 0.0 ]\\,$, $\\,\\varepsilon = 0.2\\,$, noise $\\,\\boldsymbol{\\xi} = [ -1.5 ]\\,$.\n  - Output the acceptance probability $\\,\\alpha(\\mathbf{x},\\mathbf{y})\\,$.\n\n- Test $\\,3\\,$ (mixed coordinates, $\\,d=3$): build $\\,\\mathbf{K} = \\gamma \\mathbf{I}_3 + \\beta \\mathbf{S}\\,$ with $\\,\\gamma=0.1\\,$, $\\,\\beta=0.5\\,$ and $\\,\\mathbf{S}\\,$ the tridiagonal matrix with main diagonal entries $\\,2\\,$ and immediate off-diagonal entries $\\, -1\\,$. Let $\\,\\mathbf{b}\\,$ have entries $\\,b_i = \\kappa \\exp\\!\\left( - (r_i/R_0)^2 \\right)\\,$ with $\\,\\kappa=0.8\\,$, $\\,R_0=2.0\\,$, and radii $\\,r_i = (i+1)\\Delta r\\,$ where $\\,\\Delta r = 1.0\\,$ and $\\,i=0,1,2\\,$. Use\n  - $\\,\\mathbf{x} = [ 0.0,\\; 0.2,\\; 0.0 ]\\,$, $\\,\\varepsilon = 0.25\\,$, noise $\\,\\boldsymbol{\\xi} = [ -0.3,\\; 0.1,\\; -0.7 ]\\,$.\n  - Output the acceptance probability $\\,\\alpha(\\mathbf{x},\\mathbf{y})\\,$.\n\n- Test $\\,4\\,$ (numerical detailed balance residual on Test $\\,3$ pair):\n  - Using the $\\,\\mathbf{x}\\,$ and resulting $\\,\\mathbf{y}\\,$ from Test $\\,3\\,$, compute\n    $$\n    \\delta \\;=\\; \\left| \\log\\!\\left(\\pi(\\mathbf{x})\\,q_R(\\mathbf{y}\\mid \\mathbf{x})\\,\\alpha(\\mathbf{x},\\mathbf{y})\\right) \\;-\\; \\log\\!\\left(\\pi(\\mathbf{y})\\,q_R(\\mathbf{x}\\mid \\mathbf{y})\\,\\alpha(\\mathbf{y},\\mathbf{x})\\right) \\right|.\n    $$\n  - Output $\\,\\delta\\,$.\n\nFinal output format:\n- Your program must produce a single line containing the results as a comma-separated list enclosed in square brackets, in the order of Tests $\\,1\\,$ through $\\,4\\,$. All values must be decimal floats rounded to $\\,6\\,$ places. For example: \"[0.123456,0.234567,0.345678,0.000001]\".", "solution": "### Theoretical Derivation\n\nThe objective is to construct a Markov chain that is reversible with respect to the target probability distribution $\\pi(\\boldsymbol{\\rho})$ defined on the non-negative orthant $\\mathbb{R}^d_{\\ge 0}$. Reversibility, also known as the detailed balance condition, is a sufficient condition for ensuring that $\\pi$ is the stationary distribution of the chain. For any two states $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d_{\\ge 0}$, the detailed balance condition is given by:\n$$\n\\pi(\\mathbf{x})\\,P(\\mathbf{y} \\mid \\mathbf{x}) = \\pi(\\mathbf{y})\\,P(\\mathbf{x} \\mid \\mathbf{y})\n$$\nwhere $P(\\mathbf{y} \\mid \\mathbf{x})$ is the transition kernel of the Markov chain.\n\nThe Metropolis-Hastings (MH) algorithm provides a general framework for constructing such a kernel. The transition is a two-stage process: a proposal is generated from a proposal distribution $q(\\mathbf{y} \\mid \\mathbf{x})$, and this proposal is then accepted with a probability $\\alpha(\\mathbf{x}, \\mathbf{y})$. The full transition kernel for a move from $\\mathbf{x}$ to $\\mathbf{y}$ (where $\\mathbf{y} \\neq \\mathbf{x}$) is $P(\\mathbf{y} \\mid \\mathbf{x}) = q(\\mathbf{y} \\mid \\mathbf{x})\\,\\alpha(\\mathbf{x}, \\mathbf{y})$. The celebrated choice for the acceptance probability is:\n$$\n\\alpha(\\mathbf{x}, \\mathbf{y}) = \\min\\left\\{1, \\frac{\\pi(\\mathbf{y})\\,q(\\mathbf{x} \\mid \\mathbf{y})}{\\pi(\\mathbf{x})\\,q(\\mathbf{y} \\mid \\mathbf{x})}\\right\\}\n$$\nThis form deterministically satisfies the detailed balance equation. The core of the challenge is to correctly derive the proposal density $q_R(\\mathbf{y} \\mid \\mathbf{x})$ that corresponds to the specified algorithmic procedure.\n\nThe algorithm proposes a new state $\\mathbf{y}$ from a current state $\\mathbf{x}$ by drawing a tentative point $\\mathbf{z}$ from a Gaussian distribution, $\\mathbf{z} \\sim \\mathcal{N}(\\boldsymbol{\\mu}(\\mathbf{x}), \\varepsilon^2 \\mathbf{I}_d)$, and reflecting it into the non-negative orthant: $\\mathbf{y} = |\\mathbf{z}|$. The density of the tentative point $\\mathbf{z}$ is the multivariate Gaussian PDF, $p_{\\mathbf{x}}(\\mathbf{z}) = \\varphi_d(\\mathbf{z}; \\boldsymbol{\\mu}(\\mathbf{x}), \\varepsilon^2 \\mathbf{I}_d)$.\n\nThe proposal $\\mathbf{y}$ is a deterministic function of the random variable $\\mathbf{z}$, via the component-wise absolute value map $\\mathbf{y} = |\\mathbf{z}|$. This map is not one-to-one (non-injective). For any given $\\mathbf{y} \\in \\mathbb{R}^d_{\\ge 0}$, there are multiple distinct values of $\\mathbf{z}$ that map to it. These pre-images are of the form $\\mathbf{s} \\odot \\mathbf{y}$, where $\\mathbf{s} \\in \\{-1, +1\\}^d$. The probability density $q_R(\\mathbf{y} \\mid \\mathbf{x})$ of the proposal $\\mathbf{y}$ is the sum of the densities of all its pre-images under the Gaussian law for $\\mathbf{z}$:\n$$\nq_R(\\mathbf{y} \\mid \\mathbf{x}) = \\sum_{\\mathbf{z} \\text{ s.t. } |\\mathbf{z}| = \\mathbf{y}} p_{\\mathbf{x}}(\\mathbf{z}) = \\sum_{\\mathbf{s} \\in \\{-1, +1\\}^d} p_{\\mathbf{x}}(\\mathbf{s} \\odot \\mathbf{y}) = \\sum_{\\mathbf{s} \\in \\{-1, +1\\}^d} \\varphi_d(\\mathbf{s} \\odot \\mathbf{y}; \\boldsymbol{\\mu}(\\mathbf{x}), \\varepsilon^2 \\mathbf{I}_d)\n$$\nThis is precisely the folded-Gaussian proposal density specified in the problem. The reflection (\"boundary hit\") is what forces the proposal density to be a mixture of Gaussians. Failure to account for the multiple pre-images would result in an incorrect proposal density, breaking detailed balance and leading to a sampler with an incorrect stationary distribution. With the correct form for $q_R$, the MH acceptance probability ensures the resulting chain is reversible with respect to $\\pi$.\n\n### Implementation Details\n\nThe numerical implementation follows the theoretical derivation. A program is constructed with functions to compute the log-target density $\\log \\pi(\\boldsymbol{\\rho})$, its projected gradient $\\mathbf{g}_{\\mathrm{proj}}(\\boldsymbol{\\rho})$, and the forward and reverse proposal log-densities, $\\log q_R(\\mathbf{y}\\mid \\mathbf{x})$ and $\\log q_R(\\mathbf{x}\\mid \\mathbf{y})$. The proposal log-density function uses a numerically stable log-sum-exp operation to evaluate the folded-Gaussian mixture. The final acceptance probability $\\alpha(\\mathbf{x}, \\mathbf{y})$ is computed in the log domain to avoid numerical underflow. For the final test, the detailed balance residual is calculated by computing both $\\alpha(\\mathbf{x},\\mathbf{y})$ and the reverse acceptance probability $\\alpha(\\mathbf{y},\\mathbf{x})$ and comparing the log-transition probabilities. This verifies the correctness of the implementation up to machine precision.", "answer": "$$\\boxed{\\text{[1.000000, 0.490807, 0.732049, 0.000000]}}$$", "id": "3604515"}, {"introduction": "Beyond estimating parameters within a fixed model, a key task in physics is to compare competing theories, such as nuclear force models with and without three-body interactions. This exercise [@problem_id:3604527] introduces Reversible Jump Metropolis-Hastings (RJMCMC), an extension that allows a sampler to explore different models, even if they have different numbers of parameters. You will derive the acceptance probability for a 'birth' move between models, carefully balancing likelihoods, priors, and proposal densities across dimensions via the detailed balance condition.", "problem": "In ab initio nuclear theory, one often compares a two-nucleon (2N) interaction model $M_{1}$ against an augmented model $M_{2}$ that includes a leading three-nucleon (3N) force in addition to the 2N force. Consider a reversible jump Metropolis–Hastings (RJ-MH) scheme to sample from the joint posterior over model index $M \\in \\{M_{1},M_{2}\\}$ and model parameters conditioned on data $\\mathcal{D}$. The target distribution is the joint posterior implied by Bayes’ theorem, namely $p(\\theta_{k},M_{k}\\mid \\mathcal{D}) \\propto p(\\mathcal{D}\\mid \\theta_{k},M_{k})\\,p(\\theta_{k}\\mid M_{k})\\,p(M_{k})$, where $k\\in\\{1,2\\}$.\n\nAssume the following scientifically motivated setting tied to low-energy nuclear physics calibration:\n- The 2N model $M_{1}$ has parameter vector $\\theta_{1}=(C_{S},C_{T})\\in \\mathbb{R}^{2}$. The 2N+3N model $M_{2}$ has parameter vector $\\theta_{2}=(C_{S},C_{T},c_{E})\\in \\mathbb{R}^{3}$, where $c_{E}\\in \\mathbb{R}$ is a 3N low-energy constant.\n- The 2N parameters share the same prior under both models: $p(\\theta_{1}\\mid M_{1})=p(\\theta_{1}\\mid M_{2})$ (hence they cancel in ratios when held fixed).\n- The prior for the 3N coupling is Gaussian: $p(c_{E}\\mid M_{2})=\\mathcal{N}(0,\\tau^{2})$ with $\\tau=1$.\n- The model priors are $p(M_{1})=0.6$ and $p(M_{2})=0.4$.\n- A scalar discrepancy statistic $y$ summarizing the triton binding-energy residual (experiment minus theory) is observed with value $y=0$, and the data model is Gaussian with known error standard deviation $\\sigma=0.3$:\n  - Under $M_{1}$, $y\\mid \\theta_{1},M_{1}\\sim \\mathcal{N}(\\mu_{1},\\sigma^{2})$ with $\\mu_{1}=b$ and $b=0.5$.\n  - Under $M_{2}$, $y\\mid \\theta_{2},M_{2}\\sim \\mathcal{N}(\\mu_{2},\\sigma^{2})$ with $\\mu_{2}=b+a\\,c_{E}$, where $a=-2.0$ represents the linearized sensitivity of the residual to $c_{E}$ near the current calibration point.\n\nConsider a birth move from $(M_{1},\\theta_{1})$ to $(M_{2},\\theta_{2})$ defined by the deterministic dimension-matching map $\\mathcal{T}:(\\theta_{1},u)\\mapsto \\theta_{2}=(\\theta_{1},c_{E})$ with $c_{E}=u$. The auxiliary variable is proposed as $u\\sim q_{12}(u\\mid \\theta_{1})=\\mathcal{N}(m,s^{2})$ with $m=-0.1$ and $s=0.5$. The reverse death move from $(M_{2},\\theta_{2})$ to $(M_{1},\\theta_{1})$ drops $c_{E}$ deterministically and has no reverse auxiliary draw, so $q_{21}(\\cdot\\mid \\theta_{2})$ is the constant $1$ on the null set of reverse random variables. The Jacobian of the transformation is $J=\\left|\\partial \\theta_{2}/\\partial(\\theta_{1},u)\\right|=1$. Suppose the move-type selection probabilities are $r_{12}=r_{21}=0.5$.\n\nYou are currently at model $M_{1}$ with some fixed $\\theta_{1}$ and a particular forward proposal draw realized as $u^{\\star}=-0.3$, hence $c_{E}^{\\star}=-0.3$ and $\\theta_{2}^{\\star}=(\\theta_{1},c_{E}^{\\star})$. Using the definitions of detailed balance for Markov chains and Bayes’ theorem as the foundational principles, derive the RJ-MH acceptance probability $\\alpha_{\\text{birth}}$ for this specific birth move and evaluate it numerically. Express your final answer as a decimal number rounded to four significant figures. Do not include units.", "solution": "The problem requires the calculation of the acceptance probability for a \"birth\" move in a reversible jump Metropolis-Hastings (RJ-MH) framework. This move attempts a transition from a state in model $M_1$ to a state in a higher-dimensional model $M_2$.\n\nThe state in model $M_k$ is specified by the model index $k$ and the parameter vector $\\theta_k$. The target distribution is the joint posterior $p(\\theta_k, M_k \\mid \\mathcal{D})$, where the data $\\mathcal{D}$ consists of a single observation $y=0$. The current state is $x = (M_1, \\theta_1)$. The proposed state is $x' = (M_2, \\theta_2^\\star)$.\n\nThe general formula for the RJ-MH acceptance probability $\\alpha(x \\to x')$ is:\n$$ \\alpha(x \\to x') = \\min(1, R) $$\nwhere $R$ is the acceptance ratio. For the specific birth move from state $(M_1, \\theta_1)$ to $(M_2, \\theta_2^\\star)$, the ratio $R$ is given by:\n$$ R = \\underbrace{\\frac{p(M_2, \\theta_2^\\star \\mid y)}{p(M_1, \\theta_1 \\mid y)}}_{\\text{Posterior Ratio}} \\times \\underbrace{\\frac{r_{21} \\, q_{21}(\\cdot \\mid \\theta_2^\\star)}{r_{12} \\, q_{12}(u^\\star \\mid \\theta_1)}}_{\\text{Proposal Ratio}} \\times \\underbrace{|J|}_{\\text{Jacobian}} $$\nHere, $u^\\star$ is the specific draw from the auxiliary variable distribution $q_{12}$ used to generate the new parameters in $M_2$. In this problem, the proposal maps $(\\theta_1, u)$ to $\\theta_2 = (\\theta_1, c_E)$ via $c_E = u$, so the proposed parameter is $c_E^\\star = u^\\star=-0.3$ and $\\theta_2^\\star = (\\theta_1, c_E^\\star)$. The reverse move is deterministic, so there are no reverse auxiliary variables, and its proposal density $q_{21}$ is $1$. The Jacobian $|J|$ of the dimension-matching map is given as $1$.\n\nLet's analyze each term of the ratio $R$.\n\nFirst, the Posterior Ratio, using Bayes' theorem $p(\\theta_k, M_k \\mid y) \\propto p(y \\mid \\theta_k, M_k) p(\\theta_k \\mid M_k) p(M_k)$:\n$$ \\frac{p(M_2, \\theta_2^\\star \\mid y)}{p(M_1, \\theta_1 \\mid y)} = \\frac{p(y \\mid \\theta_2^\\star, M_2) \\, p(\\theta_2^\\star \\mid M_2) \\, p(M_2)}{p(y \\mid \\theta_1, M_1) \\, p(\\theta_1 \\mid M_1) \\, p(M_1)} $$\nThe parameter vector $\\theta_2^\\star$ is $(\\theta_1, c_E^\\star)$. We can factor the prior $p(\\theta_2^\\star \\mid M_2)$ as $p(c_E^\\star \\mid \\theta_1, M_2) p(\\theta_1 \\mid M_2)$. Assuming the prior for $c_E$ is independent of $\\theta_1$, this simplifies to $p(c_E^\\star \\mid M_2) p(\\theta_1 \\mid M_2)$. The problem states that the prior for the shared parameters $\\theta_1$ is the same under both models, i.e., $p(\\theta_1 \\mid M_1) = p(\\theta_1 \\mid M_2)$. These terms thus cancel. The posterior ratio simplifies to:\n$$ \\frac{p(M_2, \\theta_2^\\star \\mid y)}{p(M_1, \\theta_1 \\mid y)} = \\frac{p(y \\mid \\theta_2^\\star, M_2) \\, p(c_E^\\star \\mid M_2) \\, p(M_2)}{p(y \\mid \\theta_1, M_1) \\, p(M_1)} $$\n\nNow, let's substitute the given distributions:\nThe likelihood ratio is:\n$$ \\frac{p(y \\mid \\theta_2^\\star, M_2)}{p(y \\mid \\theta_1, M_1)} = \\frac{\\mathcal{N}(y \\mid b+ac_E^\\star, \\sigma^2)}{\\mathcal{N}(y \\mid b, \\sigma^2)} = \\frac{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - (b+ac_E^\\star))^2}{2\\sigma^2}\\right)}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-b)^2}{2\\sigma^2}\\right)} $$\nWith $y=0$, this becomes:\n$$ \\exp\\left(\\frac{b^2 - (b+ac_E^\\star)^2}{2\\sigma^2}\\right) $$\nThe parameter prior for $c_E$ is $p(c_E^\\star \\mid M_2) = \\mathcal{N}(c_E^\\star \\mid 0, \\tau^2) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left(-\\frac{(c_E^\\star)^2}{2\\tau^2}\\right)$.\nThe model prior ratio is $\\frac{p(M_2)}{p(M_1)}$.\n\nSecond, the Proposal Ratio:\n$$ \\frac{r_{21} \\, q_{21}(\\cdot \\mid \\theta_2^\\star)}{r_{12} \\, q_{12}(u^\\star \\mid \\theta_1)} $$\nGiven $r_{12}=r_{21}=0.5$, this ratio is $1$. The reverse proposal $q_{21}=1$. The forward proposal for the auxiliary variable is $q_{12}(u^\\star \\mid \\theta_1) = \\mathcal{N}(u^\\star \\mid m, s^2) = \\frac{1}{\\sqrt{2\\pi s^2}} \\exp\\left(-\\frac{(u^\\star - m)^2}{2s^2}\\right)$.\nSo the proposal ratio term evaluates to $\\frac{1}{q_{12}(u^\\star \\mid \\theta_1)}$.\n\nThird, the Jacobian is given as $|J|=1$.\n\nCombining all terms, and recalling $c_E^\\star=u^\\star$, the full acceptance ratio $R$ is:\n$$ R = \\frac{p(y \\mid \\theta_2^\\star, M_2)}{p(y \\mid \\theta_1, M_1)} \\times \\frac{p(M_2)}{p(M_1)} \\times p(c_E^\\star \\mid M_2) \\times \\frac{1}{q_{12}(u^\\star \\mid \\theta_1)} \\times 1 $$\nSubstituting the probability density functions:\n$$ R = \\exp\\left(\\frac{b^2 - (b+au^\\star)^2}{2\\sigma^2}\\right) \\times \\frac{p(M_2)}{p(M_1)} \\times \\frac{\\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left(-\\frac{(u^\\star)^2}{2\\tau^2}\\right)}{\\frac{1}{\\sqrt{2\\pi s^2}} \\exp\\left(-\\frac{(u^\\star-m)^2}{2s^2}\\right)} $$\n$$ R = \\exp\\left(\\frac{b^2 - (b+au^\\star)^2}{2\\sigma^2}\\right) \\times \\frac{p(M_2)}{p(M_1)} \\times \\frac{s}{\\tau} \\exp\\left(\\frac{(u^\\star-m)^2}{2s^2} - \\frac{(u^\\star)^2}{2\\tau^2}\\right) $$\nNow, we substitute the numerical values provided:\n$b=0.5$, $a=-2.0$, $\\sigma=0.3$, $u^\\star=-0.3$, $p(M_1)=0.6$, $p(M_2)=0.4$, $\\tau=1$, $m=-0.1$, $s=0.5$.\n\n1.  Likelihood-related term exponent:\n    The mean for $M_2$ is $\\mu_2 = b + a u^\\star = 0.5 + (-2.0)(-0.3) = 0.5 + 0.6 = 1.1$.\n    The exponent is $\\frac{b^2 - \\mu_2^2}{2\\sigma^2} = \\frac{(0.5)^2 - (1.1)^2}{2(0.3)^2} = \\frac{0.25 - 1.21}{2(0.09)} = \\frac{-0.96}{0.18} = -\\frac{16}{3}$.\n\n2.  Model prior ratio:\n    $\\frac{p(M_2)}{p(M_1)} = \\frac{0.4}{0.6} = \\frac{2}{3}$.\n\n3.  Prior/Proposal term:\n    The prefactor is $\\frac{s}{\\tau} = \\frac{0.5}{1} = 0.5$.\n    The exponent is $\\frac{(u^\\star-m)^2}{2s^2} - \\frac{(u^\\star)^2}{2\\tau^2} = \\frac{(-0.3 - (-0.1))^2}{2(0.5)^2} - \\frac{(-0.3)^2}{2(1)^2} = \\frac{(-0.2)^2}{0.5} - \\frac{0.09}{2} = \\frac{0.04}{0.5} - 0.045 = 0.08 - 0.045 = 0.035$.\n\nCombining these pieces into the expression for $R$:\n$$ R = \\exp\\left(-\\frac{16}{3}\\right) \\times \\frac{2}{3} \\times 0.5 \\times \\exp(0.035) $$\n$$ R = \\exp\\left(-\\frac{16}{3} + 0.035\\right) \\times \\left(\\frac{2}{3} \\times \\frac{1}{2}\\right) $$\n$$ R = \\exp\\left(-5.3333... + 0.035\\right) \\times \\frac{1}{3} $$\n$$ R = \\exp\\left(-5.298333...\\right) \\times \\frac{1}{3} $$\n$$ R \\approx (0.005000406) \\times \\frac{1}{3} \\approx 0.001666802 $$\nThe acceptance probability $\\alpha_{\\text{birth}} = \\min(1, R)$. Since $R  1$, we have $\\alpha_{\\text{birth}} = R$.\nRounding the result to four significant figures gives $0.001667$.", "answer": "$$\\boxed{0.001667}$$", "id": "3604527"}]}