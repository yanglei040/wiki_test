{"hands_on_practices": [{"introduction": "An numerical algorithm is only as reliable as its stopping condition. This first exercise explores a critical, and often overlooked, subtlety in root-finding: the difference between a small residual value, $|f(x)|$, and a small error in the root's location, $|x - x^*|$. By analyzing a hypothetical quasiparticle dispersion relation [@problem_id:3532388], you will discover why residual-based stopping criteria can be deceptive, especially for functions that are nearly flat near the root, and appreciate why robust methods must guarantee convergence in the problem's domain ($x$) rather than its range ($f(x)$).", "problem": "In a numerical routine for locating the on-shell energy $E^{*}$ of a quasiparticle with three-momentum $p$ in a relativistic medium, you solve the nonlinear equation $f(E)=0$, where\n$$\nf(E) \\equiv E - \\sqrt{p^{2} + m^{2} + \\Sigma(E)} \\, ,\n$$\nwith $p$ the magnitude of the three-momentum, $m$ the bare mass, and $\\Sigma(E)$ a smooth, real-valued self-energy correction. Assume $f$ is continuous on the initial bracket $[a_{0}, b_{0}]$ with $f(a_{0}) f(b_{0})  0$, and that there is a unique simple root $E^{*} \\in (a_{0}, b_{0})$. A bisection routine generates nested subintervals $[a_{n}, b_{n}]$ and midpoints $E_{n} = (a_{n} + b_{n})/2$.\n\nTwo termination criteria are under consideration in the code:\n- Interval-width criterion: stop at the first $n$ with $b_{n} - a_{n} \\le \\tau_{x}$.\n- Residual criterion: stop at the first $n$ with $\\lvert f(E_{n}) \\rvert \\le \\tau_{f}$.\n\nYour software must meet a user-specified absolute error goal $\\lvert E_{n} - E^{*} \\rvert \\le \\varepsilon_{\\text{abs}}$ for a given $\\varepsilon_{\\text{abs}}  0$, independent of the overall scaling of $f$. In some kinematic regimes relevant to high-energy scattering, $\\Sigma(E)$ can render $f$ nearly flat near $E^{*}$, meaning $\\lvert f'(E^{*}) \\rvert$ is small but nonzero.\n\nConsider also the toy model $f(E) = \\varepsilon \\, (E - E^{*})$ with a small scale factor $\\varepsilon  0$ representing a nearly flat residual, and an initial bracket $[E^{*} - 1, \\, E^{*} + 1]$. Take specific tolerances $\\tau_{x} = 10^{-6} \\, \\mathrm{GeV}$ and $\\tau_{f} = 10^{-12} \\, \\mathrm{GeV}$, and $\\varepsilon = 10^{-8}$ (dimensionless), with energies in gigaelectronvolts (GeV).\n\nWhich of the following statements are correct? Select all that apply.\n\nA. If the bisection routine stops at iteration $n$ when $b_{n} - a_{n} \\le \\tau_{x}$, then the midpoint $E_{n}$ satisfies $\\lvert E_{n} - E^{*} \\rvert \\le \\tau_{x}/2$, regardless of the behavior or scaling of $f$.\n\nB. If the routine stops at the first $n$ such that $\\lvert f(E_{n}) \\rvert \\le \\tau_{f}$, then it necessarily follows that $\\lvert E_{n} - E^{*} \\rvert \\le \\tau_{f}$, provided $f$ is continuous and monotone on $[a_{0}, b_{0}]$.\n\nC. In the toy model $f(E) = 10^{-8} (E - E^{*})$ with initial bracket $[E^{*} - 1, \\, E^{*} + 1]$, the residual-based criterion with $\\tau_{f} = 10^{-12} \\, \\mathrm{GeV}$ can trigger termination at an iteration where $\\lvert E_{n} - E^{*} \\rvert \\approx 10^{-4} \\, \\mathrm{GeV}$, even though the interval-width criterion with $\\tau_{x} = 10^{-6} \\, \\mathrm{GeV}$ has not yet been satisfied.\n\nD. If there is a known constant $m  0$ such that $\\lvert f'(E) \\rvert \\ge m$ for all $E \\in [a_{0}, b_{0}]$, then any iterate $E_{n}$ with $\\lvert f(E_{n}) \\rvert \\le \\tau_{f}$ must satisfy $\\lvert E_{n} - E^{*} \\rvert \\le \\tau_{f}/m$.\n\nE. For bisection, the residual sequence $\\lvert f(E_{n}) \\rvert$ is monotonically nonincreasing with $n$, so residual-based and interval-based stopping are effectively equivalent for continuous $f$.\n\nAnswer options:\n- A\n- B\n- C\n- D\n- E", "solution": "### Step 1: Extract Givens\n- The equation to be solved is $f(E)=0$, where $f(E) \\equiv E - \\sqrt{p^{2} + m^{2} + \\Sigma(E)}$.\n- $p$ is the magnitude of the three-momentum, $m$ is the bare mass, and $\\Sigma(E)$ is a smooth, real-valued self-energy correction.\n- $f$ is continuous on an initial bracket $[a_{0}, b_{0}]$.\n- $f(a_{0}) f(b_{0})  0$.\n- There exists a unique simple root $E^{*} \\in (a_{0}, b_{0})$.\n- The algorithm is the bisection method, generating intervals $[a_{n}, b_{n}]$ and midpoints $E_{n} = (a_{n} + b_{n})/2$.\n- Termination criterion 1 (interval-width): stop when $b_{n} - a_{n} \\le \\tau_{x}$.\n- Termination criterion 2 (residual): stop when $\\lvert f(E_{n}) \\rvert \\le \\tau_{f}$.\n- The user requires a guaranteed absolute error $\\lvert E_{n} - E^{*} \\rvert \\le \\varepsilon_{\\text{abs}}$.\n- A key scenario is when $\\lvert f'(E^{*}) \\rvert$ is small but nonzero.\n- A toy model is provided for analysis: $f(E) = \\varepsilon \\, (E - E^{*})$ with $\\varepsilon = 10^{-8}$.\n- The initial bracket for the toy model is $[E^{*} - 1, \\, E^{*} + 1]$.\n- Specific tolerances are given: $\\tau_{x} = 10^{-6} \\, \\mathrm{GeV}$ and $\\tau_{f} = 10^{-12} \\, \\mathrm{GeV}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Groundedness**: The problem is formulated in the context of relativistic quantum field theory and many-body physics, specifically the calculation of a quasiparticle's dispersion relation. The form of the equation $E = \\sqrt{p^2 + m^2 + \\Sigma(E)}$ is standard for a particle with an energy-dependent self-energy. The use of numerical root-finding techniques like bisection is a fundamental and correct approach to solving such transcendental equations. The problem is scientifically sound.\n- **Well-Posedness**: The problem assumptions—continuity, a sign change over an interval, and a unique simple root—are precisely the conditions required by the Intermediate Value Theorem and guarantee that the bisection method will converge to the unique root. The questions asked are well-defined mathematical inquiries about the properties of the algorithm and its stopping criteria. The problem is well-posed.\n- **Objectivity**: The language is formal, precise, and devoid of any subjective or ambiguous terminology. All concepts are standard in numerical analysis and physics. The problem is objective.\n- **Completeness and Consistency**: All data and conditions necessary for the analysis are provided. The general setup, the specific numerical values for the toy model, and the assumptions about the function $f$ are self-consistent and sufficient to evaluate the given statements.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will now proceed with the analysis of each option.\n\n---\n\n### Analysis of Provided Options\n\n**Option A: If the bisection routine stops at iteration $n$ when $b_{n} - a_{n} \\le \\tau_{x}$, then the midpoint $E_{n}$ satisfies $\\lvert E_{n} - E^{*} \\rvert \\le \\tau_{x}/2$, regardless of the behavior or scaling of $f$.**\n\nThe bisection algorithm guarantees that at each step $n$, the root $E^{*}$ lies within the interval $[a_{n}, b_{n}]$. The midpoint of this interval is $E_{n} = (a_{n} + b_{n})/2$. The maximum possible distance between the midpoint $E_{n}$ and any point within the interval $[a_{n}, b_{n}]$ is half the length of the interval. Since $E^{*} \\in [a_{n}, b_{n}]$, the absolute error of the midpoint is bounded by:\n$$\n\\lvert E_{n} - E^{*} \\rvert \\le \\frac{b_{n} - a_{n}}{2}\n$$\nThe routine terminates at the first iteration $n$ for which $b_{n} - a_{n} \\le \\tau_{x}$. Substituting this condition into the error bound gives:\n$$\n\\lvert E_{n} - E^{*} \\rvert \\le \\frac{b_{n} - a_{n}}{2} \\le \\frac{\\tau_{x}}{2}\n$$\nThis bound on the error depends only on the width of the final interval and not on any properties of the function $f(E)$ itself, such as its derivative or scaling, as long as the initial conditions for convergence are met. Therefore, this statement is a fundamental property of the bisection method.\n\n**Verdict: Correct.**\n\n**Option B: If the routine stops at the first $n$ such that $\\lvert f(E_{n}) \\rvert \\le \\tau_{f}$, then it necessarily follows that $\\lvert E_{n} - E^{*} \\rvert \\le \\tau_{f}$, provided $f$ is continuous and monotone on $[a_{0}, b_{0}]$.**\n\nThis statement proposes a direct relationship between the residual tolerance $\\tau_{f}$ and the absolute error $\\lvert E_{n} - E^{*} \\rvert$. We can analyze this using the Mean Value Theorem. Since $f$ is continuous on $[a_n, b_n]$ and differentiable in $(a_n, b_n)$ (from the problem stating $\\Sigma(E)$ is smooth), there exists a point $\\xi$ between $E_{n}$ and $E^{*}$ such that:\n$$\nf(E_{n}) - f(E^{*}) = f'(\\xi)(E_{n} - E^{*})\n$$\nGiven that $f(E^{*}) = 0$, this simplifies to $f(E_{n}) = f'(\\xi)(E_{n} - E^{*})$. Taking the absolute value, we find the relationship between the absolute error and the residual:\n$$\n\\lvert E_{n} - E^{*} \\rvert = \\frac{\\lvert f(E_{n}) \\rvert}{\\lvert f'(\\xi) \\rvert}\n$$\nThe stopping condition is $\\lvert f(E_{n}) \\rvert \\le \\tau_{f}$. Substituting this gives:\n$$\n\\lvert E_{n} - E^{*} \\rvert \\le \\frac{\\tau_{f}}{\\lvert f'(\\xi) \\rvert}\n$$\nThe statement claims $\\lvert E_{n} - E^{*} \\rvert \\le \\tau_{f}$. This would only be guaranteed if $\\lvert f'(\\xi) \\rvert \\ge 1$. However, the problem explicitly states we must consider regimes where $\\lvert f'(E^{*}) \\rvert$ is small. If $\\lvert f'(\\xi) \\rvert \\ll 1$, then the absolute error $\\lvert E_{n} - E^{*} \\rvert$ can be significantly larger than the residual $\\lvert f(E_{n}) \\rvert$. The condition of monotonicity only ensures that $f'(E)$ does not change sign, not that its magnitude is large.\n\n**Verdict: Incorrect.**\n\n**Option C: In the toy model $f(E) = 10^{-8} (E - E^{*})$ with initial bracket $[E^{*} - 1, \\, E^{*} + 1]$, the residual-based criterion with $\\tau_{f} = 10^{-12} \\, \\mathrm{GeV}$ can trigger termination at an iteration where $\\lvert E_{n} - E^{*} \\rvert \\approx 10^{-4} \\, \\mathrm{GeV}$, even though the interval-width criterion with $\\tau_{x} = 10^{-6} \\, \\mathrm{GeV}$ has not yet been satisfied.**\n\nLet's analyze the conditions based on the toy model.\nThe residual-based criterion is $\\lvert f(E_{n}) \\rvert \\le \\tau_{f}$. Substituting the model and values:\n$$\n\\lvert 10^{-8} (E_{n} - E^{*}) \\rvert \\le 10^{-12} \\, \\mathrm{GeV}\n$$\n$$\n\\lvert E_{n} - E^{*} \\rvert \\le \\frac{10^{-12}}{10^{-8}} \\, \\mathrm{GeV} = 10^{-4} \\, \\mathrm{GeV}\n$$\nThis means that for the residual criterion to be met, the absolute error must be at most $10^{-4} \\, \\mathrm{GeV}$. Termination can occur if the algorithm produces an iterate $E_n$ such that $\\lvert E_{n} - E^{*} \\rvert \\approx 10^{-4} \\, \\mathrm{GeV}$ (or less).\n\nNow, let's determine if this can happen before the interval-width criterion, $b_{n} - a_{n} \\le \\tau_{x} = 10^{-6} \\, \\mathrm{GeV}$, is met.\nThe initial interval width is $b_{0} - a_{0} = (E^{*} + 1) - (E^{*} - 1) = 2 \\, \\mathrm{GeV}$.\nAt iteration $n$, the interval width is $b_{n} - a_{n} = (b_{0} - a_{0}) / 2^{n} = 2 / 2^{n} = 2^{1-n} \\, \\mathrm{GeV}$.\nThe error of the midpoint is bounded by $\\lvert E_{n} - E^{*} \\rvert \\le (b_{n} - a_{n})/2 = 2^{-n} \\, \\mathrm{GeV}$.\n\nCan we find an iteration $n$ where the residual criterion is met but the interval criterion is not? This requires:\n1. It is possible for $\\lvert E_{n} - E^{*} \\rvert \\approx 10^{-4} \\, \\mathrm{GeV}$. For this to occur, the error bound must allow it: $2^{-n} \\ge 10^{-4} \\, \\mathrm{GeV}$. This implies $-n \\log_{10}(2) \\ge -4$, or $n \\le 4/\\log_{10}(2) \\approx 4/0.301 \\approx 13.3$. So, for iterations up to $n=13$, the error can be on the order of $10^{-4} \\, \\mathrm{GeV}$. For instance, at $n=13$, the error bound is $2^{-13} \\approx 1.22 \\times 10^{-4} \\, \\mathrm{GeV}$. It is entirely possible for the actual error $\\lvert E_{13} - E^{*} \\rvert$ to be, for instance, $0.9 \\times 10^{-4} \\, \\mathrm{GeV}$.\n2. If $\\lvert E_{n} - E^{*} \\rvert = 0.9 \\times 10^{-4} \\, \\mathrm{GeV}$, the residual would be $\\lvert f(E_{n}) \\rvert = 10^{-8} \\times (0.9 \\times 10^{-4}) = 0.9 \\times 10^{-12} \\, \\mathrm{GeV}$. This value satisfies the residual criterion $\\lvert f(E_{n}) \\rvert \\le \\tau_{f} = 10^{-12} \\, \\mathrm{GeV}$. Thus, termination could occur.\n3. At this iteration, say $n=13$, the interval width is $b_{13} - a_{13} = 2^{1-13} = 2^{-12} \\approx 2.44 \\times 10^{-4} \\, \\mathrm{GeV}$. This width is greater than the interval tolerance $\\tau_{x} = 10^{-6} \\, \\mathrm{GeV}$.\n\nSo, at $n=13$, it is possible for the residual criterion to be met, triggering termination with an error of $\\approx 10^{-4} \\, \\mathrm{GeV}$, while the interval-width criterion has not been satisfied.\n\n**Verdict: Correct.**\n\n**Option D: If there is a known constant $m  0$ such that $\\lvert f'(E) \\rvert \\ge m$ for all $E \\in [a_{0}, b_{0}]$, then any iterate $E_{n}$ with $\\lvert f(E_{n}) \\rvert \\le \\tau_{f}$ must satisfy $\\lvert E_{n} - E^{*} \\rvert \\le \\tau_{f}/m$.**\n\nThis statement provides a condition under which the residual $\\lvert f(E_{n}) \\rvert$ can be used to bound the absolute error $\\lvert E_{n} - E^{*} \\rvert$. As derived in the analysis for Option B, the Mean Value Theorem gives:\n$$\n\\lvert E_{n} - E^{*} \\rvert = \\frac{\\lvert f(E_{n}) \\rvert}{\\lvert f'(\\xi) \\rvert}\n$$\nfor some $\\xi$ in the interval between $E_n$ and $E^*$, which is a subinterval of $[a_0, b_0]$.\nThe problem posits that a lower bound $m  0$ for the derivative's magnitude is known, i.e., $\\lvert f'(E) \\rvert \\ge m$ for all $E \\in [a_{0}, b_{0}]$. This implies $\\lvert f'(\\xi) \\rvert \\ge m$, and therefore $1/\\lvert f'(\\xi) \\rvert \\le 1/m$.\nUsing this in the error relation:\n$$\n\\lvert E_{n} - E^{*} \\rvert = \\frac{\\lvert f(E_{n}) \\rvert}{\\lvert f'(\\xi) \\rvert} \\le \\frac{\\lvert f(E_{n}) \\rvert}{m}\n$$\nIf an iterate $E_{n}$ satisfies the residual criterion $\\lvert f(E_{n}) \\rvert \\le \\tau_{f}$, we can substitute this bound:\n$$\n\\lvert E_{n} - E^{*} \\rvert \\le \\frac{\\tau_{f}}{m}\n$$\nThe derivation is direct and mathematically sound. This provides a robust way to relate the residual tolerance to the desired error tolerance, provided a bound $m$ can be found.\n\n**Verdict: Correct.**\n\n**Option E: For bisection, the residual sequence $\\lvert f(E_{n}) \\rvert$ is monotonically nonincreasing with $n$, so residual-based and interval-based stopping are effectively equivalent for continuous $f$.**\n\nThis statement makes two claims. Let's analyze the first one: \"the residual sequence $\\lvert f(E_{n}) \\rvert$ is monotonically nonincreasing with $n$.\" This means $\\lvert f(E_{n+1}) \\rvert \\le \\lvert f(E_{n}) \\rvert$ for all $n$.\nThe bisection method chooses the midpoint $E_n$ of the current interval $[a_n, b_n]$. The next interval, $[a_{n+1}, b_{n+1}]$, is one half of $[a_n, b_n]$, and the next midpoint $E_{n+1}$ is the center of that new interval. While the interval containing the root $E^*$ shrinks monotonically, the sequence of midpoints $E_n$ does not necessarily get closer to $E^*$ at every step. Consequently, the residual $\\lvert f(E_n) \\rvert$ is also not guaranteed to decrease monotonically.\nLet's construct a simple counterexample. Let $f(E) = E - E^{*}$ with $E^{*} = 0.6$. Let the initial interval be $[a_{0}, b_{0}] = [0, 2]$.\n- Step $n=0$: $E_{0} = (0+2)/2 = 1$. The absolute error is $\\lvert 1 - 0.6 \\rvert = 0.4$. The residual is $\\lvert f(E_0) \\rvert = 0.4$. Since $f(E_0) = 0.4  0$, the new interval is $[a_1, b_1] = [0, 1]$.\n- Step $n=1$: $E_{1} = (0+1)/2 = 0.5$. The absolute error is $\\lvert 0.5 - 0.6 \\rvert = 0.1$. The residual is $\\lvert f(E_1) \\rvert = 0.1$. Here, $\\lvert f(E_1) \\rvert  \\lvert f(E_0) \\rvert$.\n- Step $n=2$: Since $f(E_1) = -0.1  0$, the new interval is $[a_2, b_2] = [0.5, 1]$. The midpoint is $E_{2} = (0.5+1)/2 = 0.75$. The absolute error is $\\lvert 0.75 - 0.6 \\rvert = 0.15$. The residual is $\\lvert f(E_2) \\rvert = 0.15$.\nWe have found that $\\lvert f(E_{2}) \\rvert = 0.15  \\lvert f(E_{1}) \\rvert = 0.1$. The sequence of residuals is not monotonically nonincreasing.\n\nThe first part of the statement is false. The conclusion—\"so residual-based and interval-based stopping are effectively equivalent\"—is also false, as demonstrated comprehensively in the analyses of options B and C. The two criteria measure different things, and their relationship depends critically on the local derivative of the function, $\\lvert f'\\rvert$.\n\n**Verdict: Incorrect.**", "answer": "$$\\boxed{ACD}$$", "id": "3532388"}, {"introduction": "Newton's method is prized for its speed, but it can be notoriously unstable, especially when the function exhibits singularities. This practice delves into the failure modes of Newton's method by examining the classic quantum mechanical problem of finding bound-state energies in a finite square well, where the transcendental equation features poles [@problem_id:3588646]. You will analyze how an iterate behaves near these poles and evaluate modern safeguarding techniques, such as line searches on a merit function, that prevent divergence and ensure convergence.", "problem": "Consider a one-dimensional finite square-well potential of width $a0$ and depth $V_00$ used as a toy model in computational nuclear physics to approximate single-particle bound states in a mean-field. The even-parity bound-state energies $E$ in the interval $0EV_0$ satisfy the transcendental condition $f(E)=0$, where $f(E)$ is defined by $f(E)=k(E)\\cot\\!\\big(a\\,k(E)\\big)+\\kappa(E)$ with $k(E)=\\frac{\\sqrt{2 m E}}{\\hbar}$ and $\\kappa(E)=\\frac{\\sqrt{2 m (V_0-E)}}{\\hbar}$, for mass $m$ and reduced Planck constant $\\hbar$. The function $f(E)$ has simple poles at the energies where $a\\,k(E)=n\\pi$ for integer $n\\geq 1$. The goal is to find $E$ such that $f(E)=0$ by a root-finding algorithm. You are asked to analyze failure modes of the Newton iteration $E_{k+1}=E_k-\\frac{f(E_k)}{f^{\\prime}(E_k)}$ when applied to this problem and to evaluate a damping strategy based on a line search applied to the descent direction $-\\frac{f^{\\prime}(E_k)}{f(E_k)}$. Let $\\theta(E)=a\\,k(E)$ and let $\\pi$ denote the usual circular constant.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. In exact arithmetic, if $E_0$ is chosen so that $\\theta(E_0)\\in(n\\pi,(n+1)\\pi)$ for some integer $n$, then the first Newton update cannot land exactly on a pole; moreover, for $\\theta(E_0)$ sufficiently close to $n\\pi$ one finds the local mapping in $\\theta$-space is approximately $\\theta(E_{1})\\approx 2\\,\\theta(E_0)-n\\pi$, which moves away from the pole.\n\nB. In floating-point arithmetic, if $E_0$ is chosen so that $\\theta(E_0)=n\\pi+\\varepsilon$ with $0\\varepsilon$ on the order of machine precision, then the evaluation of $f(E_0)$ and $f^{\\prime}(E_0)$ can overflow or suffer severe loss of significance due to the pole of $\\cot$, so that subsequent steps may become undefined or jump outside the physical domain $0EV_0$, effectively causing divergence triggered by proximity to the pole.\n\nC. A damping strategy of the form $E_{k+1}=E_k-\\alpha_k\\,\\frac{f^{\\prime}(E_k)}{f(E_k)}$ together with a backtracking line search that enforces a strict decrease of the merit function $\\phi(E)=\\tfrac{1}{2}\\ln\\big(f(E)^2\\big)$ prevents crossing any pole, because $\\phi(E)\\to+\\infty$ as $E$ approaches a pole; the acceptance criterion rejects steps that would increase $\\phi$, forcing $\\alpha_k$ to be sufficiently small to keep the iterate within the open interval between consecutive poles.\n\nD. A robust practical strategy is to first bracket a root by identifying an interval between consecutive poles where $f(E)$ changes sign and then apply a safeguarded Newton method starting from the midpoint of that interval; this avoids the divergence associated with poles and yields global convergence to the unique root in that interval.\n\nE. Choosing $E_0$ exactly at a pole, that is $\\theta(E_0)=n\\pi$, yields immediate convergence of Newton’s method because both $f(E_0)$ and $f^{\\prime}(E_0)$ are infinite, making their ratio finite and well-defined.", "solution": "The function to find the roots of is $f(E) = k(E)\\cot(a k(E)) + \\kappa(E)$. Let's denote $\\theta(E) = a k(E)$. The poles of $f(E)$ occur when $\\theta(E) = n\\pi$ for an integer $n \\ge 1$. The corresponding energies of the poles are $E_{p,n} = \\frac{n^2 \\pi^2 \\hbar^2}{2ma^2}$. We are tasked with analyzing the behavior of Newton's method and related strategies for finding roots $E$ where $f(E)=0$.\n\n**A. In exact arithmetic, if $E_0$ is chosen so that $\\theta(E_0)\\in(n\\pi,(n+1)\\pi)$ for some integer $n$, then the first Newton update cannot land exactly on a pole; moreover, for $\\theta(E_0)$ sufficiently close to $n\\pi$ one finds the local mapping in $\\theta$-space is approximately $\\theta(E_{1})\\approx 2\\,\\theta(E_0)-n\\pi$, which moves away from the pole.**\n\nLet's analyze the behavior of Newton's method near a pole $E_p$ defined by $\\theta(E_p) = n\\pi$. Let the current iterate be $E_0$, close to $E_p$. Let $E_0 = E_p + \\delta E$.\nIn this region, the cotangent term dominates $f(E)$. Let $\\delta\\theta = \\theta(E_0) - n\\pi$.\nUsing a Taylor expansion for $\\theta(E_0)$ around $E_p$: $\\theta(E_0) \\approx \\theta(E_p) + \\left.\\frac{d\\theta}{dE}\\right|_{E_p} \\delta E$, so $\\delta\\theta \\approx \\left.\\frac{d\\theta}{dE}\\right|_{E_p} \\delta E$.\nThe function $f(E_0)$ behaves as:\n$f(E_0) \\approx k(E_p)\\cot(n\\pi+\\delta\\theta) = k(E_p)\\cot(\\delta\\theta) \\approx \\frac{k(E_p)}{\\delta\\theta}$.\nTo find $f'(E_0)$, we use the chain rule: $f'(E) = \\frac{df}{d\\theta}\\frac{d\\theta}{dE}$.\n$\\frac{df}{d\\theta} = \\frac{d}{d\\theta}[k\\cot\\theta+\\kappa] = \\frac{dk}{d\\theta}\\cot\\theta - k\\csc^2\\theta + \\frac{d\\kappa}{d\\theta}$. Near the pole, the $k\\csc^2\\theta$ term dominates.\n$\\frac{df}{d\\theta} \\approx -k(E_p)\\csc^2(n\\pi+\\delta\\theta) \\approx -\\frac{k(E_p)}{(\\delta\\theta)^2}$.\nSo, $f'(E_0) \\approx \\left( -\\frac{k(E_p)}{(\\delta\\theta)^2} \\right) \\left. \\frac{d\\theta}{dE} \\right|_{E_p}$.\nThe Newton update is $E_1 = E_0 - \\frac{f(E_0)}{f'(E_0)}$.\n$\\frac{f(E_0)}{f'(E_0)} \\approx \\frac{k(E_p)/\\delta\\theta}{(-k(E_p)/(\\delta\\theta)^2) \\frac{d\\theta}{dE}|_{E_p}} = -\\frac{\\delta\\theta}{d\\theta/dE|_{E_p}} \\approx -\\delta E$.\nSo, $E_1 \\approx E_0 - (-\\delta E) = E_0 + \\delta E = (E_p + \\delta E) + \\delta E = E_p + 2\\delta E$.\nThis means $E_1 - E_p \\approx 2(E_0 - E_p)$. The distance from the pole approximately doubles; the iterate is repelled by the pole.\nIn $\\theta$-space: $\\theta(E_1) - n\\pi \\approx \\left.\\frac{d\\theta}{dE}\\right|_{E_p} (E_1-E_p) \\approx \\left.\\frac{d\\theta}{dE}\\right|_{E_p} (2(E_0 - E_p)) = 2(\\theta(E_0)-n\\pi)$.\nLetting $\\theta_0 = \\theta(E_0)$ and $\\theta_1 = \\theta(E_1)$, we have $\\theta_1 - n\\pi \\approx 2(\\theta_0 - n\\pi)$, which rearranges to $\\theta_1 \\approx 2\\theta_0 - n\\pi$. This matches the expression in the statement. Since the distance to the pole $|\\theta_1 - n\\pi|$ is approximately twice the old distance $|\\theta_0 - n\\pi|$, the method moves away from the pole. An iterate near a pole is pushed away and does not land on it (unless it was already there, where the method is undefined).\n**Verdict: Correct.**\n\n**B. In floating-point arithmetic, if $E_0$ is chosen so that $\\theta(E_0)=n\\pi+\\varepsilon$ with $0\\varepsilon$ on the order of machine precision, then the evaluation of $f(E_0)$ and $f^{\\prime}(E_0)$ can overflow or suffer severe loss of significance due to the pole of $\\cot$, so that subsequent steps may become undefined or jump outside the physical domain $0EV_0$, effectively causing divergence triggered by proximity to the pole.**\nIf $\\theta_0 = n\\pi + \\varepsilon$ where $\\varepsilon$ is a very small positive number (e.g., machine precision, $\\approx 10^{-16}$ for double precision), then:\n$\\cot(\\theta_0) = \\cot(\\varepsilon) \\approx 1/\\varepsilon$. This value will be very large, e.g., $\\approx 10^{16}$.\n$f(E_0) = k(E_0)\\cot(\\theta_0) + \\kappa(E_0)$, which will be a very large number, potentially causing floating-point overflow.\nFor the derivative, $f'(E) = \\frac{1}{2E}[k\\cot(ak) - ak^2\\csc^2(ak)] - \\frac{m}{\\hbar^2\\kappa}$.\nThe term $\\csc^2(\\theta_0) = \\csc^2(\\varepsilon) \\approx 1/\\varepsilon^2$. This will be an extremely large number (e.g., $\\approx 10^{32}$), which is very likely to cause an overflow during the calculation of $f'(E_0)$.\nWhen computers handle such large numbers, the ratio $f(E_0)/f'(E_0)$ becomes highly unstable. The computed values of $f(E_0)$ and $f'(E_0)$ might be `Inf` or `NaN`, or simply grossly inaccurate. A small error in the denominator $f'(E_0)$ (perhaps making it close to zero or giving it the wrong sign) can make the Newton step $E_1-E_0$ enormous and unpredictable. Such a large, erroneous step can easily cause $E_1$ to be less than $0$ or greater than $V_0$, violating the physical constraints of the problem and causing the algorithm to fail. This is a classic example of numerical instability near a singularity.\n**Verdict: Correct.**\n\n**C. A damping strategy of the form $E_{k+1}=E_k-\\alpha_k\\,\\frac{f^{\\prime}(E_k)}{f(E_k)}$ together with a backtracking line search that enforces a strict decrease of the merit function $\\phi(E)=\\tfrac{1}{2}\\ln\\big(f(E)^2\\big)$ prevents crossing any pole, because $\\phi(E)\\to+\\infty$ as $E$ approaches a pole; the acceptance criterion rejects steps that would increase $\\phi$, forcing $\\alpha_k$ to be sufficiently small to keep the iterate within the open interval between consecutive poles.**\nThe merit function is $\\phi(E) = \\frac{1}{2}\\ln(f(E)^2) = \\ln|f(E)|$. Its derivative is $\\phi'(E) = \\frac{f'(E)}{f(E)}$.\nThe proposed update is $E_{k+1} = E_k - \\alpha_k \\frac{f'(E_k)}{f(E_k)}$. This is a gradient descent method for the function $\\phi(E)$, since the search direction is the negative of the gradient of $\\phi(E)$.\nAs an iterate $E$ approaches a pole $E_p$, we have $|f(E)| \\to \\infty$. Consequently, $\\phi(E) = \\ln|f(E)| \\to +\\infty$. The poles of $f(E)$ correspond to infinite vertical asymptotes for the merit function $\\phi(E)$.\nA backtracking line search aims to find a step size $\\alpha_k  0$ such that $\\phi(E_{k+1})  \\phi(E_k)$. Let's say the current iterate $E_k$ lies in an interval between two consecutive poles, $(E_{p,n}, E_{p,n+1})$. In this interval, $\\phi(E_k)$ is finite. If a proposed step (e.g., with $\\alpha_k=1$) would cause $E_{k+1}$ to \"cross\" a pole and land in another interval, the path from $E_k$ to $E_{k+1}$ must pass through a point where $\\phi(E)$ is infinite. A line search algorithm that evaluates $\\phi$ at trial points cannot step over an infinite barrier. The acceptance criterion (e.g. strict decrease or the Armijo condition) will fail for any step that crosses the pole. The backtracking mechanism will then successively reduce $\\alpha_k$ until the new point $E_{k+1}$ is close enough to $E_k$ to satisfy the descent condition, which will necessarily be within the original interval $(E_{p,n}, E_{p,n+1})$. The infinite barriers of the merit function effectively act as walls, confining the search.\n**Verdict: Correct.**\n\n**D. A robust practical strategy is to first bracket a root by identifying an interval between consecutive poles where $f(E)$ changes sign and then apply a safeguarded Newton method starting from the midpoint of that interval; this avoids the divergence associated with poles and yields global convergence to the unique root in that interval.**\nLet's analyze the function $f(E)$ in an interval $(E_{p,n}, E_{p,n+1})$ between two consecutive poles.\nAs $E \\to E_{p,n}^+$, we have $\\theta(E) \\to (n\\pi)^+$, so $\\cot(\\theta(E)) \\to +\\infty$. Thus, $\\lim_{E\\to E_{p,n}^+} f(E) = +\\infty$.\nAs $E \\to E_{p,n+1}^-$, we have $\\theta(E) \\to ((n+1)\\pi)^-$, so $\\cot(\\theta(E)) \\to -\\infty$. Thus, $\\lim_{E\\to E_{p,n+1}^-} f(E) = -\\infty$.\nSince $f(E)$ is continuous in $(E_{p,n}, E_{p,n+1})$ and goes from $+\\infty$ to $-\\infty$, the Intermediate Value Theorem guarantees at least one root in this interval.\nTo check for uniqueness, we examine the derivative $f'(E)$.\n$f'(E) = \\frac{d}{dE}[k\\cot(ak)] + \\frac{d\\kappa}{dE}$.\nThe second term is $\\frac{d\\kappa}{dE} = -\\frac{m}{\\hbar^2\\kappa}  0$.\nThe first term is $\\frac{d}{dE}[k\\cot(ak)] = \\frac{d\\theta/dE}{a}[\\cot\\theta - \\theta\\csc^2\\theta]$. Since $\\frac{d\\theta}{dE} = a\\frac{dk}{dE} = \\frac{ak}{2E}  0$, we check the sign of $\\cot\\theta - \\theta\\csc^2\\theta = \\frac{\\sin\\theta\\cos\\theta - \\theta}{\\sin^2\\theta}$. The function $g(\\theta)=\\sin\\theta\\cos\\theta-\\theta$ has derivative $g'(\\theta) = \\cos^2\\theta - \\sin^2\\theta - 1 = -2\\sin^2\\theta \\le 0$. For $\\theta0$, $g(\\theta)g(0)=0$. Thus $\\frac{d}{dE}[k\\cot(ak)]  0$.\nSince both terms in $f'(E)$ are strictly negative, $f'(E)  0$ for all $E$ between poles. Therefore, $f(E)$ is strictly decreasing between any two poles. This guarantees that the root in each such interval is unique.\nA strategy that first identifies such an interval $(E_{p,n}, E_{p,n+1})$ has successfully bracketed the unique root. Applying a safeguarded method, such as Newton-Bisection which combines the speed of Newton's method with the robustness of bisection, is guaranteed to converge to the root within the bracket. This is a standard and highly effective technique in numerical root-finding.\n**Verdict: Correct.**\n\n**E. Choosing $E_0$ exactly at a pole, that is $\\theta(E_0)=n\\pi$, yields immediate convergence of Newton’s method because both $f(E_0)$ and $f^{\\prime}(E_0)$ are infinite, making their ratio finite and well-defined.**\nThis statement is fundamentally incorrect. The Newton-Raphson method requires the evaluation of $f(E_0)$ and $f'(E_0)$. At a pole $E_p$, the function $f(E)$ is undefined; its value is infinite. A numerical algorithm cannot operate with infinite values. Attempting to compute $f(E_p)$ would result in a fatal error (e.g., division by zero in the computation of a cotangent) or an `Infinity` representation. The fraction $f(E_0)/f'(E_0)$ would become `Inf/Inf` or `NaN`, which is not a well-defined number that can be used for an update step. The idea of \"the ratio of two infinities\" is a concept from limit theory (e.g., L'Hôpital's rule) and has no place in the direct execution of an algorithm at a point of singularity. The method is simply not defined at a pole. Furthermore, as shown in the analysis for option A, the behavior *near* a pole is repulsion, not convergence.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{ABCD}$$", "id": "3588646"}, {"introduction": "We now synthesize these concepts to tackle a problem representative of modern computational nuclear physics: solving the finite-temperature gap equation for a dynamical mass. This task requires you to design and implement a complete, robust hybrid algorithm combining the Newton-Raphson and bisection methods [@problem_id:3532433]. You will confront the practical challenges of deriving derivatives for integral-defined functions, performing numerical quadrature, and establishing a mathematically sound bracketing interval based on the physical properties of the system.", "problem": "Consider a relativistic Nambu–Jona-Lasinio-type finite-temperature gap equation formulated in natural units (Planck constant $\\hbar$, speed of light $c$, and Boltzmann constant $k_{\\mathrm{B}}$ set to $1$), where all energies, momenta, and temperatures are measured in gigaelectronvolts (GeV). The quasiparticle dispersion is modeled as $E_{p}(\\Delta) = \\sqrt{p^{2} + \\Delta^{2}}$ for a dynamical mass gap $\\Delta \\ge 0$, and the finite-temperature gap equation is defined by the nonlinear function\n$$\nf(\\Delta) \\equiv \\Delta - \\lambda \\int_{|p| \\le \\Lambda} d^{3}p \\,\\frac{\\tanh\\!\\left(\\dfrac{E_{p}(\\Delta)}{2T}\\right)}{2\\,E_{p}(\\Delta)} \\,,\n$$\nwith coupling parameter $\\lambda$ (in $\\mathrm{GeV}^{-1}$), ultraviolet momentum cutoff $\\Lambda$ (in $\\mathrm{GeV}$), and temperature $T$ (in $\\mathrm{GeV}$). Using spherical symmetry, the integral can be expressed as\n$$\n\\int_{|p| \\le \\Lambda} d^{3}p \\,\\frac{\\tanh\\!\\left(\\dfrac{E_{p}(\\Delta)}{2T}\\right)}{2\\,E_{p}(\\Delta)} \\;=\\; 4\\pi \\int_{0}^{\\Lambda} p^{2}\\,\\frac{\\tanh\\!\\left(\\dfrac{\\sqrt{p^{2}+\\Delta^{2}}}{2T}\\right)}{2\\,\\sqrt{p^{2}+\\Delta^{2}}}\\,dp \\,.\n$$\nThe hyperbolic tangent obeys the identity $\\tanh\\!\\left(\\dfrac{E}{2T}\\right) = 1 - 2\\,n_{\\mathrm{F}}(E)$ where $n_{\\mathrm{F}}(E) = \\dfrac{1}{e^{E/T}+1}$ is the Fermi–Dirac distribution. From the fundamental properties of $n_{\\mathrm{F}}(E)$ and the positivity of $E_{p}(\\Delta)$, it follows that $f(\\Delta)$ is strictly increasing in $\\Delta$ for $\\Delta \\ge 0$, and that $f(0)  0$ while $\\lim_{\\Delta \\to \\infty} f(\\Delta)  0$, guaranteeing a unique root $\\Delta^{\\star}  0$.\n\nTask:\n- Derive, from first principles, an algorithm that robustly finds the unique root $\\Delta^{\\star}$ of $f(\\Delta) = 0$ for given $(\\lambda,\\Lambda,T)$ by combining the Newton–Raphson method with bisection. The design must:\n  1. Start from a mathematically justified bracketing interval $[\\Delta_{\\mathrm{L}}, \\Delta_{\\mathrm{R}}]$ using the monotonicity of $f(\\Delta)$.\n  2. Compute the Newton–Raphson step via the derivative $\\dfrac{df}{d\\Delta}$ derived from the fundamental definitions, and accept it only if it stays inside the current bracket and decreases the residual; otherwise, fall back to a bisection step.\n  3. Use numerically stable quadrature to evaluate the integral for $f(\\Delta)$ and its derivative $\\dfrac{df}{d\\Delta}$ for $\\Delta  0$.\n- Implement this Newton–bisection hybrid in a complete, runnable program that outputs the numerical values of $\\Delta^{\\star}$ for a specified test suite.\n\nUnits and answers:\n- All inputs $\\lambda$, $\\Lambda$, and $T$ must be treated in $\\mathrm{GeV}^{-1}$, $\\mathrm{GeV}$, and $\\mathrm{GeV}$ respectively.\n- The program must output the gap values $\\Delta^{\\star}$ in $\\mathrm{GeV}$ as floating-point numbers.\n- Each reported $\\Delta^{\\star}$ must be rounded to $6$ decimal places.\n\nTest suite:\n- Use the following parameter triples $(\\lambda,\\Lambda,T)$, which probe different regimes:\n  1. $(\\lambda, \\Lambda, T) = (\\,\\;0.10\\,\\mathrm{GeV}^{-1},\\;\\;1.00\\,\\mathrm{GeV},\\;\\;0.05\\,\\mathrm{GeV}\\;\\,)$\n  2. $(\\lambda, \\Lambda, T) = (\\,\\;0.20\\,\\mathrm{GeV}^{-1},\\;\\;1.00\\,\\mathrm{GeV},\\;\\;0.01\\,\\mathrm{GeV}\\;\\,)$\n  3. $(\\lambda, \\Lambda, T) = (\\,\\;0.10\\,\\mathrm{GeV}^{-1},\\;\\;1.00\\,\\mathrm{GeV},\\;\\;0.20\\,\\mathrm{GeV}\\;\\,)$\n  4. $(\\lambda, \\Lambda, T) = (\\,\\;0.05\\,\\mathrm{GeV}^{-1},\\;\\;0.50\\,\\mathrm{GeV},\\;\\;0.02\\,\\mathrm{GeV}\\;\\,)$\n\nFinal output format:\n- Your program should produce a single line of output containing the four computed $\\Delta^{\\star}$ values, in the order of the test suite above, as a comma-separated list enclosed in square brackets, for example:\n$$\n\\texttt{[0.123456,0.234567,0.345678,0.456789]}\n$$\nNo additional text should be printed.", "solution": "The problem statement is a valid and well-posed problem in computational physics. It asks for the derivation and implementation of a robust numerical algorithm to solve a specific nonlinear equation.\n\n**Problem Statement Analysis**\nThe task is to find the unique positive root, $\\Delta^{\\star}$, of the equation $f(\\Delta) = 0$, where the function $f(\\Delta)$ is given by a Nambu–Jona-Lasinio-type gap equation. The function is defined as:\n$$f(\\Delta) \\equiv \\Delta - \\lambda \\int_{|p| \\le \\Lambda} d^{3}p \\,\\frac{\\tanh\\!\\left(\\dfrac{E_{p}(\\Delta)}{2T}\\right)}{2\\,E_{p}(\\Delta)}$$\nHere, $E_{p}(\\Delta) = \\sqrt{p^{2} + \\Delta^{2}}$ is the quasiparticle energy, $\\lambda$ is a coupling constant, $\\Lambda$ is a momentum cutoff, and $T$ is the temperature. Using spherical coordinates, the integral simplifies to:\n$$f(\\Delta) = \\Delta - 2\\pi\\lambda \\int_{0}^{\\Lambda} p^{2}\\,\\frac{\\tanh\\!\\left(\\dfrac{\\sqrt{p^{2}+\\Delta^{2}}}{2T}\\right)}{\\sqrt{p^{2}+\\Delta^{2}}}\\,dp$$\nThe problem guarantees that $f(\\Delta)$ is strictly increasing for $\\Delta \\ge 0$, with $f(0)  0$ and $\\lim_{\\Delta \\to \\infty} f(\\Delta)  0$. These properties ensure that a unique root $\\Delta^{\\star}  0$ exists.\n\n**Derivation of the Algorithm**\nThe required algorithm is a hybrid of the Newton-Raphson method and the bisection method.\n\n**1. Newton-Raphson Component and Derivative Calculation**\nThe Newton-Raphson method finds roots by iteration: $\\Delta_{k+1} = \\Delta_k - \\frac{f(\\Delta_k)}{f'(\\Delta_k)}$. This requires the derivative of $f(\\Delta)$ with respect to $\\Delta$. We compute this using the Leibniz integral rule (differentiation under the integral sign), as the integration limits $0$ and $\\Lambda$ are constant with respect to $\\Delta$.\n$$f'(\\Delta) = \\frac{d}{d\\Delta} \\left( \\Delta - 2\\pi\\lambda \\int_{0}^{\\Lambda} p^{2}\\,\\frac{\\tanh\\left(\\frac{E_{p}(\\Delta)}{2T}\\right)}{E_{p}(\\Delta)}\\,dp \\right)$$\n$$f'(\\Delta) = 1 - 2\\pi\\lambda \\int_{0}^{\\Lambda} p^{2} \\frac{d}{d\\Delta} \\left[ \\frac{\\tanh\\left(\\frac{E_{p}(\\Delta)}{2T}\\right)}{E_{p}(\\Delta)} \\right] dp$$\nUsing the chain rule, $\\frac{d}{d\\Delta} = \\frac{dE_p}{d\\Delta} \\frac{d}{dE_p}$, where $\\frac{dE_p}{d\\Delta} = \\frac{\\Delta}{E_p}$. Differentiating the term in brackets with respect to $E_p$ yields:\n$$\\frac{d}{dE_p}\\left[ \\frac{\\tanh(E_p/2T)}{E_p} \\right] = \\frac{\\left(\\frac{1}{2T}\\text{sech}^2\\left(\\frac{E_p}{2T}\\right)\\right)E_p - \\tanh\\left(\\frac{E_p}{2T}\\right)}{E_p^2}$$\nSubstituting this back, we obtain the full expression for the derivative:\n$$ f'(\\Delta) = 1 - 2\\pi\\lambda\\Delta \\int_{0}^{\\Lambda} \\frac{p^2}{E_p^3(\\Delta)} \\left[ \\frac{E_p(\\Delta)}{2T}\\text{sech}^2\\left(\\frac{E_p(\\Delta)}{2T}\\right) - \\tanh\\left(\\frac{E_p(\\Delta)}{2T}\\right) \\right] dp $$\nThis derivative is essential for the Newton-Raphson step but is undefined at $\\Delta = 0$ because the integrand diverges. Therefore, any evaluation must be for $\\Delta  0$.\n\n**2. Bracketing Interval**\nA robust root-finding algorithm begins with an interval $[\\Delta_L, \\Delta_R]$ that is guaranteed to contain the root, i.e., $f(\\Delta_L)  0$ and $f(\\Delta_R)  0$.\n- **Lower Bound $\\Delta_L$**: The problem states $f(0)  0$. To avoid the singularity in $f'(0)$, we choose a small positive number $\\Delta_L = \\epsilon  0$ (e.g., $\\epsilon=10^{-12}$). Since $f$ is continuous, $f(\\epsilon) \\approx f(0)  0$.\n- **Upper Bound $\\Delta_R$**: A mathematically justified upper bound can be constructed. We require $f(\\Delta_R)  0$, which means $\\Delta_R  2\\pi\\lambda\\int_{0}^{\\Lambda}p^2\\frac{\\tanh(E_p(\\Delta_R)/2T)}{E_p(\\Delta_R)}dp$. A simple and effective upper bound is $\\Delta_R = \\lambda \\Lambda^2 / 2$. This comes from the $T \\to 0$ limit where $\\tanh \\to 1$. More generally, since $\\tanh(x)/x \\le 1$, the integral is bounded by $\\int_0^\\Lambda p^2/E_p dp  \\int_0^\\Lambda p^2/p dp = \\Lambda^2/2$. Thus $f(\\Delta)$ is bounded below by $\\Delta - \\pi\\lambda\\Lambda^2$. Setting this to zero gives an estimate for the root's location. A simpler, robust choice is to find a $\\Delta_R$ by marching, or to use the bound derived from $f(0)$: let $\\Delta_R = -f(0) = 2\\pi\\lambda \\int_{0}^{\\Lambda} p \\tanh\\left(\\frac{p}{2T}\\right) dp$. With this choice, and using the fact that $\\tanh(x)/x$ is a decreasing function, we have:\n$$ f(\\Delta_R) = \\Delta_R - 2\\pi\\lambda \\int_{0}^{\\Lambda} p^2 \\frac{\\tanh(E_p(\\Delta_R)/2T)}{E_p(\\Delta_R)} dp > \\Delta_R - 2\\pi\\lambda \\int_{0}^{\\Lambda} p \\tanh\\left(\\frac{p}{2T}\\right) dp = \\Delta_R - \\Delta_R = 0 $$\nThus, the interval $[\\epsilon, -f(0)]$ provides a robust and mathematically justified bracket for the root $\\Delta^{\\star}$.\n\n**3. Hybrid Iteration Strategy**\nThe algorithm proceeds as follows for a given tolerance $\\tau$:\n1. Initialize the bracket $[\\Delta_L, \\Delta_R]$ as derived above. Let $\\Delta_k$ be the current best guess for the root.\n2. In each iteration, compute the Newton-Raphson step from $\\Delta_k$: $\\Delta_{NR} = \\Delta_k - f(\\Delta_k)/f'(\\Delta_k)$.\n3. **Acceptance Condition**: If the Newton step falls within the current bracket (i.e., $\\Delta_L  \\Delta_{NR}  \\Delta_R$), it is considered a safe and productive step. Set the next candidate root to be $\\Delta_{k+1} = \\Delta_{NR}$.\n4. **Fallback**: If the Newton step is outside the bracket, it is deemed unreliable. The algorithm must fall back to the bisection method, which guarantees convergence, albeit more slowly. Set the next candidate root to be the midpoint of the bracket: $\\Delta_{k+1} = (\\Delta_L + \\Delta_R)/2$.\n5. **Bracket Update**: Evaluate $f(\\Delta_{k+1})$. Based on its sign, update either $\\Delta_L$ or $\\Delta_R$ to $\\Delta_{k+1}$ to ensure the root remains bracketed in the new, smaller interval.\n6. **Termination**: Repeat the process until the width of the bracket, $\\Delta_R - \\Delta_L$, is less than the specified tolerance $\\tau$. The final approximation of the root is the midpoint of the final bracket.\n\n**4. Numerical Quadrature**\nThe integrals for $f(\\Delta)$ and $f'(\\Delta)$ do not have elementary analytical solutions and must be computed numerically. For this purpose, a robust adaptive quadrature method, such as the one provided by `scipy.integrate.quad`, is employed. The integrands are well-behaved for $\\Delta > 0$ and $p \\in [0, \\Lambda]$, ensuring the reliability of the numerical integration.\n\nThis combination of a fast-converging (Newton-Raphson) and a robust (bisection) method results in an efficient and reliable algorithm for finding the root $\\Delta^{\\star}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Solves the NJL gap equation for a series of test cases and prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lambda [GeV^-1], Lambda [GeV], T [GeV])\n        (0.10, 1.00, 0.05),\n        (0.20, 1.00, 0.01),\n        (0.10, 1.00, 0.20),\n        (0.05, 0.50, 0.02),\n    ]\n\n    results = []\n    for case in test_cases:\n        lam, L, T = case\n        # Find the root for the given parameters.\n        try:\n            delta_star = find_gap_root(lam, L, T)\n            results.append(delta_star)\n        except (ValueError, RuntimeError) as e:\n            # Handle cases where root finding might fail, for robustness.\n            print(f\"Error solving for case {case}: {e}\")\n            results.append(np.nan)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\ndef find_gap_root(lam, L, T):\n    \"\"\"\n    Finds the root of the gap equation using a hybrid Newton-bisection method.\n\n    Args:\n        lam (float): Coupling parameter in GeV^-1.\n        L (float): Ultraviolet momentum cutoff in GeV.\n        T (float): Temperature in GeV.\n\n    Returns:\n        float: The calculated gap Delta* in GeV, rounded to 6 decimal places.\n    \"\"\"\n    PI = np.pi\n    TOL = 1e-9      # Convergence tolerance\n    MAX_ITER = 100  # Maximum number of iterations\n    EPS = 1e-12     # Small positive number to avoid singularities at Delta=0\n\n    # --- Integrands for f(Delta) and f'(Delta) ---\n    def integrand_f(p, delta, lam_p, L_p, T_p):\n        E = np.sqrt(p**2 + delta**2)\n        # The tanh can be numerically unstable if E/T is huge, but np.tanh handles it.\n        return p**2 * np.tanh(E / (2 * T_p)) / E\n\n    def integrand_fprime(p, delta, lam_p, L_p, T_p):\n        if delta  EPS:\n            return np.inf  # Derivative diverges at Delta=0\n        E = np.sqrt(p**2 + delta**2)\n        x = E / (2 * T_p)\n        # Using np.cosh for sech to avoid potential overflow/underflow with np.exp\n        sech_x = 1.0 / np.cosh(x)\n        tanh_x = np.tanh(x)\n        term_in_brackets = x * sech_x**2 - tanh_x\n        return p**2 / E**3 * term_in_brackets\n    \n    # Integrand for calculating the initial upper bound Delta_R = -f(0)\n    def integrand_f0(p, lam_p, L_p, T_p):\n        return p * np.tanh(p / (2 * T_p))\n\n    # --- Functions f(Delta) and f'(Delta) using numerical quadrature ---\n    def f(delta):\n        integral_val, _ = quad(integrand_f, 0, L, args=(delta, lam, L, T))\n        return delta - 2 * PI * lam * integral_val\n\n    def fprime(delta):\n        integral_val, _ = quad(integrand_fprime, 0, L, args=(delta, lam, L, T))\n        return 1.0 - 2 * PI * lam * delta * integral_val\n\n    # --- Step 1: Establish a bracketing interval [Delta_L, Delta_R] ---\n    delta_L = EPS\n    f_L = f(delta_L)\n\n    # Calculate Delta_R = -f(0)\n    integral_f0_val, _ = quad(integrand_f0, 0, L, args=(lam, L, T))\n    f0 = -2 * PI * lam * integral_f0_val\n    delta_R = -f0\n    f_R = f(delta_R)\n\n    if not (f_L  0 and f_R > 0):\n        # This fallback should not be needed based on the derivation, but is good practice.\n        delta_R = PI * lam * L**2 # A safe but loose upper bound\n        f_R = f(delta_R)\n        if f_R  0:\n            raise RuntimeError(\"Failed to bracket the root. The model parameters may be unphysical.\")\n\n    # --- Step 2: Hybrid Newton-Bisection Iteration ---\n    delta_k = (delta_L + delta_R) / 2.0  # Initial guess\n    \n    for i in range(MAX_ITER):\n        f_k = f(delta_k)\n        \n        # Check for convergence on the function value\n        if abs(f_k)  TOL:\n            break\n        \n        # Fallback to bisection if bracket is small enough\n        if (delta_R - delta_L)  TOL:\n            delta_k = (delta_L + delta_R) / 2.0\n            break\n        \n        # Calculate Newton step\n        fp_k = fprime(delta_k)\n        if abs(fp_k) > 1e-15:\n            delta_NR = delta_k - f_k / fp_k\n        else:\n            delta_NR = delta_L - 1 # Force bisection by making the step invalid\n        \n        # Accept Newton step only if it's within the current bracket\n        if delta_L  delta_NR  delta_R:\n            delta_k = delta_NR\n        else:\n            # Fallback to bisection if Newton step is not safe\n            delta_k = (delta_L + delta_R) / 2.0\n        \n        # Update the bracket\n        f_k_new = f(delta_k)\n        if f_k_new  0:\n            delta_L = delta_k\n        else:\n            delta_R = delta_k\n\n    return round(delta_k, 6)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3532433"}]}