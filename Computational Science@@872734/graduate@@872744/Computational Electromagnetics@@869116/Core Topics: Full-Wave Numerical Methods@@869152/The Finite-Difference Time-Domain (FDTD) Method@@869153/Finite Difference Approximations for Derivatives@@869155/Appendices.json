{"hands_on_practices": [{"introduction": "The second-order central difference is a foundational tool in computational science for approximating derivatives. This exercise begins with its derivation from first principles using Taylor series expansions. You will then move from theory to practice by implementing a grid refinement study, a critical technique for verifying code and understanding its error characteristics. This practice will illuminate the trade-off between truncation error, which decreases with grid spacing $h$, and round-off error, which becomes dominant for very small $h$, providing essential skills for diagnosing numerical simulations. [@problem_id:3307315]", "problem": "Consider the role of spatial differentiation in Computational Electromagnetics, specifically in finite-difference discretizations of Maxwell's equations. In methods such as the Finite-Difference Time-Domain (FDTD) scheme, spatial derivatives are approximated on a grid by finite differences, and the accuracy and stability of the electromagnetic field update equations depend on how derivative operators are discretized and on the step size. To study these numerical properties in a controlled setting, focus on approximating the derivative of a smooth scalar function using a symmetric stencil.\n\nStarting from the Taylor expansion of a sufficiently smooth function $f$ about a point $x$, derive a symmetric, two-point stencil approximation to the derivative $f'(x)$ that arises from truncating the Taylor series at the lowest order necessary to eliminate odd-order terms in $h$ and achieves second-order consistency in $h$. Use that approximation in your program for the specific function $f(x) = \\cos x$ at the point $x = 1$, with angles measured in radians.\n\nDefine the absolute error for a step size $h$ as $E(h) = \\left|D(h) - f'(1)\\right|$, where $D(h)$ is your numerical approximation for $f'(1)$ obtained using the symmetric stencil and $f'(1)$ is the exact derivative at $x = 1$. Using a sequence of decreasing step sizes $(h_i)$, define the observed order between successive steps as\n$$\np_i = \\frac{\\log\\left(E(h_i)/E(h_{i+1})\\right)}{\\log\\left(h_i/h_{i+1}\\right)}.\n$$\nIn a grid refinement study, theoretical second-order behavior implies $p_i \\approx 2$ when truncation errors dominate. For sufficiently small $h$, subtractive cancellation in $f(x+h) - f(x-h)$ and floating-point round-off (characterized by machine precision) can cause deviations from this ideal behavior. Your solution must explain the origin of these deviations in terms of truncation error and round-off error.\n\nImplement a program that, for each of the following test sequences of $h$, computes:\n- The sequence of numerical approximations $D(h_i)$ for $f'(1)$,\n- The sequence of absolute errors $E(h_i)$,\n- The sequence of observed orders $(p_i)$ for successive pairs $(h_i, h_{i+1})$.\n\nAngles must be in radians. There are no other physical units in this problem. The program’s final output should aggregate the observed orders for all test sequences into a single line containing a list of lists, where each inner list corresponds to one test sequence and contains the $p_i$ values as floating-point numbers.\n\nTest suite of step-size sequences:\n- Case A (moderate refinement): $[10^{-1},\\, 5 \\cdot 10^{-2},\\, 2.5 \\cdot 10^{-2},\\, 1.25 \\cdot 10^{-2}]$.\n- Case B (coarse-to-moderate refinement): $[5 \\cdot 10^{-1},\\, 2.5 \\cdot 10^{-1},\\, 1.25 \\cdot 10^{-1},\\, 6.25 \\cdot 10^{-2}]$.\n- Case C (near the expected optimal $h$ balancing truncation and round-off): $[10^{-5},\\, 5 \\cdot 10^{-6},\\, 2.5 \\cdot 10^{-6},\\, 1.25 \\cdot 10^{-6}]$.\n- Case D (extremely small steps where round-off dominates): $[10^{-8},\\, 5 \\cdot 10^{-9},\\, 2.5 \\cdot 10^{-9},\\, 1.25 \\cdot 10^{-9}]$.\n- Case E (non-uniform ratios to test generality): $[10^{-3},\\, 7.5 \\cdot 10^{-4},\\, 3 \\cdot 10^{-4},\\, 10^{-4},\\, 7.5 \\cdot 10^{-5}]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the inner list of observed orders for the corresponding case (for example, $[[p_{A,1},p_{A,2},p_{A,3}],[p_{B,1},\\dots],\\dots]$).", "solution": "The problem requires the derivation of a second-order accurate finite difference approximation for the first derivative, an analysis of its numerical error properties, and an implementation to compute the observed order of convergence for a specific function and several sets of step sizes.\n\n**1. Derivation of the Symmetric Finite Difference Stencil**\n\nTo derive a symmetric, two-point stencil for the first derivative $f'(x)$, we begin with the Taylor series expansions for a sufficiently smooth function $f(x)$ around a point $x$ for the points $x+h$ and $x-h$, where $h > 0$ is the step size.\n\nThe Taylor expansion for $f(x+h)$ about $x$ is:\n$$\nf(x+h) = f(x) + hf'(x) + \\frac{h^2}{2!}f''(x) + \\frac{h^3}{3!}f'''(x) + \\frac{h^4}{4!}f^{(4)}(x) + \\dots\n$$\n\nThe Taylor expansion for $f(x-h)$ about $x$ is:\n$$\nf(x-h) = f(x) - hf'(x) + \\frac{h^2}{2!}f''(x) - \\frac{h^3}{3!}f'''(x) + \\frac{h^4}{4!}f^{(4)}(x) - \\dots\n$$\n\nTo isolate the first derivative term, $f'(x)$, we subtract the second expansion from the first. This conveniently cancels all terms with even powers of $h$:\n$$\nf(x+h) - f(x-h) = (f(x) - f(x)) + (h - (-h))f'(x) + \\left(\\frac{h^2}{2} - \\frac{h^2}{2}\\right)f''(x) + \\left(\\frac{h^3}{6} - \\left(-\\frac{h^3}{6}\\right)\\right)f'''(x) + \\dots\n$$\n$$\nf(x+h) - f(x-h) = 2hf'(x) + \\frac{2h^3}{6}f'''(x) + O(h^5)\n$$\nwhere $O(h^5)$ represents terms of order $h^5$ and higher.\n\nRearranging this equation to solve for $f'(x)$ gives:\n$$\nf'(x) = \\frac{f(x+h) - f(x-h)}{2h} - \\frac{h^2}{6}f'''(x) - O(h^4)\n$$\n\nFrom this, we define the numerical approximation $D(h)$ for $f'(x)$ by truncating the series:\n$$\nD(h) = \\frac{f(x+h) - f(x-h)}{2h}\n$$\nThis is the required symmetric, two-point stencil, commonly known as the central difference formula.\n\nThe error of this approximation, known as the truncation error $E_t(h)$, is the difference between the exact derivative and its approximation:\n$$\nE_t(h) = f'(x) - D(h) = -\\frac{h^2}{6}f'''(x) - O(h^4)\n$$\nSince the leading term of the error is proportional to $h^2$, the method is second-order accurate (or has second-order consistency).\n\n**2. Analysis of Numerical Error Sources**\n\nThe total absolute error $E(h) = |D(h) - f'(x)|$ in a floating-point computation is a combination of two primary sources: truncation error and round-off error.\n\n**Truncation Error**: As derived above, the truncation error results from approximating an infinite process (the Taylor series) with a finite one (the stencil). For small $h$, this error is dominated by its leading term:\n$$\nE_t(h) \\approx \\left|-\\frac{h^2}{6}f'''(x)\\right| = C_t h^2\n$$\nwhere $C_t = |f'''(x)|/6$. This error decreases quadratically as $h$ decreases. For the specified function $f(x) = \\cos x$, we have $f'(x) = -\\sin x$, $f''(x) = -\\cos x$, and $f'''(x) = \\sin x$. At $x=1$, the constant is $C_t = \\sin(1)/6 \\approx 0.140$.\n\n**Round-off Error**: Round-off error arises because digital computers represent real numbers with finite precision. When $h$ is very small, the values of $f(x+h)$ and $f(x-h)$ become very close. The subtraction $f(x+h) - f(x-h)$ then suffers from **subtractive cancellation**, leading to a loss of significant digits. Let $\\epsilon_{mach}$ be the machine epsilon (the upper bound on the relative error due to rounding in floating-point arithmetic). The error in computing the numerator $f(x+h) - f(x-h)$ is roughly proportional to $|f(x)|\\epsilon_{mach}$. This error is then magnified by division by the small number $2h$. The round-off error $E_r(h)$ can therefore be modeled as:\n$$\nE_r(h) \\approx \\frac{C_r}{h}\n$$\nwhere the constant $C_r$ is on the order of $|f(x)|\\epsilon_{mach}$. This error increases as $h$ decreases.\n\n**Total Error and Optimal Step Size**: The total error is the sum of these two components:\n$$\nE(h) \\approx C_t h^2 + \\frac{C_r}{h}\n$$\nFor large $h$, the $C_t h^2$ term dominates. For very small $h$, the $C_r/h$ term dominates. There exists an optimal step size $h_{opt}$ that minimizes this total error, found by setting the derivative of $E(h)$ with respect to $h$ to zero:\n$$\n\\frac{dE}{dh} = 2C_t h - \\frac{C_r}{h^2} = 0 \\implies h_{opt} = \\left(\\frac{C_r}{2C_t}\\right)^{1/3}\n$$\nFor double-precision floating-point arithmetic, $\\epsilon_{mach} \\approx 2.22 \\times 10^{-16}$. At $x=1$ for $f(x)=\\cos x$, $h_{opt}$ is on the order of $10^{-5}$ to $10^{-6}$.\n\n**3. Observed Order of Convergence**\n\nThe observed order of convergence, $p_i$, between two successive step sizes $h_i$ and $h_{i+1}$ is calculated as:\n$$\np_i = \\frac{\\log\\left(E(h_i)/E(h_{i+1})\\right)}{\\log\\left(h_i/h_{i+1}\\right)}\n$$\nThis formula is derived from the assumption that the error behaves as $E(h) \\approx C h^p$. If this holds, then $E(h_i)/E(h_{i+1}) \\approx (h_i/h_{i+1})^p$, and taking the logarithm of both sides allows solving for $p$.\n\nThe expected behavior of $p_i$ for the test cases is as follows:\n- **Cases A, B, and E**: The step sizes $h_i$ are in the regime where truncation error dominates ($E(h) \\propto h^2$). Therefore, we expect the observed order $p_i$ to be approximately $2$.\n- **Case C**: The step sizes are near $h_{opt}$. In this region, both truncation and round-off errors are significant. The error does not follow a simple power law, so $p_i$ will deviate from $2$, likely decreasing.\n- **Case D**: The step sizes are very small, deep in the round-off dominated regime. Error is expected to increase as $h$ decreases, roughly as $E(h) \\propto 1/h$. In this case, $E(h_i)/E(h_{i+1}) \\approx (1/h_i)/(1/h_{i+1}) = h_{i+1}/h_i$. The formula for $p_i$ yields $p_i \\approx \\log(h_{i+1}/h_i) / \\log(h_i/h_{i+1}) = -1$. Due to the stochastic nature of floating-point arithmetic, the observed values may be chaotic but should generally be negative or close to zero, indicating a breakdown of convergence.\nThe implementation will now compute these values according to the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies a central difference approximation for f'(x) to study\n    numerical error behavior. The function computes numerical derivatives, errors,\n    and observed orders of convergence for several sequences of step sizes.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (moderate refinement)\n        [1e-1, 5e-2, 2.5e-2, 1.25e-2],\n        # Case B (coarse-to-moderate refinement)\n        [5e-1, 2.5e-1, 1.25e-1, 6.25e-2],\n        # Case C (near the expected optimal h)\n        [1e-5, 5e-6, 2.5e-6, 1.25e-6],\n        # Case D (extremely small steps where round-off dominates)\n        [1e-8, 5e-9, 2.5e-9, 1.25e-9],\n        # Case E (non-uniform ratios to test generality)\n        [1e-3, 7.5e-4, 3e-4, 1e-4, 7.5e-5],\n    ]\n\n    # The point at which to evaluate the derivative\n    x = 1.0\n    \n    # The function f(x) = cos(x)\n    f = np.cos\n    \n    # The exact derivative f'(x) = -sin(x) at x=1\n    f_prime_exact = -np.sin(x)\n\n    all_p_results = []\n\n    for h_sequence in test_cases:\n        errors = []\n        for h in h_sequence:\n            # Symmetric, two-point stencil (central difference) approximation\n            # D(h) = (f(x+h) - f(x-h)) / (2*h)\n            d_approx = (f(x + h) - f(x - h)) / (2.0 * h)\n            \n            # Absolute error E(h) = |D(h) - f'(x)|\n            error = np.abs(d_approx - f_prime_exact)\n            errors.append(error)\n\n        p_values = []\n        # Calculate observed order p_i for successive pairs (h_i, h_{i+1})\n        for i in range(len(h_sequence) - 1):\n            h_i = h_sequence[i]\n            h_i_plus_1 = h_sequence[i+1]\n            \n            error_i = errors[i]\n            error_i_plus_1 = errors[i+1]\n            \n            # Formula for observed order: p_i = log(E_i/E_{i+1}) / log(h_i/h_{i+1})\n            # Handle cases where error might be zero to avoid log(0)\n            if error_i_plus_1 == 0.0 or error_i == 0.0:\n                # If the error becomes zero, the order is theoretically infinite.\n                # In practice with floating point, this suggests perfect cancellation or\n                # reaching the limits of precision.\n                p = np.inf\n            else:\n                log_error_ratio = np.log(error_i / error_i_plus_1)\n                log_h_ratio = np.log(h_i / h_i_plus_1)\n                p = log_error_ratio / log_h_ratio\n            \n            p_values.append(p)\n        \n        all_p_results.append(p_values)\n\n    # Format the final output string exactly as required: [[...],[...],...]\n    # without extraneous spaces.\n    outer_list_str = []\n    for p_list in all_p_results:\n        inner_list_str = \"[\" + \",\".join(map(str, p_list)) + \"]\"\n        outer_list_str.append(inner_list_str)\n    \n    final_output = \"[\" + \",\".join(outer_list_str) + \"]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3307315"}, {"introduction": "To improve accuracy beyond the standard central difference, one straightforward approach is to incorporate more information from neighboring grid points. This practice guides you through the construction of a fourth-order accurate approximation for the first derivative using a wider, five-point stencil. By systematically setting up and solving for coefficients that cancel lower-order error terms in the Taylor series, you will gain proficiency in designing higher-order explicit schemes, a key technique for reducing numerical errors in complex wave simulations. [@problem_id:3307345]", "problem": "In computational electromagnetics, spatial derivatives in the curl terms of Maxwell’s equations are often discretized on a uniform Cartesian grid using finite differences. To reduce numerical dispersion in the Finite-Difference Time-Domain (FDTD) method, consider approximating the derivative of a sufficiently smooth scalar field $f$ at a point $x$ using only the grid samples at $x \\pm h$ and $x \\pm 2h$, where $h > 0$ is the uniform grid spacing.\n\nStarting from the definition of the derivative and Taylor’s theorem with remainder, construct an antisymmetric linear combination of the samples at $x \\pm h$ and $x \\pm 2h$, divided by $h$, that approximates $f'(x)$. Determine the coefficients so that the approximation achieves fourth-order accuracy in $h$, meaning the truncation error scales as $O(h^{4})$. Then simplify the resulting stencil to a single analytic expression and verify, by explicit series expansion, that the leading truncation error term is proportional to $h^{4}$.\n\nProvide as your final answer the single closed-form analytic expression for the fourth-order central difference approximation to $f'(x)$ that uses only the points $x \\pm h$ and $x \\pm 2h$.", "solution": "The problem asks for the construction of a fourth-order accurate finite difference approximation for the first derivative, $f'(x)$, using a specific set of grid points. The process begins with validating the problem statement. The problem is found to be scientifically grounded, well-posed, and objective. It is a standard problem in numerical analysis, particularly in the context of deriving high-order finite difference stencils, which are crucial in methods like the Finite-Difference Time-Domain (FDTD) for solving Maxwell's equations. All necessary conditions are provided, and no contradictions exist. Therefore, the problem is valid, and we may proceed with the solution.\n\nLet the approximation of $f'(x)$ be denoted by $D_h f(x)$. The problem specifies that this approximation must be an antisymmetric linear combination of the function values sampled at $x \\pm h$ and $x \\pm 2h$, divided by the grid spacing $h$. This structure can be written as:\n$$ D_h f(x) = \\frac{c_1 [f(x+h) - f(x-h)] + c_2 [f(x+2h) - f(x-2h)]}{h} $$\nwhere $c_1$ and $c_2$ are the coefficients to be determined. The field $f$ is assumed to be sufficiently smooth, which allows for the use of Taylor series expansions around the point $x$.\n\nWe expand each function evaluation in a Taylor series about $x$:\n$$ f(x+kh) = f(x) + (kh)f'(x) + \\frac{(kh)^2}{2!}f''(x) + \\frac{(kh)^3}{3!}f'''(x) + \\frac{(kh)^4}{4!}f^{(4)}(x) + \\frac{(kh)^5}{5!}f^{(5)}(x) + O(h^6) $$\n$$ f(x-kh) = f(x) - (kh)f'(x) + \\frac{(kh)^2}{2!}f''(x) - \\frac{(kh)^3}{3!}f'''(x) + \\frac{(kh)^4}{4!}f^{(4)}(x) - \\frac{(kh)^5}{5!}f^{(5)}(x) + O(h^6) $$\nBy subtracting these two expansions, we obtain a general formula for the central difference:\n$$ f(x+kh) - f(x-kh) = 2(kh)f'(x) + \\frac{2(kh)^3}{3!}f'''(x) + \\frac{2(kh)^5}{5!}f^{(5)}(x) + O(h^7) $$\nFor the term involving $x \\pm h$, we set $k=1$:\n$$ f(x+h) - f(x-h) = 2hf'(x) + \\frac{2h^3}{6}f'''(x) + \\frac{2h^5}{120}f^{(5)}(x) + O(h^7) $$\n$$ f(x+h) - f(x-h) = 2hf'(x) + \\frac{h^3}{3}f'''(x) + \\frac{h^5}{60}f^{(5)}(x) + O(h^7) $$\nFor the term involving $x \\pm 2h$, we set $k=2$:\n$$ f(x+2h) - f(x-2h) = 2(2h)f'(x) + \\frac{2(2h)^3}{6}f'''(x) + \\frac{2(2h)^5}{120}f^{(5)}(x) + O(h^7) $$\n$$ f(x+2h) - f(x-2h) = 4hf'(x) + \\frac{16h^3}{6}f'''(x) + \\frac{64h^5}{120}f^{(5)}(x) + O(h^7) $$\n$$ f(x+2h) - f(x-2h) = 4hf'(x) + \\frac{8h^3}{3}f'''(x) + \\frac{8h^5}{15}f^{(5)}(x) + O(h^7) $$\nNow, we substitute these expressions back into the formula for $D_h f(x)$:\n$$ D_h f(x) = \\frac{c_1}{h} \\left( 2hf'(x) + \\frac{h^3}{3}f'''(x) + \\frac{h^5}{60}f^{(5)}(x) + \\dots \\right) + \\frac{c_2}{h} \\left( 4hf'(x) + \\frac{8h^3}{3}f'''(x) + \\frac{8h^5}{15}f^{(5)}(x) + \\dots \\right) $$\nDividing by $h$ and collecting terms based on the order of the derivative of $f(x)$:\n$$ D_h f(x) = (2c_1 + 4c_2)f'(x) + \\left(\\frac{c_1}{3} + \\frac{8c_2}{3}\\right)h^2 f'''(x) + \\left(\\frac{c_1}{60} + \\frac{8c_2}{15}\\right)h^4 f^{(5)}(x) + O(h^6) $$\nTo achieve fourth-order accuracy, we require that the approximation is exact for polynomials of degree up to $4$. This translates to the following conditions on the coefficients.\nFirst, for the approximation to be consistent with $f'(x)$, the coefficient of $f'(x)$ must be $1$:\n$$ 2c_1 + 4c_2 = 1 $$\nSecond, to achieve an accuracy higher than second-order, the coefficient of the $h^2 f'''(x)$ term must be zero, thereby eliminating the leading error term of the standard central difference:\n$$ \\frac{c_1}{3} + \\frac{8c_2}{3} = 0 \\implies c_1 + 8c_2 = 0 \\implies c_1 = -8c_2 $$\nWe now solve this system of two linear equations for $c_1$ and $c_2$. Substituting the second equation into the first gives:\n$$ 2(-8c_2) + 4c_2 = 1 $$\n$$ -16c_2 + 4c_2 = 1 $$\n$$ -12c_2 = 1 \\implies c_2 = -\\frac{1}{12} $$\nSubstituting this value back to find $c_1$:\n$$ c_1 = -8\\left(-\\frac{1}{12}\\right) = \\frac{8}{12} = \\frac{2}{3} $$\nThe coefficients are thus $c_1 = 2/3$ and $c_2 = -1/12$. We substitute these back into the original formula for $D_h f(x)$:\n$$ D_h f(x) = \\frac{\\frac{2}{3}[f(x+h) - f(x-h)] - \\frac{1}{12}[f(x+2h) - f(x-2h)]}{h} $$\nTo obtain a single analytic expression, we find a common denominator for the coefficients:\n$$ D_h f(x) = \\frac{1}{h} \\left( \\frac{8}{12}[f(x+h) - f(x-h)] - \\frac{1}{12}[f(x+2h) - f(x-2h)] \\right) $$\n$$ D_h f(x) = \\frac{1}{12h} \\left( 8(f(x+h) - f(x-h)) - (f(x+2h) - f(x-2h)) \\right) $$\n$$ D_h f(x) = \\frac{8f(x+h) - 8f(x-h) - f(x+2h) + f(x-2h)}{12h} $$\nRearranging the terms in the numerator based on their position relative to $x$:\n$$ D_h f(x) = \\frac{-f(x+2h) + 8f(x+h) - 8f(x-h) + f(x-2h)}{12h} $$\nThis is the desired fourth-order central difference approximation.\n\nFinally, we must verify that the leading term of the truncation error is proportional to $h^4$. The truncation error $T(h)$ is given by $T(h) = D_h f(x) - f'(x)$. From our expansion of $D_h f(x)$, with the $f'(x)$ and $f'''(x)$ terms correctly set, we have:\n$$ D_h f(x) = f'(x) + \\left(\\frac{c_1}{60} + \\frac{8c_2}{15}\\right)h^4 f^{(5)}(x) + O(h^6) $$\nThe leading error term is governed by the coefficient of $h^4 f^{(5)}(x)$. Substituting the values of $c_1$ and $c_2$:\n$$ \\text{Coefficient of } h^4 f^{(5)}(x) = \\frac{c_1}{60} + \\frac{8c_2}{15} = \\frac{c_1}{60} + \\frac{32c_2}{60} = \\frac{1}{60}(c_1 + 32c_2) $$\n$$ = \\frac{1}{60} \\left( \\frac{2}{3} + 32\\left(-\\frac{1}{12}\\right) \\right) = \\frac{1}{60} \\left( \\frac{2}{3} - \\frac{32}{12} \\right) = \\frac{1}{60} \\left( \\frac{2}{3} - \\frac{8}{3} \\right) = \\frac{1}{60} \\left( -\\frac{6}{3} \\right) = \\frac{-2}{60} = -\\frac{1}{30} $$\nSo, the approximation is:\n$$ D_h f(x) = f'(x) - \\frac{1}{30}h^4f^{(5)}(x) + O(h^6) $$\nThe truncation error is $T(h) = D_h f(x) - f'(x) = -\\frac{1}{30}h^4 f^{(5)}(x) + O(h^6)$. The leading term is indeed proportional to $h^4$, confirming the fourth-order accuracy of the derived stencil.\nThe final expression for the approximation is the closed form derived above.", "answer": "$$ \\boxed{\\frac{-f(x+2h) + 8f(x+h) - 8f(x-h) + f(x-2h)}{12h}} $$", "id": "3307345"}, {"introduction": "An alternative path to high accuracy involves implicit, or 'compact', finite difference schemes, which offer superior spectral properties on a narrow computational stencil. This exercise challenges you to derive and implement a fourth-order compact scheme, where the unknown derivatives at neighboring points are related through a linear system. You will explore how periodic boundary conditions give rise to a cyclic tridiagonal system and consider efficient solution strategies, gaining insight into a powerful class of methods used in advanced computational electromagnetics. [@problem_id:3307323]", "problem": "Consider the task of computing the spatial derivative $\\partial_x E_y$ on a one-dimensional periodic grid, as arises in the discretization of the curl operators in Maxwell's equations for Computational Electromagnetics. Work on a uniform, periodic grid on the interval $[0,2\\pi)$ with $N$ points, grid spacing $\\Delta x = 2\\pi/N$, and node locations $x_i = i \\,\\Delta x$ for $i = 0,1,\\dots,N-1$. Angles must be treated in radians. Starting from the definition of the derivative and Taylor series expansions about a grid point, derive a three-point, fourth-order compact finite difference scheme for $\\partial_x E_y$ with the following characteristics:\n\n- The approximation at node $i$ uses a linear system that couples the discrete derivatives $\\{(\\partial_x E_y)_j\\}$ through a symmetric, nearest-neighbor stencil on the left-hand side, and uses a nearest-neighbor antisymmetric stencil in the function values $\\{E_y(x_j)\\}$ on the right-hand side.\n- The scheme must be fourth-order accurate on a smooth periodic function $E_y(x)$, in the sense that the local truncation error is $\\mathcal{O}(\\Delta x^4)$.\n- The resulting global linear system for the vector of discrete derivatives must be circulant-cyclic tridiagonal due to the periodic boundary conditions.\n\nFrom these requirements:\n\n1. Determine the unique coefficients (if any) that produce fourth-order accuracy for a three-point compact scheme consistent with the above constraints. Derive them by matching Taylor series coefficients using only the fundamental definition of the derivative and Taylor expansions for $E_y(x)$ and its derivatives at neighboring nodes.\n\n2. Explicitly write the resulting linear system that must be solved to obtain the vector of discrete derivatives $\\mathbf{d} \\in \\mathbb{R}^N$ where $d_i \\approx (\\partial_x E_y)(x_i)$, emphasizing the circulant-cyclic tridiagonal structure of the system matrix. Give a principled sketch of an efficient solver that exploits this structure without using dense linear algebra. Your outline must be based on fundamental linear algebra identities for low-rank updates to an invertible matrix.\n\n3. Implement the method as a program that:\n   - Constructs the periodic grid $x_i$ with the specified $N$ and uses $\\Delta x = 2\\pi/N$.\n   - For each test function $E_y(x)$ listed below, assembles the corresponding right-hand side and solves the cyclic tridiagonal linear system to obtain the compact fourth-order approximation to $\\partial_x E_y$.\n   - Compares the numerical result to the exact derivative at each grid point and returns the maximum absolute error over the grid for each test case.\n\nUse only pure mathematical units (no physical units are required), and angles must be in radians. The final output must aggregate the results of all test cases as a single line, a comma-separated list enclosed in square brackets, where each entry is the maximum absolute error for the corresponding test case, printed as a floating-point number. You must round each entry to $12$ significant digits in scientific notation.\n\nTest Suite (each case must be evaluated independently and in the given order):\n\n- Case $1$ (smooth, moderate wavenumber): $N = 64$, $E_y(x) = \\sin(3 x)$, exact derivative $\\partial_x E_y = 3 \\cos(3 x)$.\n- Case $2$ (constant field): $N = 32$, $E_y(x) = 5$, exact derivative $\\partial_x E_y = 0$.\n- Case $3$ (high-frequency near-Nyquist): $N = 128$, let $k = N/2 - 1 = 63$, $E_y(x) = \\sin(k x)$, exact derivative $\\partial_x E_y = k \\cos(k x)$.\n- Case $4$ (non-polynomial smooth function): $N = 100$, $E_y(x) = \\exp(\\sin x)$, exact derivative $\\partial_x E_y = \\cos x \\,\\exp(\\sin x)$.\n- Case $5$ (very small grid): $N = 4$, $E_y(x) = \\sin x$, exact derivative $\\partial_x E_y = \\cos x$.\n\nFinal Output Format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases (e.g., \"[v1,v2,v3,v4,v5]\"). Each value must be a floating-point number in scientific notation, rounded to $12$ significant digits.\n\nNo input should be read from the user, and no external files should be used. The implementation should be expressed in a general way that would work for any $N \\ge 4$ and any smooth periodic function $E_y(x)$ following the specified stencil, but it should compute and print results only for the test suite given above.", "solution": "The problem requires the derivation, analysis, and implementation of a three-point, fourth-order compact finite difference scheme for the first derivative on a one-dimensional periodic grid. The validation process confirms that the problem is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. It is a standard problem in numerical analysis for partial differential equations.\n\n**Part 1: Derivation of the Compact Scheme Coefficients**\n\nWe are tasked with finding a finite difference approximation for the derivative $\\partial_x E_y$ at grid points $x_i = i \\Delta x$ for $i = 0, 1, \\dots, N-1$ on a uniform periodic grid with spacing $\\Delta x = 2\\pi/N$. Let $d_i$ be the numerical approximation to $E_y'(x_i) \\equiv (\\partial_x E_y)(x_i)$, and let $E_{y,i}$ denote $E_y(x_i)$.\n\nThe problem specifies the form of the compact scheme. At each node $i$, the discrete derivatives $\\{d_j\\}$ are coupled through a symmetric, nearest-neighbor stencil, and the function values $\\{E_{y,j}\\}$ are coupled through a nearest-neighbor antisymmetric stencil. This structure can be expressed as:\n$$\n\\alpha d_{i-1} + \\beta d_i + \\alpha d_{i+1} = \\frac{1}{\\Delta x} \\left( c E_{y,i+1} - c E_{y,i-1} \\right)\n$$\nwhere $\\alpha$, $\\beta$, and $c$ are the unknown coefficients to be determined. The symmetry on the left-hand side (LHS) dictates that the coefficients for $d_{i-1}$ and $d_{i+1}$ are identical. The antisymmetry on the right-hand side (RHS) gives the form $E_{y,i+1} - E_{y,i-1}$. Without loss of generality, we can normalize the coefficient of $d_i$ to $\\beta=1$. The factor $1/\\Delta x$ on the RHS is conventional for first-derivative approximations. Let us also rescale the RHS coefficient to be $a/2$ for algebraic convenience, matching the form of a standard centered difference. The scheme becomes:\n$$\n\\alpha d_{i-1} + d_i + \\alpha d_{i+1} = \\frac{a}{2\\Delta x} (E_{y,i+1} - E_{y,i-1})\n$$\n\nTo find the coefficients $\\alpha$ and $a$ that yield fourth-order accuracy, we replace all terms with their Taylor series expansions around the point $x_i$. We denote the exact derivatives of $E_y(x)$ at $x_i$ as $E_y'$, $E_y''$, etc. The numerical derivatives $d_j$ are assumed to be equal to the exact derivatives $E_y'(x_j)$ for this error analysis.\n\nThe Taylor series for $E_y(x)$ at neighboring points $x_{i\\pm1} = x_i \\pm \\Delta x$ are:\n$$\nE_{y,i\\pm1} = E_y \\pm \\Delta x E_y' + \\frac{(\\Delta x)^2}{2!} E_y'' \\pm \\frac{(\\Delta x)^3}{3!} E_y''' + \\frac{(\\Delta x)^4}{4!} E_y^{(4)} \\pm \\frac{(\\Delta x)^5}{5!} E_y^{(5)} + \\mathcal{O}((\\Delta x)^6)\n$$\nThe Taylor series for the derivative $E_y'(x)$ at neighboring points are:\n$$\nd_{i\\pm1} \\approx E_y'(x_{i\\pm1}) = E_y' \\pm \\Delta x E_y'' + \\frac{(\\Delta x)^2}{2!} E_y''' \\pm \\frac{(\\Delta x)^3}{3!} E_y^{(4)} + \\frac{(\\Delta x)^4}{4!} E_y^{(5)} + \\mathcal{O}((\\Delta x)^5)\n$$\n\nFirst, we expand the RHS of the scheme:\n$$\n\\text{RHS} = \\frac{a}{2\\Delta x} (E_{y,i+1} - E_{y,i-1}) = \\frac{a}{2\\Delta x} \\left( 2\\Delta x E_y' + 2\\frac{(\\Delta x)^3}{6} E_y''' + 2\\frac{(\\Delta x)^5}{120} E_y^{(5)} + \\mathcal{O}((\\Delta x)^7) \\right)\n$$\n$$\n\\text{RHS} = a E_y' + \\frac{a(\\Delta x)^2}{6} E_y''' + \\frac{a(\\Delta x)^4}{120} E_y^{(5)} + \\mathcal{O}((\\Delta x)^6)\n$$\n\nNext, we expand the LHS of the scheme:\n$$\n\\text{LHS} = \\alpha (d_{i-1} + d_{i+1}) + d_i \\approx \\alpha (E_y'(x_{i-1}) + E_y'(x_{i+1})) + E_y'(x_i)\n$$\n$$\n\\text{LHS} \\approx \\alpha \\left( 2E_y' + 2\\frac{(\\Delta x)^2}{2} E_y''' + 2\\frac{(\\Delta x)^4}{24} E_y^{(5)} + \\mathcal{O}((\\Delta x)^6) \\right) + E_y'\n$$\n$$\n\\text{LHS} \\approx (1+2\\alpha)E_y' + \\alpha(\\Delta x)^2 E_y''' + \\frac{\\alpha(\\Delta x)^4}{12} E_y^{(5)} + \\mathcal{O}((\\Delta x)^6)\n$$\n\nFor the scheme to be accurate, the expression LHS - RHS must be as close to zero as possible. We equate the coefficients of the derivatives of $E_y$ on both sides.\n\\begin{itemize}\n    \\item Coefficient of $E_y'$: $1+2\\alpha = a$\n    \\item Coefficient of $E_y''$: The odd derivatives of $E_y$ vanish due to symmetry, which is a key property of this centered scheme.\n    \\item Coefficient of $E_y'''$: To achieve an accuracy higher than second-order, the $(\\Delta x)^2$ terms must cancel.\n    $$ \\alpha(\\Delta x)^2 = \\frac{a(\\Delta x)^2}{6} \\implies \\alpha = \\frac{a}{6} $$\n\\end{itemize}\n\nWe now have a system of two linear equations for the two unknowns $\\alpha$ and $a$:\n1. $a = 1 + 2\\alpha$\n2. $a = 6\\alpha$\n\nSubstituting (2) into (1) gives $6\\alpha = 1 + 2\\alpha$, which yields $4\\alpha = 1$, so $\\alpha = 1/4$.\nSubstituting $\\alpha=1/4$ back into (2) gives $a = 6(1/4) = 3/2$.\n\nThe unique coefficients are $\\alpha = 1/4$ and $a = 3/2$. The scheme is:\n$$\n\\frac{1}{4} d_{i-1} + d_i + \\frac{1}{4} d_{i+1} = \\frac{3/2}{2\\Delta x} (E_{y,i+1} - E_{y,i-1}) = \\frac{3}{4\\Delta x} (E_{y,i+1} - E_{y,i-1})\n$$\n\nThe local truncation error (LTE) is determined by the first non-vanishing term in the expansion. We compare the coefficients of the $(\\Delta x)^4$ terms:\n$$\n\\text{LTE} \\propto \\left( \\frac{\\alpha(\\Delta x)^4}{12} - \\frac{a(\\Delta x)^4}{120} \\right) E_y^{(5)} = \\left( \\frac{1/4}{12} - \\frac{3/2}{120} \\right) (\\Delta x)^4 E_y^{(5)}\n$$\n$$\n= \\left( \\frac{1}{48} - \\frac{3}{240} \\right) (\\Delta x)^4 E_y^{(5)} = \\left( \\frac{5}{240} - \\frac{3}{240} \\right) (\\Delta x)^4 E_y^{(5)} = \\frac{2}{240} (\\Delta x)^4 E_y^{(5)} = \\frac{1}{120} (\\Delta x)^4 E_y^{(5)}\n$$\nThe error is proportional to $(\\Delta x)^4$, confirming the scheme is fourth-order accurate. The truncation error is formally defined as the residual when the exact solution is substituted into the normalized difference operator. After dividing by the normalization factor $1+2\\alpha = 3/2$, the error is $T_i = -\\frac{1}{180}(\\Delta x)^4 E_y^{(5)}(x_i) + \\mathcal{O}((\\Delta x)^6)$.\n\n**Part 2: The Linear System and Efficient Solver**\n\nThe scheme must be applied to all $N$ grid points, from $i=0$ to $i=N-1$. This generates a system of $N$ linear equations for the $N$ unknown derivatives $d_0, d_1, \\dots, d_{N-1}$. Let $\\mathbf{d} = [d_0, d_1, \\dots, d_{N-1}]^T$ be the vector of discrete derivatives. The system can be written in matrix form as $A \\mathbf{d} = \\mathbf{b}$, where $A$ is the coefficient matrix and $\\mathbf{b}$ is the right-hand side vector.\n\nThe equation for a generic interior point $i$ is:\n$$\n(\\frac{1}{4}) d_{i-1} + (1) d_i + (\\frac{1}{4}) d_{i+1} = b_i\n$$\nwhere $b_i = \\frac{3}{4\\Delta x} (E_{y,i+1} - E_{y,i-1})$.\n\nDue to the periodic boundary conditions, indices are taken modulo $N$. For $i=0$, $d_{i-1}=d_{-1}$ becomes $d_{N-1}$. For $i=N-1$, $d_{i+1}=d_N$ becomes $d_0$. This \"wraparound\" nature gives the matrix $A$ its characteristic structure:\n$$\nA = \\begin{pmatrix}\n1 & 1/4 & 0 & \\dots & 0 & 1/4 \\\\\n1/4 & 1 & 1/4 & \\dots & 0 & 0 \\\\\n0 & 1/4 & 1 & \\dots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\dots & 1 & 1/4 \\\\\n1/4 & 0 & 0 & \\dots & 1/4 & 1\n\\end{pmatrix}\n$$\nThis is a symmetric, tridiagonal, circulant-cyclic matrix. The right-hand side vector $\\mathbf{b}$ has components $b_i = \\frac{3}{4\\Delta x} (E_{y,(i+1)\\%N} - E_{y,(i-1+N)\\%N})$.\n\nAn efficient solver must exploit this circulant structure.\nA highly efficient method, with complexity $\\mathcal{O}(N \\log N)$, is based on the Discrete Fourier Transform (DFT), as circulant matrices are diagonalized by the DFT.\n1.  Let $\\mathbf{c} = [1, 1/4, 0, \\dots, 0, 1/4]$ be the first row of $A$. The convolution theorem states that for a circulant matrix $A$, a matrix-vector product $A\\mathbf{d}$ is equivalent to the circular convolution of $\\mathbf{c}$ and $\\mathbf{d}$.\n2.  Applying the DFT (denoted by $\\mathcal{F}$) to the system $A\\mathbf{d} = \\mathbf{b}$ gives $\\mathcal{F}(A\\mathbf{d}) = \\mathcal{F}(\\mathbf{b})$.\n3.  This becomes $\\mathcal{F}(\\mathbf{c}) \\odot \\mathcal{F}(\\mathbf{d}) = \\mathcal{F}(\\mathbf{b})$, where $\\odot$ denotes element-wise multiplication. The vector $\\mathbf{\\Lambda} = \\mathcal{F}(\\mathbf{c})$ contains the eigenvalues of $A$.\n4.  The system is solved in the frequency domain by element-wise division: $\\mathcal{F}(\\mathbf{d}) = \\mathcal{F}(\\mathbf{b}) / \\mathbf{\\Lambda}$.\n5.  The solution vector $\\mathbf{d}$ is recovered by applying the inverse DFT: $\\mathbf{d} = \\mathcal{F}^{-1}(\\mathcal{F}(\\mathbf{b}) / \\mathbf{\\Lambda})$. The use of Fast Fourier Transform (FFT) algorithms for the DFT and its inverse makes this method very fast. The eigenvalues of $A$ are given by $\\lambda_j = 1 + (1/4)e^{i 2\\pi j/N} + (1/4)e^{-i 2\\pi j/N} = 1 + (1/2)\\cos(2\\pi j/N)$ for $j=0, \\dots, N-1$. Since $|\\cos(\\theta)| \\le 1$, all eigenvalues are positive, ensuring the matrix is nonsingular.\n\nAnother efficient $\\mathcal{O}(N)$ method, hinted at by the prompt's mention of \"low-rank updates,\" is to use the Sherman-Morrison-Woodbury formula. The matrix $A$ can be written as a simple tridiagonal matrix $T$ plus a rank-$2$ correction $U V^T$ for the corner elements: $A = T + \\alpha(\\mathbf{e}_0 \\mathbf{e}_{N-1}^T + \\mathbf{e}_{N-1} \\mathbf{e}_0^T)$, where $\\alpha=1/4$. The solution to $A\\mathbf{d} = \\mathbf{b}$ can then be found by solving a few tridiagonal systems (using the $\\mathcal{O}(N)$ Thomas algorithm) and a small $2 \\times 2$ system.\n\n**Part 3: Implementation Strategy**\n\nThe implementation will follow the FFT-based approach, which is conveniently provided by `scipy.linalg.solve_circulant`.\nFor each test case specified:\n1.  Set the grid size $N$ and define the function $E_y(x)$ and its analytical derivative.\n2.  Construct the grid $x_i = i (2\\pi/N)$ for $i=0, \\dots, N-1$.\n3.  Evaluate the function on the grid to create the vector $\\mathbf{E}_y$.\n4.  Construct the right-hand side vector $\\mathbf{b}$ using the formula $b_i = \\frac{3}{4\\Delta x} (E_{y,i+1} - E_{y,i-1})$. Periodic indexing is handled efficiently using `numpy.roll`.\n5.  Define the first row of the circulant matrix $A$: $\\mathbf{c} = [1, 1/4, 0, \\dots, 0, 1/4]$.\n6.  Solve the system $A\\mathbf{d} = \\mathbf{b}$ for $\\mathbf{d}$ using `scipy.linalg.solve_circulant(c, b)`.\n7.  Evaluate the exact derivative on the grid to get the vector $\\mathbf{d}_{\\text{exact}}$.\n8.  Calculate the maximum absolute error over the grid: $\\max(|\\mathbf{d} - \\mathbf{d}_{\\text{exact}}|)$.\n9.  The errors for all test cases are collected and formatted into the required output string.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_circulant\n\ndef solve():\n    \"\"\"\n    Derives and implements a fourth-order compact finite difference scheme\n    to compute the first derivative of periodic functions.\n    \"\"\"\n\n    def compact_fourth_order_derivative(Ey_values, N):\n        \"\"\"\n        Computes the derivative using the 4th-order compact scheme.\n\n        Args:\n            Ey_values (np.ndarray): The function values E_y(x_i) on the grid.\n            N (int): The number of grid points.\n\n        Returns:\n            np.ndarray: The numerical derivative d_i at each grid point.\n        \"\"\"\n        # Grid spacing\n        delta_x = 2.0 * np.pi / N\n\n        # Coefficients of the scheme derived from Taylor series analysis.\n        # Scheme: (1/4)d_{i-1} + d_i + (1/4)d_{i+1} = (3/2)/(2*delta_x) * (E_{y,i+1} - E_{y,i-1})\n        alpha = 1.0 / 4.0\n        a = 3.0 / 2.0\n\n        # Construct the right-hand side (RHS) vector b.\n        # np.roll handles the periodic boundary conditions efficiently.\n        Ey_plus_1 = np.roll(Ey_values, -1)\n        Ey_minus_1 = np.roll(Ey_values, 1)\n        \n        rhs_b = (a / (2.0 * delta_x)) * (Ey_plus_1 - Ey_minus_1)\n\n        # Construct the first row of the circulant-cyclic tridiagonal matrix A.\n        # A has 1 on the main diagonal, and alpha on the sub- and super-diagonals,\n        # with cyclic wrap-around.\n        circulant_first_row = np.zeros(N)\n        circulant_first_row[0] = 1.0\n        circulant_first_row[1] = alpha\n        circulant_first_row[N - 1] = alpha  # or circulant_first_row[-1]\n\n        # Solve the linear system A*d = b using the efficient circulant solver,\n        # which is based on FFTs.\n        d_numerical = solve_circulant(circulant_first_row, rhs_b)\n\n        return d_numerical\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"N\": 64,\n            \"Ey_func\": lambda x: np.sin(3 * x),\n            \"dEy_exact_func\": lambda x: 3 * np.cos(3 * x),\n        },\n        {\n            \"N\": 32,\n            \"Ey_func\": lambda x: np.full_like(x, 5.0),\n            \"dEy_exact_func\": lambda x: np.zeros_like(x),\n        },\n        {\n            \"N\": 128,\n            \"k\": 63, # k = N/2 - 1\n            \"Ey_func\": lambda x, k=63: np.sin(k * x),\n            \"dEy_exact_func\": lambda x, k=63: k * np.cos(k * x),\n        },\n        {\n            \"N\": 100,\n            \"Ey_func\": lambda x: np.exp(np.sin(x)),\n            \"dEy_exact_func\": lambda x: np.cos(x) * np.exp(np.sin(x)),\n        },\n        {\n            \"N\": 4,\n            \"Ey_func\": lambda x: np.sin(x),\n            \"dEy_exact_func\": lambda x: np.cos(x),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        \n        # Create the periodic grid\n        x_grid = np.arange(N) * (2.0 * np.pi / N)\n\n        # Evaluate the function and its exact derivative on the grid\n        Ey_values = case[\"Ey_func\"](x_grid)\n        dEy_exact = case[\"dEy_exact_func\"](x_grid)\n        \n        # Compute the numerical derivative\n        dEy_numerical = compact_fourth_order_derivative(Ey_values, N)\n        \n        # Calculate the maximum absolute error\n        max_abs_error = np.max(np.abs(dEy_numerical - dEy_exact))\n        \n        results.append(max_abs_error)\n\n    # Format output as a comma-separated list of values in scientific notation\n    # with 12 significant digits.\n    formatted_results = [f\"{res:.11e}\" for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "3307323"}]}