{"hands_on_practices": [{"introduction": "The efficiency of explicit DGTD methods hinges on the structure of the element mass matrix, which arises from the time-derivative term in the weak formulation. This exercise guides you through the construction of an orthonormal polynomial basis on a tetrahedral element, a crucial step for achieving a diagonal mass matrix. By working through the details of the basis definition and its mapping to a physical element, you will gain hands-on experience with the foundational mathematics that makes DGTD computationally attractive [@problem_id:3300621].", "problem": "Consider the semi-discrete formulation of Maxwell’s equations in a Discontinuous Galerkin (DG) method, specifically the Discontinuous Galerkin Time-Domain (DGTD) scheme on a single tetrahedral element. The semi-discrete system for the electric field involves a local mass matrix defined by the volume inner product of basis functions. Let the reference tetrahedron be $\\widehat{T} = \\{(r,s,t) \\in \\mathbb{R}^{3} : r \\ge 0,\\ s \\ge 0,\\ t \\ge 0,\\ r+s+t \\le 1\\}$, and let $K$ be a physical tetrahedral element obtained from $\\widehat{T}$ by an affine map.\n\nYour tasks are as follows:\n\n1) Define a modal polynomial basis $\\{\\widehat{\\psi}_{\\ell m n}\\}_{\\ell+m+n \\le p}$ on $\\widehat{T}$ up to total degree $p$, that is orthonormal with respect to the $L^{2}$ inner product on $\\widehat{T}$. Use collapsed coordinates and Jacobi polynomials to express a general basis element $\\widehat{\\psi}_{\\ell m n}(r,s,t)$, and include normalization through a constant $N_{\\ell m n}$ so that\n$$\n\\int_{\\widehat{T}} \\widehat{\\psi}_{\\ell m n}(r,s,t)\\,\\widehat{\\psi}_{\\ell' m' n'}(r,s,t)\\,\\mathrm{d}V = \\delta_{\\ell\\ell'}\\,\\delta_{mm'}\\,\\delta_{n n'}.\n$$\nGive the symbolic form of $\\widehat{\\psi}_{\\ell m n}$ in terms of Jacobi polynomials and explicitly state how the normalization constant $N_{\\ell m n}$ is chosen to enforce orthonormality.\n\n2) Let the physical tetrahedron $K$ have vertices $(0,0,0)$, $(2,0,0)$, $(0,3,0)$, $(0,0,4)$. Consider the affine map $\\Phi : \\widehat{T} \\to K$ given by $(x,y,z) = (2r,\\,3s,\\,4t)$, and define the pulled-back modal basis on $K$ by $\\psi_{\\ell m n}(x,y,z) = \\widehat{\\psi}_{\\ell m n}(r,s,t)$ with $(r,s,t) = \\Phi^{-1}(x,y,z)$. For the volume mass matrix\n$$\nM_{\\alpha\\beta} = \\int_{K} \\psi_{\\alpha}(x,y,z)\\,\\psi_{\\beta}(x,y,z)\\,\\mathrm{d}V,\n$$\ncompute representative entries $M_{(0,0,0),(0,0,0)}$ and $M_{(1,0,0),(0,0,0)}$ under exact integration.\n\n3) State precise conditions on a tetrahedral volume quadrature rule under which the discrete mass matrix computed by quadrature remains diagonal when using the orthonormal modal basis up to degree $p$, assuming an affine mapping. Express your conditions in terms of the required polynomial exactness degree of the quadrature relative to $p$, and justify why these conditions guarantee diagonalization.\n\n4) Provide the value of $M_{(0,0,0),(0,0,0)}$ for the physical element $K$ as a single real number. No rounding is required, and no units are necessary.", "solution": "The user has provided a multi-part problem concerning the Discontinuous Galerkin Time-Domain (DGTD) method on a tetrahedral element. The problem is scientifically grounded, well-posed, and objective. It is a standard exercise in the field of computational electromagnetics and numerical methods for partial differential equations. The problem is valid.\n\n**1) Orthonormal Modal Basis on the Reference Tetrahedron**\n\nThe first task is to define an orthonormal polynomial basis on the reference tetrahedron $\\widehat{T} = \\{(r,s,t) \\in \\mathbb{R}^{3} : r \\ge 0,\\ s \\ge 0,\\ t \\ge 0,\\ r+s+t \\le 1\\}$. A standard approach is to construct an orthogonal basis using collapsed coordinates and Jacobi polynomials, and then normalize it.\n\nWe define a set of collapsed coordinates $(\\xi_1, \\xi_2, \\xi_3) \\in [-1,1]^3$ from the Cartesian coordinates $(r,s,t) \\in \\widehat{T}$:\n$$\n\\xi_1 = 2\\frac{r}{1-s-t}-1, \\quad \\xi_2 = 2\\frac{s}{1-t}-1, \\quad \\xi_3 = 2t-1.\n$$\nAn orthogonal (but not yet orthonormal) modal basis $\\{\\widehat{\\phi}_{\\ell m n}\\}_{\\ell+m+n \\le p}$ on $\\widehat{T}$ for total polynomial degree up to $p$ is constructed as a product of Jacobi polynomials $P_k^{(\\alpha,\\beta)}$ with these coordinates:\n$$\n\\widehat{\\phi}_{\\ell m n}(r,s,t) = P_{\\ell}^{(0,0)}(\\xi_1) (1-s-t)^{\\ell} \\cdot P_{m}^{(2\\ell+1,0)}(\\xi_2) (1-t)^{m} \\cdot P_{n}^{(2\\ell+2m+2,0)}(\\xi_3).\n$$\nThe factors involving $(1-s-t)$ and $(1-t)$ are introduced to make the three components of the product orthogonal with respect to the standard volume measure on $\\widehat{T}$.\n\nTo find the normalization constant, we compute the squared $L^2$-norm of these basis functions, $h_{\\ell m n}^2 = \\int_{\\widehat{T}} |\\widehat{\\phi}_{\\ell m n}(r,s,t)|^2\\,\\mathrm{d}V$. This integral can be evaluated by a sequence of 1D integrations using the change of variables to $(\\xi_1, \\xi_2, \\xi_3)$. The volume element transforms as $\\mathrm{d}V = \\mathrm{d}r\\,\\mathrm{d}s\\,\\mathrm{d}t = \\frac{(1-t)^2(1-s)}{8}\\,\\mathrm{d}\\xi_1\\,\\mathrm{d}\\xi_2\\,\\mathrm{d}\\xi_3$. The integral separates due to the structure of the basis functions and the properties of Jacobi polynomials. The squared norm is found to be:\n$$\nh_{\\ell m n}^2 = \\frac{1}{(2\\ell+1) \\cdot 2(m+\\ell+1) \\cdot (2n+2\\ell+2m+3)}.\n$$\nThe orthonormal basis $\\{\\widehat{\\psi}_{\\ell m n}\\}$ is then obtained by normalizing $\\{\\widehat{\\phi}_{\\ell m n}\\}$:\n$$\n\\widehat{\\psi}_{\\ell m n} = \\frac{1}{h_{\\ell m n}} \\widehat{\\phi}_{\\ell m n} = N_{\\ell m n} \\widehat{\\phi}_{\\ell m n}.\n$$\nThe normalization constant $N_{\\ell m n}$ is therefore the reciprocal of $h_{\\ell m n}$:\n$$\nN_{\\ell m n} = \\sqrt{(2\\ell+1) \\cdot 2(m+\\ell+1) \\cdot (2n+2\\ell+2m+3)}.\n$$\nThe final expression for the orthonormal basis function is:\n$$\n\\widehat{\\psi}_{\\ell m n}(r,s,t) = N_{\\ell m n} \\cdot P_{\\ell}^{(0,0)}\\left(2\\frac{r}{1-s-t}-1\\right) (1-s-t)^{\\ell} \\cdot P_{m}^{(2\\ell+1,0)}\\left(2\\frac{s}{1-t}-1\\right) (1-t)^{m} \\cdot P_{n}^{(2\\ell+2m+2,0)}(2t-1).\n$$\nThis basis satisfies the required condition $\\int_{\\widehat{T}} \\widehat{\\psi}_{\\ell m n}\\,\\widehat{\\psi}_{\\ell' m' n'}\\,\\mathrm{d}V = \\delta_{\\ell\\ell'}\\,\\delta_{mm'}\\,\\delta_{nn'}$.\n\n**2) Mass Matrix Entries for a Physical Element**\n\nThe mass matrix on the physical element $K$ is given by $M_{\\alpha\\beta} = \\int_{K} \\psi_{\\alpha}\\psi_{\\beta}\\,\\mathrm{d}V$, where $\\alpha=(\\ell,m,n)$ and $\\beta=(\\ell',m',n')$. The basis functions on $K$ are defined by pulling back the reference basis, $\\psi_{\\alpha}(x,y,z) = \\widehat{\\psi}_{\\alpha}(\\Phi^{-1}(x,y,z))$.\n\nThe affine map is $\\Phi: (r,s,t) \\mapsto (x,y,z) = (2r, 3s, 4t)$. The Jacobian matrix of this transformation is constant:\n$$\nJ = \\begin{pmatrix} \\frac{\\partial x}{\\partial r} & \\frac{\\partial x}{\\partial s} & \\frac{\\partial x}{\\partial t} \\\\ \\frac{\\partial y}{\\partial r} & \\frac{\\partial y}{\\partial s} & \\frac{\\partial y}{\\partial t} \\\\ \\frac{\\partial z}{\\partial r} & \\frac{\\partial z}{\\partial s} & \\frac{\\partial z}{\\partial t} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix}.\n$$\nThe determinant of the Jacobian is $|J| = 2 \\cdot 3 \\cdot 4 = 24$.\nUsing the change of variables formula for integration, the mass matrix entry is:\n$$\nM_{\\alpha\\beta} = \\int_{K} \\psi_{\\alpha}(\\mathbf{x})\\psi_{\\beta}(\\mathbf{x})\\,\\mathrm{d}\\mathbf{x} = \\int_{\\widehat{T}} \\widehat{\\psi}_{\\alpha}(\\mathbf{r})\\widehat{\\psi}_{\\beta}(\\mathbf{r})\\,|J|\\,\\mathrm{d}\\mathbf{r} = |J| \\int_{\\widehat{T}} \\widehat{\\psi}_{\\alpha}(\\mathbf{r})\\widehat{\\psi}_{\\beta}(\\mathbf{r})\\,\\mathrm{d}\\mathbf{r}.\n$$\nDue to the orthonormality of the basis $\\{\\widehat{\\psi}_{\\alpha}\\}$ on $\\widehat{T}$, the integral is simply the Kronecker delta $\\delta_{\\alpha\\beta}$:\n$$\nM_{\\alpha\\beta} = |J|\\,\\delta_{\\alpha\\beta} = 24\\,\\delta_{\\ell\\ell'}\\,\\delta_{mm'}\\,\\delta_{nn'}.\n$$\nThis shows that for an affine mapping, the mass matrix is diagonal, with each diagonal entry equal to the constant Jacobian determinant $|J|$.\n\nWe can now compute the requested entries:\n- For $\\alpha = \\beta = (0,0,0)$:\n$$M_{(0,0,0),(0,0,0)} = 24\\,\\delta_{00}\\,\\delta_{00}\\,\\delta_{00} = 24 \\cdot 1 \\cdot 1 \\cdot 1 = 24.$$\n- For $\\alpha = (1,0,0)$ and $\\beta = (0,0,0)$:\n$$M_{(1,0,0),(0,0,0)} = 24\\,\\delta_{10}\\,\\delta_{00}\\,\\delta_{00} = 24 \\cdot 0 \\cdot 1 \\cdot 1 = 0.$$\n\n**3) Condition for Diagonal Quadrature-Based Mass Matrix**\n\nThe discrete mass matrix $M^h$ is assembled using a numerical quadrature rule on the physical element $K$. Its entries are $M^h_{\\alpha\\beta} = \\sum_{q=1}^{N_q} w_q \\psi_{\\alpha}(x_q)\\psi_{\\beta}(x_q)$, where $(x_q, w_q)$ are the quadrature points and weights on $K$.\n\nTo analyze this, we transform the quadrature to the reference element $\\widehat{T}$. Let $(\\widehat{x}_q, \\widehat{w}_q)$ be the quadrature rule on $\\widehat{T}$. For an affine map $\\Phi$, the corresponding rule on $K$ is $x_q = \\Phi(\\widehat{x}_q)$ and $w_q = |J|\\widehat{w}_q$. The discrete mass matrix entry becomes:\n$$\nM^h_{\\alpha\\beta} = \\sum_{q=1}^{N_q} (|J|\\widehat{w}_q) \\widehat{\\psi}_{\\alpha}(\\widehat{x}_q)\\widehat{\\psi}_{\\beta}(\\widehat{x}_q) = |J| \\sum_{q=1}^{N_q} \\widehat{w}_q \\widehat{\\psi}_{\\alpha}(\\widehat{x}_q)\\widehat{\\psi}_{\\beta}(\\widehat{x}_q).\n$$\nFor this matrix to be diagonal, we require $M^h_{\\alpha\\beta} = C \\delta_{\\alpha\\beta}$ for some constant $C$. Comparing with the exact mass matrix $M_{\\alpha\\beta} = |J|\\delta_{\\alpha\\beta}$, we see that we must have:\n$$\n\\sum_{q=1}^{N_q} \\widehat{w}_q \\widehat{\\psi}_{\\alpha}(\\widehat{x}_q)\\widehat{\\psi}_{\\beta}(\\widehat{x}_q) = \\int_{\\widehat{T}} \\widehat{\\psi}_{\\alpha}(\\mathbf{r})\\widehat{\\psi}_{\\beta}(\\mathbf{r})\\,\\mathrm{d}\\mathbf{r} = \\delta_{\\alpha\\beta}.\n$$\nThis means the quadrature rule on the reference element must exactly integrate the product of any two basis functions $\\widehat{\\psi}_{\\alpha}$ and $\\widehat{\\psi}_{\\beta}$ up to degree $p$.\n\nThe basis function $\\widehat{\\psi}_{\\alpha} = \\widehat{\\psi}_{\\ell m n}$ is a polynomial of total degree $\\ell+m+n$. Since $\\ell+m+n \\le p$, the degree of $\\widehat{\\psi}_{\\alpha}$ is at most $p$. The product $\\widehat{\\psi}_{\\alpha}\\widehat{\\psi}_{\\beta}$ is therefore a polynomial of degree at most $p+p=2p$.\n\nThus, the condition for the quadrature to preserve the diagonality of the mass matrix is that the quadrature rule on the reference tetrahedron must be exact for all polynomials of total degree up to $2p$. For an affine map, the constant Jacobian determinant does not increase the required polynomial degree.\n\n**4) Value of $M_{(0,0,0),(0,0,0)}$**\n\nAs computed in part 2, the value for this mass matrix entry is:\n$$\nM_{(0,0,0),(0,0,0)} = 24.\n$$\nThis can also be verified directly. The basis function $\\widehat{\\psi}_{000}$ is a constant. From Part 1, $N_{000} = \\sqrt{(1) \\cdot 2(1) \\cdot (3)} = \\sqrt{6}$. Since $P_0=1$, $\\widehat{\\phi}_{000}=1$, so $\\widehat{\\psi}_{000} = \\sqrt{6}$. Thus, $\\psi_{000}(x,y,z) = \\sqrt{6}$ on $K$. The entry is:\n$$M_{(0,0,0),(0,0,0)} = \\int_K (\\sqrt{6})^2\\,\\mathrm{d}V = 6 \\int_K \\mathrm{d}V = 6 \\cdot \\text{Volume}(K).$$\nThe volume of the tetrahedron $K$ with vertices $(0,0,0)$, $(2,0,0)$, $(0,3,0)$, $(0,0,4)$ is\n$$\\text{Volume}(K) = \\frac{1}{6} \\left| \\det \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} \\right| = \\frac{24}{6} = 4.$$\nTherefore, $M_{(0,0,0),(0,0,0)} = 6 \\cdot 4 = 24$.", "answer": "$$\\boxed{24}$$", "id": "3300621"}, {"introduction": "After exploring the construction of orthonormal bases, it is vital to understand their practical impact on the overall algorithm. This practice problem contrasts the computational cost and structure of explicit time-stepping schemes when using diagonal versus full mass matrices. Analyzing these scenarios will clarify why the pursuit of diagonal mass matrices is a central theme in the design of efficient DGTD methods, enabling fast, element-local updates in an explicit time-stepping context [@problem_id:3300605].", "problem": "In Discontinuous Galerkin time-domain (DGTD) discretizations of source-free Maxwell curl equations in a lossless, homogeneous dielectric, one starts from the continuous equations\n$$\n\\frac{\\partial \\mathbf{D}}{\\partial t} = \\nabla \\times \\mathbf{H}, \\qquad \\frac{\\partial \\mathbf{B}}{\\partial t} = - \\nabla \\times \\mathbf{E},\n$$\nwith constitutive relations $\\mathbf{D} = \\varepsilon \\mathbf{E}$ and $\\mathbf{B} = \\mu \\mathbf{H}$, where $\\varepsilon > 0$ and $\\mu > 0$. On a mesh of non-overlapping elements $K_e$, the discontinuous Galerkin weak form uses an elementwise test space and numerical fluxes on interfaces. After choosing a polynomial basis $\\{\\phi_i\\}_{i=1}^{N_p}$ on each element $K_e$ to expand the field components and integrating by parts elementwise, the semi-discrete system on each element takes the form\n$$\n\\mathbf{M}_e \\frac{d \\mathbf{U}_e}{d t} = \\mathbf{R}_e(\\{\\mathbf{U}_{e'}\\}),\n$$\nwhere $\\mathbf{U}_e \\in \\mathbb{R}^{N_{\\text{fld}} N_p}$ collects the coefficients of the field components on $K_e$ for $N_{\\text{fld}}$ field components, $\\mathbf{M}_e$ is the element mass matrix defined by the $L^2$ inner product of basis functions over $K_e$, and $\\mathbf{R}_e$ includes both volume differentiation and numerical flux surface terms involving neighboring element traces $\\{\\mathbf{U}_{e'}\\}$. An explicit Runge–Kutta time integrator updates via repeated evaluations of $\\mathbf{M}_e^{-1}\\mathbf{R}_e$ on each element.\n\nConsider two choices of elementwise bases: (i) a non-orthonormal nodal basis, leading to a full (dense) $\\mathbf{M}_e$, and (ii) an orthonormal modal basis with respect to the $L^2$ inner product on a reference element $\\hat{K}$, mapped to $K_e$ by an affine mapping with constant Jacobian determinant $J_e > 0$. Assume exact quadrature sufficient to integrate all polynomial products arising in $\\mathbf{M}_e$ and in the volume terms. Let $N_p$ denote the number of scalar basis functions per element for a given polynomial order $p$, and consider a uniform mesh with affine elements so that all $J_e$ are constants per element.\n\nSelect all statements that are correct regarding the effect of diagonal versus full element mass matrices on the efficiency and stability of explicit time stepping, and the role of orthonormal modal bases:\n\nA. With a strictly orthonormal modal basis on $\\hat{K}$ and exact quadrature, the element mass matrix on $K_e$ is diagonal up to the constant scaling by $J_e$, i.e., $\\mathbf{M}_e = J_e \\mathbf{I}$ for scalar fields (and block-diagonal with identical diagonal blocks for multiple field components). Consequently, each explicit stage requires only componentwise scalings by $J_e^{-1}$ to apply $\\mathbf{M}_e^{-1}$ locally.\n\nB. Using a full (dense) element mass matrix in an explicit DGTD scheme reduces the global Courant–Friedrichs–Lewy (CFL) time step restriction compared to a diagonal mass matrix because the dense inverse $\\mathbf{M}_e^{-1}$ damps high-frequency modal content and thereby enlarges the stability region effectively.\n\nC. For affine elements in $3$ dimensions with polynomial order $p$ and $N_p$ unknowns per scalar field per element, the per-stage cost to apply $\\mathbf{M}_e^{-1}$ scales as $\\mathcal{O}(N_p)$ for a diagonal mass (orthonormal modal basis) and as $\\mathcal{O}(N_p^2)$ when applying a precomputed dense inverse for a full mass matrix, neglecting memory hierarchy effects.\n\nD. Orthonormality of a modal basis with respect to the $L^2$ inner product can be achieved only on tensor-product hexahedra; on simplices, any reasonable polynomial basis produces an element mass matrix that is at least block-dense, so $\\mathbf{M}_e$ cannot be diagonalized by a fixed modal choice.\n\nE. The primary benefit of diagonal element mass matrices in DGTD is that they eliminate the need to evaluate numerical fluxes on element interfaces, making the update entirely local within each element at every explicit stage.\n\nChoose all that apply and justify your selection by reasoning from the elementwise weak form, the definition of the mass matrix, and the structure of explicit Runge–Kutta updates, without assuming any unstated algorithmic shortcuts.", "solution": "The problem statement describes a standard Discontinuous Galerkin Time-Domain (DGTD) method for Maxwell's equations and asks to evaluate several statements concerning the choice of basis functions and its effect on the element mass matrix, computational efficiency, and stability.\n\n### Problem Validation\n\nFirst, I will validate the problem statement itself.\n\n**Step 1: Extract Givens**\n\n-   **Governing Equations:** $\\frac{\\partial \\mathbf{D}}{\\partial t} = \\nabla \\times \\mathbf{H}$, $\\frac{\\partial \\mathbf{B}}{\\partial t} = - \\nabla \\times \\mathbf{E}$.\n-   **Medium:** Source-free, lossless, homogeneous dielectric with $\\mathbf{D} = \\varepsilon \\mathbf{E}$, $\\mathbf{B} = \\mu \\mathbf{H}$, $\\varepsilon > 0$, $\\mu > 0$.\n-   **Discretization:** A mesh of non-overlapping elements $K_e$. A polynomial basis $\\{\\phi_i\\}_{i=1}^{N_p}$ is used on each element. The weak form is elementwise with numerical fluxes on interfaces.\n-   **Semi-discrete form:** $\\mathbf{M}_e \\frac{d \\mathbf{U}_e}{d t} = \\mathbf{R}_e(\\{\\mathbf{U}_{e'}\\})$ on each element $K_e$.\n-   **Definitions:**\n    -   $\\mathbf{U}_e \\in \\mathbb{R}^{N_{\\text{fld}} N_p}$ are the field coefficients on element $K_e$.\n    -   $\\mathbf{M}_e$ is the element mass matrix from the $L^2$ inner product of basis functions over $K_e$.\n    -   $\\mathbf{R}_e$ includes volume and surface (numerical flux) terms.\n-   **Time Integration:** Explicit Runge-Kutta, requiring evaluations of $\\mathbf{M}_e^{-1}\\mathbf{R}_e$.\n-   **Basis Choices:**\n    -   (i) Non-orthonormal nodal basis leading to a full (dense) $\\mathbf{M}_e$.\n    -   (ii) Orthonormal modal basis on a reference element $\\hat{K}$.\n-   **Mapping:** Affine mapping from $\\hat{K}$ to $K_e$ with a constant Jacobian determinant $J_e > 0$.\n-   **Assumptions:** Exact quadrature for polynomial products in $\\mathbf{M}_e$ and volume terms. A uniform mesh with affine elements.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientific Grounding:** The problem is firmly based on the established principles of computational electromagnetics and numerical analysis, specifically the DGTD method for Maxwell's equations. The formulation is standard and correct.\n-   **Well-Posedness:** The question asks for an analysis of consequences stemming from different basis choices within a well-defined framework. The requested analysis is feasible and leads to a definite set of correct statements.\n-   **Objectivity:** The language is technical, precise, and free of subjective elements.\n\nThe problem statement does not exhibit any of the invalidity flaws. It is scientifically sound, well-posed, objective, complete, and asks a non-trivial question relevant to the field.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. I will proceed to derive the solution and evaluate the options.\n\n### Solution Derivation\n\nThe element mass matrix $\\mathbf{M}_e$ for a single scalar field has entries given by the $L^2$ inner product of the basis functions on the physical element $K_e$:\n$$\n(M_e)_{ij} = \\int_{K_e} \\phi_i(\\mathbf{x}) \\phi_j(\\mathbf{x}) \\, dV\n$$\nThe basis functions $\\phi_i$ on $K_e$ are defined through a mapping from a reference element $\\hat{K}$, where a reference basis $\\{\\hat{\\phi}_i\\}$ is defined. For an affine map $\\mathbf{x} = \\mathcal{F}_e(\\hat{\\mathbf{x}})$, the volume element transforms as $dV = J_e \\, d\\hat{V}$, where $J_e$ is the constant Jacobian determinant. The basis functions are related by $\\phi_i(\\mathbf{x}) = \\hat{\\phi}_i(\\mathcal{F}_e^{-1}(\\mathbf{x}))$. The integral becomes:\n$$\n(M_e)_{ij} = J_e \\int_{\\hat{K}} \\hat{\\phi}_i(\\hat{\\mathbf{x}}) \\hat{\\phi}_j(\\hat{\\mathbf{x}}) \\, d\\hat{V}\n$$\nIf the reference basis $\\{\\hat{\\phi}_i\\}_{i=1}^{N_p}$ is chosen to be orthonormal with respect to the $L^2$ inner product on $\\hat{K}$, then by definition:\n$$\n\\int_{\\hat{K}} \\hat{\\phi}_i(\\hat{\\mathbf{x}}) \\hat{\\phi}_j(\\hat{\\mathbf{x}}) \\, d\\hat{V} = \\delta_{ij}\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. Substituting this into the expression for $(M_e)_{ij}$ yields:\n$$\n(M_e)_{ij} = J_e \\delta_{ij}\n$$\nThis means that for a scalar field, the element mass matrix is diagonal: $\\mathbf{M}_e = J_e \\mathbf{I}$, where $\\mathbf{I}$ is the $N_p \\times N_p$ identity matrix.\n\nFor vector fields with $N_{\\text{fld}}$ components, where each component is expanded in the same scalar basis, the full element mass matrix becomes block-diagonal, with each of the $N_{\\text{fld}}$ diagonal blocks being the scalar mass matrix $J_e \\mathbf{I}$. This is itself a diagonal matrix of size $(N_{\\text{fld}}N_p) \\times (N_{\\text{fld}}N_p)$.\n\nThe per-stage cost of an explicit method is dominated by the evaluation of the right-hand side, which involves applying $\\mathbf{M}_e^{-1}$.\n-   If $\\mathbf{M}_e = J_e\\mathbf{I}$, then $\\mathbf{M}_e^{-1} = J_e^{-1}\\mathbf{I}$. Applying this inverse to a vector of length $N_p$ is a simple scaling, which costs $\\mathcal{O}(N_p)$ operations.\n-   If $\\mathbf{M}_e$ is a dense $N_p \\times N_p$ matrix (from a non-orthonormal basis), its inverse $\\mathbf{M}_e^{-1}$ is also dense. Applying the inverse (via a matrix-vector product with a precomputed inverse) costs $\\mathcal{O}(N_p^2)$ operations.\n\n### Option-by-Option Analysis\n\n**A. With a strictly orthonormal modal basis on $\\hat{K}$ and exact quadrature, the element mass matrix on $K_e$ is diagonal up to the constant scaling by $J_e$, i.e., $\\mathbf{M}_e = J_e \\mathbf{I}$ for scalar fields (and block-diagonal with identical diagonal blocks for multiple field components). Consequently, each explicit stage requires only componentwise scalings by $J_e^{-1}$ to apply $\\mathbf{M}_e^{-1}$ locally.**\n\n-   **Justification:** As derived above, an orthonormal basis on the reference element directly leads to $(M_e)_{ij} = J_e \\delta_{ij}$ for an affine mapping. This means $\\mathbf{M}_e = J_e \\mathbf{I}$ for a scalar field. For multiple field components, the matrix is block-diagonal with these $J_e \\mathbf{I}$ matrices on its diagonal. The inverse is then $J_e^{-1}\\mathbf{I}$ (or block-diagonal with $J_e^{-1}\\mathbf{I}$ blocks), and its application is a simple scaling operation. The statement is a precise and accurate description of the consequences of using an orthonormal basis.\n-   **Verdict:** Correct.\n\n**B. Using a full (dense) element mass matrix in an explicit DGTD scheme reduces the global Courant–Friedrichs–Lewy (CFL) time step restriction compared to a diagonal mass matrix because the dense inverse $\\mathbf{M}_e^{-1}$ damps high-frequency modal content and thereby enlarges the stability region effectively.**\n\n-   **Justification:** This statement is logically flawed. The term \"reduces the ... restriction\" is ambiguous. If it means makes the restriction more severe (i.e., reduces the maximum allowable time step $\\Delta t_{max}$), then the claim contradicts the provided reason (\"enlarges the stability region\"). If it means makes the restriction less severe (i.e., increases $\\Delta t_{max}$), it's a very poor choice of words. Assuming the latter interpretation to align with the provided reason, the statement claims that a full mass matrix increases $\\Delta t_{max}$. While it is true that a consistent (full) mass matrix can slightly increase the maximum stable time step compared to a lumped/diagonal mass matrix by having a smaller spectral radius for the discrete operator, the statement as written is self-contradictory under the most natural reading of \"reduces the restriction\". A scientific statement must be unambiguous. Due to this severe ambiguity and internal contradiction, the statement is invalid.\n-   **Verdict:** Incorrect.\n\n**C. For affine elements in $3$ dimensions with polynomial order $p$ and $N_p$ unknowns per scalar field per element, the per-stage cost to apply $\\mathbf{M}_e^{-1}$ scales as $\\mathcal{O}(N_p)$ for a diagonal mass (orthonormal modal basis) and as $\\mathcal{O}(N_p^2)$ when applying a precomputed dense inverse for a full mass matrix, neglecting memory hierarchy effects.**\n\n-   **Justification:** This addresses the computational cost of inverting the mass matrix. For a diagonal mass matrix of size $N_p \\times N_p$, applying the inverse requires $N_p$ divisions, an $\\mathcal{O}(N_p)$ operation. For a dense mass matrix, applying its precomputed dense inverse requires a matrix-vector multiplication of an $N_p \\times N_p$ matrix with an $N_p$-vector, which costs $\\mathcal{O}(N_p^2)$ floating-point operations. The statement accurately contrasts these two computational complexities.\n-   **Verdict:** Correct.\n\n**D. Orthonormality of a modal basis with respect to the $L^2$ inner product can be achieved only on tensor-product hexahedra; on simplices, any reasonable polynomial basis produces an element mass matrix that is at least block-dense, so $\\mathbf{M}_e$ cannot be diagonalized by a fixed modal choice.**\n\n-   **Justification:** This statement is factually incorrect. It is possible to construct polynomial bases that are orthonormal with respect to the $L^2$ inner product on any standard element shape, including simplices (triangles in 2D, tetrahedra in 3D). Such bases can be constructed, for instance, by applying a Gram-Schmidt orthogonalization process to a standard basis like the monomials. Well-known examples for simplices exist, such as Dubiner polynomials for the triangle. Therefore, it is entirely feasible to obtain a diagonal mass matrix on simplicial elements by choosing an appropriate modal basis.\n-   **Verdict:** Incorrect.\n\n**E. The primary benefit of diagonal element mass matrices in DGTD is that they eliminate the need to evaluate numerical fluxes on element interfaces, making the update entirely local within each element at every explicit stage.**\n\n-   **Justification:** This statement demonstrates a fundamental misunderstanding of the DGTD method. The mass matrix $\\mathbf{M}_e$ arises from the time derivative term ($\\partial \\mathbf{U} / \\partial t$), while the numerical fluxes are part of the spatial operator on the right-hand side, $\\mathbf{R}_e$. The fluxes are essential for coupling adjacent elements and ensuring stability and accuracy; they are independent of the mass matrix structure. A diagonal mass matrix only simplifies the inversion step ($\\mathbf{M}_e^{-1} \\mathbf{R}_e$), making the update of coefficients on an element computationally cheap. However, the calculation of the update vector $\\mathbf{R}_e$ still requires communication with neighboring elements to evaluate the numerical fluxes. The update is not \"entirely local\" in the sense of being independent of neighbors.\n-   **Verdict:** Incorrect.", "answer": "$$\\boxed{AC}$$", "id": "3300605"}, {"introduction": "While discretizing Maxwell's curl equations is the primary focus of DGTD, ensuring that the divergence constraints (Gauss's laws) are also satisfied is critical for physical accuracy and long-term stability. This problem explores why naive schemes can fail to preserve these constraints and introduces the Generalized Lagrange Multiplier (GLM) method as a powerful remedy. You will design an augmented system and analyze its properties, learning how to build more robust and physically consistent numerical models [@problem_id:3300578].", "problem": "Consider linear, isotropic, source-driven Maxwell equations in a homogeneous medium with permittivity $\\varepsilon$ and permeability $\\mu$,\n$$\n\\partial_{t}\\mathbf{D}=\\nabla\\times\\mathbf{H}-\\mathbf{J},\\quad\n\\partial_{t}\\mathbf{B}=-\\nabla\\times\\mathbf{E},\\quad\n\\nabla\\cdot\\mathbf{D}=\\rho,\\quad\n\\nabla\\cdot\\mathbf{B}=0,\n$$\nwith constitutive relations $\\mathbf{D}=\\varepsilon \\mathbf{E}$ and $\\mathbf{B}=\\mu \\mathbf{H}$, and charge continuity\n$$\n\\partial_{t}\\rho+\\nabla\\cdot\\mathbf{J}=0.\n$$\nA Discontinuous Galerkin Time-Domain (DGTD) semidiscretization that approximates only the curl equations using standard numerical fluxes does not, in general, enforce the divergence constraints at the discrete level.\n\n(a) Starting from the above equations and the structure of discontinuous Galerkin methods, explain why discretizing only the curl equations may fail to enforce $\\nabla\\cdot\\mathbf{D}=\\rho$ in a discrete sense. Your explanation should relate to the lack of exact commutation between discrete differential operators on broken polynomial spaces and to possible inconsistency with the discrete continuity equation induced by numerical fluxes and time integration.\n\n(b) To restore control of divergence errors in a manner compatible with Discontinuous Galerkin Time-Domain (DGTD) discretizations, augment Maxwell’s system with a Generalized Lagrange Multiplier (GLM) scalar field $\\psi$ for the electric field. Propose a hyperbolic–parabolic GLM formulation by adding a gradient term to Ampère’s law and an evolution equation for $\\psi$ that couples to $\\nabla\\cdot\\mathbf{D}$ and damps $\\psi$. The augmented continuous system should be linear and consistent with the original Maxwell equations at steady state when the constraints are satisfied. State your proposed augmented equations explicitly.\n\n(c) Let $q=\\nabla\\cdot\\mathbf{D}-\\rho$ denote the electric divergence error. Using only the fundamental equations in parts (a) and (b) and the charge continuity equation, derive a closed partial differential equation governing $q$ and characterize the propagation and damping of $q$. Then, consider a spatially periodic domain and a single Fourier mode with wavenumber magnitude $k$, i.e., assume $q(\\mathbf{x},t)=\\widehat{q}\\exp(\\mathrm{i}\\mathbf{k}\\cdot\\mathbf{x}+s t)$ and $\\psi(\\mathbf{x},t)=\\widehat{\\psi}\\exp(\\mathrm{i}\\mathbf{k}\\cdot\\mathbf{x}+s t)$ with $|\\mathbf{k}|=k$. Derive the dispersion relation for the divergence-cleaning mode and provide the two temporal eigenvalues $s_{\\pm}(k)$ in closed form in terms of the GLM wave speed parameter $\\alpha>0$, the damping parameter $\\kappa\\ge 0$, and the wavenumber magnitude $k\\ge 0$.\n\nExpress your final eigenvalue expression in $\\mathrm{s}^{-1}$, assuming $k$ is given in $\\mathrm{rad}\\,\\mathrm{m}^{-1}$, $\\alpha$ in $\\mathrm{m}\\,\\mathrm{s}^{-1}$, and $\\kappa$ in $\\mathrm{s}^{-1}$. No numerical evaluation is required, and no rounding is needed. Your final answer must be only the pair of temporal eigenvalues as a single analytic expression.", "solution": "The problem presents a multi-part question concerning the numerical solution of Maxwell's equations using Discontinuous Galerkin Time-Domain (DGTD) methods, focusing on the issue of divergence error control. The problem is valid as it is scientifically grounded in classical electromagnetism and computational physics, is well-posed, objective, and contains a complete and consistent setup. We proceed with the solution.\n\n(a) Explanation of Discrete Divergence Error\n\nIn the continuous setting, Maxwell's equations inherently preserve the divergence constraints if they are satisfied initially. Taking the divergence of Ampère's law, $\\partial_{t}\\mathbf{D}=\\nabla\\times\\mathbf{H}-\\mathbf{J}$, yields:\n$$\n\\nabla\\cdot(\\partial_{t}\\mathbf{D}) = \\nabla\\cdot(\\nabla\\times\\mathbf{H}) - \\nabla\\cdot\\mathbf{J}\n$$\nUsing the vector calculus identity $\\nabla\\cdot(\\nabla\\times\\mathbf{F})=0$ for any vector field $\\mathbf{F}$, and swapping the time and space derivatives on the left side, we get:\n$$\n\\partial_{t}(\\nabla\\cdot\\mathbf{D}) = -\\nabla\\cdot\\mathbf{J}\n$$\nThe charge continuity equation states $\\partial_{t}\\rho+\\nabla\\cdot\\mathbf{J}=0$, which implies $-\\nabla\\cdot\\mathbf{J} = \\partial_{t}\\rho$. Substituting this into the previous equation gives:\n$$\n\\partial_{t}(\\nabla\\cdot\\mathbf{D}) = \\partial_{t}\\rho \\quad \\implies \\quad \\partial_{t}(\\nabla\\cdot\\mathbf{D}-\\rho) = 0\n$$\nThis result shows that the quantity $(\\nabla\\cdot\\mathbf{D}-\\rho)$, which represents the error in Gauss's law for electricity, is constant in time. Therefore, if Gauss's law ($\\nabla\\cdot\\mathbf{D}=\\rho$) is satisfied at the initial time $t=0$, it remains satisfied for all subsequent times $t>0$.\n\nIn a DGTD framework, the computational domain is decomposed into a mesh of elements, and the solution is approximated by piecewise polynomials within each element, which are discontinuous across element faces. The differential operators $\\nabla\\cdot$ and $\\nabla\\times$ are replaced by discrete counterparts, let's call them $\\mathcal{D}_h$ and $\\mathcal{C}_h$, which act on this space of \"broken\" (discontinuous) polynomials. These discrete operators are typically defined via the weak formulation, involving volume integrals over elements and surface integrals over faces where numerical fluxes enforce coupling between elements.\n\nA DGTD semidiscretization of Ampère's law, focusing only on the curl equations, takes the form:\n$$\n\\partial_{t}\\mathbf{D}_h = \\mathcal{C}_h\\mathbf{H}_h - \\mathbf{J}_h\n$$\nwhere $\\mathbf{D}_h$, $\\mathbf{H}_h$, and $\\mathbf{J}_h$ are the discrete approximations of the corresponding fields. Applying the discrete divergence operator $\\mathcal{D}_h$ to this equation gives:\n$$\n\\partial_{t}(\\mathcal{D}_h\\mathbf{D}_h) = \\mathcal{D}_h(\\mathcal{C}_h\\mathbf{H}_h) - \\mathcal{D}_h\\mathbf{J}_h\n$$\nThe fundamental problem arises because, for standard DGTD methods, the discrete operators do not preserve the structure of the continuous vector identity. Specifically, the composition of the discrete divergence and curl operators is not identically zero:\n$$\n\\mathcal{D}_h\\mathcal{C}_h \\neq 0\n$$\nThis lack of exact commutation is a well-known property of many numerical schemes on staggered or non-staggered grids, including DGTD with standard fluxes. Consequently, even if the discrete charge continuity equation $\\partial_{t}\\rho_h + \\mathcal{D}_h\\mathbf{J}_h = 0$ is satisfied exactly, the evolution of the discrete divergence error $q_h = \\mathcal{D}_h\\mathbf{D}_h-\\rho_h$ becomes:\n$$\n\\partial_{t}q_h = \\partial_{t}(\\mathcal{D}_h\\mathbf{D}_h) - \\partial_{t}\\rho_h = (\\mathcal{D}_h\\mathcal{C}_h\\mathbf{H}_h - \\mathcal{D}_h\\mathbf{J}_h) - (- \\mathcal{D}_h\\mathbf{J}_h) = \\mathcal{D}_h\\mathcal{C}_h\\mathbf{H}_h\n$$\nSince $\\mathcal{D}_h\\mathcal{C}_h\\mathbf{H}_h$ is generally non-zero, this term acts as a source for the divergence error. Even if the discrete Gauss's law $q_h=0$ is satisfied at $t=0$, numerical errors and the evolving field $\\mathbf{H}_h$ will cause $q_h$ to grow over time, leading to a violation of the divergence constraint at the discrete level.\n\n(b) Proposed Augmented GLM System\n\nTo restore control over the divergence error, we introduce a Generalized Lagrange Multiplier (GLM) formulation. This involves augmenting Maxwell's equations with an additional scalar field $\\psi$ and modifying the equations to create a mechanism that actively damps divergence errors. A suitable hyperbolic-parabolic GLM formulation is proposed as follows.\n\nWe modify Ampère's law by adding a gradient term involving $\\psi$:\n$$\n\\partial_{t}\\mathbf{D}=\\nabla\\times\\mathbf{H}-\\mathbf{J} - \\nabla\\psi\n$$\nThe other curl equation, Faraday's law, remains unchanged:\n$$\n\\partial_{t}\\mathbf{B}=-\\nabla\\times\\mathbf{E}\n$$\nWe introduce a new evolution equation for the scalar field $\\psi$. This equation couples $\\psi$ to the electric divergence error $(\\nabla\\cdot\\mathbf{D}-\\rho)$, includes a damping term proportional to $\\psi$, and a parameter $\\alpha$ that governs the propagation speed of the divergence-cleaning waves:\n$$\n\\partial_{t}\\psi + \\alpha^2(\\nabla\\cdot\\mathbf{D}-\\rho) + \\kappa\\psi = 0\n$$\nHere, $\\alpha$ is the GLM wave speed parameter with units of $\\mathrm{m}\\,\\mathrm{s}^{-1}$, and $\\kappa$ is the damping parameter with units of $\\mathrm{s}^{-1}$.\n\nThis augmented system is linear. It is consistent with the original Maxwell's equations in the sense that if the constraint $\\nabla\\cdot\\mathbf{D}=\\rho$ is satisfied and we are in a steady state where all time derivatives are zero, the GLM equation requires $\\kappa\\psi=0$. For $\\kappa>0$, this implies $\\psi=0$, which in turn means $\\nabla\\psi=0$. The augmented Ampère's law then reduces to the original steady-state equation $0=\\nabla\\times\\mathbf{H}-\\mathbf{J}$.\n\n(c) Divergence Error PDE and Dispersion Relation\n\nLet $q=\\nabla\\cdot\\mathbf{D}-\\rho$ be the electric divergence error. We will derive a closed partial differential equation for $q$. First, take the divergence of the augmented Ampère's law from part (b):\n$$\n\\partial_{t}(\\nabla\\cdot\\mathbf{D}) = \\nabla\\cdot(\\nabla\\times\\mathbf{H}) - \\nabla\\cdot\\mathbf{J} - \\nabla\\cdot(\\nabla\\psi) = -\\nabla\\cdot\\mathbf{J} - \\nabla^2\\psi\n$$\nUsing the charge continuity equation $\\nabla\\cdot\\mathbf{J} = -\\partial_t\\rho$, we obtain:\n$$\n\\partial_{t}(\\nabla\\cdot\\mathbf{D}) = \\partial_{t}\\rho - \\nabla^2\\psi\n$$\nThis gives the first equation for the evolution of the error $q$:\n$$\n\\partial_{t}q = \\partial_{t}(\\nabla\\cdot\\mathbf{D}-\\rho) = -\\nabla^2\\psi\n$$\nThe second equation is the GLM evolution equation, written in terms of $q$:\n$$\n\\partial_{t}\\psi + \\alpha^2 q + \\kappa\\psi = 0\n$$\nTo obtain a single, closed PDE for $q$, we eliminate $\\psi$. Differentiate the GLM equation with respect to time:\n$$\n\\partial_{t}^2\\psi + \\alpha^2 \\partial_{t}q + \\kappa\\partial_{t}\\psi = 0\n$$\nFrom the first derived equation, we have $\\nabla^2\\psi = -\\partial_t q$. Applying the Laplacian operator $\\nabla^2$ to the time-differentiated GLM equation is not straightforward. Instead, we can operate on the GLM equation with $\\nabla^2$:\n$$\n\\nabla^2(\\partial_{t}\\psi) + \\alpha^2\\nabla^2 q + \\kappa\\nabla^2\\psi = 0\n$$\nNow, substitute $\\nabla^2\\psi = -\\partial_t q$ and $\\nabla^2(\\partial_t \\psi) = \\partial_t(\\nabla^2\\psi) = \\partial_t(-\\partial_t q) = -\\partial_t^2 q$:\n$$\n-\\partial_{t}^2 q + \\alpha^2\\nabla^2 q + \\kappa(-\\partial_t q) = 0\n$$\nRearranging the terms gives the closed PDE for $q$:\n$$\n\\frac{\\partial^2 q}{\\partial t^2} + \\kappa \\frac{\\partial q}{\\partial t} - \\alpha^2 \\nabla^2 q = 0\n$$\nThis is a damped wave equation. It demonstrates that the divergence error $q$ propagates with a speed $\\alpha$ and is attenuated at a rate related to $\\kappa$. Thus, any initial divergence error is actively propagated away and damped out, rather than persisting or growing.\n\nTo find the dispersion relation, we assume a single Fourier mode solution of the form $q(\\mathbf{x},t)=\\widehat{q}\\exp(\\mathrm{i}\\mathbf{k}\\cdot\\mathbf{x}+s t)$, where $|\\mathbf{k}|=k$ is the wavenumber magnitude and $s$ is the temporal eigenvalue. We substitute this ansatz into the PDE for $q$. The derivatives become:\n- $\\frac{\\partial q}{\\partial t} = s q$\n- $\\frac{\\partial^2 q}{\\partial t^2} = s^2 q$\n- $\\nabla^2 q = -k^2 q$\n\nSubstituting these into the PDE gives:\n$$\ns^2 q + \\kappa(s q) - \\alpha^2(-k^2 q) = 0\n$$\nDividing by $q$ (assuming a non-trivial solution), we obtain the dispersion relation:\n$$\ns^2 + \\kappa s + \\alpha^2 k^2 = 0\n$$\nThis is a quadratic equation for $s$. We solve for the two temporal eigenvalues $s_{\\pm}(k)$ using the quadratic formula:\n$$\ns_{\\pm}(k) = \\frac{-\\kappa \\pm \\sqrt{\\kappa^2 - 4(1)(\\alpha^2 k^2)}}{2}\n$$\nThe final expression for the two temporal eigenvalues is:\n$$\ns_{\\pm}(k) = \\frac{-\\kappa \\pm \\sqrt{\\kappa^2 - 4\\alpha^2 k^2}}{2}\n$$\nThese eigenvalues characterize the temporal behavior (damping and/or oscillation) of a divergence error mode with wavenumber $k$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{-\\kappa + \\sqrt{\\kappa^{2} - 4\\alpha^{2}k^{2}}}{2} & \\frac{-\\kappa - \\sqrt{\\kappa^{2} - 4\\alpha^{2}k^{2}}}{2}\n\\end{pmatrix}\n}\n$$", "id": "3300578"}]}