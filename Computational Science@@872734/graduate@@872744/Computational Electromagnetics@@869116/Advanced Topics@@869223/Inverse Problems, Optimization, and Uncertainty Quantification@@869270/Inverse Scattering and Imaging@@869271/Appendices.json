{"hands_on_practices": [{"introduction": "The foundation of any inverse problem is a deep understanding of the forward problem: how a physical object generates measurable data. This exercise focuses on deriving the relationship between a scattering object's properties and the far-field data it produces. By working through the Born and Rytov approximations, you will linearize the complex scattering physics, revealing a direct and elegant connection between the object's permittivity contrast and its Fourier transform. This principle, known as the Fourier diffraction theorem, is the cornerstone of many practical imaging techniques like diffraction tomography and is the essential first step toward formulating linear inverse scattering algorithms. [@problem_id:3320197]", "problem": "Consider two-dimensional time-harmonic electromagnetic scattering under Transverse Electric (TE) polarization, where the out-of-plane electric field $E(\\mathbf{r})$ satisfies the scalar Helmholtz equation in a nonmagnetic medium with spatially varying permittivity $\\epsilon(\\mathbf{r})$ and background permittivity $\\epsilon_{b}$. Let the angular frequency be $\\omega$ and the background wavenumber be $k_{b} = \\omega \\sqrt{\\mu_{0} \\epsilon_{b}}$, where $\\mu_{0}$ is the vacuum permeability. Define the dimensionless permittivity contrast by $\\chi(\\mathbf{r}) = \\frac{\\epsilon(\\mathbf{r}) - \\epsilon_{b}}{\\epsilon_{b}}$, and assume $\\chi(\\mathbf{r})$ is compactly supported and sufficiently small in magnitude so that single-scattering physics dominates.\n\nThe total field $E(\\mathbf{r})$ obeys the inhomogeneous Helmholtz equation $(\\nabla^{2} + k_{b}^{2}) E(\\mathbf{r}) = -k_{b}^{2} \\chi(\\mathbf{r}) E(\\mathbf{r})$ and admits the Lippmann–Schwinger integral representation with the two-dimensional background Green’s function $G(\\mathbf{r}, \\mathbf{r}') = \\frac{i}{4} H_{0}^{(1)}(k_{b} |\\mathbf{r} - \\mathbf{r}'|)$, where $H_{0}^{(1)}(\\cdot)$ is the Hankel function of the first kind of order zero. The incident field is a unit-amplitude plane wave $E_{i}(\\mathbf{r}) = \\exp(i k_{b} \\hat{\\mathbf{s}}_{i} \\cdot \\mathbf{r})$ propagating in direction $\\hat{\\mathbf{s}}_{i}$, and the receiver is located at $\\mathbf{r}_{r} = R \\hat{\\mathbf{s}}_{r}$ with $R \\gg a$, where $\\hat{\\mathbf{s}}_{r}$ is the observation direction and $a$ is a characteristic length scale of the inhomogeneity.\n\nLet the permittivity contrast be a centered Gaussian,\n$$\n\\chi(\\mathbf{r}) = \\chi_{0} \\exp\\!\\left(-\\frac{|\\mathbf{r}|^{2}}{a^{2}}\\right),\n$$\nwith constant amplitude $\\chi_{0}$ satisfying $|\\chi_{0}| \\ll 1$. Define the scattering vector by $\\mathbf{q} = k_{b} (\\hat{\\mathbf{s}}_{r} - \\hat{\\mathbf{s}}_{i})$.\n\nUsing only first principles of electromagnetics and asymptotic analysis appropriate to the two-dimensional far field, derive the leading-order far-field, linearized data function obtained both by the Born linearization and by the Rytov linearization. To remove geometric spreading and receiver-phase factors, define the normalized far-field Rytov data function\n$$\nD(\\hat{\\mathbf{s}}_{r}, \\hat{\\mathbf{s}}_{i}) \\equiv \\sqrt{\\frac{\\pi k_{b} R}{2}} \\, \\exp\\!\\left(-i\\big(k_{b} R - \\tfrac{\\pi}{4} - k_{b} \\hat{\\mathbf{s}}_{i} \\cdot \\mathbf{r}_{r}\\big)\\right) \\left[\\ln\\!\\big(E(\\mathbf{r}_{r})\\big) - \\ln\\!\\big(E_{i}(\\mathbf{r}_{r})\\big)\\right].\n$$\n\nAssuming $|\\chi_{0}| \\ll 1$ and working to leading order in $\\chi_{0}$, provide a single closed-form analytic expression for $D(\\hat{\\mathbf{s}}_{r}, \\hat{\\mathbf{s}}_{i})$ in terms of $\\chi_{0}$, $a$, $k_{b}$, and $|\\mathbf{q}|$. Your answer must be a single analytic expression. No numerical evaluation is required and no rounding is needed.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It represents a standard formulation in inverse scattering theory.\n\nThe starting point is the Lippmann-Schwinger integral equation for the total electric field $E(\\mathbf{r})$, which is a sum of the incident field $E_i(\\mathbf{r})$ and the scattered field $E_s(\\mathbf{r})$:\n$$\nE(\\mathbf{r}) = E_i(\\mathbf{r}) + \\int_{\\mathbb{R}^2} G(\\mathbf{r}, \\mathbf{r}') k_b^2 \\chi(\\mathbf{r}') E(\\mathbf{r}') d^2\\mathbf{r}'\n$$\nHere, $\\mathbf{r}$ is the observation point, $\\mathbf{r}'$ is the integration variable over the scattering object's domain, $k_b$ is the background wavenumber, $\\chi(\\mathbf{r}')$ is the permittivity contrast, and $G(\\mathbf{r}, \\mathbf{r}')$ is the two-dimensional background Green's function. The total field can be written as $E = E_i + E_s$.\n\nThe problem requires deriving a data function based on linearization. We consider two standard linearizations: the Born and Rytov approximations, both valid for weak scattering, i.e., when $|\\chi_0| \\ll 1$.\n\nThe first Born approximation consists of replacing the unknown total field $E(\\mathbf{r}')$ inside the integral with the known incident field $E_i(\\mathbf{r}') = \\exp(i k_b \\hat{\\mathbf{s}}_i \\cdot \\mathbf{r}')$. This linearizes the equation with respect to $\\chi$ and yields the Born scattered field, $E_s^{(B)}$:\n$$\nE_s^{(B)}(\\mathbf{r}) = k_b^2 \\int_{\\mathbb{R}^2} G(\\mathbf{r}, \\mathbf{r}') \\chi(\\mathbf{r}') E_i(\\mathbf{r}') d^2\\mathbf{r}'\n$$\nThe Rytov approximation represents the total field as the exponential of a complex phase, $E(\\mathbf{r}) = \\exp[\\phi(\\mathbf{r})]$. The incident field is $E_i(\\mathbf{r}) = \\exp[\\phi_i(\\mathbf{r})]$, where $\\phi_i(\\mathbf{r}) = i k_b \\hat{\\mathbf{s}}_i \\cdot \\mathbf{r}$. The total phase is then $\\phi(\\mathbf{r}) = \\phi_i(\\mathbf{r}) + \\phi_s(\\mathbf{r})$, where $\\phi_s(\\mathbf{r})$ is the scattered phase. The logarithmic quantity in the definition of the data function $D$ is $\\ln(E(\\mathbf{r}_r)) - \\ln(E_i(\\mathbf{r}_r)) = \\ln(E(\\mathbf{r}_r)/E_i(\\mathbf{r}_r)) = \\phi_s(\\mathbf{r}_r)$.\n\nUnder the weak scattering assumption, the Born and Rytov approximations are related. The total field under the Born approximation is $E^{(B)}(\\mathbf{r}) = E_i(\\mathbf{r}) + E_s^{(B)}(\\mathbf{r}) = E_i(\\mathbf{r}) (1 + E_s^{(B)}(\\mathbf{r})/E_i(\\mathbf{r}))$. The scattered phase to first order in the contrast $\\chi$ is given by:\n$$\n\\phi_s(\\mathbf{r}) = \\ln\\left(\\frac{E(\\mathbf{r})}{E_i(\\mathbf{r})}\\right) \\approx \\ln\\left(1 + \\frac{E_s^{(B)}(\\mathbf{r})}{E_i(\\mathbf{r})}\\right) \\approx \\frac{E_s^{(B)}(\\mathbf{r})}{E_i(\\mathbf{r})}\n$$\nThe last step uses the Taylor expansion $\\ln(1+z) \\approx z$ for small $z$, which is justified since $E_s^{(B)}$ is of order $\\chi_0 \\ll 1$. Thus, to find the leading-order Rytov data function, we can compute the Born scattered field $E_s^{(B)}$.\n\nWe evaluate $E_s^{(B)}$ at the far-field receiver location $\\mathbf{r}_r = R \\hat{\\mathbf{s}}_r$, where $R \\gg a$ and $a$ is the characteristic size of the scatterer. We substitute the given forms for $G(\\mathbf{r}, \\mathbf{r}')$, $E_i(\\mathbf{r}')$, and $\\chi(\\mathbf{r}')$:\n$$\nE_s^{(B)}(\\mathbf{r}_r) = k_b^2 \\int_{\\mathbb{R}^2} \\frac{i}{4} H_0^{(1)}(k_b |\\mathbf{r}_r - \\mathbf{r}'|) \\chi(\\mathbf{r}') \\exp(i k_b \\hat{\\mathbf{s}}_i \\cdot \\mathbf{r}') d^2\\mathbf{r}'\n$$\nIn the far-field limit ($|\\mathbf{r}_r| = R \\to \\infty$), we use the asymptotic expansion for the Hankel function $H_0^{(1)}(z) \\approx \\sqrt{\\frac{2}{\\pi z}} \\exp(i(z - \\frac{\\pi}{4}))$ for large $z$. The argument is $z = k_b |\\mathbf{r}_r - \\mathbf{r}'|$. Since $R \\gg |\\mathbf{r}'|$, we approximate the distance as $|\\mathbf{r}_r - \\mathbf{r}'| \\approx R - \\hat{\\mathbf{s}}_r \\cdot \\mathbf{r}'$. This approximation is used in the rapidly varying phase term of the exponential. In the slowly varying amplitude term $\\sqrt{1/z}$, we can approximate $|\\mathbf{r}_r - \\mathbf{r}'| \\approx R$.\n$$\nH_0^{(1)}(k_b |\\mathbf{r}_r - \\mathbf{r}'|) \\approx \\sqrt{\\frac{2}{\\pi k_b R}} \\exp\\left(i\\left(k_b (R - \\hat{\\mathbf{s}}_r \\cdot \\mathbf{r}') - \\frac{\\pi}{4}\\right)\\right)\n$$\nSubstituting this into the integral for $E_s^{(B)}(\\mathbf{r}_r)$:\n$$\nE_s^{(B)}(\\mathbf{r}_r) \\approx \\frac{i k_b^2}{4} \\int_{\\mathbb{R}^2} \\left[ \\sqrt{\\frac{2}{\\pi k_b R}} \\exp\\left(i(k_b R - \\frac{\\pi}{4})\\right) \\exp(-i k_b \\hat{\\mathbf{s}}_r \\cdot \\mathbf{r}') \\right] \\chi(\\mathbf{r}') \\exp(i k_b \\hat{\\mathbf{s}}_i \\cdot \\mathbf{r}') d^2\\mathbf{r}'\n$$\nFactoring out terms independent of $\\mathbf{r}'$:\n$$\nE_s^{(B)}(\\mathbf{r}_r) \\approx \\frac{i k_b^2}{4} \\sqrt{\\frac{2}{\\pi k_b R}} \\exp\\left(i(k_b R - \\frac{\\pi}{4})\\right) \\int_{\\mathbb{R}^2} \\chi(\\mathbf{r}') \\exp(-i k_b (\\hat{\\mathbf{s}}_r - \\hat{\\mathbf{s}}_i) \\cdot \\mathbf{r}') d^2\\mathbf{r}'\n$$\nThe integral is the two-dimensional Fourier transform of the contrast $\\chi(\\mathbf{r}')$, denoted $\\tilde{\\chi}(\\mathbf{k})$, evaluated at the scattering vector $\\mathbf{q} = k_b (\\hat{\\mathbf{s}}_r - \\hat{\\mathbf{s}}_i)$.\n$$\n\\tilde{\\chi}(\\mathbf{q}) = \\int_{\\mathbb{R}^2} \\chi(\\mathbf{r}') \\exp(-i \\mathbf{q} \\cdot \\mathbf{r}') d^2\\mathbf{r}'\n$$\nSo, the scattered field is:\n$$\nE_s^{(B)}(\\mathbf{r}_r) \\approx \\frac{i k_b^2}{4} \\sqrt{\\frac{2}{\\pi k_b R}} \\exp\\left(i(k_b R - \\frac{\\pi}{4})\\right) \\tilde{\\chi}(\\mathbf{q})\n$$\nNow we compute the Rytov data function $D(\\hat{\\mathbf{s}}_r, \\hat{\\mathbf{s}}_i)$. Using $\\ln(E/E_i) \\approx E_s^{(B)}/E_i$:\n$$\nD(\\hat{\\mathbf{s}}_{r}, \\hat{\\mathbf{s}}_{i}) \\approx \\sqrt{\\frac{\\pi k_{b} R}{2}} \\, \\exp\\left(-i\\left(k_{b} R - \\frac{\\pi}{4} - k_{b} \\hat{\\mathbf{s}}_{i} \\cdot \\mathbf{r}_{r}\\right)\\right) \\frac{E_s^{(B)}(\\mathbf{r}_r)}{E_i(\\mathbf{r}_r)}\n$$\nSubstituting $E_i(\\mathbf{r}_r) = \\exp(i k_b \\hat{\\mathbf{s}}_i \\cdot \\mathbf{r}_r)$:\n$$\nD(\\hat{\\mathbf{s}}_{r}, \\hat{\\mathbf{s}}_{i}) \\approx \\sqrt{\\frac{\\pi k_{b} R}{2}} \\, \\exp\\left(-i(k_{b} R - \\frac{\\pi}{4})\\right) \\exp\\left(i k_{b} \\hat{\\mathbf{s}}_{i} \\cdot \\mathbf{r}_{r}\\right) \\frac{E_s^{(B)}(\\mathbf{r}_r)}{\\exp(i k_b \\hat{\\mathbf{s}}_i \\cdot \\mathbf{r}_r)}\n$$\nThe terms involving $\\exp(i k_b \\hat{\\mathbf{s}}_i \\cdot \\mathbf{r}_r)$ cancel:\n$$\nD(\\hat{\\mathbf{s}}_{r}, \\hat{\\mathbf{s}}_{i}) \\approx \\sqrt{\\frac{\\pi k_{b} R}{2}} \\, \\exp\\left(-i(k_{b} R - \\frac{\\pi}{4})\\right) E_s^{(B)}(\\mathbf{r}_r)\n$$\nSubstituting our expression for $E_s^{(B)}(\\mathbf{r}_r)$:\n$$\nD \\approx \\sqrt{\\frac{\\pi k_{b} R}{2}} \\exp\\left(-i(k_{b} R - \\frac{\\pi}{4})\\right) \\left[ \\frac{i k_b^2}{4} \\sqrt{\\frac{2}{\\pi k_b R}} \\exp\\left(i(k_b R - \\frac{\\pi}{4})\\right) \\tilde{\\chi}(\\mathbf{q}) \\right]\n$$\nThe geometric and phase factors cancel out:\n$$\n\\left(\\sqrt{\\frac{\\pi k_b R}{2}}\\sqrt{\\frac{2}{\\pi k_b R}}\\right) \\left(\\exp\\left(-i(k_b R - \\frac{\\pi}{4})\\right)\\exp\\left(i(k_b R - \\frac{\\pi}{4})\\right)\\right) = 1 \\cdot 1 = 1\n$$\nThis leaves a remarkably simple relation:\n$$\nD(\\hat{\\mathbf{s}}_{r}, \\hat{\\mathbf{s}}_{i}) \\approx \\frac{i k_b^2}{4} \\tilde{\\chi}(\\mathbf{q})\n$$\nThe final step is to calculate the Fourier transform of the given Gaussian contrast, $\\chi(\\mathbf{r}) = \\chi_0 \\exp(-|\\mathbf{r}|^2/a^2)$.\n$$\n\\tilde{\\chi}(\\mathbf{q}) = \\chi_0 \\int_{\\mathbb{R}^2} \\exp\\left(-\\frac{|\\mathbf{r}'|^2}{a^2}\\right) \\exp(-i \\mathbf{q} \\cdot \\mathbf{r}') d^2\\mathbf{r}'\n$$\nThis is a standard 2D Fourier transform of a Gaussian. In Cartesian coordinates $\\mathbf{r}'=(x,y)$ and $\\mathbf{q}=(q_x,q_y)$, the integral separates:\n$$\n\\tilde{\\chi}(\\mathbf{q}) = \\chi_0 \\left( \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{x^2}{a^2}\\right)\\exp(-iq_x x)dx \\right) \\left( \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{y^2}{a^2}\\right)\\exp(-iq_y y)dy \\right)\n$$\nUsing the standard 1D Gaussian Fourier transform integral $\\int_{-\\infty}^{\\infty} \\exp(-\\alpha u^2) \\exp(-i k u) du = \\sqrt{\\pi/\\alpha} \\exp(-k^2/(4\\alpha))$, with $\\alpha = 1/a^2$:\n$$\n\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{x^2}{a^2}\\right)\\exp(-iq_x x)dx = \\sqrt{\\pi a^2} \\exp\\left(-\\frac{q_x^2 a^2}{4}\\right) = a\\sqrt{\\pi} \\exp\\left(-\\frac{q_x^2 a^2}{4}\\right)\n$$\nThe integral over $y$ is identical. Multiplying them gives:\n$$\n\\tilde{\\chi}(\\mathbf{q}) = \\chi_0 (a\\sqrt{\\pi})(a\\sqrt{\\pi}) \\exp\\left(-\\frac{q_x^2 a^2}{4}\\right) \\exp\\left(-\\frac{q_y^2 a^2}{4}\\right) = \\chi_0 \\pi a^2 \\exp\\left(-\\frac{(q_x^2+q_y^2)a^2}{4}\\right)\n$$\nRecognizing that $q_x^2+q_y^2 = |\\mathbf{q}|^2$, we find:\n$$\n\\tilde{\\chi}(\\mathbf{q}) = \\chi_0 \\pi a^2 \\exp\\left(-\\frac{|\\mathbf{q}|^2 a^2}{4}\\right)\n$$\nFinally, substituting this into the expression for $D(\\hat{\\mathbf{s}}_r, \\hat{\\mathbf{s}}_i)$:\n$$\nD(\\hat{\\mathbf{s}}_{r}, \\hat{\\mathbf{s}}_{i}) = \\frac{i k_b^2}{4} \\left( \\chi_0 \\pi a^2 \\exp\\left(-\\frac{|\\mathbf{q}|^2 a^2}{4}\\right) \\right) = \\frac{i \\pi \\chi_0 a^2 k_b^2}{4} \\exp\\left(-\\frac{|\\mathbf{q}|^2 a^2}{4}\\right)\n$$\nThis is the final closed-form expression for the normalized Rytov data function to leading order in $\\chi_0$.", "answer": "$$\\boxed{\\frac{i \\pi \\chi_{0} a^{2} k_{b}^{2}}{4} \\exp\\left(-\\frac{|\\mathbf{q}|^{2} a^{2}}{4}\\right)}$$", "id": "3320197"}, {"introduction": "Moving beyond the idealized physics of the forward problem, this practice confronts a crucial aspect of all real-world inversion: the presence of noise and the need for stabilization. Building upon the linear models derived in the previous exercise, we now frame the inverse problem within a statistical context. You will apply Bayes' rule to derive a Maximum A Posteriori (MAP) estimator, a powerful technique that incorporates prior knowledge about the unknown object to regularize the solution and achieve stability in the presence of measurement noise. This problem provides hands-on experience with Bayesian inversion, a fundamental tool for obtaining meaningful results from imperfect data. [@problem_id:3320225]", "problem": "Consider a two-dimensional, time-harmonic Electromagnetic (EM) scattering experiment in Transverse Magnetic (TM) polarization at angular frequency $\\omega$, in a homogeneous, lossless background with relative permittivity $\\epsilon_{b}$ and permeability $\\mu_{0}$. A single, subwavelength dielectric inclusion at location $\\mathbf{r}_{0}$ is modeled by a real-valued, scalar contrast parameter $\\chi$ (dimensionless), supported on a small region of area $\\Delta S$ around $\\mathbf{r}_{0}$. The total field $E(\\mathbf{r})$ satisfies the frequency-domain Maxwell equations, which reduce (for TM polarization) to the scalar Helmholtz equation for $E_{z}(\\mathbf{r})$ with wavenumber $k=\\omega\\sqrt{\\mu_{0}\\epsilon_{b}}$, and the scattered field satisfies the Lippmann–Schwinger integral equation. Under the first Born approximation, with multiple illuminations and receivers that have been phase-referenced so that the Green-function phases are removed from the data model, the $M$-component real data vector $\\mathbf{y}\\in\\mathbb{R}^{M}$ obeys the linear model\n$$\n\\mathbf{y}=\\mathbf{A}\\,\\chi+\\mathbf{n},\n$$\nwhere $\\mathbf{A}\\in\\mathbb{R}^{M\\times 1}$ is a known sensitivity vector that incorporates the incident field and the background Green’s function evaluated at $\\mathbf{r}_{0}$, and $\\mathbf{n}\\in\\mathbb{R}^{M}$ is additive measurement noise.\n\nAssume the following for a particular experiment:\n- The number of measurements is $M=3$ and the sensitivity vector is\n$$\n\\mathbf{A}=\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}.\n$$\n- The noise $\\mathbf{n}$ is a zero-mean Gaussian random vector with a colored covariance\n$$\n\\boldsymbol{\\Sigma}_{n}=\\begin{bmatrix}\n1 & 0.5 & 0 \\\\\n0.5 & 1 & 0.5 \\\\\n0 & 0.5 & 1\n\\end{bmatrix}.\n$$\n- The prior on the scalar contrast is Gaussian, $\\chi\\sim\\mathcal{N}(0,\\sigma_{\\chi}^{2})$, with variance $\\sigma_{\\chi}^{2}=1$.\n- The measured data vector is\n$$\n\\mathbf{y}=\\begin{bmatrix} 0.5 \\\\ 1.0 \\\\ 0.5 \\end{bmatrix}.\n$$\n\nStarting from the fundamental scattering model and Bayes’ rule, derive the Maximum A Posteriori (MAP) estimator for the scalar $\\chi$ under the stated Gaussian likelihood and prior. Then, using only the given numerical values, evaluate the MAP estimate of $\\chi$. Round your answer to four significant figures and express it as a dimensionless number.", "solution": "The problem is valid. It is a well-posed problem in statistical inverse theory, specifically Bayesian estimation, applied to a standard linearized model of electromagnetic scattering. All necessary data are provided, and the underlying physical and mathematical models are sound.\n\nThe objective is to find the Maximum A Posteriori (MAP) estimate of the scalar contrast parameter $\\chi$. The MAP estimate is the value of $\\chi$ that maximizes the posterior probability density function (PDF) $p(\\chi|\\mathbf{y})$. According to Bayes' rule, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(\\chi|\\mathbf{y}) \\propto p(\\mathbf{y}|\\chi) p(\\chi)\n$$\nwhere $p(\\mathbf{y}|\\chi)$ is the likelihood function and $p(\\chi)$ is the prior PDF for the parameter $\\chi$.\n\nFirst, we define the likelihood function. The data model is given by the linear equation $\\mathbf{y} = \\mathbf{A}\\chi + \\mathbf{n}$, where the noise vector $\\mathbf{n}$ is a zero-mean Gaussian random variable with covariance $\\boldsymbol{\\Sigma}_n$, i.e., $\\mathbf{n} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_n)$. This implies that the conditional distribution of the data $\\mathbf{y}$ given a value of $\\chi$ is also Gaussian, with mean $\\mathbf{A}\\chi$ and covariance $\\boldsymbol{\\Sigma}_n$:\n$$\n\\mathbf{y}|\\chi \\sim \\mathcal{N}(\\mathbf{A}\\chi, \\boldsymbol{\\Sigma}_n)\n$$\nThe likelihood function is the PDF of this distribution, evaluated at the measured data $\\mathbf{y}$:\n$$\np(\\mathbf{y}|\\chi) = \\frac{1}{(2\\pi)^{M/2} |\\det(\\boldsymbol{\\Sigma}_n)|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{y} - \\mathbf{A}\\chi)^T \\boldsymbol{\\Sigma}_n^{-1} (\\mathbf{y} - \\mathbf{A}\\chi)\\right)\n$$\n\nSecond, we define the prior PDF. The problem states that the prior on $\\chi$ is a zero-mean Gaussian distribution with variance $\\sigma_{\\chi}^2$, i.e., $\\chi \\sim \\mathcal{N}(0, \\sigma_{\\chi}^2)$. The corresponding PDF is:\n$$\np(\\chi) = \\frac{1}{\\sqrt{2\\pi \\sigma_{\\chi}^2}} \\exp\\left(-\\frac{\\chi^2}{2\\sigma_{\\chi}^2}\\right)\n$$\n\nMaximizing the posterior PDF $p(\\chi|\\mathbf{y})$ is equivalent to minimizing its negative logarithm. Let $J_{\\text{MAP}}(\\chi) = -\\ln(p(\\chi|\\mathbf{y}))$. Dropping constant terms that do not depend on $\\chi$, we obtain the MAP objective function:\n$$\nJ_{\\text{MAP}}(\\chi) = \\frac{1}{2}(\\mathbf{y} - \\mathbf{A}\\chi)^T \\boldsymbol{\\Sigma}_n^{-1} (\\mathbf{y} - \\mathbf{A}\\chi) + \\frac{1}{2}\\frac{\\chi^2}{\\sigma_{\\chi}^2}\n$$\nTo find the MAP estimate $\\hat{\\chi}_{\\text{MAP}}$, we must find the value of $\\chi$ that minimizes this function. We do this by taking the derivative of $J_{\\text{MAP}}(\\chi)$ with respect to $\\chi$ and setting it to zero. Since $\\chi$ is a scalar, we expand the quadratic form involving $\\chi$:\n$$\n(\\mathbf{y} - \\mathbf{A}\\chi)^T \\boldsymbol{\\Sigma}_n^{-1} (\\mathbf{y} - \\mathbf{A}\\chi) = \\mathbf{y}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{y} - 2\\chi\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{y} + \\chi^2\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{A}\n$$\nSubstituting this into the objective function:\n$$\nJ_{\\text{MAP}}(\\chi) = \\frac{1}{2}\\left(\\mathbf{y}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{y} - 2\\chi\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{y} + \\chi^2\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{A}\\right) + \\frac{1}{2}\\frac{\\chi^2}{\\sigma_{\\chi}^2}\n$$\nDifferentiating with respect to $\\chi$:\n$$\n\\frac{d J_{\\text{MAP}}(\\chi)}{d\\chi} = -\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{y} + \\chi\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{A} + \\frac{\\chi}{\\sigma_{\\chi}^2}\n$$\nSetting the derivative to zero and solving for $\\hat{\\chi}_{\\text{MAP}}$:\n$$\n-\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{y} + \\hat{\\chi}_{\\text{MAP}}\\left(\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{A} + \\frac{1}{\\sigma_{\\chi}^2}\\right) = 0\n$$\n$$\n\\hat{\\chi}_{\\text{MAP}} = \\frac{\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{y}}{\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{A} + \\frac{1}{\\sigma_{\\chi}^2}}\n$$\nThis is the general expression for the MAP estimator. We now substitute the given numerical values:\n$\\mathbf{A}=\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}$, $\\mathbf{y}=\\begin{bmatrix} 0.5 \\\\ 1.0 \\\\ 0.5 \\end{bmatrix}$, $\\sigma_{\\chi}^{2}=1$, and $\\boldsymbol{\\Sigma}_{n}=\\begin{bmatrix} 1 & 0.5 & 0 \\\\ 0.5 & 1 & 0.5 \\\\ 0 & 0.5 & 1 \\end{bmatrix}$.\n\nFirst, we must compute the inverse of the covariance matrix $\\boldsymbol{\\Sigma}_n$. The determinant is:\n$$\n\\det(\\boldsymbol{\\Sigma}_n) = 1(1 \\cdot 1 - 0.5 \\cdot 0.5) - 0.5(0.5 \\cdot 1 - 0.5 \\cdot 0) + 0 = 1(0.75) - 0.5(0.5) = 0.75 - 0.25 = 0.5\n$$\nThe adjugate matrix, $\\text{adj}(\\boldsymbol{\\Sigma}_n)$, is the transpose of the cofactor matrix. For a symmetric matrix, it is equal to the cofactor matrix.\n$$\n\\text{adj}(\\boldsymbol{\\Sigma}_n) = \\begin{bmatrix}\n\\det\\begin{pmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{pmatrix} & -\\det\\begin{pmatrix} 0.5 & 0.5 \\\\ 0 & 1 \\end{pmatrix} & \\det\\begin{pmatrix} 0.5 & 1 \\\\ 0 & 0.5 \\end{pmatrix} \\\\\n-\\det\\begin{pmatrix} 0.5 & 0 \\\\ 0.5 & 1 \\end{pmatrix} & \\det\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} & -\\det\\begin{pmatrix} 1 & 0.5 \\\\ 0 & 0.5 \\end{pmatrix} \\\\\n\\det\\begin{pmatrix} 0.5 & 0 \\\\ 1 & 0.5 \\end{pmatrix} & -\\det\\begin{pmatrix} 1 & 0 \\\\ 0.5 & 0.5 \\end{pmatrix} & \\det\\begin{pmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{pmatrix}\n\\end{bmatrix}^T = \\begin{bmatrix} 0.75 & -0.5 & 0.25 \\\\ -0.5 & 1 & -0.5 \\\\ 0.25 & -0.5 & 0.75 \\end{bmatrix}\n$$\nThe inverse is $\\boldsymbol{\\Sigma}_n^{-1} = \\frac{1}{\\det(\\boldsymbol{\\Sigma}_n)}\\text{adj}(\\boldsymbol{\\Sigma}_n)$:\n$$\n\\boldsymbol{\\Sigma}_n^{-1} = \\frac{1}{0.5}\\begin{bmatrix} 0.75 & -0.5 & 0.25 \\\\ -0.5 & 1 & -0.5 \\\\ 0.25 & -0.5 & 0.75 \\end{bmatrix} = \\begin{bmatrix} 1.5 & -1 & 0.5 \\\\ -1 & 2 & -1 \\\\ 0.5 & -1 & 1.5 \\end{bmatrix}\n$$\nNext, we compute the term $\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1}$:\n$$\n\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1} = \\begin{bmatrix} 1 & 2 & 1 \\end{bmatrix} \\begin{bmatrix} 1.5 & -1 & 0.5 \\\\ -1 & 2 & -1 \\\\ 0.5 & -1 & 1.5 \\end{bmatrix}\n$$\n$$\n= \\begin{bmatrix} 1(1.5)+2(-1)+1(0.5) & 1(-1)+2(2)+1(-1) & 1(0.5)+2(-1)+1(1.5) \\end{bmatrix}\n$$\n$$\n= \\begin{bmatrix} 1.5-2+0.5 & -1+4-1 & 0.5-2+1.5 \\end{bmatrix} = \\begin{bmatrix} 0 & 2 & 0 \\end{bmatrix}\n$$\nNow we can compute the numerator of the estimator, $\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{y}$:\n$$\n\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{y} = \\begin{bmatrix} 0 & 2 & 0 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ 1.0 \\\\ 0.5 \\end{bmatrix} = 0(0.5) + 2(1.0) + 0(0.5) = 2\n$$\nNext, we compute the first term in the denominator of the estimator, $\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{A}$:\n$$\n\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1}\\mathbf{A} = (\\mathbf{A}^T\\boldsymbol{\\Sigma}_n^{-1})\\mathbf{A} = \\begin{bmatrix} 0 & 2 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix} = 0(1) + 2(2) + 0(1) = 4\n$$\nThe second term in the denominator is $\\frac{1}{\\sigma_{\\chi}^2} = \\frac{1}{1} = 1$.\nFinally, we substitute these numerical results into the expression for $\\hat{\\chi}_{\\text{MAP}}$:\n$$\n\\hat{\\chi}_{\\text{MAP}} = \\frac{2}{4 + 1} = \\frac{2}{5} = 0.4\n$$\nThe problem requires the answer to be rounded to four significant figures. Thus, the MAP estimate of $\\chi$ is $0.4000$.", "answer": "$$\\boxed{0.4000}$$", "id": "3320225"}, {"introduction": "When the weak-scattering assumption no longer holds, linear models fail and we must confront the full nonlinearity of the inverse problem. This challenge requires a shift from direct inversion formulas to iterative, optimization-based approaches, often termed Full-Waveform Inversion (FWI). This hands-on coding exercise guides you through the process of building a modern inverse scattering solver from the ground up, centered on the adjoint-state method for efficiently computing the gradient of the misfit functional. Verifying this gradient with a Taylor test is a critical skill for any developer of large-scale optimization-based inversion codes, and this practice provides a concrete example in a tangible one-dimensional setting. [@problem_id:3320249]", "problem": "You are to formulate and implement an optimization-based inverse scattering and imaging solver with an adjoint-enabled gradient in a one-dimensional setting relevant to computational electromagnetics. The derivation must begin from first principles founded on Maxwell’s equations and proceed to a scalar frequency-domain reduction. Work in non-dimensionalized units throughout; no explicit physical unit conversion is required.\n\nConsider a one-dimensional spatial domain $[0,L]$ with $L = 1$. Let $\\omega > 0$ denote the angular frequency and let $\\alpha \\ge 0$ denote a small absorption parameter. The unknown model is the non-dimensional relative permittivity profile $m(x) > 0$. Under a scalar reduction consistent with a transverse-magnetic polarization and longitudinal invariance, the time-harmonic field $u(x)$ satisfies the scalar Helmholtz-type boundary value problem\n$$\n\\frac{\\mathrm{d}^2 u}{\\mathrm{d} x^2}(x) + \\omega^2 \\left(1 + \\mathrm{i}\\,\\alpha\\right) m(x)\\, u(x) = s(x), \\quad x \\in (0,L), \\quad u(0) = 0,\\; u(L) = 0,\n$$\nwhere $s(x)$ is a source term. For computational purposes, let there be a finite set of sources indexed by $s \\in \\mathcal{S}$ at positions inside $(0,L)$, each modeled as a point load in the discrete system, and a finite set of receivers indexed by $r \\in \\mathcal{R}$ at positions inside $(0,L)$ that sample the field $u(x)$.\n\nDefine the misfit functional for a given model $m$ as\n$$\nJ(m) = \\frac{1}{2}\\sum_{s \\in \\mathcal{S}} \\left\\| R u_s(m) - d_s \\right\\|_2^2 + \\frac{\\beta}{2}\\,\\left\\| D m \\right\\|_2^2,\n$$\nwhere $u_s(m)$ is the field produced by source $s$ solving the forward problem at model $m$, $R$ is the sampling operator that maps grid fields to receiver samples, $d_s$ are fixed data vectors for each source, $\\beta \\ge 0$ is a Tikhonov regularization weight, and $D$ is the first-order forward-difference operator on the model grid.\n\nYour tasks are:\n- Derive a consistent second-order finite-difference discretization of the forward problem on $N$ interior points with grid spacing $h = L/(N+1)$ and homogeneous Dirichlet boundary conditions at $x=0$ and $x=L$. Express the discrete forward operator as a sparse linear system $A(m) u = b$, where $A(m)$ depends on $m$ and $b$ encodes the source term for a given source location.\n- From a first-principles adjoint-state derivation, obtain an expression for the gradient $\\nabla J(m)$ with respect to the model $m$, making no symmetry or reality assumptions beyond those stated, and accounting correctly for complex arithmetic and the Hermitian adjoint in the discrete setting. The derivation must start from the scalar reduction of Maxwell’s equations, then the weak form or Lagrangian constrained by the discrete forward operator, and then eliminate dependence on the field variation via an adjoint variable that satisfies an adjoint equation.\n- Implement a program that assembles the discrete forward operator $A(m)$, computes the data $d_s$ by solving the forward problem at a fixed “true” model $m_\\mathrm{true}(x)$ for each source, and then, for a fixed “initial” model $m_0(x)$ and a fixed direction $h(x)$, performs a central-difference Taylor test of the derived gradient. Specifically, for a given small scalar $\\varepsilon>0$, compute\n$$\n\\mathrm{FD}(\\varepsilon) = \\frac{J(m_0 + \\varepsilon\\,\\delta m) - J(m_0 - \\varepsilon\\,\\delta m)}{2\\,\\varepsilon}, \\quad \\mathrm{ADJ} = \\langle \\nabla J(m_0),\\, \\delta m \\rangle,\n$$\nwhere $\\delta m$ is the chosen direction function discretized on the grid, and $\\langle \\cdot,\\cdot \\rangle$ is the standard Euclidean inner product over the model grid. Report the relative error\n$$\n\\mathrm{err} = \\frac{\\left| \\mathrm{FD}(\\varepsilon) - \\mathrm{ADJ} \\right|}{\\max\\left(1,\\left| \\mathrm{ADJ} \\right|\\right)}.\n$$\n\nModeling specifics and test suite:\n- Use $L = 1$. Use $N$ interior grid points, $h = L/(N+1)$, and interior grid coordinates $x_i = i\\,h$ for $i=1,\\dots,N$.\n- Use the second-order central-difference approximation for the second derivative and impose $u(0)=u(L)=0$ via homogeneous Dirichlet boundary conditions.\n- Source discretization: for a source at position $x_s \\in (0,L)$, construct a right-hand side vector $b \\in \\mathbb{C}^N$ with a unit entry at the grid index closest to $x_s$ and zeros elsewhere.\n- Receiver sampling: for a receiver at $x_r \\in (0,L)$, the sampled data are the field entries at the closest grid index to $x_r$.\n- Let the true model be a smooth two-Gaussian profile\n$$\nm_\\mathrm{true}(x) = 1 + 0.7\\,\\exp\\!\\left(-\\frac{(x-0.65)^2}{0.08^2}\\right) + 0.5\\,\\exp\\!\\left(-\\frac{(x-0.30)^2}{0.06^2}\\right).\n$$\n- Let the initial model be the constant profile $m_0(x) \\equiv 1$.\n- Let the direction for the Taylor test be\n$$\n\\delta m(x) = \\sin(2\\pi x) + 0.1\\,\\cos(5\\pi x),\n$$\ndiscretized on the grid and normalized to unit $\\ell_2$ norm before use.\n- No noise is to be added to data. All quantities are non-dimensional.\n\nImplement your solver and gradient, and then evaluate the relative Taylor test error for each of the following test cases. For each case, construct the source and receiver index sets by rounding the given positions to the nearest grid index in $(0,L)$.\n\nTest Suite (each item lists $(N,\\;\\omega,\\;\\alpha,\\;\\beta,\\;\\varepsilon,\\;\\text{sources},\\;\\text{receivers})$):\n- Case A (happy path coverage): $(80,\\;20.0,\\;0.10,\\;0.0,\\;10^{-4},\\;\\{0.25,\\,0.50,\\,0.75\\},\\;\\{0.20,\\,0.40,\\,0.60,\\,0.80\\})$.\n- Case B (no absorption edge): $(80,\\;10.0,\\;0.00,\\;0.0,\\;5\\times 10^{-4},\\;\\{0.25,\\,0.50,\\,0.75\\},\\;\\{0.20,\\,0.40,\\,0.60,\\,0.80\\})$.\n- Case C (regularization active): $(80,\\;30.0,\\;0.05,\\;10^{-2},\\;5\\times 10^{-4},\\;\\{0.25,\\,0.50,\\,0.75\\},\\;\\{0.20,\\,0.40,\\,0.60,\\,0.80\\})$.\n- Case D (small grid edge): $(30,\\;8.0,\\;0.02,\\;10^{-3},\\;10^{-3},\\;\\{0.50\\},\\;\\{0.25,\\,0.75\\})$.\n\nFinal output format:\n- Your program should produce a single line of output containing the relative Taylor test errors for Cases A, B, C, and D, as a comma-separated list enclosed in square brackets, in the same order as the cases above, for example, $[e_A,e_B,e_C,e_D]$, where each $e_\\cdot$ is a floating-point number.", "solution": "The problem requires the formulation and implementation of an inverse scattering solver based on a gradient-derived optimization approach in a one-dimensional setting. The core tasks are to discretize the governing partial differential equation, derive the gradient of a misfit functional using the adjoint-state method, and verify the correctness of the derived gradient via a Taylor test.\n\nThe physical context is electromagnetic wave propagation. The governing equation is a one-dimensional scalar Helmholtz-type equation, which arises from Maxwell's equations under specific assumptions. For a time-harmonic field $\\mathbf{E}(\\mathbf{r},t) = \\mathbf{E}(\\mathbf{r})e^{-\\mathrm{i}\\omega t}$, Maxwell's curl equations in a source-free medium with spatially varying permittivity $\\epsilon(\\mathbf{r})$ and constant permeability $\\mu_0$ become $\\nabla \\times \\mathbf{E} = \\mathrm{i}\\omega\\mu_0 \\mathbf{H}$ and $\\nabla \\times \\mathbf{H} = -\\mathrm{i}\\omega\\epsilon(\\mathbf{r}) \\mathbf{E}$. For a transverse-magnetic (TM) polarization where the electric field has only a z-component, $\\mathbf{E} = u(x,y) \\hat{\\mathbf{z}}$, and assuming no variation in the y-direction ($\\partial/\\partial y = 0$), these equations reduce to the two-dimensional scalar Helmholtz equation $\\nabla^2 u + \\omega^2 \\mu_0 \\epsilon(x) u = 0$. The problem provides a one-dimensional analogue of this equation, incorporating a source term $s(x)$ and an absorption term $\\alpha$. The model $m(x)$ represents the non-dimensional relative permittivity profile.\n\nThe forward problem is given by the boundary value problem on the domain $x \\in [0,L]$ with $L=1$:\n$$\n\\frac{\\mathrm{d}^2 u}{\\mathrm{d} x^2}(x) + \\omega^2 \\left(1 + \\mathrm{i}\\,\\alpha\\right) m(x)\\, u(x) = s(x), \\quad u(0) = u(L) = 0.\n$$\n\nWe discretize this problem using a second-order finite-difference scheme on a uniform grid with $N$ interior points. The grid points are $x_i = i h$ for $i_g = 0, 1, \\dots, N, N+1$, where the grid spacing is $h = L/(N+1) = 1/(N+1)$. The computational grid consists of the interior points indexed by $i = 1, \\dots, N$. Let $u_i \\approx u(x_i)$ and $m_i=m(x_i)$. The second derivative is approximated as:\n$$\n\\frac{\\mathrm{d}^2 u}{\\mathrm{d} x^2}(x_i) \\approx \\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2}.\n$$\nSubstituting this into the governing equation for each interior point $i=1,\\dots,N$ yields the discrete system:\n$$\n\\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2} + \\omega^2 (1 + \\mathrm{i}\\alpha) m_i u_i = s_i.\n$$\nThe boundary conditions $u_0 = u(0) = 0$ and $u_{N+1} = u(L) = 0$ are incorporated directly. This system of $N$ linear equations can be written in matrix form as $A(m) \\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u} \\in \\mathbb{C}^N$ is the vector of field values at interior grid points, $\\mathbf{b} \\in \\mathbb{C}^N$ is the discrete source vector, and $A(m) \\in \\mathbb{C}^{N \\times N}$ is the discrete forward operator. The operator $A(m)$ is a sparse, symmetric, tridiagonal matrix defined as:\n$$\nA(m) = L_{dd} + k^2 \\mathrm{diag}(\\mathbf{m}),\n$$\nwhere $L_{dd}$ is the discrete Laplacian matrix, $k^2 = \\omega^2(1+\\mathrm{i}\\alpha)$ is the complex wavenumber squared (for a background medium $m=1$), and $\\mathbf{m}$ is the vector of model parameters $m_i$. The matrix $L_{dd}$ is given by:\n$$\nL_{dd} = \\frac{1}{h^2} \\begin{pmatrix}\n-2 & 1 & 0 & \\dots & 0 \\\\\n1 & -2 & 1 & \\dots & 0 \\\\\n0 & \\ddots & \\ddots & \\ddots & 0 \\\\\n0 & \\dots & 1 & -2 & 1 \\\\\n0 & \\dots & 0 & 1 & -2\n\\end{pmatrix}.\n$$\n\nThe misfit functional to be minimized is:\n$$\nJ(\\mathbf{m}) = \\frac{1}{2}\\sum_{s \\in \\mathcal{S}} \\left\\| R \\mathbf{u}_s(\\mathbf{m}) - \\mathbf{d}_s \\right\\|_2^2 + \\frac{\\beta}{2}\\,\\left\\| D \\mathbf{m} \\right\\|_2^2,\n$$\nwhere $\\mathbf{d}_s$ is the data vector for source $s$, $R$ is a sampling operator (a sparse matrix that selects entries of $\\mathbf{u}_s$ corresponding to receiver locations), and $D$ is the discrete first-order forward-difference operator. The term $\\frac{\\beta}{2}\\|D\\mathbf{m}\\|_2^2$ is a Tikhonov regularizer that penalizes roughness in the model.\n\nTo derive the gradient of $J(\\mathbf{m})$ with respect to the real-valued model parameters $\\mathbf{m}$, we employ the adjoint-state method. The gradient is the sum of the gradients of the misfit term and the regularization term: $\\nabla J(\\mathbf{m}) = \\nabla J_{misfit}(\\mathbf{m}) + \\nabla J_{reg}(\\mathbf{m})$.\n\nThe regularization term $J_{reg}(\\mathbf{m}) = \\frac{\\beta}{2} \\|D\\mathbf{m}\\|_2^2 = \\frac{\\beta}{2}\\mathbf{m}^T D^T D \\mathbf{m}$ is a quadratic function of $\\mathbf{m}$. Its gradient is straightforwardly computed as:\n$$\n\\nabla J_{reg}(\\mathbf{m}) = \\beta D^T D \\mathbf{m}.\n$$\n\nFor the misfit term, we consider its variation $\\delta J_{misfit}$ due to a small real perturbation $\\delta \\mathbf{m}$ in the model.\n$$\nJ_{misfit}(\\mathbf{m}) = \\frac{1}{2} \\sum_{s \\in \\mathcal{S}} (\\mathbf{r}_s)^H (\\mathbf{r}_s), \\quad \\text{where } \\mathbf{r}_s = R\\mathbf{u}_s(\\mathbf{m}) - \\mathbf{d}_s.\n$$\nTo first order, the variation is $\\delta J_{misfit} = \\mathfrak{Re} \\left( \\sum_{s \\in \\mathcal{S}} (\\mathbf{r}_s)^H R \\, \\delta \\mathbf{u}_s \\right)$. The variation of the field, $\\delta \\mathbf{u}_s$, is related to $\\delta \\mathbf{m}$ through the forward equation $A(\\mathbf{m})\\mathbf{u}_s = \\mathbf{b}_s$. Differentiating this equation gives:\n$$\n(\\delta A)\\mathbf{u}_s + A(\\mathbf{m})\\delta \\mathbf{u}_s = 0 \\implies \\delta \\mathbf{u}_s = -A(\\mathbf{m})^{-1}(\\delta A)\\mathbf{u}_s.\n$$\nThe variation in the operator, $\\delta A$, is found from its definition:\n$$\nA(\\mathbf{m} + \\delta \\mathbf{m}) = L_{dd} + k^2\\mathrm{diag}(\\mathbf{m}+\\delta \\mathbf{m}) = A(\\mathbf{m}) + k^2\\mathrm{diag}(\\delta \\mathbf{m}).\n$$\nThus, $\\delta A = k^2\\mathrm{diag}(\\delta \\mathbf{m})$. Substituting these into the expression for $\\delta J_{misfit}$:\n$$\n\\delta J_{misfit} = \\mathfrak{Re} \\left( \\sum_{s \\in \\mathcal{S}} (\\mathbf{r}_s)^H R (-A^{-1} \\delta A \\mathbf{u}_s) \\right) = - \\mathfrak{Re} \\left( \\sum_{s \\in \\mathcal{S}} (R^H \\mathbf{r}_s)^H A^{-1} \\delta A \\mathbf{u}_s \\right).\n$$\nUsing the property $(X Y)^H = Y^H X^H$, we get $(A^{-1})^H = (A^H)^{-1}$. We define the adjoint field $\\mathbf{v}_s$ for each source as the solution to the adjoint equation:\n$$\nA(\\mathbf{m})^H \\mathbf{v}_s = R^H \\mathbf{r}_s,\n$$\nwhere $A^H$ is the Hermitian (conjugate) transpose of $A$. The right-hand side, $R^H \\mathbf{r}_s$, represents the data residuals back-propagated from the receivers to the computational grid. With this definition, $\\mathbf{v}_s = (A^H)^{-1} R^H \\mathbf{r}_s$, and the variation becomes:\n$$\n\\delta J_{misfit} = - \\mathfrak{Re} \\left( \\sum_{s \\in \\mathcal{S}} \\mathbf{v}_s^H (\\delta A) \\mathbf{u}_s \\right).\n$$\nSubstituting $\\delta A = k^2\\mathrm{diag}(\\delta \\mathbf{m})$:\n$$\n\\delta J_{misfit} = - \\mathfrak{Re} \\left( \\sum_{s \\in \\mathcal{S}} \\mathbf{v}_s^H (k^2 \\mathrm{diag}(\\delta \\mathbf{m})) \\mathbf{u}_s \\right) = - \\mathfrak{Re} \\left( k^2 \\sum_{s \\in \\mathcal{S}} \\sum_{j=1}^N \\overline{v_{s,j}} \\delta m_j u_{s,j} \\right).\n$$\nSince $\\delta m_j$ are real, we can rearrange the sum:\n$$\n\\delta J_{misfit} = \\sum_{j=1}^N \\left( - \\mathfrak{Re} \\left[ k^2 \\sum_{s \\in \\mathcal{S}} u_{s,j} \\overline{v_{s,j}} \\right] \\right) \\delta m_j.\n$$\nThis expression is in the form of an inner product $\\langle \\nabla J_{misfit}, \\delta \\mathbf{m} \\rangle$. Thus, the $j$-th component of the misfit gradient is:\n$$\n(\\nabla J_{misfit}(\\mathbf{m}))_j = - \\mathfrak{Re} \\left( k^2 \\sum_{s \\in \\mathcal{S}} u_{s,j} \\overline{v_{s,j}} \\right) = -\\omega^2 \\mathfrak{Re} \\left( (1+\\mathrm{i}\\alpha) \\sum_{s \\in \\mathcal{S}} u_{s,j} \\overline{v_{s,j}} \\right).\n$$\nThe total gradient is the sum of the two components:\n$$\n\\nabla J(\\mathbf{m}) = -\\omega^2 \\mathfrak{Re} \\left( (1+\\mathrm{i}\\alpha) \\sum_{s \\in \\mathcal{S}} \\mathbf{u}_s \\odot \\overline{\\mathbf{v}_s} \\right) + \\beta D^T D \\mathbf{m},\n$$\nwhere $\\odot$ denotes the element-wise (Hadamard) product.\n\nThe algorithm to compute the gradient $\\nabla J(\\mathbf{m})$ is as follows:\n1. For each source $s \\in \\mathcal{S}$:\n    a. Solve the forward problem $A(\\mathbf{m})\\mathbf{u}_s = \\mathbf{b}_s$ to find the forward field $\\mathbf{u}_s$.\n    b. Compute the data residual $\\mathbf{r}_s = R\\mathbf{u}_s - \\mathbf{d}_s$.\n    c. Construct the adjoint source $R^H \\mathbf{r}_s$.\n    d. Solve the adjoint problem $A(\\mathbf{m})^H \\mathbf{v}_s = R^H \\mathbf{r}_s$ to find the adjoint field $\\mathbf{v}_s$.\n2. Sum the contributions from all sources to compute the misfit gradient using the derived formula.\n3. Compute the regularization gradient $\\beta D^T D \\mathbf{m}$.\n4. Sum the misfit and regularization gradients to obtain the total gradient $\\nabla J(\\mathbf{m})$.\n\nThe Taylor test verifies this gradient. The directional derivative of $J$ at $\\mathbf{m}_0$ in the direction $\\delta \\mathbf{m}$ is $\\langle \\nabla J(\\mathbf{m}_0), \\delta \\mathbf{m} \\rangle$. This is approximated by a central finite difference:\n$$\n\\langle \\nabla J(\\mathbf{m}_0), \\delta \\mathbf{m} \\rangle \\approx \\frac{J(\\mathbf{m}_0 + \\varepsilon \\delta \\mathbf{m}) - J(\\mathbf{m}_0 - \\varepsilon \\delta \\mathbf{m})}{2\\varepsilon}.\n$$\nThe relative error between the adjoint-based calculation ($\\mathrm{ADJ}$) and the finite-difference approximation ($\\mathrm{FD}(\\varepsilon)$) should be small, typically on the order of $\\varepsilon^2$, confirming the correctness of the derived gradient.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Formulates and implements an optimization-based inverse scattering solver,\n    and performs a Taylor test to verify the adjoint-state gradient.\n    \"\"\"\n    \n    test_cases = [\n        # (N, omega, alpha, beta, epsilon, sources, receivers)\n        (80, 20.0, 0.10, 0.0, 1e-4, [0.25, 0.50, 0.75], [0.20, 0.40, 0.60, 0.80]), # Case A\n        (80, 10.0, 0.00, 0.0, 5e-4, [0.25, 0.50, 0.75], [0.20, 0.40, 0.60, 0.80]), # Case B\n        (80, 30.0, 0.05, 1e-2, 5e-4, [0.25, 0.50, 0.75], [0.20, 0.40, 0.60, 0.80]), # Case C\n        (30, 8.0,  0.02, 1e-3, 1e-3, [0.50], [0.25, 0.75]),                         # Case D\n    ]\n\n    results = []\n\n    for case in test_cases:\n        N, omega, alpha, beta, epsilon, src_pos, rec_pos = case\n        \n        # 1. Setup Grid and Models\n        L = 1.0\n        h = L / (N + 1)\n        x = h * np.arange(1, N + 1)\n\n        # Discretize models\n        m_true = 1.0 + 0.7 * np.exp(-(x - 0.65)**2 / 0.08**2) + \\\n                 0.5 * np.exp(-(x - 0.30)**2 / 0.06**2)\n        m0 = np.ones(N)\n        \n        delta_m = np.sin(2 * np.pi * x) + 0.1 * np.cos(5 * np.pi * x)\n        delta_m /= np.linalg.norm(delta_m)\n\n        # Get discrete indices for sources and receivers\n        src_indices = [int(round(p / h)) - 1 for p in src_pos]\n        rec_indices = [int(round(p / h)) - 1 for p in rec_pos]\n\n        # 2. Define Operators\n        def get_forward_operator(m, N_op, h_op, omega_op, alpha_op):\n            k2 = omega_op**2 * (1 + 1j * alpha_op)\n            diagonals = [np.ones(N_op), -2 * np.ones(N_op), np.ones(N_op)]\n            lap = sparse.spdiags(diagonals, [-1, 0, 1], N_op, N_op, format='csc') / h_op**2\n            mass_term = sparse.spdiags([k2 * m], [0], N_op, N_op, format='csc')\n            return lap + mass_term\n\n        # Regularization operator D\n        D = sparse.spdiags([-np.ones(N), np.ones(N)], [0, 1], N - 1, N, format='csr')\n        DTD = D.T @ D\n        \n        # 3. Generate Synthetic Data\n        A_true = get_forward_operator(m_true, N, h, omega, alpha)\n        data = {}\n        for i, s_idx in enumerate(src_indices):\n            b = np.zeros(N, dtype=complex)\n            b[s_idx] = 1.0\n            u_true = spsolve(A_true, b)\n            data[i] = u_true[rec_indices]\n\n        # 4. Define Objective Functional J(m)\n        def calculate_J(m, N_J, h_J, omega_J, alpha_J, beta_J, src_indices_J, rec_indices_J, data_J, D_J):\n            A = get_forward_operator(m, N_J, h_J, omega_J, alpha_J)\n            misfit = 0.0\n            for i, s_idx in enumerate(src_indices_J):\n                b = np.zeros(N_J, dtype=complex)\n                b[s_idx] = 1.0\n                u = spsolve(A, b)\n                res = u[rec_indices_J] - data_J[i]\n                misfit += 0.5 * np.linalg.norm(res)**2\n            \n            reg = 0.0\n            if beta_J > 0:\n                reg = 0.5 * beta_J * np.linalg.norm(D_J @ m)**2\n            \n            return misfit + reg\n\n        # 5. Calculate ADJ = <nabla J(m0), delta_m>\n        # (a) Solve forward and adjoint problems at m0\n        A0 = get_forward_operator(m0, N, h, omega, alpha)\n        A0H = A0.conj().T\n        \n        U = np.zeros((len(src_indices), N), dtype=complex)\n        V = np.zeros((len(src_indices), N), dtype=complex)\n\n        for i, s_idx in enumerate(src_indices):\n            b = np.zeros(N, dtype=complex)\n            b[s_idx] = 1.0\n            u0 = spsolve(A0, b)\n            U[i, :] = u0\n            \n            residual = u0[rec_indices] - data[i]\n            adjoint_source = np.zeros(N, dtype=complex)\n            adjoint_source[rec_indices] = residual\n            \n            v0 = spsolve(A0H, adjoint_source)\n            V[i, :] = v0\n\n        # (b) Compute gradient\n        k2 = omega**2 * (1 + 1j * alpha)\n        # Element-wise product and sum over sources\n        grad_misfit = -np.real(k2 * np.sum(U * np.conj(V), axis=0))\n        \n        grad_reg = np.zeros(N)\n        if beta > 0:\n            grad_reg = beta * (DTD @ m0)\n            \n        grad_J = grad_misfit + grad_reg\n        \n        adj = np.dot(grad_J, delta_m)\n\n        # 6. Calculate FD\n        J_plus = calculate_J(m0 + epsilon * delta_m, N, h, omega, alpha, beta, src_indices, rec_indices, data, D)\n        J_minus = calculate_J(m0 - epsilon * delta_m, N, h, omega, alpha, beta, src_indices, rec_indices, data, D)\n        \n        fd = (J_plus - J_minus) / (2 * epsilon)\n        \n        # 7. Calculate Relative Error\n        rel_error = np.abs(fd - adj) / max(1.0, np.abs(adj))\n        results.append(rel_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3320249"}]}