{"hands_on_practices": [{"introduction": "The foundation of many inverse scattering methods, including Contrast Source Inversion, lies in iterative, gradient-based optimization. The efficiency of these methods hinges on the adjoint-state technique, which allows for the computation of the objective function's gradient at a cost comparable to a single forward solve. This practice provides a crucial hands-on verification of this cornerstone concept, guiding you to numerically confirm that your adjoint-derived gradient accurately reflects the local change in the data misfit functional [@problem_id:3295894].", "problem": "Consider a one-dimensional frequency-domain scattering problem in a homogeneous background, discretized as a Contrast Source Inversion (CSI) setting. Let the total field $E(x)$ satisfy the Lippmann–Schwinger integral equation on a bounded interval $\\Omega = [0,L]$,\n$$\nE(x) \\;=\\; E_{\\mathrm{inc}}(x) \\;+\\; \\int_{\\Omega} G(x,x')\\, k_0^2\\, \\chi(x')\\, E(x')\\, dx',\n$$\nwhere $E_{\\mathrm{inc}}(x)$ is a given incident field, $\\chi(x)$ is the unknown (real-valued) contrast, $k_0$ is the background wavenumber, and $G(x,x')$ is the one-dimensional free-space Green’s function for the scalar Helmholtz equation,\n$$\nG(x,x') \\;=\\; \\frac{i}{2 k_0}\\, e^{i k_0 |x - x'|}.\n$$\nWe work in dimensionless units throughout.\n\nDiscretize $\\Omega$ with $N$ equispaced nodes $x_n$ with spacing $\\Delta x = L/(N-1)$. Denote $E \\in \\mathbb{C}^N$ as the vector of total fields $E_n \\approx E(x_n)$, and $\\chi \\in \\mathbb{R}^N$ as the vector of contrast samples $\\chi_n \\approx \\chi(x_n)$. Define the dense matrix $G \\in \\mathbb{C}^{N \\times N}$ with entries\n$$\nG_{mn} \\;=\\; \\Delta x \\,\\frac{i}{2 k_0}\\, e^{i k_0 |x_m - x_n|},\n$$\nand the diagonal matrix $V(\\chi) = k_0^2 \\,\\mathrm{diag}(\\chi) \\in \\mathbb{R}^{N \\times N}$. The discrete total field $E(\\chi)$ solves the linear system\n$$\nA(\\chi)\\, E(\\chi) \\;=\\; E_{\\mathrm{inc}}, \n\\qquad A(\\chi) \\;=\\; I \\;-\\; G\\,V(\\chi),\n$$\nwith $I$ the identity matrix and $E_{\\mathrm{inc}} \\in \\mathbb{C}^N$ the incident field sampled at $\\{x_n\\}$.\n\nLet $P \\in \\{0,1\\}^{M \\times N}$ be a selection operator that extracts $M$ interior samples of the total field. Given synthetic data $d \\in \\mathbb{C}^M$ produced by a chosen ground-truth contrast $\\chi_{\\mathrm{true}}$, define the data-misfit objective\n$$\nJ(\\chi) \\;=\\; \\frac{1}{2}\\, \\| P\\,E(\\chi) \\;-\\; d \\|_2^2.\n$$\nBy the adjoint-state method, introduce $\\lambda \\in \\mathbb{C}^N$ solving the adjoint linear system\n$$\nA(\\chi)^{H}\\, \\lambda \\;=\\; P^{H}\\,\\big(P\\,E(\\chi) \\;-\\; d\\big),\n$$\nwhere ${}^{H}$ denotes the conjugate transpose. The gradient of $J$ with respect to $\\chi$ is the real vector $g(\\chi) \\in \\mathbb{R}^N$ with components\n$$\ng_n(\\chi) \\;=\\; k_0^2\\, \\mathrm{Re}\\!\\left\\{ \\overline{\\left(G^{H}\\lambda\\right)_n}\\; E_n(\\chi) \\right\\}, \n\\qquad n = 1,\\dots,N,\n$$\nwhere the overline denotes complex conjugation. This gradient provides the first-order directional derivative of $J$ with respect to real-valued perturbations $\\delta\\chi$ via\n$$\n\\delta J \\;=\\; \\sum_{n=1}^{N} g_n(\\chi)\\, \\delta\\chi_n \\;+\\; \\mathcal{O}(\\|\\delta\\chi\\|^2).\n$$\n\nYour task is to write a complete program that:\n- Constructs the forward model as above for a specific test configuration.\n- Computes the adjoint-based gradient $g(\\chi)$.\n- Verifies numerically that this adjoint-based gradient matches finite-difference approximations of $J$ to first order in $\\delta\\chi$.\n\nUse the following test configuration and test suite, all in dimensionless units:\n- Domain length: $L = 1$.\n- Number of grid points: $N = 80$.\n- Wavenumber: $k_0 = 20\\pi$.\n- Incident field: $E_{\\mathrm{inc}}(x) = e^{i k_0 x}$ sampled at $\\{x_n\\}$.\n- Ground-truth contrast: a Gaussian bump centered at $x=L/2$,\n  $$\n  \\chi_{\\mathrm{true}}(x) \\;=\\; 0.5 \\,\\exp\\!\\left( -\\frac{(x - L/2)^2}{2\\sigma^2} \\right),\n  \\quad \\text{with } \\sigma = 0.15\\,L.\n  $$\n- Data: $d = P\\,E(\\chi_{\\mathrm{true}})$ with the same forward model and selection operator $P$.\n- Selection operator $P$: choose $M = 16$ equispaced interior grid indices (exclude the first and last four nodes at each boundary to avoid edge effects), and let $P$ extract those samples of $E$.\n- Test background contrasts:\n  - For Tests $1$ and $3$, use \n    $$\n    \\chi_0(x) \\;=\\; 0.1\\, \\sin\\!\\left( \\frac{2\\pi x}{L} \\right).\n    $$\n  - For Test $2$, use $\\chi_0(x) \\equiv 0$.\n- Direction vectors:\n  - For Test $1$, use a single-pixel perturbation, i.e., $\\delta\\chi = \\mathbf{e}_p$ with $p = \\lfloor N/3 \\rfloor$.\n  - For Test $2$, and Test $3$, use a fixed pseudorandom real vector $\\delta\\chi$ with unit $\\ell_2$-norm (seeded for reproducibility), supported on all $N$ entries.\n- Finite-difference step sizes: $\\varepsilon_1 = 1 \\times 10^{-3}$ and $\\varepsilon_2 = 5 \\times 10^{-4}$.\n\nDefine the following three tests and return booleans indicating whether the expected first-order consistency holds:\n- Test $1$ (forward-difference first-order scaling): With $\\chi_0$ as above and $\\delta\\chi = \\mathbf{e}_p$, define\n  $$\n  \\mathrm{FD}(\\varepsilon) \\;=\\; \\frac{J(\\chi_0 + \\varepsilon\\,\\delta\\chi) \\;-\\; J(\\chi_0)}{\\varepsilon}, \n  \\qquad \\mathrm{AD} \\;=\\; g(\\chi_0)^{T} \\delta\\chi,\n  $$\n  and the mismatch $R(\\varepsilon) = \\big| \\mathrm{FD}(\\varepsilon) - \\mathrm{AD} \\big|$. Verify that $R(\\varepsilon_2)/R(\\varepsilon_1)$ lies in $[0.3,\\,0.7]$.\n- Test $2$ (central-difference consistency at zero contrast): With $\\chi_0 \\equiv 0$ and a unit-norm random $\\delta\\chi$, define\n  $$\n  \\mathrm{CD}(\\varepsilon) \\;=\\; \\frac{J(\\chi_0 + \\varepsilon\\,\\delta\\chi) \\;-\\; J(\\chi_0 - \\varepsilon\\,\\delta\\chi)}{2\\varepsilon},\n  \\qquad \\mathrm{AD} \\;=\\; g(\\chi_0)^{T} \\delta\\chi,\n  $$\n  and the mismatch $C(\\varepsilon) = \\big| \\mathrm{CD}(\\varepsilon) - \\mathrm{AD} \\big|$. Verify $C(10^{-4}) \\le 10^{-6}$.\n- Test $3$ (central-difference second-order scaling): With $\\chi_0$ as in Test $1$ and a unit-norm random $\\delta\\chi$, verify that the ratio $C(\\varepsilon_2)/C(\\varepsilon_1)$ lies in $[0.2,\\,0.3]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each result must be a boolean. No other output is permitted.", "solution": "The user requests a numerical verification of the adjoint-state gradient for a one-dimensional contrast source inversion problem. The problem is well-posed, scientifically grounded, and provides all necessary information for a complete solution. The verification procedure, often called a Taylor test or gradient check, is a standard and essential step in the development of optimization-based inversion algorithms. It ensures that the analytically derived gradient correctly represents the local first-order behavior of the objective function.\n\nThe core of the problem lies in the Lippmann-Schwinger integral equation, which describes the total electric field $E(x)$ as the sum of an incident field $E_{\\mathrm{inc}}(x)$ and a scattered field generated by the contrast object $\\chi(x)$:\n$$\nE(x) \\;=\\; E_{\\mathrm{inc}}(x) \\;+\\; k_0^2 \\int_{\\Omega} G(x,x')\\, \\chi(x')\\, E(x')\\, dx'\n$$\nUpon discretization on a grid of $N$ points $\\{x_n\\}$, this equation transforms into a linear system for the vector of field samples $E \\in \\mathbb{C}^N$:\n$$\nA(\\chi)\\, E(\\chi) \\;=\\; E_{\\mathrm{inc}}\n$$\nwhere $A(\\chi) = I - G V(\\chi)$. Here, $I \\in \\mathbb{R}^{N \\times N}$ is the identity matrix, $G \\in \\mathbb{C}^{N \\times N}$ is the discretized Green's function operator, and $V(\\chi) \\in \\mathbb{R}^{N \\times N}$ is a diagonal matrix containing the contrast values $\\chi_n \\approx \\chi(x_n)$, scaled by $k_0^2$. Solving this system for $E(\\chi)$ constitutes the forward model.\n\nThe goal of the inversion is to find a contrast $\\chi$ that minimizes the data-misfit objective function $J(\\chi)$. This function quantifies the squared $\\ell_2$-norm difference between the measured data $d \\in \\mathbb{C}^M$ and the corresponding samples of the computed field $E(\\chi)$:\n$$\nJ(\\chi) \\;=\\; \\frac{1}{2}\\, \\| P\\,E(\\chi) \\;-\\; d \\|_2^2\n$$\nThe matrix $P \\in \\{0,1\\}^{M \\times N}$ is a selection operator that extracts the field values at the measurement locations.\n\nTo use gradient-based optimization methods, we need the gradient of $J$ with respect to $\\chi$. A direct computation would be prohibitively expensive, as it would require solving the forward problem once for each component of $\\chi$. The adjoint-state method provides an efficient alternative. By introducing an adjoint field $\\lambda \\in \\mathbb{C}^N$, which is the solution to the adjoint linear system\n$$\nA(\\chi)^{H}\\, \\lambda \\;=\\; P^{H}\\,\\big(P\\,E(\\chi) \\;-\\; d\\big),\n$$\nthe gradient vector $g(\\chi) \\in \\mathbb{R}^N$ can be computed with just one additional linear system solve. The components of the gradient are given by:\n$$\ng_n(\\chi) \\;=\\; k_0^2\\, \\mathrm{Re}\\!\\left\\{ \\overline{\\left(G^{H}\\lambda\\right)_n}\\; E_n(\\chi) \\right\\}\n$$\nThis gradient gives the directional derivative of $J$ in any direction $\\delta\\chi$ as $g(\\chi)^T \\delta\\chi$.\n\nTo verify the correctness of this adjoint-based gradient $g(\\chi)$, we compare its prediction of the change in $J$ with finite-difference approximations. The Taylor series expansion of the objective function around a point $\\chi_0$ in a direction $\\delta\\chi$ is:\n$$\nJ(\\chi_0 + \\varepsilon\\,\\delta\\chi) = J(\\chi_0) + \\varepsilon\\, g(\\chi_0)^T \\delta\\chi + \\mathcal{O}(\\varepsilon^2)\n$$\nFrom this, we can derive the forward-difference approximation for the directional derivative:\n$$\n\\frac{J(\\chi_0 + \\varepsilon\\,\\delta\\chi) - J(\\chi_0)}{\\varepsilon} = g(\\chi_0)^T \\delta\\chi + \\mathcal{O}(\\varepsilon)\n$$\nThe error in this approximation is first-order in $\\varepsilon$. Test $1$ verifies this by checking that halving the step size $\\varepsilon$ approximately halves the error, i.e., the ratio of errors is close to $0.5$.\n\nA more accurate approximation is the central difference:\n$$\n\\frac{J(\\chi_0 + \\varepsilon\\,\\delta\\chi) - J(\\chi_0 - \\varepsilon\\,\\delta\\chi)}{2\\varepsilon} = g(\\chi_0)^T \\delta\\chi + \\mathcal{O}(\\varepsilon^2)\n$$\nThe error for the central-difference approximation is second-order in $\\varepsilon$. Test $2$ verifies the accuracy of the gradient calculation by checking that the central-difference approximation is very close to the adjoint-based directional derivative for a small $\\varepsilon$. Test $3$ verifies the second-order convergence by checking that halving $\\varepsilon$ reduces the error by a factor of approximately $(\\frac{1}{2})^2 = 0.25$.\n\nThe implementation will proceed as follows:\n1.  Set up the computational grid and define physical constants ($L$, $N$, $k_0$).\n2.  Construct the discrete Green's matrix $G$ and the incident field vector $E_{\\mathrm{inc}}$.\n3.  Define the true contrast profile $\\chi_{\\mathrm{true}}$ and generate the synthetic data $d$ by solving the forward problem for $\\chi_{\\mathrm{true}}$ and selecting the required samples.\n4.  Implement functions to:\n    a. Solve the forward problem $A(\\chi)E=E_{\\mathrm{inc}}$ to obtain $E(\\chi)$.\n    b. Calculate the objective function $J(\\chi)$.\n    c. Solve the adjoint problem $A(\\chi)^H \\lambda = \\text{rhs}$ and subsequently calculate the gradient $g(\\chi)$.\n5.  Execute the three specified tests by comparing finite-difference approximations with the adjoint-based directional derivative $g(\\chi_0)^T \\delta\\chi$ for the given test contrasts $\\chi_0$ and perturbation directions $\\delta\\chi$.\n6.  Return the boolean outcomes of the three tests.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the contrast source inversion gradient verification problem.\n    \"\"\"\n    # 1. Define problem parameters and discretize the domain\n    L = 1.0\n    N = 80\n    k0 = 20.0 * np.pi\n    M = 16\n    \n    # Grid\n    x = np.linspace(0, L, N)\n    dx = L / (N - 1)\n    \n    # Incident field\n    E_inc = np.exp(1j * k0 * x)\n    \n    # Green's function matrix\n    x_m, x_n = np.meshgrid(x, x, indexing='ij')\n    G_mat = dx * (1j / (2 * k0)) * np.exp(1j * k0 * np.abs(x_m - x_n))\n\n    # Selection operator (indices)\n    # Exclude first and last 4 nodes. Select from indices 4 to N-5=75.\n    P_indices = np.round(np.linspace(4, N - 5, M)).astype(int)\n\n    # 2. Generate synthetic data from ground-truth contrast\n    sigma = 0.15 * L\n    chi_true = 0.5 * np.exp(-(x - L/2)**2 / (2 * sigma**2))\n\n    # Helper functions that capture the simulation context (G_mat, k0, E_inc, etc.)\n    def get_total_field(chi: np.ndarray) -> np.ndarray:\n        \"\"\"Solves the forward problem for the total field E(chi).\"\"\"\n        I = np.eye(N, dtype=np.complex128)\n        # V(chi) is diagonal, so G*V(chi) is G_mn * k0^2 * chi_n\n        # This is equivalent to scaling columns of G\n        A_chi = I - G_mat * (k0**2 * chi[None, :])\n        E = np.linalg.solve(A_chi, E_inc)\n        return E\n\n    E_true = get_total_field(chi_true)\n    d = E_true[P_indices]\n\n    # 3. Implement core computational functions\n    def get_objective(chi: np.ndarray) -> float:\n        \"\"\"Computes the data misfit objective J(chi).\"\"\"\n        E_chi = get_total_field(chi)\n        residual = E_chi[P_indices] - d\n        return 0.5 * np.linalg.norm(residual)**2\n\n    def get_gradient(chi: np.ndarray) -> np.ndarray:\n        \"\"\"Computes the adjoint-based gradient g(chi).\"\"\"\n        E_chi = get_total_field(chi)\n        \n        # Adjoint RHS: P^H * (P*E(chi) - d)\n        adj_rhs = np.zeros(N, dtype=np.complex128)\n        residual = E_chi[P_indices] - d\n        adj_rhs[P_indices] = residual\n        \n        # Adjoint matrix: A^H = I - V(chi) * G^H\n        # V is real diagonal, so V^H = V\n        # Product V*G^H corresponds to scaling rows of G^H\n        A_chi_H = np.eye(N, dtype=np.complex128) - (k0**2 * chi[:, None]) * G_mat.conj().T\n        \n        lambda_vec = np.linalg.solve(A_chi_H, adj_rhs)\n        \n        # Gradient formula\n        GH_lambda = G_mat.conj().T @ lambda_vec\n        g = k0**2 * np.real(np.conj(GH_lambda) * E_chi)\n        return g\n\n    results = []\n    \n    # 4. Define test configurations\n    chi0_test1_3 = 0.1 * np.sin(2 * np.pi * x / L)\n    chi0_test2 = np.zeros(N)\n\n    p = N // 3\n    delta_chi_test1 = np.zeros(N)\n    delta_chi_test1[p] = 1.0\n\n    np.random.seed(0)\n    delta_chi_test2_3 = np.random.randn(N)\n    delta_chi_test2_3 /= np.linalg.norm(delta_chi_test2_3)\n\n    eps1 = 1e-3\n    eps2 = 5e-4\n\n    # 5. Execute tests\n    # --- Test 1: Forward-difference first-order scaling ---\n    g0_t1 = get_gradient(chi0_test1_3)\n    AD_t1 = g0_t1 @ delta_chi_test1\n    J0_t1 = get_objective(chi0_test1_3)\n    \n    J_p_eps1 = get_objective(chi0_test1_3 + eps1 * delta_chi_test1)\n    FD1 = (J_p_eps1 - J0_t1) / eps1\n    R1 = np.abs(FD1 - AD_t1)\n\n    J_p_eps2 = get_objective(chi0_test1_3 + eps2 * delta_chi_test1)\n    FD2 = (J_p_eps2 - J0_t1) / eps2\n    R2 = np.abs(FD2 - AD_t1)\n\n    ratio1 = R2 / R1 if R1 != 0 else 0\n    results.append(0.3 <= ratio1 <= 0.7)\n\n    # --- Test 2: Central-difference consistency at zero contrast ---\n    g0_t2 = get_gradient(chi0_test2)\n    AD_t2 = g0_t2 @ delta_chi_test2_3\n    eps_t2 = 1e-4\n    \n    J_p = get_objective(chi0_test2 + eps_t2 * delta_chi_test2_3)\n    J_m = get_objective(chi0_test2 - eps_t2 * delta_chi_test2_3)\n    CD_t2 = (J_p - J_m) / (2 * eps_t2)\n    C_eps = np.abs(CD_t2 - AD_t2)\n    \n    results.append(C_eps <= 1e-6)\n    \n    # --- Test 3: Central-difference second-order scaling ---\n    g0_t3 = get_gradient(chi0_test1_3) # Same chi0 as Test 1\n    AD_t3 = g0_t3 @ delta_chi_test2_3 # Same random delta_chi as Test 2\n    \n    Jp1 = get_objective(chi0_test1_3 + eps1 * delta_chi_test2_3)\n    Jm1 = get_objective(chi0_test1_3 - eps1 * delta_chi_test2_3)\n    CD1 = (Jp1 - Jm1) / (2 * eps1)\n    C1 = np.abs(CD1 - AD_t3)\n    \n    Jp2 = get_objective(chi0_test1_3 + eps2 * delta_chi_test2_3)\n    Jm2 = get_objective(chi0_test1_3 - eps2 * delta_chi_test2_3)\n    CD2 = (Jp2 - Jm2) / (2 * eps2)\n    C2 = np.abs(CD2 - AD_t3)\n\n    ratio3 = C2 / C1 if C1 != 0 else 0\n    results.append(0.2 <= ratio3 <= 0.3)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3295894"}, {"introduction": "A successful inversion algorithm produces not just a mathematically plausible result, but one that is physically meaningful. This exercise bridges the gap between the numerical output of a CSI reconstruction and the fundamental physical principle of passivity, which dictates that a material cannot be a net source of energy. You will derive a diagnostic from first principles—Poynting's theorem—to check if a reconstructed susceptibility $\\chi$ corresponds to a passive material, a critical step in validating inversion results [@problem_id:3295880].", "problem": "Consider a time-harmonic electromagnetic scattering setting under the time dependence convention $e^{-i \\omega t}$ in which a computational reconstruction produces an estimate of a scalar, isotropic electric susceptibility field $\\chi(\\mathbf{r})$ on a discrete grid. In a common formulation of contrast source inversion, the constitutive relation is $ \\mathbf{D}(\\mathbf{r}) = \\varepsilon_b \\left( 1 + \\chi(\\mathbf{r}) \\right) \\mathbf{E}(\\mathbf{r}) $, where $\\varepsilon_b$ is the real, positive background permittivity and $\\mathbf{E}(\\mathbf{r})$ is the complex phasor of the electric field. A material is passive if and only if it does not generate net electromagnetic energy. Under the above time convention, passivity implies a local material inequality $ \\operatorname{Im}\\{\\chi(\\mathbf{r})\\} \\ge 0 $ when the background is lossless, and a global dissipated power nonnegativity: the time-averaged total dissipated power in any finite region must be nonnegative.\n\nStarting from fundamental electromagnetic principles, including Maxwell’s equations and the Poynting theorem, derive a diagnostic that, given discrete samples $\\{\\chi_n\\}_{n=1}^N$ and $\\{E_n\\}_{n=1}^N$ on a grid of $N$ cells with volume $\\{\\Delta V_n\\}_{n=1}^N$, checks:\n- The local passivity condition $ \\operatorname{Im}\\{\\chi_n\\} \\ge 0 $ for all $n$.\n- The global dissipated power nonnegativity condition $ P_{\\mathrm{diss}} \\ge 0 $, where $ P_{\\mathrm{diss}} $ is the discrete approximation of the time-averaged dissipated power over the grid.\n\nYour derivation must begin from a valid base for computational electromagnetics: Maxwell’s equations and the Poynting theorem. Do not assume any shortcut formulas; derive the discrete diagnostic from those principles. Clearly state any assumptions that justify the diagnostic.\n\nThen, implement a program that applies this diagnostic to a set of synthetic “noisy reconstructions” $\\{\\widehat{\\chi}_n\\}$ obtained by perturbing a known ground-truth $\\{\\chi^{\\mathrm{true}}_n\\}$ with independent, identically distributed complex Gaussian noise $\\eta_n \\sim \\mathcal{N}(0,\\sigma^2) + i\\,\\mathcal{N}(0,\\sigma^2)$, i.e., $ \\widehat{\\chi}_n = \\chi^{\\mathrm{true}}_n + \\eta_n $. The diagnostic must flag violations of local passivity and of global dissipated power nonnegativity.\n\nAll physical quantities must be expressed in the International System of Units (SI). Frequencies must be given in hertz (for $f$) or radians per second (for $\\omega$). Phases must be specified in radians. Power must be reported in watts, rounded to $12$ decimal places.\n\nUse the following constants and assumptions:\n- Background relative permittivity $ \\varepsilon_{r,b} = 2.25 $, vacuum permittivity $ \\varepsilon_0 = 8.854187817 \\times 10^{-12} \\,\\mathrm{F/m} $, and background permittivity $ \\varepsilon_b = \\varepsilon_0 \\varepsilon_{r,b} $.\n- Operating frequency $ f = 3.0 \\times 10^{9} \\,\\mathrm{Hz} $, angular frequency $ \\omega = 2\\pi f $.\n- Uniform cell volume $ \\Delta V_n = 1.0 \\times 10^{-6} \\,\\mathrm{m}^3 $ for all $n$.\n\nYour program must implement the diagnostic and evaluate the following test suite. For each test case, construct $E_n$ as a scalar complex field using magnitudes and phases: $ E_n = |E_n| e^{i \\phi_n} $.\n\nTest suite:\n- Case A (nominal, low noise, typical field variation):\n  - Grid size $ N = 5 $.\n  - Ground-truth susceptibility real parts $ \\operatorname{Re}\\{\\chi^{\\mathrm{true}}_n\\} $: $ [0.10,\\,0.05,\\,0.00,\\,0.02,\\,0.08] $.\n  - Ground-truth susceptibility imaginary parts $ \\operatorname{Im}\\{\\chi^{\\mathrm{true}}_n\\} $: $ [0.020,\\,0.010,\\,0.030,\\,0.005,\\,0.015] $.\n  - Field magnitudes $ |E_n| $: $ [1.0,\\,2.0,\\,1.5,\\,0.0,\\,0.5] $.\n  - Field phases $ \\phi_n $ in radians: $ [0.0,\\,\\pi/4,\\,-\\pi/2,\\,0.0,\\,\\pi] $.\n  - Noise standard deviation $ \\sigma = 0.002 $.\n  - Pseudorandom seed $ s = 12345 $.\n- Case B (higher noise, varied field, likely violations):\n  - Grid size $ N = 5 $.\n  - Ground-truth susceptibility real parts $ \\operatorname{Re}\\{\\chi^{\\mathrm{true}}_n\\} $: $ [0.10,\\,0.05,\\,0.00,\\,0.02,\\,0.08] $.\n  - Ground-truth susceptibility imaginary parts $ \\operatorname{Im}\\{\\chi^{\\mathrm{true}}_n\\} $: $ [0.020,\\,0.010,\\,0.030,\\,0.005,\\,0.015] $.\n  - Field magnitudes $ |E_n| $: $ [0.3,\\,0.7,\\,1.2,\\,2.0,\\,1.0] $.\n  - Field phases $ \\phi_n $ in radians: $ [\\pi/3,\\,-\\pi/6,\\,\\pi/2,\\,\\pi/8,\\,-\\pi/5] $.\n  - Noise standard deviation $ \\sigma = 0.030 $.\n  - Pseudorandom seed $ s = 2025 $.\n- Case C (edge case with zero fields, moderate noise):\n  - Grid size $ N = 4 $.\n  - Ground-truth susceptibility real parts $ \\operatorname{Re}\\{\\chi^{\\mathrm{true}}_n\\} $: $ [0.0,\\,0.10,\\,-0.05,\\,0.20] $.\n  - Ground-truth susceptibility imaginary parts $ \\operatorname{Im}\\{\\chi^{\\mathrm{true}}_n\\} $: $ [0.0,\\,0.0,\\,0.0,\\,0.0] $.\n  - Field magnitudes $ |E_n| $: $ [0.0,\\,0.0,\\,0.0,\\,0.0] $.\n  - Field phases $ \\phi_n $ in radians: $ [0.0,\\,0.0,\\,0.0,\\,0.0] $.\n  - Noise standard deviation $ \\sigma = 0.010 $.\n  - Pseudorandom seed $ s = 7 $.\n\nFor each case, the program must:\n- Generate $ \\widehat{\\chi}_n $ from $ \\chi^{\\mathrm{true}}_n $ and the specified noise.\n- Compute booleans indicating whether all $ \\operatorname{Im}\\{\\widehat{\\chi}_n\\} \\ge 0 $ and whether the total dissipated power $ P_{\\mathrm{diss}} $ is nonnegative.\n- Count the number of indices $n$ for which $ \\operatorname{Im}\\{\\widehat{\\chi}_n\\} < 0 $.\n- Compute the total dissipated power $ P_{\\mathrm{diss}} $ in watts, rounded to $12$ decimal places.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing a Python-style list with one entry per test case. Each entry must itself be a list of the form $[\\text{all\\_imag\\_nonneg}, \\text{total\\_power\\_nonneg}, \\text{num\\_negative\\_pixels}, \\text{P\\_diss\\_W}]$, where the first two entries are booleans, the third is an integer, and the fourth is a float rounded to $12$ decimal places. For example, the printed output should look like $[[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]]$ on a single line with no extra text.", "solution": "The problem requires the derivation and implementation of a diagnostic to verify the physical passivity of a reconstructed electric susceptibility field $\\chi(\\mathbf{r})$ on a discrete grid. The derivation must originate from Maxwell's equations and the Poynting theorem.\n\n### Derivation of the Passivity Diagnostic\n\nWe begin with the macroscopic Maxwell's equations for time-harmonic fields, assuming a time-dependence of $e^{-i\\omega t}$, in a region with no free magnetic charges and no external source currents:\n$$ \\nabla \\times \\mathbf{E} = i\\omega \\mathbf{B} \\quad (1) $$\n$$ \\nabla \\times \\mathbf{H} = -i\\omega \\mathbf{D} \\quad (2) $$\nHere, $\\mathbf{E}$ and $\\mathbf{H}$ are the complex phasors for the electric and magnetic fields, respectively, while $\\mathbf{D}$ and $\\mathbf{B}$ are the corresponding flux densities. The angular frequency is $\\omega$.\n\nThe problem specifies a non-magnetic medium, so the magnetic constitutive relation is $\\mathbf{B} = \\mu_0 \\mathbf{H}$, where $\\mu_0$ is the permeability of free space. The electric constitutive relation is given as $\\mathbf{D}(\\mathbf{r}) = \\varepsilon_b (1 + \\chi(\\mathbf{r})) \\mathbf{E}(\\mathbf{r})$, where $\\varepsilon_b$ is the real, positive permittivity of a lossless background medium and $\\chi(\\mathbf{r})$ is the scalar electric susceptibility contrast.\n\nWe can express the electric displacement field $\\mathbf{D}$ as the sum of the background displacement and the induced electric polarization $\\mathbf{P}$:\n$$ \\mathbf{D} = \\varepsilon_b \\mathbf{E} + \\varepsilon_b \\chi \\mathbf{E} = \\varepsilon_b \\mathbf{E} + \\mathbf{P} $$\nThus, the polarization vector is $\\mathbf{P} = \\varepsilon_b \\chi \\mathbf{E}$. The polarization gives rise to an equivalent volume current density, $\\mathbf{J}_{eq}$, which is the time derivative of the polarization in the time domain. In the frequency domain, this corresponds to:\n$$ \\mathbf{J}_{eq} = -i\\omega \\mathbf{P} = -i\\omega \\varepsilon_b \\chi \\mathbf{E} \\quad (3) $$\nThis current represents the response of the material contrast to the total electric field $\\mathbf{E}$.\n\nThe time-averaged power dissipated by the electromagnetic field within a volume is given by the work done by the field on the currents. The density of this power, $p_{diss}$, is given by $\\frac{1}{2}\\operatorname{Re}\\{\\mathbf{E} \\cdot \\mathbf{J}^*\\}$, where $\\mathbf{J}$ is the relevant current density. In our case, we are interested in the power dissipated specifically by the material contrast, which is represented by the equivalent current $\\mathbf{J}_{eq}$.\nTherefore, the time-averaged power density dissipated by the susceptibility contrast is:\n$$ p_{diss}(\\mathbf{r}) = \\frac{1}{2}\\operatorname{Re}\\{\\mathbf{E}(\\mathbf{r}) \\cdot \\mathbf{J}_{eq}(\\mathbf{r})^*\\} \\quad (4) $$\nWe take the complex conjugate of $\\mathbf{J}_{eq}$ from equation $(3)$. Since $\\omega$ and $\\varepsilon_b$ are real, we have:\n$$ \\mathbf{J}_{eq}^* = (-i\\omega \\varepsilon_b \\chi \\mathbf{E})^* = i\\omega \\varepsilon_b \\chi^* \\mathbf{E}^* $$\nSubstituting this into the expression for $p_{diss}$:\n$$ p_{diss} = \\frac{1}{2}\\operatorname{Re}\\{\\mathbf{E} \\cdot (i\\omega \\varepsilon_b \\chi^* \\mathbf{E}^*)\\} $$\nSince $\\chi$ is a scalar quantity, we can rearrange the terms. The dot product $\\mathbf{E} \\cdot \\mathbf{E}^*$ is equal to $|\\mathbf{E}|^2$, the squared magnitude of the electric field vector.\n$$ p_{diss} = \\frac{1}{2}\\operatorname{Re}\\{i\\omega \\varepsilon_b \\chi^* |\\mathbf{E}|^2\\} $$\nLet's express the complex susceptibility $\\chi$ in terms of its real and imaginary parts, $\\chi = \\operatorname{Re}\\{\\chi\\} + i\\operatorname{Im}\\{\\chi\\}$. Its conjugate is $\\chi^* = \\operatorname{Re}\\{\\chi\\} - i\\operatorname{Im}\\{\\chi\\}$.\n$$ p_{diss} = \\frac{1}{2}\\operatorname{Re}\\{i\\omega \\varepsilon_b (\\operatorname{Re}\\{\\chi\\} - i\\operatorname{Im}\\{\\chi\\}) |\\mathbf{E}|^2\\} $$\n$$ p_{diss} = \\frac{1}{2}\\operatorname{Re}\\{i\\omega \\varepsilon_b \\operatorname{Re}\\{\\chi\\} |\\mathbf{E}|^2 + \\omega \\varepsilon_b \\operatorname{Im}\\{\\chi\\} |\\mathbf{E}|^2\\} $$\nThe first term inside the curly braces is purely imaginary, while the second term is purely real. Taking the real part of the expression yields:\n$$ p_{diss}(\\mathbf{r}) = \\frac{1}{2}\\omega \\varepsilon_b \\operatorname{Im}\\{\\chi(\\mathbf{r})\\} |\\mathbf{E}(\\mathbf{r})|^2 \\quad (5) $$\nThis equation provides the fundamental relationship between the local material properties and the dissipated power density.\n\nA material is defined as passive if it does not generate net electromagnetic energy. This imposes two conditions:\n1.  **Local Passivity**: At any point $\\mathbf{r}$, the time-averaged dissipated power density must be non-negative, $p_{diss}(\\mathbf{r}) \\ge 0$. As the quantities $\\omega$, $\\varepsilon_b$, and $|\\mathbf{E}(\\mathbf{r})|^2$ are all non-negative, equation $(5)$ implies that local passivity is equivalent to the condition:\n    $$ \\operatorname{Im}\\{\\chi(\\mathbf{r})\\} \\ge 0 $$\n    This forms the basis for the first part of our diagnostic.\n\n2.  **Global Passivity**: The total time-averaged power dissipated within any finite volume $V$ must be non-negative. This total power, $P_{diss}$, is the integral of the power density over the volume:\n    $$ P_{diss} = \\int_V p_{diss}(\\mathbf{r}) \\, dV = \\int_V \\frac{1}{2}\\omega \\varepsilon_b \\operatorname{Im}\\{\\chi(\\mathbf{r})\\} |\\mathbf{E}(\\mathbf{r})|^2 \\, dV \\ge 0 $$\n\n### Discretization for the Computational Diagnostic\n\nThe problem provides the reconstructed susceptibility $\\{\\widehat{\\chi}_n\\}_{n=1}^N$ and the electric field $\\{E_n\\}_{n=1}^N$ on a discrete grid of $N$ cells, each with volume $\\Delta V_n$. To formulate the diagnostic, we assume that the susceptibility and electric field are piecewise constant within each cell $n$, taking the values $\\widehat{\\chi}_n$ and $E_n$. The problem also specifies that $E_n$ is a scalar complex field, which is a common simplification in 2D models where only a single vector component is non-zero (e.g., $E_z$). We thus make the assumption that $|\\mathbf{E}(\\mathbf{r})|^2$ can be replaced by $|E_n|^2$ within cell $n$.\n\nThe discrete diagnostic follows directly:\n-   **Local Passivity Check**: For each cell $n \\in \\{1, \\dots, N\\}$, we check if the local passivity criterion is met:\n    $$ \\operatorname{Im}\\{\\widehat{\\chi}_n\\} \\ge 0 $$\n    The diagnostic will count the number of cells $n$ where this condition is violated (i.e., $\\operatorname{Im}\\{\\widehat{\\chi}_n\\} < 0$).\n\n-   **Global Dissipated Power Check**: The integral for the total dissipated power $P_{diss}$ is approximated by a sum over all cells:\n    $$ P_{diss} = \\sum_{n=1}^{N} \\left( \\frac{1}{2}\\omega \\varepsilon_b \\operatorname{Im}\\{\\widehat{\\chi}_n\\} |E_n|^2 \\right) \\Delta V_n $$\n    The diagnostic will compute this sum and check if the global passivity criterion $P_{diss} \\ge 0$ is satisfied.\n\nThese two checks constitute the complete diagnostic to be implemented. The implementation will use the provided noisy susceptibility values $\\widehat{\\chi}_n$ in place of the analytical $\\chi(\\mathbf{r})$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies a passivity diagnostic to noisy susceptibility reconstructions.\n    \"\"\"\n\n    # --- Constants ---\n    EPSILON_0 = 8.854187817e-12  # Vacuum permittivity in F/m\n    EPSILON_R_B = 2.25            # Background relative permittivity\n    FREQ = 3.0e9                  # Operating frequency in Hz\n    DELTA_V = 1.0e-6              # Uniform cell volume in m^3\n\n    # --- Derived Constants ---\n    EPSILON_B = EPSILON_0 * EPSILON_R_B  # Background permittivity\n    OMEGA = 2 * np.pi * FREQ           # Angular frequency in rad/s\n\n    # Prefactor for dissipated power calculation\n    power_prefactor = 0.5 * OMEGA * EPSILON_B * DELTA_V\n\n    # --- Test Suite ---\n    test_cases = [\n        {\n            \"N\": 5,\n            \"re_chi_true\": np.array([0.10, 0.05, 0.00, 0.02, 0.08]),\n            \"im_chi_true\": np.array([0.020, 0.010, 0.030, 0.005, 0.015]),\n            \"E_mag\": np.array([1.0, 2.0, 1.5, 0.0, 0.5]),\n            \"E_phase\": np.array([0.0, np.pi/4, -np.pi/2, 0.0, np.pi]),\n            \"sigma\": 0.002,\n            \"seed\": 12345,\n        },\n        {\n            \"N\": 5,\n            \"re_chi_true\": np.array([0.10, 0.05, 0.00, 0.02, 0.08]),\n            \"im_chi_true\": np.array([0.020, 0.010, 0.030, 0.005, 0.015]),\n            \"E_mag\": np.array([0.3, 0.7, 1.2, 2.0, 1.0]),\n            \"E_phase\": np.array([np.pi/3, -np.pi/6, np.pi/2, np.pi/8, -np.pi/5]),\n            \"sigma\": 0.030,\n            \"seed\": 2025,\n        },\n        {\n            \"N\": 4,\n            \"re_chi_true\": np.array([0.0, 0.10, -0.05, 0.20]),\n            \"im_chi_true\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"E_mag\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"E_phase\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"sigma\": 0.010,\n            \"seed\": 7,\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        N = case[\"N\"]\n        sigma = case[\"sigma\"]\n        seed = case[\"seed\"]\n\n        # --- Setup for the case ---\n        # Initialize random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # 1. Construct ground-truth and electric field arrays\n        chi_true = case[\"re_chi_true\"] + 1j * case[\"im_chi_true\"]\n        E_field = case[\"E_mag\"] * np.exp(1j * case[\"E_phase\"])\n\n        # 2. Generate noisy susceptibility\n        noise_real = rng.normal(loc=0.0, scale=sigma, size=N)\n        noise_imag = rng.normal(loc=0.0, scale=sigma, size=N)\n        eta = noise_real + 1j * noise_imag\n        chi_hat = chi_true + eta\n        \n        # --- Apply Passivity Diagnostic ---\n        \n        # 3. Check local passivity: Im{chi_hat_n} >= 0 for all n\n        im_chi_hat = chi_hat.imag\n        negative_mask = im_chi_hat < 0\n        num_negative_pixels = int(np.sum(negative_mask))\n        all_imag_nonneg = (num_negative_pixels == 0)\n\n        # 4. Compute total dissipated power: P_diss >= 0\n        E_mag_sq = np.abs(E_field)**2\n        power_terms = im_chi_hat * E_mag_sq\n        P_diss_W = power_prefactor * np.sum(power_terms)\n        total_power_nonneg = (P_diss_W >= 0)\n        \n        # Round power to 12 decimal places\n        P_diss_W_rounded = round(P_diss_W, 12)\n\n        all_results.append([\n            all_imag_nonneg,\n            total_power_nonneg,\n            num_negative_pixels,\n            P_diss_W_rounded\n        ])\n    \n    # --- Format and Print Output ---\n    # Manually format the output string to match the required specifications\n    # (e.g., lowercase booleans, no spaces, specific float format)\n    case_strs = []\n    for r in all_results:\n        # Format: [bool,bool,int,float]\n        case_str = (\n            f\"[{str(r[0]).lower()},\"\n            f\"{str(r[1]).lower()},\"\n            f\"{r[2]},\"\n            f\"{r[3]:.12f}]\"\n        )\n        case_strs.append(case_str)\n\n    # Final format: [[...],[...],[...]]\n    final_output = f\"[{','.join(case_strs)}]\"\n    print(final_output)\n\n\nsolve()\n```", "id": "3295880"}, {"introduction": "While direct implementations of CSI are instructive, their quadratic computational complexity, $\\mathcal{O}(N^2)$, renders them impractical for the large-scale three-dimensional problems encountered in practice. This hands-on exercise explores the solution: fast algorithms, such as the Fast Multipole Method (FMM), which reduce the complexity to nearly linear. By building and analyzing a computational cost model, you will quantify the dramatic speedup offered by FMM, revealing how it enables CSI to tackle realistic imaging scenarios with millions of unknowns [@problem_id:3295853].", "problem": "Consider time-harmonic electromagnetic scattering in a homogeneous background with angular frequency $\\omega$, background wavenumber $k$, and free-space dyadic Green's function $G_{k}(\\mathbf{r},\\mathbf{r}')$ such that the scattered field can be written as a volume potential of a contrast source. In the Contrast Source Inversion (CSI) framework, the contrast source $w(\\mathbf{r})$ is defined by $w(\\mathbf{r}) = \\chi(\\mathbf{r}) E(\\mathbf{r})$, where $\\chi$ is the contrast and $E$ is the total electric field. The forward scattering operator $S$ and its adjoint $S^{\\ast}$ act as\n$$\n(S w)(\\mathbf{r}) = \\int_{\\Omega} G_{k}(\\mathbf{r},\\mathbf{r}') w(\\mathbf{r}') \\,\\mathrm{d}\\mathbf{r}',\\quad\n(S^{\\ast} v)(\\mathbf{r}) = \\int_{\\Omega} \\overline{G_{k}(\\mathbf{r}',\\mathbf{r})}\\, v(\\mathbf{r}') \\,\\mathrm{d}\\mathbf{r}',\n$$\nwhere $\\Omega \\subset \\mathbb{R}^{3}$ is the inversion domain and $\\overline{(\\cdot)}$ denotes complex conjugation. A standard discrete approximation uses a set of $N$ points at positions $\\{\\mathbf{r}_{j}\\}_{j=1}^{N}$ with associated volumes $\\Delta V$ and unknowns $\\{w_{j}\\}_{j=1}^{N}$, yielding dense $N \\times N$ linear operators. A single application of $S$ on a vector requires a sum over $N$ sources for each of $N$ targets, which leads to a direct complexity of $\\mathcal{O}(N^{2})$ operations per application. In CSI, a single iteration typically requires evaluating both $S$ and $S^{\\ast}$ at least once, plus local pointwise updates such as $w \\leftarrow \\chi E$ or gradient steps based on residuals, which are linear in $N$.\n\nTo scale to large three-dimensional problems, one may accelerate both $S$ and $S^{\\ast}$ using the Fast Multipole Method (FMM). Assume a uniform spatial distribution of points in a unit cube and construct a balanced octree with leaf capacity $m$ points per box. Let the number of leaf boxes be $L$ and the total number of boxes across all levels be $B_{\\text{tot}}$. For uniform refinement, one may model\n$$\nL \\approx \\left\\lceil \\frac{N}{m} \\right\\rceil,\\quad B_{\\text{tot}} \\approx \\left\\lceil \\frac{8}{7} L \\right\\rceil,\n$$\nbased on the geometric series of octree levels. A standard interaction list of well-separated boxes per target box at a given level is of size $c_{\\text{int}}$ (a constant independent of $N$ for a fixed separation criterion). The FMM evaluation of $S$ (and similarly $S^{\\ast}$) consists of the following phases:\n- Particle-to-multipole (P2M) at leaves, upward multipole-to-multipole (M2M), multipole-to-local translations (M2L) between well-separated boxes, downward local-to-local (L2L), local-to-particle (L2P) at leaves, and near-field particle-to-particle (P2P) interactions restricted to neighboring boxes.\n- The near-field neighbor count per leaf in $3$D can be approximated by $27$ boxes (including the box itself and its face, edge, and corner neighbors), which yields near-field pairs on the order of $27\\, N\\, m$ for interior boxes in a large, uniformly filled cube.\n\nAssume the following micro-kernel floating-point operation (FLOP) model for a single application of $S$ or $S^{\\ast}$, parameterized by the multipole order $p$:\n- P2P kernel evaluation per source-target pair: $c_{\\text{P2P}} = 40$ FLOPs.\n- P2M cost per point per order: $c_{\\text{P2M}} = 6$ FLOPs, so P2M at a leaf of size $m$ costs $c_{\\text{P2M}}\\, m\\, p$.\n- M2M cost per box per child in terms of $p^{2}$ coefficients: $c_{\\text{M2M}} = 20$ FLOPs per $p^{2}$, aggregated as a factor $8$ per parent from its children.\n- M2L cost per box interaction in terms of $p^{2}$ coefficients: $c_{\\text{M2L}} = 30$ FLOPs per $p^{2}$.\n- L2L cost per box per child in terms of $p^{2}$ coefficients: $c_{\\text{L2L}} = 20$ FLOPs per $p^{2}$, aggregated with factor $8$ per parent.\n- L2P cost per point per order: $c_{\\text{L2P}} = 6$ FLOPs, so L2P at a leaf of size $m$ costs $c_{\\text{L2P}}\\, m\\, p$.\n- Local pointwise update cost per unknown (to account for CSI-local steps such as $w \\leftarrow \\chi E$ or a gradient step) is $c_{\\text{upd}} = 8$ FLOPs.\n\nLet the near-field P2P pairs be the minimum between the neighbor estimate and the total possible pairs, to capture small-$N$ boundary effects,\n$$\nN_{\\text{near}} = \\min\\left(27\\, N\\, m,\\; N^{2}\\right).\n$$\nOne application of $S$ or $S^{\\ast}$ is then modeled by\n$$\nC_{\\text{apply,FMM}}(N,m,p,c_{\\text{int}}) = c_{\\text{P2P}}\\, N_{\\text{near}} + c_{\\text{P2M}}\\, N\\, p + c_{\\text{M2M}}\\, (B_{\\text{tot}}-L)\\, 8\\, p^{2} + c_{\\text{M2L}}\\, B_{\\text{tot}}\\, c_{\\text{int}}\\, p^{2} + c_{\\text{L2L}}\\, (B_{\\text{tot}}-L)\\, 8\\, p^{2} + c_{\\text{L2P}}\\, N\\, p.\n$$\nA CSI iteration uses both $S$ and $S^{\\ast}$ and a local update, so its FMM-modeled cost is\n$$\nC_{\\text{iter,FMM}}(N,m,p,c_{\\text{int}}) = 2\\, C_{\\text{apply,FMM}}(N,m,p,c_{\\text{int}}) + c_{\\text{upd}}\\, N.\n$$\nThe direct dense method cost per application (without FMM) is modeled as\n$$\nC_{\\text{apply,dir}}(N) = c_{\\text{P2P}}\\, N^{2},\n$$\nand the direct CSI iteration cost is\n$$\nC_{\\text{iter,dir}}(N) = 2\\, c_{\\text{P2P}}\\, N^{2} + c_{\\text{upd}}\\, N.\n$$\nConsider $K$ CSI iterations. The total modeled costs are $K$ times the per-iteration costs. Define the speedup factor over $K$ iterations as\n$$\n\\text{speedup}(N,m,p,c_{\\text{int}},K) = \\frac{K\\, C_{\\text{iter,dir}}(N)}{K\\, C_{\\text{iter,FMM}}(N,m,p,c_{\\text{int}})} = \\frac{C_{\\text{iter,dir}}(N)}{C_{\\text{iter,FMM}}(N,m,p,c_{\\text{int}})}.\n$$\n\nTask:\n- Starting from the integral definitions of $S$ and $S^{\\ast}$ and the discrete approximation, argue why the direct cost scales as $\\mathcal{O}(N^{2})$ and why an octree-based Fast Multipole Method (FMM) yields a modeled cost that is near-linear in $N$ for fixed $m$, $p$, and $c_{\\text{int}}$, using the above FLOP model. Derive the expressions for $L$, $B_{\\text{tot}}$, $N_{\\text{near}}$, $C_{\\text{apply,FMM}}$, $C_{\\text{iter,FMM}}$, $C_{\\text{apply,dir}}$, and $C_{\\text{iter,dir}}$ explicitly from first principles and structural counting arguments on the octree and interaction lists.\n- Implement a program that evaluates $\\text{speedup}(N,m,p,c_{\\text{int}},K)$ for each test case below using the given FLOP model. The program should not build or traverse an explicit octree; it should compute the modeled counts analytically as above.\n\nUse the following test suite of parameter values, each specified as $(N,m,p,c_{\\text{int}},K)$:\n- Test $1$: $(10^{5}, 100, 6, 189, 5)$.\n- Test $2$ (million-unknown grid): $(10^{6}, 200, 10, 189, 5)$.\n- Test $3$ (small-grid edge case): $(4096, 32, 4, 189, 3)$.\n- Test $4$ (alternate interaction list size): $(10^{5}, 200, 10, 125, 5)$.\n- Test $5$ (million-unknown, lower order): $(10^{6}, 100, 6, 343, 10)$.\n\nYour program should produce a single line of output containing the speedup results for the five test cases as a comma-separated list enclosed in square brackets, for example, $[\\text{result1},\\text{result2},\\text{result3},\\text{result4},\\text{result5}]$. The results must be floating-point numbers. No physical units are required; all quantities are dimensionless counts or ratios.", "solution": "The analysis of the problem begins with the validation of its premises and then proceeds to a detailed derivation of the computational costs for both the direct and the Fast Multipole Method (FMM) accelerated approaches for solving the Contrast Source Inversion (CSI) problem.\n\nThe problem statement describes the time-harmonic electromagnetic scattering problem governed by an integral equation. The scattered field is represented as a volume integral over a contrast source $w(\\mathbf{r})$, which is the product of the contrast $\\chi(\\mathbf{r})$ and the total electric field $E(\\mathbf{r})$. The forward scattering operator $S$ maps the contrast source $w$ to the scattered field it produces within the inversion domain $\\Omega$. The discrete form of this operator application on a vector of $N$ unknowns $\\{w_j\\}_{j=1}^{N}$ at positions $\\{\\mathbf{r}_j\\}_{j=1}^{N}$ is a matrix-vector product. Let the discrete representation of the operator be a matrix $\\mathbf{S} \\in \\mathbb{C}^{N \\times N}$ with entries $\\mathbf{S}_{ij} = G_k(\\mathbf{r}_i, \\mathbf{r}_j) \\Delta V$, where $\\Delta V$ is the volume associated with each discretization point. Evaluating the scattered field at all $N$ points, $\\mathbf{E}_{\\text{scat}} = \\mathbf{S} \\mathbf{w}$, requires computing a sum for each of the $N$ components of the resulting vector:\n$$\n(\\mathbf{S} \\mathbf{w})_i = \\sum_{j=1}^{N} \\mathbf{S}_{ij} w_j\n$$\nFor each index $i$ from $1$ to $N$, this summation involves $N$ multiplications and $N-1$ additions, resulting in $\\mathcal{O}(N)$ floating-point operations (FLOPs). Since this must be done for all $N$ components of the output vector, the total computational cost for a single application of $S$ scales as $N \\times \\mathcal{O}(N) = \\mathcal{O}(N^2)$. The problem provides a simplified cost model $C_{\\text{apply,dir}}(N) = c_{\\text{P2P}} N^2$, where $c_{\\text{P2P}}=40$ represents the FLOP count for evaluating a single source-target interaction via the Green's function. This model directly reflects the all-pairs summation. A similar argument applies to the adjoint operator $S^\\ast$, whose discrete form is the conjugate transpose of $\\mathbf{S}$, leading to an identical $\\mathcal{O}(N^2)$ complexity. A typical CSI iteration involves one application of $S$, one of $S^\\ast$, and local updates (e.g., gradient steps) that require operations at each point, costing $\\mathcal{O}(N)$. Thus, the total cost for a direct CSI iteration is dominated by the two matrix-vector products, yielding the model:\n$$\nC_{\\text{iter,dir}}(N) = 2 \\cdot C_{\\text{apply,dir}}(N) + c_{\\text{upd}} N = 2 c_{\\text{P2P}} N^2 + c_{\\text{upd}} N\n$$\nwhere $c_{\\text{upd}}=8$ is the cost of the local update per point.\n\nThe FMM accelerates this computation by hierarchically decomposing interactions into near-field and far-field components using a spatial subdivision, typically an octree for three-dimensional problems. For a large number of uniformly distributed points $N$, this leads to a near-linear computational complexity, $\\mathcal{O}(N \\log N)$ or $\\mathcal{O}(N)$, for fixed accuracy. The provided cost model details this for a fixed multipole order $p$ and leaf box size $m$.\n\nFirst, we establish the properties of the octree structure. The domain is recursively subdivided into $8$ child boxes until each leaf box contains no more than $m$ points. For $N$ uniformly distributed points, the number of leaf boxes $L$ is approximately $N/m$. The model refines this as $L \\approx \\lceil N/m \\rceil$. The total number of boxes $B_{\\text{tot}}$ in a balanced octree is the sum of a geometric series. If the lowest level has $L$ boxes, the total number of boxes is approximately $\\sum_{k=0}^{\\log_8 L} 8^k \\approx \\frac{8}{7}L$. The model uses $B_{\\text{tot}} \\approx \\lceil \\frac{8}{7} L \\rceil$. For a fixed $m$, both $L$ and $B_{\\text{tot}}$ scale linearly with $N$.\n\nThe FMM evaluation of $S$ or $S^\\ast$ consists of several steps, with costs derived as follows:\n1.  **Near-Field (P2P)**: Interactions between points in neighboring boxes are computed directly. For a box deep inside the domain, its neighbors consist of a $3 \\times 3 \\times 3$ block of $27$ boxes (including itself). With an average of $m$ points per box, each of the $N$ points interacts directly with approximately $27m$ neighbors. The total number of near-field pairs is thus on the order of $27Nm$. However, if $N$ is small, all points may be in each other's near-field, making the interaction count $N(N-1) \\approx N^2$. The model correctly captures this by defining the number of near-field pairs as $N_{\\text{near}} = \\min(27Nm, N^2)$. The cost is $c_{\\text{P2P}} N_{\\text{near}}$. For large $N$ and fixed $m$, this term is $\\mathcal{O}(N)$.\n\n2.  **Particle-to-Multipole (P2M)**: At each of the $L$ leaf boxes, the sources within are used to form a multipole expansion. The cost per leaf is proportional to the number of points $m$ and the complexity of the expansion, modeled as being proportional to the order $p$. As there are approximately $N$ points in total across all leaves, the total P2M cost is $c_{\\text{P2M}} N p$. This is an $\\mathcal{O}(N)$ operation.\n\n3.  **Multipole-to-Multipole (M2M)**: In the upward pass of the FMM, multipole expansions of $8$ child boxes are shifted and aggregated to form the expansion of their parent. This is performed for all $(B_{\\text{tot}} - L)$ non-leaf boxes. The cost per parent is proportional to the number of expansion coefficients ($\\mathcal{O}(p^2)$) and the number of children ($8$). The model gives this total cost as $c_{\\text{M2M}} (B_{\\text{tot}}-L) 8 p^2$. Since $B_{\\text{tot}}-L$ is proportional to $N$, this cost is $\\mathcal{O}(N)$.\n\n4.  **Multipole-to-Local (M2L)**: This is the translation step where the far-field influence is computed. For each target box, the multipole expansions of all well-separated source boxes in its \"interaction list\" are converted into a local expansion at the target box's center. For a given precision, the size of this interaction list, $c_{\\text{int}}$, is a constant independent of $N$. The operation is performed for all $B_{\\text{tot}}$ boxes. The cost per translation is proportional to the number of coefficients, modeled as $\\mathcal{O}(p^2)$. The total cost is $c_{\\text{M2L}} B_{\\text{tot}} c_{\\text{int}} p^2$. As $B_{\\text{tot}} \\propto N$, this cost is $\\mathcal{O}(N)$.\n\n5.  **Local-to-Local (L2L)**: In the downward pass, the local expansion of a parent box is shifted to the centers of its $8$ children. This is structurally identical to the M2M pass and applies to all $(B_{\\text{tot}} - L)$ non-leaf boxes. The total cost is modeled as $c_{\\text{L2L}} (B_{\\text{tot}}-L) 8 p^2$, which is $\\mathcal{O}(N)$.\n\n6.  **Local-to-Particle (L2P)**: At each leaf box, the accumulated local expansion is evaluated at each of the interior points to find their potential due to all far-field sources. This is symmetric to the P2M step, and its total cost is $c_{\\text{L2P}} N p$, which is $\\mathcal{O}(N)$.\n\nSumming these contributions gives the cost for one application of $S$ or $S^\\ast$ with FMM:\n$$\nC_{\\text{apply,FMM}}(N,m,p,c_{\\text{int}}) = c_{\\text{P2P}} N_{\\text{near}} + (c_{\\text{P2M}} + c_{\\text{L2P}}) N p + (c_{\\text{M2M}} + c_{\\text{L2L}}) (B_{\\text{tot}}-L) 8 p^2 + c_{\\text{M2L}} B_{\\text{tot}} c_{\\text{int}} p^2\n$$\nSince each term's dependence on $N$ is linear (for fixed $m, p, c_{\\text{int}}$), the total cost $C_{\\text{apply,FMM}}$ is $\\mathcal{O}(N)$.\nThe FMM-accelerated CSI iteration cost is then:\n$$\nC_{\\text{iter,FMM}}(N,m,p,c_{\\text{int}}) = 2 C_{\\text{apply,FMM}}(N,m,p,c_{\\text{int}}) + c_{\\text{upd}} N\n$$\nwhich also scales as $\\mathcal{O}(N)$. This near-linear scaling is a dramatic improvement over the direct method's $\\mathcal{O}(N^2)$ scaling for large $N$, which is what the speedup factor quantifies. The parameter $K$, the number of iterations, appears in both the numerator and denominator of the speedup calculation, and thus cancels out, as stated in the problem.\n$$\n\\text{speedup} = \\frac{K \\cdot C_{\\text{iter,dir}}(N)}{K \\cdot C_{\\text{iter,FMM}}(N,m,p,c_{\\text{int}})} = \\frac{C_{\\text{iter,dir}}(N)}{C_{\\text{iter,FMM}}(N,m,p,c_{\\text{int}})}\n$$\nThis completes the derivation of all expressions based on the provided model and first principles of the algorithms.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating the FMM speedup for given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (N, m, p, c_int, K)\n    # The number of iterations K is not used in the calculation as it cancels out.\n    test_cases = [\n        (10**5, 100, 6, 189, 5),\n        (10**6, 200, 10, 189, 5),\n        (4096, 32, 4, 189, 3),\n        (10**5, 200, 10, 125, 5),\n        (10**6, 100, 6, 343, 10),\n    ]\n\n    # FLOP model constants\n    c_P2P = 40.0\n    c_P2M = 6.0\n    c_M2M = 20.0\n    c_M2L = 30.0\n    c_L2L = 20.0\n    c_L2P = 6.0\n    c_upd = 8.0\n\n    def calculate_speedup(N, m, p, c_int):\n        \"\"\"\n        Calculates the speedup factor for a single set of parameters.\n\n        Args:\n            N (int): Number of points/unknowns.\n            m (int): Leaf capacity of the octree.\n            p (int): Multipole expansion order.\n            c_int (int): Size of the interaction list.\n\n        Returns:\n            float: The calculated speedup factor.\n        \"\"\"\n        # Ensure floating point arithmetic for large numbers\n        N, m, p, c_int = float(N), float(m), float(p), float(c_int)\n\n        # Cost model for the direct method\n        C_iter_dir = 2.0 * c_P2P * (N**2) + c_upd * N\n\n        # Cost model for the FMM-accelerated method\n        # 1. Octree structure counts\n        L = np.ceil(N / m)\n        B_tot = np.ceil((8.0 / 7.0) * L)\n\n        # 2. Near-field interaction count\n        N_near = min(27.0 * N * m, N**2)\n\n        # 3. Cost for one application of S or S* with FMM\n        p2_term = p**2\n        term_p2p = c_P2P * N_near\n        term_p_linear = (c_P2M + c_L2P) * N * p\n        term_box_translate = (c_M2M + c_L2L) * (B_tot - L) * 8.0 * p2_term\n        term_m2l = c_M2L * B_tot * c_int * p2_term\n        \n        C_apply_FMM = term_p2p + term_p_linear + term_box_translate + term_m2l\n\n        # 4. Total FMM iteration cost\n        C_iter_FMM = 2.0 * C_apply_FMM + c_upd * N\n\n        # 5. Speedup calculation\n        speedup = C_iter_dir / C_iter_FMM\n        return speedup\n\n    results = []\n    for case in test_cases:\n        N, m, p, c_int, K = case\n        result = calculate_speedup(N, m, p, c_int)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3295853"}]}