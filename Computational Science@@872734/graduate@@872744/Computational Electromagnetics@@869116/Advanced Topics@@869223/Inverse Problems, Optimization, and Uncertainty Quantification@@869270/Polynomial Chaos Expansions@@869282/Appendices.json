{"hands_on_practices": [{"introduction": "Once a model's output has been represented by a Polynomial Chaos Expansion (PCE), the most direct and fundamental application is the computation of its statistical moments. This exercise guides you through the process of deriving the mean and variance of the output quantity directly from its PCE coefficients [@problem_id:3341826]. By leveraging the orthonormality property of the basis polynomials, you will see how these crucial statistical descriptors are available through simple algebraic expressions, providing a powerful and efficient link between the expansion coefficients and the model's statistical behavior.", "problem": "Consider a frequency-domain computational electromagnetics model in which a homogeneous, isotropic medium has uncertain permittivity $\\,\\epsilon(\\xi) = \\epsilon_{0}\\,(1+\\xi)\\,$, where $\\,\\epsilon_{0} > 0\\,$ is a deterministic constant and $\\,\\xi \\sim \\mathcal{N}(0,\\sigma^{2})\\,$ is a scalar Gaussian random input with variance $\\,\\sigma^{2} > 0\\,$. Let $\\,u(\\xi)\\,$ denote the nonnegative magnitude of a scalar field solution evaluated at a fixed point and frequency, obtained from the governing linear boundary value problem induced by Maxwell’s equations (e.g., the scalar Helmholtz operator with wavenumber $\\,k(\\xi) = \\omega \\sqrt{\\mu\\,\\epsilon(\\xi)}\\,$), under boundary conditions that ensure a unique solution for admissible values of $\\,\\xi\\,$. Assume $\\,u(\\xi)\\,$ belongs to the square-integrable space with respect to the Gaussian measure of $\\,\\xi\\,$, so that a Polynomial Chaos Expansion (PCE) in the Wiener–Hermite class exists.\n\nSuppose $\\,u(\\xi)\\,$ is represented by a finite-order PCE with respect to an orthonormal Hermite basis adapted to the distribution of $\\,\\xi\\,$:\n$$\nu(\\xi) \\approx \\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi),\n$$\nwhere $\\,p \\in \\mathbb{N}\\,$ is the truncation order, $\\,\\hat{u}_{k} \\in \\mathbb{R}\\,$ are deterministic coefficients, and $\\,\\{\\Psi_{k}(\\xi)\\}_{k=0}^{p}\\,$ is an orthonormal polynomial basis with respect to the Gaussian measure of $\\,\\xi\\,$, satisfying $\\,\\mathbb{E}[\\Psi_{m}(\\xi)\\,\\Psi_{n}(\\xi)] = \\delta_{mn}\\,$ and $\\,\\Psi_{0}(\\xi) \\equiv 1\\,$.\n\nStarting from first principles—namely, the linearity of the governing operator with respect to the field, the definition of second-order moments, and orthonormality of the Wiener–Hermite basis—derive expressions for the mean $\\,\\mathbb{E}[u(\\xi)]\\,$ and the variance $\\,\\operatorname{Var}(u(\\xi))\\,$ in terms of the coefficients $\\,\\{\\hat{u}_{k}\\}_{k=0}^{p}\\,$. Then, state mathematically precise conditions on the input $\\,\\xi\\,$, the permittivity map $\\,\\epsilon(\\xi)\\,$, and the deterministic operator that guarantee mean-square convergence of the truncated PCE $\\,\\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi)\\,$ to $\\,u(\\xi)\\,$ as $\\,p \\to \\infty\\,$.\n\nProvide your final answer as a single row vector containing the mean and variance in closed-form analytic expressions in terms of $\\,\\{\\hat{u}_{k}\\}\\,$ and $\\,p\\,$. No rounding is required, and no units should be included in the final answer.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n-   Uncertain permittivity: $\\epsilon(\\xi) = \\epsilon_{0}\\,(1+\\xi)$, where $\\epsilon_{0} > 0$ is a deterministic constant.\n-   Random input variable: $\\xi \\sim \\mathcal{N}(0,\\sigma^{2})$ with variance $\\sigma^{2} > 0$.\n-   Field solution: $u(\\xi)$ is the nonnegative magnitude of a scalar field at a fixed point and frequency.\n-   Governing physics: A linear boundary value problem derived from Maxwell’s equations, with a wavenumber $k(\\xi) = \\omega \\sqrt{\\mu\\,\\epsilon(\\xi)}$.\n-   Boundary conditions: Such that a unique solution exists for admissible values of $\\xi$.\n-   Function space: $u(\\xi)$ is in the square-integrable space with respect to the Gaussian measure of $\\xi$, i.e., $\\mathbb{E}[u(\\xi)^2]  \\infty$.\n-   Polynomial Chaos Expansion (PCE): $u(\\xi) \\approx \\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi)$, where $p \\in \\mathbb{N}$ is the truncation order.\n-   PCE coefficients: $\\hat{u}_{k} \\in \\mathbb{R}$ are deterministic.\n-   Orthonormal basis: $\\{\\Psi_{k}(\\xi)\\}_{k=0}^{p}$ is a Wiener–Hermite basis.\n-   Orthonormality condition: $\\mathbb{E}[\\Psi_{m}(\\xi)\\,\\Psi_{n}(\\xi)] = \\delta_{mn}$.\n-   Zeroth-order basis polynomial: $\\Psi_{0}(\\xi) \\equiv 1$.\n-   Task 1: Derive expressions for the mean $\\mathbb{E}[u(\\xi)]$ and variance $\\operatorname{Var}(u(\\xi))$ in terms of $\\{\\hat{u}_{k}\\}_{k=0}^{p}$.\n-   Task 2: State mathematically precise conditions on $\\xi$, $\\epsilon(\\xi)$, and the operator that guarantee mean-square convergence of the PCE.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is well-grounded in the theory of Uncertainty Quantification (UQ) for computational electromagnetics. The use of Polynomial Chaos Expansions, specifically the Wiener-Hermite class for a Gaussian random variable, is a standard and scientifically rigorous technique. The modeling of permittivity as a random variable is a common approach to study the effects of material uncertainty. The potential for negative permittivity does not invalidate the problem, as this is physically meaningful in certain contexts (e.g., metamaterials) and the problem statement uses the term \"admissible values of $\\xi$\", implicitly handling any mathematical or physical singularities.\n-   **Well-Posed**: The problem is well-posed. It asks for the derivation of statistical moments from a given PCE representation and a statement of the conditions for convergence. The former is a direct calculation, and the latter relates to a fundamental theorem in PCE theory (Cameron-Martin theorem).\n-   **Objective**: The problem is stated using precise, objective mathematical language.\n-   **Conclusion**: The problem is free of scientific flaws, ambiguities, or contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\nThe problem asks for the derivation of the mean and variance of the random variable $u(\\xi)$ from its Polynomial Chaos Expansion (PCE), and for the conditions that guarantee the mean-square convergence of this expansion. The PCE of $u(\\xi)$ is given in a truncated form as $u_p(\\xi) = \\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi)$. The linearity of the governing operator with respect to the field $u$ is a foundational requirement for using spectral methods like PCE-Galerkin to determine the coefficients $\\hat{u}_k$, but the calculation of moments from the expansion itself relies on the linearity of the expectation operator and the properties of the basis polynomials.\n\nFirst, we derive the mean, $\\mathbb{E}[u(\\xi)]$. For the truncated series $u_p(\\xi)$, the mean is found by applying the expectation operator $\\mathbb{E}[\\cdot]$.\n$$\n\\mathbb{E}[u_p(\\xi)] = \\mathbb{E}\\left[\\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi)\\right]\n$$\nBy the linearity of the expectation operator, we can move it inside the summation. Since the coefficients $\\hat{u}_k$ are deterministic constants, they can be factored out of the expectation.\n$$\n\\mathbb{E}[u_p(\\xi)] = \\sum_{k=0}^{p} \\hat{u}_{k}\\,\\mathbb{E}[\\Psi_{k}(\\xi)]\n$$\nTo evaluate $\\mathbb{E}[\\Psi_{k}(\\xi)]$, we use the provided properties of the orthonormal basis. We are given that $\\Psi_0(\\xi) \\equiv 1$ and the orthonormality condition is $\\mathbb{E}[\\Psi_m(\\xi)\\Psi_n(\\xi)] = \\delta_{mn}$, where $\\delta_{mn}$ is the Kronecker delta. We can express the expectation of $\\Psi_k(\\xi)$ as an inner product with $\\Psi_0(\\xi)$:\n$$\n\\mathbb{E}[\\Psi_{k}(\\xi)] = \\mathbb{E}[\\Psi_k(\\xi) \\cdot 1] = \\mathbb{E}[\\Psi_k(\\xi)\\,\\Psi_0(\\xi)] = \\delta_{k0}\n$$\nThis result shows that the expectation of any basis polynomial is zero, except for the zeroth-order polynomial, for which it is one. Substituting this into the expression for the mean of $u_p(\\xi)$:\n$$\n\\mathbb{E}[u_p(\\xi)] = \\sum_{k=0}^{p} \\hat{u}_{k}\\,\\delta_{k0} = \\hat{u}_0 \\cdot 1 + \\hat{u}_1 \\cdot 0 + \\dots + \\hat{u}_p \\cdot 0 = \\hat{u}_0\n$$\nThus, the mean of the random field is exactly the zeroth-order PCE coefficient. As $p \\to \\infty$, if the expansion converges, $\\mathbb{E}[u(\\xi)] = \\hat{u}_0$.\n\nNext, we derive the variance, $\\operatorname{Var}(u(\\xi))$. The variance is defined as $\\operatorname{Var}(u_p(\\xi)) = \\mathbb{E}[u_p(\\xi)^2] - (\\mathbb{E}[u_p(\\xi)])^2$. We have already found $\\mathbb{E}[u_p(\\xi)] = \\hat{u}_0$. We now need to compute the second moment, $\\mathbb{E}[u_p(\\xi)^2]$.\n$$\n\\mathbb{E}[u_p(\\xi)^2] = \\mathbb{E}\\left[\\left(\\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi)\\right)^2\\right] = \\mathbb{E}\\left[\\left(\\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi)\\right)\\left(\\sum_{j=0}^{p} \\hat{u}_{j}\\,\\Psi_{j}(\\xi)\\right)\\right]\n$$\nExpanding the product and using the linearity of expectation:\n$$\n\\mathbb{E}[u_p(\\xi)^2] = \\mathbb{E}\\left[\\sum_{k=0}^{p}\\sum_{j=0}^{p} \\hat{u}_k \\hat{u}_j \\Psi_k(\\xi) \\Psi_j(\\xi)\\right] = \\sum_{k=0}^{p}\\sum_{j=0}^{p} \\hat{u}_k \\hat{u}_j \\mathbb{E}[\\Psi_k(\\xi) \\Psi_j(\\xi)]\n$$\nUsing the orthonormality condition $\\mathbb{E}[\\Psi_k(\\xi)\\Psi_j(\\xi)] = \\delta_{kj}$:\n$$\n\\mathbb{E}[u_p(\\xi)^2] = \\sum_{k=0}^{p}\\sum_{j=0}^{p} \\hat{u}_k \\hat{u}_j \\delta_{kj}\n$$\nThe Kronecker delta $\\delta_{kj}$ collapses the inner summation, as terms are non-zero only when $j=k$:\n$$\n\\mathbb{E}[u_p(\\xi)^2] = \\sum_{k=0}^{p} \\hat{u}_k^2\n$$\nThis is the second moment of $u_p(\\xi)$. Now, we substitute the second moment and the mean into the variance formula:\n$$\n\\operatorname{Var}(u_p(\\xi)) = \\mathbb{E}[u_p(\\xi)^2] - (\\mathbb{E}[u_p(\\xi)])^2 = \\sum_{k=0}^{p} \\hat{u}_k^2 - (\\hat{u}_0)^2\n$$\nSeparating the $k=0$ term from the summation gives:\n$$\n\\operatorname{Var}(u_p(\\xi)) = \\left(\\hat{u}_0^2 + \\sum_{k=1}^{p} \\hat{u}_k^2\\right) - \\hat{u}_0^2 = \\sum_{k=1}^{p} \\hat{u}_k^2\n$$\nThe variance is the sum of the squares of all higher-order ($k \\geq 1$) PCE coefficients.\n\nFinally, we address the conditions for mean-square convergence. The truncated PCE $\\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi)$ converges in the mean-square sense to $u(\\xi)$ as $p \\to \\infty$ if the mean-square error tends to zero:\n$$\n\\lim_{p \\to \\infty} \\mathbb{E}\\left[ \\left(u(\\xi) - \\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi)\\right)^2 \\right] = 0\n$$\nFor a Wiener-Hermite PCE, which is used for a Gaussian random variable $\\xi$, the Cameron-Martin theorem provides the necessary and sufficient condition for this convergence.\nThe single, mathematically precise condition is that the function $u(\\xi)$ must be square-integrable with respect to the Gaussian probability measure, i.e., it must belong to the Hilbert space $L^2(\\mathbb{R}, w(\\xi)d\\xi)$, where $w(\\xi)$ is the Gaussian probability density function for $\\xi$. This is equivalent to requiring that the second moment of $u(\\xi)$ is finite:\n$$\n\\mathbb{E}[u(\\xi)^2]  \\infty\n$$\nThe problem statement explicitly assumes this condition (\"Assume $u(\\xi)$ belongs to the square-integrable space...\"). This condition on the output function $u(\\xi)$ imposes implicit conditions on the system's components:\n$1$. The input random variable $\\xi$ and the permittivity map $\\epsilon(\\xi) = \\epsilon_0(1+\\xi)$ are already defined. The Gaussian measure has support over all of $\\mathbb{R}$.\n$2$. The deterministic operator (e.g., the scalar Helmholtz operator), its associated boundary conditions, and any forcing terms must be such that the solution $u(\\xi)$ resulting from the parameterization $k(\\xi) = \\omega\\sqrt{\\mu\\epsilon(\\xi)}$ satisfies the finite-variance condition. This means that any singularities in the solution $u(\\xi)$ must be integrable in the mean-square sense. For example, in a boundary value problem, the operator may become singular at certain resonant wavenumbers. A sufficient (but not necessary) condition to guarantee square-integrability is that the range of $k(\\xi)$ for $\\xi \\in \\mathbb{R}$ does not include any of these resonant wavenumbers, ensuring a bounded solution $u(\\xi)$ and thus a finite second moment. More generally, even if $u(\\xi)$ has singularities (e.g., at $\\xi=-1$ where $\\epsilon=0$ or at resonant wavenumbers), the growth of the solution magnitude $|u(\\xi)|$ near these singularities must be slow enough for the integral $\\int_{-\\infty}^{\\infty} |u(\\xi)|^2 \\exp(-\\xi^2/(2\\sigma^2)) d\\xi$ to converge.\nIn summary, the fundamental convergence guarantee is the square-integrability of the solution, which is a combined property of the operator, boundary conditions, and the specific parameterization.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\hat{u}_{0}  \\sum_{k=1}^{p} \\hat{u}_{k}^{2} \\end{pmatrix}}\n$$", "id": "3341826"}, {"introduction": "Polynomial Chaos Expansions offer insights that go far beyond basic statistical moments, providing a complete framework for variance-based global sensitivity analysis (GSA). This practice demonstrates how the PCE coefficients can be used to decompose the total output variance and compute Sobol' indices, which quantify the influence of each uncertain input parameter on the model's output [@problem_id:3523157]. This powerful post-processing step allows us to identify which sources of uncertainty are most critical to the system's response without requiring any additional model evaluations.", "problem": "In a conjugate heat transfer–thermoelastic coupling, a reduced-order model predicts the steady interface temperature response $u(\\boldsymbol{\\xi})$ of a micro heat-exchanger subjected to uncertain thermal and mechanical boundary conditions. The uncertain inputs are mapped to three independent standardized variables $\\boldsymbol{\\xi}=(\\xi_{1},\\xi_{2},\\xi_{3})$, with $\\xi_{i}\\sim \\mathcal{U}[-1,1]$ and mutually independent. A Polynomial Chaos Expansion (PCE) represents the model output as\n$$\nu(\\boldsymbol{\\xi})=\\sum_{\\boldsymbol{\\alpha}\\in \\mathbb{N}_{0}^{3}} c_{\\boldsymbol{\\alpha}}\\,\\psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi}),\n$$\nwhere $\\psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})=\\prod_{i=1}^{3}\\varphi_{\\alpha_{i}}(\\xi_{i})$ and $\\{\\varphi_{n}\\}_{n\\ge 0}$ are the orthonormal Legendre polynomials on $[-1,1]$ with respect to the uniform measure, i.e., $\\mathbb{E}[\\varphi_{n}(\\xi)\\varphi_{m}(\\xi)]=\\delta_{nm}$ and $\\varphi_{0}(\\xi)=1$. Consequently, the multivariate basis is orthonormal: $\\mathbb{E}[\\psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})\\psi_{\\boldsymbol{\\beta}}(\\boldsymbol{\\xi})]=\\delta_{\\boldsymbol{\\alpha}\\boldsymbol{\\beta}}$. The nonzero coefficients obtained from a nonintrusive PCE calibration are\n- $c_{(0,0,0)}=2$,\n- $c_{(1,0,0)}=\\frac{1}{2}$, $c_{(0,1,0)}=-\\frac{1}{4}$, $c_{(0,0,1)}=\\frac{1}{8}$,\n- $c_{(2,0,0)}=\\frac{1}{4}$, $c_{(0,2,0)}=-\\frac{1}{8}$, $c_{(0,0,2)}=\\frac{1}{16}$,\n- $c_{(1,1,0)}=\\frac{1}{8}$, $c_{(1,0,1)}=-\\frac{1}{16}$, $c_{(0,1,1)}=\\frac{1}{32}$, $c_{(1,1,1)}=-\\frac{1}{32}$,\nand all other $c_{\\boldsymbol{\\alpha}}$ are zero.\n\nStarting from the orthonormality of the basis, the definition of variance $\\mathbb{V}[u]=\\mathbb{E}\\big[(u-\\mathbb{E}[u])^{2}\\big]$, and the variance-based global sensitivity analysis definitions (first-order Sobol index $S_{i}=\\mathbb{V}\\big(\\mathbb{E}[u\\mid \\xi_{i}]\\big)/\\mathbb{V}[u]$ and total Sobol index $S_{T_{i}}=1-\\mathbb{V}\\big(\\mathbb{E}[u\\mid \\boldsymbol{\\xi}_{\\sim i}]\\big)/\\mathbb{V}[u]$, where $\\boldsymbol{\\xi}_{\\sim i}$ denotes all inputs except $\\xi_{i}$), do the following:\n\n1. Derive an expression for $\\mathbb{V}[u]$ in terms of the coefficients $c_{\\boldsymbol{\\alpha}}$.\n2. Derive expressions for the first-order Sobol indices $S_{i}$ in terms of sums of $c_{\\boldsymbol{\\alpha}}^{2}$ associated with multiindices $\\boldsymbol{\\alpha}$ supported only on variable $\\xi_{i}$.\n3. Derive expressions for the total Sobol indices $S_{T_{i}}$ in terms of sums of $c_{\\boldsymbol{\\alpha}}^{2}$ associated with multiindices $\\boldsymbol{\\alpha}$ for which $\\alpha_{i}0$.\n4. Evaluate the derived expressions for $i\\in\\{1,2,3\\}$ using the provided coefficients.\n\nExpress your final answer as a single row matrix\n$$\n\\big[\\ \\mathbb{V}[u],\\ S_{1},\\ S_{2},\\ S_{3},\\ S_{T_{1}},\\ S_{T_{2}},\\ S_{T_{3}}\\ \\big]\n$$\nwith exact values as simplified fractions. No rounding is required and no units are to be reported.", "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Model output: $u(\\boldsymbol{\\xi})$, where $\\boldsymbol{\\xi}=(\\xi_{1},\\xi_{2},\\xi_{3})$.\n- Input variables: $\\xi_{i}$ are independent and identically distributed as $\\xi_{i}\\sim \\mathcal{U}[-1,1]$ for $i=1, 2, 3$.\n- Polynomial Chaos Expansion (PCE): $u(\\boldsymbol{\\xi})=\\sum_{\\boldsymbol{\\alpha}\\in \\mathbb{N}_{0}^{3}} c_{\\boldsymbol{\\alpha}}\\,\\psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})$.\n- Basis functions: $\\psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})=\\prod_{i=1}^{3}\\varphi_{\\alpha_{i}}(\\xi_{i})$.\n- Univariate polynomials: $\\{\\varphi_{n}\\}_{n\\ge 0}$ are orthonormal Legendre polynomials on $[-1,1]$, with $\\mathbb{E}[\\varphi_{n}(\\xi)\\varphi_{m}(\\xi)]=\\delta_{nm}$ and $\\varphi_{0}(\\xi)=1$.\n- Multivariate basis property: The basis $\\{\\psi_{\\boldsymbol{\\alpha}}\\}$ is orthonormal, $\\mathbb{E}[\\psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})\\psi_{\\boldsymbol{\\beta}}(\\boldsymbol{\\xi})]=\\delta_{\\boldsymbol{\\alpha}\\boldsymbol{\\beta}}$.\n- Nonzero PCE coefficients ($c_{\\boldsymbol{\\alpha}}$):\n  - $c_{(0,0,0)}=2$\n  - $c_{(1,0,0)}=\\frac{1}{2}$, $c_{(0,1,0)}=-\\frac{1}{4}$, $c_{(0,0,1)}=\\frac{1}{8}$\n  - $c_{(2,0,0)}=\\frac{1}{4}$, $c_{(0,2,0)}=-\\frac{1}{8}$, $c_{(0,0,2)}=\\frac{1}{16}$\n  - $c_{(1,1,0)}=\\frac{1}{8}$, $c_{(1,0,1)}=-\\frac{1}{16}$, $c_{(0,1,1)}=\\frac{1}{32}$\n  - $c_{(1,1,1)}=-\\frac{1}{32}$\n- All other coefficients $c_{\\boldsymbol{\\alpha}}$ are zero.\n- Definitions:\n  - Variance: $\\mathbb{V}[u]=\\mathbb{E}\\big[(u-\\mathbb{E}[u])^{2}\\big]$.\n  - First-order Sobol index: $S_{i}=\\mathbb{V}\\big(\\mathbb{E}[u\\mid \\xi_{i}]\\big)/\\mathbb{V}[u]$.\n  - Total Sobol index: $S_{T_{i}}=1-\\mathbb{V}\\big(\\mathbb{E}[u\\mid \\boldsymbol{\\xi}_{\\sim i}]\\big)/\\mathbb{V}[u]$, where $\\boldsymbol{\\xi}_{\\sim i}$ denotes all inputs except $\\xi_{i}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly rooted in the theory of uncertainty quantification, specifically using Polynomial Chaos Expansions for sensitivity analysis. This is a standard and well-established methodology in computational science and engineering.\n- **Well-Posed:** All necessary information, including the form of the expansion, the properties of the basis functions, and the numerical values of all non-zero coefficients, is provided. The tasks are clearly defined and lead to a unique, derivable solution.\n- **Objective:** The problem is stated in precise mathematical terms, free of any subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Solution\n\nThe solution requires performing four sequential tasks: deriving expressions for the total variance, first-order Sobol indices, and total Sobol indices in terms of the PCE coefficients, and then evaluating these quantities.\n\n**1. Derivation of the Variance $\\mathbb{V}[u]$**\n\nThe mean of the model output $u(\\boldsymbol{\\xi})$ is computed first. Using the linearity of the expectation operator and the orthonormality of the basis functions:\n$$\n\\mathbb{E}[u] = \\mathbb{E}\\left[\\sum_{\\boldsymbol{\\alpha}\\in \\mathbb{N}_{0}^{3}} c_{\\boldsymbol{\\alpha}}\\psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})\\right] = \\sum_{\\boldsymbol{\\alpha}\\in \\mathbb{N}_{0}^{3}} c_{\\boldsymbol{\\alpha}}\\mathbb{E}[\\psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})]\n$$\nSince $\\psi_{\\boldsymbol{0}}(\\boldsymbol{\\xi})=\\varphi_{0}(\\xi_1)\\varphi_{0}(\\xi_2)\\varphi_{0}(\\xi_3)=1 \\cdot 1 \\cdot 1 = 1$, the expectation of a basis function is:\n$$\n\\mathbb{E}[\\psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})] = \\mathbb{E}[\\psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})\\psi_{\\boldsymbol{0}}(\\boldsymbol{\\xi})] = \\delta_{\\boldsymbol{\\alpha}\\boldsymbol{0}}\n$$\nwhere $\\boldsymbol{0} = (0,0,0)$ is the zero multi-index. Thus, the mean is simply the first coefficient:\n$$\n\\mathbb{E}[u] = \\sum_{\\boldsymbol{\\alpha}\\in \\mathbb{N}_{0}^{3}} c_{\\boldsymbol{\\alpha}}\\delta_{\\boldsymbol{\\alpha}\\boldsymbol{0}} = c_{\\boldsymbol{0}} = c_{(0,0,0)}\n$$\nThe variance is defined as $\\mathbb{V}[u] = \\mathbb{E}[u^2] - (\\mathbb{E}[u])^2$. We compute the mean square value $\\mathbb{E}[u^2]$:\n$$\n\\mathbb{E}[u^2] = \\mathbb{E}\\left[\\left(\\sum_{\\boldsymbol{\\alpha}} c_{\\boldsymbol{\\alpha}}\\psi_{\\boldsymbol{\\alpha}}\\right)\\left(\\sum_{\\boldsymbol{\\beta}} c_{\\boldsymbol{\\beta}}\\psi_{\\boldsymbol{\\beta}}\\right)\\right] = \\sum_{\\boldsymbol{\\alpha},\\boldsymbol{\\beta}} c_{\\boldsymbol{\\alpha}}c_{\\boldsymbol{\\beta}}\\mathbb{E}[\\psi_{\\boldsymbol{\\alpha}}\\psi_{\\boldsymbol{\\beta}}]\n$$\nUsing the orthonormality $\\mathbb{E}[\\psi_{\\boldsymbol{\\alpha}}\\psi_{\\boldsymbol{\\beta}}] = \\delta_{\\boldsymbol{\\alpha}\\boldsymbol{\\beta}}$:\n$$\n\\mathbb{E}[u^2] = \\sum_{\\boldsymbol{\\alpha},\\boldsymbol{\\beta}} c_{\\boldsymbol{\\alpha}}c_{\\boldsymbol{\\beta}}\\delta_{\\boldsymbol{\\alpha}\\boldsymbol{\\beta}} = \\sum_{\\boldsymbol{\\alpha}} c_{\\boldsymbol{\\alpha}}^2\n$$\nSubstituting these results into the variance formula yields:\n$$\n\\mathbb{V}[u] = \\sum_{\\boldsymbol{\\alpha}} c_{\\boldsymbol{\\alpha}}^2 - c_{\\boldsymbol{0}}^2 = \\sum_{\\boldsymbol{\\alpha}\\neq\\boldsymbol{0}} c_{\\boldsymbol{\\alpha}}^2\n$$\nThis is the desired expression for the total variance.\n\n**2. Derivation of the First-Order Sobol Indices $S_i$**\n\nThe first-order Sobol index is $S_i = \\mathbb{V}_i / \\mathbb{V}[u]$, where $\\mathbb{V}_i = \\mathbb{V}[\\mathbb{E}[u \\mid \\xi_i]]$. We first compute the conditional expectation $\\mathbb{E}[u \\mid \\xi_i]$:\n$$\n\\mathbb{E}[u \\mid \\xi_i] = \\mathbb{E}\\left[\\sum_{\\boldsymbol{\\alpha}} c_{\\boldsymbol{\\alpha}} \\prod_{j=1}^3 \\varphi_{\\alpha_j}(\\xi_j) \\mid \\xi_i \\right] = \\sum_{\\boldsymbol{\\alpha}} c_{\\boldsymbol{\\alpha}} \\mathbb{E}\\left[\\prod_{j=1}^3 \\varphi_{\\alpha_j}(\\xi_j) \\mid \\xi_i \\right]\n$$\nDue to the independence of the $\\xi_j$ variables, the conditional expectation becomes:\n$$\n\\mathbb{E}\\left[\\prod_{j=1}^3 \\varphi_{\\alpha_j}(\\xi_j) \\mid \\xi_i \\right] = \\varphi_{\\alpha_i}(\\xi_i) \\prod_{j \\neq i} \\mathbb{E}[\\varphi_{\\alpha_j}(\\xi_j)]\n$$\nSince $\\mathbb{E}[\\varphi_{\\alpha_j}(\\xi_j)] = \\delta_{\\alpha_j, 0}$, the product $\\prod_{j \\neq i} \\delta_{\\alpha_j, 0}$ is non-zero (and equal to $1$) only if $\\alpha_j = 0$ for all $j \\neq i$. This restricts the sum to multi-indices that have at most one non-zero component, which must be at position $i$.\nLet $\\mathcal{A}_i = \\{\\boldsymbol{\\alpha} \\in \\mathbb{N}_0^3 \\mid \\alpha_j = 0, \\forall j \\neq i\\}$. Then:\n$$\nu_i \\equiv \\mathbb{E}[u \\mid \\xi_i] = \\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{A}_i} c_{\\boldsymbol{\\alpha}} \\psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi}) = \\sum_{k=0}^{\\infty} c_{(0,..,k,..,0)} \\varphi_k(\\xi_i)\n$$\nThe variance $\\mathbb{V}_i = \\mathbb{V}[u_i]$ is the variance of this partial sum. The mean is $\\mathbb{E}[u_i]=\\mathbb{E}[\\mathbb{E}[u|\\xi_i]]=\\mathbb{E}[u]=c_{\\boldsymbol{0}}$. Using the same logic as for the total variance:\n$$\n\\mathbb{V}_i = \\mathbb{E}[u_i^2] - (\\mathbb{E}[u_i])^2 = \\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{A}_i} c_{\\boldsymbol{\\alpha}}^2 - c_{\\boldsymbol{0}}^2 = \\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{A}_i, \\boldsymbol{\\alpha} \\neq \\boldsymbol{0}} c_{\\boldsymbol{\\alpha}}^2\n$$\nThis sum includes squares of coefficients whose multi-indices are supported only on variable $\\xi_i$. So, the first-order index is:\n$$\nS_i = \\frac{\\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{A}_i, \\boldsymbol{\\alpha} \\neq \\boldsymbol{0}} c_{\\boldsymbol{\\alpha}}^2}{\\sum_{\\boldsymbol{\\beta} \\neq \\boldsymbol{0}} c_{\\boldsymbol{\\beta}}^2}\n$$\n\n**3. Derivation of the Total Sobol Indices $S_{T_i}$**\n\nThe total Sobol index is $S_{T_i} = 1 - \\mathbb{V}[\\mathbb{E}[u \\mid \\boldsymbol{\\xi}_{\\sim i}]] / \\mathbb{V}[u]$. We compute the conditional expectation on all variables except $\\xi_i$, denoted by $\\boldsymbol{\\xi}_{\\sim i}$:\n$$\n\\mathbb{E}[u \\mid \\boldsymbol{\\xi}_{\\sim i}] = \\sum_{\\boldsymbol{\\alpha}} c_{\\boldsymbol{\\alpha}} \\mathbb{E}\\left[\\prod_{j=1}^3 \\varphi_{\\alpha_j}(\\xi_j) \\mid \\boldsymbol{\\xi}_{\\sim i} \\right] = \\sum_{\\boldsymbol{\\alpha}} c_{\\boldsymbol{\\alpha}} \\left(\\prod_{j \\neq i} \\varphi_{\\alpha_j}(\\xi_j) \\right) \\mathbb{E}[\\varphi_{\\alpha_i}(\\xi_i)]\n$$\nAgain, $\\mathbb{E}[\\varphi_{\\alpha_i}(\\xi_i)] = \\delta_{\\alpha_i, 0}$. The sum is restricted to indices $\\boldsymbol{\\alpha}$ for which $\\alpha_i = 0$. Let $\\mathcal{B}_i = \\{\\boldsymbol{\\alpha} \\in \\mathbb{N}_0^3 \\mid \\alpha_i=0 \\}$. Then:\n$$\nu_{\\sim i} \\equiv \\mathbb{E}[u \\mid \\boldsymbol{\\xi}_{\\sim i}] = \\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{B}_i} c_{\\boldsymbol{\\alpha}} \\psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})\n$$\nThe variance of this term is $\\mathbb{V}[u_{\\sim i}] = \\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{B}_i, \\boldsymbol{\\alpha} \\neq \\boldsymbol{0}} c_{\\boldsymbol{\\alpha}}^2$. Then,\n$$\nS_{T_i} = 1 - \\frac{\\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{B}_i, \\boldsymbol{\\alpha} \\neq \\boldsymbol{0}} c_{\\boldsymbol{\\alpha}}^2}{\\sum_{\\boldsymbol{\\beta} \\neq \\boldsymbol{0}} c_{\\boldsymbol{\\beta}}^2} = \\frac{\\left(\\sum_{\\boldsymbol{\\beta} \\neq \\boldsymbol{0}} c_{\\boldsymbol{\\beta}}^2\\right) - \\left(\\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{B}_i, \\boldsymbol{\\alpha} \\neq \\boldsymbol{0}} c_{\\boldsymbol{\\alpha}}^2\\right)}{\\sum_{\\boldsymbol{\\beta} \\neq \\boldsymbol{0}} c_{\\boldsymbol{\\beta}}^2}\n$$\nThe set of all non-zero multi-indices $\\{\\boldsymbol{\\beta} \\neq \\boldsymbol{0}\\}$ can be partitioned into those with $\\beta_i=0$ (i.e., $\\mathcal{B}_i \\setminus \\{\\boldsymbol{0}\\}$) and those with $\\beta_i  0$. The numerator is the sum of $c_{\\boldsymbol{\\alpha}}^2$ over all multi-indices $\\boldsymbol{\\alpha}$ for which $\\alpha_i  0$.\n$$\nS_{T_i} = \\frac{\\sum_{\\boldsymbol{\\alpha} \\text{ with } \\alpha_i  0} c_{\\boldsymbol{\\alpha}}^2}{\\sum_{\\boldsymbol{\\beta} \\neq \\boldsymbol{0}} c_{\\boldsymbol{\\beta}}^2}\n$$\n\n**4. Evaluation of Expressions**\n\nFirst, we calculate the total variance $\\mathbb{V}[u] = \\sum_{\\boldsymbol{\\alpha}\\neq\\boldsymbol{0}} c_{\\boldsymbol{\\alpha}}^2$. The squares of the given non-zero coefficients ($c_{\\boldsymbol{\\alpha}}$ for $\\boldsymbol{\\alpha} \\neq \\boldsymbol{0}$) are:\n- $c_{(1,0,0)}^2 = (\\frac{1}{2})^2 = \\frac{1}{4}$\n- $c_{(0,1,0)}^2 = (-\\frac{1}{4})^2 = \\frac{1}{16}$\n- $c_{(0,0,1)}^2 = (\\frac{1}{8})^2 = \\frac{1}{64}$\n- $c_{(2,0,0)}^2 = (\\frac{1}{4})^2 = \\frac{1}{16}$\n- $c_{(0,2,0)}^2 = (-\\frac{1}{8})^2 = \\frac{1}{64}$\n- $c_{(0,0,2)}^2 = (\\frac{1}{16})^2 = \\frac{1}{256}$\n- $c_{(1,1,0)}^2 = (\\frac{1}{8})^2 = \\frac{1}{64}$\n- $c_{(1,0,1)}^2 = (-\\frac{1}{16})^2 = \\frac{1}{256}$\n- $c_{(0,1,1)}^2 = (\\frac{1}{32})^2 = \\frac{1}{1024}$\n- $c_{(1,1,1)}^2 = (-\\frac{1}{32})^2 = \\frac{1}{1024}$\n$\\mathbb{V}[u] = \\frac{1}{4} + \\frac{1}{16} + \\frac{1}{64} + \\frac{1}{16} + \\frac{1}{64} + \\frac{1}{256} + \\frac{1}{64} + \\frac{1}{256} + \\frac{1}{1024} + \\frac{1}{1024}$\n$\\mathbb{V}[u] = \\frac{256+64+16+64+16+4+16+4+1+1}{1024} = \\frac{442}{1024} = \\frac{221}{512}$.\n\nNext, we calculate the partial variances for $S_i$:\n- $\\mathbb{V}_1 = c_{(1,0,0)}^2 + c_{(2,0,0)}^2 = \\frac{1}{4} + \\frac{1}{16} = \\frac{5}{16}$.\n- $\\mathbb{V}_2 = c_{(0,1,0)}^2 + c_{(0,2,0)}^2 = \\frac{1}{16} + \\frac{1}{64} = \\frac{5}{64}$.\n- $\\mathbb{V}_3 = c_{(0,0,1)}^2 + c_{(0,0,2)}^2 = \\frac{1}{64} + \\frac{1}{256} = \\frac{5}{256}$.\n$S_1 = \\frac{5/16}{221/512} = \\frac{5}{16} \\cdot \\frac{512}{221} = \\frac{5 \\cdot 32}{221} = \\frac{160}{221}$.\n$S_2 = \\frac{5/64}{221/512} = \\frac{5}{64} \\cdot \\frac{512}{221} = \\frac{5 \\cdot 8}{221} = \\frac{40}{221}$.\n$S_3 = \\frac{5/256}{221/512} = \\frac{5}{256} \\cdot \\frac{512}{221} = \\frac{5 \\cdot 2}{221} = \\frac{10}{221}$.\n\nFinally, we calculate the partial variances for $S_{T_i}$:\n- $\\mathbb{V}_{T_1} = c_{(1,0,0)}^2+c_{(2,0,0)}^2+c_{(1,1,0)}^2+c_{(1,0,1)}^2+c_{(1,1,1)}^2 = \\frac{1}{4}+\\frac{1}{16}+\\frac{1}{64}+\\frac{1}{256}+\\frac{1}{1024} = \\frac{256+64+16+4+1}{1024} = \\frac{341}{1024}$.\n- $\\mathbb{V}_{T_2} = c_{(0,1,0)}^2+c_{(0,2,0)}^2+c_{(1,1,0)}^2+c_{(0,1,1)}^2+c_{(1,1,1)}^2 = \\frac{1}{16}+\\frac{1}{64}+\\frac{1}{64}+\\frac{1}{1024}+\\frac{1}{1024} = \\frac{64+16+16+1+1}{1024} = \\frac{98}{1024} = \\frac{49}{512}$.\n- $\\mathbb{V}_{T_3} = c_{(0,0,1)}^2+c_{(0,0,2)}^2+c_{(1,0,1)}^2+c_{(0,1,1)}^2+c_{(1,1,1)}^2 = \\frac{1}{64}+\\frac{1}{256}+\\frac{1}{256}+\\frac{1}{1024}+\\frac{1}{1024} = \\frac{16+4+4+1+1}{1024} = \\frac{26}{1024} = \\frac{13}{512}$.\n$S_{T_1} = \\frac{341/1024}{442/1024} = \\frac{341}{442}$.\n$S_{T_2} = \\frac{49/512}{221/512} = \\frac{49}{221}$.\n$S_{T_3} = \\frac{13/512}{221/512} = \\frac{13}{221} = \\frac{1}{17}$.\n\nThe final results are collected into the required matrix format.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{221}{512}  \\frac{160}{221}  \\frac{40}{221}  \\frac{10}{221}  \\frac{341}{442}  \\frac{49}{221}  \\frac{1}{17} \\end{pmatrix}}\n$$", "id": "3523157"}, {"introduction": "While standard orthogonal polynomials are readily available for canonical probability distributions, real-world uncertainty quantification often involves arbitrary measures known only through their moments. This computational exercise addresses the fundamental question of how to construct a suitable basis in such scenarios [@problem_id:3523203]. You will implement the Gram-Schmidt orthonormalization procedure to build a custom polynomial basis directly from a given sequence of raw moments, a foundational technique for applying arbitrary Polynomial Chaos Expansions when standard bases are not applicable.", "problem": "You are given a univariate input uncertainty described only through its sequence of raw moments $\\{\\mu_{k}\\}_{k=0}^{2p}$ up to order $2p$, with $\\mu_{0}=1$ and $\\mu_{k}=\\mathbb{E}[X^{k}]$ for a real-valued random variable $X$. Consider the polynomial space $\\mathcal{P}_{p}$ of all real polynomials of degree at most $p$. Define the inner product on $\\mathcal{P}_{p}$ induced by the underlying measure as\n$$\n\\langle f,g\\rangle \\equiv \\mathbb{E}[f(X)g(X)].\n$$\nWhen $f$ and $g$ are represented in the monomial basis $\\{1,x,x^{2},\\dots,x^{p}\\}$, the inner product reduces to a bilinear form involving moments:\n$$\n\\langle x^{i},x^{j}\\rangle=\\mu_{i+j},\\quad \\text{for all } i,j\\in\\{0,1,\\dots,p\\}.\n$$\nYour task is to construct an orthonormal polynomial chaos basis $\\{\\phi_{k}(x)\\}_{k=0}^{p}$ in $\\mathcal{P}_{p}$ by applying Gram–Schmidt orthonormalization to the monomial sequence $\\{1,x,\\dots,x^{p}\\}$ under the inner product $\\langle\\cdot,\\cdot\\rangle$ defined above. Each polynomial $\\phi_{k}(x)$ must be represented by its coefficient vector in the monomial basis, and orthonormality must satisfy\n$$\n\\langle \\phi_{i},\\phi_{j}\\rangle=\\delta_{ij},\\quad \\text{for all } i,j\\in\\{0,1,\\dots,p\\},\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta.\n\nFrom first principles, use only the definitions above and the raw moments $\\{\\mu_k\\}_{k=0}^{2p}$ to:\n- implement a numerically stable modified Gram–Schmidt procedure on the monomial basis,\n- produce the orthonormal basis coefficients $\\{\\phi_{k}\\}_{k=0}^{p}$,\n- verify orthonormality numerically by assembling the Gram matrix $G\\in\\mathbb{R}^{(p+1)\\times(p+1)}$ with entries\n$$\nG_{ij}=\\langle \\phi_{i},\\phi_{j}\\rangle,\n$$\ncomputed using the raw moments, and\n- report the maximum absolute deviation from the identity matrix,\n$$\n\\varepsilon \\equiv \\max_{0\\le i,j\\le p}\\left|G_{ij}-\\delta_{ij}\\right|.\n$$\n\nAll quantities are dimensionless. Angles are not involved. Percentages are not involved.\n\nTest suite. Your program must run the procedure above for the following five distributions, each specified by a valid moment sequence up to order $2p$, and return one scalar $\\varepsilon$ per test case:\n\n- Case $1$ (standard normal distribution): $X\\sim\\mathcal{N}(0,1)$. Use degree $p=6$. Its raw moments are given by the well-tested identities\n$$\n\\mu_{2n}=(2n-1)!!=\\prod_{k=1}^{n}(2k-1),\\quad \\mu_{2n+1}=0,\\quad \\text{for all } n\\in\\mathbb{N}_{0}.\n$$\n\n- Case $2$ (uniform distribution): $X\\sim\\mathcal{U}(-1,1)$. Use degree $p=6$. Its raw moments are\n$$\n\\mu_{2n}=\\frac{1}{2n+1},\\quad \\mu_{2n+1}=0,\\quad \\text{for all } n\\in\\mathbb{N}_{0}.\n$$\n\n- Case $3$ (exponential distribution): $X\\sim\\mathrm{Exp}(\\lambda)$ with rate $\\lambda=1$. Use degree $p=6$. Its raw moments are\n$$\n\\mu_{k}=k!,\\quad \\text{for all } k\\in\\mathbb{N}_{0}.\n$$\n\n- Case $4$ (beta distribution): $X\\sim\\mathrm{Beta}(\\alpha,\\beta)$ with $\\alpha=2$ and $\\beta=3$. Use degree $p=6$. Its raw moments are\n$$\n\\mu_{k}=\\frac{(\\alpha)_{k}}{(\\alpha+\\beta)_{k}}=\\frac{\\Gamma(\\alpha+k)\\,\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\,\\Gamma(\\alpha+\\beta+k)}=\\frac{24}{(k+4)(k+3)(k+2)},\\quad \\text{for all } k\\in\\mathbb{N}_{0},\n$$\nwhere $(a)_{k}$ is the rising Pochhammer symbol.\n\n- Case $5$ (two-point discrete distribution): $X$ takes values $a=-1$ with probability $w=0.7$ and $b=2$ with probability $1-w=0.3$. Use degree $p=1$ so that the induced bilinear form on $\\mathcal{P}_{1}$ is nonsingular. Its raw moments are\n$$\n\\mu_{k}=w\\,a^{k}+(1-w)\\,b^{k},\\quad \\text{for all } k\\in\\mathbb{N}_{0}.\n$$\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_{1},r_{2},r_{3},r_{4},r_{5}]$), where $r_{i}$ is the scalar $\\varepsilon$ computed for Case $i$. Each $r_{i}$ must be a real number (floating-point). No other text should be printed.", "solution": "The problem is valid. It is a well-posed, scientifically-grounded problem in numerical linear algebra and approximation theory, with direct applications to uncertainty quantification via polynomial chaos expansions. All necessary data and definitions are provided, and the task is computationally feasible.\n\nThe objective is to construct an orthonormal polynomial basis, $\\{\\phi_k(x)\\}_{k=0}^p$, for the space of polynomials of degree at most $p$, denoted $\\mathcal{P}_p$. The construction is performed with respect to an inner product induced by a random variable $X$, whose probability measure is defined through a given sequence of raw moments, $\\{\\mu_k\\}_{k=0}^{2p}$. The specified method is the modified Gram-Schmidt (MGS) procedure applied to the monomial basis $\\{1, x, \\dots, x^p\\}$.\n\nFirst, we represent each polynomial $f(x) \\in \\mathcal{P}_p$ as a vector of its coefficients in the monomial basis. A polynomial $f(x) = \\sum_{i=0}^p c_i x^i$ is uniquely identified by the coefficient vector $\\mathbf{c} = [c_0, c_1, \\dots, c_p]^T \\in \\mathbb{R}^{p+1}$. In this representation, the monomial basis $\\{1, x, \\dots, x^p\\}$ corresponds to the standard basis vectors $\\{\\mathbf{e}_0, \\mathbf{e}_1, \\dots, \\mathbf{e}_p\\}$ of $\\mathbb{R}^{p+1}$, where $\\mathbf{e}_k$ is a vector with a $1$ at index $k$ and zeros elsewhere.\n\nThe inner product $\\langle f, g \\rangle \\equiv \\mathbb{E}[f(X)g(X)]$ can be expressed in terms of these coefficient vectors. Let $f(x)$ and $g(x)$ have coefficient vectors $\\mathbf{a}=[a_0, \\dots, a_p]^T$ and $\\mathbf{b}=[b_0, \\dots, b_p]^T$ respectively. By linearity of expectation, the inner product becomes a bilinear form:\n$$\n\\langle f, g \\rangle = \\mathbb{E}\\left[ \\left( \\sum_{i=0}^p a_i X^i \\right) \\left( \\sum_{j=0}^p b_j X^j \\right) \\right] = \\sum_{i=0}^p \\sum_{j=0}^p a_i b_j \\mathbb{E}[X^{i+j}] = \\sum_{i=0}^p \\sum_{j=0}^p a_i b_j \\mu_{i+j}\n$$\nThis calculation requires the raw moments up to order $2p$, since the product of two polynomials of degree $p$ is a polynomial of degree up to $2p$, and its expectation, $\\langle x^p, x^p \\rangle = \\mu_{2p}$, is the highest-order moment needed.\n\nThe task is to orthonormalize the monomial basis vectors, which we denote as $\\{\\mathbf{v}_k = \\mathbf{e}_k\\}_{k=0}^p$. We apply the modified Gram-Schmidt (MGS) algorithm to this set. The MGS process is known for its superior numerical stability compared to the classical Gram-Schmidt method.\n\nLet us denote the initial set of vectors (the monomial coefficient vectors) as $\\{\\mathbf{v}_j\\}_{j=0}^p$. The MGS algorithm generates an orthonormal set of vectors $\\{\\mathbf{q}_j\\}_{j=0}^p$ as follows:\nInitialize a working set of vectors $\\{\\mathbf{u}_j\\}_{j=0}^p$ by setting $\\mathbf{u}_j = \\mathbf{v}_j$ for all $j$.\n\nFor $j = 0, \\dots, p$:\n1.  Compute the norm of the current vector $\\mathbf{u}_j$ using the moment-defined inner product: $N_j = \\sqrt{\\langle \\mathbf{u}_j, \\mathbf{u}_j \\rangle}$.\n2.  Normalize the vector to obtain the $j$-th orthonormal vector: $\\mathbf{q}_j = \\mathbf{u}_j / N_j$.\n3.  For all subsequent vectors $\\mathbf{u}_k$ with $k  j$, subtract their components parallel to $\\mathbf{q}_j$:\n    $$\n    \\mathbf{u}_k \\leftarrow \\mathbf{u}_k - \\langle\\mathbf{u}_k, \\mathbf{q}_j\\rangle \\mathbf{q}_j\n    $$\n    This step ensures that the updated vector $\\mathbf{u}_k$ becomes orthogonal to $\\mathbf{q}_j$. The process is repeated, making the remaining vectors in the working set orthogonal to each newly generated basis vector in sequence.\n\nThe resulting set of vectors $\\{\\mathbf{q}_j\\}_{j=0}^p$ contains the coefficients of the desired orthonormal polynomials $\\{\\phi_j(x)\\}_{j=0}^p$.\n\nFinally, to verify the orthonormality of the computed basis, we assemble the Gram matrix $G$ with entries $G_{ij} = \\langle \\phi_i, \\phi_j \\rangle$. In our vector representation, this is $G_{ij} = \\langle \\mathbf{q}_i, \\mathbf{q}_j \\rangle$. For a perfectly orthonormal basis, this matrix must be the identity matrix, $I$. The numerical error is quantified by the maximum absolute deviation from identity, as specified by the metric $\\varepsilon$:\n$$\n\\varepsilon \\equiv \\max_{0\\le i,j\\le p}\\left|G_{ij}-\\delta_{ij}\\right|\n$$\nThis procedure is implemented for each of the five test cases, starting with the generation of the required moment sequence for each specified probability distribution.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import factorial, factorial2\nimport math\n\ndef get_moments(case_name, p):\n    \"\"\"\n    Generates the sequence of raw moments {mu_k} up to order 2*p\n    for a given distribution.\n    \n    Args:\n        case_name (str): The name of the distribution case.\n        p (int): The maximum polynomial degree.\n        \n    Returns:\n        numpy.ndarray: An array of moments from order 0 to 2*p.\n    \"\"\"\n    max_order = 2 * p\n    moments = np.zeros(max_order + 1, dtype=np.float64)\n    \n    if case_name == 'normal':\n        # X ~ N(0,1), mu_{2n} = (2n-1)!!, mu_{2n+1} = 0\n        for n in range(p + 1):\n            k = 2 * n\n            if k == 0:\n                moments[k] = 1.0\n            else:\n                # scipy.special.factorial2 computes the double factorial\n                moments[k] = float(factorial2(k - 1, exact=True))\n    \n    elif case_name == 'uniform':\n        # X ~ U(-1,1), mu_{2n} = 1/(2n+1), mu_{2n+1} = 0\n        for n in range(p + 1):\n            k = 2 * n\n            moments[k] = 1.0 / (k + 1)\n            \n    elif case_name == 'exponential':\n        # X ~ Exp(1), mu_k = k!\n        for k in range(max_order + 1):\n            moments[k] = float(factorial(k, exact=True))\n            \n    elif case_name == 'beta':\n        # X ~ Beta(2,3), mu_k = 24 / ((k+4)(k+3)(k+2))\n        for k in range(max_order + 1):\n            moments[k] = 24.0 / ((k + 4.0) * (k + 3.0) * (k + 2.0))\n            \n    elif case_name == 'discrete':\n        # Two-point distribution\n        a, b, w = -1.0, 2.0, 0.7\n        # mu_k = w*a^k + (1-w)*b^k\n        for k in range(max_order + 1):\n            moments[k] = w * (a**k) + (1.0 - w) * (b**k)\n            \n    return moments\n\ndef inner_product(c1, c2, moments):\n    \"\"\"\n    Computes the inner product f,g where f and g are polynomials\n    represented by their coefficient vectors c1 and c2.\n    \"\"\"\n    p_dim = len(c1)\n    res = 0.0\n    for i in range(p_dim):\n        if c1[i] == 0.0:\n            continue\n        for j in range(p_dim):\n            if c2[j] == 0.0:\n                continue\n            res += c1[i] * c2[j] * moments[i + j]\n    return res\n\ndef run_mgs_and_verify(p, moments):\n    \"\"\"\n    Constructs an orthonormal polynomial basis using the Modified Gram-Schmidt\n    procedure and verifies its orthonormality.\n    \n    Args:\n        p (int): The maximum polynomial degree.\n        moments (numpy.ndarray): The raw moments of the distribution.\n    \n    Returns:\n        float: The maximum absolute deviation of the Gram matrix from identity.\n    \"\"\"\n    p_dim = p + 1\n    \n    # Initialize u_vectors with the coefficient vectors of the monomial basis.\n    u_vectors = np.eye(p_dim, dtype=np.float64)\n    # q_vectors will store the resulting orthonormal basis coefficient vectors.\n    q_vectors = np.zeros_like(u_vectors, dtype=np.float64)\n    \n    for j in range(p_dim):\n        u_j = u_vectors[:, j]\n        \n        # 1. Normalize the vector u_j.\n        norm_squared = inner_product(u_j, u_j, moments)\n        norm = math.sqrt(norm_squared)\n        \n        if norm  1e-15:\n            # This indicates linear dependence, which should not occur here.\n            raise ValueError(f\"Vector norm is close to zero at step {j}.\")\n\n        q_j = u_j / norm\n        q_vectors[:, j] = q_j\n        \n        # 2. Orthogonalize the remaining vectors u_k (k  j) against q_j.\n        for k in range(j + 1, p_dim):\n            u_k = u_vectors[:, k]\n            proj_coeff = inner_product(u_k, q_j, moments)\n            u_vectors[:, k] = u_k - proj_coeff * q_j\n            \n    # Verification: Assemble the Gram matrix G_ij = q_i, q_j\n    gram_matrix = np.zeros((p_dim, p_dim), dtype=np.float64)\n    for i in range(p_dim):\n        for j in range(i, p_dim):\n            val = inner_product(q_vectors[:, i], q_vectors[:, j], moments)\n            gram_matrix[i, j] = val\n            gram_matrix[j, i] = val\n            \n    identity_matrix = np.eye(p_dim)\n    epsilon = np.max(np.abs(gram_matrix - identity_matrix))\n    \n    return epsilon\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'name': 'normal', 'p': 6},\n        {'name': 'uniform', 'p': 6},\n        {'name': 'exponential', 'p': 6},\n        {'name': 'beta', 'p': 6},\n        {'name': 'discrete', 'p': 1},\n    ]\n\n    results = []\n    for case in test_cases:\n        p = case['p']\n        name = case['name']\n        moments = get_moments(name, p)\n        epsilon = run_mgs_and_verify(p, moments)\n        results.append(epsilon)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3523203"}]}