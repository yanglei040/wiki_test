{"hands_on_practices": [{"introduction": "To effectively use gradient-based optimization, one must first understand how the gradient is derived. This practice builds that foundational knowledge by guiding you through a first-principles derivation of the adjoint sensitivity for a frequency-domain electromagnetic problem discretized via the Finite Element Method (FEM). By carefully applying complex variational calculus, you will see precisely how the structure of the adjoint equations arises and why the final sensitivity expression involves a real-part operator, demystifying the core mechanics of the adjoint method. [@problem_id:3312413]", "problem": "Consider the frequency-domain Maxwell curl-curl equation for the electric field $\\mathbf{E}(\\mathbf{r};\\alpha)$ in a source-driven, nonmagnetic medium,\n$$\n\\nabla \\times \\mu_0^{-1} \\nabla \\times \\mathbf{E} - \\omega^2 \\epsilon(\\mathbf{r};\\alpha)\\,\\mathbf{E} = i \\omega \\mathbf{J},\n$$\nwhere $\\mu_0$ is the permeability of free space, $\\omega$ is the angular frequency, $\\epsilon(\\mathbf{r};\\alpha) = \\epsilon_0(\\mathbf{r}) + \\alpha\\,p(\\mathbf{r})$ is a scalar permittivity distribution parametrized by a real scalar design variable $\\alpha$, and $\\mathbf{J}$ is a given impressed current density. Let the complex inner product on fields be defined by $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\int \\mathbf{u}^*(\\mathbf{r}) \\cdot \\mathbf{v}(\\mathbf{r})\\, d\\mathbf{r}$, which is conjugate-linear in its first argument. Assume perfectly electric conductor boundary conditions on the computational boundary so that boundary terms vanish upon integration by parts.\n\nA standard edge-element Galerkin finite element method (FEM) discretization yields a linear system for the complex degrees of freedom $\\mathbf{x}(\\alpha) \\in \\mathbb{C}^n$,\n$$\n\\mathbf{A}(\\alpha)\\,\\mathbf{x}(\\alpha) = \\mathbf{b},\n$$\nwhere $\\mathbf{A}(\\alpha) = \\mathbf{K} - \\omega^2 \\operatorname{diag}(\\boldsymbol{\\epsilon}_0 + \\alpha \\mathbf{p})\\,\\mathbf{M}$, with stiffness matrix $\\mathbf{K} \\in \\mathbb{C}^{n \\times n}$, mass matrix $\\mathbf{M} \\in \\mathbb{C}^{n \\times n}$, and vectors $\\boldsymbol{\\epsilon}_0, \\mathbf{p} \\in \\mathbb{R}^n$. Let the complex inner product on $\\mathbb{C}^n$ be $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\mathbf{u}^* \\mathbf{v}$. Consider the real-valued objective functional\n$$\nJ(\\alpha) = \\frac{1}{2}\\,\\| \\mathbf{C}\\,\\mathbf{x}(\\alpha) - \\mathbf{d} \\|_2^2,\n$$\nwhere $\\mathbf{C} \\in \\mathbb{C}^{m \\times n}$ and $\\mathbf{d} \\in \\mathbb{C}^m$ are given, and the norm is induced by the complex inner product.\n\n1) Starting from the frequency-domain Maxwell equation and the above definition of the complex inner product, and using only first principles of variational calculus in complex vector spaces (for example, the Lagrangian method with the convention that the inner product is conjugate-linear in its first slot, or the equivalent Wirtinger calculus), derive a first-order sensitivity expression for $\\frac{dJ}{d\\alpha}$ in terms of the state vector $\\mathbf{x}(\\alpha)$, a suitably defined adjoint vector $\\mathbf{y}(\\alpha)$ that enforces the field constraint, and the derivative $\\frac{\\partial \\mathbf{A}}{\\partial \\alpha}$. Your derivation must explicitly show where the real part operator arises from the complex inner product and why the adjoint linear system has the form it does.\n\n2) Evaluate your derived expression for the specific $n = 2$ instance with\n$$\n\\omega = 1,\\quad\n\\mathbf{K} = \\begin{pmatrix} 3 & 1 + i \\\\ 1 - i & 4 \\end{pmatrix},\\quad\n\\mathbf{M} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix},\n$$\n$$\n\\boldsymbol{\\epsilon}_0 = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix},\\quad\n\\mathbf{p} = \\begin{pmatrix} \\tfrac{1}{2} \\\\ -\\tfrac{1}{4} \\end{pmatrix},\\quad\n\\mathbf{b} = \\begin{pmatrix} 1 \\\\ i \\end{pmatrix},\\quad\n\\mathbf{C} = \\begin{pmatrix} 1 & 0 \\end{pmatrix},\\quad\n\\mathbf{d} = 0,\n$$\nso that $\\mathbf{A}(\\alpha) = \\mathbf{K} - \\operatorname{diag}(\\boldsymbol{\\epsilon}_0 + \\alpha \\mathbf{p})\\,\\mathbf{M}$. Compute the numerical value of $\\frac{dJ}{d\\alpha}$ at $\\alpha = 0.2$ by:\n- solving the forward system for $\\mathbf{x}(\\alpha)$,\n- solving the adjoint system you derived for $\\mathbf{y}(\\alpha)$,\n- evaluating the sensitivity in terms of $\\mathbf{x}(\\alpha)$, $\\mathbf{y}(\\alpha)$, and $\\frac{\\partial \\mathbf{A}}{\\partial \\alpha}$.\n\nRound your final numerical answer for $\\frac{dJ}{d\\alpha}$ at $\\alpha = 0.2$ to four significant figures. Express the final answer as a pure number with no units.", "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard problem in computational electromagnetics sensitivity analysis. We proceed with the solution.\n\nThe problem is divided into two parts: first, to derive a general expression for the sensitivity $\\frac{dJ}{d\\alpha}$ using the adjoint method, and second, to apply this expression to a specific numerical example.\n\n**Part 1: Derivation of the Adjoint Sensitivity Expression**\n\nWe are given the objective functional $J(\\alpha)$, which is a real-valued function of the design parameter $\\alpha$:\n$$\nJ(\\alpha) = \\frac{1}{2}\\,\\| \\mathbf{C}\\,\\mathbf{x}(\\alpha) - \\mathbf{d} \\|_2^2\n$$\nHere, $\\mathbf{x}(\\alpha) \\in \\mathbb{C}^n$ is the state vector, which depends implicitly on $\\alpha$ through the linear system (the state equation):\n$$\n\\mathbf{A}(\\alpha)\\,\\mathbf{x}(\\alpha) = \\mathbf{b}\n$$\nThe norm is induced by the complex inner product $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\mathbf{u}^* \\mathbf{v} = \\mathbf{u}^H \\mathbf{v}$, where $\\mathbf{u}^H$ is the conjugate transpose of $\\mathbf{u}$. We can write the objective function using this inner product:\n$$\nJ(\\alpha) = \\frac{1}{2} \\langle \\mathbf{C}\\mathbf{x}(\\alpha) - \\mathbf{d}, \\mathbf{C}\\mathbf{x}(\\alpha) - \\mathbf{d} \\rangle\n$$\nTo find the sensitivity $\\frac{dJ}{d\\alpha}$, we differentiate $J$ with respect to $\\alpha$ using the chain rule. Let $\\mathbf{x}'(\\alpha)$ denote $\\frac{d\\mathbf{x}}{d\\alpha}$.\n$$\n\\frac{dJ}{d\\alpha} = \\frac{d}{d\\alpha} \\left[ \\frac{1}{2} (\\mathbf{C}\\mathbf{x} - \\mathbf{d})^H (\\mathbf{C}\\mathbf{x} - \\mathbf{d}) \\right]\n$$\nUsing the product rule for differentiation, we get:\n$$\n\\frac{dJ}{d\\alpha} = \\frac{1}{2} \\left[ (\\mathbf{C}\\mathbf{x}')^H (\\mathbf{C}\\mathbf{x} - \\mathbf{d}) + (\\mathbf{C}\\mathbf{x} - \\mathbf{d})^H (\\mathbf{C}\\mathbf{x}') \\right]\n$$\nThe two terms in the brackets are complex conjugates of each other. The sum of a complex number $z$ and its conjugate $z^*$ is $2 \\operatorname{Re}(z)$. This is the origin of the real-part operator. Letting $z = (\\mathbf{C}\\mathbf{x} - \\mathbf{d})^H (\\mathbf{C}\\mathbf{x}')$, we have:\n$$\n\\frac{dJ}{d\\alpha} = \\operatorname{Re}\\left[ (\\mathbf{C}\\mathbf{x} - \\mathbf{d})^H (\\mathbf{C}\\mathbf{x}') \\right]\n$$\nUsing the definition of the complex inner product, this can be written as:\n$$\n\\frac{dJ}{d\\alpha} = \\operatorname{Re}\\left[ \\langle \\mathbf{C}\\mathbf{x} - \\mathbf{d}, \\mathbf{C}\\mathbf{x}' \\rangle \\right]\n$$\nWe use the property of the adjoint operator (in this case, the conjugate transpose $\\mathbf{C}^H$) with respect to the inner product: $\\langle \\mathbf{u}, \\mathbf{T}\\mathbf{v} \\rangle = \\langle \\mathbf{T}^H\\mathbf{u}, \\mathbf{v} \\rangle$.\n$$\n\\frac{dJ}{d\\alpha} = \\operatorname{Re}\\left[ \\langle \\mathbf{C}^H(\\mathbf{C}\\mathbf{x} - \\mathbf{d}), \\mathbf{x}' \\rangle \\right]\n$$\nThis expression for the gradient depends on $\\mathbf{x}' = \\frac{d\\mathbf{x}}{d\\alpha}$, which is computationally expensive to find directly. The adjoint method circumvents this. We find an expression for $\\mathbf{x}'$ by differentiating the state equation $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ with respect to $\\alpha$:\n$$\n\\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} + \\mathbf{A} \\frac{d\\mathbf{x}}{d\\alpha} = 0 \\implies \\mathbf{x}' = -\\mathbf{A}^{-1} \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x}\n$$\nSubstituting this into the expression for $\\frac{dJ}{d\\alpha}$:\n$$\n\\frac{dJ}{d\\alpha} = \\operatorname{Re}\\left[ \\langle \\mathbf{C}^H(\\mathbf{C}\\mathbf{x} - \\mathbf{d}), -\\mathbf{A}^{-1} \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} \\rangle \\right]\n$$\nAgain, we use the adjoint property of the inner product, this time for the operator $T = -\\mathbf{A}^{-1}$:\n$$\n\\frac{dJ}{d\\alpha} = \\operatorname{Re}\\left[ \\langle -(\\mathbf{A}^{-1})^H \\mathbf{C}^H(\\mathbf{C}\\mathbf{x} - \\mathbf{d}), \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} \\rangle \\right]\n$$\nWe introduce an adjoint vector $\\mathbf{y}(\\alpha) \\in \\mathbb{C}^n$ defined as the solution to the adjoint linear system, which is chosen to eliminate the inverse of $\\mathbf{A}$:\n$$\n\\mathbf{A}^H \\mathbf{y} = \\mathbf{C}^H(\\mathbf{C}\\mathbf{x} - \\mathbf{d})\n$$\nThis is the adjoint equation. The right-hand side is the derivative of the objective function $J$ with respect to $\\mathbf{x}^*$ (or proportional to it, depending on the convention for complex gradients). Solving this system gives $\\mathbf{y} = (\\mathbf{A}^H)^{-1} \\mathbf{C}^H(\\mathbf{C}\\mathbf{x} - \\mathbf{d})$. We can then substitute $\\mathbf{y}$ into the sensitivity expression:\n$$\n\\frac{dJ}{d\\alpha} = \\operatorname{Re}\\left[ \\langle -\\mathbf{y}, \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} \\rangle \\right]\n$$\nDue to the conjugate-linearity of the inner product in the first argument, $\\langle - \\mathbf{y}, \\mathbf{v} \\rangle = -\\langle \\mathbf{y}, \\mathbf{v} \\rangle$. Therefore:\n$$\n\\frac{dJ}{d\\alpha} = -\\operatorname{Re}\\left[ \\langle \\mathbf{y}, \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} \\rangle \\right] = -\\operatorname{Re}\\left[ \\mathbf{y}^H \\left(\\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x}\\right) \\right]\n$$\nThis is the final first-order sensitivity expression. It requires solving one forward linear system for $\\mathbf{x}$ and one adjoint linear system for $\\mathbf{y}$.\n\n**Part 2: Numerical Evaluation**\n\nWe now apply this formula to the given numerical instance at $\\alpha=0.2$.\n\n1. **Assemble the matrix $\\mathbf{A}(\\alpha)$ at $\\alpha=0.2$:**\nThe parameterized permittivity vector is $\\boldsymbol{\\epsilon}(\\alpha) = \\boldsymbol{\\epsilon}_0 + \\alpha \\mathbf{p}$:\n$$\n\\boldsymbol{\\epsilon}(0.2) = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix} + 0.2 \\begin{pmatrix} 0.5 \\\\ -0.25 \\end{pmatrix} = \\begin{pmatrix} 2.1 \\\\ 2.95 \\end{pmatrix}\n$$\nWith $\\omega = 1$, the matrix $\\mathbf{A}(\\alpha)$ is $\\mathbf{A}(\\alpha) = \\mathbf{K} - \\operatorname{diag}(\\boldsymbol{\\epsilon}(\\alpha))\\,\\mathbf{M}$:\n$$\n\\mathbf{A}(0.2) = \\begin{pmatrix} 3 & 1 + i \\\\ 1 - i & 4 \\end{pmatrix} - \\begin{pmatrix} 2.1 & 0 \\\\ 0 & 2.95 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 3 & 1 + i \\\\ 1 - i & 4 \\end{pmatrix} - \\begin{pmatrix} 4.2 & 0 \\\\ 0 & 2.95 \\end{pmatrix}\n$$\n$$\n\\mathbf{A}(0.2) = \\begin{pmatrix} -1.2 & 1 + i \\\\ 1 - i & 1.05 \\end{pmatrix}\n$$\n\n2. **Solve the forward system for $\\mathbf{x}(0.2)$:**\nWe solve $\\mathbf{A}(0.2)\\mathbf{x} = \\mathbf{b}$:\n$$\n\\begin{pmatrix} -1.2 & 1 + i \\\\ 1 - i & 1.05 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ i \\end{pmatrix}\n$$\nThe determinant is $\\det(\\mathbf{A}) = (-1.2)(1.05) - (1+i)(1-i) = -1.26 - 2 = -3.26$.\nThe inverse is $\\mathbf{A}^{-1} = \\frac{1}{-3.26}\\begin{pmatrix} 1.05 & -1-i \\\\ -1+i & -1.2 \\end{pmatrix}$.\nThe solution is $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$:\n$$\n\\mathbf{x} = \\frac{1}{-3.26} \\begin{pmatrix} 1.05 - i(1+i) \\\\ -1+i - 1.2i \\end{pmatrix} = \\frac{1}{-3.26} \\begin{pmatrix} 1.05 - i + 1 \\\\ -1 - 0.2i \\end{pmatrix} = \\frac{1}{-3.26} \\begin{pmatrix} 2.05 - i \\\\ -1 - 0.2i \\end{pmatrix}\n$$\n\n3. **Solve the adjoint system for $\\mathbf{y}(0.2)$:**\nThe matrix $\\mathbf{K}$ is Hermitian, and $\\operatorname{diag}(\\boldsymbol{\\epsilon}) \\mathbf{M}$ is real and symmetric (thus Hermitian). Therefore, $\\mathbf{A}(\\alpha)$ is Hermitian, so $\\mathbf{A}^H = \\mathbf{A}$. The adjoint system is $\\mathbf{A} \\mathbf{y} = \\mathbf{C}^H(\\mathbf{C}\\mathbf{x} - \\mathbf{d})$.\nGiven $\\mathbf{C} = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$ and $\\mathbf{d} = 0$, the right-hand side is:\n$$\n\\mathbf{C}^H(\\mathbf{C}\\mathbf{x}) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\left( \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\right) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} x_1 = \\begin{pmatrix} x_1 \\\\ 0 \\end{pmatrix}\n$$\nSo we solve $\\mathbf{A}\\mathbf{y} = \\begin{pmatrix} x_1 \\\\ 0 \\end{pmatrix}$. The solution is $\\mathbf{y} = \\mathbf{A}^{-1} \\begin{pmatrix} x_1 \\\\ 0 \\end{pmatrix}$:\n$$\n\\mathbf{y} = \\frac{1}{-3.26} \\begin{pmatrix} 1.05 & -1-i \\\\ -1+i & -1.2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ 0 \\end{pmatrix} = \\frac{x_1}{-3.26} \\begin{pmatrix} 1.05 \\\\ -1+i \\end{pmatrix}\n$$\nSubstituting $x_1 = \\frac{2.05-i}{-3.26}$:\n$$\n\\mathbf{y} = \\frac{2.05-i}{(-3.26)^2} \\begin{pmatrix} 1.05 \\\\ -1+i \\end{pmatrix} = \\frac{2.05-i}{10.6276} \\begin{pmatrix} 1.05 \\\\ -1+i \\end{pmatrix}\n$$\n\n4. **Evaluate the sensitivity $\\frac{dJ}{d\\alpha}$:**\nFirst, we find the derivative of the matrix $\\mathbf{A}$ with respect to $\\alpha$:\n$$\n\\frac{\\partial \\mathbf{A}}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left( \\mathbf{K} - \\omega^2 \\operatorname{diag}(\\boldsymbol{\\epsilon}_0 + \\alpha \\mathbf{p})\\,\\mathbf{M} \\right) = -\\omega^2 \\operatorname{diag}(\\mathbf{p})\\,\\mathbf{M}\n$$\nPlugging in the values with $\\omega=1$:\n$$\n\\frac{\\partial \\mathbf{A}}{\\partial \\alpha} = -1^2 \\begin{pmatrix} 0.5 & 0 \\\\ 0 & -0.25 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} = -\\begin{pmatrix} 1 & 0 \\\\ 0 & -0.25 \\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\ 0 & 0.25 \\end{pmatrix}\n$$\nThe sensitivity is $\\frac{dJ}{d\\alpha} = -\\operatorname{Re}\\left[ \\mathbf{y}^H \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} \\right]$. Let's compute the term in brackets:\n$$\n\\mathbf{y}^H = \\frac{2.05+i}{10.6276} \\begin{pmatrix} 1.05 & -1-i \\end{pmatrix}\n$$\n$$\n\\frac{\\partial \\mathbf{A}}{\\partial \\alpha}\\mathbf{x} = \\begin{pmatrix} -1 & 0 \\\\ 0 & 0.25 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} -x_1 \\\\ 0.25 x_2 \\end{pmatrix}\n$$\n$$\n\\mathbf{y}^H \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} = \\frac{2.05+i}{10.6276} \\left( 1.05(-x_1) + (-1-i)(0.25 x_2) \\right) = \\frac{- (2.05+i)}{10.6276} \\left( 1.05 x_1 + 0.25(1+i)x_2 \\right)\n$$\nLet $c=-3.26$. We have $x_1 = \\frac{2.05-i}{c}$ and $x_2 = \\frac{-1-0.2i}{c}$.\nThe term in parentheses is:\n$$\n1.05 \\frac{2.05-i}{c} + 0.25(1+i) \\frac{-1-0.2i}{c} = \\frac{1}{c} \\left[ (2.1525 - 1.05i) + (0.25)(-1-0.2i-i+0.2) \\right]\n$$\n$$\n= \\frac{1}{c} \\left[ 2.1525 - 1.05i - 0.2 - 0.3i \\right] = \\frac{1.9525 - 1.35i}{c}\n$$\nSo, we have:\n$$\n\\mathbf{y}^H \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} = \\frac{- (2.05+i)}{10.6276} \\frac{1.9525 - 1.35i}{-3.26} = \\frac{- (2.05+i)(1.9525 - 1.35i)}{-34.645976}\n$$\nNumerator product: $(2.05+i)(1.9525 - 1.35i) = 4.002625 - 2.7675i + 1.9525i + 1.35 = 5.352625 - 0.815i$.\n$$\n\\mathbf{y}^H \\frac{\\partial \\mathbf{A}}{\\partial \\alpha} \\mathbf{x} = \\frac{-(5.352625 - 0.815i)}{-34.645976} = \\frac{5.352625 - 0.815i}{34.645976} \\approx 0.154497 - 0.023523i\n$$\nFinally, the sensitivity is:\n$$\n\\frac{dJ}{d\\alpha} = -\\operatorname{Re}\\left[ 0.154497 - 0.023523i \\right] = -0.154497\n$$\nRounding to four significant figures, we get $-0.1545$.", "answer": "$$ \\boxed{-0.1545} $$", "id": "3312413"}, {"introduction": "Real-world device optimization is often subject to physical constraints, such as the requirement for material passivity. A powerful way to handle such constraints is to introduce a penalty term into the objective function. This exercise focuses on deriving the gradient for a common penalty functional that discourages non-passive (i.e., active) material properties, providing a crucial tool for ensuring physically realistic designs. [@problem_id:3312402]", "problem": "Consider a time-harmonic electromagnetic formulation with the time dependence taken as $e^{-i \\omega t}$ and the constitutive relation $\\mathbf{D}(\\mathbf{r}) = \\epsilon(\\mathbf{r}) \\mathbf{E}(\\mathbf{r})$, where $\\epsilon(\\mathbf{r}) \\in \\mathbb{C}$ is the complex-valued relative permittivity at spatial index $\\mathbf{r}$. Passivity requires the imaginary part of the permittivity to be nonnegative, that is, $\\Im\\{\\epsilon(\\mathbf{r})\\} \\ge 0$, in order to ensure nonnegative cycle-averaged dissipated power density. To enforce passivity in a gradient-based optimizer, define a penalty functional\n$$\n\\phi(\\epsilon) = \\sum_{\\mathbf{r}} \\max\\!\\big(0, -\\Im\\{\\epsilon(\\mathbf{r})\\}\\big)^2,\n$$\nwhich is a real-valued function of the complex design field $\\epsilon(\\mathbf{r})$ over a discrete set of spatial indices. The penalty is dimensionless and the sum is over all indices $\\mathbf{r}$ in the discretization. Your task is to derive the gradient needed for a complex-valued optimizer and implement it.\n\nStarting from the foundational physical principles that determine passivity under the $e^{-i \\omega t}$ convention and the mathematical definitions of the complex variable derivatives, derive the gradient of $\\phi$ with respect to the complex $\\epsilon(\\mathbf{r})$ suitable for use in a complex-valued optimizer. You must treat the complex differentiation carefully using the appropriate calculus for real-valued objectives with complex variables and show how the piecewise definition contributes to the derivative. Avoid shortcuts by beginning from the definition of $\\phi$ and the properties of the maximum function, and proceed using clear, rigorous logic to a gradient that an algorithm can use.\n\nThen, write a complete program that, for each test case listed below, computes:\n- The scalar penalty value $\\,\\phi(\\epsilon)\\,$.\n- A boolean value that is true if and only if the identity $\\,\\frac{\\partial \\phi}{\\partial \\Im\\{\\epsilon\\}} = 2\\,\\Im\\!\\left\\{\\frac{\\partial \\phi}{\\partial \\epsilon^*}\\right\\}\\,$ holds elementwise for the given $\\epsilon$ (with the convention $\\frac{\\partial \\phi}{\\partial \\epsilon^*}$ denoting the Wirtinger derivative).\n- The list of real parts of the Wirtinger gradient $\\,\\frac{\\partial \\phi}{\\partial \\epsilon^*}\\,$ for all entries of $\\epsilon$.\n- The list of imaginary parts of the Wirtinger gradient $\\,\\frac{\\partial \\phi}{\\partial \\epsilon^*}\\,$ for all entries of $\\epsilon$.\n\nYou must not use any physical units in your answer; all quantities here are dimensionless.\n\nUse the following test suite of complex-valued permittivity arrays (each array is one test case, and each entry corresponds to one spatial index):\n1. $\\epsilon = [\\,1 - 0.5 i,\\; 2 + 0.1 i,\\; -1 + 0 i\\,]$.\n2. $\\epsilon = [\\,0 + 0.2 i,\\; 1 + 3 i,\\; -2 + 10^{-9} i\\,]$.\n3. $\\epsilon = [\\,0.3 - 10^{-9} i,\\; 0.0 - 2.0 i,\\; 5.0 + 0.0 i\\,]$.\n4. $\\epsilon = [\\,-0.1 - 0.7 i,\\; 0.4 + 0.0 i,\\; -3.3 - 0.0 i,\\; 2.2 + 0.5 i\\,]$.\n\nYour program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets. Each test case result must itself be a list with four entries in the exact order: $[\\phi, \\text{identity\\_boolean}, \\text{grad\\_real\\_list}, \\text{grad\\_imag\\_list}]$. For example, the outer structure should look like\n$$\n[\\,[\\phi_1, \\text{bool}_1, [\\dots], [\\dots]],\\; [\\phi_2, \\text{bool}_2, [\\dots], [\\dots]],\\; [\\phi_3, \\text{bool}_3, [\\dots], [\\dots]],\\; [\\phi_4, \\text{bool}_4, [\\dots], [\\dots]]\\,].\n$$\nAll scalars must be printed as standard decimal numbers, all booleans as either true or false in lowercase, and all lists as Python-style bracketed lists. No additional text should be printed.", "solution": "The problem requires the derivation of the gradient of a passivity penalty functional, $\\phi(\\epsilon)$, with respect to the complex permittivity field $\\epsilon(\\mathbf{r})$. This is a problem in complex calculus, specifically involving the differentiation of a real-valued function with respect to complex variables, which is handled using Wirtinger calculus.\n\nThe penalty functional is given as\n$$\n\\phi(\\epsilon) = \\sum_{\\mathbf{r}} \\max\\!\\big(0, -\\Im\\{\\epsilon(\\mathbf{r})\\}\\big)^2\n$$\nwhere the sum is over a discrete set of spatial indices $\\mathbf{r}$, and $\\epsilon(\\mathbf{r})$ is the complex permittivity at index $\\mathbf{r}$. The functional $\\phi$ is real-valued. The gradient required for many optimization algorithms, particularly in the context of complex variables, is the Wirtinger derivative with respect to the complex conjugate variable, $\\epsilon^*$.\n\nThe total functional $\\phi$ is a sum of independent terms, one for each spatial index $\\mathbf{r}$. The derivative is a linear operator, so we can analyze the derivative for a single component and the result will generalize. Let us consider a single complex variable $\\epsilon_j = \\epsilon(\\mathbf{r}_j)$ and its corresponding penalty term $\\phi_j = \\max(0, -\\Im\\{\\epsilon_j\\})^2$. The total gradient $\\frac{\\partial \\phi}{\\partial \\epsilon_j^*}$ will then be equal to $\\frac{\\partial \\phi_j}{\\partial \\epsilon_j^*}$.\n\nLet the complex variable $\\epsilon_j$ be represented by its real and imaginary parts: $\\epsilon_j = x_j + i y_j$, where $x_j = \\Re\\{\\epsilon_j\\}$ and $y_j = \\Im\\{\\epsilon_j\\}$. The penalty term $\\phi_j$ can be written as a function of $x_j$ and $y_j$:\n$$\n\\phi_j(x_j, y_j) = \\max(0, -y_j)^2\n$$\nIn Wirtinger calculus, the derivative of a function $f(z)$ with respect to the complex conjugate $z^*$ is defined as:\n$$\n\\frac{\\partial f}{\\partial z^*} = \\frac{1}{2} \\left( \\frac{\\partial f}{\\partial x} + i \\frac{\\partial f}{\\partial y} \\right)\n$$\nwhere $z=x+iy$. To compute $\\frac{\\partial \\phi_j}{\\partial \\epsilon_j^*}$, we first need the partial derivatives of $\\phi_j$ with respect to $x_j$ and $y_j$.\n\nFirst, the partial derivative with respect to $x_j$:\n$$\n\\frac{\\partial \\phi_j}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left[ \\max(0, -y_j)^2 \\right] = 0\n$$\nsince $\\phi_j$ has no explicit or implicit dependence on $x_j$.\n\nNext, the partial derivative with respect to $y_j$. Let $g(y_j) = \\max(0, -y_j)$. Then $\\phi_j = (g(y_j))^2$. Using the chain rule:\n$$\n\\frac{\\partial \\phi_j}{\\partial y_j} = 2 g(y_j) \\cdot \\frac{d g}{d y_j}\n$$\nWe analyze the derivative of $g(y_j)$ piecewise:\n1.  If $-y_j > 0$ (i.e., $y_j < 0$): $g(y_j) = -y_j$. The derivative is $\\frac{d g}{d y_j} = -1$.\n2.  If $-y_j \\le 0$ (i.e., $y_j \\ge 0$): $g(y_j) = 0$. The derivative is $\\frac{d g}{d y_j} = 0$. (At $y_j=0$, the function $\\max(0, -y_j)$ is not differentiable in the strict sense, but it is subdifferentiable. For optimization purposes, taking the derivative to be $0$ is a standard and valid choice).\n\nCombining these cases for $\\frac{\\partial \\phi_j}{\\partial y_j}$:\n1.  If $y_j < 0$:\n    $$\n    \\frac{\\partial \\phi_j}{\\partial y_j} = 2 (-y_j) \\cdot (-1) = 2 y_j\n    $$\n2.  If $y_j \\ge 0$:\n    $$\n    \\frac{\\partial \\phi_j}{\\partial y_j} = 2 (0) \\cdot (0) = 0\n    $$\nThis can be summarized as:\n$$\n\\frac{\\partial \\phi_j}{\\partial y_j} = \\begin{cases} 2 y_j & \\text{if } y_j < 0 \\\\ 0 & \\text{if } y_j \\ge 0 \\end{cases}\n$$\n\nNow we can assemble the Wirtinger derivative $\\frac{\\partial \\phi_j}{\\partial \\epsilon_j^*}$:\n$$\n\\frac{\\partial \\phi_j}{\\partial \\epsilon_j^*} = \\frac{1}{2} \\left( \\frac{\\partial \\phi_j}{\\partial x_j} + i \\frac{\\partial \\phi_j}{\\partial y_j} \\right) = \\frac{1}{2} \\left( 0 + i \\frac{\\partial \\phi_j}{\\partial y_j} \\right) = \\frac{i}{2} \\frac{\\partial \\phi_j}{\\partial y_j}\n$$\nSubstituting our result for $\\frac{\\partial \\phi_j}{\\partial y_j}$:\n1.  If $y_j = \\Im\\{\\epsilon_j\\} < 0$:\n    $$\n    \\frac{\\partial \\phi_j}{\\partial \\epsilon_j^*} = \\frac{i}{2} (2 y_j) = i y_j = i \\Im\\{\\epsilon_j\\}\n    $$\n2.  If $y_j = \\Im\\{\\epsilon_j\\} \\ge 0$:\n    $$\n    \\frac{\\partial \\phi_j}{\\partial \\epsilon_j^*} = \\frac{i}{2} (0) = 0\n    $$\n\nThis is the final form of the gradient for a single component $\\epsilon_j$. The gradient for the entire vector $\\epsilon$ is a vector where each component is computed according to this rule. The gradient is non-zero only for those permittivity values that violate the passivity constraint ($\\Im\\{\\epsilon_j\\} < 0$), and the update direction is purely imaginary, which is expected as the penalty only depends on the imaginary part of $\\epsilon_j$.\n\nThe problem also requires verification of the identity $\\frac{\\partial \\phi}{\\partial \\Im\\{\\epsilon\\}} = 2\\,\\Im\\!\\left\\{\\frac{\\partial \\phi}{\\partial \\epsilon^*}\\right\\}$ for each component. Let's verify this for component $j$.\n\nThe left-hand side (LHS) is $\\frac{\\partial \\phi_j}{\\partial \\Im\\{\\epsilon_j\\}} = \\frac{\\partial \\phi_j}{\\partial y_j}$, which we found to be:\n$$\n\\text{LHS} = \\begin{cases} 2 y_j & \\text{if } y_j < 0 \\\\ 0 & \\text{if } y_j \\ge 0 \\end{cases}\n$$\n\nThe right-hand side (RHS) is $2\\,\\Im\\!\\left\\{\\frac{\\partial \\phi_j}{\\partial \\epsilon_j^*}\\right\\}$. Using our derived gradient:\n$$\n\\text{RHS} = 2\\,\\Im\\!\\left\\{ \\begin{cases} i y_j & \\text{if } y_j < 0 \\\\ 0 & \\text{if } y_j \\ge 0 \\end{cases} \\right\\} = \\begin{cases} 2\\,\\Im\\{i y_j\\} & \\text{if } y_j < 0 \\\\ 2\\,\\Im\\{0\\} & \\text{if } y_j \\ge 0 \\end{cases} = \\begin{cases} 2 y_j & \\text{if } y_j < 0 \\\\ 0 & \\text{if } y_j \\ge 0 \\end{cases}\n$$\nSince LHS = RHS for all values of $y_j$, the identity is confirmed to be true. This identity holds more generally for any real-valued function of a complex variable that depends only on its imaginary part.\n\nThese derived expressions will now be implemented to compute the required quantities for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the passivity penalty and its gradient for several test cases\n    of complex permittivity arrays.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        [1 - 0.5j, 2 + 0.1j, -1 + 0j],\n        [0 + 0.2j, 1 + 3j, -2 + 1e-9j],\n        [0.3 - 1e-9j, 0.0 - 2.0j, 5.0 + 0.0j],\n        [-0.1 - 0.7j, 0.4 + 0.0j, -3.3 - 0.0j, 2.2 + 0.5j]\n    ]\n\n    results = []\n    for case in test_cases:\n        epsilon = np.array(case, dtype=np.complex128)\n        \n        # 1. Compute the scalar penalty value phi\n        im_eps = epsilon.imag\n        # The penalty is active only for negative imaginary parts\n        penalty_terms = np.maximum(0, -im_eps)**2\n        phi = np.sum(penalty_terms)\n        \n        # 2. Compute the Wirtinger gradient (d(phi)/d(epsilon*))\n        # The gradient is i*Im(epsilon) where Im(epsilon) < 0, and 0 otherwise.\n        grad_conj = np.zeros_like(epsilon, dtype=np.complex128)\n        violating_indices = im_eps < 0\n        grad_conj[violating_indices] = 1j * im_eps[violating_indices]\n\n        # 3. Verify the identity: d(phi)/d(Im{epsilon}) = 2*Im{d(phi)/d(epsilon*)}\n        # LHS: d(phi)/d(Im{epsilon})\n        dphi_dy = np.zeros_like(im_eps, dtype=np.float64)\n        # From derivation, d(phi)/d(y) = 2*y for y < 0, and 0 otherwise.\n        dphi_dy[violating_indices] = 2 * im_eps[violating_indices]\n        \n        # RHS: 2*Im{d(phi)/d(epsilon*)}\n        identity_rhs = 2 * grad_conj.imag\n        \n        # Check for element-wise equality using a tolerance\n        identity_holds = np.allclose(dphi_dy, identity_rhs)\n\n        # 4. Extract real and imaginary parts of the gradient for output\n        grad_real_list = grad_conj.real.tolist()\n        grad_imag_list = grad_conj.imag.tolist()\n        \n        # Format the result for the current test case\n        # Booleans need to be lowercase as per problem spec\n        case_result_str = (\n            f\"[{phi},{str(identity_holds).lower()},\"\n            f\"{grad_real_list},{grad_imag_list}]\"\n        )\n        results.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3312402"}, {"introduction": "An analytical gradient is only useful if it is correct, and implementation errors can be subtle and difficult to find. This capstone practice transitions from theory to robust software implementation by having you code a complete adjoint solver for a 2D scalar wave problem and then verify your analytical gradient using the finite-difference method. Mastering gradient validation is an indispensable skill for any researcher or engineer employing gradient-based optimization, as it provides the necessary confidence that your optimizer is being guided by correct information. [@problem_id:3312449]", "problem": "Construct a complete, runnable program that validates an adjoint-based gradient for a frequency-domain scalar electromagnetics model by comparing it against finite-difference checks. The program must implement a two-dimensional discrete scalar Helmholtz problem with zero Dirichlet boundary conditions, an objective function that measures mismatch at selected probe locations with optional quadratic regularization on the material parameter, and both the analytical adjoint gradient and finite-difference gradient approximations. The final output must be a single line containing a list of floating-point numbers that quantify gradient-validation errors for a given test suite.\n\nThe starting point is a frequency-domain scalar Helmholtz equation derived from Maxwellâ€™s equations under a single-polarization, homogeneous magnetic permeability assumption. On a rectangular grid, with nondimensionalized units, the discrete forward problem is\n$$\nA(\\varepsilon) \\, u = b,\n$$\nwhere $u \\in \\mathbb{C}^{N}$ is the vector of field values at interior nodes, $\\varepsilon \\in \\mathbb{R}^{N}$ is the vector of relative permittivity values (one per interior node), and $b \\in \\mathbb{C}^{N}$ is a fixed source. The operator is\n$$\nA(\\varepsilon) = L - c \\,\\mathrm{diag}(\\varepsilon),\n$$\nwhere $L \\in \\mathbb{R}^{N \\times N}$ is the standard $5$-point finite-difference Laplacian with zero Dirichlet boundaries and unit grid spacing, and $c = \\omega^{2} + \\mathrm{i}\\,\\eta$ for angular frequency $\\omega$ and loss parameter $\\eta \\ge 0$. The interior unknowns are the nodes excluding the boundary; for a grid with $N_{x}$ and $N_{y}$ total nodes including boundary layers, the number of unknowns is $N = (N_{x}-2)(N_{y}-2)$.\n\nThe objective function to be minimized is\n$$\nJ(\\varepsilon) = \\tfrac{1}{2}\\,\\lVert S u(\\varepsilon) - u_{\\mathrm{t}}\\rVert_{2}^{2} + \\tfrac{\\alpha}{2}\\,\\lVert \\varepsilon - \\varepsilon_{\\mathrm{ref}} \\rVert_{2}^{2},\n$$\nwhere $S$ is a linear selection operator that extracts the components of $u$ at specified measurement nodes (probes), $u_{\\mathrm{t}}$ is the target field at those probes, $\\alpha \\ge 0$ is a real regularization weight, and $\\varepsilon_{\\mathrm{ref}}$ is a reference permittivity vector. All quantities are nondimensional; no physical units are required.\n\nYour program must:\n- Assemble the interior-grid Laplacian $L$ for each test case using a $5$-point stencil with unit grid spacing and zero Dirichlet boundary conditions.\n- Solve the forward problem $A(\\varepsilon) u = b$ for $u$.\n- Compute the objective $J(\\varepsilon)$.\n- Derive and implement the analytical adjoint gradient $\\nabla_{\\varepsilon} J$ using a principled first-variation and adjoint approach based on the discrete equations and the complex-valued inner products appropriate for frequency-domain electromagnetics.\n- Independently compute a central finite-difference approximation of $\\nabla_{\\varepsilon} J$ with step size $h$ specified below.\n- Perform a directional-derivative check by comparing $\\nabla_{\\varepsilon} J \\cdot p$ against a central-difference approximation of $\\tfrac{\\mathrm{d}}{\\mathrm{d}t} J(\\varepsilon + t p)$ at a small scalar step $t$ along a specified random direction $p$.\n- Report, for each test case, a single floating-point number equal to the maximum of two relative errors: the relative $\\ell_{2}$-error between the analytical and finite-difference gradient vectors, and the relative error between the analytical and finite-difference directional derivatives.\n\nDefinitions and implementation details:\n- Grid and indexing: Let the total grid have $N_{x}$ and $N_{y}$ nodes including boundary layers. The interior index set is $\\{(i,j)\\,|\\, i=0,\\dots,N_{x}-3;\\, j=0,\\dots,N_{y}-3\\}$, with row-major flattening $k = i + (N_{x}-2)\\,j$, yielding $N=(N_{x}-2)(N_{y}-2)$.\n- Laplacian $L$: For each interior node $(i,j)$ with flattened index $k$, set\n  $$\n  L_{k,k} = -4,\\quad\n  L_{k,k\\pm 1} = 1 \\text{ for valid horizontal neighbors},\\quad\n  L_{k,k\\pm (N_{x}-2)} = 1 \\text{ for valid vertical neighbors}.\n  $$\n- Selection operator $S$: Implement $S$ implicitly by indexing the appropriate entries in $u$; its Hermitian transpose $S^{H}$ corresponds to scattering measurement residuals back into the full interior-sized vector at the selected indices.\n- Objective $J$: Use the standard complex least-squares form with Hermitian inner product on the residual, i.e., $\\tfrac{1}{2}\\,\\lVert r \\rVert_{2}^{2}$ where $r = S u - u_{\\mathrm{t}}$, which equals $\\tfrac{1}{2}\\, r^{H} r$.\n- Finite-difference gradient: Use central differences,\n  $$\n  \\left[\\nabla_{\\varepsilon} J\\right]_{i} \\approx \\frac{J(\\varepsilon + h e_{i}) - J(\\varepsilon - h e_{i})}{2h},\n  $$\n  with step size $h = 10^{-6}$, where $e_{i}$ is the $i$-th canonical basis vector.\n- Directional-derivative check: For a given direction $p \\in \\mathbb{R}^{N}$ and step $t = 10^{-6}$, compare $\\nabla_{\\varepsilon} J(\\varepsilon)^{\\top} p$ against\n  $$\n  \\frac{J(\\varepsilon + t p) - J(\\varepsilon - t p)}{2 t}.\n  $$\n\nTest suite:\n- Case A (general case):\n  - Grid: $N_{x} = 10$, $N_{y} = 12$.\n  - Frequency and loss: $\\omega = 5.0$, $\\eta = 0.2$ so $c = \\omega^{2} + \\mathrm{i}\\,\\eta$.\n  - Regularization: $\\alpha = 0.01$, $\\varepsilon_{\\mathrm{ref}}$ is the constant vector with entries $1.5$.\n  - Source: single point source of unit amplitude at interior index $(i_{s}, j_{s})$ with $i_{s} = \\lfloor (N_{x}-2)/3 \\rfloor$, $j_{s} = \\lfloor (N_{y}-2)/2 \\rfloor$.\n  - Initial permittivity: for interior coordinates $(i,j)$, set\n    $$\n    \\varepsilon_{0}(i,j) = 2.0 + 0.1 \\sin\\!\\left( \\frac{2\\pi i}{N_{x}-2} \\right) \\sin\\!\\left( \\frac{3\\pi j}{N_{y}-2} \\right).\n    $$\n  - Measurements: all interior nodes where $i \\bmod 3 = 1$ and $j \\bmod 4 = 2$.\n  - Target field: $u_{\\mathrm{t}}$ is the zero vector of appropriate length.\n- Case B (boundary regularization case):\n  - Grid: $N_{x} = 8$, $N_{y} = 8$.\n  - Frequency and loss: $\\omega = 3.0$, $\\eta = 0.05$, so $c = \\omega^{2} + \\mathrm{i}\\,\\eta$.\n  - Regularization: $\\alpha = 0$, $\\varepsilon_{\\mathrm{ref}}$ is the constant vector with entries $1.0$.\n  - Source: single point source of unit amplitude at the interior center $(i_{c}, j_{c})$ with $i_{c} = \\lfloor (N_{x}-2)/2 \\rfloor$, $j_{c} = \\lfloor (N_{y}-2)/2 \\rfloor$.\n  - Initial permittivity: Gaussian bump,\n    $$\n    \\varepsilon_{0}(i,j) = 1.0 + 0.2 \\exp\\!\\left( -\\frac{(i-i_{c})^{2} + (j-j_{c})^{2}}{2 \\sigma^{2}} \\right), \\quad \\sigma = 2.0.\n    $$\n  - Measurements: single interior probe at $(i_{c}, j_{c})$.\n  - Target field: $u_{\\mathrm{t}}$ is a scalar with value $0.1$ at the single probe.\n- Case C (nearer-to-resonance but stabilized by loss):\n  - Grid: $N_{x} = 12$, $N_{y} = 12$.\n  - Frequency and loss: $\\omega = 10.0$, $\\eta = 0.8$, so $c = \\omega^{2} + \\mathrm{i}\\,\\eta$.\n  - Regularization: $\\alpha = 0.001$, $\\varepsilon_{\\mathrm{ref}}$ is the constant vector with entries $1.0$.\n  - Source: two unit-amplitude point sources at interior indices $(i_{c}, j_{c})$ and $(i_{c}-2, j_{c}+1)$ where $i_{c} = \\lfloor (N_{x}-2)/2 \\rfloor$, $j_{c} = \\lfloor (N_{y}-2)/2 \\rfloor$.\n  - Initial permittivity: uniform,\n    $$\n    \\varepsilon_{0}(i,j) = 1.5.\n    $$\n  - Measurements: cross through the interior center, i.e., all interior nodes with $i = i_{c}$ and $j$ even, together with all interior nodes with $j = j_{c}$ and $i$ even; duplicates are included only once.\n  - Target field: $u_{\\mathrm{t}}$ is the zero vector of appropriate length.\n\nValidation metrics and output:\n- For each case, compute:\n  - The relative $\\ell_{2}$-error between the analytical gradient and the central finite-difference gradient,\n    $$\n    E_{\\mathrm{grad}} = \\frac{\\lVert \\nabla_{\\varepsilon} J - \\nabla_{\\varepsilon}^{\\mathrm{FD}} J \\rVert_{2}}{\\max\\{\\lVert \\nabla_{\\varepsilon}^{\\mathrm{FD}} J \\rVert_{2},\\, 10^{-12}\\}}.\n    $$\n  - The relative error in the directional derivative along a deterministic random direction $p$,\n    $$\n    E_{\\mathrm{dir}} = \\frac{\\left| \\nabla_{\\varepsilon} J^{\\top} p - \\frac{J(\\varepsilon + t p) - J(\\varepsilon - t p)}{2 t} \\right|}{\\max\\left\\{\\left|\\frac{J(\\varepsilon + t p) - J(\\varepsilon - t p)}{2 t}\\right|,\\, 10^{-12}\\right\\}},\n    $$\n    using $t = 10^{-6}$ and a fixed seed for the random number generator unique to each case.\n- For each case, report a single float\n  $$\n  E = \\max\\{E_{\\mathrm{grad}},\\, E_{\\mathrm{dir}}\\}.\n  $$\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[e_A,e_B,e_C]\"), where $e_{\\mathrm{A}}$, $e_{\\mathrm{B}}$, and $e_{\\mathrm{C}}$ are the values of $E$ for Case A, Case B, and Case C, respectively.\n\nNo user input or external files are permitted. All computations must be performed using the specified test suite and parameters. Angles do not appear; there is no angle-unit requirement. All quantities are nondimensional; there is no physical-unit requirement.", "solution": "The user requests a Python program to validate an adjoint-based gradient for a 2D scalar Helmholtz problem against finite-difference approximations. The problem is well-defined, scientifically sound, and provides all necessary parameters and equations for a complete implementation. The physics is based on the standard scalar Helmholtz equation, and the validation methodology (gradient checking via finite differences and directional derivatives) is a cornerstone of gradient-based optimization. The problem is valid.\n\nThe solution proceeds in several steps for each test case:\n1.  **Discretization Setup**: The continuous 2D domain is discretized onto a grid. The interior nodes, where the field is unknown, are flattened into a 1D vector of size $N = (N_x-2)(N_y-2)$. The 5-point finite-difference Laplacian operator $L$ for zero Dirichlet boundary conditions is constructed as a sparse matrix.\n\n2.  **Forward Problem**: The discrete Helmholtz equation is a linear system $A(\\varepsilon)u = b$, where $A(\\varepsilon) = L - c \\cdot \\mathrm{diag}(\\varepsilon)$ with $c = \\omega^2 + i\\eta$. This system is solved for the complex-valued field vector $u$ using a sparse linear solver.\n\n3.  **Objective Function**: The objective function $J(\\varepsilon)$ is defined as the sum of a field mismatch term and a regularization term:\n    $$\n    J(\\varepsilon) = \\tfrac{1}{2}\\,\\lVert S u(\\varepsilon) - u_{\\mathrm{t}}\\rVert_{2}^{2} + \\tfrac{\\alpha}{2}\\,\\lVert \\varepsilon - \\varepsilon_{\\mathrm{ref}} \\rVert_{2}^{2}\n    $$\n    Here, $S$ is a selection operator extracting field values at probe locations. The first term, involving complex vectors, is computed as $\\tfrac{1}{2} r^H r$, where $r = S u(\\varepsilon) - u_{\\mathrm{t}}$ is the residual.\n\n4.  **Adjoint Gradient Derivation**: To compute the gradient $\\nabla_{\\varepsilon} J$ efficiently, the adjoint method is used. We start with the first variation of $J$:\n    $$\n    \\delta J = \\mathrm{Re}\\left[ (Su - u_{\\mathrm{t}})^H S (\\delta u) \\right] + \\alpha (\\varepsilon - \\varepsilon_{\\mathrm{ref}})^{\\top} \\delta\\varepsilon\n    $$\n    The variation of the field, $\\delta u$, is related to the variation of the permittivity, $\\delta\\varepsilon$, through the forward equation:\n    $$\n    \\delta(A u) = 0 \\implies (\\delta A) u + A (\\delta u) = 0 \\implies \\delta u = -A^{-1} (\\delta A) u\n    $$\n    Since $A = L - c \\cdot \\mathrm{diag}(\\varepsilon)$, its variation is $\\delta A = -c \\cdot \\mathrm{diag}(\\delta\\varepsilon)$. This yields:\n    $$\n    \\delta u = c A^{-1} \\mathrm{diag}(u) \\delta\\varepsilon\n    $$\n    Substituting this into the expression for $\\delta J$:\n    $$\n    \\delta J = \\mathrm{Re}\\left[ c \\, (Su - u_{\\mathrm{t}})^H S A^{-1} \\mathrm{diag}(u) \\delta\\varepsilon \\right] + \\alpha (\\varepsilon - \\varepsilon_{\\mathrm{ref}})^{\\top} \\delta\\varepsilon\n    $$\n    To avoid the computationally expensive matrix inverse $A^{-1}$, we introduce an adjoint problem. Let the adjoint source be $r_{\\text{adj}} = S^H(Su - u_{\\mathrm{t}})$. The expression can be rewritten using the property $(X A^{-1} Y) = ((A^{-1})^H X)^H Y = ((A^H)^{-1} X)^H Y$. Let the adjoint field $\\lambda$ be the solution to the adjoint equation:\n    $$\n    A^H \\lambda = r_{\\text{adj}}\n    $$\n    where $A^H = (L - c \\cdot \\mathrm{diag}(\\varepsilon))^H = L^T - c^* \\cdot \\mathrm{diag}(\\varepsilon)^T$. Since $L$ is real and symmetric ($L^T=L$) and $\\varepsilon$ is real, $A^H = L - c^* \\cdot \\mathrm{diag}(\\varepsilon)$.\n    With $\\lambda = (A^H)^{-1} r_{\\text{adj}}$, the variation becomes:\n    $$\n    \\delta J = \\mathrm{Re}\\left[ c \\, \\lambda^H \\mathrm{diag}(u) \\delta\\varepsilon \\right] + \\alpha (\\varepsilon - \\varepsilon_{\\mathrm{ref}})^{\\top} \\delta\\varepsilon\n    $$\n    The term $\\lambda^H \\mathrm{diag}(u) \\delta\\varepsilon$ is equivalent to the inner product of the element-wise product $(\\overline{\\lambda} \\odot u)$ with $\\delta\\varepsilon$, where $\\overline{\\lambda}$ is the complex conjugate of $\\lambda$. Thus, we can identify the gradient:\n    $$\n    \\nabla_{\\varepsilon} J = \\mathrm{Re}[c \\cdot (\\overline{\\lambda} \\odot u)] + \\alpha (\\varepsilon - \\varepsilon_{\\mathrm{ref}})\n    $$\n    where `Re[...]` and the element-wise product `$\\odot$` are applied component-wise. This expression is computationally efficient as it requires only one forward solve (for $u$) and one adjoint solve (for $\\lambda$).\n\n5.  **Finite-Difference Gradient**: For validation, the gradient is also approximated using the central finite-difference formula for each component $i$:\n    $$\n    \\left[\\nabla_{\\varepsilon}^{\\mathrm{FD}} J\\right]_{i} = \\frac{J(\\varepsilon + h e_{i}) - J(\\varepsilon - h e_{i})}{2h}\n    $$\n    This requires $2N$ evaluations of the objective function, making it much slower than the adjoint method but useful for verification.\n\n6.  **Directional Derivative Check**: A further check is performed by comparing the analytical directional derivative, $\\nabla_{\\varepsilon} J^{\\top} p$, with a finite-difference approximation along a random direction $p$:\n    $$\n    \\frac{\\mathrm{d}J(\\varepsilon + t p)}{\\mathrm{d}t}\\bigg|_{t=0} \\approx \\frac{J(\\varepsilon + t p) - J(\\varepsilon - t p)}{2 t}\n    $$\n    This test is computationally cheap (two objective function evaluations) and provides a robust check of the overall gradient implementation.\n\n7.  **Error Calculation**: The validation is quantified by two relative errors: $E_{\\text{grad}}$, the normalized $\\ell_2$ distance between the adjoint and finite-difference gradients, and $E_{\\text{dir}}$, the relative error between the analytical and finite-difference directional derivatives. The final reported error for each test case is $E = \\max\\{E_{\\text{grad}}, E_{\\text{dir}}\\}$.\n\nThe implementation will encapsulate this logic, processing each test case from the provided suite and producing the specified list of error values.", "answer": "```python\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Main function to run the gradient validation program for a set of test cases.\n    \"\"\"\n\n    def run_case(params):\n        \"\"\"\n        Processes a single test case for gradient validation.\n        \"\"\"\n        # Unpack parameters\n        Nx, Ny = params['grid']\n        omega, eta = params['freq_loss']\n        alpha, eps_ref_val = params['reg']\n        source_locs = params['source_locs']\n        eps0_func = params['eps0_func']\n        probe_cond = params['probe_cond']\n        ut_def = params['ut_def']\n        seed = params['seed']\n\n        # h for full gradient FD, t for directional derivative FD\n        h = 1e-6\n        t = 1e-6\n\n        # Grid and indexing setup\n        nx_int, ny_int = Nx - 2, Ny - 2\n        N = nx_int * ny_int\n        \n        # Create 1D grid coord arrays for vectorized eps0 calculation\n        i_coords = np.arange(nx_int)\n        j_coords = np.arange(ny_int)\n        i_grid, j_grid = np.meshgrid(i_coords, j_coords, indexing='ij')\n\n        # Build sparse 5-point Laplacian L with zero Dirichlet BCs\n        def build_laplacian(ni, nj):\n            num_nodes = ni * nj\n            diagonals = [\n                np.ones(num_nodes),    # Main diagonal fill\n                np.ones(num_nodes-1),  # Off-diagonal for i neighbors\n                np.ones(num_nodes-1),  # Off-diagonal for i neighbors\n                np.ones(num_nodes-ni), # Off-diagonal for j neighbors\n                np.ones(num_nodes-ni)  # Off-diagonal for j neighbors\n            ]\n            diagonals[0] = -4.0 * diagonals[0]\n            offsets = [0, -1, 1, -ni, ni]\n            L = sp.diags(diagonals, offsets, shape=(num_nodes, num_nodes), format='csr')\n\n            # Remove connections between rows for the +/- 1 diagonals\n            for j in range(1, nj):\n                k = j * ni - 1\n                if k < num_nodes - 1:\n                    L[k, k + 1] = 0\n                    L[k + 1, k] = 0\n            return L\n\n        L = build_laplacian(nx_int, ny_int)\n\n        # Setup problem constants and vectors\n        c = omega**2 + 1j * eta\n        eps0 = eps0_func(i_grid, j_grid).flatten() # Use default C (row-major) order\n        eps_ref = np.full(N, eps_ref_val)\n\n        b = np.zeros(N, dtype=complex)\n        for isrc, jsrc in source_locs:\n            k_src = isrc + nx_int * jsrc\n            b[k_src] = 1.0\n\n        probe_indices = [i + nx_int * j for i in i_coords for j in j_coords if probe_cond(i, j)]\n        # Ensure probe_indices are unique and sorted for consistent behavior\n        probe_indices = sorted(list(set(probe_indices)))\n\n        u_t = ut_def(len(probe_indices))\n\n        # Helper function to compute the objective J(eps)\n        def compute_objective(epsilon_vec):\n            A = L - c * sp.diags(epsilon_vec)\n            u = spsolve(A, b)\n            \n            residual = u[probe_indices] - u_t\n            J_field = 0.5 * np.real(np.vdot(residual, residual))\n            J_reg = 0.5 * alpha * np.linalg.norm(epsilon_vec - eps_ref)**2\n            return J_field + J_reg\n\n        # 1. Analytical Adjoint Gradient\n        A0 = L - c * sp.diags(eps0)\n        u0 = spsolve(A0, b)\n        \n        residual = u0[probe_indices] - u_t\n        adj_source = np.zeros(N, dtype=complex)\n        adj_source[probe_indices] = residual\n        \n        AH = L.T - np.conj(c) * sp.diags(eps0) # L is symmetric, so L.T = L\n        lambda_vec = spsolve(AH, adj_source)\n        \n        grad_adj_field = np.real(c * np.conj(lambda_vec) * u0)\n        grad_adj_reg = alpha * (eps0 - eps_ref)\n        grad_adj = grad_adj_field + grad_adj_reg\n\n        # 2. Finite-Difference Gradient\n        grad_fd = np.zeros(N)\n        for i in range(N):\n            eps_plus = eps0.copy()\n            eps_minus = eps0.copy()\n            eps_plus[i] += h\n            eps_minus[i] -= h\n            J_plus = compute_objective(eps_plus)\n            J_minus = compute_objective(eps_minus)\n            grad_fd[i] = (J_plus - J_minus) / (2.0 * h)\n\n        # 3. Directional Derivative Check\n        rng = np.random.default_rng(seed)\n        p = rng.random(N)\n        \n        dir_deriv_adj = np.dot(grad_adj, p)\n        \n        J_plus_p = compute_objective(eps0 + t * p)\n        J_minus_p = compute_objective(eps0 - t * p)\n        dir_deriv_fd = (J_plus_p - J_minus_p) / (2.0 * t)\n\n        # 4. Compute Errors\n        E_grad_num = np.linalg.norm(grad_adj - grad_fd)\n        E_grad_den = max(np.linalg.norm(grad_fd), 1e-12)\n        E_grad = E_grad_num / E_grad_den\n\n        E_dir_num = np.abs(dir_deriv_adj - dir_deriv_fd)\n        E_dir_den = max(np.abs(dir_deriv_fd), 1e-12)\n        E_dir = E_dir_num / E_dir_den\n        \n        return max(E_grad, E_dir)\n\n    # Test Suite Definition\n    test_cases = [\n        # Case A\n        {\n            'grid': (10, 12), 'freq_loss': (5.0, 0.2), 'reg': (0.01, 1.5),\n            'source_locs': [(int((10-2)/3), int((12-2)/2))],\n            'eps0_func': lambda i, j: 2.0 + 0.1 * np.sin(2*np.pi*i/(10-2)) * np.sin(3*np.pi*j/(12-2)),\n            'probe_cond': lambda i, j: i % 3 == 1 and j % 4 == 2,\n            'ut_def': lambda n_probes: np.zeros(n_probes),\n            'seed': 1\n        },\n        # Case B\n        {\n            'grid': (8, 8), 'freq_loss': (3.0, 0.05), 'reg': (0.0, 1.0),\n            'source_locs': [(int((8-2)/2), int((8-2)/2))],\n            'eps0_func': lambda i, j: (lambda ic, jc, s: 1.0 + 0.2 * np.exp(-((i-ic)**2 + (j-jc)**2) / (2*s**2)))(int((8-2)/2), int((8-2)/2), 2.0),\n            'probe_cond': lambda i, j: i == int((8-2)/2) and j == int((8-2)/2),\n            'ut_def': lambda n_probes: np.array([0.1]),\n            'seed': 2\n        },\n        # Case C\n        {\n            'grid': (12, 12), 'freq_loss': (10.0, 0.8), 'reg': (0.001, 1.0),\n            'source_locs': [(int((12-2)/2), int((12-2)/2)), (int((12-2)/2)-2, int((12-2)/2)+1)],\n            'eps0_func': lambda i, j: np.full_like(i, 1.5, dtype=float),\n            'probe_cond': lambda i, j: (i == int((12-2)/2) and j % 2 == 0) or (j == int((12-2)/2) and i % 2 == 0),\n            'ut_def': lambda n_probes: np.zeros(n_probes),\n            'seed': 3\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        error = run_case(case)\n        results.append(error)\n\n    # Print results in the required format\n    print(f\"[{','.join(f'{r:.8e}' for r in results)}]\")\n\nsolve()\n```", "id": "3312449"}]}