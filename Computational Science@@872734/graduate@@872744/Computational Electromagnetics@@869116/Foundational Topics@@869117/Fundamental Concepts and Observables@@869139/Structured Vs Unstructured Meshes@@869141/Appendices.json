{"hands_on_practices": [{"introduction": "A primary consideration when choosing a mesh is the resulting number of degrees of freedom (DoFs), as this directly impacts memory requirements and computational cost. This foundational exercise provides a direct, analytical comparison of how DoFs scale for a structured hexahedral grid versus an unstructured tetrahedral mesh discretizing the same volume. By working through the combinatorial counting, you will gain a concrete understanding of the inherent density differences between these common mesh types. [@problem_id:3351211]", "problem": "Consider the time-harmonic Maxwell curl-curl formulation in a cubic cavity domain $\\Omega = [0,L]^{3}$ with side length $L$. Suppose the electric field is discretized using first-order curl-conforming Nédélec edge elements (first kind), so that each unique mesh edge carries exactly one scalar degree of freedom. Ignore boundary condition constraints; that is, count every mesh edge as an independent unknown.\n\nTwo meshes of $\\Omega$ are considered, both parameterized by a positive integer $n$ via the relation $h = L/n$:\n\n- A structured hexahedral mesh: the cube is partitioned into $n$ uniform intervals per axis, yielding $n^{3}$ congruent hexahedral cells of side length $h$.\n\n- A tetrahedral mesh whose average edge length is $\\Theta(h)$, obtained by subdividing each hexahedral cell of the structured grid into $6$ tetrahedra using a consistent Freudenthal-type diagonalization: in each hexahedron, insert the long space diagonal and select one diagonal per quadrilateral face, with the diagonal choices consistent across neighboring cells so that shared faces have the same diagonal.\n\nUsing only the following foundational facts: (i) first-order Nédélec edge elements associate one degree of freedom per unique mesh edge, and (ii) the numbers of hexahedral grid nodes, edges, faces, and cells can be counted combinatorially as functions of $n$, do the following:\n\n- Derive the exact total number of edge unknowns $N_{\\mathrm{hex}}(n)$ on the structured hexahedral mesh in terms of $n$.\n\n- Derive the exact total number of edge unknowns $N_{\\mathrm{tet}}(n)$ on the tetrahedral mesh constructed as above, in terms of $n$.\n\n- From these expressions, determine the leading-order scaling with $h$ for each mesh and then compute the asymptotic ratio\n$$\n\\lim_{h \\to 0^{+}} \\frac{N_{\\mathrm{tet}}(h)}{N_{\\mathrm{hex}}(h)} \\, ,\n$$\nexpressed purely as a function of constants (i.e., independent of $h$ and $L$). Here $N_{\\mathrm{hex}}(h)$ and $N_{\\mathrm{tet}}(h)$ denote the counts expressed in terms of $h$ via $n=L/h$.\n\nReport the final asymptotic ratio as a single simplified rational number. No rounding is required. The answer is dimensionless; do not include units.", "solution": "The problem requires the derivation of the number of degrees of freedom (DoFs) for two different mesh discretizations of a cubic domain $\\Omega = [0,L]^3$. The DoFs correspond to the number of unique mesh edges, as first-order Nédélec elements are used. The analysis must be performed for a structured hexahedral mesh and a derived tetrahedral mesh. Finally, the asymptotic ratio of the number of edges for the two meshes must be computed as the mesh size $h$ approaches zero.\n\nFirst, we derive the exact number of edge unknowns $N_{\\mathrm{hex}}(n)$ for the structured hexahedral mesh. This mesh is formed by partitioning the cube into $n$ intervals along each of the three axes. This creates a grid of $(n+1) \\times (n+1) \\times (n+1)$ vertices, which can be indexed by $(i,j,k)$ where $i, j, k \\in \\{0, 1, \\dots, n\\}$. The edges of this grid are the lines connecting adjacent vertices. We can count the edges by classifying them according to their orientation.\n\n1.  Edges parallel to the $x$-axis: These edges connect vertices of the form $(i, j, k)$ and $(i+1, j, k)$. The index $i$ can range from $0$ to $n-1$ (a total of $n$ choices), while indices $j$ and $k$ can each range from $0$ to $n$ (a total of $n+1$ choices each). Thus, the number of edges parallel to the $x$-axis is $n \\times (n+1) \\times (n+1)$.\n\n2.  Edges parallel to the $y$-axis: Similarly, these edges connect vertices $(i, j, k)$ and $(i, j+1, k)$. The index $j$ runs from $0$ to $n-1$, while $i$ and $k$ run from $0$ to $n$. The number of such edges is $(n+1) \\times n \\times (n+1)$.\n\n3.  Edges parallel to the $z$-axis: These connect vertices $(i, j, k)$ and $(i, j, k+1)$. The index $k$ runs from $0$ to $n-1$, while $i$ and $j$ run from $0$ to $n$. The number is $(n+1) \\times (n+1) \\times n$.\n\nThe total number of edges, $N_{\\mathrm{hex}}(n)$, is the sum of these three counts. By symmetry, the counts are identical.\n$$\nN_{\\mathrm{hex}}(n) = 3 \\times n \\times (n+1)^2\n$$\nExpanding this expression gives the polynomial form:\n$$\nN_{\\mathrm{hex}}(n) = 3n(n^2 + 2n + 1) = 3n^3 + 6n^2 + 3n\n$$\nNext, we derive the exact number of edge unknowns $N_{\\mathrm{tet}}(n)$ for the tetrahedral mesh. This mesh is constructed by subdividing each of the $n^3$ hexahedral cells of the original grid. The problem specifies that the subdivision adds new edges to the mesh: one long space diagonal is inserted within each hexahedron, and one diagonal is selected for each quadrilateral face. The choices of face diagonals are consistent across shared faces.\n\nThe total set of edges in the tetrahedral mesh is therefore the union of three disjoint sets of edges:\n1.  The original edges of the hexahedral grid. The number of these edges is $N_{\\mathrm{hex}}(n) = 3n(n+1)^2$.\n2.  Newly created space diagonals. Each of the $n^3$ hexahedral cells has one space diagonal added to its interior. Since these diagonals are internal to the cells, they are not shared. Thus, this adds exactly $n^3$ new edges.\n3.  Newly created face diagonals. The problem states that a single diagonal is introduced for each face of the hexahedral grid. This means the number of new face diagonals is equal to the total number of faces in the hexahedral grid. We can count these faces by orientation:\n    -   Faces parallel to the $xy$-plane: There are $n \\times n$ such faces in each of the $n+1$ horizontal layers of vertices. Total: $n^2(n+1)$.\n    -   Faces parallel to the $yz$-plane: There are $n \\times n$ such faces in each of the $n+1$ sagittal planes. Total: $n^2(n+1)$.\n    -   Faces parallel to the $xz$-plane: There are $n \\times n$ such faces in each of the $n+1$ coronal planes. Total: $n^2(n+1)$.\n    The total number of faces is the sum, which is $3n^2(n+1)$. Therefore, $3n^2(n+1)$ new edges are created from face diagonals.\n\nThe total number of edges in the tetrahedral mesh, $N_{\\mathrm{tet}}(n)$, is the sum of the counts from these three sources:\n$$\nN_{\\mathrm{tet}}(n) = N_{\\mathrm{hex}}(n) + (\\text{number of new space diagonals}) + (\\text{number of new face diagonals})\n$$\n$$\nN_{\\mathrm{tet}}(n) = [3n(n+1)^2] + [n^3] + [3n^2(n+1)]\n$$\nWe expand each term and sum them:\n$$\nN_{\\mathrm{tet}}(n) = (3n^3 + 6n^2 + 3n) + n^3 + (3n^3 + 3n^2)\n$$\n$$\nN_{\\mathrm{tet}}(n) = (3+1+3)n^3 + (6+3)n^2 + 3n\n$$\n$$\nN_{\\mathrm{tet}}(n) = 7n^3 + 9n^2 + 3n\n$$\nFinally, we compute the asymptotic ratio $\\lim_{h \\to 0^{+}} \\frac{N_{\\mathrm{tet}}(h)}{N_{\\mathrm{hex}}(h)}$. The relationship between the mesh parameter $n$ and the mesh size $h$ is given by $h = L/n$. As $h \\to 0^+$, for a fixed domain size $L$, the integer $n$ must approach infinity ($n \\to \\infty$). Therefore, the limit can be computed with respect to $n$:\n$$\n\\lim_{h \\to 0^{+}} \\frac{N_{\\mathrm{tet}}(h)}{N_{\\mathrm{hex}}(h)} = \\lim_{n \\to \\infty} \\frac{N_{\\mathrm{tet}}(n)}{N_{\\mathrm{hex}}(n)}\n$$\nWe substitute the derived polynomial expressions for $N_{\\mathrm{tet}}(n)$ and $N_{\\mathrm{hex}}(n)$:\n$$\n\\lim_{n \\to \\infty} \\frac{7n^3 + 9n^2 + 3n}{3n^3 + 6n^2 + 3n}\n$$\nFor a limit of a rational function of polynomials as the variable approaches infinity, the limit is the ratio of the coefficients of the highest power term in the numerator and the denominator. The highest power is $n^3$.\n$$\n\\lim_{n \\to \\infty} \\frac{n^3(7 + \\frac{9}{n} + \\frac{3}{n^2})}{n^3(3 + \\frac{6}{n} + \\frac{3}{n^2})} = \\lim_{n \\to \\infty} \\frac{7 + \\frac{9}{n} + \\frac{3}{n^2}}{3 + \\frac{6}{n} + \\frac{3}{n^2}}\n$$\nAs $n \\to \\infty$, terms such as $\\frac{9}{n}$, $\\frac{3}{n^2}$, and $\\frac{6}{n}$ approach $0$.\n$$\n\\frac{7 + 0 + 0}{3 + 0 + 0} = \\frac{7}{3}\n$$\nThe leading-order scaling for both $N_{\\mathrm{hex}}$ and $N_{\\mathrm{tet}}$ is $O(n^3)$, which corresponds to $O(h^{-3})$. The asymptotic ratio is the ratio of the leading-order coefficients, $7/3$.", "answer": "$$\n\\boxed{\\frac{7}{3}}\n$$", "id": "3351211"}, {"introduction": "Beyond the sheer number of elements, the quality of those elements is paramount to obtaining an accurate and numerically stable solution. While unstructured meshes offer great flexibility in conforming to complex geometries, they run the risk of generating poorly shaped or \"sliver\" elements. This practice explores the severe impact these degenerate elements can have on the conditioning of the discrete curl-curl operator, which is fundamental to the stability and convergence of the numerical solution. [@problem_id:3351151]", "problem": "Consider the time-harmonic Maxwell equations in a simply connected, bounded three-dimensional domain with perfectly conducting boundary. In the absence of sources, the electric field satisfies the weak form that involves the curl-curl operator. Let the discrete approximation use first-order edge elements of Jean-Claude Nédélec on a tetrahedral mesh. The discrete stiffness matrix is assembled from element contributions defined by the bilinear form of the curl-curl operator. Two meshes of the same domain are provided: a baseline mesh and a perturbed mesh that contains a set of sliver tetrahedra. You are asked to estimate the expected increase in the condition number of the global curl-curl stiffness matrix due to the presence of sliver elements, using a shape-quality-based spectral equivalence argument derived from the element mapping, and then propose and model the effect of a remedial refinement strategy that targets the sliver elements.\n\nFundamental base and definitions to be used in the derivation and computation:\n- The time-harmonic Maxwell equations in a source-free medium with constant material properties reduce to the eigenproblem of the curl-curl operator. The weak form on a domain $\\Omega$ for the electric field $\\mathbf{E}$ and test functions $\\mathbf{v}$ in the $\\mathbf{H}(\\mathrm{curl},\\Omega)$ space is given by\n$$\na(\\mathbf{E},\\mathbf{v}) = \\int_{\\Omega} \\mu^{-1} \\, (\\nabla \\times \\mathbf{E}) \\cdot (\\nabla \\times \\mathbf{v}) \\, \\mathrm{d}x,\n$$\nwhere $\\mu$ is the magnetic permeability, and the stiffness matrix entries are $A_{ij} = a(\\mathbf{N}_i,\\mathbf{N}_j)$ with $\\mathbf{N}_i$ being the Nédélec basis functions.\n- Each physical tetrahedral element $K$ is obtained from an affine mapping of a reference tetrahedron via $F_K(\\boldsymbol{\\xi}) = \\mathbf{x}_0 + \\mathbf{J}_K \\boldsymbol{\\xi}$, where $\\mathbf{J}_K$ is the Jacobian matrix on $K$. The curl transformation under the mapping yields a metric tensor $\\mathbf{G}_K = \\mathbf{J}_K^{-T} \\mathbf{J}_K^{-1}$ entering the element bilinear form after pullback. The element-wise contribution to the stiffness is spectrally equivalent to the reference element contribution scaled by the eigenvalues of $\\mathbf{G}_K$.\n- Let the singular values of $\\mathbf{J}_K$ be $\\sigma_1(K) \\ge \\sigma_2(K) \\ge \\sigma_3(K) > 0$. Define the element shape regularity factor $\\kappa(K) = \\sigma_1(K)/\\sigma_3(K)$. Sliver tetrahedra have large $\\kappa(K)$ due to a very small $\\sigma_3(K)$ compared to $\\sigma_1(K)$.\n- For a mesh $\\mathcal{T}$, define $h_{\\min}(\\mathcal{T})$ as the minimum characteristic size among elements and $\\kappa_{\\max}(\\mathcal{T}) = \\max_{K\\in \\mathcal{T}} \\kappa(K)$. Denote the domain diameter by $H$. In the absence of material contrast and for quasi-uniform meshes, a widely used estimate for the discrete condition number of the global stiffness matrix for the curl-curl operator is\n$$\n\\operatorname{cond}(A(\\mathcal{T})) \\lesssim C \\, \\left(\\frac{H}{h_{\\min}(\\mathcal{T})}\\right)^2 \\, \\kappa_{\\max}(\\mathcal{T})^2,\n$$\nwith a mesh-independent constant $C$, reflecting that poor element shapes and large size ratios degrade conditioning. When comparing two meshes of the same domain, the ratio of their condition numbers satisfies\n$$\nR(\\mathcal{T}_B \\Vert \\mathcal{T}_A) \\approx \\left(\\frac{h_{\\min}(\\mathcal{T}_A)}{h_{\\min}(\\mathcal{T}_B)}\\right)^2 \\left(\\frac{\\kappa_{\\max}(\\mathcal{T}_B)}{\\kappa_{\\max}(\\mathcal{T}_A)}\\right)^2,\n$$\nso the constant and $H$ cancel in the ratio. This ratio is dimensionless, and all answers must therefore be dimensionless real numbers.\n\nRemedial refinement model:\n- Identify sliver elements by a threshold $\\tau$: an element $K$ is classified as a sliver if $\\kappa(K) \\ge \\tau$.\n- Apply a local refinement and smoothing strategy conceptually similar to longest-edge bisection followed by vertex smoothing in the sliver region. In the spectral model, this increases the smallest singular value of the Jacobian by a factor $\\eta > 1$ while leaving the largest singular value unchanged, i.e., $\\sigma_3'(K) = \\min\\{\\eta \\, \\sigma_3(K), \\sigma_1(K)\\}$ for sliver elements and $\\sigma_3'(K) = \\sigma_3(K)$ otherwise. This decreases $\\kappa(K)$ by approximately a factor of $\\eta$ for slivers. Assume $h_{\\min}$ does not deteriorate under this remediation in these test cases, since the strategy targets shape without reducing sizes.\n- The refined mesh condition number ratio is then\n$$\nR'(\\mathcal{T}_B \\Vert \\mathcal{T}_A) \\approx \\left(\\frac{h_{\\min}(\\mathcal{T}_A)}{h_{\\min}(\\mathcal{T}_B)}\\right)^2 \\left(\\frac{\\kappa_{\\max}'(\\mathcal{T}_B)}{\\kappa_{\\max}(\\mathcal{T}_A)}\\right)^2,\n$$\nwhere $\\kappa_{\\max}'(\\mathcal{T}_B)$ is computed from $\\sigma_1(K)$ and $\\sigma_3'(K)$.\n\nYour task:\n- For each test case, compute the estimated condition number increase $R(\\mathcal{T}_B \\Vert \\mathcal{T}_A)$ using the formula above, and the refined ratio $R'(\\mathcal{T}_B \\Vert \\mathcal{T}_A)$ after applying the remedial refinement model.\n- Report the results as floats.\n\nTest suite and data:\n- Each mesh is specified by a list of tetrahedral elements, each element by a tuple of its Jacobian singular values $(\\sigma_1,\\sigma_2,\\sigma_3)$ and a characteristic size $h_K$. For each mesh, use $h_{\\min}$ equal to the minimum over its $h_K$ values. For each test case, also specify the sliver threshold $\\tau$ and the improvement factor $\\eta$ for the remedial refinement. All numbers are given below.\n\n- Test case $1$ (happy path, structured baseline versus unstructured with slivers):\n  Baseline mesh $\\mathcal{T}_A$ elements:\n  $[(1.00,0.95,0.90,0.25),(1.00,0.96,0.88,0.25),(1.00,0.94,0.89,0.25),(1.00,0.97,0.91,0.25),(1.00,0.95,0.92,0.25),(1.00,0.96,0.93,0.25),(1.00,0.94,0.92,0.25),(1.00,0.97,0.93,0.25)]$.\n  Perturbed mesh $\\mathcal{T}_B$ elements:\n  $[(1.00,0.99,0.02,0.25),(1.00,0.98,0.03,0.25),(1.00,0.97,0.04,0.25),(1.00,0.95,0.92,0.25),(1.00,0.96,0.93,0.25),(1.00,0.94,0.90,0.25),(1.00,0.95,0.91,0.25),(1.00,0.96,0.92,0.25)]$.\n  Parameters: $\\tau = 10$, $\\eta = 5$.\n\n- Test case $2$ (boundary case, both meshes near-structured, minimal sliver presence):\n  Baseline mesh $\\mathcal{T}_A$ elements:\n  $[(0.50,0.50,0.50,0.10),(0.50,0.50,0.50,0.10),(0.50,0.50,0.50,0.10),(0.50,0.50,0.50,0.10),(0.50,0.50,0.50,0.10),(0.50,0.50,0.50,0.10),(0.50,0.50,0.50,0.10),(0.50,0.50,0.50,0.10),(0.50,0.50,0.50,0.10),(0.50,0.50,0.50,0.10)]$.\n  Perturbed mesh $\\mathcal{T}_B$ elements:\n  $[(0.55,0.50,0.45,0.10),(0.60,0.50,0.40,0.10),(0.52,0.50,0.48,0.10),(0.52,0.50,0.48,0.10),(0.52,0.50,0.48,0.10),(0.52,0.50,0.48,0.10),(0.52,0.50,0.48,0.10),(0.52,0.50,0.48,0.10),(0.52,0.50,0.48,0.10),(0.52,0.50,0.48,0.10)]$.\n  Parameters: $\\tau = 10$, $\\eta = 5$.\n\n- Test case $3$ (edge case, extreme slivers in unstructured mesh):\n  Baseline mesh $\\mathcal{T}_A$ elements:\n  $[(1.00,0.80,0.40,0.30),(1.00,0.85,0.45,0.30),(1.00,0.82,0.42,0.30),(1.00,0.83,0.43,0.30),(1.00,0.84,0.44,0.30)]$.\n  Perturbed mesh $\\mathcal{T}_B$ elements:\n  $[(1.00,0.999,0.001,0.30),(1.00,0.995,0.002,0.30),(1.00,0.90,0.60,0.30),(1.00,0.85,0.50,0.30),(1.00,0.88,0.55,0.30)]$.\n  Parameters: $\\tau = 10$, $\\eta = 20$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output the two-element list $[R,R']$ where $R$ is the estimated condition number ratio and $R'$ is the refined ratio. Aggregate the three test cases into one list, for example, $[[R_1,R_1'],[R_2,R_2'],[R_3,R_3']]$. No physical units are required because the ratios are dimensionless.", "solution": "The starting point is the time-harmonic Maxwell equations in a source-free, homogeneous medium on a domain with perfectly conducting boundary. The weak form employs the curl-curl operator and, when discretized by first-order Nédélec edge elements, yields a global stiffness matrix that assembles element-wise contributions of the bilinear form \n$$\na(\\mathbf{E},\\mathbf{v}) = \\int_{\\Omega} \\mu^{-1} \\, (\\nabla \\times \\mathbf{E}) \\cdot (\\nabla \\times \\mathbf{v}) \\, \\mathrm{d}x.\n$$\nThe assembly process maps each physical tetrahedron $K$ from the reference element via an affine mapping $F_K(\\boldsymbol{\\xi}) = \\mathbf{x}_0 + \\mathbf{J}_K \\boldsymbol{\\xi}$ with Jacobian $\\mathbf{J}_K$. The curl transforms as $\\nabla_{\\mathbf{x}} \\times \\mathbf{E}(\\mathbf{x}) = (\\det \\mathbf{J}_K)^{-1} \\mathbf{J}_K \\, (\\nabla_{\\boldsymbol{\\xi}} \\times \\hat{\\mathbf{E}}(\\boldsymbol{\\xi}))$, where hats denote quantities on the reference element. Consequently, the element bilinear form after pullback acquires the metric tensor \n$$\n\\mathbf{G}_K = \\mathbf{J}_K^{-T} \\mathbf{J}_K^{-1},\n$$ \nin the sense that the reference-element contribution is scaled by $\\mathbf{G}_K$ through the factor $\\int_{\\hat{K}} (\\nabla_{\\boldsymbol{\\xi}} \\times \\hat{\\mathbf{N}}_i) \\cdot \\mathbf{G}_K \\, (\\nabla_{\\boldsymbol{\\xi}} \\times \\hat{\\mathbf{N}}_j) \\, \\mathrm{d}\\boldsymbol{\\xi}$. The eigenvalues of $\\mathbf{G}_K$ are $\\lambda_i(\\mathbf{G}_K) = \\sigma_i(\\mathbf{J}_K)^{-2}$, where $\\sigma_i(\\mathbf{J}_K)$ are the singular values of $\\mathbf{J}_K$. The spectral conditioning of $\\mathbf{G}_K$ is thus governed by the ratio \n$$\n\\frac{\\lambda_{\\max}(\\mathbf{G}_K)}{\\lambda_{\\min}(\\mathbf{G}_K)} = \\left(\\frac{\\sigma_1(K)}{\\sigma_3(K)}\\right)^2 = \\kappa(K)^2,\n$$\nwith $\\kappa(K) = \\sigma_1(K)/\\sigma_3(K)$ serving as a shape regularity factor. Sliver tetrahedra have $\\sigma_3(K)$ much smaller than $\\sigma_1(K)$, yielding large $\\kappa(K)$ and consequently large local stiffness contributions that vary widely, which causes poor global conditioning when assembled.\n\nFor a mesh $\\mathcal{T}$, the global stiffness matrix condition number scales with both the mesh size ratio and element shape quality. A standard estimate for conforming finite elements under reasonable quasi-uniformity assumptions is \n$$\n\\operatorname{cond}(A(\\mathcal{T})) \\lesssim C \\, \\left(\\frac{H}{h_{\\min}(\\mathcal{T})}\\right)^2 \\, \\kappa_{\\max}(\\mathcal{T})^2,\n$$\nwhere $H$ is the domain diameter, $h_{\\min}(\\mathcal{T})$ is the minimum element size, $\\kappa_{\\max}(\\mathcal{T}) = \\max_{K\\in \\mathcal{T}} \\kappa(K)$ is the worst shape factor, and $C$ is a mesh-independent constant that depends on polynomial order but cancels in ratios. For two meshes $\\mathcal{T}_A$ and $\\mathcal{T}_B$ of the same domain, the ratio of their condition numbers satisfies \n$$\nR(\\mathcal{T}_B \\Vert \\mathcal{T}_A) \\approx \\left(\\frac{h_{\\min}(\\mathcal{T}_A)}{h_{\\min}(\\mathcal{T}_B)}\\right)^2 \\left(\\frac{\\kappa_{\\max}(\\mathcal{T}_B)}{\\kappa_{\\max}(\\mathcal{T}_A)}\\right)^2,\n$$\nbecause $H$ is identical and $C$ cancels out. This provides a physics- and geometry-based estimate grounded in the mapping and the $\\mathbf{H}(\\mathrm{curl})$ element theory.\n\nTo model a remedial refinement strategy focusing on slivers, consider a threshold $\\tau$ and an improvement factor $\\eta$. Identify slivers by $\\kappa(K) \\ge \\tau$. A local refinement such as longest-edge bisection combined with vertex smoothing increases the smallest singular value while largely preserving the largest singular value for slivers, thereby improving shape quality. In spectral terms,\n$$\n\\sigma_3'(K) = \\min\\{\\eta \\, \\sigma_3(K), \\sigma_1(K)\\}, \\quad \\sigma_1'(K) = \\sigma_1(K),\n$$\nand for non-sliver elements, $\\sigma_3'(K)=\\sigma_3(K)$. The refined shape factor becomes \n$$\n\\kappa'(K) = \\frac{\\sigma_1'(K)}{\\sigma_3'(K)} = \\frac{\\sigma_1(K)}{\\min\\{\\eta \\, \\sigma_3(K), \\sigma_1(K)\\}},\n$$\nwhich is approximately $\\kappa(K)/\\eta$ for $\\eta \\, \\sigma_3(K) \\le \\sigma_1(K)$. The refined worst-case factor is $\\kappa_{\\max}'(\\mathcal{T}_B) = \\max_{K\\in \\mathcal{T}_B} \\kappa'(K)$. Assuming the refinement targets shape quality without reducing the smallest element size in the test scenarios, $h_{\\min}(\\mathcal{T}_B)$ is unchanged, and the refined ratio is \n$$\nR'(\\mathcal{T}_B \\Vert \\mathcal{T}_A) \\approx \\left(\\frac{h_{\\min}(\\mathcal{T}_A)}{h_{\\min}(\\mathcal{T}_B)}\\right)^2 \\left(\\frac{\\kappa_{\\max}'(\\mathcal{T}_B)}{\\kappa_{\\max}(\\mathcal{T}_A)}\\right)^2.\n$$\n\nAlgorithmic steps implemented by the program for each test case:\n- Compute $\\kappa(K) = \\sigma_1(K)/\\sigma_3(K)$ for every element in $\\mathcal{T}_A$ and $\\mathcal{T}_B$.\n- Determine $\\kappa_{\\max}(\\mathcal{T}_A)$ and $\\kappa_{\\max}(\\mathcal{T}_B)$.\n- Compute $h_{\\min}(\\mathcal{T}_A)$ and $h_{\\min}(\\mathcal{T}_B)$ from the element sizes.\n- Evaluate \n$$\nR = \\left(\\frac{h_{\\min}(\\mathcal{T}_A)}{h_{\\min}(\\mathcal{T}_B)}\\right)^2 \\left(\\frac{\\kappa_{\\max}(\\mathcal{T}_B)}{\\kappa_{\\max}(\\mathcal{T}_A)}\\right)^2.\n$$\n- Apply the remedial refinement model to all sliver elements in $\\mathcal{T}_B$: for each $K$ with $\\kappa(K) \\ge \\tau$, set $\\sigma_3'(K) = \\min\\{\\eta \\, \\sigma_3(K), \\sigma_1(K)\\}$ and recompute $\\kappa'(K)$; otherwise, keep $\\sigma_3'(K)=\\sigma_3(K)$. Then find $\\kappa_{\\max}'(\\mathcal{T}_B)$ and compute \n$$\nR' = \\left(\\frac{h_{\\min}(\\mathcal{T}_A)}{h_{\\min}(\\mathcal{T}_B)}\\right)^2 \\left(\\frac{\\kappa_{\\max}'(\\mathcal{T}_B)}{\\kappa_{\\max}(\\mathcal{T}_A)}\\right)^2.\n$$\n\nInterpretation relative to structured versus unstructured meshes in computational electromagnetics: Structured meshes typically maintain bounded $\\kappa(K)$ and uniform $h_K$, resulting in smaller condition numbers. Unstructured meshes may contain slivers with large $\\kappa(K)$, severely degrading conditioning. The proposed targeted refinement reduces $\\kappa_{\\max}$ by improving sliver shape while not unduly changing $h_{\\min}$, thereby restoring conditioning closer to the baseline structured mesh.\n\nThe program implements these computations precisely for the provided test suite and outputs a single line containing $[[R_1,R_1'],[R_2,R_2'],[R_3,R_3']]$, with each $R_i$ and $R_i'$ represented as floating-point numbers. The ratios are dimensionless, so no physical units are required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef kappa_max(elements):\n    # elements: list of tuples (s1, s2, s3, h)\n    kappas = []\n    for (s1, s2, s3, h) in elements:\n        # Avoid division by zero; s3 > 0 by construction in test suite\n        kappas.append(s1 / s3)\n    return max(kappas)\n\ndef h_min(elements):\n    return min(h for (_, _, _, h) in elements)\n\ndef refine_elements(elements, tau, eta):\n    # Return new list of elements with refined s3 for slivers\n    refined = []\n    for (s1, s2, s3, h) in elements:\n        kappa = s1 / s3\n        if kappa >= tau:\n            s3_new = min(eta * s3, s1)  # increase smallest singular value, cap at s1\n            refined.append((s1, s2, s3_new, h))\n        else:\n            refined.append((s1, s2, s3, h))\n    return refined\n\ndef cond_ratio(meshA, meshB):\n    kA = kappa_max(meshA)\n    kB = kappa_max(meshB)\n    hA = h_min(meshA)\n    hB = h_min(meshB)\n    return (hA / hB) ** 2 * (kB / kA) ** 2\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"meshA\": [\n                (1.00, 0.95, 0.90, 0.25),\n                (1.00, 0.96, 0.88, 0.25),\n                (1.00, 0.94, 0.89, 0.25),\n                (1.00, 0.97, 0.91, 0.25),\n                (1.00, 0.95, 0.92, 0.25),\n                (1.00, 0.96, 0.93, 0.25),\n                (1.00, 0.94, 0.92, 0.25),\n                (1.00, 0.97, 0.93, 0.25),\n            ],\n            \"meshB\": [\n                (1.00, 0.99, 0.02, 0.25),\n                (1.00, 0.98, 0.03, 0.25),\n                (1.00, 0.97, 0.04, 0.25),\n                (1.00, 0.95, 0.92, 0.25),\n                (1.00, 0.96, 0.93, 0.25),\n                (1.00, 0.94, 0.90, 0.25),\n                (1.00, 0.95, 0.91, 0.25),\n                (1.00, 0.96, 0.92, 0.25),\n            ],\n            \"tau\": 10.0,\n            \"eta\": 5.0,\n        },\n        # Test case 2\n        {\n            \"meshA\": [\n                (0.50, 0.50, 0.50, 0.10),\n                (0.50, 0.50, 0.50, 0.10),\n                (0.50, 0.50, 0.50, 0.10),\n                (0.50, 0.50, 0.50, 0.10),\n                (0.50, 0.50, 0.50, 0.10),\n                (0.50, 0.50, 0.50, 0.10),\n                (0.50, 0.50, 0.50, 0.10),\n                (0.50, 0.50, 0.50, 0.10),\n                (0.50, 0.50, 0.50, 0.10),\n                (0.50, 0.50, 0.50, 0.10),\n            ],\n            \"meshB\": [\n                (0.55, 0.50, 0.45, 0.10),\n                (0.60, 0.50, 0.40, 0.10),\n                (0.52, 0.50, 0.48, 0.10),\n                (0.52, 0.50, 0.48, 0.10),\n                (0.52, 0.50, 0.48, 0.10),\n                (0.52, 0.50, 0.48, 0.10),\n                (0.52, 0.50, 0.48, 0.10),\n                (0.52, 0.50, 0.48, 0.10),\n                (0.52, 0.50, 0.48, 0.10),\n                (0.52, 0.50, 0.48, 0.10),\n            ],\n            \"tau\": 10.0,\n            \"eta\": 5.0,\n        },\n        # Test case 3\n        {\n            \"meshA\": [\n                (1.00, 0.80, 0.40, 0.30),\n                (1.00, 0.85, 0.45, 0.30),\n                (1.00, 0.82, 0.42, 0.30),\n                (1.00, 0.83, 0.43, 0.30),\n                (1.00, 0.84, 0.44, 0.30),\n            ],\n            \"meshB\": [\n                (1.00, 0.999, 0.001, 0.30),\n                (1.00, 0.995, 0.002, 0.30),\n                (1.00, 0.90, 0.60, 0.30),\n                (1.00, 0.85, 0.50, 0.30),\n                (1.00, 0.88, 0.55, 0.30),\n            ],\n            \"tau\": 10.0,\n            \"eta\": 20.0,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        meshA = case[\"meshA\"]\n        meshB = case[\"meshB\"]\n        tau = case[\"tau\"]\n        eta = case[\"eta\"]\n\n        R = cond_ratio(meshA, meshB)\n        meshB_refined = refine_elements(meshB, tau, eta)\n        R_refined = cond_ratio(meshA, meshB_refined)\n\n        # Round moderately for cleaner output\n        results.append([float(f\"{R:.6f}\"), float(f\"{R_refined:.6f}\")])\n\n    # Final print statement in the exact required format.\n    # Single line, comma-separated list enclosed in square brackets.\n    print(f\"[{','.join(str(item) for item in results)}]\")\n\nsolve()\n```", "id": "3351151"}, {"introduction": "In time-domain simulations using explicit methods, the mesh structure has profound implications for computational efficiency through the stability condition, which links the maximum allowable time step to the smallest spatial feature in the grid. This practice demonstrates how a single small element in an unstructured mesh can create a severe bottleneck, forcing a tiny time step for the entire simulation. You will implement and analyze a powerful mitigation strategy, local time stepping (LTS), which allows different parts of the mesh to be advanced with different time steps, dramatically improving efficiency. [@problem_id:3351147]", "problem": "Consider the one-dimensional reduction of Maxwell’s equations for Transverse Electric polarization, in which the electric field component $u(x,t)$ satisfies the scalar wave equation $u_{tt} = c^2 u_{xx}$ on a finite interval with Dirichlet boundary conditions. Use the Finite Element Method (FEM) with piecewise linear basis functions on a mesh of segments with lengths $h_e$, leading to the semi-discrete second-order system $M u_{tt} + K u = 0$, where $M$ is the mass matrix and $K$ is the stiffness matrix assembled from element contributions. For a fully explicit central difference time integrator of this semi-discrete system, stability is controlled by the largest generalized eigenvalue of the pencil $(K, M)$: if $K v = \\omega^2 M v$ has maximum $\\omega_{\\max}$, then the global time step must satisfy $\\Delta t \\le \\frac{2}{\\omega_{\\max}}$ to be stable. On unstructured meshes, the smallest element size $h_{\\min}$ often dictates $\\Delta t$ through the spectral radius of $M^{-1}K$. Mass lumping replaces the consistent mass matrix by a diagonal matrix obtained by row-sum lumping, which changes the spectrum and enables larger stable $\\Delta t$ while preserving explicitness.\n\nStarting from the scalar wave equation $u_{tt} = c^2 u_{xx}$, the weak form, and the FEM assembly of $M$ and $K$, design an algorithm that:\n- Assembles the consistent mass matrix $M$ and the stiffness matrix $K$ for a one-dimensional mesh with Dirichlet boundary conditions at both ends, using element contributions $K_e = \\frac{c^2}{h_e}\\begin{bmatrix}1 & -1 \\\\ -1 & 1\\end{bmatrix}$ and $M_e = \\frac{h_e}{6}\\begin{bmatrix}2 & 1 \\\\ 1 & 2\\end{bmatrix}$.\n- Constructs a mass-lumped matrix $M_{\\mathrm{lump}}$ by diagonal row-sum lumping of $M$.\n- Computes the stable global time steps $\\Delta t_{\\mathrm{global,cons}} = \\frac{2}{\\omega_{\\max,\\mathrm{cons}}}$ and $\\Delta t_{\\mathrm{global,lump}} = \\frac{2}{\\omega_{\\max,\\mathrm{lump}}}$, where $\\omega_{\\max,\\mathrm{cons}}$ and $\\omega_{\\max,\\mathrm{lump}}$ are the maximum generalized eigenvalues satisfying $K v = \\omega^2 M v$ or $K v = \\omega^2 M_{\\mathrm{lump}} v$, respectively.\n- Designs a local time stepping schedule on elements that achieves larger local time steps while maintaining stability of nearest-neighbor coupling. Use the smallest stable global time step $\\Delta t_{\\min}$ (for each mass choice) and assign each element a target time step $\\Delta t_e^{\\mathrm{target}} = \\Delta t_{\\min}\\,\\frac{h_e}{h_{\\min}}$. Then quantize each $\\Delta t_e^{\\mathrm{target}}$ to a power-of-two multiple of $\\Delta t_{\\min}$, that is, choose an integer $k_e \\ge 0$ such that $\\Delta t_e = \\Delta t_{\\min}\\,2^{k_e} \\le \\Delta t_e^{\\mathrm{target}}$ and $\\Delta t_e$ is as large as possible under this constraint. Enforce a nearest-neighbor stability constraint by ensuring that for every adjacent element pair $(e,e+1)$, the ratio of time steps is at most $2$, equivalently $|k_e - k_{e+1}| \\le 1$; if violated, reduce the larger $k$ until the constraint is satisfied everywhere.\n- Measures the computational efficiency in a simple, unit-cost model where the cost is proportional to the number of element updates per unit simulated time. For a total simulation time $T$, define the global explicit cost $C_{\\mathrm{global}} = N \\,\\frac{T}{\\Delta t_{\\min}}$ for $N$ elements, and the local-time-stepping cost $C_{\\mathrm{LTS}} = \\sum_{e=1}^N \\frac{T}{\\Delta t_e}$. The speed-up is $S = \\frac{C_{\\mathrm{global}}}{C_{\\mathrm{LTS}}}$.\n\nYour program must implement the above assembly and computations exactly and produce numerical results for the following test suite. In all cases, report time in seconds, with $c$ in meters per second and $h_e$ in meters, and take the total simulation time to be $T = 1$ second.\n\nTest suite:\n- Case $1$ (structured mesh, happy path): $h = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]$, $c = 3\\times 10^8$.\n- Case $2$ (unstructured mesh with one very small element, bottleneck): $h = [0.01, 0.01, 0.0001, 0.01, 0.01]$, $c = 3\\times 10^8$.\n- Case $3$ (graded unstructured mesh, edge condition coverage): $h = [0.0005, 0.001, 0.002, 0.004, 0.008]$, $c = 3\\times 10^8$.\n\nFor each case, compute:\n- $\\Delta t_{\\mathrm{global,cons}}$ and $\\Delta t_{\\mathrm{global,lump}}$ in seconds.\n- The speed-ups $S_{\\mathrm{cons}}$ and $S_{\\mathrm{lump}}$ for their respective local time stepping schedules.\n- The total number of distinct local time step groups $G_{\\mathrm{cons}} = \\max_e k_e + 1$ and $G_{\\mathrm{lump}} = \\max_e k_e + 1$ after enforcing the nearest-neighbor constraint.\n- A boolean indicating whether the nearest-neighbor constraint $|k_e - k_{e+1}| \\le 1$ holds everywhere after enforcement for the consistent and mass-lumped designs.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sub-list in the order [$ \\Delta t_{\\mathrm{global,cons}} $, $ \\Delta t_{\\mathrm{global,lump}} $, $ S_{\\mathrm{cons}} $, $ S_{\\mathrm{lump}} $, $ G_{\\mathrm{cons}} $, $ G_{\\mathrm{lump}} $, $adj_{\\mathrm{cons}}$, $adj_{\\mathrm{lump}}$]. For example, the overall output format is $[[\\dots],[\\dots],[\\dots]]$ with numerical values in seconds and booleans as plain true or false indicators.", "solution": "The problem requires the design and analysis of explicit time-stepping schemes for a Finite Element Method (FEM) discretization of the one-dimensional scalar wave equation, $u_{tt} = c^2 u_{xx}$. The analysis compares a standard global time-stepping approach with a local time-stepping (LTS) approach for both consistent and lumped mass matrix formulations. The key performance metrics are the maximum stable time step, the computational speed-up afforded by LTS, and the complexity of the resulting LTS schedule.\n\nThe foundation of the method is the semi-discrete system of ordinary differential equations $M \\ddot{\\mathbf{u}} + K \\mathbf{u} = \\mathbf{0}$, which arises from applying the FEM to the wave equation's weak form. Here, $\\mathbf{u}(t)$ is the vector of electric field values at the mesh nodes, $M$ is the mass matrix, and $K$ is the stiffness matrix. The stability of an explicit time integration scheme, such as the central difference method, is governed by the Courant-Friedrichs-Lewy (CFL) condition. For this system, the condition is $\\Delta t \\le \\frac{2}{\\omega_{\\max}}$, where $\\omega_{\\max}$ is the highest natural frequency of the discrete system, found by solving the generalized eigenvalue problem $K \\mathbf{v} = \\omega^2 M \\mathbf{v}$.\n\nThe specific tasks involve assembling $M$ and $K$ for a given 1D mesh, calculating the maximum stable global time steps, and then designing and evaluating an LTS scheme.\n\n**1. Matrix Assembly and Boundary Conditions**\n\nThe domain is discretized into $N$ line-segment elements, with element $e$ having length $h_e$. This mesh has $N+1$ nodes. We first assemble the global matrices of size $(N+1) \\times (N+1)$ by accumulating contributions from each element. For element $e$ connecting nodes $i$ and $i+1$, the element stiffness matrix $K_e$ and consistent mass matrix $M_e$ are given as:\n$$K_e = \\frac{c^2}{h_e}\\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}, \\quad M_e = \\frac{h_e}{6}\\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$$\nThese are assembled into global matrices $K$ and $M$. The problem specifies Dirichlet boundary conditions at both ends of the interval (nodes $0$ and $N$), which means $u_0(t) = 0$ and $u_N(t) = 0$. These degrees of freedom are fixed and removed from the dynamic system. This is accomplished by taking the submatrices of $K$ and $M$ corresponding to the interior nodes ($1, 2, \\dots, N-1$). The resulting dynamic system involves matrices of size $(N-1) \\times (N-1)$.\n\nThe mass-lumped matrix, $M_{\\mathrm{lump}}$, is a diagonal matrix that simplifies the time-stepping update by making the inverse $M^{-1}$ trivial to compute. It is constructed from the global consistent mass matrix $M$ (before applying boundary conditions) by summing the entries of each row and placing the sum on the corresponding diagonal entry:\n$$ (M_{\\mathrm{lump}})_{ii} = \\sum_{j=0}^{N} M_{ij} $$\nThe same Dirichlet boundary conditions are then applied to $M_{\\mathrm{lump}}$ by selecting the submatrix for the interior nodes.\n\n**2. Stability Analysis and Global Time Step Calculation**\n\nThe stability of the explicit central difference scheme is limited by the maximum frequency $\\omega_{\\max}$ supported by the discretized system. We compute this for both the consistent mass ($M_{\\mathrm{cons}}$) and mass-lumped ($M_{\\mathrm{lump}}$) formulations by solving the generalized eigenvalue problem $K \\mathbf{v} = \\lambda M \\mathbf{v}$, where $\\lambda = \\omega^2$. Let $\\lambda_{\\max}$ be the largest eigenvalue. Then $\\omega_{\\max} = \\sqrt{\\lambda_{\\max}}$. The maximum stable global time step is then:\n$$ \\Delta t_{\\mathrm{global}} = \\frac{2}{\\omega_{\\max}} $$\nWe compute $\\Delta t_{\\mathrm{global,cons}}$ using $M=M_{\\mathrm{cons}}$ and $\\Delta t_{\\mathrm{global,lump}}$ using $M=M_{\\mathrm{lump}}$. These calculations require a numerical generalized eigenvalue solver.\n\n**3. Local Time-Stepping (LTS) Algorithm**\n\nLTS aims to improve efficiency on unstructured meshes where a few small elements dictate a very small global time step. Instead, each element is advanced with a time step appropriate for its own size. The algorithm is as follows:\n\nLet $\\Delta t_{\\min}$ be the global stable time step for a given mass matrix formulation (e.g., $\\Delta t_{\\min} = \\Delta t_{\\mathrm{global,cons}}$). Let $h_{\\min} = \\min_e h_e$.\n\na. **Assign Target Time Steps**: For each element $e$, a target time step is set proportionally to its size:\n$$ \\Delta t_e^{\\mathrm{target}} = \\Delta t_{\\min} \\frac{h_e}{h_{\\min}} $$\n\nb. **Quantize Time Steps**: The target time step is quantized to a power-of-two multiple of $\\Delta t_{\\min}$ for algorithmic simplicity. We find the largest integer $k_e \\ge 0$ such that the actual element time step $\\Delta t_e = \\Delta t_{\\min} 2^{k_e}$ does not exceed the target. This gives:\n$$ 2^{k_e} \\le \\frac{h_e}{h_{\\min}} \\implies k_e = \\left\\lfloor \\log_2\\left(\\frac{h_e}{h_{\\min}}\\right) \\right\\rfloor $$\n\nc. **Enforce Adjacency Constraint**: To ensure a simple and stable coupling between adjacent elements, the time steps of neighbors must not differ by more than a factor of $2$. This translates to the constraint $|k_e - k_{e+1}| \\le 1$ for all adjacent element pairs $(e, e+1)$. This constraint is enforced iteratively. In each iteration, we scan all adjacent pairs. If the constraint is violated, the larger of the two $k$ values is reduced to satisfy it, i.e., $k_{\\text{larger}} = k_{\\text{smaller}} + 1$. This process is repeated until no more changes occur. This iterative process is guaranteed to converge as the $k_e$ values are non-negative and only ever decrease.\n\n**4. Efficiency Evaluation**\n\nThe computational efficiency is measured by comparing the cost of a global time step simulation with that of an LTS simulation for a total time $T$. The cost is assumed to be proportional to the total number of element updates.\n\n- **Global Cost**: All $N$ elements are updated at every time step $\\Delta t_{\\min}$.\n$$ C_{\\mathrm{global}} = N \\frac{T}{\\Delta t_{\\min}} $$\n- **LTS Cost**: Each element $e$ is updated with its own time step $\\Delta t_e$.\n$$ C_{\\mathrm{LTS}} = \\sum_{e=1}^{N} \\frac{T}{\\Delta t_e} $$\n- **Speed-up**: The speed-up $S$ is the ratio of these costs.\n$$ S = \\frac{C_{\\mathrm{global}}}{C_{\\mathrm{LTS}}} = \\frac{N \\frac{T}{\\Delta t_{\\min}}}{\\sum_{e=1}^N \\frac{T}{\\Delta t_e}} = \\frac{N}{\\sum_{e=1}^N \\frac{\\Delta t_{\\min}}{\\Delta t_e}} = \\frac{N}{\\sum_{e=1}^N 2^{-k_e}} $$\nThe number of distinct time step sizes, or groups, is $G = \\max_e k_e + 1$. The algorithm is implemented for both consistent and lumped mass matrices to compute $S_{\\mathrm{cons}}, S_{\\mathrm{lump}}, G_{\\mathrm{cons}}$, and $G_{\\mathrm{lump}}$.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eig\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        {'h': [0.01] * 10, 'c': 3e8},\n        {'h': [0.01, 0.01, 0.0001, 0.01, 0.01], 'c': 3e8},\n        {'h': [0.0005, 0.001, 0.002, 0.004, 0.008], 'c': 3e8}\n    ]\n\n    all_results = []\n    for case in test_cases:\n        h_elements = case['h']\n        c = case['c']\n        result = process_case(h_elements, c)\n        all_results.append(result)\n\n    # Format the final output string\n    case_strings = []\n    for res in all_results:\n        # Format: [dt_c, dt_l, S_c, S_l, G_c, G_l, adj_c, adj_l]\n        # Booleans must be lowercase 'true' or 'false'\n        s_res = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]},{res[5]},{str(res[6]).lower()},{str(res[7]).lower()}]\"\n        case_strings.append(s_res)\n    \n    print(f\"[{','.join(case_strings)}]\")\n\ndef process_case(h_elements, c):\n    \"\"\"\n    Processes a single test case: assembles matrices, solves eigenvalue problems,\n    and computes LTS metrics.\n    \"\"\"\n    N = len(h_elements)\n    num_nodes = N + 1\n\n    # 1. Assemble global matrices K and M_consistent\n    K_global = np.zeros((num_nodes, num_nodes))\n    M_consistent_global = np.zeros((num_nodes, num_nodes))\n\n    for i, h_e in enumerate(h_elements):\n        # Element matrices\n        K_e = (c**2 / h_e) * np.array([[1, -1], [-1, 1]])\n        M_e = (h_e / 6.0) * np.array([[2, 1], [1, 2]])\n        \n        # Assembly into global matrices\n        K_global[i:i+2, i:i+2] += K_e\n        M_consistent_global[i:i+2, i:i+2] += M_e\n\n    # 2. Construct mass-lumped matrix\n    M_lumped_global = np.diag(np.sum(M_consistent_global, axis=1))\n\n    # 3. Apply Dirichlet BCs by taking interior node submatrices\n    # Interior nodes are 1, ..., N-1 (0-indexed from 1 to N-1)\n    if N > 1:\n        K_int = K_global[1:N, 1:N]\n        Mc_int = M_consistent_global[1:N, 1:N]\n        Ml_int = M_lumped_global[1:N, 1:N]\n    else: # No interior nodes\n        K_int, Mc_int, Ml_int = np.array([]), np.array([]), np.array([])\n    \n    # 4. Solve eigenvalue problems and compute global time steps\n    if K_int.shape[0] > 0:\n        # Consistent mass\n        eigvals_c = eig(K_int, Mc_int, right=False)\n        omega_max_c = np.sqrt(np.max(np.real(eigvals_c)))\n        dt_global_c = 2.0 / omega_max_c\n\n        # Lumped mass\n        eigvals_l = eig(K_int, Ml_int, right=False)\n        omega_max_l = np.sqrt(np.max(np.real(eigvals_l)))\n        dt_global_l = 2.0 / omega_max_l\n    else:\n        dt_global_c = float('inf')\n        dt_global_l = float('inf')\n\n    # 5. Calculate LTS metrics\n    S_c, G_c, adj_c = calculate_lts_metrics(h_elements, dt_global_c)\n    S_l, G_l, adj_l = calculate_lts_metrics(h_elements, dt_global_l)\n\n    return [dt_global_c, dt_global_l, S_c, S_l, G_c, G_l, adj_c, adj_l]\n\ndef calculate_lts_metrics(h_elements, dt_min):\n    \"\"\"\n    Calculates the LTS speed-up (S), number of groups (G), and verifies\n    the adjacency constraint.\n    \"\"\"\n    N = len(h_elements)\n    if N == 0 or dt_min == float('inf'):\n        return 1.0, 0, True\n\n    h_arr = np.array(h_elements, dtype=float)\n    h_min = np.min(h_arr)\n\n    # Calculate initial k_e values\n    k = np.floor(np.log2(h_arr / h_min)).astype(int)\n    \n    # Iteratively enforce adjacency constraint |k_e - k_{e+1}| <= 1\n    if N > 1:\n        while True:\n            changed = False\n            for e in range(N - 1):\n                if abs(k[e] - k[e + 1]) > 1:\n                    changed = True\n                    if k[e] > k[e + 1]:\n                        k[e] = k[e + 1] + 1\n                    else:\n                        k[e + 1] = k[e] + 1\n            if not changed:\n                break\n    \n    # Calculate speed-up S\n    speedup = N / np.sum(2.0**(-k))\n    \n    # Calculate number of groups G\n    num_groups = np.max(k) + 1 if k.size > 0 else 0\n    \n    # Verify final adjacency constraint\n    adj_ok = True\n    if N > 1:\n        for e in range(N - 1):\n            if abs(k[e] - k[e + 1]) > 1:\n                adj_ok = False\n                break\n                \n    return speedup, int(num_groups), adj_ok\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3351147"}]}