{"hands_on_practices": [{"introduction": "At the heart of any subtraction scheme lies the calculation of kinematic invariants, which become singular in soft and collinear limits. This exercise focuses on the most fundamental practical challenge: computing these invariants with numerical stability. You will investigate the kinematic invariant $s_{ij} = (p_i + p_j)^2$, which is central to all subtraction formalisms, and discover how a naive implementation can fail dramatically due to catastrophic cancellation in floating-point arithmetic. By comparing a direct computation with a mathematically equivalent but numerically robust formula, you will gain a crucial, hands-on understanding of why careful algorithm choice is non-negotiable for reliable physics simulations [@problem_id:3538683].", "problem": "Consider the evaluation of the kinematic invariant for a pair of massless four-momenta in Minkowski space with metric signature $(+,-,-,-)$, defined by $s_{ij} = (p_i + p_j)^2$. In the context of Catani-Seymour (CS) dipole subtraction and Frixione-Kunszt-Signer (FKS) schemes, the behavior of $s_{ij}$ near the infrared and collinear limits is critical. You will stress-test numerical precision when $s_{ij}$ approaches values near machine epsilon, and compare the stability of two computational approaches in the evaluation of a simplified dipole factor that depends on $s_{ij}$.\n\nUse the following fundamental bases:\n- The Minkowski dot product for four-vectors $p^\\mu = (E, \\vec{p})$ with $p^2 = E^2 - \\|\\vec{p}\\|^2$.\n- For massless momenta, $E = \\|\\vec{p}\\|$ and $s_{ij} = 2\\,p_i \\cdot p_j = 2 E_i E_j \\left(1 - \\cos \\theta_{ij}\\right)$, where $\\theta_{ij}$ is the angle between $\\vec{p}_i$ and $\\vec{p}_j$.\n- The half-angle identity $\\sin^2\\left(\\frac{\\theta}{2}\\right) = \\frac{1}{2}\\left(1 - \\cos \\theta\\right)$.\n\nDefine two computational methods for $s_{ij}$ for massless momenta:\n1. Direct invariant method: Compute $s_{ij}$ via the Minkowski norm of the sum $(p_i + p_j)^2$, explicitly forming the spatial components to realize $s_{ij} = (E_i + E_j)^2 - \\|\\vec{p}_i + \\vec{p}_j\\|^2$. This evaluation entails subtracting nearly equal large numbers when $\\theta_{ij}$ is small.\n2. Rescaled-angle method: Compute $s_{ij}$ via the half-angle representation $s_{ij} = 4 E_i E_j \\sin^2\\left(\\frac{\\theta_{ij}}{2}\\right)$, which avoids catastrophic cancellation in the collinear limit.\n\nDefine the simplified dipole factor $D_{ij} = \\frac{1}{s_{ij}}$, which captures the leading collinear sensitivity relevant to subtraction schemes.\n\nFor each test case below, construct massless three-momenta in the $x$-$z$ plane as follows:\n- Let $\\vec{p}_i = (0, 0, E_i)$.\n- Let $\\vec{p}_j = (E_j \\sin \\theta, 0, E_j \\cos \\theta)$.\n- Then $p_i = (E_i, \\vec{p}_i)$ and $p_j = (E_j, \\vec{p}_j)$.\n\nFor ground-truth evaluation, use $s_{ij}^{\\text{true}} = 2 E_i E_j \\left(1 - \\cos \\theta\\right)$, where for very small angles the series expansion must be used:\n$$\n1 - \\cos \\theta = \\frac{\\theta^2}{2} - \\frac{\\theta^4}{24} + \\frac{\\theta^6}{720} - \\frac{\\theta^8}{40320},\n$$\nand for larger angles use the exact cosine.\n\nCompute, for each test case, the absolute relative error of $D_{ij}^{\\text{direct}}$ and $D_{ij}^{\\text{rescaled}}$ with respect to $D_{ij}^{\\text{true}} = \\frac{1}{s_{ij}^{\\text{true}}}$, and output an integer $1$ if the rescaled-angle method yields a strictly smaller absolute relative error than the direct invariant method, otherwise output $0$. If $D_{ij}^{\\text{true}}$ is undefined or infinite (e.g., when $\\theta = 0$ so that $s_{ij}^{\\text{true}} = 0$), output $0$ for that case.\n\nAngles must be provided in radians and energies must be provided in gigaelectronvolts (GeV). No unit conversion is required; computations are purely dimensionless ratios and invariants derived from these inputs.\n\nTest suite (energies in GeV, angles in radians):\n- Case A (happy path): $(E_i, E_j, \\theta) = (100, 100, 10^{-3})$.\n- Case B (near-collinear, small angle): $(E_i, E_j, \\theta) = (10^{3}, 10^{3}, 10^{-12})$.\n- Case C (near machine epsilon): $(E_i, E_j, \\theta) = (10^{3}, 10^{3}, 10^{-16})$.\n- Case D (extreme energy ratio, near machine epsilon): $(E_i, E_j, \\theta) = (10^{9}, 1, 10^{-16})$.\n- Case E (exactly collinear as boundary): $(E_i, E_j, \\theta) = (100, 35, 0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above, for example, $\\left[\\texttt{resultA},\\texttt{resultB},\\texttt{resultC},\\texttt{resultD},\\texttt{resultE}\\right]$ where each entry is either $0$ or $1$.", "solution": "The problem statement has been analyzed and is deemed **valid**. It is scientifically grounded in the principles of special relativity and computational physics, specifically addressing the numerical stability issues encountered when calculating kinematic invariants in the collinear limit—a critical aspect of Next-to-Leading-Order (NLO) calculations in high-energy physics. The problem is well-posed, with all necessary data, formulas, and evaluation criteria explicitly defined.\n\nThe core task is to compare the numerical stability of two methods for computing the kinematic invariant $s_{ij} = (p_i + p_j)^2$ for two massless four-momenta, $p_i$ and $p_j$, in the limit where the angle $\\theta_{ij}$ between their spatial momenta $\\vec{p}_i$ and $\\vec{p}_j$ approaches zero (the collinear limit). The stability is assessed by calculating the absolute relative error of a simplified dipole factor $D_{ij} = 1/s_{ij}$ computed with each method against a high-precision ground truth value.\n\nThe four-momenta for massless particles are given by $p^\\mu = (E, \\vec{p})$ where the energy $E$ is equal to the magnitude of the three-momentum, $E = \\|\\vec{p}\\|$. The metric signature is $(+, -, -, -)$. The invariant $s_{ij}$ is analytically equivalent to $s_{ij} = 2 p_i \\cdot p_j = 2 E_i E_j (1 - \\cos \\theta_{ij})$.\n\nThe problem defines three computational approaches for $s_{ij}$: a ground-truth method, a direct invariant method, and a rescaled-angle method.\n\n**1. Ground-Truth Calculation ($s_{ij}^{\\text{true}}$)**\n\nThe ground-truth value is based on the formula $s_{ij}^{\\text{true}} = 2 E_i E_j (1 - \\cos \\theta)$. A naive computation of the term $(1 - \\cos \\theta)$ for very small $\\theta$ suffers from catastrophic cancellation, as $\\cos \\theta$ approaches $1$. To ensure high precision for the reference value, the problem mandates the use of the Taylor series expansion for $1 - \\cos \\theta$ for small angles:\n$$\n1 - \\cos \\theta = \\frac{\\theta^2}{2!} - \\frac{\\theta^4}{4!} + \\frac{\\theta^6}{6!} - \\frac{\\theta^8}{8!} + \\mathcal{O}(\\theta^{10})\n$$\nIn our implementation, we will use this series for angles $\\theta < 10^{-6}$ radians, a threshold below which standard double-precision floating-point arithmetic for $(1 - \\cos \\theta)$ begins to lose significant precision. For $\\theta \\ge 10^{-6}$, the direct computation is sufficiently accurate.\n\n**2. Method 1: Direct Invariant Method ($s_{ij}^{\\text{direct}}$)**\n\nThis method computes $s_{ij}$ by a direct implementation of the definition of the invariant of a sum of four-vectors:\n$$\ns_{ij}^{\\text{direct}} = (p_i + p_j)^2 = (E_i + E_j)^2 - \\|\\vec{p}_i + \\vec{p}_j\\|^2\n$$\nThe massless three-momenta are constructed in the $x$-$z$ plane: $\\vec{p}_i = (0, 0, E_i)$ and $\\vec{p}_j = (E_j \\sin \\theta, 0, E_j \\cos \\theta)$. Their sum is $\\vec{p}_i + \\vec{p}_j = (E_j \\sin \\theta, 0, E_i + E_j \\cos \\theta)$. The squared norm is:\n$$\n\\|\\vec{p}_i + \\vec{p}_j\\|^2 = (E_j \\sin \\theta)^2 + (E_i + E_j \\cos \\theta)^2 = E_j^2 \\sin^2 \\theta + E_i^2 + 2 E_i E_j \\cos \\theta + E_j^2 \\cos^2 \\theta = E_i^2 + E_j^2 + 2 E_i E_j \\cos \\theta\n$$\nSubstituting this into the expression for $s_{ij}^{\\text{direct}}$ leads to:\n$$\ns_{ij}^{\\text{direct}} = (E_i^2 + 2 E_i E_j + E_j^2) - (E_i^2 + E_j^2 + 2 E_i E_j \\cos \\theta)\n$$\nIn the collinear limit ($\\theta \\to 0$), $\\cos \\theta \\to 1$ and thus $\\|\\vec{p}_i + \\vec{p}_j\\|^2 \\to (E_i + E_j)^2$. The calculation involves the subtraction of two very large, nearly identical numbers, which is a classic source of catastrophic cancellation and severe loss of precision. This method is therefore expected to perform poorly for small $\\theta$.\n\n**3. Method 2: Rescaled-Angle Method ($s_{ij}^{\\text{rescaled}}$)**\n\nThis method reformulates the expression for $s_{ij}$ using the trigonometric half-angle identity, $1 - \\cos \\theta = 2 \\sin^2(\\frac{\\theta}{2})$.\n$$\ns_{ij}^{\\text{rescaled}} = 2 E_i E_j (1 - \\cos \\theta) = 2 E_i E_j \\left(2 \\sin^2\\left(\\frac{\\theta}{2}\\right)\\right) = 4 E_i E_j \\sin^2\\left(\\frac{\\theta}{2}\\right)\n$$\nFor small $\\theta$, the argument $\\theta/2$ is also small, and $\\sin(\\theta/2) \\approx \\theta/2$. The computation involves multiplying small numbers, which is numerically stable and preserves relative precision. This method is expected to be highly accurate even for extremely small angles.\n\n**Evaluation and Comparison**\n\nFor each test case, we compute the dipole factors $D_{ij}^{\\text{direct}} = 1/s_{ij}^{\\text{direct}}$ and $D_{ij}^{\\text{rescaled}} = 1/s_{ij}^{\\text{rescaled}}$ and compare them to the ground-truth $D_{ij}^{\\text{true}} = 1/s_{ij}^{\\text{true}}$. The comparison metric is the absolute relative error, e.g., for the direct method:\n$$\n\\text{err}_{\\text{direct}} = \\left| \\frac{D_{ij}^{\\text{direct}} - D_{ij}^{\\text{true}}}{D_{ij}^{\\text{true}}} \\right| = \\left| \\frac{1/s_{ij}^{\\text{direct}} - 1/s_{ij}^{\\text{true}}}{1/s_{ij}^{\\text{true}}} \\right| = \\left| \\frac{s_{ij}^{\\text{true}}}{s_{ij}^{\\text{direct}}} - 1 \\right|\n$$\nThe latter form is preferable for computation as it is more stable if the dipole factors are very large. An integer $1$ is recorded if $\\text{err}_{\\text{rescaled}} < \\text{err}_{\\text{direct}}$, and $0$ otherwise. For the case where $\\theta = 0$, $s_{ij}^{\\text{true}}=0$, making $D_{ij}^{\\text{true}}$ infinite; the problem specifies an output of $0$ for this boundary case.\n\n**Analysis of Test Cases:**\n- **Case A ($E_i=100, E_j=100, \\theta=10^{-3}$):** The angle is small but not extreme. Both methods should provide reasonable results, but the rescaled-angle method is expected to be more accurate.\n- **Case B ($E_i=10^3, E_j=10^3, \\theta=10^{-12}$):** The angle is very small. The direct method will suffer significant catastrophic cancellation, yielding a highly inaccurate or zero result. The rescaled-angle method will remain accurate.\n- **Case C ($E_i=10^3, E_j=10^3, \\theta=10^{-16}$):** The angle is near machine epsilon for double precision. The term `1 - cos(theta)` will evaluate to exactly zero in standard floating-point math, causing the direct method to fail completely ($s_{ij}^{\\text{direct}}=0$). The rescaled-angle method, computing $\\sin(\\theta/2)$, will produce a valid, non-zero result.\n- **Case D ($E_i=10^9, E_j=1, \\theta=10^{-16}$):** Similar to Case C but with an extreme energy ratio. The direct method will again fail due to the small angle, compounded by the large numerical range of the energies. The rescaled-angle method remains robust.\n- **Case E ($E_i=100, E_j=35, \\theta=0$):** This represents the exact collinear singularity. As per the problem's rules, the output is $0$ because $s_{ij}^{\\text{true}}=0$.\n\nBased on this analysis, we anticipate the rescaled-angle method to be strictly superior for all cases where $\\theta$ is small (A, B, C, D), while Case E is determined by the specific problem rule.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the numerical stability of two methods for calculating\n    the kinematic invariant s_ij in the collinear limit.\n    \"\"\"\n    # Test suite (E_i [GeV], E_j [GeV], theta [rad])\n    test_cases = [\n        (100.0, 100.0, 1.0e-3),   # Case A\n        (1.0e3, 1.0e3, 1.0e-12),  # Case B\n        (1.0e3, 1.0e3, 1.0e-16),  # Case C\n        (1.0e9, 1.0, 1.0e-16),    # Case D\n        (100.0, 35.0, 0.0),       # Case E\n    ]\n\n    results = []\n    \n    # Threshold for using Taylor series for 1 - cos(theta)\n    TAYLOR_THRESHOLD = 1.0e-6\n\n    for case in test_cases:\n        E_i, E_j, theta = case\n\n        # Per problem specification, if theta=0, output 0.\n        if theta == 0.0:\n            results.append(0)\n            continue\n\n        # 1. Ground-Truth Calculation (s_true)\n        # Use Taylor series for small theta to maintain high precision\n        if theta < TAYLOR_THRESHOLD:\n            theta2 = theta**2\n            theta4 = theta**4\n            theta6 = theta**6\n            theta8 = theta**8\n            # Series: theta^2/2 - theta^4/24 + theta^6/720 - theta^8/40320\n            one_minus_cos = theta2/2.0 - theta4/24.0 + theta6/720.0 - theta8/40320.0\n        else:\n            one_minus_cos = 1.0 - np.cos(theta)\n        \n        s_true = 2.0 * E_i * E_j * one_minus_cos\n        \n        # If s_true is zero (due to underflow), D_true is infinite/undefined. Output 0.\n        if s_true == 0.0:\n            results.append(0)\n            continue\n\n        # 2. Method 1: Direct Invariant (s_direct)\n        p_i_vec = np.array([0.0, 0.0, E_i])\n        p_j_vec = np.array([E_j * np.sin(theta), 0.0, E_j * np.cos(theta)])\n        p_sum_vec = p_i_vec + p_j_vec\n        p_sum_vec_norm_sq = np.dot(p_sum_vec, p_sum_vec)\n        s_direct = (E_i + E_j)**2 - p_sum_vec_norm_sq\n\n        # 3. Method 2: Rescaled-Angle (s_rescaled)\n        s_rescaled = 4.0 * E_i * E_j * (np.sin(theta / 2.0))**2\n\n        # 4. Comparison\n        # Handle cases where a method returns zero, implying infinite error.\n        if s_direct == 0.0:\n            # If direct method fails and rescaled doesn't, rescaled is better.\n            if s_rescaled != 0.0:\n                results.append(1)\n                continue\n            # If both fail, rescaled is not strictly better.\n            else:\n                results.append(0)\n                continue\n        \n        # If rescaled method fails (and direct didn't), it's not better.\n        if s_rescaled == 0.0:\n            results.append(0)\n            continue\n\n        # Calculate absolute relative errors using the stable form.\n        err_direct = abs(s_true / s_direct - 1.0)\n        err_rescaled = abs(s_true / s_rescaled - 1.0)\n        \n        if err_rescaled < err_direct:\n            results.append(1)\n        else:\n            results.append(0)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3538683"}, {"introduction": "Once the building blocks are numerically stable, the next step is to validate the overall logic of the subtraction scheme. A powerful method for this is to cross-check its results against a conceptually simpler, albeit less efficient, technique like phase-space slicing. Both methods aim to isolate infrared divergences to reveal a universal, finite physical result, and they must agree in the limit where the regulator (the slicing cutoff $\\delta$) vanishes. This practice guides you through the implementation of a toy model to perform exactly this cross-validation [@problem_id:3538697], reinforcing the deep principle that the finite remainders calculated are independent of the particular regularization scheme employed.", "problem": "Consider a toy model for validating local subtraction counterterms against phase-space slicing in the context of subtraction schemes for infrared divergences used in computational high-energy physics, such as the Catani-Seymour dipole subtraction and the Frixione-Kunszt-Signer (FKS) subtraction. Let the real-emission differential weight be modeled over a two-dimensional unit phase space with variables $x \\in (0,1)$ and $z \\in (0,1)$ by an integrand $R(x,z)$ that has soft and collinear singular structures approximated by leading terms as $x \\to 0$ and $z \\to 1$, respectively. Define three smooth functions\n$$\nA(x,z) = a_0 + a_1 x + a_2 (1 - z) + a_3 x (1 - z), \\quad\nB(x,z) = b_0 + b_1 x + b_2 (1 - z) + b_3 x (1 - z), \\quad\nG(x,z) = \\gamma\\, x^{r}\\, (1 - z)^{s},\n$$\nwith real parameters $a_0,a_1,a_2,a_3,b_0,b_1,b_2,b_3,\\gamma,r,s$, where $\\gamma \\ge 0$, $r \\ge 0$, and $s \\ge 0$. The toy-model real-emission integrand is\n$$\nR(x,z) = \\frac{A(x,z)}{x} + \\frac{B(x,z)}{1 - z} + G(x,z).\n$$\nTo emulate a local subtraction scheme (in the spirit of Catani-Seymour or Frixione-Kunszt-Signer), define the local counterterms\n$$\nS_{\\text{soft}}(x,z) = \\frac{A(0,z)}{x}, \\qquad S_{\\text{coll}}(x,z) = \\frac{B(x,1)}{1 - z},\n$$\nwith $A(0,z) = a_0 + a_2 (1 - z)$ and $B(x,1) = b_0 + b_1 x$. The subtracted integrand\n$$\nF(x,z) = R(x,z) - S_{\\text{soft}}(x,z) - S_{\\text{coll}}(x,z)\n$$\nmust be finite on the full unit square for the given polynomial $A(x,z)$ and $B(x,z)$, provided $r \\ge 0$ and $s \\ge 0$. The integrated finite remainder of the subtraction scheme is defined as\n$$\nC_{\\text{sub}} = \\int_0^1 \\int_0^1 F(x,z)\\, \\mathrm{d}x\\, \\mathrm{d}z.\n$$\nIndependently, define a phase-space slicing scheme with an infrared cutoff $\\delta \\in (0,1)$ on both soft and collinear regions. The sliced integral of the real-emission integrand is\n$$\nI_{\\text{slice}}(\\delta) = \\int_{0}^{1-\\delta} \\int_{\\delta}^{1} R(x,z)\\, \\mathrm{d}x\\, \\mathrm{d}z.\n$$\nTo isolate the finite remainder that is comparable to the subtraction scheme, consider removing the logarithmic dependence on the cutoff by subtracting the leading singular weights integrated over the complementary variables:\n$$\nC_{\\text{slice}}(\\delta) = I_{\\text{slice}}(\\delta) - \\ln\\!\\left(\\frac{1}{\\delta}\\right)\\left(\\int_0^1 A(0,z)\\, \\mathrm{d}z + \\int_0^1 B(x,1)\\, \\mathrm{d}x\\right).\n$$\nThe cross-validation objective is to demonstrate numerically that, for the toy model described, the two definitions of the integrated finite remainder are equivalent in the limit $\\delta \\to 0$, namely\n$$\n\\lim_{\\delta \\to 0} \\left[ C_{\\text{slice}}(\\delta) - C_{\\text{sub}} \\right] = 0.\n$$\nStart from the fundamental base that infrared divergences manifest as integrable singularities against infrared-safe test functions and that subtracting pointwise the leading singular behavior renders the remainder integrable over the full phase space. Use the intermediate-value properties of the integral and the dominated convergence theorem as needed, but do not assume any shortcut formulas beyond the definitions above.\n\nYour task is to write a complete program that, for a set of parameterized test cases, computes $C_{\\text{sub}}$ and $C_{\\text{slice}}(\\delta)$ for three cutoff values $\\delta \\in \\{10^{-2}, 10^{-4}, 10^{-6}\\}$, and returns the absolute differences $\\left|C_{\\text{slice}}(\\delta) - C_{\\text{sub}}\\right|$ for each test case and each $\\delta$. All quantities are dimensionless, so no physical units are required. Angles do not appear, so no angle unit specification is required.\n\nImplement numerical integration robustly over the specified domains. For the logarithmic subtraction term, compute $\\int_0^1 A(0,z)\\, \\mathrm{d}z$ and $\\int_0^1 B(x,1)\\, \\mathrm{d}x$ exactly from the given polynomials.\n\nUse the following test suite of parameter values, which probes different singular structures:\n- Test Case 1 (soft-only singular behavior): $(a_0,a_1,a_2,a_3) = (2.0, 1.0, 0.5, 0.3)$, $(b_0,b_1,b_2,b_3) = (0.0, 0.0, 0.0, 0.0)$, $\\gamma = 0.2$, $r = 1.0$, $s = 1.0$.\n- Test Case 2 (collinear-only singular behavior): $(a_0,a_1,a_2,a_3) = (0.0, 0.0, 0.0, 0.0)$, $(b_0,b_1,b_2,b_3) = (1.5, 0.4, 0.7, 0.2)$, $\\gamma = 0.3$, $r = 1.0$, $s = 1.0$.\n- Test Case 3 (combined soft and collinear singular behavior): $(a_0,a_1,a_2,a_3) = (1.0, 0.5, 0.2, 0.1)$, $(b_0,b_1,b_2,b_3) = (0.8, 0.3, 0.4, 0.1)$, $\\gamma = 0.25$, $r = 1.0$, $s = 0.5$.\n- Test Case 4 (finite-only baseline): $(a_0,a_1,a_2,a_3) = (0.0, 0.0, 0.0, 0.0)$, $(b_0,b_1,b_2,b_3) = (0.0, 0.0, 0.0, 0.0)$, $\\gamma = 0.5$, $r = 0.0$, $s = 0.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists, each inner list giving the absolute differences for $\\delta = 10^{-2}, 10^{-4}, 10^{-6}$ in that order. For example, the output format must be\n$$\n\\texttt{[[d\\_1(\\delta\\_1),d\\_1(\\delta\\_2),d\\_1(\\delta\\_3)],[d\\_2(\\delta\\_1),d\\_2(\\delta\\_2),d\\_2(\\delta\\_3)],\\ldots]}\n$$\nwith no additional text.", "solution": "The problem requires a numerical validation of a toy model for local subtraction schemes used to handle infrared divergences in high-energy physics calculations. The core task is to demonstrate that the finite part of a real-emission integral, as computed by a local subtraction method ($C_{\\text{sub}}$), is equivalent to the finite part computed via a phase-space slicing method ($C_{\\text{slice}}(\\delta)$) in the limit where the slicing cutoff $\\delta$ approaches zero.\n\nFirst, we analyze the quantity $C_{\\text{sub}}$, which is the integrated finite remainder from the subtraction scheme. It is defined as:\n$$\nC_{\\text{sub}} = \\int_0^1 \\int_0^1 F(x,z)\\, \\mathrm{d}x\\, \\mathrm{d}z\n$$\nThe integrand $F(x,z)$ is the real-emission integrand $R(x,z)$ minus the local subtraction counterterms $S_{\\text{soft}}(x,z)$ and $S_{\\text{coll}}(x,z)$:\n$$\nF(x,z) = R(x,z) - S_{\\text{soft}}(x,z) - S_{\\text{coll}}(x,z)\n$$\nSubstituting the definitions of $R$, $S_{\\text{soft}}$, and $S_{\\text{coll}}$:\n$$\nF(x,z) = \\left( \\frac{A(x,z)}{x} + \\frac{B(x,z)}{1 - z} + G(x,z) \\right) - \\frac{A(0,z)}{x} - \\frac{B(x,1)}{1 - z}\n$$\nBy rearranging terms, we can see how the singularities are canceled locally:\n$$\nF(x,z) = \\frac{A(x,z) - A(0,z)}{x} + \\frac{B(x,z) - B(x,1)}{1-z} + G(x,z)\n$$\nLet us evaluate the numerators. Given $A(x,z) = a_0 + a_1 x + a_2 (1 - z) + a_3 x (1 - z)$, we have $A(0,z) = a_0 + a_2(1-z)$. The difference is:\n$$\nA(x,z) - A(0,z) = (a_0 + a_1 x + a_2 (1 - z) + a_3 x (1 - z)) - (a_0 + a_2(1-z)) = a_1 x + a_3 x (1-z) = x(a_1 + a_3(1-z))\n$$\nThus, the first term of $F(x,z)$ simplifies to a regular polynomial:\n$$\n\\frac{A(x,z) - A(0,z)}{x} = a_1 + a_3(1-z)\n$$\nSimilarly, for the second term, with $B(x,z) = b_0 + b_1 x + b_2 (1 - z) + b_3 x (1 - z)$ and $B(x,1) = b_0 + b_1 x$, the difference is:\n$$\nB(x,z) - B(x,1) = (b_0 + b_1 x + b_2 (1 - z) + b_3 x (1 - z)) - (b_0 + b_1 x) = b_2(1-z) + b_3 x (1-z) = (1-z)(b_2 + b_3 x)\n$$\nThis simplifies the second term:\n$$\n\\frac{B(x,z) - B(x,1)}{1-z} = b_2 + b_3 x\n$$\nThe subtracted integrand $F(x,z)$ is therefore a sum of regular functions over the entire domain $[0,1] \\times [0,1]$:\n$$\nF(x,z) = (a_1 + a_3(1-z)) + (b_2 + b_3 x) + \\gamma x^r (1-z)^s\n$$\nSince $F(x,z)$ is regular, its integral $C_{\\text{sub}}$ can be computed analytically. We integrate each term over $x \\in [0,1]$ and $z \\in [0,1]$:\n$$\n\\int_0^1 \\int_0^1 a_1 \\, \\mathrm{d}x \\mathrm{d}z = a_1\n$$\n$$\n\\int_0^1 \\int_0^1 a_3(1-z) \\, \\mathrm{d}x \\mathrm{d}z = a_3 \\left( \\int_0^1 \\! \\mathrm{d}x \\right) \\left( \\int_0^1 (1-z) \\mathrm{d}z \\right) = a_3 (1) \\left[z - \\frac{z^2}{2}\\right]_0^1 = \\frac{a_3}{2}\n$$\n$$\n\\int_0^1 \\int_0^1 b_2 \\, \\mathrm{d}x \\mathrm{d}z = b_2\n$$\n$$\n\\int_0^1 \\int_0^1 b_3 x \\, \\mathrm{d}x \\mathrm{d}z = b_3 \\left( \\int_0^1 x \\mathrm{d}x \\right) \\left( \\int_0^1 \\! \\mathrm{d}z \\right) = b_3 \\left[\\frac{x^2}{2}\\right]_0^1 (1) = \\frac{b_3}{2}\n$$\n$$\n\\int_0^1 \\int_0^1 \\gamma x^r (1-z)^s \\, \\mathrm{d}x \\mathrm{d}z = \\gamma \\left( \\int_0^1 x^r \\mathrm{d}x \\right) \\left( \\int_0^1 (1-z)^s \\mathrm{d}z \\right) = \\gamma \\left[\\frac{x^{r+1}}{r+1}\\right]_0^1 \\left[-\\frac{(1-z)^{s+1}}{s+1}\\right]_0^1 = \\frac{\\gamma}{(r+1)(s+1)}\n$$\nSumming these results yields the analytical expression for $C_{\\text{sub}}$:\n$$\nC_{\\text{sub}} = a_1 + \\frac{a_3}{2} + b_2 + \\frac{b_3}{2} + \\frac{\\gamma}{(r+1)(s+1)}\n$$\nNext, we analyze the quantity $C_{\\text{slice}}(\\delta)$, defined as:\n$$\nC_{\\text{slice}}(\\delta) = I_{\\text{slice}}(\\delta) - \\ln\\!\\left(\\frac{1}{\\delta}\\right)\\left(\\int_0^1 A(0,z)\\, \\mathrm{d}z + \\int_0^1 B(x,1)\\, \\mathrm{d}x\\right)\n$$\nThe integral $I_{\\text{slice}}(\\delta)$ is performed over a region that excludes the singularities, so it is finite. However, the full integrand $R(x,z)$ is sufficiently complicated to warrant numerical integration. The integration domain is $x \\in [\\delta, 1]$ and $z \\in [0, 1-\\delta]$.\n$$\nI_{\\text{slice}}(\\delta) = \\int_{0}^{1-\\delta} \\int_{\\delta}^{1} \\left( \\frac{A(x,z)}{x} + \\frac{B(x,z)}{1 - z} + G(x,z) \\right) \\, \\mathrm{d}x\\, \\mathrm{d}z\n$$\nThe logarithmic subtraction term requires the analytical integration of the leading singular behaviors. Let $I_{\\text{log}} = \\int_0^1 A(0,z)\\, \\mathrm{d}z + \\int_0^1 B(x,1)\\, \\mathrm{d}x$. We compute these two integrals:\n$$\n\\int_0^1 A(0,z)\\, \\mathrm{d}z = \\int_0^1 (a_0 + a_2(1-z)) \\, \\mathrm{d}z = \\left[a_0 z + a_2\\left(z-\\frac{z^2}{2}\\right)\\right]_0^1 = a_0 + \\frac{a_2}{2}\n$$\n$$\n\\int_0^1 B(x,1)\\, \\mathrm{d}x = \\int_0^1 (b_0 + b_1 x) \\, \\mathrm{d}x = \\left[b_0 x + b_1 \\frac{x^2}{2}\\right]_0^1 = b_0 + \\frac{b_1}{2}\n$$\nTherefore, the coefficient of the logarithm is:\n$$\nI_{\\text{log}} = a_0 + \\frac{a_2}{2} + b_0 + \\frac{b_1}{2}\n$$\nThe procedure for each test case is as follows:\n1.  Using the given parameters, compute the exact value of $C_{\\text{sub}}$ with the derived analytical formula.\n2.  Compute the exact value of the logarithmic coefficient $I_{\\text{log}}$.\n3.  For each cutoff value $\\delta \\in \\{10^{-2}, 10^{-4}, 10^{-6}\\}$:\n    a. Numerically evaluate $I_{\\text{slice}}(\\delta)$ by integrating $R(x,z)$ over the domain $x \\in [\\delta, 1]$ and $z \\in [0, 1-\\delta]$.\n    b. Calculate $C_{\\text{slice}}(\\delta) = I_{\\text{slice}}(\\delta) - \\ln(1/\\delta) \\cdot I_{\\text{log}}$.\n    c. Compute the absolute difference $|C_{\\text{slice}}(\\delta) - C_{\\text{sub}}|$.\nThe expectation is that this difference diminishes as $\\delta$ becomes smaller, confirming the consistency between the two regularization methods. The program implements this procedure.", "answer": "```python\nimport numpy as np\nfrom scipy import integrate\n\ndef solve():\n    \"\"\"\n    Computes and compares finite remainders from subtraction and slicing schemes\n    for a toy model of infrared divergences.\n    \"\"\"\n    \n    # Test cases:\n    # (a_0, a_1, a_2, a_3), (b_0, b_1, b_2, b_3), gamma, r, s\n    test_cases = [\n        ((2.0, 1.0, 0.5, 0.3), (0.0, 0.0, 0.0, 0.0), 0.2, 1.0, 1.0),\n        ((0.0, 0.0, 0.0, 0.0), (1.5, 0.4, 0.7, 0.2), 0.3, 1.0, 1.0),\n        ((1.0, 0.5, 0.2, 0.1), (0.8, 0.3, 0.4, 0.1), 0.25, 1.0, 0.5),\n        ((0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0), 0.5, 0.0, 0.0),\n    ]\n\n    deltas = [1e-2, 1e-4, 1e-6]\n\n    # Helper functions for the integrand components\n    def A_func(x, z, a_params):\n        a0, a1, a2, a3 = a_params\n        return a0 + a1 * x + a2 * (1.0 - z) + a3 * x * (1.0 - z)\n\n    def B_func(x, z, b_params):\n        b0, b1, b2, b3 = b_params\n        return b0 + b1 * x + b2 * (1.0 - z) + b3 * x * (1.0 - z)\n\n    def G_func(x, z, gamma, r, s):\n        # Use np.power for safe handling of 0**0 = 1\n        return gamma * np.power(x, r) * np.power(1.0 - z, s)\n\n    # The full real-emission integrand R(x, z)\n    def R_integrand(x, z, a_params, b_params, gamma, r, s):\n        # The integration domain for I_slice ensures x > 0 and 1-z > 0,\n        # so direct division is safe.\n        term_A = A_func(x, z, a_params) / x\n        term_B = B_func(x, z, b_params) / (1.0 - z)\n        term_G = G_func(x, z, gamma, r, s)\n        return term_A + term_B + term_G\n\n    all_results = []\n    for case in test_cases:\n        a_params, b_params, gamma, r, s = case\n        a0, a1, a2, a3 = a_params\n        b0, b1, b2, b3 = b_params\n\n        # 1. Calculate C_sub analytically\n        c_sub = a1 + a3 / 2.0 + b2 + b3 / 2.0 + gamma / ((r + 1.0) * (s + 1.0))\n\n        # 2. Calculate the logarithmic coefficient I_log analytically\n        i_log = (a0 + a2 / 2.0) + (b0 + b1 / 2.0)\n\n        case_diffs = []\n        for delta in deltas:\n            # 3a. Numerically evaluate I_slice(delta)\n            # nquad integrates func(x,y,...) over ranges=[[xmin,xmax],[ymin,ymax],...]\n            # The order of variables in the function must match the order in ranges.\n            integrand_args = (a_params, b_params, gamma, r, s)\n            ranges = [[delta, 1.0], [0.0, 1.0 - delta]]\n            \n            # Using lambda to match the argument signature for nquad\n            # The function to integrate takes x and z as first arguments\n            i_slice, _ = integrate.nquad(lambda x, z: R_integrand(x, z, *integrand_args), ranges)\n\n            # 3b. Calculate C_slice(delta)\n            c_slice = i_slice - np.log(1.0 / delta) * i_log\n            \n            # 3c. Compute the absolute difference\n            diff = np.abs(c_slice - c_sub)\n            case_diffs.append(diff)\n        \n        all_results.append(case_diffs)\n\n    # Format the final output string\n    # e.g., [[d_1(d_1),d_1(d_2),d_1(d_3)],[d_2(d_1),d_2(d_2),d_2(d_3)],...]\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n    print(output_str)\n\nsolve()\n\n```", "id": "3538697"}, {"introduction": "After establishing correctness and numerical stability, the final frontier in practical high-energy calculations is efficiency. Real-world cross-sections are computed via Monte Carlo integration over high-dimensional phase spaces, a process whose convergence can be notoriously slow. This exercise demonstrates how to turn the theoretical structure of subtraction terms into a powerful practical advantage. By exploiting the symmetries inherent in the Catani–Seymour dipole construction, you can implement a variance reduction technique known as correlated (or antithetic) sampling [@problem_id:3538648]. This practice will allow you to analytically quantify the significant computational speedup gained, a vital skill for developing performant and practical NLO calculators.", "problem": "Consider numerical integration at next-to-leading order in Quantum Chromodynamics (QCD) using a subtraction scheme such as the Catani–Seymour dipole method. After subtraction, the integrand difference $f(z,\\phi) = \\lvert M \\rvert^2 - D_{ij,k}$ is finite in the unresolved variables $z \\in (0,1)$ and $\\phi \\in [0,2\\pi)$, where $\\phi$ is an azimuthal angle measured in radians. In a simplified but symmetry-faithful toy model, define the dimensionless residual integrand\n$$\nf_{\\varepsilon,\\eta}(z,\\phi) \\;=\\; \\varepsilon \\,\\Big[\\, a_1 \\,\\sin(2\\pi z)\\,\\cos\\phi \\;+\\; a_2 \\,\\cos(4\\pi z) \\;+\\; a_3 \\,\\cos(2\\phi) \\;+\\; \\eta \\,\\big( b_1 \\,\\sin(2\\pi z) \\;+\\; b_2 \\,\\cos\\phi \\big) \\,\\Big],\n$$\nto be integrated with respect to the uniform measure on $(z,\\phi) \\in (0,1)\\times[0,2\\pi)$. The map\n$$\nT:\\;(z,\\phi) \\mapsto (1-z,\\;\\phi+\\pi)\n$$\ncorresponds to exchanging the roles of emitter and spectator in a Catani–Seymour dipole and flips the azimuth by $\\pi$ radians. Under $T$, functions that are symmetric (unchanged) model dipole-symmetric remnants, while those that are antisymmetric (change sign) model residual anticorrelations between the exact matrix element and the dipole.\n\nYou are to benchmark correlated sampling based on $T$ against independent sampling, in terms of the time-normalized variance required to estimate the integral $\\mathbb{E}[f_{\\varepsilon,\\eta}]$.\n\nDefinitions to use:\n- Independent estimator: draw $N$ independent $(z,\\phi)$ uniformly and average $f_{\\varepsilon,\\eta}(z,\\phi)$. The variance of this estimator scales as $\\mathrm{Var}[f_{\\varepsilon,\\eta}]/N$.\n- Correlated (antithetic) estimator: form $N/2$ independent pairs by drawing $(z,\\phi)$ uniformly and pairing it with $T(z,\\phi)$, then average $Y = \\tfrac{1}{2}\\left(f_{\\varepsilon,\\eta}(z,\\phi)+f_{\\varepsilon,\\eta}(T(z,\\phi))\\right)$ over all pairs. The variance of this estimator scales as $\\mathrm{Var}[Y]/(N/2)$.\n\nDefine the time-normalized speedup as the ratio\n$$\nS \\;=\\; \\frac{\\text{variance per function evaluation for independent sampling}}{\\text{variance per function evaluation for correlated sampling}},\n$$\nwhere a single independent sample consumes one evaluation of $f_{\\varepsilon,\\eta}$, and a single correlated pair consumes two evaluations of $f_{\\varepsilon,\\eta}$. Express $S$ as a real number. Assume all random draws are uniform and ideal; do not include any overheads beyond the count of function evaluations.\n\nUsing only the mathematical definitions above and starting from first principles (laws of large numbers, variance of means, and orthogonality of trigonometric functions under the uniform measure), compute $S$ exactly for each of the following test cases, each specified as a tuple $(a_1,a_2,a_3,b_1,b_2,\\eta,\\varepsilon)$:\n1. $(0.7,\\,0.3,\\,0.2,\\,0.9,\\,0.5,\\,1.0,\\,1.0)$\n2. $(0.7,\\,0.3,\\,0.2,\\,0.9,\\,0.5,\\,0.0,\\,1.0)$\n3. $(0.7,\\,0.3,\\,0.2,\\,0.9,\\,0.5,\\,2.5,\\,1.0)$\n4. $(1.5,\\,1.2,\\,0.8,\\,0.3,\\,0.2,\\,1.5,\\,1.0)$\n\nRequirements:\n- Treat $\\phi$ in radians.\n- Your program must compute the exact analytical value of $S$ implied by the definitions and the uniform measure on $(0,1)\\times[0,2\\pi)$, without Monte Carlo noise.\n- The final result for each test case must be a real number (a float).\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite (e.g., \"[r1,r2,r3,r4]\").", "solution": "The problem requires the calculation of the time-normalized speedup, $S$, of a correlated-sampling (antithetic variate) estimator relative to an independent-sampling estimator for the integral of a function $f_{\\varepsilon,\\eta}(z,\\phi)$.\n\nFirst, we formalize the definition of the speedup $S$. The problem defines $S$ as the ratio of the variance per function evaluation for independent sampling to that for correlated sampling.\nFor the independent estimator, $N$ function evaluations yield $N$ independent samples $f_i = f_{\\varepsilon,\\eta}(z_i, \\phi_i)$. The estimator for the mean is $\\hat{\\mu}_{ind} = \\frac{1}{N} \\sum_{i=1}^N f_i$, and its variance is $\\mathrm{Var}[\\hat{\\mu}_{ind}] = \\frac{\\mathrm{Var}[f_{\\varepsilon,\\eta}]}{N}$. The variance per function evaluation is $N \\times \\mathrm{Var}[\\hat{\\mu}_{ind}] = \\mathrm{Var}[f_{\\varepsilon,\\eta}]$.\n\nFor the correlated estimator, $N$ function evaluations are used to form $N/2$ pairs. Each pair yields a sample $Y_i = \\frac{1}{2}\\left(f_{\\varepsilon,\\eta}(z_i,\\phi_i)+f_{\\varepsilon,\\eta}(T(z_i,\\phi_i))\\right)$. The estimator for the mean is $\\hat{\\mu}_{corr} = \\frac{2}{N} \\sum_{i=1}^{N/2} Y_i$, and its variance is $\\mathrm{Var}[\\hat{\\mu}_{corr}] = \\frac{\\mathrm{Var}[Y]}{(N/2)}$. To compare on the basis of an equal number of total function evaluations, we consider the variance achieved with $N$ evaluations. This is $\\frac{2\\mathrm{Var}[Y]}{N}$. The variance per function evaluation is $N \\times \\mathrm{Var}[\\hat{\\mu}_{corr}] = 2\\mathrm{Var}[Y]$.\n\nThus, the speedup is given by:\n$$\nS = \\frac{\\mathrm{Var}[f_{\\varepsilon,\\eta}]}{2\\mathrm{Var}[Y]}\n$$\nwhere $Y = \\frac{1}{2}\\left(f_{\\varepsilon,\\eta}(z,\\phi) + f_{\\varepsilon,\\eta}(T(z,\\phi))\\right)$. To compute these variances, we recall that for any random variable $X$, $\\mathrm{Var}[X] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. The expectation is taken with respect to the uniform measure on $(z,\\phi) \\in (0,1)\\times[0,2\\pi)$, so $\\mathbb{E}[g] = \\frac{1}{2\\pi}\\int_0^{2\\pi} d\\phi \\int_0^1 dz \\, g(z,\\phi)$.\n\nLet's first compute the expectation of $f_{\\varepsilon,\\eta}$. The function is a linear combination of trigonometric terms:\n$$\nf_{\\varepsilon,\\eta}(z,\\phi) \\;=\\; \\varepsilon \\,\\Big[\\, a_1 \\,\\sin(2\\pi z)\\,\\cos\\phi \\;+\\; a_2 \\,\\cos(4\\pi z) \\;+\\; a_3 \\,\\cos(2\\phi) \\;+\\; \\eta \\,\\big( b_1 \\,\\sin(2\\pi z) \\;+\\; b_2 \\,\\cos\\phi \\big) \\,\\Big]\n$$\nThe integral of each trigonometric term over its respective domain $(0,1)$ or $[0,2\\pi)$ is zero:\n$\\int_0^1 \\sin(2\\pi z) dz = 0$, $\\int_0^1 \\cos(4\\pi z) dz = 0$, $\\int_0^{2\\pi} \\cos\\phi d\\phi = 0$, $\\int_0^{2\\pi} \\cos(2\\phi) d\\phi = 0$.\nDue to the linearity of integration, every term in $f_{\\varepsilon,\\eta}$ has an expectation value of zero. Thus, $\\mathbb{E}[f_{\\varepsilon,\\eta}] = 0$.\nThis simplifies the variance calculation to $\\mathrm{Var}[f_{\\varepsilon,\\eta}] = \\mathbb{E}[f_{\\varepsilon,\\eta}^2]$.\n\nSimilarly, for $Y$, its expectation is $\\mathbb{E}[Y] = \\mathbb{E}\\left[\\frac{1}{2}(f_{\\varepsilon,\\eta} + f_{\\varepsilon,\\eta} \\circ T)\\right] = \\frac{1}{2}(\\mathbb{E}[f_{\\varepsilon,\\eta}] + \\mathbb{E}[f_{\\varepsilon,\\eta} \\circ T])$. Since the transformation $T$ is a measure-preserving map of the integration domain onto itself, $\\mathbb{E}[f_{\\varepsilon,\\eta} \\circ T] = \\mathbb{E}[f_{\\varepsilon,\\eta}] = 0$. Therefore, $\\mathbb{E}[Y] = 0$, and $\\mathrm{Var}[Y] = \\mathbb{E}[Y^2]$.\n\nThe problem is now reduced to computing $\\mathbb{E}[f_{\\varepsilon,\\eta}^2]$ and $\\mathbb{E}[Y^2]$.\nLet us decompose $f_{\\varepsilon,\\eta}$ into its symmetric part $f_S = \\frac{1}{2}(f_{\\varepsilon,\\eta} + f_{\\varepsilon,\\eta} \\circ T)$ and its antisymmetric part $f_A = \\frac{1}{2}(f_{\\varepsilon,\\eta} - f_{\\varepsilon,\\eta} \\circ T)$ with respect to the transformation $T$. Note that by definition, $Y = f_S$.\nTo find these parts, we examine the behavior of each term in $f_{\\varepsilon,\\eta}$ under $T:(z,\\phi) \\mapsto (1-z,\\phi+\\pi)$:\n\\begin{itemize}\n    \\item $\\sin(2\\pi(1-z))\\,\\cos(\\phi+\\pi) = (-\\sin(2\\pi z))(-\\cos\\phi) = \\sin(2\\pi z)\\cos\\phi$ (Symmetric)\n    \\item $\\cos(4\\pi(1-z)) = \\cos(4\\pi z)$ (Symmetric)\n    \\item $\\cos(2(\\phi+\\pi)) = \\cos(2\\phi)$ (Symmetric)\n    \\item $\\sin(2\\pi(1-z)) = -\\sin(2\\pi z)$ (Antisymmetric)\n    \\item $\\cos(\\phi+\\pi) = -\\cos\\phi$ (Antisymmetric)\n\\end{itemize}\nThus, the symmetric and antisymmetric parts of $f_{\\varepsilon,\\eta}$ are:\n$$\nf_S(z,\\phi) = \\varepsilon \\left[ a_1 \\sin(2\\pi z)\\cos\\phi + a_2 \\cos(4\\pi z) + a_3 \\cos(2\\phi) \\right]\n$$\n$$\nf_A(z,\\phi) = \\varepsilon \\eta \\left[ b_1 \\sin(2\\pi z) + b_2 \\cos\\phi \\right]\n$$\nThe set of basis functions $\\{\\sin(2\\pi z)\\cos\\phi, \\cos(4\\pi z), \\cos(2\\phi), \\sin(2\\pi z), \\cos\\phi\\}$ is orthogonal under the given integration measure. This means that $\\mathbb{E}[f_S f_A] = 0$.\nThe variance of $f_{\\varepsilon,\\eta} = f_S + f_A$ is therefore $\\mathrm{Var}[f_{\\varepsilon,\\eta}] = \\mathbb{E}[(f_S+f_A)^2] = \\mathbb{E}[f_S^2] + \\mathbb{E}[f_A^2] + 2\\mathbb{E}[f_S f_A] = \\mathrm{Var}[f_S] + \\mathrm{Var}[f_A]$.\n\nThe speedup can be expressed as:\n$$\nS = \\frac{\\mathrm{Var}[f_{\\varepsilon,\\eta}]}{2\\mathrm{Var}[Y]} = \\frac{\\mathrm{Var}[f_S] + \\mathrm{Var}[f_A]}{2\\mathrm{Var}[f_S]} = \\frac{1}{2} \\left(1 + \\frac{\\mathrm{Var}[f_A]}{\\mathrm{Var}[f_S]}\\right)\n$$\nWe compute the variances $\\mathrm{Var}[f_S] = \\mathbb{E}[f_S^2]$ and $\\mathrm{Var}[f_A] = \\mathbb{E}[f_A^2]$ by integrating the squares of the terms, leveraging orthogonality. We use the following standard integrals:\n$$\n\\int_0^1 \\sin^2(k\\pi z) dz = \\frac{1}{2}, \\quad \\int_0^1 \\cos^2(k\\pi z) dz = \\frac{1}{2} \\quad (\\text{for integer } k \\neq 0)\n$$\n$$\n\\frac{1}{2\\pi}\\int_0^{2\\pi} \\cos^2(m\\phi) d\\phi = \\frac{1}{2} \\quad (\\text{for integer } m \\neq 0)\n$$\nThe variances of the constituent terms are:\n$\\mathbb{E}[(\\sin(2\\pi z)\\cos\\phi)^2] = \\mathbb{E}[\\sin^2(2\\pi z)]\\mathbb{E}[\\cos^2\\phi] = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$.\n$\\mathbb{E}[\\cos^2(4\\pi z)] = \\frac{1}{2}$.\n$\\mathbb{E}[\\cos^2(2\\phi)] = \\frac{1}{2}$.\n$\\mathbb{E}[\\sin^2(2\\pi z)] = \\frac{1}{2}$.\n$\\mathbb{E}[\\cos^2\\phi] = \\frac{1}{2}$.\n\nSumming the variances of the orthogonal components:\n$$\n\\mathrm{Var}[f_S] = \\mathbb{E}[f_S^2] = \\varepsilon^2 \\left( a_1^2 \\mathbb{E}[(\\sin(2\\pi z)\\cos\\phi)^2] + a_2^2 \\mathbb{E}[\\cos^2(4\\pi z)] + a_3^2 \\mathbb{E}[\\cos^2(2\\phi)] \\right)\n$$\n$$\n\\mathrm{Var}[f_S] = \\varepsilon^2 \\left( a_1^2 \\frac{1}{4} + a_2^2 \\frac{1}{2} + a_3^2 \\frac{1}{2} \\right) = \\frac{\\varepsilon^2}{4} \\left( a_1^2 + 2a_2^2 + 2a_3^2 \\right)\n$$\n$$\n\\mathrm{Var}[f_A] = \\mathbb{E}[f_A^2] = (\\varepsilon\\eta)^2 \\left( b_1^2 \\mathbb{E}[\\sin^2(2\\pi z)] + b_2^2 \\mathbb{E}[\\cos^2\\phi] \\right)\n$$\n$$\n\\mathrm{Var}[f_A] = \\varepsilon^2\\eta^2 \\left( b_1^2 \\frac{1}{2} + b_2^2 \\frac{1}{2} \\right) = \\frac{\\varepsilon^2\\eta^2}{2} \\left( b_1^2 + b_2^2 \\right)\n$$\nThe speedup $S$ is then:\n$$\nS = \\frac{1}{2} \\left(1 + \\frac{\\frac{\\varepsilon^2\\eta^2}{2}(b_1^2 + b_2^2)}{\\frac{\\varepsilon^2}{4}(a_1^2 + 2a_2^2 + 2a_3^2)}\\right)\n$$\nThe factor $\\varepsilon^2$ cancels, as expected. The final analytical formula for the speedup is:\n$$\nS = \\frac{1}{2} \\left( 1 + \\frac{2\\eta^2(b_1^2 + b_2^2)}{a_1^2 + 2a_2^2 + 2a_3^2} \\right)\n$$\nThis formula is valid provided the denominator is non-zero, which is true for all given test cases. We now apply this formula to each case.\n\nCase 1: $(0.7, 0.3, 0.2, 0.9, 0.5, 1.0, 1.0)$\nDenominator: $0.7^2 + 2(0.3^2) + 2(0.2^2) = 0.49 + 2(0.09) + 2(0.04) = 0.49 + 0.18 + 0.08 = 0.75$.\nNumerator: $2(1.0^2)(0.9^2 + 0.5^2) = 2(0.81 + 0.25) = 2(1.06) = 2.12$.\n$S = \\frac{1}{2} (1 + \\frac{2.12}{0.75}) = \\frac{1}{2} (1 + \\frac{212}{75}) = \\frac{1}{2} (\\frac{75+212}{75}) = \\frac{287}{150} \\approx 1.91333...$\n\nCase 2: $(0.7, 0.3, 0.2, 0.9, 0.5, 0.0, 1.0)$\nHere, $\\eta=0$. The formula simplifies to $S = \\frac{1}{2}(1 + 0) = 0.5$.\n\nCase 3: $(0.7, 0.3, 0.2, 0.9, 0.5, 2.5, 1.0)$\nDenominator is the same as Case 1: $0.75$.\nNumerator: $2(2.5^2)(0.9^2 + 0.5^2) = 2(6.25)(1.06) = 12.5(1.06) = 13.25$.\n$S = \\frac{1}{2} (1 + \\frac{13.25}{0.75}) = \\frac{1}{2} (1 + \\frac{1325}{75}) = \\frac{1}{2} (1 + \\frac{53}{3}) = \\frac{1}{2} (\\frac{3+53}{3}) = \\frac{56}{6} = \\frac{28}{3} \\approx 9.33333...$\n\nCase 4: $(1.5, 1.2, 0.8, 0.3, 0.2, 1.5, 1.0)$\nDenominator: $1.5^2 + 2(1.2^2) + 2(0.8^2) = 2.25 + 2(1.44) + 2(0.64) = 2.25 + 2.88 + 1.28 = 6.41$.\nNumerator: $2(1.5^2)(0.3^2 + 0.2^2) = 2(2.25)(0.09 + 0.04) = 4.5(0.13) = 0.585$.\n$S = \\frac{1}{2} (1 + \\frac{0.585}{6.41}) = \\frac{1}{2} (\\frac{6.41+0.585}{6.41}) = \\frac{6.995}{2 \\times 6.41} = \\frac{6.995}{12.82} = \\frac{1399}{2564} \\approx 0.54563...$\n\nThese analytical values are implemented in the final program.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the exact time-normalized speedup S for correlated sampling\n    based on the analytical formula derived from the problem statement.\n    \"\"\"\n\n    # Define the test cases from the problem statement as a list of tuples.\n    # Each tuple is (a1, a2, a3, b1, b2, eta, epsilon).\n    test_cases = [\n        (0.7, 0.3, 0.2, 0.9, 0.5, 1.0, 1.0),\n        (0.7, 0.3, 0.2, 0.9, 0.5, 0.0, 1.0),\n        (0.7, 0.3, 0.2, 0.9, 0.5, 2.5, 1.0),\n        (1.5, 1.2, 0.8, 0.3, 0.2, 1.5, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        a1, a2, a3, b1, b2, eta, _ = case # epsilon is not needed\n\n        # The analytical formula for speedup S is:\n        # S = 0.5 * (1 + (Var_antisymmetric / Var_symmetric))\n        # Var_symmetric is proportional to a1^2 + 2*a2^2 + 2*a3^2\n        # Var_antisymmetric is proportional to 2*eta^2 * (b1^2 + b2^2)\n        \n        # Calculate the term corresponding to the variance from symmetric components.\n        # This is the denominator of the ratio term in the formula for S.\n        var_sym_term = a1**2 + 2 * a2**2 + 2 * a3**2\n        \n        # Calculate the term corresponding to the variance from antisymmetric components.\n        # This is the numerator of the ratio term.\n        var_anti_term = 2 * eta**2 * (b1**2 + b2**2)\n        \n        # Check for division by zero, though not expected for the given test cases.\n        # If the symmetric part is zero, the correlated estimator variance is zero,\n        # leading to infinite speedup (unless the antisymmetric part is also zero).\n        if var_sym_term == 0:\n            if var_anti_term == 0:\n                # f is identically zero. Speedup is ill-defined, let's say 1.\n                s = 1.0\n            else:\n                s = float('inf')\n        else:\n            # Apply the final analytical formula for S.\n            s = 0.5 * (1 + var_anti_term / var_sym_term)\n            \n        results.append(s)\n\n    # Format the output as a comma-separated list of floats inside square brackets.\n    # The map(str, ...) ensures float representation for each result.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3538648"}]}