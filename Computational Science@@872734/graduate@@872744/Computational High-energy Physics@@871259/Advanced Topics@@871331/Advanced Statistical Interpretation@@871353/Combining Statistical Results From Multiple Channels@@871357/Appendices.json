{"hands_on_practices": [{"introduction": "The core task in many physics analyses is combining data from different regions, such as Signal Regions (SR) and Control Regions (CR), to simultaneously estimate a signal strength ($\\mu$) and constrain background uncertainties. This exercise ([@problem_id:3509004]) provides a hands-on derivation of the combined Maximum Likelihood Estimator (MLE) by profiling background nuisance parameters. Mastering this method is essential, as it forms the foundation of countless measurements in high-energy physics.", "problem": "In a dual-channel measurement typical of computational high-energy physics, consider two statistically independent channels indexed by $k \\in \\{1,2\\}$. Each channel has a Signal Region (SR) count $n^{SR}_k$ and a Control Region (CR) count $n^{CR}_k$. The SR is modeled as a Poisson process with mean\n$$\n\\lambda^{SR}_k(\\mu, b_k) = \\mu\\, s^{SR}_k + t_k\\, b_k,\n$$\nwhere $\\mu$ is a common signal strength parameter that scales the known expected signal yield $s^{SR}_k$ in the SR, and $t_k$ is a known transfer factor relating the CR background rate $b_k$ to the SR background expectation. The CR is modeled as a Poisson process with mean\n$$\n\\lambda^{CR}_k(b_k) = b_k.\n$$\nAssume the channels are independent conditioned on $(\\mu, b_1, b_2)$, and the joint likelihood is the product of the Poisson likelihoods in the two regions for both channels.\n\nStarting from the definitions of the Poisson likelihood and the notion of Maximum Likelihood Estimators (MLE), derive the combined MLE $\\hat{\\mu}$ by profiling the background rates $b_k$, and compute the approximate standard deviation of $\\hat{\\mu}$ using the observed information, defined as the negative Hessian of the log-likelihood evaluated at the MLE and profiled over $b_k$.\n\nUse the following specified numbers:\n- Channel $1$: $n^{SR}_1 = 11$, $n^{CR}_1 = 40$, $s^{SR}_1 = 5.0$, $t_1 = 0.2$.\n- Channel $2$: $n^{SR}_2 = 10$, $n^{CR}_2 = 14$, $s^{SR}_2 = 5.0$, $t_2 = 0.5$.\n\nRound both the combined $\\hat{\\mu}$ and its approximate standard deviation to four significant figures. Express both quantities as dimensionless numbers.", "solution": "First, we define the parameters and observables for the two channels, indexed by $k \\in \\{1,2\\}$:\n- Observed counts in the Signal Region (SR): $n^{SR}_k$\n- Observed counts in the Control Region (CR): $n^{CR}_k$\n- Expected signal yield in the SR: $s^{SR}_k$\n- Transfer factor from CR to SR: $t_k$\n- Signal strength modifier: $\\mu$\n- Background rate in the CR (nuisance parameter): $b_k$\n\nThe SR and CR counts are modeled as independent Poisson processes. The mean of the Poisson distribution for each region is given by:\n- SR mean: $\\lambda^{SR}_k(\\mu, b_k) = \\mu\\, s^{SR}_k + t_k\\, b_k$\n- CR mean: $\\lambda^{CR}_k(b_k) = b_k$\n\nThe joint likelihood function $L(\\mu, b_1, b_2)$ is the product of four Poisson probabilities:\n$$L(\\mu, b_1, b_2) = \\prod_{k=1,2} \\left[ \\frac{(\\lambda^{SR}_k)^{n^{SR}_k} \\exp(-\\lambda^{SR}_k)}{n^{SR}_k!} \\cdot \\frac{(\\lambda^{CR}_k)^{n^{CR}_k} \\exp(-\\lambda^{CR}_k)}{n^{CR}_k!} \\right]$$\nTo find the Maximum Likelihood Estimators (MLEs), we maximize the log-likelihood function $\\ell = \\ln L$. Ignoring the constant factorial terms, we have:\n$$\\ell(\\mu, b_1, b_2) = \\sum_{k=1,2} \\left[ n^{SR}_k \\ln(\\lambda^{SR}_k) - \\lambda^{SR}_k + n^{CR}_k \\ln(\\lambda^{CR}_k) - \\lambda^{CR}_k \\right]$$\nSubstituting the expressions for the means:\n$$\\ell(\\mu, b_1, b_2) = \\sum_{k=1,2} \\left[ n^{SR}_k \\ln(\\mu s^{SR}_k + t_k b_k) - (\\mu s^{SR}_k + t_k b_k) + n^{CR}_k \\ln(b_k) - b_k \\right]$$\nTo find the MLEs $(\\hat{\\mu}, \\hat{b}_1, \\hat{b}_2)$, we set the partial derivatives of $\\ell$ with respect to each parameter to zero:\n1. $\\frac{\\partial \\ell}{\\partial \\mu} = \\sum_{k=1,2} \\left( \\frac{n^{SR}_k s^{SR}_k}{\\mu s^{SR}_k + t_k b_k} - s^{SR}_k \\right) = 0$\n2. $\\frac{\\partial \\ell}{\\partial b_k} = \\frac{n^{SR}_k t_k}{\\mu s^{SR}_k + t_k b_k} - t_k + \\frac{n^{CR}_k}{b_k} - 1 = 0 \\quad \\text{for } k=1,2$\n\nNote that the MLE for a simple Poisson measurement $n$ with mean $\\lambda$ is $\\hat{\\lambda}=n$. In our multi-parameter problem, the parameters are constrained. However, let us investigate if the unconstrained maximum, where the expected number of events equals the observed number in each region, satisfies the model constraints for the given data.\nThe unconstrained MLEs for the means would be $\\hat{\\lambda}^{SR}_k = n^{SR}_k$ and $\\hat{\\lambda}^{CR}_k = n^{CR}_k$.\nFrom the CR definition, $\\hat{\\lambda}^{CR}_k = b_k$, so this implies $\\hat{b}_k = n^{CR}_k$.\nSubstituting these into the model equation for the SR mean gives a condition on $\\mu$:\n$$n^{SR}_k = \\mu s^{SR}_k + t_k n^{CR}_k \\implies \\mu = \\frac{n^{SR}_k - t_k n^{CR}_k}{s^{SR}_k}$$\nFor this to be a valid solution for the combined fit, the value of $\\mu$ derived from each channel must be the same. Let's test this with the provided numbers.\n- Channel $1$: $n^{SR}_1 = 11$, $n^{CR}_1 = 40$, $s^{SR}_1 = 5.0$, $t_1 = 0.2$.\n$$\\mu_1 = \\frac{11 - (0.2)(40)}{5.0} = \\frac{11 - 8}{5.0} = \\frac{3}{5.0} = 0.6$$\n- Channel $2$: $n^{SR}_2 = 10$, $n^{CR}_2 = 14$, $s^{SR}_2 = 5.0$, $t_2 = 0.5$.\n$$\\mu_2 = \\frac{10 - (0.5)(14)}{5.0} = \\frac{10 - 7}{5.0} = \\frac{3}{5.0} = 0.6$$\nSince $\\mu_1 = \\mu_2$, the point $(\\hat{\\mu}, \\hat{b}_1, \\hat{b}_2) = (0.6, 40, 14)$ simultaneously satisfies the first-order conditions for all three parameters. This point corresponds to the global maximum of the likelihood function correctly residing on the constraint surface defined by the model.\nTherefore, the combined MLE for the signal strength is $\\hat{\\mu} = 0.6$.\n\nNext, we calculate the approximate standard deviation of $\\hat{\\mu}$. For a large sample size, the variance of the MLE $\\hat{\\theta}$ can be approximated by the inverse of the observed information matrix $J(\\hat{\\theta}) = -\\frac{\\partial^2 \\ell}{\\partial \\theta_i \\partial \\theta_j} \\big|_{\\hat{\\theta}}$.\nWhen nuisance parameters (here, $b_k$) are present, the variance of the parameter of interest ($\\mu$) is obtained from the corresponding diagonal element of the inverse of the full information matrix. This is equivalent to using the inverse of the profiled information for $\\mu$.\nThe information on the profiled parameter $\\mu$ is given by $I_{p}(\\mu) = I_{\\mu\\mu} - \\mathbf{I}_{\\mu b} \\mathbf{I}_{bb}^{-1} \\mathbf{I}_{b\\mu}$, where $I_{ij} = -H_{ij} = -\\frac{\\partial^2 \\ell}{\\partial\\theta_i \\partial\\theta_j}$.\nThe Hessian matrix elements are:\n$H_{\\mu\\mu} = \\frac{\\partial^2 \\ell}{\\partial \\mu^2} = -\\sum_k \\frac{n^{SR}_k (s^{SR}_k)^2}{(\\lambda^{SR}_k)^2}$\n$H_{b_k b_k} = \\frac{\\partial^2 \\ell}{\\partial b_k^2} = -\\frac{n^{SR}_k t_k^2}{(\\lambda^{SR}_k)^2} - \\frac{n^{CR}_k}{b_k^2}$\n$H_{\\mu b_k} = \\frac{\\partial^2 \\ell}{\\partial \\mu \\partial b_k} = -\\frac{n^{SR}_k s^{SR}_k t_k}{(\\lambda^{SR}_k)^2}$\n$H_{b_1 b_2} = 0$, which makes the matrix $\\mathbf{H}_{bb}$ diagonal.\n\nWe evaluate these at the MLEs: $\\hat{\\mu}=0.6$, $\\hat{b}_1=40$, $\\hat{b}_2=14$. At this point, $\\hat{\\lambda}^{SR}_k = n^{SR}_k$ and $\\hat{b}_k = n^{CR}_k$.\n$H_{\\mu\\mu} = -\\sum_k \\frac{(s^{SR}_k)^2}{n^{SR}_k} = -\\left( \\frac{5.0^2}{11} + \\frac{5.0^2}{10} \\right) = -25 \\left(\\frac{1}{11} + \\frac{1}{10}\\right) = -\\frac{525}{110} = -\\frac{105}{22}$.\n$H_{b_k b_k} = -\\frac{t_k^2}{n^{SR}_k} - \\frac{1}{n^{CR}_k}$.\n$H_{\\mu b_k} = -\\frac{s^{SR}_k t_k}{n^{SR}_k}$.\n\nThe variance $\\sigma^2_{\\hat{\\mu}}$ is given by $(I_p(\\hat{\\mu}))^{-1}$.\nThe information from each channel adds linearly. For a single channel $k$, the information is:\n$I_k = -H_{\\mu\\mu, k} - \\frac{H_{\\mu b_k,k}^2}{H_{b_k b_k,k}} = \\frac{(s^{SR}_k)^2}{n^{SR}_k} - \\frac{(s^{SR}_k t_k/n^{SR}_k)^2}{t_k^2/n^{SR}_k + 1/n^{CR}_k}$\n$I_k = \\frac{(s^{SR}_k)^2}{n^{SR}_k} \\left( 1 - \\frac{t_k^2/n^{SR}_k}{(t_k^2 n^{CR}_k + n^{SR}_k)/(n^{SR}_k n^{CR}_k)} \\right) = \\frac{(s^{SR}_k)^2}{n^{SR}_k} \\left( 1 - \\frac{t_k^2 n^{CR}_k}{t_k^2 n^{CR}_k + n^{SR}_k} \\right)$\n$I_k = \\frac{(s^{SR}_k)^2}{n^{SR}_k} \\left( \\frac{n^{SR}_k}{n^{SR}_k + t_k^2 n^{CR}_k} \\right) = \\frac{(s^{SR}_k)^2}{n^{SR}_k + t_k^2 n^{CR}_k}$.\nThis is a standard result. The total information is the sum over the channels: $I_p(\\hat{\\mu}) = \\sum_k I_k$.\n\n- Channel $1$: $I_1 = \\frac{5.0^2}{11 + 0.2^2(40)} = \\frac{25}{11 + 1.6} = \\frac{25}{12.6} = \\frac{125}{63}$.\n- Channel $2$: $I_2 = \\frac{5.0^2}{10 + 0.5^2(14)} = \\frac{25}{10 + 3.5} = \\frac{25}{13.5} = \\frac{50}{27}$.\n\n$I_p(\\hat{\\mu}) = I_1 + I_2 = \\frac{125}{63} + \\frac{50}{27} = \\frac{375 + 350}{189} = \\frac{725}{189}$.\nThe variance is $\\sigma^2_{\\hat{\\mu}} = \\frac{1}{I_p(\\hat{\\mu})} = \\frac{189}{725}$.\nThe standard deviation is $\\sigma_{\\hat{\\mu}} = \\sqrt{\\frac{189}{725}} \\approx 0.510577765$.\n\nRounding the results to four significant figures:\n$\\hat{\\mu} = 0.6000$\n$\\sigma_{\\hat{\\mu}} \\approx 0.5106$", "answer": "$$ \\boxed{ \\begin{pmatrix} 0.6000 & 0.5106 \\end{pmatrix} } $$", "id": "3509004"}, {"introduction": "While combining channels increases statistical power, the final precision is often limited by systematic uncertainties. This exercise ([@problem_id:3509031]) demonstrates the profound impact of how these uncertainties are correlated across channels. By comparing scenarios of independence and full correlation within a simplified Gaussian framework, you will quantify why accurately modeling the covariance structure is not just a detail, but a critical factor in any combined measurement.", "problem": "Consider a two-channel measurement of a signal strength parameter $\\mu$ in computational High-Energy Physics (HEP), where each channel $i \\in \\{1,2\\}$ provides an approximately Gaussian-distributed observable $y_{i}$ with mean $\\mu s_{i}$ and an uncertainty budget composed of statistical uncertainty and a dominant normalization systematic. Work in the large-sample (asymptotic) regime and assume a Gaussian likelihood for the joint vector $\\mathbf{y} = (y_{1}, y_{2})^{\\top}$ with mean $\\boldsymbol{\\mu} = \\mu \\mathbf{s}$, where $\\mathbf{s} = (s_{1}, s_{2})^{\\top}$ denotes the nominal signal yields for $\\mu = 1$.\n\nThe dominant systematic is an integrated luminosity uncertainty that acts multiplicatively on the yields with fractional size $f$, and its effect on the channel-wise yields at $\\mu = 1$ can be linearized as absolute standard deviations $d_{i} = f s_{i}$. You are to compare two hypotheses for this single dominant systematic across the two channels:\n- Independence hypothesis: the luminosity uncertainty contributions are treated as independent between channels, adding $d_{i}^{2}$ only to the diagonal elements of the covariance.\n- Full-correlation hypothesis: the luminosity uncertainty is treated as fully correlated, contributing a rank-$1$ covariance term $\\mathbf{d}\\mathbf{d}^{\\top}$, where $\\mathbf{d} = (d_{1}, d_{2})^{\\top}$.\n\nAssume the following scientifically plausible inputs for an analysis targeting $\\mu$ near unity:\n- Nominal signal yields: $s_{1} = 80$, $s_{2} = 120$.\n- Statistical standard deviations: $\\sigma_{\\text{stat},1} = 20$, $\\sigma_{\\text{stat},2} = 25$.\n- Dominant luminosity fractional uncertainty: $f = 0.05$.\n\nUsing only the Gaussian likelihood framework and first principles of the Fisher Information, derive and compute the variance of the combined estimator $\\hat{\\mu}$ under each hypothesis for the dominant systematic, and then compute the change in variance defined by\n$$\\Delta \\mathrm{Var}(\\hat{\\mu}) \\equiv \\mathrm{Var}_{\\text{full corr}}(\\hat{\\mu}) - \\mathrm{Var}_{\\text{indep}}(\\hat{\\mu}).$$\nRound your final numerical answer to four significant figures. Express the final quantity as a dimensionless number (no units).", "solution": "The problem requires the calculation of the variance of a maximum likelihood estimator (MLE) for a signal strength parameter $\\mu$. The framework is a two-channel measurement with observables $\\mathbf{y} = (y_1, y_2)^{\\top}$ described by a multivariate Gaussian likelihood. In the large-sample limit, the variance of the MLE $\\hat{\\mu}$ is given by the Cram√©r-Rao Lower Bound, which is the inverse of the Fisher Information, $I(\\mu)$.\n\nThe likelihood function is given by:\n$$L(\\mu; \\mathbf{y}) \\propto \\exp\\left(-\\frac{1}{2} (\\mathbf{y} - \\mu\\mathbf{s})^{\\top} \\mathbf{V}^{-1} (\\mathbf{y} - \\mu\\mathbf{s})\\right)$$\nwhere $\\mathbf{s} = (s_1, s_2)^{\\top}$ is the vector of nominal signal yields and $\\mathbf{V}$ is the total covariance matrix of the observables. The log-likelihood is:\n$$\\ln L(\\mu) = C - \\frac{1}{2} (\\mathbf{y} - \\mu\\mathbf{s})^{\\top} \\mathbf{V}^{-1} (\\mathbf{y} - \\mu\\mathbf{s})$$\nwhere $C$ is a constant independent of $\\mu$.\n\nThe Fisher Information $I(\\mu)$ is calculated as $I(\\mu) = -E\\left[\\frac{\\partial^2 \\ln L}{\\partial \\mu^2}\\right]$.\nThe first derivative with respect to $\\mu$ is:\n$$\\frac{\\partial \\ln L}{\\partial \\mu} = - \\frac{1}{2} \\left[ (-\\mathbf{s})^{\\top} \\mathbf{V}^{-1} (\\mathbf{y} - \\mu\\mathbf{s}) + (\\mathbf{y} - \\mu\\mathbf{s})^{\\top} \\mathbf{V}^{-1} (-\\mathbf{s}) \\right] = \\mathbf{s}^{\\top} \\mathbf{V}^{-1} (\\mathbf{y} - \\mu\\mathbf{s})$$\nThe second derivative is:\n$$\\frac{\\partial^2 \\ln L}{\\partial \\mu^2} = \\frac{\\partial}{\\partial \\mu} \\left[ \\mathbf{s}^{\\top} \\mathbf{V}^{-1} \\mathbf{y} - \\mu \\mathbf{s}^{\\top} \\mathbf{V}^{-1} \\mathbf{s} \\right] = -\\mathbf{s}^{\\top} \\mathbf{V}^{-1} \\mathbf{s}$$\nSince the second derivative is independent of the data $\\mathbf{y}$, the expectation is trivial:\n$$I(\\mu) = -E\\left[-\\mathbf{s}^{\\top} \\mathbf{V}^{-1} \\mathbf{s}\\right] = \\mathbf{s}^{\\top} \\mathbf{V}^{-1} \\mathbf{s}$$\nThe variance of the estimator $\\hat{\\mu}$ is therefore:\n$$\\mathrm{Var}(\\hat{\\mu}) = I(\\mu)^{-1} = (\\mathbf{s}^{\\top} \\mathbf{V}^{-1} \\mathbf{s})^{-1}$$\nWe must now construct the covariance matrix $\\mathbf{V}$ for each of the two hypotheses. The total covariance matrix is the sum of the statistical and systematic covariance matrices, $\\mathbf{V} = \\mathbf{V}_{\\text{stat}} + \\mathbf{V}_{\\text{sys}}$.\n\nFirst, we define the given quantities in matrix/vector form:\nNominal signal yields: $\\mathbf{s} = \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix} = \\begin{pmatrix} 80 \\\\ 120 \\end{pmatrix}$.\nStatistical standard deviations: $\\sigma_{\\text{stat},1} = 20$, $\\sigma_{\\text{stat},2} = 25$.\nThe statistical covariance matrix $\\mathbf{V}_{\\text{stat}}$ is diagonal, as statistical uncertainties are independent:\n$$\\mathbf{V}_{\\text{stat}} = \\begin{pmatrix} \\sigma_{\\text{stat},1}^2 & 0 \\\\ 0 & \\sigma_{\\text{stat},2}^2 \\end{pmatrix} = \\begin{pmatrix} 20^2 & 0 \\\\ 0 & 25^2 \\end{pmatrix} = \\begin{pmatrix} 400 & 0 \\\\ 0 & 625 \\end{pmatrix}$$\nThe systematic uncertainty is defined by a fractional size $f=0.05$. The absolute standard deviations on the yields are $d_i = f s_i$:\n$d_1 = 0.05 \\times 80 = 4$\n$d_2 = 0.05 \\times 120 = 6$\nThe vector of systematic standard deviations is $\\mathbf{d} = \\begin{pmatrix} 4 \\\\ 6 \\end{pmatrix}$.\n\n**Independence Hypothesis**\nUnder this hypothesis, the systematic uncertainties are uncorrelated. The systematic covariance matrix $\\mathbf{V}_{\\text{sys, indep}}$ is diagonal:\n$$\\mathbf{V}_{\\text{sys, indep}} = \\begin{pmatrix} d_1^2 & 0 \\\\ 0 & d_2^2 \\end{pmatrix} = \\begin{pmatrix} 4^2 & 0 \\\\ 0 & 6^2 \\end{pmatrix} = \\begin{pmatrix} 16 & 0 \\\\ 0 & 36 \\end{pmatrix}$$\nThe total covariance matrix $\\mathbf{V}_{\\text{indep}}$ is:\n$$\\mathbf{V}_{\\text{indep}} = \\mathbf{V}_{\\text{stat}} + \\mathbf{V}_{\\text{sys, indep}} = \\begin{pmatrix} 400 & 0 \\\\ 0 & 625 \\end{pmatrix} + \\begin{pmatrix} 16 & 0 \\\\ 0 & 36 \\end{pmatrix} = \\begin{pmatrix} 416 & 0 \\\\ 0 & 661 \\end{pmatrix}$$\nThe Fisher Information is $I_{\\text{indep}} = \\mathbf{s}^{\\top} \\mathbf{V}_{\\text{indep}}^{-1} \\mathbf{s}$. Since $\\mathbf{V}_{\\text{indep}}$ is diagonal, this is straightforward:\n$$I_{\\text{indep}} = \\begin{pmatrix} 80 & 120 \\end{pmatrix} \\begin{pmatrix} 1/416 & 0 \\\\ 0 & 1/661 \\end{pmatrix} \\begin{pmatrix} 80 \\\\ 120 \\end{pmatrix} = \\frac{80^2}{416} + \\frac{120^2}{661} = \\frac{6400}{416} + \\frac{14400}{661}$$\n$$I_{\\text{indep}} = \\frac{200}{13} + \\frac{14400}{661} = \\frac{200 \\times 661 + 14400 \\times 13}{13 \\times 661} = \\frac{132200 + 187200}{8593} = \\frac{319400}{8593}$$\nThe variance under the independence hypothesis is:\n$$\\mathrm{Var}_{\\text{indep}}(\\hat{\\mu}) = I_{\\text{indep}}^{-1} = \\frac{8593}{319400}$$\n\n**Full-Correlation Hypothesis**\nUnder this hypothesis, the systematic uncertainty is fully correlated, contributing a rank-1 term $\\mathbf{V}_{\\text{sys, full corr}} = \\mathbf{d}\\mathbf{d}^{\\top}$:\n$$\\mathbf{V}_{\\text{sys, full corr}} = \\begin{pmatrix} 4 \\\\ 6 \\end{pmatrix} \\begin{pmatrix} 4 & 6 \\end{pmatrix} = \\begin{pmatrix} 16 & 24 \\\\ 24 & 36 \\end{pmatrix}$$\nThe total covariance matrix $\\mathbf{V}_{\\text{full corr}}$ is:\n$$\\mathbf{V}_{\\text{full corr}} = \\mathbf{V}_{\\text{stat}} + \\mathbf{V}_{\\text{sys, full corr}} = \\begin{pmatrix} 400 & 0 \\\\ 0 & 625 \\end{pmatrix} + \\begin{pmatrix} 16 & 24 \\\\ 24 & 36 \\end{pmatrix} = \\begin{pmatrix} 416 & 24 \\\\ 24 & 661 \\end{pmatrix}$$\nTo compute the Fisher Information $I_{\\text{full corr}} = \\mathbf{s}^{\\top} \\mathbf{V}_{\\text{full corr}}^{-1} \\mathbf{s}$, we first find the inverse of $\\mathbf{V}_{\\text{full corr}}$. The determinant is:\n$$\\det(\\mathbf{V}_{\\text{full corr}}) = (416)(661) - (24)^2 = 274976 - 576 = 274400$$\nThe inverse is:\n$$\\mathbf{V}_{\\text{full corr}}^{-1} = \\frac{1}{274400} \\begin{pmatrix} 661 & -24 \\\\ -24 & 416 \\end{pmatrix}$$\nNow, we compute the Fisher Information:\n$$I_{\\text{full corr}} = \\frac{1}{274400} \\begin{pmatrix} 80 & 120 \\end{pmatrix} \\begin{pmatrix} 661 & -24 \\\\ -24 & 416 \\end{pmatrix} \\begin{pmatrix} 80 \\\\ 120 \\end{pmatrix}$$\n$$I_{\\text{full corr}} = \\frac{1}{274400} \\left[ 80^2(661) - 2(80)(120)(24) + 120^2(416) \\right]$$\n$$I_{\\text{full corr}} = \\frac{1}{274400} \\left[ (6400)(661) - (19200)(24) + (14400)(416) \\right]$$\n$$I_{\\text{full corr}} = \\frac{1}{274400} \\left[ 4230400 - 460800 + 5990400 \\right] = \\frac{9760000}{274400} = \\frac{12200}{343}$$\nThe variance under the full-correlation hypothesis is:\n$$\\mathrm{Var}_{\\text{full corr}}(\\hat{\\mu}) = I_{\\text{full corr}}^{-1} = \\frac{343}{12200}$$\n\n**Change in Variance**\nThe final step is to compute the change in variance, $\\Delta \\mathrm{Var}(\\hat{\\mu}) = \\mathrm{Var}_{\\text{full corr}}(\\hat{\\mu}) - \\mathrm{Var}_{\\text{indep}}(\\hat{\\mu})$:\n$$\\Delta \\mathrm{Var}(\\hat{\\mu}) = \\frac{343}{12200} - \\frac{8593}{319400}$$\nConverting to numerical values:\n$$\\mathrm{Var}_{\\text{full corr}}(\\hat{\\mu}) \\approx 0.028114754...$$\n$$\\mathrm{Var}_{\\text{indep}}(\\hat{\\mu}) \\approx 0.026903570...$$\n$$\\Delta \\mathrm{Var}(\\hat{\\mu}) \\approx 0.028114754 - 0.026903570 = 0.001211184...$$\nRounding the result to four significant figures gives $0.001211$.", "answer": "$$\\boxed{0.001211}$$", "id": "3509031"}, {"introduction": "What happens when our model of systematic correlations is wrong? This advanced practice ([@problem_id:3509055]) delves into a subtle but dangerous pitfall: underestimating or ignoring existing correlations between nuisance parameters. You will implement a computational analysis to see how this mis-specification can lead to biased results and unreliable uncertainties, and more importantly, you will develop diagnostics to help flag such harmful modeling errors in a real-world scenario.", "problem": "Consider a combination of $C$ independent analysis channels, each producing a single real-valued summary measurement organized into the vector $y \\in \\mathbb{R}^C$. The combination aims to estimate a common signal-strength parameter $\\,\\mu \\in \\mathbb{R}\\,$ subject to a physical constraint $\\,\\mu \\ge 0\\,$. Let $s \\in \\mathbb{R}^C$ denote the per-channel sensitivity vector (so that the signal contribution is $\\,\\mu s\\,$), and let $A \\in \\mathbb{R}^{C \\times K}$ describe the additive response of the channels to $K$ nuisance parameters $\\,\\theta \\in \\mathbb{R}^K$. Assume the following generative model for the observed summary vector:\n$$\ny \\;=\\; \\mu_0\\, s \\;+\\; A\\,\\theta \\;+\\; \\varepsilon,\n$$\nwhere $\\,\\mu_0 \\ge 0\\,$ is the true signal strength, $\\,\\theta \\sim \\mathcal{N}(0,\\,\\Sigma_{\\text{true}})\\,$ is a zero-mean Gaussian vector of nuisance parameters with true covariance $\\,\\Sigma_{\\text{true}} \\in \\mathbb{R}^{K \\times K}\\,$, and $\\,\\varepsilon \\sim \\mathcal{N}(0,\\,V)\\,$ is the statistical noise with diagonal covariance $\\,V = \\mathrm{diag}(v_1,\\dots,v_C)$.\n\nThe combination is performed by minimizing the Gaussian penalized objective\n$$\nQ(\\mu,\\theta \\,|\\, y;\\,\\Sigma_{\\text{assumed}}) \\;=\\; \\left(y \\;-\\; \\mu s \\;-\\; A\\theta\\right)^{\\!\\top} V^{-1} \\left(y \\;-\\; \\mu s \\;-\\; A\\theta\\right) \\;+\\; \\theta^{\\!\\top}\\,\\Sigma_{\\text{assumed}}^{-1}\\,\\theta,\n$$\nwith respect to $\\,\\mu \\ge 0\\,$ and $\\,\\theta \\in \\mathbb{R}^K\\,$, where the penalty uses an assumed nuisance covariance $\\,\\Sigma_{\\text{assumed}}$. This optimization defines the combined estimator $\\,\\hat\\mu \\ge 0\\,$. When $\\,\\mu\\,$ is unconstrained, the optimizer yields a linear estimator $\\,\\tilde\\mu = w^{\\!\\top} y\\,$ for some weight vector $\\,w \\in \\mathbb{R}^C\\,$ that depends on $\\,s, A, V\\,$ and $\\,\\Sigma_{\\text{assumed}}\\,$. With the physical constraint $\\,\\mu \\ge 0\\,$ enforced, one obtains the truncated estimator $\\,\\hat\\mu = \\max(0, \\tilde\\mu)\\,$.\n\nIn this problem, you will quantify the bias in $\\,\\hat\\mu\\,$ when the assumed nuisance correlation matrix sets all pairwise correlations to zero, i.e. the assumed covariance $\\,\\Sigma_{\\text{assumed}}\\,$ is diagonal, while the true covariance $\\,\\Sigma_{\\text{true}}\\,$ has nonzero off-diagonal terms. The bias is defined as\n$$\n\\mathrm{Bias}(\\hat\\mu) \\;=\\; \\mathbb{E}[\\hat\\mu] \\;-\\; \\mu_0,\n$$\nwhere the expectation is taken over the true generative model for $\\,y\\,$.\n\nStarting from first principles of Gaussian penalized estimation and basic properties of the truncated normal distribution, derive the weight vector $\\,w\\,$ for the unconstrained linear estimator $\\,\\tilde\\mu\\,$, and obtain the variance $\\,\\sigma^2 = \\mathrm{Var}(\\tilde\\mu)\\,$ under the true generative model. Use these derivations to express $\\,\\mathbb{E}[\\hat\\mu]\\,$ in closed form for the truncation at $\\,0\\,$. Based on these results, define and compute diagnostics that flag harmful mis-specification when $\\,\\Sigma_{\\text{assumed}}\\,$ ignores correlations:\n- Diagnostic 1: The negative-tail probability $\\,p_{\\text{neg}} = \\mathbb{P}(\\tilde\\mu < 0)\\,$ under the true generative variance $\\,\\sigma^2\\,$.\n- Diagnostic 2: The ratio $\\,r_{\\text{sd}} = \\sigma_{\\text{true}} / \\sigma_{\\text{assumed-pred}}\\,$, where $\\,\\sigma_{\\text{true}}^2 = w^{\\!\\top}(A\\Sigma_{\\text{true}}A^{\\!\\top} + V)w\\,$ is the true variance of $\\,\\tilde\\mu\\,$, and $\\,\\sigma_{\\text{assumed-pred}}^2 = w^{\\!\\top}(A\\Sigma_{\\text{assumed}}A^{\\!\\top} + V)w\\,$ is the variance predicted by the mis-specified assumed covariance using the same weights $\\,w\\,$ constructed from $\\,\\Sigma_{\\text{assumed}}\\,$. Values $\\,r_{\\text{sd}} > 1\\,$ indicate underestimation of uncertainty when correlations are ignored.\n\nImplement a complete program that, for each test case below, computes:\n1. The bias $\\,\\mathrm{Bias}(\\hat\\mu)\\,$.\n2. The diagnostic $\\,p_{\\text{neg}}\\,$.\n3. The diagnostic $\\,r_{\\text{sd}}\\,$.\n\nYour program must produce a single line of output containing the results for all test cases as a comma-separated list of lists of floats in the exact format $\\,[[\\dots],[\\dots],[\\dots]]\\,$.\n\nUse the following test suite, covering a happy-path case, a boundary case, and a strong-correlation edge case. For each case, treat $\\,\\Sigma_{\\text{assumed}}\\,$ as diagonal with the same marginal variances as $\\,\\Sigma_{\\text{true}}\\,$ but with all off-diagonal elements set to $\\,0\\,$.\n\n- Test Case 1 (boundary at $\\,\\mu_0 = 0\\,$):\n  - $C = 3$, $K = 2$.\n  - $s = [10.0,\\,8.0,\\,6.0]$.\n  - $V = \\mathrm{diag}([25.0,\\,16.0,\\,9.0])$.\n  - $A = \\begin{bmatrix}\n      1.0 & 0.5 \\\\\n      0.6 & 0.3 \\\\\n      0.2 & 0.9\n    \\end{bmatrix}$.\n  - $\\,\\Sigma_{\\text{true}} = \\begin{bmatrix}\n      1.0 & 0.669 \\\\\n      0.669 & 0.7\n    \\end{bmatrix}$, $\\,\\Sigma_{\\text{assumed}} = \\mathrm{diag}([1.0,\\,0.7])$.\n  - $\\,\\mu_0 = 0.0$.\n\n- Test Case 2 (no mis-specification in correlations):\n  - $C = 3$, $K = 2$.\n  - $s = [15.0,\\,5.0,\\,7.0]$.\n  - $V = \\mathrm{diag}([9.0,\\,25.0,\\,16.0])$.\n  - $A = \\begin{bmatrix}\n      0.4 & 0.1 \\\\\n      0.2 & 0.5 \\\\\n      0.3 & 0.3\n    \\end{bmatrix}$.\n  - $\\,\\Sigma_{\\text{true}} = \\mathrm{diag}([0.5,\\,0.5])$, $\\,\\Sigma_{\\text{assumed}} = \\mathrm{diag}([0.5,\\,0.5])$.\n  - $\\,\\mu_0 = 0.2$.\n\n- Test Case 3 (strong true correlations ignored in the combination):\n  - $C = 4$, $K = 2$.\n  - $s = [8.0,\\,6.0,\\,4.0,\\,2.0]$.\n  - $V = \\mathrm{diag}([16.0,\\,9.0,\\,4.0,\\,1.0])$.\n  - $A = \\begin{bmatrix}\n      0.9 & 0.2 \\\\\n      0.5 & 0.7 \\\\\n      0.1 & 0.4 \\\\\n      0.3 & 0.3\n    \\end{bmatrix}$.\n  - $\\,\\Sigma_{\\text{true}} = \\begin{bmatrix}\n      2.0 & 1.645 \\\\\n      1.645 & 1.5\n    \\end{bmatrix}$, $\\,\\Sigma_{\\text{assumed}} = \\mathrm{diag}([2.0,\\,1.5])$.\n  - $\\,\\mu_0 = 0.1$.\n\nYour program must:\n- Derive the unconstrained weight vector $\\,w\\,$ from the Gaussian penalized objective using $\\,\\Sigma_{\\text{assumed}}\\,$.\n- Compute $\\,\\sigma_{\\text{true}}^2 = w^{\\!\\top}(A\\Sigma_{\\text{true}}A^{\\!\\top} + V)w\\,$ and $\\,\\sigma_{\\text{assumed-pred}}^2 = w^{\\!\\top}(A\\Sigma_{\\text{assumed}}A^{\\!\\top} + V)w\\,$.\n- Use the closed-form expectation for a truncated normal to compute $\\,\\mathbb{E}[\\hat\\mu]$ and thus the bias $\\,\\mathrm{Bias}(\\hat\\mu)\\,$.\n- Report $\\,p_{\\text{neg}} = \\Phi\\!\\left(-\\mu_0/\\sigma_{\\text{true}}\\right)\\,$ and $\\,r_{\\text{sd}} = \\sigma_{\\text{true}}/\\sigma_{\\text{assumed-pred}}\\,$.\n\nFinal output format: a single line\n$$\n[[\\mathrm{Bias}_1,\\,p_{\\text{neg},1},\\,r_{\\text{sd},1}],\\,[\\mathrm{Bias}_2,\\,p_{\\text{neg},2},\\,r_{\\text{sd},2}],\\,[\\mathrm{Bias}_3,\\,p_{\\text{neg},3},\\,r_{\\text{sd},3}]]\n$$\nwith floats for each test case, in the order Test Case 1, Test Case 2, Test Case 3. No other output is permitted.", "solution": "The problem requires the calculation of bias and two diagnostic metrics for a truncated estimator of a signal strength parameter $\\mu$, derived from a combination of measurements from multiple channels. The core of the problem lies in the mis-specification of the covariance matrix of nuisance parameters, $\\Sigma_{\\text{assumed}}$, which is taken to be diagonal, while the true covariance, $\\Sigma_{\\text{true}}$, contains non-zero off-diagonal correlation terms.\n\nThe solution proceeds in four steps:\n1.  Derive the weight vector $w$ for the unconstrained linear estimator $\\tilde\\mu = w^{\\!\\top} y$.\n2.  Determine the statistical properties (mean and variance) of $\\tilde\\mu$ under the true generative model.\n3.  Derive the expectation of the truncated estimator $\\hat\\mu = \\max(0, \\tilde\\mu)$.\n4.  Use these results to compute the bias $\\mathrm{Bias}(\\hat\\mu)$, the negative-tail probability $p_{\\text{neg}}$, and the standard deviation ratio $r_{\\text{sd}}$.\n\n**1. Derivation of the Unconstrained Estimator's Weights**\n\nThe unconstrained estimator $(\\tilde\\mu, \\tilde\\theta)$ is found by minimizing the penalized objective function $Q$ with respect to $\\mu$ and $\\theta$:\n$$\nQ(\\mu,\\theta) = \\left(y - \\mu s - A\\theta\\right)^{\\!\\top} V^{-1} \\left(y - \\mu s - A\\theta\\right) + \\theta^{\\!\\top}\\,\\Sigma_{\\text{assumed}}^{-1}\\,\\theta\n$$\nWe find the minimum by setting the partial derivatives with respect to $\\mu$ and the vector $\\theta$ to zero.\n\nThe partial derivative with respect to $\\mu$ is:\n$$\n\\frac{\\partial Q}{\\partial \\mu} = -2 s^{\\!\\top} V^{-1} (y - \\mu s - A\\theta) = 0\n$$\n$$\n\\implies s^{\\!\\top} V^{-1} y - \\mu (s^{\\!\\top} V^{-1} s) - s^{\\!\\top} V^{-1} A\\theta = 0 \\quad (1)\n$$\nThe partial derivative with respect to the vector $\\theta$ is:\n$$\n\\frac{\\partial Q}{\\partial \\theta} = -2 A^{\\!\\top} V^{-1} (y - \\mu s - A\\theta) + 2 \\Sigma_{\\text{assumed}}^{-1} \\theta = 0\n$$\n$$\n\\implies -A^{\\!\\top} V^{-1} y + \\mu (A^{\\!\\top} V^{-1} s) + (A^{\\!\\top} V^{-1} A + \\Sigma_{\\text{assumed}}^{-1}) \\theta = 0 \\quad (2)\n$$\nLet $H_{\\theta\\theta} = A^{\\!\\top} V^{-1} A + \\Sigma_{\\text{assumed}}^{-1}$. From equation $(2)$, we can express $\\tilde\\theta$ in terms of $\\tilde\\mu$ and $y$:\n$$\n\\tilde\\theta = H_{\\theta\\theta}^{-1} (A^{\\!\\top} V^{-1} y - \\tilde\\mu A^{\\!\\top} V^{-1} s) = H_{\\theta\\theta}^{-1} A^{\\!\\top} V^{-1} (y - \\tilde\\mu s)\n$$\nThis is the \"profiled\" solution for the nuisance parameters. Substituting this expression for $\\tilde\\theta$ back into equation $(1)$:\n$$\ns^{\\!\\top} V^{-1} y - \\tilde\\mu (s^{\\!\\top} V^{-1} s) - s^{\\!\\top} V^{-1} A \\left[ H_{\\theta\\theta}^{-1} A^{\\!\\top} V^{-1} (y - \\tilde\\mu s) \\right] = 0\n$$\nWe group terms multiplying $\\tilde\\mu$ and terms multiplying $y$:\n$$\n\\left( s^{\\!\\top} V^{-1} - s^{\\!\\top} V^{-1} A H_{\\theta\\theta}^{-1} A^{\\!\\top} V^{-1} \\right) y = \\tilde\\mu \\left( s^{\\!\\top} V^{-1} s - s^{\\!\\top} V^{-1} A H_{\\theta\\theta}^{-1} A^{\\!\\top} V^{-1} s \\right)\n$$\nSolving for $\\tilde\\mu$ gives the linear estimator $\\tilde\\mu = w^{\\!\\top} y$, where the weight vector $w$ is defined by its transpose:\n$$\nw^{\\!\\top} = \\frac{ s^{\\!\\top} V^{-1} - s^{\\!\\top} V^{-1} A H_{\\theta\\theta}^{-1} A^{\\!\\top} V^{-1} }{ s^{\\!\\top} V^{-1} s - s^{\\!\\top} V^{-1} A H_{\\theta\\theta}^{-1} A^{\\!\\top} V^{-1} s }\n$$\nThe denominator is a scalar normalization factor.\n\n**2. Statistical Properties of the Unconstrained Estimator**\n\nThe true generative model for the observation $y$ is $y = \\mu_0 s + A\\theta + \\varepsilon$, where $\\theta \\sim \\mathcal{N}(0, \\Sigma_{\\text{true}})$ and $\\varepsilon \\sim \\mathcal{N}(0, V)$. The expectation of $y$ is $\\mathbb{E}[y] = \\mu_0 s$.\n\nThe expectation of the unconstrained estimator $\\tilde\\mu = w^{\\!\\top} y$ is:\n$$\n\\mathbb{E}[\\tilde\\mu] = \\mathbb{E}[w^{\\!\\top} y] = w^{\\!\\top} \\mathbb{E}[y] = w^{\\!\\top} (\\mu_0 s) = \\mu_0 (w^{\\!\\top} s)\n$$\nFrom the expression for $w^{\\!\\top}$, we can see that the numerator evaluated at $s$ is identical to the denominator, thus $w^{\\!\\top} s = 1$. This implies:\n$$\n\\mathbb{E}[\\tilde\\mu] = \\mu_0\n$$\nThe unconstrained estimator $\\tilde\\mu$ is unbiased for the true signal strength $\\mu_0$, even when $\\Sigma_{\\text{assumed}} \\neq \\Sigma_{\\text{true}}$.\n\nThe variance of $\\tilde\\mu$ is found by considering the variance of $y$. Since $\\theta$ and $\\varepsilon$ are independent, the true covariance of $y$ is:\n$$\n\\text{Cov}(y) = \\text{Cov}(A\\theta + \\varepsilon) = A\\,\\text{Cov}(\\theta)A^{\\!\\top} + \\text{Cov}(\\varepsilon) = A\\Sigma_{\\text{true}}A^{\\!\\top} + V\n$$\nThe true variance of $\\tilde\\mu = w^{\\!\\top} y$ is therefore:\n$$\n\\sigma_{\\text{true}}^2 = \\text{Var}(\\tilde\\mu) = w^{\\!\\top} \\text{Cov}(y) w = w^{\\!\\top} (A\\Sigma_{\\text{true}}A^{\\!\\top} + V) w\n$$\nThis matches the definition provided in the problem. The estimator $\\tilde\\mu$ is a linear combination of Gaussian random variables, so its distribution is Gaussian: $\\tilde\\mu \\sim \\mathcal{N}(\\mu_0, \\sigma_{\\text{true}}^2)$.\n\nThe variance predicted by the mis-specified model is calculated by substituting $\\Sigma_{\\text{assumed}}$ for $\\Sigma_{\\text{true}}$:\n$$\n\\sigma_{\\text{assumed-pred}}^2 = w^{\\!\\top} (A\\Sigma_{\\text{assumed}}A^{\\!\\top} + V) w\n$$\n\n**3. Expectation of the Truncated Estimator**\n\nThe final estimator is $\\hat\\mu = \\max(0, \\tilde\\mu)$. Its expectation is computed by integrating over the distribution of $\\tilde\\mu$:\n$$\n\\mathbb{E}[\\hat\\mu] = \\int_{-\\infty}^{\\infty} \\max(0, x) \\, f_{\\tilde\\mu}(x) \\, dx = \\int_{0}^{\\infty} x \\, f_{\\tilde\\mu}(x) \\, dx\n$$\nwhere $f_{\\tilde\\mu}(x)$ is the probability density function of a $\\mathcal{N}(\\mu_0, \\sigma_{\\text{true}}^2)$ distribution. The result of this integral for a generic normal variable $X \\sim \\mathcal{N}(m, s^2)$ is a standard result for the mean of a truncated normal distribution:\n$$\n\\mathbb{E}[\\max(0, X)] = m\\,\\Phi\\left(\\frac{m}{s}\\right) + s\\,\\phi\\left(\\frac{m}{s}\\right)\n$$\nwhere $\\Phi$ is the standard normal cumulative distribution function (CDF) and $\\phi$ is the standard normal probability density function (PDF). Applying this to $\\tilde\\mu$, with mean $m = \\mu_0$ and standard deviation $s = \\sigma_{\\text{true}}$, we get:\n$$\n\\mathbb{E}[\\hat\\mu] = \\mu_0\\,\\Phi\\left(\\frac{\\mu_0}{\\sigma_{\\text{true}}}\\right) + \\sigma_{\\text{true}}\\,\\phi\\left(\\frac{\\mu_0}{\\sigma_{\\text{true}}}\\right)\n$$\n\n**4. Calculation of Bias and Diagnostics**\n\nHaving derived the necessary expressions, we can now compute the required quantities.\n\n- **Bias**: The bias of the estimator $\\hat\\mu$ is:\n$$\n\\mathrm{Bias}(\\hat\\mu) = \\mathbb{E}[\\hat\\mu] - \\mu_0 = \\left[ \\mu_0\\,\\Phi\\left(\\frac{\\mu_0}{\\sigma_{\\text{true}}}\\right) + \\sigma_{\\text{true}}\\,\\phi\\left(\\frac{\\mu_0}{\\sigma_{\\text{true}}}\\right) \\right] - \\mu_0\n$$\n\n- **Diagnostic 1, $p_{\\text{neg}}$**: This is the probability that the unconstrained estimator is negative. Since $\\tilde\\mu \\sim \\mathcal{N}(\\mu_0, \\sigma_{\\text{true}}^2)$, we have:\n$$\np_{\\text{neg}} = \\mathbb{P}(\\tilde\\mu < 0) = \\mathbb{P}\\left(\\frac{\\tilde\\mu - \\mu_0}{\\sigma_{\\text{true}}} < \\frac{-\\mu_0}{\\sigma_{\\text{true}}}\\right) = \\Phi\\left(-\\frac{\\mu_0}{\\sigma_{\\text{true}}}\\right)\n$$\n\n- **Diagnostic 2, $r_{\\text{sd}}$**: This is the ratio of the true standard deviation to the predicted standard deviation:\n$$\nr_{\\text{sd}} = \\frac{\\sigma_{\\text{true}}}{\\sigma_{\\text{assumed-pred}}} = \\frac{\\sqrt{w^{\\!\\top} (A\\Sigma_{\\text{true}}A^{\\!\\top} + V) w}}{\\sqrt{w^{\\!\\top} (A\\Sigma_{\\text{assumed}}A^{\\!\\top} + V) w}}\n$$\n\nThese formulas provide a complete algorithm to solve the problem for each test case by substituting the given numerical values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes bias and diagnostics for a truncated signal strength estimator\n    under nuisance parameter correlation mis-specification.\n    \"\"\"\n    test_cases = [\n        # Test Case 1: boundary at mu_0 = 0\n        {\n            \"s\": np.array([10.0, 8.0, 6.0]),\n            \"V\": np.diag([25.0, 16.0, 9.0]),\n            \"A\": np.array([\n                [1.0, 0.5],\n                [0.6, 0.3],\n                [0.2, 0.9]\n            ]),\n            \"Sigma_true\": np.array([\n                [1.0, 0.669],\n                [0.669, 0.7]\n            ]),\n            \"mu_0\": 0.0\n        },\n        # Test Case 2: no mis-specification\n        {\n            \"s\": np.array([15.0, 5.0, 7.0]),\n            \"V\": np.diag([9.0, 25.0, 16.0]),\n            \"A\": np.array([\n                [0.4, 0.1],\n                [0.2, 0.5],\n                [0.3, 0.3]\n            ]),\n            \"Sigma_true\": np.diag([0.5, 0.5]),\n            \"mu_0\": 0.2\n        },\n        # Test Case 3: strong true correlations ignored\n        {\n            \"s\": np.array([8.0, 6.0, 4.0, 2.0]),\n            \"V\": np.diag([16.0, 9.0, 4.0, 1.0]),\n            \"A\": np.array([\n                [0.9, 0.2],\n                [0.5, 0.7],\n                [0.1, 0.4],\n                [0.3, 0.3]\n            ]),\n            \"Sigma_true\": np.array([\n                [2.0, 1.645],\n                [1.645, 1.5]\n            ]),\n            \"mu_0\": 0.1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        s = case[\"s\"].reshape(-1, 1)\n        V = case[\"V\"]\n        A = case[\"A\"]\n        Sigma_true = case[\"Sigma_true\"]\n        mu_0 = case[\"mu_0\"]\n\n        # As per the problem, Sigma_assumed is diagonal with variances from Sigma_true\n        Sigma_assumed = np.diag(np.diag(Sigma_true))\n\n        # --- 1. Derive the weight vector w ---\n        V_inv = np.linalg.inv(V)\n        Sigma_assumed_inv = np.linalg.inv(Sigma_assumed)\n\n        H_tt = A.T @ V_inv @ A + Sigma_assumed_inv\n        H_tt_inv = np.linalg.inv(H_tt)\n\n        # Numerator and denominator for w transpose\n        # w^T = w_T_num / w_T_den\n        w_T_num_term1 = s.T @ V_inv\n        w_T_num_term2 = s.T @ V_inv @ A @ H_tt_inv @ A.T @ V_inv\n        w_T_num = w_T_num_term1 - w_T_num_term2\n        \n        # The denominator is a scalar\n        w_T_den = (s.T @ V_inv @ s - s.T @ V_inv @ A @ H_tt_inv @ A.T @ V_inv @ s)[0, 0]\n        \n        w_T = w_T_num / w_T_den\n        w = w_T.T\n\n        # --- 2. Calculate true and assumed-predicted variances ---\n        # Cov(y) = A * Sigma * A.T + V\n        true_cov_y = A @ Sigma_true @ A.T + V\n        sigma_true_sq = (w.T @ true_cov_y @ w)[0, 0]\n        sigma_true = np.sqrt(sigma_true_sq)\n\n        assumed_pred_cov_y = A @ Sigma_assumed @ A.T + V\n        sigma_assumed_pred_sq = (w.T @ assumed_pred_cov_y @ w)[0, 0]\n        sigma_assumed_pred = np.sqrt(sigma_assumed_pred_sq)\n        \n        # --- 3. Compute diagnostics ---\n        # Diagnostic 2: r_sd\n        r_sd = sigma_true / sigma_assumed_pred\n\n        # Diagnostic 1: p_neg\n        z_neg = -mu_0 / sigma_true\n        p_neg = norm.cdf(z_neg)\n\n        # --- 4. Compute bias ---\n        # E[hat_mu] = mu_0 * Phi(mu_0/sigma) + sigma * phi(mu_0/sigma)\n        z_pos = mu_0 / sigma_true\n        E_hat_mu = mu_0 * norm.cdf(z_pos) + sigma_true * norm.pdf(z_pos)\n        bias = E_hat_mu - mu_0\n        \n        results.append([bias, p_neg, r_sd])\n\n    # Format output as a list of lists of floats\n    output_str = \"[\" + ','.join([f\"[{b},{p},{r}]\" for b, p, r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3509055"}]}