{"hands_on_practices": [{"introduction": "Representing jets as images on the pseudorapidity-azimuth plane allows us to leverage powerful Convolutional Neural Networks (CNNs). This first practice provides a concrete calculation of a single convolutional layer, connecting the abstract operation of a kernel cross-correlation to the extraction of a physically meaningful feature. By completing this exercise, you will gain a tangible understanding of how a CNN's receptive field relates to angular scales in jet substructure analysis. [@problem_id:3510623]", "problem": "You are analyzing a jet substructure classifier built with a Convolutional Neural Network (CNN) on a discretized pseudorapidity–azimuth plane used in high-energy physics event analysis. The jet is centered at pseudorapidity $\\eta \\approx 0$, and you use a local $3 \\times 3$ patch of a jet image with unit-normalized, dimensionless intensities:\n$$\nI \\;=\\; \\begin{pmatrix}\n0.12 & 0.08 & 0.05\\\\\n0.20 & 0.15 & 0.10\\\\\n0.18 & 0.12 & 0.06\n\\end{pmatrix}.\n$$\nA learned $2 \\times 2$ kernel, which approximates an oriented contrast operator, is given by\n$$\nK \\;=\\; \\begin{pmatrix}\n0.5 & -0.25\\\\\n-0.5 & 0.25\n\\end{pmatrix}.\n$$\nIn the forward pass of the CNN, adopt the standard deep learning convention that the layer performs discrete cross-correlation (no kernel flipping), with stride $s=1$ and no padding. The output is the valid $2 \\times 2$ activation map $Y$ with entries\n$$\nY_{i,j} \\;=\\; \\sum_{u=0}^{1}\\sum_{v=0}^{1} K_{u,v}\\, I_{i+u,\\,j+v}, \\quad \\text{for } i,j \\in \\{1,2\\}.\n$$\nAssume a square pixelization of the $\\eta$–$\\phi$ plane with $\\Delta \\eta = 0.1$ and $\\Delta \\phi = 0.1$, where $\\phi$ is measured in radians. In the standard jet analysis metric, the angular distance is defined by $\\Delta R = \\sqrt{(\\Delta \\eta)^{2} + (\\Delta \\phi)^{2}}$. For a single activation of $Y$, the receptive field is the corresponding $2 \\times 2$ input patch; define its angular span $\\Delta R_{\\text{span}}$ as the Euclidean length of the diagonal of this receptive field in the $\\eta$–$\\phi$ plane.\n\nCompute the following in order:\n- The full $2 \\times 2$ activation map $Y$.\n- The scalar $S$ equal to the sum of all entries of $Y$.\n- The angular span $\\Delta R_{\\text{span}}$ of a single receptive field.\n- The final scalar $P \\equiv S \\times \\Delta R_{\\text{span}}$.\n\nExpress your final scalar $P$ as a pure number (dimensionless) rounded to four significant figures. All intermediate quantities may be left exact until the final rounding instruction is applied. Angles are in radians.", "solution": "The problem requires a sequence of four calculations: the activation map $Y$, its sum $S$, the angular span of the receptive field $\\Delta R_{\\text{span}}$, and the final product $P$.\n\nFirst, we compute the $2 \\times 2$ activation map $Y$ using the provided discrete cross-correlation formula. With the input intensity matrix $I$ and kernel $K$, and assuming $1$-based indexing for $I$ and $0$-based for $K$ as implied by the formula, we calculate the elements of $Y$:\n\n$Y_{1,1} = \\sum_{u=0}^{1}\\sum_{v=0}^{1} K_{u,v}\\, I_{1+u,\\,1+v} = (0.5)(0.12) + (-0.25)(0.08) + (-0.5)(0.20) + (0.25)(0.15) = 0.06 - 0.02 - 0.10 + 0.0375 = -0.0225$\n\n$Y_{1,2} = \\sum_{u=0}^{1}\\sum_{v=0}^{1} K_{u,v}\\, I_{1+u,\\,2+v} = (0.5)(0.08) + (-0.25)(0.05) + (-0.5)(0.15) + (0.25)(0.10) = 0.04 - 0.0125 - 0.075 + 0.025 = -0.0225$\n\n$Y_{2,1} = \\sum_{u=0}^{1}\\sum_{v=0}^{1} K_{u,v}\\, I_{2+u,\\,1+v} = (0.5)(0.20) + (-0.25)(0.15) + (-0.5)(0.18) + (0.25)(0.12) = 0.10 - 0.0375 - 0.09 + 0.03 = 0.0025$\n\n$Y_{2,2} = \\sum_{u=0}^{1}\\sum_{v=0}^{1} K_{u,v}\\, I_{2+u,\\,2+v} = (0.5)(0.15) + (-0.25)(0.10) + (-0.5)(0.12) + (0.25)(0.06) = 0.075 - 0.025 - 0.06 + 0.015 = 0.0050$\n\nThe full activation map is:\n$$\nY = \\begin{pmatrix}\n-0.0225 & -0.0225 \\\\\n0.0025 & 0.0050\n\\end{pmatrix}\n$$\nSecond, we compute the scalar $S$, which is the sum of all entries of $Y$.\n$$\nS = (-0.0225) + (-0.0225) + 0.0025 + 0.0050 = -0.0375\n$$\nThird, we compute the angular span $\\Delta R_{\\text{span}}$. The receptive field is a $2 \\times 2$ patch of pixels. Each pixel has dimensions $\\Delta\\eta = 0.1$ and $\\Delta\\phi = 0.1$. The total region covered is a rectangle in the $\\eta$-$\\phi$ plane with side lengths $L_{\\eta} = 2 \\times \\Delta\\eta = 0.2$ and $L_{\\phi} = 2 \\times \\Delta\\phi = 0.2$. The angular span $\\Delta R_{\\text{span}}$ is the length of this rectangle's diagonal.\n$$\n\\Delta R_{\\text{span}} = \\sqrt{L_{\\eta}^2 + L_{\\phi}^2} = \\sqrt{(0.2)^2 + (0.2)^2} = \\sqrt{0.08} = 0.2\\sqrt{2}\n$$\nFourth, we compute the final scalar $P = S \\times \\Delta R_{\\text{span}}$.\n$$\nP = (-0.0375) \\times (0.2\\sqrt{2}) = -0.0075\\sqrt{2}\n$$\nFinally, we provide the numerical value of $P$ rounded to four significant figures.\n$$\nP \\approx -0.0075 \\times 1.41421356 \\approx -0.0106066\n$$\nRounding to four significant figures gives $-0.01061$.", "answer": "$$\n\\boxed{-0.01061}\n$$", "id": "3510623"}, {"introduction": "While jet images are useful, a more fundamental description of a particle physics event is an unordered set of particles. Graph Neural Networks (GNNs) are perfectly suited for such data, representing particles as nodes and their relationships as edges. This exercise walks you through a single message-passing step, the core operation of a GNN, demonstrating how information is propagated and aggregated between particles in a way that respects physical symmetries. [@problem_id:3510657]", "problem": "A graph neural network for event-level analysis in computational high-energy physics operates on a directed complete graph of four reconstructed particles (nodes), each with kinematic features $x_i=(p_{T,i},\\eta_i,\\phi_i)$, where $p_{T,i}$ is transverse momentum, $\\eta_i$ is pseudorapidity, and $\\phi_i$ is azimuthal angle. The initial scalar hidden state for node $i$ is defined by a linear embedding $h_i^{(0)}=w_p \\ln p_{T,i} + w_{\\eta}\\,\\eta_i$, which respects azimuthal rotational invariance by using only $\\ln p_{T,i}$ and $\\eta_i$. For each directed edge $(i \\leftarrow j)$, define the edge feature $e_{ij}=(\\Delta \\eta_{ij}, \\Delta \\phi_{ij}, \\ln p_{T,j})$, with $\\Delta \\eta_{ij}=\\eta_j-\\eta_i$ and $\\Delta \\phi_{ij}$ given by the wrapped difference on the interval $[-\\pi,\\pi]$ so that it is the minimal signed separation in azimuth. The message from node $j$ to node $i$ is computed as\n$$\nm_{ij}=\\tanh\\!\\left(a\\,h_j^{(0)}+b_1\\,\\Delta \\eta_{ij}+b_2\\,\\cos(\\Delta \\phi_{ij})+b_3\\,\\ln p_{T,j}+c\\right),\n$$\nand the node update uses sum aggregation,\n$$\na_i=\\sum_{j\\neq i} m_{ij},\\quad h_i^{(1)}=h_i^{(0)}+\\alpha\\,a_i.\n$$\nYou are given the following constants and node features:\n- Embedding weights: $w_p=0.6$, $w_{\\eta}=0.3$.\n- Message weights: $a=0.8$, $b_1=0.15$, $b_2=0.2$, $b_3=0.05$, $c=-0.02$.\n- Update coefficient: $\\alpha=0.7$.\n- Node 1: $(p_{T,1}, \\eta_1, \\phi_1)=(50, 0.2, 0.10)$.\n- Node 2: $(p_{T,2}, \\eta_2, \\phi_2)=(40, -0.3, 3.05)$.\n- Node 3: $(p_{T,3}, \\eta_3, \\phi_3)=(25, 1.1, -3.00)$.\n- Node 4: $(p_{T,4}, \\eta_4, \\phi_4)=(10, -2.0, 2.90)$.\n\nAll angles are in radians, and $\\ln$ denotes the natural logarithm. Using the above definitions, compute the updated hidden state $h_2^{(1)}$ for node $2$. Express your final result as a real number and round your answer to four significant figures. No physical units are required for the final answer.", "solution": "To compute the updated hidden state $h_2^{(1)}$, we follow the definitions provided. The calculation proceeds in three main steps: 1. Compute the initial hidden states $h_j^{(0)}$ for all nodes. 2. Compute the incoming messages $m_{2j}$ to node 2. 3. Aggregate the messages and compute the final updated state $h_2^{(1)}$.\n\n**1. Calculation of Initial Hidden States $h_j^{(0)}$**\n\nThe initial hidden state for a node $j$ is $h_j^{(0)} = w_p \\ln p_{T,j} + w_{\\eta} \\eta_j$, with $w_p=0.6$ and $w_{\\eta}=0.3$.\n- $\\ln p_{T,1} = \\ln(50) \\approx 3.91202$\n- $\\ln p_{T,2} = \\ln(40) \\approx 3.68888$\n- $\\ln p_{T,3} = \\ln(25) \\approx 3.21888$\n- $\\ln p_{T,4} = \\ln(10) \\approx 2.30259$\n\nUsing these, we compute the initial hidden states:\n- $h_1^{(0)} = (0.6)(3.91202) + (0.3)(0.2) = 2.34721 + 0.06 = 2.40721$\n- $h_2^{(0)} = (0.6)(3.68888) + (0.3)(-0.3) = 2.21333 - 0.09 = 2.12333$\n- $h_3^{(0)} = (0.6)(3.21888) + (0.3)(1.1) = 1.93133 + 0.33 = 2.26133$\n- $h_4^{(0)} = (0.6)(2.30259) + (0.3)(-2.0) = 1.38155 - 0.6 = 0.78155$\n\n**2. Calculation of Messages $m_{2j}$ to Node 2**\n\nThe message from node $j$ to node $i=2$ is $m_{2j}=\\tanh(Z_{2j})$, where $Z_{2j} = a\\,h_j^{(0)}+b_1\\,\\Delta \\eta_{2j}+b_2\\,\\cos(\\Delta \\phi_{2j})+b_3\\,\\ln p_{T,j}+c$. We use the weights $a=0.8$, $b_1=0.15$, $b_2=0.2$, $b_3=0.05$, $c=-0.02$.\n\n**Message $m_{21}$ (from node $j=1$):**\n- $\\Delta \\eta_{21} = \\eta_1 - \\eta_2 = 0.2 - (-0.3) = 0.5$\n- $\\Delta \\phi_{21} = \\phi_1 - \\phi_2 = 0.10 - 3.05 = -2.95$\n- $\\cos(\\Delta \\phi_{21}) = \\cos(-2.95) \\approx -0.98506$\n$Z_{21} \\approx (0.8)(2.40721) + (0.15)(0.5) + (0.2)(-0.98506) + (0.05)(3.91202) - 0.02 \\approx 1.92577 + 0.075 - 0.19701 + 0.19560 - 0.02 = 1.97936$\n$m_{21} = \\tanh(1.97936) \\approx 0.96255$\n\n**Message $m_{23}$ (from node $j=3$):**\n- $\\Delta \\eta_{23} = \\eta_3 - \\eta_2 = 1.1 - (-0.3) = 1.4$\n- $\\Delta \\phi_{23} = \\phi_3 - \\phi_2 = -3.00 - 3.05 = -6.05$. Wrapped to $[-\\pi, \\pi]$: $-6.05 + 2\\pi \\approx 0.23318$.\n- $\\cos(\\Delta \\phi_{23}) = \\cos(0.23318) \\approx 0.97290$\n$Z_{23} \\approx (0.8)(2.26133) + (0.15)(1.4) + (0.2)(0.97290) + (0.05)(3.21888) - 0.02 \\approx 1.80906 + 0.21 + 0.19458 + 0.16094 - 0.02 = 2.35458$\n$m_{23} = \\tanh(2.35458) \\approx 0.98207$\n\n**Message $m_{24}$ (from node $j=4$):**\n- $\\Delta \\eta_{24} = \\eta_4 - \\eta_2 = -2.0 - (-0.3) = -1.7$\n- $\\Delta \\phi_{24} = \\phi_4 - \\phi_2 = 2.90 - 3.05 = -0.15$\n- $\\cos(\\Delta \\phi_{24}) = \\cos(-0.15) \\approx 0.98877$\n$Z_{24} \\approx (0.8)(0.78155) + (0.15)(-1.7) + (0.2)(0.98877) + (0.05)(2.30259) - 0.02 \\approx 0.62524 - 0.255 + 0.19775 + 0.11513 - 0.02 = 0.66312$\n$m_{24} = \\tanh(0.66312) \\approx 0.58055$\n\n**3. Calculation of the Aggregated Message $a_2$ and Updated State $h_2^{(1)}$**\n\nThe aggregated message for node 2 is the sum of the incoming messages:\n$a_2 = m_{21} + m_{23} + m_{24} \\approx 0.96255 + 0.98207 + 0.58055 = 2.52517$\n\nFinally, we update the hidden state for node 2 using $h_2^{(1)} = h_2^{(0)} + \\alpha a_2$ with $\\alpha = 0.7$:\n$h_2^{(1)} \\approx 2.12333 + (0.7)(2.52517) \\approx 2.12333 + 1.76762 = 3.89095$\n\nRounding the result to four significant figures gives $3.891$.", "answer": "$$\n\\boxed{3.891}\n$$", "id": "3510657"}, {"introduction": "To capture complex, long-range correlations within an event, we can turn to the attention mechanism, the key component of Transformer architectures. This practice demystifies scaled dot-product attention by guiding you through a manual calculation for a small set of particles. You will see how attention weights are derived and how a custom mask, based on angular distance, can be used to enforce a physics-based locality bias, blending the flexibility of attention with domain knowledge. [@problem_id:3510640]", "problem": "Consider a simplified event in computational high-energy physics comprising $3$ reconstructed particles with angular coordinates $(\\eta, \\phi)$ given by Particle $1$: $(\\eta_1, \\phi_1) = (0.0, 0.0)$, Particle $2$: $(\\eta_2, \\phi_2) = (0.2, 0.1)$, and Particle $3$: $(\\eta_3, \\phi_3) = (0.8, 0.7)$, where angles are in radians. We study a single-head scaled dot-product attention mechanism applied to these particles, consistent with modern deep learning architectures for event analysis. Use the following foundational definitions:\n\n- The scaled dot-product attention scores for a query index $i$ and key index $j$ are $s_{ij} = \\frac{Q_i \\cdot K_j}{\\sqrt{d_k}} + M_{ij}$, where $Q_i \\in \\mathbb{R}^{d_k}$ is the query of particle $i$, $K_j \\in \\mathbb{R}^{d_k}$ is the key of particle $j$, $d_k$ is the key dimensionality, and $M_{ij}$ is a mask.\n- The attention weights are $a_{ij} = \\frac{\\exp(s_{ij})}{\\sum_{j'=1}^{3} \\exp(s_{ij'})}$.\n- The output for particle $i$ is $O_i = \\sum_{j=1}^{3} a_{ij} V_j$, with $V_j \\in \\mathbb{R}^{d_v}$ the value of particle $j$.\n\nAdopt a locality mask that encodes a nearest-neighbor attention within the event topology: for each query particle $i$, let $M_{ij} = 0$ if $j = i$ or $j$ is the unique nearest neighbor of $i$ in angular distance $\\Delta R_{ij} = \\sqrt{(\\eta_i - \\eta_j)^2 + (\\phi_i - \\phi_j)^2}$; otherwise set $M_{ij} = -\\infty$. Assume ties do not occur for the given coordinates.\n\nLet the single-head parameters be $d_k = 2$, $d_v = 2$, and the query, key, and value matrices (rows correspond to particles $1,2,3$) be\n$$\nQ = \\begin{pmatrix}\n1 & -1 \\\\\n0 & 2 \\\\\n-1 & 0\n\\end{pmatrix}, \\quad\nK = \\begin{pmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\n0 & 0\n\\end{pmatrix}, \\quad\nV = \\begin{pmatrix}\n1 & 2 \\\\\n2 & -1 \\\\\n0 & 3\n\\end{pmatrix}.\n$$\n\nUsing only the above definitions and the provided data, compute the single-head attention outputs $O_1$, $O_2$, and $O_3$ for the three particles under the locality mask. Then, compute the scalar\n$$S = \\|O_1\\|_2^2 + \\|O_2\\|_2^2 + \\|O_3\\|_2^2,$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm on $\\mathbb{R}^{2}$. Provide the final value of $S$ as an exact real number. No rounding is required.", "solution": "The solution involves several steps: first, determining the locality mask based on angular distances; second, calculating the attention scores and weights; third, computing the output vectors; and finally, summing their squared norms.\n\n**1. Determine the Locality Mask**\n\nFirst, we calculate the squared angular distances $\\Delta R_{ij}^2 = (\\eta_i - \\eta_j)^2 + (\\phi_i - \\phi_j)^2$ to find the nearest neighbor for each particle.\n- $\\Delta R_{12}^2 = (0.0 - 0.2)^2 + (0.0 - 0.1)^2 = 0.04 + 0.01 = 0.05$\n- $\\Delta R_{13}^2 = (0.0 - 0.8)^2 + (0.0 - 0.7)^2 = 0.64 + 0.49 = 1.13$\n- $\\Delta R_{23}^2 = (0.2 - 0.8)^2 + (0.1 - 0.7)^2 = (-0.6)^2 + (-0.6)^2 = 0.36 + 0.36 = 0.72$\n\nNext, we identify the nearest neighbor for each particle $i$ by finding the particle $j \\neq i$ with the minimum $\\Delta R_{ij}^2$:\n- For particle 1: $\\Delta R_{12}^2 < \\Delta R_{13}^2$, so the nearest neighbor is particle 2.\n- For particle 2: $\\Delta R_{21}^2 < \\Delta R_{23}^2$, so the nearest neighbor is particle 1.\n- For particle 3: $\\Delta R_{32}^2 < \\Delta R_{31}^2$, so the nearest neighbor is particle 2.\n\nThe mask $M_{ij}$ is $0$ if particle $j$ is the same as particle $i$ or is particle $i$'s nearest neighbor; otherwise, $M_{ij} = -\\infty$. This gives the mask matrix:\n$$\nM = \\begin{pmatrix}\n0 & 0 & -\\infty \\\\\n0 & 0 & -\\infty \\\\\n-\\infty & 0 & 0\n\\end{pmatrix}\n$$\n\n**2. Calculate Attention Scores and Weights**\n\nThe attention scores are $s_{ij} = \\frac{Q_i \\cdot K_j}{\\sqrt{d_k}} + M_{ij}$. Since the key matrix $K$ is the zero matrix, each key vector $K_j$ is $(0,0)$. Therefore, the dot product term $Q_i \\cdot K_j$ is always $0$. This simplifies the scores to be equal to the mask: $s_{ij} = M_{ij}$.\n$$\ns = \\begin{pmatrix}\n0 & 0 & -\\infty \\\\\n0 & 0 & -\\infty \\\\\n-\\infty & 0 & 0\n\\end{pmatrix}\n$$\nThe attention weights $a_{ij}$ are calculated by applying softmax to each row of the score matrix. We use $\\exp(0) = 1$ and $\\exp(-\\infty) = 0$.\n- For row 1: The sum of exponentials is $\\exp(0) + \\exp(0) + \\exp(-\\infty) = 1 + 1 + 0 = 2$. The weights are $a_{11}=\\frac{1}{2}$, $a_{12}=\\frac{1}{2}$, $a_{13}=0$.\n- For row 2: The sum is identical to row 1. The weights are $a_{21}=\\frac{1}{2}$, $a_{22}=\\frac{1}{2}$, $a_{23}=0$.\n- For row 3: The sum is $\\exp(-\\infty) + \\exp(0) + \\exp(0) = 0 + 1 + 1 = 2$. The weights are $a_{31}=0$, $a_{32}=\\frac{1}{2}$, $a_{33}=\\frac{1}{2}$.\n\nThe attention weight matrix is:\n$$\nA = \\begin{pmatrix}\n1/2 & 1/2 & 0 \\\\\n1/2 & 1/2 & 0 \\\\\n0 & 1/2 & 1/2\n\\end{pmatrix}\n$$\n\n**3. Compute Output Vectors**\n\nThe output vectors $O_i$ are the weighted sums of the value vectors $V_j$.\n$V_1 = (1, 2)$, $V_2 = (2, -1)$, $V_3 = (0, 3)$.\n- $O_1 = a_{11}V_1 + a_{12}V_2 + a_{13}V_3 = \\frac{1}{2}(1, 2) + \\frac{1}{2}(2, -1) + 0 = (\\frac{1}{2}, 1) + (1, -\\frac{1}{2}) = (\\frac{3}{2}, \\frac{1}{2})$\n- $O_2 = a_{21}V_1 + a_{22}V_2 + a_{23}V_3 = \\frac{1}{2}(1, 2) + \\frac{1}{2}(2, -1) + 0 = (\\frac{3}{2}, \\frac{1}{2})$\n- $O_3 = a_{31}V_1 + a_{32}V_2 + a_{33}V_3 = 0 + \\frac{1}{2}(2, -1) + \\frac{1}{2}(0, 3) = (1, -\\frac{1}{2}) + (0, \\frac{3}{2}) = (1, 1)$\n\n**4. Calculate Final Scalar**\n\nFinally, we compute $S = \\|O_1\\|_2^2 + \\|O_2\\|_2^2 + \\|O_3\\|_2^2$.\n- $\\|O_1\\|_2^2 = (\\frac{3}{2})^2 + (\\frac{1}{2})^2 = \\frac{9}{4} + \\frac{1}{4} = \\frac{10}{4} = \\frac{5}{2}$\n- $\\|O_2\\|_2^2 = (\\frac{3}{2})^2 + (\\frac{1}{2})^2 = \\frac{5}{2}$\n- $\\|O_3\\|_2^2 = 1^2 + 1^2 = 2$\n\nThe sum is:\n$$\nS = \\frac{5}{2} + \\frac{5}{2} + 2 = 5 + 2 = 7\n$$", "answer": "$$\n\\boxed{7}\n$$", "id": "3510640"}]}