## Introduction
The search for physics beyond the Standard Model is one of the most compelling frontiers in science. While traditional searches excel at targeting specific, theoretically-motivated signatures, they risk overlooking unexpected phenomena that do not conform to our preconceptions. Model-independent [anomaly detection](@entry_id:634040) offers a powerful, complementary approach: instead of looking for a specific signal, we ask the data a more fundamental question: "Is there anything here that we don't understand?" This paradigm shift from targeted hunting to general surveillance requires a sophisticated toolkit of statistical and computational methods to identify the "unknown unknown" with scientific rigor.

This article navigates the principles, applications, and practical implementation of model-independent [anomaly detection](@entry_id:634040) in [high-energy physics](@entry_id:181260). It addresses the core challenge of how to find and validate potential discoveries without a predefined signal model, a task that sits at the intersection of statistics, machine learning, and [experimental physics](@entry_id:264797). Across three comprehensive chapters, you will gain a deep understanding of this exciting field.

The first chapter, "Principles and Mechanisms," establishes the foundational concepts, from the statistical basis of likelihood tests to the advanced techniques for constructing powerful anomaly scores and ensuring their robustness. Next, "Applications and Interdisciplinary Connections" demonstrates how these principles are put into practice, exploring their role in the physics analysis workflow, real-time data processing, and their relevance to other domains like [neutrino physics](@entry_id:162115). Finally, "Hands-On Practices" offers the opportunity to apply these concepts through guided exercises, solidifying your grasp of key algorithms and validation techniques. We begin by exploring the core principles that enable us to systematically search for the unexpected.

## Principles and Mechanisms

The pursuit of model-independent [anomaly detection](@entry_id:634040) is a paradigm shift from traditional searches that target specific, theoretically-motivated signatures of new physics. Instead of looking for a pre-defined "bump" or deviation, we seek to answer a more general question: "Is there anything in the data that is inconsistent with our understanding of the Standard Model background?" This requires a framework that is both sensitive to a wide range of potential anomalies and statistically rigorous. This chapter delves into the core principles and mechanisms that form the foundation of modern [anomaly detection](@entry_id:634040) strategies, from the construction of sensitive anomaly scores to the methods for their robust statistical interpretation.

### The Likelihood Principle in Anomaly Detection

At its heart, [anomaly detection](@entry_id:634040) is a problem of [hypothesis testing](@entry_id:142556). We formulate a [null hypothesis](@entry_id:265441), $H_0$, that an observed event, represented by a feature vector $x$, is a background process. The [alternative hypothesis](@entry_id:167270), $H_1$, is that the event originates from some form of new physics. The central challenge in [model-independent searches](@entry_id:752062) is that $H_1$ is not a single, well-defined hypothesis but a vast, composite class of "anything other than background."

According to the celebrated **Neyman-Pearson lemma**, for a simple null hypothesis with probability density function (PDF) $p_0(x)$ versus a simple alternative with PDF $p_1(x)$, the [most powerful test](@entry_id:169322) statistic is the **[likelihood ratio](@entry_id:170863)**, $\Lambda(x) = p_1(x)/p_0(x)$. Events are ranked by this ratio, and a threshold is chosen to achieve a desired Type I error rate (the probability of a background event being misclassified as anomalous).

In a model-independent context, we typically have a well-defined background model $p_0(x)$, often derived from high-fidelity Monte Carlo simulations or estimated from data in control regions. However, by definition, we do not have a specific $p_1(x)$ for the unknown new physics signal. This prevents the direct application of the Neyman-Pearson lemma.

A practical and widely adopted alternative is to reframe the goal: instead of looking for events that match a specific signal, we look for events that are highly *unlikely* under the background-only hypothesis. The unlikeliness of an event $x$ under $H_0$ is inversely proportional to its density $p_0(x)$. This motivates an anomaly score that is a [monotonic function](@entry_id:140815) of $1/p_0(x)$. For [numerical stability](@entry_id:146550) and its connection to information theory, the **[negative log-likelihood](@entry_id:637801)** is a standard choice for a raw anomaly score:

$s(x) = -\ln p_0(x)$

A large value of $s(x)$ corresponds to a small value of $p_0(x)$, thus flagging the event as a potential anomaly. Since the distribution of $s(X)$ under the [null hypothesis](@entry_id:265441) is fully determined by $p_0(x)$, one can set a threshold on $s(x)$ to achieve any desired Type I error rate. This provides rigorous [statistical control](@entry_id:636808) over false alarms. However, it is crucial to recognize that this test is not guaranteed to be the most powerful for any arbitrary signal model $p_1(x)$. Its power is highest for signals that preferentially populate regions of low background density [@problem_id:3504686].

The challenge of an unspecified alternative can also be approached via the **Generalized Likelihood Ratio Test (GLRT)**. If we imagine the alternative is a composite family of densities, for instance, a contamination model $p(x) = (1-\alpha)p_0(x) + \alpha q(x)$ where $\alpha$ is a mixing fraction and $q(x)$ is an unknown signal density, the GLRT statistic would be $\Lambda(x) = \sup_{\alpha, q} [p(x) / p_0(x)]$. However, if the class of possible signal densities $q(x)$ is non-parametric (i.e., completely arbitrary), this statistic is ill-posed at the single-event level. For any given point $x$, one could choose a density $q(x)$ that is infinitely peaked at that point (approaching a Dirac delta function), causing the [supremum](@entry_id:140512) to diverge. This reveals a fundamental limitation: meaningful event-level tests require some constraint on the space of alternatives, either by parameterizing the signal shape or by aggregating events into regions and performing tests on counts, as we will see next [@problem_id:3504686].

### Constructing Anomaly Scores

The effectiveness of any [anomaly detection](@entry_id:634040) pipeline hinges on the construction of a sensitive and robust anomaly score. The choice of method depends on the nature of the data, the assumptions about the background, and the specific goals of the search.

#### Bump Hunting: A Classical Paradigm

One of the most intuitive and historically successful forms of [anomaly detection](@entry_id:634040) is the "bump hunt," a search for localized excesses in a one-dimensional distribution, such as an [invariant mass](@entry_id:265871) spectrum. This method provides a concrete illustration of several key principles.

Consider a search for a resonance in an invariant [mass distribution](@entry_id:158451) $m$. The background is assumed to be a smooth, falling spectrum, but its exact normalization is unknown. A **sliding-window bump hunt** proceeds by defining a signal window $W(m_0)$ of a fixed width $\Delta$ centered at $m_0$, and adjacent **sideband regions** $S(m_0)$ used to estimate the local background level [@problem_id:3504695]. Let $n_W$ be the observed event count in the signal window and $n_S$ be the count in the [sidebands](@entry_id:261079). The background-only hypothesis $H_0$ posits that the [expected counts](@entry_id:162854), $\mu_W$ and $\mu_S$, are related by a known transfer factor $\alpha = \mu_W / \mu_S$, which is determined from the background shape (e.g., from a functional fit to a control region). The [alternative hypothesis](@entry_id:167270) $H_1$ allows for an additional signal contribution $s \ge 0$ in the signal window, such that $\mu_W \to \mu_W + s$.

The overall background normalization is an uninteresting **[nuisance parameter](@entry_id:752755)**. The [profile likelihood ratio](@entry_id:753793) test provides a principled way to construct a [test statistic](@entry_id:167372) that is powerful while accounting for this nuisance. The resulting [test statistic](@entry_id:167372), derived from the Poisson likelihoods of observing $n_W$ and $n_S$, is:

$$t(m_0) = 2\left[n_W\ln\left(\frac{n_W(\alpha+1)}{\alpha(n_W+n_S)}\right) + n_S\ln\left(\frac{n_S(\alpha+1)}{n_W+n_S}\right)\right]$$

Crucially, because we are only searching for an excess (a "bump"), this statistic is defined to be zero if the observed count in the signal window is less than or equal to the background prediction from the sidebands (i.e., if $n_W \le \alpha n_S$). This one-sided nature is a direct consequence of the constraint $s \ge 0$ in the [hypothesis test](@entry_id:635299). By scanning $m_0$ across the mass range, one can search for a localized anomaly without committing to a specific mass beforehand [@problem_id:3504695].

#### Learning the Background from Data

In many realistic scenarios, the background density $p_0(x)$ is not known from a perfect simulator but must be learned from a finite sample of background-like data, for instance from a dedicated control region. **Non-parametric [density estimation](@entry_id:634063)** provides a powerful toolkit for this task.

A widely used method is **Kernel Density Estimation (KDE)**. Given a set of background training events $\{x_j\}_{j=1}^n$, the density at a new point $m$ is estimated by averaging the contributions of kernel functions (e.g., Gaussians) centered on each training point:

$\hat{p}_0(m; h) = \frac{1}{nh} \sum_{j=1}^{n} \varphi\left(\frac{m - x_j}{h}\right)$

where $\varphi(u)$ is the standard normal PDF and $h$ is the **bandwidth**. The bandwidth is a critical hyperparameter that controls the smoothness of the density estimate. A principled, data-driven method for choosing $h$ is to minimize an estimate of the [generalization error](@entry_id:637724). **Leave-one-out cross-validation (LOOCV)** achieves this by minimizing the average [negative log-likelihood](@entry_id:637801) on held-out data. For each point $x_i$ in the [training set](@entry_id:636396), one computes its likelihood using a KDE model built from all *other* points. The bandwidth $h^\star$ that minimizes this average loss, $\mathcal{L}(h) = -\frac{1}{n} \sum_{i=1}^{n} \ln \hat{p}_{-i}(x_i; h)$, is chosen. Once $h^\star$ is found, the final background model $\hat{p}_0(x; h^\star)$ is constructed using all training data, and the anomaly score for a new event is computed as $s(x) = -\ln \hat{p}_0(x; h^\star)$ [@problem_id:3504750].

#### Classification-Based Anomaly Detection

An alternative and powerful approach, particularly when a reliable background simulator exists, is to re-purpose [binary classification](@entry_id:142257) algorithms for [anomaly detection](@entry_id:634040). This family of methods, often termed **Classification Without Labels (CWoLa)** or density ratio estimation, operates on a mixed sample of real collision data (which may contain a small, unknown signal fraction) and simulated background events.

Let the unknown data density be $p_D(x)$ and the simulated background density be $p_B(x)$. We train a classifier to distinguish between these two samples. A calibrated classifier will output the [posterior probability](@entry_id:153467) $s(x) = \mathbb{P}(\text{data} | x)$. Using Bayes' theorem, one can show that the true density ratio $r(x) = p_D(x) / p_B(x)$ can be recovered from the classifier's output and the class priors ($\pi_D, \pi_B$) used in training:

$r(x) = \frac{\pi_B}{\pi_D} \frac{s(x)}{1-s(x)}$

This density ratio is itself a powerful anomaly score. Regions where $r(x) > 1$ indicate an excess of data over the background prediction, signaling a potential anomaly. This technique is "likelihood-free" in the sense that it only requires the ability to sample from the densities, not to evaluate them directly.

This method can also be used for **[importance sampling](@entry_id:145704)**, where the weights $w(x) = r(x)$ are used to reweight background Monte Carlo events to estimate properties of the data distribution. However, this comes with a statistical caveat. If the weights have a [heavy-tailed distribution](@entry_id:145815), specifically if the second moment $\mathbb{E}_{p_B}[w(X)^2]$ is infinite, the variance of standard [importance sampling](@entry_id:145704) estimators will also be infinite, leading to unstable estimates. **Self-normalized [importance sampling](@entry_id:145704)**, which divides by the sum of the weights, provides a more stable (though technically biased for finite samples) and [consistent estimator](@entry_id:266642), and is standard practice in this context [@problem_id:3504708] [@problem_id:3504708]. The consistency of this procedure is guaranteed as long as the classifier's score can be calibrated to provide a consistent estimate of the [posterior probability](@entry_id:153467) [@problem_id:3504708].

### Building Symmetries and Robustness into Scores

Modern [anomaly detection](@entry_id:634040) methods increasingly leverage [deep learning](@entry_id:142022) to operate on low-level, high-dimensional data representations. To be physically meaningful, these models must respect the fundamental symmetries of the underlying physics and be robust against known sources of [systematic uncertainty](@entry_id:263952).

#### Encoding Physical Symmetries

In [hadron](@entry_id:198809) collider physics, events can be viewed as point clouds of particles, and jets are likewise point clouds of their constituents. The physical description of these objects must be independent of the arbitrary ordering in which the particles are detected. This requires models to be **permutation invariant**. Furthermore, the underlying physics of the hard scatter is invariant under rotations around the beam axis ($z$-axis), an $SO(2)$ symmetry. An anomaly score designed to find new physics, rather than detector artifacts, should also respect this symmetry.

These symmetries can be explicitly built into the architecture of a neural network. Permutation invariance is achieved by using symmetric aggregation functions (e.g., summation or averaging) over the set of constituents, as exemplified by **Deep Sets** architectures. Rotational invariance is enforced by designing input features that are themselves invariant. For instance, instead of using the absolute pseudorapidity $\eta$ and azimuth $\phi$ of jet constituents, one should use coordinates relative to the jet axis: $\Delta\eta_i = \eta_i - \eta_J$ and $\Delta\phi_i = \phi_i - \phi_J$. As $\Delta\phi$ is periodic, it is best represented as $(\cos\Delta\phi, \sin\Delta\phi)$ to avoid discontinuities. By constructing models that are invariant by design, we ensure that they learn physically meaningful patterns rather than relying on the network to learn symmetries from data, which is less reliable and data-efficient [@problem_id:3504688] [@problem_id:3504688].

#### Orthogonalization for Nuisance Insensitivity

Background models are rarely perfect and often depend on [nuisance parameters](@entry_id:171802) that describe sources of [systematic uncertainty](@entry_id:263952), such as detector calibration or luminosity. An ideal anomaly score should be sensitive to genuine anomalies but insensitive to small variations in these [nuisance parameters](@entry_id:171802). **Information geometry** provides a principled framework for achieving this.

The **score vector**, $t_{\boldsymbol{\theta}}(x) = \nabla_{\boldsymbol{\theta}} \log p_{\boldsymbol{\theta}}(x)$, represents the direction in the space of functions on $x$ corresponding to an infinitesimal change in the [nuisance parameter](@entry_id:752755) vector $\boldsymbol{\theta}$. An anomaly score can be made locally insensitive to these nuisances by making it orthogonal to the subspace spanned by the score vectors. Given a raw anomaly score $u(x)$, we can construct an "efficient" score $s_{\perp}(x)$ by projecting out the components aligned with the nuisance directions:

$s_{\perp}(x) = u(x) - \mathbf{c}^\top \mathbf{t}_{\boldsymbol{\theta}}(x)$

The projection coefficients $\mathbf{c}$ are found by solving a linear system defined by the **Fisher Information Matrix**, $I(\boldsymbol{\theta}) = \mathbb{E}_{p_{\boldsymbol{\theta}}}[t_{\boldsymbol{\theta}}(x) t_{\boldsymbol{\theta}}(x)^{\top}]$. This procedure effectively "blinds" the anomaly score to the specific deformations of the background model caused by [systematic uncertainties](@entry_id:755766), thereby reducing the rate of spurious anomalies arising from such effects [@problem_id:3504703].

### From Scores to Statistical Significance

An anomaly score provides a ranking of events, but it does not, by itself, constitute a statistical discovery. To claim evidence for new physics, one must translate a high score into a p-value: the probability, under the [null hypothesis](@entry_id:265441), of observing an anomaly score as high as or higher than the one found.

#### The Look-Elsewhere Effect and Random Field Theory

In searches that scan a continuous parameter space, such as a bump hunt over a mass range, we face the **[look-elsewhere effect](@entry_id:751461)**. A small local excess that might seem significant in isolation becomes less surprising when one considers the vast number of places it could have occurred. The "local" [p-value](@entry_id:136498) of a single excess is not the same as the "global" [p-value](@entry_id:136498) of the entire search.

**Random [field theory](@entry_id:155241) (RFT)** provides the mathematical framework to correctly compute this [global p-value](@entry_id:749928). For a smooth, continuous test-statistic field $X(x)$ over a parameter manifold $M$, the [global p-value](@entry_id:749928) is $\mathbb{P}(\sup_{x \in M} X(x) \ge u_{\text{obs}})$. For high thresholds $u_{\text{obs}}$, this probability is accurately approximated by the **Euler characteristic heuristic**. The key insight is that for a large threshold, the excursion set $A_u = \{x \in M : X(x) \ge u\}$ will typically consist of a few isolated, simple "blobs." In this regime, the Euler characteristic $\chi(A_u)$ simply counts the number of these blobs, and the [global p-value](@entry_id:749928) is well approximated by its expectation, $\mathbb{P}(\sup X \ge u) \approx \mathbb{E}[\chi(A_u)]$.

A celebrated result of RFT provides a formula for this expectation, which depends on universal **Euler characteristic densities** and [geometric invariants](@entry_id:178611) of the manifold $M$ known as **Lipschitz-Killing curvatures**. Critically, the relevant geometry is not arbitrary but is induced by the covariance structure of the random field itself, allowing the method to handle complex, non-stationary search statistics. This formalism provides the rigorous foundation for assigning a global significance to the most significant excess found in a continuous scan [@problem_id:3504712].

#### Conformal Prediction: Model-Agnostic p-values

For complex, high-dimensional anomaly scores derived from machine learning models, calculating a p-value analytically is often impossible. **Conformal prediction** is a powerful, non-parametric technique that can convert any anomaly score into a statistically valid [p-value](@entry_id:136498), with finite-sample guarantees.

The **split conformal** method works in two stages. First, an anomaly score model $\eta(x)$ is trained on a dedicated training set, $\mathcal{D}_{\text{train}}$. Second, the model is applied to an independent, background-only calibration set, $\mathcal{D}_{\text{cal}}$, to generate a set of reference scores $\{\eta(X^{\text{cal}}_j)\}$. The p-value for a new test event $x$ is then its empirical quantile with respect to the calibration scores, with a small smoothing factor:

$$p(x) = \frac{1 + \#\{\eta(X^{\text{cal}}_{j}) \ge \eta(x)\}}{1 + n_{\text{cal}}}$$

The theoretical guarantee of [conformal prediction](@entry_id:635847) is that if the test event is also from the background distribution (i.e., under $H_0$), it is exchangeable with the calibration events. This symmetry implies that the resulting [p-value](@entry_id:136498) is **super-uniform**, meaning $\mathbb{P}_0(p(x) \le \alpha) \le \alpha$ for any [significance level](@entry_id:170793) $\alpha \in (0,1)$. This provides exact, non-asymptotic control over the Type I error rate. This validity holds for *any* choice of nonconformity score $\eta(x)$; the quality of the score affects the statistical power of the test, not its validity. The only strict requirements are the [exchangeability](@entry_id:263314) of calibration and test data under the null and the strict separation of the training and calibration datasets [@problem_id:3504731] [@problem_id:3504731].

A crucial practical challenge is that the distribution of an anomaly score may depend on nuisance variables $Z$, such as the amount of pileup or detector run conditions. A single global calibration set may not be appropriate, as it could lead to different [false positive](@entry_id:635878) rates for different values of $Z$. **Conditional [conformal prediction](@entry_id:635847)** addresses this by modeling the [conditional distribution](@entry_id:138367) of the score $S$ given the covariates $Z$. By using techniques like [quantile regression](@entry_id:169107) to estimate the conditional CDF, $\hat{F}_z(s) \approx \mathbb{P}(S \le s | Z=z)$, one can compute p-values that are calibrated specific to the conditions of each event. If the conditional model is accurate, this restores approximate uniformity of p-values across all covariate values, ensuring a more robust and reliable [anomaly detection](@entry_id:634040) system [@problem_id:3504744].

### Foundational Identifiability Conditions

Underlying many [anomaly detection](@entry_id:634040) methods is the implicit assumption that the signal, however rare, is in principle distinguishable from the background. This can be formalized by considering a contaminated mixture model where the observed data density is $p(x) = (1-\epsilon)b(x) + \epsilon p_s(x)$, with $b(x)$ a known background density, and $\epsilon$ and $p_s(x)$ the unknown signal fraction and shape, respectively. The question of **[identifiability](@entry_id:194150)** asks: can we uniquely determine $\epsilon$ and $p_s(x)$ from observing $p(x)$?

The answer hinges on the relationship between the signal and background distributions. If the signal is merely a relabeling of the background (i.e., $p_s(x) = b(x)$), then the mixture is just $p(x) = b(x)$, and $\epsilon$ is completely unidentifiable. More generally, if the signal density contains a non-zero fraction of the background density everywhere, identifiability breaks down.

The formal condition for [identifiability](@entry_id:194150) is that the signal must be **irreducible** with respect to the background. This is equivalent to the condition that the essential infimum of the density ratio is zero: $\operatorname*{ess\,inf}_{x} \frac{p_s(x)}{b(x)} = 0$. In simpler terms, this means there must be some region of the feature space where the signal is "less present" than the background, allowing for their statistical separation. For example, if the signal and background have disjoint supports, or if there is any region where the signal density is zero but the background density is not, the signal fraction $\epsilon$ and shape $p_s(x)$ become non-parametrically identifiable. This theoretical condition provides the fundamental justification for methods that seek to learn a signal component from unlabeled data [@problem_id:3504742].