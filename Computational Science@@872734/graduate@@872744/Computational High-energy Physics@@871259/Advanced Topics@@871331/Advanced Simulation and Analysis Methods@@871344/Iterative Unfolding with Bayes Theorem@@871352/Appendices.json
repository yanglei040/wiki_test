{"hands_on_practices": [{"introduction": "To begin, we will build the iterative unfolding procedure from its foundation in Bayes' theorem. This first exercise [@problem_id:3518211] guides you through deriving the update rule for the first iteration, revealing its elegant interpretation as a normalized back-projection of the measured data. By contrasting this result with a naive matrix inversion, you will gain a direct, quantitative appreciation for the stability and regularizing power that the Bayesian approach introduces.", "problem": "Consider a simplified detector model in computational high-energy physics with three truth bins indexed by $i \\in \\{1,2,3\\}$ and three measured bins indexed by $j \\in \\{1,2,3\\}$. The detector response is described by a response matrix $A_{ji}$, where $A_{ji}$ is the conditional probability $P(j \\mid i)$ that an event originating in truth bin $i$ is reconstructed in measured bin $j$. Let the measured counts be the vector $m = (m_{1}, m_{2}, m_{3})$ and the unknown true counts be $n = (n_{1}, n_{2}, n_{3})$. Assume that acceptance effects are encoded in the columns of $A_{ji}$ (so column sums can be less than one), and ignore any out-of-acceptance bin.\n\nStarting only from Bayes’ theorem and the definition $A_{ji} = P(j \\mid i)$, derive the expression for the first Bayesian iterative unfolding estimate $n_{i}^{(1)}$ under a flat prior $n_{i}^{(0)} = c$ (where $c  0$ is constant across $i$). Show that this first iteration can be interpreted as a normalized back-projection of the measured counts through $A_{ji}$.\n\nThen, for the specific three-bin case with\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0.70  0.10  0.05 \\\\\n0.20  0.70  0.20 \\\\\n0.05  0.15  0.60\n\\end{pmatrix}\n\\quad\\text{and}\\quad\nm \\;=\\; \\begin{pmatrix} 1200 \\\\ 800 \\\\ 500 \\end{pmatrix},\n$$\ncompute:\n1. The first-iteration estimate $n^{(1)}$ obtained from your Bayes-based derivation with the flat prior $n_{i}^{(0)} = c$.\n2. The matrix-inversion unfolded estimate $n^{\\mathrm{MI}}$ defined by solving $A\\,n^{\\mathrm{MI}} = m$.\n\nFinally, quantify the deviation of the first iteration from the matrix-inversion result using the relative Euclidean norm\n$$\nr \\;=\\; \\frac{\\|\\,n^{(1)} - n^{\\mathrm{MI}}\\,\\|_{2}}{\\|\\,n^{\\mathrm{MI}}\\,\\|_{2}}.\n$$\nExpress your final answer for $r$ as a decimal fraction with no units, and round your answer to four significant figures.", "solution": "The problem statement has been validated and is deemed sound, well-posed, and scientifically grounded. It presents a standard, albeit simplified, problem in computational high-energy physics that can be rigorously solved using established mathematical methods.\n\nThe problem asks for three main components: a derivation, a numerical calculation of two estimators, and a comparison between them.\n\n**Part 1: Derivation of the First Iteration Formula**\n\nThe goal is to derive the expression for the first iterative unfolding estimate, $n_i^{(1)}$, starting from Bayes' theorem and the given definitions.\n\nBayes' theorem relates the posterior probability of a cause (event in truth bin $i$) given an effect (event in measured bin $j$) to the conditional probability of the effect given the cause and the prior probability of the cause. Formally,\n$$P(i \\mid j) = \\frac{P(j \\mid i) P(i)}{P(j)}$$\nwhere:\n- $P(i \\mid j)$ is the posterior probability that an event measured in bin $j$ originated from truth bin $i$.\n- $P(j \\mid i)$ is the conditional probability of measuring an event in bin $j$ given it originated in truth bin $i$. This is given by the response matrix element, $A_{ji}$.\n- $P(i)$ is the prior probability that an event originates in truth bin $i$.\n- $P(j)$ is the total probability of measuring an event in bin $j$, which acts as a normalization constant. It is obtained by marginalizing over all possible causes: $P(j) = \\sum_k P(j \\mid k) P(k)$.\n\nThe unfolded number of events in truth bin $i$, which we denote $n_i'$, can be estimated by considering the measured counts $m_j$. For each measured bin $j$, the number of counts is $m_j$. We can re-distribute these counts back to the truth bins according to the posterior probability $P(i \\mid j)$. Summing the contributions from all measured bins gives the estimate for $n_i'$:\n$$n_i' = \\sum_j m_j P(i \\mid j)$$\nThis formulation assumes that the total number of unfolded events is equal to the total number of measured events, which is a feature of this specific Bayesian update step when efficiencies are not explicitly corrected for in a separate step.\n\nIn the iterative Bayesian unfolding scheme, the prior $P(i)$ for a given iteration is determined by the result of the previous iteration. For the first iteration, $n_i^{(1)}$, the prior $P_0(i)$ is based on the initial guess, $n_i^{(0)}$. The problem specifies a flat prior, $n_i^{(0)} = c$ for some constant $c0$, for all $i \\in \\{1, 2, 3\\}$. This implies a uniform prior probability distribution:\n$$P_0(i) = \\frac{n_i^{(0)}}{\\sum_k n_k^{(0)}} = \\frac{c}{3c} = \\frac{1}{3}$$\nThe prior probability $P_0(i)$ is constant for all $i$. Let's denote this constant by $K$.\n\nNow, we can compute the terms in Bayes' theorem for this first iteration:\n$P(j \\mid i) = A_{ji}$\n$P_0(i) = K$\n$P(j) = \\sum_k P(j \\mid k) P_0(k) = \\sum_k A_{jk} K = K \\sum_k A_{jk}$\n\nSubstituting these into the expression for the posterior probability $P(i \\mid j)$:\n$$P(i \\mid j) = \\frac{A_{ji} K}{K \\sum_k A_{jk}} = \\frac{A_{ji}}{\\sum_k A_{jk}}$$\nThe first-iteration estimate $n_i^{(1)}$ is then found by substituting this posterior probability back into the redistribution formula:\n$$n_i^{(1)} = \\sum_j m_j \\frac{A_{ji}}{\\sum_k A_{jk}}$$\nThis expression for $n_i^{(1)}$ is independent of the constant $c$ from the flat prior, as required for a well-defined procedure.\n\n**Interpretation as Normalized Back-Projection**\n\nThe derived formula $n_i^{(1)} = \\sum_j m_j \\frac{A_{ji}}{\\sum_k A_{jk}}$ can be interpreted as a normalized back-projection.\n- The term $A_{ji}$ corresponds to an element of the transposed response matrix, $(A^T)_{ij}$. Multiplying the vector of measured counts $m$ by $A^T$ is known as a \"back-projection\". The term $m_j A_{ji}$ represents the contribution of the measurement in bin $j$ back-projected to the true bin $i$.\n- The denominator, $\\sum_k A_{jk}$, is the sum of the $j$-th row of the matrix $A$. As shown in the derivation, this term serves as the normalization factor required to transform the forward conditional probabilities $P(j \\mid i) = A_{ji}$ into the reverse conditional probabilities $P(i \\mid j)$, under the specific assumption of a flat prior $P_0(i)$.\n- Thus, each term in the sum, $m_j \\frac{A_{ji}}{\\sum_k A_{jk}}$, represents the counts measured in bin $j$ re-assigned to truth bin $i$ based on the normalized probability that an event seen in $j$ originated in $i$. Summing over all measured bins $j$ provides the total estimated count for the true bin $i$.\n\n**Part 2: Numerical Calculations**\n\nThe given data are:\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0.70  0.10  0.05 \\\\\n0.20  0.70  0.20 \\\\\n0.05  0.15  0.60\n\\end{pmatrix}\n\\quad\\text{and}\\quad\nm \\;=\\; \\begin{pmatrix} 1200 \\\\ 800 \\\\ 500 \\end{pmatrix}\n$$\n\n**1. Compute the first-iteration estimate $n^{(1)}$**\n\nFirst, we calculate the row sums of $A$, $R_j = \\sum_k A_{jk}$:\n$R_1 = 0.70 + 0.10 + 0.05 = 0.85$\n$R_2 = 0.20 + 0.70 + 0.20 = 1.10$\n$R_3 = 0.05 + 0.15 + 0.60 = 0.80$\n\nNow, we apply the formula for $n_i^{(1)}$:\n$n_1^{(1)} = \\sum_j m_j \\frac{A_{j1}}{R_j} = m_1 \\frac{A_{11}}{R_1} + m_2 \\frac{A_{21}}{R_2} + m_3 \\frac{A_{31}}{R_3}$\n$n_1^{(1)} = (1200) \\frac{0.70}{0.85} + (800) \\frac{0.20}{1.10} + (500) \\frac{0.05}{0.80} \\approx 988.2353 + 145.4545 + 31.25 = 1164.9398$\n\n$n_2^{(1)} = \\sum_j m_j \\frac{A_{j2}}{R_j} = m_1 \\frac{A_{12}}{R_1} + m_2 \\frac{A_{22}}{R_2} + m_3 \\frac{A_{32}}{R_3}$\n$n_2^{(1)} = (1200) \\frac{0.10}{0.85} + (800) \\frac{0.70}{1.10} + (500) \\frac{0.15}{0.80} \\approx 141.1765 + 509.0909 + 93.75 = 744.0174$\n\n$n_3^{(1)} = \\sum_j m_j \\frac{A_{j3}}{R_j} = m_1 \\frac{A_{13}}{R_1} + m_2 \\frac{A_{23}}{R_2} + m_3 \\frac{A_{33}}{R_3}$\n$n_3^{(1)} = (1200) \\frac{0.05}{0.85} + (800) \\frac{0.20}{1.10} + (500) \\frac{0.60}{0.80} \\approx 70.5882 + 145.4545 + 375 = 591.0427$\n\nSo, the first-iteration estimate is $n^{(1)} \\approx (1164.94, 744.02, 591.04)$.\n\n**2. Compute the matrix-inversion estimate $n^{\\mathrm{MI}}$**\n\nThe matrix-inversion estimate is the solution to the linear system $A n^{\\mathrm{MI}} = m$, which is $n^{\\mathrm{MI}} = A^{-1} m$.\nFirst, we find the determinant of $A$:\n$\\det(A) = 0.70(0.70 \\cdot 0.60 - 0.20 \\cdot 0.15) - 0.10(0.20 \\cdot 0.60 - 0.20 \\cdot 0.05) + 0.05(0.20 \\cdot 0.15 - 0.70 \\cdot 0.05)$\n$\\det(A) = 0.70(0.39) - 0.10(0.11) + 0.05(-0.005) = 0.273 - 0.011 - 0.00025 = 0.26175$.\nSince $\\det(A) \\neq 0$, the matrix is invertible. The inverse is $A^{-1} = \\frac{1}{\\det(A)}\\text{adj}(A)$, where $\\text{adj}(A)$ is the adjugate matrix of $A$.\nThe adjugate of $A$ is the transpose of the cofactor matrix:\n$\\text{adj}(A) = \n\\begin{pmatrix}\n0.39  -0.0525  -0.015 \\\\\n-0.11  0.4175  -0.13 \\\\\n-0.005  -0.10  0.47\n\\end{pmatrix}$\n\nNow we compute $n^{\\mathrm{MI}} = A^{-1}m$:\n$n^{\\mathrm{MI}} = \\frac{1}{0.26175}\n\\begin{pmatrix}\n0.39  -0.0525  -0.015 \\\\\n-0.11  0.4175  -0.13 \\\\\n-0.005  -0.10  0.47\n\\end{pmatrix}\n\\begin{pmatrix}\n1200 \\\\\n800 \\\\\n500\n\\end{pmatrix}$\n$n^{\\mathrm{MI}} = \\frac{1}{0.26175}\n\\begin{pmatrix}\n0.39(1200) - 0.0525(800) - 0.015(500) \\\\\n-0.11(1200) + 0.4175(800) - 0.13(500) \\\\\n-0.005(1200) - 0.10(800) + 0.47(500)\n\\end{pmatrix}\n= \\frac{1}{0.26175}\n\\begin{pmatrix}\n468 - 42 - 7.5 \\\\\n-132 + 334 - 65 \\\\\n-6 - 80 + 235\n\\end{pmatrix}\n= \\frac{1}{0.26175}\n\\begin{pmatrix}\n418.5 \\\\\n137 \\\\\n149\n\\end{pmatrix}$\n$n^{\\mathrm{MI}} \\approx \n\\begin{pmatrix}\n1598.8548 \\\\\n523.4986 \\\\\n569.2593\n\\end{pmatrix}$\n\n**Part 3: Quantify the Deviation**\n\nFinally, we compute the relative Euclidean norm $r = \\frac{\\|\\,n^{(1)} - n^{\\mathrm{MI}}\\,\\|_{2}}{\\|\\,n^{\\mathrm{MI}}\\,\\|_{2}}$.\nFirst, the difference vector $d = n^{(1)} - n^{\\mathrm{MI}}$:\n$d \\approx (1164.9398 - 1598.8548, 744.0174 - 523.4986, 591.0427 - 569.2593)$\n$d \\approx (-433.9150, 220.5188, 21.7834)$\n\nNext, the Euclidean norms:\n$\\|\\,d\\,\\|_{2} = \\sqrt{(-433.9150)^2 + (220.5188)^2 + (21.7834)^2} \\approx \\sqrt{188282.1 + 48628.5 + 474.5} = \\sqrt{237385.1} \\approx 487.2218$\n$\\|\\,n^{\\mathrm{MI}}\\,\\|_{2} = \\sqrt{(1598.8548)^2 + (523.4986)^2 + (569.2593)^2} \\approx \\sqrt{2556336.8 + 274050.8 + 324056.2} = \\sqrt{3154443.8} \\approx 1776.0754$\n\nThe relative norm $r$ is:\n$r = \\frac{487.2218}{1776.0754} \\approx 0.2743256$\n\nRounding to four significant figures, we get $r \\approx 0.2743$.", "answer": "$$\\boxed{0.2743}$$", "id": "3518211"}, {"introduction": "The power of Bayesian unfolding lies not just in the iteration, but also in the principled choice of its starting point, or prior. This practice [@problem_id:3518183] delves into the theoretical underpinnings of prior selection by having you derive the Jeffreys prior for Poisson-distributed counts, a common scenario in binned data analysis. You will see how this formal principle translates into a practical and robust initialization strategy that prevents unphysical results, especially when dealing with bins containing few or zero events.", "problem": "In computational High Energy Physics (HEP), unfolding aims to infer the distribution of a true quantity discretized into bins, denoted by $T_i$ for $i$ in a finite set of bin indices, from measurements recorded in reconstructed bins, denoted by $R_j$. In iterative Bayesian unfolding (IBU), one updates an initial prior over the true-bin means using Bayes’ theorem and the detector response. Consider a two-bin ($i \\in \\{1,2\\}$) true distribution and a two-bin ($j \\in \\{1,2\\}$) reconstructed distribution. The detector response probabilities are $A_{j i} \\equiv P(R_j \\mid T_i)$, assumed known and given by $A_{11} = 0.8$, $A_{12} = 0.2$, $A_{21} = 0.2$, $A_{22} = 0.8$. The observed reconstructed counts are $m_1 = 0$ and $m_2 = 3$.\n\n(a) Starting from first principles, use the definition of the Jeffreys prior $\\pi(\\theta) \\propto \\sqrt{I(\\theta)}$, where $I(\\theta)$ is the Fisher information, and the Poisson likelihood for an observed count $k$ given a mean $\\mu$, to derive the Jeffreys prior for a Poisson mean $\\mu_i$ in bin $i$, showing that $\\pi(\\mu_i) \\propto \\mu_i^{-1/2}$.\n\n(b) Based on part (a), explain how one may encode this prior information as an initial pseudo-count $n_i^{(0)}$ per true bin when initializing IBU, and analyze the implications of this choice for low-statistics situations (for example, when $m_j = 0$ for some $j$). Your explanation should rely on the connection between Jeffreys’ prior and conjugate Bayesian updating for the Poisson model, and on Bayes’ theorem and the law of total probability, not on heuristic rules.\n\n(c) Using the conclusion of part (b) in the case of two true bins with no additional shape information (so that the initial prior over true-bin membership is symmetric), perform the first IBU update to compute the unfolded estimate $n_1^{(1)}$ for the true bin $i=1$. You may treat the initial prior probabilities over true bins as proportional to the initial pseudo-counts $n_i^{(0)}$ that arise from the Jeffreys prior and normalize them across bins. Report the exact value of $n_1^{(1)}$ with no rounding and no units.", "solution": "This problem is valid as it is scientifically grounded in the principles of Bayesian statistics and its application to data analysis in high-energy physics, is well-posed with a complete and consistent set of givens, and is expressed in objective, formal language. We may proceed with a solution.\n\nThe problem is divided into three parts. We will address them sequentially, building upon the results of each part.\n\n**(a) Derivation of the Jeffreys Prior for a Poisson Mean**\n\nWe are tasked with deriving the Jeffreys prior for the mean $\\mu_i$ of a Poisson distribution. The Jeffreys prior for a parameter $\\theta$ is defined as $\\pi(\\theta) \\propto \\sqrt{I(\\theta)}$, where $I(\\theta)$ is the Fisher information.\n\nLet us consider a single bin $i$, and for simplicity, let its mean be denoted by $\\mu$. The probability of observing $k$ counts in this bin, given the mean $\\mu$, follows a Poisson distribution. The likelihood function is:\n$$L(\\mu \\mid k) = P(k \\mid \\mu) = \\frac{\\mu^k \\exp(-\\mu)}{k!}$$\nThe natural logarithm of the likelihood, or log-likelihood, is:\n$$\\ln L(\\mu \\mid k) = k \\ln \\mu - \\mu - \\ln(k!)$$\nThe Fisher information $I(\\mu)$ is defined as the negative of the expectation value of the second derivative of the log-likelihood with respect to the parameter $\\mu$. First, we compute the first and second derivatives:\n$$\\frac{\\partial}{\\partial \\mu} \\ln L(\\mu \\mid k) = \\frac{k}{\\mu} - 1$$\n$$\\frac{\\partial^2}{\\partial \\mu^2} \\ln L(\\mu \\mid k) = -\\frac{k}{\\mu^2}$$\nThe Fisher information is then:\n$$I(\\mu) = -E\\left[\\frac{\\partial^2}{\\partial \\mu^2} \\ln L(\\mu \\mid k)\\right]$$\nThe expectation is taken with respect to the distribution of the data $k$, which is the Poisson distribution with mean $\\mu$. Therefore, $E[k] = \\mu$. Substituting this into the expression for $I(\\mu)$:\n$$I(\\mu) = -E\\left[-\\frac{k}{\\mu^2}\\right] = \\frac{E[k]}{\\mu^2} = \\frac{\\mu}{\\mu^2} = \\frac{1}{\\mu}$$\nThe Jeffreys prior $\\pi(\\mu)$ is proportional to the square root of the Fisher information:\n$$\\pi(\\mu) \\propto \\sqrt{I(\\mu)} = \\sqrt{\\frac{1}{\\mu}} = \\mu^{-1/2}$$\nThis derivation holds for the mean $\\mu_i$ of any bin $i$. Thus, we have shown that $\\pi(\\mu_i) \\propto \\mu_i^{-1/2}$.\n\n**(b) Interpretation of the Jeffreys Prior as an Initial Pseudo-Count**\n\nTo understand how this prior information is encoded, we examine the framework of conjugate Bayesian updating for the Poisson model. The conjugate prior for a Poisson likelihood is the Gamma distribution. A Gamma distribution for the mean $\\mu$ with shape parameter $\\alpha$ and rate parameter $\\beta$ is given by:\n$$\\pi(\\mu \\mid \\alpha, \\beta) \\propto \\mu^{\\alpha-1} \\exp(-\\beta\\mu)$$\nThe Jeffreys prior derived in part (a), $\\pi(\\mu) \\propto \\mu^{-1/2}$, can be identified as a Gamma distribution with parameters $\\alpha = 1/2$ and $\\beta=0$. This is an improper prior since its integral over $\\mu \\in [0, \\infty)$ diverges.\n\nWhen we update this prior with an observation of $k$ counts from a Poisson process, the posterior distribution for $\\mu$ is also a Gamma distribution. The posterior is proportional to the product of the likelihood and the prior:\n$$P(\\mu \\mid k) \\propto L(k \\mid \\mu) \\pi(\\mu) \\propto \\left(\\mu^k \\exp(-\\mu)\\right) \\left(\\mu^{\\alpha-1} \\exp(-\\beta\\mu)\\right) = \\mu^{k+\\alpha-1} \\exp(-(\\beta+1)\\mu)$$\nFor the Jeffreys prior ($\\alpha=1/2, \\beta=0$), the posterior distribution is:\n$$P(\\mu \\mid k) \\propto \\mu^{k+1/2-1} \\exp(-\\mu)$$\nThis is a proper Gamma distribution, specifically $\\text{Gamma}(k+1/2, 1)$. The mean of this posterior distribution is $E[\\mu \\mid k] = \\frac{k+1/2}{1} = k + 1/2$.\n\nThis result provides a clear interpretation: starting with the Jeffreys prior is equivalent to obtaining a posterior estimate for the mean by adding $1/2$ to the observed count $k$. This suggests that the Jeffreys prior can be encoded as an initial \"pseudo-count\" of $n_i^{(0)} = 1/2$ for each true bin $i$. This serves as the starting point for the iterative unfolding procedure before any data are considered.\n\nThe implications for low-statistics situations are significant. The Iterative Bayesian Unfolding (IBU) algorithm updates an estimate for the number of true-bin counts, $n_i^{(k)}$, at each iteration $k$. A common update rule is $n_i^{(k+1)} = \\sum_j m_j P^{(k)}(T_i \\mid R_j)$, where $P^{(k)}(T_i \\mid R_j)$ depends on the prior probabilities from the previous step, $P^{(k)}(T_i) \\propto n_i^{(k)}$. If we were to initialize with $n_i^{(0)} = 0$ for some bin $i$, its prior probability $P^{(0)}(T_i)$ would be zero, and consequently, its unfolded estimate $n_i^{(k)}$ would remain zero for all subsequent iterations, regardless of the observed data. This is an undesirable property, as it prematurely rules out a true bin as a possible cause for the observed effects.\n\nBy choosing $n_i^{(0)} = 1/2$ for all $i$, we ensure that every true bin starts with a non-zero count. This regularizes the problem, preventing the estimates for any bin from being permanently fixed at zero. Even if a reconstructed bin $j$ has zero observed counts ($m_j = 0$), the true bins that could contribute to it are not eliminated from consideration for contributing to other observed bins. This initialization guarantees that the initial priors $P^{(0)}(T_i)$ are well-defined and non-zero, allowing the algorithm to distribute the observed counts $m_j$ among all possible true causes according to the response probabilities.\n\n**(c) First Iteration of Unfolding**\n\nWe are asked to perform the first IBU update to compute the unfolded estimate $n_1^{(1)}$ for the true bin $i=1$.\nBased on part (b), and the condition of no additional shape information (implying symmetry), we initialize the pseudo-counts for the two true bins as:\n$$n_1^{(0)} = \\frac{1}{2}, \\quad n_2^{(0)} = \\frac{1}{2}$$\nThe total initial pseudo-count is $N^{(0)} = n_1^{(0)} + n_2^{(0)} = 1/2 + 1/2 = 1$. The initial prior probabilities over the true bins are normalized:\n$$P^{(0)}(T_1) = \\frac{n_1^{(0)}}{N^{(0)}} = \\frac{1/2}{1} = \\frac{1}{2}$$\n$$P^{(0)}(T_2) = \\frac{n_2^{(0)}}{N^{(0)}} = \\frac{1/2}{1} = \\frac{1}{2}$$\nThe IBU update formula for the first iteration is:\n$$n_i^{(1)} = \\sum_{j=1}^{2} m_j P(T_i \\mid R_j)$$\nwhere the posterior probability $P(T_i \\mid R_j)$ is computed using Bayes' theorem with the initial prior $P^{(0)}(T_i)$:\n$$P(T_i \\mid R_j) = \\frac{P(R_j \\mid T_i) P^{(0)}(T_i)}{\\sum_{l=1}^{2} P(R_j \\mid T_l) P^{(0)}(T_l)} = \\frac{A_{ji} P^{(0)}(T_i)}{\\sum_{l=1}^{2} A_{jl} P^{(0)}(T_l)}$$\nWe need to compute $n_1^{(1)}$:\n$$n_1^{(1)} = m_1 P(T_1 \\mid R_1) + m_2 P(T_1 \\mid R_2)$$\nSubstituting the formula for the posterior probabilities:\n$$n_1^{(1)} = m_1 \\frac{A_{11} P^{(0)}(T_1)}{A_{11} P^{(0)}(T_1) + A_{12} P^{(0)}(T_2)} + m_2 \\frac{A_{21} P^{(0)}(T_1)}{A_{21} P^{(0)}(T_1) + A_{22} P^{(0)}(T_2)}$$\nWe are given the following values:\n\\begin{itemize}\n    \\item Observed counts: $m_1 = 0$, $m_2 = 3$\n    \\item Response matrix elements: $A_{11} = 0.8$, $A_{12} = 0.2$, $A_{21} = 0.2$, $A_{22} = 0.8$\n    \\item Initial priors: $P^{(0)}(T_1) = 1/2$, $P^{(0)}(T_2) = 1/2$\n\\end{itemize}\nThe first term in the expression for $n_1^{(1)}$ is zero because $m_1 = 0$. We only need to compute the second term:\n$$n_1^{(1)} = 0 + 3 \\times \\frac{(0.2) \\times (1/2)}{(0.2) \\times (1/2) + (0.8) \\times (1/2)}$$\nSimplifying the expression within the fraction:\n$$n_1^{(1)} = 3 \\times \\frac{0.1}{0.1 + 0.4} = 3 \\times \\frac{0.1}{0.5}$$\n$$n_1^{(1)} = 3 \\times \\frac{1}{5} = \\frac{3}{5}$$\nConverting to decimal form, this is $0.6$. The question asks for the exact value.", "answer": "$$\\boxed{\\frac{3}{5}}$$", "id": "3518183"}, {"introduction": "A real-world analysis must always account for uncertainties in the detector model. This final exercise [@problem_id:3518217] provides a powerful \"stress test\" for the unfolding algorithm, exploring how the result is affected by a misspecified response matrix. By applying first-order perturbation theory, you will derive an analytic expression for the bias in the unfolded spectrum, gaining a concrete understanding of how systematic uncertainties propagate through the iterative procedure.", "problem": "Consider a two-bin toy model of detector smearing used in computational high-energy physics to study iterative unfolding with Bayes' theorem. Let the true response matrix be the conditional probability matrix $A$ with elements $A_{ij} = P(m_i \\mid t_j)$, where $i \\in \\{1,2\\}$ indexes measured bins and $j \\in \\{1,2\\}$ indexes true bins. Assume unit detection efficiency so that for each true bin $j$ one has $\\sum_{i} A_{ij} = 1$. Take\n$$\nA \\;=\\; \\begin{pmatrix}\n0.8  0.2 \\\\\n0.2  0.8\n\\end{pmatrix}.\n$$\nLet the true spectrum be $n^{\\star} = \\begin{pmatrix} 1200  800 \\end{pmatrix}$, and assume noiseless data equal to the expectation value $m = A\\,n^{\\star}$, i.e.,\n$$\nm_i \\;=\\; \\sum_{\\ell=1}^{2} A_{i\\ell}\\,n^{\\star}_{\\ell}.\n$$\nYou will perform one iteration of Bayesian iterative unfolding (derived from Bayes' theorem) starting from the prior $n^{(0)} = n^{\\star}$. Specifically, the iterative update for the unfolded counts after one iteration is\n$$\nn_j^{(1)}(\\delta) \\;=\\; n_j^{(0)} \\sum_{i=1}^{2} \\left[A_{ij} + \\delta\\,\\Delta_{ij}\\right] \\frac{m_i}{\\mu_i^{(0)}(\\delta)},\n$$\nwith\n$$\n\\mu_i^{(0)}(\\delta) \\;=\\; \\sum_{\\ell=1}^{2} \\left[A_{i\\ell} + \\delta\\,\\Delta_{i\\ell}\\right] n_{\\ell}^{(0)},\n$$\nwhere the unfolding uses a misspecified response matrix $A(\\delta) = A + \\delta\\,\\Delta$ with a small shift parameter $\\delta$ and perturbation matrix\n$$\n\\Delta \\;=\\; \\begin{pmatrix}\n1  -1 \\\\\n-1  1\n\\end{pmatrix}.\n$$\nThis perturbation preserves column normalization to first order, i.e., $\\sum_{i} \\Delta_{ij} = 0$ for each $j$. Treat $\\delta$ as a small parameter satisfying $0  \\delta \\ll 0.2$ so that all matrix elements remain in the interval $[0,1]$.\n\nUsing first-order perturbation theory in $\\delta$ applied to the Bayesian iterative unfolding update, derive the analytic first-order bias vector $b^{(1)}(\\delta) = n^{(1)}(\\delta) - n^{(1)}(0)$ after exactly one iteration. Express your final answer as a closed-form row vector in terms of $\\delta$ only. No rounding is required, and you must provide an exact expression. Do not include units in your final expression.", "solution": "### Step 1: Extract Givens\nThe problem provides the following information:\n- The true response matrix, $A_{ij} = P(m_i \\mid t_j)$, is $A = \\begin{pmatrix} 0.8  0.2 \\\\ 0.2  0.8 \\end{pmatrix}$.\n- The indices are $i, j \\in \\{1,2\\}$.\n- Detection efficiency is unity, so $\\sum_{i} A_{ij} = 1$ for each true bin $j$.\n- The true spectrum is $n^{\\star} = \\begin{pmatrix} 1200  800 \\end{pmatrix}$. For matrix operations, we treat this as a column vector $n^{\\star} = \\begin{pmatrix} 1200 \\\\ 800 \\end{pmatrix}$.\n- The measured data, $m$, is noiseless: $m_i = \\sum_{\\ell=1}^{2} A_{i\\ell}\\,n^{\\star}_{\\ell}$, or in vector form, $m = An^{\\star}$.\n- The prior for the first iteration is the true spectrum: $n^{(0)} = n^{\\star}$.\n- The iterative update formula is $n_j^{(1)}(\\delta) = n_j^{(0)} \\sum_{i=1}^{2} \\left[A_{ij} + \\delta\\,\\Delta_{ij}\\right] \\frac{m_i}{\\mu_i^{(0)}(\\delta)}$.\n- The denominator in the update is $\\mu_i^{(0)}(\\delta) = \\sum_{\\ell=1}^{2} \\left[A_{i\\ell} + \\delta\\,\\Delta_{i\\ell}\\right] n_{\\ell}^{(0)}$.\n- The perturbation matrix is $\\Delta = \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix}$.\n- $\\delta$ is a small parameter satisfying $0  \\delta \\ll 0.2$.\n- The goal is to find the first-order bias vector $b^{(1)}(\\delta) = n^{(1)}(\\delta) - n^{(1)}(0)$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem describes a standard technique in computational high-energy physics known as Bayesian iterative unfolding, specifically following the formulation by D'Agostini. It uses established principles of probability (Bayes' theorem) and linear algebra to model detector response and correct for it. The setup is a \"toy model,\" a common and valid method for studying the properties of algorithms.\n2.  **Well-Posed:** The problem is mathematically well-defined. All necessary matrices, vectors, and equations are provided. The objective is clearly stated: derive an analytic expression for the first-order bias. The structure allows for a unique solution.\n3.  **Objective:** The problem is stated in precise, formal language, free of ambiguity or subjective claims.\n4.  **Completeness and Consistency:** The problem is self-contained. The condition $\\sum_i A_{ij} = 1$ is satisfied by the given matrix $A$ ($0.8+0.2=1$). The condition on the perturbation, $\\sum_i \\Delta_{ij} = 0$, is also satisfied ($1+(-1)=0$ and $-1+1=0$). The constraint $0  \\delta \\ll 0.2$ ensures that the elements of the perturbed matrix $A(\\delta) = A + \\delta\\Delta$ remain valid probabilities (e.g., $A_{12}(\\delta) = 0.2 - \\delta  0$). The setup is internally consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically sound, well-posed, and internally consistent. We can proceed with the solution.\n\n### Detailed Solution\nThe goal is to compute the first-order bias vector $b^{(1)}(\\delta) = n^{(1)}(\\delta) - n^{(1)}(0)$ after one iteration. We will use a first-order Taylor expansion in the small parameter $\\delta$.\n\nFirst, let's calculate the quantities at $\\delta=0$. The prior is $n^{(0)} = n^{\\star} = \\begin{pmatrix} 1200 \\\\ 800 \\end{pmatrix}$.\nThe noiseless measured data vector $m$ is:\n$$\nm = An^{\\star} = \\begin{pmatrix} 0.8  0.2 \\\\ 0.2  0.8 \\end{pmatrix} \\begin{pmatrix} 1200 \\\\ 800 \\end{pmatrix} = \\begin{pmatrix} 0.8 \\cdot 1200 + 0.2 \\cdot 800 \\\\ 0.2 \\cdot 1200 + 0.8 \\cdot 800 \\end{pmatrix} = \\begin{pmatrix} 960 + 160 \\\\ 240 + 640 \\end{pmatrix} = \\begin{pmatrix} 1120 \\\\ 880 \\end{pmatrix}.\n$$\n\nThe denominator of the update rule, $\\mu_i^{(0)}(\\delta)$, is given by $\\mu_i^{(0)}(\\delta) = \\sum_{\\ell=1}^{2} (A_{i\\ell} + \\delta \\Delta_{i\\ell}) n_{\\ell}^{(0)}$. At $\\delta=0$, this becomes:\n$$\n\\mu_i^{(0)}(0) = \\sum_{\\ell=1}^{2} A_{i\\ell} n_{\\ell}^{(0)}.\n$$\nSince $n^{(0)} = n^{\\star}$, this is identical to the definition of the measured data $m_i$. Thus, $\\mu^{(0)}(0) = A n^{(0)} = A n^{\\star} = m$.\n\nNow, we can find the unperturbed unfolded counts, $n^{(1)}(0)$:\n$$\nn_j^{(1)}(0) = n_j^{(0)} \\sum_{i=1}^{2} A_{ij} \\frac{m_i}{\\mu_i^{(0)}(0)} = n_j^{(0)} \\sum_{i=1}^{2} A_{ij} \\frac{m_i}{m_i} = n_j^{(0)} \\sum_{i=1}^{2} A_{ij}.\n$$\nThe problem states that the response matrix has unit efficiency, $\\sum_i A_{ij} = 1$. Therefore,\n$$\nn_j^{(1)}(0) = n_j^{(0)} \\cdot 1 = n_j^{(0)}.\n$$\nThis means that for the unperturbed case, the unfolded spectrum after one iteration is identical to the prior, which is the true spectrum: $n^{(1)}(0) = n^{(0)} = n^{\\star}$.\n\nNext, we expand the update formula for $n_j^{(1)}(\\delta)$ to first order in $\\delta$. The core of the expansion lies in the term $\\frac{m_i}{\\mu_i^{(0)}(\\delta)}$.\nLet's expand the denominator $\\mu_i^{(0)}(\\delta)$:\n$$\n\\mu_i^{(0)}(\\delta) = \\sum_{\\ell=1}^{2} A_{i\\ell}n_{\\ell}^{(0)} + \\delta \\sum_{\\ell=1}^{2} \\Delta_{i\\ell}n_{\\ell}^{(0)} = \\mu_i^{(0)}(0) + \\delta \\left(\\Delta n^{(0)}\\right)_i = m_i + \\delta \\left(\\Delta n^{(0)}\\right)_i.\n$$\nLet's compute the vector $\\Delta n^{(0)}$:\n$$\n\\Delta n^{(0)} = \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix} \\begin{pmatrix} 1200 \\\\ 800 \\end{pmatrix} = \\begin{pmatrix} 1200 - 800 \\\\ -1200 + 800 \\end{pmatrix} = \\begin{pmatrix} 400 \\\\ -400 \\end{pmatrix}.\n$$\nSo, $\\mu_i^{(0)}(\\delta) = m_i + \\delta (\\Delta n^{(0)})_i$. Now we expand the fraction $\\frac{m_i}{\\mu_i^{(0)}(\\delta)}$:\n$$\n\\frac{m_i}{\\mu_i^{(0)}(\\delta)} = \\frac{m_i}{m_i + \\delta (\\Delta n^{(0)})_i} = \\frac{1}{1 + \\delta \\frac{(\\Delta n^{(0)})_i}{m_i}} \\approx 1 - \\delta \\frac{(\\Delta n^{(0)})_i}{m_i} + \\mathcal{O}(\\delta^2).\n$$\nSubstitute this expansion into the formula for $n_j^{(1)}(\\delta)$:\n$$\nn_j^{(1)}(\\delta) = n_j^{(0)} \\sum_{i=1}^{2} \\left[A_{ij} + \\delta\\,\\Delta_{ij}\\right] \\left(1 - \\delta \\frac{(\\Delta n^{(0)})_i}{m_i}\\right).\n$$\nExpanding the product and keeping terms up to first order in $\\delta$:\n$$\nn_j^{(1)}(\\delta) \\approx n_j^{(0)} \\sum_{i=1}^{2} \\left( A_{ij} - \\delta A_{ij} \\frac{(\\Delta n^{(0)})_i}{m_i} + \\delta \\Delta_{ij} \\right)\n$$\n$$\nn_j^{(1)}(\\delta) \\approx n_j^{(0)} \\sum_{i=1}^{2} A_{ij} + \\delta \\, n_j^{(0)} \\sum_{i=1}^{2} \\left( \\Delta_{ij} - A_{ij} \\frac{(\\Delta n^{(0)})_i}{m_i} \\right).\n$$\nThe first term is $n_j^{(0)} \\sum_i A_{ij} = n_j^{(0)} = n_j^{(1)}(0)$.\nThe first-order bias is therefore:\n$$\nb_j^{(1)}(\\delta) = n_j^{(1)}(\\delta) - n_j^{(1)}(0) \\approx \\delta \\, n_j^{(0)} \\sum_{i=1}^{2} \\left( \\Delta_{ij} - A_{ij} \\frac{(\\Delta n^{(0)})_i}{m_i} \\right).\n$$\nNow we compute the components of the bias vector $b^{(1)}(\\delta) = \\begin{pmatrix} b_1^{(1)}(\\delta)  b_2^{(1)}(\\delta) \\end{pmatrix}$.\n\nFor bin $j=1$:\n$n_1^{(0)} = 1200$. The sum is over $i=1,2$.\n$$\nb_1^{(1)}(\\delta) = \\delta \\cdot 1200 \\left[ \\left(\\Delta_{11} - A_{11} \\frac{(\\Delta n^{(0)})_1}{m_1}\\right) + \\left(\\Delta_{21} - A_{21} \\frac{(\\Delta n^{(0)})_2}{m_2}\\right) \\right].\n$$\nUsing the values:\n$\\Delta = \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix}$, $A = \\begin{pmatrix} 0.8  0.2 \\\\ 0.2  0.8 \\end{pmatrix}$, $m=\\begin{pmatrix} 1120 \\\\ 880 \\end{pmatrix}$, $\\Delta n^{(0)}=\\begin{pmatrix} 400 \\\\ -400 \\end{pmatrix}$.\n$$\nb_1^{(1)}(\\delta) = \\delta \\cdot 1200 \\left[ \\left(1 - 0.8 \\cdot \\frac{400}{1120}\\right) + \\left(-1 - 0.2 \\cdot \\frac{-400}{880}\\right) \\right].\n$$\nSimplify the fractions: $\\frac{400}{1120} = \\frac{40}{112} = \\frac{5}{14}$ and $\\frac{-400}{880} = \\frac{-40}{88} = -\\frac{5}{11}$.\n$$\nb_1^{(1)}(\\delta) = \\delta \\cdot 1200 \\left[ \\left(1 - \\frac{4}{5} \\cdot \\frac{5}{14}\\right) + \\left(-1 - \\frac{1}{5} \\cdot \\left(-\\frac{5}{11}\\right)\\right) \\right]\n$$\n$$\nb_1^{(1)}(\\delta) = \\delta \\cdot 1200 \\left[ \\left(1 - \\frac{4}{14}\\right) + \\left(-1 + \\frac{1}{11}\\right) \\right] = \\delta \\cdot 1200 \\left[ \\left(1 - \\frac{2}{7}\\right) + \\left(-\\frac{10}{11}\\right) \\right]\n$$\n$$\nb_1^{(1)}(\\delta) = \\delta \\cdot 1200 \\left[ \\frac{5}{7} - \\frac{10}{11} \\right] = \\delta \\cdot 1200 \\left[ \\frac{55 - 70}{77} \\right] = \\delta \\cdot 1200 \\left( -\\frac{15}{77} \\right) = -\\frac{18000}{77}\\delta.\n$$\n\nFor bin $j=2$:\n$n_2^{(0)} = 800$.\n$$\nb_2^{(1)}(\\delta) = \\delta \\cdot 800 \\left[ \\left(\\Delta_{12} - A_{12} \\frac{(\\Delta n^{(0)})_1}{m_1}\\right) + \\left(\\Delta_{22} - A_{22} \\frac{(\\Delta n^{(0)})_2}{m_2}\\right) \\right].\n$$\n$$\nb_2^{(1)}(\\delta) = \\delta \\cdot 800 \\left[ \\left(-1 - 0.2 \\cdot \\frac{400}{1120}\\right) + \\left(1 - 0.8 \\cdot \\frac{-400}{880}\\right) \\right].\n$$\n$$\nb_2^{(1)}(\\delta) = \\delta \\cdot 800 \\left[ \\left(-1 - \\frac{1}{5} \\cdot \\frac{5}{14}\\right) + \\left(1 - \\frac{4}{5} \\cdot \\left(-\\frac{5}{11}\\right)\\right) \\right]\n$$\n$$\nb_2^{(1)}(\\delta) = \\delta \\cdot 800 \\left[ \\left(-1 - \\frac{1}{14}\\right) + \\left(1 + \\frac{4}{11}\\right) \\right] = \\delta \\cdot 800 \\left[ -\\frac{15}{14} + \\frac{15}{11} \\right]\n$$\n$$\nb_2^{(1)}(\\delta) = \\delta \\cdot 800 \\cdot 15 \\left[ \\frac{1}{11} - \\frac{1}{14} \\right] = \\delta \\cdot 12000 \\left[ \\frac{14 - 11}{154} \\right] = \\delta \\cdot 12000 \\left( \\frac{3}{154} \\right)\n$$\n$$\nb_2^{(1)}(\\delta) = \\delta \\cdot \\frac{36000}{154} = \\delta \\cdot \\frac{18000}{77} = \\frac{18000}{77}\\delta.\n$$\nThe first-order bias vector is $b^{(1)}(\\delta) = \\begin{pmatrix} b_1^{(1)}(\\delta)  b_2^{(1)}(\\delta) \\end{pmatrix} = \\begin{pmatrix} -\\frac{18000}{77}\\delta  \\frac{18000}{77}\\delta \\end{pmatrix}$.\nNote that the sum of the biases is zero, $b_1^{(1)}(\\delta) + b_2^{(1)}(\\delta) = 0$, which means the total number of events is conserved to first order, as expected.\nThe final answer is requested as a row vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{18000}{77}\\delta  \\frac{18000}{77}\\delta \\end{pmatrix}}\n$$", "id": "3518217"}]}