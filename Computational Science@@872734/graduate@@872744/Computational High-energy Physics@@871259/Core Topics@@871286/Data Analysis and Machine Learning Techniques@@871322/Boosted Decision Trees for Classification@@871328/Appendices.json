{"hands_on_practices": [{"introduction": "Modern gradient boosting frameworks rely on a second-order Taylor expansion of the loss function to guide the training process. This practice delves into the core optimization step for determining the output value of each new leaf added to the ensemble. By deriving the optimal regularized leaf weight [@problem_id:3506549], you will gain a fundamental understanding of how gradients ($g_i$), Hessians ($h_i$), and regularization ($\\lambda$) come together to control overfitting and ensure numerical stability in state-of-the-art BDTs.", "problem": "Consider binary classification in computational high-energy physics using Boosted Decision Trees (BDTs), where a tree is grown to partition events and each terminal node (leaf) outputs a constant score added to the current model. In gradient boosting with a twice differentiable convex loss, for a single leaf with output $v$, the regularized objective over the events $i$ routed to that leaf is approximated by a second-order Taylor expansion around the current margin. Let $g_i$ denote the first derivative (gradient) of the loss with respect to the margin for event $i$, let $h_i$ denote the second derivative (Hessian) for event $i$, and let $\\lambda$ be the strength of the squared $\\ell_2$ regularization on the leaf output. Starting from the second-order expansion of the regularized objective and using the fact that convex losses yield $h_i \\ge 0$, derive the unique minimizer $v$ in terms of $\\{g_i\\}$, $\\{h_i\\}$, and $\\lambda$, and explain why this choice is numerically stable when $\\sum_i h_i$ is small and how $\\lambda$ controls the tendency to overfit.\n\nImplement a program that, for each test case, takes a finite list of real numbers $\\{g_i\\}$, a finite list of real numbers $\\{h_i\\}$ (with each $h_i \\ge 0$), and a real number $\\lambda \\ge 0$, and returns the derived regularized leaf output $v$. For numerical robustness, adopt the following rule: define $S_g = \\sum_i g_i$ and $S_h = \\sum_i h_i$, and if $S_h + \\lambda \\le \\varepsilon$ with $\\varepsilon = 10^{-12}$, output $0.0$; otherwise output the unique minimizer from the derivation. This rule ensures a safe output when curvature and regularization vanish or are extremely small.\n\nYour program must evaluate the following test suite:\n- Case $1$: $\\{g_i\\} = [\\,0.8,\\,-0.5,\\,0.1\\,]$, $\\{h_i\\} = [\\,0.25,\\,0.2,\\,0.1\\,]$, $\\lambda = 1.0$.\n- Case $2$: $\\{g_i\\} = [\\,0.1,\\,0.1,\\,-0.1\\,]$, $\\{h_i\\} = [\\,10^{-12},\\,2\\times 10^{-12},\\,0.0\\,]$, $\\lambda = 0.0$.\n- Case $3$: $\\{g_i\\} = [\\,0.1,\\,0.1,\\,-0.1\\,]$, $\\{h_i\\} = [\\,10^{-12},\\,2\\times 10^{-12},\\,0.0\\,]$, $\\lambda = 10^{-6}$.\n- Case $4$: $\\{g_i\\} = [\\,-2.0,\\,1.0,\\,-0.5,\\,0.3\\,]$, $\\{h_i\\} = [\\,0.5,\\,0.4,\\,0.3,\\,0.2\\,]$, $\\lambda = 1000.0$.\n- Case $5$: $\\{g_i\\} = [\\,1000.0,\\,-950.0,\\,25.0,\\,-10.0\\,]$, $\\{h_i\\} = [\\,0.01,\\,0.01,\\,0.01,\\,0.01\\,]$, $\\lambda = 1.0$.\n- Case $6$: $\\{g_i\\} = [\\,0.0,\\,0.0,\\,0.0\\,]$, $\\{h_i\\} = [\\,0.2,\\,0.3,\\,0.1\\,]$, $\\lambda = 0.5$.\n- Case $7$: $\\{g_i\\} = [\\,0.5,\\,-0.5\\,]$, $\\{h_i\\} = [\\,0.0,\\,0.0\\,]$, $\\lambda = 0.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\,\\text{result}_1,\\text{result}_2,\\ldots\\,]$). Each result must be a real number (a float). No physical units or angles are involved in this computation, and no percentages are required.", "solution": "The problem statement is critically evaluated for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Topic**: Binary classification in computational high-energy physics using Boosted Decision Trees (BDTs).\n- **Model Component**: A single terminal node (leaf) in a decision tree.\n- **Objective Function**: The regularized objective for a leaf is approximated by a second-order Taylor expansion around the current margin.\n- **Variables**:\n    - $v$: The constant output score for the leaf.\n    - $i$: An index for events routed to the leaf.\n    - $g_i$: First derivative (gradient) of the loss with respect to the margin for event $i$.\n    - $h_i$: Second derivative (Hessian) of the loss with respect to the margin for event $i$.\n- **Constants and Conditions**:\n    - The loss function is twice differentiable and convex, which implies $h_i \\ge 0$.\n    - $\\lambda$: The strength of the squared $\\ell_2$ regularization on the leaf output, with $\\lambda \\ge 0$.\n- **Task**:\n    1.  Derive the unique minimizer $v$ in terms of $\\{g_i\\}$, $\\{h_i\\}$, and $\\lambda$.\n    2.  Explain the numerical stability of this choice when $\\sum_i h_i$ is small.\n    3.  Explain how $\\lambda$ controls overfitting.\n    4.  Implement a program to compute $v$ for several test cases.\n- **Numerical Robustness Rule**:\n    - Define $S_g = \\sum_i g_i$ and $S_h = \\sum_i h_i$.\n    - If $S_h + \\lambda \\le \\varepsilon$ with $\\varepsilon = 10^{-12}$, the output $v$ must be $0.0$.\n    - Otherwise, output the derived unique minimizer.\n- **Test Suite**:\n    - Case $1$: $\\{g_i\\} = [\\,0.8,\\,-0.5,\\,0.1\\,]$, $\\{h_i\\} = [\\,0.25,\\,0.2,\\,0.1\\,]$, $\\lambda = 1.0$.\n    - Case $2$: $\\{g_i\\} = [\\,0.1,\\,0.1,\\,-0.1\\,]$, $\\{h_i\\} = [\\,10^{-12},\\,2\\times 10^{-12},\\,0.0\\,]$, $\\lambda = 0.0$.\n    - Case $3$: $\\{g_i\\} = [\\,0.1,\\,0.1,\\,-0.1\\,]$, $\\{h_i\\} = [\\,10^{-12},\\,2\\times 10^{-12},\\,0.0\\,]$, $\\lambda = 10^{-6}$.\n    - Case $4$: $\\{g_i\\} = [\\,-2.0,\\,1.0,\\,-0.5,\\,0.3\\,]$, $\\{h_i\\} = [\\,0.5,\\,0.4,\\,0.3,\\,0.2\\,]$, $\\lambda = 1000.0$.\n    - Case $5$: $\\{g_i\\} = [\\,1000.0,\\,-950.0,\\,25.0,\\,-10.0\\,]$, $\\{h_i\\} = [\\,0.01,\\,0.01,\\,0.01,\\,0.01\\,]$, $\\lambda = 1.0$.\n    - Case $6$: $\\{g_i\\} = [\\,0.0,\\,0.0,\\,0.0\\,]$, $\\{h_i\\} = [\\,0.2,\\,0.3,\\,0.1\\,]$, $\\lambda = 0.5$.\n    - Case $7$: $\\{g_i\\} = [\\,0.5,\\,-0.5\\,]$, $\\{h_i\\} = [\\,0.0,\\,0.0\\,]$, $\\lambda = 0.0$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation against the specified criteria.\n- **Scientifically Grounded**: The problem describes the core optimization step for determining leaf weights in gradient boosting machines, a standard and widely used machine learning algorithm. The use of a second-order Taylor approximation for the loss function, gradients ($g_i$), Hessians ($h_i$), and $\\ell_2$ regularization is the fundamental basis for algorithms like XGBoost. The application to high-energy physics is a common and appropriate use case for BDTs. The problem is scientifically and mathematically sound.\n- **Well-Posed**: The problem asks for the derivation of a minimizer for a quadratic function, which is a well-defined mathematical task. All necessary quantities ($g_i, h_i, \\lambda$) and conditions ($h_i \\ge 0, \\lambda \\ge 0$) are provided. The existence of a unique minimum is guaranteed as long as the coefficient of the quadratic term is positive. The numerical rule handles the case where it is zero or near-zero. Therefore, a unique and meaningful solution exists.\n- **Objective**: The problem is stated in precise, unambiguous mathematical terms. There is no subjective or opinion-based content.\n\nThe problem does not exhibit any of the listed invalidity flaws. It is self-contained, consistent, and scientifically grounded.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation and Explanation\n\nLet the set of events routed to a particular leaf be indexed by $i$. The total objective function, $J(v)$, for this leaf is the sum of the approximated losses for each event plus a regularization term for the leaf's output value, $v$. The loss for each event $i$ is approximated by a second-order Taylor expansion. The regularization term is the squared $\\ell_2$ norm of the leaf output, scaled by $\\lambda$. Conventionally, a factor of $1/2$ is included in both terms for mathematical convenience, simplifying the derivatives.\n\nThe objective function to be minimized with respect to $v$ is:\n$$\nJ(v) = \\sum_{i \\in \\text{leaf}} \\left( g_i v + \\frac{1}{2} h_i v^2 \\right) + \\frac{1}{2} \\lambda v^2\n$$\nHere, $g_i v + \\frac{1}{2} h_i v^2$ is the second-order Taylor approximation of the loss function contribution from event $i$, excluding constant terms which are irrelevant for the optimization of $v$. The term $\\frac{1}{2} \\lambda v^2$ is the $\\ell_2$ regularization penalty.\n\nTo find the optimal value of $v$ that minimizes $J(v)$, we can regroup the terms:\n$$\nJ(v) = \\left( \\sum_i g_i \\right) v + \\frac{1}{2} \\left( \\sum_i h_i + \\lambda \\right) v^2\n$$\nLet $S_g = \\sum_i g_i$ and $S_h = \\sum_i h_i$. The objective function simplifies to:\n$$\nJ(v) = S_g v + \\frac{1}{2} (S_h + \\lambda) v^2\n$$\nThis is a quadratic function of $v$. To find its extremum, we compute its first derivative with respect to $v$ and set it to zero:\n$$\n\\frac{dJ}{dv} = S_g + (S_h + \\lambda) v = 0\n$$\nSolving for $v$ yields the optimal leaf value:\n$$\nv (S_h + \\lambda) = -S_g \\implies v = - \\frac{S_g}{S_h + \\lambda}\n$$\nTo confirm this is a minimum, we examine the second derivative:\n$$\n\\frac{d^2J}{dv^2} = S_h + \\lambda = \\sum_i h_i + \\lambda\n$$\nThe problem states that the loss function is convex, so its second derivative is non-negative, i.e., $h_i \\ge 0$ for all $i$. The regularization strength is also non-negative, $\\lambda \\ge 0$. Therefore, their sums, $\\sum_i h_i$ and hence $\\sum_i h_i + \\lambda$, are also non-negative. If $\\sum_i h_i + \\lambda > 0$, the second derivative is positive, and the derived $v$ is a unique minimum. If $\\sum_i h_i + \\lambda = 0$, the objective function is linear in $v$ (or constant if $S_g$ is also $0$), and does not have a unique, finite minimum unless $S_g=0$.\n\n**Numerical Stability:**\nThe derived expression for $v$ involves a division by $S_h + \\lambda$. If this denominator is very close to zero, the value of $v$ can become extremely large, leading to numerical instability and large updates that cause the boosting process to diverge. This situation typically occurs when the region of the feature space defined by the leaf contains very few data points, or when the loss function has very little curvature ($h_i \\approx 0$) in that region, and no regularization is used ($\\lambda = 0$). The regularization parameter $\\lambda$ plays a crucial role in ensuring stability. By being a non-negative constant, it \"lifts\" the denominator away from zero, preventing division by a very small number. The problem's explicit numerical rule—setting $v = 0.0$ if $S_h + \\lambda \\le 10^{-12}$—is a direct, practical safeguard against this instability, effectively treating cases with insufficient curvature or regularization as having no update.\n\n**Overfitting Control:**\nOverfitting in BDTs occurs when the model adapts too closely to the training data, including its noise, leading to poor performance on unseen data. This often manifests as individual trees making very large contributions (large $|v|$) to correct misclassifications of a few training examples. The $\\ell_2$ regularization term $\\frac{1}{2} \\lambda v^2$ penalizes large leaf values. From the formula $v = -S_g / (S_h + \\lambda)$, it is evident that increasing the regularization strength $\\lambda$ increases the magnitude of the denominator. This, in turn, \"shrinks\" the optimal value of $v$ towards zero. This shrinkage reduces the influence of each individual tree, forcing the algorithm to rely on the consensus of a larger number of weaker learners. This process makes the model less sensitive to noise in the training set and improves its ability to generalize, thus controlling overfitting.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the BDT leaf value problem for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {'g': [0.8, -0.5, 0.1], 'h': [0.25, 0.2, 0.1], 'lambda': 1.0},\n        # Case 2\n        {'g': [0.1, 0.1, -0.1], 'h': [1e-12, 2e-12, 0.0], 'lambda': 0.0},\n        # Case 3\n        {'g': [0.1, 0.1, -0.1], 'h': [1e-12, 2e-12, 0.0], 'lambda': 1e-6},\n        # Case 4\n        {'g': [-2.0, 1.0, -0.5, 0.3], 'h': [0.5, 0.4, 0.3, 0.2], 'lambda': 1000.0},\n        # Case 5\n        {'g': [1000.0, -950.0, 25.0, -10.0], 'h': [0.01, 0.01, 0.01, 0.01], 'lambda': 1.0},\n        # Case 6\n        {'g': [0.0, 0.0, 0.0], 'h': [0.2, 0.3, 0.1], 'lambda': 0.5},\n        # Case 7\n        {'g': [0.5, -0.5], 'h': [0.0, 0.0], 'lambda': 0.0},\n    ]\n\n    def calculate_leaf_output(g_list, h_list, lam):\n        \"\"\"\n        Calculates the regularized leaf output value v.\n\n        Args:\n            g_list: A list of gradient values {g_i}.\n            h_list: A list of Hessian values {h_i}.\n            lam: The L2 regularization strength lambda.\n\n        Returns:\n            The calculated leaf output value v as a float.\n        \"\"\"\n        # Epsilon for the numerical stability check, as defined in the problem.\n        epsilon = 1e-12\n        \n        # Calculate the sum of gradients (S_g) and Hessians (S_h).\n        # Convert lists to numpy arrays for efficient summation.\n        S_g = np.sum(np.array(g_list))\n        S_h = np.sum(np.array(h_list))\n        \n        denominator = S_h + lam\n        \n        # Apply the numerical robustness rule.\n        if denominator = epsilon:\n            return 0.0\n        else:\n            # Calculate the unique minimizer for the leaf value v.\n            v = -S_g / denominator\n            return v\n\n    results = []\n    for case in test_cases:\n        g = case['g']\n        h = case['h']\n        lam = case['lambda']\n        result = calculate_leaf_output(g, h, lam)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3506549"}, {"introduction": "Training BDTs on the massive datasets common in high-energy physics demands computationally efficient algorithms. This hands-on exercise explores a key optimization: the use of histogram-based methods to approximate optimal split points by discretizing continuous features into bins. Calculating the expected quantization error [@problem_id:3506484] will illuminate the trade-off between computational speed and split precision, helping you appreciate how design choices like bin width ($\\Delta$) impact the BDT's learning process.", "problem": "In computational high-energy physics, classification tasks such as separating signal from background often rely on ensemble methods like Boosted Decision Trees (BDTs). To reduce computational cost in large datasets, many implementations use histogram-based approximate splits. In this approach, each continuous feature is discretized into a finite set of bins, and candidate split thresholds are restricted to bin boundaries rather than arbitrary continuous values. Consider a single feature variable $x$ with a range $[x_{\\min}, x_{\\max}]$, discretized into $M$ uniform-width bins, each of width $\\Delta = \\frac{x_{\\max} - x_{\\min}}{M}$. During training, suppose the exact impurity-optimal split threshold, denoted $t^{*}$, lies within some bin of width $\\Delta$, but the histogram-based approximate split restricts the threshold to the nearest bin boundary, which we denote $t_{q}$.\n\nAssume the following scenario to model the quantization error introduced by histogram-based splits:\n\n- Within the bin that contains $t^{*}$, the position of $t^{*}$ relative to the left boundary of that bin is uniformly distributed over the interval $[0, \\Delta]$, due to locally smooth impurity gain around the optimum.\n- The histogram-based approximate split selects $t_{q}$ to be the nearest bin boundary to $t^{*}$.\n\nDefine the quantization error of the split threshold as the squared deviation $(t_{q} - t^{*})^{2}$ between the approximate split $t_{q}$ and the exact split $t^{*}$. Starting from these assumptions, and using basic probabilistic reasoning and properties of uniform random variables, derive the expected quantization error $\\mathbb{E}\\!\\left[(t_{q} - t^{*})^{2}\\right]$ as a function of $\\Delta$ for uniform binning.\n\nExpress your final answer as a single simplified closed-form expression in terms of $\\Delta$. No units are required because $\\Delta$ is a feature-space measure. Do not provide an inequality or an equation as the final answer; provide only the expression. No numerical rounding is required.", "solution": "We are asked to derive the expected quantization error, $\\mathbb{E}[(t_q - t^*)^2]$, where $t^*$ is the exact optimal split threshold and $t_q$ is its histogram-based approximation.\n\nLet's formalize the setup based on the problem's assumptions:\n1.  The feature is discretized into bins of uniform width $\\Delta$.\n2.  The exact optimum $t^*$ falls within a specific bin. Let's denote the boundaries of this bin as $[b, b+\\Delta]$.\n3.  The position of $t^*$ within this bin is modeled as a random variable. Let $X = t^* - b$ be the offset from the left boundary. The problem states that $X$ is uniformly distributed over $[0, \\Delta]$. Thus, $X \\sim U(0, \\Delta)$, and its probability density function is $f_X(x) = 1/\\Delta$ for $x \\in [0, \\Delta]$, and $0$ otherwise.\n4.  The approximate threshold $t_q$ is the bin boundary nearest to $t^*$. This means:\n    - If $t^*$ is in the first half of the bin, i.e., $t^* \\in [b, b+\\Delta/2]$, then $t_q=b$. This corresponds to $X \\in [0, \\Delta/2]$.\n    - If $t^*$ is in the second half of the bin, i.e., $t^* \\in (b+\\Delta/2, b+\\Delta]$, then $t_q=b+\\Delta$. This corresponds to $X \\in (\\Delta/2, \\Delta]$.\n\nThe quantization error is defined as $(t_q - t^*)^2$. We can express this error in terms of the random variable $X$:\n- For $X \\in [0, \\Delta/2]$, the error is $(b - (b+X))^2 = (-X)^2 = X^2$.\n- For $X \\in (\\Delta/2, \\Delta]$, the error is $((b+\\Delta) - (b+X))^2 = (\\Delta-X)^2$.\n\nThe expected quantization error is the expectation of this squared error over the distribution of $X$:\n$$\n\\mathbb{E}[(t_q - t^*)^2] = \\int_{0}^{\\Delta} (t_q(x) - (b+x))^2 f_X(x) \\,dx\n$$\nWe split the integral into two parts according to the definition of $t_q$:\n$$\n\\mathbb{E}[(t_q - t^*)^2] = \\int_{0}^{\\Delta/2} x^2 \\left(\\frac{1}{\\Delta}\\right) \\,dx + \\int_{\\Delta/2}^{\\Delta} (\\Delta-x)^2 \\left(\\frac{1}{\\Delta}\\right) \\,dx\n$$\nNow, we evaluate the integrals:\n$$\n\\int_{0}^{\\Delta/2} x^2 \\,dx = \\left[ \\frac{x^3}{3} \\right]_{0}^{\\Delta/2} = \\frac{(\\Delta/2)^3}{3} - 0 = \\frac{\\Delta^3}{24}\n$$\nFor the second integral, let $u = \\Delta - x$. Then $du = -dx$. When $x=\\Delta/2$, $u=\\Delta/2$. When $x=\\Delta$, $u=0$.\n$$\n\\int_{\\Delta/2}^{\\Delta} (\\Delta-x)^2 \\,dx = \\int_{\\Delta/2}^{0} u^2 (-du) = \\int_{0}^{\\Delta/2} u^2 \\,du = \\frac{\\Delta^3}{24}\n$$\nCombining the results:\n$$\n\\mathbb{E}[(t_q - t^*)^2] = \\frac{1}{\\Delta} \\left( \\frac{\\Delta^3}{24} + \\frac{\\Delta^3}{24} \\right) = \\frac{1}{\\Delta} \\left( \\frac{2\\Delta^3}{24} \\right) = \\frac{1}{\\Delta} \\left( \\frac{\\Delta^3}{12} \\right) = \\frac{\\Delta^2}{12}\n$$\nThus, the expected quantization error is $\\frac{\\Delta^2}{12}$.", "answer": "$$\n\\boxed{\\frac{\\Delta^{2}}{12}}\n$$", "id": "3506484"}, {"introduction": "A trained BDT classifier produces a continuous output score, but a physics analysis requires a discrete decision boundary to select candidate signal events. This practice bridges the gap between the classifier's output and its practical application by tasking you to find an optimal decision threshold, $t$. By maximizing a physics-relevant performance metric like the weighted $F_1$ score [@problem_id:3506489], you will learn to translate abstract model outputs into concrete, optimized decisions that directly impact the sensitivity of a scientific analysis.", "problem": "A classification analysis in computational high-energy physics uses a Boosted Decision Tree (BDT) whose output is calibrated to represent the posterior probability of the signal class, denoted by $s \\in [0,1]$. Consider a test sample of proton-proton collision events with two classes: signal and background. Each event carries a physical weight derived from the product of cross section, integrated luminosity, and selection efficiency. The total weighted yields for signal and background in the test sample are given by $W_{S}$ and $W_{B}$, respectively. The BDT output $s$ for signal events is modeled by the density $f_{S}(s) = 2s$ on $[0,1]$, and for background events by $f_{B}(s) = 2(1-s)$ on $[0,1]$, consistent with a calibrated classifier that is more likely to produce larger $s$ for signal and smaller $s$ for background.\n\nUse the following physically motivated parameters to compute the total weighted yields:\n- Integrated luminosity $L = 100\\,\\mathrm{fb}^{-1}$.\n- Signal cross section $\\sigma_{S} = 0.5\\,\\mathrm{pb}$ and selection efficiency $\\epsilon_{S} = 0.4$.\n- Background cross section $\\sigma_{B} = 2.0\\,\\mathrm{pb}$ and selection efficiency $\\epsilon_{B} = 0.3$.\n\nAssume $1\\,\\mathrm{pb} = 10^{-12}\\,\\mathrm{b}$ and $1\\,\\mathrm{fb}^{-1} = 10^{15}\\,\\mathrm{b}^{-1}$, so that $W_{S} = \\sigma_{S} L \\epsilon_{S}$ and $W_{B} = \\sigma_{B} L \\epsilon_{B}$ are dimensionless expected event yields.\n\nDefine the weighted true positives $\\mathrm{TP}(t)$, false positives $\\mathrm{FP}(t)$, and false negatives $\\mathrm{FN}(t)$ for a threshold $t \\in [0,1]$ by classifying an event as signal if $s \\geq t$, otherwise background. The weighted $F_{1}$ score is given by $F_{1}(t) = \\dfrac{2\\,\\mathrm{TP}(t)}{2\\,\\mathrm{TP}(t) + \\mathrm{FP}(t) + \\mathrm{FN}(t)}$. Starting from these definitions and the given densities, derive the threshold $t^{\\star}$ that maximizes $F_{1}(t)$ and compute its numerical value using the provided parameters. Express your final threshold as a decimal number rounded to four significant figures. The threshold is dimensionless.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It provides a complete and consistent setup for a standard optimization problem in the context of machine learning for high-energy physics.\n\nThe objective is to find the threshold $t^{\\star}$ that maximizes the weighted $F_{1}$ score, defined as $F_{1}(t) = \\dfrac{2\\,\\mathrm{TP}(t)}{2\\,\\mathrm{TP}(t) + \\mathrm{FP}(t) + \\mathrm{FN}(t)}$, where $\\mathrm{TP}(t)$, $\\mathrm{FP}(t)$, and $\\mathrm{FN}(t)$ are the weighted true positives, false positives, and false negatives for a classification threshold $t \\in [0,1]$.\n\nFirst, we calculate the total weighted yields for signal ($W_S$) and background ($W_B$) using the provided parameters:\nIntegrated luminosity $L = 100\\,\\mathrm{fb}^{-1}$\nSignal cross section $\\sigma_{S} = 0.5\\,\\mathrm{pb}$\nSignal selection efficiency $\\epsilon_{S} = 0.4$\nBackground cross section $\\sigma_{B} = 2.0\\,\\mathrm{pb}$\nBackground selection efficiency $\\epsilon_{B} = 0.3$\n\nThe yields are given by the formula $W = \\sigma L \\epsilon$. Using the provided conversion factors $1\\,\\mathrm{pb} = 10^{-12}\\,\\mathrm{b}$ and $1\\,\\mathrm{fb}^{-1} = 10^{15}\\,\\mathrm{b}^{-1}$:\n$$W_{S} = \\sigma_{S} L \\epsilon_{S} = (0.5 \\times 10^{-12}\\,\\mathrm{b}) \\times (100 \\times 10^{15}\\,\\mathrm{b}^{-1}) \\times 0.4 = 20000$$\n$$W_{B} = \\sigma_{B} L \\epsilon_{B} = (2.0 \\times 10^{-12}\\,\\mathrm{b}) \\times (100 \\times 10^{15}\\,\\mathrm{b}^{-1}) \\times 0.3 = 60000$$\n\nNext, we express $\\mathrm{TP}(t)$, $\\mathrm{FP}(t)$, and $\\mathrm{FN}(t)$ as functions of the threshold $t$. An event is classified as signal if its BDT score $s$ satisfies $s \\geq t$.\n\nThe number of weighted true positives, $\\mathrm{TP}(t)$, is the weighted number of signal events classified as signal. This is the total signal yield $W_S$ multiplied by the probability of a signal event having $s \\geq t$:\n$$\\mathrm{TP}(t) = W_{S} \\int_{t}^{1} f_{S}(s) \\,ds = W_{S} \\int_{t}^{1} 2s \\,ds = W_{S} [s^2]_{t}^{1} = W_{S}(1 - t^2)$$\n\nThe number of weighted false positives, $\\mathrm{FP}(t)$, is the weighted number of background events classified as signal. This is the total background yield $W_B$ multiplied by the probability of a background event having $s \\geq t$:\n$$\\mathrm{FP}(t) = W_{B} \\int_{t}^{1} f_{B}(s) \\,ds = W_{B} \\int_{t}^{1} 2(1-s) \\,ds = W_{B} [2s - s^2]_{t}^{1} = W_{B}((2-1) - (2t-t^2)) = W_{B}(1 - 2t + t^2) = W_{B}(1-t)^2$$\n\nThe number of weighted false negatives, $\\mathrm{FN}(t)$, is the weighted number of signal events classified as background ($s  t$). This can be calculated as the total signal yield minus the true positives:\n$$\\mathrm{FN}(t) = W_{S} - \\mathrm{TP}(t) = W_{S} - W_{S}(1-t^2) = W_{S}t^2$$\n\nNow we construct the $F_1(t)$ function:\n$$F_{1}(t) = \\frac{2\\,\\mathrm{TP}(t)}{2\\,\\mathrm{TP}(t) + \\mathrm{FP}(t) + \\mathrm{FN}(t)} = \\frac{2 W_{S}(1-t^2)}{2 W_{S}(1-t^2) + W_{B}(1-t)^2 + W_{S}t^2}$$\nTo find the maximum, we set the derivative $\\frac{dF_1(t)}{dt}$ to $0$. Let $N(t) = 2 W_{S}(1-t^2)$ be the numerator and $D(t) = 2 W_{S}(1-t^2) + W_{B}(1-t)^2 + W_{S}t^2$ be the denominator. The derivative is zero when $N'(t)D(t) - N(t)D'(t) = 0$.\nThe denominator can be simplified:\n$$D(t) = 2W_{S} - 2W_{S}t^2 + W_{B}(1-2t+t^2) + W_{S}t^2 = (W_{B} - W_{S})t^2 - 2W_{B}t + (2W_{S} + W_{B})$$\nThe derivatives are:\n$$N'(t) = -4W_{S}t$$\n$$D'(t) = 2(W_{B} - W_{S})t - 2W_{B}$$\nSetting the numerator of the quotient rule derivative to zero:\n$$(-4W_{S}t)[(W_{B} - W_{S})t^2 - 2W_{B}t + (2W_{S} + W_{B})] - [2W_{S}(1-t^2)][2(W_{B} - W_{S})t - 2W_{B}] = 0$$\nAssuming $W_S > 0$ and $t \\neq 1$, we can divide by $-4W_S$:\n$$t[(W_{B} - W_{S})t^2 - 2W_{B}t + (2W_{S} + W_{B})] + (1-t^2)[(W_{B}-W_{S})t - W_B] = 0$$\nExpanding and collecting terms:\n$$(W_{B}-W_{S})t^3 - 2W_{B}t^2 + (2W_{S}+W_{B})t + (W_{B}-W_{S})t - W_B - (W_{B}-W_{S})t^3 + W_{B}t^2 = 0$$\nThe cubic terms cancel out. We are left with a quadratic equation:\n$$(-2W_{B}+W_{B})t^2 + (2W_{S}+W_{B} + W_{B}-W_{S})t - W_B = 0$$\n$$-W_{B}t^2 + (W_{S} + 2W_{B})t - W_{B} = 0$$\n$$W_{B}t^2 - (W_{S} + 2W_{B})t + W_{B} = 0$$\nWe solve this for $t$ using the quadratic formula:\n$$t = \\frac{(W_{S} + 2W_{B}) \\pm \\sqrt{(-(W_{S} + 2W_{B}))^2 - 4(W_{B})(W_{B})}}{2W_{B}}$$\n$$t = \\frac{W_{S} + 2W_{B} \\pm \\sqrt{W_{S}^2 + 4W_{S}W_{B} + 4W_{B}^2 - 4W_{B}^2}}{2W_{B}}$$\n$$t = \\frac{W_{S} + 2W_{B} \\pm \\sqrt{W_{S}(W_{S} + 4W_{B})}}{2W_{B}}$$\nThe solution must be in the interval $[0,1]$.\nConsider the solution with the plus sign:\n$$t_{+} = \\frac{W_{S} + 2W_{B} + \\sqrt{W_{S}^2 + 4W_{S}W_{B}}}{2W_{B}} = \\frac{W_{S}}{2W_{B}} + 1 + \\frac{\\sqrt{W_{S}^2 + 4W_{S}W_{B}}}{2W_{B}}$$\nSince $W_S > 0$ and $W_B > 0$, $t_{+} > 1$. This solution is not physically valid.\nConsider the solution with the minus sign:\n$$t^{\\star} = t_{-} = \\frac{W_{S} + 2W_{B} - \\sqrt{W_{S}(W_{S} + 4W_{B})}}{2W_{B}}$$\nTo verify $t^{\\star} \\in [0,1]$, we observe that $\\sqrt{W_{S}^2 + 4W_{S}W_{B}}  \\sqrt{W_{S}^2 + 4W_{S}W_{B} + 4W_{B}^2} = W_{S} + 2W_{B}$, so the numerator is positive, and $t^{\\star} > 0$. Also, $\\sqrt{W_{S}^2 + 4W_{S}W_{B}} > \\sqrt{W_{S}^2} = W_{S}$, so $W_{S} - \\sqrt{W_{S}(W_{S} + 4W_{B})}  0$, which implies $W_{S} + 2W_{B} - \\sqrt{W_{S}(W_{S} + 4W_{B})}  2W_{B}$. Thus, $t^{\\star}  1$. This is the correct physical solution.\n\nNow, we substitute the numerical values $W_{S}=20000$ and $W_{B}=60000$:\n$$t^{\\star} = \\frac{20000 + 2(60000) - \\sqrt{20000(20000 + 4 \\cdot 60000)}}{2(60000)}$$\n$$t^{\\star} = \\frac{140000 - \\sqrt{20000(260000)}}{120000} = \\frac{140000 - \\sqrt{5.2 \\times 10^9}}{120000}$$\n$$t^{\\star} = \\frac{140000 - 10^4\\sqrt{52}}{120000} = \\frac{14 - 2\\sqrt{13}}{12} = \\frac{7 - \\sqrt{13}}{6}$$\nComputing the final numerical value:\n$$t^{\\star} \\approx \\frac{7 - 3.605551275}{6} \\approx \\frac{3.394448725}{6} \\approx 0.56574145$$\nRounding to four significant figures, we get $0.5657$.", "answer": "$$\\boxed{0.5657}$$", "id": "3506489"}]}