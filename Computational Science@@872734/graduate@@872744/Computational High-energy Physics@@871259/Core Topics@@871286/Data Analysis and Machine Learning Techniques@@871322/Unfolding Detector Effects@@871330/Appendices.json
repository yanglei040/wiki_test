{"hands_on_practices": [{"introduction": "Before we can correct for detector effects, we must first build a quantitative model of the detector's response. This exercise provides foundational practice in constructing the essential tool for this task, the response matrix, directly from raw Monte Carlo simulation outputs. By computing the acceptance matrix $A_{ji}$, truth-bin efficiencies $\\epsilon_i$, and the acceptance-corrected response $\\tilde{R}_{ji}$, you will build a complete forward model that characterizes how a true particle-level distribution is distorted into a measured one [@problem_id:3540787].", "problem": "A detector is simulated with Monte Carlo (MC) for a process whose particle-level (truth) phase space is partitioned into three bins indexed by $i \\in \\{1,2,3\\}$, and whose reconstructed (reco) phase space is partitioned into four bins indexed by $j \\in \\{1,2,3,4\\}$. Let $N_{i}^{\\text{truth}}$ denote the total number of MC truth events generated in truth bin $i$, and let $N_{ji}$ denote the number of those truth-bin-$i$ events that are reconstructed in reco bin $j$ by the detector simulation. The MC yields are:\n- Truth counts: $N_{1}^{\\text{truth}}=1000$, $N_{2}^{\\text{truth}}=1200$, $N_{3}^{\\text{truth}}=800$.\n- Migration counts (reco given truth): for $i=1$, $(N_{1,1},N_{2,1},N_{3,1},N_{4,1})=(320,280,100,50)$; for $i=2$, $(N_{1,2},N_{2,2},N_{3,2},N_{4,2})=(150,420,360,90)$; for $i=3$, $(N_{1,3},N_{2,3},N_{3,3},N_{4,3})=(40,160,280,120)$.\n\nStarting from the frequentist definitions of probability and conditional probability, and treating the MC as a large-sample estimator of those probabilities, derive and compute:\n1. The acceptance (also called response with inefficiency) matrix $A_{ji}$, defined conceptually as the conditional probability to be reconstructed in reco bin $j$ given the event was in truth bin $i$, estimated from the MC counts.\n2. The efficiency $\\epsilon_{i}$ for each truth bin $i$, defined conceptually as the total probability that an event in truth bin $i$ is reconstructed into any of the four reco bins.\n3. The acceptance-corrected response $\\tilde{R}_{ji}$, defined conceptually as the conditional probability to be in reco bin $j$ given the event was in truth bin $i$ and that it was reconstructed (i.e., conditioned on acceptance), obtained by normalizing each truth column of $A_{ji}$ by its corresponding $\\epsilon_{i}$.\n\nAssume all estimates are exact ratios of counts without any further regularization or smoothing. Express all probabilities as exact rational numbers, without rounding. Report your final answer as a single row vector using the $\\mathrm{pmatrix}$ environment, listing entries in the following order:\n- First the matrix $A_{ji}$ column-wise by truth bin $i=1,2,3$, and within each column by $j=1,2,3,4$;\n- Then the efficiencies $(\\epsilon_{1},\\epsilon_{2},\\epsilon_{3})$;\n- Then the acceptance-corrected response matrix $\\tilde{R}_{ji}$ in the same column-wise order.", "solution": "The problem statement has been validated and is deemed to be self-contained, scientifically grounded, and well-posed. It presents a standard computational task in high-energy physics related to detector simulation and unfolding. All necessary data and definitions are provided, and there are no internal contradictions. We may therefore proceed with the solution.\n\nThe problem asks for the computation of three related quantities describing detector effects: the acceptance matrix $A_{ji}$, the truth-bin efficiencies $\\epsilon_i$, and the acceptance-corrected response matrix $\\tilde{R}_{ji}$. We will derive and compute each in turn, based on the frequentist interpretation of probability applied to the provided Monte Carlo (MC) event counts.\n\nLet $T_i$ be the event that a particle is generated in the truth bin $i$, and let $R_j$ be the event that it is reconstructed in the reco bin $j$. The given counts are:\n- Total number of events generated in truth bin $i$: $N(\\text{T}_i) = N_{i}^{\\text{truth}}$.\n- Number of events generated in truth bin $i$ and reconstructed in reco bin $j$: $N(\\text{R}_j \\cap \\text{T}_i) = N_{ji}$.\n\nThe provided numerical values are:\n- $N_{1}^{\\text{truth}} = 1000$, $N_{2}^{\\text{truth}} = 1200$, $N_{3}^{\\text{truth}} = 800$.\n- For $i=1$: $(N_{1,1}, N_{2,1}, N_{3,1}, N_{4,1}) = (320, 280, 100, 50)$.\n- For $i=2$: $(N_{1,2}, N_{2,2}, N_{3,2}, N_{4,2}) = (150, 420, 360, 90)$.\n- For $i=3$: $(N_{1,3}, N_{2,3}, N_{3,3}, N_{4,3}) = (40, 160, 280, 120)$.\n\nAll probabilities are estimated as ratios of counts.\n\n**1. The Acceptance Matrix $A_{ji}$**\n\nThe acceptance matrix $A_{ji}$ is defined as the conditional probability of an event being reconstructed in reco bin $j$, given that it originated in truth bin $i$. This is denoted $P(\\text{R}_j | \\text{T}_i)$. Using the frequentist definition of conditional probability, we estimate this as:\n$$\nA_{ji} = P(\\text{R}_j | \\text{T}_i) = \\frac{N(\\text{R}_j \\cap \\text{T}_i)}{N(\\text{T}_i)} = \\frac{N_{ji}}{N_{i}^{\\text{truth}}}\n$$\nWe compute the $4 \\times 3$ matrix elements column by column for each truth bin $i \\in \\{1,2,3\\}$.\n\nFor truth bin $i=1$ ($N_{1}^{\\text{truth}}=1000$):\n$A_{11} = \\frac{N_{11}}{N_{1}^{\\text{truth}}} = \\frac{320}{1000} = \\frac{8}{25}$\n$A_{21} = \\frac{N_{21}}{N_{1}^{\\text{truth}}} = \\frac{280}{1000} = \\frac{7}{25}$\n$A_{31} = \\frac{N_{31}}{N_{1}^{\\text{truth}}} = \\frac{100}{1000} = \\frac{1}{10}$\n$A_{41} = \\frac{N_{41}}{N_{1}^{\\text{truth}}} = \\frac{50}{1000} = \\frac{1}{20}$\n\nFor truth bin $i=2$ ($N_{2}^{\\text{truth}}=1200$):\n$A_{12} = \\frac{N_{12}}{N_{2}^{\\text{truth}}} = \\frac{150}{1200} = \\frac{1}{8}$\n$A_{22} = \\frac{N_{22}}{N_{2}^{\\text{truth}}} = \\frac{420}{1200} = \\frac{7}{20}$\n$A_{32} = \\frac{N_{32}}{N_{2}^{\\text{truth}}} = \\frac{360}{1200} = \\frac{3}{10}$\n$A_{42} = \\frac{N_{42}}{N_{2}^{\\text{truth}}} = \\frac{90}{1200} = \\frac{3}{40}$\n\nFor truth bin $i=3$ ($N_{3}^{\\text{truth}}=800$):\n$A_{13} = \\frac{N_{13}}{N_{3}^{\\text{truth}}} = \\frac{40}{800} = \\frac{1}{20}$\n$A_{23} = \\frac{N_{23}}{N_{3}^{\\text{truth}}} = \\frac{160}{800} = \\frac{1}{5}$\n$A_{33} = \\frac{N_{33}}{N_{3}^{\\text{truth}}} = \\frac{280}{800} = \\frac{7}{20}$\n$A_{43} = \\frac{N_{43}}{N_{3}^{\\text{truth}}} = \\frac{120}{800} = \\frac{3}{20}$\n\n**2. The Efficiency $\\epsilon_i$**\n\nThe efficiency $\\epsilon_i$ for a truth bin $i$ is the total probability that an event from that bin is reconstructed in *any* of the reco bins. This is the sum of the conditional probabilities over all possible reconstruction outcomes $j$.\n$$\n\\epsilon_i = P(\\text{reconstructed} | \\text{T}_i) = \\sum_{j=1}^{4} P(\\text{R}_j | \\text{T}_i) = \\sum_{j=1}^{4} A_{ji}\n$$\nEquivalently, it is the total number of reconstructed events originating from truth bin $i$ divided by the total number of events generated in truth bin $i$.\n$$\n\\epsilon_i = \\frac{\\sum_{j=1}^{4} N_{ji}}{N_{i}^{\\text{truth}}}\n$$\nWe compute $\\epsilon_i$ for each truth bin $i \\in \\{1,2,3\\}$.\n\nFor truth bin $i=1$:\n$\\epsilon_1 = \\frac{320+280+100+50}{1000} = \\frac{750}{1000} = \\frac{3}{4}$\n\nFor truth bin $i=2$:\n$\\epsilon_2 = \\frac{150+420+360+90}{1200} = \\frac{1020}{1200} = \\frac{17}{20}$\n\nFor truth bin $i=3$:\n$\\epsilon_3 = \\frac{40+160+280+120}{800} = \\frac{600}{800} = \\frac{3}{4}$\n\n**3. The Acceptance-Corrected Response $\\tilde{R}_{ji}$**\n\nThe acceptance-corrected response $\\tilde{R}_{ji}$ is the conditional probability that an event is reconstructed in reco bin $j$, given that it originated in truth bin $i$ *and* that it was reconstructed. Let 'Reco'd' be the event that a particle is reconstructed in any of the bins. Then $\\tilde{R}_{ji} = P(\\text{R}_j | \\text{T}_i \\cap \\text{Reco'd})$.\nBy the definition of conditional probability:\n$$\n\\tilde{R}_{ji} = \\frac{P(\\text{R}_j \\cap (\\text{T}_i \\cap \\text{Reco'd}))}{P(\\text{T}_i \\cap \\text{Reco'd})}\n$$\nSince the event $\\text{R}_j$ implies the event 'Reco'd', the intersection $\\text{R}_j \\cap \\text{Reco'd}$ is simply $\\text{R}_j$. Thus, the numerator becomes $P(\\text{R}_j \\cap \\text{T}_i)$. The expression simplifies to:\n$$\n\\tilde{R}_{ji} = \\frac{P(\\text{R}_j \\cap \\text{T}_i)}{P(\\text{Reco'd} \\cap \\text{T}_i)}\n$$\nDividing the numerator and denominator by $P(\\text{T}_i)$ yields:\n$$\n\\tilde{R}_{ji} = \\frac{P(\\text{R}_j \\cap \\text{T}_i) / P(\\text{T}_i)}{P(\\text{Reco'd} \\cap \\text{T}_i) / P(\\text{T}_i)} = \\frac{P(\\text{R}_j | \\text{T}_i)}{P(\\text{Reco'd} | \\text{T}_i)} = \\frac{A_{ji}}{\\epsilon_i}\n$$\nThis confirms the prescription of normalizing each column of $A_{ji}$ by the corresponding efficiency $\\epsilon_i$. For each truth bin $i$, the column $\\tilde{R}_{ji}$ is a properly normalized probability distribution, i.e., $\\sum_{j=1}^{4} \\tilde{R}_{ji} = 1$.\n\nFor truth bin $i=1$ ($\\epsilon_1 = 3/4$):\n$\\tilde{R}_{11} = \\frac{A_{11}}{\\epsilon_1} = \\frac{8/25}{3/4} = \\frac{32}{75}$\n$\\tilde{R}_{21} = \\frac{A_{21}}{\\epsilon_1} = \\frac{7/25}{3/4} = \\frac{28}{75}$\n$\\tilde{R}_{31} = \\frac{A_{31}}{\\epsilon_1} = \\frac{1/10}{3/4} = \\frac{4}{30} = \\frac{2}{15}$\n$\\tilde{R}_{41} = \\frac{A_{41}}{\\epsilon_1} = \\frac{1/20}{3/4} = \\frac{4}{60} = \\frac{1}{15}$\n\nFor truth bin $i=2$ ($\\epsilon_2 = 17/20$):\n$\\tilde{R}_{12} = \\frac{A_{12}}{\\epsilon_2} = \\frac{1/8}{17/20} = \\frac{20}{136} = \\frac{5}{34}$\n$\\tilde{R}_{22} = \\frac{A_{22}}{\\epsilon_2} = \\frac{7/20}{17/20} = \\frac{7}{17}$\n$\\tilde{R}_{32} = \\frac{A_{32}}{\\epsilon_2} = \\frac{3/10}{17/20} = \\frac{60}{170} = \\frac{6}{17}$\n$\\tilde{R}_{42} = \\frac{A_{42}}{\\epsilon_2} = \\frac{3/40}{17/20} = \\frac{60}{680} = \\frac{3}{34}$\n\nFor truth bin $i=3$ ($\\epsilon_3 = 3/4$):\n$\\tilde{R}_{13} = \\frac{A_{13}}{\\epsilon_3} = \\frac{1/20}{3/4} = \\frac{4}{60} = \\frac{1}{15}$\n$\\tilde{R}_{23} = \\frac{A_{23}}{\\epsilon_3} = \\frac{1/5}{3/4} = \\frac{4}{15}$\n$\\tilde{R}_{33} = \\frac{A_{33}}{\\epsilon_3} = \\frac{7/20}{3/4} = \\frac{28}{60} = \\frac{7}{15}$\n$\\tilde{R}_{43} = \\frac{A_{43}}{\\epsilon_3} = \\frac{3/20}{3/4} = \\frac{12}{60} = \\frac{1}{5} = \\frac{3}{15}$\n\nCombining all results into a single row vector as specified completes the task.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{8}{25} & \\frac{7}{25} & \\frac{1}{10} & \\frac{1}{20} & \\frac{1}{8} & \\frac{7}{20} & \\frac{3}{10} & \\frac{3}{40} & \\frac{1}{20} & \\frac{1}{5} & \\frac{7}{20} & \\frac{3}{20} & \\frac{3}{4} & \\frac{17}{20} & \\frac{3}{4} & \\frac{32}{75} & \\frac{28}{75} & \\frac{2}{15} & \\frac{1}{15} & \\frac{5}{34} & \\frac{7}{17} & \\frac{6}{17} & \\frac{3}{34} & \\frac{1}{15} & \\frac{4}{15} & \\frac{7}{15} & \\frac{3}{15}\n\\end{pmatrix}\n}\n$$", "id": "3540787"}, {"introduction": "With a response matrix defined, unfolding becomes an inverse problem, which is often ill-posed and requires regularization to yield stable, physical results. This coding practice challenges you to compare different regularization strategiesâ€”specifically Tikhonov regularization, which penalizes the $\\ell_2$ norm of the solution's derivatives, and Total Variation regularization, which uses an $\\ell_1$ norm. By implementing and applying these methods to a spectrum with a sharp discontinuity, you will gain practical insight into their distinct impacts on smoothness and edge preservation, a critical skill for choosing the right tool for a given analysis [@problem_id:3540846].", "problem": "You are given a one-dimensional unfolding problem that models detector smearing and a sharp efficiency drop at a threshold. The fundamental base is the linear forward model that connects the unknown true spectrum to the measured spectrum. Let $N$ denote the number of bins, let $\\mathbf{t} \\in \\mathbb{R}^{N}$ denote the unknown true bin contents, and let $\\mathbf{m} \\in \\mathbb{R}^{N}$ denote the measured bin contents. The detector response is modeled by a matrix $\\mathbf{A} \\in \\mathbb{R}^{N \\times N}$, such that the forward model is\n$$\n\\mathbf{m} = \\mathbf{A} \\mathbf{t}.\n$$\nThe response matrix incorporates a bin-wise efficiency $\\varepsilon_i$ and resolution smearing. The smearing is modeled by a Gaussian kernel in bin space with width $\\sigma$, and the efficiency applies multiplicatively per true bin. For each true bin index $i \\in \\{0,\\dots,N-1\\}$, the column $i$ of $\\mathbf{A}$ is given by\n$$\nA_{j i} = \\varepsilon_i \\, \\frac{\\exp\\left( -\\frac{(j-i)^2}{2 \\sigma^2} \\right)}{\\sum\\limits_{k=0}^{N-1} \\exp\\left( -\\frac{(k-i)^2}{2 \\sigma^2} \\right)}, \\quad j \\in \\{0,\\dots,N-1\\}.\n$$\nThe efficiency model contains a sharp drop at a threshold bin index $\\theta$. Specifically,\n$$\n\\varepsilon_i = \\begin{cases}\n1, & i < \\theta, \\\\\n\\varepsilon_{\\mathrm{drop}}, & i \\ge \\theta,\n\\end{cases}\n$$\nwhere $0 < \\varepsilon_{\\mathrm{drop}} < 1$.\n\nYour task is to construct the response matrix based on the above definitions, synthesize a measured spectrum from a known true spectrum, and perform unfolding by three methods: unregularized least squares, Tikhonov regularization with a first-difference quadratic penalty, and Total Variation (TV) regularization with a first-difference $\\ell_1$ penalty. The unfolded spectrum estimates are denoted $\\widehat{\\mathbf{t}}_{\\mathrm{LS}}$, $\\widehat{\\mathbf{t}}_{\\mathrm{Tik}}$, and $\\widehat{\\mathbf{t}}_{\\mathrm{TV}}$, respectively.\n\n1. Forward model and true spectrum:\n   - Use $N = 50$ bins.\n   - Use the true spectrum $\\mathbf{t}$ with components $t_i = 1$ for all $i \\in \\{0,\\dots,N-1\\}$.\n   - Construct $\\mathbf{A}$ using the Gaussian smearing width $\\sigma$ and the efficiency model with threshold $\\theta$ and drop $\\varepsilon_{\\mathrm{drop}}$ as specified in the test suite.\n\n2. Unfolding methods:\n   - Unregularized least squares: solve\n     $$\n     \\widehat{\\mathbf{t}}_{\\mathrm{LS}} = \\operatorname*{arg\\,min}_{\\mathbf{x} \\in \\mathbb{R}^N} \\frac{1}{2} \\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{m} \\rVert_2^2.\n     $$\n   - Tikhonov regularization (first-difference quadratic): let $\\mathbf{D} \\in \\mathbb{R}^{(N-1)\\times N}$ be the first-difference operator defined by $(\\mathbf{D}\\mathbf{x})_i = x_{i+1} - x_i$. For a given regularization strength $\\alpha > 0$, solve\n     $$\n     \\widehat{\\mathbf{t}}_{\\mathrm{Tik}} = \\operatorname*{arg\\,min}_{\\mathbf{x} \\in \\mathbb{R}^N} \\frac{1}{2} \\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{m} \\rVert_2^2 + \\frac{\\alpha}{2} \\lVert \\mathbf{D}\\mathbf{x} \\rVert_2^2.\n     $$\n     This yields the normal equations\n     $$\n     \\left( \\mathbf{A}^\\top \\mathbf{A} + \\alpha \\mathbf{D}^\\top \\mathbf{D} \\right) \\widehat{\\mathbf{t}}_{\\mathrm{Tik}} = \\mathbf{A}^\\top \\mathbf{m}.\n     $$\n   - Total Variation (TV) regularization (first-difference $\\ell_1$): for a given TV strength $\\tau > 0$, solve\n     $$\n     \\widehat{\\mathbf{t}}_{\\mathrm{TV}} = \\operatorname*{arg\\,min}_{\\mathbf{x} \\in \\mathbb{R}^N} \\frac{1}{2} \\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{m} \\rVert_2^2 + \\tau \\lVert \\mathbf{D}\\mathbf{x} \\rVert_1.\n     $$\n     Implement this by the Alternating Direction Method of Multipliers (ADMM), introducing an auxiliary variable $\\mathbf{z} = \\mathbf{D}\\mathbf{x}$ and a penalty parameter $\\rho > 0$, with iterations\n     $$\n     \\mathbf{x}^{k+1} = \\left( \\mathbf{A}^\\top \\mathbf{A} + \\rho \\mathbf{D}^\\top \\mathbf{D} \\right)^{-1}\\left( \\mathbf{A}^\\top \\mathbf{m} + \\rho \\mathbf{D}^\\top(\\mathbf{z}^k - \\mathbf{u}^k) \\right),\n     $$\n     $$\n     \\mathbf{z}^{k+1} = \\operatorname{soft}\\left(\\mathbf{D}\\mathbf{x}^{k+1} + \\mathbf{u}^k, \\frac{\\tau}{\\rho}\\right),\n     $$\n     $$\n     \\mathbf{u}^{k+1} = \\mathbf{u}^k + \\mathbf{D}\\mathbf{x}^{k+1} - \\mathbf{z}^{k+1},\n     $$\n     where the soft-thresholding operator is defined componentwise by $\\operatorname{soft}(v, \\kappa) = \\operatorname{sign}(v)\\max(|v| - \\kappa, 0)$.\n\n3. Metrics to quantify how regularizers handle the induced edge:\n   For each unfolded spectrum $\\widehat{\\mathbf{t}}$ and each test case, compute:\n   - Mean squared error (MSE): \n     $$\n     \\mathrm{MSE}(\\widehat{\\mathbf{t}}) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( \\widehat{t}_i - t_i \\right)^2.\n     $$\n   - Edge residual magnitude at the threshold:\n     Define the threshold index as $\\theta$. If $\\theta \\in \\{0,\\dots,N-2\\}$, use\n     $$\n     E(\\widehat{\\mathbf{t}}) = \\left| \\widehat{t}_{\\theta+1} - \\widehat{t}_{\\theta} \\right|.\n     $$\n     If $\\theta = N-1$, use\n     $$\n     E(\\widehat{\\mathbf{t}}) = \\left| \\widehat{t}_{\\theta} - \\widehat{t}_{\\theta-1} \\right|.\n     $$\n   - High-region mean bias relative to the true spectrum:\n     $$\n     B(\\widehat{\\mathbf{t}}) = \\frac{1}{N-\\theta} \\sum_{i=\\theta}^{N-1} \\widehat{t}_i - \\frac{1}{N-\\theta} \\sum_{i=\\theta}^{N-1} t_i.\n     $$\n     Since $t_i = 1$, this simplifies to $B(\\widehat{\\mathbf{t}}) = \\frac{1}{N-\\theta} \\sum_{i=\\theta}^{N-1} \\widehat{t}_i - 1$.\n\n4. Test suite:\n   Use the following three parameter sets to form the response matrix, synthesize $\\mathbf{m} = \\mathbf{A}\\mathbf{t}$, and unfold:\n   - Case $1$ (general case): $N = 50$, $\\sigma = 1.5$, $\\theta = 30$, $\\varepsilon_{\\mathrm{drop}} = 0.05$, $\\alpha = 10^{-2}$, $\\tau = 10^{-2}$, $\\rho = 0.5$.\n   - Case $2$ (boundary threshold at the last bin): $N = 50$, $\\sigma = 1.5$, $\\theta = 49$, $\\varepsilon_{\\mathrm{drop}} = 0.05$, $\\alpha = 10^{-2}$, $\\tau = 10^{-2}$, $\\rho = 0.5$.\n   - Case $3$ (minimal smearing, extreme drop): $N = 50$, $\\sigma = 0.5$, $\\theta = 25$, $\\varepsilon_{\\mathrm{drop}} = 0.01$, $\\alpha = 10^{-2}$, $\\tau = 10^{-2}$, $\\rho = 0.5$.\n\n5. Required final output format:\n   Your program should produce a single line of output containing the results as a list of lists, one inner list per test case. For each test case, output a list containing $9$ floating-point numbers in the following order:\n   $$\n   \\left[ \\mathrm{MSE}(\\widehat{\\mathbf{t}}_{\\mathrm{LS}}), \\, E(\\widehat{\\mathbf{t}}_{\\mathrm{LS}}), \\, B(\\widehat{\\mathbf{t}}_{\\mathrm{LS}}), \\, \\mathrm{MSE}(\\widehat{\\mathbf{t}}_{\\mathrm{Tik}}), \\, E(\\widehat{\\mathbf{t}}_{\\mathrm{Tik}}), \\, B(\\widehat{\\mathbf{t}}_{\\mathrm{Tik}}), \\, \\mathrm{MSE}(\\widehat{\\mathbf{t}}_{\\mathrm{TV}}), \\, E(\\widehat{\\mathbf{t}}_{\\mathrm{TV}}), \\, B(\\widehat{\\mathbf{t}}_{\\mathrm{TV}}) \\right].\n   $$\n   Aggregate the three test cases as a single list in the above order, yielding an output of the form\n   $$\n   \\left[ [\\cdots], [\\cdots], [\\cdots] \\right].\n   $$\n   No other text should be printed.", "solution": "The posed problem is a well-defined exercise in computational high-energy physics, specifically addressing the deconvolution or \"unfolding\" of detector effects from a measured one-dimensional spectrum. The problem is scientifically grounded, mathematically consistent, and provides all necessary information to proceed with a solution.\n\nThe core of the problem is the linear forward model, which relates the true, unknown distribution $\\mathbf{t} \\in \\mathbb{R}^N$ to the measured distribution $\\mathbf{m} \\in \\mathbb{R}^N$ via a detector response matrix $\\mathbf{A} \\in \\mathbb{R}^{N \\times N}$. This relationship is given by:\n$$\n\\mathbf{m} = \\mathbf{A} \\mathbf{t}\n$$\nThe goal of unfolding is to estimate the true spectrum $\\mathbf{t}$ given the measured spectrum $\\mathbf{m}$ and the response matrix $\\mathbf{A}$. This is an inverse problem, often ill-posed due to the properties of $\\mathbf{A}$.\n\nThe response matrix $\\mathbf{A}$ models two primary detector effects: resolution smearing and detection efficiency.\nFor each true bin $i$, the corresponding column of $\\mathbf{A}$ describes how events originating in bin $i$ are distributed across the measured bins $j$.\nThe smearing is modeled by a normalized Gaussian kernel with width $\\sigma$, representing the finite resolution of the detector which blurs sharp features. The efficiency $\\varepsilon_i$ is a multiplicative factor for each true bin, representing the probability that an event in bin $i$ is detected at all. The problem specifies a sharp drop in efficiency at a threshold bin $\\theta$. The elements of $\\mathbf{A}$ are thus defined as:\n$$\nA_{j i} = \\varepsilon_i \\, \\frac{\\exp\\left( -\\frac{(j-i)^2}{2 \\sigma^2} \\right)}{\\sum_{k=0}^{N-1} \\exp\\left( -\\frac{(k-i)^2}{2 \\sigma^2} \\right)}\n$$\nwhere the efficiency $\\varepsilon_i$ is a step function:\n$$\n\\varepsilon_i = \\begin{cases}\n1, & \\text{if } i < \\theta \\\\\n\\varepsilon_{\\mathrm{drop}}, & \\text{if } i \\ge \\theta\n\\end{cases}\n$$\nWith the known true spectrum $\\mathbf{t}$ (a flat distribution with $t_i = 1$), the measured spectrum $\\mathbf{m}$ is synthesized as $\\mathbf{m} = \\mathbf{A} \\mathbf{t}$.\n\nThree unfolding methods are employed to estimate $\\mathbf{t}$ from $\\mathbf{m}$ and $\\mathbf{A}$.\n\n1.  **Unregularized Least Squares (LS)**: This is the most direct approach to solving the inverse problem. It seeks to find an estimate $\\widehat{\\mathbf{t}}_{\\mathrm{LS}}$ that minimizes the squared difference between the re-convolved estimate $\\mathbf{A}\\mathbf{x}$ and the measured data $\\mathbf{m}$. The objective function is:\n    $$\n    \\widehat{\\mathbf{t}}_{\\mathrm{LS}} = \\operatorname*{arg\\,min}_{\\mathbf{x} \\in \\mathbb{R}^N} \\frac{1}{2} \\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{m} \\rVert_2^2\n    $$\n    This is a standard linear least squares problem, the solution of which is found by solving the normal equations:\n    $$\n    \\mathbf{A}^\\top \\mathbf{A} \\, \\widehat{\\mathbf{t}}_{\\mathrm{LS}} = \\mathbf{A}^\\top \\mathbf{m}\n    $$\n    However, the matrix $\\mathbf{A}^\\top \\mathbf{A}$ is often ill-conditioned, causing the solution to be highly sensitive to small perturbations, which typically manifest as large, unphysical oscillations.\n\n2.  **Tikhonov Regularization**: This method extends least squares by adding a penalty term that regularizes the solution, suppressing oscillations. A common choice, used here, penalizes the squared $\\ell_2$-norm of the first differences of the solution vector, promoting smoothness. The first-difference operator is a matrix $\\mathbf{D} \\in \\mathbb{R}^{(N-1)\\times N}$. The objective function is:\n    $$\n    \\widehat{\\mathbf{t}}_{\\mathrm{Tik}} = \\operatorname*{arg\\,min}_{\\mathbf{x} \\in \\mathbb{R}^N} \\frac{1}{2} \\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{m} \\rVert_2^2 + \\frac{\\alpha}{2} \\lVert \\mathbf{D}\\mathbf{x} \\rVert_2^2\n    $$\n    Here, $\\alpha > 0$ is the regularization strength, which balances the goodness-of-fit with the smoothness of the solution. This also has a closed-form solution via a modified set of normal equations:\n    $$\n    \\left( \\mathbf{A}^\\top \\mathbf{A} + \\alpha \\mathbf{D}^\\top \\mathbf{D} \\right) \\widehat{\\mathbf{t}}_{\\mathrm{Tik}} = \\mathbf{A}^\\top \\mathbf{m}\n    $$\n    The addition of the term $\\alpha \\mathbf{D}^\\top \\mathbf{D}$ generally improves the condition number of the matrix, leading to a more stable solution. However, this regularization can oversmooth sharp features, like the edge induced by the efficiency drop.\n\n3.  **Total Variation (TV) Regularization**: This method uses an $\\ell_1$-norm penalty on the first differences, which is known for its ability to preserve sharp edges while smoothing flat regions. The objective function is:\n    $$\n    \\widehat{\\mathbf{t}}_{\\mathrm{TV}} = \\operatorname*{arg\\,min}_{\\mathbf{x} \\in \\mathbb{R}^N} \\frac{1}{2} \\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{m} \\rVert_2^2 + \\tau \\lVert \\mathbf{D}\\mathbf{x} \\rVert_1\n    $$\n    The parameter $\\tau > 0$ controls the strength of the TV regularization. Unlike Tikhonov regularization, this problem does not have a simple closed-form solution due to the non-differentiable $\\ell_1$-norm. It is solved here using the Alternating Direction Method of Multipliers (ADMM). This iterative algorithm introduces an auxiliary variable $\\mathbf{z}$ and solves the problem by alternating between updates for the primary variable $\\mathbf{x}$, the auxiliary variable $\\mathbf{z}$, and a dual variable $\\mathbf{u}$. The update steps as specified are implemented:\n    The $\\mathbf{x}$-update involves solving a linear system similar to the Tikhonov case. The $\\mathbf{z}$-update involves a component-wise soft-thresholding operation, which is the key step that promotes sparsity in the differences $\\mathbf{D}\\mathbf{x}$, thereby preserving edges. The $\\mathbf{u}$-update is a simple step to enforce the constraint $\\mathbf{z} = \\mathbf{D}\\mathbf{x}$.\n\nFinally, the performance of each unfolding method is evaluated using three metrics:\n-   **Mean Squared Error (MSE)**: $\\frac{1}{N} \\sum (\\widehat{t}_i - t_i)^2$. This provides a global measure of the accuracy of the unfolded spectrum.\n-   **Edge Residual Magnitude ($E$)**: $|\\widehat{t}_{\\theta+1} - \\widehat{t}_{\\theta}|$ (or similar at the boundary). This specifically quantifies how well the sharp drop in the true spectrum (which is implicitly introduced by the efficiency model) is reconstructed. Since the true spectrum is flat ($t_i=1$), the ideal difference is $0$, but the unfolding must counteract the efficiency step. This tests the regularizers' handling of edges.\n-   **High-Region Mean Bias ($B$)**: $\\frac{1}{N-\\theta} \\sum_{i=\\theta}^{N-1} \\widehat{t}_i - 1$. This measures the systematic deviation of the mean of the unfolded solution from the true mean in the region affected by the efficiency drop, quantifying any bias introduced by the unfolding process.\n\nThe implementation will proceed by systematically constructing the matrices, synthesizing the data, applying each of the three unfolding algorithms, and computing the nine specified metrics for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_case(N, sigma, theta, eps_drop, alpha, tau, rho):\n    \"\"\"\n    Runs a single test case for the unfolding problem.\n    \"\"\"\n    # 1. Setup: Define the true spectrum\n    t_true = np.ones(N)\n\n    # 2. Construct model matrices\n\n    # Construct efficiency vector\n    eps = np.ones(N)\n    if theta < N:\n        eps[theta:] = eps_drop\n\n    # Construct response matrix A\n    A = np.zeros((N, N))\n    indices = np.arange(N)\n    for i in range(N):\n        # Gaussian smearing kernel for column i\n        kernel_col = np.exp(-(indices - i)**2 / (2 * sigma**2))\n        # The kernel must be normalized to conserve events before efficiency\n        norm = np.sum(kernel_col)\n        if norm > 0:\n            kernel_col /= norm\n        # Apply efficiency\n        A[:, i] = eps[i] * kernel_col\n\n    # Construct first-difference operator D\n    D = np.zeros((N - 1, N))\n    row_indices = np.arange(N - 1)\n    D[row_indices, row_indices] = -1\n    D[row_indices, row_indices + 1] = 1\n\n    # 3. Synthesize measured data\n    m = A @ t_true\n\n    # 4. Perform unfolding with the three methods\n\n    # 4.1. Unregularized Least Squares\n    # Solve (A^T A) x = A^T m\n    try:\n        lhs_ls = A.T @ A\n        rhs_ls = A.T @ m\n        t_hat_ls = np.linalg.solve(lhs_ls, rhs_ls)\n    except np.linalg.LinAlgError:\n        # Use pseudo-inverse for singular matrices\n        t_hat_ls = np.linalg.pinv(A) @ m\n\n    # 4.2. Tikhonov Regularization\n    # Solve (A^T A + alpha D^T D) x = A^T m\n    lhs_tik = A.T @ A + alpha * (D.T @ D)\n    rhs_tik = A.T @ m\n    t_hat_tik = np.linalg.solve(lhs_tik, rhs_tik)\n\n    # 4.3. Total Variation Regularization via ADMM\n    def soft_threshold(v, kappa):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(v) * np.maximum(np.abs(v) - kappa, 0)\n    \n    # Initialize ADMM variables\n    x = np.zeros(N)\n    z = np.zeros(N - 1)\n    u = np.zeros(N - 1)\n    \n    # Pre-compute the inverse matrix for the x-update step for efficiency\n    x_update_matrix = A.T @ A + rho * (D.T @ D)\n    x_update_matrix_inv = np.linalg.inv(x_update_matrix)\n    \n    # ADMM iterations\n    num_iterations = 200\n    for _ in range(num_iterations):\n        # x-update\n        rhs_x = A.T @ m + rho * D.T @ (z - u)\n        x = x_update_matrix_inv @ rhs_x\n        \n        # z-update\n        z_arg = D @ x + u\n        z = soft_threshold(z_arg, tau / rho)\n        \n        # u-update\n        u = u + D @ x - z\n    \n    t_hat_tv = x\n\n    # 5. Calculate and collect metrics\n    \n    all_results = []\n    for t_hat in [t_hat_ls, t_hat_tik, t_hat_tv]:\n        # Metric 1: Mean Squared Error (MSE)\n        mse = np.mean((t_hat - t_true)**2)\n        \n        # Metric 2: Edge Residual Magnitude (E)\n        if theta == N - 1:\n            edge_residual = np.abs(t_hat[theta] - t_hat[theta - 1])\n        elif theta < N - 1:\n            edge_residual = np.abs(t_hat[theta + 1] - t_hat[theta])\n        else: # Case where theta >= N, not in tests, but for completeness.\n            edge_residual = 0.0\n\n        # Metric 3: High-Region Mean Bias (B)\n        # The true mean in the high region is 1.0 since t_true is all ones.\n        if N > theta:\n            bias = np.mean(t_hat[theta:]) - 1.0\n        else: # Case theta >= N\n            bias = 0.0\n\n        all_results.extend([mse, edge_residual, bias])\n        \n    return all_results\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general case)\n        {'N': 50, 'sigma': 1.5, 'theta': 30, 'eps_drop': 0.05, \n         'alpha': 1e-2, 'tau': 1e-2, 'rho': 0.5},\n        # Case 2 (boundary threshold)\n        {'N': 50, 'sigma': 1.5, 'theta': 49, 'eps_drop': 0.05, \n         'alpha': 1e-2, 'tau': 1e-2, 'rho': 0.5},\n        # Case 3 (minimal smearing, extreme drop)\n        {'N': 50, 'sigma': 0.5, 'theta': 25, 'eps_drop': 0.01, \n         'alpha': 1e-2, 'tau': 1e-2, 'rho': 0.5},\n    ]\n\n    results = []\n    for params in test_cases:\n        case_result = run_case(**params)\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # We construct the string manually to avoid spaces python's default str() adds.\n    case_strings = []\n    for res_list in results:\n        case_strings.append(f\"[{','.join(map(str, res_list))}]\")\n    \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n\n```", "id": "3540846"}, {"introduction": "Beyond regularized matrix inversion, iterative Bayesian unfolding offers a popular and conceptually different framework for correcting detector effects. This exercise delves into a crucial property of this method: its dependence on the initial prior distribution, which can be a significant source of systematic uncertainty. By analytically calculating the sensitivity of the first-iteration unfolded result to a change in the prior, you will develop a deeper, quantitative understanding of how prior beliefs propagate through the D'Agostini algorithm and influence the final result [@problem_id:3540819].", "problem": "Consider a counting measurement in computational high-energy physics where a truth-level distribution with two bins, indexed by $j \\in \\{1,2\\}$, is observed with finite resolution and inefficiency in two reconstructed bins, indexed by $i \\in \\{1,2\\}$. The detector response is characterized by the conditional probabilities $A_{ij} = P(i \\mid j)$, and the detection efficiency in truth bin $j$ is $\\epsilon_{j} = \\sum_{i} A_{ij}$, with $0 \\leq \\epsilon_{j} \\leq 1$. Let the measured reconstructed counts be $m_{1} = 1000$ and $m_{2} = 800$. The initial prior truth counts are $f_{1}^{(0)} = 900$ and $f_{2}^{(0)} = 700$. The response matrix is\n$$\nA \\;=\\; \\begin{pmatrix}\n0.8 & 0.2 \\\\\n0.2 & 0.6\n\\end{pmatrix},\n$$\nwith $A_{11} = 0.8$, $A_{12} = 0.2$, $A_{21} = 0.2$, and $A_{22} = 0.6$.\n\nAssume the unfolding is performed using the iterative Bayesian method introduced by G. D'Agostini, with the update rule derived from Bayes' theorem and the known detection efficiency. Starting from the fundamental relation of Bayes' theorem $P(j \\mid i) = \\frac{P(i \\mid j)\\,P(j)}{\\sum_{k} P(i \\mid k)\\,P(k)}$ and the fact that the expected reconstructed counts are given by folding the truth through the response, derive the first-iteration unfolded estimate $f_{j}^{(1)}$ in terms of $f_{j}^{(0)}$, $m_{i}$, $A_{ij}$, and $\\epsilon_{j}$. Then, analytically compute the sensitivity of the unfolded result in truth bin $j=1$ to the initial prior in truth bin $\\ell=2$ at the first iteration, that is, compute $\\frac{\\partial f_{1}^{(1)}}{\\partial f_{2}^{(0)}}$ evaluated at the provided numerical values. Round your final numerical answer to four significant figures. Express your final answer without units.", "solution": "### Step 1: Extract Givens\nThe problem provides the following data and definitions:\n-   Number of truth bins: $j \\in \\{1,2\\}$\n-   Number of reconstructed bins: $i \\in \\{1,2\\}$\n-   Response matrix of conditional probabilities $A_{ij} = P(i \\mid j)$:\n    $$ A = \\begin{pmatrix} 0.8 & 0.2 \\\\ 0.2 & 0.6 \\end{pmatrix} $$\n    This implies $A_{11} = 0.8$, $A_{12} = 0.2$, $A_{21} = 0.2$, and $A_{22} = 0.6$.\n-   Detection efficiency in truth bin $j$: $\\epsilon_{j} = \\sum_{i} A_{ij}$, where $0 \\leq \\epsilon_{j} \\leq 1$.\n-   Measured reconstructed counts: $m_{1} = 1000$ and $m_{2} = 800$.\n-   Initial prior truth counts (iteration $0$): $f_{1}^{(0)} = 900$ and $f_{2}^{(0)} = 700$.\n-   The unfolding method is the iterative Bayesian method by G. D'Agostini.\n-   The starting point for the derivation is Bayes' theorem: $P(j \\mid i) = \\frac{P(i \\mid j)\\,P(j)}{\\sum_{k} P(i \\mid k)\\,P(k)}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria.\n-   **Scientifically Grounded (Critical)**: The problem is based on the iterative Bayesian unfolding technique developed by G. D'Agostini, a standard and well-documented method in experimental high-energy physics. The underlying principles are Bayesian statistics and matrix operations, which are mathematically and scientifically sound.\n-   **Well-Posed**: The problem asks for a derivation and a subsequent calculation. All necessary numerical values ($m_i$, $f_j^{(0)}$, $A_{ij}$) and definitions are provided. The question is unambiguous and leads to a unique analytical and numerical answer.\n-   **Objective (Critical)**: The problem is stated using precise, quantitative, and unbiased language. There are no subjective or opinion-based components.\n-   **Incomplete or Contradictory Setup**: The setup is complete and self-contained. The response matrix elements are probabilities ($0 \\leq A_{ij} \\leq 1$), and the resulting efficiencies $\\epsilon_j = \\sum_i A_{ij}$ are also within the valid range $[0, 1]$. Specifically, $\\epsilon_1 = A_{11} + A_{21} = 0.8 + 0.2 = 1.0$, and $\\epsilon_2 = A_{12} + A_{22} = 0.2 + 0.6 = 0.8$. These values are physically consistent.\n-   **Unrealistic or Infeasible**: The provided numerical values for counts, priors, and response probabilities are plausible for a typical physics counting experiment.\n-   **No other flaws are detected.** The problem is formally structured and aligns with standard practices in the specified field of computational high-energy physics.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Solution Derivation and Calculation\n\nThe problem requires the derivation of the first-iteration unfolded estimate $f_{j}^{(1)}$ and the calculation of a specific sensitivity, $\\frac{\\partial f_{1}^{(1)}}{\\partial f_{2}^{(0)}}$.\n\n**Part 1: Derivation of the Unfolding Formula**\n\nThe core of the D'Agostini unfolding method is an application of Bayes' theorem to iteratively update the estimate of the true distribution. We begin, as instructed, with Bayes' theorem for the probability of a cause (event from truth bin $j$) given an effect (event observed in reconstructed bin $i$):\n$$ P(j \\mid i) = \\frac{P(i \\mid j) P(j)}{\\sum_{k} P(i \\mid k) P(k)} $$\nIn the context of this problem:\n-   $P(i \\mid j)$ is the probability of observing an event in reco bin $i$ given it originated from truth bin $j$. This is given by the response matrix, $P(i \\mid j) = A_{ij}$.\n-   $P(j)$ is the prior probability that an event originates from truth bin $j$. This is taken to be proportional to our current best estimate of the true number of events in that bin, $f_j^{(0)}$. That is, $P(j) = \\frac{f_j^{(0)}}{\\sum_l f_l^{(0)}}$.\n\nSubstituting these into Bayes' theorem, the normalization constant $\\sum_l f_l^{(0)}$ cancels from the numerator and denominator:\n$$ P(j \\mid i) = \\frac{A_{ij} f_j^{(0)}}{\\sum_{k} A_{ik} f_k^{(0)}} $$\nThis expression, $P(j \\mid i)$, represents the fraction of events observed in reconstructed bin $i$ that are estimated to have originated from truth bin $j$.\n\nGiven $m_i$ measured events in reconstructed bin $i$, the number of these events that are assigned to truth bin $j$ is $m_i P(j \\mid i)$. To find the total number of *observed* events associated with truth bin $j$, we sum over all reconstructed bins $i$:\n$$ n_{\\text{obs}, j} = \\sum_{i} m_i P(j \\mid i) = \\sum_{i} m_i \\frac{A_{ij} f_j^{(0)}}{\\sum_{k} A_{ik} f_k^{(0)}} $$\nThis quantity, $n_{\\text{obs}, j}$, represents the number of events from truth bin $j$ that were actually detected. The total number of events originating from truth bin $j$, which we denote as our updated estimate $f_j^{(1)}$, is related to $n_{\\text{obs}, j}$ by the detection efficiency $\\epsilon_j$:\n$$ n_{\\text{obs}, j} = \\epsilon_j f_j^{(1)} $$\nwhere $\\epsilon_j = \\sum_i A_{ij}$ is the probability that an event from truth bin $j$ is detected in *any* reconstructed bin.\n\nSolving for $f_j^{(1)}$, we obtain the one-step iterative Bayesian unfolding formula:\n$$ f_j^{(1)} = \\frac{1}{\\epsilon_j} n_{\\text{obs}, j} = \\frac{1}{\\epsilon_j} \\sum_{i} m_i \\frac{A_{ij} f_j^{(0)}}{\\sum_{k} A_{ik} f_k^{(0)}} $$\nThis is the derived expression for the first-iteration unfolded estimate.\n\n**Part 2: Analytical Calculation of Sensitivity**\n\nWe are asked to compute the sensitivity of the unfolded result in truth bin $j=1$ to the initial prior in truth bin $\\ell=2$, which is the partial derivative $\\frac{\\partial f_{1}^{(1)}}{\\partial f_{2}^{(0)}}$.\nFor our $2$-bin case ($j, k, \\ell \\in \\{1,2\\}$), the expression for $f_1^{(1)}$ is:\n$$ f_1^{(1)} = \\frac{1}{\\epsilon_1} \\left( m_1 \\frac{A_{11} f_1^{(0)}}{A_{11} f_1^{(0)} + A_{12} f_2^{(0)}} + m_2 \\frac{A_{21} f_1^{(0)}}{A_{21} f_1^{(0)} + A_{22} f_2^{(0)}} \\right) $$\nWe will now differentiate this expression with respect to $f_2^{(0)}$, treating $f_1^{(0)}$, $m_1$, and $m_2$ as constants. We use the chain rule for terms of the form $\\frac{c}{u(x)}$, where $\\frac{d}{dx}(\\frac{c}{u(x)}) = -\\frac{c}{u(x)^2} \\frac{du}{dx}$.\n\nFor the first term in the sum:\n$$ \\frac{\\partial}{\\partial f_2^{(0)}} \\left( m_1 \\frac{A_{11} f_1^{(0)}}{A_{11} f_1^{(0)} + A_{12} f_2^{(0)}} \\right) = m_1 A_{11} f_1^{(0)} \\left( - \\frac{A_{12}}{(A_{11} f_1^{(0)} + A_{12} f_2^{(0)})^2} \\right) = - \\frac{m_1 A_{11} A_{12} f_1^{(0)}}{(A_{11} f_1^{(0)} + A_{12} f_2^{(0)})^2} $$\nFor the second term in the sum:\n$$ \\frac{\\partial}{\\partial f_2^{(0)}} \\left( m_2 \\frac{A_{21} f_1^{(0)}}{A_{21} f_1^{(0)} + A_{22} f_2^{(0)}} \\right) = m_2 A_{21} f_1^{(0)} \\left( - \\frac{A_{22}}{(A_{21} f_1^{(0)} + A_{22} f_2^{(0)})^2} \\right) = - \\frac{m_2 A_{21} A_{22} f_1^{(0)}}{(A_{21} f_1^{(0)} + A_{22} f_2^{(0)})^2} $$\nCombining these results, the full partial derivative is:\n$$ \\frac{\\partial f_1^{(1)}}{\\partial f_2^{(0)}} = \\frac{1}{\\epsilon_1} \\left( - \\frac{m_1 A_{11} A_{12} f_1^{(0)}}{(A_{11} f_1^{(0)} + A_{12} f_2^{(0)})^2} - \\frac{m_2 A_{21} A_{22} f_1^{(0)}}{(A_{21} f_1^{(0)} + A_{22} f_2^{(0)})^2} \\right) $$\n\n**Part 3: Numerical Evaluation**\n\nWe substitute the given numerical values into the derived expression.\nFirst, calculate the efficiency $\\epsilon_1$:\n$$ \\epsilon_1 = \\sum_{i=1}^2 A_{i1} = A_{11} + A_{21} = 0.8 + 0.2 = 1.0 $$\nThe given values are:\n-   $m_1 = 1000$, $m_2 = 800$\n-   $f_1^{(0)} = 900$, $f_2^{(0)} = 700$\n-   $A_{11} = 0.8$, $A_{12} = 0.2$\n-   $A_{21} = 0.2$, $A_{22} = 0.6$\n\nNow, we evaluate the two terms in the derivative:\n**Term 1:**\n-   Denominator: $(A_{11} f_1^{(0)} + A_{12} f_2^{(0)})^2 = (0.8 \\times 900 + 0.2 \\times 700)^2 = (720 + 140)^2 = 860^2 = 739600$.\n-   Numerator: $-m_1 A_{11} A_{12} f_1^{(0)} = -1000 \\times 0.8 \\times 0.2 \\times 900 = -1000 \\times 0.16 \\times 900 = -144000$.\n-   First fraction: $\\frac{-144000}{739600}$.\n\n**Term 2:**\n-   Denominator: $(A_{21} f_1^{(0)} + A_{22} f_2^{(0)})^2 = (0.2 \\times 900 + 0.6 \\times 700)^2 = (180 + 420)^2 = 600^2 = 360000$.\n-   Numerator: $-m_2 A_{21} A_{22} f_1^{(0)} = -800 \\times 0.2 \\times 0.6 \\times 900 = -800 \\times 0.12 \\times 900 = -86400$.\n-   Second fraction: $\\frac{-86400}{360000}$.\n\nNow, we compute the sum:\n$$ \\frac{\\partial f_1^{(1)}}{\\partial f_2^{(0)}} = \\frac{1}{1.0} \\left( \\frac{-144000}{739600} + \\frac{-86400}{360000} \\right) $$\n$$ \\frac{\\partial f_1^{(1)}}{\\partial f_2^{(0)}} = -0.19469983... - 0.24 $$\n$$ \\frac{\\partial f_1^{(1)}}{\\partial f_2^{(0)}} = -0.43469983... $$\nRounding the final result to four significant figures gives $-0.4347$. The negative sign indicates that as the prior belief in the number of events in truth bin $2$ increases, the unfolded estimate for the number of events in truth bin $1$ decreases, which is an expected feature of the unfolding procedure due to the re-attribution of shared measured counts.", "answer": "$$\n\\boxed{-0.4347}\n$$", "id": "3540819"}]}