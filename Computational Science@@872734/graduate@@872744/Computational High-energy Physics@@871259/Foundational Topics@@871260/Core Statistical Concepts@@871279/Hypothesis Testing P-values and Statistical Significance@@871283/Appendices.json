{"hands_on_practices": [{"introduction": "A cornerstone of hypothesis testing is the ability to derive a $p$-value from first principles. This exercise guides you through a classic high-energy physics scenario: the search for Charge-Parity (CP) violation by counting charge-conjugate decays. You will learn to navigate the presence of a \"nuisance parameter\"—in this case, an unknown detection efficiency—by employing the powerful technique of conditioning to construct an exact test statistic. This practice is invaluable for understanding how frequentist tests can be made robust against systematic uncertainties that are not of primary interest.", "problem": "In a search for Charge-Parity (CP) violation in a heavy flavor decay channel at a hadron collider, an analysis counts reconstructed decays into charge-conjugate final states, yielding observed counts $N^{+}$ and $N^{-}$. The CP asymmetry estimator is defined by $A=\\frac{N^{+}-N^{-}}{N^{+}+N^{-}}$. Assume that, at the level of particle production and decay, the numbers of decays to the two charge-conjugate final states are generated by independent Poisson processes with means $\\frac{\\mu(1+A_{0})}{2}$ and $\\frac{\\mu(1-A_{0})}{2}$, respectively, where $\\mugt;0$ is an unknown overall rate parameter and $A_{0}$ is the true physical CP asymmetry. Each produced decay is reconstructed independently with a common detection efficiency $\\epsilon\\in(0,1]$ that is unknown and potentially time-varying but constant over the data-taking period relevant to these counts. There are no backgrounds.\n\nYou are tasked with testing the null hypothesis $H_{0}:A_{0}=0$ against a two-sided alternative using the exact finite-sample distribution. Treat the unknown detection efficiency $\\epsilon$ as a nuisance parameter and use a likelihood-based argument appropriate for computational high-energy physics to eliminate $\\epsilon$ by profiling. Construct a test based on the extremeness of $|A|$ (equivalently $|N^{+}-N^{-}|$) under $H_{0}$. Starting from first principles about Poisson processes, independent Bernoulli detection, and the conditional distribution of one Poisson component given their sum, derive an exact, closed-form expression for the two-sided $p$-value, denoted $p(A)$, as a function of the observed $A$ and the total count $N=N^{+}+N^{-}$, after profiling over $\\epsilon$.\n\nYour final answer must be a single closed-form analytic expression for $p(A)$ in terms of $A$ and $N$. Do not provide any numerical evaluation or approximation, and do not include physical units. Clearly define any symbols you introduce. The two-sided $p$-value must be constructed using the exact distribution under $H_{0}$ and must properly account for the discreteness of the data by including all outcomes at least as extreme as the observation with respect to $|A|$.", "solution": "The user wants to derive the exact, two-sided $p$-value for a test of the null hypothesis of no Charge-Parity (CP) asymmetry ($H_0: A_0=0$) in a counting experiment.\n\n### Step 1: Statistical Model of Observed Counts\n\nLet $n^{+}$ and $n^{-}$ be the true number of decays into the charge-conjugate final states. According to the problem statement, these are independent random variables drawn from Poisson distributions:\n$$ n^{+} \\sim \\text{Pois}\\left(\\lambda^{+} = \\frac{\\mu(1+A_{0})}{2}\\right) $$\n$$ n^{-} \\sim \\text{Pois}\\left(\\lambda^{-} = \\frac{\\mu(1-A_{0})}{2}\\right) $$\nHere, $\\mu$ is the total production rate and $A_0$ is the true physical asymmetry.\n\nEach of these decays is detected with a common efficiency $\\epsilon \\in (0,1]$. The observation of any single decay is an independent Bernoulli trial. A known property of the Poisson distribution is that if we sample it with a Bernoulli process (a process called \"thinning\"), the resulting distribution of counts is also Poisson. Therefore, the observed counts, $N^{+}$ and $N^{-}$, are also independent Poisson random variables with modified means:\n$$ N^{+} \\sim \\text{Pois}\\left(\\mu_{+} = \\epsilon \\lambda^{+} = \\frac{\\epsilon\\mu(1+A_{0})}{2}\\right) $$\n$$ N^{-} \\sim \\text{Pois}\\left(\\mu_{-} = \\epsilon \\lambda^{-} = \\frac{\\epsilon\\mu(1-A_{0})}{2}\\right) $$\nThe model for our observables $N^{+}$ and $N^{-}$ depends on the parameter of interest, $A_0$, and two nuisance parameters, $\\mu$ and $\\epsilon$. We can combine the nuisance parameters into a single unknown total observed rate, $\\mu_{\\text{obs}} = \\epsilon \\mu$. The means are then $\\mu_{\\pm} = \\frac{\\mu_{\\text{obs}}(1\\pm A_0)}{2}$.\n\n### Step 2: Eliminating Nuisance Parameters via a Conditional Test\n\nThe problem requires a test using an exact finite-sample distribution after eliminating the nuisance parameter $\\epsilon$ (and by extension $\\mu$). The standard and rigorous method to achieve this in this context, explicitly suggested by the problem, is to construct a test conditional on the total observed number of events, $N = N^{+} + N^{-}$.\n\nThe sum of two independent Poisson variables is also a Poisson variable. Thus, the total count $N$ follows a Poisson distribution with mean $\\mu_{+}+\\mu_{-}$:\n$$ \\mu_{+}+\\mu_{-} = \\frac{\\mu_{\\text{obs}}(1+A_{0})}{2} + \\frac{\\mu_{\\text{obs}}(1-A_{0})}{2} = \\mu_{\\text{obs}} $$\nSo, $N \\sim \\text{Pois}(\\mu_{\\text{obs}})$.\n\nWe now derive the distribution of $N^{+}$ conditional on the total count $N=k_{tot}$. For any potential count $k \\in \\{0, 1, ..., k_{tot}\\}$, the conditional probability is:\n$$ P(N^{+} = k | N^{+} + N^{-} = k_{tot}) = \\frac{P(N^{+} = k \\text{ and } N^{-} = k_{tot}-k)}{P(N^{+} + N^{-} = k_{tot})} $$\nDue to independence, this is:\n$$ P(N^{+} = k | N = k_{tot}) = \\frac{P(N^{+} = k) P(N^{-} = k_{tot}-k)}{P(N = k_{tot})} $$\nSubstituting the Poisson probability mass functions (PMFs):\n$$ P(N^{+} = k | N = k_{tot}) = \\frac{\\left(\\frac{\\mu_{+}^{k} e^{-\\mu_{+}}}{k!}\\right) \\left(\\frac{\\mu_{-}^{k_{tot}-k} e^{-\\mu_{-}}}{(k_{tot}-k)!}\\right)}{\\frac{(\\mu_{+}+\\mu_{-})^{k_{tot}} e^{-(\\mu_{+}+\\mu_{-})}}{k_{tot}!}} $$\nThe exponential terms cancel out. Rearranging the terms gives:\n$$ P(N^{+} = k | N = k_{tot}) = \\frac{k_{tot}!}{k!(k_{tot}-k)!} \\frac{\\mu_{+}^{k} \\mu_{-}^{k_{tot}-k}}{(\\mu_{+}+\\mu_{-})^{k_{tot}}} = \\binom{k_{tot}}{k} \\left(\\frac{\\mu_{+}}{\\mu_{+}+\\mu_{-}}\\right)^{k} \\left(1 - \\frac{\\mu_{+}}{\\mu_{+}+\\mu_{-}}\\right)^{k_{tot}-k} $$\nThis is the PMF of a binomial distribution, $\\text{Binomial}(k_{tot}, p)$. The success probability $p$ is:\n$$ p = \\frac{\\mu_{+}}{\\mu_{+}+\\mu_{-}} = \\frac{\\frac{\\mu_{\\text{obs}}(1+A_{0})}{2}}{\\mu_{\\text{obs}}} = \\frac{1+A_{0}}{2} $$\nCrucially, this probability $p$ depends only on the parameter of interest, $A_0$, and not on the nuisance parameter $\\mu_{\\text{obs}} = \\epsilon \\mu$. Thus, by conditioning on the total number of observed events $N$, we have successfully constructed a test framework that is independent of the unknown rate and efficiency.\n\n### Step 3: Constructing the p-value\n\nThe null hypothesis is $H_{0}: A_{0}=0$. Under $H_0$, the binomial probability becomes $p_0 = \\frac{1+0}{2} = \\frac{1}{2}$. Therefore, under the null hypothesis, the distribution of $N^{+}$ conditional on the total count $N$ is:\n$$ (N^{+} | N, H_0) \\sim \\text{Binomial}\\left(N, \\frac{1}{2}\\right) $$\nThe test is based on the extremeness of the observed CP asymmetry estimator $A = \\frac{N^{+}-N^{-}}{N^{+}+N^{-}}$. We can express $A$ in terms of $N^{+}$ and the fixed total count $N$:\n$$ A = \\frac{N^{+} - (N-N^{+})}{N} = \\frac{2N^{+} - N}{N} $$\nThe extremeness of $|A|$ is therefore equivalent to the extremeness of $|2N^{+} - N|$, which is equivalent to testing for values of $N^{+}$ that are far from the expected value under $H_0$, which is $E[N^{+}|N, H_0] = N \\times p_0 = \\frac{N}{2}$.\n\nThe two-sided $p$-value is the probability, calculated under $H_0$, of observing a result at least as extreme as the one actually observed. Let the observed count be $N^{+}_{\\text{obs}}$. The extremeness of this observation is measured by its distance from the mean, $|N^{+}_{\\text{obs}} - N/2|$. The $p$-value is the sum of probabilities of all possible outcomes $k$ (for the random variable $N^{+}$) that are at least as far from the mean:\n$$ p = P_{H_0}\\left(\\left|N^{+} - \\frac{N}{2}\\right| \\ge \\left|N^{+}_{\\text{obs}} - \\frac{N}{2}\\right| \\Big| N\\right) $$\nWe can express the observed count $N^{+}_{\\text{obs}}$ in terms of the observed asymmetry $A$ and total count $N$:\n$$ AN = 2N^{+}_{\\text{obs}} - N \\implies N^{+}_{\\text{obs}} = \\frac{N(1+A)}{2} $$\nSubstituting this into the extremeness condition:\n$$ \\left|N^{+}_{\\text{obs}} - \\frac{N}{2}\\right| = \\left|\\frac{N(1+A)}{2} - \\frac{N}{2}\\right| = \\left|\\frac{NA}{2}\\right| = \\frac{N|A|}{2} $$\nSo the condition for an outcome $k$ to be at least as extreme as the observation is $\\left|k - \\frac{N}{2}\\right| \\ge \\frac{N|A|}{2}$.\n\nThe $p$-value is the sum of the binomial probabilities for all such $k$:\n$$ p(A) = \\sum_{k=0}^{N} P(N^{+}=k | N, H_0) \\cdot I\\left(\\left|k - \\frac{N}{2}\\right| \\ge \\frac{N|A|}{2}\\right) $$\nwhere $I(\\cdot)$ is the indicator function, which is $1$ if its argument is true and $0$ otherwise. Substituting the binomial probability $P(N^{+}=k | N, H_0) = \\binom{N}{k}(\\frac{1}{2})^{N}$:\n$$ p(A) = \\sum_{k=0}^{N} \\binom{N}{k} \\left(\\frac{1}{2}\\right)^{N} I\\left(\\left|k - \\frac{N}{2}\\right| \\ge \\frac{N|A|}{2}\\right) $$\nThis expression can be written as:\n$$ p(A) = \\left(\\frac{1}{2}\\right)^{N} \\sum_{k=0}^{N} \\binom{N}{k} I\\left(\\left|k - \\frac{N}{2}\\right| \\ge \\frac{N|A|}{2}\\right) $$\nThis is the final, exact, closed-form expression for the two-sided $p$-value. It is a function of the observed asymmetry $A$ and total count $N$, as required. The finite sum is considered a closed form as it does not involve limits, integrals, or recursion.", "answer": "$$ \\boxed{ \\left(\\frac{1}{2}\\right)^{N} \\sum_{k=0}^{N} \\binom{N}{k} I\\left(\\left|k - \\frac{N}{2}\\right| \\ge \\frac{N|A|}{2}\\right) } $$", "id": "3517328"}, {"introduction": "While exact analytical tests are instructive, many real-world analyses rely on asymptotic approximations like Wilks' theorem for the profile likelihood ratio. However, these approximations have limits. This computational exercise tasks you with investigating the \"coverage\" of such asymptotic confidence intervals in the challenging low-count regime, which is common in searches for new physics. By simulating experiments and comparing the empirical coverage to the nominal confidence level, you will directly observe the potential for these methods to undercover, reinforcing the critical need to validate statistical tools before deploying them.", "problem": "You are tasked with quantifying the empirical coverage of asymptotic profile likelihood confidence intervals for a signal strength parameter in a low-count counting experiment typical of computational high-energy physics. Consider a simple search channel with a single-bin counting measurement, where the observation $n$ follows a Poisson distribution with mean $\\mu = s + b$, with $s \\ge 0$ the non-negative signal yield and $b \\ge 0$ a known background yield. The likelihood for one observation is $L(n \\mid s) \\propto (s + b)^{n} e^{-(s+b)}$, and the corresponding log-likelihood is $\\ell(n \\mid s) = n \\log(s+b) - (s+b)$ up to an additive constant that does not depend on $s$. The Maximum Likelihood Estimator (MLE) satisfies $\\hat{s} = \\max(0, n - b)$. The profile likelihood ratio is $\\lambda(s) = L(n \\mid s) / L(n \\mid \\hat{s})$, and the corresponding likelihood ratio test statistic is $q(s) = -2 \\log \\lambda(s) = 2 [\\ell(n \\mid \\hat{s}) - \\ell(n \\mid s)]$. Under regularity conditions and in the asymptotic regime, $q(s)$ converges in distribution to a chi-square distribution with one degree of freedom for a fixed $s$, according to Wilks’ theorem.\n\nIn practice, at low counts and in the presence of the non-negativity boundary $s \\ge 0$, the two-sided asymptotic confidence interval constructed by inverting the test $q(s) \\le c_{\\alpha}$, where $c_{\\alpha}$ is the $(1 - \\alpha)$ quantile of the chi-square distribution with one degree of freedom, may undercover. Your task is to use Monte Carlo simulation to empirically measure the coverage of these asymptotic intervals at low counts as a function of the true signal $s$.\n\nYour program must implement the following from first principles:\n- The Poisson model for a single counting experiment with known background $b$ and non-negative signal strength $s$.\n- The profile likelihood ratio test statistic $q(s) = 2 [\\ell(n \\mid \\hat{s}) - \\ell(n \\mid s)]$ with $\\hat{s} = \\max(0, n - b)$.\n- The asymptotic inversion rule that defines the two-sided set $\\{ s \\ge 0 : q(s) \\le c_{\\alpha} \\}$ as the $(1 - \\alpha)$ confidence interval for $s$, where $c_{\\alpha}$ is the $(1 - \\alpha)$ chi-square critical value with one degree of freedom.\n- Numerical determination of the interval endpoints for a given observed count $n$, background $b$, and level $\\alpha$, by solving $q(s) = c_{\\alpha}$ for the lower and upper endpoints subject to $s \\ge 0$. No closed-form shortcuts are allowed; use monotonic bracketing and root finding on $q(s) - c_{\\alpha}$, and treat the boundary $s = 0$ correctly.\n- Monte Carlo coverage estimation: for a given true $s$ and $b$, simulate $N$ independent observations $n \\sim \\text{Poisson}(s + b)$, compute the asymptotic interval for each observed $n$, and compute the empirical coverage as the fraction of simulations where the true $s$ lies within the constructed interval (inclusive of endpoints). Use a fixed random seed $123456$ for reproducibility.\n\nBase your derivation and implementation only on the following foundational elements: the Poisson likelihood for independent counts, the definition of the profile likelihood ratio and its asymptotic chi-square behavior (Wilks’ theorem), and basic properties of maximum likelihood estimators. Do not use any pre-tabulated interval formulas or problem-specific heuristics.\n\nTest Suite:\nImplement and run the following three test cases. For each case, report the empirical coverage for each $s$ value in the specified grid, in the same order as listed.\n\n- Case A (very low background): background $b = 0.5$, nominal confidence level $1 - \\alpha = 0.95$ (i.e., $\\alpha = 0.05$), signal grid $s \\in \\{0.0, 0.2, 0.5, 1.0, 2.0\\}$, number of Monte Carlo repetitions $N = 4000$.\n- Case B (moderate background): background $b = 3.0$, nominal confidence level $1 - \\alpha = 0.95$ (i.e., $\\alpha = 0.05$), signal grid $s \\in \\{0.0, 1.0, 2.0, 5.0\\}$, number of Monte Carlo repetitions $N = 4000$.\n- Case C (no background, boundary-dominated): background $b = 0.0$, nominal confidence level $1 - \\alpha = 0.90$ (i.e., $\\alpha = 0.10$), signal grid $s \\in \\{0.0, 0.5, 1.0\\}$, number of Monte Carlo repetitions $N = 6000$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result should itself be a list of floating-point empirical coverages corresponding to the $s$ grid for that case. For example, the output should look like $[[c_{A,1}, c_{A,2}, \\dots],[c_{B,1}, \\dots],[c_{C,1}, \\dots]]$, where each $c_{\\cdot,\\cdot}$ is a float in $[0,1]$ expressed as a decimal number. No other text should be printed.", "solution": "The problem requires an empirical validation of the coverage property of asymptotic confidence intervals derived from the profile likelihood ratio method in a single-bin Poisson counting experiment. The core of the task is to perform a Monte Carlo simulation to quantify how often the constructed confidence interval contains the true value of the signal parameter $s$, especially in the low-count regime where asymptotic approximations may be less accurate.\n\nThe solution is developed from first principles, following a sequence of logical steps: defining the statistical model, constructing the test statistic, deriving the confidence interval procedure, and implementing a Monte Carlo simulation to assess its performance.\n\n**1. Statistical Model and Likelihood**\n\nThe experiment observes a number of events $n$ in a single bin. This count is assumed to follow a Poisson distribution with a mean $\\mu$ that is the sum of an unknown non-negative signal yield $s \\ge 0$ and a known non-negative background yield $b \\ge 0$.\nThe probability of observing $n$ events is given by the Poisson probability mass function:\n$$ P(n \\mid s, b) = \\frac{(s+b)^n e^{-(s+b)}}{n!} $$\nFor a fixed observation $n$, the likelihood function for the parameter $s$ is proportional to this probability:\n$$ L(s \\mid n) \\propto (s+b)^n e^{-(s+b)} $$\nIt is computationally more convenient to work with the log-likelihood function, $\\ell(s \\mid n) = \\log L(s \\mid n)$. Ignoring terms that do not depend on $s$, we have:\n$$ \\ell(s \\mid n) = n \\log(s+b) - (s+b) $$\nThe Maximum Likelihood Estimator (MLE) for $s$, denoted $\\hat{s}$, is the value of $s$ that maximizes $\\ell(s \\mid n)$ subject to the physical constraint $s \\ge 0$. By setting the derivative $\\frac{d\\ell}{ds} = \\frac{n}{s+b} - 1$ to zero, we find the unconstrained estimator is $s = n-b$. Incorporating the boundary condition $s \\ge 0$, the MLE is:\n$$ \\hat{s} = \\max(0, n-b) $$\n\n**2. Profile Likelihood Ratio Test Statistic**\n\nTo construct a confidence interval for $s$, we use the profile likelihood ratio test. The test statistic, $q(s)$, compares the likelihood at a hypothesized value of $s$ to the maximum possible likelihood (i.e., the likelihood at $\\hat{s}$). The profile likelihood ratio $\\lambda(s)$ is defined as:\n$$ \\lambda(s) = \\frac{L(s \\mid n)}{L(\\hat{s} \\mid n)} $$\nThe test statistic $q(s)$ is then given by:\n$$ q(s) = -2 \\log \\lambda(s) = -2 (\\ell(s \\mid n) - \\ell(\\hat{s} \\mid n)) $$\nSubstituting the expression for the log-likelihood yields:\n$$ q(s) = 2 \\left[ (n \\log(\\hat{s}+b) - (\\hat{s}+b)) - (n \\log(s+b) - (s+b)) \\right] $$\nAccording to Wilks' theorem, in the large-sample (asymptotic) limit, the distribution of $q(s)$ for a fixed true value of $s$ converges to a chi-square ($\\chi^2$) distribution with one degree of freedom, corresponding to the single parameter of interest.\n\n**3. Confidence Interval Construction via Test Inversion**\n\nA $(1-\\alpha)$ confidence interval for $s$ is constructed by \"inverting\" the hypothesis test. The interval consists of all values of $s_0$ for which the null hypothesis $H_0: s=s_0$ would not be rejected at significance level $\\alpha$. The rejection region for the test is $q(s_0)  c_\\alpha$, where $c_\\alpha$ is the $(1-\\alpha)$ quantile of the $\\chi^2_1$ distribution (i.e., $P(\\chi^2_1 \\le c_\\alpha) = 1-\\alpha$). Consequently, the acceptance region, and thus the confidence interval, is the set of all $s$ values satisfying:\n$$ \\{ s \\ge 0 \\mid q(s) \\le c_\\alpha \\} $$\nThe endpoints of this interval, $s_{\\text{low}}$ and $s_{\\text{up}}$, are found by solving the equation:\n$$ q(s) - c_\\alpha = 0 $$\nSince $q(s)$ is not generally a simple function, its roots must be found numerically. The algorithm is designed as follows:\n- **Root-finding function**: We define a function $f(s) = q(s) - c_\\alpha$ whose roots we seek.\n- **Lower bound, $s_{\\text{low}}$**: The function $q(s)$ has its minimum of $q(\\hat{s}) = 0$ at $s=\\hat{s}$. If $\\hat{s}=0$, the minimum is at the boundary, so the interval must start at $s_{\\text{low}}=0$. If $\\hat{s}  0$, we evaluate $q(s)$ at the boundary $s=0$. If $q(0) \\le c_\\alpha$, the boundary is included in the interval, so $s_{\\text{low}}=0$. If $q(0)  c_\\alpha$, there must be a root $s_{\\text{low}} \\in (0, \\hat{s})$. This root is found numerically using a bracketing algorithm (like Brent's method) on the interval $[0, \\hat{s}]$.\n- **Upper bound, $s_{\\text{up}}$**: For $s  \\hat{s}$, the function $q(s)$ is monotonically increasing. The upper bound $s_{\\text{up}}$ is the root of $f(s)=0$ for $s  \\hat{s}$. To find this root, we first establish a bracket $[\\ s_a, s_b ]$ such that $f(s_a)  0$ and $f(s_b)  0$. We can set $s_a = \\hat{s}$ (since $f(\\hat{s}) = -c_\\alpha  0$). Then, we search for a suitable $s_b$ by starting with a reasonable guess (e.g., $s_{\\text{guess}} = \\hat{s} + \\sqrt{\\hat{s}+b+1}$) and expanding the search range (e.g., by doubling the step size) until $f(s_b)  0$. Once the bracket is found, the same numerical root-finder is used.\n\n**4. Monte Carlo Simulation for Empirical Coverage**\n\nThe purpose of the simulation is to check if the nominal confidence level $(1-\\alpha)$ matches the actual fraction of times the procedure yields an interval containing the true value of $s$. This fraction is the empirical coverage. The procedure is as follows for each test case defined by a true signal $s_{\\text{true}}$, background $b$, and confidence level $(1-\\alpha)$:\n1. Fix the parameters $s_{\\text{true}}$, $b$, and $\\alpha$. Pre-calculate the critical value $c_\\alpha$.\n2. Initialize a random number generator with a fixed seed for reproducibility.\n3. Repeat $N$ times (for $N$ Monte Carlo \"toy\" experiments):\n    a. Generate a single pseudo-observation $n$ by drawing from a Poisson distribution with mean $\\mu_{\\text{true}} = s_{\\text{true}} + b$.\n    b. For this generated $n$, calculate the confidence interval $[s_{\\text{low}}, s_{\\text{up}}]$ using the numerical procedure described in the previous section.\n    c. Check if the true signal value lies within this computed interval: $s_{\\textlow} \\le s_{\\text{true}} \\le s_{\\text{up}}$.\n4. The empirical coverage is the total number of successful \"coverages\" (from step 3c) divided by the total number of repetitions, $N$.\n5. This process is repeated for each value of $s_{\\text{true}}$ in the specified grid for each test case.\n\nThe final Python implementation encapsulates these steps into distinct functions: one for the log-likelihood, one to construct the test statistic $q(s)$, one to compute the confidence interval for a given observation, and a main simulation function to calculate the empirical coverage over many repetitions. This modular design directly reflects the principled derivation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\nfrom scipy.optimize import brentq\n\n# Define global seed for reproducibility as required by the problem\nSEED = 123456\n\ndef log_likelihood(s, n, b):\n    \"\"\"Computes the log-likelihood l(s|n) = n*log(s+b) - (s+b).\"\"\"\n    mu = s + b\n    # Use a small epsilon to avoid log(0) issues when s+b is effectively zero\n    if mu = 1e-15:\n        if n == 0:\n            return 0.0\n        else:\n            return -np.inf  # Effectively infinite -log(L)\n    return n * np.log(mu) - mu\n\ndef get_q_s_and_s_hat(n, b):\n    \"\"\"Returns a function for the test statistic q(s) and the MLE s_hat.\"\"\"\n    s_hat = max(0.0, n - b)\n    ll_s_hat = log_likelihood(s_hat, n, b)\n\n    def q_s(s):\n        \"\"\"Computes q(s) = 2 * (l(s_hat) - l(s)).\"\"\"\n        if s  0:\n            # For root finder robustness, signal strength s must be non-negative\n            return np.inf\n        \n        ll_s = log_likelihood(s, n, b)\n        if not np.isfinite(ll_s):\n            return np.inf # If log_likelihood is -inf, q becomes +inf\n            \n        test_statistic = 2 * (ll_s_hat - ll_s)\n        # By definition q(s) = 0. Floating point precision might result in a small negative value.\n        return max(0.0, test_statistic)\n        \n    return q_s, s_hat\n\ndef compute_interval(n, b, c_alpha):\n    \"\"\"\n    Computes the (1-alpha) confidence interval [s_low, s_up] for a given\n    observation n, background b, and critical value c_alpha.\n    \"\"\"\n    q_s, s_hat = get_q_s_and_s_hat(n, b)\n\n    def root_func(s):\n        return q_s(s) - c_alpha\n\n    # --- Lower bound s_low ---\n    s_low = 0.0\n    # A root for the lower bound only exists if s_hat  0 and q(0)  c_alpha.\n    if s_hat  1e-9:\n        if root_func(0.0)  1e-9: # Add tolerance for FP comparison\n            try:\n                # The root is between 0 and s_hat. brentq requires opposite signs.\n                # root_func(0)  0 and root_func(s_hat) = -c_alpha  0\n                s_low = brentq(root_func, 0.0, s_hat)\n            except ValueError:\n                # Fallback for rare numerical precision issues at the boundaries.\n                if abs(root_func(0.0))  1e-9: s_low = 0.0\n                elif abs(root_func(s_hat))  1e-9: s_low = s_hat\n                else: s_low = 0.0\n\n    # --- Upper bound s_up ---\n    # The upper root is always  s_hat. Start search for a bracket [s_a, s_b].\n    s_a = s_hat\n    \n    # Find an upper search bracket s_b where root_func(s_b)  0.\n    # The step size should scale with the expected width. sqrt(s_hat+b) ~ sqrt(n) is a good choice.\n    step = np.sqrt(s_hat + b + 1.0)\n    s_b = s_hat + step\n    \n    # Exponentially increase search range until bracket is found.\n    max_bracket_iters = 30\n    for _ in range(max_bracket_iters):\n        if root_func(s_b)  0:\n            break\n        step *= 2.0\n        s_b = s_hat + step\n    else: # This else executes if the for-loop completes without a break.\n        return s_low, np.inf\n\n    try:\n        s_up = brentq(root_func, s_a, s_b)\n    except ValueError:\n        # Fallback for rare numerical precision issues at the boundaries.\n        if abs(root_func(s_a))  1e-9: s_up = s_a\n        elif abs(root_func(s_b))  1e-9: s_up = s_b\n        else: s_up = s_b\n    \n    return s_low, s_up\n\n\ndef run_mc_for_coverage(s_true_grid, b, alpha, N, rng):\n    \"\"\"\n    Runs the Monte Carlo simulation to estimate empirical coverage.\n    \"\"\"\n    c_alpha = chi2.ppf(1.0 - alpha, df=1)\n    coverages = []\n\n    for s_true in s_true_grid:\n        mu_true = s_true + b\n        \n        # Generate all toy observations at once for efficiency\n        observations = rng.poisson(lam=mu_true, size=N)\n        \n        coverage_count = 0\n        for n_obs in observations:\n            s_low, s_up = compute_interval(n_obs, b, c_alpha)\n            # Check if the true signal value is within the computed interval\n            if s_low = s_true = s_up:\n                coverage_count += 1\n        \n        coverages.append(coverage_count / N)\n    \n    return coverages\n\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the solver, and prints the final result in the specified format.\n    \"\"\"\n    test_cases = [\n        # Case A: very low background\n        {'b': 0.5, 'alpha': 0.05, 's_grid': [0.0, 0.2, 0.5, 1.0, 2.0], 'N': 4000},\n        # Case B: moderate background\n        {'b': 3.0, 'alpha': 0.05, 's_grid': [0.0, 1.0, 2.0, 5.0], 'N': 4000},\n        # Case C: no background, boundary-dominated\n        {'b': 0.0, 'alpha': 0.10, 's_grid': [0.0, 0.5, 1.0], 'N': 6000},\n    ]\n\n    # Initialize the random number generator with the specified seed for reproducibility\n    rng = np.random.default_rng(SEED)\n    \n    all_results = []\n    for case in test_cases:\n        results = run_mc_for_coverage(\n            s_true_grid=case['s_grid'],\n            b=case['b'],\n            alpha=case['alpha'],\n            N=case['N'],\n            rng=rng\n        )\n        all_results.append(results)\n\n    # Required output format: [[c_A_1,...],[c_B_1,...],[c_C_1,...]]\n    # Using str() on a list of lists produces this format but with spaces, which are then removed.\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n\n```", "id": "3517351"}, {"introduction": "Having established that standard asymptotic methods can falter near physical boundaries [@problem_id:3517351], we now turn to a sophisticated solution developed within high-energy physics to address this very issue. This practice delves into the Feldman-Cousins unified approach, a celebrated method for constructing confidence intervals that respects physical constraints and guarantees correct frequentist coverage. By implementing the construction of acceptance regions based on its unique likelihood-ratio ordering rule, you will gain a deep, principle-based understanding of how this procedure prevents unphysical results and has become a standard for reporting experimental limits.", "problem": "Consider a counting experiment commonly encountered in computational high-energy physics, where an observed event count $n \\in \\{0,1,2,\\dots\\}$ arises from a Poisson process with mean $\\mu = s + b$. The signal strength $s$ is an unknown nonnegative parameter subject to the physical boundary $s \\ge 0$, and the background $b \\ge 0$ is known. The goal is to study the coverage of unified Feldman–Cousins (FC) confidence intervals near the boundary $s \\ge 0$ and to connect the observed boundary behavior to the ordering properties intrinsic to the FC construction.\n\nDefinitions and requirements:\n- The data model is $N \\sim \\mathrm{Poisson}(s + b)$.\n- For each hypothesized $s \\ge 0$, define the likelihood $L(n \\mid s) \\propto (s+b)^n e^{-(s+b)}$ for $n \\in \\{0,1,2,\\dots\\}$.\n- Let $\\hat{s}(n)$ denote the constrained maximum likelihood estimate (MLE) of $s$ given $n$ under the constraint $s \\ge 0$. For the Poisson model with known $b$, this is $\\hat{s}(n) = \\max\\{0, n - b\\}$.\n- The Feldman–Cousins (FC) unified ordering uses the likelihood ratio\n$$\nR(n; s) \\equiv \\frac{L(n \\mid s)}{L\\bigl(n \\mid \\hat{s}(n)\\bigr)},\n$$\nand constructs, for each fixed $s$, an acceptance region $\\mathcal{A}_s \\subset \\{0,1,2,\\dots\\}$ by ranking outcomes $n$ from largest to smallest $R(n; s)$ and including outcomes until the cumulative probability under $N \\sim \\mathrm{Poisson}(s + b)$ meets or exceeds the target confidence level (CL). If multiple outcomes share the same likelihood ratio value at the inclusion threshold (a tie), include the entire tied group to avoid arbitrary tie-breaking. This yields a nonrandomized acceptance region with probability at least the target confidence level under $s$.\n- The confidence set reported by the FC construction for an observed $n_{\\mathrm{obs}}$ is the set of all $s$ whose acceptance region $\\mathcal{A}_s$ contains $n_{\\mathrm{obs}}$.\n- The coverage at a fixed true signal $s_0$ is the long-run probability that the FC interval constructed from a draw $N \\sim \\mathrm{Poisson}(s_0 + b)$ contains $s_0$. Compute this coverage exactly by summing probabilities over the count outcomes that lie in the acceptance region for $s_0$. Do not use Monte Carlo simulation. Truncate the infinite Poisson sum by choosing an upper summation limit $n_{\\max}$ large enough that the omitted Poisson tail probability is at most $10^{-15}$; justify this truncation numerically in the code and ensure the acceptance-region construction accounts for ties at the truncation boundary so that the final coverage is accurate to within $10^{-12}$ in absolute error.\n\nTasks:\n1. Implement a program that, for each test case $(s_0, b, \\mathrm{CL})$, constructs the FC acceptance region $\\mathcal{A}_{s_0}$ under the ordering rule above and computes the coverage \n$$\n\\mathrm{Cov}(s_0; b, \\mathrm{CL}) = \\sum_{n \\in \\mathcal{A}_{s_0}} \\Pr\\{N=n \\mid s_0 + b\\},\n$$\nusing exact Poisson probabilities and numerically safe handling of underflow via logarithms if necessary. Use a deterministic tie-handling rule that includes all outcomes sharing the same likelihood ratio at the threshold to ensure nonrandomized coverage that is at least the target confidence level.\n2. Ensure scientific realism and numerical robustness by:\n   - Constructing the acceptance region using the complete ranking by $R(n; s_0)$, grouping exactly equal ratios within a numerical tolerance of $10^{-12}$ in the logarithm of the ratio.\n   - Truncating the Poisson support at $n_{\\max}$ so that the neglected tail probability under $N \\sim \\mathrm{Poisson}(s_0 + b)$ is at most $10^{-15}$, and adding a small integer safety margin to $n_{\\max}$ to guarantee that no tied group is artificially split at the boundary.\n   - Verifying that the resulting cumulative probability of the constructed acceptance set is at least the target $\\mathrm{CL}$ up to a tolerance of $10^{-12}$; if not, adaptively increase $n_{\\max}$ until this condition is met.\n3. Explain, in your solution write-up, how the ordering rule interacts with the boundary condition $s \\ge 0$, and why this leads to characteristic coverage behavior near $s \\approx 0$.\n\nTest suite:\nCompute the coverage for the following parameter sets $(s_0, b, \\mathrm{CL})$, where all numbers are real and given as decimals:\n- $(0.0, 3.0, 0.90)$\n- $(0.1, 3.0, 0.90)$\n- $(0.5, 3.0, 0.90)$\n- $(1.0, 3.0, 0.90)$\n- $(0.0, 0.5, 0.90)$\n- $(0.0, 3.0, 0.95)$\n- $(3.0, 3.0, 0.90)$\n\nFinal output format:\n- Your program should produce a single line of output containing the coverage values for the test suite in the same order, rounded to six digits after the decimal point, as a comma-separated list enclosed in square brackets, for example, $\\texttt{[0.912345,0.901234,\\dots]}$.\n- There are no physical units to report. Angles are not involved. All reported coverage values must be decimals.", "solution": "The problem requires the computation of the coverage of Feldman-Cousins (FC) confidence intervals for a Poisson process with a known background, particularly focusing on the behavior near the physical boundary of the signal parameter $s=0$. The solution involves a precise implementation of the FC construction for acceptance regions and an explanation of the underlying statistical principles.\n\n### Principle-Based Design\n\nThe core of the problem lies in the construction of the acceptance region $\\mathcal{A}_s$ for a given signal hypothesis $s_0$. This construction is governed by the Feldman-Cousins ordering principle, which ranks possible observed outcomes $n$ based on a likelihood ratio.\n\n1.  **The Model and Likelihoods**: The number of observed events $n$ follows a Poisson distribution with mean $\\mu = s + b$, where $s$ is the signal strength and $b$ is the known background. The probability mass function (PMF) is $P(n \\mid s, b) = \\frac{e^{-(s+b)}(s+b)^n}{n!}$. The likelihood function, seen as a function of $s$ for a fixed observation $n$, is $L(n \\mid s) \\propto (s+b)^n e^{-(s+b)}$.\n\n2.  **The Ordering Principle**: The FC method orders the outcomes $n$ for a fixed hypothesized signal $s$ using the likelihood ratio:\n    $$\n    R(n; s) = \\frac{L(n \\mid s)}{L(n \\mid \\hat{s}(n))}\n    $$\n    where $\\hat{s}(n)$ is the maximum likelihood estimate (MLE) of $s$ for the observation $n$, subject to the physical constraint $s \\ge 0$. For a Poisson mean $\\mu = s+b$, the unconstrained MLE of $\\mu$ is $n$, leading to an unconstrained estimate for $s$ of $n-b$. Incorporating the constraint $s \\ge 0$ gives the constrained MLE:\n    $$\n    \\hat{s}(n) = \\max\\{0, n - b\\}\n    $$\n    The denominator of the ratio, $L(n \\mid \\hat{s}(n))$, represents the likelihood of the data $n$ under the best-fit physically-allowed hypothesis. A larger ratio $R(n; s)$ indicates that the hypothesized $s$ explains the data $n$ relatively well compared to the best possible explanation.\n\n3.  **Numerical Calculation of the Ratio**: To maintain numerical stability, we work with the logarithm of the ratio. The term proportional to $1/n!$ in the likelihood cancels, leaving:\n    $$\n    \\log R(n; s) = \\log\\left( \\frac{(s+b)^n e^{-(s+b)}}{(\\hat{s}(n)+b)^n e^{-(\\hat{s}(n)+b)}} \\right) = n \\log\\left(\\frac{s+b}{\\hat{s}(n)+b}\\right) - (s - \\hat{s}(n))\n    $$\n    Let $\\mu_0 = s_0 + b$ be the hypothesized mean and $\\hat{\\mu}(n) = \\hat{s}(n) + b = \\max\\{b, n\\}$ be the best-fit mean for an observation $n$. The log-ratio for a test hypothesis $s_0$ is:\n    $$\n    \\log R(n; s_0) = n \\log\\left(\\frac{\\mu_0}{\\hat{\\mu}(n)}\\right) - (\\mu_0 - \\hat{\\mu}(n))\n    $$\n    This form is robust as long as $\\mu_0  0$ and $b \\ge 0$ (implying $\\hat{\\mu}(n)  0$ for the test cases provided).\n\n4.  **Constructing the Acceptance Region $\\mathcal{A}_{s_0}$**:\n    a.  **Define a Search Space for $n$**: The Poisson distribution is defined for all $n \\in \\{0, 1, 2, \\dots\\}$. We must truncate this infinite set. We find an integer $n_{\\max}$ such that the tail probability $\\sum_{n=n_{\\max}+1}^{\\infty} P(n \\mid s_0, b) \\le 10^{-15}$. This can be found using the inverse survival function of the Poisson distribution. A small safety margin is added to $n_{\\max}$ to ensure that any group of outcomes with tied ranks at the cutoff is not artificially split.\n    b.  **Rank Outcomes**: For each integer $n$ from $0$ to $n_{\\max}$, we calculate the log-ratio $\\log R(n; s_0)$.\n    c.  **Group and Sort by Rank**: We identify unique values of $\\log R(n; s_0)$ (within a numerical tolerance of $10^{-12}$) and sort these unique ranks in descending order.\n    d.  **Accumulate Probabilities**: We iterate through the sorted ranks. For each rank, we collect all $n$ values that share this rank. We add the sum of their corresponding Poisson probabilities, $P(n \\mid s_0, b)$, to a running cumulative probability. We add all these $n$ values to the acceptance region $\\mathcal{A}_{s_0}$. The process stops as soon as the cumulative probability meets or exceeds the target confidence level (CL). The tie-handling rule, which requires including the entire group of outcomes at the threshold rank, ensures the coverage is at least CL.\n\n5.  **Calculating Coverage**: The coverage for a true signal $s_0$ is the probability that an experiment will yield an outcome $n$ that falls within the acceptance region $\\mathcal{A}_{s_0}$. By construction, this is precisely the final cumulative probability calculated in the previous step.\n    $$\n    \\mathrm{Cov}(s_0; b, \\mathrm{CL}) = \\sum_{n \\in \\mathcal{A}_{s_0}} P(N=n \\mid s_0 + b)\n    $$\n\n### Interaction with the Boundary Condition $s \\ge 0$\n\nThe distinctive behavior of the FC procedure near the physical boundary $s=0$ is a direct consequence of the ordering rule's interaction with the constrained MLE, $\\hat{s}(n)$.\n\n-   **At the Boundary ($s_0=0$)**: The hypothesis is that there is no signal. The hypothesized mean is $\\mu_0 = b$.\n    -   For observed counts $n \\le b$, the best-fit signal is $\\hat{s}(n) = \\max\\{0, n-b\\} = 0$. The best-fit hypothesis is identical to the null hypothesis ($s=0$).\n    -   Consequently, the likelihood ratio is $R(n; s_0=0) = 1$ for all $n \\le \\lfloor b \\rfloor$.\n    -   This creates a massive tie: all outcomes $n \\in \\{0, 1, \\dots, \\lfloor b \\rfloor\\}$ have the highest possible rank.\n    -   According to the tie-handling rule, the construction of $\\mathcal{A}_{s_0=0}$ must begin by including this entire block of outcomes. The initial cumulative probability is $\\sum_{n=0}^{\\lfloor b \\rfloor} P(n \\mid b)$. If this sum is less than the confidence level CL, more outcomes (with $n  b$, for which $R  1$) are added based on their rank until the CL is exceeded.\n    -   Because a large block of probability mass is added at the first step, the final cumulative probability—the coverage—is often significantly greater than the nominal CL. This effect is known as over-coverage.\n\n-   **Away from the Boundary ($s_0  0$)**: Even for a very small positive signal $s_0  0$, the perfect tie is broken.\n    -   The hypothesized mean is $\\mu_0 = s_0 + b$.\n    -   For $n \\le b$, we still have $\\hat{s}(n) = 0$. The log-ratio becomes $\\log R(n; s_0) = n\\log\\left(\\frac{s_0+b}{b}\\right) - s_0$. Since $\\log((s_0+b)/b)0$, this is a strictly decreasing linear function of $n$.\n    -   The outcomes $n=0, 1, \\dots, \\lfloor b \\rfloor$ are now uniquely ranked: $n=0$ is highest, then $n=1$, and so on.\n    -   The acceptance region can be built by adding outcomes one by one. This finer granularity allows the cumulative probability to stop much closer to the nominal CL.\n    -   As a result, the coverage for $s_0  0$ drops sharply from the over-coverage at $s_0=0$ to a value very close to (but guaranteed to be at least) CL.\n\nThis behavior demonstrates the key feature of the Feldman-Cousins method: it automatically transitions from one-sided intervals at the boundary (where it protects against falsely excluding $s=0$) to two-sided intervals away from the boundary, providing correct coverage across the entire parameter space without flip-flopping. The over-coverage at $s=0$ is the price paid for strictly enforcing the frequentist coverage principle at the boundary without producing empty confidence sets for low $n$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\nfrom collections import defaultdict\n\ndef solve():\n    \"\"\"\n    Computes the coverage of Feldman-Cousins confidence intervals for a Poisson\n    process with known background, for a suite of test cases.\n    \"\"\"\n    \n    # Test cases: (s0, b, CL)\n    test_cases = [\n        (0.0, 3.0, 0.90),\n        (0.1, 3.0, 0.90),\n        (0.5, 3.0, 0.90),\n        (1.0, 3.0, 0.90),\n        (0.0, 0.5, 0.90),\n        (0.0, 3.0, 0.95),\n        (3.0, 3.0, 0.90),\n    ]\n\n    results = []\n    for s0, b, cl in test_cases:\n        coverage = compute_fc_coverage(s0, b, cl)\n        results.append(f\"{coverage:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef compute_fc_coverage(s0, b, cl):\n    \"\"\"\n    Calculates the exact coverage for a given signal hypothesis s0, background b,\n    and confidence level CL.\n\n    Args:\n        s0 (float): The true (hypothesized) signal strength.\n        b (float): The known background.\n        cl (float): The target confidence level.\n\n    Returns:\n        float: The exact coverage probability.\n    \"\"\"\n    mu0 = s0 + b\n    \n    # Determine the upper limit for n summation.\n    # We choose n_max such that the omitted Poisson tail probability is at most 1e-15.\n    # P(N  n_max) = 1 - P(N = n_max), so we want P(N = n_max) = 1 - 1e-15.\n    # The Percent Point Function (ppf) is the inverse of the CDF.\n    if mu0  0:\n        n_max_crit = stats.poisson.ppf(1 - 1e-15, mu0)\n    else: # Handle mu0=0 case (only n=0 has non-zero probability)\n        n_max_crit = 5\n        \n    # Add a safety margin to ensure no tied group is split at the boundary.\n    # This also helps satisfy the a-posteriori check on coverage = CL.\n    n_max = int(n_max_crit) + 20\n    \n    n_values = np.arange(0, n_max + 1)\n\n    # Calculate the constrained MLE for s and the corresponding mean mu_hat.\n    s_hat = np.maximum(0, n_values - b)\n    mu_hat = s_hat + b  # This is equivalent to np.maximum(b, n_values)\n\n    # Calculate the log of the likelihood ratio R(n; s0) for ordering.\n    # log R = n*log(mu0/mu_hat) - (mu0-mu_hat)\n    # Handle mu_hat=0: occurs only if b=0 and n=0. Test cases have b  0, except for s0=0, b=0 case.\n    # In provided tests, b is always > 0, so mu_hat is always > 0.\n    log_mu0 = np.log(mu0) if mu0  0 else -np.inf\n    log_mu_hat = np.log(mu_hat)\n    \n    log_R = n_values * (log_mu0 - log_mu_hat) - (mu0 - mu_hat)\n\n    # Get the Poisson probabilities for each n under the hypothesis s0.\n    poisson_probs = stats.poisson.pmf(n_values, mu0)\n\n    # Group outcomes n by their rank (log_R value) to handle ties correctly.\n    # A tolerance of 1e-12 is used as per the problem description.\n    # We group by rounding the log_R value to a certain precision.\n    tie_tolerance = 1e-12\n    # Using integer casting after scaling is a robust way to group floats.\n    scaled_log_R = np.round(log_R / tie_tolerance).astype(np.int64)\n    \n    # Use a dictionary to group probabilities by their rank (scaled_log_R value).\n    # defaultdict is convenient for this.\n    ranked_prob_groups = defaultdict(float)\n    for i in range(len(n_values)):\n        rank_val = scaled_log_R[i]\n        ranked_prob_groups[rank_val] += poisson_probs[i]\n        \n    # Sort the ranks in descending order.\n    sorted_ranks = sorted(ranked_prob_groups.keys(), reverse=True)\n\n    # Build the acceptance region by adding groups of tied ranks\n    # until the cumulative probability exceeds the confidence level.\n    cumulative_prob = 0.0\n    for rank in sorted_ranks:\n        prob_of_group = ranked_prob_groups[rank]\n        if cumulative_prob + prob_of_group  cl and not np.isclose(cumulative_prob + prob_of_group, cl):\n            cumulative_prob += prob_of_group\n        else:\n            # This is the final group to add. Include it entirely.\n            cumulative_prob += prob_of_group\n            break\n            \n    # The final cumulative probability is the coverage.\n    # A check ensures that the implementation correctly yields coverage = CL.\n    # Small floating point errors can make it slightly less, so a small\n    # tolerance is used in the assertion.\n    assert cumulative_prob = cl - 1e-12, f\"Coverage {cumulative_prob} is less than CL {cl}\"\n    \n    return cumulative_prob\n\nsolve()\n```", "id": "3517293"}]}