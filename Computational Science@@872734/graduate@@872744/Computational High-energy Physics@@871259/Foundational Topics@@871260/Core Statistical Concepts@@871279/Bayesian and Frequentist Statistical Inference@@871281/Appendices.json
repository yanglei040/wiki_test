{"hands_on_practices": [{"introduction": "A cornerstone of frequentist inference in high-energy physics is the method of maximum likelihood estimation (MLE). This practice provides a concrete application of MLE to a scenario ubiquitous in particle physics: a counting experiment where the signal of interest is accompanied by a background process. Here, the background is not perfectly known but is constrained by an independent auxiliary measurement, introducing what is known as a nuisance parameter. By working through this exercise [@problem_id:3506272], you will derive the maximum likelihood estimates for both the signal and background yields and, crucially, compute their asymptotic covariance matrix, which quantifies their uncertainties and correlation.", "problem": "A single-bin counting experiment is conducted in a search for a heavy resonance in the dimuon channel at the Large Hadron Collider (LHC). The analysis proceeds under the following generative model: the observed count in the signal region, denoted by $n$, is assumed to follow a Poisson distribution with mean $s + b$, where $s \\ge 0$ is the unknown mean signal yield and $b \\ge 0$ is the unknown mean background yield. An independent auxiliary background calibration produces a single Gaussian measurement $x$ of the background with known standard deviation $\\sigma$, modeled as $x \\sim \\mathcal{N}(b, \\sigma^{2})$. Independence is assumed between the Poisson count and the Gaussian calibration conditional on $b$ and $s$.\n\nThe well-tested facts and definitions to be used are: the Poisson probability mass function $P(n \\mid \\lambda) = \\exp(-\\lambda)\\lambda^{n}/n!$ for integer $n \\ge 0$ and mean $\\lambda > 0$, the Gaussian probability density $f(x \\mid \\mu,\\sigma^{2}) = (2\\pi\\sigma^{2})^{-1/2}\\exp\\!\\big(- (x - \\mu)^{2}/(2\\sigma^{2})\\big)$ for $x \\in \\mathbb{R}$, and the definition of the maximum likelihood estimate (MLE) as the parameter value that maximizes the likelihood function. The asymptotic covariance matrix of the MLE is obtained by inverting the observed information matrix, defined as the negative Hessian (matrix of second derivatives) of the log-likelihood evaluated at the MLE, under standard regularity conditions.\n\nYou are given the observed data $n = 35$, the auxiliary measurement $x = 28$, and the known standard deviation $\\sigma = 6$. Starting only from the above definitions and facts, derive the joint MLEs for $(s,b)$ and obtain their asymptotic covariance matrix by inverting the observed information at the MLE. Assume the interior solution $s > 0$ and $b > 0$ is valid for the provided data. Express your final answer as a single row matrix containing, in order, $s^{\\ast}$, $b^{\\ast}$, $\\operatorname{Var}(s^{\\ast})$, $\\operatorname{Cov}(s^{\\ast}, b^{\\ast})$, $\\operatorname{Cov}(b^{\\ast}, s^{\\ast})$, and $\\operatorname{Var}(b^{\\ast})$. No rounding is required, and no physical units are to be included in the final numeric values.", "solution": "The problem statement is formally validated and found to be self-contained, scientifically grounded, and well-posed. All necessary data, models, and definitions are provided, and there are no internal contradictions or ambiguities. The problem is a standard application of maximum likelihood estimation in a context representative of data analysis in high-energy physics. We may therefore proceed with the solution.\n\nThe goal is to find the maximum likelihood estimates (MLEs) for the parameters $s$ (signal) and $b$ (background), and their asymptotic covariance matrix. The data consist of an observed count $n$ and an auxiliary measurement $x$.\n\nThe statistical model is defined by two components:\n1.  The count $n$ follows a Poisson distribution with mean $s+b$: $n \\sim \\text{Poisson}(s+b)$. The probability mass function is $P(n \\mid s, b) = \\frac{\\exp(-(s+b))(s+b)^n}{n!}$.\n2.  The auxiliary measurement $x$ follows a Gaussian distribution with mean $b$ and known variance $\\sigma^2$: $x \\sim \\mathcal{N}(b, \\sigma^2)$. The probability density function is $f(x \\mid b, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-b)^2}{2\\sigma^2}\\right)$.\n\nGiven that the two measurements are conditionally independent, the joint likelihood function $L(s, b)$ for observing the data pair $(n, x)$ is the product of the individual probability functions:\n$$L(s, b) = P(n \\mid s, b) \\times f(x \\mid b, \\sigma^2) = \\frac{\\exp(-(s+b))(s+b)^n}{n!} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-b)^2}{2\\sigma^2}\\right)$$\nIt is computationally more convenient to work with the natural logarithm of the likelihood, the log-likelihood function $\\ell(s, b) = \\ln L(s, b)$:\n$$\\ell(s, b) = -(s+b) + n\\ln(s+b) - \\ln(n!) - \\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(x-b)^2}{2\\sigma^2}$$\nTo find the MLEs, we maximize this function with respect to $s$ and $b$. We can disregard the terms $-\\ln(n!)$ and $-\\frac{1}{2}\\ln(2\\pi\\sigma^2)$ as they are constants that do not depend on the parameters $s$ and $b$. The relevant part of the log-likelihood is:\n$$\\ell(s, b) = -(s+b) + n\\ln(s+b) - \\frac{(x-b)^2}{2\\sigma^2}$$\nThe MLEs, denoted $s^*$ and $b^*$, are found by solving the system of equations where the first partial derivatives of $\\ell(s, b)$ are zero:\n$$\\frac{\\partial\\ell}{\\partial s} = 0 \\quad \\text{and} \\quad \\frac{\\partial\\ell}{\\partial b} = 0$$\nLet's compute the partial derivatives:\n$$\\frac{\\partial\\ell}{\\partial s} = -1 + \\frac{n}{s+b}$$\n$$\\frac{\\partial\\ell}{\\partial b} = -1 + \\frac{n}{s+b} + \\frac{x-b}{\\sigma^2}$$\nSetting the first equation to zero gives:\n$$-1 + \\frac{n}{s^*+b^*} = 0 \\implies s^*+b^* = n$$\nSubstituting this result into the second equation set to zero:\n$$-1 + \\frac{n}{n} + \\frac{x-b^*}{\\sigma^2} = 0 \\implies -1 + 1 + \\frac{x-b^*}{\\sigma^2} = 0 \\implies \\frac{x-b^*}{\\sigma^2} = 0 \\implies b^* = x$$\nUsing this result for $b^*$ in the first condition, we find $s^*$:\n$$s^* + x = n \\implies s^* = n-x$$\nThe MLEs are $s^* = n-x$ and $b^*=x$. We are given $n=35$ and $x=28$.\n$$s^* = 35 - 28 = 7$$\n$$b^* = 28$$\nThe problem states to assume an interior solution ($s>0, b>0$), which is consistent with these results.\n\nNext, we derive the asymptotic covariance matrix of the MLEs. This is given by the inverse of the observed information matrix, $I(s^*, b^*)$, which is the negative of the Hessian matrix of the log-likelihood, evaluated at the MLEs. The Hessian matrix $H$ is the matrix of second partial derivatives.\n\nLet's compute the second partial derivatives of $\\ell(s,b)$:\n$$\\frac{\\partial^2\\ell}{\\partial s^2} = \\frac{\\partial}{\\partial s} \\left(-1 + \\frac{n}{s+b}\\right) = -\\frac{n}{(s+b)^2}$$\n$$\\frac{\\partial^2\\ell}{\\partial b^2} = \\frac{\\partial}{\\partial b} \\left(-1 + \\frac{n}{s+b} + \\frac{x-b}{\\sigma^2}\\right) = -\\frac{n}{(s+b)^2} - \\frac{1}{\\sigma^2}$$\n$$\\frac{\\partial^2\\ell}{\\partial s \\partial b} = \\frac{\\partial}{\\partial b} \\left(-1 + \\frac{n}{s+b}\\right) = -\\frac{n}{(s+b)^2}$$\nBy Clairaut's theorem, $\\frac{\\partial^2\\ell}{\\partial b \\partial s} = \\frac{\\partial^2\\ell}{\\partial s \\partial b}$. The Hessian matrix is:\n$$H(s,b) = \\begin{pmatrix} \\frac{\\partial^2\\ell}{\\partial s^2} & \\frac{\\partial^2\\ell}{\\partial s \\partial b} \\\\ \\frac{\\partial^2\\ell}{\\partial b \\partial s} & \\frac{\\partial^2\\ell}{\\partial b^2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{n}{(s+b)^2} & -\\frac{n}{(s+b)^2} \\\\ -\\frac{n}{(s+b)^2} & -\\frac{n}{(s+b)^2} - \\frac{1}{\\sigma^2} \\end{pmatrix}$$\nWe evaluate this matrix at the MLEs, where $s^*+b^* = n$:\n$$H(s^*, b^*) = \\begin{pmatrix} -\\frac{n}{n^2} & -\\frac{n}{n^2} \\\\ -\\frac{n}{n^2} & -\\frac{n}{n^2} - \\frac{1}{\\sigma^2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{n} & -\\frac{1}{n} \\\\ -\\frac{1}{n} & -\\frac{1}{n} - \\frac{1}{\\sigma^2} \\end{pmatrix}$$\nThe observed information matrix at the MLE is $I(s^*, b^*) = -H(s^*, b^*)$:\n$$I(s^*, b^*) = \\begin{pmatrix} \\frac{1}{n} & \\frac{1}{n} \\\\ \\frac{1}{n} & \\frac{1}{n} + \\frac{1}{\\sigma^2} \\end{pmatrix}$$\nThe asymptotic covariance matrix $C$ is the inverse of $I(s^*, b^*)$. For a $2 \\times 2$ matrix $A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, its inverse is $A^{-1} = \\frac{1}{ad-bc}\\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\nThe determinant of $I(s^*, b^*)$ is:\n$$\\det(I) = \\left(\\frac{1}{n}\\right)\\left(\\frac{1}{n} + \\frac{1}{\\sigma^2}\\right) - \\left(\\frac{1}{n}\\right)\\left(\\frac{1}{n}\\right) = \\frac{1}{n^2} + \\frac{1}{n\\sigma^2} - \\frac{1}{n^2} = \\frac{1}{n\\sigma^2}$$\nNow we compute the inverse matrix $C$:\n$$C = [I(s^*, b^*)]^{-1} = \\frac{1}{1/(n\\sigma^2)} \\begin{pmatrix} \\frac{1}{n} + \\frac{1}{\\sigma^2} & -\\frac{1}{n} \\\\ -\\frac{1}{n} & \\frac{1}{n} \\end{pmatrix} = n\\sigma^2 \\begin{pmatrix} \\frac{\\sigma^2+n}{n\\sigma^2} & -\\frac{1}{n} \\\\ -\\frac{1}{n} & \\frac{1}{n} \\end{pmatrix}$$\n$$C = \\begin{pmatrix} n\\sigma^2 \\left(\\frac{\\sigma^2+n}{n\\sigma^2}\\right) & n\\sigma^2 \\left(-\\frac{1}{n}\\right) \\\\ n\\sigma^2 \\left(-\\frac{1}{n}\\right) & n\\sigma^2 \\left(\\frac{1}{n}\\right) \\end{pmatrix} = \\begin{pmatrix} n+\\sigma^2 & -\\sigma^2 \\\\ -\\sigma^2 & \\sigma^2 \\end{pmatrix}$$\nThe covariance matrix $C$ contains the variances and covariances of the estimators:\n$$C = \\begin{pmatrix} \\operatorname{Var}(s^*) & \\operatorname{Cov}(s^*, b^*) \\\\ \\operatorname{Cov}(b^*, s^*) & \\operatorname{Var}(b^*) \\end{pmatrix}$$\nSo, we have:\n$\\operatorname{Var}(s^*) = n+\\sigma^2$\n$\\operatorname{Cov}(s^*, b^*) = \\operatorname{Cov}(b^*, s^*) = -\\sigma^2$\n$\\operatorname{Var}(b^*) = \\sigma^2$\n\nWe are given $n=35$ and $\\sigma=6$, so $\\sigma^2=36$. Substituting these values:\n$\\operatorname{Var}(s^*) = 35 + 36 = 71$\n$\\operatorname{Cov}(s^*, b^*) = -36$\n$\\operatorname{Var}(b^*) = 36$\n\nThe required six values to be reported in a single row matrix are $s^{\\ast}$, $b^{\\ast}$, $\\operatorname{Var}(s^{\\ast})$, $\\operatorname{Cov}(s^{\\ast}, b^{\\ast})$, $\\operatorname{Cov}(b^{\\ast}, s^{\\ast})$, and $\\operatorname{Var}(b^{\\ast})$.\nThe values are:\n$s^* = 7$\n$b^* = 28$\n$\\operatorname{Var}(s^*) = 71$\n$\\operatorname{Cov}(s^*, b^*) = -36$\n$\\operatorname{Cov}(b^*, s^*) = -36$\n$\\operatorname{Var}(b^*) = 36$\nThese will be presented in the specified format.", "answer": "$$\n\\boxed{\\begin{pmatrix} 7 & 28 & 71 & -36 & -36 & 36 \\end{pmatrix}}\n$$", "id": "3506272"}, {"introduction": "Moving beyond point estimation, this practice explores the construction and interpretation of statistical intervals, revealing fundamental differences between the frequentist and Bayesian paradigms. You will begin by deriving the Jeffreys prior for a binomial efficiency parameter $\\epsilon$, a key example of an objective prior derived from principles of information geometry. This exercise [@problem_id:3506258] then pits the resulting Bayesian credible interval against the classic frequentist Clopper-Pearson confidence interval, using the frequentist metric of coverage to systematically compare their performance. This analysis highlights the different philosophical guarantees that each type of interval provides.", "problem": "You are given a binomial model where the number of observed successes $k$ in $n$ trials is modeled as $k \\sim \\mathrm{Bin}(n,\\epsilon)$ with an unknown efficiency parameter $\\epsilon \\in [0,1]$. Starting from first principles of statistical inference and information geometry, perform the following tasks:\n\n- Derive the Fisher information $I(\\epsilon)$ for the binomial model with parameter $\\epsilon$ and fixed $n$. Use the definition $I(\\epsilon) = \\mathbb{E}\\left[-\\frac{\\partial^2}{\\partial \\epsilon^2} \\log p(k \\mid n,\\epsilon)\\right]$ where $p(k \\mid n,\\epsilon)$ is the binomial likelihood.\n- Using the definition of Jeffreys prior, $\\pi(\\epsilon) \\propto \\sqrt{I(\\epsilon)}$, derive the form of the Jeffreys prior for $\\epsilon$ in the binomial model.\n- Derive the posterior distribution $p(\\epsilon \\mid k,n)$ under the Jeffreys prior and the binomial likelihood by applying Bayes' theorem. Define the equal-tailed Bayesian credible interval at nominal level $1-\\alpha$ as the interval between the $\\alpha/2$ and $1-\\alpha/2$ posterior quantiles.\n- Define the Clopper–Pearson (CP) confidence interval by inverting the binomial cumulative distribution for a two-sided test at level $\\alpha$ and express its endpoints in terms of quantiles of suitable Beta distributions. The CP interval is used as a frequentist benchmark.\n\nImplement a complete program that:\n1. Computes, for each $k \\in \\{0,1,\\dots,n\\}$, the equal-tailed Bayesian credible interval for $\\epsilon$ under the Jeffreys prior at nominal level $1-\\alpha$.\n2. Computes, for each $k \\in \\{0,1,\\dots,n\\}$, the Clopper–Pearson (CP) confidence interval at the same nominal level $1-\\alpha$.\n3. For a specified grid of true efficiencies $\\epsilon$ values, evaluates the exact frequentist coverage of each interval by summing the binomial probabilities of $k$ for which the interval contains the true $\\epsilon$. For a fixed true $\\epsilon$, define coverage as $C(\\epsilon) = \\sum_{k=0}^{n} \\mathbb{I}\\{\\epsilon \\in [L(k),U(k)]\\} \\cdot \\mathrm{Bin}(k;n,\\epsilon)$, where $[L(k),U(k)]$ is the interval computed for $k$, and $\\mathbb{I}\\{\\cdot\\}$ is the indicator function.\n4. For each test case, computes two summary metrics for each interval type: \n   - The worst-case deficit relative to nominal coverage, defined as $\\min_{\\epsilon \\text{ in grid}} \\left( C(\\epsilon) - (1-\\alpha) \\right)$.\n   - The mean absolute coverage error over the grid, defined as $\\frac{1}{M}\\sum_{i=1}^{M} \\left| C(\\epsilon_i) - (1-\\alpha) \\right|$ where $M$ is the number of grid points.\n\nUse the following test suite of parameter values that probe typical conditions and edge cases:\n- Case $1$: $n=10$, $\\alpha=0.1$, and a grid of $21$ efficiencies $\\epsilon \\in \\{0.00,0.05,\\dots,1.00\\}$.\n- Case $2$: $n=20$, $\\alpha=0.1$, and a grid of $11$ efficiencies $\\epsilon \\in \\{0.00,0.10,\\dots,1.00\\}$.\n- Case $3$: $n=20$, $\\alpha=0.32$, and a grid of $21$ efficiencies $\\epsilon \\in \\{0.00,0.05,\\dots,1.00\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list of four floats: $[$worst-case deficit for Bayesian intervals, worst-case deficit for Clopper–Pearson intervals, mean absolute coverage error for Bayesian intervals, mean absolute coverage error for Clopper–Pearson intervals$]$. Aggregate the three test-case result lists into one top-level list and print it. For example, $\\left[\\left[a_1,b_1,c_1,d_1\\right],\\left[a_2,b_2,c_2,d_2\\right],\\left[a_3,b_3,c_3,d_3\\right]\\right]$. All coverage quantities and deficits must be expressed as decimal fractions, with no percentage signs.", "solution": "The problem requires a thorough analysis and comparison of Bayesian credible intervals and frequentist confidence intervals for a binomial proportion parameter $\\epsilon$. The validation confirms that the problem is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The definitions and methodologies are standard in the field of statistical inference.\n\n### Theoretical Derivations\n\nFirst, we derive the necessary theoretical components from first principles as requested.\n\n**1. Fisher Information for the Binomial Model**\n\nThe Fisher information $I(\\epsilon)$ quantifies the amount of information that an observable random variable $k$ carries about an unknown parameter $\\epsilon$. For a binomial model, the likelihood of observing $k$ successes in $n$ trials is given by the probability mass function:\n$$p(k \\mid n,\\epsilon) = \\binom{n}{k} \\epsilon^k (1-\\epsilon)^{n-k}$$\nThe log-likelihood, $\\ell(\\epsilon) = \\log p(k \\mid n,\\epsilon)$, is:\n$$\\ell(\\epsilon) = \\log\\binom{n}{k} + k \\log \\epsilon + (n-k) \\log(1-\\epsilon)$$\nTo find the Fisher information, we compute the second partial derivative of the log-likelihood with respect to $\\epsilon$:\n$$\\frac{\\partial \\ell}{\\partial \\epsilon} = \\frac{k}{\\epsilon} - \\frac{n-k}{1-\\epsilon}$$\n$$\\frac{\\partial^2 \\ell}{\\partial \\epsilon^2} = -\\frac{k}{\\epsilon^2} - \\frac{n-k}{(1-\\epsilon)^2}$$\nThe Fisher information is defined as the expectation of the negative of this second derivative:\n$$I(\\epsilon) = \\mathbb{E}\\left[-\\frac{\\partial^2 \\ell}{\\partial \\epsilon^2}\\right] = \\mathbb{E}\\left[\\frac{k}{\\epsilon^2} + \\frac{n-k}{(1-\\epsilon)^2}\\right]$$\nUsing the linearity of expectation and the fact that $\\mathbb{E}[k] = n\\epsilon$ for a binomial distribution, we get:\n$$I(\\epsilon) = \\frac{\\mathbb{E}[k]}{\\epsilon^2} + \\frac{n-\\mathbb{E}[k]}{(1-\\epsilon)^2} = \\frac{n\\epsilon}{\\epsilon^2} + \\frac{n-n\\epsilon}{(1-\\epsilon)^2}$$\n$$I(\\epsilon) = \\frac{n}{\\epsilon} + \\frac{n(1-\\epsilon)}{(1-\\epsilon)^2} = \\frac{n}{\\epsilon} + \\frac{n}{1-\\epsilon} = \\frac{n(1-\\epsilon) + n\\epsilon}{\\epsilon(1-\\epsilon)}$$\n$$I(\\epsilon) = \\frac{n}{\\epsilon(1-\\epsilon)}$$\n\n**2. Jeffreys Prior for the Binomial Parameter**\n\nThe Jeffreys prior is a non-informative prior derived from the Fisher information, defined as $\\pi(\\epsilon) \\propto \\sqrt{I(\\epsilon)}$. It is designed to be invariant under reparameterization.\nUsing the derived Fisher information:\n$$\\pi(\\epsilon) \\propto \\sqrt{\\frac{n}{\\epsilon(1-\\epsilon)}} \\propto \\frac{1}{\\sqrt{\\epsilon(1-\\epsilon)}} = \\epsilon^{-1/2} (1-\\epsilon)^{-1/2}$$\nThis functional form is proportional to the probability density function of a Beta distribution, $\\mathrm{Beta}(\\epsilon; \\alpha, \\beta) \\propto \\epsilon^{\\alpha-1} (1-\\epsilon)^{\\beta-1}$. By comparing the exponents, we find:\n$$\\alpha - 1 = -1/2 \\implies \\alpha = 1/2$$\n$$\\beta - 1 = -1/2 \\implies \\beta = 1/2$$\nThus, the Jeffreys prior for the binomial parameter $\\epsilon$ is a $\\mathrm{Beta}(1/2, 1/2)$ distribution.\n\n**3. Bayesian Posterior and Credible Interval**\n\nThe posterior distribution $p(\\epsilon \\mid k,n)$ is obtained by applying Bayes' theorem, which states that the posterior is proportional to the product of the likelihood and the prior:\n$$p(\\epsilon \\mid k,n) \\propto p(k \\mid n,\\epsilon) \\pi(\\epsilon)$$\nSubstituting the binomial likelihood and the Jeffreys prior:\n$$p(\\epsilon \\mid k,n) \\propto \\left(\\epsilon^k (1-\\epsilon)^{n-k}\\right) \\cdot \\left(\\epsilon^{1/2-1} (1-\\epsilon)^{1/2-1}\\right)$$\n$$p(\\epsilon \\mid k,n) \\propto \\epsilon^{k+1/2-1} (1-\\epsilon)^{n-k+1/2-1}$$\nThis is the kernel of a Beta distribution with updated parameters. The posterior distribution for $\\epsilon$ is:\n$$p(\\epsilon \\mid k,n) = \\mathrm{Beta}(\\epsilon; k+1/2, n-k+1/2)$$\nThe equal-tailed Bayesian credible interval at a nominal level of $1-\\alpha$ is the interval $[\\epsilon_L, \\epsilon_U]$ defined by the $\\alpha/2$ and $1-\\alpha/2$ quantiles of this posterior distribution.\n\n**4. Clopper–Pearson Confidence Interval**\n\nThe Clopper–Pearson (CP) interval is a frequentist method that guarantees the coverage probability is at least $1-\\alpha$ for all possible values of $\\epsilon$. It is constructed by inverting a family of two-sided binomial tests. For an observed number of successes $k$, the interval $[\\epsilon_L, \\epsilon_U]$ is found by solving the following equations for $\\epsilon$:\n$$ \\text{For the lower bound } \\epsilon_L: \\quad P(X \\ge k \\mid n, \\epsilon_L) = \\sum_{j=k}^{n} \\binom{n}{j} \\epsilon_L^j (1-\\epsilon_L)^{n-j} = \\frac{\\alpha}{2} $$\n$$ \\text{For the upper bound } \\epsilon_U: \\quad P(X \\le k \\mid n, \\epsilon_U) = \\sum_{j=0}^{k} \\binom{n}{j} \\epsilon_U^j (1-\\epsilon_U)^{n-j} = \\frac{\\alpha}{2} $$\nThese equations can be expressed in terms of quantiles of the Beta distribution. The lower bound $\\epsilon_L$ is the $\\alpha/2$ quantile of a $\\mathrm{Beta}(k, n-k+1)$ distribution. The upper bound $\\epsilon_U$ is the $1-\\alpha/2$ quantile of a $\\mathrm{Beta}(k+1, n-k)$ distribution. For the edge cases, when $k=0$, $\\epsilon_L = 0$, and when $k=n$, $\\epsilon_U = 1$.\n\n### Computational Strategy\n\nThe implementation will follow these theoretical results to compute and compare the two interval types.\n\n1.  **Interval Calculation**: For a given test case $(n, \\alpha)$, we first iterate through all possible numbers of successes $k \\in \\{0, 1, \\dots, n\\}$. For each $k$, we compute the Bayesian and Clopper-Pearson intervals.\n    *   **Bayesian Interval**: The endpoints are the $\\alpha/2$ and $1-\\alpha/2$ quantiles of the posterior distribution $\\mathrm{Beta}(k+0.5, n-k+0.5)$.\n    *   **Clopper–Pearson Interval**: The endpoints are the $\\alpha/2$ quantile of $\\mathrm{Beta}(k, n-k+1)$ and the $1-\\alpha/2$ quantile of $\\mathrm{Beta}(k+1, n-k)$, with special handling for $k=0$ (lower bound is $0$) and $k=n$ (upper bound is $1$).\n\n2.  **Coverage Evaluation**: For each true efficiency value $\\epsilon$ in the specified grid, we calculate the frequentist coverage of each interval type. The coverage $C(\\epsilon)$ is the probability that the random interval $[L(k), U(k)]$ contains the true $\\epsilon$. This is computed by summing the binomial probabilities $\\mathrm{Bin}(k; n, \\epsilon)$ for all outcomes $k$ where the true $\\epsilon$ falls within the computed interval for that $k$:\n    $$C(\\epsilon) = \\sum_{k=0}^{n} \\mathbb{I}\\{\\epsilon \\in [L(k), U(k)]\\} \\cdot \\mathrm{Bin}(k; n, \\epsilon)$$\n    where $\\mathbb{I}\\{\\cdot\\}$ is the indicator function.\n\n3.  **Metrics Calculation**: After computing the coverage $C(\\epsilon_i)$ for all $M$ points in the grid, we calculate two summary metrics for each interval type:\n    *   **Worst-Case Deficit**: The minimum difference between the actual coverage and the nominal coverage $1-\\alpha$ over the grid: $\\min_{\\epsilon_i} (C(\\epsilon_i) - (1-\\alpha))$. A negative value indicates under-coverage.\n    *   **Mean Absolute Coverage Error**: The average absolute deviation of the coverage from the nominal level: $\\frac{1}{M}\\sum_{i=1}^{M} |C(\\epsilon_i) - (1-\\alpha)|$.\n\nThis procedure is repeated for each of the three test cases defined in the problem. The final program will aggregate and print the four computed metrics for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta, binom\n\ndef compute_metrics(n: int, alpha: float, epsilon_grid: np.ndarray):\n    \"\"\"\n    Computes and evaluates Bayesian and Clopper-Pearson intervals.\n\n    Args:\n        n: The number of trials.\n        alpha: The significance level.\n        epsilon_grid: A grid of true efficiency values to evaluate coverage.\n\n    Returns:\n        A list containing four floats:\n        [worst-case deficit for Bayesian, worst-case deficit for CP,\n         mean absolute error for Bayesian, mean absolute error for CP].\n    \"\"\"\n    ks = np.arange(n + 1)\n    \n    # 1. Compute intervals for all possible outcomes k\n    bayesian_intervals = np.zeros((n + 1, 2))\n    cp_intervals = np.zeros((n + 1, 2))\n\n    for k in ks:\n        # Bayesian interval with Jeffreys prior (Posterior is Beta(k+0.5, n-k+0.5))\n        post_a = k + 0.5\n        post_b = n - k + 0.5\n        bayesian_intervals[k, :] = beta.ppf([alpha / 2, 1 - alpha / 2], post_a, post_b)\n\n        # Clopper-Pearson interval\n        cp_lower = 0.0 if k == 0 else beta.ppf(alpha / 2, k, n - k + 1)\n        cp_upper = 1.0 if k == n else beta.ppf(1 - alpha / 2, k + 1, n - k)\n        cp_intervals[k, :] = [cp_lower, cp_upper]\n\n    # 2. Compute frequentist coverage for each epsilon in the grid\n    nominal_coverage = 1.0 - alpha\n    bayesian_coverages = []\n    cp_coverages = []\n\n    for eps in epsilon_grid:\n        # Binomial probabilities for all k given the true epsilon\n        binom_probs = binom.pmf(ks, n, eps)\n        \n        # Check which k's result in an interval that covers the true epsilon\n        # Add a small tolerance for floating point comparisons at the boundary\n        is_covered_bayes = (bayesian_intervals[:, 0] <= eps) & (eps <= bayesian_intervals[:, 1])\n        is_covered_cp = (cp_intervals[:, 0] <= eps) & (eps <= cp_intervals[:, 1])\n        \n        # Sum the probabilities of the k's that produce a covering interval\n        bayesian_coverages.append(np.sum(binom_probs[is_covered_bayes]))\n        cp_coverages.append(np.sum(binom_probs[is_covered_cp]))\n    \n    bayesian_coverages = np.array(bayesian_coverages)\n    cp_coverages = np.array(cp_coverages)\n    \n    # 3. Compute summary metrics\n    # Worst-case deficit relative to nominal coverage\n    bayesian_deficit = np.min(bayesian_coverages - nominal_coverage)\n    cp_deficit = np.min(cp_coverages - nominal_coverage)\n    \n    # Mean absolute coverage error\n    bayesian_mae = np.mean(np.abs(bayesian_coverages - nominal_coverage))\n    cp_mae = np.mean(np.abs(cp_coverages - nominal_coverage))\n\n    return [bayesian_deficit, cp_deficit, bayesian_mae, cp_mae]\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'n': 10, 'alpha': 0.1, 'grid_points': 21},\n        {'n': 20, 'alpha': 0.1, 'grid_points': 11},\n        {'n': 20, 'alpha': 0.32, 'grid_points': 21},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n = case['n']\n        alpha = case['alpha']\n        epsilon_grid = np.linspace(0.0, 1.0, case['grid_points'])\n        \n        result = compute_metrics(n, alpha, epsilon_grid)\n        all_results.append(result)\n\n    # Format the final output string to match the problem specification\n    # e.g., [[a1,b1,c1,d1],[a2,b2,c2,d2],[a3,b3,c3,d3]] with no spaces.\n    case_strings = []\n    for res in all_results:\n        case_strings.append(f\"[{','.join(map(str, res))}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3506258"}, {"introduction": "This final practice synthesizes the concepts of interval estimation and nuisance parameters in a realistic scenario central to the search for new physics: setting an upper limit on a signal in the presence of uncertain background. The exercise [@problem_id:3506302] requires a computational comparison between the frequentist $\\mathrm{CL_s}$ method, a standard in modern high-energy physics, and a fully Bayesian approach. You will tackle the challenge of incorporating a non-Gaussian (log-normal) systematic uncertainty on the background, providing invaluable hands-on experience with the sophisticated statistical machinery used to report the results of experimental searches.", "problem": "Consider a single-bin counting experiment typical in computational high-energy physics. An observed count $n$ is modeled as a Poisson random variable with mean $\\mu_s + \\mu_b$, where $\\mu_s \\ge 0$ is the unknown signal strength and $\\mu_b > 0$ is a background contribution subject to non-Gaussian prior uncertainty. The nuisance parameter $\\mu_b$ follows a log-normal prior specified by a normal distribution for the logarithm, i.e., $\\log \\mu_b \\sim \\mathcal{N}(\\mu_t, \\sigma_t^2)$.\n\nStarting from first principles:\n- The Poisson probability mass function is $P(N=n \\mid \\lambda) = \\exp(-\\lambda) \\lambda^n / n!$ for integer $n \\ge 0$ and rate $\\lambda > 0$.\n- Bayes' theorem states $p(\\mu_s \\mid n) \\propto p(n \\mid \\mu_s) \\, \\pi(\\mu_s)$, where $p(n \\mid \\mu_s) = \\int P(N=n \\mid \\mu_s + \\mu_b) \\, \\pi(\\mu_b) \\, \\mathrm{d}\\mu_b$ and $\\pi(\\mu_s)$, $\\pi(\\mu_b)$ are priors.\n- The log-normal prior for $\\mu_b$ is defined by the normal density of $t=\\log \\mu_b$ with mean $\\mu_t$ and standard deviation $\\sigma_t$, i.e., $\\phi(t; \\mu_t, \\sigma_t) = \\frac{1}{\\sqrt{2\\pi}\\sigma_t} \\exp\\left( -\\frac{(t-\\mu_t)^2}{2\\sigma_t^2} \\right)$.\n\nDefine the Confidence Level (CL) metric $\\mathrm{CL_s}$ for an upper limit on $\\mu_s$ at level $\\alpha$ as the ratio\n$$\n\\mathrm{CL_s}(\\mu_s; n) = \\frac{p_{\\mathrm{sb}}(n; \\mu_s)}{p_{\\mathrm{b}}(n)},\n$$\nwhere $p_{\\mathrm{sb}}(n; \\mu_s)$ is the one-sided downward tail probability under the signal-plus-background hypothesis and $p_{\\mathrm{b}}(n)$ is the corresponding tail probability under the background-only hypothesis:\n$$\np_{\\mathrm{sb}}(n; \\mu_s) = \\int \\left[ \\sum_{k=0}^{n} \\exp\\left( -(\\mu_s+\\mu_b) \\right) \\frac{(\\mu_s+\\mu_b)^k}{k!} \\right] \\pi(\\mu_b) \\, \\mathrm{d}\\mu_b,\n$$\n$$\np_{\\mathrm{b}}(n) = \\int \\left[ \\sum_{k=0}^{n} \\exp\\left( -\\mu_b \\right) \\frac{\\mu_b^k}{k!} \\right] \\pi(\\mu_b) \\, \\mathrm{d}\\mu_b.\n$$\nAn upper limit $\\mu_s^{95}$ is defined as the smallest $\\mu_s$ satisfying $\\mathrm{CL_s}(\\mu_s; n) \\le \\alpha$, where $\\alpha$ is fixed (for this problem take $\\alpha = 0.05$).\n\nFor the Bayesian analysis, adopt an improper non-informative prior for the signal $\\pi(\\mu_s) \\propto 1$ on $\\mu_s \\ge 0$ and the log-normal prior for the nuisance $\\mu_b$ described above. The posterior $p(\\mu_s \\mid n)$ is proportional to the marginal likelihood $p(n \\mid \\mu_s)$ obtained by integrating out $\\mu_b$. The equal-tailed $(1-\\alpha)$ Bayesian credible interval for $\\mu_s$ is $[\\mu_{s,\\mathrm{lo}}, \\mu_{s,\\mathrm{hi}}]$ satisfying\n$$\n\\int_{0}^{\\mu_{s,\\mathrm{lo}}} p(\\mu_s \\mid n) \\, \\mathrm{d}\\mu_s = \\frac{\\alpha}{2}, \\quad \\int_{0}^{\\mu_{s,\\mathrm{hi}}} p(\\mu_s \\mid n) \\, \\mathrm{d}\\mu_s = 1 - \\frac{\\alpha}{2}.\n$$\nThe interval length to be reported is $\\mu_{s,\\mathrm{hi}} - \\mu_{s,\\mathrm{lo}}$.\n\nImplement a program that, for each test case, computes:\n1. The $\\mathrm{CL_s}$ $95\\%$ upper limit $\\mu_s^{95}$ (unitless), defined as the smallest $\\mu_s$ such that $\\mathrm{CL_s}(\\mu_s; n) \\le 0.05$ using the downward tail definition above.\n2. The length $\\mu_{s,\\mathrm{hi}} - \\mu_{s,\\mathrm{lo}}$ of the $(1-0.05)=0.95$ equal-tailed Bayesian credible interval for $\\mu_s$ (unitless).\n\nAll quantities are dimensionless; no physical units are required. Angles are not involved. Express interval probabilities as decimals in $[0,1]$.\n\nYour implementation must be derived from the definitions above, using the log-normal prior parameterization for $\\mu_b$ via $\\log \\mu_b \\sim \\mathcal{N}(\\mu_t, \\sigma_t^2)$. The program should not rely on any closed-form shortcut formula that is not derived from these definitions; it must be based on rigorous derivations and numerically robust algorithms.\n\nTest suite parameter sets:\n- Case $1$: $n=0$, $\\mu_t=\\ln(2)$, $\\sigma_t=0.6$.\n- Case $2$: $n=3$, $\\mu_t=\\ln(1)$, $\\sigma_t=1.0$.\n- Case $3$: $n=7$, $\\mu_t=\\ln(5)$, $\\sigma_t=0.3$.\n- Case $4$: $n=10$, $\\mu_t=\\ln(2)$, $\\sigma_t=0.8$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list $[\\mu_s^{95}, \\ell]$ with $\\ell = \\mu_{s,\\mathrm{hi}} - \\mu_{s,\\mathrm{lo}}$. For example, a valid output line has the form $[[x_1,y_1],[x_2,y_2],[x_3,y_3],[x_4,y_4]]$.", "solution": "The user-provided problem statement has been meticulously validated.\n\n### Step 1: Extract Givens\n- **Model**: A single-bin counting experiment.\n- **Observation**: An integer count $n \\ge 0$.\n- **Likelihood**: The count $n$ is a Poisson random variable, $N \\sim \\text{Poisson}(\\lambda)$, with probability mass function $P(N=n \\mid \\lambda) = \\exp(-\\lambda) \\lambda^n / n!$. The rate is $\\lambda = \\mu_s + \\mu_b$.\n- **Signal Parameter**: $\\mu_s \\ge 0$ is the unknown signal strength.\n- **Nuisance Parameter**: $\\mu_b > 0$ is the background contribution.\n- **Prior for $\\mu_b$**: $\\mu_b$ follows a log-normal prior, specified by $\\log \\mu_b \\sim \\mathcal{N}(\\mu_t, \\sigma_t^2)$. The density for $t = \\log \\mu_b$ is $\\phi(t; \\mu_t, \\sigma_t) = \\frac{1}{\\sqrt{2\\pi}\\sigma_t} \\exp\\left( -\\frac{(t-\\mu_t)^2}{2\\sigma_t^2} \\right)$.\n- **Frequentist Metric**: $\\mathrm{CL_s}(\\mu_s; n) = \\frac{p_{\\mathrm{sb}}(n; \\mu_s)}{p_{\\mathrm{b}}(n)}$.\n  - $p_{\\mathrm{sb}}(n; \\mu_s) = \\int \\left[ \\sum_{k=0}^{n} \\exp\\left( -(\\mu_s+\\mu_b) \\right) \\frac{(\\mu_s+\\mu_b)^k}{k!} \\right] \\pi(\\mu_b) \\, \\mathrm{d}\\mu_b$.\n  - $p_{\\mathrm{b}}(n) = \\int \\left[ \\sum_{k=0}^{n} \\exp\\left( -\\mu_b \\right) \\frac{\\mu_b^k}{k!} \\right] \\pi(\\mu_b) \\, \\mathrm{d}\\mu_b$.\n- **Frequentist Upper Limit**: The $95\\%$ upper limit, $\\mu_s^{95}$, is the smallest $\\mu_s$ such that $\\mathrm{CL_s}(\\mu_s; n) \\le \\alpha$, with $\\alpha = 0.05$.\n- **Bayesian Prior for $\\mu_s$**: Improper non-informative prior $\\pi(\\mu_s) \\propto 1$ for $\\mu_s \\ge 0$.\n- **Bayesian Credible Interval**: The equal-tailed $(1-\\alpha)$ credible interval $[\\mu_{s,\\mathrm{lo}}, \\mu_{s,\\mathrm{hi}}]$ for $\\mu_s$ is defined by $\\int_{0}^{\\mu_{s,\\mathrm{lo}}} p(\\mu_s \\mid n) \\, \\mathrm{d}\\mu_s = \\alpha/2$ and $\\int_{0}^{\\mu_{s,\\mathrm{hi}}} p(\\mu_s \\mid n) \\, \\mathrm{d}\\mu_s = 1 - \\alpha/2$, with $\\alpha=0.05$.\n- **Requested Bayesian Ouput**: The length of the interval, $\\mu_{s,\\mathrm{hi}} - \\mu_{s,\\mathrm{lo}}$.\n- **Test Cases**:\n  1. $n=0$, $\\mu_t=\\ln(2)$, $\\sigma_t=0.6$.\n  2. $n=3$, $\\mu_t=\\ln(1)$, $\\sigma_t=1.0$.\n  3. $n=7$, $\\mu_t=\\ln(5)$, $\\sigma_t=0.3$.\n  4. $n=10$, $\\mu_t=\\ln(2)$, $\\sigma_t=0.8$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a canonical example of statistical inference in high-energy physics, employing standard Poisson models, log-normal priors for systematic uncertainties, and well-established frequentist ($\\mathrm{CL_s}$) and Bayesian methodologies. The premises are factually sound and rooted in probability theory.\n- **Well-Posed**: The problem provides all necessary definitions, data, and constraints to determine a unique solution for each test case.\n- **Objective**: The problem is stated using precise, unambiguous mathematical and statistical language.\n\nThe problem does not exhibit any of the invalidity flaws.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A principled solution will be constructed.\n\n### Solution Derivation\n\nThe problem requires the computation of a frequentist upper limit and a Bayesian credible interval length. Both calculations depend on averaging over the nuisance parameter $\\mu_b$, which has a log-normal prior distribution.\n\n**1. Frequentist $\\mathrm{CL_s}$ Limit Calculation**\n\nThe core quantity is $\\mathrm{CL_s}(\\mu_s; n) = p_{\\mathrm{sb}}(n; \\mu_s) / p_{\\mathrm{b}}(n)$. The terms $p_{\\mathrm{sb}}$ and $p_{\\mathrm{b}}$ are defined as integrals. The expression inside the square brackets is the cumulative distribution function (CDF) of a Poisson distribution, $F_{\\text{Poisson}}(n; \\lambda) = \\sum_{k=0}^{n} e^{-\\lambda} \\lambda^k / k!$.\nThus, we have:\n$$\np_{\\mathrm{sb}}(n; \\mu_s) = \\int_0^\\infty F_{\\text{Poisson}}(n; \\mu_s + \\mu_b) \\, \\pi(\\mu_b) \\, \\mathrm{d}\\mu_b\n$$\n$$\np_{\\mathrm{b}}(n) = \\int_0^\\infty F_{\\text{Poisson}}(n; \\mu_b) \\, \\pi(\\mu_b) \\, \\mathrm{d}\\mu_b\n$$\nThe prior for $\\mu_b$ is log-normal, meaning $t = \\log \\mu_b$ is normally distributed with mean $\\mu_t$ and variance $\\sigma_t^2$. The probability density for $t$ is $\\phi(t; \\mu_t, \\sigma_t)$. To evaluate the integrals, we change the integration variable from $\\mu_b$ to $t = \\log\\mu_b$. This gives $\\mu_b = e^t$ and $\\mathrm{d}\\mu_b = e^t \\, \\mathrm{d}t$. The prior density for $\\mu_b$ is $\\pi(\\mu_b) = \\phi(\\log \\mu_b; \\mu_t, \\sigma_t) / \\mu_b$. The integrals become:\n$$\np_{\\mathrm{sb}}(n; \\mu_s) = \\int_{-\\infty}^\\infty F_{\\text{Poisson}}(n; \\mu_s + e^t) \\, \\phi(t; \\mu_t, \\sigma_t) \\, \\mathrm{d}t\n$$\nThese integrals over a Gaussian kernel are ideally suited for Gauss-Hermite quadrature. We perform a substitution to match the standard Gauss-Hermite weight function $e^{-x^2}$. Let $x = (t - \\mu_t) / (\\sqrt{2}\\sigma_t)$, so $t = \\sqrt{2}\\sigma_t x + \\mu_t$ and $\\phi(t; \\mu_t, \\sigma_t) \\, \\mathrm{d}t = \\frac{1}{\\sqrt{\\pi}} e^{-x^2} \\, \\mathrm{d}x$. The integral for $p_{\\mathrm{sb}}$ (and similarly for $p_{\\mathrm{b}}$) becomes:\n$$\np_{\\mathrm{sb}}(n; \\mu_s) = \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^\\infty F_{\\text{Poisson}}(n; \\mu_s + e^{\\sqrt{2}\\sigma_t x + \\mu_t}) \\, e^{-x^2} \\, \\mathrm{d}x\n$$\nThis is approximated by the quadrature sum:\n$$\np_{\\mathrm{sb}}(n; \\mu_s) \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^{N_{GH}} w_i F_{\\text{Poisson}}(n; \\mu_s + e^{\\sqrt{2}\\sigma_t x_i + \\mu_t})\n$$\nwhere $x_i$ and $w_i$ are the nodes and weights of the Gauss-Hermite quadrature of order $N_{GH}$. The factor $1/\\sqrt{\\pi}$ cancels in the $\\mathrm{CL_s}$ ratio, simplifying the computation.\n\nThe upper limit $\\mu_s^{95}$ is the root of the equation $\\mathrm{CL_s}(\\mu_s; n) - 0.05 = 0$. The function $\\mathrm{CL_s}(\\mu_s; n)$ is monotonic decreasing in $\\mu_s$, ensuring a unique root that can be found efficiently using a numerical solver like Brent's method.\n\n**2. Bayesian Credible Interval Calculation**\n\nThe posterior probability density for $\\mu_s$ is given by Bayes' theorem: $p(\\mu_s \\mid n) \\propto p(n \\mid \\mu_s) \\pi(\\mu_s)$. With the flat prior $\\pi(\\mu_s) \\propto 1$ for $\\mu_s \\ge 0$, the posterior is proportional to the marginal likelihood, $p(\\mu_s \\mid n) \\propto p(n \\mid \\mu_s)$. The marginal likelihood is obtained by integrating out the nuisance parameter $\\mu_b$:\n$$\np(n \\mid \\mu_s) = \\int_0^\\infty P(N=n \\mid \\mu_s + \\mu_b) \\, \\pi(\\mu_b) \\, \\mathrm{d}\\mu_b\n$$\nThe posterior cumulative distribution function (CDF) is $C(\\mu_s) = \\int_0^{\\mu_s} p(\\mu_s' \\mid n) \\, \\mathrm{d}\\mu_s'$. To compute this, we would typically face a challenging double integral. However, by swapping the order of integration, we find a remarkable simplification:\n$$\n\\int_0^{\\mu_s} p(n \\mid \\mu_s') \\, \\mathrm{d}\\mu_s' = \\int_0^\\infty \\pi(\\mu_b) \\left( \\int_0^{\\mu_s} P(N=n \\mid \\mu_s' + \\mu_b) \\, \\mathrm{d}\\mu_s' \\right) \\mathrm{d}\\mu_b\n$$\nThe inner integral over $\\mu_s'$ can be solved analytically. Let $\\lambda' = \\mu_s'+\\mu_b$. The integral is $\\int_{\\mu_b}^{\\mu_s+\\mu_b} P(N=n \\mid \\lambda') \\, \\mathrm{d}\\lambda' = \\int_{\\mu_b}^{\\mu_s+\\mu_b} \\frac{(\\lambda')^n e^{-\\lambda'}}{n!} \\, \\mathrm{d}\\lambda'$. This is equal to $F_{\\text{Poisson}}(n; \\mu_b) - F_{\\text{Poisson}}(n; \\mu_s + \\mu_b)$. Substituting this back gives:\n$$\n\\int_0^{\\mu_s} p(n \\mid \\mu_s') \\, \\mathrm{d}\\mu_s' = \\int_0^\\infty \\left[ F_{\\text{Poisson}}(n; \\mu_b) - F_{\\text{Poisson}}(n; \\mu_s + \\mu_b) \\right] \\pi(\\mu_b) \\, \\mathrm{d}\\mu_b = p_{\\mathrm{b}}(n) - p_{\\mathrm{sb}}(n; \\mu_s)\n$$\nThe normalization constant for the posterior is the integral up to $\\mu_s \\to \\infty$, where $p_{\\mathrm{sb}} \\to 0$. Thus, the normalization is $p_{\\mathrm{b}}(n)$. The posterior CDF is therefore:\n$$\nC(\\mu_s) = \\frac{p_{\\mathrm{b}}(n) - p_{\\mathrm{sb}}(n; \\mu_s)}{p_{\\mathrm{b}}(n)} = 1 - \\mathrm{CL_s}(\\mu_s; n)\n$$\nThis elegant result connects the Bayesian posterior CDF directly to the frequentist $\\mathrm{CL_s}$ metric. The equal-tailed credible interval bounds $[\\mu_{s,\\mathrm{lo}}, \\mu_{s,\\mathrm{hi}}]$ are found by solving:\n- $C(\\mu_{s,\\mathrm{lo}}) = \\alpha/2 \\implies 1 - \\mathrm{CL_s}(\\mu_{s,\\mathrm{lo}}; n) = 0.025 \\implies \\mathrm{CL_s}(\\mu_{s,\\mathrm{lo}}; n) = 0.975$\n- $C(\\mu_{s,\\mathrm{hi}}) = 1-\\alpha/2 \\implies 1 - \\mathrm{CL_s}(\\mu_{s,\\mathrm{hi}}; n) = 0.975 \\implies \\mathrm{CL_s}(\\mu_{s,\\mathrm{hi}}; n) = 0.025$\n\nThe same numerical machinery used to find $\\mu_s^{95}$ can be used to find $\\mu_{s,\\mathrm{lo}}$ and $\\mu_{s,\\mathrm{hi}}$.\n\n**3. Special Case: $n=0$**\n\nFor an observed count of $n=0$, the Poisson CDF is $F_{\\text{Poisson}}(0; \\lambda) = e^{-\\lambda}$. The integrals simplify significantly:\n- $p_{\\mathrm{sb}}(0; \\mu_s) = \\int e^{-(\\mu_s+\\mu_b)} \\pi(\\mu_b) \\, \\mathrm{d}\\mu_b = e^{-\\mu_s} \\int e^{-\\mu_b} \\pi(\\mu_b) \\, \\mathrm{d}\\mu_b$\n- $p_{\\mathrm{b}}(0) = \\int e^{-\\mu_b} \\pi(\\mu_b) \\, \\mathrm{d}\\mu_b$\nThe ratio is $\\mathrm{CL_s}(\\mu_s; 0) = e^{-\\mu_s}$, which is independent of the background prior. This allows for analytic solutions for Test Case $1$:\n- $\\mu_s^{95}: e^{-\\mu_s} = 0.05 \\implies \\mu_s^{95} = -\\ln(0.05)$.\n- $\\mu_{s,\\mathrm{lo}}: e^{-\\mu_s} = 0.975 \\implies \\mu_{s,\\mathrm{lo}} = -\\ln(0.975)$.\n- $\\mu_{s,\\mathrm{hi}}: e^{-\\mu_s} = 0.025 \\implies \\mu_{s,\\mathrm{hi}} = -\\ln(0.025)$.\nThe interval length is $\\ell = \\mu_{s,\\mathrm{hi}} - \\mu_{s,\\mathrm{lo}} = -\\ln(0.025) + \\ln(0.975) = \\ln(0.975/0.025) = \\ln(39)$.\n\nThe implementation will use these analytic forms for $n=0$ and the numerical approach for $n>0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import poisson\nfrom scipy.special import roots_hermite\nfrom scipy.optimize import brentq\nimport math\n\n# Global constants for numerical precision and calculations\nN_GH = 64  # Degree of Gauss-Hermite quadrature\nNODES, WEIGHTS = roots_hermite(N_GH)\n\ndef calculate_cls(mu_s, n, mu_t, sigma_t):\n    \"\"\"\n    Calculates the CLs value for a given signal strength mu_s.\n    \n    The calculation is based on the ratio of two integrals, p_sb/p_b, which are\n    evaluated using Gauss-Hermite quadrature.\n    \"\"\"\n    # For n=0, CLs has a simple analytic form, exp(-mu_s), independent of the\n    # background prior. This avoids numerical integration and is exact.\n    if n == 0:\n        # mu_s must be non-negative.\n        if mu_s < 0:\n            return 1.0 # By continuous extension as CLs(0)=1\n        return np.exp(-mu_s)\n\n    # Change of variables for Gauss-Hermite quadrature.\n    # The integration is over a standard normal variable 'x', which is related\n    # to t=log(mu_b) by t = sqrt(2)*sigma_t*x + mu_t.\n    t_values = math.sqrt(2) * sigma_t * NODES + mu_t\n    mu_b_values = np.exp(t_values)\n    \n    # Calculate Poisson CDF values for signal+background and background-only hypotheses\n    # at each quadrature node.\n    # poisson.cdf(k, mu) calculates sum_{i=0 to k} exp(-mu) * mu^i / i!\n    poisson_cdf_sb = poisson.cdf(n, mu_s + mu_b_values)\n    poisson_cdf_b = poisson.cdf(n, mu_b_values)\n\n    # The integrals for p_sb and p_b are approximated by the weighted sums.\n    # The factor 1/sqrt(pi) from the change of variables cancels in the ratio.\n    p_sb_integral_sum = np.sum(WEIGHTS * poisson_cdf_sb)\n    p_b_integral_sum = np.sum(WEIGHTS * poisson_cdf_b)\n    \n    # Handle the case where the denominator integral is numerically zero.\n    # This might occur if mu_b is very large, making the Poisson CDF vanish.\n    if p_b_integral_sum == 0.0:\n        return 0.0\n\n    return p_sb_integral_sum / p_b_integral_sum\n\ndef find_mu_s_for_cls_target(cls_target, n, mu_t, sigma_t):\n    \"\"\"\n    Finds the value of mu_s for which CLs(mu_s) equals a given target value.\n    This is achieved by finding the root of f(mu_s) = CLs(mu_s) - cls_target.\n    \"\"\"\n    \n    # Define the function whose root we want to find.\n    def root_func(mu_s):\n        return calculate_cls(mu_s, n, mu_t, sigma_t) - cls_target\n\n    # The CLs function is monotonic decreasing from 1 to 0.\n    # The root must be bracketed for the solver.\n    # root_func(0) = 1 - cls_target, which is > 0 for cls_target < 1.\n    lower_bound = 0.0\n    upper_bound = 1.0 # Initial guess for the upper bracket boundary.\n    \n    # Expand the search interval until the root is bracketed.\n    # Failsafe limit to prevent infinite loops.\n    max_search_val = 1000.0\n    while root_func(upper_bound) > 0:\n        upper_bound *= 2.0\n        if upper_bound > max_search_val:\n            raise RuntimeError(f\"Failed to bracket root for CLs target {cls_target}\")\n    \n    # Use Brent's method to find the root with high precision.\n    return brentq(root_func, lower_bound, upper_bound)\n\ndef solve_case(n, mu_t, sigma_t):\n    \"\"\"\n    Computes the CLs 95% upper limit and the Bayesian 95% credible interval length\n    for a single test case.\n    \"\"\"\n    # 1. Compute the CLs 95% upper limit (mu_s^95).\n    # This is the value of mu_s where CLs(mu_s) = 0.05.\n    mu_s_95 = find_mu_s_for_cls_target(0.05, n, mu_t, sigma_t)\n\n    # 2. Compute the Bayesian 95% credible interval length.\n    # Due to the flat prior on mu_s, the posterior CDF C(mu_s) = 1 - CLs(mu_s).\n    # The 95% equal-tailed interval [lo, hi] is found by solving:\n    # C(lo) = 0.025 => CLs(lo) = 0.975\n    # C(hi) = 0.975 => CLs(hi) = 0.025\n    \n    mu_s_lo_bayes = find_mu_s_for_cls_target(0.975, n, mu_t, sigma_t)\n    mu_s_hi_bayes = find_mu_s_for_cls_target(0.025, n, mu_t, sigma_t)\n\n    interval_length = mu_s_hi_bayes - mu_s_lo_bayes\n    \n    return [mu_s_95, interval_length]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0, math.log(2), 0.6),\n        (3, math.log(1), 1.0),\n        (7, math.log(5), 0.3),\n        (10, math.log(2), 0.8),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, mu_t, sigma_t = case\n        result = solve_case(n, mu_t, sigma_t)\n        results.append(result)\n\n    # Format the output exactly as specified: [[x1,y1],[x2,y2],...]\n    formatted_results = [f\"[{r[0]},{r[1]}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3506302"}]}