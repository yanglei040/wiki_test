{"hands_on_practices": [{"introduction": "The cornerstone of any histogram-based analysis is the reliable assignment of data points to their respective bins. This exercise [@problem_id:3510256] moves beyond naive comparisons to address the critical challenge of numerical stability at bin edges, a consequence of finite-precision floating-point arithmetic. Mastering this low-level implementation detail is essential for building robust and reproducible physics analysis software that avoids subtle biases from misclassified events.", "problem": "You are given an ordered array of real-valued bin edges $b_0  b_1  \\dots  b_M$ (with $M \\geq 2$), defining half-open histogram bins under the convention $[b_i, b_{i+1})$ for $i \\in \\{0,1,\\dots,M-1\\}$. In computational high-energy physics, physical observables such as transverse momentum $p_T$ are measured in gigaelectronvolts (GeV), and histogramming requires unambiguous, reproducible bin assignment near floating-point boundaries. Under the half-open convention, any value $x$ that satisfies $b_i \\le x  b_{i+1}$ must be assigned to bin index $i$, values with $x  b_0$ are underflows, and values with $x \\ge b_M$ are overflows (i.e., excluded from the defined bins). Your task is to implement a numerically stable assignment function that respects the half-open bin definition and avoids floating-point edge misclassification for values extremely close to $b_{i+1}$.\n\nStart from the following foundational base:\n- The Institute of Electrical and Electronics Engineers (IEEE) 754 floating-point standard defines representable numbers on the real line with machine-dependent spacing, includes a notion of the unit in the last place (ULP), and provides the successor/predecessor function $\\operatorname{nextafter}(y, z)$, which returns the representable number immediately adjacent to $y$ in the direction of $z$.\n- Monotonic binary search on a sorted array yields indices consistent with total order comparisons and the half-open interval partition when comparisons are defined appropriately.\n- A half-open partition $[b_i, b_{i+1})$ is disjoint and covers the interval $[b_0, b_M)$.\n\nDesign and implement an assignment function that:\n1. For each input value $x$, returns a valid bin index $i \\in \\{0,\\dots,M-1\\}$ whenever $b_0 \\le x  b_M$ and excludes underflows ($x  b_0$) and overflows ($x \\ge b_M$) from bin counts.\n2. Ensures that values equal to a right edge $b_{i+1}$ by floating-point representation are assigned to the next bin $i+1$ (in accordance with half-open bins), and prevents misclassification of values that are within one ULP of $b_{i+1}$ by using a comparison strategy or boundary handling consistent with IEEE 754 semantics, not ad hoc absolute tolerances.\n3. Produces per-bin counts as integers for given test data.\n\nImplement your solution as a complete, runnable program. Use the following test suite. All observables are in gigaelectronvolts (GeV); counts are dimensionless integers. In all cases, adopt the half-open convention $[b_i, b_{i+1})$ and exclude underflow ($x  b_0$) and overflow ($x \\ge b_M$) from counts.\n\nTest Case $1$ (happy path, explicit edge probing):\n- Bin edges $b = [0.0, 50.0, 100.0, 200.0]$ GeV.\n- Values $x$ (GeV): $[0.0, 12.5, \\operatorname{nextafter}(50.0, -\\infty), 50.0, \\operatorname{nextafter}(50.0, +\\infty), 75.0, \\operatorname{nextafter}(100.0, -\\infty), 100.0, \\operatorname{nextafter}(100.0, +\\infty), 150.0, \\operatorname{nextafter}(200.0, -\\infty), 200.0, 250.0]$.\n\nTest Case $2$ (nonuniform bins typical in transverse momentum):\n- Bin edges $b = [0.5, 1.0, 2.0, 5.0, 10.0]$ GeV.\n- Values $x$ (GeV): $[0.4999, 0.5, \\operatorname{nextafter}(0.5, +\\infty), \\operatorname{nextafter}(1.0, -\\infty), 1.0, \\operatorname{nextafter}(1.0, +\\infty), 1.5, \\operatorname{nextafter}(2.0, -\\infty), 2.0, 3.0, \\operatorname{nextafter}(5.0, -\\infty), 5.0, 7.5, \\operatorname{nextafter}(10.0, -\\infty), 10.0]$.\n\nTest Case $3$ (boundary stress with small upper range):\n- Bin edges $b = [0.0, 0.1, 1.0]$ GeV.\n- Values $x$ (GeV): $[-10^{-12}, 0.0, \\operatorname{nextafter}(0.0, +\\infty), \\operatorname{nextafter}(0.1, -\\infty), 0.1, \\operatorname{nextafter}(0.1, +\\infty), \\operatorname{nextafter}(1.0, -\\infty), 1.0]$.\n\nTest Case $4$ (wider physics-motivated range):\n- Bin edges $b = [0.0, 20.0, 50.0, 100.0, 200.0, 400.0]$ GeV.\n- Values $x$ (GeV): $[0.0, \\operatorname{nextafter}(20.0, -\\infty), 20.0, 35.0, \\operatorname{nextafter}(50.0, -\\infty), 50.0, 75.0, \\operatorname{nextafter}(100.0, -\\infty), 100.0, 150.0, \\operatorname{nextafter}(200.0, -\\infty), 200.0, 300.0, \\operatorname{nextafter}(400.0, -\\infty), 400.0, 800.0]$.\n\nYour program must output a single line containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and must be the list of per-bin counts, excluding underflow and overflow, following the half-open assignment. Concretely, the output format must be:\n\"[$R_1, R_2, R_3, R_4$]\"\nwhere $R_k$ is the list of integer counts for test case $k$ in bin order $i = 0,1,\\dots,M-1$. For example, an element might look like \"[3,4,4]\". No other text must be printed.", "solution": "The problem requires the implementation of a numerically stable histogram binning function. Given an ordered array of $M+1$ bin edges $b = \\{b_0, b_1, \\dots, b_M\\}$ with $b_0  b_1  \\dots  b_M$, and a set of data values $\\{x\\}$, we must assign each value $x$ to a unique bin index $i \\in \\{0, 1, \\dots, M-1\\}$. The binning convention is defined by half-open intervals $[b_i, b_{i+1})$, meaning a value $x$ belongs to bin $i$ if and only if the condition $b_i \\le x  b_{i+1}$ is met. Values outside the total range $[b_0, b_M)$, specifically underflows ($x  b_0$) and overflows ($x \\ge b_M$), are to be excluded from the counts. The primary challenge is to ensure this logic is upheld robustly, especially for floating-point values that are extremely close to a bin edge $b_{i+1}$, without resorting to ad hoc absolute tolerances.\n\nThe solution must be grounded in the formal properties of IEEE 754 floating-point arithmetic. The standard comparison operators ($, \\le, , \\ge$) are well-defined for all representable floating-point numbers. A correct implementation of the strict inequality $x  b_{i+1}$ is therefore a matter of using these fundamental operators correctly, which high-quality numerical libraries do. The problem's mention of $\\operatorname{nextafter}(y, z)$ serves to construct test values that probe these boundary conditions with machine precision.\n\nA robust and efficient algorithm for this task can be built upon the principle of binary search, as the bin edges are sorted. The `NumPy` library provides a highly optimized and well-tested function, `numpy.digitize`, which is designed for this exact purpose. This function finds the ULP-precise bin for each value in an array.\n\nThe specific implementation strategy is as follows:\n\n1.  **Bin Indexing with `numpy.digitize`**: We will use the function `numpy.digitize(x, bins, right=False)`. The `bins` argument corresponds to our edge array $b$, and `x` is the array of values to be binned. The parameter `right=False` specifies that the bin intervals are of the form $[b_{i-1}, b_i)$, which directly corresponds to our required convention $[b_i, b_{i+1})$. The function returns 1-based indices.\n    - For a value $x$ such that $b_i \\le x  b_{i+1}$ for $i \\in \\{0, \\dots, M-1\\}$, `numpy.digitize` returns the index $i+1$.\n    - For an underflow value $x  b_0$, it returns $0$.\n    - For an overflow value $x \\ge b_M$, it returns $M+1$.\n\n2.  **Index Transformation**: The 1-based indices from `numpy.digitize` can be converted to a more convenient 0-based representation for our bins. By subtracting $1$ from the returned array of indices:\n    - A value in bin $i$ (for $i \\in \\{0, \\dots, M-1\\}$) is mapped to index $(i+1) - 1 = i$.\n    - An underflow value is mapped to index $0 - 1 = -1$.\n    - An overflow value is mapped to index $(M+1) - 1 = M$.\n    This creates a clear separation: valid bin indices fall in the range $[0, M-1]$, while underflows and overflows are mapped to indices outside this range.\n\n3.  **Filtering and Counting**: We can now isolate the values that fall within the defined bins. We create a boolean mask to select only those indices $i_{bin}$ that satisfy $0 \\le i_{bin}  M$. After applying this mask, we are left with an array of valid, 0-indexed bin numbers for all in-range values.\n\n4.  **Aggregation with `numpy.bincount`**: The final step is to count the number of occurrences of each bin index. The function `numpy.bincount` is ideal for this task. It takes an array of non-negative integers and returns an array where the value at index $k$ is the number of times $k$ appeared in the input array. By providing the `minlength=M` argument, we ensure that the resulting counts array has a length equal to the number of bins, with $0$ for any bins that happen to be empty. This produces the required per-bin counts.\n\nThis methodology is not only computationally efficient but also numerically robust. It relies on a standard, heavily scrutinized library function (`numpy.digitize`) that correctly implements comparisons at the limits of floating-point precision, thereby satisfying all requirements of the problem statement.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the histogram binning problem for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"b_edges\": [0.0, 50.0, 100.0, 200.0],\n            \"x_values\": [\n                0.0, \n                12.5, \n                np.nextafter(50.0, -np.inf), \n                50.0, \n                np.nextafter(50.0, np.inf), \n                75.0, \n                np.nextafter(100.0, -np.inf), \n                100.0, \n                np.nextafter(100.0, np.inf), \n                150.0, \n                np.nextafter(200.0, -np.inf), \n                200.0, \n                250.0\n            ]\n        },\n        {\n            \"b_edges\": [0.5, 1.0, 2.0, 5.0, 10.0],\n            \"x_values\": [\n                0.4999, \n                0.5, \n                np.nextafter(0.5, np.inf), \n                np.nextafter(1.0, -np.inf), \n                1.0, \n                np.nextafter(1.0, np.inf), \n                1.5, \n                np.nextafter(2.0, -np.inf), \n                2.0, \n                3.0, \n                np.nextafter(5.0, -np.inf), \n                5.0, \n                7.5, \n                np.nextafter(10.0, -np.inf), \n                10.0\n            ]\n        },\n        {\n            \"b_edges\": [0.0, 0.1, 1.0],\n            \"x_values\": [\n                -1e-12, \n                0.0, \n                np.nextafter(0.0, np.inf), \n                np.nextafter(0.1, -np.inf), \n                0.1, \n                np.nextafter(0.1, np.inf), \n                np.nextafter(1.0, -np.inf), \n                1.0\n            ]\n        },\n        {\n            \"b_edges\": [0.0, 20.0, 50.0, 100.0, 200.0, 400.0],\n            \"x_values\": [\n                0.0, \n                np.nextafter(20.0, -np.inf), \n                20.0, \n                35.0, \n                np.nextafter(50.0, -np.inf), \n                50.0, \n                75.0, \n                np.nextafter(100.0, -np.inf), \n                100.0, \n                150.0, \n                np.nextafter(200.0, -np.inf), \n                200.0, \n                300.0, \n                np.nextafter(400.0, -np.inf), \n                400.0, \n                800.0\n            ]\n        }\n    ]\n\n    def calculate_bin_counts(b_edges_np, x_values_np):\n        \"\"\"\n        Calculates per-bin counts for given edges and values.\n\n        Args:\n            b_edges_np (np.ndarray): Sorted array of bin edges.\n            x_values_np (np.ndarray): Array of values to bin.\n\n        Returns:\n            list: A list of integer counts for each bin.\n        \"\"\"\n        num_bins = len(b_edges_np) - 1\n        \n        # np.digitize uses right=False for [b[i-1], b[i]) interval convention.\n        # It returns 1-based bin indices.\n        # Underflows (x  b[0]) get index 0.\n        # Overflows (x = b[-1]) get index len(b).\n        digitized_indices = np.digitize(x_values_np, b_edges_np, right=False)\n        \n        # Convert to 0-based indices:\n        # A value in bin `i` gets index `i`.\n        # Underflows get index -1.\n        # Overflows get index num_bins.\n        bin_indices = digitized_indices - 1\n        \n        # Filter for indices that correspond to actual bins (0 to num_bins-1)\n        valid_mask = (bin_indices = 0)  (bin_indices  num_bins)\n        valid_indices = bin_indices[valid_mask]\n        \n        # Count occurrences of each valid bin index.\n        # minlength ensures all bins are represented in the output, even if empty.\n        counts = np.bincount(valid_indices, minlength=num_bins)\n        \n        return counts.tolist()\n\n    all_results = []\n    for case in test_cases:\n        b = np.array(case[\"b_edges\"], dtype=float)\n        x = np.array(case[\"x_values\"], dtype=float)\n        \n        counts = calculate_bin_counts(b, x)\n        all_results.append(counts)\n\n    # Format the final output string according to the specification.\n    # R_k is formatted as a string like \"[c1,c2,c3]\" with no spaces.\n    formatted_results = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    \n    # The final output is a comma-separated list of these formatted strings,\n    # enclosed in a single pair of square brackets.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3510256"}, {"introduction": "While binning data is straightforward, the choice of bin width profoundly impacts the resulting histogram's shape and our interpretation of the underlying physics. This practice [@problem_id:3510284] introduces a powerful, data-driven approach—least-squares cross-validation—to navigate the bias-variance trade-off and select an optimal bin width. By deriving and implementing this technique, you will gain a principled method for density estimation that minimizes reliance on arbitrary choices and lets the data speak for itself.", "problem": "You are given a univariate dataset of size $N$ and asked to perform density estimation using a piecewise-constant histogram estimator with bin width $h$. Consider the following definitions as the fundamental base.\n\n- The histogram density estimator $\\hat{f}_h(x)$ with bin width $h$ is defined by partitioning the real line into adjacent bins of width $h$ and assigning a constant density value within each bin equal to the bin count divided by $N h$.\n- The risk of a density estimator can be measured by the Integrated Squared Error (ISE), defined as $\\int \\left(\\hat{f}_h(x) - f(x)\\right)^2 \\, dx$, where $f(x)$ is the unknown true density.\n- Leave-One-Out Cross-Validation (LOOCV) constructs, for each data point $x_i$, a density estimator that is fit on all data except $x_i$, which we denote $\\hat{f}_{-i,h}(x)$, to mitigate the bias of evaluating the estimator at training points.\n\nTask:\n1. Starting from the above definitions and without invoking any pre-formed shortcut formulas, derive a principled least-squares cross-validation (LSCV) criterion based on LOOCV that estimates the ISE for histogram density estimation. Your derivation must express the LOOCV objective exclusively in terms of the dataset $\\{x_i\\}_{i=1}^N$, the bin width $h$, and the induced bin counts under $h$. You should make explicit the binning scheme and how to map a data point to its bin.\n2. Implement a procedure that, for a given finite candidate set of bin widths, computes the LOOCV loss for each candidate $h$ and returns the $h$ that minimizes the loss. If multiple $h$ attain the same minimum (up to numerical equality), return the smallest such $h$.\n3. Binning scheme specification (must be used exactly for full credit):\n   - For a given $h0$, define the anchor $a = \\left\\lfloor \\frac{\\min_i x_i}{h} \\right\\rfloor h$.\n   - Define bin edges as $a + k h$ for integer $k$, and use half-open bins of the form $[a + k h, a + (k+1) h)$ for all bins except the last one, which includes the right endpoint of the data range. In practice, construct enough consecutive bins to cover all $x_i$ so that every point lies in exactly one bin.\n   - Map each $x_i$ to its bin index via $b(i) = \\left\\lfloor \\frac{x_i - a}{h} \\right\\rfloor$.\n4. Constraints and assumptions:\n   - Assume $N \\geq 2$.\n   - Your implementation must be deterministic and must not rely on any randomization.\n   - No physical units are involved in this problem.\n5. Test Suite:\n   For each test case, you are given a dataset $\\{x_i\\}$ and a candidate set $\\mathcal{H}$ of bin widths. For each test case, return the single $h \\in \\mathcal{H}$ that minimizes your LOOCV loss.\n   - Test case $1$:\n     - Data: $[\\,0.1,\\,0.2,\\,0.25,\\,0.9,\\,1.1,\\,1.2,\\,1.25,\\,2.0,\\,2.1,\\,2.2\\,]$\n     - Candidates $\\mathcal{H}$: $[\\,0.1,\\,0.2,\\,0.5,\\,1.0\\,]$\n   - Test case $2$:\n     - Data: $[\\,-1.0,\\,-0.9,\\,-0.9,\\,0.0,\\,0.1,\\,0.1,\\,0.2,\\,1.5,\\,1.6\\,]$\n     - Candidates $\\mathcal{H}$: $[\\,0.1,\\,0.25,\\,0.5,\\,1.0\\,]$\n   - Test case $3$:\n     - Data: $[\\,0.0,\\,0.49,\\,1.01\\,]$\n     - Candidates $\\mathcal{H}$: $[\\,0.25,\\,0.5,\\,1.0\\,]$\n   - Test case $4$:\n     - Data: $[\\,3.0,\\,3.1,\\,3.2,\\,7.8,\\,7.9,\\,8.0,\\,8.1,\\,8.2\\,]$\n     - Candidates $\\mathcal{H}$: $[\\,0.1,\\,0.2,\\,0.5,\\,1.0,\\,2.0\\,]$\n6. Final output format:\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases. For example, the output must look like $[h_1,h_2,h_3,h_4]$, where each $h_j$ is the selected bin width for test case $j$ expressed as a decimal number.\n\nYour program must be a complete, runnable implementation that takes no input and prints exactly one line in the specified format. It must use only the standard library and the specified numerical libraries, and reproduce the specified binning rules exactly. The answer for each test case is a single float (the chosen $h$).", "solution": "The objective is to derive a least-squares cross-validation (LSCV) criterion for selecting the bin width $h$ of a histogram density estimator and to implement an algorithm to find the optimal $h$ from a candidate set.\n\nThe derivation begins with the Integrated Squared Error (ISE), which measures the discrepancy between the true density $f(x)$ and the histogram estimator $\\hat{f}_h(x)$. The ISE is defined as:\n$$\nJ(h) = \\text{ISE} = \\int \\left( \\hat{f}_h(x) - f(x) \\right)^2 dx\n$$\nExpanding the square, we obtain:\n$$\nJ(h) = \\int \\hat{f}_h(x)^2 dx - 2 \\int \\hat{f}_h(x) f(x) dx + \\int f(x)^2 dx\n$$\nThe goal of cross-validation is to find the value of $h$ that minimizes $J(h)$. The term $\\int f(x)^2 dx$ is a constant with respect to $h$ and can be ignored during minimization. We are left with minimizing the risk function $R(h)$:\n$$\nR(h) = \\int \\hat{f}_h(x)^2 dx - 2 \\int \\hat{f}_h(x) f(x) dx\n$$\nThe challenge is that $R(h)$ still depends on the unknown density $f(x)$. The Leave-One-Out Cross-Validation (LOOCV) approach provides a nearly unbiased estimate of $R(h)$ by replacing the expectation term $\\mathbb{E}[\\hat{f}_h(X)] = \\int \\hat{f}_h(x) f(x) dx$ with its leave-one-out estimate. This involves averaging the values of a leave-one-out estimator $\\hat{f}_{-i,h}(x)$ at each omitted point $x_i$. The LSCV objective function, which we denote $L(h)$, is thus:\n$$\nL(h) = \\int \\hat{f}_h(x)^2 dx - \\frac{2}{N} \\sum_{i=1}^N \\hat{f}_{-i,h}(x_i)\n$$\nWe now derive expressions for both terms in $L(h)$ using the provided binning scheme. Let the set of bins be $\\{B_k\\}$ and the count of data points in bin $B_k$ be $n_k$. The total number of data points is $N = \\sum_k n_k$. The histogram estimator is defined as $\\hat{f}_h(x) = \\frac{n_k}{Nh}$ for any $x \\in B_k$. The width of each bin is $h$.\n\nThe first term, $\\int \\hat{f}_h(x)^2 dx$, is the integrated squared value of the estimator. Since the estimator is piecewise constant, we can write the integral as a sum over a complete set of bins:\n$$\n\\int \\hat{f}_h(x)^2 dx = \\sum_k \\int_{B_k} \\left( \\frac{n_k}{Nh} \\right)^2 dx = \\sum_k \\left( \\frac{n_k}{Nh} \\right)^2 \\cdot (\\text{width of } B_k) = \\sum_k \\frac{n_k^2}{N^2 h^2} h = \\frac{1}{N^2 h} \\sum_k n_k^2\n$$\n\nThe second term involves the leave-one-out estimator $\\hat{f}_{-i,h}(x_i)$. This estimator is constructed from a dataset of size $N-1$ (all points except $x_i$) but using the same binning grid as the full-sample estimator $\\hat{f}_h(x)$. Let $x_i$ be a point in bin $B_{k_i}$. The count of points in this bin for the full dataset is $n_{k_i}$. For the leave-one-out estimator $\\hat{f}_{-i,h}$, the total number of points is $N-1$, and the count of points in bin $B_{k_i}$ is $n_{k_i}-1$. Therefore, the value of the estimator at $x_i$ is:\n$$\n\\hat{f}_{-i,h}(x_i) = \\frac{\\text{count in bin } B_{k_i} \\text{ (w/o } x_i)}{\\text{(total points w/o } x_i) \\times h} = \\frac{n_{k_i} - 1}{(N-1)h}\n$$\nWe sum this quantity over all $i=1, \\dots, N$:\n$$\n\\sum_{i=1}^N \\hat{f}_{-i,h}(x_i) = \\sum_{i=1}^N \\frac{n_{k_i} - 1}{(N-1)h}\n$$\nThis sum can be regrouped by bins. For each bin $B_k$, there are $n_k$ points, and for each of these points, the term is $\\frac{n_k - 1}{(N-1)h}$. So, the sum becomes:\n$$\n\\sum_{i=1}^N \\hat{f}_{-i,h}(x_i) = \\sum_k n_k \\left( \\frac{n_k - 1}{(N-1)h} \\right) = \\frac{1}{(N-1)h} \\sum_k (n_k^2 - n_k)\n$$\n\nNow we substitute these two derived expressions back into the objective function $L(h)$:\n$$\nL(h) = \\frac{1}{N^2 h} \\sum_k n_k^2 - \\frac{2}{N} \\left( \\frac{1}{(N-1)h} \\sum_k (n_k^2 - n_k) \\right)\n$$\nFactoring out common terms and using $\\sum_k n_k = N$:\n$$\nL(h) = \\frac{1}{h} \\left[ \\frac{1}{N^2} \\sum_k n_k^2 - \\frac{2}{N(N-1)} \\left( \\sum_k n_k^2 - \\sum_k n_k \\right) \\right]\n$$\n$$\nL(h) = \\frac{1}{h} \\left[ \\frac{1}{N^2} \\sum_k n_k^2 - \\frac{2}{N(N-1)} \\left( \\sum_k n_k^2 - N \\right) \\right]\n$$\n$$\nL(h) = \\frac{1}{h} \\left[ \\left(\\frac{1}{N^2} - \\frac{2}{N(N-1)}\\right) \\sum_k n_k^2 + \\frac{2N}{N(N-1)} \\right]\n$$\nCombining the coefficients of $\\sum_k n_k^2$:\n$$\n\\frac{1}{N^2} - \\frac{2}{N(N-1)} = \\frac{N-1 - 2N}{N^2(N-1)} = \\frac{-(N+1)}{N^2(N-1)}\n$$\nSubstituting this back, we get:\n$$\nL(h) = \\frac{1}{h} \\left[ \\frac{2N}{N(N-1)} - \\frac{N+1}{N^2(N-1)} \\sum_k n_k^2 \\right]\n$$\nMultiplying the second term by $N/N$ to get a common denominator gives the final expression for the LSCV loss function:\n$$\nL(h) = \\frac{2N^2 - (N+1)\\sum_k n_k^2}{N^2(N-1)h}\n$$\nThis expression depends only on the sample size $N$, the bin width $h$, and the bin counts $\\{n_k\\}$ induced by $h$.\n\nThe algorithmic procedure is to iterate through each candidate bin width $h$ from the set $\\mathcal{H}$. For each $h$, we perform the following steps:\n1.  Calculate the binning anchor $a = \\lfloor \\frac{\\min_i x_i}{h} \\rfloor h$.\n2.  For each data point $x_i$, determine its bin index $b(i) = \\lfloor \\frac{x_i - a}{h} \\rfloor$.\n3.  Compute the bin counts $\\{n_k\\}$ by counting the occurrences of each unique bin index.\n4.  Calculate the sum of squared counts, $\\sum_k n_k^2$.\n5.  Evaluate the loss $L(h)$ using the derived formula.\n6.  The optimal $h$ is the one that results in the minimum value of $L(h)$. If a tie occurs, the smallest $h$ is chosen.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LSCV problem for histogram density estimation across multiple test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([0.1, 0.2, 0.25, 0.9, 1.1, 1.2, 1.25, 2.0, 2.1, 2.2]), [0.1, 0.2, 0.5, 1.0]),\n        (np.array([-1.0, -0.9, -0.9, 0.0, 0.1, 0.1, 0.2, 1.5, 1.6]), [0.1, 0.25, 0.5, 1.0]),\n        (np.array([0.0, 0.49, 1.01]), [0.25, 0.5, 1.0]),\n        (np.array([3.0, 3.1, 3.2, 7.8, 7.9, 8.0, 8.1, 8.2]), [0.1, 0.2, 0.5, 1.0, 2.0]),\n    ]\n\n    results = []\n    for data, candidates in test_cases:\n        N = data.shape[0]\n        min_loss = float('inf')\n        best_h = -1.0\n\n        # Iterate through each candidate bin width\n        for h in candidates:\n            # 1. Calculate the binning anchor\n            min_x = np.min(data)\n            a = np.floor(min_x / h) * h\n\n            # 2. Determine bin index for each data point\n            bin_indices = np.floor((data - a) / h)\n\n            # 3. Compute bin counts\n            # np.unique returns unique elements and their counts\n            _, counts = np.unique(bin_indices, return_counts=True)\n\n            # 4. Calculate the sum of squared counts\n            sum_nk_sq = np.sum(np.power(counts, 2))\n\n            # 5. Evaluate the LSCV loss L(h) using the derived formula\n            # L(h) = (2*N^2 - (N+1)*sum(n_k^2)) / (N^2*(N-1)*h)\n            # We can ignore the constant positive denominator N^2*(N-1)\n            # when comparing losses for different h values, but we will\n            # compute the full loss for correctness.\n            # The case N=1 is not possible due to problem constraints (N=2).\n            numerator = 2 * N**2 - (N + 1) * sum_nk_sq\n            denominator = N**2 * (N - 1) * h\n            loss = numerator / denominator\n\n            # 6. Track the h that minimizes the loss\n            # The candidate list is sorted, so the first h to achieve the\n            # minimum will be the smallest one.\n            if loss  min_loss:\n                min_loss = loss\n                best_h = h\n        \n        results.append(best_h)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3510284"}, {"introduction": "In particle physics, bin counts are fundamentally Poisson-distributed, yet the Gaussian approximation with uncertainty $\\sigma = \\sqrt{n}$ is ubiquitous due to its simplicity. This exercise [@problem_id:3510291] provides a hands-on exploration of the limits of this approximation by tasking you with computing the true frequentist coverage of the resulting confidence intervals. By determining the threshold count below which this simple rule fails, you will develop a deeper, practical understanding of statistical uncertainties in histogram-based measurements.", "problem": "In computational high-energy physics, binned event counts in histograms are modeled as draws from a Poisson process. Let $N \\sim \\mathrm{Poisson}(\\lambda)$ denote the count in a single histogram bin for a fixed but unknown mean rate $\\lambda \\in [0,\\infty)$. A common approximation for the statistical uncertainty on a count is the Gaussian rule $\\sigma = \\sqrt{n}$, applied symmetrically to form an interval around the observed count $n \\in \\{0,1,2,\\dots\\}$ on the count scale. When such a symmetric error bar is reused as an approximate confidence interval for the underlying mean rate $\\lambda$, the frequentist coverage can deviate from a specified nominal coverage because $N$ is discrete and $\\lambda$ is nonnegative.\n\nDefine the symmetric interval on the mean-rate scale induced by the Gaussian rule as\n$$\nI(n; z) = \\left[\\max\\{0,\\, n - z\\sqrt{n}\\},\\; n + z\\sqrt{n}\\right],\n$$\nwhere $z \\ge 0$ is a chosen standard-normal quantile used to define the nominal central coverage. The nominal central coverage associated with $z$ is\n$$\nC_0(z) = \\Phi(z) - \\Phi(-z),\n$$\nwhere $\\Phi$ is the cumulative distribution function of the standard normal distribution. For a fixed $\\lambda$, the frequentist coverage of the interval rule $I(\\cdot; z)$ is\n$$\nC(\\lambda; z) = \\sum_{k=0}^{\\infty} \\mathbf{1}\\{\\lambda \\in I(k; z)\\} \\, \\Pr(N=k \\mid \\lambda),\n$$\nwith $\\Pr(N=k \\mid \\lambda) = e^{-\\lambda} \\lambda^k/k!$ and $\\mathbf{1}\\{\\cdot\\}$ the indicator function.\n\nYou are asked to determine the count regime in which the Gaussian approximation with $\\sigma=\\sqrt{n}$ achieves a coverage error smaller than a specified tolerance. For a given tolerance parameter $\\epsilon0$ and a specified error metric type, define the coverage error at $\\lambda$ as either the absolute error\n$$\nE_{\\mathrm{abs}}(\\lambda; z) = \\left|C(\\lambda; z) - C_0(z)\\right|\n$$\nor the relative error\n$$\nE_{\\mathrm{rel}}(\\lambda; z) = \\frac{\\left|C(\\lambda; z) - C_0(z)\\right|}{C_0(z)}.\n$$\nFor a given grid $\\{\\lambda_j\\}$ on $[0,\\Lambda_{\\max}]$ with uniform step $\\Delta\\lambda$, define the minimal threshold count $n_{\\min}$ as the smallest integer satisfying that for all grid points $\\lambda_j \\ge n_{\\min}$, the chosen error $E(\\lambda_j; z)$ is less than or equal to $\\epsilon$.\n\nStarting from the fundamental definitions above and without using any pre-tabulated approximations, write a program that:\n- Computes $C(\\lambda; z)$ by summing the Poisson probability mass function across all $k \\in \\{0,1,2,\\dots\\}$ for which $\\lambda \\in I(k; z)$, with a mathematically justified truncation of the infinite sum that guarantees a negligible residual beyond the truncation.\n- Computes the nominal central normal coverage $C_0(z)$ from first principles.\n- Evaluates either $E_{\\mathrm{abs}}$ or $E_{\\mathrm{rel}}$ on a uniform grid $\\{\\lambda_j\\}_{j=0}^{J}$ with $\\lambda_0 = 0$, $\\lambda_J = \\Lambda_{\\max}$, and $\\lambda_{j+1} - \\lambda_j = \\Delta\\lambda$.\n- Finds the minimal integer $n_{\\min}$ such that for every grid point $\\lambda_j \\ge n_{\\min}$ the error is less than or equal to $\\epsilon$. If no such integer exists within the scanned range, return $-1$.\n\nYour implementation must avoid any external input and must use the following test suite of parameter values:\n- Test case $1$ (general case): $z = 1.0$, relative error with $\\epsilon = 0.10$, $\\Lambda_{\\max} = 200.0$, $\\Delta\\lambda = 0.05$.\n- Test case $2$ (stricter tolerance): $z = 1.0$, relative error with $\\epsilon = 0.05$, $\\Lambda_{\\max} = 200.0$, $\\Delta\\lambda = 0.05$.\n- Test case $3$ (wider interval): $z = 2.0$, relative error with $\\epsilon = 0.10$, $\\Lambda_{\\max} = 200.0$, $\\Delta\\lambda = 0.05$.\n- Test case $4$ (absolute error): $z = 1.0$, absolute error with $\\epsilon = 0.02$, $\\Lambda_{\\max} = 200.0$, $\\Delta\\lambda = 0.05$.\n\nYour program should produce a single line of output containing the four thresholds $[n_{\\min}^{(1)}, n_{\\min}^{(2)}, n_{\\min}^{(3)}, n_{\\min}^{(4)}]$ as a comma-separated list enclosed in square brackets, for the four test cases listed in the order above. All values must be integers. No physical units are involved. All tolerances are specified as decimals as required. The algorithm must be expressed in purely mathematical and logical terms and must be understandable and implementable in any modern programming language; you must output the results exactly in the specified single-line format.", "solution": "### Principle-Based Solution Design\n\nThe core task is to find the minimum integer count threshold, $n_{\\min}$, above which a symmetric Gaussian-inspired confidence interval for a Poisson mean $\\lambda$ achieves a specified level of coverage accuracy. The algorithm proceeds in four main stages: (1) calculating the nominal coverage $C_0(z)$, (2) computing the actual frequentist coverage $C(\\lambda; z)$ over a grid of $\\lambda$ values, (3) evaluating the coverage error, and (4) searching for the threshold $n_{\\min}$.\n\n#### 1. Nominal Coverage Calculation\n\nThe nominal central coverage, $C_0(z)$, is defined by the area under the standard normal distribution between $-z$ and $z$.\n$$\nC_0(z) = \\Phi(z) - \\Phi(-z)\n$$\nwhere $\\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution. The standard normal CDF can be expressed in terms of the error function, $\\mathrm{erf}(x)$, which is widely available in scientific computing libraries. The relationship is $\\Phi(x) = \\frac{1}{2}\\left(1 + \\mathrm{erf}(x/\\sqrt{2})\\right)$. Substituting this into the definition of $C_0(z)$ and using the property that $\\mathrm{erf}(-x) = -\\mathrm{erf}(x)$, we obtain a direct formula:\n$$\nC_0(z) = \\frac{1}{2}\\left(1 + \\mathrm{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right) - \\frac{1}{2}\\left(1 + \\mathrm{erf}\\left(\\frac{-z}{\\sqrt{2}}\\right)\\right) = \\frac{1}{2}\\left(\\mathrm{erf}\\left(\\frac{z}{\\sqrt{2}}\\right) - \\mathrm{erf}\\left(\\frac{-z}{\\sqrt{2}}\\right)\\right) = \\mathrm{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\n$$\nThis formula provides a numerically stable and efficient way to compute $C_0(z)$ from first principles.\n\n#### 2. Frequentist Coverage Calculation\n\nThe frequentist coverage $C(\\lambda; z)$ for a given mean rate $\\lambda$ is the probability that the random interval $I(N; z)$ contains $\\lambda$. This is calculated by summing the probabilities of all observed counts $k$ for which the corresponding interval $I(k; z)$ \"covers\" $\\lambda$.\n$$\nC(\\lambda; z) = \\sum_{k=0}^{\\infty} \\mathbf{1}\\{\\lambda \\in I(k; z)\\} \\, \\Pr(N=k \\mid \\lambda)\n$$\nFor any given $\\lambda \\ge 0$, the set of integers $k$ for which the indicator function $\\mathbf{1}\\{\\lambda \\in I(k; z)\\}$ is non-zero is finite. We can determine the exact integer range for the summation, thus avoiding any approximation from truncation.\n\nThe condition $\\lambda \\in I(k; z)$ is equivalent to the inequality system:\n$$\n\\max\\{0, k - z\\sqrt{k}\\} \\le \\lambda \\le k + z\\sqrt{k}\n$$\nFor $\\lambda  0$, this simplifies to a pair of inequalities for $k$. By letting $y = \\sqrt{k}$ (so $y \\ge 0$), the inequalities become quadratic in $y$:\n1. $y^2 - zy - \\lambda \\le 0$\n2. $y^2 + zy - \\lambda \\ge 0$\n\nSolving the corresponding quadratic equalities for their roots and analyzing the inequalities for $y = \\sqrt{k} \\ge 0$ gives the solution range for $\\sqrt{k}$:\n$$\n\\frac{-z + \\sqrt{z^2 + 4\\lambda}}{2} \\le \\sqrt{k} \\le \\frac{z + \\sqrt{z^2 + 4\\lambda}}{2}\n$$\nSquaring these bounds gives the range for $k$:\n$$\nk_{\\min}(\\lambda, z) \\le k \\le k_{\\max}(\\lambda, z)\n$$\nwhere $k_{\\min}(\\lambda, z) = \\left(\\frac{-z + \\sqrt{z^2 + 4\\lambda}}{2}\\right)^2$ and $k_{\\max}(\\lambda, z) = \\left(\\frac{z + \\sqrt{z^2 + 4\\lambda}}{2}\\right)^2$.\nThe summation is therefore over all integers $k$ such that $\\lceil k_{\\min}(\\lambda, z) \\rceil \\le k \\le \\lfloor k_{\\max}(\\lambda, z) \\rfloor$. This range is finite for any $\\lambda  0$. If $\\lceil k_{\\min} \\rceil  \\lfloor k_{\\max} \\rfloor$, the sum is empty and $C(\\lambda, z) = 0$.\n\nFor the special case $\\lambda = 0$, the coverage condition $0 \\in I(k; z)$ simplifies to $\\max\\{0, k - z\\sqrt{k}\\} = 0$, which holds if and only if $k - z\\sqrt{k} \\le 0$, i.e., $k \\le z^2$. The only non-zero term in the Poisson sum for $\\lambda=0$ is $\\Pr(N=0 \\mid \\lambda=0) = 1$. Since $k=0$ always satisfies $k \\le z^2$ (as $z \\ge 0$), the interval $I(0;z)=[0,0]$ covers $\\lambda=0$, and thus $C(0;z) = 1$.\n\nThe algorithm computes $C(\\lambda; z)$ by iterating over the determined integer range $[\\lceil k_{\\min} \\rceil, \\lfloor k_{\\max} \\rfloor]$ and summing the Poisson probabilities $\\Pr(N=k \\mid \\lambda) = e^{-\\lambda}\\lambda^k/k!$, which are calculated using a numerically stable library function.\n\n#### 3. Error Evaluation\n\nWith $C_0(z)$ and $C(\\lambda_j; z)$ for each point on the grid $\\{\\lambda_j\\}$, the error is calculated as specified:\n- Absolute error: $E_{\\mathrm{abs}}(\\lambda_j; z) = |C(\\lambda_j; z) - C_0(z)|$\n- Relative error: $E_{\\mathrm{rel}}(\\lambda_j; z) = |C(\\lambda_j; z) - C_0(z)| / C_0(z)$\n\nThis produces an array of error values, one for each point $\\lambda_j$ on the grid.\n\n#### 4. Determination of the Threshold $n_{\\min}$\n\nThe final step is to find the smallest integer $n_{\\min}$ such that for every grid point $\\lambda_j \\ge n_{\\min}$, the error $E(\\lambda_j; z)$ does not exceed the tolerance $\\epsilon$. A direct and efficient method is to search backwards from the end of the $\\lambda$ grid.\n\n1.  We identify the largest value on the grid, let's call it $\\lambda_{\\text{fail}}$, for which the error condition $E(\\lambda_{\\text{fail}}; z) \\le \\epsilon$ is violated. This is found by iterating backwards through the computed error array.\n2.  If no such $\\lambda_{\\text{fail}}$ exists (i.e., the error is within tolerance for all $\\lambda_j$), then the condition holds for all $\\lambda_j \\ge 0$, and the smallest integer threshold is $n_{\\min} = 0$.\n3.  If such a $\\lambda_{\\text{fail}}$ is found, any valid integer threshold $n_{\\min}$ must be strictly greater than $\\lambda_{\\text{fail}}$ to ensure all subsequent grid points $\\lambda_j \\ge n_{\\min}$ satisfy the error criterion. The smallest integer that satisfies this is $n_{\\min} = \\lfloor \\lambda_{\\text{fail}} \\rfloor + 1$.\n4.  Finally, we check if the computed $n_{\\min}$ is within the scanned range up to $\\Lambda_{\\max}$. If $n_{\\min}  \\lfloor \\Lambda_{\\max} \\rfloor$, it implies that a satisfactory threshold does not exist within the specified integer range, and the result is $-1$. Otherwise, the computed $n_{\\min}$ is the answer.", "answer": "```python\nimport numpy as np\nfrom scipy.special import erf\nfrom scipy.stats import poisson\n\ndef solve():\n    \"\"\"\n    Solves for the minimal threshold count n_min for four test cases.\n    \"\"\"\n    test_cases = [\n        # z, error_type, epsilon, Lambda_max, delta_lambda\n        (1.0, 'relative', 0.10, 200.0, 0.05),\n        (1.0, 'relative', 0.05, 200.0, 0.05),\n        (2.0, 'relative', 0.10, 200.0, 0.05),\n        (1.0, 'absolute', 0.02, 200.0, 0.05),\n    ]\n\n    results = []\n\n    for z, error_type, epsilon, Lambda_max, delta_lambda in test_cases:\n        # Step 1: Compute nominal coverage C_0(z)\n        # C_0(z) = Phi(z) - Phi(-z) = erf(z / sqrt(2))\n        c0 = erf(z / np.sqrt(2))\n\n        # Step 2: Set up the lambda grid and calculate coverage errors\n        num_points = int(round(Lambda_max / delta_lambda)) + 1\n        lambdas = np.linspace(0.0, Lambda_max, num_points)\n        errors = np.zeros_like(lambdas)\n\n        for i, lambda_val in enumerate(lambdas):\n            # Step 2a: Compute frequentist coverage C(lambda, z)\n            if lambda_val == 0.0:\n                # For lambda=0, P(N=0)=1. The interval I(0;z)=[0,0] covers lambda=0.\n                # For k0, P(N=k)=0. So C(0,z) = 1.\n                c_lambda_z = 1.0\n            else:\n                # For lambda  0, find the finite range of k for the sum.\n                # The condition is k - z*sqrt(k) = lambda = k + z*sqrt(k).\n                # This defines a range [k_min, k_max].\n                sqrt_term = np.sqrt(z**2 + 4 * lambda_val)\n                k_min_val = ((-z + sqrt_term) / 2)**2\n                k_max_val = ((z + sqrt_term) / 2)**2\n                \n                k_start = int(np.ceil(k_min_val))\n                k_end = int(np.floor(k_max_val))\n\n                if k_start  k_end:\n                    c_lambda_z = 0.0\n                else:\n                    k_range = np.arange(k_start, k_end + 1)\n                    # Sum Poisson probabilities over the determined range of k\n                    pmf_values = poisson.pmf(k_range, mu=lambda_val)\n                    c_lambda_z = np.sum(pmf_values)\n\n            # Step 2b: Compute the error E(lambda, z)\n            if error_type == 'absolute':\n                error = np.abs(c_lambda_z - c0)\n            else:  # 'relative'\n                # Avoid division by zero if c0 is zero, though not possible for z0\n                error = np.abs(c_lambda_z - c0) / c0 if c0 != 0 else 0.0\n            \n            errors[i] = error\n\n        # Step 3: Find the minimal threshold count n_min\n        # Find the index of the last lambda value where the error exceeds the tolerance.\n        last_bad_idx = -1\n        # Search backwards from the end of the grid.\n        for i in range(len(lambdas) - 1, -1, -1):\n            if errors[i]  epsilon:\n                last_bad_idx = i\n                break\n        \n        if last_bad_idx == -1:\n            # If no lambda value has an error  epsilon, the condition holds for all lambda = 0.\n            n_min = 0\n        else:\n            # The condition is violated up to lambda_fail = lambdas[last_bad_idx].\n            # n_min must be an integer  lambda_fail.\n            lambda_fail = lambdas[last_bad_idx]\n            n_min = int(np.floor(lambda_fail)) + 1\n        \n        # If the required n_min is outside the scanned range, no solution exists.\n        if n_min  int(Lambda_max):\n            results.append(-1)\n        else:\n            results.append(n_min)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3510291"}]}