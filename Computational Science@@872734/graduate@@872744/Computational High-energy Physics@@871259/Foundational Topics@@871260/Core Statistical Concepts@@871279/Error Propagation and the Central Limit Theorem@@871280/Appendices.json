{"hands_on_practices": [{"introduction": "The standard formula for error propagation, which relies on a first-order Taylor expansion, is a cornerstone of data analysis. However, it is an approximation that assumes a linear relationship between a measured quantity and its underlying sources of uncertainty. This practice [@problem_id:3513019] challenges you to go beyond this linear approximation by exploring a scenario where the curvature, or second-order term, in the relationship is significant. By deriving and quantifying the correction to the variance, you will gain a deeper understanding of how non-linear systematic effects can impact the final uncertainty of a measurement, a crucial consideration in precision analyses like extracting a resonance mass.", "problem": "In a computational extraction of a resonance pole mass from a high-statistics sample, consider a maximum-likelihood estimator $\\hat{M}$ for the resonance mass that depends on a single dominant nuisance parameter $\\varepsilon$ representing a fractional global energy-scale offset. The calibration procedure estimates $\\varepsilon$ by averaging many independent channels, so by the Central Limit Theorem (CLT) the distribution of $\\varepsilon$ is well approximated as Gaussian with mean $0$ and known variance $\\sigma_{\\varepsilon}^{2}$. For sufficiently small $|\\varepsilon|$, the mapping $\\hat{M}(\\varepsilon)$ is smooth and can be expanded to second order about $\\varepsilon=0$ as\n$$\n\\hat{M}(\\varepsilon) = M_{0} + A\\,\\varepsilon + B\\,\\varepsilon^{2} + \\mathcal{O}(\\varepsilon^{3}),\n$$\nwhere $M_{0}$, $A$, and $B$ are constants determined by the fit model and data-taking conditions. Assume that any statistical fluctuations of $\\hat{M}$ unrelated to $\\varepsilon$ are negligible for this exercise, so that the only source of uncertainty is the randomness of $\\varepsilon$.\n\nStarting from first principles (the definition of variance and a second-order Taylor expansion), and assuming $\\varepsilon$ is Gaussian with mean $0$, derive an expression for $\\mathrm{Var}[\\hat{M}]$ up to and including terms of order $\\sigma_{\\varepsilon}^{4}$. In your derivation, identify the linear error-propagation contribution $\\mathrm{Var}_{\\mathrm{lin}} = A^{2}\\sigma_{\\varepsilon}^{2}$ and the additional curvature-induced correction $\\Delta \\mathrm{Var}$ that appears at order $\\sigma_{\\varepsilon}^{4}$. Provide the final symbolic expression for $\\Delta \\mathrm{Var}$ in terms of $A$, $B$, and $\\sigma_{\\varepsilon}$.\n\nDefine the ratio $R \\equiv \\Delta \\mathrm{Var}/\\mathrm{Var}_{\\mathrm{lin}}$ and evaluate it numerically for $A = 125.1\\,\\text{GeV}$, $B = 350\\,\\text{GeV}$, and $\\sigma_{\\varepsilon} = 3.0 \\times 10^{-3}$. Express the final numerical answer for $R$ as a pure number rounded to four significant figures. Do not include units in your final answer.", "solution": "The problem requires the derivation of the variance of a maximum-likelihood estimator $\\hat{M}$ which is a function of a Gaussian-distributed nuisance parameter $\\varepsilon$. The estimator is given by the second-order Taylor expansion:\n$$\n\\hat{M}(\\varepsilon) = M_{0} + A\\,\\varepsilon + B\\,\\varepsilon^{2}\n$$\nwhere we neglect terms of $\\mathcal{O}(\\varepsilon^{3})$ and higher for this derivation. The nuisance parameter $\\varepsilon$ follows a Gaussian distribution with mean $E[\\varepsilon]=0$ and variance $\\mathrm{Var}[\\varepsilon]=\\sigma_{\\varepsilon}^{2}$.\n\nThe variance of a random variable $X$ is defined as $\\mathrm{Var}[X] = E[(X - E[X])^2]$, where $E[\\cdot]$ denotes the expectation value. We will apply this definition to $X = \\hat{M}(\\varepsilon)$.\n\nFirst, we calculate the expectation value of $\\hat{M}$:\n$$\nE[\\hat{M}] = E[M_{0} + A\\,\\varepsilon + B\\,\\varepsilon^{2}]\n$$\nUsing the linearity of the expectation operator, and since $M_0$, $A$, and $B$ are constants:\n$$\nE[\\hat{M}] = M_{0} + A\\,E[\\varepsilon] + B\\,E[\\varepsilon^{2}]\n$$\nWe are given that $E[\\varepsilon] = 0$. The expectation value $E[\\varepsilon^{2}]$ is related to the variance of $\\varepsilon$ by the formula $\\mathrm{Var}[\\varepsilon] = E[\\varepsilon^{2}] - (E[\\varepsilon])^2$. Since $E[\\varepsilon]=0$, we have $E[\\varepsilon^{2}] = \\mathrm{Var}[\\varepsilon] = \\sigma_{\\varepsilon}^{2}$.\nSubstituting these values, we get the expectation of $\\hat{M}$:\n$$\nE[\\hat{M}] = M_{0} + A(0) + B(\\sigma_{\\varepsilon}^{2}) = M_{0} + B\\sigma_{\\varepsilon}^{2}\n$$\nNext, we calculate the term $\\hat{M} - E[\\hat{M}]$:\n$$\n\\hat{M} - E[\\hat{M}] = (M_{0} + A\\,\\varepsilon + B\\,\\varepsilon^{2}) - (M_{0} + B\\sigma_{\\varepsilon}^{2}) = A\\,\\varepsilon + B(\\varepsilon^{2} - \\sigma_{\\varepsilon}^{2})\n$$\nNow, we can compute the variance of $\\hat{M}$:\n$$\n\\mathrm{Var}[\\hat{M}] = E\\left[ \\left( A\\,\\varepsilon + B(\\varepsilon^{2} - \\sigma_{\\varepsilon}^{2}) \\right)^2 \\right]\n$$\nExpanding the square:\n$$\n\\mathrm{Var}[\\hat{M}] = E\\left[ A^2\\varepsilon^2 + 2AB\\varepsilon(\\varepsilon^2 - \\sigma_{\\varepsilon}^2) + B^2(\\varepsilon^2 - \\sigma_{\\varepsilon}^2)^2 \\right]\n$$\nBy linearity of expectation:\n$$\n\\mathrm{Var}[\\hat{M}] = A^2E[\\varepsilon^2] + 2AB\\,E[\\varepsilon^3 - \\varepsilon\\sigma_{\\varepsilon}^2] + B^2E[(\\varepsilon^2 - \\sigma_{\\varepsilon}^2)^2]\n$$\nTo evaluate this, we need the moments of the centered Gaussian distribution $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^2)$. The odd moments are zero, and the even moments are given by $E[\\varepsilon^n] = (n-1)!!\\,\\sigma_{\\varepsilon}^n$, where $(n-1)!! = (n-1)(n-3)\\cdots 1$.\nThe required moments are:\n$E[\\varepsilon] = 0$\n$E[\\varepsilon^2] = 1!!\\,\\sigma_{\\varepsilon}^2 = \\sigma_{\\varepsilon}^2$\n$E[\\varepsilon^3] = 0$\n$E[\\varepsilon^4] = 3!!\\,\\sigma_{\\varepsilon}^4 = 3\\sigma_{\\varepsilon}^4$\n\nLet's evaluate each term in the variance expression:\nThe first term is $A^2E[\\varepsilon^2] = A^2\\sigma_{\\varepsilon}^2$.\nThe second term is $2AB\\,(E[\\varepsilon^3] - \\sigma_{\\varepsilon}^2E[\\varepsilon]) = 2AB\\,(0 - \\sigma_{\\varepsilon}^2(0)) = 0$.\nThe third term is $B^2E[(\\varepsilon^2 - \\sigma_{\\varepsilon}^2)^2] = B^2E[\\varepsilon^4 - 2\\varepsilon^2\\sigma_{\\varepsilon}^2 + \\sigma_{\\varepsilon}^4]$.\nUsing linearity of expectation for the third term:\n$B^2(E[\\varepsilon^4] - 2\\sigma_{\\varepsilon}^2E[\\varepsilon^2] + E[\\sigma_{\\varepsilon}^4]) = B^2(3\\sigma_{\\varepsilon}^4 - 2\\sigma_{\\varepsilon}^2(\\sigma_{\\varepsilon}^2) + \\sigma_{\\varepsilon}^4) = B^2(3\\sigma_{\\varepsilon}^4 - 2\\sigma_{\\varepsilon}^4 + \\sigma_{\\varepsilon}^4) = 2B^2\\sigma_{\\varepsilon}^4$.\n\nCombining all terms, the total variance of $\\hat{M}$ up to order $\\sigma_{\\varepsilon}^4$ is:\n$$\n\\mathrm{Var}[\\hat{M}] = A^2\\sigma_{\\varepsilon}^2 + 2B^2\\sigma_{\\varepsilon}^4\n$$\nThe problem asks to identify the linear error propagation term, $\\mathrm{Var}_{\\mathrm{lin}} = A^2\\sigma_{\\varepsilon}^2$. This is the standard first-order approximation and corresponds to the first term in our derived expression. The remaining term is the curvature-induced correction $\\Delta\\mathrm{Var}$:\n$$\n\\Delta\\mathrm{Var} = 2B^2\\sigma_{\\varepsilon}^4\n$$\nThis is the required symbolic expression for $\\Delta\\mathrm{Var}$.\n\nThe ratio $R$ is defined as $R \\equiv \\Delta\\mathrm{Var}/\\mathrm{Var}_{\\mathrm{lin}}$. We can write this as:\n$$\nR = \\frac{2B^2\\sigma_{\\varepsilon}^4}{A^2\\sigma_{\\varepsilon}^2} = \\frac{2B^2\\sigma_{\\varepsilon}^2}{A^2} = 2\\left(\\frac{B\\sigma_{\\varepsilon}}{A}\\right)^2\n$$\nNow, we substitute the given numerical values: $A = 125.1\\,\\text{GeV}$, $B = 350\\,\\text{GeV}$, and $\\sigma_{\\varepsilon} = 3.0 \\times 10^{-3}$. Note that $\\varepsilon$ is a fractional offset, so $\\sigma_\\varepsilon$ is dimensionless. The units of $A$ and $B$ will cancel in the ratio.\n$$\nR = 2\\left(\\frac{(350) \\cdot (3.0 \\times 10^{-3})}{125.1}\\right)^2\n$$\nFirst, calculate the argument of the square:\n$$\n\\frac{B\\sigma_{\\varepsilon}}{A} = \\frac{350 \\cdot 0.003}{125.1} = \\frac{1.05}{125.1} \\approx 0.0083932854\n$$\nNow, square this value and multiply by $2$:\n$$\nR \\approx 2 \\cdot (0.0083932854)^2 \\approx 2 \\cdot (7.044724 \\times 10^{-5}) \\approx 1.408945 \\times 10^{-4}\n$$\nRounding this result to four significant figures gives:\n$$\nR \\approx 1.409 \\times 10^{-4}\n$$", "answer": "$$\\boxed{1.409 \\times 10^{-4}}$$", "id": "3513019"}, {"introduction": "Combining results from multiple experiments or analysis channels is a fundamental task in high-energy physics, essential for maximizing statistical power and achieving a comprehensive understanding of a physical parameter. When these channels share systematic uncertainties, their measurements become correlated, and a naive weighted average is no longer optimal. This exercise [@problem_id:3513023] guides you through the implementation of two mathematically equivalent and powerful methods for performing a statistically optimal combination in the presence of such correlations: the Best Linear Unbiased Estimator (BLUE) and a joint likelihood fit with nuisance parameter profiling. This practice provides direct, hands-on experience with the machinery used to produce world-average values for fundamental constants.", "problem": "You are given a set of linear-Gaussian measurement models for combining experiment-level estimates of a dimensionless signal strength. Each experiment produces a scalar estimator that, by the Central Limit Theorem, may be treated as approximately Gaussian. The experiments share some systematic effects that are common across experiments and modeled as Gaussian-constrained nuisance parameters, and they also include uncorrelated effects. From a computational perspective, the task is to implement two mathematically equivalent combinations of these measurements: a Best Linear Unbiased Estimator that uses the full covariance matrix, and a joint-likelihood combination that profiles Gaussian-constrained nuisance parameters. The combination must be carried out using linear algebra only, and all reported numbers are dimensionless.\n\nFundamental starting assumptions:\n- The Central Limit Theorem implies that the sampling distribution of per-experiment estimators is well-approximated by a multivariate normal distribution for sufficiently large counts, even for counting experiments common in high-energy physics.\n- Gaussian error propagation in linear models with Gaussian priors is closed under marginalization and profiling, producing a multivariate normal form with an effective covariance obtained by summing uncorrelated and correlated contributions.\n\nGeneral setting for a single test case:\n- There are $N$ experiments providing an observed vector $y \\in \\mathbb{R}^{N}$, each component an approximate Gaussian draw centered around a common but unknown scalar parameter $\\mu$ (the dimensionless signal strength).\n- The measurement model is linear: $y = \\mu \\mathbf{1} + B \\theta + \\varepsilon$. Here, $\\mathbf{1}$ is the $N$-vector of ones, $B \\in \\mathbb{R}^{N \\times m}$ encodes how the $m$ shared nuisance parameters $\\theta \\in \\mathbb{R}^{m}$ shift the experiments, and $\\varepsilon \\sim \\mathcal{N}(0, D)$ is zero-mean Gaussian noise with diagonal covariance $D = \\mathrm{diag}(\\sigma_1^2, \\ldots, \\sigma_N^2)$ capturing uncorrelated components (statistical plus experiment-specific systematics). The nuisance parameters have a zero-mean Gaussian prior $\\theta \\sim \\mathcal{N}(0, T)$ with symmetric positive-definite covariance $T \\in \\mathbb{R}^{m \\times m}$. All quantities are dimensionless.\n- The effective total covariance after propagating the shared systematics is $S = D + B T B^{\\top}$.\n\nTasks to implement per test case:\n1. Construct $D$, $B$, and $T$ from the provided inputs and compute $S = D + B T B^{\\top}$.\n2. Compute the Best Linear Unbiased Estimator (BLUE) for $\\mu$ using the full covariance $S$, along with its standard deviation. Do not use any prepackaged combination formula; derive the estimator from the normal equations implied by linear-Gaussian error propagation.\n3. Compute the joint-likelihood combination by forming the penalized Gaussian negative log-likelihood for $(\\mu, \\theta)$ with the given $D$ and $T$, and then solving the corresponding linear system to profile out $\\theta$ and obtain the maximum-likelihood estimate for $\\mu$ and its standard deviation from the curvature. Again, use linear algebra; no numerical optimization loops are permitted.\n4. Report the absolute differences between the two estimates and between the two standard deviations as checks of equivalence.\n5. All quantities are dimensionless. All matrix inversions must be performed in a numerically stable way using linear solvers rather than explicit inverses where possible.\n\nTest suite:\nFor each test case $i$, you are given the tuple $(y^{(i)}, \\sigma^{(i)}, B^{(i)}, T^{(i)})$, where $y^{(i)}$ is a list of length $N$, $\\sigma^{(i)}$ is a list of length $N$ whose square defines the diagonal of $D$, $B^{(i)}$ is an $N \\times m$ matrix (possibly with $m=0$), and $T^{(i)}$ is an $m \\times m$ symmetric positive-definite matrix (possibly empty when $m=0$).\n\nUse the following test cases:\n- Case A: $N=3$, one shared nuisance ($m=1$).\n  - $y = [\\,1.05,\\,0.98,\\,1.10\\,]$\n  - $\\sigma = [\\,0.10,\\,0.12,\\,0.08\\,]$\n  - $B = \\begin{bmatrix} 0.20 \\\\ 0.10 \\\\ 0.15 \\end{bmatrix}$\n  - $T = [\\,1.0\\,]$\n- Case B: $N=4$, two shared nuisances ($m=2$).\n  - $y = [\\,0.90,\\,1.20,\\,1.00,\\,1.05\\,]$\n  - $\\sigma = [\\,0.25,\\,0.20,\\,0.15,\\,0.18\\,]$\n  - $B = \\begin{bmatrix} 0.30  0.05 \\\\ 0.25  0.05 \\\\ 0.05  0.20 \\\\ 0.05  0.25 \\end{bmatrix}$\n  - $T = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$\n- Case C: $N=2$, one shared nuisance ($m=1$), near-strong correlation.\n  - $y = [\\,1.01,\\,0.99\\,]$\n  - $\\sigma = [\\,0.02,\\,0.02\\,]$\n  - $B = \\begin{bmatrix} 0.05 \\\\ 0.05 \\end{bmatrix}$\n  - $T = [\\,1.0\\,]$\n- Case D: $N=5$, no shared nuisances ($m=0$).\n  - $y = [\\,1.00,\\,1.02,\\,0.97,\\,1.03,\\,0.99\\,]$\n  - $\\sigma = [\\,0.10,\\,0.10,\\,0.10,\\,0.10,\\,0.10\\,]$\n  - $B$ has shape $5 \\times 0$ (empty), $T$ is $0 \\times 0$ (empty).\n- Case E: $N=3$, two shared nuisances ($m=2$) with correlated priors and nearly collinear couplings.\n  - $y = [\\,0.95,\\,1.05,\\,1.00\\,]$\n  - $\\sigma = [\\,0.05,\\,0.05,\\,0.05\\,]$\n  - $B = \\begin{bmatrix} 0.20  0.40 \\\\ 0.19  0.39 \\\\ 0.21  0.41 \\end{bmatrix}$\n  - $T = \\begin{bmatrix} 1.0  0.6 \\\\ 0.6  2.0 \\end{bmatrix}$\n\nRequired outputs:\n- For each test case, compute and return a list of six floating-point numbers:\n  - $[\\mu_{\\mathrm{BLUE}},\\,\\sigma_{\\mathrm{BLUE}},\\,\\mu_{\\mathrm{joint}},\\,\\sigma_{\\mathrm{joint}},\\,|\\mu_{\\mathrm{BLUE}}-\\mu_{\\mathrm{joint}}|,\\,|\\sigma_{\\mathrm{BLUE}}-\\sigma_{\\mathrm{joint}}|]$.\n- Your program should produce a single line of output containing the results as a comma-separated list of per-case lists, with no spaces, enclosed in a single pair of square brackets, for example: \"[[a,b,c,d,e,f],[...],...]\".\n- To ensure deterministic comparison, each floating-point number in the final line must be rounded to $12$ significant digits before printing.\n\nAll quantities are dimensionless. Angles are not involved. Express differences as plain decimals (not percentages).", "solution": "The user has provided a problem concerning the combination of measurements in the presence of correlated systematic uncertainties, a common task in experimental science, particularly high-energy physics. The problem is to implement and verify the equivalence of two statistical methods for this combination: the Best Linear Unbiased Estimator (BLUE) using a full covariance matrix, and a direct joint-likelihood fit where nuisance parameters are profiled. The problem is scientifically grounded, well-posed, and all necessary information is provided. It constitutes a valid and substantive exercise in computational statistics and linear algebra.\n\n### Theoretical Derivations\n\nLet there be $N$ measurements provided as a vector $y \\in \\mathbb{R}^{N}$. Each measurement $y_i$ is an estimator of a common true signal strength $\\mu$. The measurement model is given as:\n$$\ny = \\mu \\mathbf{1} + B \\theta + \\varepsilon\n$$\nwhere $\\mathbf{1}$ is the $N$-vector of ones, $B \\in \\mathbb{R}^{N \\times m}$ is a matrix describing the response of each measurement to $m$ shared nuisance parameters $\\theta \\in \\mathbb{R}^{m}$, and $\\varepsilon \\in \\mathbb{R}^{N}$ represents uncorrelated statistical and systematic uncertainties. The uncertainties are modeled as zero-mean Gaussian distributions: $\\varepsilon \\sim \\mathcal{N}(0, D)$ where $D = \\mathrm{diag}(\\sigma_1^2, \\ldots, \\sigma_N^2)$ is a diagonal covariance matrix. The nuisance parameters are constrained by a Gaussian prior, $\\theta \\sim \\mathcal{N}(0, T)$, where $T$ is their $m \\times m$ covariance matrix.\n\n#### Method 1: Best Linear Unbiased Estimator (BLUE)\n\nThis method first marginalizes over the nuisance parameters $\\theta$ to obtain an effective likelihood for $\\mu$ alone. The variance from the nuisance parameters, $B \\mathrm{Cov}(\\theta) B^\\top = B T B^\\top$, is propagated to the measurements. The total covariance matrix $S$ for the vector $y$ is the sum of the uncorrelated and correlated components:\n$$\nS = D + B T B^{\\top}\n$$\nThe measurement vector $y$ is now considered a single draw from a multivariate normal distribution:\n$$\ny \\sim \\mathcal{N}(\\mu \\mathbf{1}, S)\n$$\nThe negative log-likelihood function (up to an additive constant) for $\\mu$ is:\n$$\n-\\ln L(\\mu) = \\frac{1}{2} (y - \\mu \\mathbf{1})^{\\top} S^{-1} (y - \\mu \\mathbf{1})\n$$\nTo find the maximum likelihood estimator $\\hat{\\mu}_{\\mathrm{BLUE}}$, we differentiate with respect to $\\mu$ and set the result to zero:\n$$\n\\frac{\\partial(-\\ln L)}{\\partial \\mu} = -\\mathbf{1}^{\\top} S^{-1} (y - \\hat{\\mu}_{\\mathrm{BLUE}} \\mathbf{1}) = 0\n$$\nSolving for $\\hat{\\mu}_{\\mathrm{BLUE}}$ yields:\n$$\n\\hat{\\mu}_{\\mathrm{BLUE}} = \\frac{\\mathbf{1}^{\\top} S^{-1} y}{\\mathbf{1}^{\\top} S^{-1} \\mathbf{1}}\n$$\nThe variance of this estimator, $\\sigma^2_{\\hat{\\mu}_{\\mathrm{BLUE}}}$, is found from the inverse of the second derivative of the negative log-likelihood (the Fisher information):\n$$\n\\frac{\\partial^2(-\\ln L)}{\\partial \\mu^2} = \\mathbf{1}^{\\top} S^{-1} \\mathbf{1}\n$$\nThus, the variance and standard deviation are:\n$$\n\\sigma^2_{\\hat{\\mu}_{\\mathrm{BLUE}}} = (\\mathbf{1}^{\\top} S^{-1} \\mathbf{1})^{-1} \\quad \\implies \\quad \\sigma_{\\hat{\\mu}_{\\mathrm{BLUE}}} = \\sqrt{(\\mathbf{1}^{\\top} S^{-1} \\mathbf{1})^{-1}}\n$$\nFor numerical stability, instead of computing $S^{-1}$ explicitly, we can solve the linear system $S v = \\mathbf{1}$ for $v = S^{-1}\\mathbf{1}$. The estimator and its variance become:\n$$\n\\hat{\\mu}_{\\mathrm{BLUE}} = \\frac{v^{\\top} y}{v^{\\top} \\mathbf{1}}, \\qquad \\sigma^2_{\\hat{\\mu}_{\\mathrm{BLUE}}} = \\frac{1}{v^{\\top} \\mathbf{1}}\n$$\n\n#### Method 2: Joint Likelihood with Nuisance Parameter Profiling\n\nThis method constructs a joint likelihood for both the parameter of interest $\\mu$ and the nuisance parameters $\\theta$. The penalized negative log-likelihood, or $\\chi^2$, is the sum of the term for the measurements and the term for the nuisance parameter constraints:\n$$\n\\chi^2(\\mu, \\theta) = (y - \\mu \\mathbf{1} - B \\theta)^{\\top} D^{-1} (y - \\mu \\mathbf{1} - B \\theta) + \\theta^{\\top} T^{-1} \\theta\n$$\nWe minimize this $\\chi^2$ jointly with respect to $\\mu$ and $\\theta$ by setting the partial derivatives to zero. This yields a system of linear equations. Let $p = (\\mu, \\theta_1, \\ldots, \\theta_m)^\\top$ be the vector of all parameters. The system is $H p = V$, where $H$ is the Hessian matrix of $\\frac{1}{2}\\chi^2$ and $V$ is derived from the gradient.\n$$\nH = \\frac{1}{2}\\frac{\\partial^2 \\chi^2}{\\partial p^2} = \\begin{pmatrix}\n\\mathbf{1}^{\\top} D^{-1} \\mathbf{1}  \\mathbf{1}^{\\top} D^{-1} B \\\\\nB^{\\top} D^{-1} \\mathbf{1}  B^{\\top} D^{-1} B + T^{-1}\n\\end{pmatrix}\n$$\n$$\nV = \\begin{pmatrix}\n\\mathbf{1}^{\\top} D^{-1} y \\\\\nB^{\\top} D^{-1} y\n\\end{pmatrix}\n$$\nThe solution to $H p = V$ provides the estimates $(\\hat{\\mu}_{\\mathrm{joint}}, \\hat{\\theta}_{\\mathrm{joint}}^\\top)^\\top$. The first component is the desired estimate for $\\mu$. The covariance matrix of the parameters is given by $H^{-1}$. The variance of $\\hat{\\mu}_{\\mathrm{joint}}$ is the top-left element of this inverse matrix:\n$$\n\\sigma^2_{\\hat{\\mu}_{\\mathrm{joint}}} = (H^{-1})_{00}\n$$\nThe standard deviation $\\sigma_{\\hat{\\mu}_{\\mathrm{joint}}}$ is the square root of this variance.\n\nThe mathematical equivalence of the two methods can be shown using the Sherman-Morrison-Woodbury formula for matrix inverses, which states:\n$$\n(D + B T B^{\\top})^{-1} = D^{-1} - D^{-1}B(T^{-1} + B^{\\top}D^{-1}B)^{-1}B^{\\top}D^{-1}\n$$\nUsing this identity, one can demonstrate that the expressions for both the estimator $\\hat{\\mu}$ and its variance $\\sigma^2_{\\hat{\\mu}}$ are identical in both formalisms. The implementation will demonstrate this equivalence numerically.\n\n### Implementation Strategy\n\nFor each test case, the implementation will proceed as follows:\n1.  Construct the matrices $D$, $B$, and $T$ from the input lists and convert them to `numpy` arrays. The vector of ones, $\\mathbf{1}$, is also created.\n\n2.  For the **BLUE method**:\n    a. Compute the total covariance matrix $S = D + B T B^{\\top}$. Note that if $m=0$, $BTB^{\\top}$ is a zero matrix.\n    b. Solve the linear system $S v = \\mathbf{1}$ for the vector $v$.\n    c. Calculate $\\hat{\\mu}_{\\mathrm{BLUE}} = (v^\\top y) / (v^\\top \\mathbf{1})$ and $\\sigma_{\\mathrm{BLUE}} = \\sqrt{1 / (v^\\top \\mathbf{1})}$.\n\n3.  For the **joint likelihood method**:\n    a. A special case is handled for $m=0$ nuisances. Here, the model simplifies to a standard weighted average, $y \\sim \\mathcal{N}(\\mu\\mathbf{1}, D)$, and the results are computed directly from the formulas $\\hat{\\mu}_{\\mathrm{joint}} = (\\sum y_i/\\sigma_i^2) / (\\sum 1/\\sigma_i^2)$ and $\\sigma^2_{\\mathrm{joint}} = 1 / (\\sum 1/\\sigma_i^2)$.\n    b. For $m0$, construct the $(m+1) \\times (m+1)$ Hessian matrix $H$ and the $(m+1)$ right-hand side vector $V$ as defined in the theoretical derivation. This involves inverting $T$ and using the diagonal $D^{-1}$.\n    c. Solve the linear system $H p = V$ for the parameter vector $p$. The estimate $\\hat{\\mu}_{\\mathrm{joint}}$ is the first element of $p$.\n    d. Compute the inverse of the Hessian, $H^{-1}$. The variance $\\sigma^2_{\\hat{\\mu}_{\\mathrm{joint}}}$ is the first diagonal element, $(H^{-1})_{00}$. The standard deviation is its square root.\n\n4.  Finally, the absolute differences $|\\hat{\\mu}_{\\mathrm{BLUE}} - \\hat{\\mu}_{\\mathrm{joint}}|$ and $|\\sigma_{\\mathrm{BLUE}} - \\sigma_{\\mathrm{joint}}|$ are computed to verify the equivalence.\n\n5.  All six resulting floating-point numbers for each case are rounded to $12$ significant digits and formatted into the required output string.", "answer": "```python\nimport numpy as np\n\ndef format_num__to_12_sig_digits(n):\n    \"\"\"Formats a number to a string with 12 significant digits.\"\"\"\n    return f\"{n:.12g}\"\n\ndef solve():\n    \"\"\"\n    Solves the measurement combination problem for a suite of test cases,\n    demonstrating the equivalence of the BLUE and joint-likelihood profiling methods.\n    \"\"\"\n    test_cases = [\n        # Case A: N=3, m=1\n        {\n            'y': np.array([1.05, 0.98, 1.10]),\n            'sigma': np.array([0.10, 0.12, 0.08]),\n            'B': np.array([[0.20], [0.10], [0.15]]),\n            'T': np.array([[1.0]])\n        },\n        # Case B: N=4, m=2\n        {\n            'y': np.array([0.90, 1.20, 1.00, 1.05]),\n            'sigma': np.array([0.25, 0.20, 0.15, 0.18]),\n            'B': np.array([[0.30, 0.05], [0.25, 0.05], [0.05, 0.20], [0.05, 0.25]]),\n            'T': np.array([[1.0, 0.0], [0.0, 1.0]])\n        },\n        # Case C: N=2, m=1\n        {\n            'y': np.array([1.01, 0.99]),\n            'sigma': np.array([0.02, 0.02]),\n            'B': np.array([[0.05], [0.05]]),\n            'T': np.array([[1.0]])\n        },\n        # Case D: N=5, m=0\n        {\n            'y': np.array([1.00, 1.02, 0.97, 1.03, 0.99]),\n            'sigma': np.array([0.10, 0.10, 0.10, 0.10, 0.10]),\n            'B': np.empty((5, 0)),\n            'T': np.empty((0, 0))\n        },\n        # Case E: N=3, m=2\n        {\n            'y': np.array([0.95, 1.05, 1.00]),\n            'sigma': np.array([0.05, 0.05, 0.05]),\n            'B': np.array([[0.20, 0.40], [0.19, 0.39], [0.21, 0.41]]),\n            'T': np.array([[1.0, 0.6], [0.6, 2.0]])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        y, sigma, B, T = case['y'], case['sigma'], case['B'], case['T']\n        N = len(y)\n        m = B.shape[1]\n        \n        ones = np.ones(N)\n        D = np.diag(sigma**2)\n\n        # --- Method 1: BLUE ---\n        # S = D + B @ T @ B.T handles m=0 case correctly (B@T@B.T is zero matrix)\n        S = D + B @ T @ B.T\n        \n        # Solve Sv = 1 for v = S^-1 * 1\n        v = np.linalg.solve(S, ones)\n        \n        # mu = (v.T @ y) / (v.T @ 1)\n        mu_blue = (v @ y) / (v @ ones)\n        # sigma^2 = 1 / (v.T @ 1)\n        sigma_blue = np.sqrt(1 / (v @ ones))\n\n        # --- Method 2: Joint Likelihood Profiling ---\n        D_inv_diag = 1 / sigma**2\n        D_inv = np.diag(D_inv_diag)\n        \n        if m == 0:\n            # Simplified case: weighted average\n            weights = D_inv_diag\n            mu_joint = np.sum(weights * y) / np.sum(weights)\n            sigma_joint = np.sqrt(1 / np.sum(weights))\n        else:\n            # Build the Hessian H and vector V for the system Hp = V\n            # p = [mu, theta_1, ..., theta_m].T\n            A_block = np.sum(D_inv_diag)\n            C_block = ones @ D_inv @ B\n            \n            T_inv = np.linalg.inv(T)\n            G_block = B.T @ D_inv @ B + T_inv\n            \n            H = np.zeros((1 + m, 1 + m))\n            H[0, 0] = A_block\n            H[0, 1:] = C_block\n            H[1:, 0] = C_block\n            H[1:, 1:] = G_block\n\n            V = np.zeros(1 + m)\n            V[0] = np.sum(D_inv_diag * y)\n            V[1:] = B.T @ D_inv @ y\n\n            # Solve for parameters [mu, theta]\n            p = np.linalg.solve(H, V)\n            mu_joint = p[0]\n            \n            # Variance of mu is the top-left element of H^-1\n            H_inv = np.linalg.inv(H)\n            sigma_joint = np.sqrt(H_inv[0, 0])\n\n        diff_mu = abs(mu_blue - mu_joint)\n        diff_sigma = abs(sigma_blue - sigma_joint)\n\n        case_results = [mu_blue, sigma_blue, mu_joint, sigma_joint, diff_mu, diff_sigma]\n        results.append(case_results)\n\n    # Format the final output string\n    formatted_results = []\n    for res_list in results:\n        formatted_list = [format_num__to_12_sig_digits(r) for r in res_list]\n        formatted_results.append(f\"[{','.join(formatted_list)}]\")\n    \n    final_output = f\"[{','.join(formatted_results)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3513023"}, {"introduction": "Many of our statistical tools, including the methods for combining measurements, rely on the powerful guarantees of the Central Limit Theorem (CLT), which states that the sum or average of many random variables will approximate a Gaussian distribution. While robust, the CLT is not a magical incantation, and its applicability depends on certain conditions. This advanced practice [@problem_id:3513055] invites you to test the limits of the CLT through a Monte Carlo simulation. You will construct a combined estimator and investigate its distribution, discovering how factors like the number of channels and the nature of correlated systematics determine whether the final uncertainty can be safely treated as Gaussian—a critical insight for correctly interpreting experimental results and setting confidence intervals.", "problem": "You are given a computational task rooted in the Best Linear Unbiased Estimator (BLUE) and the Central Limit Theorem (CLT) for combining multiple sub-channel measurements in high-energy physics. The estimator must aggregate $n$ sub-channel measurements of a shared parameter under correlated systematic uncertainties. The aim is two-fold: compute the combined estimator under a specified covariance structure and assess convergence to normality as $n$ increases and correlation structures vary.\n\nStarting point and constraints:\n- Assume $n$ scalar measurements collected in a vector $\\mathbf{x} \\in \\mathbb{R}^n$ that follow the model $\\mathbf{x} = \\theta \\mathbf{1} + \\boldsymbol{\\alpha}s + \\mathbf{e}$, where $\\theta$ is an unknown constant, $\\mathbf{1}$ is the $n$-dimensional vector of ones, $s$ is a single random systematic mode (mean zero) shared across sub-channels with known variance, $\\boldsymbol{\\alpha} \\in \\mathbb{R}^n$ encodes the coupling of the shared systematic to each sub-channel (allowing partial correlation), and $\\mathbf{e}$ is a random vector of independent sub-channel statistical errors with known per-channel variances. The covariance of the noise is $\\mathbf{V} = \\operatorname{diag}(\\sigma_1^2,\\ldots,\\sigma_n^2) + \\tau^2 \\boldsymbol{\\alpha}\\boldsymbol{\\alpha}^\\top$, where $\\sigma_i^2$ are known and $\\tau^2$ is the variance of $s$.\n- The Best Linear Unbiased Estimator (BLUE) is defined as the linear estimator $\\hat{\\mu} = \\mathbf{w}^\\top \\mathbf{x}$ that is unbiased under the model and has minimal variance among all linear unbiased estimators. The unbiasedness constraint is that $\\mathbf{w}^\\top \\mathbf{1} = 1$.\n- The Central Limit Theorem (CLT) states that sums of independent random variables with finite variance tend to normality as the number of terms increases, subject to conditions such as Lindeberg’s condition for non-identically distributed variables. In the present context, the combined estimator includes a shared systematic component $s$ which may or may not be suppressible depending on $\\boldsymbol{\\alpha}$.\n\nTask requirements:\n- Implement a program that constructs $\\mathbf{V}$, computes the BLUE weights $\\mathbf{w}$ and the analytic variance of the combined estimator under the model for each test case. Then, perform Monte Carlo simulations to generate repeated experiments, compute the standardized combined estimator $z = (\\hat{\\mu}-\\theta)/\\sqrt{\\operatorname{Var}(\\hat{\\mu})}$, and test its closeness to a standard normal distribution.\n- Use the following distributions for $\\mathbf{e}$ and $s$ where specified. For a Laplace distribution with scale $b$, the variance is $2b^2$. For a Student’s $t$ distribution with degrees of freedom $\\nu2$, the variance is $\\nu/(\\nu-2)$ for the standard distribution; if a scale $\\sigma$ is applied, the variance becomes $\\sigma^2 \\nu/(\\nu-2)$. For a Gaussian distribution with standard deviation $\\sigma$, the variance is $\\sigma^2$. The program must set the scale parameters such that the variances of $\\mathbf{e}$ and $s$ match the given $\\sigma_i^2$ and $\\tau^2$ respectively.\n- The unknown parameter is $\\theta = 0$ (dimensionless), so unbiasedness is equivalent to preserving the mean at $0$. The analytic variance of the combined estimator under the model must be computed from the covariance $\\mathbf{V}$ and the weights $\\mathbf{w}$.\n\nMonte Carlo procedure:\n- For each test case, simulate a large number of independent experiments to produce samples of $\\hat{\\mu}$, then standardize to $z$ using the analytic variance from $\\mathbf{V}$. Assess normality of $z$ using D’Agostino’s $K^2$ test and report the associated $p$-value for each test case.\n\nTest suite:\nProvide results for the following test cases. In each case, report the $p$-value of the normality test for the standardized combined estimator $z$.\n\n- Case A (general happy path, moderate correlation, heavy-tailed statistical errors, fully common systematic):\n  - $n = 8$,\n  - $\\boldsymbol{\\sigma}^2 = [0.6, 0.7, 0.8, 0.9, 1.0, 0.7, 0.9, 1.1]$,\n  - $\\tau^2 = 0.04$,\n  - $\\boldsymbol{\\alpha} = \\mathbf{1}$,\n  - $\\mathbf{e}$ distribution: Laplace (per-channel scales chosen to match $\\sigma_i^2$),\n  - $s$ distribution: Gaussian (scale chosen to match $\\tau^2$),\n  - number of simulated experiments: $30000$.\n\n- Case B (boundary case, dominant fully correlated heavy-tailed systematic, many channels):\n  - $n = 32$,\n  - $\\boldsymbol{\\sigma}^2 = [0.05, 0.05, \\ldots, 0.05]$ (length $32$),\n  - $\\tau^2 = 1.0$,\n  - $\\boldsymbol{\\alpha} = \\mathbf{1}$,\n  - $\\mathbf{e}$ distribution: Gaussian (per-channel scales chosen to match $\\sigma_i^2$),\n  - $s$ distribution: Student’s $t$ with $\\nu = 3$ degrees of freedom (scale chosen to match $\\tau^2$),\n  - number of simulated experiments: $30000$.\n\n- Case C (partial correlations enabling BLUE to suppress systematic, heavy tails in both components):\n  - $n = 16$,\n  - $\\boldsymbol{\\sigma}^2 = [0.5, 0.5, \\ldots, 0.5]$ (length $16$),\n  - $\\tau^2 = 0.5$,\n  - $\\boldsymbol{\\alpha}$ linearly spaced from $-0.5$ to $0.5$ across $n$ components,\n  - $\\mathbf{e}$ distribution: Student’s $t$ with $\\nu = 5$ (per-channel scales chosen to match $\\sigma_i^2$),\n  - $s$ distribution: Student’s $t$ with $\\nu = 4$ (scale chosen to match $\\tau^2$),\n  - number of simulated experiments: $30000$.\n\n- Case D (edge case, very few channels, fully common heavy-tailed systematic and statistical errors):\n  - $n = 2$,\n  - $\\boldsymbol{\\sigma}^2 = [1.0, 0.5]$,\n  - $\\tau^2 = 0.2$,\n  - $\\boldsymbol{\\alpha} = \\mathbf{1}$,\n  - $\\mathbf{e}$ distribution: Laplace (per-channel scales chosen to match $\\sigma_i^2$),\n  - $s$ distribution: Laplace (scale chosen to match $\\tau^2$),\n  - number of simulated experiments: $40000$.\n\nOutput specification:\n- Your program should produce a single line of output containing the $p$-values for Cases A, B, C, and D, as a comma-separated list enclosed in square brackets (for example, $[p_A,p_B,p_C,p_D]$).\n- Each reported value must be a float.\n\nScientific and algorithmic expectations:\n- Begin from the definition of linear unbiased estimators and the covariance model $\\mathbf{V}$ to construct the estimator and its analytic variance.\n- Avoid any shortcuts or pre-specified formulas in the problem statement; derive and implement them in the solution section and code.\n- Ensure scientific realism by using the stated distributions and by matching the variances via appropriate scale parameters.\n- No physical units are involved; all quantities are dimensionless.", "solution": "The user wants to solve a problem in computational high-energy physics involving the Best Linear Unbiased Estimator (BLUE) and the Central Limit Theorem (CLT).\n\n### **Problem Validation**\n\n**Step 1: Extract Givens**\n- **Model for measurements**: $\\mathbf{x} = \\theta \\mathbf{1} + \\boldsymbol{\\alpha}s + \\mathbf{e}$, where $\\mathbf{x} \\in \\mathbb{R}^n$, $\\theta$ is an unknown constant, $\\mathbf{1}$ is the $n$-dimensional vector of ones, $s$ is a random systematic mode with mean zero and known variance $\\tau^2$, $\\boldsymbol{\\alpha} \\in \\mathbb{R}^n$ is a coupling vector, and $\\mathbf{e}$ is a vector of independent statistical errors with known variances $\\sigma_i^2$.\n- **Covariance Matrix**: $\\mathbf{V} = \\operatorname{diag}(\\sigma_1^2,\\ldots,\\sigma_n^2) + \\tau^2 \\boldsymbol{\\alpha}\\boldsymbol{\\alpha}^\\top$.\n- **Estimator Definition**: The BLUE is a linear estimator $\\hat{\\mu} = \\mathbf{w}^\\top \\mathbf{x}$.\n- **Unbiasedness Constraint**: $\\mathbf{w}^\\top \\mathbf{1} = 1$.\n- **Minimality Condition**: The variance of $\\hat{\\mu}$ is minimal among all linear unbiased estimators.\n- **`True` Parameter Value**: $\\theta = 0$.\n- **Distributions**: Gaussian, Laplace, and Student's $t$ distributions are to be used for generating random errors $s$ and $\\mathbf{e}$, with scales chosen to match the specified variances $\\tau^2$ and $\\sigma_i^2$.\n- **Normality Test**: D’Agostino’s $K^2$ test on the standardized estimator $z = (\\hat{\\mu}-\\theta)/\\sqrt{\\operatorname{Var}(\\hat{\\mu})}$.\n- **Test Cases**:\n    - **Case A**: $n = 8$, $\\boldsymbol{\\sigma}^2 = [0.6, 0.7, 0.8, 0.9, 1.0, 0.7, 0.9, 1.1]$, $\\tau^2 = 0.04$, $\\boldsymbol{\\alpha} = \\mathbf{1}$, $\\mathbf{e}$ is Laplace, $s$ is Gaussian, $30000$ experiments.\n    - **Case B**: $n = 32$, $\\boldsymbol{\\sigma}^2 = \\mathbf{1} \\times 0.05$, $\\tau^2 = 1.0$, $\\boldsymbol{\\alpha} = \\mathbf{1}$, $\\mathbf{e}$ is Gaussian, $s$ is Student's $t$ ($\\nu=3$), $30000$ experiments.\n    - **Case C**: $n = 16$, $\\boldsymbol{\\sigma}^2 = \\mathbf{1} \\times 0.5$, $\\tau^2 = 0.5$, $\\boldsymbol{\\alpha}$ is linear from $-0.5$ to $0.5$, $\\mathbf{e}$ is Student's $t$ ($\\nu=5$), $s$ is Student's $t$ ($\\nu=4$), $30000$ experiments.\n    - **Case D**: $n = 2$, $\\boldsymbol{\\sigma}^2 = [1.0, 0.5]$, $\\tau^2 = 0.2$, $\\boldsymbol{\\alpha} = \\mathbf{1}$, $\\mathbf{e}$ is Laplace, $s$ is Laplace, $40000$ experiments.\n- **Output**: A list of $p$-values for the normality test for each case: $[p_A, p_B, p_C, p_D]$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly based on established principles of statistical inference (BLUE) and probability theory (CLT), which are standard tools in experimental data analysis, particularly in high-energy physics. The model is a common linear model for combining measurements with correlated uncertainties.\n- **Well-Posed:** The problem is well-posed. It provides a clear objective, all necessary data, and a valid mathematical framework. The covariance matrix $\\mathbf{V}$ is constructed as the sum of a positive-definite diagonal matrix (variances are positive) and a positive semi-definite matrix, ensuring $\\mathbf{V}$ is positive-definite and thus invertible. This guarantees a unique solution for the BLUE weights.\n- **Objective:** The problem is stated in precise, formal, and unbiased language.\n- **Completeness and Consistency:** All required parameters for each test case are fully specified. There are no contradictions in the setup.\n- **Realism and Feasibility:** The scenarios, while simplified, reflect realistic challenges in experimental physics, such as handling heavy-tailed errors and partially correlated systematics. The specified computations are algorithmically feasible.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, and complete. A solution will be provided.\n\n### **Solution Derivation and Algorithmic Design**\n\nThe problem requires the calculation of a combined estimate from several measurements and an assessment of its statistical properties. This involves three main parts: deriving the Best Linear Unbiased Estimator (BLUE), calculating its theoretical variance, and performing a Monte Carlo simulation to study the distribution of the estimator.\n\n**1. Derivation of the Best Linear Unbiased Estimator (BLUE)**\n\nThe model for the $n$ measurements is given by the vector equation $\\mathbf{x} = \\theta \\mathbf{1} + \\boldsymbol{\\epsilon}$, where $\\boldsymbol{\\epsilon} = \\boldsymbol{\\alpha}s + \\mathbf{e}$ represents the total noise. The parameter $\\theta$ is the quantity to be estimated. A linear estimator for $\\theta$ is of the form $\\hat{\\theta} = \\mathbf{w}^\\top \\mathbf{x}$, where $\\mathbf{w}$ is a vector of weights.\n\nThe estimator must be unbiased, meaning its expected value is equal to the true value $\\theta$. The expectation of $\\mathbf{x}$ is $E[\\mathbf{x}] = E[\\theta \\mathbf{1} + \\boldsymbol{\\alpha}s + \\mathbf{e}] = \\theta \\mathbf{1} + \\boldsymbol{\\alpha}E[s] + E[\\mathbf{e}]$. Since the errors $s$ and $\\mathbf{e}$ have zero mean, $E[\\mathbf{x}] = \\theta \\mathbf{1}$. The expectation of the estimator is then $E[\\hat{\\theta}] = E[\\mathbf{w}^\\top \\mathbf{x}] = \\mathbf{w}^\\top E[\\mathbf{x}] = \\mathbf{w}^\\top (\\theta \\mathbf{1}) = \\theta (\\mathbf{w}^\\top \\mathbf{1})$. For $E[\\hat{\\theta}]=\\theta$ to hold for any $\\theta$, the unbiasedness constraint must be satisfied:\n$$ \\mathbf{w}^\\top \\mathbf{1} = 1 $$\n\nThe \"best\" estimator is the one with the minimum variance. The variance of $\\hat{\\theta}$ is given by:\n$$ \\operatorname{Var}(\\hat{\\theta}) = \\operatorname{Var}(\\mathbf{w}^\\top \\mathbf{x}) = \\mathbf{w}^\\top \\operatorname{Cov}(\\mathbf{x}) \\mathbf{w} = \\mathbf{w}^\\top \\mathbf{V} \\mathbf{w} $$\nwhere $\\mathbf{V} = \\operatorname{Cov}(\\mathbf{x})$ is the covariance matrix of the measurements. As given, $\\mathbf{V} = \\operatorname{diag}(\\sigma_1^2, \\ldots, \\sigma_n^2) + \\tau^2 \\boldsymbol{\\alpha}\\boldsymbol{\\alpha}^\\top$.\n\nWe must minimize $\\mathbf{w}^\\top \\mathbf{V} \\mathbf{w}$ subject to the constraint $\\mathbf{w}^\\top \\mathbf{1} = 1$. This constrained optimization problem is solved using the method of Lagrange multipliers. The Lagrangian is:\n$$ \\mathcal{L}(\\mathbf{w}, \\lambda) = \\mathbf{w}^\\top \\mathbf{V} \\mathbf{w} - 2\\lambda (\\mathbf{w}^\\top \\mathbf{1} - 1) $$\nwhere $2\\lambda$ is the Lagrange multiplier. Taking the gradient with respect to $\\mathbf{w}$ and setting it to zero yields:\n$$ \\nabla_{\\mathbf{w}} \\mathcal{L} = 2\\mathbf{V}\\mathbf{w} - 2\\lambda\\mathbf{1} = \\mathbf{0} \\implies \\mathbf{V}\\mathbf{w} = \\lambda\\mathbf{1} $$\nAssuming $\\mathbf{V}$ is invertible, we can write $\\mathbf{w} = \\lambda \\mathbf{V}^{-1}\\mathbf{1}$. We solve for $\\lambda$ by substituting this into the constraint equation:\n$$ (\\lambda \\mathbf{V}^{-1}\\mathbf{1})^\\top \\mathbf{1} = 1 \\implies \\lambda (\\mathbf{1}^\\top (\\mathbf{V}^{-1})^\\top \\mathbf{1}) = 1 $$\nSince $\\mathbf{V}$ is a symmetric matrix, its inverse $\\mathbf{V}^{-1}$ is also symmetric. Thus, $\\lambda = (\\mathbf{1}^\\top \\mathbf{V}^{-1} \\mathbf{1})^{-1}$. The optimal weights are:\n$$ \\mathbf{w}_{\\text{BLUE}} = \\frac{\\mathbf{V}^{-1}\\mathbf{1}}{\\mathbf{1}^\\top \\mathbf{V}^{-1} \\mathbf{1}} $$\n\nThe variance of this BLUE estimator is:\n$$ \\operatorname{Var}(\\hat{\\theta}) = \\mathbf{w}_{\\text{BLUE}}^\\top \\mathbf{V} \\mathbf{w}_{\\text{BLUE}} = \\frac{(\\mathbf{V}^{-1}\\mathbf{1})^\\top}{\\mathbf{1}^\\top \\mathbf{V}^{-1} \\mathbf{1}} \\mathbf{V} \\frac{\\mathbf{V}^{-1}\\mathbf{1}}{\\mathbf{1}^\\top \\mathbf{V}^{-1} \\mathbf{1}} = \\frac{\\mathbf{1}^\\top \\mathbf{V}^{-1} \\mathbf{V} \\mathbf{V}^{-1} \\mathbf{1}}{(\\mathbf{1}^\\top \\mathbf{V}^{-1} \\mathbf{1})^2} = \\frac{\\mathbf{1}^\\top \\mathbf{V}^{-1} \\mathbf{1}}{(\\mathbf{1}^\\top \\mathbf{V}^{-1} \\mathbf{1})^2} = \\frac{1}{\\mathbf{1}^\\top \\mathbf{V}^{-1} \\mathbf{1}} $$\n\n**2. Monte Carlo Simulation and Normality Assessment**\n\nThe simulation will generate a large number of pseudo-experiments to sample the distribution of the estimator $\\hat{\\theta}$. For each test case:\n1.  Construct the covariance matrix $\\mathbf{V}$ from the given $\\boldsymbol{\\sigma}^2$, $\\tau^2$, and $\\boldsymbol{\\alpha}$.\n2.  Compute the BLUE weights $\\mathbf{w}$ and the analytic variance $\\sigma_{\\hat{\\theta}}^2 = \\operatorname{Var}(\\hat{\\theta})$ using the formulas derived above.\n3.  For each of the specified number of experiments:\n    a. Generate a random value for the systematic error $s$ from its specified distribution. The distribution must be scaled to have variance $\\tau^2$.\n    b. Generate a random vector of statistical errors $\\mathbf{e}$ from their specified distribution. Each component $e_i$ must be scaled to have variance $\\sigma_i^2$.\n    c. Construct the measurement vector $\\mathbf{x} = \\boldsymbol{\\alpha}s + \\mathbf{e}$ (since $\\theta=0$).\n    d. Calculate the estimate $\\hat{\\theta} = \\mathbf{w}^\\top \\mathbf{x}$.\n4.  After all experiments, a sample of $\\hat{\\theta}$ values is obtained.\n5.  Standardize the sample to get $z_i = (\\hat{\\theta}_i - \\theta) / \\sigma_{\\hat{\\theta}} = \\hat{\\theta}_i / \\sigma_{\\hat{\\theta}}$.\n6.  Perform D'Agostino's $K^2$ test on the sample of $z$ values. This test assesses the null hypothesis that the sample comes from a standard normal distribution. We report the resulting $p$-value.\n\n**Distribution Scaling:**\n- For a distribution with inherent variance $V_0$ (e.g., $V_0=1$ for standard Student's $t$ with $\\nu$ degrees of freedom being $\\nu/(\\nu-2)$), to achieve a target variance $V_{\\text{target}}$, the random variates must be multiplied by a scale factor $k = \\sqrt{V_{\\text{target}}/V_0}$.\n- **Laplace**: Variance is $2b^2$ for scale $b$. To get variance $V_{\\text{target}}$, we set $b = \\sqrt{V_{\\text{target}}/2}$.\n- **Student's $t$**: Variance is $s^2 \\nu/(\\nu-2)$ for scale $s$ and degrees of freedom $\\nu$. To get variance $V_{\\text{target}}$, we set $s = \\sqrt{V_{\\text{target}} (\\nu-2)/\\nu}$.\n- **Gaussian**: Variance is $\\sigma^2$ for scale (standard deviation) $\\sigma$. To get variance $V_{\\text{target}}$, we set $\\sigma = \\sqrt{V_{\\text{target}}}$.\n\n**Interpretation of Test Cases and the Central Limit Theorem (CLT)**\nThe estimator is $\\hat{\\theta} - \\theta = s(\\mathbf{w}^\\top \\boldsymbol{\\alpha}) + \\mathbf{w}^\\top \\mathbf{e}$. The term $\\mathbf{w}^\\top \\mathbf{e}$ is a weighted sum of $n$ independent random variables. By the CLT, this term approaches a normal distribution as $n$ increases. The overall normality of $\\hat{\\theta}$ depends on the second term, $s(\\mathbf{w}^\\top \\boldsymbol{\\alpha})$.\n\n- **Cases A, B, D ($\\boldsymbol{\\alpha}=\\mathbf{1}$)**: The systematic is fully correlated. The unbiasedness constraint $\\mathbf{w}^\\top \\mathbf{1} = 1$ implies $\\mathbf{w}^\\top \\boldsymbol{\\alpha} = 1$. The estimator becomes $\\hat{\\theta} - \\theta = s + \\mathbf{w}^\\top \\mathbf{e}$. The final distribution is a convolution of the distribution of $s$ and the (approximately normal) distribution of $\\mathbf{w}^\\top\\mathbf{e}$.\n    - In Case B, the systematic variance $\\tau^2=1.0$ is large, and its distribution is a heavy-tailed Student's $t$ ($\\nu=3$). This non-normal component will dominate, leading to a non-normal result and a low $p$-value.\n    - In Case D, $n=2$ is too small for the CLT to take effect, and both $s$ and $\\mathbf{e}$ are from the heavy-tailed Laplace distribution. The result will be non-normal, yielding a low $p$-value.\n    - In Case A, $\\tau^2=0.04$ is smaller, $n=8$ is moderate, and $s$ is Gaussian. The sum should be closer to normal than in B and D, but the heavy-tailed Laplace errors in $\\mathbf{e}$ might prevent full convergence with $n=8$.\n\n- **Case C ($\\boldsymbol{\\alpha}$ has varying signs)**: The structure of $\\boldsymbol{\\alpha}$ (linearly spaced around $0$) makes it possible for the BLUE optimization to find weights $\\mathbf{w}$ such that $\\mathbf{w}^\\top\\boldsymbol{\\alpha} \\approx 0$. This effectively suppresses the contribution from the systematic error $s$. The estimator becomes $\\hat{\\theta} - \\theta \\approx \\mathbf{w}^\\top \\mathbf{e}$. As this is a sum of $n=16$ variables, the CLT suggests the result will be approximately normal, even though the constituent errors follow a Student's $t$ distribution. We expect a high $p$-value.\n\nThe implementation will follow this logic to compute the required $p$-values.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, laplace, t, normaltest\n\ndef solve():\n    \"\"\"\n    Solves the problem by running Monte Carlo simulations for four test cases\n    and reporting the p-values of a normality test on the combined estimator.\n    \"\"\"\n\n    def run_simulation_case(n, sigma2_vec, tau2, alpha_vec, e_dist_info, s_dist_info, num_experiments):\n        \"\"\"\n        Runs the simulation for a single test case.\n        \"\"\"\n        # Set random seed for reproducibility. Note: The problem doesn't require a fixed seed,\n        # so minor fluctuations in p-values are expected across runs. Using a seed ensures\n        # deterministic output for this specific implementation.\n        np.random.seed(42)\n\n        # 1. Construct the covariance matrix V\n        V_stat = np.diag(sigma2_vec)\n        alpha_vec = alpha_vec.reshape(-1, 1)\n        V_syst = tau2 * (alpha_vec @ alpha_vec.T)\n        V = V_stat + V_syst\n\n        # 2. Compute BLUE weights and analytic variance\n        V_inv = np.linalg.inv(V)\n        ones_vec = np.ones((n, 1))\n        \n        # Denominator for weights and inverse of variance\n        denom = ones_vec.T @ V_inv @ ones_vec\n        \n        # BLUE weights\n        w = (V_inv @ ones_vec) / denom\n        \n        # Analytic variance of the estimator\n        var_theta_hat = 1.0 / denom[0, 0]\n        std_theta_hat = np.sqrt(var_theta_hat)\n\n        # 3. Monte Carlo Simulation\n        \n        # Generate systematic errors 's'\n        s_dist_type, s_params = s_dist_info\n        if s_dist_type == 'gaussian':\n            scale_s = np.sqrt(tau2)\n            s_samples = norm.rvs(loc=0, scale=scale_s, size=num_experiments)\n        elif s_dist_type == 'laplace':\n            # Variance of laplace is 2*b^2, b is scale\n            scale_s = np.sqrt(tau2 / 2.0)\n            s_samples = laplace.rvs(loc=0, scale=scale_s, size=num_experiments)\n        elif s_dist_type == 'student_t':\n            nu = s_params['nu']\n            # Variance of t is s^2 * nu/(nu-2)\n            scale_s = np.sqrt(tau2 * (nu - 2) / nu)\n            s_samples = t.rvs(df=nu, loc=0, scale=scale_s, size=num_experiments)\n        \n        # Generate statistical errors 'e'\n        e_dist_type, e_params = e_dist_info\n        e_samples = np.zeros((n, num_experiments))\n        for i in range(n):\n            var_ei = sigma2_vec[i]\n            if e_dist_type == 'gaussian':\n                scale_e = np.sqrt(var_ei)\n                e_samples[i, :] = norm.rvs(loc=0, scale=scale_e, size=num_experiments)\n            elif e_dist_type == 'laplace':\n                scale_e = np.sqrt(var_ei / 2.0)\n                e_samples[i, :] = laplace.rvs(loc=0, scale=scale_e, size=num_experiments)\n            elif e_dist_type == 'student_t':\n                nu = e_params['nu']\n                scale_e = np.sqrt(var_ei * (nu - 2) / nu)\n                e_samples[i, :] = t.rvs(df=nu, loc=0, scale=scale_e, size=num_experiments)\n        \n        # Construct measurement vectors x (for theta=0)\n        # x = alpha * s + e\n        x_samples = alpha_vec * s_samples + e_samples\n\n        # Calculate estimator for each experiment\n        theta_hat_samples = (w.T @ x_samples).flatten()\n\n        # 4. Standardize the estimator and test for normality\n        z_samples = theta_hat_samples / std_theta_hat\n        \n        k2_stat, p_value = normaltest(z_samples)\n        \n        return p_value\n\n\n    test_cases = [\n        # Case A\n        {\n            \"n\": 8,\n            \"sigma2_vec\": np.array([0.6, 0.7, 0.8, 0.9, 1.0, 0.7, 0.9, 1.1]),\n            \"tau2\": 0.04,\n            \"alpha_vec\": np.ones(8),\n            \"e_dist_info\": ('laplace', {}),\n            \"s_dist_info\": ('gaussian', {}),\n            \"num_experiments\": 30000\n        },\n        # Case B\n        {\n            \"n\": 32,\n            \"sigma2_vec\": np.full(32, 0.05),\n            \"tau2\": 1.0,\n            \"alpha_vec\": np.ones(32),\n            \"e_dist_info\": ('gaussian', {}),\n            \"s_dist_info\": ('student_t', {'nu': 3}),\n            \"num_experiments\": 30000\n        },\n        # Case C\n        {\n            \"n\": 16,\n            \"sigma2_vec\": np.full(16, 0.5),\n            \"tau2\": 0.5,\n            \"alpha_vec\": np.linspace(-0.5, 0.5, 16),\n            \"e_dist_info\": ('student_t', {'nu': 5}),\n            \"s_dist_info\": ('student_t', {'nu': 4}),\n            \"num_experiments\": 30000\n        },\n        # Case D\n        {\n            \"n\": 2,\n            \"sigma2_vec\": np.array([1.0, 0.5]),\n            \"tau2\": 0.2,\n            \"alpha_vec\": np.ones(2),\n            \"e_dist_info\": ('laplace', {}),\n            \"s_dist_info\": ('laplace', {}),\n            \"num_experiments\": 40000\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        p_value = run_simulation_case(**case)\n        results.append(p_value)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3513055"}]}