{"hands_on_practices": [{"introduction": "A central challenge in applying MCMC methods to the high-dimensional problems common in high-energy physics is tuning the algorithm's parameters for optimal performance. The step size of a random-walk proposal is a prime example: too small, and the exploration is inefficiently slow; too large, and nearly every proposal is rejected. This exercise [@problem_id:3521310] provides a foundational hands-on derivation of the optimal scaling law for the random-walk Metropolis algorithm. By analyzing the high-dimensional limit for a canonical Gaussian target, you will determine how the proposal scale should shrink with dimensionality to maintain a constant acceptance rate and maximize sampling efficiency, a cornerstone result in MCMC theory.", "problem": "Consider a high-dimensional parameter vector $\\boldsymbol{\\theta}\\in\\mathbb{R}^{d}$ arising from a whitened posterior in a collider analysis within computational high-energy physics, where the local curvature of the log-posterior at its mode has been used to transform coordinates so that the target distribution is approximately a product of standard normal components. That is, the target density is\n$$\n\\pi_{d}(\\boldsymbol{x}) \\propto \\prod_{i=1}^{d} \\exp\\!\\left(-\\frac{x_{i}^{2}}{2}\\right),\n$$\nwith $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ denoting the whitened coordinates. A Markov chain Monte Carlo (MCMC) algorithm based on the Metropolis-Hastings (MH) method is used to sample $\\pi_{d}$, with a random-walk Gaussian proposal:\n$$\n\\boldsymbol{x}' \\sim \\mathcal{N}\\!\\left(\\boldsymbol{x},\\,\\sigma^{2}\\,\\boldsymbol{I}_{d}\\right),\n$$\nwhere $\\boldsymbol{I}_{d}$ is the $d\\times d$ identity matrix and $\\sigma0$ is the scalar proposal standard deviation. Let the Metropolis-Hastings acceptance probability be defined by its core rule: for a current state $\\boldsymbol{x}$ and a proposed state $\\boldsymbol{x}'$, the acceptance probability is\n$$\na(\\boldsymbol{x},\\boldsymbol{x}') \\equiv \\min\\!\\left\\{1,\\,\\exp\\!\\Big(\\ln\\pi_{d}(\\boldsymbol{x}')-\\ln\\pi_{d}(\\boldsymbol{x})\\Big)\\right\\}.\n$$\n\nIn the $d\\to\\infty$ regime, adopt the standard non-degenerate scaling ansatz $\\sigma = \\ell/\\sqrt{d}$, where $\\ell0$ is a dimensionless step-size parameter, and assume stationarity of the chain under $\\pi_{d}$ along with independence between the current state and the proposal increment.\n\nStarting strictly from the above definitions and assumptions, and without introducing any untested shortcuts beyond the Central Limit Theorem and properties of the normal distribution, carry out the following:\n\n- Derive an explicit expression for the asymptotic acceptance probability as a function of $\\ell$, denoted $\\alpha(\\ell)$, in the limit $d\\to\\infty$.\n\n- Define the expected squared jump distance per coordinate,\n$$\n\\mathrm{ESJD}_{j} \\equiv \\mathbb{E}\\!\\left[\\,(x'_{j}-x_{j})^{2}\\,\\mathbf{1}\\{ \\text{proposal accepted} \\}\\,\\right],\n$$\nfor a fixed coordinate index $j\\in\\{1,\\dots,d\\}$. Derive how $\\mathrm{ESJD}_{j}$ scales with $d$ under the proposal scaling $\\sigma=\\ell/\\sqrt{d}$, and express the dependence on $\\ell$.\n\n- Determine the value $\\ell^{\\star}0$ that maximizes $\\mathrm{ESJD}_{j}$ with respect to $\\ell$, and deduce the corresponding optimal proposal scale $\\sigma^{\\star}$ as an explicit function of $d$.\n\n- Evaluate numerically $\\ell^{\\star}$ and the leading-order coefficient of $\\mathrm{ESJD}_{j}$ at $\\ell^{\\star}$, rounding all reported numerical constants to four significant figures. Express your final answer as the pair $\\big(\\sigma^{\\star},\\,\\mathrm{ESJD}_{j}\\text{ scaling}\\big)$.\n\nExpress the final answer without units. If you report a numerical value, round it to four significant figures.", "solution": "The problem statement is first validated against the specified criteria.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n- Target probability density: $\\pi_{d}(\\boldsymbol{x}) \\propto \\prod_{i=1}^{d} \\exp(-x_{i}^{2}/2)$, for $\\boldsymbol{x}\\in\\mathbb{R}^{d}$.\n- Proposal distribution for Metropolis-Hastings: A random-walk Gaussian proposal, $\\boldsymbol{x}' \\sim \\mathcal{N}(\\boldsymbol{x}, \\sigma^{2}\\boldsymbol{I}_{d})$, where $\\boldsymbol{I}_{d}$ is the $d\\times d$ identity matrix and $\\sigma0$.\n- Acceptance probability: $a(\\boldsymbol{x},\\boldsymbol{x}') \\equiv \\min\\{1, \\exp(\\ln\\pi_{d}(\\boldsymbol{x}')-\\ln\\pi_{d}(\\boldsymbol{x}))\\}$.\n- Asymptotic regime: The limit $d\\to\\infty$.\n- Proposal scaling ansatz: $\\sigma = \\ell/\\sqrt{d}$, where $\\ell0$.\n- Assumptions: The Markov chain is in stationarity, meaning the current state $\\boldsymbol{x}$ is distributed according to $\\pi_{d}(\\boldsymbol{x})$. The current state $\\boldsymbol{x}$ is independent of the proposal increment $\\boldsymbol{z} = \\boldsymbol{x}' - \\boldsymbol{x}$.\n- Quantity of interest: Expected squared jump distance per coordinate, $\\mathrm{ESJD}_{j} \\equiv \\mathbb{E}[(x'_{j}-x_{j})^{2}\\,\\mathbf{1}\\{ \\text{proposal accepted} \\}]$.\n- Tasks:\n    1.  Derive the asymptotic acceptance probability $\\alpha(\\ell)$ as a function of $\\ell$ as $d\\to\\infty$.\n    2.  Derive the scaling of $\\mathrm{ESJD}_{j}$ with $d$ and its dependence on $\\ell$.\n    3.  Find the value $\\ell^{\\star}$ that maximizes $\\mathrm{ESJD}_{j}$ and the corresponding optimal $\\sigma^{\\star}$.\n    4.  Numerically evaluate $\\ell^{\\star}$ and the leading-order coefficient of $\\mathrm{ESJD}_{j}$ at the optimum, rounded to four significant figures.\n    5.  Provide the final answer as the pair $(\\sigma^{\\star}, \\mathrm{ESJD}_{j}\\text{ scaling})$.\n\n#### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a classic and fundamentally important topic in the theory of Markov chain Monte Carlo methods, specifically concerning the optimal scaling of random-walk Metropolis algorithms in high dimensions. The setup (Gaussian target, Gaussian proposal) serves as a canonical model for local behavior of MCMC algorithms on more general, complex posterior distributions. The analysis is firmly rooted in probability theory, statistics, and the Central Limit Theorem.\n- **Well-Posed:** The problem is well-posed, with all necessary functions, parameters, and assumptions clearly defined to derive a unique, meaningful solution for the requested quantities.\n- **Objective:** The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n- **Completeness and Consistency:** The problem is self-contained and all provided information is internally consistent.\n\n#### Step 3: Verdict and Action\nThe problem is valid. It is a standard, non-trivial exercise in computational statistics and physics that can be solved rigorously from the givens. The solution process will now proceed.\n\n### Derivation of the Solution\n\n#### Asymptotic Acceptance Probability $\\alpha(\\ell)$\nThe target density is $\\pi_{d}(\\boldsymbol{x}) \\propto \\exp(-\\frac{1}{2}\\sum_{i=1}^{d} x_i^2) = \\exp(-\\frac{1}{2}\\|\\boldsymbol{x}\\|^2)$. The unnormalized log-posterior is $\\ln\\pi_{d}(\\boldsymbol{x}) = -\\frac{1}{2}\\|\\boldsymbol{x}\\|^2 + C$, where $C$ is a constant. The change in the log-posterior for a move from $\\boldsymbol{x}$ to $\\boldsymbol{x}'$ is\n$$\n\\Delta E = \\ln\\pi_{d}(\\boldsymbol{x}') - \\ln\\pi_{d}(\\boldsymbol{x}) = -\\frac{1}{2}(\\|\\boldsymbol{x}'\\|^2 - \\|\\boldsymbol{x}\\|^2).\n$$\nThe proposal is $\\boldsymbol{x}' = \\boldsymbol{x} + \\boldsymbol{z}$, where the increment $\\boldsymbol{z} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^2\\boldsymbol{I}_d)$. Substituting this into $\\Delta E$:\n$$\n\\Delta E = -\\frac{1}{2}(\\|\\boldsymbol{x} + \\boldsymbol{z}\\|^2 - \\|\\boldsymbol{x}\\|^2) = -\\frac{1}{2}(\\|\\boldsymbol{x}\\|^2 + 2\\boldsymbol{x}\\cdot\\boldsymbol{z} + \\|\\boldsymbol{z}\\|^2 - \\|\\boldsymbol{x}\\|^2) = -\\left(\\boldsymbol{x}\\cdot\\boldsymbol{z} + \\frac{1}{2}\\|\\boldsymbol{z}\\|^2\\right).\n$$\nUnder the stationarity assumption, the current state $\\boldsymbol{x}$ is drawn from the target distribution, which is a standard multivariate normal in $d$ dimensions, $\\boldsymbol{x} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I}_d)$, so $x_i \\sim \\mathcal{N}(0,1)$ are independent and identically distributed (i.i.d.). The proposal increment components are $z_i \\sim \\mathcal{N}(0, \\sigma^2)$, also i.i.d.\n\nWe analyze the two terms in $\\Delta E$ in the limit $d\\to\\infty$ with the scaling $\\sigma = \\ell/\\sqrt{d}$.\n1.  The term $\\|\\boldsymbol{z}\\|^2 = \\sum_{i=1}^d z_i^2$. The expected value is $\\mathbb{E}[\\|\\boldsymbol{z}\\|^2] = \\sum_{i=1}^d \\mathbb{E}[z_i^2] = d\\sigma^2 = d(\\ell^2/d) = \\ell^2$. By the Law of Large Numbers, as $d\\to\\infty$, the sum of i.i.d. variables converges to its expected value, so $\\|\\boldsymbol{z}\\|^2 \\to \\ell^2$ in probability.\n2.  The term $\\boldsymbol{x}\\cdot\\boldsymbol{z} = \\sum_{i=1}^d x_i z_i$. This is a sum of $d$ i.i.d. random variables $Y_i = x_i z_i$. Since $\\boldsymbol{x}$ and $\\boldsymbol{z}$ are independent, $\\mathbb{E}[Y_i] = \\mathbb{E}[x_i]\\mathbb{E}[z_i] = 0$. The variance is $\\mathbb{Var}(Y_i) = \\mathbb{E}[Y_i^2] - (\\mathbb{E}[Y_i])^2 = \\mathbb{E}[x_i^2]\\mathbb{E}[z_i^2] = (1)(\\sigma^2) = \\ell^2/d$.\n    The variance of the sum is $\\mathbb{Var}(\\boldsymbol{x}\\cdot\\boldsymbol{z}) = \\sum_{i=1}^d \\mathbb{Var}(Y_i) = d(\\ell^2/d) = \\ell^2$.\n    By the Central Limit Theorem, as $d\\to\\infty$, the distribution of $\\boldsymbol{x}\\cdot\\boldsymbol{z}$ converges to a normal distribution with mean $0$ and variance $\\ell^2$. So, $\\boldsymbol{x}\\cdot\\boldsymbol{z} \\to W \\sim \\mathcal{N}(0, \\ell^2)$.\n\nCombining these results, $\\Delta E$ converges in distribution to a random variable $V$:\n$$\nV = -\\left(W + \\frac{1}{2}\\ell^2\\right) \\sim \\mathcal{N}\\left(-\\frac{\\ell^2}{2}, \\ell^2\\right).\n$$\nThe asymptotic acceptance rate $\\alpha(\\ell)$ is the expectation of $\\min\\{1, \\exp(V)\\}$.\n$$\n\\alpha(\\ell) = \\mathbb{E}[\\min\\{1, \\exp(V)\\}] = \\int_{-\\infty}^{\\infty} \\min\\{1, \\exp(v)\\} p(v) dv,\n$$\nwhere $p(v)$ is the density of $V \\sim \\mathcal{N}(-\\ell^2/2, \\ell^2)$. Let $V = -\\ell^2/2 + \\ell Z$, where $Z \\sim \\mathcal{N}(0,1)$. The expectation becomes:\n$$\n\\alpha(\\ell) = \\mathbb{E}_Z[\\min\\{1, \\exp(-\\ell^2/2 + \\ell Z)\\}] = \\int_{-\\infty}^{\\infty} \\min\\{1, \\exp(-\\ell^2/2 + \\ell z)\\} \\phi(z) dz,\n$$\nwhere $\\phi(z) = (2\\pi)^{-1/2}\\exp(-z^2/2)$. The term $\\exp(-\\ell^2/2 + \\ell z)  1$ if $-\\ell^2/2 + \\ell z  0$, which implies $z  \\ell/2$. We split the integral at $z=\\ell/2$:\n$$\n\\alpha(\\ell) = \\int_{-\\infty}^{\\ell/2} \\exp(-\\ell^2/2 + \\ell z) \\phi(z) dz + \\int_{\\ell/2}^{\\infty} 1 \\cdot \\phi(z) dz.\n$$\nThe first integral is:\n$$\n\\int_{-\\infty}^{\\ell/2} \\exp(-\\ell^2/2 + \\ell z) \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2) dz = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\ell/2} \\exp\\left(-\\frac{1}{2}(z^2 - 2\\ell z + \\ell^2)\\right) dz = \\int_{-\\infty}^{\\ell/2} \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{(z-\\ell)^2}{2}\\right) dz.\n$$\nLet $u = z-\\ell$. The integral becomes $\\int_{-\\infty}^{-\\ell/2} \\phi(u) du = \\Phi(-\\ell/2)$, where $\\Phi$ is the standard normal CDF.\nThe second integral is $\\int_{\\ell/2}^{\\infty} \\phi(z) dz = 1 - \\Phi(\\ell/2)$.\nThus, $\\alpha(\\ell) = \\Phi(-\\ell/2) + 1 - \\Phi(\\ell/2)$. Using the symmetry property $\\Phi(-x) = 1 - \\Phi(x)$, we get:\n$$\n\\alpha(\\ell) = 2(1 - \\Phi(\\ell/2)) = 2\\Phi(-\\ell/2).\n$$\n\n#### Expected Squared Jump Distance $\\mathrm{ESJD}_{j}$\nThe quantity of interest is $\\mathrm{ESJD}_{j} = \\mathbb{E}[(x'_{j}-x_{j})^{2}\\,\\mathbf{1}\\{ \\text{proposal accepted} \\}]$.\nLet $\\boldsymbol{z} = \\boldsymbol{x}'-\\boldsymbol{x}$. Then $x'_j-x_j = z_j$. The acceptance event depends on $\\Delta E = -\\sum_{k=1}^d (x_k z_k + z_k^2/2)$. In the limit $d\\to\\infty$, the contribution of any single component (like $z_j$) to the sums becomes negligible. Therefore, the random variable $z_j^2$ becomes independent of the acceptance event. This allows us to decouple the expectation:\n$$\n\\mathrm{ESJD}_{j} \\approx \\mathbb{E}[z_j^2] \\cdot \\mathbb{E}[\\mathbf{1}\\{ \\text{proposal accepted} \\}] = \\mathbb{E}[z_j^2] \\cdot \\mathbb{E}[a(\\boldsymbol{x}, \\boldsymbol{x}')].\n$$\nWe have $\\mathbb{E}[z_j^2] = \\mathbb{Var}(z_j) = \\sigma^2$. The second term is the average acceptance rate, which is $\\alpha(\\ell)$ in the large-$d$ limit.\nSubstituting $\\sigma = \\ell/\\sqrt{d}$ and the expression for $\\alpha(\\ell)$:\n$$\n\\mathrm{ESJD}_{j} \\approx \\sigma^2 \\alpha(\\ell) = \\frac{\\ell^2}{d} \\cdot 2(1 - \\Phi(\\ell/2)).\n$$\n$\\mathrm{ESJD}_{j}$ scales as $1/d$. The dependence on $\\ell$ is through the function $f(\\ell) = 2\\ell^2(1 - \\Phi(\\ell/2))$.\n\n#### Optimization and Numerical Evaluation\nTo maximize $\\mathrm{ESJD}_{j}$ with respect to $\\ell$, we must maximize $f(\\ell)$ for $\\ell0$. We find the derivative of $f(\\ell)$ and set it to zero.\n$$\n\\frac{df}{d\\ell} = \\frac{d}{d\\ell} \\left[ 2\\ell^2(1 - \\Phi(\\ell/2)) \\right] = 4\\ell(1 - \\Phi(\\ell/2)) + 2\\ell^2 \\left(-\\phi(\\ell/2) \\cdot \\frac{1}{2}\\right) = 4\\ell(1 - \\Phi(\\ell/2)) - \\ell^2\\phi(\\ell/2).\n$$\nSetting $\\frac{df}{d\\ell}=0$ for $\\ell0$ gives the optimality condition:\n$$\n4(1 - \\Phi(\\ell/2)) - \\ell\\phi(\\ell/2) = 0.\n$$\nThis is a transcendental equation for the optimal value $\\ell^{\\star}$. Let $x = \\ell/2$. The equation is $2(1-\\Phi(x)) = x\\phi(x)$. This equation must be solved numerically. The solution is $x^{\\star} \\approx 1.1906$, which gives:\n$$\n\\ell^{\\star} = 2x^{\\star} \\approx 2 \\times 1.1906 = 2.3812.\n$$\nRounding to four significant figures, $\\ell^{\\star} \\approx 2.381$.\nThe corresponding optimal proposal scale $\\sigma^{\\star}$ is:\n$$\n\\sigma^{\\star} = \\frac{\\ell^{\\star}}{\\sqrt{d}} = \\frac{2.381}{\\sqrt{d}}.\n$$\nThe leading-order coefficient of $\\mathrm{ESJD}_{j}$ at the optimum is $f(\\ell^{\\star})/d$. The coefficient itself is $f(\\ell^{\\star}) = 2(\\ell^{\\star})^2(1 - \\Phi(\\ell^{\\star}/2))$.\nFrom the optimality condition, $1 - \\Phi(\\ell^{\\star}/2) = \\frac{\\ell^{\\star}}{4}\\phi(\\ell^{\\star}/2)$. Substituting this into $f(\\ell^{\\star})$:\n$$\nf(\\ell^{\\star}) = 2(\\ell^{\\star})^2 \\left(\\frac{\\ell^{\\star}}{4}\\phi(\\ell^{\\star}/2)\\right) = \\frac{(\\ell^{\\star})^3}{2}\\phi(\\ell^{\\star}/2).\n$$\nUsing $\\ell^{\\star} \\approx 2.3812$:\n$$\nf(\\ell^{\\star}) \\approx \\frac{(2.3812)^3}{2} \\phi\\left(\\frac{2.3812}{2}\\right) = \\frac{13.515}{2} \\phi(1.1906).\n$$\nWith $\\phi(1.1906) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-(1.1906)^2/2) \\approx 0.19634$.\n$$\nf(\\ell^{\\star}) \\approx 6.7575 \\times 0.19634 \\approx 1.3268.\n$$\nRounding to four significant figures, the coefficient is $1.327$.\nThe scaling of $\\mathrm{ESJD}_j$ at the optimum is therefore $\\frac{1.327}{d}$.\n\nThe final answer requires the pair $(\\sigma^{\\star}, \\mathrm{ESJD}_{j}\\text{ scaling})$.\n- $\\sigma^{\\star} = \\frac{2.381}{\\sqrt{d}}$\n- $\\mathrm{ESJD}_{j}\\text{ scaling} = \\frac{1.327}{d}$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2.381}{\\sqrt{d}}  \\frac{1.327}{d} \\end{pmatrix}}\n$$", "id": "3521310"}, {"introduction": "Physical parameters are often subject to constraints, such as branching fractions summing to one, which confines them to a simplex. Naively applying standard MCMC proposals that operate in an unconstrained Euclidean space will violate these constraints and produce invalid results. This practice [@problem_id:3521320] guides you through the standard and rigorous solution: reparameterization. You will learn how to map the constrained simplex space to an unconstrained $\\mathbb{R}^{d-1}$ space using a logistic transform, enabling the use of simple Gaussian proposals, and critically, you will derive the Jacobian determinant of this transformation, which is essential for preserving the correct target probability distribution.", "problem": "A heavy neutral resonance in a high-energy collider experiment decays into $d$ exclusive channels. Let the branching fraction vector be $b = (b_1, b_2, \\dots, b_d)$ constrained to the unit simplex, so that $b_i  0$ for all $i$ and $\\sum_{i=1}^{d} b_i = 1$. You observe counts $n_1, n_2, \\dots, n_d$ from $N = \\sum_{i=1}^{d} n_i$ decays, and you model the likelihood by a multinomial distribution $L(n \\mid b) \\propto \\prod_{i=1}^{d} b_i^{n_i}$ with a Dirichlet prior $p(b) \\propto \\prod_{i=1}^{d} b_i^{a_i - 1}$, where the hyperparameters $a_i  0$ are fixed. You wish to sample from the posterior distribution $\\pi_Y(b \\mid n) \\propto L(n \\mid b) p(b)$ using Markov chain Monte Carlo (MCMC), specifically the Metropolis-Hastings (MH) method, while respecting the simplex constraints.\n\nTo build an unconstrained parameterization, consider the additive log-ratio logistic transform from the simplex to $\\mathbb{R}^{d-1}$ defined by\n$$\nz_i = \\ln\\!\\left(\\frac{b_i}{b_d}\\right), \\quad i = 1, 2, \\dots, d-1,\n$$\nwith inverse map\n$$\nb_i(z) = \\frac{\\exp(z_i)}{1 + \\sum_{k=1}^{d-1} \\exp(z_k)}, \\quad i = 1, \\dots, d-1, \\qquad\nb_d(z) = \\frac{1}{1 + \\sum_{k=1}^{d-1} \\exp(z_k)}.\n$$\nYou design an MH scheme that proposes in the unconstrained space via a symmetric Gaussian random walk $q(z' \\mid z)$ on $\\mathbb{R}^{d-1}$, and then maps back to $b' = b(z')$ for evaluation in the physical space. By the change-of-variables formula, the target density in $z$-space is\n$$\n\\pi_Z(z \\mid n) \\propto \\pi_Y(b(z) \\mid n) \\left| \\det\\!\\left(\\frac{\\partial b}{\\partial z}\\right) \\right|,\n$$\nwhere $\\frac{\\partial b}{\\partial z}$ denotes the $(d-1) \\times (d-1)$ Jacobian matrix of the inverse map $z \\mapsto (b_1, \\dots, b_{d-1})$.\n\nStarting from the fundamental definitions of the Metropolis-Hastings acceptance rule and the change-of-variables theorem for probability densities, derive the required Jacobian factor $\\left| \\det\\!\\left(\\frac{\\partial b}{\\partial z}\\right) \\right|$ as a closed-form expression in terms of $b_1, \\dots, b_d$. Express your final answer as a single analytic expression. No numerical approximation is required.", "solution": "The problem requires the derivation of the Jacobian factor $\\left| \\det\\left(\\frac{\\partial b}{\\partial z}\\right) \\right|$ for a change of variables from a constrained simplex space to an unconstrained Euclidean space. This factor is essential for correctly formulating the Metropolis-Hastings acceptance probability when proposing moves in the unconstrained space.\n\nLet the vector of unconstrained variables be $z = (z_1, z_2, \\dots, z_{d-1}) \\in \\mathbb{R}^{d-1}$. The problem defines the mapping from this space back to the first $d-1$ components of the branching fraction vector $b = (b_1, b_2, \\dots, b_d)$. The vector $b$ lives on the $(d-1)$-simplex $\\mathcal{S}^{d-1}$, defined by $b_i  0$ for all $i=1, \\dots, d$ and $\\sum_{i=1}^{d} b_i = 1$. The inverse map is given by:\n$$\nb_i(z) = \\frac{\\exp(z_i)}{1 + \\sum_{k=1}^{d-1} \\exp(z_k)}, \\quad i = 1, \\dots, d-1\n$$\n$$\nb_d(z) = \\frac{1}{1 + \\sum_{k=1}^{d-1} \\exp(z_k)}\n$$\nThe quantity to be calculated is the absolute value of the determinant of the Jacobian matrix $J = \\frac{\\partial b}{\\partial z}$, where $J$ is a $(d-1) \\times (d-1)$ matrix with entries $J_{ij} = \\frac{\\partial b_i}{\\partial z_j}$ for $i, j \\in \\{1, 2, \\dots, d-1\\}$.\n\nFirst, we calculate the partial derivatives $J_{ij}$. Let us define the denominator as $S(z) = 1 + \\sum_{k=1}^{d-1} \\exp(z_k)$. From the provided map definitions, we can establish a direct relationship between $b_i$ and $S$.\nNotice that $b_d = 1/S$ and $b_i = b_d \\exp(z_i)$ for $i=1, \\dots, d-1$. This implies $\\exp(z_i) = b_i/b_d$.\nWe can verify the consistency of $S(z)$:\n$$\nS(z) = 1 + \\sum_{k=1}^{d-1} \\exp(z_k) = 1 + \\sum_{k=1}^{d-1} \\frac{b_k}{b_d} = \\frac{b_d + \\sum_{k=1}^{d-1} b_k}{b_d} = \\frac{\\sum_{k=1}^{d} b_k}{b_d} = \\frac{1}{b_d}\n$$\nThis confirms that $b_i = \\exp(z_i) / S = \\exp(z_i) b_d$.\n\nNow, we compute the partial derivatives $\\frac{\\partial b_i}{\\partial z_j}$ for $i, j \\in \\{1, \\dots, d-1\\}$ using the quotient rule.\n\nCase 1: $i = j$.\n$$\n\\frac{\\partial b_i}{\\partial z_i} = \\frac{\\partial}{\\partial z_i} \\left( \\frac{\\exp(z_i)}{S(z)} \\right) = \\frac{\\frac{\\partial \\exp(z_i)}{\\partial z_i} S(z) - \\exp(z_i) \\frac{\\partial S(z)}{\\partial z_i}}{S(z)^2}\n$$\nSince $\\frac{\\partial S(z)}{\\partial z_i} = \\exp(z_i)$, we have:\n$$\n\\frac{\\partial b_i}{\\partial z_i} = \\frac{\\exp(z_i) S(z) - \\exp(z_i) \\exp(z_i)}{S(z)^2} = \\frac{\\exp(z_i)}{S(z)} - \\left( \\frac{\\exp(z_i)}{S(z)} \\right)^2\n$$\nSubstituting $b_i = \\exp(z_i)/S(z)$, we get:\n$$\n\\frac{\\partial b_i}{\\partial z_i} = b_i - b_i^2 = b_i(1 - b_i)\n$$\n\nCase 2: $i \\neq j$.\n$$\n\\frac{\\partial b_i}{\\partial z_j} = \\frac{\\partial}{\\partial z_j} \\left( \\frac{\\exp(z_i)}{S(z)} \\right) = \\frac{0 \\cdot S(z) - \\exp(z_i) \\frac{\\partial S(z)}{\\partial z_j}}{S(z)^2}\n$$\nSince $\\frac{\\partial S(z)}{\\partial z_j} = \\exp(z_j)$, we have:\n$$\n\\frac{\\partial b_i}{\\partial z_j} = \\frac{-\\exp(z_i) \\exp(z_j)}{S(z)^2} = - \\left( \\frac{\\exp(z_i)}{S(z)} \\right) \\left( \\frac{\\exp(z_j)}{S(z)} \\right)\n$$\nSubstituting $b_i = \\exp(z_i)/S(z)$ and $b_j = \\exp(z_j)/S(z)$, we get:\n$$\n\\frac{\\partial b_i}{\\partial z_j} = -b_i b_j\n$$\n\nThe entries of the Jacobian matrix $J$ are therefore:\n$$\nJ_{ij} = \\frac{\\partial b_i}{\\partial z_j} =\n\\begin{cases}\nb_i(1 - b_i)  \\text{if } i = j \\\\\n-b_i b_j  \\text{if } i \\neq j\n\\end{cases}\n$$\nThis can be written compactly as $J_{ij} = \\delta_{ij} b_i - b_i b_j$, where $\\delta_{ij}$ is the Kronecker delta.\n\nWe now compute the determinant of this $(d-1) \\times (d-1)$ matrix $J$. We can factor out $b_i$ from each row $i$:\n$$\nJ = \n\\begin{pmatrix}\nb_1(1-b_1)  -b_1 b_2  \\dots  -b_1 b_{d-1} \\\\\n-b_2 b_1  b_2(1-b_2)  \\dots  -b_2 b_{d-1} \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n-b_{d-1} b_1  -b_{d-1} b_2  \\dots  b_{d-1}(1-b_{d-1})\n\\end{pmatrix}\n$$\nLet $D = \\text{diag}(b_1, b_2, \\dots, b_{d-1})$. We can write $J = D \\cdot M$, where $M$ is the matrix with entries $M_{ij} = \\delta_{ij} - b_j$.\n$$\nM = \n\\begin{pmatrix}\n1-b_1  -b_2  \\dots  -b_{d-1} \\\\\n-b_1  1-b_2  \\dots  -b_{d-1} \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n-b_1  -b_2  \\dots  1-b_{d-1}\n\\end{pmatrix}\n$$\nThe determinant of $J$ is $\\det(J) = \\det(D) \\det(M)$.\nThe determinant of the diagonal matrix $D$ is simply the product of its diagonal elements:\n$$\n\\det(D) = \\prod_{i=1}^{d-1} b_i\n$$\nTo find the determinant of $M$, we recognize that $M$ has the form $I - uv^T$, where $I$ is the $(d-1) \\times (d-1)$ identity matrix, $u$ is a column vector of ones, $u = (1, 1, \\dots, 1)^T$, and $v$ is the column vector of the first $d-1$ branching fractions, $v = (b_1, b_2, \\dots, b_{d-1})^T$.\n$$\nuv^T = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} \\begin{pmatrix} b_1  b_2  \\dots  b_{d-1} \\end{pmatrix} = \\begin{pmatrix}\nb_1  b_2  \\dots  b_{d-1} \\\\\nb_1  b_2  \\dots  b_{d-1} \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\nb_1  b_2  \\dots  b_{d-1}\n\\end{pmatrix}\n$$\nThe matrix $M$ is indeed $I - uv^T$. By the matrix determinant lemma, for any vectors $u, v$, we have $\\det(I - uv^T) = 1 - v^T u$.\nIn our case,\n$$\nv^T u = \\begin{pmatrix} b_1  b_2  \\dots  b_{d-1} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} = \\sum_{k=1}^{d-1} b_k\n$$\nTherefore, the determinant of $M$ is:\n$$\n\\det(M) = 1 - \\sum_{k=1}^{d-1} b_k\n$$\nUsing the simplex constraint $\\sum_{k=1}^{d} b_k = 1$, we can express this result in terms of $b_d$:\n$$\n1 - \\sum_{k=1}^{d-1} b_k = b_d\n$$\nSo, $\\det(M) = b_d$.\n\nCombining these results, the determinant of the Jacobian matrix $J$ is:\n$$\n\\det(J) = \\det(D) \\det(M) = \\left( \\prod_{i=1}^{d-1} b_i \\right) b_d\n$$\nThis gives the product of all $d$ branching fractions:\n$$\n\\det(J) = \\prod_{i=1}^{d} b_i\n$$\nThe final step is to take the absolute value as required for the change of variables formula for probability densities. The problem states that $b_i  0$ for all $i$. Consequently, the product $\\prod_{i=1}^{d} b_i$ is strictly positive.\nThus, the required Jacobian factor is:\n$$\n\\left| \\det\\left(\\frac{\\partial b}{\\partial z}\\right) \\right| = \\left| \\prod_{i=1}^{d} b_i \\right| = \\prod_{i=1}^{d} b_i\n$$\nThis is the closed-form expression for the Jacobian determinant in terms of the branching fractions $b_1, \\dots, b_d$.", "answer": "$$\\boxed{\\prod_{i=1}^{d} b_i}$$", "id": "3521320"}, {"introduction": "Many target distributions in computational physics, such as those in lattice gauge theory, are multimodal, possessing multiple distinct regions of high probability (like topological sectors) separated by high energy barriers. A standard MCMC sampler can become trapped in a single mode, failing to explore the full distribution. This advanced practice [@problem_id:3521299] addresses this challenge by having you design a key component of Parallel Tempering, a powerful enhanced sampling technique. Starting from physical principles, you will derive the optimal spacing for a temperature ladder that ensures efficient exchange of information between parallel simulations, allowing the system to overcome energy barriers and achieve ergodic sampling.", "problem": "You are tasked with constructing a complete, runnable program that designs a temperature ladder for tempered transitions tailored to sampling distinct instanton sectors in a lattice gauge theory setting. The target distribution over gauge fields is given by a Boltzmann weight with action $S(U)$, so that configurations are distributed according to $\\pi_{\\beta}(U) \\propto \\exp(-\\beta S(U))$ where $\\beta = 1/T$ is the inverse temperature and $T$ is the temperature. Consider a set of $m$ replicas at temperatures $T_1  T_2  \\dots  T_m$, with replica exchange moves allowed between adjacent temperatures. For a swap proposal that exchanges configurations between replicas $i$ and $j$, the Metropolis-Hastings acceptance probability is\n$$\n\\alpha_{i \\leftrightarrow j} = \\min\\left\\{1, \\exp\\left[(1/T_i - 1/T_j)(S(U_j) - S(U_i))\\right]\\right\\}.\n$$\nYour objective is to design the temperature ladder $T_1  T_2  \\dots  T_m$ that maximizes the expected swap acceptance between adjacent replicas subject to a fixed computational budget determined by a fixed number of replicas $m$ and fixed boundary temperatures $T_1$ and $T_m$. You must build the ladder from fundamental principles of Markov Chain Monte Carlo (MCMC), particularly the Metropolis-Hastings method, and well-tested thermodynamic facts.\n\nStarting point and modeling assumptions:\n- Use the canonical ensemble with inverse temperature $\\beta = 1/T$ and Boltzmann constant set to $k_B = 1$.\n- Model the action as a random variable $E$ with a Gaussian distribution that is consistent with constant heat capacity $C_v$:\n  - Mean $\\mu(T) = E_0 + C_v T$,\n  - Variance $\\sigma^2(T) = C_v T^2$.\n- These relations agree with standard canonical identities: $\\mathrm{Var}(E) = C_v T^2$ and $d\\langle E\\rangle/dT = C_v$, together with $d\\langle E\\rangle/d\\beta = -\\mathrm{Var}(E)$, ensuring scientific realism within computational high-energy physics.\n\nTasks:\n1. From first principles, design an algorithm that, under small inter-replica temperature gaps, approximately maximizes the expected swap acceptance between adjacent replicas by appropriately spacing the inverse temperatures $\\beta_i = 1/T_i$ along the ladder. You must derive the spacing rule starting from the Metropolis-Hastings acceptance definition and the Gaussian action model, using a principled approximation that is valid for small increments.\n2. Implement a numerically stable function that computes the expected swap acceptance between two temperatures $T_i$ and $T_j$ under the Gaussian model, without simulating individual gauge configurations. The expectation must be computed analytically by integrating over the Gaussian distributions of $E_i \\sim \\mathcal{N}(\\mu(T_i),\\sigma^2(T_i))$ and $E_j \\sim \\mathcal{N}(\\mu(T_j),\\sigma^2(T_j))$.\n3. Using your derived spacing rule, construct the temperature ladder for each test case below. Ensure the temperatures are strictly increasing and lie within the prescribed bounds.\n4. For each test case, output the temperature ladder as a list of floating-point numbers rounded to six decimal places.\n\nTest suite:\n- Case A (general case): $m=6$, $T_1=0.5$, $T_m=2.0$, $C_v=20$, $E_0=0.0$.\n- Case B (different range and offset): $m=5$, $T_1=0.7$, $T_m=1.8$, $C_v=15$, $E_0=1.0$.\n- Case C (edge case with tight ladder at higher heat capacity): $m=4$, $T_1=0.9$, $T_m=1.2$, $C_v=50$, $E_0=0.0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the ladders for all test cases as a comma-separated list enclosed in square brackets, where each element is itself a list of temperatures for one test case (e.g., $[[t_{1,1},t_{1,2},\\dots,t_{1,m}], [t_{2,1},\\dots,t_{2,m}], [t_{3,1},\\dots,t_{3,m}]]$).\n- All temperatures must be rounded to six decimal places.\n- Temperatures are dimensionless in this formulation, so no physical unit needs to be printed.", "solution": "The problem requires the design of an optimal temperature ladder for replica exchange Monte Carlo, also known as parallel tempering. The objective is to maximize the swap acceptance rate between adjacent replicas, given a fixed number of replicas $m$ and boundary temperatures $T_1$ and $T_m$. The system's action (energy) $S(U)$ at a given temperature $T$ is modeled as a Gaussian random variable $E$ with mean $\\mu(T) = E_0 + C_v T$ and variance $\\sigma^2(T) = C_v T^2$.\n\n### Principle-Based Derivation of the Temperature Spacing Rule\n\n**1. Maximizing Swap Acceptance**\n\nThe Metropolis-Hastings acceptance probability for a swap of configurations between replicas at temperatures $T_i$ and $T_{i+1}$ (with corresponding inverse temperatures $\\beta_i=1/T_i$ and $\\beta_{i+1}=1/T_{i+1}$) is given by:\n$$\n\\alpha_{i \\leftrightarrow i+1} = \\min\\left\\{1, \\exp\\left[(\\beta_i - \\beta_{i+1})(E_{i+1} - E_i)\\right]\\right\\}\n$$\nwhere $E_i$ and $E_{i+1}$ are the energies of the configurations in the respective replicas. To ensure an efficient random walk through temperature space, it is desirable to have a uniform (and high) acceptance probability $\\langle \\alpha_{i \\leftrightarrow i+1} \\rangle$ across all adjacent pairs $(i, i+1)$. A bottleneck with a low acceptance rate anywhere in the ladder would trap configurations and hinder sampling efficiency.\n\n**2. The Thermodynamic Distance Heuristic**\n\nDirectly solving the equation $\\langle \\alpha_{i \\leftrightarrow i+1} \\rangle = \\text{const}$ for all $i$ is analytically challenging. A widely used and effective heuristic is to instead require that the \"thermodynamic distance\" between adjacent temperatures is constant. This is based on the reasoning that for small temperature gaps, the acceptance probability is a monotonic function of the statistical overlap between the energy distributions of adjacent replicas. By keeping this overlap constant, we expect the acceptance rate to also be roughly constant.\n\n**3. Defining the Metric**\n\nA natural way to define the distance between two statistical systems at infinitesimally different inverse temperatures $\\beta$ and $\\beta+d\\beta$ is through an information-geometric metric. The metric element $ds$ can be chosen to be proportional to the change $d\\beta$ scaled by a factor that accounts for the typical energy fluctuations. A standard choice, which can be derived from the Fisher information metric for the canonical ensemble, is to relate the step size to the standard deviation of the energy, $\\sigma_E(T)$. We define the infinitesimal distance as:\n$$\nds \\propto d\\beta \\cdot \\sigma_E(T)\n$$\nUsing the provided model, $\\sigma_E(T) = \\sqrt{\\mathrm{Var}(E)} = \\sqrt{C_v T^2} = \\sqrt{C_v} T$. Substituting $T=1/\\beta$:\n$$\nds = k \\cdot d\\beta \\cdot \\frac{\\sqrt{C_v}}{\\beta}\n$$\nwhere $k$ is an arbitrary constant of proportionality. For the spacing to be optimal, we need to take equal steps in this new coordinate $s$.\n\n**4. Integration and Discretization**\n\nTo create $m-1$ equal intervals between $T_1$ (or $\\beta_1$) and $T_m$ (or $\\beta_m$), we first compute the total thermodynamic distance $S_{total}$ by integrating $ds$ over the entire range. Since we have $T_1  T_m$, it follows that $\\beta_1  \\beta_m$.\n$$\nS_{total} = \\int_{\\beta_m}^{\\beta_1} k \\frac{\\sqrt{C_v}}{\\beta} d\\beta = k\\sqrt{C_v} [\\ln \\beta]_{\\beta_m}^{\\beta_1} = k\\sqrt{C_v} (\\ln \\beta_1 - \\ln \\beta_m)\n$$\nWe divide this total distance into $m-1$ equal segments, each of length $\\Delta S = \\frac{S_{total}}{m-1}$. The condition for the temperatures in the ladder is that the distance between any adjacent pair $(\\beta_{i+1}, \\beta_i)$ is equal to $\\Delta S$:\n$$\n\\int_{\\beta_{i+1}}^{\\beta_i} k \\frac{\\sqrt{C_v}}{\\beta} d\\beta = \\Delta S\n$$\nThis yields:\n$$\nk\\sqrt{C_v} (\\ln \\beta_i - \\ln \\beta_{i+1}) = \\frac{k\\sqrt{C_v} (\\ln \\beta_1 - \\ln \\beta_m)}{m-1}\n$$\nSimplifying, we obtain the core spacing rule:\n$$\n\\ln \\beta_i - \\ln \\beta_{i+1} = \\frac{\\ln \\beta_1 - \\ln \\beta_m}{m-1} = \\text{const}\n$$\nThis demonstrates that the logarithms of the inverse temperatures, $\\ln \\beta_i$, must be linearly spaced.\n\n**5. The Geometric Progression Rule**\n\nThe linear spacing of $\\ln \\beta_i$ implies a geometric progression for the $\\beta_i$ values themselves. Let the constant difference be $C = \\ln\\beta_i - \\ln\\beta_{i+1}$. Then $\\ln\\beta_{i+1} = \\ln\\beta_i - C$, which means $\\beta_{i+1} = \\beta_i e^{-C}$. The inverse temperatures form a geometric series.\n\nThe $i$-th inverse temperature $\\beta_i$ (for $i=1, \\dots, m$) can be expressed as:\n$$\n\\beta_i = \\beta_1 \\cdot r^{i-1}\n$$\nwhere $r$ is the common ratio. We can find $r$ using the boundary condition at $i=m$:\n$$\n\\beta_m = \\beta_1 \\cdot r^{m-1} \\implies r = \\left(\\frac{\\beta_m}{\\beta_1}\\right)^{\\frac{1}{m-1}}\n$$\nSince $T_i = 1/\\beta_i$, the temperatures also form a geometric progression:\n$$\nT_i = \\frac{1}{\\beta_1 \\cdot r^{i-1}} = T_1 \\cdot \\left(\\frac{1}{r}\\right)^{i-1}\n$$\nLet the temperature ratio be $q = 1/r$. Substituting the expression for $r$:\n$$\nq = \\left(\\frac{\\beta_1}{\\beta_m}\\right)^{\\frac{1}{m-1}} = \\left(\\frac{T_m}{T_1}\\right)^{\\frac{1}{m-1}}\n$$\nThus, the temperature ladder is given by the geometric progression:\n$$\nT_i = T_1 \\cdot q^{i-1} = T_1 \\cdot \\left(\\frac{T_m}{T_1}\\right)^{\\frac{i-1}{m-1}} \\quad \\text{for } i = 1, 2, \\dots, m\n$$\nThis algorithm depends only on the boundary temperatures $T_1$, $T_m$, and the number of replicas $m$. The specific value of the constant heat capacity $C_v$ was essential for the derivation (specifically, that $C_v$ is constant), but it cancels out of the final formula for the relative spacing.\n\n### Analytical Formula for Expected Acceptance\n\nAs a secondary task, we derive the analytical expression for the expected swap acceptance probability $\\langle \\alpha_{a \\leftrightarrow b} \\rangle$ between two temperatures $T_a$ and $T_b$. Let $T_b  T_a$, so $\\beta_b  \\beta_a$. The swap exponent is $X = (\\beta_a - \\beta_b)(E_b - E_a)$. Since $E_a$ and $E_b$ are independent Gaussian variables, $E_a \\sim \\mathcal{N}(\\mu_a, \\sigma_a^2)$ and $E_b \\sim \\mathcal{N}(\\mu_b, \\sigma_b^2)$, their difference $E_b - E_a$ is also Gaussian. Thus, $X$ is a Gaussian random variable, $X \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)$, with parameters:\n$$\n\\mu_X = (\\beta_a - \\beta_b)(\\mu_b - \\mu_a) = C_v \\frac{(T_b - T_a)^2}{T_a T_b}\n$$\n$$\n\\sigma_X^2 = (\\beta_a - \\beta_b)^2 (\\sigma_a^2 + \\sigma_b^2) = C_v \\frac{(T_b - T_a)^2 (T_a^2 + T_b^2)}{(T_a T_b)^2}\n$$\nThe expected acceptance is the integral over the distribution $p(x)$ of $X$:\n$$\n\\langle \\alpha \\rangle = \\int_{-\\infty}^{\\infty} \\min(1, e^x) p(x) dx = \\int_{-\\infty}^{0} e^x p(x) dx + \\int_{0}^{\\infty} p(x) dx\n$$\nThe evaluation of these integrals yields the closed-form expression:\n$$\n\\langle \\alpha \\rangle = e^{\\mu_X + \\sigma_X^2/2} \\Phi\\left(-\\frac{\\mu_X + \\sigma_X^2}{\\sigma_X}\\right) + \\Phi\\left(\\frac{\\mu_X}{\\sigma_X}\\right)\n$$\nwhere $\\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution. This formula can be implemented for verification but is not used in constructing the ladder itself.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef calculate_expected_acceptance(Ta, Tb, Cv, E0):\n    \"\"\"\n    Computes the expected swap acceptance probability between two replicas\n    at temperatures Ta and Tb, based on the analytical formula for a\n    Gaussian energy model.\n\n    This function is implemented as per the problem description but is not\n    used for generating the temperature ladder itself.\n\n    Args:\n        Ta (float): Temperature of the first replica.\n        Tb (float): Temperature of the second replica.\n        Cv (float): Constant volume heat capacity.\n        E0 (float): Energy offset.\n\n    Returns:\n        float: The expected acceptance probability.\n    \"\"\"\n    if Ta == Tb:\n        return 1.0\n\n    # Ensure Ta  Tb for consistency\n    if Ta  Tb:\n        Ta, Tb = Tb, Ta\n\n    beta_a = 1.0 / Ta\n    beta_b = 1.0 / Tb\n\n    # Parameters for the energy distributions E_a and E_b\n    # mu_a = E0 + Cv * Ta\n    # mu_b = E0 + Cv * Tb\n    # sigma_sq_a = Cv * Ta**2\n    # sigma_sq_b = Cv * Tb**2\n    \n    # The random variable for the exponent is X = (beta_a - beta_b) * (E_b - E_a)\n    # X follows a normal distribution N(mu_X, sigma_sq_X)\n    \n    # Calculate mu_X\n    mu_b_minus_mu_a = Cv * (Tb - Ta)\n    beta_a_minus_beta_b = (1.0 / Ta) - (1.0 / Tb)\n    mu_X = beta_a_minus_beta_b * mu_b_minus_mu_a\n\n    # Calculate sigma_sq_X\n    sigma_sq_a_plus_sigma_sq_b = Cv * (Ta**2 + Tb**2)\n    sigma_sq_X = (beta_a_minus_beta_b**2) * sigma_sq_a_plus_sigma_sq_b\n\n    if sigma_sq_X  1e-12: # Avoid division by zero if temperatures are very close\n        return 1.0 if mu_X =0 else np.exp(mu_X)\n\n    sigma_X = np.sqrt(sigma_sq_X)\n\n    # The formula for expected acceptance is:\n    # E[alpha] = exp(mu_X + sigma_sq_X/2) * Phi(-(mu_X + sigma_sq_X)/sigma_X) + Phi(mu_X/sigma_X)\n    # where Phi is the standard normal CDF.\n\n    # This can be numerically unstable if the arguments are large.\n    # We use log-space calculations for the first term.\n    arg1_cdf = -(mu_X + sigma_sq_X) / sigma_X\n    log_term1 = mu_X + sigma_sq_X / 2.0 + norm.logcdf(arg1_cdf)\n    term1 = np.exp(log_term1)\n    \n    arg2_cdf = mu_X / sigma_X\n    term2 = norm.cdf(arg2_cdf)\n\n    return term1 + term2\n\ndef construct_temperature_ladder(m, T1, Tm, Cv, E0):\n    \"\"\"\n    Constructs a temperature ladder with m replicas between T1 and Tm.\n\n    The ladder is a geometric progression in temperature, derived from the\n    principle of keeping the \"thermodynamic distance\" between adjacent\n    replicas constant for a system with constant heat capacity Cv.\n\n    Args:\n        m (int): The number of replicas.\n        T1 (float): The lowest temperature.\n        Tm (float): The highest temperature.\n        Cv (float): Constant volume heat capacity (used in derivation, not formula).\n        E0 (float): Energy offset (used in derivation, not formula).\n\n    Returns:\n        list[float]: A list of m temperatures forming the ladder.\n    \"\"\"\n    if m  2:\n        return [T1] if m == 1 else []\n        \n    # The temperatures {T_i} form a geometric progression T_i = T_1 * q^(i-1)\n    # The ratio q is determined by the boundary conditions T_1 and T_m.\n    # T_m = T_1 * q^(m-1) = q = (T_m / T_1)^(1 / (m-1))\n    \n    ratio = (Tm / T1)**(1.0 / (m - 1))\n    \n    temperatures = [T1 * (ratio**i) for i in range(m)]\n    \n    return temperatures\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: m=6, T1=0.5, Tm=2.0, Cv=20, E0=0.0\n        {'m': 6, 'T1': 0.5, 'Tm': 2.0, 'Cv': 20, 'E0': 0.0},\n        # Case B: m=5, T1=0.7, Tm=1.8, Cv=15, E0=1.0\n        {'m': 5, 'T1': 0.7, 'Tm': 1.8, 'Cv': 15, 'E0': 1.0},\n        # Case C: m=4, T1=0.9, Tm=1.2, Cv=50, E0=0.0\n        {'m': 4, 'T1': 0.9, 'Tm': 1.2, 'Cv': 50, 'E0': 0.0},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        ladder = construct_temperature_ladder(case['m'], case['T1'], case['Tm'], case['Cv'], case['E0'])\n        # Format results to six decimal places as required.\n        formatted_ladder = [f\"{temp:.6f}\" for temp in ladder]\n        all_results.append(formatted_ladder)\n\n    # Format the final output string to be exactly as specified:\n    # [[t1,t2,...],[...]] with no spaces.\n    inner_strings = []\n    for res_list in all_results:\n        inner_str = '[' + ','.join(res_list) + ']'\n        inner_strings.append(inner_str)\n    \n    final_output_string = '[' + ','.join(inner_strings) + ']'\n    \n    print(final_output_string)\n\nsolve()\n```", "id": "3521299"}]}